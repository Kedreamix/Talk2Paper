<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Talking Head Generation">
    <meta name="description" content="Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2025-09-28  Unlocking Financial Insights An advanced Multimodal Summarization with   Multimodal Output Framework for Financial Advisory Videos">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Talking Head Generation | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Talking Head Generation/2509.20961v1/page_5_1.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Talking Head Generation</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                                <span class="chip bg-color">Talking Head Generation</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                Talking Head Generation
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-09-28
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-10-02
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    6.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    26 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-09-28-更新"><a href="#2025-09-28-更新" class="headerlink" title="2025-09-28 更新"></a>2025-09-28 更新</h1><h2 id="Unlocking-Financial-Insights-An-advanced-Multimodal-Summarization-with-Multimodal-Output-Framework-for-Financial-Advisory-Videos"><a href="#Unlocking-Financial-Insights-An-advanced-Multimodal-Summarization-with-Multimodal-Output-Framework-for-Financial-Advisory-Videos" class="headerlink" title="Unlocking Financial Insights: An advanced Multimodal Summarization with   Multimodal Output Framework for Financial Advisory Videos"></a>Unlocking Financial Insights: An advanced Multimodal Summarization with   Multimodal Output Framework for Financial Advisory Videos</h2><p><strong>Authors:Sarmistha Das, R E Zera Marveen Lyngkhoi, Sriparna Saha, Alka Maurya</strong></p>
<p>The dynamic propagation of social media has broadened the reach of financial advisory content through podcast videos, yet extracting insights from lengthy, multimodal segments (30-40 minutes) remains challenging. We introduce FASTER (Financial Advisory Summariser with Textual Embedded Relevant images), a modular framework that tackles three key challenges: (1) extracting modality-specific features, (2) producing optimized, concise summaries, and (3) aligning visual keyframes with associated textual points. FASTER employs BLIP for semantic visual descriptions, OCR for textual patterns, and Whisper-based transcription with Speaker diarization as BOS features. A modified Direct Preference Optimization (DPO)-based loss function, equipped with BOS-specific fact-checking, ensures precision, relevance, and factual consistency against the human-aligned summary. A ranker-based retrieval mechanism further aligns keyframes with summarized content, enhancing interpretability and cross-modal coherence. To acknowledge data resource scarcity, we introduce Fin-APT, a dataset comprising 470 publicly accessible financial advisory pep-talk videos for robust multimodal research. Comprehensive cross-domain experiments confirm FASTER’s strong performance, robustness, and generalizability when compared to Large Language Models (LLMs) and Vision-Language Models (VLMs). By establishing a new standard for multimodal summarization, FASTER makes financial advisory content more accessible and actionable, thereby opening new avenues for research. The dataset and code are available at: <a target="_blank" rel="noopener" href="https://github.com/sarmistha-D/FASTER">https://github.com/sarmistha-D/FASTER</a> </p>
<blockquote>
<p>社交媒体动态的传播方式通过播客视频扩大了金融咨询内容的覆盖范围，但从长达30-40分钟的多媒体片段中提取见解仍然具有挑战性。我们推出了FASTER（带文本嵌入相关图像的金融咨询摘要器），这是一个模块化框架，解决了三个关键挑战：（1）提取特定模态的特征，（2）生成优化、简洁的摘要，（3）将视觉关键帧与相关的文本点对齐。FASTER采用BLIP进行语义视觉描述、OCR进行文本模式识别，以及基于Whisper的转录和说话者分档作为BOS特征。一个经过修改的基于直接偏好优化（DPO）的损失函数，配备了BOS特定的事实核查，确保了与人类对齐摘要的精确性、相关性和事实一致性。基于排名的检索机制进一步将关键帧与摘要内容对齐，提高了可解释性和跨模态一致性。为了应对数据资源稀缺的问题，我们推出了Fin-APT数据集，该数据集包含470个可公开访问的金融咨询演讲视频，用于稳健的多模态研究。与大型语言模型（LLM）和视觉语言模型（VLM）相比，全面的跨域实验证实了FASTER在性能、稳健性和通用性方面的优势。通过建立多模态摘要的新标准，FASTER使金融咨询内容更容易获取和操作，从而为研究开辟了新途径。数据集和代码可在以下网址找到：<a target="_blank" rel="noopener" href="https://github.com/sarmistha-D/FASTER">https://github.com/sarmistha-D/FASTER</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.20961v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>金融咨询内容通过社交媒体动态传播，以播客视频的形式扩大影响力范围。然而，从冗长、多模式（长达30至40分钟）的片段中提取见解仍具挑战性。本文介绍了FASTER（配备文本嵌入相关图像的金融咨询摘要器），该模块化框架解决了三个关键问题：提取模式特定特征、生成优化简洁摘要以及将视觉关键帧与关联文本点对齐。通过BLIP进行语义视觉描述、OCR识别文本模式，并结合基于Whisper的转录与演讲者分度作为语音特征。通过修改后的基于直接偏好优化（DPO）的损失函数与特定于语音的事实核查，确保摘要的精确度、相关性和事实一致性。基于排名器的检索机制进一步将关键帧与摘要内容对齐，提高了可解释性和跨模态一致性。为解决数据资源稀缺问题，本文引入了包含470个公开可访问金融咨询演讲视频的Fin-APT数据集，用于稳健的多模态研究。实验表明，相较于大型语言模型和视觉语言模型，FASTER表现出强大的性能、稳健性和泛化能力。它建立了新的多模态摘要标准，使金融咨询内容更加易于获取和操作，为相关研究开辟了新途径。数据集和代码可通过链接访问：[链接地址]。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>金融咨询内容通过社交媒体传播，特别是通过播客视频形式扩大影响力范围。</li>
<li>从冗长的金融咨询视频中提取见解存在挑战，需要模块化框架来解决关键问题。</li>
<li>FASTER框架解决了提取模式特定特征、生成简洁摘要和对齐视觉关键帧与文本点的问题。</li>
<li>FASTER使用多种技术如BLIP、OCR和Whisper进行语义描述、文本识别及语音转录。</li>
<li>通过修改后的DPO损失函数和语音事实核查确保摘要的精确度、相关性和事实一致性。</li>
<li>基于排名器的检索机制提高了关键帧与摘要内容的对齐，增强了可解释性和跨模态一致性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.20961">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Talking Head Generation/2509.20961v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Talking Head Generation/2509.20961v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Talking Head Generation/2509.20961v1/page_1_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Talking Head Generation/2509.20961v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Talking Head Generation/2509.20961v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Talking Head Generation/2509.20961v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Talking Head Generation/2509.20961v1/page_5_1.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="DRES-Benchmarking-LLMs-for-Disfluency-Removal"><a href="#DRES-Benchmarking-LLMs-for-Disfluency-Removal" class="headerlink" title="DRES: Benchmarking LLMs for Disfluency Removal"></a>DRES: Benchmarking LLMs for Disfluency Removal</h2><p><strong>Authors:Maria Teleki, Sai Janjur, Haoran Liu, Oliver Grabner, Ketan Verma, Thomas Docog, Xiangjue Dong, Lingfeng Shi, Cong Wang, Stephanie Birkelbach, Jason Kim, Yin Zhang, James Caverlee</strong></p>
<p>Disfluencies – such as “um,” “uh,” interjections, parentheticals, and edited statements – remain a persistent challenge for speech-driven systems, degrading accuracy in command interpretation, summarization, and conversational agents. We introduce DRES (Disfluency Removal Evaluation Suite), a controlled text-level benchmark that establishes a reproducible semantic upper bound for this task. DRES builds on human-annotated Switchboard transcripts, isolating disfluency removal from ASR errors and acoustic variability. We systematically evaluate proprietary and open-source LLMs across scales, prompting strategies, and architectures. Our results reveal that (i) simple segmentation consistently improves performance, even for long-context models; (ii) reasoning-oriented models tend to over-delete fluent tokens; and (iii) fine-tuning achieves near state-of-the-art precision and recall but harms generalization abilities. We further present a set of LLM-specific error modes and offer nine practical recommendations (R1-R9) for deploying disfluency removal in speech-driven pipelines. DRES provides a reproducible, model-agnostic foundation for advancing robust spoken-language systems. </p>
<blockquote>
<p>口语中的不流畅表达，如“嗯”、“呃”、感叹词、括号词和编辑过的语句，对于语音驱动系统来说仍然是一个持续存在的挑战，它们会降低命令解读、摘要和对话代理的准确性。我们引入了DRES（不流畅性移除评估套件），这是一个受控的文本级别基准测试，为这项任务建立了可复制的语义上限。DRES基于人工标注的Switchboard转录本，将不流畅性的移除与ASR错误和声音变化隔离开来。我们系统地评估了专有和开源的LLM，涵盖了规模、提示策略和架构。我们的结果揭示：（i）即使在长语境模型中，简单的分段也能始终提高性能；（ii）以推理为导向的模型往往会过度删除流畅的语言符号；（iii）微调可以实现接近最新技术的精确度和召回率，但会损害泛化能力。我们还针对部署不流畅性移除的语音驱动管道提出了LLM特定的错误模式和九条实用建议（R1-R9）。DRES为开发健壮的口语系统提供了一个可复制、模型无关的基础。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.20321v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了说话人常见的言语不流畅现象（如“嗯”、“呃”、插入语等），对语音驱动系统的挑战在于降低了命令解读、摘要和对话代理的准确性。为此，文章提出了DRES（言语不流畅性去除评估套件），一个可控文本基准测试，旨在解决这一问题并建立语义上限标准。DRES基于人类注释的Switchboard转录本，在语音识别错误和声学差异上分离了言语不流畅性的去除工作。研究评价了不同规模、提示策略和架构的专有和开源大型语言模型（LLM）。研究发现，简单的分割技术对于长期上下文模型也有良好的改善效果；推理导向模型可能会过度删除流畅的语言片段；微调可实现接近当前状态的精确度和召回率，但可能损害泛化能力。文章还提供了针对大型语言模型的特定错误模式及九条实际应用建议（R1-R9），为推进稳健的语音驱动系统提供模型无关的可复制基础。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>言语不流畅性是语音驱动系统的一大挑战，影响命令解读、摘要和对话代理的准确性。</li>
<li>DRES是一个可控文本基准测试，用于解决言语不流畅性问题并建立语义上限标准。</li>
<li>基于人类注释的Switchboard转录本建立，分离了言语不流畅性的去除与语音识别错误和声学差异。</li>
<li>简单分割技术能有效改善长期上下文模型的性能。</li>
<li>推理导向模型可能过度删除流畅的语言片段。</li>
<li>精细调整能提高模型的精确度和召回率，但可能影响其泛化能力。</li>
<li>文章提供了针对大型语言模型的特定错误模式及实用建议。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.20321">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Talking Head Generation/2509.20321v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Talking Head Generation/2509.20321v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Talking Head Generation/2509.20321v1/page_1_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Talking Head Generation/2509.20321v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Talking Head Generation/2509.20321v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Talking Head Generation/2509.20321v1/page_3_1.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="KSDiff-Keyframe-Augmented-Speech-Aware-Dual-Path-Diffusion-for-Facial-Animation"><a href="#KSDiff-Keyframe-Augmented-Speech-Aware-Dual-Path-Diffusion-for-Facial-Animation" class="headerlink" title="KSDiff: Keyframe-Augmented Speech-Aware Dual-Path Diffusion for Facial   Animation"></a>KSDiff: Keyframe-Augmented Speech-Aware Dual-Path Diffusion for Facial   Animation</h2><p><strong>Authors:Tianle Lyu, Junchuan Zhao, Ye Wang</strong></p>
<p>Audio-driven facial animation has made significant progress in multimedia applications, with diffusion models showing strong potential for talking-face synthesis. However, most existing works treat speech features as a monolithic representation and fail to capture their fine-grained roles in driving different facial motions, while also overlooking the importance of modeling keyframes with intense dynamics. To address these limitations, we propose KSDiff, a Keyframe-Augmented Speech-Aware Dual-Path Diffusion framework. Specifically, the raw audio and transcript are processed by a Dual-Path Speech Encoder (DPSE) to disentangle expression-related and head-pose-related features, while an autoregressive Keyframe Establishment Learning (KEL) module predicts the most salient motion frames. These components are integrated into a Dual-path Motion generator to synthesize coherent and realistic facial motions. Extensive experiments on HDTF and VoxCeleb demonstrate that KSDiff achieves state-of-the-art performance, with improvements in both lip synchronization accuracy and head-pose naturalness. Our results highlight the effectiveness of combining speech disentanglement with keyframe-aware diffusion for talking-head generation. </p>
<blockquote>
<p>音频驱动的面部动画在多媒体应用中已经取得了显著的进展，扩散模型在说话面部合成中显示出强大的潜力。然而，大多数现有工作将语音特征视为单一表示，未能捕获其在驱动不同面部运动中的精细作用，同时忽视了用强烈动力学对关键帧建模的重要性。为了解决这些局限性，我们提出了KSDiff，一种带关键帧增强的语音感知双路径扩散框架。具体而言，原始音频和文本通过双路径语音编码器（DPSE）进行处理，以解开表情相关和头部姿态相关的特征，同时自回归的关键帧建立学习（KEL）模块预测最显著的动态帧。这些组件被集成到双路径运动生成器中，以合成连贯且逼真的面部运动。在HDTF和VoxCeleb上的大量实验表明，KSDiff达到了最先进的性能，唇同步准确性和头部姿态自然性都有所提高。我们的结果突出了结合语音解纠缠和关键帧感知扩散进行说话头部生成的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.20128v1">PDF</a> 5 pages, 3 figures, 3 tables</p>
<p><strong>Summary</strong></p>
<p>音频驱动的面部动画在多媒体应用中已取得显著进展，扩散模型在说话人脸合成方面展现出强大潜力。然而，现有方法往往将语音特征视为单一表示，未能精细捕捉其在驱动不同面部运动中的作用，同时忽略了建模具有强烈动态的关键帧的重要性。为解决这些局限，我们提出KSDiff，一种结合关键帧增强的语音感知双路径扩散框架。具体而言，原始音频和文本通过双路径语音编码器处理，以解开表情和头部姿态相关特征；同时，通过自回归的关键帧建立学习模块预测最显著的动态帧。这些组件被整合到双路径运动生成器中，以合成连贯且逼真的面部运动。在HDTF和VoxCeleb上的实验表明，KSDiff达到了业界最佳性能，提高了唇同步准确性和头部姿态的自然性。我们的结果突显了结合语音解耦和关键帧感知扩散在说话人头部生成中的有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>音频驱动的面部动画在多媒体应用中有显著进展，扩散模型在谈话面部合成中展现出强潜力。</li>
<li>现有方法未能精细捕捉语音特征在驱动不同面部运动中的作用，且忽略了关键帧建模的重要性。</li>
<li>提出KSDiff框架，结合关键帧增强的语音感知双路径扩散。</li>
<li>双路径语音编码器能解开表情和头部姿态相关特征。</li>
<li>自回归关键帧建立学习模块可预测最显著的动态帧。</li>
<li>KSDiff在HDTF和VoxCeleb上的实验达到业界最佳性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.20128">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Talking Head Generation/2509.20128v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Talking Head Generation/2509.20128v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Talking Head Generation/2509.20128v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="SynchroRaMa-Lip-Synchronized-and-Emotion-Aware-Talking-Face-Generation-via-Multi-Modal-Emotion-Embedding"><a href="#SynchroRaMa-Lip-Synchronized-and-Emotion-Aware-Talking-Face-Generation-via-Multi-Modal-Emotion-Embedding" class="headerlink" title="SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation   via Multi-Modal Emotion Embedding"></a>SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation   via Multi-Modal Emotion Embedding</h2><p><strong>Authors:Phyo Thet Yee, Dimitrios Kollias, Sudeepta Mishra, Abhinav Dhall</strong></p>
<p>Audio-driven talking face generation has received growing interest, particularly for applications requiring expressive and natural human-avatar interaction. However, most existing emotion-aware methods rely on a single modality (either audio or image) for emotion embedding, limiting their ability to capture nuanced affective cues. Additionally, most methods condition on a single reference image, restricting the model’s ability to represent dynamic changes in actions or attributes across time. To address these issues, we introduce SynchroRaMa, a novel framework that integrates a multi-modal emotion embedding by combining emotional signals from text (via sentiment analysis) and audio (via speech-based emotion recognition and audio-derived valence-arousal features), enabling the generation of talking face videos with richer and more authentic emotional expressiveness and fidelity. To ensure natural head motion and accurate lip synchronization, SynchroRaMa includes an audio-to-motion (A2M) module that generates motion frames aligned with the input audio. Finally, SynchroRaMa incorporates scene descriptions generated by Large Language Model (LLM) as additional textual input, enabling it to capture dynamic actions and high-level semantic attributes. Conditioning the model on both visual and textual cues enhances temporal consistency and visual realism. Quantitative and qualitative experiments on benchmark datasets demonstrate that SynchroRaMa outperforms the state-of-the-art, achieving improvements in image quality, expression preservation, and motion realism. A user study further confirms that SynchroRaMa achieves higher subjective ratings than competing methods in overall naturalness, motion diversity, and video smoothness. Our project page is available at <a target="_blank" rel="noopener" href="https://novicemm.github.io/synchrorama">https://novicemm.github.io/synchrorama</a>. </p>
<blockquote>
<p>音频驱动的对话面部生成技术日益受到关注，特别是在需要表达自然的人形互动应用中。然而，大多数现有的情感感知方法依赖于单一模态（音频或图像）进行情感嵌入，这限制了它们捕捉微妙情感线索的能力。此外，大多数方法以一个单一的参考图像为条件，限制了模型在时间上表现动态动作或属性变化的能力。为了解决这些问题，我们引入了SynchroRaMa，这是一个结合多模态情感嵌入的新框架，它通过结合文本中的情感信号（通过情感分析）和音频中的情感信号（通过语音情感识别和音频衍生的效价激活特征），使生成的对话面部视频具有更丰富、更真实的情感表达和情感忠实度。为确保自然的头部运动和准确的唇部同步，SynchroRaMa包括一个音频到运动（A2M）模块，该模块生成与输入音频对齐的运动帧。最后，SynchroRaMa还结合了大型语言模型（LLM）生成的场景描述作为额外的文本输入，使其能够捕捉动态动作和高级语义属性。以视觉和文本线索为条件的模型增强了时间一致性和视觉真实性。在基准数据集上的定量和定性实验表明，SynchroRaMa优于最新技术，在图像质量、表情保留和运动现实性方面取得了改进。用户研究进一步证实，在整体自然性、运动多样性和视频平滑度方面，SynchroRaMa的主观评分高于其他方法。我们的项目页面可访问于<a target="_blank" rel="noopener" href="https://novicemm.github.io/synchrorama">https://novicemm.github.io/synchrorama</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.19965v1">PDF</a> Accepted at WACV 2026, project page :   <a target="_blank" rel="noopener" href="https://novicemm.github.io/synchrorama">https://novicemm.github.io/synchrorama</a></p>
<p><strong>Summary</strong></p>
<p>本文介绍了音频驱动的人脸动画生成技术的新进展。针对现有技术存在的问题，如单一模态的情感嵌入和缺乏动态动作表示，提出了名为SynchroRaMa的新框架。该框架结合了文本和音频的多模态情感嵌入，通过情感分析和语音情感识别等技术提取情感信号，并引入了音频到运动的模块确保自然的头部运动和准确的唇部同步。此外，SynchroRaMa还利用大型语言模型生成场景描述作为额外的文本输入，提高了模型的动态动作捕捉能力。实验结果证明，SynchroRaMa在图像质量、表情保持和运动真实感方面优于现有技术，用户研究也证实了其在整体自然性、运动多样性和视频平滑度方面的优势。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>音频驱动的人脸动画生成技术日益受到关注，特别是在需要表达和情感交互的应用中。</li>
<li>现有方法主要依赖单一模态（音频或图像）进行情感嵌入，限制了其对细微情感线索的捕捉能力。</li>
<li>SynchroRaMa框架结合了文本和音频的多模态情感嵌入，通过情感分析和语音情感识别等技术提取情感信号。</li>
<li>SynchroRaMa引入了音频到运动的模块，确保自然的头部运动和准确的唇部同步。</li>
<li>大型语言模型生成的场景描述作为额外的文本输入，增强了模型捕捉动态动作的能力。</li>
<li>实验结果表明，SynchroRaMa在图像质量、表情保持和运动真实感方面超越了现有技术。</li>
<li>用户研究证实了SynchroRaMa在整体自然性、运动多样性和视频平滑度方面的优势。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.19965">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Talking Head Generation/2509.19965v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Talking Head Generation/2509.19965v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Talking Head Generation/2509.19965v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Talking-Head-Generation-via-AU-Guided-Landmark-Prediction"><a href="#Talking-Head-Generation-via-AU-Guided-Landmark-Prediction" class="headerlink" title="Talking Head Generation via AU-Guided Landmark Prediction"></a>Talking Head Generation via AU-Guided Landmark Prediction</h2><p><strong>Authors:Shao-Yu Chang, Jingyi Xu, Hieu Le, Dimitris Samaras</strong></p>
<p>We propose a two-stage framework for audio-driven talking head generation with fine-grained expression control via facial Action Units (AUs). Unlike prior methods relying on emotion labels or implicit AU conditioning, our model explicitly maps AUs to 2D facial landmarks, enabling physically grounded, per-frame expression control. In the first stage, a variational motion generator predicts temporally coherent landmark sequences from audio and AU intensities. In the second stage, a diffusion-based synthesizer generates realistic, lip-synced videos conditioned on these landmarks and a reference image. This separation of motion and appearance improves expression accuracy, temporal stability, and visual realism. Experiments on the MEAD dataset show that our method outperforms state-of-the-art baselines across multiple metrics, demonstrating the effectiveness of explicit AU-to-landmark modeling for expressive talking head generation. </p>
<blockquote>
<p>我们提出了一种两阶段的音频驱动说话人头部生成框架，通过面部动作单元（AUs）进行精细的表情控制。不同于以往依赖情感标签或隐式AU条件的方法，我们的模型将AUs明确映射到2D面部特征点，实现了基于物理的逐帧表情控制。在第一阶段，变异运动生成器根据音频和AU强度预测时间连贯的特征点序列。在第二阶段，基于扩散的合成器根据这些特征点和参考图像生成现实、同步的唇视频。运动和外观的分离提高了表情的准确性、时间稳定性和视觉真实性。在MEAD数据集上的实验表明，我们的方法在多个指标上优于最先进的基础模型，证明了显式AU到特征点建模在表情丰富的说话人头部生成中的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.19749v1">PDF</a> </p>
<p><strong>Summary</strong><br>     该研究提出一个两阶段的音频驱动说话人头部生成框架，通过面部动作单元（AUs）进行精细表情控制。该研究不同于依赖情感标签或隐性AU条件的方法，显式地将AUs映射到2D面部特征点，实现物理基础、逐帧的表情控制。第一阶段，变异运动生成器从音频和AU强度预测时间连贯的特征点序列。第二阶段，基于扩散的合成器根据这些特征点和参考图像生成真实、唇同步的视频。这种运动和外观的分离提高了表情的准确性、时间稳定性和视觉真实性。在MEAD数据集上的实验表明，该方法在多个指标上优于最新基线，证明了显式AU-to-landmark建模在表情说话人头部生成中的有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>该研究采用两阶段框架进行音频驱动的说话头部生成。</li>
<li>通过面部动作单元（AUs）实现精细表情控制。</li>
<li>显式地将AUs映射到2D面部特征点，实现物理基础、逐帧的表情控制。</li>
<li>变异运动生成器从音频和AU强度预测时间连贯的特征点序列。</li>
<li>基于扩散的合成器根据特征点和参考图像生成真实、唇同步的视频。</li>
<li>分离运动和外观，提高表情准确性、时间稳定性和视觉真实性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.19749">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Talking Head Generation/2509.19749v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Talking Head Generation/2509.19749v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Talking Head Generation/2509.19749v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Talking Head Generation/2509.19749v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Audio-Driven-Universal-Gaussian-Head-Avatars"><a href="#Audio-Driven-Universal-Gaussian-Head-Avatars" class="headerlink" title="Audio-Driven Universal Gaussian Head Avatars"></a>Audio-Driven Universal Gaussian Head Avatars</h2><p><strong>Authors:Kartik Teotia, Helge Rhodin, Mohit Mendiratta, Hyeongwoo Kim, Marc Habermann, Christian Theobalt</strong></p>
<p>We introduce the first method for audio-driven universal photorealistic avatar synthesis, combining a person-agnostic speech model with our novel Universal Head Avatar Prior (UHAP). UHAP is trained on cross-identity multi-view videos. In particular, our UHAP is supervised with neutral scan data, enabling it to capture the identity-specific details at high fidelity. In contrast to previous approaches, which predominantly map audio features to geometric deformations only while ignoring audio-dependent appearance variations, our universal speech model directly maps raw audio inputs into the UHAP latent expression space. This expression space inherently encodes, both, geometric and appearance variations. For efficient personalization to new subjects, we employ a monocular encoder, which enables lightweight regression of dynamic expression variations across video frames. By accounting for these expression-dependent changes, it enables the subsequent model fine-tuning stage to focus exclusively on capturing the subject’s global appearance and geometry. Decoding these audio-driven expression codes via UHAP generates highly realistic avatars with precise lip synchronization and nuanced expressive details, such as eyebrow movement, gaze shifts, and realistic mouth interior appearance as well as motion. Extensive evaluations demonstrate that our method is not only the first generalizable audio-driven avatar model that can account for detailed appearance modeling and rendering, but it also outperforms competing (geometry-only) methods across metrics measuring lip-sync accuracy, quantitative image quality, and perceptual realism. </p>
<blockquote>
<p>我们介绍了第一种音频驱动通用写实头像合成的方法，该方法将人物无关的语音模型与我们的新型通用头像先验值（UHAP）相结合。UHAP经过跨身份多视角视频的训练。特别的是，我们的UHAP受到中性扫描数据的监督，从而能够以高保真度捕获身份特定的细节。与之前的方法相比，我们主要将音频特征映射到几何变形上，而忽视音频相关的外观变化，而我们的通用语音模型直接将原始音频输入映射到UHAP潜在表达空间中。该表达空间本质上编码了几何和外观变化。为了实现高效的新主题个性化，我们采用单目编码器，它可以在视频帧之间进行动态表情变化的轻量级回归。考虑到这些表情相关的变化，它使随后的模型微调阶段能够专注于捕捉主题的总体外观和几何结构。通过UHAP解码这些音频驱动的表情代码，生成高度逼真的头像，具有精确的唇部同步和微妙的表情细节，如眉毛动作、眼神变化和真实的嘴唇内部外观以及运动。大量评估表明，我们的方法不仅是首个可以兼顾详细外观建模和渲染的通用音频驱动头像模型，而且在衡量唇部同步准确性、图像质量和感知真实性的指标上，也优于仅基于几何的竞争对手方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.18924v1">PDF</a> (SIGGRAPH Asia 2025) Project page:   <a target="_blank" rel="noopener" href="https://kartik-teotia.github.io/UniGAHA/">https://kartik-teotia.github.io/UniGAHA/</a></p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种结合通用头部化身先验（UHAP）和人物无关的语音模型的音频驱动通用逼真化身合成方法。UHAP通过跨身份多视角视频进行训练，并能从中性扫描数据中提取身份特定细节，实现高保真捕捉。该方法直接将原始音频输入映射到UHAP潜在表达空间，同时编码几何和外观变化。为了实现对新主体的有效个性化，采用单目编码器，可在视频帧之间实现动态表达变化的轻量级回归。通过考虑这些表达相关的变化，使后续模型微调阶段能够专注于捕捉主体的全局外观和几何特征。通过UHAP解码这些音频驱动的表达式代码，生成高度逼真的化身，具有精确的唇部同步和微妙的表情细节，如眉毛动作、目光转移以及逼真的口腔内部外观和运动。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>介绍了结合通用头部化身先验（UHAP）和人物无关语音模型的音频驱动通用逼真化身合成方法。</li>
<li>UHAP通过跨身份多视角视频训练，并能从中性扫描数据中提取高保真身份特定细节。</li>
<li>方法直接将原始音频输入映射到UHAP潜在表达空间，同时编码几何和外观变化。</li>
<li>采用单目编码器实现动态表达变化的轻量级回归，为后续模型微调阶段提供基础。</li>
<li>解码音频驱动的表达式代码生成高度逼真的化身，具有精确的唇部同步和微妙的表情细节。</li>
<li>该方法是首个可泛化的音频驱动化身模型，能够建模和渲染详细的外观。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.18924">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Talking Head Generation/2509.18924v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Talking Head Generation/2509.18924v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Talking Head Generation/2509.18924v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-28/Talking%20Head%20Generation/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-28/Talking%20Head%20Generation/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                                    <span class="chip bg-color">Talking Head Generation</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-28/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_R1_Reasoning/2509.21268v1/page_1_0.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning 方向最新论文已更新，请持续关注 Update in 2025-09-28  SciReasoner Laying the Scientific Reasoning Ground Across Disciplines
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-28
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-24/Interactive/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Interactive/2509.17711v1/page_3_2.jpg" class="responsive-img" alt="Interactive">
                        
                        <span class="card-title">Interactive</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Interactive 方向最新论文已更新，请持续关注 Update in 2025-09-24  DA-Mamba Dialogue-aware selective state-space model for multimodal   engagement estimation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-24
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Interactive/" class="post-category">
                                    Interactive
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Interactive/">
                        <span class="chip bg-color">Interactive</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">29474.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
