<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-28  SciReasoner Laying the Scientific Reasoning Ground Across Disciplines">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic-private.zhihu.com/v2-1f7850256198d4ecd194f6d8d253885c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684357&auth_key=1760684357-0-0-0ecf4c1bdc3c54683c5d4e5f9a95885c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-28
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-17
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    87 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-09-28-æ›´æ–°"><a href="#2025-09-28-æ›´æ–°" class="headerlink" title="2025-09-28 æ›´æ–°"></a>2025-09-28 æ›´æ–°</h1><h2 id="SciReasoner-Laying-the-Scientific-Reasoning-Ground-Across-Disciplines"><a href="#SciReasoner-Laying-the-Scientific-Reasoning-Ground-Across-Disciplines" class="headerlink" title="SciReasoner: Laying the Scientific Reasoning Ground Across Disciplines"></a>SciReasoner: Laying the Scientific Reasoning Ground Across Disciplines</h2><p><strong>Authors:Yizhou Wang, Chen Tang, Han Deng, Jiabei Xiao, Jiaqi Liu, Jianyu Wu, Jun Yao, Pengze Li, Encheng Su, Lintao Wang, Guohang Zhuang, Yuchen Ren, Ben Fei, Ming Hu, Xin Chen, Dongzhan Zhou, Junjun He, Xiangyu Yue, Zhenfei Yin, Jiamin Wu, Qihao Zheng, Yuhao Zhou, Huihui Xu, Chenglong Ma, Yan Lu, Wenlong Zhang, Chunfeng Song, Philip Torr, Shixiang Tang, Xinzhu Ma, Wanli Ouyang, Lei Bai</strong></p>
<p>We present a scientific reasoning foundation model that aligns natural language with heterogeneous scientific representations. The model is pretrained on a 206B-token corpus spanning scientific text, pure sequences, and sequence-text pairs, then aligned via SFT on 40M instructions, annealed cold-start bootstrapping to elicit long-form chain-of-thought, and reinforcement learning with task-specific reward shaping, which instills deliberate scientific reasoning. It supports four capability families, covering up to 103 tasks across workflows: (i) faithful translation between text and scientific formats, (ii) text&#x2F;knowledge extraction, (iii) property prediction, (iv) property classification, (v) unconditional and conditional sequence generation and design. Compared with specialist systems, our approach broadens instruction coverage, improves cross-domain generalization, and enhances fidelity. We detail data curation and training and show that cross-discipline learning strengthens transfer and downstream reliability. The model, instruct tuning datasets and the evaluation code are open-sourced at <a target="_blank" rel="noopener" href="https://huggingface.co/SciReason">https://huggingface.co/SciReason</a> and <a target="_blank" rel="noopener" href="https://github.com/open-sciencelab/SciReason">https://github.com/open-sciencelab/SciReason</a>. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç§‘å­¦æ¨ç†åŸºç¡€æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å°†è‡ªç„¶è¯­è¨€ä¸å¼‚è´¨ç§‘å­¦è¡¨ç¤ºè¿›è¡Œå¯¹é½ã€‚è¯¥æ¨¡å‹åœ¨åŒ…å«ç§‘å­¦æ–‡æœ¬ã€çº¯åºåˆ—å’Œåºåˆ—æ–‡æœ¬å¯¹çš„206Bæ ‡è®°è¯­æ–™åº“ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œç„¶åé€šè¿‡40MæŒ‡ä»¤è¿›è¡ŒSFTå¯¹é½ï¼Œé‡‡ç”¨é€€ç«å†·å¯åŠ¨å¼•å¯¼æ¥æ¿€å‘é•¿å½¢å¼æ€ç»´é“¾ï¼Œå¹¶ä½¿ç”¨é’ˆå¯¹ä»»åŠ¡çš„å¥–åŠ±å¡‘å½¢è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼Œä»è€ŒçŒè¾“æœ‰æ„è¯†çš„ç§‘å­¦æ¨ç†ã€‚å®ƒæ”¯æŒå››ä¸ªèƒ½åŠ›å®¶æ—ï¼Œæ¶µç›–103ä¸ªä»»åŠ¡çš„å·¥ä½œæµç¨‹ï¼š(i)æ–‡æœ¬å’Œç§‘å­¦æ ¼å¼ä¹‹é—´çš„å¿ å®ç¿»è¯‘ï¼Œ(ii)æ–‡æœ¬&#x2F;çŸ¥è¯†æå–ï¼Œ(iii)å±æ€§é¢„æµ‹ï¼Œ(iv)å±æ€§åˆ†ç±»ï¼Œ(v)æ— æ¡ä»¶å’Œæœ‰æ¡ä»¶åºåˆ—ç”Ÿæˆå’Œè®¾è®¡ã€‚ä¸ä¸“ç”¨ç³»ç»Ÿç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ‰©å¤§äº†æŒ‡ä»¤è¦†ç›–èŒƒå›´ï¼Œæé«˜äº†è·¨åŸŸæ³›åŒ–èƒ½åŠ›ï¼Œå¹¶å¢å¼ºäº†ä¿çœŸåº¦ã€‚æˆ‘ä»¬è¯¦ç»†æè¿°äº†æ•°æ®æ•´ç†å’Œè®­ç»ƒï¼Œå¹¶è¡¨æ˜è·¨å­¦ç§‘å­¦ä¹ åŠ å¼ºäº†è¿ç§»å’Œä¸‹æ¸¸å¯é æ€§ã€‚æ¨¡å‹ã€æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†å’Œè¯„ä¼°ä»£ç å·²åœ¨<a target="_blank" rel="noopener" href="https://huggingface.co/SciReason%E5%92%8Chttps://github.com/open-sciencelab/SciReason%E5%BC%80%E6%BA%90%E3%80%82">https://huggingface.co/SciReasonå’Œhttps://github.com/open-sciencelab/SciReasonå¼€æºã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.21320v1">PDF</a> technical report</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡ç« ä»‹ç»äº†ä¸€ä¸ªç§‘å­¦æ¨ç†åŸºç¡€æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å°†è‡ªç„¶è¯­è¨€ä¸å¼‚è´¨ç§‘å­¦è¡¨ç¤ºå¯¹é½ã€‚æ¨¡å‹åœ¨åŒ…å«ç§‘å­¦æ–‡æœ¬ã€çº¯åºåˆ—å’Œåºåˆ—æ–‡æœ¬å¯¹çš„206Bæ ‡è®°è¯­æ–™åº“ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œç„¶åé€šè¿‡SFTå¯¹é½çš„40MæŒ‡ä»¤è¿›è¡Œå¾®è°ƒï¼Œé‡‡ç”¨å†·å¯åŠ¨å¼•å¯¼æ³•æ¿€å‘é•¿å½¢å¼çš„æ€ç»´é“¾ï¼Œå¹¶é€šè¿‡ä»»åŠ¡ç‰¹å®šçš„å¥–åŠ±å¡‘å½¢å¼ºåŒ–å­¦ä¹ ï¼ŒåŸ¹å…»äº†æœ‰æ„çš„ç§‘å­¦æ¨ç†èƒ½åŠ›ã€‚è¯¥æ¨¡å‹æ”¯æŒå››å¤§èƒ½åŠ›é¢†åŸŸï¼Œæ¶µç›–103é¡¹ä»»åŠ¡å’Œå·¥ä½œæµç¨‹ï¼ŒåŒ…æ‹¬æ–‡æœ¬ä¸ç§‘å­¦æ ¼å¼ä¹‹é—´çš„å¿ å®ç¿»è¯‘ã€æ–‡æœ¬&#x2F;çŸ¥è¯†æå–ã€å±æ€§é¢„æµ‹ã€å±æ€§åˆ†ç±»ã€æ— æ¡ä»¶å’Œæœ‰æ¡ä»¶çš„åºåˆ—ç”Ÿæˆå’Œè®¾è®¡ç­‰ã€‚ä¸ä¸“é¡¹ç³»ç»Ÿç›¸æ¯”ï¼Œè¯¥æ–¹æ³•æ‰©å¤§äº†æŒ‡ä»¤è¦†ç›–é¢ï¼Œæé«˜äº†è·¨åŸŸæ³›åŒ–èƒ½åŠ›ï¼Œå¹¶æé«˜äº†ä¿çœŸåº¦ã€‚æ–‡ç« è¯¦ç»†æè¿°äº†æ•°æ®æ•´ç†å’Œè®­ç»ƒè¿‡ç¨‹ï¼Œå¹¶è¯æ˜è·¨å­¦ç§‘å­¦ä¹ èƒ½åŠ å¼ºè¿ç§»å’Œä¸‹æ¸¸å¯é æ€§ã€‚æ¨¡å‹å’Œè¯„ä¼°ä»£ç å·²åœ¨Hugging Faceå’ŒGitHubä¸Šå¼€æºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¨¡å‹å°†è‡ªç„¶è¯­è¨€ä¸å¼‚è´¨ç§‘å­¦è¡¨ç¤ºå¯¹é½ã€‚</li>
<li>æ¨¡å‹åœ¨å¤šç§ç§‘å­¦æ–‡æœ¬ã€çº¯åºåˆ—å’Œåºåˆ—æ–‡æœ¬å¯¹ä¸Šè¿›è¡Œé¢„è®­ç»ƒã€‚</li>
<li>ä½¿ç”¨SFTå¯¹é½æŒ‡ä»¤è¿›è¡Œå¾®è°ƒï¼Œæ¿€å‘é•¿å½¢å¼çš„æ€ç»´é“¾ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ é€šè¿‡ä»»åŠ¡ç‰¹å®šå¥–åŠ±å¡‘å½¢åŸ¹å…»ç§‘å­¦æ¨ç†èƒ½åŠ›ã€‚</li>
<li>æ¨¡å‹æ”¯æŒå¤šç§ä»»åŠ¡å’Œå·¥ä½œæµç¨‹ï¼ŒåŒ…æ‹¬ç¿»è¯‘ã€æå–ã€é¢„æµ‹ã€åˆ†ç±»ä»¥åŠåºåˆ—ç”Ÿæˆå’Œè®¾è®¡ã€‚</li>
<li>ä¸å…¶ä»–ç³»ç»Ÿç›¸æ¯”ï¼Œè¯¥æ¨¡å‹æ‰©å¤§äº†æŒ‡ä»¤è¦†ç›–é¢ï¼Œæé«˜äº†æ³›åŒ–èƒ½åŠ›å’Œä¿çœŸåº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.21320">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-27aea524a018873f54c99c215d356132~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684365&auth_key=1760684365-0-0-8b8c7d4909b776b60c314f88c11af223&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b6305c19980605360763578c81ffe369~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684373&auth_key=1760684373-0-0-d491e23472c15044454f83f1943f09b4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-afe996cc209842e261724c91f84852e7~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684380&auth_key=1760684380-0-0-0589c1f10cfae3796abe7b6058d0100a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="SAGE-A-Realistic-Benchmark-for-Semantic-Understanding"><a href="#SAGE-A-Realistic-Benchmark-for-Semantic-Understanding" class="headerlink" title="SAGE: A Realistic Benchmark for Semantic Understanding"></a>SAGE: A Realistic Benchmark for Semantic Understanding</h2><p><strong>Authors:Samarth Goel, Reagan J. Lee, Kannan Ramchandran</strong></p>
<p>As large language models (LLMs) achieve strong performance on traditional benchmarks, there is an urgent need for more challenging evaluation frameworks that probe deeper aspects of semantic understanding. We introduce SAGE (Semantic Alignment &amp; Generalization Evaluation), a rigorous benchmark designed to assess both embedding models and similarity metrics across five categories: Human Preference Alignment, Transformation Robustness, Information Sensitivity, Clustering Performance, and Retrieval Robustness. Unlike existing benchmarks that focus on isolated capabilities, SAGE evaluates semantic understanding through adversarial conditions, noisy transformations, and nuanced human judgment tasks across 30+ datasets. Our comprehensive evaluation of 9 embedding models and classical metrics reveals significant performance gaps, with no single approach excelling across all dimensions. For instance, while state-of-the-art embedding models like OpenAIâ€™s text-embedding-3-large dominate in aligning with human preferences (0.682 vs. 0.591 for the best classical metric), they are significantly outperformed by classical metrics on information sensitivity tasks, where Jaccard Similarity achieves a score of 0.905 compared to the top embedding score of 0.794. SAGE further uncovers critical trade-offs: OpenAIâ€™s text-embedding-3-small achieves the highest clustering performance (0.483) but demonstrates extreme brittleness with the lowest robustness score (0.011). SAGE exposes critical limitations in current semantic understanding capabilities and provides a more realistic assessment of model robustness for real-world deployment. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä¼ ç»ŸåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œæˆ‘ä»¬æ€¥éœ€æ›´å…·æŒ‘æˆ˜æ€§çš„è¯„ä¼°æ¡†æ¶æ¥æ·±å…¥æ¢ç´¢è¯­ä¹‰ç†è§£çš„æ›´æ·±å±‚æ¬¡æ–¹é¢ã€‚æˆ‘ä»¬å¼•å…¥äº†SAGEï¼ˆè¯­ä¹‰å¯¹é½ä¸æ³›åŒ–è¯„ä¼°ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸¥æ ¼çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°åµŒå…¥æ¨¡å‹å’Œç›¸ä¼¼æ€§åº¦é‡åœ¨äº”ä¸ªç±»åˆ«ä¸­çš„è¡¨ç°ï¼šäººç±»åå¥½å¯¹é½ã€è½¬æ¢é²æ£’æ€§ã€ä¿¡æ¯æ•æ„Ÿæ€§ã€èšç±»æ€§èƒ½å’Œæ£€ç´¢é²æ£’æ€§ã€‚ä¸ç°æœ‰çš„ä¸»è¦å…³æ³¨å­¤ç«‹èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ä¸åŒï¼ŒSAGEé€šè¿‡å¯¹æŠ—æ€§æ¡ä»¶ã€å˜ˆæ‚çš„è½¬æ¢å’Œå¾®å¦™çš„äººç±»åˆ¤æ–­ä»»åŠ¡åœ¨30å¤šä¸ªæ•°æ®é›†ä¸Šè¯„ä¼°è¯­ä¹‰ç†è§£ã€‚æˆ‘ä»¬å¯¹9ç§åµŒå…¥æ¨¡å‹å’Œç»å…¸æŒ‡æ ‡çš„å…¨é¢è¯„ä¼°æ˜¾ç¤ºï¼Œå­˜åœ¨æ˜¾è‘—çš„æ€§èƒ½å·®è·ï¼Œæ²¡æœ‰ä»»ä½•å•ä¸€çš„æ–¹æ³•åœ¨æ‰€æœ‰ç»´åº¦ä¸Šéƒ½è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œè™½ç„¶æœ€å…ˆè¿›çš„åµŒå…¥æ¨¡å‹å¦‚OpenAIçš„æ–‡æœ¬åµŒå…¥æ¨¡å‹åœ¨äººç±»åå¥½å¯¹é½æ–¹é¢è¡¨ç°å‡ºè‰²ï¼ˆå¾—åˆ†ä¸º0.682ï¼Œè€Œæœ€ä½³ç»å…¸æŒ‡æ ‡å¾—åˆ†ä¸º0.591ï¼‰ï¼Œä½†åœ¨ä¿¡æ¯æ•æ„Ÿæ€§ä»»åŠ¡ä¸Šå´è¢«ç»å…¸æŒ‡æ ‡æ˜¾è‘—è¶…è¶Šï¼Œå…¶ä¸­Jaccardç›¸ä¼¼åº¦è¾¾åˆ°äº†0.905åˆ†çš„é«˜åˆ†ï¼Œè€Œè¡¨ç°æœ€ä½³çš„åµŒå…¥æ¨¡å‹å¾—åˆ†ä¸ºä»…çš„è¾¾åˆ°â€‹â€‹ äº†0.â€‹ 794åˆ†ã€‚SAGEè¿›ä¸€æ­¥æ­ç¤ºäº†å…³é”®æƒè¡¡ï¼šOpenAIçš„æ–‡æœ¬åµŒå…¥æ¨¡å‹è™½ç„¶èšç±»æ€§èƒ½æœ€é«˜ï¼ˆå¾—åˆ†ä¸ºâ€‹â€‹ 0.â€‹ 483ï¼‰ï¼Œä½†åœ¨é²æ£’æ€§æ–¹é¢å´è¡¨ç°å‡ºæç«¯çš„è„†å¼±æ€§ï¼ˆå¾—åˆ†æœ€ä½ï¼Œä»…ä¸ºâ€‹â€‹ 0.â€‹ 011ï¼‰ã€‚SAGEæ­ç¤ºäº†å½“å‰è¯­ä¹‰ç†è§£èƒ½åŠ›çš„å…³é”®å±€é™æ€§ï¼Œå¹¶ä¸ºç°å®ä¸–ç•Œçš„éƒ¨ç½²æä¾›äº†æ›´ç°å®çš„æ¨¡å‹é²æ£’æ€§è¯„ä¼°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.21310v1">PDF</a> 39th Conference on Neural Information Processing Systems (NeurIPS   2025) Workshop: Evaluating the Evolving LLM Lifecycle: Benchmarks, Emergent   Abilities, and Scaling</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªåä¸ºSAGEçš„æ–°è¯„ä¼°æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°åµŒå…¥æ¨¡å‹å’Œç›¸ä¼¼æ€§åº¦é‡åœ¨äº”ä¸ªç±»åˆ«ä¸­çš„è¡¨ç°ï¼ŒåŒ…æ‹¬äººç±»åå¥½å¯¹é½ã€è½¬æ¢é²æ£’æ€§ã€ä¿¡æ¯æ•æ„Ÿæ€§ã€èšç±»æ€§èƒ½å’Œæ£€ç´¢é²æ£’æ€§ã€‚SAGEé€šè¿‡è·¨è¶…è¿‡30ä¸ªæ•°æ®é›†çš„å¯¹æŠ—æ¡ä»¶ã€å™ªå£°è½¬æ¢å’Œå¾®å¦™çš„äººç±»åˆ¤æ–­ä»»åŠ¡æ¥è¯„ä¼°è¯­ä¹‰ç†è§£ã€‚å¯¹ç°æœ‰åµŒå…¥æ¨¡å‹å’Œç»å…¸æŒ‡æ ‡çš„å…¨é¢è¯„ä¼°æ˜¾ç¤ºï¼Œå„æ–¹æ³•ä¹‹é—´å­˜åœ¨æ˜¾è‘—æ€§èƒ½å·®è·ï¼Œæ²¡æœ‰å•ä¸€æ–¹æ³•åœ¨æ‰€æœ‰ç»´åº¦ä¸Šéƒ½è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚SAGEæ­ç¤ºäº†å…³é”®æƒè¡¡ï¼Œå¹¶æš´éœ²äº†å½“å‰è¯­ä¹‰ç†è§£çš„å±€é™æ€§ï¼Œä¸ºç°å®ä¸–ç•Œçš„æ¨¡å‹éƒ¨ç½²æä¾›äº†æ›´ç°å®çš„è¯„ä¼°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>SAGEæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°åµŒå…¥æ¨¡å‹å’Œç›¸ä¼¼æ€§åº¦é‡çš„æ–°è¯„ä¼°æ¡†æ¶ã€‚</li>
<li>SAGEåŒ…æ‹¬äº”ä¸ªç±»åˆ«ï¼šäººç±»åå¥½å¯¹é½ã€è½¬æ¢é²æ£’æ€§ã€ä¿¡æ¯æ•æ„Ÿæ€§ã€èšç±»æ€§èƒ½å’Œæ£€ç´¢é²æ£’æ€§ã€‚</li>
<li>SAGEä½¿ç”¨è¶…è¿‡30ä¸ªæ•°æ®é›†è¿›è¡Œå¯¹æŠ—æ¡ä»¶ã€å™ªå£°è½¬æ¢å’Œå¾®å¦™çš„åˆ¤æ–­ä»»åŠ¡æ¥è¯„ä¼°è¯­ä¹‰ç†è§£ã€‚</li>
<li>ç°æœ‰åµŒå…¥æ¨¡å‹å’Œç»å…¸æŒ‡æ ‡åœ¨SAGEè¯„ä¼°ä¸­å­˜åœ¨æ˜¾è‘—æ€§èƒ½å·®è·ã€‚</li>
<li>æ²¡æœ‰å•ä¸€æ–¹æ³•åœ¨æ‰€æœ‰ç»´åº¦ä¸Šéƒ½è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>SAGEæ­ç¤ºäº†å…³é”®æƒè¡¡ï¼Œä¾‹å¦‚æŸäº›æ¨¡å‹åœ¨æŸäº›ä»»åŠ¡ä¸Šçš„ä¼˜ç§€è¡¨ç°ä¸å…¶ä»–ä»»åŠ¡ä¸Šçš„è„†å¼±æ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.21310">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-1c66143d766f25aa8cd2539cbff3046b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684387&auth_key=1760684387-0-0-8f3d621a224c079aacd150bd18a664aa&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-22559c80ac075f10f2155e10c8dd97e4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684394&auth_key=1760684394-0-0-a91abc5eeeca8b7a715a242eb680ecd9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="MMR1-Enhancing-Multimodal-Reasoning-with-Variance-Aware-Sampling-and-Open-Resources"><a href="#MMR1-Enhancing-Multimodal-Reasoning-with-Variance-Aware-Sampling-and-Open-Resources" class="headerlink" title="MMR1: Enhancing Multimodal Reasoning with Variance-Aware Sampling and   Open Resources"></a>MMR1: Enhancing Multimodal Reasoning with Variance-Aware Sampling and   Open Resources</h2><p><strong>Authors:Sicong Leng, Jing Wang, Jiaxi Li, Hao Zhang, Zhiqiang Hu, Boqiang Zhang, Yuming Jiang, Hang Zhang, Xin Li, Lidong Bing, Deli Zhao, Wei Lu, Yu Rong, Aixin Sun, Shijian Lu</strong></p>
<p>Large multimodal reasoning models have achieved rapid progress, but their advancement is constrained by two major limitations: the absence of open, large-scale, high-quality long chain-of-thought (CoT) data, and the instability of reinforcement learning (RL) algorithms in post-training. Group Relative Policy Optimization (GRPO), the standard framework for RL fine-tuning, is prone to gradient vanishing when reward variance is low, which weakens optimization signals and impairs convergence. This work makes three contributions: (1) We propose Variance-Aware Sampling (VAS), a data selection strategy guided by Variance Promotion Score (VPS) that combines outcome variance and trajectory diversity to promote reward variance and stabilize policy optimization. (2) We release large-scale, carefully curated resources containing ~1.6M long CoT cold-start data and ~15k RL QA pairs, designed to ensure quality, difficulty, and diversity, along with a fully reproducible end-to-end training codebase. (3) We open-source a family of multimodal reasoning models in multiple scales, establishing standardized baselines for the community. Experiments across mathematical reasoning benchmarks demonstrate the effectiveness of both the curated data and the proposed VAS. Comprehensive ablation studies and analyses provide further insight into the contributions of each component. In addition, we theoretically establish that reward variance lower-bounds the expected policy gradient magnitude, with VAS serving as a practical mechanism to realize this guarantee. Our code, data, and checkpoints are available at <a target="_blank" rel="noopener" href="https://github.com/LengSicong/MMR1">https://github.com/LengSicong/MMR1</a>. </p>
<blockquote>
<p>å¤šæ¨¡æ€æ¨ç†æ¨¡å‹è™½ç„¶å–å¾—äº†å¿«é€Ÿè¿›å±•ï¼Œä½†å®ƒä»¬çš„è¿›æ­¥å—åˆ°ä¸¤ä¸ªä¸»è¦é™åˆ¶ï¼šç¼ºä¹å¼€æ”¾çš„å¤§è§„æ¨¡ã€é«˜è´¨é‡çš„é•¿æ€ç»´é“¾ï¼ˆCoTï¼‰æ•°æ®ï¼Œä»¥åŠè®­ç»ƒåå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç®—æ³•çš„ä¸ç¨³å®šæ€§ã€‚å¼ºåŒ–å­¦ä¹ å¾®è°ƒçš„æ ‡å‡†æ¡†æ¶â€”â€”ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰åœ¨å¥–åŠ±æ–¹å·®ä½æ—¶å®¹æ˜“å‡ºç°æ¢¯åº¦æ¶ˆå¤±ï¼Œè¿™å‰Šå¼±äº†ä¼˜åŒ–ä¿¡å·å¹¶æŸå®³äº†æ”¶æ•›ã€‚æœ¬æ–‡æœ‰ä»¥ä¸‹ä¸‰ä¸ªè´¡çŒ®ï¼šï¼ˆ1ï¼‰æˆ‘ä»¬æå‡ºäº†æ–¹å·®æ„ŸçŸ¥é‡‡æ ·ï¼ˆVASï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å—æ–¹å·®ä¿ƒè¿›åˆ†æ•°ï¼ˆVPSï¼‰å¼•å¯¼çš„æ•°æ®é€‰æ‹©ç­–ç•¥ï¼Œç»“åˆç»“æœæ–¹å·®å’Œè½¨è¿¹å¤šæ ·æ€§æ¥ä¿ƒè¿›å¥–åŠ±æ–¹å·®å¹¶ç¨³å®šç­–ç•¥ä¼˜åŒ–ã€‚ï¼ˆ2ï¼‰æˆ‘ä»¬å‘å¸ƒäº†å¤§è§„æ¨¡ç²¾å¿ƒç­–åˆ’çš„èµ„æºï¼ŒåŒ…å«çº¦160ä¸‡æ¡é•¿æ€ç»´é“¾å†·å¯åŠ¨æ•°æ®å’Œçº¦1.5ä¸‡æ¡RLé—®ç­”å¯¹ï¼Œæ—¨åœ¨ç¡®ä¿è´¨é‡ã€éš¾åº¦å’Œå¤šæ ·æ€§ï¼Œä»¥åŠä¸€ä¸ªå¯å®Œå…¨å¤ç°çš„ç«¯åˆ°ç«¯è®­ç»ƒä»£ç åº“ã€‚ï¼ˆ3ï¼‰æˆ‘ä»¬å¼€æºäº†å¤šä¸ªè§„æ¨¡çš„å¤šæ¨¡æ€æ¨ç†æ¨¡å‹å®¶æ—ï¼Œä¸ºç¤¾åŒºå»ºç«‹æ ‡å‡†åŒ–åŸºå‡†çº¿ã€‚åœ¨è·¨æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•çš„å®éªŒä¸­ï¼ŒéªŒè¯äº†ç²¾é€‰æ•°æ®å’Œæå‡ºçš„VASçš„æœ‰æ•ˆæ€§ã€‚å…¨é¢çš„æ¶ˆèç ”ç©¶å’Œåˆ†æè¿›ä¸€æ­¥æ·±å…¥äº†è§£æ¯ä¸ªç»„ä»¶çš„è´¡çŒ®ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä»ç†è®ºä¸Šè¯æ˜äº†å¥–åŠ±æ–¹å·®æ˜¯é¢„æœŸç­–ç•¥æ¢¯åº¦å¹…åº¦çš„ä¸‹é™ï¼ŒVASä½œä¸ºå®ç°è¿™ä¸€ä¿è¯çš„å®é™…æœºåˆ¶ã€‚æˆ‘ä»¬çš„ä»£ç ã€æ•°æ®å’Œæ£€æŸ¥ç‚¹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/LengSicong/MMR1%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/LengSicong/MMR1æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.21268v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹å¤šæ¨¡æ€æ¨ç†æ¨¡å‹çš„è¿›æ­¥å—åˆ°æ•°æ®ç¼ºå¤±å’Œå¼ºåŒ–å­¦ä¹ ç®—æ³•ä¸ç¨³å®šä¸¤å¤§é™åˆ¶ã€‚æœ¬æ–‡æå‡ºVariance-Aware Samplingï¼ˆVASï¼‰ç­–ç•¥ï¼Œé€šè¿‡ç»“åˆç»“æœæ–¹å·®å’Œè½¨è¿¹å¤šæ ·æ€§æ¥ä¿ƒè¿›å¥–åŠ±æ–¹å·®å¹¶ç¨³å®šç­–ç•¥ä¼˜åŒ–ã€‚åŒæ—¶ï¼Œå¼€æ”¾å¤§è§„æ¨¡é«˜è´¨é‡çš„é•¿é“¾æ€ç»´æ•°æ®èµ„æºï¼Œå»ºç«‹æ ‡å‡†åŒ–åŸºçº¿æ¨¡å‹ã€‚å®éªŒè¯æ˜ï¼Œæ–¹æ³•å’Œæ•°æ®å‡æœ‰æ•ˆã€‚åŒæ—¶ç†è®ºä¸Šè¯æ˜äº†å¥–åŠ±æ–¹å·®å¯¹é¢„æœŸç­–ç•¥æ¢¯åº¦å¹…åº¦çš„å½±å“ï¼Œå¹¶åœ¨å®è·µä¸­é€šè¿‡VASå®ç°æ­¤ä¿éšœã€‚ç›¸å…³èµ„æºå’Œä»£ç å·²å¼€æºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹å¤šæ¨¡æ€æ¨ç†æ¨¡å‹é¢ä¸´å¼€æ”¾æ•°æ®ç¼ºå¤±å’Œå¼ºåŒ–å­¦ä¹ ç®—æ³•ä¸ç¨³å®šä¸¤å¤§æŒ‘æˆ˜ã€‚</li>
<li>VASç­–ç•¥ç»“åˆç»“æœæ–¹å·®å’Œè½¨è¿¹å¤šæ ·æ€§ä»¥ä¿ƒè¿›å¥–åŠ±æ–¹å·®å¹¶ç¨³å®šç­–ç•¥ä¼˜åŒ–ã€‚</li>
<li>å¼€æ”¾å¤§è§„æ¨¡é«˜è´¨é‡çš„é•¿é“¾æ€ç»´æ•°æ®èµ„æºã€‚</li>
<li>å»ºç«‹æ ‡å‡†åŒ–åŸºçº¿æ¨¡å‹å¹¶å¼€æºä¸€ç³»åˆ—å¤šæ¨¡æ€æ¨ç†æ¨¡å‹ã€‚</li>
<li>å®éªŒè¯æ˜æ–¹æ³•å’Œæ•°æ®çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>ç†è®ºä¸Šè¯æ˜äº†å¥–åŠ±æ–¹å·®å¯¹é¢„æœŸç­–ç•¥æ¢¯åº¦å¹…åº¦çš„å½±å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.21268">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-04be6096340cb409e08414a2d0e54b5e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684402&auth_key=1760684402-0-0-ac27d86ea44fbc457bbc9659e51a90de&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1f7850256198d4ecd194f6d8d253885c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684409&auth_key=1760684409-0-0-f97029872b22b156fe8fbff9ae338379&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-082d801de85c5fc8203d83c914527832~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684416&auth_key=1760684416-0-0-02106230ac5bc412cc37b37e3c721929&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4213c45c556b44b1423751747aed5bd4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684423&auth_key=1760684423-0-0-4fb1ffc6dcc1e29efbd4bfcf7802a718&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Instruction-tuned-Self-Questioning-Framework-for-Multimodal-Reasoning"><a href="#Instruction-tuned-Self-Questioning-Framework-for-Multimodal-Reasoning" class="headerlink" title="Instruction-tuned Self-Questioning Framework for Multimodal Reasoning"></a>Instruction-tuned Self-Questioning Framework for Multimodal Reasoning</h2><p><strong>Authors:You-Won Jang, Yu-Jung Heo, Jaeseok Kim, Minsu Lee, Du-Seong Chang, Byoung-Tak Zhang</strong></p>
<p>The field of vision-language understanding has been actively researched in recent years, thanks to the development of Large Language Models~(LLMs). However, it still needs help with problems requiring multi-step reasoning, even for very simple questions. Recent studies adopt LLMs to tackle this problem by iteratively generating sub-questions and answers. However, there are disadvantages such as 1) the fine-grained visual contents of images are not available using LLMs that cannot read visual information, 2) internal mechanisms are inaccessible and difficult to reproduce by using black-box LLMs. To solve these problems, we propose the SQ (Self-Questioning)-InstructBLIP, which improves inference performance by generating image-aware informative sub-questions and sub-answers iteratively. The SQ-InstructBLIP, which consists of a Questioner, Answerer, and Reasoner that share the same architecture. Questioner and Answerer generate sub-questions and sub-answers to help infer the main-question, and Reasoner performs reasoning on the main-question considering the generated sub-question information. Our experiments show that the proposed method SQ-InstructBLIP, which uses the generated sub-questions as additional information when solving the VQA task, performs more accurate reasoning than the previous works. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€ç†è§£é¢†åŸŸè¿‘å¹´æ¥å¾—åˆ°äº†å¹¿æ³›çš„ç ”ç©¶ï¼Œè¿™å¾—ç›Šäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å‘å±•ã€‚ç„¶è€Œï¼Œå®ƒä»ç„¶éœ€è¦è§£å†³éœ€è¦è¿›è¡Œå¤šæ­¥éª¤æ¨ç†çš„é—®é¢˜ï¼Œå³ä½¿æ˜¯å¯¹äºéå¸¸ç®€å•çš„é—®é¢˜ã€‚æœ€è¿‘çš„ç ”ç©¶é‡‡ç”¨LLMsæ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œé€šè¿‡è¿­ä»£ç”Ÿæˆå­é—®é¢˜å’Œç­”æ¡ˆã€‚ç„¶è€Œï¼Œå­˜åœ¨ä¸€äº›ç¼ºç‚¹ï¼Œä¾‹å¦‚1ï¼‰LLMsæ— æ³•è¯»å–è§†è§‰ä¿¡æ¯ï¼Œå› æ­¤æ— æ³•è·å–å›¾åƒçš„ç²¾ç»†è§†è§‰å†…å®¹ï¼›2ï¼‰ä½¿ç”¨é»‘ç®±LLMsæ—¶ï¼Œå…¶å†…éƒ¨æœºåˆ¶æ— æ³•è®¿é—®ä¸”éš¾ä»¥å¤åˆ¶ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†SQï¼ˆè‡ªæˆ‘æé—®ï¼‰-InstructBLIPï¼Œå®ƒé€šè¿‡è¿­ä»£ç”Ÿæˆå›¾åƒæ„ŸçŸ¥ä¿¡æ¯ä¸°å¯Œçš„å­é—®é¢˜å’Œå­ç­”æ¡ˆï¼Œä»è€Œæé«˜æ¨ç†æ€§èƒ½ã€‚SQ-InstructBLIPç”±å…·æœ‰ç›¸åŒæ¶æ„çš„æé—®è€…ã€å›ç­”è€…å’Œæ¨ç†è€…ç»„æˆã€‚æé—®è€…å’Œå›ç­”è€…ç”Ÿæˆå­é—®é¢˜å’Œå­ç­”æ¡ˆä»¥å¸®åŠ©æ¨æ–­ä¸»é—®é¢˜ï¼Œè€Œæ¨ç†è€…åˆ™è€ƒè™‘ç”Ÿæˆçš„å­é—®é¢˜ä¿¡æ¯è¿›è¡Œä¸»é—®é¢˜çš„æ¨ç†ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„SQ-InstructBLIPæ–¹æ³•åœ¨ä½¿ç”¨å­é—®é¢˜ä½œä¸ºè§£å†³VQAä»»åŠ¡æ—¶çš„é™„åŠ ä¿¡æ¯æ—¶ï¼Œæ¯”ä»¥å‰çš„å·¥ä½œè¡¨ç°å‡ºæ›´å‡†ç¡®çš„æ¨ç†èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.21251v1">PDF</a> This paper was accepted to the â€œCLVL: 5th Workshop on Closing the   Loop Between Vision and Language (ICCV 2023 CLVL workshop).â€</p>
<p><strong>Summary</strong><br>è§†è§‰è¯­è¨€ç†è§£é¢†åŸŸè¿‘å¹´æ¥å¾—åˆ°äº†å¹¿æ³›çš„ç ”ç©¶ï¼Œä½†ä»é¢ä¸´å¤šæ­¥æ¨ç†é—®é¢˜ã€‚æœ€æ–°ç ”ç©¶é‡‡ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è§£å†³æ­¤é—®é¢˜ï¼Œé€šè¿‡è¿­ä»£ç”Ÿæˆå­é—®é¢˜å’Œç­”æ¡ˆã€‚ç„¶è€Œï¼Œå­˜åœ¨ç¼ºç‚¹å¦‚æ— æ³•è¯»å–å›¾åƒä¸­çš„ç²¾ç»†è§†è§‰å†…å®¹ä»¥åŠé»‘ç®±LLMsçš„å†…éƒ¨æœºåˆ¶ä¸å¯è®¿é—®å’Œéš¾ä»¥å¤åˆ¶ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºSQ-InstructBLIPï¼Œé€šè¿‡ç”Ÿæˆå›¾åƒæ„ŸçŸ¥ä¿¡æ¯å­é—®é¢˜å’Œå­ç­”æ¡ˆæ¥æé«˜æ¨ç†æ€§èƒ½ã€‚SQ-InstructBLIPåŒ…æ‹¬æé—®è€…ã€å›ç­”è€…å’Œæ¨ç†è€…ï¼Œä¸‰è€…å…±äº«ç›¸åŒæ¶æ„ã€‚æé—®è€…å’Œå›ç­”è€…ç”Ÿæˆå­é—®é¢˜å’Œå­ç­”æ¡ˆä»¥è¾…åŠ©æ¨ç†ä¸»é—®é¢˜ï¼Œè€Œæ¨ç†è€…åˆ™è€ƒè™‘ç”Ÿæˆçš„å­é—®é¢˜ä¿¡æ¯è¿›è¡Œæ¨ç†ã€‚å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨ç”Ÿæˆçš„å­é—®é¢˜ä½œä¸ºè§£å†³è§†è§‰é—®ç­”ä»»åŠ¡æ—¶çš„é™„åŠ ä¿¡æ¯ï¼ŒSQ-InstructBLIPè¡¨ç°å‡ºæ¯”å…ˆå‰å·¥ä½œæ›´å‡†ç¡®çš„æ¨ç†èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è§†è§‰è¯­è¨€ç†è§£é¢†åŸŸé¢ä¸´å¤šæ­¥æ¨ç†æŒ‘æˆ˜ã€‚</li>
<li>æœ€æ–°ç ”ç©¶é€šè¿‡è¿­ä»£ç”Ÿæˆå­é—®é¢˜å’Œç­”æ¡ˆæ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>LLMsæ— æ³•è¯»å–å›¾åƒä¸­çš„ç²¾ç»†è§†è§‰ä¿¡æ¯æ˜¯ä¸€ä¸ªç¼ºç‚¹ã€‚</li>
<li>é»‘ç®±LLMsçš„å†…éƒ¨æœºåˆ¶ä¸å¯è®¿é—®å’Œéš¾ä»¥å¤åˆ¶ä¹Ÿæ˜¯ä¸€å¤§æŒ‘æˆ˜ã€‚</li>
<li>SQ-InstructBLIPé€šè¿‡ç”Ÿæˆå›¾åƒæ„ŸçŸ¥ä¿¡æ¯å­é—®é¢˜å’Œå­ç­”æ¡ˆæ¥æé«˜æ¨ç†æ€§èƒ½ã€‚</li>
<li>SQ-InstructBLIPåŒ…æ‹¬æé—®è€…ã€å›ç­”è€…å’Œæ¨ç†è€…ä¸‰ä¸ªç»„æˆéƒ¨åˆ†ï¼Œå…±äº«ç›¸åŒæ¶æ„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.21251">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-3103c9e9c2acf9a6c0370f5a12db0fbf~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684430&auth_key=1760684430-0-0-39e59d6cc9bcc0bac87919e8a914661d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-93b95d80be647d623e97cccf90fef700~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684438&auth_key=1760684438-0-0-556d9e76a6cd32d21eb00fb71e16af43&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ff72602a852b5474d0e8104191c13390~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684445&auth_key=1760684445-0-0-317c51edb1b9d2ba7d4fa3acb5447106&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0a848c335ddd32e6149a36f79758ef7b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684452&auth_key=1760684452-0-0-1e9706cd04c0d9c94932326adaf79694&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="RetoVLA-Reusing-Register-Tokens-for-Spatial-Reasoning-in-Vision-Language-Action-Models"><a href="#RetoVLA-Reusing-Register-Tokens-for-Spatial-Reasoning-in-Vision-Language-Action-Models" class="headerlink" title="RetoVLA: Reusing Register Tokens for Spatial Reasoning in   Vision-Language-Action Models"></a>RetoVLA: Reusing Register Tokens for Spatial Reasoning in   Vision-Language-Action Models</h2><p><strong>Authors:Jiyeon Koo, Taewan Cho, Hyunjoon Kang, Eunseom Pyo, Tae Gyun Oh, Taeryang Kim, Andrew Jaeyong Choi</strong></p>
<p>Recent Vision-Language-Action (VLA) models demonstrate remarkable generalization in robotics but are restricted by their substantial size and computational cost, limiting real-world deployment. However, conventional lightweighting methods often sacrifice critical capabilities, particularly spatial reasoning. This creates a trade-off between efficiency and performance. To address this challenge, our work reuses Register Tokens, which were introduced for artifact removal in Vision Transformers but subsequently discarded. We suppose that these tokens contain essential spatial information and propose RetoVLA, a novel architecture that reuses them directly by injecting them into the Action Expert.   RetoVLA maintains a lightweight structure while leveraging this repurposed spatial context to enhance reasoning. We demonstrate RetoVLAâ€™s effectiveness through a series of comprehensive experiments. On our custom-built 7-DOF robot arm, the model achieves a 17.1%p absolute improvement in success rates for complex manipulation tasks. Our results confirm that reusing Register Tokens directly enhances spatial reasoning, demonstrating that what was previously discarded as an artifact is in fact a valuable, unexplored resource for robotic intelligence. A video demonstration is available at: <a target="_blank" rel="noopener" href="https://youtu.be/2CseBR-snZg">https://youtu.be/2CseBR-snZg</a> </p>
<blockquote>
<p>æœ€è¿‘çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹åœ¨æœºå™¨äººæŠ€æœ¯ä¸­å±•ç°å‡ºå“è¶Šçš„æ³›åŒ–èƒ½åŠ›ï¼Œä½†å—é™äºå…¶åºå¤§çš„ä½“ç§¯å’Œè®¡ç®—æˆæœ¬ï¼Œåˆ¶çº¦äº†å…¶åœ¨ç°å®ä¸–ç•Œçš„éƒ¨ç½²ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„è½»é‡åŒ–æ–¹æ³•å¾€å¾€ä¼šç‰ºç‰²å…³é”®åŠŸèƒ½ï¼Œç‰¹åˆ«æ˜¯ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚è¿™ä½¿å¾—æ•ˆç‡å’Œæ€§èƒ½ä¹‹é—´å­˜åœ¨æƒè¡¡ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬çš„å·¥ä½œé‡æ–°åˆ©ç”¨äº†è§†è§‰è½¬æ¢å™¨ä¸­ç”¨äºå»é™¤ä¼ªå½±çš„å¯„å­˜å™¨ä»¤ç‰Œï¼ˆRegister Tokensï¼‰ï¼Œè¿™äº›ä»¤ç‰Œä¹‹åè¢«ä¸¢å¼ƒã€‚æˆ‘ä»¬å‡è®¾è¿™äº›ä»¤ç‰ŒåŒ…å«é‡è¦çš„ç©ºé—´ä¿¡æ¯ï¼Œå¹¶æå‡ºRetoVLAï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡ç›´æ¥å°†å…¶æ³¨å…¥åŠ¨ä½œä¸“å®¶ï¼ˆAction Expertï¼‰æ¥é‡æ–°åˆ©ç”¨å®ƒä»¬çš„æ–°å‹æ¶æ„ã€‚RetoVLAåœ¨ä¿æŒè½»é‡åŒ–ç»“æ„çš„åŒæ—¶ï¼Œåˆ©ç”¨è¿™ç§é‡æ–°å®šä½çš„ç©ºé—´ä¸Šä¸‹æ–‡æ¥æé«˜æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬é€šè¿‡ä¸€ç³»åˆ—å…¨é¢çš„å®éªŒè¯æ˜äº†RetoVLAçš„æœ‰æ•ˆæ€§ã€‚åœ¨æˆ‘ä»¬è‡ªä¸»æ„å»ºçš„7è‡ªç”±åº¦ï¼ˆ7-DOFï¼‰æœºæ¢°è‡‚ä¸Šï¼Œè¯¥æ¨¡å‹åœ¨å¤æ‚æ“ä½œä»»åŠ¡æ–¹é¢çš„æˆåŠŸç‡æé«˜äº†ç»å¯¹è¾¾17.1%ã€‚æˆ‘ä»¬çš„ç»“æœè¯å®ï¼Œç›´æ¥é‡æ–°åˆ©ç”¨å¯„å­˜å™¨ä»¤ç‰Œå¯ä»¥å¢å¼ºç©ºé—´æ¨ç†èƒ½åŠ›ï¼Œè¡¨æ˜å…ˆå‰è¢«ä¸¢å¼ƒä½œä¸ºä¼ªå½±çš„ä¸œè¥¿å®é™…ä¸Šæ˜¯ä¸€ä¸ªæœ‰ä»·å€¼çš„ã€å°šæœªæ¢ç´¢çš„æœºå™¨äººæ™ºèƒ½èµ„æºã€‚è§†é¢‘æ¼”ç¤ºå¯åœ¨ä»¥ä¸‹é“¾æ¥æ‰¾åˆ°ï¼š[<a target="_blank" rel="noopener" href="https://youtu.be/2CseBR-snZg]%EF%BC%88%E6%B3%A8%EF%BC%9A%E9%93%BE%E6%8E%A5%E9%9C%80%E6%A0%B9%E6%8D%AE%E5%AE%9E%E9%99%85%E6%83%85%E5%86%B5%E8%BF%9B%E8%A1%8C%E6%9B%BF%E6%8D%A2%EF%BC%89%E3%80%82">https://youtu.be/2CseBR-snZg]ï¼ˆæ³¨ï¼šé“¾æ¥éœ€æ ¹æ®å®é™…æƒ…å†µè¿›è¡Œæ›¿æ¢ï¼‰ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.21243v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸï¼Œè§†è§‰è¯­è¨€åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹åœ¨æœºå™¨äººé¢†åŸŸå±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œä½†å…¶åºå¤§çš„è§„æ¨¡å’Œè®¡ç®—æˆæœ¬é™åˆ¶äº†å®é™…åº”ç”¨ã€‚ä¸ºè§£å†³æ•ˆç‡ä¸æ€§èƒ½ä¹‹é—´çš„æƒè¡¡é—®é¢˜ï¼Œæœ¬ç ”ç©¶é‡æ–°åˆ©ç”¨äº†åœ¨è§†è§‰è½¬æ¢å™¨ä¸­ç”¨äºå»é™¤ä¼ªç‰¹å¾çš„å¯„å­˜å™¨ä»¤ç‰Œï¼ˆRegister Tokensï¼‰ã€‚æˆ‘ä»¬å‡è®¾è¿™äº›ä»¤ç‰ŒåŒ…å«é‡è¦çš„ç©ºé—´ä¿¡æ¯ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°çš„æ¶æ„RetoVLAï¼Œå®ƒé€šè¿‡ç›´æ¥æ³¨å…¥åŠ¨ä½œä¸“å®¶ï¼ˆAction Expertï¼‰æ¥åˆ©ç”¨è¿™äº›ä»¤ç‰Œã€‚RetoVLAä¿æŒè½»é‡çº§ç»“æ„ï¼Œå¹¶åˆ©ç”¨æ­¤é‡æ–°è®¾è®¡çš„ç©ºé—´ä¸Šä¸‹æ–‡å¢å¼ºæ¨ç†èƒ½åŠ›ã€‚åœ¨è‡ªå®šä¹‰çš„7è‡ªç”±åº¦æœºå™¨äººæ‰‹è‡‚ä¸Šè¿›è¡Œçš„å®éªŒè¯æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å¤æ‚æ“ä½œä»»åŠ¡ä¸Šçš„æˆåŠŸç‡æé«˜äº†17.1%ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œé‡æ–°åˆ©ç”¨å¯„å­˜å™¨ä»¤ç‰Œå¯å¢å¼ºç©ºé—´æ¨ç†èƒ½åŠ›ï¼Œè¯æ˜ä»¥å‰è¢«è§†ä¸ºä¼ªç‰¹å¾çš„ä»¤ç‰Œå®é™…ä¸Šæ˜¯æœºå™¨äººæ™ºèƒ½ä¸­æœªè¢«æ¢ç´¢çš„å®è´µèµ„æºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VLAæ¨¡å‹åœ¨æœºå™¨äººé¢†åŸŸå…·æœ‰å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œä½†å­˜åœ¨è®¡ç®—æˆæœ¬é«˜å’Œéƒ¨ç½²å›°éš¾çš„é—®é¢˜ã€‚</li>
<li>å¯„å­˜å™¨ä»¤ç‰Œï¼ˆRegister Tokensï¼‰æœ€åˆç”¨äºå»é™¤ä¼ªç‰¹å¾ï¼Œåœ¨æœ¬ç ”ç©¶ä¸­è¢«é‡æ–°åˆ©ç”¨ã€‚</li>
<li>é€šè¿‡ç›´æ¥æ³¨å…¥åŠ¨ä½œä¸“å®¶ï¼ˆAction Expertï¼‰ï¼Œæå‡ºæ–°çš„æ¶æ„RetoVLAã€‚</li>
<li>RetoVLAåœ¨ä¿æŒè½»é‡çº§ç»“æ„çš„åŒæ—¶ï¼Œåˆ©ç”¨ç©ºé—´ä¸Šä¸‹æ–‡å¢å¼ºæ¨ç†èƒ½åŠ›ã€‚</li>
<li>åœ¨è‡ªå®šä¹‰æœºå™¨äººæ‰‹è‡‚ä¸Šçš„å®éªŒè¯æ˜ï¼ŒRetoVLAåœ¨å¤æ‚æ“ä½œä»»åŠ¡ä¸Šçš„æˆåŠŸç‡æ˜¾è‘—æé«˜ã€‚</li>
<li>é‡æ–°åˆ©ç”¨å¯„å­˜å™¨ä»¤ç‰Œå¢å¼ºäº†ç©ºé—´æ¨ç†èƒ½åŠ›ï¼Œè¯æ˜äº†å…¶ä½œä¸ºæœºå™¨äººæ™ºèƒ½ä¸­æœªè¢«æ¢ç´¢èµ„æºçš„é‡è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.21243">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-9406606693842acb4dc28c2a171eb21e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684460&auth_key=1760684460-0-0-274e6f4a3ba30f0c512727609e0233c1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-985944478b86173881fa23c81a013a0e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684468&auth_key=1760684468-0-0-471bd889eae61cdcda59361b5f35fd98&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0045a970110011d4d3c26abd50b5fbd8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684475&auth_key=1760684475-0-0-493fceec411f33bfafcac3495511f08c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-632a22352b44794a1862c3b0cf019255~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684482&auth_key=1760684482-0-0-b115ea3b09152f442ffb3dc80ebc1e6e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f795d74d5d67c36b43ae0aeba9a22d75~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684488&auth_key=1760684488-0-0-870822c85dd07562eb9022fbd74ead6d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b201b519850d4d7876e3dfbc61e52b60~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684495&auth_key=1760684495-0-0-9b67da9365e6a4b93433d81ea9642093&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a907fef0c2a21203ae22ae995d39ab71~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684501&auth_key=1760684501-0-0-9d35fa8a08be9f3098583fb00c0a89e8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-843c6cfa03b02ccda980d3ff2984cd4f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684508&auth_key=1760684508-0-0-31219337bda967233c35068c2cb5bfc7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Tree-Search-for-LLM-Agent-Reinforcement-Learning"><a href="#Tree-Search-for-LLM-Agent-Reinforcement-Learning" class="headerlink" title="Tree Search for LLM Agent Reinforcement Learning"></a>Tree Search for LLM Agent Reinforcement Learning</h2><p><strong>Authors:Yuxiang Ji, Ziyu Ma, Yong Wang, Guanhua Chen, Xiangxiang Chu, Liaoni Wu</strong></p>
<p>Recent advances in reinforcement learning (RL) have significantly enhanced the agentic capabilities of large language models (LLMs). In long-term and multi-turn agent tasks, existing approaches driven solely by outcome rewards often suffer from the problem of sparse supervision. To address the challenge, we propose Tree-based Group Relative Policy Optimization (Tree-GRPO), a grouped agent RL method based on tree search, where each tree node represents the complete agent interaction step. By sharing common prefixes, the tree search sampling increases the number of rollouts achievable within a fixed budget of tokens or tool calls. Moreover, we find that the tree-structured trajectory naturally allows the construction of step-wise process supervised signals even using only the outcome reward. Based on this, Tree-GRPO estimates the grouped relative advantages both on intra-tree and inter-tree levels. Through theoretical analysis, we demonstrate that the objective of intra-tree level group relative policy optimization is equivalent to that of step-level direct preference learning. Experiments across 11 datasets and 3 types of QA tasks demonstrate the superiority of the proposed tree-based RL over the chain-based RL method. </p>
<blockquote>
<p>æœ€è¿‘å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„è¿›å±•æå¤§åœ°å¢å¼ºäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä»£ç†èƒ½åŠ›ã€‚åœ¨é•¿æœŸå’Œå¤šè½®ä»£ç†ä»»åŠ¡ä¸­ï¼Œä»…ç”±ç»“æœå¥–åŠ±é©±åŠ¨çš„ä¼ ç»Ÿæ–¹æ³•ç»å¸¸é¢ä¸´ç›‘ç£ç¨€ç–çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºæ ‘ç»„çš„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆTree-GRPOï¼‰ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºæ ‘æœç´¢çš„åˆ†ç»„ä»£ç†RLæ–¹æ³•ï¼Œå…¶ä¸­æ¯ä¸ªæ ‘èŠ‚ç‚¹ä»£è¡¨å®Œæ•´çš„ä»£ç†äº¤äº’æ­¥éª¤ã€‚é€šè¿‡å…±äº«å…¬å…±å‰ç¼€ï¼Œæ ‘æœç´¢é‡‡æ ·å¢åŠ äº†åœ¨å›ºå®šé¢„ç®—çš„ä»¤ç‰Œæˆ–å·¥å…·è°ƒç”¨æœŸé—´å¯å®ç°çš„rolloutsæ•°é‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°æ ‘çŠ¶è½¨è¿¹è‡ªç„¶åœ°å…è®¸ä½¿ç”¨ä»…ç»“æœå¥–åŠ±æ¥æ„å»ºé€æ­¥è¿‡ç¨‹ç›‘ç£ä¿¡å·ã€‚åŸºäºæ­¤ï¼ŒTree-GRPOä¼°è®¡äº†æ ‘å†…å’Œæ ‘é—´çš„åˆ†ç»„ç›¸å¯¹ä¼˜åŠ¿ã€‚é€šè¿‡ç†è®ºåˆ†æï¼Œæˆ‘ä»¬è¯æ˜äº†æ ‘å†…çº§åˆ«åˆ†ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–çš„ç›®æ ‡ä¸æ­¥éª¤çº§åˆ«ç›´æ¥åå¥½å­¦ä¹ çš„ç›®æ ‡æ˜¯ä¸€è‡´çš„ã€‚åœ¨11ä¸ªæ•°æ®é›†å’Œ3ç§é—®ç­”ä»»åŠ¡ä¸Šçš„å®éªŒè¯æ˜äº†æ‰€æå‡ºçš„åŸºäºæ ‘çš„RLä¼˜äºåŸºäºé“¾çš„RLæ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.21240v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¼ºåŒ–å­¦ä¹ åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„åº”ç”¨æ˜¾è‘—æå‡å…¶åœ¨é•¿æœŸå’Œå¤šè½®ä»»åŠ¡ä¸­çš„ä»£ç†èƒ½åŠ›ã€‚é’ˆå¯¹ä»…ä¾èµ–ç»“æœå¥–åŠ±å¸¦æ¥çš„ç¨€ç–ç›‘ç£é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†åŸºäºæ ‘æœç´¢çš„æ ‘ç»“æ„ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆTree-GRPOï¼‰æ–¹æ³•ã€‚æ ‘ç»“æ„å¯ä»¥æœ‰æ•ˆæ„å»ºæ­¥éª¤çº§çš„ç›‘ç£ä¿¡å·ï¼Œåˆ©ç”¨ç»“æœå¥–åŠ±è¿›è¡Œç»„å†…ç›¸å¯¹ä¼˜åŠ¿ä¼°è®¡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ ‘ç»“æ„çš„å¼ºåŒ–å­¦ä¹ ç›¸è¾ƒäºé“¾å¼å¼ºåŒ–å­¦ä¹ æ–¹æ³•æ›´å…·ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„åº”ç”¨å¢å¼ºäº†å…¶åœ¨é•¿æœŸå’Œå¤šè½®ä»»åŠ¡ä¸­çš„ä»£ç†èƒ½åŠ›ã€‚</li>
<li>Tree-GRPOæ˜¯ä¸€ç§åŸºäºæ ‘æœç´¢çš„ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–æ–¹æ³•ï¼Œè§£å†³äº†ä»…ä¾èµ–ç»“æœå¥–åŠ±å¸¦æ¥çš„ç¨€ç–ç›‘ç£é—®é¢˜ã€‚</li>
<li>æ ‘ç»“æ„å¯ä»¥æœ‰æ•ˆæ„å»ºæ­¥éª¤çº§çš„ç›‘ç£ä¿¡å·ã€‚</li>
<li>Tree-GRPOé€šè¿‡ä¼°è®¡ç»„å†…ç›¸å¯¹ä¼˜åŠ¿æ¥è§£å†³ç¨€ç–ç›‘ç£é—®é¢˜ã€‚</li>
<li>æ ‘ç»“æ„çš„å¼ºåŒ–å­¦ä¹ ç›¸è¾ƒäºé“¾å¼å¼ºåŒ–å­¦ä¹ æ–¹æ³•å…·æœ‰ä¼˜åŠ¿ã€‚</li>
<li>Tree-GRPOåœ¨ç†è®ºåˆ†æä¸å®éªŒéªŒè¯ä¸­éƒ½è¡¨ç°å‡ºå…¶æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.21240">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-d57591234dfc8351abfab2140917af24~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684515&auth_key=1760684515-0-0-2d5655c14db24f4ef68af455ef795c3f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c7f19634b6665ebcc39d85c51eec0f1a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684523&auth_key=1760684523-0-0-6a877781653ea7a64ccd69b66df56d63&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-afcb6cf789dae8597d7b22aafea00fbf~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684531&auth_key=1760684531-0-0-929ff0d1648b4cba5ebf25cbcf2886b8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="A-Fano-Style-Accuracy-Upper-Bound-for-LLM-Single-Pass-Reasoning-in-Multi-Hop-QA"><a href="#A-Fano-Style-Accuracy-Upper-Bound-for-LLM-Single-Pass-Reasoning-in-Multi-Hop-QA" class="headerlink" title="A Fano-Style Accuracy Upper Bound for LLM Single-Pass Reasoning in   Multi-Hop QA"></a>A Fano-Style Accuracy Upper Bound for LLM Single-Pass Reasoning in   Multi-Hop QA</h2><p><strong>Authors:Kaiyang Wan, Lang Gao, Honglin Mu, Preslav Nakov, Yuxia Wang, Xiuying Chen</strong></p>
<p>Multi-Hop Question Answering (MHQA) requires integrating dispersed, interdependent evidence through sequential reasoning under noise. This task is challenging for LLMs as they have a finite per-pass output capacity, beyond which the integration of task-relevant evidence proves unreliable. Consequently, the single-pass reasoning paradigm is inherently vulnerable to this capacity overflow. To formalize this bottleneck, our analysis establishes a Fano-style accuracy upper bound, defining a theoretical performance ceiling for single-pass LLMs. This bound reveals that accuracy inevitably collapses once task complexity exceeds model capacity, providing general principles for capacity-aware representation and structuring of MHQA in LLMs. Building on these principles, we introduce a proof-of-concept multi-call framework for MHQA, InfoQA. It ensures high per-step accuracy by combining capacity-aware task decomposition with active pruning of prior reasoning traces, keeping the information load within the single-pass limit. It further achieves robustness by a dependency-explicit workflow that enables precise control over the reasoning path. We construct a stringent and noise-rich benchmark to validate our theory and framework. Experimental results show that model behavior aligns with our predicted capacity curves while InfoQA achieves consistent performance improvements. We hope our work inspires more LLM multi-step reasoning methods: \faGithub \href{<a target="_blank" rel="noopener" href="https://github.com/KaiyangWan/InfoQA%7D%7BInfoQA%7D">https://github.com/KaiyangWan/InfoQA}{InfoQA}</a>. </p>
<blockquote>
<p>å¤šè·³é—®ç­”ï¼ˆMHQAï¼‰éœ€è¦åœ¨å™ªå£°ç¯å¢ƒä¸‹é€šè¿‡é¡ºåºæ¨ç†æ•´åˆåˆ†æ•£ä¸”ç›¸äº’ä¾èµ–çš„è¯æ®ã€‚å¯¹äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¥è¯´ï¼Œè¿™é¡¹ä»»åŠ¡å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºå®ƒä»¬æ¯æ¬¡ä¼ é€’çš„è¾“å‡ºå®¹é‡æ˜¯æœ‰é™çš„ï¼Œè¶…å‡ºæ­¤å®¹é‡åï¼Œä¸ä»»åŠ¡ç›¸å…³çš„è¯æ®æ•´åˆä¾¿å˜å¾—ä¸å¯é ã€‚å› æ­¤ï¼Œå•è·¯å¾„æ¨ç†æ¨¡å¼æœ¬è´¨ä¸Šå®¹æ˜“å—åˆ°è¿™ç§å®¹é‡æº¢å‡ºçš„å½±å“ã€‚ä¸ºäº†å½¢å¼åŒ–è¿™ä¸ªç“¶é¢ˆï¼Œæˆ‘ä»¬çš„åˆ†æå»ºç«‹äº†Fanoé£æ ¼çš„å‡†ç¡®æ€§ä¸Šé™ï¼Œä¸ºå•è·¯å¾„LLMså®šä¹‰äº†ç†è®ºæ€§èƒ½å¤©èŠ±æ¿ã€‚è¿™ä¸ªç•Œé™è¡¨æ˜ï¼Œä¸€æ—¦ä»»åŠ¡å¤æ‚åº¦è¶…è¿‡æ¨¡å‹å®¹é‡ï¼Œå‡†ç¡®æ€§ä¸å¯é¿å…åœ°ä¼šå´©æºƒï¼Œä¸ºåœ¨LLMsä¸­å®æ–½å®¹é‡æ„ŸçŸ¥è¡¨ç¤ºå’Œå¤šè·³é—®ç­”ç»“æ„åŒ–æä¾›äº†é€šç”¨åŸåˆ™ã€‚åŸºäºè¿™äº›åŸåˆ™ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªç”¨äºMHQAçš„æ¦‚å¿µéªŒè¯å¤šè°ƒç”¨æ¡†æ¶â€”â€”InfoQAã€‚å®ƒé€šè¿‡ç»“åˆå®¹é‡æ„ŸçŸ¥ä»»åŠ¡åˆ†è§£å’Œå…ˆå‰æ¨ç†ç—•è¿¹çš„ä¸»åŠ¨ä¿®å‰ªï¼Œç¡®ä¿æ¯ä¸€æ­¥çš„é«˜å‡†ç¡®æ€§ï¼ŒåŒæ—¶å°†ä¿¡æ¯è´Ÿè½½ä¿æŒåœ¨å•è·¯å¾„é™åˆ¶ä¹‹å†…ã€‚é€šè¿‡ä¾èµ–æ˜ç¡®çš„å·¥ä½œæµç¨‹ï¼Œå®ƒè¿›ä¸€æ­¥å®ç°äº†ç¨³å¥æ€§ï¼Œèƒ½å¤Ÿç²¾ç¡®æ§åˆ¶æ¨ç†è·¯å¾„ã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªä¸¥æ ¼ä¸”å¯Œå«å™ªå£°çš„åŸºå‡†æµ‹è¯•æ¥éªŒè¯æˆ‘ä»¬çš„ç†è®ºå’Œæ¡†æ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ¨¡å‹è¡Œä¸ºä¸æˆ‘ä»¬çš„é¢„æµ‹å®¹é‡æ›²çº¿ä¸€è‡´ï¼ŒåŒæ—¶InfoQAå®ç°äº†æ€§èƒ½çš„ä¸€è‡´æ€§æ”¹è¿›ã€‚æˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„å·¥ä½œèƒ½æ¿€å‘æ›´å¤šå…³äºå¤§å‹è¯­è¨€æ¨¡å‹å¤šæ­¥éª¤æ¨ç†æ–¹æ³•çš„ç ”ç©¶ï¼šInfoQAï¼ˆ<a target="_blank" rel="noopener" href="https://github.com/KaiyangWan/InfoQA">ç‚¹å‡»æ­¤å¤„æŸ¥çœ‹GitHubä»“åº“</a>ï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.21199v1">PDF</a> 21 pages, 6 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¤šè·³é—®ç­”ï¼ˆMHQAï¼‰çš„æŒ‘æˆ˜ï¼ŒæŒ‡å‡ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ•´åˆåˆ†æ•£ã€ç›¸äº’ä¾èµ–çš„è¯æ®æ—¶å­˜åœ¨å®¹é‡é™åˆ¶çš„é—®é¢˜ã€‚æ–‡ç« æå‡ºäº†ä¸€ä¸ªç†è®ºæ€§èƒ½ä¸Šé™çš„Fanoé£æ ¼ç²¾åº¦è¾¹ç•Œï¼Œæ­ç¤ºäº†å½“ä»»åŠ¡å¤æ‚åº¦è¶…è¿‡æ¨¡å‹å®¹é‡æ—¶ï¼Œå‡†ç¡®æ€§ä¸å¯é¿å…åœ°ä¼šå´©æºƒã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæ–‡ç« æå‡ºäº†ä¸€ä¸ªå¤šè°ƒç”¨æ¡†æ¶InfoQAï¼Œå®ƒé€šè¿‡å®¹é‡æ„ŸçŸ¥ä»»åŠ¡åˆ†è§£å’Œä¸»åŠ¨å‰ªé™¤å…ˆå‰æ¨ç†ç—•è¿¹çš„æ–¹å¼ï¼Œç¡®ä¿æ¯ä¸€æ­¥çš„é«˜ç²¾åº¦ï¼ŒåŒæ—¶é€šè¿‡ä¾èµ–æ˜ç¡®çš„å·¥ä½œæµç¨‹å®ç°æ¨ç†è·¯å¾„çš„ç²¾ç¡®æ§åˆ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒInfoQAå®ç°äº†æ€§èƒ½æ”¹è¿›ï¼Œå¹¶ä¸é¢„æµ‹å®¹é‡æ›²çº¿ä¸€è‡´ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤šè·³é—®ç­”ï¼ˆMHQAï¼‰è¦æ±‚æ•´åˆåˆ†æ•£ã€ç›¸äº’ä¾èµ–çš„è¯æ®ï¼Œè¿™ä¸€ä»»åŠ¡å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>LLMsåœ¨æ•´åˆä»»åŠ¡ç›¸å…³è¯æ®æ—¶å­˜åœ¨å®¹é‡é™åˆ¶ï¼Œè¶…å‡ºå®¹é‡èŒƒå›´åï¼Œå•é€šé“æ¨ç†æ¨¡å¼å®¹æ˜“å‡ºé”™ã€‚</li>
<li>æ–‡ç« å»ºç«‹äº†Fanoé£æ ¼çš„ç²¾åº¦ä¸Šé™ï¼Œå®šä¹‰äº†å•é€šé“LLMsçš„ç†è®ºæ€§èƒ½å¤©èŠ±æ¿ã€‚</li>
<li>å½“ä»»åŠ¡å¤æ‚åº¦è¶…è¿‡æ¨¡å‹å®¹é‡æ—¶ï¼Œå‡†ç¡®æ€§ä¼šä¸å¯é¿å…åœ°å´©æºƒã€‚</li>
<li>InfoQAæ¡†æ¶é€šè¿‡å®¹é‡æ„ŸçŸ¥ä»»åŠ¡åˆ†è§£å’Œä¸»åŠ¨å‰ªé™¤å…ˆå‰æ¨ç†ç—•è¿¹çš„æ–¹å¼ï¼Œç¡®ä¿é«˜ç²¾åº¦å’Œå¤šæ­¥éª¤æ¨ç†çš„ç¨³å¥æ€§ã€‚</li>
<li>InfoQAå®ç°äº†æ€§èƒ½æ”¹è¿›ï¼Œå¹¶é€šè¿‡ä¸¥æ ¼çš„å™ªå£°ä¸°å¯Œçš„åŸºå‡†æµ‹è¯•éªŒè¯äº†å…¶ç†è®ºå’Œæ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.21199">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-843c56abd40d303a3fbe7a90cec7a167~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684539&auth_key=1760684539-0-0-f8e83ac3acfcb773f621e3e1de79cd0b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9a246078697c59cdb06fd67eb858cf56~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684547&auth_key=1760684547-0-0-6fe62c2afca3c19d9cc9ada2bd41e75f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-966648e8e2fd47f3176ff1bf47855591~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684554&auth_key=1760684554-0-0-f136e1d1899ea5232c9bad285bb4fdfb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-de2cc303cae39ab89a7bccbbcf5dda6a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684560&auth_key=1760684560-0-0-6258252af5b1c95022dff3f7d8b6d5dd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Eigen-1-Adaptive-Multi-Agent-Refinement-with-Monitor-Based-RAG-for-Scientific-Reasoning"><a href="#Eigen-1-Adaptive-Multi-Agent-Refinement-with-Monitor-Based-RAG-for-Scientific-Reasoning" class="headerlink" title="Eigen-1: Adaptive Multi-Agent Refinement with Monitor-Based RAG for   Scientific Reasoning"></a>Eigen-1: Adaptive Multi-Agent Refinement with Monitor-Based RAG for   Scientific Reasoning</h2><p><strong>Authors:Xiangru Tang, Wanghan Xu, Yujie Wang, Zijie Guo, Daniel Shao, Jiapeng Chen, Cixuan Zhang, Ziyi Wang, Lixin Zhang, Guancheng Wan, Wenlong Zhang, Lei Bai, Zhenfei Yin, Philip Torr, Hanrui Wang, Di Jin</strong></p>
<p>Large language models (LLMs) have recently shown strong progress on scientific reasoning, yet two major bottlenecks remain. First, explicit retrieval fragments reasoning, imposing a hidden â€œtool taxâ€ of extra tokens and steps. Second, multi-agent pipelines often dilute strong solutions by averaging across all candidates. We address these challenges with a unified framework that combines implicit retrieval and structured collaboration. At its foundation, a Monitor-based retrieval module operates at the token level, integrating external knowledge with minimal disruption to reasoning. On top of this substrate, Hierarchical Solution Refinement (HSR) iteratively designates each candidate as an anchor to be repaired by its peers, while Quality-Aware Iterative Reasoning (QAIR) adapts refinement to solution quality. On Humanityâ€™s Last Exam (HLE) Bio&#x2F;Chem Gold, our framework achieves 48.3% accuracy â€“ the highest reported to date, surpassing the strongest agent baseline by 13.4 points and leading frontier LLMs by up to 18.1 points, while simultaneously reducing token usage by 53.5% and agent steps by 43.7%. Results on SuperGPQA and TRQA confirm robustness across domains. Error analysis shows that reasoning failures and knowledge gaps co-occur in over 85% of cases, while diversity analysis reveals a clear dichotomy: retrieval tasks benefit from solution variety, whereas reasoning tasks favor consensus. Together, these findings demonstrate how implicit augmentation and structured refinement overcome the inefficiencies of explicit tool use and uniform aggregation. Code is available at: <a target="_blank" rel="noopener" href="https://github.com/tangxiangru/Eigen-1">https://github.com/tangxiangru/Eigen-1</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç§‘å­¦æ¢ç©¶æ¨ç†æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ä»å­˜åœ¨ä¸¤ä¸ªä¸»è¦ç“¶é¢ˆã€‚é¦–å…ˆï¼Œæ˜ç¡®çš„æ£€ç´¢ç‰‡æ®µåŒ–æ¨ç†ï¼Œå¢åŠ äº†é¢å¤–çš„æ ‡è®°å’Œæ­¥éª¤ï¼Œæ„æˆäº†ä¸€ç§éšè—çš„â€œå·¥å…·ç¨â€ã€‚å…¶æ¬¡ï¼Œå¤šæ™ºèƒ½ä½“ç®¡é“å¾€å¾€ä¼šé€šè¿‡å¹³å‡æ‰€æœ‰å€™é€‰è€…æ¥ç¨€é‡Šä¼˜ç§€è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬ç”¨ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†éšå¼æ£€ç´¢å’Œç»“æ„åŒ–åä½œã€‚åœ¨å…¶åŸºç¡€ä¸Šï¼Œä¸€ä¸ªåŸºäºç›‘è§†å™¨çš„æ£€ç´¢æ¨¡å—åœ¨ä»¤ç‰Œå±‚é¢è¿è¡Œï¼Œå°†å¤–éƒ¨çŸ¥è¯†ä¸æ¨ç†ä¹‹é—´çš„å¹²æ‰°é™è‡³æœ€ä½ã€‚åœ¨æ­¤åŸºç¡€ä¹‹ä¸Šï¼Œåˆ†å±‚è§£å†³æ–¹æ¡ˆç²¾ç‚¼ï¼ˆHSRï¼‰ä¼šè¿­ä»£åœ°æŒ‡å®šæ¯ä¸ªå€™é€‰è€…ä½œä¸ºé”šç‚¹ï¼Œç”±åŒè¡Œè¿›è¡Œä¿®å¤ï¼ŒåŒæ—¶è´¨é‡æ„ŸçŸ¥è¿­ä»£æ¨ç†ï¼ˆQAIRï¼‰ä¼šæ ¹æ®è§£å†³æ–¹æ¡ˆè´¨é‡è¿›è¡Œç²¾ç‚¼ã€‚åœ¨äººç±»æœ€åçš„è€ƒè¯•ï¼ˆHLEï¼‰ç”Ÿç‰©&#x2F;åŒ–å­¦é‡‘ç‰Œç§‘ç›®ä¸Šï¼Œæˆ‘ä»¬çš„æ¡†æ¶è¾¾åˆ°äº†48.3%çš„å‡†ç¡®ç‡ï¼Œè¿™æ˜¯è¿„ä»Šä¸ºæ­¢æŠ¥é“çš„æœ€é«˜å‡†ç¡®ç‡ã€‚ä¸æœ€å¼ºçš„æ™ºèƒ½ä½“åŸºå‡†ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„å‡†ç¡®ç‡æé«˜äº†13.4ä¸ªç™¾åˆ†ç‚¹ï¼Œå¹¶é¢†å…ˆå‰æ²¿çš„å¤§å‹è¯­è¨€æ¨¡å‹æœ€å¤šè¾¾18.1ä¸ªç™¾åˆ†ç‚¹ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬çš„æ¡†æ¶å‡å°‘äº†53.5%çš„ä»¤ç‰Œä½¿ç”¨é‡å’Œ43.7%çš„æ™ºèƒ½ä½“æ­¥éª¤ã€‚åœ¨SuperGPQAå’ŒTRQAä¸Šçš„ç»“æœè¯å®äº†å…¶åœ¨ä¸åŒé¢†åŸŸçš„ç¨³å¥æ€§ã€‚é”™è¯¯åˆ†æè¡¨æ˜ï¼Œæ¨ç†å¤±è´¥å’ŒçŸ¥è¯†å·®è·åœ¨è¶…è¿‡85%çš„æƒ…å†µä¸‹åŒæ—¶å­˜åœ¨ã€‚å¤šæ ·æ€§åˆ†ææ˜¾ç¤ºäº†ä¸€ç§æ˜ç¡®çš„å·®å¼‚ï¼šæ£€ç´¢ä»»åŠ¡å—ç›Šäºè§£å†³æ–¹æ¡ˆçš„å¤šæ ·æ€§ï¼Œè€Œæ¨ç†ä»»åŠ¡åˆ™å€¾å‘äºè¾¾æˆå…±è¯†ã€‚è¿™äº›å‘ç°å…±åŒè¡¨æ˜ï¼Œéšå¼å¢å¼ºå’Œç»“æ„åŒ–ç²¾ç‚¼å¦‚ä½•å…‹æœæ˜¾å¼å·¥å…·ä½¿ç”¨å’Œç»Ÿä¸€èšåˆçš„æ— æ•ˆæ€§ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/tangxiangru/Eigen-1%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/tangxiangru/Eigen-1ä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.21193v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç§‘å­¦æ¨ç†æ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†ä»é¢ä¸´ä¸¤ä¸ªä¸»è¦ç“¶é¢ˆï¼šæ˜¾å¼æ£€ç´¢ç‰‡æ®µåŒ–æ¨ç†å’Œå¤šä»£ç†ç®¡é“å¹³å‡æ–¹æ¡ˆã€‚ç ”ç©¶å›¢é˜Ÿæå‡ºä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œç»“åˆéšæ€§æ£€ç´¢å’Œç»“æ„åŒ–åä½œæ¥è§£å†³è¿™äº›é—®é¢˜ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬åŸºäºç›‘è§†å™¨çš„æ£€ç´¢æ¨¡å—å’Œåˆ†å±‚è§£å†³æ–¹æ¡ˆç»†åŒ–ï¼Œèƒ½å¤Ÿåœ¨æœ€å°‘å¹²æ‰°çš„æƒ…å†µä¸‹æ•´åˆå¤–éƒ¨çŸ¥è¯†ã€‚åœ¨Humanityâ€™s Last Examç”Ÿç‰©&#x2F;åŒ–å­¦é‡‘ç‰Œä»»åŠ¡ä¸Šï¼Œè¯¥æ¡†æ¶å–å¾—äº†48.3%çš„æœ€é«˜å‡†ç¡®ç‡ï¼Œé¢†å…ˆäºæœ€å¼ºçš„ä»£ç†åŸºå‡†ç‚¹å’Œå‰æ²¿çš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚åŒæ—¶ï¼Œå®ƒå‡å°‘äº†53.5%çš„ä»¤ç‰Œä½¿ç”¨é‡å’Œ43.7%çš„ä»£ç†æ­¥éª¤ã€‚ä»£ç å·²å…¬å¼€åœ¨GitHubä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç§‘å­¦æ¨ç†æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†ä»é¢ä¸´ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šæ˜¾å¼æ£€ç´¢çš„ç“¶é¢ˆå’Œå¤šä»£ç†ç®¡é“çš„å¹³å‡æ–¹æ¡ˆé—®é¢˜ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§ç»Ÿä¸€æ¡†æ¶ï¼Œç»“åˆéšæ€§æ£€ç´¢å’Œç»“æ„åŒ–åä½œï¼Œä»¥æé«˜è¯­è¨€æ¨¡å‹åœ¨æ¨ç†ä»»åŠ¡ä¸Šçš„æ•ˆç‡ã€‚</li>
<li>è¯¥æ¡†æ¶åŒ…æ‹¬åŸºäºç›‘è§†å™¨çš„æ£€ç´¢æ¨¡å—å’Œåˆ†å±‚è§£å†³æ–¹æ¡ˆç»†åŒ–ï¼Œèƒ½æœ€å°åŒ–å¤–éƒ¨çŸ¥è¯†ä¸æ¨ç†çš„å¹²æ‰°ã€‚</li>
<li>åœ¨Humanityâ€™s Last Examç”Ÿç‰©&#x2F;åŒ–å­¦é‡‘ç‰Œä»»åŠ¡ä¸Šï¼Œè¯¥æ¡†æ¶å–å¾—äº†æ˜¾è‘—æˆæœï¼Œè¾¾åˆ°48.3%çš„å‡†ç¡®ç‡ï¼Œè¶…è¶Šå…¶ä»–æ¨¡å‹ã€‚</li>
<li>ä¸å…¶ä»–æ¨¡å‹ç›¸æ¯”ï¼Œè¯¥æ¡†æ¶å‡å°‘äº†ä»¤ç‰Œä½¿ç”¨é‡å’Œä»£ç†æ­¥éª¤ã€‚</li>
<li>é”™è¯¯åˆ†ææ˜¾ç¤ºï¼Œæ¨ç†å¤±è´¥å’ŒçŸ¥è¯†å·®è·åœ¨è¶…è¿‡85%çš„æƒ…å†µä¸‹åŒæ—¶å‘ç”Ÿã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.21193">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-9bff588bf2f845bb230f58bfe527cba0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684569&auth_key=1760684569-0-0-b2a03cb54175d3fbd9e514f0883879b7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7ff3bc4d2de06bdecfb2efd0e63bc7bf~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684577&auth_key=1760684577-0-0-25a566a5b89d7a6e5030aa3bd63cff08&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-aa1eb57e8ca4a53950dc0e667763ac0a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684583&auth_key=1760684583-0-0-8ebd0376d727b6797e4703b26f977bbd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-cf21faff35809d36c0d2cefbb412ec60~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684590&auth_key=1760684590-0-0-7723f7989599198c6f6ed82d8f35e95c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-bf5c86e8d96d122821067678190a9dac~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684598&auth_key=1760684598-0-0-5ff1f050c3d519f7760819993bc6cb29&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Whoâ€™s-Laughing-Now-An-Overview-of-Computational-Humour-Generation-and-Explanation"><a href="#Whoâ€™s-Laughing-Now-An-Overview-of-Computational-Humour-Generation-and-Explanation" class="headerlink" title="Whoâ€™s Laughing Now? An Overview of Computational Humour Generation and   Explanation"></a>Whoâ€™s Laughing Now? An Overview of Computational Humour Generation and   Explanation</h2><p><strong>Authors:Tyler Loakman, William Thorne, Chenghua Lin</strong></p>
<p>The creation and perception of humour is a fundamental human trait, positioning its computational understanding as one of the most challenging tasks in natural language processing (NLP). As an abstract, creative, and frequently context-dependent construct, humour requires extensive reasoning to understand and create, making it a pertinent task for assessing the common-sense knowledge and reasoning abilities of modern large language models (LLMs). In this work, we survey the landscape of computational humour as it pertains to the generative tasks of creation and explanation. We observe that, despite the task of understanding humour bearing all the hallmarks of a foundational NLP task, work on generating and explaining humour beyond puns remains sparse, while state-of-the-art models continue to fall short of human capabilities. We bookend our literature survey by motivating the importance of computational humour processing as a subdiscipline of NLP and presenting an extensive discussion of future directions for research in the area that takes into account the subjective and ethically ambiguous nature of humour. </p>
<blockquote>
<p>å¹½é»˜çš„åˆ›é€ ä¸æ„ŸçŸ¥æ˜¯äººç±»çš„åŸºæœ¬ç‰¹è´¨ï¼Œè¿™ä¹Ÿä½¿å¾—è®¡ç®—æœºå¯¹å…¶çš„ç†è§£æˆä¸ºè‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä¸­æœ€å…·æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ä¹‹ä¸€ã€‚å¹½é»˜ä½œä¸ºä¸€ä¸ªæŠ½è±¡ã€å¯Œæœ‰åˆ›é€ åŠ›çš„æ¦‚å¿µï¼Œå¹¶ä¸”ç»å¸¸ä¾èµ–äºç‰¹å®šçš„è¯­å¢ƒï¼Œç†è§£å’Œåˆ›é€ å®ƒéœ€è¦å¹¿æ³›çš„æ¨ç†èƒ½åŠ›ã€‚å› æ­¤ï¼Œå®ƒæ˜¯è¯„ä¼°ç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¸¸è¯†çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›çš„ä¸€ä¸ªç›¸å…³ä»»åŠ¡ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è°ƒæŸ¥äº†å¹½é»˜è®¡ç®—é¢†åŸŸçš„æ¦‚å†µï¼Œç‰¹åˆ«æ˜¯å®ƒä¸åˆ›ä½œå’Œè§£é‡Šç­‰ç”Ÿæˆä»»åŠ¡çš„ç›¸å…³æ€§ã€‚æˆ‘ä»¬å‘ç°ï¼Œå°½ç®¡ç†è§£å¹½é»˜çš„ä»»åŠ¡å…·æœ‰åŸºç¡€NLPä»»åŠ¡çš„ç‰¹å¾ï¼Œä½†åœ¨åŒå…³è¯­ä¹‹å¤–çš„å¹½é»˜åˆ›ä½œå’Œè§£é‡Šå·¥ä½œä»ç„¶å¾ˆå°‘ï¼Œè€Œæœ€å…ˆè¿›çš„æ¨¡å‹ä»ç„¶è¾¾ä¸åˆ°äººç±»çš„èƒ½åŠ›æ°´å¹³ã€‚æˆ‘ä»¬ä»¥å¼ºè°ƒå¹½é»˜è®¡ç®—å¤„ç†ä½œä¸ºNLPå­å­¦ç§‘çš„é‡è¦æ€§ä½œä¸ºæ–‡çŒ®ç»¼è¿°çš„å¼€ç«¯ï¼Œå¹¶è®¨è®ºäº†è€ƒè™‘å¹½é»˜çš„ä¸»è§‚æ€§å’Œä¼¦ç†æ¨¡ç³Šæ€§çš„ç ”ç©¶æ–¹å‘çš„å¹¿æ³›è®¨è®ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.21175v1">PDF</a> Accepted to INLG 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†è®¡ç®—å¹½é»˜åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸçš„é‡è¦æ€§åŠå…¶ç”Ÿæˆä»»åŠ¡ä¸­çš„æŒ‘æˆ˜ã€‚è™½ç„¶å¹½é»˜ç†è§£æ˜¯NLPçš„åŸºç¡€ä»»åŠ¡ä¹‹ä¸€ï¼Œä½†åœ¨ç”Ÿæˆå’Œè§£é‡Šå¹½é»˜æ–¹é¢ï¼Œå°¤å…¶æ˜¯è¶…è¶ŠåŒå…³è¯­çš„ç ”ç©¶ä»ç„¶ç¨€ç¼ºã€‚å½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹ä»ç„¶æ— æ³•ä¸äººç±»çš„èƒ½åŠ›ç›¸æå¹¶è®ºã€‚æœ¬æ–‡è¿˜å¼ºè°ƒäº†è®¡ç®—å¹½é»˜å¤„ç†ä½œä¸ºNLPå­å­¦ç§‘çš„é‡è¦æ€§ï¼Œå¹¶è®¨è®ºäº†è€ƒè™‘åˆ°å¹½é»˜çš„ä¸»è§‚æ€§å’Œä¼¦ç†æ¨¡ç³Šæ€§ç‰¹å¾çš„æœªæ¥ç ”ç©¶æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¹½é»˜æ˜¯äººç±»çš„åŸºæœ¬ç‰¹è´¨ä¹‹ä¸€ï¼Œå…¶è®¡ç®—ç†è§£åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä¸­æ˜¯æå…·æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚</li>
<li>ç”Ÿæˆå’Œè§£é‡Šå¹½é»˜æ˜¯è®¡ç®—å¹½é»˜çš„ä¸¤ä¸ªæ ¸å¿ƒä»»åŠ¡ï¼Œç›®å‰ç›¸å…³å·¥ä½œä»è¾ƒä¸ºç¨€ç¼ºã€‚</li>
<li>å°½ç®¡å­˜åœ¨è®¸å¤šå…ˆè¿›æ¨¡å‹ï¼Œä½†åœ¨ç”Ÿæˆå’Œè§£é‡Šå¹½é»˜æ–¹é¢ï¼Œå®ƒä»¬ä»æ— æ³•ä¸äººç±»çš„èƒ½åŠ›ç›¸åŒ¹é…ã€‚</li>
<li>å¹½é»˜çš„ç†è§£éœ€è¦å¹¿æ³›çš„æ¨ç†èƒ½åŠ›ï¼Œè¿™ä½¿å…¶æˆä¸ºè¯„ä¼°ç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¸¸è¯†çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›çš„æ°å½“ä»»åŠ¡ã€‚</li>
<li>è®¡ç®—å¹½é»˜å¤„ç†æ˜¯NLPçš„ä¸€ä¸ªé‡è¦å­é¢†åŸŸã€‚</li>
<li>æœªæ¥çš„ç ”ç©¶æ–¹å‘éœ€è¦è€ƒè™‘åˆ°å¹½é»˜çš„ä¸»è§‚æ€§å’Œä¼¦ç†æ¨¡ç³Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.21175">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-a5c98ca08ecbf9261ebd69d177c7ef3a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684606&auth_key=1760684606-0-0-baa4108f4f442926b6879901ea84458a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c3f0f8c1b7362ea30aadd66cb3249426~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684613&auth_key=1760684613-0-0-81272877c87f8f69042e2ac8ef4dedde&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Fine-Tuning-LLMs-to-Analyze-Multiple-Dimensions-of-Code-Review-A-Maximum-Entropy-Regulated-Long-Chain-of-Thought-Approach"><a href="#Fine-Tuning-LLMs-to-Analyze-Multiple-Dimensions-of-Code-Review-A-Maximum-Entropy-Regulated-Long-Chain-of-Thought-Approach" class="headerlink" title="Fine-Tuning LLMs to Analyze Multiple Dimensions of Code Review: A   Maximum Entropy Regulated Long Chain-of-Thought Approach"></a>Fine-Tuning LLMs to Analyze Multiple Dimensions of Code Review: A   Maximum Entropy Regulated Long Chain-of-Thought Approach</h2><p><strong>Authors:Yongda Yu, Guohao Shi, Xianwei Wu, Haochuan He, XueMing Gu, Qianqian Zhao, Kui Liu, Qiushi Wang, Zhao Tian, Haifeng Shen, Guoping Rong</strong></p>
<p>Large Language Models (LLMs) have shown great potential in supporting automated code review due to their impressive capabilities in context understanding and reasoning. However, these capabilities are still limited compared to human-level cognition because they are heavily influenced by the training data. Recent research has demonstrated significantly improved performance through fine-tuning LLMs with code review data. However, compared to human reviewers who often simultaneously analyze multiple dimensions of code review to better identify issues, the full potential of these methods is hampered by the limited or vague information used to fine-tune the models. This paper contributes MelcotCR, a chain-of-thought (COT) fine-tuning approach that trains LLMs with an impressive reasoning ability to analyze multiple dimensions of code review by harnessing long COT techniques to provide rich structured information. To address context loss and reasoning logic loss issues that frequently occur when LLMs process long COT prompts, we propose a solution that combines the Maximum Entropy (ME) modeling principle with pre-defined reasoning pathways in MelcotCR to enable more effective utilization of in-context knowledge within long COT prompts while strengthening the logical tightness of the reasoning process. Empirical evaluations on our curated MelcotCR dataset and the public CodeReviewer dataset reveal that a low-parameter base model, such as 14B Qwen2.5, fine-tuned with MelcotCR can surpass state-of-the-art methods in terms of the accuracy of detecting and describing code issues, with its performance remarkably on par with that of the 671B DeepSeek-R1 model. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ”¯æŒè‡ªåŠ¨åŒ–ä»£ç å®¡æŸ¥æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ï¼Œå®ƒä»¬åœ¨ä¸Šä¸‹æ–‡ç†è§£å’Œæ¨ç†æ–¹é¢çš„èƒ½åŠ›ä»¤äººå°è±¡æ·±åˆ»ã€‚ç„¶è€Œï¼Œä¸äººç±»æ°´å¹³çš„è®¤çŸ¥ç›¸æ¯”ï¼Œè¿™äº›èƒ½åŠ›ä»ç„¶æœ‰é™ï¼Œå› ä¸ºå®ƒä»¬å—åˆ°è®­ç»ƒæ•°æ®çš„å¤§é‡å½±å“ã€‚æœ€è¿‘æœ‰ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡ç”¨ä»£ç å®¡æŸ¥æ•°æ®å¾®è°ƒLLMï¼Œå¯ä»¥æ˜¾è‘—æé«˜æ€§èƒ½ã€‚ç„¶è€Œï¼Œä¸äººç±»è¯„å®¡è€…é€šå¸¸åŒæ—¶åˆ†æä»£ç å®¡æŸ¥çš„å¤šä¸ªç»´åº¦ä»¥æ›´å¥½åœ°å‘ç°é—®é¢˜ç›¸æ¯”ï¼Œè¿™äº›æ–¹æ³•çš„å…¨æ½œå—åˆ°äº†ç”¨äºå¾®è°ƒæ¨¡å‹çš„æœ‰é™æˆ–æ¨¡ç³Šä¿¡æ¯çš„é˜»ç¢ã€‚æœ¬æ–‡æå‡ºäº†MelcotCRï¼Œè¿™æ˜¯ä¸€ç§æ€ç»´é“¾ï¼ˆCOTï¼‰å¾®è°ƒæ–¹æ³•ï¼Œå®ƒé€šè¿‡è¿ç”¨ä»¤äººå°è±¡æ·±åˆ»çš„æ¨ç†èƒ½åŠ›è®­ç»ƒLLMï¼Œé€šè¿‡åˆ©ç”¨é•¿COTæŠ€æœ¯æ¥åˆ†æä»£ç å®¡æŸ¥çš„å¤šä¸ªç»´åº¦ï¼Œæä¾›ä¸°å¯Œçš„ç»“æ„åŒ–ä¿¡æ¯ã€‚ä¸ºäº†è§£å†³LLMå¤„ç†é•¿COTæç¤ºæ—¶ç»å¸¸å‡ºç°çš„ä¸Šä¸‹æ–‡ä¸¢å¤±å’Œæ¨ç†é€»è¾‘ä¸¢å¤±é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»“åˆæœ€å¤§ç†µï¼ˆMEï¼‰å»ºæ¨¡åŸç†å’ŒMelcotCRä¸­é¢„å…ˆå®šä¹‰çš„æ¨ç†é€”å¾„çš„è§£å†³æ–¹æ¡ˆï¼Œä»¥ä¾¿æ›´æœ‰æ•ˆåœ°åˆ©ç”¨é•¿COTæç¤ºä¸­çš„ä¸Šä¸‹æ–‡çŸ¥è¯†ï¼ŒåŒæ—¶åŠ å¼ºæ¨ç†è¿‡ç¨‹çš„é€»è¾‘ä¸¥è°¨æ€§ã€‚åœ¨æˆ‘ä»¬ç²¾é€‰çš„MelcotCRæ•°æ®é›†å’Œå…¬å…±CodeRevieweræ•°æ®é›†ä¸Šçš„ç»éªŒè¯„ä¼°è¡¨æ˜ï¼Œä½¿ç”¨MelcotCRå¾®è°ƒçš„ä½å‚æ•°åŸºç¡€æ¨¡å‹ï¼Œå¦‚14B Qwen2.5ï¼Œåœ¨æ£€æµ‹æè¿°ä»£ç é—®é¢˜æ–¹é¢çš„å‡†ç¡®æ€§å¯ä»¥è¶…è¶Šæœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå…¶æ€§èƒ½ä¸671B DeepSeek-R1æ¨¡å‹ç›¸å½“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.21170v1">PDF</a> 22 pages</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä»£ç å®¡æŸ¥è‡ªåŠ¨åŒ–æ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œå…¶ä¸Šä¸‹æ–‡ç†è§£å’Œæ¨ç†èƒ½åŠ›ä»¤äººå°è±¡æ·±åˆ»ã€‚ç„¶è€Œï¼Œå—é™äºè®­ç»ƒæ•°æ®ï¼Œå…¶æ€§èƒ½ä»ä½äºäººç±»è®¤çŸ¥ã€‚æœ€æ–°ç ”ç©¶é€šè¿‡ç”¨ä»£ç å®¡æŸ¥æ•°æ®å¾®è°ƒLLMsï¼Œæé«˜äº†å…¶æ€§èƒ½ã€‚ç„¶è€Œï¼Œä¸äººç±»å®¡æŸ¥è€…åŒæ—¶åˆ†æä»£ç çš„å¤šä¸ªç»´åº¦ä»¥æ›´å¥½åœ°å‘ç°é—®é¢˜ç›¸æ¯”ï¼Œè¿™äº›æ–¹æ³•ä»æœªèƒ½å……åˆ†å‘æŒ¥æ½œåŠ›ã€‚æœ¬æ–‡æå‡ºMelcotCRï¼Œä¸€ç§é“¾å¼æ€ç»´ï¼ˆCOTï¼‰å¾®è°ƒæ–¹æ³•ï¼Œé€šè¿‡åˆ©ç”¨é•¿COTæŠ€æœ¯è®­ç»ƒå…·æœ‰å¼ºå¤§æ¨ç†èƒ½åŠ›çš„LLMsæ¥åˆ†æä»£ç å®¡æŸ¥çš„å¤šä¸ªç»´åº¦ã€‚ä¸ºè§£å†³LLMså¤„ç†é•¿COTæç¤ºæ—¶ç»å¸¸å‡ºç°çš„ä¸Šä¸‹æ–‡ä¸¢å¤±å’Œæ¨ç†é€»è¾‘ä¸¢å¤±é—®é¢˜ï¼Œæˆ‘ä»¬ç»“åˆæœ€å¤§ç†µå»ºæ¨¡åŸç†å’ŒMelcotCRä¸­çš„é¢„å®šä¹‰æ¨ç†è·¯å¾„ï¼Œæ›´æœ‰æ•ˆåœ°åˆ©ç”¨é•¿COTæç¤ºä¸­çš„ä¸Šä¸‹æ–‡çŸ¥è¯†ï¼ŒåŒæ—¶åŠ å¼ºæ¨ç†è¿‡ç¨‹çš„é€»è¾‘ä¸¥è°¨æ€§ã€‚åœ¨MelcotCRæ•°æ®é›†å’Œå…¬å…±CodeRevieweræ•°æ®é›†ä¸Šçš„å®è¯è¯„ä¼°è¡¨æ˜ï¼Œä½¿ç”¨MelcotCRå¾®è°ƒçš„ä½å‚æ•°åŸºç¡€æ¨¡å‹ï¼Œå¦‚14B Qwen2.5ï¼Œåœ¨æ£€æµ‹æè¿°ä»£ç é—®é¢˜æ–¹é¢çš„å‡†ç¡®åº¦å¯è¶…è¶Šç°æœ‰æœ€å…ˆè¿›çš„æ¨¡å‹ï¼Œå…¶æ€§èƒ½ä¸671B DeepSeek-R1æ¨¡å‹ç›¸å½“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä»£ç å®¡æŸ¥è‡ªåŠ¨åŒ–æ–¹é¢å…·æœ‰æ½œåŠ›ï¼Œä½†å—é™äºè®­ç»ƒæ•°æ®çš„æ€§èƒ½ã€‚</li>
<li>é€šè¿‡ä½¿ç”¨ä»£ç å®¡æŸ¥æ•°æ®å¾®è°ƒLLMså¯ä»¥æé«˜å…¶æ€§èƒ½ã€‚</li>
<li>MelcotCRæ˜¯ä¸€ç§æ–°çš„å¾®è°ƒæ–¹æ³•ï¼Œèƒ½è®©LLMsåˆ†æä»£ç å®¡æŸ¥çš„å¤šä¸ªç»´åº¦ã€‚</li>
<li>MelcotCRåˆ©ç”¨é•¿COTæŠ€æœ¯æ¥æä¾›ä¸°å¯Œçš„ç»“æ„åŒ–ä¿¡æ¯ã€‚</li>
<li>ä¸ºè§£å†³ä¸Šä¸‹æ–‡å’Œé€»è¾‘ä¸¢å¤±é—®é¢˜ï¼Œç»“åˆæœ€å¤§ç†µå»ºæ¨¡åŸç†å’Œé¢„å®šä¹‰æ¨ç†è·¯å¾„ã€‚</li>
<li>MelcotCRåœ¨æ£€æµ‹ä»£ç é—®é¢˜æ–¹é¢çš„æ€§èƒ½è¶…è¶Šç°æœ‰æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.21170">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-d91a659bb0d76b8ce239b899d9f038f9~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684621&auth_key=1760684621-0-0-338c1e1b0a581079b54776b573abc30a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-39f6d35040b1847f725497b7732d3e8e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684628&auth_key=1760684628-0-0-5a60c08c142d1f24fc6bd41cf0b0cd73&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-10691c9f42e595b211d681dcce870533~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684635&auth_key=1760684635-0-0-6d62f0df333da6962c413140ac5e4903&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Learning-the-Wrong-Lessons-Syntactic-Domain-Spurious-Correlations-in-Language-Models"><a href="#Learning-the-Wrong-Lessons-Syntactic-Domain-Spurious-Correlations-in-Language-Models" class="headerlink" title="Learning the Wrong Lessons: Syntactic-Domain Spurious Correlations in   Language Models"></a>Learning the Wrong Lessons: Syntactic-Domain Spurious Correlations in   Language Models</h2><p><strong>Authors:Chantal Shaib, Vinith M. Suriyakumar, Levent Sagun, Byron C. Wallace, Marzyeh Ghassemi</strong></p>
<p>For an LLM to correctly respond to an instruction it must understand both the semantics and the domain (i.e., subject area) of a given task-instruction pair. However, syntax can also convey implicit information Recent work shows that syntactic templatesâ€“frequent sequences of Part-of-Speech (PoS) tagsâ€“are prevalent in training data and often appear in model outputs. In this work we characterize syntactic templates, domain, and semantics in task-instruction pairs. We identify cases of spurious correlations between syntax and domain, where models learn to associate a domain with syntax during training; this can sometimes override prompt semantics. Using a synthetic training dataset, we find that the syntactic-domain correlation can lower performance (mean 0.51 +&#x2F;- 0.06) on entity knowledge tasks in OLMo-2 models (1B-13B). We introduce an evaluation framework to detect this phenomenon in trained models, and show that it occurs on a subset of the FlanV2 dataset in open (OLMo-2-7B; Llama-4-Maverick), and closed (GPT-4o) models. Finally, we present a case study on the implications for safety finetuning, showing that unintended syntactic-domain correlations can be used to bypass refusals in OLMo-2-7B Instruct and GPT-4o. Our findings highlight two needs: (1) to explicitly test for syntactic-domain correlations, and (2) to ensure syntactic diversity in training data, specifically within domains, to prevent such spurious correlations. </p>
<blockquote>
<p>å¯¹äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ­£ç¡®å“åº”æŒ‡ä»¤ï¼Œå®ƒå¿…é¡»ç†è§£ç»™å®šä»»åŠ¡æŒ‡ä»¤å¯¹ä¸­çš„è¯­ä¹‰å’Œé¢†åŸŸï¼ˆå³ä¸»é¢˜é¢†åŸŸï¼‰ã€‚ç„¶è€Œï¼Œè¯­æ³•ä¹Ÿèƒ½ä¼ è¾¾éšå«ä¿¡æ¯ã€‚æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œè¯­æ³•æ¨¡æ¿â€”â€”è¯æ€§ï¼ˆPart-of-Speechï¼ŒPoSï¼‰æ ‡ç­¾çš„é¢‘ç¹åºåˆ—ï¼Œåœ¨è®­ç»ƒæ•°æ®ä¸­æ™®éå­˜åœ¨ï¼Œå¹¶ä¸”ç»å¸¸å‡ºç°åœ¨æ¨¡å‹è¾“å‡ºä¸­ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¯¹ä»»åŠ¡æŒ‡ä»¤å¯¹ä¸­çš„è¯­æ³•æ¨¡æ¿ã€é¢†åŸŸå’Œè¯­ä¹‰è¿›è¡Œäº†ç‰¹å¾æè¿°ã€‚æˆ‘ä»¬ç¡®å®šäº†è¯­æ³•å’Œé¢†åŸŸä¹‹é—´çš„è™šå‡å…³è”æƒ…å†µï¼Œæ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å­¦ä¼šäº†å°†é¢†åŸŸä¸è¯­æ³•ç›¸å…³è”ï¼›è¿™æœ‰æ—¶ä¼šè¦†ç›–æŒ‡ä»¤çš„è¯­ä¹‰ã€‚æˆ‘ä»¬ä½¿ç”¨åˆæˆè®­ç»ƒæ•°æ®é›†å‘ç°ï¼Œåœ¨OLMo-2æ¨¡å‹ï¼ˆ1B-13Bï¼‰çš„å®ä½“çŸ¥è¯†ä»»åŠ¡ä¸Šï¼Œè¯­æ³•é¢†åŸŸç›¸å…³æ€§ä¼šé™ä½æ€§èƒ½ï¼ˆå¹³å‡å€¼Â±æ ‡å‡†å·®ä¸º0.51Â±0.06ï¼‰ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªè¯„ä¼°æ¡†æ¶æ¥æ£€æµ‹è®­ç»ƒæ¨¡å‹ä¸­çš„è¿™ç§ç°è±¡ï¼Œå¹¶æ˜¾ç¤ºå®ƒå‘ç”Ÿåœ¨å¼€æ”¾ï¼ˆOLMo-2-7Bï¼›Llama-4-Maverickï¼‰å’Œå°é—­ï¼ˆGPT-4oï¼‰æ¨¡å‹çš„FlanV2æ•°æ®é›†çš„å­é›†ä¸Šã€‚æœ€åï¼Œæˆ‘ä»¬å¯¹å®‰å…¨å¾®è°ƒçš„å½±å“è¿›è¡Œäº†æ¡ˆä¾‹ç ”ç©¶ï¼Œè¡¨æ˜æ— æ„ä¸­çš„è¯­æ³•é¢†åŸŸç›¸å…³æ€§å¯ç”¨äºç»•è¿‡OLMo-2-7BæŒ‡ä»¤å’ŒGPT-4oçš„æ‹’ç»ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœå¼ºè°ƒäº†ä¸¤ä¸ªéœ€æ±‚ï¼šï¼ˆ1ï¼‰æ˜ç¡®æµ‹è¯•è¯­æ³•é¢†åŸŸç›¸å…³æ€§ï¼›ï¼ˆ2ï¼‰ç¡®ä¿è®­ç»ƒæ•°æ®ä¸­çš„è¯­æ³•å¤šæ ·æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨é¢†åŸŸå†…ï¼Œä»¥é˜²æ­¢è¿™ç§è™šå‡çš„ç›¸å…³æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.21155v1">PDF</a> NeurIPS 2025 Spotlight</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤„ç†ä»»åŠ¡æŒ‡ä»¤å¯¹æ—¶ï¼Œéœ€è¦åŒæ—¶ç†è§£è¯­ä¹‰å’Œç‰¹å®šé¢†åŸŸçš„å±€é™æ€§ã€‚è™½ç„¶å¥æ³•èƒ½ä¼ é€’éšå«ä¿¡æ¯ï¼Œä½†è¯­æ³•æ¨¡æ¿çš„å­˜åœ¨ä½¿å¾—åœ¨æŸäº›æƒ…å†µä¸‹æ¨¡å‹ä¼šå°†ç‰¹å®šçš„é¢†åŸŸä¸å¥æ³•ç»“æ„è”ç³»èµ·æ¥ï¼Œè€Œå¿½è§†äº†è¯­ä¹‰å«ä¹‰ã€‚ä½œè€…è¯†åˆ«å‡ºåœ¨åˆæˆè®­ç»ƒæ•°æ®é›†ä¸Šçš„æ¨¡å‹ä¸­å­˜åœ¨çš„å¥æ³•-é¢†åŸŸç›¸å…³æ€§å¯èƒ½å¯¹å®ä½“çŸ¥è¯†ä»»åŠ¡çš„æ€§èƒ½äº§ç”Ÿå½±å“çš„é—®é¢˜ï¼Œå¹¶æå‡ºä¸€ç§è¯„ä¼°æ¡†æ¶æ¥æ£€æµ‹å·²è®­ç»ƒæ¨¡å‹ä¸­çš„è¿™ç§ç°è±¡ã€‚åŒæ—¶ä½œè€…ä¹Ÿç ”ç©¶äº†è¿™ä¸€é—®é¢˜çš„å®‰å…¨æ€§å½±å“ï¼Œå±•ç¤ºäº†åˆ©ç”¨é”™è¯¯çš„å¥æ³•-é¢†åŸŸç›¸å…³æ€§æ¥ç»•è¿‡æ‹’ç»å‘½ä»¤çš„æƒ…å†µã€‚æ–‡ç« å¼ºè°ƒäº†æ˜ç¡®æµ‹è¯•è¯­æ³•æ¨¡æ¿é¢†åŸŸå…³è”å’Œç¡®ä¿è®­ç»ƒæ•°æ®ä¸­çš„è¯­æ³•å¤šæ ·æ€§çš„é‡è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨å“åº”æŒ‡ä»¤æ—¶éœ€è¦ç†è§£ä»»åŠ¡æŒ‡ä»¤å¯¹çš„è¯­ä¹‰å’Œé¢†åŸŸã€‚</li>
<li>è¯­æ³•æ¨¡æ¿åœ¨è®­ç»ƒæ•°æ®å’Œæ¨¡å‹è¾“å‡ºä¸­é¢‘ç¹å‡ºç°ï¼Œå¯èƒ½å¯¼è‡´æ¨¡å‹å…³æ³¨å¥æ³•ç»“æ„è€Œå¿½è§†è¯­ä¹‰å«ä¹‰ã€‚</li>
<li>åœ¨åˆæˆè®­ç»ƒæ•°æ®é›†ä¸Šï¼Œè¯­æ³•æ¨¡æ¿å’Œé¢†åŸŸçš„ç›¸å…³æ€§å¯èƒ½é™ä½æ¨¡å‹åœ¨å®ä½“çŸ¥è¯†ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚</li>
<li>æ–‡ç« ä»‹ç»äº†ä¸€ä¸ªè¯„ä¼°æ¡†æ¶ï¼Œç”¨ä»¥æ£€æµ‹è®­ç»ƒæ¨¡å‹ä¸­å­˜åœ¨çš„è¯­æ³•æ¨¡æ¿å’Œé¢†åŸŸå…³è”çš„ç°è±¡ã€‚</li>
<li>è¯¥ç°è±¡åœ¨å®‰å…¨è°ƒæ•´è¿‡ç¨‹ä¸­äº§ç”Ÿå½±å“ï¼Œå¯èƒ½ä¼šç»•è¿‡æŸäº›æ‹’ç»æŒ‡ä»¤çš„æƒ…å†µã€‚</li>
<li>éœ€è¦æ˜ç¡®æµ‹è¯•è¯­æ³•æ¨¡æ¿ä¸é¢†åŸŸä¹‹é—´çš„å…³è”ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.21155">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-0576b6d6f242c114d0f50e6471d62043~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684642&auth_key=1760684642-0-0-9455980b27df5abfaee0ee20fca24319&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1f75f7708172906f44f46c55630f5183~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684650&auth_key=1760684650-0-0-046b9e82bb08a8653e376e0a53ae82c4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5fc09578ef66ddd206b110f842622626~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684657&auth_key=1760684657-0-0-cdd1f4d12b78f2d18732b833d41b1495&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-03a435315489d08cef8ef03e517a4e0c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684663&auth_key=1760684663-0-0-8c182a508cccef97eaaf6f5b593bfb26&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-bc1321ca6a67f3be107c874a9fc91e4f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684670&auth_key=1760684670-0-0-2169a1548840ed53d8b2344708316150&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="GRPO-is-Secretly-a-Process-Reward-Model"><a href="#GRPO-is-Secretly-a-Process-Reward-Model" class="headerlink" title="GRPO is Secretly a Process Reward Model"></a>GRPO is Secretly a Process Reward Model</h2><p><strong>Authors:Michael Sullivan</strong></p>
<p>We prove theoretically that the GRPO RL algorithm induces a non-trivial process reward model (PRM), under certain assumptions regarding within-group overlap of token sequences across completions. We then show empirically that these assumptions are met under real-world conditions: GRPO does in fact induce a non-trivial PRM. Leveraging the framework of GRPO-as-a-PRM, we identify a flaw in the GRPO objective: non-uniformly distributed process steps hinder both exploration and exploitation (under different conditions). We propose a simple modification to the algorithm to mitigate this defect ($\lambda$-GRPO), and show that LLMs trained with $\lambda$-GRPO achieve higher validation accuracy and performance on downstream reasoning tasks$-$and reach peak performance more rapidly$-$than LLMs trained with standard GRPO. Our results call into question the advantage of costly, explicitly-defined PRMs for GRPO: we show that it is possible to instead leverage the hidden, built-in PRM structure within the vanilla GRPO algorithm to boost model performance with a negligible impact on training time and cost. </p>
<blockquote>
<p>æˆ‘ä»¬ç†è®ºä¸Šè¯æ˜äº†ï¼Œåœ¨ä¸€å®šçš„å‡è®¾æ¡ä»¶ä¸‹ï¼Œå…³äºå®Œæˆä»»åŠ¡è¿‡ç¨‹ä¸­æ ‡è®°åºåˆ—ç»„å†…é‡å çš„éƒ¨åˆ†ï¼ŒGRPO RLç®—æ³•ä¼šå¼•å‘ä¸€ä¸ªéå¹³å‡¡çš„è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰ã€‚ç„¶åï¼Œæˆ‘ä»¬é€šè¿‡å®è¯è¡¨æ˜è¿™äº›å‡è®¾åœ¨çœŸå®æ¡ä»¶ä¸‹æ˜¯æˆç«‹çš„ï¼šGRPOç¡®å®ä¼šå¼•å‘ä¸€ä¸ªéå¹³å‡¡çš„è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ã€‚åˆ©ç”¨GRPOä½œä¸ºPRMçš„æ¡†æ¶ï¼Œæˆ‘ä»¬å‘ç°äº†GRPOç›®æ ‡ä¸­çš„ä¸€ä¸ªç¼ºé™·ï¼šè¿‡ç¨‹æ­¥éª¤çš„éå‡åŒ€åˆ†å¸ƒé˜»ç¢äº†æ¢ç´¢å’Œåˆ©ç”¨ï¼ˆåœ¨ä¸åŒçš„æ¡ä»¶ä¸‹ï¼‰ã€‚ä¸ºäº†ç¼“è§£è¿™ä¸€ç¼ºé™·ï¼Œæˆ‘ä»¬å¯¹ç®—æ³•è¿›è¡Œäº†ç®€å•çš„ä¿®æ”¹ï¼ˆÎ»-GRPOï¼‰ï¼Œå¹¶è¯æ˜ä½¿ç”¨Î»-GRPOè®­ç»ƒçš„LLMåœ¨ä¸‹æ¸¸æ¨ç†ä»»åŠ¡ä¸­è¾¾åˆ°äº†æ›´é«˜çš„éªŒè¯ç²¾åº¦å’Œæ€§èƒ½ï¼Œå¹¶ä¸”æ›´å¿«è¾¾åˆ°å³°å€¼æ€§èƒ½ã€‚ä¸ä¼ ç»Ÿçš„GRPOç›¸æ¯”ï¼Œæˆ‘ä»¬åœ¨æˆæœ¬é«˜æ˜‚ã€æ˜ç¡®å®šä¹‰çš„PRMçš„ä¼˜åŠ¿ä¸Šäº§ç”Ÿäº†è´¨ç–‘ï¼šæˆ‘ä»¬å±•ç¤ºäº†å¯ä»¥å……åˆ†åˆ©ç”¨åŸå§‹çš„GRPOç®—æ³•ä¸­éšè—çš„å†…ç½®PRMç»“æ„æ¥æå‡æ¨¡å‹æ€§èƒ½ï¼Œè€Œå¯¹è®­ç»ƒæ—¶é—´å’Œæˆæœ¬çš„å½±å“å¾®ä¹å…¶å¾®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.21154v1">PDF</a> 14 pages, 6 figures; under review at ICLR 2026</p>
<p><strong>Summary</strong></p>
<p>GRPO RLç®—æ³•åœ¨ç‰¹å®šå‡è®¾ä¸‹èƒ½å¼•å‘éå¹³å‡¡çš„è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰ã€‚å®è¯ç ”ç©¶è¡¨æ˜ï¼Œè¿™äº›å‡è®¾åœ¨ç°å®ä¸–ç•Œæ¡ä»¶ä¸‹æ˜¯æˆç«‹çš„ã€‚é€šè¿‡åˆ†æGRPOä½œä¸ºPRMçš„æ¡†æ¶ï¼Œå‘ç°GRPOç›®æ ‡ä¸­å­˜åœ¨ç¼ºé™·ï¼šè¿‡ç¨‹æ­¥éª¤çš„éå‡åŒ€åˆ†å¸ƒé˜»ç¢æ¢ç´¢å’Œåˆ©ç”¨ã€‚æå‡ºå¯¹ç®—æ³•è¿›è¡Œç®€å•ä¿®æ”¹ï¼ˆÎ»-GRPOï¼‰ï¼Œä½¿ç”¨Î»-GRPOè®­ç»ƒçš„è¯­è¨€æ¨¡å‹åœ¨ä¸‹æ¸¸æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°å‡ºæ›´é«˜çš„éªŒè¯ç²¾åº¦å’Œæ€§èƒ½ï¼Œå¹¶èƒ½æ›´å¿«è¾¾åˆ°å³°å€¼æ€§èƒ½ã€‚ç ”ç©¶ç»“æœå¯¹æ˜‚è´µã€æ˜ç¡®å®šä¹‰çš„PRMåœ¨GRPOä¸­çš„ä¼˜åŠ¿æå‡ºè´¨ç–‘ï¼Œæ˜¾ç¤ºåˆ©ç”¨GRPOç®—æ³•ä¸­çš„å†…ç½®PRMç»“æ„æå‡æ¨¡å‹æ€§èƒ½æ˜¯å¯èƒ½çš„ï¼Œä¸”å¯¹è®­ç»ƒæ—¶é—´å’Œæˆæœ¬çš„å½±å“å¾®ä¹å…¶å¾®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GRPO RLç®—æ³•åœ¨ç‰¹å®šå‡è®¾ä¸‹èƒ½æ¿€å‘éå¹³å‡¡çš„è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰ã€‚</li>
<li>å®è¯ç ”ç©¶éªŒè¯è¿™äº›å‡è®¾åœ¨ç°å®ä¸–ç•Œä¸­çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>åˆ†æå‘ç°GRPOç›®æ ‡ä¸­å­˜åœ¨ç¼ºé™·ï¼šè¿‡ç¨‹æ­¥éª¤çš„éå‡åŒ€åˆ†å¸ƒå½±å“æ¢ç´¢å’Œåˆ©ç”¨ã€‚</li>
<li>æå‡ºæ”¹è¿›ç®—æ³•Î»-GRPOï¼Œæœ‰æ•ˆæå‡è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>Î»-GRPOè®­ç»ƒçš„è¯­è¨€æ¨¡å‹åœ¨ä¸‹æ¸¸æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°æ›´ä¼˜ç§€ï¼ŒéªŒè¯ç²¾åº¦æ›´é«˜ï¼Œè¾¾åˆ°å³°å€¼æ€§èƒ½æ›´å¿«ã€‚</li>
<li>ç ”ç©¶ç»“æœå¯¹æ˜ç¡®å®šä¹‰çš„PRMåœ¨GRPOä¸­çš„å¿…è¦æ€§æå‡ºè´¨ç–‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.21154">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-739f3db9cbbc9e2ae9b1f7d0755c19ce~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684678&auth_key=1760684678-0-0-13d87f2bbf1c26e4aaa6817e1b70e82e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a4fe643239d55bff086db53c360b2d37~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684685&auth_key=1760684685-0-0-576db8cca6556e2714623f11682972cf&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c590e3346b3cc22dd19020f6ad7b3979~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684693&auth_key=1760684693-0-0-2a8ed9c3ff9763ebe30df8033f3a3da0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="ToMPO-Training-LLM-Strategic-Decision-Making-from-a-Multi-Agent-Perspective"><a href="#ToMPO-Training-LLM-Strategic-Decision-Making-from-a-Multi-Agent-Perspective" class="headerlink" title="ToMPO: Training LLM Strategic Decision Making from a Multi-Agent   Perspective"></a>ToMPO: Training LLM Strategic Decision Making from a Multi-Agent   Perspective</h2><p><strong>Authors:Yiwen Zhang, Ziang Chen, Fanqi Kong, Yizhe Huang, Xue Feng</strong></p>
<p>Large Language Models (LLMs) have been used to make decisions in complex scenarios, where they need models to think deeply, reason logically, and decide wisely. Many existing studies focus solely on multi-round conversations in social tasks or simulated environments, neglecting the various types of decisions and their interdependence. Current reinforcement learning methods struggle to consider the strategies of others during training. To address these issues, we first define a strategic decision-making problem that includes two types of decisions and their temporal dependencies. Furthermore, we propose <strong>T</strong>heory <strong>o</strong>f <strong>M</strong>ind <strong>P</strong>olicy <strong>O</strong>ptimization <strong>(ToMPO)</strong> algorithm to optimize the perception of other individual strategies and the game situation trends. Compared to the Group Relative Policy Optimization (GRPO) algorithm, ToMPO enhances the LLMâ€™s strategic decision-making mainly by: 1) generating rollouts based on reasoning the strategies of other individuals, 2) estimating advantages at both the graph-level and sample-level, and 3) balancing global and partial rewards. The ToMPO algorithm outperforms the GRPO method by 35% in terms of model output compliance and cooperative outcomes. Additionally, when compared to models with parameter sizes 100 times larger, it shows an 18% improvement. This demonstrates the effectiveness of the ToMPO algorithm in enhancing the modelâ€™s strategic decision-making capabilities. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²åœ¨å¤æ‚åœºæ™¯ä¸­è¢«ç”¨äºå†³ç­–ï¼Œè¿™äº›åœºæ™¯éœ€è¦æ¨¡å‹è¿›è¡Œæ·±å…¥æ€è€ƒã€é€»è¾‘æ¨ç†å’Œæ˜æ™ºå†³ç­–ã€‚ç°æœ‰çš„è®¸å¤šç ”ç©¶ä»…ä¸“æ³¨äºç¤¾ä¼šä»»åŠ¡æˆ–æ¨¡æ‹Ÿç¯å¢ƒä¸­çš„å¤šè½®å¯¹è¯ï¼Œå¿½è§†äº†å„ç§ç±»å‹çš„å†³ç­–åŠå…¶ç›¸äº’ä¾èµ–æ€§ã€‚å½“å‰çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¾ˆéš¾è€ƒè™‘ä»–äººçš„ç­–ç•¥ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬é¦–å…ˆå®šä¹‰äº†ä¸€ä¸ªæˆ˜ç•¥å†³ç­–é—®é¢˜ï¼ŒåŒ…æ‹¬ä¸¤ç§ç±»å‹çš„å†³ç­–å’Œå®ƒä»¬çš„æ—¶é—´ä¾èµ–æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†<strong>T</strong>ç†è®º<strong>o</strong>çš„<strong>M</strong>ç­–ç•¥ä¼˜åŒ–æ€æƒ³**(ToMPO)**ç®—æ³•ï¼Œä»¥ä¼˜åŒ–å¯¹å…¶ä»–ä¸ªä½“ç­–ç•¥å’Œæ¸¸æˆå±€åŠ¿è¶‹åŠ¿çš„æ„ŸçŸ¥ã€‚ä¸ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ç®—æ³•ç›¸æ¯”ï¼ŒToMPOä¸»è¦é€šè¿‡ä»¥ä¸‹æ–¹å¼æé«˜LLMçš„æˆ˜ç•¥å†³ç­–èƒ½åŠ›ï¼š1ï¼‰åŸºäºæ¨ç†å…¶ä»–ä¸ªä½“çš„ç­–ç•¥ç”Ÿæˆæ¨æ¼”ç»“æœï¼Œ2ï¼‰åœ¨å›¾å½¢çº§åˆ«å’Œæ ·æœ¬çº§åˆ«ä¸Šä¼°è®¡ä¼˜åŠ¿ï¼Œä»¥åŠ3ï¼‰å¹³è¡¡å…¨å±€å’Œå±€éƒ¨å¥–åŠ±ã€‚åœ¨æ¨¡å‹è¾“å‡ºç¬¦åˆåº¦å’Œåˆä½œç»“æœæ–¹é¢ï¼ŒToMPOç®—æ³•çš„æ€§èƒ½æ¯”GRPOæ–¹æ³•é«˜å‡º35%ã€‚æ­¤å¤–ï¼Œä¸å‚æ•°è§„æ¨¡å¤§100å€çš„æ¨¡å‹ç›¸æ¯”ï¼Œå®ƒæ˜¾ç¤ºå‡º18%çš„æ”¹è¿›ã€‚è¿™è¯æ˜äº†ToMPOç®—æ³•åœ¨æé«˜æ¨¡å‹æˆ˜ç•¥å†³ç­–èƒ½åŠ›æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.21134v1">PDF</a> 22 pages, 14 figures</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤æ‚åœºæ™¯ä¸­çš„å†³ç­–èƒ½åŠ›å—åˆ°å¹¿æ³›å…³æ³¨ã€‚ç°æœ‰ç ”ç©¶å¤šä¾§é‡äºç¤¾ä¼šä»»åŠ¡æˆ–æ¨¡æ‹Ÿç¯å¢ƒä¸­çš„å¤šè½®å¯¹è¯ï¼Œå¿½ç•¥äº†ä¸åŒç±»å‹å†³ç­–åŠå…¶ç›¸äº’ä¾èµ–æ€§ã€‚é’ˆå¯¹è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†æˆ˜ç•¥å†³ç­–åˆ¶å®šçš„é—®é¢˜å®šä¹‰ï¼ŒåŒ…æ‹¬ä¸¤ç§ç±»å‹çš„å†³ç­–åŠå…¶æ—¶é—´ä¾èµ–æ€§ã€‚åŒæ—¶ï¼Œæå‡ºäº†ç†è®ºä¼˜åŒ–ç®—æ³•ï¼ˆToMPOï¼‰ï¼Œè¯¥ç®—æ³•ä¸»è¦ä¼˜åŒ–äº†å¯¹å…¶ä»–ä¸ªä½“ç­–ç•¥å’Œæ¸¸æˆå±€åŠ¿è¶‹åŠ¿çš„æ„ŸçŸ¥ã€‚ç›¸è¾ƒäºç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ç®—æ³•ï¼ŒToMPOåœ¨æˆ˜ç•¥å†³ç­–åˆ¶å®šæ–¹é¢å¢å¼ºäº†LLMçš„èƒ½åŠ›ï¼Œé€šè¿‡åŸºäºå…¶ä»–ä¸ªä½“ç­–ç•¥çš„æ¨ç†ç”Ÿæˆæ»šåŠ¨é¢„æµ‹ã€åœ¨å›¾å½¢å’Œæ ·æœ¬å±‚é¢ä¼°ç®—ä¼˜åŠ¿ä»¥åŠå¹³è¡¡å…¨å±€å’Œå±€éƒ¨å¥–åŠ±ç­‰æ–¹æ³•ã€‚å®éªŒæ˜¾ç¤ºï¼ŒToMPOç®—æ³•åœ¨æ¨¡å‹è¾“å‡ºç¬¦åˆåº¦å’Œåˆä½œç»“æœæ–¹é¢æ¯”GRPOæ–¹æ³•é«˜å‡º35%ï¼Œå¹¶ä¸”åœ¨ä¸å‚æ•°è§„æ¨¡å¤§100å€çš„æ¨¡å‹å¯¹æ¯”ä¸­ä»è¡¨ç°å‡º18%çš„æå‡ã€‚è¿™è¯æ˜äº†ToMPOç®—æ³•åœ¨æå‡æ¨¡å‹æˆ˜ç•¥å†³ç­–èƒ½åŠ›æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤æ‚åœºæ™¯ä¸­çš„å†³ç­–èƒ½åŠ›å—åˆ°ç ”ç©¶å…³æ³¨ã€‚</li>
<li>ç°æœ‰ç ”ç©¶å¤šä¾§é‡äºç¤¾ä¼šä»»åŠ¡æˆ–æ¨¡æ‹Ÿç¯å¢ƒä¸­çš„å¤šè½®å¯¹è¯ï¼Œç¼ºä¹å¯¹ä¸åŒç±»å‹å†³ç­–åŠå…¶ç›¸äº’ä¾èµ–æ€§çš„ç ”ç©¶ã€‚</li>
<li>æå‡ºäº†æˆ˜ç•¥å†³ç­–åˆ¶å®šçš„é—®é¢˜å®šä¹‰ï¼Œæ¶‰åŠä¸¤ç§ç±»å‹çš„å†³ç­–åŠå…¶æ—¶é—´ä¾èµ–æ€§ã€‚</li>
<li>ä»‹ç»äº†ç†è®ºä¼˜åŒ–ç®—æ³•ï¼ˆToMPOï¼‰ï¼Œè¯¥ç®—æ³•ä¼˜åŒ–äº†LLMå¯¹å…¶ä»–ä¸ªä½“ç­–ç•¥å’Œæ¸¸æˆå±€åŠ¿è¶‹åŠ¿çš„æ„ŸçŸ¥èƒ½åŠ›ã€‚</li>
<li>ToMPOé€šè¿‡åŸºäºå…¶ä»–ä¸ªä½“ç­–ç•¥çš„æ¨ç†ç”Ÿæˆæ»šåŠ¨é¢„æµ‹ã€åœ¨å›¾å½¢å’Œæ ·æœ¬å±‚é¢ä¼°ç®—ä¼˜åŠ¿ç­‰æ–¹æ³•ï¼Œå¢å¼ºäº†LLMçš„æˆ˜ç•¥å†³ç­–èƒ½åŠ›ã€‚</li>
<li>ToMPOç®—æ³•åœ¨æ¨¡å‹è¾“å‡ºç¬¦åˆåº¦å’Œåˆä½œç»“æœæ–¹é¢è¡¨ç°å‡ºè¾ƒé«˜çš„æ€§èƒ½ï¼Œç›¸è¾ƒäºå…¶ä»–ç®—æ³•æœ‰æ˜æ˜¾çš„ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.21134">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-674b984e17296e33db2b16ba758eb09d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684700&auth_key=1760684700-0-0-bc3ad64771aa8b2fbed96b1b28da7c90&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-52bd13bf0cbe16dfd12bf59b1eaf68fe~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684708&auth_key=1760684708-0-0-14d1cba7ce9d712b480847c1cdde2d53&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-72210afc4dc35a39691f4d859382e28d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684714&auth_key=1760684714-0-0-62e47831731bac9b3ccfe8a3ef2949de&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1d89946a1fd385d6aebeda5862f84f78~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684721&auth_key=1760684721-0-0-ec43f1bcde5811702922dbb870db6e7a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="RL-Squeezes-SFT-Expands-A-Comparative-Study-of-Reasoning-LLMs"><a href="#RL-Squeezes-SFT-Expands-A-Comparative-Study-of-Reasoning-LLMs" class="headerlink" title="RL Squeezes, SFT Expands: A Comparative Study of Reasoning LLMs"></a>RL Squeezes, SFT Expands: A Comparative Study of Reasoning LLMs</h2><p><strong>Authors:Kohsei Matsutani, Shota Takashiro, Gouki Minegishi, Takeshi Kojima, Yusuke Iwasawa, Yutaka Matsuo</strong></p>
<p>Large language models (LLMs) are typically trained by reinforcement learning (RL) with verifiable rewards (RLVR) and supervised fine-tuning (SFT) on reasoning traces to improve their reasoning abilities. However, how these methods shape reasoning capabilities remains largely elusive. Going beyond an accuracy-based investigation of how these two components sculpt the reasoning process, this paper introduces a novel analysis framework that quantifies reasoning paths and captures their qualitative changes under each training process (with models of 1.5B, 7B, and 14B parameters on mathematical domains). Specifically, we investigate the reasoning process at two levels of granularity: the trajectory-level, which examines complete reasoning outputs, and the step-level, which analyzes reasoning graphs whose nodes correspond to individual reasoning steps. Notably, clustering of unique reasoning trajectories shows complementary effects: RL compresses incorrect trajectories, whereas SFT expands correct ones. Step-level analysis reveals that RL steepens (about 2.5 times), while SFT flattens (reduced to about one-third), the decay rates of node visitation frequency, degree, and betweenness centrality distributions in the reasoning graph. This indicates that RL concentrates reasoning functionality into a small subset of steps, while SFT homogenizes it across many steps. Furthermore, by evaluating the reasoning graph topologies from multiple perspectives, we delineate the shared and distinct characteristics of RL and SFT. Our work presents a novel reasoning path perspective that explains why the current best practice of two-stage training, with SFT followed by RL, is successful, and offers practical implications for data construction and more efficient learning approaches. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šå¸¸é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å’Œå¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰ä»¥åŠåŸºäºæ¨ç†è½¨è¿¹çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰è¿›è¡Œè®­ç»ƒï¼Œä»¥æé«˜å…¶æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å¦‚ä½•å¡‘é€ æ¨ç†èƒ½åŠ›ä»ç„¶å¾ˆå¤§ç¨‹åº¦ä¸Šä¸å¾—è€ŒçŸ¥ã€‚æœ¬æ–‡ä¸ä»…ä»å‡†ç¡®æ€§çš„è§’åº¦ç ”ç©¶è¿™ä¸¤ç§æˆåˆ†å¦‚ä½•å¡‘é€ æ¨ç†è¿‡ç¨‹ï¼Œè¿˜å¼•å…¥äº†ä¸€ä¸ªæ–°å‹åˆ†ææ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯ä»¥é‡åŒ–æ¨ç†è·¯å¾„å¹¶æ•è·æ¯ç§è®­ç»ƒè¿‡ç¨‹ä¸­æ¨ç†è·¯å¾„çš„å®šæ€§å˜åŒ–ï¼ˆé’ˆå¯¹æ•°å­¦é¢†åŸŸçš„1.5Bã€7Bå’Œ14Bå‚æ•°æ¨¡å‹ï¼‰ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä»ä¸¤ä¸ªç²’åº¦çº§åˆ«ç ”ç©¶æ¨ç†è¿‡ç¨‹ï¼šè½¨è¿¹çº§åˆ«ï¼Œå®ƒæ£€æŸ¥å®Œæ•´çš„æ¨ç†è¾“å‡ºï¼›æ­¥éª¤çº§åˆ«ï¼Œå®ƒåˆ†ææ¨ç†å›¾ï¼Œå…¶ä¸­èŠ‚ç‚¹å¯¹åº”äºå•ä¸ªæ¨ç†æ­¥éª¤ã€‚ç‹¬ç‰¹çš„æ¨ç†è½¨è¿¹èšç±»æ˜¾ç¤ºå‡ºäº’è¡¥æ•ˆåº”ï¼šå¼ºåŒ–å­¦ä¹ å‹ç¼©äº†é”™è¯¯çš„è½¨è¿¹ï¼Œè€Œç›‘ç£å¾®è°ƒåˆ™æ‰©å±•äº†æ­£ç¡®çš„è½¨è¿¹ã€‚æ­¥éª¤çº§åˆ†æè¡¨æ˜ï¼Œå¼ºåŒ–å­¦ä¹ ä½¿èŠ‚ç‚¹è®¿é—®é¢‘ç‡ã€åº¦æ•°å’Œä¸­é—´ä¸­å¿ƒåˆ†å¸ƒè¡°å‡ç‡æ€¥å‰§ä¸Šå‡ï¼ˆçº¦2.5å€ï¼‰ï¼Œè€Œç›‘ç£å¾®è°ƒåˆ™ä½¿å…¶å¹³ç¼“ä¸‹é™ï¼ˆé™è‡³çº¦ä¸‰åˆ†ä¹‹ä¸€ï¼‰ã€‚è¿™è¡¨æ˜å¼ºåŒ–å­¦ä¹ å°†æ¨ç†åŠŸèƒ½é›†ä¸­åœ¨å°‘æ•°å‡ ä¸ªæ­¥éª¤ä¸­ï¼Œè€Œç›‘ç£å¾®è°ƒåˆ™å°†å…¶å‡åŒ€åˆ†å¸ƒåˆ°å¤šä¸ªæ­¥éª¤ä¸­ã€‚æ­¤å¤–ï¼Œé€šè¿‡ä»å¤šä¸ªè§’åº¦è¯„ä¼°æ¨ç†å›¾æ‹“æ‰‘ç»“æ„ï¼Œæˆ‘ä»¬é˜è¿°äº†å¼ºåŒ–å­¦ä¹ å’Œç›‘ç£è°ƒå…±äº«çš„ç‰¹æ€§å’Œç‹¬ç‰¹ä¹‹å¤„ã€‚æˆ‘ä»¬çš„å·¥ä½œä»æ–°çš„æ¨ç†è·¯å¾„è§’åº¦è§£é‡Šäº†ä¸ºä»€ä¹ˆå½“å‰æœ€ä½³å®è·µçš„ä¸¤é˜¶æ®µè®­ç»ƒï¼ˆå…ˆç›‘ç£å¾®è°ƒåå¼ºåŒ–å­¦ä¹ ï¼‰æ˜¯æˆåŠŸçš„ï¼Œå¹¶ä¸ºæ•°æ®æ„å»ºå’Œæ›´æœ‰æ•ˆçš„å­¦ä¹ æ–¹æ³•æä¾›äº†å®é™…å¯ç¤ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.21128v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å’Œå¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰ä»¥åŠç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰è¿›è¡Œè®­ç»ƒï¼Œä»¥æé«˜å…¶æ¨ç†èƒ½åŠ›ã€‚æœ¬æ–‡å¼•å…¥äº†ä¸€ä¸ªæ–°å‹åˆ†ææ¡†æ¶ï¼Œé‡åŒ–æ¨ç†è·¯å¾„å¹¶æ•æ‰æ¯ç§è®­ç»ƒè¿‡ç¨‹ä¸­çš„å®šæ€§å˜åŒ–ã€‚ç ”ç©¶å‘ç°ï¼Œå¼ºåŒ–å­¦ä¹ èƒ½å‹ç¼©é”™è¯¯æ¨ç†è½¨è¿¹ï¼Œè€Œç›‘ç£å¾®è°ƒåˆ™æ‰©å±•æ­£ç¡®è½¨è¿¹ã€‚æ­¤å¤–ï¼Œå¼ºåŒ–å­¦ä¹ ä¼šåŠ å‰§èŠ‚ç‚¹è®¿é—®é¢‘ç‡ã€åº¦æ•°å’Œä¸­ä»‹ä¸­å¿ƒæ€§åˆ†å¸ƒçš„è¡°å‡ç‡ï¼Œè€Œç›‘ç£å¾®è°ƒåˆ™ä¼šå‡ç¼“è¿™ä¸€è¶‹åŠ¿ã€‚æœ¬ç ”ç©¶ä»æ¨ç†è·¯å¾„è§’åº¦è§£é‡Šäº†ä¸ºä½•å½“å‰æœ€ä½³å®è·µæ˜¯ä¸¤é˜¶æ®µè®­ç»ƒï¼ˆå…ˆSFTåRLï¼‰ï¼Œå¹¶ä¸ºæ•°æ®æ„å»ºå’Œæ›´é«˜æ•ˆçš„å­¦ä¹ æ–¹æ³•æä¾›äº†å®è·µå¯ç¤ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹é€šè¿‡å¼ºåŒ–å­¦ä¹ å’Œç›‘ç£å¾®è°ƒæé«˜æ¨ç†èƒ½åŠ›ã€‚</li>
<li>æ–°å‹åˆ†ææ¡†æ¶ç”¨äºé‡åŒ–æ¨ç†è·¯å¾„å¹¶æ•æ‰è®­ç»ƒè¿‡ç¨‹ä¸­çš„å˜åŒ–ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ å‹ç¼©é”™è¯¯æ¨ç†è½¨è¿¹ï¼Œç›‘ç£å¾®è°ƒæ‰©å±•æ­£ç¡®è½¨è¿¹ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ ä¼šåŠ å‰§èŠ‚ç‚¹è®¿é—®é¢‘ç‡ç­‰åˆ†å¸ƒçš„è¡°å‡ç‡ï¼Œè€Œç›‘ç£å¾®è°ƒåˆ™å‡ç¼“è¿™ä¸€è¶‹åŠ¿ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ å°†æ¨ç†åŠŸèƒ½é›†ä¸­åœ¨å°‘æ•°æ­¥éª¤ä¸Šï¼Œè€Œç›‘ç£å¾®è°ƒåˆ™ä½¿å…¶å‡åŒ€åˆ†å¸ƒã€‚</li>
<li>ä»æ¨ç†è·¯å¾„è§’åº¦è§£é‡Šäº†ä¸ºä½•ä¸¤é˜¶æ®µè®­ç»ƒï¼ˆå…ˆSFTåRLï¼‰æ˜¯æˆåŠŸçš„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.21128">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-c4ee240f6320cd072c732ed0a32c37c3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684729&auth_key=1760684729-0-0-16f0edbcfd901b4de82507feef353cbb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e4737d11bc0384942fcc3e2cffbf6fbc~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684737&auth_key=1760684737-0-0-16b6e117af6239073818a5f727a000fb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-619a2f4f1d015d94ee68e36463495ab7~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684743&auth_key=1760684743-0-0-7c63f6b1b81c7e6f7041d4d4a9a6defa&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4127445a80576dee1789236c7b863154~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684750&auth_key=1760684750-0-0-650194c2ef298695bf0d0a3b992334ef&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-382880b89a38c96eb61adbdc48e1a136~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684757&auth_key=1760684757-0-0-30ba35e658b80642f97c2f73e9112dc6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="TrustJudge-Inconsistencies-of-LLM-as-a-Judge-and-How-to-Alleviate-Them"><a href="#TrustJudge-Inconsistencies-of-LLM-as-a-Judge-and-How-to-Alleviate-Them" class="headerlink" title="TrustJudge: Inconsistencies of LLM-as-a-Judge and How to Alleviate Them"></a>TrustJudge: Inconsistencies of LLM-as-a-Judge and How to Alleviate Them</h2><p><strong>Authors:Yidong Wang, Yunze Song, Tingyuan Zhu, Xuanwang Zhang, Zhuohao Yu, Hao Chen, Chiyu Song, Qiufeng Wang, Cunxiang Wang, Zhen Wu, Xinyu Dai, Yue Zhang, Wei Ye, Shikun Zhang</strong></p>
<p>The adoption of Large Language Models (LLMs) as automated evaluators (LLM-as-a-judge) has revealed critical inconsistencies in current evaluation frameworks. We identify two fundamental types of inconsistencies: (1) Score-Comparison Inconsistency, where lower-rated responses outperform higher-scored ones in pairwise comparisons, and (2) Pairwise Transitivity Inconsistency, manifested through circular preference chains (A&gt;B&gt;C&gt;A) and equivalence contradictions (A&#x3D;B&#x3D;C\neq A). We argue that these issues come from information loss in discrete rating systems and ambiguous tie judgments during pairwise evaluation. We propose TrustJudge, a probabilistic framework that addresses these limitations through two key innovations: 1) distribution-sensitive scoring that computes continuous expectations from discrete rating probabilities, preserving information entropy for more precise scoring, and 2) likelihood-aware aggregation that resolves transitivity violations using bidirectional preference probabilities or perplexity. We also formalize the theoretical limitations of current LLM-as-a-judge frameworks and demonstrate how TrustJudgeâ€™s components overcome them. When evaluated with Llama-3.1-70B-Instruct as judge using our dataset, TrustJudge reduces Score-Comparison inconsistency by 8.43% (from 23.32% to 14.89%) and Pairwise Transitivity inconsistency by 10.82% (from 15.22% to 4.40%), while maintaining higher evaluation accuracy. Our work provides the first systematic analysis of evaluation framework inconsistencies in LLM-as-a-judge paradigms, offering both theoretical insights and practical solutions for reliable automated assessment. The framework demonstrates consistent improvements across various model architectures and scales, enabling more trustworthy LLM evaluation without requiring additional training or human annotations. The codes can be found at <a target="_blank" rel="noopener" href="https://github.com/TrustJudge/TrustJudge">https://github.com/TrustJudge/TrustJudge</a>. </p>
<blockquote>
<p>é‡‡ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºè‡ªåŠ¨è¯„ä¼°å™¨ï¼ˆLLM-as-a-judgeï¼‰å·²ç»æ­ç¤ºäº†å½“å‰è¯„ä¼°æ¡†æ¶ä¸­çš„å…³é”®ä¸ä¸€è‡´ä¹‹å¤„ã€‚æˆ‘ä»¬è¯†åˆ«å‡ºä¸¤ç§åŸºæœ¬ç±»å‹çš„ä¸ä¸€è‡´æ€§ï¼šï¼ˆ1ï¼‰è¯„åˆ†æ¯”è¾ƒä¸ä¸€è‡´æ€§ï¼Œå…¶ä¸­ä½è¯„çº§çš„å›åº”åœ¨é…å¯¹æ¯”è¾ƒä¸­è¡¨ç°ä¼˜äºé«˜è¯„çº§çš„å›åº”ï¼›ï¼ˆ2ï¼‰é…å¯¹ä¼ é€’æ€§ä¸ä¸€è‡´æ€§ï¼Œè¡¨ç°ä¸ºå¾ªç¯åå¥½é“¾ï¼ˆA&gt;B&gt;C&gt;Aï¼‰å’Œç­‰ä»·çŸ›ç›¾ï¼ˆA&#x3D;B&#x3D;Câ‰ Aï¼‰ã€‚æˆ‘ä»¬è®¤ä¸ºè¿™äº›é—®é¢˜æºäºç¦»æ•£è¯„åˆ†ç³»ç»Ÿä¸­çš„ä¿¡æ¯æŸå¤±å’Œé…å¯¹è¯„ä¼°æœŸé—´çš„æ¨¡ç³Šå¹³å±€åˆ¤æ–­ã€‚æˆ‘ä»¬æå‡ºäº†TrustJudgeï¼Œè¿™æ˜¯ä¸€ä¸ªæ¦‚ç‡æ¡†æ¶ï¼Œé€šè¿‡ä¸¤ä¸ªå…³é”®åˆ›æ–°æ¥è§£å†³è¿™äº›å±€é™æ€§ï¼š1ï¼‰åˆ†å¸ƒæ•æ„Ÿè¯„åˆ†ï¼Œæ ¹æ®ç¦»æ•£è¯„åˆ†æ¦‚ç‡è®¡ç®—è¿ç»­æœŸæœ›ï¼Œä¿ç•™ä¿¡æ¯ç†µä»¥è¿›è¡Œæ›´ç²¾ç¡®è¯„åˆ†ï¼›ä»¥åŠ2ï¼‰å¯èƒ½æ€§æ„ŸçŸ¥èšåˆï¼Œä½¿ç”¨åŒå‘åå¥½æ¦‚ç‡æˆ–å›°æƒ‘åº¦è§£å†³ä¼ é€’æ€§è¿è§„ã€‚æˆ‘ä»¬è¿˜æ­£å¼æå‡ºäº†å½“å‰LLM-as-a-judgeæ¡†æ¶çš„ç†è®ºå±€é™æ€§ï¼Œå¹¶å±•ç¤ºäº†TrustJudgeçš„ç»„ä»¶æ˜¯å¦‚ä½•å…‹æœå®ƒä»¬çš„ã€‚ä½¿ç”¨Llama-3.1-70B-Instructä½œä¸ºè¯„å§”å¯¹æˆ‘ä»¬çš„æ•°æ®é›†è¿›è¡Œè¯„ä¼°æ—¶ï¼ŒTrustJudgeå°†è¯„åˆ†æ¯”è¾ƒä¸ä¸€è‡´æ€§é™ä½äº†8.43%ï¼ˆä»23.32%é™è‡³14.89%ï¼‰ï¼Œé…å¯¹ä¼ é€’æ€§ä¸ä¸€è‡´æ€§é™ä½äº†10.82%ï¼ˆä»15.22%é™è‡³4.4%ï¼‰ï¼ŒåŒæ—¶ä¿æŒäº†æ›´é«˜çš„è¯„ä¼°å‡†ç¡®æ€§ã€‚æˆ‘ä»¬çš„å·¥ä½œæä¾›äº†LLM-as-a-judgeèŒƒå¼ä¸­è¯„ä¼°æ¡†æ¶ä¸ä¸€è‡´æ€§çš„ç³»ç»Ÿåˆ†æï¼Œä¸ºå¯é çš„è‡ªåŠ¨è¯„ä¼°æä¾›äº†ç†è®ºè§è§£å’Œå®é™…è§£å†³æ–¹æ¡ˆã€‚è¯¥æ¡†æ¶åœ¨å„ç§æ¨¡å‹æ¶æ„å’Œè§„æ¨¡ä¸Šå‡æ˜¾ç¤ºå‡ºæŒç»­æ”¹è¿›çš„èƒ½åŠ›ï¼Œå¯åœ¨æ— éœ€é¢å¤–è®­ç»ƒæˆ–äººå·¥æ³¨é‡Šçš„æƒ…å†µä¸‹ï¼Œè¿›è¡Œæ›´å¯é çš„LLMè¯„ä¼°ã€‚ç›¸å…³ä»£ç å¯è§äº<a target="_blank" rel="noopener" href="https://github.com/TrustJudge/TrustJudge%E3%80%82">https://github.com/TrustJudge/TrustJudgeã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.21117v1">PDF</a> 22 pages, 9 figures, 6 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºè‡ªåŠ¨è¯„ä¼°å™¨ï¼ˆLLM-as-a-judgeï¼‰åœ¨ç°æœ‰è¯„ä¼°æ¡†æ¶ä¸­çš„å…³é”®ä¸ä¸€è‡´æ€§é—®é¢˜ã€‚æ–‡ç« æŒ‡å‡ºäº†ä¸¤ç§æ ¹æœ¬æ€§çš„ä¸ä¸€è‡´æ€§ï¼šï¼ˆ1ï¼‰è¯„åˆ†æ¯”è¾ƒä¸ä¸€è‡´æ€§ï¼Œå…¶ä¸­ä½è¯„çº§çš„å›åº”åœ¨æˆå¯¹æ¯”è¾ƒä¸­è¡¨ç°å‡ºä¼˜äºé«˜è¯„åˆ†å›åº”çš„æƒ…å†µï¼›ï¼ˆ2ï¼‰æˆå¯¹ä¼ é€’æ€§ä¸ä¸€è‡´æ€§ï¼Œè¡¨ç°ä¸ºå¾ªç¯åå¥½é“¾å’Œç­‰ä»·çŸ›ç›¾ã€‚æ–‡ç« æå‡ºTrustJudgeæ¡†æ¶æ¥è§£å†³è¿™äº›é—®é¢˜ï¼Œé€šè¿‡åˆ†å¸ƒæ•æ„Ÿè¯„åˆ†å’Œå¯èƒ½æ€§æ„ŸçŸ¥èšåˆä¸¤ä¸ªå…³é”®åˆ›æ–°ç‚¹æ¥æ”¹è¿›ç°æœ‰æ¡†æ¶çš„ç†è®ºå±€é™æ€§ã€‚åœ¨ç‰¹å®šæ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒTrustJudgeèƒ½å‡å°‘è¯„åˆ†æ¯”è¾ƒä¸ä¸€è‡´æ€§å’Œæˆå¯¹ä¼ é€’æ€§ä¸ä¸€è‡´æ€§ï¼ŒåŒæ—¶ä¿æŒè¾ƒé«˜çš„è¯„ä¼°å‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºè‡ªåŠ¨è¯„ä¼°å™¨åœ¨ç°æœ‰è¯„ä¼°æ¡†æ¶ä¸­å­˜åœ¨å…³é”®ä¸ä¸€è‡´æ€§é—®é¢˜ã€‚</li>
<li>ä¸»è¦çš„ä¸¤ç§ä¸ä¸€è‡´æ€§åŒ…æ‹¬è¯„åˆ†æ¯”è¾ƒä¸ä¸€è‡´æ€§å’Œæˆå¯¹ä¼ é€’æ€§ä¸ä¸€è‡´æ€§ã€‚</li>
<li>TrustJudgeæ¡†æ¶é€šè¿‡åˆ†å¸ƒæ•æ„Ÿè¯„åˆ†å’Œå¯èƒ½æ€§æ„ŸçŸ¥èšåˆæ¥è§£å†³è¿™äº›ä¸ä¸€è‡´æ€§é—®é¢˜ã€‚</li>
<li>TrustJudgeèƒ½å‡å°‘è¯„åˆ†æ¯”è¾ƒä¸ä¸€è‡´æ€§å’Œæˆå¯¹ä¼ é€’æ€§ä¸ä¸€è‡´æ€§è¾¾ä¸€å®šæ¯”ä¾‹ã€‚</li>
<li>TrustJudgeæ¡†æ¶åœ¨å¤šç§æ¨¡å‹å’Œè§„æ¨¡ä¸Šè¡¨ç°ä¸€è‡´ï¼Œå¯æé«˜LLMè¯„ä¼°çš„å¯é æ€§ã€‚</li>
<li>TrustJudgeæ¡†æ¶ä¸éœ€è¦é¢å¤–çš„è®­ç»ƒæˆ–äººå·¥æ³¨é‡Šï¼Œå³å¯å®ç°å¯é çš„LLMè¯„ä¼°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.21117">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-3dfba6d46207db6303dfefd4d6758c1d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684765&auth_key=1760684765-0-0-5e33c25e321f178c69d64f06f7bd0d3e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a721b037d83867c96c27f135d4bf7d63~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684772&auth_key=1760684772-0-0-66f6d59d6619fe8537dd3439aafda729&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6dc42159008a4cc2327d44b6c03a1639~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684778&auth_key=1760684778-0-0-e4168251c5e935045d39c905e95ee723&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="ScaleDiff-Scaling-Difficult-Problems-for-Advanced-Mathematical-Reasoning"><a href="#ScaleDiff-Scaling-Difficult-Problems-for-Advanced-Mathematical-Reasoning" class="headerlink" title="ScaleDiff: Scaling Difficult Problems for Advanced Mathematical   Reasoning"></a>ScaleDiff: Scaling Difficult Problems for Advanced Mathematical   Reasoning</h2><p><strong>Authors:Qizhi Pei, Zhuoshi Pan, Honglin Lin, Xin Gao, Yu Li, Zinan Tang, Conghui He, Rui Yan, Lijun Wu</strong></p>
<p>Large Reasoning Models (LRMs) have shown impressive capabilities in complex problem-solving, often benefiting from training on difficult mathematical problems that stimulate intricate reasoning. Recent efforts have explored automated synthesis of mathematical problems by prompting proprietary models or large-scale open-source models from seed data or inherent mathematical concepts. However, scaling up these methods remains challenging due to their high computational&#x2F;API cost, complexity of prompting, and limited difficulty level of the generated problems. To overcome these limitations, we propose ScaleDiff, a simple yet effective pipeline designed to scale the creation of difficult problems. We efficiently identify difficult problems from existing datasets with only a single forward pass using an adaptive thinking model, which can perceive problem difficulty and automatically switch between â€œThinkingâ€ and â€œNoThinkingâ€ modes. We then train a specialized difficult problem generator (DiffGen-8B) on this filtered difficult data, which can produce new difficult problems in large scale, eliminating the need for complex, per-instance prompting and its associated high API costs. Fine-tuning Qwen2.5-Math-7B-Instruct on the ScaleDiff-Math dataset yields a substantial performance increase of 11.3% compared to the original dataset and achieves a 65.9% average accuracy on AIMEâ€™24, AIMEâ€™25, HMMT-Febâ€™25, BRUMOâ€™25, and MATH500, outperforming recent strong LRMs like OpenThinker3. Notably, this performance is achieved using the cost-efficient Qwen3-8B model as a teacher, demonstrating that our pipeline can effectively transfer advanced reasoning capabilities without relying on larger, more expensive teacher models. Furthermore, we observe a clear scaling phenomenon in model performance on difficult benchmarks as the quantity of difficult problems increases. Code: <a target="_blank" rel="noopener" href="https://github.com/QizhiPei/ScaleDiff">https://github.com/QizhiPei/ScaleDiff</a>. </p>
<blockquote>
<p>å¤§è§„æ¨¡æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰åœ¨å¤æ‚é—®é¢˜è§£å†³æ–¹é¢å±•ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ï¼Œé€šå¸¸å—ç›Šäºé€šè¿‡åœ¨å¤æ‚æ•°å­¦é—®é¢˜ä¸Šçš„è®­ç»ƒï¼Œåˆºæ¿€å…¶å¤æ‚çš„æ¨ç†èƒ½åŠ›ã€‚æœ€è¿‘çš„åŠªåŠ›å·²ç»å°è¯•é€šè¿‡æç¤ºä¸“æœ‰æ¨¡å‹æˆ–å¤§è§„æ¨¡å¼€æºæ¨¡å‹ä»ç§å­æ•°æ®æˆ–å›ºæœ‰çš„æ•°å­¦æ¦‚å¿µæ¥è‡ªåŠ¨åˆæˆæ•°å­¦é—®é¢˜ã€‚ç„¶è€Œï¼Œç”±äºè®¡ç®—&#x2F;APIæˆæœ¬é«˜ã€æç¤ºå¤æ‚ä»¥åŠç”Ÿæˆçš„é—®é¢˜éš¾åº¦æœ‰é™ï¼Œè¿™äº›æ–¹æ³•åœ¨è§„æ¨¡åŒ–æ–¹é¢ä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ScaleDiffï¼Œè¿™æ˜¯ä¸€ä¸ªç®€å•æœ‰æ•ˆçš„æµæ°´çº¿è®¾è®¡ï¼Œæ—¨åœ¨è§„æ¨¡åŒ–åˆ›å»ºå›°éš¾é—®é¢˜ã€‚æˆ‘ä»¬ä»…é€šè¿‡ä¸€æ¬¡å‰å‘ä¼ é€’æœ‰æ•ˆåœ°ä»ç°æœ‰æ•°æ®é›†ä¸­è¯†åˆ«å‡ºå›°éš¾é—®é¢˜ï¼Œå¹¶ä½¿ç”¨è‡ªé€‚åº”æ€è€ƒæ¨¡å‹æ¥æ„ŸçŸ¥é—®é¢˜éš¾åº¦ï¼Œè¯¥æ¨¡å‹å¯ä»¥åœ¨â€œæ€è€ƒâ€å’Œâ€œä¸æ€è€ƒâ€æ¨¡å¼ä¹‹é—´è‡ªåŠ¨åˆ‡æ¢ã€‚ç„¶åæˆ‘ä»¬åœ¨è¿™äº›ç­›é€‰å‡ºçš„å›°éš¾æ•°æ®ä¸Šè®­ç»ƒäº†ä¸€ä¸ªä¸“é—¨çš„å›°éš¾é—®é¢˜ç”Ÿæˆå™¨ï¼ˆDiffGen-8Bï¼‰ï¼Œå®ƒèƒ½å¤Ÿå¤§è§„æ¨¡ç”Ÿæˆæ–°çš„é—®é¢˜ï¼Œä»è€Œæ— éœ€å¤æ‚çš„é€ä¸ªå®ä¾‹æç¤ºåŠå…¶ç›¸å…³çš„é«˜APIæˆæœ¬ã€‚åœ¨ScaleDiff-Mathæ•°æ®é›†ä¸Šå¯¹Qwen2.5-Math-7B-Instructè¿›è¡Œå¾®è°ƒï¼Œä¸åŸå§‹æ•°æ®é›†ç›¸æ¯”ï¼Œæ€§èƒ½æé«˜äº†11.3%ï¼Œåœ¨AIMEâ€™24ã€AIMEâ€™25ã€HMMT-Febâ€™25ã€BRUMOâ€™25å’ŒMATH500ä¸Šçš„å¹³å‡å‡†ç¡®ç‡ä¸º65.9%ï¼Œè¶…è¿‡äº†æœ€è¿‘çš„å¼ºå¤§LRMsï¼Œå¦‚OpenThinker3ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¿™ä¸€æ€§èƒ½æ˜¯ä½¿ç”¨æˆæœ¬æ•ˆç›Šé«˜çš„Qwen3-8Bæ¨¡å‹ä½œä¸ºæ•™å¸ˆæ¨¡å‹å®ç°çš„ï¼Œè¿™è¡¨æ˜æˆ‘ä»¬çš„æµæ°´çº¿å¯ä»¥æœ‰æ•ˆåœ°è½¬ç§»é«˜çº§æ¨ç†èƒ½åŠ›ï¼Œè€Œæ— éœ€ä¾èµ–æ›´å¤§ã€æ›´æ˜‚è´µçš„æ•™å¸ˆæ¨¡å‹ã€‚æ­¤å¤–ï¼Œéšç€éš¾é¢˜æ•°é‡çš„å¢åŠ ï¼Œæˆ‘ä»¬åœ¨å›°éš¾åŸºå‡†æµ‹è¯•ä¸Šçš„æ¨¡å‹æ€§èƒ½å‡ºç°äº†æ˜æ˜¾çš„è§„æ¨¡æ•ˆåº”ã€‚ä»£ç åœ°å€ï¼š<a target="_blank" rel="noopener" href="https://github.com/QizhiPei/ScaleDiff%E3%80%82">https://github.com/QizhiPei/ScaleDiffã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.21070v1">PDF</a> 15 pages</p>
<p><strong>Summary</strong><br>å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰åœ¨å¤„ç†å¤æ‚é—®é¢˜æ—¶å±•ç°å‡ºæ˜¾è‘—çš„èƒ½åŠ›ã€‚é’ˆå¯¹å½“å‰è‡ªåŠ¨åˆæˆæ•°å­¦é—®é¢˜æ–¹æ³•çš„é«˜è®¡ç®—æˆæœ¬ã€ç”Ÿæˆé¢˜ç›®éš¾åº¦æœ‰é™ç­‰å±€é™æ€§ï¼Œç ”ç©¶è€…æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„åä¸ºScalediffçš„è§£å†³æ–¹æ¡ˆã€‚è¯¥æ–¹æ¡ˆä½¿ç”¨è‡ªé€‚åº”æ€è€ƒæ¨¡å‹å¿«é€Ÿè¯†åˆ«ç°æœ‰æ•°æ®é›†ä¸­çš„éš¾é¢˜ï¼Œå¹¶é€šè¿‡è®­ç»ƒä¸“é—¨çš„éš¾é¢˜ç”Ÿæˆå™¨æ¥å¤§è§„æ¨¡ç”Ÿæˆæ–°éš¾é¢˜ï¼Œä»è€Œæ— éœ€å¤æ‚çš„é€ä¸ªå®ä¾‹æç¤ºåŠå…¶ç›¸å…³çš„é«˜APIæˆæœ¬ã€‚å¯¹æ¨¡å‹çš„å¾®è°ƒå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œç‰¹åˆ«æ˜¯åœ¨é¢å¯¹å…·æœ‰ä¸€å®šéš¾åº¦çš„åŸºå‡†æµ‹è¯•æ—¶ï¼Œå®ç°äº†æˆæœ¬æ•ˆç›Šä¼˜åŒ–çš„æˆæœã€‚ç›®å‰ï¼Œè¯¥æ–¹æ¡ˆå·²åœ¨GitHubä¸Šå…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LRMså±•ç°å‡ºè§£å†³å¤æ‚é—®é¢˜çš„èƒ½åŠ›ï¼Œå¾—ç›Šäºè®­ç»ƒåœ¨å›°éš¾æ•°å­¦é—®é¢˜ä¸Šçš„æˆæœã€‚</li>
<li>å½“å‰è‡ªåŠ¨åˆæˆæ•°å­¦é—®é¢˜çš„æŒ‘æˆ˜åŒ…æ‹¬é«˜è®¡ç®—æˆæœ¬ã€æç¤ºå¤æ‚æ€§å’Œé—®é¢˜éš¾åº¦æœ‰é™ã€‚</li>
<li>ScaleDiffè¢«æå‡ºä»¥è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œé€šè¿‡è‡ªé€‚åº”æ€è€ƒæ¨¡å‹å¿«é€Ÿè¯†åˆ«éš¾é¢˜å¹¶åˆ©ç”¨éš¾é¢˜ç”Ÿæˆå™¨è¿›è¡Œå¤§è§„æ¨¡éš¾é¢˜ç”Ÿæˆã€‚</li>
<li>ScaleDiffæ–¹æ¡ˆå‡å°‘äº†å¤æ‚çš„é€ä¸ªå®ä¾‹æç¤ºåŠå…¶ç›¸å…³çš„é«˜APIæˆæœ¬ã€‚</li>
<li>å¯¹æ¨¡å‹çš„å¾®è°ƒåœ¨éš¾é¢˜åŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†æ˜¾è‘—æ€§èƒ½æå‡ï¼Œå¹¶ä¸”èƒ½å¤Ÿåœ¨ä¸ä½¿ç”¨å¤§å‹æ˜‚è´µæ•™å¸ˆæ¨¡å‹çš„æƒ…å†µä¸‹å®ç°é«˜æ€§èƒ½è¡¨ç°ã€‚</li>
<li>æ¨¡å‹åœ¨é¢ä¸´ä¸åŒéš¾åº¦çº§åˆ«çš„é—®é¢˜æ—¶å±•ç°å‡ºæ˜æ˜¾çš„æ€§èƒ½æå‡è¶‹åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.21070">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-d6ef83a43170f7f8bae450727ec54b55~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684786&auth_key=1760684786-0-0-5646f769cf2dc480de20a311245f4b6d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-746e40edbc4e1613f831ee76738d97c5~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684793&auth_key=1760684793-0-0-cfa4106867b7f2ae10dab4a29a222d6a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4c887d95a08d254dad978b1524e8fdce~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684800&auth_key=1760684800-0-0-820ebefa1964d2eebe9f429413f1d986&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-220fc06a76d6d9cb8e4ec76c3ac426fa~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684806&auth_key=1760684806-0-0-5e0abdd28fbbd1fbc6324d315d777ef2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="GeoRef-Referring-Expressions-in-Geometry-via-Task-Formulation-Synthetic-Supervision-and-Reinforced-MLLM-based-Solutions"><a href="#GeoRef-Referring-Expressions-in-Geometry-via-Task-Formulation-Synthetic-Supervision-and-Reinforced-MLLM-based-Solutions" class="headerlink" title="GeoRef: Referring Expressions in Geometry via Task Formulation,   Synthetic Supervision, and Reinforced MLLM-based Solutions"></a>GeoRef: Referring Expressions in Geometry via Task Formulation,   Synthetic Supervision, and Reinforced MLLM-based Solutions</h2><p><strong>Authors:Bing Liu, Wenqiang Yv, Xuzheng Yang, Shichang Wang, Junzhuo Liu, Peng Wang, Guoqing Wang, Yang Yang, Heng Tao Shen</strong></p>
<p>AI-driven geometric problem solving is a complex vision-language task that requires accurate diagram interpretation, mathematical reasoning, and robust cross-modal grounding. A foundational yet underexplored capability for this task is the ability to identify and interpret geometric elements based on natural language queries. To address this, we introduce the task of Referring Expression Comprehension (REC) for geometric problems, which evaluates whether models can localize points, shapes, and spatial relations in diagrams in response to textual prompts. We present GeoRef, a benchmark dataset constructed from existing geometric problem corpora, featuring diverse, high-quality annotations and queries. Due to the lack of annotated data for this task, we generate a large-scale synthetic training dataset using a structured geometric formal language, enabling broad coverage of geometric concepts and facilitating model adaptation. We explore two fine-tuning approaches: Supervised Fine-Tuning (SFT) and Group Relative Policy Optimization (GRPO). Our results show that GRPO significantly outperforms SFT by better aligning model behavior with task-specific rewards. Furthermore, we propose a verify-and-regenerate mechanism that detects incorrect predictions and re-infers answers using contextual reasoning history, further boosting accuracy. Notably, even state-of-the-art Multimodal Large Language Models (MLLMs) struggle with this task, underscoring the necessity of explicitly evaluating and strengthening geometric grounding as a prerequisite for robust geometric problem solving. Moreover, models trained on GeoRef demonstrate measurable improvements on downstream geometric reasoning tasks, highlighting the broader value of REC as a foundation for multimodal mathematical understanding. </p>
<blockquote>
<p>äººå·¥æ™ºèƒ½é©±åŠ¨çš„å‡ ä½•é—®é¢˜æ±‚è§£æ˜¯ä¸€é¡¹å¤æ‚çš„è§†è§‰è¯­è¨€ä»»åŠ¡ï¼Œéœ€è¦å‡†ç¡®çš„å›¾è¡¨è§£è¯»ã€æ•°å­¦æ¨ç†å’Œç¨³å¥çš„è·¨æ¨¡æ€æ¥åœ°èƒ½åŠ›ã€‚å¯¹æ­¤ä»»åŠ¡çš„åŸºç¡€ä½†å°šæœªè¢«å……åˆ†æ¢ç´¢çš„èƒ½åŠ›æ˜¯ï¼ŒåŸºäºè‡ªç„¶è¯­è¨€æŸ¥è¯¢æ¥è¯†åˆ«å’Œè§£é‡Šå‡ ä½•å…ƒç´ çš„èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç”¨äºå‡ ä½•é—®é¢˜çš„æŒ‡ä»£è¡¨è¾¾å¼ç†è§£ï¼ˆRECï¼‰ä»»åŠ¡ï¼Œè¯¥ä»»åŠ¡è¯„ä¼°æ¨¡å‹æ˜¯å¦èƒ½æ ¹æ®æ–‡æœ¬æç¤ºåœ¨å›¾è¡¨ä¸­å®šä½ç‚¹ã€å½¢çŠ¶å’Œç©ºé—´å…³ç³»ã€‚æˆ‘ä»¬å±•ç¤ºäº†GeoRefï¼Œè¿™æ˜¯ä¸€ä¸ªç”±ç°æœ‰å‡ ä½•é—®é¢˜è¯­æ–™åº“æ„å»ºçš„åŸºå‡†æ•°æ®é›†ï¼Œå…·æœ‰å¤šæ ·ä¸”é«˜è´¨é‡çš„æ³¨é‡Šå’ŒæŸ¥è¯¢ã€‚ç”±äºæ­¤ä»»åŠ¡ç¼ºä¹æ ‡æ³¨æ•°æ®ï¼Œæˆ‘ä»¬ä½¿ç”¨ç»“æ„åŒ–çš„å‡ ä½•å½¢å¼è¯­è¨€ç”Ÿæˆå¤§è§„æ¨¡åˆæˆè®­ç»ƒæ•°æ®é›†ï¼Œä»¥å®ç°å¯¹å‡ ä½•æ¦‚å¿µçš„å¹¿æ³›è¦†ç›–å¹¶ä¿ƒè¿›æ¨¡å‹é€‚åº”ã€‚æˆ‘ä»¬æ¢ç´¢äº†ä¸¤ç§å¾®è°ƒæ–¹æ³•ï¼šç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œé€šè¿‡æ›´å¥½åœ°å°†æ¨¡å‹è¡Œä¸ºä¸ä»»åŠ¡ç‰¹å®šå¥–åŠ±å¯¹é½ï¼ŒGRPOæ˜¾è‘—ä¼˜äºSFTã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§éªŒè¯å’Œå†ç”Ÿæœºåˆ¶ï¼Œè¯¥æœºåˆ¶å¯ä»¥æ£€æµ‹é”™è¯¯çš„é¢„æµ‹å¹¶ä½¿ç”¨ä¸Šä¸‹æ–‡æ¨ç†å†å²é‡æ–°æ¨æ–­ç­”æ¡ˆï¼Œä»è€Œè¿›ä¸€æ­¥æé«˜å‡†ç¡®æ€§ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œç”šè‡³æœ€å…ˆè¿›çš„å¤šåª’ä½“å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¹Ÿåœ¨è¿™ä¸ªä»»åŠ¡ä¸ŠæŒ£æ‰ï¼Œè¿™å¼ºè°ƒäº†æ˜ç¡®è¯„ä¼°å’ŒåŠ å¼ºå‡ ä½•æ¥åœ°ä½œä¸ºç¨³å¥å‡ ä½•é—®é¢˜æ±‚è§£å…ˆå†³æ¡ä»¶çš„å¿…è¦æ€§ã€‚æ­¤å¤–ï¼Œåœ¨GeoRefä¸Šè®­ç»ƒçš„æ¨¡å‹åœ¨ä¸‹æ¸¸å‡ ä½•æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå¯è¡¡é‡çš„æ”¹è¿›ï¼Œçªæ˜¾äº†RECä½œä¸ºå¤šæ¨¡æ€æ•°å­¦ç†è§£åŸºç¡€çš„æ›´å¹¿æ³›ä»·å€¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.21050v1">PDF</a> </p>
<p><strong>Summary</strong><br>åœ¨è¿™ä¸ªä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬é¢ä¸´çš„æ˜¯AIé©±åŠ¨çš„å‡ ä½•é—®é¢˜æ±‚è§£è¿™ä¸€å¤æ‚çš„è§†è§‰è¯­è¨€ä»»åŠ¡ï¼Œè¿™éœ€è¦å‡†ç¡®çš„å›¾è¡¨è§£è¯»ã€æ•°å­¦æ¨ç†å’Œè·¨æ¨¡æ€çš„ç¨³å¥åŸºç¡€ã€‚æˆ‘ä»¬å¼•å…¥äº†é¢å‘å‡ ä½•é—®é¢˜çš„æŒ‡ä»£è¡¨è¾¾å¼ç†è§£ï¼ˆRECï¼‰ä»»åŠ¡ï¼Œä»¥è¯„ä¼°æ¨¡å‹æ˜¯å¦èƒ½æ ¹æ®æ–‡æœ¬æç¤ºåœ¨å›¾è¡¨ä¸­å®šä½ç‚¹ã€å½¢çŠ¶å’Œç©ºé—´å…³ç³»ã€‚æˆ‘ä»¬æ¨å‡ºäº†GeoRefæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†ç”±ç°æœ‰çš„å‡ ä½•é—®é¢˜è¯­æ–™åº“æ„å»ºè€Œæˆï¼Œå…·æœ‰å¤šæ ·åŒ–å’Œé«˜è´¨é‡çš„æ³¨é‡Šå’ŒæŸ¥è¯¢ã€‚ç”±äºç¼ºä¹æ­¤ä»»åŠ¡çš„æ³¨é‡Šæ•°æ®ï¼Œæˆ‘ä»¬åˆ©ç”¨ç»“æ„åŒ–å‡ ä½•å½¢å¼è¯­è¨€ç”Ÿæˆå¤§è§„æ¨¡åˆæˆè®­ç»ƒæ•°æ®é›†ï¼Œä»¥æ¶µç›–å¹¿æ³›çš„å‡ ä½•æ¦‚å¿µå¹¶ä¿ƒè¿›æ¨¡å‹é€‚åº”ã€‚æˆ‘ä»¬çš„ç»“æœç ”ç©¶è¡¨æ˜ï¼Œç›¸å¯¹äºç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œé‡‡ç”¨ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰çš„å¾®è°ƒæ–¹æ³•èƒ½æ›´å¥½åœ°é€‚åº”ä»»åŠ¡ç‰¹å®šå¥–åŠ±ï¼Œä»è€Œå®ç°æ˜¾è‘—çš„æ•ˆæœæå‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§éªŒè¯å’Œå†ç”Ÿæœºåˆ¶ï¼Œèƒ½å¤Ÿæ£€æµ‹é”™è¯¯çš„é¢„æµ‹å¹¶å€ŸåŠ©ä¸Šä¸‹æ–‡æ¨ç†å†å²é‡æ–°æ¨æ–­ç­”æ¡ˆï¼Œè¿›ä¸€æ­¥æå‡å‡†ç¡®ç‡ã€‚å°½ç®¡æœ€å…ˆè¿›çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨æ­¤ä»»åŠ¡ä¸Šä»ç„¶é¢ä¸´æŒ‘æˆ˜ï¼Œä½†æ¨¡å‹åœ¨GeoRefä¸Šçš„è®­ç»ƒåœ¨ä¸‹æ¸¸å‡ ä½•æ¨ç†ä»»åŠ¡ä¸Šæ˜¾ç¤ºå‡ºå¯è¡¡é‡çš„æ”¹è¿›ï¼Œå‡¸æ˜¾äº†æ˜ç¡®è¯„ä¼°å’Œå¼ºåŒ–å‡ ä½•åŸºç¡€ä½œä¸ºç¨³å¥å‡ ä½•é—®é¢˜è§£å†³çš„å…ˆå†³æ¡ä»¶çš„å¿…è¦æ€§ï¼Œå¹¶è¡¨æ˜äº†RECä½œä¸ºå¤šæ¨¡æ€æ•°å­¦ç†è§£åŸºç¡€çš„é‡è¦æ€§ã€‚æ€»çš„æ¥è¯´è¿™æ˜¯ä¸€ä¸ªæŒ‘æˆ˜éå¸¸å¤§çš„é—®é¢˜éœ€è¦é€šè¿‡å‡†ç¡®çš„æ–¹æ³•è¿›è¡Œåˆ†æè§£å†³ã€‚å¯¹äºè¿™æ ·çš„å¤æ‚ä»»åŠ¡ï¼Œæ•°æ®é›†ã€æ–¹æ³•å’ŒéªŒè¯æœºåˆ¶çš„è¿›ä¸€æ­¥å‘å±•å°†å…·æœ‰æå…¶é‡è¦çš„ä»·å€¼ã€‚ç®€åŒ–åçš„æ‘˜è¦å°±æ˜¯â€œäººå·¥æ™ºèƒ½è§£å†³å‡ ä½•é—®é¢˜éœ€è¦ç»“åˆè‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯å’Œæ•°å­¦çŸ¥è¯†çš„å­¦ä¹ æ¨¡å‹ç ”ç©¶ã€‚â€ã€‚è¿™ä½“ç°äº†æ¨¡å‹çš„å¤æ‚æ€§ä»¥åŠä»»åŠ¡æœ¬èº«çš„æŒ‘æˆ˜æ€§ã€‚<br><strong>Key Takeaways</strong></p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.21050">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-e9fce308da667c3eeacaeeac478485d8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684815&auth_key=1760684815-0-0-dc965b6b78f09d456855d0cb60e4f633&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7b8bce9759d25dd97569688f8d26a4e0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684822&auth_key=1760684822-0-0-f5655471d8923fe623d4850365a11d00&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-67d2cdc66b8644421dc9cc6ca392dd9e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684829&auth_key=1760684829-0-0-0e55dc2567dfef081631fef3d8cd70fc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5e2d5f9f5e7dd79ca43e94570b8f51e6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684836&auth_key=1760684836-0-0-62f260b80c7ccee845bc56c3804d534d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d67bbfabb8ca1632ed966c88cd78ab01~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684843&auth_key=1760684843-0-0-bf44e3141ebb38d18f5bdba87f625cd9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f48bb1388273012946f128a13a52fbc4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684849&auth_key=1760684849-0-0-b74e3bdab2dca0ed9f67ae26490607ff&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-def19f5e1b65d31dcea1ae3d539a9d72~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684856&auth_key=1760684856-0-0-54ecd7201dee90675977cb510308ad07&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Reinforcement-Learning-Fine-Tuning-Enhances-Activation-Intensity-and-Diversity-in-the-Internal-Circuitry-of-LLMs"><a href="#Reinforcement-Learning-Fine-Tuning-Enhances-Activation-Intensity-and-Diversity-in-the-Internal-Circuitry-of-LLMs" class="headerlink" title="Reinforcement Learning Fine-Tuning Enhances Activation Intensity and   Diversity in the Internal Circuitry of LLMs"></a>Reinforcement Learning Fine-Tuning Enhances Activation Intensity and   Diversity in the Internal Circuitry of LLMs</h2><p><strong>Authors:Honglin Zhang, Qianyue Hao, Fengli Xu, Yong Li</strong></p>
<p>Large language models (LLMs) acquire extensive prior knowledge through large-scale pretraining and can be further enhanced via supervised fine-tuning (SFT) or reinforcement learning (RL)-based post-training. A growing body of evidence has shown that RL fine-tuning improves the capability of LLMs beyond what SFT alone achieves. However, the underlying mechanisms why RL fine-tuning is able to enhance the capability of various LLMs with distinct intrinsic characteristics remain underexplored. In this study, we draw inspiration from prior work on edge attribution patching (EAP) to investigate the internal differences of LLMs before and after RL fine-tuning. Our analysis across multiple model families shows two robust effects of online RL post-training: (i) an overall increase in activation intensity, indicating that more internal pathways are engaged and their signals become stronger, and (ii) greater diversity in activation patterns, reflected by higher entropy and less concentrated edge distributions. These changes suggest that RL reshapes information flow to be both more redundant and more flexible, which may explain its advantage in generalization. Notably, models fine-tuned with Direct Preference Optimization (DPO) deviate from these trends, exhibiting substantially weaker or inconsistent internal changes compared to PPO- and GRPO-based training. Together, our findings provide a unified view of how RL fine-tuning systematically alters the internal circuitry of LLMs and highlight the methodological distinctions between online RL and preference-based approaches. Our code is open source at <a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/llm_rl_probing_analysis-F673">https://anonymous.4open.science/r/llm_rl_probing_analysis-F673</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡å¤§è§„æ¨¡é¢„è®­ç»ƒè·å¾—ä¸°å¯Œçš„å…ˆéªŒçŸ¥è¯†ï¼Œå¹¶å¯é€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æˆ–åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„åè®­ç»ƒè¿›ä¸€æ­¥å¢å¼ºã€‚è¶Šæ¥è¶Šå¤šçš„è¯æ®è¡¨æ˜ï¼ŒRLå¾®è°ƒæé«˜äº†LLMçš„èƒ½åŠ›ï¼Œè¶…è¶Šäº†ä»…ä½¿ç”¨SFTæ‰€è¾¾åˆ°çš„æ°´å¹³ã€‚ç„¶è€Œï¼ŒRLå¾®è°ƒèƒ½å¤Ÿå¢å¼ºå…·æœ‰ä¸åŒå†…åœ¨ç‰¹æ€§çš„å„ç§LLMèƒ½åŠ›çš„åº•å±‚æœºåˆ¶ä»ç„¶æœªè¢«å……åˆ†æ¢ç´¢ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬ä»è¾¹ç¼˜å½’å±ä¿®è¡¥ï¼ˆEAPï¼‰çš„å…ˆå‰ç ”ç©¶ä¸­æ±²å–çµæ„Ÿï¼Œè°ƒæŸ¥LLMåœ¨RLå¾®è°ƒå‰åçš„å†…éƒ¨å·®å¼‚ã€‚æˆ‘ä»¬å¯¹å¤šä¸ªæ¨¡å‹å®¶æ—çš„åˆ†ææ˜¾ç¤ºï¼Œåœ¨çº¿RLåè®­ç»ƒçš„ä¸¤ç§ç¨³å¥æ•ˆæœï¼šï¼ˆiï¼‰æ¿€æ´»å¼ºåº¦çš„æ€»ä½“å¢åŠ ï¼Œè¡¨æ˜æ›´å¤šçš„å†…éƒ¨è·¯å¾„è¢«æ¿€æ´»ä¸”å…¶ä¿¡å·å˜å¾—æ›´å¼ºï¼›ï¼ˆiiï¼‰æ¿€æ´»æ¨¡å¼çš„å¤šæ ·æ€§æ›´é«˜ï¼Œä½“ç°åœ¨æ›´é«˜çš„ç†µå€¼å’Œæ›´ä¸é›†ä¸­çš„è¾¹ç¼˜åˆ†å¸ƒã€‚è¿™äº›å˜åŒ–è¡¨æ˜RLä½¿ä¿¡æ¯æµåŠ¨æ›´åŠ å†—ä½™å’Œçµæ´»ï¼Œè¿™å¯èƒ½è§£é‡Šäº†å…¶åœ¨æ³›åŒ–æ–¹é¢çš„ä¼˜åŠ¿ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä½¿ç”¨ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰è¿›è¡Œå¾®è°ƒçš„æ¨¡å‹ä¸è¿™äº›è¶‹åŠ¿èƒŒé“è€Œé©°ï¼Œä¸åŸºäºPPOå’ŒGRPOçš„è®­ç»ƒç›¸æ¯”ï¼Œå…¶å†…éƒ¨å˜åŒ–æ›´å¼±æˆ–ä¸ä¸€è‡´ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»¼åˆåœ°å±•ç¤ºäº†RLå¾®è°ƒå¦‚ä½•ç³»ç»Ÿåœ°æ”¹å˜LLMçš„å†…éƒ¨ç»“æ„ï¼Œå¹¶çªå‡ºäº†åœ¨çº¿RLå’ŒåŸºäºåå¥½çš„æ–¹æ³•ä¹‹é—´çš„æ–¹æ³•è®ºåŒºåˆ«ã€‚æˆ‘ä»¬çš„ä»£ç åœ¨<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/llm_rl_probing_analysis-F673%E5%BC%B9%E7%9C%8B%E3%80%82">https://anonymous.4open.science/r/llm_rl_probing_analysis-F673å¼€æ”¾è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.21044v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡å¤§è§„æ¨¡é¢„è®­ç»ƒè·å¾—ä¸°å¯Œçš„å…ˆéªŒçŸ¥è¯†ï¼Œå¹¶å¯è¿›ä¸€æ­¥é€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç­‰åè®­ç»ƒæ–¹å¼è¿›è¡Œå¢å¼ºã€‚ç ”ç©¶è¡¨æ˜ï¼Œå¼ºåŒ–å­¦ä¹ å¾®è°ƒèƒ½æé«˜LLMçš„èƒ½åŠ›ï¼Œè¶…è¶Šä»…ä½¿ç”¨SFTæ‰€èƒ½è¾¾åˆ°çš„æ°´å¹³ã€‚ç„¶è€Œï¼Œå…³äºä¸ºä½•RLå¾®è°ƒèƒ½å¤Ÿæå‡å…·æœ‰ä¸åŒå†…åœ¨ç‰¹æ€§çš„å„ç§LLMçš„èƒ½åŠ›çš„åº•å±‚æœºåˆ¶å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æœ¬ç ”ç©¶å—è¾¹ç¼˜å±æ€§è¡¥ä¸ï¼ˆEAPï¼‰çš„å¯å‘ï¼Œæ¢è®¨äº†RLå¾®è°ƒå‰åLLMçš„å†…éƒ¨å·®å¼‚ã€‚å¯¹å¤šä¸ªæ¨¡å‹å®¶æ—çš„åˆ†ææ˜¾ç¤ºï¼Œåœ¨çº¿RLåè®­ç»ƒçš„ä¸¤ä¸ªç¨³å¥æ•ˆåº”ä¸ºï¼šï¼ˆä¸€ï¼‰æ¿€æ´»å¼ºåº¦çš„æ€»ä½“å¢åŠ ï¼Œè¡¨æ˜æ›´å¤šçš„å†…éƒ¨è·¯å¾„è¢«æ¿€æ´»ä¸”ä¿¡å·å˜å¾—æ›´å¼ºçƒˆï¼›ï¼ˆäºŒï¼‰æ¿€æ´»æ¨¡å¼çš„å¤šæ ·æ€§å¢åŠ ï¼Œåæ˜ ä¸ºè¾¹ç¼˜åˆ†å¸ƒçš„é«˜ç†µå’Œè¾ƒå°‘é›†ä¸­ã€‚è¿™äº›å˜åŒ–è¡¨æ˜ï¼ŒRLé‡å¡‘äº†æ›´ä¸ºå†—ä½™å’Œçµæ´»çš„ä¿¡æ¯æµï¼Œè¿™å¯èƒ½è§£é‡Šäº†å…¶åœ¨æ³›åŒ–æ–¹é¢çš„ä¼˜åŠ¿ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä½¿ç”¨ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰è¿›è¡Œå¾®è°ƒæ¨¡å‹çš„è¶‹åŠ¿æœ‰æ‰€ä¸åŒï¼Œä¸å…¶ä»–åŸºäºPPOå’ŒGRPOçš„è®­ç»ƒç›¸æ¯”ï¼Œå…¶å†…éƒ¨å˜åŒ–è¾ƒå¼±æˆ–ä¸ä¸€è‡´ã€‚æ€»ä½“è€Œè¨€ï¼Œæˆ‘ä»¬çš„ç ”ç©¶ç³»ç»Ÿåœ°æ­ç¤ºäº†RLå¾®è°ƒå¦‚ä½•æ”¹å˜LLMçš„å†…éƒ¨ç»“æ„ï¼Œå¹¶çªå‡ºäº†åœ¨çº¿RLå’ŒåŸºäºåå¥½çš„æ–¹æ³•ä¹‹é—´çš„æ–¹æ³•è®ºå·®å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ å¾®è°ƒèƒ½å¤Ÿæå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œè¶…è¶Šç›‘ç£å¾®è°ƒçš„æ•ˆæœã€‚</li>
<li>åœ¨çº¿RLåè®­ç»ƒå¯¼è‡´LLMçš„æ¿€æ´»å¼ºåº¦æ€»ä½“å¢åŠ ï¼Œè¡¨æ˜æ›´å¤šå†…éƒ¨è·¯å¾„è¢«æ¿€æ´»ä¸”ä¿¡å·å¼ºçƒˆã€‚</li>
<li>RLè®­ç»ƒä½¿å¾—LLMçš„æ¿€æ´»æ¨¡å¼æ›´åŠ å¤šæ ·ï¼Œè¡¨ç°ä¸ºé«˜ç†µå’Œè¾ƒå°‘é›†ä¸­çš„è¾¹ç¼˜åˆ†å¸ƒã€‚</li>
<li>RLé‡å¡‘äº†LLMçš„ä¿¡æ¯æµï¼Œä½¿å…¶æ›´åŠ å†—ä½™å’Œçµæ´»ï¼Œå¯èƒ½æé«˜äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰åœ¨LLMå¾®è°ƒä¸­çš„æ•ˆæœä¸å…¶ä»–åŸºäºPPOå’ŒGRPOçš„è®­ç»ƒæ–¹æ³•æœ‰æ‰€ä¸åŒã€‚</li>
<li>DPOå¾®è°ƒæ¨¡å‹çš„å†…éƒ¨å˜åŒ–è¾ƒå¼±æˆ–ä¸ä¸€è‡´ï¼Œä¸å…¶ä»–è®­ç»ƒæ–¹æ³•å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.21044">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-778ddc559806a0c53f39d7d566eea8eb~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684863&auth_key=1760684863-0-0-5392c7be6817135583edfaeae9dfae8b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-76c045537a909dad8d716e24376e2667~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684871&auth_key=1760684871-0-0-d94993a6cab477b530e46ae693996261&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-25663bd91b882717e9ab6956b7d8a3f1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684878&auth_key=1760684878-0-0-f1a43f86634f70e5ff1ab799eb2f56c2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Predicting-LLM-Reasoning-Performance-with-Small-Proxy-Model"><a href="#Predicting-LLM-Reasoning-Performance-with-Small-Proxy-Model" class="headerlink" title="Predicting LLM Reasoning Performance with Small Proxy Model"></a>Predicting LLM Reasoning Performance with Small Proxy Model</h2><p><strong>Authors:Woosung Koh, Juyoung Suk, Sungjun Han, Se-Young Yun, Jay Shin</strong></p>
<p>Given the prohibitive cost of pre-training large language models, it is essential to leverage smaller proxy models to optimize datasets before scaling up. However, this approach becomes challenging for reasoning capabilities, which exhibit emergent behavior that only appear reliably at larger model sizes, often exceeding 7B parameters. To address this, we introduce rBridge, showing that small proxies ($\leq$1B) can effectively predict large-model reasoning by aligning more closely with (1) the pre-training objective and (2) the target task. rBridge achieves this by weighting negative log-likelihood with task alignment, using reasoning traces from frontier models as gold labels. In our experiments, rBridge (i) reduces dataset ranking costs by over 100x relative to the best baseline, (ii) achieves the strongest correlation across six reasoning benchmarks at 1B to 32B scale, and (iii) zero-shot transfers predictive relationships across pre-training datasets at 1B to 7B scale. These findings indicate that rBridge offers a practical path for exploring reasoning-oriented pre-training at lower cost. </p>
<blockquote>
<p>è€ƒè™‘åˆ°é¢„è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹çš„æˆæœ¬é«˜æ˜‚ï¼Œåˆ©ç”¨å°å‹ä»£ç†æ¨¡å‹å¯¹æ•°æ®é›†è¿›è¡Œä¼˜åŒ–ï¼Œç„¶åå†è¿›è¡Œæ‰©å±•æ˜¯è‡³å…³é‡è¦çš„ã€‚ç„¶è€Œï¼Œå¯¹äºå±•ç°å‡ºçš„æ–°å…´è¡Œä¸ºåªåœ¨è¾ƒå¤§æ¨¡å‹è§„æ¨¡ä¸­å¯é å‡ºç°çš„æ¨ç†èƒ½åŠ›ï¼Œè¿™ç§æ–¹æ³•å˜å¾—å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œé€šå¸¸è¶…è¿‡70äº¿å‚æ•°ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†rBridgeï¼Œå±•ç¤ºå°å‹ä»£ç†ï¼ˆâ‰¤1Bï¼‰å¯ä»¥é€šè¿‡æ›´ç´§å¯†åœ°ç¬¦åˆï¼ˆ1ï¼‰é¢„è®­ç»ƒç›®æ ‡å’Œï¼ˆ2ï¼‰ç›®æ ‡ä»»åŠ¡ï¼Œæœ‰æ•ˆåœ°é¢„æµ‹å¤§å‹æ¨¡å‹çš„æ¨ç†ã€‚rBridgeé€šè¿‡ç”¨ä»»åŠ¡å¯¹é½å¯¹è´Ÿå¯¹æ•°å¯èƒ½æ€§è¿›è¡ŒåŠ æƒæ¥å®ç°è¿™ä¸€ç‚¹ï¼Œåˆ©ç”¨å‰æ²¿æ¨¡å‹çš„æ¨ç†è½¨è¿¹ä½œä¸ºé»„é‡‘æ ‡ç­¾ã€‚åœ¨æˆ‘ä»¬çš„å®éªŒä¸­ï¼ŒrBridgeï¼ˆiï¼‰å°†æ•°æ®é›†æ’åæˆæœ¬é™ä½äº†è¶…è¿‡100å€ï¼Œç›¸å¯¹äºæœ€ä½³åŸºçº¿ï¼Œï¼ˆiiï¼‰åœ¨1Båˆ°32Bè§„æ¨¡ä¸Šå®ç°äº†å…­ä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸­çš„æœ€å¼ºç›¸å…³æ€§ï¼Œï¼ˆiiiï¼‰åœ¨1Båˆ°7Bè§„æ¨¡ä¸Šå®ç°äº†é¢„è®­ç»ƒæ•°æ®é›†ä¹‹é—´çš„é›¶å°„å‡»è½¬ç§»é¢„æµ‹å…³ç³»ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼ŒrBridgeä¸ºåœ¨è¾ƒä½æˆæœ¬ä¸‹æ¢ç´¢ä»¥æ¨ç†ä¸ºå¯¼å‘çš„é¢„è®­ç»ƒæä¾›äº†ä¸€æ¡å®ç”¨é€”å¾„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.21013v1">PDF</a> Pre-print</p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬ä»‹ç»äº†é¢å¯¹å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹é«˜æ˜‚çš„è®­ç»ƒæˆæœ¬é—®é¢˜ï¼Œåˆ©ç”¨å°å‹ä»£ç†æ¨¡å‹ä¼˜åŒ–æ•°æ®é›†çš„ç­–ç•¥å˜å¾—å°¤ä¸ºå…³é”®ã€‚ä½†é—®é¢˜åœ¨äºè¿™ç§ç­–ç•¥åœ¨å¤„ç†éœ€è¦å¤§é‡å‚æ•°æ‰èƒ½è¾¾åˆ°çš„ä¼˜ç§€æ¨ç†èƒ½åŠ›ä¸Šæ˜¾å¾—æ‰è¥Ÿè§è‚˜ï¼Œå¾€å¾€è¦æ±‚æ¨¡å‹å‚æ•°è¶…è¿‡7Bã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†rBridgeæŠ€æœ¯ã€‚è¯¥æŠ€æœ¯é€šè¿‡æ›´ç´§å¯†åœ°ç»“åˆé¢„è®­ç»ƒç›®æ ‡å’Œç›®æ ‡ä»»åŠ¡ï¼Œä½¿å°å‹ä»£ç†æ¨¡å‹ï¼ˆâ‰¤1Bï¼‰èƒ½å¤Ÿæœ‰æ•ˆé¢„æµ‹å¤§å‹æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒrBridgeæ˜¾è‘—é™ä½äº†æ•°æ®é›†æ’åæˆæœ¬ï¼ŒåŒæ—¶åœ¨ä¸åŒè§„æ¨¡çš„æ¨¡å‹ä¸­å®ç°äº†å¼ºå¤§çš„æ¨ç†èƒ½åŠ›åŸºå‡†æµ‹è¯•å…³è”å’Œé›¶æ ·æœ¬è¿ç§»é¢„æµ‹å…³ç³»ã€‚è¿™äº›å‘ç°è¯æ˜rBridgeåœ¨ä½æˆæœ¬æ¡ä»¶ä¸‹æ¢ç´¢ä»¥æ¨ç†ä¸ºé‡ç‚¹çš„é¢„è®­ç»ƒæ˜¯ä¸€ä¸ªå¯è¡Œçš„é€”å¾„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¢å¯¹å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„é«˜æ˜‚è®­ç»ƒæˆæœ¬ï¼Œåˆ©ç”¨å°å‹ä»£ç†æ¨¡å‹ä¼˜åŒ–æ•°æ®é›†æˆä¸ºä¸€ç§ç­–ç•¥ã€‚</li>
<li>å°å‹ä»£ç†æ¨¡å‹åœ¨å¤„ç†éœ€è¦å¤§é‡å‚æ•°çš„æ¨ç†ä»»åŠ¡æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>rBridgeæŠ€æœ¯å¼•å…¥ï¼Œæ—¨åœ¨è§£å†³å°å‹ä»£ç†æ¨¡å‹åœ¨é¢„æµ‹å¤§å‹æ¨¡å‹æ¨ç†èƒ½åŠ›ä¸Šçš„ä¸è¶³ã€‚</li>
<li>rBridgeæŠ€æœ¯ç»“åˆé¢„è®­ç»ƒç›®æ ‡å’Œç›®æ ‡ä»»åŠ¡ï¼Œä½¿å°å‹ä»£ç†æ¨¡å‹æ›´ç´§å¯†åœ°æ¨¡æ‹Ÿå¤§å‹æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>rBridgeæ˜¾è‘—é™ä½äº†æ•°æ®é›†æ’åæˆæœ¬ï¼Œæé«˜äº†æ•ˆç‡ã€‚</li>
<li>rBridgeåœ¨ä¸åŒè§„æ¨¡çš„æ¨¡å‹ä¸­å®ç°äº†å¼ºå¤§çš„æ¨ç†èƒ½åŠ›åŸºå‡†æµ‹è¯•å…³è”ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.21013">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-f444aded06ff79468d4b2dbe0df217d4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684886&auth_key=1760684886-0-0-53d4c30f710d39bc15eb4b77f1650c86&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f5de0556f66d5778ac9ea3ec39dc3199~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684893&auth_key=1760684893-0-0-d0279d0e9e3675aa0d6c9cb4a03e8058&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1f59ea2bc6b5c2e1fe7995acbde326d7~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684900&auth_key=1760684900-0-0-4ef89cd5bcf6531715ffc1d535d1c915&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-827d7987be92a54b80281a40d6da929f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684907&auth_key=1760684907-0-0-0f9897511e8797a140e53b9f33642f48&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9500cf96ddac9f8a646101a937b586cd~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684914&auth_key=1760684914-0-0-f69e5f14c559f2714c311e1c8e0212d1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Lossless-Compression-A-New-Benchmark-for-Time-Series-Model-Evaluation"><a href="#Lossless-Compression-A-New-Benchmark-for-Time-Series-Model-Evaluation" class="headerlink" title="Lossless Compression: A New Benchmark for Time Series Model Evaluation"></a>Lossless Compression: A New Benchmark for Time Series Model Evaluation</h2><p><strong>Authors:Meng Wan, Benxi Tian, Jue Wang, Cui Hui, Ningming Nie, Tiantian Liu, Zongguo Wang, Cao Rongqiang, Peng Shi, Yangang Wang</strong></p>
<p>The evaluation of time series models has traditionally focused on four canonical tasks: forecasting, imputation, anomaly detection, and classification. While these tasks have driven significant progress, they primarily assess task-specific performance and do not rigorously measure whether a model captures the full generative distribution of the data. We introduce lossless compression as a new paradigm for evaluating time series models, grounded in Shannonâ€™s source coding theorem. This perspective establishes a direct equivalence between optimal compression length and the negative log-likelihood, providing a strict and unified information-theoretic criterion for modeling capacity. Then We define a standardized evaluation protocol and metrics. We further propose and open-source a comprehensive evaluation framework TSCom-Bench, which enables the rapid adaptation of time series models as backbones for lossless compression. Experiments across diverse datasets on state-of-the-art models, including TimeXer, iTransformer, and PatchTST, demonstrate that compression reveals distributional weaknesses overlooked by classic benchmarks. These findings position lossless compression as a principled task that complements and extends existing evaluation for time series modeling. </p>
<blockquote>
<p>æ—¶é—´åºåˆ—æ¨¡å‹çš„è¯„ä¼°ä¼ ç»Ÿä¸Šä¸»è¦é›†ä¸­åœ¨å››ä¸ªå…¸å‹ä»»åŠ¡ä¸Šï¼šé¢„æµ‹ã€è¡¥å…¨ã€å¼‚å¸¸æ£€æµ‹å’Œåˆ†ç±»ã€‚è™½ç„¶è¿™äº›ä»»åŠ¡æ¨åŠ¨äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œä½†å®ƒä»¬ä¸»è¦è¯„ä¼°ç‰¹å®šä»»åŠ¡çš„æ€§èƒ½ï¼Œå¹¶ä¸èƒ½ä¸¥æ ¼åœ°è¡¡é‡æ¨¡å‹æ˜¯å¦æ•æ‰åˆ°äº†æ•°æ®çš„å…¨ç”Ÿæˆåˆ†å¸ƒã€‚æˆ‘ä»¬å¼•å…¥æ— æŸå‹ç¼©ä½œä¸ºè¯„ä¼°æ—¶é—´åºåˆ—æ¨¡å‹çš„æ–°èŒƒå¼ï¼Œè¯¥èŒƒå¼åŸºäºé¦™å†œçš„æºä»£ç ç¼–ç å®šç†ã€‚è¿™ä¸ªè§‚ç‚¹å»ºç«‹äº†æœ€ä¼˜å‹ç¼©é•¿åº¦å’Œè´Ÿå¯¹æ•°ä¼¼ç„¶ä¹‹é—´çš„ç›´æ¥ç­‰ä»·å…³ç³»ï¼Œä¸ºæ¨¡å‹å®¹é‡æä¾›äº†ä¸¥æ ¼ä¸”ç»Ÿä¸€çš„ä¿¡æ¯ç†è®ºæ ‡å‡†ã€‚ç„¶åï¼Œæˆ‘ä»¬å®šä¹‰äº†æ ‡å‡†åŒ–çš„è¯„ä¼°åè®®å’ŒæŒ‡æ ‡ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºå¹¶å¼€æºäº†ä¸€ä¸ªå…¨é¢çš„è¯„ä¼°æ¡†æ¶TSCom-Benchï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿä½¿æ—¶é—´åºåˆ—æ¨¡å‹å¿«é€Ÿé€‚åº”æ— æŸå‹ç¼©ã€‚åœ¨åŒ…æ‹¬TimeXerã€iTransformerå’ŒPatchTSTç­‰æœ€æ–°æ¨¡å‹ä¸Šçš„å¤šæ ·åŒ–æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œå‹ç¼©æ­ç¤ºäº†ç»å…¸åŸºå‡†æµ‹è¯•æ‰€å¿½è§†çš„åˆ†å¸ƒå¼±ç‚¹ã€‚è¿™äº›å‘ç°å°†æ— æŸå‹ç¼©å®šä½ä¸ºä¸€ä¸ªæœ‰åŸåˆ™çš„ä»»åŠ¡ï¼Œå®ƒè¡¥å……å¹¶æ‰©å±•äº†ç°æœ‰çš„æ—¶é—´åºåˆ—å»ºæ¨¡è¯„ä¼°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.21002v1">PDF</a> 24 pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä»¥æ— æŸå‹ç¼©ä½œä¸ºè¯„ä¼°æ—¶é—´åºåˆ—æ¨¡å‹çš„æ–°èŒƒå¼ï¼Œè¯¥èŒƒå¼åŸºäºé¦™å†œçš„æºç¼–ç å®šç†ï¼Œå»ºç«‹æœ€ä¼˜å‹ç¼©é•¿åº¦ä¸è´Ÿå¯¹æ•°ä¼¼ç„¶ä¹‹é—´çš„ç›´æ¥ç­‰ä»·å…³ç³»ï¼Œä¸ºæ¨¡å‹å®¹é‡æä¾›äº†ä¸¥æ ¼ä¸”ç»Ÿä¸€çš„ä¿¡æ¯ç†è®ºæ ‡å‡†ã€‚æ–‡ç« è¿˜å®šä¹‰äº†æ ‡å‡†åŒ–çš„è¯„ä¼°åè®®å’ŒæŒ‡æ ‡ï¼Œå¹¶å¼€æºäº†ä¸€ä¸ªå…¨é¢çš„è¯„ä¼°æ¡†æ¶TSCom-Benchï¼Œä½¿æ—¶é—´åºåˆ—æ¨¡å‹èƒ½è¿…é€Ÿé€‚åº”æ— æŸå‹ç¼©ã€‚å®éªŒè¯æ˜ï¼Œå‹ç¼©æ­ç¤ºäº†ç»å…¸åŸºå‡†æµ‹è¯•æœªèƒ½å‘ç°çš„åˆ†å¸ƒå¼±ç‚¹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ—¶é—´åºåˆ—æ¨¡å‹çš„è¯„ä¼°ä¼ ç»Ÿä¸Šé›†ä¸­åœ¨å››ä¸ªå…¸å‹ä»»åŠ¡ä¸Šï¼šé¢„æµ‹ã€æ’è¡¥ã€å¼‚å¸¸æ£€æµ‹å’Œåˆ†ç±»ã€‚</li>
<li>è¿™äº›ä»»åŠ¡ä¸»è¦è¯„ä¼°ç‰¹å®šä»»åŠ¡çš„æ€§èƒ½ï¼Œå¹¶æœªä¸¥æ ¼æµ‹é‡æ¨¡å‹æ˜¯å¦æ•æ‰æ•°æ®çš„å…¨ç”Ÿæˆåˆ†å¸ƒã€‚</li>
<li>æ— æŸå‹ç¼©è¢«å¼•å…¥ä½œä¸ºè¯„ä¼°æ—¶é—´åºåˆ—æ¨¡å‹çš„æ–°èŒƒå¼ï¼ŒåŸºäºé¦™å†œçš„æºç¼–ç å®šç†ã€‚</li>
<li>æ— æŸå‹ç¼©å»ºç«‹æœ€ä¼˜å‹ç¼©é•¿åº¦ä¸è´Ÿå¯¹æ•°ä¼¼ç„¶ä¹‹é—´çš„ç›´æ¥ç­‰ä»·ï¼Œæä¾›ä¿¡æ¯ç†è®ºçš„æ ‡å‡†ã€‚</li>
<li>æ–‡ç« å®šä¹‰äº†æ ‡å‡†åŒ–çš„è¯„ä¼°åè®®å’ŒæŒ‡æ ‡ï¼Œå¹¶å¼€æºè¯„ä¼°æ¡†æ¶TSCom-Benchã€‚</li>
<li>å®éªŒæ˜¾ç¤ºï¼Œæ— æŸå‹ç¼©èƒ½æ­ç¤ºè¢«ç»å…¸åŸºå‡†æµ‹è¯•å¿½ç•¥çš„åˆ†å¸ƒå¼±ç‚¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.21002">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-0d29d6508f091a26c7389f95952222e4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684922&auth_key=1760684922-0-0-1bf75ed012b49b8422287509c60efa8e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c4b05a39c54d7a5c702e8b6a036b3736~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684929&auth_key=1760684929-0-0-0d120186b7d86123c1d1b8bfe1da852e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-920d44e23fb89f65b7beefbc8e3b16a0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684936&auth_key=1760684936-0-0-b589ac3f8a3c616606ff56ba0a752922&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-28/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-28/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-28/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-841c4ac40fb96c2b1c0d5a52a7cd602b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684943&auth_key=1760684943-0-0-e1c2afbc1575e640fd0c05e47df73eb6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-28  SciReasoner Laying the Scientific Reasoning Ground Across Disciplines
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-28
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-28/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-93d89661c3fcaa224f6739e8ecf6c8fe~resize:0:q75.jpg?source=1f5c5e47&expiration=1760687921&auth_key=1760687921-0-0-e59d19d5424a265e0256bf86711056c3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-28  Unlocking Financial Insights An advanced Multimodal Summarization with   Multimodal Output Framework for Financial Advisory Videos
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-28
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">30341.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
