<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="3DGS">
    <meta name="description" content="3DGS 方向最新论文已更新，请持续关注 Update in 2025-09-28  Gaussian splatting holography">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>3DGS | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic-private.zhihu.com/v2-d2865c189a251d46fd0c4be9ce253c50~resize:0:q75.jpg?source=1f5c5e47&expiration=1760037583&auth_key=1760037583-0-0-77f2ec003eb4d5ebeb5b26a036bf8a49&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">3DGS</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/3DGS/">
                                <span class="chip bg-color">3DGS</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/3DGS/" class="post-category">
                                3DGS
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-09-28
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-10-11
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    19.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    79 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-09-28-更新"><a href="#2025-09-28-更新" class="headerlink" title="2025-09-28 更新"></a>2025-09-28 更新</h1><h2 id="Gaussian-splatting-holography"><a href="#Gaussian-splatting-holography" class="headerlink" title="Gaussian splatting holography"></a>Gaussian splatting holography</h2><p><strong>Authors:Shuhe Zhang, Liangcai Cao</strong></p>
<p>In-line holography offers high space-bandwidth product imaging with a simplified lens-free optical system. However, in-line holographic reconstruction is troubled by twin images arising from the Hermitian symmetry of complex fields. Twin images disrupt the reconstruction in solving the ill-posed phase retrieval problem. The known parameters are less than the unknown parameters, causing phase ambiguities. State-of-the-art deep-learning or non-learning methods face challenges in balancing data fidelity with twin-image disturbance. We propose the Gaussian splatting holography (GSH) for twin-image-suppressed holographic reconstruction. GSH uses Gaussian splatting for optical field representation and compresses the number of unknown parameters by a maximum of 15 folds, transforming the original ill-posed phase retrieval into a well-posed one with reduced phase ambiguities. Additionally, the Gaussian splatting tends to form sharp patterns rather than those with noisy twin-image backgrounds as each Gaussian has a spatially slow-varying profile. Experiments show that GSH achieves constraint-free recovery for in-line holography with accuracy comparable to state-of-the-art constraint-based methods, with an average peak signal-to-noise ratio equal to 26 dB, and structure similarity equal to 0.8. Combined with total variation, GSH can be further improved, obtaining a peak signal-to-noise ratio of 31 dB, and a high compression ability of up to 15 folds. </p>
<blockquote>
<p>内联全息术通过简化的无透镜光学系统提供高空间带宽积成像。然而，内联全息重建受到来自复场Hermitian对称性的孪生图像的困扰。孪生图像破坏了解决不适定相位检索问题时的重建。已知参数少于未知参数，导致相位模糊。现有的最先进的深度学习或非学习方法在平衡数据保真与孪生图像干扰方面面临挑战。我们提出高斯斑点全息术（GSH）用于抑制孪生图像的全息重建。GSH使用高斯斑点进行光学场表示，并通过最多减少15倍未知参数的数量，将原始的不适定相位检索问题转变为具有较少相位模糊的良好定位问题。此外，高斯斑点倾向于形成清晰的图案，而不是带有嘈杂孪生图像背景的图案，因为每个高斯都具有空间缓慢变化的轮廓。实验表明，GSH实现了内联全息图的约束自由恢复，其准确性可与最先进的基于约束的方法相媲美，平均峰值信噪比达到26分贝，结构相似性为0.8。结合全变，GSH可以进一步改进，获得峰值信噪比31分贝，并具备高达15倍的出色压缩能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.20774v1">PDF</a> </p>
<p><strong>摘要</strong><br>    全息成像技术利用无透镜光学系统实现高空间带宽产品成像。然而，全息重建受到复场Hermite对称性的困扰，导致出现镜像干扰问题。由于已知参数少于未知参数，相位重建变得不明确。最新的深度学习方法或非学习方法在平衡数据保真度和镜像干扰方面面临挑战。本文提出高斯喷溅全息术（GSH）用于抑制镜像干扰的全息重建。GSH利用高斯喷溅表示光学场，将未知参数数量最多减少15倍，将原来的病态相位检索问题转化为具有较少相位模糊的良好相位检索问题。此外，高斯喷溅倾向于形成清晰模式，避免噪声较大的镜像背景干扰。实验表明，GSH实现无约束在线全息恢复，准确率与先进的约束方法相当，平均峰值信噪比达到26分贝，结构相似性为0.8。与总变差相结合时，GSH性能进一步提升，峰值信噪比可达31分贝，压缩能力高达15倍。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>在全息成像中，镜像干扰问题是由于复场的Hermite对称性引起的。</li>
<li>已知参数少于未知参数导致相位重建变得不明确。</li>
<li>高斯喷溅全息术（GSH）通过减少未知参数数量来简化全息重建问题。</li>
<li>GSH可将病态相位检索问题转化为良好相位检索问题，降低相位模糊。</li>
<li>实验结果显示，GSH在在线全息恢复方面表现出色，准确率与现有方法相当。</li>
<li>结合总变差技术时，GSH的峰值信噪比和压缩能力得到进一步提升。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.20774">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-fff92a5568841fa148f727c9dcd2f2e9~resize:0:q75.jpg?source=1f5c5e47&expiration=1760037590&auth_key=1760037590-0-0-57411b416f957c59261affe14e296ac7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-97ee81314b77f0b6264531eaa01d1832~resize:0:q75.jpg?source=1f5c5e47&expiration=1760037597&auth_key=1760037597-0-0-e6b1c77abb4c9d6671a075e70244bd62&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-123fa47eebd9c6ebe121c2983327ac9f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760037604&auth_key=1760037604-0-0-f5d412901b6e3db9ede6f38d28a9a5f6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="SeHDR-Single-Exposure-HDR-Novel-View-Synthesis-via-3D-Gaussian-Bracketing"><a href="#SeHDR-Single-Exposure-HDR-Novel-View-Synthesis-via-3D-Gaussian-Bracketing" class="headerlink" title="SeHDR: Single-Exposure HDR Novel View Synthesis via 3D Gaussian   Bracketing"></a>SeHDR: Single-Exposure HDR Novel View Synthesis via 3D Gaussian   Bracketing</h2><p><strong>Authors:Yiyu Li, Haoyuan Wang, Ke Xu, Gerhard Petrus Hancke, Rynson W. H. Lau</strong></p>
<p>This paper presents SeHDR, a novel high dynamic range 3D Gaussian Splatting (HDR-3DGS) approach for generating HDR novel views given multi-view LDR images. Unlike existing methods that typically require the multi-view LDR input images to be captured from different exposures, which are tedious to capture and more likely to suffer from errors (e.g., object motion blurs and calibration&#x2F;alignment inaccuracies), our approach learns the HDR scene representation from multi-view LDR images of a single exposure. Our key insight to this ill-posed problem is that by first estimating Bracketed 3D Gaussians (i.e., with different exposures) from single-exposure multi-view LDR images, we may then be able to merge these bracketed 3D Gaussians into an HDR scene representation. Specifically, SeHDR first learns base 3D Gaussians from single-exposure LDR inputs, where the spherical harmonics parameterize colors in a linear color space. We then estimate multiple 3D Gaussians with identical geometry but varying linear colors conditioned on exposure manipulations. Finally, we propose the Differentiable Neural Exposure Fusion (NeEF) to integrate the base and estimated 3D Gaussians into HDR Gaussians for novel view rendering. Extensive experiments demonstrate that SeHDR outperforms existing methods as well as carefully designed baselines. </p>
<blockquote>
<p>本文提出了SeHDR，这是一种新型的高动态范围3D高斯拼贴（HDR-3DGS）方法，用于根据多视角LDR图像生成HDR新颖视图。与通常需要从不同曝光的多视角LDR输入图像捕获现有方法不同，我们的方法可以从单曝光的多视角LDR图像中学习HDR场景表示。对于这个不适定问题的关键见解是，首先通过估计单曝光多视角LDR图像的括号3D高斯（即具有不同曝光的）来合并这些括号中的3D高斯值，然后将其合并成HDR场景表示。具体来说，SeHDR首先从单曝光LDR输入中学习基础3D高斯值，其中球面谐波在线性颜色空间中描述颜色。然后，我们估计具有相同几何形状但随曝光操作变化的不同线性颜色的多个3D高斯值。最后，我们提出了可区分的神经曝光融合（NeEF）方法，将基础和估计的3D高斯值集成到HDR高斯值中，用于新的视图渲染。大量实验表明，SeHDR的性能优于现有方法和精心设计的基线。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.20400v1">PDF</a> ICCV 2025 accepted paper</p>
<p><strong>摘要</strong></p>
<p>本文提出一种名为SeHDR的新型高动态范围三维高斯点喷绘技术（HDR-3DGS），可从多视角的低动态范围图像生成高动态范围的新视角图像。与其他方法不同，我们的方法不需要从不同曝光的多视角LDR图像中获取输入，避免了捕捉时的繁琐和可能出现的误差（如物体运动模糊和校准&#x2F;对齐不准确）。我们的关键见解是，首先通过单曝光多视角LDR图像估计不同曝光的括号三维高斯，然后将其合并为HDR场景表示。具体来说，SeHDR首先从单曝光LDR输入中学习基础三维高斯，使用球面谐波在线性色彩空间中描述颜色。然后，我们估计具有相同几何形状但不同线性颜色的多个三维高斯，并根据曝光调整进行调整。最后，我们提出了可微神经曝光融合（NeEF）技术，将基础估计的三维高斯融合为HDR高斯，用于新型视角渲染。大量实验表明，SeHDR在性能上优于现有方法和精心设计的基线。</p>
<p><strong>要点</strong></p>
<ol>
<li>SeHDR是一种新型HDR-3DGS技术，可从多视角LDR图像生成HDR新视角。</li>
<li>与其他方法不同，SeHDR从单曝光的多视角LDR图像中学习HDR场景表示，简化捕捉过程并减少误差。</li>
<li>SeHDR通过学习基础三维高斯和估计的不同曝光三维高斯来解决这一问题。</li>
<li>利用球面谐波在线性色彩空间中描述颜色。</li>
<li>引入可微神经曝光融合（NeEF）技术，将基础与估计的三维高斯融合为HDR高斯。</li>
<li>实验证明，SeHDR在性能上优于现有方法和基线。</li>
<li>SeHDR为生成HDR新视角提供了一种有效、高性能的方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.20400">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-fb5779edacc87019e680c09ed77f7bdf~resize:0:q75.jpg?source=1f5c5e47&expiration=1760037611&auth_key=1760037611-0-0-a4a0e25a66d10ce8d77d5d466692a246&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-231f23815eaa9bcb695bcf5189af6b05~resize:0:q75.jpg?source=1f5c5e47&expiration=1760037619&auth_key=1760037619-0-0-a58cfff9eb75077a91a15c0272bc5e8d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d3c8ff06d5e6b02c27d742dfd4e0fb3f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760037625&auth_key=1760037625-0-0-5e03ec4e476e5e73384190c458a30931&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="4D-Driving-Scene-Generation-With-Stereo-Forcing"><a href="#4D-Driving-Scene-Generation-With-Stereo-Forcing" class="headerlink" title="4D Driving Scene Generation With Stereo Forcing"></a>4D Driving Scene Generation With Stereo Forcing</h2><p><strong>Authors:Hao Lu, Zhuang Ma, Guangfeng Jiang, Wenhang Ge, Bohan Li, Yuzhan Cai, Wenzhao Zheng, Yunpeng Zhang, Yingcong Chen</strong></p>
<p>Current generative models struggle to synthesize dynamic 4D driving scenes that simultaneously support temporal extrapolation and spatial novel view synthesis (NVS) without per-scene optimization. Bridging generation and novel view synthesis remains a major challenge. We present PhiGenesis, a unified framework for 4D scene generation that extends video generation techniques with geometric and temporal consistency. Given multi-view image sequences and camera parameters, PhiGenesis produces temporally continuous 4D Gaussian splatting representations along target 3D trajectories. In its first stage, PhiGenesis leverages a pre-trained video VAE with a novel range-view adapter to enable feed-forward 4D reconstruction from multi-view images. This architecture supports single-frame or video inputs and outputs complete 4D scenes including geometry, semantics, and motion. In the second stage, PhiGenesis introduces a geometric-guided video diffusion model, using rendered historical 4D scenes as priors to generate future views conditioned on trajectories. To address geometric exposure bias in novel views, we propose Stereo Forcing, a novel conditioning strategy that integrates geometric uncertainty during denoising. This method enhances temporal coherence by dynamically adjusting generative influence based on uncertainty-aware perturbations. Our experimental results demonstrate that our method achieves state-of-the-art performance in both appearance and geometric reconstruction, temporal generation and novel view synthesis (NVS) tasks, while simultaneously delivering competitive performance in downstream evaluations. Homepage is at \href{<a target="_blank" rel="noopener" href="https://jiangxb98.github.io/PhiGensis%7D%7BPhiGensis%7D">https://jiangxb98.github.io/PhiGensis}{PhiGensis}</a>. </p>
<blockquote>
<p>当前生成模型在合成动态四维驾驶场景方面面临挑战，这些场景需要同时支持时间外推和空间新颖视图合成（NVS），而无需针对每个场景进行优化。生成和新颖视图合成之间的桥梁仍然是一个主要挑战。我们提出了PhiGenesis，这是一个用于四维场景生成的统一框架，它结合了视频生成技术，实现了几何和时间一致性。给定多视图图像序列和相机参数，PhiGenesis可以沿着目标三维轨迹生成时间上连续的四维高斯溅射表示。在第一阶段，PhiGenesis利用预训练的视频VAE和新颖的范围视图适配器，实现从多视图图像的前馈四维重建。此架构支持单帧或视频输入，并输出完整的四维场景，包括几何、语义和运动。在第二阶段，PhiGenesis引入了一个受几何指导的视频扩散模型，使用渲染的历史四维场景作为先验来根据轨迹生成未来视图。为了解决新颖视图中的几何曝光偏差问题，我们提出了立体强制（Stereo Forcing）这一新颖的条件策略，它在去噪过程中整合了几何不确定性。这种方法通过根据不确定性感知扰动动态调整生成影响，增强了时间连贯性。我们的实验结果表明，我们的方法在外观和几何重建、时间生成和新颖视图合成（NVS）任务方面达到了最新性能水平，同时在下游评估中表现出竞争力。主页是<a target="_blank" rel="noopener" href="https://jiangxb98.github.io/PhiGensis">PhiGenesis</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.20251v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了一种名为PhiGenesis的统一框架，用于4D场景生成。该框架扩展了视频生成技术，具有几何和时间一致性。PhiGenesis可以从多视角图像序列和相机参数出发，生成时间上连续的4D高斯摊铺表示，沿着目标3D轨迹。它采用预训练的视频VAE和新型范围视图适配器，支持单帧或视频输入，并输出完整的4D场景，包括几何、语义和运动。第二阶段引入了几何指导的视频扩散模型，使用渲染的历史4D场景作为先验来生成未来视图。为解决新型视图中的几何曝光偏差，提出了Stereo Forcing这一新型条件策略，在降噪过程中整合几何不确定性。此方法通过基于不确定性感知扰动的动态调整生成影响，增强了时间连贯性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PhiGenesis是一个统一框架，用于4D场景生成，结合了视频生成技术与几何和时间的一致性。</li>
<li>利用多视角图像序列和相机参数生成4D高斯摊铺表示。</li>
<li>采用预训练的视频VAE和范围视图适配器进行4D重建。</li>
<li>支持单帧或视频输入，输出包括几何、语义和运动的完整4D场景。</li>
<li>引入几何指导的视频扩散模型，使用历史4D场景作为未来视图的先验。</li>
<li>提出Stereo Forcing策略来解决新型视图中的几何曝光偏差。</li>
<li>通过动态调整基于不确定性感知的扰动增强时间连贯性，达到先进性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.20251">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-82664f4f0d8c3114c482d82cdcf2aed4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760037633&auth_key=1760037633-0-0-b09bac2585391c981a20ad6952575f61&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-91265e34f81bf6c2676c3c343be743d1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760037640&auth_key=1760037640-0-0-44ee3958127645412054cc926273f6e3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b326c26539bbae1c06efb1b231f6c241~resize:0:q75.jpg?source=1f5c5e47&expiration=1760037648&auth_key=1760037648-0-0-0a8535041a75e81bfd8c5e21dc901675&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-885b7eeefee57f6ec5147f64bb016e9c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760037656&auth_key=1760037656-0-0-cb4855fb597a9c08d6bbfc3cffc89f5b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-90f77a423f1976bd31cc905a6f4ff750~resize:0:q75.jpg?source=1f5c5e47&expiration=1760037662&auth_key=1760037662-0-0-50feaf799e5edf94951d0df648c20dfb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d2865c189a251d46fd0c4be9ce253c50~resize:0:q75.jpg?source=1f5c5e47&expiration=1760037670&auth_key=1760037670-0-0-315869a45eb2965f701da51d15a14921&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b554aa4c2f4d4fb6ff6a5634a36bbe19~resize:0:q75.jpg?source=1f5c5e47&expiration=1760037677&auth_key=1760037677-0-0-2578d1ccad0ac25806f9b5aa3835626e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="PU-Gaussian-Point-Cloud-Upsampling-using-3D-Gaussian-Representation"><a href="#PU-Gaussian-Point-Cloud-Upsampling-using-3D-Gaussian-Representation" class="headerlink" title="PU-Gaussian: Point Cloud Upsampling using 3D Gaussian Representation"></a>PU-Gaussian: Point Cloud Upsampling using 3D Gaussian Representation</h2><p><strong>Authors:Mahmoud Khater, Mona Strauss, Philipp von Olshausen, Alexander Reiterer</strong></p>
<p>Point clouds produced by 3D sensors are often sparse and noisy, posing challenges for tasks requiring dense and high-fidelity 3D representations. Prior work has explored both implicit feature-based upsampling and distance-function learning to address this, but often at the expense of geometric interpretability or robustness to input sparsity. To overcome these limitations, we propose PU-Gaussian, a novel upsampling network that models the local neighborhood around each point using anisotropic 3D Gaussian distributions. These Gaussians capture the underlying geometric structure, allowing us to perform upsampling explicitly in the local geometric domain by direct point sampling. The sampling process generates a dense, but coarse, point cloud. A subsequent refinement network adjusts the coarse output to produce a more uniform distribution and sharper edges. We perform extensive testing on the PU1K and PUGAN datasets, demonstrating that PU-Gaussian achieves state-of-the-art performance. We make code and model weights publicly available at <a target="_blank" rel="noopener" href="https://github.com/mvg-inatech/PU-Gaussian.git">https://github.com/mvg-inatech/PU-Gaussian.git</a>. </p>
<blockquote>
<p>由3D传感器产生的点云通常是稀疏且嘈杂的，这给需要密集和高保真度的3D表示的任务带来了挑战。早期的工作已经探索了基于隐特征的上采样和距离函数学习来解决这个问题，但这往往以牺牲几何解释性或对输入稀疏性的稳健性为代价。为了克服这些局限性，我们提出了PU-Gaussian，这是一种新型的上采样网络，它利用各向异性的3D高斯分布对每个点的局部邻域进行建模。这些高斯数捕捉了潜在的几何结构，使我们能够在局部几何域中通过直接点采样显式执行上采样。采样过程生成一个密集但粗糙的点云。随后的细化网络调整粗略输出，以产生更均匀的分部率和更清晰的边缘。我们在PU1K和PUGAN数据集上进行了广泛的测试，证明了PU-Gaussian实现了最先进的性能。我们将代码和模型权重公开发布在<a target="_blank" rel="noopener" href="https://github.com/mvg-inatech/PU-Gaussian.git%E4%B8%8A%E3%80%82">https://github.com/mvg-inatech/PU-Gaussian.git上。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.20207v1">PDF</a> Accepted for the ICCV 2025 e2e3D Workshop. To be published in the   Proceedings of the IEEE&#x2F;CVF International Conference on Computer Vision   Workshops (ICCVW)</p>
<p><strong>Summary</strong></p>
<p>本文提出了一种名为PU-Gaussian的新型点云上采样网络，通过利用各向异性的三维高斯分布对点云的局部邻域进行建模，解决了稀疏和噪声点云带来的挑战。该网络通过直接点采样在局部几何域中执行上采样，生成密集但粗糙的点云。随后，一个精炼网络对粗输出进行调整，以产生更均匀分布和更清晰边缘的点云。在PU1K和PUGAN数据集上的测试表明，PU-Gaussian达到了最先进的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PU-Gaussian网络解决了点云稀疏和噪声问题。</li>
<li>该网络利用各向异性的三维高斯分布对点云的局部邻域进行建模。</li>
<li>通过直接点采样在局部几何域中执行上采样，生成密集但粗糙的点云。</li>
<li>有一个后续的网络用于调整粗输出，产生更均匀分布和更清晰边缘的点云。</li>
<li>在PU1K和PUGAN数据集上的测试表明，PU-Gaussian性能达到最新水平。</li>
<li>公开了代码和模型权重以供使用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.20207">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-9c153da9eca07ad291bf8d4b1d51579d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760037685&auth_key=1760037685-0-0-3fa9d738f9c1c6a24e1abf19972a7ff4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7fd9b74372a97da24dab59fbb6ec80e4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760037692&auth_key=1760037692-0-0-ab00eff6b61df912a40f81747815b490&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-57ffd162b3c8e26c36c3e66f40e30d06~resize:0:q75.jpg?source=1f5c5e47&expiration=1760037699&auth_key=1760037699-0-0-2df52ab22063366231ef8f79dcbb80cf&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-97bb4fe5ea9fad38b800de941fc2cddc~resize:0:q75.jpg?source=1f5c5e47&expiration=1760037707&auth_key=1760037707-0-0-57c271cc38db265a860a0d131289f535&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-985451e438efde79ad345b7510308671~resize:0:q75.jpg?source=1f5c5e47&expiration=1760037714&auth_key=1760037714-0-0-fb54afee58d1c147bb702fe35bee9495&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="GS-RoadPatching-Inpainting-Gaussians-via-3D-Searching-and-Placing-for-Driving-Scenes"><a href="#GS-RoadPatching-Inpainting-Gaussians-via-3D-Searching-and-Placing-for-Driving-Scenes" class="headerlink" title="GS-RoadPatching: Inpainting Gaussians via 3D Searching and Placing for   Driving Scenes"></a>GS-RoadPatching: Inpainting Gaussians via 3D Searching and Placing for   Driving Scenes</h2><p><strong>Authors:Guo Chen, Jiarun Liu, Sicong Du, Chenming Wu, Deqi Li, Shi-Sheng Huang, Guofeng Zhang, Sheng Yang</strong></p>
<p>This paper presents GS-RoadPatching, an inpainting method for driving scene completion by referring to completely reconstructed regions, which are represented by 3D Gaussian Splatting (3DGS). Unlike existing 3DGS inpainting methods that perform generative completion relying on 2D perspective-view-based diffusion or GAN models to predict limited appearance or depth cues for missing regions, our approach enables substitutional scene inpainting and editing directly through the 3DGS modality, extricating it from requiring spatial-temporal consistency of 2D cross-modals and eliminating the need for time-intensive retraining of Gaussians. Our key insight is that the highly repetitive patterns in driving scenes often share multi-modal similarities within the implicit 3DGS feature space and are particularly suitable for structural matching to enable effective 3DGS-based substitutional inpainting. Practically, we construct feature-embedded 3DGS scenes to incorporate a patch measurement method for abstracting local context at different scales and, subsequently, propose a structural search method to find candidate patches in 3D space effectively. Finally, we propose a simple yet effective substitution-and-fusion optimization for better visual harmony. We conduct extensive experiments on multiple publicly available datasets to demonstrate the effectiveness and efficiency of our proposed method in driving scenes, and the results validate that our method achieves state-of-the-art performance compared to the baseline methods in terms of both quality and interoperability. Additional experiments in general scenes also demonstrate the applicability of the proposed 3D inpainting strategy. The project page and code are available at: <a target="_blank" rel="noopener" href="https://shanzhaguoo.github.io/GS-RoadPatching/">https://shanzhaguoo.github.io/GS-RoadPatching/</a> </p>
<blockquote>
<p>本文介绍了GS-RoadPatching，这是一种通过参考由三维高斯拼贴（3DGS）表示的完全重建区域来完成驾驶场景补全的内填方法。与传统的依赖于二维透视视图扩散或生成对抗网络（GAN）模型预测缺失区域的有限外观或深度线索的3DGS内填方法不同，我们的方法能够直接通过3DGS模式进行替代场景内填和编辑，无需依赖二维跨模态的时空一致性，并消除了对高斯模型耗时耗力的重新训练需求。我们的关键见解是，驾驶场景中高度重复的模式在隐式三维高斯拼贴特征空间内通常具有多模态相似性，非常适合结构匹配，以实现有效的基于三维高斯拼贴的替代性内填。在实践中，我们构建了特征嵌入的3DGS场景，并开发了一种贴片测量方法，以在不同尺度上抽象局部上下文，然后提出了一种结构搜索方法，以在三维空间中找到有效的候选贴片。最后，我们提出了一种简单而有效的替代和融合优化方法，以实现更好的视觉和谐。我们在多个公开数据集上进行了大量实验，以证明我们提出的方法在驾驶场景中的有效性和效率。结果验证了我们的方法在质量和可操作性方面均达到了最新水平。在一般场景中的额外实验也证明了所提出的3D内填策略的应用性。项目页面和代码可通过以下链接访问：<a target="_blank" rel="noopener" href="https://shanzhaguoo.github.io/GS-RoadPatching/">https://shanzhaguoo.github.io/GS-RoadPatching/</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.19937v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了GS-RoadPatching方法，这是一种基于三维高斯喷溅（3DGS）的驾驶场景补全方法。不同于依赖二维透视视角扩散或GAN模型的现有3DGS补全方法，GS-RoadPatching直接在3DGS模式下进行替代场景补全和编辑，无需二维跨模态的空间时间一致性，并消除了高斯重训练的时间密集需求。其主要思想是利用驾驶场景中高度重复模式的隐式多模态相似性进行结构匹配，实现有效的基于结构匹配的替代补全。本文构建特征嵌入的3DGS场景，采用补丁测量方法在不同尺度上抽象局部上下文，并提出一种有效的结构搜索方法，在三维空间中找到候选补丁。最后，通过大量实验验证了该方法在驾驶场景中的有效性和高效性，达到当前领先水平。此外，在一般场景中的实验也证明了该方法的适用性。相关内容和代码可通过链接访问：[shanzhaguoo.github.io&#x2F;GS-RoadPatching&#x2F;]查看。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>GS-RoadPatching是一种基于三维高斯喷溅（3DGS）的驾驶场景补全方法。</li>
<li>该方法直接在3DGS模式下进行替代场景补全和编辑，避免二维跨模态的空间时间一致性问题。</li>
<li>主要利用驾驶场景中高度重复模式的隐式多模态相似性进行结构匹配以实现有效补全。</li>
<li>构建特征嵌入的3DGS场景并采用补丁测量方法在不同尺度上抽象局部上下文。</li>
<li>提出结构搜索方法和替代融合优化技术来寻找合适的补丁并进行视觉融合。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.19937">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-92cf377951a9d6ada899befcaae1161b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760037721&auth_key=1760037721-0-0-4d579c04dc2eb1fe92426d3a8ab76ad8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-618886cb1f082e4b4cd40f168097fb6b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760037729&auth_key=1760037729-0-0-b03424878b16764064f928251a03ad90&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-67faef39d25538c7f655a3217a8ce5da~resize:0:q75.jpg?source=1f5c5e47&expiration=1760037736&auth_key=1760037736-0-0-93e4dd7daebf3935578113009e27dda5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9b6540e0810e2969c0201d2640ad0b6e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760037785&auth_key=1760037785-0-0-82eb831483dfe13003b7551cafd52aae&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-99def8b932f4c26aaea16fc1708b80eb~resize:0:q75.jpg?source=1f5c5e47&expiration=1760037791&auth_key=1760037791-0-0-80be2f589d2cf26d24b873eb0f21225b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="PolGS-Polarimetric-Gaussian-Splatting-for-Fast-Reflective-Surface-Reconstruction"><a href="#PolGS-Polarimetric-Gaussian-Splatting-for-Fast-Reflective-Surface-Reconstruction" class="headerlink" title="PolGS: Polarimetric Gaussian Splatting for Fast Reflective Surface   Reconstruction"></a>PolGS: Polarimetric Gaussian Splatting for Fast Reflective Surface   Reconstruction</h2><p><strong>Authors:Yufei Han, Bowen Tie, Heng Guo, Youwei Lyu, Si Li, Boxin Shi, Yunpeng Jia, Zhanyu Ma</strong></p>
<p>Efficient shape reconstruction for surfaces with complex reflectance properties is crucial for real-time virtual reality. While 3D Gaussian Splatting (3DGS)-based methods offer fast novel view rendering by leveraging their explicit surface representation, their reconstruction quality lags behind that of implicit neural representations, particularly in the case of recovering surfaces with complex reflective reflectance. To address these problems, we propose PolGS, a Polarimetric Gaussian Splatting model allowing fast reflective surface reconstruction in 10 minutes. By integrating polarimetric constraints into the 3DGS framework, PolGS effectively separates specular and diffuse components, enhancing reconstruction quality for challenging reflective materials. Experimental results on the synthetic and real-world dataset validate the effectiveness of our method. </p>
<blockquote>
<p>对于具有复杂反射属性的表面进行高效形状重建对实时虚拟现实至关重要。虽然基于三维高斯贴图（3DGS）的方法通过利用其明确的表面表示实现了快速的新视角渲染，但在重建质量方面仍落后于隐式神经表示，特别是在恢复具有复杂反射反射的表面时。为了解决这些问题，我们提出了PolGS，一种极坐标高斯贴图模型，可在10分钟内快速重建反射表面。通过将极坐标约束集成到3DGS框架中，PolGS有效地分离了镜面反射和漫反射成分，提高了对具有挑战性的反射材料的重建质量。在合成数据集和真实世界数据集上的实验结果验证了我们的方法的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.19726v1">PDF</a> Accepted by ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了针对具有复杂反射属性的表面进行高效形状重建的重要性，特别是在实时虚拟现实领域。针对现有技术如基于三维高斯喷溅（3DGS）的方法在重建质量上存在的缺陷，提出了一种名为PolGS的极坐标高斯喷溅模型。该模型通过将极坐标约束融入3DGS框架，成功分离了镜面反射和漫反射成分，有效提升了复杂反射材料的重建质量。实验结果表明，该方法在合成和真实数据集上均表现出优异效果，可在十分钟内实现快速反射表面重建。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>PolGS模型解决了具有复杂反射属性的表面在虚拟现实中的高效形状重建问题。</li>
<li>通过结合极坐标约束到三维高斯喷溅（3DGS）框架中，提升了重建质量。</li>
<li>PolGS模型能有效分离镜面反射和漫反射成分，特别适用于处理复杂反射材料。</li>
<li>PolGS模型可实现快速反射表面重建，处理时间仅需十分钟。</li>
<li>实验结果证明，该模型在合成和真实数据集上都有良好表现。</li>
<li>该模型的提出推动了虚拟现实领域中的表面重建技术向前发展。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.19726">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-ca613cebf073827bce799c79adfaf649~resize:0:q75.jpg?source=1f5c5e47&expiration=1760037798&auth_key=1760037798-0-0-77e85e47b076ed6d5a19387e39d5800c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-271dae328e44bbba367bb5b4ebcebd87~resize:0:q75.jpg?source=1f5c5e47&expiration=1760037806&auth_key=1760037806-0-0-ea3c3125a192ff7bb15f1c317b69f926&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1705d1a59472b1eed106799727c1f888~resize:0:q75.jpg?source=1f5c5e47&expiration=1760037813&auth_key=1760037813-0-0-82498e5e35131a1c36812f9a01e9689c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1376d3bcb7ed2999a830beb743c090f7~resize:0:q75.jpg?source=1f5c5e47&expiration=1760037820&auth_key=1760037820-0-0-9be7aab48f78016ea1df611a84af3c75&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-72677b1b9d5cde643fbeb37ad4c52422~resize:0:q75.jpg?source=1f5c5e47&expiration=1760037826&auth_key=1760037826-0-0-742ac3f51483660e1d59ea4d5b979a52&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="VolSplat-Rethinking-Feed-Forward-3D-Gaussian-Splatting-with-Voxel-Aligned-Prediction"><a href="#VolSplat-Rethinking-Feed-Forward-3D-Gaussian-Splatting-with-Voxel-Aligned-Prediction" class="headerlink" title="VolSplat: Rethinking Feed-Forward 3D Gaussian Splatting with   Voxel-Aligned Prediction"></a>VolSplat: Rethinking Feed-Forward 3D Gaussian Splatting with   Voxel-Aligned Prediction</h2><p><strong>Authors:Weijie Wang, Yeqing Chen, Zeyu Zhang, Hengyu Liu, Haoxiao Wang, Zhiyuan Feng, Wenkang Qin, Zheng Zhu, Donny Y. Chen, Bohan Zhuang</strong></p>
<p>Feed-forward 3D Gaussian Splatting (3DGS) has emerged as a highly effective solution for novel view synthesis. Existing methods predominantly rely on a pixel-aligned Gaussian prediction paradigm, where each 2D pixel is mapped to a 3D Gaussian. We rethink this widely adopted formulation and identify several inherent limitations: it renders the reconstructed 3D models heavily dependent on the number of input views, leads to view-biased density distributions, and introduces alignment errors, particularly when source views contain occlusions or low texture. To address these challenges, we introduce VolSplat, a new multi-view feed-forward paradigm that replaces pixel alignment with voxel-aligned Gaussians. By directly predicting Gaussians from a predicted 3D voxel grid, it overcomes pixel alignment’s reliance on error-prone 2D feature matching, ensuring robust multi-view consistency. Furthermore, it enables adaptive control over Gaussian density based on 3D scene complexity, yielding more faithful Gaussian point clouds, improved geometric consistency, and enhanced novel-view rendering quality. Experiments on widely used benchmarks including RealEstate10K and ScanNet demonstrate that VolSplat achieves state-of-the-art performance while producing more plausible and view-consistent Gaussian reconstructions. In addition to superior results, our approach establishes a more scalable framework for feed-forward 3D reconstruction with denser and more robust representations, paving the way for further research in wider communities. The video results, code and trained models are available on our project page: <a target="_blank" rel="noopener" href="https://lhmd.top/volsplat">https://lhmd.top/volsplat</a>. </p>
<blockquote>
<p>前馈三维高斯展布（3DGS）已成为一种用于合成新视角的高效解决方案。现有方法主要依赖于像素对齐的高斯预测范式，其中每个二维像素被映射到三维高斯上。我们重新思考这一广泛应用的方法并发现了几个固有的局限性：它使得重建的3D模型严重依赖于输入视角的数量，导致视角偏向的密度分布，并在源视角包含遮挡或低纹理时引入对齐误差。为了解决这些挑战，我们引入了VolSplat，这是一种新的前馈多视角范式，它用体素对齐的高斯替换了像素对齐。通过直接从预测的3D体素网格预测高斯值，它克服了像素对齐对容易出现错误的二维特征匹配的依赖，确保了稳健的多视角一致性。此外，它可以根据3D场景的复杂性对高斯密度进行自适应控制，产生更真实的高斯点云、改进几何一致性和提高新颖视角的渲染质量。在包括RealEstate10K和ScanNet在内的常用基准测试上的实验表明，VolSplat达到了最先进的性能，同时产生了更合理和视角一致的高斯重建。除了优越的结果外，我们的方法为前馈三维重建建立了一个更可扩展的框架，具有更密集和更稳健的表示形式，为更广泛的社区中的进一步研究铺平了道路。视频结果、代码和训练模型可在我们的项目页面找到：<a target="_blank" rel="noopener" href="https://lhmd.top/volsplat%E3%80%82">https://lhmd.top/volsplat。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.19297v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://lhmd.top/volsplat">https://lhmd.top/volsplat</a>, Code:   <a target="_blank" rel="noopener" href="https://github.com/ziplab/VolSplat">https://github.com/ziplab/VolSplat</a></p>
<p><strong>Summary</strong><br>     基于前馈的3D高斯点云（3DGS）已成为有效的视点合成解决方案。现有方法主要依赖像素对齐的高斯预测模式，存在依赖输入视角数量、视角密度分布偏差及对齐误差等问题。我们提出VolSplat，采用体素对齐高斯的新范式，预测三维体素网格上的高斯分布，克服像素对齐的缺陷，实现多视角一致性。此外，它可根据三维场景的复杂性自适应控制高斯密度，提高几何一致性及新视角渲染质量。在广泛使用的基准测试中表现卓越，实现更逼真、视角一致的高斯重建。同时建立可扩展的框架，为进一步的点云重建研究铺平道路。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>现有基于像素对齐的3DGS存在依赖输入视角数量、视角密度分布偏差及对齐误差等问题。</li>
<li>VolSplat采用体素对齐高斯的新范式，克服了像素对齐的缺陷，实现更稳定的多视角一致性。</li>
<li>VolSplat根据三维场景的复杂性自适应控制高斯密度，提高几何一致性及新视角渲染质量。</li>
<li>VolSplat在基准测试中表现卓越，实现了更逼真、视角一致的高斯重建，性能超越现有技术。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.19297">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-9793958c56a57fd64d90ce133614c132~resize:0:q75.jpg?source=1f5c5e47&expiration=1760037834&auth_key=1760037834-0-0-94f77c610c5f05c3104e2677bed72118&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-fb0f3126a327c619f44347569b95eee3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760037841&auth_key=1760037841-0-0-abf281fc530955b5b2c0d4709b35e422&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8d91bb6c1d222f7dd683f6928f964607~resize:0:q75.jpg?source=1f5c5e47&expiration=1760037849&auth_key=1760037849-0-0-88c3a1fcba593a0509e392c90130acab&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6a56313ea70206b46e6cb8878773900c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760037855&auth_key=1760037855-0-0-2f1c2dcc031f1aa57ba1dbeb325831df&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-eab02a07b1bf0e5ac6d2d5af237d2bde~resize:0:q75.jpg?source=1f5c5e47&expiration=1760037863&auth_key=1760037863-0-0-c9de97f34a1febd0bf06f7c608db3a30&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Lyra-Generative-3D-Scene-Reconstruction-via-Video-Diffusion-Model-Self-Distillation"><a href="#Lyra-Generative-3D-Scene-Reconstruction-via-Video-Diffusion-Model-Self-Distillation" class="headerlink" title="Lyra: Generative 3D Scene Reconstruction via Video Diffusion Model   Self-Distillation"></a>Lyra: Generative 3D Scene Reconstruction via Video Diffusion Model   Self-Distillation</h2><p><strong>Authors:Sherwin Bahmani, Tianchang Shen, Jiawei Ren, Jiahui Huang, Yifeng Jiang, Haithem Turki, Andrea Tagliasacchi, David B. Lindell, Zan Gojcic, Sanja Fidler, Huan Ling, Jun Gao, Xuanchi Ren</strong></p>
<p>The ability to generate virtual environments is crucial for applications ranging from gaming to physical AI domains such as robotics, autonomous driving, and industrial AI. Current learning-based 3D reconstruction methods rely on the availability of captured real-world multi-view data, which is not always readily available. Recent advancements in video diffusion models have shown remarkable imagination capabilities, yet their 2D nature limits the applications to simulation where a robot needs to navigate and interact with the environment. In this paper, we propose a self-distillation framework that aims to distill the implicit 3D knowledge in the video diffusion models into an explicit 3D Gaussian Splatting (3DGS) representation, eliminating the need for multi-view training data. Specifically, we augment the typical RGB decoder with a 3DGS decoder, which is supervised by the output of the RGB decoder. In this approach, the 3DGS decoder can be purely trained with synthetic data generated by video diffusion models. At inference time, our model can synthesize 3D scenes from either a text prompt or a single image for real-time rendering. Our framework further extends to dynamic 3D scene generation from a monocular input video. Experimental results show that our framework achieves state-of-the-art performance in static and dynamic 3D scene generation. </p>
<blockquote>
<p>生成虚拟环境的能力对于从游戏到物理人工智能领域（如机器人技术、自动驾驶和工业人工智能）的应用至关重要。当前基于学习的3D重建方法依赖于捕获的实时多视角数据的可用性，而这并非总是轻易可得。视频扩散模型的最新进展表现出了惊人的想象力能力，但它们的2D性质限制了其在机器人需要导航和与环境交互的模拟中的应用。在本文中，我们提出了一种自蒸馏框架，旨在将视频扩散模型中的隐式3D知识蒸馏成明确的3D高斯平铺（3DGS）表示，从而无需多视角训练数据。具体来说，我们用3DGS解码器扩充了典型的RGB解码器，该解码器由RGB解码器的输出进行监督。在这种方法中，3DGS解码器可以仅使用由视频扩散模型生成的合成数据进行训练。在推理时间，我们的模型可以从文本提示或单张图像中合成3D场景，以进行实时渲染。我们的框架进一步扩展到基于单目输入视频的动态3D场景生成。实验结果表明，我们的框架在静态和动态3D场景生成方面达到了最新技术水平。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.19296v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://research.nvidia.com/labs/toronto-ai/lyra/">https://research.nvidia.com/labs/toronto-ai/lyra/</a></p>
<p><strong>Summary</strong></p>
<p>本文提出了一种基于自蒸馏技术的框架，能够将视频扩散模型中的隐性三维知识转化为显式三维高斯飞溅（3DGS）表示形式，从而实现无需多视角训练数据的三维场景生成。该框架通过增加一个三维高斯飞溅解码器来扩展传统的RGB解码器，使其可以从文本提示或单幅图像中合成三维场景，支持实时渲染和从单目输入视频中生成动态三维场景。实验结果表明，该框架在静态和动态三维场景生成方面达到了最新技术水平。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>该框架能够将视频扩散模型中的隐性三维知识转化为显式三维高斯飞溅（3DGS）表示形式。</li>
<li>提出了一种基于自蒸馏技术的框架，不需要多视角训练数据。</li>
<li>通过增加一个三维高斯飞溅解码器来扩展传统的RGB解码器，以实现更好的三维场景生成。</li>
<li>支持从文本提示或单幅图像中合成三维场景，并能够实现实时渲染。</li>
<li>能够从单目输入视频中生成动态三维场景。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.19296">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-74e03d2958e791f851ff3cf6981d9211~resize:0:q75.jpg?source=1f5c5e47&expiration=1760037872&auth_key=1760037872-0-0-b6a4d70efbd02954bff241cf520df608&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-917fdaee759f1fad519ef9536b017a70~resize:0:q75.jpg?source=1f5c5e47&expiration=1760037879&auth_key=1760037879-0-0-9d657cf3abd7304ce43b49dd3d42c8fa&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f1fc2b81389fc2d1c3422dae6fd8ea5a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760037886&auth_key=1760037886-0-0-73cf983da6532d925136d6634f286835&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Seeing-Through-Reflections-Advancing-3D-Scene-Reconstruction-in-Mirror-Containing-Environments-with-Gaussian-Splatting"><a href="#Seeing-Through-Reflections-Advancing-3D-Scene-Reconstruction-in-Mirror-Containing-Environments-with-Gaussian-Splatting" class="headerlink" title="Seeing Through Reflections: Advancing 3D Scene Reconstruction in   Mirror-Containing Environments with Gaussian Splatting"></a>Seeing Through Reflections: Advancing 3D Scene Reconstruction in   Mirror-Containing Environments with Gaussian Splatting</h2><p><strong>Authors:Zijing Guo, Yunyang Zhao, Lin Wang</strong></p>
<p>Mirror-containing environments pose unique challenges for 3D reconstruction and novel view synthesis (NVS), as reflective surfaces introduce view-dependent distortions and inconsistencies. While cutting-edge methods such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) excel in typical scenes, their performance deteriorates in the presence of mirrors. Existing solutions mainly focus on handling mirror surfaces through symmetry mapping but often overlook the rich information carried by mirror reflections. These reflections offer complementary perspectives that can fill in absent details and significantly enhance reconstruction quality. To advance 3D reconstruction in mirror-rich environments, we present MirrorScene3D, a comprehensive dataset featuring diverse indoor scenes, 1256 high-quality images, and annotated mirror masks, providing a benchmark for evaluating reconstruction methods in reflective settings. Building on this, we propose ReflectiveGS, an extension of 3D Gaussian Splatting that utilizes mirror reflections as complementary viewpoints rather than simple symmetry artifacts, enhancing scene geometry and recovering absent details. Experiments on MirrorScene3D show that ReflectiveGaussian outperforms existing methods in SSIM, PSNR, LPIPS, and training speed, setting a new benchmark for 3D reconstruction in mirror-rich environments. </p>
<blockquote>
<p>包含镜子的环境为三维重建和新颖视图合成（NVS）带来了独特的挑战，因为反射表面引入了视图相关的失真和不一致性。虽然最前沿的方法，如神经辐射场（NeRF）和三维高斯喷绘（3DGS）在典型场景中表现优异，但在存在镜子的情况下，它们的性能会下降。现有解决方案主要通过对称映射处理镜子表面，但往往忽略了镜子反射所携带的丰富信息。这些反射提供了可以补充的视角，可以填补缺失的细节，并显著提高重建质量。为了推进在镜子丰富的环境中的三维重建，我们推出了MirrorScene3D数据集，其中包含多样化的室内场景、1256张高质量图像和注释的镜子掩膜，为反射环境中重建方法的评估提供了基准。在此基础上，我们提出了ReflectiveGS，它是三维高斯喷绘的一个扩展，利用镜子反射作为补充视角，而不是简单的对称产物，从而增强场景几何并恢复缺失的细节。在MirrorScene3D上的实验表明，ReflectiveGaussian在SSIM、PSNR、LPIPS和训练速度上均优于现有方法，为镜子丰富的环境中的三维重建设定了新的基准。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.18956v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该文介绍了在含有镜子的环境中进行3D重建和新型视图合成面临的挑战。现有方法如Neural Radiance Fields和3D Gaussian Splatting在处理镜子时性能下降。文章提出了MirrorScene3D数据集和ReflectiveGS方法，利用镜子反射作为补充视角，提高场景几何和恢复缺失细节的能力，为在镜子丰富的环境中进行3D重建设立了新的基准。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>镜子包含的环境对3D重建和新型视图合成带来独特挑战，因反射表面引入视图依赖的失真和不一致性。</li>
<li>现有方法在镜子存在时性能下降，主要通过对称映射处理镜子表面，但忽视了镜子反射所携带的丰富信息。</li>
<li>MirrorScene3D数据集提供室内场景的多样化图像和标注的镜子掩膜，为评估反射设置中的重建方法提供了基准。</li>
<li>ReflectiveGS方法利用镜子反射作为补充视角，提高场景几何和恢复缺失细节的能力。</li>
<li>ReflectiveGS是3D Gaussian Splatting的扩展，可以更好地利用镜子反射信息。</li>
<li>在MirrorScene3D上的实验表明，ReflectiveGaussian在SSIM、PSNR、LPIPS和训练速度上优于现有方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.18956">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-59ef441157f520f0d170006ac31fc7fd~resize:0:q75.jpg?source=1f5c5e47&expiration=1760037894&auth_key=1760037894-0-0-12d2746a292c5b2a00d249ea95699795&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-dacb7f30185d0ad15a107c6e4d34b02b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760037901&auth_key=1760037901-0-0-687cbd1c272cf180a8574ace31414fff&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f81cdbf33669c68eb401663d50edadd4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760037908&auth_key=1760037908-0-0-58eeeffcc45c60a2c60a1a0be24c4b87&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-10d7da124699a6f4627ec89f45d97718~resize:0:q75.jpg?source=1f5c5e47&expiration=1760037915&auth_key=1760037915-0-0-feabcfedf964e03d5ffa510017eee565&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-89684003f438d8b81377651af5552a03~resize:0:q75.jpg?source=1f5c5e47&expiration=1760037922&auth_key=1760037922-0-0-95b0c0eef1bd501281eb9eb2219ddacf&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="DeblurSplat-SfM-free-3D-Gaussian-Splatting-with-Event-Camera-for-Robust-Deblurring"><a href="#DeblurSplat-SfM-free-3D-Gaussian-Splatting-with-Event-Camera-for-Robust-Deblurring" class="headerlink" title="DeblurSplat: SfM-free 3D Gaussian Splatting with Event Camera for Robust   Deblurring"></a>DeblurSplat: SfM-free 3D Gaussian Splatting with Event Camera for Robust   Deblurring</h2><p><strong>Authors:Pengteng Li, Yunfan Lu, Pinhao Song, Weiyu Guo, Huizai Yao, F. Richard Yu, Hui Xiong</strong></p>
<p>In this paper, we propose the first Structure-from-Motion (SfM)-free deblurring 3D Gaussian Splatting method via event camera, dubbed DeblurSplat. We address the motion-deblurring problem in two ways. First, we leverage the pretrained capability of the dense stereo module (DUSt3R) to directly obtain accurate initial point clouds from blurred images. Without calculating camera poses as an intermediate result, we avoid the cumulative errors transfer from inaccurate camera poses to the initial point clouds’ positions. Second, we introduce the event stream into the deblur pipeline for its high sensitivity to dynamic change. By decoding the latent sharp images from the event stream and blurred images, we can provide a fine-grained supervision signal for scene reconstruction optimization. Extensive experiments across a range of scenes demonstrate that DeblurSplat not only excels in generating high-fidelity novel views but also achieves significant rendering efficiency compared to the SOTAs in deblur 3D-GS. </p>
<blockquote>
<p>在这篇论文中，我们提出了一种基于事件相机的无结构从运动（SfM）的模糊3D高斯点云散播方法，名为DeblurSplat。我们采用两种方法解决运动去模糊问题。首先，我们利用密集立体模块（DUSt3R）的预训练能力，直接从模糊图像中获取精确的点云初始位置。无需计算相机姿态作为中间结果，从而避免了由不准确的相机姿态对初始点云位置造成的累积误差传递。其次，我们将事件流引入去模糊流程中，利用其对动态变化的高灵敏度。通过解码事件流和模糊图像中的潜在清晰图像，我们可以为场景重建优化提供精细的监督信号。在多个场景的大量实验表明，DeblurSplat不仅在生成高保真新视角方面表现出色，而且在去模糊3D-GS的渲染效率方面也有显著提高。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.18898v1">PDF</a> </p>
<p><strong>Summary</strong><br>本文提出了一种基于事件相机的无结构从运动（SfM）的3D高斯扩展去模糊方法，名为DeblurSplat。它通过两种方式解决运动去模糊问题：一是利用密集立体模块（DUSt3R）的预训练能力直接从模糊图像中获取精确初始点云；二是将事件流引入去模糊管道，通过解码事件流和模糊图像中的潜在尖锐图像，为场景重建优化提供精细的监督信号。实验表明，DeblurSplat不仅在高保真度生成新型视图方面表现出色，而且在3D-GS去模糊方面与当前最佳技术相比实现了显著的渲染效率。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出了首个基于事件相机的无结构从运动（SfM）去模糊3D高斯扩展方法，名为DeblurSplat。</li>
<li>利用密集立体模块（DUSt3R）的预训练能力直接从模糊图像获取精确初始点云，避免了因相机姿态计算不准确导致的误差传递。</li>
<li>引入事件流到去模糊流程中，利用其对于动态变化的高敏感性。</li>
<li>通过解码事件流和模糊图像中的潜在尖锐图像，为场景重建优化提供精细监督信号。</li>
<li>DeblurSplat能够生成高保真度的新型视图。</li>
<li>与当前最佳技术相比，DeblurSplat在去模糊3D-GS方面实现了显著的渲染效率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.18898">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-a172d79a0e75866d8450b8db7081033d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760037930&auth_key=1760037930-0-0-257caaaa799ae5860b623f0f7b5a41ac&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6e68157f252cbb7f903b4fab30ee40d0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760037937&auth_key=1760037937-0-0-4a5141be3f9dbfcd589ec7222d005882&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a42cb0d50434bd853c5bbdd06766ed39~resize:0:q75.jpg?source=1f5c5e47&expiration=1760037944&auth_key=1760037944-0-0-66d69091baddefefcaaafe68532fe31a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-061cec6493bb220b6954bad6e7363dd8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760037951&auth_key=1760037951-0-0-6f614a62af8859be10c9dbbaa0b8a741&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5add0e33d4c8afcb060b9315273f69c3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760037958&auth_key=1760037958-0-0-b926462958469458b25333208b485c99&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6e6eac6276fd370b8d1dec08f0386a15~resize:0:q75.jpg?source=1f5c5e47&expiration=1760037964&auth_key=1760037964-0-0-8967a6432e98a81db96d595aba656c54&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b3da04b0bf1625c3f8f126f6570b54f7~resize:0:q75.jpg?source=1f5c5e47&expiration=1760037972&auth_key=1760037972-0-0-b1edcc6cefb9eb9addee2c67354ed363&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="FixingGS-Enhancing-3D-Gaussian-Splatting-via-Training-Free-Score-Distillation"><a href="#FixingGS-Enhancing-3D-Gaussian-Splatting-via-Training-Free-Score-Distillation" class="headerlink" title="FixingGS: Enhancing 3D Gaussian Splatting via Training-Free Score   Distillation"></a>FixingGS: Enhancing 3D Gaussian Splatting via Training-Free Score   Distillation</h2><p><strong>Authors:Zhaorui Wang, Yi Gu, Deming Zhou, Renjing Xu</strong></p>
<p>Recently, 3D Gaussian Splatting (3DGS) has demonstrated remarkable success in 3D reconstruction and novel view synthesis. However, reconstructing 3D scenes from sparse viewpoints remains highly challenging due to insufficient visual information, which results in noticeable artifacts persisting across the 3D representation. To address this limitation, recent methods have resorted to generative priors to remove artifacts and complete missing content in under-constrained areas. Despite their effectiveness, these approaches struggle to ensure multi-view consistency, resulting in blurred structures and implausible details. In this work, we propose FixingGS, a training-free method that fully exploits the capabilities of the existing diffusion model for sparse-view 3DGS reconstruction enhancement. At the core of FixingGS is our distillation approach, which delivers more accurate and cross-view coherent diffusion priors, thereby enabling effective artifact removal and inpainting. In addition, we propose an adaptive progressive enhancement scheme that further refines reconstructions in under-constrained regions. Extensive experiments demonstrate that FixingGS surpasses existing state-of-the-art methods with superior visual quality and reconstruction performance. Our code will be released publicly. </p>
<blockquote>
<p>近期，3D高斯拼贴（3DGS）在3D重建和新型视角合成方面取得了显著的成功。然而，从稀疏视角重建3D场景仍然是一个巨大的挑战，因为视觉信息不足，导致3D表示中持续存在明显的伪影。为了解决这一局限性，最近的方法采用生成先验来消除伪影并完成约束不足区域的缺失内容。尽管这些方法很有效，但它们很难保证多视角的一致性，导致结构模糊和不切实际的细节。在这项工作中，我们提出了FixinGS，这是一种无需训练的方法，充分利用现有扩散模型的潜力，用于稀疏视角的3DGS重建增强。FixinGS的核心是我们的蒸馏方法，它提供了更准确和跨视角一致性的扩散先验，从而实现了有效的伪影消除和补全。此外，我们还提出了一种自适应渐进增强方案，进一步改进了约束不足区域的重建。大量实验表明，FixinGS超越了现有最先进的方法，具有优越的视觉质量和重建性能。我们的代码将公开发布。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.18759v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>3DGS在三维重建和新型视角合成方面取得了显著的成功，但从未约束视角重建三维场景仍存在挑战，因为缺乏足够的视觉信息导致重建结果中仍存在显著的人工痕迹。为解决此问题，现有方法采用生成先验来消除人工痕迹并完成缺失内容的填充。然而，这些方法难以确保多视角的一致性，导致结构模糊和不切实际的细节。本研究提出一种无需训练的 FixingGS 方法，充分利用现有扩散模型的潜力进行稀疏视角的 3DGS 重建增强。其核心在于蒸馏法，可生成更准确且跨视角一致的扩散先验，从而实现有效的人工痕迹去除和补全。此外，还提出了一种自适应渐进增强方案，进一步改进了重建结果的不足约束区域。实验证明，FixingGS 在视觉质量和重建性能上超越了现有最先进的方法。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>3DGS在三维重建和新型视角合成上取得了显著进展。</li>
<li>从稀疏视角重建三维场景仍然具有挑战性，因为缺乏足够的视觉信息。</li>
<li>现有方法使用生成先验去除人工痕迹并填充缺失内容，但难以确保多视角一致性。</li>
<li>FixingGS是一种无需训练的方法，利用扩散模型的潜力增强稀疏视角的3DGS重建。</li>
<li>FixingGS的核心在于生成更准确和跨视角一致的扩散先验，以实现更好的人工痕迹去除和补全。</li>
<li>FixingGS还采用自适应渐进增强方案改进了重建结果的不足约束区域。</li>
<li>实验证明，FixingGS在视觉质量和重建性能上超越了现有方法。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.18759">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-865b4128d5c09e11a2af033baefa0d45~resize:0:q75.jpg?source=1f5c5e47&expiration=1760037980&auth_key=1760037980-0-0-35bc4834ebb5a79714d828f354671859&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d50cc46063359360260d649dec67b215~resize:0:q75.jpg?source=1f5c5e47&expiration=1760037987&auth_key=1760037987-0-0-58631ab21df36bd1049f3a58be711029&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e888594cd5af2a09cb50359122784cbc~resize:0:q75.jpg?source=1f5c5e47&expiration=1760037994&auth_key=1760037994-0-0-bc84869b43c767dd9089be46fbd07b1e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6b7a7aad654e4ca94902e53630bc2914~resize:0:q75.jpg?source=1f5c5e47&expiration=1760038001&auth_key=1760038001-0-0-8b5d1beeb972028865238e9aa0e9f309&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="SINGER-An-Onboard-Generalist-Vision-Language-Navigation-Policy-for-Drones"><a href="#SINGER-An-Onboard-Generalist-Vision-Language-Navigation-Policy-for-Drones" class="headerlink" title="SINGER: An Onboard Generalist Vision-Language Navigation Policy for   Drones"></a>SINGER: An Onboard Generalist Vision-Language Navigation Policy for   Drones</h2><p><strong>Authors:Maximilian Adang, JunEn Low, Ola Shorinwa, Mac Schwager</strong></p>
<p>Large vision-language models have driven remarkable progress in open-vocabulary robot policies, e.g., generalist robot manipulation policies, that enable robots to complete complex tasks specified in natural language. Despite these successes, open-vocabulary autonomous drone navigation remains an unsolved challenge due to the scarcity of large-scale demonstrations, real-time control demands of drones for stabilization, and lack of reliable external pose estimation modules. In this work, we present SINGER for language-guided autonomous drone navigation in the open world using only onboard sensing and compute. To train robust, open-vocabulary navigation policies, SINGER leverages three central components: (i) a photorealistic language-embedded flight simulator with minimal sim-to-real gap using Gaussian Splatting for efficient data generation, (ii) an RRT-inspired multi-trajectory generation expert for collision-free navigation demonstrations, and these are used to train (iii) a lightweight end-to-end visuomotor policy for real-time closed-loop control. Through extensive hardware flight experiments, we demonstrate superior zero-shot sim-to-real transfer of our policy to unseen environments and unseen language-conditioned goal objects. When trained on ~700k-1M observation action pairs of language conditioned visuomotor data and deployed on hardware, SINGER outperforms a velocity-controlled semantic guidance baseline by reaching the query 23.33% more on average, and maintains the query in the field of view 16.67% more on average, with 10% fewer collisions. </p>
<blockquote>
<p>大型视觉语言模型已经推动了开放词汇机器人策略的巨大进步，例如通用机器人操作策略，这些策略使机器人能够完成自然语言指定的复杂任务。尽管取得了这些成功，但由于缺乏大规模演示、无人机稳定控制的实时需求以及可靠的外部姿态估计模块的缺失，开放词汇的自主无人机导航仍然是一个未解决的挑战。在这项工作中，我们提出了SINGER，一种仅使用机载传感器和计算设备进行开放世界语言指导的自主无人机导航的方法。为了训练稳健的开放词汇导航策略，SINGER利用三个核心组件：（i）使用高斯拼贴法生成高效数据的逼真的语言嵌入飞行模拟器，其模拟与现实的差距最小化；（ii）一种受RRT启发的多轨迹生成专家，用于实现无碰撞导航演示；这些被用来训练（iii）一个轻量级的端到端视觉运动策略，用于实时闭环控制。通过大量的硬件飞行实验，我们证明了我们的策略在未见过的环境和未见过的语言条件目标对象上的零样本模拟到现实转移优越性。当在约70万至1百万的语言条件视觉运动数据观察动作对上训练，并部署在硬件上，SINGER的表现优于速度控制的语义指导基准，平均到达查询目标的次数高出23.33%，平均保持查询目标在视野中的时间高出16.67%，碰撞次数减少了10%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.18610v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了利用大型视觉语言模型实现无人机自主导航的新进展。通过使用三个关键组件：真实语言嵌入飞行模拟器、基于RRT的多轨迹生成专家以及轻量级端到端视觉运动策略，实现了零样本模拟到真实环境的迁移，并在硬件飞行实验中展现出卓越性能。该策略在未知环境和未知语言目标对象上表现优越，提高了目标到达率和视野保持率，并减少了碰撞。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>大型视觉语言模型推动了开放词汇机器人策略的显著进步。</li>
<li>开放世界中的无人机自主导航仍是挑战，因为缺乏大规模演示、实时控制需求和可靠的外部姿态估计模块。</li>
<li>SINGER通过使用真实语言嵌入飞行模拟器解决无人机导航问题，具备鲁棒性和开放词汇导航策略。</li>
<li>该模拟器利用高斯斑点法高效生成数据，减少模拟与现实的差距。</li>
<li>使用基于RRT的多轨迹生成专家为无碰撞导航提供演示。</li>
<li>训练轻量级端到端视觉运动策略用于实时闭环控制。</li>
<li>在硬件飞行实验中，SINGER策略表现出卓越性能，优于速度控制的语义指导基线。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.18610">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-5cb01b03d4790ca3464f91cf3d544cd7~resize:0:q75.jpg?source=1f5c5e47&expiration=1760038008&auth_key=1760038008-0-0-9fe0a2ff9afa91ab82d5b200a6477875&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4148faccd160cb1517697105849c8c45~resize:0:q75.jpg?source=1f5c5e47&expiration=1760038015&auth_key=1760038015-0-0-ae175ec05efed29b36837e89e593fadb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b3ee31c2586d4ab2a0ef8285c9ac6587~resize:0:q75.jpg?source=1f5c5e47&expiration=1760038022&auth_key=1760038022-0-0-fd503a293890c0f342324806a5f09c4a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-098a3be45cb3842ab800ed89eb7a9f12~resize:0:q75.jpg?source=1f5c5e47&expiration=1760038028&auth_key=1760038028-0-0-242e9b2047c89514d5307da7e066af1c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Event-guided-3D-Gaussian-Splatting-for-Dynamic-Human-and-Scene-Reconstruction"><a href="#Event-guided-3D-Gaussian-Splatting-for-Dynamic-Human-and-Scene-Reconstruction" class="headerlink" title="Event-guided 3D Gaussian Splatting for Dynamic Human and Scene   Reconstruction"></a>Event-guided 3D Gaussian Splatting for Dynamic Human and Scene   Reconstruction</h2><p><strong>Authors:Xiaoting Yin, Hao Shi, Kailun Yang, Jiajun Zhai, Shangwei Guo, Lin Wang, Kaiwei Wang</strong></p>
<p>Reconstructing dynamic humans together with static scenes from monocular videos remains difficult, especially under fast motion, where RGB frames suffer from motion blur. Event cameras exhibit distinct advantages, e.g., microsecond temporal resolution, making them a superior sensing choice for dynamic human reconstruction. Accordingly, we present a novel event-guided human-scene reconstruction framework that jointly models human and scene from a single monocular event camera via 3D Gaussian Splatting. Specifically, a unified set of 3D Gaussians carries a learnable semantic attribute; only Gaussians classified as human undergo deformation for animation, while scene Gaussians stay static. To combat blur, we propose an event-guided loss that matches simulated brightness changes between consecutive renderings with the event stream, improving local fidelity in fast-moving regions. Our approach removes the need for external human masks and simplifies managing separate Gaussian sets. On two benchmark datasets, ZJU-MoCap-Blur and MMHPSD-Blur, it delivers state-of-the-art human-scene reconstruction, with notable gains over strong baselines in PSNR&#x2F;SSIM and reduced LPIPS, especially for high-speed subjects. </p>
<blockquote>
<p>从单目视频中重建动态人体和静态场景仍然具有一定的挑战性，特别是在快速运动情况下，RGB帧会受到运动模糊的影响。事件相机具有独特的优势，例如微秒级的时序分辨率，使其成为动态人体重建的出色感知选择。因此，我们提出了一种新型的事件引导人体-场景重建框架，该框架通过3D高斯喷涂技术，利用单个单目事件相机对人体和场景进行联合建模。具体来说，一组统一的3D高斯携带可学习的语义属性；只有被分类为人类的高斯才会发生变形以实现动画效果，而场景高斯保持不变。为了对抗模糊，我们提出了一种事件引导损失，该损失匹配连续渲染之间的模拟亮度变化与事件流，提高了快速移动区域的局部保真度。我们的方法消除了对外部人体掩膜的需求，并简化了单独管理高斯集的过程。在ZJU-MoCap-Blur和MMHPSD-Blur两个基准数据集上，它实现了最先进的人体-场景重建，在PSNR&#x2F;SSIM上有显著的收益，并且降低了LPIPS，特别是在高速主题上表现尤为出色。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.18566v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了一种基于事件引导的人类场景重建框架，通过单目事件相机对动态人类和静态场景进行建模。采用动态与静态的分离表示方式，并结合三维高斯形态模板，提升了动态人类场景重建的准确度。提出的事件引导损失函数可有效解决快速运动区域中的模糊问题。该方法简化了任务，减少了单独的高斯集管理，并且在ZJU-MoCap-Blur和MMHPSD-Blur数据集上实现了最优的重建效果。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>利用事件相机的优势进行动态人类场景重建。</li>
<li>提出一种新颖的事件引导的人类场景重建框架，通过单一事件相机对人和场景进行建模。</li>
<li>采用三维高斯形态模板进行动态与静态分离表示，简化动画过程并提升重建精度。</li>
<li>为解决快速运动区域的模糊问题，引入事件引导的损失函数，通过匹配连续渲染的亮度变化与事件流实现。</li>
<li>不需要外部人类掩膜，简化了管理独立高斯集的过程。</li>
<li>在两个基准数据集上实现了最优的重建效果，特别是在高速运动主体上表现突出。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.18566">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-ed73fa86bb1f0115e2addba1200fd750~resize:0:q75.jpg?source=1f5c5e47&expiration=1760038036&auth_key=1760038036-0-0-b4bc5de64b5b8e90ba2b43a9c920fafc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d6f5c4fa428221af56bca01e4d84305b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760038043&auth_key=1760038043-0-0-4eb4d8a47284930214b4243eaf3af851&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-fe647ec9625d0d562d0f9163aa59abd1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760038050&auth_key=1760038050-0-0-9716f2acfe31af13b0ab96a2bb4cf25a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8555562a87e5ad40955df604045b9695~resize:0:q75.jpg?source=1f5c5e47&expiration=1760038057&auth_key=1760038057-0-0-e1bdbd6b8203e874eedc86f27669cd78&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="BridgeSplat-Bidirectionally-Coupled-CT-and-Non-Rigid-Gaussian-Splatting-for-Deformable-Intraoperative-Surgical-Navigation"><a href="#BridgeSplat-Bidirectionally-Coupled-CT-and-Non-Rigid-Gaussian-Splatting-for-Deformable-Intraoperative-Surgical-Navigation" class="headerlink" title="BridgeSplat: Bidirectionally Coupled CT and Non-Rigid Gaussian Splatting   for Deformable Intraoperative Surgical Navigation"></a>BridgeSplat: Bidirectionally Coupled CT and Non-Rigid Gaussian Splatting   for Deformable Intraoperative Surgical Navigation</h2><p><strong>Authors:Maximilian Fehrentz, Alexander Winkler, Thomas Heiliger, Nazim Haouchine, Christian Heiliger, Nassir Navab</strong></p>
<p>We introduce BridgeSplat, a novel approach for deformable surgical navigation that couples intraoperative 3D reconstruction with preoperative CT data to bridge the gap between surgical video and volumetric patient data. Our method rigs 3D Gaussians to a CT mesh, enabling joint optimization of Gaussian parameters and mesh deformation through photometric supervision. By parametrizing each Gaussian relative to its parent mesh triangle, we enforce alignment between Gaussians and mesh and obtain deformations that can be propagated back to update the CT. We demonstrate BridgeSplat’s effectiveness on visceral pig surgeries and synthetic data of a human liver under simulation, showing sensible deformations of the preoperative CT on monocular RGB data. Code, data, and additional resources can be found at <a target="_blank" rel="noopener" href="https://maxfehrentz.github.io/ct-informed-splatting/">https://maxfehrentz.github.io/ct-informed-splatting/</a> . </p>
<blockquote>
<p>我们介绍了BridgeSplat，这是一种新型的变形手术导航方法。它将术中3D重建与术前CT数据相结合，以弥手术视频和患者体积数据之间的鸿沟。我们的方法将3D高斯映射到CT网格上，通过光度监督实现高斯参数和网格变形的联合优化。通过将每个高斯相对于其父网格三角形进行参数化，我们强制高斯和网格之间的对齐，并获取可以传播回以更新CT的变形。我们在模拟的猪内脏手术和模拟的人类肝脏合成数据上展示了BridgeSplat的有效性，在单目RGB数据上显示出术前CT的敏感变形。代码、数据和附加资源可以在<a target="_blank" rel="noopener" href="https://maxfehrentz.github.io/ct-informed-splatting/%E6%89%BE%E5%88%B0%E3%80%82">https://maxfehrentz.github.io/ct-informed-splatting/找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.18501v1">PDF</a> Accepted at MICCAI 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了BridgeSplat这一新型的可变形手术导航技术，该技术结合了术中三维重建与术前CT数据，从而拉近手术视频与病人三维体积数据之间的距离。BridgeSplat将3D高斯应用于CT网格上，使得可以通过光光度监控同时优化高斯参数和网格变形。通过相对其母网格三角形参数化每个高斯，确保了高斯与网格之间的对齐，从而取得能回溯至CT更新的变形效果。该方法对猪的腹腔内手术及模拟条件下的人体肝脏合成数据有很好的应用效果，证明可以在单色RGB数据上对术前CT进行有效变形处理。有关代码、数据和额外资源可以在网站上查阅（链接已提供）。</p>
<p><strong>Key Takeaways</strong></p>
<p>以下是文本中的关键要点：</p>
<ul>
<li>BridgeSplat是一种新型的可变形手术导航技术。</li>
<li>该技术结合了术中三维重建与术前CT数据，用于提高手术过程的准确性。</li>
<li>BridgeSplat采用的高斯优化方法使得在手术过程中能同时优化高斯参数和网格变形。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.18501">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-9faf73a090769f59a4e112d7fbe0e9c9~resize:0:q75.jpg?source=1f5c5e47&expiration=1760038064&auth_key=1760038064-0-0-a7be4a450faeef29c2753e8e3bff2ac0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1053d17b5bb91c9f6fc1a5f32fa461dd~resize:0:q75.jpg?source=1f5c5e47&expiration=1760038072&auth_key=1760038072-0-0-8de6c6aa07a1c462a06cc31ee5c85f7a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f128e5417567fb019b885b5c9534bf75~resize:0:q75.jpg?source=1f5c5e47&expiration=1760038078&auth_key=1760038078-0-0-2f66db805f941def4cf85365a7ee498b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8c92f9b2a746b5f57277a8234cb4130a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760038085&auth_key=1760038085-0-0-6dea63bb34ae9cd0c5fb96cc2312518e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="EmbodiedSplat-Personalized-Real-to-Sim-to-Real-Navigation-with-Gaussian-Splats-from-a-Mobile-Device"><a href="#EmbodiedSplat-Personalized-Real-to-Sim-to-Real-Navigation-with-Gaussian-Splats-from-a-Mobile-Device" class="headerlink" title="EmbodiedSplat: Personalized Real-to-Sim-to-Real Navigation with Gaussian   Splats from a Mobile Device"></a>EmbodiedSplat: Personalized Real-to-Sim-to-Real Navigation with Gaussian   Splats from a Mobile Device</h2><p><strong>Authors:Gunjan Chhablani, Xiaomeng Ye, Muhammad Zubair Irshad, Zsolt Kira</strong></p>
<p>The field of Embodied AI predominantly relies on simulation for training and evaluation, often using either fully synthetic environments that lack photorealism or high-fidelity real-world reconstructions captured with expensive hardware. As a result, sim-to-real transfer remains a major challenge. In this paper, we introduce EmbodiedSplat, a novel approach that personalizes policy training by efficiently capturing the deployment environment and fine-tuning policies within the reconstructed scenes. Our method leverages 3D Gaussian Splatting (GS) and the Habitat-Sim simulator to bridge the gap between realistic scene capture and effective training environments. Using iPhone-captured deployment scenes, we reconstruct meshes via GS, enabling training in settings that closely approximate real-world conditions. We conduct a comprehensive analysis of training strategies, pre-training datasets, and mesh reconstruction techniques, evaluating their impact on sim-to-real predictivity in real-world scenarios. Experimental results demonstrate that agents fine-tuned with EmbodiedSplat outperform both zero-shot baselines pre-trained on large-scale real-world datasets (HM3D) and synthetically generated datasets (HSSD), achieving absolute success rate improvements of 20% and 40% on real-world Image Navigation task. Moreover, our approach yields a high sim-vs-real correlation (0.87-0.97) for the reconstructed meshes, underscoring its effectiveness in adapting policies to diverse environments with minimal effort. Project page: <a target="_blank" rel="noopener" href="https://gchhablani.github.io/embodied-splat">https://gchhablani.github.io/embodied-splat</a>. </p>
<blockquote>
<p>人工智能实体领域主要依赖模拟进行训练和评估，通常使用缺乏逼真感的全合成环境或使用昂贵的硬件捕捉的高保真现实世界重建。因此，模拟到现实的转移仍然是一个主要挑战。在本文中，我们介绍了EmbodiedSplat，这是一种通过高效捕获部署环境并在重建场景中对策略进行微调来个性化策略训练的新方法。我们的方法利用3D高斯喷绘（GS）和 Habitat-Sim模拟器来弥合现实场景捕捉和有效训练环境之间的差距。我们使用iPhone捕获的部署场景，通过GS重建网格，能够在接近真实世界条件的设置中进行培训。我们对训练策略、预训练数据集和网格重建技术进行了综合分析，评估了它们在现实场景中模拟到现实的预测能力的影响。实验结果表明，使用EmbodiedSplat进行微调的代理在真实世界的图像导航任务上优于零基准预训练的大型现实世界数据集（HM3D）和合成数据集（HSSD），成功率提高了20%和40%。此外，我们的方法对重建网格的模拟与真实相关性高达0.87-0.97，突显了其在适应各种环境并最小化努力调整策略方面的有效性。项目页面：<a target="_blank" rel="noopener" href="https://gchhablani.github.io/embodied-splat%E3%80%82">https://gchhablani.github.io/embodied-splat。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.17430v2">PDF</a> 16 pages, 18 figures, paper accepted at ICCV, 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种名为EmbodiedSplat的新方法，该方法通过高效捕捉部署环境并对政策进行微调，解决了模拟到现实的转移问题。该方法利用3D高斯涂敷（GS）和栖息地模拟器，缩小了真实场景捕捉和有效训练环境之间的差距。通过iPhone捕捉的部署场景进行网格重建，使训练环境更贴近真实世界条件。实验结果表明，使用EmbodiedSplat进行微调的代理在现实世界图像导航任务上的表现优于预训练的大型现实世界数据集和合成数据集，成功率提高了20%和40%。该方法还实现了高模拟与真实相关性，表明其适应不同环境并最小化努力调整政策的有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>EmbodiedSplat方法个性化政策训练，通过捕捉部署环境并微调政策，解决模拟到现实的转移问题。</li>
<li>利用3D高斯涂敷（GS）和栖息地模拟器，实现真实场景捕捉和训练环境之间的桥梁。</li>
<li>通过iPhone捕捉的部署场景进行网格重建，使训练更接近真实世界条件。</li>
<li>全面的策略分析，包括训练策略、预训练数据集和网格重建技术，对模拟到现实的预测能力进行评估。</li>
<li>实验结果表明，使用EmbodiedSplat的代理在现实世界任务上的表现优于其他方法，成功率显著提高。</li>
<li>EmbodiedSplat方法实现了高模拟与真实的相关性，表明其适应各种环境的有效性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.17430">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-240d926c05b977ef3e92d5492f1f6d09~resize:0:q75.jpg?source=1f5c5e47&expiration=1760038092&auth_key=1760038092-0-0-d98ecfbbf7a7800e2e764dc7779d155f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b884f6399ce1b4d179e260a58c79e304~resize:0:q75.jpg?source=1f5c5e47&expiration=1760038099&auth_key=1760038099-0-0-c0e8696a90fff986ea9aac3efb9d8f1e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-11f758a15307d54bd9e799151836f869~resize:0:q75.jpg?source=1f5c5e47&expiration=1760038106&auth_key=1760038106-0-0-6d63c82fd45a5511e9a45a3b131c36af&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f18b57bac1e1ef3d71e774964df448f2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760038187&auth_key=1760038187-0-0-ae1ecc2c9f18ad9315c8ac3f1ae97c25&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b8a6c602d7ec0706dbcf6f29b2c07aa1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760038194&auth_key=1760038194-0-0-c5a1d992aad22cfa5e36b83f63ede630&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-62cfcbefc4801fb0a4af2f1a4f9ddfb6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760038201&auth_key=1760038201-0-0-92b82a6bdb5cf1f5f2e3d83bd6dfb9be&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="HyRF-Hybrid-Radiance-Fields-for-Memory-efficient-and-High-quality-Novel-View-Synthesis"><a href="#HyRF-Hybrid-Radiance-Fields-for-Memory-efficient-and-High-quality-Novel-View-Synthesis" class="headerlink" title="HyRF: Hybrid Radiance Fields for Memory-efficient and High-quality Novel   View Synthesis"></a>HyRF: Hybrid Radiance Fields for Memory-efficient and High-quality Novel   View Synthesis</h2><p><strong>Authors:Zipeng Wang, Dan Xu</strong></p>
<p>Recently, 3D Gaussian Splatting (3DGS) has emerged as a powerful alternative to NeRF-based approaches, enabling real-time, high-quality novel view synthesis through explicit, optimizable 3D Gaussians. However, 3DGS suffers from significant memory overhead due to its reliance on per-Gaussian parameters to model view-dependent effects and anisotropic shapes. While recent works propose compressing 3DGS with neural fields, these methods struggle to capture high-frequency spatial variations in Gaussian properties, leading to degraded reconstruction of fine details. We present Hybrid Radiance Fields (HyRF), a novel scene representation that combines the strengths of explicit Gaussians and neural fields. HyRF decomposes the scene into (1) a compact set of explicit Gaussians storing only critical high-frequency parameters and (2) grid-based neural fields that predict remaining properties. To enhance representational capacity, we introduce a decoupled neural field architecture, separately modeling geometry (scale, opacity, rotation) and view-dependent color. Additionally, we propose a hybrid rendering scheme that composites Gaussian splatting with a neural field-predicted background, addressing limitations in distant scene representation. Experiments demonstrate that HyRF achieves state-of-the-art rendering quality while reducing model size by over 20 times compared to 3DGS and maintaining real-time performance. Our project page is available at <a target="_blank" rel="noopener" href="https://wzpscott.github.io/hyrf/">https://wzpscott.github.io/hyrf/</a>. </p>
<blockquote>
<p>最近，3D高斯扩展（3DGS）作为一种强大的替代NeRF的方法出现，它通过明确的、可优化的3D高斯实现实时高质量的新视角合成。然而，由于3DGS依赖于高斯参数来模拟视角相关的效果和各向异性形状，因此存在较大的内存开销。虽然最近的工作提出使用神经网络对3DGS进行压缩，但这些方法在捕获高斯属性的高频空间变化方面表现较差，导致精细细节的重建退化。我们提出了混合辐射场（HyRF），这是一种结合显式高斯和神经网络优点的新型场景表示方法。HyRF将场景分解为（1）一组紧凑的显式高斯，仅存储关键的高频参数，（2）基于网格的神经网络场，预测其余属性。为了增强表示能力，我们引入了一个分离的神经网络体系结构，分别模拟几何（尺度、不透明度、旋转）和视角相关的颜色。此外，我们提出了一种混合渲染方案，将高斯扩展与神经网络预测的背景进行组合，解决远距离场景表示的局限性。实验表明，HyRF在达到最先进的渲染质量的同时，与3DGS相比将模型大小缩小了超过20倍，并保持实时性能。我们的项目页面可在<a target="_blank" rel="noopener" href="https://wzpscott.github.io/hyrf/%E6%89%BE%E5%88%B0%E3%80%82">https://wzpscott.github.io/hyrf/找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.17083v2">PDF</a> Accepted at NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了Hybrid Radiance Fields（HyRF）技术，该技术结合了显式高斯和神经场的优点，用于实时高质量的新型视图合成。针对现有技术的内存开销大和对高频空间变化捕捉能力弱的问题，HyRF通过分解场景为关键的高频参数显式高斯和基于网格的神经场进行预测来解决。此外，还引入了去耦的神经场架构和混合渲染方案，以提高表现力和解决远距离场景表示的限制。实验表明，HyRF达到了最先进的渲染质量，同时模型大小减少了超过20倍，且保持了实时性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>3DGS已成为NeRF基方法的强大替代方案，可实现实时高质量的新型视图合成。</li>
<li>3DGS存在内存开销大的问题，依赖于高斯参数模拟视图相关效应和形状。</li>
<li>现有压缩方法难以捕捉高斯属性的高频空间变化，导致精细细节重建退化。</li>
<li>HyRF结合了显式高斯和神经场的优点，分解场景为高频参数显式高斯和基于网格的神经场预测。</li>
<li>引入去耦的神经场架构，分别模拟几何和视图相关颜色，提高表现力。</li>
<li>提出混合渲染方案，解决远距离场景表示的限制。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.17083">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-e6de2c14c3fada49ecef3df765ff1dbc~resize:0:q75.jpg?source=1f5c5e47&expiration=1760038208&auth_key=1760038208-0-0-9422c056a57427284c33e8e187cfe17a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d02592ac4473bc388a274f9989394a3d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760038216&auth_key=1760038216-0-0-a3d37eef2d8c388691dec10f4a468291&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4a83be3a634fed56dd535ea757c0b253~resize:0:q75.jpg?source=1f5c5e47&expiration=1760038222&auth_key=1760038222-0-0-c44644333f28264856a4ea05c9a7488a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7a871bfd24a47a759cf52edeebb5c8d6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760038229&auth_key=1760038229-0-0-33ee432e1e00859c7c7d8b66e8d3782c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="MEGS-2-Memory-Efficient-Gaussian-Splatting-via-Spherical-Gaussians-and-Unified-Pruning"><a href="#MEGS-2-Memory-Efficient-Gaussian-Splatting-via-Spherical-Gaussians-and-Unified-Pruning" class="headerlink" title="MEGS$^{2}$: Memory-Efficient Gaussian Splatting via Spherical Gaussians   and Unified Pruning"></a>MEGS$^{2}$: Memory-Efficient Gaussian Splatting via Spherical Gaussians   and Unified Pruning</h2><p><strong>Authors:Jiarui Chen, Yikeng Chen, Yingshuang Zou, Ye Huang, Peng Wang, Yuan Liu, Yujing Sun, Wenping Wang</strong></p>
<p>3D Gaussian Splatting (3DGS) has emerged as a dominant novel-view synthesis technique, but its high memory consumption severely limits its applicability on edge devices. A growing number of 3DGS compression methods have been proposed to make 3DGS more efficient, yet most only focus on storage compression and fail to address the critical bottleneck of rendering memory. To address this problem, we introduce MEGS$^{2}$, a novel memory-efficient framework that tackles this challenge by jointly optimizing two key factors: the total primitive number and the parameters per primitive, achieving unprecedented memory compression. Specifically, we replace the memory-intensive spherical harmonics with lightweight, arbitrarily oriented spherical Gaussian lobes as our color representations. More importantly, we propose a unified soft pruning framework that models primitive-number and lobe-number pruning as a single constrained optimization problem. Experiments show that MEGS$^{2}$ achieves a 50% static VRAM reduction and a 40% rendering VRAM reduction compared to existing methods, while maintaining comparable rendering quality. Project page: <a target="_blank" rel="noopener" href="https://megs-2.github.io/">https://megs-2.github.io/</a> </p>
<blockquote>
<p>3D高斯延展（3DGS）作为一种新兴的主导视图合成技术备受关注，但其高内存消耗严重限制了其在边缘设备上的应用。越来越多的3DGS压缩方法被提出以提高3DGS的效率，但大多数方法只关注存储压缩，未能解决渲染内存的关键瓶颈问题。为了解决这一问题，我们引入了MEGS$^{2}$，这是一种新型的内存高效框架，通过联合优化两个关键因素：总基元数量和每个基元的参数，来解决这一挑战，实现了前所未有的内存压缩。具体来说，我们用轻量级的任意方向球面高斯波瓣替代了内存密集型的球面谐波作为我们的颜色表示。更重要的是，我们提出了一个统一的软修剪框架，将基元数量和波瓣数量修剪建模为一个带有约束的优化问题。实验表明，与现有方法相比，MEGS$^{2}$实现了50%的静态VRAM减少和40%的渲染VRAM减少，同时保持了相当的渲染质量。项目页面：<a target="_blank" rel="noopener" href="https://megs-2.github.io/">https://megs-2.github.io/</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.07021v2">PDF</a> 20 pages, 8 figures. Project page at <a target="_blank" rel="noopener" href="https://megs-2.github.io/">https://megs-2.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>3D Gaussian Splatting（3DGS）是一种新兴的主流视图合成技术，但其高内存消耗严重限制了其在边缘设备上的应用。针对这一问题，提出了MEGS^2记忆效率框架，通过同时优化原始总数和每个原始参数两个关键因素，实现了前所未有的内存压缩。采用轻量级、任意定向球面高斯波瓣代替内存密集型球面谐波作为颜色表示，并提出统一的软修剪框架，将原始数量波瓣数和修剪数建模为一个约束优化问题。实验表明，与现有方法相比，MEGS^2可实现静态VRAM减少50%，渲染VRAM减少40%，同时保持相当的渲染质量。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>3DGS已成为主流视图合成技术，但内存消耗较高，限制了其在边缘设备上的应用。</li>
<li>MEGS^2框架旨在解决这一问题，通过联合优化原始总数和每个原始参数实现高效内存使用。</li>
<li>MEGS^2采用轻量级球面高斯波瓣替换内存密集型球面谐波作为颜色表示。</li>
<li>引入统一的软修剪框架，将原始数量波瓣数和修剪数建模为约束优化问题。</li>
<li>MEGS^2实现了静态VRAM和渲染VRAM的显著减少，分别达到了50%和40%的缩减。</li>
<li>MEGS^2在保持较高渲染质量的同时实现了内存的优化。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.07021">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-f0e757abd095a610a60828f1c2c7c02f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760038236&auth_key=1760038236-0-0-392492a495dde189c28ad8e0edbd5718&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0d725c19167b3b029f2250facbc76ee1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760038243&auth_key=1760038243-0-0-998fbdd54d5038691f17707850be1a9b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Temporal-Smoothness-Aware-Rate-Distortion-Optimized-4D-Gaussian-Splatting"><a href="#Temporal-Smoothness-Aware-Rate-Distortion-Optimized-4D-Gaussian-Splatting" class="headerlink" title="Temporal Smoothness-Aware Rate-Distortion Optimized 4D Gaussian   Splatting"></a>Temporal Smoothness-Aware Rate-Distortion Optimized 4D Gaussian   Splatting</h2><p><strong>Authors:Hyeongmin Lee, Kyungjune Baek</strong></p>
<p>Dynamic 4D Gaussian Splatting (4DGS) effectively extends the high-speed rendering capabilities of 3D Gaussian Splatting (3DGS) to represent volumetric videos. However, the large number of Gaussians, substantial temporal redundancies, and especially the absence of an entropy-aware compression framework result in large storage requirements. Consequently, this poses significant challenges for practical deployment, efficient edge-device processing, and data transmission. In this paper, we introduce a novel end-to-end RD-optimized compression framework tailored for 4DGS, aiming to enable flexible, high-fidelity rendering across varied computational platforms. Leveraging Fully Explicit Dynamic Gaussian Splatting (Ex4DGS), one of the state-of-the-art 4DGS methods, as our baseline, we start from the existing 3DGS compression methods for compatibility while effectively addressing additional challenges introduced by the temporal axis. In particular, instead of storing motion trajectories independently per point, we employ a wavelet transform to reflect the real-world smoothness prior, significantly enhancing storage efficiency. This approach yields significantly improved compression ratios and provides a user-controlled balance between compression efficiency and rendering quality. Extensive experiments demonstrate the effectiveness of our method, achieving up to 91$\times$ compression compared to the original Ex4DGS model while maintaining high visual fidelity. These results highlight the applicability of our framework for real-time dynamic scene rendering in diverse scenarios, from resource-constrained edge devices to high-performance environments. The source code is available at <a target="_blank" rel="noopener" href="https://github.com/HyeongminLEE/RD4DGS">https://github.com/HyeongminLEE/RD4DGS</a>. </p>
<blockquote>
<p>动态四维高斯映射（4DGS）有效地将三维高斯映射（3DGS）的高速渲染能力扩展到体积视频表示。然而，大量的高斯、大量的时间冗余以及缺乏熵感知压缩框架导致存储需求巨大。因此，这给实际部署、高效的边缘设备处理和数据传输带来了重大挑战。在本文中，我们针对4DGS引入了一种新型端到端RD优化压缩框架，旨在实现在各种计算平台上的灵活、高保真渲染。我们以目前最先进的动态高斯映射方法之一的全显式动态高斯映射（Ex4DGS）为基线，从现有的兼容性良好的三维高斯映射压缩方法出发，有效解决了由时间轴引入的额外挑战。特别是，我们没有像传统方法那样独立存储每个点的运动轨迹，而是采用小波变换来反映现实世界中的平滑先验知识，从而显著提高存储效率。这种方法实现了显著的压缩比，并在压缩效率和渲染质量之间提供了用户可控的平衡。大量实验证明了我们的方法的有效性，与原始Ex4DGS模型相比，我们的方法实现了高达91倍的压缩，同时保持了高度的视觉保真度。这些结果突显了我们的框架在资源受限的边缘设备到高性能环境的各种场景中实时动态场景渲染的适用性。源代码可在<a target="_blank" rel="noopener" href="https://github.com/HyeongminLEE/RD4DGS%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/HyeongminLEE/RD4DGS找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.17336v2">PDF</a> 24 pages, 10 figures, NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>这篇论文提出了一种针对动态四维高斯点绘（4DGS）的端到端优化压缩框架。该框架旨在实现跨不同计算平台的高保真渲染，通过利用小波变换来反映现实世界中的平滑先验信息，提高了存储效率，实现了高压缩比和高视觉保真度的平衡。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>动态四维高斯点绘（4DGS）扩展了三维高斯点绘（3DGS）的高速渲染能力，以表示体积视频。</li>
<li>现有技术面临的挑战包括大量高斯、时间冗余和缺乏熵感知压缩框架导致的存储需求大。</li>
<li>新框架旨在解决这些问题并实现灵活的、高保真度的跨平台渲染。</li>
<li>通过利用小波变换反映现实世界平滑度先验信息，提高存储效率。</li>
<li>实现显著改进的压缩比和用户控制的压缩效率和渲染质量之间的平衡。</li>
<li>实验结果显示，与原始Ex4DGS模型相比，新方法可实现高达91倍的压缩，同时保持高视觉保真度。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.17336">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-56ad927da2261cbf683e7e46ce95ab5a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760038250&auth_key=1760038250-0-0-cac8bb1a9c2b70ebdaa55101f5508d24&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="DWTGS-Rethinking-Frequency-Regularization-for-Sparse-view-3D-Gaussian-Splatting"><a href="#DWTGS-Rethinking-Frequency-Regularization-for-Sparse-view-3D-Gaussian-Splatting" class="headerlink" title="DWTGS: Rethinking Frequency Regularization for Sparse-view 3D Gaussian   Splatting"></a>DWTGS: Rethinking Frequency Regularization for Sparse-view 3D Gaussian   Splatting</h2><p><strong>Authors:Hung Nguyen, Runfa Li, An Le, Truong Nguyen</strong></p>
<p>Sparse-view 3D Gaussian Splatting (3DGS) presents significant challenges in reconstructing high-quality novel views, as it often overfits to the widely-varying high-frequency (HF) details of the sparse training views. While frequency regularization can be a promising approach, its typical reliance on Fourier transforms causes difficult parameter tuning and biases towards detrimental HF learning. We propose DWTGS, a framework that rethinks frequency regularization by leveraging wavelet-space losses that provide additional spatial supervision. Specifically, we supervise only the low-frequency (LF) LL subbands at multiple DWT levels, while enforcing sparsity on the HF HH subband in a self-supervised manner. Experiments across benchmarks show that DWTGS consistently outperforms Fourier-based counterparts, as this LF-centric strategy improves generalization and reduces HF hallucinations. </p>
<blockquote>
<p>稀疏视角的3D高斯飞溅（3DGS）在重建高质量新颖视角方面存在重大挑战，因为它经常过度拟合稀疏训练视角中广泛变化的高频（HF）细节。虽然频率正则化可能是一种有前途的方法，但它通常依赖于傅里叶变换，导致参数调整困难，并偏向于有害的高频学习。我们提出了DWTGS，这是一个利用小波空间损失重新思考频率正则化的框架，它提供了额外的空间监督。具体来说，我们在多个DWT级别仅监督低频（LF）LL子带，同时以自我监督的方式在高频HH子带上强制执行稀疏性。跨基准的实验表明，DWTGS始终优于基于傅立叶的对应方法，因为这种以LF为中心的策略提高了泛化能力并减少了高频幻觉。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.15690v2">PDF</a> Accepted to VCIP 2025</p>
<p><strong>Summary</strong></p>
<p>基于稀疏视角的3D高斯喷绘（3DGS）在重建高质量新视角时面临重大挑战，易对稀疏训练视角中变化多端的高频细节产生过拟合现象。虽然频率正则化是一种有前途的方法，但其通常依赖于傅里叶变换，导致参数调整困难且偏向有害的高频学习。本研究提出DWTGS框架，重新思考频率正则化，利用小波空间损失提供额外的空间监督。具体来说，我们在多个DWT级别仅监督低频LL子带，同时以自我监督的方式在高频HH子带上实施稀疏性。跨基准实验的测试表明，DWTGS在低频为中心的策略提高了泛化能力并减少了高频幻觉，因此它持续优于基于傅立叶的方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>稀疏视角的3D高斯喷绘（3DGS）在重建高质量新视角时面临挑战，易过拟合于高频细节。</li>
<li>频率正则化是一种解决此问题的有前途的方法，但传统的频率正则化依赖于傅立叶变换，存在参数调整困难和偏向高频学习的缺陷。</li>
<li>本研究提出DWTGS框架，利用小波空间损失进行频率正则化，提供额外的空间监督。</li>
<li>DWTGS通过监督多个DWT级别的低频LL子带并自我监督高频HH子带的稀疏性，实现了一种低频为中心的策略。</li>
<li>实验表明，DWTGS框架能提高泛化能力并减少高频幻觉。</li>
<li>DWTGS持续优于基于傅立叶的方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.15690">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-17bcd51353ace54d0ad8aa6013b26d01~resize:0:q75.jpg?source=1f5c5e47&expiration=1760038259&auth_key=1760038259-0-0-a677c44cb27f475975e09f869b923ca0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-42a4ac4053caf53d9a7c69e25b318e86~resize:0:q75.jpg?source=1f5c5e47&expiration=1760038266&auth_key=1760038266-0-0-352ca4e069ae89e43cfb2768debd77a9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9600037a6ceef4a4de66bec7a707a6b3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760038273&auth_key=1760038273-0-0-4a7ed142dc51f88ad3475d01eea8a118&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-cf85e5e2208e523b23a2da8f534b0dd1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760038280&auth_key=1760038280-0-0-1a42cbed6e90a2edbffe6cf5a2cf5e63&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-311626e83c079f165d31c8bea52da013~resize:0:q75.jpg?source=1f5c5e47&expiration=1760038286&auth_key=1760038286-0-0-3e3279f7cb0a836da1024a7fbdde10df&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7c6719b56ee6eb16d31a8041b8a50774~resize:0:q75.jpg?source=1f5c5e47&expiration=1760038293&auth_key=1760038293-0-0-fa41e64cab3a440b1655d18946d57ee0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-22bece77c37640fe4be82de922674dff~resize:0:q75.jpg?source=1f5c5e47&expiration=1760038299&auth_key=1760038299-0-0-81d8c3d8fb2604cb9178c08989d3e738&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="GAF-Gaussian-Action-Field-as-a-4D-Representation-for-Dynamic-World-Modeling-in-Robotic-Manipulation"><a href="#GAF-Gaussian-Action-Field-as-a-4D-Representation-for-Dynamic-World-Modeling-in-Robotic-Manipulation" class="headerlink" title="GAF: Gaussian Action Field as a 4D Representation for Dynamic World   Modeling in Robotic Manipulation"></a>GAF: Gaussian Action Field as a 4D Representation for Dynamic World   Modeling in Robotic Manipulation</h2><p><strong>Authors:Ying Chai, Litao Deng, Ruizhi Shao, Jiajun Zhang, Kangchen Lv, Liangjun Xing, Xiang Li, Hongwen Zhang, Yebin Liu</strong></p>
<p>Accurate scene perception is critical for vision-based robotic manipulation. Existing approaches typically follow either a Vision-to-Action (V-A) paradigm, predicting actions directly from visual inputs, or a Vision-to-3D-to-Action (V-3D-A) paradigm, leveraging intermediate 3D representations. However, these methods often struggle with action inaccuracies due to the complexity and dynamic nature of manipulation scenes. In this paper, we adopt a V-4D-A framework that enables direct action reasoning from motion-aware 4D representations via a Gaussian Action Field (GAF). GAF extends 3D Gaussian Splatting (3DGS) by incorporating learnable motion attributes, allowing 4D modeling of dynamic scenes and manipulation actions. To learn time-varying scene geometry and action-aware robot motion, GAF provides three interrelated outputs: reconstruction of the current scene, prediction of future frames, and estimation of init action via Gaussian motion. Furthermore, we employ an action-vision-aligned denoising framework, conditioned on a unified representation that combines the init action and the Gaussian perception, both generated by the GAF, to further obtain more precise actions. Extensive experiments demonstrate significant improvements, with GAF achieving +11.5385 dB PSNR, +0.3864 SSIM and -0.5574 LPIPS improvements in reconstruction quality, while boosting the average +7.3% success rate in robotic manipulation tasks over state-of-the-art methods. </p>
<blockquote>
<p>准确场景感知对于基于视觉的机器人操作至关重要。现有方法通常遵循视觉到动作（V-A）范式，直接从视觉输入预测动作，或者遵循视觉到三维到动作（V-3D-A）范式，利用中间三维表示。然而，这些方法通常由于操作场景的复杂性和动态性而面临动作不准确的问题。在本文中，我们采用了一种V-4D-A框架，该框架能够通过高斯动作场（GAF）从感知运动的4D表示中进行直接动作推理。GAF通过结合可学习的运动属性扩展了三维高斯拼贴（3DGS），允许对动态场景和操纵动作进行4D建模。为了学习随时间变化的场景几何和感知动作的机器人运动，GAF提供了三个相互关联的输出：当前场景的重建、未来帧的预测以及通过高斯运动估计的初始动作。此外，我们采用了一种与行动视觉对齐的去噪框架，该框架以由GAF生成的初始动作和高斯感知的结合的统一表示为条件，进一步获得更精确的动作。大量实验表明，GAF在重建质量方面实现了+11.5385分贝峰值信噪比（PSNR）、+0.3864结构相似性（SSIM）和-0.5574学习感知损失（LPIPS）的显著改善，同时在机器人操作任务上的成功率平均提高了+7.3%，超过了最先进的方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.14135v4">PDF</a> <a target="_blank" rel="noopener" href="http://chaiying1.github.io/GAF.github.io/project_page/">http://chaiying1.github.io/GAF.github.io/project_page/</a></p>
<p><strong>Summary</strong></p>
<p>本文提出一个基于动态场景的V-4D-A框架，利用高斯动作场（GAF）实现直接从运动感知的4D表示中进行动作推理。GAF扩展了3D高斯拼贴技术，通过引入可学习的运动属性，实现对动态场景和操控动作的4D建模。此外，GAF还提供三种相关输出，用于学习随时间变化的场景几何和机器人动作感知。采用与动作视觉对齐的降噪框架，进一步提高动作的精确度。实验表明，GAF在重建质量和机器人操作任务上的成功率均显著提高。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>引入V-4D-A框架，结合动态场景与直接动作推理。</li>
<li>提出高斯动作场（GAF），扩展3D高斯拼贴技术，实现4D建模。</li>
<li>GAF提供三种输出：重建当前场景、预测未来帧、估计初始动作。</li>
<li>结合初始动作和高斯感知的统表示，采用动作视觉对齐的降噪框架。</li>
<li>GAF在重建质量上较现有技术有显著改进，如PSNR、SSIM和LPIPS指标。</li>
<li>在机器人操作任务上，GAF较现有方法的成功率平均提升7.3%。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.14135">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-c6812e81b452e29955a55087c433dbc6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760038306&auth_key=1760038306-0-0-dffa16c3e519ffd9dac3e554916f1d1c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-042bea39661132dbdcb052ff0d4f262b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760038314&auth_key=1760038314-0-0-15729d282642adedca782a5b27ac8f32&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-fb997de45b43dc0fa97d6d526e494854~resize:0:q75.jpg?source=1f5c5e47&expiration=1760038320&auth_key=1760038320-0-0-91e8b14bb1877579be43d8764485dcc3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-76dca6175577a6d9597638e9077b03e1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760038327&auth_key=1760038327-0-0-ebe737185f96b6466c48d9c5157f8d55&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-53d4db65bf8aa9868d7cd9ea9b93f06b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760038333&auth_key=1760038333-0-0-a2eb25f6e558e90d76688cc3c5b6da76&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1be85e39b75845a5968e813bf94a1213~resize:0:q75.jpg?source=1f5c5e47&expiration=1760038340&auth_key=1760038340-0-0-7098bbcfe00066b697a266dcdaa18559&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5a7cb15becb54bd26b2584ba6ea6663f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760038346&auth_key=1760038346-0-0-e9e6420fa7dc9cba7d85a57c11c7ae24&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-28/3DGS/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-28/3DGS/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/3DGS/">
                                    <span class="chip bg-color">3DGS</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-28/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-cdc7ffdf5f932180a83612b9c0ec803a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760038353&auth_key=1760038353-0-0-d06f619d3b00a9ced5afeb5212c1e0eb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF 方向最新论文已更新，请持续关注 Update in 2025-09-28  Integrating Object Interaction Self-Attention and GAN-Based Debiasing   for Visual Question Answering
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-28
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-28/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-4c5d37a29b968f296b28266d27044f27~resize:0:q75.jpg?source=1f5c5e47&expiration=1760037527&auth_key=1760037527-0-0-81408f4b70fc271b5deb9b067f67c1d6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="元宇宙/虚拟人">
                        
                        <span class="card-title">元宇宙/虚拟人</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            元宇宙/虚拟人 方向最新论文已更新，请持续关注 Update in 2025-09-28  Audio-Driven Universal Gaussian Head Avatars
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-28
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/" class="post-category">
                                    元宇宙/虚拟人
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                        <span class="chip bg-color">元宇宙/虚拟人</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">30341.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
