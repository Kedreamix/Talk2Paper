<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-28  Does FLUX Already Know How to Perform Physically Plausible Image   Composition?">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-465cef20062acec8cb146b8c5b964d76')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-28
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-15
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    86 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-09-28-æ›´æ–°"><a href="#2025-09-28-æ›´æ–°" class="headerlink" title="2025-09-28 æ›´æ–°"></a>2025-09-28 æ›´æ–°</h1><h2 id="Does-FLUX-Already-Know-How-to-Perform-Physically-Plausible-Image-Composition"><a href="#Does-FLUX-Already-Know-How-to-Perform-Physically-Plausible-Image-Composition" class="headerlink" title="Does FLUX Already Know How to Perform Physically Plausible Image   Composition?"></a>Does FLUX Already Know How to Perform Physically Plausible Image   Composition?</h2><p><strong>Authors:Shilin Lu, Zhuming Lian, Zihan Zhou, Shaocong Zhang, Chen Zhao, Adams Wai-Kin Kong</strong></p>
<p>Image composition aims to seamlessly insert a user-specified object into a new scene, but existing models struggle with complex lighting (e.g., accurate shadows, water reflections) and diverse, high-resolution inputs. Modern text-to-image diffusion models (e.g., SD3.5, FLUX) already encode essential physical and resolution priors, yet lack a framework to unleash them without resorting to latent inversion, which often locks object poses into contextually inappropriate orientations, or brittle attention surgery. We propose SHINE, a training-free framework for Seamless, High-fidelity Insertion with Neutralized Errors. SHINE introduces manifold-steered anchor loss, leveraging pretrained customization adapters (e.g., IP-Adapter) to guide latents for faithful subject representation while preserving background integrity. Degradation-suppression guidance and adaptive background blending are proposed to further eliminate low-quality outputs and visible seams. To address the lack of rigorous benchmarks, we introduce ComplexCompo, featuring diverse resolutions and challenging conditions such as low lighting, strong illumination, intricate shadows, and reflective surfaces. Experiments on ComplexCompo and DreamEditBench show state-of-the-art performance on standard metrics (e.g., DINOv2) and human-aligned scores (e.g., DreamSim, ImageReward, VisionReward). Code and benchmark will be publicly available upon publication. </p>
<blockquote>
<p>å›¾åƒç»„åˆæ—¨åœ¨æ— ç¼æ’å…¥ç”¨æˆ·æŒ‡å®šçš„å¯¹è±¡åˆ°æ–°åœºæ™¯ä¸­ï¼Œä½†ç°æœ‰æ¨¡å‹åœ¨å¤„ç†å¤æ‚å…‰ç…§ï¼ˆä¾‹å¦‚å‡†ç¡®é˜´å½±ã€æ°´é¢åå°„ï¼‰å’Œå¤šæ ·åŒ–ã€é«˜åˆ†è¾¨ç‡è¾“å…¥æ—¶é‡åˆ°å›°éš¾ã€‚ç°ä»£æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼ˆä¾‹å¦‚SD3.5ã€FLUXï¼‰å·²ç»ç¼–ç äº†åŸºæœ¬çš„ç‰©ç†å’Œåˆ†è¾¨ç‡å…ˆéªŒçŸ¥è¯†ï¼Œä½†ç¼ºä¹ä¸€ä¸ªæ¡†æ¶æ¥é‡Šæ”¾å®ƒä»¬ï¼Œè€Œä¸å¿…æ±‚åŠ©äºæ½œåœ¨çš„åæ¼”ï¼Œè¿™é€šå¸¸ä¼šå°†å¯¹è±¡å§¿åŠ¿é”å®šåœ¨ä¸Šä¸‹æ–‡ä¸é€‚å½“çš„æ–¹å‘ä¸Šï¼Œæˆ–è„†å¼±çš„æ³¨æ„åŠ›æ‰‹æœ¯ã€‚æˆ‘ä»¬æå‡ºSHINEï¼Œè¿™æ˜¯ä¸€ä¸ªæ— éœ€è®­ç»ƒçš„æ— ç¼ã€é«˜ä¿çœŸæ’å…¥ä¸­æ€§åŒ–é”™è¯¯æ¡†æ¶ã€‚SHINEå¼•å…¥äº†æµå½¢å¼•å¯¼é”šç‚¹æŸå¤±ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„å®šåˆ¶é€‚é…å™¨ï¼ˆä¾‹å¦‚IP-Adapterï¼‰æ¥å¼•å¯¼æ½œåœ¨è¡¨ç¤ºä»¥å¿ å®äºä¸»é¢˜è¡¨ç¤ºï¼ŒåŒæ—¶ä¿æŒèƒŒæ™¯å®Œæ•´æ€§ã€‚æå‡ºé€€åŒ–æŠ‘åˆ¶æŒ‡å¯¼å’Œè‡ªé€‚åº”èƒŒæ™¯èåˆï¼Œä»¥è¿›ä¸€æ­¥æ¶ˆé™¤ä½è´¨é‡è¾“å‡ºå’Œå¯è§æ¥ç¼ã€‚ä¸ºäº†è§£å†³ç¼ºä¹ä¸¥æ ¼åŸºå‡†çš„é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ComplexCompoï¼Œå®ƒå…·æœ‰å¤šç§åˆ†è¾¨ç‡å’ŒæŒ‘æˆ˜æ€§æ¡ä»¶ï¼Œä¾‹å¦‚ä½å…‰ç…§ã€å¼ºç…§æ˜ã€å¤æ‚çš„é˜´å½±å’Œåå°„è¡¨é¢ã€‚åœ¨ComplexCompoå’ŒDreamEditBenchä¸Šçš„å®éªŒè¡¨æ˜ï¼Œå…¶åœ¨æ ‡å‡†æŒ‡æ ‡ï¼ˆä¾‹å¦‚DINOv2ï¼‰å’Œäººç±»å¯¹é½åˆ†æ•°ï¼ˆä¾‹å¦‚DreamSimã€ImageRewardã€VisionRewardï¼‰ä¸Šå‡è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚ä»£ç å’ŒåŸºå‡†å°†åœ¨å‘å¸ƒæ—¶å…¬å¼€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.21278v1">PDF</a> Preprint</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†SHINEæ¡†æ¶ï¼Œå®ƒæ˜¯ä¸€ç§æ— éœ€è®­ç»ƒå³å¯å®ç°æ— ç¼ã€é«˜ä¿çœŸæ’å…¥çš„æŠ€æœ¯ï¼Œé€šè¿‡å¼•å…¥æµå½¢å¼•å¯¼é”šç‚¹æŸå¤±å’Œé¢„è®­ç»ƒå®šåˆ¶é€‚é…å™¨ï¼Œèƒ½å¤Ÿåœ¨ä¸ç ´åèƒŒæ™¯å®Œæ•´æ€§çš„æƒ…å†µä¸‹ï¼Œå¿ å®å‘ˆç°ä¸»é¢˜è¡¨ç¤ºã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†é€€åŒ–æŠ‘åˆ¶æŒ‡å¯¼å’Œè‡ªé€‚åº”èƒŒæ™¯èåˆç­‰æ–¹æ³•ï¼Œä»¥æ¶ˆé™¤ä½è´¨é‡è¾“å‡ºå’Œå¯è§æ¥ç¼ã€‚ä¸ºè§£å†³ç¼ºä¹ä¸¥æ ¼åŸºå‡†çš„é—®é¢˜ï¼Œå¼•å…¥äº†ComplexCompoåŸºå‡†æµ‹è¯•ï¼Œå¯¹å¤šç§åˆ†è¾¨ç‡å’Œå¤æ‚æ¡ä»¶ä¸‹çš„å›¾åƒç»„åˆæ€§èƒ½è¿›è¡Œè¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSHINEæ¡†æ¶åœ¨ComplexCompoå’ŒDreamEditBenchä¸Šçš„è¡¨ç°è¾¾åˆ°æˆ–è¶…è¶Šäº†ç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SHINEæ¡†æ¶æ—¨åœ¨è§£å†³ç°æœ‰æ¨¡å‹åœ¨å¤„ç†å¤æ‚å…‰ç…§å’Œå¤šæ ·åŒ–é«˜åˆ†è¾¨ç‡è¾“å…¥æ—¶çš„æ— ç¼æ’å…¥éš¾é¢˜ã€‚</li>
<li>SHINEæ¡†æ¶å¼•å…¥äº†æµå½¢å¼•å¯¼é”šç‚¹æŸå¤±ï¼Œåˆ©ç”¨é¢„è®­ç»ƒå®šåˆ¶é€‚é…å™¨æ¥å¼•å¯¼æ½œåœ¨è¡¨ç¤ºï¼Œå¿ å®å‘ˆç°ä¸»é¢˜è¡¨ç¤ºåŒæ—¶ä¿æŒèƒŒæ™¯å®Œæ•´æ€§ã€‚</li>
<li>é€šè¿‡æå‡ºé€€åŒ–æŠ‘åˆ¶æŒ‡å¯¼å’Œè‡ªé€‚åº”èƒŒæ™¯èåˆç­‰æ–¹æ³•ï¼Œè¿›ä¸€æ­¥æ¶ˆé™¤ä½è´¨é‡è¾“å‡ºå’Œå¯è§æ¥ç¼ã€‚</li>
<li>ComplexCompoåŸºå‡†æµ‹è¯•è¢«å¼•å…¥ï¼Œä»¥è¯„ä¼°å›¾åƒç»„åˆæ€§èƒ½ï¼ŒåŒ…æ‹¬å¤šç§åˆ†è¾¨ç‡å’Œå¤æ‚æ¡ä»¶ä¸‹çš„æŒ‘æˆ˜ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒSHINEæ¡†æ¶åœ¨ComplexCompoå’ŒDreamEditBenchä¸Šçš„è¡¨ç°è¾¾åˆ°æˆ–è¶…è¶Šäº†ç°æœ‰æŠ€æœ¯ã€‚</li>
<li>SHINEæ¡†æ¶ä¸éœ€è¦è®­ç»ƒå³å¯åº”ç”¨ï¼Œè§£å†³äº†ç°æœ‰æ¨¡å‹å¦‚æ½œç‚¹é€†è½¬ç­‰é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.21278">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9cb54fbd964c60d0b33ba0779ad23f69" align="middle">
<img src="https://picx.zhimg.com/v2-543e394eb19c3f4ecc9d77842b8bba99" align="middle">
<img src="https://picx.zhimg.com/v2-19385df0e2e4cd68732e1d4fb866703f" align="middle">
<img src="https://picx.zhimg.com/v2-b1357ed26f6f88de226bcfaa81081cc9" align="middle">
<img src="https://picx.zhimg.com/v2-4429b55330372bb1e8e99e9715e946f0" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="T2I-Diff-fMRI-Signal-Generation-via-Time-Frequency-Image-Transform-and-Classifier-Free-Denoising-Diffusion-Models"><a href="#T2I-Diff-fMRI-Signal-Generation-via-Time-Frequency-Image-Transform-and-Classifier-Free-Denoising-Diffusion-Models" class="headerlink" title="T2I-Diff: fMRI Signal Generation via Time-Frequency Image Transform and   Classifier-Free Denoising Diffusion Models"></a>T2I-Diff: fMRI Signal Generation via Time-Frequency Image Transform and   Classifier-Free Denoising Diffusion Models</h2><p><strong>Authors:Hwa Hui Tew, Junn Yong Loo, Yee-Fan Tan, Xinyu Tang, Hernando Ombao, Fuad Noman, Raphael C. -W. Phan, Chee-Ming Ting</strong></p>
<p>Functional Magnetic Resonance Imaging (fMRI) is an advanced neuroimaging method that enables in-depth analysis of brain activity by measuring dynamic changes in the blood oxygenation level-dependent (BOLD) signals. However, the resource-intensive nature of fMRI data acquisition limits the availability of high-fidelity samples required for data-driven brain analysis models. While modern generative models can synthesize fMRI data, they often underperform because they overlook the complex non-stationarity and nonlinear BOLD dynamics. To address these challenges, we introduce T2I-Diff, an fMRI generation framework that leverages time-frequency representation of BOLD signals and classifier-free denoising diffusion. Specifically, our framework first converts BOLD signals into windowed spectrograms via a time-dependent Fourier transform, capturing both the underlying temporal dynamics and spectral evolution. Subsequently, a classifier-free diffusion model is trained to generate class-conditioned frequency spectrograms, which are then reverted to BOLD signals via inverse Fourier transforms. Finally, we validate the efficacy of our approach by demonstrating improved accuracy and generalization in downstream fMRI-based brain network classification. </p>
<blockquote>
<p>åŠŸèƒ½ç£å…±æŒ¯æˆåƒï¼ˆfMRIï¼‰æ˜¯ä¸€ç§å…ˆè¿›çš„ç¥ç»æˆåƒæ–¹æ³•ï¼Œå®ƒé€šè¿‡æµ‹é‡è¡€æ°§æ°´å¹³ä¾èµ–ï¼ˆBOLDï¼‰ä¿¡å·çš„åŠ¨æ€å˜åŒ–ï¼Œå¯¹è„‘æ´»åŠ¨è¿›è¡Œæ·±å…¥åˆ†æã€‚ç„¶è€Œï¼ŒfMRIæ•°æ®è·å–èµ„æºå¯†é›†å‹çš„ç‰¹æ€§ï¼Œé™åˆ¶äº†å¯¹æ•°æ®é©±åŠ¨çš„å¤§è„‘åˆ†ææ¨¡å‹æ‰€éœ€çš„é«˜ä¿çœŸæ ·æœ¬çš„å¯ç”¨æ€§ã€‚è™½ç„¶ç°ä»£ç”Ÿæˆæ¨¡å‹å¯ä»¥åˆæˆfMRIæ•°æ®ï¼Œä½†å®ƒä»¬å¾€å¾€è¡¨ç°ä¸ä½³ï¼Œå› ä¸ºå®ƒä»¬å¿½ç•¥äº†å¤æ‚çš„éå¹³ç¨³å’Œéçº¿æ€§BOLDåŠ¨æ€ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†T2I-Diff fMRIç”Ÿæˆæ¡†æ¶ï¼Œå®ƒåˆ©ç”¨BOLDä¿¡å·çš„æ—¶é¢‘è¡¨ç¤ºå’Œæ— åˆ†ç±»å™¨å»å™ªæ‰©æ•£ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ¡†æ¶é¦–å…ˆé€šè¿‡æ—¶é—´ä¾èµ–å‚…ç«‹å¶å˜æ¢å°†BOLDä¿¡å·è½¬æ¢ä¸ºåˆ†çª—é¢‘è°±å›¾ï¼Œæ•è·æ½œåœ¨çš„åŠ¨æ€å˜åŒ–å’Œå…‰è°±æ¼”åŒ–ã€‚éšåï¼Œè®­ç»ƒä¸€ä¸ªæ— åˆ†ç±»å™¨æ‰©æ•£æ¨¡å‹æ¥ç”Ÿæˆç±»åˆ«æ¡ä»¶é¢‘ç‡é¢‘è°±å›¾ï¼Œç„¶åå†é€šè¿‡å‚…ç«‹å¶é€†å˜æ¢å°†å…¶è½¬å›BOLDä¿¡å·ã€‚æœ€åï¼Œæˆ‘ä»¬é€šè¿‡å±•ç¤ºä¸‹æ¸¸fMRIè„‘ç½‘ç»œåˆ†ç±»çš„æ›´é«˜å‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼ŒéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.20822v1">PDF</a> Accepted at the 28th International Conference on Medical Image   Computing and Computer Assisted Intervention (MICCAI 2025)</p>
<p><strong>æ‘˜è¦</strong><br>    fMRIæ˜¯ä¸€ç§å…ˆè¿›çš„ç¥ç»æˆåƒæ–¹æ³•ï¼Œé€šè¿‡å¯¹è¡€æ¶²ä¸­æ°§å«é‡å˜åŒ–çš„åŠ¨æ€æµ‹é‡æ¥æ·±å…¥åˆ†æè„‘æ´»åŠ¨ã€‚ä½†ç”±äºfMRIæ•°æ®é‡‡é›†èµ„æºå¯†é›†ï¼Œå¯¹æ•°æ®é©±åŠ¨çš„å¤§è„‘åˆ†ææ¨¡å‹æ‰€éœ€çš„é«˜ä¿çœŸæ ·æœ¬å¯ç”¨æ€§æœ‰é™ã€‚ä¸ºè§£å†³ç°ä»£ç”Ÿæˆæ¨¡å‹å¿½ç•¥å¤æ‚éå¹³ç¨³å’Œéçº¿æ€§BOLDåŠ¨åŠ›å­¦çš„é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†T2I-Diff fMRIç”Ÿæˆæ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡æ—¶é—´ä¾èµ–çš„å‚…ç«‹å¶å˜æ¢å°†BOLDä¿¡å·è½¬æ¢ä¸ºåˆ†çª—é¢‘è°±å›¾ï¼Œæ•è·åŸºæœ¬çš„æ—¶é—´åŠ¨æ€å’Œé¢‘è°±æ¼”å˜ã€‚ç„¶åè®­ç»ƒæ— åˆ†ç±»å™¨å»å™ªæ‰©æ•£æ¨¡å‹æ¥ç”Ÿæˆç±»æ¡ä»¶é¢‘ç‡é¢‘è°±å›¾ï¼Œå†ç»åå‚…ç«‹å¶å˜æ¢è¿˜åŸä¸ºBOLDä¿¡å·ã€‚æœ€åï¼Œæˆ‘ä»¬éªŒè¯äº†è¯¥æ–¹æ³•åœ¨æé«˜ä¸‹æ¸¸fMRIè„‘ç½‘ç»œåˆ†ç±»çš„å‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>fMRIæ˜¯ä¸€ç§é€šè¿‡æµ‹é‡è¡€æ¶²ä¸­æ°§å«é‡å˜åŒ–çš„åŠ¨æ€æ¥æ·±å…¥åˆ†æè„‘æ´»åŠ¨çš„å…ˆè¿›ç¥ç»æˆåƒæ–¹æ³•ã€‚</li>
<li>ç”±äºfMRIæ•°æ®é‡‡é›†çš„èµ„æºå¯†é›†æ€§ï¼Œé«˜ä¿çœŸæ ·æœ¬çš„å¯ç”¨æ€§å—åˆ°é™åˆ¶ã€‚</li>
<li>ç°ä»£ç”Ÿæˆæ¨¡å‹åœ¨åˆæˆfMRIæ•°æ®æ—¶å¸¸å¸¸è¡¨ç°ä¸ä½³ï¼Œå› ä¸ºå®ƒä»¬å¿½ç•¥äº†å¤æ‚çš„éå¹³ç¨³å’Œéçº¿æ€§BOLDåŠ¨åŠ›å­¦ã€‚</li>
<li>T2I-Diffæ¡†æ¶é€šè¿‡æ—¶é—´ä¾èµ–çš„å‚…ç«‹å¶å˜æ¢å°†BOLDä¿¡å·è½¬æ¢ä¸ºåˆ†çª—é¢‘è°±å›¾ï¼Œä»¥æ•è·åŸºæœ¬çš„æ—¶é—´åŠ¨æ€å’Œé¢‘è°±æ¼”å˜ã€‚</li>
<li>T2I-Diffæ¡†æ¶ä½¿ç”¨æ— åˆ†ç±»å™¨å»å™ªæ‰©æ•£æ¨¡å‹ç”Ÿæˆç±»æ¡ä»¶é¢‘ç‡é¢‘è°±å›¾ã€‚</li>
<li>ç”Ÿæˆçš„åˆ†çª—é¢‘è°±å›¾é€šè¿‡åå‚…ç«‹å¶å˜æ¢è¿˜åŸä¸ºBOLDä¿¡å·ã€‚</li>
<li>T2I-Diffæ¡†æ¶æé«˜äº†ä¸‹æ¸¸fMRIè„‘ç½‘ç»œåˆ†ç±»çš„å‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.20822">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e330e32ccfbd4626b9161cb45e4a2b49" align="middle">
<img src="https://picx.zhimg.com/v2-6b60248d05ddd2d8a4b6ca27935907f0" align="middle">
<img src="https://picx.zhimg.com/v2-d638fe07fb8391dd61092d56af8c92c2" align="middle">
<img src="https://picx.zhimg.com/v2-823862ace60737ab3ef7b2008df41b90" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="CusEnhancer-A-Zero-Shot-Scene-and-Controllability-Enhancement-Method-for-Photo-Customization-via-ResInversion"><a href="#CusEnhancer-A-Zero-Shot-Scene-and-Controllability-Enhancement-Method-for-Photo-Customization-via-ResInversion" class="headerlink" title="CusEnhancer: A Zero-Shot Scene and Controllability Enhancement Method   for Photo Customization via ResInversion"></a>CusEnhancer: A Zero-Shot Scene and Controllability Enhancement Method   for Photo Customization via ResInversion</h2><p><strong>Authors:Maoye Ren, Praneetha Vaddamanu, Jianjin Xu, Fernando De la Torre Frade</strong></p>
<p>Recently remarkable progress has been made in synthesizing realistic human photos using text-to-image diffusion models. However, current approaches face degraded scenes, insufficient control, and suboptimal perceptual identity. We introduce CustomEnhancer, a novel framework to augment existing identity customization models. CustomEnhancer is a zero-shot enhancement pipeline that leverages face swapping techniques, pretrained diffusion model, to obtain additional representations in a zeroshot manner for encoding into personalized models. Through our proposed triple-flow fused PerGeneration approach, which identifies and combines two compatible counter-directional latent spaces to manipulate a pivotal space of personalized model, we unify the generation and reconstruction processes, realizing generation from three flows. Our pipeline also enables comprehensive training-free control over the generation process of personalized models, offering precise controlled personalization for them and eliminating the need for controller retraining for per-model. Besides, to address the high time complexity of null-text inversion (NTI), we introduce ResInversion, a novel inversion method that performs noise rectification via a pre-diffusion mechanism, reducing the inversion time by 129 times. Experiments demonstrate that CustomEnhancer reach SOTA results at scene diversity, identity fidelity, training-free controls, while also showing the efficiency of our ResInversion over NTI. The code will be made publicly available upon paper acceptance. </p>
<blockquote>
<p>è¿‘æœŸï¼Œåˆ©ç”¨æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹åˆæˆé€¼çœŸçš„äººç‰©ç…§ç‰‡æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ–¹æ³•ä»ç„¶é¢ä¸´åœºæ™¯è´¨é‡ä¸‹é™ã€æ§åˆ¶ä¸è¶³ä»¥åŠæ„ŸçŸ¥èº«ä»½ä¸ç†æƒ³ç­‰é—®é¢˜ã€‚æˆ‘ä»¬å¼•å…¥äº†CustomEnhancerï¼Œè¿™æ˜¯ä¸€ä¸ªå¢å¼ºç°æœ‰èº«ä»½å®šåˆ¶æ¨¡å‹çš„æ–°å‹æ¡†æ¶ã€‚CustomEnhanceræ˜¯ä¸€ç§é›¶æ ·æœ¬å¢å¼ºæµç¨‹ï¼Œå®ƒåˆ©ç”¨é¢éƒ¨æ›¿æ¢æŠ€æœ¯ã€é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ï¼Œä»¥é›¶æ ·æœ¬çš„æ–¹å¼è·å¾—é¢å¤–çš„è¡¨ç¤ºï¼Œç„¶åç¼–ç åˆ°ä¸ªæ€§åŒ–æ¨¡å‹ä¸­ã€‚é€šè¿‡æˆ‘ä»¬æå‡ºçš„èåˆPerGenerationçš„ä¸‰é‡æµæ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿè¯†åˆ«å’Œç»“åˆä¸¤ä¸ªå…¼å®¹çš„é€†å‘æ½œåœ¨ç©ºé—´æ¥æ“ä½œä¸ªæ€§åŒ–æ¨¡å‹çš„å…³é”®ç©ºé—´ï¼Œæˆ‘ä»¬ç»Ÿä¸€äº†ç”Ÿæˆå’Œé‡å»ºè¿‡ç¨‹ï¼Œå®ç°äº†ä»ä¸‰ä¸ªæµçš„ç”Ÿæˆã€‚æˆ‘ä»¬çš„æµç¨‹è¿˜ä¸ºä¸ªæ€§åŒ–æ¨¡å‹çš„ç”Ÿæˆè¿‡ç¨‹æä¾›äº†å…¨é¢çš„æ— è®­ç»ƒæ§åˆ¶ï¼Œä¸ºå®ƒä»¬æä¾›äº†ç²¾ç¡®çš„æ§åˆ¶ä¸ªæ€§åŒ–ï¼Œå¹¶æ¶ˆé™¤äº†å¯¹ä¸ªæ€§åŒ–æ¨¡å‹çš„æ§åˆ¶å™¨å†è®­ç»ƒçš„éœ€æ±‚ã€‚æ­¤å¤–ï¼Œä¸ºäº†è§£å†³æ— æ–‡æœ¬åè½¬ï¼ˆNTIï¼‰çš„é«˜æ—¶é—´å¤æ‚åº¦é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ResInversionï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„åè½¬æ–¹æ³•ï¼Œå®ƒé€šè¿‡é¢„æ‰©æ•£æœºåˆ¶è¿›è¡Œå™ªå£°æ ¡æ­£ï¼Œå°†åè½¬æ—¶é—´å‡å°‘äº†129å€ã€‚å®éªŒè¡¨æ˜ï¼ŒCustomEnhanceråœ¨åœºæ™¯å¤šæ ·æ€§ã€èº«ä»½ä¿çœŸåº¦ã€æ— è®­ç»ƒæ§åˆ¶ç­‰æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼ŒåŒæ—¶æˆ‘ä»¬çš„ResInversionç›¸æ¯”NTIä¹Ÿæ˜¾ç¤ºå‡ºé«˜æ•ˆç‡ã€‚è®ºæ–‡è¢«æ¥å—åï¼Œä»£ç å°†å…¬å¼€å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.20775v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä½¿ç”¨æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹åˆæˆé€¼çœŸçš„äººåƒç…§ç‰‡çš„æœ€æ–°è¿›å±•ï¼Œå¹¶æŒ‡å‡ºå½“å‰æ–¹æ³•é¢ä¸´åœºæ™¯è´¨é‡ä¸‹é™ã€æ§åˆ¶ä¸è¶³å’Œæ„ŸçŸ¥èº«ä»½ä¸ä¼˜ç­‰é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…æå‡ºäº†ä¸€ç§æ–°å‹æ¡†æ¶CustomEnhancerï¼Œç»“åˆé¢éƒ¨æ›¿æ¢æŠ€æœ¯å’Œé¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ï¼Œä»¥é›¶æ ·æœ¬æ–¹å¼è·å¾—é¢å¤–è¡¨ç¤ºå¹¶å°†å…¶ç¼–ç ä¸ºä¸ªæ€§åŒ–æ¨¡å‹ã€‚é€šè¿‡æå‡ºçš„ä¸‰æµèåˆPerGenerationæ–¹æ³•ï¼Œç»Ÿä¸€ç”Ÿæˆå’Œé‡å»ºè¿‡ç¨‹ï¼Œå®ç°ä¸‰æµç”Ÿæˆã€‚æ­¤å¤–ï¼ŒCustomEnhancerè¿˜æä¾›å…¨é¢çš„è®­ç»ƒå‰æ§åˆ¶ä¸ªæ€§åŒ–æ¨¡å‹ç”Ÿæˆè¿‡ç¨‹ï¼Œæ¶ˆé™¤å¯¹æ§åˆ¶å™¨è¿›è¡Œé€æ¨¡å‹é‡è®­çš„éœ€æ±‚ã€‚ä¸ºè§£å†³ç©ºæ–‡æœ¬åè½¬ï¼ˆNTIï¼‰çš„é«˜æ—¶é—´å¤æ‚åº¦é—®é¢˜ï¼Œå¼•å…¥ResInversionæ–°åè½¬æ–¹æ³•ï¼Œé€šè¿‡é¢„æ‰©æ•£æœºåˆ¶è¿›è¡Œå™ªå£°æ ¡æ­£ï¼Œå°†åè½¬æ—¶é—´ç¼©çŸ­129å€ã€‚å®éªŒè¡¨æ˜ï¼ŒCustomEnhanceråœ¨åœºæ™¯å¤šæ ·æ€§ã€èº«ä»½ä¿çœŸåº¦å’Œè®­ç»ƒå‰æ§åˆ¶æ–¹é¢è¾¾åˆ°æœ€æ–°æ°´å¹³ï¼ŒåŒæ—¶ResInversionç›¸æ¯”NTIä¹Ÿå±•ç°å‡ºé«˜æ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ€æ–°ç ”ç©¶åœ¨åˆ©ç”¨æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹åˆæˆäººåƒç…§ç‰‡æ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†å­˜åœ¨åœºæ™¯è´¨é‡ã€æ§åˆ¶å’Œæ„ŸçŸ¥èº«ä»½ç­‰é—®é¢˜ã€‚</li>
<li>CustomEnhanceræ¡†æ¶ç»“åˆé¢éƒ¨æ›¿æ¢æŠ€æœ¯å’Œé¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ï¼Œæå‡ä¸ªæ€§åŒ–æ¨¡å‹çš„ç”Ÿæˆè´¨é‡ã€‚</li>
<li>PerGenerationæ–¹æ³•é€šè¿‡èåˆä¸‰æµæŠ€æœ¯ï¼Œç»Ÿä¸€ç”Ÿæˆå’Œé‡å»ºè¿‡ç¨‹ã€‚</li>
<li>CustomEnhanceræä¾›å…¨é¢çš„è®­ç»ƒå‰æ§åˆ¶ï¼Œæ”¯æŒä¸ªæ€§åŒ–æ¨¡å‹çš„ç²¾å‡†æ§åˆ¶ï¼Œæ— éœ€é€æ¨¡å‹é‡è®­æ§åˆ¶å™¨ã€‚</li>
<li>ç ”ç©¶äººå‘˜ä¸ºè§£å†³ç©ºæ–‡æœ¬åè½¬çš„é«˜æ—¶é—´å¤æ‚åº¦é—®é¢˜ï¼Œå¼•å…¥äº†ResInversionæ–°åè½¬æ–¹æ³•ã€‚</li>
<li>ResInversioné€šè¿‡é¢„æ‰©æ•£æœºåˆ¶è¿›è¡Œå™ªå£°æ ¡æ­£ï¼Œæ˜¾è‘—ç¼©çŸ­åè½¬æ—¶é—´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.20775">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a3a4af467c6b46d959a99125ece09e19" align="middle">
<img src="https://picx.zhimg.com/v2-b61cb09dd5f06b50d01400688a485296" align="middle">
<img src="https://picx.zhimg.com/v2-465cef20062acec8cb146b8c5b964d76" align="middle">
<img src="https://picx.zhimg.com/v2-9a88d910854ba9c5a4466f73452b0a40" align="middle">
<img src="https://picx.zhimg.com/v2-023e9a71e42f1e1b3683678a01dfa2e3" align="middle">
<img src="https://picx.zhimg.com/v2-a9d42c694b7fe53c5104dce5bf468336" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="PhysCtrl-Generative-Physics-for-Controllable-and-Physics-Grounded-Video-Generation"><a href="#PhysCtrl-Generative-Physics-for-Controllable-and-Physics-Grounded-Video-Generation" class="headerlink" title="PhysCtrl: Generative Physics for Controllable and Physics-Grounded Video   Generation"></a>PhysCtrl: Generative Physics for Controllable and Physics-Grounded Video   Generation</h2><p><strong>Authors:Chen Wang, Chuhao Chen, Yiming Huang, Zhiyang Dou, Yuan Liu, Jiatao Gu, Lingjie Liu</strong></p>
<p>Existing video generation models excel at producing photo-realistic videos from text or images, but often lack physical plausibility and 3D controllability. To overcome these limitations, we introduce PhysCtrl, a novel framework for physics-grounded image-to-video generation with physical parameters and force control. At its core is a generative physics network that learns the distribution of physical dynamics across four materials (elastic, sand, plasticine, and rigid) via a diffusion model conditioned on physics parameters and applied forces. We represent physical dynamics as 3D point trajectories and train on a large-scale synthetic dataset of 550K animations generated by physics simulators. We enhance the diffusion model with a novel spatiotemporal attention block that emulates particle interactions and incorporates physics-based constraints during training to enforce physical plausibility. Experiments show that PhysCtrl generates realistic, physics-grounded motion trajectories which, when used to drive image-to-video models, yield high-fidelity, controllable videos that outperform existing methods in both visual quality and physical plausibility. Project Page: <a target="_blank" rel="noopener" href="https://cwchenwang.github.io/physctrl">https://cwchenwang.github.io/physctrl</a> </p>
<blockquote>
<p>ç°æœ‰è§†é¢‘ç”Ÿæˆæ¨¡å‹æ“…é•¿ä»æ–‡æœ¬æˆ–å›¾åƒç”Ÿæˆé€¼çœŸçš„è§†é¢‘ï¼Œä½†å¾€å¾€ç¼ºä¹ç‰©ç†å¯è¡Œæ€§å’Œ3Då¯æ§æ€§ã€‚ä¸ºäº†å…‹æœè¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬å¼•å…¥äº†PhysCtrlï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„åŸºäºç‰©ç†çš„å›¾åƒåˆ°è§†é¢‘ç”Ÿæˆæ¡†æ¶ï¼Œå…·æœ‰ç‰©ç†å‚æ•°å’ŒåŠ›æ§åˆ¶åŠŸèƒ½ã€‚å…¶æ ¸å¿ƒæ˜¯ä¸€ä¸ªç”Ÿæˆç‰©ç†ç½‘ç»œï¼Œé€šè¿‡æ‰©æ•£æ¨¡å‹å­¦ä¹ å››ç§ææ–™ï¼ˆå¼¹æ€§ã€æ²™å­ã€å¡‘æ–™å’Œåˆšæ€§ï¼‰çš„ç‰©ç†åŠ¨æ€åˆ†å¸ƒï¼Œè¯¥æ¨¡å‹æ ¹æ®ç‰©ç†å‚æ•°å’Œåº”ç”¨åŠ›è¿›è¡Œæ¡ä»¶æ§åˆ¶ã€‚æˆ‘ä»¬å°†ç‰©ç†åŠ¨æ€è¡¨ç¤ºä¸º3Dç‚¹è½¨è¿¹ï¼Œå¹¶åœ¨ç”±ç‰©ç†æ¨¡æ‹Ÿå™¨ç”Ÿæˆçš„å¤§è§„æ¨¡åˆæˆæ•°æ®é›†ï¼ˆ55ä¸‡ä¸ªåŠ¨ç”»ï¼‰ä¸Šè¿›è¡Œè®­ç»ƒã€‚æˆ‘ä»¬å¢å¼ºæ‰©æ•£æ¨¡å‹ï¼Œå¢åŠ äº†ä¸€ç§æ–°å‹æ—¶ç©ºæ³¨æ„åŠ›å—ï¼Œè¯¥å—æ¨¡æ‹Ÿç²’å­äº¤äº’å¹¶åœ¨è®­ç»ƒæœŸé—´åŠ å…¥åŸºäºç‰©ç†çš„çº¦æŸï¼Œä»¥å¼ºåˆ¶ç‰©ç†åˆç†æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒPhysCtrlç”Ÿæˆçš„ç‰©ç†åŸºç¡€è¿åŠ¨è½¨è¿¹ç°å®ï¼Œå½“ç”¨äºé©±åŠ¨å›¾åƒåˆ°è§†é¢‘æ¨¡å‹æ—¶ï¼Œå¯ä»¥ç”Ÿæˆé«˜è´¨é‡ã€å¯æ§çš„è§†é¢‘ï¼Œåœ¨è§†è§‰è´¨é‡å’Œç‰©ç†åˆç†æ€§æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://cwchenwang.github.io/physctrl">https://cwchenwang.github.io/physctrl</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.20358v1">PDF</a> Accepted by NeurIPS 2025. This is the preview version; the   camera-ready version is still in preparation</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºPhysCtrlçš„æ–°å‹æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†ç‰©ç†å‚æ•°å’ŒåŠ›æ§åˆ¶ï¼Œç”¨äºåŸºäºå›¾åƒçš„è§†é¢‘ç”Ÿæˆã€‚å®ƒä½¿ç”¨æ‰©æ•£æ¨¡å‹å­¦ä¹ å››ç§ææ–™çš„ç‰©ç†åŠ¨æ€åˆ†å¸ƒï¼Œå¹¶é€šè¿‡å¤§å‹åˆæˆæ•°æ®é›†è¿›è¡Œè®­ç»ƒã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†ä¸€ç§æ–°çš„æ—¶ç©ºæ³¨æ„åŠ›å—ï¼Œç”¨äºæ¨¡æ‹Ÿç²’å­äº¤äº’å¹¶åœ¨è®­ç»ƒä¸­èå…¥ç‰©ç†çº¦æŸï¼Œä»¥ç”ŸæˆçœŸå®ä¸”ç¬¦åˆç‰©ç†è§„å¾‹çš„è½¨è¿¹ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡ä¸”å¯æ§çš„è§†é¢‘ï¼Œåœ¨è§†è§‰è´¨é‡å’Œç‰©ç†çœŸå®æ€§æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PhysCtrlæ¡†æ¶ç»“åˆäº†ç‰©ç†å‚æ•°å’ŒåŠ›æ§åˆ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰è§†é¢‘ç”Ÿæˆæ¨¡å‹åœ¨ç‰©ç†çœŸå®æ€§å’Œ3Då¯æ§æ€§æ–¹é¢çš„å±€é™æ€§ã€‚</li>
<li>å¼•å…¥ç”Ÿæˆç‰©ç†ç½‘ç»œï¼Œé€šè¿‡æ‰©æ•£æ¨¡å‹å­¦ä¹ å››ç§ææ–™çš„ç‰©ç†åŠ¨æ€åˆ†å¸ƒã€‚</li>
<li>ä½¿ç”¨å¤§å‹åˆæˆæ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼ŒåŒ…å«550Kä¸ªåŠ¨ç”»ã€‚</li>
<li>å¼•å…¥æ–°çš„æ—¶ç©ºæ³¨æ„åŠ›å—ï¼Œæ¨¡æ‹Ÿç²’å­äº¤äº’ï¼Œå¹¶åœ¨è®­ç»ƒä¸­èå…¥ç‰©ç†çº¦æŸã€‚</li>
<li>PhysCtrlèƒ½å¤Ÿç”ŸæˆçœŸå®ä¸”ç¬¦åˆç‰©ç†è§„å¾‹çš„è½¨è¿¹ã€‚</li>
<li>å½“ç”¨äºé©±åŠ¨å›¾åƒåˆ°è§†é¢‘æ¨¡å‹æ—¶ï¼ŒPhysCtrlç”Ÿæˆçš„é«˜è´¨é‡ä¸”å¯æ§çš„è§†é¢‘åœ¨è§†è§‰è´¨é‡å’Œç‰©ç†çœŸå®æ€§æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.20358">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7e9a2ea67b62b516b3135bc2a3c2b12f" align="middle">
<img src="https://picx.zhimg.com/v2-71fad2fd258ab0947f288140cd7d3f16" align="middle">
<img src="https://picx.zhimg.com/v2-93323784143bd33cb3f3e932f5b95c61" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="4D-Driving-Scene-Generation-With-Stereo-Forcing"><a href="#4D-Driving-Scene-Generation-With-Stereo-Forcing" class="headerlink" title="4D Driving Scene Generation With Stereo Forcing"></a>4D Driving Scene Generation With Stereo Forcing</h2><p><strong>Authors:Hao Lu, Zhuang Ma, Guangfeng Jiang, Wenhang Ge, Bohan Li, Yuzhan Cai, Wenzhao Zheng, Yunpeng Zhang, Yingcong Chen</strong></p>
<p>Current generative models struggle to synthesize dynamic 4D driving scenes that simultaneously support temporal extrapolation and spatial novel view synthesis (NVS) without per-scene optimization. Bridging generation and novel view synthesis remains a major challenge. We present PhiGenesis, a unified framework for 4D scene generation that extends video generation techniques with geometric and temporal consistency. Given multi-view image sequences and camera parameters, PhiGenesis produces temporally continuous 4D Gaussian splatting representations along target 3D trajectories. In its first stage, PhiGenesis leverages a pre-trained video VAE with a novel range-view adapter to enable feed-forward 4D reconstruction from multi-view images. This architecture supports single-frame or video inputs and outputs complete 4D scenes including geometry, semantics, and motion. In the second stage, PhiGenesis introduces a geometric-guided video diffusion model, using rendered historical 4D scenes as priors to generate future views conditioned on trajectories. To address geometric exposure bias in novel views, we propose Stereo Forcing, a novel conditioning strategy that integrates geometric uncertainty during denoising. This method enhances temporal coherence by dynamically adjusting generative influence based on uncertainty-aware perturbations. Our experimental results demonstrate that our method achieves state-of-the-art performance in both appearance and geometric reconstruction, temporal generation and novel view synthesis (NVS) tasks, while simultaneously delivering competitive performance in downstream evaluations. Homepage is at \href{<a target="_blank" rel="noopener" href="https://jiangxb98.github.io/PhiGensis%7D%7BPhiGensis%7D">https://jiangxb98.github.io/PhiGensis}{PhiGensis}</a>. </p>
<blockquote>
<p>å½“å‰ç”Ÿæˆæ¨¡å‹åœ¨åˆæˆåŠ¨æ€å››ç»´é©¾é©¶åœºæ™¯æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œè¿™äº›åœºæ™¯éœ€è¦åŒæ—¶æ”¯æŒæ—¶é—´å¤–æ¨å’Œæ— éœ€é’ˆå¯¹æ¯ä¸ªåœºæ™¯ä¼˜åŒ–çš„ç©ºé—´æ–°è§†è§’åˆæˆï¼ˆNVSï¼‰ã€‚ç”Ÿæˆå’Œæ–°é¢–è§†è§’åˆæˆä¹‹é—´çš„æ¡¥æ¢ä»ç„¶æ˜¯ä¸€ä¸ªä¸»è¦æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†PhiGenesisï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºå››ç»´åœºæ™¯ç”Ÿæˆçš„ç»Ÿä¸€æ¡†æ¶ï¼Œå®ƒæ‰©å±•äº†è§†é¢‘ç”ŸæˆæŠ€æœ¯ï¼Œå…·æœ‰å‡ ä½•å’Œæ—¶é—´ä¸€è‡´æ€§ã€‚ç»™å®šå¤šè§†è§’å›¾åƒåºåˆ—å’Œç›¸æœºå‚æ•°ï¼ŒPhiGenesiså¯ä»¥ç”Ÿæˆæ²¿ç›®æ ‡ä¸‰ç»´è½¨è¿¹çš„æ—¶é—´è¿ç»­å››ç»´é«˜æ–¯å–·å°„è¡¨ç¤ºã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼ŒPhiGenesisåˆ©ç”¨é¢„è®­ç»ƒçš„è§†é¢‘VAEå’Œæ–°å‹èŒƒå›´è§†å›¾é€‚é…å™¨ï¼Œå®ç°ä»å¤šè§†è§’å›¾åƒçš„å‰é¦ˆå››ç»´é‡å»ºã€‚æ­¤æ¶æ„æ”¯æŒå•å¸§æˆ–è§†é¢‘è¾“å…¥ï¼Œå¹¶è¾“å‡ºå®Œæ•´çš„å››ç»´åœºæ™¯ï¼ŒåŒ…æ‹¬å‡ ä½•ã€è¯­ä¹‰å’Œè¿åŠ¨ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼ŒPhiGenesiså¼•å…¥äº†ä¸€ä¸ªå—å‡ ä½•æŒ‡å¯¼çš„è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œä½¿ç”¨å‘ˆç°çš„å†å²å››ç»´åœºæ™¯ä½œä¸ºå…ˆéªŒæ¥æ ¹æ®è½¨è¿¹ç”Ÿæˆæœªæ¥è§†è§’ã€‚ä¸ºäº†è§£å†³æ–°è§†è§’çš„å‡ ä½•æ›å…‰åå·®é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ç«‹ä½“å¼ºåˆ¶ï¼ˆStereo Forcingï¼‰è¿™ä¸€æ–°å‹æ¡ä»¶ç­–ç•¥ï¼Œå®ƒåœ¨å»å™ªè¿‡ç¨‹ä¸­æ•´åˆäº†å‡ ä½•ä¸ç¡®å®šæ€§ã€‚æ­¤æ–¹æ³•é€šè¿‡åŸºäºä¸ç¡®å®šæ€§æ„ŸçŸ¥æ‰°åŠ¨çš„åŠ¨æ€è°ƒæ•´ç”Ÿæˆå½±å“æ¥å¢å¼ºæ—¶é—´è¿è´¯æ€§ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤–è§‚å’Œå‡ ä½•é‡å»ºã€æ—¶é—´ç”Ÿæˆå’Œæ–°è§†è§’åˆæˆï¼ˆNVSï¼‰ä»»åŠ¡ä¸Šå‡è¾¾åˆ°äº†æœ€æ–°æ€§èƒ½æ°´å¹³ï¼ŒåŒæ—¶åœ¨ä¸‹æ¸¸è¯„ä¼°ä¸­å–å¾—äº†æœ‰ç«äº‰åŠ›çš„è¡¨ç°ã€‚ä¸»é¡µä½äºï¼š<a target="_blank" rel="noopener" href="https://jiangxb98.github.io/PhiGensis">PhiGensis</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.20251v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå½“å‰ç”Ÿæˆæ¨¡å‹é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¦‚åŠ¨æ€å››ç»´é©¾é©¶åœºæ™¯çš„åˆæˆå’ŒåŒæ—¶æ”¯æŒæ—¶é—´å¤–æ¨å’Œç©ºé—´æ–°è§†è§’åˆæˆç­‰ï¼Œæœ¬æ–‡æå‡ºäº†PhiGenesisç»Ÿä¸€æ¡†æ¶ã€‚è¯¥æ¡†æ¶æ‰©å±•äº†è§†é¢‘ç”ŸæˆæŠ€æœ¯ï¼Œå…·æœ‰å‡ ä½•å’Œæ—¶é—´ä¸€è‡´æ€§ã€‚å®ƒé‡‡ç”¨å¤šè§†è§’å›¾åƒåºåˆ—å’Œæ‘„åƒæœºå‚æ•°ï¼Œç”Ÿæˆæ²¿ç›®æ ‡ä¸‰ç»´è½¨è¿¹çš„å››ç»´é«˜æ–¯è´´å›¾è¡¨ç¤ºã€‚è¯¥æ¡†æ¶åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼Œç¬¬ä¸€é˜¶æ®µåˆ©ç”¨é¢„è®­ç»ƒçš„è§†é¢‘VAEå’Œæ–°é¢–çš„èŒƒå›´è§†å›¾é€‚é…å™¨è¿›è¡Œå››ç»´é‡å»ºï¼Œç¬¬äºŒé˜¶æ®µå¼•å…¥å‡ ä½•å¯¼å‘çš„è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œåˆ©ç”¨æ¸²æŸ“çš„å†å²å››ç»´åœºæ™¯ä½œä¸ºå…ˆéªŒæ¥ç”Ÿæˆæœªæ¥è§†è§’çš„æ¡ä»¶è½¨è¿¹ã€‚ä¸ºè§£å†³æ–°è§†è§’çš„å‡ ä½•æ›å…‰åå·®é—®é¢˜ï¼Œæå‡ºäº†ç«‹ä½“å¼ºåˆ¶è¿™ä¸€æ–°å‹æ¡ä»¶ç­–ç•¥ï¼Œåœ¨é™å™ªè¿‡ç¨‹ä¸­æ•´åˆå‡ ä½•ä¸ç¡®å®šæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤–è§‚å’Œå‡ ä½•é‡å»ºã€æ—¶é—´ç”Ÿæˆå’Œæ–°è§†è§’åˆæˆä»»åŠ¡ä¸Šå‡è¾¾åˆ°æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰ç”Ÿæˆæ¨¡å‹åœ¨åˆæˆåŠ¨æ€å››ç»´é©¾é©¶åœºæ™¯æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œéœ€è¦åŒæ—¶æ”¯æŒæ—¶é—´å¤–æ¨å’Œç©ºé—´æ–°è§†è§’åˆæˆã€‚</li>
<li>PhiGenesisæ˜¯ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è¿™äº›é—®é¢˜ï¼Œé€šè¿‡æ‰©å±•è§†é¢‘ç”ŸæˆæŠ€æœ¯å®ç°å‡ ä½•å’Œæ—¶é—´ä¸€è‡´æ€§ã€‚</li>
<li>PhiGenesisåˆ©ç”¨å¤šè§†è§’å›¾åƒåºåˆ—å’Œæ‘„åƒæœºå‚æ•°ç”Ÿæˆå››ç»´é«˜æ–¯è´´å›¾è¡¨ç¤ºã€‚</li>
<li>è¯¥æ¡†æ¶åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šå››ç»´é‡å»ºé˜¶æ®µå’ŒåŸºäºå‡ ä½•å¯¼å‘çš„è§†é¢‘æ‰©æ•£æ¨¡å‹ç”Ÿæˆé˜¶æ®µã€‚</li>
<li>ç¬¬ä¸€é˜¶æ®µåˆ©ç”¨è§†é¢‘VAEå’ŒèŒƒå›´è§†å›¾é€‚é…å™¨è¿›è¡Œé‡å»ºã€‚</li>
<li>ç¬¬äºŒé˜¶æ®µå¼•å…¥ç«‹ä½“å¼ºåˆ¶ç­–ç•¥æ¥è§£å†³æ–°è§†è§’çš„å‡ ä½•æ›å…‰åå·®é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.20251">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-82664f4f0d8c3114c482d82cdcf2aed4" align="middle">
<img src="https://picx.zhimg.com/v2-91265e34f81bf6c2676c3c343be743d1" align="middle">
<img src="https://picx.zhimg.com/v2-b326c26539bbae1c06efb1b231f6c241" align="middle">
<img src="https://picx.zhimg.com/v2-885b7eeefee57f6ec5147f64bb016e9c" align="middle">
<img src="https://picx.zhimg.com/v2-90f77a423f1976bd31cc905a6f4ff750" align="middle">
<img src="https://picx.zhimg.com/v2-d2865c189a251d46fd0c4be9ce253c50" align="middle">
<img src="https://picx.zhimg.com/v2-b554aa4c2f4d4fb6ff6a5634a36bbe19" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Unleashing-the-Potential-of-the-Semantic-Latent-Space-in-Diffusion-Models-for-Image-Dehazing"><a href="#Unleashing-the-Potential-of-the-Semantic-Latent-Space-in-Diffusion-Models-for-Image-Dehazing" class="headerlink" title="Unleashing the Potential of the Semantic Latent Space in Diffusion   Models for Image Dehazing"></a>Unleashing the Potential of the Semantic Latent Space in Diffusion   Models for Image Dehazing</h2><p><strong>Authors:Zizheng Yang, Hu Yu, Bing Li, Jinghao Zhang, Jie Huang, Feng Zhao</strong></p>
<p>Diffusion models have recently been investigated as powerful generative solvers for image dehazing, owing to their remarkable capability to model the data distribution. However, the massive computational burden imposed by the retraining of diffusion models, coupled with the extensive sampling steps during the inference, limit the broader application of diffusion models in image dehazing. To address these issues, we explore the properties of hazy images in the semantic latent space of frozen pre-trained diffusion models, and propose a Diffusion Latent Inspired network for Image Dehazing, dubbed DiffLI$^2$D. Specifically, we first reveal that the semantic latent space of pre-trained diffusion models can represent the content and haze characteristics of hazy images, as the diffusion time-step changes. Building upon this insight, we integrate the diffusion latent representations at different time-steps into a delicately designed dehazing network to provide instructions for image dehazing. Our DiffLI$^2$D avoids re-training diffusion models and iterative sampling process by effectively utilizing the informative representations derived from the pre-trained diffusion models, which also offers a novel perspective for introducing diffusion models to image dehazing. Extensive experiments on multiple datasets demonstrate that the proposed method achieves superior performance to existing image dehazing methods. Code is available at <a target="_blank" rel="noopener" href="https://github.com/aaaasan111/difflid">https://github.com/aaaasan111/difflid</a>. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹å› å…¶å¯¹æ•°æ®åˆ†å¸ƒçš„å‡ºè‰²å»ºæ¨¡èƒ½åŠ›ï¼Œæœ€è¿‘è¢«ç ”ç©¶ä¸ºå›¾åƒå»é›¾çš„å¼ºå¤§ç”Ÿæˆæ±‚è§£å™¨ã€‚ç„¶è€Œï¼Œæ‰©æ•£æ¨¡å‹çš„é‡è®­ç»ƒæ‰€å¸¦æ¥çš„å·¨å¤§è®¡ç®—è´Ÿæ‹…ï¼Œä»¥åŠæ¨ç†è¿‡ç¨‹ä¸­çš„å¤§é‡é‡‡æ ·æ­¥éª¤ï¼Œé™åˆ¶äº†æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒå»é›¾ä¸­çš„æ›´å¹¿æ³›åº”ç”¨ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æ¢ç´¢äº†å†»ç»“çš„é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„è¯­ä¹‰æ½œåœ¨ç©ºé—´ä¸­çš„é›¾å›¾åƒçš„å±æ€§ï¼Œå¹¶æå‡ºäº†ä¸€ç§ç”¨äºå›¾åƒå»é›¾çš„æ‰©æ•£æ½œåœ¨å¯å‘ç½‘ç»œï¼Œç§°ä¸ºDiffLI$^2$Dã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆå‘ç°ï¼Œéšç€æ‰©æ•£æ—¶é—´æ­¥é•¿çš„å˜åŒ–ï¼Œé¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹çš„è¯­ä¹‰æ½œåœ¨ç©ºé—´å¯ä»¥è¡¨ç¤ºé›¾å›¾åƒçš„å†…å®¹å’Œé›¾ç‰¹æ€§ã€‚åŸºäºè¿™ä¸€è§è§£ï¼Œæˆ‘ä»¬å°†ä¸åŒæ—¶é—´æ­¥é•¿çš„æ‰©æ•£æ½œåœ¨è¡¨ç¤ºé›†æˆåˆ°ä¸€ä¸ªç²¾å¿ƒè®¾è®¡çš„å»é›¾ç½‘ç»œä¸­ï¼Œä¸ºå›¾åƒå»é›¾æä¾›æŒ‡å¯¼ã€‚æˆ‘ä»¬çš„DiffLI$^2$Dé€šè¿‡æœ‰æ•ˆåˆ©ç”¨é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹äº§ç”Ÿçš„ä¿¡æ¯è¡¨ç¤ºï¼Œé¿å…äº†é‡æ–°è®­ç»ƒæ‰©æ•£æ¨¡å‹å’Œè¿­ä»£é‡‡æ ·è¿‡ç¨‹ï¼Œè¿™ä¸ºå°†æ‰©æ•£æ¨¡å‹å¼•å…¥åˆ°å›¾åƒå»é›¾ä¸­æä¾›äº†æ–°çš„è§†è§’ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨ç°æœ‰å›¾åƒå»é›¾æ–¹æ³•ä¸­å®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/aaaasan111/difflid%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/aaaasan111/difflidè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.20091v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ‰©æ•£æ¨¡å‹å› å…¶å¯¹æ•°æ®åˆ†å¸ƒçš„å‡ºè‰²å»ºæ¨¡èƒ½åŠ›è€Œè¢«ç ”ç©¶ä¸ºå›¾åƒå»é›¾çš„ç”Ÿæˆæ±‚è§£å™¨ã€‚ç„¶è€Œï¼Œæ‰©æ•£æ¨¡å‹çš„å·¨å¤§è®¡ç®—è´Ÿæ‹…ä»¥åŠæ¨ç†è¿‡ç¨‹ä¸­çš„å¤§é‡é‡‡æ ·æ­¥éª¤é™åˆ¶äº†å…¶åœ¨å›¾åƒå»é›¾ä¸­çš„æ›´å¹¿æ³›åº”ç”¨ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æ¢ç´¢äº†å†»ç»“çš„é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„è¯­ä¹‰æ½œåœ¨ç©ºé—´ä¸­çš„é›¾å›¾åƒçš„å±æ€§ï¼Œå¹¶æå‡ºäº†ç”¨äºå›¾åƒå»é›¾çš„æ‰©æ•£æ½œåœ¨å¯å‘ç½‘ç»œï¼Œç§°ä¸ºDiffLI$^2$Dã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆæ­ç¤ºéšç€æ‰©æ•£æ—¶é—´æ­¥çš„å˜åŒ–ï¼Œé¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹çš„è¯­ä¹‰æ½œåœ¨ç©ºé—´å¯ä»¥è¡¨ç¤ºé›¾å›¾åƒçš„å†…å®¹å’Œé›¾ç‰¹å¾ã€‚åŸºäºæ­¤æ´å¯Ÿï¼Œæˆ‘ä»¬å°†ä¸åŒæ—¶é—´æ­¥çš„æ‰©æ•£æ½œåœ¨è¡¨ç¤ºé›†æˆåˆ°ä¸€ä¸ªç²¾å¿ƒè®¾è®¡çš„å»é›¾ç½‘ç»œä¸­ï¼Œä»¥ä¸ºå›¾åƒå»é›¾æä¾›æŒ‡å¯¼ã€‚DiffLI$^2$Dé¿å…äº†é‡æ–°è®­ç»ƒæ‰©æ•£æ¨¡å‹å’Œè¿­ä»£é‡‡æ ·è¿‡ç¨‹ï¼Œé€šè¿‡æœ‰æ•ˆåˆ©ç”¨ä»é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ä¸­å¾—å‡ºçš„ä¿¡æ¯è¡¨ç¤ºï¼Œå®ƒè¿˜ä¸ºå°†æ‰©æ•£æ¨¡å‹å¼•å…¥å›¾åƒå»é›¾æä¾›äº†æ–°é¢–çš„è§†è§’ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç°æœ‰å›¾åƒå»é›¾æ–¹æ³•ä¸­å®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/aaaasan111/difflid">é“¾æ¥</a>ä¸­æ‰¾åˆ°ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹å› å…¶å‡ºè‰²çš„æ•°æ®åˆ†å¸ƒå»ºæ¨¡èƒ½åŠ›è€Œåœ¨å›¾åƒå»é›¾ä¸­å—åˆ°å…³æ³¨ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹çš„å·¨å¤§è®¡ç®—è´Ÿæ‹…å’Œæ¨ç†è¿‡ç¨‹ä¸­çš„é‡‡æ ·æ­¥éª¤é™åˆ¶äº†å…¶åº”ç”¨ã€‚</li>
<li>å†»ç»“çš„é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„è¯­ä¹‰æ½œåœ¨ç©ºé—´å¯ä»¥è¡¨ç¤ºé›¾å›¾åƒçš„å†…å®¹å’Œé›¾ç‰¹å¾ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ½œåœ¨å¯å‘çš„å›¾åƒå»é›¾ç½‘ç»œï¼ˆDiffLI$^2$Dï¼‰ã€‚</li>
<li>DiffLI$^2$Dåˆ©ç”¨é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ä¸­çš„ä¿¡æ¯è¡¨ç¤ºï¼Œé¿å…äº†æ¨¡å‹é‡æ–°è®­ç»ƒå’Œè¿­ä»£é‡‡æ ·è¿‡ç¨‹ã€‚</li>
<li>DiffLI$^2$Dä¸ºå°†æ‰©æ•£æ¨¡å‹åº”ç”¨äºå›¾åƒå»é›¾æä¾›äº†æ–°é¢–çš„è§†è§’ã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨ç°æœ‰å›¾åƒå»é›¾æ–¹æ³•ä¸­è¡¨ç°å“è¶Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.20091">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fa397a99f3ac8cf83da73f82f15988f4" align="middle">
<img src="https://picx.zhimg.com/v2-4c4767cdec6506f24b90cd350fa34243" align="middle">
<img src="https://picx.zhimg.com/v2-12f964da2a14528841ae957813caa163" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Learnable-Sampler-Distillation-for-Discrete-Diffusion-Models"><a href="#Learnable-Sampler-Distillation-for-Discrete-Diffusion-Models" class="headerlink" title="Learnable Sampler Distillation for Discrete Diffusion Models"></a>Learnable Sampler Distillation for Discrete Diffusion Models</h2><p><strong>Authors:Feiyang Fu, Tongxian Guo, Zhaoqiang Liu</strong></p>
<p>Discrete diffusion models (DDMs) have shown powerful generation ability for discrete data modalities like text and molecules. However, their practical application is hindered by inefficient sampling, requiring a large number of sampling steps. Accelerating DDMs by using larger step sizes typically introduces significant problems in generation quality, as it amplifies the impact of both the compounding decoding error due to factorized predictions and discretization error from numerical approximations, leading to a significant decrease in sampling quality. To address these challenges, we propose learnable sampler distillation (LSD), a novel approach to train fast and high-fidelity samplers for DDMs. LSD employs a distillation approach where a student sampler with a few steps learns to align its intermediate score trajectory with that of a high-quality teacher sampler with numerous steps. This alignment is achieved by optimizing learnable sampler coefficients that adaptively adjust sampling dynamics. Additionally, we further propose LSD+, which also learns time schedules that allocate steps non-uniformly. Experiments across text generation, image generation, and synthetic tasks demonstrate that our proposed approaches outperform existing samplers for DDMs, achieving substantially higher sampling quality with significantly fewer sampling steps. Our code is available at \href{<a target="_blank" rel="noopener" href="https://github.com/feiyangfu/LSD%7D%7Bhttps://github.com/feiyangfu/LSD%7D">https://github.com/feiyangfu/LSD}{https://github.com/feiyangfu/LSD}</a>. </p>
<blockquote>
<p>ç¦»æ•£æ‰©æ•£æ¨¡å‹ï¼ˆDDMsï¼‰å·²ç»æ˜¾ç¤ºå‡ºå¯¹ç¦»æ•£æ•°æ®æ¨¡å¼ï¼ˆå¦‚æ–‡æœ¬å’Œåˆ†å­ï¼‰çš„å¼ºå¤§ç”Ÿæˆèƒ½åŠ›ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨å®é™…åº”ç”¨ä¸­çš„é‡‡æ ·æ•ˆç‡ä½ä¸‹ï¼Œéœ€è¦å¤§é‡çš„é‡‡æ ·æ­¥éª¤ã€‚é€šè¿‡å¢å¤§æ­¥é•¿æ¥åŠ é€ŸDDMsé€šå¸¸ä¼šå¼•å…¥ä¸¥é‡çš„ç”Ÿæˆè´¨é‡é—®é¢˜ï¼Œå› ä¸ºå®ƒæ”¾å¤§äº†ç”±äºåˆ†è§£é¢„æµ‹å’Œæ•°å€¼é€¼è¿‘å¼•èµ·çš„ç¦»æ•£åŒ–è¯¯å·®ï¼Œå¯¼è‡´é‡‡æ ·è´¨é‡æ˜¾è‘—é™ä½ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†å¯å­¦ä¹ é‡‡æ ·å™¨è’¸é¦ï¼ˆLSDï¼‰è¿™ä¸€æ–°æ–¹æ³•ï¼Œç”¨äºè®­ç»ƒDDMsçš„å¿«é€Ÿé«˜ä¿çœŸé‡‡æ ·å™¨ã€‚LSDé‡‡ç”¨äº†ä¸€ç§è’¸é¦æ–¹æ³•ï¼Œå…¶ä¸­å…·æœ‰å‡ æ­¥çš„å­¦ç”Ÿé‡‡æ ·å™¨å­¦ä¹ ä½¿å…¶ä¸­é—´åˆ†æ•°è½¨è¿¹ä¸å…·æœ‰å¤šæ­¥çš„é«˜è´¨é‡æ•™å¸ˆé‡‡æ ·å™¨çš„è½¨è¿¹å¯¹é½ã€‚è¿™ç§å¯¹é½æ˜¯é€šè¿‡ä¼˜åŒ–å¯å­¦ä¹ é‡‡æ ·å™¨ç³»æ•°æ¥å®ç°çš„ï¼Œè¿™äº›ç³»æ•°å¯ä»¥è‡ªé€‚åº”åœ°è°ƒæ•´é‡‡æ ·åŠ¨æ€ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¿›ä¸€æ­¥æå‡ºäº†LSD+ï¼Œå®ƒè¿˜å­¦ä¹ éå‡åŒ€åˆ†é…æ­¥éª¤çš„æ—¶é—´è¡¨ã€‚åœ¨æ–‡æœ¬ç”Ÿæˆã€å›¾åƒç”Ÿæˆå’Œåˆæˆä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•åœ¨DDMsçš„é‡‡æ ·å™¨ä¸Šä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œåœ¨å¤§å¤§å‡å°‘é‡‡æ ·æ­¥éª¤çš„æƒ…å†µä¸‹å®ç°äº†æ›´é«˜çš„é‡‡æ ·è´¨é‡ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/feiyangfu/LSD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/feiyangfu/LSDæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.19962v1">PDF</a> NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>ç¦»æ•£æ‰©æ•£æ¨¡å‹ï¼ˆDDMsï¼‰åœ¨æ–‡æœ¬å’Œåˆ†å­ç­‰ç¦»æ•£æ•°æ®æ¨¡æ€çš„ç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†å®é™…åº”ç”¨ä¸­å­˜åœ¨é‡‡æ ·æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ï¼Œéœ€è¦å¤§é‡çš„é‡‡æ ·æ­¥éª¤ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†å¯å­¦ä¹ é‡‡æ ·å™¨è’¸é¦ï¼ˆLSDï¼‰æ–¹æ³•ï¼Œç”¨äºè®­ç»ƒDDMsçš„å¿«é€Ÿé«˜ä¿çœŸé‡‡æ ·å™¨ã€‚LSDé‡‡ç”¨è’¸é¦æ–¹æ³•ï¼Œé€šè¿‡ä¼˜åŒ–å¯å­¦ä¹ é‡‡æ ·å™¨ç³»æ•°ï¼Œä½¿å…·æœ‰è¾ƒå°‘æ­¥éª¤çš„å­¦ç”Ÿé‡‡æ ·å™¨å­¦ä¹ è·Ÿéšé«˜è´¨é‡æ•™å¸ˆé‡‡æ ·å™¨çš„ä¸­é—´åˆ†æ•°è½¨è¿¹ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†LSD+ï¼Œè¯¥æ–¹æ³•è¿˜å­¦ä¹ éå‡åŒ€åˆ†é…æ­¥éª¤çš„æ—¶é—´è¡¨ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨æ–‡æœ¬ç”Ÿæˆã€å›¾åƒç”Ÿæˆå’Œåˆæˆä»»åŠ¡ä¸­ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨é‡‡æ ·æ­¥éª¤å¤§å¤§å‡å°‘çš„æƒ…å†µä¸‹ï¼Œå®ç°äº†æ›´é«˜çš„é‡‡æ ·è´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¦»æ•£æ‰©æ•£æ¨¡å‹ï¼ˆDDMsï¼‰å…·æœ‰å¼ºå¤§çš„ç”Ÿæˆèƒ½åŠ›ï¼Œä½†åœ¨é‡‡æ ·æ•ˆç‡æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰åŠ é€ŸDDMsçš„æ–¹æ³•å¸¸å¸¸åœ¨æ”¾å¤§å› å­åŒ–é¢„æµ‹å’Œæ•°å€¼é€¼è¿‘å¸¦æ¥çš„è¯¯å·®æ—¶å¼•å…¥æ–°çš„é—®é¢˜ï¼Œå¯¼è‡´é‡‡æ ·è´¨é‡ä¸‹é™ã€‚</li>
<li>æœ¬æ–‡æå‡ºçš„å¯å­¦ä¹ é‡‡æ ·å™¨è’¸é¦ï¼ˆLSDï¼‰æ–¹æ³•é€šè¿‡ä¼˜åŒ–å¯å­¦ä¹ é‡‡æ ·å™¨ç³»æ•°ï¼Œå®ç°é«˜æ•ˆä¸”é«˜è´¨é‡çš„é‡‡æ ·å™¨è®­ç»ƒã€‚</li>
<li>LSDæ–¹æ³•ä½¿å­¦ç”Ÿé‡‡æ ·å™¨é€šè¿‡è¾ƒå°‘çš„æ­¥éª¤å°±èƒ½å­¦ä¹ è·Ÿéšé«˜è´¨é‡æ•™å¸ˆé‡‡æ ·å™¨çš„ä¸­é—´åˆ†æ•°è½¨è¿¹ï¼Œä»è€Œæé«˜é‡‡æ ·è´¨é‡ã€‚</li>
<li>æ­¤å¤–ï¼ŒLSD+è¿˜å­¦ä¹ éå‡åŒ€åˆ†é…æ­¥éª¤çš„æ—¶é—´è¡¨ï¼Œä»¥è¿›ä¸€æ­¥ä¼˜åŒ–é‡‡æ ·è¿‡ç¨‹ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨æ–‡æœ¬ç”Ÿæˆã€å›¾åƒç”Ÿæˆå’Œåˆæˆä»»åŠ¡ä¸­å®ç°äº†æ›´é«˜çš„é‡‡æ ·è´¨é‡ï¼Œå¹¶æ˜¾è‘—å‡å°‘äº†é‡‡æ ·æ­¥éª¤ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.19962">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4b8140a49ffb02811e7d1546d4f337e1" align="middle">
<img src="https://picx.zhimg.com/v2-860311a23823cb777ab0011034de5faa" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Lyra-Generative-3D-Scene-Reconstruction-via-Video-Diffusion-Model-Self-Distillation"><a href="#Lyra-Generative-3D-Scene-Reconstruction-via-Video-Diffusion-Model-Self-Distillation" class="headerlink" title="Lyra: Generative 3D Scene Reconstruction via Video Diffusion Model   Self-Distillation"></a>Lyra: Generative 3D Scene Reconstruction via Video Diffusion Model   Self-Distillation</h2><p><strong>Authors:Sherwin Bahmani, Tianchang Shen, Jiawei Ren, Jiahui Huang, Yifeng Jiang, Haithem Turki, Andrea Tagliasacchi, David B. Lindell, Zan Gojcic, Sanja Fidler, Huan Ling, Jun Gao, Xuanchi Ren</strong></p>
<p>The ability to generate virtual environments is crucial for applications ranging from gaming to physical AI domains such as robotics, autonomous driving, and industrial AI. Current learning-based 3D reconstruction methods rely on the availability of captured real-world multi-view data, which is not always readily available. Recent advancements in video diffusion models have shown remarkable imagination capabilities, yet their 2D nature limits the applications to simulation where a robot needs to navigate and interact with the environment. In this paper, we propose a self-distillation framework that aims to distill the implicit 3D knowledge in the video diffusion models into an explicit 3D Gaussian Splatting (3DGS) representation, eliminating the need for multi-view training data. Specifically, we augment the typical RGB decoder with a 3DGS decoder, which is supervised by the output of the RGB decoder. In this approach, the 3DGS decoder can be purely trained with synthetic data generated by video diffusion models. At inference time, our model can synthesize 3D scenes from either a text prompt or a single image for real-time rendering. Our framework further extends to dynamic 3D scene generation from a monocular input video. Experimental results show that our framework achieves state-of-the-art performance in static and dynamic 3D scene generation. </p>
<blockquote>
<p>ç”Ÿæˆè™šæ‹Ÿç¯å¢ƒçš„èƒ½åŠ›å¯¹äºä»æ¸¸æˆåˆ°ç‰©ç†äººå·¥æ™ºèƒ½é¢†åŸŸï¼ˆå¦‚æœºå™¨äººã€è‡ªåŠ¨é©¾é©¶å’Œå·¥ä¸šäººå·¥æ™ºèƒ½ï¼‰çš„åº”ç”¨è‡³å…³é‡è¦ã€‚å½“å‰åŸºäºå­¦ä¹ çš„3Dé‡å»ºæ–¹æ³•ä¾èµ–äºæ•è·çš„ç°å®ä¸–ç•Œå¤šè§†è§’æ•°æ®çš„å¯ç”¨æ€§ï¼Œè€Œè¿™å¹¶éæ€»æ˜¯å®¹æ˜“è·å¾—çš„ã€‚è§†é¢‘æ‰©æ•£æ¨¡å‹çš„æœ€æ–°è¿›å±•è¡¨ç°å‡ºäº†æƒŠäººçš„æƒ³è±¡åŠ›èƒ½åŠ›ï¼Œç„¶è€Œå®ƒä»¬çš„2Dæ€§è´¨é™åˆ¶äº†å…¶åœ¨ä»¿çœŸä¸­çš„åº”ç”¨ï¼Œåœ¨ä»¿çœŸä¸­æœºå™¨äººéœ€è¦å¯¼èˆªå¹¶ä¸ç¯å¢ƒè¿›è¡Œäº¤äº’ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è‡ªè’¸é¦æ¡†æ¶ï¼Œæ—¨åœ¨å°†è§†é¢‘æ‰©æ•£æ¨¡å‹ä¸­çš„éšå¼3DçŸ¥è¯†è’¸é¦åˆ°æ˜ç¡®çš„3Dé«˜æ–¯æ‹¼è´´ï¼ˆ3DGSï¼‰è¡¨ç¤ºä¸­ï¼Œä»è€Œæ¶ˆé™¤äº†å¯¹å¤šè§†è§’è®­ç»ƒæ•°æ®çš„éœ€æ±‚ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ç”¨ä¸€ä¸ª3DGSè§£ç å™¨å¢å¼ºäº†å…¸å‹çš„RGBè§£ç å™¨ï¼Œè¯¥è§£ç å™¨ç”±RGBè§£ç å™¨çš„è¾“å‡ºè¿›è¡Œç›‘ç£ã€‚åœ¨è¿™ç§æ–¹æ³•ä¸­ï¼Œ3DGSè§£ç å™¨å¯ä»¥ä»…é€šè¿‡ç”±è§†é¢‘æ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„åˆæˆæ•°æ®è¿›è¡Œè®­ç»ƒã€‚åœ¨æ¨ç†æ—¶ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å¯ä»¥ä»æ–‡æœ¬æç¤ºæˆ–å•å¹…å›¾åƒä¸­åˆæˆ3Dåœºæ™¯ï¼Œç”¨äºå®æ—¶æ¸²æŸ“ã€‚æˆ‘ä»¬çš„æ¡†æ¶è¿›ä¸€æ­¥æ‰©å±•åˆ°ä»å•ç›®è¾“å…¥è§†é¢‘ç”ŸæˆåŠ¨æ€3Dåœºæ™¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶åœ¨é™æ€å’ŒåŠ¨æ€3Dåœºæ™¯ç”Ÿæˆæ–¹é¢è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.19296v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://research.nvidia.com/labs/toronto-ai/lyra/">https://research.nvidia.com/labs/toronto-ai/lyra/</a></p>
<p><strong>Summary</strong><br>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè§†é¢‘æ‰©æ•£æ¨¡å‹çš„è‡ªè’¸é¦æ¡†æ¶ï¼Œæ—¨åœ¨å°†è§†é¢‘æ‰©æ•£æ¨¡å‹ä¸­çš„éšå¼3DçŸ¥è¯†è’¸é¦åˆ°æ˜¾å¼çš„ä¸‰ç»´é«˜æ–¯å–·æº…ï¼ˆ3DGSï¼‰è¡¨ç¤ºä¸­ï¼Œæ— éœ€ä½¿ç”¨å¤šè§†è§’è®­ç»ƒæ•°æ®ã€‚é€šè¿‡å¢åŠ ä¸€ä¸ª3DGSè§£ç å™¨ï¼Œå¯¹RGBè§£ç å™¨çš„è¾“å‡ºè¿›è¡Œç›‘ç£å­¦ä¹ ã€‚æ¨¡å‹å¯ä»¥æ ¹æ®æ–‡æœ¬æç¤ºæˆ–å•å¼ å›¾åƒåˆæˆä¸‰ç»´åœºæ™¯ï¼Œå¹¶å®æ—¶æ¸²æŸ“ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶è¿˜å¯ä»¥æ‰©å±•åˆ°åŸºäºå•ç›®è¾“å…¥è§†é¢‘çš„åŠ¨æ€ä¸‰ç»´åœºæ™¯ç”Ÿæˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨é™æ€å’ŒåŠ¨æ€ä¸‰ç»´åœºæ™¯ç”Ÿæˆæ–¹é¢å‡è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†åŸºäºè§†é¢‘æ‰©æ•£æ¨¡å‹çš„è‡ªè’¸é¦æ¡†æ¶ï¼Œç”¨äºç”Ÿæˆè™šæ‹Ÿç¯å¢ƒã€‚</li>
<li>é€šè¿‡å°†éšå¼3DçŸ¥è¯†è’¸é¦åˆ°æ˜¾å¼çš„ä¸‰ç»´é«˜æ–¯å–·æº…ï¼ˆ3DGSï¼‰è¡¨ç¤ºä¸­ï¼Œè§£å†³äº†ç¼ºä¹å¤šè§†è§’è®­ç»ƒæ•°æ®çš„é—®é¢˜ã€‚</li>
<li>å¼•å…¥äº†RGBè§£ç å™¨å’Œä¸€ä¸ªé¢å¤–çš„3DGSè§£ç å™¨è¿›è¡ŒååŒå·¥ä½œï¼Œä»¥å®ç°æ›´å¥½çš„ä¸‰ç»´åœºæ™¯åˆæˆã€‚</li>
<li>æ¨¡å‹èƒ½å¤Ÿæ ¹æ®æ–‡æœ¬æç¤ºæˆ–å•å¼ å›¾åƒåˆæˆä¸‰ç»´åœºæ™¯ï¼Œå¹¶å¯å®æ—¶æ¸²æŸ“ã€‚</li>
<li>æ¡†æ¶èƒ½å¤Ÿæ‰©å±•åˆ°åŸºäºå•ç›®è¾“å…¥è§†é¢‘çš„åŠ¨æ€ä¸‰ç»´åœºæ™¯ç”Ÿæˆã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜è¯¥æ¡†æ¶åœ¨é™æ€å’ŒåŠ¨æ€ä¸‰ç»´åœºæ™¯ç”Ÿæˆæ–¹é¢å‡è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.19296">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-74e03d2958e791f851ff3cf6981d9211" align="middle">
<img src="https://picx.zhimg.com/v2-917fdaee759f1fad519ef9536b017a70" align="middle">
<img src="https://picx.zhimg.com/v2-f1fc2b81389fc2d1c3422dae6fd8ea5a" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Lavida-O-Elastic-Large-Masked-Diffusion-Models-for-Unified-Multimodal-Understanding-and-Generation"><a href="#Lavida-O-Elastic-Large-Masked-Diffusion-Models-for-Unified-Multimodal-Understanding-and-Generation" class="headerlink" title="Lavida-O: Elastic Large Masked Diffusion Models for Unified Multimodal   Understanding and Generation"></a>Lavida-O: Elastic Large Masked Diffusion Models for Unified Multimodal   Understanding and Generation</h2><p><strong>Authors:Shufan Li, Jiuxiang Gu, Kangning Liu, Zhe Lin, Zijun Wei, Aditya Grover, Jason Kuen</strong></p>
<p>We propose Lavida-O, a unified Masked Diffusion Model (MDM) for multimodal understanding and generation. Unlike existing multimodal MDMs such as MMaDa and Muddit which only support simple image-level understanding tasks and low-resolution image generation, Lavida-O presents a single framework that enables image-level understanding, object grounding, image editing, and high-resolution (1024px) text-to-image synthesis. Lavida-O incorporates a novel Elastic Mixture-of-Transformers (Elastic-MoT) architecture that couples a lightweight generation branch with a larger understanding branch, supported by token compression, universal text conditioning and stratified sampling for efficient and high-quality generation. Lavida-O further incorporates planning and iterative self-reflection in image generation and editing tasks, seamlessly boosting generation quality with its understanding capabilities. Lavida-O achieves state-of-the-art performance on a wide range of benchmarks including RefCOCO object grounding, GenEval text-to-image generation, and ImgEdit image editing, outperforming existing autoregressive models and continuous diffusion models such as Qwen2.5-VL and FluxKontext-dev, while offering considerable speedup at inference. These advances establish Lavida-O as a new paradigm for scalable multimodal reasoning and generation. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†Lavida-Oï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„Masked Diffusion Modelï¼ˆMDMï¼‰ç”¨äºå¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆã€‚ä¸åŒäºç°æœ‰çš„å¤šæ¨¡æ€MDMï¼Œå¦‚MMaDaå’ŒMudditï¼Œå®ƒä»¬ä»…æ”¯æŒç®€å•çš„å›¾åƒçº§åˆ«ç†è§£ä»»åŠ¡å’Œä½åˆ†è¾¨ç‡å›¾åƒç”Ÿæˆï¼ŒLavida-Oå‘ˆç°äº†ä¸€ä¸ªå•ä¸€æ¡†æ¶ï¼Œèƒ½å¤Ÿå®ç°å›¾åƒçº§åˆ«ç†è§£ã€å¯¹è±¡å®šä½ã€å›¾åƒç¼–è¾‘å’Œé«˜è¾¾1024åƒç´ çš„é«˜åˆ†è¾¨ç‡æ–‡æœ¬åˆ°å›¾åƒåˆæˆã€‚Lavida-Oé‡‡ç”¨äº†æ–°å‹Elastic Mixture-of-Transformersï¼ˆElastic-MoTï¼‰æ¶æ„ï¼Œè¯¥æ¶æ„å°†è½»é‡çº§çš„ç”Ÿæˆåˆ†æ”¯ä¸æ›´å¤§çš„ç†è§£åˆ†æ”¯ç›¸ç»“åˆï¼Œå¹¶é€šè¿‡ä»¤ç‰Œå‹ç¼©ã€é€šç”¨æ–‡æœ¬æ¡ä»¶å’Œåˆ†å±‚é‡‡æ ·æ¥æ”¯æŒé«˜æ•ˆå’Œé«˜è´¨é‡çš„ç”Ÿæˆã€‚Lavida-Oè¿›ä¸€æ­¥åœ¨å›¾åƒç”Ÿæˆå’Œç¼–è¾‘ä»»åŠ¡ä¸­èå…¥äº†è§„åˆ’å’Œè¿­ä»£è‡ªæˆ‘åæ€ï¼Œåˆ©ç”¨å…¶ç†è§£èƒ½åŠ›æ— ç¼æå‡ç”Ÿæˆè´¨é‡ã€‚Lavida-Oåœ¨å¹¿æ³›çš„åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†å“è¶Šçš„æ€§èƒ½ï¼ŒåŒ…æ‹¬RefCOCOå¯¹è±¡å®šä½ã€GenEvalæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆå’ŒImgEditå›¾åƒç¼–è¾‘ï¼Œè¶…è¶Šäº†ç°æœ‰çš„è‡ªå›å½’æ¨¡å‹å’Œè¿ç»­æ‰©æ•£æ¨¡å‹ï¼Œå¦‚Qwen2.5-VLå’ŒFluxKontext-devï¼ŒåŒæ—¶åœ¨æ¨ç†è¿‡ç¨‹ä¸­æä¾›äº†æ˜¾è‘—çš„é€Ÿåº¦æå‡ã€‚è¿™äº›è¿›æ­¥ä½¿Lavida-Oæˆä¸ºå¯æ‰©å±•çš„å¤šæ¨¡æ€æ¨ç†å’Œç”Ÿæˆçš„æ–°èŒƒå¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.19244v2">PDF</a> 31 pages, 15 figures</p>
<p><strong>Summary</strong></p>
<p>Lavida-Oæ˜¯ä¸€ç§ç»Ÿä¸€çš„Masked Diffusion Modelï¼ˆMDMï¼‰ï¼Œç”¨äºå¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆã€‚ä¸ç°æœ‰çš„å¤šæ¨¡æ€MDMç›¸æ¯”ï¼ŒLavida-Oæ”¯æŒå›¾åƒçº§åˆ«çš„ç†è§£ã€å¯¹è±¡å®šä½ã€å›¾åƒç¼–è¾‘ä»¥åŠé«˜åˆ†è¾¨ç‡æ–‡æœ¬åˆ°å›¾åƒçš„åˆæˆã€‚å®ƒé‡‡ç”¨æ–°é¢–Elastic Mixture-of-Transformersæ¶æ„ï¼Œå®ç°äº†é«˜æ•ˆå’Œé«˜è´¨é‡çš„ç”Ÿæˆã€‚Lavida-Oåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°ä¸šç•Œé¢†å…ˆæ€§èƒ½ï¼Œå¦‚RefCOCOå¯¹è±¡å®šä½ã€GenEvalæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆå’ŒImgEditå›¾åƒç¼–è¾‘ä»»åŠ¡ï¼Œå±•ç°å‡ºå¼ºå¤§çš„æ¨ç†å’Œç”Ÿæˆèƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Lavida-Oæ˜¯ä¸€ä¸ªå¤šæ¨¡æ€çš„Masked Diffusion Modelï¼ˆMDMï¼‰ã€‚</li>
<li>ä¸å…¶ä»–MDMç›¸æ¯”ï¼ŒLavida-Oæ”¯æŒæ›´å¹¿æ³›çš„ä»»åŠ¡ï¼ŒåŒ…æ‹¬å›¾åƒçº§åˆ«çš„ç†è§£ã€å¯¹è±¡å®šä½ã€å›¾åƒç¼–è¾‘å’Œé«˜åˆ†è¾¨ç‡æ–‡æœ¬åˆ°å›¾åƒçš„åˆæˆã€‚</li>
<li>Lavida-Oé‡‡ç”¨Elastic Mixture-of-Transformersæ¶æ„ï¼Œå®ç°äº†é«˜æ•ˆå’Œé«˜è´¨é‡çš„ç”Ÿæˆã€‚</li>
<li>Lavida-Oåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå¦‚RefCOCOã€GenEvalå’ŒImgEditç­‰ä»»åŠ¡ã€‚</li>
<li>Lavida-Oè¶…è¶Šäº†ç°æœ‰çš„è‡ªå›å½’æ¨¡å‹å’Œè¿ç»­æ‰©æ•£æ¨¡å‹ï¼Œå¦‚Qwen2.5-VLå’ŒFluxKontext-devã€‚</li>
<li>Lavida-Oåœ¨æ¨ç†è¿‡ç¨‹ä¸­æä¾›äº†æ˜¾è‘—çš„é€Ÿåº¦æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.19244">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-656a98b6d105501d92425b9621562a74" align="middle">
<img src="https://picx.zhimg.com/v2-77f6390c43ebc406aeabf880064c3b2a" align="middle">
<img src="https://picx.zhimg.com/v2-c996981b30d6d0e41e27bacf8caff46f" align="middle">
<img src="https://picx.zhimg.com/v2-2714b6cdb016d9620bf715016e120932" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Efficient-Rectified-Flow-for-Image-Fusion"><a href="#Efficient-Rectified-Flow-for-Image-Fusion" class="headerlink" title="Efficient Rectified Flow for Image Fusion"></a>Efficient Rectified Flow for Image Fusion</h2><p><strong>Authors:Zirui Wang, Jiayi Zhang, Tianwei Guan, Yuhan Zhou, Xingyuan Li, Minjing Dong, Jinyuan Liu</strong></p>
<p>Image fusion is a fundamental and important task in computer vision, aiming to combine complementary information from different modalities to fuse images. In recent years, diffusion models have made significant developments in the field of image fusion. However, diffusion models often require complex computations and redundant inference time, which reduces the applicability of these methods. To address this issue, we propose RFfusion, an efficient one-step diffusion model for image fusion based on Rectified Flow. We incorporate Rectified Flow into the image fusion task to straighten the sampling path in the diffusion model, achieving one-step sampling without the need for additional training, while still maintaining high-quality fusion results. Furthermore, we propose a task-specific variational autoencoder (VAE) architecture tailored for image fusion, where the fusion operation is embedded within the latent space to further reduce computational complexity. To address the inherent discrepancy between conventional reconstruction-oriented VAE objectives and the requirements of image fusion, we introduce a two-stage training strategy. This approach facilitates the effective learning and integration of complementary information from multi-modal source images, thereby enabling the model to retain fine-grained structural details while significantly enhancing inference efficiency. Extensive experiments demonstrate that our method outperforms other state-of-the-art methods in terms of both inference speed and fusion quality. Code is available at <a target="_blank" rel="noopener" href="https://github.com/zirui0625/RFfusion">https://github.com/zirui0625/RFfusion</a>. </p>
<blockquote>
<p>å›¾åƒèåˆæ˜¯è®¡ç®—æœºè§†è§‰ä¸­çš„ä¸€ä¸ªåŸºæœ¬ä¸”é‡è¦çš„ä»»åŠ¡ï¼Œæ—¨åœ¨ç»“åˆä¸åŒæ¨¡æ€çš„äº’è¡¥ä¿¡æ¯æ¥èåˆå›¾åƒã€‚è¿‘å¹´æ¥ï¼Œæ‰©æ•£æ¨¡å‹åœ¨å›¾åƒèåˆé¢†åŸŸå–å¾—äº†æ˜¾è‘—çš„å‘å±•ã€‚ç„¶è€Œï¼Œæ‰©æ•£æ¨¡å‹é€šå¸¸éœ€è¦å¤æ‚çš„è®¡ç®—å’Œå†—ä½™çš„æ¨ç†æ—¶é—´ï¼Œè¿™é™ä½äº†è¿™äº›æ–¹æ³•çš„é€‚ç”¨æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†RFfusionï¼Œè¿™æ˜¯ä¸€ç§åŸºäºRectified Flowçš„é«˜æ•ˆä¸€æ­¥å›¾åƒèåˆæ‰©æ•£æ¨¡å‹ã€‚æˆ‘ä»¬å°†Rectified Flowèå…¥åˆ°å›¾åƒèåˆä»»åŠ¡ä¸­ï¼Œä»¥çŸ«æ­£æ‰©æ•£æ¨¡å‹ä¸­çš„é‡‡æ ·è·¯å¾„ï¼Œå®ç°ä¸€æ­¥é‡‡æ ·è€Œæ— éœ€é¢å¤–çš„è®­ç»ƒï¼ŒåŒæ—¶ä»èƒ½ä¿æŒé«˜è´¨é‡çš„èåˆç»“æœã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é’ˆå¯¹å›¾åƒèåˆçš„ç‰¹å®šä»»åŠ¡å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰æ¶æ„ï¼Œå…¶ä¸­èåˆæ“ä½œè¢«åµŒå…¥åˆ°æ½œåœ¨ç©ºé—´ä¸­ï¼Œä»¥è¿›ä¸€æ­¥é™ä½è®¡ç®—å¤æ‚æ€§ã€‚ä¸ºäº†è§£å†³ä¼ ç»Ÿä»¥é‡å»ºä¸ºå¯¼å‘çš„VAEç›®æ ‡ä¸å›¾åƒèåˆè¦æ±‚ä¹‹é—´çš„å›ºæœ‰å·®å¼‚ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ã€‚è¿™ç§æ–¹æ³•ä¿ƒè¿›äº†å¤šæ¨¡æ€æºå›¾åƒä¸­äº’è¡¥ä¿¡æ¯çš„æœ‰æ•ˆå­¦ä¹ å’Œæ•´åˆï¼Œä»è€Œä½¿æ¨¡å‹åœ¨ä¿ç•™ç²¾ç»†ç»“æ„ç»†èŠ‚çš„åŒæ—¶ï¼Œæ˜¾è‘—æé«˜æ¨ç†æ•ˆç‡ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ¨ç†é€Ÿåº¦å’Œèåˆè´¨é‡æ–¹é¢éƒ½ä¼˜äºå…¶ä»–æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/zirui0625/RFfusion%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/zirui0625/RFfusionæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.16549v2">PDF</a> Accepted by NeurIPS 2025</p>
<p><strong>Summary</strong><br>     èåˆæ¨¡å‹åŸºäºä¿®æ­£æµï¼ˆRectified Flowï¼‰æå‡ºä¸€ç§é«˜æ•ˆçš„ä¸€æ­¥æˆåƒèåˆæ–¹æ³•RFfusionã€‚æ­¤æ–¹æ³•ç®€åŒ–äº†æ‰©æ•£æ¨¡å‹çš„é‡‡æ ·è·¯å¾„ï¼Œæ— éœ€é¢å¤–è®­ç»ƒå³å¯å®ç°ä¸€æ­¥é‡‡æ ·ï¼ŒåŒæ—¶ä¿æŒé«˜è´¨é‡çš„èåˆç»“æœã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿè¿˜ä¸ºå›¾åƒèåˆä»»åŠ¡å®šåˆ¶äº†é¢å‘ä»»åŠ¡çš„å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰æ¶æ„ï¼Œå°†èåˆæ“ä½œåµŒå…¥æ½œåœ¨ç©ºé—´ä»¥é™ä½è®¡ç®—å¤æ‚åº¦ã€‚é’ˆå¯¹ä¼ ç»Ÿé‡å»ºå¯¼å‘çš„VAEç›®æ ‡ä¸å›¾åƒèåˆéœ€æ±‚ä¹‹é—´çš„å›ºæœ‰å·®å¼‚ï¼Œç ”ç©¶å›¢é˜Ÿå¼•å…¥äº†ä¸€ç§ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ã€‚æ­¤æ–¹æ³•æœ‰åŠ©äºæœ‰æ•ˆå­¦ä¹ å’Œæ•´åˆå¤šæ¨¡æ€æºå›¾åƒä¸­çš„äº’è¡¥ä¿¡æ¯ï¼Œä½¿æ¨¡å‹åœ¨ä¿ç•™ç²¾ç»†ç»“æ„ç»†èŠ‚çš„åŒæ—¶æ˜¾è‘—æé«˜æ¨ç†æ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒèåˆé¢†åŸŸå–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†è®¡ç®—å¤æ‚å’Œæ¨ç†æ—¶é—´é•¿é™åˆ¶äº†å…¶åº”ç”¨ã€‚</li>
<li>RFfusionæ–¹æ³•åŸºäºä¿®æ­£æµæå‡ºï¼Œå®ç°é«˜æ•ˆçš„ä¸€æ­¥å›¾åƒèåˆã€‚</li>
<li>ä¿®æ­£æµæŠ€æœ¯ç”¨äºç®€åŒ–æ‰©æ•£æ¨¡å‹çš„é‡‡æ ·è·¯å¾„ï¼Œæ— éœ€é¢å¤–è®­ç»ƒå³å¯å®ç°ä¸€æ­¥é‡‡æ ·ã€‚</li>
<li>å®šåˆ¶é¢å‘ä»»åŠ¡çš„å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰æ¶æ„ç”¨äºå›¾åƒèåˆï¼Œå°†èåˆæ“ä½œåµŒå…¥æ½œåœ¨ç©ºé—´é™ä½è®¡ç®—å¤æ‚åº¦ã€‚</li>
<li>é’ˆå¯¹ä¼ ç»Ÿé‡å»ºå¯¼å‘çš„VAEç›®æ ‡ä¸å›¾åƒèåˆéœ€æ±‚çš„å·®å¼‚ï¼Œå¼•å…¥ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ã€‚</li>
<li>æ­¤æ–¹æ³•èƒ½å¤Ÿåœ¨ä¿ç•™ç²¾ç»†ç»“æ„ç»†èŠ‚çš„åŒæ—¶æ˜¾è‘—æé«˜æ¨ç†æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.16549">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bde3e24b0de5105870a88d071664a985" align="middle">
<img src="https://picx.zhimg.com/v2-1623829a9cdc544286d7248df8d24935" align="middle">
<img src="https://picx.zhimg.com/v2-de58bbbdbd4671b2cd839fab7de9b91c" align="middle">
<img src="https://picx.zhimg.com/v2-e404cf6232cc63af25b1759910e30710" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="LazyDrag-Enabling-Stable-Drag-Based-Editing-on-Multi-Modal-Diffusion-Transformers-via-Explicit-Correspondence"><a href="#LazyDrag-Enabling-Stable-Drag-Based-Editing-on-Multi-Modal-Diffusion-Transformers-via-Explicit-Correspondence" class="headerlink" title="LazyDrag: Enabling Stable Drag-Based Editing on Multi-Modal Diffusion   Transformers via Explicit Correspondence"></a>LazyDrag: Enabling Stable Drag-Based Editing on Multi-Modal Diffusion   Transformers via Explicit Correspondence</h2><p><strong>Authors:Zixin Yin, Xili Dai, Duomin Wang, Xianfang Zeng, Lionel M. Ni, Gang Yu, Heung-Yeung Shum</strong></p>
<p>The reliance on implicit point matching via attention has become a core bottleneck in drag-based editing, resulting in a fundamental compromise on weakened inversion strength and costly test-time optimization (TTO). This compromise severely limits the generative capabilities of diffusion models, suppressing high-fidelity inpainting and text-guided creation. In this paper, we introduce LazyDrag, the first drag-based image editing method for Multi-Modal Diffusion Transformers, which directly eliminates the reliance on implicit point matching. In concrete terms, our method generates an explicit correspondence map from user drag inputs as a reliable reference to boost the attention control. This reliable reference opens the potential for a stable full-strength inversion process, which is the first in the drag-based editing task. It obviates the necessity for TTO and unlocks the generative capability of models. Therefore, LazyDrag naturally unifies precise geometric control with text guidance, enabling complex edits that were previously out of reach: opening the mouth of a dog and inpainting its interior, generating new objects like a &#96;&#96;tennis ballâ€™â€™, or for ambiguous drags, making context-aware changes like moving a hand into a pocket. Additionally, LazyDrag supports multi-round workflows with simultaneous move and scale operations. Evaluated on the DragBench, our method outperforms baselines in drag accuracy and perceptual quality, as validated by VIEScore and human evaluation. LazyDrag not only establishes new state-of-the-art performance, but also paves a new way to editing paradigms. </p>
<blockquote>
<p>å¯¹åŸºäºæ³¨æ„åŠ›çš„éšå¼ç‚¹åŒ¹é…çš„ä¾èµ–å·²æˆä¸ºåŸºäºæ‹–åŠ¨ç¼–è¾‘çš„æ ¸å¿ƒç“¶é¢ˆï¼Œå¯¼è‡´å¯¹å‡å¼±åæ¼”å¼ºåº¦å’Œæ˜‚è´µçš„æµ‹è¯•æ—¶é—´ä¼˜åŒ–ï¼ˆTTOï¼‰çš„åŸºæœ¬å¦¥åã€‚è¿™ç§å¦¥åä¸¥é‡é™åˆ¶äº†æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ï¼ŒæŠ‘åˆ¶äº†é«˜ä¿çœŸè¡¥å…¨å’Œæ–‡æœ¬å¼•å¯¼çš„åˆ›ä½œã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†LazyDragï¼Œè¿™æ˜¯é’ˆå¯¹å¤šæ¨¡æ€æ‰©æ•£è½¬æ¢å™¨çš„é¦–ä¸ªåŸºäºæ‹–åŠ¨çš„å›¾åƒç¼–è¾‘æ–¹æ³•ï¼Œå®ƒç›´æ¥æ¶ˆé™¤äº†å¯¹éšå¼ç‚¹åŒ¹é…çš„ä¾èµ–ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç”Ÿæˆä¸€ä¸ªæ˜¾å¼å¯¹åº”å›¾ï¼Œä»ç”¨æˆ·çš„æ‹–åŠ¨è¾“å…¥ä½œä¸ºå¯é å‚è€ƒæ¥æå‡æ³¨æ„åŠ›æ§åˆ¶ã€‚è¿™ä¸ªå¯é çš„å‚è€ƒä¸ºç¨³å®šçš„å®Œå…¨å¼ºåº¦åæ¼”è¿‡ç¨‹æ‰“å¼€äº†å¯èƒ½æ€§ï¼Œè¿™æ˜¯åŸºäºæ‹–åŠ¨ç¼–è¾‘ä»»åŠ¡ä¸­çš„ç¬¬ä¸€æ¬¡ã€‚å®ƒæ¶ˆé™¤äº†å¯¹TTOçš„éœ€è¦ï¼Œå¹¶é‡Šæ”¾äº†æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ã€‚å› æ­¤ï¼ŒLazyDragè‡ªç„¶åœ°ç»Ÿä¸€äº†ç²¾ç¡®çš„å‡ ä½•æ§åˆ¶ä¸æ–‡æœ¬æŒ‡å¯¼ï¼Œèƒ½å¤Ÿå®ç°ä»¥å‰æ— æ³•å®ç°çš„å¤æ‚ç¼–è¾‘ï¼šå¦‚æ‰“å¼€ç‹—çš„å˜´å·´å¹¶å¡«å……å…¶å†…éƒ¨ï¼Œç”Ÿæˆæ–°å¯¹è±¡ï¼ˆå¦‚â€œç½‘çƒâ€ï¼‰ï¼Œæˆ–å¯¹æ¨¡ç³Šçš„æ‹–åŠ¨è¿›è¡Œä¸Šä¸‹æ–‡æ„ŸçŸ¥æ›´æ”¹ï¼ˆå¦‚å°†æ‰‹ç§»å…¥å£è¢‹ï¼‰ã€‚æ­¤å¤–ï¼ŒLazyDragæ”¯æŒå¤šè½®å·¥ä½œæµç¨‹ï¼Œå¯åŒæ—¶æ‰§è¡Œç§»åŠ¨å’Œç¼©æ”¾æ“ä½œã€‚åœ¨DragBenchä¸Šè¿›è¡Œçš„è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ‹–åŠ¨å‡†ç¡®æ€§å’Œæ„ŸçŸ¥è´¨é‡æ–¹é¢ä¼˜äºåŸºçº¿ï¼Œè¿™å·²å¾—åˆ°VIEScoreå’Œäººç±»è¯„ä¼°çš„éªŒè¯ã€‚LazyDragä¸ä»…åˆ›é€ äº†æ–°çš„æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¿˜ä¸ºç¼–è¾‘èŒƒå¼é“ºå°±äº†æ–°è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.12203v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†LazyDragï¼Œä¸€ç§åŸºäºå¤šæ¨¡æ€æ‰©æ•£å˜å‹å™¨çš„æ‹–æ‹½å¼å›¾åƒç¼–è¾‘æ–¹æ³•ã€‚è¯¥æ–¹æ³•æ¶ˆé™¤äº†å¯¹éšå¼ç‚¹åŒ¹é…çš„ä¾èµ–ï¼Œé€šè¿‡ç”Ÿæˆæ˜¾å¼å¯¹åº”å›¾æ¥æé«˜æ³¨æ„åŠ›æ§åˆ¶ï¼Œä»è€Œå¢å¼ºæ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ã€‚LazyDragæ”¯æŒç²¾ç¡®çš„å‡ ä½•æ§åˆ¶å’Œæ–‡æœ¬æŒ‡å¯¼ï¼Œèƒ½å¤Ÿå®Œæˆå¤æ‚çš„ç¼–è¾‘ä»»åŠ¡ï¼Œå¦‚æ‰“å¼€ç‹—çš„å˜´å·´å¹¶ä¿®å¤å†…éƒ¨ã€ç”Ÿæˆæ–°å¯¹è±¡æˆ–åœ¨æ¨¡ç³Šæ‹–åŠ¨çš„æƒ…å†µä¸‹è¿›è¡Œä¸Šä¸‹æ–‡æ„ŸçŸ¥æ›´æ”¹ã€‚å®ƒæ”¯æŒå¤šè½®å·¥ä½œæµç¨‹ï¼ŒåŒæ—¶æ‰§è¡Œç§»åŠ¨å’Œç¼©æ”¾æ“ä½œï¼Œå¹¶åœ¨DragBenchä¸Šè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LazyDragæ˜¯é¦–ä¸ªåŸºäºå¤šæ¨¡æ€æ‰©æ•£å˜å‹å™¨çš„æ‹–æ‹½å¼å›¾åƒç¼–è¾‘æ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡ç”Ÿæˆæ˜¾å¼å¯¹åº”å›¾ï¼Œæ¶ˆé™¤äº†å¯¹éšå¼ç‚¹åŒ¹é…çš„ä¾èµ–ï¼Œæé«˜äº†æ³¨æ„åŠ›æ§åˆ¶ã€‚</li>
<li>LazyDragç»“åˆäº†ç²¾ç¡®çš„å‡ ä½•æ§åˆ¶å’Œæ–‡æœ¬æŒ‡å¯¼ï¼Œèƒ½å¤Ÿå®Œæˆå¤æ‚çš„ç¼–è¾‘ä»»åŠ¡ã€‚</li>
<li>è¯¥æ–¹æ³•æ”¯æŒå¤šè½®ç¼–è¾‘æµç¨‹ï¼Œå¯ä»¥åŒæ—¶è¿›è¡Œç§»åŠ¨å’Œç¼©æ”¾æ“ä½œã€‚</li>
<li>LazyDragåœ¨DragBenchä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œå…·æœ‰æ›´é«˜çš„æ‹–æ‹½å‡†ç¡®æ€§å’Œæ„ŸçŸ¥è´¨é‡ã€‚</li>
<li>LazyDragä¸ä»…åˆ›é€ äº†æ–°çš„æœ€ä½³æ€§èƒ½ï¼Œè¿˜ä¸ºç¼–è¾‘æ¨¡å¼æä¾›äº†æ–°çš„æ–¹å¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.12203">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-10c602a8e879bbe0d0e51ce05980bd66" align="middle">
<img src="https://picx.zhimg.com/v2-d6fcb621767b768313eb9c29f6637429" align="middle">
<img src="https://picx.zhimg.com/v2-c86b1a13e82762e6d3eb013ffed1b74f" align="middle">
<img src="https://picx.zhimg.com/v2-080466d0521df6e79780dab2d69109f1" align="middle">
<img src="https://picx.zhimg.com/v2-8216cc4cefa212e1f0701a1a40926337" align="middle">
<img src="https://picx.zhimg.com/v2-17eb4fc478ac68ae50ca606bfcf083a9" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="CNS-Bench-Benchmarking-Image-Classifier-Robustness-Under-Continuous-Nuisance-Shifts"><a href="#CNS-Bench-Benchmarking-Image-Classifier-Robustness-Under-Continuous-Nuisance-Shifts" class="headerlink" title="CNS-Bench: Benchmarking Image Classifier Robustness Under Continuous   Nuisance Shifts"></a>CNS-Bench: Benchmarking Image Classifier Robustness Under Continuous   Nuisance Shifts</h2><p><strong>Authors:Olaf DÃ¼nkel, Artur Jesslen, Jiahao Xie, Christian Theobalt, Christian Rupprecht, Adam Kortylewski</strong></p>
<p>An important challenge when using computer vision models in the real world is to evaluate their performance in potential out-of-distribution (OOD) scenarios. While simple synthetic corruptions are commonly applied to test OOD robustness, they often fail to capture nuisance shifts that occur in the real world. Recently, diffusion models have been applied to generate realistic images for benchmarking, but they are restricted to binary nuisance shifts. In this work, we introduce CNS-Bench, a Continuous Nuisance Shift Benchmark to quantify OOD robustness of image classifiers for continuous and realistic generative nuisance shifts. CNS-Bench allows generating a wide range of individual nuisance shifts in continuous severities by applying LoRA adapters to diffusion models. To address failure cases, we propose a filtering mechanism that outperforms previous methods, thereby enabling reliable benchmarking with generative models. With the proposed benchmark, we perform a large-scale study to evaluate the robustness of more than 40 classifiers under various nuisance shifts. Through carefully designed comparisons and analyses, we find that model rankings can change for varying shifts and shift scales, which cannot be captured when applying common binary shifts. Additionally, we show that evaluating the model performance on a continuous scale allows the identification of model failure points, providing a more nuanced understanding of model robustness. Project page including code and data: <a target="_blank" rel="noopener" href="https://genintel.github.io/CNS">https://genintel.github.io/CNS</a>. </p>
<blockquote>
<p>åœ¨ä½¿ç”¨è®¡ç®—æœºè§†è§‰æ¨¡å‹è¿›è¡Œç°å®ä¸–ç•Œåº”ç”¨æ—¶ï¼Œé¢ä¸´çš„ä¸€ä¸ªé‡è¦æŒ‘æˆ˜æ˜¯è¯„ä¼°å…¶åœ¨æ½œåœ¨çš„éåˆ†å¸ƒåœºæ™¯ä¸­çš„æ€§èƒ½ã€‚è™½ç„¶é€šå¸¸ä½¿ç”¨ç®€å•çš„åˆæˆè…èš€æ¥æµ‹è¯•éåˆ†å¸ƒåœºæ™¯çš„é²æ£’æ€§ï¼Œä½†å®ƒä»¬å¾€å¾€æ— æ³•æ•æ‰åˆ°ç°å®ä¸–ç•Œä¸­å‡ºç°çš„å¹²æ‰°å˜åŒ–ã€‚æœ€è¿‘ï¼Œæ‰©æ•£æ¨¡å‹è¢«åº”ç”¨äºç”ŸæˆçœŸå®å›¾åƒä»¥è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œä½†å®ƒä»¬ä»…é™äºäºŒå…ƒå¹²æ‰°å˜åŒ–ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†CNS-Benchï¼Œä¸€ä¸ªè¿ç»­å¹²æ‰°å˜åŒ–åŸºå‡†æµ‹è¯•ï¼Œä»¥é‡åŒ–å›¾åƒåˆ†ç±»å™¨åœ¨é¢å¯¹è¿ç»­ä¸”çœŸå®çš„ç”Ÿæˆå¹²æ‰°å˜åŒ–æ—¶çš„ç¨³å¥æ€§ã€‚CNS-Benchå…è®¸é€šè¿‡åº”ç”¨LoRAé€‚é…å™¨åˆ°æ‰©æ•£æ¨¡å‹æ¥ç”Ÿæˆä¸€ç³»åˆ—è¿ç»­çš„ä¸ªä½“å¹²æ‰°å˜åŒ–ã€‚ä¸ºäº†è§£å†³å¤±è´¥æ¡ˆä¾‹ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è¿‡æ»¤æœºåˆ¶ï¼Œè¯¥æ–¹æ³•ä¼˜äºä»¥å‰çš„æ–¹æ³•ï¼Œä»è€Œå®ç°äº†å¯é çš„åŸºå‡†æµ‹è¯•ä¸ç”Ÿæˆæ¨¡å‹ã€‚é€šè¿‡ä½¿ç”¨æ‰€æå‡ºçš„åŸºå‡†æµ‹è¯•ï¼Œæˆ‘ä»¬å¯¹è¶…è¿‡40ä¸ªåˆ†ç±»å™¨è¿›è¡Œäº†å¤§è§„æ¨¡ç ”ç©¶è¯„ä¼°å…¶åœ¨å„ç§å¹²æ‰°å˜åŒ–ä¸‹çš„ç¨³å¥æ€§ã€‚é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„æ¯”è¾ƒå’Œåˆ†æï¼Œæˆ‘ä»¬å‘ç°éšç€å˜åŒ–çš„ç§ç±»å’Œè§„æ¨¡çš„æ”¹å˜ï¼Œæ¨¡å‹æ’åå¯èƒ½ä¼šå‘ç”Ÿå˜åŒ–ï¼Œè¿™æ˜¯åœ¨ä½¿ç”¨å¸¸è§çš„äºŒå…ƒå˜åŒ–æ—¶æ‰€æ— æ³•æ•æ‰åˆ°çš„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¡¨æ˜ï¼Œåœ¨è¿ç»­è§„æ¨¡ä¸Šè¯„ä¼°æ¨¡å‹æ€§èƒ½å¯ä»¥è¯†åˆ«å‡ºæ¨¡å‹çš„å¤±è´¥ç‚¹ï¼Œä»è€Œæ›´æ·±å…¥åœ°äº†è§£æ¨¡å‹çš„ç¨³å¥æ€§ã€‚é¡¹ç›®é¡µé¢åŒ…æ‹¬ä»£ç å’Œæ•°æ®ï¼š<a target="_blank" rel="noopener" href="https://genintel.github.io/CNS%E3%80%82">https://genintel.github.io/CNSã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.17651v2">PDF</a> ICCV 2025. Project page: <a target="_blank" rel="noopener" href="https://genintel.github.io/CNS">https://genintel.github.io/CNS</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªåä¸ºCNS-Benchçš„æ–°è¿ç»­å¹²æ‰°ç§»ä½åŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¡¡é‡å›¾åƒåˆ†ç±»å™¨åœ¨è¿ç»­ä¸”ç°å®çš„ç”Ÿæˆå¹²æ‰°ç§»ä½åœºæ™¯ä¸­çš„ç¨³å¥æ€§ã€‚è¯¥åŸºå‡†æµ‹è¯•é€šè¿‡åº”ç”¨LoRAé€‚é…å™¨åˆ°æ‰©æ•£æ¨¡å‹æ¥ç”Ÿæˆå„ç§ä¸ªäººå¹²æ‰°ç§»ä½ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§è¿‡æ»¤æœºåˆ¶æ¥è§£å†³å¤±è´¥æƒ…å†µï¼Œä»è€Œå®ç°äº†å¯é çš„åŸºå‡†æµ‹è¯•ã€‚å¤§è§„æ¨¡ç ”ç©¶æ˜¾ç¤ºï¼Œåœ¨ä¸åŒçš„å¹²æ‰°ç§»ä½å’Œç§»ä½å°ºåº¦ä¸‹ï¼Œæ¨¡å‹æ’åå¯èƒ½ä¼šå‘ç”Ÿå˜åŒ–ï¼Œè€Œå¸¸è§çš„äºŒå…ƒç§»ä½æ— æ³•æ•æ‰åˆ°è¿™ä¸€ç‚¹ã€‚è¯„ä¼°æ¨¡å‹æ€§èƒ½çš„è¿ç»­æ€§æœ‰åŠ©äºå‘ç°æ¨¡å‹çš„å¤±è´¥ç‚¹ï¼Œæä¾›æ›´æ·±å…¥çš„æ¨¡å‹ç¨³å¥æ€§ç†è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CNS-Benchæ˜¯ä¸€ä¸ªæ–°çš„è¿ç»­å¹²æ‰°ç§»ä½åŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°å›¾åƒåˆ†ç±»å™¨åœ¨ç°å®ä¸–ç•Œçš„OODåœºæ™¯ä¸­çš„ç¨³å¥æ€§ã€‚<br>2.CNS-Benché€šè¿‡ä½¿ç”¨æ‰©æ•£æ¨¡å‹å’ŒLoRAé€‚é…å™¨ç”Ÿæˆå¹¿æ³›çš„å¹²æ‰°ç§»ä½ï¼Œè¿™äº›ç§»ä½æ˜¯è¿ç»­çš„å¹¶åœ¨å®é™…åœºæ™¯ä¸­å¯èƒ½å‘ç”Ÿã€‚</li>
<li>æå‡ºäº†ä¸€ç§è¿‡æ»¤æœºåˆ¶ï¼Œè§£å†³äº†åœ¨åŸºå‡†æµ‹è¯•ä¸­å‡ºç°çš„æ¨¡å‹å¤±è´¥é—®é¢˜ï¼Œä½¿å¾—ä½¿ç”¨ç”Ÿæˆæ¨¡å‹çš„åŸºå‡†æµ‹è¯•æ›´åŠ å¯é ã€‚</li>
<li>å¤§è§„æ¨¡ç ”ç©¶æ˜¾ç¤ºï¼Œåœ¨ä¸åŒçš„å¹²æ‰°ç§»ä½å’Œç§»ä½å°ºåº¦ä¸‹ï¼Œæ¨¡å‹çš„æ€§èƒ½æ’åå¯èƒ½ä¼šå‘ç”Ÿå˜åŒ–ï¼Œè¿™æ— æ³•é€šè¿‡ä¼ ç»Ÿçš„äºŒå…ƒç§»ä½æµ‹è¯•æ¥æ•æ‰ã€‚</li>
<li>ä½¿ç”¨è¿ç»­æ€§çš„è¯„ä¼°å°ºåº¦å¯ä»¥æ­ç¤ºæ¨¡å‹çš„å¼±ç‚¹ï¼Œä¸ºæ¨¡å‹ç¨³å¥æ€§æä¾›äº†æ›´æ·±å…¥çš„ç†è§£ã€‚<br>6.CNS-BenchåŒ…æ‹¬ä»£ç å’Œæ•°æ®ï¼Œå¯ä¾›å…¬ä¼—è®¿é—®å’Œä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.17651">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b8f44b756bb39811f1975822a6fa94d1" align="middle">
<img src="https://picx.zhimg.com/v2-906aed3357bf8295b33e93dda3ad7242" align="middle">
<img src="https://picx.zhimg.com/v2-108ab934cb012030dadd656ed5b5d694" align="middle">
<img src="https://picx.zhimg.com/v2-2173a1f9ff11fa53de35c8cf10dae425" align="middle">
<img src="https://picx.zhimg.com/v2-2cb07983d6d7360f1df7b9b2e1e09829" align="middle">
<img src="https://picx.zhimg.com/v2-a2fa6bac8327dcfe461140ef959413a4" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="EC-Diffuser-Multi-Object-Manipulation-via-Entity-Centric-Behavior-Generation"><a href="#EC-Diffuser-Multi-Object-Manipulation-via-Entity-Centric-Behavior-Generation" class="headerlink" title="EC-Diffuser: Multi-Object Manipulation via Entity-Centric Behavior   Generation"></a>EC-Diffuser: Multi-Object Manipulation via Entity-Centric Behavior   Generation</h2><p><strong>Authors:Carl Qi, Dan Haramati, Tal Daniel, Aviv Tamar, Amy Zhang</strong></p>
<p>Object manipulation is a common component of everyday tasks, but learning to manipulate objects from high-dimensional observations presents significant challenges. These challenges are heightened in multi-object environments due to the combinatorial complexity of the state space as well as of the desired behaviors. While recent approaches have utilized large-scale offline data to train models from pixel observations, achieving performance gains through scaling, these methods struggle with compositional generalization in unseen object configurations with constrained network and dataset sizes. To address these issues, we propose a novel behavioral cloning (BC) approach that leverages object-centric representations and an entity-centric Transformer with diffusion-based optimization, enabling efficient learning from offline image data. Our method first decomposes observations into an object-centric representation, which is then processed by our entity-centric Transformer that computes attention at the object level, simultaneously predicting object dynamics and the agentâ€™s actions. Combined with the ability of diffusion models to capture multi-modal behavior distributions, this results in substantial performance improvements in multi-object tasks and, more importantly, enables compositional generalization. We present BC agents capable of zero-shot generalization to tasks with novel compositions of objects and goals, including larger numbers of objects than seen during training. We provide video rollouts on our webpage: <a target="_blank" rel="noopener" href="https://sites.google.com/view/ec-diffuser">https://sites.google.com/view/ec-diffuser</a>. </p>
<blockquote>
<p>æ—¥å¸¸ä»»åŠ¡ä¸­å¸¸è§ç‰©ä½“æ“ä½œè¿™ä¸€ç¯èŠ‚ï¼Œä½†ä»é«˜ç»´è§‚å¯Ÿä¸­å­¦ä¹ ç‰©ä½“æ“ä½œä»å­˜åœ¨è¯¸å¤šæŒ‘æˆ˜ã€‚åœ¨å¤šç‰©ä½“ç¯å¢ƒä¸­ï¼Œç”±äºçŠ¶æ€ç©ºé—´å’Œæ‰€éœ€è¡Œä¸ºçš„ç»„åˆå¤æ‚æ€§ï¼Œè¿™äº›æŒ‘æˆ˜æ›´ä¸ºçªå‡ºã€‚è™½ç„¶è¿‘æœŸçš„æ–¹æ³•åˆ©ç”¨å¤§è§„æ¨¡ç¦»çº¿æ•°æ®ä»åƒç´ è§‚å¯Ÿä¸­è®­ç»ƒæ¨¡å‹ï¼Œå¹¶é€šè¿‡æ‰©å±•å®ç°æ€§èƒ½æå‡ï¼Œä½†è¿™äº›æ–¹æ³•åœ¨æœªè§ç‰©ä½“é…ç½®çš„ç»„åˆæ³›åŒ–æ–¹é¢é¢ä¸´å›°éš¾ï¼Œä¸”å—ç½‘ç»œå’Œæ•°æ®é›†å¤§å°çš„é™åˆ¶ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„è¡Œä¸ºå…‹éš†ï¼ˆBCï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨ä»¥ç‰©ä½“ä¸ºä¸­å¿ƒçš„è¡¨è¾¾å’Œå®ä½“ä¸ºä¸­å¿ƒçš„Transformerä¸åŸºäºæ‰©æ•£çš„ä¼˜åŒ–ï¼Œå®ç°ä»ç¦»çº¿å›¾åƒæ•°æ®çš„é«˜æ•ˆå­¦ä¹ ã€‚æˆ‘ä»¬çš„æ–¹æ³•é¦–å…ˆå°†è§‚å¯Ÿç»“æœåˆ†è§£ä¸ºä»¥ç‰©ä½“ä¸ºä¸­å¿ƒçš„è¡¨è¾¾ï¼Œç„¶åå°†å…¶è¾“å…¥å®ä½“ä¸ºä¸­å¿ƒçš„Transformerè¿›è¡Œå¤„ç†ï¼Œè¯¥å¤„ç†æ¨¡å—åœ¨ç‰©ä½“å±‚é¢è®¡ç®—æ³¨æ„åŠ›ï¼ŒåŒæ—¶é¢„æµ‹ç‰©ä½“åŠ¨æ€å’Œä»£ç†è¡Œä¸ºã€‚ç»“åˆæ‰©æ•£æ¨¡å‹æ•æ‰å¤šæ¨¡å¼è¡Œä¸ºåˆ†å¸ƒçš„èƒ½åŠ›ï¼Œè¿™å¤§å¤§æé«˜äº†å¤šç‰©ä½“ä»»åŠ¡çš„æ€§èƒ½ï¼Œæ›´é‡è¦çš„æ˜¯ï¼Œå®ç°äº†ç»„åˆæ³›åŒ–ã€‚æˆ‘ä»¬å±•ç¤ºäº†BCä»£ç†èƒ½å¤Ÿé›¶æ ·æœ¬æ³›åŒ–è‡³å…·æœ‰æ–°å‹ç‰©ä½“å’Œç›®æ ‡ç»„åˆçš„ä»»åŠ¡ï¼ŒåŒ…æ‹¬è®­ç»ƒæ—¶æœªè§è¿‡çš„æ›´å¤šç‰©ä½“ã€‚è§†é¢‘å›æ”¾è¯·è§æˆ‘ä»¬çš„ç½‘é¡µï¼š[ç½‘ç«™é“¾æ¥]ï¼ˆ<a target="_blank" rel="noopener" href="https://sites.google.com/view/ec-diffuser%EF%BC%89%E3%80%82">https://sites.google.com/view/ec-diffuserï¼‰ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.18907v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åˆ©ç”¨å¯¹è±¡ä¸­å¿ƒè¡¨ç¤ºå’Œå®ä½“ä¸­å¿ƒTransformerä¸åŸºäºæ‰©æ•£çš„ä¼˜åŒ–ç›¸ç»“åˆçš„è¡Œä¸ºå…‹éš†æ–¹æ³•ï¼Œèƒ½å¤Ÿä»ç¦»çº¿å›¾åƒæ•°æ®ä¸­é«˜æ•ˆå­¦ä¹ ã€‚è¯¥æ–¹æ³•é¦–å…ˆè§‚å¯Ÿå¯¹è±¡ä¸­å¿ƒè¡¨ç¤ºï¼Œç„¶åç”±å®ä½“ä¸­å¿ƒTransformerå¤„ç†ï¼Œè®¡ç®—å¯¹è±¡çº§åˆ«çš„æ³¨æ„åŠ›ï¼ŒåŒæ—¶é¢„æµ‹å¯¹è±¡åŠ¨æ€å’Œä»£ç†è¡Œä¸ºã€‚ç»“åˆæ‰©æ•£æ¨¡å‹æ•æ‰å¤šæ¨¡æ€è¡Œä¸ºåˆ†å¸ƒçš„èƒ½åŠ›ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç›®æ ‡ä»»åŠ¡ä¸­å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æ”¹è¿›ï¼Œå¹¶å®ç°äº†ç»„åˆæ³›åŒ–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¯¹è±¡æ“ä½œæ˜¯æ—¥å¸¸ä»»åŠ¡ä¸­çš„å¸¸è§ç»„æˆéƒ¨åˆ†ï¼Œä½†ä»é«˜ç»´è§‚å¯Ÿä¸­å­¦ä¹ å¯¹è±¡æ“ä½œå­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>å¤šç›®æ ‡ç¯å¢ƒä¸­çš„æŒ‘æˆ˜åŠ å‰§ï¼Œå› ä¸ºçŠ¶æ€ç©ºé—´å’Œæ‰€éœ€è¡Œä¸ºç»„åˆçš„å¤æ‚æ€§ã€‚</li>
<li>æœ€è¿‘çš„æ–¹æ³•åˆ©ç”¨å¤§è§„æ¨¡ç¦»çº¿æ•°æ®ä»åƒç´ è§‚æµ‹ä¸­è®­ç»ƒæ¨¡å‹ï¼Œé€šè¿‡æ‰©å±•å®ç°æ€§èƒ½æå‡ï¼Œä½†åœ¨æœªè§è¿‡çš„å¯¹è±¡é…ç½®ä¸Šéš¾ä»¥å®ç°ç»„åˆæ³›åŒ–ã€‚</li>
<li>æå‡ºä¸€ç§ç»“åˆå¯¹è±¡ä¸­å¿ƒè¡¨ç¤ºå’Œå®ä½“ä¸­å¿ƒTransformerçš„è¡Œä¸ºå…‹éš†æ–¹æ³•ï¼Œé€šè¿‡æ‰©æ•£ä¼˜åŒ–å®ç°é«˜æ•ˆå­¦ä¹ ã€‚</li>
<li>æ–¹æ³•èƒ½åŒæ—¶é¢„æµ‹å¯¹è±¡åŠ¨æ€å’Œä»£ç†è¡Œä¸ºï¼Œç»“åˆæ‰©æ•£æ¨¡å‹æ•æ‰å¤šæ¨¡æ€è¡Œä¸ºåˆ†å¸ƒçš„èƒ½åŠ›ï¼Œå®ç°å¤šç›®æ ‡ä»»åŠ¡æ€§èƒ½æ”¹è¿›ã€‚</li>
<li>æ–¹æ³•ä½¿ä»£ç†èƒ½å¤Ÿé›¶å°„å‡»æ³›åŒ–åˆ°å…·æœ‰æ–°é¢–å¯¹è±¡ç»„åˆå’Œç›®æ ‡çš„ä»»åŠ¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.18907">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a7d762d8b68b746c991cf0c3b2c79983" align="middle">
<img src="https://picx.zhimg.com/v2-0c11eb75b7c61c447a55a83f7815f70c" align="middle">
<img src="https://picx.zhimg.com/v2-7d0b3fac6cc58c8cca8ffef8fde7061c" align="middle">
<img src="https://picx.zhimg.com/v2-418e3177dc7cb6161f4edc13103d302e" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="From-Slow-Bidirectional-to-Fast-Autoregressive-Video-Diffusion-Models"><a href="#From-Slow-Bidirectional-to-Fast-Autoregressive-Video-Diffusion-Models" class="headerlink" title="From Slow Bidirectional to Fast Autoregressive Video Diffusion Models"></a>From Slow Bidirectional to Fast Autoregressive Video Diffusion Models</h2><p><strong>Authors:Tianwei Yin, Qiang Zhang, Richard Zhang, William T. Freeman, Fredo Durand, Eli Shechtman, Xun Huang</strong></p>
<p>Current video diffusion models achieve impressive generation quality but struggle in interactive applications due to bidirectional attention dependencies. The generation of a single frame requires the model to process the entire sequence, including the future. We address this limitation by adapting a pretrained bidirectional diffusion transformer to an autoregressive transformer that generates frames on-the-fly. To further reduce latency, we extend distribution matching distillation (DMD) to videos, distilling 50-step diffusion model into a 4-step generator. To enable stable and high-quality distillation, we introduce a student initialization scheme based on teacherâ€™s ODE trajectories, as well as an asymmetric distillation strategy that supervises a causal student model with a bidirectional teacher. This approach effectively mitigates error accumulation in autoregressive generation, allowing long-duration video synthesis despite training on short clips. Our model achieves a total score of 84.27 on the VBench-Long benchmark, surpassing all previous video generation models. It enables fast streaming generation of high-quality videos at 9.4 FPS on a single GPU thanks to KV caching. Our approach also enables streaming video-to-video translation, image-to-video, and dynamic prompting in a zero-shot manner. </p>
<blockquote>
<p>å½“å‰çš„è§†é¢‘æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆè´¨é‡æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†ç”±äºåŒå‘æ³¨æ„åŠ›ä¾èµ–æ€§ï¼Œåœ¨äº¤äº’å¼åº”ç”¨ä¸­è¡¨ç°å›°éš¾ã€‚ç”Ÿæˆå•ä¸ªå¸§éœ€è¦æ¨¡å‹å¤„ç†æ•´ä¸ªåºåˆ—ï¼ŒåŒ…æ‹¬æœªæ¥ä¿¡æ¯ã€‚æˆ‘ä»¬é€šè¿‡å°†é¢„è®­ç»ƒçš„åŒå‘æ‰©æ•£å˜å‹å™¨è‡ªé€‚åº”ä¸ºè‡ªå›å½’å˜å‹å™¨æ¥è§£å†³è¿™ä¸€é™åˆ¶ï¼Œè¯¥å˜å‹å™¨å¯ä»¥å³æ—¶ç”Ÿæˆå¸§ã€‚ä¸ºäº†è¿›ä¸€æ­¥é™ä½å»¶è¿Ÿï¼Œæˆ‘ä»¬å°†åˆ†å¸ƒåŒ¹é…è’¸é¦ï¼ˆDMDï¼‰æ‰©å±•åˆ°è§†é¢‘é¢†åŸŸï¼Œå°†50æ­¥æ‰©æ•£æ¨¡å‹ç²¾ç‚¼ä¸º4æ­¥ç”Ÿæˆå™¨ã€‚ä¸ºäº†å®ç°ç¨³å®šå’Œé«˜è´¨é‡çš„è’¸é¦ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºäºæ•™å¸ˆODEè½¨è¿¹çš„å­¦ç”Ÿåˆå§‹åŒ–æ–¹æ¡ˆï¼Œä»¥åŠä¸€ç§ä¸å¯¹ç§°çš„è’¸é¦ç­–ç•¥ï¼Œç”¨åŒå‘æ•™å¸ˆç›‘ç£å› æœå­¦ç”Ÿæ¨¡å‹ã€‚è¿™ç§æ–¹æ³•æœ‰æ•ˆåœ°å‡è½»äº†è‡ªå›å½’ç”Ÿæˆä¸­çš„è¯¯å·®ç§¯ç´¯ï¼Œå³ä½¿åœ¨çŸ­ç‰‡æ®µè®­ç»ƒçš„æƒ…å†µä¸‹ä¹Ÿèƒ½å®ç°é•¿æ—¶é•¿è§†é¢‘åˆæˆã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨VBench-LongåŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†84.27çš„æ€»åˆ†ï¼Œè¶…è¶Šäº†æ‰€æœ‰å…ˆå‰çš„è§†é¢‘ç”Ÿæˆæ¨¡å‹ã€‚å¾—ç›ŠäºKVç¼“å­˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹èƒ½å¤Ÿåœ¨å•ä¸ªGPUä¸Šä»¥9.4 FPSçš„é€Ÿåº¦å¿«é€Ÿç”Ÿæˆé«˜è´¨é‡è§†é¢‘ã€‚æˆ‘ä»¬çš„æ–¹æ³•è¿˜æ”¯æŒæµå¼è§†é¢‘åˆ°è§†é¢‘çš„ç¿»è¯‘ã€å›¾åƒåˆ°è§†é¢‘ä»¥åŠé›¶æ ·æœ¬æ–¹å¼çš„åŠ¨æ€æç¤ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07772v4">PDF</a> CVPR 2025. Project Page: <a target="_blank" rel="noopener" href="https://causvid.github.io/">https://causvid.github.io/</a></p>
<p><strong>Summary</strong><br>     é’ˆå¯¹å½“å‰è§†é¢‘æ‰©æ•£æ¨¡å‹åœ¨äº¤äº’å¼åº”ç”¨ä¸­é‡åˆ°çš„åŒå‘æ³¨æ„åŠ›ä¾èµ–é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§è‡ªé€‚åº”çš„é¢„è®­ç»ƒåŒå‘æ‰©æ•£å˜å‹å™¨æ”¹è¿›æ–¹æ¡ˆï¼Œå°†æ¨¡å‹è½¬å˜ä¸ºå…·æœ‰å³æ—¶ç”Ÿæˆå¸§èƒ½åŠ›çš„è‡ªå›å½’å˜å‹å™¨ã€‚ä¸ºè¿›ä¸€æ­¥æé«˜æ•ˆç‡ï¼Œæœ¬æ–‡é€šè¿‡æ‰©å±•åˆ†å¸ƒåŒ¹é…è’¸é¦ï¼ˆDMDï¼‰è‡³è§†é¢‘é¢†åŸŸï¼Œå°†50æ­¥æ‰©æ•£æ¨¡å‹ç®€åŒ–ä¸º4æ­¥ç”Ÿæˆå™¨ã€‚åŒæ—¶å¼•å…¥åŸºäºæ•™å¸ˆå¸¸å¾®åˆ†æ–¹ç¨‹è½¨è¿¹çš„å­¦ç”Ÿåˆå§‹åŒ–æ–¹æ¡ˆåŠä¸å¯¹ç§°è’¸é¦ç­–ç•¥ï¼Œç¡®ä¿ç¨³å®šä¸”é«˜è´¨é‡åœ°è¿›è¡Œè’¸é¦ï¼Œå¹¶æœ‰æ•ˆå‡å°‘è‡ªå›å½’ç”Ÿæˆä¸­çš„è¯¯å·®ç´¯ç§¯ã€‚è¯¥æ¨¡å‹åœ¨VBench-LongåŸºå‡†æµ‹è¯•ä¸­å¾—åˆ†84.27ï¼Œè¶…è¶Šæ‰€æœ‰å…ˆå‰è§†é¢‘ç”Ÿæˆæ¨¡å‹ã€‚å…¶èƒ½åœ¨å•ä¸ªGPUä¸Šä»¥9.4 FPSçš„é€Ÿåº¦å¿«é€Ÿç”Ÿæˆé«˜è´¨é‡è§†é¢‘ï¼Œå¹¶æ”¯æŒè§†é¢‘åˆ°è§†é¢‘çš„æµå¼è½¬æ¢ã€å›¾åƒåˆ°è§†é¢‘è½¬æ¢ä»¥åŠé›¶æ ·æœ¬åŠ¨æ€æç¤ºç”Ÿæˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰è§†é¢‘æ‰©æ•£æ¨¡å‹åœ¨äº¤äº’å¼åº”ç”¨ä¸­é¢ä¸´åŒå‘æ³¨æ„åŠ›ä¾èµ–é—®é¢˜ã€‚</li>
<li>æå‡ºä¸€ç§æ”¹è¿›çš„é¢„è®­ç»ƒåŒå‘æ‰©æ•£å˜å‹å™¨ï¼Œè½¬å˜ä¸ºè‡ªå›å½’æ¨¡å‹ä»¥å³æ—¶ç”Ÿæˆå¸§ã€‚</li>
<li>é€šè¿‡æ‰©å±•åˆ†å¸ƒåŒ¹é…è’¸é¦ï¼ˆDMDï¼‰è‡³è§†é¢‘é¢†åŸŸï¼Œæé«˜ç”Ÿæˆæ•ˆç‡ï¼Œç®€åŒ–æ¨¡å‹æ­¥éª¤ã€‚</li>
<li>å¼•å…¥åŸºäºæ•™å¸ˆå¸¸å¾®åˆ†æ–¹ç¨‹è½¨è¿¹çš„å­¦ç”Ÿåˆå§‹åŒ–æ–¹æ¡ˆï¼Œç¡®ä¿ç¨³å®šä¸”é«˜è´¨é‡åœ°è¿›è¡Œè’¸é¦ã€‚</li>
<li>é‡‡ç”¨ä¸å¯¹ç§°è’¸é¦ç­–ç•¥ï¼Œå‡å°‘è‡ªå›å½’ç”Ÿæˆä¸­çš„è¯¯å·®ç´¯ç§¯ã€‚</li>
<li>æ¨¡å‹åœ¨VBench-LongåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå¾—åˆ†84.27ï¼Œè¶…è¶Šå…ˆå‰æ‰€æœ‰è§†é¢‘ç”Ÿæˆæ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.07772">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ca135ca9a2b78f63cb7b6c29a6304e23" align="middle">
<img src="https://picx.zhimg.com/v2-6a143035a715c0602a4cfe27a4075096" align="middle">
<img src="https://picx.zhimg.com/v2-0c2e990fa887b600dd18975b8a076df3" align="middle">
<img src="https://picx.zhimg.com/v2-91bed3d0447daa99273bfa587ea27ba0" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Supercharged-One-step-Text-to-Image-Diffusion-Models-with-Negative-Prompts"><a href="#Supercharged-One-step-Text-to-Image-Diffusion-Models-with-Negative-Prompts" class="headerlink" title="Supercharged One-step Text-to-Image Diffusion Models with Negative   Prompts"></a>Supercharged One-step Text-to-Image Diffusion Models with Negative   Prompts</h2><p><strong>Authors:Viet Nguyen, Anh Nguyen, Trung Dao, Khoi Nguyen, Cuong Pham, Toan Tran, Anh Tran</strong></p>
<p>The escalating demand for real-time image synthesis has driven significant advancements in one-step diffusion models, which inherently offer expedited generation speeds compared to traditional multi-step methods. However, this enhanced efficiency is frequently accompanied by a compromise in the controllability of image attributes. While negative prompting, typically implemented via classifier-free guidance (CFG), has proven effective for fine-grained control in multi-step models, its application to one-step generators remains largely unaddressed. Due to the lack of iterative refinement, as in multi-step diffusion, directly applying CFG to one-step generation leads to blending artifacts and diminished output quality. To fill this gap, we introduce \textbf{N}egative-\textbf{A}way \textbf{S}teer \textbf{A}ttention (NASA), an efficient method that integrates negative prompts into one-step diffusion models. NASA operates within the intermediate representation space by leveraging cross-attention mechanisms to suppress undesired visual attributes. This strategy avoids the blending artifacts inherent in output-space guidance and achieves high efficiency, incurring only a minimal 1.89% increase in FLOPs compared to the computational doubling of CFG. Furthermore, NASA can be seamlessly integrated into existing timestep distillation frameworks, enhancing the studentâ€™s output quality. Experimental results demonstrate that NASA substantially improves controllability and output quality, achieving an HPSv2 score of \textbf{31.21}, setting a new state-of-the-art benchmark for one-step diffusion models. </p>
<blockquote>
<p>éšç€å¯¹å®æ—¶å›¾åƒåˆæˆéœ€æ±‚çš„ä¸æ–­å¢åŠ ï¼Œä¸€æ­¥æ‰©æ•£æ¨¡å‹å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œä¸ä¼ ç»Ÿçš„å¤šæ­¥æ–¹æ³•ç›¸æ¯”ï¼Œå…¶å¤©ç”Ÿå°±æä¾›äº†æ›´å¿«çš„ç”Ÿæˆé€Ÿåº¦ã€‚ç„¶è€Œï¼Œè¿™ç§æé«˜æ•ˆç‡å¾€å¾€ä¼´éšç€å›¾åƒå±æ€§å¯æ§æ€§çš„å¦¥åã€‚è´Ÿæç¤ºï¼ˆé€šå¸¸é€šè¿‡æ— åˆ†ç±»å™¨å¼•å¯¼ï¼ˆCFGï¼‰æ¥å®ç°ï¼‰å·²åœ¨å¤šæ­¥æ¨¡å‹ä¸­å®ç°äº†ç²¾ç»†æ§åˆ¶ï¼Œä½†å…¶åœ¨ä¸€æ­¥ç”Ÿæˆå™¨ä¸­çš„åº”ç”¨ä»ç„¶å¾ˆå°‘è¢«ç ”ç©¶ã€‚ç”±äºç¼ºä¹å¦‚å¤šæ­¥æ‰©æ•£ä¸­çš„è¿­ä»£ä¼˜åŒ–ï¼Œç›´æ¥å°†CFGåº”ç”¨äºä¸€æ­¥ç”Ÿæˆä¼šå¯¼è‡´æ··åˆä¼ªå½±å’Œè¾“å‡ºè´¨é‡ä¸‹é™ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¼•å…¥äº†è´Ÿå‘æ§åˆ¶æ³¨æ„åŠ›ï¼ˆNASAï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å°†è´Ÿæç¤ºæœ‰æ•ˆåœ°é›†æˆåˆ°ä¸€æ­¥æ‰©æ•£æ¨¡å‹ä¸­çš„é«˜æ•ˆæ–¹æ³•ã€‚NASAé€šè¿‡åœ¨ä¸­é—´è¡¨ç¤ºç©ºé—´ä¸­æ“ä½œï¼Œåˆ©ç”¨äº¤å‰æ³¨æ„åŠ›æœºåˆ¶æ¥æŠ‘åˆ¶ä¸å¸Œæœ›çš„è§†è§‰å±æ€§ã€‚è¿™ç§ç­–ç•¥é¿å…äº†è¾“å‡ºç©ºé—´æŒ‡å¯¼æ‰€å›ºæœ‰çš„æ··åˆä¼ªå½±ï¼Œå¹¶å®ç°äº†é«˜æ•ˆç‡ï¼Œä¸CFGçš„è®¡ç®—åŠ å€ç›¸æ¯”ï¼Œä»…å¢åŠ äº†1.89%çš„FLOPsã€‚æ­¤å¤–ï¼ŒNASAå¯ä»¥æ— ç¼åœ°é›†æˆåˆ°ç°æœ‰çš„æ—¶é—´æ­¥è’¸é¦æ¡†æ¶ä¸­ï¼Œå¢å¼ºäº†å­¦ç”Ÿçš„è¾“å‡ºè´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒNASAåœ¨å¯æ§æ€§å’Œè¾“å‡ºè´¨é‡æ–¹é¢éƒ½æœ‰æ˜¾è‘—æé«˜ï¼Œè¾¾åˆ°äº†HPSv2è¯„åˆ†31.21åˆ†ï¼Œä¸ºä¸€æ­¥æ‰©æ•£æ¨¡å‹æ ‘ç«‹äº†æ–°çš„ä¸šç•Œæ ‡æ†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.02687v3">PDF</a> Accepted at ICCV 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>å®æ—¶å›¾åƒåˆæˆéœ€æ±‚çš„ä¸æ–­å‡çº§æ¨åŠ¨äº†ä¸€æ­¥æ‰©æ•£æ¨¡å‹çš„æ˜¾è‘—å‘å±•ï¼Œè¯¥æ¨¡å‹ç›¸è¾ƒäºä¼ ç»Ÿå¤šæ­¥æ–¹æ³•æä¾›äº†æ›´å¿«çš„ç”Ÿæˆé€Ÿåº¦ã€‚ç„¶è€Œï¼Œè¿™ç§æå‡çš„æ•ˆç‡é€šå¸¸ä¼´éšç€å›¾åƒå±æ€§æ§åˆ¶æ€§çš„å¦¥åã€‚è™½ç„¶è´Ÿæç¤ºé€šè¿‡æ— åˆ†ç±»å¼•å¯¼ï¼ˆCFGï¼‰åœ¨å¤šéƒ¨æ¨¡å‹ä¸­å·²å®ç°ç²¾ç»†æ§åˆ¶ï¼Œä½†å…¶åœ¨ä¸€æ­¥ç”Ÿæˆå™¨ä¸­çš„åº”ç”¨ä»ç„¶åŸºæœ¬æœªè¢«è§£å†³ã€‚ç”±äºç¼ºä¹å¦‚å¤šæ­¥æ‰©æ•£ä¸­çš„è¿­ä»£ä¼˜åŒ–ï¼Œç›´æ¥åœ¨ä¸€æ­¥ç”Ÿæˆä¸­åº”ç”¨CFGä¼šå¯¼è‡´æ··åˆä¼ªå½±å’Œè¾“å‡ºè´¨é‡ä¸‹é™ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¼•å…¥äº†è´Ÿå‘æ³¨æ„åŠ›ï¼ˆNASAï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å°†è´Ÿæç¤ºæ•´åˆåˆ°ä¸€æ­¥æ‰©æ•£æ¨¡å‹ä¸­çš„é«˜æ•ˆæ–¹æ³•ã€‚NASAé€šè¿‡åœ¨ä¸­é—´è¡¨ç¤ºç©ºé—´å†…åˆ©ç”¨äº¤å‰æ³¨æ„åŠ›æœºåˆ¶æ¥æŠ‘åˆ¶ä¸æƒ³è¦çš„è§†è§‰å±æ€§æ¥å·¥ä½œã€‚è¿™ç§ç­–ç•¥é¿å…äº†è¾“å‡ºç©ºé—´æŒ‡å¯¼çš„å›ºæœ‰æ··åˆä¼ªå½±ï¼Œå¹¶ä¸”æ•ˆç‡é«˜ï¼Œç›¸è¾ƒäºè®¡ç®—ç¿»å€çš„CFGåªå¢åŠ äº†é¢å¤–çš„1.89%æµ®ç‚¹è¿ç®—é‡ã€‚æ­¤å¤–ï¼ŒNASAå¯ä»¥æ— ç¼é›†æˆåˆ°ç°æœ‰çš„æ—¶é—´æ­¥è’¸é¦æ¡†æ¶ä¸­ï¼Œæé«˜äº†å­¦ç”Ÿæ¨¡å‹çš„è¾“å‡ºè´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒNASAåœ¨æ§åˆ¶æ€§å’Œè¾“å‡ºè´¨é‡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æå‡ï¼Œå®ç°äº†HPSv2è¯„åˆ†31.21çš„æ–°é‡Œç¨‹ç¢‘æˆç»©ï¼Œä¸ºä¸€æ­¥æ‰©æ•£æ¨¡å‹åˆ›é€ äº†æ–°çš„å…ˆè¿›åŸºå‡†ã€‚ </p>
<p><strong>å…³é”®è§è§£</strong></p>
<p>ä¸€ã€å®æ—¶å›¾åƒåˆæˆéœ€æ±‚çš„å¢é•¿æ¨åŠ¨äº†ä¸€æ­¥æ‰©æ•£æ¨¡å‹çš„å¿«é€Ÿå‘å±•ï¼Œå…¶æä¾›æ›´å¿«çš„ç”Ÿæˆé€Ÿåº¦ã€‚<br>äºŒã€è™½ç„¶è´Ÿæç¤ºåœ¨å¤šæ­¥æ¨¡å‹ä¸­å·²è¢«è¯æ˜å¯¹ç²¾ç»†æ§åˆ¶æœ‰æ•ˆï¼Œä½†å…¶åœ¨ä¸€æ­¥ç”Ÿæˆå™¨ä¸­çš„åº”ç”¨ä»å­˜åœ¨æŒ‘æˆ˜ï¼Œå¯èƒ½å¯¼è‡´æ··åˆä¼ªå½±å’Œè¾“å‡ºè´¨é‡ä¸‹é™ã€‚<br>ä¸‰ã€NASAæ˜¯ä¸€ç§é«˜æ•ˆçš„è´Ÿæç¤ºé›†æˆæ–¹æ³•ï¼Œèƒ½åœ¨ä¸€æ­¥æ‰©æ•£æ¨¡å‹ä¸­æŠ‘åˆ¶ä¸æƒ³è¦çš„è§†è§‰å±æ€§ã€‚<br>å››ã€NASAåœ¨ä¸­é—´è¡¨ç¤ºç©ºé—´å†…æ“ä½œï¼Œé¿å…äº†è¾“å‡ºç©ºé—´æŒ‡å¯¼çš„æ··åˆä¼ªå½±é—®é¢˜ã€‚<br>äº”ã€ç›¸è¾ƒäºè®¡ç®—ç¿»å€çš„CFGï¼ŒNASAä»…å¢åŠ äº†é¢å¤–çš„1.89%æµ®ç‚¹è¿ç®—é‡ï¼Œå…·æœ‰é«˜æ•ˆç‡ä¼˜åŠ¿ã€‚<br>å…­ã€NASAå¯ä»¥ä¸ç°æœ‰çš„æ—¶é—´æ­¥è’¸é¦æ¡†æ¶æ— ç¼é›†æˆï¼Œè¿›ä¸€æ­¥æé«˜å­¦ç”Ÿæ¨¡å‹çš„è¾“å‡ºè´¨é‡ã€‚</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.02687">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9670197ec5c81232cf6f03478eae69ab" align="middle">
<img src="https://picx.zhimg.com/v2-79d95e7efe24e185bdd1b26d5787090f" align="middle">
<img src="https://picx.zhimg.com/v2-ff73ec55a5ee2111d0e35fe1af55fb90" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Training-Free-Layout-to-Image-Generation-with-Marginal-Attention-Constraints"><a href="#Training-Free-Layout-to-Image-Generation-with-Marginal-Attention-Constraints" class="headerlink" title="Training-Free Layout-to-Image Generation with Marginal Attention   Constraints"></a>Training-Free Layout-to-Image Generation with Marginal Attention   Constraints</h2><p><strong>Authors:Huancheng Chen, Jingtao Li, Weiming Zhuang, Haris Vikalo, Lingjuan Lyu</strong></p>
<p>Recently, many text-to-image diffusion models excel at generating high-resolution images from text but struggle with precise control over spatial composition and object counting. To address these challenges, prior works developed layout-to-image (L2I) approaches that incorporate layout instructions into text-to-image models. However, existing L2I methods typically require fine-tuning of pre-trained parameters or training additional control modules for the diffusion models. In this work, we propose a training-free L2I approach, MAC (Marginal Attention Constrained Generation), which eliminates the need for additional modules or fine-tuning. Specifically, we use text-visual cross-attention feature maps to quantify inconsistencies between the layout of the generated images and the provided instructions, and then compute loss functions to optimize latent features during the diffusion reverse process. To enhance spatial controllability and mitigate semantic failures in complex layout instructions, we leverage pixel-to-pixel correlations in the self-attention feature maps to align cross-attention maps and combine three loss functions constrained by boundary attention to update latent features. Comprehensive experimental results on both L2I and non-L2I pretrained diffusion models demonstrate that our method outperforms existing training-free L2I techniques both quantitatively and qualitatively in terms of image composition on the DrawBench and HRS benchmarks. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œè®¸å¤šæ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹åœ¨æ ¹æ®æ–‡æœ¬ç”Ÿæˆé«˜åˆ†è¾¨ç‡å›¾åƒæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ç©ºé—´æ„å›¾å’Œç‰©ä½“è®¡æ•°æ–¹é¢çš„ç²¾ç¡®æ§åˆ¶ä¸Šé‡åˆ°äº†å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæ—©æœŸçš„ç ”ç©¶å¼€å‘äº†å¸ƒå±€åˆ°å›¾åƒï¼ˆL2Iï¼‰çš„æ–¹æ³•ï¼Œå°†å¸ƒå±€æŒ‡ä»¤èå…¥æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ä¸­ã€‚ç„¶è€Œï¼Œç°æœ‰çš„L2Iæ–¹æ³•é€šå¸¸éœ€è¦è°ƒæ•´é¢„è®­ç»ƒå‚æ•°æˆ–ä¸ºæ‰©æ•£æ¨¡å‹è®­ç»ƒé¢å¤–çš„æ§åˆ¶æ¨¡å—ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„L2Iæ–¹æ³•ï¼Œåä¸ºMACï¼ˆè¾¹é™…æ³¨æ„åŠ›çº¦æŸç”Ÿæˆï¼‰ï¼Œå®ƒæ— éœ€é¢å¤–çš„æ¨¡å—æˆ–å¾®è°ƒã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä½¿ç”¨æ–‡æœ¬è§†è§‰äº¤å‰æ³¨æ„åŠ›ç‰¹å¾å›¾æ¥é‡åŒ–ç”Ÿæˆå›¾åƒå¸ƒå±€ä¸æ‰€æä¾›æŒ‡ä»¤ä¹‹é—´çš„ä¸ä¸€è‡´æ€§ï¼Œç„¶åè®¡ç®—æŸå¤±å‡½æ•°ï¼Œåœ¨æ‰©æ•£åè½¬è¿‡ç¨‹ä¸­ä¼˜åŒ–æ½œåœ¨ç‰¹å¾ã€‚ä¸ºäº†æé«˜ç©ºé—´å¯æ§æ€§å¹¶ç¼“è§£å¤æ‚å¸ƒå±€æŒ‡ä»¤ä¸­çš„è¯­ä¹‰é”™è¯¯ï¼Œæˆ‘ä»¬åˆ©ç”¨è‡ªæ³¨æ„åŠ›ç‰¹å¾å›¾ä¸­çš„åƒç´ åˆ°åƒç´ ç›¸å…³æ€§æ¥å¯¹é½äº¤å‰æ³¨æ„åŠ›å›¾ï¼Œå¹¶ç»“åˆä¸‰ç§å—è¾¹ç•Œæ³¨æ„åŠ›çº¦æŸçš„æŸå¤±å‡½æ•°æ¥æ›´æ–°æ½œåœ¨ç‰¹å¾ã€‚åœ¨L2Iå’ŒéL2Ié¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ä¸Šçš„ç»¼åˆå®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨DrawBenchå’ŒHRSåŸºå‡†æµ‹è¯•ä¸Šï¼Œæ— è®ºåœ¨æ•°é‡ä¸Šè¿˜æ˜¯åœ¨è´¨é‡ä¸Šï¼Œéƒ½è¶…è¶Šäº†ç°æœ‰çš„æ— éœ€è®­ç»ƒçš„L2IæŠ€æœ¯ï¼Œç‰¹åˆ«æ˜¯åœ¨å›¾åƒæ„å›¾æ–¹é¢ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.10495v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„æ–‡æœ¬å¸ƒå±€è½¬å›¾åƒï¼ˆL2Iï¼‰æ–¹æ³•ï¼Œåä¸ºMACï¼ˆè¾¹é™…æ³¨æ„åŠ›çº¦æŸç”Ÿæˆï¼‰ã€‚è¯¥æ–¹æ³•åˆ©ç”¨æ–‡æœ¬è§†è§‰äº¤å‰æ³¨æ„åŠ›ç‰¹å¾å›¾æ¥é‡åŒ–ç”Ÿæˆå›¾åƒå¸ƒå±€ä¸æŒ‡ä»¤ä¹‹é—´çš„ä¸€è‡´æ€§ï¼Œé€šè¿‡è®¡ç®—æŸå¤±å‡½æ•°åœ¨æ‰©æ•£åå‘è¿‡ç¨‹ä¸­ä¼˜åŒ–æ½œåœ¨ç‰¹å¾ï¼Œæé«˜ç©ºé—´æ§åˆ¶åŠ›å¹¶å‡å°‘å¤æ‚å¸ƒå±€æŒ‡ä»¤ä¸­çš„è¯­ä¹‰é”™è¯¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨L2Iå’ŒéL2Ié¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ä¸Šå‡ä¼˜äºç°æœ‰çš„æ— è®­ç»ƒL2IæŠ€æœ¯ï¼Œç‰¹åˆ«æ˜¯åœ¨å›¾åƒç»„åˆæ–¹é¢ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬è½¬å›¾åƒæ‰©æ•£æ¨¡å‹é¢ä¸´ç©ºé—´ç»„åˆå’Œå¯¹è±¡è®¡æ•°æ§åˆ¶æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰L2Iæ–¹æ³•é€šå¸¸éœ€è¦å¾®è°ƒé¢„è®­ç»ƒå‚æ•°æˆ–è®­ç»ƒé¢å¤–çš„æ§åˆ¶æ¨¡å—ã€‚</li>
<li>MACæ–¹æ³•æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„L2IæŠ€æœ¯ï¼Œåˆ©ç”¨æ–‡æœ¬è§†è§‰äº¤å‰æ³¨æ„åŠ›ç‰¹å¾å›¾é‡åŒ–å¸ƒå±€ä¸ä¸€è‡´æ€§ã€‚</li>
<li>MACæ–¹æ³•é€šè¿‡è®¡ç®—æŸå¤±å‡½æ•°ä¼˜åŒ–æ½œåœ¨ç‰¹å¾åœ¨æ‰©æ•£åå‘è¿‡ç¨‹ä¸­ã€‚</li>
<li>MACæ–¹æ³•æé«˜äº†ç©ºé—´æ§åˆ¶åŠ›å¹¶å‡å°‘äº†å¤æ‚å¸ƒå±€æŒ‡ä»¤ä¸­çš„è¯­ä¹‰é”™è¯¯ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒMACæ–¹æ³•åœ¨L2Iå’ŒéL2Ié¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.10495">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2e1a3cb96c57e0bca33a3002410ab8ad" align="middle">
<img src="https://picx.zhimg.com/v2-67d6c96662b0144def553359648f520f" align="middle">
<img src="https://picx.zhimg.com/v2-d1aee85048b14bbcb8c9aa170354688a" align="middle">
<img src="https://picx.zhimg.com/v2-616208ea8de1e4b22ba161e092023dff" align="middle">
<img src="https://picx.zhimg.com/v2-cc4829dded45e5bbefdfa8bbf5242227" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="IDEATOR-Jailbreaking-and-Benchmarking-Large-Vision-Language-Models-Using-Themselves"><a href="#IDEATOR-Jailbreaking-and-Benchmarking-Large-Vision-Language-Models-Using-Themselves" class="headerlink" title="IDEATOR: Jailbreaking and Benchmarking Large Vision-Language Models   Using Themselves"></a>IDEATOR: Jailbreaking and Benchmarking Large Vision-Language Models   Using Themselves</h2><p><strong>Authors:Ruofan Wang, Juncheng Li, Yixu Wang, Bo Wang, Xiaosen Wang, Yan Teng, Yingchun Wang, Xingjun Ma, Yu-Gang Jiang</strong></p>
<p>As large Vision-Language Models (VLMs) gain prominence, ensuring their safe deployment has become critical. Recent studies have explored VLM robustness against jailbreak attacks-techniques that exploit model vulnerabilities to elicit harmful outputs. However, the limited availability of diverse multimodal data has constrained current approaches to rely heavily on adversarial or manually crafted images derived from harmful text datasets, which often lack effectiveness and diversity across different contexts. In this paper, we propose IDEATOR, a novel jailbreak method that autonomously generates malicious image-text pairs for black-box jailbreak attacks. IDEATOR is grounded in the insight that VLMs themselves could serve as powerful red team models for generating multimodal jailbreak prompts. Specifically, IDEATOR leverages a VLM to create targeted jailbreak texts and pairs them with jailbreak images generated by a state-of-the-art diffusion model. Extensive experiments demonstrate IDEATORâ€™s high effectiveness and transferability, achieving a 94% attack success rate (ASR) in jailbreaking MiniGPT-4 with an average of only 5.34 queries, and high ASRs of 82%, 88%, and 75% when transferred to LLaVA, InstructBLIP, and Chameleon, respectively. Building on IDEATORâ€™s strong transferability and automated process, we introduce the VLJailbreakBench, a safety benchmark comprising 3,654 multimodal jailbreak samples. Our benchmark results on 11 recently released VLMs reveal significant gaps in safety alignment. For instance, our challenge set achieves ASRs of 46.31% on GPT-4o and 19.65% on Claude-3.5-Sonnet, underscoring the urgent need for stronger defenses. VLJailbreakBench is publicly available at <a target="_blank" rel="noopener" href="https://roywang021.github.io/VLJailbreakBench">https://roywang021.github.io/VLJailbreakBench</a>. </p>
<blockquote>
<p>éšç€å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰è¶Šæ¥è¶Šçªå‡ºï¼Œç¡®ä¿å…¶å®‰å…¨éƒ¨ç½²å·²æˆä¸ºå…³é”®ã€‚è¿‘æœŸçš„ç ”ç©¶å·²ç»æ¢ç´¢äº†VLMå¯¹æŠ—è¶Šç‹±æ”»å‡»ï¼ˆåˆ©ç”¨æ¨¡å‹æ¼æ´è¯±å‘æœ‰å®³è¾“å‡ºçš„æŠ€æœ¯ï¼‰çš„ç¨³å¥æ€§ã€‚ç„¶è€Œï¼Œå¤šæ ·æ€æ•°æ®çš„æœ‰é™å¯ç”¨æ€§é™åˆ¶äº†å½“å‰æ–¹æ³•ä¸¥é‡ä¾èµ–äºä»æœ‰å®³æ–‡æœ¬æ•°æ®é›†æ´¾ç”Ÿçš„å¯¹æŠ—æ€§æˆ–æ‰‹å·¥åˆ¶ä½œçš„å›¾åƒï¼Œè¿™åœ¨ä¸åŒçš„ä¸Šä¸‹æ–‡ç¯å¢ƒä¸­å¾€å¾€ç¼ºä¹æœ‰æ•ˆæ€§å’Œå¤šæ ·æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†IDEATORï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹è¶Šç‹±æ–¹æ³•ï¼Œèƒ½å¤Ÿè‡ªä¸»ç”Ÿæˆæ¶æ„å›¾åƒæ–‡æœ¬å¯¹ï¼Œç”¨äºé»‘ç®±è¶Šç‹±æ”»å‡»ã€‚IDEATORçš„è§è§£åœ¨äºï¼ŒVLMæœ¬èº«å¯ä»¥ä½œä¸ºå¼ºå¤§çš„çº¢é˜Ÿæ¨¡å‹ï¼Œç”¨äºç”Ÿæˆå¤šæ¨¡å¼è¶Šç‹±æç¤ºã€‚å…·ä½“æ¥è¯´ï¼ŒIDEATORåˆ©ç”¨VLMæ¥åˆ›å»ºæœ‰é’ˆå¯¹æ€§çš„è¶Šç‹±æ–‡æœ¬ï¼Œå¹¶å°†å®ƒä»¬ä¸ç”±æœ€å…ˆè¿›çš„æ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„è¶Šç‹±å›¾åƒé…å¯¹ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒIDEATORå…·æœ‰å¾ˆé«˜çš„æœ‰æ•ˆæ€§å’Œå¯è¿ç§»æ€§ï¼Œåœ¨è¶Šç‹±MiniGPT-4æ—¶ï¼Œæ”»å‡»æˆåŠŸç‡ï¼ˆASRï¼‰è¾¾åˆ°94%ï¼Œå¹³å‡æŸ¥è¯¢æ¬¡æ•°ä»…ä¸º5.34æ¬¡ï¼›å½“è¿ç§»åˆ°LLaVAã€InstructBLIPå’ŒChameleonæ—¶ï¼ŒASRåˆ†åˆ«è¾¾åˆ°äº†82%ã€88%å’Œ75%ã€‚å»ºç«‹åœ¨IDEATORçš„å¼ºå¤§å¯è¿ç§»æ€§å’Œè‡ªåŠ¨åŒ–æµç¨‹ä¹‹ä¸Šï¼Œæˆ‘ä»¬å¼•å…¥äº†VLJailbreakBenchï¼Œè¿™æ˜¯ä¸€ä¸ªå®‰å…¨åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«3654ä¸ªå¤šæ¨¡å¼è¶Šç‹±æ ·æœ¬ã€‚æˆ‘ä»¬å¯¹æœ€è¿‘å‘å¸ƒçš„11ä¸ªVLMçš„åŸºå‡†æµ‹è¯•ç»“æœæ­ç¤ºäº†å®‰å…¨å¯¹é½æ–¹é¢çš„é‡å¤§å·®è·ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬çš„æŒ‘æˆ˜é›†åœ¨GPT-4oä¸Šçš„ASRè¾¾åˆ°46.31%ï¼Œåœ¨Claude-3.5-Sonnetä¸Šçš„ASRä¸º19.65%ï¼Œè¿™çªæ˜¾äº†æ›´å¼ºå¤§é˜²å¾¡çš„è¿«åˆ‡éœ€è¦ã€‚VLJailbreakBenchå¯åœ¨å…¬å¼€è®¿é—®<a target="_blank" rel="noopener" href="https://roywang021.github.io/VLJailbreakBench%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://roywang021.github.io/VLJailbreakBenchä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.00827v6">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å…³æ³¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„å®‰å…¨éƒ¨ç½²é—®é¢˜ã€‚é’ˆå¯¹VLMsçš„jailbreakæ”»å‡»ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ”»å‡»æ–¹æ³•IDEATORï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿè‡ªä¸»ç”Ÿæˆæ¶æ„å›¾åƒæ–‡æœ¬å¯¹ï¼Œç”¨äºé»‘ç›’jailbreakæ”»å‡»ã€‚IDEATORåˆ©ç”¨VLMç”Ÿæˆæœ‰é’ˆå¯¹æ€§çš„jailbreakæ–‡æœ¬ï¼Œå¹¶ä¸ç”±æœ€å…ˆè¿›çš„æ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„jailbreakå›¾åƒé…å¯¹ã€‚å®éªŒè¡¨æ˜ï¼ŒIDEATORå…·æœ‰é«˜æœ‰æ•ˆæ€§å’Œå¯è¿ç§»æ€§ï¼Œèƒ½å¤Ÿåœ¨MiniGPT-4ä¸Šè¾¾åˆ°94%çš„æ”»å‡»æˆåŠŸç‡ï¼ˆASRï¼‰ï¼Œå¹¶ä¸”å…·æœ‰è¾ƒé«˜çš„å¯è¿ç§»æ€§ã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜ä»‹ç»äº†åŸºäºIDEATORçš„VLJailbreakBenchå®‰å…¨åŸºå‡†ï¼Œè¯¥åŸºå‡†åŒ…å«3654ä¸ªå¤šæ¨¡æ€jailbreakæ ·æœ¬ï¼Œæ­ç¤ºäº†VLMsåœ¨å®‰å…¨å¯¹é½æ–¹é¢çš„æ˜¾è‘—å·®è·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„å®‰å…¨éƒ¨ç½²è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰çš„VLMså®¹æ˜“å—åˆ°jailbreakæ”»å‡»ï¼Œè¿™åˆ©ç”¨æ¨¡å‹æ¼æ´äº§ç”Ÿæœ‰å®³è¾“å‡ºã€‚</li>
<li>IDEATORæ˜¯ä¸€ç§æ–°å‹æ”»å‡»æ–¹æ³•ï¼Œå¯è‡ªä¸»ç”Ÿæˆæ¶æ„å›¾åƒæ–‡æœ¬å¯¹è¿›è¡Œé»‘ç›’jailbreakæ”»å‡»ã€‚</li>
<li>IDEATORåˆ©ç”¨VLMç”Ÿæˆé’ˆå¯¹æ€§çš„jailbreakæ–‡æœ¬ï¼Œç»“åˆæ‰©æ•£æ¨¡å‹ç”Ÿæˆå›¾åƒã€‚</li>
<li>IDEATORå…·æœ‰é«˜æœ‰æ•ˆæ€§å’Œå¯è¿ç§»æ€§ï¼Œèƒ½åœ¨ä¸åŒæ¨¡å‹ä¸Šå®ç°è¾ƒé«˜çš„æ”»å‡»æˆåŠŸç‡ã€‚</li>
<li>VLJailbreakBenchå®‰å…¨åŸºå‡†åŒ…å«å¤§é‡å¤šæ¨¡æ€jailbreakæ ·æœ¬ï¼Œç”¨äºè¯„ä¼°VLMsçš„å®‰å…¨æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.00827">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-35aa3555d08a81262324dd663780f21d" align="middle">
<img src="https://picx.zhimg.com/v2-5687588b2db0ce2c1701e83436a4a12c" align="middle">
<img src="https://picx.zhimg.com/v2-e24fa3f0626f934c781f274998e123a5" align="middle">
<img src="https://picx.zhimg.com/v2-9f77e4d08c85f3b65c1c8cdf9ba14e0c" align="middle">
<img src="https://picx.zhimg.com/v2-367514a55c965c3f8d17bb85708b5f62" align="middle">
<img src="https://picx.zhimg.com/v2-50761405ecea72bc7ea14ca18a5ce16e" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Diffusion-Curriculum-Synthetic-to-Real-Generative-Curriculum-Learning-via-Image-Guided-Diffusion"><a href="#Diffusion-Curriculum-Synthetic-to-Real-Generative-Curriculum-Learning-via-Image-Guided-Diffusion" class="headerlink" title="Diffusion Curriculum: Synthetic-to-Real Generative Curriculum Learning   via Image-Guided Diffusion"></a>Diffusion Curriculum: Synthetic-to-Real Generative Curriculum Learning   via Image-Guided Diffusion</h2><p><strong>Authors:Yijun Liang, Shweta Bhardwaj, Tianyi Zhou</strong></p>
<p>Low-quality or scarce data has posed significant challenges for training deep neural networks in practice. While classical data augmentation cannot contribute very different new data, diffusion models opens up a new door to build self-evolving AI by generating high-quality and diverse synthetic data through text-guided prompts. However, text-only guidance cannot control synthetic imagesâ€™ proximity to the original images, resulting in out-of-distribution data detrimental to the model performance. To overcome the limitation, we study image guidance to achieve a spectrum of interpolations between synthetic and real images. With stronger image guidance, the generated images are similar to the training data but hard to learn. While with weaker image guidance, the synthetic images will be easier for model but contribute to a larger distribution gap with the original data. The generated full spectrum of data enables us to build a novel â€œDiffusion Curriculum (DisCL)â€. DisCL adjusts the image guidance level of image synthesis for each training stage: It identifies and focuses on hard samples for the model and assesses the most effective guidance level of synthetic images to improve hard data learning. We apply DisCL to two challenging tasks: long-tail (LT) classification and learning from low-quality data. It focuses on lower-guidance images of high-quality to learn prototypical features as a warm-up of learning higher-guidance images that might be weak on diversity or quality. Extensive experiments showcase a gain of 2.7% and 2.1% in OOD and ID macro-accuracy when applying DisCL to iWildCam dataset. On ImageNet-LT, DisCL improves the base modelâ€™s tail-class accuracy from 4.4% to 23.64% and leads to a 4.02% improvement in all-class accuracy. </p>
<blockquote>
<p>åœ¨å®è·µä¸­ï¼Œä½è´¨é‡æˆ–ç¨€ç¼ºæ•°æ®ä¸ºè®­ç»ƒæ·±åº¦ç¥ç»ç½‘ç»œå¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ã€‚è™½ç„¶ç»å…¸çš„æ•°æ®å¢å¼ºæ— æ³•æä¾›éå¸¸ä¸åŒçš„æ–°æ•°æ®ï¼Œä½†æ‰©æ•£æ¨¡å‹é€šè¿‡æ–‡æœ¬æŒ‡å¯¼æç¤ºç”Ÿæˆé«˜è´¨é‡å’Œå¤šæ ·åŒ–çš„åˆæˆæ•°æ®ï¼Œä»è€Œæ‰“å¼€äº†æ„å»ºè‡ªæˆ‘è¿›åŒ–çš„AIçš„æ–°å¤§é—¨ã€‚ç„¶è€Œï¼Œä»…æ–‡æœ¬çš„æŒ‡å¯¼æ— æ³•æ§åˆ¶åˆæˆå›¾åƒä¸åŸå§‹å›¾åƒçš„ç›¸ä¼¼æ€§ï¼Œè¿™ä¼šå¯¼è‡´å¯¹æ¨¡å‹æ€§èƒ½æœ‰å®³çš„åˆ†å¸ƒå¤–æ•°æ®ã€‚ä¸ºäº†å…‹æœè¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬ç ”ç©¶äº†å›¾åƒæŒ‡å¯¼æ¥å®ç°åˆæˆå›¾åƒå’ŒçœŸå®å›¾åƒä¹‹é—´çš„å…‰è°±æ’å€¼ã€‚ä½¿ç”¨æ›´å¼ºçš„å›¾åƒæŒ‡å¯¼ï¼Œç”Ÿæˆçš„å›¾åƒä¸è®­ç»ƒæ•°æ®ç›¸ä¼¼ï¼Œä½†éš¾ä»¥å­¦ä¹ ã€‚è€Œä½¿ç”¨è¾ƒå¼±çš„å›¾åƒæŒ‡å¯¼æ—¶ï¼Œåˆæˆå›¾åƒå¯¹æ¨¡å‹è€Œè¨€æ›´å®¹æ˜“ï¼Œä½†ä¸åŸå§‹æ•°æ®çš„åˆ†å¸ƒå·®è·æ›´å¤§ã€‚ç”Ÿæˆçš„å…¨æ•°æ®å…‰è°±ä½¿æˆ‘ä»¬èƒ½å¤Ÿæ„å»ºä¸€ç§æ–°å‹çš„â€œæ‰©æ•£è¯¾ç¨‹ï¼ˆDisCLï¼‰â€ã€‚DisCLé’ˆå¯¹æ¯ä¸ªè®­ç»ƒé˜¶æ®µè°ƒæ•´å›¾åƒåˆæˆçš„å›¾åƒæŒ‡å¯¼çº§åˆ«ï¼šå®ƒè¯†åˆ«å¹¶å…³æ³¨æ¨¡å‹çš„å›°éš¾æ ·æœ¬ï¼Œå¹¶è¯„ä¼°åˆæˆå›¾åƒæœ€æœ‰æ•ˆçš„æŒ‡å¯¼çº§åˆ«ï¼Œä»¥æé«˜å›°éš¾æ•°æ®çš„å­¦ä¹ æ•ˆæœã€‚æˆ‘ä»¬å°†DisCLåº”ç”¨äºä¸¤ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼šé•¿å°¾ï¼ˆLTï¼‰åˆ†ç±»å’Œä»ä½è´¨é‡æ•°æ®ä¸­å­¦ä¹ ã€‚å®ƒä¸“æ³¨äºé«˜è´¨é‡ä½†æŒ‡å¯¼è¾ƒå°‘çš„å›¾åƒä½œä¸ºå­¦ä¹ é«˜æŒ‡å¯¼ä½†å¯èƒ½åœ¨å¤šæ ·æ€§æˆ–è´¨é‡ä¸Šè¾ƒå¼±çš„å›¾åƒçš„çƒ­èº«ã€‚åœ¨iWildCamæ•°æ®é›†ä¸Šåº”ç”¨DisCLçš„å®éªŒæ˜¾ç¤ºï¼Œåœ¨OODå’ŒIDå®å‡†ç¡®ç‡ä¸Šåˆ†åˆ«æé«˜äº†2.7%å’Œ2.1%ã€‚åœ¨ImageNet-LTä¸Šï¼ŒDisCLå°†åŸºç¡€æ¨¡å‹çš„å°¾éƒ¨ç±»åˆ«å‡†ç¡®ç‡ä»4.4%æé«˜åˆ°23.64%ï¼Œå¹¶åœ¨æ‰€æœ‰ç±»åˆ«å‡†ç¡®ç‡ä¸Šå®ç°äº†4.02%çš„æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.13674v3">PDF</a> Accepted in ICCV2025. 22 pages, including references and appendix.   Code is available at <a target="_blank" rel="noopener" href="http://github.com/tianyi-lab/DisCL">http://github.com/tianyi-lab/DisCL</a></p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹é€šè¿‡æ–‡æœ¬å¼•å¯¼ç”Ÿæˆåˆæˆæ•°æ®ï¼Œä¸ºæ„å»ºè‡ªæˆ‘è¿›åŒ–çš„AIæ‰“å¼€äº†æ–°çš„å¤§é—¨ã€‚ç„¶è€Œï¼Œæ–‡æœ¬å¼•å¯¼æ— æ³•æ§åˆ¶åˆæˆå›¾åƒä¸åŸå§‹å›¾åƒçš„æ¥è¿‘ç¨‹åº¦ï¼Œå¯¼è‡´æ¨¡å‹æ€§èƒ½å—åˆ°æŸå®³ã€‚æœ¬ç ”ç©¶é€šè¿‡å›¾åƒå¼•å¯¼æ¥å…‹æœè¿™ä¸€å±€é™ï¼Œå®ç°äº†åˆæˆå›¾åƒå’ŒçœŸå®å›¾åƒä¹‹é—´çš„é¢‘è°±æ’å€¼ã€‚é€šè¿‡è°ƒæ•´å›¾åƒå¼•å¯¼æ°´å¹³ï¼Œç”Ÿæˆçš„å›¾åƒæ—¢ä¸è®­ç»ƒæ•°æ®ç›¸ä¼¼ï¼Œåˆå…·æœ‰ä¸€å®šçš„å­¦ä¹ éš¾åº¦ã€‚æœ¬ç ”ç©¶æ„å»ºäº†ä¸€ç§æ–°å‹çš„â€œæ‰©æ•£è¯¾ç¨‹ï¼ˆDisCLï¼‰â€ï¼Œè¯¥è¯¾ç¨‹åœ¨æ¯ä¸ªè®­ç»ƒé˜¶æ®µè°ƒæ•´å›¾åƒåˆæˆçš„å›¾åƒå¼•å¯¼æ°´å¹³ï¼Œè¯†åˆ«å¹¶ä¸“æ³¨äºæ¨¡å‹çš„å›°éš¾æ ·æœ¬ï¼Œå¹¶è¯„ä¼°åˆæˆå›¾åƒçš„æœ€æœ‰æ•ˆå¼•å¯¼æ°´å¹³ï¼Œä»¥æé«˜å›°éš¾æ•°æ®çš„å­¦ä¹ æ•ˆæœã€‚åœ¨é•¿å°¾åˆ†ç±»å’Œå­¦ä¹ ä½è´¨é‡æ•°æ®ç­‰ä»»åŠ¡ä¸­ï¼ŒDisCLå–å¾—äº†æ˜¾è‘—çš„æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹é€šè¿‡æ–‡æœ¬ç”Ÿæˆé«˜è´¨é‡å’Œå¤šæ ·åŒ–çš„åˆæˆæ•°æ®ï¼Œä¸ºæ„å»ºè‡ªæˆ‘è¿›åŒ–çš„AIæä¾›äº†æ–°çš„é€”å¾„ã€‚</li>
<li>æ–‡æœ¬å¼•å¯¼åœ¨åˆæˆå›¾åƒæ—¶å­˜åœ¨å±€é™æ€§ï¼Œæ— æ³•æ§åˆ¶åˆæˆå›¾åƒä¸åŸå§‹å›¾åƒçš„æ¥è¿‘ç¨‹åº¦ã€‚</li>
<li>å›¾åƒå¼•å¯¼å…‹æœäº†æ–‡æœ¬å¼•å¯¼çš„å±€é™ï¼Œå®ç°äº†åˆæˆå›¾åƒå’ŒçœŸå®å›¾åƒä¹‹é—´çš„é¢‘è°±æ’å€¼ã€‚</li>
<li>é€šè¿‡è°ƒæ•´å›¾åƒå¼•å¯¼æ°´å¹³ï¼Œå¯ä»¥åœ¨ä¿è¯å›¾åƒç›¸ä¼¼æ€§çš„åŒæ—¶å¢åŠ å­¦ä¹ éš¾åº¦ã€‚</li>
<li>DisCLæ–¹æ³•æ ¹æ®æ¯ä¸ªè®­ç»ƒé˜¶æ®µè°ƒæ•´å›¾åƒåˆæˆçš„å›¾åƒå¼•å¯¼æ°´å¹³ï¼Œä»¥æé«˜æ¨¡å‹å¯¹å›°éš¾æ•°æ®çš„å­¦ä¹ æ•ˆæœã€‚</li>
<li>DisCLåœ¨é•¿å°¾åˆ†ç±»ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ˜¾è‘—çš„æ•ˆæœï¼Œç‰¹åˆ«æ˜¯åœ¨ä½è´¨é‡æ•°æ®çš„å¤„ç†æ–¹é¢ã€‚</li>
<li>åœ¨iWildCamæ•°æ®é›†å’ŒImageNet-LTä»»åŠ¡ä¸Šï¼ŒDisCLå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.13674">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8854f3b56859d2782b135c3c20270905" align="middle">
<img src="https://picx.zhimg.com/v2-743d50417ccc2f8863f084c5c28fcc65" align="middle">
<img src="https://picx.zhimg.com/v2-7a58d1b1ed9c63298ccdbe69523a653e" align="middle">
<img src="https://picx.zhimg.com/v2-481b4ed886f3121529a3cc661e2e4c0b" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Sample-what-you-cant-compress"><a href="#Sample-what-you-cant-compress" class="headerlink" title="Sample what you cant compress"></a>Sample what you cant compress</h2><p><strong>Authors:Vighnesh Birodkar, Gabriel Barcik, James Lyon, Sergey Ioffe, David Minnen, Joshua V. Dillon</strong></p>
<p>For learned image representations, basic autoencoders often produce blurry results. Reconstruction quality can be improved by incorporating additional penalties such as adversarial (GAN) and perceptual losses. Arguably, these approaches lack a principled interpretation. Concurrently, in generative settings diffusion has demonstrated a remarkable ability to create crisp, high quality results and has solid theoretical underpinnings (from variational inference to direct study as the Fisher Divergence). Our work combines autoencoder representation learning with diffusion and is, to our knowledge, the first to demonstrate jointly learning a continuous encoder and decoder under a diffusion-based loss and showing that it can lead to higher compression and better generation. We demonstrate that this approach yields better reconstruction quality as compared to GAN-based autoencoders while being easier to tune. We also show that the resulting representation is easier to model with a latent diffusion model as compared to the representation obtained from a state-of-the-art GAN-based loss. Since our decoder is stochastic, it can generate details not encoded in the otherwise deterministic latent representation; we therefore name our approach â€œSample what you canâ€™t compressâ€, or SWYCC for short. </p>
<blockquote>
<p>å¯¹äºå­¦ä¹ åˆ°çš„å›¾åƒè¡¨ç¤ºï¼ŒåŸºç¡€è‡ªç¼–ç å™¨é€šå¸¸ä¼šäº§ç”Ÿæ¨¡ç³Šçš„ç»“æœã€‚é€šè¿‡å¼•å…¥å¯¹æŠ—æ€§ï¼ˆGANï¼‰å’Œæ„ŸçŸ¥æŸå¤±ç­‰é¢å¤–çš„æƒ©ç½šï¼Œå¯ä»¥æ”¹å–„é‡å»ºè´¨é‡ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•çš„è§£é‡ŠåŸåˆ™æ€§è¾ƒå·®ã€‚åŒæ—¶ï¼Œåœ¨ç”Ÿæˆç¯å¢ƒä¸­ï¼Œæ‰©æ•£ç°è±¡è¡¨ç°å‡ºäº†äº§ç”Ÿæ¸…æ™°ã€é«˜è´¨é‡ç»“æœçš„æ˜¾è‘—èƒ½åŠ›ï¼Œå¹¶å…·æœ‰åšå®çš„ç†è®ºåŸºç¡€ï¼ˆä»å˜åˆ†æ¨ç†åˆ°ç›´æ¥ç ”ç©¶è´¹èˆå°”æ•£åº¦ï¼‰ã€‚æˆ‘ä»¬çš„å·¥ä½œå°†è‡ªç¼–ç å™¨è¡¨ç¤ºå­¦ä¹ ä¸æ‰©æ•£ç›¸ç»“åˆï¼Œæ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œé¦–æ¬¡å±•ç¤ºäº†åœ¨æ‰©æ•£æŸå¤±ä¸‹è”åˆå­¦ä¹ è¿ç»­ç¼–ç å™¨å’Œè§£ç å™¨ï¼Œå¹¶è¡¨æ˜è¿™å¯ä»¥æé«˜å‹ç¼©ç‡å’Œç”Ÿæˆè´¨é‡ã€‚æˆ‘ä»¬è¯æ˜ï¼Œè¿™ç§æ–¹æ³•ä¸åŸºäºGANçš„è‡ªç¼–ç å™¨ç›¸æ¯”ï¼Œå…·æœ‰æ›´å¥½çš„é‡å»ºè´¨é‡ï¼Œè€Œä¸”æ›´å®¹æ˜“è°ƒæ•´ã€‚æˆ‘ä»¬è¿˜è¡¨æ˜ï¼Œä¸ä»åŸºäºGANçš„æŸå¤±ä¸­è·å¾—çš„çŠ¶æ€æœ€ä½³è¡¨ç¤ºç›¸æ¯”ï¼Œä½¿ç”¨æ½œåœ¨æ‰©æ•£æ¨¡å‹å¯¹ç”±æ­¤äº§ç”Ÿçš„è¡¨ç¤ºè¿›è¡Œå»ºæ¨¡æ›´ä¸ºå®¹æ˜“ã€‚ç”±äºæˆ‘ä»¬çš„è§£ç å™¨æ˜¯éšæœºçš„ï¼Œå®ƒå¯ä»¥ç”Ÿæˆæœªç¼–ç åœ¨ç¡®å®šæ€§æ½œåœ¨è¡¨ç¤ºä¸­çš„ç»†èŠ‚ï¼›å› æ­¤ï¼Œæˆ‘ä»¬å°†æˆ‘ä»¬çš„æ–¹æ³•å‘½åä¸ºâ€œå‹ç¼©æ‰€ä¸èƒ½é‡‡æ ·çš„ä¸œè¥¿â€ï¼Œç®€ç§°SWYCCã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.02529v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå­¦ä¹ åˆ°çš„å›¾åƒè¡¨ç¤ºï¼ŒåŸºæœ¬è‡ªç¼–ç å™¨é€šå¸¸ä¼šäº§ç”Ÿæ¨¡ç³Šçš„ç»“æœã€‚ä¸ºæé«˜é‡å»ºè´¨é‡ï¼Œå¯ä»¥å¼•å…¥å¯¹æŠ—æ€§ï¼ˆGANï¼‰å’Œæ„ŸçŸ¥æŸå¤±ç­‰é¢å¤–æƒ©ç½šã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•ç¼ºä¹åŸåˆ™æ€§çš„è§£é‡Šã€‚åŒæ—¶ï¼Œæ‰©æ•£åœ¨ç”Ÿæˆç¯å¢ƒä¸­è¡¨ç°å‡ºäº†æƒŠäººçš„åˆ›å»ºæ¸…æ™°ã€é«˜è´¨é‡ç»“æœçš„èƒ½åŠ›ï¼Œå¹¶æœ‰åšå®çš„ç†è®ºåŸºç¡€ï¼ˆä»å˜åˆ†æ¨æ–­åˆ°ç›´æ¥ç ”ç©¶è´¹èˆå°”æ•£åº¦ï¼‰ã€‚æˆ‘ä»¬çš„å·¥ä½œç»“åˆäº†è‡ªç¼–ç å™¨è¡¨ç¤ºå­¦ä¹ ä¸æ‰©æ•£ï¼Œæ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œæ˜¯é¦–æ¬¡å±•ç¤ºåœ¨æ‰©æ•£æŸå¤±ä¸‹è”åˆå­¦ä¹ è¿ç»­ç¼–ç å™¨å’Œè§£ç å™¨ï¼Œå¹¶è¡¨æ˜å®ƒå¯ä»¥å®ç°æ›´é«˜çš„å‹ç¼©ç‡å’Œæ›´å¥½çš„ç”Ÿæˆæ•ˆæœã€‚æˆ‘ä»¬è¯æ˜ï¼Œä¸å…¶ä»–åŸºäºGANçš„è‡ªç¼–ç å™¨ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å…·æœ‰æ›´å¥½çš„é‡å»ºè´¨é‡ï¼Œä¸”æ›´æ˜“äºè°ƒæ•´ã€‚æˆ‘ä»¬è¿˜è¡¨æ˜ï¼Œä¸æ¥è‡ªæœ€å…ˆè¿›çš„åŸºäºGANçš„æŸå¤±æ‰€è·å¾—çš„è¡¨ç¤ºç›¸æ¯”ï¼Œä½¿ç”¨æˆ‘ä»¬æ–¹æ³•å¾—åˆ°çš„è¡¨ç¤ºæ›´å®¹æ˜“ç”¨æ½œåœ¨æ‰©æ•£æ¨¡å‹å»ºæ¨¡ã€‚ç”±äºæˆ‘ä»¬çš„è§£ç å™¨æ˜¯éšæœºçš„ï¼Œå®ƒå¯ä»¥ç”Ÿæˆæœªç¼–ç åœ¨ç¡®å®šæ€§æ½œåœ¨è¡¨ç¤ºä¸­çš„ç»†èŠ‚ï¼›å› æ­¤ï¼Œæˆ‘ä»¬å°†æˆ‘ä»¬çš„æ–¹æ³•å‘½åä¸ºâ€œå‹ç¼©æ‰€ä¸èƒ½é‡‡æ ·çš„â€ï¼Œç®€ç§°SWYCCã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŸºæœ¬è‡ªç¼–ç å™¨ä¼šäº§ç”Ÿæ¨¡ç³Šç»“æœï¼Œä¸ºæé«˜é‡å»ºè´¨é‡éœ€å¼•å…¥é¢å¤–æƒ©ç½šã€‚</li>
<li>æ‰©æ•£åœ¨ç”Ÿæˆç¯å¢ƒä¸­èƒ½åˆ›å»ºæ¸…æ™°ã€é«˜è´¨é‡ç»“æœï¼Œå…·å¤‡åšå®çš„ç†è®ºåŸºç¡€ã€‚</li>
<li>é¦–æ¬¡å±•ç¤ºç»“åˆè‡ªç¼–ç å™¨è¡¨ç¤ºå­¦ä¹ ä¸æ‰©æ•£ï¼Œè”åˆå­¦ä¹ è¿ç»­ç¼–ç å™¨å’Œè§£ç å™¨ã€‚</li>
<li>æå‡ºçš„æ–¹æ³•å¯ä»¥å®ç°æ›´é«˜çš„å‹ç¼©ç‡å’Œæ›´å¥½çš„ç”Ÿæˆæ•ˆæœã€‚</li>
<li>ä¸åŸºäºGANçš„è‡ªç¼–ç å™¨ç›¸æ¯”ï¼Œæ­¤æ–¹æ³•å…·æœ‰æ›´å¥½çš„é‡å»ºè´¨é‡ä¸”æ›´æ˜“äºè°ƒæ•´ã€‚</li>
<li>ä½¿ç”¨æ­¤æ–¹æ³•å¾—åˆ°çš„è¡¨ç¤ºæ›´å®¹æ˜“ç”¨æ½œåœ¨æ‰©æ•£æ¨¡å‹å»ºæ¨¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.02529">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5fc36433fc8b9114ea71e74517dc89dd" align="middle">
<img src="https://picx.zhimg.com/v2-5f5a28c62329c1b2adc1a5bdc83cc55e" align="middle">
<img src="https://picx.zhimg.com/v2-f4248d8bca11c0e701e0166ea04a1d86" align="middle">
<img src="https://picx.zhimg.com/v2-c63d76be087e114b982575f4adf875c5" align="middle">
<img src="https://picx.zhimg.com/v2-1fa23f1db3490cdb1f2b981949d2e15e" align="middle">
<img src="https://picx.zhimg.com/v2-b9a868a26126a9ae710570aecec1860e" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Ctrl-Room-Controllable-Text-to-3D-Room-Meshes-Generation-with-Layout-Constraints"><a href="#Ctrl-Room-Controllable-Text-to-3D-Room-Meshes-Generation-with-Layout-Constraints" class="headerlink" title="Ctrl-Room: Controllable Text-to-3D Room Meshes Generation with Layout   Constraints"></a>Ctrl-Room: Controllable Text-to-3D Room Meshes Generation with Layout   Constraints</h2><p><strong>Authors:Chuan Fang, Yuan Dong, Kunming Luo, Xiaotao Hu, Rakesh Shrestha, Ping Tan</strong></p>
<p>Text-driven 3D indoor scene generation is useful for gaming, the film industry, and AR&#x2F;VR applications. However, existing methods cannot faithfully capture the room layout, nor do they allow flexible editing of individual objects in the room. To address these problems, we present Ctrl-Room, which can generate convincing 3D rooms with designer-style layouts and high-fidelity textures from just a text prompt. Moreover, Ctrl-Room enables versatile interactive editing operations such as resizing or moving individual furniture items. Our key insight is to separate the modeling of layouts and appearance. Our proposed method consists of two stages: a Layout Generation Stage and an Appearance Generation Stage. The Layout Generation Stage trains a text-conditional diffusion model to learn the layout distribution with our holistic scene code parameterization. Next, the Appearance Generation Stage employs a fine-tuned ControlNet to produce a vivid panoramic image of the room guided by the 3D scene layout and text prompt. We thus achieve a high-quality 3D room generation with convincing layouts and lively textures. Benefiting from the scene code parameterization, we can easily edit the generated room model through our mask-guided editing module, without expensive edit-specific training. Extensive experiments on the Structured3D dataset demonstrate that our method outperforms existing methods in producing more reasonable, view-consistent, and editable 3D rooms from natural language prompts. </p>
<blockquote>
<p>æ–‡æœ¬é©±åŠ¨çš„3Då®¤å†…åœºæ™¯ç”Ÿæˆåœ¨æ¸¸æˆã€ç”µå½±äº§ä¸šå’ŒAR&#x2F;VRåº”ç”¨ä¸­éå¸¸æœ‰ç”¨ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•æ— æ³•çœŸå®åœ°æ•æ‰æˆ¿é—´å¸ƒå±€ï¼Œä¹Ÿä¸å…è®¸å¯¹æˆ¿é—´ä¸­çš„å•ä¸ªç‰©ä½“è¿›è¡Œçµæ´»çš„ç¼–è¾‘ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Ctrl-Roomï¼Œå®ƒä»…é€šè¿‡æ–‡æœ¬æç¤ºå°±èƒ½ç”Ÿæˆå…·æœ‰è®¾è®¡å¸ˆé£æ ¼çš„å¸ƒå±€å’Œé«˜ä¿çœŸçº¹ç†çš„ä»¤äººä¿¡æœçš„3Dæˆ¿é—´ã€‚æ­¤å¤–ï¼ŒCtrl-Roomè¿˜æ”¯æŒå¤šæ ·åŒ–çš„äº¤äº’ç¼–è¾‘æ“ä½œï¼Œå¦‚è°ƒæ•´å¤§å°æˆ–ç§»åŠ¨å•ä¸ªå®¶å…·é¡¹ç›®ã€‚æˆ‘ä»¬çš„å…³é”®è§è§£æ˜¯å°†å¸ƒå±€å’Œå¤–è§‚çš„å»ºæ¨¡åˆ†å¼€ã€‚æˆ‘ä»¬æå‡ºçš„æ–¹æ³•åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šå¸ƒå±€ç”Ÿæˆé˜¶æ®µå’Œå¤–è§‚ç”Ÿæˆé˜¶æ®µã€‚å¸ƒå±€ç”Ÿæˆé˜¶æ®µè®­ç»ƒä¸€ä¸ªæ–‡æœ¬æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼Œä»¥é€šè¿‡æˆ‘ä»¬çš„æ•´ä½“åœºæ™¯ä»£ç å‚æ•°åŒ–æ¥å­¦ä¹ å¸ƒå±€åˆ†å¸ƒã€‚æ¥ä¸‹æ¥ï¼Œå¤–è§‚ç”Ÿæˆé˜¶æ®µé‡‡ç”¨ç»è¿‡å¾®è°ƒçš„æ§åˆ¶ç½‘ç»œï¼ˆControlNetï¼‰ï¼Œæ ¹æ®3Dåœºæ™¯å¸ƒå±€å’Œæ–‡æœ¬æç¤ºç”Ÿæˆç”ŸåŠ¨çš„å…¨æ™¯æˆ¿é—´å›¾åƒã€‚å› æ­¤ï¼Œæˆ‘ä»¬å®ç°äº†å…·æœ‰ä»¤äººä¿¡æœçš„å¸ƒå±€å’Œç”ŸåŠ¨çº¹ç†çš„é«˜è´¨é‡3Dæˆ¿é—´ç”Ÿæˆã€‚å¾—ç›Šäºåœºæ™¯ä»£ç å‚æ•°åŒ–ï¼Œæˆ‘ä»¬å¯ä»¥è½»æ¾é€šè¿‡æˆ‘ä»¬çš„é®ç½©å¼•å¯¼ç¼–è¾‘æ¨¡å—ç¼–è¾‘ç”Ÿæˆçš„æˆ¿é—´æ¨¡å‹ï¼Œè€Œæ— éœ€è¿›è¡Œæ˜‚è´µçš„ç‰¹å®šç¼–è¾‘è®­ç»ƒã€‚åœ¨Structured3Dæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ ¹æ®è‡ªç„¶è¯­è¨€æç¤ºç”Ÿæˆæ›´åˆç†ã€è§†è§’ä¸€è‡´å’Œå¯ç¼–è¾‘çš„3Dæˆ¿é—´æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03602v5">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºCtrl-Roomçš„æ–‡æœ¬é©±åŠ¨3Då®¤å†…åœºæ™¯ç”Ÿæˆæ–¹æ³•ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿä»…é€šè¿‡æ–‡æœ¬æç¤ºç”Ÿæˆå…·æœ‰è®¾è®¡å¸ˆé£æ ¼çš„å¸ƒå±€å’Œé«˜ä¿çœŸçº¹ç†çš„é€¼çœŸ3Dæˆ¿é—´ï¼Œå¹¶å®ç°äº†çµæ´»çš„äº¤äº’å¼ç¼–è¾‘æ“ä½œï¼Œå¦‚è°ƒæ•´å®¶å…·å¤§å°æˆ–ç§»åŠ¨å®¶å…·ä½ç½®ã€‚å…¶å…³é”®åœ¨äºå°†å¸ƒå±€å’Œå¤–è§‚å»ºæ¨¡åˆ†ç¦»ï¼Œé‡‡ç”¨ä¸¤ä¸ªé˜¶æ®µçš„æ–¹æ³•ï¼šå¸ƒå±€ç”Ÿæˆé˜¶æ®µå’Œå¤–è§‚ç”Ÿæˆé˜¶æ®µã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Ctrl-Roomæ˜¯ä¸€ç§æ–‡æœ¬é©±åŠ¨çš„3Då®¤å†…åœºæ™¯ç”Ÿæˆæ–¹æ³•ï¼Œé€‚ç”¨äºæ¸¸æˆã€ç”µå½±ã€AR&#x2F;VRç­‰é¢†åŸŸã€‚</li>
<li>ç°æœ‰æ–¹æ³•æ— æ³•çœŸå®åœ°æ•æ‰æˆ¿é—´å¸ƒå±€ï¼ŒCtrl-Roomé€šè¿‡æ–‡æœ¬æç¤ºç”Ÿæˆå…·æœ‰è®¾è®¡å¸ˆé£æ ¼çš„å¸ƒå±€ã€‚</li>
<li>Ctrl-Roomå®ç°äº†çµæ´»çš„äº¤äº’å¼ç¼–è¾‘æ“ä½œï¼Œå¦‚ç§»åŠ¨å’Œè°ƒæ•´å®¶å…·å¤§å°ã€‚</li>
<li>è¯¥æ–¹æ³•å°†å¸ƒå±€å’Œå¤–è§‚å»ºæ¨¡åˆ†ç¦»ï¼Œåˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šå¸ƒå±€ç”Ÿæˆé˜¶æ®µå’Œå¤–è§‚ç”Ÿæˆé˜¶æ®µã€‚</li>
<li>é‡‡ç”¨å…¨æ™¯å›¾åƒç”ŸæˆæŠ€æœ¯ï¼Œç»“åˆ3Dåœºæ™¯å¸ƒå±€å’Œæ–‡æœ¬æç¤ºï¼Œäº§ç”Ÿç”ŸåŠ¨çš„å…¨æ™¯å›¾åƒã€‚</li>
<li>é€šè¿‡åœºæ™¯ä»£ç å‚æ•°åŒ–ï¼Œå®ç°äº†è½»æ¾ç¼–è¾‘ç”Ÿæˆçš„æˆ¿é—´æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2310.03602">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bea9926a4e2f8c5e930ca82883f46caa" align="middle">
<img src="https://picx.zhimg.com/v2-43759e3ff9997de23baff69bab48b97f" align="middle">
<img src="https://picx.zhimg.com/v2-30fecab3f3c587e5931f3053348d9bf3" align="middle">
<img src="https://picx.zhimg.com/v2-7c6ee6b04a4d76065268f0d847a5f61e" align="middle">
<img src="https://picx.zhimg.com/v2-a9a70e47864e84ca5d635252207a8354" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-28/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-28/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-28/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-e5b81a0641aae81e9654bbc7f602975f" class="responsive-img" alt="ç‰™é½¿ä¿®å¤">
                        
                        <span class="card-title">ç‰™é½¿ä¿®å¤</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            ç‰™é½¿ä¿®å¤ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-28  An Interpretable Single-Index Mixed-Effects Model for Non-Gaussian   National Survey Data
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-28
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/" class="post-category">
                                    ç‰™é½¿ä¿®å¤
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                        <span class="chip bg-color">ç‰™é½¿ä¿®å¤</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-28/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-cdc7ffdf5f932180a83612b9c0ec803a" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-28  Integrating Object Interaction Self-Attention and GAN-Based Debiasing   for Visual Question Answering
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-28
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32306k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
