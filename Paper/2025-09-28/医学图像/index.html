<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-28  Modelling the effect of stellar metallicity on the XUV evolution of   low-mass stars and its impact on exoplanet atmospheres/habitability">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-726284912b86b25d60f795829f872ae7')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-28
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-19
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    84 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-09-28-æ›´æ–°"><a href="#2025-09-28-æ›´æ–°" class="headerlink" title="2025-09-28 æ›´æ–°"></a>2025-09-28 æ›´æ–°</h1><h2 id="Modelling-the-effect-of-stellar-metallicity-on-the-XUV-evolution-of-low-mass-stars-and-its-impact-on-exoplanet-atmospheres-habitability"><a href="#Modelling-the-effect-of-stellar-metallicity-on-the-XUV-evolution-of-low-mass-stars-and-its-impact-on-exoplanet-atmospheres-habitability" class="headerlink" title="Modelling the effect of stellar metallicity on the XUV evolution of   low-mass stars and its impact on exoplanet atmospheres&#x2F;habitability"></a>Modelling the effect of stellar metallicity on the XUV evolution of   low-mass stars and its impact on exoplanet atmospheres&#x2F;habitability</h2><p><strong>Authors:Victor See, Charlotte Fairman, Louis Amard, Oliver Hall</strong></p>
<p>Understanding how exoplanet atmospheres evolve is a key question in the context of habitability. One key process governing this evolution is atmospheric evaporation by stellar X-ray and EUV emission (collectively, XUV). As such, the evolution of exoplanet atmospheres is closely tied to the evolution of the host starâ€™s magnetic activity. Many studies have modelled the combined evolution of exoplanet atmospheres and their host stars. However, to date, the impact of the host starâ€™s metallicity on stellar activity&#x2F;exoplanet atmosphere evolution has not been explored. In this work, we investigate how stellar metallicity affects the rotation and activity evolution of solar-like stars as well as the corresponding exoplanet atmospheric evolution. We reconfirm previous results that metal-rich stars spin down more rapidly than metal-poor stars. We also find that the XUV flux that an exoplanet in the habitable zone of its host star receives is larger when the host star is more metal-rich. As such, the atmospheres of exoplanets in the habitable zones of metal-rich stars are evaporated more rapidly than exoplanets in the habitable zones of metal-poor stars. Lastly, we find that the atmospheric evolution is most sensitive to the host star metallicity when the host star has a higher mass. In the highest mass solar-stars, the metallicity can have a larger influence on the atmospheric evolution than the initial rotation period of the star. </p>
<blockquote>
<p>åœ¨å®œå±…æ€§çš„èƒŒæ™¯ä¸‹ï¼Œäº†è§£å¤–è¡Œæ˜Ÿå¤§æ°”å¦‚ä½•æ¼”åŒ–æ˜¯ä¸€ä¸ªå…³é”®é—®é¢˜ã€‚æ§åˆ¶è¿™ç§æ¼”åŒ–çš„å…³é”®è¿‡ç¨‹ä¹‹ä¸€æ˜¯æ’æ˜ŸXå°„çº¿å’Œæç´«å¤–çº¿ï¼ˆç»Ÿç§°ä¸ºXUVï¼‰è¾å°„å¼•èµ·çš„å¤§æ°”è’¸å‘ã€‚å› æ­¤ï¼Œå¤–è¡Œæ˜Ÿå¤§æ°”çš„æ¼”åŒ–ä¸å®¿ä¸»æ˜Ÿç£æ´»åŠ¨çš„æ¼”åŒ–å¯†åˆ‡ç›¸å…³ã€‚è®¸å¤šç ”ç©¶å·²ç»å»ºç«‹äº†å¤–è¡Œæ˜Ÿå¤§æ°”å’Œå…¶å®¿ä¸»æ˜Ÿçš„å…±åŒæ¼”åŒ–æ¨¡å‹ã€‚ç„¶è€Œï¼Œè¿„ä»Šä¸ºæ­¢ï¼Œå®¿ä¸»æ˜Ÿçš„é‡‘å±ä¸°åº¦å¯¹æ’æ˜Ÿæ´»åŠ¨&#x2F;å¤–è¡Œæ˜Ÿå¤§æ°”æ¼”åŒ–çš„å½±å“å°šæœªè¢«æ¢ç´¢ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†æ’æ˜Ÿé‡‘å±ä¸°åº¦å¯¹å¤ªé˜³å‹æ’æ˜Ÿçš„æ—‹è½¬å’Œæ´»åŠ¨æ¼”åŒ–çš„å½±å“ï¼Œä»¥åŠç›¸åº”çš„å¤–è¡Œæ˜Ÿå¤§æ°”æ¼”åŒ–ã€‚æˆ‘ä»¬å†æ¬¡è¯å®äº†ä¹‹å‰çš„ç»“è®ºï¼Œå³å¯Œé‡‘å±æ’æ˜Ÿçš„è‡ªæ—‹è¡°å‡é€Ÿåº¦æ¯”è´«é‡‘å±æ’æ˜Ÿæ›´å¿«ã€‚æˆ‘ä»¬è¿˜å‘ç°ï¼Œå½“å®¿ä¸»æ˜Ÿä¸ºå¯Œé‡‘å±æ—¶ï¼Œå¤„äºå®œå±…å¸¦çš„å¤–è¡Œæ˜Ÿæ¥æ”¶åˆ°çš„XUVæµé‡æ›´å¤§ã€‚å› æ­¤ï¼Œå¤„äºå¯Œé‡‘å±æ˜Ÿå®œå±…åŒºçš„å¤–è¡Œæ˜Ÿå¤§æ°”æ¯”å¤„äºè´«é‡‘å±æ˜Ÿå®œå±…åŒºçš„å¤–è¡Œæ˜Ÿå¤§æ°”è’¸å‘å¾—æ›´å¿«ã€‚æœ€åï¼Œæˆ‘ä»¬å‘ç°å½“å®¿ä¸»æ˜Ÿè´¨é‡è¾ƒé«˜æ—¶ï¼Œå¤§æ°”æ¼”åŒ–å¯¹å®¿ä¸»æ˜Ÿé‡‘å±ä¸°åº¦çš„æ•æ„Ÿæ€§æœ€å¼ºã€‚åœ¨è´¨é‡æœ€é«˜çš„å¤ªé˜³å‹æ’æ˜Ÿä¸­ï¼Œé‡‘å±ä¸°åº¦å¯¹å¤§æ°”æ¼”åŒ–çš„å½±å“å¯èƒ½å¤§äºæ’æ˜Ÿçš„åˆå§‹è‡ªè½¬å‘¨æœŸã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.21276v1">PDF</a> 12 pages, 7 figures, accepted for publication in MNRAS</p>
<p><strong>Summary</strong><br>     æ­¤ç ”ç©¶æ¢è®¨äº†å®¿ä¸»æ’æ˜Ÿé‡‘å±é‡å¯¹å¤ªé˜³ç³»ç±»ä¼¼æ’æ˜Ÿçš„è‡ªè½¬å’Œæ´»åŠ¨æ¼”åŒ–çš„å½±å“ï¼Œä»¥åŠç›¸åº”å¤–è¡Œæ˜Ÿå¤§æ°”æ¼”åŒ–çš„å˜åŒ–ã€‚ç ”ç©¶å‘ç°é‡‘å±å«é‡è¾ƒé«˜çš„æ’æ˜Ÿè‡ªè½¬å‡é€Ÿæ›´å¿«ï¼Œé‡‘å±å«é‡è¾ƒé«˜çš„å®¿ä¸»æ˜Ÿçš„å¤–è¡Œæ˜Ÿåœ¨å®œå±…å¸¦æ‰€æ¥æ”¶åˆ°çš„XUVè¾å°„è¾ƒå¤§ï¼Œå¯¼è‡´å¤§æ°”è’¸å‘æ›´å¿«ã€‚æ­¤å¤–ï¼Œå¯¹äºè´¨é‡è¾ƒé«˜çš„å®¿ä¸»æ’æ˜Ÿï¼Œå…¶é‡‘å±é‡å¯¹å¤§æ°”æ¼”åŒ–çš„å½±å“å¤§äºåˆå§‹è‡ªè½¬å‘¨æœŸçš„å½±å“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å®¿ä¸»æ’æ˜Ÿçš„é‡‘å±é‡å½±å“å¤ªé˜³ç±»ä¼¼æ’æ˜Ÿçš„è‡ªè½¬å’Œæ´»åŠ¨æ¼”åŒ–ã€‚</li>
<li>é‡‘å±å«é‡è¾ƒé«˜çš„æ’æ˜Ÿè‡ªè½¬å‡é€Ÿæ›´å¿«ã€‚</li>
<li>é‡‘å±å«é‡è¾ƒé«˜çš„å®¿ä¸»æ˜Ÿçš„å¤–è¡Œæ˜Ÿåœ¨å®œå±…å¸¦æ‰€æ¥æ”¶åˆ°çš„XUVè¾å°„è¾ƒå¤§ã€‚</li>
<li>å¤–è¡Œæ˜Ÿåœ¨å®œå±…å¸¦çš„é‡‘å±å«é‡è¾ƒé«˜çš„å®¿ä¸»æ˜Ÿçš„å¤§æ°”è’¸å‘æ›´å¿«ã€‚</li>
<li>å®¿ä¸»æ˜Ÿè´¨é‡è¾ƒé«˜æ—¶ï¼Œå…¶é‡‘å±é‡å¯¹å¤§æ°”æ¼”åŒ–çš„å½±å“æ›´å¤§ã€‚</li>
<li>åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œé‡‘å±é‡å¯¹å¤§æ°”æ¼”åŒ–çš„å½±å“å¯èƒ½è¶…è¿‡å®¿ä¸»æ˜Ÿçš„åˆå§‹è‡ªè½¬å‘¨æœŸçš„å½±å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.21276">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-70ffa8c8ae9ba74860905fca49c77ec4" align="middle">
<img src="https://picx.zhimg.com/v2-046869baeadf5a0dda84fb217e82c819" align="middle">
<img src="https://picx.zhimg.com/v2-64ec4b4df1f67ebbba0ff54f99bcdfa8" align="middle">
<img src="https://picx.zhimg.com/v2-0e9cc34ec2059a9b0388c90619d118b7" align="middle">
<img src="https://picx.zhimg.com/v2-7d2e654b224c4feaf89f44ffa7acf05c" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="A-sample-of-3403-galaxy-clusters-identified-in-XMM-Newton-X-ray-images"><a href="#A-sample-of-3403-galaxy-clusters-identified-in-XMM-Newton-X-ray-images" class="headerlink" title="A sample of 3403 galaxy clusters identified in XMM-Newton X-ray images"></a>A sample of 3403 galaxy clusters identified in XMM-Newton X-ray images</h2><p><strong>Authors:Z. S. Yuan, Z. L. Wen, W. Xu, J. L. Han</strong></p>
<p>Currently, the number of galaxy clusters identified using galaxy data has far exceeded the number derived from intracluster medium data. In this study, we used positional information from large optical cluster catalogues to search for previously unrecognized X-ray galaxy clusters in archival XMM-Newton data. We successfully identified 1490 galaxy clusters in X-ray images for the first time. By incorporating 1913 previously known X-ray clusters, we constructed a sample of 3403 galaxy clusters observed by XMM-Newton. Our cluster mass estimates show broad consistency with previous measurements. Comparative analyses between the known and newly identified subsamples revealed that new X-ray clusters exhibit systematically higher redshifts, lower masses, and smaller X-ray-to-optical mass ratios, but show no systematic differences in dynamical properties. The newly identified X-ray clusters are a valuable addition to previous X-ray samples and are important for future statistical studies. </p>
<blockquote>
<p>å½“å‰ï¼Œä½¿ç”¨æ˜Ÿç³»æ•°æ®è¯†åˆ«å‡ºçš„æ˜Ÿç³»å›¢æ•°é‡è¿œè¿œè¶…è¿‡ä»æ˜Ÿç³»é—´ä»‹è´¨æ•°æ®ä¸­å¾—å‡ºçš„æ•°é‡ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†å¤§å‹å…‰å­¦æ˜Ÿå›¢ç›®å½•ä¸­çš„ä½ç½®ä¿¡æ¯ï¼Œåœ¨XMM-Newtonçš„å­˜æ¡£æ•°æ®ä¸­æœç´¢ä¹‹å‰æœªè¯†åˆ«çš„Xå°„çº¿æ˜Ÿç³»å›¢ã€‚æˆ‘ä»¬é¦–æ¬¡åœ¨Xå°„çº¿å›¾åƒä¸­æˆåŠŸè¯†åˆ«å‡º1490ä¸ªæ˜Ÿç³»å›¢ã€‚é€šè¿‡çº³å…¥1913ä¸ªå…ˆå‰å·²çŸ¥çš„Xå°„çº¿æ˜Ÿå›¢ï¼Œæˆ‘ä»¬æ„å»ºäº†ç”±XMM-Newtonè§‚æµ‹çš„3403ä¸ªæ˜Ÿç³»å›¢æ ·æœ¬ã€‚æˆ‘ä»¬å¯¹æ˜Ÿå›¢è´¨é‡çš„ä¼°è®¡ä¸ä¹‹å‰çš„æµ‹é‡ç»“æœå¤§è‡´ä¸€è‡´ã€‚å·²çŸ¥å’Œæ–°è¯†åˆ«çš„äºšæ ·æœ¬ä¹‹é—´çš„å¯¹æ¯”åˆ†æè¡¨æ˜ï¼Œæ–°çš„Xå°„çº¿æ˜Ÿå›¢å…·æœ‰æ›´é«˜çš„çº¢ç§»ã€è¾ƒä½çš„è´¨é‡å’Œè¾ƒä½çš„Xå°„çº¿ä¸å…‰å­¦è´¨é‡æ¯”ï¼Œä½†åœ¨åŠ¨åŠ›å­¦ç‰¹æ€§ä¸Šæ²¡æœ‰ç³»ç»Ÿæ€§å·®å¼‚ã€‚æ–°è¯†åˆ«çš„Xå°„çº¿æ˜Ÿå›¢æ˜¯å¯¹ä¹‹å‰Xå°„çº¿æ ·æœ¬çš„å®è´µè¡¥å……ï¼Œå¯¹æœªæ¥ç»Ÿè®¡ç ”ç©¶å…·æœ‰é‡è¦æ„ä¹‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.21031v1">PDF</a> 10 pages, 3 figures, 2 tables. Accepted for publication in MNRAS</p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶åˆ©ç”¨å¤§å‹å…‰å­¦é›†ç¾¤ç›®å½•çš„ä½ç½®ä¿¡æ¯ï¼Œåœ¨XMM-Newtonå­˜æ¡£æ•°æ®ä¸­æœç´¢ä¹‹å‰æœªè¯†åˆ«çš„Xå°„çº¿æ˜Ÿç³»å›¢ï¼ŒæˆåŠŸé¦–æ¬¡è¯†åˆ«å‡º1490ä¸ªæ˜Ÿç³»å›¢ã€‚ç»“åˆä¹‹å‰å·²çŸ¥çš„1913ä¸ªXå°„çº¿é›†ç¾¤ï¼Œæ„å»ºäº†XMM-Newtonè§‚æµ‹çš„3403ä¸ªæ˜Ÿç³»å›¢æ ·æœ¬ã€‚æ–°è¯†åˆ«çš„Xå°„çº¿é›†ç¾¤å¯¹ä¹‹å‰çš„Xå°„çº¿æ ·æœ¬æœ‰ä»·å€¼è¡¥å……ï¼Œå¯¹å°†æ¥çš„ç»Ÿè®¡ç ”ç©¶å…·æœ‰é‡è¦æ„ä¹‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶ä½¿ç”¨å…‰å­¦é›†ç¾¤ç›®å½•çš„ä½ç½®ä¿¡æ¯åœ¨XMM-Newtonå­˜æ¡£æ•°æ®ä¸­æˆåŠŸè¯†åˆ«å‡º1490ä¸ªæ–°çš„Xå°„çº¿æ˜Ÿç³»å›¢ã€‚</li>
<li>ç»“åˆå·²çŸ¥çš„1913ä¸ªXå°„çº¿é›†ç¾¤ï¼Œæ„å»ºäº†åŒ…å«3403ä¸ªæ˜Ÿç³»å›¢çš„æ ·æœ¬ã€‚</li>
<li>æ–°è¯†åˆ«çš„Xå°„çº¿é›†ç¾¤åœ¨çº¢ç§»ã€è´¨é‡ã€Xå°„çº¿ä¸å…‰å­¦è´¨é‡æ¯”ç­‰æ–¹é¢å…·æœ‰ç‰¹å¾ã€‚</li>
<li>æ–°è¯†åˆ«çš„é›†ç¾¤ä¸å·²çŸ¥é›†ç¾¤åœ¨åŠ¨åŠ›å­¦ç‰¹æ€§ä¸Šæ²¡æœ‰ç³»ç»Ÿæ€§å·®å¼‚ã€‚</li>
<li>æ–°è¯†åˆ«çš„Xå°„çº¿é›†ç¾¤å¯¹ç°æœ‰çš„Xå°„çº¿æ ·æœ¬æœ‰ä»·å€¼è¡¥å……ã€‚</li>
<li>è¿™äº›æ–°è¯†åˆ«çš„é›†ç¾¤å¯¹äºæœªæ¥çš„ç»Ÿè®¡ç ”ç©¶å…·æœ‰é‡è¦æ„ä¹‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.21031">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a8756cbebede5b9401f1ab5784209db9" align="middle">
<img src="https://picx.zhimg.com/v2-3562ccb2e821402f02bc6af09aa2bdb8" align="middle">
<img src="https://picx.zhimg.com/v2-5f0d4d10b9c963234cf9f101e339e495" align="middle">
<img src="https://picx.zhimg.com/v2-b9ebaf45afee4925a22059a39e523cd5" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="4D-Computational-Ultrasound-Imaging-of-Carotid-Artery-Flow"><a href="#4D-Computational-Ultrasound-Imaging-of-Carotid-Artery-Flow" class="headerlink" title="4D Computational Ultrasound Imaging of Carotid Artery Flow"></a>4D Computational Ultrasound Imaging of Carotid Artery Flow</h2><p><strong>Authors:Yuyang Hu, Michael Brown, Didem Dogan, MahÃ© Bulot, Maxime Cheppe, Guillaume Ferin, Geert Leus, Antonius F. W. van der Steen, Pieter Kruizinga, Johannes G. Bosch</strong></p>
<p>Computational ultrasound imaging (cUSi) with few elements and spatial field encoding can provide high-resolution volumetric B-mode imaging. In this work, we extend its application to 4D carotid artery (CA) flow imaging using a custom large-aperture 240-element matrix probe. We implemented a frequency band-based matched filtering strategy that balances resolution and contrast. The systemâ€™s inherent imaging capabilities were evaluated and validated in flow phantom and human CA experiments. In the phantom study, 3D&#x2F;4D power Doppler image and speckle-tracking analyses confirmed the systemâ€™s ability to resolve flow structures and hemodynamics. In the human study, the CA bifurcation flow structure and its local pulsatile flow dynamics were successfully reconstructed. These results demonstrate the feasibility of using a large-footprint, few-element cUSi system for 4D CA flow assessment. </p>
<blockquote>
<p>è®¡ç®—è¶…å£°æˆåƒï¼ˆcUSiï¼‰ä½¿ç”¨å°‘é‡å…ƒä»¶å’Œç©ºé—´åœºç¼–ç æŠ€æœ¯ï¼Œå¯æä¾›é«˜åˆ†è¾¨ç‡çš„ä¸‰ç»´Bæ¨¡å¼å›¾åƒã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨å®šåˆ¶çš„å¤§å­”å¾„240å…ƒç´ çŸ©é˜µæ¢å¤´å°†å…¶æ‰©å±•åˆ°å››ç»´é¢ˆåŠ¨è„‰ï¼ˆCAï¼‰è¡€æµæˆåƒã€‚æˆ‘ä»¬å®æ–½äº†ä¸€ç§åŸºäºé¢‘å¸¦çš„åŒ¹é…æ»¤æ³¢ç­–ç•¥ï¼Œè¯¥ç­–ç•¥å¯å¹³è¡¡åˆ†è¾¨ç‡å’Œå¯¹æ¯”åº¦ã€‚è¯¥ç³»ç»Ÿåœ¨è¡€æµæ¨¡å‹å’Œäººä½“é¢ˆåŠ¨è„‰å®éªŒä¸­è¿›è¡Œäº†æˆåƒèƒ½åŠ›çš„è¯„ä¼°å’ŒéªŒè¯ã€‚åœ¨æ¨¡å‹ç ”ç©¶ä¸­ï¼Œä¸‰ç»´&#x2F;å››ç»´åŠŸç‡å¤šæ™®å‹’å›¾åƒå’Œæ–‘ç‚¹è·Ÿè¸ªåˆ†æè¯å®äº†è¯¥ç³»ç»Ÿè§£å†³æµåŠ¨ç»“æ„å’Œè¡€æµåŠ¨åŠ›å­¦çš„èƒ½åŠ›ã€‚åœ¨äººç±»ç ”ç©¶ä¸­ï¼ŒæˆåŠŸé‡å»ºäº†é¢ˆåŠ¨è„‰åˆ†å‰çš„è¡€æµç»“æ„åŠå…¶å±€éƒ¨è„‰åŠ¨è¡€æµåŠ¨åŠ›å­¦ã€‚è¿™äº›ç»“æœè¯æ˜äº†ä½¿ç”¨å¤§è¶³è¿¹ã€å°‘é‡å…ƒä»¶çš„cUSiç³»ç»Ÿè¿›è¡Œå››ç»´é¢ˆåŠ¨è„‰è¡€æµè¯„ä¼°çš„å¯è¡Œæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.20963v1">PDF</a> The submission consists of an 11-page manuscript with 5 figures,   followed by 2 pages of supplemental files</p>
<p><strong>Summary</strong><br>     è®¡ç®—è¶…å£°æˆåƒï¼ˆcUSiï¼‰é‡‡ç”¨å°‘é‡å…ƒä»¶å’Œç©ºé—´åœºç¼–ç å¯å®ç°é«˜åˆ†è¾¨ç‡ä¸‰ç»´Bæ¨¡å¼æˆåƒã€‚æœ¬ç ”ç©¶å°†å…¶åº”ç”¨æ‰©å±•è‡³å››ç»´é¢ˆåŠ¨è„‰ï¼ˆCAï¼‰è¡€æµæˆåƒï¼Œé‡‡ç”¨è‡ªå®šä¹‰å¤§å­”å¾„240å…ƒä»¶çŸ©é˜µæ¢å¤´ã€‚æˆ‘ä»¬å®æ–½äº†åŸºäºé¢‘å¸¦çš„åŒ¹é…æ»¤æ³¢ç­–ç•¥ï¼Œåœ¨åˆ†è¾¨ç‡å’Œå¯¹æ¯”åº¦ä¹‹é—´å–å¾—å¹³è¡¡ã€‚ç³»ç»Ÿçš„å›ºæœ‰æˆåƒèƒ½åŠ›åœ¨è¡€æµæ¨¡å‹å’Œäººä½“é¢ˆåŠ¨è„‰å®éªŒä¸­å¾—åˆ°äº†è¯„ä¼°å’ŒéªŒè¯ã€‚æ¨¡å‹ç ”ç©¶é€šè¿‡ä¸‰ç»´&#x2F;å››ç»´åŠŸç‡å¤šæ™®å‹’å›¾åƒå’Œæ–‘ç‚¹è¿½è¸ªåˆ†æè¯å®äº†ç³»ç»Ÿè§£å†³æµåŠ¨ç»“æ„å’Œè¡€æµåŠ¨åŠ›å­¦çš„èƒ½åŠ›ã€‚äººä½“ç ”ç©¶ä¸­æˆåŠŸé‡å»ºäº†é¢ˆåŠ¨è„‰åˆ†å‰æµåŠ¨ç»“æ„åŠå…¶å±€éƒ¨è„‰åŠ¨æµåŠ¨åŠ¨åŠ›å­¦ã€‚è¿™äº›ç»“æœè¯æ˜ä½¿ç”¨å¤§è¶³è¿¹ã€å°‘é‡å…ƒä»¶çš„cUSiç³»ç»Ÿè¿›è¡Œå››ç»´é¢ˆåŠ¨è„‰è¡€æµè¯„ä¼°æ˜¯å¯è¡Œçš„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®¡ç®—è¶…å£°æˆåƒï¼ˆcUSiï¼‰å¯ä»¥å®ç°é«˜åˆ†è¾¨ç‡çš„ä¸‰ç»´Bæ¨¡å¼æˆåƒï¼Œä½¿ç”¨å°‘é‡å…ƒä»¶å’Œç©ºé—´åœºç¼–ç æŠ€æœ¯ã€‚</li>
<li>ç ”ç©¶å°†cUSiåº”ç”¨æ‰©å±•è‡³å››ç»´é¢ˆåŠ¨è„‰ï¼ˆCAï¼‰è¡€æµæˆåƒã€‚</li>
<li>é‡‡ç”¨è‡ªå®šä¹‰å¤§å­”å¾„çŸ©é˜µæ¢å¤´è¿›è¡Œå››ç»´é¢ˆåŠ¨è„‰è¡€æµæˆåƒã€‚</li>
<li>å®æ–½åŸºäºé¢‘å¸¦çš„åŒ¹é…æ»¤æ³¢ç­–ç•¥ï¼Œä»¥å¹³è¡¡åˆ†è¾¨ç‡å’Œå¯¹æ¯”åº¦ã€‚</li>
<li>è¡€æµæ¨¡å‹å’Œäººä½“é¢ˆåŠ¨è„‰å®éªŒéªŒè¯äº†ç³»ç»Ÿçš„å›ºæœ‰æˆåƒèƒ½åŠ›ã€‚</li>
<li>æ¨¡å‹ç ”ç©¶è¯å®äº†ç³»ç»Ÿèƒ½å¤Ÿè§£å†³æµåŠ¨ç»“æ„å’Œè¡€æµåŠ¨åŠ›å­¦é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.20963">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-af1e1729c5024ed731d9e89735551c29" align="middle">
<img src="https://picx.zhimg.com/v2-0586f30989344c26680fd15c39dd05da" align="middle">
<img src="https://picx.zhimg.com/v2-a4d53096809cae10aa6ea3c3b422356d" align="middle">
<img src="https://picx.zhimg.com/v2-22989f6a6d09e566672375e5bde7553d" align="middle">
<img src="https://picx.zhimg.com/v2-3e5b2b32a67a9b1b8386b1e2bdd821b9" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="SwinMamba-A-hybrid-local-global-mamba-framework-for-enhancing-semantic-segmentation-of-remotely-sensed-images"><a href="#SwinMamba-A-hybrid-local-global-mamba-framework-for-enhancing-semantic-segmentation-of-remotely-sensed-images" class="headerlink" title="SwinMamba: A hybrid local-global mamba framework for enhancing semantic   segmentation of remotely sensed images"></a>SwinMamba: A hybrid local-global mamba framework for enhancing semantic   segmentation of remotely sensed images</h2><p><strong>Authors:Qinfeng Zhu, Han Li, Liang He, Lei Fan</strong></p>
<p>Semantic segmentation of remote sensing imagery is a fundamental task in computer vision, supporting a wide range of applications such as land use classification, urban planning, and environmental monitoring. However, this task is often challenged by the high spatial resolution, complex scene structures, and diverse object scales present in remote sensing data. To address these challenges, various deep learning architectures have been proposed, including convolutional neural networks, Vision Transformers, and the recently introduced Vision Mamba. Vision Mamba features a global receptive field and low computational complexity, demonstrating both efficiency and effectiveness in image segmentation. However, its reliance on global scanning tends to overlook critical local features, such as textures and edges, which are essential for achieving accurate segmentation in remote sensing contexts. To tackle this limitation, we propose SwinMamba, a novel framework inspired by the Swin Transformer. SwinMamba integrates localized Mamba-style scanning within shifted windows with a global receptive field, to enhance the modelâ€™s perception of both local and global features. Specifically, the first two stages of SwinMamba perform local scanning to capture fine-grained details, while its subsequent two stages leverage global scanning to fuse broader contextual information. In our model, the use of overlapping shifted windows enhances inter-region information exchange, facilitating more robust feature integration across the entire image. Extensive experiments on the LoveDA and ISPRS Potsdam datasets demonstrate that SwinMamba outperforms state-of-the-art methods, underscoring its effectiveness and potential as a superior solution for semantic segmentation of remotely sensed imagery. </p>
<blockquote>
<p>é¥æ„Ÿå½±åƒçš„è¯­ä¹‰åˆ†å‰²æ˜¯è®¡ç®—æœºè§†è§‰ä¸­çš„ä¸€é¡¹åŸºæœ¬ä»»åŠ¡ï¼Œæ”¯æŒåœŸåœ°åˆ©ç”¨åˆ†ç±»ã€åŸå¸‚è§„åˆ’å’Œç¯å¢ƒç›‘æµ‹ç­‰å¹¿æ³›åº”ç”¨ã€‚ç„¶è€Œï¼Œè¿™é¡¹ä»»åŠ¡å¸¸å¸¸é¢ä¸´é¥æ„Ÿæ•°æ®é«˜åˆ†è¾¨ç‡ã€å¤æ‚åœºæ™¯ç»“æ„å’Œå¤šæ ·å¯¹è±¡å°ºåº¦æ‰€å¸¦æ¥çš„æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œå·²ç»æå‡ºäº†å„ç§æ·±åº¦å­¦ä¹ æ¶æ„ï¼ŒåŒ…æ‹¬å·ç§¯ç¥ç»ç½‘ç»œã€è§†è§‰å˜å‹å™¨å’Œæœ€è¿‘æ¨å‡ºçš„è§†è§‰æ›¼å·´ã€‚è§†è§‰æ›¼å·´å…·æœ‰å…¨å±€æ„Ÿå—é‡å’Œä½è®¡ç®—å¤æ‚åº¦ï¼Œåœ¨å›¾åƒåˆ†å‰²ä¸­è¡¨ç°å‡ºé«˜æ•ˆæ€§å’Œæœ‰æ•ˆæ€§ã€‚ç„¶è€Œï¼Œå®ƒå¯¹å…¨å±€æ‰«æçš„ä¾èµ–å¾€å¾€ä¼šå¿½ç•¥å…³é”®çš„å±€éƒ¨ç‰¹å¾ï¼Œå¦‚çº¹ç†å’Œè¾¹ç¼˜ï¼Œè¿™äº›ç‰¹å¾å¯¹äºåœ¨é¥æ„Ÿç¯å¢ƒä¸­å®ç°å‡†ç¡®åˆ†å‰²è‡³å…³é‡è¦ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†å—Swin Transformerå¯å‘çš„æ–°å‹æ¡†æ¶SwinMambaã€‚SwinMambaç»“åˆäº†å±€éƒ¨æ›¼å·´é£æ ¼æ‰«æå’Œå…¨å±€æ„Ÿå—é‡çš„ç§»ä½çª—å£ï¼Œå¢å¼ºäº†æ¨¡å‹å¯¹å±€éƒ¨å’Œå…¨å±€ç‰¹å¾çš„æ„ŸçŸ¥ã€‚å…·ä½“æ¥è¯´ï¼ŒSwinMambaçš„å‰ä¸¤ä¸ªé˜¶æ®µè¿›è¡Œå±€éƒ¨æ‰«æä»¥æ•æ‰ç²¾ç»†ç»†èŠ‚ï¼Œè€Œåä¸¤ä¸ªé˜¶æ®µåˆ™åˆ©ç”¨å…¨å±€æ‰«ææ¥èåˆæ›´å¹¿æ³›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚åœ¨æˆ‘ä»¬çš„æ¨¡å‹ä¸­ï¼Œä½¿ç”¨é‡å çš„ç§»ä½çª—å£å¢å¼ºäº†åŒºåŸŸé—´çš„ä¿¡æ¯äº¤æ¢ï¼Œä¿ƒè¿›äº†æ•´ä¸ªå›¾åƒä¸Šæ›´ç¨³å¥çš„ç‰¹å¾èåˆã€‚åœ¨LoveDAå’ŒISPRS Potsdamæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSwinMambaä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œçªæ˜¾äº†å…¶æœ‰æ•ˆæ€§å’Œä½œä¸ºé¥æ„Ÿå½±åƒè¯­ä¹‰åˆ†å‰²ä¼˜è¶Šè§£å†³æ–¹æ¡ˆçš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.20918v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é¥æ„Ÿå½±åƒè¯­ä¹‰åˆ†å‰²çš„æŒ‘æˆ˜å’Œæœ€æ–°è¿›å±•ã€‚é’ˆå¯¹ç°æœ‰æ¨¡å‹å¿½è§†å±€éƒ¨ç‰¹å¾çš„é—®é¢˜ï¼Œæå‡ºäº†èåˆå±€éƒ¨ä¸å…¨å±€ç‰¹å¾çš„SwinMambaæ¨¡å‹ã€‚è¯¥æ¨¡å‹é€šè¿‡ç»“åˆMambaé£æ ¼çš„å±€éƒ¨æ‰«æå’Œå…¨å±€æ„Ÿå—é‡ï¼Œæå‡äº†é¥æ„Ÿå½±åƒåˆ†å‰²çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚åœ¨LoveDAå’ŒISPRS Potsdamæ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜äº†SwinMambaçš„ä¼˜è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¥æ„Ÿå½±åƒè¯­ä¹‰åˆ†å‰²æ˜¯è®¡ç®—æœºè§†è§‰ä¸­çš„åŸºç¡€ä»»åŠ¡ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨ä»·å€¼ï¼Œå¦‚åœŸåœ°åˆ©ç”¨åˆ†ç±»ã€åŸå¸‚è§„åˆ’å’Œç¯å¢ƒç›‘æµ‹ç­‰ã€‚</li>
<li>è¯¥ä»»åŠ¡é¢ä¸´é«˜ç©ºé—´åˆ†è¾¨ç‡ã€å¤æ‚åœºæ™¯ç»“æ„å’Œå¤šæ ·å¯¹è±¡å°ºåº¦ç­‰æŒ‘æˆ˜ã€‚</li>
<li>æ·±åº¦å­¦ä¹ æ¶æ„ï¼Œå¦‚å·ç§¯ç¥ç»ç½‘ç»œã€Vision Transformerså’ŒVision Mambaï¼Œå·²è¢«åº”ç”¨äºè§£å†³è¿™äº›æŒ‘æˆ˜ã€‚</li>
<li>Vision Mambaå…·æœ‰å…¨å±€æ„Ÿå—é‡å’Œä½è®¡ç®—å¤æ‚æ€§ï¼Œä½†åœ¨é¥æ„Ÿå½±åƒåˆ†å‰²ä¸­å¿½è§†äº†å±€éƒ¨ç‰¹å¾çš„é‡è¦æ€§ã€‚</li>
<li>SwinMambaæ¨¡å‹ç»“åˆäº†Mambaé£æ ¼çš„å±€éƒ¨æ‰«æå’Œå…¨å±€æ„Ÿå—é‡ï¼Œæ—¨åœ¨å¢å¼ºå¯¹å±€éƒ¨å’Œå…¨å±€ç‰¹å¾çš„æ„ŸçŸ¥ã€‚</li>
<li>SwinMambaæ¨¡å‹é€šè¿‡é‡å çš„ç§»ä½çª—å£å¢å¼ºåŒºåŸŸé—´ä¿¡æ¯äº¤æ¢ï¼Œå®ç°æ›´ç¨³å¥çš„ç‰¹å¾æ•´åˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.20918">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fc0dc560e10339c27d5ac4013b4b7e6d" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Revolutionizing-Precise-Low-Back-Pain-Diagnosis-via-Contrastive-Learning"><a href="#Revolutionizing-Precise-Low-Back-Pain-Diagnosis-via-Contrastive-Learning" class="headerlink" title="Revolutionizing Precise Low Back Pain Diagnosis via Contrastive Learning"></a>Revolutionizing Precise Low Back Pain Diagnosis via Contrastive Learning</h2><p><strong>Authors:Thanh Binh Le, Hoang Nhat Khang Vo, Tan-Ha Mai, Trong Nhan Phan</strong></p>
<p>Low back pain affects millions worldwide, driving the need for robust diagnostic models that can jointly analyze complex medical images and accompanying text reports. We present LumbarCLIP, a novel multimodal framework that leverages contrastive language-image pretraining to align lumbar spine MRI scans with corresponding radiological descriptions. Built upon a curated dataset containing axial MRI views paired with expert-written reports, LumbarCLIP integrates vision encoders (ResNet-50, Vision Transformer, Swin Transformer) with a BERT-based text encoder to extract dense representations. These are projected into a shared embedding space via learnable projection heads, configurable as linear or non-linear, and normalized to facilitate stable contrastive training using a soft CLIP loss. Our model achieves state-of-the-art performance on downstream classification, reaching up to 95.00% accuracy and 94.75% F1-score on the test set, despite inherent class imbalance. Extensive ablation studies demonstrate that linear projection heads yield more effective cross-modal alignment than non-linear variants. LumbarCLIP offers a promising foundation for automated musculoskeletal diagnosis and clinical decision support. </p>
<blockquote>
<p>è…°èƒŒç—›å½±å“äº†å…¨çƒæ•°ç™¾ä¸‡äººçš„å¥åº·ï¼Œè¿™ä¿ƒä½¿äº†å¯¹ç¨³å¥çš„è¯Šæ–­æ¨¡å‹çš„éœ€æ±‚ï¼Œè¿™äº›æ¨¡å‹èƒ½å¤Ÿè”åˆåˆ†æå¤æ‚çš„åŒ»å­¦å›¾åƒå’Œä¼´éšçš„æ–‡æœ¬æŠ¥å‘Šã€‚æˆ‘ä»¬æå‡ºäº†LumbarCLIPï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„å¤šæ¨¡å¼æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼Œå°†è…°æ¤MRIæ‰«æä¸ç›¸åº”çš„æ”¾å°„å­¦æè¿°å¯¹é½ã€‚LumbarCLIPå»ºç«‹åœ¨ç²¾é€‰æ•°æ®é›†ä¹‹ä¸Šï¼Œè¯¥æ•°æ®é›†åŒ…å«è½´å‘MRIè§†å›¾ä¸ä¸“å®¶æ’°å†™çš„æŠ¥å‘Šç›¸é…å¯¹ï¼Œå®ƒå°†è§†è§‰ç¼–ç å™¨ï¼ˆResNet-50ã€Vision Transformerã€Swin Transformerï¼‰ä¸åŸºäºBERTçš„æ–‡æœ¬ç¼–ç å™¨ç›¸ç»“åˆï¼Œä»¥æå–å¯†é›†è¡¨ç¤ºã€‚è¿™äº›è¡¨ç¤ºé€šè¿‡å¯å­¦ä¹ çš„æŠ•å½±å¤´æŠ•å½±åˆ°å…±äº«åµŒå…¥ç©ºé—´ä¸­ï¼Œå¯é…ç½®ä¸ºçº¿æ€§æˆ–éçº¿æ€§ï¼Œå¹¶è¿›è¡Œå½’ä¸€åŒ–ï¼Œä»¥ä¾¿ä½¿ç”¨è½¯CLIPæŸå¤±è¿›è¡Œç¨³å®šå¯¹æ¯”è®­ç»ƒã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨ä¸‹æ¸¸åˆ†ç±»ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå°½ç®¡å­˜åœ¨å›ºæœ‰çš„ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ï¼Œä½†åœ¨æµ‹è¯•é›†ä¸Šä»è¾¾åˆ°äº†95.00%çš„å‡†ç¡®ç‡å’Œ94.75%çš„F1åˆ†æ•°ã€‚å¹¿æ³›çš„æ¶ˆèç ”ç©¶è¡¨æ˜ï¼Œçº¿æ€§æŠ•å½±å¤´æ¯”éçº¿æ€§æŠ•å½±å¤´åœ¨è·¨æ¨¡å¼å¯¹é½æ–¹é¢æ›´æœ‰æ•ˆã€‚LumbarCLIPä¸ºè‡ªåŠ¨åŒ–è‚Œè‚‰éª¨éª¼è¯Šæ–­å’Œä¸´åºŠå†³ç­–æ”¯æŒæä¾›äº†æœ‰å‰æ™¯çš„åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.20813v1">PDF</a> 12 pages, 4 figures</p>
<p><strong>Summary</strong></p>
<p>LumbarCLIPæ˜¯ä¸€ç§æ–°å‹çš„å¤šæ¨¡æ€è¯Šæ–­æ¡†æ¶ï¼Œèƒ½å¤Ÿç»“åˆåŒ»å­¦å›¾åƒå’Œæ–‡æœ¬æŠ¥å‘Šè¿›è¡Œåˆ†æã€‚å®ƒåˆ©ç”¨å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒæŠ€æœ¯ï¼Œå°†è…°æ¤MRIæ‰«æä¸ç›¸åº”çš„æ”¾å°„å­¦æè¿°å¯¹é½ã€‚LumbarCLIPé›†æˆäº†è§†è§‰ç¼–ç å™¨å’ŒåŸºäºBERTçš„æ–‡æœ¬ç¼–ç å™¨ï¼Œä»¥æå–å¯†é›†è¡¨ç¤ºï¼Œå¹¶é€šè¿‡å¯å­¦ä¹ çš„æŠ•å½±å¤´å°†å…¶æŠ•å½±åˆ°å…±äº«åµŒå…¥ç©ºé—´ä¸­ã€‚è¯¥æ¨¡å‹å®ç°äº†ä¸‹æ¸¸åˆ†ç±»çš„å“è¶Šæ€§èƒ½ï¼Œåœ¨æµ‹è¯•é›†ä¸Šè¾¾åˆ°äº†95.00%çš„å‡†ç¡®ç‡å’Œ94.75%çš„F1åˆ†æ•°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LumbarCLIPæ˜¯ä¸€ä¸ªå¤šæ¨¡æ€æ¡†æ¶ï¼Œç”¨äºåˆ†æåŒ»å­¦å›¾åƒå’Œæ–‡æœ¬æŠ¥å‘Šã€‚</li>
<li>å®ƒåˆ©ç”¨å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒæŠ€æœ¯ï¼Œå°†è…°æ¤MRIæ‰«æä¸æ”¾å°„å­¦æè¿°å¯¹é½ã€‚</li>
<li>LumbarCLIPé›†æˆäº†è§†è§‰ç¼–ç å™¨å’Œæ–‡æœ¬ç¼–ç å™¨ä»¥æå–å¯†é›†è¡¨ç¤ºã€‚</li>
<li>æ¨¡å‹ä½¿ç”¨å¯å­¦ä¹ çš„æŠ•å½±å¤´å°†è¡¨ç¤ºæŠ•å½±åˆ°å…±äº«åµŒå…¥ç©ºé—´ã€‚</li>
<li>æ¨¡å‹æ€§èƒ½åœ¨ä¸‹æ¸¸åˆ†ç±»ä»»åŠ¡ä¸­è¡¨ç°å“è¶Šï¼Œè¾¾åˆ°äº†é«˜å‡†ç¡®ç‡å’ŒF1åˆ†æ•°ã€‚</li>
<li>çº¿æ€§æŠ•å½±å¤´åœ¨è·¨æ¨¡æ€å¯¹é½æ–¹é¢æ¯”éçº¿æ€§æŠ•å½±å¤´æ›´æœ‰æ•ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.20813">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-026570f301ba40500ce8719955958708" align="middle">
<img src="https://picx.zhimg.com/v2-6b38b86ed31d9610c1ddc1d9abb57297" align="middle">
<img src="https://picx.zhimg.com/v2-a24f7e2386b7a4a284bd582063874aaa" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="The-X-ray-Emission-of-NGC-5005-An-Unobscured-Low-Luminosity-AGN-with-a-Weakly-Accreting-Broad-Line-Region"><a href="#The-X-ray-Emission-of-NGC-5005-An-Unobscured-Low-Luminosity-AGN-with-a-Weakly-Accreting-Broad-Line-Region" class="headerlink" title="The X-ray Emission of NGC 5005: An Unobscured Low-Luminosity AGN with a   Weakly Accreting Broad-Line Region"></a>The X-ray Emission of NGC 5005: An Unobscured Low-Luminosity AGN with a   Weakly Accreting Broad-Line Region</h2><p><strong>Authors:Anna Trindade FalcÃ£o, R. Middei, G. Fabbiano, M. Elvis, P. Zhu, W. P. Maksym, D. Å. KrÃ³l, L. Feuillet</strong></p>
<p>We present deep Chandra X-ray observations of NGC 5005, a LINER-dominated galaxy previously reported to host a broad H$\alpha$ emission line. The diffuse soft X-ray emission ($&lt;$3 keV) extends out to $\sim$800 pc, while harder emission ($&gt;$3 keV) is confined to the central $\sim$400 pc. Spatially resolved spectroscopy of the nuclear ($r&lt;150$ pc) and extended ($150&lt;r&lt;500$ pc) regions reveals that these are best described by models including both photoionized and thermal plasma components, consistent with excitation by a low-luminosity AGN and shock-heated gas. Narrow-band imaging and excitation maps from the Hubble Space Telescope (HST) support this interpretation, closely matching the X-ray morphology and ionization structure. The detection of a faint hard X-ray nuclear source with Chandra, combined with stringent upper limits from NuSTAR and Swift, and consistency with the X-ray luminosity predicted from the HST [O III]$\lambda$5007 emission, indicates that NGC 5005 hosts an intrinsically low-luminosity ($L_{\rm bol} \sim 10^{41}$ erg s$^{-1}$), unobscured AGN. Despite the extremely low Eddington ratio inferred from our measurements ($\lambda_{\rm Edd} \sim 5 \times 10^{-6}$), the presence of a broad H$\alpha$ line in the optical spectrum suggests the persistence of a thin accretion disk, challenging standard paradigms of accretion flow configurations at such low accretion rates. </p>
<blockquote>
<p>æˆ‘ä»¬å±•ç¤ºäº†NGC 5005çš„æ·±åº¦é’±å¾·æ‹‰Xå°„çº¿è§‚æµ‹ç»“æœã€‚NGC 5005æ˜¯ä¸€ä¸ªä»¥å‰æŠ¥å‘Šå­˜åœ¨å®½HÎ±å‘å°„çº¿çš„LINERæ˜Ÿç³»ã€‚æ¼«å°„è½¯Xå°„çº¿å‘å°„ï¼ˆï¼œ3 keVï¼‰å»¶ä¼¸è‡³çº¦800 pcï¼Œè€Œè¾ƒç¡¬çš„å‘å°„ï¼ˆï¼3 keVï¼‰ä»…é™äºä¸­å¤®çº¦400 pcã€‚å¯¹æ ¸å¿ƒåŒºåŸŸï¼ˆrï¼œ150 pcï¼‰å’Œæ‰©å±•åŒºåŸŸï¼ˆ150ï¼œrï¼œ500 pcï¼‰çš„ç©ºé—´åˆ†è¾¨å…‰è°±è¡¨æ˜ï¼Œè¿™äº›åŒºåŸŸæœ€é€‚åˆåŒ…å«å…‰è‡´ç”µç¦»å’Œçƒ­ç­‰ç¦»å­ä½“æˆåˆ†åœ¨å†…çš„æ¨¡å‹æè¿°ï¼Œè¿™ä¸ä½å…‰åº¦çš„æ´»åŠ¨æ˜Ÿç³»æ ¸å’Œå†²å‡»åŠ çƒ­æ°”ä½“çš„æ¿€å‘ç›¸ä¸€è‡´ã€‚å“ˆå‹ƒå¤ªç©ºæœ›è¿œé•œï¼ˆHSTï¼‰çš„çª„å¸¦æˆåƒå’Œæ¿€å‘å›¾æ”¯æŒè¿™ä¸€è§£é‡Šï¼Œä¸Xå°„çº¿çš„å½¢æ€å’Œç”µç¦»ç»“æ„ç´§å¯†åŒ¹é…ã€‚é’±å¾·æ‹‰æ£€æµ‹åˆ°ä¸€ä¸ªå¾®å¼±ç¡¬Xå°„çº¿æ ¸æºï¼Œç»“åˆNuSTARå’ŒSwiftçš„ä¸¥æ ¼ä¸Šé™ï¼Œå¹¶ä¸é€šè¿‡HST [O III]Î»5007å‘å°„é¢„æµ‹çš„Xå°„çº¿å…‰åº¦ä¸€è‡´ï¼Œè¡¨æ˜NGC 5005å­˜åœ¨ä¸€ä¸ªå›ºæœ‰ä½å…‰åº¦ï¼ˆå…‰åº¦çº¦ä¸º10^41 erg s^-1ï¼‰çš„æ— é®æŒ¡æ´»åŠ¨æ˜Ÿç³»æ ¸ã€‚å°½ç®¡æˆ‘ä»¬ä»æµ‹é‡ä¸­æ¨æ–­å‡ºçš„çˆ±ä¸é¡¿æ¯”æä½ï¼ˆÎ»_{edd} ~ 5 Ã— 10^-6ï¼‰ï¼Œä½†å…‰å­¦å…‰è°±ä¸­å­˜åœ¨å®½HÎ±çº¿è¡¨æ˜å­˜åœ¨è–„è–„çš„å¸ç§¯ç›˜ï¼Œè¿™å¯¹å¦‚æ­¤ä½çš„å¸ç§¯ç‡ä¸‹æ ‡å‡†å¸ç§¯æµé…ç½®çš„èŒƒå¼æå‡ºäº†æŒ‘æˆ˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.20597v1">PDF</a> Accepted for publication on ApJ</p>
<p><strong>æ‘˜è¦</strong><br>    NGC 5005çš„æ·±å±‚é’±å¾·æ‹‰Xå°„çº¿è§‚æµ‹æ­ç¤ºï¼Œå…¶å¼¥æ•£è½¯Xå°„çº¿å‘å°„å»¶ä¼¸è‡³çº¦800 pcï¼Œç¡¬å‘å°„æ›´é›†ä¸­åœ¨ä¸­å¤®çº¦400 pcã€‚æ ¸åŒºå’Œæ‰©å±•åŒºçš„ç©ºé—´è§£æå…‰è°±æ˜¾ç¤ºï¼ŒåŒ…æ‹¬å…‰è‡´ç”µç¦»å’Œçƒ­ç­‰ç¦»å­ä½“æˆåˆ†åœ¨å†…çš„æ¨¡å‹æœ€èƒ½æè¿°è¿™äº›åŒºåŸŸï¼Œä¸ä½å…‰åº¦æ´»åŠ¨æ˜Ÿç³»æ ¸ï¼ˆAGNï¼‰æ¿€å‘å’Œå†²å‡»åŠ çƒ­æ°”ä½“çš„çŒœæƒ³ä¸€è‡´ã€‚å“ˆå‹ƒå¤ªç©ºæœ›è¿œé•œï¼ˆHSTï¼‰çš„çª„å¸¦æˆåƒå’Œæ¿€å‘å›¾æ”¯æŒè¿™ä¸€è§£é‡Šï¼Œä¸Xå°„çº¿å½¢æ€å’Œç”µç¦»ç»“æ„ç›¸åŒ¹é…ã€‚é’±å¾·æ‹‰æ£€æµ‹åˆ°çš„å¾®å¼±ç¡¬Xå°„çº¿æ ¸æºä¸NuSTARå’ŒSwiftçš„ä¸¥æ ¼ä¸Šé™ç›¸ä¸€è‡´ï¼Œå¹¶ä¸HSTçš„[O III]Î»5007å‘å°„é¢„æµ‹çš„Xå°„çº¿å…‰åº¦ä¸€è‡´ï¼Œè¡¨æ˜NGC 5005æ‹¥æœ‰ä¸€ä¸ªå†…åœ¨ä½å…‰åº¦ï¼ˆLbol ~ 1041 erg s-1ï¼‰æœªé®è”½çš„æ´»è·ƒæ˜Ÿç³»æ ¸ã€‚å°½ç®¡ä»æˆ‘ä»¬çš„æµ‹é‡ä¸­æ¨æ–­å‡ºæä½çš„è‰¾ä¸é¡¿æ¯”ç‡ï¼ˆÎ»Edd ~ 5 Ã— 10-6ï¼‰ï¼Œä½†å…‰å­¦å…‰è°±ä¸­å­˜åœ¨å®½HÎ±çº¿ï¼Œæš—ç¤ºäº†è–„ç›˜çš„å­˜åœ¨ï¼Œå¯¹å¦‚æ­¤ä½å¸ç§¯ç‡ä¸‹çš„æ ‡å‡†å¸ç§¯æµé…ç½®æå‡ºäº†æŒ‘æˆ˜ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>NGC 5005çš„æ·±å±‚é’±å¾·æ‹‰Xå°„çº¿è§‚æµ‹å‘ç°å…¶Xå°„çº¿å‘å°„åŒ…æ‹¬å¼¥æ•£è½¯Xå°„çº¿å‘å°„å’Œæ›´é›†ä¸­çš„ç¡¬å‘å°„ã€‚</li>
<li>æ ¸åŒºå’Œæ‰©å±•åŒºçš„å…‰è°±æ˜¾ç¤ºå­˜åœ¨å…‰è‡´ç”µç¦»å’Œçƒ­ç­‰ç¦»å­ä½“æˆåˆ†ã€‚</li>
<li>å“ˆå‹ƒå¤ªç©ºæœ›è¿œé•œçš„è§‚æµ‹æ”¯æŒäº†ä½å…‰åº¦æ´»åŠ¨æ˜Ÿç³»æ ¸ï¼ˆAGNï¼‰å’Œå†²å‡»åŠ çƒ­æ°”ä½“çš„è§£é‡Šï¼Œä¸Xå°„çº¿è§‚æµ‹ç»“æœç›¸åŒ¹é…ã€‚</li>
<li>æ£€æµ‹åˆ°å¾®å¼±çš„ç¡¬Xå°„çº¿æ ¸æºï¼Œä¸NGC 5005å­˜åœ¨æœªé®è”½çš„æ´»è·ƒæ˜Ÿç³»æ ¸çš„å‡è®¾ä¸€è‡´ã€‚</li>
<li>è¯¥æ´»è·ƒæ˜Ÿç³»æ ¸å…·æœ‰å†…åœ¨ä½å…‰åº¦ï¼Œä¸æŸäº›å…‰å­¦ç‰¹å¾å¦‚å®½HÎ±çº¿å…±å­˜ã€‚</li>
<li>è§‚æµ‹åˆ°çš„è‰¾ä¸é¡¿æ¯”ç‡æä½ï¼Œå¯¹æ ‡å‡†å¸ç§¯æµé…ç½®ç†è®ºæå‡ºäº†æŒ‘æˆ˜ã€‚</li>
<li>ç»“æœå¼ºè°ƒäº†æ˜Ÿç³»æ ¸ç»“æ„å’Œæ´»åŠ¨çš„å¤æ‚æ€§ï¼Œéœ€è¦æ›´æ·±å…¥çš„ç ”ç©¶æ¥ç†è§£å…¶ç‰©ç†æœºåˆ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.20597">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f5450a8afdcad10bbd44298ed029e6e0" align="middle">
<img src="https://picx.zhimg.com/v2-7b713979532024d2f0d7ee96b4fe4d6f" align="middle">
<img src="https://picx.zhimg.com/v2-08c4c60bc7f3552a8185fcba0cde62d6" align="middle">
<img src="https://picx.zhimg.com/v2-7a05d3cc5c6966351330fc8f88f206c3" align="middle">
<img src="https://picx.zhimg.com/v2-5675b0bac8743c68036c00d14365af46" align="middle">
<img src="https://picx.zhimg.com/v2-5e0be46086ef76106cca0808981b67c1" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="InstructVTON-Optimal-Auto-Masking-and-Natural-Language-Guided-Interactive-Style-Control-for-Inpainting-Based-Virtual-Try-On"><a href="#InstructVTON-Optimal-Auto-Masking-and-Natural-Language-Guided-Interactive-Style-Control-for-Inpainting-Based-Virtual-Try-On" class="headerlink" title="InstructVTON: Optimal Auto-Masking and Natural-Language-Guided   Interactive Style Control for Inpainting-Based Virtual Try-On"></a>InstructVTON: Optimal Auto-Masking and Natural-Language-Guided   Interactive Style Control for Inpainting-Based Virtual Try-On</h2><p><strong>Authors:Julien Han, Shuwen Qiu, Qi Li, Xingzi Xu, Mehmet Saygin Seyfioglu, Kavosh Asadi, Karim Bouyarmane</strong></p>
<p>We present InstructVTON, an instruction-following interactive virtual try-on system that allows fine-grained and complex styling control of the resulting generation, guided by natural language, on single or multiple garments. A computationally efficient and scalable formulation of virtual try-on formulates the problem as an image-guided or image-conditioned inpainting task. These inpainting-based virtual try-on models commonly use a binary mask to control the generation layout. Producing a mask that yields desirable result is difficult, requires background knowledge, might be model dependent, and in some cases impossible with the masking-based approach (e.g. trying on a long-sleeve shirt with â€œsleeves rolled upâ€ styling on a person wearing long-sleeve shirt with sleeves down, where the mask will necessarily cover the entire sleeve). InstructVTON leverages Vision Language Models (VLMs) and image segmentation models for automated binary mask generation. These masks are generated based on user-provided images and free-text style instructions. InstructVTON simplifies the end-user experience by removing the necessity of a precisely drawn mask, and by automating execution of multiple rounds of image generation for try-on scenarios that cannot be achieved with masking-based virtual try-on models alone. We show that InstructVTON is interoperable with existing virtual try-on models to achieve state-of-the-art results with styling control. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†InstructVTONï¼Œè¿™æ˜¯ä¸€ä¸ªéµå¾ªæŒ‡ä»¤çš„äº¤äº’å¼è™šæ‹Ÿè¯•ç©¿ç³»ç»Ÿï¼Œå…è®¸é€šè¿‡è‡ªç„¶è¯­è¨€æŒ‡å¯¼å¯¹å•ä¸€æˆ–å¤šä¸ªæœè£…è¿›è¡Œç²¾ç»†ç²’åº¦å’Œå¤æ‚çš„æ ·å¼æ§åˆ¶ï¼Œä»è€Œç”Ÿæˆç»“æœã€‚è™šæ‹Ÿè¯•ç©¿çš„è®¡ç®—é«˜æ•ˆå’Œå¯æ‰©å±•çš„å…¬å¼å°†å…¶è¡¨è¿°ä¸ºå›¾åƒå¼•å¯¼æˆ–å›¾åƒæ¡ä»¶çš„ä¿®å¤ä»»åŠ¡ã€‚è¿™äº›åŸºäºä¿®å¤æŠ€æœ¯çš„è™šæ‹Ÿè¯•ç©¿æ¨¡å‹é€šå¸¸ä½¿ç”¨äºŒè¿›åˆ¶è’™ç‰ˆæ¥æ§åˆ¶ç”Ÿæˆå¸ƒå±€ã€‚äº§ç”Ÿç†æƒ³çš„è’™ç‰ˆæ˜¯å›°éš¾çš„ï¼Œéœ€è¦èƒŒæ™¯çŸ¥è¯†ï¼Œå¯èƒ½æ˜¯æ¨¡å‹ä¾èµ–çš„ï¼Œå¹¶ä¸”åœ¨æŸäº›æƒ…å†µä¸‹ä½¿ç”¨åŸºäºè’™ç‰ˆçš„æ–¹æ³•æ˜¯ä¸å¯èƒ½çš„ï¼ˆä¾‹å¦‚åœ¨ç©¿ç€æ”¾ä¸‹è¢–å­é•¿è¢–è¡¬è¡«çš„äººèº«ä¸Šè¯•ç©¿â€œè¢–å­å·èµ·â€çš„é•¿è¢–è¡¬è¡«ï¼Œè’™ç‰ˆå¿…ç„¶ä¼šè¦†ç›–æ•´ä¸ªè¢–å­ï¼‰ã€‚InstructVTONåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å’Œå›¾åƒåˆ†å‰²æ¨¡å‹è¿›è¡Œè‡ªåŠ¨äºŒè¿›åˆ¶è’™ç‰ˆç”Ÿæˆã€‚è¿™äº›è’™ç‰ˆæ˜¯æ ¹æ®ç”¨æˆ·æä¾›çš„å›¾åƒå’Œè‡ªç”±æ–‡æœ¬æ ·å¼æŒ‡ä»¤ç”Ÿæˆçš„ã€‚InstructVTONç®€åŒ–äº†æœ€ç»ˆç”¨æˆ·çš„ä½¿ç”¨ä½“éªŒï¼Œé€šè¿‡å»é™¤ç²¾ç¡®ç»˜åˆ¶è’™ç‰ˆçš„å¿…è¦æ€§ï¼Œå¹¶è‡ªåŠ¨åŒ–æ‰§è¡Œå¤šè½®å›¾åƒç”Ÿæˆï¼Œä»¥å®ç°ä»…ä½¿ç”¨åŸºäºè’™ç‰ˆçš„è™šæ‹Ÿè¯•ç©¿æ¨¡å‹æ— æ³•å®ç°çš„è¯•ç©¿åœºæ™¯ã€‚æˆ‘ä»¬è¯æ˜äº†InstructVTONå¯ä»¥ä¸ç°æœ‰çš„è™šæ‹Ÿè¯•ç©¿æ¨¡å‹ååŒå·¥ä½œï¼Œä»¥å®ç°å…·æœ‰æ ·å¼æ§åˆ¶çš„æœ€æ–°ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.20524v1">PDF</a> Submitted to CVPR 2025 and Published at CVPR 2025 AI for Content   Creation workshop</p>
<p><strong>Summary</strong></p>
<p>InstructVTONæ˜¯ä¸€ä¸ªæ”¯æŒç²¾ç»†åŒ–å¤æ‚æ ·å¼æ§åˆ¶çš„äº¤äº’å¼è™šæ‹Ÿè¯•ç©¿ç³»ç»Ÿã€‚å®ƒé€šè¿‡è‡ªç„¶è¯­è¨€æŒ‡å¯¼ï¼Œä»¥å›¾åƒå¼•å¯¼æˆ–å›¾åƒæ¡ä»¶ä¿®å¤ä»»åŠ¡çš„å½¢å¼å®ç°è™šæ‹Ÿè¯•ç©¿ï¼Œé‡‡ç”¨é«˜æ•ˆå¯ä¼¸ç¼©çš„å…¬å¼åŒ–æ–¹æ³•ã€‚ä¸ºè§£å†³ä¼ ç»Ÿè™šæ‹Ÿè¯•ç©¿æ¨¡å‹ä¸­éœ€è¦å¤æ‚æ©è†œçš„é—®é¢˜ï¼ŒInstructVTONåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹å’Œå›¾åƒåˆ†å‰²æ¨¡å‹è‡ªåŠ¨ç”ŸæˆåŸºäºç”¨æˆ·æä¾›çš„å›¾åƒå’Œè‡ªç”±æ–‡æœ¬æ ·å¼æŒ‡ä»¤çš„æ©è†œã€‚å®ƒç®€åŒ–äº†æœ€ç»ˆç”¨æˆ·ä½“éªŒï¼Œé€šè¿‡è‡ªåŠ¨åŒ–æ‰§è¡Œå¤šè½®å›¾åƒç”Ÿæˆï¼Œå®ç°äº†æ— æ³•ä»…é€šè¿‡åŸºäºæ©è†œçš„è™šæ‹Ÿè¯•ç©¿æ¨¡å‹å®ç°çš„è¯•ç©¿åœºæ™¯ã€‚InstructVTONä¸ç°æœ‰è™šæ‹Ÿè¯•ç©¿æ¨¡å‹çš„å…¼å®¹æ€§è‰¯å¥½ï¼Œå¯å®ç°ä¸šç•Œé¢†å…ˆçš„æ ·å¼æ§åˆ¶ç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>InstructVTONæ˜¯ä¸€ä¸ªæ”¯æŒç²¾ç»†åŒ–å¤æ‚æ ·å¼æ§åˆ¶çš„äº¤äº’å¼è™šæ‹Ÿè¯•ç©¿ç³»ç»Ÿã€‚</li>
<li>å®ƒè§£å†³äº†ä¼ ç»Ÿè™šæ‹Ÿè¯•ç©¿æ¨¡å‹ä¸­éœ€è¦å¤æ‚æ©è†œçš„é—®é¢˜ã€‚</li>
<li>InstructVTONåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹å’Œå›¾åƒåˆ†å‰²æ¨¡å‹è‡ªåŠ¨ç”Ÿæˆæ©è†œã€‚</li>
<li>è¯¥ç³»ç»Ÿè‡ªåŠ¨æ‰§è¡Œå¤šè½®å›¾åƒç”Ÿæˆï¼Œä»¥å®ç°æ›´ä¸°å¯Œçš„è¯•ç©¿åœºæ™¯ã€‚</li>
<li>InstructVTONå…¼å®¹ç°æœ‰è™šæ‹Ÿè¯•ç©¿æ¨¡å‹ï¼Œå¹¶å®ç°äº†ä¸šç•Œé¢†å…ˆçš„æ ·å¼æ§åˆ¶ç»“æœã€‚</li>
<li>é€šè¿‡è‡ªç„¶è¯­è¨€æŒ‡å¯¼ï¼Œç”¨æˆ·å¯è½»æ¾æ§åˆ¶ç”Ÿæˆç»“æœçš„æ ·å¼å’Œç»†èŠ‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.20524">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-10c145a1ce350ad9a38f98fa8457d114" align="middle">
<img src="https://picx.zhimg.com/v2-05bdaca0e2b96835caa291b835adeffd" align="middle">
<img src="https://picx.zhimg.com/v2-94c9809cb7c95f436d632f547d5c9ef6" align="middle">
<img src="https://picx.zhimg.com/v2-b85d03dac09d5e65d6da786c963bdba7" align="middle">
<img src="https://picx.zhimg.com/v2-e83832b135f103b7e4fa7c0e85f38cda" align="middle">
<img src="https://picx.zhimg.com/v2-924a0306eeba019d1590a60a0e144329" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Boosting-LiDAR-Based-Localization-with-Semantic-Insight-Camera-Projection-versus-Direct-LiDAR-Segmentation"><a href="#Boosting-LiDAR-Based-Localization-with-Semantic-Insight-Camera-Projection-versus-Direct-LiDAR-Segmentation" class="headerlink" title="Boosting LiDAR-Based Localization with Semantic Insight: Camera   Projection versus Direct LiDAR Segmentation"></a>Boosting LiDAR-Based Localization with Semantic Insight: Camera   Projection versus Direct LiDAR Segmentation</h2><p><strong>Authors:Sven Ochs, Philip SchÃ¶rner, Marc RenÃ© Zofka, J. Marius ZÃ¶llner</strong></p>
<p>Semantic segmentation of LiDAR data presents considerable challenges, particularly when dealing with diverse sensor types and configurations. However, incorporating semantic information can significantly enhance the accuracy and robustness of LiDAR-based localization techniques for autonomous mobile systems. We propose an approach that integrates semantic camera data with LiDAR segmentation to address this challenge. By projecting LiDAR points into the semantic segmentation space of the camera, our method enhances the precision and reliability of the LiDAR-based localization pipeline.   For validation, we utilize the CoCar NextGen platform from the FZI Research Center for Information Technology, which offers diverse sensor modalities and configurations. The sensor setup of CoCar NextGen enables a thorough analysis of different sensor types. Our evaluation leverages the state-of-the-art Depth-Anything network for camera image segmentation and an adaptive segmentation network for LiDAR segmentation. To establish a reliable ground truth for LiDAR-based localization, we make us of a Global Navigation Satellite System (GNSS) solution with Real-Time Kinematic corrections (RTK). Additionally, we conduct an extensive 55 km drive through the city of Karlsruhe, Germany, covering a variety of environments, including urban areas, multi-lane roads, and rural highways. This multimodal approach paves the way for more reliable and precise autonomous navigation systems, particularly in complex real-world environments. </p>
<blockquote>
<p>æ¿€å…‰é›·è¾¾æ•°æ®çš„è¯­ä¹‰åˆ†å‰²å­˜åœ¨ç›¸å½“å¤§çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤šç§ä¼ æ„Ÿå™¨ç±»å‹å’Œé…ç½®æ—¶ã€‚ç„¶è€Œï¼Œèå…¥è¯­ä¹‰ä¿¡æ¯å¯ä»¥æ˜¾è‘—æé«˜åŸºäºæ¿€å…‰é›·è¾¾çš„è‡ªä¸»ç§»åŠ¨ç³»ç»Ÿå®šä½æŠ€æœ¯çš„å‡†ç¡®æ€§å’Œç¨³å¥æ€§ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§å°†è¯­ä¹‰ç›¸æœºæ•°æ®ä¸æ¿€å…‰é›·è¾¾åˆ†å‰²ç›¸ç»“åˆçš„æ–¹æ³•ï¼Œä»¥åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ã€‚é€šè¿‡å°†æ¿€å…‰é›·è¾¾ç‚¹æŠ•å½±åˆ°ç›¸æœºçš„è¯­ä¹‰åˆ†å‰²ç©ºé—´ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æé«˜äº†åŸºäºæ¿€å…‰é›·è¾¾çš„å®šä½æµç¨‹çš„ç²¾åº¦å’Œå¯é æ€§ã€‚</p>
</blockquote>
<p>ä¸ºäº†éªŒè¯ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†FZIä¿¡æ¯æŠ€æœ¯ç ”ç©¶ä¸­å¿ƒçš„CoCar NextGenå¹³å°ï¼Œè¯¥å¹³å°æä¾›äº†å¤šç§ä¼ æ„Ÿå™¨æ¨¡å¼å’Œé…ç½®ã€‚CoCar NextGençš„ä¼ æ„Ÿå™¨è®¾ç½®èƒ½å¤Ÿå¯¹ä¸åŒçš„ä¼ æ„Ÿå™¨ç±»å‹è¿›è¡Œå½»åº•åˆ†æã€‚æˆ‘ä»¬çš„è¯„ä¼°åˆ©ç”¨æœ€å…ˆè¿›çš„ç”¨äºç›¸æœºå›¾åƒåˆ†å‰²çš„Depth-Anythingç½‘ç»œï¼Œä»¥åŠç”¨äºæ¿€å…‰é›·è¾¾åˆ†å‰²çš„è‡ªé€‚åº”åˆ†å‰²ç½‘ç»œã€‚ä¸ºäº†å»ºç«‹å¯é çš„åŸºäºæ¿€å…‰é›·è¾¾çš„å®šä½çš„åœ°é¢çœŸå®æƒ…å†µï¼Œæˆ‘ä»¬é‡‡ç”¨äº†å…·æœ‰å®æ—¶åŠ¨æ€æ ¡æ­£çš„å…¨çƒå¯¼èˆªå«æ˜Ÿç³»ç»Ÿï¼ˆGNSSï¼‰è§£å†³æ–¹æ¡ˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨å¾·å›½å¡å°”æ–¯é²å„å¸‚è¿›è¡Œäº†é•¿è¾¾55å…¬é‡Œçš„é©¾é©¶æµ‹è¯•ï¼Œæ¶µç›–äº†å¤šç§ç¯å¢ƒï¼ŒåŒ…æ‹¬åŸåŒºã€å¤šè½¦é“å…¬è·¯å’Œä¹¡æ‘é«˜é€Ÿå…¬è·¯ã€‚è¿™ç§å¤šæ¨¡å¼æ–¹æ³•ä¸ºæ›´å¯é å’Œç²¾ç¡®çš„è‡ªä¸»å¯¼èˆªç³»ç»Ÿé“ºå¹³äº†é“è·¯ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚çš„ç°å®ç¯å¢ƒä¸­ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.20486v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªç»“åˆè¯­ä¹‰ç›¸æœºæ•°æ®å’Œæ¿€å…‰é›·è¾¾åˆ†å‰²çš„æ–¹æ³•ï¼Œä»¥è§£å†³è¯­ä¹‰åˆ†å‰²æ¿€å…‰é›·è¾¾æ•°æ®é¢ä¸´çš„æŒ‘æˆ˜ã€‚é€šè¿‡å°†æ¿€å…‰é›·è¾¾ç‚¹æŠ•å½±åˆ°è¯­ä¹‰åˆ†å‰²ç©ºé—´çš„ç›¸æœºä¸­ï¼Œè¯¥æ–¹æ³•æé«˜äº†æ¿€å…‰é›·è¾¾å®šä½ç®¡é“çš„ç²¾åº¦å’Œå¯é æ€§ã€‚å®éªŒé‡‡ç”¨FZIä¿¡æ¯æŠ€æœ¯ç ”ç©¶ä¸­å¿ƒçš„CoCar NextGenå¹³å°ï¼Œè¿›è¡Œå¤šä¼ æ„Ÿå™¨æ¨¡æ€å’Œé…ç½®çš„å…¨é¢åˆ†æéªŒè¯ã€‚è¯¥æ–¹æ³•åœ¨çœŸå®å¤æ‚ç¯å¢ƒä¸­è¡¨ç°è‰¯å¥½ï¼Œä¸ºå¤šæ¨¡å¼å¯é ç²¾ç¡®çš„è‡ªä¸»å¯¼èˆªç³»ç»Ÿé“ºå¹³äº†é“è·¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­ä¹‰åˆ†å‰²æ¿€å…‰é›·è¾¾æ•°æ®å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œç‰¹åˆ«æ˜¯å¤„ç†å¤šæ ·ä¼ æ„Ÿå™¨ç±»å‹å’Œé…ç½®æ—¶ã€‚</li>
<li>æ•´åˆè¯­ä¹‰ä¿¡æ¯èƒ½æ˜¾è‘—æé«˜æ¿€å…‰é›·è¾¾å®šä½æŠ€æœ¯çš„å‡†ç¡®æ€§å’Œç¨³å¥æ€§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§ç»“åˆè¯­ä¹‰ç›¸æœºæ•°æ®å’Œæ¿€å…‰é›·è¾¾åˆ†å‰²çš„æ–¹æ³•æ¥è§£å†³æ­¤æŒ‘æˆ˜ã€‚</li>
<li>æ–¹æ³•é€šè¿‡å°†æ¿€å…‰é›·è¾¾ç‚¹æŠ•å½±åˆ°è¯­ä¹‰åˆ†å‰²çš„ç›¸æœºç©ºé—´æ¥æé«˜æ¿€å…‰é›·è¾¾å®šä½ç²¾åº¦å’Œå¯é æ€§ã€‚</li>
<li>åˆ©ç”¨CoCar NextGenå¹³å°éªŒè¯äº†è¯¥æ–¹æ³•çš„æ€§èƒ½ï¼Œè¯¥å¹³å°å…·å¤‡å¤šç§ä¼ æ„Ÿå™¨æ¨¡å¼å’Œé…ç½®ã€‚</li>
<li>é‡‡ç”¨æœ€å…ˆè¿›çš„æ·±åº¦ç›¸æœºå›¾åƒåˆ†å‰²ç½‘ç»œè‡ªé€‚åº”æ¿€å…‰é›·è¾¾åˆ†å‰²ç½‘ç»œè¿›è¡Œè¯„ä¼°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.20486">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-254bc0e679af4cb373e0c92476724e8f" align="middle">
<img src="https://picx.zhimg.com/v2-0f94ae90d4da38874d60943e7fa889b5" align="middle">
<img src="https://picx.zhimg.com/v2-ba8b80e4eef2314f681c250325609276" align="middle">
<img src="https://picx.zhimg.com/v2-09b6e89b716abd1141632a1dfa02ff5e" align="middle">
<img src="https://picx.zhimg.com/v2-6c81c98cf223d7268198bb4b5a44b595" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="A-Contrastive-Learning-Framework-for-Breast-Cancer-Detection"><a href="#A-Contrastive-Learning-Framework-for-Breast-Cancer-Detection" class="headerlink" title="A Contrastive Learning Framework for Breast Cancer Detection"></a>A Contrastive Learning Framework for Breast Cancer Detection</h2><p><strong>Authors:Samia Saeed, Khuram Naveed</strong></p>
<p>Breast cancer, the second leading cause of cancer-related deaths globally, accounts for a quarter of all cancer cases [1]. To lower this death rate, it is crucial to detect tumors early, as early-stage detection significantly improves treatment outcomes. Advances in non-invasive imaging techniques have made early detection possible through computer-aided detection (CAD) systems which rely on traditional image analysis to identify malignancies. However, there is a growing shift towards deep learning methods due to their superior effectiveness. Despite their potential, deep learning methods often struggle with accuracy due to the limited availability of large-labeled datasets for training. To address this issue, our study introduces a Contrastive Learning (CL) framework, which excels with smaller labeled datasets. In this regard, we train Resnet-50 in semi supervised CL approach using similarity index on a large amount of unlabeled mammogram data. In this regard, we use various augmentation and transformations which help improve the performance of our approach. Finally, we tune our model on a small set of labelled data that outperforms the existing state of the art. Specifically, we observed a 96.7% accuracy in detecting breast cancer on benchmark datasets INbreast and MIAS. </p>
<blockquote>
<p>ä¹³è…ºç™Œæ˜¯å…¨çƒç¬¬äºŒå¤§ç™Œç—‡è‡´æ­»åŸå› ï¼Œå æ‰€æœ‰ç™Œç—‡ç—…ä¾‹çš„å››åˆ†ä¹‹ä¸€ï¼ˆæ¥æºï¼š1ï¼‰ã€‚ä¸ºäº†é™ä½æ­»äº¡ç‡ï¼Œæ—©æœŸå‘ç°è‚¿ç˜¤è‡³å…³é‡è¦ï¼Œå› ä¸ºæ—©æœŸå‘ç°èƒ½å¤Ÿæ˜¾è‘—æé«˜æ²»ç–—æ•ˆæœã€‚éä¾µå…¥æ€§æˆåƒæŠ€æœ¯çš„è¿›æ­¥å·²ç»èƒ½å¤Ÿé€šè¿‡è®¡ç®—æœºè¾…åŠ©æ£€æµ‹ï¼ˆCADï¼‰ç³»ç»Ÿè¿›è¡Œæ—©æœŸæ£€æµ‹ï¼Œè¿™äº›ç³»ç»Ÿä¾èµ–äºä¼ ç»Ÿå›¾åƒåˆ†ææ¥è¯†åˆ«æ¶æ€§è‚¿ç˜¤ã€‚ç„¶è€Œï¼Œç”±äºæ·±åº¦å­¦ä¹ æ–¹æ³•å…·æœ‰å‡ºè‰²çš„æ•ˆæœï¼Œè¶Šæ¥è¶Šå¤šçš„ç ”ç©¶å¼€å§‹è½¬å‘æ·±åº¦å­¦ä¹ æ–¹æ³•ã€‚å°½ç®¡å…·æœ‰æ½œåŠ›ï¼Œä½†ç”±äºç”¨äºè®­ç»ƒçš„å¤§å‹æ ‡è®°æ•°æ®é›†æœ‰é™ï¼Œæ·±åº¦å­¦ä¹ æ–¹æ³•å¾€å¾€åœ¨å‡†ç¡®æ€§æ–¹é¢é‡åˆ°å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬çš„ç ”ç©¶å¼•å…¥äº†å¯¹æ¯”å­¦ä¹ ï¼ˆCLï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åœ¨è¾ƒå°çš„æ ‡è®°æ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ã€‚åœ¨è¿™æ–¹é¢ï¼Œæˆ‘ä»¬ä½¿ç”¨å¤§é‡çš„æœªæ ‡è®°çš„ä¹³è…ºXçº¿æ‘„å½±æ•°æ®ä»¥ç›¸ä¼¼åº¦ä¸ºåŸºå‡†åœ¨ä¸€ç§åŠç›‘ç£CLæ–¹æ³•çš„æŒ‡å¯¼ä¸‹è®­ç»ƒäº†Resnet-50ç½‘ç»œæ¨¡å‹ã€‚åœ¨è¿™ä¸€æ¡†æ¶å†…ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†å¤šç§æ•°æ®å¢å¼ºå’Œè½¬æ¢æ‰‹æ®µæ¥å¸®åŠ©æå‡æ–¹æ³•çš„æ€§èƒ½ã€‚æœ€ç»ˆæˆ‘ä»¬åœ¨å°‘é‡æ ‡è®°æ•°æ®ä¸Šå¯¹æ¨¡å‹è¿›è¡Œäº†å¾®è°ƒï¼Œè¶…è¶Šäº†ç°æœ‰çš„æŠ€æœ¯æ°´å¹³ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åœ¨åŸºå‡†æ•°æ®é›†INbreastå’ŒMIASä¸Šè§‚å¯Ÿåˆ°96.7%çš„ä¹³è…ºç™Œæ£€æµ‹å‡†ç¡®ç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.20474v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¹³è…ºç™Œä½œä¸ºå…¨çƒç¬¬äºŒå¤§ç™Œç—‡è‡´æ­»åŸå› ï¼Œæ—©æœŸæ£€æµ‹çš„é‡è¦æ€§ä»¥åŠéä¾µå…¥æ€§æˆåƒæŠ€æœ¯åœ¨ä¹³è…ºç™Œæ—©æœŸæ£€æµ‹ä¸­çš„åº”ç”¨ã€‚ç ”ç©¶å¼•å…¥äº†ä¸€ç§åŸºäºå¯¹æ¯”å­¦ä¹ ï¼ˆCLï¼‰çš„æ¡†æ¶ï¼Œä½¿ç”¨åŠç›‘ç£CLæ–¹æ³•å’Œç›¸ä¼¼åº¦æŒ‡æ•°åœ¨å¤§é‡æœªæ ‡è®°çš„ä¹³è…ºXçº¿å½±åƒæ•°æ®ä¸Šè®­ç»ƒResnet-50æ¨¡å‹ï¼Œèƒ½å¤Ÿæé«˜åœ¨è¾ƒå°æ ‡è®°æ•°æ®é›†ä¸Šçš„å‡†ç¡®æ€§ã€‚æœ€ç»ˆï¼Œåœ¨åŸºå‡†æ•°æ®é›†INbreastå’ŒMIASä¸Šï¼Œæ¨¡å‹æ£€æµ‹ä¹³è…ºç™Œçš„å‡†ç¡®åº¦è¾¾åˆ°96.7%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¹³è…ºç™Œæ˜¯å…¨çƒç¬¬äºŒå¤§ç™Œç—‡è‡´æ­»åŸå› ï¼Œæ—©æœŸæ£€æµ‹å¯¹é™ä½æ­»äº¡ç‡è‡³å…³é‡è¦ã€‚</li>
<li>éä¾µå…¥æ€§æˆåƒæŠ€æœ¯å’Œè®¡ç®—æœºè¾…åŠ©æ£€æµ‹ï¼ˆCADï¼‰ç³»ç»Ÿåœ¨æ—©æœŸæ£€æµ‹ä¹³è…ºç™Œä¸­èµ·åˆ°é‡è¦ä½œç”¨ã€‚</li>
<li>æ·±åº¦å­¦ä¹ æ–¹æ³•çš„å¼•å…¥æé«˜äº†ä¹³è…ºç™Œæ£€æµ‹çš„å‡†ç¡®æ€§ï¼Œä½†å—é™äºå¤§å‹æ ‡è®°æ•°æ®é›†çš„å¯è·å¾—æ€§ã€‚</li>
<li>å¯¹æ¯”å­¦ä¹ ï¼ˆCLï¼‰æ¡†æ¶è¢«å¼•å…¥åˆ°è§£å†³æ ‡è®°æ•°æ®é›†æœ‰é™çš„é—®é¢˜ï¼Œè¡¨ç°ä¼˜ç§€ã€‚</li>
<li>ä½¿ç”¨åŠç›‘ç£CLæ–¹æ³•å’Œç›¸ä¼¼åº¦æŒ‡æ•°åœ¨å¤§é‡æœªæ ‡è®°çš„ä¹³è…ºXçº¿å½±åƒæ•°æ®ä¸Šè®­ç»ƒResnet-50æ¨¡å‹ã€‚</li>
<li>é€šè¿‡æ•°æ®å¢å¼ºå’Œå˜æ¢æŠ€æœ¯æé«˜äº†æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.20474">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-75022ea54328eb13de5394195779f1d3" align="middle">
<img src="https://picx.zhimg.com/v2-2ad3dfd2cad1be4896d1379dd9899151" align="middle">
<img src="https://picx.zhimg.com/v2-ee14cf817689358ca85d52c9593a4f89" align="middle">
<img src="https://picx.zhimg.com/v2-bd321a93da0efb51fc1ab0d497c28f6e" align="middle">
<img src="https://picx.zhimg.com/v2-7bd8b5779fe5dc8ca12771d2861c429c" align="middle">
<img src="https://picx.zhimg.com/v2-b39be939927eaaf03d97c15883aa2141" align="middle">
<img src="https://picx.zhimg.com/v2-88aa7ade756ce48097340916621eeed1" align="middle">
<img src="https://picx.zhimg.com/v2-4df0c2b97dcd5e143d7644b394c9a650" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="HiPerformer-A-High-Performance-Global-Local-Segmentation-Model-with-Modular-Hierarchical-Fusion-Strategy"><a href="#HiPerformer-A-High-Performance-Global-Local-Segmentation-Model-with-Modular-Hierarchical-Fusion-Strategy" class="headerlink" title="HiPerformer: A High-Performance Global-Local Segmentation Model with   Modular Hierarchical Fusion Strategy"></a>HiPerformer: A High-Performance Global-Local Segmentation Model with   Modular Hierarchical Fusion Strategy</h2><p><strong>Authors:Dayu Tan, Zhenpeng Xu, Yansen Su, Xin Peng, Chunhou Zheng, Weimin Zhong</strong></p>
<p>Both local details and global context are crucial in medical image segmentation, and effectively integrating them is essential for achieving high accuracy. However, existing mainstream methods based on CNN-Transformer hybrid architectures typically employ simple feature fusion techniques such as serial stacking, endpoint concatenation, or pointwise addition, which struggle to address the inconsistencies between features and are prone to information conflict and loss. To address the aforementioned challenges, we innovatively propose HiPerformer. The encoder of HiPerformer employs a novel modular hierarchical architecture that dynamically fuses multi-source features in parallel, enabling layer-wise deep integration of heterogeneous information. The modular hierarchical design not only retains the independent modeling capability of each branch in the encoder, but also ensures sufficient information transfer between layers, effectively avoiding the degradation of features and information loss that come with traditional stacking methods. Furthermore, we design a Local-Global Feature Fusion (LGFF) module to achieve precise and efficient integration of local details and global semantic information, effectively alleviating the feature inconsistency problem and resulting in a more comprehensive feature representation. To further enhance multi-scale feature representation capabilities and suppress noise interference, we also propose a Progressive Pyramid Aggregation (PPA) module to replace traditional skip connections. Experiments on eleven public datasets demonstrate that the proposed method outperforms existing segmentation techniques, demonstrating higher segmentation accuracy and robustness. The code is available at <a target="_blank" rel="noopener" href="https://github.com/xzphappy/HiPerformer">https://github.com/xzphappy/HiPerformer</a>. </p>
<blockquote>
<p>åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­ï¼Œå±€éƒ¨ç»†èŠ‚å’Œå…¨å±€ä¸Šä¸‹æ–‡éƒ½è‡³å…³é‡è¦ï¼Œæœ‰æ•ˆåœ°å°†å®ƒä»¬ç»“åˆèµ·æ¥å¯¹äºå®ç°é«˜ç²¾åº¦è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ä¸»æµæ–¹æ³•ä¸»è¦åŸºäºCNN-Transformeræ··åˆæ¶æ„ï¼Œé€šå¸¸é‡‡ç”¨ç®€å•çš„ç‰¹å¾èåˆæŠ€æœ¯ï¼Œå¦‚ä¸²è¡Œå †å ã€ç«¯ç‚¹æ‹¼æ¥æˆ–é€ç‚¹æ·»åŠ ã€‚è¿™äº›æ–¹æ³•åœ¨å¤„ç†ç‰¹å¾ä¹‹é—´çš„ä¸ä¸€è‡´æ€§æ—¶é‡åˆ°å›°éš¾ï¼Œå®¹æ˜“å‡ºç°ä¿¡æ¯å†²çªå’ŒæŸå¤±ã€‚ä¸ºäº†è§£å†³ä¸Šè¿°æŒ‘æˆ˜ï¼Œæˆ‘ä»¬åˆ›æ–°åœ°æå‡ºäº†HiPerformerã€‚HiPerformerçš„ç¼–ç å™¨é‡‡ç”¨æ–°å‹æ¨¡å—åŒ–åˆ†å±‚æ¶æ„ï¼ŒåŠ¨æ€å¹¶è¡Œèåˆå¤šæºç‰¹å¾ï¼Œå®ç°åˆ†å±‚æ·±åº¦é›†æˆå¼‚è´¨ä¿¡æ¯ã€‚æ¨¡å—åŒ–åˆ†å±‚è®¾è®¡ä¸ä»…ä¿ç•™äº†ç¼–ç å™¨ä¸­æ¯ä¸ªåˆ†æ”¯çš„ç‹¬ç«‹å»ºæ¨¡èƒ½åŠ›ï¼Œè€Œä¸”ç¡®ä¿äº†å±‚ä¹‹é—´çš„ä¿¡æ¯å……åˆ†ä¼ é€’ï¼Œæœ‰æ•ˆé¿å…äº†ä¼ ç»Ÿå †å æ–¹æ³•å¸¦æ¥çš„ç‰¹å¾é€€åŒ–å’Œä¿¡æ¯æŸå¤±ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªå±€éƒ¨-å…¨å±€ç‰¹å¾èåˆï¼ˆLGFFï¼‰æ¨¡å—ï¼Œä»¥å®ç°å±€éƒ¨ç»†èŠ‚å’Œå…¨å±€è¯­ä¹‰ä¿¡æ¯çš„ç²¾ç¡®é«˜æ•ˆèåˆï¼Œæœ‰æ•ˆç¼“è§£äº†ç‰¹å¾ä¸ä¸€è‡´é—®é¢˜ï¼Œå¹¶äº§ç”Ÿäº†æ›´å…¨é¢çš„ç‰¹å¾è¡¨ç¤ºã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜å¤šå°ºåº¦ç‰¹å¾è¡¨ç¤ºèƒ½åŠ›å¹¶æŠ‘åˆ¶å™ªå£°å¹²æ‰°ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†æ¸è¿›é‡‘å­—å¡”èšåˆï¼ˆPPAï¼‰æ¨¡å—æ¥æ›¿ä»£ä¼ ç»Ÿçš„è·³è·ƒè¿æ¥ã€‚åœ¨11ä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•ä¼˜äºç°æœ‰çš„åˆ†å‰²æŠ€æœ¯ï¼Œè¡¨ç°å‡ºæ›´é«˜çš„åˆ†å‰²ç²¾åº¦å’Œç¨³å¥æ€§ã€‚ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/xzphappy/HiPerformer%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/xzphappy/HiPerformerè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.20280v1">PDF</a> </p>
<p><strong>Summary</strong><br>     åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­ï¼Œå±€éƒ¨ç»†èŠ‚å’Œå…¨å±€ä¸Šä¸‹æ–‡éƒ½è‡³å…³é‡è¦ã€‚ç°æœ‰ä¸»æµæ–¹æ³•å¦‚CNN-Transformeræ··åˆæ¶æ„åœ¨ç‰¹å¾èåˆä¸Šé‡‡ç”¨ç®€å•ç­–ç•¥ï¼Œå¦‚ä¸²è¡Œå †å ã€ç«¯ç‚¹æ‹¼æ¥æˆ–ç‚¹åŠ ï¼Œéš¾ä»¥è§£å†³ç‰¹å¾ä¸ä¸€è‡´é—®é¢˜ï¼Œæ˜“å¯¼è‡´ä¿¡æ¯å†²çªå’ŒæŸå¤±ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬åˆ›æ–°æå‡ºHiPerformerï¼Œå…¶ç¼–ç å™¨é‡‡ç”¨æ–°å‹æ¨¡å—åŒ–å±‚æ¬¡ç»“æ„ï¼ŒåŠ¨æ€èåˆå¤šæºç‰¹å¾ï¼Œå®ç°å±‚çº§æ·±åº¦é›†æˆå¼‚è´¨ä¿¡æ¯ã€‚åŒæ—¶ï¼Œè®¾è®¡å±€éƒ¨å…¨å±€ç‰¹å¾èåˆæ¨¡å—ï¼Œç²¾å‡†é«˜æ•ˆæ•´åˆå±€éƒ¨ç»†èŠ‚å’Œå…¨å±€è¯­ä¹‰ä¿¡æ¯ï¼Œç¼“è§£ç‰¹å¾ä¸ä¸€è‡´é—®é¢˜ã€‚ä¸ºå¢å¼ºå¤šå°ºåº¦ç‰¹å¾è¡¨ç¤ºèƒ½åŠ›å’ŒæŠ‘åˆ¶å™ªå£°å¹²æ‰°ï¼Œæˆ‘ä»¬è¿˜æå‡ºæ¸è¿›é‡‘å­—å¡”èšåˆæ¨¡å—æ›¿ä»£ä¼ ç»Ÿè·³è·ƒè¿æ¥ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªå…¬å…±æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå…·æœ‰æ›´é«˜çš„åˆ†å‰²ç²¾åº¦å’Œé²æ£’æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†å‰²éœ€å…¼é¡¾å±€éƒ¨ç»†èŠ‚å’Œå…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚</li>
<li>ç°æœ‰CNN-Transformeræ··åˆæ¶æ„åœ¨ç‰¹å¾èåˆä¸Šå­˜ä¸è¶³ï¼Œå¦‚ä¿¡æ¯ä¸ä¸€è‡´ã€ä¿¡æ¯æŸå¤±ç­‰ã€‚</li>
<li>HiPerformeré€šè¿‡æ¨¡å—åŒ–å±‚æ¬¡ç»“æ„åŠ¨æ€èåˆå¤šæºç‰¹å¾ï¼Œæé«˜ä¿¡æ¯é›†æˆæ•ˆç‡ã€‚</li>
<li>HiPerformerè®¾è®¡å±€éƒ¨å…¨å±€ç‰¹å¾èåˆæ¨¡å—ï¼Œè§£å†³ç‰¹å¾ä¸ä¸€è‡´é—®é¢˜ï¼Œå®ç°ç²¾å‡†ä¿¡æ¯æ•´åˆã€‚</li>
<li>æ¸è¿›é‡‘å­—å¡”èšåˆæ¨¡å—å¢å¼ºå¤šå°ºåº¦ç‰¹å¾è¡¨ç¤ºï¼ŒæŠ‘åˆ¶å™ªå£°å¹²æ‰°ã€‚</li>
<li>å®éªŒè¯æ˜HiPerformeråœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œåˆ†å‰²ç²¾åº¦å’Œé²æ£’æ€§é«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.20280">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a9ecdaac6589bbaef69a20aa55e9e959" align="middle">
<img src="https://picx.zhimg.com/v2-151038066e0f02975aa3698d89ab1c99" align="middle">
<img src="https://picx.zhimg.com/v2-cf3ce88201cd0c50c28b042c06cb744f" align="middle">
<img src="https://picx.zhimg.com/v2-d3ebb4fbbcb8ec2c688a9ddfe034a21e" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="A-co-evolving-agentic-AI-system-for-medical-imaging-analysis"><a href="#A-co-evolving-agentic-AI-system-for-medical-imaging-analysis" class="headerlink" title="A co-evolving agentic AI system for medical imaging analysis"></a>A co-evolving agentic AI system for medical imaging analysis</h2><p><strong>Authors:Songhao Li, Jonathan Xu, Tiancheng Bao, Yuxuan Liu, Yuchen Liu, Yihang Liu, Lilin Wang, Wenhui Lei, Sheng Wang, Yinuo Xu, Yan Cui, Jialu Yao, Shunsuke Koga, Zhi Huang</strong></p>
<p>Agentic AI is rapidly advancing in healthcare and biomedical research. However, in medical image analysis, their performance and adoption remain limited due to the lack of a robust ecosystem, insufficient toolsets, and the absence of real-time interactive expert feedback. Here we present â€œTissueLabâ€, a co-evolving agentic AI system that allows researchers to ask direct questions, automatically plan and generate explainable workflows, and conduct real-time analyses where experts can visualize intermediate results and refine them. TissueLab integrates tool factories across pathology, radiology, and spatial omics domains. By standardizing inputs, outputs, and capabilities of diverse tools, the system determines when and how to invoke them to address research and clinical questions. Across diverse tasks with clinically meaningful quantifications that inform staging, prognosis, and treatment planning, TissueLab achieves state-of-the-art performance compared with end-to-end vision-language models (VLMs) and other agentic AI systems such as GPT-5. Moreover, TissueLab continuously learns from clinicians, evolving toward improved classifiers and more effective decision strategies. With active learning, it delivers accurate results in unseen disease contexts within minutes, without requiring massive datasets or prolonged retraining. Released as a sustainable open-source ecosystem, TissueLab aims to accelerate computational research and translational adoption in medical imaging while establishing a foundation for the next generation of medical AI. </p>
<blockquote>
<p>åŒ»å­¦äººå·¥æ™ºèƒ½ï¼ˆAgentic AIï¼‰åœ¨åŒ»ç–—ä¿å¥å’Œç”Ÿç‰©åŒ»å­¦ç ”ç©¶é¢†åŸŸæ­£å¿«é€Ÿå‘å±•ã€‚ç„¶è€Œï¼Œåœ¨åŒ»å­¦å›¾åƒåˆ†ææ–¹é¢ï¼Œå…¶æ€§èƒ½å’Œé‡‡ç”¨ç‡å—åˆ°ç¼ºä¹ç¨³å¥ç”Ÿæ€ç³»ç»Ÿã€å·¥å…·é›†ä¸è¶³ä»¥åŠç¼ºä¹å®æ—¶äº¤äº’ä¸“å®¶åé¦ˆçš„é™åˆ¶ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æ¨å‡ºâ€œTissueLabâ€ï¼Œè¿™æ˜¯ä¸€ä¸ªååŒè¿›åŒ–çš„åŒ»å­¦äººå·¥æ™ºèƒ½ç³»ç»Ÿï¼Œå…è®¸ç ”ç©¶äººå‘˜æå‡ºç›´æ¥é—®é¢˜ã€è‡ªåŠ¨è§„åˆ’å’Œç”Ÿæˆå¯è§£é‡Šçš„å·¥ä½œæµç¨‹ï¼Œå¹¶è¿›è¡Œå®æ—¶åˆ†æï¼Œä¸“å®¶å¯ä»¥å¯è§†åŒ–ä¸­é—´ç»“æœå¹¶è¿›è¡Œä¼˜åŒ–ã€‚TissueLabæ•´åˆäº†ç—…ç†å­¦ã€æ”¾å°„å­¦å’Œç©ºé—´ç»„å­¦é¢†åŸŸçš„å·¥å…·å·¥å‚ã€‚é€šè¿‡æ ‡å‡†åŒ–å„ç§å·¥å…·çš„è¾“å‡ºã€è¾“å…¥å’Œèƒ½åŠ›ï¼Œè¯¥ç³»ç»Ÿç¡®å®šä½•æ—¶ä»¥åŠå¦‚ä½•è°ƒç”¨å®ƒä»¬æ¥è§£å†³ç ”ç©¶å’Œä¸´åºŠé—®é¢˜ã€‚åœ¨å…·æœ‰ä¸´åºŠæ„ä¹‰çš„é‡åŒ–ä»»åŠ¡çš„å¤šæ ·åŒ–æ–¹é¢ï¼Œè¿™äº›ä»»åŠ¡å¯ä»¥æä¾›åˆ†æœŸã€é¢„åå’Œæ²»ç–—è®¡åˆ’çš„ä¿¡æ¯ï¼ŒTissueLabä¸ç«¯åˆ°ç«¯è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å’Œå…¶ä»–åŒ»å­¦äººå·¥æ™ºèƒ½ç³»ç»Ÿç›¸æ¯”ï¼Œå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒTissueLabèƒ½å¤Ÿä»ä¸´åºŠåŒ»ç”Ÿé‚£é‡ŒæŒç»­å­¦ä¹ ï¼Œæœç€æ›´é«˜çº§çš„åˆ†ç±»å™¨å’Œæ›´æœ‰æ•ˆçš„å†³ç­–ç­–ç•¥å‘å±•ã€‚é€šè¿‡ä¸»åŠ¨å­¦ä¹ ï¼Œå®ƒåœ¨æœªè§ç–¾ç—…èƒŒæ™¯ä¸‹å‡ åˆ†é’Ÿå†…æä¾›å‡†ç¡®ç»“æœï¼Œæ— éœ€å¤§è§„æ¨¡æ•°æ®é›†æˆ–é•¿æœŸé‡æ–°è®­ç»ƒã€‚ä½œä¸ºå¯æŒç»­çš„å¼€æºç”Ÿæ€ç³»ç»Ÿå‘å¸ƒï¼ŒTissueLabæ—¨åœ¨åŠ é€ŸåŒ»å­¦æˆåƒçš„è®¡ç®—ç ”ç©¶å’Œå®é™…åº”ç”¨ï¼ŒåŒæ—¶ä¸ºä¸‹ä¸€ä»£åŒ»å­¦äººå·¥æ™ºèƒ½å¥ å®šåŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.20279v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿™æ˜¯ä¸€ç¯‡å…³äºAgentic AIåœ¨åŒ»ç–—å¥åº·å’Œç”Ÿç‰©åŒ»å­¦ç ”ç©¶ä¸­çš„è¿›å±•ä¸æŒ‘æˆ˜çš„æ–‡ç« ã€‚æ–‡ç« ä»‹ç»äº†TissueLabç³»ç»Ÿï¼Œå®ƒæ˜¯ä¸€ä¸ªååŒè¿›åŒ–çš„agentic AIç³»ç»Ÿï¼Œç”¨äºåŒ»å­¦å›¾åƒåˆ†æã€‚è¯¥ç³»ç»Ÿå…è®¸ç ”ç©¶äººå‘˜ç›´æ¥æé—®ã€è‡ªåŠ¨è§„åˆ’ç”Ÿæˆå¯è§£é‡Šçš„å·¥ä½œæµï¼Œå¹¶è¿›è¡Œå®æ—¶åˆ†æã€‚TissueLabé›†æˆäº†è·¨ç—…ç†å­¦ã€æ”¾å°„å­¦å’Œç©ºé—´ç»„å­¦é¢†åŸŸçš„å·¥å…·å·¥å‚ï¼Œé€šè¿‡æ ‡å‡†åŒ–å·¥å…·è¾“å…¥ã€è¾“å‡ºå’Œèƒ½åŠ›ï¼Œç¡®å®šä½•æ—¶ä»¥åŠå¦‚ä½•è°ƒç”¨å®ƒä»¬æ¥è§£å†³ç ”ç©¶å’Œä¸´åºŠé—®é¢˜ã€‚ä¸ä¼ ç»Ÿçš„ç«¯åˆ°ç«¯è§†è§‰è¯­è¨€æ¨¡å‹å’Œå…¶ä»–agentic AIç³»ç»Ÿç›¸æ¯”ï¼ŒTissueLabåœ¨å…·æœ‰ä¸´åºŠæ„ä¹‰çš„ä»»åŠ¡ä¸Šå®ç°äº†å“è¶Šæ€§èƒ½ï¼Œå¹¶ä¸”èƒ½å¤Ÿä»ä¸´åºŠåŒ»ç”Ÿé‚£é‡ŒæŒç»­å­¦ä¹ ï¼Œä¸æ–­è¿›åŒ–åˆ†ç±»å™¨å’Œå†³ç­–ç­–ç•¥ã€‚ä½œä¸ºå¯æŒç»­çš„å¼€æºç”Ÿæ€ç³»ç»Ÿï¼ŒTissueLabæ—¨åœ¨åŠ é€ŸåŒ»å­¦æˆåƒçš„è®¡ç®—ç ”ç©¶å’Œç¿»è¯‘åº”ç”¨ï¼Œä¸ºä¸‹ä¸€ä»£åŒ»ç–—äººå·¥æ™ºèƒ½å¥ å®šåŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Agentic AIåœ¨åŒ»ç–—å¥åº·å’Œç”Ÿç‰©åŒ»å­¦ç ”ç©¶ä¸­çš„åº”ç”¨æ­£åœ¨å¿«é€Ÿå‘å±•ï¼Œä½†åœ¨åŒ»å­¦å›¾åƒåˆ†æé¢†åŸŸä»é¢ä¸´ç”Ÿæ€ç³»ç»Ÿä¸å¥å…¨ã€å·¥å…·é›†ä¸è¶³å’Œç¼ºä¹å®æ—¶äº’åŠ¨ä¸“å®¶åé¦ˆçš„æŒ‘æˆ˜ã€‚</li>
<li>TissueLabæ˜¯ä¸€ä¸ªååŒè¿›åŒ–çš„agentic AIç³»ç»Ÿï¼Œè§£å†³äº†ä¸Šè¿°é—®é¢˜ï¼Œå…è®¸ç ”ç©¶äººå‘˜ç›´æ¥äº¤äº’ã€è‡ªåŠ¨è§„åˆ’å·¥ä½œæµç¨‹ï¼Œå¹¶è¿›è¡Œå®æ—¶åˆ†æã€‚</li>
<li>TissueLabé›†æˆäº†è·¨ç—…ç†å­¦ã€æ”¾å°„å­¦å’Œç©ºé—´ç»„å­¦é¢†åŸŸçš„å·¥å…·ï¼Œé€šè¿‡æ ‡å‡†åŒ–å·¥å…·è¾“å…¥ã€è¾“å‡ºå’Œèƒ½åŠ›æ¥è§£å†³ä¸´åºŠå’Œç ”ç©¶é—®é¢˜ã€‚</li>
<li>TissueLabå®ç°äº†å“è¶Šçš„æ€§èƒ½ï¼Œä¸ä¼ ç»Ÿè§†è§‰è¯­è¨€æ¨¡å‹å’Œå…¶ä»–agentic AIç³»ç»Ÿç›¸æ¯”ï¼Œå…¶åœ¨å…·æœ‰ä¸´åºŠæ„ä¹‰çš„ä»»åŠ¡ä¸Šè¡¨ç°æ›´ä½³ã€‚</li>
<li>TissueLabèƒ½ä»ä¸´åºŠåŒ»ç”Ÿé‚£é‡Œå­¦ä¹ å¹¶æŒç»­è¿›åŒ–ï¼Œé€šè¿‡æ´»è·ƒçš„å­¦ä¹ èƒ½åŠ›ï¼Œåœ¨æœªçŸ¥ç–¾ç—…èƒŒæ™¯ä¸‹å¿«é€Ÿæä¾›å‡†ç¡®ç»“æœï¼Œæ— éœ€å¤§è§„æ¨¡æ•°æ®é›†æˆ–é•¿æœŸå†è®­ç»ƒã€‚</li>
<li>TissueLabè¢«å‘å¸ƒä¸ºä¸€ä¸ªå¯æŒç»­çš„å¼€æºç”Ÿæ€ç³»ç»Ÿï¼Œæ—¨åœ¨åŠ é€ŸåŒ»å­¦æˆåƒçš„è®¡ç®—ç ”ç©¶å’Œç¿»è¯‘åº”ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.20279">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-60c35fdd2670f99cdff51a0e5250b508" align="middle">
<img src="https://picx.zhimg.com/v2-71e2df6f42bf113d219b0f621c596adb" align="middle">
<img src="https://picx.zhimg.com/v2-388326fcfef5e3546227e4372a350e2d" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="SHMoAReg-Spark-Deformable-Image-Registration-via-Spatial-Heterogeneous-Mixture-of-Experts-and-Attention-Heads"><a href="#SHMoAReg-Spark-Deformable-Image-Registration-via-Spatial-Heterogeneous-Mixture-of-Experts-and-Attention-Heads" class="headerlink" title="SHMoAReg: Spark Deformable Image Registration via Spatial Heterogeneous   Mixture of Experts and Attention Heads"></a>SHMoAReg: Spark Deformable Image Registration via Spatial Heterogeneous   Mixture of Experts and Attention Heads</h2><p><strong>Authors:Yuxi Zheng, Jianhui Feng, Tianran Li, Marius Staring, Yuchuan Qiao</strong></p>
<p>Encoder-Decoder architectures are widely used in deep learning-based Deformable Image Registration (DIR), where the encoder extracts multi-scale features and the decoder predicts deformation fields by recovering spatial locations. However, current methods lack specialized extraction of features (that are useful for registration) and predict deformation jointly and homogeneously in all three directions. In this paper, we propose a novel expert-guided DIR network with Mixture of Experts (MoE) mechanism applied in both encoder and decoder, named SHMoAReg. Specifically, we incorporate Mixture of Attention heads (MoA) into encoder layers, while Spatial Heterogeneous Mixture of Experts (SHMoE) into the decoder layers. The MoA enhances the specialization of feature extraction by dynamically selecting the optimal combination of attention heads for each image token. Meanwhile, the SHMoE predicts deformation fields heterogeneously in three directions for each voxel using experts with varying kernel sizes. Extensive experiments conducted on two publicly available datasets show consistent improvements over various methods, with a notable increase from 60.58% to 65.58% in Dice score for the abdominal CT dataset. Furthermore, SHMoAReg enhances model interpretability by differentiating expertsâ€™ utilities across&#x2F;within different resolution layers. To the best of our knowledge, we are the first to introduce MoE mechanism into DIR tasks. The code will be released soon. </p>
<blockquote>
<p>åŸºäºæ·±åº¦å­¦ä¹ çš„å¯å˜å½¢å›¾åƒé…å‡†ï¼ˆDIRï¼‰ä¸­å¹¿æ³›ä½¿ç”¨Encoder-Decoderæ¶æ„ï¼Œå…¶ä¸­ç¼–ç å™¨æå–å¤šå°ºåº¦ç‰¹å¾ï¼Œè§£ç å™¨é€šè¿‡æ¢å¤ç©ºé—´ä½ç½®æ¥é¢„æµ‹å˜å½¢åœºã€‚ç„¶è€Œï¼Œå½“å‰çš„æ–¹æ³•ç¼ºä¹ç‰¹å¾çš„ä¸“ä¸šæå–ï¼ˆè¿™äº›ç‰¹å¾å¯¹äºé…å‡†æ˜¯æœ‰ç”¨çš„ï¼‰ï¼Œå¹¶ä¸”åœ¨æ‰€æœ‰ä¸‰ä¸ªæ–¹å‘ä¸Šè”åˆé¢„æµ‹å˜å½¢æ˜¯å‡åŒ€çš„ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹ä¸“å®¶æŒ‡å¯¼çš„DIRç½‘ç»œï¼Œè¯¥ç½‘ç»œåœ¨ç¼–ç å™¨å’Œè§£ç å™¨ä¸­å‡åº”ç”¨äº†æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æœºåˆ¶ï¼Œåä¸ºSHMoARegã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åœ¨ç¼–ç å™¨å±‚ä¸­å¼•å…¥äº†æ··åˆæ³¨æ„åŠ›å¤´ï¼ˆMoAï¼‰ï¼Œè€Œåœ¨è§£ç å™¨å±‚ä¸­å¼•å…¥äº†ç©ºé—´å¼‚è´¨æ··åˆä¸“å®¶ï¼ˆSHMoEï¼‰ã€‚MoAé€šè¿‡åŠ¨æ€é€‰æ‹©æ¯ä¸ªå›¾åƒä»¤ç‰Œçš„æ³¨æ„åŠ›å¤´çš„æœ€ä½³ç»„åˆï¼Œå¢å¼ºäº†ç‰¹å¾æå–çš„ä¸“ä¸šæ€§ã€‚åŒæ—¶ï¼ŒSHMoEä½¿ç”¨å…·æœ‰ä¸åŒå†…æ ¸å¤§å°çš„ä¸“å®¶åœ¨æ¯ä¸ªæ–¹å‘ä¸Šå¯¹å˜å½¢åœºè¿›è¡Œå¼‚è´¨é¢„æµ‹ã€‚åœ¨ä¸¤ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œåœ¨å„ç§æ–¹æ³•ä¸Šå‡å–å¾—äº†æŒç»­çš„æ”¹è¿›ï¼Œç‰¹åˆ«æ˜¯åœ¨è…¹éƒ¨CTæ•°æ®é›†ä¸Šçš„Diceå¾—åˆ†ä»60.58%æ˜¾è‘—æé«˜è‡³65.58%ã€‚æ­¤å¤–ï¼ŒSHMoARegé€šè¿‡åœ¨ä¸åŒåˆ†è¾¨ç‡å±‚ä¹‹é—´&#x2F;å†…éƒ¨åŒºåˆ†ä¸“å®¶çš„å®ç”¨æ€§æ¥å¢å¼ºæ¨¡å‹çš„è§£é‡Šæ€§ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œæˆ‘ä»¬æ˜¯é¦–æ¬¡å°†MoEæœºåˆ¶å¼•å…¥DIRä»»åŠ¡çš„ç ”ç©¶è€…ã€‚ä»£ç å³å°†å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.20073v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„æ–°å‹ä¸“å®¶å¼•å¯¼å¯å˜å½¢å›¾åƒé…å‡†ç½‘ç»œï¼ˆSHMoARegï¼‰ï¼Œåœ¨ç¼–ç å™¨å’Œè§£ç å™¨ä¸­åº”ç”¨äº†æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æœºåˆ¶ã€‚è¯¥ç½‘ç»œé€šè¿‡å¼•å…¥æ··åˆæ³¨æ„åŠ›å¤´ï¼ˆMoAï¼‰å¢å¼ºç‰¹å¾æå–çš„ä¸“ä¸šæ€§ï¼Œå¹¶é€šè¿‡ç©ºé—´å¼‚è´¨æ··åˆä¸“å®¶ï¼ˆSHMoEï¼‰å®ç°å„æ–¹å‘ä¸Šçš„å¼‚è´¨æ€§å˜å½¢é¢„æµ‹ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥ç½‘ç»œåœ¨å…¬å¼€æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨è…¹éƒ¨CTæ•°æ®é›†ä¸ŠDiceå¾—åˆ†æé«˜äº†ä»60.58%åˆ°65.58%ã€‚åŒæ—¶ï¼ŒSHMoARegè¿˜æé«˜äº†æ¨¡å‹çš„è§£é‡Šæ€§ã€‚æœ¬æ–‡é¦–æ¬¡å°†MoEæœºåˆ¶å¼•å…¥DIRä»»åŠ¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®ºæ–‡ä»‹ç»äº†åŸºäºæ·±åº¦å­¦ä¹ çš„å¯å˜å½¢å›¾åƒé…å‡†ï¼ˆDIRï¼‰ä¸­ç¼–ç å™¨-è§£ç å™¨æ¶æ„çš„å¹¿æ³›åº”ç”¨ã€‚</li>
<li>å½“å‰æ–¹æ³•ç¼ºä¹é’ˆå¯¹é…å‡†çš„ç‰¹å¾æå–ï¼Œå¹¶å‡åŒ€é¢„æµ‹æ‰€æœ‰æ–¹å‘çš„å˜å½¢ã€‚</li>
<li>è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°å‹ä¸“å®¶å¼•å¯¼çš„DIRç½‘ç»œSHMoARegï¼Œç»“åˆMoAå’ŒSHMoEæœºåˆ¶ä¼˜åŒ–ç‰¹å¾æå–å’Œå˜å½¢é¢„æµ‹ã€‚</li>
<li>MoAæœºåˆ¶é€šè¿‡åŠ¨æ€é€‰æ‹©æ³¨æ„åŠ›å¤´å®ç°ç‰¹å¾æå–çš„ä¸“ä¸šåŒ–ã€‚</li>
<li>SHMoEæœºåˆ¶èƒ½å¼‚è´¨åœ°é¢„æµ‹ä¸‰ä¸ªæ–¹å‘çš„å˜å½¢åœºï¼Œæé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>åœ¨å…¬å¼€æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºSHMoARegä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨è…¹éƒ¨CTæ•°æ®é›†ä¸ŠDiceå¾—åˆ†æ˜¾è‘—æé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.20073">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d512ccc9e29361602871a80f7cb4c868" align="middle">
<img src="https://picx.zhimg.com/v2-0029e9dc6af35c96eec1b0681ddf8bc5" align="middle">
<img src="https://picx.zhimg.com/v2-ffb1a7a4f99cdb4f698d59c5a9cb6916" align="middle">
<img src="https://picx.zhimg.com/v2-a0956e4bd2bb71cab72ac6d6af77fc4a" align="middle">
<img src="https://picx.zhimg.com/v2-3f295db46c976650a286eca3907dea3c" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="PS3-A-Multimodal-Transformer-Integrating-Pathology-Reports-with-Histology-Images-and-Biological-Pathways-for-Cancer-Survival-Prediction"><a href="#PS3-A-Multimodal-Transformer-Integrating-Pathology-Reports-with-Histology-Images-and-Biological-Pathways-for-Cancer-Survival-Prediction" class="headerlink" title="PS3: A Multimodal Transformer Integrating Pathology Reports with   Histology Images and Biological Pathways for Cancer Survival Prediction"></a>PS3: A Multimodal Transformer Integrating Pathology Reports with   Histology Images and Biological Pathways for Cancer Survival Prediction</h2><p><strong>Authors:Manahil Raza, Ayesha Azam, Talha Qaiser, Nasir Rajpoot</strong></p>
<p>Current multimodal fusion approaches in computational oncology primarily focus on integrating multi-gigapixel histology whole slide images (WSIs) with genomic or transcriptomic data, demonstrating improved survival prediction. We hypothesize that incorporating pathology reports can further enhance prognostic performance. Pathology reports, as essential components of clinical workflows, offer readily available complementary information by summarizing histopathological findings and integrating expert interpretations and clinical context. However, fusing these modalities poses challenges due to their heterogeneous nature. WSIs are high-dimensional, each containing several billion pixels, whereas pathology reports consist of concise text summaries of varying lengths, leading to potential modality imbalance. To address this, we propose a prototype-based approach to generate balanced representations, which are then integrated using a Transformer-based fusion model for survival prediction that we term PS3 (Predicting Survival from Three Modalities). Specifically, we present: (1) Diagnostic prototypes from pathology reports, leveraging self-attention to extract diagnostically relevant sections and standardize text representation; (2) Histological prototypes to compactly represent key morphological patterns in WSIs; and (3) Biological pathway prototypes to encode transcriptomic expressions, accurately capturing cellular functions. PS3, the three-modal transformer model, processes the resulting prototype-based multimodal tokens and models intra-modal and cross-modal interactions across pathology reports, WSIs and transcriptomic data. The proposed model outperforms state-of-the-art methods when evaluated against clinical, unimodal and multimodal baselines on six datasets from The Cancer Genome Atlas (TCGA). The code is available at: <a target="_blank" rel="noopener" href="https://github.com/manahilr/PS3">https://github.com/manahilr/PS3</a>. </p>
<blockquote>
<p>å½“å‰è®¡ç®—è‚¿ç˜¤å­¦ä¸­çš„å¤šæ¨¡æ€èåˆæ–¹æ³•ä¸»è¦é›†ä¸­åœ¨å°†å¤šåƒå…†åƒç´ å…¨åˆ‡ç‰‡å›¾åƒï¼ˆWhole Slide Images, WSIï¼‰ä¸åŸºå› ç»„æˆ–è½¬å½•ç»„æ•°æ®è¿›è¡Œæ•´åˆï¼Œä»¥æ”¹å–„ç”Ÿå­˜é¢„æµ‹ã€‚æˆ‘ä»¬å‡è®¾åŠ å…¥ç—…ç†æŠ¥å‘Šå¯ä»¥è¿›ä¸€æ­¥æé«˜é¢„åæ€§èƒ½ã€‚ä½œä¸ºä¸´åºŠå·¥ä½œæµç¨‹çš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼Œç—…ç†æŠ¥å‘Šé€šè¿‡æ€»ç»“ç—…ç†ç»„ç»‡å­¦å‘ç°ã€æ•´åˆä¸“å®¶è§£è¯»å’Œä¸´åºŠèƒŒæ™¯ï¼Œæä¾›äº†å¯è½»æ˜“è·å¾—çš„è¡¥å……ä¿¡æ¯ã€‚ç„¶è€Œï¼Œç”±äºè¿™äº›æ¨¡æ€çš„å¼‚è´¨æ€§ï¼Œèåˆå®ƒä»¬å…·æœ‰æŒ‘æˆ˜æ€§ã€‚å…¨åˆ‡ç‰‡å›¾åƒæ˜¯é«˜ç»´çš„ï¼Œæ¯ä¸ªå›¾åƒéƒ½åŒ…å«æ•°åäº¿åƒç´ ï¼Œè€Œç—…ç†æŠ¥å‘Šåˆ™åŒ…å«ä¸åŒé•¿åº¦çš„ç®€æ´æ–‡æœ¬æ‘˜è¦ï¼Œè¿™å¯èƒ½å¯¼è‡´æ¨¡æ€ä¸å¹³è¡¡çš„é—®é¢˜ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºåŸå‹çš„ç”Ÿæˆå¹³è¡¡è¡¨ç¤ºçš„æ–¹æ³•ï¼Œç„¶åä½¿ç”¨åŸºäºTransformerçš„èåˆæ¨¡å‹è¿›è¡Œç”Ÿå­˜é¢„æµ‹ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºPS3ï¼ˆä»ä¸‰ç§æ¨¡æ€é¢„æµ‹ç”Ÿå­˜ï¼‰ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å±•ç¤ºäº†ï¼šï¼ˆ1ï¼‰æ¥è‡ªç—…ç†æŠ¥å‘Šçš„è¯Šæ–­åŸå‹ï¼Œåˆ©ç”¨è‡ªæ³¨æ„åŠ›æå–è¯Šæ–­ç›¸å…³çš„éƒ¨åˆ†å¹¶æ ‡å‡†åŒ–æ–‡æœ¬è¡¨ç¤ºï¼›ï¼ˆ2ï¼‰å…¨åˆ‡ç‰‡å›¾åƒä¸­çš„ç»„ç»‡åŸå‹ï¼Œç´§å‡‘åœ°è¡¨ç¤ºå…³é”®å½¢æ€æ¨¡å¼ï¼›ï¼ˆ3ï¼‰è½¬å½•ç»„åŸå‹ï¼Œç¼–ç è½¬å½•ç»„è¡¨è¾¾ï¼Œå‡†ç¡®æ•è·ç»†èƒåŠŸèƒ½ã€‚PS3ä¸‰æ¨¡æ€Transformeræ¨¡å‹å¤„ç†åŸºäºåŸå‹çš„å¤šæ¨¡æ€ç¬¦å·ï¼Œå¹¶æ¨¡æ‹Ÿç—…ç†æŠ¥å‘Šã€å…¨åˆ‡ç‰‡å›¾åƒå’Œè½¬å½•ç»„æ•°æ®ä¹‹é—´çš„å†…éƒ¨å’Œå¤–éƒ¨äº¤äº’ä½œç”¨ã€‚æ‰€æå‡ºçš„æ¨¡å‹åœ¨æ¥è‡ªç™Œç—‡åŸºå› ç»„å›¾è°±ï¼ˆTCGAï¼‰çš„å…­ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œä¸ä¸´åºŠã€å•æ¨¡æ€å’Œå¤šæ¨¡æ€åŸºçº¿ç›¸æ¯”ï¼Œè¡¨ç°å‡ºä¼˜äºå½“å‰æœ€æ–°æ–¹æ³•çš„æ•ˆæœã€‚ä»£ç å¯åœ¨ä»¥ä¸‹ç½‘å€è·å–ï¼š<a target="_blank" rel="noopener" href="https://github.com/manahilr/PS3">https://github.com/manahilr/PS3</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.20022v1">PDF</a> Accepted at ICCV 2025. Copyright 2025 IEEE. Personal use of this   material is permitted. Permission from IEEE must be obtained for all other   uses including reprinting&#x2F;republishing this material for advertising or   promotional purposes, creating new collective works, for resale or   redistribution to servers or lists, or reuse of any copyrighted component of   this work in other works</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¤šæ¨¡æ€èåˆæ–¹æ³•åœ¨è®¡ç®—è‚¿ç˜¤å­¦é¢†åŸŸçš„åº”ç”¨ï¼Œé‡ç‚¹èšç„¦äºå°†é«˜åƒç´ ç»„ç»‡å­¦å…¨åˆ‡ç‰‡å›¾åƒï¼ˆWSIsï¼‰ä¸åŸºå› ç»„å­¦æˆ–è½¬å½•ç»„å­¦æ•°æ®ç›¸ç»“åˆï¼Œä»¥æé«˜ç”Ÿå­˜é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚æ–‡ç« æå‡ºç»“åˆç—…ç†å­¦æŠ¥å‘Šèƒ½è¿›ä¸€æ­¥æå‡é¢„æµ‹æ€§èƒ½çš„è§‚ç‚¹ï¼Œå¹¶è¯¦ç»†ä»‹ç»äº†ä¸ºè§£å†³ä¸åŒæ¨¡æ€æ•°æ®å¼‚è´¨æ€§é—®é¢˜è€Œé‡‡ç”¨çš„åŸºäºåŸå‹çš„èåˆæ¨¡å‹â€”â€”PS3ï¼ˆä»ä¸‰ç§æ¨¡æ€é¢„æµ‹ç”Ÿå­˜çš„æ¨¡å‹ï¼‰ã€‚PS3æ¨¡å‹é€šè¿‡å¤„ç†åŸºäºåŸå‹çš„å¤šæ¨¡æ€æ ‡è®°ï¼Œå¹¶å»ºæ¨¡ç—…ç†æŠ¥å‘Šã€WSIså’Œè½¬å½•ç»„å­¦æ•°æ®ä¹‹é—´çš„å†…æ¨¡æ€å’Œè·¨æ¨¡æ€äº¤äº’ï¼Œå±•ç°äº†ä¼˜è¶Šçš„æ€§èƒ½ï¼Œä¼˜äºåœ¨TCGAå…­ä¸ªæ•°æ®é›†ä¸Šçš„å…¶ä»–å…ˆè¿›æ–¹æ³•ã€‚æ¨¡å‹ä»£ç å·²å…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰è®¡ç®—è‚¿ç˜¤å­¦ä¸­çš„å¤šæ¨¡æ€èåˆæ–¹æ³•ä¸»è¦å…³æ³¨äºæ•´åˆé«˜åƒç´ ç»„ç»‡å­¦å…¨åˆ‡ç‰‡å›¾åƒï¼ˆWSIsï¼‰ä¸åŸºå› ç»„æˆ–è½¬å½•ç»„æ•°æ®ï¼Œä»¥æé«˜ç”Ÿå­˜é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚</li>
<li>èå…¥ç—…ç†å­¦æŠ¥å‘Šä¿¡æ¯èƒ½å¤Ÿè¿›ä¸€æ­¥æå‡é¢„æµ‹æ€§èƒ½ï¼Œå› ä¸ºè¿™äº›æŠ¥å‘Šæ€»ç»“äº†ç—…ç†ç»„ç»‡å­¦å‘ç°ï¼Œå¹¶èå…¥äº†ä¸“å®¶è§£è¯»å’Œä¸´åºŠèƒŒæ™¯ã€‚</li>
<li>WSIsä¸ç—…ç†å­¦æŠ¥å‘Šçš„èåˆé¢ä¸´æŒ‘æˆ˜ï¼Œå› ä¸ºä¸¤è€…çš„ç»´åº¦å’Œæ€§è´¨ä¸åŒï¼ˆWSIsé«˜ç»´ï¼Œè€ŒæŠ¥å‘Šæ–‡æœ¬ç®€çŸ­ï¼‰ã€‚</li>
<li>ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†åŸºäºåŸå‹çš„è¡¨ç¤ºæ–¹æ³•ï¼Œç”Ÿæˆå‡è¡¡è¡¨ç¤ºï¼Œå¹¶è¿›ä¸€æ­¥ä½¿ç”¨åŸºäºTransformerçš„èåˆæ¨¡å‹PS3è¿›è¡Œç”Ÿå­˜é¢„æµ‹ã€‚</li>
<li>PS3æ¨¡å‹åŒ…æ‹¬ä»ç—…ç†å­¦æŠ¥å‘Šä¸­æå–è¯Šæ–­åŸå‹ã€ä»WSIsä¸­æå–ç»„ç»‡å­¦åŸå‹ä»¥åŠç¼–ç è½¬å½•ç»„è¡¨è¾¾çš„ç”Ÿç‰©é€šè·¯åŸå‹ã€‚</li>
<li>PS3æ¨¡å‹åœ¨TCGAçš„å…­ä¸ªæ•°æ®é›†ä¸Šçš„æ€§èƒ½è¶…è¿‡äº†å…¶ä»–å…ˆè¿›æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.20022">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bd7c11ad3c7778e5aeeb8958d4b84ae1" align="middle">
<img src="https://picx.zhimg.com/v2-b00dc44ad6179e3d6e43c98f8c0b3596" align="middle">
<img src="https://picx.zhimg.com/v2-58e24fba732f7f17ca6837f47717524a" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Generalized-Shortest-Path-based-Superpixels-for-3D-Spherical-Image-Segmentation"><a href="#Generalized-Shortest-Path-based-Superpixels-for-3D-Spherical-Image-Segmentation" class="headerlink" title="Generalized Shortest Path-based Superpixels for 3D Spherical Image   Segmentation"></a>Generalized Shortest Path-based Superpixels for 3D Spherical Image   Segmentation</h2><p><strong>Authors:RÃ©mi Giraud, Rodrigo Borba Pinheiro, Yannick Berthoumieu</strong></p>
<p>The growing use of wide angle image capture devices and the need for fast and accurate image analysis in computer visions have enforced the need for dedicated under-representation approaches. Most recent decomposition methods segment an image into a small number of irregular homogeneous regions, called superpixels. Nevertheless, these approaches are generally designed to segment standard 2D planar images, i.e., captured with a 90o angle view without distortion. In this work, we introduce a new general superpixel method called SphSPS (for Spherical Shortest Path-based Superpixels)1 , dedicated to wide 360o spherical or omnidirectional images. Our method respects the geometry of the 3D spherical acquisition space and generalizes the notion of shortest path between a pixel and a superpixel center, to fastly extract relevant clustering features. We demonstrate that considering the geometry of the acquisition space to compute the shortest path enables to jointly improve the segmentation accuracy and the shape regularity of superpixels. To evaluate this regularity aspect, we also generalize a global regularity metric to the spherical space, addressing the limitations of the only existing spherical compactness measure. Finally, the proposed SphSPS method is validated on the reference 360o spherical panorama segmentation dataset and on synthetic road omnidirectional images. Our method significantly outperforms both planar and spherical state-of-the-art approaches in terms of segmentation accuracy,robustness to noise and regularity, providing a very interesting tool for superpixel-based applications on 360o images. </p>
<blockquote>
<p>éšç€å¹¿è§’å›¾åƒé‡‡é›†è®¾å¤‡çš„ä½¿ç”¨æ—¥ç›Šå¢å¤šï¼Œä»¥åŠè®¡ç®—æœºè§†è§‰ä¸­å¯¹å¿«é€Ÿå’Œå‡†ç¡®å›¾åƒåˆ†æçš„éœ€æ±‚ï¼Œè¿«åˆ‡éœ€è¦ä¸“é—¨çš„æ¬ è¡¨ç¤ºæ–¹æ³•ã€‚æœ€è¿‘çš„æ–¹æ³•é€šå¸¸å°†å›¾åƒåˆ†å‰²æˆå°‘é‡ä¸è§„åˆ™çš„åŒè´¨åŒºåŸŸï¼Œç§°ä¸ºè¶…åƒç´ ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é€šå¸¸æ˜¯ä¸ºæ ‡å‡†äºŒç»´å¹³é¢å›¾åƒè®¾è®¡çš„ï¼Œå³é‡‡ç”¨90åº¦è§†è§’æ‹æ‘„ï¼Œæ— å¤±çœŸã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§æ–°çš„é€šç”¨è¶…åƒç´ æ–¹æ³•ï¼Œç§°ä¸ºåŸºäºçƒé¢æœ€çŸ­è·¯å¾„çš„è¶…åƒç´ ï¼ˆSphSPSï¼‰ï¼Œä¸“é—¨ç”¨äºå®½360åº¦çƒé¢æˆ–å…¨å‘å›¾åƒã€‚æˆ‘ä»¬çš„æ–¹æ³•å°Šé‡ä¸‰ç»´çƒçŠ¶é‡‡é›†ç©ºé—´çš„å‡ ä½•ç»“æ„ï¼Œå¹¶æ¨å¹¿äº†åƒç´ ä¸è¶…åƒç´ ä¸­å¿ƒä¹‹é—´çš„æœ€çŸ­è·¯å¾„çš„æ¦‚å¿µï¼Œä»¥å¿«é€Ÿæå–ç›¸å…³çš„èšç±»ç‰¹å¾ã€‚æˆ‘ä»¬è¯æ˜äº†è€ƒè™‘é‡‡é›†ç©ºé—´çš„å‡ ä½•ç»“æ„æ¥è®¡ç®—æœ€çŸ­è·¯å¾„å¯ä»¥è”åˆæé«˜è¶…åƒç´ çš„åˆ†å‰²ç²¾åº¦å’Œå½¢çŠ¶è§„åˆ™æ€§ã€‚ä¸ºäº†è¯„ä¼°è¿™ç§è§„åˆ™æ€§æ–¹é¢ï¼Œæˆ‘ä»¬è¿˜é’ˆå¯¹çƒé¢ç©ºé—´æ¨å¹¿äº†å…¨å±€è§„åˆ™æ€§åº¦é‡æ ‡å‡†ï¼Œè§£å†³äº†ç°æœ‰çƒé¢ç´§å‡‘æ€§åº¦é‡çš„å±€é™æ€§ã€‚æœ€åï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨å‚è€ƒçš„360åº¦å…¨æ™¯åˆ†å‰²æ•°æ®é›†å’Œåˆæˆå…¨å‘é“è·¯å›¾åƒä¸Šè¿›è¡Œäº†éªŒè¯ã€‚åœ¨åˆ†å‰²ç²¾åº¦ã€å™ªå£°é²æ£’æ€§å’Œè§„åˆ™æ€§æ–¹é¢ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—ä¼˜äºå¹³é¢å’Œè¶…çƒé¢çš„ç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ï¼Œä¸ºåŸºäºè¶…åƒç´ çš„360åº¦å›¾åƒåº”ç”¨æä¾›äº†éå¸¸æœ‰ç”¨çš„å·¥å…·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.19895v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>é’ˆå¯¹å¹¿è§’å›¾åƒæ•è·è®¾å¤‡æ—¥ç›Šæ™®åŠä»¥åŠå¯¹å¿«é€Ÿå’Œå‡†ç¡®è®¡ç®—æœºè§†è§‰å›¾åƒåˆ†æçš„éœ€æ±‚ï¼Œè¿«åˆ‡éœ€è¦ä¸“é—¨çš„æ¬ è¡¨è¾¾æ–¹æ³•ã€‚æœ€æ–°çš„åˆ†è§£æ–¹æ³•å°†å›¾åƒåˆ†å‰²æˆå°‘é‡ä¸è§„åˆ™çš„åŒè´¨åŒºåŸŸï¼Œç§°ä¸ºè¶…åƒç´ ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é€šå¸¸è®¾è®¡ç”¨äºåˆ†å‰²æ ‡å‡†äºŒç»´å¹³é¢å›¾åƒï¼Œå³ä½¿ç”¨æ— å¤±çœŸçš„è§†è§’è¿›è¡Œæ•è·ã€‚åœ¨æ­¤å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„é€šç”¨è¶…åƒç´ æ–¹æ³•ï¼Œç§°ä¸ºçƒå½¢æœ€çŸ­è·¯å¾„è¶…åƒç´ ï¼ˆSphSPSï¼‰ï¼Œä¸“é—¨ç”¨äºå¤„ç†å®½è§†è§’çš„360åº¦çƒå½¢æˆ–å…¨æ–¹ä½å›¾åƒã€‚æˆ‘ä»¬çš„æ–¹æ³•å°Šé‡ä¸‰ç»´çƒå½¢é‡‡é›†ç©ºé—´çš„å‡ ä½•ç»“æ„ï¼Œå¹¶æ¨å¹¿åƒç´ ä¸è¶…åƒç´ ä¸­å¿ƒä¹‹é—´çš„æœ€çŸ­è·¯å¾„æ¦‚å¿µï¼Œä»¥å¿«é€Ÿæå–ç›¸å…³çš„èšç±»ç‰¹å¾ã€‚æˆ‘ä»¬è¯æ˜äº†è€ƒè™‘é‡‡é›†ç©ºé—´çš„å‡ ä½•ç»“æ„æ¥è®¡ç®—æœ€çŸ­è·¯å¾„èƒ½å¤ŸåŒæ—¶æé«˜è¶…åƒç´ çš„åˆ†å‰²ç²¾åº¦å’Œå½¢çŠ¶è§„åˆ™æ€§ã€‚ä¸ºäº†è¯„ä¼°è¿™ç§è§„åˆ™æ€§æ–¹é¢ï¼Œæˆ‘ä»¬è¿˜é’ˆå¯¹çƒå½¢ç©ºé—´æ¨å¹¿äº†å…¨å±€è§„åˆ™æ€§åº¦é‡æŒ‡æ ‡ï¼Œè§£å†³äº†ç°æœ‰å”¯ä¸€çƒå½¢ç´§å‡‘æ€§åº¦é‡æŒ‡æ ‡çš„å±€é™æ€§ã€‚æœ€åï¼Œåœ¨å‚è€ƒçš„360åº¦çƒå½¢å…¨æ™¯åˆ†å‰²æ•°æ®é›†å’Œåˆæˆå…¨æ–¹ä½é“è·¯å›¾åƒä¸ŠéªŒè¯äº†æ‰€æå‡ºçš„SphSPSæ–¹æ³•ã€‚åœ¨åˆ†å‰²ç²¾åº¦ã€å™ªå£°é²æ£’æ€§å’Œè§„åˆ™æ€§æ–¹é¢ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºå¹³é¢å’Œçƒå½¢çš„æœ€æ–°æ–¹æ³•ï¼Œä¸ºåŸºäºè¶…åƒç´ çš„360åº¦å›¾åƒåº”ç”¨æä¾›äº†éå¸¸æœ‰ç”¨çš„å·¥å…·ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ä»‹ç»äº†ä¸€ç§æ–°çš„é€šç”¨è¶…åƒç´ æ–¹æ³•â€”â€”çƒå½¢æœ€çŸ­è·¯å¾„è¶…åƒç´ ï¼ˆSphSPSï¼‰ï¼Œä¸“é—¨ç”¨äºå¤„ç†å®½è§†è§’çš„çƒå½¢æˆ–å…¨æ–¹ä½å›¾åƒã€‚</li>
<li>è€ƒè™‘ä¸‰ç»´çƒå½¢é‡‡é›†ç©ºé—´çš„å‡ ä½•ç»“æ„ï¼Œæé«˜äº†è¶…åƒç´ çš„åˆ†å‰²ç²¾åº¦å’Œå½¢çŠ¶è§„åˆ™æ€§ã€‚</li>
<li>æ¨å¹¿äº†åƒç´ ä¸è¶…åƒç´ ä¸­å¿ƒä¹‹é—´çš„æœ€çŸ­è·¯å¾„æ¦‚å¿µï¼Œä»¥å¿«é€Ÿæå–ç›¸å…³çš„èšç±»ç‰¹å¾ã€‚</li>
<li>é’ˆå¯¹çƒå½¢ç©ºé—´æ¨å¹¿äº†å…¨å±€è§„åˆ™æ€§åº¦é‡æŒ‡æ ‡ï¼Œä»¥è¯„ä¼°è¶…åƒç´ çš„è§„åˆ™æ€§ã€‚</li>
<li>åœ¨åˆ†å‰²æ•°æ®é›†å’Œåˆæˆå›¾åƒä¸ŠéªŒè¯äº†æ‰€æå‡ºçš„SphSPSæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨åˆ†å‰²ç²¾åº¦ã€å™ªå£°é²æ£’æ€§å’Œè§„åˆ™æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰çš„æœ€æ–°æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.19895">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5384b40da136acd2a91ea7d5f91c59c3" align="middle">
<img src="https://picx.zhimg.com/v2-4c39ec100663c157f17969da1328a006" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="PPGFlowECG-Latent-Rectified-Flow-with-Cross-Modal-Encoding-for-PPG-Guided-ECG-Generation-and-Cardiovascular-Disease-Detection"><a href="#PPGFlowECG-Latent-Rectified-Flow-with-Cross-Modal-Encoding-for-PPG-Guided-ECG-Generation-and-Cardiovascular-Disease-Detection" class="headerlink" title="PPGFlowECG: Latent Rectified Flow with Cross-Modal Encoding for   PPG-Guided ECG Generation and Cardiovascular Disease Detection"></a>PPGFlowECG: Latent Rectified Flow with Cross-Modal Encoding for   PPG-Guided ECG Generation and Cardiovascular Disease Detection</h2><p><strong>Authors:Xiaocheng Fang, Jiarui Jin, Haoyu Wang, Che Liu, Jieyi Cai, Guangkun Nie, Jun Li, Hongyan Li, Shenda Hong</strong></p>
<p>In clinical practice, electrocardiography (ECG) remains the gold standard for cardiac monitoring, providing crucial insights for diagnosing a wide range of cardiovascular diseases (CVDs). However, its reliance on specialized equipment and trained personnel limits feasibility for continuous routine monitoring. Photoplethysmography (PPG) offers accessible, continuous monitoring but lacks definitive electrophysiological information, preventing conclusive diagnosis. Generative models present a promising approach to translate PPG into clinically valuable ECG signals, yet current methods face substantial challenges, including the misalignment of physiological semantics in generative models and the complexity of modeling in high-dimensional signals. To this end, we propose PPGFlowECG, a two-stage framework that aligns PPG and ECG in a shared latent space via the CardioAlign Encoder and employs latent rectified flow to generate ECGs with high fidelity and interpretability. To the best of our knowledge, this is the first study to experiment on MCMED, a newly released clinical-grade dataset comprising over 10 million paired PPG-ECG samples from more than 118,000 emergency department visits with expert-labeled cardiovascular disease annotations. Results demonstrate the effectiveness of our method for PPG-to-ECG translation and cardiovascular disease detection. Moreover, cardiologist-led evaluations confirm that the synthesized ECGs achieve high fidelity and improve diagnostic reliability, underscoring our methodâ€™s potential for real-world cardiovascular screening. </p>
<blockquote>
<p>åœ¨ä¸´åºŠå®è·µä¸­ï¼Œå¿ƒç”µå›¾ï¼ˆECGï¼‰ä»ç„¶æ˜¯å¿ƒè„ç›‘æµ‹çš„é‡‘æ ‡å‡†ï¼Œä¸ºè¯Šæ–­å„ç§å¿ƒè¡€ç®¡ç–¾ç—…ï¼ˆCVDï¼‰æä¾›å…³é”®è§è§£ã€‚ç„¶è€Œï¼Œå®ƒå¯¹ä¸“ä¸šè®¾å¤‡å’Œå—è¿‡è®­ç»ƒçš„äººå‘˜çš„ä¾èµ–é™åˆ¶äº†å…¶è¿›è¡ŒæŒç»­å¸¸è§„ç›‘æµ‹çš„å¯è¡Œæ€§ã€‚å…‰ç”µå®¹ç§¯è„‰ææ³¢æè®°æ³•ï¼ˆPPGï¼‰æä¾›äº†å¯è®¿é—®çš„è¿ç»­ç›‘æµ‹ï¼Œä½†ç¼ºä¹æ˜ç¡®çš„ç”µç”Ÿç†ä¿¡æ¯ï¼Œæ— æ³•ä½œå‡ºç¡®åˆ‡è¯Šæ–­ã€‚ç”Ÿæˆæ¨¡å‹ä½œä¸ºä¸€ç§æœ‰å‰æ™¯çš„æ–¹æ³•ï¼Œå¯å°†PPGè½¬åŒ–ä¸ºå…·æœ‰ä¸´åºŠä»·å€¼çš„å¿ƒç”µå›¾ä¿¡å·ï¼Œä½†å½“å‰æ–¹æ³•é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ç”Ÿæˆæ¨¡å‹ä¸­çš„ç”Ÿç†è¯­ä¹‰ä¸åŒ¹é…å’Œé«˜ç»´ä¿¡å·å»ºæ¨¡çš„å¤æ‚æ€§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†PPGFlowECGï¼Œè¿™æ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µçš„æ¡†æ¶ï¼Œé€šè¿‡CardioAlignç¼–ç å™¨åœ¨å…±äº«æ½œåœ¨ç©ºé—´ä¸­å¯¹PPGå’ŒECGè¿›è¡Œå¯¹é½ï¼Œå¹¶ä½¿ç”¨æ½œåœ¨æ•´æµæµç”Ÿæˆå…·æœ‰é«˜ä¿çœŸåº¦å’Œå¯è§£é‡Šæ€§çš„ECGã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯ç¬¬ä¸€é¡¹åœ¨æ–°å‘å¸ƒçš„ä¸´åºŠçº§æ•°æ®é›†MCMEDä¸Šè¿›è¡Œå®éªŒçš„ç ”ç©¶ï¼Œè¯¥æ•°æ®é›†åŒ…å«è¶…è¿‡1000ä¸‡å¯¹PPG-ECGæ ·æœ¬ï¼Œæ¥è‡ªè¶…è¿‡11.8ä¸‡æ¬¡æ€¥è¯Šå°±è¯Šè®°å½•ï¼Œå¹¶é™„æœ‰ä¸“å®¶æ ‡æ³¨çš„å¿ƒè¡€ç®¡ç–¾ç—…æ³¨é‡Šã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨PPGåˆ°ECGçš„ç¿»è¯‘å’Œå¿ƒè¡€ç®¡ç–¾ç—…æ£€æµ‹æ–¹é¢éå¸¸æœ‰æ•ˆã€‚æ­¤å¤–ï¼Œç”±å¿ƒè„ç—…å­¦å®¶é¢†å¯¼çš„è¯„ä»·è¯å®ï¼Œåˆæˆçš„ECGå…·æœ‰é«˜ä¿çœŸåº¦ï¼Œæé«˜äº†è¯Šæ–­çš„å¯é æ€§ï¼Œè¿™çªæ˜¾äº†æˆ‘ä»¬æ–¹æ³•åœ¨ç°å®ä¸–ç•Œå¿ƒè¡€ç®¡ç­›æŸ¥ä¸­çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.19774v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¿ƒç”µå›¾ï¼ˆECGï¼‰åœ¨ä¸´åºŠå®è·µä¸­ä»æ˜¯å¿ƒè„ç›‘æµ‹çš„é‡‘æ ‡å‡†ï¼Œä½†å…¶å¯¹äºä¸“ä¸šè®¾å¤‡å’Œäººå‘˜çš„ä¾èµ–é™åˆ¶äº†å…¶åœ¨æŒç»­å¸¸è§„ç›‘æµ‹ä¸­çš„å¯è¡Œæ€§ã€‚å…‰ä½“ç§¯æè®°æ³•ï¼ˆPPGï¼‰æä¾›äº†ä¸€ç§å¯è®¿é—®çš„è¿ç»­ç›‘æµ‹æ–¹æ³•ï¼Œä½†ç¼ºä¹æ˜ç¡®çš„ç”Ÿç†ä¿¡æ¯ï¼Œæ— æ³•åšå‡ºç¡®åˆ‡è¯Šæ–­ã€‚ç ”ç©¶äººå‘˜æå‡ºäº†PPGFlowECGæ¡†æ¶ï¼Œé€šè¿‡ä¸¤é˜¶æ®µè¿‡ç¨‹å°†PPGè½¬åŒ–ä¸ºå…·æœ‰ä¸´åºŠä»·å€¼çš„å¿ƒç”µå›¾ä¿¡å·ï¼Œå¹¶é€šè¿‡CardioAlign Encoderåœ¨å…±äº«æ½œåœ¨ç©ºé—´ä¸­å¯¹PPGå’ŒECGè¿›è¡Œå¯¹é½ã€‚è¯¥ç ”ç©¶é¦–æ¬¡åœ¨MCMEDæ•°æ®é›†ä¸Šè¿›è¡Œå®éªŒï¼Œè¯¥æ•°æ®é›†åŒ…å«è¶…è¿‡100ä¸‡å¯¹PPG-ECGæ ·æœ¬å’Œä¸“å®¶æ ‡æ³¨çš„å¿ƒè¡€ç®¡ç–¾ç—…æ³¨é‡Šã€‚å®éªŒç»“æœè¡¨æ˜è¯¥æ–¹æ³•åœ¨PPGåˆ°ECGçš„ç¿»è¯‘å’Œå¿ƒè¡€ç®¡ç–¾ç—…æ£€æµ‹æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œå¹¶å¾—åˆ°äº†å¿ƒè„ç—…ä¸“å®¶çš„è¯„ä¼°ç¡®è®¤ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¿ƒç”µå›¾ï¼ˆECGï¼‰æ˜¯è¯Šæ–­å¿ƒè¡€ç®¡ç–¾ç—…ï¼ˆCVDï¼‰çš„é‡‘æ ‡å‡†ï¼Œä½†å…¶åœ¨æŒç»­å¸¸è§„ç›‘æµ‹ä¸­çš„å¯è¡Œæ€§å—é™ã€‚</li>
<li>å…‰ä½“ç§¯æè®°æ³•ï¼ˆPPGï¼‰æä¾›è¿ç»­ç›‘æµ‹ï¼Œä½†ç¼ºä¹æ˜ç¡®çš„ç”Ÿç†ä¿¡æ¯ï¼Œæ— æ³•ç”¨äºç¡®è¯Šã€‚</li>
<li>ç ”ç©¶äººå‘˜æå‡ºäº†PPGFlowECGæ¡†æ¶ï¼Œæ—¨åœ¨å°†PPGè½¬åŒ–ä¸ºå…·æœ‰ä¸´åºŠä»·å€¼çš„å¿ƒç”µå›¾ä¿¡å·ã€‚</li>
<li>è¯¥æ¡†æ¶åŒ…æ‹¬ä¸€ä¸ªä¸¤é˜¶æ®µè¿‡ç¨‹ï¼Œé€šè¿‡CardioAlign Encoderåœ¨å…±äº«æ½œåœ¨ç©ºé—´ä¸­å¯¹PPGå’ŒECGè¿›è¡Œå¯¹é½ã€‚</li>
<li>ç ”ç©¶é¦–æ¬¡åœ¨åŒ…å«è¶…è¿‡ä¸€ç™¾ä¸‡å¯¹PPG-ECGæ ·æœ¬å’Œä¸“å®¶æ ‡æ³¨çš„MCMEDæ•°æ®é›†ä¸Šè¿›è¡Œå®éªŒã€‚</li>
<li>å®éªŒç»“æœè¯æ˜äº†è¯¥æ–¹æ³•åœ¨PPGåˆ°ECGç¿»è¯‘å’Œå¿ƒè¡€ç®¡ç–¾ç—…æ£€æµ‹æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.19774">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3a7cedbad0bb1bfd94ab3d251efaaea0" align="middle">
<img src="https://picx.zhimg.com/v2-879f8d125d383f4d9b954bccaa818ff4" align="middle">
<img src="https://picx.zhimg.com/v2-059a643354965a25f4ef198392c84e15" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="nnFilterMatch-A-Unified-Semi-Supervised-Learning-Framework-with-Uncertainty-Aware-Pseudo-Label-Filtering-for-Efficient-Medical-Segmentation"><a href="#nnFilterMatch-A-Unified-Semi-Supervised-Learning-Framework-with-Uncertainty-Aware-Pseudo-Label-Filtering-for-Efficient-Medical-Segmentation" class="headerlink" title="nnFilterMatch: A Unified Semi-Supervised Learning Framework with   Uncertainty-Aware Pseudo-Label Filtering for Efficient Medical Segmentation"></a>nnFilterMatch: A Unified Semi-Supervised Learning Framework with   Uncertainty-Aware Pseudo-Label Filtering for Efficient Medical Segmentation</h2><p><strong>Authors:Yi Yang</strong></p>
<p>Semi-supervised learning (SSL) has emerged as a promising paradigm in medical image segmentation, offering competitive performance while substantially reducing the need for extensive manual annotation. When combined with active learning (AL), these strategies further minimize annotation burden by selectively incorporating the most informative samples. However, conventional SSL_AL hybrid approaches often rely on iterative and loop-based retraining cycles after each annotation round, incurring significant computational overhead and limiting scalability in clinical applications. In this study, we present a novel, annotation-efficient, and self-adaptive deep segmentation framework that integrates SSL with entropy-based pseudo-label filtering (FilterMatch), an AL-inspired mechanism, within the single-pass nnU-Net training segmentation framework (nnFilterMatch). By selectively excluding high-confidence pseudo-labels during training, our method circumvents the need for retraining loops while preserving the benefits of uncertainty-guided learning. We validate the proposed framework across multiple clinical segmentation benchmarks and demonstrate that it achieves performance comparable to or exceeding fully supervised models, even with only 5%â€“20% labeled data. This work introduces a scalable, end-to-end learning strategy for reducing annotation demands in medical image segmentation without compromising accuracy. Code is available here: <a target="_blank" rel="noopener" href="https://github.com/Ordi117/nnFilterMatch.git">https://github.com/Ordi117/nnFilterMatch.git</a>. </p>
<blockquote>
<p>åŠç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰å·²æˆä¸ºåŒ»å­¦å›¾åƒåˆ†å‰²é¢†åŸŸçš„ä¸€ä¸ªæœ‰å‰é€”çš„èŒƒå¼ï¼Œå®ƒåœ¨æä¾›å…·æœ‰ç«äº‰åŠ›æ€§èƒ½çš„åŒæ—¶ï¼Œå¤§å¹…å‡å°‘å¯¹å¤§é‡æ‰‹åŠ¨æ³¨é‡Šçš„éœ€æ±‚ã€‚å½“ä¸ä¸»åŠ¨å­¦ä¹ ï¼ˆALï¼‰ç»“åˆæ—¶ï¼Œè¿™äº›ç­–ç•¥é€šè¿‡é€‰æ‹©æ€§åœ°èå…¥æœ€æœ‰ä¿¡æ¯é‡çš„æ ·æœ¬ï¼Œè¿›ä¸€æ­¥å‡è½»äº†æ³¨é‡Šçš„è´Ÿæ‹…ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„SSL_ALæ··åˆæ–¹æ³•é€šå¸¸ä¾èµ–äºæ¯æ¬¡æ³¨é‡Šè½®æ¬¡åçš„è¿­ä»£å’ŒåŸºäºå¾ªç¯çš„é‡æ–°è®­ç»ƒå‘¨æœŸï¼Œè¿™äº§ç”Ÿäº†å·¨å¤§çš„è®¡ç®—å¼€é”€ï¼Œå¹¶é™åˆ¶äº†å…¶åœ¨ä¸´åºŠåº”ç”¨ä¸­çš„å¯æ‰©å±•æ€§ã€‚åœ¨ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–ã€æ³¨é‡Šæ•ˆç‡é«˜ã€è‡ªé€‚åº”çš„æ·±åº¦åˆ†å‰²æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†åŠç›‘ç£å­¦ä¹ ä¸åŸºäºç†µçš„ä¼ªæ ‡ç­¾è¿‡æ»¤ï¼ˆFilterMatchï¼‰ç›¸ç»“åˆï¼Œè¿™æ˜¯ä¸€ç§å—ä¸»åŠ¨å­¦ä¹ å¯å‘çš„æœºåˆ¶ï¼Œå¹¶é›†æˆåˆ°å•é€šé“nnU-Netè®­ç»ƒåˆ†å‰²æ¡†æ¶ä¸­ï¼ˆnnFilterMatchï¼‰ã€‚é€šè¿‡è®­ç»ƒè¿‡ç¨‹ä¸­é€‰æ‹©æ€§æ’é™¤é«˜ä¿¡å¿ƒä¼ªæ ‡ç­¾ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é¿å…äº†é‡æ–°è®­ç»ƒå¾ªç¯çš„éœ€è¦ï¼ŒåŒæ—¶ä¿ç•™äº†ä¸ç¡®å®šæ€§å¼•å¯¼å­¦ä¹ çš„ä¼˜ç‚¹ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªä¸´åºŠåˆ†å‰²åŸºå‡†æµ‹è¯•ä¸­éªŒè¯äº†æ‰€æå‡ºæ¡†æ¶çš„æ€§èƒ½ï¼Œå¹¶è¯æ˜å…¶å³ä½¿åœ¨åªæœ‰5%~20%æ ‡è®°æ•°æ®çš„æƒ…å†µä¸‹ï¼Œæ€§èƒ½ä¹Ÿå¯ä¸å…¨ç›‘ç£æ¨¡å‹ç›¸å½“æˆ–æ›´ä½³ã€‚è¿™é¡¹å·¥ä½œä»‹ç»äº†ä¸€ç§å¯æ‰©å±•çš„ç«¯åˆ°ç«¯å­¦ä¹ ç­–ç•¥ï¼Œå¯åœ¨ä¸æŸå®³å‡†ç¡®æ€§çš„æƒ…å†µä¸‹å‡å°‘åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„æ³¨é‡Šéœ€æ±‚ã€‚ä»£ç å¯é€šè¿‡ä»¥ä¸‹é“¾æ¥è·å–ï¼š<a target="_blank" rel="noopener" href="https://github.com/Ordi117/nnFilterMatch.git">https://github.com/Ordi117/nnFilterMatch.git</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.19746v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>åŠç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰ç»“åˆä¸»åŠ¨å­¦ä¹ ï¼ˆALï¼‰åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œèƒ½å¤Ÿå‡å°‘å¤§é‡æ‰‹åŠ¨æ ‡æ³¨çš„éœ€æ±‚ï¼Œå¹¶æä¾›ç«äº‰åŠ›å¼ºçš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„SSL_ALæ··åˆæ–¹æ³•å¸¸å¸¸ä¾èµ–äºè¿­ä»£å’Œå¾ªç¯å†è®­ç»ƒï¼Œè¿™å¢åŠ äº†è®¡ç®—è´Ÿæ‹…å¹¶é™åˆ¶äº†å…¶åœ¨ä¸´åºŠåº”ç”¨ä¸­çš„å¯æ‰©å±•æ€§ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹ã€æ ‡æ³¨é«˜æ•ˆã€è‡ªé€‚åº”æ·±åº¦åˆ†å‰²æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†SSLä¸åŸºäºï¿½ï¿½ndropyçš„ä¼ªæ ‡ç­¾è¿‡æ»¤æœºåˆ¶ï¼ˆFilterMatchï¼‰ç›¸ç»“åˆï¼Œåœ¨å•æ¬¡é€šè¿‡nnU-Netè®­ç»ƒåˆ†å‰²æ¡†æ¶ï¼ˆnnFilterMatchï¼‰å†…å®ç°ã€‚é€šè¿‡é€‰æ‹©æ€§æ’é™¤é«˜ç½®ä¿¡åº¦çš„ä¼ªæ ‡ç­¾è¿›è¡Œè®­ç»ƒï¼Œè¯¥æ–¹æ³•é¿å…äº†å†è®­ç»ƒå¾ªç¯çš„éœ€è¦ï¼ŒåŒæ—¶ä¿ç•™äº†ä¸ç¡®å®šæ€§å¼•å¯¼å­¦ä¹ çš„ä¼˜ç‚¹ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªä¸´åºŠåˆ†å‰²åŸºå‡†æµ‹è¯•ä¸­éªŒè¯äº†æ‰€ææ¡†æ¶ï¼Œå¹¶è¯æ˜å³ä½¿åªæœ‰5%~20%çš„æ•°æ®è¢«æ ‡æ³¨ï¼Œå…¶æ€§èƒ½ä¹Ÿèƒ½ä¸æˆ–è¶…è¿‡å…¨ç›‘ç£æ¨¡å‹ã€‚æœ¬ç ”ç©¶ä¸ºåŒ»å­¦å›¾åƒåˆ†å‰²é¢†åŸŸæä¾›äº†ä¸€ç§å¯æ‰©å±•çš„ç«¯åˆ°ç«¯å­¦ä¹ ç­–ç•¥ï¼Œèƒ½å¤Ÿåœ¨ä¸ç‰ºç‰²ç²¾åº¦çš„æƒ…å†µä¸‹å‡å°‘æ ‡æ³¨éœ€æ±‚ã€‚ç›¸å…³ä»£ç å¯é€šè¿‡é“¾æ¥è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/Ordi117/nnFilterMatch.git%E3%80%82">https://github.com/Ordi117/nnFilterMatch.gitã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>åŠç›‘ç£å­¦ä¹ ä¸ä¸»åŠ¨å­¦ä¹ ç»“åˆï¼Œæ˜¾è‘—å‡å°‘äº†åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­æ‰‹åŠ¨æ ‡æ³¨çš„éœ€æ±‚ã€‚</li>
<li>æå‡ºäº†æ–°å‹çš„æ ‡æ³¨é«˜æ•ˆè‡ªé€‚åº”æ·±åº¦åˆ†å‰²æ¡†æ¶nnFilterMatchã€‚</li>
<li>é€šè¿‡ç»“åˆSSLå’ŒåŸºäºç†µçš„ä¼ªæ ‡ç­¾è¿‡æ»¤ï¼ˆFilterMatchï¼‰ï¼Œå®ç°äº†æ— éœ€å†è®­ç»ƒå¾ªç¯çš„å­¦ä¹ è¿‡ç¨‹ã€‚</li>
<li>æ–¹æ³•åœ¨å¤šä¸ªä¸´åºŠåˆ†å‰²åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼Œä¸å…¨ç›‘ç£æ¨¡å‹ç›¸å½“æˆ–æ›´ä¼˜ã€‚</li>
<li>ä»…éœ€å°‘é‡æ ‡æ³¨æ•°æ®ï¼ˆ5%~20%ï¼‰ï¼Œå³å¯å®ç°é«˜æ€§èƒ½åˆ†å‰²ã€‚</li>
<li>æä¾›äº†å¯æ‰©å±•çš„ç«¯åˆ°ç«¯å­¦ä¹ ç­–ç•¥ï¼Œæé«˜äº†åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„å®é™…åº”ç”¨èƒ½åŠ›ã€‚</li>
<li>ç›¸å…³ä»£ç å¯é€šè¿‡åœ¨çº¿é“¾æ¥è®¿é—®ï¼Œä¾¿äºç ”ç©¶è€…å’Œå¼€å‘è€…ä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.19746">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2ce22a11b6ea02c467f30bb07b8b6df2" align="middle">
<img src="https://picx.zhimg.com/v2-d2a3fd52620ab5684cfacf7ed7de70f5" align="middle">
<img src="https://picx.zhimg.com/v2-b495b0fca097af7c459dda69e983766d" align="middle">
<img src="https://picx.zhimg.com/v2-359f719b11f87120db5455ffcb19a101" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Frequency-domain-Multi-modal-Fusion-for-Language-guided-Medical-Image-Segmentation"><a href="#Frequency-domain-Multi-modal-Fusion-for-Language-guided-Medical-Image-Segmentation" class="headerlink" title="Frequency-domain Multi-modal Fusion for Language-guided Medical Image   Segmentation"></a>Frequency-domain Multi-modal Fusion for Language-guided Medical Image   Segmentation</h2><p><strong>Authors:Bo Yu, Jianhua Yang, Zetao Du, Yan Huang, Chenglong Li, Liang Wang</strong></p>
<p>Automatically segmenting infected areas in radiological images is essential for diagnosing pulmonary infectious diseases. Recent studies have demonstrated that the accuracy of the medical image segmentation can be improved by incorporating clinical text reports as semantic guidance. However, the complex morphological changes of lesions and the inherent semantic gap between vision-language modalities prevent existing methods from effectively enhancing the representation of visual features and eliminating semantically irrelevant information, ultimately resulting in suboptimal segmentation performance. To address these problems, we propose a Frequency-domain Multi-modal Interaction model (FMISeg) for language-guided medical image segmentation. FMISeg is a late fusion model that establishes interaction between linguistic features and frequency-domain visual features in the decoder. Specifically, to enhance the visual representation, our method introduces a Frequency-domain Feature Bidirectional Interaction (FFBI) module to effectively fuse frequency-domain features. Furthermore, a Language-guided Frequency-domain Feature Interaction (LFFI) module is incorporated within the decoder to suppress semantically irrelevant visual features under the guidance of linguistic information. Experiments on QaTa-COV19 and MosMedData+ demonstrated that our method outperforms the state-of-the-art methods qualitatively and quantitatively. </p>
<blockquote>
<p>å¯¹æ”¾å°„å›¾åƒä¸­çš„æ„ŸæŸ“åŒºåŸŸè¿›è¡Œè‡ªåŠ¨åˆ†å‰²æ˜¯è¯Šæ–­è‚ºéƒ¨æ„ŸæŸ“æ€§ç–¾ç—…çš„å…³é”®ã€‚è¿‘æœŸç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡èå…¥ä¸´åºŠæ–‡æœ¬æŠ¥å‘Šä½œä¸ºè¯­ä¹‰æŒ‡å¯¼ï¼Œå¯ä»¥æé«˜åŒ»å­¦å›¾åƒåˆ†å‰²çš„å‡†ç¡®æ€§ã€‚ç„¶è€Œï¼Œç—…ç¶çš„å¤æ‚å½¢æ€å˜åŒ–ä»¥åŠè§†è§‰è¯­è¨€æ¨¡æ€ä¹‹é—´å›ºæœ‰çš„è¯­ä¹‰é¸¿æ²Ÿï¼Œä½¿å¾—ç°æœ‰æ–¹æ³•æ— æ³•æœ‰æ•ˆåœ°å¢å¼ºè§†è§‰ç‰¹å¾çš„è¡¨ç¤ºå¹¶æ¶ˆé™¤è¯­ä¹‰ä¸Šæ— å…³çš„ä¿¡æ¯ï¼Œæœ€ç»ˆå¯¼è‡´åˆ†å‰²æ€§èƒ½ä¸ä½³ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è¯­è¨€å¼•å¯¼åŒ»å­¦å›¾åƒåˆ†å‰²çš„é¢‘åŸŸå¤šæ¨¡æ€äº¤äº’æ¨¡å‹ï¼ˆFMISegï¼‰ã€‚FMISegæ˜¯ä¸€ç§åæœŸèåˆæ¨¡å‹ï¼Œåœ¨è§£ç å™¨ä¸­å»ºç«‹è¯­è¨€ç‰¹å¾ä¸é¢‘åŸŸè§†è§‰ç‰¹å¾ä¹‹é—´çš„äº’åŠ¨ã€‚å…·ä½“æ¥è¯´ï¼Œä¸ºäº†å¢å¼ºè§†è§‰è¡¨ç¤ºï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†ä¸€ä¸ªé¢‘åŸŸç‰¹å¾åŒå‘äº¤äº’ï¼ˆFFBIï¼‰æ¨¡å—ï¼Œä»¥æœ‰æ•ˆåœ°èåˆé¢‘åŸŸç‰¹å¾ã€‚æ­¤å¤–ï¼Œè§£ç å™¨ä¸­è¿˜èå…¥äº†ä¸€ä¸ªè¯­è¨€å¼•å¯¼é¢‘åŸŸç‰¹å¾äº¤äº’ï¼ˆLFFIï¼‰æ¨¡å—ï¼Œåœ¨è¯­è¨€çš„æŒ‡å¯¼ä¸‹æŠ‘åˆ¶è¯­ä¹‰ä¸Šæ— å…³çš„è§†è§‰ç‰¹å¾ã€‚åœ¨QaTa-COV19å’ŒMosMedData+ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å®šæ€§å’Œå®šé‡ä¸Šå‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.19719v1">PDF</a> Accepted by MICCAI 2025</p>
<p><strong>Summary</strong><br>     é’ˆå¯¹è‚ºéƒ¨æ„ŸæŸ“æ€§ç–¾ç—…è¯Šæ–­ä¸­çš„å›¾åƒè‡ªåŠ¨åˆ†å‰²é—®é¢˜ï¼Œèåˆä¸´åºŠæ–‡æœ¬æŠ¥å‘Šä½œä¸ºè¯­ä¹‰æŒ‡å¯¼èƒ½æé«˜åŒ»å­¦å›¾åƒåˆ†å‰²çš„å‡†ç¡®æ€§ã€‚ä¸ºè§£å†³ç—…ç¶å¤æ‚å½¢æ€å˜åŒ–å’Œè§†è§‰è¯­è¨€æ¨¡æ€é—´çš„è¯­ä¹‰é¸¿æ²Ÿç­‰é—®é¢˜ï¼Œæå‡ºä¸€ç§åŸºäºé¢‘åŸŸå¤šæ¨¡æ€äº¤äº’æ¨¡å‹ï¼ˆFMISegï¼‰ã€‚è¯¥æ¨¡å‹ä¸ºè¯­è¨€å¼•å¯¼ä¸‹çš„åŒ»å­¦å›¾åƒåˆ†å‰²çš„åæœŸèåˆæ¨¡å‹ï¼Œåœ¨è§£ç å™¨ä¸­å»ºç«‹è¯­è¨€ç‰¹å¾ä¸é¢‘åŸŸè§†è§‰ç‰¹å¾ä¹‹é—´çš„äº¤äº’ã€‚é€šè¿‡å¼•å…¥é¢‘åŸŸç‰¹å¾åŒå‘äº¤äº’ï¼ˆFFBIï¼‰æ¨¡å—å¢å¼ºè§†è§‰è¡¨å¾ï¼Œå¹¶ç»“åˆè¯­è¨€å¼•å¯¼é¢‘åŸŸç‰¹å¾äº¤äº’ï¼ˆLFFIï¼‰æ¨¡å—ï¼Œåœ¨è¯­è¨€çš„å¼•å¯¼ä¸‹æŠ‘åˆ¶è¯­ä¹‰ä¸Šæ— å…³çš„è§†è§‰ç‰¹å¾ã€‚åœ¨QaTa-COV19å’ŒMosMedData+ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å®šæ€§å’Œå®šé‡ä¸Šå‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>åŒ»å­¦å›¾åƒè‡ªåŠ¨åˆ†å‰²å¯¹è¯Šæ–­è‚ºéƒ¨æ„ŸæŸ“æ€§ç–¾ç—…è‡³å…³é‡è¦ã€‚</li>
<li>èåˆä¸´åºŠæ–‡æœ¬æŠ¥å‘Šä½œä¸ºè¯­ä¹‰æŒ‡å¯¼å¯ä»¥æé«˜åŒ»å­¦å›¾åƒåˆ†å‰²çš„å‡†ç¡®æ€§ã€‚</li>
<li>ç°æœ‰æ–¹æ³•é¢ä¸´ç—…ç¶å¤æ‚å½¢æ€å˜åŒ–å’Œè§†è§‰è¯­è¨€æ¨¡æ€é—´çš„è¯­ä¹‰é¸¿æ²Ÿç­‰æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºä¸€ç§åŸºäºé¢‘åŸŸå¤šæ¨¡æ€äº¤äº’æ¨¡å‹ï¼ˆFMISegï¼‰è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>FMISegæ¨¡å‹é€šè¿‡å¼•å…¥FFBIæ¨¡å—å¢å¼ºè§†è§‰è¡¨å¾ï¼Œå¹¶ç»“åˆLFFIæ¨¡å—æŠ‘åˆ¶è¯­ä¹‰ä¸Šæ— å…³çš„è§†è§‰ç‰¹å¾ã€‚</li>
<li>FMISegæ¨¡å‹åœ¨QaTa-COV19å’ŒMosMedData+æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.19719">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-aa312f30f6f7a281a1f83e5606f3f792" align="middle">
<img src="https://picx.zhimg.com/v2-536a69ce723261bf8f2a81fdfeb09008" align="middle">
<img src="https://picx.zhimg.com/v2-5224a0b0add243fea46a0da76906b786" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Towards-Robust-In-Context-Learning-for-Medical-Image-Segmentation-via-Data-Synthesis"><a href="#Towards-Robust-In-Context-Learning-for-Medical-Image-Segmentation-via-Data-Synthesis" class="headerlink" title="Towards Robust In-Context Learning for Medical Image Segmentation via   Data Synthesis"></a>Towards Robust In-Context Learning for Medical Image Segmentation via   Data Synthesis</h2><p><strong>Authors:Jiesi Hu, Yanwu Yang, Zhiyu Ye, Chenfei Ye, Hanyang Peng, Jianfeng Cao, Ting Ma</strong></p>
<p>The rise of In-Context Learning (ICL) for universal medical image segmentation has introduced an unprecedented demand for large-scale, diverse datasets for training, exacerbating the long-standing problem of data scarcity. While data synthesis offers a promising solution, existing methods often fail to simultaneously achieve both high data diversity and a domain distribution suitable for medical data. To bridge this gap, we propose \textbf{SynthICL}, a novel data synthesis framework built upon domain randomization. SynthICL ensures realism by leveraging anatomical priors from real-world datasets, generates diverse anatomical structures to cover a broad data distribution, and explicitly models inter-subject variations to create data cohorts suitable for ICL. Extensive experiments on four held-out datasets validate our frameworkâ€™s effectiveness, showing that models trained with our data achieve performance gains of up to 63% in average Dice and substantially enhanced generalization to unseen anatomical domains. Our work helps mitigate the data bottleneck for ICL-based segmentation, paving the way for robust models. Our code and the generated dataset are publicly available at <a target="_blank" rel="noopener" href="https://github.com/jiesihu/Neuroverse3D">https://github.com/jiesihu/Neuroverse3D</a>. </p>
<blockquote>
<p>åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²é¢†åŸŸï¼Œä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰çš„å…´èµ·å¸¦æ¥äº†å¯¹å¤§è§„æ¨¡ã€å¤šæ ·åŒ–è®­ç»ƒæ•°æ®é›†å‰æ‰€æœªæœ‰çš„éœ€æ±‚ï¼ŒåŠ å‰§äº†é•¿æœŸå­˜åœ¨çš„æ•°æ®ç¨€ç¼ºé—®é¢˜ã€‚è™½ç„¶æ•°æ®åˆæˆæä¾›äº†ä¸€ä¸ªæœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆï¼Œä½†ç°æœ‰æ–¹æ³•å¾€å¾€ä¸èƒ½åŒæ—¶å®ç°é«˜æ•°æ®å¤šæ ·æ€§å’Œé€‚åˆåŒ»å­¦æ•°æ®çš„é¢†åŸŸåˆ†å¸ƒã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºé¢†åŸŸéšæœºåŒ–çš„æ–°å‹æ•°æ®åˆæˆæ¡†æ¶<strong>SynthICL</strong>ã€‚SynthICLé€šè¿‡åˆ©ç”¨çœŸå®ä¸–ç•Œæ•°æ®é›†çš„äººä½“è§£å‰–å…ˆéªŒçŸ¥è¯†æ¥ç¡®ä¿çœŸå®æ€§ï¼Œç”Ÿæˆå¤šæ ·åŒ–çš„äººä½“è§£å‰–ç»“æ„ä»¥è¦†ç›–å¹¿æ³›çš„æ•°æ®åˆ†å¸ƒï¼Œå¹¶æ˜¾å¼åœ°æ¨¡æ‹Ÿå—è¯•è€…ä¹‹é—´çš„å·®å¼‚æ¥åˆ›å»ºé€‚åˆICLçš„æ•°æ®é›†ã€‚åœ¨å››ä¸ªç‹¬ç«‹æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒéªŒè¯äº†æˆ‘ä»¬æ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼Œç»“æœæ˜¾ç¤ºä½¿ç”¨æˆ‘ä»¬çš„æ•°æ®è¿›è¡Œè®­ç»ƒçš„æ¨¡å‹åœ¨å¹³å‡Diceç³»æ•°ä¸Šæé«˜äº†é«˜è¾¾63%ï¼Œå¹¶ä¸”åœ¨æœªè§è¿‡çš„è§£å‰–é¢†åŸŸå®ç°äº†æ˜¾è‘—å¢å¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬çš„å·¥ä½œç¼“è§£äº†åŸºäºICLçš„åˆ†å‰²ä¸­çš„æ•°æ®ç“¶é¢ˆé—®é¢˜ï¼Œä¸ºç¨³å¥çš„æ¨¡å‹é“ºå¹³äº†é“è·¯ã€‚æˆ‘ä»¬çš„ä»£ç å’Œç”Ÿæˆçš„æ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/jiesihu/Neuroverse3D%E4%B8%8A%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/jiesihu/Neuroverse3Dä¸Šå…¬å¼€è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.19711v1">PDF</a> </p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰çš„å…´èµ·å¯¹å¤§å°ºåº¦ã€å¤šæ ·åŒ–æ•°æ®é›†çš„éœ€æ±‚æ€¥å‰§å¢åŠ ï¼ŒåŠ å‰§äº†é•¿æœŸå­˜åœ¨çš„æ•°æ®ç¨€ç¼ºé—®é¢˜ã€‚å°½ç®¡æ•°æ®åˆæˆæä¾›äº†æœ‰å¸Œæœ›çš„è§£å†³æ–¹æ¡ˆï¼Œä½†ç°æœ‰æ–¹æ³•å¾€å¾€éš¾ä»¥åŒæ—¶å®ç°æ•°æ®å¤šæ ·æ€§å’Œé€‚åˆåŒ»å­¦æ•°æ®çš„é¢†åŸŸåˆ†å¸ƒã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºé¢†åŸŸéšæœºåŒ–çš„æ–°å‹æ•°æ®åˆæˆæ¡†æ¶SynthICLã€‚å®ƒé€šè¿‡åˆ©ç”¨çœŸå®æ•°æ®é›†çš„äººä½“è§£å‰–å…ˆéªŒçŸ¥è¯†ç¡®ä¿çœŸå®æ€§ï¼Œç”Ÿæˆå¤šæ ·åŒ–çš„äººä½“è§£å‰–ç»“æ„ä»¥è¦†ç›–å¹¿æ³›çš„æ•°æ®åˆ†å¸ƒï¼Œå¹¶æ˜¾å¼åœ°æ¨¡æ‹Ÿä¸»ä½“é—´å˜åŒ–ä»¥åˆ›å»ºé€‚åˆICLçš„æ•°æ®ç¾¤ä½“ã€‚åœ¨å››ä¸ªç‹¬ç«‹æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒéªŒè¯äº†æˆ‘ä»¬æ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼Œè¡¨æ˜ä½¿ç”¨æˆ‘ä»¬çš„æ•°æ®è¿›è¡Œè®­ç»ƒæ¨¡å‹å¹³å‡Diceæå‡é«˜è¾¾63%ï¼Œå¹¶åœ¨æœªè§è¿‡çš„è§£å‰–é¢†åŸŸå®ç°æ˜¾è‘—å¢å¼ºçš„ä¸€èˆ¬åŒ–ã€‚æˆ‘ä»¬çš„å·¥ä½œæœ‰åŠ©äºç¼“è§£åŸºäºICLçš„åˆ†å‰²ä¸­çš„æ•°æ®ç“¶é¢ˆé—®é¢˜ï¼Œä¸ºç¨³å¥æ¨¡å‹çš„å‘å±•é“ºå¹³é“è·¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>In-Context Learning (ICL) åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„éœ€æ±‚å¯¼è‡´å¤§è§„æ¨¡ã€å¤šæ ·åŒ–æ•°æ®é›†çš„éœ€æ±‚å¢åŠ ã€‚</li>
<li>æ•°æ®åˆæˆæ˜¯è§£å†³æ•°æ®ç¨€ç¼ºé—®é¢˜çš„ä¸€ç§æœ‰å‰é€”çš„æ–¹æ³•ã€‚</li>
<li>ç°æœ‰æ•°æ®åˆæˆæ–¹æ³•éš¾ä»¥åŒæ—¶å®ç°æ•°æ®å¤šæ ·æ€§å’Œé€‚åˆåŒ»å­¦æ•°æ®çš„é¢†åŸŸåˆ†å¸ƒã€‚</li>
<li>æå‡ºçš„SynthICLæ¡†æ¶åŸºäºé¢†åŸŸéšæœºåŒ–ï¼Œåˆ©ç”¨çœŸå®æ•°æ®é›†çš„äººä½“è§£å‰–å…ˆéªŒçŸ¥è¯†ç¡®ä¿çœŸå®æ€§ã€‚</li>
<li>SynthICLèƒ½ç”Ÿæˆå¤šæ ·åŒ–çš„äººä½“è§£å‰–ç»“æ„ï¼Œè¦†ç›–å¹¿æ³›çš„æ•°æ®åˆ†å¸ƒï¼Œå¹¶æ¨¡æ‹Ÿä¸»ä½“é—´å˜åŒ–ã€‚</li>
<li>åœ¨å››ä¸ªç‹¬ç«‹æ•°æ®é›†ä¸Šçš„å®éªŒæ˜¾ç¤ºï¼Œä½¿ç”¨SynthICLæ•°æ®è®­ç»ƒçš„æ¨¡å‹æ€§èƒ½æœ‰æ‰€æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.19711">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4a056ef3bc4029836b84a0e9631c08fd" align="middle">
<img src="https://picx.zhimg.com/v2-4a17fe45b51711425e1c7d234645d519" align="middle">
<img src="https://picx.zhimg.com/v2-331bd2d7632ea1af4eddacfa69a2240e" align="middle">
<img src="https://picx.zhimg.com/v2-e1c97890d41d16f2c27704d91fe97ec5" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Graph-Radiomic-Learning-GrRAiL-Descriptor-to-Characterize-Imaging-Heterogeneity-in-Confounding-Tumor-Pathologies"><a href="#Graph-Radiomic-Learning-GrRAiL-Descriptor-to-Characterize-Imaging-Heterogeneity-in-Confounding-Tumor-Pathologies" class="headerlink" title="Graph-Radiomic Learning (GrRAiL) Descriptor to Characterize Imaging   Heterogeneity in Confounding Tumor Pathologies"></a>Graph-Radiomic Learning (GrRAiL) Descriptor to Characterize Imaging   Heterogeneity in Confounding Tumor Pathologies</h2><p><strong>Authors:Dheerendranath Battalapalli, Apoorva Safai, Maria Jaramillo, Hyemin Um, Gustavo Adalfo Pineda Ortiz, Ulas Bagci, Manmeet Singh Ahluwalia, Marwa Ismail, Pallavi Tiwari</strong></p>
<p>A significant challenge in solid tumors is reliably distinguishing confounding pathologies from malignant neoplasms on routine imaging. While radiomics methods seek surrogate markers of lesion heterogeneity on CT&#x2F;MRI, many aggregate features across the region of interest (ROI) and miss complex spatial relationships among varying intensity compositions. We present a new Graph-Radiomic Learning (GrRAiL) descriptor for characterizing intralesional heterogeneity (ILH) on clinical MRI scans. GrRAiL (1) identifies clusters of sub-regions using per-voxel radiomic measurements, then (2) computes graph-theoretic metrics to quantify spatial associations among clusters. The resulting weighted graphs encode higher-order spatial relationships within the ROI, aiming to reliably capture ILH and disambiguate confounding pathologies from malignancy. To assess efficacy and clinical feasibility, GrRAiL was evaluated in n&#x3D;947 subjects spanning three use cases: differentiating tumor recurrence from radiation effects in glioblastoma (GBM; n&#x3D;106) and brain metastasis (n&#x3D;233), and stratifying pancreatic intraductal papillary mucinous neoplasms (IPMNs) into no+low vs high risk (n&#x3D;608). In a multi-institutional setting, GrRAiL consistently outperformed state-of-the-art baselines - Graph Neural Networks (GNNs), textural radiomics, and intensity-graph analysis. In GBM, cross-validation (CV) and test accuracies for recurrence vs pseudo-progression were 89% and 78% with &gt;10% test-accuracy gains over comparators. In brain metastasis, CV and test accuracies for recurrence vs radiation necrosis were 84% and 74% (&gt;13% improvement). For IPMN risk stratification, CV and test accuracies were 84% and 75%, showing &gt;10% improvement. </p>
<blockquote>
<p>åœ¨å®ä½“è‚¿ç˜¤ä¸­ï¼Œä¸€ä¸ªé‡å¤§çš„æŒ‘æˆ˜æ˜¯åœ¨å¸¸è§„æˆåƒä¸­å¯é åœ°åŒºåˆ†æ··æ·†çš„ç—…ç†å’Œæ¶æ€§è‚¿ç˜¤ã€‚è™½ç„¶æ”¾å°„å­¦æ–¹æ³•è¯•å›¾åœ¨CT&#x2F;MRIä¸Šå¯»æ‰¾ç—…å˜å¼‚è´¨æ€§çš„æ›¿ä»£æ ‡å¿—ç‰©ï¼Œä½†è®¸å¤šç‰¹å¾éƒ½é›†ä¸­åœ¨æ„Ÿå…´è¶£åŒºåŸŸï¼ˆROIï¼‰å†…ï¼Œè€Œå¿½ç•¥äº†ä¸åŒå¼ºåº¦æˆåˆ†ä¹‹é—´å¤æ‚çš„ç©ºé—´å…³ç³»ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„åŸºäºå›¾æ”¾å°„å­¦å­¦ä¹ ï¼ˆGrRAiLï¼‰æè¿°ç¬¦ï¼Œç”¨äºè¡¨å¾ä¸´åºŠMRIæ‰«æä¸­çš„ç—…ç¶å†…å¼‚è´¨æ€§ï¼ˆILHï¼‰ã€‚GrRAiLï¼ˆ1ï¼‰ä½¿ç”¨åŸºäºä½“ç´ çš„æ”¾å°„å­¦æµ‹é‡æ¥è¯†åˆ«å­åŒºåŸŸé›†ç¾¤ï¼Œç„¶åï¼ˆ2ï¼‰è®¡ç®—å›¾è®ºåº¦é‡æ¥é‡åŒ–é›†ç¾¤ä¹‹é—´çš„ç©ºé—´å…³è”ã€‚æ‰€å¾—çš„åŠ æƒå›¾ç¼–ç äº†ROIå†…çš„é«˜é˜¶ç©ºé—´å…³ç³»ï¼Œæ—¨åœ¨å¯é åœ°æ•è·ILHï¼Œå¹¶ä»æ··æ·†çš„ç—…ç†ä¸­åŒºåˆ†å‡ºæ¶æ€§è‚¿ç˜¤ã€‚ä¸ºäº†è¯„ä¼°GrRAiLçš„æœ‰æ•ˆæ€§å’Œä¸´åºŠå¯è¡Œæ€§ï¼Œæˆ‘ä»¬åœ¨947åå—è¯•è€…ä¸­è¿›è¡Œäº†è¯„ä¼°ï¼Œæ¶‰åŠä¸‰ç§åº”ç”¨åœºæ™¯ï¼šåŒºåˆ†èƒ¶è´¨æ¯ç»†èƒç˜¤ï¼ˆGBMï¼›n&#x3D;106ï¼‰å’Œè„‘è½¬ç§»ç˜¤ï¼ˆn&#x3D;233ï¼‰ä¸­çš„è‚¿ç˜¤å¤å‘ä¸è¾å°„æ•ˆåº”ï¼Œä»¥åŠå°†èƒ°è…ºå¯¼ç®¡å†…ä¹³å¤´çŠ¶é»æ¶²ç˜¤ï¼ˆIPMNsï¼‰åˆ†å±‚ä¸ºä½é£é™©å’Œæ— +ä½é£é™©ç»„å’Œé«˜é£é™©ç»„ï¼ˆn&#x3D;608ï¼‰ã€‚åœ¨å¤šæœºæ„è®¾ç½®ä¸­ï¼ŒGrRAiLæŒç»­ä¼˜äºæœ€å…ˆè¿›çš„åŸºçº¿æŠ€æœ¯â€”â€”å›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰ã€çº¹ç†æ”¾å°„å­¦å’Œå¼ºåº¦å›¾åˆ†æã€‚åœ¨GBMä¸­ï¼Œå¯¹äºå¤å‘ä¸å‡è¿›å±•çš„äº¤å‰éªŒè¯ï¼ˆCVï¼‰å’Œæµ‹è¯•å‡†ç¡®ç‡åˆ†åˆ«ä¸º89%å’Œ78%ï¼Œè¾ƒæ¯”è¾ƒæ–¹æ³•æé«˜äº†è¶…è¿‡10%çš„æµ‹è¯•å‡†ç¡®ç‡ã€‚åœ¨è„‘è½¬ç§»ç˜¤ä¸­ï¼Œå¯¹äºå¤å‘ä¸è¾å°„åæ­»çš„CVå’Œæµ‹è¯•å‡†ç¡®ç‡åˆ†åˆ«ä¸º84%å’Œ74%ï¼ˆæé«˜13%ï¼‰ã€‚å¯¹äºIPMNçš„é£é™©åˆ†å±‚ï¼ŒCVå’Œæµ‹è¯•å‡†ç¡®ç‡åˆ†åˆ«ä¸º84%å’Œ75%ï¼Œæ˜¾ç¤ºå‡ºè¶…è¿‡10%çš„æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.19258v1">PDF</a> Under Review: npj Digital Medicine</p>
<p><strong>Summary</strong><br>     é’ˆå¯¹å®ä½“ç˜¤åœ¨å¸¸è§„æˆåƒä¸­å¯é åœ°åŒºåˆ†æ··æ·†æ€§ç—…ç†ä¸æ¶æ€§è‚¿ç˜¤çš„éš¾é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„Graph-Radiomic Learningï¼ˆGrRAiLï¼‰æè¿°ç¬¦ï¼Œç”¨äºè¡¨å¾ä¸´åºŠMRIæ‰«æä¸­çš„ç—…ç¶å†…å¼‚è´¨æ€§ï¼ˆILHï¼‰ã€‚GrRAiLé€šè¿‡ä½“ç´ çº§æ”¾å°„å­¦æµ‹é‡è¯†åˆ«å­åŒºåŸŸé›†ç¾¤ï¼Œå¹¶è®¡ç®—å›¾å½¢ç†è®ºåº¦é‡ä»¥é‡åŒ–é›†ç¾¤ä¹‹é—´çš„ç©ºé—´å…³è”ã€‚è¯„ä¼°æ˜¾ç¤ºï¼ŒGrRAiLåœ¨å¤šæœºæ„ç¯å¢ƒä¸­ï¼Œç›¸è¾ƒäºåŸºçº¿æ–¹æ³•å¦‚å›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰ã€çº¹ç†æ”¾å°„å­¦åŠå¼ºåº¦å›¾åˆ†æï¼Œè¡¨ç°å‡ºæ›´ä¼˜å¼‚çš„è¡¨ç°ã€‚åœ¨èƒ¶è´¨æ¯ç»†èƒç˜¤ï¼ˆGBMï¼‰ã€è„‘è½¬ç§»ç™Œä»¥åŠèƒ°è…ºå¯¼ç®¡å†…ä¹³å¤´çŠ¶é»æ¶²ç˜¤ï¼ˆIPMNï¼‰çš„é£é™©åˆ†å±‚ç­‰å¤šä¸ªåº”ç”¨åœºæ™¯ä¸­ï¼ŒGrRAiLçš„äº¤å‰éªŒè¯å’Œæµ‹è¯•å‡†ç¡®ç‡å‡æ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å®ä½“ç˜¤åœ¨å¸¸è§„æˆåƒä¸­åŒºåˆ†æ··æ·†æ€§ç—…ç†ä¸æ¶æ€§è‚¿ç˜¤å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>Graph-Radiomic Learning (GrRAiL) æè¿°ç¬¦ç”¨äºè¡¨å¾ä¸´åºŠMRIæ‰«æä¸­çš„ç—…ç¶å†…å¼‚è´¨æ€§ï¼ˆILHï¼‰ã€‚</li>
<li>GrRAiLé€šè¿‡è¯†åˆ«å­åŒºåŸŸé›†ç¾¤å’Œè®¡ç®—å›¾å½¢ç†è®ºåº¦é‡ï¼Œé‡åŒ–ç©ºé—´å…³è”ã€‚</li>
<li>GrRAiLåœ¨å¤šæœºæ„ç¯å¢ƒä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä¼˜äºå›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰ã€çº¹ç†æ”¾å°„å­¦åŠå¼ºåº¦å›¾åˆ†æç­‰æ–¹æ³•ã€‚</li>
<li>åœ¨èƒ¶è´¨æ¯ç»†èƒç˜¤ï¼ˆGBMï¼‰åŒºåˆ†å¤å‘ä¸è¾å°„æ•ˆåº”æ–¹é¢ï¼ŒGrRAiLè¡¨ç°å‡ºé«˜å‡†ç¡®ç‡ã€‚</li>
<li>åœ¨è„‘è½¬ç§»ç™ŒåŒºåˆ†å¤å‘ä¸åæ­»æ–¹é¢ï¼ŒGrRAiLåŒæ ·å±•ç°å‡ºé«˜å‡†ç¡®ç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.19258">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-735b48bef55a86b4e4412700a1ca302c" align="middle">
<img src="https://picx.zhimg.com/v2-ff16c34370bb386c8026539e617fc819" align="middle">
<img src="https://picx.zhimg.com/v2-5717fb837b578255ed3b05cabed2d748" align="middle">
<img src="https://picx.zhimg.com/v2-726284912b86b25d60f795829f872ae7" align="middle">
<img src="https://picx.zhimg.com/v2-5ab9a5fd1034e3601e1e6724f70d4182" align="middle">
<img src="https://picx.zhimg.com/v2-712ac9bbc8dc09081c6a0b5407395daf" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Citrus-V-Advancing-Medical-Foundation-Models-with-Unified-Medical-Image-Grounding-for-Clinical-Reasoning"><a href="#Citrus-V-Advancing-Medical-Foundation-Models-with-Unified-Medical-Image-Grounding-for-Clinical-Reasoning" class="headerlink" title="Citrus-V: Advancing Medical Foundation Models with Unified Medical Image   Grounding for Clinical Reasoning"></a>Citrus-V: Advancing Medical Foundation Models with Unified Medical Image   Grounding for Clinical Reasoning</h2><p><strong>Authors:Guoxin Wang, Jun Zhao, Xinyi Liu, Yanbo Liu, Xuyang Cao, Chao Li, Zhuoyun Liu, Qintian Sun, Fangru Zhou, Haoqiang Xing, Zhenhong Yang</strong></p>
<p>Medical imaging provides critical evidence for clinical diagnosis, treatment planning, and surgical decisions, yet most existing imaging models are narrowly focused and require multiple specialized networks, limiting their generalization. Although large-scale language and multimodal models exhibit strong reasoning and multi-task capabilities, real-world clinical applications demand precise visual grounding, multimodal integration, and chain-of-thought reasoning. We introduce Citrus-V, a multimodal medical foundation model that combines image analysis with textual reasoning. The model integrates detection, segmentation, and multimodal chain-of-thought reasoning, enabling pixel-level lesion localization, structured report generation, and physician-like diagnostic inference in a single framework. We propose a novel multimodal training approach and release a curated open-source data suite covering reasoning, detection, segmentation, and document understanding tasks. Evaluations demonstrate that Citrus-V outperforms existing open-source medical models and expert-level imaging systems across multiple benchmarks, delivering a unified pipeline from visual grounding to clinical reasoning and supporting precise lesion quantification, automated reporting, and reliable second opinions. </p>
<blockquote>
<p>åŒ»å­¦æˆåƒä¸ºä¸´åºŠè¯Šæ–­ã€æ²»ç–—è®¡åˆ’å’Œæ‰‹æœ¯å†³ç­–æä¾›å…³é”®è¯æ®ï¼Œä½†å¤§å¤šæ•°ç°æœ‰æˆåƒæ¨¡å‹ä¸“æ³¨äºç‰¹å®šä»»åŠ¡ï¼Œéœ€è¦å¤šä¸ªä¸“ç”¨ç½‘ç»œï¼Œä»è€Œé™åˆ¶äº†å…¶é€šç”¨æ€§ã€‚å°½ç®¡å¤§è§„æ¨¡è¯­è¨€å’Œå¤šåª’ä½“æ¨¡å‹å±•ç°å‡ºå¼ºå¤§çš„æ¨ç†å’Œå¤šä»»åŠ¡èƒ½åŠ›ï¼Œä½†ç°å®ä¸–ç•Œçš„ä¸´åºŠåº”ç”¨éœ€è¦ç²¾ç¡®çš„è§†è§‰å®šä½ã€å¤šåª’ä½“èåˆå’Œè¿è´¯æ¨ç†ã€‚æˆ‘ä»¬ä»‹ç»äº†Citrus-Vï¼Œè¿™æ˜¯ä¸€ä¸ªç»“åˆå›¾åƒåˆ†æä¸æ–‡æœ¬æ¨ç†çš„å¤šåª’ä½“åŒ»å­¦åŸºç¡€æ¨¡å‹ã€‚è¯¥æ¨¡å‹é›†æˆäº†æ£€æµ‹ã€åˆ†å‰²å’Œå¤šåª’ä½“è¿è´¯æ¨ç†ï¼Œèƒ½å¤Ÿåœ¨å•ä¸€æ¡†æ¶å†…è¿›è¡Œåƒç´ çº§ç—…ç¶å®šä½ã€ç»“æ„åŒ–æŠ¥å‘Šç”Ÿæˆå’Œç±»ä¼¼åŒ»ç”Ÿçš„è¯Šæ–­æ¨æ–­ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„å¤šåª’ä½“è®­ç»ƒæ–¹æ³•å’Œä¸€ä¸ªç²¾é€‰çš„å¼€æºæ•°æ®é›†å¥—ä»¶ï¼Œæ¶µç›–æ¨ç†ã€æ£€æµ‹ã€åˆ†å‰²å’Œæ–‡æ¡£ç†è§£ä»»åŠ¡ã€‚è¯„ä¼°è¡¨æ˜ï¼ŒCitrus-Våœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ä¼˜äºç°æœ‰å¼€æºåŒ»å­¦æ¨¡å‹å’Œä¸“å®¶çº§æˆåƒç³»ç»Ÿï¼Œæä¾›äº†ä¸€ä¸ªä»è§†è§‰å®šä½åˆ°ä¸´åºŠæ¨ç†çš„ç»Ÿä¸€æµç¨‹ï¼Œå¹¶æ”¯æŒç²¾ç¡®ç—…ç¶é‡åŒ–ã€è‡ªåŠ¨åŒ–æŠ¥å‘Šå’Œå¯é äºŒæ¬¡æ„è§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.19090v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Citrus-Vè¿™ä¸€å¤šæ¨¡æ€åŒ»å­¦åŸºç¡€æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ç»“åˆäº†å›¾åƒåˆ†æä¸æ–‡æœ¬æ¨ç†ï¼Œå®ç°äº†åƒç´ çº§ç—…å˜å®šä½ã€ç»“æ„åŒ–æŠ¥å‘Šç”Ÿæˆå’ŒåŒ»ç”Ÿçº§åˆ«çš„è¯Šæ–­æ¨æ–­ã€‚å®ƒé‡‡ç”¨äº†ä¸€ç§æ–°é¢–çš„å¤šæ¨¡æ€è®­ç»ƒæ–¹æ³•ï¼Œå¹¶å‘å¸ƒäº†ä¸€å¥—æ¶µç›–æ¨ç†ã€æ£€æµ‹ã€åˆ†å‰²å’Œæ–‡æ¡£ç†è§£ä»»åŠ¡çš„å¼€æºæ•°æ®é›†ã€‚è¯„ä¼°è¡¨æ˜ï¼ŒCitrus-Våœ¨å¤šåŸºå‡†æµ‹è¯•ä¸­ä¼˜äºç°æœ‰å¼€æºåŒ»å­¦æ¨¡å‹å’Œä¸“å®¶çº§æˆåƒç³»ç»Ÿï¼Œä¸ºä»è§†è§‰å®šä½åˆ°ä¸´åºŠæ¨ç†æä¾›äº†ä¸€ä¸ªç»Ÿä¸€çš„ç®¡é“ï¼Œå¹¶æ”¯æŒç²¾ç¡®ç—…å˜é‡åŒ–ã€è‡ªåŠ¨åŒ–æŠ¥å‘Šå’Œå¯é äºŒæ¬¡æ„è§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Citrus-Væ˜¯ä¸€ä¸ªå¤šæ¨¡æ€åŒ»å­¦åŸºç¡€æ¨¡å‹ï¼Œç»“åˆäº†å›¾åƒåˆ†æå’Œæ–‡æœ¬æ¨ç†ã€‚</li>
<li>è¯¥æ¨¡å‹å®ç°äº†åƒç´ çº§ç—…å˜å®šä½ã€ç»“æ„åŒ–æŠ¥å‘Šç”Ÿæˆå’ŒåŒ»ç”Ÿçº§åˆ«çš„è¯Šæ–­æ¨æ–­ã€‚</li>
<li>Citrus-Vé‡‡ç”¨æ–°é¢–çš„å¤šæ¨¡æ€è®­ç»ƒæ–¹æ³•ï¼Œæ—¨åœ¨æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>æ¨¡å‹å‘å¸ƒäº†ä¸€å¥—æ¶µç›–å¤šä¸ªä»»åŠ¡çš„å¼€æºæ•°æ®é›†ï¼Œç”¨äºæ¨åŠ¨åŒ»å­¦æˆåƒç ”ç©¶ã€‚</li>
<li>Citrus-Våœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºç°æœ‰æ¨¡å‹å’Œä¸“å®¶ç³»ç»Ÿã€‚</li>
<li>å®ƒæä¾›äº†ä¸€ä¸ªç»Ÿä¸€çš„ç®¡é“ï¼Œä»è§†è§‰å®šä½åˆ°ä¸´åºŠæ¨ç†ï¼Œæ”¯æŒç²¾ç¡®ç—…å˜é‡åŒ–ã€è‡ªåŠ¨åŒ–æŠ¥å‘Šç”Ÿæˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.19090">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-36009d0a79f612a156d7c903ccfc6ab5" align="middle">
<img src="https://picx.zhimg.com/v2-a1e3069bbae55995e65608d793631659" align="middle">
<img src="https://picx.zhimg.com/v2-df89c51e854bad02ca320779c6636064" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-28/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-28/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-28/TTS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-e607b8f83468f4baea55d969f7d47e58" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-28  UniSS Unified Expressive Speech-to-Speech Translation with Your Voice
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-28
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-28/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-e5b81a0641aae81e9654bbc7f602975f" class="responsive-img" alt="ç‰™é½¿ä¿®å¤">
                        
                        <span class="card-title">ç‰™é½¿ä¿®å¤</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            ç‰™é½¿ä¿®å¤ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-28  An Interpretable Single-Index Mixed-Effects Model for Non-Gaussian   National Survey Data
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-28
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/" class="post-category">
                                    ç‰™é½¿ä¿®å¤
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                        <span class="chip bg-color">ç‰™é½¿ä¿®å¤</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">33125.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
