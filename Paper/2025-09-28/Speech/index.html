<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech 方向最新论文已更新，请持续关注 Update in 2025-09-28  MeanSE Efficient Generative Speech Enhancement with Mean Flows">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic-private.zhihu.com/v2-e94936736293e3c4c16cb9c549985aa1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036621&auth_key=1760036621-0-0-4742fa29e1c1238285fa6847bca983d5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-09-28
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-10-17
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    17k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    69 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-09-28-更新"><a href="#2025-09-28-更新" class="headerlink" title="2025-09-28 更新"></a>2025-09-28 更新</h1><h2 id="MeanSE-Efficient-Generative-Speech-Enhancement-with-Mean-Flows"><a href="#MeanSE-Efficient-Generative-Speech-Enhancement-with-Mean-Flows" class="headerlink" title="MeanSE: Efficient Generative Speech Enhancement with Mean Flows"></a>MeanSE: Efficient Generative Speech Enhancement with Mean Flows</h2><p><strong>Authors:Jiahe Wang, Hongyu Wang, Wei Wang, Lei Yang, Chenda Li, Wangyou Zhang, Lufen Tan, Yanmin Qian</strong></p>
<p>Speech enhancement (SE) improves degraded speech’s quality, with generative models like flow matching gaining attention for their outstanding perceptual quality. However, the flow-based model requires multiple numbers of function evaluations (NFEs) to achieve stable and satisfactory performance, leading to high computational load and poor 1-NFE performance. In this paper, we propose MeanSE, an efficient generative speech enhancement model using mean flows, which models the average velocity field to achieve high-quality 1-NFE enhancement. Experimental results demonstrate that our proposed MeanSE significantly outperforms the flow matching baseline with a single NFE, exhibiting extremely better out-of-domain generalization capabilities. </p>
<blockquote>
<p>语音增强（SE）旨在提高退化语音的质量，生成模型如流匹配由于其出色的感知质量而受到关注。然而，基于流的模型需要在函数评估次数（NFE）上多次迭代才能达到稳定和满意的性能，从而导致计算负载较高以及单NFE性能较差。在本文中，我们提出了MeanSE，这是一种使用平均流的有效生成语音增强模型，通过建模平均速度场实现高质量的单NFE增强。实验结果表明，我们提出的MeanSE在单NFE的情况下显著优于流匹配基线，并表现出极强的跨域泛化能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.21214v1">PDF</a> Submitted to ICASSP 2026</p>
<p><strong>Summary</strong></p>
<p>本文提出一种基于均值流的生成式语音增强模型MeanSE，通过建模平均速度场实现高质量的单步功能评估（1-NFE）增强。实验结果表明，与基于流动匹配的方法相比，MeanSE在单步评估时表现出显著优势，并具有较强的跨域泛化能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>语音增强（SE）旨在提高退化语音的质量。</li>
<li>生成模型如流匹配已引起关注，因其出色的感知质量。</li>
<li>流模型需要多次功能评估（NFEs）以达到稳定和满意性能，导致计算负载高和单步评估性能差。</li>
<li>本文提出MeanSE模型，使用均值流实现高效生成式语音增强。</li>
<li>MeanSE模型通过建模平均速度场实现高质量增强。</li>
<li>实验结果表明，MeanSE在单步功能评估时显著优于流动匹配方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.21214">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-6b6e255c2c574b3330430f114d369097~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036628&auth_key=1760036628-0-0-ecd43282de168951a4de34fa222c382e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6b18e71d0b68f597bea739b91df96b0b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036636&auth_key=1760036636-0-0-9ce22911c4c3596619231218e56e6a0d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Hybrid-Real-And-Complex-Valued-Neural-Network-Concept-For-Low-Complexity-Phase-Aware-Speech-Enhancement"><a href="#Hybrid-Real-And-Complex-Valued-Neural-Network-Concept-For-Low-Complexity-Phase-Aware-Speech-Enhancement" class="headerlink" title="Hybrid Real- And Complex-Valued Neural Network Concept For   Low-Complexity Phase-Aware Speech Enhancement"></a>Hybrid Real- And Complex-Valued Neural Network Concept For   Low-Complexity Phase-Aware Speech Enhancement</h2><p><strong>Authors:Luan Vinícius Fiorio, Alex Young, Ronald M. Aarts</strong></p>
<p>In this paper, we propose hybrid real- and complex-valued neural networks for speech enhancement. Real- or complex-valued models are either inefficient or present high complexity. We devise a straightforward design method for extending a real-valued network into its hybrid counterpart. Based on speech intelligibility and quality metrics, we compare the real, complex, and hybrid versions of a convolutional and a convolutional-recurrent architecture. The hybrid network consistently outperforms its counterparts with the same number of parameters. Additionally, the hybrid models’ complexity in terms of multiply-accumulate operations is substantially lower than that of their counterparts. </p>
<blockquote>
<p>在这篇论文中，我们提出了用于语音增强的混合实值和复值神经网络。实值或复值模型要么效率低下，要么复杂度较高。我们设计了一种将实值网络直接扩展为其混合对应网络的方法。基于语音清晰度和质量指标，我们比较了卷积和卷积递归架构的实值、复值和混合版本。混合网络在相同参数数量的情况下始终优于其他网络。此外，混合模型在乘积累加操作方面的复杂度远低于其他模型。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.21185v1">PDF</a> </p>
<p><strong>Summary</strong>：<br>本文提出混合实数和复数神经网络用于语音增强。文章介绍了将实值网络扩展为混合网络的设计方法，并通过语音清晰度和质量指标比较了实值网络、复数网络和混合网络的卷积和卷积递归架构。结果显示混合网络在参数数量相同的情况下性能更优秀，同时混合模型的乘积累加操作复杂度也较低。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>提出混合实数和复数神经网络用于语音增强。</li>
<li>介绍了一种将实值网络扩展为混合网络的设计方法。</li>
<li>对比了实值网络、复数网络和混合网络的卷积和卷积递归架构的语音增强效果。</li>
<li>混合网络在参数数量相同的情况下性能更优秀。</li>
<li>混合模型的复杂度低于实值和复数模型。</li>
<li>语音清晰度和质量指标是评估模型性能的重要标准。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.21185">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-6edc9e819d8a39f64cb15f094c84ad9a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036644&auth_key=1760036644-0-0-7c165e8105711a4a2ed309c95d088828&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f59f81a2217469af2d132f86761f5468~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036651&auth_key=1760036651-0-0-26276084713c2c7c0ec2dcd91d21403f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6980754c94a930cb39b94a739511e963~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036657&auth_key=1760036657-0-0-157d9dd07c5f7175bc569d8125ab50c2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0e99c8013576b06a5862aefea537679a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036664&auth_key=1760036664-0-0-bbb7f42f2f0f887d97025cf8b4bca347&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="i-LAVA-Insights-on-Low-Latency-Voice-2-Voice-Architecture-for-Agents"><a href="#i-LAVA-Insights-on-Low-Latency-Voice-2-Voice-Architecture-for-Agents" class="headerlink" title="i-LAVA: Insights on Low Latency Voice-2-Voice Architecture for Agents"></a>i-LAVA: Insights on Low Latency Voice-2-Voice Architecture for Agents</h2><p><strong>Authors:Anupam Purwar, Aditya Choudhary</strong></p>
<p>We experiment with a low-latency, end-to-end voice-to-voice communication model to optimize it for real-time conversational applications. By analyzing components essential to voice to voice (V-2-V) system viz. automatic speech recognition (ASR), text-to-speech (TTS), and dialog management, our work analyzes how to reduce processing time while maintaining high-quality interactions to identify the levers for optimizing V-2-V system. Our work identifies that TTS component which generates life-like voice, full of emotions including natural pauses and exclamations has highest impact on Real time factor (RTF). The experimented V-2-V architecture utilizes CSM1b has the capability to understand tone as well as context of conversation by ingesting both audio and text of prior exchanges to generate contextually accurate speech. We explored optimization of Residual Vector Quantization (RVQ) iterations by the TTS decoder which come at a cost of decrease in the quality of voice generated. Our experimental evaluations also demonstrate that for V-2-V implementations based on CSM most important optimizations can be brought by reducing the number of RVQ Iterations along with the codebooks used in Mimi. </p>
<blockquote>
<p>我们对一个低延迟、端到端的语音对语音通信模型进行了实验，以优化其适用于实时对话应用。通过分析语音对语音（V-2-V）系统的关键组件，即自动语音识别（ASR）、文本到语音（TTS）和对话管理，我们的工作分析了如何在保持高质量交互的同时减少处理时间，以确定优化V-2-V系统的关键因素。我们的工作发现，TTS组件产生逼真的语音，充满情感，包括自然停顿和感叹，对实时因子（RTF）的影响最大。经过实验的V-2-V架构采用CSM1b，它具有通过摄入先前对话的音频和文本来理解语调以及对话上下文的能力，以生成上下文准确的语音。我们探索了通过TTS解码器优化剩余矢量量化（RVQ）迭代，但这会导致生成的语音质量下降。我们的实验评估还表明，对于基于CSM的V-2-V实现，最重要的优化可以通过减少RVQ迭代次数以及Mimi中使用的代码簿来实现。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.20971v1">PDF</a> This paper analyzes a low-latency, end-to-end voice-to-voice (V-2-V)   architecture, identifying that the Text-to-Speech (TTS) component has the   highest impact on real-time performance. By reducing the number of Residual   Vector Quantization (RVQ) iterations in the TTS model, latency can be   effectively halved, creating a direct trade-off between conversational speed   and audio quality</p>
<p><strong>Summary</strong></p>
<p>该文本介绍了针对端到端的语音通信模型的实验研究，主要优化了语音到语音系统的核心组件以降低延迟并维护高质量交互。发现文本到语音组件对实时性因子影响最大，利用CSM的实验架构能够通过吸收先前对话的音频和文本来生成语境准确的语音。同时探索了通过减少RVQ迭代次数来优化TTS解码器的方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>该实验针对端到端的语音通信模型进行研究，主要优化实时对话应用的性能。</li>
<li>实验涉及的核心组件包括自动语音识别、文本到语音和对话管理。</li>
<li>发现文本到语音组件对保持实时对话的实时性因子影响最大。</li>
<li>利用CSM的实验架构能够生成具有情感和语境准确的语音，理解对话的语调与上下文。</li>
<li>通过减少RVQ迭代次数来优化TTS解码器，但可能会降低生成的语音质量。</li>
<li>实验评估表明，对于基于CSM的语音到语音实现，最重要的优化是通过减少RVQ迭代次数以及使用Mimi中的码本。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.20971">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-3bb81516e8015be4b3e8144c6c180994~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036671&auth_key=1760036671-0-0-48167e2e724a3de1fb98b36fb8665fab&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3cbb1be5f6f4abfe46e04380b855efe8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036678&auth_key=1760036678-0-0-076f3b62e2706728241956af98accf43&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-01288a991ef3f2796f87fa445020472c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036684&auth_key=1760036684-0-0-583f5141d25487bec2094108fd9abd91&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-73080709ee9ddccebf7778fd101a721c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036690&auth_key=1760036690-0-0-24753e3596a0d6970ba609ea67f73b8c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b57654b72585c80284a6f451fa33f172~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036696&auth_key=1760036696-0-0-43399b57ac8451ab0fedc57dd429445b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5b79dcef8256e74f28cbd8cbd4af3a5a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036703&auth_key=1760036703-0-0-5002ea821ebebd6fe34c38dcbef077ce&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e94936736293e3c4c16cb9c549985aa1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036709&auth_key=1760036709-0-0-b818ef5ed4ce2b68721a968fee97b33e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-04da26b77e97dedca53121eaf10c46f0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036716&auth_key=1760036716-0-0-ca3fb2e0fb95e17f9e3f5fb17a9ebc06&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f172e27dd6f3e845c2d74b84595f16d0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036722&auth_key=1760036722-0-0-33c2d672b14742e5e44082f4ae869b44&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="SingVERSE-A-Diverse-Real-World-Benchmark-for-Singing-Voice-Enhancement"><a href="#SingVERSE-A-Diverse-Real-World-Benchmark-for-Singing-Voice-Enhancement" class="headerlink" title="SingVERSE: A Diverse, Real-World Benchmark for Singing Voice Enhancement"></a>SingVERSE: A Diverse, Real-World Benchmark for Singing Voice Enhancement</h2><p><strong>Authors:Shaohan Jiang, Junan Zhang, Yunjia Zhang, Jing Yang, Fan Fan, Zhizheng Wu</strong></p>
<p>This paper presents a benchmark for singing voice enhancement. The development of singing voice enhancement is limited by the lack of realistic evaluation data. To address this gap, this paper introduces SingVERSE, the first real-world benchmark for singing voice enhancement, covering diverse acoustic scenarios and providing paired, studio-quality clean references. Leveraging SingVERSE, we conduct a comprehensive evaluation of state-of-the-art models and uncover a consistent trade-off between perceptual quality and intelligibility. Finally, we show that training on in-domain singing data substantially improves enhancement performance without degrading speech capabilities, establishing a simple yet effective path forward. This work offers the community a foundational benchmark together with critical insights to guide future advances in this underexplored domain. Demopage: <a target="_blank" rel="noopener" href="https://singverse.github.io/">https://singverse.github.io</a> </p>
<blockquote>
<p>本文提出了一个歌唱声音增强的基准测试。歌唱声音增强的开发受到缺乏现实评估数据的限制。为了解决这一空白，本文介绍了SingVERSE，这是歌唱声音增强的第一个真实世界基准测试，涵盖了多种声学场景，并提供了配对、工作室质量的清洁参考。利用SingVERSE，我们对最先进的模型进行了全面评估，并发现了感知质量与可理解性之间的持续权衡。最后，我们证明了在域内歌唱数据上进行训练可以显著提高增强性能，而不会降低语音功能，为简单有效的前进方向铺平了道路。这项工作为社区提供了一个基准测试，以及关键的见解，以指导未来在这个未被充分探索的领域取得进展。Demo页面：<a target="_blank" rel="noopener" href="https://singverse.github.io/">https://singverse.github.io</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.20969v1">PDF</a> Demopage: <a target="_blank" rel="noopener" href="https://singverse.github.io/">https://singverse.github.io</a>, Dataset:   <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/amphion/SingVERSE">https://huggingface.co/datasets/amphion/SingVERSE</a></p>
<p><strong>Summary</strong></p>
<p>本文介绍了一个歌唱声音增强的基准测试。由于缺乏真实的评估数据，歌唱声音增强的开发受到限制。本文引入了SingVERSE，这是第一个真实的歌唱声音增强基准测试，涵盖了各种声学场景，并提供了配对的、工作室质量的干净参考。利用SingVERSE，我们对最新技术模型进行了全面评估，发现了感知质量与可理解性之间的权衡。最后，我们表明在歌唱数据域内进行训练能显著提高增强性能，且不影响语音能力，为未来的研究提供了一个基础性的基准测试和关键见解。更多详情可见：<a target="_blank" rel="noopener" href="https://singverse.github.io/">https://singverse.github.io</a>。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>介绍了SingVERSE基准测试：首个针对歌唱声音增强的真实世界基准测试。</li>
<li>SingVERSE涵盖了多样的声学场景，并提供配对的、高质量干净参考。</li>
<li>通过对最新技术模型的评估，发现了感知质量与可理解性之间的权衡。</li>
<li>在歌唱数据域内训练能显著提高增强性能。</li>
<li>该训练策略不会影响语音能力。</li>
<li>SingVERSE为歌唱声音增强领域提供了基础性基准测试。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.20969">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-be0b39267f3449e6eb333647b93bbc2b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036730&auth_key=1760036730-0-0-f1d0e71e567bab86c7555cca59ff4e57&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-06b4ad0e5835d4327afe9b0d4b9f6f2c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036737&auth_key=1760036737-0-0-257ff2855b19bdb28d09bf636204ea11&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9c5cd9987b383186870180664ae75bb6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036744&auth_key=1760036744-0-0-5b9f90be800a66b411a4b4dc30209a4a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c95dde72c7558edff25739e73db3eff2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036751&auth_key=1760036751-0-0-02691fe04f167d6bd2a6ce54b61085de&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c36dad41d686c388e59e440e8d2db7b3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036758&auth_key=1760036758-0-0-41f2efa23682a3654070508ad9323e22&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="PAS-SE-Personalized-Auxiliary-Sensor-Speech-Enhancement-for-Voice-Pickup-in-Hearables"><a href="#PAS-SE-Personalized-Auxiliary-Sensor-Speech-Enhancement-for-Voice-Pickup-in-Hearables" class="headerlink" title="PAS-SE: Personalized Auxiliary-Sensor Speech Enhancement for Voice   Pickup in Hearables"></a>PAS-SE: Personalized Auxiliary-Sensor Speech Enhancement for Voice   Pickup in Hearables</h2><p><strong>Authors:Mattes Ohlenbusch, Mikolaj Kegler, Marko Stamenovic</strong></p>
<p>Speech enhancement for voice pickup in hearables aims to improve the user’s voice by suppressing noise and interfering talkers, while maintaining own-voice quality. For single-channel methods, it is particularly challenging to distinguish the target from interfering talkers without additional context. In this paper, we compare two strategies to resolve this ambiguity: personalized speech enhancement (PSE), which uses enrollment utterances to represent the target, and auxiliary-sensor speech enhancement (AS-SE), which uses in-ear microphones as additional input. We evaluate the strategies on two public datasets, employing different auxiliary sensor arrays, to investigate their cross-dataset generalization. We propose training-time augmentations to facilitate cross-dataset generalization of AS-SE systems. We also show that combining PSE and AS-SE (PAS-SE) provides complementary performance benefits, especially when enrollment speech is recorded with the in-ear microphone. We further demonstrate that PAS-SE personalized with noisy in-ear enrollments maintains performance benefits over the AS-SE system. </p>
<blockquote>
<p>针对可佩戴设备中的语音采集语音增强旨在通过抑制噪声和干扰说话者来提高用户语音质量，同时保持自己的语音质量。对于单通道方法而言，尤其是在没有额外上下文的情况下区分目标是说话者特别具有挑战性。在本文中，我们比较了两种解决此模糊性的策略：个性化语音增强（PSE），使用注册语音来代表目标语音；辅助传感器语音增强（AS-SE），使用入耳式麦克风作为额外输入。我们在两个公共数据集上评估了这两种策略，使用不同的辅助传感器阵列，以研究它们的跨数据集泛化能力。我们提出了训练时增强技术，以促进AS-SE系统的跨数据集泛化。我们还表明，将PSE和AS-SE（PAS-SE）相结合提供了互补的性能优势，尤其是当注册语音使用入耳式麦克风录制时。我们进一步证明，使用带有噪声入耳式注册的PAS-SE在性能上仍然优于AS-SE系统。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.20875v1">PDF</a> Submitted to ICASSP 2026</p>
<p><strong>Summary</strong></p>
<p>此文探讨了在可穿戴设备中进行语音增强的挑战和方法。针对单通道方法难以区分目标和干扰说话者的问题，比较了个性化语音增强（PSE）和使用入耳式麦克风作为额外输入的辅助传感器语音增强（AS-SE）两种策略。文章在公共数据集上评估了这两种策略，研究了它们的跨数据集泛化能力，并发现结合使用PSE和AS-SE可以提供额外的性能优势。此外，还介绍了增强系统泛化能力的训练时间增强技术。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>语音增强在可穿戴设备中用于提高用户声音质量，抑制噪声和干扰说话者。</li>
<li>单通道方法区分目标和干扰说话者具有挑战性。</li>
<li>个性化语音增强（PSE）使用注册语音代表目标说话者。</li>
<li>辅助传感器语音增强（AS-SE）使用入耳式麦克风作为额外输入。</li>
<li>在公共数据集上评估了PSE和AS-SE的跨数据集泛化能力。</li>
<li>结合PSE和AS-SE（PAS-SE）提供额外的性能优势，尤其在注册语音使用入耳式麦克风录制时。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.20875">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-03a7c9356a2c17b04de177ffc768e858~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036766&auth_key=1760036766-0-0-6e45dd3b0f5fc7c5ac2b38967a48eb07&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-28c4cda9e20e173f964ebad7b0293011~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036773&auth_key=1760036773-0-0-d1ee3f4379b0c0dcef6d0c847bcc1739&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2eba37620ce408587eab13d06157f5c0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036780&auth_key=1760036780-0-0-adddb87c764181f4b1ce334139419c6c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1fe30770e8fa642ea7214ba1b91297c2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036787&auth_key=1760036787-0-0-c7351a14880b124c6aefa391555a3953&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Variational-Low-Rank-Adaptation-for-Personalized-Impaired-Speech-Recognition"><a href="#Variational-Low-Rank-Adaptation-for-Personalized-Impaired-Speech-Recognition" class="headerlink" title="Variational Low-Rank Adaptation for Personalized Impaired Speech   Recognition"></a>Variational Low-Rank Adaptation for Personalized Impaired Speech   Recognition</h2><p><strong>Authors:Niclas Pokel, Pehuén Moure, Roman Boehringer, Shih-Chii Liu, Yingqiang Gao</strong></p>
<p>Speech impairments resulting from congenital disorders, such as cerebral palsy, down syndrome, or apert syndrome, as well as acquired brain injuries due to stroke, traumatic accidents, or tumors, present major challenges to automatic speech recognition (ASR) systems. Despite recent advancements, state-of-the-art ASR models like Whisper still struggle with non-normative speech due to limited training data availability and high acoustic variability. Moreover, collecting and annotating non-normative speech is burdensome: speaking is effortful for many affected individuals, while laborious annotation often requires caregivers familiar with the speaker. This work introduces a novel ASR personalization method based on Bayesian Low-rank Adaptation for data-efficient fine-tuning. We validate our method on the English UA-Speech dataset and a newly collected German speech dataset, BF-Sprache, from a child with structural speech impairment. The dataset and approach are designed to reflect the challenges of low-resource settings that include individuals with speech impairments. Our method significantly improves ASR accuracy for impaired speech while maintaining data and annotation efficiency, offering a practical path toward inclusive ASR. </p>
<blockquote>
<p>由于先天性疾病（如脑性瘫痪、唐氏综合征或 Apert 综合征）以及后天性脑损伤（如中风、事故或肿瘤）导致的言语障碍，给自动语音识别（ASR）系统带来了重大挑战。尽管最近取得了进展，但最先进的ASR模型（如Whisper）仍然难以处理非标准语音，这是由于训练数据有限和声音变化较大。此外，收集和注释非标准语音是一项艰巨的任务：对于许多受影响的人来说，说话是一项艰巨的任务，而繁琐的注释通常需要熟悉说话者的护理人员。这项工作介绍了一种基于贝叶斯低秩适应的新型ASR个性化方法，用于高效微调数据。我们在英文UA-Speech数据集和从一名有结构性言语障碍的儿童那里收集的全新德语语音数据集BF-Sprache上验证了我们的方法。数据集和方法旨在反映资源匮乏环境中的挑战，包括有言语障碍的个人。我们的方法在提高受损语音的ASR准确性方面取得了显著成效，同时保持了数据和注释的效率，为实现包容性ASR提供了一条实用途径。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.20397v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>针对先天性障碍（如脑性瘫痪、唐氏综合症或口裂综合症）以及后天性脑损伤（如中风、事故或肿瘤）导致的言语障碍，自动语音识别（ASR）系统面临重大挑战。尽管有最新技术进步，但最先进的ASR模型（如Whisper）仍因训练数据有限和声音变化大，难以识别非标准言语。此外，收集并标注非标准言语既困难又繁琐：对许多受影响的人来说说话已经很吃力，而繁琐的标注工作还需要熟悉发言者的护理人员参与。本文提出了一种新型的ASR个性化方法——基于贝叶斯低秩适应的少量数据精细调整法。我们在英语UA-Speech数据集和新收集的德语数据集BF-Sprache（来自一个有结构性言语障碍的儿童）上验证了该方法。数据集和方法反映了低资源设置中的挑战，包括患有言语障碍的人群。此方法显著提高了对受损语音的ASR准确性，同时保持了数据和标注的效率，为包容性ASR提供了实用途径。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>先天性障碍和后天性脑损伤导致的言语障碍对ASR系统构成挑战。</li>
<li>当前ASR模型在识别非标准言语方面存在困难，主要原因是训练数据有限和声音变化大。</li>
<li>收集并标注非标准言语数据既困难又繁琐，需要护理人员参与。</li>
<li>提出了一种基于贝叶斯低秩适应的ASR个性化方法，旨在实现少量数据的精细调整。</li>
<li>方法在英语和德语数据集上进行了验证，特别是针对患有结构性言语障碍的人群。</li>
<li>此方法提高了对受损语音的ASR准确性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.20397">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-ffd1bc012ffd333ccb307edacc5de8a3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036795&auth_key=1760036795-0-0-027c4b5be9f22637939b1a8b254e6a6e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-96ed4e3431d10206f15e17cb6f1da87a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036803&auth_key=1760036803-0-0-80fbf8b605dbb62955dcd3bad3e27390&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6f814ac8feed571da063c44f96d0d3b1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036810&auth_key=1760036810-0-0-21dbe17cd24a0af22d643085b45c043d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9eed20ddc3add5ca77544c1d84b8c4a3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036817&auth_key=1760036817-0-0-f2323de0de6a2438ffd188c70916eee6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Data-Efficient-ASR-Personalization-for-Non-Normative-Speech-Using-an-Uncertainty-Based-Phoneme-Difficulty-Score-for-Guided-Sampling"><a href="#Data-Efficient-ASR-Personalization-for-Non-Normative-Speech-Using-an-Uncertainty-Based-Phoneme-Difficulty-Score-for-Guided-Sampling" class="headerlink" title="Data-Efficient ASR Personalization for Non-Normative Speech Using an   Uncertainty-Based Phoneme Difficulty Score for Guided Sampling"></a>Data-Efficient ASR Personalization for Non-Normative Speech Using an   Uncertainty-Based Phoneme Difficulty Score for Guided Sampling</h2><p><strong>Authors:Niclas Pokel, Pehuén Moure, Roman Boehringer, Yingqiang Gao</strong></p>
<p>Automatic speech recognition (ASR) systems struggle with non-normative speech from individuals with impairments caused by conditions like cerebral palsy or structural anomalies. The high acoustic variability and scarcity of training data severely degrade model performance. This work introduces a data-efficient personalization method that quantifies phoneme-level uncertainty to guide fine-tuning. We leverage Monte Carlo Dropout to estimate which phonemes a model finds most difficult and use these estimates for a targeted oversampling strategy. We validate our method on English and German datasets. Crucially, we demonstrate that our model-derived uncertainty strongly correlates with phonemes identified as challenging in an expert clinical logopedic report, marking, to our knowledge, the first work to successfully align model uncertainty with expert assessment of speech difficulty. Our results show that this clinically-validated, uncertainty-guided sampling significantly improves ASR accuracy, delivering a practical framework for personalized and inclusive ASR. </p>
<blockquote>
<p>自动语音识别（ASR）系统在处理因脑性瘫痪或结构性异常等状况导致的个体非规范性语音时面临挑战。高音频变体和训练数据的稀缺严重降低了模型性能。这项工作引入了一种数据高效个性化方法，该方法量化音素级不确定性以指导微调。我们利用蒙特卡洛Dropout来估计模型认为哪些音素最困难，并使用这些估计值进行有针对性的过采样策略。我们在英语和德语数据集上验证了我们的方法。关键的是，我们证明我们的模型衍生出的不确定性与专家临床语音报告中识别的具有挑战性的音素密切相关，据我们所知，这是首次成功将模型不确定性与专家语音难度评估对齐的工作。我们的结果表明，这种经过临床验证的不确定性指导采样显著提高了ASR的准确性，为个性化和包容性的ASR提供了实用的框架。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.20396v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种数据高效的个性化方法，通过量化音素级不确定性来引导微调，以解决自动语音识别（ASR）系统对于非规范语音的识别难题。利用Monte Carlo Dropout估计模型难以识别的音素，并使用这些估计值进行有针对性的过采样策略。该方法在英语和德语数据集上得到验证，且模型衍生出的不确定性与专家临床语音困难评估结果高度一致。该方法显著提高了ASR的准确性，为个性化且包容性的ASR提供了实用框架。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ASR系统对于非规范语音存在挑战，特别是对于那些因脑性瘫痪或结构异常等条件导致语音障碍的个体。</li>
<li>高声学变率和训练数据稀缺严重降低了模型的性能。</li>
<li>引入了一种数据高效的个性化方法，通过量化音素级不确定性来引导微调。</li>
<li>利用Monte Carlo Dropout估计模型难以识别的音素，并采用针对性过采样策略。</li>
<li>该方法在英语和德语数据集上得到验证。</li>
<li>模型衍生出的不确定性与专家临床语音困难评估结果一致。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.20396">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-8ea7403b21b80656a78a93d0c922636b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036824&auth_key=1760036824-0-0-979d1c2120c11a4a02fed7490d018235&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2858997d8f77cf24a0e8f9b7314aacf2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036831&auth_key=1760036831-0-0-fd58af731aa497ca7bf0ab25ea33310b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-763c7e2134e7a5cd7ba9b9325825d570~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036838&auth_key=1760036838-0-0-b42b88de5cb2387e898ac2e80de8f45d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-dbe97cf43b0421367b1fde95eee95fb3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036845&auth_key=1760036845-0-0-31ae960e8c89d15028380512c7cefe0f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Discrete-Diffusion-for-Generative-Modeling-of-Text-Aligned-Speech-Tokens"><a href="#Discrete-Diffusion-for-Generative-Modeling-of-Text-Aligned-Speech-Tokens" class="headerlink" title="Discrete Diffusion for Generative Modeling of Text-Aligned Speech Tokens"></a>Discrete Diffusion for Generative Modeling of Text-Aligned Speech Tokens</h2><p><strong>Authors:Pin-Jui Ku, He Huang, Jean-Marie Lemercier, Subham Sekhar Sahoo, Zhehuai Chen, Ante Jukić</strong></p>
<p>This paper introduces a discrete diffusion model (DDM) framework for text-aligned speech tokenization and reconstruction. By replacing the auto-regressive speech decoder with a discrete diffusion counterpart, our model achieves significantly better reconstruction quality, stronger ASR performance, and faster inference. We provide a comprehensive analysis of applying DDMs to speech reconstruction, examining sampler choices, inference steps, and robustness to length-scale estimation errors. Furthermore, we improve the original TASTE by systematically comparing vector quantization modules, showing that FSQ yields up to a 35% relative WER reduction and +0.14 UT-MOS improvement over RVQ for AR models, while also enhancing DDM performance. Our model generates speech in just 10 denoising steps and even supports single-step generation with only minor quality degradation. </p>
<blockquote>
<p>本文介绍了一种用于文本对齐语音分词和重建的离散扩散模型（DDM）框架。通过用离散扩散对应物替换自回归语音解码器，我们的模型在重建质量、语音识别性能以及推理速度上实现了显著提升。我们对将DDMs应用于语音重建进行了全面的分析，研究了采样器选择、推理步骤以及对长度尺度估计错误的稳健性。此外，我们通过系统地比较向量量化模块改进了原始的TASTE，表明FSQ相对于RVQ在AR模型上实现了高达35%的相对WER降低和+0.14的UT-MOS提升，同时也提升了DDM的性能。我们的模型仅在10个去噪步骤中生成语音，甚至支持单步生成，只有轻微的质量下降。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.20060v1">PDF</a> 5 pages. submitted to ICASSP 2026</p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种基于离散扩散模型（DDM）框架的文本对齐语音切分与重建技术。通过用离散扩散解码器替代自回归语音解码器，该模型实现了更高的重建质量、更强的语音识别性能以及更快的推理速度。文章全面分析了将DDM应用于语音重建的方法，探讨了采样器选择、推理步骤以及对长度尺度估算错误的稳健性。此外，该研究还改进了原始模型TASTE，对比了向量量化模块，发现FSQ在AR模型上相对于RVQ实现了最高达35%的相对WER减少和+0.14的UT-MOS提升，同时也提升了DDM的性能。该模型仅需在10步降噪过程中生成语音，并支持单步生成，仅伴有轻微质量损失。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>离散扩散模型（DDM）框架用于文本对齐的语音切分与重建。</li>
<li>DDM实现了更高的重建质量、增强的语音识别性能和更快的推理速度。</li>
<li>采样器选择、推理步骤和对长度尺度估算错误的稳健性是DDM应用于语音重建的关键分析点。</li>
<li>研究改进了原始模型TASTE，通过对比向量量化模块，发现FSQ在AR模型上显著提升了性能。</li>
<li>FSQ相较于RVQ在AR模型上实现了WER和UT-MOS的显著改善。</li>
<li>DDM模型在生成语音时仅需10步降噪过程，并支持单步生成。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.20060">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-c99f8f3535d6b948dc26afd1eea0156a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036852&auth_key=1760036852-0-0-db7e89ec65b844a8d5291da495377027&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7885dd9bcb946bd455807a9cd594e289~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036859&auth_key=1760036859-0-0-ca59f96b0fe4218c64885220b25f7481&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-03f65ab4f83c127e715b632df8de5a3e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036866&auth_key=1760036866-0-0-db6caa7b67d0288dfe730a60a03605ed&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-711fc93289ce2ea7b8595529415b1497~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036873&auth_key=1760036873-0-0-299e6a693d8fc13254413e6d3fcf1de8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="SynchroRaMa-Lip-Synchronized-and-Emotion-Aware-Talking-Face-Generation-via-Multi-Modal-Emotion-Embedding"><a href="#SynchroRaMa-Lip-Synchronized-and-Emotion-Aware-Talking-Face-Generation-via-Multi-Modal-Emotion-Embedding" class="headerlink" title="SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation   via Multi-Modal Emotion Embedding"></a>SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation   via Multi-Modal Emotion Embedding</h2><p><strong>Authors:Phyo Thet Yee, Dimitrios Kollias, Sudeepta Mishra, Abhinav Dhall</strong></p>
<p>Audio-driven talking face generation has received growing interest, particularly for applications requiring expressive and natural human-avatar interaction. However, most existing emotion-aware methods rely on a single modality (either audio or image) for emotion embedding, limiting their ability to capture nuanced affective cues. Additionally, most methods condition on a single reference image, restricting the model’s ability to represent dynamic changes in actions or attributes across time. To address these issues, we introduce SynchroRaMa, a novel framework that integrates a multi-modal emotion embedding by combining emotional signals from text (via sentiment analysis) and audio (via speech-based emotion recognition and audio-derived valence-arousal features), enabling the generation of talking face videos with richer and more authentic emotional expressiveness and fidelity. To ensure natural head motion and accurate lip synchronization, SynchroRaMa includes an audio-to-motion (A2M) module that generates motion frames aligned with the input audio. Finally, SynchroRaMa incorporates scene descriptions generated by Large Language Model (LLM) as additional textual input, enabling it to capture dynamic actions and high-level semantic attributes. Conditioning the model on both visual and textual cues enhances temporal consistency and visual realism. Quantitative and qualitative experiments on benchmark datasets demonstrate that SynchroRaMa outperforms the state-of-the-art, achieving improvements in image quality, expression preservation, and motion realism. A user study further confirms that SynchroRaMa achieves higher subjective ratings than competing methods in overall naturalness, motion diversity, and video smoothness. Our project page is available at <a target="_blank" rel="noopener" href="https://novicemm.github.io/synchrorama">https://novicemm.github.io/synchrorama</a>. </p>
<blockquote>
<p>音频驱动的说话人脸生成技术已引起广泛关注，特别是在需要表达自然的人形交互应用中。然而，大多数现有的情感感知方法依赖于单一模态（音频或图像）进行情感嵌入，这限制了它们捕捉微妙情感线索的能力。此外，大多数方法都依赖于单张参考图像，这限制了模型在动作或属性随时间变化时的表示能力。为了解决这些问题，我们引入了SynchroRaMa这一新型框架，它通过结合文本（通过情感分析）和音频（通过基于语音的情感识别和音频衍生的效价唤起特征）的多模态情感嵌入，使生成具有更丰富、更真实情感表现力和保真度的说话人脸视频成为可能。为确保自然头部运动和准确的唇部同步，SynchroRaMa包含一个音频到运动（A2M）模块，用于生成与输入音频对齐的运动帧。最后，SynchroRaMa还结合了大型语言模型（LLM）生成的场景描述作为额外的文本输入，使其能够捕捉动态动作和高级语义属性。以视觉和文本线索训练模型，提高了时间一致性和视觉真实性。在基准数据集上的定量和定性实验表明，SynchroRaMa优于最新技术，在图像质量、表情保持和运动真实性方面取得了改进。用户研究进一步证实，在整体自然性、运动多样性和视频平滑度方面，SynchroRaMa相较于其他方法获得了更高的主观评分。我们的项目页面可在<a target="_blank" rel="noopener" href="https://novicemm.github.io/synchrorama">https://novicemm.github.io/synchrorama</a>找到。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.19965v1">PDF</a> Accepted at WACV 2026, project page :   <a target="_blank" rel="noopener" href="https://novicemm.github.io/synchrorama">https://novicemm.github.io/synchrorama</a></p>
<p><strong>Summary</strong><br>本文介绍了SynchroRaMa框架，该框架结合了文本和音频的多模态情感嵌入，用于生成具有更丰富和更真实情感表达力的动态人脸视频。通过情感文本分析、语音情感识别和音频衍生情感特征等技术，SynchroRaMa实现了更精细的情感捕捉。同时，它还包括音频到动作（A2M）模块，确保头部动作的自然性和唇部的精确同步。此外，SynchroRaMa还结合了大型语言模型生成的场景描述作为额外文本输入，捕捉动态动作和高层次语义属性。在基准数据集上的定量和定性实验表明，SynchroRaMa优于现有技术，在图像质量、表情保持和运动真实性方面有所改善。用户研究进一步证实，SynchroRaMa在整体自然性、运动多样性和视频流畅性方面获得更高的主观评分。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SynchroRaMa结合了文本和音频的多模态情感嵌入，增强了动态人脸视频的情感表达力。</li>
<li>通过情感文本分析、语音情感识别和音频衍生情感特征等技术，实现了更精细的情感捕捉。</li>
<li>SynchroRaMa包括音频到动作（A2M）模块，确保头部动作的自然性和唇部的精确同步。</li>
<li>结合大型语言模型生成的场景描述作为额外文本输入，捕捉动态动作和高层次语义属性。</li>
<li>SynchroRaMa优于现有技术，在图像质量、表情保持和运动真实性方面有所改善。</li>
<li>用户研究证实，SynchroRaMa在整体自然性、运动多样性和视频流畅性方面获得更高的主观评分。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.19965">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-203b13a4ecb75bdc634479b9c04c3023~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036881&auth_key=1760036881-0-0-2cb06a55c62f15ba3c5f1f683096ef5e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-78a059d9c3f124ac593d2a3826528bb6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036889&auth_key=1760036889-0-0-5e10e35ba99ce8422f13129b47d72061&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4fedce492adfc84675229b0fd7b45fcb~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036896&auth_key=1760036896-0-0-673ff04d796c9cf4f5641579acfeba08&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="WEST-LLM-based-Speech-Toolkit-for-Speech-Understanding-Generation-and-Interaction"><a href="#WEST-LLM-based-Speech-Toolkit-for-Speech-Understanding-Generation-and-Interaction" class="headerlink" title="WEST: LLM based Speech Toolkit for Speech Understanding, Generation, and   Interaction"></a>WEST: LLM based Speech Toolkit for Speech Understanding, Generation, and   Interaction</h2><p><strong>Authors:Binbin Zhang, Chengdong Liang, Shuai Wang, Xuelong Geng, Zhao Guo, Haoyu Li, Hao Yin, Xipeng Yang, Pengshen Zhang, Changwei Ma, Lei Xie</strong></p>
<p>In this paper, we present WEST(WE Speech Toolkit), a speech toolkit based on a large language model (LLM) for speech understanding, generation, and interaction. There are three key features of WEST: 1) Fully LLM-based: Standing on the shoulders of giants by reusing mature architectures, ecosystems (e.g., Hugging Face), and methods (e.g., sequence packing) from large models. 2) Full-stack: Supports tasks such as recognition, synthesis, understanding, dialogue, and multimodal capabilities, with extensibility to incorporate open-source models. 3) Simple and Stupid: A simple and stupid speech toolkit that everyone can Touch. In addition, WEST provides two types of recipes, models, and experimental results. The first is entirely based on open-source models and open-source data, allowing users to fully reproduce the experiments in this paper and serving as a verification system or minimal system baseline. The second is trained on massive data, offering superior performance so the user can directly apply it out of the box. WEST is publicly avilable at <a target="_blank" rel="noopener" href="https://github.com/wenet-e2e/west/">https://github.com/wenet-e2e/west/</a> </p>
<blockquote>
<p>本文介绍了WEST（WE语音工具包），这是一个基于大型语言模型（LLM）的语音工具包，用于语音理解、生成和交互。WEST有三个关键特点：1）完全基于LLM：利用大型模型的成熟架构、生态系统（例如Hugging Face）和方法（例如序列打包）站在巨人的肩膀上。2）全栈支持：支持识别、合成、理解、对话和多模式功能等任务，可扩展以纳入开源模型。3）简单易懂：一个简单明了的语音工具包，每个人都能轻松上手。此外，WEST提供两种类型的模型、食谱和实验结果。第一种完全基于开源模型和开源数据，允许用户充分重现本文中的实验，并作为验证系统或最小系统基线。第二种是在大量数据上训练的，提供卓越性能，用户可以直接开箱即用。WEST在<a target="_blank" rel="noopener" href="https://github.com/wenet-e2e/west/%E5%85%AC%E5%BC%80%E5%8F%AF%E7%94%A8%E3%80%82">https://github.com/wenet-e2e/west/公开可用。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.19902v1">PDF</a> </p>
<p><strong>Summary</strong><br>文本介绍了WEST（WE Speech Toolkit），一个基于大型语言模型（LLM）的语音识别、生成和交互工具包。WEST具有三大特点：完全基于LLM、全栈支持以及简单易用。此外，WEST提供两种类型的模型与实验结果，一种完全基于开源模型和开源数据，用于验证或作为最小系统基线；另一种经过大量数据训练，提供卓越性能，可直接应用。WEST已公开可用。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>WEST是一个基于大型语言模型的语音工具包，用于语音理解、生成和交互。</li>
<li>WEST具有三大特点：完全基于LLM、全栈支持和简单易用。</li>
<li>WEST提供两种类型的模型和实验结果，分别基于开源模型和大规模数据训练。</li>
<li>开源模型与数据允许用户完全重现实验，可作为验证系统或最小系统基线。</li>
<li>经过大规模数据训练的模型提供卓越性能，可直接应用。</li>
<li>WEST具有可扩展性，可融入开源模型。</li>
<li>WEST支持多种任务，如识别、合成、理解、对话和多模式功能。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.19902">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-ac8eb06d6dca11263e9319724df04f8f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036904&auth_key=1760036904-0-0-9a0869584afb6b4fc891dfcd36afea17&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a7aebdefc68a0afeb8f510e10a338f37~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036912&auth_key=1760036912-0-0-04bf79ad6ea94e0d45dd251ea4cc49c1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9b028003bfa661bbe008b2eeec6f9ddc~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036919&auth_key=1760036919-0-0-51fcf5c33db329097a31045d6fd9acde&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7014297b4d924d8489747d92b0aed1d0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036925&auth_key=1760036925-0-0-f9f56bda3dc152d687657c097b03ea74&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7857ceef61c98497d19470cfe0ceec6e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036932&auth_key=1760036932-0-0-ee1efe0720e05e6b219962962441986e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="MAGE-A-Coarse-to-Fine-Speech-Enhancer-with-Masked-Generative-Model"><a href="#MAGE-A-Coarse-to-Fine-Speech-Enhancer-with-Masked-Generative-Model" class="headerlink" title="MAGE: A Coarse-to-Fine Speech Enhancer with Masked Generative Model"></a>MAGE: A Coarse-to-Fine Speech Enhancer with Masked Generative Model</h2><p><strong>Authors:The Hieu Pham, Tan Dat Nguyen, Phuong Thanh Tran, Joon Son Chung, Duc Dung Nguyen</strong></p>
<p>Speech enhancement remains challenging due to the trade-off between efficiency and perceptual quality. In this paper, we introduce MAGE, a Masked Audio Generative Enhancer that advances generative speech enhancement through a compact and robust design. Unlike prior masked generative models with random masking, MAGE employs a scarcity-aware coarse-to-fine masking strategy that prioritizes frequent tokens in early steps and rare tokens in later refinements, improving efficiency and generalization. We also propose a lightweight corrector module that further stabilizes inference by detecting low-confidence predictions and re-masking them for refinement. Built on BigCodec and finetuned from Qwen2.5-0.5B, MAGE is reduced to 200M parameters through selective layer retention. Experiments on DNS Challenge and noisy LibriSpeech show that MAGE achieves state-of-the-art perceptual quality and significantly reduces word error rate for downstream recognition, outperforming larger baselines. Audio examples are available at <a target="_blank" rel="noopener" href="https://hieugiaosu.github.io/MAGE/">https://hieugiaosu.github.io/MAGE/</a>. </p>
<blockquote>
<p>语音增强因效率和感知质量之间的权衡而仍然具有挑战性。在本文中，我们介绍了MAGE，这是一种带有遮蔽音频生成增强器（Masked Audio Generative Enhancer）的技术，它通过紧凑而稳健的设计推动了生成式语音增强的发展。与先前采用随机遮蔽的遮蔽生成模型不同，MAGE采用了一种匮乏感知的由粗到细的遮蔽策略，该策略优先处理早期的频繁令牌，并在后期的改进中处理罕见的令牌，从而提高了效率和泛化能力。我们还提出了一种轻量级的校正模块，通过检测低置信度的预测并对其进行重新遮蔽以进行改进，从而进一步稳定了推断。MAGE建立在BigCodec之上，并以Qwen2.5-0.5B进行微调，通过选择性层保留减少到200M参数。在DNS Challenge和嘈杂的LibriSpeech上的实验表明，MAGE达到了最先进的感知质量，并显著降低了下游识别的词错误率，超越了较大的基准模型。音频示例可在<a target="_blank" rel="noopener" href="https://hieugiaosu.github.io/MAGE/%E6%89%BE%E5%88%B0%E3%80%82">https://hieugiaosu.github.io/MAGE/找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.19881v2">PDF</a> Submitted to ICASSP 2026</p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种名为MAGE的音频生成增强器，它采用紧凑且稳健的设计，推进了生成式语音增强的研究。不同于以往的随机掩模生成模型，MAGE采用了一种稀缺感知的粗到细掩模策略，提高了效率和泛化能力。此外，还提出了一种轻量级的校正模块，通过检测低置信度预测并进行重新掩模以进一步稳定推断。实验表明，MAGE在DNS挑战和嘈杂的LibriSpeech上实现了最佳感知质量和显著减少的词错误率，并在下游识别任务上优于大型基线模型。更多音频示例可通过<a target="_blank" rel="noopener" href="https://hieugiaosu.github.io/MAGE/%E8%AE%BF%E9%97%AE%E3%80%82">https://hieugiaosu.github.io/MAGE/访问。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MAGE是一种新型的音频生成增强器，具有紧凑和稳健的设计。</li>
<li>与其他随机掩模生成模型不同，MAGE采用了一种稀缺感知的粗到细掩模策略。</li>
<li>MAGE在效率和泛化能力上有所提升。</li>
<li>MAGE包含一个轻量级的校正模块，用于检测并重新掩模低置信度预测，从而稳定推断过程。</li>
<li>实验表明，MAGE在DNS挑战和LibriSpeech数据集上实现了最佳感知质量和词错误率降低。</li>
<li>MAGE使用BigCodec构建并经过Qwen2.5-0.5B微调，最后通过选择性层保留减少至200M参数。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.19881">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-98a93fe60ddc1904c2836931ba82bc8e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036939&auth_key=1760036939-0-0-38f761f45d80e7a79b66e77090c3bb92&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-131d56c9611e914993dfd908a87ec433~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036946&auth_key=1760036946-0-0-686a49d5c878e0926dc6a3b6d86930ab&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8f5bb8c702ed02c5e36b31afee6700a2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036953&auth_key=1760036953-0-0-2eb0893ad11ef7050ec73e25be5dbece&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ddccd4afe47297e40a2586489d206ac2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036960&auth_key=1760036960-0-0-8d9eb5f25c61795feba0d1cb1205ab26&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f8fbfcc93bca4350a50b77ae1a1718e2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036967&auth_key=1760036967-0-0-0f9c72ec80e06713401db732df7b6cca&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="MMedFD-A-Real-world-Healthcare-Benchmark-for-Multi-turn-Full-Duplex-Automatic-Speech-Recognition"><a href="#MMedFD-A-Real-world-Healthcare-Benchmark-for-Multi-turn-Full-Duplex-Automatic-Speech-Recognition" class="headerlink" title="MMedFD: A Real-world Healthcare Benchmark for Multi-turn Full-Duplex   Automatic Speech Recognition"></a>MMedFD: A Real-world Healthcare Benchmark for Multi-turn Full-Duplex   Automatic Speech Recognition</h2><p><strong>Authors:Hongzhao Chen, XiaoYang Wang, Jing Lan, Hexiao Ding, Yufeng Jiang MingHui Yang, DanHui Xu, Jun Luo, Nga-Chun Ng, Gerald W. Y. Cheng, Yunlin Mao, Jung Sun Yoo</strong></p>
<p>Automatic speech recognition (ASR) in clinical dialogue demands robustness to full-duplex interaction, speaker overlap, and low-latency constraints, yet open benchmarks remain scarce. We present MMedFD, the first real-world Chinese healthcare ASR corpus designed for multi-turn, full-duplex settings. Captured from a deployed AI assistant, the dataset comprises 5,805 annotated sessions with synchronized user and mixed-channel views, RTTM&#x2F;CTM timing, and role labels. We introduce a model-agnostic pipeline for streaming segmentation, speaker attribution, and dialogue memory, and fine-tune Whisper-small on role-concatenated audio for long-context recognition. ASR evaluation includes WER, CER, and HC-WER, which measures concept-level accuracy across healthcare settings. LLM-generated responses are assessed using rubric-based and pairwise protocols. MMedFD establishes a reproducible framework for benchmarking streaming ASR and end-to-end duplex agents in healthcare deployment. The dataset and related resources are publicly available at <a target="_blank" rel="noopener" href="https://github.com/Kinetics-JOJO/MMedFD">https://github.com/Kinetics-JOJO/MMedFD</a> </p>
<blockquote>
<p>自动语音识别（ASR）在临床对话中需要应对全双工交互、说话人重叠和低延迟约束的稳健性挑战，然而开放的标准测试集仍然稀缺。我们推出了MMedFD，这是首个为多轮、全双工环境设计的现实世界中文医疗ASR语料库。该数据集来自部署的AI助理的捕捉，包含5805个带注释的会话，具有同步的用户和混合通道视图、RTTM&#x2F;CTM定时和角色标签。我们介绍了一个模型无关的管道，用于流式分割、说话人归属和对话记忆，并对用于长上下文识别的角色连接音频进行微调Whisper-small。ASR评估包括WER、CER和HC-WER，后者衡量医疗保健环境中的概念级准确性。对于大型语言模型生成的响应，我们采用基于评分标准和成对协议进行评估。MMedFD为医疗部署中的流式ASR和端到端双工代理建立了可重复使用的基准测试框架。数据集和相关资源可在<a target="_blank" rel="noopener" href="https://github.com/Kinetics-JOJO/MMedFD%E5%85%AC%E5%BC%80%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/Kinetics-JOJO/MMedFD公开获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.19817v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>针对临床对话中的自动语音识别（ASR）系统需要应对全双工交互、说话人重叠和低延迟约束的挑战，但现有的开放基准测试仍然缺乏。我们推出了MMedFD，首个为多轮、全双工设置设计的现实世界中用于中文医疗ASR语料库。该数据集从部署的AI助理中提取，包含5805个已标注的会话，具有用户同步和混合通道视图、RTTM&#x2F;CTM定时和角色标签。我们引入了一个模型无关的管道，用于流式分割、说话人归属和对话记忆，并对角色连接的音频微调Whisper-small进行长上下文识别。ASR评估包括词错误率（WER）、字符错误率（CER）和医疗保健环境中的概念级准确度（HC-WER）。使用基于评分标准和配对协议的LLM生成响应进行评估。MMedFD为医疗部署中的流式ASR和端到端双工代理提供了可复制的基准测试框架。数据集和相关资源可在<a target="_blank" rel="noopener" href="https://github.com/Kinetics-JOJO/MMedFD%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/Kinetics-JOJO/MMedFD公开访问。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MMedFD是首个针对多轮、全双工设置的现实世界中用于中文医疗ASR语料库。</li>
<li>数据集捕获自部署的AI助理，包含丰富多样的医疗对话场景。</li>
<li>引入模型无关的管道，用于流式分割、说话人归属和对话记忆处理。</li>
<li>对ASR系统评估包括词错误率、字符错误率和概念级准确度。</li>
<li>采用LLM生成的响应进行评估，确保系统的实际应用效果。</li>
<li>MMedFD提供了可复制的基准测试框架，便于医疗部署中的流式ASR和端到端双工代理的评估。</li>
<li>数据集及相关资源已公开，便于研究和应用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.19817">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-4ea2142c900d0eb9ea5dbdd23d498fcf~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036975&auth_key=1760036975-0-0-8778e2b30abf8b48bed6eeb27735c8d2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-20dba94554bdb1529264b2f308c13822~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036982&auth_key=1760036982-0-0-b7a387d0bfc3d593c9ae53b6a2d2c80a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8fe181dc8bdcb96dd161c40758dd0810~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036988&auth_key=1760036988-0-0-fbf98fe0f5e2a07c2c09a789da2e612e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6c4c86d21ba6d0aa9de662164fa620ea~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036995&auth_key=1760036995-0-0-5e4b7836e477fcbd632d024bbf0625ee&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Selective-Classifier-free-Guidance-for-Zero-shot-Text-to-speech"><a href="#Selective-Classifier-free-Guidance-for-Zero-shot-Text-to-speech" class="headerlink" title="Selective Classifier-free Guidance for Zero-shot Text-to-speech"></a>Selective Classifier-free Guidance for Zero-shot Text-to-speech</h2><p><strong>Authors:John Zheng, Farhad Maleki</strong></p>
<p>In zero-shot text-to-speech, achieving a balance between fidelity to the target speaker and adherence to text content remains a challenge. While classifier-free guidance (CFG) strategies have shown promising results in image generation, their application to speech synthesis are underexplored. Separating the conditions used for CFG enables trade-offs between different desired characteristics in speech synthesis. In this paper, we evaluate the adaptability of CFG strategies originally developed for image generation to speech synthesis and extend separated-condition CFG approaches for this domain. Our results show that CFG strategies effective in image generation generally fail to improve speech synthesis. We also find that we can improve speaker similarity while limiting degradation of text adherence by applying standard CFG during early timesteps and switching to selective CFG only in later timesteps. Surprisingly, we observe that the effectiveness of a selective CFG strategy is highly text-representation dependent, as differences between the two languages of English and Mandarin can lead to different results even with the same model. </p>
<blockquote>
<p>在零样本文本到语音的任务中，如何在忠实于目标说话人和遵循文本内容之间取得平衡仍然是一个挑战。虽然无分类器引导（CFG）策略在图像生成中取得了有前景的结果，但它们在语音合成中的应用却被探索得很少。为CFG使用的条件分离能够实现语音合成中不同期望特征之间的权衡。在本文中，我们评估了原本为图像生成而开发的CFG策略的适应性，并将其扩展到语音合成领域。我们的结果表明，在图像生成中有效的CFG策略通常无法改善语音合成。我们还发现，通过在早期时间步长应用标准CFG，并在后期时间步长仅选择CFG，我们可以提高说话人相似性，同时限制文本贴合度的降低。令人惊讶的是，我们观察到选择性CFG策略的有效性高度依赖于文本表示，因为英语和普通话两种语言之间的差异即使在同一模型下也会导致不同的结果。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.19668v1">PDF</a> 5 pages, 7 figures, 1 table. Submitted to ICASSP 2026</p>
<p><strong>Summary</strong></p>
<p>文本探讨了零样本文本到语音转换中的挑战，特别是如何在保持目标说话人保真度的同时遵循文本内容的问题。虽然无分类器引导策略在图像生成中表现出有前景的结果，但其在语音合成中的应用仍被忽视。本研究评估了原本为图像生成设计的无分类器引导策略的适应性，并扩展了针对语音合成领域的分离条件无分类器引导方法。研究结果显示，在图像生成中有效的无分类器引导策略通常无法改善语音合成。同时发现，通过在早期时间步应用标准无分类器引导，并在后期时间步仅选择性地应用该策略，可以在提高说话人相似性的同时限制文本粘附性的降低。有趣的是，选择性无分类器引导策略的有效性高度依赖于文本表示，因为英语和普通话两种语言的差异会导致即使使用相同模型也会产生不同的结果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>零样本文本到语音转换中保持目标说话人保真度和遵循文本内容的平衡是一个挑战。</li>
<li>无分类器引导策略在图像生成中有前景，但在语音合成中的应用尚未得到充分探索。</li>
<li>分离条件的无分类器引导方法可以提高语音合成的性能。</li>
<li>在语音合成中，无分类器引导策略并不总是有效，需要适应性地应用。</li>
<li>通过在早期和后期时间步采用不同的无分类器引导策略，可以在提高说话人相似性的同时保持文本粘附性。</li>
<li>文本表示对无分类器引导策略的有效性有重要影响，不同语言可能导致不同结果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.19668">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-db440639895e76a3383500f1ff8bd717~resize:0:q75.jpg?source=1f5c5e47&expiration=1760037003&auth_key=1760037003-0-0-172358a844c229dd2fe07d30077f7ce4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3c3784fb77013aa46b67a57fe4783627~resize:0:q75.jpg?source=1f5c5e47&expiration=1760037010&auth_key=1760037010-0-0-26a37294a4e2a10dbbb160357d6a690a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-030fe4607b88fd01f6c516e7009cf426~resize:0:q75.jpg?source=1f5c5e47&expiration=1760037017&auth_key=1760037017-0-0-28672f61a254d4df2d2321f71a8a1822&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-cdebdcb8e5f0bd770dee9bac6cff5521~resize:0:q75.jpg?source=1f5c5e47&expiration=1760037024&auth_key=1760037024-0-0-3ff52d81ec51ec7b09be288e670e47ac&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d8a87fdb734f8ececb3e1aed910d47a0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760037031&auth_key=1760037031-0-0-7f3fa11c2d608af9465577ee0292f976&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Advancing-Speech-Summarization-in-Multi-modal-LLMs-with-Reinforcement-Learning"><a href="#Advancing-Speech-Summarization-in-Multi-modal-LLMs-with-Reinforcement-Learning" class="headerlink" title="Advancing Speech Summarization in Multi-modal LLMs with Reinforcement   Learning"></a>Advancing Speech Summarization in Multi-modal LLMs with Reinforcement   Learning</h2><p><strong>Authors:Shaoshi Ling, Gang Liu, Guoli Ye, Jinyu Li</strong></p>
<p>Speech summarization is a critical component of spoken content understanding, particularly in the era of rapidly growing spoken and audiovisual data. Recent advances in multi-modal large language models (MLLMs), leveraging the power of LLMs, enable generating textual summaries directly from speech without intermediate transcriptions, while supporting controllable styles and zero-shot generalization. However, open-source MLLMs continue to lag behind the state-of-the-art text-based LLMs, limiting their practical deployment for speech summarization. In this work, we present a novel multi-stage reinforcement learning training framework to enhance the speech summarization capabilities in MLLMs. Our model delivers substantial improvements over strong baselines, outperforms much larger MLLMs, and significantly narrows the gap with state-of-the-art text-based LLMs. </p>
<blockquote>
<p>语音识别摘要（Speech summarization）是口语内容理解的重要组成部分，特别是在口语和视听数据快速增长的时代。近期利用大型语言模型（LLMs）的多模态大型语言模型（MLLMs）的进步，能够直接从语音生成文本摘要，无需中间转录过程，同时支持可控风格和零样本泛化。然而，开源的MLLMs仍然落后于最新的基于文本的大型语言模型，限制了它们在语音摘要中的实际应用。在这项研究中，我们提出了一种新型的多阶段强化学习训练框架，旨在提高MLLMs的语音摘要能力。我们的模型在强大的基线模型上取得了实质性改进，优于更大的MLLMs，并显著缩小了与最新基于文本的LLMs的差距。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.19631v1">PDF</a> </p>
<p><strong>Summary</strong><br>语音摘要是对口语内容理解的核心组成部分，尤其在口语和视听数据迅速增长的时重要代。多模态大型语言模型（MLLMs）能够直接从语音生成文本摘要，支持可控风格和零样本泛化。然而，开源MLLMs相较于最先进的文本基础LLLM仍显落后。本研究提出一种新型的多阶段强化学习训练框架，以提高MLLMs的语音摘要能力，模型相较于强大基线有显著改善，超越规模更大的MLLMs，并显著缩小与文本基础LLLM的差距。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>语音摘要是口语内容理解的重要组成部分。</li>
<li>多模态大型语言模型（MLLMs）可以直接从语音生成文本摘要。</li>
<li>MLLMs在语音摘要方面相较于文本基础LLMs仍有差距。</li>
<li>本研究提出了一种多阶段强化学习训练框架来提高MLLMs的语音摘要能力。</li>
<li>模型表现超越了一些强大的基线以及规模更大的MLLMs。</li>
<li>该模型显著缩小了与文本基础LLLM的差距。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.19631">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-7a4cc08b02a8323fae7a589bc1f45a6a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760037040&auth_key=1760037040-0-0-52a4114b5fa4d8fa00f3b4301252fee3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-351afe21de61b9bead128d27a4ff0fd6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760037046&auth_key=1760037046-0-0-64f6e254c5823d6267a34cc03a362070&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-610adbed329312676e92f65345c03841~resize:0:q75.jpg?source=1f5c5e47&expiration=1760037053&auth_key=1760037053-0-0-b868ac0f929e3785e587ebef0889dfbc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-11aa0b9269a800addbdd05965b602d69~resize:0:q75.jpg?source=1f5c5e47&expiration=1760037059&auth_key=1760037059-0-0-20b3e5375b470f4b6bdf2efed0459090&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ecedfeb902e45350d4932b95cb805d5a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760037066&auth_key=1760037066-0-0-f723010e3eb1f899138f4aa925235791&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="HD-PPT-Hierarchical-Decoding-of-Content-and-Prompt-Preference-Tokens-for-Instruction-based-TTS"><a href="#HD-PPT-Hierarchical-Decoding-of-Content-and-Prompt-Preference-Tokens-for-Instruction-based-TTS" class="headerlink" title="HD-PPT: Hierarchical Decoding of Content- and Prompt-Preference Tokens   for Instruction-based TTS"></a>HD-PPT: Hierarchical Decoding of Content- and Prompt-Preference Tokens   for Instruction-based TTS</h2><p><strong>Authors:Sihang Nie, Xiaofen Xing, Jingyuan Xing, Baiji Liu, Xiangmin Xu</strong></p>
<p>Large Language Model (LLM)-based Text-to-Speech (TTS) models have already reached a high degree of naturalness. However, the precision control of TTS inference is still challenging. Although instruction-based Text-to-Speech (Instruct-TTS) models are proposed, these models still lack fine-grained control due to the modality gap between single-level text instructions and multilevel speech tokens. To address this limitation, we propose HD-PPT, a framework that transforms speech synthesis into a structured, hierarchical task. To enable fine-grained control, we introduce a novel speech codec to extract distinct prompt-preference and content-preference tokens from the complex speech tokens, supervised by automatic speech recognition (ASR) and cross-lingual audio-text pre-training (CLAP) objectives. To bridge the modality gap of these tokens, we propose a hierarchical decoding strategy, where the LLM generates tokens in a structured order: first semantic, then fine-grained style, and finally complete acoustic representation. Extensive experiments demonstrate that this hierarchical paradigm significantly improves instruction adherence and achieves state-of-the-art naturalness, validating our approach for precise and controllable speech synthesis. Audio samples are available at <a target="_blank" rel="noopener" href="https://xxh333.github.io/">https://xxh333.github.io/</a>. </p>
<blockquote>
<p>基于大语言模型（LLM）的文本到语音（TTS）模型已经达到了很高的自然度。然而，TTS推理的精确控制仍然具有挑战性。虽然提出了基于指令的文本到语音（Instruct-TTS）模型，但这些模型仍然缺乏精细控制，原因是单一层次的文本指令和多层次语音令牌之间的模态差距。为了解决这一限制，我们提出了HD-PPT框架，将语音合成转化为一个结构化、分层的任务。为了实现精细控制，我们引入了一种新的语音编解码器，从复杂的语音令牌中提取出不同的提示偏好和内容偏好令牌，由自动语音识别（ASR）和跨语言音频文本预训练（CLAP）目标进行监督。为了弥合这些令牌的模态差距，我们提出了一种分层解码策略，LLM以结构化顺序生成令牌：首先是语义，然后是精细风格，最后是完整的声学表示。大量实验表明，这种分层范式显著提高了指令遵循能力，并实现了最先进的自然度，验证了我们方法在精确可控语音合成方面的有效性。音频样本可在<a target="_blank" rel="noopener" href="https://xxh333.github.io/">链接</a>找到。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.19001v1">PDF</a> 5 pages, 2 figures, submitted to ICASSP2026</p>
<p><strong>Summary</strong></p>
<p>基于大语言模型（LLM）的文本到语音（TTS）模型已具备高度自然度，但TTS推理的精准控制仍具挑战。为缩小指令与多级语音标记间的模态差距，本文提出HD-PPT框架，将语音合成转化为结构化、层次化任务。通过引入新型语音编解码器，从复杂语音标记中提取不同的提示偏好和内容偏好标记，并借助自动语音识别（ASR）和跨语言音频文本预训练（CLAP）目标进行监督。为弥合这些标记的模态差距，本文采取层次解码策略，让LLM按结构化顺序生成标记：首先是语义，然后是精细风格，最后是完整的声音表达。实验证明，这种层次化范式显著提高了指令遵循度，并实现了业界领先的自然度，验证了我们在精准可控语音合成方面的方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM-based TTS模型已具备高自然度，但推理控制仍存在挑战。</li>
<li>指令式TTS模型因模态差距限制了精细控制。</li>
<li>HD-PPT框架将语音合成转化为结构化、层次化任务来解决模态差距问题。</li>
<li>引入新型语音编解码器，从复杂语音标记中提取不同的提示和内容偏好标记。</li>
<li>通过ASR和CLAP目标进行监督以改善模型性能。</li>
<li>采取层次解码策略，按语义、风格和声音表达的顺序生成标记。</li>
<li>实验证明该层次化范式提高指令遵循度并达到业界领先自然度。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.19001">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-b09dcd92ad76464ed6cbe7e055a9a964~resize:0:q75.jpg?source=1f5c5e47&expiration=1760037073&auth_key=1760037073-0-0-030cbb3dbbc864b0330ec9e3e2bde664&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1976cdf2828329169cb27f2182435fd8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760037081&auth_key=1760037081-0-0-a99b9fe84026c0d790e10e1fc7d5b34a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1d39c7b1dcc23baa6baf55c0cea3c6ac~resize:0:q75.jpg?source=1f5c5e47&expiration=1760037088&auth_key=1760037088-0-0-de584115aacff828d5ce588d1203297a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Generalizability-of-Predictive-and-Generative-Speech-Enhancement-Models-to-Pathological-Speakers"><a href="#Generalizability-of-Predictive-and-Generative-Speech-Enhancement-Models-to-Pathological-Speakers" class="headerlink" title="Generalizability of Predictive and Generative Speech Enhancement Models   to Pathological Speakers"></a>Generalizability of Predictive and Generative Speech Enhancement Models   to Pathological Speakers</h2><p><strong>Authors:Mingchi Hou, Ante Jukic, Ina Kodrasi</strong></p>
<p>State of the art speech enhancement (SE) models achieve strong performance on neurotypical speech, but their effectiveness is substantially reduced for pathological speech. In this paper, we investigate strategies to address this gap for both predictive and generative SE models, including i) training models from scratch using pathological data, ii) finetuning models pretrained on neurotypical speech with additional data from pathological speakers, and iii) speaker specific personalization using only data from the individual pathological test speaker. Our results show that, despite the limited size of pathological speech datasets, SE models can be successfully trained or finetuned on such data. Finetuning models with data from several pathological speakers yields the largest performance improvements, while speaker specific personalization is less effective, likely due to the small amount of data available per speaker. These findings highlight the challenges and potential strategies for improving SE performance for pathological speakers. </p>
<blockquote>
<p>前沿的语音增强（SE）模型在神经典型语音上表现强劲，但其在病理性语音上的效果却大大降低。在本文中，我们针对预测性和生成性SE模型，探讨了解决这一差距的策略，包括一）使用病理性数据从头开始训练模型，二）使用来自病理性说话者的附加数据对神经典型语音上预训练的模型进行微调，以及三）仅使用来自个别病理性测试说话者的数据进行特定说话者个性化。我们的结果表明，尽管病理性语音数据集的大小有限，但SE模型可以在这些数据上进行成功训练或微调。使用来自多个病理性说话者的数据微调模型会带来最大的性能提升，而特定说话者个性化则效果较小，这可能是由于每个说话者的数据量较小所致。这些发现突出了改善病理性说话者SE性能的挑战和潜在策略。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.18890v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>这篇论文针对当前先进的语音增强模型在处理病理性语音时的性能下降问题进行了深入研究。文章探讨了针对预测型和生成型语音增强模型的改进策略，包括使用病理性数据从头开始训练模型、使用神经典型语音预训练模型进行微调以及仅使用个体病理性测试说话者的数据进行个性化设置。研究结果表明，尽管病理性语音数据集规模有限，但语音增强模型仍可在这些数据上进行成功训练或微调。使用多个病理性说话者的数据进行微调能带来最大的性能提升，而针对个别说话者的个性化设置效果较小，这可能是由于每个说话者的数据量较小。这些发现突出了提高病理性说话者语音增强性能的挑战和潜在策略。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>当前先进的语音增强模型在处理病理性语音时性能下降。</li>
<li>论文探讨了针对预测型和生成型语音增强模型的改进策略，包括使用病理性数据训练新模型、微调预训练模型和个性化设置。</li>
<li>使用多个病理性说话者的数据进行微调能显著提高语音增强模型的性能。</li>
<li>相对于大规模数据，个性化设置因数据量较小而效果有限。</li>
<li>挑战在于病理性语音数据集规模有限。</li>
<li>研究结果强调了提高针对病理性说话者的语音增强性能的重要性和潜在策略。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.18890">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-30c03850615e5408394dd959cf5d76dd~resize:0:q75.jpg?source=1f5c5e47&expiration=1760037096&auth_key=1760037096-0-0-1ac6fa131e02e638c58c163505827354&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4adda567f5f571dfe11946be5dcfec3f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760037103&auth_key=1760037103-0-0-08d8665c0e359c27362ffb39427f7006&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-36e4bef4407c9917581fc28a818c3ed9~resize:0:q75.jpg?source=1f5c5e47&expiration=1760037109&auth_key=1760037109-0-0-d3f14c4fbf82f3251f80231b9f9e4976&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e22c98778409a7c4f24f861b80720964~resize:0:q75.jpg?source=1f5c5e47&expiration=1760037116&auth_key=1760037116-0-0-29aa7f1cf542aafa900b8792b1a3beb6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Influence-of-Clean-Speech-Characteristics-on-Speech-Enhancement-Performance"><a href="#Influence-of-Clean-Speech-Characteristics-on-Speech-Enhancement-Performance" class="headerlink" title="Influence of Clean Speech Characteristics on Speech Enhancement   Performance"></a>Influence of Clean Speech Characteristics on Speech Enhancement   Performance</h2><p><strong>Authors:Mingchi Hou, Ina Kodrasi</strong></p>
<p>Speech enhancement (SE) performance is known to depend on noise characteristics and signal to noise ratio (SNR), yet intrinsic properties of the clean speech signal itself remain an underexplored factor. In this work, we systematically analyze how clean speech characteristics influence enhancement difficulty across multiple state of the art SE models, languages, and noise conditions. We extract a set of pitch, formant, loudness, and spectral flux features from clean speech and compute correlations with objective SE metrics, including frequency weighted segmental SNR and PESQ. Our results show that formant amplitudes are consistently predictive of SE performance, with higher and more stable formants leading to larger enhancement gains. We further demonstrate that performance varies substantially even within a single speaker’s utterances, highlighting the importance of intraspeaker acoustic variability. These findings provide new insights into SE challenges, suggesting that intrinsic speech characteristics should be considered when designing datasets, evaluation protocols, and enhancement models. </p>
<blockquote>
<p>语音增强的性能已知依赖于噪声特性和信噪比（SNR），但清洁语音信号本身的固有属性仍然是一个被忽略的因素。在这项工作中，我们系统地分析了清洁语音特征如何影响多个前沿语音增强模型、语言和噪声条件下的增强难度。我们从清洁语音中提取了音高、共振峰、响度和谱流特征，并计算了与客观语音增强指标的相关性，包括频率加权分段SNR和PESQ。我们的结果表明，共振峰幅度始终与语音增强性能相关，较高的共振峰幅度和更稳定的共振峰会带来更大的增强增益。我们进一步证明，即使在单个说话人的话语中，性能也有很大差异，这凸显了说话人内部声学变化的重要性。这些发现对语音增强挑战提供了新的见解，建议在设计数据集、评估协议和增强模型时考虑内在语音特征。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.18885v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文研究了干净语音特性对多种先进的语音增强模型性能的影响，涉及不同语言和噪声条件。通过提取干净语音的音调、共振峰、响度和频谱波动特征，与客观语音增强指标（如频率加权分段信噪比和PESQ）进行相关性分析。结果显示，共振峰幅度能预测语音增强性能，具有较高且稳定的共振峰能带来更大的增强效果。此外，即使在单个说话人的发音内，性能也存在显著变化，凸显出说话人内部声音变化的重要性。这些发现为设计数据集、评估协议和增强模型时考虑内在语音特性提供了新的见解。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>干净语音特性对语音增强模型性能的影响被系统性地分析。</li>
<li>研究涉及多种先进的语音增强模型、语言和噪声条件。</li>
<li>通过提取干净语音的特征（如音调、共振峰、响度和频谱波动）与客观语音增强指标进行相关性分析。</li>
<li>共振峰幅度能预测语音增强性能。</li>
<li>具有较高且稳定的共振峰能带来更大的语音增强效果。</li>
<li>说话人内部的声音变化对语音增强性能有显著影响。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.18885">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-6cc25e2be199a101258e332816cf7069~resize:0:q75.jpg?source=1f5c5e47&expiration=1760037123&auth_key=1760037123-0-0-4a9462842501c7e4512e21d1896f35db&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-dd66c4a3a22fb7b379518efe9a341a52~resize:0:q75.jpg?source=1f5c5e47&expiration=1760037131&auth_key=1760037131-0-0-f9a2a35083d5e7a2688671bcb653323d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e36ad594a20d13288c3f9376d668c55d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760037137&auth_key=1760037137-0-0-73d730ed294445bbe70b43dc6346ae99&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-08583bcdb6358c120ad8104c77af7528~resize:0:q75.jpg?source=1f5c5e47&expiration=1760037144&auth_key=1760037144-0-0-c3edec6b1af68681501b77b94f72c7c6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a5524c27c27cbddfcfa529e5de981263~resize:0:q75.jpg?source=1f5c5e47&expiration=1760037151&auth_key=1760037151-0-0-8c134dced2bfef95d1e48c32aacfa9b7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Towards-Evaluating-Generative-Audio-Insights-from-Neural-Audio-Codec-Embedding-Distances"><a href="#Towards-Evaluating-Generative-Audio-Insights-from-Neural-Audio-Codec-Embedding-Distances" class="headerlink" title="Towards Evaluating Generative Audio: Insights from Neural Audio Codec   Embedding Distances"></a>Towards Evaluating Generative Audio: Insights from Neural Audio Codec   Embedding Distances</h2><p><strong>Authors:Arijit Biswas, Lars Villemoes</strong></p>
<p>Neural audio codecs (NACs) achieve low-bitrate compression by learning compact audio representations, which can also serve as features for perceptual quality evaluation. We introduce DACe, an enhanced, higher-fidelity version of the Descript Audio Codec (DAC), trained on diverse real and synthetic tonal data with balanced sampling. We systematically compare Fr&#39;echet Audio Distance (FAD) and Maximum Mean Discrepancy (MMD) on MUSHRA tests across speech, music, and mixed content. FAD consistently outperforms MMD, and embeddings from higher-fidelity NACs (such as DACe) show stronger correlations with human judgments. While CLAP LAION Music (CLAP-M) and OpenL3 Mel128 (OpenL3-128M) embeddings achieve higher correlations, NAC embeddings provide a practical zero-shot approach to audio quality assessment, requiring only unencoded audio for training. These results demonstrate the dual utility of NACs for compression and perceptually informed audio evaluation. </p>
<blockquote>
<p>神经音频编解码器（NACs）通过学习紧凑的音频表示来实现低比特率压缩，这些表示也可以作为感知质量评估的特征。我们介绍了DACe，这是增强型高保真版本的描述音频编解码器（DAC），在均衡采样的多样真实和合成音调数据上进行训练。我们在语音、音乐和混合内容上的MUSHRA测试中对Fr’echet音频距离（FAD）和最大均值差异（MMD）进行了系统比较。FAD持续优于MMD，并且来自高保真NAC（如DACe）的嵌入与人的判断表现出更强的相关性。虽然CLAP LAION音乐（CLAP-M）和OpenL3 Mel128（OpenL3-128M）嵌入实现了更高的相关性，但NAC嵌入提供了一种实用的零样本音频质量评估方法，仅需要未编码的音频进行训练。这些结果证明了NACs在压缩和感知驱动的音频评估中的双重实用性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.18823v1">PDF</a> Pre-review version submitted to ICASSP 2026</p>
<p><strong>Summary</strong></p>
<p>神经网络音频编码（NAC）通过学习紧凑的音频表示来实现低比特率压缩，这种表示也可以作为感知质量评估的特征。本文介绍了DACe，它是描述音频编码（DAC）的一个增强版，能够在多样化和平衡的真实和合成音调数据上进行训练。通过系统地比较Fréchet音频距离（FAD）和最大均值差异（MMD）在语音、音乐和混合内容上的MUSHRA测试，发现FAD始终优于MMD。高保真NAC（如DACe）的嵌入与人类判断表现出更强的相关性。虽然CLAP LAION音乐（CLAP-M）和OpenL3 Mel128（OpenL3-128M）嵌入具有更高的相关性，但NAC嵌入为音频质量评估提供了一个实用的无样本方法，只需要未编码的音频即可进行训练。这些结果证明了NACs在压缩和感知驱动的音频评估中的双重效用。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>神经网络音频编码（NAC）结合了音频压缩和感知质量评估功能。</li>
<li>DACe作为描述音频编码（DAC）的增强版本，训练时涉及多样化且平衡的真实和合成音调数据。</li>
<li>Fréchet音频距离（FAD）在语音、音乐和混合内容的MUSHRA测试中表现优于最大均值差异（MMD）。</li>
<li>高保真NAC的嵌入与人类对音频质量的判断高度相关。</li>
<li>CLAP LAION音乐（CLAP-M）和OpenL3 Mel128嵌入虽具有较高相关性，但NAC嵌入提供了一种零样本音频质量评估方法。</li>
<li>NACs既可用于音频压缩，也可用于感知驱动的音频评估。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.18823">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-766cc2acbaf3471dc3b8ac340e62324a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760037159&auth_key=1760037159-0-0-e9a02ddd79f773a47e3621083b06e967&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f0073df87189c88abb224358d187c5a3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760037166&auth_key=1760037166-0-0-d14fdcf185b8fac864e822b1c02d5d4f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e92c92ec8f62b3e57f44df1194bf0c6d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760037173&auth_key=1760037173-0-0-c00f61018efca69d0113d99c45feb98d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9d2ade63688ba6f0a68a48e2103253c7~resize:0:q75.jpg?source=1f5c5e47&expiration=1760037179&auth_key=1760037179-0-0-823db4b538356de2c8d9929572d00795&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Group-Relative-Policy-Optimization-for-Text-to-Speech-with-Large-Language-Models"><a href="#Group-Relative-Policy-Optimization-for-Text-to-Speech-with-Large-Language-Models" class="headerlink" title="Group Relative Policy Optimization for Text-to-Speech with Large   Language Models"></a>Group Relative Policy Optimization for Text-to-Speech with Large   Language Models</h2><p><strong>Authors:Chang Liu, Ya-Jun Hu, Ying-Ying Gao, Shi-Lei Zhang, Zhen-Hua Ling</strong></p>
<p>This paper proposes a GRPO-based approach to enhance the performance of large language model (LLM)-based text-to-speech (TTS) models by deriving rewards from an off-the-shelf automatic speech recognition (ASR) model. Compared to previous reinforcement learning methods for LLM-based TTS, our method requires no dedicated model for reward computation or training. Moreover, we design a composite reward function that combines character error rate (CER) with negative log-likelihood (NLL) obtained from the ASR model, providing more informative and accurate reward signals. We apply GRPO fine-tuning to pre-trained LLM-based TTS models and evaluate their zero-shot TTS performance. Experimental results show that the proposed method substantially improves both the intelligibility and naturalness of synthesized speech. Ablation studies and further analyses confirm the effectiveness of integrating the two reward components. </p>
<blockquote>
<p>本文提出了一种基于GRPO的方法，通过利用现成的自动语音识别（ASR）模型得到的奖励来提高基于大型语言模型（LLM）的文本到语音（TTS）模型的性能。与以前用于LLM-based TTS的强化学习方法相比，我们的方法不需要用于计算奖励或训练的专用模型。此外，我们设计了一个组合奖励函数，将字符错误率（CER）与从ASR模型获得的负对数似然值（NLL）相结合，以提供更准确且信息丰富的奖励信号。我们将GRPO微调应用于基于预训练LLM的TTS模型，并评估其零样本TTS性能。实验结果表明，该方法显著提高了合成语音的可理解性和自然度。消融研究和进一步的分析证实了整合两种奖励成分的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.18798v1">PDF</a> 5 pages,submitted to ICASSP2026</p>
<p><strong>摘要</strong></p>
<p>本论文提出了一种基于GRPO的方法，通过利用现成的语音识别（ASR）模型来计算奖励，以提高大语言模型（LLM）为基础的文本到语音（TTS）模型的性能。与之前用于LLM-based TTS的强化学习方法相比，我们的方法无需专门的奖励计算或训练模型。此外，我们设计了一个组合奖励函数，将字符错误率（CER）与从ASR模型获得的负对数似然值（NLL）相结合，以提供更准确和更丰富的奖励信号。我们对预训练的LLM-based TTS模型应用GRPO微调技术，并评估其零样本TTS性能。实验结果表明，该方法可显著提高合成语音的清晰度和自然度。消融研究和进一步的分析证实了整合这两种奖励成分的有效性。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>提出了一种基于GRPO的方法，利用现成的语音识别（ASR）模型来提高文本到语音（TTS）模型的性能。</li>
<li>无需专门的奖励计算或训练模型，简化了强化学习过程。</li>
<li>设计了一个组合奖励函数，结合了字符错误率（CER）和负对数似然值（NLL），以提供更准确的奖励信号。</li>
<li>通过GRPO微调技术应用于预训练的TTS模型，提升了零样本TTS性能。</li>
<li>实验结果表明，该方法能提高合成语音的清晰度和自然度。</li>
<li>消融研究证实了整合奖励成分的有效性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.18798">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-093805a931faa0832676b7211f46d198~resize:0:q75.jpg?source=1f5c5e47&expiration=1760037186&auth_key=1760037186-0-0-3be925da539a3d91b66bf62a8ad935fc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f94d5c567ee4d88fcdc9f1c42cc93c9f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760037194&auth_key=1760037194-0-0-2a87a6ca8c117296a4e4b93651dab99b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e23b95872de562eb4ac79f11135f894b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760037200&auth_key=1760037200-0-0-5268fbc50764de82588ed12e67651e8b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e0954217ec6dc08da5252ee5e0f037c0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760037208&auth_key=1760037208-0-0-d325dfd3f5783d2c0ec240baff6dcd7f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="M4SER-Multimodal-Multirepresentation-Multitask-and-Multistrategy-Learning-for-Speech-Emotion-Recognition"><a href="#M4SER-Multimodal-Multirepresentation-Multitask-and-Multistrategy-Learning-for-Speech-Emotion-Recognition" class="headerlink" title="M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy   Learning for Speech Emotion Recognition"></a>M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy   Learning for Speech Emotion Recognition</h2><p><strong>Authors:Jiajun He, Xiaohan Shi, Cheng-Hung Hu, Jinyi Mi, Xingfeng Li, Tomoki Toda</strong></p>
<p>Multimodal speech emotion recognition (SER) has emerged as pivotal for improving human-machine interaction. Researchers are increasingly leveraging both speech and textual information obtained through automatic speech recognition (ASR) to comprehensively recognize emotional states from speakers. Although this approach reduces reliance on human-annotated text data, ASR errors possibly degrade emotion recognition performance. To address this challenge, in our previous work, we introduced two auxiliary tasks, namely, ASR error detection and ASR error correction, and we proposed a novel multimodal fusion (MF) method for learning modality-specific and modality-invariant representations across different modalities. Building on this foundation, in this paper, we introduce two additional training strategies. First, we propose an adversarial network to enhance the diversity of modality-specific representations. Second, we introduce a label-based contrastive learning strategy to better capture emotional features. We refer to our proposed method as M4SER and validate its superiority over state-of-the-art methods through extensive experiments using IEMOCAP and MELD datasets. </p>
<blockquote>
<p>多模态语音情感识别（SER）对于改善人机交互至关重要。研究人员越来越多地利用通过自动语音识别（ASR）获得的语音和文本信息，以全面识别说话人的情感状态。虽然这种方法减少了对人类注释文本数据的依赖，但ASR错误可能会降低情感识别的性能。为了应对这一挑战，在我们之前的工作中，我们引入了两个辅助任务，即ASR错误检测和ASR错误校正，并提出了一种新的多模态融合（MF）方法，用于学习不同模态的模态特定和模态不变表示。在此基础上，本文介绍了两种额外的训练策略。首先，我们提出了一种对抗网络，以增强模态特定表示的多样性。其次，我们引入了一种基于标签的对比学习策略，以更好地捕捉情感特征。我们将所提出的方法称为M4SER，并通过在IEMOCAP和MELD数据集上进行的大量实验验证了其优于最新技术方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.18706v1">PDF</a> Accepted by IEEE Transactions on Audio, Speech and Language   Processing</p>
<p><strong>Summary</strong></p>
<p>本文介绍了多模态语音情感识别（SER）在改善人机交互中的重要作用。研究人员越来越多地利用语音和通过自动语音识别（ASR）获得的文本信息，以全面识别说话者的情感状态。针对ASR误差可能降低情感识别性能的问题，本文引入两种额外的训练策略：一是采用对抗网络增强模态特定表示的多样性；二是引入基于标签的对比学习策略以更好地捕捉情感特征。所提出的M4SER方法通过IEMOCAP和MELD数据集的广泛实验验证了其优越性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态语音情感识别对改善人机交互至关重要。</li>
<li>ASR误差可能影响情感识别的准确性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.18706">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-0722de142e8fa7b23eb3442906693c9e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760037215&auth_key=1760037215-0-0-3d424f120b0d9e57015a022aa97f00e5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-148cd7bd1f5a96bbd25ae1f963f79d08~resize:0:q75.jpg?source=1f5c5e47&expiration=1760037223&auth_key=1760037223-0-0-16d519c86299dd2581db3d9b4e7aee8d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-cd2db849a309c6179af291a7787607a9~resize:0:q75.jpg?source=1f5c5e47&expiration=1760037230&auth_key=1760037230-0-0-a7dfd3cb1aa17c83a1b094b7a89f966e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c232bf68de05b8c62680bdefbe4400ba~resize:0:q75.jpg?source=1f5c5e47&expiration=1760037237&auth_key=1760037237-0-0-1f496a225512339b8d44129d4d1d43b6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-28/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-28/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-28/Face%20Swapping/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-664052c6a5c137f454c05f54494f913a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760037277&auth_key=1760037277-0-0-453b26037ce0e7a34d8cac65dc2943a0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="Face Swapping">
                        
                        <span class="card-title">Face Swapping</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Face Swapping 方向最新论文已更新，请持续关注 Update in 2025-09-28  CusEnhancer A Zero-Shot Scene and Controllability Enhancement Method   for Photo Customization via ResInversion
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-28
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Face-Swapping/" class="post-category">
                                    Face Swapping
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Face-Swapping/">
                        <span class="chip bg-color">Face Swapping</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-28/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F_Breast%20Ultrasound/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-080aa3b5008d9ae20dca9fcb207b15a5~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036572&auth_key=1760036572-0-0-1b2b33b504a59705c5ab67b77b3fa370&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="医学影像/Breast Ultrasound">
                        
                        <span class="card-title">医学影像/Breast Ultrasound</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学影像/Breast Ultrasound 方向最新论文已更新，请持续关注 Update in 2025-09-28  Multimodal Deep Learning for Phyllodes Tumor Classification from   Ultrasound and Clinical Data
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-28
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/" class="post-category">
                                    医学影像/Breast Ultrasound
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/">
                        <span class="chip bg-color">医学影像/Breast Ultrasound</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">30341.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
