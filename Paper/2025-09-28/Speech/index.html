<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-28  MeanSE Efficient Generative Speech Enhancement with Mean Flows">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Speech/2509.20971v1/page_3_1.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-28
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-07
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    17k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    69 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-09-28-æ›´æ–°"><a href="#2025-09-28-æ›´æ–°" class="headerlink" title="2025-09-28 æ›´æ–°"></a>2025-09-28 æ›´æ–°</h1><h2 id="MeanSE-Efficient-Generative-Speech-Enhancement-with-Mean-Flows"><a href="#MeanSE-Efficient-Generative-Speech-Enhancement-with-Mean-Flows" class="headerlink" title="MeanSE: Efficient Generative Speech Enhancement with Mean Flows"></a>MeanSE: Efficient Generative Speech Enhancement with Mean Flows</h2><p><strong>Authors:Jiahe Wang, Hongyu Wang, Wei Wang, Lei Yang, Chenda Li, Wangyou Zhang, Lufen Tan, Yanmin Qian</strong></p>
<p>Speech enhancement (SE) improves degraded speechâ€™s quality, with generative models like flow matching gaining attention for their outstanding perceptual quality. However, the flow-based model requires multiple numbers of function evaluations (NFEs) to achieve stable and satisfactory performance, leading to high computational load and poor 1-NFE performance. In this paper, we propose MeanSE, an efficient generative speech enhancement model using mean flows, which models the average velocity field to achieve high-quality 1-NFE enhancement. Experimental results demonstrate that our proposed MeanSE significantly outperforms the flow matching baseline with a single NFE, exhibiting extremely better out-of-domain generalization capabilities. </p>
<blockquote>
<p>è¯­éŸ³å¢å¼ºï¼ˆSEï¼‰æ—¨åœ¨æé«˜é€€åŒ–è¯­éŸ³çš„è´¨é‡ï¼Œç”Ÿæˆæ¨¡å‹å¦‚æµåŒ¹é…ç”±äºå…¶å‡ºè‰²çš„æ„ŸçŸ¥è´¨é‡è€Œå—åˆ°å…³æ³¨ã€‚ç„¶è€Œï¼ŒåŸºäºæµçš„æ¨¡å‹éœ€è¦åœ¨å‡½æ•°è¯„ä¼°æ¬¡æ•°ï¼ˆNFEï¼‰ä¸Šå¤šæ¬¡è¿­ä»£æ‰èƒ½è¾¾åˆ°ç¨³å®šå’Œæ»¡æ„çš„æ€§èƒ½ï¼Œä»è€Œå¯¼è‡´è®¡ç®—è´Ÿè½½è¾ƒé«˜ä»¥åŠå•NFEæ€§èƒ½è¾ƒå·®ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†MeanSEï¼Œè¿™æ˜¯ä¸€ç§ä½¿ç”¨å¹³å‡æµçš„æœ‰æ•ˆç”Ÿæˆè¯­éŸ³å¢å¼ºæ¨¡å‹ï¼Œé€šè¿‡å»ºæ¨¡å¹³å‡é€Ÿåº¦åœºå®ç°é«˜è´¨é‡çš„å•NFEå¢å¼ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„MeanSEåœ¨å•NFEçš„æƒ…å†µä¸‹æ˜¾è‘—ä¼˜äºæµåŒ¹é…åŸºçº¿ï¼Œå¹¶è¡¨ç°å‡ºæå¼ºçš„è·¨åŸŸæ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.21214v1">PDF</a> Submitted to ICASSP 2026</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºå‡å€¼æµçš„ç”Ÿæˆå¼è¯­éŸ³å¢å¼ºæ¨¡å‹MeanSEï¼Œé€šè¿‡å»ºæ¨¡å¹³å‡é€Ÿåº¦åœºå®ç°é«˜è´¨é‡çš„å•æ­¥åŠŸèƒ½è¯„ä¼°ï¼ˆ1-NFEï¼‰å¢å¼ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸åŸºäºæµåŠ¨åŒ¹é…çš„æ–¹æ³•ç›¸æ¯”ï¼ŒMeanSEåœ¨å•æ­¥è¯„ä¼°æ—¶è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ï¼Œå¹¶å…·æœ‰è¾ƒå¼ºçš„è·¨åŸŸæ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­éŸ³å¢å¼ºï¼ˆSEï¼‰æ—¨åœ¨æé«˜é€€åŒ–è¯­éŸ³çš„è´¨é‡ã€‚</li>
<li>ç”Ÿæˆæ¨¡å‹å¦‚æµåŒ¹é…å·²å¼•èµ·å…³æ³¨ï¼Œå› å…¶å‡ºè‰²çš„æ„ŸçŸ¥è´¨é‡ã€‚</li>
<li>æµæ¨¡å‹éœ€è¦å¤šæ¬¡åŠŸèƒ½è¯„ä¼°ï¼ˆNFEsï¼‰ä»¥è¾¾åˆ°ç¨³å®šå’Œæ»¡æ„æ€§èƒ½ï¼Œå¯¼è‡´è®¡ç®—è´Ÿè½½é«˜å’Œå•æ­¥è¯„ä¼°æ€§èƒ½å·®ã€‚</li>
<li>æœ¬æ–‡æå‡ºMeanSEæ¨¡å‹ï¼Œä½¿ç”¨å‡å€¼æµå®ç°é«˜æ•ˆç”Ÿæˆå¼è¯­éŸ³å¢å¼ºã€‚</li>
<li>MeanSEæ¨¡å‹é€šè¿‡å»ºæ¨¡å¹³å‡é€Ÿåº¦åœºå®ç°é«˜è´¨é‡å¢å¼ºã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒMeanSEåœ¨å•æ­¥åŠŸèƒ½è¯„ä¼°æ—¶æ˜¾è‘—ä¼˜äºæµåŠ¨åŒ¹é…æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.21214">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Speech/2509.21214v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Speech/2509.21214v1/page_3_0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Hybrid-Real-And-Complex-Valued-Neural-Network-Concept-For-Low-Complexity-Phase-Aware-Speech-Enhancement"><a href="#Hybrid-Real-And-Complex-Valued-Neural-Network-Concept-For-Low-Complexity-Phase-Aware-Speech-Enhancement" class="headerlink" title="Hybrid Real- And Complex-Valued Neural Network Concept For   Low-Complexity Phase-Aware Speech Enhancement"></a>Hybrid Real- And Complex-Valued Neural Network Concept For   Low-Complexity Phase-Aware Speech Enhancement</h2><p><strong>Authors:Luan VinÃ­cius Fiorio, Alex Young, Ronald M. Aarts</strong></p>
<p>In this paper, we propose hybrid real- and complex-valued neural networks for speech enhancement. Real- or complex-valued models are either inefficient or present high complexity. We devise a straightforward design method for extending a real-valued network into its hybrid counterpart. Based on speech intelligibility and quality metrics, we compare the real, complex, and hybrid versions of a convolutional and a convolutional-recurrent architecture. The hybrid network consistently outperforms its counterparts with the same number of parameters. Additionally, the hybrid modelsâ€™ complexity in terms of multiply-accumulate operations is substantially lower than that of their counterparts. </p>
<blockquote>
<p>åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ç”¨äºè¯­éŸ³å¢å¼ºçš„æ··åˆå®å€¼å’Œå¤å€¼ç¥ç»ç½‘ç»œã€‚å®å€¼æˆ–å¤å€¼æ¨¡å‹è¦ä¹ˆæ•ˆç‡ä½ä¸‹ï¼Œè¦ä¹ˆå¤æ‚åº¦è¾ƒé«˜ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ç§å°†å®å€¼ç½‘ç»œç›´æ¥æ‰©å±•ä¸ºå…¶æ··åˆå¯¹åº”ç½‘ç»œçš„æ–¹æ³•ã€‚åŸºäºè¯­éŸ³æ¸…æ™°åº¦å’Œè´¨é‡æŒ‡æ ‡ï¼Œæˆ‘ä»¬æ¯”è¾ƒäº†å·ç§¯å’Œå·ç§¯é€’å½’æ¶æ„çš„å®å€¼ã€å¤å€¼å’Œæ··åˆç‰ˆæœ¬ã€‚æ··åˆç½‘ç»œåœ¨ç›¸åŒå‚æ•°æ•°é‡çš„æƒ…å†µä¸‹å§‹ç»ˆä¼˜äºå…¶ä»–ç½‘ç»œã€‚æ­¤å¤–ï¼Œæ··åˆæ¨¡å‹åœ¨ä¹˜ç§¯ç´¯åŠ æ“ä½œæ–¹é¢çš„å¤æ‚åº¦è¿œä½äºå…¶ä»–æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.21185v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>æœ¬æ–‡æå‡ºæ··åˆå®æ•°å’Œå¤æ•°ç¥ç»ç½‘ç»œç”¨äºè¯­éŸ³å¢å¼ºã€‚æ–‡ç« ä»‹ç»äº†å°†å®å€¼ç½‘ç»œæ‰©å±•ä¸ºæ··åˆç½‘ç»œçš„è®¾è®¡æ–¹æ³•ï¼Œå¹¶é€šè¿‡è¯­éŸ³æ¸…æ™°åº¦å’Œè´¨é‡æŒ‡æ ‡æ¯”è¾ƒäº†å®å€¼ç½‘ç»œã€å¤æ•°ç½‘ç»œå’Œæ··åˆç½‘ç»œçš„å·ç§¯å’Œå·ç§¯é€’å½’æ¶æ„ã€‚ç»“æœæ˜¾ç¤ºæ··åˆç½‘ç»œåœ¨å‚æ•°æ•°é‡ç›¸åŒçš„æƒ…å†µä¸‹æ€§èƒ½æ›´ä¼˜ç§€ï¼ŒåŒæ—¶æ··åˆæ¨¡å‹çš„ä¹˜ç§¯ç´¯åŠ æ“ä½œå¤æ‚åº¦ä¹Ÿè¾ƒä½ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æå‡ºæ··åˆå®æ•°å’Œå¤æ•°ç¥ç»ç½‘ç»œç”¨äºè¯­éŸ³å¢å¼ºã€‚</li>
<li>ä»‹ç»äº†ä¸€ç§å°†å®å€¼ç½‘ç»œæ‰©å±•ä¸ºæ··åˆç½‘ç»œçš„è®¾è®¡æ–¹æ³•ã€‚</li>
<li>å¯¹æ¯”äº†å®å€¼ç½‘ç»œã€å¤æ•°ç½‘ç»œå’Œæ··åˆç½‘ç»œçš„å·ç§¯å’Œå·ç§¯é€’å½’æ¶æ„çš„è¯­éŸ³å¢å¼ºæ•ˆæœã€‚</li>
<li>æ··åˆç½‘ç»œåœ¨å‚æ•°æ•°é‡ç›¸åŒçš„æƒ…å†µä¸‹æ€§èƒ½æ›´ä¼˜ç§€ã€‚</li>
<li>æ··åˆæ¨¡å‹çš„å¤æ‚åº¦ä½äºå®å€¼å’Œå¤æ•°æ¨¡å‹ã€‚</li>
<li>è¯­éŸ³æ¸…æ™°åº¦å’Œè´¨é‡æŒ‡æ ‡æ˜¯è¯„ä¼°æ¨¡å‹æ€§èƒ½çš„é‡è¦æ ‡å‡†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.21185">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Speech/2509.21185v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Speech/2509.21185v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Speech/2509.21185v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Speech/2509.21185v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="i-LAVA-Insights-on-Low-Latency-Voice-2-Voice-Architecture-for-Agents"><a href="#i-LAVA-Insights-on-Low-Latency-Voice-2-Voice-Architecture-for-Agents" class="headerlink" title="i-LAVA: Insights on Low Latency Voice-2-Voice Architecture for Agents"></a>i-LAVA: Insights on Low Latency Voice-2-Voice Architecture for Agents</h2><p><strong>Authors:Anupam Purwar, Aditya Choudhary</strong></p>
<p>We experiment with a low-latency, end-to-end voice-to-voice communication model to optimize it for real-time conversational applications. By analyzing components essential to voice to voice (V-2-V) system viz. automatic speech recognition (ASR), text-to-speech (TTS), and dialog management, our work analyzes how to reduce processing time while maintaining high-quality interactions to identify the levers for optimizing V-2-V system. Our work identifies that TTS component which generates life-like voice, full of emotions including natural pauses and exclamations has highest impact on Real time factor (RTF). The experimented V-2-V architecture utilizes CSM1b has the capability to understand tone as well as context of conversation by ingesting both audio and text of prior exchanges to generate contextually accurate speech. We explored optimization of Residual Vector Quantization (RVQ) iterations by the TTS decoder which come at a cost of decrease in the quality of voice generated. Our experimental evaluations also demonstrate that for V-2-V implementations based on CSM most important optimizations can be brought by reducing the number of RVQ Iterations along with the codebooks used in Mimi. </p>
<blockquote>
<p>æˆ‘ä»¬å¯¹ä¸€ä¸ªä½å»¶è¿Ÿã€ç«¯åˆ°ç«¯çš„è¯­éŸ³å¯¹è¯­éŸ³é€šä¿¡æ¨¡å‹è¿›è¡Œäº†å®éªŒï¼Œä»¥ä¼˜åŒ–å…¶é€‚ç”¨äºå®æ—¶å¯¹è¯åº”ç”¨ã€‚é€šè¿‡åˆ†æè¯­éŸ³å¯¹è¯­éŸ³ï¼ˆV-2-Vï¼‰ç³»ç»Ÿçš„å…³é”®ç»„ä»¶ï¼Œå³è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ã€æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰å’Œå¯¹è¯ç®¡ç†ï¼Œæˆ‘ä»¬çš„å·¥ä½œåˆ†æäº†å¦‚ä½•åœ¨ä¿æŒé«˜è´¨é‡äº¤äº’çš„åŒæ—¶å‡å°‘å¤„ç†æ—¶é—´ï¼Œä»¥ç¡®å®šä¼˜åŒ–V-2-Vç³»ç»Ÿçš„å…³é”®å› ç´ ã€‚æˆ‘ä»¬çš„å·¥ä½œå‘ç°ï¼ŒTTSç»„ä»¶äº§ç”Ÿé€¼çœŸçš„è¯­éŸ³ï¼Œå……æ»¡æƒ…æ„Ÿï¼ŒåŒ…æ‹¬è‡ªç„¶åœé¡¿å’Œæ„Ÿå¹ï¼Œå¯¹å®æ—¶å› å­ï¼ˆRTFï¼‰çš„å½±å“æœ€å¤§ã€‚ç»è¿‡å®éªŒçš„V-2-Væ¶æ„é‡‡ç”¨CSM1bï¼Œå®ƒå…·æœ‰é€šè¿‡æ‘„å…¥å…ˆå‰å¯¹è¯çš„éŸ³é¢‘å’Œæ–‡æœ¬æ¥ç†è§£è¯­è°ƒä»¥åŠå¯¹è¯ä¸Šä¸‹æ–‡çš„èƒ½åŠ›ï¼Œä»¥ç”Ÿæˆä¸Šä¸‹æ–‡å‡†ç¡®çš„è¯­éŸ³ã€‚æˆ‘ä»¬æ¢ç´¢äº†é€šè¿‡TTSè§£ç å™¨ä¼˜åŒ–å‰©ä½™çŸ¢é‡é‡åŒ–ï¼ˆRVQï¼‰è¿­ä»£ï¼Œä½†è¿™ä¼šå¯¼è‡´ç”Ÿæˆçš„è¯­éŸ³è´¨é‡ä¸‹é™ã€‚æˆ‘ä»¬çš„å®éªŒè¯„ä¼°è¿˜è¡¨æ˜ï¼Œå¯¹äºåŸºäºCSMçš„V-2-Vå®ç°ï¼Œæœ€é‡è¦çš„ä¼˜åŒ–å¯ä»¥é€šè¿‡å‡å°‘RVQè¿­ä»£æ¬¡æ•°ä»¥åŠMimiä¸­ä½¿ç”¨çš„ä»£ç ç°¿æ¥å®ç°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.20971v1">PDF</a> This paper analyzes a low-latency, end-to-end voice-to-voice (V-2-V)   architecture, identifying that the Text-to-Speech (TTS) component has the   highest impact on real-time performance. By reducing the number of Residual   Vector Quantization (RVQ) iterations in the TTS model, latency can be   effectively halved, creating a direct trade-off between conversational speed   and audio quality</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†é’ˆå¯¹ç«¯åˆ°ç«¯çš„è¯­éŸ³é€šä¿¡æ¨¡å‹çš„å®éªŒç ”ç©¶ï¼Œä¸»è¦ä¼˜åŒ–äº†è¯­éŸ³åˆ°è¯­éŸ³ç³»ç»Ÿçš„æ ¸å¿ƒç»„ä»¶ä»¥é™ä½å»¶è¿Ÿå¹¶ç»´æŠ¤é«˜è´¨é‡äº¤äº’ã€‚å‘ç°æ–‡æœ¬åˆ°è¯­éŸ³ç»„ä»¶å¯¹å®æ—¶æ€§å› å­å½±å“æœ€å¤§ï¼Œåˆ©ç”¨CSMçš„å®éªŒæ¶æ„èƒ½å¤Ÿé€šè¿‡å¸æ”¶å…ˆå‰å¯¹è¯çš„éŸ³é¢‘å’Œæ–‡æœ¬æ¥ç”Ÿæˆè¯­å¢ƒå‡†ç¡®çš„è¯­éŸ³ã€‚åŒæ—¶æ¢ç´¢äº†é€šè¿‡å‡å°‘RVQè¿­ä»£æ¬¡æ•°æ¥ä¼˜åŒ–TTSè§£ç å™¨çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥å®éªŒé’ˆå¯¹ç«¯åˆ°ç«¯çš„è¯­éŸ³é€šä¿¡æ¨¡å‹è¿›è¡Œç ”ç©¶ï¼Œä¸»è¦ä¼˜åŒ–å®æ—¶å¯¹è¯åº”ç”¨çš„æ€§èƒ½ã€‚</li>
<li>å®éªŒæ¶‰åŠçš„æ ¸å¿ƒç»„ä»¶åŒ…æ‹¬è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ã€æ–‡æœ¬åˆ°è¯­éŸ³å’Œå¯¹è¯ç®¡ç†ã€‚</li>
<li>å‘ç°æ–‡æœ¬åˆ°è¯­éŸ³ç»„ä»¶å¯¹ä¿æŒå®æ—¶å¯¹è¯çš„å®æ—¶æ€§å› å­å½±å“æœ€å¤§ã€‚</li>
<li>åˆ©ç”¨CSMçš„å®éªŒæ¶æ„èƒ½å¤Ÿç”Ÿæˆå…·æœ‰æƒ…æ„Ÿå’Œè¯­å¢ƒå‡†ç¡®çš„è¯­éŸ³ï¼Œç†è§£å¯¹è¯çš„è¯­è°ƒä¸ä¸Šä¸‹æ–‡ã€‚</li>
<li>é€šè¿‡å‡å°‘RVQè¿­ä»£æ¬¡æ•°æ¥ä¼˜åŒ–TTSè§£ç å™¨ï¼Œä½†å¯èƒ½ä¼šé™ä½ç”Ÿæˆçš„è¯­éŸ³è´¨é‡ã€‚</li>
<li>å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œå¯¹äºåŸºäºCSMçš„è¯­éŸ³åˆ°è¯­éŸ³å®ç°ï¼Œæœ€é‡è¦çš„ä¼˜åŒ–æ˜¯é€šè¿‡å‡å°‘RVQè¿­ä»£æ¬¡æ•°ä»¥åŠä½¿ç”¨Mimiä¸­çš„ç æœ¬ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.20971">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Speech/2509.20971v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Speech/2509.20971v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Speech/2509.20971v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Speech/2509.20971v1/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Speech/2509.20971v1/page_2_2.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Speech/2509.20971v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Speech/2509.20971v1/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Speech/2509.20971v1/page_3_2.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Speech/2509.20971v1/page_3_3.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="SingVERSE-A-Diverse-Real-World-Benchmark-for-Singing-Voice-Enhancement"><a href="#SingVERSE-A-Diverse-Real-World-Benchmark-for-Singing-Voice-Enhancement" class="headerlink" title="SingVERSE: A Diverse, Real-World Benchmark for Singing Voice Enhancement"></a>SingVERSE: A Diverse, Real-World Benchmark for Singing Voice Enhancement</h2><p><strong>Authors:Shaohan Jiang, Junan Zhang, Yunjia Zhang, Jing Yang, Fan Fan, Zhizheng Wu</strong></p>
<p>This paper presents a benchmark for singing voice enhancement. The development of singing voice enhancement is limited by the lack of realistic evaluation data. To address this gap, this paper introduces SingVERSE, the first real-world benchmark for singing voice enhancement, covering diverse acoustic scenarios and providing paired, studio-quality clean references. Leveraging SingVERSE, we conduct a comprehensive evaluation of state-of-the-art models and uncover a consistent trade-off between perceptual quality and intelligibility. Finally, we show that training on in-domain singing data substantially improves enhancement performance without degrading speech capabilities, establishing a simple yet effective path forward. This work offers the community a foundational benchmark together with critical insights to guide future advances in this underexplored domain. Demopage: <a target="_blank" rel="noopener" href="https://singverse.github.io/">https://singverse.github.io</a> </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ­Œå”±å£°éŸ³å¢å¼ºçš„åŸºå‡†æµ‹è¯•ã€‚æ­Œå”±å£°éŸ³å¢å¼ºçš„å¼€å‘å—åˆ°ç¼ºä¹ç°å®è¯„ä¼°æ•°æ®çš„é™åˆ¶ã€‚ä¸ºäº†è§£å†³è¿™ä¸€ç©ºç™½ï¼Œæœ¬æ–‡ä»‹ç»äº†SingVERSEï¼Œè¿™æ˜¯æ­Œå”±å£°éŸ³å¢å¼ºçš„ç¬¬ä¸€ä¸ªçœŸå®ä¸–ç•ŒåŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–äº†å¤šç§å£°å­¦åœºæ™¯ï¼Œå¹¶æä¾›äº†é…å¯¹ã€å·¥ä½œå®¤è´¨é‡çš„æ¸…æ´å‚è€ƒã€‚åˆ©ç”¨SingVERSEï¼Œæˆ‘ä»¬å¯¹æœ€å…ˆè¿›çš„æ¨¡å‹è¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œå¹¶å‘ç°äº†æ„ŸçŸ¥è´¨é‡ä¸å¯ç†è§£æ€§ä¹‹é—´çš„æŒç»­æƒè¡¡ã€‚æœ€åï¼Œæˆ‘ä»¬è¯æ˜äº†åœ¨åŸŸå†…æ­Œå”±æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒå¯ä»¥æ˜¾è‘—æé«˜å¢å¼ºæ€§èƒ½ï¼Œè€Œä¸ä¼šé™ä½è¯­éŸ³åŠŸèƒ½ï¼Œä¸ºç®€å•æœ‰æ•ˆçš„å‰è¿›æ–¹å‘é“ºå¹³äº†é“è·¯ã€‚è¿™é¡¹å·¥ä½œä¸ºç¤¾åŒºæä¾›äº†ä¸€ä¸ªåŸºå‡†æµ‹è¯•ï¼Œä»¥åŠå…³é”®çš„è§è§£ï¼Œä»¥æŒ‡å¯¼æœªæ¥åœ¨è¿™ä¸ªæœªè¢«å……åˆ†æ¢ç´¢çš„é¢†åŸŸå–å¾—è¿›å±•ã€‚Demoé¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://singverse.github.io/">https://singverse.github.io</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.20969v1">PDF</a> Demopage: <a target="_blank" rel="noopener" href="https://singverse.github.io/">https://singverse.github.io</a>, Dataset:   <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/amphion/SingVERSE">https://huggingface.co/datasets/amphion/SingVERSE</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªæ­Œå”±å£°éŸ³å¢å¼ºçš„åŸºå‡†æµ‹è¯•ã€‚ç”±äºç¼ºä¹çœŸå®çš„è¯„ä¼°æ•°æ®ï¼Œæ­Œå”±å£°éŸ³å¢å¼ºçš„å¼€å‘å—åˆ°é™åˆ¶ã€‚æœ¬æ–‡å¼•å…¥äº†SingVERSEï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªçœŸå®çš„æ­Œå”±å£°éŸ³å¢å¼ºåŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–äº†å„ç§å£°å­¦åœºæ™¯ï¼Œå¹¶æä¾›äº†é…å¯¹çš„ã€å·¥ä½œå®¤è´¨é‡çš„å¹²å‡€å‚è€ƒã€‚åˆ©ç”¨SingVERSEï¼Œæˆ‘ä»¬å¯¹æœ€æ–°æŠ€æœ¯æ¨¡å‹è¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œå‘ç°äº†æ„ŸçŸ¥è´¨é‡ä¸å¯ç†è§£æ€§ä¹‹é—´çš„æƒè¡¡ã€‚æœ€åï¼Œæˆ‘ä»¬è¡¨æ˜åœ¨æ­Œå”±æ•°æ®åŸŸå†…è¿›è¡Œè®­ç»ƒèƒ½æ˜¾è‘—æé«˜å¢å¼ºæ€§èƒ½ï¼Œä¸”ä¸å½±å“è¯­éŸ³èƒ½åŠ›ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†ä¸€ä¸ªåŸºç¡€æ€§çš„åŸºå‡†æµ‹è¯•å’Œå…³é”®è§è§£ã€‚æ›´å¤šè¯¦æƒ…å¯è§ï¼š<a target="_blank" rel="noopener" href="https://singverse.github.io/">https://singverse.github.io</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†SingVERSEåŸºå‡†æµ‹è¯•ï¼šé¦–ä¸ªé’ˆå¯¹æ­Œå”±å£°éŸ³å¢å¼ºçš„çœŸå®ä¸–ç•ŒåŸºå‡†æµ‹è¯•ã€‚</li>
<li>SingVERSEæ¶µç›–äº†å¤šæ ·çš„å£°å­¦åœºæ™¯ï¼Œå¹¶æä¾›é…å¯¹çš„ã€é«˜è´¨é‡å¹²å‡€å‚è€ƒã€‚</li>
<li>é€šè¿‡å¯¹æœ€æ–°æŠ€æœ¯æ¨¡å‹çš„è¯„ä¼°ï¼Œå‘ç°äº†æ„ŸçŸ¥è´¨é‡ä¸å¯ç†è§£æ€§ä¹‹é—´çš„æƒè¡¡ã€‚</li>
<li>åœ¨æ­Œå”±æ•°æ®åŸŸå†…è®­ç»ƒèƒ½æ˜¾è‘—æé«˜å¢å¼ºæ€§èƒ½ã€‚</li>
<li>è¯¥è®­ç»ƒç­–ç•¥ä¸ä¼šå½±å“è¯­éŸ³èƒ½åŠ›ã€‚</li>
<li>SingVERSEä¸ºæ­Œå”±å£°éŸ³å¢å¼ºé¢†åŸŸæä¾›äº†åŸºç¡€æ€§åŸºå‡†æµ‹è¯•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.20969">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Speech/2509.20969v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Speech/2509.20969v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Speech/2509.20969v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Speech/2509.20969v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Speech/2509.20969v1/page_3_1.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="PAS-SE-Personalized-Auxiliary-Sensor-Speech-Enhancement-for-Voice-Pickup-in-Hearables"><a href="#PAS-SE-Personalized-Auxiliary-Sensor-Speech-Enhancement-for-Voice-Pickup-in-Hearables" class="headerlink" title="PAS-SE: Personalized Auxiliary-Sensor Speech Enhancement for Voice   Pickup in Hearables"></a>PAS-SE: Personalized Auxiliary-Sensor Speech Enhancement for Voice   Pickup in Hearables</h2><p><strong>Authors:Mattes Ohlenbusch, Mikolaj Kegler, Marko Stamenovic</strong></p>
<p>Speech enhancement for voice pickup in hearables aims to improve the userâ€™s voice by suppressing noise and interfering talkers, while maintaining own-voice quality. For single-channel methods, it is particularly challenging to distinguish the target from interfering talkers without additional context. In this paper, we compare two strategies to resolve this ambiguity: personalized speech enhancement (PSE), which uses enrollment utterances to represent the target, and auxiliary-sensor speech enhancement (AS-SE), which uses in-ear microphones as additional input. We evaluate the strategies on two public datasets, employing different auxiliary sensor arrays, to investigate their cross-dataset generalization. We propose training-time augmentations to facilitate cross-dataset generalization of AS-SE systems. We also show that combining PSE and AS-SE (PAS-SE) provides complementary performance benefits, especially when enrollment speech is recorded with the in-ear microphone. We further demonstrate that PAS-SE personalized with noisy in-ear enrollments maintains performance benefits over the AS-SE system. </p>
<blockquote>
<p>é’ˆå¯¹å¯ä½©æˆ´è®¾å¤‡ä¸­çš„è¯­éŸ³é‡‡é›†è¯­éŸ³å¢å¼ºæ—¨åœ¨é€šè¿‡æŠ‘åˆ¶å™ªå£°å’Œå¹²æ‰°è¯´è¯è€…æ¥æé«˜ç”¨æˆ·è¯­éŸ³è´¨é‡ï¼ŒåŒæ—¶ä¿æŒè‡ªå·±çš„è¯­éŸ³è´¨é‡ã€‚å¯¹äºå•é€šé“æ–¹æ³•è€Œè¨€ï¼Œå°¤å…¶æ˜¯åœ¨æ²¡æœ‰é¢å¤–ä¸Šä¸‹æ–‡çš„æƒ…å†µä¸‹åŒºåˆ†ç›®æ ‡æ˜¯è¯´è¯è€…ç‰¹åˆ«å…·æœ‰æŒ‘æˆ˜æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ¯”è¾ƒäº†ä¸¤ç§è§£å†³æ­¤æ¨¡ç³Šæ€§çš„ç­–ç•¥ï¼šä¸ªæ€§åŒ–è¯­éŸ³å¢å¼ºï¼ˆPSEï¼‰ï¼Œä½¿ç”¨æ³¨å†Œè¯­éŸ³æ¥ä»£è¡¨ç›®æ ‡è¯­éŸ³ï¼›è¾…åŠ©ä¼ æ„Ÿå™¨è¯­éŸ³å¢å¼ºï¼ˆAS-SEï¼‰ï¼Œä½¿ç”¨å…¥è€³å¼éº¦å…‹é£ä½œä¸ºé¢å¤–è¾“å…¥ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªå…¬å…±æ•°æ®é›†ä¸Šè¯„ä¼°äº†è¿™ä¸¤ç§ç­–ç•¥ï¼Œä½¿ç”¨ä¸åŒçš„è¾…åŠ©ä¼ æ„Ÿå™¨é˜µåˆ—ï¼Œä»¥ç ”ç©¶å®ƒä»¬çš„è·¨æ•°æ®é›†æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºäº†è®­ç»ƒæ—¶å¢å¼ºæŠ€æœ¯ï¼Œä»¥ä¿ƒè¿›AS-SEç³»ç»Ÿçš„è·¨æ•°æ®é›†æ³›åŒ–ã€‚æˆ‘ä»¬è¿˜è¡¨æ˜ï¼Œå°†PSEå’ŒAS-SEï¼ˆPAS-SEï¼‰ç›¸ç»“åˆæä¾›äº†äº’è¡¥çš„æ€§èƒ½ä¼˜åŠ¿ï¼Œå°¤å…¶æ˜¯å½“æ³¨å†Œè¯­éŸ³ä½¿ç”¨å…¥è€³å¼éº¦å…‹é£å½•åˆ¶æ—¶ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è¯æ˜ï¼Œä½¿ç”¨å¸¦æœ‰å™ªå£°å…¥è€³å¼æ³¨å†Œçš„PAS-SEåœ¨æ€§èƒ½ä¸Šä»ç„¶ä¼˜äºAS-SEç³»ç»Ÿã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.20875v1">PDF</a> Submitted to ICASSP 2026</p>
<p><strong>Summary</strong></p>
<p>æ­¤æ–‡æ¢è®¨äº†åœ¨å¯ç©¿æˆ´è®¾å¤‡ä¸­è¿›è¡Œè¯­éŸ³å¢å¼ºçš„æŒ‘æˆ˜å’Œæ–¹æ³•ã€‚é’ˆå¯¹å•é€šé“æ–¹æ³•éš¾ä»¥åŒºåˆ†ç›®æ ‡å’Œå¹²æ‰°è¯´è¯è€…çš„é—®é¢˜ï¼Œæ¯”è¾ƒäº†ä¸ªæ€§åŒ–è¯­éŸ³å¢å¼ºï¼ˆPSEï¼‰å’Œä½¿ç”¨å…¥è€³å¼éº¦å…‹é£ä½œä¸ºé¢å¤–è¾“å…¥çš„è¾…åŠ©ä¼ æ„Ÿå™¨è¯­éŸ³å¢å¼ºï¼ˆAS-SEï¼‰ä¸¤ç§ç­–ç•¥ã€‚æ–‡ç« åœ¨å…¬å…±æ•°æ®é›†ä¸Šè¯„ä¼°äº†è¿™ä¸¤ç§ç­–ç•¥ï¼Œç ”ç©¶äº†å®ƒä»¬çš„è·¨æ•°æ®é›†æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶å‘ç°ç»“åˆä½¿ç”¨PSEå’ŒAS-SEå¯ä»¥æä¾›é¢å¤–çš„æ€§èƒ½ä¼˜åŠ¿ã€‚æ­¤å¤–ï¼Œè¿˜ä»‹ç»äº†å¢å¼ºç³»ç»Ÿæ³›åŒ–èƒ½åŠ›çš„è®­ç»ƒæ—¶é—´å¢å¼ºæŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­éŸ³å¢å¼ºåœ¨å¯ç©¿æˆ´è®¾å¤‡ä¸­ç”¨äºæé«˜ç”¨æˆ·å£°éŸ³è´¨é‡ï¼ŒæŠ‘åˆ¶å™ªå£°å’Œå¹²æ‰°è¯´è¯è€…ã€‚</li>
<li>å•é€šé“æ–¹æ³•åŒºåˆ†ç›®æ ‡å’Œå¹²æ‰°è¯´è¯è€…å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>ä¸ªæ€§åŒ–è¯­éŸ³å¢å¼ºï¼ˆPSEï¼‰ä½¿ç”¨æ³¨å†Œè¯­éŸ³ä»£è¡¨ç›®æ ‡è¯´è¯è€…ã€‚</li>
<li>è¾…åŠ©ä¼ æ„Ÿå™¨è¯­éŸ³å¢å¼ºï¼ˆAS-SEï¼‰ä½¿ç”¨å…¥è€³å¼éº¦å…‹é£ä½œä¸ºé¢å¤–è¾“å…¥ã€‚</li>
<li>åœ¨å…¬å…±æ•°æ®é›†ä¸Šè¯„ä¼°äº†PSEå’ŒAS-SEçš„è·¨æ•°æ®é›†æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>ç»“åˆPSEå’ŒAS-SEï¼ˆPAS-SEï¼‰æä¾›é¢å¤–çš„æ€§èƒ½ä¼˜åŠ¿ï¼Œå°¤å…¶åœ¨æ³¨å†Œè¯­éŸ³ä½¿ç”¨å…¥è€³å¼éº¦å…‹é£å½•åˆ¶æ—¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.20875">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Speech/2509.20875v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Speech/2509.20875v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Speech/2509.20875v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Speech/2509.20875v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Variational-Low-Rank-Adaptation-for-Personalized-Impaired-Speech-Recognition"><a href="#Variational-Low-Rank-Adaptation-for-Personalized-Impaired-Speech-Recognition" class="headerlink" title="Variational Low-Rank Adaptation for Personalized Impaired Speech   Recognition"></a>Variational Low-Rank Adaptation for Personalized Impaired Speech   Recognition</h2><p><strong>Authors:Niclas Pokel, PehuÃ©n Moure, Roman Boehringer, Shih-Chii Liu, Yingqiang Gao</strong></p>
<p>Speech impairments resulting from congenital disorders, such as cerebral palsy, down syndrome, or apert syndrome, as well as acquired brain injuries due to stroke, traumatic accidents, or tumors, present major challenges to automatic speech recognition (ASR) systems. Despite recent advancements, state-of-the-art ASR models like Whisper still struggle with non-normative speech due to limited training data availability and high acoustic variability. Moreover, collecting and annotating non-normative speech is burdensome: speaking is effortful for many affected individuals, while laborious annotation often requires caregivers familiar with the speaker. This work introduces a novel ASR personalization method based on Bayesian Low-rank Adaptation for data-efficient fine-tuning. We validate our method on the English UA-Speech dataset and a newly collected German speech dataset, BF-Sprache, from a child with structural speech impairment. The dataset and approach are designed to reflect the challenges of low-resource settings that include individuals with speech impairments. Our method significantly improves ASR accuracy for impaired speech while maintaining data and annotation efficiency, offering a practical path toward inclusive ASR. </p>
<blockquote>
<p>ç”±äºå…ˆå¤©æ€§ç–¾ç—…ï¼ˆå¦‚è„‘æ€§ç˜«ç—ªã€å”æ°ç»¼åˆå¾æˆ– Apert ç»¼åˆå¾ï¼‰ä»¥åŠåå¤©æ€§è„‘æŸä¼¤ï¼ˆå¦‚ä¸­é£ã€äº‹æ•…æˆ–è‚¿ç˜¤ï¼‰å¯¼è‡´çš„è¨€è¯­éšœç¢ï¼Œç»™è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿå¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ã€‚å°½ç®¡æœ€è¿‘å–å¾—äº†è¿›å±•ï¼Œä½†æœ€å…ˆè¿›çš„ASRæ¨¡å‹ï¼ˆå¦‚Whisperï¼‰ä»ç„¶éš¾ä»¥å¤„ç†éæ ‡å‡†è¯­éŸ³ï¼Œè¿™æ˜¯ç”±äºè®­ç»ƒæ•°æ®æœ‰é™å’Œå£°éŸ³å˜åŒ–è¾ƒå¤§ã€‚æ­¤å¤–ï¼Œæ”¶é›†å’Œæ³¨é‡Šéæ ‡å‡†è¯­éŸ³æ˜¯ä¸€é¡¹è‰°å·¨çš„ä»»åŠ¡ï¼šå¯¹äºè®¸å¤šå—å½±å“çš„äººæ¥è¯´ï¼Œè¯´è¯æ˜¯ä¸€é¡¹è‰°å·¨çš„ä»»åŠ¡ï¼Œè€Œç¹ççš„æ³¨é‡Šé€šå¸¸éœ€è¦ç†Ÿæ‚‰è¯´è¯è€…çš„æŠ¤ç†äººå‘˜ã€‚è¿™é¡¹å·¥ä½œä»‹ç»äº†ä¸€ç§åŸºäºè´å¶æ–¯ä½ç§©é€‚åº”çš„æ–°å‹ASRä¸ªæ€§åŒ–æ–¹æ³•ï¼Œç”¨äºé«˜æ•ˆå¾®è°ƒæ•°æ®ã€‚æˆ‘ä»¬åœ¨è‹±æ–‡UA-Speechæ•°æ®é›†å’Œä»ä¸€åæœ‰ç»“æ„æ€§è¨€è¯­éšœç¢çš„å„¿ç«¥é‚£é‡Œæ”¶é›†çš„å…¨æ–°å¾·è¯­è¯­éŸ³æ•°æ®é›†BF-Spracheä¸ŠéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ã€‚æ•°æ®é›†å’Œæ–¹æ³•æ—¨åœ¨åæ˜ èµ„æºåŒ®ä¹ç¯å¢ƒä¸­çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æœ‰è¨€è¯­éšœç¢çš„ä¸ªäººã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨æé«˜å—æŸè¯­éŸ³çš„ASRå‡†ç¡®æ€§æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆæ•ˆï¼ŒåŒæ—¶ä¿æŒäº†æ•°æ®å’Œæ³¨é‡Šçš„æ•ˆç‡ï¼Œä¸ºå®ç°åŒ…å®¹æ€§ASRæä¾›äº†ä¸€æ¡å®ç”¨é€”å¾„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.20397v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>é’ˆå¯¹å…ˆå¤©æ€§éšœç¢ï¼ˆå¦‚è„‘æ€§ç˜«ç—ªã€å”æ°ç»¼åˆç—‡æˆ–å£è£‚ç»¼åˆç—‡ï¼‰ä»¥åŠåå¤©æ€§è„‘æŸä¼¤ï¼ˆå¦‚ä¸­é£ã€äº‹æ•…æˆ–è‚¿ç˜¤ï¼‰å¯¼è‡´çš„è¨€è¯­éšœç¢ï¼Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿé¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚å°½ç®¡æœ‰æœ€æ–°æŠ€æœ¯è¿›æ­¥ï¼Œä½†æœ€å…ˆè¿›çš„ASRæ¨¡å‹ï¼ˆå¦‚Whisperï¼‰ä»å› è®­ç»ƒæ•°æ®æœ‰é™å’Œå£°éŸ³å˜åŒ–å¤§ï¼Œéš¾ä»¥è¯†åˆ«éæ ‡å‡†è¨€è¯­ã€‚æ­¤å¤–ï¼Œæ”¶é›†å¹¶æ ‡æ³¨éæ ‡å‡†è¨€è¯­æ—¢å›°éš¾åˆç¹çï¼šå¯¹è®¸å¤šå—å½±å“çš„äººæ¥è¯´è¯´è¯å·²ç»å¾ˆåƒåŠ›ï¼Œè€Œç¹ççš„æ ‡æ³¨å·¥ä½œè¿˜éœ€è¦ç†Ÿæ‚‰å‘è¨€è€…çš„æŠ¤ç†äººå‘˜å‚ä¸ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„ASRä¸ªæ€§åŒ–æ–¹æ³•â€”â€”åŸºäºè´å¶æ–¯ä½ç§©é€‚åº”çš„å°‘é‡æ•°æ®ç²¾ç»†è°ƒæ•´æ³•ã€‚æˆ‘ä»¬åœ¨è‹±è¯­UA-Speechæ•°æ®é›†å’Œæ–°æ”¶é›†çš„å¾·è¯­æ•°æ®é›†BF-Spracheï¼ˆæ¥è‡ªä¸€ä¸ªæœ‰ç»“æ„æ€§è¨€è¯­éšœç¢çš„å„¿ç«¥ï¼‰ä¸ŠéªŒè¯äº†è¯¥æ–¹æ³•ã€‚æ•°æ®é›†å’Œæ–¹æ³•åæ˜ äº†ä½èµ„æºè®¾ç½®ä¸­çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æ‚£æœ‰è¨€è¯­éšœç¢çš„äººç¾¤ã€‚æ­¤æ–¹æ³•æ˜¾è‘—æé«˜äº†å¯¹å—æŸè¯­éŸ³çš„ASRå‡†ç¡®æ€§ï¼ŒåŒæ—¶ä¿æŒäº†æ•°æ®å’Œæ ‡æ³¨çš„æ•ˆç‡ï¼Œä¸ºåŒ…å®¹æ€§ASRæä¾›äº†å®ç”¨é€”å¾„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å…ˆå¤©æ€§éšœç¢å’Œåå¤©æ€§è„‘æŸä¼¤å¯¼è‡´çš„è¨€è¯­éšœç¢å¯¹ASRç³»ç»Ÿæ„æˆæŒ‘æˆ˜ã€‚</li>
<li>å½“å‰ASRæ¨¡å‹åœ¨è¯†åˆ«éæ ‡å‡†è¨€è¯­æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œä¸»è¦åŸå› æ˜¯è®­ç»ƒæ•°æ®æœ‰é™å’Œå£°éŸ³å˜åŒ–å¤§ã€‚</li>
<li>æ”¶é›†å¹¶æ ‡æ³¨éæ ‡å‡†è¨€è¯­æ•°æ®æ—¢å›°éš¾åˆç¹çï¼Œéœ€è¦æŠ¤ç†äººå‘˜å‚ä¸ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºè´å¶æ–¯ä½ç§©é€‚åº”çš„ASRä¸ªæ€§åŒ–æ–¹æ³•ï¼Œæ—¨åœ¨å®ç°å°‘é‡æ•°æ®çš„ç²¾ç»†è°ƒæ•´ã€‚</li>
<li>æ–¹æ³•åœ¨è‹±è¯­å’Œå¾·è¯­æ•°æ®é›†ä¸Šè¿›è¡Œäº†éªŒè¯ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹æ‚£æœ‰ç»“æ„æ€§è¨€è¯­éšœç¢çš„äººç¾¤ã€‚</li>
<li>æ­¤æ–¹æ³•æé«˜äº†å¯¹å—æŸè¯­éŸ³çš„ASRå‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.20397">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Speech/2509.20397v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Speech/2509.20397v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Speech/2509.20397v1/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Speech/2509.20397v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Data-Efficient-ASR-Personalization-for-Non-Normative-Speech-Using-an-Uncertainty-Based-Phoneme-Difficulty-Score-for-Guided-Sampling"><a href="#Data-Efficient-ASR-Personalization-for-Non-Normative-Speech-Using-an-Uncertainty-Based-Phoneme-Difficulty-Score-for-Guided-Sampling" class="headerlink" title="Data-Efficient ASR Personalization for Non-Normative Speech Using an   Uncertainty-Based Phoneme Difficulty Score for Guided Sampling"></a>Data-Efficient ASR Personalization for Non-Normative Speech Using an   Uncertainty-Based Phoneme Difficulty Score for Guided Sampling</h2><p><strong>Authors:Niclas Pokel, PehuÃ©n Moure, Roman Boehringer, Yingqiang Gao</strong></p>
<p>Automatic speech recognition (ASR) systems struggle with non-normative speech from individuals with impairments caused by conditions like cerebral palsy or structural anomalies. The high acoustic variability and scarcity of training data severely degrade model performance. This work introduces a data-efficient personalization method that quantifies phoneme-level uncertainty to guide fine-tuning. We leverage Monte Carlo Dropout to estimate which phonemes a model finds most difficult and use these estimates for a targeted oversampling strategy. We validate our method on English and German datasets. Crucially, we demonstrate that our model-derived uncertainty strongly correlates with phonemes identified as challenging in an expert clinical logopedic report, marking, to our knowledge, the first work to successfully align model uncertainty with expert assessment of speech difficulty. Our results show that this clinically-validated, uncertainty-guided sampling significantly improves ASR accuracy, delivering a practical framework for personalized and inclusive ASR. </p>
<blockquote>
<p>è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿåœ¨å¤„ç†å› è„‘æ€§ç˜«ç—ªæˆ–ç»“æ„æ€§å¼‚å¸¸ç­‰çŠ¶å†µå¯¼è‡´çš„ä¸ªä½“éè§„èŒƒæ€§è¯­éŸ³æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚é«˜éŸ³é¢‘å˜ä½“å’Œè®­ç»ƒæ•°æ®çš„ç¨€ç¼ºä¸¥é‡é™ä½äº†æ¨¡å‹æ€§èƒ½ã€‚è¿™é¡¹å·¥ä½œå¼•å…¥äº†ä¸€ç§æ•°æ®é«˜æ•ˆä¸ªæ€§åŒ–æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é‡åŒ–éŸ³ç´ çº§ä¸ç¡®å®šæ€§ä»¥æŒ‡å¯¼å¾®è°ƒã€‚æˆ‘ä»¬åˆ©ç”¨è’™ç‰¹å¡æ´›Dropoutæ¥ä¼°è®¡æ¨¡å‹è®¤ä¸ºå“ªäº›éŸ³ç´ æœ€å›°éš¾ï¼Œå¹¶ä½¿ç”¨è¿™äº›ä¼°è®¡å€¼è¿›è¡Œæœ‰é’ˆå¯¹æ€§çš„è¿‡é‡‡æ ·ç­–ç•¥ã€‚æˆ‘ä»¬åœ¨è‹±è¯­å’Œå¾·è¯­æ•°æ®é›†ä¸ŠéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ã€‚å…³é”®çš„æ˜¯ï¼Œæˆ‘ä»¬è¯æ˜æˆ‘ä»¬çš„æ¨¡å‹è¡ç”Ÿå‡ºçš„ä¸ç¡®å®šæ€§ä¸ä¸“å®¶ä¸´åºŠè¯­éŸ³æŠ¥å‘Šä¸­è¯†åˆ«çš„å…·æœ‰æŒ‘æˆ˜æ€§çš„éŸ³ç´ å¯†åˆ‡ç›¸å…³ï¼Œæ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯é¦–æ¬¡æˆåŠŸå°†æ¨¡å‹ä¸ç¡®å®šæ€§ä¸ä¸“å®¶è¯­éŸ³éš¾åº¦è¯„ä¼°å¯¹é½çš„å·¥ä½œã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œè¿™ç§ç»è¿‡ä¸´åºŠéªŒè¯çš„ä¸ç¡®å®šæ€§æŒ‡å¯¼é‡‡æ ·æ˜¾è‘—æé«˜äº†ASRçš„å‡†ç¡®æ€§ï¼Œä¸ºä¸ªæ€§åŒ–å’ŒåŒ…å®¹æ€§çš„ASRæä¾›äº†å®ç”¨çš„æ¡†æ¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.20396v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ•°æ®é«˜æ•ˆçš„ä¸ªæ€§åŒ–æ–¹æ³•ï¼Œé€šè¿‡é‡åŒ–éŸ³ç´ çº§ä¸ç¡®å®šæ€§æ¥å¼•å¯¼å¾®è°ƒï¼Œä»¥è§£å†³è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿå¯¹äºéè§„èŒƒè¯­éŸ³çš„è¯†åˆ«éš¾é¢˜ã€‚åˆ©ç”¨Monte Carlo Dropoutä¼°è®¡æ¨¡å‹éš¾ä»¥è¯†åˆ«çš„éŸ³ç´ ï¼Œå¹¶ä½¿ç”¨è¿™äº›ä¼°è®¡å€¼è¿›è¡Œæœ‰é’ˆå¯¹æ€§çš„è¿‡é‡‡æ ·ç­–ç•¥ã€‚è¯¥æ–¹æ³•åœ¨è‹±è¯­å’Œå¾·è¯­æ•°æ®é›†ä¸Šå¾—åˆ°éªŒè¯ï¼Œä¸”æ¨¡å‹è¡ç”Ÿå‡ºçš„ä¸ç¡®å®šæ€§ä¸ä¸“å®¶ä¸´åºŠè¯­éŸ³å›°éš¾è¯„ä¼°ç»“æœé«˜åº¦ä¸€è‡´ã€‚è¯¥æ–¹æ³•æ˜¾è‘—æé«˜äº†ASRçš„å‡†ç¡®æ€§ï¼Œä¸ºä¸ªæ€§åŒ–ä¸”åŒ…å®¹æ€§çš„ASRæä¾›äº†å®ç”¨æ¡†æ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ASRç³»ç»Ÿå¯¹äºéè§„èŒƒè¯­éŸ³å­˜åœ¨æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯å¯¹äºé‚£äº›å› è„‘æ€§ç˜«ç—ªæˆ–ç»“æ„å¼‚å¸¸ç­‰æ¡ä»¶å¯¼è‡´è¯­éŸ³éšœç¢çš„ä¸ªä½“ã€‚</li>
<li>é«˜å£°å­¦å˜ç‡å’Œè®­ç»ƒæ•°æ®ç¨€ç¼ºä¸¥é‡é™ä½äº†æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ•°æ®é«˜æ•ˆçš„ä¸ªæ€§åŒ–æ–¹æ³•ï¼Œé€šè¿‡é‡åŒ–éŸ³ç´ çº§ä¸ç¡®å®šæ€§æ¥å¼•å¯¼å¾®è°ƒã€‚</li>
<li>åˆ©ç”¨Monte Carlo Dropoutä¼°è®¡æ¨¡å‹éš¾ä»¥è¯†åˆ«çš„éŸ³ç´ ï¼Œå¹¶é‡‡ç”¨é’ˆå¯¹æ€§è¿‡é‡‡æ ·ç­–ç•¥ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨è‹±è¯­å’Œå¾·è¯­æ•°æ®é›†ä¸Šå¾—åˆ°éªŒè¯ã€‚</li>
<li>æ¨¡å‹è¡ç”Ÿå‡ºçš„ä¸ç¡®å®šæ€§ä¸ä¸“å®¶ä¸´åºŠè¯­éŸ³å›°éš¾è¯„ä¼°ç»“æœä¸€è‡´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.20396">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Speech/2509.20396v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Speech/2509.20396v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Speech/2509.20396v1/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Speech/2509.20396v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Discrete-Diffusion-for-Generative-Modeling-of-Text-Aligned-Speech-Tokens"><a href="#Discrete-Diffusion-for-Generative-Modeling-of-Text-Aligned-Speech-Tokens" class="headerlink" title="Discrete Diffusion for Generative Modeling of Text-Aligned Speech Tokens"></a>Discrete Diffusion for Generative Modeling of Text-Aligned Speech Tokens</h2><p><strong>Authors:Pin-Jui Ku, He Huang, Jean-Marie Lemercier, Subham Sekhar Sahoo, Zhehuai Chen, Ante JukiÄ‡</strong></p>
<p>This paper introduces a discrete diffusion model (DDM) framework for text-aligned speech tokenization and reconstruction. By replacing the auto-regressive speech decoder with a discrete diffusion counterpart, our model achieves significantly better reconstruction quality, stronger ASR performance, and faster inference. We provide a comprehensive analysis of applying DDMs to speech reconstruction, examining sampler choices, inference steps, and robustness to length-scale estimation errors. Furthermore, we improve the original TASTE by systematically comparing vector quantization modules, showing that FSQ yields up to a 35% relative WER reduction and +0.14 UT-MOS improvement over RVQ for AR models, while also enhancing DDM performance. Our model generates speech in just 10 denoising steps and even supports single-step generation with only minor quality degradation. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ç”¨äºæ–‡æœ¬å¯¹é½è¯­éŸ³åˆ†è¯å’Œé‡å»ºçš„ç¦»æ•£æ‰©æ•£æ¨¡å‹ï¼ˆDDMï¼‰æ¡†æ¶ã€‚é€šè¿‡ç”¨ç¦»æ•£æ‰©æ•£å¯¹åº”ç‰©æ›¿æ¢è‡ªå›å½’è¯­éŸ³è§£ç å™¨ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨é‡å»ºè´¨é‡ã€è¯­éŸ³è¯†åˆ«æ€§èƒ½ä»¥åŠæ¨ç†é€Ÿåº¦ä¸Šå®ç°äº†æ˜¾è‘—æå‡ã€‚æˆ‘ä»¬å¯¹å°†DDMsåº”ç”¨äºè¯­éŸ³é‡å»ºè¿›è¡Œäº†å…¨é¢çš„åˆ†æï¼Œç ”ç©¶äº†é‡‡æ ·å™¨é€‰æ‹©ã€æ¨ç†æ­¥éª¤ä»¥åŠå¯¹é•¿åº¦å°ºåº¦ä¼°è®¡é”™è¯¯çš„ç¨³å¥æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡ç³»ç»Ÿåœ°æ¯”è¾ƒå‘é‡é‡åŒ–æ¨¡å—æ”¹è¿›äº†åŸå§‹çš„TASTEï¼Œè¡¨æ˜FSQç›¸å¯¹äºRVQåœ¨ARæ¨¡å‹ä¸Šå®ç°äº†é«˜è¾¾35%çš„ç›¸å¯¹WERé™ä½å’Œ+0.14çš„UT-MOSæå‡ï¼ŒåŒæ—¶ä¹Ÿæå‡äº†DDMçš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„æ¨¡å‹ä»…åœ¨10ä¸ªå»å™ªæ­¥éª¤ä¸­ç”Ÿæˆè¯­éŸ³ï¼Œç”šè‡³æ”¯æŒå•æ­¥ç”Ÿæˆï¼Œåªæœ‰è½»å¾®çš„è´¨é‡ä¸‹é™ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.20060v1">PDF</a> 5 pages. submitted to ICASSP 2026</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºç¦»æ•£æ‰©æ•£æ¨¡å‹ï¼ˆDDMï¼‰æ¡†æ¶çš„æ–‡æœ¬å¯¹é½è¯­éŸ³åˆ‡åˆ†ä¸é‡å»ºæŠ€æœ¯ã€‚é€šè¿‡ç”¨ç¦»æ•£æ‰©æ•£è§£ç å™¨æ›¿ä»£è‡ªå›å½’è¯­éŸ³è§£ç å™¨ï¼Œè¯¥æ¨¡å‹å®ç°äº†æ›´é«˜çš„é‡å»ºè´¨é‡ã€æ›´å¼ºçš„è¯­éŸ³è¯†åˆ«æ€§èƒ½ä»¥åŠæ›´å¿«çš„æ¨ç†é€Ÿåº¦ã€‚æ–‡ç« å…¨é¢åˆ†æäº†å°†DDMåº”ç”¨äºè¯­éŸ³é‡å»ºçš„æ–¹æ³•ï¼Œæ¢è®¨äº†é‡‡æ ·å™¨é€‰æ‹©ã€æ¨ç†æ­¥éª¤ä»¥åŠå¯¹é•¿åº¦å°ºåº¦ä¼°ç®—é”™è¯¯çš„ç¨³å¥æ€§ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜æ”¹è¿›äº†åŸå§‹æ¨¡å‹TASTEï¼Œå¯¹æ¯”äº†å‘é‡é‡åŒ–æ¨¡å—ï¼Œå‘ç°FSQåœ¨ARæ¨¡å‹ä¸Šç›¸å¯¹äºRVQå®ç°äº†æœ€é«˜è¾¾35%çš„ç›¸å¯¹WERå‡å°‘å’Œ+0.14çš„UT-MOSæå‡ï¼ŒåŒæ—¶ä¹Ÿæå‡äº†DDMçš„æ€§èƒ½ã€‚è¯¥æ¨¡å‹ä»…éœ€åœ¨10æ­¥é™å™ªè¿‡ç¨‹ä¸­ç”Ÿæˆè¯­éŸ³ï¼Œå¹¶æ”¯æŒå•æ­¥ç”Ÿæˆï¼Œä»…ä¼´æœ‰è½»å¾®è´¨é‡æŸå¤±ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¦»æ•£æ‰©æ•£æ¨¡å‹ï¼ˆDDMï¼‰æ¡†æ¶ç”¨äºæ–‡æœ¬å¯¹é½çš„è¯­éŸ³åˆ‡åˆ†ä¸é‡å»ºã€‚</li>
<li>DDMå®ç°äº†æ›´é«˜çš„é‡å»ºè´¨é‡ã€å¢å¼ºçš„è¯­éŸ³è¯†åˆ«æ€§èƒ½å’Œæ›´å¿«çš„æ¨ç†é€Ÿåº¦ã€‚</li>
<li>é‡‡æ ·å™¨é€‰æ‹©ã€æ¨ç†æ­¥éª¤å’Œå¯¹é•¿åº¦å°ºåº¦ä¼°ç®—é”™è¯¯çš„ç¨³å¥æ€§æ˜¯DDMåº”ç”¨äºè¯­éŸ³é‡å»ºçš„å…³é”®åˆ†æç‚¹ã€‚</li>
<li>ç ”ç©¶æ”¹è¿›äº†åŸå§‹æ¨¡å‹TASTEï¼Œé€šè¿‡å¯¹æ¯”å‘é‡é‡åŒ–æ¨¡å—ï¼Œå‘ç°FSQåœ¨ARæ¨¡å‹ä¸Šæ˜¾è‘—æå‡äº†æ€§èƒ½ã€‚</li>
<li>FSQç›¸è¾ƒäºRVQåœ¨ARæ¨¡å‹ä¸Šå®ç°äº†WERå’ŒUT-MOSçš„æ˜¾è‘—æ”¹å–„ã€‚</li>
<li>DDMæ¨¡å‹åœ¨ç”Ÿæˆè¯­éŸ³æ—¶ä»…éœ€10æ­¥é™å™ªè¿‡ç¨‹ï¼Œå¹¶æ”¯æŒå•æ­¥ç”Ÿæˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.20060">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Speech/2509.20060v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Speech/2509.20060v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Speech/2509.20060v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Speech/2509.20060v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="SynchroRaMa-Lip-Synchronized-and-Emotion-Aware-Talking-Face-Generation-via-Multi-Modal-Emotion-Embedding"><a href="#SynchroRaMa-Lip-Synchronized-and-Emotion-Aware-Talking-Face-Generation-via-Multi-Modal-Emotion-Embedding" class="headerlink" title="SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation   via Multi-Modal Emotion Embedding"></a>SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation   via Multi-Modal Emotion Embedding</h2><p><strong>Authors:Phyo Thet Yee, Dimitrios Kollias, Sudeepta Mishra, Abhinav Dhall</strong></p>
<p>Audio-driven talking face generation has received growing interest, particularly for applications requiring expressive and natural human-avatar interaction. However, most existing emotion-aware methods rely on a single modality (either audio or image) for emotion embedding, limiting their ability to capture nuanced affective cues. Additionally, most methods condition on a single reference image, restricting the modelâ€™s ability to represent dynamic changes in actions or attributes across time. To address these issues, we introduce SynchroRaMa, a novel framework that integrates a multi-modal emotion embedding by combining emotional signals from text (via sentiment analysis) and audio (via speech-based emotion recognition and audio-derived valence-arousal features), enabling the generation of talking face videos with richer and more authentic emotional expressiveness and fidelity. To ensure natural head motion and accurate lip synchronization, SynchroRaMa includes an audio-to-motion (A2M) module that generates motion frames aligned with the input audio. Finally, SynchroRaMa incorporates scene descriptions generated by Large Language Model (LLM) as additional textual input, enabling it to capture dynamic actions and high-level semantic attributes. Conditioning the model on both visual and textual cues enhances temporal consistency and visual realism. Quantitative and qualitative experiments on benchmark datasets demonstrate that SynchroRaMa outperforms the state-of-the-art, achieving improvements in image quality, expression preservation, and motion realism. A user study further confirms that SynchroRaMa achieves higher subjective ratings than competing methods in overall naturalness, motion diversity, and video smoothness. Our project page is available at <a target="_blank" rel="noopener" href="https://novicemm.github.io/synchrorama">https://novicemm.github.io/synchrorama</a>. </p>
<blockquote>
<p>éŸ³é¢‘é©±åŠ¨çš„è¯´è¯äººè„¸ç”ŸæˆæŠ€æœ¯å·²å¼•èµ·å¹¿æ³›å…³æ³¨ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦è¡¨è¾¾è‡ªç„¶çš„äººå½¢äº¤äº’åº”ç”¨ä¸­ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰çš„æƒ…æ„Ÿæ„ŸçŸ¥æ–¹æ³•ä¾èµ–äºå•ä¸€æ¨¡æ€ï¼ˆéŸ³é¢‘æˆ–å›¾åƒï¼‰è¿›è¡Œæƒ…æ„ŸåµŒå…¥ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬æ•æ‰å¾®å¦™æƒ…æ„Ÿçº¿ç´¢çš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œå¤§å¤šæ•°æ–¹æ³•éƒ½ä¾èµ–äºå•å¼ å‚è€ƒå›¾åƒï¼Œè¿™é™åˆ¶äº†æ¨¡å‹åœ¨åŠ¨ä½œæˆ–å±æ€§éšæ—¶é—´å˜åŒ–æ—¶çš„è¡¨ç¤ºèƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†SynchroRaMaè¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œå®ƒé€šè¿‡ç»“åˆæ–‡æœ¬ï¼ˆé€šè¿‡æƒ…æ„Ÿåˆ†æï¼‰å’ŒéŸ³é¢‘ï¼ˆé€šè¿‡åŸºäºè¯­éŸ³çš„æƒ…æ„Ÿè¯†åˆ«å’ŒéŸ³é¢‘è¡ç”Ÿçš„æ•ˆä»·å”¤èµ·ç‰¹å¾ï¼‰çš„å¤šæ¨¡æ€æƒ…æ„ŸåµŒå…¥ï¼Œä½¿ç”Ÿæˆå…·æœ‰æ›´ä¸°å¯Œã€æ›´çœŸå®æƒ…æ„Ÿè¡¨ç°åŠ›å’Œä¿çœŸåº¦çš„è¯´è¯äººè„¸è§†é¢‘æˆä¸ºå¯èƒ½ã€‚ä¸ºç¡®ä¿è‡ªç„¶å¤´éƒ¨è¿åŠ¨å’Œå‡†ç¡®çš„å”‡éƒ¨åŒæ­¥ï¼ŒSynchroRaMaåŒ…å«ä¸€ä¸ªéŸ³é¢‘åˆ°è¿åŠ¨ï¼ˆA2Mï¼‰æ¨¡å—ï¼Œç”¨äºç”Ÿæˆä¸è¾“å…¥éŸ³é¢‘å¯¹é½çš„è¿åŠ¨å¸§ã€‚æœ€åï¼ŒSynchroRaMaè¿˜ç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆçš„åœºæ™¯æè¿°ä½œä¸ºé¢å¤–çš„æ–‡æœ¬è¾“å…¥ï¼Œä½¿å…¶èƒ½å¤Ÿæ•æ‰åŠ¨æ€åŠ¨ä½œå’Œé«˜çº§è¯­ä¹‰å±æ€§ã€‚ä»¥è§†è§‰å’Œæ–‡æœ¬çº¿ç´¢è®­ç»ƒæ¨¡å‹ï¼Œæé«˜äº†æ—¶é—´ä¸€è‡´æ€§å’Œè§†è§‰çœŸå®æ€§ã€‚åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å®šé‡å’Œå®šæ€§å®éªŒè¡¨æ˜ï¼ŒSynchroRaMaä¼˜äºæœ€æ–°æŠ€æœ¯ï¼Œåœ¨å›¾åƒè´¨é‡ã€è¡¨æƒ…ä¿æŒå’Œè¿åŠ¨çœŸå®æ€§æ–¹é¢å–å¾—äº†æ”¹è¿›ã€‚ç”¨æˆ·ç ”ç©¶è¿›ä¸€æ­¥è¯å®ï¼Œåœ¨æ•´ä½“è‡ªç„¶æ€§ã€è¿åŠ¨å¤šæ ·æ€§å’Œè§†é¢‘å¹³æ»‘åº¦æ–¹é¢ï¼ŒSynchroRaMaç›¸è¾ƒäºå…¶ä»–æ–¹æ³•è·å¾—äº†æ›´é«˜çš„ä¸»è§‚è¯„åˆ†ã€‚æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢å¯åœ¨<a target="_blank" rel="noopener" href="https://novicemm.github.io/synchrorama">https://novicemm.github.io/synchrorama</a>æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.19965v1">PDF</a> Accepted at WACV 2026, project page :   <a target="_blank" rel="noopener" href="https://novicemm.github.io/synchrorama">https://novicemm.github.io/synchrorama</a></p>
<p><strong>Summary</strong><br>æœ¬æ–‡ä»‹ç»äº†SynchroRaMaæ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†æ–‡æœ¬å’ŒéŸ³é¢‘çš„å¤šæ¨¡æ€æƒ…æ„ŸåµŒå…¥ï¼Œç”¨äºç”Ÿæˆå…·æœ‰æ›´ä¸°å¯Œå’Œæ›´çœŸå®æƒ…æ„Ÿè¡¨è¾¾åŠ›çš„åŠ¨æ€äººè„¸è§†é¢‘ã€‚é€šè¿‡æƒ…æ„Ÿæ–‡æœ¬åˆ†æã€è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«å’ŒéŸ³é¢‘è¡ç”Ÿæƒ…æ„Ÿç‰¹å¾ç­‰æŠ€æœ¯ï¼ŒSynchroRaMaå®ç°äº†æ›´ç²¾ç»†çš„æƒ…æ„Ÿæ•æ‰ã€‚åŒæ—¶ï¼Œå®ƒè¿˜åŒ…æ‹¬éŸ³é¢‘åˆ°åŠ¨ä½œï¼ˆA2Mï¼‰æ¨¡å—ï¼Œç¡®ä¿å¤´éƒ¨åŠ¨ä½œçš„è‡ªç„¶æ€§å’Œå”‡éƒ¨çš„ç²¾ç¡®åŒæ­¥ã€‚æ­¤å¤–ï¼ŒSynchroRaMaè¿˜ç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„åœºæ™¯æè¿°ä½œä¸ºé¢å¤–æ–‡æœ¬è¾“å…¥ï¼Œæ•æ‰åŠ¨æ€åŠ¨ä½œå’Œé«˜å±‚æ¬¡è¯­ä¹‰å±æ€§ã€‚åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å®šé‡å’Œå®šæ€§å®éªŒè¡¨æ˜ï¼ŒSynchroRaMaä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œåœ¨å›¾åƒè´¨é‡ã€è¡¨æƒ…ä¿æŒå’Œè¿åŠ¨çœŸå®æ€§æ–¹é¢æœ‰æ‰€æ”¹å–„ã€‚ç”¨æˆ·ç ”ç©¶è¿›ä¸€æ­¥è¯å®ï¼ŒSynchroRaMaåœ¨æ•´ä½“è‡ªç„¶æ€§ã€è¿åŠ¨å¤šæ ·æ€§å’Œè§†é¢‘æµç•…æ€§æ–¹é¢è·å¾—æ›´é«˜çš„ä¸»è§‚è¯„åˆ†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SynchroRaMaç»“åˆäº†æ–‡æœ¬å’ŒéŸ³é¢‘çš„å¤šæ¨¡æ€æƒ…æ„ŸåµŒå…¥ï¼Œå¢å¼ºäº†åŠ¨æ€äººè„¸è§†é¢‘çš„æƒ…æ„Ÿè¡¨è¾¾åŠ›ã€‚</li>
<li>é€šè¿‡æƒ…æ„Ÿæ–‡æœ¬åˆ†æã€è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«å’ŒéŸ³é¢‘è¡ç”Ÿæƒ…æ„Ÿç‰¹å¾ç­‰æŠ€æœ¯ï¼Œå®ç°äº†æ›´ç²¾ç»†çš„æƒ…æ„Ÿæ•æ‰ã€‚</li>
<li>SynchroRaMaåŒ…æ‹¬éŸ³é¢‘åˆ°åŠ¨ä½œï¼ˆA2Mï¼‰æ¨¡å—ï¼Œç¡®ä¿å¤´éƒ¨åŠ¨ä½œçš„è‡ªç„¶æ€§å’Œå”‡éƒ¨çš„ç²¾ç¡®åŒæ­¥ã€‚</li>
<li>ç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„åœºæ™¯æè¿°ä½œä¸ºé¢å¤–æ–‡æœ¬è¾“å…¥ï¼Œæ•æ‰åŠ¨æ€åŠ¨ä½œå’Œé«˜å±‚æ¬¡è¯­ä¹‰å±æ€§ã€‚</li>
<li>SynchroRaMaä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œåœ¨å›¾åƒè´¨é‡ã€è¡¨æƒ…ä¿æŒå’Œè¿åŠ¨çœŸå®æ€§æ–¹é¢æœ‰æ‰€æ”¹å–„ã€‚</li>
<li>ç”¨æˆ·ç ”ç©¶è¯å®ï¼ŒSynchroRaMaåœ¨æ•´ä½“è‡ªç„¶æ€§ã€è¿åŠ¨å¤šæ ·æ€§å’Œè§†é¢‘æµç•…æ€§æ–¹é¢è·å¾—æ›´é«˜çš„ä¸»è§‚è¯„åˆ†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.19965">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Speech/2509.19965v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Speech/2509.19965v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Speech/2509.19965v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="WEST-LLM-based-Speech-Toolkit-for-Speech-Understanding-Generation-and-Interaction"><a href="#WEST-LLM-based-Speech-Toolkit-for-Speech-Understanding-Generation-and-Interaction" class="headerlink" title="WEST: LLM based Speech Toolkit for Speech Understanding, Generation, and   Interaction"></a>WEST: LLM based Speech Toolkit for Speech Understanding, Generation, and   Interaction</h2><p><strong>Authors:Binbin Zhang, Chengdong Liang, Shuai Wang, Xuelong Geng, Zhao Guo, Haoyu Li, Hao Yin, Xipeng Yang, Pengshen Zhang, Changwei Ma, Lei Xie</strong></p>
<p>In this paper, we present WEST(WE Speech Toolkit), a speech toolkit based on a large language model (LLM) for speech understanding, generation, and interaction. There are three key features of WEST: 1) Fully LLM-based: Standing on the shoulders of giants by reusing mature architectures, ecosystems (e.g., Hugging Face), and methods (e.g., sequence packing) from large models. 2) Full-stack: Supports tasks such as recognition, synthesis, understanding, dialogue, and multimodal capabilities, with extensibility to incorporate open-source models. 3) Simple and Stupid: A simple and stupid speech toolkit that everyone can Touch. In addition, WEST provides two types of recipes, models, and experimental results. The first is entirely based on open-source models and open-source data, allowing users to fully reproduce the experiments in this paper and serving as a verification system or minimal system baseline. The second is trained on massive data, offering superior performance so the user can directly apply it out of the box. WEST is publicly avilable at <a target="_blank" rel="noopener" href="https://github.com/wenet-e2e/west/">https://github.com/wenet-e2e/west/</a> </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†WESTï¼ˆWEè¯­éŸ³å·¥å…·åŒ…ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¯­éŸ³å·¥å…·åŒ…ï¼Œç”¨äºè¯­éŸ³ç†è§£ã€ç”Ÿæˆå’Œäº¤äº’ã€‚WESTæœ‰ä¸‰ä¸ªå…³é”®ç‰¹ç‚¹ï¼š1ï¼‰å®Œå…¨åŸºäºLLMï¼šåˆ©ç”¨å¤§å‹æ¨¡å‹çš„æˆç†Ÿæ¶æ„ã€ç”Ÿæ€ç³»ç»Ÿï¼ˆä¾‹å¦‚Hugging Faceï¼‰å’Œæ–¹æ³•ï¼ˆä¾‹å¦‚åºåˆ—æ‰“åŒ…ï¼‰ç«™åœ¨å·¨äººçš„è‚©è†€ä¸Šã€‚2ï¼‰å…¨æ ˆæ”¯æŒï¼šæ”¯æŒè¯†åˆ«ã€åˆæˆã€ç†è§£ã€å¯¹è¯å’Œå¤šæ¨¡å¼åŠŸèƒ½ç­‰ä»»åŠ¡ï¼Œå¯æ‰©å±•ä»¥çº³å…¥å¼€æºæ¨¡å‹ã€‚3ï¼‰ç®€å•æ˜“æ‡‚ï¼šä¸€ä¸ªç®€å•æ˜äº†çš„è¯­éŸ³å·¥å…·åŒ…ï¼Œæ¯ä¸ªäººéƒ½èƒ½è½»æ¾ä¸Šæ‰‹ã€‚æ­¤å¤–ï¼ŒWESTæä¾›ä¸¤ç§ç±»å‹çš„æ¨¡å‹ã€é£Ÿè°±å’Œå®éªŒç»“æœã€‚ç¬¬ä¸€ç§å®Œå…¨åŸºäºå¼€æºæ¨¡å‹å’Œå¼€æºæ•°æ®ï¼Œå…è®¸ç”¨æˆ·å……åˆ†é‡ç°æœ¬æ–‡ä¸­çš„å®éªŒï¼Œå¹¶ä½œä¸ºéªŒè¯ç³»ç»Ÿæˆ–æœ€å°ç³»ç»ŸåŸºçº¿ã€‚ç¬¬äºŒç§æ˜¯åœ¨å¤§é‡æ•°æ®ä¸Šè®­ç»ƒçš„ï¼Œæä¾›å“è¶Šæ€§èƒ½ï¼Œç”¨æˆ·å¯ä»¥ç›´æ¥å¼€ç®±å³ç”¨ã€‚WESTåœ¨<a target="_blank" rel="noopener" href="https://github.com/wenet-e2e/west/%E5%85%AC%E5%BC%80%E5%8F%AF%E7%94%A8%E3%80%82">https://github.com/wenet-e2e/west/å…¬å¼€å¯ç”¨ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.19902v1">PDF</a> </p>
<p><strong>Summary</strong><br>æ–‡æœ¬ä»‹ç»äº†WESTï¼ˆWE Speech Toolkitï¼‰ï¼Œä¸€ä¸ªåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¯­éŸ³è¯†åˆ«ã€ç”Ÿæˆå’Œäº¤äº’å·¥å…·åŒ…ã€‚WESTå…·æœ‰ä¸‰å¤§ç‰¹ç‚¹ï¼šå®Œå…¨åŸºäºLLMã€å…¨æ ˆæ”¯æŒä»¥åŠç®€å•æ˜“ç”¨ã€‚æ­¤å¤–ï¼ŒWESTæä¾›ä¸¤ç§ç±»å‹çš„æ¨¡å‹ä¸å®éªŒç»“æœï¼Œä¸€ç§å®Œå…¨åŸºäºå¼€æºæ¨¡å‹å’Œå¼€æºæ•°æ®ï¼Œç”¨äºéªŒè¯æˆ–ä½œä¸ºæœ€å°ç³»ç»ŸåŸºçº¿ï¼›å¦ä¸€ç§ç»è¿‡å¤§é‡æ•°æ®è®­ç»ƒï¼Œæä¾›å“è¶Šæ€§èƒ½ï¼Œå¯ç›´æ¥åº”ç”¨ã€‚WESTå·²å…¬å¼€å¯ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>WESTæ˜¯ä¸€ä¸ªåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„è¯­éŸ³å·¥å…·åŒ…ï¼Œç”¨äºè¯­éŸ³ç†è§£ã€ç”Ÿæˆå’Œäº¤äº’ã€‚</li>
<li>WESTå…·æœ‰ä¸‰å¤§ç‰¹ç‚¹ï¼šå®Œå…¨åŸºäºLLMã€å…¨æ ˆæ”¯æŒå’Œç®€å•æ˜“ç”¨ã€‚</li>
<li>WESTæä¾›ä¸¤ç§ç±»å‹çš„æ¨¡å‹å’Œå®éªŒç»“æœï¼Œåˆ†åˆ«åŸºäºå¼€æºæ¨¡å‹å’Œå¤§è§„æ¨¡æ•°æ®è®­ç»ƒã€‚</li>
<li>å¼€æºæ¨¡å‹ä¸æ•°æ®å…è®¸ç”¨æˆ·å®Œå…¨é‡ç°å®éªŒï¼Œå¯ä½œä¸ºéªŒè¯ç³»ç»Ÿæˆ–æœ€å°ç³»ç»ŸåŸºçº¿ã€‚</li>
<li>ç»è¿‡å¤§è§„æ¨¡æ•°æ®è®­ç»ƒçš„æ¨¡å‹æä¾›å“è¶Šæ€§èƒ½ï¼Œå¯ç›´æ¥åº”ç”¨ã€‚</li>
<li>WESTå…·æœ‰å¯æ‰©å±•æ€§ï¼Œå¯èå…¥å¼€æºæ¨¡å‹ã€‚</li>
<li>WESTæ”¯æŒå¤šç§ä»»åŠ¡ï¼Œå¦‚è¯†åˆ«ã€åˆæˆã€ç†è§£ã€å¯¹è¯å’Œå¤šæ¨¡å¼åŠŸèƒ½ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.19902">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Speech/2509.19902v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Speech/2509.19902v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Speech/2509.19902v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Speech/2509.19902v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Speech/2509.19902v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="MAGE-A-Coarse-to-Fine-Speech-Enhancer-with-Masked-Generative-Model"><a href="#MAGE-A-Coarse-to-Fine-Speech-Enhancer-with-Masked-Generative-Model" class="headerlink" title="MAGE: A Coarse-to-Fine Speech Enhancer with Masked Generative Model"></a>MAGE: A Coarse-to-Fine Speech Enhancer with Masked Generative Model</h2><p><strong>Authors:The Hieu Pham, Tan Dat Nguyen, Phuong Thanh Tran, Joon Son Chung, Duc Dung Nguyen</strong></p>
<p>Speech enhancement remains challenging due to the trade-off between efficiency and perceptual quality. In this paper, we introduce MAGE, a Masked Audio Generative Enhancer that advances generative speech enhancement through a compact and robust design. Unlike prior masked generative models with random masking, MAGE employs a scarcity-aware coarse-to-fine masking strategy that prioritizes frequent tokens in early steps and rare tokens in later refinements, improving efficiency and generalization. We also propose a lightweight corrector module that further stabilizes inference by detecting low-confidence predictions and re-masking them for refinement. Built on BigCodec and finetuned from Qwen2.5-0.5B, MAGE is reduced to 200M parameters through selective layer retention. Experiments on DNS Challenge and noisy LibriSpeech show that MAGE achieves state-of-the-art perceptual quality and significantly reduces word error rate for downstream recognition, outperforming larger baselines. Audio examples are available at <a target="_blank" rel="noopener" href="https://hieugiaosu.github.io/MAGE/">https://hieugiaosu.github.io/MAGE/</a>. </p>
<blockquote>
<p>è¯­éŸ³å¢å¼ºå› æ•ˆç‡å’Œæ„ŸçŸ¥è´¨é‡ä¹‹é—´çš„æƒè¡¡è€Œä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†MAGEï¼Œè¿™æ˜¯ä¸€ç§å¸¦æœ‰é®è”½éŸ³é¢‘ç”Ÿæˆå¢å¼ºå™¨ï¼ˆMasked Audio Generative Enhancerï¼‰çš„æŠ€æœ¯ï¼Œå®ƒé€šè¿‡ç´§å‡‘è€Œç¨³å¥çš„è®¾è®¡æ¨åŠ¨äº†ç”Ÿæˆå¼è¯­éŸ³å¢å¼ºçš„å‘å±•ã€‚ä¸å…ˆå‰é‡‡ç”¨éšæœºé®è”½çš„é®è”½ç”Ÿæˆæ¨¡å‹ä¸åŒï¼ŒMAGEé‡‡ç”¨äº†ä¸€ç§åŒ®ä¹æ„ŸçŸ¥çš„ç”±ç²—åˆ°ç»†çš„é®è”½ç­–ç•¥ï¼Œè¯¥ç­–ç•¥ä¼˜å…ˆå¤„ç†æ—©æœŸçš„é¢‘ç¹ä»¤ç‰Œï¼Œå¹¶åœ¨åæœŸçš„æ”¹è¿›ä¸­å¤„ç†ç½•è§çš„ä»¤ç‰Œï¼Œä»è€Œæé«˜äº†æ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§è½»é‡çº§çš„æ ¡æ­£æ¨¡å—ï¼Œé€šè¿‡æ£€æµ‹ä½ç½®ä¿¡åº¦çš„é¢„æµ‹å¹¶å¯¹å…¶è¿›è¡Œé‡æ–°é®è”½ä»¥è¿›è¡Œæ”¹è¿›ï¼Œä»è€Œè¿›ä¸€æ­¥ç¨³å®šäº†æ¨æ–­ã€‚MAGEå»ºç«‹åœ¨BigCodecä¹‹ä¸Šï¼Œå¹¶ä»¥Qwen2.5-0.5Bè¿›è¡Œå¾®è°ƒï¼Œé€šè¿‡é€‰æ‹©æ€§å±‚ä¿ç•™å‡å°‘åˆ°200Må‚æ•°ã€‚åœ¨DNS Challengeå’Œå˜ˆæ‚çš„LibriSpeechä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMAGEè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ„ŸçŸ¥è´¨é‡ï¼Œå¹¶æ˜¾è‘—é™ä½äº†ä¸‹æ¸¸è¯†åˆ«çš„è¯é”™è¯¯ç‡ï¼Œè¶…è¶Šäº†è¾ƒå¤§çš„åŸºå‡†æ¨¡å‹ã€‚éŸ³é¢‘ç¤ºä¾‹å¯åœ¨<a target="_blank" rel="noopener" href="https://hieugiaosu.github.io/MAGE/%E6%89%BE%E5%88%B0%E3%80%82">https://hieugiaosu.github.io/MAGE/æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.19881v2">PDF</a> Submitted to ICASSP 2026</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºMAGEçš„éŸ³é¢‘ç”Ÿæˆå¢å¼ºå™¨ï¼Œå®ƒé‡‡ç”¨ç´§å‡‘ä¸”ç¨³å¥çš„è®¾è®¡ï¼Œæ¨è¿›äº†ç”Ÿæˆå¼è¯­éŸ³å¢å¼ºçš„ç ”ç©¶ã€‚ä¸åŒäºä»¥å¾€çš„éšæœºæ©æ¨¡ç”Ÿæˆæ¨¡å‹ï¼ŒMAGEé‡‡ç”¨äº†ä¸€ç§ç¨€ç¼ºæ„ŸçŸ¥çš„ç²—åˆ°ç»†æ©æ¨¡ç­–ç•¥ï¼Œæé«˜äº†æ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†ä¸€ç§è½»é‡çº§çš„æ ¡æ­£æ¨¡å—ï¼Œé€šè¿‡æ£€æµ‹ä½ç½®ä¿¡åº¦é¢„æµ‹å¹¶è¿›è¡Œé‡æ–°æ©æ¨¡ä»¥è¿›ä¸€æ­¥ç¨³å®šæ¨æ–­ã€‚å®éªŒè¡¨æ˜ï¼ŒMAGEåœ¨DNSæŒ‘æˆ˜å’Œå˜ˆæ‚çš„LibriSpeechä¸Šå®ç°äº†æœ€ä½³æ„ŸçŸ¥è´¨é‡å’Œæ˜¾è‘—å‡å°‘çš„è¯é”™è¯¯ç‡ï¼Œå¹¶åœ¨ä¸‹æ¸¸è¯†åˆ«ä»»åŠ¡ä¸Šä¼˜äºå¤§å‹åŸºçº¿æ¨¡å‹ã€‚æ›´å¤šéŸ³é¢‘ç¤ºä¾‹å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://hieugiaosu.github.io/MAGE/%E8%AE%BF%E9%97%AE%E3%80%82">https://hieugiaosu.github.io/MAGE/è®¿é—®ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MAGEæ˜¯ä¸€ç§æ–°å‹çš„éŸ³é¢‘ç”Ÿæˆå¢å¼ºå™¨ï¼Œå…·æœ‰ç´§å‡‘å’Œç¨³å¥çš„è®¾è®¡ã€‚</li>
<li>ä¸å…¶ä»–éšæœºæ©æ¨¡ç”Ÿæˆæ¨¡å‹ä¸åŒï¼ŒMAGEé‡‡ç”¨äº†ä¸€ç§ç¨€ç¼ºæ„ŸçŸ¥çš„ç²—åˆ°ç»†æ©æ¨¡ç­–ç•¥ã€‚</li>
<li>MAGEåœ¨æ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›ä¸Šæœ‰æ‰€æå‡ã€‚</li>
<li>MAGEåŒ…å«ä¸€ä¸ªè½»é‡çº§çš„æ ¡æ­£æ¨¡å—ï¼Œç”¨äºæ£€æµ‹å¹¶é‡æ–°æ©æ¨¡ä½ç½®ä¿¡åº¦é¢„æµ‹ï¼Œä»è€Œç¨³å®šæ¨æ–­è¿‡ç¨‹ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒMAGEåœ¨DNSæŒ‘æˆ˜å’ŒLibriSpeechæ•°æ®é›†ä¸Šå®ç°äº†æœ€ä½³æ„ŸçŸ¥è´¨é‡å’Œè¯é”™è¯¯ç‡é™ä½ã€‚</li>
<li>MAGEä½¿ç”¨BigCodecæ„å»ºå¹¶ç»è¿‡Qwen2.5-0.5Bå¾®è°ƒï¼Œæœ€åé€šè¿‡é€‰æ‹©æ€§å±‚ä¿ç•™å‡å°‘è‡³200Må‚æ•°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.19881">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Speech/2509.19881v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Speech/2509.19881v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Speech/2509.19881v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Speech/2509.19881v2/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Speech/2509.19881v2/page_3_0.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="MMedFD-A-Real-world-Healthcare-Benchmark-for-Multi-turn-Full-Duplex-Automatic-Speech-Recognition"><a href="#MMedFD-A-Real-world-Healthcare-Benchmark-for-Multi-turn-Full-Duplex-Automatic-Speech-Recognition" class="headerlink" title="MMedFD: A Real-world Healthcare Benchmark for Multi-turn Full-Duplex   Automatic Speech Recognition"></a>MMedFD: A Real-world Healthcare Benchmark for Multi-turn Full-Duplex   Automatic Speech Recognition</h2><p><strong>Authors:Hongzhao Chen, XiaoYang Wang, Jing Lan, Hexiao Ding, Yufeng Jiang MingHui Yang, DanHui Xu, Jun Luo, Nga-Chun Ng, Gerald W. Y. Cheng, Yunlin Mao, Jung Sun Yoo</strong></p>
<p>Automatic speech recognition (ASR) in clinical dialogue demands robustness to full-duplex interaction, speaker overlap, and low-latency constraints, yet open benchmarks remain scarce. We present MMedFD, the first real-world Chinese healthcare ASR corpus designed for multi-turn, full-duplex settings. Captured from a deployed AI assistant, the dataset comprises 5,805 annotated sessions with synchronized user and mixed-channel views, RTTM&#x2F;CTM timing, and role labels. We introduce a model-agnostic pipeline for streaming segmentation, speaker attribution, and dialogue memory, and fine-tune Whisper-small on role-concatenated audio for long-context recognition. ASR evaluation includes WER, CER, and HC-WER, which measures concept-level accuracy across healthcare settings. LLM-generated responses are assessed using rubric-based and pairwise protocols. MMedFD establishes a reproducible framework for benchmarking streaming ASR and end-to-end duplex agents in healthcare deployment. The dataset and related resources are publicly available at <a target="_blank" rel="noopener" href="https://github.com/Kinetics-JOJO/MMedFD">https://github.com/Kinetics-JOJO/MMedFD</a> </p>
<blockquote>
<p>è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰åœ¨ä¸´åºŠå¯¹è¯ä¸­éœ€è¦åº”å¯¹å…¨åŒå·¥äº¤äº’ã€è¯´è¯äººé‡å å’Œä½å»¶è¿Ÿçº¦æŸçš„ç¨³å¥æ€§æŒ‘æˆ˜ï¼Œç„¶è€Œå¼€æ”¾çš„æ ‡å‡†æµ‹è¯•é›†ä»ç„¶ç¨€ç¼ºã€‚æˆ‘ä»¬æ¨å‡ºäº†MMedFDï¼Œè¿™æ˜¯é¦–ä¸ªä¸ºå¤šè½®ã€å…¨åŒå·¥ç¯å¢ƒè®¾è®¡çš„ç°å®ä¸–ç•Œä¸­æ–‡åŒ»ç–—ASRè¯­æ–™åº“ã€‚è¯¥æ•°æ®é›†æ¥è‡ªéƒ¨ç½²çš„AIåŠ©ç†çš„æ•æ‰ï¼ŒåŒ…å«5805ä¸ªå¸¦æ³¨é‡Šçš„ä¼šè¯ï¼Œå…·æœ‰åŒæ­¥çš„ç”¨æˆ·å’Œæ··åˆé€šé“è§†å›¾ã€RTTM&#x2F;CTMå®šæ—¶å’Œè§’è‰²æ ‡ç­¾ã€‚æˆ‘ä»¬ä»‹ç»äº†ä¸€ä¸ªæ¨¡å‹æ— å…³çš„ç®¡é“ï¼Œç”¨äºæµå¼åˆ†å‰²ã€è¯´è¯äººå½’å±å’Œå¯¹è¯è®°å¿†ï¼Œå¹¶å¯¹ç”¨äºé•¿ä¸Šä¸‹æ–‡è¯†åˆ«çš„è§’è‰²è¿æ¥éŸ³é¢‘è¿›è¡Œå¾®è°ƒWhisper-smallã€‚ASRè¯„ä¼°åŒ…æ‹¬WERã€CERå’ŒHC-WERï¼Œåè€…è¡¡é‡åŒ»ç–—ä¿å¥ç¯å¢ƒä¸­çš„æ¦‚å¿µçº§å‡†ç¡®æ€§ã€‚å¯¹äºå¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„å“åº”ï¼Œæˆ‘ä»¬é‡‡ç”¨åŸºäºè¯„åˆ†æ ‡å‡†å’Œæˆå¯¹åè®®è¿›è¡Œè¯„ä¼°ã€‚MMedFDä¸ºåŒ»ç–—éƒ¨ç½²ä¸­çš„æµå¼ASRå’Œç«¯åˆ°ç«¯åŒå·¥ä»£ç†å»ºç«‹äº†å¯é‡å¤ä½¿ç”¨çš„åŸºå‡†æµ‹è¯•æ¡†æ¶ã€‚æ•°æ®é›†å’Œç›¸å…³èµ„æºå¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Kinetics-JOJO/MMedFD%E5%85%AC%E5%BC%80%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/Kinetics-JOJO/MMedFDå…¬å¼€è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.19817v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>é’ˆå¯¹ä¸´åºŠå¯¹è¯ä¸­çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿéœ€è¦åº”å¯¹å…¨åŒå·¥äº¤äº’ã€è¯´è¯äººé‡å å’Œä½å»¶è¿Ÿçº¦æŸçš„æŒ‘æˆ˜ï¼Œä½†ç°æœ‰çš„å¼€æ”¾åŸºå‡†æµ‹è¯•ä»ç„¶ç¼ºä¹ã€‚æˆ‘ä»¬æ¨å‡ºäº†MMedFDï¼Œé¦–ä¸ªä¸ºå¤šè½®ã€å…¨åŒå·¥è®¾ç½®è®¾è®¡çš„ç°å®ä¸–ç•Œä¸­ç”¨äºä¸­æ–‡åŒ»ç–—ASRè¯­æ–™åº“ã€‚è¯¥æ•°æ®é›†ä»éƒ¨ç½²çš„AIåŠ©ç†ä¸­æå–ï¼ŒåŒ…å«5805ä¸ªå·²æ ‡æ³¨çš„ä¼šè¯ï¼Œå…·æœ‰ç”¨æˆ·åŒæ­¥å’Œæ··åˆé€šé“è§†å›¾ã€RTTM&#x2F;CTMå®šæ—¶å’Œè§’è‰²æ ‡ç­¾ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ¨¡å‹æ— å…³çš„ç®¡é“ï¼Œç”¨äºæµå¼åˆ†å‰²ã€è¯´è¯äººå½’å±å’Œå¯¹è¯è®°å¿†ï¼Œå¹¶å¯¹è§’è‰²è¿æ¥çš„éŸ³é¢‘å¾®è°ƒWhisper-smallè¿›è¡Œé•¿ä¸Šä¸‹æ–‡è¯†åˆ«ã€‚ASRè¯„ä¼°åŒ…æ‹¬è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ã€å­—ç¬¦é”™è¯¯ç‡ï¼ˆCERï¼‰å’ŒåŒ»ç–—ä¿å¥ç¯å¢ƒä¸­çš„æ¦‚å¿µçº§å‡†ç¡®åº¦ï¼ˆHC-WERï¼‰ã€‚ä½¿ç”¨åŸºäºè¯„åˆ†æ ‡å‡†å’Œé…å¯¹åè®®çš„LLMç”Ÿæˆå“åº”è¿›è¡Œè¯„ä¼°ã€‚MMedFDä¸ºåŒ»ç–—éƒ¨ç½²ä¸­çš„æµå¼ASRå’Œç«¯åˆ°ç«¯åŒå·¥ä»£ç†æä¾›äº†å¯å¤åˆ¶çš„åŸºå‡†æµ‹è¯•æ¡†æ¶ã€‚æ•°æ®é›†å’Œç›¸å…³èµ„æºå¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Kinetics-JOJO/MMedFD%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/Kinetics-JOJO/MMedFDå…¬å¼€è®¿é—®ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MMedFDæ˜¯é¦–ä¸ªé’ˆå¯¹å¤šè½®ã€å…¨åŒå·¥è®¾ç½®çš„ç°å®ä¸–ç•Œä¸­ç”¨äºä¸­æ–‡åŒ»ç–—ASRè¯­æ–™åº“ã€‚</li>
<li>æ•°æ®é›†æ•è·è‡ªéƒ¨ç½²çš„AIåŠ©ç†ï¼ŒåŒ…å«ä¸°å¯Œå¤šæ ·çš„åŒ»ç–—å¯¹è¯åœºæ™¯ã€‚</li>
<li>å¼•å…¥æ¨¡å‹æ— å…³çš„ç®¡é“ï¼Œç”¨äºæµå¼åˆ†å‰²ã€è¯´è¯äººå½’å±å’Œå¯¹è¯è®°å¿†å¤„ç†ã€‚</li>
<li>å¯¹ASRç³»ç»Ÿè¯„ä¼°åŒ…æ‹¬è¯é”™è¯¯ç‡ã€å­—ç¬¦é”™è¯¯ç‡å’Œæ¦‚å¿µçº§å‡†ç¡®åº¦ã€‚</li>
<li>é‡‡ç”¨LLMç”Ÿæˆçš„å“åº”è¿›è¡Œè¯„ä¼°ï¼Œç¡®ä¿ç³»ç»Ÿçš„å®é™…åº”ç”¨æ•ˆæœã€‚</li>
<li>MMedFDæä¾›äº†å¯å¤åˆ¶çš„åŸºå‡†æµ‹è¯•æ¡†æ¶ï¼Œä¾¿äºåŒ»ç–—éƒ¨ç½²ä¸­çš„æµå¼ASRå’Œç«¯åˆ°ç«¯åŒå·¥ä»£ç†çš„è¯„ä¼°ã€‚</li>
<li>æ•°æ®é›†åŠç›¸å…³èµ„æºå·²å…¬å¼€ï¼Œä¾¿äºç ”ç©¶å’Œåº”ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.19817">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Speech/2509.19817v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Speech/2509.19817v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Speech/2509.19817v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Speech/2509.19817v1/page_3_1.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Selective-Classifier-free-Guidance-for-Zero-shot-Text-to-speech"><a href="#Selective-Classifier-free-Guidance-for-Zero-shot-Text-to-speech" class="headerlink" title="Selective Classifier-free Guidance for Zero-shot Text-to-speech"></a>Selective Classifier-free Guidance for Zero-shot Text-to-speech</h2><p><strong>Authors:John Zheng, Farhad Maleki</strong></p>
<p>In zero-shot text-to-speech, achieving a balance between fidelity to the target speaker and adherence to text content remains a challenge. While classifier-free guidance (CFG) strategies have shown promising results in image generation, their application to speech synthesis are underexplored. Separating the conditions used for CFG enables trade-offs between different desired characteristics in speech synthesis. In this paper, we evaluate the adaptability of CFG strategies originally developed for image generation to speech synthesis and extend separated-condition CFG approaches for this domain. Our results show that CFG strategies effective in image generation generally fail to improve speech synthesis. We also find that we can improve speaker similarity while limiting degradation of text adherence by applying standard CFG during early timesteps and switching to selective CFG only in later timesteps. Surprisingly, we observe that the effectiveness of a selective CFG strategy is highly text-representation dependent, as differences between the two languages of English and Mandarin can lead to different results even with the same model. </p>
<blockquote>
<p>åœ¨é›¶æ ·æœ¬æ–‡æœ¬åˆ°è¯­éŸ³çš„ä»»åŠ¡ä¸­ï¼Œå¦‚ä½•åœ¨å¿ å®äºç›®æ ‡è¯´è¯äººå’Œéµå¾ªæ–‡æœ¬å†…å®¹ä¹‹é—´å–å¾—å¹³è¡¡ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚è™½ç„¶æ— åˆ†ç±»å™¨å¼•å¯¼ï¼ˆCFGï¼‰ç­–ç•¥åœ¨å›¾åƒç”Ÿæˆä¸­å–å¾—äº†æœ‰å‰æ™¯çš„ç»“æœï¼Œä½†å®ƒä»¬åœ¨è¯­éŸ³åˆæˆä¸­çš„åº”ç”¨å´è¢«æ¢ç´¢å¾—å¾ˆå°‘ã€‚ä¸ºCFGä½¿ç”¨çš„æ¡ä»¶åˆ†ç¦»èƒ½å¤Ÿå®ç°è¯­éŸ³åˆæˆä¸­ä¸åŒæœŸæœ›ç‰¹å¾ä¹‹é—´çš„æƒè¡¡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è¯„ä¼°äº†åŸæœ¬ä¸ºå›¾åƒç”Ÿæˆè€Œå¼€å‘çš„CFGç­–ç•¥çš„é€‚åº”æ€§ï¼Œå¹¶å°†å…¶æ‰©å±•åˆ°è¯­éŸ³åˆæˆé¢†åŸŸã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œåœ¨å›¾åƒç”Ÿæˆä¸­æœ‰æ•ˆçš„CFGç­–ç•¥é€šå¸¸æ— æ³•æ”¹å–„è¯­éŸ³åˆæˆã€‚æˆ‘ä»¬è¿˜å‘ç°ï¼Œé€šè¿‡åœ¨æ—©æœŸæ—¶é—´æ­¥é•¿åº”ç”¨æ ‡å‡†CFGï¼Œå¹¶åœ¨åæœŸæ—¶é—´æ­¥é•¿ä»…é€‰æ‹©CFGï¼Œæˆ‘ä»¬å¯ä»¥æé«˜è¯´è¯äººç›¸ä¼¼æ€§ï¼ŒåŒæ—¶é™åˆ¶æ–‡æœ¬è´´åˆåº¦çš„é™ä½ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°é€‰æ‹©æ€§CFGç­–ç•¥çš„æœ‰æ•ˆæ€§é«˜åº¦ä¾èµ–äºæ–‡æœ¬è¡¨ç¤ºï¼Œå› ä¸ºè‹±è¯­å’Œæ™®é€šè¯ä¸¤ç§è¯­è¨€ä¹‹é—´çš„å·®å¼‚å³ä½¿åœ¨åŒä¸€æ¨¡å‹ä¸‹ä¹Ÿä¼šå¯¼è‡´ä¸åŒçš„ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.19668v1">PDF</a> 5 pages, 7 figures, 1 table. Submitted to ICASSP 2026</p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬æ¢è®¨äº†é›¶æ ·æœ¬æ–‡æœ¬åˆ°è¯­éŸ³è½¬æ¢ä¸­çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯å¦‚ä½•åœ¨ä¿æŒç›®æ ‡è¯´è¯äººä¿çœŸåº¦çš„åŒæ—¶éµå¾ªæ–‡æœ¬å†…å®¹çš„é—®é¢˜ã€‚è™½ç„¶æ— åˆ†ç±»å™¨å¼•å¯¼ç­–ç•¥åœ¨å›¾åƒç”Ÿæˆä¸­è¡¨ç°å‡ºæœ‰å‰æ™¯çš„ç»“æœï¼Œä½†å…¶åœ¨è¯­éŸ³åˆæˆä¸­çš„åº”ç”¨ä»è¢«å¿½è§†ã€‚æœ¬ç ”ç©¶è¯„ä¼°äº†åŸæœ¬ä¸ºå›¾åƒç”Ÿæˆè®¾è®¡çš„æ— åˆ†ç±»å™¨å¼•å¯¼ç­–ç•¥çš„é€‚åº”æ€§ï¼Œå¹¶æ‰©å±•äº†é’ˆå¯¹è¯­éŸ³åˆæˆé¢†åŸŸçš„åˆ†ç¦»æ¡ä»¶æ— åˆ†ç±»å™¨å¼•å¯¼æ–¹æ³•ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œåœ¨å›¾åƒç”Ÿæˆä¸­æœ‰æ•ˆçš„æ— åˆ†ç±»å™¨å¼•å¯¼ç­–ç•¥é€šå¸¸æ— æ³•æ”¹å–„è¯­éŸ³åˆæˆã€‚åŒæ—¶å‘ç°ï¼Œé€šè¿‡åœ¨æ—©æœŸæ—¶é—´æ­¥åº”ç”¨æ ‡å‡†æ— åˆ†ç±»å™¨å¼•å¯¼ï¼Œå¹¶åœ¨åæœŸæ—¶é—´æ­¥ä»…é€‰æ‹©æ€§åœ°åº”ç”¨è¯¥ç­–ç•¥ï¼Œå¯ä»¥åœ¨æé«˜è¯´è¯äººç›¸ä¼¼æ€§çš„åŒæ—¶é™åˆ¶æ–‡æœ¬ç²˜é™„æ€§çš„é™ä½ã€‚æœ‰è¶£çš„æ˜¯ï¼Œé€‰æ‹©æ€§æ— åˆ†ç±»å™¨å¼•å¯¼ç­–ç•¥çš„æœ‰æ•ˆæ€§é«˜åº¦ä¾èµ–äºæ–‡æœ¬è¡¨ç¤ºï¼Œå› ä¸ºè‹±è¯­å’Œæ™®é€šè¯ä¸¤ç§è¯­è¨€çš„å·®å¼‚ä¼šå¯¼è‡´å³ä½¿ä½¿ç”¨ç›¸åŒæ¨¡å‹ä¹Ÿä¼šäº§ç”Ÿä¸åŒçš„ç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é›¶æ ·æœ¬æ–‡æœ¬åˆ°è¯­éŸ³è½¬æ¢ä¸­ä¿æŒç›®æ ‡è¯´è¯äººä¿çœŸåº¦å’Œéµå¾ªæ–‡æœ¬å†…å®¹çš„å¹³è¡¡æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚</li>
<li>æ— åˆ†ç±»å™¨å¼•å¯¼ç­–ç•¥åœ¨å›¾åƒç”Ÿæˆä¸­æœ‰å‰æ™¯ï¼Œä½†åœ¨è¯­éŸ³åˆæˆä¸­çš„åº”ç”¨å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚</li>
<li>åˆ†ç¦»æ¡ä»¶çš„æ— åˆ†ç±»å™¨å¼•å¯¼æ–¹æ³•å¯ä»¥æé«˜è¯­éŸ³åˆæˆçš„æ€§èƒ½ã€‚</li>
<li>åœ¨è¯­éŸ³åˆæˆä¸­ï¼Œæ— åˆ†ç±»å™¨å¼•å¯¼ç­–ç•¥å¹¶ä¸æ€»æ˜¯æœ‰æ•ˆï¼Œéœ€è¦é€‚åº”æ€§åœ°åº”ç”¨ã€‚</li>
<li>é€šè¿‡åœ¨æ—©æœŸå’ŒåæœŸæ—¶é—´æ­¥é‡‡ç”¨ä¸åŒçš„æ— åˆ†ç±»å™¨å¼•å¯¼ç­–ç•¥ï¼Œå¯ä»¥åœ¨æé«˜è¯´è¯äººç›¸ä¼¼æ€§çš„åŒæ—¶ä¿æŒæ–‡æœ¬ç²˜é™„æ€§ã€‚</li>
<li>æ–‡æœ¬è¡¨ç¤ºå¯¹æ— åˆ†ç±»å™¨å¼•å¯¼ç­–ç•¥çš„æœ‰æ•ˆæ€§æœ‰é‡è¦å½±å“ï¼Œä¸åŒè¯­è¨€å¯èƒ½å¯¼è‡´ä¸åŒç»“æœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.19668">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Speech/2509.19668v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Speech/2509.19668v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Speech/2509.19668v1/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Speech/2509.19668v1/page_2_2.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Speech/2509.19668v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Advancing-Speech-Summarization-in-Multi-modal-LLMs-with-Reinforcement-Learning"><a href="#Advancing-Speech-Summarization-in-Multi-modal-LLMs-with-Reinforcement-Learning" class="headerlink" title="Advancing Speech Summarization in Multi-modal LLMs with Reinforcement   Learning"></a>Advancing Speech Summarization in Multi-modal LLMs with Reinforcement   Learning</h2><p><strong>Authors:Shaoshi Ling, Gang Liu, Guoli Ye, Jinyu Li</strong></p>
<p>Speech summarization is a critical component of spoken content understanding, particularly in the era of rapidly growing spoken and audiovisual data. Recent advances in multi-modal large language models (MLLMs), leveraging the power of LLMs, enable generating textual summaries directly from speech without intermediate transcriptions, while supporting controllable styles and zero-shot generalization. However, open-source MLLMs continue to lag behind the state-of-the-art text-based LLMs, limiting their practical deployment for speech summarization. In this work, we present a novel multi-stage reinforcement learning training framework to enhance the speech summarization capabilities in MLLMs. Our model delivers substantial improvements over strong baselines, outperforms much larger MLLMs, and significantly narrows the gap with state-of-the-art text-based LLMs. </p>
<blockquote>
<p>è¯­éŸ³è¯†åˆ«æ‘˜è¦ï¼ˆSpeech summarizationï¼‰æ˜¯å£è¯­å†…å®¹ç†è§£çš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼Œç‰¹åˆ«æ˜¯åœ¨å£è¯­å’Œè§†å¬æ•°æ®å¿«é€Ÿå¢é•¿çš„æ—¶ä»£ã€‚è¿‘æœŸåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„è¿›æ­¥ï¼Œèƒ½å¤Ÿç›´æ¥ä»è¯­éŸ³ç”Ÿæˆæ–‡æœ¬æ‘˜è¦ï¼Œæ— éœ€ä¸­é—´è½¬å½•è¿‡ç¨‹ï¼ŒåŒæ—¶æ”¯æŒå¯æ§é£æ ¼å’Œé›¶æ ·æœ¬æ³›åŒ–ã€‚ç„¶è€Œï¼Œå¼€æºçš„MLLMsä»ç„¶è½åäºæœ€æ–°çš„åŸºäºæ–‡æœ¬çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œé™åˆ¶äº†å®ƒä»¬åœ¨è¯­éŸ³æ‘˜è¦ä¸­çš„å®é™…åº”ç”¨ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„å¤šé˜¶æ®µå¼ºåŒ–å­¦ä¹ è®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨æé«˜MLLMsçš„è¯­éŸ³æ‘˜è¦èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨å¼ºå¤§çš„åŸºçº¿æ¨¡å‹ä¸Šå–å¾—äº†å®è´¨æ€§æ”¹è¿›ï¼Œä¼˜äºæ›´å¤§çš„MLLMsï¼Œå¹¶æ˜¾è‘—ç¼©å°äº†ä¸æœ€æ–°åŸºäºæ–‡æœ¬çš„LLMsçš„å·®è·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.19631v1">PDF</a> </p>
<p><strong>Summary</strong><br>è¯­éŸ³æ‘˜è¦æ˜¯å¯¹å£è¯­å†…å®¹ç†è§£çš„æ ¸å¿ƒç»„æˆéƒ¨åˆ†ï¼Œå°¤å…¶åœ¨å£è¯­å’Œè§†å¬æ•°æ®è¿…é€Ÿå¢é•¿çš„æ—¶é‡è¦ä»£ã€‚å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰èƒ½å¤Ÿç›´æ¥ä»è¯­éŸ³ç”Ÿæˆæ–‡æœ¬æ‘˜è¦ï¼Œæ”¯æŒå¯æ§é£æ ¼å’Œé›¶æ ·æœ¬æ³›åŒ–ã€‚ç„¶è€Œï¼Œå¼€æºMLLMsç›¸è¾ƒäºæœ€å…ˆè¿›çš„æ–‡æœ¬åŸºç¡€LLLMä»æ˜¾è½åã€‚æœ¬ç ”ç©¶æå‡ºä¸€ç§æ–°å‹çš„å¤šé˜¶æ®µå¼ºåŒ–å­¦ä¹ è®­ç»ƒæ¡†æ¶ï¼Œä»¥æé«˜MLLMsçš„è¯­éŸ³æ‘˜è¦èƒ½åŠ›ï¼Œæ¨¡å‹ç›¸è¾ƒäºå¼ºå¤§åŸºçº¿æœ‰æ˜¾è‘—æ”¹å–„ï¼Œè¶…è¶Šè§„æ¨¡æ›´å¤§çš„MLLMsï¼Œå¹¶æ˜¾è‘—ç¼©å°ä¸æ–‡æœ¬åŸºç¡€LLLMçš„å·®è·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­éŸ³æ‘˜è¦æ˜¯å£è¯­å†…å®¹ç†è§£çš„é‡è¦ç»„æˆéƒ¨åˆ†ã€‚</li>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å¯ä»¥ç›´æ¥ä»è¯­éŸ³ç”Ÿæˆæ–‡æœ¬æ‘˜è¦ã€‚</li>
<li>MLLMsåœ¨è¯­éŸ³æ‘˜è¦æ–¹é¢ç›¸è¾ƒäºæ–‡æœ¬åŸºç¡€LLMsä»æœ‰å·®è·ã€‚</li>
<li>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§å¤šé˜¶æ®µå¼ºåŒ–å­¦ä¹ è®­ç»ƒæ¡†æ¶æ¥æé«˜MLLMsçš„è¯­éŸ³æ‘˜è¦èƒ½åŠ›ã€‚</li>
<li>æ¨¡å‹è¡¨ç°è¶…è¶Šäº†ä¸€äº›å¼ºå¤§çš„åŸºçº¿ä»¥åŠè§„æ¨¡æ›´å¤§çš„MLLMsã€‚</li>
<li>è¯¥æ¨¡å‹æ˜¾è‘—ç¼©å°äº†ä¸æ–‡æœ¬åŸºç¡€LLLMçš„å·®è·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.19631">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Speech/2509.19631v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Speech/2509.19631v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Speech/2509.19631v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Speech/2509.19631v1/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Speech/2509.19631v1/page_3_2.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="HD-PPT-Hierarchical-Decoding-of-Content-and-Prompt-Preference-Tokens-for-Instruction-based-TTS"><a href="#HD-PPT-Hierarchical-Decoding-of-Content-and-Prompt-Preference-Tokens-for-Instruction-based-TTS" class="headerlink" title="HD-PPT: Hierarchical Decoding of Content- and Prompt-Preference Tokens   for Instruction-based TTS"></a>HD-PPT: Hierarchical Decoding of Content- and Prompt-Preference Tokens   for Instruction-based TTS</h2><p><strong>Authors:Sihang Nie, Xiaofen Xing, Jingyuan Xing, Baiji Liu, Xiangmin Xu</strong></p>
<p>Large Language Model (LLM)-based Text-to-Speech (TTS) models have already reached a high degree of naturalness. However, the precision control of TTS inference is still challenging. Although instruction-based Text-to-Speech (Instruct-TTS) models are proposed, these models still lack fine-grained control due to the modality gap between single-level text instructions and multilevel speech tokens. To address this limitation, we propose HD-PPT, a framework that transforms speech synthesis into a structured, hierarchical task. To enable fine-grained control, we introduce a novel speech codec to extract distinct prompt-preference and content-preference tokens from the complex speech tokens, supervised by automatic speech recognition (ASR) and cross-lingual audio-text pre-training (CLAP) objectives. To bridge the modality gap of these tokens, we propose a hierarchical decoding strategy, where the LLM generates tokens in a structured order: first semantic, then fine-grained style, and finally complete acoustic representation. Extensive experiments demonstrate that this hierarchical paradigm significantly improves instruction adherence and achieves state-of-the-art naturalness, validating our approach for precise and controllable speech synthesis. Audio samples are available at <a target="_blank" rel="noopener" href="https://xxh333.github.io/">https://xxh333.github.io/</a>. </p>
<blockquote>
<p>åŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å‹å·²ç»è¾¾åˆ°äº†å¾ˆé«˜çš„è‡ªç„¶åº¦ã€‚ç„¶è€Œï¼ŒTTSæ¨ç†çš„ç²¾ç¡®æ§åˆ¶ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚è™½ç„¶æå‡ºäº†åŸºäºæŒ‡ä»¤çš„æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆInstruct-TTSï¼‰æ¨¡å‹ï¼Œä½†è¿™äº›æ¨¡å‹ä»ç„¶ç¼ºä¹ç²¾ç»†æ§åˆ¶ï¼ŒåŸå› æ˜¯å•ä¸€å±‚æ¬¡çš„æ–‡æœ¬æŒ‡ä»¤å’Œå¤šå±‚æ¬¡è¯­éŸ³ä»¤ç‰Œä¹‹é—´çš„æ¨¡æ€å·®è·ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†HD-PPTæ¡†æ¶ï¼Œå°†è¯­éŸ³åˆæˆè½¬åŒ–ä¸ºä¸€ä¸ªç»“æ„åŒ–ã€åˆ†å±‚çš„ä»»åŠ¡ã€‚ä¸ºäº†å®ç°ç²¾ç»†æ§åˆ¶ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„è¯­éŸ³ç¼–è§£ç å™¨ï¼Œä»å¤æ‚çš„è¯­éŸ³ä»¤ç‰Œä¸­æå–å‡ºä¸åŒçš„æç¤ºåå¥½å’Œå†…å®¹åå¥½ä»¤ç‰Œï¼Œç”±è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰å’Œè·¨è¯­è¨€éŸ³é¢‘æ–‡æœ¬é¢„è®­ç»ƒï¼ˆCLAPï¼‰ç›®æ ‡è¿›è¡Œç›‘ç£ã€‚ä¸ºäº†å¼¥åˆè¿™äº›ä»¤ç‰Œçš„æ¨¡æ€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ†å±‚è§£ç ç­–ç•¥ï¼ŒLLMä»¥ç»“æ„åŒ–é¡ºåºç”Ÿæˆä»¤ç‰Œï¼šé¦–å…ˆæ˜¯è¯­ä¹‰ï¼Œç„¶åæ˜¯ç²¾ç»†é£æ ¼ï¼Œæœ€åæ˜¯å®Œæ•´çš„å£°å­¦è¡¨ç¤ºã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œè¿™ç§åˆ†å±‚èŒƒå¼æ˜¾è‘—æé«˜äº†æŒ‡ä»¤éµå¾ªèƒ½åŠ›ï¼Œå¹¶å®ç°äº†æœ€å…ˆè¿›çš„è‡ªç„¶åº¦ï¼ŒéªŒè¯äº†æˆ‘ä»¬æ–¹æ³•åœ¨ç²¾ç¡®å¯æ§è¯­éŸ³åˆæˆæ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚éŸ³é¢‘æ ·æœ¬å¯åœ¨<a target="_blank" rel="noopener" href="https://xxh333.github.io/">é“¾æ¥</a>æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.19001v1">PDF</a> 5 pages, 2 figures, submitted to ICASSP2026</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å‹å·²å…·å¤‡é«˜åº¦è‡ªç„¶åº¦ï¼Œä½†TTSæ¨ç†çš„ç²¾å‡†æ§åˆ¶ä»å…·æŒ‘æˆ˜ã€‚ä¸ºç¼©å°æŒ‡ä»¤ä¸å¤šçº§è¯­éŸ³æ ‡è®°é—´çš„æ¨¡æ€å·®è·ï¼Œæœ¬æ–‡æå‡ºHD-PPTæ¡†æ¶ï¼Œå°†è¯­éŸ³åˆæˆè½¬åŒ–ä¸ºç»“æ„åŒ–ã€å±‚æ¬¡åŒ–ä»»åŠ¡ã€‚é€šè¿‡å¼•å…¥æ–°å‹è¯­éŸ³ç¼–è§£ç å™¨ï¼Œä»å¤æ‚è¯­éŸ³æ ‡è®°ä¸­æå–ä¸åŒçš„æç¤ºåå¥½å’Œå†…å®¹åå¥½æ ‡è®°ï¼Œå¹¶å€ŸåŠ©è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰å’Œè·¨è¯­è¨€éŸ³é¢‘æ–‡æœ¬é¢„è®­ç»ƒï¼ˆCLAPï¼‰ç›®æ ‡è¿›è¡Œç›‘ç£ã€‚ä¸ºå¼¥åˆè¿™äº›æ ‡è®°çš„æ¨¡æ€å·®è·ï¼Œæœ¬æ–‡é‡‡å–å±‚æ¬¡è§£ç ç­–ç•¥ï¼Œè®©LLMæŒ‰ç»“æ„åŒ–é¡ºåºç”Ÿæˆæ ‡è®°ï¼šé¦–å…ˆæ˜¯è¯­ä¹‰ï¼Œç„¶åæ˜¯ç²¾ç»†é£æ ¼ï¼Œæœ€åæ˜¯å®Œæ•´çš„å£°éŸ³è¡¨è¾¾ã€‚å®éªŒè¯æ˜ï¼Œè¿™ç§å±‚æ¬¡åŒ–èŒƒå¼æ˜¾è‘—æé«˜äº†æŒ‡ä»¤éµå¾ªåº¦ï¼Œå¹¶å®ç°äº†ä¸šç•Œé¢†å…ˆçš„è‡ªç„¶åº¦ï¼ŒéªŒè¯äº†æˆ‘ä»¬åœ¨ç²¾å‡†å¯æ§è¯­éŸ³åˆæˆæ–¹é¢çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM-based TTSæ¨¡å‹å·²å…·å¤‡é«˜è‡ªç„¶åº¦ï¼Œä½†æ¨ç†æ§åˆ¶ä»å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>æŒ‡ä»¤å¼TTSæ¨¡å‹å› æ¨¡æ€å·®è·é™åˆ¶äº†ç²¾ç»†æ§åˆ¶ã€‚</li>
<li>HD-PPTæ¡†æ¶å°†è¯­éŸ³åˆæˆè½¬åŒ–ä¸ºç»“æ„åŒ–ã€å±‚æ¬¡åŒ–ä»»åŠ¡æ¥è§£å†³æ¨¡æ€å·®è·é—®é¢˜ã€‚</li>
<li>å¼•å…¥æ–°å‹è¯­éŸ³ç¼–è§£ç å™¨ï¼Œä»å¤æ‚è¯­éŸ³æ ‡è®°ä¸­æå–ä¸åŒçš„æç¤ºå’Œå†…å®¹åå¥½æ ‡è®°ã€‚</li>
<li>é€šè¿‡ASRå’ŒCLAPç›®æ ‡è¿›è¡Œç›‘ç£ä»¥æ”¹å–„æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>é‡‡å–å±‚æ¬¡è§£ç ç­–ç•¥ï¼ŒæŒ‰è¯­ä¹‰ã€é£æ ¼å’Œå£°éŸ³è¡¨è¾¾çš„é¡ºåºç”Ÿæˆæ ‡è®°ã€‚</li>
<li>å®éªŒè¯æ˜è¯¥å±‚æ¬¡åŒ–èŒƒå¼æé«˜æŒ‡ä»¤éµå¾ªåº¦å¹¶è¾¾åˆ°ä¸šç•Œé¢†å…ˆè‡ªç„¶åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.19001">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Speech/2509.19001v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Speech/2509.19001v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Speech/2509.19001v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Generalizability-of-Predictive-and-Generative-Speech-Enhancement-Models-to-Pathological-Speakers"><a href="#Generalizability-of-Predictive-and-Generative-Speech-Enhancement-Models-to-Pathological-Speakers" class="headerlink" title="Generalizability of Predictive and Generative Speech Enhancement Models   to Pathological Speakers"></a>Generalizability of Predictive and Generative Speech Enhancement Models   to Pathological Speakers</h2><p><strong>Authors:Mingchi Hou, Ante Jukic, Ina Kodrasi</strong></p>
<p>State of the art speech enhancement (SE) models achieve strong performance on neurotypical speech, but their effectiveness is substantially reduced for pathological speech. In this paper, we investigate strategies to address this gap for both predictive and generative SE models, including i) training models from scratch using pathological data, ii) finetuning models pretrained on neurotypical speech with additional data from pathological speakers, and iii) speaker specific personalization using only data from the individual pathological test speaker. Our results show that, despite the limited size of pathological speech datasets, SE models can be successfully trained or finetuned on such data. Finetuning models with data from several pathological speakers yields the largest performance improvements, while speaker specific personalization is less effective, likely due to the small amount of data available per speaker. These findings highlight the challenges and potential strategies for improving SE performance for pathological speakers. </p>
<blockquote>
<p>å‰æ²¿çš„è¯­éŸ³å¢å¼ºï¼ˆSEï¼‰æ¨¡å‹åœ¨ç¥ç»å…¸å‹è¯­éŸ³ä¸Šè¡¨ç°å¼ºåŠ²ï¼Œä½†å…¶åœ¨ç—…ç†æ€§è¯­éŸ³ä¸Šçš„æ•ˆæœå´å¤§å¤§é™ä½ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é’ˆå¯¹é¢„æµ‹æ€§å’Œç”Ÿæˆæ€§SEæ¨¡å‹ï¼Œæ¢è®¨äº†è§£å†³è¿™ä¸€å·®è·çš„ç­–ç•¥ï¼ŒåŒ…æ‹¬ä¸€ï¼‰ä½¿ç”¨ç—…ç†æ€§æ•°æ®ä»å¤´å¼€å§‹è®­ç»ƒæ¨¡å‹ï¼ŒäºŒï¼‰ä½¿ç”¨æ¥è‡ªç—…ç†æ€§è¯´è¯è€…çš„é™„åŠ æ•°æ®å¯¹ç¥ç»å…¸å‹è¯­éŸ³ä¸Šé¢„è®­ç»ƒçš„æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»¥åŠä¸‰ï¼‰ä»…ä½¿ç”¨æ¥è‡ªä¸ªåˆ«ç—…ç†æ€§æµ‹è¯•è¯´è¯è€…çš„æ•°æ®è¿›è¡Œç‰¹å®šè¯´è¯è€…ä¸ªæ€§åŒ–ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå°½ç®¡ç—…ç†æ€§è¯­éŸ³æ•°æ®é›†çš„å¤§å°æœ‰é™ï¼Œä½†SEæ¨¡å‹å¯ä»¥åœ¨è¿™äº›æ•°æ®ä¸Šè¿›è¡ŒæˆåŠŸè®­ç»ƒæˆ–å¾®è°ƒã€‚ä½¿ç”¨æ¥è‡ªå¤šä¸ªç—…ç†æ€§è¯´è¯è€…çš„æ•°æ®å¾®è°ƒæ¨¡å‹ä¼šå¸¦æ¥æœ€å¤§çš„æ€§èƒ½æå‡ï¼Œè€Œç‰¹å®šè¯´è¯è€…ä¸ªæ€§åŒ–åˆ™æ•ˆæœè¾ƒå°ï¼Œè¿™å¯èƒ½æ˜¯ç”±äºæ¯ä¸ªè¯´è¯è€…çš„æ•°æ®é‡è¾ƒå°æ‰€è‡´ã€‚è¿™äº›å‘ç°çªå‡ºäº†æ”¹å–„ç—…ç†æ€§è¯´è¯è€…SEæ€§èƒ½çš„æŒ‘æˆ˜å’Œæ½œåœ¨ç­–ç•¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.18890v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿™ç¯‡è®ºæ–‡é’ˆå¯¹å½“å‰å…ˆè¿›çš„è¯­éŸ³å¢å¼ºæ¨¡å‹åœ¨å¤„ç†ç—…ç†æ€§è¯­éŸ³æ—¶çš„æ€§èƒ½ä¸‹é™é—®é¢˜è¿›è¡Œäº†æ·±å…¥ç ”ç©¶ã€‚æ–‡ç« æ¢è®¨äº†é’ˆå¯¹é¢„æµ‹å‹å’Œç”Ÿæˆå‹è¯­éŸ³å¢å¼ºæ¨¡å‹çš„æ”¹è¿›ç­–ç•¥ï¼ŒåŒ…æ‹¬ä½¿ç”¨ç—…ç†æ€§æ•°æ®ä»å¤´å¼€å§‹è®­ç»ƒæ¨¡å‹ã€ä½¿ç”¨ç¥ç»å…¸å‹è¯­éŸ³é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒä»¥åŠä»…ä½¿ç”¨ä¸ªä½“ç—…ç†æ€§æµ‹è¯•è¯´è¯è€…çš„æ•°æ®è¿›è¡Œä¸ªæ€§åŒ–è®¾ç½®ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå°½ç®¡ç—…ç†æ€§è¯­éŸ³æ•°æ®é›†è§„æ¨¡æœ‰é™ï¼Œä½†è¯­éŸ³å¢å¼ºæ¨¡å‹ä»å¯åœ¨è¿™äº›æ•°æ®ä¸Šè¿›è¡ŒæˆåŠŸè®­ç»ƒæˆ–å¾®è°ƒã€‚ä½¿ç”¨å¤šä¸ªç—…ç†æ€§è¯´è¯è€…çš„æ•°æ®è¿›è¡Œå¾®è°ƒèƒ½å¸¦æ¥æœ€å¤§çš„æ€§èƒ½æå‡ï¼Œè€Œé’ˆå¯¹ä¸ªåˆ«è¯´è¯è€…çš„ä¸ªæ€§åŒ–è®¾ç½®æ•ˆæœè¾ƒå°ï¼Œè¿™å¯èƒ½æ˜¯ç”±äºæ¯ä¸ªè¯´è¯è€…çš„æ•°æ®é‡è¾ƒå°ã€‚è¿™äº›å‘ç°çªå‡ºäº†æé«˜ç—…ç†æ€§è¯´è¯è€…è¯­éŸ³å¢å¼ºæ€§èƒ½çš„æŒ‘æˆ˜å’Œæ½œåœ¨ç­–ç•¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰å…ˆè¿›çš„è¯­éŸ³å¢å¼ºæ¨¡å‹åœ¨å¤„ç†ç—…ç†æ€§è¯­éŸ³æ—¶æ€§èƒ½ä¸‹é™ã€‚</li>
<li>è®ºæ–‡æ¢è®¨äº†é’ˆå¯¹é¢„æµ‹å‹å’Œç”Ÿæˆå‹è¯­éŸ³å¢å¼ºæ¨¡å‹çš„æ”¹è¿›ç­–ç•¥ï¼ŒåŒ…æ‹¬ä½¿ç”¨ç—…ç†æ€§æ•°æ®è®­ç»ƒæ–°æ¨¡å‹ã€å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹å’Œä¸ªæ€§åŒ–è®¾ç½®ã€‚</li>
<li>ä½¿ç”¨å¤šä¸ªç—…ç†æ€§è¯´è¯è€…çš„æ•°æ®è¿›è¡Œå¾®è°ƒèƒ½æ˜¾è‘—æé«˜è¯­éŸ³å¢å¼ºæ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>ç›¸å¯¹äºå¤§è§„æ¨¡æ•°æ®ï¼Œä¸ªæ€§åŒ–è®¾ç½®å› æ•°æ®é‡è¾ƒå°è€Œæ•ˆæœæœ‰é™ã€‚</li>
<li>æŒ‘æˆ˜åœ¨äºç—…ç†æ€§è¯­éŸ³æ•°æ®é›†è§„æ¨¡æœ‰é™ã€‚</li>
<li>ç ”ç©¶ç»“æœå¼ºè°ƒäº†æé«˜é’ˆå¯¹ç—…ç†æ€§è¯´è¯è€…çš„è¯­éŸ³å¢å¼ºæ€§èƒ½çš„é‡è¦æ€§å’Œæ½œåœ¨ç­–ç•¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.18890">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Speech/2509.18890v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Speech/2509.18890v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Speech/2509.18890v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Speech/2509.18890v1/page_3_1.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Influence-of-Clean-Speech-Characteristics-on-Speech-Enhancement-Performance"><a href="#Influence-of-Clean-Speech-Characteristics-on-Speech-Enhancement-Performance" class="headerlink" title="Influence of Clean Speech Characteristics on Speech Enhancement   Performance"></a>Influence of Clean Speech Characteristics on Speech Enhancement   Performance</h2><p><strong>Authors:Mingchi Hou, Ina Kodrasi</strong></p>
<p>Speech enhancement (SE) performance is known to depend on noise characteristics and signal to noise ratio (SNR), yet intrinsic properties of the clean speech signal itself remain an underexplored factor. In this work, we systematically analyze how clean speech characteristics influence enhancement difficulty across multiple state of the art SE models, languages, and noise conditions. We extract a set of pitch, formant, loudness, and spectral flux features from clean speech and compute correlations with objective SE metrics, including frequency weighted segmental SNR and PESQ. Our results show that formant amplitudes are consistently predictive of SE performance, with higher and more stable formants leading to larger enhancement gains. We further demonstrate that performance varies substantially even within a single speakerâ€™s utterances, highlighting the importance of intraspeaker acoustic variability. These findings provide new insights into SE challenges, suggesting that intrinsic speech characteristics should be considered when designing datasets, evaluation protocols, and enhancement models. </p>
<blockquote>
<p>è¯­éŸ³å¢å¼ºçš„æ€§èƒ½å·²çŸ¥ä¾èµ–äºå™ªå£°ç‰¹æ€§å’Œä¿¡å™ªæ¯”ï¼ˆSNRï¼‰ï¼Œä½†æ¸…æ´è¯­éŸ³ä¿¡å·æœ¬èº«çš„å›ºæœ‰å±æ€§ä»ç„¶æ˜¯ä¸€ä¸ªè¢«å¿½ç•¥çš„å› ç´ ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°åˆ†æäº†æ¸…æ´è¯­éŸ³ç‰¹å¾å¦‚ä½•å½±å“å¤šä¸ªå‰æ²¿è¯­éŸ³å¢å¼ºæ¨¡å‹ã€è¯­è¨€å’Œå™ªå£°æ¡ä»¶ä¸‹çš„å¢å¼ºéš¾åº¦ã€‚æˆ‘ä»¬ä»æ¸…æ´è¯­éŸ³ä¸­æå–äº†éŸ³é«˜ã€å…±æŒ¯å³°ã€å“åº¦å’Œè°±æµç‰¹å¾ï¼Œå¹¶è®¡ç®—äº†ä¸å®¢è§‚è¯­éŸ³å¢å¼ºæŒ‡æ ‡çš„ç›¸å…³æ€§ï¼ŒåŒ…æ‹¬é¢‘ç‡åŠ æƒåˆ†æ®µSNRå’ŒPESQã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå…±æŒ¯å³°å¹…åº¦å§‹ç»ˆä¸è¯­éŸ³å¢å¼ºæ€§èƒ½ç›¸å…³ï¼Œè¾ƒé«˜çš„å…±æŒ¯å³°å¹…åº¦å’Œæ›´ç¨³å®šçš„å…±æŒ¯å³°ä¼šå¸¦æ¥æ›´å¤§çš„å¢å¼ºå¢ç›Šã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è¯æ˜ï¼Œå³ä½¿åœ¨å•ä¸ªè¯´è¯äººçš„è¯è¯­ä¸­ï¼Œæ€§èƒ½ä¹Ÿæœ‰å¾ˆå¤§å·®å¼‚ï¼Œè¿™å‡¸æ˜¾äº†è¯´è¯äººå†…éƒ¨å£°å­¦å˜åŒ–çš„é‡è¦æ€§ã€‚è¿™äº›å‘ç°å¯¹è¯­éŸ³å¢å¼ºæŒ‘æˆ˜æä¾›äº†æ–°çš„è§è§£ï¼Œå»ºè®®åœ¨è®¾è®¡æ•°æ®é›†ã€è¯„ä¼°åè®®å’Œå¢å¼ºæ¨¡å‹æ—¶è€ƒè™‘å†…åœ¨è¯­éŸ³ç‰¹å¾ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.18885v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†å¹²å‡€è¯­éŸ³ç‰¹æ€§å¯¹å¤šç§å…ˆè¿›çš„è¯­éŸ³å¢å¼ºæ¨¡å‹æ€§èƒ½çš„å½±å“ï¼Œæ¶‰åŠä¸åŒè¯­è¨€å’Œå™ªå£°æ¡ä»¶ã€‚é€šè¿‡æå–å¹²å‡€è¯­éŸ³çš„éŸ³è°ƒã€å…±æŒ¯å³°ã€å“åº¦å’Œé¢‘è°±æ³¢åŠ¨ç‰¹å¾ï¼Œä¸å®¢è§‚è¯­éŸ³å¢å¼ºæŒ‡æ ‡ï¼ˆå¦‚é¢‘ç‡åŠ æƒåˆ†æ®µä¿¡å™ªæ¯”å’ŒPESQï¼‰è¿›è¡Œç›¸å…³æ€§åˆ†æã€‚ç»“æœæ˜¾ç¤ºï¼Œå…±æŒ¯å³°å¹…åº¦èƒ½é¢„æµ‹è¯­éŸ³å¢å¼ºæ€§èƒ½ï¼Œå…·æœ‰è¾ƒé«˜ä¸”ç¨³å®šçš„å…±æŒ¯å³°èƒ½å¸¦æ¥æ›´å¤§çš„å¢å¼ºæ•ˆæœã€‚æ­¤å¤–ï¼Œå³ä½¿åœ¨å•ä¸ªè¯´è¯äººçš„å‘éŸ³å†…ï¼Œæ€§èƒ½ä¹Ÿå­˜åœ¨æ˜¾è‘—å˜åŒ–ï¼Œå‡¸æ˜¾å‡ºè¯´è¯äººå†…éƒ¨å£°éŸ³å˜åŒ–çš„é‡è¦æ€§ã€‚è¿™äº›å‘ç°ä¸ºè®¾è®¡æ•°æ®é›†ã€è¯„ä¼°åè®®å’Œå¢å¼ºæ¨¡å‹æ—¶è€ƒè™‘å†…åœ¨è¯­éŸ³ç‰¹æ€§æä¾›äº†æ–°çš„è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¹²å‡€è¯­éŸ³ç‰¹æ€§å¯¹è¯­éŸ³å¢å¼ºæ¨¡å‹æ€§èƒ½çš„å½±å“è¢«ç³»ç»Ÿæ€§åœ°åˆ†æã€‚</li>
<li>ç ”ç©¶æ¶‰åŠå¤šç§å…ˆè¿›çš„è¯­éŸ³å¢å¼ºæ¨¡å‹ã€è¯­è¨€å’Œå™ªå£°æ¡ä»¶ã€‚</li>
<li>é€šè¿‡æå–å¹²å‡€è¯­éŸ³çš„ç‰¹å¾ï¼ˆå¦‚éŸ³è°ƒã€å…±æŒ¯å³°ã€å“åº¦å’Œé¢‘è°±æ³¢åŠ¨ï¼‰ä¸å®¢è§‚è¯­éŸ³å¢å¼ºæŒ‡æ ‡è¿›è¡Œç›¸å…³æ€§åˆ†æã€‚</li>
<li>å…±æŒ¯å³°å¹…åº¦èƒ½é¢„æµ‹è¯­éŸ³å¢å¼ºæ€§èƒ½ã€‚</li>
<li>å…·æœ‰è¾ƒé«˜ä¸”ç¨³å®šçš„å…±æŒ¯å³°èƒ½å¸¦æ¥æ›´å¤§çš„è¯­éŸ³å¢å¼ºæ•ˆæœã€‚</li>
<li>è¯´è¯äººå†…éƒ¨çš„å£°éŸ³å˜åŒ–å¯¹è¯­éŸ³å¢å¼ºæ€§èƒ½æœ‰æ˜¾è‘—å½±å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.18885">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Speech/2509.18885v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Speech/2509.18885v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Speech/2509.18885v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Speech/2509.18885v1/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Speech/2509.18885v1/page_3_2.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Towards-Evaluating-Generative-Audio-Insights-from-Neural-Audio-Codec-Embedding-Distances"><a href="#Towards-Evaluating-Generative-Audio-Insights-from-Neural-Audio-Codec-Embedding-Distances" class="headerlink" title="Towards Evaluating Generative Audio: Insights from Neural Audio Codec   Embedding Distances"></a>Towards Evaluating Generative Audio: Insights from Neural Audio Codec   Embedding Distances</h2><p><strong>Authors:Arijit Biswas, Lars Villemoes</strong></p>
<p>Neural audio codecs (NACs) achieve low-bitrate compression by learning compact audio representations, which can also serve as features for perceptual quality evaluation. We introduce DACe, an enhanced, higher-fidelity version of the Descript Audio Codec (DAC), trained on diverse real and synthetic tonal data with balanced sampling. We systematically compare Fr&#39;echet Audio Distance (FAD) and Maximum Mean Discrepancy (MMD) on MUSHRA tests across speech, music, and mixed content. FAD consistently outperforms MMD, and embeddings from higher-fidelity NACs (such as DACe) show stronger correlations with human judgments. While CLAP LAION Music (CLAP-M) and OpenL3 Mel128 (OpenL3-128M) embeddings achieve higher correlations, NAC embeddings provide a practical zero-shot approach to audio quality assessment, requiring only unencoded audio for training. These results demonstrate the dual utility of NACs for compression and perceptually informed audio evaluation. </p>
<blockquote>
<p>ç¥ç»éŸ³é¢‘ç¼–è§£ç å™¨ï¼ˆNACsï¼‰é€šè¿‡å­¦ä¹ ç´§å‡‘çš„éŸ³é¢‘è¡¨ç¤ºæ¥å®ç°ä½æ¯”ç‰¹ç‡å‹ç¼©ï¼Œè¿™äº›è¡¨ç¤ºä¹Ÿå¯ä»¥ä½œä¸ºæ„ŸçŸ¥è´¨é‡è¯„ä¼°çš„ç‰¹å¾ã€‚æˆ‘ä»¬ä»‹ç»äº†DACeï¼Œè¿™æ˜¯å¢å¼ºå‹é«˜ä¿çœŸç‰ˆæœ¬çš„æè¿°éŸ³é¢‘ç¼–è§£ç å™¨ï¼ˆDACï¼‰ï¼Œåœ¨å‡è¡¡é‡‡æ ·çš„å¤šæ ·çœŸå®å’ŒåˆæˆéŸ³è°ƒæ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒã€‚æˆ‘ä»¬åœ¨è¯­éŸ³ã€éŸ³ä¹å’Œæ··åˆå†…å®¹ä¸Šçš„MUSHRAæµ‹è¯•ä¸­å¯¹Frâ€™echetéŸ³é¢‘è·ç¦»ï¼ˆFADï¼‰å’Œæœ€å¤§å‡å€¼å·®å¼‚ï¼ˆMMDï¼‰è¿›è¡Œäº†ç³»ç»Ÿæ¯”è¾ƒã€‚FADæŒç»­ä¼˜äºMMDï¼Œå¹¶ä¸”æ¥è‡ªé«˜ä¿çœŸNACï¼ˆå¦‚DACeï¼‰çš„åµŒå…¥ä¸äººçš„åˆ¤æ–­è¡¨ç°å‡ºæ›´å¼ºçš„ç›¸å…³æ€§ã€‚è™½ç„¶CLAP LAIONéŸ³ä¹ï¼ˆCLAP-Mï¼‰å’ŒOpenL3 Mel128ï¼ˆOpenL3-128Mï¼‰åµŒå…¥å®ç°äº†æ›´é«˜çš„ç›¸å…³æ€§ï¼Œä½†NACåµŒå…¥æä¾›äº†ä¸€ç§å®ç”¨çš„é›¶æ ·æœ¬éŸ³é¢‘è´¨é‡è¯„ä¼°æ–¹æ³•ï¼Œä»…éœ€è¦æœªç¼–ç çš„éŸ³é¢‘è¿›è¡Œè®­ç»ƒã€‚è¿™äº›ç»“æœè¯æ˜äº†NACsåœ¨å‹ç¼©å’Œæ„ŸçŸ¥é©±åŠ¨çš„éŸ³é¢‘è¯„ä¼°ä¸­çš„åŒé‡å®ç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.18823v1">PDF</a> Pre-review version submitted to ICASSP 2026</p>
<p><strong>Summary</strong></p>
<p>ç¥ç»ç½‘ç»œéŸ³é¢‘ç¼–ç ï¼ˆNACï¼‰é€šè¿‡å­¦ä¹ ç´§å‡‘çš„éŸ³é¢‘è¡¨ç¤ºæ¥å®ç°ä½æ¯”ç‰¹ç‡å‹ç¼©ï¼Œè¿™ç§è¡¨ç¤ºä¹Ÿå¯ä»¥ä½œä¸ºæ„ŸçŸ¥è´¨é‡è¯„ä¼°çš„ç‰¹å¾ã€‚æœ¬æ–‡ä»‹ç»äº†DACeï¼Œå®ƒæ˜¯æè¿°éŸ³é¢‘ç¼–ç ï¼ˆDACï¼‰çš„ä¸€ä¸ªå¢å¼ºç‰ˆï¼Œèƒ½å¤Ÿåœ¨å¤šæ ·åŒ–å’Œå¹³è¡¡çš„çœŸå®å’ŒåˆæˆéŸ³è°ƒæ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒã€‚é€šè¿‡ç³»ç»Ÿåœ°æ¯”è¾ƒFrÃ©chetéŸ³é¢‘è·ç¦»ï¼ˆFADï¼‰å’Œæœ€å¤§å‡å€¼å·®å¼‚ï¼ˆMMDï¼‰åœ¨è¯­éŸ³ã€éŸ³ä¹å’Œæ··åˆå†…å®¹ä¸Šçš„MUSHRAæµ‹è¯•ï¼Œå‘ç°FADå§‹ç»ˆä¼˜äºMMDã€‚é«˜ä¿çœŸNACï¼ˆå¦‚DACeï¼‰çš„åµŒå…¥ä¸äººç±»åˆ¤æ–­è¡¨ç°å‡ºæ›´å¼ºçš„ç›¸å…³æ€§ã€‚è™½ç„¶CLAP LAIONéŸ³ä¹ï¼ˆCLAP-Mï¼‰å’ŒOpenL3 Mel128ï¼ˆOpenL3-128Mï¼‰åµŒå…¥å…·æœ‰æ›´é«˜çš„ç›¸å…³æ€§ï¼Œä½†NACåµŒå…¥ä¸ºéŸ³é¢‘è´¨é‡è¯„ä¼°æä¾›äº†ä¸€ä¸ªå®ç”¨çš„æ— æ ·æœ¬æ–¹æ³•ï¼Œåªéœ€è¦æœªç¼–ç çš„éŸ³é¢‘å³å¯è¿›è¡Œè®­ç»ƒã€‚è¿™äº›ç»“æœè¯æ˜äº†NACsåœ¨å‹ç¼©å’Œæ„ŸçŸ¥é©±åŠ¨çš„éŸ³é¢‘è¯„ä¼°ä¸­çš„åŒé‡æ•ˆç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¥ç»ç½‘ç»œéŸ³é¢‘ç¼–ç ï¼ˆNACï¼‰ç»“åˆäº†éŸ³é¢‘å‹ç¼©å’Œæ„ŸçŸ¥è´¨é‡è¯„ä¼°åŠŸèƒ½ã€‚</li>
<li>DACeä½œä¸ºæè¿°éŸ³é¢‘ç¼–ç ï¼ˆDACï¼‰çš„å¢å¼ºç‰ˆæœ¬ï¼Œè®­ç»ƒæ—¶æ¶‰åŠå¤šæ ·åŒ–ä¸”å¹³è¡¡çš„çœŸå®å’ŒåˆæˆéŸ³è°ƒæ•°æ®ã€‚</li>
<li>FrÃ©chetéŸ³é¢‘è·ç¦»ï¼ˆFADï¼‰åœ¨è¯­éŸ³ã€éŸ³ä¹å’Œæ··åˆå†…å®¹çš„MUSHRAæµ‹è¯•ä¸­è¡¨ç°ä¼˜äºæœ€å¤§å‡å€¼å·®å¼‚ï¼ˆMMDï¼‰ã€‚</li>
<li>é«˜ä¿çœŸNACçš„åµŒå…¥ä¸äººç±»å¯¹éŸ³é¢‘è´¨é‡çš„åˆ¤æ–­é«˜åº¦ç›¸å…³ã€‚</li>
<li>CLAP LAIONéŸ³ä¹ï¼ˆCLAP-Mï¼‰å’ŒOpenL3 Mel128åµŒå…¥è™½å…·æœ‰è¾ƒé«˜ç›¸å…³æ€§ï¼Œä½†NACåµŒå…¥æä¾›äº†ä¸€ç§é›¶æ ·æœ¬éŸ³é¢‘è´¨é‡è¯„ä¼°æ–¹æ³•ã€‚</li>
<li>NACsæ—¢å¯ç”¨äºéŸ³é¢‘å‹ç¼©ï¼Œä¹Ÿå¯ç”¨äºæ„ŸçŸ¥é©±åŠ¨çš„éŸ³é¢‘è¯„ä¼°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.18823">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Speech/2509.18823v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Speech/2509.18823v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Speech/2509.18823v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Speech/2509.18823v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Group-Relative-Policy-Optimization-for-Text-to-Speech-with-Large-Language-Models"><a href="#Group-Relative-Policy-Optimization-for-Text-to-Speech-with-Large-Language-Models" class="headerlink" title="Group Relative Policy Optimization for Text-to-Speech with Large   Language Models"></a>Group Relative Policy Optimization for Text-to-Speech with Large   Language Models</h2><p><strong>Authors:Chang Liu, Ya-Jun Hu, Ying-Ying Gao, Shi-Lei Zhang, Zhen-Hua Ling</strong></p>
<p>This paper proposes a GRPO-based approach to enhance the performance of large language model (LLM)-based text-to-speech (TTS) models by deriving rewards from an off-the-shelf automatic speech recognition (ASR) model. Compared to previous reinforcement learning methods for LLM-based TTS, our method requires no dedicated model for reward computation or training. Moreover, we design a composite reward function that combines character error rate (CER) with negative log-likelihood (NLL) obtained from the ASR model, providing more informative and accurate reward signals. We apply GRPO fine-tuning to pre-trained LLM-based TTS models and evaluate their zero-shot TTS performance. Experimental results show that the proposed method substantially improves both the intelligibility and naturalness of synthesized speech. Ablation studies and further analyses confirm the effectiveness of integrating the two reward components. </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºGRPOçš„æ–¹æ³•ï¼Œé€šè¿‡åˆ©ç”¨ç°æˆçš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹å¾—åˆ°çš„å¥–åŠ±æ¥æé«˜åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å‹çš„æ€§èƒ½ã€‚ä¸ä»¥å‰ç”¨äºLLM-based TTSçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸éœ€è¦ç”¨äºè®¡ç®—å¥–åŠ±æˆ–è®­ç»ƒçš„ä¸“ç”¨æ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªç»„åˆå¥–åŠ±å‡½æ•°ï¼Œå°†å­—ç¬¦é”™è¯¯ç‡ï¼ˆCERï¼‰ä¸ä»ASRæ¨¡å‹è·å¾—çš„è´Ÿå¯¹æ•°ä¼¼ç„¶å€¼ï¼ˆNLLï¼‰ç›¸ç»“åˆï¼Œä»¥æä¾›æ›´å‡†ç¡®ä¸”ä¿¡æ¯ä¸°å¯Œçš„å¥–åŠ±ä¿¡å·ã€‚æˆ‘ä»¬å°†GRPOå¾®è°ƒåº”ç”¨äºåŸºäºé¢„è®­ç»ƒLLMçš„TTSæ¨¡å‹ï¼Œå¹¶è¯„ä¼°å…¶é›¶æ ·æœ¬TTSæ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æé«˜äº†åˆæˆè¯­éŸ³çš„å¯ç†è§£æ€§å’Œè‡ªç„¶åº¦ã€‚æ¶ˆèç ”ç©¶å’Œè¿›ä¸€æ­¥çš„åˆ†æè¯å®äº†æ•´åˆä¸¤ç§å¥–åŠ±æˆåˆ†çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.18798v1">PDF</a> 5 pages,submitted to ICASSP2026</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºGRPOçš„æ–¹æ³•ï¼Œé€šè¿‡åˆ©ç”¨ç°æˆçš„è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹æ¥è®¡ç®—å¥–åŠ±ï¼Œä»¥æé«˜å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸ºåŸºç¡€çš„æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å‹çš„æ€§èƒ½ã€‚ä¸ä¹‹å‰ç”¨äºLLM-based TTSçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ— éœ€ä¸“é—¨çš„å¥–åŠ±è®¡ç®—æˆ–è®­ç»ƒæ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªç»„åˆå¥–åŠ±å‡½æ•°ï¼Œå°†å­—ç¬¦é”™è¯¯ç‡ï¼ˆCERï¼‰ä¸ä»ASRæ¨¡å‹è·å¾—çš„è´Ÿå¯¹æ•°ä¼¼ç„¶å€¼ï¼ˆNLLï¼‰ç›¸ç»“åˆï¼Œä»¥æä¾›æ›´å‡†ç¡®å’Œæ›´ä¸°å¯Œçš„å¥–åŠ±ä¿¡å·ã€‚æˆ‘ä»¬å¯¹é¢„è®­ç»ƒçš„LLM-based TTSæ¨¡å‹åº”ç”¨GRPOå¾®è°ƒæŠ€æœ¯ï¼Œå¹¶è¯„ä¼°å…¶é›¶æ ·æœ¬TTSæ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å¯æ˜¾è‘—æé«˜åˆæˆè¯­éŸ³çš„æ¸…æ™°åº¦å’Œè‡ªç„¶åº¦ã€‚æ¶ˆèç ”ç©¶å’Œè¿›ä¸€æ­¥çš„åˆ†æè¯å®äº†æ•´åˆè¿™ä¸¤ç§å¥–åŠ±æˆåˆ†çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§åŸºäºGRPOçš„æ–¹æ³•ï¼Œåˆ©ç”¨ç°æˆçš„è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹æ¥æé«˜æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>æ— éœ€ä¸“é—¨çš„å¥–åŠ±è®¡ç®—æˆ–è®­ç»ƒæ¨¡å‹ï¼Œç®€åŒ–äº†å¼ºåŒ–å­¦ä¹ è¿‡ç¨‹ã€‚</li>
<li>è®¾è®¡äº†ä¸€ä¸ªç»„åˆå¥–åŠ±å‡½æ•°ï¼Œç»“åˆäº†å­—ç¬¦é”™è¯¯ç‡ï¼ˆCERï¼‰å’Œè´Ÿå¯¹æ•°ä¼¼ç„¶å€¼ï¼ˆNLLï¼‰ï¼Œä»¥æä¾›æ›´å‡†ç¡®çš„å¥–åŠ±ä¿¡å·ã€‚</li>
<li>é€šè¿‡GRPOå¾®è°ƒæŠ€æœ¯åº”ç”¨äºé¢„è®­ç»ƒçš„TTSæ¨¡å‹ï¼Œæå‡äº†é›¶æ ·æœ¬TTSæ€§èƒ½ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½æé«˜åˆæˆè¯­éŸ³çš„æ¸…æ™°åº¦å’Œè‡ªç„¶åº¦ã€‚</li>
<li>æ¶ˆèç ”ç©¶è¯å®äº†æ•´åˆå¥–åŠ±æˆåˆ†çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.18798">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Speech/2509.18798v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Speech/2509.18798v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Speech/2509.18798v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Speech/2509.18798v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="M4SER-Multimodal-Multirepresentation-Multitask-and-Multistrategy-Learning-for-Speech-Emotion-Recognition"><a href="#M4SER-Multimodal-Multirepresentation-Multitask-and-Multistrategy-Learning-for-Speech-Emotion-Recognition" class="headerlink" title="M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy   Learning for Speech Emotion Recognition"></a>M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy   Learning for Speech Emotion Recognition</h2><p><strong>Authors:Jiajun He, Xiaohan Shi, Cheng-Hung Hu, Jinyi Mi, Xingfeng Li, Tomoki Toda</strong></p>
<p>Multimodal speech emotion recognition (SER) has emerged as pivotal for improving human-machine interaction. Researchers are increasingly leveraging both speech and textual information obtained through automatic speech recognition (ASR) to comprehensively recognize emotional states from speakers. Although this approach reduces reliance on human-annotated text data, ASR errors possibly degrade emotion recognition performance. To address this challenge, in our previous work, we introduced two auxiliary tasks, namely, ASR error detection and ASR error correction, and we proposed a novel multimodal fusion (MF) method for learning modality-specific and modality-invariant representations across different modalities. Building on this foundation, in this paper, we introduce two additional training strategies. First, we propose an adversarial network to enhance the diversity of modality-specific representations. Second, we introduce a label-based contrastive learning strategy to better capture emotional features. We refer to our proposed method as M4SER and validate its superiority over state-of-the-art methods through extensive experiments using IEMOCAP and MELD datasets. </p>
<blockquote>
<p>å¤šæ¨¡æ€è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰å¯¹äºæ”¹å–„äººæœºäº¤äº’è‡³å…³é‡è¦ã€‚ç ”ç©¶äººå‘˜è¶Šæ¥è¶Šå¤šåœ°åˆ©ç”¨é€šè¿‡è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰è·å¾—çš„è¯­éŸ³å’Œæ–‡æœ¬ä¿¡æ¯ï¼Œä»¥å…¨é¢è¯†åˆ«è¯´è¯äººçš„æƒ…æ„ŸçŠ¶æ€ã€‚è™½ç„¶è¿™ç§æ–¹æ³•å‡å°‘äº†å¯¹äººç±»æ³¨é‡Šæ–‡æœ¬æ•°æ®çš„ä¾èµ–ï¼Œä½†ASRé”™è¯¯å¯èƒ½ä¼šé™ä½æƒ…æ„Ÿè¯†åˆ«çš„æ€§èƒ½ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œåœ¨æˆ‘ä»¬ä¹‹å‰çš„å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸¤ä¸ªè¾…åŠ©ä»»åŠ¡ï¼Œå³ASRé”™è¯¯æ£€æµ‹å’ŒASRé”™è¯¯æ ¡æ­£ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°çš„å¤šæ¨¡æ€èåˆï¼ˆMFï¼‰æ–¹æ³•ï¼Œç”¨äºå­¦ä¹ ä¸åŒæ¨¡æ€çš„æ¨¡æ€ç‰¹å®šå’Œæ¨¡æ€ä¸å˜è¡¨ç¤ºã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæœ¬æ–‡ä»‹ç»äº†ä¸¤ç§é¢å¤–çš„è®­ç»ƒç­–ç•¥ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¯¹æŠ—ç½‘ç»œï¼Œä»¥å¢å¼ºæ¨¡æ€ç‰¹å®šè¡¨ç¤ºçš„å¤šæ ·æ€§ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºæ ‡ç­¾çš„å¯¹æ¯”å­¦ä¹ ç­–ç•¥ï¼Œä»¥æ›´å¥½åœ°æ•æ‰æƒ…æ„Ÿç‰¹å¾ã€‚æˆ‘ä»¬å°†æ‰€æå‡ºçš„æ–¹æ³•ç§°ä¸ºM4SERï¼Œå¹¶é€šè¿‡åœ¨IEMOCAPå’ŒMELDæ•°æ®é›†ä¸Šè¿›è¡Œçš„å¤§é‡å®éªŒéªŒè¯äº†å…¶ä¼˜äºæœ€æ–°æŠ€æœ¯æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.18706v1">PDF</a> Accepted by IEEE Transactions on Audio, Speech and Language   Processing</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¤šæ¨¡æ€è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰åœ¨æ”¹å–„äººæœºäº¤äº’ä¸­çš„é‡è¦ä½œç”¨ã€‚ç ”ç©¶äººå‘˜è¶Šæ¥è¶Šå¤šåœ°åˆ©ç”¨è¯­éŸ³å’Œé€šè¿‡è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰è·å¾—çš„æ–‡æœ¬ä¿¡æ¯ï¼Œä»¥å…¨é¢è¯†åˆ«è¯´è¯è€…çš„æƒ…æ„ŸçŠ¶æ€ã€‚é’ˆå¯¹ASRè¯¯å·®å¯èƒ½é™ä½æƒ…æ„Ÿè¯†åˆ«æ€§èƒ½çš„é—®é¢˜ï¼Œæœ¬æ–‡å¼•å…¥ä¸¤ç§é¢å¤–çš„è®­ç»ƒç­–ç•¥ï¼šä¸€æ˜¯é‡‡ç”¨å¯¹æŠ—ç½‘ç»œå¢å¼ºæ¨¡æ€ç‰¹å®šè¡¨ç¤ºçš„å¤šæ ·æ€§ï¼›äºŒæ˜¯å¼•å…¥åŸºäºæ ‡ç­¾çš„å¯¹æ¯”å­¦ä¹ ç­–ç•¥ä»¥æ›´å¥½åœ°æ•æ‰æƒ…æ„Ÿç‰¹å¾ã€‚æ‰€æå‡ºçš„M4SERæ–¹æ³•é€šè¿‡IEMOCAPå’ŒMELDæ•°æ®é›†çš„å¹¿æ³›å®éªŒéªŒè¯äº†å…¶ä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«å¯¹æ”¹å–„äººæœºäº¤äº’è‡³å…³é‡è¦ã€‚</li>
<li>ASRè¯¯å·®å¯èƒ½å½±å“æƒ…æ„Ÿè¯†åˆ«çš„å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.18706">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Speech/2509.18706v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Speech/2509.18706v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Speech/2509.18706v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Speech/2509.18706v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-28/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-28/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-28/Face%20Swapping/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Face Swapping/2509.20281v1/page_1_0.jpg" class="responsive-img" alt="Face Swapping">
                        
                        <span class="card-title">Face Swapping</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Face Swapping æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-28  CusEnhancer A Zero-Shot Scene and Controllability Enhancement Method   for Photo Customization via ResInversion
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-28
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Face-Swapping/" class="post-category">
                                    Face Swapping
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Face-Swapping/">
                        <span class="chip bg-color">Face Swapping</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-28/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F_Breast%20Ultrasound/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_åŒ»å­¦å½±åƒ_Breast Ultrasound/2509.00213v2/page_2_1.jpg" class="responsive-img" alt="åŒ»å­¦å½±åƒ/Breast Ultrasound">
                        
                        <span class="card-title">åŒ»å­¦å½±åƒ/Breast Ultrasound</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å½±åƒ/Breast Ultrasound æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-28  Multimodal Deep Learning for Phyllodes Tumor Classification from   Ultrasound and Clinical Data
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-28
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/" class="post-category">
                                    åŒ»å­¦å½±åƒ/Breast Ultrasound
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/">
                        <span class="chip bg-color">åŒ»å­¦å½±åƒ/Breast Ultrasound</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">29885.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
