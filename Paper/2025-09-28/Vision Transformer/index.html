<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Vision Transformer">
    <meta name="description" content="Vision Transformer æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-28  Large Pre-Trained Models for Bimanual Manipulation in 3D">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Vision Transformer | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Vision Transformer/2509.20579v1/page_0_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Vision Transformer</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Vision-Transformer/">
                                <span class="chip bg-color">Vision Transformer</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                Vision Transformer
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-28
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-04
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    10k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    40 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-09-28-æ›´æ–°"><a href="#2025-09-28-æ›´æ–°" class="headerlink" title="2025-09-28 æ›´æ–°"></a>2025-09-28 æ›´æ–°</h1><h2 id="Large-Pre-Trained-Models-for-Bimanual-Manipulation-in-3D"><a href="#Large-Pre-Trained-Models-for-Bimanual-Manipulation-in-3D" class="headerlink" title="Large Pre-Trained Models for Bimanual Manipulation in 3D"></a>Large Pre-Trained Models for Bimanual Manipulation in 3D</h2><p><strong>Authors:Hanna Yurchyk, Wei-Di Chang, Gregory Dudek, David Meger</strong></p>
<p>We investigate the integration of attention maps from a pre-trained Vision Transformer into voxel representations to enhance bimanual robotic manipulation. Specifically, we extract attention maps from DINOv2, a self-supervised ViT model, and interpret them as pixel-level saliency scores over RGB images. These maps are lifted into a 3D voxel grid, resulting in voxel-level semantic cues that are incorporated into a behavior cloning policy. When integrated into a state-of-the-art voxel-based policy, our attention-guided featurization yields an average absolute improvement of 8.2% and a relative gain of 21.9% across all tasks in the RLBench bimanual benchmark. </p>
<blockquote>
<p>æˆ‘ä»¬ç ”ç©¶äº†å¦‚ä½•å°†é¢„è®­ç»ƒçš„è§†è§‰å˜å‹å™¨çš„æ³¨æ„åŠ›å›¾é›†æˆåˆ°ä½“ç´ è¡¨ç¤ºä¸­ï¼Œä»¥å¢å¼ºåŒè¶³æœºå™¨äººçš„æ“ä½œã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä»åä¸ºDINOv2çš„è‡ªæˆ‘ç›‘ç£çš„ViTæ¨¡å‹ä¸­æå–æ³¨æ„åŠ›å›¾ï¼Œå¹¶å°†å…¶è§£é‡Šä¸ºRGBå›¾åƒä¸Šçš„åƒç´ çº§æ˜¾è‘—æ€§åˆ†æ•°ã€‚è¿™äº›åœ°å›¾æå‡ä¸ºä¸‰ç»´ä½“ç´ ç½‘æ ¼ï¼Œå¾—åˆ°ç»“åˆè¡Œä¸ºå…‹éš†ç­–ç•¥çš„ä½“ç´ çº§è¯­ä¹‰çº¿ç´¢ã€‚å½“é›†æˆåˆ°æœ€å…ˆè¿›çš„åŸºäºä½“ç´ çš„æ”¿ç­–ä¸­æ—¶ï¼Œæˆ‘ä»¬çš„æ³¨æ„åŠ›å¼•å¯¼ç‰¹å¾åŒ–åœ¨RLBenchåŒè¶³åŸºå‡†æµ‹è¯•ä¸­æ‰€æœ‰ä»»åŠ¡ä¸Šçš„å¹³å‡ç»å¯¹æ”¹è¿›äº†8.2%ï¼Œç›¸å¯¹æ”¶ç›Šæé«˜äº†21.9%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.20579v1">PDF</a> Accepted to 2025 IEEE-RAS 24th International Conference on Humanoid   Robots</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†å°†é¢„è®­ç»ƒçš„Vision Transformerçš„æ³¨æ„åŠ›å›¾èå…¥ä½“ç´ è¡¨ç¤ºï¼Œä»¥å¼ºåŒ–åŒè¶³æœºå™¨äººçš„æ“ä½œã€‚é€šè¿‡ä»DINOv2ï¼ˆä¸€ç§è‡ªç›‘ç£çš„ViTæ¨¡å‹ï¼‰ä¸­æå–æ³¨æ„åŠ›å›¾ï¼Œå¹¶è§£è¯»ä¸ºRGBå›¾åƒä¸Šçš„åƒç´ çº§æ˜¾è‘—æ€§åˆ†æ•°ï¼Œå°†å…¶æå‡åˆ°3Dä½“ç´ ç½‘æ ¼ä¸­ï¼Œå¾—åˆ°ä½“ç´ çº§çš„è¯­ä¹‰çº¿ç´¢ï¼Œå¹¶èå…¥è¡Œä¸ºå…‹éš†ç­–ç•¥ã€‚åœ¨RLBenchåŒè¶³åŸºå‡†æµ‹è¯•ä¸­ï¼Œæ³¨æ„åŠ›å¯¼å‘çš„ç‰¹å¾åŒ–æŠ€æœ¯å¹³å‡ç»å¯¹æå‡8.2%ï¼Œç›¸å¯¹æå‡21.9%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶äº†å¦‚ä½•å°†é¢„è®­ç»ƒçš„Vision Transformerçš„æ³¨æ„åŠ›å›¾èå…¥æœºå™¨äººæ“ä½œã€‚</li>
<li>é‡‡ç”¨DINOv2æ¨¡å‹æå–æ³¨æ„åŠ›å›¾ï¼Œå¹¶è§£è¯»ä¸ºåƒç´ çº§æ˜¾è‘—æ€§åˆ†æ•°ã€‚</li>
<li>å°†æ³¨æ„åŠ›å›¾æå‡åˆ°3Dä½“ç´ ç½‘æ ¼ï¼Œå¾—åˆ°ä½“ç´ çº§çš„è¯­ä¹‰çº¿ç´¢ã€‚</li>
<li>è¿™äº›è¯­ä¹‰çº¿ç´¢è¢«èå…¥è¡Œä¸ºå…‹éš†ç­–ç•¥ä¸­ã€‚</li>
<li>åœ¨RLBenchåŒè¶³åŸºå‡†æµ‹è¯•ä¸­è¿›è¡Œäº†æµ‹è¯•ã€‚</li>
<li>æ³¨æ„åŠ›å¯¼å‘çš„ç‰¹å¾åŒ–æŠ€æœ¯å–å¾—äº†æ˜¾è‘—çš„æå‡æ•ˆæœï¼Œå¹³å‡ç»å¯¹æå‡8.2%ã€‚</li>
<li>ä¸åŸæœ‰ç­–ç•¥ç›¸æ¯”ï¼Œç›¸å¯¹æå‡è¾¾åˆ°äº†21.9%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.20579">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Vision Transformer/2509.20579v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Vision Transformer/2509.20579v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Vision Transformer/2509.20579v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Vision Transformer/2509.20579v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Vision Transformer/2509.20579v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Vision Transformer/2509.20579v1/page_5_1.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="ImageNet-trained-CNNs-are-not-biased-towards-texture-Revisiting-feature-reliance-through-controlled-suppression"><a href="#ImageNet-trained-CNNs-are-not-biased-towards-texture-Revisiting-feature-reliance-through-controlled-suppression" class="headerlink" title="ImageNet-trained CNNs are not biased towards texture: Revisiting feature   reliance through controlled suppression"></a>ImageNet-trained CNNs are not biased towards texture: Revisiting feature   reliance through controlled suppression</h2><p><strong>Authors:Tom Burgert, Oliver Stoll, Paolo Rota, BegÃ¼m Demir</strong></p>
<p>The hypothesis that Convolutional Neural Networks (CNNs) are inherently texture-biased has shaped much of the discourse on feature use in deep learning. We revisit this hypothesis by examining limitations in the cue-conflict experiment by Geirhos et al. To address these limitations, we propose a domain-agnostic framework that quantifies feature reliance through systematic suppression of shape, texture, and color cues, avoiding the confounds of forced-choice conflicts. By evaluating humans and neural networks under controlled suppression conditions, we find that CNNs are not inherently texture-biased but predominantly rely on local shape features. Nonetheless, this reliance can be substantially mitigated through modern training strategies or architectures (ConvNeXt, ViTs). We further extend the analysis across computer vision, medical imaging, and remote sensing, revealing that reliance patterns differ systematically: computer vision models prioritize shape, medical imaging models emphasize color, and remote sensing models exhibit a stronger reliance towards texture. Code is available at <a target="_blank" rel="noopener" href="https://github.com/tomburgert/feature-reliance">https://github.com/tomburgert/feature-reliance</a>. </p>
<blockquote>
<p>å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å…·æœ‰å›ºæœ‰çº¹ç†åå‘æ€§çš„å‡è®¾å·²ç»å½±å“äº†æ·±åº¦å­¦ä¹ ä¸­ç‰¹å¾ä½¿ç”¨çš„è®¸å¤šè®¨è®ºã€‚æˆ‘ä»¬é€šè¿‡ç ”ç©¶Geirhosç­‰äººåœ¨æç¤ºå†²çªå®éªŒä¸­çš„å±€é™æ€§ï¼Œé‡æ–°å®¡è§†è¿™ä¸€å‡è®¾ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªè·¨é¢†åŸŸçš„æ¡†æ¶ï¼Œé€šè¿‡ç³»ç»Ÿæ€§åœ°æŠ‘åˆ¶å½¢çŠ¶ã€çº¹ç†å’Œé¢œè‰²çº¿ç´¢æ¥é‡åŒ–ç‰¹å¾ä¾èµ–æ€§ï¼Œé¿å…äº†å¼ºåˆ¶é€‰æ‹©å†²çªæ‰€å¸¦æ¥çš„æ··æ·†ã€‚é€šè¿‡åœ¨å—æ§æŠ‘åˆ¶æ¡ä»¶ä¸‹å¯¹äººç±»å’Œç¥ç»ç½‘ç»œè¿›è¡Œè¯„ä¼°ï¼Œæˆ‘ä»¬å‘ç°CNNå¹¶éå›ºæœ‰åœ°åå‘äºçº¹ç†ï¼Œè€Œæ˜¯ä¸»è¦ä¾èµ–äºå±€éƒ¨å½¢çŠ¶ç‰¹å¾ã€‚ç„¶è€Œï¼Œé€šè¿‡ç°ä»£è®­ç»ƒç­–ç•¥æˆ–æ¶æ„ï¼ˆå¦‚ConvNeXtå’ŒViTsï¼‰ï¼Œè¿™ç§ä¾èµ–æ€§å¯ä»¥å¾—åˆ°æ˜¾è‘—ç¼“è§£ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æ‰©å±•äº†è®¡ç®—æœºè§†è§‰ã€åŒ»å­¦æˆåƒå’Œé¥æ„Ÿåˆ†æé¢†åŸŸçš„ç ”ç©¶ï¼Œå‘ç°ä¾èµ–æ¨¡å¼å­˜åœ¨ç³»ç»Ÿæ€§å·®å¼‚ï¼šè®¡ç®—æœºè§†è§‰æ¨¡å‹ä¼˜å…ˆè€ƒè™‘å½¢çŠ¶ï¼ŒåŒ»å­¦æˆåƒæ¨¡å‹å¼ºè°ƒé¢œè‰²ï¼Œé¥æ„Ÿæ¨¡å‹åˆ™è¡¨ç°å‡ºæ›´å¼ºçš„çº¹ç†ä¾èµ–æ€§ã€‚ç›¸å…³ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/tomburgert/feature-reliance%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/tomburgert/feature-relianceè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.20234v1">PDF</a> Accepted at NeurIPS 2025 (oral)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦æ¢è®¨å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰æ˜¯å¦å›ºæœ‰åœ°åå‘äºçº¹ç†ç‰¹å¾çš„é—®é¢˜ï¼Œé‡æ–°è¯„ä¼°äº†Geirhosç­‰äººåœ¨å®éªŒä¸­çš„å±€é™æ€§ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºä¸€ç§é€šç”¨çš„æ¡†æ¶æ¥é‡åŒ–æ¨¡å‹å¯¹å„ç§ç‰¹å¾çš„ä¾èµ–ç¨‹åº¦ï¼Œå¦‚å½¢çŠ¶ã€çº¹ç†å’Œé¢œè‰²ç­‰ã€‚é€šè¿‡æ§åˆ¶æ¡ä»¶ä¸‹çš„å®éªŒè¯„ä¼°ï¼Œå‘ç°CNNå¹¶éå›ºæœ‰åœ°åå‘äºçº¹ç†ç‰¹å¾ï¼Œè€Œæ˜¯ä¸»è¦ä¾èµ–äºå±€éƒ¨å½¢çŠ¶ç‰¹å¾ã€‚æ­¤å¤–ï¼Œç°ä»£è®­ç»ƒç­–ç•¥å’Œæ¶æ„ï¼ˆå¦‚ConvNeXtå’ŒViTsï¼‰èƒ½æœ‰æ•ˆå‡è½»è¿™ç§ä¾èµ–ã€‚åœ¨ä¸åŒé¢†åŸŸï¼ˆå¦‚è®¡ç®—æœºè§†è§‰ã€åŒ»å­¦æˆåƒå’Œé¥æ„Ÿï¼‰ä¸­ï¼Œæ¨¡å‹ä¾èµ–ç‰¹å¾çš„æ¨¡å¼ä¹Ÿå­˜åœ¨ç³»ç»Ÿæ€§å·®å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰åœ¨å®éªŒä¸­çš„çº¹ç†åå‘é—®é¢˜éœ€è¦é‡æ–°å®¡è§†ã€‚</li>
<li>æå‡ºä¸€ç§æ¡†æ¶æ¥é‡åŒ–æ¨¡å‹å¯¹å½¢çŠ¶ã€çº¹ç†å’Œé¢œè‰²ç‰¹å¾çš„ä¾èµ–ç¨‹åº¦ã€‚</li>
<li>é€šè¿‡æ§åˆ¶æ¡ä»¶ä¸‹çš„å®éªŒè¯„ä¼°ï¼Œå‘ç°CNNä¸»è¦ä¾èµ–äºå±€éƒ¨å½¢çŠ¶ç‰¹å¾ï¼Œè€Œéå›ºæœ‰åœ°åå‘äºçº¹ç†ç‰¹å¾ã€‚</li>
<li>ç°ä»£è®­ç»ƒç­–ç•¥å’Œæ¶æ„å¯ä»¥æœ‰æ•ˆå‡è½»CNNå¯¹ç‰¹å®šç‰¹å¾çš„ä¾èµ–ã€‚</li>
<li>ä¸åŒé¢†åŸŸä¸­çš„æ¨¡å‹ä¾èµ–ç‰¹å¾æ¨¡å¼å­˜åœ¨ç³»ç»Ÿæ€§å·®å¼‚ã€‚</li>
<li>è®¡ç®—æœºè§†è§‰æ¨¡å‹ä¸»è¦å…³æ³¨å½¢çŠ¶ç‰¹å¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.20234">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Vision Transformer/2509.20234v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Vision Transformer/2509.20234v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Vision Transformer/2509.20234v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Vision Transformer/2509.20234v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Vision Transformer/2509.20234v1/page_4_1.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Algorithms-for-Adversarially-Robust-Deep-Learning"><a href="#Algorithms-for-Adversarially-Robust-Deep-Learning" class="headerlink" title="Algorithms for Adversarially Robust Deep Learning"></a>Algorithms for Adversarially Robust Deep Learning</h2><p><strong>Authors:Alexander Robey</strong></p>
<p>Given the widespread use of deep learning models in safety-critical applications, ensuring that the decisions of such models are robust against adversarial exploitation is of fundamental importance. In this thesis, we discuss recent progress toward designing algorithms that exhibit desirable robustness properties. First, we discuss the problem of adversarial examples in computer vision, for which we introduce new technical results, training paradigms, and certification algorithms. Next, we consider the problem of domain generalization, wherein the task is to train neural networks to generalize from a family of training distributions to unseen test distributions. We present new algorithms that achieve state-of-the-art generalization in medical imaging, molecular identification, and image classification. Finally, we study the setting of jailbreaking large language models (LLMs), wherein an adversarial user attempts to design prompts that elicit objectionable content from an LLM. We propose new attacks and defenses, which represent the frontier of progress toward designing robust language-based agents. </p>
<blockquote>
<p>é‰´äºæ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨å…³é”®å®‰å…¨åº”ç”¨ä¸­çš„å¹¿æ³›åº”ç”¨ï¼Œç¡®ä¿è¿™äº›æ¨¡å‹çš„å†³ç­–åœ¨é¢å¯¹å¯¹æŠ—æ€§æ”»å‡»æ—¶å…·æœ‰é²æ£’æ€§å˜å¾—è‡³å…³é‡è¦ã€‚åœ¨æœ¬è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬è®¨è®ºäº†è®¾è®¡å…·æœ‰ç†æƒ³é²æ£’æ€§çš„ç®—æ³•çš„æœ€æ–°è¿›å±•ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬è®¨è®ºè®¡ç®—æœºè§†è§‰ä¸­çš„å¯¹æŠ—æ€§æ ·æœ¬é—®é¢˜ï¼Œä¸ºæ­¤æˆ‘ä»¬ä»‹ç»äº†æ–°æŠ€æœ¯æˆæœã€è®­ç»ƒèŒƒå¼å’Œè®¤è¯ç®—æ³•ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬è€ƒè™‘åŸŸæ³›åŒ–é—®é¢˜ï¼Œå…¶ä»»åŠ¡æ˜¯è®­ç»ƒç¥ç»ç½‘ç»œï¼Œä»¥ä»ä¸€ç³»åˆ—è®­ç»ƒåˆ†å¸ƒæ¨å¹¿åˆ°æœªè§è¿‡çš„æµ‹è¯•åˆ†å¸ƒã€‚æˆ‘ä»¬æå‡ºäº†åœ¨åŒ»å­¦æˆåƒã€åˆ†å­è¯†åˆ«å’Œå›¾åƒåˆ†ç±»æ–¹é¢å®ç°æœ€æ–°æ³›åŒ–çš„æ–°ç®—æ³•ã€‚æœ€åï¼Œæˆ‘ä»¬ç ”ç©¶ç ´è§£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è®¾ç½®ï¼Œå…¶ä¸­å¯¹æŠ—æ€§ç”¨æˆ·è¯•å›¾è®¾è®¡æç¤ºï¼Œä»¥ä»LLMä¸­å¼•å‘ä»¤äººåæ„Ÿçš„å†…å®¹ã€‚æˆ‘ä»¬æå‡ºäº†æ–°çš„æ”»å‡»å’Œé˜²å¾¡æ‰‹æ®µï¼Œè¿™ä»£è¡¨äº†åœ¨è®¾è®¡å¥å£®çš„è¯­è¨€æ™ºèƒ½ä½“æ–¹é¢çš„æœ€å‰æ²¿è¿›å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.19100v1">PDF</a> PhD thesis</p>
<p><strong>Summary</strong><br>æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨å…³é”®å®‰å…¨é¢†åŸŸåº”ç”¨å¹¿æ³›ï¼Œå…¶å†³ç­–é²æ£’æ€§è‡³å…³é‡è¦ã€‚æœ¬è®ºæ–‡æ¢è®¨è®¾è®¡å…·å¤‡ç†æƒ³é²æ£’æ€§çš„ç®—æ³•çš„æœ€æ–°è¿›å±•ã€‚åŒ…æ‹¬ç ”ç©¶è®¡ç®—æœºè§†è§‰ä¸­çš„å¯¹æŠ—æ ·æœ¬é—®é¢˜ï¼Œå¼•å…¥æ–°æŠ€æœ¯ã€è®­ç»ƒèŒƒå¼å’Œè®¤è¯ç®—æ³•ï¼›è§£å†³ç¥ç»ç½‘ç»œåœ¨å¤šç§é¢†åŸŸä¸­çš„åŸŸæ³›åŒ–é—®é¢˜ï¼Œå±•ç¤ºåœ¨åŒ»å­¦æˆåƒç­‰é¢†åŸŸçš„æ–°ç®—æ³•çš„å‰æ²¿æ€§ï¼›ç ”ç©¶çªç ´å¤§å‹è¯­è¨€æ¨¡å‹çš„é—®é¢˜ï¼Œæå‡ºæ–°çš„æ”»å‡»å’Œé˜²å¾¡æ‰‹æ®µï¼Œæ¨è¿›è¯­è¨€æ™ºèƒ½ä»£ç†çš„ç¨³å¥æ€§è®¾è®¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ æ¨¡å‹çš„å†³ç­–é²æ£’æ€§åœ¨å…³é”®å®‰å…¨é¢†åŸŸè‡³å…³é‡è¦ã€‚</li>
<li>è®ºæ–‡è®¨è®ºäº†è®¡ç®—æœºè§†è§‰ä¸­çš„å¯¹æŠ—æ ·æœ¬é—®é¢˜ï¼Œå¼•å…¥æ–°æŠ€æœ¯å’Œç®—æ³•æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>ç ”ç©¶äº†ç¥ç»ç½‘ç»œçš„åŸŸæ³›åŒ–é—®é¢˜ï¼Œå±•ç¤ºæ–°ç®—æ³•åœ¨åŒ»å­¦æˆåƒç­‰é¢†åŸŸçš„ä¼˜åŠ¿ã€‚</li>
<li>é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„â€œjailbreakingâ€é—®é¢˜è¿›è¡Œç ”ç©¶ï¼Œæå‡ºæ–°çš„æ”»å‡»å’Œé˜²å¾¡ç­–ç•¥ã€‚</li>
<li>è®ºæ–‡å¼ºè°ƒè®¾è®¡å…·å¤‡ç†æƒ³é²æ£’æ€§çš„ç®—æ³•çš„é‡è¦æ€§ï¼Œä»¥ä¿æŠ¤æ·±åº¦å­¦ä¹ æ¨¡å‹å…å—å¯¹æŠ—æ€§æ”»å‡»çš„å½±å“ã€‚</li>
<li>é€šè¿‡æ–°æŠ€æœ¯å’Œè®­ç»ƒèŒƒå¼ï¼Œæé«˜äº†æ·±åº¦å­¦ä¹ æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œé²æ£’æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.19100">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Vision Transformer/2509.19100v1/page_0_0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="No-Labels-Needed-Zero-Shot-Image-Classification-with-Collaborative-Self-Learning"><a href="#No-Labels-Needed-Zero-Shot-Image-Classification-with-Collaborative-Self-Learning" class="headerlink" title="No Labels Needed: Zero-Shot Image Classification with Collaborative   Self-Learning"></a>No Labels Needed: Zero-Shot Image Classification with Collaborative   Self-Learning</h2><p><strong>Authors:Matheus VinÃ­cius Todescato, Joel LuÃ­s Carbonera</strong></p>
<p>While deep learning, including Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs), has significantly advanced classification performance, its typical reliance on extensive annotated datasets presents a major obstacle in many practical scenarios where such data is scarce. Vision-language models (VLMs) and transfer learning with pre-trained visual models appear as promising techniques to deal with this problem. This paper proposes a novel zero-shot image classification framework that combines a VLM and a pre-trained visual model within a self-learning cycle. Requiring only the set of class names and no labeled training data, our method utilizes a confidence-based pseudo-labeling strategy to train a lightweight classifier directly on the test data, enabling dynamic adaptation. The VLM identifies high-confidence samples, and the pre-trained visual model enhances their visual representations. These enhanced features then iteratively train the classifier, allowing the system to capture complementary semantic and visual cues without supervision. Notably, our approach avoids VLM fine-tuning and the use of large language models, relying on the visual-only model to reduce the dependence on semantic representation. Experimental evaluations on ten diverse datasets demonstrate that our approach outperforms the baseline zero-shot method. </p>
<blockquote>
<p>å°½ç®¡æ·±åº¦å­¦ä¹ ï¼ŒåŒ…æ‹¬å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å’Œè§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰ï¼Œå·²ç»æ˜¾è‘—æé«˜äº†åˆ†ç±»æ€§èƒ½ï¼Œä½†å®ƒé€šå¸¸ä¾èµ–äºå¤§é‡çš„æ³¨é‡Šæ•°æ®é›†ï¼Œè¿™åœ¨è®¸å¤šæ­¤ç±»æ•°æ®ç¨€ç¼ºçš„å®é™…åœºæ™¯ä¸­æ„æˆäº†ä¸€ä¸ªä¸»è¦éšœç¢ã€‚è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å’Œå€ŸåŠ©é¢„è®­ç»ƒè§†è§‰æ¨¡å‹çš„è¿ç§»å­¦ä¹ ä¼¼ä¹æ˜¯æœ‰å¸Œæœ›è§£å†³è¿™ä¸ªé—®é¢˜çš„æŠ€æœ¯ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„é›¶æ ·æœ¬å›¾åƒåˆ†ç±»æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†VLMå’Œé¢„è®­ç»ƒçš„è§†è§‰æ¨¡å‹ï¼Œåœ¨ä¸€ä¸ªè‡ªå­¦ä¹ å¾ªç¯å†…ã€‚è¯¥æ–¹æ³•ä»…éœ€è¦ç±»åˆ«åç§°é›†åˆï¼Œè€Œä¸éœ€è¦æ ‡è®°çš„è®­ç»ƒæ•°æ®ï¼Œå®ƒé‡‡ç”¨åŸºäºç½®ä¿¡åº¦çš„ä¼ªæ ‡ç­¾ç­–ç•¥ç›´æ¥åœ¨æµ‹è¯•æ•°æ®ä¸Šè®­ç»ƒè½»é‡çº§åˆ†ç±»å™¨ï¼Œä»è€Œå®ç°åŠ¨æ€é€‚åº”ã€‚VLMè¯†åˆ«é«˜ç½®ä¿¡åº¦æ ·æœ¬ï¼Œé¢„è®­ç»ƒçš„è§†è§‰æ¨¡å‹å¢å¼ºå®ƒä»¬çš„è§†è§‰è¡¨ç¤ºã€‚è¿™äº›å¢å¼ºçš„ç‰¹å¾ç„¶åè¿­ä»£åœ°è®­ç»ƒåˆ†ç±»å™¨ï¼Œä½¿ç³»ç»Ÿèƒ½å¤Ÿæ•è·æ— ç›‘ç£çš„äº’è¡¥è¯­ä¹‰å’Œè§†è§‰çº¿ç´¢ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é¿å…äº†VLMå¾®è°ƒå’Œä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œè€Œä¾èµ–äºä»…è§†è§‰æ¨¡å‹æ¥å‡å°‘å¯¹è¯­ä¹‰è¡¨ç¤ºçš„ä¾èµ–ã€‚åœ¨åä¸ªä¸åŒæ•°æ®é›†ä¸Šçš„å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºåŸºå‡†é›¶æ ·æœ¬æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.18938v1">PDF</a> This paper was accepted at International Conference on Tools with   Artificial Intelligence (ICTAI) 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“åˆè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å’Œé¢„è®­ç»ƒè§†è§‰æ¨¡å‹çš„é›¶æ ·æœ¬å›¾åƒåˆ†ç±»æ¡†æ¶ã€‚è¯¥æ–¹æ³•ä»…éœ€ç±»åˆ«åç§°ï¼Œæ— éœ€æ ‡æ³¨è®­ç»ƒæ•°æ®ï¼Œé€šè¿‡åŸºäºè‡ªä¿¡çš„ä¼ªæ ‡ç­¾ç­–ç•¥ç›´æ¥åœ¨æµ‹è¯•æ•°æ®ä¸Šè®­ç»ƒè½»é‡çº§åˆ†ç±»å™¨ï¼Œå®ç°åŠ¨æ€é€‚åº”ã€‚VLMè¯†åˆ«é«˜ç½®ä¿¡æ ·æœ¬ï¼Œé¢„è®­ç»ƒè§†è§‰æ¨¡å‹å¢å¼ºå…¶è§†è§‰è¡¨å¾ã€‚è¿™äº›å¢å¼ºçš„ç‰¹å¾è¿­ä»£åœ°è®­ç»ƒåˆ†ç±»å™¨ï¼Œä½¿ç³»ç»Ÿèƒ½å¤Ÿæ•è·äº’è¡¥çš„è¯­ä¹‰å’Œè§†è§‰çº¿ç´¢ï¼Œæ— éœ€ç›‘ç£ã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºåŸºçº¿é›¶æ ·æœ¬æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§é›¶æ ·æœ¬å›¾åƒåˆ†ç±»æ¡†æ¶ï¼Œç»“åˆäº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å’Œé¢„è®­ç»ƒè§†è§‰æ¨¡å‹ã€‚</li>
<li>è¯¥æ–¹æ³•ä»…éœ€ç±»åˆ«åç§°ï¼Œæ— éœ€æ ‡æ³¨è®­ç»ƒæ•°æ®ï¼Œé€šè¿‡è‡ªä¿¡ä¼ªæ ‡ç­¾ç­–ç•¥ç›´æ¥åœ¨æµ‹è¯•æ•°æ®ä¸Šè®­ç»ƒåˆ†ç±»å™¨ã€‚</li>
<li>VLMç”¨äºè¯†åˆ«é«˜ç½®ä¿¡æ ·æœ¬ï¼Œé¢„è®­ç»ƒè§†è§‰æ¨¡å‹å¢å¼ºè¿™äº›æ ·æœ¬çš„è§†è§‰è¡¨å¾ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡è¿­ä»£è®­ç»ƒåˆ†ç±»å™¨ï¼Œèƒ½å¤Ÿæ•è·äº’è¡¥çš„è¯­ä¹‰å’Œè§†è§‰çº¿ç´¢ã€‚</li>
<li>è¯¥æ–¹æ³•é¿å…äº†VLMå¾®è°ƒå’Œå¤§è¯­è¨€æ¨¡å‹çš„ä½¿ç”¨ï¼Œé™ä½äº†å¯¹è¯­ä¹‰è¡¨ç¤ºçš„ä¾èµ–ã€‚</li>
<li>å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºåŸºçº¿é›¶æ ·æœ¬æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.18938">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Vision Transformer/2509.18938v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Vision Transformer/2509.18938v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Vision Transformer/2509.18938v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Vision Transformer/2509.18938v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Vision Transformer/2509.18938v1/page_5_1.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Benchmarking-Vision-Language-and-Multimodal-Large-Language-Models-in-Zero-shot-and-Few-shot-Scenarios-A-study-on-Christian-Iconography"><a href="#Benchmarking-Vision-Language-and-Multimodal-Large-Language-Models-in-Zero-shot-and-Few-shot-Scenarios-A-study-on-Christian-Iconography" class="headerlink" title="Benchmarking Vision-Language and Multimodal Large Language Models in   Zero-shot and Few-shot Scenarios: A study on Christian Iconography"></a>Benchmarking Vision-Language and Multimodal Large Language Models in   Zero-shot and Few-shot Scenarios: A study on Christian Iconography</h2><p><strong>Authors:Gianmarco Spinaci, Lukas Klic, Giovanni Colavizza</strong></p>
<p>This study evaluates the capabilities of Multimodal Large Language Models (LLMs) and Vision Language Models (VLMs) in the task of single-label classification of Christian Iconography. The goal was to assess whether general-purpose VLMs (CLIP and SigLIP) and LLMs, such as GPT-4o and Gemini 2.5, can interpret the Iconography, typically addressed by supervised classifiers, and evaluate their performance. Two research questions guided the analysis: (RQ1) How do multimodal LLMs perform on image classification of Christian saints? And (RQ2), how does performance vary when enriching input with contextual information or few-shot exemplars? We conducted a benchmarking study using three datasets supporting Iconclass natively: ArtDL, ICONCLASS, and Wikidata, filtered to include the top 10 most frequent classes. Models were tested under three conditions: (1) classification using class labels, (2) classification with Iconclass descriptions, and (3) few-shot learning with five exemplars. Results were compared against ResNet50 baselines fine-tuned on the same datasets. The findings show that Gemini-2.5 Pro and GPT-4o outperformed the ResNet50 baselines. Accuracy dropped significantly on the Wikidata dataset, where Siglip reached the highest accuracy score, suggesting model sensitivity to image size and metadata alignment. Enriching prompts with class descriptions generally improved zero-shot performance, while few-shot learning produced lower results, with only occasional and minimal increments in accuracy. We conclude that general-purpose multimodal LLMs are capable of classification in visually complex cultural heritage domains. These results support the application of LLMs as metadata curation tools in digital humanities workflows, suggesting future research on prompt optimization and the expansion of the study to other classification strategies and models. </p>
<blockquote>
<p>æœ¬ç ”ç©¶è¯„ä¼°äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨åŸºç£æ•™è‚–åƒå­¦å•ä¸€æ ‡ç­¾åˆ†ç±»ä»»åŠ¡ä¸­çš„èƒ½åŠ›ã€‚ç›®çš„æ˜¯è¯„ä¼°é€šç”¨VLMsï¼ˆCLIPå’ŒSigLIPï¼‰ä»¥åŠLLMsï¼ˆå¦‚GPT-4oå’ŒGemini 2.5ï¼‰æ˜¯å¦èƒ½è§£è¯»é€šå¸¸é€šè¿‡ç›‘ç£åˆ†ç±»å™¨å¤„ç†çš„è‚–åƒå­¦ï¼Œå¹¶è¯„ä¼°å…¶æ€§èƒ½ã€‚æœ¬ç ”ç©¶ç”±ä¸¤ä¸ªç ”ç©¶é—®é¢˜å¼•å¯¼åˆ†æï¼šï¼ˆRQ1ï¼‰å¤šæ¨¡æ€LLMsåœ¨åŸºç£æ•™åœ£äººå›¾åƒåˆ†ç±»æ–¹é¢çš„è¡¨ç°å¦‚ä½•ï¼Ÿï¼ˆRQ2ï¼‰å½“è¾“å…¥ä¸°å¯Œä¸Šä¸‹æ–‡ä¿¡æ¯æˆ–å°‘é‡æ ·æœ¬æ—¶ï¼Œæ€§èƒ½å¦‚ä½•å˜åŒ–ï¼Ÿæˆ‘ä»¬ä½¿ç”¨æ”¯æŒIconclassçš„åŸç”Ÿæ•°æ®é›†ArtDLã€ICONCLASSå’ŒWikidataè¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œç­›é€‰å‡ºå‰10ä¸ªæœ€é¢‘ç¹çš„ç±»åˆ«ã€‚æ¨¡å‹åœ¨ä¸‰ç§æ¡ä»¶ä¸‹è¿›è¡Œæµ‹è¯•ï¼šï¼ˆ1ï¼‰ä½¿ç”¨ç±»åˆ«æ ‡ç­¾è¿›è¡Œåˆ†ç±»ï¼Œï¼ˆ2ï¼‰ä½¿ç”¨Iconclassæè¿°è¿›è¡Œåˆ†ç±»ï¼Œä»¥åŠï¼ˆ3ï¼‰äº”ä¸ªæ ·æœ¬çš„å°‘é‡å­¦ä¹ ã€‚ç»“æœå°†ä¸åœ¨åŒæ•°æ®é›†ä¸Šå¾®è°ƒè¿‡çš„ResNet50åŸºå‡†çº¿è¿›è¡Œæ¯”è¾ƒã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒGemini-2.5 Proå’ŒGPT-4oçš„è¡¨ç°ä¼˜äºResNet50åŸºå‡†çº¿ã€‚åœ¨Wikidataæ•°æ®é›†ä¸Šï¼Œå‡†ç¡®ç‡æ˜¾è‘—ä¸‹é™ï¼ŒSiglipè¾¾åˆ°æœ€é«˜å‡†ç¡®ç‡ï¼Œè¿™è¡¨æ˜æ¨¡å‹å¯¹å›¾åƒå¤§å°å’Œå…ƒæ•°æ®å¯¹é½çš„æ•æ„Ÿæ€§ã€‚ç”¨ç±»åˆ«æè¿°ä¸°å¯Œæç¤ºé€šå¸¸å¯ä»¥æé«˜é›¶æ ·æœ¬æ€§èƒ½ï¼Œè€Œå°‘é‡å­¦ä¹ äº§ç”Ÿçš„ç»“æœè¾ƒä½ï¼Œåªæœ‰å¶å°”å’Œå¾®å°çš„å‡†ç¡®ç‡æå‡ã€‚æˆ‘ä»¬å¾—å‡ºç»“è®ºï¼Œé€šç”¨å¤šæ¨¡æ€LLMsèƒ½å¤Ÿè¿›è¡Œè§†è§‰å¤æ‚æ–‡åŒ–é—äº§é¢†åŸŸçš„åˆ†ç±»ã€‚è¿™äº›ç»“æœæ”¯æŒå°†LLMsåº”ç”¨äºæ•°å­—äººæ–‡å·¥ä½œæµç¨‹ä¸­çš„å…ƒæ•°æ®æ•´ç†å·¥å…·ï¼Œå¹¶å»ºè®®æœªæ¥ç ”ç©¶ä¼˜åŒ–æç¤ºå¹¶æ‰©å¤§ç ”ç©¶åˆ°å…¶ä»–åˆ†ç±»ç­–ç•¥å’Œæ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.18839v1">PDF</a> 11 pages, 2 figures</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬ç ”ç©¶è¯„ä¼°äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨åŸºç£æ•™è‚–åƒå­¦å•æ ‡ç­¾åˆ†ç±»ä»»åŠ¡ä¸­çš„èƒ½åŠ›ã€‚ç ”ç©¶æ—¨åœ¨æ¢è®¨é€šç”¨VLMsï¼ˆCLIPå’ŒSigLIPï¼‰å’ŒLLMsï¼ˆå¦‚GPT-4oå’ŒGemini 2.5ï¼‰æ˜¯å¦èƒ½è§£è¯»é€šå¸¸é€šè¿‡ç›‘ç£åˆ†ç±»å™¨å¤„ç†çš„è‚–åƒå­¦ï¼Œå¹¶è¯„ä¼°å…¶æ€§èƒ½ã€‚ç ”ç©¶æå‡ºäº†ä¸¤ä¸ªé—®é¢˜ï¼šRQ1ï¼‰å¤šæ¨¡æ€LLMsåœ¨åŸºç£æ•™åœ£äººå›¾åƒåˆ†ç±»ä¸­çš„è¡¨ç°å¦‚ä½•ï¼ŸRQ2ï¼‰å½“è¾“å…¥ä¿¡æ¯å¢åŠ ä¸Šä¸‹æ–‡ä¿¡æ¯æˆ–å°‘é‡çš„æ ·æœ¬èŒƒä¾‹æ—¶ï¼Œæ€§èƒ½å¦‚ä½•å˜åŒ–ï¼Ÿç ”ç©¶ä½¿ç”¨äº†æ”¯æŒIconclassçš„ä¸‰ä¸ªæ•°æ®é›†ï¼šArtDLã€ICONCLASSå’ŒWikidataï¼Œç­›é€‰å‡ºå‰åä¸ªæœ€é¢‘ç¹çš„ç±»åˆ«ã€‚æ¨¡å‹åœ¨ä¸‰ç§æ¡ä»¶ä¸‹è¿›è¡Œæµ‹è¯•ï¼š1ï¼‰ä½¿ç”¨ç±»åˆ«æ ‡ç­¾è¿›è¡Œåˆ†ç±»ï¼Œ2ï¼‰ä½¿ç”¨Iconclassæè¿°è¿›è¡Œåˆ†ç±»ï¼Œ3ï¼‰äº”ä¸ªèŒƒä¾‹çš„å°‘é‡æ ·æœ¬å­¦ä¹ ã€‚ç»“æœä¸åœ¨åŒä¸€æ•°æ®é›†ä¸Šå¾®è°ƒè¿‡çš„ResNet50åŸºå‡†æ¨¡å‹è¿›è¡Œäº†æ¯”è¾ƒã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒGemini-2.5 Proå’ŒGPT-4oçš„è¡¨ç°ä¼˜äºResNet50åŸºå‡†æ¨¡å‹ã€‚åœ¨Wikidataæ•°æ®é›†ä¸Šï¼Œå‡†ç¡®æ€§æ˜¾è‘—ä¸‹é™ï¼ŒSiglipè¾¾åˆ°æœ€é«˜å‡†ç¡®ç‡ï¼Œè¿™è¡¨æ˜æ¨¡å‹å¯¹å›¾åƒå¤§å°å’Œå…ƒæ•°æ®å¯¹é½è¾ƒä¸ºæ•æ„Ÿã€‚ä¸€èˆ¬è€Œè¨€ï¼Œé€šè¿‡ç±»æè¿°ä¸°å¯Œæç¤ºä¿¡æ¯å¯ä»¥æé«˜é›¶æ ·æœ¬æ€§èƒ½ï¼Œè€Œå°‘é‡æ ·æœ¬å­¦ä¹ åˆ™äº§ç”Ÿè¾ƒä½ç»“æœï¼Œä»…å¶å°”å‡ºç°å‡†ç¡®åº¦çš„å¾®å°å¢é•¿ã€‚æˆ‘ä»¬å¾—å‡ºç»“è®ºï¼Œé€šç”¨å¤šæ¨¡æ€LLMsæœ‰èƒ½åŠ›è¿›è¡Œè§†è§‰å¤æ‚æ–‡åŒ–é—äº§é¢†åŸŸçš„åˆ†ç±»ã€‚è¿™äº›ç»“æœæ”¯æŒå°†LLMsåº”ç”¨äºæ•°å­—äººæ–‡å·¥ä½œæµç¨‹ä¸­çš„å…ƒæ•°æ®æ•´ç†å·¥å…·ï¼Œå¹¶å»ºè®®æœªæ¥ç ”ç©¶ä¼˜åŒ–æç¤ºå¹¶æ‰©å¤§ç ”ç©¶èŒƒå›´æ¶µç›–å…¶ä»–åˆ†ç±»ç­–ç•¥å’Œæ¨¡å‹ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨åŸºç£æ•™è‚–åƒå­¦å•æ ‡ç­¾åˆ†ç±»ä»»åŠ¡ä¸­å±•ç°å‡ºäº†èƒ½åŠ›ã€‚</li>
<li>ç ”ç©¶å¯¹æ¯”äº†åŒ…æ‹¬CLIPã€SigLIPåœ¨å†…çš„è§†è§‰è¯­è¨€æ¨¡å‹å’ŒGPT-4oã€Gemini 2.5ç­‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å›¾åƒåˆ†ç±»ä¸Šçš„è¡¨ç°ã€‚</li>
<li>ç ”ç©¶ç»“æœè¡¨æ˜Gemini-2.5 Proå’ŒGPT-4oç›¸è¾ƒäºResNet50åŸºå‡†æ¨¡å‹åœ¨åˆ†ç±»ä»»åŠ¡ä¸Šè¡¨ç°æ›´ä¼˜ã€‚</li>
<li>æ¨¡å‹å¯¹å›¾åƒå¤§å°å’Œå…ƒæ•°æ®å¯¹é½çš„æ•æ„Ÿæ€§è¾ƒé«˜ï¼Œè¿™åœ¨Wikidataæ•°æ®é›†ä¸Šçš„å‡†ç¡®æ€§ä¸‹é™ä¸­å¾—åˆ°äº†ä½“ç°ã€‚</li>
<li>é€šè¿‡ç±»æè¿°ä¸°å¯Œæç¤ºä¿¡æ¯å¯ä»¥æé«˜æ¨¡å‹çš„é›¶æ ·æœ¬æ€§èƒ½ã€‚</li>
<li>å°‘é‡æ ·æœ¬å­¦ä¹ åœ¨åˆ†ç±»ä»»åŠ¡ä¸­çš„æ•ˆæœå¹¶ä¸ç†æƒ³ï¼Œä»…å¶å°”å‡ºç°å‡†ç¡®åº¦çš„å¾®å°å¢é•¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.18839">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Vision Transformer/2509.18839v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Vision Transformer/2509.18839v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Visual-Instruction-Pretraining-for-Domain-Specific-Foundation-Models"><a href="#Visual-Instruction-Pretraining-for-Domain-Specific-Foundation-Models" class="headerlink" title="Visual Instruction Pretraining for Domain-Specific Foundation Models"></a>Visual Instruction Pretraining for Domain-Specific Foundation Models</h2><p><strong>Authors:Yuxuan Li, Yicheng Zhang, Wenhao Tang, Yimian Dai, Ming-Ming Cheng, Xiang Li, Jian Yang</strong></p>
<p>Modern computer vision is converging on a closed loop in which perception, reasoning and generation mutually reinforce each other. However, this loop remains incomplete: the top-down influence of high-level reasoning on the foundational learning of low-level perceptual features is not yet underexplored. This paper addresses this gap by proposing a new paradigm for pretraining foundation models in downstream domains. We introduce Visual insTruction Pretraining (ViTP), a novel approach that directly leverages reasoning to enhance perception. ViTP embeds a Vision Transformer (ViT) backbone within a Vision-Language Model and pretrains it end-to-end using a rich corpus of visual instruction data curated from target downstream domains. ViTP is powered by our proposed Visual Robustness Learning (VRL), which compels the ViT to learn robust and domain-relevant features from a sparse set of visual tokens. Extensive experiments on 16 challenging remote sensing and medical imaging benchmarks demonstrate that ViTP establishes new state-of-the-art performance across a diverse range of downstream tasks. The code is available at <a target="_blank" rel="noopener" href="https://github.com/zcablii/ViTP">https://github.com/zcablii/ViTP</a>. </p>
<blockquote>
<p>ç°ä»£è®¡ç®—æœºè§†è§‰æ­£åœ¨å½¢æˆä¸€ä¸ªé—­ç¯ï¼Œæ„ŸçŸ¥ã€æ¨ç†å’Œç”Ÿæˆåœ¨å½¼æ­¤ä¹‹é—´äº’ç›¸å¼ºåŒ–ã€‚ç„¶è€Œï¼Œè¿™ä¸ªå¾ªç¯ä»ç„¶ä¸å®Œæ•´ï¼šé«˜çº§æ¨ç†å¯¹ä½çº§æ„ŸçŸ¥ç‰¹å¾åŸºç¡€å­¦ä¹ çš„è‡ªä¸Šè€Œä¸‹å½±å“å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æœ¬æ–‡æ—¨åœ¨é€šè¿‡ä¸ºä¸‹æ¸¸é¢†åŸŸçš„åŸºç¡€æ¨¡å‹é¢„è®­ç»ƒæå‡ºä¸€ç§æ–°çš„èŒƒå¼æ¥è§£å†³è¿™ä¸€å·®è·ã€‚æˆ‘ä»¬å¼•å…¥äº†è§†è§‰æŒ‡ä»¤é¢„è®­ç»ƒï¼ˆViTPï¼‰ï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨æ¨ç†æ¥ç›´æ¥å¢å¼ºæ„ŸçŸ¥çš„æ–°å‹æ–¹æ³•ã€‚ViTPå°†è§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰ä¸»å¹²åµŒå…¥åˆ°è§†è§‰è¯­è¨€æ¨¡å‹ä¸­ï¼Œå¹¶ä½¿ç”¨ä»ç›®æ ‡ä¸‹æ¸¸é¢†åŸŸç²¾å¿ƒæŒ‘é€‰çš„ä¸°å¯Œè§†è§‰æŒ‡ä»¤æ•°æ®å¯¹å…¶è¿›è¡Œç«¯åˆ°ç«¯é¢„è®­ç»ƒã€‚ViTPç”±æˆ‘ä»¬æå‡ºçš„è§†è§‰ç¨³å¥æ€§å­¦ä¹ ï¼ˆVRLï¼‰é©±åŠ¨ï¼Œå®ƒè¿«ä½¿ViTä»ç¨€ç–çš„è§†è§‰ç¬¦å·é›†ä¸­å­¦ä¹ ç¨³å¥å’Œä¸é¢†åŸŸç›¸å…³çš„ç‰¹å¾ã€‚åœ¨16ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„é¥æ„Ÿå’ŒåŒ»å­¦å½±åƒåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒViTPåœ¨ä¸åŒçš„ä¸‹æ¸¸ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æ–°çš„æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/zcablii/ViTP%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/zcablii/ViTPæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.17562v2">PDF</a> </p>
<p><strong>Summary</strong><br>è§†è§‰æŒ‡ä»¤é¢„è®­ç»ƒï¼ˆViTPï¼‰æ˜¯ä¸€ç§æ–°å‹çš„é¢„è®­ç»ƒèŒƒå¼ï¼Œæ—¨åœ¨åˆ©ç”¨æ¨ç†æ¥å¢å¼ºæ„ŸçŸ¥èƒ½åŠ›ã€‚è¯¥ç ”ç©¶é€šè¿‡å¼•å…¥è§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰éª¨å¹²ç½‘ä¸è§†è§‰è¯­è¨€æ¨¡å‹ç›¸ç»“åˆçš„æ–¹å¼ï¼Œåˆ©ç”¨æ¥è‡ªç›®æ ‡ä¸‹æ¸¸é¢†åŸŸä¸°å¯Œçš„è§†è§‰æŒ‡ä»¤æ•°æ®ï¼Œå¯¹æ¨¡å‹è¿›è¡Œç«¯åˆ°ç«¯çš„é¢„è®­ç»ƒã€‚è¯¥ç ”ç©¶è¿˜æå‡ºäº†è§†è§‰ç¨³å¥æ€§å­¦ä¹ ï¼ˆVRLï¼‰ï¼Œä½¿å¾—ViTPèƒ½å¤Ÿä»å°‘é‡çš„è§†è§‰æ ‡è®°ä¸­å­¦ä¹ ç¨³å¥ä¸”ä¸é¢†åŸŸç›¸å…³çš„ç‰¹å¾ã€‚åœ¨è¿œç¨‹é¥æ„Ÿå’ŒåŒ»å­¦å½±åƒç­‰é¢†åŸŸçš„å¤šä¸ªæŒ‘æˆ˜ä»»åŠ¡ä¸Šï¼ŒViTPå–å¾—äº†æ–°çš„æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°ä»£è®¡ç®—æœºè§†è§‰æ­£åœ¨å½¢æˆä¸€ä¸ªæ„ŸçŸ¥ã€æ¨ç†å’Œç”Ÿæˆç›¸äº’åŠ å¼ºçš„é—­ç¯ï¼Œä½†ä»éœ€ç ”ç©¶é«˜æ°´å¹³æ¨ç†å¯¹åŸºç¡€ä½çº§åˆ«æ„ŸçŸ¥ç‰¹å¾å­¦ä¹ çš„å½±å“ã€‚</li>
<li>è§†è§‰æŒ‡ä»¤é¢„è®­ç»ƒï¼ˆViTPï¼‰è§£å†³äº†è¿™ä¸€ç©ºç™½ï¼Œå¼•å…¥äº†ä¸€ç§æ–°æ–¹æ³•æ¥åŠ å¼ºæ„ŸçŸ¥çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>ViTPå°†è§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰éª¨å¹²ç½‘åµŒå…¥åˆ°è§†è§‰è¯­è¨€æ¨¡å‹ä¸­ï¼Œå¹¶åœ¨ç›®æ ‡ä¸‹æ¸¸é¢†åŸŸçš„æ•°æ®é›†ä¸Šè¿›è¡Œç«¯åˆ°ç«¯çš„é¢„è®­ç»ƒã€‚</li>
<li>ViTPåˆ©ç”¨äº†ä¸°å¯Œçš„è§†è§‰æŒ‡ä»¤æ•°æ®ï¼Œè¿™äº›æ•°æ®æ¥è‡ªäºç›®æ ‡ä¸‹æ¸¸é¢†åŸŸã€‚</li>
<li>è§†è§‰ç¨³å¥æ€§å­¦ä¹ ï¼ˆVRLï¼‰æ˜¯ViTPçš„æ ¸å¿ƒç»„æˆéƒ¨åˆ†ï¼Œèƒ½ä½¿æ¨¡å‹ä»ç¨€ç–çš„è§†è§‰æ ‡è®°ä¸­å­¦ä¹ ç¨³å¥å’Œä¸é¢†åŸŸç›¸å…³çš„ç‰¹å¾ã€‚</li>
<li>åœ¨å¤šä¸ªé¥æ„ŸåŠåŒ»å­¦å½±åƒçš„åŸºå‡†æµ‹è¯•ä¸­ï¼ŒViTPè¾¾åˆ°äº†æ–°çš„æœ€ä½³æ€§èƒ½æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.17562">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Vision Transformer/2509.17562v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Vision Transformer/2509.17562v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Vision Transformer/2509.17562v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Vision Transformer/2509.17562v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="ViSpec-Accelerating-Vision-Language-Models-with-Vision-Aware-Speculative-Decoding"><a href="#ViSpec-Accelerating-Vision-Language-Models-with-Vision-Aware-Speculative-Decoding" class="headerlink" title="ViSpec: Accelerating Vision-Language Models with Vision-Aware   Speculative Decoding"></a>ViSpec: Accelerating Vision-Language Models with Vision-Aware   Speculative Decoding</h2><p><strong>Authors:Jialiang Kang, Han Shu, Wenshuo Li, Yingjie Zhai, Xinghao Chen</strong></p>
<p>Speculative decoding is a widely adopted technique for accelerating inference in large language models (LLMs), yet its application to vision-language models (VLMs) remains underexplored, with existing methods achieving only modest speedups (&lt;1.5x). This gap is increasingly significant as multimodal capabilities become central to large-scale models. We hypothesize that large VLMs can effectively filter redundant image information layer by layer without compromising textual comprehension, whereas smaller draft models struggle to do so. To address this, we introduce Vision-Aware Speculative Decoding (ViSpec), a novel framework tailored for VLMs. ViSpec employs a lightweight vision adaptor module to compress image tokens into a compact representation, which is seamlessly integrated into the draft modelâ€™s attention mechanism while preserving original image positional information. Additionally, we extract a global feature vector for each input image and augment all subsequent text tokens with this feature to enhance multimodal coherence. To overcome the scarcity of multimodal datasets with long assistant responses, we curate a specialized training dataset by repurposing existing datasets and generating extended outputs using the target VLM with modified prompts. Our training strategy mitigates the risk of the draft model exploiting direct access to the target modelâ€™s hidden states, which could otherwise lead to shortcut learning when training solely on target model outputs. Extensive experiments validate ViSpec, achieving, to our knowledge, the first substantial speedup in VLM speculative decoding. Code is available at <a target="_blank" rel="noopener" href="https://github.com/KangJialiang/ViSpec">https://github.com/KangJialiang/ViSpec</a>. </p>
<blockquote>
<p>æ¨æµ‹è§£ç æ˜¯åŠ é€Ÿå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†çš„å¹¿æ³›é‡‡ç”¨çš„æŠ€æœ¯ï¼Œç„¶è€Œå®ƒåœ¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ä¸­çš„åº”ç”¨ä»ç„¶è¢«æ¢ç´¢å¾—ä¸å¤Ÿæ·±å…¥ï¼Œç°æœ‰æ–¹æ³•åªèƒ½å®ç°è¾ƒå°çš„åŠ é€Ÿï¼ˆ&lt;1.5å€ï¼‰ã€‚éšç€å¤šæ¨¡æ€èƒ½åŠ›æˆä¸ºå¤§è§„æ¨¡æ¨¡å‹çš„æ ¸å¿ƒï¼Œè¿™ä¸€å·®è·å˜å¾—è¶Šæ¥è¶Šæ˜¾è‘—ã€‚æˆ‘ä»¬å‡è®¾å¤§å‹VLMå¯ä»¥æœ‰æ•ˆåœ°é€å±‚è¿‡æ»¤æ‰å†—ä½™çš„å›¾åƒä¿¡æ¯ï¼Œè€Œä¸æŸå®³æ–‡æœ¬ç†è§£ï¼Œè€Œè¾ƒå°çš„è‰ç¨¿æ¨¡å‹åˆ™å¾ˆéš¾åšåˆ°è¿™ä¸€ç‚¹ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†é’ˆå¯¹VLMé‡èº«å®šåˆ¶çš„Vision-Aware Speculative Decodingï¼ˆViSpecï¼‰æ–°å‹æ¡†æ¶ã€‚ViSpecé‡‡ç”¨è½»é‡çº§çš„è§†è§‰é€‚é…å™¨æ¨¡å—ï¼Œå°†å›¾åƒä»¤ç‰Œå‹ç¼©æˆç´§å‡‘çš„è¡¨ç¤ºå½¢å¼ï¼Œæ— ç¼åœ°é›†æˆåˆ°è‰ç¨¿æ¨¡å‹çš„æ³¨æ„åŠ›æœºåˆ¶ä¸­ï¼ŒåŒæ—¶ä¿ç•™åŸå§‹å›¾åƒçš„ä½ç½®ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä¸ºæ¯ä¸ªè¾“å…¥å›¾åƒæå–å…¨å±€ç‰¹å¾å‘é‡ï¼Œå¹¶å°†å…¶å¢å¼ºåˆ°æ‰€æœ‰åç»­çš„æ–‡æœ¬ä»¤ç‰Œä¸­ï¼Œä»¥æé«˜å¤šæ¨¡æ€ä¸€è‡´æ€§ã€‚ä¸ºäº†å…‹æœç¼ºä¹å¸¦æœ‰é•¿åŠ©ç†å“åº”çš„å¤šæ¨¡æ€æ•°æ®é›†çš„é—®é¢˜ï¼Œæˆ‘ä»¬é€šè¿‡é‡æ–°åˆ©ç”¨ç°æœ‰æ•°æ®é›†å¹¶ç”Ÿæˆæ‰©å±•è¾“å‡ºï¼Œä½¿ç”¨ç›®æ ‡VLMå’Œä¿®æ”¹åçš„æç¤ºæ¥åˆ¶ä½œä¸“ç”¨çš„è®­ç»ƒæ•°æ®é›†ã€‚æˆ‘ä»¬çš„è®­ç»ƒç­–ç•¥å‡è½»äº†è‰ç¨¿æ¨¡å‹ç›´æ¥è®¿é—®ç›®æ ‡æ¨¡å‹çš„éšè—çŠ¶æ€çš„é£é™©ï¼Œå¦‚æœä»…åœ¨ç›®æ ‡æ¨¡å‹è¾“å‡ºä¸Šè¿›è¡Œè®­ç»ƒï¼Œè¿™å¯èƒ½ä¼šå¯¼è‡´æ·å¾„å­¦ä¹ ã€‚å¤§é‡å®éªŒéªŒè¯äº†ViSpecçš„æœ‰æ•ˆæ€§ï¼Œæ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯VLMæ¨æµ‹è§£ç ä¸­çš„é¦–æ¬¡å®è´¨æ€§åŠ é€Ÿã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/KangJialiang/ViSpec%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/KangJialiang/ViSpecæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.15235v3">PDF</a> NeurIPS 2025</p>
<p><strong>Summary</strong>ï¼š<br>é’ˆå¯¹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„æ¨ç†åŠ é€ŸæŠ€æœ¯å­˜åœ¨ä¸è¶³çš„é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºVision-Aware Speculative Decodingï¼ˆViSpecï¼‰çš„æ–°æ¡†æ¶ã€‚å®ƒé€šè¿‡è½»é‡çº§è§†è§‰é€‚é…å™¨æ¨¡å—å°†å›¾åƒä»¤ç‰Œå‹ç¼©æˆç´§å‡‘è¡¨ç¤ºï¼Œé›†æˆåˆ°è‰ç¨¿æ¨¡å‹çš„æ³¨æ„åŠ›æœºåˆ¶ä¸­ï¼ŒåŒæ—¶ä¿ç•™åŸå§‹å›¾åƒçš„ä½ç½®ä¿¡æ¯ã€‚æ­¤å¤–ï¼ŒViSpecè¿˜ä¸ºæ¯ä¸ªè¾“å…¥å›¾åƒæå–å…¨å±€ç‰¹å¾å‘é‡å¹¶å°†å…¶æ·»åŠ åˆ°æ‰€æœ‰åç»­æ–‡æœ¬ä»¤ç‰Œä¸­ï¼Œä»¥æé«˜è·¨æ¨¡æ€ä¸€è‡´æ€§ã€‚æœ¬æ–‡è§£å†³çš„æ–¹æ³•æ˜¯å®šåˆ¶è®­ç»ƒæ•°æ®é›†ä»¥åŠè®­ç»ƒç­–ç•¥çš„é—®é¢˜ï¼Œæœ€ç»ˆå®ç°äº†ä¸€ç§æ–°çš„è§†è§‰æ¨¡å‹åŠ é€ŸæŠ€æœ¯ã€‚ç›¸å…³ä»£ç å·²åœ¨GitHubä¸Šå‘å¸ƒã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ul>
<li>æœ¬æ–‡æå‡ºä¸€ç§é’ˆå¯¹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„æ¨ç†åŠ é€Ÿæ¡†æ¶â€”â€”Vision-Aware Speculative Decodingï¼ˆViSpecï¼‰ã€‚</li>
<li>ViSpecä½¿ç”¨è½»é‡çº§è§†è§‰é€‚é…å™¨æ¨¡å—å‹ç¼©å›¾åƒä»¤ç‰Œï¼ŒåŒæ—¶ä¿ç•™åŸå§‹å›¾åƒçš„ä½ç½®ä¿¡æ¯ã€‚</li>
<li>ViSpecé€šè¿‡æå–å…¨å±€ç‰¹å¾å‘é‡å¹¶å°†å…¶æ·»åŠ åˆ°æ–‡æœ¬ä»¤ç‰Œä¸­ï¼Œæé«˜è·¨æ¨¡æ€ä¸€è‡´æ€§ã€‚</li>
<li>é’ˆå¯¹ç¼ºä¹å¤šæ ·åŒ–å¤šåª’ä½“æ•°æ®é›†çš„é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºä¸€ç§ç‰¹æ®Šçš„è®­ç»ƒæ•°æ®é›†åˆ›å»ºæ–¹æ³•ï¼Œå³é€šè¿‡é‡æ–°åˆ©ç”¨ç°æœ‰æ•°æ®é›†å¹¶ä½¿ç”¨ç›®æ ‡VLMè¿›è¡Œæ‰©å±•è¾“å‡ºç”Ÿæˆä¿®æ”¹åçš„æç¤ºã€‚</li>
<li>è®­ç»ƒç­–ç•¥é¿å…äº†æ¨¡å‹ç›´æ¥è®¿é—®ç›®æ ‡æ¨¡å‹çš„éšè—çŠ¶æ€çš„é£é™©ï¼Œé˜²æ­¢äº†æ·å¾„å­¦ä¹ ç°è±¡çš„å‘ç”Ÿã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.15235">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Vision Transformer/2509.15235v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Vision Transformer/2509.15235v3/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Vision Transformer/2509.15235v3/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Vision Transformer/2509.15235v3/page_5_0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Technical-report-on-label-informed-logit-redistribution-for-better-domain-generalization-in-low-shot-classification-with-foundation-models"><a href="#Technical-report-on-label-informed-logit-redistribution-for-better-domain-generalization-in-low-shot-classification-with-foundation-models" class="headerlink" title="Technical report on label-informed logit redistribution for better   domain generalization in low-shot classification with foundation models"></a>Technical report on label-informed logit redistribution for better   domain generalization in low-shot classification with foundation models</h2><p><strong>Authors:Behraj Khan, Tahir Syed</strong></p>
<p>Confidence calibration is an emerging challenge in real-world decision systems based on foundations models when used for downstream vision classification tasks. Due to various reasons exposed, logit scores on the CLIP head remain large irrespective of whether the image-language pairs reconcile. It is difficult to address in data space, given the few-shot regime. We propose a penalty incorporated into loss objective that penalizes incorrect classifications whenever one is made during finetuning, by moving an amount of log-likelihood to the true class commensurate to the relative amplitudes of the two likelihoods. We refer to it as \textit{confidence misalignment penalty (CMP)}. Extensive experiments on $12$ vision datasets and $5$ domain generalization datasets supports the calibration performance of our method against stat-of-the-art. CMP outperforms the benchmarked prompt learning methods, demonstrating average improvement in Expected Calibration Error (ECE) by average $6.01$%, $4.01$ % at minimum and $9.72$% at maximum. </p>
<blockquote>
<p>åœ¨åŸºäºåŸºç¡€æ¨¡å‹çš„ç°å®ä¸–ç•Œå†³ç­–ç³»ç»Ÿä¸­ï¼Œå½“ç”¨äºä¸‹æ¸¸è§†è§‰åˆ†ç±»ä»»åŠ¡æ—¶ï¼Œç½®ä¿¡åº¦æ ¡å‡†æˆä¸ºä¸€ä¸ªæ–°å…´çš„æŒ‘æˆ˜ã€‚ç”±äºå„ç§åŸå› çš„æš´éœ²ï¼ŒCLIPå¤´éƒ¨ä¸Šçš„é€»è¾‘åˆ†æ•°æ— è®ºå›¾åƒ-è¯­è¨€å¯¹æ˜¯å¦åè°ƒéƒ½ä¿æŒè¾ƒå¤§ã€‚è€ƒè™‘åˆ°å°æ ·æœ¬åˆ¶åº¦ï¼Œè¿™åœ¨æ•°æ®ç©ºé—´å¾ˆéš¾è§£å†³ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åœ¨æŸå¤±ç›®æ ‡ä¸­èå…¥æƒ©ç½šçš„æ–¹æ³•ï¼Œé€šè¿‡åœ¨å¾®è°ƒæ—¶é”™è¯¯åˆ†ç±»æ—¶ç§»åŠ¨ä¸ä¸¤ä¸ªæ¦‚ç‡çš„ç›¸å¯¹å¹…åº¦ç›¸ç§°çš„å¯¹æ•°ä¼¼ç„¶åˆ°çœŸå®ç±»åˆ«æ¥æƒ©ç½šé”™è¯¯åˆ†ç±»ã€‚æˆ‘ä»¬å°†å…¶ç§°ä¸ºâ€œç½®ä¿¡åº¦ä¸åŒ¹é…æƒ©ç½šï¼ˆCMPï¼‰â€ã€‚åœ¨12ä¸ªè§†è§‰æ•°æ®é›†å’Œ5ä¸ªé¢†åŸŸæ³›åŒ–æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒæ”¯æŒæˆ‘ä»¬çš„æ–¹æ³•ä¸æœ€æ–°æŠ€æœ¯ç›¸æ¯”çš„æ ¡å‡†æ€§èƒ½ã€‚CMPä¼˜äºåŸºå‡†æç¤ºå­¦ä¹ æ–¹æ³•ï¼Œåœ¨é¢„æœŸæ ¡å‡†è¯¯å·®ï¼ˆECEï¼‰æ–¹é¢å¹³å‡æé«˜äº†6.01%ï¼Œæœ€ä½æé«˜4.01%ï¼Œæœ€é«˜æé«˜9.72%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.17595v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§å‹æ¨¡å‹çš„ä¸‹æ¸¸è§†è§‰åˆ†ç±»ä»»åŠ¡ä¸­ï¼Œç½®ä¿¡åº¦æ ¡å‡†æ˜¯ä¸€ä¸ªæ–°å…´çš„æŒ‘æˆ˜ã€‚æ–‡ç« æŒ‡å‡ºCLIPå¤´è¾“å‡ºçš„logitåˆ†æ•°è¾ƒå¤§ï¼Œéš¾ä»¥åœ¨æ•°æ®ç©ºé—´è§£å†³è¯¥é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæ–‡ç« æå‡ºäº†ä¸€ç§æŸå¤±ç›®æ ‡ä¸­çš„æƒ©ç½šæœºåˆ¶ï¼Œå³åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­ï¼Œæ¯å½“å‡ºç°é”™è¯¯åˆ†ç±»æ—¶ï¼Œå°†éƒ¨åˆ†å¯¹æ•°ä¼¼ç„¶æ¦‚ç‡è½¬ç§»åˆ°çœŸå®ç±»åˆ«ï¼Œè¯¥è½¬ç§»é‡ä¸ä¸¤ä¸ªç±»åˆ«çš„ç›¸å¯¹å¹…åº¦æœ‰å…³ã€‚è¿™ç§æƒ©ç½šè¢«ç§°ä¸ºç½®ä¿¡åº¦ä¸åŒ¹é…æƒ©ç½šï¼ˆCMPï¼‰ã€‚åœ¨å¤šä¸ªè§†è§‰æ•°æ®é›†å’Œé¢†åŸŸæ³›åŒ–æ•°æ®é›†ä¸Šçš„å®éªŒæ”¯æŒäº†è¯¥æ–¹æ³•ç›¸è¾ƒäºæœ€æ–°æŠ€æœ¯çš„æ ¡å‡†æ€§èƒ½ã€‚CMPåœ¨é¢„æœŸæ ¡å‡†è¯¯å·®ï¼ˆECEï¼‰ä¸Šå¹³å‡æé«˜äº†6.01%ï¼Œæœ€å°æé«˜4.01%ï¼Œæœ€å¤§æé«˜9.72%ï¼Œä¼˜äºåŸºå‡†æç¤ºå­¦ä¹ æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ç½®ä¿¡åº¦æ ¡å‡†æ˜¯å¤§å‹æ¨¡å‹åœ¨è§†è§‰åˆ†ç±»ä»»åŠ¡ä¸­é¢ä¸´çš„æ–°å…´æŒ‘æˆ˜ã€‚</li>
<li>CLIPå¤´è¾“å‡ºçš„logitåˆ†æ•°åœ¨å¤„ç†å›¾åƒ-è¯­è¨€å¯¹æ—¶å­˜åœ¨ä¸åŒ¹é…é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ç½®ä¿¡åº¦ä¸åŒ¹é…æƒ©ç½šï¼ˆCMPï¼‰æ–¹æ³•æ¥è§£å†³æ­¤é—®é¢˜ã€‚</li>
<li>CMPæ˜¯ä¸€ç§æŸå¤±ç›®æ ‡ä¸­çš„æƒ©ç½šæœºåˆ¶ï¼Œé€šè¿‡å¯¹é”™è¯¯åˆ†ç±»æ–½åŠ æƒ©ç½šæ¥æ”¹å–„æ¨¡å‹æ ¡å‡†ã€‚</li>
<li>CMPé€šè¿‡è°ƒæ•´å¯¹æ•°ä¼¼ç„¶æ¦‚ç‡æ¥å·¥ä½œï¼Œå°†éƒ¨åˆ†æ¦‚ç‡è½¬ç§»åˆ°çœŸå®ç±»åˆ«ã€‚</li>
<li>CMPåœ¨å¤šä¸ªè§†è§‰æ•°æ®é›†å’Œé¢†åŸŸæ³›åŒ–æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨ç°ä¼˜äºæœ€æ–°æŠ€æœ¯ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.17595">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Vision Transformer/2501.17595v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Vision Transformer/2501.17595v3/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Vision Transformer/2501.17595v3/page_5_0.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Vim-F-Visual-State-Space-Model-Benefiting-from-Learning-in-the-Frequency-Domain"><a href="#Vim-F-Visual-State-Space-Model-Benefiting-from-Learning-in-the-Frequency-Domain" class="headerlink" title="Vim-F: Visual State Space Model Benefiting from Learning in the   Frequency Domain"></a>Vim-F: Visual State Space Model Benefiting from Learning in the   Frequency Domain</h2><p><strong>Authors:Juntao Zhang, Shaogeng Liu, Jun Zhou, Kun Bian, You Zhou, Jianning Liu, Pei Zhang, Bingyan Liu</strong></p>
<p>In recent years, State Space Models (SSMs) with efficient hardware-aware designs, known as the Mamba deep learning models, have made significant progress in modeling long sequences such as language understanding. Therefore, building efficient and general-purpose visual backbones based on SSMs is a promising direction. Compared to traditional convolutional neural networks (CNNs) and Vision Transformers (ViTs), the performance of Vision Mamba (ViM) methods is not yet fully competitive. To enable SSMs to process image data, ViMs typically flatten 2D images into 1D sequences, inevitably ignoring some 2D local dependencies, thereby weakening the modelâ€™s ability to interpret spatial relationships from a global perspective. We use Fast Fourier Transform (FFT) to obtain the spectrum of the feature map and add it to the original feature map, enabling ViM to model a unified visual representation in both frequency and spatial domains. The introduction of frequency domain information enables ViM to have a global receptive field during scanning. We propose a novel model called Vim-F, which employs pure Mamba encoders and scans in both the frequency and spatial domains. Moreover, we question the necessity of position embedding in ViM and remove it accordingly in Vim-F, which helps to fully utilize the efficient long-sequence modeling capability of ViM. Finally, we redesign a patch embedding for Vim-F, leveraging a convolutional stem to capture more local correlations, further improving the performance of Vim-F. Code is available at: <a target="_blank" rel="noopener" href="https://github.com/yws-wxs/Vim-F">https://github.com/yws-wxs/Vim-F</a>. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œå…·æœ‰é«˜æ•ˆç¡¬ä»¶æ„ŸçŸ¥è®¾è®¡çš„çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMsï¼‰ï¼Œåˆç§°ä¸ºMambaæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œåœ¨å»ºæ¨¡é•¿åºåˆ—ï¼ˆå¦‚è¯­è¨€ç†è§£ï¼‰æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚å› æ­¤ï¼ŒåŸºäºSSMæ„å»ºé«˜æ•ˆä¸”é€šç”¨çš„è§†è§‰ä¸»å¹²æ˜¯ä¸€ä¸ªå……æ»¡å¸Œæœ›çš„æ–¹å‘ã€‚ä¸ä¼ ç»Ÿçš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰å’Œè§†è§‰å˜å‹å™¨ï¼ˆViTsï¼‰ç›¸æ¯”ï¼Œè§†è§‰Mambaï¼ˆViMï¼‰æ–¹æ³•çš„æ€§èƒ½å°šæœªè¾¾åˆ°å®Œå…¨ç«äº‰æ°´å¹³ã€‚ä¸ºäº†ä½¿SSMå¤„ç†å›¾åƒæ•°æ®ï¼ŒViMé€šå¸¸å°†2Då›¾åƒå±•å¹³ä¸º1Dåºåˆ—ï¼Œè¿™ä¸å¯é¿å…åœ°ä¼šå¿½ç•¥ä¸€äº›2Då±€éƒ¨ä¾èµ–æ€§ï¼Œä»è€Œå‰Šå¼±äº†æ¨¡å‹ä»å…¨å±€è§’åº¦è§£é‡Šç©ºé—´å…³ç³»çš„èƒ½åŠ›ã€‚æˆ‘ä»¬ä½¿ç”¨å¿«é€Ÿå‚…é‡Œå¶å˜æ¢ï¼ˆFFTï¼‰è·å¾—ç‰¹å¾å›¾çš„é¢‘è°±å¹¶å°†å…¶æ·»åŠ åˆ°åŸå§‹ç‰¹å¾å›¾ï¼Œä½¿ViMèƒ½å¤Ÿåœ¨é¢‘ç‡å’Œç©ºé—´åŸŸä¸­å»ºç«‹ç»Ÿä¸€çš„è§†è§‰è¡¨ç¤ºã€‚å¼•å…¥é¢‘åŸŸä¿¡æ¯ä½¿ViMåœ¨æ‰«æè¿‡ç¨‹ä¸­å…·æœ‰å…¨å±€æ„Ÿå—é‡ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹æ¨¡å‹Vim-Fï¼Œå®ƒé‡‡ç”¨çº¯Mambaç¼–ç å™¨ï¼Œå¹¶åœ¨é¢‘ç‡å’Œç©ºé—´åŸŸä¸­è¿›è¡Œæ‰«æã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¯¹ViMä¸­ä½ç½®åµŒå…¥çš„å¿…è¦æ€§æå‡ºäº†è´¨ç–‘ï¼Œå¹¶åœ¨Vim-Fä¸­ç›¸åº”åœ°å°†å…¶ç§»é™¤ï¼Œè¿™æœ‰åŠ©äºå……åˆ†åˆ©ç”¨ViMå¯¹é•¿åºåˆ—å»ºæ¨¡çš„é«˜æ•ˆèƒ½åŠ›ã€‚æœ€åï¼Œæˆ‘ä»¬ä¸ºVim-Fé‡æ–°è®¾è®¡äº†è¡¥ä¸åµŒå…¥ï¼Œåˆ©ç”¨å·ç§¯å¹²ç»†èƒæ•è·æ›´å¤šå±€éƒ¨ç›¸å…³æ€§ï¼Œè¿›ä¸€æ­¥æé«˜äº†Vim-Fçš„æ€§èƒ½ã€‚ä»£ç å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/yws-wxs/Vim-F%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/yws-wxs/Vim-Fè·å¾—ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.18679v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿‘å¹´æ¥ï¼ŒåŸºäºçŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMsï¼‰çš„é«˜æ•ˆç¡¬ä»¶æ„ŸçŸ¥è®¾è®¡ï¼Œå³æ·±åº¦å­¦ä¹ æ¨¡å‹Mambaï¼Œåœ¨å»ºæ¨¡é•¿åºåˆ—æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚å› æ­¤ï¼Œå»ºç«‹åŸºäºSSMçš„é«˜æ•ˆé€šç”¨è§†è§‰éª¨å¹²ç½‘æ˜¯ä¸€ä¸ªæœ‰å‰é€”çš„æ–¹å‘ã€‚å°½ç®¡Vision Mambaï¼ˆViMï¼‰æ–¹æ³•çš„æ€§èƒ½å°šæœªè¾¾åˆ°ä¼ ç»Ÿå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰å’Œè§†è§‰å˜å‹å™¨ï¼ˆViTsï¼‰çš„æ°´å¹³ï¼Œä½†ViMé€šè¿‡å°†å›¾åƒæ•°æ®æ‰å¹³åŒ–ä¸ºåºåˆ—æ¥å¤„ç†å›¾åƒæ•°æ®ã€‚ä¸ºäº†å¢å¼ºViMå¯¹ç©ºé—´å…³ç³»çš„å…¨å±€è§£è¯»èƒ½åŠ›ï¼Œæœ¬æ–‡å¼•å…¥äº†é¢‘è°±ä¿¡æ¯ï¼Œå¹¶æå‡ºäº†Vim-Fæ¨¡å‹ï¼Œè¯¥æ¨¡å‹é‡‡ç”¨çº¯Mambaç¼–ç å™¨ï¼Œåœ¨é¢‘ç‡åŸŸå’Œç©ºé—´åŸŸè¿›è¡Œæ‰«æï¼Œå¹¶é‡æ–°è®¾è®¡äº†è¡¥ä¸åµŒå…¥ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è´¨ç–‘äº†ViMä¸­ä½ç½®åµŒå…¥çš„å¿…è¦æ€§å¹¶ç›¸åº”åœ°åœ¨Vim-Fä¸­å°†å…¶ç§»é™¤ï¼Œä»¥å……åˆ†åˆ©ç”¨ViMå¯¹é•¿åºåˆ—å»ºæ¨¡çš„é«˜æ•ˆèƒ½åŠ›ã€‚ä»£ç å·²å…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMsï¼‰åœ¨å»ºæ¨¡é•¿åºåˆ—æ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ï¼Œè¢«åº”ç”¨äºæ·±åº¦å­¦ä¹ æ¨¡å‹Mambaã€‚</li>
<li>å»ºç«‹åŸºäºSSMçš„é«˜æ•ˆé€šç”¨è§†è§‰éª¨å¹²ç½‘æ˜¯ä¸€ä¸ªæœ‰å‰é€”çš„ç ”ç©¶æ–¹å‘ã€‚</li>
<li>Vision Mambaï¼ˆViMï¼‰æ–¹æ³•åœ¨å¤„ç†å›¾åƒæ•°æ®æ—¶å°†å…¶æ‰å¹³åŒ–ä¸ºåºåˆ—ï¼Œä½†æ€§èƒ½å°šæœªè¾¾åˆ°ä¼ ç»ŸCNNså’ŒViTsçš„æ°´å¹³ã€‚</li>
<li>é€šè¿‡å¼•å…¥é¢‘è°±ä¿¡æ¯å¢å¼ºViMå¯¹ç©ºé—´å…³ç³»çš„å…¨å±€è§£è¯»èƒ½åŠ›ã€‚</li>
<li>æå‡ºäº†Vim-Fæ¨¡å‹ï¼Œé‡‡ç”¨çº¯Mambaç¼–ç å™¨ï¼Œåœ¨é¢‘ç‡åŸŸå’Œç©ºé—´åŸŸè¿›è¡Œæ‰«æã€‚</li>
<li>Vim-Fé‡æ–°è®¾è®¡äº†è¡¥ä¸åµŒå…¥å¹¶ç§»é™¤ä½ç½®åµŒå…¥ä»¥æé«˜æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.18679">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Vision Transformer/2405.18679v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Vision Transformer/2405.18679v3/page_3_0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="CLIP-Can-Understand-Depth"><a href="#CLIP-Can-Understand-Depth" class="headerlink" title="CLIP Can Understand Depth"></a>CLIP Can Understand Depth</h2><p><strong>Authors:Sohee Kim, Jisu Kang, Dunam Kim, Seokju Lee</strong></p>
<p>In this paper, we demonstrate that CLIP can also be adapted to downstream tasks where its vision-language alignment is suboptimally learned during pre-training on web-crawled data, all without requiring fine-tuning. We explore the case of monocular depth estimation, where CLIPâ€™s contrastive prior struggles to generalize, compared to its success in domains such as generative modeling and semantic segmentation. Since CLIP fails to consistently capture similarities between image patches and natural language prompts describing distance, we eliminate the use of its pre-trained natural language token embeddings and distill the semantic prior of its frozen text encoder into a single learnable embedding matrix called â€œmirrorâ€. The main design goal of mirror is to derive a non-human language prompt that approximates an optimal natural language prompt: â€œHow far is this location from the camera?â€ Using this approach, we jointly train two lightweight modules, a mirror and a compact decoder, on top of a frozen CLIP for dense depth prediction. Compared to conventional depth models, our framework is significantly more efficient in terms of parameters and computation. The resulting model exhibits impressive performance, matching several state-of-the-art vision models on the NYU Depth v2 and KITTI benchmark datasets, while outperforming all vision-language depth models based on a frozen CLIP prior. Experiments demonstrate that the suboptimal depth understanding of CLIP in terms of spatial and temporal consistency can be significantly corrected without either fine-tuning it or concatenating mirror with its pre-trained subword token embeddings. Furthermore, an ablation study on the convergence status of mirror shows that it is implicitly trained to capture objects, such as humans and windows, where semantic cues play an important role in detection. </p>
<blockquote>
<p>æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†CLIPä¹Ÿå¯ä»¥é€‚åº”ä¸‹æ¸¸ä»»åŠ¡ï¼Œè¿™äº›ä»»åŠ¡åœ¨ç½‘é¡µçˆ¬è™«æ•°æ®çš„é¢„è®­ç»ƒè¿‡ç¨‹ä¸­è§†è§‰è¯­è¨€å¯¹é½æ˜¯æ¬¡ä¼˜çš„ï¼Œè€Œä¸”è¿™ä¸€åˆ‡éƒ½ä¸éœ€è¦å¾®è°ƒã€‚æˆ‘ä»¬ç ”ç©¶äº†å•ç›®æ·±åº¦ä¼°è®¡çš„æƒ…å†µï¼ŒCLIPçš„å¯¹æ¯”å…ˆéªŒåœ¨é€šç”¨æ€§æ–¹é¢é‡åˆ°äº†å›°éš¾ï¼Œä¸å…¶åœ¨ç”Ÿæˆå»ºæ¨¡å’Œè¯­ä¹‰åˆ†å‰²ç­‰é¢†åŸŸçš„æˆåŠŸç›¸æ¯”å½¢æˆé²œæ˜å¯¹æ¯”ã€‚ç”±äºCLIPæ— æ³•æŒç»­æ•è·å›¾åƒè¡¥ä¸ä¸è‡ªç„¶è¯­è¨€æç¤ºä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œè¿™äº›æç¤ºæè¿°äº†è·ç¦»ï¼Œå› æ­¤æˆ‘ä»¬æ¶ˆé™¤äº†é¢„è®­ç»ƒçš„è‡ªç„¶è¯­è¨€ä»¤ç‰ŒåµŒå…¥çš„ä½¿ç”¨ï¼Œå¹¶å°†å†»ç»“æ–‡æœ¬ç¼–ç å™¨çš„è¯­ä¹‰å…ˆéªŒç®€åŒ–ä¸ºä¸€ä¸ªç§°ä¸ºâ€œé•œåƒâ€çš„å¯å­¦ä¹ åµŒå…¥çŸ©é˜µã€‚é•œåƒçš„ä¸»è¦è®¾è®¡ç›®æ ‡æ˜¯æ¨å¯¼å‡ºä¸€ç§éäººç±»è¯­è¨€æç¤ºï¼Œè¯¥æç¤ºè¿‘ä¼¼äºæœ€ä½³è‡ªç„¶è¯­è¨€æç¤ºï¼šâ€œè¿™ä¸ªä½ç½®ç¦»ç›¸æœºæœ‰å¤šè¿œï¼Ÿâ€é‡‡ç”¨è¿™ç§æ–¹æ³•ï¼Œæˆ‘ä»¬åœ¨å†»ç»“CLIPä¹‹ä¸Šè”åˆè®­ç»ƒäº†ä¸¤ä¸ªè½»é‡çº§æ¨¡å—â€”â€”ä¸€ä¸ªé•œåƒå’Œä¸€ä¸ªç´§å‡‘çš„è§£ç å™¨â€”â€”ç”¨äºå¯†é›†çš„æ·±åº¦é¢„æµ‹ã€‚ä¸ä¼ ç»Ÿçš„æ·±åº¦æ¨¡å‹ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ¡†æ¶åœ¨å‚æ•°å’Œè®¡ç®—æ–¹é¢æ›´åŠ é«˜æ•ˆã€‚æ‰€å¾—æ¨¡å‹æ€§èƒ½ä»¤äººå°è±¡æ·±åˆ»ï¼Œåœ¨NYU Depth v2å’ŒKITTIåŸºå‡†æ•°æ®é›†ä¸Šè¾¾åˆ°äº†è®¸å¤šæœ€å…ˆè¿›çš„è§†è§‰æ¨¡å‹çš„æ°´å¹³ï¼ŒåŒæ—¶è¶…è¶Šäº†æ‰€æœ‰åŸºäºå†»ç»“CLIPå…ˆéªŒçš„è§†è§‰è¯­è¨€æ·±åº¦æ¨¡å‹ã€‚å®éªŒè¡¨æ˜ï¼ŒCLIPåœ¨ç©ºé—´å’Œæ—¶é—´ä¸€è‡´æ€§æ–¹é¢çš„æ·±åº¦ç†è§£ä¸ä½³å¯ä»¥å¾—åˆ°æ˜¾è‘—çº æ­£ï¼Œè€Œæ— éœ€å¯¹å…¶è¿›è¡Œå¾®è°ƒæˆ–å°†é•œåƒä¸é¢„è®­ç»ƒçš„å­è¯ä»¤ç‰ŒåµŒå…¥è¿›è¡Œæ‹¼æ¥ã€‚æ­¤å¤–ï¼Œå¯¹é•œåƒæ”¶æ•›çŠ¶æ€çš„æ¶ˆèç ”ç©¶è¡¨æ˜ï¼Œå®ƒç»è¿‡éšæ€§è®­ç»ƒä»¥æ•è·å¯¹è±¡ï¼ˆå¦‚äººç±»å’Œçª—æˆ·ï¼‰ï¼Œå…¶ä¸­è¯­ä¹‰çº¿ç´¢åœ¨æ£€æµ‹ä¸­èµ·ç€é‡è¦ä½œç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2402.03251v2">PDF</a> Accepted in Pattern Recognition, 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢ç´¢äº†CLIPæ¨¡å‹åœ¨ä¸‹æ¸¸ä»»åŠ¡å¦‚å•çœ¼æ·±åº¦ä¼°è®¡ä¸­çš„é€‚åº”æ€§ã€‚ç”±äºCLIPåœ¨é¢„è®­ç»ƒæœŸé—´æœªèƒ½ä¼˜åŒ–å›¾åƒå—ä¸è‡ªç„¶è¯­è¨€æè¿°ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œæœ¬ç ”ç©¶ä¸ä½¿ç”¨å…¶é¢„è®­ç»ƒçš„è‡ªç„¶è¯­è¨€ä»¤ç‰ŒåµŒå…¥ï¼Œè€Œæ˜¯æç‚¼å‡ºå†»ç»“æ–‡æœ¬ç¼–ç å™¨çš„è¯­ä¹‰å…ˆéªŒï¼Œå½¢æˆä¸€ä¸ªåä¸ºâ€œé•œåƒâ€çš„å¯å­¦ä¹ åµŒå…¥çŸ©é˜µã€‚é€šè¿‡è”åˆè®­ç»ƒé•œåƒå’Œç´§å‡‘è§£ç å™¨ï¼Œå®ç°äº†å¯¹å†»ç»“CLIPçš„æ·±åº¦é¢„æµ‹ã€‚è¯¥æ–¹æ³•æ˜¾è‘—æé«˜äº†æ€§èƒ½ï¼Œä¸ä¸€æµè§†è§‰æ¨¡å‹åŒ¹é…ï¼Œå¹¶è¶…è¶Šäº†åŸºäºå†»ç»“CLIPå…ˆéªŒçš„æ‰€æœ‰è§†è§‰è¯­è¨€æ·±åº¦æ¨¡å‹ã€‚ç ”ç©¶è¿˜è¡¨æ˜ï¼ŒCLIPçš„æ·±åº¦ç†è§£å¯ä»¥åœ¨ä¸å¾®è°ƒæˆ–ç»“åˆå…¶é¢„è®­ç»ƒå­è¯ä»¤ç‰ŒåµŒå…¥çš„æƒ…å†µä¸‹å¾—åˆ°æ˜¾è‘—æ”¹è¿›ã€‚é•œåƒçš„æ”¶æ•›çŠ¶æ€ç ”ç©¶è¿˜æ˜¾ç¤ºå…¶èƒ½å¤Ÿæ•æ‰åˆ°å¦‚äººç±»å’Œçª—æˆ·ç­‰å¯¹è±¡ï¼Œå…¶ä¸­è¯­ä¹‰çº¿ç´¢åœ¨æ£€æµ‹ä¸­èµ·åˆ°äº†é‡è¦ä½œç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯æœ¬æ–‡çš„ä¸»è¦è§è§£ï¼š</p>
<ol>
<li>CLIPæ¨¡å‹å¯é€‚åº”ä¸‹æ¸¸ä»»åŠ¡ï¼Œå³ä½¿å…¶åœ¨ç½‘ç»œçˆ¬è™«æ•°æ®é¢„è®­ç»ƒä¸­çš„è§†è§‰è¯­è¨€å¯¹é½æ˜¯æ¬¡ä¼˜çš„ã€‚</li>
<li>åœ¨å•çœ¼æ·±åº¦ä¼°è®¡ä»»åŠ¡ä¸­ï¼ŒCLIPçš„å¯¹æ¯”ä¼˜å…ˆç­–ç•¥ç›¸è¾ƒäºå…¶åœ¨ç”Ÿæˆå»ºæ¨¡å’Œè¯­ä¹‰åˆ†å‰²ç­‰é¢†åŸŸçš„åº”ç”¨è¡¨ç°è¾ƒå·®ã€‚</li>
<li>ç”±äºCLIPæ— æ³•æŒç»­æ•æ‰å›¾åƒå—ä¸è‡ªç„¶è¯­è¨€æè¿°ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œç ”ç©¶æå‡ºäº†åä¸ºâ€œé•œåƒâ€çš„å¯å­¦ä¹ åµŒå…¥çŸ©é˜µæ¥æç‚¼è¯­ä¹‰å…ˆéªŒã€‚</li>
<li>é€šè¿‡è”åˆè®­ç»ƒé•œåƒå’Œç´§å‡‘è§£ç å™¨ï¼Œå®ç°äº†å¯¹å†»ç»“CLIPçš„æ·±åº¦é¢„æµ‹ï¼Œæ˜¾è‘—æé«˜äº†æ€§èƒ½ã€‚</li>
<li>è¯¥æ–¹æ³•è¶…è¶Šäº†åŸºäºå†»ç»“CLIPå…ˆéªŒçš„æ‰€æœ‰è§†è§‰è¯­è¨€æ·±åº¦æ¨¡å‹ï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ã€‚</li>
<li>CLIPçš„æ·±åº¦ç†è§£å¯ä»¥åœ¨ä¸å¾®è°ƒæˆ–ç»“åˆå…¶é¢„è®­ç»ƒå­è¯ä»¤ç‰ŒåµŒå…¥çš„æƒ…å†µä¸‹å¾—åˆ°æ”¹è¿›ã€‚è¿™è¡¨æ˜CLIPæ¨¡å‹å…·æœ‰è¾ƒå¤§çš„ä¼˜åŒ–æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2402.03251">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Vision Transformer/2402.03251v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Vision Transformer/2402.03251v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Vision Transformer/2402.03251v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-28/Vision%20Transformer/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-28/Vision%20Transformer/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Vision-Transformer/">
                                    <span class="chip bg-color">Vision Transformer</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-28/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2407.01016v2/page_0_0.jpg" class="responsive-img" alt="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª">
                        
                        <span class="card-title">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-28  SwinMamba A hybrid local-global mamba framework for enhancing semantic   segmentation of remotely sensed images
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-28
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                    æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                        <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-28/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_è§†é¢‘ç†è§£/2506.17873v2/page_2_0.jpg" class="responsive-img" alt="è§†é¢‘ç†è§£">
                        
                        <span class="card-title">è§†é¢‘ç†è§£</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            è§†é¢‘ç†è§£ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-28  VIR-Bench Evaluating Geospatial and Temporal Understanding of MLLMs via   Travel Video Itinerary Reconstruction
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-28
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                    è§†é¢‘ç†è§£
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                        <span class="chip bg-color">è§†é¢‘ç†è§£</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">29739.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
