<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="NeRF">
    <meta name="description" content="NeRF 方向最新论文已更新，请持续关注 Update in 2025-09-28  Integrating Object Interaction Self-Attention and GAN-Based Debiasing   for Visual Question Answering">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>NeRF | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic-private.zhihu.com/v2-cdc7ffdf5f932180a83612b9c0ec803a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760038353&auth_key=1760038353-0-0-d06f619d3b00a9ced5afeb5212c1e0eb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">NeRF</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/NeRF/">
                                <span class="chip bg-color">NeRF</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                NeRF
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-09-28
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-10-11
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    4.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    20 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-09-28-更新"><a href="#2025-09-28-更新" class="headerlink" title="2025-09-28 更新"></a>2025-09-28 更新</h1><h2 id="Integrating-Object-Interaction-Self-Attention-and-GAN-Based-Debiasing-for-Visual-Question-Answering"><a href="#Integrating-Object-Interaction-Self-Attention-and-GAN-Based-Debiasing-for-Visual-Question-Answering" class="headerlink" title="Integrating Object Interaction Self-Attention and GAN-Based Debiasing   for Visual Question Answering"></a>Integrating Object Interaction Self-Attention and GAN-Based Debiasing   for Visual Question Answering</h2><p><strong>Authors:Zhifei Li, Feng Qiu, Yiran Wang, Yujing Xia, Kui Xiao, Miao Zhang, Yan Zhang</strong></p>
<p>Visual Question Answering (VQA) presents a unique challenge by requiring models to understand and reason about visual content to answer questions accurately. Existing VQA models often struggle with biases introduced by the training data, leading to over-reliance on superficial patterns and inadequate generalization to diverse questions and images. This paper presents a novel model, IOG-VQA, which integrates Object Interaction Self-Attention and GAN-Based Debiasing to enhance VQA model performance. The self-attention mechanism allows our model to capture complex interactions between objects within an image, providing a more comprehensive understanding of the visual context. Meanwhile, the GAN-based debiasing framework generates unbiased data distributions, helping the model to learn more robust and generalizable features. By leveraging these two components, IOG-VQA effectively combines visual and textual information to address the inherent biases in VQA datasets. Extensive experiments on the VQA-CP v1 and VQA-CP v2 datasets demonstrate that our model shows excellent performance compared with the existing methods, particularly in handling biased and imbalanced data distributions highlighting the importance of addressing both object interactions and dataset biases in advancing VQA tasks. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/HubuKG/IOG-VQA">https://github.com/HubuKG/IOG-VQA</a>. </p>
<blockquote>
<p>视觉问答（VQA）提出了一个独特的挑战，要求模型理解和推理视觉内容以准确回答问题。现有的VQA模型经常受到训练数据引入的偏见的影响，导致过分依赖表面模式，对多样的问题和图像泛化能力不足。本文提出了一种新型模型IOG-VQA，它集成了对象交互自注意力机制和基于GAN的去偏置技术，以提高VQA模型的性能。自注意力机制允许我们的模型捕捉图像内对象之间的复杂交互，提供更全面的视觉上下文理解。同时，基于GAN的去偏框架生成无偏的数据分布，帮助模型学习更稳健和可泛化的特征。通过利用这两个组件，IOG-VQA有效地结合了视觉和文本信息，以解决VQA数据集中的固有偏见。在VQA-CP v1和VQA-CP v2数据集上的大量实验表明，与现有方法相比，我们的模型表现出优异的性能，特别是在处理偏见和不平衡的数据分布时，这突显了在推进VQA任务时解决对象交互和数据集偏见的重要性。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/HubuKG/IOG-VQA%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/HubuKG/IOG-VQA找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.20884v1">PDF</a> 14 pages, 6 figures. ACCEPTED for publication as a REGULAR paper in   the IEEE Transactions on Multimedia 2025</p>
<p><strong>Summary</strong><br>视觉问答（VQA）要求模型理解和推理视觉内容以准确回答问题，这带来了独特的挑战。现有VQA模型常因训练数据引入的偏见而过度依赖表面模式，且对多样的问题和图像泛化能力有限。本文提出一种新型模型IOG-VQA，它集成了对象交互自注意力机制和基于GAN的去偏框架，以提升VQA模型的性能。自注意力机制让模型能捕捉图像内对象间的复杂交互，为视觉上下文提供更全面的理解。基于GAN的去偏框架则生成无偏见的数据分布，帮助模型学习更稳健和可泛化的特征。利用这两个组件，IOG-VQA有效地结合了视觉和文本信息，解决VQA数据集固有的偏见问题。在VQA-CP v1和VQA-CP v2数据集上的实验显示，我们的模型相较于现有方法表现出卓越性能，特别是在处理有偏见和不平衡的数据分布时，强调了在推进VQA任务时同时处理对象交互和数据集偏见的重要性。我们的代码公开在：<a target="_blank" rel="noopener" href="https://github.com/HubuKG/IOG-VQA%E3%80%82">https://github.com/HubuKG/IOG-VQA。</a> </p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VQA模型面临由训练数据引入的偏见问题，导致过度依赖表面模式和泛化能力有限。</li>
<li>IOG-VQA模型通过集成对象交互自注意力机制，提高模型对图像内对象间复杂交互的理解。</li>
<li>基于GAN的去偏框架帮助生成无偏见的数据分布，促进模型学习更稳健和可泛化的特征。</li>
<li>IOG-VQA结合视觉和文本信息，有效解决VQA数据集的固有偏见问题。</li>
<li>在VQA-CP v1和VQA-CP v2数据集上的实验显示，IOG-VQA模型在处理有偏见和不平衡数据分布时表现出卓越性能。</li>
<li>IOG-VQA模型公开可用，为VQA研究提供新的解决方案。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.20884">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-9cd38d945c4d6f1a7304132c8736fc1a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760038360&auth_key=1760038360-0-0-603c6e79653eb42e13e6a5a5e98ecbb6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-08aa596c7d5d56f836b04a6b19f3deb2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760038367&auth_key=1760038367-0-0-e865c8cbb570b6e21041921c6a7d72db&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-dddf1834b0d1e602871028c5cff4c2f9~resize:0:q75.jpg?source=1f5c5e47&expiration=1760038374&auth_key=1760038374-0-0-ec8d131c8cb9a152241bbe147eb94cf0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-09a0cd0b362121636f03aaa140fb2642~resize:0:q75.jpg?source=1f5c5e47&expiration=1760038381&auth_key=1760038381-0-0-2a76007d121644fdf67b0bf03947daab&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Adversarially-Refined-VQ-GAN-with-Dense-Motion-Tokenization-for-Spatio-Temporal-Heatmaps"><a href="#Adversarially-Refined-VQ-GAN-with-Dense-Motion-Tokenization-for-Spatio-Temporal-Heatmaps" class="headerlink" title="Adversarially-Refined VQ-GAN with Dense Motion Tokenization for   Spatio-Temporal Heatmaps"></a>Adversarially-Refined VQ-GAN with Dense Motion Tokenization for   Spatio-Temporal Heatmaps</h2><p><strong>Authors:Gabriel Maldonado, Narges Rashvand, Armin Danesh Pazho, Ghazal Alinezhad Noghre, Vinit Katariya, Hamed Tabkhi</strong></p>
<p>Continuous human motion understanding remains a core challenge in computer vision due to its high dimensionality and inherent redundancy. Efficient compression and representation are crucial for analyzing complex motion dynamics. In this work, we introduce an adversarially-refined VQ-GAN framework with dense motion tokenization for compressing spatio-temporal heatmaps while preserving the fine-grained traces of human motion. Our approach combines dense motion tokenization with adversarial refinement, which eliminates reconstruction artifacts like motion smearing and temporal misalignment observed in non-adversarial baselines. Our experiments on the CMU Panoptic dataset provide conclusive evidence of our method’s superiority, outperforming the dVAE baseline by 9.31% SSIM and reducing temporal instability by 37.1%. Furthermore, our dense tokenization strategy enables a novel analysis of motion complexity, revealing that 2D motion can be optimally represented with a compact 128-token vocabulary, while 3D motion’s complexity demands a much larger 1024-token codebook for faithful reconstruction. These results establish practical deployment feasibility across diverse motion analysis applications. The code base for this work is available at <a target="_blank" rel="noopener" href="https://github.com/TeCSAR-UNCC/Pose-Quantization">https://github.com/TeCSAR-UNCC/Pose-Quantization</a>. </p>
<blockquote>
<p>连续人体运动理解是计算机视觉领域的一个核心挑战，因为其高维度和内在冗余性。对于分析复杂的运动动力学，高效压缩和表示至关重要。在这项工作中，我们引入了一个利用密集运动标记的对抗优化VQ-GAN框架，用于压缩时空热图，同时保留人体运动的精细痕迹。我们的方法结合了密集运动标记和对抗优化，消除了在非对抗基线中观察到的重建伪影，如运动模糊和时间错位。我们在CMU全景数据集上的实验证明了我们方法的优越性，相对于dVAE基线提高了9.31%的SSIM，并减少了37.1%的时间不稳定性。此外，我们的密集标记策略实现了对运动复杂性的新型分析，揭示出二维运动可以用紧凑的128个标记词汇表示最优，而三维运动的复杂性则需要更大的1024个标记代码本以实现忠实重建。这些结果证明了在各种运动分析应用中实际部署的可行性。该工作的代码库可在<a target="_blank" rel="noopener" href="https://github.com/TeCSAR-UNCC/Pose-Quantization%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/TeCSAR-UNCC/Pose-Quantization获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.19252v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文引入了一种基于对抗训练的VQ-GAN框架，结合密集运动令牌化，用于压缩时空热图，同时保留人类运动的精细痕迹。实验证明，该方法在CMU全景数据集上的表现优于dVAE基线，提高了9.31%的SSIM得分，并降低了37.1%的临时不稳定性。此外，该研究还揭示了不同运动复杂度的最优令牌表示方式，为实际应用提供了可行性依据。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>引入了一种新的VQ-GAN框架，结合了密集运动令牌化和对抗训练。</li>
<li>该方法能够在压缩时空热图时保留人类运动的精细痕迹。</li>
<li>在CMU全景数据集上的实验表现优于dVAE基线。</li>
<li>方法提高了9.31%的SSIM得分，并降低了37.1%的临时不稳定性。</li>
<li>密集令牌化策略能够揭示不同运动复杂度的最优表示方式。</li>
<li>2D运动可以用紧凑的128令牌词汇表最优地表示，而3D运动的复杂性则需要更大的1024令牌代码本进行忠实重建。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.19252">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-01dcb4767d55dd766c4029289d56c7b3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760038388&auth_key=1760038388-0-0-eb52311a3260d62dd01f45cd9513ce17&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-871f61e488e586ff37210dea98b1969d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760038396&auth_key=1760038396-0-0-c9692ba813b495d1063921508229cdf1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9ec71e6d05f3d78e2ed19955ca9b7eca~resize:0:q75.jpg?source=1f5c5e47&expiration=1760038403&auth_key=1760038403-0-0-2a6f801965d177c3894d1745b448b970&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f52fd9d565b771be2e34c4cb3ce47b72~resize:0:q75.jpg?source=1f5c5e47&expiration=1760038410&auth_key=1760038410-0-0-cf17b9494b155f2cb7a31feb5075be3e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-89b68a5da977977378518e7debfb7f59~resize:0:q75.jpg?source=1f5c5e47&expiration=1760038417&auth_key=1760038417-0-0-31a54715713f9107ff7bf2161becc844&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Seeing-Through-Reflections-Advancing-3D-Scene-Reconstruction-in-Mirror-Containing-Environments-with-Gaussian-Splatting"><a href="#Seeing-Through-Reflections-Advancing-3D-Scene-Reconstruction-in-Mirror-Containing-Environments-with-Gaussian-Splatting" class="headerlink" title="Seeing Through Reflections: Advancing 3D Scene Reconstruction in   Mirror-Containing Environments with Gaussian Splatting"></a>Seeing Through Reflections: Advancing 3D Scene Reconstruction in   Mirror-Containing Environments with Gaussian Splatting</h2><p><strong>Authors:Zijing Guo, Yunyang Zhao, Lin Wang</strong></p>
<p>Mirror-containing environments pose unique challenges for 3D reconstruction and novel view synthesis (NVS), as reflective surfaces introduce view-dependent distortions and inconsistencies. While cutting-edge methods such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) excel in typical scenes, their performance deteriorates in the presence of mirrors. Existing solutions mainly focus on handling mirror surfaces through symmetry mapping but often overlook the rich information carried by mirror reflections. These reflections offer complementary perspectives that can fill in absent details and significantly enhance reconstruction quality. To advance 3D reconstruction in mirror-rich environments, we present MirrorScene3D, a comprehensive dataset featuring diverse indoor scenes, 1256 high-quality images, and annotated mirror masks, providing a benchmark for evaluating reconstruction methods in reflective settings. Building on this, we propose ReflectiveGS, an extension of 3D Gaussian Splatting that utilizes mirror reflections as complementary viewpoints rather than simple symmetry artifacts, enhancing scene geometry and recovering absent details. Experiments on MirrorScene3D show that ReflectiveGaussian outperforms existing methods in SSIM, PSNR, LPIPS, and training speed, setting a new benchmark for 3D reconstruction in mirror-rich environments. </p>
<blockquote>
<p>包含镜子的环境对3D重建和新颖视角合成（NVS）提出了独特的挑战，因为反射表面引入了视角相关的失真和不一致性。虽然最先进的方法，如神经辐射场（NeRF）和三维高斯涂料（3DGS）在典型场景中表现出色，但在镜子存在的情况下，它们的性能会下降。现有解决方案主要通过对称映射处理镜面，但往往忽略了镜子反射所携带的丰富信息。这些反射提供了可以补充的视角，可以填补缺失的细节，并显着提高重建质量。为了推进在镜子丰富的环境中的3D重建，我们推出了MirrorScene3D数据集，它包含多样的室内场景、1256张高质量图像和注释的镜子掩膜，为反射环境中重建方法的评估提供了基准。在此基础上，我们提出了ReflectiveGS，它是基于三维高斯涂料的一种扩展，利用镜子反射作为补充视角，而不是简单的对称伪像，以增强场景几何并恢复缺失的细节。在MirrorScene3D上的实验表明，ReflectiveGaussian在SSIM、PSNR、LPIPS和训练速度上均优于现有方法，为镜子丰富的环境中的3D重建设定了新的基准。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.18956v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了在含有镜子的环境中，3D重建和新颖视图合成面临的挑战。现有方法如Neural Radiance Fields和3D Gaussian Splatting在典型场景中表现优异，但在镜子存在时性能下降。文章提出了MirrorScene3D数据集和ReflectiveGS方法，利用镜子反射作为补充视角，提高场景几何结构和恢复缺失细节的能力，为在镜子丰富的环境中进行3D重建设立了新的基准。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>镜子在3D重建和新颖视图合成中带来挑战，因反射表面产生视图依赖的失真和不一致性。</li>
<li>现有方法在处理镜子表面时主要通过对称映射，但忽略了镜子反射所携带的丰富信息。</li>
<li>MirrorScene3D数据集提供室内场景的多样化图像和标注的镜子掩膜，为评估反射环境下的重建方法提供了基准。</li>
<li>ReflectiveGS方法利用镜子反射作为补充视角，而非简单的对称产物。</li>
<li>ReflectiveGS提高了场景几何结构和恢复缺失细节的能力。</li>
<li>在MirrorScene3D数据集上的实验显示，ReflectiveGaussian在SSIM、PSNR、LPIPS和训练速度上超越了现有方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.18956">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-59ef441157f520f0d170006ac31fc7fd~resize:0:q75.jpg?source=1f5c5e47&expiration=1760038424&auth_key=1760038424-0-0-c2ee83428ccb9120abc9ebc07a1cb016&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-dacb7f30185d0ad15a107c6e4d34b02b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760038431&auth_key=1760038431-0-0-e26cdae59dd01a5b8cd821aa27ceea7a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f81cdbf33669c68eb401663d50edadd4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760038438&auth_key=1760038438-0-0-c9d07d041704dffbdd446e860213e91d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-10d7da124699a6f4627ec89f45d97718~resize:0:q75.jpg?source=1f5c5e47&expiration=1760038444&auth_key=1760038444-0-0-3d9fccdc121bf5603120f275db87aaff&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-89684003f438d8b81377651af5552a03~resize:0:q75.jpg?source=1f5c5e47&expiration=1760038451&auth_key=1760038451-0-0-08b82b4ee428ccbebe2d47e2379cfdcb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="HyRF-Hybrid-Radiance-Fields-for-Memory-efficient-and-High-quality-Novel-View-Synthesis"><a href="#HyRF-Hybrid-Radiance-Fields-for-Memory-efficient-and-High-quality-Novel-View-Synthesis" class="headerlink" title="HyRF: Hybrid Radiance Fields for Memory-efficient and High-quality Novel   View Synthesis"></a>HyRF: Hybrid Radiance Fields for Memory-efficient and High-quality Novel   View Synthesis</h2><p><strong>Authors:Zipeng Wang, Dan Xu</strong></p>
<p>Recently, 3D Gaussian Splatting (3DGS) has emerged as a powerful alternative to NeRF-based approaches, enabling real-time, high-quality novel view synthesis through explicit, optimizable 3D Gaussians. However, 3DGS suffers from significant memory overhead due to its reliance on per-Gaussian parameters to model view-dependent effects and anisotropic shapes. While recent works propose compressing 3DGS with neural fields, these methods struggle to capture high-frequency spatial variations in Gaussian properties, leading to degraded reconstruction of fine details. We present Hybrid Radiance Fields (HyRF), a novel scene representation that combines the strengths of explicit Gaussians and neural fields. HyRF decomposes the scene into (1) a compact set of explicit Gaussians storing only critical high-frequency parameters and (2) grid-based neural fields that predict remaining properties. To enhance representational capacity, we introduce a decoupled neural field architecture, separately modeling geometry (scale, opacity, rotation) and view-dependent color. Additionally, we propose a hybrid rendering scheme that composites Gaussian splatting with a neural field-predicted background, addressing limitations in distant scene representation. Experiments demonstrate that HyRF achieves state-of-the-art rendering quality while reducing model size by over 20 times compared to 3DGS and maintaining real-time performance. Our project page is available at <a target="_blank" rel="noopener" href="https://wzpscott.github.io/hyrf/">https://wzpscott.github.io/hyrf/</a>. </p>
<blockquote>
<p>最近，3D高斯贴图（3DGS）作为一种强大的NeRF替代方法崭露头角，它通过明确的、可优化的3D高斯实现实时高质量的新型视图合成。然而，由于3DGS依赖于高斯参数来模拟视图相关效果和各向异性形状，它产生了巨大的内存开销。尽管最近的工作提出用神经网络场来压缩3DGS，但这些方法在捕捉高斯属性的高频空间变化方面存在困难，导致精细细节的重建效果降低。我们提出了混合辐射场（HyRF），这是一种结合显式高斯和神经网络场优点的新型场景表示方法。HyRF将场景分解为（1）一组紧凑的显式高斯，只存储关键的高频参数，（2）基于网格的神经网络场，用于预测其余属性。为了提高表示能力，我们引入了解耦神经网络体系结构，分别模拟几何（尺度、不透明度、旋转）和视图相关的颜色。此外，我们提出了一种混合渲染方案，将高斯贴图与神经网络预测的背景进行合成，解决了远距离场景表示的局限性。实验表明，HyRF达到了最先进的渲染质量，与3DGS相比将模型大小缩小了20倍以上，同时保持了实时性能。我们的项目页面可在[<a target="_blank" rel="noopener" href="https://wzpscott.github.io/hyrf/]%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://wzpscott.github.io/hyrf/]上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.17083v2">PDF</a> Accepted at NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了Hybrid Radiance Fields（HyRF）这一新型场景表示方法，它结合了显式高斯和神经网络字段的优势。HyRF将场景分解为两组：一组是存储关键高频参数的显式高斯；另一组是基于网格的神经场预测剩余属性。此外，还提出了一种混合渲染方案，将高斯溅射与神经网络场预测的背景相结合，解决了远距离场景表示的限制。HyRF实现了高质量的渲染效果，模型大小相较于传统方法减少了超过20倍，同时保持了实时性能。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>3D Gaussian Splatting (3DGS) 是基于NeRF的替代方案，能进行实时高质量的新视角合成，但它存在内存开销大的问题。</li>
<li>HyRF 结合了显式高斯和神经场的技术优势，解决单一技术方案的不足。它将关键的高频参数存储在显式的Gaussians中，其余的参数通过神经网络场预测。</li>
<li>提出了一种新型的神经网络场架构，将几何（尺度、不透明度、旋转）和视角依赖的颜色分开建模，增强了表示能力。</li>
<li>采用混合渲染方案，结合了高斯溅射和神经网络场预测的背景，解决了远距离场景表示的问题。</li>
<li>实验表明，HyRF实现了高质量的渲染效果，模型大小相较于传统方法减少了超过20倍，并保持了实时性能。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.17083">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-e6de2c14c3fada49ecef3df765ff1dbc~resize:0:q75.jpg?source=1f5c5e47&expiration=1760038459&auth_key=1760038459-0-0-ccd656d5f3662f5b0cdfb9338047644e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d02592ac4473bc388a274f9989394a3d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760038466&auth_key=1760038466-0-0-30143580ccab000dccd4118daea82770&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4a83be3a634fed56dd535ea757c0b253~resize:0:q75.jpg?source=1f5c5e47&expiration=1760038472&auth_key=1760038472-0-0-2281e3b8f8c0a94fba05ccb2462d8bf3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7a871bfd24a47a759cf52edeebb5c8d6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760038479&auth_key=1760038479-0-0-0258d7b090f7c477c9ecb053524623d7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="GeMix-Conditional-GAN-Based-Mixup-for-Improved-Medical-Image-Augmentation"><a href="#GeMix-Conditional-GAN-Based-Mixup-for-Improved-Medical-Image-Augmentation" class="headerlink" title="GeMix: Conditional GAN-Based Mixup for Improved Medical Image   Augmentation"></a>GeMix: Conditional GAN-Based Mixup for Improved Medical Image   Augmentation</h2><p><strong>Authors:Hugo Carlesso, Maria Eliza Patulea, Moncef Garouani, Radu Tudor Ionescu, Josiane Mothe</strong></p>
<p>Mixup has become a popular augmentation strategy for image classification, yet its naive pixel-wise interpolation often produces unrealistic images that can hinder learning, particularly in high-stakes medical applications. We propose GeMix, a two-stage framework that replaces heuristic blending with a learned, label-aware interpolation powered by class-conditional GANs. First, a StyleGAN2-ADA generator is trained on the target dataset. During augmentation, we sample two label vectors from Dirichlet priors biased toward different classes and blend them via a Beta-distributed coefficient. Then, we condition the generator on this soft label to synthesize visually coherent images that lie along a continuous class manifold. We benchmark GeMix on the large-scale COVIDx-CT-3 dataset using three backbones (ResNet-50, ResNet-101, EfficientNet-B0). When combined with real data, our method increases macro-F1 over traditional mixup for all backbones, reducing the false negative rate for COVID-19 detection. GeMix is thus a drop-in replacement for pixel-space mixup, delivering stronger regularization and greater semantic fidelity, without disrupting existing training pipelines. We publicly release our code at <a target="_blank" rel="noopener" href="https://github.com/hugocarlesso/GeMix">https://github.com/hugocarlesso/GeMix</a> to foster reproducibility and further research. </p>
<blockquote>
<p>数据增强策略Mixup在图像分类中很受欢迎，但其朴素的像素级插值往往会产生不真实的图像，可能阻碍学习，特别是在高风险的医疗应用中。我们提出了GeMix，这是一个两阶段的框架，它用基于类别条件GAN的学习感知插值替换启发式混合。首先，我们在目标数据集上训练StyleGAN2-ADA生成器。在数据增强过程中，我们从偏向不同类别的Dirichlet先验中采样两个标签向量，并通过Beta分布系数将它们混合。然后，我们将生成器设置在这个软标签上，合成视觉上连贯的图像，这些图像位于一个连续的类别流形上。我们在大规模COVIDx-CT-3数据集上采用ResNet-50、ResNet-101和EfficientNet-B0三种骨干网对GeMix进行基准测试。当与真实数据结合时，我们的方法在所有骨干网上都提高了宏观F1分数，降低了COVID-19检测的误报率。因此，GeMix可以作为像素空间mixup的替代方案，提供更强的正则化和更高的语义保真度，而不会破坏现有的训练流程。我们公开发布我们的代码在<a target="_blank" rel="noopener" href="https://github.com/hugocarlesso/GeMix%E4%BB%A5%E4%BF%83%E8%BF%9B%E5%8F%AF%E9%87%8D%E5%A4%8D%E6%80%A7%E5%92%8C%E8%BF%9B%E4%B8%80%E6%AD%A5%E7%A0%94%E7%A9%B6%E3%80%82">https://github.com/hugocarlesso/GeMix以促进可重复性和进一步研究。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.15577v2">PDF</a> Accepted at CBMI 2025</p>
<p><strong>Summary</strong></p>
<p>基于StyleGAN2-ADA生成器的GeMix方法，通过采用标签感知插值技术，实现了在图像分类中的混合增强策略。此方法以软标签为条件合成图像，可生成连续的类别流形中的图像，增强了正则化并提高了语义保真度。在COVIDx-CT-3数据集上进行的实验表明，GeMix与传统混合方法相比，可提高所有背骨网络的宏观F1得分，并降低COVID-19检测的误报率。GeMix易于集成现有训练管道中，增强了医学应用的实用性。代码已公开于hugocarlesso&#x2F;GeMix。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GeMix是一种基于StyleGAN2-ADA生成器的两阶段框架，用于图像分类中的增强策略。</li>
<li>它采用标签感知插值技术，通过合成视觉上连贯的图像来增强正则化和语义保真度。这些图像位于连续的类别流形中。</li>
<li>在COVIDx-CT-3数据集上的实验表明，GeMix提高了宏观F1得分，降低了误报率。它对COVID-19检测具有实际应用价值。</li>
<li>GeMix易于集成现有训练管道中，作为像素空间混合的替代品使用。</li>
<li>GeMix通过合成更真实的图像解决了传统混合策略可能产生的不现实图像问题。</li>
<li>GeMix公开了源代码，有助于促进可重复性和进一步研究。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.15577">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-030bf440ac55c96263e57bc08421900a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760038486&auth_key=1760038486-0-0-35b564855b690fe9d82ca36a90590fe6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5d29c3256c94661b424630d8f2a1417a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760038493&auth_key=1760038493-0-0-272422b2090bcf37195a0fe2dcc4291e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-cdc7ffdf5f932180a83612b9c0ec803a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760038500&auth_key=1760038500-0-0-72d21f16f09c6c12a7041a0e9f467cdd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-92a2067546ec4c4a0b1c309aa86006d2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760038506&auth_key=1760038506-0-0-67416858ae3e1ac6e998cac3bb64c510&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-28/NeRF/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-28/NeRF/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/NeRF/">
                                    <span class="chip bg-color">NeRF</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-28/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-465cef20062acec8cb146b8c5b964d76~resize:0:q75.jpg?source=1f5c5e47&expiration=1760038513&auth_key=1760038513-0-0-41f11b84ad1b9618716472c01c0913f5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-09-28  Does FLUX Already Know How to Perform Physically Plausible Image   Composition?
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-28
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-28/3DGS/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-d2865c189a251d46fd0c4be9ce253c50~resize:0:q75.jpg?source=1f5c5e47&expiration=1760037583&auth_key=1760037583-0-0-77f2ec003eb4d5ebeb5b26a036bf8a49&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="3DGS">
                        
                        <span class="card-title">3DGS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            3DGS 方向最新论文已更新，请持续关注 Update in 2025-09-28  Gaussian splatting holography
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-28
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/3DGS/" class="post-category">
                                    3DGS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/3DGS/">
                        <span class="chip bg-color">3DGS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">30341.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
