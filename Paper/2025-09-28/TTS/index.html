<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="TTS">
    <meta name="description" content="TTS 方向最新论文已更新，请持续关注 Update in 2025-09-28  UniSS Unified Expressive Speech-to-Speech Translation with Your Voice">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>TTS | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_TTS/2509.15373v2/page_0_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">TTS</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/TTS/">
                                <span class="chip bg-color">TTS</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                TTS
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-09-28
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-09-29
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    15.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    64 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-09-28-更新"><a href="#2025-09-28-更新" class="headerlink" title="2025-09-28 更新"></a>2025-09-28 更新</h1><h2 id="UniSS-Unified-Expressive-Speech-to-Speech-Translation-with-Your-Voice"><a href="#UniSS-Unified-Expressive-Speech-to-Speech-Translation-with-Your-Voice" class="headerlink" title="UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice"></a>UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice</h2><p><strong>Authors:Sitong Cheng, Weizhen Bian, Xinsheng Wang, Ruibin Yuan, Jianyi Chen, Shunshun Yin, Yike Guo, Wei Xue</strong></p>
<p>The ultimate goal of expressive speech-to-speech translation (S2ST) is to accurately translate spoken content while preserving the speaker identity and emotional style. However, progress in this field is largely hindered by three key challenges: the scarcity of paired speech data that retains expressive styles, the complexity of multi-stage processing pipelines, and the limited transfer of translation capabilities from large language models (LLMs). In this work, we address these challenges by introducing UniSS, a novel single-stage framework for expressive S2ST. Our approach features carefully designed speech semantic and style modeling, enabling seamless integration with existing text-based LLM frameworks to develop a unified text-speech language model. To transfer translation capabilities from text to speech, we propose a cross-modal chain-of-thought prompting process that progressively aligns audio semantics with text and ensures style preservation in the decoded results. Furthermore, we construct and release a large-scale, high-quality expressive S2ST dataset, UniST, comprising 44.8k hours of data. Experimental results show that UniSS significantly outperforms previous methods in translation fidelity and speech quality while preserving voice, emotion, and duration consistency. Our work establishes a simpler and more effective paradigm for building the next generation of expressive S2ST systems. Audio samples are available at <a target="_blank" rel="noopener" href="https://cmots.github.io/uniss-demo">https://cmots.github.io/uniss-demo</a>. </p>
<blockquote>
<p>表达性语音到语音翻译（S2ST）的最终目标是准确翻译口语内容，同时保留说话人的身份和情感风格。然而，该领域的进展主要受三个关键挑战的影响：缺乏保留表达风格的配对语音数据、多阶段处理流程的复杂性，以及从大语言模型（LLM）迁移翻译能力有限。在这项工作中，我们通过引入UniSS，即一种新型的单阶段表达性S2ST框架来解决这些挑战。我们的方法通过精心设计的语音语义和风格建模，实现与现有的文本基于LLM框架无缝集成，以开发统一的文本-语音语言模型。为了从文本到语音迁移翻译能力，我们提出了一种跨模态思维链提示过程，该过程逐步对齐音频语义和文本，并确保在解码结果中保持风格。此外，我们构建并发布了一个大规模、高质量的表达性S2ST数据集UniST，包含44.8k小时的数据。实验结果表明，UniSS在翻译保真度和语音质量方面显著优于以前的方法，同时保留语音、情感和持续时间一致性。我们的工作为建立下一代表达性S2ST系统建立了更简单、更有效的范式。音频样本可在<a target="_blank" rel="noopener" href="https://cmots.github.io/uniss-demo%E8%8E%B7%E5%8F%96%E3%80%82">https://cmots.github.io/uniss-demo获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.21144v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了表达性语音到语音翻译（S2ST）领域的最新研究进展。针对该领域面临的挑战，如缺乏保留表达风格的数据、多阶段处理管道复杂以及大型语言模型（LLM）的翻译能力迁移有限，研究者提出了一种名为UniSS的新型单一阶段框架。该框架结合了语音语义和风格建模，可与现有文本语言模型框架无缝集成，通过跨模态思维链提示过程实现翻译能力的跨模态迁移。此外，还构建并发布了一个大规模高质量的表达性S2ST数据集UniST。实验结果表明，UniSS在翻译准确性和语音质量方面显著优于以前的方法，同时保持了语音、情感和持续时间的一致性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>表达性语音到语音翻译（S2ST）旨在准确翻译语音内容，同时保留说话人的身份和情感风格。</li>
<li>S2ST领域面临的主要挑战包括缺乏保留表达风格的数据、多阶段处理管道的复杂性以及大型语言模型的翻译能力迁移限制。</li>
<li>UniSS是一种新型单一阶段框架，通过结合语音语义和风格建模来解决这些挑战。</li>
<li>UniSS框架可与现有文本语言模型无缝集成，通过跨模态思维链提示过程实现翻译能力的跨模态迁移。</li>
<li>UniST数据集的构建和发布为表达性S2ST研究提供了大规模高质量的数据资源。</li>
<li>实验结果表明，UniSS在翻译准确性和语音质量方面显著优于以往方法，能够保持语音、情感和持续时间的一致性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.21144">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_TTS/2509.21144v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_TTS/2509.21144v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_TTS/2509.21144v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_TTS/2509.21144v1/page_5_0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="i-LAVA-Insights-on-Low-Latency-Voice-2-Voice-Architecture-for-Agents"><a href="#i-LAVA-Insights-on-Low-Latency-Voice-2-Voice-Architecture-for-Agents" class="headerlink" title="i-LAVA: Insights on Low Latency Voice-2-Voice Architecture for Agents"></a>i-LAVA: Insights on Low Latency Voice-2-Voice Architecture for Agents</h2><p><strong>Authors:Anupam Purwar, Aditya Choudhary</strong></p>
<p>We experiment with a low-latency, end-to-end voice-to-voice communication model to optimize it for real-time conversational applications. By analyzing components essential to voice to voice (V-2-V) system viz. automatic speech recognition (ASR), text-to-speech (TTS), and dialog management, our work analyzes how to reduce processing time while maintaining high-quality interactions to identify the levers for optimizing V-2-V system. Our work identifies that TTS component which generates life-like voice, full of emotions including natural pauses and exclamations has highest impact on Real time factor (RTF). The experimented V-2-V architecture utilizes CSM1b has the capability to understand tone as well as context of conversation by ingesting both audio and text of prior exchanges to generate contextually accurate speech. We explored optimization of Residual Vector Quantization (RVQ) iterations by the TTS decoder which come at a cost of decrease in the quality of voice generated. Our experimental evaluations also demonstrate that for V-2-V implementations based on CSM most important optimizations can be brought by reducing the number of RVQ Iterations along with the codebooks used in Mimi. </p>
<blockquote>
<p>我们实验了一种低延迟、端到端的语音到语音通信模型，以优化其适用于实时对话应用。通过分析语音到语音（V-2-V）系统的关键组件，包括自动语音识别（ASR）、文本到语音（TTS）和对话管理，我们的工作分析了如何在保持高质量交互的同时减少处理时间，以确定优化V-2-V系统的关键因素。我们的工作确定，对实时因子（RTF）影响最大的是TTS组件，该组件生成逼真的声音，充满情感，包括自然停顿和感叹。所实验的V-2-V架构采用CSM1b，具有通过摄取先前的音频和文本对话来理解和把握对话语调及上下文的能力，从而生成语境准确的语音。我们探索了通过TTS解码器优化剩余矢量量化（RVQ）迭代的方法，但这会导致生成的语音质量下降。我们的实验评估还表明，对于基于CSM的V-2-V实现，最重要的优化可以通过减少RVQ迭代次数以及与Mimi中使用的代码簿一起实现。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.20971v1">PDF</a> This paper analyzes a low-latency, end-to-end voice-to-voice (V-2-V)   architecture, identifying that the Text-to-Speech (TTS) component has the   highest impact on real-time performance. By reducing the number of Residual   Vector Quantization (RVQ) iterations in the TTS model, latency can be   effectively halved, creating a direct trade-off between conversational speed   and audio quality</p>
<p><strong>Summary</strong><br>     我们实验了一个低延迟、端到端的语音到语音通信模型，以优化其适用于实时对话应用。通过分析语音到语音（V-2-V）系统的关键组件，如自动语音识别（ASR）、文本到语音（TTS）和对话管理，我们的工作分析了如何在保持高质量交互的同时减少处理时间，以找出优化V-2-V系统的关键手段。研究发现，文本到语音转换环节对实时性能影响最大，生成的语音需真实、充满情感，包括自然停顿和感叹。实验的V-2-V架构利用CSM1b，能够理解和适应对话的语境和语调，通过输入先前的对话音频和文本生成语境准确的语音。我们探索了通过减少TTS解码器的剩余向量量化（RVQ）迭代次数来优化性能，但这可能会导致生成的语音质量下降。实验评估表明，对于基于CSM的V-2-V实现，最重要的优化措施是减少RVQ迭代次数和所使用的码本数量。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>实验研究了低延迟端到端的语音到语音通信模型。</li>
<li>研究分析了语音到语音系统的关键组件，包括自动语音识别、文本到语音转换和对话管理。</li>
<li>发现文本到语音转换环节对实时性能的影响最大，并需要关注生成语音的自然性和情感表达。</li>
<li>实验中使用的V-2-V架构利用CSM技术能够理解和适应对话的语境和语调。</li>
<li>优化通过减少剩余向量量化（RVQ）迭代次数来实现性能提升是一个可行的方法，但需权衡生成的语音质量损失。</li>
<li>实验评估表明减少RVQ迭代次数和使用较少的码本数量是优化基于CSM的V-2-V实现的关键措施。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.20971">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_TTS/2509.20971v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_TTS/2509.20971v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_TTS/2509.20971v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_TTS/2509.20971v1/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_TTS/2509.20971v1/page_2_2.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_TTS/2509.20971v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_TTS/2509.20971v1/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_TTS/2509.20971v1/page_3_2.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_TTS/2509.20971v1/page_3_3.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="SPADE-Structured-Pruning-and-Adaptive-Distillation-for-Efficient-LLM-TTS"><a href="#SPADE-Structured-Pruning-and-Adaptive-Distillation-for-Efficient-LLM-TTS" class="headerlink" title="SPADE: Structured Pruning and Adaptive Distillation for Efficient   LLM-TTS"></a>SPADE: Structured Pruning and Adaptive Distillation for Efficient   LLM-TTS</h2><p><strong>Authors:Tan Dat Nguyen, Jaehun Kim, Ji-Hoon Kim, Shukjae Choi, Youshin Lim, Joon Son Chung</strong></p>
<p>The goal of this paper is to introduce SPADE, a framework for Structured Pruning and Adaptive Distillation for Efficient Large Language Model-based text-to-speech (LLM-TTS). Recent LLM-TTS systems achieve strong controllability and zero-shot generalization, but their large parameter counts and high latency limit real-world deployment. SPADE addresses this by combining (i) a pruning step guided by a word-error-rate-based layer importance index to remove non-essential Transformer layers, with (ii) multi-level knowledge distillation to restore autoregressive coherence. On zero-shot benchmarks, SPADE preserves near-parity perceptual quality while halving Transformer depth, reducing VRAM usage by up to 20%, and achieving up to 1.7x faster real-time factor with less than 5% of the original training data. These results show that compact LLM-TTS models can maintain naturalness and speaker similarity while enabling practical real-time speech generation. Audio samples are available at <a target="_blank" rel="noopener" href="https://mm.kaist.ac.kr/projects/SPADE/">https://mm.kaist.ac.kr/projects/SPADE/</a>. </p>
<blockquote>
<p>本文的目标是介绍SPADE，这是一个基于结构化剪枝和自适应蒸馏的框架，用于实现基于大型语言模型（LLM）的文本到语音（TTS）的高效转换。最近的LLM-TTS系统实现了强大的可控性和零样本泛化能力，但它们的参数数量庞大和延迟较高，限制了其在现实世界中的部署。SPADE通过结合（i）以基于词错误率的层重要性指数为指导的剪枝步骤来去除非必要的Transformer层，以及（ii）多层次知识蒸馏来恢复自回归连贯性来解决这一问题。在零样本基准测试中，SPADE在保持近乎相同的感知质量的同时，将Transformer深度减半，将VRAM使用量减少高达20%，并且使用不到原始训练数据的5%即可达到高达1.7倍的实时因子。这些结果表明，紧凑的LLM-TTS模型能够在保持自然度和说话人相似性的同时，实现实用的实时语音生成。音频样本可在<a target="_blank" rel="noopener" href="https://mm.kaist.ac.kr/projects/SPADE/%E6%89%BE%E5%88%B0%E3%80%82">https://mm.kaist.ac.kr/projects/SPADE/找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.20802v1">PDF</a> Submitted to ICASSP 2026</p>
<p><strong>Summary</strong></p>
<p>本文介绍了SPADE框架，该框架结合了结构化剪枝和自适应蒸馏技术，旨在提高基于大型语言模型的文本到语音（LLM-TTS）的效率。SPADE通过剪枝步骤去除非必要的Transformer层，并结合多层次知识蒸馏来保持文本的连贯性。在零样本基准测试中，SPADE在减少Transformer深度、降低VRAM使用量和提高实时语音生成速度的同时，保持了近乎相同的感知质量。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SPADE是一个针对大型语言模型文本到语音转换（LLM-TTS）的效率优化框架。</li>
<li>它通过结合结构化剪枝和自适应蒸馏技术来实现优化。</li>
<li>SPADE通过基于单词错误率的层重要性指数来指导剪枝步骤，去除非必要的Transformer层。</li>
<li>多层次知识蒸馏用于恢复文本的连贯性。</li>
<li>在零样本基准测试中，SPADE在保持近乎相同的感知质量的同时，实现了Transformer深度的减半。</li>
<li>SPADE降低了VRAM使用量，最高可达20%。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.20802">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_TTS/2509.20802v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_TTS/2509.20802v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_TTS/2509.20802v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_TTS/2509.20802v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Beyond-Global-Emotion-Fine-Grained-Emotional-Speech-Synthesis-with-Dynamic-Word-Level-Modulation"><a href="#Beyond-Global-Emotion-Fine-Grained-Emotional-Speech-Synthesis-with-Dynamic-Word-Level-Modulation" class="headerlink" title="Beyond Global Emotion: Fine-Grained Emotional Speech Synthesis with   Dynamic Word-Level Modulation"></a>Beyond Global Emotion: Fine-Grained Emotional Speech Synthesis with   Dynamic Word-Level Modulation</h2><p><strong>Authors:Sirui Wang, Andong Chen, Tiejun Zhao</strong></p>
<p>Emotional text-to-speech (E-TTS) is central to creating natural and trustworthy human-computer interaction. Existing systems typically rely on sentence-level control through predefined labels, reference audio, or natural language prompts. While effective for global emotion expression, these approaches fail to capture dynamic shifts within a sentence. To address this limitation, we introduce Emo-FiLM, a fine-grained emotion modeling framework for LLM-based TTS. Emo-FiLM aligns frame-level features from emotion2vec to words to obtain word-level emotion annotations, and maps them through a Feature-wise Linear Modulation (FiLM) layer, enabling word-level emotion control by directly modulating text embeddings. To support evaluation, we construct the Fine-grained Emotion Dynamics Dataset (FEDD) with detailed annotations of emotional transitions. Experiments show that Emo-FiLM outperforms existing approaches on both global and fine-grained tasks, demonstrating its effectiveness and generality for expressive speech synthesis. </p>
<blockquote>
<p>情感文本转语音（E-TTS）是创建自然可信的人机交互的核心。现有系统通常依赖于通过预设标签、参考音频或自然语言提示来进行句子级别的控制。虽然这些方法对于全局情感表达有效，但它们无法捕捉句子中的动态变化。为了解决这一局限性，我们引入了Emo-FiLM，这是一个基于大型语言模型（LLM）的TTS系统的精细情感建模框架。Emo-FiLM通过对齐来自emotion2vec的帧级特征，获得词级情感注释，并通过特征线性调制（FiLM）层进行映射，通过直接调制文本嵌入来实现词级情感控制。为了支持评估，我们构建了精细情感动态数据集（FEDD），其中包含了情感转变的详细注释。实验表明，Emo-FiLM在全局和精细任务上的表现都优于现有方法，证明了其在表现力语音合成中的有效性和普遍性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.20378v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>情感文本到语音（E-TTS）是创建自然和可信赖的人机交互的核心。现有系统通常依赖于通过预定义标签、参考音频或自然语言提示进行句子级别的控制，虽然对全球情感表达有效，但无法捕捉句子内的动态变化。为解决这一局限，我们引入了基于LLM的TTS的精细情感建模框架Emo-FiLM。Emo-FiLM通过对齐情感特征的帧级数据（emotion2vec）与单词，获得单词级别的情感注释，并通过特征线性调制（FiLM）层进行映射，能够直接调制文本嵌入，实现单词级别的情感控制。为支持评估，我们构建了精细情感动态数据集（FEDD），包含详细的情感过渡注释。实验表明，Emo-FiLM在全局和精细任务上均优于现有方法，证明其在表情语音合成中的有效性和普遍性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>E-TTS在创建自然和可信赖的人机交互中起关键作用。</li>
<li>现有系统无法捕捉句子内的动态情感变化。</li>
<li>Emo-FiLM框架被引入以解决此问题，并实现单词级别的情感控制。</li>
<li>Emo-FiLM通过对齐情感特征的帧级数据与单词来获得单词级别的情感注释。</li>
<li>Feature-wise Linear Modulation（FiLM）层用于映射和调制文本嵌入，实现精细情感控制。</li>
<li>为支持评估，构建了精细情感动态数据集（FEDD），包含详细的情感过渡注释。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.20378">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_TTS/2509.20378v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_TTS/2509.20378v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_TTS/2509.20378v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_TTS/2509.20378v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Measuring-Prosody-Diversity-in-Zero-Shot-TTS-A-New-Metric-Benchmark-and-Exploration"><a href="#Measuring-Prosody-Diversity-in-Zero-Shot-TTS-A-New-Metric-Benchmark-and-Exploration" class="headerlink" title="Measuring Prosody Diversity in Zero-Shot TTS: A New Metric, Benchmark,   and Exploration"></a>Measuring Prosody Diversity in Zero-Shot TTS: A New Metric, Benchmark,   and Exploration</h2><p><strong>Authors:Yifan Yang, Bing Han, Hui Wang, Long Zhou, Wei Wang, Mingyu Cui, Xu Tan, Xie Chen</strong></p>
<p>Prosody diversity is essential for achieving naturalness and expressiveness in zero-shot text-to-speech (TTS). However, frequently used acoustic metrics capture only partial views of prosodic variation and correlate poorly with human perception, leaving the problem of reliably quantifying prosody diversity underexplored. To bridge this gap, we introduce ProsodyEval, a prosody diversity assessment dataset that provides Prosody Mean Opinion Score (PMOS) alongside conventional acoustic metrics. ProsodyEval comprises 1000 speech samples derived from 7 mainstream TTS systems, with 2000 human ratings. Building on this, we propose the Discretized Speech Weighted Edit Distance (DS-WED), a new objective diversity metric that quantifies prosodic variation via weighted edit distance over semantic tokens. Experiments on ProsodyEval show that DS-WED achieves substantially higher correlation with human judgments than existing acoustic metrics, while remaining highly robust in speech tokenization from HuBERT and WavLM. Leveraging DS-WED, we benchmark state-of-the-art open-source TTS systems on LibriSpeech test-clean and Seed-TTS test-en, and further explorations uncover several factors that influence prosody diversity, including generative modeling paradigms, duration control, and reinforcement learning. Moreover, we find that current large audio language models (LALMs) remain limited in capturing prosodic variations. Audio samples are available at <a target="_blank" rel="noopener" href="https://prosodyeval.github.io/">https://prosodyeval.github.io</a>. </p>
<blockquote>
<p>韵律多样性对于实现零样本文本到语音（TTS）的自然度和表现力至关重要。然而，常用的声学指标只能捕捉部分韵律变化，与人类感知的相关性较差，导致可靠地量化韵律多样性的问题尚未得到充分探索。为了弥补这一差距，我们引入了ProsodyEval数据集，一个提供韵律多样性评估的数据集，同时提供韵律平均意见得分（PMOS）和传统声学指标。ProsodyEval包含来自7个主流TTS系统的1000个语音样本和2000个人类评分。在此基础上，我们提出了离散语音加权编辑距离（DS-WED），这是一种新的客观多样性指标，它通过语义标记的加权编辑距离来量化韵律变化。在ProsodyEval上的实验表明，与现有的声学指标相比，DS-WED与人类判断的相关性更高，同时在HuBERT和WavLM的语音标记化中表现出高度稳健性。利用DS-WED，我们对LibriSpeech测试清洁版和Seed-TTS测试英语版中的最新开源TTS系统进行了基准测试，进一步探索影响韵律多样性的几个因素，包括生成建模范式、持续时间控制和强化学习。此外，我们发现当前的大型音频语言模型（LALM）在捕捉韵律变化方面仍然存在局限性。音频样本可在<a target="_blank" rel="noopener" href="https://prosodyeval.github.io/">https://prosodyeval.github.io</a>获取。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.19928v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文强调了语调多样性在零样本文本转语音（TTS）中的重要性，并提出一个名为ProsodyEval的语调多样性评估数据集，用于提供语调平均意见得分（PMOS）和传统声学指标。文章还提出了一种新的客观多样性指标——离散语音加权编辑距离（DS-WED），用于量化语义标记的加权编辑距离来评估语调变化。实验表明，DS-WED与人类判断的相关性高于现有声学指标，同时高度稳健于语音标记化过程。基于此指标，作者对领先的开源TTS系统进行了评估，并探讨了影响语调多样性的多种因素，包括生成建模范式、时长控制和强化学习等。同时指出当前大型音频语言模型在捕捉语调变化方面的局限性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文本强调了语调多样性在零样本文本转语音中的重要性。</li>
<li>提出新的语调多样性评估数据集ProsodyEval，结合PMOS和常规声学指标进行评估。</li>
<li>提出新的客观多样性指标DS-WED，用于量化语调变化。</li>
<li>DS-WED与人类判断的相关性高于现有声学指标，且在语音标记化过程中表现稳健。</li>
<li>基于DS-WED指标评估了领先的开源TTS系统性能。</li>
<li>研究发现生成建模范式、时长控制和强化学习等因素会影响语调多样性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.19928">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_TTS/2509.19928v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_TTS/2509.19928v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_TTS/2509.19928v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_TTS/2509.19928v2/page_3_0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Eliminating-stability-hallucinations-in-llm-based-tts-models-via-attention-guidance"><a href="#Eliminating-stability-hallucinations-in-llm-based-tts-models-via-attention-guidance" class="headerlink" title="Eliminating stability hallucinations in llm-based tts models via   attention guidance"></a>Eliminating stability hallucinations in llm-based tts models via   attention guidance</h2><p><strong>Authors:ShiMing Wang, ZhiHao Du, Yang Xiang, TianYu Zhao, Han Zhao, Qian Chen, XianGang Li, HanJie Guo, ZhenHua Ling</strong></p>
<p>This paper focuses on resolving stability hallucinations (e.g., repetitive or omitted speech) in LLM-based Text-to-Speech (TTS) models by improving and leveraging the attention mechanism. First, we analyzed the alignment mechanism between text tokens and speech tokens in LLMs. We then proposed a metric termed the Optimal Alignment Score (OAS), which employs the Viterbi algorithm to evaluate text-speech alignment quality. Subsequently, OAS was integrated into the training of CosyVoice2 to assist LLMs in learning continuous, stable alignment. Additionally, the pre-trained attention value is employed to guide the training of the student CosyVoice2 via chain-of-thought (CoT), which further reduces stability hallucinations in synthesized speech. Experiments on the Seed-TTS-Eval and CV3-Eval test sets demonstrate that the proposed methods can effectively reduce the stability hallucinations of CosyVoice2 without introducing additional negative effects. The appendix is available at <a target="_blank" rel="noopener" href="https://wsmzzz.github.io/llm_attn">https://wsmzzz.github.io/llm_attn</a>. </p>
<blockquote>
<p>本文重点关注基于LLM的文本转语音（TTS）模型中的稳定性幻觉（如重复或遗漏的语音）问题，通过改进和利用注意力机制来解决。首先，我们分析了LLM中文本标记和语音标记之间的对齐机制。随后，我们提出了一种名为最佳对齐分数（OAS）的指标，该指标利用维特比算法评估文本-语音对齐质量。接着，将OAS集成到CosyVoice2的训练中，帮助LLM学习连续、稳定的对齐。此外，还使用预训练的注意力值，通过思维链（CoT）引导学生CosyVoice2的训练，这进一步减少了合成语音中的稳定性幻觉。在Seed-TTS-Eval和CV3-Eval测试集上的实验表明，所提出的方法可以有效减少CosyVoice2的稳定性幻觉，且不会引入额外的负面影响。附录可访问于：<a target="_blank" rel="noopener" href="https://wsmzzz.github.io/llm_attn%E3%80%82">https://wsmzzz.github.io/llm_attn。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.19852v1">PDF</a> 5 pages, submitted to ICASSP2026</p>
<p><strong>Summary</strong></p>
<p>本论文通过改进和利用注意力机制，解决了基于LLM的文本到语音（TTS）模型中的稳定性幻觉（如重复或省略语音）问题。研究团队分析了LLM中文本标记和语音标记的对齐机制，提出了名为最佳对齐分数（OAS）的评估指标，利用维特比算法评估文本到语音的对齐质量。此外，还将OAS集成到CosyVoice2的训练中，帮助学生网络学习连续稳定的对齐。通过利用预训练的注意力值，进一步降低了合成语音的稳定性幻觉。实验表明，该方法能有效减少CosyVoice2的稳定性幻觉，且未引入其他负面影响。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>论文聚焦于解决LLM-based TTS模型的稳定性幻觉问题，如重复或省略语音。</li>
<li>分析了LLM中文本标记和语音标记的对齐机制。</li>
<li>提出了名为最佳对齐分数（OAS）的评估指标，用于评价文本到语音的对齐质量。</li>
<li>利用维特比算法实现OAS，并将其集成到CosyVoice2的训练中。</li>
<li>引入预训练的注意力值，通过思维链（CoT）指导CosyVoice2的训练。</li>
<li>实验证明，所提方法能有效减少CosyVoice2合成语音的稳定性幻觉，且没有引入其他负面影响。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.19852">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_TTS/2509.19852v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_TTS/2509.19852v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_TTS/2509.19852v1/page_1_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_TTS/2509.19852v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_TTS/2509.19852v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Selective-Classifier-free-Guidance-for-Zero-shot-Text-to-speech"><a href="#Selective-Classifier-free-Guidance-for-Zero-shot-Text-to-speech" class="headerlink" title="Selective Classifier-free Guidance for Zero-shot Text-to-speech"></a>Selective Classifier-free Guidance for Zero-shot Text-to-speech</h2><p><strong>Authors:John Zheng, Farhad Maleki</strong></p>
<p>In zero-shot text-to-speech, achieving a balance between fidelity to the target speaker and adherence to text content remains a challenge. While classifier-free guidance (CFG) strategies have shown promising results in image generation, their application to speech synthesis are underexplored. Separating the conditions used for CFG enables trade-offs between different desired characteristics in speech synthesis. In this paper, we evaluate the adaptability of CFG strategies originally developed for image generation to speech synthesis and extend separated-condition CFG approaches for this domain. Our results show that CFG strategies effective in image generation generally fail to improve speech synthesis. We also find that we can improve speaker similarity while limiting degradation of text adherence by applying standard CFG during early timesteps and switching to selective CFG only in later timesteps. Surprisingly, we observe that the effectiveness of a selective CFG strategy is highly text-representation dependent, as differences between the two languages of English and Mandarin can lead to different results even with the same model. </p>
<blockquote>
<p>在零样本文本到语音的转换中，如何在忠实于目标说话人和遵守文本内容之间取得平衡仍然是一个挑战。虽然无分类器引导（CFG）策略在图像生成方面已经取得了有希望的结果，但它们在语音合成中的应用尚未得到充分探索。使用CFG时，对条件进行分离能够使语音合成中的不同期望特征之间进行权衡。在本文中，我们评估了原本为图像生成而开发的CFG策略对语音合成的适应性，并对此领域的分离条件CFG方法进行了扩展。我们的结果表明，在图像生成中有效的CFG策略通常无法改善语音合成。我们还发现，通过在早期时间步长应用标准CFG，并在后期时间步长仅选择CFG，我们可以提高说话人的相似性，同时限制文本贴合度的降低。令人惊讶的是，我们发现选择性CFG策略的有效性高度依赖于文本表示，因为英语和普通话两种语言之间的差异即使使用相同的模型也会导致不同的结果。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.19668v1">PDF</a> 5 pages, 7 figures, 1 table. Submitted to ICASSP 2026</p>
<p><strong>Summary</strong></p>
<p>零样本文本转语音（Text-to-Speech）中，实现目标语音与文本内容的平衡是一大挑战。虽然无分类器引导（Classifier-Free Guidance，CFG）策略在图像生成领域展现出良好前景，但在语音合成领域的应用尚未得到充分探索。本文通过评估适用于图像生成的CFG策略在语音合成中的适应性，并扩展了分条件CFG方法。研究发现，图像生成中有效的CFG策略通常无法提升语音合成效果。通过早期时序步使用标准CFG，并在后期时序步转为选择性CFG，可提升说话人相似性并限制文本贴合度的降低。有趣的是，选择性CFG策略的有效性高度依赖于文本表征，中英文之间的差异甚至会导致不同结果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>零样本文本转语音中平衡语音与文本内容的挑战。</li>
<li>无分类器引导（CFG）策略在语音合成中的应用尚未广泛探索。</li>
<li>图像生成中的CFG策略通常不能直接应用于语音合成。</li>
<li>通过早期和后期时序步采用不同的CFG策略，可提高说话人相似性和文本贴合度。</li>
<li>选择性CFG策略的有效性依赖于文本表征。</li>
<li>不同语言（如中英文）之间的差异可能影响CFG策略的效果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.19668">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_TTS/2509.19668v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_TTS/2509.19668v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_TTS/2509.19668v1/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_TTS/2509.19668v1/page_2_2.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_TTS/2509.19668v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Investigating-Test-Time-Scaling-with-Reranking-for-Machine-Translation"><a href="#Investigating-Test-Time-Scaling-with-Reranking-for-Machine-Translation" class="headerlink" title="Investigating Test-Time Scaling with Reranking for Machine Translation"></a>Investigating Test-Time Scaling with Reranking for Machine Translation</h2><p><strong>Authors:Shaomu Tan, Ryosuke Mitani, Ritvik Choudhary, Toshiyuki Sekiya</strong></p>
<p>Scaling model parameters has become the de facto strategy for improving NLP systems, but it comes with substantial computational costs. Test-Time Scaling (TTS) offers an alternative by allocating more computation at inference: generating multiple candidates and selecting the best. While effective in tasks such as mathematical reasoning, TTS has not been systematically explored for machine translation (MT). In this paper, we present the first systematic study of TTS for MT, investigating a simple but practical best-of-N framework on WMT24 benchmarks. Our experiments cover six high-resource and one low-resource language pairs, five model sizes (3B-72B), and various TTS compute budget (N up to 1024). Our results show that a) For high-resource languages, TTS generally improves translation quality according to multiple neural MT evaluation metrics, and our human evaluation confirms these gains; b) Augmenting smaller models with large $N$ can match or surpass larger models at $N{&#x3D;}1$ with more compute cost; c) Under fixed compute budgets, larger models are typically more efficient, and TTS can degrade quality due to metric blind spots in low-resource cases. </p>
<blockquote>
<p>模型参数缩放已成为改进NLP系统的默认策略，但这需要大量的计算成本。测试时缩放（TTS）通过推理时分配更多计算提供一种替代方案：生成多个候选者并选择最佳选项。虽然在数学推理等任务中有效，但TTS在机器翻译（MT）方面尚未进行系统探索。在本文中，我们对机器翻译中的TTS进行了首次系统研究，在WMT24基准测试上探讨了一个简单但实用的最佳N框架。我们的实验涵盖了六个高资源和一个低资源语言对、五种模型大小（3B-72B）以及各种TTS计算预算（N最多达1024）。我们的结果表明：a）对于高资源语言，TTS通常根据多个神经机器翻译评估指标提高了翻译质量，我们的人工评估也证实了这些收益；b）通过大型N增强小型模型可以匹配或超越大型模型在N&#x3D;1时的计算能力；c）在固定的计算预算下，大型模型通常更有效，而在资源不足的情况下，TTS可能会由于指标盲点而降低质量。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.19020v1">PDF</a> </p>
<p><strong>Summary</strong><br>     本文探索了测试时间缩放（TTS）在机器翻译（MT）中的首次系统性应用。研究表明，TTS在高资源语言翻译中普遍提高了翻译质量，且较小的模型通过增大N值甚至能匹配或超越大型模型的表现。然而，在固定计算预算下，大型模型通常更有效，低资源情况下TTS可能因评估指标的盲点而降低质量。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>测试时间缩放（TTS）作为一种在推理时分配更多计算资源的策略，被首次系统性地研究用于机器翻译（MT）。</li>
<li>在高资源语言翻译中，TTS普遍提高了翻译质量，这一发现得到了多项神经机器翻译评估指标和人工评价的证实。</li>
<li>通过增大N值，较小的模型在机器翻译中也能匹配或超越大型模型的表现。</li>
<li>在固定计算预算下，大型模型通常更为高效。</li>
<li>TTS在高资源情况下的表现优于低资源情况，后者可能会出现因评估指标的盲点而降低翻译质量的情况。</li>
<li>该研究在WMT24基准测试上采用了简单的但实际的最优N框架，实验涵盖了从3B到72B的五种模型大小以及最多达1024的TTS计算预算。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.19020">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_TTS/2509.19020v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_TTS/2509.19020v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_TTS/2509.19020v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_TTS/2509.19020v1/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_TTS/2509.19020v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="HD-PPT-Hierarchical-Decoding-of-Content-and-Prompt-Preference-Tokens-for-Instruction-based-TTS"><a href="#HD-PPT-Hierarchical-Decoding-of-Content-and-Prompt-Preference-Tokens-for-Instruction-based-TTS" class="headerlink" title="HD-PPT: Hierarchical Decoding of Content- and Prompt-Preference Tokens   for Instruction-based TTS"></a>HD-PPT: Hierarchical Decoding of Content- and Prompt-Preference Tokens   for Instruction-based TTS</h2><p><strong>Authors:Sihang Nie, Xiaofen Xing, Jingyuan Xing, Baiji Liu, Xiangmin Xu</strong></p>
<p>Large Language Model (LLM)-based Text-to-Speech (TTS) models have already reached a high degree of naturalness. However, the precision control of TTS inference is still challenging. Although instruction-based Text-to-Speech (Instruct-TTS) models are proposed, these models still lack fine-grained control due to the modality gap between single-level text instructions and multilevel speech tokens. To address this limitation, we propose HD-PPT, a framework that transforms speech synthesis into a structured, hierarchical task. To enable fine-grained control, we introduce a novel speech codec to extract distinct prompt-preference and content-preference tokens from the complex speech tokens, supervised by automatic speech recognition (ASR) and cross-lingual audio-text pre-training (CLAP) objectives. To bridge the modality gap of these tokens, we propose a hierarchical decoding strategy, where the LLM generates tokens in a structured order: first semantic, then fine-grained style, and finally complete acoustic representation. Extensive experiments demonstrate that this hierarchical paradigm significantly improves instruction adherence and achieves state-of-the-art naturalness, validating our approach for precise and controllable speech synthesis. Audio samples are available at <a target="_blank" rel="noopener" href="https://xxh333.github.io/">https://xxh333.github.io/</a>. </p>
<blockquote>
<p>基于大型语言模型（LLM）的文本到语音（TTS）模型已经实现了很高的自然度。然而，TTS推理的精确控制仍然具有挑战性。虽然提出了基于指令的文本到语音（Instruct-TTS）模型，但这些模型仍然缺乏精细控制，原因是单一层次的文本指令和多层次的语音令牌之间存在模态差距。为了解决这一局限性，我们提出了HD-PPT框架，将语音合成转变为一个结构化、分层的任务。为了实现精细控制，我们引入了一种新的语音编解码器，从复杂的语音令牌中提取出不同的提示偏好和内容偏好令牌，由自动语音识别（ASR）和跨语言音频文本预训练（CLAP）目标进行监督。为了弥合这些令牌的模态差距，我们提出了一种分层解码策略，LLM以结构化顺序生成令牌：首先是语义，然后是精细风格，最后是完整的声学表示。大量实验表明，这种分层范式显著提高了指令遵循能力，并实现了最先进的自然度，验证了我们方法在精确可控语音合成方面的有效性。音频样本可在<a target="_blank" rel="noopener" href="https://xxh333.github.io/%E6%89%BE%E5%88%B0%E3%80%82">https://xxh333.github.io/找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.19001v1">PDF</a> 5 pages, 2 figures, submitted to ICASSP2026</p>
<p><strong>Summary</strong></p>
<p>基于大型语言模型（LLM）的文本转语音（TTS）模型已具备高度自然度，但在TTS推理的精确控制上仍面临挑战。为解决指令式TTS模型因单一层级文本指令与多级语音标记之间的模态差距导致的精细控制缺失问题，我们提出了HD-PPT框架，将语音合成转化为结构化、分层的任务。通过引入新型语音编码技术，从复杂的语音标记中提取出不同的提示偏好和内容偏好标记，受到自动语音识别（ASR）和跨语言音频文本预训练（CLAP）目标的监督。为缩小这些标记的模态差距，我们提出了分层解码策略，LLM以结构化顺序生成标记：首先是语义，然后是精细风格，最后是完整的声学表示。实验证明，这种分层范式显著提高了指令遵循性，实现了最先进的自然度，验证了我们在精确可控语音合成方面的方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM-based TTS models have achieved high naturalness but face challenges in precise control of inference.</li>
<li>指令式TTS模型因模态差距影响精细控制。</li>
<li>HD-PPT框架将语音合成转化为结构化、分层的任务以提高控制精度。</li>
<li>新型语音编码技术从复杂的语音标记中提取出提示偏好和内容偏好标记。</li>
<li>ASR和CLAP目标用于监督编码过程。</li>
<li>采用分层解码策略，按照语义、精细风格和声学表示的结构化顺序生成标记。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.19001">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_TTS/2509.19001v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_TTS/2509.19001v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_TTS/2509.19001v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Direct-Preference-Optimization-for-Speech-Autoregressive-Diffusion-Models"><a href="#Direct-Preference-Optimization-for-Speech-Autoregressive-Diffusion-Models" class="headerlink" title="Direct Preference Optimization for Speech Autoregressive Diffusion   Models"></a>Direct Preference Optimization for Speech Autoregressive Diffusion   Models</h2><p><strong>Authors:Zhijun Liu, Dongya Jia, Xiaoqiang Wang, Chenpeng Du, Shuai Wang, Zhuo Chen, Haizhou Li</strong></p>
<p>Autoregressive diffusion models (ARDMs) have recently been applied to speech generation, achieving state-of-the-art (SOTA) performance in zero-shot text-to-speech. By autoregressively generating continuous speech tokens with next-token diffusion, these models offer a promising alternative to next-token prediction, avoiding the technical complexities associated with discrete speech tokenization. As a relatively new paradigm, research on reinforcement learning (RL)-based fine-tuning of speech ARDMs remains limited. In this paper, we propose Autoregressive Diffusion-Direct Preference Optimization (ARDM-DPO) to advance this research. By fine-tuning the recently proposed zero-shot text-to-speech model DiTAR with DPO, we achieve significant improvements in terms of speech expressiveness and robustness for long texts. </p>
<blockquote>
<p>自回归扩散模型（ARDMs）最近被应用于语音生成，在零样本文本到语音转换中达到了最先进的性能。这些模型通过自回归生成连续的语音令牌，使用下一个令牌的扩散，为下一个令牌的预测提供了有前景的替代方案，避免了与离散语音令牌化相关的技术复杂性。作为一种相对较新的范式，关于基于强化学习（RL）的语音ARDMs微调的研究仍然有限。在本文中，我们提出自回归扩散直接偏好优化（ARDM-DPO）以促进这项研究。通过用DPO微调最近提出的零样本文本到语音模型DiTAR，我们在语音表达性和长文本稳健性方面取得了显著改进。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.18928v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了自回归扩散模型（ARDMs）在语音生成领域的应用，特别是在零样本文本到语音转换方面取得了最先进的性能。文章提出了使用增强型学习进行ARDM优化的新思路，提出Autoregressive Diffusion-Direct Preference Optimization（ARDM-DPO），并通过微调零样本文本到语音模型DiTAR，显著提高了语音表达的丰富性和长文本的鲁棒性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>自回归扩散模型（ARDMs）已被应用于语音生成领域，且已达到了零样本文本到语音转换的最先进性能。</li>
<li>ARDM通过连续生成语音标记来实现自回归生成，相较于离散语音标记的预测，提供了一个具有前景的替代方案。该技术避免了相关的技术复杂性。</li>
<li>虽然自回归扩散模型已经在某些任务上展现出巨大潜力，但对于其强化学习（RL）精细调整的研究仍然有限。</li>
<li>提出了一种新的方法——ARDM-DPO，用于增强自回归扩散模型的性能。</li>
<li>ARDM-DPO通过对DiTAR模型的微调显著提高了语音表达的丰富性和长文本的鲁棒性。这可能有助于提高用户的语音体验并扩大这些模型的实际应用范围。</li>
<li>ARDM-DPO的出现可能会推动更多的研究关注如何利用强化学习来优化和进一步改善现有模型的性能。这可能会对自然语言处理和人工智能领域的未来发展产生重大影响。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.18928">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_TTS/2509.18928v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_TTS/2509.18928v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_TTS/2509.18928v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_TTS/2509.18928v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_TTS/2509.18928v1/page_3_1.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Group-Relative-Policy-Optimization-for-Text-to-Speech-with-Large-Language-Models"><a href="#Group-Relative-Policy-Optimization-for-Text-to-Speech-with-Large-Language-Models" class="headerlink" title="Group Relative Policy Optimization for Text-to-Speech with Large   Language Models"></a>Group Relative Policy Optimization for Text-to-Speech with Large   Language Models</h2><p><strong>Authors:Chang Liu, Ya-Jun Hu, Ying-Ying Gao, Shi-Lei Zhang, Zhen-Hua Ling</strong></p>
<p>This paper proposes a GRPO-based approach to enhance the performance of large language model (LLM)-based text-to-speech (TTS) models by deriving rewards from an off-the-shelf automatic speech recognition (ASR) model. Compared to previous reinforcement learning methods for LLM-based TTS, our method requires no dedicated model for reward computation or training. Moreover, we design a composite reward function that combines character error rate (CER) with negative log-likelihood (NLL) obtained from the ASR model, providing more informative and accurate reward signals. We apply GRPO fine-tuning to pre-trained LLM-based TTS models and evaluate their zero-shot TTS performance. Experimental results show that the proposed method substantially improves both the intelligibility and naturalness of synthesized speech. Ablation studies and further analyses confirm the effectiveness of integrating the two reward components. </p>
<blockquote>
<p>本文提出了一种基于GRPO的方法，通过从现成的自动语音识别（ASR）模型中获取奖励，来提高基于大型语言模型（LLM）的文本到语音（TTS）模型的性能。与以往的基于LLM的TTS强化学习方法相比，我们的方法不需要专门的奖励计算或训练模型。此外，我们设计了一个组合奖励函数，将字符错误率（CER）与从ASR模型获得的负对数似然值（NLL）相结合，提供更准确且信息丰富的奖励信号。我们将GRPO微调应用于预训练的LLM-based TTS模型，并评估它们的零样本TTS性能。实验结果表明，该方法可以显著提高合成语音的可懂度和自然度。消融研究和进一步的分析证实了整合两种奖励成分的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.18798v1">PDF</a> 5 pages,submitted to ICASSP2026</p>
<p><strong>Summary</strong></p>
<p>该论文提出了一种基于GRPO的方法，用于提高基于大型语言模型（LLM）的文本到语音（TTS）模型的性能。该方法通过利用现成的自动语音识别（ASR）模型来生成奖励，无需为奖励计算或训练设置专用模型。同时，设计了一种组合奖励函数，结合了字符错误率（CER）和自动语音识别模型得到的负对数似然值（NLL），提供更准确和丰富的奖励信号。该研究将GRPO微调应用于预训练的LLM-TTS模型上，并对其零样本TTS性能进行评估。实验结果显示，该方法显著提高了合成语音的可理解性和自然度。消融研究和进一步的分析证实了结合两种奖励成分的有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出了一种新的基于GRPO的方法提高LLM-TTS模型的性能。</li>
<li>利用现成的ASR模型生成奖励，简化了奖励计算和训练过程。</li>
<li>设计了一种组合奖励函数，结合了CER和ASR模型的NLL，增强了奖励信号的准确性。</li>
<li>GRPO被用于微调预训练的LLM-TTS模型。</li>
<li>实验结果显示，该方法提高了合成语音的智听力和自然度。</li>
<li>消融研究证明了结合两种奖励成分的有效性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.18798">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_TTS/2509.18798v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_TTS/2509.18798v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_TTS/2509.18798v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_TTS/2509.18798v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="No-Verifiable-Reward-for-Prosody-Toward-Preference-Guided-Prosody-Learning-in-TTS"><a href="#No-Verifiable-Reward-for-Prosody-Toward-Preference-Guided-Prosody-Learning-in-TTS" class="headerlink" title="No Verifiable Reward for Prosody: Toward Preference-Guided Prosody   Learning in TTS"></a>No Verifiable Reward for Prosody: Toward Preference-Guided Prosody   Learning in TTS</h2><p><strong>Authors:Seungyoun Shin, Dongha Ahn, Jiwoo Kim, Sungwook Jeon</strong></p>
<p>Recent work reports gains in neural text-to-speech (TTS) with Group Relative Policy Optimization (GRPO). However, in the absence of a verifiable reward for \textit{prosody}, GRPO trained on transcription-oriented signals (CER&#x2F;NLL) lowers error rates yet collapses prosody into monotone, unnatural speech; adding speaker-similarity further destabilizes training and degrades CER. We address this with an \textit{iterative Direct Preference Optimization (DPO)} scheme that uses only a few hundred human-labeled preference pairs per round to directly optimize prosodic naturalness while regularizing to the current model. On \textbf{KoCC-TTS}, a curated dataset of authentic Korean call center interactions capturing task-oriented dialogues, our method attains the highest human preference (ELO) with competitive CER, outperforming GRPO and strong commercial baselines. These results suggest that when prosody cannot be rewarded automatically, \textit{human preference optimization} offers a practical and data-efficient path to natural and robust TTS. The demo page is available at \href{<a target="_blank" rel="noopener" href="https://tts.ch.dev}/">https://tts.ch.dev}</a> </p>
<blockquote>
<p>近期的工作报告指出，在神经文本到语音（TTS）领域，采用群组相对策略优化（GRPO）取得了一些进展。然而，由于缺乏语调的可验证奖励，基于转录导向信号（CER&#x2F;NLL）训练的GRPO虽然降低了错误率，但却将语调融合成单调、不自然的语音；增加说话人相似性会进一步破坏训练并降低CER。我们通过采用一种\emph{迭代直接偏好优化（DPO）}方案来解决这一问题，该方案每轮仅使用数百个人工标记的偏好对，以直接优化语调的自然性，同时对其进行正则化以符合当前模型。在\textbf{KoCC-TTS}数据集上——一个包含真实韩国呼叫中心交互任务导向对话的精选数据集——我们的方法达到了最高的人类偏好（ELO），同时拥有竞争性的CER，超越了GRPO和强大的商业基线。这些结果表明，当无法自动奖励语调时，\emph{人类偏好优化}为自然和稳健的TTS提供了一条实用且数据高效的路。演示页面位于：[<a target="_blank" rel="noopener" href="https://tts.ch.dev/]">https://tts.ch.dev/]</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.18531v1">PDF</a> submitted to ICASSP 2026</p>
<p><strong>摘要</strong><br>    最近的工作报告表明，在神经文本到语音（TTS）领域采用群体相对策略优化（GRPO）取得了进展。然而，由于缺乏可验证的语调奖励，仅依靠转录导向信号（CER&#x2F;NLL）训练的GRPO虽然降低了错误率，但会使语调变得单调不自然；增加说话人相似性会进一步使训练不稳定并降低CER。我们通过采用一种迭代式直接偏好优化（DPO）方案来解决这一问题，该方案每轮仅使用数百个人工标记的偏好对，在优化语调自然性的同时，对当前模型进行规则化。在真实的韩国呼叫中心互动任务导向对话集KoCC-TTS上，我们的方法以最高的人类偏好（ELO）和具有竞争力的CER表现出色，超越了GRPO和强大的商业基线。这些结果表明，当无法自动奖励语调时，人类偏好优化为自然和稳健的TTS提供了一条实用且数据高效的道路。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>Group Relative Policy Optimization (GRPO) 在神经文本到语音（TTS）中的应用虽然降低了错误率，但可能导致语调单调不自然。</li>
<li>增加说话人相似性在GRPO训练中进一步导致训练不稳定并降低CER（字符错误率）。</li>
<li>迭代式直接偏好优化（DPO）方案被提出来解决这一问题，它直接使用少量人工标记的偏好对来优化语调的自然性。</li>
<li>DPO方案在规则化当前模型的同时，能有效提高语调的自然性。</li>
<li>在KoCC-TTS数据集上，DPO方法表现出卓越的人类偏好和竞争力强的CER，超越了GRPO和其他先进方法。</li>
<li>实验结果强调了人类偏好优化在TTS中的重要性，尤其是在无法自动奖励语调的情况下。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.18531">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_TTS/2509.18531v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_TTS/2509.18531v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_TTS/2509.18531v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Frustratingly-Easy-Data-Augmentation-for-Low-Resource-ASR"><a href="#Frustratingly-Easy-Data-Augmentation-for-Low-Resource-ASR" class="headerlink" title="Frustratingly Easy Data Augmentation for Low-Resource ASR"></a>Frustratingly Easy Data Augmentation for Low-Resource ASR</h2><p><strong>Authors:Katsumi Ibaraki, David Chiang</strong></p>
<p>This paper introduces three self-contained data augmentation methods for low-resource Automatic Speech Recognition (ASR). Our techniques first generate novel text–using gloss-based replacement, random replacement, or an LLM-based approach–and then apply Text-to-Speech (TTS) to produce synthetic audio. We apply these methods, which leverage only the original annotated data, to four languages with extremely limited resources (Vatlongos, Nashta, Shinekhen Buryat, and Kakabe). Fine-tuning a pretrained Wav2Vec2-XLSR-53 model on a combination of the original audio and generated synthetic data yields significant performance gains, including a 14.3% absolute WER reduction for Nashta. The methods prove effective across all four low-resource languages and also show utility for high-resource languages like English, demonstrating their broad applicability. </p>
<blockquote>
<p>本文介绍了三种针对低资源自动语音识别（ASR）的自主数据增强方法。我们的技术首先利用基于词垢替换、随机替换或基于大型语言模型（LLM）的方法生成新的文本，然后应用文本到语音（TTS）技术生成合成音频。我们只在有限资源的四种语言（瓦特隆戈斯语、纳斯塔语、谢内肯布里亚特语和卡卡贝语）中应用这些方法，仅利用原始注释数据。通过微调预训练的Wav2Vec2-XLSR-53模型，结合原始音频和生成的合成数据，取得了显著的性能提升，其中包括纳斯塔语的绝对字词错误率（WER）降低14.3%。这些方法在四种低资源语言中都证明了其有效性，并显示出对高资源语言如英语的实用性，证明了其广泛的应用性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.15373v2">PDF</a> 5 pages, 2 figures, 2 tables, submitted to ICASSP 2026</p>
<p><strong>摘要</strong></p>
<p>本文介绍了三种针对低资源自动语音识别（ASR）的自包含数据增强方法。这些方法首先利用基于词元替换、随机替换或大型语言模型（LLM）的方法生成新文本，然后应用文本到语音（TTS）技术生成合成音频。这些方法仅利用原始注释数据，应用于四种资源极为有限的语言（瓦特龙戈语、纳斯塔语、沙涅肯·布里亚特语和卡卡贝语）。对预训练的Wav2Vec2-XLSR-53模型进行微调，结合原始音频和生成的合成数据，取得了显著的性能提升，其中纳斯塔语的绝对字词错误率（WER）降低了14.3%。这些方法在四种低资源语言中都表现出有效性和实用性，并证明在高资源语言如英语中也有应用价值。</p>
<p><strong>要点</strong></p>
<ol>
<li>介绍了三种针对低资源ASR的自包含数据增强方法。</li>
<li>通过生成新文本并利用TTS技术转化为合成音频。</li>
<li>方法仅利用原始注释数据，适用于多种低资源语言。</li>
<li>在四种测试语言上取得了显著的性能提升。</li>
<li>在纳斯塔语中，绝对字词错误率（WER）降低了14.3%。</li>
<li>方法同样适用于高资源语言，如英语。</li>
<li>证明了这些方法具有广泛的应用性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.15373">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_TTS/2509.15373v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_TTS/2509.15373v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_TTS/2509.15373v2/page_1_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_TTS/2509.15373v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_TTS/2509.15373v2/page_3_0.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="UDDETTS-Unifying-Discrete-and-Dimensional-Emotions-for-Controllable-Emotional-Text-to-Speech"><a href="#UDDETTS-Unifying-Discrete-and-Dimensional-Emotions-for-Controllable-Emotional-Text-to-Speech" class="headerlink" title="UDDETTS: Unifying Discrete and Dimensional Emotions for Controllable   Emotional Text-to-Speech"></a>UDDETTS: Unifying Discrete and Dimensional Emotions for Controllable   Emotional Text-to-Speech</h2><p><strong>Authors:Jiaxuan Liu, Yang Xiang, Han Zhao, Xiangang Li, Yingying Gao, Shilei Zhang, Zhenhua Ling</strong></p>
<p>Recent large language models (LLMs) have made great progress in the field of text-to-speech (TTS), but they still face major challenges in synthesizing fine-grained emotional speech in an interpretable manner. Traditional methods rely on discrete emotion labels to control emotion categories and intensities, which cannot capture the complexity and continuity of human emotional perception and expression. The lack of large-scale emotional speech datasets with balanced emotion distributions and fine-grained emotional annotations often causes overfitting in synthesis models and impedes effective emotion control. To address these issues, we propose UDDETTS, a universal LLM framework unifying discrete and dimensional emotions for controllable emotional TTS. This model introduces the interpretable Arousal-Dominance-Valence (ADV) space for dimensional emotion description and supports emotion control driven by either discrete emotion labels or nonlinearly quantified ADV values. Furthermore, a semi-supervised training strategy is designed to comprehensively utilize diverse speech datasets with different types of emotional annotations to train the UDDETTS. Experiments show that UDDETTS achieves linear emotion control along three interpretable dimensions, and exhibits superior end-to-end emotional speech synthesis capabilities. Code and demos are available at: <a target="_blank" rel="noopener" href="https://anonymous.4open.science/w/UDDETTS">https://anonymous.4open.science/w/UDDETTS</a>. </p>
<blockquote>
<p>最近的大型语言模型（LLM）在文本到语音（TTS）领域取得了巨大的进步，但它们在以可解释的方式合成精细情绪语音方面仍面临重大挑战。传统的方法依赖于离散的情绪标签来控制情绪类别和强度，这无法捕捉人类情绪感知和表达的复杂性和连续性。缺乏具有平衡情绪分布和精细情绪注释的大规模情绪语音数据集通常会导致合成模型中的过度拟合，并阻碍有效的情绪控制。为了解决这些问题，我们提出了UDDETTS，这是一个统一离散和维度情绪的通用LLM框架，用于可控的情绪TTS。该模型引入可解释的Arousal-Dominance-Valence（ADV）空间进行维度情绪描述，并支持由离散情绪标签或非线性量化的ADV值驱动的情绪控制。此外，设计了一种半监督训练策略，以综合利用具有不同类型情绪注释的多样化语音数据集来训练UDDETTS。实验表明，UDDETTS在三个可解释维度上实现了线性情绪控制，并展现出出色的端到端情绪语音合成能力。代码和演示可在：<a target="_blank" rel="noopener" href="https://anonymous.4open.science/w/UDDETTS%E8%AE%BF%E9%97%AE%E3%80%82">https://anonymous.4open.science/w/UDDETTS访问。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.10599v2">PDF</a> Under review</p>
<p><strong>Summary</strong></p>
<p>近期大型语言模型在文本转语音领域取得显著进展，但在精细情感语音合成方面仍面临挑战。传统方法依赖离散情感标签控制情感类别和强度，无法捕捉人类情感感知和表达的复杂性和连续性。针对这些问题，我们提出了UDDETTS模型，该模型结合了离散情感和维度情感，实现了可控的情感TTS。该模型引入可解释的激活度-支配力-价值（ADV）空间进行维度情感描述，支持由离散情感标签或非线性量化的ADV值驱动的情感控制。此外，我们设计了一种半监督训练策略，以充分利用不同类型的情感注释的语音数据集来训练UDDETTS。实验表明，UDDETTS在三个可解释维度上实现了线性情感控制，并展示了出色的端到端情感语音合成能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型在TTS领域有进步，但在精细情感语音合成上仍面临挑战。</li>
<li>传统方法使用离散情感标签，无法捕捉人类情感感知和表达的复杂性。</li>
<li>UDDETTS模型结合了离散情感和维度情感，实现可控的情感TTS。</li>
<li>UDDETTS引入ADV空间进行维度情感描述，支持离散情感标签或ADV值驱动的情感控制。</li>
<li>UDDETTS采用半监督训练策略，利用多种情感注释的语音数据集。</li>
<li>实验显示UDDETTS在三个维度上实现线性情感控制。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.10599">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_TTS/2505.10599v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_TTS/2505.10599v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_TTS/2505.10599v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="SupertonicTTS-Towards-Highly-Efficient-and-Streamlined-Text-to-Speech-System"><a href="#SupertonicTTS-Towards-Highly-Efficient-and-Streamlined-Text-to-Speech-System" class="headerlink" title="SupertonicTTS: Towards Highly Efficient and Streamlined Text-to-Speech   System"></a>SupertonicTTS: Towards Highly Efficient and Streamlined Text-to-Speech   System</h2><p><strong>Authors:Hyeongju Kim, Jinhyeok Yang, Yechan Yu, Seunghun Ji, Jacob Morton, Frederik Bous, Joon Byun, Juheon Lee</strong></p>
<p>We introduce SupertonicTTS, a novel text-to-speech (TTS) system designed for efficient and streamlined speech synthesis. SupertonicTTS comprises three components: a speech autoencoder for continuous latent representation, a text-to-latent module leveraging flow-matching for text-to-latent mapping, and an utterance-level duration predictor. To enable a lightweight architecture, we employ a low-dimensional latent space, temporal compression of latents, and ConvNeXt blocks. The TTS pipeline is further simplified by operating directly on raw character-level text and employing cross-attention for text-speech alignment, thus eliminating the need for grapheme-to-phoneme (G2P) modules and external aligners. In addition, we propose context-sharing batch expansion that accelerates loss convergence and stabilizes text-speech alignment with minimal memory and I&#x2F;O overhead. Experimental results demonstrate that SupertonicTTS delivers performance comparable to contemporary zero-shot TTS models with only 44M parameters, while significantly reducing architectural complexity and computational cost. Audio samples are available at: <a target="_blank" rel="noopener" href="https://supertonictts.github.io/">https://supertonictts.github.io/</a>. </p>
<blockquote>
<p>我们引入了SupertonicTTS，这是一个为高效和简洁的语音合成而设计的新型文本到语音（TTS）系统。SupertonicTTS包含三个组件：用于连续潜在表示的语音自动编码器、利用流匹配进行文本到潜在映射的文本到潜在模块，以及语句级持续时间预测器。为了实现轻量级架构，我们采用了低维潜在空间、潜在时间的临时压缩和ConvNeXt块。TTS管道通过直接在原始字符级文本上操作并利用跨注意力进行文本语音对齐，从而进一步简化，从而消除了对字母到音素（G2P）模块和外部对齐器的需求。此外，我们提出了上下文共享批量扩展，可加速损失收敛，并使用最小的内存和I&#x2F;O开销来稳定文本语音对齐。实验结果表明，SupertonicTTS在仅使用44M参数的情况下，性能与当代零样本TTS模型相当，同时显著降低了架构的复杂性和计算成本。音频样本可在以下网址找到：<a target="_blank" rel="noopener" href="https://supertonictts.github.io/%E3%80%82">https://supertonictts.github.io/。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.23108v3">PDF</a> 22 pages, preprint</p>
<p><strong>Summary</strong></p>
<p>SupertonicTTS是一种新型文本到语音（TTS）系统，包含三个组件：语音自编码器、文本到潜在模块和语句级时长预测器。该系统采用低维潜在空间、潜在时态压缩和ConvNeXt块实现轻量化架构，并直接操作字符级文本，采用跨注意力文本语音对齐，无需grapheme-to-phoneme（G2P）模块和外部对齐器。同时，提出上下文共享批量扩展技术，加速损失收敛，稳定文本语音对齐，且内存和I&#x2F;O开销较小。实验结果显示，SupertonicTTS在参数仅44M的情况下，性能与当前零样本TTS模型相当，同时降低了架构复杂性和计算成本。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SupertonicTTS是一种新型的文本到语音（TTS）系统，旨在实现高效和流线型的语音合成。</li>
<li>系统包含三个主要组件：语音自编码器、文本到潜在模块和语句级时长预测器。</li>
<li>采用低维潜在空间、潜在时态压缩和ConvNeXt块实现轻量化架构。</li>
<li>系统直接操作字符级文本，采用跨注意力机制进行文本语音对齐，无需额外的grapheme-to-phoneme（G2P）模块和外部对齐器。</li>
<li>提出了上下文共享批量扩展技术，以提高效率并稳定文本语音对齐。</li>
<li>实验结果表明，SupertonicTTS在参数较少的情况下实现了与当前零样本TTS模型相当的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.23108">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_TTS/2503.23108v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_TTS/2503.23108v3/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_TTS/2503.23108v3/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_TTS/2503.23108v3/page_5_0.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Scaling-Rich-Style-Prompted-Text-to-Speech-Datasets"><a href="#Scaling-Rich-Style-Prompted-Text-to-Speech-Datasets" class="headerlink" title="Scaling Rich Style-Prompted Text-to-Speech Datasets"></a>Scaling Rich Style-Prompted Text-to-Speech Datasets</h2><p><strong>Authors:Anuj Diwan, Zhisheng Zheng, David Harwath, Eunsol Choi</strong></p>
<p>We introduce Paralinguistic Speech Captions (ParaSpeechCaps), a large-scale dataset that annotates speech utterances with rich style captions. While rich abstract tags (e.g. guttural, nasal, pained) have been explored in small-scale human-annotated datasets, existing large-scale datasets only cover basic tags (e.g. low-pitched, slow, loud). We combine off-the-shelf text and speech embedders, classifiers and an audio language model to automatically scale rich tag annotations for the first time. ParaSpeechCaps covers a total of 59 style tags, including both speaker-level intrinsic tags and utterance-level situational tags. It consists of 342 hours of human-labelled data (PSC-Base) and 2427 hours of automatically annotated data (PSC-Scaled). We finetune Parler-TTS, an open-source style-prompted TTS model, on ParaSpeechCaps, and achieve improved style consistency (+7.9% Consistency MOS) and speech quality (+15.5% Naturalness MOS) over the best performing baseline that combines existing rich style tag datasets. We ablate several of our dataset design choices to lay the foundation for future work in this space. Our dataset, models and code are released at <a target="_blank" rel="noopener" href="https://github.com/ajd12342/paraspeechcaps">https://github.com/ajd12342/paraspeechcaps</a> . </p>
<blockquote>
<p>我们引入了Paralinguistic Speech Captions（ParaSpeechCaps）这一大规模数据集，该数据集对语音发声进行了丰富的风格字幕标注。虽然抽象标签（如喉音、鼻音、痛苦）已在小规模人工标注的数据集中得到探索，但现有大规模数据集仅涵盖基本标签（如低音、慢速、大声）。我们首次结合现成的文本和语音嵌入器、分类器以及音频语言模型，自动扩展丰富的标签注释。ParaSpeechCaps共涵盖59个风格标签，包括说话者级别的内在标签和发声级别的情境标签。它包含342小时的人工标注数据（PSC-Base）和2427小时的自动注释数据（PSC-Scaled）。我们在ParaSpeechCaps上微调了Parler-TTS这一开源的风格提示TTS模型，与结合现有丰富风格标签数据集的最佳基线相比，实现了风格一致性（+7.9%一致性平均意见得分）和语音质量（+15.5%自然度平均意见得分）的提升。我们对数据集设计的几个选择进行了剖析，为未来在这个领域的工作奠定基础。我们的数据集、模型和代码已在<a target="_blank" rel="noopener" href="https://github.com/ajd12342/paraspeechcaps%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/ajd12342/paraspeechcaps发布。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04713v2">PDF</a> EMNLP 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了Paralinguistic Speech Captions（ParaSpeechCaps）数据集，该数据集对语音片段进行了丰富的风格标注。它结合了现有的文本和语音嵌入器、分类器以及音频语言模型，首次实现了丰富的标签标注的大规模自动化。ParaSpeechCaps涵盖了59种风格标签，包括说话者级别的内在标签和话语级别的情境标签。该数据集包括342小时的人工标注数据和2427小时的自动标注数据。通过对Parler-TTS模型进行微调，实现了风格一致性和语音质量的改进。数据集、模型和代码已在GitHub上发布。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Paralinguistic Speech Captions（ParaSpeechCaps）是一个大型数据集，将语音片段与丰富的风格标注相结合。</li>
<li>现有大型数据集主要覆盖基本标签，而ParaSpeechCaps涵盖了59种风格标签，包括说话者级别的内在标签和话语级别的情境标签。</li>
<li>该数据集包括人工标注和自动标注的数据，分别包含342小时和2427小时的数据。</li>
<li>通过在ParaSpeechCaps上微调Parler-TTS模型，实现了风格一致性和语音质量的改进。</li>
<li>风格一致性提高了7.9%，语音质量提高了15.5%。</li>
<li>数据集、模型和代码已公开发布在GitHub上，方便未来研究使用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04713">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_TTS/2503.04713v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_TTS/2503.04713v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_TTS/2503.04713v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_TTS/2503.04713v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="OpenOmni-Advancing-Open-Source-Omnimodal-Large-Language-Models-with-Progressive-Multimodal-Alignment-and-Real-Time-Self-Aware-Emotional-Speech-Synthesis"><a href="#OpenOmni-Advancing-Open-Source-Omnimodal-Large-Language-Models-with-Progressive-Multimodal-Alignment-and-Real-Time-Self-Aware-Emotional-Speech-Synthesis" class="headerlink" title="OpenOmni: Advancing Open-Source Omnimodal Large Language Models with   Progressive Multimodal Alignment and Real-Time Self-Aware Emotional Speech   Synthesis"></a>OpenOmni: Advancing Open-Source Omnimodal Large Language Models with   Progressive Multimodal Alignment and Real-Time Self-Aware Emotional Speech   Synthesis</h2><p><strong>Authors:Run Luo, Ting-En Lin, Haonan Zhang, Yuchuan Wu, Xiong Liu, Min Yang, Yongbin Li, Longze Chen, Jiaming Li, Lei Zhang, Xiaobo Xia, Hamid Alinejad-Rokny, Fei Huang</strong></p>
<p>Recent advancements in omnimodal learning have significantly improved understanding and generation across images, text, and speech, yet these developments remain predominantly confined to proprietary models. The lack of high-quality omnimodal datasets and the challenges of real-time emotional speech synthesis have notably hindered progress in open-source research. To address these limitations, we introduce \name, a two-stage training framework that integrates omnimodal alignment and speech generation to develop a state-of-the-art omnimodal large language model. In the alignment phase, a pre-trained speech model undergoes further training on text-image tasks, enabling (near) zero-shot generalization from vision to speech, outperforming models trained on tri-modal datasets. In the speech generation phase, a lightweight decoder is trained on speech tasks with direct preference optimization, enabling real-time emotional speech synthesis with high fidelity. Experiments show that \name surpasses state-of-the-art models across omnimodal, vision-language, and speech-language benchmarks. It achieves a 4-point absolute improvement on OmniBench over the leading open-source model VITA, despite using 5x fewer training samples and a smaller model size (7B vs. 7x8B). Additionally, \name achieves real-time speech generation with &lt;1s latency at non-autoregressive mode, reducing inference time by 5x compared to autoregressive methods, and improves emotion classification accuracy by 7.7% </p>
<blockquote>
<p>近期多模态学习的进展在图像、文本和语音的理解与生成方面取得了显著的提升，但这些发展主要局限于专有模型。缺乏高质量的多模态数据集以及实时情感语音合成的挑战显著阻碍了开源研究的进展。为了解决这些局限性，我们引入了名为XX的两阶段训练框架，该框架融合了多模态对齐和语音生成，以开发先进的多模态大型语言模型。在对齐阶段，预训练的语音模型在文本-图像任务上接受进一步训练，实现了从视觉到语音的（接近）零样本泛化，超越了那些在三元模态数据集上训练的模型。在语音生成阶段，一个轻量级的解码器在语音任务上进行直接偏好优化训练，能够实现高保真度的实时情感语音合成。实验表明，XX在多模态、视觉语言和语音语言基准测试中均超越了最先进的模型。尽管使用了较少的训练样本和较小的模型规模（7B vs. 7x8B），但在OmniBench上相对于领先的开源模型VITA实现了4个点的绝对改进。此外，XX在非自回归模式下实现了实时语音生成，延迟时间小于1秒，与自回归方法相比减少了5倍的推理时间，并提高了7.7%的情感分类准确率。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.04561v6">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>近期多模态学习领域的进展在图像、文本和语音的理解与生成方面取得了显著提升，主要局限于专有模型。然而，高质量多模态数据集缺乏和实时情感语音合成的挑战限制了开源研究的进展。为解决这些问题，我们提出了名为\name的两阶段训练框架，它集成了多模态对齐和语音生成，以开发先进的多模态大型语言模型。第一阶段是对齐，预训练的语音模型在文本-图像任务上进行进一步训练，实现了从视觉到语音的零样本或少量样本泛化，超越了仅在三模态数据集上训练的模型。第二阶段是语音生成，使用直接偏好优化的轻量级解码器进行语音任务训练，实现了高保真度的实时情感语音合成。实验表明，\name在多模态、视觉语言和语音语言基准测试中超越了现有技术最先进模型。相较于领先的开源模型VITA，它在OmniBench上实现了4个点的绝对提升，且使用较少的训练样本和更小的模型规模（7B对7x8B）。此外，\name在非自回归模式下实现了&lt;1秒的延迟，推理时间减少了5倍，情感分类准确率提高了7.7%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态学习在图像、文本和语音方面的理解取得进展，但主要局限于专有模型。</li>
<li>高质量多模态数据集的缺乏和实时情感语音合成的挑战限制了开源研究。</li>
<li>\name是一个两阶段训练框架，通过对齐和语音生成解决上述挑战。</li>
<li>对齐阶段实现了从视觉到语音的零样本或少量样本泛化。</li>
<li>语音生成阶段实现了高保真度的实时情感语音合成。</li>
<li>\name在多模态基准测试中表现超越现有技术最先进模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.04561">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_TTS/2501.04561v6/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_TTS/2501.04561v6/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_TTS/2501.04561v6/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_TTS/2501.04561v6/page_4_0.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-28/TTS/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-28/TTS/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/TTS/">
                                    <span class="chip bg-color">TTS</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-28/Interactive/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Interactive/2509.19676v1/page_0_0.jpg" class="responsive-img" alt="Interactive">
                        
                        <span class="card-title">Interactive</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Interactive 方向最新论文已更新，请持续关注 Update in 2025-09-28  Thinking While Listening Simple Test Time Scaling For Audio   Classification
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-28
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Interactive/" class="post-category">
                                    Interactive
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Interactive/">
                        <span class="chip bg-color">Interactive</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-28/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_医学图像/2509.19258v1/page_4_0.jpg" class="responsive-img" alt="医学图像">
                        
                        <span class="card-title">医学图像</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学图像 方向最新论文已更新，请持续关注 Update in 2025-09-28  Modelling the effect of stellar metallicity on the XUV evolution of   low-mass stars and its impact on exoplanet atmospheres/habitability
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-28
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    医学图像
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">医学图像</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">28879.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
