<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="TTS">
    <meta name="description" content="TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-28  UniSS Unified Expressive Speech-to-Speech Translation with Your Voice">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>TTS | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic-private.zhihu.com/v2-e607b8f83468f4baea55d969f7d47e58~resize:0:q75.jpg?source=1f5c5e47&expiration=1760040003&auth_key=1760040003-0-0-59d9c6317775f1e319e369f811dc9bd4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">TTS</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/TTS/">
                                <span class="chip bg-color">TTS</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                TTS
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-28
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-07
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    15.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    64 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-09-28-æ›´æ–°"><a href="#2025-09-28-æ›´æ–°" class="headerlink" title="2025-09-28 æ›´æ–°"></a>2025-09-28 æ›´æ–°</h1><h2 id="UniSS-Unified-Expressive-Speech-to-Speech-Translation-with-Your-Voice"><a href="#UniSS-Unified-Expressive-Speech-to-Speech-Translation-with-Your-Voice" class="headerlink" title="UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice"></a>UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice</h2><p><strong>Authors:Sitong Cheng, Weizhen Bian, Xinsheng Wang, Ruibin Yuan, Jianyi Chen, Shunshun Yin, Yike Guo, Wei Xue</strong></p>
<p>The ultimate goal of expressive speech-to-speech translation (S2ST) is to accurately translate spoken content while preserving the speaker identity and emotional style. However, progress in this field is largely hindered by three key challenges: the scarcity of paired speech data that retains expressive styles, the complexity of multi-stage processing pipelines, and the limited transfer of translation capabilities from large language models (LLMs). In this work, we address these challenges by introducing UniSS, a novel single-stage framework for expressive S2ST. Our approach features carefully designed speech semantic and style modeling, enabling seamless integration with existing text-based LLM frameworks to develop a unified text-speech language model. To transfer translation capabilities from text to speech, we propose a cross-modal chain-of-thought prompting process that progressively aligns audio semantics with text and ensures style preservation in the decoded results. Furthermore, we construct and release a large-scale, high-quality expressive S2ST dataset, UniST, comprising 44.8k hours of data. Experimental results show that UniSS significantly outperforms previous methods in translation fidelity and speech quality while preserving voice, emotion, and duration consistency. Our work establishes a simpler and more effective paradigm for building the next generation of expressive S2ST systems. Audio samples are available at <a target="_blank" rel="noopener" href="https://cmots.github.io/uniss-demo">https://cmots.github.io/uniss-demo</a>. </p>
<blockquote>
<p>è¡¨è¾¾æ€§è¯­éŸ³åˆ°è¯­éŸ³ç¿»è¯‘ï¼ˆS2STï¼‰çš„æœ€ç»ˆç›®æ ‡æ˜¯å‡†ç¡®ç¿»è¯‘å£è¯­å†…å®¹ï¼ŒåŒæ—¶ä¿ç•™è¯´è¯äººçš„èº«ä»½å’Œæƒ…æ„Ÿé£æ ¼ã€‚ç„¶è€Œï¼Œè¯¥é¢†åŸŸçš„è¿›å±•ä¸»è¦å—ä¸‰ä¸ªå…³é”®æŒ‘æˆ˜çš„å½±å“ï¼šç¼ºä¹ä¿ç•™è¡¨è¾¾é£æ ¼çš„é…å¯¹è¯­éŸ³æ•°æ®ã€å¤šé˜¶æ®µå¤„ç†æµç¨‹çš„å¤æ‚æ€§ï¼Œä»¥åŠä»å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿ç§»ç¿»è¯‘èƒ½åŠ›æœ‰é™ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å¼•å…¥UniSSï¼Œå³ä¸€ç§æ–°å‹çš„å•é˜¶æ®µè¡¨è¾¾æ€§S2STæ¡†æ¶æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„è¯­éŸ³è¯­ä¹‰å’Œé£æ ¼å»ºæ¨¡ï¼Œå®ç°ä¸ç°æœ‰çš„æ–‡æœ¬åŸºäºLLMæ¡†æ¶æ— ç¼é›†æˆï¼Œä»¥å¼€å‘ç»Ÿä¸€çš„æ–‡æœ¬-è¯­éŸ³è¯­è¨€æ¨¡å‹ã€‚ä¸ºäº†ä»æ–‡æœ¬åˆ°è¯­éŸ³è¿ç§»ç¿»è¯‘èƒ½åŠ›ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è·¨æ¨¡æ€æ€ç»´é“¾æç¤ºè¿‡ç¨‹ï¼Œè¯¥è¿‡ç¨‹é€æ­¥å¯¹é½éŸ³é¢‘è¯­ä¹‰å’Œæ–‡æœ¬ï¼Œå¹¶ç¡®ä¿åœ¨è§£ç ç»“æœä¸­ä¿æŒé£æ ¼ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ„å»ºå¹¶å‘å¸ƒäº†ä¸€ä¸ªå¤§è§„æ¨¡ã€é«˜è´¨é‡çš„è¡¨è¾¾æ€§S2STæ•°æ®é›†UniSTï¼ŒåŒ…å«44.8kå°æ—¶çš„æ•°æ®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒUniSSåœ¨ç¿»è¯‘ä¿çœŸåº¦å’Œè¯­éŸ³è´¨é‡æ–¹é¢æ˜¾è‘—ä¼˜äºä»¥å‰çš„æ–¹æ³•ï¼ŒåŒæ—¶ä¿ç•™è¯­éŸ³ã€æƒ…æ„Ÿå’ŒæŒç»­æ—¶é—´ä¸€è‡´æ€§ã€‚æˆ‘ä»¬çš„å·¥ä½œä¸ºå»ºç«‹ä¸‹ä¸€ä»£è¡¨è¾¾æ€§S2STç³»ç»Ÿå»ºç«‹äº†æ›´ç®€å•ã€æ›´æœ‰æ•ˆçš„èŒƒå¼ã€‚éŸ³é¢‘æ ·æœ¬å¯åœ¨<a target="_blank" rel="noopener" href="https://cmots.github.io/uniss-demo%E8%8E%B7%E5%8F%96%E3%80%82">https://cmots.github.io/uniss-demoè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.21144v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è¡¨è¾¾æ€§è¯­éŸ³åˆ°è¯­éŸ³ç¿»è¯‘ï¼ˆS2STï¼‰é¢†åŸŸçš„æœ€æ–°ç ”ç©¶è¿›å±•ã€‚é’ˆå¯¹è¯¥é¢†åŸŸé¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¦‚ç¼ºä¹ä¿ç•™è¡¨è¾¾é£æ ¼çš„æ•°æ®ã€å¤šé˜¶æ®µå¤„ç†ç®¡é“å¤æ‚ä»¥åŠå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç¿»è¯‘èƒ½åŠ›è¿ç§»æœ‰é™ï¼Œç ”ç©¶è€…æå‡ºäº†ä¸€ç§åä¸ºUniSSçš„æ–°å‹å•ä¸€é˜¶æ®µæ¡†æ¶ã€‚è¯¥æ¡†æ¶ç»“åˆäº†è¯­éŸ³è¯­ä¹‰å’Œé£æ ¼å»ºæ¨¡ï¼Œå¯ä¸ç°æœ‰æ–‡æœ¬è¯­è¨€æ¨¡å‹æ¡†æ¶æ— ç¼é›†æˆï¼Œé€šè¿‡è·¨æ¨¡æ€æ€ç»´é“¾æç¤ºè¿‡ç¨‹å®ç°ç¿»è¯‘èƒ½åŠ›çš„è·¨æ¨¡æ€è¿ç§»ã€‚æ­¤å¤–ï¼Œè¿˜æ„å»ºå¹¶å‘å¸ƒäº†ä¸€ä¸ªå¤§è§„æ¨¡é«˜è´¨é‡çš„è¡¨è¾¾æ€§S2STæ•°æ®é›†UniSTã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒUniSSåœ¨ç¿»è¯‘å‡†ç¡®æ€§å’Œè¯­éŸ³è´¨é‡æ–¹é¢æ˜¾è‘—ä¼˜äºä»¥å‰çš„æ–¹æ³•ï¼ŒåŒæ—¶ä¿æŒäº†è¯­éŸ³ã€æƒ…æ„Ÿå’ŒæŒç»­æ—¶é—´çš„ä¸€è‡´æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¡¨è¾¾æ€§è¯­éŸ³åˆ°è¯­éŸ³ç¿»è¯‘ï¼ˆS2STï¼‰æ—¨åœ¨å‡†ç¡®ç¿»è¯‘è¯­éŸ³å†…å®¹ï¼ŒåŒæ—¶ä¿ç•™è¯´è¯äººçš„èº«ä»½å’Œæƒ…æ„Ÿé£æ ¼ã€‚</li>
<li>S2STé¢†åŸŸé¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜åŒ…æ‹¬ç¼ºä¹ä¿ç•™è¡¨è¾¾é£æ ¼çš„æ•°æ®ã€å¤šé˜¶æ®µå¤„ç†ç®¡é“çš„å¤æ‚æ€§ä»¥åŠå¤§å‹è¯­è¨€æ¨¡å‹çš„ç¿»è¯‘èƒ½åŠ›è¿ç§»é™åˆ¶ã€‚</li>
<li>UniSSæ˜¯ä¸€ç§æ–°å‹å•ä¸€é˜¶æ®µæ¡†æ¶ï¼Œé€šè¿‡ç»“åˆè¯­éŸ³è¯­ä¹‰å’Œé£æ ¼å»ºæ¨¡æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚</li>
<li>UniSSæ¡†æ¶å¯ä¸ç°æœ‰æ–‡æœ¬è¯­è¨€æ¨¡å‹æ— ç¼é›†æˆï¼Œé€šè¿‡è·¨æ¨¡æ€æ€ç»´é“¾æç¤ºè¿‡ç¨‹å®ç°ç¿»è¯‘èƒ½åŠ›çš„è·¨æ¨¡æ€è¿ç§»ã€‚</li>
<li>UniSTæ•°æ®é›†çš„æ„å»ºå’Œå‘å¸ƒä¸ºè¡¨è¾¾æ€§S2STç ”ç©¶æä¾›äº†å¤§è§„æ¨¡é«˜è´¨é‡çš„æ•°æ®èµ„æºã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒUniSSåœ¨ç¿»è¯‘å‡†ç¡®æ€§å’Œè¯­éŸ³è´¨é‡æ–¹é¢æ˜¾è‘—ä¼˜äºä»¥å¾€æ–¹æ³•ï¼Œèƒ½å¤Ÿä¿æŒè¯­éŸ³ã€æƒ…æ„Ÿå’ŒæŒç»­æ—¶é—´çš„ä¸€è‡´æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.21144">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-8a0de899618b869c8b98b45b608971c3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760040011&auth_key=1760040011-0-0-dda377bb2b3faaf0adde963bd87925e8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-de4abaebc9ee51402c2f5f09426c657d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760040018&auth_key=1760040018-0-0-ae5b4acba0772a496f4d6f0dcccf5c49&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e681117409a4f2332c6d1477cd577a59~resize:0:q75.jpg?source=1f5c5e47&expiration=1760040024&auth_key=1760040024-0-0-a4428a8162f52d59ca94b55bad480a2b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c0bbb3610ac3421c09b0e75d5fe3dac5~resize:0:q75.jpg?source=1f5c5e47&expiration=1760040031&auth_key=1760040031-0-0-9b7ae974e5725e7b13e10ce977fdcd33&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="i-LAVA-Insights-on-Low-Latency-Voice-2-Voice-Architecture-for-Agents"><a href="#i-LAVA-Insights-on-Low-Latency-Voice-2-Voice-Architecture-for-Agents" class="headerlink" title="i-LAVA: Insights on Low Latency Voice-2-Voice Architecture for Agents"></a>i-LAVA: Insights on Low Latency Voice-2-Voice Architecture for Agents</h2><p><strong>Authors:Anupam Purwar, Aditya Choudhary</strong></p>
<p>We experiment with a low-latency, end-to-end voice-to-voice communication model to optimize it for real-time conversational applications. By analyzing components essential to voice to voice (V-2-V) system viz. automatic speech recognition (ASR), text-to-speech (TTS), and dialog management, our work analyzes how to reduce processing time while maintaining high-quality interactions to identify the levers for optimizing V-2-V system. Our work identifies that TTS component which generates life-like voice, full of emotions including natural pauses and exclamations has highest impact on Real time factor (RTF). The experimented V-2-V architecture utilizes CSM1b has the capability to understand tone as well as context of conversation by ingesting both audio and text of prior exchanges to generate contextually accurate speech. We explored optimization of Residual Vector Quantization (RVQ) iterations by the TTS decoder which come at a cost of decrease in the quality of voice generated. Our experimental evaluations also demonstrate that for V-2-V implementations based on CSM most important optimizations can be brought by reducing the number of RVQ Iterations along with the codebooks used in Mimi. </p>
<blockquote>
<p>æˆ‘ä»¬å®éªŒäº†ä¸€ç§ä½å»¶è¿Ÿã€ç«¯åˆ°ç«¯çš„è¯­éŸ³åˆ°è¯­éŸ³é€šä¿¡æ¨¡å‹ï¼Œä»¥ä¼˜åŒ–å…¶é€‚ç”¨äºå®æ—¶å¯¹è¯åº”ç”¨ã€‚é€šè¿‡åˆ†æè¯­éŸ³åˆ°è¯­éŸ³ï¼ˆV-2-Vï¼‰ç³»ç»Ÿçš„å…³é”®ç»„ä»¶ï¼ŒåŒ…æ‹¬è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ã€æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰å’Œå¯¹è¯ç®¡ç†ï¼Œæˆ‘ä»¬çš„å·¥ä½œåˆ†æäº†å¦‚ä½•åœ¨ä¿æŒé«˜è´¨é‡äº¤äº’çš„åŒæ—¶å‡å°‘å¤„ç†æ—¶é—´ï¼Œä»¥ç¡®å®šä¼˜åŒ–V-2-Vç³»ç»Ÿçš„å…³é”®å› ç´ ã€‚æˆ‘ä»¬çš„å·¥ä½œç¡®å®šï¼Œå¯¹å®æ—¶å› å­ï¼ˆRTFï¼‰å½±å“æœ€å¤§çš„æ˜¯TTSç»„ä»¶ï¼Œè¯¥ç»„ä»¶ç”Ÿæˆé€¼çœŸçš„å£°éŸ³ï¼Œå……æ»¡æƒ…æ„Ÿï¼ŒåŒ…æ‹¬è‡ªç„¶åœé¡¿å’Œæ„Ÿå¹ã€‚æ‰€å®éªŒçš„V-2-Væ¶æ„é‡‡ç”¨CSM1bï¼Œå…·æœ‰é€šè¿‡æ‘„å–å…ˆå‰çš„éŸ³é¢‘å’Œæ–‡æœ¬å¯¹è¯æ¥ç†è§£å’ŒæŠŠæ¡å¯¹è¯è¯­è°ƒåŠä¸Šä¸‹æ–‡çš„èƒ½åŠ›ï¼Œä»è€Œç”Ÿæˆè¯­å¢ƒå‡†ç¡®çš„è¯­éŸ³ã€‚æˆ‘ä»¬æ¢ç´¢äº†é€šè¿‡TTSè§£ç å™¨ä¼˜åŒ–å‰©ä½™çŸ¢é‡é‡åŒ–ï¼ˆRVQï¼‰è¿­ä»£çš„æ–¹æ³•ï¼Œä½†è¿™ä¼šå¯¼è‡´ç”Ÿæˆçš„è¯­éŸ³è´¨é‡ä¸‹é™ã€‚æˆ‘ä»¬çš„å®éªŒè¯„ä¼°è¿˜è¡¨æ˜ï¼Œå¯¹äºåŸºäºCSMçš„V-2-Vå®ç°ï¼Œæœ€é‡è¦çš„ä¼˜åŒ–å¯ä»¥é€šè¿‡å‡å°‘RVQè¿­ä»£æ¬¡æ•°ä»¥åŠä¸Mimiä¸­ä½¿ç”¨çš„ä»£ç ç°¿ä¸€èµ·å®ç°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.20971v1">PDF</a> This paper analyzes a low-latency, end-to-end voice-to-voice (V-2-V)   architecture, identifying that the Text-to-Speech (TTS) component has the   highest impact on real-time performance. By reducing the number of Residual   Vector Quantization (RVQ) iterations in the TTS model, latency can be   effectively halved, creating a direct trade-off between conversational speed   and audio quality</p>
<p><strong>Summary</strong><br>     æˆ‘ä»¬å®éªŒäº†ä¸€ä¸ªä½å»¶è¿Ÿã€ç«¯åˆ°ç«¯çš„è¯­éŸ³åˆ°è¯­éŸ³é€šä¿¡æ¨¡å‹ï¼Œä»¥ä¼˜åŒ–å…¶é€‚ç”¨äºå®æ—¶å¯¹è¯åº”ç”¨ã€‚é€šè¿‡åˆ†æè¯­éŸ³åˆ°è¯­éŸ³ï¼ˆV-2-Vï¼‰ç³»ç»Ÿçš„å…³é”®ç»„ä»¶ï¼Œå¦‚è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ã€æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰å’Œå¯¹è¯ç®¡ç†ï¼Œæˆ‘ä»¬çš„å·¥ä½œåˆ†æäº†å¦‚ä½•åœ¨ä¿æŒé«˜è´¨é‡äº¤äº’çš„åŒæ—¶å‡å°‘å¤„ç†æ—¶é—´ï¼Œä»¥æ‰¾å‡ºä¼˜åŒ–V-2-Vç³»ç»Ÿçš„å…³é”®æ‰‹æ®µã€‚ç ”ç©¶å‘ç°ï¼Œæ–‡æœ¬åˆ°è¯­éŸ³è½¬æ¢ç¯èŠ‚å¯¹å®æ—¶æ€§èƒ½å½±å“æœ€å¤§ï¼Œç”Ÿæˆçš„è¯­éŸ³éœ€çœŸå®ã€å……æ»¡æƒ…æ„Ÿï¼ŒåŒ…æ‹¬è‡ªç„¶åœé¡¿å’Œæ„Ÿå¹ã€‚å®éªŒçš„V-2-Væ¶æ„åˆ©ç”¨CSM1bï¼Œèƒ½å¤Ÿç†è§£å’Œé€‚åº”å¯¹è¯çš„è¯­å¢ƒå’Œè¯­è°ƒï¼Œé€šè¿‡è¾“å…¥å…ˆå‰çš„å¯¹è¯éŸ³é¢‘å’Œæ–‡æœ¬ç”Ÿæˆè¯­å¢ƒå‡†ç¡®çš„è¯­éŸ³ã€‚æˆ‘ä»¬æ¢ç´¢äº†é€šè¿‡å‡å°‘TTSè§£ç å™¨çš„å‰©ä½™å‘é‡é‡åŒ–ï¼ˆRVQï¼‰è¿­ä»£æ¬¡æ•°æ¥ä¼˜åŒ–æ€§èƒ½ï¼Œä½†è¿™å¯èƒ½ä¼šå¯¼è‡´ç”Ÿæˆçš„è¯­éŸ³è´¨é‡ä¸‹é™ã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œå¯¹äºåŸºäºCSMçš„V-2-Vå®ç°ï¼Œæœ€é‡è¦çš„ä¼˜åŒ–æªæ–½æ˜¯å‡å°‘RVQè¿­ä»£æ¬¡æ•°å’Œæ‰€ä½¿ç”¨çš„ç æœ¬æ•°é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å®éªŒç ”ç©¶äº†ä½å»¶è¿Ÿç«¯åˆ°ç«¯çš„è¯­éŸ³åˆ°è¯­éŸ³é€šä¿¡æ¨¡å‹ã€‚</li>
<li>ç ”ç©¶åˆ†æäº†è¯­éŸ³åˆ°è¯­éŸ³ç³»ç»Ÿçš„å…³é”®ç»„ä»¶ï¼ŒåŒ…æ‹¬è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ã€æ–‡æœ¬åˆ°è¯­éŸ³è½¬æ¢å’Œå¯¹è¯ç®¡ç†ã€‚</li>
<li>å‘ç°æ–‡æœ¬åˆ°è¯­éŸ³è½¬æ¢ç¯èŠ‚å¯¹å®æ—¶æ€§èƒ½çš„å½±å“æœ€å¤§ï¼Œå¹¶éœ€è¦å…³æ³¨ç”Ÿæˆè¯­éŸ³çš„è‡ªç„¶æ€§å’Œæƒ…æ„Ÿè¡¨è¾¾ã€‚</li>
<li>å®éªŒä¸­ä½¿ç”¨çš„V-2-Væ¶æ„åˆ©ç”¨CSMæŠ€æœ¯èƒ½å¤Ÿç†è§£å’Œé€‚åº”å¯¹è¯çš„è¯­å¢ƒå’Œè¯­è°ƒã€‚</li>
<li>ä¼˜åŒ–é€šè¿‡å‡å°‘å‰©ä½™å‘é‡é‡åŒ–ï¼ˆRVQï¼‰è¿­ä»£æ¬¡æ•°æ¥å®ç°æ€§èƒ½æå‡æ˜¯ä¸€ä¸ªå¯è¡Œçš„æ–¹æ³•ï¼Œä½†éœ€æƒè¡¡ç”Ÿæˆçš„è¯­éŸ³è´¨é‡æŸå¤±ã€‚</li>
<li>å®éªŒè¯„ä¼°è¡¨æ˜å‡å°‘RVQè¿­ä»£æ¬¡æ•°å’Œä½¿ç”¨è¾ƒå°‘çš„ç æœ¬æ•°é‡æ˜¯ä¼˜åŒ–åŸºäºCSMçš„V-2-Vå®ç°çš„å…³é”®æªæ–½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.20971">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-3bb81516e8015be4b3e8144c6c180994~resize:0:q75.jpg?source=1f5c5e47&expiration=1760040038&auth_key=1760040038-0-0-bdfbeb7b62c60f643b046ac69d916278&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3cbb1be5f6f4abfe46e04380b855efe8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760040045&auth_key=1760040045-0-0-ef3191e2a5f21ebeb7ce0ace2bd9d8f1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-01288a991ef3f2796f87fa445020472c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760040051&auth_key=1760040051-0-0-792ebea97dab032ee1f11b7170490eb6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-73080709ee9ddccebf7778fd101a721c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760040057&auth_key=1760040057-0-0-4ef54be5f31e6e4df01b663398c1ea70&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b57654b72585c80284a6f451fa33f172~resize:0:q75.jpg?source=1f5c5e47&expiration=1760040064&auth_key=1760040064-0-0-77056153dd7a677370965735bf408452&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5b79dcef8256e74f28cbd8cbd4af3a5a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760040070&auth_key=1760040070-0-0-d746076bc9a0a62019d94d24edb01b45&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e94936736293e3c4c16cb9c549985aa1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760040076&auth_key=1760040076-0-0-ca8f51214c15e5c7524f9db098ce268d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-04da26b77e97dedca53121eaf10c46f0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760040082&auth_key=1760040082-0-0-fc07a30dd0ecee72099e1f315ce3e177&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f172e27dd6f3e845c2d74b84595f16d0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760040088&auth_key=1760040088-0-0-2624fc6b01314b0cda0977694e1c63eb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="SPADE-Structured-Pruning-and-Adaptive-Distillation-for-Efficient-LLM-TTS"><a href="#SPADE-Structured-Pruning-and-Adaptive-Distillation-for-Efficient-LLM-TTS" class="headerlink" title="SPADE: Structured Pruning and Adaptive Distillation for Efficient   LLM-TTS"></a>SPADE: Structured Pruning and Adaptive Distillation for Efficient   LLM-TTS</h2><p><strong>Authors:Tan Dat Nguyen, Jaehun Kim, Ji-Hoon Kim, Shukjae Choi, Youshin Lim, Joon Son Chung</strong></p>
<p>The goal of this paper is to introduce SPADE, a framework for Structured Pruning and Adaptive Distillation for Efficient Large Language Model-based text-to-speech (LLM-TTS). Recent LLM-TTS systems achieve strong controllability and zero-shot generalization, but their large parameter counts and high latency limit real-world deployment. SPADE addresses this by combining (i) a pruning step guided by a word-error-rate-based layer importance index to remove non-essential Transformer layers, with (ii) multi-level knowledge distillation to restore autoregressive coherence. On zero-shot benchmarks, SPADE preserves near-parity perceptual quality while halving Transformer depth, reducing VRAM usage by up to 20%, and achieving up to 1.7x faster real-time factor with less than 5% of the original training data. These results show that compact LLM-TTS models can maintain naturalness and speaker similarity while enabling practical real-time speech generation. Audio samples are available at <a target="_blank" rel="noopener" href="https://mm.kaist.ac.kr/projects/SPADE/">https://mm.kaist.ac.kr/projects/SPADE/</a>. </p>
<blockquote>
<p>æœ¬æ–‡çš„ç›®æ ‡æ˜¯ä»‹ç»SPADEï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºç»“æ„åŒ–å‰ªæå’Œè‡ªé€‚åº”è’¸é¦çš„æ¡†æ¶ï¼Œç”¨äºå®ç°åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰çš„é«˜æ•ˆè½¬æ¢ã€‚æœ€è¿‘çš„LLM-TTSç³»ç»Ÿå®ç°äº†å¼ºå¤§çš„å¯æ§æ€§å’Œé›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œä½†å®ƒä»¬çš„å‚æ•°æ•°é‡åºå¤§å’Œå»¶è¿Ÿè¾ƒé«˜ï¼Œé™åˆ¶äº†å…¶åœ¨ç°å®ä¸–ç•Œä¸­çš„éƒ¨ç½²ã€‚SPADEé€šè¿‡ç»“åˆï¼ˆiï¼‰ä»¥åŸºäºè¯é”™è¯¯ç‡çš„å±‚é‡è¦æ€§æŒ‡æ•°ä¸ºæŒ‡å¯¼çš„å‰ªææ­¥éª¤æ¥å»é™¤éå¿…è¦çš„Transformerå±‚ï¼Œä»¥åŠï¼ˆiiï¼‰å¤šå±‚æ¬¡çŸ¥è¯†è’¸é¦æ¥æ¢å¤è‡ªå›å½’è¿è´¯æ€§æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚åœ¨é›¶æ ·æœ¬åŸºå‡†æµ‹è¯•ä¸­ï¼ŒSPADEåœ¨ä¿æŒè¿‘ä¹ç›¸åŒçš„æ„ŸçŸ¥è´¨é‡çš„åŒæ—¶ï¼Œå°†Transformeræ·±åº¦å‡åŠï¼Œå°†VRAMä½¿ç”¨é‡å‡å°‘é«˜è¾¾20%ï¼Œå¹¶ä¸”ä½¿ç”¨ä¸åˆ°åŸå§‹è®­ç»ƒæ•°æ®çš„5%å³å¯è¾¾åˆ°é«˜è¾¾1.7å€çš„å®æ—¶å› å­ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œç´§å‡‘çš„LLM-TTSæ¨¡å‹èƒ½å¤Ÿåœ¨ä¿æŒè‡ªç„¶åº¦å’Œè¯´è¯äººç›¸ä¼¼æ€§çš„åŒæ—¶ï¼Œå®ç°å®ç”¨çš„å®æ—¶è¯­éŸ³ç”Ÿæˆã€‚éŸ³é¢‘æ ·æœ¬å¯åœ¨<a target="_blank" rel="noopener" href="https://mm.kaist.ac.kr/projects/SPADE/%E6%89%BE%E5%88%B0%E3%80%82">https://mm.kaist.ac.kr/projects/SPADE/æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.20802v1">PDF</a> Submitted to ICASSP 2026</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†SPADEæ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†ç»“æ„åŒ–å‰ªæå’Œè‡ªé€‚åº”è’¸é¦æŠ€æœ¯ï¼Œæ—¨åœ¨æé«˜åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆLLM-TTSï¼‰çš„æ•ˆç‡ã€‚SPADEé€šè¿‡å‰ªææ­¥éª¤å»é™¤éå¿…è¦çš„Transformerå±‚ï¼Œå¹¶ç»“åˆå¤šå±‚æ¬¡çŸ¥è¯†è’¸é¦æ¥ä¿æŒæ–‡æœ¬çš„è¿è´¯æ€§ã€‚åœ¨é›¶æ ·æœ¬åŸºå‡†æµ‹è¯•ä¸­ï¼ŒSPADEåœ¨å‡å°‘Transformeræ·±åº¦ã€é™ä½VRAMä½¿ç”¨é‡å’Œæé«˜å®æ—¶è¯­éŸ³ç”Ÿæˆé€Ÿåº¦çš„åŒæ—¶ï¼Œä¿æŒäº†è¿‘ä¹ç›¸åŒçš„æ„ŸçŸ¥è´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SPADEæ˜¯ä¸€ä¸ªé’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹æ–‡æœ¬åˆ°è¯­éŸ³è½¬æ¢ï¼ˆLLM-TTSï¼‰çš„æ•ˆç‡ä¼˜åŒ–æ¡†æ¶ã€‚</li>
<li>å®ƒé€šè¿‡ç»“åˆç»“æ„åŒ–å‰ªæå’Œè‡ªé€‚åº”è’¸é¦æŠ€æœ¯æ¥å®ç°ä¼˜åŒ–ã€‚</li>
<li>SPADEé€šè¿‡åŸºäºå•è¯é”™è¯¯ç‡çš„å±‚é‡è¦æ€§æŒ‡æ•°æ¥æŒ‡å¯¼å‰ªææ­¥éª¤ï¼Œå»é™¤éå¿…è¦çš„Transformerå±‚ã€‚</li>
<li>å¤šå±‚æ¬¡çŸ¥è¯†è’¸é¦ç”¨äºæ¢å¤æ–‡æœ¬çš„è¿è´¯æ€§ã€‚</li>
<li>åœ¨é›¶æ ·æœ¬åŸºå‡†æµ‹è¯•ä¸­ï¼ŒSPADEåœ¨ä¿æŒè¿‘ä¹ç›¸åŒçš„æ„ŸçŸ¥è´¨é‡çš„åŒæ—¶ï¼Œå®ç°äº†Transformeræ·±åº¦çš„å‡åŠã€‚</li>
<li>SPADEé™ä½äº†VRAMä½¿ç”¨é‡ï¼Œæœ€é«˜å¯è¾¾20%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.20802">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-005df415cfcb770f41b2399175752181~resize:0:q75.jpg?source=1f5c5e47&expiration=1760040095&auth_key=1760040095-0-0-dca446e3c9910218ca6737d486baa607&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b9098df3c5f5397b83d8f81e5b6afeee~resize:0:q75.jpg?source=1f5c5e47&expiration=1760040102&auth_key=1760040102-0-0-8074ff324b0d7ff7f16544e97f170aa1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d568c733eec88d9d70e7eac2fd141b4e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760040109&auth_key=1760040109-0-0-bb95b6be1a4b1ff482b32776c3ead0dd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1face85ddeb2ea42c7bfca1ef2f29348~resize:0:q75.jpg?source=1f5c5e47&expiration=1760040116&auth_key=1760040116-0-0-0b3794e087f9adda20f0c7e39ac96707&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Beyond-Global-Emotion-Fine-Grained-Emotional-Speech-Synthesis-with-Dynamic-Word-Level-Modulation"><a href="#Beyond-Global-Emotion-Fine-Grained-Emotional-Speech-Synthesis-with-Dynamic-Word-Level-Modulation" class="headerlink" title="Beyond Global Emotion: Fine-Grained Emotional Speech Synthesis with   Dynamic Word-Level Modulation"></a>Beyond Global Emotion: Fine-Grained Emotional Speech Synthesis with   Dynamic Word-Level Modulation</h2><p><strong>Authors:Sirui Wang, Andong Chen, Tiejun Zhao</strong></p>
<p>Emotional text-to-speech (E-TTS) is central to creating natural and trustworthy human-computer interaction. Existing systems typically rely on sentence-level control through predefined labels, reference audio, or natural language prompts. While effective for global emotion expression, these approaches fail to capture dynamic shifts within a sentence. To address this limitation, we introduce Emo-FiLM, a fine-grained emotion modeling framework for LLM-based TTS. Emo-FiLM aligns frame-level features from emotion2vec to words to obtain word-level emotion annotations, and maps them through a Feature-wise Linear Modulation (FiLM) layer, enabling word-level emotion control by directly modulating text embeddings. To support evaluation, we construct the Fine-grained Emotion Dynamics Dataset (FEDD) with detailed annotations of emotional transitions. Experiments show that Emo-FiLM outperforms existing approaches on both global and fine-grained tasks, demonstrating its effectiveness and generality for expressive speech synthesis. </p>
<blockquote>
<p>æƒ…æ„Ÿæ–‡æœ¬è½¬è¯­éŸ³ï¼ˆE-TTSï¼‰æ˜¯åˆ›å»ºè‡ªç„¶å¯ä¿¡çš„äººæœºäº¤äº’çš„æ ¸å¿ƒã€‚ç°æœ‰ç³»ç»Ÿé€šå¸¸ä¾èµ–äºé€šè¿‡é¢„è®¾æ ‡ç­¾ã€å‚è€ƒéŸ³é¢‘æˆ–è‡ªç„¶è¯­è¨€æç¤ºæ¥è¿›è¡Œå¥å­çº§åˆ«çš„æ§åˆ¶ã€‚è™½ç„¶è¿™äº›æ–¹æ³•å¯¹äºå…¨å±€æƒ…æ„Ÿè¡¨è¾¾æœ‰æ•ˆï¼Œä½†å®ƒä»¬æ— æ³•æ•æ‰å¥å­ä¸­çš„åŠ¨æ€å˜åŒ–ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†Emo-FiLMï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„TTSç³»ç»Ÿçš„ç²¾ç»†æƒ…æ„Ÿå»ºæ¨¡æ¡†æ¶ã€‚Emo-FiLMé€šè¿‡å¯¹é½æ¥è‡ªemotion2vecçš„å¸§çº§ç‰¹å¾ï¼Œè·å¾—è¯çº§æƒ…æ„Ÿæ³¨é‡Šï¼Œå¹¶é€šè¿‡ç‰¹å¾çº¿æ€§è°ƒåˆ¶ï¼ˆFiLMï¼‰å±‚è¿›è¡Œæ˜ å°„ï¼Œé€šè¿‡ç›´æ¥è°ƒåˆ¶æ–‡æœ¬åµŒå…¥æ¥å®ç°è¯çº§æƒ…æ„Ÿæ§åˆ¶ã€‚ä¸ºäº†æ”¯æŒè¯„ä¼°ï¼Œæˆ‘ä»¬æ„å»ºäº†ç²¾ç»†æƒ…æ„ŸåŠ¨æ€æ•°æ®é›†ï¼ˆFEDDï¼‰ï¼Œå…¶ä¸­åŒ…å«äº†æƒ…æ„Ÿè½¬å˜çš„è¯¦ç»†æ³¨é‡Šã€‚å®éªŒè¡¨æ˜ï¼ŒEmo-FiLMåœ¨å…¨å±€å’Œç²¾ç»†ä»»åŠ¡ä¸Šçš„è¡¨ç°éƒ½ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè¯æ˜äº†å…¶åœ¨è¡¨ç°åŠ›è¯­éŸ³åˆæˆä¸­çš„æœ‰æ•ˆæ€§å’Œæ™®éæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.20378v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æƒ…æ„Ÿæ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆE-TTSï¼‰æ˜¯åˆ›å»ºè‡ªç„¶å’Œå¯ä¿¡èµ–çš„äººæœºäº¤äº’çš„æ ¸å¿ƒã€‚ç°æœ‰ç³»ç»Ÿé€šå¸¸ä¾èµ–äºé€šè¿‡é¢„å®šä¹‰æ ‡ç­¾ã€å‚è€ƒéŸ³é¢‘æˆ–è‡ªç„¶è¯­è¨€æç¤ºè¿›è¡Œå¥å­çº§åˆ«çš„æ§åˆ¶ï¼Œè™½ç„¶å¯¹å…¨çƒæƒ…æ„Ÿè¡¨è¾¾æœ‰æ•ˆï¼Œä½†æ— æ³•æ•æ‰å¥å­å†…çš„åŠ¨æ€å˜åŒ–ã€‚ä¸ºè§£å†³è¿™ä¸€å±€é™ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºäºLLMçš„TTSçš„ç²¾ç»†æƒ…æ„Ÿå»ºæ¨¡æ¡†æ¶Emo-FiLMã€‚Emo-FiLMé€šè¿‡å¯¹é½æƒ…æ„Ÿç‰¹å¾çš„å¸§çº§æ•°æ®ï¼ˆemotion2vecï¼‰ä¸å•è¯ï¼Œè·å¾—å•è¯çº§åˆ«çš„æƒ…æ„Ÿæ³¨é‡Šï¼Œå¹¶é€šè¿‡ç‰¹å¾çº¿æ€§è°ƒåˆ¶ï¼ˆFiLMï¼‰å±‚è¿›è¡Œæ˜ å°„ï¼Œèƒ½å¤Ÿç›´æ¥è°ƒåˆ¶æ–‡æœ¬åµŒå…¥ï¼Œå®ç°å•è¯çº§åˆ«çš„æƒ…æ„Ÿæ§åˆ¶ã€‚ä¸ºæ”¯æŒè¯„ä¼°ï¼Œæˆ‘ä»¬æ„å»ºäº†ç²¾ç»†æƒ…æ„ŸåŠ¨æ€æ•°æ®é›†ï¼ˆFEDDï¼‰ï¼ŒåŒ…å«è¯¦ç»†çš„æƒ…æ„Ÿè¿‡æ¸¡æ³¨é‡Šã€‚å®éªŒè¡¨æ˜ï¼ŒEmo-FiLMåœ¨å…¨å±€å’Œç²¾ç»†ä»»åŠ¡ä¸Šå‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè¯æ˜å…¶åœ¨è¡¨æƒ…è¯­éŸ³åˆæˆä¸­çš„æœ‰æ•ˆæ€§å’Œæ™®éæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>E-TTSåœ¨åˆ›å»ºè‡ªç„¶å’Œå¯ä¿¡èµ–çš„äººæœºäº¤äº’ä¸­èµ·å…³é”®ä½œç”¨ã€‚</li>
<li>ç°æœ‰ç³»ç»Ÿæ— æ³•æ•æ‰å¥å­å†…çš„åŠ¨æ€æƒ…æ„Ÿå˜åŒ–ã€‚</li>
<li>Emo-FiLMæ¡†æ¶è¢«å¼•å…¥ä»¥è§£å†³æ­¤é—®é¢˜ï¼Œå¹¶å®ç°å•è¯çº§åˆ«çš„æƒ…æ„Ÿæ§åˆ¶ã€‚</li>
<li>Emo-FiLMé€šè¿‡å¯¹é½æƒ…æ„Ÿç‰¹å¾çš„å¸§çº§æ•°æ®ä¸å•è¯æ¥è·å¾—å•è¯çº§åˆ«çš„æƒ…æ„Ÿæ³¨é‡Šã€‚</li>
<li>Feature-wise Linear Modulationï¼ˆFiLMï¼‰å±‚ç”¨äºæ˜ å°„å’Œè°ƒåˆ¶æ–‡æœ¬åµŒå…¥ï¼Œå®ç°ç²¾ç»†æƒ…æ„Ÿæ§åˆ¶ã€‚</li>
<li>ä¸ºæ”¯æŒè¯„ä¼°ï¼Œæ„å»ºäº†ç²¾ç»†æƒ…æ„ŸåŠ¨æ€æ•°æ®é›†ï¼ˆFEDDï¼‰ï¼ŒåŒ…å«è¯¦ç»†çš„æƒ…æ„Ÿè¿‡æ¸¡æ³¨é‡Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.20378">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-31f045f76814f2554e6c43fc5279c48b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760040124&auth_key=1760040124-0-0-a5b58fa78b11892c800e4cfa695500da&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e54a30bf0860763fd05bf4cbe54d6a89~resize:0:q75.jpg?source=1f5c5e47&expiration=1760040131&auth_key=1760040131-0-0-f408f0773d25ca8efc0320ae9a00585e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6ee57311bc679cd7b497a515c7738d30~resize:0:q75.jpg?source=1f5c5e47&expiration=1760040138&auth_key=1760040138-0-0-dc1656803c851712fbe9b566b6b26c62&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8f67c96395071bd6065c9daf106c5232~resize:0:q75.jpg?source=1f5c5e47&expiration=1760040145&auth_key=1760040145-0-0-350c4475c5509f49266813a5fc1f638d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Measuring-Prosody-Diversity-in-Zero-Shot-TTS-A-New-Metric-Benchmark-and-Exploration"><a href="#Measuring-Prosody-Diversity-in-Zero-Shot-TTS-A-New-Metric-Benchmark-and-Exploration" class="headerlink" title="Measuring Prosody Diversity in Zero-Shot TTS: A New Metric, Benchmark,   and Exploration"></a>Measuring Prosody Diversity in Zero-Shot TTS: A New Metric, Benchmark,   and Exploration</h2><p><strong>Authors:Yifan Yang, Bing Han, Hui Wang, Long Zhou, Wei Wang, Mingyu Cui, Xu Tan, Xie Chen</strong></p>
<p>Prosody diversity is essential for achieving naturalness and expressiveness in zero-shot text-to-speech (TTS). However, frequently used acoustic metrics capture only partial views of prosodic variation and correlate poorly with human perception, leaving the problem of reliably quantifying prosody diversity underexplored. To bridge this gap, we introduce ProsodyEval, a prosody diversity assessment dataset that provides Prosody Mean Opinion Score (PMOS) alongside conventional acoustic metrics. ProsodyEval comprises 1000 speech samples derived from 7 mainstream TTS systems, with 2000 human ratings. Building on this, we propose the Discretized Speech Weighted Edit Distance (DS-WED), a new objective diversity metric that quantifies prosodic variation via weighted edit distance over semantic tokens. Experiments on ProsodyEval show that DS-WED achieves substantially higher correlation with human judgments than existing acoustic metrics, while remaining highly robust in speech tokenization from HuBERT and WavLM. Leveraging DS-WED, we benchmark state-of-the-art open-source TTS systems on LibriSpeech test-clean and Seed-TTS test-en, and further explorations uncover several factors that influence prosody diversity, including generative modeling paradigms, duration control, and reinforcement learning. Moreover, we find that current large audio language models (LALMs) remain limited in capturing prosodic variations. Audio samples are available at <a target="_blank" rel="noopener" href="https://prosodyeval.github.io/">https://prosodyeval.github.io</a>. </p>
<blockquote>
<p>éŸµå¾‹å¤šæ ·æ€§å¯¹äºå®ç°é›¶æ ·æœ¬æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰çš„è‡ªç„¶åº¦å’Œè¡¨ç°åŠ›è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œå¸¸ç”¨çš„å£°å­¦æŒ‡æ ‡åªèƒ½æ•æ‰éƒ¨åˆ†éŸµå¾‹å˜åŒ–ï¼Œä¸äººç±»æ„ŸçŸ¥çš„ç›¸å…³æ€§è¾ƒå·®ï¼Œå¯¼è‡´å¯é åœ°é‡åŒ–éŸµå¾‹å¤šæ ·æ€§çš„é—®é¢˜å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†ProsodyEvalæ•°æ®é›†ï¼Œä¸€ä¸ªæä¾›éŸµå¾‹å¤šæ ·æ€§è¯„ä¼°çš„æ•°æ®é›†ï¼ŒåŒæ—¶æä¾›éŸµå¾‹å¹³å‡æ„è§å¾—åˆ†ï¼ˆPMOSï¼‰å’Œä¼ ç»Ÿå£°å­¦æŒ‡æ ‡ã€‚ProsodyEvalåŒ…å«æ¥è‡ª7ä¸ªä¸»æµTTSç³»ç»Ÿçš„1000ä¸ªè¯­éŸ³æ ·æœ¬å’Œ2000ä¸ªäººç±»è¯„åˆ†ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æå‡ºäº†ç¦»æ•£è¯­éŸ³åŠ æƒç¼–è¾‘è·ç¦»ï¼ˆDS-WEDï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„å®¢è§‚å¤šæ ·æ€§æŒ‡æ ‡ï¼Œå®ƒé€šè¿‡è¯­ä¹‰æ ‡è®°çš„åŠ æƒç¼–è¾‘è·ç¦»æ¥é‡åŒ–éŸµå¾‹å˜åŒ–ã€‚åœ¨ProsodyEvalä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä¸ç°æœ‰çš„å£°å­¦æŒ‡æ ‡ç›¸æ¯”ï¼ŒDS-WEDä¸äººç±»åˆ¤æ–­çš„ç›¸å…³æ€§æ›´é«˜ï¼ŒåŒæ—¶åœ¨HuBERTå’ŒWavLMçš„è¯­éŸ³æ ‡è®°åŒ–ä¸­è¡¨ç°å‡ºé«˜åº¦ç¨³å¥æ€§ã€‚åˆ©ç”¨DS-WEDï¼Œæˆ‘ä»¬å¯¹LibriSpeechæµ‹è¯•æ¸…æ´ç‰ˆå’ŒSeed-TTSæµ‹è¯•è‹±è¯­ç‰ˆä¸­çš„æœ€æ–°å¼€æºTTSç³»ç»Ÿè¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œè¿›ä¸€æ­¥æ¢ç´¢å½±å“éŸµå¾‹å¤šæ ·æ€§çš„å‡ ä¸ªå› ç´ ï¼ŒåŒ…æ‹¬ç”Ÿæˆå»ºæ¨¡èŒƒå¼ã€æŒç»­æ—¶é—´æ§åˆ¶å’Œå¼ºåŒ–å­¦ä¹ ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°å½“å‰çš„å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆLALMï¼‰åœ¨æ•æ‰éŸµå¾‹å˜åŒ–æ–¹é¢ä»ç„¶å­˜åœ¨å±€é™æ€§ã€‚éŸ³é¢‘æ ·æœ¬å¯åœ¨<a target="_blank" rel="noopener" href="https://prosodyeval.github.io/">https://prosodyeval.github.io</a>è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.19928v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å¼ºè°ƒäº†è¯­è°ƒå¤šæ ·æ€§åœ¨é›¶æ ·æœ¬æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰ä¸­çš„é‡è¦æ€§ï¼Œå¹¶æå‡ºä¸€ä¸ªåä¸ºProsodyEvalçš„è¯­è°ƒå¤šæ ·æ€§è¯„ä¼°æ•°æ®é›†ï¼Œç”¨äºæä¾›è¯­è°ƒå¹³å‡æ„è§å¾—åˆ†ï¼ˆPMOSï¼‰å’Œä¼ ç»Ÿå£°å­¦æŒ‡æ ‡ã€‚æ–‡ç« è¿˜æå‡ºäº†ä¸€ç§æ–°çš„å®¢è§‚å¤šæ ·æ€§æŒ‡æ ‡â€”â€”ç¦»æ•£è¯­éŸ³åŠ æƒç¼–è¾‘è·ç¦»ï¼ˆDS-WEDï¼‰ï¼Œç”¨äºé‡åŒ–è¯­ä¹‰æ ‡è®°çš„åŠ æƒç¼–è¾‘è·ç¦»æ¥è¯„ä¼°è¯­è°ƒå˜åŒ–ã€‚å®éªŒè¡¨æ˜ï¼ŒDS-WEDä¸äººç±»åˆ¤æ–­çš„ç›¸å…³æ€§é«˜äºç°æœ‰å£°å­¦æŒ‡æ ‡ï¼ŒåŒæ—¶é«˜åº¦ç¨³å¥äºè¯­éŸ³æ ‡è®°åŒ–è¿‡ç¨‹ã€‚åŸºäºæ­¤æŒ‡æ ‡ï¼Œä½œè€…å¯¹é¢†å…ˆçš„å¼€æºTTSç³»ç»Ÿè¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶æ¢è®¨äº†å½±å“è¯­è°ƒå¤šæ ·æ€§çš„å¤šç§å› ç´ ï¼ŒåŒ…æ‹¬ç”Ÿæˆå»ºæ¨¡èŒƒå¼ã€æ—¶é•¿æ§åˆ¶å’Œå¼ºåŒ–å­¦ä¹ ç­‰ã€‚åŒæ—¶æŒ‡å‡ºå½“å‰å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹åœ¨æ•æ‰è¯­è°ƒå˜åŒ–æ–¹é¢çš„å±€é™æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬å¼ºè°ƒäº†è¯­è°ƒå¤šæ ·æ€§åœ¨é›¶æ ·æœ¬æ–‡æœ¬è½¬è¯­éŸ³ä¸­çš„é‡è¦æ€§ã€‚</li>
<li>æå‡ºæ–°çš„è¯­è°ƒå¤šæ ·æ€§è¯„ä¼°æ•°æ®é›†ProsodyEvalï¼Œç»“åˆPMOSå’Œå¸¸è§„å£°å­¦æŒ‡æ ‡è¿›è¡Œè¯„ä¼°ã€‚</li>
<li>æå‡ºæ–°çš„å®¢è§‚å¤šæ ·æ€§æŒ‡æ ‡DS-WEDï¼Œç”¨äºé‡åŒ–è¯­è°ƒå˜åŒ–ã€‚</li>
<li>DS-WEDä¸äººç±»åˆ¤æ–­çš„ç›¸å…³æ€§é«˜äºç°æœ‰å£°å­¦æŒ‡æ ‡ï¼Œä¸”åœ¨è¯­éŸ³æ ‡è®°åŒ–è¿‡ç¨‹ä¸­è¡¨ç°ç¨³å¥ã€‚</li>
<li>åŸºäºDS-WEDæŒ‡æ ‡è¯„ä¼°äº†é¢†å…ˆçš„å¼€æºTTSç³»ç»Ÿæ€§èƒ½ã€‚</li>
<li>ç ”ç©¶å‘ç°ç”Ÿæˆå»ºæ¨¡èŒƒå¼ã€æ—¶é•¿æ§åˆ¶å’Œå¼ºåŒ–å­¦ä¹ ç­‰å› ç´ ä¼šå½±å“è¯­è°ƒå¤šæ ·æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.19928">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-eb8d0e93f9fd021fbb408c6734802071~resize:0:q75.jpg?source=1f5c5e47&expiration=1760040153&auth_key=1760040153-0-0-7dc9df68014d4d83b922d458086504eb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6e991a39ccfe8f2fcde2de9b11a219a6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760040160&auth_key=1760040160-0-0-19de99e44e19f4645e13dfdbc122cf58&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8aa2d684d40944528e12fe58ccf583ee~resize:0:q75.jpg?source=1f5c5e47&expiration=1760040167&auth_key=1760040167-0-0-0aa161eb8d3494513ad23498bdd0bc50&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-311c5f5db6d6e2885c7ced0390ecd252~resize:0:q75.jpg?source=1f5c5e47&expiration=1760040174&auth_key=1760040174-0-0-7be300156ab93cdfb1d5b7917178897b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Eliminating-stability-hallucinations-in-llm-based-tts-models-via-attention-guidance"><a href="#Eliminating-stability-hallucinations-in-llm-based-tts-models-via-attention-guidance" class="headerlink" title="Eliminating stability hallucinations in llm-based tts models via   attention guidance"></a>Eliminating stability hallucinations in llm-based tts models via   attention guidance</h2><p><strong>Authors:ShiMing Wang, ZhiHao Du, Yang Xiang, TianYu Zhao, Han Zhao, Qian Chen, XianGang Li, HanJie Guo, ZhenHua Ling</strong></p>
<p>This paper focuses on resolving stability hallucinations (e.g., repetitive or omitted speech) in LLM-based Text-to-Speech (TTS) models by improving and leveraging the attention mechanism. First, we analyzed the alignment mechanism between text tokens and speech tokens in LLMs. We then proposed a metric termed the Optimal Alignment Score (OAS), which employs the Viterbi algorithm to evaluate text-speech alignment quality. Subsequently, OAS was integrated into the training of CosyVoice2 to assist LLMs in learning continuous, stable alignment. Additionally, the pre-trained attention value is employed to guide the training of the student CosyVoice2 via chain-of-thought (CoT), which further reduces stability hallucinations in synthesized speech. Experiments on the Seed-TTS-Eval and CV3-Eval test sets demonstrate that the proposed methods can effectively reduce the stability hallucinations of CosyVoice2 without introducing additional negative effects. The appendix is available at <a target="_blank" rel="noopener" href="https://wsmzzz.github.io/llm_attn">https://wsmzzz.github.io/llm_attn</a>. </p>
<blockquote>
<p>æœ¬æ–‡é‡ç‚¹å…³æ³¨åŸºäºLLMçš„æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å‹ä¸­çš„ç¨³å®šæ€§å¹»è§‰ï¼ˆå¦‚é‡å¤æˆ–é—æ¼çš„è¯­éŸ³ï¼‰é—®é¢˜ï¼Œé€šè¿‡æ”¹è¿›å’Œåˆ©ç”¨æ³¨æ„åŠ›æœºåˆ¶æ¥è§£å†³ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬åˆ†æäº†LLMä¸­æ–‡æœ¬æ ‡è®°å’Œè¯­éŸ³æ ‡è®°ä¹‹é—´çš„å¯¹é½æœºåˆ¶ã€‚éšåï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºæœ€ä½³å¯¹é½åˆ†æ•°ï¼ˆOASï¼‰çš„æŒ‡æ ‡ï¼Œè¯¥æŒ‡æ ‡åˆ©ç”¨ç»´ç‰¹æ¯”ç®—æ³•è¯„ä¼°æ–‡æœ¬-è¯­éŸ³å¯¹é½è´¨é‡ã€‚æ¥ç€ï¼Œå°†OASé›†æˆåˆ°CosyVoice2çš„è®­ç»ƒä¸­ï¼Œå¸®åŠ©LLMå­¦ä¹ è¿ç»­ã€ç¨³å®šçš„å¯¹é½ã€‚æ­¤å¤–ï¼Œè¿˜ä½¿ç”¨é¢„è®­ç»ƒçš„æ³¨æ„åŠ›å€¼ï¼Œé€šè¿‡æ€ç»´é“¾ï¼ˆCoTï¼‰å¼•å¯¼å­¦ç”ŸCosyVoice2çš„è®­ç»ƒï¼Œè¿™è¿›ä¸€æ­¥å‡å°‘äº†åˆæˆè¯­éŸ³ä¸­çš„ç¨³å®šæ€§å¹»è§‰ã€‚åœ¨Seed-TTS-Evalå’ŒCV3-Evalæµ‹è¯•é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•å¯ä»¥æœ‰æ•ˆå‡å°‘CosyVoice2çš„ç¨³å®šæ€§å¹»è§‰ï¼Œä¸”ä¸ä¼šå¼•å…¥é¢å¤–çš„è´Ÿé¢å½±å“ã€‚é™„å½•å¯è®¿é—®äºï¼š<a target="_blank" rel="noopener" href="https://wsmzzz.github.io/llm_attn%E3%80%82">https://wsmzzz.github.io/llm_attnã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.19852v1">PDF</a> 5 pages, submitted to ICASSP2026</p>
<p><strong>Summary</strong></p>
<p>æœ¬è®ºæ–‡é€šè¿‡æ”¹è¿›å’Œåˆ©ç”¨æ³¨æ„åŠ›æœºåˆ¶ï¼Œè§£å†³äº†åŸºäºLLMçš„æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å‹ä¸­çš„ç¨³å®šæ€§å¹»è§‰ï¼ˆå¦‚é‡å¤æˆ–çœç•¥è¯­éŸ³ï¼‰é—®é¢˜ã€‚ç ”ç©¶å›¢é˜Ÿåˆ†æäº†LLMä¸­æ–‡æœ¬æ ‡è®°å’Œè¯­éŸ³æ ‡è®°çš„å¯¹é½æœºåˆ¶ï¼Œæå‡ºäº†åä¸ºæœ€ä½³å¯¹é½åˆ†æ•°ï¼ˆOASï¼‰çš„è¯„ä¼°æŒ‡æ ‡ï¼Œåˆ©ç”¨ç»´ç‰¹æ¯”ç®—æ³•è¯„ä¼°æ–‡æœ¬åˆ°è¯­éŸ³çš„å¯¹é½è´¨é‡ã€‚æ­¤å¤–ï¼Œè¿˜å°†OASé›†æˆåˆ°CosyVoice2çš„è®­ç»ƒä¸­ï¼Œå¸®åŠ©å­¦ç”Ÿç½‘ç»œå­¦ä¹ è¿ç»­ç¨³å®šçš„å¯¹é½ã€‚é€šè¿‡åˆ©ç”¨é¢„è®­ç»ƒçš„æ³¨æ„åŠ›å€¼ï¼Œè¿›ä¸€æ­¥é™ä½äº†åˆæˆè¯­éŸ³çš„ç¨³å®šæ€§å¹»è§‰ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½æœ‰æ•ˆå‡å°‘CosyVoice2çš„ç¨³å®šæ€§å¹»è§‰ï¼Œä¸”æœªå¼•å…¥å…¶ä»–è´Ÿé¢å½±å“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®ºæ–‡èšç„¦äºè§£å†³LLM-based TTSæ¨¡å‹çš„ç¨³å®šæ€§å¹»è§‰é—®é¢˜ï¼Œå¦‚é‡å¤æˆ–çœç•¥è¯­éŸ³ã€‚</li>
<li>åˆ†æäº†LLMä¸­æ–‡æœ¬æ ‡è®°å’Œè¯­éŸ³æ ‡è®°çš„å¯¹é½æœºåˆ¶ã€‚</li>
<li>æå‡ºäº†åä¸ºæœ€ä½³å¯¹é½åˆ†æ•°ï¼ˆOASï¼‰çš„è¯„ä¼°æŒ‡æ ‡ï¼Œç”¨äºè¯„ä»·æ–‡æœ¬åˆ°è¯­éŸ³çš„å¯¹é½è´¨é‡ã€‚</li>
<li>åˆ©ç”¨ç»´ç‰¹æ¯”ç®—æ³•å®ç°OASï¼Œå¹¶å°†å…¶é›†æˆåˆ°CosyVoice2çš„è®­ç»ƒä¸­ã€‚</li>
<li>å¼•å…¥é¢„è®­ç»ƒçš„æ³¨æ„åŠ›å€¼ï¼Œé€šè¿‡æ€ç»´é“¾ï¼ˆCoTï¼‰æŒ‡å¯¼CosyVoice2çš„è®­ç»ƒã€‚</li>
<li>å®éªŒè¯æ˜ï¼Œæ‰€ææ–¹æ³•èƒ½æœ‰æ•ˆå‡å°‘CosyVoice2åˆæˆè¯­éŸ³çš„ç¨³å®šæ€§å¹»è§‰ï¼Œä¸”æ²¡æœ‰å¼•å…¥å…¶ä»–è´Ÿé¢å½±å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.19852">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-ef9fdcfc5e358eb8c38f23126b2520ad~resize:0:q75.jpg?source=1f5c5e47&expiration=1760040184&auth_key=1760040184-0-0-4115455e2df3cbd05fe05662978fa68c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d8e20f46b07b049fdb762c4f485c04c9~resize:0:q75.jpg?source=1f5c5e47&expiration=1760040191&auth_key=1760040191-0-0-3ef15e8c88be72a462011612d08f662d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-830c1a0c115ac43f462145ef1a5e07c4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760040197&auth_key=1760040197-0-0-4e36480cf1ba5ad6bbea2688c2bd6336&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e9cbc4032bd2598940b5fdb688e72cbf~resize:0:q75.jpg?source=1f5c5e47&expiration=1760040204&auth_key=1760040204-0-0-517fd2c2014f5757dd514271a6149257&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e0ba604fa074dbcadea287a7055b4c6f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760040211&auth_key=1760040211-0-0-1aa9f116b81733e4b86d9e4703ef1313&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Selective-Classifier-free-Guidance-for-Zero-shot-Text-to-speech"><a href="#Selective-Classifier-free-Guidance-for-Zero-shot-Text-to-speech" class="headerlink" title="Selective Classifier-free Guidance for Zero-shot Text-to-speech"></a>Selective Classifier-free Guidance for Zero-shot Text-to-speech</h2><p><strong>Authors:John Zheng, Farhad Maleki</strong></p>
<p>In zero-shot text-to-speech, achieving a balance between fidelity to the target speaker and adherence to text content remains a challenge. While classifier-free guidance (CFG) strategies have shown promising results in image generation, their application to speech synthesis are underexplored. Separating the conditions used for CFG enables trade-offs between different desired characteristics in speech synthesis. In this paper, we evaluate the adaptability of CFG strategies originally developed for image generation to speech synthesis and extend separated-condition CFG approaches for this domain. Our results show that CFG strategies effective in image generation generally fail to improve speech synthesis. We also find that we can improve speaker similarity while limiting degradation of text adherence by applying standard CFG during early timesteps and switching to selective CFG only in later timesteps. Surprisingly, we observe that the effectiveness of a selective CFG strategy is highly text-representation dependent, as differences between the two languages of English and Mandarin can lead to different results even with the same model. </p>
<blockquote>
<p>åœ¨é›¶æ ·æœ¬æ–‡æœ¬åˆ°è¯­éŸ³çš„è½¬æ¢ä¸­ï¼Œå¦‚ä½•åœ¨å¿ å®äºç›®æ ‡è¯´è¯äººå’Œéµå®ˆæ–‡æœ¬å†…å®¹ä¹‹é—´å–å¾—å¹³è¡¡ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚è™½ç„¶æ— åˆ†ç±»å™¨å¼•å¯¼ï¼ˆCFGï¼‰ç­–ç•¥åœ¨å›¾åƒç”Ÿæˆæ–¹é¢å·²ç»å–å¾—äº†æœ‰å¸Œæœ›çš„ç»“æœï¼Œä½†å®ƒä»¬åœ¨è¯­éŸ³åˆæˆä¸­çš„åº”ç”¨å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚ä½¿ç”¨CFGæ—¶ï¼Œå¯¹æ¡ä»¶è¿›è¡Œåˆ†ç¦»èƒ½å¤Ÿä½¿è¯­éŸ³åˆæˆä¸­çš„ä¸åŒæœŸæœ›ç‰¹å¾ä¹‹é—´è¿›è¡Œæƒè¡¡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è¯„ä¼°äº†åŸæœ¬ä¸ºå›¾åƒç”Ÿæˆè€Œå¼€å‘çš„CFGç­–ç•¥å¯¹è¯­éŸ³åˆæˆçš„é€‚åº”æ€§ï¼Œå¹¶å¯¹æ­¤é¢†åŸŸçš„åˆ†ç¦»æ¡ä»¶CFGæ–¹æ³•è¿›è¡Œäº†æ‰©å±•ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œåœ¨å›¾åƒç”Ÿæˆä¸­æœ‰æ•ˆçš„CFGç­–ç•¥é€šå¸¸æ— æ³•æ”¹å–„è¯­éŸ³åˆæˆã€‚æˆ‘ä»¬è¿˜å‘ç°ï¼Œé€šè¿‡åœ¨æ—©æœŸæ—¶é—´æ­¥é•¿åº”ç”¨æ ‡å‡†CFGï¼Œå¹¶åœ¨åæœŸæ—¶é—´æ­¥é•¿ä»…é€‰æ‹©CFGï¼Œæˆ‘ä»¬å¯ä»¥æé«˜è¯´è¯äººçš„ç›¸ä¼¼æ€§ï¼ŒåŒæ—¶é™åˆ¶æ–‡æœ¬è´´åˆåº¦çš„é™ä½ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç°é€‰æ‹©æ€§CFGç­–ç•¥çš„æœ‰æ•ˆæ€§é«˜åº¦ä¾èµ–äºæ–‡æœ¬è¡¨ç¤ºï¼Œå› ä¸ºè‹±è¯­å’Œæ™®é€šè¯ä¸¤ç§è¯­è¨€ä¹‹é—´çš„å·®å¼‚å³ä½¿ä½¿ç”¨ç›¸åŒçš„æ¨¡å‹ä¹Ÿä¼šå¯¼è‡´ä¸åŒçš„ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.19668v1">PDF</a> 5 pages, 7 figures, 1 table. Submitted to ICASSP 2026</p>
<p><strong>Summary</strong></p>
<p>é›¶æ ·æœ¬æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆText-to-Speechï¼‰ä¸­ï¼Œå®ç°ç›®æ ‡è¯­éŸ³ä¸æ–‡æœ¬å†…å®¹çš„å¹³è¡¡æ˜¯ä¸€å¤§æŒ‘æˆ˜ã€‚è™½ç„¶æ— åˆ†ç±»å™¨å¼•å¯¼ï¼ˆClassifier-Free Guidanceï¼ŒCFGï¼‰ç­–ç•¥åœ¨å›¾åƒç”Ÿæˆé¢†åŸŸå±•ç°å‡ºè‰¯å¥½å‰æ™¯ï¼Œä½†åœ¨è¯­éŸ³åˆæˆé¢†åŸŸçš„åº”ç”¨å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æœ¬æ–‡é€šè¿‡è¯„ä¼°é€‚ç”¨äºå›¾åƒç”Ÿæˆçš„CFGç­–ç•¥åœ¨è¯­éŸ³åˆæˆä¸­çš„é€‚åº”æ€§ï¼Œå¹¶æ‰©å±•äº†åˆ†æ¡ä»¶CFGæ–¹æ³•ã€‚ç ”ç©¶å‘ç°ï¼Œå›¾åƒç”Ÿæˆä¸­æœ‰æ•ˆçš„CFGç­–ç•¥é€šå¸¸æ— æ³•æå‡è¯­éŸ³åˆæˆæ•ˆæœã€‚é€šè¿‡æ—©æœŸæ—¶åºæ­¥ä½¿ç”¨æ ‡å‡†CFGï¼Œå¹¶åœ¨åæœŸæ—¶åºæ­¥è½¬ä¸ºé€‰æ‹©æ€§CFGï¼Œå¯æå‡è¯´è¯äººç›¸ä¼¼æ€§å¹¶é™åˆ¶æ–‡æœ¬è´´åˆåº¦çš„é™ä½ã€‚æœ‰è¶£çš„æ˜¯ï¼Œé€‰æ‹©æ€§CFGç­–ç•¥çš„æœ‰æ•ˆæ€§é«˜åº¦ä¾èµ–äºæ–‡æœ¬è¡¨å¾ï¼Œä¸­è‹±æ–‡ä¹‹é—´çš„å·®å¼‚ç”šè‡³ä¼šå¯¼è‡´ä¸åŒç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é›¶æ ·æœ¬æ–‡æœ¬è½¬è¯­éŸ³ä¸­å¹³è¡¡è¯­éŸ³ä¸æ–‡æœ¬å†…å®¹çš„æŒ‘æˆ˜ã€‚</li>
<li>æ— åˆ†ç±»å™¨å¼•å¯¼ï¼ˆCFGï¼‰ç­–ç•¥åœ¨è¯­éŸ³åˆæˆä¸­çš„åº”ç”¨å°šæœªå¹¿æ³›æ¢ç´¢ã€‚</li>
<li>å›¾åƒç”Ÿæˆä¸­çš„CFGç­–ç•¥é€šå¸¸ä¸èƒ½ç›´æ¥åº”ç”¨äºè¯­éŸ³åˆæˆã€‚</li>
<li>é€šè¿‡æ—©æœŸå’ŒåæœŸæ—¶åºæ­¥é‡‡ç”¨ä¸åŒçš„CFGç­–ç•¥ï¼Œå¯æé«˜è¯´è¯äººç›¸ä¼¼æ€§å’Œæ–‡æœ¬è´´åˆåº¦ã€‚</li>
<li>é€‰æ‹©æ€§CFGç­–ç•¥çš„æœ‰æ•ˆæ€§ä¾èµ–äºæ–‡æœ¬è¡¨å¾ã€‚</li>
<li>ä¸åŒè¯­è¨€ï¼ˆå¦‚ä¸­è‹±æ–‡ï¼‰ä¹‹é—´çš„å·®å¼‚å¯èƒ½å½±å“CFGç­–ç•¥çš„æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.19668">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-db440639895e76a3383500f1ff8bd717~resize:0:q75.jpg?source=1f5c5e47&expiration=1760040253&auth_key=1760040253-0-0-4a4a4bb3cb3543bbf3d59e1c012afd93&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3c3784fb77013aa46b67a57fe4783627~resize:0:q75.jpg?source=1f5c5e47&expiration=1760040260&auth_key=1760040260-0-0-9d81325ed579f8a75bd56521db531b95&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-030fe4607b88fd01f6c516e7009cf426~resize:0:q75.jpg?source=1f5c5e47&expiration=1760040266&auth_key=1760040266-0-0-b8fb36bc06c1d5e10e50f456bf682bcd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-cdebdcb8e5f0bd770dee9bac6cff5521~resize:0:q75.jpg?source=1f5c5e47&expiration=1760040273&auth_key=1760040273-0-0-3f1416a9be9d463cd336a903363e447c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d8a87fdb734f8ececb3e1aed910d47a0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760040314&auth_key=1760040314-0-0-207d78e153dfeb0229cf48961a2fb5b5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Investigating-Test-Time-Scaling-with-Reranking-for-Machine-Translation"><a href="#Investigating-Test-Time-Scaling-with-Reranking-for-Machine-Translation" class="headerlink" title="Investigating Test-Time Scaling with Reranking for Machine Translation"></a>Investigating Test-Time Scaling with Reranking for Machine Translation</h2><p><strong>Authors:Shaomu Tan, Ryosuke Mitani, Ritvik Choudhary, Toshiyuki Sekiya</strong></p>
<p>Scaling model parameters has become the de facto strategy for improving NLP systems, but it comes with substantial computational costs. Test-Time Scaling (TTS) offers an alternative by allocating more computation at inference: generating multiple candidates and selecting the best. While effective in tasks such as mathematical reasoning, TTS has not been systematically explored for machine translation (MT). In this paper, we present the first systematic study of TTS for MT, investigating a simple but practical best-of-N framework on WMT24 benchmarks. Our experiments cover six high-resource and one low-resource language pairs, five model sizes (3B-72B), and various TTS compute budget (N up to 1024). Our results show that a) For high-resource languages, TTS generally improves translation quality according to multiple neural MT evaluation metrics, and our human evaluation confirms these gains; b) Augmenting smaller models with large $N$ can match or surpass larger models at $N{&#x3D;}1$ with more compute cost; c) Under fixed compute budgets, larger models are typically more efficient, and TTS can degrade quality due to metric blind spots in low-resource cases. </p>
<blockquote>
<p>æ¨¡å‹å‚æ•°ç¼©æ”¾å·²æˆä¸ºæ”¹è¿›NLPç³»ç»Ÿçš„é»˜è®¤ç­–ç•¥ï¼Œä½†è¿™éœ€è¦å¤§é‡çš„è®¡ç®—æˆæœ¬ã€‚æµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆTTSï¼‰é€šè¿‡æ¨ç†æ—¶åˆ†é…æ›´å¤šè®¡ç®—æä¾›ä¸€ç§æ›¿ä»£æ–¹æ¡ˆï¼šç”Ÿæˆå¤šä¸ªå€™é€‰è€…å¹¶é€‰æ‹©æœ€ä½³é€‰é¡¹ã€‚è™½ç„¶åœ¨æ•°å­¦æ¨ç†ç­‰ä»»åŠ¡ä¸­æœ‰æ•ˆï¼Œä½†TTSåœ¨æœºå™¨ç¿»è¯‘ï¼ˆMTï¼‰æ–¹é¢å°šæœªè¿›è¡Œç³»ç»Ÿæ¢ç´¢ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¯¹æœºå™¨ç¿»è¯‘ä¸­çš„TTSè¿›è¡Œäº†é¦–æ¬¡ç³»ç»Ÿç ”ç©¶ï¼Œåœ¨WMT24åŸºå‡†æµ‹è¯•ä¸Šæ¢è®¨äº†ä¸€ä¸ªç®€å•ä½†å®ç”¨çš„æœ€ä½³Næ¡†æ¶ã€‚æˆ‘ä»¬çš„å®éªŒæ¶µç›–äº†å…­ä¸ªé«˜èµ„æºå’Œä¸€ä¸ªä½èµ„æºè¯­è¨€å¯¹ã€äº”ç§æ¨¡å‹å¤§å°ï¼ˆ3B-72Bï¼‰ä»¥åŠå„ç§TTSè®¡ç®—é¢„ç®—ï¼ˆNæœ€å¤šè¾¾1024ï¼‰ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼šaï¼‰å¯¹äºé«˜èµ„æºè¯­è¨€ï¼ŒTTSé€šå¸¸æ ¹æ®å¤šä¸ªç¥ç»æœºå™¨ç¿»è¯‘è¯„ä¼°æŒ‡æ ‡æé«˜äº†ç¿»è¯‘è´¨é‡ï¼Œæˆ‘ä»¬çš„äººå·¥è¯„ä¼°ä¹Ÿè¯å®äº†è¿™äº›æ”¶ç›Šï¼›bï¼‰é€šè¿‡å¤§å‹Nå¢å¼ºå°å‹æ¨¡å‹å¯ä»¥åŒ¹é…æˆ–è¶…è¶Šå¤§å‹æ¨¡å‹åœ¨N&#x3D;1æ—¶çš„è®¡ç®—èƒ½åŠ›ï¼›cï¼‰åœ¨å›ºå®šçš„è®¡ç®—é¢„ç®—ä¸‹ï¼Œå¤§å‹æ¨¡å‹é€šå¸¸æ›´æœ‰æ•ˆï¼Œè€Œåœ¨èµ„æºä¸è¶³çš„æƒ…å†µä¸‹ï¼ŒTTSå¯èƒ½ä¼šç”±äºæŒ‡æ ‡ç›²ç‚¹è€Œé™ä½è´¨é‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.19020v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æ¢ç´¢äº†æµ‹è¯•æ—¶é—´ç¼©æ”¾ï¼ˆTTSï¼‰åœ¨æœºå™¨ç¿»è¯‘ï¼ˆMTï¼‰ä¸­çš„é¦–æ¬¡ç³»ç»Ÿæ€§åº”ç”¨ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒTTSåœ¨é«˜èµ„æºè¯­è¨€ç¿»è¯‘ä¸­æ™®éæé«˜äº†ç¿»è¯‘è´¨é‡ï¼Œä¸”è¾ƒå°çš„æ¨¡å‹é€šè¿‡å¢å¤§Nå€¼ç”šè‡³èƒ½åŒ¹é…æˆ–è¶…è¶Šå¤§å‹æ¨¡å‹çš„è¡¨ç°ã€‚ç„¶è€Œï¼Œåœ¨å›ºå®šè®¡ç®—é¢„ç®—ä¸‹ï¼Œå¤§å‹æ¨¡å‹é€šå¸¸æ›´æœ‰æ•ˆï¼Œä½èµ„æºæƒ…å†µä¸‹TTSå¯èƒ½å› è¯„ä¼°æŒ‡æ ‡çš„ç›²ç‚¹è€Œé™ä½è´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æµ‹è¯•æ—¶é—´ç¼©æ”¾ï¼ˆTTSï¼‰ä½œä¸ºä¸€ç§åœ¨æ¨ç†æ—¶åˆ†é…æ›´å¤šè®¡ç®—èµ„æºçš„ç­–ç•¥ï¼Œè¢«é¦–æ¬¡ç³»ç»Ÿæ€§åœ°ç ”ç©¶ç”¨äºæœºå™¨ç¿»è¯‘ï¼ˆMTï¼‰ã€‚</li>
<li>åœ¨é«˜èµ„æºè¯­è¨€ç¿»è¯‘ä¸­ï¼ŒTTSæ™®éæé«˜äº†ç¿»è¯‘è´¨é‡ï¼Œè¿™ä¸€å‘ç°å¾—åˆ°äº†å¤šé¡¹ç¥ç»æœºå™¨ç¿»è¯‘è¯„ä¼°æŒ‡æ ‡å’Œäººå·¥è¯„ä»·çš„è¯å®ã€‚</li>
<li>é€šè¿‡å¢å¤§Nå€¼ï¼Œè¾ƒå°çš„æ¨¡å‹åœ¨æœºå™¨ç¿»è¯‘ä¸­ä¹Ÿèƒ½åŒ¹é…æˆ–è¶…è¶Šå¤§å‹æ¨¡å‹çš„è¡¨ç°ã€‚</li>
<li>åœ¨å›ºå®šè®¡ç®—é¢„ç®—ä¸‹ï¼Œå¤§å‹æ¨¡å‹é€šå¸¸æ›´ä¸ºé«˜æ•ˆã€‚</li>
<li>TTSåœ¨é«˜èµ„æºæƒ…å†µä¸‹çš„è¡¨ç°ä¼˜äºä½èµ„æºæƒ…å†µï¼Œåè€…å¯èƒ½ä¼šå‡ºç°å› è¯„ä¼°æŒ‡æ ‡çš„ç›²ç‚¹è€Œé™ä½ç¿»è¯‘è´¨é‡çš„æƒ…å†µã€‚</li>
<li>è¯¥ç ”ç©¶åœ¨WMT24åŸºå‡†æµ‹è¯•ä¸Šé‡‡ç”¨äº†ç®€å•çš„ä½†å®é™…çš„æœ€ä¼˜Næ¡†æ¶ï¼Œå®éªŒæ¶µç›–äº†ä»3Båˆ°72Bçš„äº”ç§æ¨¡å‹å¤§å°ä»¥åŠæœ€å¤šè¾¾1024çš„TTSè®¡ç®—é¢„ç®—ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.19020">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-d3f59dc30284bbeadc4efa4f705b969e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760040322&auth_key=1760040322-0-0-a16e3453830d5f9862629bd2bccbd160&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b2507040d7ec22fee0560cb02b9b9823~resize:0:q75.jpg?source=1f5c5e47&expiration=1760040329&auth_key=1760040329-0-0-02b2990e4e6d01e70579c30f4b781e0e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0c0322b321506d03ec0a80fb883e060e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760040336&auth_key=1760040336-0-0-9c199ec05b9894d325adbf8c3786b243&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-117ad0c22f0d923e068ca57b7ee1cf98~resize:0:q75.jpg?source=1f5c5e47&expiration=1760040342&auth_key=1760040342-0-0-500018c77a93639a4394a337c2813ec8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e2735ba1900a5b31e0ce656e04d9f8bf~resize:0:q75.jpg?source=1f5c5e47&expiration=1760040349&auth_key=1760040349-0-0-af9da57fbce61fe295cbca3abbd7d18b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="HD-PPT-Hierarchical-Decoding-of-Content-and-Prompt-Preference-Tokens-for-Instruction-based-TTS"><a href="#HD-PPT-Hierarchical-Decoding-of-Content-and-Prompt-Preference-Tokens-for-Instruction-based-TTS" class="headerlink" title="HD-PPT: Hierarchical Decoding of Content- and Prompt-Preference Tokens   for Instruction-based TTS"></a>HD-PPT: Hierarchical Decoding of Content- and Prompt-Preference Tokens   for Instruction-based TTS</h2><p><strong>Authors:Sihang Nie, Xiaofen Xing, Jingyuan Xing, Baiji Liu, Xiangmin Xu</strong></p>
<p>Large Language Model (LLM)-based Text-to-Speech (TTS) models have already reached a high degree of naturalness. However, the precision control of TTS inference is still challenging. Although instruction-based Text-to-Speech (Instruct-TTS) models are proposed, these models still lack fine-grained control due to the modality gap between single-level text instructions and multilevel speech tokens. To address this limitation, we propose HD-PPT, a framework that transforms speech synthesis into a structured, hierarchical task. To enable fine-grained control, we introduce a novel speech codec to extract distinct prompt-preference and content-preference tokens from the complex speech tokens, supervised by automatic speech recognition (ASR) and cross-lingual audio-text pre-training (CLAP) objectives. To bridge the modality gap of these tokens, we propose a hierarchical decoding strategy, where the LLM generates tokens in a structured order: first semantic, then fine-grained style, and finally complete acoustic representation. Extensive experiments demonstrate that this hierarchical paradigm significantly improves instruction adherence and achieves state-of-the-art naturalness, validating our approach for precise and controllable speech synthesis. Audio samples are available at <a target="_blank" rel="noopener" href="https://xxh333.github.io/">https://xxh333.github.io/</a>. </p>
<blockquote>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å‹å·²ç»å®ç°äº†å¾ˆé«˜çš„è‡ªç„¶åº¦ã€‚ç„¶è€Œï¼ŒTTSæ¨ç†çš„ç²¾ç¡®æ§åˆ¶ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚è™½ç„¶æå‡ºäº†åŸºäºæŒ‡ä»¤çš„æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆInstruct-TTSï¼‰æ¨¡å‹ï¼Œä½†è¿™äº›æ¨¡å‹ä»ç„¶ç¼ºä¹ç²¾ç»†æ§åˆ¶ï¼ŒåŸå› æ˜¯å•ä¸€å±‚æ¬¡çš„æ–‡æœ¬æŒ‡ä»¤å’Œå¤šå±‚æ¬¡çš„è¯­éŸ³ä»¤ç‰Œä¹‹é—´å­˜åœ¨æ¨¡æ€å·®è·ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†HD-PPTæ¡†æ¶ï¼Œå°†è¯­éŸ³åˆæˆè½¬å˜ä¸ºä¸€ä¸ªç»“æ„åŒ–ã€åˆ†å±‚çš„ä»»åŠ¡ã€‚ä¸ºäº†å®ç°ç²¾ç»†æ§åˆ¶ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„è¯­éŸ³ç¼–è§£ç å™¨ï¼Œä»å¤æ‚çš„è¯­éŸ³ä»¤ç‰Œä¸­æå–å‡ºä¸åŒçš„æç¤ºåå¥½å’Œå†…å®¹åå¥½ä»¤ç‰Œï¼Œç”±è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰å’Œè·¨è¯­è¨€éŸ³é¢‘æ–‡æœ¬é¢„è®­ç»ƒï¼ˆCLAPï¼‰ç›®æ ‡è¿›è¡Œç›‘ç£ã€‚ä¸ºäº†å¼¥åˆè¿™äº›ä»¤ç‰Œçš„æ¨¡æ€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ†å±‚è§£ç ç­–ç•¥ï¼ŒLLMä»¥ç»“æ„åŒ–é¡ºåºç”Ÿæˆä»¤ç‰Œï¼šé¦–å…ˆæ˜¯è¯­ä¹‰ï¼Œç„¶åæ˜¯ç²¾ç»†é£æ ¼ï¼Œæœ€åæ˜¯å®Œæ•´çš„å£°å­¦è¡¨ç¤ºã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œè¿™ç§åˆ†å±‚èŒƒå¼æ˜¾è‘—æé«˜äº†æŒ‡ä»¤éµå¾ªèƒ½åŠ›ï¼Œå¹¶å®ç°äº†æœ€å…ˆè¿›çš„è‡ªç„¶åº¦ï¼ŒéªŒè¯äº†æˆ‘ä»¬æ–¹æ³•åœ¨ç²¾ç¡®å¯æ§è¯­éŸ³åˆæˆæ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚éŸ³é¢‘æ ·æœ¬å¯åœ¨<a target="_blank" rel="noopener" href="https://xxh333.github.io/%E6%89%BE%E5%88%B0%E3%80%82">https://xxh333.github.io/æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.19001v1">PDF</a> 5 pages, 2 figures, submitted to ICASSP2026</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å‹å·²å…·å¤‡é«˜åº¦è‡ªç„¶åº¦ï¼Œä½†åœ¨TTSæ¨ç†çš„ç²¾ç¡®æ§åˆ¶ä¸Šä»é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºè§£å†³æŒ‡ä»¤å¼TTSæ¨¡å‹å› å•ä¸€å±‚çº§æ–‡æœ¬æŒ‡ä»¤ä¸å¤šçº§è¯­éŸ³æ ‡è®°ä¹‹é—´çš„æ¨¡æ€å·®è·å¯¼è‡´çš„ç²¾ç»†æ§åˆ¶ç¼ºå¤±é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†HD-PPTæ¡†æ¶ï¼Œå°†è¯­éŸ³åˆæˆè½¬åŒ–ä¸ºç»“æ„åŒ–ã€åˆ†å±‚çš„ä»»åŠ¡ã€‚é€šè¿‡å¼•å…¥æ–°å‹è¯­éŸ³ç¼–ç æŠ€æœ¯ï¼Œä»å¤æ‚çš„è¯­éŸ³æ ‡è®°ä¸­æå–å‡ºä¸åŒçš„æç¤ºåå¥½å’Œå†…å®¹åå¥½æ ‡è®°ï¼Œå—åˆ°è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰å’Œè·¨è¯­è¨€éŸ³é¢‘æ–‡æœ¬é¢„è®­ç»ƒï¼ˆCLAPï¼‰ç›®æ ‡çš„ç›‘ç£ã€‚ä¸ºç¼©å°è¿™äº›æ ‡è®°çš„æ¨¡æ€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†åˆ†å±‚è§£ç ç­–ç•¥ï¼ŒLLMä»¥ç»“æ„åŒ–é¡ºåºç”Ÿæˆæ ‡è®°ï¼šé¦–å…ˆæ˜¯è¯­ä¹‰ï¼Œç„¶åæ˜¯ç²¾ç»†é£æ ¼ï¼Œæœ€åæ˜¯å®Œæ•´çš„å£°å­¦è¡¨ç¤ºã€‚å®éªŒè¯æ˜ï¼Œè¿™ç§åˆ†å±‚èŒƒå¼æ˜¾è‘—æé«˜äº†æŒ‡ä»¤éµå¾ªæ€§ï¼Œå®ç°äº†æœ€å…ˆè¿›çš„è‡ªç„¶åº¦ï¼ŒéªŒè¯äº†æˆ‘ä»¬åœ¨ç²¾ç¡®å¯æ§è¯­éŸ³åˆæˆæ–¹é¢çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM-based TTS models have achieved high naturalness but face challenges in precise control of inference.</li>
<li>æŒ‡ä»¤å¼TTSæ¨¡å‹å› æ¨¡æ€å·®è·å½±å“ç²¾ç»†æ§åˆ¶ã€‚</li>
<li>HD-PPTæ¡†æ¶å°†è¯­éŸ³åˆæˆè½¬åŒ–ä¸ºç»“æ„åŒ–ã€åˆ†å±‚çš„ä»»åŠ¡ä»¥æé«˜æ§åˆ¶ç²¾åº¦ã€‚</li>
<li>æ–°å‹è¯­éŸ³ç¼–ç æŠ€æœ¯ä»å¤æ‚çš„è¯­éŸ³æ ‡è®°ä¸­æå–å‡ºæç¤ºåå¥½å’Œå†…å®¹åå¥½æ ‡è®°ã€‚</li>
<li>ASRå’ŒCLAPç›®æ ‡ç”¨äºç›‘ç£ç¼–ç è¿‡ç¨‹ã€‚</li>
<li>é‡‡ç”¨åˆ†å±‚è§£ç ç­–ç•¥ï¼ŒæŒ‰ç…§è¯­ä¹‰ã€ç²¾ç»†é£æ ¼å’Œå£°å­¦è¡¨ç¤ºçš„ç»“æ„åŒ–é¡ºåºç”Ÿæˆæ ‡è®°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.19001">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-b09dcd92ad76464ed6cbe7e055a9a964~resize:0:q75.jpg?source=1f5c5e47&expiration=1760040357&auth_key=1760040357-0-0-6b27b63464558c2915079f959f87aa6c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1976cdf2828329169cb27f2182435fd8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760040364&auth_key=1760040364-0-0-1231a7a66787243a217190b57de6ea1a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1d39c7b1dcc23baa6baf55c0cea3c6ac~resize:0:q75.jpg?source=1f5c5e47&expiration=1760040370&auth_key=1760040370-0-0-cd0e7afc71311a33276c5d76aca3c6e2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Direct-Preference-Optimization-for-Speech-Autoregressive-Diffusion-Models"><a href="#Direct-Preference-Optimization-for-Speech-Autoregressive-Diffusion-Models" class="headerlink" title="Direct Preference Optimization for Speech Autoregressive Diffusion   Models"></a>Direct Preference Optimization for Speech Autoregressive Diffusion   Models</h2><p><strong>Authors:Zhijun Liu, Dongya Jia, Xiaoqiang Wang, Chenpeng Du, Shuai Wang, Zhuo Chen, Haizhou Li</strong></p>
<p>Autoregressive diffusion models (ARDMs) have recently been applied to speech generation, achieving state-of-the-art (SOTA) performance in zero-shot text-to-speech. By autoregressively generating continuous speech tokens with next-token diffusion, these models offer a promising alternative to next-token prediction, avoiding the technical complexities associated with discrete speech tokenization. As a relatively new paradigm, research on reinforcement learning (RL)-based fine-tuning of speech ARDMs remains limited. In this paper, we propose Autoregressive Diffusion-Direct Preference Optimization (ARDM-DPO) to advance this research. By fine-tuning the recently proposed zero-shot text-to-speech model DiTAR with DPO, we achieve significant improvements in terms of speech expressiveness and robustness for long texts. </p>
<blockquote>
<p>è‡ªå›å½’æ‰©æ•£æ¨¡å‹ï¼ˆARDMsï¼‰æœ€è¿‘è¢«åº”ç”¨äºè¯­éŸ³ç”Ÿæˆï¼Œåœ¨é›¶æ ·æœ¬æ–‡æœ¬åˆ°è¯­éŸ³è½¬æ¢ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è¿™äº›æ¨¡å‹é€šè¿‡è‡ªå›å½’ç”Ÿæˆè¿ç»­çš„è¯­éŸ³ä»¤ç‰Œï¼Œä½¿ç”¨ä¸‹ä¸€ä¸ªä»¤ç‰Œçš„æ‰©æ•£ï¼Œä¸ºä¸‹ä¸€ä¸ªä»¤ç‰Œçš„é¢„æµ‹æä¾›äº†æœ‰å‰æ™¯çš„æ›¿ä»£æ–¹æ¡ˆï¼Œé¿å…äº†ä¸ç¦»æ•£è¯­éŸ³ä»¤ç‰ŒåŒ–ç›¸å…³çš„æŠ€æœ¯å¤æ‚æ€§ã€‚ä½œä¸ºä¸€ç§ç›¸å¯¹è¾ƒæ–°çš„èŒƒå¼ï¼Œå…³äºåŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„è¯­éŸ³ARDMså¾®è°ƒçš„ç ”ç©¶ä»ç„¶æœ‰é™ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºè‡ªå›å½’æ‰©æ•£ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆARDM-DPOï¼‰ä»¥ä¿ƒè¿›è¿™é¡¹ç ”ç©¶ã€‚é€šè¿‡ç”¨DPOå¾®è°ƒæœ€è¿‘æå‡ºçš„é›¶æ ·æœ¬æ–‡æœ¬åˆ°è¯­éŸ³æ¨¡å‹DiTARï¼Œæˆ‘ä»¬åœ¨è¯­éŸ³è¡¨è¾¾æ€§å’Œé•¿æ–‡æœ¬ç¨³å¥æ€§æ–¹é¢å–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.18928v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è‡ªå›å½’æ‰©æ•£æ¨¡å‹ï¼ˆARDMsï¼‰åœ¨è¯­éŸ³ç”Ÿæˆé¢†åŸŸçš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨é›¶æ ·æœ¬æ–‡æœ¬åˆ°è¯­éŸ³è½¬æ¢æ–¹é¢å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æ–‡ç« æå‡ºäº†ä½¿ç”¨å¢å¼ºå‹å­¦ä¹ è¿›è¡ŒARDMä¼˜åŒ–çš„æ–°æ€è·¯ï¼Œæå‡ºAutoregressive Diffusion-Direct Preference Optimizationï¼ˆARDM-DPOï¼‰ï¼Œå¹¶é€šè¿‡å¾®è°ƒé›¶æ ·æœ¬æ–‡æœ¬åˆ°è¯­éŸ³æ¨¡å‹DiTARï¼Œæ˜¾è‘—æé«˜äº†è¯­éŸ³è¡¨è¾¾çš„ä¸°å¯Œæ€§å’Œé•¿æ–‡æœ¬çš„é²æ£’æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªå›å½’æ‰©æ•£æ¨¡å‹ï¼ˆARDMsï¼‰å·²è¢«åº”ç”¨äºè¯­éŸ³ç”Ÿæˆé¢†åŸŸï¼Œä¸”å·²è¾¾åˆ°äº†é›¶æ ·æœ¬æ–‡æœ¬åˆ°è¯­éŸ³è½¬æ¢çš„æœ€å…ˆè¿›æ€§èƒ½ã€‚</li>
<li>ARDMé€šè¿‡è¿ç»­ç”Ÿæˆè¯­éŸ³æ ‡è®°æ¥å®ç°è‡ªå›å½’ç”Ÿæˆï¼Œç›¸è¾ƒäºç¦»æ•£è¯­éŸ³æ ‡è®°çš„é¢„æµ‹ï¼Œæä¾›äº†ä¸€ä¸ªå…·æœ‰å‰æ™¯çš„æ›¿ä»£æ–¹æ¡ˆã€‚è¯¥æŠ€æœ¯é¿å…äº†ç›¸å…³çš„æŠ€æœ¯å¤æ‚æ€§ã€‚</li>
<li>è™½ç„¶è‡ªå›å½’æ‰©æ•£æ¨¡å‹å·²ç»åœ¨æŸäº›ä»»åŠ¡ä¸Šå±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†å¯¹äºå…¶å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç²¾ç»†è°ƒæ•´çš„ç ”ç©¶ä»ç„¶æœ‰é™ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•â€”â€”ARDM-DPOï¼Œç”¨äºå¢å¼ºè‡ªå›å½’æ‰©æ•£æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>ARDM-DPOé€šè¿‡å¯¹DiTARæ¨¡å‹çš„å¾®è°ƒæ˜¾è‘—æé«˜äº†è¯­éŸ³è¡¨è¾¾çš„ä¸°å¯Œæ€§å’Œé•¿æ–‡æœ¬çš„é²æ£’æ€§ã€‚è¿™å¯èƒ½æœ‰åŠ©äºæé«˜ç”¨æˆ·çš„è¯­éŸ³ä½“éªŒå¹¶æ‰©å¤§è¿™äº›æ¨¡å‹çš„å®é™…åº”ç”¨èŒƒå›´ã€‚</li>
<li>ARDM-DPOçš„å‡ºç°å¯èƒ½ä¼šæ¨åŠ¨æ›´å¤šçš„ç ”ç©¶å…³æ³¨å¦‚ä½•åˆ©ç”¨å¼ºåŒ–å­¦ä¹ æ¥ä¼˜åŒ–å’Œè¿›ä¸€æ­¥æ”¹å–„ç°æœ‰æ¨¡å‹çš„æ€§èƒ½ã€‚è¿™å¯èƒ½ä¼šå¯¹è‡ªç„¶è¯­è¨€å¤„ç†å’Œäººå·¥æ™ºèƒ½é¢†åŸŸçš„æœªæ¥å‘å±•äº§ç”Ÿé‡å¤§å½±å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.18928">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-de97387e7e9421d80cb4ce71b9af91bb~resize:0:q75.jpg?source=1f5c5e47&expiration=1760040384&auth_key=1760040384-0-0-11bd06da67d01b95e34452f918df880f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-bff75ab0be7355d8ba9c91f0cf04c483~resize:0:q75.jpg?source=1f5c5e47&expiration=1760040392&auth_key=1760040392-0-0-d28c438003bb97bcaeada07b4d8c3211&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-345819a34c85626bcf67991a55e5bb55~resize:0:q75.jpg?source=1f5c5e47&expiration=1760687482&auth_key=1760687482-0-0-c5a31e136453c0cf8f8169d51981d1aa&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1760344f895bee8b9369611f864cd5e7~resize:0:q75.jpg?source=1f5c5e47&expiration=1760687489&auth_key=1760687489-0-0-4ce3a02c3c78f6e957668a6329d14caa&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-74d6ec29d2d79c69ddc4daaf9ee5d5da~resize:0:q75.jpg?source=1f5c5e47&expiration=1760687504&auth_key=1760687504-0-0-4bcb3191277a53f4ea381ee68dce8ad2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Group-Relative-Policy-Optimization-for-Text-to-Speech-with-Large-Language-Models"><a href="#Group-Relative-Policy-Optimization-for-Text-to-Speech-with-Large-Language-Models" class="headerlink" title="Group Relative Policy Optimization for Text-to-Speech with Large   Language Models"></a>Group Relative Policy Optimization for Text-to-Speech with Large   Language Models</h2><p><strong>Authors:Chang Liu, Ya-Jun Hu, Ying-Ying Gao, Shi-Lei Zhang, Zhen-Hua Ling</strong></p>
<p>This paper proposes a GRPO-based approach to enhance the performance of large language model (LLM)-based text-to-speech (TTS) models by deriving rewards from an off-the-shelf automatic speech recognition (ASR) model. Compared to previous reinforcement learning methods for LLM-based TTS, our method requires no dedicated model for reward computation or training. Moreover, we design a composite reward function that combines character error rate (CER) with negative log-likelihood (NLL) obtained from the ASR model, providing more informative and accurate reward signals. We apply GRPO fine-tuning to pre-trained LLM-based TTS models and evaluate their zero-shot TTS performance. Experimental results show that the proposed method substantially improves both the intelligibility and naturalness of synthesized speech. Ablation studies and further analyses confirm the effectiveness of integrating the two reward components. </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºGRPOçš„æ–¹æ³•ï¼Œé€šè¿‡ä»ç°æˆçš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹ä¸­è·å–å¥–åŠ±ï¼Œæ¥æé«˜åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å‹çš„æ€§èƒ½ã€‚ä¸ä»¥å¾€çš„åŸºäºLLMçš„TTSå¼ºåŒ–å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸éœ€è¦ä¸“é—¨çš„å¥–åŠ±è®¡ç®—æˆ–è®­ç»ƒæ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªç»„åˆå¥–åŠ±å‡½æ•°ï¼Œå°†å­—ç¬¦é”™è¯¯ç‡ï¼ˆCERï¼‰ä¸ä»ASRæ¨¡å‹è·å¾—çš„è´Ÿå¯¹æ•°ä¼¼ç„¶å€¼ï¼ˆNLLï¼‰ç›¸ç»“åˆï¼Œæä¾›æ›´å‡†ç¡®ä¸”ä¿¡æ¯ä¸°å¯Œçš„å¥–åŠ±ä¿¡å·ã€‚æˆ‘ä»¬å°†GRPOå¾®è°ƒåº”ç”¨äºé¢„è®­ç»ƒçš„LLM-based TTSæ¨¡å‹ï¼Œå¹¶è¯„ä¼°å®ƒä»¬çš„é›¶æ ·æœ¬TTSæ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å¯ä»¥æ˜¾è‘—æé«˜åˆæˆè¯­éŸ³çš„å¯æ‡‚åº¦å’Œè‡ªç„¶åº¦ã€‚æ¶ˆèç ”ç©¶å’Œè¿›ä¸€æ­¥çš„åˆ†æè¯å®äº†æ•´åˆä¸¤ç§å¥–åŠ±æˆåˆ†çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.18798v1">PDF</a> 5 pages,submitted to ICASSP2026</p>
<p><strong>Summary</strong></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºGRPOçš„æ–¹æ³•ï¼Œç”¨äºæé«˜åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å‹çš„æ€§èƒ½ã€‚è¯¥æ–¹æ³•é€šè¿‡åˆ©ç”¨ç°æˆçš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹æ¥ç”Ÿæˆå¥–åŠ±ï¼Œæ— éœ€ä¸ºå¥–åŠ±è®¡ç®—æˆ–è®­ç»ƒè®¾ç½®ä¸“ç”¨æ¨¡å‹ã€‚åŒæ—¶ï¼Œè®¾è®¡äº†ä¸€ç§ç»„åˆå¥–åŠ±å‡½æ•°ï¼Œç»“åˆäº†å­—ç¬¦é”™è¯¯ç‡ï¼ˆCERï¼‰å’Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«æ¨¡å‹å¾—åˆ°çš„è´Ÿå¯¹æ•°ä¼¼ç„¶å€¼ï¼ˆNLLï¼‰ï¼Œæä¾›æ›´å‡†ç¡®å’Œä¸°å¯Œçš„å¥–åŠ±ä¿¡å·ã€‚è¯¥ç ”ç©¶å°†GRPOå¾®è°ƒåº”ç”¨äºé¢„è®­ç»ƒçš„LLM-TTSæ¨¡å‹ä¸Šï¼Œå¹¶å¯¹å…¶é›¶æ ·æœ¬TTSæ€§èƒ½è¿›è¡Œè¯„ä¼°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æé«˜äº†åˆæˆè¯­éŸ³çš„å¯ç†è§£æ€§å’Œè‡ªç„¶åº¦ã€‚æ¶ˆèç ”ç©¶å’Œè¿›ä¸€æ­¥çš„åˆ†æè¯å®äº†ç»“åˆä¸¤ç§å¥–åŠ±æˆåˆ†çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§æ–°çš„åŸºäºGRPOçš„æ–¹æ³•æé«˜LLM-TTSæ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>åˆ©ç”¨ç°æˆçš„ASRæ¨¡å‹ç”Ÿæˆå¥–åŠ±ï¼Œç®€åŒ–äº†å¥–åŠ±è®¡ç®—å’Œè®­ç»ƒè¿‡ç¨‹ã€‚</li>
<li>è®¾è®¡äº†ä¸€ç§ç»„åˆå¥–åŠ±å‡½æ•°ï¼Œç»“åˆäº†CERå’ŒASRæ¨¡å‹çš„NLLï¼Œå¢å¼ºäº†å¥–åŠ±ä¿¡å·çš„å‡†ç¡®æ€§ã€‚</li>
<li>GRPOè¢«ç”¨äºå¾®è°ƒé¢„è®­ç»ƒçš„LLM-TTSæ¨¡å‹ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•æé«˜äº†åˆæˆè¯­éŸ³çš„æ™ºå¬åŠ›å’Œè‡ªç„¶åº¦ã€‚</li>
<li>æ¶ˆèç ”ç©¶è¯æ˜äº†ç»“åˆä¸¤ç§å¥–åŠ±æˆåˆ†çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.18798">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-093805a931faa0832676b7211f46d198~resize:0:q75.jpg?source=1f5c5e47&expiration=1760757794&auth_key=1760757794-0-0-4c10dd5936c9258d61640c77a94cbfd8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f94d5c567ee4d88fcdc9f1c42cc93c9f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760687530&auth_key=1760687530-0-0-38dcafe96d3aed367b7eb9b32ecc56c6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e23b95872de562eb4ac79f11135f894b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760687537&auth_key=1760687537-0-0-90ce5f859f693426a3a0e804cdcb1047&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e0954217ec6dc08da5252ee5e0f037c0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760687543&auth_key=1760687543-0-0-ca1be2e6a99674ced42921a0d521259a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="No-Verifiable-Reward-for-Prosody-Toward-Preference-Guided-Prosody-Learning-in-TTS"><a href="#No-Verifiable-Reward-for-Prosody-Toward-Preference-Guided-Prosody-Learning-in-TTS" class="headerlink" title="No Verifiable Reward for Prosody: Toward Preference-Guided Prosody   Learning in TTS"></a>No Verifiable Reward for Prosody: Toward Preference-Guided Prosody   Learning in TTS</h2><p><strong>Authors:Seungyoun Shin, Dongha Ahn, Jiwoo Kim, Sungwook Jeon</strong></p>
<p>Recent work reports gains in neural text-to-speech (TTS) with Group Relative Policy Optimization (GRPO). However, in the absence of a verifiable reward for \textit{prosody}, GRPO trained on transcription-oriented signals (CER&#x2F;NLL) lowers error rates yet collapses prosody into monotone, unnatural speech; adding speaker-similarity further destabilizes training and degrades CER. We address this with an \textit{iterative Direct Preference Optimization (DPO)} scheme that uses only a few hundred human-labeled preference pairs per round to directly optimize prosodic naturalness while regularizing to the current model. On \textbf{KoCC-TTS}, a curated dataset of authentic Korean call center interactions capturing task-oriented dialogues, our method attains the highest human preference (ELO) with competitive CER, outperforming GRPO and strong commercial baselines. These results suggest that when prosody cannot be rewarded automatically, \textit{human preference optimization} offers a practical and data-efficient path to natural and robust TTS. The demo page is available at \href{<a target="_blank" rel="noopener" href="https://tts.ch.dev}/">https://tts.ch.dev}</a> </p>
<blockquote>
<p>è¿‘æœŸçš„å·¥ä½œæŠ¥å‘ŠæŒ‡å‡ºï¼Œåœ¨ç¥ç»æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰é¢†åŸŸï¼Œé‡‡ç”¨ç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰å–å¾—äº†ä¸€äº›è¿›å±•ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹è¯­è°ƒçš„å¯éªŒè¯å¥–åŠ±ï¼ŒåŸºäºè½¬å½•å¯¼å‘ä¿¡å·ï¼ˆCER&#x2F;NLLï¼‰è®­ç»ƒçš„GRPOè™½ç„¶é™ä½äº†é”™è¯¯ç‡ï¼Œä½†å´å°†è¯­è°ƒèåˆæˆå•è°ƒã€ä¸è‡ªç„¶çš„è¯­éŸ³ï¼›å¢åŠ è¯´è¯äººç›¸ä¼¼æ€§ä¼šè¿›ä¸€æ­¥ç ´åè®­ç»ƒå¹¶é™ä½CERã€‚æˆ‘ä»¬é€šè¿‡é‡‡ç”¨ä¸€ç§\emph{è¿­ä»£ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰}æ–¹æ¡ˆæ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œè¯¥æ–¹æ¡ˆæ¯è½®ä»…ä½¿ç”¨æ•°ç™¾ä¸ªäººå·¥æ ‡è®°çš„åå¥½å¯¹ï¼Œä»¥ç›´æ¥ä¼˜åŒ–è¯­è°ƒçš„è‡ªç„¶æ€§ï¼ŒåŒæ—¶å¯¹å…¶è¿›è¡Œæ­£åˆ™åŒ–ä»¥ç¬¦åˆå½“å‰æ¨¡å‹ã€‚åœ¨\textbf{KoCC-TTS}æ•°æ®é›†ä¸Šâ€”â€”ä¸€ä¸ªåŒ…å«çœŸå®éŸ©å›½å‘¼å«ä¸­å¿ƒäº¤äº’ä»»åŠ¡å¯¼å‘å¯¹è¯çš„ç²¾é€‰æ•°æ®é›†â€”â€”æˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†æœ€é«˜çš„äººç±»åå¥½ï¼ˆELOï¼‰ï¼ŒåŒæ—¶æ‹¥æœ‰ç«äº‰æ€§çš„CERï¼Œè¶…è¶Šäº†GRPOå’Œå¼ºå¤§çš„å•†ä¸šåŸºçº¿ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œå½“æ— æ³•è‡ªåŠ¨å¥–åŠ±è¯­è°ƒæ—¶ï¼Œ\emph{äººç±»åå¥½ä¼˜åŒ–}ä¸ºè‡ªç„¶å’Œç¨³å¥çš„TTSæä¾›äº†ä¸€æ¡å®ç”¨ä¸”æ•°æ®é«˜æ•ˆçš„è·¯ã€‚æ¼”ç¤ºé¡µé¢ä½äºï¼š[<a target="_blank" rel="noopener" href="https://tts.ch.dev/]">https://tts.ch.dev/]</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.18531v1">PDF</a> submitted to ICASSP 2026</p>
<p><strong>æ‘˜è¦</strong><br>    æœ€è¿‘çš„å·¥ä½œæŠ¥å‘Šè¡¨æ˜ï¼Œåœ¨ç¥ç»æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰é¢†åŸŸé‡‡ç”¨ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰å–å¾—äº†è¿›å±•ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹å¯éªŒè¯çš„è¯­è°ƒå¥–åŠ±ï¼Œä»…ä¾é è½¬å½•å¯¼å‘ä¿¡å·ï¼ˆCER&#x2F;NLLï¼‰è®­ç»ƒçš„GRPOè™½ç„¶é™ä½äº†é”™è¯¯ç‡ï¼Œä½†ä¼šä½¿è¯­è°ƒå˜å¾—å•è°ƒä¸è‡ªç„¶ï¼›å¢åŠ è¯´è¯äººç›¸ä¼¼æ€§ä¼šè¿›ä¸€æ­¥ä½¿è®­ç»ƒä¸ç¨³å®šå¹¶é™ä½CERã€‚æˆ‘ä»¬é€šè¿‡é‡‡ç”¨ä¸€ç§è¿­ä»£å¼ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰æ–¹æ¡ˆæ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œè¯¥æ–¹æ¡ˆæ¯è½®ä»…ä½¿ç”¨æ•°ç™¾ä¸ªäººå·¥æ ‡è®°çš„åå¥½å¯¹ï¼Œåœ¨ä¼˜åŒ–è¯­è°ƒè‡ªç„¶æ€§çš„åŒæ—¶ï¼Œå¯¹å½“å‰æ¨¡å‹è¿›è¡Œè§„åˆ™åŒ–ã€‚åœ¨çœŸå®çš„éŸ©å›½å‘¼å«ä¸­å¿ƒäº’åŠ¨ä»»åŠ¡å¯¼å‘å¯¹è¯é›†KoCC-TTSä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä»¥æœ€é«˜çš„äººç±»åå¥½ï¼ˆELOï¼‰å’Œå…·æœ‰ç«äº‰åŠ›çš„CERè¡¨ç°å‡ºè‰²ï¼Œè¶…è¶Šäº†GRPOå’Œå¼ºå¤§çš„å•†ä¸šåŸºçº¿ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œå½“æ— æ³•è‡ªåŠ¨å¥–åŠ±è¯­è°ƒæ—¶ï¼Œäººç±»åå¥½ä¼˜åŒ–ä¸ºè‡ªç„¶å’Œç¨³å¥çš„TTSæä¾›äº†ä¸€æ¡å®ç”¨ä¸”æ•°æ®é«˜æ•ˆçš„é“è·¯ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>Group Relative Policy Optimization (GRPO) åœ¨ç¥ç»æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰ä¸­çš„åº”ç”¨è™½ç„¶é™ä½äº†é”™è¯¯ç‡ï¼Œä½†å¯èƒ½å¯¼è‡´è¯­è°ƒå•è°ƒä¸è‡ªç„¶ã€‚</li>
<li>å¢åŠ è¯´è¯äººç›¸ä¼¼æ€§åœ¨GRPOè®­ç»ƒä¸­è¿›ä¸€æ­¥å¯¼è‡´è®­ç»ƒä¸ç¨³å®šå¹¶é™ä½CERï¼ˆå­—ç¬¦é”™è¯¯ç‡ï¼‰ã€‚</li>
<li>è¿­ä»£å¼ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰æ–¹æ¡ˆè¢«æå‡ºæ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œå®ƒç›´æ¥ä½¿ç”¨å°‘é‡äººå·¥æ ‡è®°çš„åå¥½å¯¹æ¥ä¼˜åŒ–è¯­è°ƒçš„è‡ªç„¶æ€§ã€‚</li>
<li>DPOæ–¹æ¡ˆåœ¨è§„åˆ™åŒ–å½“å‰æ¨¡å‹çš„åŒæ—¶ï¼Œèƒ½æœ‰æ•ˆæé«˜è¯­è°ƒçš„è‡ªç„¶æ€§ã€‚</li>
<li>åœ¨KoCC-TTSæ•°æ®é›†ä¸Šï¼ŒDPOæ–¹æ³•è¡¨ç°å‡ºå“è¶Šçš„äººç±»åå¥½å’Œç«äº‰åŠ›å¼ºçš„CERï¼Œè¶…è¶Šäº†GRPOå’Œå…¶ä»–å…ˆè¿›æ–¹æ³•ã€‚</li>
<li>å®éªŒç»“æœå¼ºè°ƒäº†äººç±»åå¥½ä¼˜åŒ–åœ¨TTSä¸­çš„é‡è¦æ€§ï¼Œå°¤å…¶æ˜¯åœ¨æ— æ³•è‡ªåŠ¨å¥–åŠ±è¯­è°ƒçš„æƒ…å†µä¸‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.18531">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-9f4a958a281af1a3f0f36931994b35c3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760687552&auth_key=1760687552-0-0-ce11a9d579b7af264bca92174e1c17fc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0c2b5693a30302041ee178fa1db3dc39~resize:0:q75.jpg?source=1f5c5e47&expiration=1760687559&auth_key=1760687559-0-0-cb35d4f97d7d405ee979789a1751cbbe&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e1233dc950b94152320b2969c4aac672~resize:0:q75.jpg?source=1f5c5e47&expiration=1760687565&auth_key=1760687565-0-0-770f813a7f80ccc0e5fc8b92dd589dec&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Frustratingly-Easy-Data-Augmentation-for-Low-Resource-ASR"><a href="#Frustratingly-Easy-Data-Augmentation-for-Low-Resource-ASR" class="headerlink" title="Frustratingly Easy Data Augmentation for Low-Resource ASR"></a>Frustratingly Easy Data Augmentation for Low-Resource ASR</h2><p><strong>Authors:Katsumi Ibaraki, David Chiang</strong></p>
<p>This paper introduces three self-contained data augmentation methods for low-resource Automatic Speech Recognition (ASR). Our techniques first generate novel textâ€“using gloss-based replacement, random replacement, or an LLM-based approachâ€“and then apply Text-to-Speech (TTS) to produce synthetic audio. We apply these methods, which leverage only the original annotated data, to four languages with extremely limited resources (Vatlongos, Nashta, Shinekhen Buryat, and Kakabe). Fine-tuning a pretrained Wav2Vec2-XLSR-53 model on a combination of the original audio and generated synthetic data yields significant performance gains, including a 14.3% absolute WER reduction for Nashta. The methods prove effective across all four low-resource languages and also show utility for high-resource languages like English, demonstrating their broad applicability. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸‰ç§é’ˆå¯¹ä½èµ„æºè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰çš„è‡ªä¸»æ•°æ®å¢å¼ºæ–¹æ³•ã€‚æˆ‘ä»¬çš„æŠ€æœ¯é¦–å…ˆåˆ©ç”¨åŸºäºè¯å¢æ›¿æ¢ã€éšæœºæ›¿æ¢æˆ–åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ–¹æ³•ç”Ÿæˆæ–°çš„æ–‡æœ¬ï¼Œç„¶ååº”ç”¨æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æŠ€æœ¯ç”ŸæˆåˆæˆéŸ³é¢‘ã€‚æˆ‘ä»¬åªåœ¨æœ‰é™èµ„æºçš„å››ç§è¯­è¨€ï¼ˆç“¦ç‰¹éš†æˆˆæ–¯è¯­ã€çº³æ–¯å¡”è¯­ã€è°¢å†…è‚¯å¸ƒé‡Œäºšç‰¹è¯­å’Œå¡å¡è´è¯­ï¼‰ä¸­åº”ç”¨è¿™äº›æ–¹æ³•ï¼Œä»…åˆ©ç”¨åŸå§‹æ³¨é‡Šæ•°æ®ã€‚é€šè¿‡å¾®è°ƒé¢„è®­ç»ƒçš„Wav2Vec2-XLSR-53æ¨¡å‹ï¼Œç»“åˆåŸå§‹éŸ³é¢‘å’Œç”Ÿæˆçš„åˆæˆæ•°æ®ï¼Œå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå…¶ä¸­åŒ…æ‹¬çº³æ–¯å¡”è¯­çš„ç»å¯¹å­—è¯é”™è¯¯ç‡ï¼ˆWERï¼‰é™ä½14.3%ã€‚è¿™äº›æ–¹æ³•åœ¨å››ç§ä½èµ„æºè¯­è¨€ä¸­éƒ½è¯æ˜äº†å…¶æœ‰æ•ˆæ€§ï¼Œå¹¶æ˜¾ç¤ºå‡ºå¯¹é«˜èµ„æºè¯­è¨€å¦‚è‹±è¯­çš„å®ç”¨æ€§ï¼Œè¯æ˜äº†å…¶å¹¿æ³›çš„åº”ç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.15373v2">PDF</a> 5 pages, 2 figures, 2 tables, submitted to ICASSP 2026</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸‰ç§é’ˆå¯¹ä½èµ„æºè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰çš„è‡ªåŒ…å«æ•°æ®å¢å¼ºæ–¹æ³•ã€‚è¿™äº›æ–¹æ³•é¦–å…ˆåˆ©ç”¨åŸºäºè¯å…ƒæ›¿æ¢ã€éšæœºæ›¿æ¢æˆ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ–¹æ³•ç”Ÿæˆæ–°æ–‡æœ¬ï¼Œç„¶ååº”ç”¨æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æŠ€æœ¯ç”ŸæˆåˆæˆéŸ³é¢‘ã€‚è¿™äº›æ–¹æ³•ä»…åˆ©ç”¨åŸå§‹æ³¨é‡Šæ•°æ®ï¼Œåº”ç”¨äºå››ç§èµ„æºæä¸ºæœ‰é™çš„è¯­è¨€ï¼ˆç“¦ç‰¹é¾™æˆˆè¯­ã€çº³æ–¯å¡”è¯­ã€æ²™æ¶…è‚¯Â·å¸ƒé‡Œäºšç‰¹è¯­å’Œå¡å¡è´è¯­ï¼‰ã€‚å¯¹é¢„è®­ç»ƒçš„Wav2Vec2-XLSR-53æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œç»“åˆåŸå§‹éŸ³é¢‘å’Œç”Ÿæˆçš„åˆæˆæ•°æ®ï¼Œå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå…¶ä¸­çº³æ–¯å¡”è¯­çš„ç»å¯¹å­—è¯é”™è¯¯ç‡ï¼ˆWERï¼‰é™ä½äº†14.3%ã€‚è¿™äº›æ–¹æ³•åœ¨å››ç§ä½èµ„æºè¯­è¨€ä¸­éƒ½è¡¨ç°å‡ºæœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ï¼Œå¹¶è¯æ˜åœ¨é«˜èµ„æºè¯­è¨€å¦‚è‹±è¯­ä¸­ä¹Ÿæœ‰åº”ç”¨ä»·å€¼ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>ä»‹ç»äº†ä¸‰ç§é’ˆå¯¹ä½èµ„æºASRçš„è‡ªåŒ…å«æ•°æ®å¢å¼ºæ–¹æ³•ã€‚</li>
<li>é€šè¿‡ç”Ÿæˆæ–°æ–‡æœ¬å¹¶åˆ©ç”¨TTSæŠ€æœ¯è½¬åŒ–ä¸ºåˆæˆéŸ³é¢‘ã€‚</li>
<li>æ–¹æ³•ä»…åˆ©ç”¨åŸå§‹æ³¨é‡Šæ•°æ®ï¼Œé€‚ç”¨äºå¤šç§ä½èµ„æºè¯­è¨€ã€‚</li>
<li>åœ¨å››ç§æµ‹è¯•è¯­è¨€ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</li>
<li>åœ¨çº³æ–¯å¡”è¯­ä¸­ï¼Œç»å¯¹å­—è¯é”™è¯¯ç‡ï¼ˆWERï¼‰é™ä½äº†14.3%ã€‚</li>
<li>æ–¹æ³•åŒæ ·é€‚ç”¨äºé«˜èµ„æºè¯­è¨€ï¼Œå¦‚è‹±è¯­ã€‚</li>
<li>è¯æ˜äº†è¿™äº›æ–¹æ³•å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.15373">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-e607b8f83468f4baea55d969f7d47e58~resize:0:q75.jpg?source=1f5c5e47&expiration=1760687594&auth_key=1760687594-0-0-40acdb1941588b966832bf41febcc529&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d7d690eb562288305439ac1d0e6510cb~resize:0:q75.jpg?source=1f5c5e47&expiration=1760687609&auth_key=1760687609-0-0-aff1b12ccb4055cb8f5ddfa95200a5ec&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-67fab1ec14f6d32327c4039325b0d830~resize:0:q75.jpg?source=1f5c5e47&expiration=1760687658&auth_key=1760687658-0-0-a99d59cb1c910fcafa5b6ec04229663f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-450979640f185d77be41b3fea5f22364~resize:0:q75.jpg?source=1f5c5e47&expiration=1760757802&auth_key=1760757802-0-0-e7cae69fe504c314183aead4cb474fc9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0030000340f7ffe547f2b27343c03b7a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760757808&auth_key=1760757808-0-0-d9a7fea6e0e9c527e16aeb78199ae5b2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="UDDETTS-Unifying-Discrete-and-Dimensional-Emotions-for-Controllable-Emotional-Text-to-Speech"><a href="#UDDETTS-Unifying-Discrete-and-Dimensional-Emotions-for-Controllable-Emotional-Text-to-Speech" class="headerlink" title="UDDETTS: Unifying Discrete and Dimensional Emotions for Controllable   Emotional Text-to-Speech"></a>UDDETTS: Unifying Discrete and Dimensional Emotions for Controllable   Emotional Text-to-Speech</h2><p><strong>Authors:Jiaxuan Liu, Yang Xiang, Han Zhao, Xiangang Li, Yingying Gao, Shilei Zhang, Zhenhua Ling</strong></p>
<p>Recent large language models (LLMs) have made great progress in the field of text-to-speech (TTS), but they still face major challenges in synthesizing fine-grained emotional speech in an interpretable manner. Traditional methods rely on discrete emotion labels to control emotion categories and intensities, which cannot capture the complexity and continuity of human emotional perception and expression. The lack of large-scale emotional speech datasets with balanced emotion distributions and fine-grained emotional annotations often causes overfitting in synthesis models and impedes effective emotion control. To address these issues, we propose UDDETTS, a universal LLM framework unifying discrete and dimensional emotions for controllable emotional TTS. This model introduces the interpretable Arousal-Dominance-Valence (ADV) space for dimensional emotion description and supports emotion control driven by either discrete emotion labels or nonlinearly quantified ADV values. Furthermore, a semi-supervised training strategy is designed to comprehensively utilize diverse speech datasets with different types of emotional annotations to train the UDDETTS. Experiments show that UDDETTS achieves linear emotion control along three interpretable dimensions, and exhibits superior end-to-end emotional speech synthesis capabilities. Code and demos are available at: <a target="_blank" rel="noopener" href="https://anonymous.4open.science/w/UDDETTS">https://anonymous.4open.science/w/UDDETTS</a>. </p>
<blockquote>
<p>æœ€è¿‘çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰é¢†åŸŸå–å¾—äº†å·¨å¤§çš„è¿›æ­¥ï¼Œä½†å®ƒä»¬åœ¨ä»¥å¯è§£é‡Šçš„æ–¹å¼åˆæˆç²¾ç»†æƒ…ç»ªè¯­éŸ³æ–¹é¢ä»é¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚ä¼ ç»Ÿçš„æ–¹æ³•ä¾èµ–äºç¦»æ•£çš„æƒ…ç»ªæ ‡ç­¾æ¥æ§åˆ¶æƒ…ç»ªç±»åˆ«å’Œå¼ºåº¦ï¼Œè¿™æ— æ³•æ•æ‰äººç±»æƒ…ç»ªæ„ŸçŸ¥å’Œè¡¨è¾¾çš„å¤æ‚æ€§å’Œè¿ç»­æ€§ã€‚ç¼ºä¹å…·æœ‰å¹³è¡¡æƒ…ç»ªåˆ†å¸ƒå’Œç²¾ç»†æƒ…ç»ªæ³¨é‡Šçš„å¤§è§„æ¨¡æƒ…ç»ªè¯­éŸ³æ•°æ®é›†é€šå¸¸ä¼šå¯¼è‡´åˆæˆæ¨¡å‹ä¸­çš„è¿‡åº¦æ‹Ÿåˆï¼Œå¹¶é˜»ç¢æœ‰æ•ˆçš„æƒ…ç»ªæ§åˆ¶ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†UDDETTSï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€ç¦»æ•£å’Œç»´åº¦æƒ…ç»ªçš„é€šç”¨LLMæ¡†æ¶ï¼Œç”¨äºå¯æ§çš„æƒ…ç»ªTTSã€‚è¯¥æ¨¡å‹å¼•å…¥å¯è§£é‡Šçš„Arousal-Dominance-Valenceï¼ˆADVï¼‰ç©ºé—´è¿›è¡Œç»´åº¦æƒ…ç»ªæè¿°ï¼Œå¹¶æ”¯æŒç”±ç¦»æ•£æƒ…ç»ªæ ‡ç­¾æˆ–éçº¿æ€§é‡åŒ–çš„ADVå€¼é©±åŠ¨çš„æƒ…ç»ªæ§åˆ¶ã€‚æ­¤å¤–ï¼Œè®¾è®¡äº†ä¸€ç§åŠç›‘ç£è®­ç»ƒç­–ç•¥ï¼Œä»¥ç»¼åˆåˆ©ç”¨å…·æœ‰ä¸åŒç±»å‹æƒ…ç»ªæ³¨é‡Šçš„å¤šæ ·åŒ–è¯­éŸ³æ•°æ®é›†æ¥è®­ç»ƒUDDETTSã€‚å®éªŒè¡¨æ˜ï¼ŒUDDETTSåœ¨ä¸‰ä¸ªå¯è§£é‡Šç»´åº¦ä¸Šå®ç°äº†çº¿æ€§æƒ…ç»ªæ§åˆ¶ï¼Œå¹¶å±•ç°å‡ºå‡ºè‰²çš„ç«¯åˆ°ç«¯æƒ…ç»ªè¯­éŸ³åˆæˆèƒ½åŠ›ã€‚ä»£ç å’Œæ¼”ç¤ºå¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://anonymous.4open.science/w/UDDETTS%E8%AE%BF%E9%97%AE%E3%80%82">https://anonymous.4open.science/w/UDDETTSè®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.10599v2">PDF</a> Under review</p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸå¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ–‡æœ¬è½¬è¯­éŸ³é¢†åŸŸå–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨ç²¾ç»†æƒ…æ„Ÿè¯­éŸ³åˆæˆæ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚ä¼ ç»Ÿæ–¹æ³•ä¾èµ–ç¦»æ•£æƒ…æ„Ÿæ ‡ç­¾æ§åˆ¶æƒ…æ„Ÿç±»åˆ«å’Œå¼ºåº¦ï¼Œæ— æ³•æ•æ‰äººç±»æƒ…æ„Ÿæ„ŸçŸ¥å’Œè¡¨è¾¾çš„å¤æ‚æ€§å’Œè¿ç»­æ€§ã€‚é’ˆå¯¹è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†UDDETTSæ¨¡å‹ï¼Œè¯¥æ¨¡å‹ç»“åˆäº†ç¦»æ•£æƒ…æ„Ÿå’Œç»´åº¦æƒ…æ„Ÿï¼Œå®ç°äº†å¯æ§çš„æƒ…æ„ŸTTSã€‚è¯¥æ¨¡å‹å¼•å…¥å¯è§£é‡Šçš„æ¿€æ´»åº¦-æ”¯é…åŠ›-ä»·å€¼ï¼ˆADVï¼‰ç©ºé—´è¿›è¡Œç»´åº¦æƒ…æ„Ÿæè¿°ï¼Œæ”¯æŒç”±ç¦»æ•£æƒ…æ„Ÿæ ‡ç­¾æˆ–éçº¿æ€§é‡åŒ–çš„ADVå€¼é©±åŠ¨çš„æƒ…æ„Ÿæ§åˆ¶ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§åŠç›‘ç£è®­ç»ƒç­–ç•¥ï¼Œä»¥å……åˆ†åˆ©ç”¨ä¸åŒç±»å‹çš„æƒ…æ„Ÿæ³¨é‡Šçš„è¯­éŸ³æ•°æ®é›†æ¥è®­ç»ƒUDDETTSã€‚å®éªŒè¡¨æ˜ï¼ŒUDDETTSåœ¨ä¸‰ä¸ªå¯è§£é‡Šç»´åº¦ä¸Šå®ç°äº†çº¿æ€§æƒ…æ„Ÿæ§åˆ¶ï¼Œå¹¶å±•ç¤ºäº†å‡ºè‰²çš„ç«¯åˆ°ç«¯æƒ…æ„Ÿè¯­éŸ³åˆæˆèƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨TTSé¢†åŸŸæœ‰è¿›æ­¥ï¼Œä½†åœ¨ç²¾ç»†æƒ…æ„Ÿè¯­éŸ³åˆæˆä¸Šä»é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•ä½¿ç”¨ç¦»æ•£æƒ…æ„Ÿæ ‡ç­¾ï¼Œæ— æ³•æ•æ‰äººç±»æƒ…æ„Ÿæ„ŸçŸ¥å’Œè¡¨è¾¾çš„å¤æ‚æ€§ã€‚</li>
<li>UDDETTSæ¨¡å‹ç»“åˆäº†ç¦»æ•£æƒ…æ„Ÿå’Œç»´åº¦æƒ…æ„Ÿï¼Œå®ç°å¯æ§çš„æƒ…æ„ŸTTSã€‚</li>
<li>UDDETTSå¼•å…¥ADVç©ºé—´è¿›è¡Œç»´åº¦æƒ…æ„Ÿæè¿°ï¼Œæ”¯æŒç¦»æ•£æƒ…æ„Ÿæ ‡ç­¾æˆ–ADVå€¼é©±åŠ¨çš„æƒ…æ„Ÿæ§åˆ¶ã€‚</li>
<li>UDDETTSé‡‡ç”¨åŠç›‘ç£è®­ç»ƒç­–ç•¥ï¼Œåˆ©ç”¨å¤šç§æƒ…æ„Ÿæ³¨é‡Šçš„è¯­éŸ³æ•°æ®é›†ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºUDDETTSåœ¨ä¸‰ä¸ªç»´åº¦ä¸Šå®ç°çº¿æ€§æƒ…æ„Ÿæ§åˆ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.10599">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-653fd2e5075d3f3adb649923dd27ebd5~resize:0:q75.jpg?source=1f5c5e47&expiration=1760757850&auth_key=1760757850-0-0-e2a4eacce9e4b4748a53ab14a239749c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-429d9907ec0d39dca2c973b1375a306e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760757891&auth_key=1760757891-0-0-308ddd3febee2fb0e63dabf956ac4e6b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4ba696190f873c95114bd0dc210b3f77~resize:0:q75.jpg?source=1f5c5e47&expiration=1760757898&auth_key=1760757898-0-0-eec1d74bb2be9d589ef02fbfe86aec20&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="SupertonicTTS-Towards-Highly-Efficient-and-Streamlined-Text-to-Speech-System"><a href="#SupertonicTTS-Towards-Highly-Efficient-and-Streamlined-Text-to-Speech-System" class="headerlink" title="SupertonicTTS: Towards Highly Efficient and Streamlined Text-to-Speech   System"></a>SupertonicTTS: Towards Highly Efficient and Streamlined Text-to-Speech   System</h2><p><strong>Authors:Hyeongju Kim, Jinhyeok Yang, Yechan Yu, Seunghun Ji, Jacob Morton, Frederik Bous, Joon Byun, Juheon Lee</strong></p>
<p>We introduce SupertonicTTS, a novel text-to-speech (TTS) system designed for efficient and streamlined speech synthesis. SupertonicTTS comprises three components: a speech autoencoder for continuous latent representation, a text-to-latent module leveraging flow-matching for text-to-latent mapping, and an utterance-level duration predictor. To enable a lightweight architecture, we employ a low-dimensional latent space, temporal compression of latents, and ConvNeXt blocks. The TTS pipeline is further simplified by operating directly on raw character-level text and employing cross-attention for text-speech alignment, thus eliminating the need for grapheme-to-phoneme (G2P) modules and external aligners. In addition, we propose context-sharing batch expansion that accelerates loss convergence and stabilizes text-speech alignment with minimal memory and I&#x2F;O overhead. Experimental results demonstrate that SupertonicTTS delivers performance comparable to contemporary zero-shot TTS models with only 44M parameters, while significantly reducing architectural complexity and computational cost. Audio samples are available at: <a target="_blank" rel="noopener" href="https://supertonictts.github.io/">https://supertonictts.github.io/</a>. </p>
<blockquote>
<p>æˆ‘ä»¬å¼•å…¥äº†SupertonicTTSï¼Œè¿™æ˜¯ä¸€ä¸ªä¸ºé«˜æ•ˆå’Œç®€æ´çš„è¯­éŸ³åˆæˆè€Œè®¾è®¡çš„æ–°å‹æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿã€‚SupertonicTTSåŒ…å«ä¸‰ä¸ªç»„ä»¶ï¼šç”¨äºè¿ç»­æ½œåœ¨è¡¨ç¤ºçš„è¯­éŸ³è‡ªåŠ¨ç¼–ç å™¨ã€åˆ©ç”¨æµåŒ¹é…è¿›è¡Œæ–‡æœ¬åˆ°æ½œåœ¨æ˜ å°„çš„æ–‡æœ¬åˆ°æ½œåœ¨æ¨¡å—ï¼Œä»¥åŠè¯­å¥çº§æŒç»­æ—¶é—´é¢„æµ‹å™¨ã€‚ä¸ºäº†å®ç°è½»é‡çº§æ¶æ„ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä½ç»´æ½œåœ¨ç©ºé—´ã€æ½œåœ¨æ—¶é—´çš„ä¸´æ—¶å‹ç¼©å’ŒConvNeXtå—ã€‚TTSç®¡é“é€šè¿‡ç›´æ¥åœ¨åŸå§‹å­—ç¬¦çº§æ–‡æœ¬ä¸Šæ“ä½œå¹¶åˆ©ç”¨è·¨æ³¨æ„åŠ›è¿›è¡Œæ–‡æœ¬è¯­éŸ³å¯¹é½ï¼Œä»è€Œè¿›ä¸€æ­¥ç®€åŒ–ï¼Œä»è€Œæ¶ˆé™¤äº†å¯¹å­—æ¯åˆ°éŸ³ç´ ï¼ˆG2Pï¼‰æ¨¡å—å’Œå¤–éƒ¨å¯¹é½å™¨çš„éœ€æ±‚ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸Šä¸‹æ–‡å…±äº«æ‰¹é‡æ‰©å±•ï¼Œå¯åŠ é€ŸæŸå¤±æ”¶æ•›ï¼Œå¹¶ä½¿ç”¨æœ€å°çš„å†…å­˜å’ŒI&#x2F;Oå¼€é”€æ¥ç¨³å®šæ–‡æœ¬è¯­éŸ³å¯¹é½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSupertonicTTSåœ¨ä»…ä½¿ç”¨44Må‚æ•°çš„æƒ…å†µä¸‹ï¼Œæ€§èƒ½ä¸å½“ä»£é›¶æ ·æœ¬TTSæ¨¡å‹ç›¸å½“ï¼ŒåŒæ—¶æ˜¾è‘—é™ä½äº†æ¶æ„çš„å¤æ‚æ€§å’Œè®¡ç®—æˆæœ¬ã€‚éŸ³é¢‘æ ·æœ¬å¯åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://supertonictts.github.io/%E3%80%82">https://supertonictts.github.io/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.23108v3">PDF</a> 22 pages, preprint</p>
<p><strong>Summary</strong></p>
<p>SupertonicTTSæ˜¯ä¸€ç§æ–°å‹æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿï¼ŒåŒ…å«ä¸‰ä¸ªç»„ä»¶ï¼šè¯­éŸ³è‡ªç¼–ç å™¨ã€æ–‡æœ¬åˆ°æ½œåœ¨æ¨¡å—å’Œè¯­å¥çº§æ—¶é•¿é¢„æµ‹å™¨ã€‚è¯¥ç³»ç»Ÿé‡‡ç”¨ä½ç»´æ½œåœ¨ç©ºé—´ã€æ½œåœ¨æ—¶æ€å‹ç¼©å’ŒConvNeXtå—å®ç°è½»é‡åŒ–æ¶æ„ï¼Œå¹¶ç›´æ¥æ“ä½œå­—ç¬¦çº§æ–‡æœ¬ï¼Œé‡‡ç”¨è·¨æ³¨æ„åŠ›æ–‡æœ¬è¯­éŸ³å¯¹é½ï¼Œæ— éœ€grapheme-to-phonemeï¼ˆG2Pï¼‰æ¨¡å—å’Œå¤–éƒ¨å¯¹é½å™¨ã€‚åŒæ—¶ï¼Œæå‡ºä¸Šä¸‹æ–‡å…±äº«æ‰¹é‡æ‰©å±•æŠ€æœ¯ï¼ŒåŠ é€ŸæŸå¤±æ”¶æ•›ï¼Œç¨³å®šæ–‡æœ¬è¯­éŸ³å¯¹é½ï¼Œä¸”å†…å­˜å’ŒI&#x2F;Oå¼€é”€è¾ƒå°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSupertonicTTSåœ¨å‚æ•°ä»…44Mçš„æƒ…å†µä¸‹ï¼Œæ€§èƒ½ä¸å½“å‰é›¶æ ·æœ¬TTSæ¨¡å‹ç›¸å½“ï¼ŒåŒæ—¶é™ä½äº†æ¶æ„å¤æ‚æ€§å’Œè®¡ç®—æˆæœ¬ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SupertonicTTSæ˜¯ä¸€ç§æ–°å‹çš„æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿï¼Œæ—¨åœ¨å®ç°é«˜æ•ˆå’Œæµçº¿å‹çš„è¯­éŸ³åˆæˆã€‚</li>
<li>ç³»ç»ŸåŒ…å«ä¸‰ä¸ªä¸»è¦ç»„ä»¶ï¼šè¯­éŸ³è‡ªç¼–ç å™¨ã€æ–‡æœ¬åˆ°æ½œåœ¨æ¨¡å—å’Œè¯­å¥çº§æ—¶é•¿é¢„æµ‹å™¨ã€‚</li>
<li>é‡‡ç”¨ä½ç»´æ½œåœ¨ç©ºé—´ã€æ½œåœ¨æ—¶æ€å‹ç¼©å’ŒConvNeXtå—å®ç°è½»é‡åŒ–æ¶æ„ã€‚</li>
<li>ç³»ç»Ÿç›´æ¥æ“ä½œå­—ç¬¦çº§æ–‡æœ¬ï¼Œé‡‡ç”¨è·¨æ³¨æ„åŠ›æœºåˆ¶è¿›è¡Œæ–‡æœ¬è¯­éŸ³å¯¹é½ï¼Œæ— éœ€é¢å¤–çš„grapheme-to-phonemeï¼ˆG2Pï¼‰æ¨¡å—å’Œå¤–éƒ¨å¯¹é½å™¨ã€‚</li>
<li>æå‡ºäº†ä¸Šä¸‹æ–‡å…±äº«æ‰¹é‡æ‰©å±•æŠ€æœ¯ï¼Œä»¥æé«˜æ•ˆç‡å¹¶ç¨³å®šæ–‡æœ¬è¯­éŸ³å¯¹é½ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒSupertonicTTSåœ¨å‚æ•°è¾ƒå°‘çš„æƒ…å†µä¸‹å®ç°äº†ä¸å½“å‰é›¶æ ·æœ¬TTSæ¨¡å‹ç›¸å½“çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.23108">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-892a902da28731df64124e0db4ae3439~resize:0:q75.jpg?source=1f5c5e47&expiration=1760757905&auth_key=1760757905-0-0-a192e8ade0b618c73a272609f52c26e1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d7873df7112fb9e16ccf0bb313e8b7af~resize:0:q75.jpg?source=1f5c5e47&expiration=1760757912&auth_key=1760757912-0-0-0b604c4fa36c6c21906a10b8c0392273&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1707ed98849f8be0b21502468d32fa29~resize:0:q75.jpg?source=1f5c5e47&expiration=1760757918&auth_key=1760757918-0-0-e9473fe7197a5e0d4dd46b8d07863a78&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3501fbe9532044b5c7218fadea33f8f4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760757925&auth_key=1760757925-0-0-78bd3af290241a7e49c29cc7f66bbaf0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Scaling-Rich-Style-Prompted-Text-to-Speech-Datasets"><a href="#Scaling-Rich-Style-Prompted-Text-to-Speech-Datasets" class="headerlink" title="Scaling Rich Style-Prompted Text-to-Speech Datasets"></a>Scaling Rich Style-Prompted Text-to-Speech Datasets</h2><p><strong>Authors:Anuj Diwan, Zhisheng Zheng, David Harwath, Eunsol Choi</strong></p>
<p>We introduce Paralinguistic Speech Captions (ParaSpeechCaps), a large-scale dataset that annotates speech utterances with rich style captions. While rich abstract tags (e.g. guttural, nasal, pained) have been explored in small-scale human-annotated datasets, existing large-scale datasets only cover basic tags (e.g. low-pitched, slow, loud). We combine off-the-shelf text and speech embedders, classifiers and an audio language model to automatically scale rich tag annotations for the first time. ParaSpeechCaps covers a total of 59 style tags, including both speaker-level intrinsic tags and utterance-level situational tags. It consists of 342 hours of human-labelled data (PSC-Base) and 2427 hours of automatically annotated data (PSC-Scaled). We finetune Parler-TTS, an open-source style-prompted TTS model, on ParaSpeechCaps, and achieve improved style consistency (+7.9% Consistency MOS) and speech quality (+15.5% Naturalness MOS) over the best performing baseline that combines existing rich style tag datasets. We ablate several of our dataset design choices to lay the foundation for future work in this space. Our dataset, models and code are released at <a target="_blank" rel="noopener" href="https://github.com/ajd12342/paraspeechcaps">https://github.com/ajd12342/paraspeechcaps</a> . </p>
<blockquote>
<p>æˆ‘ä»¬å¼•å…¥äº†Paralinguistic Speech Captionsï¼ˆParaSpeechCapsï¼‰è¿™ä¸€å¤§è§„æ¨¡æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†å¯¹è¯­éŸ³å‘å£°è¿›è¡Œäº†ä¸°å¯Œçš„é£æ ¼å­—å¹•æ ‡æ³¨ã€‚è™½ç„¶æŠ½è±¡æ ‡ç­¾ï¼ˆå¦‚å–‰éŸ³ã€é¼»éŸ³ã€ç—›è‹¦ï¼‰å·²åœ¨å°è§„æ¨¡äººå·¥æ ‡æ³¨çš„æ•°æ®é›†ä¸­å¾—åˆ°æ¢ç´¢ï¼Œä½†ç°æœ‰å¤§è§„æ¨¡æ•°æ®é›†ä»…æ¶µç›–åŸºæœ¬æ ‡ç­¾ï¼ˆå¦‚ä½éŸ³ã€æ…¢é€Ÿã€å¤§å£°ï¼‰ã€‚æˆ‘ä»¬é¦–æ¬¡ç»“åˆç°æˆçš„æ–‡æœ¬å’Œè¯­éŸ³åµŒå…¥å™¨ã€åˆ†ç±»å™¨ä»¥åŠéŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼Œè‡ªåŠ¨æ‰©å±•ä¸°å¯Œçš„æ ‡ç­¾æ³¨é‡Šã€‚ParaSpeechCapså…±æ¶µç›–59ä¸ªé£æ ¼æ ‡ç­¾ï¼ŒåŒ…æ‹¬è¯´è¯è€…çº§åˆ«çš„å†…åœ¨æ ‡ç­¾å’Œå‘å£°çº§åˆ«çš„æƒ…å¢ƒæ ‡ç­¾ã€‚å®ƒåŒ…å«342å°æ—¶çš„äººå·¥æ ‡æ³¨æ•°æ®ï¼ˆPSC-Baseï¼‰å’Œ2427å°æ—¶çš„è‡ªåŠ¨æ³¨é‡Šæ•°æ®ï¼ˆPSC-Scaledï¼‰ã€‚æˆ‘ä»¬åœ¨ParaSpeechCapsä¸Šå¾®è°ƒäº†Parler-TTSè¿™ä¸€å¼€æºçš„é£æ ¼æç¤ºTTSæ¨¡å‹ï¼Œä¸ç»“åˆç°æœ‰ä¸°å¯Œé£æ ¼æ ‡ç­¾æ•°æ®é›†çš„æœ€ä½³åŸºçº¿ç›¸æ¯”ï¼Œå®ç°äº†é£æ ¼ä¸€è‡´æ€§ï¼ˆ+7.9%ä¸€è‡´æ€§å¹³å‡æ„è§å¾—åˆ†ï¼‰å’Œè¯­éŸ³è´¨é‡ï¼ˆ+15.5%è‡ªç„¶åº¦å¹³å‡æ„è§å¾—åˆ†ï¼‰çš„æå‡ã€‚æˆ‘ä»¬å¯¹æ•°æ®é›†è®¾è®¡çš„å‡ ä¸ªé€‰æ‹©è¿›è¡Œäº†å‰–æï¼Œä¸ºæœªæ¥åœ¨è¿™ä¸ªé¢†åŸŸçš„å·¥ä½œå¥ å®šåŸºç¡€ã€‚æˆ‘ä»¬çš„æ•°æ®é›†ã€æ¨¡å‹å’Œä»£ç å·²åœ¨<a target="_blank" rel="noopener" href="https://github.com/ajd12342/paraspeechcaps%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/ajd12342/paraspeechcapså‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04713v2">PDF</a> EMNLP 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Paralinguistic Speech Captionsï¼ˆParaSpeechCapsï¼‰æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†å¯¹è¯­éŸ³ç‰‡æ®µè¿›è¡Œäº†ä¸°å¯Œçš„é£æ ¼æ ‡æ³¨ã€‚å®ƒç»“åˆäº†ç°æœ‰çš„æ–‡æœ¬å’Œè¯­éŸ³åµŒå…¥å™¨ã€åˆ†ç±»å™¨ä»¥åŠéŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼Œé¦–æ¬¡å®ç°äº†ä¸°å¯Œçš„æ ‡ç­¾æ ‡æ³¨çš„å¤§è§„æ¨¡è‡ªåŠ¨åŒ–ã€‚ParaSpeechCapsæ¶µç›–äº†59ç§é£æ ¼æ ‡ç­¾ï¼ŒåŒ…æ‹¬è¯´è¯è€…çº§åˆ«çš„å†…åœ¨æ ‡ç­¾å’Œè¯è¯­çº§åˆ«çš„æƒ…å¢ƒæ ‡ç­¾ã€‚è¯¥æ•°æ®é›†åŒ…æ‹¬342å°æ—¶çš„äººå·¥æ ‡æ³¨æ•°æ®å’Œ2427å°æ—¶çš„è‡ªåŠ¨æ ‡æ³¨æ•°æ®ã€‚é€šè¿‡å¯¹Parler-TTSæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå®ç°äº†é£æ ¼ä¸€è‡´æ€§å’Œè¯­éŸ³è´¨é‡çš„æ”¹è¿›ã€‚æ•°æ®é›†ã€æ¨¡å‹å’Œä»£ç å·²åœ¨GitHubä¸Šå‘å¸ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Paralinguistic Speech Captionsï¼ˆParaSpeechCapsï¼‰æ˜¯ä¸€ä¸ªå¤§å‹æ•°æ®é›†ï¼Œå°†è¯­éŸ³ç‰‡æ®µä¸ä¸°å¯Œçš„é£æ ¼æ ‡æ³¨ç›¸ç»“åˆã€‚</li>
<li>ç°æœ‰å¤§å‹æ•°æ®é›†ä¸»è¦è¦†ç›–åŸºæœ¬æ ‡ç­¾ï¼Œè€ŒParaSpeechCapsæ¶µç›–äº†59ç§é£æ ¼æ ‡ç­¾ï¼ŒåŒ…æ‹¬è¯´è¯è€…çº§åˆ«çš„å†…åœ¨æ ‡ç­¾å’Œè¯è¯­çº§åˆ«çš„æƒ…å¢ƒæ ‡ç­¾ã€‚</li>
<li>è¯¥æ•°æ®é›†åŒ…æ‹¬äººå·¥æ ‡æ³¨å’Œè‡ªåŠ¨æ ‡æ³¨çš„æ•°æ®ï¼Œåˆ†åˆ«åŒ…å«342å°æ—¶å’Œ2427å°æ—¶çš„æ•°æ®ã€‚</li>
<li>é€šè¿‡åœ¨ParaSpeechCapsä¸Šå¾®è°ƒParler-TTSæ¨¡å‹ï¼Œå®ç°äº†é£æ ¼ä¸€è‡´æ€§å’Œè¯­éŸ³è´¨é‡çš„æ”¹è¿›ã€‚</li>
<li>é£æ ¼ä¸€è‡´æ€§æé«˜äº†7.9%ï¼Œè¯­éŸ³è´¨é‡æé«˜äº†15.5%ã€‚</li>
<li>æ•°æ®é›†ã€æ¨¡å‹å’Œä»£ç å·²å…¬å¼€å‘å¸ƒåœ¨GitHubä¸Šï¼Œæ–¹ä¾¿æœªæ¥ç ”ç©¶ä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04713">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-4ac46230d19bb662be97788a5b70bcd6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760757933&auth_key=1760757933-0-0-33758c48078fbe6b098b9706b5ceda4b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-57991546a7d82cfd76485cf5dcf57221~resize:0:q75.jpg?source=1f5c5e47&expiration=1760757941&auth_key=1760757941-0-0-87008e5d29d50d71e7bada1036625ddd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-aa1dcc99f5573f19f79b44178d7ab1bd~resize:0:q75.jpg?source=1f5c5e47&expiration=1760757948&auth_key=1760757948-0-0-807b1b626a5f2bd01d1ec47670c11765&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7e7825b3f0220f097e128e2eddfad5ac~resize:0:q75.jpg?source=1f5c5e47&expiration=1760757955&auth_key=1760757955-0-0-b289815c7ebf094bf7176276161ae635&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="OpenOmni-Advancing-Open-Source-Omnimodal-Large-Language-Models-with-Progressive-Multimodal-Alignment-and-Real-Time-Self-Aware-Emotional-Speech-Synthesis"><a href="#OpenOmni-Advancing-Open-Source-Omnimodal-Large-Language-Models-with-Progressive-Multimodal-Alignment-and-Real-Time-Self-Aware-Emotional-Speech-Synthesis" class="headerlink" title="OpenOmni: Advancing Open-Source Omnimodal Large Language Models with   Progressive Multimodal Alignment and Real-Time Self-Aware Emotional Speech   Synthesis"></a>OpenOmni: Advancing Open-Source Omnimodal Large Language Models with   Progressive Multimodal Alignment and Real-Time Self-Aware Emotional Speech   Synthesis</h2><p><strong>Authors:Run Luo, Ting-En Lin, Haonan Zhang, Yuchuan Wu, Xiong Liu, Min Yang, Yongbin Li, Longze Chen, Jiaming Li, Lei Zhang, Xiaobo Xia, Hamid Alinejad-Rokny, Fei Huang</strong></p>
<p>Recent advancements in omnimodal learning have significantly improved understanding and generation across images, text, and speech, yet these developments remain predominantly confined to proprietary models. The lack of high-quality omnimodal datasets and the challenges of real-time emotional speech synthesis have notably hindered progress in open-source research. To address these limitations, we introduce \name, a two-stage training framework that integrates omnimodal alignment and speech generation to develop a state-of-the-art omnimodal large language model. In the alignment phase, a pre-trained speech model undergoes further training on text-image tasks, enabling (near) zero-shot generalization from vision to speech, outperforming models trained on tri-modal datasets. In the speech generation phase, a lightweight decoder is trained on speech tasks with direct preference optimization, enabling real-time emotional speech synthesis with high fidelity. Experiments show that \name surpasses state-of-the-art models across omnimodal, vision-language, and speech-language benchmarks. It achieves a 4-point absolute improvement on OmniBench over the leading open-source model VITA, despite using 5x fewer training samples and a smaller model size (7B vs. 7x8B). Additionally, \name achieves real-time speech generation with &lt;1s latency at non-autoregressive mode, reducing inference time by 5x compared to autoregressive methods, and improves emotion classification accuracy by 7.7% </p>
<blockquote>
<p>è¿‘æœŸå¤šæ¨¡æ€å­¦ä¹ çš„è¿›å±•åœ¨å›¾åƒã€æ–‡æœ¬å’Œè¯­éŸ³çš„ç†è§£ä¸ç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æå‡ï¼Œä½†è¿™äº›å‘å±•ä¸»è¦å±€é™äºä¸“æœ‰æ¨¡å‹ã€‚ç¼ºä¹é«˜è´¨é‡çš„å¤šæ¨¡æ€æ•°æ®é›†ä»¥åŠå®æ—¶æƒ…æ„Ÿè¯­éŸ³åˆæˆçš„æŒ‘æˆ˜æ˜¾è‘—é˜»ç¢äº†å¼€æºç ”ç©¶çš„è¿›å±•ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†åä¸ºXXçš„ä¸¤é˜¶æ®µè®­ç»ƒæ¡†æ¶ï¼Œè¯¥æ¡†æ¶èåˆäº†å¤šæ¨¡æ€å¯¹é½å’Œè¯­éŸ³ç”Ÿæˆï¼Œä»¥å¼€å‘å…ˆè¿›çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ã€‚åœ¨å¯¹é½é˜¶æ®µï¼Œé¢„è®­ç»ƒçš„è¯­éŸ³æ¨¡å‹åœ¨æ–‡æœ¬-å›¾åƒä»»åŠ¡ä¸Šæ¥å—è¿›ä¸€æ­¥è®­ç»ƒï¼Œå®ç°äº†ä»è§†è§‰åˆ°è¯­éŸ³çš„ï¼ˆæ¥è¿‘ï¼‰é›¶æ ·æœ¬æ³›åŒ–ï¼Œè¶…è¶Šäº†é‚£äº›åœ¨ä¸‰å…ƒæ¨¡æ€æ•°æ®é›†ä¸Šè®­ç»ƒçš„æ¨¡å‹ã€‚åœ¨è¯­éŸ³ç”Ÿæˆé˜¶æ®µï¼Œä¸€ä¸ªè½»é‡çº§çš„è§£ç å™¨åœ¨è¯­éŸ³ä»»åŠ¡ä¸Šè¿›è¡Œç›´æ¥åå¥½ä¼˜åŒ–è®­ç»ƒï¼Œèƒ½å¤Ÿå®ç°é«˜ä¿çœŸåº¦çš„å®æ—¶æƒ…æ„Ÿè¯­éŸ³åˆæˆã€‚å®éªŒè¡¨æ˜ï¼ŒXXåœ¨å¤šæ¨¡æ€ã€è§†è§‰è¯­è¨€å’Œè¯­éŸ³è¯­è¨€åŸºå‡†æµ‹è¯•ä¸­å‡è¶…è¶Šäº†æœ€å…ˆè¿›çš„æ¨¡å‹ã€‚å°½ç®¡ä½¿ç”¨äº†è¾ƒå°‘çš„è®­ç»ƒæ ·æœ¬å’Œè¾ƒå°çš„æ¨¡å‹è§„æ¨¡ï¼ˆ7B vs. 7x8Bï¼‰ï¼Œä½†åœ¨OmniBenchä¸Šç›¸å¯¹äºé¢†å…ˆçš„å¼€æºæ¨¡å‹VITAå®ç°äº†4ä¸ªç‚¹çš„ç»å¯¹æ”¹è¿›ã€‚æ­¤å¤–ï¼ŒXXåœ¨éè‡ªå›å½’æ¨¡å¼ä¸‹å®ç°äº†å®æ—¶è¯­éŸ³ç”Ÿæˆï¼Œå»¶è¿Ÿæ—¶é—´å°äº1ç§’ï¼Œä¸è‡ªå›å½’æ–¹æ³•ç›¸æ¯”å‡å°‘äº†5å€çš„æ¨ç†æ—¶é—´ï¼Œå¹¶æé«˜äº†7.7%çš„æƒ…æ„Ÿåˆ†ç±»å‡†ç¡®ç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.04561v6">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸå¤šæ¨¡æ€å­¦ä¹ é¢†åŸŸçš„è¿›å±•åœ¨å›¾åƒã€æ–‡æœ¬å’Œè¯­éŸ³çš„ç†è§£ä¸ç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—æå‡ï¼Œä¸»è¦å±€é™äºä¸“æœ‰æ¨¡å‹ã€‚ç„¶è€Œï¼Œé«˜è´¨é‡å¤šæ¨¡æ€æ•°æ®é›†ç¼ºä¹å’Œå®æ—¶æƒ…æ„Ÿè¯­éŸ³åˆæˆçš„æŒ‘æˆ˜é™åˆ¶äº†å¼€æºç ”ç©¶çš„è¿›å±•ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åä¸º\nameçš„ä¸¤é˜¶æ®µè®­ç»ƒæ¡†æ¶ï¼Œå®ƒé›†æˆäº†å¤šæ¨¡æ€å¯¹é½å’Œè¯­éŸ³ç”Ÿæˆï¼Œä»¥å¼€å‘å…ˆè¿›çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ã€‚ç¬¬ä¸€é˜¶æ®µæ˜¯å¯¹é½ï¼Œé¢„è®­ç»ƒçš„è¯­éŸ³æ¨¡å‹åœ¨æ–‡æœ¬-å›¾åƒä»»åŠ¡ä¸Šè¿›è¡Œè¿›ä¸€æ­¥è®­ç»ƒï¼Œå®ç°äº†ä»è§†è§‰åˆ°è¯­éŸ³çš„é›¶æ ·æœ¬æˆ–å°‘é‡æ ·æœ¬æ³›åŒ–ï¼Œè¶…è¶Šäº†ä»…åœ¨ä¸‰æ¨¡æ€æ•°æ®é›†ä¸Šè®­ç»ƒçš„æ¨¡å‹ã€‚ç¬¬äºŒé˜¶æ®µæ˜¯è¯­éŸ³ç”Ÿæˆï¼Œä½¿ç”¨ç›´æ¥åå¥½ä¼˜åŒ–çš„è½»é‡çº§è§£ç å™¨è¿›è¡Œè¯­éŸ³ä»»åŠ¡è®­ç»ƒï¼Œå®ç°äº†é«˜ä¿çœŸåº¦çš„å®æ—¶æƒ…æ„Ÿè¯­éŸ³åˆæˆã€‚å®éªŒè¡¨æ˜ï¼Œ\nameåœ¨å¤šæ¨¡æ€ã€è§†è§‰è¯­è¨€å’Œè¯­éŸ³è¯­è¨€åŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†ç°æœ‰æŠ€æœ¯æœ€å…ˆè¿›æ¨¡å‹ã€‚ç›¸è¾ƒäºé¢†å…ˆçš„å¼€æºæ¨¡å‹VITAï¼Œå®ƒåœ¨OmniBenchä¸Šå®ç°äº†4ä¸ªç‚¹çš„ç»å¯¹æå‡ï¼Œä¸”ä½¿ç”¨è¾ƒå°‘çš„è®­ç»ƒæ ·æœ¬å’Œæ›´å°çš„æ¨¡å‹è§„æ¨¡ï¼ˆ7Bå¯¹7x8Bï¼‰ã€‚æ­¤å¤–ï¼Œ\nameåœ¨éè‡ªå›å½’æ¨¡å¼ä¸‹å®ç°äº†&lt;1ç§’çš„å»¶è¿Ÿï¼Œæ¨ç†æ—¶é—´å‡å°‘äº†5å€ï¼Œæƒ…æ„Ÿåˆ†ç±»å‡†ç¡®ç‡æé«˜äº†7.7%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å­¦ä¹ åœ¨å›¾åƒã€æ–‡æœ¬å’Œè¯­éŸ³æ–¹é¢çš„ç†è§£å–å¾—è¿›å±•ï¼Œä½†ä¸»è¦å±€é™äºä¸“æœ‰æ¨¡å‹ã€‚</li>
<li>é«˜è´¨é‡å¤šæ¨¡æ€æ•°æ®é›†çš„ç¼ºä¹å’Œå®æ—¶æƒ…æ„Ÿè¯­éŸ³åˆæˆçš„æŒ‘æˆ˜é™åˆ¶äº†å¼€æºç ”ç©¶ã€‚</li>
<li>\nameæ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µè®­ç»ƒæ¡†æ¶ï¼Œé€šè¿‡å¯¹é½å’Œè¯­éŸ³ç”Ÿæˆè§£å†³ä¸Šè¿°æŒ‘æˆ˜ã€‚</li>
<li>å¯¹é½é˜¶æ®µå®ç°äº†ä»è§†è§‰åˆ°è¯­éŸ³çš„é›¶æ ·æœ¬æˆ–å°‘é‡æ ·æœ¬æ³›åŒ–ã€‚</li>
<li>è¯­éŸ³ç”Ÿæˆé˜¶æ®µå®ç°äº†é«˜ä¿çœŸåº¦çš„å®æ—¶æƒ…æ„Ÿè¯­éŸ³åˆæˆã€‚</li>
<li>\nameåœ¨å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°è¶…è¶Šç°æœ‰æŠ€æœ¯æœ€å…ˆè¿›æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.04561">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-b3507563861df960273c091941c3ccc5~resize:0:q75.jpg?source=1f5c5e47&expiration=1760757962&auth_key=1760757962-0-0-bf25b7c2ed2c0aeae7c57f9958ab1b7c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1d8a9e63bb3d30b20f62e65444411f6b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760757969&auth_key=1760757969-0-0-bb8764e7fed3d4bb858f44a21c278cd9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7f1eca1f7fad8582fe84684373af8cac~resize:0:q75.jpg?source=1f5c5e47&expiration=1760757976&auth_key=1760757976-0-0-e3045d96bbc297f553197a0b158572fe&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-595bb46727c5f970cd56721b57c8389d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760757983&auth_key=1760757983-0-0-d4589829c79c58c44071da84ce4fb23e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-28/TTS/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-28/TTS/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/TTS/">
                                    <span class="chip bg-color">TTS</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-28/Interactive/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-21fd2027d8af8a8b6e81953d4f2a5c54~resize:0:q75.jpg?source=1f5c5e47&expiration=1760687685&auth_key=1760687685-0-0-6a28417080bca9b2c3092463978ce82b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="Interactive">
                        
                        <span class="card-title">Interactive</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Interactive æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-28  Thinking While Listening Simple Test Time Scaling For Audio   Classification
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-28
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Interactive/" class="post-category">
                                    Interactive
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Interactive/">
                        <span class="chip bg-color">Interactive</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-28/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-726284912b86b25d60f795829f872ae7~resize:0:q75.jpg?source=1f5c5e47&expiration=1760039233&auth_key=1760039233-0-0-b95d4b93f90b28f121d4f31e093ff28e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-28  Modelling the effect of stellar metallicity on the XUV evolution of   low-mass stars and its impact on exoplanet atmospheres/habitability
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-28
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">31987.3k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
