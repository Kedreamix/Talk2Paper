<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-28  SciReasoner Laying the Scientific Reasoning Ground Across Disciplines">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_LLM/2509.20581v1/page_5_1.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-28
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-02
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    84 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-09-28-æ›´æ–°"><a href="#2025-09-28-æ›´æ–°" class="headerlink" title="2025-09-28 æ›´æ–°"></a>2025-09-28 æ›´æ–°</h1><h2 id="SciReasoner-Laying-the-Scientific-Reasoning-Ground-Across-Disciplines"><a href="#SciReasoner-Laying-the-Scientific-Reasoning-Ground-Across-Disciplines" class="headerlink" title="SciReasoner: Laying the Scientific Reasoning Ground Across Disciplines"></a>SciReasoner: Laying the Scientific Reasoning Ground Across Disciplines</h2><p><strong>Authors:Yizhou Wang, Chen Tang, Han Deng, Jiabei Xiao, Jiaqi Liu, Jianyu Wu, Jun Yao, Pengze Li, Encheng Su, Lintao Wang, Guohang Zhuang, Yuchen Ren, Ben Fei, Ming Hu, Xin Chen, Dongzhan Zhou, Junjun He, Xiangyu Yue, Zhenfei Yin, Jiamin Wu, Qihao Zheng, Yuhao Zhou, Huihui Xu, Chenglong Ma, Yan Lu, Wenlong Zhang, Chunfeng Song, Philip Torr, Shixiang Tang, Xinzhu Ma, Wanli Ouyang, Lei Bai</strong></p>
<p>We present a scientific reasoning foundation model that aligns natural language with heterogeneous scientific representations. The model is pretrained on a 206B-token corpus spanning scientific text, pure sequences, and sequence-text pairs, then aligned via SFT on 40M instructions, annealed cold-start bootstrapping to elicit long-form chain-of-thought, and reinforcement learning with task-specific reward shaping, which instills deliberate scientific reasoning. It supports four capability families, covering up to 103 tasks across workflows: (i) faithful translation between text and scientific formats, (ii) text&#x2F;knowledge extraction, (iii) property prediction, (iv) property classification, (v) unconditional and conditional sequence generation and design. Compared with specialist systems, our approach broadens instruction coverage, improves cross-domain generalization, and enhances fidelity. We detail data curation and training and show that cross-discipline learning strengthens transfer and downstream reliability. The model, instruct tuning datasets and the evaluation code are open-sourced at <a target="_blank" rel="noopener" href="https://huggingface.co/SciReason">https://huggingface.co/SciReason</a> and <a target="_blank" rel="noopener" href="https://github.com/open-sciencelab/SciReason">https://github.com/open-sciencelab/SciReason</a>. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç§‘å­¦æ¨ç†åŸºç¡€æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å°†è‡ªç„¶è¯­è¨€ä¸å¼‚è´¨ç§‘å­¦è¡¨ç¤ºå¯¹é½ã€‚è¯¥æ¨¡å‹åœ¨åŒ…å«ç§‘å­¦æ–‡æœ¬ã€çº¯åºåˆ—å’Œåºåˆ—æ–‡æœ¬å¯¹çš„206Bæ ‡è®°è¯­æ–™åº“ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œç„¶åé€šè¿‡40MæŒ‡ä»¤è¿›è¡ŒSFTå¯¹é½ï¼Œé‡‡ç”¨å†·å¯åŠ¨å¼•å¯¼æ³•è¿›è¡Œé•¿æœŸæ€è€ƒé“¾çš„å¼•å¯¼ï¼Œå¹¶ç»“åˆä»»åŠ¡ç‰¹å®šå¥–åŠ±å½¢çŠ¶è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼Œä»è€ŒçŒè¾“æœ‰æ„è¯†çš„ç§‘å­¦æ¨ç†ã€‚å®ƒæ”¯æŒå››ä¸ªèƒ½åŠ›å®¶æ—ï¼Œæ¶µç›–å¤šè¾¾103ä¸ªå·¥ä½œæµç¨‹ä»»åŠ¡ï¼ŒåŒ…æ‹¬ï¼š(i)æ–‡æœ¬ä¸ç§‘å­¦æ ¼å¼ä¹‹é—´çš„å¿ å®ç¿»è¯‘ï¼Œ(ii)æ–‡æœ¬&#x2F;çŸ¥è¯†æå–ï¼Œ(iii)å±æ€§é¢„æµ‹ï¼Œ(iv)å±æ€§åˆ†ç±»ï¼Œ(v)æ— æ¡ä»¶å’Œæ¡ä»¶åºåˆ—ç”Ÿæˆå’Œè®¾è®¡ã€‚ä¸ä¸“ç”¨ç³»ç»Ÿç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ‰©å¤§äº†æŒ‡ä»¤è¦†ç›–èŒƒå›´ï¼Œæé«˜äº†è·¨åŸŸæ³›åŒ–èƒ½åŠ›ï¼Œå¹¶å¢å¼ºäº†ä¿çœŸåº¦ã€‚æˆ‘ä»¬è¯¦ç»†ä»‹ç»äº†æ•°æ®æ•´ç†å’Œè®­ç»ƒè¿‡ç¨‹ï¼Œå¹¶è¡¨æ˜è·¨å­¦ç§‘å­¦ä¹ åŠ å¼ºäº†è¿ç§»å’Œä¸‹æ¸¸å¯é æ€§ã€‚è¯¥æ¨¡å‹ã€æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†å’Œè¯„ä¼°ä»£ç å·²åœ¨<a target="_blank" rel="noopener" href="https://huggingface.co/SciReason%E5%92%8Chttps://github.com/open-sciencelab/SciReason%E4%B8%8A%E5%BC%80%E6%BA%90%E3%80%82">https://huggingface.co/SciReasonå’Œhttps://github.com/open-sciencelab/SciReasonä¸Šå¼€æºã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.21320v1">PDF</a> technical report</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ¨¡å‹é€šè¿‡ç§‘å­¦æ¨ç†åŸºç¡€å°†è‡ªç„¶è¯­è¨€ä¸å¼‚è´¨ç§‘å­¦è¡¨ç¤ºç›¸ç»“åˆï¼Œé¢„è®­ç»ƒåœ¨åŒ…å«ç§‘å­¦æ–‡æœ¬ã€çº¯åºåˆ—å’Œåºåˆ—æ–‡æœ¬å¯¹çš„å¤§å‹è¯­æ–™åº“ä¸Šï¼Œé€šè¿‡SFTå¯¹é½4åƒä¸‡æŒ‡ä»¤ï¼Œé‡‡ç”¨é€€ç«å†·å¯åŠ¨å¼•å¯¼æ¿€å‘é•¿å½¢å¼æ€ç»´é“¾ï¼Œå¹¶ç»“åˆä»»åŠ¡ç‰¹å®šå¥–åŠ±å½¢çŠ¶è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼ŒåŸ¹å…»äº†æœ‰æ„çš„ç§‘å­¦æ¨ç†èƒ½åŠ›ã€‚å®ƒæ”¯æŒå››ç§èƒ½åŠ›å®¶æ—ï¼Œæ¶µç›–è¶…è¿‡ä¸€ä¸‡é¡¹ä»»åŠ¡ï¼šæ–‡æœ¬ä¸ç§‘å­¦æ ¼å¼ä¹‹é—´çš„å¿ å®ç¿»è¯‘ã€æ–‡æœ¬&#x2F;çŸ¥è¯†æå–ã€å±æ€§é¢„æµ‹ã€å±æ€§åˆ†ç±»ã€æ— æ¡ä»¶å’Œæœ‰æ¡ä»¶çš„åºåˆ—ç”Ÿæˆå’Œè®¾è®¡ã€‚ç›¸è¾ƒäºä¸“ä¸šç³»ç»Ÿï¼Œè¯¥æ–¹æ³•æ‰©å¤§äº†æŒ‡ä»¤è¦†ç›–åº¦ï¼Œæé«˜äº†è·¨åŸŸæ³›åŒ–èƒ½åŠ›ï¼Œå¹¶æé«˜äº†ä¿çœŸåº¦ã€‚æœ¬æ–‡è¯¦ç»†æè¿°äº†æ•°æ®æ”¶é›†å’Œè®­ç»ƒè¿‡ç¨‹ï¼Œå¹¶å±•ç¤ºäº†è·¨å­¦ç§‘å­¦ä¹ å¦‚ä½•å¢å¼ºè¿ç§»å’Œä¸‹æ¸¸å¯é æ€§ã€‚æ¨¡å‹å’Œè¯„ä¼°ä»£ç å·²åœ¨Hugging Faceå’ŒGitHubä¸Šå¼€æºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¨¡å‹ç»“åˆäº†è‡ªç„¶è¯­è¨€ä¸å¼‚è´¨ç§‘å­¦è¡¨ç¤ºã€‚</li>
<li>é¢„è®­ç»ƒåœ¨æ¶µç›–ç§‘å­¦æ–‡æœ¬ã€çº¯åºåˆ—å’Œåºåˆ—æ–‡æœ¬å¯¹çš„å¤§å‹è¯­æ–™åº“ä¸Šè¿›è¡Œã€‚</li>
<li>é€šè¿‡SFTå¯¹é½æŒ‡ä»¤ï¼Œæ¿€å‘é•¿å½¢å¼æ€ç»´é“¾ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ é€šè¿‡ä»»åŠ¡ç‰¹å®šå¥–åŠ±å½¢çŠ¶åŸ¹å…»ç§‘å­¦æ¨ç†èƒ½åŠ›ã€‚</li>
<li>æ”¯æŒå¤šç§ä»»åŠ¡ï¼ŒåŒ…æ‹¬ç¿»è¯‘ã€æ–‡æœ¬&#x2F;çŸ¥è¯†æå–ã€å±æ€§é¢„æµ‹å’Œåˆ†ç±»ç­‰ã€‚</li>
<li>æ¨¡å‹ç›¸æ¯”ä¸“ä¸šç³»ç»Ÿï¼Œæ‰©å¤§äº†æŒ‡ä»¤è¦†ç›–åº¦ï¼Œæé«˜æ³›åŒ–å’Œä¿çœŸåº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.21320">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_LLM/2509.21320v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_LLM/2509.21320v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_LLM/2509.21320v1/page_3_0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="SAGE-A-Realistic-Benchmark-for-Semantic-Understanding"><a href="#SAGE-A-Realistic-Benchmark-for-Semantic-Understanding" class="headerlink" title="SAGE: A Realistic Benchmark for Semantic Understanding"></a>SAGE: A Realistic Benchmark for Semantic Understanding</h2><p><strong>Authors:Samarth Goel, Reagan J. Lee, Kannan Ramchandran</strong></p>
<p>As large language models (LLMs) achieve strong performance on traditional benchmarks, there is an urgent need for more challenging evaluation frameworks that probe deeper aspects of semantic understanding. We introduce SAGE (Semantic Alignment &amp; Generalization Evaluation), a rigorous benchmark designed to assess both embedding models and similarity metrics across five categories: Human Preference Alignment, Transformation Robustness, Information Sensitivity, Clustering Performance, and Retrieval Robustness. Unlike existing benchmarks that focus on isolated capabilities, SAGE evaluates semantic understanding through adversarial conditions, noisy transformations, and nuanced human judgment tasks across 30+ datasets. Our comprehensive evaluation of 9 embedding models and classical metrics reveals significant performance gaps, with no single approach excelling across all dimensions. For instance, while state-of-the-art embedding models like OpenAIâ€™s text-embedding-3-large dominate in aligning with human preferences (0.682 vs. 0.591 for the best classical metric), they are significantly outperformed by classical metrics on information sensitivity tasks, where Jaccard Similarity achieves a score of 0.905 compared to the top embedding score of 0.794. SAGE further uncovers critical trade-offs: OpenAIâ€™s text-embedding-3-small achieves the highest clustering performance (0.483) but demonstrates extreme brittleness with the lowest robustness score (0.011). SAGE exposes critical limitations in current semantic understanding capabilities and provides a more realistic assessment of model robustness for real-world deployment. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä¼ ç»ŸåŸºå‡†æµ‹è¯•ä¸Šå–å¾—å¼ºåŠ²è¡¨ç°ï¼Œæ€¥éœ€æ›´å…·æŒ‘æˆ˜æ€§çš„è¯„ä¼°æ¡†æ¶æ¥æ¢ç©¶è¯­ä¹‰ç†è§£çš„æ›´æ·±å±‚æ¬¡æ–¹é¢ã€‚æˆ‘ä»¬æ¨å‡ºäº†SAGEï¼ˆè¯­ä¹‰å¯¹é½ä¸æ³›åŒ–è¯„ä¼°ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸¥æ ¼çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°åµŒå…¥æ¨¡å‹å’Œç›¸ä¼¼æ€§åº¦é‡åœ¨äº”ä¸ªç±»åˆ«ä¸­çš„è¡¨ç°ï¼šäººç±»åå¥½å¯¹é½ã€è½¬æ¢ç¨³å¥æ€§ã€ä¿¡æ¯æ•æ„Ÿæ€§ã€èšç±»æ€§èƒ½å’Œæ£€ç´¢ç¨³å¥æ€§ã€‚ä¸ç°æœ‰çš„ä¸»è¦é›†ä¸­åœ¨å­¤ç«‹èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ä¸åŒï¼ŒSAGEé€šè¿‡å¯¹æŠ—æ€§æ¡ä»¶ã€å™ªå£°è½¬æ¢å’Œç²¾ç»†çš„äººç±»åˆ¤æ–­ä»»åŠ¡åœ¨30å¤šä¸ªæ•°æ®é›†ä¸Šè¯„ä¼°è¯­ä¹‰ç†è§£ã€‚æˆ‘ä»¬å¯¹9ç§åµŒå…¥æ¨¡å‹å’Œç»å…¸åº¦é‡çš„å…¨é¢è¯„ä¼°æ˜¾ç¤ºï¼Œå­˜åœ¨æ˜¾è‘—çš„æ€§èƒ½å·®è·ï¼Œæ²¡æœ‰ä»»ä½•å•ä¸€æ–¹æ³•èƒ½åœ¨æ‰€æœ‰ç»´åº¦ä¸Šéƒ½è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œè™½ç„¶æœ€å…ˆè¿›çš„åµŒå…¥æ¨¡å‹å¦‚OpenAIçš„text-embedding-3-largeåœ¨äººç±»åå¥½å¯¹é½æ–¹é¢å æ®ä¸»å¯¼åœ°ä½ï¼ˆ0.682åˆ†å¯¹æ¯”æœ€å¥½çš„ç»å…¸åº¦é‡åªæœ‰0.591åˆ†ï¼‰ï¼Œä½†åœ¨ä¿¡æ¯æ•æ„Ÿæ€§ä»»åŠ¡ä¸Šå´è¢«ç»å…¸åº¦é‡æ˜¾è‘—è¶…è¶Šï¼ŒJaccardç›¸ä¼¼åº¦å¾—åˆ†é«˜è¾¾0.905åˆ†ï¼Œè€Œé¡¶çº§åµŒå…¥å¾—åˆ†ä»…ä¸º0.794åˆ†ã€‚SAGEè¿˜æ­ç¤ºäº†å…³é”®çš„æƒè¡¡ï¼šOpenAIçš„text-embedding-3-smallè™½ç„¶èšç±»æ€§èƒ½æœ€é«˜ï¼ˆ0.483åˆ†ï¼‰ï¼Œä½†åœ¨ç¨³å¥æ€§æ–¹é¢è¡¨ç°æå·®ï¼Œå¾—åˆ†æœ€ä½ï¼ˆåªæœ‰0.011åˆ†ï¼‰ã€‚SAGEæ­ç¤ºäº†å½“å‰è¯­ä¹‰ç†è§£èƒ½åŠ›çš„å…³é”®å±€é™æ€§ï¼Œå¹¶ä¸ºæ¨¡å‹åœ¨ç°å®ä¸–ç•Œçš„éƒ¨ç½²æä¾›äº†æ›´ç°å®çš„ç¨³å¥æ€§è¯„ä¼°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.21310v1">PDF</a> 39th Conference on Neural Information Processing Systems (NeurIPS   2025) Workshop: Evaluating the Evolving LLM Lifecycle: Benchmarks, Emergent   Abilities, and Scaling</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä¼ ç»ŸåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½†äºŸéœ€æ›´å…·æŒ‘æˆ˜æ€§çš„è¯„ä¼°æ¡†æ¶æ¥æ·±å…¥æ¢ç©¶å…¶è¯­ä¹‰ç†è§£çš„èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºäº†SAGEï¼ˆè¯­ä¹‰å¯¹é½ä¸æ³›åŒ–è¯„ä¼°ï¼‰åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°åµŒå…¥æ¨¡å‹å’Œç›¸ä¼¼æ€§åº¦é‡åœ¨äº”ä¸ªç±»åˆ«ä¸­çš„è¡¨ç°ï¼šäººç±»åå¥½å¯¹é½ã€è½¬æ¢é²æ£’æ€§ã€ä¿¡æ¯æ•æ„Ÿæ€§ã€èšç±»æ€§èƒ½å’Œæ£€ç´¢é²æ£’æ€§ã€‚SAGEé€šè¿‡å¯¹æŠ—æ¡ä»¶ã€å™ªå£°è½¬æ¢å’Œç²¾ç»†çš„äººç±»åˆ¤æ–­ä»»åŠ¡ï¼Œåœ¨30å¤šä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œè¯­ä¹‰ç†è§£è¯„ä¼°ã€‚æˆ‘ä»¬å¯¹9ç§åµŒå…¥æ¨¡å‹å’Œä¼ ç»Ÿåº¦é‡æ–¹æ³•è¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œå‘ç°å­˜åœ¨æ˜¾è‘—çš„æ€§èƒ½å·®è·ï¼Œæ²¡æœ‰ä¸€ç§æ–¹æ³•èƒ½åœ¨æ‰€æœ‰ç»´åº¦ä¸Šéƒ½è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰éœ€è¦æ›´å…·æŒ‘æˆ˜æ€§çš„è¯„ä¼°æ¡†æ¶ä»¥æ·±å…¥è¯„ä¼°å…¶è¯­ä¹‰ç†è§£èƒ½åŠ›ã€‚</li>
<li>SAGEåŸºå‡†æµ‹è¯•æ—¨åœ¨å…¨é¢è¯„ä¼°åµŒå…¥æ¨¡å‹å’Œç›¸ä¼¼æ€§åº¦é‡åœ¨å¤šä¸ªæ–¹é¢çš„è¡¨ç°ã€‚</li>
<li>SAGEé€šè¿‡å¯¹æŠ—æ¡ä»¶ã€å™ªå£°è½¬æ¢å’Œç²¾ç»†çš„äººç±»åˆ¤æ–­ä»»åŠ¡æ¥è¯„ä¼°è¯­ä¹‰ç†è§£ã€‚</li>
<li>ç°æœ‰åµŒå…¥æ¨¡å‹åœ¨ç‰¹å®šé¢†åŸŸè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½†åœ¨å…¶ä»–é¢†åŸŸå­˜åœ¨æ˜æ˜¾ä¸è¶³ã€‚</li>
<li>ä¼ ç»Ÿåº¦é‡æ–¹æ³•åœ¨ä¿¡æ¯æ•æ„Ÿæ€§ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè¾ƒå¼ºæ€§èƒ½ã€‚</li>
<li>SAGEæ­ç¤ºäº†æ¨¡å‹é—´çš„æƒè¡¡ï¼šæŸäº›æ¨¡å‹åœ¨èšç±»æ€§èƒ½ä¸Šè¡¨ç°ä¼˜ç§€ï¼Œä½†åœ¨é²æ£’æ€§æ–¹é¢å­˜åœ¨ç¼ºé™·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.21310">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_LLM/2509.21310v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_LLM/2509.21310v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="VC-Agent-An-Interactive-Agent-for-Customized-Video-Dataset-Collection"><a href="#VC-Agent-An-Interactive-Agent-for-Customized-Video-Dataset-Collection" class="headerlink" title="VC-Agent: An Interactive Agent for Customized Video Dataset Collection"></a>VC-Agent: An Interactive Agent for Customized Video Dataset Collection</h2><p><strong>Authors:Yidan Zhang, Mutian Xu, Yiming Hao, Kun Zhou, Jiahao Chang, Xiaoqiang Liu, Pengfei Wan, Hongbo Fu, Xiaoguang Han</strong></p>
<p>Facing scaling laws, video data from the internet becomes increasingly important. However, collecting extensive videos that meet specific needs is extremely labor-intensive and time-consuming. In this work, we study the way to expedite this collection process and propose VC-Agent, the first interactive agent that is able to understand usersâ€™ queries and feedback, and accordingly retrieve&#x2F;scale up relevant video clips with minimal user input. Specifically, considering the user interface, our agent defines various user-friendly ways for the user to specify requirements based on textual descriptions and confirmations. As for agent functions, we leverage existing multi-modal large language models to connect the userâ€™s requirements with the video content. More importantly, we propose two novel filtering policies that can be updated when user interaction is continually performed. Finally, we provide a new benchmark for personalized video dataset collection, and carefully conduct the user study to verify our agentâ€™s usage in various real scenarios. Extensive experiments demonstrate the effectiveness and efficiency of our agent for customized video dataset collection. Project page: <a target="_blank" rel="noopener" href="https://allenyidan.github.io/vcagent_page/">https://allenyidan.github.io/vcagent_page/</a>. </p>
<blockquote>
<p>é¢å¯¹æ—¥ç›Šå¢é•¿çš„æ³•å¾‹è§„æ¨¡ï¼Œäº’è”ç½‘è§†é¢‘æ•°æ®å˜å¾—æ„ˆå‘é‡è¦ã€‚ç„¶è€Œï¼Œæ”¶é›†æ»¡è¶³ç‰¹å®šéœ€æ±‚çš„å¤§é‡è§†é¢‘æä¸ºè€—è´¹äººåŠ›å’Œæ—¶é—´ã€‚åœ¨æ­¤å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†å¦‚ä½•åŠ é€Ÿè¿™ä¸€æ”¶é›†è¿‡ç¨‹ï¼Œå¹¶æå‡ºäº†VC-Agentï¼Œè¿™æ˜¯ä¸€ä¸ªé¦–æ¬¾äº¤äº’å¼ä»£ç†ï¼Œèƒ½å¤Ÿç†è§£ç”¨æˆ·çš„æŸ¥è¯¢å’Œåé¦ˆï¼Œå¹¶æ®æ­¤ä»¥æœ€å°çš„ç”¨æˆ·è¾“å…¥æ£€ç´¢æˆ–æ‰©å……ç›¸å…³è§†é¢‘ç‰‡æ®µã€‚å…·ä½“è€Œè¨€ï¼Œå…³äºç”¨æˆ·ç•Œé¢ï¼Œæˆ‘ä»¬çš„ä»£ç†å®šä¹‰äº†å„ç§ç”¨æˆ·å‹å¥½çš„æ–¹å¼ï¼Œè®©ç”¨æˆ·åŸºäºæ–‡æœ¬æè¿°å’Œç¡®è®¤æ¥æŒ‡å®šè¦æ±‚ã€‚è‡³äºä»£ç†åŠŸèƒ½ï¼Œæˆ‘ä»¬åˆ©ç”¨ç°æœ‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹å°†ç”¨æˆ·çš„è¦æ±‚ä¸è§†é¢‘å†…å®¹è¿æ¥èµ·æ¥ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸¤ç§å¯åœ¨ç”¨æˆ·äº¤äº’æŒç»­è¿›è¡Œæ—¶æ›´æ–°çš„æ–°å‹è¿‡æ»¤ç­–ç•¥ã€‚æœ€åï¼Œæˆ‘ä»¬ä¸ºä¸ªæ€§åŒ–è§†é¢‘æ•°æ®é›†æ”¶é›†æä¾›äº†ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼Œå¹¶é€šè¿‡ç”¨æˆ·ç ”ç©¶ä»”ç»†éªŒè¯äº†æˆ‘ä»¬çš„ä»£ç†åœ¨å„ç§å®é™…åœºæ™¯ä¸­çš„åº”ç”¨ã€‚å¤§é‡å®éªŒè¯æ˜äº†æˆ‘ä»¬çš„ä»£ç†åœ¨å®šåˆ¶è§†é¢‘æ•°æ®é›†æ”¶é›†ä¸­çš„æœ‰æ•ˆæ€§å’Œæ•ˆç‡ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://allenyidan.github.io/vcagent_page/%E3%80%82">https://allenyidan.github.io/vcagent_page/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.21291v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://allenyidan.github.io/vcagent_page/">https://allenyidan.github.io/vcagent_page/</a></p>
<p><strong>æ€»ç»“</strong><br>    VC-Agentä½œä¸ºä¸€ç§å…¨æ–°çš„äº¤äº’å¼ä»£ç†ï¼Œæ—¨åœ¨åŠ é€Ÿè§†é¢‘æ•°æ®æ”¶é›†è¿‡ç¨‹ã€‚å®ƒå¯ç†è§£ç”¨æˆ·éœ€æ±‚å¹¶æ®æ­¤æ£€ç´¢ç›¸å…³è§†é¢‘ç‰‡æ®µã€‚é€šè¿‡åˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹è¿æ¥ç”¨æˆ·éœ€æ±‚ä¸è§†é¢‘å†…å®¹ï¼Œæå‡ºä¸¤ç§å¯éšç”¨æˆ·äº’åŠ¨æŒç»­æ›´æ–°çš„è¿‡æ»¤æ”¿ç­–ï¼Œå¹¶å»ºç«‹ä¸ªæ€§åŒ–è§†é¢‘æ•°æ®é›†æ”¶é›†çš„æ–°åŸºå‡†ã€‚å®éªŒè¯æ˜å…¶åœ¨å„ç§å®é™…åœºæ™¯ä¸­æ•ˆæœæ˜¾è‘—ä¸”é«˜æ•ˆã€‚æ›´å¤šè¯¦æƒ…å‚è§é¡¹ç›®é¡µé¢ï¼š[<a target="_blank" rel="noopener" href="https://allenyidan.github.io/vcagent_page/]">https://allenyidan.github.io/vcagent_page/]</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>VC-Agentæ˜¯é¦–ä¸ªèƒ½å¤Ÿç†è§£å’Œå“åº”ç”¨æˆ·æŸ¥è¯¢å’Œåé¦ˆçš„äº¤äº’å¼ä»£ç†ï¼Œç®€åŒ–äº†è§†é¢‘æ•°æ®æ”¶é›†è¿‡ç¨‹ã€‚</li>
<li>è¯¥ä»£ç†é€šè¿‡æœ€å°åŒ–ç”¨æˆ·è¾“å…¥ï¼Œå®ç°ç›¸å…³è§†é¢‘ç‰‡æ®µçš„æ£€ç´¢å’Œæ‰©å±•ã€‚</li>
<li>åˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå°†ç”¨æˆ·éœ€æ±‚ä¸è§†é¢‘å†…å®¹ç›¸è¿æ¥ã€‚</li>
<li>VC-Agentæä¾›ä¸¤ç§å¯éšç”¨æˆ·äº’åŠ¨æ›´æ–°çš„è¿‡æ»¤æ”¿ç­–ã€‚</li>
<li>å»ºç«‹äº†ä¸ªæ€§åŒ–è§†é¢‘æ•°æ®é›†æ”¶é›†çš„æ–°åŸºå‡†ã€‚</li>
<li>é€šè¿‡ç”¨æˆ·ç ”ç©¶éªŒè¯äº†VC-Agentåœ¨å„ç§å®é™…åœºæ™¯ä¸­çš„ä½¿ç”¨æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.21291">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_LLM/2509.21291v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_LLM/2509.21291v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Data-Centric-Elastic-Pipeline-Parallelism-for-Efficient-Long-Context-LLM-Training"><a href="#Data-Centric-Elastic-Pipeline-Parallelism-for-Efficient-Long-Context-LLM-Training" class="headerlink" title="Data-Centric Elastic Pipeline Parallelism for Efficient Long-Context LLM   Training"></a>Data-Centric Elastic Pipeline Parallelism for Efficient Long-Context LLM   Training</h2><p><strong>Authors:Shiju Wang, Yujie Wang, Ao Sun, Fangcheng Fu, Zijian Zhu, Bin Cui, Xu Han, Kaisheng Ma</strong></p>
<p>Long context training is crucial for LLMâ€™s context extension. Existing schemes, such as sequence parallelism, incur substantial communication overhead. Pipeline parallelism (PP) reduces this cost, but its effectiveness hinges on partitioning granularity. Batch-level PP dividing input samples exhibits high memory consumption in long-context scenario, whereas token-level PP splitting sequences into slices alleviates memory overhead but may incur hardware under-utilization. This trade-off motivates adaptively selecting PP granularity to match resource and workload characteristics. Moreover, sequence length distribution of the real-world dataset exhibits skewness, posing a challenge on PPâ€™s workload balance and efficient scheduling. Current static PP scheduling methods overlook the variance of sequence length, leading to suboptimal performance. In this paper, we propose Elastic Pipeline Parallelism (EPP) that orchestrates token-level PP and batch-level PP to adapt to resource and workload heterogeneity. We build InfiniPipe, a distributed training system that unleashes the potential of EPP via (1) a resource-aware and workload-balanced sequence processor that splits long sequences and packs short ones; and (2) a co-optimization methodology that jointly optimizes pipeline schedule and gradient checkpointing via a mechanism named stage-aware chunk-level adaptive checkpointing. Comprehensive experiments demonstrate that InfiniPipe achieves a 1.69x speedup over state-of-the-art systems. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä¸Šä¸‹æ–‡æ‰©å±•ä¸­ï¼Œé•¿ä¸Šä¸‹æ–‡è®­ç»ƒè‡³å…³é‡è¦ã€‚ç°æœ‰çš„æ–¹æ¡ˆï¼Œå¦‚åºåˆ—å¹¶è¡Œæ€§ï¼Œä¼šäº§ç”Ÿå¤§é‡çš„é€šä¿¡å¼€é”€ã€‚ç®¡é“å¹¶è¡Œæ€§ï¼ˆPPï¼‰é™ä½äº†è¿™ä¸€æˆæœ¬ï¼Œä½†å…¶æœ‰æ•ˆæ€§å–å†³äºåˆ†åŒºç²’åº¦ã€‚åœ¨é•¿ä¸Šä¸‹æ–‡åœºæ™¯ä¸­ï¼Œæ‰¹å¤„ç†çº§åˆ«çš„PPå¯¹è¾“å…¥æ ·æœ¬è¿›è¡Œåˆ’åˆ†ä¼šå¯¼è‡´è¾ƒé«˜çš„å†…å­˜æ¶ˆè€—ï¼Œè€ŒåŸºäºæ ‡è®°çš„PPé€šè¿‡å°†åºåˆ—åˆ‡ç‰‡å‡è½»äº†å†…å­˜å¼€é”€ï¼Œä½†å¯èƒ½ä¼šé€ æˆç¡¬ä»¶ä½¿ç”¨ä¸è¶³ã€‚è¿™ç§æƒè¡¡ä¿ƒä½¿æˆ‘ä»¬æ ¹æ®èµ„æºå’Œå·¥ä½œè´Ÿè½½ç‰¹æ€§è‡ªé€‚åº”é€‰æ‹©PPç²’åº¦ã€‚æ­¤å¤–ï¼Œç°å®ä¸–ç•Œæ•°æ®é›†åºåˆ—é•¿åº¦åˆ†å¸ƒçš„ä¸å¹³è¡¡æ€§å¯¹PPçš„å·¥ä½œè´Ÿè½½å¹³è¡¡å’Œé«˜æ•ˆè°ƒåº¦æå‡ºäº†æŒ‘æˆ˜ã€‚å½“å‰çš„é™æ€PPè°ƒåº¦æ–¹æ³•å¿½ç•¥äº†åºåˆ—é•¿åº¦çš„å˜åŒ–ï¼Œå¯¼è‡´æ€§èƒ½ä¸ä½³ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†å¼¹æ€§ç®¡é“å¹¶è¡Œæ€§ï¼ˆEPPï¼‰ï¼Œå®ƒç»“åˆäº†åŸºäºæ ‡è®°çš„PPå’Œæ‰¹å¤„ç†çº§åˆ«çš„PPï¼Œä»¥é€‚åº”èµ„æºå’Œå·¥ä½œè´Ÿè½½çš„å¼‚è´¨æ€§ã€‚æˆ‘ä»¬å»ºç«‹äº†åˆ†å¸ƒå¼è®­ç»ƒç³»ç»ŸInfiniPipeï¼Œå®ƒé€šè¿‡ä»¥ä¸‹ä¸¤ä¸ªæ–¹é¢é‡Šæ”¾EPPçš„æ½œåŠ›ï¼šï¼ˆ1ï¼‰ä¸€ç§èµ„æºæ„ŸçŸ¥å’Œå·¥ä½œè´Ÿè½½å¹³è¡¡åºåˆ—å¤„ç†å™¨ï¼Œç”¨äºåˆ†å‰²é•¿åºåˆ—å¹¶æ‰“åŒ…çŸ­åºåˆ—ï¼›ï¼ˆ2ï¼‰ä¸€ç§ååŒä¼˜åŒ–æ–¹æ³•ï¼Œé€šè¿‡åä¸ºstage-aware chunk-level adaptive checkpointingçš„æœºåˆ¶è”åˆä¼˜åŒ–ç®¡é“è°ƒåº¦å’Œæ¢¯åº¦æ£€æŸ¥ç‚¹ã€‚ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒInfiniPipeç›¸è¾ƒäºæœ€æ–°ç³»ç»Ÿå®ç°äº†1.69å€çš„é€Ÿåº¦æå‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.21275v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰éœ€è¦ä¸Šä¸‹æ–‡é•¿æœŸè®­ç»ƒä»¥å¢å¼ºæ•ˆæœã€‚ç°æœ‰çš„å¹¶è¡Œå¤„ç†ç­–ç•¥å¦‚åºåˆ—å¹¶è¡Œå¤„ç†ä¼šå¼•å…¥å·¨å¤§çš„é€šä¿¡å¼€é”€ã€‚ç®¡é“å¹¶è¡Œï¼ˆPPï¼‰é™ä½äº†è¿™ä¸€æˆæœ¬ï¼Œä½†å…¶æœ‰æ•ˆæ€§å–å†³äºåˆ†åŒºç²’åº¦é€‰æ‹©ã€‚æ‰¹çº§åˆ«PPåœ¨å…·æœ‰é•¿ä¸Šä¸‹æ–‡çš„æƒ…å†µä¸‹è¡¨ç°å‡ºé«˜å†…å­˜æ¶ˆè€—ï¼Œè€ŒåŸºäºtokençš„PPé€šè¿‡å°†åºåˆ—åˆ‡ç‰‡æ¥ç¼“è§£å†…å­˜å¼€é”€ï¼Œä½†å¯èƒ½å¯¼è‡´ç¡¬ä»¶åˆ©ç”¨ç‡ä¸è¶³ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§è‡ªé€‚åº”é€‰æ‹©PPç²’åº¦çš„æ–¹æ³•ä»¥é€‚åº”èµ„æºå’Œå·¥ä½œè´Ÿè½½ç‰¹æ€§ã€‚æ­¤å¤–ï¼Œç°å®ä¸–ç•Œæ•°æ®é›†åºåˆ—é•¿åº¦åˆ†å¸ƒçš„ä¸å¹³è¡¡æ€§å¯¹PPçš„å·¥ä½œè´Ÿè½½å¹³è¡¡å’Œè°ƒåº¦æ•ˆç‡æå‡ºäº†æŒ‘æˆ˜ã€‚å½“å‰é™æ€PPè°ƒåº¦æ–¹æ³•å¿½ç•¥äº†åºåˆ—é•¿åº¦çš„å˜åŒ–ï¼Œå¯¼è‡´æ€§èƒ½ä¸ä½³ã€‚æœ¬æ–‡æå‡ºäº†å¼¹æ€§ç®¡é“å¹¶è¡Œæ€§ï¼ˆEPPï¼‰ï¼Œé€šè¿‡ç»„åˆåŸºäºtokençš„PPå’Œæ‰¹çº§åˆ«PPæ¥é€‚åº”èµ„æºå’Œå·¥ä½œè´Ÿè½½çš„å¼‚è´¨æ€§ã€‚æ„å»ºäº†ä¸€ä¸ªåˆ†å¸ƒå¼è®­ç»ƒç³»ç»ŸInfiniPipeï¼Œé€šè¿‡èµ„æºæ„ŸçŸ¥å’Œå·¥ä½œè´Ÿè½½å¹³è¡¡åºåˆ—å¤„ç†å™¨æ¥åº”å¯¹é•¿åºåˆ—å¹¶æ‰“åŒ…çŸ­åºåˆ—ï¼›é‡‡ç”¨é˜¶æ®µæ„ŸçŸ¥å—çº§è‡ªé€‚åº”æ£€æŸ¥ç‚¹ç­–ç•¥è¿›è¡Œè”åˆä¼˜åŒ–ç®¡é“è°ƒåº¦å’Œæ¢¯åº¦æ£€æŸ¥ç‚¹ä¼˜åŒ–ç­–ç•¥çš„æ–¹å¼è§£å†³è¯¥æŒ‘æˆ˜ã€‚å®éªŒè¯æ˜ï¼ŒInfiniPipeç›¸è¾ƒäºç°æœ‰ç³»ç»Ÿå–å¾—äº†é«˜è¾¾1.69å€çš„é€Ÿåº¦æå‡ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>LLMçš„é•¿æœŸä¸Šä¸‹æ–‡è®­ç»ƒè‡³å…³é‡è¦ï¼Œä½†ç°æœ‰å¹¶è¡Œè®­ç»ƒæ–¹æ¡ˆå­˜åœ¨é€šä¿¡å¼€é”€é—®é¢˜ã€‚</li>
<li>ç®¡é“å¹¶è¡Œæ€§ï¼ˆPPï¼‰æ˜¯é™ä½é€šä¿¡å¼€é”€çš„æœ‰æ•ˆæ–¹æ³•ï¼Œä½†å…¶æ•ˆæœå—åˆ†åŒºç²’åº¦å½±å“ã€‚</li>
<li>æ‰¹çº§åˆ«å’ŒåŸºäºtokençš„PPå„æœ‰ä¼˜ç¼ºç‚¹ï¼Œéœ€è¦æ ¹æ®èµ„æºå’Œå·¥ä½œè´Ÿè½½ç‰¹æ€§é€‰æ‹©ã€‚</li>
<li>ç°å®ä¸–ç•Œæ•°æ®é›†çš„åºåˆ—é•¿åº¦åˆ†å¸ƒä¸å¹³è¡¡ï¼Œå¯¹PPçš„å·¥ä½œè´Ÿè½½å¹³è¡¡å’Œè°ƒåº¦æ•ˆç‡æ„æˆæŒ‘æˆ˜ã€‚</li>
<li>å½“å‰é™æ€PPè°ƒåº¦æ–¹æ³•å¿½ç•¥åºåˆ—é•¿åº¦å˜åŒ–ï¼Œå¯¼è‡´æ€§èƒ½ä¸ä½³ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†Elastic Pipeline Parallelismï¼ˆEPPï¼‰æ¥é€‚åº”èµ„æºå’Œå·¥ä½œè´Ÿè½½çš„å¼‚è´¨æ€§ï¼Œç»“åˆä½¿ç”¨ä¸¤ç§PPç­–ç•¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.21275">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_LLM/2509.21275v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_LLM/2509.21275v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_LLM/2509.21275v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_LLM/2509.21275v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="SuperOffload-Unleashing-the-Power-of-Large-Scale-LLM-Training-on-Superchips"><a href="#SuperOffload-Unleashing-the-Power-of-Large-Scale-LLM-Training-on-Superchips" class="headerlink" title="SuperOffload: Unleashing the Power of Large-Scale LLM Training on   Superchips"></a>SuperOffload: Unleashing the Power of Large-Scale LLM Training on   Superchips</h2><p><strong>Authors:Xinyu Lian, Masahiro Tanaka, Olatunji Ruwase, Minjia Zhang</strong></p>
<p>The emergence of Superchips represents a significant advancement in next-generation AI hardware. These Superchips employ a tightly coupled heterogeneous architecture that integrates GPU and CPU on the same package, which offers unprecedented computational power. However, there has been scant research investigating how LLM training benefits from this new architecture. In this work, for the first time, we study LLM training solutions based on offloading for Superchips. We observe important differences between Superchips and traditional loosely-coupled GPU-CPU architecture, which necessitate revisiting prevailing assumptions about offloading. Based on that, we present SuperOffload, a Superchip-centric offloading system that simultaneously uses Hopper GPU, Grace CPU, and NVLink-C2C interconnect more efficiently. SuperOffload accomplishes this via a combination of techniques, such as adaptive weight offloading, bucketization repartitioning, Superchip-aware casting, speculative execution, and a highly optimized Adam optimizer for Grace CPUs. Our evaluation of SuperOffload on NVIDIA GH200 demonstrates up to 2.5x throughput improvement compared to state-of-the-art offloading-based systems, enabling training of up to 25B model on a single Superchip while achieving high training throughput. We also extend SuperOffload with ZeRO-style data parallelism and DeepSpeed-Ulysses sequence parallelism, enabling training of 13B model with sequence lengths up to 1 million tokens on 8 GH200 while achieving 55% MFU. </p>
<blockquote>
<p>è¶…çº§èŠ¯ç‰‡çš„å‡ºç°ä»£è¡¨äº†ä¸‹ä¸€ä»£äººå·¥æ™ºèƒ½ç¡¬ä»¶çš„é‡å¤§è¿›æ­¥ã€‚è¿™äº›è¶…çº§èŠ¯ç‰‡é‡‡ç”¨ç´§å¯†è€¦åˆçš„å¼‚æ„æ¶æ„ï¼Œå°†GPUå’ŒCPUé›†æˆåœ¨åŒä¸€ä¸ªå°è£…ä¸­ï¼Œæä¾›äº†å‰æ‰€æœªæœ‰çš„è®¡ç®—èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå…³äºè¿™ç§æ–°æ¶æ„å¦‚ä½•å—ç›Šäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è®­ç»ƒçš„ç ”ç©¶å¯¥å¯¥æ— å‡ ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é¦–æ¬¡åŸºäºå¸è½½æŠ€æœ¯ç ”ç©¶äº†é’ˆå¯¹è¶…çº§èŠ¯ç‰‡çš„å¤§å‹è¯­è¨€æ¨¡å‹è®­ç»ƒè§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°è¶…çº§èŠ¯ç‰‡ä¸ä¼ ç»Ÿæ¾æ•£è€¦åˆçš„GPU-CPUæ¶æ„ä¹‹é—´çš„é‡å¤§å·®å¼‚ï¼Œè¿™éœ€è¦é‡æ–°å®¡è§†å…³äºå¸è½½çš„ç°æœ‰å‡è®¾ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºäº†ä»¥è¶…çº§èŠ¯ç‰‡ä¸ºä¸­å¿ƒçš„å¸è½½ç³»ç»ŸSuperOffloadï¼Œå®ƒæ›´æœ‰æ•ˆåœ°åŒæ—¶ä½¿ç”¨Hopper GPUã€Grace CPUå’ŒNVLink-C2Cäº’è”ã€‚SuperOffloadé€šè¿‡ç»“åˆå¤šç§æŠ€æœ¯å®ç°è¿™ä¸€ç›®æ ‡ï¼Œå¦‚è‡ªé€‚åº”æƒé‡å¸è½½ã€åˆ†æ¡¶å†åˆ†åŒºã€è¶…çº§èŠ¯ç‰‡æ„ŸçŸ¥è½¬æ¢ã€æ¨æµ‹æ‰§è¡Œä»¥åŠé’ˆå¯¹Grace CPUçš„é«˜åº¦ä¼˜åŒ–çš„Adamä¼˜åŒ–å™¨ã€‚æˆ‘ä»¬åœ¨NVIDIA GH200ä¸Šå¯¹SuperOffloadçš„è¯„ä¼°è¡¨æ˜ï¼Œä¸æœ€æ–°çš„åŸºäºå¸è½½çš„ç³»ç»Ÿç›¸æ¯”ï¼Œå…¶ååé‡æé«˜äº†2.5å€ï¼Œèƒ½å¤Ÿåœ¨å•ä¸ªè¶…çº§èŠ¯ç‰‡ä¸Šè®­ç»ƒé«˜è¾¾25Bçš„æ¨¡å‹ï¼ŒåŒæ—¶å®ç°é«˜è®­ç»ƒååé‡ã€‚æˆ‘ä»¬è¿˜é€šè¿‡ZeROé£æ ¼çš„æ•°æ®å¹¶è¡Œæ€§å’ŒDeepSpeed-Ulyssesåºåˆ—å¹¶è¡Œæ€§æ‰©å±•äº†SuperOffloadï¼Œèƒ½å¤Ÿåœ¨8ä¸ªGH200ä¸Šè®­ç»ƒå…·æœ‰é«˜è¾¾1ç™¾ä¸‡ä»¤ç‰Œåºåˆ—é•¿åº¦çš„13Bæ¨¡å‹ï¼ŒåŒæ—¶å®ç°55%çš„æœ€å¤§åŠŸèƒ½åˆ©ç”¨ç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.21271v1">PDF</a> 16 pages, 15 figures</p>
<p><strong>Summary</strong>ï¼š<br>è¶…çº§èŠ¯ç‰‡çš„å‡ºç°ä»£è¡¨äº†ä¸‹ä¸€ä»£äººå·¥æ™ºèƒ½ç¡¬ä»¶çš„é‡å¤§çªç ´ã€‚æœ¬æ–‡é€šè¿‡å¸è½½æ–¹å¼é¦–æ¬¡ç ”ç©¶è¶…çº§èŠ¯ç‰‡ä¸Šçš„å¤§å‹è¯­è¨€æ¨¡å‹è®­ç»ƒè§£å†³æ–¹æ¡ˆã€‚ç ”ç©¶å‘ç°è¶…çº§èŠ¯ç‰‡ä¸ä¼ ç»Ÿæ¾æ•£è€¦åˆçš„GPU-CPUæ¶æ„ä¹‹é—´å­˜åœ¨é‡è¦å·®å¼‚ï¼Œéœ€è¦é‡æ–°è€ƒè™‘å¸è½½çš„ç°æœ‰å‡è®¾ã€‚å› æ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä»¥è¶…çº§èŠ¯ç‰‡ä¸ºä¸­å¿ƒçš„å¸è½½ç³»ç»ŸSuperOffloadï¼Œé€šè¿‡ä¸€ç³»åˆ—æŠ€æœ¯æ›´é«˜æ•ˆåœ°åˆ©ç”¨Hopper GPUã€Grace CPUå’ŒNVLink-C2Cäº’è”ã€‚SuperOffloadåœ¨NVIDIA GH200ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œä¸æœ€æ–°çš„åŸºäºå¸è½½çš„ç³»ç»Ÿç›¸æ¯”ï¼Œååé‡æé«˜äº†2.5å€ï¼Œèƒ½å¤Ÿåœ¨å•ä¸ªè¶…çº§èŠ¯ç‰‡ä¸Šè®­ç»ƒé«˜è¾¾25Bçš„æ¨¡å‹ï¼ŒåŒæ—¶ä¿æŒè¾ƒé«˜çš„è®­ç»ƒååé‡ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>è¶…çº§èŠ¯ç‰‡ä»£è¡¨ä¸‹ä¸€ä»£AIç¡¬ä»¶çš„é‡å¤§è¿›å±•ï¼Œé‡‡ç”¨ç´§å¯†è€¦åˆçš„å¼‚æ„æ¶æ„ï¼Œé›†æˆGPUå’ŒCPUã€‚</li>
<li>ç›®å‰å¯¹äºè¶…çº§èŠ¯ç‰‡åœ¨å¤§å‹è¯­è¨€æ¨¡å‹è®­ç»ƒä¸­çš„æ•ˆç›Šç ”ç©¶è¾ƒå°‘ã€‚</li>
<li>æœ¬æ–‡é¦–æ¬¡ç ”ç©¶åŸºäºå¸è½½çš„è¶…çº§èŠ¯ç‰‡å¤§å‹è¯­è¨€æ¨¡å‹è®­ç»ƒè§£å†³æ–¹æ¡ˆã€‚</li>
<li>è¶…çº§èŠ¯ç‰‡ä¸ä¼ ç»ŸGPU-CPUæ¶æ„å­˜åœ¨é‡è¦å·®å¼‚ï¼Œéœ€è¦é‡æ–°å®¡è§†å¸è½½çš„ç°æœ‰å‡è®¾ã€‚</li>
<li>æå‡ºSuperOffloadç³»ç»Ÿï¼Œé’ˆå¯¹è¶…çº§èŠ¯ç‰‡è¿›è¡Œä¼˜åŒ–ï¼Œåˆ©ç”¨Hopper GPUã€Grace CPUå’ŒNVLink-C2Cäº’è”ã€‚</li>
<li>SuperOffloadé€šè¿‡ä¸€ç³»åˆ—æŠ€æœ¯å®ç°é«˜æ•ˆè®­ç»ƒï¼Œå¦‚è‡ªé€‚åº”æƒé‡å¸è½½ã€åˆ†æ¡¶é‡åˆ’åˆ†ã€è¶…çº§èŠ¯ç‰‡æ„ŸçŸ¥è½¬æ¢ã€æ¨æµ‹æ‰§è¡Œå’Œä¼˜åŒ–çš„Adamä¼˜åŒ–å™¨ã€‚</li>
<li>åœ¨NVIDIA GH200ä¸Šè¯„ä¼°æ˜¾ç¤ºï¼Œä¸æœ€æ–°å¸è½½ç³»ç»Ÿç›¸æ¯”ï¼ŒSuperOffloadæé«˜äº†2.5å€çš„ååé‡ï¼Œèƒ½åœ¨å•ä¸ªè¶…çº§èŠ¯ç‰‡ä¸Šè®­ç»ƒæ›´å¤§è§„æ¨¡çš„æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.21271">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_LLM/2509.21271v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_LLM/2509.21271v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_LLM/2509.21271v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_LLM/2509.21271v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_LLM/2509.21271v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_LLM/2509.21271v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="LLMTrace-A-Corpus-for-Classification-and-Fine-Grained-Localization-of-AI-Written-Text"><a href="#LLMTrace-A-Corpus-for-Classification-and-Fine-Grained-Localization-of-AI-Written-Text" class="headerlink" title="LLMTrace: A Corpus for Classification and Fine-Grained Localization of   AI-Written Text"></a>LLMTrace: A Corpus for Classification and Fine-Grained Localization of   AI-Written Text</h2><p><strong>Authors:Irina Tolstykh, Aleksandra Tsybina, Sergey Yakubson, Maksim Kuprashevich</strong></p>
<p>The widespread use of human-like text from Large Language Models (LLMs) necessitates the development of robust detection systems. However, progress is limited by a critical lack of suitable training data; existing datasets are often generated with outdated models, are predominantly in English, and fail to address the increasingly common scenario of mixed human-AI authorship. Crucially, while some datasets address mixed authorship, none provide the character-level annotations required for the precise localization of AI-generated segments within a text. To address these gaps, we introduce LLMTrace, a new large-scale, bilingual (English and Russian) corpus for AI-generated text detection. Constructed using a diverse range of modern proprietary and open-source LLMs, our dataset is designed to support two key tasks: traditional full-text binary classification (human vs. AI) and the novel task of AI-generated interval detection, facilitated by character-level annotations. We believe LLMTrace will serve as a vital resource for training and evaluating the next generation of more nuanced and practical AI detection models. The project page is available at \href{<a target="_blank" rel="noopener" href="https://sweetdream779.github.io/LLMTrace-info/%7D%7Biitolstykh/LLMTrace%7D">https://sweetdream779.github.io/LLMTrace-info/}{iitolstykh/LLMTrace}</a>. </p>
<blockquote>
<p>ç”±å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰äº§ç”Ÿçš„äººå½¢æ–‡æœ¬å¹¿æ³›åº”ç”¨ï¼Œè¿™ä¿ƒä½¿éœ€è¦å¼€å‘ç¨³å¥çš„æ£€æµ‹ç³»ç»Ÿã€‚ç„¶è€Œï¼Œç”±äºç¼ºå°‘åˆé€‚çš„è®­ç»ƒæ•°æ®ï¼Œè¿›å±•å—åˆ°é™åˆ¶ï¼›ç°æœ‰æ•°æ®é›†é€šå¸¸ç”±è¿‡æ—¶æ¨¡å‹ç”Ÿæˆï¼Œä¸»è¦ä¸ºè‹±è¯­ï¼Œå¹¶ä¸”æœªèƒ½è§£å†³æ—¥ç›Šæ™®éçš„äººæœºæ··åˆåˆ›ä½œåœºæ™¯ã€‚è™½ç„¶ä¸€äº›æ•°æ®é›†è§£å†³äº†æ··åˆåˆ›ä½œé—®é¢˜ï¼Œä½†æ²¡æœ‰æ•°æ®é›†æä¾›å­—ç¬¦çº§åˆ«çš„æ³¨é‡Šï¼Œæ— æ³•å‡†ç¡®å®šä½æ–‡æœ¬ä¸­AIç”Ÿæˆçš„ç‰‡æ®µä½ç½®ã€‚ä¸ºäº†è§£å†³è¿™äº›ç©ºç™½ï¼Œæˆ‘ä»¬å¼•å…¥äº†LLMTraceï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„å¤§è§„æ¨¡åŒè¯­ï¼ˆè‹±è¯­å’Œä¿„è¯­ï¼‰è¯­æ–™åº“ï¼Œç”¨äºæ£€æµ‹AIç”Ÿæˆçš„æ–‡æœ¬ã€‚é€šè¿‡ä½¿ç”¨å„ç§ç°ä»£ä¸“æœ‰å’Œå¼€æºLLMæ„å»ºï¼Œæˆ‘ä»¬çš„æ•°æ®é›†æ—¨åœ¨æ”¯æŒä¸¤é¡¹å…³é”®ä»»åŠ¡ï¼šä¼ ç»Ÿçš„å…¨æ–‡äºŒè¿›åˆ¶åˆ†ç±»ï¼ˆäººç±»ä¸AIï¼‰å’Œæ–°çš„AIç”Ÿæˆé—´éš”æ£€æµ‹ä»»åŠ¡ï¼Œåè€…ç”±å­—ç¬¦çº§æ³¨é‡Šä¿ƒè¿›ã€‚æˆ‘ä»¬ç›¸ä¿¡LLMTraceå°†æˆä¸ºåŸ¹è®­å’Œè¯„ä¼°ä¸‹ä¸€ä»£æ›´å¾®å¦™ã€æ›´å®ç”¨çš„AIæ£€æµ‹æ¨¡å‹çš„é‡è¦èµ„æºã€‚é¡¹ç›®é¡µé¢å¯é€šè¿‡é“¾æ¥iitolstykh&#x2F;LLMTraceï¼ˆ<a target="_blank" rel="noopener" href="https://sweetdream779.github.io/LLMTrace-info/%EF%BC%89%E8%AE%BF%E9%97%AE%E3%80%82">https://sweetdream779.github.io/LLMTrace-info/ï¼‰è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.21269v1">PDF</a> </p>
<p><strong>Summary</strong><br>äººå·¥æ™ºèƒ½ç”Ÿæˆæ–‡æœ¬å¹¿æ³›ä½¿ç”¨ï¼Œå¯¹å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ£€æµ‹ç³»ç»Ÿæå‡ºäº†æ›´é«˜çš„è¦æ±‚ã€‚å½“å‰è®­ç»ƒæ•°æ®å­˜åœ¨ä¸è¶³ï¼Œéœ€è¦å¼€å‘æ–°çš„æ•°æ®é›†ã€‚LLMTraceæ˜¯ä¸€ä¸ªå¤§è§„æ¨¡åŒè¯­ï¼ˆè‹±è¯­å’Œä¿„è¯­ï¼‰è¯­æ–™åº“ï¼Œç”¨äºæ£€æµ‹AIç”Ÿæˆçš„æ–‡æœ¬ã€‚è¯¥æ•°æ®é›†æ”¯æŒä¼ ç»Ÿå…¨æ–‡äºŒå…ƒåˆ†ç±»ï¼ˆäººç±»ä¸AIï¼‰å’Œæ–°ä»»åŠ¡AIç”Ÿæˆé—´éš”æ£€æµ‹ï¼Œé€šè¿‡å­—ç¬¦çº§æ³¨é‡Šå®ç°ã€‚LLMTraceå°†æˆä¸ºè®­ç»ƒå’Œè¯„ä¼°ä¸‹ä¸€ä»£æ›´ç²¾ç»†ã€æ›´å®ç”¨çš„AIæ£€æµ‹æ¨¡å‹çš„é‡è¦èµ„æºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆçš„æ–‡æœ¬å¹¿æ³›ä½¿ç”¨ï¼Œéœ€è¦å¼€å‘æ›´ç²¾ç¡®çš„AIæ–‡æœ¬æ£€æµ‹ç³»ç»Ÿã€‚</li>
<li>å½“å‰è®­ç»ƒæ•°æ®å­˜åœ¨ä¸è¶³ï¼Œé™åˆ¶äº†AIæ–‡æœ¬æ£€æµ‹ç³»ç»Ÿçš„è¿›æ­¥ã€‚</li>
<li>LLMTraceæ˜¯ä¸€ä¸ªå¤§è§„æ¨¡åŒè¯­è¯­æ–™åº“ï¼ŒåŒ…æ‹¬è‹±è¯­å’Œä¿„è¯­æ–‡æœ¬ï¼Œè§£å†³äº†å½“å‰æ•°æ®é›†çš„ä¸è¶³ã€‚</li>
<li>LLMTraceæ”¯æŒä¼ ç»Ÿå…¨æ–‡äºŒå…ƒåˆ†ç±»ä»»åŠ¡å’Œæ–°ä»»åŠ¡AIç”Ÿæˆé—´éš”æ£€æµ‹ä»»åŠ¡ã€‚</li>
<li>LLMTraceä½¿ç”¨äº†å¤šæ ·åŒ–çš„ç°ä»£ä¸“æœ‰å’Œå¼€æºå¤§è¯­è¨€æ¨¡å‹æ„å»ºè€Œæˆã€‚</li>
<li>LLMTraceé€šè¿‡å­—ç¬¦çº§æ³¨é‡Šå®ç°AIç”Ÿæˆæ–‡æœ¬çš„ç²¾å‡†å®šä½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.21269">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_LLM/2509.21269v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_LLM/2509.21269v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_LLM/2509.21269v1/page_1_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_LLM/2509.21269v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_LLM/2509.21269v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_LLM/2509.21269v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_LLM/2509.21269v1/page_5_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_LLM/2509.21269v1/page_5_2.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Grounding-AI-Explanations-in-Experience-A-Reflective-Cognitive-Architecture-for-Clinical-Decision-Support"><a href="#Grounding-AI-Explanations-in-Experience-A-Reflective-Cognitive-Architecture-for-Clinical-Decision-Support" class="headerlink" title="Grounding AI Explanations in Experience: A Reflective Cognitive   Architecture for Clinical Decision Support"></a>Grounding AI Explanations in Experience: A Reflective Cognitive   Architecture for Clinical Decision Support</h2><p><strong>Authors:Zijian Shao, Haiyang Shen, Mugeng Liu, Gecheng Fu, Yaoqi Guo, Yanfeng Wang, Yun Ma</strong></p>
<p>Effective disease prediction in modern healthcare demands the twin goals of high accuracy and transparent, clinically meaningful explanations. Existing machine learning and large language model (LLM) based approaches often struggle to balance these goals. Many models yield accurate but unclear statistical outputs, while others generate fluent but statistically unsupported narratives, often undermining both the validity of the explanation and the predictive accuracy itself. This shortcoming comes from a shallow interaction with the data, preventing the development of a deep, detailed understanding similar to a human expertâ€™s. We argue that high accuracy and high-quality explanations are not separate objectives but are mutually reinforcing outcomes of a model that develops a deep, direct understanding of the data. To achieve this, we propose the Reflective Cognitive Architecture (RCA), a novel framework that coordinates multiple LLMs to learn from direct experience. RCA features an iterative rule refinement mechanism that improves its logic from prediction errors and a distribution-aware rules check mechanism that bases its reasoning in the datasetâ€™s global statistics. By using predictive accuracy as a signal to drive deeper comprehension, RCA builds a strong internal model of the data. We evaluated RCA on one private and two public datasets against 22 baselines. The results demonstrate that RCA not only achieves state-of-the-art accuracy and robustness with a relative improvement of up to 40% over the baseline but, more importantly, leverages this deep understanding to excel in generating explanations that are clear, logical, evidence-based, and balanced, highlighting its potential for creating genuinely trustworthy clinical decision support systems. The code is available at \<a target="_blank" rel="noopener" href="https://github.com/ssssszj/RCA">https://github.com/ssssszj/RCA</a>. </p>
<blockquote>
<p>åœ¨ç°ä»£åŒ»ç–—ä¸­ï¼Œæœ‰æ•ˆçš„ç–¾ç—…é¢„æµ‹éœ€è¦åŒæ—¶å®ç°é«˜å‡†ç¡®æ€§å’Œé€æ˜ã€ä¸´åºŠæ„ä¹‰çš„è§£é‡Šè¿™ä¸¤ä¸ªç›®æ ‡ã€‚ç°æœ‰çš„åŸºäºæœºå™¨å­¦ä¹ å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ–¹æ³•å¾€å¾€å¾ˆéš¾å¹³è¡¡è¿™ä¸¤ä¸ªç›®æ ‡ã€‚è®¸å¤šæ¨¡å‹äº§ç”Ÿçš„ç»“æœå‡†ç¡®ä½†è§£é‡Šä¸æ¸…æ™°ï¼Œè€Œå¦ä¸€äº›æ¨¡å‹åˆ™ç”Ÿæˆäº†æµç•…ä½†ç¼ºä¹ç»Ÿè®¡æ”¯æŒçš„å™è¿°ï¼Œè¿™æ—¢ç ´åäº†è§£é‡Šçš„åˆç†æ€§ï¼Œä¹Ÿå½±å“äº†é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚è¿™ç§ç¼ºç‚¹æ¥è‡ªäºä¸æ•°æ®çš„æµ…å±‚äº¤äº’ï¼Œé˜»ç¢äº†ç±»ä¼¼äºäººç±»ä¸“å®¶é‚£æ ·æ·±å…¥ã€è¯¦ç»†çš„ç†è§£çš„å‘å±•ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œé«˜å‡†ç¡®æ€§å’Œé«˜è´¨é‡çš„è§£é‡Šä¸æ˜¯ç‹¬ç«‹çš„ç›®æ ‡ï¼Œè€Œæ˜¯ç›¸äº’ä¿ƒè¿›çš„ç»“æœï¼Œä¸€ä¸ªæ¨¡å‹é€šè¿‡å¯¹æ•°æ®æœ‰æ·±å…¥ã€ç›´æ¥çš„ç†è§£è€Œå¾—å‡ºçš„ç»“æœã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬æå‡ºäº†åæ€è®¤çŸ¥æ¶æ„ï¼ˆRCAï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåè°ƒå¤šä¸ªLLMä»ç›´æ¥ç»éªŒä¸­å­¦ä¹ çš„æ–°å‹æ¡†æ¶ã€‚RCAå…·æœ‰è¿­ä»£è§„åˆ™ä¼˜åŒ–æœºåˆ¶ï¼Œé€šè¿‡é¢„æµ‹è¯¯å·®æ”¹è¿›é€»è¾‘ï¼Œä»¥åŠåŸºäºæ•°æ®å…¨å±€ç»Ÿè®¡çš„åˆ†å¸ƒå¼è§„åˆ™æ£€æŸ¥æœºåˆ¶ã€‚å®ƒä»¥é¢„æµ‹å‡†ç¡®æ€§ä½œä¸ºé©±åŠ¨æ›´æ·±å±‚æ¬¡ç†è§£çš„ä¿¡å·ï¼Œå»ºç«‹å¼ºå¤§çš„å†…éƒ¨æ•°æ®æ¨¡å‹ã€‚æˆ‘ä»¬åœ¨ä¸€ä¸ªç§æœ‰æ•°æ®é›†å’Œä¸¤ä¸ªå…¬å…±æ•°æ®é›†ä¸Šå¯¹RCAè¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶ä¸22ä¸ªåŸºçº¿æ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒã€‚ç»“æœè¡¨æ˜ï¼ŒRCAä¸ä»…å®ç°äº†æœ€å…ˆè¿›çš„å‡†ç¡®æ€§å’Œç¨³å¥æ€§ï¼Œç›¸å¯¹äºåŸºçº¿æ–¹æ³•æœ‰ç€é«˜è¾¾40%çš„ç›¸å¯¹æ”¹è¿›ï¼Œæ›´é‡è¦çš„æ˜¯ï¼Œå®ƒåˆ©ç”¨è¿™ç§æ·±å…¥çš„ç†è§£åœ¨ç”Ÿæˆæ¸…æ™°ã€é€»è¾‘æ€§å¼ºã€åŸºäºè¯æ®å’Œå¹³è¡¡çš„è§£é‡Šæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œçªæ˜¾å…¶åœ¨åˆ›å»ºçœŸæ­£å¯ä¿¡çš„ä¸´åºŠå†³ç­–æ”¯æŒç³»ç»Ÿæ–¹é¢çš„æ½œåŠ›ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/ssssszj/RCA%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/ssssszj/RCAè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.21266v1">PDF</a> under review</p>
<p><strong>æ‘˜è¦</strong></p>
<p>ç°ä»£åŒ»ç–—ç–¾ç—…é¢„æµ‹è¦æ±‚åŒæ—¶å®ç°é«˜å‡†ç¡®æ€§å’Œé€æ˜ã€ä¸´åºŠæ„ä¹‰çš„è§£é‡Šã€‚ç°æœ‰çš„æœºå™¨å­¦ä¹ å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ–¹æ³•å¾€å¾€éš¾ä»¥å¹³è¡¡è¿™ä¸¤ä¸ªç›®æ ‡ã€‚è®¸å¤šæ¨¡å‹äº§ç”Ÿå‡†ç¡®çš„ä½†ä¸æ¸…æ¥šçš„ç»Ÿè®¡è¾“å‡ºï¼Œè€Œå…¶ä»–æ¨¡å‹ç”Ÿæˆæµç•…çš„ä½†ç¼ºä¹ç»Ÿè®¡æ”¯æŒçš„å™è¿°ï¼Œè¿™å¸¸å¸¸ç ´åäº†è§£é‡Šçš„æœ‰æ•ˆæ€§å’Œé¢„æµ‹å‡†ç¡®æ€§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹æ¡†æ¶â€”â€”åæ€è®¤çŸ¥æ¶æ„ï¼ˆRCAï¼‰ï¼Œé€šè¿‡åè°ƒå¤šä¸ªLLMä»ç›´æ¥ç»éªŒä¸­å­¦ä¹ æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚RCAå…·æœ‰è¿­ä»£è§„åˆ™ä¼˜åŒ–æœºåˆ¶å’Œåˆ†å¸ƒæ„ŸçŸ¥è§„åˆ™æ£€æŸ¥æœºåˆ¶ï¼Œåˆ†åˆ«ç”¨äºä»é¢„æµ‹é”™è¯¯ä¸­æ”¹è¿›é€»è¾‘å’ŒåŸºäºæ•°æ®é›†å…¨å±€ç»Ÿè®¡è¿›è¡Œæ¨ç†ã€‚é€šè¿‡åˆ©ç”¨é¢„æµ‹å‡†ç¡®æ€§ä½œä¸ºé©±åŠ¨æ›´æ·±å±‚ç†è§£çš„ä¿¡å·ï¼ŒRCAå»ºç«‹äº†å¼ºå¤§çš„å†…éƒ¨æ•°æ®æ¨¡å‹ã€‚æˆ‘ä»¬å¯¹RCAåœ¨ä¸€ä¸ªç§æœ‰å’Œä¸¤ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šä¸22ä¸ªåŸºçº¿æ–¹æ³•è¿›è¡Œäº†è¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼ŒRCAä¸ä»…å®ç°äº†æœ€å…ˆè¿›çš„å‡†ç¡®æ€§å’Œç¨³å¥æ€§ï¼Œç›¸å¯¹åŸºçº¿æ–¹æ³•æœ‰é«˜è¾¾40%çš„ç›¸å¯¹æ”¹è¿›ï¼Œè€Œä¸”æ›´é‡è¦çš„æ˜¯ï¼Œå®ƒåˆ©ç”¨æ·±å±‚ç†è§£æ¥ç”Ÿæˆæ¸…æ™°ã€é€»è¾‘ã€æœ‰è¯æ®æ”¯æŒå’Œå¹³è¡¡çš„è§£é‡Šï¼Œè¿™çªå‡ºäº†å…¶åœ¨åˆ›å»ºçœŸæ­£å¯ä¿¡çš„ä¸´åºŠå†³ç­–æ”¯æŒç³»ç»Ÿæ–¹é¢çš„æ½œåŠ›ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç°ä»£åŒ»ç–—ç–¾ç—…é¢„æµ‹éœ€è¦åŒæ—¶å®ç°é«˜å‡†ç¡®æ€§å’Œé€æ˜ã€ä¸´åºŠæ„ä¹‰çš„è§£é‡Šã€‚</li>
<li>ç°æœ‰æœºå™¨å­¦ä¹ å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¹³è¡¡é¢„æµ‹å‡†ç¡®æ€§å’Œè§£é‡Šæ¸…æ™°åº¦æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>åæ€è®¤çŸ¥æ¶æ„ï¼ˆRCAï¼‰æ˜¯ä¸€ç§æ–°å‹æ¡†æ¶ï¼Œé€šè¿‡åè°ƒå¤šä¸ªLLMä»ç›´æ¥ç»éªŒä¸­å­¦ä¹ æ¥æé«˜é¢„æµ‹å’Œè§£é‡Šçš„è´¨é‡ã€‚</li>
<li>RCAå…·æœ‰è¿­ä»£è§„åˆ™ä¼˜åŒ–å’Œåˆ†å¸ƒæ„ŸçŸ¥è§„åˆ™æ£€æŸ¥æœºåˆ¶ï¼Œä»¥æé«˜é€»è¾‘æ€§å’ŒåŸºäºæ•°æ®çš„æ¨ç†ã€‚</li>
<li>RCAåˆ©ç”¨é¢„æµ‹å‡†ç¡®æ€§ä½œä¸ºé©±åŠ¨æ›´æ·±å±‚ç†è§£çš„ä¿¡å·ï¼Œå»ºç«‹å¼ºå¤§çš„å†…éƒ¨æ•°æ®æ¨¡å‹ã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒRCAå®ç°äº†æœ€å…ˆè¿›çš„é¢„æµ‹å‡†ç¡®æ€§å’Œç¨³å¥æ€§ï¼Œç›¸å¯¹åŸºçº¿æ–¹æ³•æœ‰æ˜¾è‘—æ”¹è¿›ã€‚</li>
<li>RCAç”Ÿæˆçš„è§£é‡Šæ¸…æ™°ã€é€»è¾‘ã€æœ‰è¯æ®æ”¯æŒä¸”å¹³è¡¡ï¼Œä¸ºåˆ›å»ºå¯ä¿¡çš„ä¸´åºŠå†³ç­–æ”¯æŒç³»ç»Ÿæä¾›äº†æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.21266">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_LLM/2509.21266v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_LLM/2509.21266v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Instruction-tuned-Self-Questioning-Framework-for-Multimodal-Reasoning"><a href="#Instruction-tuned-Self-Questioning-Framework-for-Multimodal-Reasoning" class="headerlink" title="Instruction-tuned Self-Questioning Framework for Multimodal Reasoning"></a>Instruction-tuned Self-Questioning Framework for Multimodal Reasoning</h2><p><strong>Authors:You-Won Jang, Yu-Jung Heo, Jaeseok Kim, Minsu Lee, Du-Seong Chang, Byoung-Tak Zhang</strong></p>
<p>The field of vision-language understanding has been actively researched in recent years, thanks to the development of Large Language Models~(LLMs). However, it still needs help with problems requiring multi-step reasoning, even for very simple questions. Recent studies adopt LLMs to tackle this problem by iteratively generating sub-questions and answers. However, there are disadvantages such as 1) the fine-grained visual contents of images are not available using LLMs that cannot read visual information, 2) internal mechanisms are inaccessible and difficult to reproduce by using black-box LLMs. To solve these problems, we propose the SQ (Self-Questioning)-InstructBLIP, which improves inference performance by generating image-aware informative sub-questions and sub-answers iteratively. The SQ-InstructBLIP, which consists of a Questioner, Answerer, and Reasoner that share the same architecture. Questioner and Answerer generate sub-questions and sub-answers to help infer the main-question, and Reasoner performs reasoning on the main-question considering the generated sub-question information. Our experiments show that the proposed method SQ-InstructBLIP, which uses the generated sub-questions as additional information when solving the VQA task, performs more accurate reasoning than the previous works. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œéšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‘å±•ï¼Œè§†è§‰-è¯­è¨€ç†è§£é¢†åŸŸå¾—åˆ°äº†ç§¯æçš„ç ”ç©¶ã€‚ç„¶è€Œï¼Œå®ƒä»ç„¶éœ€è¦è§£å†³éœ€è¦å¤šæ­¥éª¤æ¨ç†çš„é—®é¢˜ï¼Œå³ä½¿æ˜¯å¯¹äºéå¸¸ç®€å•çš„é—®é¢˜ã€‚æœ€è¿‘çš„ç ”ç©¶é‡‡ç”¨LLMé€šè¿‡è¿­ä»£ç”Ÿæˆå­é—®é¢˜å’Œç­”æ¡ˆæ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚ç„¶è€Œï¼Œä»ç„¶å­˜åœ¨ä¸€äº›ç¼ºç‚¹ï¼Œä¾‹å¦‚1ï¼‰LLMæ— æ³•è¯»å–è§†è§‰ä¿¡æ¯ï¼Œå› æ­¤æ— æ³•è·å–å›¾åƒçš„ç²¾ç»†è§†è§‰å†…å®¹ï¼›2ï¼‰ä½¿ç”¨é»‘ç®±LLMæ— æ³•è®¿é—®å…¶å†…éƒ¨æœºåˆ¶ï¼Œä¹Ÿéš¾ä»¥è¿›è¡Œå¤åˆ¶ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†SQï¼ˆè‡ªæˆ‘æé—®ï¼‰-InstructBLIPï¼Œé€šè¿‡è¿­ä»£ç”Ÿæˆå›¾åƒæ„ŸçŸ¥ä¿¡æ¯ä¸°å¯Œçš„å­é—®é¢˜å’Œå­ç­”æ¡ˆï¼Œæé«˜æ¨ç†æ€§èƒ½ã€‚SQ-InstructBLIPç”±å…·æœ‰ç›¸åŒæ¶æ„çš„æé—®è€…ã€å›ç­”è€…å’Œæ¨ç†è€…ç»„æˆã€‚æé—®è€…å’Œå›ç­”è€…ç”Ÿæˆå­é—®é¢˜å’Œå­ç­”æ¡ˆä»¥å¸®åŠ©æ¨æ–­ä¸»é—®é¢˜ï¼Œè€Œæ¨ç†è€…åˆ™è€ƒè™‘ç”Ÿæˆçš„å­é—®é¢˜ä¿¡æ¯è¿›è¡Œä¸»é—®é¢˜çš„æ¨ç†ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„SQ-InstructBLIPæ–¹æ³•åœ¨ä½¿ç”¨ç”Ÿæˆçš„å­é—®é¢˜ä½œä¸ºè§£å†³VQAä»»åŠ¡æ—¶çš„é™„åŠ ä¿¡æ¯æ—¶ï¼Œæ¯”ä»¥å‰çš„å·¥ä½œè¡¨ç°å‡ºæ›´å‡†ç¡®çš„æ¨ç†èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.21251v1">PDF</a> This paper was accepted to the â€œCLVL: 5th Workshop on Closing the   Loop Between Vision and Language (ICCV 2023 CLVL workshop).â€</p>
<p><strong>Summary</strong><br>è§†è§‰è¯­è¨€ç†è§£é¢†åŸŸçš„ç ”ç©¶è¿‘å¹´æ¥å¾—åˆ°å¤§åŠ›å‘å±•ï¼Œä½†å­˜åœ¨å¯¹å¤šæ­¥éª¤æ¨ç†é—®é¢˜çš„å¤„ç†å›°éš¾ï¼Œå³ä½¿é¢å¯¹ç®€å•é—®é¢˜ä¹Ÿæ˜¯å¦‚æ­¤ã€‚æœ€è¿‘çš„ç ”ç©¶é‡‡ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œé€šè¿‡è¿­ä»£ç”Ÿæˆå­é—®é¢˜å’Œç­”æ¡ˆã€‚ç„¶è€Œï¼Œå­˜åœ¨å›¾åƒç²¾ç»†å†…å®¹æ— æ³•è·å–ä»¥åŠé»‘ç®±æ¨¡å‹æœºåˆ¶éš¾ä»¥å†ç°ç­‰é—®é¢˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†SQ-InstructBLIPæ–¹æ³•ï¼Œé€šè¿‡ç”Ÿæˆå›¾åƒæ„ŸçŸ¥ä¿¡æ¯å­é—®é¢˜å’Œå­ç­”æ¡ˆè¿›è¡Œè¿­ä»£æ¨ç†ï¼Œæé«˜äº†æ¨ç†æ€§èƒ½ã€‚è¯¥æ–¹æ³•åŒ…æ‹¬é—®ç­”è€…å’Œæ¨ç†è€…ä¸‰ä¸ªéƒ¨åˆ†ï¼Œå…±åŒä½¿ç”¨ç›¸åŒçš„æ¶æ„ç”Ÿæˆå­é—®é¢˜å’Œå­ç­”æ¡ˆï¼Œä»¥å¸®åŠ©æ¨æ–­ä¸»é—®é¢˜å¹¶è¿›è¡Œè€ƒè™‘å­é—®é¢˜çš„æ¨ç†ã€‚å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨ç”Ÿæˆçš„å­é—®é¢˜ä½œä¸ºè§£å†³è§†è§‰é—®ç­”ä»»åŠ¡çš„é¢å¤–ä¿¡æ¯ï¼Œè¯¥æ–¹æ³•æ¯”å…ˆå‰çš„å·¥ä½œè¡¨ç°å‡ºæ›´å‡†ç¡®çš„æ¨ç†èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è§†è§‰è¯­è¨€ç†è§£é¢†åŸŸé¢ä¸´å¤šæ­¥éª¤æ¨ç†é—®é¢˜çš„æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰ç ”ç©¶é€šè¿‡è¿­ä»£ç”Ÿæˆå­é—®é¢˜å’Œç­”æ¡ˆæ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>SQ-InstructBLIPæ–¹æ³•é€šè¿‡ç”Ÿæˆå›¾åƒæ„ŸçŸ¥ä¿¡æ¯å­é—®é¢˜å’Œå­ç­”æ¡ˆè¿›è¡Œè¿­ä»£æ¨ç†ï¼Œä»¥æé«˜æ¨ç†æ€§èƒ½ã€‚</li>
<li>SQ-InstructBLIPåŒ…æ‹¬é—®ç­”è€…å’Œæ¨ç†è€…ä¸‰ä¸ªç»„æˆéƒ¨åˆ†ï¼Œå…±åŒä½¿ç”¨ç›¸åŒçš„æ¶æ„ã€‚</li>
<li>ç”Ÿæˆå­é—®é¢˜å’Œå­ç­”æ¡ˆæœ‰åŠ©äºæ¨æ–­ä¸»é—®é¢˜ï¼Œå¹¶è€ƒè™‘å­é—®é¢˜çš„æ¨ç†ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒSQ-InstructBLIPæ–¹æ³•åœ¨è§†è§‰é—®ç­”ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ›´å‡†ç¡®çš„æ¨ç†èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.21251">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_LLM/2509.21251v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_LLM/2509.21251v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_LLM/2509.21251v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_LLM/2509.21251v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Tree-Search-for-LLM-Agent-Reinforcement-Learning"><a href="#Tree-Search-for-LLM-Agent-Reinforcement-Learning" class="headerlink" title="Tree Search for LLM Agent Reinforcement Learning"></a>Tree Search for LLM Agent Reinforcement Learning</h2><p><strong>Authors:Yuxiang Ji, Ziyu Ma, Yong Wang, Guanhua Chen, Xiangxiang Chu, Liaoni Wu</strong></p>
<p>Recent advances in reinforcement learning (RL) have significantly enhanced the agentic capabilities of large language models (LLMs). In long-term and multi-turn agent tasks, existing approaches driven solely by outcome rewards often suffer from the problem of sparse supervision. To address the challenge, we propose Tree-based Group Relative Policy Optimization (Tree-GRPO), a grouped agent RL method based on tree search, where each tree node represents the complete agent interaction step. By sharing common prefixes, the tree search sampling increases the number of rollouts achievable within a fixed budget of tokens or tool calls. Moreover, we find that the tree-structured trajectory naturally allows the construction of step-wise process supervised signals even using only the outcome reward. Based on this, Tree-GRPO estimates the grouped relative advantages both on intra-tree and inter-tree levels. Through theoretical analysis, we demonstrate that the objective of intra-tree level group relative policy optimization is equivalent to that of step-level direct preference learning. Experiments across 11 datasets and 3 types of QA tasks demonstrate the superiority of the proposed tree-based RL over the chain-based RL method. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æœ€æ–°è¿›å±•æå¤§åœ°å¢å¼ºäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä»£ç†èƒ½åŠ›ã€‚åœ¨é•¿æœŸå’Œå¤šè½®ä»£ç†ä»»åŠ¡ä¸­ï¼Œä»…ç”±ç»“æœå¥–åŠ±é©±åŠ¨çš„ä¼ ç»Ÿæ–¹æ³•å¸¸å¸¸é¢ä¸´ç›‘ç£ç¨€ç–çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºæ ‘æœç´¢çš„åˆ†ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆTree-GRPOï¼‰ï¼Œè¿™æ˜¯ä¸€ç§åˆ†ç»„ä»£ç†RLæ–¹æ³•ï¼Œå…¶ä¸­æ¯ä¸ªæ ‘èŠ‚ç‚¹ä»£è¡¨å®Œæ•´çš„ä»£ç†äº¤äº’æ­¥éª¤ã€‚é€šè¿‡å…±äº«å…¬å…±å‰ç¼€ï¼Œæ ‘æœç´¢é‡‡æ ·å¢åŠ äº†åœ¨å›ºå®šé¢„ç®—çš„ä»¤ç‰Œæˆ–å·¥å…·è°ƒç”¨èŒƒå›´å†…å¯å®ç°çš„é‡æ’­æ¬¡æ•°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°æ ‘å½¢è½¨è¿¹è‡ªç„¶åœ°å…è®¸å³ä½¿ä»…ä½¿ç”¨ç»“æœå¥–åŠ±ä¹Ÿèƒ½æ„å»ºé€æ­¥è¿‡ç¨‹ç›‘ç£ä¿¡å·ã€‚åŸºäºæ­¤ï¼ŒTree-GRPOä¼°è®¡äº†æ ‘å†…å’Œæ ‘é—´çš„åˆ†ç»„ç›¸å¯¹ä¼˜åŠ¿ã€‚é€šè¿‡ç†è®ºåˆ†æï¼Œæˆ‘ä»¬è¯æ˜äº†æ ‘å†…çº§åˆ«åˆ†ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–çš„ç›®æ ‡ä¸æ­¥éª¤çº§åˆ«ç›´æ¥åå¥½å­¦ä¹ çš„ç›®æ ‡æ˜¯ä¸€è‡´çš„ã€‚åœ¨11ä¸ªæ•°æ®é›†å’Œ3ç§é—®ç­”ä»»åŠ¡ä¸Šçš„å®éªŒè¯æ˜äº†åŸºäºæ ‘çš„RLä¼˜äºåŸºäºé“¾çš„RLæ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.21240v1">PDF</a> </p>
<p><strong>æ€»ç»“</strong><br>     æœ€è¿‘å¼ºåŒ–å­¦ä¹ åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„agenticèƒ½åŠ›æ–¹é¢çš„è¿›å±•æ˜¾è‘—ã€‚åœ¨é•¿æœŸå’Œå¤šè½®å¯¹è¯çš„agentä»»åŠ¡ä¸­ï¼Œç°æœ‰ä»…ç”±ç»“æœå¥–åŠ±é©±åŠ¨çš„æ–¹æ³•å¸¸é¢ä¸´ç›‘ç£ç¨€ç–çš„é—®é¢˜ã€‚ä¸ºè§£å†³æ­¤æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºåŸºäºæ ‘æœç´¢çš„åˆ†ç»„agentå¼ºåŒ–å­¦ä¹ æ–¹æ³•â€”â€”Tree-GRPOã€‚æ ‘ä¸­çš„æ¯ä¸ªèŠ‚ç‚¹ä»£è¡¨å®Œæ•´çš„agentäº¤äº’æ­¥éª¤ï¼Œé€šè¿‡å…±äº«å…¬å…±å‰ç¼€ï¼Œæ ‘æœç´¢é‡‡æ ·æé«˜äº†åœ¨å›ºå®šé¢„ç®—çš„æ ‡è®°æˆ–å·¥å…·è°ƒç”¨æ¬¡æ•°ä¸­å¯è¾¾åˆ°çš„å›æ»šæ¬¡æ•°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°æ ‘ç»“æ„è½¨è¿¹è‡ªç„¶åœ°å…è®¸ä½¿ç”¨ä»…ç»“æœå¥–åŠ±æ¥æ„å»ºæ­¥éª¤è¿‡ç¨‹ç›‘ç£ä¿¡å·ã€‚åŸºäºæ­¤ï¼ŒTree-GRPOä¼°è®¡äº†æ ‘å†…å’Œæ ‘é—´çš„åˆ†ç»„ç›¸å¯¹ä¼˜åŠ¿ã€‚é€šè¿‡ç†è®ºåˆ†æï¼Œæˆ‘ä»¬è¯æ˜äº†æ ‘å†…çº§åˆ«çš„åˆ†ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ç›®æ ‡ä¸æ­¥éª¤çº§åˆ«çš„ç›´æ¥åå¥½å­¦ä¹ çš„ç›®æ ‡æ˜¯ä¸€è‡´çš„ã€‚åœ¨11ä¸ªæ•°æ®é›†å’Œ3ç§é—®ç­”ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒåŸºäºæ ‘çš„å¼ºåŒ–å­¦ä¹ ä¼˜äºåŸºäºé“¾çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>Tree-GRPOæ˜¯ä¸€ç§åŸºäºæ ‘æœç´¢çš„åˆ†ç»„agentå¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œè§£å†³äº†é•¿æœŸå’Œå¤šè½®å¯¹è¯ä»»åŠ¡ä¸­ç›‘ç£ç¨€ç–çš„é—®é¢˜ã€‚</li>
<li>æ ‘æœç´¢ä¸­çš„æ¯ä¸ªèŠ‚ç‚¹ä»£è¡¨å®Œæ•´çš„agentäº¤äº’æ­¥éª¤ï¼Œæé«˜å›æ»šæ¬¡æ•°å¹¶å…è®¸å…±äº«å…¬å…±å‰ç¼€ã€‚</li>
<li>æ ‘ç»“æ„è‡ªç„¶åœ°æ”¯æŒä½¿ç”¨ä»…ç»“æœå¥–åŠ±æ„å»ºæ­¥éª¤è¿‡ç¨‹ç›‘ç£ä¿¡å·ã€‚</li>
<li>Tree-GRPOèƒ½å¤Ÿä¼°è®¡æ ‘å†…å’Œæ ‘é—´çš„åˆ†ç»„ç›¸å¯¹ä¼˜åŠ¿ã€‚</li>
<li>é€šè¿‡å¯¹Tree-GRPOè¿›è¡Œç†è®ºåˆ†æï¼Œè¯æ˜äº†å…¶ä¸æ­¥éª¤çº§åˆ«çš„ç›´æ¥åå¥½å­¦ä¹ çš„ç›®æ ‡ä¸€è‡´æ€§ã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†å’Œä¸åŒç±»å‹çš„é—®ç­”ä»»åŠ¡ä¸Šï¼ŒåŸºäºæ ‘çš„å¼ºåŒ–å­¦ä¹ ï¼ˆTree-GRPOï¼‰è¡¨ç°å‡ºä¼˜äºåŸºäºé“¾çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.21240">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_LLM/2509.21240v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_LLM/2509.21240v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_LLM/2509.21240v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Query-Centric-Graph-Retrieval-Augmented-Generation"><a href="#Query-Centric-Graph-Retrieval-Augmented-Generation" class="headerlink" title="Query-Centric Graph Retrieval Augmented Generation"></a>Query-Centric Graph Retrieval Augmented Generation</h2><p><strong>Authors:Yaxiong Wu, Jianyuan Bo, Yongyue Zhang, Sheng Liang, Yong Liu</strong></p>
<p>Graph-based retrieval-augmented generation (RAG) enriches large language models (LLMs) with external knowledge for long-context understanding and multi-hop reasoning, but existing methods face a granularity dilemma: fine-grained entity-level graphs incur high token costs and lose context, while coarse document-level graphs fail to capture nuanced relations. We introduce QCG-RAG, a query-centric graph RAG framework that enables query-granular indexing and multi-hop chunk retrieval. Our query-centric approach leverages Doc2Query and Doc2Query{-}{-} to construct query-centric graphs with controllable granularity, improving graph quality and interpretability. A tailored multi-hop retrieval mechanism then selects relevant chunks via the generated queries. Experiments on LiHuaWorld and MultiHop-RAG show that QCG-RAG consistently outperforms prior chunk-based and graph-based RAG methods in question answering accuracy, establishing a new paradigm for multi-hop reasoning. </p>
<blockquote>
<p>åŸºäºå›¾çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¢åŠ äº†å¤–éƒ¨çŸ¥è¯†ï¼Œä»¥å®ç°å¯¹é•¿æ–‡æœ¬çš„ç†è§£å’Œè·¨æ­¥æ¨ç†ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é¢ä¸´ç²’åº¦å›°å¢ƒï¼šç²¾ç»†çš„å®ä½“çº§å›¾ä¼šäº§ç”Ÿé«˜ä»¤ç‰Œæˆæœ¬å¹¶ä¸¢å¤±ä¸Šä¸‹æ–‡ï¼Œè€Œç²—ç³™çš„æ–‡æ¡£çº§å›¾åˆ™æ— æ³•æ•æ‰å¾®å¦™å…³ç³»ã€‚æˆ‘ä»¬å¼•å…¥äº†QCG-RAGï¼Œè¿™æ˜¯ä¸€ç§ä»¥æŸ¥è¯¢ä¸ºä¸­å¿ƒçš„å›¾å½¢RAGæ¡†æ¶ï¼Œèƒ½å¤Ÿå®ç°æŸ¥è¯¢ç²’åº¦ç´¢å¼•å’Œå¤šæ­¥å—æ£€ç´¢ã€‚æˆ‘ä»¬çš„ä»¥æŸ¥è¯¢ä¸ºä¸­å¿ƒçš„æ–¹æ³•åˆ©ç”¨Doc2Queryå’ŒDoc2Query{-}{-}æ„å»ºå¯æ§ç²’åº¦çš„æŸ¥è¯¢ä¸­å¿ƒå›¾ï¼Œæé«˜äº†å›¾çš„è´¨é‡å’Œå¯è§£é‡Šæ€§ã€‚å®šåˆ¶çš„è·¨æ­¥æ£€ç´¢æœºåˆ¶ç„¶åé€šè¿‡ç”Ÿæˆçš„æŸ¥è¯¢é€‰æ‹©ç›¸å…³å—ã€‚åœ¨LiHuaWorldå’ŒMultiHop-RAGä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒQCG-RAGåœ¨é—®ç­”å‡†ç¡®æ€§æ–¹é¢å§‹ç»ˆä¼˜äºå…ˆå‰çš„åŸºäºå—å’ŒåŸºäºå›¾çš„RAGæ–¹æ³•ï¼Œä¸ºè·¨æ­¥æ¨ç†å»ºç«‹äº†æ–°çš„èŒƒå¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.21237v1">PDF</a> 25 pages, 6 figures, 1 table</p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬ä»‹ç»äº†åŸºäºå›¾çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ–¹æ³•åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­èå…¥å¤–éƒ¨çŸ¥è¯†ï¼Œä»¥å®ç°é•¿æ–‡æœ¬ç†è§£å’Œå¤šè·³æ¨ç†ã€‚ç°æœ‰æ–¹æ³•é¢ä¸´ç²’åº¦é€‰æ‹©å›°å¢ƒï¼šç²¾ç»†ç²’åº¦çš„å®ä½“çº§å›¾å¯¼è‡´é«˜ä»¤ç‰Œæˆæœ¬å¹¶ä¸¢å¤±ä¸Šä¸‹æ–‡ï¼Œè€Œç²—ç³™æ–‡æ¡£çº§å›¾åˆ™æ— æ³•æ•æ‰å¾®å¦™å…³ç³»ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†æŸ¥è¯¢ä¸­å¿ƒå›¾ï¼ˆQCG-RAGï¼‰çš„RAGæ¡†æ¶ï¼Œå®ç°æŸ¥è¯¢ç²’åº¦ç´¢å¼•å’Œå¤šè·³å—æ£€ç´¢ã€‚é€šè¿‡Doc2Queryå’ŒDoc2Query{-}{-}æ„å»ºæŸ¥è¯¢ä¸­å¿ƒå›¾ï¼Œæé«˜å›¾è´¨é‡å’Œå¯è§£é‡Šæ€§ã€‚åœ¨LiHuaWorldå’ŒMultiHop-RAGä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒQCG-RAGåœ¨é—®ç­”å‡†ç¡®æ€§æ–¹é¢å§‹ç»ˆä¼˜äºåŸºäºå—å’ŒåŸºäºå›¾çš„å…ˆå‰RAGæ–¹æ³•ï¼Œä¸ºå»ºç«‹å¤šè·³æ¨ç†çš„æ–°èŒƒå¼å¥ å®šäº†åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>QCG-RAGè§£å†³äº†ç°æœ‰æ–¹æ³•åœ¨åŸºäºå›¾çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ä¸­çš„ç²’åº¦é€‰æ‹©å›°å¢ƒã€‚</li>
<li>QCG-RAGå®ç°äº†æŸ¥è¯¢ç²’åº¦ç´¢å¼•å’Œå¤šè·³å—æ£€ç´¢åŠŸèƒ½ï¼Œæœ‰åŠ©äºæé«˜ä¿¡æ¯æ£€ç´¢çš„å‡†ç¡®æ€§ã€‚</li>
<li>æŸ¥è¯¢ä¸­å¿ƒå›¾é€šè¿‡Doc2Queryå’ŒDoc2Query{-}{-}æ„å»ºï¼Œå®ç°äº†å¯æ§åˆ¶çš„ç²’åº¦ï¼Œæé«˜äº†å›¾çš„è´¨é‡å’Œå¯è§£é‡Šæ€§ã€‚</li>
<li>QCG-RAGé‡‡ç”¨å®šåˆ¶çš„å¤šè·³æ£€ç´¢æœºåˆ¶é€‰æ‹©ç›¸å…³å—ï¼Œå¢å¼ºäº†æ£€ç´¢åŠŸèƒ½çš„æ•ˆæœã€‚</li>
<li>QCG-RAGåœ¨LiHuaWorldå’ŒMultiHop-RAGçš„å®éªŒä¸­ï¼Œç›¸è¾ƒäºå…ˆå‰çš„RAGæ–¹æ³•ï¼Œè¡¨ç°å‡ºæ›´é«˜çš„é—®ç­”å‡†ç¡®æ€§ã€‚</li>
<li>QCG-RAGä¸ºå»ºç«‹å¤šè·³æ¨ç†çš„æ–°èŒƒå¼å¥ å®šäº†åŸºç¡€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.21237">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_LLM/2509.21237v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_LLM/2509.21237v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_LLM/2509.21237v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Go-With-The-Flow-Churn-Tolerant-Decentralized-Training-of-Large-Language-Models"><a href="#Go-With-The-Flow-Churn-Tolerant-Decentralized-Training-of-Large-Language-Models" class="headerlink" title="Go With The Flow: Churn-Tolerant Decentralized Training of Large   Language Models"></a>Go With The Flow: Churn-Tolerant Decentralized Training of Large   Language Models</h2><p><strong>Authors:Nikolay Blagoev, Bart Cox, JÃ©rÃ©mie Decouchant, Lydia Y. Chen</strong></p>
<p>Motivated by the emergence of large language models (LLMs) and the importance of democratizing their training, we propose GWTF, the first crash tolerant practical decentralized training framework for LLMs. Differently from existing distributed and federated training frameworks, GWTF enables the efficient collaborative training of a LLM on heterogeneous clients that volunteer their resources. In addition, GWTF addresses node churn, i.e., clients joining or leaving the system at any time, and network instabilities, i.e., network links becoming unstable or unreliable. The core of GWTF is a novel decentralized flow algorithm that finds the most effective routing that maximizes the number of microbatches trained with the lowest possible delay. We extensively evaluate GWTF on GPT-like and LLaMa-like models and compare it against the prior art. Our results indicate that GWTF reduces the training time by up to 45% in realistic and challenging scenarios that involve heterogeneous client nodes distributed over 10 different geographic locations with a high node churn rate. </p>
<blockquote>
<p>å—å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å…´èµ·åŠå…¶è®­ç»ƒæ°‘ä¸»åŒ–çš„é‡è¦æ€§æ‰€é©±åŠ¨ï¼Œæˆ‘ä»¬æå‡ºäº†GWTFï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç”¨äºLLMçš„è€å´©æºƒå®ç”¨å»ä¸­å¿ƒåŒ–è®­ç»ƒæ¡†æ¶ã€‚ä¸ç°æœ‰çš„åˆ†å¸ƒå¼å’Œè”é‚¦è®­ç»ƒæ¡†æ¶ä¸åŒï¼ŒGWTFèƒ½å¤Ÿåœ¨å¼‚è´¨å®¢æˆ·ç«¯ä¸Šå®ç°LLMçš„æœ‰æ•ˆåä½œè®­ç»ƒï¼Œè¿™äº›å®¢æˆ·ç«¯è‡ªæ„¿æä¾›å…¶èµ„æºã€‚æ­¤å¤–ï¼ŒGWTFè¿˜è§£å†³äº†èŠ‚ç‚¹å˜åŒ–ï¼Œå³å®¢æˆ·ç«¯éšæ—¶åŠ å…¥æˆ–é€€å‡ºç³»ç»Ÿï¼Œä»¥åŠç½‘ç»œä¸ç¨³å®šï¼Œå³ç½‘ç»œé“¾æ¥å˜å¾—ä¸ç¨³å®šæˆ–ä¸å¯é çš„é—®é¢˜ã€‚GWTFçš„æ ¸å¿ƒæ˜¯ä¸€ç§æ–°å‹çš„å»ä¸­å¿ƒåŒ–æµç®—æ³•ï¼Œè¯¥ç®—æ³•å¯æ‰¾åˆ°æœ€æœ‰æ•ˆçš„è·¯ç”±ï¼Œä»¥å°½å¯èƒ½ä½çš„å»¶è¿Ÿæœ€å¤§åŒ–è®­ç»ƒçš„å¾®æ‰¹æ¬¡æ•°é‡ã€‚æˆ‘ä»¬å¯¹GWTFåœ¨GPTå’ŒLLaMaç±»æ¨¡å‹ä¸Šè¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ï¼Œå¹¶å°†å…¶ä¸ç°æœ‰æŠ€æœ¯è¿›è¡Œäº†æ¯”è¾ƒã€‚ç»“æœè¡¨æ˜ï¼Œåœ¨æ¶‰åŠåˆ†å¸ƒåœ¨1 0ä¸ªä¸åŒåœ°ç†ä½ç½®çš„å¼‚è´¨å®¢æˆ·ç«¯èŠ‚ç‚¹ä¸”èŠ‚ç‚¹æ›´æ¢ç‡é«˜çš„ç°å®å’Œå…·æœ‰æŒ‘æˆ˜æ€§çš„åœºæ™¯ä¸­ï¼ŒGWTFå¯å°†è®­ç»ƒæ—¶é—´ç¼©çŸ­é«˜è¾¾45%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.21221v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å…´èµ·åŠå…¶å¯¹æ°‘ä¸»åŒ–è®­ç»ƒçš„é‡è¦æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†GWTFï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªé€‚ç”¨äºLLMçš„é²æ£’æ€§å®ç”¨åˆ†æ•£åŒ–è®­ç»ƒæ¡†æ¶ã€‚GWTFèƒ½å¤Ÿå®ç°åœ¨å¼‚æ„å®¢æˆ·ç«¯ä¸Šé«˜æ•ˆååŒè®­ç»ƒLLMï¼Œä¸å…¶ä»–ç°æœ‰çš„åˆ†å¸ƒå¼å’Œè”é‚¦è®­ç»ƒæ¡†æ¶ä¸åŒã€‚æ­¤å¤–ï¼ŒGWTFè§£å†³äº†èŠ‚ç‚¹æµå¤±å’Œç½‘ç»œä¸ç¨³å®šé—®é¢˜ã€‚å…¶æ ¸å¿ƒæ˜¯ä¸€ç§æ–°å‹çš„å»ä¸­å¿ƒåŒ–æµç®—æ³•ï¼Œè¯¥ç®—æ³•å¯æ‰¾åˆ°æœ€æœ‰æ•ˆçš„è·¯ç”±ï¼Œä»¥æœ€ä½å»¶è¿Ÿè®­ç»ƒæœ€å¤šçš„å¾®æ‰¹æ¬¡ã€‚æˆ‘ä»¬åœ¨GPTå’ŒLLaMaæ¨¡å‹ä¸Šå¯¹GWTFè¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ï¼Œå¹¶ä¸ç°æœ‰æŠ€æœ¯è¿›è¡Œäº†æ¯”è¾ƒã€‚ç»“æœè¡¨æ˜ï¼Œåœ¨æ¶‰åŠåˆ†å¸ƒåœ¨10ä¸ªä¸åŒåœ°ç†ä½ç½®çš„å¼‚æ„å®¢æˆ·ç«¯èŠ‚ç‚¹ä¸”èŠ‚ç‚¹æµå¤±ç‡é«˜çš„ç°å®å’Œå…·æœ‰æŒ‘æˆ˜æ€§çš„åœºæ™¯ä¸­ï¼ŒGWTFå¯å°†è®­ç»ƒæ—¶é—´ç¼©çŸ­é«˜è¾¾45%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GWTFæ˜¯ç¬¬ä¸€ä¸ªé’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é²æ£’æ€§å®ç”¨åˆ†æ•£åŒ–è®­ç»ƒæ¡†æ¶ã€‚</li>
<li>GWTFèƒ½åœ¨å¼‚æ„å®¢æˆ·ç«¯ä¸Šå®ç°LLMçš„é«˜æ•ˆååŒè®­ç»ƒã€‚</li>
<li>GWTFè§£å†³äº†èŠ‚ç‚¹æµå¤±å’Œç½‘ç»œä¸ç¨³å®šé—®é¢˜ã€‚</li>
<li>GWTFçš„æ ¸å¿ƒæ˜¯ä¸€ç§æ–°å‹çš„å»ä¸­å¿ƒåŒ–æµç®—æ³•ï¼Œèƒ½æ‰¾æœ€æœ‰æ•ˆè·¯ç”±å¹¶æœ€å°åŒ–è®­ç»ƒå»¶è¿Ÿã€‚</li>
<li>åœ¨æ¶‰åŠåˆ†å¸ƒåœ¨å¤šä¸ªåœ°ç†ä½ç½®çš„å¼‚æ„å®¢æˆ·ç«¯èŠ‚ç‚¹çš„ç°å®å’Œå…·æœ‰æŒ‘æˆ˜æ€§çš„åœºæ™¯ä¸­ï¼ŒGWTFå¯å°†è®­ç»ƒæ—¶é—´ç¼©çŸ­é«˜è¾¾45%ã€‚</li>
<li>GWTFåœ¨GPTå’ŒLLaMaæ¨¡å‹ä¸Šè¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.21221">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_LLM/2509.21221v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_LLM/2509.21221v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_LLM/2509.21221v1/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_LLM/2509.21221v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_LLM/2509.21221v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Dual-Path-Phishing-Detection-Integrating-Transformer-Based-NLP-with-Structural-URL-Analysis"><a href="#Dual-Path-Phishing-Detection-Integrating-Transformer-Based-NLP-with-Structural-URL-Analysis" class="headerlink" title="Dual-Path Phishing Detection: Integrating Transformer-Based NLP with   Structural URL Analysis"></a>Dual-Path Phishing Detection: Integrating Transformer-Based NLP with   Structural URL Analysis</h2><p><strong>Authors:Ibrahim Altan, Abdulla Bachir, Yousuf Parbhulkar, Abdul Muksith Rizvi, Moshiur Farazi</strong></p>
<p>Phishing emails pose a persistent and increasingly sophisticated threat, undermining email security through deceptive tactics designed to exploit both semantic and structural vulnerabilities. Traditional detection methods, often based on isolated analysis of email content or embedded URLs, fail to comprehensively address these evolving attacks. In this paper, we propose a dual-path phishing detection framework that integrates transformer-based natural language processing (NLP) with classical machine learning to jointly analyze email text and embedded URLs. Our approach leverages the complementary strengths of semantic analysis using fine-tuned transformer architectures (e.g., DistilBERT) and structural link analysis via character-level TF-IDF vectorization paired with classical classifiers (e.g., Random Forest). Empirical evaluation on representative email and URL datasets demonstrates that this combined approach significantly improves detection accuracy. Specifically, the DistilBERT model achieves a near-optimal balance between accuracy and computational efficiency for textual phishing detection, while Random Forest notably outperforms other classical classifiers in identifying malicious URLs. The modular design allows flexibility for standalone deployment or ensemble integration, facilitating real-world adoption. Collectively, our results highlight the efficacy and practical value of this dual-path approach, establishing a scalable, accurate, and interpretable solution capable of enhancing email security against contemporary phishing threats. </p>
<blockquote>
<p>ç½‘ç»œé’“é±¼é‚®ä»¶æ„æˆäº†ä¸€ç§æŒç»­ä¸”æ—¥ç›Šç²¾æ˜çš„å¨èƒï¼Œå®ƒé€šè¿‡åˆ©ç”¨è¯­ä¹‰å’Œç»“æ„æ€§æ¼æ´çš„æ¬ºéª—ç­–ç•¥æ¥ç ´åç”µå­é‚®ä»¶çš„å®‰å…¨æ€§ã€‚ä¼ ç»Ÿçš„æ£€æµ‹æ–¹æ³•é€šå¸¸åŸºäºç”µå­é‚®ä»¶å†…å®¹æˆ–åµŒå…¥é“¾æ¥çš„å­¤ç«‹åˆ†æï¼Œæ— æ³•å…¨é¢åº”å¯¹è¿™äº›ä¸æ–­æ¼”å˜çš„æ”»å‡»ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŒè·¯å¾„ç½‘ç»œé’“é±¼æ£€æµ‹æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†åŸºäºè½¬æ¢å™¨çš„è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä¸ç»å…¸æœºå™¨å­¦ä¹ ï¼Œä»¥è”åˆåˆ†æç”µå­é‚®ä»¶æ–‡æœ¬å’ŒåµŒå…¥é“¾æ¥ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨ä½¿ç”¨ç²¾ç»†è°ƒæ•´çš„è½¬æ¢å™¨æ¶æ„ï¼ˆä¾‹å¦‚DistilBERTï¼‰è¿›è¡Œè¯­ä¹‰åˆ†æçš„äº’è¡¥ä¼˜åŠ¿ï¼Œå¹¶é€šè¿‡å­—ç¬¦çº§TF-IDFå‘é‡åŒ–ä¸ç»å…¸åˆ†ç±»å™¨ï¼ˆä¾‹å¦‚éšæœºæ£®æ—ï¼‰é…å¯¹è¿›è¡Œç»“æ„é“¾æ¥åˆ†æã€‚åœ¨ä»£è¡¨æ€§çš„ç”µå­é‚®ä»¶å’Œé“¾æ¥æ•°æ®é›†ä¸Šçš„ç»éªŒè¯„ä¼°è¡¨æ˜ï¼Œè¿™ç§ç»„åˆæ–¹æ³•å¯æ˜¾è‘—æé«˜æ£€æµ‹ç²¾åº¦ã€‚å…·ä½“è€Œè¨€ï¼ŒDistilBERTæ¨¡å‹åœ¨æ–‡æœ¬é’“é±¼æ£€æµ‹æ–¹é¢å®ç°äº†ç²¾åº¦å’Œè®¡ç®—æ•ˆç‡ä¹‹é—´çš„è¿‘ä¹æœ€ä½³çš„å¹³è¡¡ï¼Œè€Œéšæœºæ£®æ—åœ¨è¯†åˆ«æ¶æ„é“¾æ¥æ–¹é¢æ˜¾è‘—ä¼˜äºå…¶ä»–ç»å…¸åˆ†ç±»å™¨ã€‚æ¨¡å—åŒ–è®¾è®¡å…è®¸è¿›è¡Œç‹¬ç«‹éƒ¨ç½²æˆ–é›†æˆç»„åˆï¼Œä¿ƒè¿›äº†å®é™…åº”ç”¨ä¸­çš„é‡‡ç”¨ã€‚æ€»ä½“è€Œè¨€ï¼Œæˆ‘ä»¬çš„ç»“æœçªå‡ºäº†åŒè·¯å¾„æ–¹æ³•çš„æ•ˆç”¨å’Œå®é™…ä»·å€¼ï¼Œå»ºç«‹äº†ä¸€ç§å¯æ‰©å±•ã€å‡†ç¡®ä¸”å¯è§£é‡Šçš„è§£å†³æ–¹æ¡ˆï¼Œèƒ½å¤Ÿå¢å¼ºç”µå­é‚®ä»¶å®‰å…¨æ€§ä»¥åº”å¯¹å½“å‰çš„é’“é±¼å¨èƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.20972v1">PDF</a> Paper accepted for presentation at the ACS&#x2F;IEEE 22nd International   Conference on Computer Systems and Applications (AICCSA 2025)</p>
<p><strong>Summary</strong>ï¼šé’“é±¼é‚®ä»¶æŒç»­æ„æˆå¨èƒï¼Œä¸”æ‰‹æ®µæ—¥ç›Šç‹¡çŒ¾ï¼Œé€šè¿‡è¯­ä¹‰å’Œç»“æ„ä¸Šçš„æ¼æ´è¿›è¡Œæ¬ºéª—æ€§æ”»å‡»ï¼Œå‰Šå¼±ç”µå­é‚®ä»¶çš„å®‰å…¨æ€§ã€‚ä¼ ç»Ÿæ£€æµ‹æ–¹å¼å¾€å¾€å±€é™äºé‚®ä»¶å†…å®¹æˆ–åµŒå…¥é“¾æ¥çš„å•ç‹¬åˆ†æï¼Œéš¾ä»¥åº”å¯¹æ—¥ç›Šå¤æ‚çš„æ”»å‡»ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŒè·¯å¾„é’“é±¼é‚®ä»¶æ£€æµ‹æ¡†æ¶ï¼Œèåˆäº†åŸºäºè½¬æ¢å™¨çš„è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰å’Œç»å…¸æœºå™¨å­¦ä¹ æŠ€æœ¯ï¼ŒåŒæ—¶åˆ†æé‚®ä»¶æ–‡æœ¬å’ŒåµŒå…¥é“¾æ¥ã€‚è¯¥æ–¹æ³•ç»“åˆäº†å¾®è°ƒè½¬æ¢å™¨æ¶æ„ï¼ˆå¦‚DistilBERTï¼‰çš„è¯­ä¹‰åˆ†æå’ŒåŸºäºå­—ç¬¦çº§åˆ«çš„TF-IDFå‘é‡åŒ–çš„ç»“æ„é“¾æ¥åˆ†æï¼Œé…åˆç»å…¸åˆ†ç±»å™¨ï¼ˆå¦‚éšæœºæ£®æ—ï¼‰ã€‚åœ¨å…·æœ‰ä»£è¡¨æ€§çš„ç”µå­é‚®ä»¶å’ŒURLæ•°æ®é›†ä¸Šçš„å®è¯è¯„ä¼°è¡¨æ˜ï¼Œè¯¥ç»„åˆæ–¹æ³•æ˜¾è‘—æé«˜äº†æ£€æµ‹ç²¾åº¦ã€‚å…·ä½“æ¥è¯´ï¼ŒDistilBERTæ¨¡å‹åœ¨å‡†ç¡®ç‡å’Œè®¡ç®—æ•ˆç‡ä¹‹é—´å®ç°äº†è¿‘ä¹æœ€ä¼˜çš„å¹³è¡¡ï¼Œè€Œéšæœºæ£®æ—åœ¨è¯†åˆ«æ¶æ„URLæ–¹é¢è¡¨ç°å°¤ä¸ºå‡ºè‰²ã€‚æœ¬è®¾è®¡å…·æœ‰æ¨¡å—åŒ–ç‰¹ç‚¹ï¼Œå¯çµæ´»éƒ¨ç½²æˆ–é›†æˆï¼Œå…·æœ‰å®é™…åº”ç”¨ä»·å€¼ã€‚æ€»ä½“è€Œè¨€ï¼Œæœ¬ç ”ç©¶ç»“æœçªæ˜¾äº†åŒè·¯å¾„æ–¹æ³•çš„æ•ˆèƒ½å’Œå®ç”¨æ€§ï¼Œå»ºç«‹äº†ä¸€ç§å¯æ‰©å±•ã€å‡†ç¡®ä¸”å¯è§£é‡Šçš„è§£å†³æ–¹æ¡ˆï¼Œå¯å¢å¼ºç”µå­é‚®ä»¶å¯¹ç°ä»£é’“é±¼å¨èƒçš„å®‰å…¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>é’“é±¼é‚®ä»¶å¨èƒæŒç»­å­˜åœ¨ä¸”ä¸æ–­è¿›åŒ–ï¼Œé€šè¿‡è¯­ä¹‰å’Œç»“æ„æ¼æ´å‰Šå¼±ç”µå­é‚®ä»¶å®‰å…¨æ€§ã€‚</li>
<li>ä¼ ç»Ÿæ£€æµ‹æ‰‹æ®µå¾€å¾€å±€é™äºå†…å®¹æˆ–é“¾æ¥çš„å•ç‹¬åˆ†æï¼Œéš¾ä»¥åº”å¯¹å½“å‰å¨èƒã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŒè·¯å¾„æ£€æµ‹æ¡†æ¶ï¼Œèåˆè‡ªç„¶è¯­è¨€å¤„ç†å’Œç»å…¸æœºå™¨å­¦ä¹ æŠ€æœ¯ã€‚</li>
<li>è¯¥æ–¹æ³•ç»“åˆäº†è¯­ä¹‰åˆ†æå’Œç»“æ„é“¾æ¥åˆ†æï¼Œé€šè¿‡DistilBERTæ¨¡å‹å’Œéšæœºæ£®æ—åˆ†ç±»å™¨å®ç°é«˜æ•ˆæ£€æµ‹ã€‚</li>
<li>DistilBERTæ¨¡å‹åœ¨å‡†ç¡®ç‡å’Œè®¡ç®—æ•ˆç‡ä¹‹é—´å–å¾—å¹³è¡¡ï¼Œéšæœºæ£®æ—åœ¨è¯†åˆ«æ¶æ„URLæ–¹é¢è¡¨ç°ä¼˜ç§€ã€‚</li>
<li>æ¨¡å—åŒ–è®¾è®¡å¯çµæ´»éƒ¨ç½²æˆ–é›†æˆï¼Œå…·æœ‰å®é™…åº”ç”¨ä»·å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.20972">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_LLM/2509.20972v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_LLM/2509.20972v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_LLM/2509.20972v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_LLM/2509.20972v1/page_4_1.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Hierarchical-Resolution-Transformers-A-Wavelet-Inspired-Architecture-for-Multi-Scale-Language-Understanding"><a href="#Hierarchical-Resolution-Transformers-A-Wavelet-Inspired-Architecture-for-Multi-Scale-Language-Understanding" class="headerlink" title="Hierarchical Resolution Transformers: A Wavelet-Inspired Architecture   for Multi-Scale Language Understanding"></a>Hierarchical Resolution Transformers: A Wavelet-Inspired Architecture   for Multi-Scale Language Understanding</h2><p><strong>Authors:Ayan Sar, Sampurna Roy, Kanav Gupta, Anurag Kaushish, Tanupriya Choudhury, Abhijit Kumar</strong></p>
<p>Transformer architectures have achieved state-of-the-art performance across natural language tasks, yet they fundamentally misrepresent the hierarchical nature of human language by processing text as flat token sequences. This results in quadratic computational cost, weak computational cost, weak compositional generalization, and inadequate discourse-level modeling. We propose Hierarchical Resolution Transformer (HRT), a novel wavelet-inspired neural architecture that processes language simultaneously across multiple resolutions, from characters to discourse-level units. HRT constructs a multi-resolution attention, enabling bottom-up composition and top-down contextualization. By employing exponential sequence reduction across scales, HRT achieves O(nlogn) complexity, offering significant efficiency improvements over standard transformers. We evaluated HRT on a diverse suite of benchmarks, including GLUE, SuperGLUE, Long Range Arena, and WikiText-103, and results demonstrated that HRT outperforms standard transformer baselines by an average of +3.8% on GLUE, +4.5% on SuperGLUE, and +6.1% on Long Range Arena, while reducing memory usage by 42% and inference latency by 37% compared to BERT and GPT style models of similar parameter count. Ablation studies confirm the effectiveness of cross-resolution attention and scale-specialized modules, showing that each contributes independently to both efficiency and accuracy. Our findings establish HRT as the first architecture to align computational structure with the hierarchical organization of human language, demonstrating that multi-scale, wavelet-inspired processing yields both theoretical efficiency gains and practical improvements in language understanding. </p>
<blockquote>
<p>Transformeræ¶æ„åœ¨è‡ªç„¶è¯­è¨€ä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä½†å®ƒä»¬é€šè¿‡å¤„ç†æ‰å¹³çš„ä»¤ç‰Œåºåˆ—æ¥æ ¹æœ¬æ€§åœ°è¯¯ä»£è¡¨äººç±»è¯­è¨€çš„å±‚æ¬¡ç»“æ„ã€‚è¿™å¯¼è‡´äº†äºŒæ¬¡æ–¹çš„è®¡ç®—æˆæœ¬ã€è¾ƒå¼±çš„è®¡ç®—æˆæœ¬å’Œä¸è¶³çš„è¯­å¢ƒå»ºæ¨¡ã€‚æˆ‘ä»¬æå‡ºäº†åˆ†å±‚è§£æTransformerï¼ˆHRTï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å—å°æ³¢å¯å‘çš„æ–°å‹ç¥ç»ç½‘ç»œæ¶æ„ï¼Œèƒ½å¤ŸåŒæ—¶å¤„ç†å¤šä¸ªåˆ†è¾¨ç‡çš„è¯­è¨€ï¼Œä»å­—ç¬¦åˆ°è¯­å¢ƒçº§å•ä½ã€‚HRTæ„å»ºäº†å¤šåˆ†è¾¨ç‡æ³¨æ„åŠ›æœºåˆ¶ï¼Œå®ç°äº†è‡ªä¸‹è€Œä¸Šçš„ç»„åˆå’Œè‡ªä¸Šè€Œä¸‹çš„è¯­å¢ƒåŒ–ã€‚é€šè¿‡é‡‡ç”¨è·¨å°ºåº¦çš„æŒ‡æ•°åºåˆ—ç¼©å‡ï¼ŒHRTå®ç°äº†O(nlogn)çš„å¤æ‚åº¦ï¼Œç›¸è¾ƒäºæ ‡å‡†Transformeræœ‰ç€æ˜¾è‘—çš„æ•ˆç‡æå‡ã€‚æˆ‘ä»¬åœ¨GLUEã€SuperGLUEã€Long Range Arenaå’ŒWikiText-103ç­‰å¤šç§åŸºå‡†æµ‹è¯•é›†ä¸Šè¯„ä¼°äº†HRTï¼Œç»“æœè¡¨æ˜ï¼ŒHRTåœ¨GLUEä¸Šçš„å¹³å‡æ€§èƒ½è¾ƒæ ‡å‡†TransformeråŸºçº¿æé«˜äº†+3.8%ï¼Œåœ¨SuperGLUEä¸Šæé«˜äº†+4.5%ï¼Œåœ¨Long Range Arenaä¸Šæé«˜äº†+6.1%ï¼ŒåŒæ—¶ç›¸è¾ƒäºç±»ä¼¼å‚æ•°æ•°é‡çš„BERTå’ŒGPTé£æ ¼æ¨¡å‹ï¼Œå†…å­˜ä½¿ç”¨ç‡é™ä½äº†42%ï¼Œæ¨ç†å»¶è¿Ÿé™ä½äº†37%ã€‚æ¶ˆèç ”ç©¶è¯å®äº†è·¨åˆ†è¾¨ç‡æ³¨æ„åŠ›å’Œå°ºåº¦ä¸“ä¸šåŒ–æ¨¡å—çš„æœ‰æ•ˆæ€§ï¼Œè¡¨æ˜å®ƒä»¬å„è‡ªå¯¹æ•ˆç‡å’Œå‡†ç¡®æ€§éƒ½æœ‰ç‹¬ç«‹çš„è´¡çŒ®ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç¡®ç«‹äº†HRTä½œä¸ºé¦–ä¸ªå°†è®¡ç®—ç»“æ„ä¸äººç±»è¯­è¨€çš„å±‚æ¬¡ç»“æ„ç›¸åŒ¹é…çš„æ¶æ„ï¼Œè¯æ˜äº†å¤šå°ºåº¦ã€å—å°æ³¢å¯å‘çš„å¤„ç†æ—¢å¸¦æ¥äº†ç†è®ºä¸Šçš„æ•ˆç‡æå‡ï¼Œä¹Ÿåœ¨è¯­è¨€ç†è§£æ–¹é¢å–å¾—äº†å®é™…çš„æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.20581v1">PDF</a> Submitted in IEEE International Conference on Big Data 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Transformeræ¶æ„åœ¨è‡ªç„¶è¯­è¨€ä»»åŠ¡ä¸­çš„æœ€æ–°è¡¨ç°ï¼Œä½†å…¶å¯¹æ–‡æœ¬çš„å¤„ç†æ–¹å¼å¿½ç•¥äº†äººç±»è¯­è¨€çš„å±‚æ¬¡ç»“æ„ï¼Œå¯¼è‡´è®¡ç®—æˆæœ¬è¾ƒé«˜ã€ç»„æˆæ³›åŒ–èƒ½åŠ›å¼±å’Œè¯è¯­çº§åˆ«çš„å»ºæ¨¡ä¸è¶³ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†åŸºäºå°æ³¢å¯å‘çš„ç¥ç»ç½‘ç»œæ¶æ„â€”â€”åˆ†å±‚è§£æTransformerï¼ˆHRTï¼‰ã€‚HRTèƒ½å¤ŸåŒæ—¶å¤„ç†å¤šä¸ªåˆ†è¾¨ç‡çš„è¯­è¨€ï¼Œä»å­—ç¬¦åˆ°è¯è¯­çº§åˆ«å•ä½ã€‚é€šè¿‡æ„å»ºå¤šåˆ†è¾¨ç‡æ³¨æ„åŠ›æœºåˆ¶å’Œå®ç°å°ºåº¦ä¸“ç”¨çš„æ¨¡å—ï¼ŒHRTå®ç°äº†O(nlogn)çš„å¤æ‚åº¦ï¼Œç›¸è¾ƒäºæ ‡å‡†Transformerï¼Œå…·æœ‰æ›´é«˜çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒHRTè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼ŒåŒ…æ‹¬GLUEã€SuperGLUEã€Long Range Arenaå’ŒWikiText-103ç­‰ä»»åŠ¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Transformeræ¶æ„åœ¨è‡ªç„¶è¯­è¨€ä»»åŠ¡ä¸­è¡¨ç°å“è¶Šï¼Œä½†å¿½ç•¥äº†äººç±»è¯­è¨€çš„å±‚æ¬¡ç»“æ„ã€‚</li>
<li>åˆ†å±‚è§£æTransformerï¼ˆHRTï¼‰æ˜¯ä¸€ç§åŸºäºå°æ³¢å¯å‘çš„ç¥ç»ç½‘ç»œæ¶æ„ï¼Œèƒ½åŒæ—¶å¤„ç†å¤šä¸ªåˆ†è¾¨ç‡çš„è¯­è¨€ã€‚</li>
<li>HRTé€šè¿‡æ„å»ºå¤šåˆ†è¾¨ç‡æ³¨æ„åŠ›å’Œå°ºåº¦ä¸“ç”¨çš„æ¨¡å—ï¼Œå®ç°äº†O(nlogn)çš„å¤æ‚åº¦ï¼Œæé«˜äº†æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚</li>
<li>HRTåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼ŒåŒ…æ‹¬GLUEã€SuperGLUEã€Long Range Arenaå’ŒWikiText-103ç­‰ä»»åŠ¡ã€‚</li>
<li>ä¸æ ‡å‡†Transformerç›¸æ¯”ï¼ŒHRTé™ä½äº†å†…å­˜ä½¿ç”¨é‡å’Œæ¨ç†å»¶è¿Ÿã€‚</li>
<li>æ¶ˆèç ”ç©¶è¯å®äº†è·¨åˆ†è¾¨ç‡æ³¨æ„åŠ›å’Œå°ºåº¦ä¸“ç”¨æ¨¡å—çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.20581">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_LLM/2509.20581v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_LLM/2509.20581v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_LLM/2509.20581v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_LLM/2509.20581v1/page_5_1.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Causal-Understanding-by-LLMs-The-Role-of-Uncertainty"><a href="#Causal-Understanding-by-LLMs-The-Role-of-Uncertainty" class="headerlink" title="Causal Understanding by LLMs: The Role of Uncertainty"></a>Causal Understanding by LLMs: The Role of Uncertainty</h2><p><strong>Authors:Oscar Lithgow-Serrano, Vani Kanjirangat, Alessandro Antonucci</strong></p>
<p>Recent papers show LLMs achieve near-random accuracy in causal relation classification, raising questions about whether such failures arise from limited pretraining exposure or deeper representational gaps. We investigate this under uncertainty-based evaluation, testing whether pretraining exposure to causal examples improves causal understanding &gt;18K PubMed sentences â€“ half from The Pile corpus, half post-2024 â€“ across seven models (Pythia-1.4B&#x2F;7B&#x2F;12B, GPT-J-6B, Dolly-7B&#x2F;12B, Qwen-7B). We analyze model behavior through: (i) causal classification, where the model identifies causal relationships in text, and (ii) verbatim memorization probing, where we assess whether the model prefers previously seen causal statements over their paraphrases. Models perform four-way classification (direct&#x2F;conditional&#x2F;correlational&#x2F;no-relationship) and select between originals and their generated paraphrases. Results show almost identical accuracy on seen&#x2F;unseen sentences (p &gt; 0.05), no memorization bias (24.8% original selection), and output distribution over the possible options is almost flat, with entropic values near the maximum (1.35&#x2F;1.39), confirming random guessing. Instruction-tuned models show severe miscalibration (Qwen: &gt; 95% confidence, 32.8% accuracy, ECE&#x3D;0.49). Conditional relations induce highest entropy (+11% vs. direct). These findings suggest that failures in causal understanding arise from the lack of structured causal representation, rather than insufficient exposure to causal examples during pretraining. </p>
<blockquote>
<p>æœ€è¿‘çš„è®ºæ–‡æ˜¾ç¤ºï¼Œå¤§å‹è¯­è¨€æ¨¡å‹åœ¨å› æœå…³ç³»åˆ†ç±»ä¸­è¾¾åˆ°äº†è¿‘ä¹éšæœºçš„å‡†ç¡®ç‡ï¼Œè¿™å¼•å‘äº†å…³äºè¿™äº›å¤±è´¥æ˜¯å¦æºäºæœ‰é™çš„é¢„è®­ç»ƒæš´éœ²æˆ–æ›´æ·±å±‚æ¬¡çš„ä»£è¡¨æ€§å·®è·çš„ç–‘é—®ã€‚æˆ‘ä»¬åœ¨åŸºäºä¸ç¡®å®šæ€§çš„è¯„ä¼°ä¸‹å¯¹æ­¤è¿›è¡Œè°ƒæŸ¥ï¼Œæµ‹è¯•é¢„è®­ç»ƒæ—¶æ¥è§¦å› æœä¾‹å­æ˜¯å¦èƒ½æé«˜å› æœç†è§£ã€‚æˆ‘ä»¬å¯¹è¶…è¿‡18,000å¥æ¥è‡ªPubMedçš„è¯­å¥è¿›è¡Œäº†å®éªŒï¼Œå…¶ä¸­ä¸€åŠæ¥è‡ªPileè¯­æ–™åº“ï¼Œä¸€åŠæ˜¯2024å¹´ä»¥åçš„æ•°æ®ï¼Œæ¶µç›–äº†ä¸ƒä¸ªæ¨¡å‹ï¼ˆPythia-1.4B&#x2F;7B&#x2F;12Bã€GPT-J-6Bã€Dolly-7B&#x2F;12Bå’ŒQwen-7Bï¼‰ã€‚æˆ‘ä»¬é€šè¿‡åˆ†ææ¨¡å‹çš„è¡Œä¸ºæ¥æ¢ç©¶ç»“æœï¼š(i)å› æœåˆ†ç±»ï¼Œå³æ¨¡å‹è¯†åˆ«æ–‡æœ¬ä¸­çš„å› æœå…³ç³»ï¼›(ii)é€å­—è®°å¿†æ¢æµ‹ï¼Œå³æˆ‘ä»¬è¯„ä¼°æ¨¡å‹æ˜¯åå¥½ä¹‹å‰è§è¿‡çš„å› æœé™ˆè¿°è¿˜æ˜¯å®ƒä»¬çš„åŒä¹‰æ›¿æ¢ã€‚æ¨¡å‹æ‰§è¡Œå››é¡¹åˆ†ç±»ï¼ˆç›´æ¥&#x2F;æ¡ä»¶&#x2F;ç›¸å…³æ€§&#x2F;æ— å…³ç³»ï¼‰ï¼Œå¹¶åœ¨åŸå§‹å¥å­å’Œå…¶ç”Ÿæˆçš„åŒä¹‰æ›¿æ¢å¥å­ä¹‹é—´è¿›è¡Œé€‰æ‹©ã€‚ç»“æœæ˜¾ç¤ºï¼Œåœ¨å·²è§&#x2F;æœªè§å¥å­ä¸Šçš„å‡†ç¡®ç‡å‡ ä¹ç›¸åŒï¼ˆp &gt; 0.05ï¼‰ï¼Œæ²¡æœ‰è®°å¿†åè§ï¼ˆ24.8%é€‰æ‹©åŸå§‹å¥å­ï¼‰ï¼Œå¹¶ä¸”è¾“å‡ºåˆ†å¸ƒå‡ ä¹æ‰å¹³åŒ–ï¼Œç†µå€¼æ¥è¿‘æœ€å¤§å€¼ï¼ˆ1.35&#x2F;1.39ï¼‰ï¼Œè¯å®äº†éšæœºçŒœæµ‹ã€‚æŒ‡ä»¤è°ƒæ•´æ¨¡å‹æ˜¾ç¤ºå‡ºä¸¥é‡çš„æ ¡å‡†è¯¯å·®ï¼ˆQwenï¼š&gt; 95%çš„ä¿¡å¿ƒï¼Œå‡†ç¡®ç‡ä»…ä¸º32.8%ï¼ŒECE&#x3D;0.49ï¼‰ã€‚æ¡ä»¶å…³ç³»å¼•èµ·æœ€é«˜çš„ç†µå€¼ï¼ˆä¸ç›´æ¥ç›¸æ¯”å¢åŠ 11%ï¼‰ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œå› æœç†è§£å¤±è´¥æºäºç¼ºä¹ç»“æ„åŒ–å› æœè¡¨å¾ï¼Œè€Œéé¢„è®­ç»ƒæœŸé—´æ¥è§¦åˆ°çš„å› æœç¤ºä¾‹ä¸è¶³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.20088v1">PDF</a> Accepted in second UncertaiNLP workshop at EMNLP 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å› æœå…³è”åˆ†ç±»ä¸­è¾¾åˆ°è¿‘ä¼¼éšæœºå‡†ç¡®æ€§çš„é—®é¢˜ï¼Œè´¨ç–‘æ˜¯å¦æºäºé¢„è®­ç»ƒæš´éœ²çš„æœ‰é™æ€§æˆ–æ›´æ·±å±‚æ¬¡çš„è¡¨å¾å·®è·ã€‚é€šè¿‡ä¸ç¡®å®šæ€§è¯„ä¼°æ–¹æ³•ï¼Œç ”ç©¶äº†é¢„è®­ç»ƒæœŸé—´æ¥è§¦å› æœç¤ºä¾‹æ˜¯å¦æœ‰åŠ©äºæ”¹å–„æ¨¡å‹çš„å› æœç†è§£ã€‚ç ”ç©¶ç»“æœæŒ‡å‡ºï¼Œæ¨¡å‹åœ¨å·²è§å’Œæœªè§å¥å­ä¸Šçš„å‡†ç¡®ç‡å‡ ä¹ç›¸åŒï¼Œæ— è®°å¿†åè§ï¼Œä¸”è¾“å‡ºåˆ†å¸ƒæ¥è¿‘å¹³å¦ï¼Œæœ€å¤§ç†µå€¼æ¥è¿‘ï¼Œè¡¨æ˜æ¨¡å‹è¡¨ç°æ¥è¿‘éšæœºçŒœæµ‹ã€‚æŒ‡ä»¤å¾®è°ƒæ¨¡å‹å­˜åœ¨ä¸¥é‡æ ¡å‡†å¤±è¯¯ã€‚è¿™äº›å‘ç°æš—ç¤ºï¼Œå› æœç†è§£çš„å¤±è´¥æºäºç¼ºä¹ç»“æ„åŒ–å› æœè¡¨å¾ï¼Œè€Œéé¢„è®­ç»ƒæœŸé—´æ¥è§¦åˆ°çš„å› æœç¤ºä¾‹ä¸è¶³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨å› æœå…³è”åˆ†ç±»ä¸­çš„è¡¨ç°æ¥è¿‘éšæœºçŒœæµ‹ï¼Œå¼•å‘å…³äºå…¶å¤±è´¥åŸå› çš„è´¨ç–‘ã€‚</li>
<li>ç ”ç©¶é€šè¿‡ä¸ç¡®å®šæ€§è¯„ä¼°æ–¹æ³•ï¼Œæ¢ç©¶é¢„è®­ç»ƒæœŸé—´æ¥è§¦å› æœç¤ºä¾‹å¯¹æ¨¡å‹å› æœç†è§£çš„å½±å“ã€‚</li>
<li>æ¨¡å‹åœ¨å·²è§å’Œæœªè§å¥å­ä¸Šçš„å‡†ç¡®ç‡ç›¸ä¼¼ï¼Œè¡¨æ˜æ¨¡å‹çš„è¡¨ç°ä¸æ˜¯åŸºäºé¢„è®­ç»ƒæš´éœ²çš„ç†Ÿæ‚‰ç¨‹åº¦ã€‚</li>
<li>æ— è®°å¿†åè§çš„ç»“æœè¡¨æ˜æ¨¡å‹ä¸åå¥½ä¹‹å‰çš„å› æœé™ˆè¿°ï¼Œè€Œæ˜¯æ ¹æ®æ–‡æœ¬å†…å®¹åšå‡ºåˆ¤æ–­ã€‚</li>
<li>è¾“å‡ºåˆ†å¸ƒæ¥è¿‘å¹³å¦å’Œé«˜ç†µå€¼æš—ç¤ºæ¨¡å‹åœ¨åˆ†ç±»ä»»åŠ¡ä¸­è¡¨ç°éšæœºï¼Œç¼ºä¹æ˜ç¡®çš„å†³ç­–è¾¹ç•Œã€‚</li>
<li>æŒ‡ä»¤å¾®è°ƒæ¨¡å‹çš„æ ¡å‡†é—®é¢˜å¯èƒ½æ˜¯å½±å“å…¶æ€§èƒ½çš„å…³é”®å› ç´ ä¹‹ä¸€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.20088">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_LLM/2509.20088v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_LLM/2509.20088v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_LLM/2509.20088v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_LLM/2509.20088v1/page_5_1.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="SINAI-at-eRisk-CLEF-2025-Transformer-Based-and-Conversational-Strategies-for-Depression-Detection"><a href="#SINAI-at-eRisk-CLEF-2025-Transformer-Based-and-Conversational-Strategies-for-Depression-Detection" class="headerlink" title="SINAI at eRisk@CLEF 2025: Transformer-Based and Conversational   Strategies for Depression Detection"></a>SINAI at eRisk@CLEF 2025: Transformer-Based and Conversational   Strategies for Depression Detection</h2><p><strong>Authors:Alba Maria Marmol-Romero, Manuel Garcia-Vega, Miguel Angel Garcia-Cumbreras, Arturo Montejo-Raez</strong></p>
<p>This paper describes the participation of the SINAI-UJA team in the eRisk@CLEF 2025 lab. Specifically, we addressed two of the proposed tasks: (i) Task 2: Contextualized Early Detection of Depression, and (ii) Pilot Task: Conversational Depression Detection via LLMs. Our approach for Task 2 combines an extensive preprocessing pipeline with the use of several transformer-based models, such as RoBERTa Base or MentalRoBERTA Large, to capture the contextual and sequential nature of multi-user conversations. For the Pilot Task, we designed a set of conversational strategies to interact with LLM-powered personas, focusing on maximizing information gain within a limited number of dialogue turns. In Task 2, our system ranked 8th out of 12 participating teams based on F1 score. However, a deeper analysis revealed that our models were among the fastest in issuing early predictions, which is a critical factor in real-world deployment scenarios. This highlights the trade-off between early detection and classification accuracy, suggesting potential avenues for optimizing both jointly in future work. In the Pilot Task, we achieved 1st place out of 5 teams, obtaining the best overall performance across all evaluation metrics: DCHR, ADODL and ASHR. Our success in this task demonstrates the effectiveness of structured conversational design when combined with powerful language models, reinforcing the feasibility of deploying LLMs in sensitive mental health assessment contexts. </p>
<blockquote>
<p>æœ¬æ–‡æè¿°äº†SINAI-UJAå›¢é˜Ÿå‚ä¸eRisk@CLEF 2025å®éªŒå®¤çš„æƒ…å†µã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è§£å†³äº†ä¸¤ä¸ªæå‡ºçš„ä»»åŠ¡ï¼šï¼ˆiï¼‰ä»»åŠ¡2ï¼šä¸Šä¸‹æ–‡æ—©æœŸæŠ‘éƒç—‡æ£€æµ‹ï¼Œä»¥åŠï¼ˆiiï¼‰è¯•ç‚¹ä»»åŠ¡ï¼šé€šè¿‡LLMè¿›è¡Œå¯¹è¯å¼æŠ‘éƒç—‡æ£€æµ‹ã€‚æˆ‘ä»¬åœ¨ä»»åŠ¡2ä¸­çš„æ–¹æ³•ç»“åˆäº†å¹¿æ³›çš„é¢„å¤„ç†æµç¨‹ï¼Œä»¥åŠä½¿ç”¨å¤šä¸ªåŸºäºtransformerçš„æ¨¡å‹ï¼Œå¦‚RoBERTa Baseæˆ–MentalRoBERTA Largeï¼Œä»¥æ•æ‰å¤šç”¨æˆ·å¯¹è¯çš„ä¸Šä¸‹æ–‡å’Œé¡ºåºç‰¹æ€§ã€‚å¯¹äºè¯•ç‚¹ä»»åŠ¡ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€å¥—å¯¹è¯ç­–ç•¥ä¸LLMé©±åŠ¨çš„è§’è‰²è¿›è¡Œäº¤äº’ï¼Œä¾§é‡äºåœ¨æœ‰é™çš„å¯¹è¯å›åˆå†…æœ€å¤§åŒ–ä¿¡æ¯è·å–ã€‚åœ¨ä»»åŠ¡2ä¸­ï¼Œæˆ‘ä»¬çš„ç³»ç»Ÿåœ¨F1åˆ†æ•°ä¸Šæ’åç¬¬8ï¼Œå…±æœ‰12æ”¯å‚èµ›é˜Ÿä¼ã€‚ç„¶è€Œï¼Œæ·±å…¥åˆ†æè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨æä¾›æ—©æœŸé¢„æµ‹æ–¹é¢é€Ÿåº¦æœ€å¿«ï¼Œè¿™åœ¨ç°å®éƒ¨ç½²åœºæ™¯ä¸­æ˜¯ä¸€ä¸ªå…³é”®å› ç´ ã€‚è¿™çªå‡ºäº†æ—©æœŸæ£€æµ‹ä¸åˆ†ç±»å‡†ç¡®æ€§ä¹‹é—´çš„æƒè¡¡ï¼Œä¸ºæœªæ¥å·¥ä½œä¸­å¯èƒ½åŒæ—¶ä¼˜åŒ–è¿™ä¸¤è€…æä¾›äº†æ½œåœ¨é€”å¾„ã€‚åœ¨è¯•ç‚¹ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬åœ¨5æ”¯é˜Ÿä¼ä¸­æ’åç¬¬ä¸€ï¼Œåœ¨æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡ä¸Šè·å¾—æœ€ä½³æ•´ä½“è¡¨ç°ï¼šDCHRã€ADODLå’ŒASHRã€‚æˆ‘ä»¬åœ¨è¿™ä¸€ä»»åŠ¡ä¸­çš„æˆåŠŸå±•ç¤ºäº†ç»“æ„åŒ–å¯¹è¯è®¾è®¡ä¸å¼ºå¤§çš„è¯­è¨€æ¨¡å‹ç›¸ç»“åˆçš„æœ‰æ•ˆæ€§ï¼Œè¿›ä¸€æ­¥è¯æ˜äº†åœ¨æ•æ„Ÿçš„ç²¾ç¥å¥åº·è¯„ä¼°ç¯å¢ƒä¸­éƒ¨ç½²LLMçš„å¯è¡Œæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.19861v1">PDF</a> 16 pages, 10 figures, 8 tables. CLEF (Working Notes). 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†SINAI-UJAå›¢é˜Ÿåœ¨eRisk@CLEF 2025å®éªŒå®¤å‚ä¸çš„ä¸¤ä¸ªä»»åŠ¡ï¼šä»»åŠ¡2æ˜¯ä¸Šä¸‹æ–‡åŒ–æ—©æœŸæŠ‘éƒç—‡æ£€æµ‹ï¼Œä»¥åŠè¯•ç‚¹ä»»åŠ¡æ˜¯ä½¿ç”¨LLMçš„å¯¹è¯å¼æŠ‘éƒç—‡æ£€æµ‹ã€‚å›¢é˜Ÿé‡‡ç”¨é¢„å¤„ç†ç®¡é“ä¸åŸºäºRoBERTa Baseå’ŒMentalRoBERTa Largeç­‰transformeræ¨¡å‹çš„ç»„åˆæ¥æ•æ‰å¤šç”¨æˆ·å¯¹è¯çš„ä¸Šä¸‹æ–‡å’Œåºåˆ—ç‰¹å¾ã€‚åœ¨ä»»åŠ¡2ä¸­ï¼Œå›¢é˜Ÿçš„ç³»ç»Ÿåœ¨F1åˆ†æ•°ä¸Šæ’åç¬¬8ä½ï¼ˆå…±12ä¸ªå›¢é˜Ÿå‚ä¸ï¼‰ã€‚æ·±å…¥åˆ†ææ˜¾ç¤ºï¼Œæ¨¡å‹åœ¨å¿«é€Ÿè¿›è¡Œæ—©æœŸé¢„æµ‹æ–¹é¢è¡¨ç°å‡ºä¼˜åŠ¿ï¼Œè¿™åœ¨å®é™…éƒ¨ç½²åœºæ™¯ä¸­æ˜¯ä¸€ä¸ªå…³é”®å› ç´ ã€‚å›¢é˜Ÿåœ¨è¯•ç‚¹ä»»åŠ¡ä¸­å–å¾—äº†ç¬¬ä¸€åï¼ˆå…±5ä¸ªå›¢é˜Ÿå‚ä¸ï¼‰ï¼Œåœ¨æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡ä¸Šéƒ½å–å¾—äº†æœ€ä½³è¡¨ç°ï¼ŒåŒ…æ‹¬DCHRã€ADODLå’ŒASHRã€‚è¿™è¡¨æ˜ç»“åˆå¼ºå¤§çš„è¯­è¨€æ¨¡å‹è¿›è¡Œç»“æ„åŒ–å¯¹è¯è®¾è®¡åœ¨æ•æ„Ÿçš„ç²¾ç¥å¥åº·è¯„ä¼°ä¸Šä¸‹æ–‡ä¸­æ˜¯æœ‰æ•ˆçš„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å›¢é˜Ÿå‚ä¸äº†eRisk@CLEF 2025çš„ä¸¤ä¸ªç›¸å…³ä»»åŠ¡ï¼Œå³ä¸Šä¸‹æ–‡æ—©æœŸæŠ‘éƒç—‡æ£€æµ‹å’Œå¯¹è®²æŠ‘éƒç—‡æ£€æµ‹è¯•ç‚¹ä»»åŠ¡ã€‚</li>
<li>å¯¹äºä»»åŠ¡2ï¼Œé‡‡ç”¨äº†å¤æ‚çš„é¢„å¤„ç†ç®¡é“å’ŒåŸºäºtransformerçš„æ¨¡å‹æ¥æ•æ‰å¯¹è¯çš„ä¸Šä¸‹æ–‡å’Œåºåˆ—ç‰¹å¾ã€‚</li>
<li>åœ¨F1åˆ†æ•°è¯„ä¼°ä¸­ï¼Œå›¢é˜Ÿåœ¨ä»»åŠ¡2ä¸­æ’åç¬¬8ä½ï¼Œä½†æ¨¡å‹åœ¨æ—©æœŸé¢„æµ‹æ–¹é¢è¡¨ç°å‡ºä¼˜åŠ¿ã€‚</li>
<li>åœ¨è¯•ç‚¹ä»»åŠ¡ä¸­ï¼Œå›¢é˜Ÿå–å¾—äº†ç¬¬ä¸€åï¼Œæ‰€æœ‰è¯„ä¼°æŒ‡æ ‡å‡è¡¨ç°æœ€ä½³ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.19861">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_LLM/2509.19861v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_LLM/2509.19861v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_LLM/2509.19861v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_LLM/2509.19861v1/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_LLM/2509.19861v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_LLM/2509.19861v1/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_LLM/2509.19861v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_LLM/2509.19861v1/page_5_1.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="BurstEngine-an-Efficient-Distributed-Framework-for-Training-Transformers-on-Extremely-Long-Sequences-of-over-1M-Tokens"><a href="#BurstEngine-an-Efficient-Distributed-Framework-for-Training-Transformers-on-Extremely-Long-Sequences-of-over-1M-Tokens" class="headerlink" title="BurstEngine: an Efficient Distributed Framework for Training   Transformers on Extremely Long Sequences of over 1M Tokens"></a>BurstEngine: an Efficient Distributed Framework for Training   Transformers on Extremely Long Sequences of over 1M Tokens</h2><p><strong>Authors:Ao Sun, Weilin Zhao, Xu Han, Cheng Yang, Zhiyuan Liu, Chuan Shi, Maosong sun</strong></p>
<p>Existing methods for training LLMs on long-sequence data, such as Tensor Parallelism and Context Parallelism, exhibit low Model FLOPs Utilization as sequence lengths and number of GPUs increase, especially when sequence lengths exceed 1M tokens. To address these challenges, we propose BurstEngine, an efficient framework designed to train LLMs on long-sequence data. BurstEngine introduces BurstAttention, an optimized distributed attention with lower communication cost than RingAttention. BurstAttention leverages topology-aware ring communication to fully utilize network bandwidth and incorporates fine-grained communication-computation overlap. Furthermore, BurstEngine introduces sequence-level selective checkpointing and fuses the language modeling head with the loss function to reduce memory cost. Additionally, BurstEngine introduces workload balance optimization for various types of attention masking. By integrating these optimizations, BurstEngine achieves a $1.2\times$ speedup with much lower memory overhead than the state-of-the-art baselines when training LLMs on extremely long sequences of over 1M tokens. We have made our code publicly available on GitHub: <a target="_blank" rel="noopener" href="https://github.com/thunlp/BurstEngine">https://github.com/thunlp/BurstEngine</a>. </p>
<blockquote>
<p>ç°æœ‰çš„è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¤„ç†é•¿åºåˆ—æ•°æ®çš„æ–¹æ³•ï¼Œå¦‚å¼ é‡å¹¶è¡Œæ€§å’Œä¸Šä¸‹æ–‡å¹¶è¡Œæ€§ï¼Œéšç€åºåˆ—é•¿åº¦çš„å¢åŠ å’ŒGPUæ•°é‡çš„å¢å¤šï¼Œæ¨¡å‹FLOPsåˆ©ç”¨ç‡é€æ¸é™ä½ï¼Œç‰¹åˆ«æ˜¯åœ¨åºåˆ—é•¿åº¦è¶…è¿‡1Mä»¤ç‰Œæ—¶ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†BurstEngineï¼Œè¿™æ˜¯ä¸€ä¸ªè®¾è®¡ç”¨äºè®­ç»ƒLLMså¤„ç†é•¿åºåˆ—æ•°æ®çš„é«˜æ•ˆæ¡†æ¶ã€‚BurstEngineå¼•å…¥äº†BurstAttentionï¼Œè¿™æ˜¯ä¸€ç§ä¼˜åŒ–çš„åˆ†å¸ƒå¼æ³¨æ„åŠ›æœºåˆ¶ï¼Œå…¶é€šä¿¡æˆæœ¬ä½äºRingAttentionã€‚BurstAttentionåˆ©ç”¨æ‹“æ‰‘æ„ŸçŸ¥ç¯å½¢é€šä¿¡æ¥å……åˆ†åˆ©ç”¨ç½‘ç»œå¸¦å®½ï¼Œå¹¶å¼•å…¥äº†ç»†ç²’åº¦çš„é€šä¿¡è®¡ç®—é‡å ã€‚æ­¤å¤–ï¼ŒBurstEngineå¼•å…¥äº†åºåˆ—çº§é€‰æ‹©æ€§æ£€æŸ¥ç‚¹ï¼Œå¹¶å°†è¯­è¨€å»ºæ¨¡å¤´ä¸æŸå¤±å‡½æ•°èåˆï¼Œä»¥é™ä½å†…å­˜æˆæœ¬ã€‚å¦å¤–ï¼ŒBurstEngineè¿˜è¿›è¡Œäº†å·¥ä½œé‡å¹³è¡¡ä¼˜åŒ–ï¼Œé€‚ç”¨äºå„ç§æ³¨æ„åŠ›å±è”½ç±»å‹ã€‚é€šè¿‡æ•´åˆè¿™äº›ä¼˜åŒ–ï¼ŒBurstEngineåœ¨è®­ç»ƒLLMså¤„ç†è¶…è¿‡1Mä»¤ç‰Œçš„é•¿åºåˆ—æ—¶ï¼Œå®ç°äº†æ¯”æœ€æ–°åŸºçº¿æŠ€æœ¯æ›´é«˜çš„1.2å€é€Ÿåº¦æå‡ï¼ŒåŒæ—¶é™ä½äº†å†…å­˜å¼€é”€ã€‚æˆ‘ä»¬çš„ä»£ç å·²åœ¨GitHubä¸Šå…¬å¼€å¯ç”¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/thunlp/BurstEngine%E3%80%82">https://github.com/thunlp/BurstEngineã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.19836v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>é’ˆå¯¹ç°æœ‰è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹å¤„ç†é•¿åºåˆ—æ•°æ®çš„æ–¹æ³•ï¼ˆå¦‚å¼ é‡å¹¶è¡Œæ€§å’Œä¸Šä¸‹æ–‡å¹¶è¡Œæ€§ï¼‰åœ¨åºåˆ—é•¿åº¦å’ŒGPUæ•°é‡å¢åŠ æ—¶æ¨¡å‹FLOPsåˆ©ç”¨ç‡ä½çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§é«˜æ•ˆæ¡†æ¶BurstEngineã€‚è¯¥æ¡†æ¶å¼•å…¥BurstAttentionä¼˜åŒ–åˆ†å¸ƒå¼æ³¨æ„åŠ›æœºåˆ¶ï¼Œé™ä½é€šä¿¡æˆæœ¬ï¼›åˆ©ç”¨æ‹“æ‰‘æ„ŸçŸ¥ç¯å½¢é€šä¿¡ï¼Œå……åˆ†åˆ©ç”¨ç½‘ç»œå¸¦å®½ï¼›é‡‡ç”¨ç»†ç²’åº¦é€šä¿¡è®¡ç®—é‡å ï¼›å¹¶å¼•å…¥åºåˆ—çº§é€‰æ‹©æ€§æ£€æŸ¥ç‚¹ä»¥åŠèåˆè¯­è¨€å»ºæ¨¡å¤´ä¸æŸå¤±å‡½æ•°ä»¥é™ä½å†…å­˜æˆæœ¬ã€‚æ­¤å¤–ï¼ŒBurstEngineè¿˜è¿›è¡Œäº†å·¥ä½œé‡å¹³è¡¡ä¼˜åŒ–ï¼Œé€‚ç”¨äºå„ç§æ³¨æ„åŠ›æ©ç ç±»å‹ã€‚è¿™äº›ä¼˜åŒ–ä½¿BurstEngineåœ¨è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹å¤„ç†è¶…è¿‡ç™¾ä¸‡ä»¤ç‰Œçš„é•¿åºåˆ—æ—¶å®ç°äº†1.2å€çš„é€Ÿåº¦æå‡ï¼ŒåŒæ—¶é™ä½äº†å†…å­˜å¼€é”€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>BurstEngineæ˜¯ä¸€ä¸ªç”¨äºè®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹å¤„ç†é•¿åºåˆ—æ•°æ®çš„æ¡†æ¶ã€‚</li>
<li>å®ƒå¼•å…¥äº†BurstAttentionä¼˜åŒ–åˆ†å¸ƒå¼æ³¨æ„åŠ›æœºåˆ¶ä»¥é™ä½é€šä¿¡æˆæœ¬ã€‚</li>
<li>BurstEngineåˆ©ç”¨æ‹“æ‰‘æ„ŸçŸ¥ç¯å½¢é€šä¿¡å’Œç»†ç²’åº¦é€šä¿¡è®¡ç®—é‡å æ¥å……åˆ†åˆ©ç”¨ç½‘ç»œå¸¦å®½ã€‚</li>
<li>é€šè¿‡å¼•å…¥åºåˆ—çº§é€‰æ‹©æ€§æ£€æŸ¥ç‚¹å’Œèåˆè¯­è¨€å»ºæ¨¡å¤´ä¸æŸå¤±å‡½æ•°ï¼Œé™ä½äº†å†…å­˜æˆæœ¬ã€‚</li>
<li>BurstEngineè¿˜åŒ…æ‹¬å·¥ä½œé‡å¹³è¡¡ä¼˜åŒ–ï¼Œé€‚ç”¨äºä¸åŒç±»å‹çš„æ³¨æ„åŠ›æ©ç ã€‚</li>
<li>è¯¥æ¡†æ¶å®ç°äº†è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹å¤„ç†é•¿åºåˆ—æ—¶çš„é€Ÿåº¦æå‡ï¼ŒåŒæ—¶é™ä½äº†å†…å­˜å¼€é”€ã€‚</li>
<li>BurstEngineçš„ä»£ç å·²å…¬å¼€åœ¨GitHubä¸Šã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.19836">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_LLM/2509.19836v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_LLM/2509.19836v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_LLM/2509.19836v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_LLM/2509.19836v1/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_LLM/2509.19836v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_LLM/2509.19836v1/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_LLM/2509.19836v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Gyges-Dynamic-Cross-Instance-Parallelism-Transformation-for-Efficient-LLM-Inference"><a href="#Gyges-Dynamic-Cross-Instance-Parallelism-Transformation-for-Efficient-LLM-Inference" class="headerlink" title="Gyges: Dynamic Cross-Instance Parallelism Transformation for Efficient   LLM Inference"></a>Gyges: Dynamic Cross-Instance Parallelism Transformation for Efficient   LLM Inference</h2><p><strong>Authors:Haoyu Chen, Xue Li, Kun Qian, Yu Guan, Jin Zhao, Xin Wang</strong></p>
<p>Efficiently processing the dynamics of requests, especially the context length variance, is important in Large Language Model (LLM) serving scenarios. However, there is an intrinsic trade-off: while leveraging parallelism strategies, such as Tensor Parallelism (TP), can coordinate multiple GPUs to accommodate larger context lengths, it inevitably results in degraded overall throughput. In this paper, we propose Cross-Instance Parallelism Transformation (Gyges), which adaptively adjusts the parallelism strategies of running instances to align with the dynamics of incoming requests. We design (1) a page-friendly, header-centric layout to accelerate KV cache transformations; (2) dedicated weight padding to accelerate model weight transformations; and (3) a transformation-aware scheduler to cooperatively schedule requests and parallelism transformations, optimizing the overall performance. Evaluations using real-world traces show that Gyges improves throughput by 1.75x-6.57x compared to state-of-the-art solutions. </p>
<blockquote>
<p>åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æœåŠ¡åœºæ™¯ä¸­ï¼Œé«˜æ•ˆå¤„ç†è¯·æ±‚çš„åŠ¨æ€å˜åŒ–ï¼Œå°¤å…¶æ˜¯ä¸Šä¸‹æ–‡é•¿åº¦å˜åŒ–æ˜¯éå¸¸é‡è¦çš„ã€‚ç„¶è€Œï¼Œå­˜åœ¨ä¸€ä¸ªå›ºæœ‰çš„æƒè¡¡ï¼šè™½ç„¶åˆ©ç”¨å¹¶è¡Œæ€§ç­–ç•¥ï¼ˆå¦‚å¼ é‡å¹¶è¡Œæ€§ï¼‰å¯ä»¥åè°ƒå¤šä¸ªGPUä»¥é€‚åº”æ›´å¤§çš„ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œä½†ä¸å¯é¿å…åœ°ä¼šå¯¼è‡´æ€»ä½“ååé‡ä¸‹é™ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†è·¨å®ä¾‹å¹¶è¡Œæ€§è½¬æ¢ï¼ˆGygesï¼‰ï¼Œå®ƒè‡ªé€‚åº”åœ°è°ƒæ•´è¿è¡Œå®ä¾‹çš„å¹¶è¡Œæ€§ç­–ç•¥ï¼Œä»¥åŒ¹é…ä¼ å…¥è¯·æ±‚çš„åŠ¨æ€å˜åŒ–ã€‚æˆ‘ä»¬è®¾è®¡äº†ï¼ˆ1ï¼‰ä¸€ç§å‹å¥½çš„ã€ä»¥é¡µçœ‰ä¸ºä¸­å¿ƒçš„å¸ƒå±€æ¥åŠ é€Ÿé”®å€¼ç¼“å­˜è½¬æ¢ï¼›ï¼ˆ2ï¼‰ä¸“ç”¨çš„æƒé‡å¡«å……æ¥åŠ é€Ÿæ¨¡å‹æƒé‡è½¬æ¢ï¼›ï¼ˆ3ï¼‰ä¸€ä¸ªæ„ŸçŸ¥è½¬æ¢çš„è°ƒåº¦å™¨ï¼Œä»¥ååŒè°ƒåº¦è¯·æ±‚å’Œå¹¶è¡Œè½¬æ¢ï¼Œä»è€Œä¼˜åŒ–æ•´ä½“æ€§èƒ½ã€‚ä½¿ç”¨çœŸå®ä¸–ç•Œç—•è¿¹çš„è¯„ä¼°è¡¨æ˜ï¼Œä¸æœ€å…ˆè¿›çš„è§£å†³æ–¹æ¡ˆç›¸æ¯”ï¼ŒGygesæé«˜äº†1.75å€è‡³6.57å€çš„ååé‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.19729v1">PDF</a> 12 pages, 15 figures</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æœåŠ¡åœºæ™¯ä¸­ï¼Œå¤„ç†è¯·æ±‚åŠ¨æ€æ€§ï¼Œç‰¹åˆ«æ˜¯ä¸Šä¸‹æ–‡é•¿åº¦å˜åŒ–éå¸¸é‡è¦ã€‚æœ¬æ–‡æå‡ºä¸€ç§åä¸ºGygesçš„è·¨å®ä¾‹å¹¶è¡Œæ€§è½¬æ¢æ–¹æ³•ï¼Œé€šè¿‡è‡ªé€‚åº”è°ƒæ•´è¿è¡Œå®ä¾‹çš„å¹¶è¡Œç­–ç•¥ï¼Œä»¥åŒ¹é…ä¼ å…¥è¯·æ±‚çš„åŠ¨æ€æ€§ã€‚Gygesé€šè¿‡ï¼ˆ1ï¼‰åŠ é€ŸKVç¼“å­˜è½¬æ¢çš„é¡µé¢å‹å¥½ã€ä»¥å¤´éƒ¨ä¸ºä¸­å¿ƒå¸ƒå±€ï¼›ï¼ˆ2ï¼‰åŠ é€Ÿæ¨¡å‹æƒé‡è½¬æ¢çš„ä¸“ç”¨æƒé‡å¡«å……ï¼›ï¼ˆ3ï¼‰ä»¥åŠä¸€ä¸ªèƒ½å¤ŸååŒè°ƒåº¦è¯·æ±‚å’Œå¹¶è¡Œè½¬æ¢çš„è½¬æ¢æ„ŸçŸ¥è°ƒåº¦å™¨ï¼Œä¼˜åŒ–æ•´ä½“æ€§èƒ½ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œä¸ç°æœ‰è§£å†³æ–¹æ¡ˆç›¸æ¯”ï¼ŒGygeså¯æé«˜ååé‡è¾¾1.75xè‡³6.57xã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤„ç†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æœåŠ¡åœºæ™¯ä¸­çš„è¯·æ±‚åŠ¨æ€æ€§è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯ä¸Šä¸‹æ–‡é•¿åº¦å˜åŒ–ã€‚</li>
<li>ç°æœ‰å¹¶è¡Œç­–ç•¥å¦‚Tensor Parallelismï¼ˆTPï¼‰è™½ç„¶å¯ä»¥å¤„ç†æ›´å¤§ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œä½†ä¼šé™ä½æ•´ä½“ååé‡ã€‚</li>
<li>Gygesæ˜¯ä¸€ç§è·¨å®ä¾‹å¹¶è¡Œæ€§è½¬æ¢æ–¹æ³•ï¼Œèƒ½å¤Ÿè‡ªé€‚åº”è°ƒæ•´è¿è¡Œå®ä¾‹çš„å¹¶è¡Œç­–ç•¥ï¼Œä»¥åŒ¹é…ä¼ å…¥è¯·æ±‚çš„åŠ¨æ€æ€§ã€‚</li>
<li>Gygesè®¾è®¡äº†é¡µé¢å‹å¥½ã€ä»¥å¤´éƒ¨ä¸ºä¸­å¿ƒçš„å¸ƒå±€ï¼ŒåŠ é€ŸKVç¼“å­˜è½¬æ¢ã€‚</li>
<li>Gygesé‡‡ç”¨ä¸“ç”¨æƒé‡å¡«å……ï¼Œä»¥åŠ é€Ÿæ¨¡å‹æƒé‡è½¬æ¢ã€‚</li>
<li>GygesåŒ…å«ä¸€ä¸ªè½¬æ¢æ„ŸçŸ¥è°ƒåº¦å™¨ï¼Œèƒ½å¤ŸååŒè°ƒåº¦è¯·æ±‚å’Œå¹¶è¡Œè½¬æ¢ï¼Œä»è€Œä¼˜åŒ–æ•´ä½“æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.19729">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_LLM/2509.19729v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_LLM/2509.19729v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_LLM/2509.19729v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_LLM/2509.19729v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_LLM/2509.19729v1/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_LLM/2509.19729v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_LLM/2509.19729v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="LLMs4All-A-Review-on-Large-Language-Models-for-Research-and-Applications-in-Academic-Disciplines"><a href="#LLMs4All-A-Review-on-Large-Language-Models-for-Research-and-Applications-in-Academic-Disciplines" class="headerlink" title="LLMs4All: A Review on Large Language Models for Research and   Applications in Academic Disciplines"></a>LLMs4All: A Review on Large Language Models for Research and   Applications in Academic Disciplines</h2><p><strong>Authors:Yanfang Fanny Ye, Zheyuan Zhang, Tianyi Ma, Zehong Wang, Yiyang Li, Shifu Hou, Weixiang Sun, Kaiwen Shi, Yijun Ma, Wei Song, Ahmed Abbasi, Ying Cheng, Jane Cleland-Huang, Steven Corcelli, Patricia Culligan, Robert Goulding, Ming Hu, Ting Hua, John Lalor, Fang Liu, Tengfei Luo, Ed Maginn, Nuno Moniz, Jason Rohr, Brett Savoie, Daniel Slate, Tom Stapleford, Matthew Webber, Olaf Wiest, Johnny Zhang, Nitesh Chawla</strong></p>
<p>Cutting-edge Artificial Intelligence (AI) techniques keep reshaping our view of the world. For example, Large Language Models (LLMs) based applications such as ChatGPT have shown the capability of generating human-like conversation on extensive topics. Due to the impressive performance on a variety of language-related tasks (e.g., open-domain question answering, translation, and document summarization), one can envision the far-reaching impacts that can be brought by the LLMs with broader real-world applications (e.g., customer service, education and accessibility, and scientific discovery). Inspired by their success, this paper will offer an overview of state-of-the-art LLMs and their integration into a wide range of academic disciplines, including: (1) arts, letters, and law (e.g., history, philosophy, political science, arts and architecture, law), (2) economics and business (e.g., finance, economics, accounting, marketing), and (3) science and engineering (e.g., mathematics, physics and mechanical engineering, chemistry and chemical engineering, life sciences and bioengineering, earth sciences and civil engineering, computer science and electrical engineering). Integrating humanity and technology, in this paper, we will explore how LLMs are shaping research and practice in these fields, while also discussing key limitations, open challenges, and future directions in the era of generative AI. The review of how LLMs are engaged across disciplines-along with key observations and insights-can help researchers and practitioners interested in exploiting LLMs to advance their works in diverse real-world applications. </p>
<blockquote>
<p>å‰æ²¿çš„äººå·¥æ™ºèƒ½æŠ€æœ¯ä¸æ–­é‡å¡‘æˆ‘ä»¬å¯¹ä¸–ç•Œçš„è®¤çŸ¥ã€‚ä¾‹å¦‚ï¼ŒåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åº”ç”¨ï¼Œå¦‚ChatGPTï¼Œå·²ç»å±•ç°å‡ºåœ¨å¹¿æ³›ä¸»é¢˜ä¸Šç”Ÿæˆäººç±»ç±»ä¼¼å¯¹è¯çš„èƒ½åŠ›ã€‚ç”±äºåœ¨å„ç§è¯­è¨€ç›¸å…³ä»»åŠ¡ï¼ˆå¦‚å¼€æ”¾åŸŸé—®ç­”ã€ç¿»è¯‘å’Œæ–‡æ¡£æ‘˜è¦ï¼‰ä¸Šçš„ä»¤äººå°è±¡æ·±åˆ»çš„è¡¨ç°ï¼Œå¯ä»¥é¢„è§LLMåœ¨æ›´å¹¿æ³›çš„ç°å®ä¸–ç•Œåº”ç”¨ï¼ˆå¦‚å®¢æˆ·æœåŠ¡ã€æ•™è‚²å’Œå¯è®¿é—®æ€§ã€ç§‘å­¦å‘ç°ï¼‰æ‰€å¸¦æ¥çš„æ·±è¿œå½±å“ã€‚å—å®ƒä»¬æˆåŠŸçš„å¯å‘ï¼Œæœ¬æ–‡å°†æ¦‚è¿°æœ€æ–°çš„å¤§å‹è¯­è¨€æ¨¡å‹åŠå…¶åœ¨å„ç§å­¦æœ¯å­¦ç§‘çš„é›†æˆï¼ŒåŒ…æ‹¬ï¼šï¼ˆ1ï¼‰è‰ºæœ¯ã€æ–‡å­¦å’Œæ³•å¾‹ï¼ˆä¾‹å¦‚å†å²ã€å“²å­¦ã€æ”¿æ²»å­¦ã€è‰ºæœ¯å’Œå»ºç­‘ã€æ³•å¾‹ï¼‰ã€ï¼ˆ2ï¼‰ç»æµå­¦å’Œå•†ä¸šï¼ˆä¾‹å¦‚é‡‘èã€ç»æµå­¦ã€ä¼šè®¡ã€å¸‚åœºè¥é”€ï¼‰ï¼Œä»¥åŠï¼ˆ3ï¼‰ç§‘å­¦å’Œå·¥ç¨‹ï¼ˆä¾‹å¦‚æ•°å­¦ã€ç‰©ç†å’Œæœºæ¢°å·¥ç¨‹ã€åŒ–å­¦å’ŒåŒ–å·¥ã€ç”Ÿå‘½ç§‘å­¦å’Œç”Ÿç‰©å·¥ç¨‹ã€åœ°çƒç§‘å­¦å’ŒåœŸæœ¨å·¥ç¨‹ã€è®¡ç®—æœºç§‘å­¦å’Œç”µæ°”å·¥ç¨‹ï¼‰ã€‚æœ¬æ–‡èåˆäº†äººæ–‡å­¦ç§‘å’ŒæŠ€æœ¯ï¼Œå°†æ¢ç´¢å¤§å‹è¯­è¨€æ¨¡å‹å¦‚ä½•å¡‘é€ è¿™äº›é¢†åŸŸçš„ç ”ç©¶å’Œå®è·µï¼ŒåŒæ—¶è®¨è®ºç”Ÿæˆäººå·¥æ™ºèƒ½æ—¶ä»£çš„å…³é”®å±€é™ã€å¼€æ”¾æŒ‘æˆ˜å’Œæœªæ¥å‘å±•æ–¹å‘ã€‚å¯¹å¤§å‹è¯­è¨€æ¨¡å‹å¦‚ä½•è·¨å­¦ç§‘åº”ç”¨çš„å®¡æŸ¥ï¼Œä»¥åŠå…³é”®è§‚å¯Ÿå’Œè§è§£ï¼Œå¯ä»¥å¸®åŠ©ç ”ç©¶äººå‘˜å’Œå®è·µè€…äº†è§£å¦‚ä½•åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æ¥æ¨åŠ¨å…¶åœ¨å„ç§ç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„å·¥ä½œã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.19580v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ€æ–°çš„äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æŠ€æœ¯ï¼Œå¦‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œæ­£åœ¨ä¸æ–­æ”¹å˜æˆ‘ä»¬å¯¹ä¸–ç•Œçš„çœ‹æ³•ã€‚åŸºäºLLMçš„åº”ç”¨ï¼Œå¦‚ChatGPTï¼Œå·²æ˜¾ç¤ºå‡ºåœ¨å¹¿æ³›ä¸»é¢˜ä¸Šç”Ÿæˆç±»ä¼¼äººç±»å¯¹è¯çš„èƒ½åŠ›ã€‚LLMåœ¨å„ç§è¯­è¨€ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå¯åº”ç”¨äºå®¢æˆ·æœåŠ¡ã€æ•™è‚²ã€æ— éšœç¢ã€ç§‘å­¦å‘ç°ç­‰é¢†åŸŸã€‚æœ¬æ–‡æ¦‚è¿°äº†æœ€å‰æ²¿çš„LLMåŠå…¶ä¸å¤šä¸ªå­¦æœ¯é¢†åŸŸçš„èåˆï¼ŒåŒ…æ‹¬è‰ºæœ¯ã€æ³•å¾‹ã€ç»æµã€ç§‘å­¦ã€å·¥ç¨‹ç­‰ã€‚åŒæ—¶ï¼Œæ–‡ç« ä¹Ÿæ¢è®¨äº†LLMçš„å…³é”®å±€é™æ€§ã€å¼€æ”¾æŒ‘æˆ˜å’Œæœªæ¥åœ¨ç”ŸæˆAIæ—¶ä»£çš„å‘å±•æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsåŸºäºAIæŠ€æœ¯ï¼Œæ­£åœ¨é‡å¡‘äººä»¬å¯¹ä¸–ç•Œçš„è®¤çŸ¥ã€‚</li>
<li>LLMsåœ¨å¤šç§è¯­è¨€ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚</li>
<li>LLMså¯ä»¥åº”ç”¨äºå®¢æˆ·æœåŠ¡ã€æ•™è‚²ã€æ— éšœç¢å’Œç§‘å­¦ç ”ç©¶ç­‰é¢†åŸŸã€‚</li>
<li>LLMsä¸å¤šä¸ªå­¦æœ¯é¢†åŸŸèåˆï¼ŒåŒ…æ‹¬è‰ºæœ¯ã€æ³•å¾‹ã€ç»æµã€ç§‘å­¦ã€å·¥ç¨‹ç­‰ã€‚</li>
<li>LLMsä¹Ÿå­˜åœ¨å…³é”®å±€é™æ€§ï¼Œé¢ä¸´å¼€æ”¾æŒ‘æˆ˜å’Œæœªæ¥å‘å±•æ–¹å‘ã€‚</li>
<li>LLMsçš„é›†æˆæœ‰åŠ©äºæ¨è¿›ç ”ç©¶å’Œå®è·µï¼Œå¯¹æ„Ÿå…´è¶£çš„å­¦è€…å’Œå®è·µè€…æä¾›äº†é‡è¦è§è§£å’Œæ´å¯ŸåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.19580">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_LLM/2509.19580v2/page_0_0.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="COLT-Enhancing-Video-Large-Language-Models-with-Continual-Tool-Usage"><a href="#COLT-Enhancing-Video-Large-Language-Models-with-Continual-Tool-Usage" class="headerlink" title="COLT: Enhancing Video Large Language Models with Continual Tool Usage"></a>COLT: Enhancing Video Large Language Models with Continual Tool Usage</h2><p><strong>Authors:Yuyang Liu, Xinyuan Shi, Xiaondan Liang</strong></p>
<p>The success of Large Language Models (LLMs) has significantly propelled the research of video understanding. To harvest the benefits of well-trained expert models (i.e., tools), video LLMs prioritize the exploration of tool usage capabilities. Existing methods either prompt closed-source LLMs or employ the instruction tuning paradigm for tool-use fine-tuning. These methods, however, assume an established repository of fixed tools and struggle to generalize to real-world environments where tool data is perpetually evolving and streaming in. To this end, we propose to enhance open-source video LLMs with COntinuaL Tool usage (termed COLT), which automatically acquires tool-use ability in a successive tool stream without suffering â€˜catastrophic forgettingâ€™ of the past learned tools. Specifically, our COLT incorporates a learnable tool codebook as a tool-specific memory system. Then relevant tools are dynamically selected based on the similarity between user instruction and tool features within the codebook. To unleash the tool usage potential of video LLMs, we collect a video-centric tool-use instruction tuning dataset VideoToolBench. Extensive experiments on both previous video LLM benchmarks and the tool-use-specific VideoToolBench dataset demonstrate the state-of-the-art performance of our proposed COLT. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æˆåŠŸæå¤§åœ°æ¨åŠ¨äº†è§†é¢‘ç†è§£ç ”ç©¶çš„å‘å±•ã€‚ä¸ºäº†åˆ©ç”¨é¢„è®­ç»ƒä¸“å®¶æ¨¡å‹çš„ä¼˜ç‚¹ï¼ˆå³å·¥å…·ï¼‰ï¼Œè§†é¢‘LLMä¼˜å…ˆæ¢ç´¢å·¥å…·ä½¿ç”¨èƒ½åŠ›ã€‚ç°æœ‰æ–¹æ³•è¦ä¹ˆæç¤ºå°é—­æºLLMï¼Œè¦ä¹ˆé‡‡ç”¨æŒ‡ä»¤å¾®è°ƒèŒƒå¼è¿›è¡Œå·¥å…·ä½¿ç”¨çš„å¾®è°ƒã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å‡å®šå­˜åœ¨ä¸€ä¸ªå›ºå®šçš„å·¥å…·åº“ï¼Œå¹¶éš¾ä»¥æ¨å¹¿åˆ°å·¥å…·æ•°æ®ä¸æ–­æ¼”å˜å’Œæµå…¥çš„ç°å®ä¸–ç•Œç¯å¢ƒã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºé€šè¿‡COLTï¼ˆè¿ç»­å·¥å…·ä½¿ç”¨ï¼‰å¢å¼ºå¼€æºè§†é¢‘LLMï¼Œå®ƒå¯ä»¥åœ¨è¿ç»­çš„å·¥å…·æµä¸­è‡ªåŠ¨è·å–å·¥å…·ä½¿ç”¨èƒ½åŠ›ï¼Œè€Œä¸ä¼šé­å—è¿‡å»å­¦ä¹ å·¥å…·çš„â€œç¾éš¾æ€§é—å¿˜â€ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„COLTå¼•å…¥äº†ä¸€ä¸ªå¯å­¦ä¹ çš„å·¥å…·ä»£ç æœ¬ä½œä¸ºå·¥å…·ç‰¹å®šçš„å†…å­˜ç³»ç»Ÿã€‚ç„¶åï¼Œæ ¹æ®ç”¨æˆ·æŒ‡ä»¤ä¸ä»£ç æœ¬ä¸­å·¥å…·ç‰¹å¾çš„ç›¸ä¼¼æ€§åŠ¨æ€é€‰æ‹©ç›¸å…³å·¥å…·ã€‚ä¸ºäº†é‡Šæ”¾è§†é¢‘LLMçš„å·¥å…·ä½¿ç”¨æ½œåŠ›ï¼Œæˆ‘ä»¬æ”¶é›†äº†ä»¥è§†é¢‘ä¸ºä¸­å¿ƒçš„å·¥å…·ä½¿ç”¨æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†VideoToolBenchã€‚åœ¨ä¹‹å‰çš„è§†é¢‘LLMåŸºå‡†æµ‹è¯•å’Œä¸“ç”¨çš„VideoToolBenchå·¥å…·ä½¿ç”¨æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„COLTå…·æœ‰æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.18754v2">PDF</a> 16 pages</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æˆåŠŸæ¨åŠ¨äº†è§†é¢‘ç†è§£ç ”ç©¶çš„è¿›å±•ã€‚ä¸ºäº†åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„ä¼˜åŠ¿ï¼Œè§†é¢‘LLMé‡ç‚¹æ¢ç´¢å·¥å…·ä½¿ç”¨èƒ½åŠ›ã€‚ç°æœ‰æ–¹æ³•é€šè¿‡æç¤ºé—­æºLLMæˆ–é‡‡ç”¨æŒ‡ä»¤å¾®è°ƒæ¨¡å¼è¿›è¡Œå·¥å…·ä½¿ç”¨å¾®è°ƒã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å‡å®šå­˜åœ¨å›ºå®šçš„å·¥å…·åº“ï¼Œéš¾ä»¥æ³›åŒ–åˆ°ç°å®ç¯å¢ƒä¸­ä¸æ–­æ¼”å˜å’ŒæµåŠ¨çš„å·¥å…·æ•°æ®ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºå¢å¼ºå¼€æºè§†é¢‘LLMçš„è¿ç»­å·¥å…·ä½¿ç”¨ï¼ˆCOLTï¼‰èƒ½åŠ›ï¼Œå¯è‡ªåŠ¨åœ¨è¿ç»­å·¥å…·æµä¸­è·å–å·¥å…·ä½¿ç”¨èƒ½åŠ›ï¼Œè€Œä¸ä¼šé—å¿˜è¿‡å»å­¦ä¹ çš„å·¥å…·ã€‚COLTé‡‡ç”¨å¯å­¦ä¹ çš„å·¥å…·ä»£ç æœ¬ä½œä¸ºå·¥å…·ç‰¹å®šè®°å¿†ç³»ç»Ÿï¼Œå¹¶æ ¹æ®ç”¨æˆ·æŒ‡ä»¤ä¸ä»£ç æœ¬ä¸­å·¥å…·ç‰¹å¾çš„ç›¸ä¼¼æ€§åŠ¨æ€é€‰æ‹©ç›¸å…³å·¥å…·ã€‚ä¸ºäº†é‡Šæ”¾è§†é¢‘LLMçš„å·¥å…·ä½¿ç”¨æ½œåŠ›ï¼Œæˆ‘ä»¬æ”¶é›†äº†ä»¥è§†é¢‘ä¸ºä¸­å¿ƒçš„å·¥å…·ä½¿ç”¨æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†VideoToolBenchã€‚åœ¨ä¹‹å‰çš„è§†é¢‘LLMåŸºå‡†æµ‹è¯•å’Œä¸“ç”¨çš„VideoToolBenchæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCOLTå…·æœ‰å“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è§†é¢‘ç†è§£ç ”ç©¶æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚</li>
<li>è§†é¢‘LLMé‡è§†æ¢ç´¢å·¥å…·ä½¿ç”¨èƒ½åŠ›ï¼Œä»¥åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„ä¼˜åŠ¿ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–äºå›ºå®šçš„å·¥å…·åº“ï¼Œéš¾ä»¥é€‚åº”ç°å®ç¯å¢ƒä¸­ä¸æ–­å˜åŒ–çš„å·¥å…·æ•°æ®ã€‚</li>
<li>æå‡ºäº†COLTæ–¹æ³•ï¼Œå¯å¢å¼ºè§†é¢‘LLMåœ¨è¿ç»­å·¥å…·æµä¸­çš„è‡ªåŠ¨è·å–å·¥å…·ä½¿ç”¨èƒ½åŠ›ï¼Œé¿å…é—å¿˜è¿‡å»çš„å­¦ä¹ å†…å®¹ã€‚</li>
<li>COLTé‡‡ç”¨å¯å­¦ä¹ çš„å·¥å…·ä»£ç æœ¬ä½œä¸ºå·¥å…·ç‰¹å®šè®°å¿†ç³»ç»Ÿï¼Œå¹¶æ ¹æ®ç”¨æˆ·æŒ‡ä»¤ä¸å·¥å…·ç‰¹å¾çš„ç›¸ä¼¼æ€§åŠ¨æ€é€‰æ‹©å·¥å…·ã€‚</li>
<li>ä¸ºé‡Šæ”¾è§†é¢‘LLMçš„å·¥å…·ä½¿ç”¨æ½œåŠ›ï¼Œç ”ç©¶è€…ä»¬æ”¶é›†äº†VideoToolBenchæ•°æ®é›†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.18754">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_LLM/2509.18754v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_LLM/2509.18754v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_LLM/2509.18754v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_LLM/2509.18754v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_LLM/2509.18754v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_LLM/2509.18754v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="TempSamp-R1-Effective-Temporal-Sampling-with-Reinforcement-Fine-Tuning-for-Video-LLMs"><a href="#TempSamp-R1-Effective-Temporal-Sampling-with-Reinforcement-Fine-Tuning-for-Video-LLMs" class="headerlink" title="TempSamp-R1: Effective Temporal Sampling with Reinforcement Fine-Tuning   for Video LLMs"></a>TempSamp-R1: Effective Temporal Sampling with Reinforcement Fine-Tuning   for Video LLMs</h2><p><strong>Authors:Yunheng Li, Jing Cheng, Shaoyong Jia, Hangyi Kuang, Shaohui Jiao, Qibin Hou, Ming-Ming Cheng</strong></p>
<p>This paper introduces TempSamp-R1, a new reinforcement fine-tuning framework designed to improve the effectiveness of adapting multimodal large language models (MLLMs) to video temporal grounding tasks. We reveal that existing reinforcement learning methods, such as Group Relative Policy Optimization (GRPO), rely on on-policy sampling for policy updates. However, in tasks with large temporal search spaces, this strategy becomes both inefficient and limited in performance, as it often fails to identify temporally accurate solutions. To address this limitation, TempSamp-R1 leverages ground-truth annotations as off-policy supervision to provide temporally precise guidance, effectively compensating for the sparsity and misalignment in on-policy solutions. To further stabilize training and reduce variance in reward-based updates, TempSamp-R1 provides a non-linear soft advantage computation method that dynamically reshapes the reward feedback via an asymmetric transformation. By employing a hybrid Chain-of-Thought (CoT) training paradigm, TempSamp-R1 optimizes a single unified model to support both CoT and non-CoT inference modes, enabling efficient handling of queries with varying reasoning complexity. Experimental results demonstrate that TempSamp-R1 outperforms GRPO-based baselines, establishing new state-of-the-art performance on benchmark datasets: Charades-STA (<a href="mailto:&#82;&#x31;&#64;&#48;&#x2e;&#x37;">&#82;&#x31;&#64;&#48;&#x2e;&#x37;</a>: 52.9%, +2.7%), ActivityNet Captions (<a href="mailto:&#x52;&#x31;&#64;&#48;&#46;&#x35;">&#x52;&#x31;&#64;&#48;&#46;&#x35;</a>: 56.0%, +5.3%), and QVHighlights (mAP: 30.0%, +3.0%). Moreover, TempSamp-R1 shows robust few-shot generalization capabilities under limited data. Code: <a target="_blank" rel="noopener" href="https://github.com/HVision-NKU/TempSamp-R1">https://github.com/HVision-NKU/TempSamp-R1</a> </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†TempSamp-R1ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„å¼ºåŒ–ç²¾ç»†è°ƒæ•´æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†é¢‘æ—¶é—´å®šä½ä»»åŠ¡ä¸­çš„é€‚åº”æ€§ã€‚æˆ‘ä»¬æ­ç¤ºï¼Œç°æœ‰çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œå¦‚ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œä¾èµ–äºç­–ç•¥æ›´æ–°çš„åœ¨ç­–ç•¥é‡‡æ ·ã€‚ç„¶è€Œï¼Œåœ¨å…·æœ‰å¤§çš„æ—¶é—´æœç´¢ç©ºé—´çš„ä»»åŠ¡ä¸­ï¼Œè¿™ç§ç­–ç•¥å˜å¾—æ•ˆç‡ä½ä¸‹ä¸”æ€§èƒ½å—é™ï¼Œå› ä¸ºå®ƒå¾€å¾€æ— æ³•è¯†åˆ«å‡ºæ—¶é—´ä¸Šçš„ç²¾ç¡®è§£ã€‚ä¸ºè§£å†³è¿™ä¸€å±€é™æ€§ï¼ŒTempSamp-R1åˆ©ç”¨çœŸå®æ³¨é‡Šä½œä¸ºç¦»çº¿ç›‘ç£æ¥æä¾›æ—¶é—´ä¸Šç²¾ç¡®çš„æŒ‡å¯¼ï¼Œæœ‰æ•ˆåœ°è¡¥å¿åœ¨çº¿ç­–ç•¥çš„ç¨€ç–æ€§å’Œä¸åŒ¹é…é—®é¢˜ã€‚ä¸ºäº†è¿›ä¸€æ­¥ç¨³å®šè®­ç»ƒå’Œå‡å°‘åŸºäºå¥–åŠ±çš„æ›´æ–°çš„æ–¹å·®ï¼ŒTempSamp-R1æä¾›äº†ä¸€ç§éçº¿æ€§è½¯ä¼˜åŠ¿è®¡ç®—æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡ä¸å¯¹ç§°è½¬æ¢åŠ¨æ€åœ°é‡å¡‘å¥–åŠ±åé¦ˆã€‚é€šè¿‡é‡‡ç”¨æ··åˆå¼çš„æ€ç»´é“¾ï¼ˆCoTï¼‰è®­ç»ƒèŒƒå¼ï¼ŒTempSamp-R1ä¼˜åŒ–äº†ä¸€ä¸ªå•ä¸€ç»Ÿä¸€çš„æ¨¡å‹æ¥æ”¯æŒCoTå’ŒéCoTæ¨ç†æ¨¡å¼ï¼Œä»è€Œæœ‰æ•ˆåœ°å¤„ç†ä¸åŒå¤æ‚åº¦çš„æŸ¥è¯¢ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTempSamp-R1åœ¨Charades-STAï¼ˆ<a href="mailto:&#82;&#x31;&#x40;&#x30;&#x2e;&#55;">&#82;&#x31;&#x40;&#x30;&#x2e;&#55;</a>ï¼š52.9%ï¼Œ+2.7%ï¼‰ã€ActivityNet Captionsï¼ˆ<a href="mailto:&#82;&#x31;&#64;&#48;&#46;&#x35;">&#82;&#x31;&#64;&#48;&#46;&#x35;</a>ï¼š56.0%ï¼Œ+5.3%ï¼‰å’ŒQVHighlightsï¼ˆmAPï¼š30.0%ï¼Œ+3.0%ï¼‰ç­‰åŸºå‡†æ•°æ®é›†ä¸Šè¶…è¶Šäº†åŸºäºGRPOçš„åŸºçº¿æ–¹æ³•ï¼Œå–å¾—äº†æœ€æ–°çš„æœ€ä½³æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒTempSamp-R1åœ¨æœ‰é™æ•°æ®ä¸‹è¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚ä»£ç åœ°å€ï¼š<a target="_blank" rel="noopener" href="https://github.com/HVision-NKU/TempSamp-R1">https://github.com/HVision-NKU/TempSamp-R1</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.18056v2">PDF</a> Accepted at NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>TempSamp-R1æ˜¯ä¸€ä¸ªé’ˆå¯¹è§†é¢‘æ—¶åºå®šä½ä»»åŠ¡çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å¼ºåŒ–å¾®è°ƒæ¡†æ¶ã€‚å®ƒè§£å†³äº†ç°æœ‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨å¤§å‹æ—¶åºæœç´¢ç©ºé—´ä¸­çš„æ•ˆç‡å’Œæ€§èƒ½é—®é¢˜ï¼Œé€šè¿‡åˆ©ç”¨åœ°é¢çœŸå®æ³¨é‡Šä½œä¸ºç¦»çº¿ç­–ç•¥ç›‘ç£ï¼Œæä¾›äº†æ—¶åºç²¾ç¡®æŒ‡å¯¼ï¼Œå¹¶é‡‡ç”¨éçº¿æ€§è½¯ä¼˜åŠ¿è®¡ç®—æ–¹æ³•åŠ¨æ€è°ƒæ•´å¥–åŠ±åé¦ˆã€‚æ­¤å¤–ï¼ŒTempSamp-R1é‡‡ç”¨æ··åˆçš„Chain-of-Thoughtè®­ç»ƒèŒƒå¼ï¼Œä¼˜åŒ–å•ä¸€ç»Ÿä¸€æ¨¡å‹ä»¥æ”¯æŒä¸åŒæ¨ç†å¤æ‚åº¦çš„æŸ¥è¯¢ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTempSamp-R1åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå®ç°äº†å“è¶Šæ€§èƒ½ï¼Œå¹¶å…·æœ‰å¼ºå¤§çš„å°‘æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TempSamp-R1æ˜¯ä¸€ä¸ªé’ˆå¯¹è§†é¢‘æ—¶åºå®šä½ä»»åŠ¡çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å¼ºåŒ–å¾®è°ƒæ¡†æ¶ï¼Œæ—¨åœ¨æé«˜æ¨¡å‹é€‚åº”æ­¤ç±»ä»»åŠ¡çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>ç°æœ‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨å¤§å‹æ—¶åºæœç´¢ç©ºé—´ä¸­é¢ä¸´æ•ˆç‡å’Œæ€§èƒ½é—®é¢˜ï¼ŒTempSamp-R1è§£å†³äº†è¿™ä¸€é—®é¢˜ã€‚</li>
<li>TempSamp-R1åˆ©ç”¨åœ°é¢çœŸå®æ³¨é‡Šä½œä¸ºç¦»çº¿ç­–ç•¥ç›‘ç£ï¼Œæä¾›ç²¾ç¡®çš„æ—¶åºæŒ‡å¯¼ï¼Œå¼¥è¡¥äº†åœ¨çº¿ç­–ç•¥è§£å†³æ–¹æ¡ˆçš„ç¨€ç–æ€§å’Œä¸å‡†ç¡®æ€§ã€‚</li>
<li>è¯¥æ–¹æ³•é‡‡ç”¨éçº¿æ€§è½¯ä¼˜åŠ¿è®¡ç®—æ–¹æ³•ï¼ŒåŠ¨æ€è°ƒæ•´å¥–åŠ±åé¦ˆï¼Œè¿›ä¸€æ­¥ç¨³å®šè®­ç»ƒå¹¶é™ä½åŸºäºå¥–åŠ±çš„æ›´æ–°çš„æ–¹å·®ã€‚</li>
<li>TempSamp-R1é‡‡ç”¨æ··åˆçš„Chain-of-Thoughtè®­ç»ƒèŒƒå¼ï¼Œæ”¯æŒä¸åŒæ¨ç†å¤æ‚åº¦çš„æŸ¥è¯¢å¤„ç†ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒTempSamp-R1åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå®ç°äº†å“è¶Šæ€§èƒ½ï¼ŒåŒ…æ‹¬Charades-STAã€ActivityNet Captionså’ŒQVHighlightsã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.18056">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_LLM/2509.18056v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_LLM/2509.18056v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_LLM/2509.18056v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_LLM/2509.18056v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-28/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-28/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-28/Agent/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Agent/2509.20900v1/page_1_0.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-28  Interactive Recommendation Agent with Active User Commands
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-28
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-28/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_R1_Reasoning/2509.21268v1/page_1_0.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-28  SciReasoner Laying the Scientific Reasoning Ground Across Disciplines
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-28
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">29474.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
