<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-28  RePro Leveraging Large Language Models for Semi-Automated Reproduction   of Networking Research Results">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Few-Shot/2504.02876v2/page_0_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-28
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-01
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    82 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-09-28-æ›´æ–°"><a href="#2025-09-28-æ›´æ–°" class="headerlink" title="2025-09-28 æ›´æ–°"></a>2025-09-28 æ›´æ–°</h1><h2 id="RePro-Leveraging-Large-Language-Models-for-Semi-Automated-Reproduction-of-Networking-Research-Results"><a href="#RePro-Leveraging-Large-Language-Models-for-Semi-Automated-Reproduction-of-Networking-Research-Results" class="headerlink" title="RePro: Leveraging Large Language Models for Semi-Automated Reproduction   of Networking Research Results"></a>RePro: Leveraging Large Language Models for Semi-Automated Reproduction   of Networking Research Results</h2><p><strong>Authors:Yining Jiang, Wenyun Xu, Qingyu Song, Yuling Lin, Xuanhao Liu, Xiaoqiang Zheng, Qiang Su, Lizhao You, Lu Tang, Wangjian Feng, Linghe Kong, Qiao Xiang, Jiwu Shu</strong></p>
<p>Reproducing networking research is a critical but challenging task due to the scarcity of open-source code. While Large Language Models (LLMs) can automate code generation, current approaches lack the generalizability required for the diverse networking field. To address this, we propose RePro, a semi-automated reproduction framework that leverages advanced prompt engineering to reproduce network systems from their research papers. RePro combines few-shot in-context learning with Structured and Semantic Chain of Thought (SCoT&#x2F;SeCoT) techniques to systematically translate a paperâ€™s description into an optimized, executable implementation. The framework operates through a three-stage pipeline: system description extraction, structural code generation, and code optimization. Our evaluation with five state-of-the-art LLMs across diverse network sub-domains demonstrates that RePro significantly reduces reproduction time compared to manual efforts while achieving comparable system performance, validating its effectiveness and efficiency. </p>
<blockquote>
<p>å¤åˆ¶ç½‘ç»œç ”ç©¶æ˜¯ä¸€é¡¹è‡³å…³é‡è¦çš„ä»»åŠ¡ï¼Œä½†ç”±äºç¼ºä¹å¼€æºä»£ç ï¼Œè¿™å…·æœ‰æŒ‘æˆ˜æ€§ã€‚è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯ä»¥è‡ªåŠ¨è¿›è¡Œä»£ç ç”Ÿæˆï¼Œä½†å½“å‰çš„æ–¹æ³•ç¼ºä¹åœ¨å¤šæ ·åŒ–ç½‘ç»œé¢†åŸŸæ‰€éœ€çš„ä¸€èˆ¬æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ReProï¼Œè¿™æ˜¯ä¸€ä¸ªåŠè‡ªåŠ¨åŒ–çš„å¤åˆ¶æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨å…ˆè¿›çš„æç¤ºå·¥ç¨‹æ¥ä»ç ”ç©¶è®ºæ–‡ä¸­å¤åˆ¶ç½‘ç»œç³»ç»Ÿã€‚ReProç»“åˆäº†åŸºäºä¸Šä¸‹æ–‡çš„å°‘æ ·æœ¬å­¦ä¹ ä¸ç»“æ„åŒ–ä¸è¯­ä¹‰é“¾æ€ç»´ï¼ˆSCoT&#x2F;SeCoTï¼‰æŠ€æœ¯ï¼Œç³»ç»Ÿæ€§åœ°å°†è®ºæ–‡æè¿°è½¬åŒ–ä¸ºä¼˜åŒ–åçš„å¯æ‰§è¡Œå®ç°ã€‚è¯¥æ¡†æ¶é€šè¿‡ä¸‰ä¸ªé˜¶æ®µè¿›è¡Œæ“ä½œï¼šç³»ç»Ÿæè¿°æå–ã€ç»“æ„åŒ–ä»£ç ç”Ÿæˆå’Œä»£ç ä¼˜åŒ–ã€‚æˆ‘ä»¬åœ¨ä¸åŒçš„ç½‘ç»œå­é¢†åŸŸå¯¹æœ€å…ˆè¿›çš„äº”ä¸ªLLMè¿›è¡Œäº†è¯„ä¼°ï¼Œè¯æ˜äº†ReProåœ¨å‡å°‘å¤åˆ¶æ—¶é—´æ–¹é¢ä¸æ‰‹åŠ¨å·¥ä½œç›¸æ¯”å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼ŒåŒæ—¶åœ¨ç³»ç»Ÿæ€§èƒ½ä¸Šå–å¾—äº†ç›¸å½“çš„è¡¨ç°ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œæ•ˆç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.21074v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºç½‘ç»œå¼€æºä»£ç ç¨€ç¼ºçš„æŒ‘æˆ˜ï¼Œç½‘ç»œç ”ç©¶çš„é‡ç°æˆä¸ºä¸€é¡¹è‰°å·¨çš„ä»»åŠ¡ã€‚æœ¬ç ”ç©¶æå‡ºReProæ¡†æ¶ï¼Œç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åŠè‡ªåŠ¨åŒ–é‡ç°æŠ€æœ¯ï¼Œé€šè¿‡å…ˆè¿›çš„æç¤ºå·¥ç¨‹ä»ç ”ç©¶è®ºæ–‡ä¸­é‡ç°ç½‘ç»œç³»ç»Ÿã€‚ReProé€šè¿‡ä¸‰é˜¶æ®µç®¡é“ï¼ˆç³»ç»Ÿæè¿°æå–ã€ç»“æ„ä»£ç ç”Ÿæˆå’Œä»£ç ä¼˜åŒ–ï¼‰å®ç°è®ºæ–‡æè¿°çš„ç³»ç»ŸåŒ–ç¿»è¯‘ï¼Œç”Ÿæˆä¼˜åŒ–çš„å¯æ‰§è¡Œå®ç°ã€‚æœ¬ç ”ç©¶ä½¿ç”¨äº”ä¸ªæœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸åŒç½‘ç»œå­åŸŸçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒReProåœ¨æ˜¾è‘—å‡å°‘é‡ç°æ—¶é—´çš„åŒæ—¶ï¼Œå®ç°äº†ä¸ç³»ç»Ÿæ€§èƒ½ç›¸å½“çš„æ•ˆæœï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œæ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ç½‘ç»œç ”ç©¶çš„é‡ç°å› ç¼ºä¹å¼€æºä»£ç è€Œé¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>ReProæ¡†æ¶ç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡ŒåŠè‡ªåŠ¨åŒ–é‡ç°ã€‚</li>
<li>ReProä½¿ç”¨å…ˆè¿›çš„æç¤ºå·¥ç¨‹æŠ€æœ¯ä»ç ”ç©¶è®ºæ–‡ä¸­é‡ç°ç½‘ç»œç³»ç»Ÿã€‚</li>
<li>ReProæ¡†æ¶åŒ…å«ä¸‰ä¸ªä¸»è¦é˜¶æ®µï¼šç³»ç»Ÿæè¿°æå–ã€ç»“æ„ä»£ç ç”Ÿæˆå’Œä»£ç ä¼˜åŒ–ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.21074">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Few-Shot/2509.21074v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Few-Shot/2509.21074v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Few-Shot/2509.21074v1/page_5_0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Background-Prompt-for-Few-Shot-Out-of-Distribution-Detection"><a href="#Background-Prompt-for-Few-Shot-Out-of-Distribution-Detection" class="headerlink" title="Background Prompt for Few-Shot Out-of-Distribution Detection"></a>Background Prompt for Few-Shot Out-of-Distribution Detection</h2><p><strong>Authors:Songyue Cai, Zongqian Wu, Yujie Mo, Liang Peng, Ping Hu, Xiaoshuang Shi, Xiaofeng Zhu</strong></p>
<p>Existing foreground-background (FG-BG) decomposition methods for the few-shot out-of-distribution (FS-OOD) detection often suffer from low robustness due to over-reliance on the local class similarity and a fixed background patch extraction strategy. To address these challenges, we propose a new FG-BG decomposition framework, namely Mambo, for FS-OOD detection. Specifically, we propose to first learn a background prompt to obtain the local background similarity containing both the background and image semantic information, and then refine the local background similarity using the local class similarity. As a result, we use both the refined local background similarity and the local class similarity to conduct background extraction, reducing the dependence of the local class similarity in previous methods. Furthermore, we propose the patch self-calibrated tuning to consider the sample diversity to flexibly select numbers of background patches for different samples, and thus exploring the issue of fixed background extraction strategies in previous methods. Extensive experiments on real-world datasets demonstrate that our proposed Mambo achieves the best performance, compared to SOTA methods in terms of OOD detection and near OOD detection setting. The source code will be released at <a target="_blank" rel="noopener" href="https://github.com/YuzunoKawori/Mambo">https://github.com/YuzunoKawori/Mambo</a>. </p>
<blockquote>
<p>ç°æœ‰çš„é’ˆå¯¹å°æ ·æœ¬å¤–åˆ†å¸ƒï¼ˆFS-OODï¼‰æ£€æµ‹çš„çš„å‰æ™¯-èƒŒæ™¯ï¼ˆFG-BGï¼‰åˆ†è§£æ–¹æ³•ï¼Œå¾€å¾€ç”±äºè¿‡åº¦ä¾èµ–å±€éƒ¨ç±»ç›¸ä¼¼æ€§å’Œå›ºå®šçš„èƒŒæ™¯æ–‘å—æå–ç­–ç•¥ï¼Œå¯¼è‡´ç¨³å¥æ€§è¾ƒä½ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„FG-BGåˆ†è§£æ¡†æ¶ï¼Œåä¸ºMamboï¼Œç”¨äºFS-OODæ£€æµ‹ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æè®®é¦–å…ˆå­¦ä¹ èƒŒæ™¯æç¤ºæ¥è·å¾—åŒ…å«èƒŒæ™¯å’Œå›¾åƒè¯­ä¹‰ä¿¡æ¯çš„å±€éƒ¨èƒŒæ™¯ç›¸ä¼¼æ€§ï¼Œç„¶åä½¿ç”¨å±€éƒ¨ç±»ç›¸ä¼¼æ€§å¯¹å±€éƒ¨èƒŒæ™¯ç›¸ä¼¼æ€§è¿›è¡Œç²¾ç‚¼ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ä½¿ç”¨ç²¾ç‚¼çš„å±€éƒ¨èƒŒæ™¯ç›¸ä¼¼æ€§å’Œå±€éƒ¨ç±»ç›¸ä¼¼æ€§è¿›è¡ŒèƒŒæ™¯æå–ï¼Œå‡å°‘äº†ä»¥å‰æ–¹æ³•å¯¹å±€éƒ¨ç±»ç›¸ä¼¼æ€§çš„ä¾èµ–ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†æ–‘å—è‡ªæ ¡å‡†è°ƒæ•´ï¼Œä»¥è€ƒè™‘æ ·æœ¬å¤šæ ·æ€§ï¼Œçµæ´»é€‰æ‹©ä¸åŒæ ·æœ¬çš„èƒŒæ™¯æ–‘å—æ•°é‡ï¼Œä»è€Œè§£å†³äº†ä»¥å‰æ–¹æ³•ä¸­å›ºå®šèƒŒæ™¯æå–ç­–ç•¥çš„é—®é¢˜ã€‚åœ¨çœŸå®æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„Mamboåœ¨OODæ£€æµ‹å’Œæ¥è¿‘OODæ£€æµ‹è®¾ç½®æ–¹é¢å®ç°äº†æœ€ä½³æ€§èƒ½ï¼Œä¸æœ€æ–°æ–¹æ³•ç›¸æ¯”å…·æœ‰ä¼˜åŠ¿ã€‚æºä»£ç å°†å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/YuzunoKawori/Mambo%E3%80%82">https://github.com/YuzunoKawori/Mamboã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.21055v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§æ–°çš„å‰æ™¯èƒŒæ™¯ï¼ˆFG-BGï¼‰åˆ†è§£æ¡†æ¶Mamboï¼Œç”¨äºå°‘æ ·æœ¬åˆ†å¸ƒå¤–ï¼ˆFS-OODï¼‰æ£€æµ‹ã€‚é€šè¿‡å¼•å…¥èƒŒæ™¯æç¤ºå­¦ä¹ å±€éƒ¨èƒŒæ™¯ç›¸ä¼¼æ€§ï¼Œå¹¶ç»“åˆå±€éƒ¨ç±»ç›¸ä¼¼æ€§è¿›è¡Œç²¾ç‚¼ã€‚åŒæ—¶ï¼Œé‡‡ç”¨è¡¥ä¸è‡ªæ ¡å‡†è°ƒæ•´ç­–ç•¥çµæ´»é€‰æ‹©ä¸åŒæ ·æœ¬çš„èƒŒæ™¯è¡¥ä¸æ•°é‡ï¼Œä»¥è§£å†³å…ˆå‰æ–¹æ³•ä¸­å›ºå®šèƒŒæ™¯æå–ç­–ç•¥çš„é—®é¢˜ã€‚å®éªŒè¡¨æ˜ï¼ŒMamboåœ¨ç°å®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„æ€§èƒ½æœ€ä½³ï¼Œä¸ç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼Œåœ¨OODæ£€æµ‹å’Œè¿‘OODæ£€æµ‹è®¾ç½®ä¸­å‡è¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Mamboæ¡†æ¶é’ˆå¯¹å°‘æ ·æœ¬åˆ†å¸ƒå¤–ï¼ˆFS-OODï¼‰æ£€æµ‹æå‡ºæ–°çš„å‰æ™¯èƒŒæ™¯åˆ†è§£æ–¹æ³•ã€‚</li>
<li>å¼•å…¥èƒŒæ™¯æç¤ºå­¦ä¹ å±€éƒ¨èƒŒæ™¯ç›¸ä¼¼æ€§ï¼ŒåŒ…å«èƒŒæ™¯ä¸å›¾åƒè¯­ä¹‰ä¿¡æ¯ã€‚</li>
<li>ç»“åˆå±€éƒ¨ç±»ç›¸ä¼¼æ€§å¯¹å±€éƒ¨èƒŒæ™¯ç›¸ä¼¼æ€§è¿›è¡Œç²¾ç‚¼ã€‚</li>
<li>é‡‡ç”¨è¡¥ä¸è‡ªæ ¡å‡†è°ƒæ•´ç­–ç•¥ï¼Œçµæ´»é€‰æ‹©ä¸åŒæ ·æœ¬çš„èƒŒæ™¯è¡¥ä¸æ•°é‡ã€‚</li>
<li>è§£å†³ç°æœ‰æ–¹æ³•è¿‡åº¦ä¾èµ–å±€éƒ¨ç±»ç›¸ä¼¼æ€§å’Œå›ºå®šèƒŒæ™¯è¡¥ä¸æå–ç­–ç•¥çš„é—®é¢˜ã€‚</li>
<li>åœ¨çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMamboåœ¨OODæ£€æµ‹å’Œè¿‘OODæ£€æµ‹è®¾ç½®ä¸­æ€§èƒ½æœ€ä½³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.21055">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Few-Shot/2509.21055v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Few-Shot/2509.21055v1/page_1_0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="FSMODNet-A-Closer-Look-at-Few-Shot-Detection-in-Multispectral-Data"><a href="#FSMODNet-A-Closer-Look-at-Few-Shot-Detection-in-Multispectral-Data" class="headerlink" title="FSMODNet: A Closer Look at Few-Shot Detection in Multispectral Data"></a>FSMODNet: A Closer Look at Few-Shot Detection in Multispectral Data</h2><p><strong>Authors:Manuel Nkegoum, Minh-Tan Pham, Ã‰lisa Fromont, Bruno Avignon, SÃ©bastien LefÃ¨vre</strong></p>
<p>Few-shot multispectral object detection (FSMOD) addresses the challenge of detecting objects across visible and thermal modalities with minimal annotated data. In this paper, we explore this complex task and introduce a framework named â€œFSMODNetâ€ that leverages cross-modality feature integration to improve detection performance even with limited labels. By effectively combining the unique strengths of visible and thermal imagery using deformable attention, the proposed method demonstrates robust adaptability in complex illumination and environmental conditions. Experimental results on two public datasets show effective object detection performance in challenging low-data regimes, outperforming several baselines we established from state-of-the-art models. All code, models, and experimental data splits can be found at <a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/Test-B48D">https://anonymous.4open.science/r/Test-B48D</a>. </p>
<blockquote>
<p>å°‘é‡æ ·æœ¬å¤šå…‰è°±ç›®æ ‡æ£€æµ‹ï¼ˆFSMODï¼‰è§£å†³äº†åœ¨å¯è§å…‰å’Œçƒ­æˆåƒæ¨¡æ€ä¸‹ï¼Œåˆ©ç”¨å°‘é‡æ ‡æ³¨æ•°æ®è¿›è¡Œç›®æ ‡æ£€æµ‹çš„éš¾é¢˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ¢è®¨äº†è¿™é¡¹å¤æ‚çš„ä»»åŠ¡ï¼Œå¹¶å¼•å…¥äº†ä¸€ä¸ªåä¸ºâ€œFSMODNetâ€çš„æ¡†æ¶ï¼Œå®ƒé€šè¿‡è·¨æ¨¡æ€ç‰¹å¾èåˆæ¥æé«˜å³ä½¿åœ¨æ ‡ç­¾æœ‰é™çš„æƒ…å†µä¸‹æ£€æµ‹æ€§èƒ½ã€‚é€šè¿‡ä½¿ç”¨å¯å˜å½¢æ³¨æ„åŠ›æœ‰æ•ˆç»“åˆå¯è§å…‰å’Œçƒ­æˆåƒçš„ç‹¬ç‰¹ä¼˜åŠ¿ï¼Œè¯¥æ–¹æ³•åœ¨å¤æ‚å…‰ç…§å’Œç¯å¢ƒæ¡ä»¶ä¸‹è¡¨ç°å‡ºå¼ºå¤§çš„é€‚åº”æ€§ã€‚åœ¨ä¸¤ä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨æŒ‘æˆ˜æ€§çš„ä½æ•°æ®æ¡ä»¶ä¸‹ï¼Œè¯¥æ–¹æ³•å¯å®ç°æœ‰æ•ˆçš„ç›®æ ‡æ£€æµ‹æ€§èƒ½ï¼Œè¶…è¶Šäº†æˆ‘ä»¬æ ¹æ®æœ€æ–°æ¨¡å‹è®¾å®šçš„å‡ ä¸ªåŸºå‡†çº¿ã€‚æ‰€æœ‰ä»£ç ã€æ¨¡å‹å’Œå®éªŒæ•°æ®åˆ†å‰²å¯åœ¨<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/Test-B48D%E6%89%BE%E5%88%B0%E3%80%82">https://anonymous.4open.science/r/Test-B48Dæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.20905v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºFSMODNetçš„æ¡†æ¶ï¼Œç”¨äºè§£å†³åœ¨æœ‰å°‘é‡æ ‡æ³¨æ•°æ®çš„æƒ…å†µä¸‹è¿›è¡Œè·¨å¯è§å…‰å’Œçƒ­æˆåƒæ¨¡æ€çš„ç›®æ ‡æ£€æµ‹é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡è·¨æ¨¡æ€ç‰¹å¾èåˆå’Œå¯å˜å½¢æ³¨æ„åŠ›æœºåˆ¶ï¼Œæœ‰æ•ˆç»“åˆäº†ä¸¤ç§æ¨¡æ€çš„ç‹¬ç‰¹ä¼˜åŠ¿ï¼Œåœ¨å¤æ‚çš„ç…§æ˜å’Œç¯å¢ƒæ¡ä»¶ä¸‹å±•ç°äº†å‡ºè‰²çš„é€‚åº”èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨æœ‰é™çš„æ ‡æ³¨æ•°æ®ä¸‹ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿæœ‰æ•ˆæ£€æµ‹ç›®æ ‡ï¼Œä¼˜äºå¤šä¸ªä½¿ç”¨å…ˆè¿›æ¨¡å‹å»ºç«‹çš„åŸºå‡†æµ‹è¯•ã€‚å…·ä½“ä¿¡æ¯å¯è®¿é—®ç›¸åº”é“¾æ¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FSMODNetæ¡†æ¶è§£å†³äº†åœ¨å°‘é‡æ ‡æ³¨æ•°æ®ä¸‹è·¨æ¨¡æ€ç›®æ ‡æ£€æµ‹çš„éš¾é¢˜ã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡è·¨æ¨¡æ€ç‰¹å¾èåˆæé«˜äº†æ£€æµ‹æ€§èƒ½ã€‚</li>
<li>å¯å˜å½¢æ³¨æ„åŠ›æœºåˆ¶è¢«ç”¨äºç»“åˆå¯è§å…‰å’Œçƒ­æˆåƒæ¨¡æ€çš„ç‹¬ç‰¹ä¼˜åŠ¿ã€‚</li>
<li>FSMODNetåœ¨å¤æ‚çš„ç…§æ˜å’Œç¯å¢ƒæ¡ä»¶ä¸‹å…·æœ‰å‡ºè‰²çš„é€‚åº”èƒ½åŠ›ã€‚</li>
<li>å®éªŒç»“æœè¯æ˜äº†è¯¥æ¡†æ¶åœ¨æœ‰é™æ ‡æ³¨æ•°æ®ä¸‹çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚</li>
<li>è®ºæ–‡æä¾›çš„ä»£ç ã€æ¨¡å‹å’Œå®éªŒæ•°æ®å¯å…¬å¼€è®¿é—®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.20905">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Few-Shot/2509.20905v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Few-Shot/2509.20905v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Few-Shot/2509.20905v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Few-Shot/2509.20905v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="DAC-LoRA-Dynamic-Adversarial-Curriculum-for-Efficient-and-Robust-Few-Shot-Adaptation"><a href="#DAC-LoRA-Dynamic-Adversarial-Curriculum-for-Efficient-and-Robust-Few-Shot-Adaptation" class="headerlink" title="DAC-LoRA: Dynamic Adversarial Curriculum for Efficient and Robust   Few-Shot Adaptation"></a>DAC-LoRA: Dynamic Adversarial Curriculum for Efficient and Robust   Few-Shot Adaptation</h2><p><strong>Authors:Ved Umrajkar</strong></p>
<p>Vision-Language Models (VLMs) are foundational to critical applications like autonomous driving, medical diagnosis, and content moderation. While Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA enable their efficient adaptation to specialized tasks, these models remain vulnerable to adversarial attacks that can compromise safety-critical decisions. CLIP, the backbone for numerous downstream VLMs, is a high-value target whose vulnerabilities can cascade across the multimodal AI ecosystem. We propose Dynamic Adversarial Curriculum DAC-LoRA, a novel framework that integrates adversarial training into PEFT. The core principle of our method i.e. an intelligent curriculum of progressively challenging attack, is general and can potentially be applied to any iterative attack method. Guided by the First-Order Stationary Condition (FOSC) and a TRADES-inspired loss, DAC-LoRA achieves substantial improvements in adversarial robustness without significantly compromising clean accuracy. Our work presents an effective, lightweight, and broadly applicable method to demonstrate that the DAC-LoRA framework can be easily integrated into a standard PEFT pipeline to significantly enhance robustness. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨è‡ªåŠ¨é©¾é©¶ã€åŒ»ç–—è¯Šæ–­å’Œå†…å®¹å®¡æ ¸ç­‰å…³é”®åº”ç”¨ä¸­å…·æœ‰åŸºç¡€æ€§ä½œç”¨ã€‚è™½ç„¶LoRAç­‰å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•èƒ½å¤Ÿå®ç°è¿™äº›æ¨¡å‹å¯¹ç‰¹å®šä»»åŠ¡çš„æ•ˆç‡é€‚åº”ï¼Œä½†è¿™äº›æ¨¡å‹ä»ç„¶å®¹æ˜“å—åˆ°å¯èƒ½å±åŠå®‰å…¨å†³ç­–çš„å¯¹æŠ—æ€§æ”»å‡»çš„å½±å“ã€‚CLIPä½œä¸ºä¼—å¤šä¸‹æ¸¸VLMsçš„éª¨å¹²ï¼Œæ˜¯ä¸€ä¸ªé«˜ä»·å€¼ç›®æ ‡ï¼Œå…¶æ¼æ´å¯èƒ½ä¼šåœ¨å¤šæ¨¡æ€AIç”Ÿæ€ç³»ç»Ÿä¸­äº§ç”Ÿè¿é”ååº”ã€‚æˆ‘ä»¬æå‡ºäº†åŠ¨æ€å¯¹æŠ—è¯¾ç¨‹ï¼ˆDACï¼‰LoRAè¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œå®ƒå°†å¯¹æŠ—æ€§è®­ç»ƒæ•´åˆåˆ°PEFTä¸­ã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„æ ¸å¿ƒåŸåˆ™ï¼Œå³æ™ºèƒ½è¯¾ç¨‹æ¸è¿›æŒ‘æˆ˜æ”»å‡»æ³•ï¼Œæ˜¯é€šç”¨çš„ï¼Œå¯åº”ç”¨äºä»»ä½•è¿­ä»£æ”»å‡»æ–¹æ³•ã€‚é€šè¿‡ä¸€é˜¶å¹³ç¨³æ¡ä»¶ï¼ˆFOSCï¼‰å’Œå—TRéƒ¨é—¨å¯å‘ä¸‹çš„æŸå¤±ï¼ˆTRå¯¹å¢ç›ŠæŒ‡æ ‡å¯èƒ½ä¹Ÿè¡¨è¿°ä¸ºä¸€ç§æŸå¤±å‡½æ•°ï¼‰çš„æŒ‡å¯¼ï¼ŒDAC-LoRAåœ¨ä¸å½±å“æ¸…æ´ç²¾åº¦çš„æƒ…å†µä¸‹ï¼Œå®ç°äº†å¯¹æŠ—æ€§ç¨³å¥æ€§çš„å®è´¨æ€§æå‡ã€‚æˆ‘ä»¬çš„å·¥ä½œæå‡ºäº†ä¸€ç§æœ‰æ•ˆã€è½»ä¾¿ä¸”å¹¿æ³›åº”ç”¨çš„æ–¹æ³•ï¼Œè¯æ˜DAC-LoRAæ¡†æ¶å¯ä»¥è½»æ¾åœ°é›†æˆåˆ°æ ‡å‡†PEFTç®¡é“ä¸­ï¼Œä»è€Œæ˜¾è‘—æé«˜ç¨³å¥æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.20792v1">PDF</a> Accepted at ICCV2025 Workshop on Safe and Trustworthy Multimodal AI   Systems</p>
<p><strong>Summary</strong></p>
<p>VLMï¼ˆè§†è§‰è¯­è¨€æ¨¡å‹ï¼‰åœ¨è‡ªåŠ¨é©¾é©¶ã€åŒ»ç–—è¯Šæ–­å’Œå†…å®¹å®¡æ ¸ç­‰å…³é”®åº”ç”¨ä¸­æ‰®æ¼”ç€é‡è¦è§’è‰²ã€‚è™½ç„¶ä½¿ç”¨LoRAç­‰å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•å¯ä»¥ä½¿å…¶é€‚åº”ç‰¹å®šä»»åŠ¡ï¼Œä½†è¿™äº›æ¨¡å‹ä»ç„¶é¢ä¸´æ˜“äºå—åˆ°å¨èƒçš„é£é™©ï¼Œå­˜åœ¨å¯èƒ½å¨èƒåˆ°å®‰å…¨å†³ç­–çš„å¯¹æŠ—æ€§æ”»å‡»ã€‚CLIPä½œä¸ºä¼—å¤šä¸‹æ¸¸VLMçš„éª¨å¹²ï¼Œæ˜¯ä¸€ä¸ªé«˜é£é™©ç›®æ ‡ï¼Œå…¶æ¼æ´å¯èƒ½ä¼šæ³¢åŠæ•´ä¸ªå¤šåª’ä½“äººå·¥æ™ºèƒ½ç”Ÿæ€ç³»ç»Ÿã€‚æœ¬æ–‡æå‡ºå°†å¯¹æŠ—æ€§è®­ç»ƒæ•´åˆåˆ°PEFTä¸­çš„åŠ¨æ€å¯¹æŠ—è¯¾ç¨‹DAC-LoRAæ–°æ¡†æ¶ã€‚è¯¥æ–¹æ³•çš„æ ¸å¿ƒåŸåˆ™å³æ˜¯ä¸€ç§æ™ºèƒ½çš„è¯¾ç¨‹ï¼ŒåŒ…æ‹¬æ¸è¿›æŒ‘æˆ˜çš„æ”»å‡»æ–¹å¼ï¼Œå…¶æ™®éé€‚ç”¨äºä»»ä½•è¿­ä»£æ”»å‡»æ–¹æ³•ã€‚ç»“åˆä¸€é˜¶å¹³ç¨³æ¡ä»¶ï¼ˆFOSCï¼‰å’ŒåŸºäºTRlaDESçš„æŸå¤±ï¼ŒDAC-LoRAåœ¨ä¸æ˜¾è‘—é™ä½æ¸…æ´ç²¾åº¦çš„æƒ…å†µä¸‹å®ç°äº†å¯¹æŠ—æ€§ç¨³å¥æ€§çš„æ˜¾è‘—æé«˜ã€‚æœ¬ç ”ç©¶æä¾›äº†ä¸€ç§æœ‰æ•ˆã€è½»ä¾¿ä¸”å¹¿æ³›é€‚ç”¨çš„æ–¹æ³•ï¼Œè¯æ˜äº†DAC-LoRAæ¡†æ¶å¯ä»¥è½»æ¾åœ°é›†æˆåˆ°æ ‡å‡†PEFTç®¡é“ä¸­ï¼Œä»è€Œæ˜¾è‘—æé«˜ç¨³å¥æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VLMsåœ¨ç°ä»£åº”ç”¨å¦‚è‡ªåŠ¨é©¾é©¶ã€åŒ»ç–—è¯Šæ–­å’Œå†…å®¹å®¡æ ¸ä¸­æ‰®æ¼”ç€å…³é”®è§’è‰²ï¼Œä½†å…¶æ˜“å—å¯¹æŠ—æ€§æ”»å‡»å½±å“ï¼Œè¿™å¯èƒ½å±åŠå®‰å…¨å†³ç­–ã€‚</li>
<li>DAC-LoRAæ¡†æ¶æ˜¯ä¸€ç§æ–°é¢–çš„ã€ç»“åˆäº†å¯¹æŠ—è®­ç»ƒä¸å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰çš„æ–¹æ³•ã€‚</li>
<li>DAC-LoRAé€šè¿‡æ™ºèƒ½è¯¾ç¨‹çš„æ–¹å¼ï¼ŒåŒ…æ‹¬æ¸è¿›æŒ‘æˆ˜çš„æ”»å‡»ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ€§ï¼Œå¯åº”ç”¨äºä»»ä½•è¿­ä»£æ”»å‡»æ–¹æ³•ã€‚</li>
<li>åˆ©ç”¨ä¸€é˜¶å¹³ç¨³æ¡ä»¶ï¼ˆFOSCï¼‰å’ŒåŸºäºTRlaDESçš„æŸå¤±ï¼ŒDAC-LoRAåœ¨å¢å¼ºæ¨¡å‹å¯¹æŠ—æ€§ç¨³å¥æ€§çš„åŒæ—¶ï¼Œä¸ä¼šæ˜¾è‘—é™ä½æ¸…æ´ç²¾åº¦ã€‚</li>
<li>å®éªŒè¯æ˜DAC-LoRAèƒ½æœ‰æ•ˆæé«˜æ¨¡å‹çš„ç¨³å¥æ€§ã€‚</li>
<li>DAC-LoRAæ¡†æ¶æ˜“äºé›†æˆåˆ°æ ‡å‡†çš„PEFTæµç¨‹ä¸­ï¼Œå…·æœ‰å®é™…åº”ç”¨ä»·å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.20792">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Few-Shot/2509.20792v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Few-Shot/2509.20792v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Few-Shot/2509.20792v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Few-Shot/2509.20792v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="EchoBench-Benchmarking-Sycophancy-in-Medical-Large-Vision-Language-Models"><a href="#EchoBench-Benchmarking-Sycophancy-in-Medical-Large-Vision-Language-Models" class="headerlink" title="EchoBench: Benchmarking Sycophancy in Medical Large Vision-Language   Models"></a>EchoBench: Benchmarking Sycophancy in Medical Large Vision-Language   Models</h2><p><strong>Authors:Botai Yuan, Yutian Zhou, Yingjie Wang, Fushuo Huo, Yongcheng Jing, Li Shen, Ying Wei, Zhiqi Shen, Ziwei Liu, Tianwei Zhang, Jie Yang, Dacheng Tao</strong></p>
<p>Recent benchmarks for medical Large Vision-Language Models (LVLMs) emphasize leaderboard accuracy, overlooking reliability and safety. We study sycophancy â€“ modelsâ€™ tendency to uncritically echo user-provided information â€“ in high-stakes clinical settings. We introduce EchoBench, a benchmark to systematically evaluate sycophancy in medical LVLMs. It contains 2,122 images across 18 departments and 20 modalities with 90 prompts that simulate biased inputs from patients, medical students, and physicians. We evaluate medical-specific, open-source, and proprietary LVLMs. All exhibit substantial sycophancy; the best proprietary model (Claude 3.7 Sonnet) still shows 45.98% sycophancy, and GPT-4.1 reaches 59.15%. Many medical-specific models exceed 95% sycophancy despite only moderate accuracy. Fine-grained analyses by bias type, department, perceptual granularity, and modality identify factors that increase susceptibility. We further show that higher data quality&#x2F;diversity and stronger domain knowledge reduce sycophancy without harming unbiased accuracy. EchoBench also serves as a testbed for mitigation: simple prompt-level interventions (negative prompting, one-shot, few-shot) produce consistent reductions and motivate training- and decoding-time strategies. Our findings highlight the need for robust evaluation beyond accuracy and provide actionable guidance toward safer, more trustworthy medical LVLMs. </p>
<blockquote>
<p>è¿‘æœŸé’ˆå¯¹åŒ»ç–—é¢†åŸŸçš„å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰çš„åŸºå‡†æµ‹è¯•ä¸»è¦ä¾§é‡äºæ’è¡Œæ¦œçš„å‡†ç¡®æ€§ï¼Œå´å¿½è§†äº†å¯é æ€§å’Œå®‰å…¨æ€§ã€‚æˆ‘ä»¬ç ”ç©¶äº†åœ¨ä¸´åºŠç¯å¢ƒä¸­é«˜é£é™©çš„æ¨¡å‹ç›²ç›®é‡å¤ç”¨æˆ·æä¾›çš„ä¿¡æ¯çš„å€¾å‘ï¼Œå³é¡ºæ‰¿æ€§ã€‚æˆ‘ä»¬å¼•å…¥äº†EchoBenchï¼Œä¸€ä¸ªç³»ç»Ÿè¯„ä¼°åŒ»ç–—LVLMsä¸­é¡ºæ‰¿æ€§çš„åŸºå‡†æµ‹è¯•ã€‚å®ƒåŒ…å«18ä¸ªç§‘å®¤çš„2,122å¼ å›¾åƒï¼Œä»¥åŠæ¨¡æ‹Ÿæ¥è‡ªæ‚£è€…ã€åŒ»å­¦ç”Ÿã€åŒ»ç”Ÿçš„åè§è¾“å…¥çš„20ç§æ¨¡æ€å’Œ90ä¸ªæç¤ºã€‚æˆ‘ä»¬è¯„ä¼°äº†åŒ»ç–—ä¸“ç”¨çš„ã€å¼€æºçš„å’Œä¸“æœ‰çš„LVLMsã€‚æ‰€æœ‰æ¨¡å‹éƒ½è¡¨ç°å‡ºæ˜æ˜¾çš„é¡ºæ‰¿æ€§ï¼›è¡¨ç°æœ€ä½³çš„ä¸“æœ‰æ¨¡å‹ï¼ˆClaude 3.7 Sonnetï¼‰ä»æœ‰45.98%çš„é¡ºæ‰¿æ€§ï¼ŒGPT-4.1è¾¾åˆ°59.15%ã€‚å°½ç®¡å‡†ç¡®æ€§åªæœ‰ä¸­ç­‰æ°´å¹³ï¼Œä½†è®¸å¤šåŒ»ç–—ä¸“ç”¨æ¨¡å‹çš„é¡ºæ‰¿æ€§è¶…è¿‡95%ã€‚é€šè¿‡åè§ç±»å‹ã€ç§‘å®¤ã€æ„ŸçŸ¥ç²’åº¦å’Œæ¨¡æ€çš„ç²¾ç»†åˆ†æï¼Œæˆ‘ä»¬ç¡®å®šäº†å¢åŠ é¡ºæ‰¿æ€§çš„å› ç´ ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è¡¨æ˜ï¼Œæ›´é«˜çš„æ•°æ®è´¨é‡å’Œå¤šæ ·æ€§ä»¥åŠæ›´å¼ºçš„é¢†åŸŸçŸ¥è¯†å¯ä»¥å‡å°‘é¡ºæ‰¿æ€§ï¼Œè€Œä¸æŸå®³æ— åè§å‡†ç¡®æ€§ã€‚EchoBenchä¹Ÿä½œä¸ºç¼“è§£æµ‹è¯•çš„æµ‹è¯•åºŠï¼šç®€å•çš„æç¤ºçº§åˆ«å¹²é¢„ï¼ˆè´Ÿé¢æç¤ºã€å•æ¬¡æç¤ºã€å°‘æ¬¡æç¤ºï¼‰äº§ç”Ÿäº†ä¸€è‡´çš„å‡å°‘ï¼Œå¹¶æ¿€åŠ±äº†è®­ç»ƒå’Œè§£ç æ—¶é—´ç­–ç•¥ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœå¼ºè°ƒäº†é™¤äº†å‡†ç¡®æ€§ä¹‹å¤–è¿›è¡Œç¨³å¥è¯„ä¼°çš„å¿…è¦æ€§ï¼Œå¹¶ä¸ºæ„å»ºæ›´å®‰å…¨ã€æ›´å¯ä¿¡èµ–çš„åŒ»ç–—LVLMsæä¾›äº†å¯æ“ä½œæ€§çš„æŒ‡å¯¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.20146v1">PDF</a> 29 pages, 6 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å…³æ³¨åŒ»ç–—é¢†åŸŸçš„å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰çš„å¯é æ€§åŠå®‰å…¨é—®é¢˜ã€‚ç ”ç©¶æ¨¡å‹è¿‡åº¦è¿åˆç”¨æˆ·è¾“å…¥ä¿¡æ¯çš„å€¾å‘ï¼Œå³â€œå¥‰æ‰¿ç°è±¡â€ï¼Œå¹¶ä»‹ç»äº†ä¸€ä¸ªæ–°çš„è¯„ä¼°åŸºå‡†EchoBenchã€‚è¯¥åŸºå‡†é€šè¿‡æ¨¡æ‹Ÿæ¥è‡ªæ‚£è€…ã€åŒ»å­¦ç”ŸåŠåŒ»å¸ˆçš„åè§è¾“å…¥ï¼Œç³»ç»Ÿåœ°è¯„ä¼°åŒ»ç–—LVLMsçš„å¥‰æ‰¿ç°è±¡ã€‚å‘ç°æ‰€æœ‰è¯„ä¼°çš„æ¨¡å‹éƒ½å­˜åœ¨æ˜¾è‘—çš„å¥‰æ‰¿ç°è±¡ï¼Œå³ä½¿æ˜¯æœ€ä¼˜ç§€çš„æ¨¡å‹ä¹Ÿä¸ä¾‹å¤–ã€‚æ­¤å¤–ï¼Œè¿˜å‘ç°æ•°æ®è´¨é‡å’Œå¤šæ ·æ€§ä»¥åŠé¢†åŸŸçŸ¥è¯†çš„å¢å¼ºå¯ä»¥å‡å°‘å¥‰æ‰¿ç°è±¡ï¼ŒåŒæ—¶ä¸å½±å“éåè§å‡†ç¡®æ€§ã€‚EchoBenchä¹Ÿä¸ºç¼“è§£è¯¥é—®é¢˜æä¾›äº†æ–¹æ³•ï¼Œç®€å•çš„æç¤ºçº§åˆ«å¹²é¢„æªæ–½å¯ä»¥äº§ç”Ÿä¸€è‡´çš„å‡å°‘æ•ˆæœï¼Œå¹¶æ¿€å‘è®­ç»ƒå’Œè§£ç æ—¶é—´ç­–ç•¥ã€‚ç ”ç©¶å¼ºè°ƒäº†é™¤äº†å‡†ç¡®æ€§ä¹‹å¤–ï¼Œå¯¹åŒ»ç–—LVLMsè¿›è¡Œç¨³å¥è¯„ä¼°çš„å¿…è¦æ€§ï¼Œå¹¶ä¸ºæ„å»ºæ›´å®‰å…¨ã€æ›´å¯ä¿¡èµ–çš„åŒ»ç–—LVLMsæä¾›äº†å®é™…æŒ‡å¯¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»ç–—LVLMsçš„å¥‰æ‰¿ç°è±¡ç ”ç©¶è‡³å…³é‡è¦ï¼Œè¿™æ¶‰åŠåˆ°æ¨¡å‹å¯¹ç”¨æˆ·æä¾›çš„æ— æ‰¹åˆ¤æ€§å›å£°çš„å€¾å‘ã€‚</li>
<li>EchoBenchåŸºå‡†è¢«å¼•å…¥ä»¥ç³»ç»Ÿåœ°è¯„ä¼°åŒ»ç–—LVLMsçš„å¥‰æ‰¿ç°è±¡ï¼Œæ¶µç›–å¤šä¸ªéƒ¨é—¨ã€æ¨¡å¼å’Œæç¤ºã€‚</li>
<li>æ‰€æœ‰è¯„ä¼°çš„æ¨¡å‹ï¼ŒåŒ…æ‹¬åŒ»ç–—ä¸“ç”¨ã€å¼€æºå’Œä¸“æœ‰LVLMsï¼Œéƒ½å­˜åœ¨æ˜¾è‘—çš„å¥‰æ‰¿ç°è±¡ã€‚</li>
<li>GPT-4.1åœ¨å¥‰æ‰¿ç°è±¡æ–¹é¢çš„è¡¨ç°è¾ƒå·®ï¼Œæœ€ä½³ä¸“æœ‰æ¨¡å‹Claude 3.7 Sonnetä¹Ÿæœ‰è¾ƒé«˜ç¨‹åº¦çš„å¥‰æ‰¿ã€‚</li>
<li>å¥‰æ‰¿ç°è±¡ä¸æ¨¡å‹çš„å‡†ç¡®æ€§å¹¶ä¸æ€»æ˜¯æ­£ç›¸å…³ï¼Œæœ‰äº›æ¨¡å‹çš„å¥‰æ‰¿ç°è±¡è¶…è¿‡95%ã€‚</li>
<li>æ•°æ®è´¨é‡å’Œå¤šæ ·æ€§ä»¥åŠé¢†åŸŸçŸ¥è¯†çš„å¢å¼ºæœ‰åŠ©äºå‡å°‘å¥‰æ‰¿ç°è±¡è€Œä¸å½±å“éåè§å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.20146">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Few-Shot/2509.20146v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Few-Shot/2509.20146v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Few-Shot/2509.20146v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Few-Shot/2509.20146v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="MMSE-Calibrated-Few-Shot-Prompting-for-Alzheimerâ€™s-Detection"><a href="#MMSE-Calibrated-Few-Shot-Prompting-for-Alzheimerâ€™s-Detection" class="headerlink" title="MMSE-Calibrated Few-Shot Prompting for Alzheimerâ€™s Detection"></a>MMSE-Calibrated Few-Shot Prompting for Alzheimerâ€™s Detection</h2><p><strong>Authors:Jana Sweidan, Mounim A. El-Yacoubi, Nasredine Semmar</strong></p>
<p>Prompting large language models is a training-free method for detecting Alzheimerâ€™s disease from speech transcripts. Using the ADReSS dataset, we revisit zero-shot prompting and study few-shot prompting with a class-balanced protocol using nested interleave and a strict schema, sweeping up to 20 examples per class. We evaluate two variants achieving state-of-the-art prompting results. (i) MMSE-Proxy Prompting: each few-shot example carries a probability anchored to Mini-Mental State Examination bands via a deterministic mapping, enabling AUC computing; this reaches 0.82 accuracy and 0.86 AUC (ii) Reasoning-augmented Prompting: few-shot examples pool is generated with a multimodal LLM (GPT-5) that takes as input the Cookie Theft image, transcript, and MMSE to output a reasoning and MMSE-aligned probability; evaluation remains transcript-only and reaches 0.82 accuracy and 0.83 AUC. To our knowledge, this is the first ADReSS study to anchor elicited probabilities to MMSE and to use multimodal construction to improve interpretability. </p>
<blockquote>
<p>é€šè¿‡è¯­éŸ³è½¬å½•æ£€æµ‹é˜¿å°”èŒ¨æµ·é»˜ç—…çš„ä¸€ç§æ— è®­ç»ƒæ–¹æ³•æ˜¯é€šè¿‡å¤§è¯­è¨€æ¨¡å‹çš„æç¤ºæ¥å®ç°çš„ã€‚æˆ‘ä»¬ä½¿ç”¨ADReSSæ•°æ®é›†é‡æ–°ç ”ç©¶äº†é›¶æ ·æœ¬æç¤ºï¼Œå¹¶ç ”ç©¶äº†ä¸€ç§ä½¿ç”¨ç±»åˆ«å¹³è¡¡åè®®çš„å°‘é‡æ ·æœ¬æç¤ºï¼Œé€šè¿‡åµŒå¥—äº¤é”™å’Œä¸¥æ ¼æ¨¡å¼æœ€å¤šä½¿ç”¨æ¯ä¸ªç±»åˆ«20ä¸ªç¤ºä¾‹ã€‚æˆ‘ä»¬è¯„ä¼°äº†ä¸¤ç§å˜ä½“ï¼Œå®ç°äº†æœ€å…ˆè¿›çš„æç¤ºç»“æœã€‚ï¼ˆiï¼‰MMSE-ä»£ç†æç¤ºï¼šæ¯ä¸ªå°‘é‡æ ·æœ¬ç¤ºä¾‹éƒ½å¸¦æœ‰æ¦‚ç‡ï¼Œé€šè¿‡ç¡®å®šæ€§æ˜ å°„ä¸Mini-MentalçŠ¶æ€æ£€æŸ¥å¸¦ç›¸å…³è”ï¼Œä»è€Œå®ç°AUCè®¡ç®—ï¼›è¿™è¾¾åˆ°äº†0.82çš„å‡†ç¡®ç‡å’Œ0.86çš„AUCï¼ˆiiï¼‰æ¨ç†å¢å¼ºæç¤ºï¼šå°‘é‡æ ·æœ¬ç¤ºä¾‹æ± æ˜¯ç”±å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆGPT-5ï¼‰ç”Ÿæˆçš„ï¼Œè¯¥æ¨¡å‹ä»¥Cookie Theftå›¾åƒã€è½¬å½•å’ŒMMSEä½œä¸ºè¾“å…¥æ¥è¾“å‡ºæ¨ç†å’Œä¸MMSEå¯¹é½çš„æ¦‚ç‡ï¼›è¯„ä¼°ä»ç„¶æ˜¯ä»…è½¬å½•çš„ï¼Œè¾¾åˆ°äº†0.82çš„å‡†ç¡®ç‡å’Œ0.83çš„AUCã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå°†ADReSSç ”ç©¶ä¸­å¾—åˆ°çš„æ¦‚ç‡ä¸MMSEç›¸å…³è”å¹¶ä½¿ç”¨å¤šæ¨¡æ€æ„å»ºæ¥æé«˜å¯è§£é‡Šæ€§çš„ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.19926v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºADReSSæ•°æ®é›†çš„ç ”ç©¶è¡¨æ˜ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„æ— è®­ç»ƒæç¤ºæ³•æ£€æµ‹é˜¿å°”èŒ¨æµ·é»˜ç—…ä»è¯­éŸ³è½¬å½•ä¸­è·å¾—é‡è¦è¿›å±•ã€‚è¯¥ç ”ç©¶é‡æ–°ç ”ç©¶äº†é›¶æç¤ºå¹¶æ¢è®¨äº†ä½¿ç”¨åµŒå¥—äº¤é”™å’Œä¸¥æ ¼æ¨¡å¼çš„ç±»å¹³è¡¡åè®®è¿›è¡Œå°‘é‡æç¤ºçš„æ–¹æ³•ï¼Œæ¯ä¸ªç±»åˆ«æœ€å¤šä½¿ç”¨20ä¸ªç¤ºä¾‹ã€‚è¯¥ç ”ç©¶è¯„ä¼°äº†ä¸¤ç§å˜ä½“æ–¹æ³•ï¼ŒåŒ…æ‹¬MMSEä»£ç†æç¤ºå’Œæ¨ç†å¢å¼ºæç¤ºï¼Œå‡è¾¾åˆ°äº†å…ˆè¿›çš„æç¤ºç»“æœã€‚å…¶ä¸­MMSEä»£ç†æç¤ºé€šè¿‡å°†æ¯ä¸ªå°‘é‡ç¤ºä¾‹ä¸åŸºäºç¡®å®šæ€§æ˜ å°„çš„Miniç²¾ç¥çŠ¶æ€æ£€æŸ¥è¯„åˆ†ç›¸ç»“åˆï¼Œä½¿å¾—å‡†ç¡®ç‡è¾¾åˆ°0.82ä¸”AUCè¾¾åˆ°0.86ã€‚æ¨ç†å¢å¼ºæç¤ºåˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå¦‚GPT-5ï¼‰ç”Ÿæˆå°‘é‡ç¤ºä¾‹æ± ï¼Œå°†å›¾åƒã€è½¬å½•å’ŒMMSEç›¸ç»“åˆè¿›è¡Œæ¨ç†ï¼Œå…¶å‡†ç¡®æ€§åŒæ ·è¾¾åˆ°äº†ä¸€å®šæ°´å¹³ã€‚è¿™äº›æ–¹æ³•çš„ç ”ç©¶å°†æœ‰åŠ©äºæ¨è¿›ä»è¯­éŸ³ä¸­æ£€æµ‹é˜¿å°”èŒ¨æµ·é»˜ç—…çš„å‡†ç¡®åº¦åŠç²¾ç¡®åº¦ã€‚ç ”ç©¶è¡¨æ˜è¯¥é¦–æ¬¡æå‡ºé€šè¿‡é›¶æ ·æœ¬æ¡ˆä¾‹é›†çš„MMSçº§åˆ«åŒ–åˆ†ææ–¹æ³•ä»¥åŠå¤šæ¨¡æ€æ„å»ºæ¥æé«˜è§£é‡Šæ€§ã€‚è¿™äº›ç ”ç©¶ä¸ºé˜¿å°”èŒ¨æµ·é»˜ç—…çš„æ—©æœŸæ£€æµ‹å’Œå¹²é¢„æä¾›äº†æ–°çš„å¯èƒ½æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯ä¸ƒä¸ªå…³äºæ–‡æœ¬çš„ä¸»è¦è§‚ç‚¹ï¼š</p>
<ul>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„è®­ç»ƒå…è´¹æ–¹æ³•æ¥æ£€æµ‹é˜¿å°”èŒ¨æµ·é»˜ç—…ä»è¯­éŸ³è½¬å½•ä¸­ã€‚</li>
<li>ç ”ç©¶é‡æ–°ç ”ç©¶äº†é›¶æç¤ºå¹¶æ¢è®¨äº†ç±»å¹³è¡¡åè®®ä¸‹çš„å°‘é‡æç¤ºæ–¹æ³•ï¼Œæ¯ä¸ªç±»åˆ«æœ€å¤šä½¿ç”¨20ä¸ªç¤ºä¾‹ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.19926">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Few-Shot/2509.19926v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Few-Shot/2509.19926v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Few-Shot/2509.19926v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Few-Shot/2509.19926v1/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Few-Shot/2509.19926v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Few-Shot/2509.19926v1/page_3_1.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="MoTiC-Momentum-Tightness-and-Contrast-for-Few-Shot-Class-Incremental-Learning"><a href="#MoTiC-Momentum-Tightness-and-Contrast-for-Few-Shot-Class-Incremental-Learning" class="headerlink" title="MoTiC: Momentum Tightness and Contrast for Few-Shot Class-Incremental   Learning"></a>MoTiC: Momentum Tightness and Contrast for Few-Shot Class-Incremental   Learning</h2><p><strong>Authors:Zeyu He, Shuai Huang, Yuwu Lu, Ming Zhao</strong></p>
<p>Few-Shot Class-Incremental Learning (FSCIL) must contend with the dual challenge of learning new classes from scarce samples while preserving old class knowledge. Existing methods use the frozen feature extractor and class-averaged prototypes to mitigate against catastrophic forgetting and overfitting. However, new-class prototypes suffer significant estimation bias due to extreme data scarcity, whereas base-class prototypes benefit from sufficient data. In this work, we theoretically demonstrate that aligning the new-class priors with old-class statistics via Bayesian analysis reduces variance and improves prototype accuracy. Furthermore, we propose large-scale contrastive learning to enforce cross-category feature tightness. To further enrich feature diversity and inject prior information for new-class prototypes, we integrate momentum self-supervision and virtual categories into the Momentum Tightness and Contrast framework (MoTiC), constructing a feature space with rich representations and enhanced interclass cohesion. Experiments on three FSCIL benchmarks produce state-of-the-art performances, particularly on the fine-grained task CUB-200, validating our methodâ€™s ability to reduce estimation bias and improve incremental learning robustness. </p>
<blockquote>
<p>Few-Shot ç±»å¢é‡å­¦ä¹ ï¼ˆFSCILï¼‰é¢ä¸´ç€å­¦ä¹ ç¨€ç¼ºæ ·æœ¬ä¸­çš„æ–°ç±»åˆ«å¹¶ä¿æŒæ—§ç±»çŸ¥è¯†çš„åŒé‡æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•ä½¿ç”¨å†»ç»“çš„ç‰¹å¾æå–å™¨å’Œç±»åˆ«å¹³å‡åŸå‹æ¥ç¼“è§£ç¾éš¾æ€§é—å¿˜å’Œè¿‡åº¦æ‹Ÿåˆçš„é—®é¢˜ã€‚ç„¶è€Œï¼Œç”±äºæç«¯çš„æ•°æ®ç¨€ç¼ºæ€§ï¼Œæ–°ç±»çš„åŸå‹ä¼šé­å—æ˜¾è‘—çš„ä¼°è®¡åå·®ï¼Œè€ŒåŸºç¡€ç±»çš„åŸå‹å—ç›Šäºå……è¶³çš„æ•°æ®ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»ç†è®ºä¸Šè¯æ˜äº†é€šè¿‡è´å¶æ–¯åˆ†æå°†æ–°ç±»å…ˆéªŒä¸æ—§ç±»ç»Ÿè®¡é‡å¯¹é½å¯ä»¥å‡å°‘æ–¹å·®å¹¶æé«˜åŸå‹å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†å¤§è§„æ¨¡å¯¹æ¯”å­¦ä¹ æ¥åŠ å¼ºè·¨ç±»åˆ«çš„ç‰¹å¾ç´§å¯†æ€§ã€‚ä¸ºäº†è¿›ä¸€æ­¥ä¸°å¯Œç‰¹å¾å¤šæ ·æ€§å’Œä¸ºæ–°ç±»åŸå‹æ³¨å…¥å…ˆéªŒä¿¡æ¯ï¼Œæˆ‘ä»¬å°†åŠ¨é‡è‡ªç›‘ç£å’Œè™šæ‹Ÿç±»åˆ«é›†æˆåˆ° Momentum Tightness and Contrast æ¡†æ¶ï¼ˆMoTiCï¼‰ä¸­ï¼Œæ„å»ºäº†ä¸€ä¸ªå…·æœ‰ä¸°å¯Œè¡¨ç¤ºå’Œå¢å¼ºç±»é—´å‡èšåŠ›çš„ç‰¹å¾ç©ºé—´ã€‚åœ¨ä¸‰ä¸ª FSCIL åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒäº§ç”Ÿäº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨ç»†ç²’åº¦ä»»åŠ¡ CUB-200 ä¸Šçš„è¡¨ç°ï¼ŒéªŒè¯äº†æˆ‘ä»¬æ–¹æ³•åœ¨å‡å°‘ä¼°è®¡åå·®å’Œæé«˜å¢é‡å­¦ä¹ ç¨³å¥æ€§æ–¹é¢çš„èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.19664v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    å°‘æ•°ç±»å¢é‡å­¦ä¹ ï¼ˆFSCILï¼‰é¢ä¸´ä»ç¨€ç¼ºæ ·æœ¬ä¸­å­¦ä¹ æ–°ç±»å¹¶ä¿æŒæ—§ç±»çŸ¥è¯†çš„åŒé‡æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•é‡‡ç”¨å†»ç»“ç‰¹å¾æå–å™¨å’Œç±»å¹³å‡åŸå‹æ¥ç¼“è§£ç¾éš¾æ€§é—å¿˜å’Œè¿‡æ‹Ÿåˆé—®é¢˜ã€‚ç„¶è€Œï¼Œç”±äºæç«¯æ•°æ®ç¨€ç¼ºï¼Œæ–°ç±»åŸå‹é­å—æ˜¾è‘—çš„ä¼°è®¡åå·®ï¼Œè€ŒåŸºæœ¬ç±»åŸå‹å—ç›Šäºå……è¶³çš„æ•°æ®ã€‚æœ¬ç ”ç©¶ä»ç†è®ºä¸Šè¯æ˜ï¼Œé€šè¿‡è´å¶æ–¯åˆ†æå°†æ–°ç±»å…ˆéªŒä¸æ—§ç±»ç»Ÿè®¡é‡å¯¹é½ï¼Œå¯ä»¥å‡å°‘æ–¹å·®å¹¶æé«˜åŸå‹å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºå¤§è§„æ¨¡å¯¹æ¯”å­¦ä¹ æ¥å¼ºåˆ¶æ‰§è¡Œè·¨ç±»åˆ«çš„ç‰¹å¾ç´§å¯†æ€§ã€‚ä¸ºäº†è¿›ä¸€æ­¥ä¼˜åŒ–ç‰¹å¾å¤šæ ·æ€§å’Œæ³¨å…¥æ–°ç±»åŸå‹ä¸­çš„å…ˆéªŒä¿¡æ¯ï¼Œæˆ‘ä»¬å°†åŠ¨é‡è‡ªæˆ‘ç›‘ç£å’Œè™šæ‹Ÿç±»åˆ«æ•´åˆåˆ°Momentum Tightness and Contrastæ¡†æ¶ï¼ˆMoTiCï¼‰ä¸­ï¼Œæ„å»ºäº†ä¸€ä¸ªå…·æœ‰ä¸°å¯Œçš„è¡¨ç¤ºå’Œå¢å¼ºçš„ç±»é—´å‡èšåŠ›çš„ç‰¹å¾ç©ºé—´ã€‚åœ¨ä¸‰ä¸ªFSCILåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒäº§ç”Ÿäº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨ç»†ç²’åº¦ä»»åŠ¡CUB-200ä¸Šçš„è¡¨ç°éªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•å‡å°‘ä¼°è®¡åå·®å’Œæé«˜å¢é‡å­¦ä¹ ç¨³å¥æ€§çš„èƒ½åŠ›ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>Few-Shot Class-Incremental Learning (FSCIL) é¢ä¸´å­¦ä¹ æ–°ç±»å’Œä¿æŒæ—§ç±»çŸ¥è¯†çš„åŒé‡æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä½¿ç”¨å†»ç»“ç‰¹å¾æå–å™¨å’Œç±»å¹³å‡åŸå‹æ¥åº”å¯¹æŒ‘æˆ˜ã€‚</li>
<li>æ–°ç±»åŸå‹å› æ•°æ®ç¨€ç¼ºè€Œé­å—æ˜¾è‘—ä¼°è®¡åå·®ï¼Œè€ŒåŸºäºå……è¶³æ•°æ®çš„æ—§ç±»åŸå‹åˆ™ç›¸å¯¹ç¨³å®šã€‚</li>
<li>é€šè¿‡è´å¶æ–¯åˆ†æå¯¹é½æ–°ç±»å…ˆéªŒä¸æ—§ç±»ç»Ÿè®¡é‡ï¼Œå¯ä»¥æé«˜åŸå‹å‡†ç¡®æ€§å¹¶å‡å°‘æ–¹å·®ã€‚</li>
<li>æå‡ºå¤§è§„æ¨¡å¯¹æ¯”å­¦ä¹ æ¥åŠ å¼ºè·¨ç±»åˆ«çš„ç‰¹å¾ç´§å¯†æ€§ã€‚</li>
<li>æ•´åˆåŠ¨é‡è‡ªæˆ‘ç›‘ç£å’Œè™šæ‹Ÿç±»åˆ«åˆ°MoTiCæ¡†æ¶ä¸­ï¼Œä¸°å¯Œç‰¹å¾è¡¨ç¤ºå¹¶å¢å¼ºç±»é—´å‡èšåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.19664">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Few-Shot/2509.19664v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Few-Shot/2509.19664v1/page_2_0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="RoboSSM-Scalable-In-context-Imitation-Learning-via-State-Space-Models"><a href="#RoboSSM-Scalable-In-context-Imitation-Learning-via-State-Space-Models" class="headerlink" title="RoboSSM: Scalable In-context Imitation Learning via State-Space Models"></a>RoboSSM: Scalable In-context Imitation Learning via State-Space Models</h2><p><strong>Authors:Youngju Yoo, Jiaheng Hu, Yifeng Zhu, Bo Liu, Qiang Liu, Roberto MartÃ­n-MartÃ­n, Peter Stone</strong></p>
<p>In-context imitation learning (ICIL) enables robots to learn tasks from prompts consisting of just a handful of demonstrations. By eliminating the need for parameter updates at deployment time, this paradigm supports few-shot adaptation to novel tasks. However, recent ICIL methods rely on Transformers, which have computational limitations and tend to underperform when handling longer prompts than those seen during training. In this work, we introduce RoboSSM, a scalable recipe for in-context imitation learning based on state-space models (SSM). Specifically, RoboSSM replaces Transformers with Longhorn â€“ a state-of-the-art SSM that provides linear-time inference and strong extrapolation capabilities, making it well-suited for long-context prompts. We evaluate our approach on the LIBERO benchmark and compare it against strong Transformer-based ICIL baselines. Experiments show that RoboSSM extrapolates effectively to varying numbers of in-context demonstrations, yields high performance on unseen tasks, and remains robust in long-horizon scenarios. These results highlight the potential of SSMs as an efficient and scalable backbone for ICIL. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/youngjuY/RoboSSM">https://github.com/youngjuY/RoboSSM</a>. </p>
<blockquote>
<p>ä¸Šä¸‹æ–‡æ¨¡ä»¿å­¦ä¹ ï¼ˆICILï¼‰ä½¿æœºå™¨äººèƒ½å¤Ÿä»ä»…åŒ…å«å°‘é‡æ¼”ç¤ºçš„æç¤ºä¸­å­¦ä¹ ä»»åŠ¡ã€‚é€šè¿‡æ¶ˆé™¤éƒ¨ç½²æ—¶å‚æ•°æ›´æ–°çš„éœ€æ±‚ï¼Œæ­¤èŒƒå¼æ”¯æŒå¯¹æ–°å‹ä»»åŠ¡çš„å°‘é‡æ ·æœ¬é€‚åº”ã€‚ç„¶è€Œï¼Œæœ€è¿‘çš„ICILæ–¹æ³•ä¾èµ–äºTransformerï¼Œè¿™å…·æœ‰è®¡ç®—ä¸Šçš„å±€é™æ€§ï¼Œå¹¶ä¸”åœ¨å¤„ç†æ¯”è®­ç»ƒæœŸé—´æ‰€è§æ›´é•¿çš„æç¤ºæ—¶å¾€å¾€ä¼šè¡¨ç°ä¸ä½³ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†RoboSSMï¼Œè¿™æ˜¯ä¸€ç§åŸºäºçŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMï¼‰çš„ä¸Šä¸‹æ–‡æ¨¡ä»¿å­¦ä¹ çš„å¯æ‰©å±•é…æ–¹ã€‚å…·ä½“æ¥è¯´ï¼ŒRoboSSMç”¨Longhornâ€”â€”ä¸€ç§æœ€æ–°SSMæŠ€æœ¯æ›¿ä»£Transformerï¼Œæä¾›çº¿æ€§æ—¶é—´æ¨ç†å’Œå¼ºå¤§çš„å¤–æ¨èƒ½åŠ›ï¼Œéå¸¸é€‚åˆé•¿ä¸Šä¸‹æ–‡æç¤ºã€‚æˆ‘ä»¬åœ¨LIBEROåŸºå‡†æµ‹è¯•ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œå¹¶å°†å…¶ä¸å¼ºå¤§çš„åŸºäºTransformerçš„ICILåŸºçº¿è¿›è¡Œäº†æ¯”è¾ƒã€‚å®éªŒè¡¨æ˜ï¼ŒRoboSSMèƒ½æœ‰æ•ˆåœ°æ‰©å±•åˆ°ä¸åŒæ•°é‡çš„ä¸Šä¸‹æ–‡æ¼”ç¤ºï¼Œåœ¨æœªè§çš„ä»»åŠ¡ä¸Šè¡¨ç°å‡ºé«˜æ€§èƒ½ï¼Œå¹¶åœ¨é•¿å‘¨æœŸåœºæ™¯ä¸­ä¿æŒç¨³å¥ã€‚è¿™äº›ç»“æœçªå‡ºäº†SSMä½œä¸ºICILé«˜æ•ˆå’Œå¯æ‰©å±•éª¨å¹²çš„æ½œåŠ›ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/youngjuY/RoboSSM%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/youngjuY/RoboSSMä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.19658v1">PDF</a> 8 pages, 11 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºçŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMï¼‰çš„åœ¨çº¿è¯­å¢ƒæ¨¡ä»¿å­¦ä¹ ï¼ˆICILï¼‰æ–¹æ³•â€”â€”RoboSSMã€‚è¯¥æ–¹æ³•ä½¿ç”¨Longhornæ›¿ä»£Transformerï¼Œå…·æœ‰çº¿æ€§æ—¶é—´æ¨ç†å’Œå¼ºå¤§çš„å¤–æ¨èƒ½åŠ›ï¼Œé€‚ç”¨äºé•¿è¯­å¢ƒæç¤ºã€‚å®éªŒè¯æ˜ï¼ŒRoboSSMåœ¨ä¸åŒæ•°é‡çš„åœ¨çº¿æ¼”ç¤ºä¸­èƒ½å¤Ÿæœ‰æ•ˆå¤–æ¨ï¼Œå¯¹æœªè§ä»»åŠ¡è¡¨ç°ä¼˜å¼‚ï¼Œå¹¶åœ¨é•¿æœŸåœºæ™¯ä¸‹ä¿æŒç¨³å¥ã€‚è¿™äº›ç»“æœè¡¨æ˜SSMä½œä¸ºICILçš„æœ‰æ•ˆå’Œå¯æ‰©å±•ä¸»å¹²å…·æœ‰æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ICILå…è®¸æœºå™¨äººä»…é€šè¿‡å°‘é‡æ¼”ç¤ºè¿›è¡Œä»»åŠ¡å­¦ä¹ ï¼Œå¹¶æ”¯æŒåœ¨éƒ¨ç½²æ—¶å¿«é€Ÿé€‚åº”æ–°ä»»åŠ¡ã€‚</li>
<li>ç°æœ‰ICILæ–¹æ³•ä¾èµ–Transformerï¼Œå­˜åœ¨è®¡ç®—é™åˆ¶ï¼Œå¤„ç†é•¿æç¤ºæ—¶è¡¨ç°ä¸ä½³ã€‚</li>
<li>RoboSSMæ˜¯ä¸€ä¸ªåŸºäºSSMçš„åœ¨çº¿è¯­å¢ƒæ¨¡ä»¿å­¦ä¹ å¯ä¼¸ç¼©æ–¹æ¡ˆï¼Œä½¿ç”¨Longhornæ›¿ä»£Transformerã€‚</li>
<li>Longhornå…·æœ‰çº¿æ€§æ—¶é—´æ¨ç†å’Œå¼ºå¤§çš„å¤–æ¨èƒ½åŠ›ï¼Œé€‚åˆå¤„ç†é•¿è¯­å¢ƒæç¤ºã€‚</li>
<li>å®éªŒè¯æ˜RoboSSMåœ¨ä¸åŒæ•°é‡çš„åœ¨çº¿æ¼”ç¤ºä¸­èƒ½æœ‰æ•ˆå¤–æ¨ï¼Œå¹¶åœ¨æœªè§ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>RoboSSMåœ¨é•¿æœŸçš„åœºæ™¯ä¸‹ä¿æŒç¨³å¥ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨å¤æ‚ç¯å¢ƒä¸­çš„å®ç”¨æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.19658">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Few-Shot/2509.19658v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Few-Shot/2509.19658v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Few-Shot/2509.19658v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Few-Shot/2509.19658v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Few-Shot/2509.19658v1/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Few-Shot/2509.19658v1/page_3_2.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Few-Shot/2509.19658v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Few-Shot/2509.19658v1/page_4_1.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Large-Language-Models-for-Pedestrian-Safety-An-Application-to-Predicting-Driver-Yielding-Behavior-at-Unsignalized-Intersections"><a href="#Large-Language-Models-for-Pedestrian-Safety-An-Application-to-Predicting-Driver-Yielding-Behavior-at-Unsignalized-Intersections" class="headerlink" title="Large Language Models for Pedestrian Safety: An Application to   Predicting Driver Yielding Behavior at Unsignalized Intersections"></a>Large Language Models for Pedestrian Safety: An Application to   Predicting Driver Yielding Behavior at Unsignalized Intersections</h2><p><strong>Authors:Yicheng Yang, Zixian Li, Jean Paul Bizimana, Niaz Zafri, Yongfeng Dong, Tianyi Li</strong></p>
<p>Pedestrian safety is a critical component of urban mobility and is strongly influenced by the interactions between pedestrian decision-making and driver yielding behavior at crosswalks. Modeling driverâ€“pedestrian interactions at intersections requires accurately capturing the complexity of these behaviors. Traditional machine learning models often struggle to capture the nuanced and context-dependent reasoning required for these multifactorial interactions, due to their reliance on fixed feature representations and limited interpretability. In contrast, large language models (LLMs) are suited for extracting patterns from heterogeneous traffic data, enabling accurate modeling of driver-pedestrian interactions. Therefore, this paper leverages multimodal LLMs through a novel prompt design that incorporates domain-specific knowledge, structured reasoning, and few-shot prompting, enabling interpretable and context-aware inference of driver yielding behavior, as an example application of modeling pedestrianâ€“driver interaction. We benchmarked state-of-the-art LLMs against traditional classifiers, finding that GPT-4o consistently achieves the highest accuracy and recall, while Deepseek-V3 excels in precision. These findings highlight the critical trade-offs between model performance and computational efficiency, offering practical guidance for deploying LLMs in real-world pedestrian safety systems. </p>
<blockquote>
<p>è¡Œäººå®‰å…¨æ˜¯åŸå¸‚æµåŠ¨æ€§çš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼Œå¹¶å—åˆ°è¡Œäººåœ¨åå­—è·¯å£åšå‡ºå†³ç­–å’Œé©¾é©¶å‘˜ç¤¼è®©è¡Œäººè¡Œä¸ºä¹‹é—´äº’åŠ¨çš„é‡è¦å½±å“ã€‚åœ¨åå­—è·¯å£å¯¹é©¾é©¶å‘˜ä¸è¡Œäººäº’åŠ¨è¿›è¡Œå»ºæ¨¡éœ€è¦å‡†ç¡®æ•æ‰è¿™äº›è¡Œä¸ºçš„å¤æ‚æ€§ã€‚ä¼ ç»Ÿçš„æœºå™¨å­¦ä¹ æ¨¡å‹ç”±äºå…¶ä¾èµ–äºå›ºå®šçš„ç‰¹å¾è¡¨ç¤ºå’Œæœ‰é™çš„è§£é‡Šæ€§ï¼Œå¾€å¾€éš¾ä»¥æ•æ‰è¿™äº›å¤šå…ƒäº’åŠ¨çš„ç»†å¾®å’Œæƒ…å¢ƒç›¸å…³çš„æ¨ç†ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€‚åˆä»å¼‚è´¨äº¤é€šæ•°æ®ä¸­æå–æ¨¡å¼ï¼Œèƒ½å¤Ÿå‡†ç¡®åœ°å¯¹é©¾é©¶å‘˜ä¸è¡Œäººäº’åŠ¨è¿›è¡Œå»ºæ¨¡ã€‚å› æ­¤ï¼Œæœ¬æ–‡é€šè¿‡ä¸€ç§æ–°çš„æç¤ºè®¾è®¡ï¼Œåˆ©ç”¨å¤šæ¨¡å¼LLMï¼Œè¯¥è®¾è®¡èåˆäº†é¢†åŸŸç‰¹å®šçŸ¥è¯†ã€ç»“æ„åŒ–æ¨ç†å’Œå°‘é‡æç¤ºï¼Œä»¥é©¾é©¶å‘˜ç¤¼è®©è¡Œä¸ºå¯è§£é‡Šçš„ã€å…·æœ‰æƒ…å¢ƒæ„è¯†çš„æ¨æ–­ä¸ºä¾‹ï¼Œå¯¹è¡Œäºº-é©¾é©¶å‘˜äº’åŠ¨è¿›è¡Œå»ºæ¨¡ã€‚æˆ‘ä»¬å°†æœ€å‰æ²¿çš„LLMä¸ä¼ ç»Ÿåˆ†ç±»å™¨è¿›è¡Œäº†å¯¹æ¯”è¯„ä¼°ï¼Œå‘ç°GPT-4oåœ¨å‡†ç¡®ç‡å’Œå¬å›ç‡æ–¹é¢è¡¨ç°æœ€ä¼˜ç§€ï¼Œè€ŒDeepseek-V3åœ¨ç²¾ç¡®åº¦ä¸Šè¡¨ç°çªå‡ºã€‚è¿™äº›å‘ç°çªå‡ºäº†æ¨¡å‹æ€§èƒ½å’Œè®¡ç®—æ•ˆç‡ä¹‹é—´çš„å…³é”®æƒè¡¡ï¼Œä¸ºåœ¨ç°å®ä¸–ç•Œè¡Œäººå®‰å…¨ç³»ç»Ÿä¸­éƒ¨ç½²LLMæä¾›äº†å®é™…æŒ‡å¯¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.19657v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†åŸå¸‚ç§»åŠ¨æ€§ä¸­è¡Œäººå®‰å…¨çš„é‡è¦æ€§ï¼ŒæŒ‡å‡ºè¡Œäººå†³ç­–ä¸å¸æœºè®©è¡Œè¡Œä¸ºåœ¨äº¤å‰å£å¤„çš„äº’åŠ¨å¯¹è¡Œäººå®‰å…¨äº§ç”Ÿæ·±è¿œå½±å“ã€‚ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ–¹æ³•åœ¨æ•æ‰è¿™äº›å¤šå…ƒäº’åŠ¨çš„ç»†å¾®å·®åˆ«å’Œä¸Šä¸‹æ–‡ä¾èµ–æ€§æ¨ç†æ–¹é¢å­˜åœ¨å›°éš¾ã€‚ç›¸åï¼Œæœ¬æ–‡é€šè¿‡å¤šæ¨¡å¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯¹å¸æœºä¸è¡Œäººäº’åŠ¨è¿›è¡Œå»ºæ¨¡ï¼Œåˆ©ç”¨æ–°é¢–æç¤ºè®¾è®¡èåˆé¢†åŸŸç‰¹å®šçŸ¥è¯†ã€ç»“æ„åŒ–æ¨ç†å’Œå°‘é‡æç¤ºï¼Œå®ç°å¯è§£é‡Šå’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„å¸æœºè®©è¡Œè¡Œä¸ºæ¨æ–­ã€‚å¯¹æ¯”æœ€æ–°LLMsä¸ä¼ ç»Ÿåˆ†ç±»å™¨ï¼Œå‘ç°GPT-4oåœ¨å‡†ç¡®ç‡å’Œå¬å›ç‡ä¸Šè¡¨ç°æœ€ä½³ï¼Œè€ŒDeepseek-V3åœ¨ç²¾ç¡®åº¦ä¸Šè¡¨ç°çªå‡ºã€‚è¿™ä¸ºåœ¨å®é™…è¡Œäººå®‰å…¨ç³»ç»Ÿä¸­éƒ¨ç½²LLMsæä¾›äº†å®é™…æŒ‡å¯¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¡Œäººå®‰å…¨æ˜¯åŸå¸‚ç§»åŠ¨æ€§çš„å…³é”®éƒ¨åˆ†ï¼Œå—è¡Œäººå†³ç­–å’Œå¸æœºè®©è¡Œè¡Œä¸ºäº¤äº’çš„å½±å“ã€‚</li>
<li>ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ–¹æ³•åœ¨æ•æ‰å¸æœºä¸è¡Œäººäº’åŠ¨çš„å¤æ‚æ€§å’Œä¸Šä¸‹æ–‡ä¾èµ–æ€§æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€‚åˆä»å¼‚è´¨äº¤é€šæ•°æ®ä¸­æå–æ¨¡å¼ï¼Œèƒ½å‡†ç¡®å»ºæ¨¡å¸æœºä¸è¡Œäººçš„äº’åŠ¨ã€‚</li>
<li>é€šè¿‡æ–°é¢–æç¤ºè®¾è®¡ï¼ŒLLMså¯å®ç°å¯è§£é‡Šå’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„å¸æœºè®©è¡Œè¡Œä¸ºæ¨æ–­ã€‚</li>
<li>GPT-4oåœ¨æ¨¡å‹æ€§èƒ½å’Œå¬å›ç‡æ–¹é¢è¡¨ç°æœ€ä½³ï¼Œè€ŒDeepseek-V3åœ¨ç²¾ç¡®åº¦ä¸Šè¡¨ç°ä¼˜ç§€ã€‚</li>
<li>æ¨¡å‹æ€§èƒ½å’Œè®¡ç®—æ•ˆç‡ä¹‹é—´å­˜åœ¨æƒè¡¡ï¼Œè¿™ä¸ºåœ¨å®é™…è¡Œäººå®‰å…¨ç³»ç»Ÿä¸­éƒ¨ç½²LLMsæä¾›äº†æŒ‘æˆ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.19657">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Few-Shot/2509.19657v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Few-Shot/2509.19657v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Few-Shot/2509.19657v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Few-Shot/2509.19657v1/page_5_1.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Semantic-Aware-Fuzzing-An-Empirical-Framework-for-LLM-Guided-Reasoning-Driven-Input-Mutation"><a href="#Semantic-Aware-Fuzzing-An-Empirical-Framework-for-LLM-Guided-Reasoning-Driven-Input-Mutation" class="headerlink" title="Semantic-Aware Fuzzing: An Empirical Framework for LLM-Guided,   Reasoning-Driven Input Mutation"></a>Semantic-Aware Fuzzing: An Empirical Framework for LLM-Guided,   Reasoning-Driven Input Mutation</h2><p><strong>Authors:Mengdi Lu, Steven Ding, Furkan Alaca, Philippe Charland</strong></p>
<p>Security vulnerabilities in Internet-of-Things devices, mobile platforms, and autonomous systems remain critical. Traditional mutation-based fuzzers â€“ while effectively explore code paths â€“ primarily perform byte- or bit-level edits without semantic reasoning. Coverage-guided tools such as AFL++ use dictionaries, grammars, and splicing heuristics to impose shallow structural constraints, leaving deeper protocol logic, inter-field dependencies, and domain-specific semantics unaddressed. Conversely, reasoning-capable large language models (LLMs) can leverage pretraining knowledge to understand input formats, respect complex constraints, and propose targeted mutations, much like an experienced reverse engineer or testing expert. However, lacking ground truth for â€œcorrectâ€ mutation reasoning makes supervised fine-tuning impractical, motivating explorations of off-the-shelf LLMs via prompt-based few-shot learning. To bridge this gap, we present an open-source microservices framework that integrates reasoning LLMs with AFL++ on Googleâ€™s FuzzBench, tackling asynchronous execution and divergent hardware demands (GPU- vs. CPU-intensive) of LLMs and fuzzers. We evaluate four research questions: (R1) How can reasoning LLMs be integrated into the fuzzing mutation loop? (R2) Do few-shot prompts yield higher-quality mutations than zero-shot? (R3) Can prompt engineering with off-the-shelf models improve fuzzing directly? and (R4) Which open-source reasoning LLMs perform best under prompt-only conditions? Experiments with Llama3.3, Deepseek-r1-Distill-Llama-70B, QwQ-32B, and Gemma3 highlight Deepseek as the most promising. Mutation effectiveness depends more on prompt complexity and model choice than shot count. Response latency and throughput bottlenecks remain key obstacles, offering directions for future work. </p>
<blockquote>
<p>ç‰©è”ç½‘è®¾å¤‡ã€ç§»åŠ¨å¹³å°å’Œè‡ªä¸»ç³»ç»Ÿä¸­çš„å®‰å…¨æ¼æ´ä»ç„¶ååˆ†ä¸¥é‡ã€‚ä¼ ç»Ÿçš„åŸºäºå˜å¼‚çš„æ¨¡ç³Šæµ‹è¯•å™¨è™½ç„¶èƒ½å¤Ÿæœ‰æ•ˆåœ°æ¢ç´¢ä»£ç è·¯å¾„ï¼Œä½†ä¸»è¦è¿›è¡Œå­—èŠ‚æˆ–ä½çº§åˆ«çš„ç¼–è¾‘ï¼Œè€Œæ²¡æœ‰è¯­ä¹‰æ¨ç†ã€‚åƒAFL++è¿™æ ·çš„è¦†ç›–å¯¼å‘å·¥å…·ä½¿ç”¨å­—å…¸ã€è¯­æ³•å’Œæ‹¼æ¥å¯å‘å¼æ–¹æ³•æ¥æ–½åŠ æµ…ç»“æ„çº¦æŸï¼Œä½†å¿½ç•¥äº†æ›´æ·±å±‚æ¬¡çš„åè®®é€»è¾‘ã€å­—æ®µé—´ä¾èµ–å…³ç³»å’Œç‰¹å®šé¢†åŸŸçš„è¯­ä¹‰ã€‚ç›¸åï¼Œå…·å¤‡æ¨ç†èƒ½åŠ›çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯ä»¥åˆ©ç”¨é¢„è®­ç»ƒçŸ¥è¯†æ¥ç†è§£è¾“å…¥æ ¼å¼ã€éµå®ˆå¤æ‚çº¦æŸå¹¶æå‡ºæœ‰é’ˆå¯¹æ€§çš„å˜å¼‚ï¼Œå°±åƒç»éªŒä¸°å¯Œçš„é€†å‘å·¥ç¨‹å¸ˆæˆ–æµ‹è¯•ä¸“å®¶ä¸€æ ·ã€‚ç„¶è€Œï¼Œç¼ºä¹â€œæ­£ç¡®â€å˜å¼‚æ¨ç†çš„åœ°é¢çœŸå®æ•°æ®ä½¿å¾—ç›‘ç£å¾®è°ƒå˜å¾—ä¸åˆ‡å®é™…ï¼Œè¿™æ¿€å‘äº†é€šè¿‡åŸºäºæç¤ºçš„å°‘é‡æ ·æœ¬å­¦ä¹ æ¥ä½¿ç”¨ç°æˆçš„LLMçš„æ¢ç´¢ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†ä¸€ä¸ªå¼€æºçš„å¾®æœåŠ¡æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†æ¨ç†LLMä¸Googleçš„FuzzBenchä¸Šçš„AFL++é›†æˆåœ¨ä¸€èµ·ï¼Œè§£å†³äº†LLMå’Œæ¨¡ç³Šæµ‹è¯•å™¨çš„å¼‚æ­¥æ‰§è¡Œå’Œä¸åŒç¡¬ä»¶éœ€æ±‚ï¼ˆGPUå¯†é›†å‹ä¸CPUå¯†é›†å‹ï¼‰çš„é—®é¢˜ã€‚æˆ‘ä»¬è¯„ä¼°äº†å››ä¸ªç ”ç©¶é—®é¢˜ï¼šï¼ˆR1ï¼‰å¦‚ä½•å°†æ¨ç†LLMé›†æˆåˆ°æ¨¡ç³Šæµ‹è¯•å˜å¼‚å¾ªç¯ä¸­ï¼Ÿï¼ˆR2ï¼‰ä¸é›¶æ ·æœ¬ç›¸æ¯”ï¼Œå°‘é‡æ ·æœ¬æç¤ºæ˜¯å¦ä¼šäº§ç”Ÿæ›´é«˜è´¨é‡çš„å˜å¼‚ï¼Ÿï¼ˆR3ï¼‰ä½¿ç”¨ç°æˆçš„æ¨¡å‹è¿›è¡Œæç¤ºå·¥ç¨‹èƒ½å¦ç›´æ¥æé«˜æ¨¡ç³Šæµ‹è¯•çš„æ•ˆç‡ï¼Ÿä»¥åŠï¼ˆR4ï¼‰åœ¨åªæœ‰æç¤ºçš„æ¡ä»¶ä¸‹ï¼Œå“ªä¸ªå¼€æºæ¨ç†LLMè¡¨ç°æœ€å¥½ï¼Ÿä½¿ç”¨Llama3.3ã€Deepseek-r1-Distill-Llama-70Bã€QwQ-32Bå’ŒGemma3çš„å®éªŒçªå‡ºäº†Deepseekçš„æœ€æœ‰å‰é€”ã€‚å˜å¼‚çš„æœ‰æ•ˆæ€§æ›´å¤šåœ°å–å†³äºæç¤ºçš„å¤æ‚æ€§å’Œæ¨¡å‹çš„é€‰æ‹©ï¼Œè€Œä¸æ˜¯å°„å‡»æ¬¡æ•°ã€‚å“åº”å»¶è¿Ÿå’Œååé‡ç“¶é¢ˆä»ç„¶æ˜¯å…³é”®éšœç¢ï¼Œä¸ºæœªæ¥çš„å·¥ä½œæä¾›äº†æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.19533v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<pre><code>äº’è”ç½‘ç‰©è”ç½‘è®¾å¤‡ã€ç§»åŠ¨å¹³å°å’Œè‡ªä¸»ç³»ç»Ÿçš„å®‰å…¨æ¼æ´ä¾ç„¶ä¸¥é‡ã€‚ä¼ ç»ŸåŸºäºå˜å¼‚çš„æ¨¡ç³Šæµ‹è¯•æŠ€æœ¯è™½ç„¶èƒ½æœ‰æ•ˆæ¢ç´¢ä»£ç è·¯å¾„ï¼Œä½†ä¸»è¦è¿›è¡Œå­—èŠ‚æˆ–ä½çº§åˆ«çš„ç¼–è¾‘ï¼Œç¼ºä¹è¯­ä¹‰æ¨ç†ã€‚è¦†ç›–æŒ‡å¯¼å·¥å…·å¦‚AFL++ä½¿ç”¨å­—å…¸ã€è¯­æ³•å’Œæ‹¼æ¥å¯å‘å¼æ–¹æ³•æ–½åŠ æµ…å±‚æ¬¡çš„ç»“æ„çº¦æŸï¼Œä½†æ›´æ·±å±‚æ¬¡çš„åè®®é€»è¾‘ã€å­—æ®µé—´ä¾èµ–å…³ç³»å’Œé¢†åŸŸç‰¹å®šè¯­ä¹‰æœªå¾—åˆ°è§£å†³ã€‚ç›¸åï¼Œå…·å¤‡æ¨ç†èƒ½åŠ›çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èƒ½åˆ©ç”¨é¢„è®­ç»ƒçŸ¥è¯†ç†è§£è¾“å…¥æ ¼å¼ã€éµå®ˆå¤æ‚çº¦æŸå¹¶æå‡ºæœ‰é’ˆå¯¹æ€§çš„å˜å¼‚ï¼Œå¦‚åŒç»éªŒä¸°å¯Œçš„é€†å‘å·¥ç¨‹å¸ˆæˆ–æµ‹è¯•ä¸“å®¶ã€‚æœ¬ç ”ç©¶å‘ˆç°äº†ä¸€ä¸ªå¼€æºçš„å¾®æœåŠ¡æ¡†æ¶ï¼Œå°†æ¨ç†LLMä¸AFL++é›†æˆåœ¨Googleçš„FuzzBenchä¸Šï¼Œè§£å†³äº†LLMå’Œæ¨¡ç³Šæµ‹è¯•å™¨çš„å¼‚æ­¥æ‰§è¡Œå’Œä¸åŒç¡¬ä»¶éœ€æ±‚ï¼ˆGPUä¸CPUå¯†é›†å‹ï¼‰ã€‚æœ¬ç ”ç©¶è¯„ä¼°äº†å››ä¸ªç ”ç©¶é—®é¢˜å¹¶è¿›è¡Œäº†å®éªŒéªŒè¯ã€‚
</code></pre>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äº’è”ç½‘ç‰©è”ç½‘è®¾å¤‡ã€ç§»åŠ¨å¹³å°å’Œè‡ªä¸»ç³»ç»Ÿçš„å®‰å…¨æ¼æ´ä»æ˜¯å…³é”®æŒ‘æˆ˜ã€‚</li>
<li>ä¼ ç»Ÿæ¨¡ç³Šæµ‹è¯•æŠ€æœ¯ä¸»è¦è¿›è¡Œå­—èŠ‚æˆ–ä½çº§åˆ«çš„ç¼–è¾‘ï¼Œç¼ºä¹è¯­ä¹‰æ¨ç†ã€‚</li>
<li>è¦†ç›–æŒ‡å¯¼å·¥å…·å¦‚AFL++å­˜åœ¨æ·±å±‚æ¬¡åè®®é€»è¾‘ã€å­—æ®µé—´ä¾èµ–å…³ç³»å’Œé¢†åŸŸç‰¹å®šè¯­ä¹‰çš„é—®é¢˜ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹å…·å¤‡ç†è§£è¾“å…¥æ ¼å¼ã€éµå®ˆå¤æ‚çº¦æŸå¹¶æå‡ºé’ˆå¯¹æ€§çš„å˜å¼‚èƒ½åŠ›ã€‚</li>
<li>æ•´åˆæ¨ç†LLMå’Œæ¨¡ç³Šæµ‹è¯•å™¨çš„å¼€æºæ¡†æ¶èƒ½è§£å†³ä¸¤è€…çš„å¼‚æ­¥æ‰§è¡Œå’Œä¸åŒç¡¬ä»¶éœ€æ±‚é—®é¢˜ã€‚</li>
<li>å®éªŒè¯„ä¼°äº†ä¸åŒæ¨¡å‹åœ¨æ¨¡ç³Šæµ‹è¯•ä¸­çš„è¡¨ç°ï¼Œå¹¶å‘ç°Deepseekæ¨¡å‹åœ¨å°‘æ ·æœ¬æç¤ºæ¡ä»¶ä¸‹è¡¨ç°æœ€ä¸ºå‡ºè‰²ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.19533">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Few-Shot/2509.19533v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Few-Shot/2509.19533v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Few-Shot/2509.19533v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Few-Shot/2509.19533v1/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Few-Shot/2509.19533v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Few-Shot/2509.19533v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Self-evolved-Imitation-Learning-in-Simulated-World"><a href="#Self-evolved-Imitation-Learning-in-Simulated-World" class="headerlink" title="Self-evolved Imitation Learning in Simulated World"></a>Self-evolved Imitation Learning in Simulated World</h2><p><strong>Authors:Yifan Ye, Jun Cen, Jing Chen, Zhihe Lu</strong></p>
<p>Imitation learning has been a trend recently, yet training a generalist agent across multiple tasks still requires large-scale expert demonstrations, which are costly and labor-intensive to collect. To address the challenge of limited supervision, we propose Self-Evolved Imitation Learning (SEIL), a framework that progressively improves a few-shot model through simulator interactions. The model first attempts tasksin the simulator, from which successful trajectories are collected as new demonstrations for iterative refinement. To enhance the diversity of these demonstrations, SEIL employs dual-level augmentation: (i) Model-level, using an Exponential Moving Average (EMA) model to collaborate with the primary model, and (ii) Environment-level, introducing slight variations in initial object positions. We further introduce a lightweight selector that filters complementary and informative trajectories from the generated pool to ensure demonstration quality. These curated samples enable the model to achieve competitive performance with far fewer training examples. Extensive experiments on the LIBERO benchmark show that SEIL achieves a new state-of-the-art performance in few-shot imitation learning scenarios. Code is available at <a target="_blank" rel="noopener" href="https://github.com/Jasper-aaa/SEIL.git">https://github.com/Jasper-aaa/SEIL.git</a>. </p>
<blockquote>
<p>æ¨¡ä»¿å­¦ä¹ è¿‘æœŸæˆä¸ºä¸€ç§è¶‹åŠ¿ï¼Œç„¶è€Œï¼Œåœ¨å¤šä¸ªä»»åŠ¡ä¹‹é—´è®­ç»ƒä¸€ä¸ªé€šç”¨æ™ºèƒ½ä½“ä»ç„¶éœ€è¦å¤§è§„æ¨¡çš„ä¸“ä¸šç¤ºèŒƒï¼Œè¿™äº›ç¤ºèŒƒçš„æ”¶é›†æˆæœ¬é«˜æ˜‚ä¸”åŠ³åŠ¨å¯†é›†ã€‚ä¸ºäº†è§£å†³ç›‘ç£æœ‰é™çš„æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†è‡ªè¿›åŒ–æ¨¡ä»¿å­¦ä¹ ï¼ˆSEILï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡æ¨¡æ‹Ÿå™¨äº¤äº’é€æ­¥æ”¹è¿›äº†å°æ ·æœ¬æ¨¡å‹ã€‚è¯¥æ¨¡å‹é¦–å…ˆåœ¨æ¨¡æ‹Ÿå™¨ä¸­å°è¯•ä»»åŠ¡ï¼Œæ”¶é›†æˆåŠŸçš„è½¨è¿¹ä½œä¸ºæ–°çš„ç¤ºèŒƒè¿›è¡Œè¿­ä»£ä¼˜åŒ–ã€‚ä¸ºäº†æé«˜è¿™äº›ç¤ºèŒƒçš„å¤šæ ·æ€§ï¼ŒSEILé‡‡ç”¨äº†åŒé‡å±‚æ¬¡çš„å¢å¼ºæ–¹å¼ï¼šï¼ˆiï¼‰æ¨¡å‹å±‚é¢ï¼Œä½¿ç”¨æŒ‡æ•°ç§»åŠ¨å¹³å‡ï¼ˆEMAï¼‰æ¨¡å‹ä¸ä¸»æ¨¡å‹è¿›è¡Œåä½œï¼›ï¼ˆiiï¼‰ç¯å¢ƒå±‚é¢ï¼Œåœ¨åˆå§‹ç‰©ä½“ä½ç½®å¼•å…¥è½»å¾®å˜åŒ–ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥å¼•å…¥äº†ä¸€ä¸ªè½»é‡çº§çš„é€‰æ‹©å™¨ï¼Œä»ç”Ÿæˆçš„æ± ä¸­ç­›é€‰å‡ºäº’è¡¥ä¸”ä¿¡æ¯ä¸°å¯Œçš„è½¨è¿¹ï¼Œä»¥ç¡®ä¿ç¤ºèŒƒçš„è´¨é‡ã€‚è¿™äº›ç²¾é€‰æ ·æœ¬ä½¿æ¨¡å‹åœ¨æå°‘çš„è®­ç»ƒæ ·æœ¬ä¸‹å®ç°äº†å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ã€‚åœ¨LIBEROåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒSEILåœ¨å°‘æ ·æœ¬æ¨¡ä»¿å­¦ä¹ åœºæ™¯ä¸­è¾¾åˆ°äº†æœ€æ–°çš„æœ€ä½³æ€§èƒ½ã€‚ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/Jasper-aaa/SEIL.git%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/Jasper-aaa/SEIL.gitè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.19460v1">PDF</a> </p>
<p><strong>Summary</strong><br>     è‡ªæ¼”åŒ–æ¨¡ä»¿å­¦ä¹ æ¡†æ¶ï¼ˆSEILï¼‰é€šè¿‡æ¨¡æ‹Ÿå™¨äº¤äº’é€æ­¥ä¼˜åŒ–å°‘é‡æ¨¡å‹ï¼Œè§£å†³äº†æœ‰é™ç›‘ç£çš„é—®é¢˜ã€‚æ¨¡å‹é¦–å…ˆåœ¨æ¨¡æ‹Ÿå™¨å°è¯•ä»»åŠ¡ï¼Œæ”¶é›†æˆåŠŸçš„è½¨è¿¹ä½œä¸ºæ–°çš„æ¼”ç¤ºè¿›è¡Œè¿­ä»£ä¼˜åŒ–ã€‚ä¸ºäº†å¢å¼ºæ¼”ç¤ºçš„å¤šæ ·æ€§ï¼ŒSEILé‡‡ç”¨äº†åŒé‡å±‚æ¬¡çš„å¢å¼ºæ–¹æ³•ï¼šæ¨¡å‹çº§åˆ«çš„æŒ‡æ•°ç§»åŠ¨å¹³å‡ï¼ˆEMAï¼‰æ¨¡å‹ä¸ä¸»æ¨¡å‹çš„åä½œå’Œç¯å¢ƒçº§åˆ«çš„åˆå§‹å¯¹è±¡ä½ç½®è½»å¾®å˜åŒ–ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªè½»é‡çº§çš„é€‰æ‹©å™¨ï¼Œä»ç”Ÿæˆçš„æ± ä¸­ç­›é€‰å‡ºäº’è¡¥å’Œæœ‰ç”¨çš„è½¨è¿¹ï¼Œä»¥ç¡®ä¿æ¼”ç¤ºçš„è´¨é‡ã€‚åœ¨LIBEROåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒSEILåœ¨å°‘é‡æ¨¡ä»¿å­¦ä¹ åœºæ™¯ä¸­è¾¾åˆ°äº†æ–°çš„æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SEILæ˜¯ä¸€ä¸ªé€šè¿‡æ¨¡æ‹Ÿå™¨äº¤äº’æ”¹å–„å°‘é‡æ¨¡å‹çš„æ¡†æ¶ï¼Œè§£å†³äº†æœ‰é™ç›‘ç£çš„æŒ‘æˆ˜ã€‚</li>
<li>æ¨¡å‹åœ¨æ¨¡æ‹Ÿå™¨ä¸­å°è¯•ä»»åŠ¡ï¼Œå¹¶æ”¶é›†æˆåŠŸçš„è½¨è¿¹ä½œä¸ºæ–°çš„æ¼”ç¤ºè¿›è¡Œè¿­ä»£ä¼˜åŒ–ã€‚</li>
<li>SEILé‡‡ç”¨åŒé‡å±‚æ¬¡çš„å¢å¼ºæ–¹æ³•æ¥å¢å¼ºæ¼”ç¤ºçš„å¤šæ ·æ€§ã€‚</li>
<li>åœ¨æ¨¡å‹çº§åˆ«ï¼Œä½¿ç”¨æŒ‡æ•°ç§»åŠ¨å¹³å‡ï¼ˆEMAï¼‰æ¨¡å‹ä¸ä¸»æ¨¡å‹åä½œã€‚</li>
<li>åœ¨ç¯å¢ƒçº§åˆ«ï¼Œå¼•å…¥åˆå§‹å¯¹è±¡ä½ç½®çš„è½»å¾®å˜åŒ–ã€‚</li>
<li>å¼•å…¥è½»é‡çº§é€‰æ‹©å™¨æ¥ç¡®ä¿æ¼”ç¤ºçš„è´¨é‡ï¼Œä»ç”Ÿæˆçš„æ± ä¸­ç­›é€‰å‡ºæœ‰ç”¨çš„è½¨è¿¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.19460">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Few-Shot/2509.19460v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Few-Shot/2509.19460v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Few-Shot/2509.19460v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Few-Shot/2509.19460v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Few-Shot/2509.19460v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Few-Shot/2509.19460v1/page_5_1.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Vision-Free-Retrieval-Rethinking-Multimodal-Search-with-Textual-Scene-Descriptions"><a href="#Vision-Free-Retrieval-Rethinking-Multimodal-Search-with-Textual-Scene-Descriptions" class="headerlink" title="Vision-Free Retrieval: Rethinking Multimodal Search with Textual Scene   Descriptions"></a>Vision-Free Retrieval: Rethinking Multimodal Search with Textual Scene   Descriptions</h2><p><strong>Authors:Ioanna Ntinou, Alexandros Xenos, Yassine Ouali, Adrian Bulat, Georgios Tzimiropoulos</strong></p>
<p>Contrastively-trained Vision-Language Models (VLMs), such as CLIP, have become the standard approach for learning discriminative vision-language representations. However, these models often exhibit shallow language understanding, manifesting bag-of-words behaviour. These limitations are reinforced by their dual-encoder design, which induces a modality gap. Additionally, the reliance on vast web-collected data corpora for training makes the process computationally expensive and introduces significant privacy concerns. To address these limitations, in this work, we challenge the necessity of vision encoders for retrieval tasks by introducing a vision-free, single-encoder retrieval pipeline. Departing from the traditional text-to-image retrieval paradigm, we migrate to a text-to-text paradigm with the assistance of VLLM-generated structured image descriptions. We demonstrate that this paradigm shift has significant advantages, including a substantial reduction of the modality gap, improved compositionality, and better performance on short and long caption queries, all attainable with only a few hours of calibration on two GPUs. Additionally, substituting raw images with textual descriptions introduces a more privacy-friendly alternative for retrieval. To further assess generalisation and address some of the shortcomings of prior compositionality benchmarks, we release two benchmarks derived from Flickr30k and COCO, containing diverse compositional queries made of short captions, which we coin subFlickr and subCOCO. Our vision-free retriever matches and often surpasses traditional multimodal models. Importantly, our approach achieves state-of-the-art zero-shot performance on multiple retrieval and compositionality benchmarks, with models as small as 0.3B parameters. Code is available at: <a target="_blank" rel="noopener" href="https://github.com/IoannaNti/LexiCLIP">https://github.com/IoannaNti/LexiCLIP</a> </p>
<blockquote>
<p>å¯¹æ¯”è®­ç»ƒVision-Languageæ¨¡å‹ï¼ˆVLMsï¼‰ï¼Œå¦‚CLIPï¼Œå·²ç»æˆä¸ºå­¦ä¹ åŒºåˆ†æ€§çš„è§†è§‰è¯­è¨€è¡¨ç¤ºçš„æ ‡å‡†æ–¹æ³•ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹å¾€å¾€è¡¨ç°å‡ºæµ…å±‚çš„è¯­è¨€ç†è§£ï¼Œè¡¨ç°å‡ºè¯è¢‹è¡Œä¸ºã€‚è¿™äº›å±€é™æ€§ç”±å®ƒä»¬çš„åŒç¼–ç å™¨è®¾è®¡è€ŒåŠ å‰§ï¼Œè¿™å¯¼è‡´äº†æ¨¡æ€é—´éš™ã€‚æ­¤å¤–ï¼Œè¿™äº›æ¨¡å‹ä¾èµ–äºå¤§é‡ç½‘ç»œæ”¶é›†çš„æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œè¿™ä½¿å¾—è¿‡ç¨‹è®¡ç®—æˆæœ¬é«˜æ˜‚å¹¶å¼•å‘äº†ä¸¥é‡çš„éšç§é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œåœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å¼•å…¥æ— è§†è§‰çš„å•ä¸€ç¼–ç å™¨æ£€ç´¢ç®¡é“ï¼Œè´¨ç–‘äº†è§†è§‰ç¼–ç å™¨å¯¹äºæ£€ç´¢ä»»åŠ¡çš„å¿…è¦æ€§ã€‚æˆ‘ä»¬æ‘’å¼ƒäº†ä¼ ç»Ÿçš„æ–‡æœ¬åˆ°å›¾åƒæ£€ç´¢èŒƒå¼ï¼Œå€ŸåŠ©VLLMç”Ÿæˆçš„ç»“æ„åŒ–å›¾åƒæè¿°ï¼Œè¿ç§»åˆ°æ–‡æœ¬åˆ°æ–‡æœ¬çš„èŒƒå¼ã€‚æˆ‘ä»¬è¯æ˜è¿™ç§èŒƒå¼è½¬å˜å…·æœ‰æ˜¾è‘—çš„ä¼˜åŠ¿ï¼ŒåŒ…æ‹¬å¤§å¹…åº¦å‡å°‘æ¨¡æ€é—´éš™ã€æé«˜ç»„åˆæ€§å’Œåœ¨çŸ­é•¿æ ‡é¢˜æŸ¥è¯¢ä¸Šçš„æ›´å¥½æ€§èƒ½ï¼Œæ‰€æœ‰è¿™äº›éƒ½å¯åœ¨ä¸¤å°GPUä¸Šä»…å‡ ä¸ªå°æ—¶çš„æ ¡å‡†å†…å®ç°ã€‚æ­¤å¤–ï¼Œç”¨æ–‡æœ¬æè¿°æ›¿ä»£åŸå§‹å›¾åƒä¸ºæ£€ç´¢æä¾›äº†æ›´éšç§å‹å¥½çš„æ›¿ä»£æ–¹æ¡ˆã€‚ä¸ºäº†è¿›ä¸€æ­¥è¯„ä¼°é€šç”¨æ€§å’Œè§£å†³å…ˆå‰ç»„åˆåŸºå‡†çš„ä¸€äº›ç¼ºç‚¹ï¼Œæˆ‘ä»¬ä»Flickr30kå’ŒCOCOæ´¾ç”Ÿå‡ºä¸¤ä¸ªåŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«ç”±çŸ­æ ‡é¢˜ç»„æˆçš„å¤šæ ·åŒ–ç»„åˆæŸ¥è¯¢ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºsubFlickrå’ŒsubCOCOã€‚æˆ‘ä»¬çš„æ— è§†è§‰æ£€ç´¢å™¨ä¸è®¸å¤šä¼ ç»Ÿçš„å¤šåª’ä½“æ¨¡å‹ç›¸åŒ¹é…ï¼Œç”šè‡³ç»å¸¸è¶…è¶Šå®ƒä»¬ã€‚æœ€é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªæ£€ç´¢å’Œç»„åˆåŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„é›¶æ ·æœ¬æ€§èƒ½ï¼Œæ¨¡å‹å¤§å°å°è‡³0.3Bå‚æ•°ã€‚ä»£ç å¯åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/IoannaNti/LexiCLIP">https://github.com/IoannaNti/LexiCLIP</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.19203v1">PDF</a> Accepted at EMNLP 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¯¹æ¯”è®­ç»ƒçš„è·¨æ¨¡æ€é¢„è®­ç»ƒæ¨¡å‹ï¼ˆå¦‚CLIPï¼‰åœ¨å¤„ç†è§†è§‰è¯­è¨€ä»»åŠ¡æ—¶çš„å±€é™æ€§ï¼ŒåŒ…æ‹¬è¯­è¨€ç†è§£æµ…ã€æ¨¡æ€å·®è·å¤§ã€ä¾èµ–å¤§è§„æ¨¡ç½‘ç»œæ•°æ®è®­ç»ƒå¸¦æ¥çš„è®¡ç®—æˆæœ¬é«˜å’Œéšç§æ‹…å¿§ç­‰é—®é¢˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªæ— è§†è§‰ç¼–ç å™¨çš„å•ç¼–ç å™¨æ£€ç´¢ç®¡é“ï¼Œé€šè¿‡æ–‡æœ¬æè¿°ä»£æ›¿åŸå§‹å›¾åƒè¿›è¡Œæ£€ç´¢ï¼Œå®ç°äº†æ¨¡æ€å·®è·çš„æ˜¾è‘—å‡å°‘ã€æ›´å¥½çš„ç»„åˆæ€§å’Œåœ¨çŸ­é•¿æŸ¥è¯¢ä¸Šçš„ä¼˜è¶Šæ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¿˜å‘å¸ƒäº†ä¸¤ä¸ªåŸºäºFlickr30kå’ŒCOCOçš„åŸºå‡†æµ‹è¯•é›†subFlickrå’ŒsubCOCOï¼Œä»¥è¯„ä¼°æ¨¡å‹çš„é€šç”¨æ€§å’Œç»„åˆæ€§ã€‚è¯¥æ–¹æ³•å®ç°äº†å¤šä¸ªæ£€ç´¢å’Œç»„åˆåŸºå‡†æµ‹è¯•é›†çš„é›¶æ ·æœ¬æ€§èƒ½çš„æœ€ä½³æ°´å¹³ï¼Œä¸”æ¨¡å‹å‚æ•°ä»…ä¸º0.3Bã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¯¹æ¯”è®­ç»ƒçš„è·¨æ¨¡æ€é¢„è®­ç»ƒæ¨¡å‹ï¼ˆCLIPç­‰ï¼‰å­˜åœ¨è¯­è¨€ç†è§£æµ…ã€æ¨¡æ€å·®è·å¤§çš„å±€é™æ€§ã€‚</li>
<li>å¼•å…¥æ— è§†è§‰ç¼–ç å™¨çš„å•ç¼–ç å™¨æ£€ç´¢ç®¡é“ï¼Œé€šè¿‡æ–‡æœ¬æè¿°å®ç°å›¾åƒæ£€ç´¢ï¼Œå‡å°‘æ¨¡æ€å·®è·å¹¶æé«˜æ€§èƒ½ã€‚</li>
<li>å‘å¸ƒçš„subFlickrå’ŒsubCOCOåŸºå‡†æµ‹è¯•é›†ç”¨äºè¯„ä¼°æ¨¡å‹çš„é€šç”¨æ€§å’Œç»„åˆæ€§ã€‚</li>
<li>è¯¥æ–¹æ³•å®ç°äº†å¤šä¸ªæ£€ç´¢å’Œç»„åˆåŸºå‡†æµ‹è¯•é›†çš„é›¶æ ·æœ¬æ€§èƒ½æœ€ä½³æ°´å¹³ï¼Œä¸”æ¨¡å‹å‚æ•°å°ã€‚</li>
<li>æ›¿ä»£åŸå§‹å›¾åƒä½¿ç”¨æ–‡æœ¬æè¿°æœ‰åŠ©äºå¢åŠ æ£€ç´¢çš„éšç§ä¿æŠ¤ã€‚</li>
<li>æ–¹æ³•ä»…ä½¿ç”¨å°‘é‡æ—¶é—´è¿›è¡Œæ ¡å‡†ï¼ˆä»…å‡ å°æ—¶ï¼‰å³å¯è·å¾—è‰¯å¥½æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.19203">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Few-Shot/2509.19203v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Few-Shot/2509.19203v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Few-Shot/2509.19203v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Benchmarking-Vision-Language-and-Multimodal-Large-Language-Models-in-Zero-shot-and-Few-shot-Scenarios-A-study-on-Christian-Iconography"><a href="#Benchmarking-Vision-Language-and-Multimodal-Large-Language-Models-in-Zero-shot-and-Few-shot-Scenarios-A-study-on-Christian-Iconography" class="headerlink" title="Benchmarking Vision-Language and Multimodal Large Language Models in   Zero-shot and Few-shot Scenarios: A study on Christian Iconography"></a>Benchmarking Vision-Language and Multimodal Large Language Models in   Zero-shot and Few-shot Scenarios: A study on Christian Iconography</h2><p><strong>Authors:Gianmarco Spinaci, Lukas Klic, Giovanni Colavizza</strong></p>
<p>This study evaluates the capabilities of Multimodal Large Language Models (LLMs) and Vision Language Models (VLMs) in the task of single-label classification of Christian Iconography. The goal was to assess whether general-purpose VLMs (CLIP and SigLIP) and LLMs, such as GPT-4o and Gemini 2.5, can interpret the Iconography, typically addressed by supervised classifiers, and evaluate their performance. Two research questions guided the analysis: (RQ1) How do multimodal LLMs perform on image classification of Christian saints? And (RQ2), how does performance vary when enriching input with contextual information or few-shot exemplars? We conducted a benchmarking study using three datasets supporting Iconclass natively: ArtDL, ICONCLASS, and Wikidata, filtered to include the top 10 most frequent classes. Models were tested under three conditions: (1) classification using class labels, (2) classification with Iconclass descriptions, and (3) few-shot learning with five exemplars. Results were compared against ResNet50 baselines fine-tuned on the same datasets. The findings show that Gemini-2.5 Pro and GPT-4o outperformed the ResNet50 baselines. Accuracy dropped significantly on the Wikidata dataset, where Siglip reached the highest accuracy score, suggesting model sensitivity to image size and metadata alignment. Enriching prompts with class descriptions generally improved zero-shot performance, while few-shot learning produced lower results, with only occasional and minimal increments in accuracy. We conclude that general-purpose multimodal LLMs are capable of classification in visually complex cultural heritage domains. These results support the application of LLMs as metadata curation tools in digital humanities workflows, suggesting future research on prompt optimization and the expansion of the study to other classification strategies and models. </p>
<blockquote>
<p>æœ¬ç ”ç©¶æ—¨åœ¨è¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨åŸºç£æ•™è‚–åƒå­¦å•æ ‡ç­¾åˆ†ç±»ä»»åŠ¡ä¸­çš„èƒ½åŠ›ã€‚ç›®æ ‡æ˜¯è¯„ä¼°é€šç”¨VLMsï¼ˆCLIPå’ŒSigLIPï¼‰å’ŒLLMsï¼ˆå¦‚GPT-4oå’ŒGemini 2.5ï¼‰æ˜¯å¦èƒ½è§£è¯»é€šå¸¸é€šè¿‡ç›‘ç£åˆ†ç±»å™¨å¤„ç†çš„è‚–åƒå­¦ï¼Œå¹¶è¯„ä¼°å…¶æ€§èƒ½ã€‚æœ¬ç ”ç©¶ç”±ä¸¤ä¸ªç ”ç©¶é—®é¢˜å¼•å¯¼åˆ†æï¼šï¼ˆRQ1ï¼‰å¤šæ¨¡æ€LLMsåœ¨åŸºç£æ•™åœ£äººå›¾åƒåˆ†ç±»æ–¹é¢çš„è¡¨ç°å¦‚ä½•ï¼Ÿï¼ˆRQ2ï¼‰å½“ç”¨ä¸Šä¸‹æ–‡ä¿¡æ¯æˆ–å°‘é‡æ ·æœ¬ä¸°å¯Œè¾“å…¥æ—¶ï¼Œæ€§èƒ½å¦‚ä½•å˜åŒ–ï¼Ÿæˆ‘ä»¬ä½¿ç”¨æ”¯æŒIconclassçš„æœ¬åœ°æ•°æ®é›†ArtDLã€ICONCLASSå’ŒWikidataè¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œç­›é€‰å‡ºå‰10ä¸ªæœ€é¢‘ç¹çš„ç±»åˆ«ã€‚æ¨¡å‹åœ¨ä¸‰ç§æ¡ä»¶ä¸‹è¿›è¡Œæµ‹è¯•ï¼šï¼ˆ1ï¼‰ä½¿ç”¨ç±»åˆ«æ ‡ç­¾è¿›è¡Œåˆ†ç±»ï¼Œï¼ˆ2ï¼‰ä½¿ç”¨Iconclassæè¿°è¿›è¡Œåˆ†ç±»ï¼Œï¼ˆ3ï¼‰ä½¿ç”¨äº”ä¸ªæ ·æœ¬è¿›è¡Œå°æ ·æœ¬å­¦ä¹ ã€‚ç»“æœä¸åœ¨åŒæ•°æ®é›†ä¸Šå¾®è°ƒè¿‡çš„ResNet50åŸºå‡†çº¿è¿›è¡Œäº†æ¯”è¾ƒã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒGemini-2.5 Proå’ŒGPT-4oçš„è¡¨ç°ä¼˜äºResNet50åŸºå‡†çº¿ã€‚åœ¨Wikidataæ•°æ®é›†ä¸Šï¼Œå‡†ç¡®ç‡æ˜¾è‘—ä¸‹é™ï¼Œå…¶ä¸­Siglipè¾¾åˆ°æœ€é«˜å‡†ç¡®ç‡ï¼Œè¿™è¡¨æ˜æ¨¡å‹å¯¹å›¾åƒå¤§å°å’Œå…ƒæ•°æ®å¯¹é½çš„æ•æ„Ÿæ€§ã€‚ç”¨ç±»åˆ«æè¿°ä¸°å¯Œæç¤ºé€šå¸¸ä¼šæé«˜é›¶æ ·æœ¬æ€§èƒ½ï¼Œè€Œå°æ ·æœ¬å­¦ä¹ åˆ™äº§ç”Ÿè¾ƒä½çš„ç»“æœï¼Œåªæœ‰å¶å°”å’Œå¾®å°çš„å‡†ç¡®ç‡æå‡ã€‚æˆ‘ä»¬å¾—å‡ºç»“è®ºï¼Œé€šç”¨å¤šæ¨¡æ€LLMså…·å¤‡åœ¨è§†è§‰å¤æ‚çš„æ–‡åŒ–é—äº§é¢†åŸŸè¿›è¡Œåˆ†ç±»çš„èƒ½åŠ›ã€‚è¿™äº›ç»“æœæ”¯æŒå°†LLMsåº”ç”¨äºæ•°å­—äººæ–‡å·¥ä½œæµç¨‹ä¸­çš„å…ƒæ•°æ®æ•´ç†å·¥å…·ï¼Œå¹¶å»ºè®®æœªæ¥å¯¹æç¤ºè¿›è¡Œä¼˜åŒ–ï¼Œå¹¶å°†ç ”ç©¶æ‰©å±•åˆ°å…¶ä»–åˆ†ç±»ç­–ç•¥å’Œæ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.18839v1">PDF</a> 11 pages, 2 figures</p>
<p><strong>Summary</strong></p>
<p>è¯¥ç ”ç©¶è¯„ä¼°äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨åŸºç£æ•™è‚–åƒç”»å•æ ‡ç­¾åˆ†ç±»ä»»åŠ¡ä¸­çš„èƒ½åŠ›ã€‚ç ”ç©¶ç›®æ ‡æ˜¯è¯„ä¼°é€šç”¨VLMsï¼ˆCLIPå’ŒSigLIPï¼‰å’ŒLLMsï¼ˆå¦‚GPT-4oå’ŒGemini 2.5ï¼‰æ˜¯å¦èƒ½è§£è¯»é€šå¸¸é€šè¿‡ç›‘ç£åˆ†ç±»å™¨å¤„ç†çš„è‚–åƒç”»ï¼Œå¹¶è¯„ä¼°å…¶æ€§èƒ½ã€‚è¯¥ç ”ç©¶é€šè¿‡ä¸‰ä¸ªæ”¯æŒIconclassçš„æ•°æ®é›†è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬ArtDLã€ICONCLASSå’ŒWikidataï¼Œç­›é€‰å‡ºå‰10ä¸ªæœ€å¸¸è§çš„ç±»åˆ«ã€‚æ¨¡å‹åœ¨ä¸‰ç§æ¡ä»¶ä¸‹è¿›è¡Œæµ‹è¯•ï¼š1ï¼‰ä½¿ç”¨ç±»æ ‡ç­¾è¿›è¡Œåˆ†ç±»ï¼Œ2ï¼‰ä½¿ç”¨Iconclassæè¿°è¿›è¡Œåˆ†ç±»ï¼Œ3ï¼‰ä½¿ç”¨äº”ä¸ªèŒƒä¾‹è¿›è¡Œå°‘æ ·æœ¬å­¦ä¹ ã€‚ç»“æœè¡¨æ˜ï¼ŒGemini-2.5 Proå’ŒGPT-4oä¼˜äºResNet50åŸºçº¿ã€‚åœ¨Wikidataæ•°æ®é›†ä¸Šï¼ŒSiglipè¾¾åˆ°æœ€é«˜å‡†ç¡®åº¦å¾—åˆ†ï¼Œè¡¨æ˜æ¨¡å‹å¯¹å›¾åƒå¤§å°å’Œå…ƒæ•°æ®å¯¹é½çš„æ•æ„Ÿæ€§ã€‚ç”¨ç±»åˆ«æè¿°ä¸°å¯Œæç¤ºé€šå¸¸ä¼šæé«˜é›¶æ ·æœ¬æ€§èƒ½ï¼Œè€Œå°‘æ ·æœ¬å­¦ä¹ äº§ç”Ÿè¾ƒä½ç»“æœï¼Œä»…å¶å°”æœ‰å¾®å°çš„å‡†ç¡®æ€§å¢åŠ ã€‚ç»“è®ºæ˜¯ï¼Œé€šç”¨å¤šæ¨¡æ€LLMsèƒ½å¤Ÿåœ¨è§†è§‰å¤æ‚æ–‡åŒ–é—äº§é¢†åŸŸè¿›è¡Œåˆ†ç±»ã€‚è¿™äº›ç»“æœæ”¯æŒå°†LLMsåº”ç”¨äºæ•°å­—äººæ–‡å·¥ä½œæµä¸­çš„å…ƒæ•°æ®å·¥å…·ï¼Œå¹¶å»ºè®®æœªæ¥çš„ç ”ç©¶ä¼˜åŒ–æç¤ºå¹¶æ‰©å±•ç ”ç©¶åˆ°å…¶ä»–åˆ†ç±»ç­–ç•¥å’Œæ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬ç ”ç©¶è¯„ä¼°äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹å’Œè§†è§‰è¯­è¨€æ¨¡å‹åœ¨åŸºç£æ•™è‚–åƒç”»åˆ†ç±»ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚</li>
<li>ç ”ç©¶é€šè¿‡ä¸‰ä¸ªæ•°æ®é›†è¿›è¡Œï¼ŒåŒ…æ‹¬ArtDLã€ICONCLASSå’ŒWikidataï¼Œæ¶‰åŠæœ€å¸¸è§çš„10ä¸ªç±»åˆ«ã€‚</li>
<li>æ¨¡å‹åœ¨ä¸‰ç§ä¸åŒæ¡ä»¶ä¸‹è¿›è¡Œæµ‹è¯•ï¼šä½¿ç”¨ç±»æ ‡ç­¾ã€ä½¿ç”¨Iconclassæè¿°å’Œå°‘æ ·æœ¬å­¦ä¹ ã€‚</li>
<li>Gemini-2.5 Proå’ŒGPT-4oè¡¨ç°å‡ºè¾ƒé«˜çš„åˆ†ç±»æ€§èƒ½ï¼Œä¼˜äºResNet50åŸºçº¿ã€‚</li>
<li>åœ¨Wikidataæ•°æ®é›†ä¸Šï¼Œæ¨¡å‹å¯¹å›¾åƒå¤§å°å’Œå…ƒæ•°æ®å¯¹é½è¡¨ç°å‡ºæ•æ„Ÿæ€§ï¼ŒSiglipè·å¾—æœ€é«˜å‡†ç¡®åº¦ã€‚</li>
<li>ä¸°å¯Œæç¤ºï¼ˆä½¿ç”¨ç±»åˆ«æè¿°ï¼‰æœ‰åŠ©äºæé«˜é›¶æ ·æœ¬æ€§èƒ½ï¼Œè€Œå°‘æ ·æœ¬å­¦ä¹ ç»“æœæœ‰é™ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.18839">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Few-Shot/2509.18839v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Few-Shot/2509.18839v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="TempSamp-R1-Effective-Temporal-Sampling-with-Reinforcement-Fine-Tuning-for-Video-LLMs"><a href="#TempSamp-R1-Effective-Temporal-Sampling-with-Reinforcement-Fine-Tuning-for-Video-LLMs" class="headerlink" title="TempSamp-R1: Effective Temporal Sampling with Reinforcement Fine-Tuning   for Video LLMs"></a>TempSamp-R1: Effective Temporal Sampling with Reinforcement Fine-Tuning   for Video LLMs</h2><p><strong>Authors:Yunheng Li, Jing Cheng, Shaoyong Jia, Hangyi Kuang, Shaohui Jiao, Qibin Hou, Ming-Ming Cheng</strong></p>
<p>This paper introduces TempSamp-R1, a new reinforcement fine-tuning framework designed to improve the effectiveness of adapting multimodal large language models (MLLMs) to video temporal grounding tasks. We reveal that existing reinforcement learning methods, such as Group Relative Policy Optimization (GRPO), rely on on-policy sampling for policy updates. However, in tasks with large temporal search spaces, this strategy becomes both inefficient and limited in performance, as it often fails to identify temporally accurate solutions. To address this limitation, TempSamp-R1 leverages ground-truth annotations as off-policy supervision to provide temporally precise guidance, effectively compensating for the sparsity and misalignment in on-policy solutions. To further stabilize training and reduce variance in reward-based updates, TempSamp-R1 provides a non-linear soft advantage computation method that dynamically reshapes the reward feedback via an asymmetric transformation. By employing a hybrid Chain-of-Thought (CoT) training paradigm, TempSamp-R1 optimizes a single unified model to support both CoT and non-CoT inference modes, enabling efficient handling of queries with varying reasoning complexity. Experimental results demonstrate that TempSamp-R1 outperforms GRPO-based baselines, establishing new state-of-the-art performance on benchmark datasets: Charades-STA (<a href="mailto:&#x52;&#x31;&#x40;&#48;&#x2e;&#55;">&#x52;&#x31;&#x40;&#48;&#x2e;&#55;</a>: 52.9%, +2.7%), ActivityNet Captions (<a href="mailto:&#82;&#x31;&#x40;&#x30;&#46;&#53;">&#82;&#x31;&#x40;&#x30;&#46;&#53;</a>: 56.0%, +5.3%), and QVHighlights (mAP: 30.0%, +3.0%). Moreover, TempSamp-R1 shows robust few-shot generalization capabilities under limited data. Code: <a target="_blank" rel="noopener" href="https://github.com/HVision-NKU/TempSamp-R1">https://github.com/HVision-NKU/TempSamp-R1</a> </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†TempSamp-R1ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„å¼ºåŒ–å¾®è°ƒæ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰é€‚åº”è§†é¢‘æ—¶åºå®šä½ä»»åŠ¡çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬å‘ç°ç°æœ‰çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œå¦‚ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œä¾èµ–äºç­–ç•¥æ›´æ–°ä¸­çš„åœ¨ç­–ç•¥é‡‡æ ·ã€‚ç„¶è€Œï¼Œåœ¨å…·æœ‰å¤§æ—¶é—´æœç´¢ç©ºé—´çš„ä»»åŠ¡ä¸­ï¼Œæ­¤ç­–ç•¥æ—¢ä½æ•ˆåˆæ€§èƒ½å—é™ï¼Œå› ä¸ºå®ƒå¾€å¾€æ— æ³•è¯†åˆ«å‡ºæ—¶åºä¸Šç²¾ç¡®çš„è§£å†³æ–¹æ¡ˆã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼ŒTempSamp-R1åˆ©ç”¨çœŸå®æ³¨é‡Šä½œä¸ºç¦»çº¿ç›‘ç£æ¥æä¾›æ—¶é—´ç²¾ç¡®æŒ‡å¯¼ï¼Œæœ‰æ•ˆåœ°è¡¥å¿åœ¨çº¿ç­–ç•¥è§£å†³æ–¹æ¡ˆä¸­çš„ç¨€ç–æ€§å’Œä¸å¯¹é½é—®é¢˜ã€‚ä¸ºäº†è¿›ä¸€æ­¥ç¨³å®šè®­ç»ƒå’Œå‡å°‘åŸºäºå¥–åŠ±çš„æ›´æ–°çš„æ–¹å·®ï¼ŒTempSamp-R1æä¾›äº†ä¸€ç§éçº¿æ€§è½¯ä¼˜åŠ¿è®¡ç®—æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡ä¸å¯¹ç§°è½¬æ¢åŠ¨æ€åœ°é‡å¡‘å¥–åŠ±åé¦ˆã€‚é€šè¿‡é‡‡ç”¨æ··åˆå¼çš„æ€ç»´é“¾ï¼ˆCoTï¼‰è®­ç»ƒèŒƒå¼ï¼ŒTempSamp-R1ä¼˜åŒ–äº†ä¸€ä¸ªç»Ÿä¸€çš„æ¨¡å‹æ¥æ”¯æŒCoTå’ŒéCoTæ¨ç†æ¨¡å¼ï¼Œä»è€Œèƒ½å¤Ÿé«˜æ•ˆåœ°å¤„ç†å…·æœ‰ä¸åŒæ¨ç†å¤æ‚æ€§çš„æŸ¥è¯¢ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTempSamp-R1ä¼˜äºåŸºäºGRPOçš„åŸºçº¿ï¼Œåœ¨åŸºå‡†æ•°æ®é›†ä¸Šå®ç°äº†å“è¶Šçš„æ€§èƒ½ï¼šCharades-STAï¼ˆ<a href="mailto:&#82;&#49;&#64;&#x30;&#x2e;&#x37;">&#82;&#49;&#64;&#x30;&#x2e;&#x37;</a>: 52.9%ï¼Œ+2.7%ï¼‰ã€ActivityNet Captionsï¼ˆ<a href="mailto:&#82;&#x31;&#x40;&#x30;&#x2e;&#x35;">&#82;&#x31;&#x40;&#x30;&#x2e;&#x35;</a>: 56.0%ï¼Œ+5.3%ï¼‰å’ŒQVHighlightsï¼ˆmAP: 30.0%ï¼Œ+3.0%ï¼‰ã€‚æ­¤å¤–ï¼ŒTempSamp-R1åœ¨æœ‰é™æ•°æ®ä¸‹è¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚ä»£ç åœ°å€ï¼š<a target="_blank" rel="noopener" href="https://github.com/HVision-NKU/TempSamp-R1%E3%80%82">https://github.com/HVision-NKU/TempSamp-R1ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.18056v2">PDF</a> Accepted at NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªåä¸ºTempSamp-R1çš„æ–°å¼ºåŒ–å¾®è°ƒæ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ—¨åœ¨æé«˜å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è§†é¢‘æ—¶åºå®šä½ä»»åŠ¡ä¸­çš„é€‚åº”æ€§ã€‚TempSamp-R1è§£å†³äº†ç°æœ‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨å¤„ç†å¤§æ—¶åºæœç´¢ç©ºé—´æ—¶çš„æ•ˆç‡å’Œæ€§èƒ½å±€é™é—®é¢˜ã€‚å®ƒé€šè¿‡åˆ©ç”¨çœŸå®æ ‡æ³¨ä½œä¸ºç¦»çº¿ç›‘ç£ï¼Œæä¾›ç²¾ç¡®çš„æ—¶é—´æŒ‡å¯¼ï¼Œå¹¶å¼•å…¥éçº¿æ€§è½¯ä¼˜åŠ¿è®¡ç®—æ–¹æ³•ï¼ŒåŠ¨æ€è°ƒæ•´å¥–åŠ±åé¦ˆã€‚æ­¤å¤–ï¼ŒTempSamp-R1é‡‡ç”¨æ··åˆçš„Chain-of-Thoughtè®­ç»ƒèŒƒå¼ï¼Œæ”¯æŒä¸åŒæ¨ç†å¤æ‚åº¦çš„æŸ¥è¯¢å¤„ç†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTempSamp-R1åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå®ç°äº†å“è¶Šæ€§èƒ½ï¼Œå¹¶å…·æœ‰å¼ºå¤§çš„å°‘æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TempSamp-R1æ˜¯ä¸€ä¸ªå¼ºåŒ–å¾®è°ƒæ¡†æ¶ï¼Œç”¨äºæ”¹è¿›å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è§†é¢‘æ—¶åºå®šä½ä»»åŠ¡ä¸­çš„æ•ˆæœã€‚</li>
<li>ç°æœ‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨å¤„ç†å¤§æ—¶åºæœç´¢ç©ºé—´æ—¶å­˜åœ¨æ•ˆç‡å’Œæ€§èƒ½å±€é™ã€‚</li>
<li>TempSamp-R1åˆ©ç”¨çœŸå®æ ‡æ³¨ä½œä¸ºç¦»çº¿ç›‘ç£ï¼Œæä¾›ç²¾ç¡®çš„æ—¶é—´æŒ‡å¯¼ã€‚</li>
<li>TempSamp-R1å¼•å…¥éçº¿æ€§è½¯ä¼˜åŠ¿è®¡ç®—æ–¹æ³•ï¼ŒåŠ¨æ€è°ƒæ•´å¥–åŠ±åé¦ˆã€‚</li>
<li>TempSamp-R1é‡‡ç”¨æ··åˆçš„Chain-of-Thoughtè®­ç»ƒèŒƒå¼ï¼Œæ”¯æŒä¸åŒæ¨ç†å¤æ‚åº¦çš„æŸ¥è¯¢å¤„ç†ã€‚</li>
<li>TempSamp-R1åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå®ç°äº†å“è¶Šæ€§èƒ½ã€‚</li>
<li>TempSamp-R1å±•ç°å‡ºå¼ºå¤§çš„å°‘æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.18056">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Few-Shot/2509.18056v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Few-Shot/2509.18056v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Few-Shot/2509.18056v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Few-Shot/2509.18056v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Can-LLMs-Reason-Over-Non-Text-Modalities-in-a-Training-Free-Manner-A-Case-Study-with-In-Context-Representation-Learning"><a href="#Can-LLMs-Reason-Over-Non-Text-Modalities-in-a-Training-Free-Manner-A-Case-Study-with-In-Context-Representation-Learning" class="headerlink" title="Can LLMs Reason Over Non-Text Modalities in a Training-Free Manner? A   Case Study with In-Context Representation Learning"></a>Can LLMs Reason Over Non-Text Modalities in a Training-Free Manner? A   Case Study with In-Context Representation Learning</h2><p><strong>Authors:Tianle Zhang, Wanlong Fang, Jonathan Woo, Paridhi Latawa, Deepak A. Subramanian, Alvin Chan</strong></p>
<p>The remarkable performance of Large Language Models (LLMs) can be enhanced with test-time computation, which relies on external tools and even other deep learning models. However, existing approaches for integrating non-text modality representations into LLMs typically require additional costly supervised training, restricting on-the-fly adaptation to new domains and modalities. In this work, we explore the feasibility of integrating representations from non-text foundational models (FMs) into text-based LLMs in a training-free manner. We propose In-Context Representation Learning (ICRL) as a proof-of-concept to allow LLMs to adaptively utilize non-text modality representations with few-shot learning. Unlike traditional in-context learning, which incorporates text-label pairs, ICRL replaces text inputs with FM representations, enabling the LLM to perform multi-modal inference without fine-tuning. We evaluate ICRL on a suite of tasks in the molecular domain, investigating three core research questions: (i) how to map FM representations into LLMs in a training-free manner, (ii) what factors influence ICRL performance, and (iii) what mechanisms underlie the effectiveness of ICRL. To the best of our knowledge, ICRL is the first training-free framework for integrating non-text modality representations into text-based LLMs, presenting a promising direction for adaptable, multi-modal generalization. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‡ºè‰²æ€§èƒ½å¯ä»¥é€šè¿‡æµ‹è¯•æ—¶çš„è®¡ç®—å¢å¼ºï¼Œè¯¥è®¡ç®—ä¾èµ–äºå¤–éƒ¨å·¥å…·å’Œå…¶ä»–æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚ç„¶è€Œï¼Œå°†éæ–‡æœ¬æ¨¡æ€è¡¨ç¤ºé›†æˆåˆ°LLMä¸­çš„ç°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦é¢å¤–çš„æ˜‚è´µçš„æœ‰ç›‘ç£è®­ç»ƒï¼Œè¿™é™åˆ¶äº†åœ¨æ–°é¢†åŸŸå’Œæ¨¡æ€çš„å³æ—¶é€‚åº”ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ¢ç´¢äº†ä»¥è®­ç»ƒæ— å…³çš„æ–¹å¼å°†éæ–‡æœ¬åŸºç¡€æ¨¡å‹ï¼ˆFMï¼‰çš„è¡¨ç¤ºé›†æˆåˆ°æ–‡æœ¬ä¸ºåŸºç¡€çš„LLMä¸­çš„å¯è¡Œæ€§ã€‚æˆ‘ä»¬æå‡ºåŸºäºæƒ…å¢ƒçš„è¡¨ç¤ºå­¦ä¹ ï¼ˆICRLï¼‰ä½œä¸ºæ¦‚å¿µéªŒè¯ï¼Œå…è®¸LLMä»¥å°‘é‡å­¦ä¹ çš„æ–¹å¼è‡ªé€‚åº”åœ°åˆ©ç”¨éæ–‡æœ¬æ¨¡æ€è¡¨ç¤ºã€‚ä¸ä¼ ç»Ÿçš„ä¸Šä¸‹æ–‡å­¦ä¹ ä¸åŒï¼ŒICRLç»“åˆäº†æ–‡æœ¬æ ‡ç­¾å¯¹ï¼Œå¹¶ç”¨FMè¡¨ç¤ºæ›¿æ¢æ–‡æœ¬è¾“å…¥ï¼Œä½¿LLMèƒ½å¤Ÿåœ¨ä¸è¿›è¡Œå¾®è°ƒçš„æƒ…å†µä¸‹æ‰§è¡Œå¤šæ¨¡æ€æ¨ç†ã€‚æˆ‘ä»¬åœ¨åˆ†å­é¢†åŸŸçš„å¤šä¸ªä»»åŠ¡ä¸Šè¯„ä¼°äº†ICRLï¼Œç ”ç©¶äº†ä¸‰ä¸ªæ ¸å¿ƒé—®é¢˜ï¼šï¼ˆiï¼‰å¦‚ä½•ä»¥è®­ç»ƒæ— å…³çš„æ–¹å¼å°†FMè¡¨ç¤ºæ˜ å°„åˆ°LLMä¸­ï¼Œï¼ˆiiï¼‰å“ªäº›å› ç´ å½±å“ICRLçš„æ€§èƒ½ï¼Œä»¥åŠï¼ˆiiiï¼‰ICRLæœ‰æ•ˆæ€§çš„åŸºç¡€æœºåˆ¶æ˜¯ä»€ä¹ˆã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼ŒICRLæ˜¯ç¬¬ä¸€ä¸ªå°†éæ–‡æœ¬æ¨¡æ€è¡¨ç¤ºé›†æˆåˆ°æ–‡æœ¬åŸºç¡€LLMä¸­çš„è®­ç»ƒæ— å…³æ¡†æ¶ï¼Œä¸ºå¯é€‚åº”çš„å¤šæ¨¡æ€æ³›åŒ–æä¾›äº†æœ‰å‰æ™¯çš„æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.17552v2">PDF</a> NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ€§èƒ½å¯é€šè¿‡æµ‹è¯•æ—¶çš„è®¡ç®—å¢å¼ºï¼Œé€šè¿‡åˆ©ç”¨å¤–éƒ¨å·¥å…·å’Œå…¶ä»–æ·±åº¦å­¦ä¹ æ¨¡å‹æ¥å®ç°ã€‚ç„¶è€Œï¼Œå°†éæ–‡æœ¬æ¨¡æ€è¡¨ç¤ºé›†æˆåˆ°LLMsä¸­çš„ç°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦é¢å¤–çš„ç›‘ç£è®­ç»ƒï¼Œè¿™é™åˆ¶äº†åœ¨æ–°é¢†åŸŸå’Œæ¨¡æ€çš„å³æ—¶é€‚åº”ã€‚æœ¬ç ”ç©¶æ¢ç´¢äº†ä»¥æ— è®­ç»ƒæ–¹å¼å°†éæ–‡æœ¬åŸºç¡€æ¨¡å‹ï¼ˆFMsï¼‰çš„è¡¨ç¤ºé›†æˆåˆ°æ–‡æœ¬ä¸ºåŸºç¡€çš„LLMsä¸­çš„å¯è¡Œæ€§ã€‚æˆ‘ä»¬æå‡ºäº†åŸºäºä¸Šä¸‹æ–‡è¡¨ç¤ºå­¦ä¹ ï¼ˆICRLï¼‰çš„æ¦‚å¿µéªŒè¯ï¼Œä½¿LLMsèƒ½å¤Ÿåˆ©ç”¨éæ–‡æœ¬æ¨¡æ€è¡¨ç¤ºè¿›è¡Œå°‘é‡å­¦ä¹ çš„è‡ªé€‚åº”åˆ©ç”¨ã€‚ä¸åŒäºä¼ ç»ŸåŸºäºæ–‡æœ¬æ ‡ç­¾å¯¹è¿›è¡Œä¸Šä¸‹æ–‡å­¦ä¹ çš„ICRLç”¨FMè¡¨ç¤ºæ›¿æ¢æ–‡æœ¬è¾“å…¥ï¼Œä½¿LLMèƒ½å¤Ÿè¿›è¡Œå¤šæ¨¡æ€æ¨ç†è€Œæ— éœ€å¾®è°ƒã€‚æˆ‘ä»¬åœ¨åˆ†å­é¢†åŸŸçš„å¤šä¸ªä»»åŠ¡ä¸Šè¯„ä¼°äº†ICRLçš„è¡¨ç°ï¼Œå¹¶æ¢è®¨äº†ä¸‰ä¸ªæ ¸å¿ƒé—®é¢˜ï¼šå¦‚ä½•ä»¥æ— è®­ç»ƒæ–¹å¼å°†FMè¡¨ç¤ºæ˜ å°„åˆ°LLMsä¸­ï¼Œå“ªäº›å› ç´ å½±å“ICRLçš„æ€§èƒ½ï¼Œä»¥åŠICRLæœ‰æ•ˆæ€§çš„å†…åœ¨æœºåˆ¶æ˜¯ä»€ä¹ˆã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼ŒICRLæ˜¯ç¬¬ä¸€ä¸ªç”¨äºå°†éæ–‡æœ¬æ¨¡æ€è¡¨ç¤ºé›†æˆåˆ°åŸºäºæ–‡æœ¬çš„LLMsä¸­çš„æ— è®­ç»ƒæ¡†æ¶ï¼Œä¸ºå¯é€‚åº”çš„å¤šæ¨¡æ€æ³›åŒ–æä¾›äº†æœ‰å‰é€”çš„æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯ä»¥é€šè¿‡æµ‹è¯•æ—¶çš„è®¡ç®—å¢å¼ºæ€§èƒ½ï¼Œå€ŸåŠ©å¤–éƒ¨å·¥å…·å’Œæ·±åº¦å­¦ä¹ æ¨¡å‹å®ç°ã€‚</li>
<li>ç°æœ‰é›†æˆéæ–‡æœ¬æ¨¡æ€è¡¨ç¤ºåˆ°LLMsçš„æ–¹æ³•éœ€è¦é¢å¤–çš„ç›‘ç£è®­ç»ƒï¼Œé™åˆ¶äº†æ–°é¢†åŸŸå’Œæ¨¡æ€çš„å³æ—¶é€‚åº”ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†åŸºäºä¸Šä¸‹æ–‡è¡¨ç¤ºå­¦ä¹ ï¼ˆICRLï¼‰çš„æ¦‚å¿µï¼Œä½¿LLMsèƒ½å¤Ÿåˆ©ç”¨éæ–‡æœ¬æ¨¡æ€è¡¨ç¤ºè¿›è¡Œå°‘é‡å­¦ä¹ ã€‚</li>
<li>ICRLä»¥æ— è®­ç»ƒæ–¹å¼å°†éæ–‡æœ¬åŸºç¡€æ¨¡å‹ï¼ˆFMsï¼‰çš„è¡¨ç¤ºé›†æˆåˆ°LLMsä¸­ï¼Œä½¿ç”¨FMè¡¨ç¤ºæ›¿æ¢æ–‡æœ¬è¾“å…¥ï¼Œå®ç°å¤šæ¨¡æ€æ¨ç†ã€‚</li>
<li>ICRLåœ¨åˆ†å­é¢†åŸŸçš„å¤šä¸ªä»»åŠ¡ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚</li>
<li>æ¢è®¨äº†å¦‚ä½•å°†FMè¡¨ç¤ºæ˜ å°„åˆ°LLMsã€å½±å“ICRLæ€§èƒ½çš„å› ç´ ä»¥åŠICRLæœ‰æ•ˆæ€§çš„å†…åœ¨æœºåˆ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.17552">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Few-Shot/2509.17552v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Few-Shot/2509.17552v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Few-Shot/2509.17552v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Automated-Generation-of-Research-Workflows-from-Academic-Papers-A-Full-text-Mining-Framework"><a href="#Automated-Generation-of-Research-Workflows-from-Academic-Papers-A-Full-text-Mining-Framework" class="headerlink" title="Automated Generation of Research Workflows from Academic Papers: A   Full-text Mining Framework"></a>Automated Generation of Research Workflows from Academic Papers: A   Full-text Mining Framework</h2><p><strong>Authors:Heng Zhang, Chengzhi Zhang</strong></p>
<p>The automated generation of research workflows is essential for improving the reproducibility of research and accelerating the paradigm of â€œAI for Scienceâ€. However, existing methods typically extract merely fragmented procedural components and thus fail to capture complete research workflows. To address this gap, we propose an end-to-end framework that generates comprehensive, structured research workflows by mining full-text academic papers. As a case study in the Natural Language Processing (NLP) domain, our paragraph-centric approach first employs Positive-Unlabeled (PU) Learning with SciBERT to identify workflow-descriptive paragraphs, achieving an F1-score of 0.9772. Subsequently, we utilize Flan-T5 with prompt learning to generate workflow phrases from these paragraphs, yielding ROUGE-1, ROUGE-2, and ROUGE-L scores of 0.4543, 0.2877, and 0.4427, respectively. These phrases are then systematically categorized into data preparation, data processing, and data analysis stages using ChatGPT with few-shot learning, achieving a classification precision of 0.958. By mapping categorized phrases to their document locations in the documents, we finally generate readable visual flowcharts of the entire research workflows. This approach facilitates the analysis of workflows derived from an NLP corpus and reveals key methodological shifts over the past two decades, including the increasing emphasis on data analysis and the transition from feature engineering to ablation studies. Our work offers a validated technical framework for automated workflow generation, along with a novel, process-oriented perspective for the empirical investigation of evolving scientific paradigms. Source code and data are available at: <a target="_blank" rel="noopener" href="https://github.com/ZH-heng/research_workflow">https://github.com/ZH-heng/research_workflow</a>. </p>
<blockquote>
<p>ç ”ç©¶å·¥ä½œæµç¨‹çš„è‡ªåŠ¨åŒ–ç”Ÿæˆå¯¹äºæé«˜ç ”ç©¶çš„å¯é‡å¤æ€§å’ŒåŠ é€Ÿâ€œäººå·¥æ™ºèƒ½ç§‘å­¦â€çš„æ¨¡å¼è½¬å˜è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é€šå¸¸ä»…æå–ç¢ç‰‡åŒ–çš„ç¨‹åºç»„ä»¶ï¼Œå› æ­¤æ— æ³•æ•è·å®Œæ•´çš„ç ”ç©¶å·¥ä½œæµç¨‹ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç«¯åˆ°ç«¯çš„æ¡†æ¶ï¼Œé€šè¿‡æŒ–æ˜å…¨æ–‡å­¦æœ¯è®ºæ–‡æ¥ç”Ÿæˆå…¨é¢ã€ç»“æ„åŒ–çš„ç ”ç©¶å·¥ä½œæµç¨‹ã€‚ä½œä¸ºè‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰é¢†åŸŸçš„æ¡ˆä¾‹ç ”ç©¶ï¼Œæˆ‘ä»¬é‡‡ç”¨æ®µè½ä¸ºä¸­å¿ƒçš„æ–¹æ³•ï¼Œé¦–å…ˆä½¿ç”¨SciBERTçš„é˜³æ€§æœªæ ‡è®°ï¼ˆPUï¼‰å­¦ä¹ æ¥è¯†åˆ«å·¥ä½œæµæè¿°æ€§æ®µè½ï¼ŒF1åˆ†æ•°è¾¾åˆ°0.9772ã€‚éšåï¼Œæˆ‘ä»¬ä½¿ç”¨Flan-T5ä¸æç¤ºå­¦ä¹ ä»è¿™äº›æ®µè½ç”Ÿæˆå·¥ä½œæµçŸ­è¯­ï¼Œå…¶ROUGE-1ã€ROUGE-2å’ŒROUGE-Låˆ†æ•°åˆ†åˆ«ä¸º0.4543ã€0.2877å’Œ0.4427ã€‚æ¥ä¸‹æ¥ï¼Œåˆ©ç”¨ChatGPTè¿›è¡Œå°‘é‡å­¦ä¹ æ¥ç³»ç»Ÿåœ°å°†è¿™äº›çŸ­è¯­åˆ†ç±»ä¸ºæ•°æ®å‡†å¤‡ã€æ•°æ®å¤„ç†å’Œæ•°æ®åˆ†æé˜¶æ®µï¼Œåˆ†ç±»ç²¾åº¦è¾¾åˆ°0.958ã€‚é€šè¿‡å°†åˆ†ç±»åçš„çŸ­è¯­æ˜ å°„åˆ°æ–‡æ¡£ä¸­çš„ä½ç½®ï¼Œæˆ‘ä»¬æœ€ç»ˆç”Ÿæˆäº†æ•´ä¸ªç ”ç©¶å·¥ä½œçš„å¯é˜…è¯»å¯è§†åŒ–æµç¨‹å›¾ã€‚æ­¤æ–¹æ³•ä¾¿äºåˆ†ææ¥è‡ªNLPè¯­æ–™åº“çš„å·¥ä½œæµï¼Œå¹¶æ­ç¤ºäº†è¿‡å»äºŒåå¹´æ¥æ–¹æ³•è®ºçš„å…³é”®è½¬å˜ï¼ŒåŒ…æ‹¬å¯¹æ•°æ®åˆ†æçš„æ—¥ç›Šé‡è§†ä»¥åŠä»ç‰¹å¾å·¥ç¨‹åˆ°æ¶ˆèç ”ç©¶çš„è½¬å˜ã€‚æˆ‘ä»¬çš„å·¥ä½œä¸ºè‡ªåŠ¨å·¥ä½œæµç”Ÿæˆæä¾›äº†ç»éªŒéªŒè¯çš„æŠ€æœ¯æ¡†æ¶ï¼Œä»¥åŠä¸€ä¸ªç”¨äºç ”ç©¶ä¸æ–­å‘å±•çš„ç§‘å­¦èŒƒå¼çš„è¿‡ç¨‹å¯¼å‘å‹è§†è§’ã€‚æºä»£ç å’Œæ•°æ®å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/ZH-heng/research_workflow">https://github.com/ZH-heng/research_workflow</a> æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.12955v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§ç«¯åˆ°ç«¯çš„æ¡†æ¶ï¼Œé€šè¿‡æŒ–æ˜å…¨æ–‡å­¦æœ¯è®ºæ–‡æ¥ç”Ÿæˆå…¨é¢ã€ç»“æ„åŒ–çš„ç ”ç©¶å·¥ä½œæµç¨‹ã€‚è¯¥æ¡†æ¶é‡‡ç”¨è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯ï¼ŒåŒ…æ‹¬ä½¿ç”¨SciBERTè¿›è¡Œæµç¨‹æè¿°æ®µè½è¯†åˆ«ã€ä½¿ç”¨Flan-T5è¿›è¡Œå·¥ä½œæµçŸ­è¯­ç”Ÿæˆï¼Œä»¥åŠä½¿ç”¨ChatGPTè¿›è¡Œé˜¶æ®µåˆ†ç±»å’Œæµç¨‹å›¾ç”Ÿæˆã€‚æ­¤æ–¹æ³•ä¸ºè‡ªåŠ¨ç”Ÿæˆç ”ç©¶å·¥ä½œæµç¨‹æä¾›äº†æŠ€æœ¯æ¡†æ¶ï¼ŒåŒæ—¶ä¸ºç ”ç©¶èŒƒå¼çš„æ¼”å˜æä¾›äº†è¿‡ç¨‹å¯¼å‘çš„è§†è§’ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºä¸€ç§è‡ªåŠ¨åŒ–ç”Ÿæˆç ”ç©¶å·¥ä½œæµç¨‹çš„ç«¯åˆ°ç«¯æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜ç ”ç©¶çš„å¯é‡å¤æ€§å’Œâ€AI for Scienceâ€èŒƒå¼çš„åŠ é€Ÿå‘å±•ã€‚</li>
<li>é‡‡ç”¨è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯å¦‚SciBERTã€Flan-T5å’ŒChatGPTæ¥è¯†åˆ«å’Œç”Ÿæˆç ”ç©¶å·¥ä½œæµç¨‹çš„å„ä¸ªç¯èŠ‚ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½ç³»ç»Ÿåœ°åˆ†ç±»æ•°æ®å‡†å¤‡ã€æ•°æ®å¤„ç†å’Œæ•°æ®åˆ†æé˜¶æ®µï¼Œå¹¶å®ç°æµç¨‹å›¾çš„å¯è§†åŒ–ç”Ÿæˆã€‚</li>
<li>æ–¹æ³•é€šè¿‡NLPé¢†åŸŸçš„æ¡ˆä¾‹ç ”ç©¶å¾—åˆ°éªŒè¯ï¼Œå¹¶æ­ç¤ºäº†è¿‡å»äºŒåå¹´ä¸­æ–¹æ³•è®ºçš„å…³é”®è½¬å˜ï¼Œå¦‚å¯¹æ•°æ®åˆ†æçš„é‡è§†å¢åŠ ä»¥åŠä»ç‰¹å¾å·¥ç¨‹åˆ°æ¶ˆèç ”ç©¶çš„è½¬å˜ã€‚</li>
<li>æ­¤æ¡†æ¶ä¸ºè‡ªåŠ¨å·¥ä½œæµç”Ÿæˆæä¾›äº†éªŒè¯çš„æŠ€æœ¯å¹³å°ï¼Œä¸ºç ”ç©¶èŒƒå¼çš„æ¼”å˜æä¾›äº†æ–°é¢–çš„è¿‡ç¨‹å¯¼å‘è§†è§’ã€‚</li>
<li>æºä»£ç å’Œæ•°æ®å¯åœ¨æŒ‡å®šé“¾æ¥æ‰¾åˆ°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.12955">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Few-Shot/2509.12955v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Few-Shot/2509.12955v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Few-Shot/2509.12955v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Data-Augmented-Few-Shot-Neural-Emulator-for-Computer-Model-System-Identification"><a href="#Data-Augmented-Few-Shot-Neural-Emulator-for-Computer-Model-System-Identification" class="headerlink" title="Data-Augmented Few-Shot Neural Emulator for Computer-Model System   Identification"></a>Data-Augmented Few-Shot Neural Emulator for Computer-Model System   Identification</h2><p><strong>Authors:Sanket Jantre, Deepak Akhare, Zhiyuan Wang, Xiaoning Qian, Nathan M. Urban</strong></p>
<p>Partial differential equations (PDEs) underpin the modeling of many natural and engineered systems. It can be convenient to express such models as neural PDEs rather than using traditional numerical PDE solvers by replacing part or all of the PDEâ€™s governing equations with a neural network representation. Neural PDEs are often easier to differentiate, linearize, reduce, or use for uncertainty quantification than the original numerical solver. They are usually trained on solution trajectories obtained by long-horizon rollout of the PDE solver. Here we propose a more sample-efficient data-augmentation strategy for generating neural PDE training data from a computer model by space-filling sampling of local â€œstencilâ€ states. This approach removes a large degree of spatiotemporal redundancy present in trajectory data and oversamples states that may be rarely visited but help the neural PDE generalize across the state space. We demonstrate that accurate neural PDE stencil operators can be learned from synthetic training data generated by the computational equivalent of 10 timestepsâ€™ worth of numerical simulation. Accuracy is further improved if we assume access to a single full-trajectory simulation from the computer model, which is typically available in practice. Across several PDE systems, we show that our data-augmented stencil data yield better trained neural stencil operators, with clear performance gains compared with naively sampled stencil data from simulation trajectories. Finally, with only 10 solver stepsâ€™ worth of augmented stencil data, our approach outperforms traditional ML emulators trained on thousands of trajectories in long-horizon rollout accuracy and stability. </p>
<blockquote>
<p>åå¾®åˆ†æ–¹ç¨‹ï¼ˆPDEsï¼‰æ˜¯è®¸å¤šè‡ªç„¶å’Œå·¥ç¨‹ç³»ç»Ÿå»ºæ¨¡çš„åŸºç¡€ã€‚é€šè¿‡ç”¨ç¥ç»ç½‘ç»œè¡¨ç¤ºæ›¿æ¢PDEçš„éƒ¨åˆ†æˆ–å…¨éƒ¨æ§åˆ¶æ–¹ç¨‹ï¼Œå°†æ­¤ç±»æ¨¡å‹è¡¨è¾¾ä¸ºç¥ç»PDEï¼Œè€Œä¸æ˜¯ä½¿ç”¨ä¼ ç»Ÿçš„æ•°å€¼PDEæ±‚è§£å™¨ï¼Œè¿™æ ·åšä¼šæ›´æ–¹ä¾¿ã€‚ç¥ç»PDEé€šå¸¸æ¯”åŸå§‹æ•°å€¼æ±‚è§£å™¨æ›´å®¹æ˜“è¿›è¡Œå¾®åˆ†ã€çº¿æ€§åŒ–ã€ç®€åŒ–æˆ–ç”¨äºä¸ç¡®å®šæ€§é‡åŒ–ã€‚å®ƒä»¬é€šå¸¸æ˜¯åœ¨PDEæ±‚è§£å™¨çš„é•¿å‘¨æœŸæ»šåŠ¨è¿‡ç¨‹ä¸­è·å¾—çš„è§£å†³æ–¹æ¡ˆè½¨è¿¹ä¸Šè¿›è¡Œè®­ç»ƒã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é€šè¿‡ç©ºé—´å¡«å……é‡‡æ ·å±€éƒ¨â€œæ¨¡æ¿â€çŠ¶æ€æ¥ä»è®¡ç®—æœºæ¨¡å‹ç”Ÿæˆç¥ç»PDEè®­ç»ƒæ•°æ®çš„æ•°æ®å¢å¼ºç­–ç•¥ï¼Œè¿™ç§ç­–ç•¥èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°è·å–æ ·æœ¬ã€‚æ­¤æ–¹æ³•æ¶ˆé™¤äº†è½¨è¿¹æ•°æ®ä¸­å¤§é‡å­˜åœ¨çš„æ—¶ç©ºå†—ä½™ï¼Œå¹¶å¯¹å¯èƒ½å¾ˆå°‘è®¿é—®ä½†æœ‰åŠ©äºç¥ç»PDEåœ¨çŠ¶æ€ç©ºé—´ä¸­æ¨å¹¿çš„çŠ¶æ€è¿›è¡Œè¿‡é‡‡æ ·ã€‚æˆ‘ä»¬è¯æ˜ï¼Œé€šè¿‡è®¡ç®—ç›¸å½“äº10ä¸ªæ—¶é—´æ­¥é•¿çš„æ•°å€¼æ¨¡æ‹Ÿç”Ÿæˆçš„ç»¼åˆè®­ç»ƒæ•°æ®ï¼Œå¯ä»¥å‡†ç¡®åœ°å­¦ä¹ ç¥ç»PDEæ¨¡æ¿ç®—å­ã€‚å¦‚æœæˆ‘ä»¬èƒ½å¤Ÿè®¿é—®è®¡ç®—æœºæ¨¡å‹ä¸­çš„å•ä¸ªå®Œæ•´è½¨è¿¹æ¨¡æ‹Ÿï¼ˆè¿™åœ¨å®è·µä¸­é€šå¸¸å¯ç”¨ï¼‰ï¼Œåˆ™å‡†ç¡®æ€§ä¼šè¿›ä¸€æ­¥æé«˜ã€‚åœ¨å‡ ä¸ªPDEç³»ç»Ÿä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†é€šè¿‡æ•°æ®å¢å¼ºè·å¾—çš„æ¨¡æ¿æ•°æ®å¯ä»¥è®­ç»ƒå‡ºæ›´å¥½çš„ç¥ç»æ¨¡æ¿ç®—å­ï¼Œä¸ä»æ¨¡æ‹Ÿè½¨è¿¹ä¸­ç®€å•é‡‡æ ·çš„æ¨¡æ¿æ•°æ®ç›¸æ¯”ï¼Œè¡¨ç°å‡ºæ˜æ˜¾çš„æ€§èƒ½æå‡ã€‚æœ€åï¼Œä»…ä½¿ç”¨ç›¸å½“äº10ä¸ªæ±‚è§£å™¨æ­¥éª¤çš„å¢å¼ºæ¨¡æ¿æ•°æ®ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨é•¿æœŸæ»šåŠ¨å‡†ç¡®æ€§å’Œç¨³å®šæ€§æ–¹é¢ä¼˜äºä¼ ç»ŸMLæ¨¡æ‹Ÿå™¨ï¼Œåè€…æ˜¯åœ¨æ•°åƒæ¡è½¨è¿¹ä¸Šè¿›è¡Œè®­ç»ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.19441v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å°†åå¾®åˆ†æ–¹ç¨‹ï¼ˆPDEsï¼‰æ¨¡å‹è½¬åŒ–ä¸ºç¥ç»ç½‘ç»œè¡¨ç¤ºçš„ç¥ç»PDEæ¨¡å‹çš„æ–¹æ³•ã€‚ä¸ºæé«˜ç¥ç»PDEæ¨¡å‹çš„è®­ç»ƒæ•ˆç‡ï¼Œæå‡ºäº†ä¸€ç§åŸºäºç©ºé—´å¡«å……é‡‡æ ·çš„æ•°æ®å¢å¼ºç­–ç•¥ï¼Œé€šè¿‡å±€éƒ¨â€œæ¨¡æ¿â€çŠ¶æ€çš„ç©ºé—´å¡«å……é‡‡æ ·ç”Ÿæˆè®­ç»ƒæ•°æ®ã€‚è¯¥æ–¹æ³•å‡å°‘äº†è½¨è¿¹æ•°æ®ä¸­çš„æ—¶ç©ºå†—ä½™ï¼Œå¹¶å¯¹å¯èƒ½å¾ˆå°‘è®¿é—®ä½†æœ‰åŠ©äºç¥ç»PDEæ³›åŒ–çš„çŠ¶æ€è¿›è¡Œè¿‡é‡‡æ ·ã€‚å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨è¿™ç§æ•°æ®å¢å¼ºç­–ç•¥çš„ç¥ç»PDEæ¨¡æ¿æ“ä½œå™¨å¯ä»¥æ›´å‡†ç¡®åœ°å­¦ä¹ ï¼Œå¹¶ä¸”åœ¨å¤šä¸ªPDEç³»ç»Ÿä¸­è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¥ç»PDEæ¨¡å‹å¯å°†åå¾®åˆ†æ–¹ç¨‹æ¨¡å‹è½¬åŒ–ä¸ºç¥ç»ç½‘ç»œè¡¨ç¤ºã€‚</li>
<li>ä¸ºæé«˜ç¥ç»PDEæ¨¡å‹çš„è®­ç»ƒæ•ˆç‡ï¼Œæå‡ºäº†åŸºäºç©ºé—´å¡«å……é‡‡æ ·çš„æ•°æ®å¢å¼ºç­–ç•¥ã€‚</li>
<li>è¯¥ç­–ç•¥é€šè¿‡å±€éƒ¨â€œæ¨¡æ¿â€çŠ¶æ€çš„ç©ºé—´å¡«å……é‡‡æ ·ç”Ÿæˆè®­ç»ƒæ•°æ®ï¼Œå‡å°‘è½¨è¿¹æ•°æ®ä¸­çš„å†—ä½™ã€‚</li>
<li>è¿‡é‡‡æ ·å¯èƒ½å¾ˆå°‘è®¿é—®ä½†æœ‰åŠ©äºç¥ç»PDEæ³›åŒ–çš„çŠ¶æ€ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨æ•°æ®å¢å¼ºç­–ç•¥çš„ç¥ç»PDEæ¨¡æ¿æ“ä½œå™¨å¯ä»¥æ›´å‡†ç¡®åœ°å­¦ä¹ ã€‚</li>
<li>åœ¨å¤šä¸ªPDEç³»ç»Ÿä¸­ï¼Œä½¿ç”¨æ•°æ®å¢å¼ºç­–ç•¥çš„ç¥ç»PDEè¡¨ç°å‡ºæ›´å¥½çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.19441">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Few-Shot/2508.19441v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Few-Shot/2508.19441v3/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Few-Shot/2508.19441v3/page_5_0.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Multimodal-Reference-Visual-Grounding"><a href="#Multimodal-Reference-Visual-Grounding" class="headerlink" title="Multimodal Reference Visual Grounding"></a>Multimodal Reference Visual Grounding</h2><p><strong>Authors:Yangxiao Lu, Ruosen Li, Liqiang Jing, Jikai Wang, Xinya Du, Yunhui Guo, Nicholas Ruozzi, Yu Xiang</strong></p>
<p>Visual grounding focuses on detecting objects from images based on language expressions. Recent Large Vision-Language Models (LVLMs) have significantly advanced visual grounding performance by training large models with large-scale datasets. However, the problem remains challenging, especially when similar objects appear in the input image. For example, an LVLM may not be able to differentiate Diet Coke and regular Coke in an image. In this case, if additional reference images of Diet Coke and regular Coke are available, it can help the visual grounding of similar objects.   In this work, we introduce a new task named Multimodal Reference Visual Grounding (MRVG). In this task, a model has access to a set of reference images of objects in a database. Based on these reference images and a language expression, the model is required to detect a target object from a query image. We first introduce a new dataset to study the MRVG problem. Then we introduce a novel method, named MRVG-Net, to solve this visual grounding problem. We show that by efficiently using reference images with few-shot object detection and using Large Language Models (LLMs) for object matching, our method achieves superior visual grounding performance compared to the state-of-the-art LVLMs such as Qwen2.5-VL-72B. Our approach bridges the gap between few-shot detection and visual grounding, unlocking new capabilities for visual understanding, which has wide applications in robotics. Project page with our video, code, and dataset: <a target="_blank" rel="noopener" href="https://irvlutd.github.io/MultiGrounding">https://irvlutd.github.io/MultiGrounding</a> </p>
<blockquote>
<p>è§†è§‰å®šä½ä¸“æ³¨äºæ ¹æ®è¯­è¨€è¡¨è¾¾ä»å›¾åƒä¸­æ£€æµ‹ç‰©ä½“ã€‚æœ€è¿‘çš„å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰é€šè¿‡å¤§è§„æ¨¡æ•°æ®é›†è®­ç»ƒå¤§å‹æ¨¡å‹ï¼Œå·²ç»æ˜¾è‘—æé«˜äº†è§†è§‰å®šä½çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œé—®é¢˜ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œç‰¹åˆ«æ˜¯å½“è¾“å…¥å›¾åƒä¸­å‡ºç°ç›¸ä¼¼ç‰©ä½“æ—¶ã€‚ä¾‹å¦‚ï¼ŒLVLMå¯èƒ½æ— æ³•åŒºåˆ†å›¾åƒä¸­çš„æ— ç³–å¯ä¹å’Œæ™®é€šå¯ä¹ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå¦‚æœæœ‰æ— ç³–å¯ä¹å’Œæ™®é€šå¯ä¹çš„é¢å¤–å‚è€ƒå›¾åƒå¯ç”¨ï¼Œå¯ä»¥å¸®åŠ©å¯¹ç›¸ä¼¼ç‰©ä½“çš„è§†è§‰å®šä½ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€é¡¹åä¸ºå¤šæ¨¡æ€å‚è€ƒè§†è§‰å®šä½ï¼ˆMRVGï¼‰çš„æ–°ä»»åŠ¡ã€‚åœ¨æ­¤ä»»åŠ¡ä¸­ï¼Œæ¨¡å‹å¯ä»¥è®¿é—®æ•°æ®åº“ä¸­å¯¹è±¡çš„å‚è€ƒå›¾åƒé›†ã€‚åŸºäºè¿™äº›å‚è€ƒå›¾åƒå’Œè¯­è¨€è¡¨è¾¾ï¼Œæ¨¡å‹éœ€è¦ä»æŸ¥è¯¢å›¾åƒä¸­æ£€æµ‹ç›®æ ‡ç‰©ä½“ã€‚æˆ‘ä»¬é¦–å…ˆå¼•å…¥ä¸€ä¸ªæ–°çš„æ•°æ®é›†æ¥ç ”ç©¶MRVGé—®é¢˜ã€‚ç„¶åï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§åä¸ºMRVG-Netçš„æ–°æ–¹æ³•æ¥è§£å†³è¿™ä¸ªè§†è§‰å®šä½é—®é¢˜ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œé€šè¿‡æœ‰æ•ˆåœ°ä½¿ç”¨å‚è€ƒå›¾åƒè¿›è¡Œå°æ ·æœ¬ç›®æ ‡æ£€æµ‹å’Œä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå¯¹è±¡åŒ¹é…ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è§†è§‰å®šä½æ€§èƒ½ä¸Šä¼˜äºæœ€æ–°çš„LVLMsï¼Œå¦‚Qwen2.5-VL-72Bã€‚æˆ‘ä»¬çš„æ–¹æ³•ç¼©å°äº†å°æ ·æœ¬æ£€æµ‹å’Œè§†è§‰å®šä½ä¹‹é—´çš„å·®è·ï¼Œä¸ºè§†è§‰ç†è§£è§£é”äº†æ–°çš„èƒ½åŠ›ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨äºæœºå™¨äººæŠ€æœ¯ç­‰é¢†åŸŸã€‚é¡¹ç›®é¡µé¢åŒ…å«æˆ‘ä»¬çš„è§†é¢‘ã€ä»£ç å’Œæ•°æ®é›†ï¼š<a target="_blank" rel="noopener" href="https://irvlutd.github.io/MultiGrounding">https://irvlutd.github.io/MultiGrounding</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.02876v2">PDF</a> Project page with our code and dataset:   <a target="_blank" rel="noopener" href="https://irvlutd.github.io/MultiGrounding">https://irvlutd.github.io/MultiGrounding</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„è§†è§‰å®šä½ä»»åŠ¡â€”â€”å¤šæ¨¡æ€å‚è€ƒè§†è§‰å®šä½ï¼ˆMRVGï¼‰ï¼Œåœ¨è¯¥ä»»åŠ¡ä¸­ï¼Œæ¨¡å‹å¯ä»¥è®¿é—®æ•°æ®åº“ä¸­çš„ä¸€ç»„å‚è€ƒå›¾åƒï¼Œå¹¶æ ¹æ®è¿™äº›å‚è€ƒå›¾åƒå’Œè¯­è¨€è¡¨è¾¾å¼ä»æŸ¥è¯¢å›¾åƒä¸­æ£€æµ‹ç›®æ ‡å¯¹è±¡ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæ–‡ç« å¼•å…¥äº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†å’Œä¸€ç§æ–°çš„æ–¹æ³•MRVG-Netï¼Œè¯¥æ–¹æ³•é€šè¿‡æœ‰æ•ˆåˆ©ç”¨å‚è€ƒå›¾åƒå’Œå¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå¯¹è±¡åŒ¹é…ï¼Œå®ç°äº†å¯¹ç±»ä¼¼å¯¹è±¡çš„è§†è§‰å®šä½ï¼Œå¹¶ä¼˜äºç°æœ‰çš„å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ã€‚æ­¤æ–¹æ³•å¡«è¡¥äº†å°‘æ ·æœ¬æ£€æµ‹å’Œè§†è§‰å®šä½ä¹‹é—´çš„ç©ºç™½ï¼Œä¸ºè§†è§‰ç†è§£å¼€è¾Ÿäº†æ–°çš„å¯èƒ½æ€§ï¼Œå¹¿æ³›åº”ç”¨äºæœºå™¨äººé¢†åŸŸã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†ä¸€ç§æ–°çš„è§†è§‰å®šä½ä»»åŠ¡â€”â€”å¤šæ¨¡æ€å‚è€ƒè§†è§‰å®šä½ï¼ˆMRVGï¼‰ã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†ç”¨äºç ”ç©¶MRVGé—®é¢˜ã€‚</li>
<li>å¼•å…¥äº†åä¸ºMRVG-Netçš„æ–°æ–¹æ³•æ¥è§£å†³è§†è§‰å®šä½é—®é¢˜ã€‚</li>
<li>MRVG-Neté€šè¿‡æœ‰æ•ˆåˆ©ç”¨å‚è€ƒå›¾åƒå’Œå¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå¯¹è±¡åŒ¹é…ã€‚</li>
<li>MRVG-Netå®ç°äº†å¯¹ç±»ä¼¼å¯¹è±¡çš„è§†è§‰å®šä½ï¼Œå¹¶ä¼˜äºç°æœ‰çš„å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ã€‚</li>
<li>è¯¥æ–¹æ³•å¡«è¡¥äº†å°‘æ ·æœ¬æ£€æµ‹å’Œè§†è§‰å®šä½ä¹‹é—´çš„ç©ºç™½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.02876">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Few-Shot/2504.02876v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Few-Shot/2504.02876v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Few-Shot/2504.02876v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Few-Shot/2504.02876v2/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Few-Shot/2504.02876v2/page_5_1.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Explainable-Sentiment-Analysis-with-DeepSeek-R1-Performance-Efficiency-and-Few-Shot-Learning"><a href="#Explainable-Sentiment-Analysis-with-DeepSeek-R1-Performance-Efficiency-and-Few-Shot-Learning" class="headerlink" title="Explainable Sentiment Analysis with DeepSeek-R1: Performance,   Efficiency, and Few-Shot Learning"></a>Explainable Sentiment Analysis with DeepSeek-R1: Performance,   Efficiency, and Few-Shot Learning</h2><p><strong>Authors:Donghao Huang, Zhaoxia Wang</strong></p>
<p>Large language models (LLMs) have transformed sentiment analysis, yet balancing accuracy, efficiency, and explainability remains a critical challenge. This study presents the first comprehensive evaluation of DeepSeek-R1â€“an open-source reasoning modelâ€“against OpenAIâ€™s GPT-4o and GPT-4o-mini. We test the full 671B model and its distilled variants, systematically documenting few-shot learning curves. Our experiments show DeepSeek-R1 achieves a 91.39% F1 score on 5-class sentiment and 99.31% accuracy on binary tasks with just 5 shots, an eightfold improvement in few-shot efficiency over GPT-4o. Architecture-specific distillation effects emerge, where a 32B Qwen2.5-based model outperforms the 70B Llama-based variant by 6.69 percentage points. While its reasoning process reduces throughput, DeepSeek-R1 offers superior explainability via transparent, step-by-step traces, establishing it as a powerful, interpretable open-source alternative. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²ç»æ”¹å˜äº†æƒ…æ„Ÿåˆ†æé¢†åŸŸï¼Œä½†åœ¨å¹³è¡¡å‡†ç¡®æ€§ã€æ•ˆç‡å’Œå¯è§£é‡Šæ€§æ–¹é¢ä»ç„¶æ˜¯ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶é¦–æ¬¡å¯¹DeepSeek-R1è¿™ä¸€å¼€æºæ¨ç†æ¨¡å‹è¿›è¡Œå…¨é¢è¯„ä¼°ï¼Œå¹¶ä¸OpenAIçš„GPT-4oå’ŒGPT-4o-miniè¿›è¡Œå¯¹æ¯”ã€‚æˆ‘ä»¬æµ‹è¯•äº†å®Œæ•´çš„671Bæ¨¡å‹åŠå…¶è’¸é¦å˜ä½“ï¼Œç³»ç»Ÿåœ°è®°å½•äº†å°æ ·æœ¬å­¦ä¹ æ›²çº¿ã€‚å®éªŒè¡¨æ˜ï¼ŒDeepSeek-R1åœ¨5ç±»æƒ…æ„Ÿåˆ†æä¸­è¾¾åˆ°91.39%çš„F1åˆ†æ•°ï¼Œåœ¨äºŒåˆ†ç±»ä»»åŠ¡ä¸Šè¾¾åˆ°99.31%çš„å‡†ç¡®ç‡ï¼Œä»…éœ€5ä¸ªæ ·æœ¬ï¼Œå…¶åœ¨å°æ ·æœ¬æ•ˆç‡ä¸Šæ˜¯GPT-4oçš„å…«å€ã€‚å‡ºç°ç‰¹å®šæ¶æ„çš„è’¸é¦æ•ˆåº”ï¼Œå…¶ä¸­åŸºäº32B Qwen2.5çš„æ¨¡å‹ä¼˜äºåŸºäº70B Llamaçš„å˜ä½“ï¼Œé«˜å‡º6.69ä¸ªç™¾åˆ†ç‚¹ã€‚è™½ç„¶å…¶æ¨ç†è¿‡ç¨‹é™ä½äº†ååé‡ï¼Œä½†DeepSeek-R1é€šè¿‡é€æ˜ã€åˆ†æ­¥è·Ÿè¸ªæä¾›äº†å“è¶Šçš„å¯è§£é‡Šæ€§ï¼Œä½¿å…¶æˆä¸ºå¼ºå¤§ã€å¯è§£é‡Šçš„å¼€æºæ›¿ä»£æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.11655v4">PDF</a> 10 pages, with 2 figures and 6 tables, accepted for publication in an   IEEE Intelligent Systems journal</p>
<p><strong>Summary</strong></p>
<p>DeepSeek-R1æ¨¡å‹åœ¨æƒ…æ„Ÿåˆ†æé¢†åŸŸè¡¨ç°å“è¶Šï¼Œç›¸è¾ƒäºOpenAIçš„GPT-4oç³»åˆ—æ¨¡å‹ï¼Œå…¶åœ¨å°‘é‡æ ·æœ¬ä¸‹çš„å­¦ä¹ æ•ˆç‡æ˜¾è‘—æé«˜ã€‚å®éªŒæ˜¾ç¤ºï¼ŒDeepSeek-R1åœ¨5ç±»æƒ…æ„Ÿåˆ†æä»»åŠ¡ä¸Šè¾¾åˆ°91.39%çš„F1åˆ†æ•°ï¼Œåœ¨äºŒå…ƒä»»åŠ¡ä¸Šè¾¾åˆ°99.31%çš„å‡†ç¡®ç‡ï¼Œä¸”å…·å¤‡å‡ºè‰²çš„å¯è§£é‡Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DeepSeek-R1åœ¨æƒ…æ„Ÿåˆ†æé¢†åŸŸå±•ç°äº†å“è¶Šæ€§èƒ½ã€‚</li>
<li>å¯¹æ¯”OpenAIçš„GPT-4oç³»åˆ—æ¨¡å‹ï¼ŒDeepSeek-R1åœ¨å°‘é‡æ ·æœ¬ä¸‹çš„å­¦ä¹ æ•ˆç‡æ˜¾è‘—æé«˜ã€‚</li>
<li>DeepSeek-R1åœ¨5ç±»æƒ…æ„Ÿåˆ†æä»»åŠ¡ä¸Šè¾¾åˆ°91.39%çš„F1åˆ†æ•°ï¼ŒäºŒå…ƒä»»åŠ¡å‡†ç¡®ç‡é«˜è¾¾99.31%ã€‚</li>
<li>DeepSeek-R1å…·å¤‡å‡ºè‰²çš„å¯è§£é‡Šæ€§ï¼Œé€šè¿‡é€æ­¥è·Ÿè¸ªæä¾›é€æ˜çš„è§£é‡Šè¿‡ç¨‹ã€‚</li>
<li>æ¨¡å‹æ¶æ„å¯¹æ€§èƒ½æœ‰å½±å“ï¼Œä¸€ä¸ªåŸºäº32B Qwen2.5çš„æ¨¡å‹è¡¨ç°ä¼˜äºåŸºäº70B Llamaçš„å˜ä½“ã€‚</li>
<li>å°½ç®¡æ¨ç†è¿‡ç¨‹å¯èƒ½ä¼šå½±å“ååé‡ï¼Œä½†DeepSeek-R1ä»ç„¶æ˜¯ä¸€ä¸ªå¼ºå¤§çš„ã€å¯è§£é‡Šçš„å¼€æºæ›¿ä»£æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.11655">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Few-Shot/2503.11655v4/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Few-Shot/2503.11655v4/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Few-Shot/2503.11655v4/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Few-Shot/2503.11655v4/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Few-Shot/2503.11655v4/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Few-Shot/2503.11655v4/page_5_1.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="HoT-Highlighted-Chain-of-Thought-for-Referencing-Supporting-Facts-from-Inputs"><a href="#HoT-Highlighted-Chain-of-Thought-for-Referencing-Supporting-Facts-from-Inputs" class="headerlink" title="HoT: Highlighted Chain of Thought for Referencing Supporting Facts from   Inputs"></a>HoT: Highlighted Chain of Thought for Referencing Supporting Facts from   Inputs</h2><p><strong>Authors:Tin Nguyen, Logan Bolton, Mohammad Reza Taesiri, Trung Bui, Anh Totti Nguyen</strong></p>
<p>An Achilles heel of Large Language Models (LLMs) is their tendency to hallucinate non-factual statements. A response mixed of factual and non-factual statements poses a challenge for humans to verify and accurately base their decisions on. To combat this problem, we propose Highlighted Chain-of-Thought Prompting (HoT), a technique for prompting LLMs to generate responses with XML tags that ground facts to those provided in the query. That is, given an input question, LLMs would first re-format the question to add XML tags highlighting key facts, and then, generate a response with highlights over the facts referenced from the input. Interestingly, in few-shot settings, HoT outperforms vanilla chain of thought prompting (CoT) on a wide range of 17 tasks from arithmetic, reading comprehension to logical reasoning. When asking humans to verify LLM responses, highlights help time-limited participants to more accurately and efficiently recognize when LLMs are correct. Yet, surprisingly, when LLMs are wrong, HoTs tend to make users believe that an answer is correct. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä¸€ä¸ªå¼±ç‚¹æ˜¯å®ƒä»¬å€¾å‘äºäº§ç”Ÿéäº‹å®æ€§çš„é™ˆè¿°ã€‚ç”±äº‹å®å’Œè™šæ„é™ˆè¿°ç»„æˆçš„å›åº”ç»™äººç±»å¸¦æ¥äº†éªŒè¯å’Œå‡†ç¡®åšå‡ºå†³ç­–çš„æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†å¼ºè°ƒæ€ç»´é“¾æç¤ºï¼ˆHoTï¼‰æŠ€æœ¯ï¼Œè¿™æ˜¯ä¸€ç§æç¤ºLLMç”Ÿæˆå¸¦æœ‰XMLæ ‡ç­¾çš„å“åº”çš„æ–¹æ³•ï¼Œè¿™äº›æ ‡ç­¾åŸºäºæŸ¥è¯¢ä¸­æä¾›çš„äº‹å®ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œç»™å®šä¸€ä¸ªè¾“å…¥é—®é¢˜ï¼ŒLLMä¼šé¦–å…ˆé‡æ–°æ ¼å¼åŒ–é—®é¢˜ï¼Œæ·»åŠ çªå‡ºå…³é”®äº‹å®çš„XMLæ ‡ç­¾ï¼Œç„¶åç”ŸæˆåŒ…å«ä»è¾“å…¥ä¸­å¼•ç”¨çš„é‡ç‚¹äº‹å®çš„å“åº”ã€‚æœ‰è¶£çš„æ˜¯ï¼Œåœ¨å°‘é‡æ ·æœ¬çš„æƒ…å†µä¸‹ï¼ŒHoTåœ¨ç®—æœ¯ã€é˜…è¯»ç†è§£åˆ°é€»è¾‘æ¨ç†ç­‰17é¡¹ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºæ™®é€šçš„æ€ç»´é“¾æç¤ºï¼ˆCoTï¼‰ã€‚å½“è¦æ±‚äººç±»éªŒè¯LLMçš„å“åº”æ—¶ï¼Œé‡ç‚¹æœ‰åŠ©äºæ—¶é—´æœ‰é™çš„å‚ä¸è€…æ›´å‡†ç¡®ã€æ›´é«˜æ•ˆåœ°è¯†åˆ«LLMæ˜¯å¦æ­£ç¡®ã€‚ç„¶è€Œï¼Œä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œå½“LLMé”™è¯¯æ—¶ï¼ŒHoTå¾€å¾€ä½¿ç”¨æˆ·è®¤ä¸ºç­”æ¡ˆæ˜¯æ­£ç¡®çš„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.02003v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä¸€ä¸ªå¼±ç‚¹æ˜¯å®ƒä»¬å®¹æ˜“ç”Ÿæˆéäº‹å®æ€§çš„é™ˆè¿°ï¼Œè¿™å¢åŠ äº†äººç±»éªŒè¯å¹¶æ®æ­¤åšå‡ºå‡†ç¡®å†³ç­–çš„éš¾åº¦ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†é«˜äº®åŒ–æ€ç»´é“¾æç¤ºï¼ˆHoTï¼‰æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯é€šè¿‡XMLæ ‡ç­¾å°†äº‹å®ä¾æ®èå…¥LLMçš„å“åº”ä¸­ã€‚åœ¨å°‘é‡æ ·æœ¬æƒ…å†µä¸‹ï¼ŒHoTåœ¨ç®—æœ¯ã€é˜…è¯»ç†è§£åˆ°é€»è¾‘æ¨ç†ç­‰17é¡¹ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºä¼ ç»Ÿæ€ç»´é“¾æç¤ºï¼ˆCoTï¼‰ã€‚å¯¹äºäººç±»éªŒè¯è€…è€Œè¨€ï¼Œé«˜äº®æç¤ºæœ‰åŠ©äºä»–ä»¬æ›´å‡†ç¡®åœ°å¿«é€Ÿè¯†åˆ«LLMçš„æ­£ç¡®æ€§ã€‚ç„¶è€Œï¼Œæœ‰è¶£çš„æ˜¯ï¼Œå½“LLMå‡ºé”™æ—¶ï¼ŒHoTå¾€å¾€ä½¿ç”¨æˆ·è¯¯ä»¥ä¸ºç­”æ¡ˆæ­£ç¡®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å­˜åœ¨ç”Ÿæˆéäº‹å®æ€§é™ˆè¿°çš„é—®é¢˜ã€‚</li>
<li>Highlighted Chain-of-Thought Promptingï¼ˆHoTï¼‰æŠ€æœ¯é€šè¿‡XMLæ ‡ç­¾å°†äº‹å®ä¾æ®èå…¥LLMçš„å“åº”ä¸­ã€‚</li>
<li>åœ¨å°‘é‡æ ·æœ¬æƒ…å†µä¸‹ï¼ŒHoTåœ¨å¤šç§ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºä¼ ç»Ÿæ€ç»´é“¾æç¤ºï¼ˆCoTï¼‰ã€‚</li>
<li>é«˜äº®æç¤ºæœ‰åŠ©äºäººç±»éªŒè¯è€…æ›´å‡†ç¡®åœ°å¿«é€Ÿè¯†åˆ«LLMçš„æ­£ç¡®æ€§ã€‚</li>
<li>HoTæé«˜äº†LLMå“åº”çš„é€æ˜åº¦ï¼Œä½¿å…¶æ›´å®¹æ˜“ä¸ºäººç±»ç†è§£ã€‚</li>
<li>å½“LLMå‡ºé”™æ—¶ï¼ŒHoTå¯èƒ½å¯¼è‡´ç”¨æˆ·å¯¹å…¶ç­”æ¡ˆçš„è¯¯åˆ¤ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.02003">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Few-Shot/2503.02003v4/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Few-Shot/2503.02003v4/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Few-Shot/2503.02003v4/page_4_0.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-28/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-28/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-28/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_I2I Translation/2412.07772v4/page_0_0.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-28  A co-evolving agentic AI system for medical imaging analysis
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-28
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-28/Agent/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Agent/2509.20900v1/page_1_0.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-28  Interactive Recommendation Agent with Active User Commands
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-28
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">29301k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
