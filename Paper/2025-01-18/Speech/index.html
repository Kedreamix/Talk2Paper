<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-18  PIER A Novel Metric for Evaluating What Matters in Code-Switching">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-6ec41090a2a3cd6a3668b66f99bffe23.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-18
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-18
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    8.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    34 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-01-18-æ›´æ–°"><a href="#2025-01-18-æ›´æ–°" class="headerlink" title="2025-01-18 æ›´æ–°"></a>2025-01-18 æ›´æ–°</h1><h2 id="PIER-A-Novel-Metric-for-Evaluating-What-Matters-in-Code-Switching"><a href="#PIER-A-Novel-Metric-for-Evaluating-What-Matters-in-Code-Switching" class="headerlink" title="PIER: A Novel Metric for Evaluating What Matters in Code-Switching"></a>PIER: A Novel Metric for Evaluating What Matters in Code-Switching</h2><p><strong>Authors:Enes Yavuz Ugan, Ngoc-Quan Pham, Leonard BÃ¤rmann, Alex Waibel</strong></p>
<p>Code-switching, the alternation of languages within a single discourse, presents a significant challenge for Automatic Speech Recognition. Despite the unique nature of the task, performance is commonly measured with established metrics such as Word-Error-Rate (WER). However, in this paper, we question whether these general metrics accurately assess performance on code-switching. Specifically, using both Connectionist-Temporal-Classification and Encoder-Decoder models, we show fine-tuning on non-code-switched data from both matrix and embedded language improves classical metrics on code-switching test sets, although actual code-switched words worsen (as expected). Therefore, we propose Point-of-Interest Error Rate (PIER), a variant of WER that focuses only on specific words of interest. We instantiate PIER on code-switched utterances and show that this more accurately describes the code-switching performance, showing huge room for improvement in future work. This focused evaluation allows for a more precise assessment of model performance, particularly in challenging aspects such as inter-word and intra-word code-switching. </p>
<blockquote>
<p>è¯­è¨€è½¬æ¢ï¼ˆcode-switchingï¼‰æŒ‡çš„æ˜¯åœ¨åŒä¸€è¯­å¢ƒä¸­äº¤æ›¿ä½¿ç”¨ä¸åŒçš„è¯­è¨€ï¼Œè¿™ç»™è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰å¸¦æ¥äº†å¾ˆå¤§çš„æŒ‘æˆ˜ã€‚å°½ç®¡è¯¥ä»»åŠ¡å…·æœ‰ç‹¬ç‰¹æ€§ï¼Œä½†é€šå¸¸ä½¿ç”¨æ ‡å‡†çš„åº¦é‡æ–¹æ³•ï¼ˆå¦‚å•è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ï¼‰æ¥è¯„ä¼°æ€§èƒ½ã€‚ç„¶è€Œï¼Œæœ¬æ–‡å¯¹æ­¤æå‡ºè´¨ç–‘ï¼Œè®¤ä¸ºè¿™äº›é€šç”¨æŒ‡æ ‡æ˜¯å¦èƒ½å‡†ç¡®è¯„ä¼°è¯­è¨€è½¬æ¢æƒ…å†µä¸‹çš„æ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åˆ©ç”¨è¿æ¥æ—¶åºåˆ†ç±»å’Œç¼–ç å™¨-è§£ç å™¨æ¨¡å‹ï¼Œå±•ç¤ºäº†å¯¹æ¥è‡ªçŸ©é˜µå’ŒåµŒå…¥å¼è¯­è¨€çš„éè¯­è¨€è½¬æ¢æ•°æ®çš„å¾®è°ƒèƒ½æé«˜è¯­è¨€è½¬æ¢æµ‹è¯•é›†ä¸Šçš„ç»å…¸æŒ‡æ ‡ï¼Œå°½ç®¡å®é™…çš„è½¬æ¢è¯æœ‰æ‰€æ¶åŒ–ï¼ˆç¬¦åˆé¢„æœŸï¼‰ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†å…´è¶£ç‚¹é”™è¯¯ç‡ï¼ˆPIERï¼‰ï¼Œè¿™æ˜¯WERçš„ä¸€ä¸ªå˜ä½“ï¼Œåªå…³æ³¨ç‰¹å®šçš„å…´è¶£è¯ã€‚æˆ‘ä»¬åœ¨è¯­è¨€è½¬æ¢çš„ç‰‡æ®µä¸Šå®ç°äº†PIERï¼Œå¹¶è¯æ˜å®ƒèƒ½æ›´å‡†ç¡®åœ°æè¿°è¯­è¨€è½¬æ¢çš„æ€§èƒ½ï¼Œæ˜¾ç¤ºå‡ºæœªæ¥å·¥ä½œä¸­æœ‰å¾ˆå¤§çš„æ”¹è¿›ç©ºé—´ã€‚è¿™ç§æœ‰é’ˆå¯¹æ€§çš„è¯„ä¼°å…è®¸å¯¹æ¨¡å‹æ€§èƒ½è¿›è¡Œæ›´ç²¾ç¡®çš„åˆ¤æ–­ï¼Œç‰¹åˆ«æ˜¯åœ¨è¯è¯­é—´å’Œè¯è¯­å†…çš„è¯­è¨€è½¬æ¢ç­‰å…·æœ‰æŒ‘æˆ˜æ€§çš„æ–¹é¢ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.09512v1">PDF</a> Accepted at ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦æ¢è®¨äº†è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ä¸­çš„è¯­è¨€ä»£ç è½¬æ¢é—®é¢˜ã€‚ç ”ç©¶æŒ‡å‡ºä¼ ç»Ÿè¯„ä¼°æŒ‡æ ‡å¦‚è¯é”™è¯¯ç‡ï¼ˆWERï¼‰æ— æ³•å‡†ç¡®è¯„ä¼°ä»£ç åˆ‡æ¢åœºæ™¯ä¸‹çš„æ€§èƒ½ã€‚é€šè¿‡ä½¿ç”¨CTCå’ŒEncoder-Decoderæ¨¡å‹ï¼Œç ”ç©¶å‘ç°åœ¨éä»£ç åˆ‡æ¢æ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒä¼šæé«˜åœ¨ä»£ç åˆ‡æ¢æµ‹è¯•é›†ä¸Šçš„ç»å…¸æŒ‡æ ‡å¾—åˆ†ï¼Œä½†å¯¹å®é™…ä»£ç åˆ‡æ¢å•è¯çš„æ€§èƒ½ä¾ç„¶æœ‰å¾…æé«˜ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„è¯„ä¼°æ–¹æ³•â€”â€”å…³é”®è¯é”™è¯¯ç‡ï¼ˆPIERï¼‰ï¼Œå®ƒèƒ½æ›´å‡†ç¡®åœ°æè¿°ä»£ç åˆ‡æ¢çš„æ€§èƒ½ï¼Œæ˜¾ç¤ºå‡ºæœªæ¥ç ”ç©¶å­˜åœ¨å·¨å¤§æ”¹è¿›ç©ºé—´ã€‚è¯¥è¯„ä¼°æ–¹æ³•æœ‰åŠ©äºæ›´ç²¾ç¡®åœ°è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨è·¨è¯å’Œè¯å†…ä»£ç åˆ‡æ¢ç­‰æŒ‘æˆ˜æ–¹é¢ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»£ç è½¬æ¢åœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ä¸­æ˜¯ä¸€é¡¹é‡è¦æŒ‘æˆ˜ã€‚</li>
<li>ä¼ ç»Ÿè¯„ä¼°æŒ‡æ ‡å¦‚è¯é”™è¯¯ç‡ï¼ˆWERï¼‰æ— æ³•å‡†ç¡®è¯„ä¼°ä»£ç åˆ‡æ¢åœºæ™¯ä¸‹çš„æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>ä½¿ç”¨CTCå’ŒEncoder-Decoderæ¨¡å‹çš„å¾®è°ƒèƒ½æé«˜åœ¨éä»£ç åˆ‡æ¢æ•°æ®ä¸Šçš„æ€§èƒ½ï¼Œä½†åœ¨å®é™…ä»£ç åˆ‡æ¢å•è¯ä¸Šçš„æ€§èƒ½ä»ç„¶æœ‰å¾…æé«˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„è¯„ä¼°æ–¹æ³•â€”â€”å…³é”®è¯é”™è¯¯ç‡ï¼ˆPIERï¼‰ï¼Œèƒ½æ›´å‡†ç¡®åœ°åæ˜ æ¨¡å‹åœ¨ä»£ç åˆ‡æ¢åœºæ™¯ä¸‹çš„æ€§èƒ½ã€‚</li>
<li>PIERè¯„ä¼°æ–¹æ³•ç‰¹åˆ«å…³æ³¨ç‰¹å®šå•è¯çš„é”™è¯¯ï¼Œé€‚ç”¨äºè¯„ä¼°è·¨è¯å’Œè¯å†…ä»£ç åˆ‡æ¢ç­‰æŒ‘æˆ˜æ–¹é¢çš„æ€§èƒ½ã€‚</li>
<li>PIERè¯„ä¼°æ–¹æ³•çš„æå‡ºæ˜¾ç¤ºå‡ºåœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ä¸­çš„ä»£ç è½¬æ¢é—®é¢˜ä¸Šä»å­˜åœ¨å·¨å¤§çš„æ”¹è¿›ç©ºé—´ã€‚</li>
<li>ç²¾ç¡®è¯„ä¼°æ¨¡å‹æ€§èƒ½å¯¹äºè§£å†³è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ä¸­çš„ä»£ç è½¬æ¢é—®é¢˜è‡³å…³é‡è¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.09512">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-59274d75199b99016a6ba80fe1e1de23.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2c187cfffd44e50ad1c3a36a94339a29.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a3dbc2713c789a5ac63ea18ed99c05e1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ec41090a2a3cd6a3668b66f99bffe23.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c15603fc7336fafc052fb7b4b16b17a5.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Multimodal-Marvels-of-Deep-Learning-in-Medical-Diagnosis-A-Comprehensive-Review-of-COVID-19-Detection"><a href="#Multimodal-Marvels-of-Deep-Learning-in-Medical-Diagnosis-A-Comprehensive-Review-of-COVID-19-Detection" class="headerlink" title="Multimodal Marvels of Deep Learning in Medical Diagnosis: A   Comprehensive Review of COVID-19 Detection"></a>Multimodal Marvels of Deep Learning in Medical Diagnosis: A   Comprehensive Review of COVID-19 Detection</h2><p><strong>Authors:Md Shofiqul Islama, Khondokar Fida Hasanc, Hasibul Hossain Shajeebd, Humayan Kabir Ranae, Md Saifur Rahmand, Md Munirul Hasanb, AKM Azadf, Ibrahim Abdullahg, Mohammad Ali Moni</strong></p>
<p>This study presents a comprehensive review of the potential of multimodal deep learning (DL) in medical diagnosis, using COVID-19 as a case example. Motivated by the success of artificial intelligence applications during the COVID-19 pandemic, this research aims to uncover the capabilities of DL in disease screening, prediction, and classification, and to derive insights that enhance the resilience, sustainability, and inclusiveness of science, technology, and innovation systems. Adopting a systematic approach, we investigate the fundamental methodologies, data sources, preprocessing steps, and challenges encountered in various studies and implementations. We explore the architecture of deep learning models, emphasising their data-specific structures and underlying algorithms. Subsequently, we compare different deep learning strategies utilised in COVID-19 analysis, evaluating them based on methodology, data, performance, and prerequisites for future research. By examining diverse data types and diagnostic modalities, this research contributes to scientific understanding and knowledge of the multimodal application of DL and its effectiveness in diagnosis. We have implemented and analysed 11 deep learning models using COVID-19 image, text, and speech (ie, cough) data. Our analysis revealed that the MobileNet model achieved the highest accuracy of 99.97% for COVID-19 image data and 93.73% for speech data (i.e., cough). However, the BiGRU model demonstrated superior performance in COVID-19 text classification with an accuracy of 99.89%. The broader implications of this research suggest potential benefits for other domains and disciplines that could leverage deep learning techniques for image, text, and speech analysis. </p>
<blockquote>
<p>æœ¬ç ”ç©¶å…¨é¢å›é¡¾äº†å¤šæ¨¡æ€æ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰åœ¨åŒ»å­¦è¯Šæ–­ä¸­çš„æ½œåŠ›ï¼Œå¹¶ä»¥COVID-19ä½œä¸ºæ¡ˆä¾‹è¿›è¡Œç ”ç©¶ã€‚æœ¬ç ”ç©¶å—COVID-19ç–«æƒ…æœŸé—´äººå·¥æ™ºèƒ½åº”ç”¨æˆåŠŸçš„å¯å‘ï¼Œæ—¨åœ¨æ­ç¤ºæ·±åº¦å­¦ä¹ åœ¨ç–¾ç—…ç­›æŸ¥ã€é¢„æµ‹å’Œåˆ†ç±»æ–¹é¢çš„èƒ½åŠ›ï¼Œå¹¶å€Ÿæ­¤è·å¾—èƒ½å¤Ÿå¢å¼ºç§‘å­¦ã€æŠ€æœ¯å’Œåˆ›æ–°ç³»ç»ŸéŸ§æ€§ã€å¯æŒç»­æ€§å’ŒåŒ…å®¹æ€§çš„è§è§£ã€‚æˆ‘ä»¬é‡‡ç”¨ç³»ç»Ÿçš„æ–¹æ³•ï¼Œç ”ç©¶äº†å„ç§ç ”ç©¶å’Œå®è·µä¸­çš„åŸºæœ¬æ–¹æ³•ã€æ•°æ®æ¥æºã€é¢„å¤„ç†æ­¥éª¤å’Œæ‰€é¢ä¸´çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬æ¢è®¨äº†æ·±åº¦å­¦ä¹ æ¨¡å‹çš„æ¶æ„ï¼Œé‡ç‚¹ä»‹ç»äº†å®ƒä»¬é’ˆå¯¹æ•°æ®çš„ç‰¹å®šç»“æ„å’ŒåŸºç¡€ç®—æ³•ã€‚éšåï¼Œæˆ‘ä»¬å¯¹COVID-19åˆ†æä¸­ä½¿ç”¨çš„ä¸åŒæ·±åº¦å­¦ä¹ ç­–ç•¥è¿›è¡Œäº†æ¯”è¾ƒï¼Œæ ¹æ®æ–¹æ³•ã€æ•°æ®ã€æ€§èƒ½å’Œæœªæ¥ç ”ç©¶çš„å‰æè¦æ±‚è¿›è¡Œäº†è¯„ä¼°ã€‚æœ¬ç ”ç©¶é€šè¿‡æ£€æŸ¥å¤šç§æ•°æ®ç±»å‹å’Œè¯Šæ–­æ–¹å¼ï¼Œæ¢è®¨äº†æ·±åº¦å­¦ä¹ å¤šæ¨¡æ€åº”ç”¨åœ¨è¯Šæ–­ä¸­çš„ç§‘å­¦ç†è§£å’Œæœ‰æ•ˆæ€§æ–¹é¢çš„è´¡çŒ®ã€‚æˆ‘ä»¬ä½¿ç”¨COVID-19å›¾åƒã€æ–‡æœ¬å’Œè¯­éŸ³ï¼ˆå³å’³å—½å£°ï¼‰æ•°æ®å®ç°äº†11ä¸ªæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œå¹¶å¯¹å…¶è¿›è¡Œäº†åˆ†æã€‚åˆ†æç»“æœæ˜¾ç¤ºï¼ŒMobileNetæ¨¡å‹åœ¨COVID-19å›¾åƒæ•°æ®ä¸Šè¾¾åˆ°äº†99.97%çš„å‡†ç¡®ç‡ï¼Œåœ¨è¯­éŸ³æ•°æ®ï¼ˆå³å’³å—½å£°ï¼‰ä¸Šè¾¾åˆ°äº†93.73%çš„å‡†ç¡®ç‡ã€‚ç„¶è€Œï¼ŒBiGRUæ¨¡å‹åœ¨COVID-19æ–‡æœ¬åˆ†ç±»æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œå‡†ç¡®ç‡ä¸º99.89%ã€‚è¿™é¡¹ç ”ç©¶çš„æ›´å¹¿æ³›æ„ä¹‰è¡¨æ˜ï¼Œå…¶ä»–é¢†åŸŸå’Œå­¦ç§‘ä¹Ÿæœ‰å¯èƒ½å—ç›Šäºæ·±åº¦å­¦ä¹ æŠ€æœ¯åœ¨å›¾åƒã€æ–‡æœ¬å’Œè¯­éŸ³åˆ†ææ–¹é¢çš„åº”ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.09506v1">PDF</a> 43 pages</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡å…¨é¢ç»¼è¿°äº†å¤šæ¨¡æ€æ·±åº¦å­¦ä¹ åœ¨åŒ»å­¦è¯Šæ–­ä¸­çš„æ½œåŠ›ï¼Œä»¥COVID-19ä¸ºä¾‹ã€‚è¯¥ç ”ç©¶æ—¨åœ¨æ­ç¤ºæ·±åº¦å­¦ä¹ åœ¨ç–¾ç—…ç­›æŸ¥ã€é¢„æµ‹å’Œåˆ†ç±»æ–¹é¢çš„èƒ½åŠ›ï¼Œå¹¶å€Ÿæ­¤å¢å¼ºç§‘å­¦ã€æŠ€æœ¯å’Œåˆ›æ–°ç³»ç»Ÿçš„å¤åŸåŠ›ã€å¯æŒç»­æ€§å’ŒåŒ…å®¹æ€§ã€‚ç ”ç©¶é‡‡ç”¨ç³»ç»Ÿæ–¹æ³•ï¼Œæ¢è®¨äº†æ ¹æœ¬çš„æ–¹æ³•è®ºã€æ•°æ®æ¥æºã€é¢„å¤„ç†æ­¥éª¤ä»¥åŠåœ¨ä¸åŒç ”ç©¶å’Œå®è·µä¸­çš„æŒ‘æˆ˜ã€‚æœ¬æ–‡æ¢è®¨äº†æ·±åº¦å­¦ä¹ æ¨¡å‹çš„æ¶æ„ï¼Œé‡ç‚¹ä»‹ç»äº†å…¶æ•°æ®ç‰¹å®šç»“æ„å’ŒåŸºç¡€ç®—æ³•ã€‚é€šè¿‡è€ƒå¯Ÿä¸åŒç±»å‹çš„æ•°æ®å’Œè¯Šæ–­æ–¹å¼ï¼Œæœ¬ç ”ç©¶æœ‰åŠ©äºç†è§£å¤šæ¨¡æ€æ·±åº¦å­¦ä¹ åœ¨è¯Šæ–­ä¸­çš„æœ‰æ•ˆæ€§ã€‚ç ”ç©¶å‘ç°ï¼ŒMobileNetæ¨¡å‹åœ¨COVID-19å›¾åƒå’Œè¯­éŸ³æ•°æ®ä¸Šçš„å‡†ç¡®ç‡æœ€é«˜ï¼Œåˆ†åˆ«ä¸º99.97%å’Œ93.73%ï¼Œè€ŒBiGRUæ¨¡å‹åœ¨COVID-19æ–‡æœ¬åˆ†ç±»ä¸­è¡¨ç°æœ€ä½³ï¼Œå‡†ç¡®ç‡ä¸º99.89%ã€‚æ­¤ç ”ç©¶å¯¹å…¶ä»–é¢†åŸŸå’Œå­¦ç§‘å…·æœ‰æ½œåœ¨åº”ç”¨ä»·å€¼ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç ”ç©¶å…¨é¢å›é¡¾äº†å¤šæ¨¡æ€æ·±åº¦å­¦ä¹ åœ¨åŒ»å­¦è¯Šæ–­ä¸­çš„æ½œåŠ›ï¼Œä»¥COVID-19ä¸ºæ¡ˆä¾‹è¿›è¡Œè¯¦ç»†åˆ†æã€‚</li>
<li>æ·±åº¦å­¦ä¹ åœ¨ç–¾ç—…ç­›æŸ¥ã€é¢„æµ‹å’Œåˆ†ç±»æ–¹é¢å±•ç°å‡ºå¼ºå¤§èƒ½åŠ›ï¼Œæœ‰åŠ©äºå¢å¼ºç§‘å­¦ç³»ç»Ÿçš„å¤åŸåŠ›ã€å¯æŒç»­æ€§å’ŒåŒ…å®¹æ€§ã€‚</li>
<li>ç ”ç©¶æ·±å…¥æ¢è®¨äº†æ·±åº¦å­¦ä¹ æ¨¡å‹çš„åŸºç¡€æ¶æ„å’Œæ•°æ®ç‰¹å®šç»“æ„ï¼Œå¼ºè°ƒäº†å…¶åœ¨å¤„ç†ä¸åŒæ•°æ®ç±»å‹ï¼ˆå¦‚å›¾åƒã€æ–‡æœ¬å’Œè¯­éŸ³ï¼‰æ—¶çš„ä¼˜åŠ¿ã€‚</li>
<li>MobileNetæ¨¡å‹åœ¨COVID-19å›¾åƒå’Œè¯­éŸ³æ•°æ®å‡†ç¡®ç‡é«˜ï¼Œè€ŒBiGRUæ¨¡å‹åœ¨æ–‡æœ¬åˆ†ç±»ä¸­è¡¨ç°æœ€ä½³ã€‚</li>
<li>ç ”ç©¶é€šè¿‡ç³»ç»Ÿæ–¹æ³•ï¼Œæ¢è®¨äº†æ·±åº¦å­¦ä¹ åœ¨å®è·µä¸­çš„åº”ç”¨æŒ‘æˆ˜å’Œæœªæ¥å‘å±•æ‰€éœ€çš„å‰ææ¡ä»¶ã€‚</li>
<li>æ­¤ç ”ç©¶ä¸ä»…å±€é™äºCOVID-19çš„åŒ»å­¦è¯Šæ–­ï¼Œå¯¹å…¶ä»–é¢†åŸŸå’Œå­¦ç§‘çš„æ·±åº¦å­¦ä¹ åº”ç”¨ä¹Ÿæä¾›äº†æœ‰ä»·å€¼çš„å‚è€ƒã€‚</li>
<li>ç ”ç©¶æˆæœæœ‰åŠ©äºæå‡å¯¹å¤šæ¨¡æ€æ·±åº¦å­¦ä¹ åº”ç”¨çš„ç†è§£å’ŒçŸ¥è¯†è´¡çŒ®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.09506">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-74f86bf3b7dd3d3932c7ba39bb10090c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8252398af67b74141fcbd6291b255a1d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-abcb8d99a7f8560b0bf3c11d50be1e77.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-28e5bcced15d881ab182c2f4028cd717.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e3a16057f105f4272461c74284394bb1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-164a52d64536f7d41479e4d42e09553f.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Teaching-Wav2Vec2-the-Language-of-the-Brain"><a href="#Teaching-Wav2Vec2-the-Language-of-the-Brain" class="headerlink" title="Teaching Wav2Vec2 the Language of the Brain"></a>Teaching Wav2Vec2 the Language of the Brain</h2><p><strong>Authors:Tobias Fiedler, Leon Hermann, Florian MÃ¼ller, Sarel Cohen, Peter Chin, Tobias Friedrich, Eilon Vaadia</strong></p>
<p>The decoding of continuously spoken speech from neuronal activity has the potential to become an important clinical solution for paralyzed patients. Deep Learning Brain Computer Interfaces (BCIs) have recently successfully mapped neuronal activity to text contents in subjects who attempted to formulate speech. However, only small BCI datasets are available. In contrast, labeled data and pre-trained models for the closely related task of speech recognition from audio are widely available. One such model is Wav2Vec2 which has been trained in a self-supervised fashion to create meaningful representations of speech audio data. In this study, we show that patterns learned by Wav2Vec2 are transferable to brain data. Specifically, we replace its audio feature extractor with an untrained Brain Feature Extractor (BFE) model. We then execute full fine-tuning with pre-trained weights for Wav2Vec2, training â€˜â€™from scratchâ€™â€™ without pre-trained weights as well as freezing a pre-trained Wav2Vec2 and training only the BFE each for 45 different BFE architectures. Across these experiments, the best run is from full fine-tuning with pre-trained weights, achieving a Character Error Rate (CER) of 18.54%, outperforming the best training from scratch run by 20.46% and that of frozen Wav2Vec2 training by 15.92% percentage points. These results indicate that knowledge transfer from audio speech recognition to brain decoding is possible and significantly improves brain decoding performance for the same architectures. Related source code is available at <a target="_blank" rel="noopener" href="https://github.com/tfiedlerdev/Wav2Vec2ForBrain">https://github.com/tfiedlerdev/Wav2Vec2ForBrain</a>. </p>
<blockquote>
<p>ä»ç¥ç»å…ƒæ´»åŠ¨è§£ç è¿ç»­è¯­éŸ³å…·æœ‰æˆä¸ºç˜«ç—ªæ‚£è€…é‡è¦ä¸´åºŠè§£å†³æ–¹æ¡ˆçš„æ½œåŠ›ã€‚æ·±åº¦å­¦ä¹ è„‘æœºæ¥å£ï¼ˆBCIsï¼‰æœ€è¿‘æˆåŠŸåœ°å°†ç¥ç»å…ƒæ´»åŠ¨æ˜ å°„åˆ°å°è¯•è¿›è¡Œè¯­éŸ³çš„ä¸»ä½“æ–‡æœ¬å†…å®¹ä¸­ã€‚ç„¶è€Œï¼Œå¯ç”¨çš„BCIæ•°æ®é›†å¾ˆå°ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œä¸ä»éŸ³é¢‘è¿›è¡Œè¯­éŸ³è¯†åˆ«è¿™ä¸€ç´§å¯†ç›¸å…³çš„ä»»åŠ¡ç›¸å…³çš„æ ‡è®°æ•°æ®å’Œé¢„è®­ç»ƒæ¨¡å‹å´éšå¤„å¯è§ã€‚å…¶ä¸­ä¸€ç§æ¨¡å‹æ˜¯Wav2Vec2ï¼Œå®ƒé‡‡ç”¨è‡ªæˆ‘ç›‘ç£çš„æ–¹å¼è¿›è¡Œè®­ç»ƒï¼Œèƒ½å¤Ÿåˆ›å»ºè¯­éŸ³éŸ³é¢‘æ•°æ®çš„æœ‰æ„ä¹‰è¡¨ç¤ºã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†Wav2Vec2æ‰€å­¦ä¹ åˆ°çš„æ¨¡å¼å¯ä»¥è½¬ç§»åˆ°è„‘æ•°æ®ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ç”¨æœªç»è®­ç»ƒçš„è„‘ç‰¹å¾æå–å™¨ï¼ˆBFEï¼‰æ¨¡å‹æ›¿æ¢äº†å…¶éŸ³é¢‘ç‰¹å¾æå–å™¨ã€‚ç„¶åæˆ‘ä»¬å¯¹Wav2Vec2è¿›è¡Œé¢„è®­ç»ƒæƒé‡çš„å…¨é¢å¾®è°ƒï¼ŒåŒæ—¶è®­ç»ƒâ€œä»å¤´å¼€å§‹â€çš„æ¨¡å‹ï¼Œä»¥åŠå†»ç»“é¢„è®­ç»ƒçš„Wav2Vec2å¹¶ä»…å¯¹BFEè¿›è¡Œè®­ç»ƒï¼Œå…±å°è¯•äº†45ç§ä¸åŒçš„BFEæ¶æ„ã€‚åœ¨è¿™äº›å®éªŒä¸­ï¼Œæœ€å¥½çš„ç»“æœæ˜¯ä½¿ç”¨é¢„è®­ç»ƒæƒé‡è¿›è¡Œçš„å…¨å¾®è°ƒï¼Œå­—ç¬¦é”™è¯¯ç‡ï¼ˆCERï¼‰è¾¾åˆ°18.54%ï¼Œæ¯”ä»å¤´å¼€å§‹è®­ç»ƒçš„æœ€å¥½æˆç»©é«˜å‡º20.46%ï¼Œæ¯”å†»ç»“çš„Wav2Vec2è®­ç»ƒé«˜å‡º15.92ä¸ªç™¾åˆ†ç‚¹ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œä»è¯­éŸ³è¯†åˆ«åˆ°å¤§è„‘è§£ç çš„çŸ¥è¯†è½¬ç§»æ˜¯å¯èƒ½çš„ï¼Œå¹¶æ˜¾è‘—æé«˜äº†ç›¸åŒæ¶æ„çš„å¤§è„‘è§£ç æ€§èƒ½ã€‚ç›¸å…³æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/tfiedlerdev/Wav2Vec2ForBrain%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/tfiedlerdev/Wav2Vec2ForBrainæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.09459v1">PDF</a> Paper was submitted to ICASSP 2025 but marginally rejected</p>
<p><strong>æ‘˜è¦</strong></p>
<p>åŸºäºæ·±åº¦å­¦ä¹ çš„å¤§è„‘è®¡ç®—æœºæ¥å£ï¼ˆBCIsï¼‰å¯ä»ç¥ç»å…ƒæ´»åŠ¨ä¸­è§£ç è¿ç»­å£è¯­ï¼Œä¸ºç˜«ç—ªæ‚£è€…å¸¦æ¥é‡è¦çš„ä¸´åºŠè§£å†³æ–¹æ¡ˆã€‚æœ¬ç ”ç©¶åˆ©ç”¨Wav2Vec2æ¨¡å‹ï¼Œä¸€ç§ç»è¿‡è‡ªæˆ‘ç›‘ç£è®­ç»ƒçš„è¯­éŸ³éŸ³é¢‘æ•°æ®è¡¨ç¤ºæ–¹æ³•ï¼Œå±•ç¤ºå…¶ä»éŸ³é¢‘åˆ°å¤§è„‘çš„çŸ¥è¯†çš„å¯è¿ç§»æ€§ã€‚é€šè¿‡å°†Wav2Vec2çš„éŸ³é¢‘ç‰¹å¾æå–å™¨æ›¿æ¢ä¸ºæœªè®­ç»ƒçš„å¤§è„‘ç‰¹å¾æå–å™¨ï¼ˆBFEï¼‰æ¨¡å‹ï¼Œå¹¶è¿›è¡Œå…¨é¢å¾®è°ƒï¼Œå®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨é¢„è®­ç»ƒæƒé‡çš„å…¨å¾®è°ƒæ–¹å¼è¡¨ç°æœ€ä½³ï¼Œå­—ç¬¦é”™è¯¯ç‡ï¼ˆCERï¼‰è¾¾åˆ°18.54%ï¼Œç›¸è¾ƒäºä»å¤´å¼€å§‹è®­ç»ƒå’Œå†»ç»“Wav2Vec2ä»…è®­ç»ƒBFEçš„æ–¹å¼ï¼Œåˆ†åˆ«æé«˜äº†20.46%å’Œ15.92%ã€‚è¿™è¡¨æ˜è¯­éŸ³è¯†åˆ«çš„çŸ¥è¯†å¯ä»¥è¿ç§»åˆ°å¤§è„‘è§£ç ä¸­ï¼Œå¹¶æ˜¾è‘—æé«˜è§£ç æ€§èƒ½ã€‚ç›¸å…³æºä»£ç å·²å…¬å¼€ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç ”ç©¶æ¢ç´¢äº†å°†æ·±åº¦å­¦ä¹ ä¸å¤§è„‘è®¡ç®—æœºæ¥å£ï¼ˆBCIsï¼‰ç»“åˆçš„æ–¹æ³•ï¼Œä½¿ä»ç¥ç»å…ƒæ´»åŠ¨ä¸­è§£ç è¿ç»­å£è¯­æˆä¸ºå¯èƒ½ã€‚</li>
<li>åˆ©ç”¨Wav2Vec2æ¨¡å‹ï¼Œä¸€ä¸ªç»è¿‡è‡ªæˆ‘ç›‘ç£è®­ç»ƒçš„è¯­éŸ³éŸ³é¢‘æ•°æ®è¡¨ç¤ºæ–¹æ³•ï¼Œåœ¨å¤§è„‘è§£ç ä»»åŠ¡ä¸­å±•ç¤ºäº†å…¶æœ‰æ•ˆæ€§ã€‚</li>
<li>ç ”ç©¶é€šè¿‡æ›¿æ¢Wav2Vec2çš„éŸ³é¢‘ç‰¹å¾æå–å™¨ä¸ºå¤§è„‘ç‰¹å¾æå–å™¨ï¼ˆBFEï¼‰æ¨¡å‹ï¼Œå®ç°äº†çŸ¥è¯†çš„è¿ç§»å­¦ä¹ ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºå…¨å¾®è°ƒæ–¹å¼è¡¨ç°æœ€ä½³ï¼Œå­—ç¬¦é”™è¯¯ç‡ï¼ˆCERï¼‰è¾¾åˆ°18.54%ã€‚</li>
<li>ä¸ä»å¤´å¼€å§‹è®­ç»ƒå’Œå†»ç»“Wav2Vec2ä»…è®­ç»ƒBFEçš„æ–¹å¼ç›¸æ¯”ï¼Œä½¿ç”¨é¢„è®­ç»ƒæƒé‡çš„å…¨å¾®è°ƒæ–¹å¼æ˜¾è‘—æé«˜äº†è§£ç æ€§èƒ½ã€‚</li>
<li>çŸ¥è¯†ä»è¯­éŸ³è¯†åˆ«è¿ç§»åˆ°å¤§è„‘è§£ç æ˜¯å¯èƒ½çš„ï¼Œè¿™æœ‰åŠ©äºä¸ºç˜«ç—ªæ‚£è€…æä¾›ä¸´åºŠè§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.09459">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0b8488298f3e19f38787ccf7f6c94d12.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b191f7ac164867bcd496f0d9369b415d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1c0cca2b39d386bf3ea6cf0a909c3a05.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4a48d76fd897c7c7408cc9a5d280ef0e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9adc867c41cb1d778051bd50b0cceb02.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Delayed-Fusion-Integrating-Large-Language-Models-into-First-Pass-Decoding-in-End-to-end-Speech-Recognition"><a href="#Delayed-Fusion-Integrating-Large-Language-Models-into-First-Pass-Decoding-in-End-to-end-Speech-Recognition" class="headerlink" title="Delayed Fusion: Integrating Large Language Models into First-Pass   Decoding in End-to-end Speech Recognition"></a>Delayed Fusion: Integrating Large Language Models into First-Pass   Decoding in End-to-end Speech Recognition</h2><p><strong>Authors:Takaaki Hori, Martin Kocour, Adnan Haider, Erik McDermott, Xiaodan Zhuang</strong></p>
<p>This paper presents an efficient decoding approach for end-to-end automatic speech recognition (E2E-ASR) with large language models (LLMs). Although shallow fusion is the most common approach to incorporate language models into E2E-ASR decoding, we face two practical problems with LLMs. (1) LLM inference is computationally costly. (2) There may be a vocabulary mismatch between the ASR model and the LLM. To resolve this mismatch, we need to retrain the ASR model and&#x2F;or the LLM, which is at best time-consuming and in many cases not feasible. We propose â€œdelayed fusion,â€ which applies LLM scores to ASR hypotheses with a delay during decoding and enables easier use of pre-trained LLMs in ASR tasks. This method can reduce not only the number of hypotheses scored by the LLM but also the number of LLM inference calls. It also allows re-tokenizion of ASR hypotheses during decoding if ASR and LLM employ different tokenizations. We demonstrate that delayed fusion provides improved decoding speed and accuracy compared to shallow fusion and N-best rescoring using the LibriHeavy ASR corpus and three public LLMs, OpenLLaMA 3B &amp; 7B and Mistral 7B. </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œç«¯åˆ°ç«¯è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆE2E-ASRï¼‰çš„é«˜æ•ˆè§£ç æ–¹æ³•ã€‚å°½ç®¡æµ…èåˆæ˜¯å°†è¯­è¨€æ¨¡å‹èå…¥E2E-ASRè§£ç çš„æœ€å¸¸è§æ–¹æ³•ï¼Œä½†åœ¨ä½¿ç”¨LLMsæ—¶ï¼Œæˆ‘ä»¬é¢ä¸´ä¸¤ä¸ªå®é™…é—®é¢˜ã€‚(1) LLMæ¨ç†è®¡ç®—æˆæœ¬é«˜æ˜‚ã€‚(2) ASRæ¨¡å‹å’ŒLLMä¹‹é—´å¯èƒ½å­˜åœ¨è¯æ±‡ä¸åŒ¹é…çš„é—®é¢˜ã€‚ä¸ºè§£å†³è¿™ä¸€ä¸åŒ¹é…é—®é¢˜ï¼Œæˆ‘ä»¬éœ€è¦é‡æ–°è®­ç»ƒASRæ¨¡å‹æˆ–LLMï¼Œè¿™è‡³å°‘å¾ˆè€—æ—¶ï¼Œå¹¶ä¸”åœ¨è®¸å¤šæƒ…å†µä¸‹å¹¶ä¸å¯è¡Œã€‚æˆ‘ä»¬æå‡ºäº†â€œå»¶è¿Ÿèåˆâ€çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨è§£ç è¿‡ç¨‹ä¸­å»¶è¿Ÿå°†LLMåˆ†æ•°åº”ç”¨äºASRå‡è®¾ï¼Œä½¿å¾—åœ¨ASRä»»åŠ¡ä¸­æ›´å®¹æ˜“ä½¿ç”¨é¢„è®­ç»ƒçš„LLMã€‚è¿™ç§æ–¹æ³•ä¸ä»…å¯ä»¥å‡å°‘LLMè¯„åˆ†çš„å‡è®¾æ•°é‡ï¼Œè¿˜å¯ä»¥å‡å°‘LLMæ¨ç†è°ƒç”¨æ¬¡æ•°ã€‚å¦‚æœASRå’ŒLLMé‡‡ç”¨ä¸åŒçš„æ ‡è®°åŒ–æ–¹å¼ï¼Œå®ƒè¿˜å¯ä»¥åœ¨è§£ç è¿‡ç¨‹ä¸­é‡æ–°æ ‡è®°ASRå‡è®¾ã€‚æˆ‘ä»¬åœ¨LibriHeavy ASRè¯­æ–™åº“å’Œä¸‰ä¸ªå…¬å…±LLMï¼ˆOpenLLaMA 3B &amp; 7Bå’ŒMistral 7Bï¼‰ä¸Šæ¼”ç¤ºäº†å»¶è¿Ÿèåˆç›¸æ¯”æµ…èåˆå’ŒN-besté‡è¯„åˆ†æä¾›äº†æ›´é«˜çš„è§£ç é€Ÿåº¦å’Œå‡†ç¡®æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.09258v1">PDF</a> Accepted to ICASSP2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§é«˜æ•ˆçš„è§£ç æ–¹æ³•ï¼Œç”¨äºç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œç«¯åˆ°ç«¯è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆE2E-ASRï¼‰ã€‚ä¼ ç»Ÿçš„æµ…èåˆæ–¹æ³•é¢ä¸´è®¡ç®—æˆæœ¬é«˜å’Œè¯æ±‡ä¸åŒ¹é…çš„é—®é¢˜ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†å»¶è¿Ÿèåˆç­–ç•¥ï¼Œåœ¨è§£ç è¿‡ç¨‹ä¸­å»¶è¿Ÿåº”ç”¨LLMè¯„åˆ†ï¼Œå‡å°‘äº†LLMè¯„åˆ†å‡è®¾æ•°é‡å’Œæ¨ç†è°ƒç”¨æ¬¡æ•°ï¼Œå¹¶å…è®¸åœ¨è§£ç è¿‡ç¨‹ä¸­è¿›è¡ŒASRå‡è®¾çš„é‡æ ‡è®°åŒ–ã€‚å®éªŒè¡¨æ˜ï¼Œå»¶è¿Ÿèåˆç›¸è¾ƒäºæµ…èåˆå’ŒN-besté‡è¯„åˆ†æ–¹æ³•ï¼Œèƒ½æé«˜è§£ç é€Ÿåº¦å’Œå‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„è§£ç æ–¹æ³•ï¼Œç”¨äºå°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸ç«¯åˆ°ç«¯è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆE2E-ASRï¼‰ç³»ç»Ÿç»“åˆã€‚</li>
<li>ä¼ ç»Ÿæµ…èåˆæ–¹æ³•å­˜åœ¨è®¡ç®—æˆæœ¬é«˜å’Œè¯æ±‡ä¸åŒ¹é…çš„é—®é¢˜ã€‚</li>
<li>å»¶è¿Ÿèåˆç­–ç•¥èƒ½å¤Ÿå‡å°‘LLMè¯„åˆ†çš„å‡è®¾æ•°é‡å’Œæ¨ç†è°ƒç”¨æ¬¡æ•°ã€‚</li>
<li>å»¶è¿Ÿèåˆå…è®¸åœ¨è§£ç è¿‡ç¨‹ä¸­è¿›è¡ŒASRå‡è®¾çš„é‡æ ‡è®°åŒ–ï¼Œä»¥é€‚åº”ä¸åŒçš„è¯æ±‡è¡¨æˆ–æ ‡è®°åŒ–æ–¹å¼ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œå»¶è¿Ÿèåˆç­–ç•¥ç›¸è¾ƒäºä¼ ç»Ÿçš„æµ…èåˆå’ŒN-besté‡è¯„åˆ†æ–¹æ³•ï¼Œå…·æœ‰æ›´é«˜çš„è§£ç é€Ÿåº¦å’Œå‡†ç¡®æ€§ã€‚</li>
<li>è®ºæ–‡ä½¿ç”¨äº†LibriHeavy ASRè¯­æ–™åº“å’Œä¸‰ä¸ªå…¬å¼€çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆOpenLLaMA 3B &amp; 7Bä»¥åŠMistral 7Bï¼‰è¿›è¡ŒéªŒè¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.09258">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-d744c92cdf58011bf73d9863ab90e0e5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4cf1562d48bd6b4f8e6bc4b44db7f7cc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f3dd404a74cbfe3200f15cbe9311a0ae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c5ea93d67a01b55feaeda942a62fc245.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-95118164814a5a8d895167648bc0d098.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Beyond-Speaker-Identity-Text-Guided-Target-Speech-Extraction"><a href="#Beyond-Speaker-Identity-Text-Guided-Target-Speech-Extraction" class="headerlink" title="Beyond Speaker Identity: Text Guided Target Speech Extraction"></a>Beyond Speaker Identity: Text Guided Target Speech Extraction</h2><p><strong>Authors:Mingyue Huo, Abhinav Jain, Cong Phuoc Huynh, Fanjie Kong, Pichao Wang, Zhu Liu, Vimal Bhat</strong></p>
<p>Target Speech Extraction (TSE) traditionally relies on explicit clues about the speakerâ€™s identity like enrollment audio, face images, or videos, which may not always be available. In this paper, we propose a text-guided TSE model StyleTSE that uses natural language descriptions of speaking style in addition to the audio clue to extract the desired speech from a given mixture. Our model integrates a speech separation network adapted from SepFormer with a bi-modality clue network that flexibly processes both audio and text clues. To train and evaluate our model, we introduce a new dataset TextrolMix with speech mixtures and natural language descriptions. Experimental results demonstrate that our method effectively separates speech based not only on who is speaking, but also on how they are speaking, enhancing TSE in scenarios where traditional audio clues are absent. Demos are at: <a target="_blank" rel="noopener" href="https://mingyue66.github.io/TextrolMix/demo/">https://mingyue66.github.io/TextrolMix/demo/</a> </p>
<blockquote>
<p>ä¼ ç»Ÿç›®æ ‡è¯­éŸ³æå–ï¼ˆTSEï¼‰ä¾èµ–äºå…³äºè¯´è¯äººèº«ä»½çš„æ˜ç¡®çº¿ç´¢ï¼Œå¦‚æ³¨å†ŒéŸ³é¢‘ã€é¢éƒ¨å›¾åƒæˆ–è§†é¢‘ï¼Œä½†è¿™äº›å¯èƒ½å¹¶ä¸æ€»æ˜¯å¯ç”¨ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–‡æœ¬å¼•å¯¼çš„TSEæ¨¡å‹StyleTSEï¼Œè¯¥æ¨¡å‹ä½¿ç”¨å¯¹è¯´è¯é£æ ¼çš„è‡ªç„¶è¯­è¨€æè¿°ä»¥åŠéŸ³é¢‘çº¿ç´¢ï¼Œä»ç»™å®šçš„æ··åˆè¯­éŸ³ä¸­æå–æ‰€éœ€çš„è¯­éŸ³ã€‚æˆ‘ä»¬çš„æ¨¡å‹å°†åŸºäºSepFormerçš„è¯­éŸ³åˆ†ç¦»ç½‘ç»œä¸åŒæ¨¡æ€çº¿ç´¢ç½‘ç»œç›¸ç»“åˆï¼Œè¯¥ç½‘ç»œå¯ä»¥çµæ´»å¤„ç†éŸ³é¢‘å’Œæ–‡æœ¬çº¿ç´¢ã€‚ä¸ºäº†è®­ç»ƒå’Œè¯„ä¼°æˆ‘ä»¬çš„æ¨¡å‹ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†TextrolMixï¼Œè¯¥æ•°æ®é›†åŒ…å«æ··åˆè¯­éŸ³å’Œè‡ªç„¶è¯­è¨€æè¿°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…æœ‰æ•ˆåœ°æ ¹æ®â€œè°åœ¨è¯´è¯â€è¿›è¡Œè¯­éŸ³åˆ†ç¦»ï¼Œè¿˜æ ¹æ®â€œä»–ä»¬å¦‚ä½•è¯´è¯â€è¿›è¡Œè¯­éŸ³åˆ†ç¦»ï¼Œå¢å¼ºäº†åœ¨ç¼ºå°‘ä¼ ç»ŸéŸ³é¢‘çº¿ç´¢çš„åœºæ™¯ä¸‹çš„TSEã€‚ç›¸å…³æ¼”ç¤ºè¯·è®¿é—®ï¼š[<a target="_blank" rel="noopener" href="https://mingyue66.github.io/TextrolMix/demo/]">https://mingyue66.github.io/TextrolMix/demo/]</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.09169v1">PDF</a> Accepted by ICASSP 2025</p>
<p><strong>Summary</strong><br>ç›®æ ‡è¯­éŸ³æå–ï¼ˆTSEï¼‰é€šå¸¸ä¾èµ–äºè¯´è¯äººèº«ä»½çš„æ˜ç¡®çº¿ç´¢ï¼Œå¦‚æ³¨å†ŒéŸ³é¢‘ã€é¢éƒ¨å›¾åƒæˆ–è§†é¢‘ï¼Œä½†è¿™äº›ä¿¡æ¯å¹¶éæ€»æ˜¯å¯ç”¨ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–‡æœ¬å¼•å¯¼çš„ç›®æ ‡è¯­éŸ³æå–æ¨¡å‹StyleTSEï¼Œå®ƒä½¿ç”¨éŸ³é¢‘çº¿ç´¢ä¹‹å¤–çš„æ–‡æœ¬æè¿°çš„è¯´è¯é£æ ¼æ¥ä»ç»™å®šçš„æ··åˆå£°éŸ³ä¸­æå–æ‰€éœ€çš„è¯­éŸ³ã€‚æ¨¡å‹ç»“åˆäº†SepFormeræ”¹è¿›çš„è‡ªé€‚åº”è¯­éŸ³åˆ†ç¦»ç½‘ç»œå’ŒåŒæ¨¡æ€çº¿ç´¢ç½‘ç»œï¼Œå¯ä»¥çµæ´»å¤„ç†éŸ³é¢‘å’Œæ–‡æœ¬çº¿ç´¢ã€‚ä¸ºäº†è®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹ï¼Œå¼•å…¥äº†æ–°æ•°æ®é›†TextrolMixåŒ…å«æ··åˆè¯­éŸ³å’Œæ–‡æœ¬æè¿°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¸ä»…åŸºäºè¯´è¯äººèº«ä»½ï¼Œè¿˜åŸºäºè¯´è¯æ–¹å¼æœ‰æ•ˆåœ°åˆ†ç¦»è¯­éŸ³ï¼Œæé«˜äº†åœ¨æ²¡æœ‰ä¼ ç»ŸéŸ³é¢‘çº¿ç´¢çš„åœºæ™¯ä¸‹çš„TSEæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>StyleTSEæ¨¡å‹ç»“åˆäº†è¯­éŸ³åˆ†ç¦»ç½‘ç»œå’ŒåŒæ¨¡æ€çº¿ç´¢ç½‘ç»œï¼Œä»¥å¤„ç†éŸ³é¢‘å’Œæ–‡æœ¬æè¿°çš„è¯´è¯é£æ ¼ã€‚</li>
<li>æ¨¡å‹ä½¿ç”¨è‡ªç„¶è¯­è¨€çš„è¯´è¯é£æ ¼æè¿°ä½œä¸ºæå–ç›®æ ‡è¯­éŸ³çš„é¢å¤–çº¿ç´¢ã€‚</li>
<li>å¼•å…¥æ–°æ•°æ®é›†TextrolMixç”¨äºè®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒStyleTSEèƒ½å¤Ÿåœ¨æ— ä¼ ç»ŸéŸ³é¢‘çº¿ç´¢çš„åœºæ™¯ä¸‹æœ‰æ•ˆåˆ†ç¦»è¯­éŸ³ã€‚</li>
<li>è¯¥æ–¹æ³•ä¸ä»…èƒ½æ ¹æ®è¯´è¯äººèº«ä»½åˆ†ç¦»è¯­éŸ³ï¼Œè¿˜èƒ½æ ¹æ®è¯´è¯æ–¹å¼åˆ†ç¦»è¯­éŸ³ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.09169">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-a512c54fa0008851d93f9ddf27671dde.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-678d35e67f0355c33ceb66b853f5b61e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ac36a0956e8ae263d4541e8eacfadb37.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-872034ccab8bb85311c77a4135f087cc.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="persoDA-Personalized-Data-Augmentation-forPersonalized-ASR"><a href="#persoDA-Personalized-Data-Augmentation-forPersonalized-ASR" class="headerlink" title="persoDA: Personalized Data Augmentation forPersonalized ASR"></a>persoDA: Personalized Data Augmentation forPersonalized ASR</h2><p><strong>Authors:Pablo Peso Parada, Spyros Fontalis, Md Asif Jalal, Karthikeyan Saravanan, Anastasios Drosou, Mete Ozay, Gil Ho Lee, Jungin Lee, Seokyeong Jung</strong></p>
<p>Data augmentation (DA) is ubiquitously used in training of Automatic Speech Recognition (ASR) models. DA offers increased data variability, robustness and generalization against different acoustic distortions. Recently, personalization of ASR models on mobile devices has been shown to improve Word Error Rate (WER). This paper evaluates data augmentation in this context and proposes persoDA; a DA method driven by userâ€™s data utilized to personalize ASR. persoDA aims to augment training with data specifically tuned towards acoustic characteristics of the end-user, as opposed to standard augmentation based on Multi-Condition Training (MCT) that applies random reverberation and noises. Our evaluation with an ASR conformer-based baseline trained on Librispeech and personalized for VOICES shows that persoDA achieves a 13.9% relative WER reduction over using standard data augmentation (using random noise &amp; reverberation). Furthermore, persoDA shows 16% to 20% faster convergence over MCT. </p>
<blockquote>
<p>æ•°æ®å¢å¼ºï¼ˆDAï¼‰åœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹çš„è®­ç»ƒä¸­å¾—åˆ°äº†å¹¿æ³›åº”ç”¨ã€‚DAæä¾›äº†å¢åŠ æ•°æ®å˜åŒ–ã€ç¨³å¥æ€§å’Œå¯¹ä¸åŒå£°å­¦å¤±çœŸçš„æ³›åŒ–èƒ½åŠ›ã€‚æœ€è¿‘ï¼Œåœ¨ç§»åŠ¨è®¾å¤‡ä¸Šè¿›è¡ŒASRæ¨¡å‹çš„ä¸ªæ€§åŒ–å·²è¢«è¯æ˜å¯ä»¥é™ä½è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ã€‚æœ¬æ–‡åœ¨æ­¤èƒŒæ™¯ä¸‹è¯„ä¼°æ•°æ®å¢å¼ºï¼Œå¹¶æå‡ºpersoDAï¼›ä¸€ç§åˆ©ç”¨ç”¨æˆ·æ•°æ®é©±åŠ¨çš„DAæ–¹æ³•ï¼Œç”¨äºä¸ªæ€§åŒ–ASRã€‚persoDAæ—¨åœ¨åˆ©ç”¨é’ˆå¯¹æœ€ç»ˆç”¨æˆ·å£°å­¦ç‰¹æ€§è°ƒæ•´çš„æ•°æ®æ¥å¢å¼ºè®­ç»ƒï¼Œè€Œä¸æ˜¯åŸºäºå¤šæ¡ä»¶è®­ç»ƒï¼ˆMCTï¼‰çš„æ ‡å‡†å¢å¼ºï¼Œåè€…åº”ç”¨éšæœºæ··å“å’Œå™ªå£°ã€‚æˆ‘ä»¬åœ¨Librispeechä¸Šè®­ç»ƒçš„åŸºäºè¯­éŸ³è¯†åˆ«çš„ç¡®è®¤è€…åŸºçº¿è¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶å¯¹äººå£°è¿›è¡Œäº†ä¸ªæ€§åŒ–å¤„ç†ï¼Œç»“æœè¡¨æ˜ï¼Œä¸æ ‡å‡†æ•°æ®å¢å¼ºï¼ˆä½¿ç”¨éšæœºå™ªå£°å’Œæ··å“ï¼‰ç›¸æ¯”ï¼ŒpersoDAå®ç°äº†ç›¸å¯¹é™ä½13.9%çš„WERã€‚æ­¤å¤–ï¼Œä¸MCTç›¸æ¯”ï¼ŒpersoDAæ˜¾ç¤ºå‡ºæ›´å¿«çš„æ”¶æ•›é€Ÿåº¦ï¼Œæé«˜äº†16%è‡³20%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.09113v1">PDF</a> Accepted at ICASSP 2025; â€œ\c{opyright} 20XX IEEE. Personal use of   this material is permitted. Permission from IEEE must be obtained for all   other uses, in any current or future media, including reprinting&#x2F;republishing   this material for advertising or promotional purposes, creating new   collective works, for resale or redistribution to servers or lists, or reuse   of any copyrighted component of â€¦â€</p>
<p><strong>Summary</strong>ï¼š<br>æ•°æ®å¢å¼ºï¼ˆDAï¼‰å¹¿æ³›åº”ç”¨äºè¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹çš„è®­ç»ƒï¼Œèƒ½å¤Ÿå¢åŠ æ•°æ®å¤šæ ·æ€§ã€æé«˜æ¨¡å‹çš„ç¨³å¥æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚æœ¬æ–‡è¯„ä¼°äº†ä¸ªæ€§åŒ–ASRæ¨¡å‹ä¸­æ•°æ®å¢å¼ºçš„æ•ˆæœï¼Œå¹¶æå‡ºä¸€ç§åŸºäºç”¨æˆ·æ•°æ®é©±åŠ¨çš„æ•°æ®å¢å¼ºæ–¹æ³•â€”â€”persoDAã€‚è¯¥æ–¹æ³•æ—¨åœ¨é€šè¿‡é’ˆå¯¹æœ€ç»ˆç”¨æˆ·çš„å£°å­¦ç‰¹æ€§è°ƒæ•´æ•°æ®æ¥å¢å¼ºè®­ç»ƒï¼Œè€Œä¸æ˜¯åŸºäºå¤šæ¡ä»¶è®­ç»ƒï¼ˆMCTï¼‰åº”ç”¨éšæœºæ··å“å’Œå™ªå£°ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œä¸æ ‡å‡†æ•°æ®å¢å¼ºç›¸æ¯”ï¼ŒpersoDAåœ¨è¯­éŸ³è¯†åˆ«çš„è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ä¸Šé™ä½äº†13.9%ï¼Œå¹¶ä¸”æ”¶æ•›é€Ÿåº¦æé«˜äº†16%è‡³20%ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æ•°æ®å¢å¼ºï¼ˆDAï¼‰å¹¿æ³›åº”ç”¨äºè¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹çš„è®­ç»ƒï¼Œå¯å¢åŠ æ•°æ®å¤šæ ·æ€§å’Œæ¨¡å‹ç¨³å¥æ€§ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†åŸºäºç”¨æˆ·æ•°æ®é©±åŠ¨çš„æ•°æ®å¢å¼ºæ–¹æ³•â€”â€”persoDAï¼Œæ—¨åœ¨é’ˆå¯¹æœ€ç»ˆç”¨æˆ·çš„å£°å­¦ç‰¹æ€§è°ƒæ•´æ•°æ®ã€‚</li>
<li>persoDAç›¸å¯¹äºæ ‡å‡†çš„å¤šæ¡ä»¶è®­ç»ƒï¼ˆMCTï¼‰ï¼Œæ›´ä¾§é‡äºåº”ç”¨ç”¨æˆ·çš„ä¸ªæ€§åŒ–æ•°æ®ã€‚</li>
<li>persoDAåœ¨è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œç›¸è¾ƒäºæ ‡å‡†æ•°æ®å¢å¼ºé™ä½äº†13.9%ã€‚</li>
<li>persoDAè¿˜æ˜¾ç¤ºå‡ºæ¯”MCTæ›´å¿«çš„æ”¶æ•›é€Ÿåº¦ï¼Œæé«˜äº†16%è‡³20%ã€‚</li>
<li>æ­¤æ–¹æ³•å¯¹äºä¸ªæ€§åŒ–ASRæ¨¡å‹çš„åº”ç”¨å…·æœ‰æ½œåŠ›ï¼Œå¯ä»¥æé«˜æ¨¡å‹åœ¨ç‰¹å®šç”¨æˆ·å£°å­¦ç‰¹æ€§ä¸Šçš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.09113">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0bbbde71e317cf41ad6bc76f3564cce7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8af275ab2812330140db0dc8942b5f41.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-077b60f4d996a62378f6026433dee485.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="VITA-1-5-Towards-GPT-4o-Level-Real-Time-Vision-and-Speech-Interaction"><a href="#VITA-1-5-Towards-GPT-4o-Level-Real-Time-Vision-and-Speech-Interaction" class="headerlink" title="VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction"></a>VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction</h2><p><strong>Authors:Chaoyou Fu, Haojia Lin, Xiong Wang, Yi-Fan Zhang, Yunhang Shen, Xiaoyu Liu, Yangze Li, Zuwei Long, Heting Gao, Ke Li, Long Ma, Xiawu Zheng, Rongrong Ji, Xing Sun, Caifeng Shan, Ran He</strong></p>
<p>Recent Multimodal Large Language Models (MLLMs) have typically focused on integrating visual and textual modalities, with less emphasis placed on the role of speech in enhancing interaction. However, speech plays a crucial role in multimodal dialogue systems, and implementing high-performance in both vision and speech tasks remains a significant challenge due to the fundamental modality differences. In this paper, we propose a carefully designed multi-stage training methodology that progressively trains LLM to understand both visual and speech information, ultimately enabling fluent vision and speech interaction. Our approach not only preserves strong vision-language capacity, but also enables efficient speech-to-speech dialogue capabilities without separate ASR and TTS modules, significantly accelerating multimodal end-to-end response speed. By comparing our method against state-of-the-art counterparts across benchmarks for image, video, and speech tasks, we demonstrate that our model is equipped with both strong visual and speech capabilities, making near real-time vision and speech interaction. </p>
<blockquote>
<p>æœ€è¿‘çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸»è¦èšç„¦äºæ•´åˆè§†è§‰å’Œæ–‡æœ¬æ¨¡æ€ï¼Œè¾ƒå°‘å¼ºè°ƒè¯­éŸ³åœ¨å¢å¼ºäº¤äº’ä¸­çš„ä½œç”¨ã€‚ç„¶è€Œï¼Œè¯­éŸ³åœ¨å¤šæ¨¡æ€å¯¹è¯ç³»ç»Ÿä¸­æ‰®æ¼”å…³é”®è§’è‰²ï¼Œç”±äºåœ¨åŸºç¡€æ¨¡æ€ä¸Šçš„å·®å¼‚ï¼Œåœ¨è§†è§‰å’Œè¯­éŸ³ä»»åŠ¡ä¸­éƒ½å®ç°é«˜æ€§èƒ½ä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç²¾å¿ƒè®¾è®¡çš„å¤šé˜¶æ®µè®­ç»ƒæ–¹æ³•è®ºï¼Œé€æ­¥è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ä»¥ç†è§£è§†è§‰å’Œè¯­éŸ³ä¿¡æ¯ï¼Œæœ€ç»ˆä½¿æµç•…çš„è§†è§‰å’Œè¯­éŸ³äº¤äº’æˆä¸ºå¯èƒ½ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…ä¿ç•™äº†å¼ºå¤§çš„è§†è§‰è¯­è¨€åŠŸèƒ½ï¼Œè¿˜å®ç°äº†é«˜æ•ˆçš„è¯­éŸ³å¯¹è¯èƒ½åŠ›ï¼Œæ— éœ€å•ç‹¬çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰å’Œæ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å—ï¼Œä»è€Œæ˜¾è‘—åŠ å¿«äº†å¤šæ¨¡æ€ç«¯åˆ°ç«¯çš„å“åº”é€Ÿåº¦ã€‚é€šè¿‡ä¸å›¾åƒã€è§†é¢‘å’Œè¯­éŸ³ä»»åŠ¡çš„æœ€æ–°å‰æ²¿æŠ€æœ¯è¿›è¡Œå¯¹æ¯”è¯„ä¼°ï¼Œæˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬çš„æ¨¡å‹å…·å¤‡å¼ºå¤§çš„è§†è§‰å’Œè¯­éŸ³åŠŸèƒ½ï¼Œå¯å®ç°è¿‘ä¹å®æ—¶çš„è§†è§‰å’Œè¯­éŸ³äº¤äº’ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.01957v2">PDF</a> <a target="_blank" rel="noopener" href="https://github.com/VITA-MLLM/VITA">https://github.com/VITA-MLLM/VITA</a></p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨é›†æˆè§†è§‰å’Œæ–‡æœ¬æ¨¡æ€æ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†å¯¹è¯­éŸ³åœ¨å¢å¼ºäº¤äº’ä¸­çš„ä½œç”¨é‡è§†ä¸è¶³ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç²¾å¿ƒè®¾è®¡çš„å¤šé˜¶æ®µè®­ç»ƒæ–¹æ³•ï¼Œé€æ­¥è®­ç»ƒLLMç†è§£è§†è§‰å’Œè¯­éŸ³ä¿¡æ¯ï¼Œå®ç°æµç•…çš„è§†å¬äº¤äº’ã€‚è¯¥æ–¹æ³•ä¸ä»…ä¿ç•™äº†å¼ºå¤§çš„è§†è§‰è¯­è¨€èƒ½åŠ›ï¼Œè¿˜èƒ½å®ç°é«˜æ•ˆçš„è¯­éŸ³å¯¹è¯èƒ½åŠ›ï¼Œæ— éœ€é¢å¤–çš„è¯­éŸ³è¯†åˆ«å’Œæ–‡æœ¬è½¬è¯­éŸ³æ¨¡å—ï¼Œæ˜¾è‘—åŠ å¿«å¤šæ¨¡æ€ç«¯åˆ°ç«¯çš„å“åº”é€Ÿåº¦ã€‚å¯¹æ¯”ç°æœ‰å…ˆè¿›æŠ€æœ¯ï¼Œæœ¬æ–‡æ¨¡å‹åœ¨å›¾åƒã€è§†é¢‘å’Œè¯­éŸ³ä»»åŠ¡åŸºå‡†æµ‹è¯•ä¸­å‡è¡¨ç°å‡ºå¼ºå¤§çš„è§†è§‰å’Œè¯­éŸ³èƒ½åŠ›ï¼Œå®ç°è¿‘ä¹å®æ—¶çš„è§†å¬äº¤äº’ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¿‘æœŸå¤šæ¨¡æ€è¯­è¨€æ¨¡å‹å€¾å‘äºèåˆè§†è§‰å’Œæ–‡æœ¬æ¨¡æ€ï¼Œä½†å¯¹è¯­éŸ³åœ¨äº¤äº’ä¸­çš„é‡è¦æ€§å…³æ³¨ä¸è¶³ã€‚</li>
<li>è¯­éŸ³åœ¨å¤šåª’ä½“å¯¹è¯ç³»ç»Ÿä¸­èµ·å…³é”®ä½œç”¨ï¼Œå®ç°è§†è§‰å’Œè¯­éŸ³ä»»åŠ¡çš„é«˜æ€§èƒ½æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§å¤šé˜¶æ®µè®­ç»ƒæ–¹æ³•ï¼Œä½¿LLMèƒ½å¤Ÿç†è§£è§†è§‰å’Œè¯­éŸ³ä¿¡æ¯ï¼Œå®ç°æµç•…çš„è§†å¬äº¤äº’ã€‚</li>
<li>è¯¥æ–¹æ³•ä¿ç•™äº†å¼ºå¤§çš„è§†è§‰è¯­è¨€èƒ½åŠ›ï¼Œå¹¶å…·å¤‡é«˜æ•ˆçš„è¯­éŸ³å¯¹è¯èƒ½åŠ›ï¼Œæ— éœ€é¢å¤–çš„ASRå’ŒTTSæ¨¡å—ã€‚</li>
<li>æ¨¡å‹åœ¨å›¾åƒã€è§†é¢‘å’Œè¯­éŸ³ä»»åŠ¡åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå…·å¤‡å¼ºå¤§çš„è§†è§‰å’Œè¯­éŸ³èƒ½åŠ›ã€‚</li>
<li>è¯¥æ¨¡å‹åŠ é€Ÿäº†å¤šæ¨¡æ€ç«¯åˆ°ç«¯çš„å“åº”é€Ÿåº¦ï¼Œå®ç°äº†è¿‘ä¹å®æ—¶çš„è§†å¬äº¤äº’ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.01957">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-58744d62eb0fbf66540b32115e33c365.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-458784e265fee8832f68789bf9d105f9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e262945a140a0fbcdb7ddae1a7741766.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-396c9589614e6d25affa510578046020.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-200583adacfa33383773d2e9f3835530.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Target-Speaker-ASR-with-Whisper"><a href="#Target-Speaker-ASR-with-Whisper" class="headerlink" title="Target Speaker ASR with Whisper"></a>Target Speaker ASR with Whisper</h2><p><strong>Authors:Alexander Polok, Dominik Klement, Matthew Wiesner, Sanjeev Khudanpur, Jan ÄŒernockÃ½, LukÃ¡Å¡ Burget</strong></p>
<p>We propose a novel approach to enable the use of large, single-speaker ASR models, such as Whisper, for target speaker ASR. The key claim of this method is that it is much easier to model relative differences among speakers by learning to condition on frame-level diarization outputs than to learn the space of all speaker embeddings. We find that adding even a single bias term per diarization output type before the first transformer block can transform single-speaker ASR models into target-speaker ASR models. Our approach also supports speaker-attributed ASR by sequentially generating transcripts for each speaker in a diarization output. This simplified method outperforms baseline speech separation and diarization cascade by 12.9 % absolute ORC-WER on the NOTSOFAR-1 dataset. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹æ–¹æ³•ï¼Œæ—¨åœ¨å°†å¤§å‹å•è¯´è¯äººè¯­éŸ³è¯†åˆ«æ¨¡å‹ï¼ˆå¦‚Whisperï¼‰åº”ç”¨äºç›®æ ‡è¯´è¯äººè¯­éŸ³è¯†åˆ«ã€‚è¯¥æ–¹æ³•çš„å…³é”®ä¸»å¼ æ˜¯ï¼Œé€šè¿‡å­¦ä¹ ä»¥å¸§çº§èšç±»è¾“å‡ºä¸ºæ¡ä»¶ï¼Œå¯¹è¯´è¯äººä¹‹é—´çš„ç›¸å¯¹å·®å¼‚è¿›è¡Œå»ºæ¨¡ï¼Œæ¯”å­¦ä¹ æ‰€æœ‰è¯´è¯äººåµŒå…¥çš„ç©ºé—´æ›´å®¹æ˜“ã€‚æˆ‘ä»¬å‘ç°ï¼Œåœ¨ç¬¬ä¸€ä¸ªtransformerå—ä¹‹å‰ä¸ºæ¯ç§èšç±»è¾“å‡ºç±»å‹æ·»åŠ ä¸€ä¸ªåå·®é¡¹ï¼Œå°±å¯ä»¥å°†å•è¯´è¯äººè¯­éŸ³è¯†åˆ«æ¨¡å‹è½¬å˜ä¸ºç›®æ ‡è¯´è¯äººè¯­éŸ³è¯†åˆ«æ¨¡å‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•è¿˜æ”¯æŒé€šè¿‡é¡ºåºç”Ÿæˆæ¯ä¸ªè¯´è¯äººçš„å­—å¹•æ¥è¿›è¡Œå±æ€§åŒ–è¯­éŸ³è¯†åˆ«ã€‚åœ¨NOTSOFAR-1æ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•ç›¸å¯¹äºåŸºçº¿è¯­éŸ³åˆ†ç¦»å’Œèšç±»ä¸²è”æ–¹æ³•ï¼Œç»å¯¹ORC-WERæé«˜äº†12.9%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.09543v2">PDF</a> Accepted to ICASSP 2025</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æå‡ºä¸€ç§æ–°æ–¹æ³•ï¼Œé€šè¿‡åŸºäºå¸§çº§åˆ«çš„åˆ†æ è¾“å‡ºæ¡ä»¶è®­ç»ƒï¼Œä½¿å¤§å‹å•ä¸€è¯´è¯äººè‡ªåŠ¨è¯­éŸ³è¯†åˆ«æ¨¡å‹ï¼ˆå¦‚Whisperï¼‰èƒ½å¤Ÿç”¨äºç›®æ ‡è¯´è¯äººè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ã€‚è¯¥æ–¹æ³•çš„å…³é”®åœ¨äºï¼Œé€šè¿‡å»ºæ¨¡ä¸åŒè¯´è¯äººä¹‹é—´çš„ç›¸å¯¹å·®å¼‚æ¥æ”¹è¿›æ¨¡å‹æ€§èƒ½ï¼Œè€Œéå­¦ä¹ æ‰€æœ‰è¯´è¯äººçš„åµŒå…¥ç©ºé—´ã€‚æ·»åŠ å•ä¸€åç½®é¡¹ï¼Œåœ¨åˆ†æ è¾“å‡ºç±»å‹ä¹‹å‰è¿›è¡Œé¢„å¤„ç†å³å¯å°†å•ä¸€è¯´è¯äººè‡ªåŠ¨è¯­éŸ³è¯†åˆ«æ¨¡å‹è½¬åŒ–ä¸ºç›®æ ‡è¯´è¯äººè‡ªåŠ¨è¯­éŸ³è¯†åˆ«æ¨¡å‹ã€‚æ­¤æ–¹æ³•è¿˜å…·å¤‡ç”Ÿæˆæ¯ä½è¯´è¯äººå¯¹åº”è½¬å†™çš„åŠŸèƒ½ã€‚åœ¨NOTSOFAR-1æ•°æ®é›†ä¸Šï¼Œç›¸è¾ƒäºåŸºå‡†è¯­éŸ³åˆ†ç¦»å’Œå¯¹é½çº§è”æ–¹æ³•ï¼Œæ­¤æ–¹æ³•åœ¨ç»å¯¹è¯¯å·®ç‡ä¸Šæé«˜äº†12.9%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºä¸€ç§æ–°æ–¹æ³•ä½¿å¤§å‹å•ä¸€è¯´è¯äººASRæ¨¡å‹ï¼ˆå¦‚Whisperï¼‰å¯ç”¨äºç›®æ ‡è¯´è¯äººASRã€‚</li>
<li>å»ºæ¨¡ç›¸å¯¹å·®å¼‚ä¼˜äºå­¦ä¹ æ‰€æœ‰è¯´è¯äººçš„åµŒå…¥ç©ºé—´ã€‚</li>
<li>æ·»åŠ åç½®é¡¹å¯ç®€åŒ–æ¨¡å‹è½¬æ¢è¿‡ç¨‹ã€‚</li>
<li>æ–¹æ³•æ”¯æŒåŸºäºåˆ†æ è¾“å‡ºçš„è¯´è¯äººå±æ€§ASRã€‚</li>
<li>è¯¥æ–¹æ³•ç”Ÿæˆæ¯ä½è¯´è¯äººçš„è½¬å†™å†…å®¹ã€‚</li>
<li>æ–¹æ³•åœ¨NOTSOFAR-1æ•°æ®é›†ä¸Šæ€§èƒ½æ˜¾è‘—æå‡ï¼Œç»å¯¹è¯¯å·®ç‡é™ä½12.9%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.09543">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-60d1f1cd17fe8838aa4c5e546466bceb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-eadb71e7c6b96b9c2775cebe2b686192.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fde5bc2b63aa96c3e15afef846c3a40b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-99978c7b3e1ac1901a437e58c3ca1a74.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-39ed844928da0dd31f7e2b8f60dc9387.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Toward-Any-to-Any-Emotion-Voice-Conversion-using-Disentangled-Diffusion-Framework"><a href="#Toward-Any-to-Any-Emotion-Voice-Conversion-using-Disentangled-Diffusion-Framework" class="headerlink" title="Toward Any-to-Any Emotion Voice Conversion using Disentangled Diffusion   Framework"></a>Toward Any-to-Any Emotion Voice Conversion using Disentangled Diffusion   Framework</h2><p><strong>Authors:Hsing-Hang Chou, Yun-Shao Lin, Ching-Chin Sung, Yu Tsao, Chi-Chun Lee</strong></p>
<p>Emotional Voice Conversion (EVC) aims to modify the emotional expression of speech for various applications, such as human-machine interaction. Previous deep learning-based approaches using generative adversarial networks and autoencoder models have shown promise but suffer from quality degradation and limited emotion control. To address these issues, a novel diffusion-based EVC framework with disentangled loss and expressive guidance is proposed. Our method separates speaker and emotional features to maintain speech quality while improving emotional expressiveness. Tested on real-world and acted-out datasets, the approach achieved significant improvements in emotion classification accuracy for both in-the-wild and act-out datasets and showed reduced distortion compared to state-of-the-art models. </p>
<blockquote>
<p>æƒ…æ„Ÿè¯­éŸ³è½¬æ¢ï¼ˆEVCï¼‰æ—¨åœ¨ä¿®æ”¹è¯­éŸ³çš„æƒ…æ„Ÿè¡¨è¾¾ï¼Œä»¥åº”ç”¨äºäººæœºäº¤äº’ç­‰å„ç§åœºæ™¯ã€‚è™½ç„¶åŸºäºæ·±åº¦å­¦ä¹ çš„å¯¹æŠ—ç”Ÿæˆç½‘ç»œå’Œè‡ªç¼–ç å™¨æ¨¡å‹çš„æ–¹æ³•å·²ç»å±•ç°å‡ºä¸€å®šçš„æ½œåŠ›ï¼Œä½†å®ƒä»¬ä»é¢ä¸´è´¨é‡ä¸‹é™å’Œæƒ…æ„Ÿæ§åˆ¶æœ‰é™çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹çš„åŸºäºæ‰©æ•£çš„EVCæ¡†æ¶ï¼Œè¯¥æ¡†æ¶å…·æœ‰åˆ†ç¦»æŸå¤±å’Œè¡¨è¾¾æŒ‡å¯¼çš„ç‰¹ç‚¹ã€‚æˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿåˆ†ç¦»è¯´è¯è€…å’Œæƒ…æ„Ÿç‰¹å¾ï¼Œä»è€Œä¿æŒè¯­éŸ³è´¨é‡ï¼ŒåŒæ—¶æé«˜æƒ…æ„Ÿè¡¨ç°åŠ›ã€‚åœ¨çœŸå®ä¸–ç•Œå’Œæ¼”ç»æ•°æ®é›†ä¸Šè¿›è¡Œæµ‹è¯•ï¼Œè¯¥æ–¹æ³•åœ¨é‡ç”Ÿå’Œæ¼”ç»æ•°æ®é›†çš„æƒ…æ„Ÿåˆ†ç±»ç²¾åº¦ä¸Šå–å¾—äº†æ˜¾è‘—çš„æé«˜ï¼Œä¸æœ€å…ˆè¿›çš„æ¨¡å‹ç›¸æ¯”ï¼Œå¤±çœŸåº¦ä¹Ÿæœ‰æ‰€é™ä½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.03636v3">PDF</a> 5 pages; revised arguments, typos and references corrected</p>
<p><strong>Summary</strong>ï¼šæƒ…æ„Ÿè¯­éŸ³è½¬æ¢ï¼ˆEVCï¼‰æ—¨åœ¨ä¿®æ”¹è¯­éŸ³çš„æƒ…æ„Ÿè¡¨è¾¾ï¼Œä»¥åº”ç”¨äºäººæœºäº¤äº’ç­‰å„ç§åœºæ™¯ã€‚é’ˆå¯¹ä»¥å¾€åŸºäºæ·±åº¦å­¦ä¹ çš„å¯¹æŠ—ç”Ÿæˆç½‘ç»œå’Œè‡ªç¼–ç å™¨æ¨¡å‹åœ¨æƒ…æ„Ÿæ§åˆ¶ä¸Šçš„ä¸è¶³å’Œè¯­éŸ³è´¨é‡ä¸‹é™çš„é—®é¢˜ï¼Œæå‡ºäº†æ–°å‹æ‰©æ•£æ¨¡å‹åŸºç¡€çš„EVCæ¡†æ¶ï¼Œå…·æœ‰åˆ†ç¦»æŸå¤±å’Œè¡¨ç°æ€§æŒ‡å¯¼çš„ç‰¹æ€§ï¼Œå¯ä¿æŒè¯­éŸ³è´¨é‡çš„åŒæ—¶æå‡æƒ…æ„Ÿè¡¨è¾¾åŠ›ã€‚åœ¨çœŸå®ä¸–ç•Œå’Œæ¨¡æ‹Ÿæ•°æ®é›†ä¸Šçš„æµ‹è¯•æ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨é‡å¤–å’Œæ¨¡æ‹Ÿæ•°æ®é›†çš„æƒ…æ„Ÿåˆ†ç±»å‡†ç¡®ç‡ä¸Šå®ç°äº†æ˜¾è‘—æå‡ï¼Œä¸å½“å‰ä¸»æµæ¨¡å‹ç›¸æ¯”å¤±çœŸåº¦é™ä½ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æƒ…æ„Ÿè¯­éŸ³è½¬æ¢ï¼ˆEVCï¼‰æ—¨åœ¨ä¿®æ”¹è¯­éŸ³çš„æƒ…æ„Ÿè¡¨è¾¾ï¼Œåº”ç”¨äºäººæœºäº¤äº’ç­‰åœºæ™¯ã€‚</li>
<li>ä»¥å¾€çš„æ·±åº¦å­¦ä¹ æ–¹æ³•å­˜åœ¨è¯­éŸ³è´¨é‡ä¸‹é™å’Œæœ‰é™æƒ…æ„Ÿæ§åˆ¶çš„é—®é¢˜ã€‚</li>
<li>æ–°æå‡ºçš„æ‰©æ•£æ¨¡å‹åŸºç¡€çš„EVCæ¡†æ¶æ—¨åœ¨è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>æ–°æ¡†æ¶é€šè¿‡åˆ†ç¦»æŸå¤±å’Œè¡¨ç°æ€§æŒ‡å¯¼æ¥ä¿æŒè¯­éŸ³è´¨é‡å¹¶æå‡æƒ…æ„Ÿè¡¨è¾¾åŠ›ã€‚</li>
<li>åœ¨çœŸå®ä¸–ç•Œå’Œæ¨¡æ‹Ÿæ•°æ®é›†ä¸Šçš„æµ‹è¯•æ˜¾ç¤ºæ–°æ¡†æ¶çš„æƒ…æ„Ÿåˆ†ç±»å‡†ç¡®ç‡æ˜¾è‘—æå‡ã€‚</li>
<li>ä¸ç°æœ‰ä¸»æµæ¨¡å‹ç›¸æ¯”ï¼Œæ–°æ¡†æ¶çš„è¯­éŸ³å¤±çœŸåº¦è¾ƒä½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.03636">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-5eee8dc7a149e1d99b0453b1a7536e84.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8d57b6a038fb3ddd18bf01eee492b48c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b2dc559ebef309472ac40db95c8125a0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ead311cf31d4c3ba6fe3951b36625387.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-18/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-18/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-18/GAN/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-4a0328cfb43a3f15aee87113989fcf27.jpg" class="responsive-img" alt="GAN">
                        
                        <span class="card-title">GAN</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            GAN æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-18  iFADIT Invertible Face Anonymization via Disentangled Identity   Transform
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-18
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/GAN/" class="post-category">
                                    GAN
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/GAN/">
                        <span class="chip bg-color">GAN</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-18/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F_Breast%20Ultrasound/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-feb68adb274eb01e606b2bd8c5e3883d.jpg" class="responsive-img" alt="åŒ»å­¦å½±åƒ/Breast Ultrasound">
                        
                        <span class="card-title">åŒ»å­¦å½±åƒ/Breast Ultrasound</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å½±åƒ/Breast Ultrasound æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-18  Efficient Few-Shot Medical Image Analysis via Hierarchical Contrastive   Vision-Language Learning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-18
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/" class="post-category">
                                    åŒ»å­¦å½±åƒ/Breast Ultrasound
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/">
                        <span class="chip bg-color">åŒ»å­¦å½±åƒ/Breast Ultrasound</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">14643.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
