<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech 方向最新论文已更新，请持续关注 Update in 2025-01-18  PIER A Novel Metric for Evaluating What Matters in Code-Switching">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-6ec41090a2a3cd6a3668b66f99bffe23.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-01-18
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-01-18
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    8.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    34 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-01-18-更新"><a href="#2025-01-18-更新" class="headerlink" title="2025-01-18 更新"></a>2025-01-18 更新</h1><h2 id="PIER-A-Novel-Metric-for-Evaluating-What-Matters-in-Code-Switching"><a href="#PIER-A-Novel-Metric-for-Evaluating-What-Matters-in-Code-Switching" class="headerlink" title="PIER: A Novel Metric for Evaluating What Matters in Code-Switching"></a>PIER: A Novel Metric for Evaluating What Matters in Code-Switching</h2><p><strong>Authors:Enes Yavuz Ugan, Ngoc-Quan Pham, Leonard Bärmann, Alex Waibel</strong></p>
<p>Code-switching, the alternation of languages within a single discourse, presents a significant challenge for Automatic Speech Recognition. Despite the unique nature of the task, performance is commonly measured with established metrics such as Word-Error-Rate (WER). However, in this paper, we question whether these general metrics accurately assess performance on code-switching. Specifically, using both Connectionist-Temporal-Classification and Encoder-Decoder models, we show fine-tuning on non-code-switched data from both matrix and embedded language improves classical metrics on code-switching test sets, although actual code-switched words worsen (as expected). Therefore, we propose Point-of-Interest Error Rate (PIER), a variant of WER that focuses only on specific words of interest. We instantiate PIER on code-switched utterances and show that this more accurately describes the code-switching performance, showing huge room for improvement in future work. This focused evaluation allows for a more precise assessment of model performance, particularly in challenging aspects such as inter-word and intra-word code-switching. </p>
<blockquote>
<p>语言转换（code-switching）指的是在同一语境中交替使用不同的语言，这给自动语音识别（ASR）带来了很大的挑战。尽管该任务具有独特性，但通常使用标准的度量方法（如单词错误率（WER））来评估性能。然而，本文对此提出质疑，认为这些通用指标是否能准确评估语言转换情况下的性能。具体来说，我们利用连接时序分类和编码器-解码器模型，展示了对来自矩阵和嵌入式语言的非语言转换数据的微调能提高语言转换测试集上的经典指标，尽管实际的转换词有所恶化（符合预期）。因此，我们提出了兴趣点错误率（PIER），这是WER的一个变体，只关注特定的兴趣词。我们在语言转换的片段上实现了PIER，并证明它能更准确地描述语言转换的性能，显示出未来工作中有很大的改进空间。这种有针对性的评估允许对模型性能进行更精确的判断，特别是在词语间和词语内的语言转换等具有挑战性的方面。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.09512v1">PDF</a> Accepted at ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>本文主要探讨了自动语音识别中的语言代码转换问题。研究指出传统评估指标如词错误率（WER）无法准确评估代码切换场景下的性能。通过使用CTC和Encoder-Decoder模型，研究发现在非代码切换数据上进行微调会提高在代码切换测试集上的经典指标得分，但对实际代码切换单词的性能依然有待提高。为此，提出了一种新的评估方法——关键词错误率（PIER），它能更准确地描述代码切换的性能，显示出未来研究存在巨大改进空间。该评估方法有助于更精确地评估模型性能，特别是在跨词和词内代码切换等挑战方面。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>代码转换在自动语音识别中是一项重要挑战。</li>
<li>传统评估指标如词错误率（WER）无法准确评估代码切换场景下的模型性能。</li>
<li>使用CTC和Encoder-Decoder模型的微调能提高在非代码切换数据上的性能，但在实际代码切换单词上的性能仍然有待提高。</li>
<li>提出了一种新的评估方法——关键词错误率（PIER），能更准确地反映模型在代码切换场景下的性能。</li>
<li>PIER评估方法特别关注特定单词的错误，适用于评估跨词和词内代码切换等挑战方面的性能。</li>
<li>PIER评估方法的提出显示出在自动语音识别中的代码转换问题上仍存在巨大的改进空间。</li>
<li>精确评估模型性能对于解决自动语音识别中的代码转换问题至关重要。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.09512">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-59274d75199b99016a6ba80fe1e1de23.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2c187cfffd44e50ad1c3a36a94339a29.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a3dbc2713c789a5ac63ea18ed99c05e1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ec41090a2a3cd6a3668b66f99bffe23.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c15603fc7336fafc052fb7b4b16b17a5.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Multimodal-Marvels-of-Deep-Learning-in-Medical-Diagnosis-A-Comprehensive-Review-of-COVID-19-Detection"><a href="#Multimodal-Marvels-of-Deep-Learning-in-Medical-Diagnosis-A-Comprehensive-Review-of-COVID-19-Detection" class="headerlink" title="Multimodal Marvels of Deep Learning in Medical Diagnosis: A   Comprehensive Review of COVID-19 Detection"></a>Multimodal Marvels of Deep Learning in Medical Diagnosis: A   Comprehensive Review of COVID-19 Detection</h2><p><strong>Authors:Md Shofiqul Islama, Khondokar Fida Hasanc, Hasibul Hossain Shajeebd, Humayan Kabir Ranae, Md Saifur Rahmand, Md Munirul Hasanb, AKM Azadf, Ibrahim Abdullahg, Mohammad Ali Moni</strong></p>
<p>This study presents a comprehensive review of the potential of multimodal deep learning (DL) in medical diagnosis, using COVID-19 as a case example. Motivated by the success of artificial intelligence applications during the COVID-19 pandemic, this research aims to uncover the capabilities of DL in disease screening, prediction, and classification, and to derive insights that enhance the resilience, sustainability, and inclusiveness of science, technology, and innovation systems. Adopting a systematic approach, we investigate the fundamental methodologies, data sources, preprocessing steps, and challenges encountered in various studies and implementations. We explore the architecture of deep learning models, emphasising their data-specific structures and underlying algorithms. Subsequently, we compare different deep learning strategies utilised in COVID-19 analysis, evaluating them based on methodology, data, performance, and prerequisites for future research. By examining diverse data types and diagnostic modalities, this research contributes to scientific understanding and knowledge of the multimodal application of DL and its effectiveness in diagnosis. We have implemented and analysed 11 deep learning models using COVID-19 image, text, and speech (ie, cough) data. Our analysis revealed that the MobileNet model achieved the highest accuracy of 99.97% for COVID-19 image data and 93.73% for speech data (i.e., cough). However, the BiGRU model demonstrated superior performance in COVID-19 text classification with an accuracy of 99.89%. The broader implications of this research suggest potential benefits for other domains and disciplines that could leverage deep learning techniques for image, text, and speech analysis. </p>
<blockquote>
<p>本研究全面回顾了多模态深度学习（DL）在医学诊断中的潜力，并以COVID-19作为案例进行研究。本研究受COVID-19疫情期间人工智能应用成功的启发，旨在揭示深度学习在疾病筛查、预测和分类方面的能力，并借此获得能够增强科学、技术和创新系统韧性、可持续性和包容性的见解。我们采用系统的方法，研究了各种研究和实践中的基本方法、数据来源、预处理步骤和所面临的挑战。我们探讨了深度学习模型的架构，重点介绍了它们针对数据的特定结构和基础算法。随后，我们对COVID-19分析中使用的不同深度学习策略进行了比较，根据方法、数据、性能和未来研究的前提要求进行了评估。本研究通过检查多种数据类型和诊断方式，探讨了深度学习多模态应用在诊断中的科学理解和有效性方面的贡献。我们使用COVID-19图像、文本和语音（即咳嗽声）数据实现了11个深度学习模型，并对其进行了分析。分析结果显示，MobileNet模型在COVID-19图像数据上达到了99.97%的准确率，在语音数据（即咳嗽声）上达到了93.73%的准确率。然而，BiGRU模型在COVID-19文本分类方面表现出卓越的性能，准确率为99.89%。这项研究的更广泛意义表明，其他领域和学科也有可能受益于深度学习技术在图像、文本和语音分析方面的应用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.09506v1">PDF</a> 43 pages</p>
<p><strong>摘要</strong></p>
<p>本文全面综述了多模态深度学习在医学诊断中的潜力，以COVID-19为例。该研究旨在揭示深度学习在疾病筛查、预测和分类方面的能力，并借此增强科学、技术和创新系统的复原力、可持续性和包容性。研究采用系统方法，探讨了根本的方法论、数据来源、预处理步骤以及在不同研究和实践中的挑战。本文探讨了深度学习模型的架构，重点介绍了其数据特定结构和基础算法。通过考察不同类型的数据和诊断方式，本研究有助于理解多模态深度学习在诊断中的有效性。研究发现，MobileNet模型在COVID-19图像和语音数据上的准确率最高，分别为99.97%和93.73%，而BiGRU模型在COVID-19文本分类中表现最佳，准确率为99.89%。此研究对其他领域和学科具有潜在应用价值。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>研究全面回顾了多模态深度学习在医学诊断中的潜力，以COVID-19为案例进行详细分析。</li>
<li>深度学习在疾病筛查、预测和分类方面展现出强大能力，有助于增强科学系统的复原力、可持续性和包容性。</li>
<li>研究深入探讨了深度学习模型的基础架构和数据特定结构，强调了其在处理不同数据类型（如图像、文本和语音）时的优势。</li>
<li>MobileNet模型在COVID-19图像和语音数据准确率高，而BiGRU模型在文本分类中表现最佳。</li>
<li>研究通过系统方法，探讨了深度学习在实践中的应用挑战和未来发展所需的前提条件。</li>
<li>此研究不仅局限于COVID-19的医学诊断，对其他领域和学科的深度学习应用也提供了有价值的参考。</li>
<li>研究成果有助于提升对多模态深度学习应用的理解和知识贡献。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.09506">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-74f86bf3b7dd3d3932c7ba39bb10090c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8252398af67b74141fcbd6291b255a1d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-abcb8d99a7f8560b0bf3c11d50be1e77.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-28e5bcced15d881ab182c2f4028cd717.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e3a16057f105f4272461c74284394bb1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-164a52d64536f7d41479e4d42e09553f.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Teaching-Wav2Vec2-the-Language-of-the-Brain"><a href="#Teaching-Wav2Vec2-the-Language-of-the-Brain" class="headerlink" title="Teaching Wav2Vec2 the Language of the Brain"></a>Teaching Wav2Vec2 the Language of the Brain</h2><p><strong>Authors:Tobias Fiedler, Leon Hermann, Florian Müller, Sarel Cohen, Peter Chin, Tobias Friedrich, Eilon Vaadia</strong></p>
<p>The decoding of continuously spoken speech from neuronal activity has the potential to become an important clinical solution for paralyzed patients. Deep Learning Brain Computer Interfaces (BCIs) have recently successfully mapped neuronal activity to text contents in subjects who attempted to formulate speech. However, only small BCI datasets are available. In contrast, labeled data and pre-trained models for the closely related task of speech recognition from audio are widely available. One such model is Wav2Vec2 which has been trained in a self-supervised fashion to create meaningful representations of speech audio data. In this study, we show that patterns learned by Wav2Vec2 are transferable to brain data. Specifically, we replace its audio feature extractor with an untrained Brain Feature Extractor (BFE) model. We then execute full fine-tuning with pre-trained weights for Wav2Vec2, training ‘’from scratch’’ without pre-trained weights as well as freezing a pre-trained Wav2Vec2 and training only the BFE each for 45 different BFE architectures. Across these experiments, the best run is from full fine-tuning with pre-trained weights, achieving a Character Error Rate (CER) of 18.54%, outperforming the best training from scratch run by 20.46% and that of frozen Wav2Vec2 training by 15.92% percentage points. These results indicate that knowledge transfer from audio speech recognition to brain decoding is possible and significantly improves brain decoding performance for the same architectures. Related source code is available at <a target="_blank" rel="noopener" href="https://github.com/tfiedlerdev/Wav2Vec2ForBrain">https://github.com/tfiedlerdev/Wav2Vec2ForBrain</a>. </p>
<blockquote>
<p>从神经元活动解码连续语音具有成为瘫痪患者重要临床解决方案的潜力。深度学习脑机接口（BCIs）最近成功地将神经元活动映射到尝试进行语音的主体文本内容中。然而，可用的BCI数据集很小。相比之下，与从音频进行语音识别这一紧密相关的任务相关的标记数据和预训练模型却随处可见。其中一种模型是Wav2Vec2，它采用自我监督的方式进行训练，能够创建语音音频数据的有意义表示。在这项研究中，我们展示了Wav2Vec2所学习到的模式可以转移到脑数据。具体来说，我们用未经训练的脑特征提取器（BFE）模型替换了其音频特征提取器。然后我们对Wav2Vec2进行预训练权重的全面微调，同时训练“从头开始”的模型，以及冻结预训练的Wav2Vec2并仅对BFE进行训练，共尝试了45种不同的BFE架构。在这些实验中，最好的结果是使用预训练权重进行的全微调，字符错误率（CER）达到18.54%，比从头开始训练的最好成绩高出20.46%，比冻结的Wav2Vec2训练高出15.92个百分点。这些结果表明，从语音识别到大脑解码的知识转移是可能的，并显著提高了相同架构的大脑解码性能。相关源代码可在<a target="_blank" rel="noopener" href="https://github.com/tfiedlerdev/Wav2Vec2ForBrain%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/tfiedlerdev/Wav2Vec2ForBrain找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.09459v1">PDF</a> Paper was submitted to ICASSP 2025 but marginally rejected</p>
<p><strong>摘要</strong></p>
<p>基于深度学习的大脑计算机接口（BCIs）可从神经元活动中解码连续口语，为瘫痪患者带来重要的临床解决方案。本研究利用Wav2Vec2模型，一种经过自我监督训练的语音音频数据表示方法，展示其从音频到大脑的知识的可迁移性。通过将Wav2Vec2的音频特征提取器替换为未训练的大脑特征提取器（BFE）模型，并进行全面微调，实验结果显示，使用预训练权重的全微调方式表现最佳，字符错误率（CER）达到18.54%，相较于从头开始训练和冻结Wav2Vec2仅训练BFE的方式，分别提高了20.46%和15.92%。这表明语音识别的知识可以迁移到大脑解码中，并显著提高解码性能。相关源代码已公开。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>研究探索了将深度学习与大脑计算机接口（BCIs）结合的方法，使从神经元活动中解码连续口语成为可能。</li>
<li>利用Wav2Vec2模型，一个经过自我监督训练的语音音频数据表示方法，在大脑解码任务中展示了其有效性。</li>
<li>研究通过替换Wav2Vec2的音频特征提取器为大脑特征提取器（BFE）模型，实现了知识的迁移学习。</li>
<li>实验结果显示全微调方式表现最佳，字符错误率（CER）达到18.54%。</li>
<li>与从头开始训练和冻结Wav2Vec2仅训练BFE的方式相比，使用预训练权重的全微调方式显著提高了解码性能。</li>
<li>知识从语音识别迁移到大脑解码是可能的，这有助于为瘫痪患者提供临床解决方案。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.09459">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0b8488298f3e19f38787ccf7f6c94d12.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b191f7ac164867bcd496f0d9369b415d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1c0cca2b39d386bf3ea6cf0a909c3a05.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4a48d76fd897c7c7408cc9a5d280ef0e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9adc867c41cb1d778051bd50b0cceb02.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Delayed-Fusion-Integrating-Large-Language-Models-into-First-Pass-Decoding-in-End-to-end-Speech-Recognition"><a href="#Delayed-Fusion-Integrating-Large-Language-Models-into-First-Pass-Decoding-in-End-to-end-Speech-Recognition" class="headerlink" title="Delayed Fusion: Integrating Large Language Models into First-Pass   Decoding in End-to-end Speech Recognition"></a>Delayed Fusion: Integrating Large Language Models into First-Pass   Decoding in End-to-end Speech Recognition</h2><p><strong>Authors:Takaaki Hori, Martin Kocour, Adnan Haider, Erik McDermott, Xiaodan Zhuang</strong></p>
<p>This paper presents an efficient decoding approach for end-to-end automatic speech recognition (E2E-ASR) with large language models (LLMs). Although shallow fusion is the most common approach to incorporate language models into E2E-ASR decoding, we face two practical problems with LLMs. (1) LLM inference is computationally costly. (2) There may be a vocabulary mismatch between the ASR model and the LLM. To resolve this mismatch, we need to retrain the ASR model and&#x2F;or the LLM, which is at best time-consuming and in many cases not feasible. We propose “delayed fusion,” which applies LLM scores to ASR hypotheses with a delay during decoding and enables easier use of pre-trained LLMs in ASR tasks. This method can reduce not only the number of hypotheses scored by the LLM but also the number of LLM inference calls. It also allows re-tokenizion of ASR hypotheses during decoding if ASR and LLM employ different tokenizations. We demonstrate that delayed fusion provides improved decoding speed and accuracy compared to shallow fusion and N-best rescoring using the LibriHeavy ASR corpus and three public LLMs, OpenLLaMA 3B &amp; 7B and Mistral 7B. </p>
<blockquote>
<p>本文提出了一种利用大型语言模型（LLM）进行端到端自动语音识别（E2E-ASR）的高效解码方法。尽管浅融合是将语言模型融入E2E-ASR解码的最常见方法，但在使用LLMs时，我们面临两个实际问题。(1) LLM推理计算成本高昂。(2) ASR模型和LLM之间可能存在词汇不匹配的问题。为解决这一不匹配问题，我们需要重新训练ASR模型或LLM，这至少很耗时，并且在许多情况下并不可行。我们提出了“延迟融合”的方法，该方法在解码过程中延迟将LLM分数应用于ASR假设，使得在ASR任务中更容易使用预训练的LLM。这种方法不仅可以减少LLM评分的假设数量，还可以减少LLM推理调用次数。如果ASR和LLM采用不同的标记化方式，它还可以在解码过程中重新标记ASR假设。我们在LibriHeavy ASR语料库和三个公共LLM（OpenLLaMA 3B &amp; 7B和Mistral 7B）上演示了延迟融合相比浅融合和N-best重评分提供了更高的解码速度和准确性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.09258v1">PDF</a> Accepted to ICASSP2025</p>
<p><strong>Summary</strong></p>
<p>本文提出一种高效的解码方法，用于结合大型语言模型（LLMs）进行端到端自动语音识别（E2E-ASR）。传统的浅融合方法面临计算成本高和词汇不匹配的问题。为解决这一问题，本文提出了延迟融合策略，在解码过程中延迟应用LLM评分，减少了LLM评分假设数量和推理调用次数，并允许在解码过程中进行ASR假设的重标记化。实验表明，延迟融合相较于浅融合和N-best重评分方法，能提高解码速度和准确性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>论文提出了一种新的解码方法，用于将大型语言模型（LLMs）与端到端自动语音识别（E2E-ASR）系统结合。</li>
<li>传统浅融合方法存在计算成本高和词汇不匹配的问题。</li>
<li>延迟融合策略能够减少LLM评分的假设数量和推理调用次数。</li>
<li>延迟融合允许在解码过程中进行ASR假设的重标记化，以适应不同的词汇表或标记化方式。</li>
<li>实验结果表明，延迟融合策略相较于传统的浅融合和N-best重评分方法，具有更高的解码速度和准确性。</li>
<li>论文使用了LibriHeavy ASR语料库和三个公开的大型语言模型（OpenLLaMA 3B &amp; 7B以及Mistral 7B）进行验证。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.09258">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-d744c92cdf58011bf73d9863ab90e0e5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4cf1562d48bd6b4f8e6bc4b44db7f7cc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f3dd404a74cbfe3200f15cbe9311a0ae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c5ea93d67a01b55feaeda942a62fc245.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-95118164814a5a8d895167648bc0d098.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Beyond-Speaker-Identity-Text-Guided-Target-Speech-Extraction"><a href="#Beyond-Speaker-Identity-Text-Guided-Target-Speech-Extraction" class="headerlink" title="Beyond Speaker Identity: Text Guided Target Speech Extraction"></a>Beyond Speaker Identity: Text Guided Target Speech Extraction</h2><p><strong>Authors:Mingyue Huo, Abhinav Jain, Cong Phuoc Huynh, Fanjie Kong, Pichao Wang, Zhu Liu, Vimal Bhat</strong></p>
<p>Target Speech Extraction (TSE) traditionally relies on explicit clues about the speaker’s identity like enrollment audio, face images, or videos, which may not always be available. In this paper, we propose a text-guided TSE model StyleTSE that uses natural language descriptions of speaking style in addition to the audio clue to extract the desired speech from a given mixture. Our model integrates a speech separation network adapted from SepFormer with a bi-modality clue network that flexibly processes both audio and text clues. To train and evaluate our model, we introduce a new dataset TextrolMix with speech mixtures and natural language descriptions. Experimental results demonstrate that our method effectively separates speech based not only on who is speaking, but also on how they are speaking, enhancing TSE in scenarios where traditional audio clues are absent. Demos are at: <a target="_blank" rel="noopener" href="https://mingyue66.github.io/TextrolMix/demo/">https://mingyue66.github.io/TextrolMix/demo/</a> </p>
<blockquote>
<p>传统目标语音提取（TSE）依赖于关于说话人身份的明确线索，如注册音频、面部图像或视频，但这些可能并不总是可用。在本文中，我们提出了一种文本引导的TSE模型StyleTSE，该模型使用对说话风格的自然语言描述以及音频线索，从给定的混合语音中提取所需的语音。我们的模型将基于SepFormer的语音分离网络与双模态线索网络相结合，该网络可以灵活处理音频和文本线索。为了训练和评估我们的模型，我们引入了一个新的数据集TextrolMix，该数据集包含混合语音和自然语言描述。实验结果表明，我们的方法不仅有效地根据“谁在说话”进行语音分离，还根据“他们如何说话”进行语音分离，增强了在缺少传统音频线索的场景下的TSE。相关演示请访问：[<a target="_blank" rel="noopener" href="https://mingyue66.github.io/TextrolMix/demo/]">https://mingyue66.github.io/TextrolMix/demo/]</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.09169v1">PDF</a> Accepted by ICASSP 2025</p>
<p><strong>Summary</strong><br>目标语音提取（TSE）通常依赖于说话人身份的明确线索，如注册音频、面部图像或视频，但这些信息并非总是可用。本文提出了一种文本引导的目标语音提取模型StyleTSE，它使用音频线索之外的文本描述的说话风格来从给定的混合声音中提取所需的语音。模型结合了SepFormer改进的自适应语音分离网络和双模态线索网络，可以灵活处理音频和文本线索。为了训练和评估模型，引入了新数据集TextrolMix包含混合语音和文本描述。实验结果表明，该方法不仅基于说话人身份，还基于说话方式有效地分离语音，提高了在没有传统音频线索的场景下的TSE性能。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>StyleTSE模型结合了语音分离网络和双模态线索网络，以处理音频和文本描述的说话风格。</li>
<li>模型使用自然语言的说话风格描述作为提取目标语音的额外线索。</li>
<li>引入新数据集TextrolMix用于训练和评估模型。</li>
<li>实验结果表明，StyleTSE能够在无传统音频线索的场景下有效分离语音。</li>
<li>该方法不仅能根据说话人身份分离语音，还能根据说话方式分离语音。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.09169">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-a512c54fa0008851d93f9ddf27671dde.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-678d35e67f0355c33ceb66b853f5b61e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ac36a0956e8ae263d4541e8eacfadb37.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-872034ccab8bb85311c77a4135f087cc.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="persoDA-Personalized-Data-Augmentation-forPersonalized-ASR"><a href="#persoDA-Personalized-Data-Augmentation-forPersonalized-ASR" class="headerlink" title="persoDA: Personalized Data Augmentation forPersonalized ASR"></a>persoDA: Personalized Data Augmentation forPersonalized ASR</h2><p><strong>Authors:Pablo Peso Parada, Spyros Fontalis, Md Asif Jalal, Karthikeyan Saravanan, Anastasios Drosou, Mete Ozay, Gil Ho Lee, Jungin Lee, Seokyeong Jung</strong></p>
<p>Data augmentation (DA) is ubiquitously used in training of Automatic Speech Recognition (ASR) models. DA offers increased data variability, robustness and generalization against different acoustic distortions. Recently, personalization of ASR models on mobile devices has been shown to improve Word Error Rate (WER). This paper evaluates data augmentation in this context and proposes persoDA; a DA method driven by user’s data utilized to personalize ASR. persoDA aims to augment training with data specifically tuned towards acoustic characteristics of the end-user, as opposed to standard augmentation based on Multi-Condition Training (MCT) that applies random reverberation and noises. Our evaluation with an ASR conformer-based baseline trained on Librispeech and personalized for VOICES shows that persoDA achieves a 13.9% relative WER reduction over using standard data augmentation (using random noise &amp; reverberation). Furthermore, persoDA shows 16% to 20% faster convergence over MCT. </p>
<blockquote>
<p>数据增强（DA）在自动语音识别（ASR）模型的训练中得到了广泛应用。DA提供了增加数据变化、稳健性和对不同声学失真的泛化能力。最近，在移动设备上进行ASR模型的个性化已被证明可以降低词错误率（WER）。本文在此背景下评估数据增强，并提出persoDA；一种利用用户数据驱动的DA方法，用于个性化ASR。persoDA旨在利用针对最终用户声学特性调整的数据来增强训练，而不是基于多条件训练（MCT）的标准增强，后者应用随机混响和噪声。我们在Librispeech上训练的基于语音识别的确认者基线进行了评估，并对人声进行了个性化处理，结果表明，与标准数据增强（使用随机噪声和混响）相比，persoDA实现了相对降低13.9%的WER。此外，与MCT相比，persoDA显示出更快的收敛速度，提高了16%至20%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.09113v1">PDF</a> Accepted at ICASSP 2025; “\c{opyright} 20XX IEEE. Personal use of   this material is permitted. Permission from IEEE must be obtained for all   other uses, in any current or future media, including reprinting&#x2F;republishing   this material for advertising or promotional purposes, creating new   collective works, for resale or redistribution to servers or lists, or reuse   of any copyrighted component of …”</p>
<p><strong>Summary</strong>：<br>数据增强（DA）广泛应用于语音识别（ASR）模型的训练，能够增加数据多样性、提高模型的稳健性和泛化能力。本文评估了个性化ASR模型中数据增强的效果，并提出一种基于用户数据驱动的数据增强方法——persoDA。该方法旨在通过针对最终用户的声学特性调整数据来增强训练，而不是基于多条件训练（MCT）应用随机混响和噪声。评估结果显示，与标准数据增强相比，persoDA在语音识别的词错误率（WER）上降低了13.9%，并且收敛速度提高了16%至20%。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>数据增强（DA）广泛应用于语音识别（ASR）模型的训练，可增加数据多样性和模型稳健性。</li>
<li>本文提出了基于用户数据驱动的数据增强方法——persoDA，旨在针对最终用户的声学特性调整数据。</li>
<li>persoDA相对于标准的多条件训练（MCT），更侧重于应用用户的个性化数据。</li>
<li>persoDA在词错误率（WER）上取得了显著的改进，相较于标准数据增强降低了13.9%。</li>
<li>persoDA还显示出比MCT更快的收敛速度，提高了16%至20%。</li>
<li>此方法对于个性化ASR模型的应用具有潜力，可以提高模型在特定用户声学特性上的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.09113">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0bbbde71e317cf41ad6bc76f3564cce7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8af275ab2812330140db0dc8942b5f41.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-077b60f4d996a62378f6026433dee485.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="VITA-1-5-Towards-GPT-4o-Level-Real-Time-Vision-and-Speech-Interaction"><a href="#VITA-1-5-Towards-GPT-4o-Level-Real-Time-Vision-and-Speech-Interaction" class="headerlink" title="VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction"></a>VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction</h2><p><strong>Authors:Chaoyou Fu, Haojia Lin, Xiong Wang, Yi-Fan Zhang, Yunhang Shen, Xiaoyu Liu, Yangze Li, Zuwei Long, Heting Gao, Ke Li, Long Ma, Xiawu Zheng, Rongrong Ji, Xing Sun, Caifeng Shan, Ran He</strong></p>
<p>Recent Multimodal Large Language Models (MLLMs) have typically focused on integrating visual and textual modalities, with less emphasis placed on the role of speech in enhancing interaction. However, speech plays a crucial role in multimodal dialogue systems, and implementing high-performance in both vision and speech tasks remains a significant challenge due to the fundamental modality differences. In this paper, we propose a carefully designed multi-stage training methodology that progressively trains LLM to understand both visual and speech information, ultimately enabling fluent vision and speech interaction. Our approach not only preserves strong vision-language capacity, but also enables efficient speech-to-speech dialogue capabilities without separate ASR and TTS modules, significantly accelerating multimodal end-to-end response speed. By comparing our method against state-of-the-art counterparts across benchmarks for image, video, and speech tasks, we demonstrate that our model is equipped with both strong visual and speech capabilities, making near real-time vision and speech interaction. </p>
<blockquote>
<p>最近的多模态大型语言模型（MLLMs）主要聚焦于整合视觉和文本模态，较少强调语音在增强交互中的作用。然而，语音在多模态对话系统中扮演关键角色，由于在基础模态上的差异，在视觉和语音任务中都实现高性能仍然是一个重大挑战。在本文中，我们提出了一种精心设计的多阶段训练方法论，逐步训练大型语言模型以理解视觉和语音信息，最终使流畅的视觉和语音交互成为可能。我们的方法不仅保留了强大的视觉语言功能，还实现了高效的语音对话能力，无需单独的自动语音识别（ASR）和文本到语音（TTS）模块，从而显著加快了多模态端到端的响应速度。通过与图像、视频和语音任务的最新前沿技术进行对比评估，我们证明了我们的模型具备强大的视觉和语音功能，可实现近乎实时的视觉和语音交互。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.01957v2">PDF</a> <a target="_blank" rel="noopener" href="https://github.com/VITA-MLLM/VITA">https://github.com/VITA-MLLM/VITA</a></p>
<p><strong>Summary</strong></p>
<p>近期多模态大型语言模型（MLLMs）在集成视觉和文本模态方面取得了进展，但对语音在增强交互中的作用重视不足。本文提出了一种精心设计的多阶段训练方法，逐步训练LLM理解视觉和语音信息，实现流畅的视听交互。该方法不仅保留了强大的视觉语言能力，还能实现高效的语音对话能力，无需额外的语音识别和文本转语音模块，显著加快多模态端到端的响应速度。对比现有先进技术，本文模型在图像、视频和语音任务基准测试中均表现出强大的视觉和语音能力，实现近乎实时的视听交互。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>近期多模态语言模型倾向于融合视觉和文本模态，但对语音在交互中的重要性关注不足。</li>
<li>语音在多媒体对话系统中起关键作用，实现视觉和语音任务的高性能是一个重大挑战。</li>
<li>提出了一种多阶段训练方法，使LLM能够理解视觉和语音信息，实现流畅的视听交互。</li>
<li>该方法保留了强大的视觉语言能力，并具备高效的语音对话能力，无需额外的ASR和TTS模块。</li>
<li>模型在图像、视频和语音任务基准测试中表现优异，具备强大的视觉和语音能力。</li>
<li>该模型加速了多模态端到端的响应速度，实现了近乎实时的视听交互。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.01957">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-58744d62eb0fbf66540b32115e33c365.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-458784e265fee8832f68789bf9d105f9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e262945a140a0fbcdb7ddae1a7741766.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-396c9589614e6d25affa510578046020.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-200583adacfa33383773d2e9f3835530.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Target-Speaker-ASR-with-Whisper"><a href="#Target-Speaker-ASR-with-Whisper" class="headerlink" title="Target Speaker ASR with Whisper"></a>Target Speaker ASR with Whisper</h2><p><strong>Authors:Alexander Polok, Dominik Klement, Matthew Wiesner, Sanjeev Khudanpur, Jan Černocký, Lukáš Burget</strong></p>
<p>We propose a novel approach to enable the use of large, single-speaker ASR models, such as Whisper, for target speaker ASR. The key claim of this method is that it is much easier to model relative differences among speakers by learning to condition on frame-level diarization outputs than to learn the space of all speaker embeddings. We find that adding even a single bias term per diarization output type before the first transformer block can transform single-speaker ASR models into target-speaker ASR models. Our approach also supports speaker-attributed ASR by sequentially generating transcripts for each speaker in a diarization output. This simplified method outperforms baseline speech separation and diarization cascade by 12.9 % absolute ORC-WER on the NOTSOFAR-1 dataset. </p>
<blockquote>
<p>我们提出了一种新型方法，旨在将大型单说话人语音识别模型（如Whisper）应用于目标说话人语音识别。该方法的关键主张是，通过学习以帧级聚类输出为条件，对说话人之间的相对差异进行建模，比学习所有说话人嵌入的空间更容易。我们发现，在第一个transformer块之前为每种聚类输出类型添加一个偏差项，就可以将单说话人语音识别模型转变为目标说话人语音识别模型。我们的方法还支持通过顺序生成每个说话人的字幕来进行属性化语音识别。在NOTSOFAR-1数据集上，该方法相对于基线语音分离和聚类串联方法，绝对ORC-WER提高了12.9%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.09543v2">PDF</a> Accepted to ICASSP 2025</p>
<p><strong>Summary</strong><br>     本文提出一种新方法，通过基于帧级别的分栏输出条件训练，使大型单一说话人自动语音识别模型（如Whisper）能够用于目标说话人自动语音识别。该方法的关键在于，通过建模不同说话人之间的相对差异来改进模型性能，而非学习所有说话人的嵌入空间。添加单一偏置项，在分栏输出类型之前进行预处理即可将单一说话人自动语音识别模型转化为目标说话人自动语音识别模型。此方法还具备生成每位说话人对应转写的功能。在NOTSOFAR-1数据集上，相较于基准语音分离和对齐级联方法，此方法在绝对误差率上提高了12.9%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出一种新方法使大型单一说话人ASR模型（如Whisper）可用于目标说话人ASR。</li>
<li>建模相对差异优于学习所有说话人的嵌入空间。</li>
<li>添加偏置项可简化模型转换过程。</li>
<li>方法支持基于分栏输出的说话人属性ASR。</li>
<li>该方法生成每位说话人的转写内容。</li>
<li>方法在NOTSOFAR-1数据集上性能显著提升，绝对误差率降低12.9%。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.09543">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-60d1f1cd17fe8838aa4c5e546466bceb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-eadb71e7c6b96b9c2775cebe2b686192.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fde5bc2b63aa96c3e15afef846c3a40b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-99978c7b3e1ac1901a437e58c3ca1a74.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-39ed844928da0dd31f7e2b8f60dc9387.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Toward-Any-to-Any-Emotion-Voice-Conversion-using-Disentangled-Diffusion-Framework"><a href="#Toward-Any-to-Any-Emotion-Voice-Conversion-using-Disentangled-Diffusion-Framework" class="headerlink" title="Toward Any-to-Any Emotion Voice Conversion using Disentangled Diffusion   Framework"></a>Toward Any-to-Any Emotion Voice Conversion using Disentangled Diffusion   Framework</h2><p><strong>Authors:Hsing-Hang Chou, Yun-Shao Lin, Ching-Chin Sung, Yu Tsao, Chi-Chun Lee</strong></p>
<p>Emotional Voice Conversion (EVC) aims to modify the emotional expression of speech for various applications, such as human-machine interaction. Previous deep learning-based approaches using generative adversarial networks and autoencoder models have shown promise but suffer from quality degradation and limited emotion control. To address these issues, a novel diffusion-based EVC framework with disentangled loss and expressive guidance is proposed. Our method separates speaker and emotional features to maintain speech quality while improving emotional expressiveness. Tested on real-world and acted-out datasets, the approach achieved significant improvements in emotion classification accuracy for both in-the-wild and act-out datasets and showed reduced distortion compared to state-of-the-art models. </p>
<blockquote>
<p>情感语音转换（EVC）旨在修改语音的情感表达，以应用于人机交互等各种场景。虽然基于深度学习的对抗生成网络和自编码器模型的方法已经展现出一定的潜力，但它们仍面临质量下降和情感控制有限的问题。为了解决这些问题，提出了一种新型的基于扩散的EVC框架，该框架具有分离损失和表达指导的特点。我们的方法能够分离说话者和情感特征，从而保持语音质量，同时提高情感表现力。在真实世界和演绎数据集上进行测试，该方法在野生和演绎数据集的情感分类精度上取得了显著的提高，与最先进的模型相比，失真度也有所降低。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.03636v3">PDF</a> 5 pages; revised arguments, typos and references corrected</p>
<p><strong>Summary</strong>：情感语音转换（EVC）旨在修改语音的情感表达，以应用于人机交互等各种场景。针对以往基于深度学习的对抗生成网络和自编码器模型在情感控制上的不足和语音质量下降的问题，提出了新型扩散模型基础的EVC框架，具有分离损失和表现性指导的特性，可保持语音质量的同时提升情感表达力。在真实世界和模拟数据集上的测试显示，该方法在野外和模拟数据集的情感分类准确率上实现了显著提升，与当前主流模型相比失真度降低。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>情感语音转换（EVC）旨在修改语音的情感表达，应用于人机交互等场景。</li>
<li>以往的深度学习方法存在语音质量下降和有限情感控制的问题。</li>
<li>新提出的扩散模型基础的EVC框架旨在解决上述问题。</li>
<li>新框架通过分离损失和表现性指导来保持语音质量并提升情感表达力。</li>
<li>在真实世界和模拟数据集上的测试显示新框架的情感分类准确率显著提升。</li>
<li>与现有主流模型相比，新框架的语音失真度较低。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.03636">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-5eee8dc7a149e1d99b0453b1a7536e84.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8d57b6a038fb3ddd18bf01eee492b48c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b2dc559ebef309472ac40db95c8125a0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ead311cf31d4c3ba6fe3951b36625387.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-18/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-18/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-18/GAN/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-4a0328cfb43a3f15aee87113989fcf27.jpg" class="responsive-img" alt="GAN">
                        
                        <span class="card-title">GAN</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            GAN 方向最新论文已更新，请持续关注 Update in 2025-01-18  iFADIT Invertible Face Anonymization via Disentangled Identity   Transform
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-18
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/GAN/" class="post-category">
                                    GAN
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/GAN/">
                        <span class="chip bg-color">GAN</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-18/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F_Breast%20Ultrasound/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-feb68adb274eb01e606b2bd8c5e3883d.jpg" class="responsive-img" alt="医学影像/Breast Ultrasound">
                        
                        <span class="card-title">医学影像/Breast Ultrasound</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学影像/Breast Ultrasound 方向最新论文已更新，请持续关注 Update in 2025-01-18  Efficient Few-Shot Medical Image Analysis via Hierarchical Contrastive   Vision-Language Learning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-18
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/" class="post-category">
                                    医学影像/Breast Ultrasound
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/">
                        <span class="chip bg-color">医学影像/Breast Ultrasound</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">14643.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
