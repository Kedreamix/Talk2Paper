<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM 方向最新论文已更新，请持续关注 Update in 2025-01-18  Distilling Multi-modal Large Language Models for Autonomous Driving">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-5f828c668f0ca27558841cc1b0debd93.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-01-18
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-01-18
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    18.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    74 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-01-18-更新"><a href="#2025-01-18-更新" class="headerlink" title="2025-01-18 更新"></a>2025-01-18 更新</h1><h2 id="Distilling-Multi-modal-Large-Language-Models-for-Autonomous-Driving"><a href="#Distilling-Multi-modal-Large-Language-Models-for-Autonomous-Driving" class="headerlink" title="Distilling Multi-modal Large Language Models for Autonomous Driving"></a>Distilling Multi-modal Large Language Models for Autonomous Driving</h2><p><strong>Authors:Deepti Hegde, Rajeev Yasarla, Hong Cai, Shizhong Han, Apratim Bhattacharyya, Shweta Mahajan, Litian Liu, Risheek Garrepalli, Vishal M. Patel, Fatih Porikli</strong></p>
<p>Autonomous driving demands safe motion planning, especially in critical “long-tail” scenarios. Recent end-to-end autonomous driving systems leverage large language models (LLMs) as planners to improve generalizability to rare events. However, using LLMs at test time introduces high computational costs. To address this, we propose DiMA, an end-to-end autonomous driving system that maintains the efficiency of an LLM-free (or vision-based) planner while leveraging the world knowledge of an LLM. DiMA distills the information from a multi-modal LLM to a vision-based end-to-end planner through a set of specially designed surrogate tasks. Under a joint training strategy, a scene encoder common to both networks produces structured representations that are semantically grounded as well as aligned to the final planning objective. Notably, the LLM is optional at inference, enabling robust planning without compromising on efficiency. Training with DiMA results in a 37% reduction in the L2 trajectory error and an 80% reduction in the collision rate of the vision-based planner, as well as a 44% trajectory error reduction in longtail scenarios. DiMA also achieves state-of-the-art performance on the nuScenes planning benchmark. </p>
<blockquote>
<p>自动驾驶需要安全的运动规划，特别是在关键的“长尾”场景中。最近的端到端自动驾驶系统利用大型语言模型（LLM）作为规划器，以提高对罕见事件的通用性。然而，在测试时使用LLM会引入较高的计算成本。为了解决这一问题，我们提出了DiMA，这是一个端到端的自动驾驶系统，它能在不依赖LLM（或基于视觉）的规划器的情况下保持效率，同时利用LLM的世界知识。DiMA通过一系列专门设计的替代任务，将从多模态LLM中提取的信息蒸馏到基于视觉的端到端规划器中。在联合训练策略下，两个网络共用的场景编码器产生结构化表示，这些表示在语义上是固定的，并与最终的规划目标对齐。值得注意的是，推理过程中LLM是可选的，能够在不损害效率的情况下实现稳健的规划。通过DiMA进行训练，实现了基于视觉的规划器的L2轨迹误差降低37%，碰撞率降低80%，以及在长尾场景中的轨迹误差降低44%。DiMA还在nuScenes规划基准测试上达到了最先进的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.09757v1">PDF</a> </p>
<p><strong>Summary</strong>：</p>
<p>自主驾驶需要安全运动规划，特别是在关键的“长尾”场景中。近期端到端的自主驾驶系统利用大型语言模型（LLM）作为规划器，以提高对罕见事件的泛化能力。然而，使用LLM进行测试时会产生高计算成本。为解决这一问题，我们提出了DiMA系统，它是一个端到端的自主驾驶系统，在保持无LLM（或基于视觉）规划器的效率的同时，利用LLM的世界知识。DiMA通过一系列专门设计的替代任务，从多模态LLM中提炼信息到基于视觉的端到端规划器。在联合训练策略下，一个用于两者的场景编码器产生结构化的表示形式，这些表示既语义上地根植于知识又符合最终的规划目标。值得注意的是，LLM在推断中是可选的，能够实现高效而稳健的规划。经过DiMA训练后，基于视觉的规划器的L2轨迹误差降低了37%，碰撞率降低了80%，长尾场景中的轨迹误差降低了44%。DiMA还在nuScenes规划基准测试中达到了业界领先水平。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>自主驾驶需要安全运动规划，特别是在长尾场景中。</li>
<li>端到端的自主驾驶系统利用大型语言模型（LLM）提高泛化能力。</li>
<li>使用LLM进行测试会导致高计算成本。</li>
<li>DiMA系统能够在无LLM的情况下保持规划效率并引入LLM的世界知识。</li>
<li>DiMA通过专门设计的替代任务将LLM信息提炼并应用到基于视觉的规划器中。</li>
<li>DiMA训练后显著提高基于视觉的规划器性能，包括减少轨迹误差和碰撞率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.09757">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a190b935718895415022ec0a583d3165.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ba88a0517c3d6d77a42eeae9fa4f5b65.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9227c617802f3a4ff25dfae781018582.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f4fa68d5acf5aae544036e34c7a792e4.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Lost-in-Translation-Found-in-Context-Sign-Language-Translation-with-Contextual-Cues"><a href="#Lost-in-Translation-Found-in-Context-Sign-Language-Translation-with-Contextual-Cues" class="headerlink" title="Lost in Translation, Found in Context: Sign Language Translation with   Contextual Cues"></a>Lost in Translation, Found in Context: Sign Language Translation with   Contextual Cues</h2><p><strong>Authors:Youngjoon Jang, Haran Raajesh, Liliane Momeni, Gül Varol, Andrew Zisserman</strong></p>
<p>Our objective is to translate continuous sign language into spoken language text. Inspired by the way human interpreters rely on context for accurate translation, we incorporate additional contextual cues together with the signing video, into a new translation framework. Specifically, besides visual sign recognition features that encode the input video, we integrate complementary textual information from (i) captions describing the background show, (ii) translation of previous sentences, as well as (iii) pseudo-glosses transcribing the signing. These are automatically extracted and inputted along with the visual features to a pre-trained large language model (LLM), which we fine-tune to generate spoken language translations in text form. Through extensive ablation studies, we show the positive contribution of each input cue to the translation performance. We train and evaluate our approach on BOBSL – the largest British Sign Language dataset currently available. We show that our contextual approach significantly enhances the quality of the translations compared to previously reported results on BOBSL, and also to state-of-the-art methods that we implement as baselines. Furthermore, we demonstrate the generality of our approach by applying it also to How2Sign, an American Sign Language dataset, and achieve competitive results. </p>
<blockquote>
<p>我们的目标是将连续的手语翻译成口语文本。受人类翻译者依赖上下文进行准确翻译的启发，我们将额外的上下文线索与手语视频结合，融入新的翻译框架。具体来说，除了编码输入视频的视觉手语识别特征外，我们还整合了（i）描述背景显示的字幕、（ii）之前句子的翻译以及（iii）手语转录的伪语音等补充文本信息。这些自动提取并与视觉特征一起输入到预训练的大型语言模型（LLM）中，我们对模型进行微调以生成口语形式的文本翻译。通过广泛的消融研究，我们证明了每个输入线索对翻译性能的积极贡献。我们在BOBSL（当前可用的最大英国手语数据集）上对我们的方法进行了训练和评估。我们证明了我们的上下文方法在BOBSL上的翻译质量相比以前报告的结果以及我们实现的最新方法作为基准线有显著提升。此外，我们将该方法应用于How2Sign（美国手语数据集），并获得了具有竞争力的结果，展示了其通用性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.09754v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文主要介绍了一种将连续手语实时翻译成文字的新方法。该方法借鉴了人类翻译依赖语境进行准确翻译的方式，结合手语视频中的额外语境线索，构建了一个新的翻译框架。通过整合手语识别特征、背景描述字幕、先前句子的翻译以及手语转录的伪术语等文本信息，输入到预训练的大型语言模型中，生成口语形式的翻译。在BOBSL数据集上的实验表明，该方法显著提高了翻译质量，并在How2Sign数据集上取得了具有竞争力的结果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究目标是将连续手语实时翻译成文字。</li>
<li>借鉴人类翻译依赖语境的方式，结合手语视频的额外语境线索构建新的翻译框架。</li>
<li>通过整合多种文本信息（手语识别特征、背景描述字幕等）来提高翻译准确性。</li>
<li>使用预训练的大型语言模型（LLM），并进行微调以生成口语形式的翻译。</li>
<li>在BOBSL数据集上的实验证明了该方法的有效性，提高了翻译质量。</li>
<li>将该方法应用于How2Sign数据集，取得了具有竞争力的结果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.09754">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d3fa6c22b0695c7556f700427b7366a4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0f4ff25d2c354490eee8d63a7df8b1e1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3c22852dc98eabead8edd925fc48a176.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Enhancing-Lexicon-Based-Text-Embeddings-with-Large-Language-Models"><a href="#Enhancing-Lexicon-Based-Text-Embeddings-with-Large-Language-Models" class="headerlink" title="Enhancing Lexicon-Based Text Embeddings with Large Language Models"></a>Enhancing Lexicon-Based Text Embeddings with Large Language Models</h2><p><strong>Authors:Yibin Lei, Tao Shen, Yu Cao, Andrew Yates</strong></p>
<p>Recent large language models (LLMs) have demonstrated exceptional performance on general-purpose text embedding tasks. While dense embeddings have dominated related research, we introduce the first Lexicon-based EmbeddiNgS (LENS) leveraging LLMs that achieve competitive performance on these tasks. Regarding the inherent tokenization redundancy issue and unidirectional attention limitations in traditional causal LLMs, LENS consolidates the vocabulary space through token embedding clustering, and investigates bidirectional attention and various pooling strategies. Specifically, LENS simplifies lexicon matching by assigning each dimension to a specific token cluster, where semantically similar tokens are grouped together, and unlocking the full potential of LLMs through bidirectional attention. Extensive experiments demonstrate that LENS outperforms dense embeddings on the Massive Text Embedding Benchmark (MTEB), delivering compact feature representations that match the sizes of dense counterparts. Notably, combining LENSE with dense embeddings achieves state-of-the-art performance on the retrieval subset of MTEB (i.e. BEIR). </p>
<blockquote>
<p>最近的大型语言模型（LLM）在通用文本嵌入任务上表现出了卓越的性能。虽然密集嵌入在相关研究中长期占据主导地位，但我们引入了首款基于词典的嵌入（LENS），利用LLM在这些任务上实现了竞争性的性能。针对传统因果LLM中的固有分词冗余问题和单向注意力限制，LENS通过令牌嵌入聚类整合词汇空间，并研究双向注意力和各种池策略。具体来说，LENS通过为每个维度分配特定的令牌集群来简化词典匹配，其中语义相似的令牌被组合在一起，并通过双向注意力释放LLM的全部潜力。大量实验表明，LENS在大型文本嵌入基准测试（MTEB）上的性能优于密集嵌入，提供紧凑的特征表示，与密集对应的规模相匹配。值得注意的是，将LENS与密集嵌入相结合，在MTEB的检索子集上达到了最先进的性能（即BEIR）。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.09749v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>近期的大型语言模型（LLM）在通用文本嵌入任务上表现出卓越性能。尽管密集嵌入相关研究占据主导，我们首次引入利用LLM的Lexicon-based EmbeddiNgS（LENS），在这些任务上实现具有竞争力的表现。针对传统因果LLM中的内在令牌化冗余问题和单向注意力限制，LENS通过令牌嵌入聚类整合词汇空间，并研究双向注意力以及各种池策略。具体来说，LENS简化了词汇匹配过程，将每个维度分配给特定的令牌簇，其中语义相似的令牌会被组合在一起，从而解锁LLM的潜力通过双向注意力。大量实验表明，LENS在大型文本嵌入基准测试（MTEB）上的性能优于密集嵌入，产生紧凑的特征表示，可与密集对应的规模相匹配。值得注意的是，将LENS与密集嵌入相结合在MTEB检索子集上达到了最先进的性能（即BEIR）。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMs在通用文本嵌入任务上表现卓越。</li>
<li>引入了一种新的基于LLM的文本嵌入方法——Lexicon-based EmbeddiNgS（LENS）。</li>
<li>LENS解决了传统LLM中的令牌化冗余和单向注意力限制问题。</li>
<li>LENS通过令牌嵌入聚类整合词汇空间，并采用双向注意力和池策略。</li>
<li>LENS简化了词汇匹配过程，将语义相似的令牌组合在一起。</li>
<li>LENS在大型文本嵌入基准测试（MTEB）上的性能优于密集嵌入。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.09749">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d6a1faf3407aeb18a7e4efaf7637a204.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-17a9385a7e749c7e053d81f2c189ef44.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d7f1b8b997b61ae7c3f306432a33310c.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Generating-particle-physics-Lagrangians-with-transformers"><a href="#Generating-particle-physics-Lagrangians-with-transformers" class="headerlink" title="Generating particle physics Lagrangians with transformers"></a>Generating particle physics Lagrangians with transformers</h2><p><strong>Authors:Yong Sheng Koay, Rikard Enberg, Stefano Moretti, Eliel Camargo-Molina</strong></p>
<p>In physics, Lagrangians provide a systematic way to describe laws governing physical systems. In the context of particle physics, they encode the interactions and behavior of the fundamental building blocks of our universe. By treating Lagrangians as complex, rule-based constructs similar to linguistic expressions, we trained a transformer model – proven to be effective in natural language tasks – to predict the Lagrangian corresponding to a given list of particles. We report on the transformer’s performance in constructing Lagrangians respecting the Standard Model $\mathrm{SU}(3)\times \mathrm{SU}(2)\times \mathrm{U}(1)$ gauge symmetries. The resulting model is shown to achieve high accuracies (over 90%) with Lagrangians up to six matter fields, with the capacity to generalize beyond the training distribution, albeit within architectural constraints. We show through an analysis of input embeddings that the model has internalized concepts such as group representations and conjugation operations as it learned to generate Lagrangians. We make the model and training datasets available to the community. An interactive demonstration can be found at: \url{<a target="_blank" rel="noopener" href="https://huggingface.co/spaces/JoseEliel/generate-lagrangians%7D">https://huggingface.co/spaces/JoseEliel/generate-lagrangians}</a>. </p>
<blockquote>
<p>在物理学中，拉格朗日量为描述物理系统的规律提供了一种系统的方法。在粒子物理学的背景下，它们编码了我们宇宙基本构成要素的相互作用和行为。通过把拉格朗日量视为复杂的、基于规则的结构，类似于语言表达，我们训练了一种变压器模型——在自然语言任务中被证明是有效的——来预测对应于给定粒子列表的拉格朗日量。我们报告了变压器在构建尊重标准模型$\mathrm{SU}(3)\times \mathrm{SU}(2)\times \mathrm{U}(1)$规范对称性的拉格朗日量时的性能。结果模型显示，在拉格朗日量达到六个物质场的情况下，其准确率高达90%以上，虽然在架构约束内，但其能力可以推广到训练分布之外。通过对输入嵌入的分析，我们表明该模型已经内化了诸如群表示和共轭运算等概念，因为它学会了生成拉格朗日量。我们向社区提供模型和训练数据集。一个交互式演示可以在以下网址找到：<a target="_blank" rel="noopener" href="https://huggingface.co/spaces/JoseEliel/generate-lagrangians">https://huggingface.co/spaces/JoseEliel/generate-lagrangians</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.09729v1">PDF</a> 32 pages, 11 figues, 18 tables</p>
<p><strong>Summary</strong></p>
<p>物理学中，拉格朗日函数提供了一种描述物理系统规律的系统方法。在粒子物理学背景下，它们编码了宇宙基本构建块的相互作用和行为。本文提出将拉格朗日函数视为复杂的规则结构，类似于语言表达式，并训练了一种已在自然语言任务中表现优异的变压器模型来预测给定粒子列表对应的拉格朗日函数。本文报道了该模型在遵循标准模型SU(3)×SU(2)×U(1)规范对称性的前提下构建拉格朗日的性能。结果表明，该模型在生成涉及多达六个物质场的拉格朗日函数时，准确率超过90%，并在建筑约束内具有超越训练分布的概括能力。通过对输入嵌入的分析，我们展示了该模型已经掌握了诸如群表示和共轭运算等概念。我们向公众提供了模型和训练数据集。一个交互式演示可以在链接中找到：<a target="_blank" rel="noopener" href="https://huggingface.co/spaces/JoseEliel/generate-lagrangians">https://huggingface.co/spaces/JoseEliel/generate-lagrangians</a>。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>拉格朗日函数在物理学中用于描述物理系统的规律，尤其是在粒子物理学中描述基本粒子的相互作用和行为。</li>
<li>通过将拉格朗日函数视为语言表达式并应用变压器模型，可以实现给定粒子列表的拉格朗日预测。</li>
<li>模型在遵循标准模型SU(3)×SU(2)×U(1)规范对称性的条件下表现出高性能。</li>
<li>模型在生成涉及多达六个物质场的拉格朗日函数时准确率超过90%。</li>
<li>模型具有概括能力，能够在建筑约束内超越训练分布。</li>
<li>输入嵌入分析显示模型掌握了群表示和共轭运算等概念。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.09729">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-a05d1675bd4a28c614c41a4a85325707.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="LLM-Based-Routing-in-Mixture-of-Experts-A-Novel-Framework-for-Trading"><a href="#LLM-Based-Routing-in-Mixture-of-Experts-A-Novel-Framework-for-Trading" class="headerlink" title="LLM-Based Routing in Mixture of Experts: A Novel Framework for Trading"></a>LLM-Based Routing in Mixture of Experts: A Novel Framework for Trading</h2><p><strong>Authors:Kuan-Ming Liu, Ming-Chih Lo</strong></p>
<p>Recent advances in deep learning and large language models (LLMs) have facilitated the deployment of the mixture-of-experts (MoE) mechanism in the stock investment domain. While these models have demonstrated promising trading performance, they are often unimodal, neglecting the wealth of information available in other modalities, such as textual data. Moreover, the traditional neural network-based router selection mechanism fails to consider contextual and real-world nuances, resulting in suboptimal expert selection. To address these limitations, we propose LLMoE, a novel framework that employs LLMs as the router within the MoE architecture. Specifically, we replace the conventional neural network-based router with LLMs, leveraging their extensive world knowledge and reasoning capabilities to select experts based on historical price data and stock news. This approach provides a more effective and interpretable selection mechanism. Our experiments on multimodal real-world stock datasets demonstrate that LLMoE outperforms state-of-the-art MoE models and other deep neural network approaches. Additionally, the flexible architecture of LLMoE allows for easy adaptation to various downstream tasks. </p>
<blockquote>
<p>最近深度学习和大语言模型（LLM）的进步促进了专家混合（MoE）机制在股票投资领域的应用。虽然这些模型已经表现出了有前景的交易性能，但它们通常是单模态的，忽略了其他模态（如文本数据）中丰富的信息。此外，基于传统神经网络的路由器选择机制没有考虑到上下文和现实世界中的细微差别，导致专家选择不佳。为了解决这些局限性，我们提出了LLMoE这一新型框架，它采用LLM作为MoE架构中的路由器。具体来说，我们用LLM替代了基于常规神经网络的路由器，利用其丰富的世界知识和推理能力，根据历史价格数据和股票新闻选择专家。这种方法提供了一个更有效和可解释的选择机制。我们在多模态现实世界股票数据集上的实验表明，LLMoE优于最新的MoE模型和其他深度神经网络方法。此外，LLMoE的灵活架构可轻松适应各种下游任务。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.09636v1">PDF</a> Accepted by AAAI 2025 Workshop on AI for Social Impact - Bridging   Innovations in Finance, Social Media, and Crime Prevention</p>
<p><strong>Summary</strong><br>     深度学习与大型语言模型（LLM）的最新进展促进了混合专家（MoE）机制在股票投资领域的应用。传统神经网络路由选择机制缺乏上下文与现实世界的细微差别，导致专家选择不尽人意。为此，我们提出LLMoE框架，采用LLM作为MoE架构中的路由器，基于历史价格数据与股票新闻选择专家。实验证明，LLMoE在模态现实股票数据集上的表现优于最先进MoE模型和其他深度神经网络方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>深度学习及大型语言模型（LLM）的进展推动了混合专家（MoE）机制在股票投资中的应用。</li>
<li>传统神经网络路由选择机制存在局限性，无法充分考虑上下文与现实世界的细微差别。</li>
<li>LLMoE框架采用LLM作为路由器，基于历史价格数据与股票新闻选择专家。</li>
<li>LLMoE框架在模态现实股票数据集上的表现优于其他方法。</li>
<li>LLMoE框架具有灵活性和易适应性，可轻松适应各种下游任务。</li>
<li>LLMs的丰富知识和推理能力在股票投资领域具有应用价值。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.09636">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-cac5ea9a6cfb32ecc1e56317b879882f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c3a42e88b738b756ac1e6357b982a80a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-70c476004962a7c3351e42d261325042.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e532199298767c7cd68ed582970b40fe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-237f0e21d645cca6f05c865b282dd8ce.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="From-Scarcity-to-Capability-Empowering-Fake-News-Detection-in-Low-Resource-Languages-with-LLMs"><a href="#From-Scarcity-to-Capability-Empowering-Fake-News-Detection-in-Low-Resource-Languages-with-LLMs" class="headerlink" title="From Scarcity to Capability: Empowering Fake News Detection in   Low-Resource Languages with LLMs"></a>From Scarcity to Capability: Empowering Fake News Detection in   Low-Resource Languages with LLMs</h2><p><strong>Authors:Hrithik Majumdar Shibu, Shrestha Datta, Md. Sumon Miah, Nasrullah Sami, Mahruba Sharmin Chowdhury, Md. Saiful Islam</strong></p>
<p>The rapid spread of fake news presents a significant global challenge, particularly in low-resource languages like Bangla, which lack adequate datasets and detection tools. Although manual fact-checking is accurate, it is expensive and slow to prevent the dissemination of fake news. Addressing this gap, we introduce BanFakeNews-2.0, a robust dataset to enhance Bangla fake news detection. This version includes 11,700 additional, meticulously curated fake news articles validated from credible sources, creating a proportional dataset of 47,000 authentic and 13,000 fake news items across 13 categories. In addition, we created a manually curated independent test set of 460 fake and 540 authentic news items for rigorous evaluation. We invest efforts in collecting fake news from credible sources and manually verified while preserving the linguistic richness. We develop a benchmark system utilizing transformer-based architectures, including fine-tuned Bidirectional Encoder Representations from Transformers variants (F1-87%) and Large Language Models with Quantized Low-Rank Approximation (F1-89%), that significantly outperforms traditional methods. BanFakeNews-2.0 offers a valuable resource to advance research and application in fake news detection for low-resourced languages. We publicly release our dataset and model on Github to foster research in this direction. </p>
<blockquote>
<p>虚假新闻的迅速传播给全球带来了重大挑战，特别是在像孟加拉语这样的资源匮乏的语言中，缺乏足够的数据集和检测工具。虽然手动事实核查是准确的，但其在阻止虚假新闻的传播方面是昂贵且缓慢的。为了解决这一差距，我们推出了BanFakeNews-2.0，这是一个增强孟加拉语虚假新闻检测能力的稳健数据集。此版本新增了11,700篇精心策划的虚假新闻文章，这些文章来自可靠来源并经过了验证，创建了包含47,000篇真实和13,000篇虚假新闻文章的均衡数据集，涵盖13个类别。此外，我们还创建了包含460篇虚假新闻和540篇真实新闻的独立测试集，以便进行严格评估。我们努力从可靠来源收集虚假新闻，并进行手动验证，同时保留语言的丰富性。我们开发了一个基于Transformer架构的基准系统，包括微调后的双向编码器表示（F1-87％）和具有量化低秩逼近的大型语言模型（F1-89％），该系统显著优于传统方法。BanFakeNews-2.0为推进虚假新闻检测研究与应用提供了宝贵资源，特别是对于资源匮乏的语言。我们在GitHub上公开发布了我们的数据集和模型，以促进这一方向的研究。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.09604v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>Bangla语由于缺乏足够的语料库和检测工具，假新闻的快速传播成为一个重大挑战。尽管手动事实核查很准确，但其高昂的成本和缓慢的进度无法阻止假新闻的散布。为解决这一问题，推出BanFakeNews-2.0数据集，旨在增强Bangla假新闻的检测能力。此版本新增了来自可靠来源的11700篇精心策划的假新闻文章，创建了包含比例适当的语料库，涵盖真实新闻47000篇和假新闻13000篇，共涉及13个类别。同时创建了一个独立的手动验证测试集，包括460篇假新闻和540篇真实新闻，用于严格的评估。我们致力于收集来自可靠来源的假新闻并进行手动验证，同时保留语言的丰富性。我们开发了一个基于Transformer架构的基准系统，包括微调后的双向编码器表示（F1-87%）和低秩近似量化的大型语言模型（F1-89%），显著优于传统方法。BanFakeNews-2.0对于推动低资源语言假新闻检测的研究和应用具有重要意义。我们在GitHub上公开发布数据集和模型，以促进这一方向的研究发展。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>假新闻在全球范围内的传播构成重大挑战，特别是在缺乏数据和检测工具的Bangla等低资源语言中尤为明显。</li>
<li>手动事实核查虽然准确但成本高昂且进度缓慢，难以满足阻止假新闻传播的需求。</li>
<li>推出BanFakeNews-2.0数据集，包含真实新闻和假新闻文章数量得到显著扩充，便于构建更为完善的检测模型。</li>
<li>采用基于Transformer架构开发基准系统，包括微调双向编码器表示和低秩近似量化的语言模型等先进技术，显著提高假新闻检测准确率。</li>
<li>手动验证确保数据集和模型的有效性和准确性，强调语言学上的丰富性对模型训练的重要性。</li>
<li>数据集及模型的公开提供有利于低资源语言假新闻检测研究的进步与应用开发。此举可通过共享平台和资源加强该领域的协作与创新。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.09604">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-d3c295b8a4447045bd2d6a56d1b816fb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e522a19e59c550cd46c787aa2ad8b83e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d5985ad83a99bc70f3ae4b808a4298ac.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9cb4d7872b4e78b9f2f6bf2ece85b4a7.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Atleus-Accelerating-Transformers-on-the-Edge-Enabled-by-3D-Heterogeneous-Manycore-Architectures"><a href="#Atleus-Accelerating-Transformers-on-the-Edge-Enabled-by-3D-Heterogeneous-Manycore-Architectures" class="headerlink" title="Atleus: Accelerating Transformers on the Edge Enabled by 3D   Heterogeneous Manycore Architectures"></a>Atleus: Accelerating Transformers on the Edge Enabled by 3D   Heterogeneous Manycore Architectures</h2><p><strong>Authors:Pratyush Dhingra, Janardhan Rao Doppa, Partha Pratim Pande</strong></p>
<p>Transformer architectures have become the standard neural network model for various machine learning applications including natural language processing and computer vision. However, the compute and memory requirements introduced by transformer models make them challenging to adopt for edge applications. Furthermore, fine-tuning pre-trained transformers (e.g., foundation models) is a common task to enhance the model’s predictive performance on specific tasks&#x2F;applications. Existing transformer accelerators are oblivious to complexities introduced by fine-tuning. In this paper, we propose the design of a three-dimensional (3D) heterogeneous architecture referred to as Atleus that incorporates heterogeneous computing resources specifically optimized to accelerate transformer models for the dual purposes of fine-tuning and inference. Specifically, Atleus utilizes non-volatile memory and systolic array for accelerating transformer computational kernels using an integrated 3D platform. Moreover, we design a suitable NoC to achieve high performance and energy efficiency. Finally, Atleus adopts an effective quantization scheme to support model compression. Experimental results demonstrate that Atleus outperforms existing state-of-the-art by up to 56x and 64.5x in terms of performance and energy efficiency respectively </p>
<blockquote>
<p>Transformer架构已成为各种机器学习应用的标准神经网络模型，包括自然语言处理和计算机视觉。然而，Transformer模型带来的计算和内存要求，使得其在边缘应用的采用面临挑战。此外，微调预训练的Transformer（例如基础模型）是一个常见任务，旨在提高模型在特定任务&#x2F;应用上的预测性能。现有的Transformer加速器对微调引入的复杂性缺乏认识。在本文中，我们提出了一种名为Atleus的三维异构架构设计，该架构结合了异构计算资源，专门用于加速Transformer模型的微调与推理。具体来说，Atleus利用非易失性内存和脉动阵列，通过集成三维平台来加速Transformer计算内核。此外，我们设计了一种合适的网络芯片互联技术（NoC），以实现高性能和能效。最后，Atleus采用有效的量化方案来支持模型压缩。实验结果表明，在性能和能效方面，Atleus分别达到了最高56倍和最高至64.5倍的最佳状态</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.09588v1">PDF</a> Accepted for Publication in IEEE Transactions on Computer-Aided   Design of Integrated Circuits and Systems (TCAD)</p>
<p><strong>Summary</strong></p>
<p>本文提出了一种名为Atleus的三维异构架构，该架构专门优化了用于加速转换器模型的微调与推断任务。通过利用非易失性内存和脉动阵列，Atleus在一个集成化的三维平台上加速转换器计算内核。此外，设计了一种高效的NoC，以实现高性能和能源效率。Atleus还采用有效的量化方案以支持模型压缩，并且实验结果表明其性能优于现有技术，性能提升最高达56倍，能效提升最高达64.5倍。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Transformer架构已成为各种机器学习应用的标准神经网络模型，包括自然语言处理和计算机视觉。</li>
<li>转换器模型对计算和内存的需求使其难以在边缘应用中使用。</li>
<li>对预训练的转换器进行微调是增强模型对特定任务&#x2F;应用预测性能的一种常见任务。现有的转换器加速器忽略了微调带来的复杂性。</li>
<li>本文提出了一种名为Atleus的三维异构架构，旨在加速转换器模型的微调与推断。</li>
<li>Atleus利用非易失性内存和脉动阵列来加速转换器计算内核。</li>
<li>Atleus设计了一种高效的NoC，以实现高性能和能源效率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.09588">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-37e0d474b6e42ed44c48f968b00acc96.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ca75bda4b44efd596708fc25793fc318.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-38828c5f78085617cca12dd2e5cd763a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2b6ae7fa95ba4a876ca7a26f6689b52e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0272fc103a38ce4eb48c30bbe6db9e55.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7528a67c943a01071c7cad6742d824f9.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Confidence-Estimation-for-Error-Detection-in-Text-to-SQL-Systems"><a href="#Confidence-Estimation-for-Error-Detection-in-Text-to-SQL-Systems" class="headerlink" title="Confidence Estimation for Error Detection in Text-to-SQL Systems"></a>Confidence Estimation for Error Detection in Text-to-SQL Systems</h2><p><strong>Authors:Oleg Somov, Elena Tutubalina</strong></p>
<p>Text-to-SQL enables users to interact with databases through natural language, simplifying the retrieval and synthesis of information. Despite the success of large language models (LLMs) in converting natural language questions into SQL queries, their broader adoption is limited by two main challenges: achieving robust generalization across diverse queries and ensuring interpretative confidence in their predictions. To tackle these issues, our research investigates the integration of selective classifiers into Text-to-SQL systems. We analyse the trade-off between coverage and risk using entropy based confidence estimation with selective classifiers and assess its impact on the overall performance of Text-to-SQL models. Additionally, we explore the models’ initial calibration and improve it with calibration techniques for better model alignment between confidence and accuracy. Our experimental results show that encoder-decoder T5 is better calibrated than in-context-learning GPT 4 and decoder-only Llama 3, thus the designated external entropy-based selective classifier has better performance. The study also reveal that, in terms of error detection, selective classifier with a higher probability detects errors associated with irrelevant questions rather than incorrect query generations. </p>
<blockquote>
<p>文本到SQL使用户能够通过自然语言与数据库进行交互，简化了信息的检索和合成。尽管大型语言模型（LLM）在将自然语言问题转换为SQL查询方面取得了成功，但它们更广泛的采用受到两个主要挑战的限制：实现跨不同查询的稳健泛化，以及确保对其预测的解释信心。为了应对这些问题，我们的研究探讨了选择性分类器在文本到SQL系统中的集成。我们使用基于熵的置信估计来分析覆盖率和风险之间的权衡，并评估其对文本到SQL模型整体性能的影响。此外，我们探索了模型的初始校准，并通过校准技术对其进行改进，以更好地实现置信和准确性之间的模型对齐。实验结果表明，编码器-解码器T5的校准效果优于上下文学习GPT 4和仅解码器Llama 3，因此指定的外部基于熵的选择性分类器具有更好的性能。该研究还表明，就错误检测而言，具有较高概率的选择性分类器更擅长检测与无关问题相关的错误，而不是不正确的查询生成。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.09527v1">PDF</a> 15 pages, 11 figures, to be published in AAAI 2025 Proceedings</p>
<p><strong>Summary</strong></p>
<p>文本介绍了如何通过集成选择性分类器来解决文本到SQL转换系统中的两个主要挑战：实现跨不同查询的稳健泛化和确保预测的可解释性信心。研究探讨了基于熵的置信估计的权衡，并评估了其对文本到SQL模型整体性能的影响。此外，该研究还探索了模型的初始校准问题，并使用校准技术提高其性能。实验结果表明，编码器-解码器T5相较于上下文学习GPT 4和仅解码器Llama 3具有更好的校准性能，所设计的外置基于熵的选择性分类器表现更佳。同时发现，选择性分类器在检测到与问题不相关错误方面的概率更高，而非错误的查询生成。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文本到SQL技术简化了数据库的信息检索与合成，让用户通过自然语言与数据库进行交互。</li>
<li>大型语言模型（LLM）在将自然语言问题转换为SQL查询方面取得了成功，但仍面临两个主要挑战：泛化能力和预测的可解释性。</li>
<li>研究通过集成选择性分类器来解决上述挑战，以提高模型的泛化能力和预测的准确性及可解释性。</li>
<li>基于熵的置信估计被用于分析覆盖面与风险之间的权衡，以评估对模型整体性能的影响。</li>
<li>研究发现编码器-解码器T5相较于其他模型具有更好的校准性能。</li>
<li>外部基于熵的选择性分类器表现更佳，特别是在检测与问题不相关的错误方面。</li>
<li>错误更多地与问题不相关性相关，而非错误的查询生成。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.09527">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-19b2d4d38a49b64a440b7688a1467047.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-886aea6744df2c764b08a92a4eef9eb0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fd39c23078a013290e15f7dee7416d12.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eba93da7721d3c3119b4f210f248ef5f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f0293a9152fb205c936f8b0785dc0ef7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-17b567f0ae4ec2a48736463988d8493b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-98f5cf97419b20cf8c7c21757c86786b.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Omni-Emotion-Extending-Video-MLLM-with-Detailed-Face-and-Audio-Modeling-for-Multimodal-Emotion-Analysis"><a href="#Omni-Emotion-Extending-Video-MLLM-with-Detailed-Face-and-Audio-Modeling-for-Multimodal-Emotion-Analysis" class="headerlink" title="Omni-Emotion: Extending Video MLLM with Detailed Face and Audio Modeling   for Multimodal Emotion Analysis"></a>Omni-Emotion: Extending Video MLLM with Detailed Face and Audio Modeling   for Multimodal Emotion Analysis</h2><p><strong>Authors:Qize Yang, Detao Bai, Yi-Xing Peng, Xihan Wei</strong></p>
<p>Understanding emotions accurately is essential for fields like human-computer interaction. Due to the complexity of emotions and their multi-modal nature (e.g., emotions are influenced by facial expressions and audio), researchers have turned to using multi-modal models to understand human emotions rather than single-modality. However, current video multi-modal large language models (MLLMs) encounter difficulties in effectively integrating audio and identifying subtle facial micro-expressions. Furthermore, the lack of detailed emotion analysis datasets also limits the development of multimodal emotion analysis. To address these issues, we introduce a self-reviewed dataset and a human-reviewed dataset, comprising 24,137 coarse-grained samples and 3,500 manually annotated samples with detailed emotion annotations, respectively. These datasets allow models to learn from diverse scenarios and better generalize to real-world applications. Moreover, in addition to the audio modeling, we propose to explicitly integrate facial encoding models into the existing advanced Video MLLM, enabling the MLLM to effectively unify audio and the subtle facial cues for emotion understanding. By aligning these features within a unified space and employing instruction tuning in our proposed datasets, our Omni-Emotion achieves state-of-the-art performance in both emotion recognition and reasoning tasks. </p>
<blockquote>
<p>准确地理解情绪对于人机交互等领域至关重要。由于情绪的复杂性和其多模态性质（例如，情绪受到面部表情和音频的影响），研究人员已经转向使用多模态模型来理解人类情绪，而不是单模态模型。然而，当前的视频多模态大型语言模型（MLLM）在有效整合音频和识别微妙的面部表情微动作方面遇到困难。此外，缺乏详细的情感分析数据集也限制了多模态情感分析的发展。为了解决这些问题，我们引入了一个自我审查的数据集和一个人工审查的数据集，分别包含24,137个粗粒度样本和3,500个带有详细情感注释的手动注释样本。这些数据集使模型能够学习各种场景，并更好地推广现实世界的实际应用。除了音频建模之外，我们还提议将面部编码模型显式地集成到现有的先进视频MLLM中，使MLLM能够有效地统一音频和微妙的面部线索来进行情感理解。通过在一个统一的空间内对齐这些特征，并在我们提出的数据集中进行指令调整，我们的Omni-Emotion在情感识别和推理任务上均达到了最先进的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.09502v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了多模态情感分析的重要性，特别是在处理人类与计算机交互时。当前视频多模态大语言模型在处理音频和识别微妙的面部表情时面临挑战。为解决这些问题，文章引入了一个自我审查的数据集和一个经过人工审查的数据集，并建议将面部编码模型显式地集成到现有的高级视频多模态语言模型中，以提高情感理解的准确性。通过在这些数据集中采用指令微调的方法，Omni-Emotion模型在情感识别和推理任务上达到了最佳性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态情感分析对于人类与计算机交互至关重要。</li>
<li>当前视频多模态大语言模型在处理音频和识别微妙的面部表情方面存在挑战。</li>
<li>引入自我审查的数据集和经过人工审查的数据集，以提供更详细的情感注释。</li>
<li>建议将面部编码模型集成到现有的高级视频多模态语言模型中。</li>
<li>通过在数据集中采用指令微调的方法，Omni-Emotion模型可以提高情感理解的准确性。</li>
<li>Omni-Emotion模型在情感识别和推理任务上达到了最佳性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.09502">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-5f828c668f0ca27558841cc1b0debd93.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-00bc06da24d713f02e787d3edd58313b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aef77c49ab3cdf4b68ccccef952c60e5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6a88dc9b5861a1c697e78979926b7972.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3861a3139b9abb88278a0327c59ee103.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Double-Visual-Defense-Adversarial-Pre-training-and-Instruction-Tuning-for-Improving-Vision-Language-Model-Robustness"><a href="#Double-Visual-Defense-Adversarial-Pre-training-and-Instruction-Tuning-for-Improving-Vision-Language-Model-Robustness" class="headerlink" title="Double Visual Defense: Adversarial Pre-training and Instruction Tuning   for Improving Vision-Language Model Robustness"></a>Double Visual Defense: Adversarial Pre-training and Instruction Tuning   for Improving Vision-Language Model Robustness</h2><p><strong>Authors:Zeyu Wang, Cihang Xie, Brian Bartoldson, Bhavya Kailkhura</strong></p>
<p>This paper investigates the robustness of vision-language models against adversarial visual perturbations and introduces a novel &#96;&#96;double visual defense” to enhance this robustness. Unlike previous approaches that resort to lightweight adversarial fine-tuning of a pre-trained CLIP model, we perform large-scale adversarial vision-language pre-training from scratch using web-scale data. We then strengthen the defense by incorporating adversarial visual instruction tuning. The resulting models from each stage, $\Delta$CLIP and $\Delta^2$LLaVA, show substantially enhanced zero-shot robustness and set a new state-of-the-art in adversarial defense for vision-language models. For example, the adversarial robustness of $\Delta$CLIP surpasses that of the previous best models on ImageNet-1k by ~20%. %For example, $\Delta$CLIP surpasses the previous best models on ImageNet-1k by ~20% in terms of adversarial robustness. Similarly, compared to prior art, $\Delta^2$LLaVA brings a ~30% robustness improvement to image captioning task and a ~20% robustness improvement to visual question answering task. Furthermore, our models exhibit stronger zero-shot recognition capability, fewer hallucinations, and superior reasoning performance compared to baselines. Our project page is <a target="_blank" rel="noopener" href="https://doublevisualdefense.github.io/">https://doublevisualdefense.github.io/</a>. </p>
<blockquote>
<p>本文研究了视觉语言模型对抗对视觉干扰的鲁棒性，并引入了一种新型的“双重视觉防御”方法来提高这种鲁棒性。与其他方法不同，我们没有使用预训练的CLIP模型进行轻量级的对抗微调，而是使用大规模网络数据从头开始进行大规模的对抗性视觉语言预训练。然后我们通过融入对抗视觉指令微调来加强防御。各阶段得出的模型，包括ΔCLIP和Δ²LLaVA，表现出大幅增强的零样本鲁棒性，并在视觉语言模型的对抗防御方面创造了新的技术记录。例如，ΔCLIP在ImageNet-1k上的对抗鲁棒性超过了以前最优秀的模型约20%。与现有技术相比，Δ²LLaVA为图像描述任务带来了约30%的鲁棒性提升，并为视觉问答任务带来了约20%的鲁棒性提升。此外，我们的模型还表现出更强的零样本识别能力、更少的幻觉和优于基准线的推理性能。我们的项目页面是：[<a target="_blank" rel="noopener" href="https://doublevisualdefense.github.io/]">https://doublevisualdefense.github.io/]</a> 。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.09446v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文研究了视觉语言模型对抗视觉扰动攻击的鲁棒性，并提出了一种新的“双重视觉防御”方法来增强模型的鲁棒性。不同于之前的方法，我们采用大规模的对抗性视觉语言预训练，从原始数据开始使用网络规模数据进行训练。随后，通过引入对抗性视觉指令调整来加强防御。由此产生的模型，如$\Delta$CLIP和$\Delta^2$LLaVA，在零样本鲁棒性上有显著增强，并在视觉语言模型的对抗性防御方面创造了新的最佳状态。例如，$\Delta$CLIP在ImageNet-1k上的对抗性鲁棒性超过了以前的最佳模型约20%。我们的项目页面是<a target="_blank" rel="noopener" href="https://doublevisualdefense.github.io/%E3%80%82">https://doublevisualdefense.github.io/。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>该论文研究了视觉语言模型面对对抗性视觉扰动的鲁棒性。</li>
<li>论文提出了“双重视觉防御”策略以增强模型的鲁棒性。</li>
<li>与之前的方法不同，该研究采用大规模的对抗性视觉语言预训练。</li>
<li>通过引入对抗性视觉指令调整，进一步加强了模型的防御能力。</li>
<li>$\Delta$CLIP和$\Delta^2$LLaVA模型显著提高了零样本鲁棒性，并在对抗性防御方面达到新的最佳水平。</li>
<li>$\Delta$CLIP在ImageNet-1k上的对抗性鲁棒性优于之前的最佳模型约20%。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.09446">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-6665ed36043f278859a6a55830b05e6c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-767b185b59c3c4eed70baa9c5a9e8322.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7c166aae33ad0dab36200cbe9f4001ca.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Monte-Carlo-Tree-Search-for-Comprehensive-Exploration-in-LLM-Based-Automatic-Heuristic-Design"><a href="#Monte-Carlo-Tree-Search-for-Comprehensive-Exploration-in-LLM-Based-Automatic-Heuristic-Design" class="headerlink" title="Monte Carlo Tree Search for Comprehensive Exploration in LLM-Based   Automatic Heuristic Design"></a>Monte Carlo Tree Search for Comprehensive Exploration in LLM-Based   Automatic Heuristic Design</h2><p><strong>Authors:Zhi Zheng, Zhuoliang Xie, Zhenkun Wang, Bryan Hooi</strong></p>
<p>Handcrafting heuristics for solving complex planning tasks (e.g., NP-hard combinatorial optimization (CO) problems) is a common practice but requires extensive domain knowledge. Recently, Large Language Model (LLM)-based automatic heuristics design (AHD) methods have shown promise in generating high-quality heuristics without manual intervention. Existing LLM-based AHD methods employ a population to maintain a fixed number of top-performing LLM-generated heuristics and introduce evolutionary computation (EC) to enhance the population iteratively. However, the population-based procedure brings greedy properties, often resulting in convergence to local optima. Instead, to more comprehensively explore the space of heuristics, we propose using Monte Carlo Tree Search (MCTS) for LLM-based heuristic evolution while preserving all LLM-generated heuristics in a tree structure. With a novel thought-alignment process and an exploration-decay technique, the proposed MCTS-AHD method delivers significantly higher-quality heuristics on various complex tasks. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/zz1358m/MCTS-AHD-master">https://github.com/zz1358m/MCTS-AHD-master</a>. </p>
<blockquote>
<p>手动设计启发式方法来解决复杂的规划任务（例如NP难的组合优化问题）是一种常见的做法，但需要广泛的领域知识。最近，基于大型语言模型（LLM）的自动启发式设计（AHD）方法在生成高质量启发式方面显示出巨大潜力，无需人工干预。现有的基于LLM的AHD方法采用种群方式维持性能最佳的LLM生成启发式方法的数量，并引入进化计算（EC）来逐步增强种群。然而，基于种群的方法具有贪婪性，往往会导致收敛到局部最优解。相反，为了更全面地探索启发式方法的空间，我们提出使用蒙特卡洛树搜索（MCTS）来进行基于LLM的启发式方法演化，同时将所有由LLM生成的启发式方法保存在树结构中。通过一种新的思想对齐过程和探索衰减技术，所提出的MCTS-AHD方法在多种复杂任务上提供了更高质量的启发式方法。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/zz1358m/MCTS-AHD-master%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/zz1358m/MCTS-AHD-master找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.08603v2">PDF</a> </p>
<p><strong>Summary</strong><br>大规模语言模型（LLM）自动启发式设计（AHD）是解决复杂规划任务的有效方法。虽然传统方法采用种群来维护最佳启发式，但存在局限性。本文提出使用蒙特卡洛树搜索（MCTS）进行LLM启发式进化，并将所有生成的启发式保存在树结构中。新方法提高了启发式质量，适用于多种复杂任务。相关代码已公开分享。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LLM在启发式设计（AHD）领域展现出潜力，能够生成高质量启发式而无需人工干预。</li>
<li>传统LLM-based AHD方法使用种群维护最佳启发式，但可能陷入局部最优解。</li>
<li>提出使用蒙特卡洛树搜索（MCTS）进行LLM-based启发式进化，以更全面地探索启发式空间。</li>
<li>新方法通过思想对齐过程和探索衰减技术提高了启发式质量。</li>
<li>MCTS-AHD方法在多种复杂任务中表现优异。</li>
<li>相关研究代码已公开分享，便于进一步研究和应用。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.08603">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-df4e7f274144f09d5a746f466013951e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3b7bad3523aadde1fcaa44e9ae50ebbc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d958ce1c774f9c8f44f9a14d8e876c42.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5216657749c61699431096c9b46f9d69.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Super-class-guided-Transformer-for-Zero-Shot-Attribute-Classification"><a href="#Super-class-guided-Transformer-for-Zero-Shot-Attribute-Classification" class="headerlink" title="Super-class guided Transformer for Zero-Shot Attribute Classification"></a>Super-class guided Transformer for Zero-Shot Attribute Classification</h2><p><strong>Authors:Sehyung Kim, Chanhyeong Yang, Jihwan Park, Taehoon Song, Hyunwoo J. Kim</strong></p>
<p>Attribute classification is crucial for identifying specific characteristics within image regions. Vision-Language Models (VLMs) have been effective in zero-shot tasks by leveraging their general knowledge from large-scale datasets. Recent studies demonstrate that transformer-based models with class-wise queries can effectively address zero-shot multi-label classification. However, poor utilization of the relationship between seen and unseen attributes makes the model lack generalizability. Additionally, attribute classification generally involves many attributes, making maintaining the model’s scalability difficult. To address these issues, we propose Super-class guided transFormer (SugaFormer), a novel framework that leverages super-classes to enhance scalability and generalizability for zero-shot attribute classification. SugaFormer employs Super-class Query Initialization (SQI) to reduce the number of queries, utilizing common semantic information from super-classes, and incorporates Multi-context Decoding (MD) to handle diverse visual cues. To strengthen generalizability, we introduce two knowledge transfer strategies that utilize VLMs. During training, Super-class guided Consistency Regularization (SCR) aligns model’s features with VLMs using super-class guided prompts, and during inference, Zero-shot Retrieval-based Score Enhancement (ZRSE) refines predictions for unseen attributes. Extensive experiments demonstrate that SugaFormer achieves state-of-the-art performance across three widely-used attribute classification benchmarks under zero-shot, and cross-dataset transfer settings. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/mlvlab/SugaFormer">https://github.com/mlvlab/SugaFormer</a>. </p>
<blockquote>
<p>属性分类对于识别图像区域内的特定特征至关重要。视觉语言模型（VLMs）通过利用大规模数据集的一般知识，在零样本任务中表现出色。最近的研究表明，基于transformer的具有类别查询的模型可以有效地解决零样本多标签分类问题。然而，可见和不可见属性之间关系利用不足使得模型缺乏泛化能力。此外，属性分类通常涉及许多属性，使得保持模型的可扩展性变得困难。为了解决这些问题，我们提出了Super-class guided transFormer（SugaFormer）这一新颖框架，利用超类增强零样本属性分类的可扩展性和泛化能力。SugaFormer采用Super-class Query Initialization（SQI）减少查询数量，利用超类的通用语义信息，并结合Multi-context Decoding（MD）处理多样的视觉线索。为了加强泛化能力，我们引入了两种利用VLMs的知识转移策略。在训练过程中，Super-class guided Consistency Regularization（SCR）通过超类引导提示将模型特征与VLMs对齐，而在推理过程中，Zero-shot Retrieval-based Score Enhancement（ZRSE）对未见过的属性预测进行细化。大量实验表明，SugaFormer在三个广泛使用的属性分类基准测试下实现了零样本和跨数据集迁移设置的最新性能。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/mlvlab/SugaFormer%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/mlvlab/SugaFormer找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.05728v2">PDF</a> AAAI25</p>
<p><strong>Summary</strong></p>
<p>本文介绍了属性分类在识别图像区域特定特征中的重要性。尽管视觉语言模型（VLMs）在零样本任务中表现出色，但现有研究在解决零样本多标签分类时，未能充分利用已见和未见属性之间的关系，导致模型泛化性不足。为解决这些问题，提出了Super-class guided transFormer（SugaFormer）框架，通过利用超类增强零样本属性分类的扩展性和泛化性。该框架采用超级类查询初始化（SQI）减少查询数量，利用超级类的通用语义信息，并结合多上下文解码（MD）处理多样的视觉线索。此外，还引入两种利用VLMs的知识转移策略以增强泛化能力。在三个广泛使用的属性分类基准测试下，SugaFormer在零样本和跨数据集转移设置中实现了最佳性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>属性分类是识别图像区域特定特征的关键。</li>
<li>视觉语言模型（VLMs）在零样本任务中表现良好，但在多标签分类中泛化性不足。</li>
<li>SugaFormer框架通过利用超类增强零样本属性分类的泛化性和扩展性。</li>
<li>SugaFormer采用超级类查询初始化和多上下文解码处理视觉信息。</li>
<li>SugaFormer引入两种知识转移策略，利用VLMs增强泛化能力。</li>
<li>SugaFormer在多个属性分类基准测试中实现最佳性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.05728">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-c23148e0133900a7bf7d14b6132c838c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ce1ec599dd84fdfc626d4a840a458d31.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8a253ebcf799571b261cbe3fa9594210.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e74b5349d2ab95debfe9e4d33c8e888b.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="VITA-1-5-Towards-GPT-4o-Level-Real-Time-Vision-and-Speech-Interaction"><a href="#VITA-1-5-Towards-GPT-4o-Level-Real-Time-Vision-and-Speech-Interaction" class="headerlink" title="VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction"></a>VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction</h2><p><strong>Authors:Chaoyou Fu, Haojia Lin, Xiong Wang, Yi-Fan Zhang, Yunhang Shen, Xiaoyu Liu, Yangze Li, Zuwei Long, Heting Gao, Ke Li, Long Ma, Xiawu Zheng, Rongrong Ji, Xing Sun, Caifeng Shan, Ran He</strong></p>
<p>Recent Multimodal Large Language Models (MLLMs) have typically focused on integrating visual and textual modalities, with less emphasis placed on the role of speech in enhancing interaction. However, speech plays a crucial role in multimodal dialogue systems, and implementing high-performance in both vision and speech tasks remains a significant challenge due to the fundamental modality differences. In this paper, we propose a carefully designed multi-stage training methodology that progressively trains LLM to understand both visual and speech information, ultimately enabling fluent vision and speech interaction. Our approach not only preserves strong vision-language capacity, but also enables efficient speech-to-speech dialogue capabilities without separate ASR and TTS modules, significantly accelerating multimodal end-to-end response speed. By comparing our method against state-of-the-art counterparts across benchmarks for image, video, and speech tasks, we demonstrate that our model is equipped with both strong visual and speech capabilities, making near real-time vision and speech interaction. </p>
<blockquote>
<p>最近的跨模态大型语言模型（MLLM）主要集中于整合视觉和文本模态，对语音在增强交互中的作用重视不够。然而，语音在多模态对话系统中起着至关重要的作用，由于基本模态差异，在视觉和语音任务中实现高性能仍然是一个巨大的挑战。在本文中，我们提出了一种精心设计的多阶段训练方法论，逐步训练大型语言模型以理解视觉和语音信息，最终实现流畅的视觉和语音交互。我们的方法不仅保留了强大的视觉语言功能，还实现了高效的语音对话功能，无需使用单独的自动语音识别（ASR）和文本转语音（TTS）模块，从而显著加快了多模态端到端的响应速度。通过与图像、视频和语音任务的最新基准进行比较，我们证明了我们的模型拥有强大的视觉和语音功能，能够实现接近实时的视觉和语音交互。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.01957v2">PDF</a> <a target="_blank" rel="noopener" href="https://github.com/VITA-MLLM/VITA">https://github.com/VITA-MLLM/VITA</a></p>
<p><strong>Summary</strong>：<br>近期多模态大型语言模型（MLLMs）主要聚焦于整合视觉和文本模态，较少关注语音在增强交互中的作用。然而，语音在多模态对话系统中扮演关键角色，实现在视觉和语音任务中的高性能仍然是一个重大挑战，因为存在基本的模态差异。本文提出了一种精心设计的多阶段训练方法论，逐步训练大型语言模型以理解视觉和语音信息，最终实现流畅的视觉和语音交互。该方法不仅保留了强大的视觉语言功能，还能实现高效的语音对话能力，无需额外的自动语音识别和文本转语音模块，显著加快多模态端到端的响应速度。通过与图像、视频和语音任务的最新前沿模型进行对比，本文证明了该模型具备强大的视觉和语音功能，可实现近乎实时的视觉和语音交互。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>MLLMs 在整合视觉和文本模态时较少关注语音的重要性。</li>
<li>实现视觉和语音任务的高性能是一个重大挑战，因为存在基本的模态差异。</li>
<li>提出了一种多阶段训练方法论来训练大型语言模型，以理解视觉和语音信息。</li>
<li>该方法实现了流畅的视觉和语音交互。</li>
<li>模型不仅具备强大的视觉语言功能，还能实现高效的语音对话能力。</li>
<li>模型无需额外的ASR和TTS模块即可实现语音交互，加快响应速度。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.01957">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-58744d62eb0fbf66540b32115e33c365.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-458784e265fee8832f68789bf9d105f9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e262945a140a0fbcdb7ddae1a7741766.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-396c9589614e6d25affa510578046020.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-200583adacfa33383773d2e9f3835530.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="SPRec-Leveraging-Self-Play-to-Debias-Preference-Alignment-for-Large-Language-Model-based-Recommendations"><a href="#SPRec-Leveraging-Self-Play-to-Debias-Preference-Alignment-for-Large-Language-Model-based-Recommendations" class="headerlink" title="SPRec: Leveraging Self-Play to Debias Preference Alignment for Large   Language Model-based Recommendations"></a>SPRec: Leveraging Self-Play to Debias Preference Alignment for Large   Language Model-based Recommendations</h2><p><strong>Authors:Chongming Gao, Ruijun Chen, Shuai Yuan, Kexin Huang, Yuanqing Yu, Xiangnan He</strong></p>
<p>Large language models (LLMs) have attracted significant attention in recommendation systems. Current LLM-based recommender systems primarily rely on supervised fine-tuning (SFT) to train the model for recommendation tasks. However, relying solely on positive samples limits the model’s ability to align with user satisfaction and expectations. To address this, researchers have introduced Direct Preference Optimization (DPO), which explicitly aligns recommendations with user preferences using offline preference ranking data. Despite its advantages, our theoretical analysis reveals that DPO inherently biases the model towards a few items, exacerbating the filter bubble issue and ultimately degrading user experience. In this paper, we propose SPRec, a novel self-play recommendation framework designed to mitigate over-recommendation and improve fairness without requiring additional data or manual intervention. In each self-play iteration, the model undergoes an SFT step followed by a DPO step, treating offline interaction data as positive samples and the predicted outputs from the previous iteration as negative samples. This effectively re-weights the DPO loss function using the model’s logits, adaptively suppressing biased items. Extensive experiments on multiple real-world datasets demonstrate SPRec’s effectiveness in enhancing recommendation accuracy and addressing fairness concerns. The implementation is available via <a target="_blank" rel="noopener" href="https://github.com/RegionCh/SPRec">https://github.com/RegionCh/SPRec</a> </p>
<blockquote>
<p>大型语言模型（LLM）在推荐系统中引起了广泛关注。当前的基于LLM的推荐系统主要依赖于监督微调（SFT）来训练推荐任务模型。然而，仅依赖正面样本限制了模型与用户满意度和期望对齐的能力。为了解决这一问题，研究人员引入了直接偏好优化（DPO），它利用离线偏好排名数据显式地将推荐与用户偏好对齐。尽管DPO有其优势，但我们的理论分析表明，它天生会使模型偏向于少数物品，加剧了过滤泡沫问题，最终降低了用户体验。在本文中，我们提出了SPRec，这是一种新型的自玩推荐框架，旨在缓解过度推荐问题，提高公平性，而无需额外数据或人工干预。在每次自玩迭代中，模型会经历一个SFT步骤，然后是一个DPO步骤，将离线交互数据视为正面样本，而上一轮的预测输出视为负面样本。这有效地使用模型的逻辑重新加权DPO损失函数，自适应地抑制偏见物品。在多个真实世界数据集上的广泛实验证明了SPRec在提高推荐准确性和解决公平性问题方面的有效性。实现可通过<a target="_blank" rel="noopener" href="https://github.com/RegionCh/SPRec%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/RegionCh/SPRec获得。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.09243v2">PDF</a> </p>
<p><strong>Summary</strong>：LLM在推荐系统中的应用引起广泛关注。现有LLM推荐系统主要依赖监督微调（SFT）进行训练，但仅依赖正样本限制了模型与用户满意度和期望的契合度。研究者引入直接偏好优化（DPO）以利用离线偏好排名数据显式匹配用户偏好。然而，理论分析表明DPO会导致模型偏向少数物品，加剧滤镜泡沫问题并降低用户体验。本文提出SPRec，一种新型自我博弈推荐框架，旨在缓解过度推荐并改善公平性，无需额外数据或人工干预。SPRec通过迭代使用离线交互数据作为正样本，以及前一次迭代的预测输出作为负样本，自适应调整DPO损失函数权重。实验证明SPRec在提高推荐准确性和解决公平性问题方面有效。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>LLM在推荐系统中的应用受到关注，但依赖监督微调（SFT）存在局限性。</li>
<li>直接偏好优化（DPO）能够利用用户偏好数据提高推荐效果。</li>
<li>DPO可能导致模型偏向少数物品，加剧滤镜泡沫问题。</li>
<li>SPRec框架旨在通过自我博弈方式缓解过度推荐问题并改善公平性。</li>
<li>SPRec利用离线交互数据迭代更新模型，使用前一次迭代的预测输出作为负样本。</li>
<li>SPRec通过自适应调整DPO损失函数权重，提高推荐准确性并解决公平性问题。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.09243">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0c23541395fc6e121e54ad49de88e83c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-86508638f5e3135e666ca5fdf6a4e600.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-062d936c9217d1cfd2f47fe65f7ed4cf.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Can-ChatGPT-Overcome-Behavioral-Biases-in-the-Financial-Sector-Classify-and-Rethink-Multi-Step-Zero-Shot-Reasoning-in-the-Gold-Investment"><a href="#Can-ChatGPT-Overcome-Behavioral-Biases-in-the-Financial-Sector-Classify-and-Rethink-Multi-Step-Zero-Shot-Reasoning-in-the-Gold-Investment" class="headerlink" title="Can ChatGPT Overcome Behavioral Biases in the Financial Sector?   Classify-and-Rethink: Multi-Step Zero-Shot Reasoning in the Gold Investment"></a>Can ChatGPT Overcome Behavioral Biases in the Financial Sector?   Classify-and-Rethink: Multi-Step Zero-Shot Reasoning in the Gold Investment</h2><p><strong>Authors:Shuoling Liu, Gaoguo Jia, Yuhang Jiang, Liyuan Chen, Qiang Yang</strong></p>
<p>Large Language Models (LLMs) have achieved remarkable success recently, displaying exceptional capabilities in creating understandable and organized text. These LLMs have been utilized in diverse fields, such as clinical research, where domain-specific models like Med-Palm have achieved human-level performance. Recently, researchers have employed advanced prompt engineering to enhance the general reasoning ability of LLMs. Despite the remarkable success of zero-shot Chain-of-Thoughts (CoT) in solving general reasoning tasks, the potential of these methods still remains paid limited attention in the financial reasoning task.To address this issue, we explore multiple prompt strategies and incorporated semantic news information to improve LLMs’ performance on financial reasoning tasks.To the best of our knowledge, we are the first to explore this important issue by applying ChatGPT to the gold investment.In this work, our aim is to investigate the financial reasoning capabilities of LLMs and their capacity to generate logical and persuasive investment opinions. We will use ChatGPT, one of the most powerful LLMs recently, and prompt engineering to achieve this goal. Our research will focus on understanding the ability of LLMs in sophisticated analysis and reasoning within the context of investment decision-making. Our study finds that ChatGPT with CoT prompt can provide more explainable predictions and overcome behavioral biases, which is crucial in finance-related tasks and can achieve higher investment returns. </p>
<blockquote>
<p>大型语言模型（LLM）最近取得了显著的成就，展现出在创建可理解和有组织的文本方面的卓越能力。这些LLM已被应用于多个领域，如临床研究，其中特定领域的模型（如Med-Palm）已达到了人类水平的性能。最近，研究人员采用先进的提示工程来增强LLM的一般推理能力。尽管零镜头链思维（CoT）在解决一般推理任务方面取得了显著成功，但这些方法在财务推理任务中的潜力仍然受到的关注有限。</p>
</blockquote>
<p>为了解决这一问题，我们探索了多种提示策略并融入了语义新闻信息，以提高LLM在财务推理任务上的性能。据我们所知，我们是第一个将ChatGPT应用于黄金投资来探索这一重要问题。</p>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.13599v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在创建可理解和组织化的文本方面取得了显著的成功，并在临床研域等领域得到应用。最近，研究人员通过先进的提示工程技术增强了LLM的一般推理能力。尽管零样本链思维（CoT）在解决一般推理任务中表现出色，但其在金融推理任务中的潜力仍未得到充分关注。本研究旨在探索将ChatGPT等LLM应用于金融推理的能力，并关注其在投资决策中的复杂分析和推理能力。研究发现，带有CoT提示的ChatGPT可以提供更可解释的预测，并克服行为偏见，实现更高的投资回报。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMs已在多个领域取得显著成功，包括临床研究和金融领域。</li>
<li>通过先进的提示工程技术，LLM的推理能力得到了增强。</li>
<li>零样本链思维（CoT）在一般推理任务中表现出良好的性能，但在金融推理任务中的潜力尚未得到充分研究。</li>
<li>本研究首次将ChatGPT应用于金融投资领域，探索其在金融推理方面的能力。</li>
<li>使用ChatGPT和CoT提示可以提供更可解释的预测，这在金融任务中至关重要。</li>
<li>LLMs能够克服人类行为偏见，实现更高的投资回报。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.13599">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-a5635b174330008441c9ce6d11c8356b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ffa358b2985c6bf76eaba2f479473c0a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7ad0959ed2f0923617aaa302d06acc7d.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Crafting-Customisable-Characters-with-LLMs-Introducing-SimsChat-a-Persona-Driven-Role-Playing-Agent-Framework"><a href="#Crafting-Customisable-Characters-with-LLMs-Introducing-SimsChat-a-Persona-Driven-Role-Playing-Agent-Framework" class="headerlink" title="Crafting Customisable Characters with LLMs: Introducing SimsChat, a   Persona-Driven Role-Playing Agent Framework"></a>Crafting Customisable Characters with LLMs: Introducing SimsChat, a   Persona-Driven Role-Playing Agent Framework</h2><p><strong>Authors:Bohao Yang, Dong Liu, Chenghao Xiao, Kun Zhao, Chen Tang, Chao Li, Lin Yuan, Guang Yang, Lanxiao Huang, Chenghua Lin</strong></p>
<p>Large Language Models (LLMs) demonstrate remarkable ability to comprehend instructions and generate human-like text, enabling sophisticated agent simulation beyond basic behavior replication. However, the potential for creating freely customisable characters remains underexplored. We introduce the Customisable Conversation Agent Framework, which employs LLMs to simulate real-world characters through personalised characteristic feature injection, enabling diverse character creation according to user preferences. We propose the SimsConv dataset, comprising 68 customised characters and 13,971 multi-turn role-playing dialogues across 1,360 real-world scenes. Characters are initially customised using pre-defined elements (career, aspiration, traits, skills), then expanded through personal and social profiles. Building on this, we present SimsChat, a freely customisable role-playing agent incorporating various realistic settings and topic-specified character interactions. Experimental results on both SimsConv and WikiRoleEval datasets demonstrate SimsChat’s superior performance in maintaining character consistency, knowledge accuracy, and appropriate question rejection compared to existing models. Our framework provides valuable insights for developing more accurate and customisable human simulacra. Our data and code are publicly available at <a target="_blank" rel="noopener" href="https://github.com/Bernard-Yang/SimsChat">https://github.com/Bernard-Yang/SimsChat</a>. </p>
<blockquote>
<p>大型语言模型（LLM）表现出令人瞩目的理解和执行指令的能力，以及生成类似人类的文本，能够实现超越基本行为复制的复杂代理模拟。然而，创建可自由定制角色的潜力尚未得到充分探索。我们引入了可定制对话代理框架，该框架采用LLM通过个性化特征注入模拟现实世界角色，并根据用户偏好实现多样化角色创建。我们提出了SimsConv数据集，包含68个自定义角色和13971个跨1360个现实场景的多轮角色扮演对话。角色最初使用预定义元素（职业、志向、特质、技能）进行定制，然后通过个人和社会概况进行扩展。在此基础上，我们推出了SimsChat，一个可自由定制的角色扮演代理，包含各种现实设置和主题特定的角色交互。在SimsConv和WikiRoleEval数据集上的实验结果证明了SimsChat在保持角色一致性、知识准确性和适当问题拒绝方面的性能优于现有模型。我们的框架为开发更准确、可定制的人类模拟物提供了宝贵的见解。我们的数据和代码可在<a target="_blank" rel="noopener" href="https://github.com/Bernard-Yang/SimsChat%E5%85%AC%E5%BC%BA%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/Bernard-Yang/SimsChat公开访问。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.17962v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）具备理解指令和生成类似人类文本的能力，可以实现超越基本行为复制的复杂代理模拟。然而，创建可自由定制角色的潜力尚未得到充分探索。本文介绍了可定制对话代理框架，该框架利用LLM模拟真实世界角色，通过个性化特征注入实现根据用户偏好创建多样化角色。本文还提出了SimsConv数据集，包含68个自定义角色和13971个跨1360个真实场景的多轮角色扮演对话。通过SimsChat这一可自由定制的角色扮演代理，结合各种现实场景和主题特定的角色交互，实验结果表明SimsChat在保持角色一致性、知识准确性和适当的问题拒绝方面优于现有模型。该框架为开发更准确和可定制的人类模拟体提供了有价值的见解。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM具备理解和生成类似人类文本的能力，可实现复杂代理模拟。</li>
<li>创建可自由定制角色的潜力尚未充分探索。</li>
<li>介绍了可定制对话代理框架，利用LLM模拟真实世界角色，实现角色个性化定制。</li>
<li>提出了SimsConv数据集，包含自定义角色和跨真实场景的多轮角色扮演对话。</li>
<li>SimsChat是自由可定制的角色扮演代理，结合现实场景和主题特定的角色交互。</li>
<li>实验结果表明SimsChat在角色一致性、知识准确性和问题拒绝方面优于现有模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.17962">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-19857b36c718308e59a54843c5b65dd4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b4c5244b4a9d50e72b6e1124bc10a55a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7ae555b04f297b9ac22b3048a7fa2d40.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-64c3adf27271332468cf886dff829d8c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d57663d9a037c9951e23a8da2f3d00e6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6bffccd2ed6bf4ab389d3547d8e32180.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3396c571b913e3b5a7e4a18a54ebe250.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Lean-Attention-Hardware-Aware-Scalable-Attention-Mechanism-for-the-Decode-Phase-of-Transformers"><a href="#Lean-Attention-Hardware-Aware-Scalable-Attention-Mechanism-for-the-Decode-Phase-of-Transformers" class="headerlink" title="Lean Attention: Hardware-Aware Scalable Attention Mechanism for the   Decode-Phase of Transformers"></a>Lean Attention: Hardware-Aware Scalable Attention Mechanism for the   Decode-Phase of Transformers</h2><p><strong>Authors:Rya Sanovar, Srikant Bharadwaj, Renee St. Amant, Victor Rühle, Saravan Rajmohan</strong></p>
<p>Transformer-based models have emerged as one of the most widely used architectures for natural language processing, natural language generation, and image generation. The size of the state-of-the-art models has increased steadily reaching billions of parameters. These huge models are memory hungry and incur significant inference latency even on cutting edge AI-accelerators, such as GPUs. Specifically, the time and memory complexity of the attention operation is quadratic in terms of the total context length, i.e., prompt and output tokens. Thus, several optimizations such as key-value tensor caching and FlashAttention computation have been proposed to deliver the low latency demands of applications relying on such large models. However, these techniques do not cater to the computationally distinct nature of different phases during inference.   To that end, we propose LeanAttention, a scalable technique of computing self-attention for the token-generation phase (decode-phase) of decoder-only transformer models. LeanAttention enables scaling the attention mechanism implementation for the challenging case of long context lengths by re-designing the execution flow for the decode-phase. We identify that the associative property of online softmax can be treated as a reduction operation thus allowing us to parallelize the attention computation over these large context lengths. We extend the “stream-K” style reduction of tiled calculation to self-attention to enable parallel computation resulting in an average of 2.6x attention execution speedup over FlashAttention-2 and up to 8.33x speedup for 512k context lengths. </p>
<blockquote>
<p>基于Transformer的模型已成为自然语言处理、自然语言生成和图像生成领域最广泛使用的架构之一。最先进的模型规模持续增大，已达到数十亿参数。这些大型模型需要大量内存，即使在先进的AI加速器（如GPU）上也会产生显著的推理延迟。具体来说，注意力操作的时间和内存复杂度与总上下文长度成二次方关系，即提示和输出令牌。因此，已经提出了键值张量缓存和FlashAttention计算等优化措施，以满足依赖这些大型模型的应用程序的低延迟需求。然而，这些技术并不适应推理过程中不同阶段的计算特性。为此，我们提出了LeanAttention，这是一种用于解码器仅解码器transformer模型的令牌生成阶段（解码阶段）的自注意力计算的可扩展技术。LeanAttention通过重新设计解码阶段的执行流程，实现了对长上下文情况下注意力机制的缩放。我们发现在线softmax的关联属性可以视为归约操作，从而允许我们在这些长上下文上并行计算注意力。我们将“流K”风格的平铺计算归约扩展到自注意力，以实现并行计算，相对于FlashAttention-2实现平均2.6倍的注意力执行速度提升，对于512k的上下文长度甚至达到8.33倍的速度提升。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.10480v2">PDF</a> 13 pages, 10 figures</p>
<p><strong>Summary</strong></p>
<p>本文介绍了Transformer模型在自然语言处理、自然语言生成和图像生成等领域中的广泛应用。针对大型Transformer模型在推理过程中面临的内存消耗大、延迟高的问题，提出了一种名为LeanAttention的可扩展自注意力计算方法。该方法针对解码阶段的token生成阶段进行优化，通过重新设计执行流程，实现对长上下文情况下注意力机制的扩展。利用在线softmax的关联属性，将其视为归约操作，从而允许在大上下文长度上并行计算注意力。实验结果表明，LeanAttention在大型上下文长度下实现了显著的速度提升。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Transformer模型在自然语言处理和图像生成领域广泛应用，但大型模型面临内存消耗大、延迟高的问题。</li>
<li>现有优化技术如键值张量缓存和FlashAttention计算不能满足推理过程中不同阶段的计算特点。</li>
<li>LeanAttention提出了一种可扩展的自注意力计算方法，针对解码阶段的token生成进行优化。</li>
<li>LeanAttention通过重新设计执行流程，实现对长上下文情况下注意力机制的扩展。</li>
<li>利用在线softmax的关联属性，将其视为归约操作，允许在大上下文长度上并行计算注意力。</li>
<li>LeanAttention相比FlashAttention-2实现了平均2.6倍的速度提升，对于512k的上下文长度实现了最高8.33倍的速度提升。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.10480">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-49450984dd6e0517cc4eef139b2fe67d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-76cb39e5d16114a6ba9558735df70166.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-21113e20aa1daab0c14f43943ce83ea5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bd59eacc91f666c6df4a3c0368220d76.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-09ae430c78a162aa835825422bad5cad.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Meaning-Typed-Programming-Language-level-Abstractions-and-Runtime-for-GenAI-Applications"><a href="#Meaning-Typed-Programming-Language-level-Abstractions-and-Runtime-for-GenAI-Applications" class="headerlink" title="Meaning-Typed Programming: Language-level Abstractions and Runtime for   GenAI Applications"></a>Meaning-Typed Programming: Language-level Abstractions and Runtime for   GenAI Applications</h2><p><strong>Authors:Jason Mars, Yiping Kang, Jayanaka L. Dantanarayana, Kugesan Sivasothynathan, Christopher Clarke, Baichuan Li, Krisztian Flautner, Lingjia Tang</strong></p>
<p>Software is rapidly evolving from being programmed with traditional logical code, to neuro-integrated applications that leverage generative AI and large language models (LLMs) for application functionality. This shift increases the complexity of building applications, as developers now must reasoning about, program, and prompt LLMs. Despite efforts to create tools to assist with prompt engineering, these solutions often introduce additional layers of complexity to the development of neuro-integrated applications. This paper proposes meaning-typed programming (MTP), a novel approach to simplify the creation of neuro-integrated applications by introducing new language-level abstractions that hide the complexities of LLM integration. Our key insight is that typical conventional code already possesses a high level of semantic richness that can be automatically reasoned about, as it is designed to be readable and maintainable by humans. Leveraging this insight, we conceptualize LLMs as meaning-typed code constructs and introduce a by abstraction at the language level, MT-IR, a new meaning-based intermediate representation at the compiler level, and MT Runtime, an automated run-time engine for LLM integration and operations. We implement MTP in a production-grade Python super-set language called Jac and perform an extensive evaluation. Our results demonstrate that MTP not only simplifies the development process but also meets or exceeds the efficacy of state-of-the-art manual and tool-assisted prompt engineering techniques in terms of accuracy and usability. </p>
<blockquote>
<p>软件正从使用传统逻辑代码编程迅速演变为利用生成式人工智能和大型语言模型（LLM）的神经集成应用程序。这一转变增加了构建应用程序的复杂性，因为开发者现在必须理解、编程和提示LLM。尽管有创建辅助提示工程的工具的努力，但这些解决方案往往为神经集成应用程序的开发增加了额外的复杂层。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.08965v3">PDF</a> </p>
<p><strong>Summary</strong><br>在现代软件开发中，逻辑编程正逐渐转向以生成式人工智能和大语言模型（LLM）为核心的神经集成应用。然而，这增加了开发复杂性。本文提出意义类型编程（MTP），通过引入新的语言级抽象来简化神经集成应用的创建，隐藏LLM集成的复杂性。研究的关键见解在于利用传统代码的高语义丰富性，将LLM视为意义类型代码构造。通过编译器级别的意义基础中间表示（MT-IR）和自动化运行时引擎（MT Runtime），实现了MTP。在Python超集语言Jac中的实现并进行评估表明，MTP不仅简化了开发过程，而且在准确性和可用性方面达到或超过了最新手动和工具辅助提示工程技术的效果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>软件正在从逻辑编程向利用生成式人工智能和大语言模型（LLM）的神经集成应用转变。</li>
<li>这种转变增加了开发复杂性，需要开发者理解和操作LLM。</li>
<li>当前工具在辅助提示工程时，可能引入额外的复杂性。</li>
<li>提出意义类型编程（MTP）来简化神经集成应用的开发。</li>
<li>MTP利用传统代码的高语义丰富性，将LLM视为意义类型代码构造。</li>
<li>通过引入新的语言级抽象（如MT-IR和MT Runtime），实现了MTP。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.08965">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-9ec759b4da04a5269d4a302651f65a13.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-518ff93827e21d21ab732342e3e925b6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-062a0bbfe73720ba927969ba7b6f1e2b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ad3e3cb331bb3144ef8eb5d42d10c4f8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-58b237005ef27571bc957d52ce2ccb4e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8eec6f1839a168ab7c12d31c94debdac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5758327b5c8f843c4f9d81af4e79a2ab.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="SelectIT-Selective-Instruction-Tuning-for-LLMs-via-Uncertainty-Aware-Self-Reflection"><a href="#SelectIT-Selective-Instruction-Tuning-for-LLMs-via-Uncertainty-Aware-Self-Reflection" class="headerlink" title="SelectIT: Selective Instruction Tuning for LLMs via Uncertainty-Aware   Self-Reflection"></a>SelectIT: Selective Instruction Tuning for LLMs via Uncertainty-Aware   Self-Reflection</h2><p><strong>Authors:Liangxin Liu, Xuebo Liu, Derek F. Wong, Dongfang Li, Ziyi Wang, Baotian Hu, Min Zhang</strong></p>
<p>Instruction tuning (IT) is crucial to tailoring large language models (LLMs) towards human-centric interactions. Recent advancements have shown that the careful selection of a small, high-quality subset of IT data can significantly enhance the performance of LLMs. Despite this, common approaches often rely on additional models or data, which increases costs and limits widespread adoption. In this work, we propose a novel approach, termed SelectIT, that capitalizes on the foundational capabilities of the LLM itself. Specifically, we exploit the intrinsic uncertainty present in LLMs to more effectively select high-quality IT data, without the need for extra resources. Furthermore, we introduce a curated IT dataset, the Selective Alpaca, created by applying SelectIT to the Alpaca-GPT4 dataset. Empirical results demonstrate that IT using Selective Alpaca leads to substantial model ability enhancement. The robustness of SelectIT has also been corroborated in various foundation models and domain-specific tasks. Our findings suggest that longer and more computationally intensive IT data may serve as superior sources of IT, offering valuable insights for future research in this area. Data, code, and scripts are freely available at <a target="_blank" rel="noopener" href="https://github.com/Blue-Raincoat/SelectIT">https://github.com/Blue-Raincoat/SelectIT</a>. </p>
<blockquote>
<p>指令微调（IT）对于针对人类中心交互的大型语言模型（LLM）定制至关重要。最近的进展表明，仔细选择一小部分高质量的IT数据可以显著提高LLM的性能。尽管如此，常见的方法往往依赖于额外的模型或数据，这增加了成本并限制了广泛应用。在这项工作中，我们提出了一种新方法，称为SelectIT，它利用LLM本身的基础能力。具体来说，我们利用LLM中存在的内在不确定性来更有效地选择高质量的IT数据，无需额外的资源。此外，我们通过将SelectIT应用于Alpaca-GPT4数据集，引入了一个精选的IT数据集Selective Alpaca。经验结果表明，使用Selective Alpaca的IT大大提高了模型的能力。SelectIT在各种基础模型和特定领域任务中的稳健性也得到了证实。我们的研究结果表明，更长和计算上更密集的IT数据可能作为IT的优质来源，为这一领域的未来研究提供了宝贵的见解。数据、代码和脚本可在<a target="_blank" rel="noopener" href="https://github.com/Blue-Raincoat/SelectIT%E5%85%8D%E8%B4%B9%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/Blue-Raincoat/SelectIT免费获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2402.16705v2">PDF</a> Accepted to NeurIPS 2024</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在指令微调（IT）下能更好地进行人类交互。研究表明，选择高质量的小型IT数据集能有效提升LLM性能。但现有方法常依赖额外模型或数据，导致成本增加并限制了普及。本研究提出了一种新方法SelectIT，利用LLM的基础能力来选择高质量的IT数据，无需额外资源。我们还通过SelectIT方法创建了Selective Alpaca数据集，并证明其能显著提升模型能力。SelectIT在多种基础模型和领域特定任务中表现出稳健性。研究指出，长且计算密集的IT数据可能是更优的IT来源，为未来的研究提供了宝贵启示。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>指令微调（IT）对大型语言模型（LLM）进行人类交互至关重要。</li>
<li>选择高质量的小型IT数据集能显著提升LLM性能。</li>
<li>现有方法常依赖额外资源，SelectIT方法利用LLM的基础能力选择IT数据，无需额外资源。</li>
<li>通过SelectIT方法创建了Selective Alpaca数据集，表现优异。</li>
<li>SelectIT在多种模型和任务中表现出稳健性。</li>
<li>更长、计算密集的IT数据可能是更优的IT来源。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2402.16705">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-f548f150fa0410f19dc77ae0b3ec6876.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7a8a5d0b279a13966d2d549f2326c8e3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-351c68912052b4967394f66ef602196a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c18c0cfde75b62c5e789a610e647184d.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-18/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-18/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-18/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-b4c5244b4a9d50e72b6e1124bc10a55a.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2025-01-18  Crafting Customisable Characters with LLMs Introducing SimsChat, a   Persona-Driven Role-Playing Agent Framework
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-18
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-17/TTS/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-43adc801f1d3aa8a7f76510ab47e37b1.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS 方向最新论文已更新，请持续关注 Update in 2025-01-17  Speech Synthesis along Perceptual Voice Quality Dimensions
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-17
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">16765.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
