<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-18  Distilling Multi-modal Large Language Models for Autonomous Driving">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-5f828c668f0ca27558841cc1b0debd93.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-18
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-18
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    18.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    74 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-01-18-æ›´æ–°"><a href="#2025-01-18-æ›´æ–°" class="headerlink" title="2025-01-18 æ›´æ–°"></a>2025-01-18 æ›´æ–°</h1><h2 id="Distilling-Multi-modal-Large-Language-Models-for-Autonomous-Driving"><a href="#Distilling-Multi-modal-Large-Language-Models-for-Autonomous-Driving" class="headerlink" title="Distilling Multi-modal Large Language Models for Autonomous Driving"></a>Distilling Multi-modal Large Language Models for Autonomous Driving</h2><p><strong>Authors:Deepti Hegde, Rajeev Yasarla, Hong Cai, Shizhong Han, Apratim Bhattacharyya, Shweta Mahajan, Litian Liu, Risheek Garrepalli, Vishal M. Patel, Fatih Porikli</strong></p>
<p>Autonomous driving demands safe motion planning, especially in critical â€œlong-tailâ€ scenarios. Recent end-to-end autonomous driving systems leverage large language models (LLMs) as planners to improve generalizability to rare events. However, using LLMs at test time introduces high computational costs. To address this, we propose DiMA, an end-to-end autonomous driving system that maintains the efficiency of an LLM-free (or vision-based) planner while leveraging the world knowledge of an LLM. DiMA distills the information from a multi-modal LLM to a vision-based end-to-end planner through a set of specially designed surrogate tasks. Under a joint training strategy, a scene encoder common to both networks produces structured representations that are semantically grounded as well as aligned to the final planning objective. Notably, the LLM is optional at inference, enabling robust planning without compromising on efficiency. Training with DiMA results in a 37% reduction in the L2 trajectory error and an 80% reduction in the collision rate of the vision-based planner, as well as a 44% trajectory error reduction in longtail scenarios. DiMA also achieves state-of-the-art performance on the nuScenes planning benchmark. </p>
<blockquote>
<p>è‡ªåŠ¨é©¾é©¶éœ€è¦å®‰å…¨çš„è¿åŠ¨è§„åˆ’ï¼Œç‰¹åˆ«æ˜¯åœ¨å…³é”®çš„â€œé•¿å°¾â€åœºæ™¯ä¸­ã€‚æœ€è¿‘çš„ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºè§„åˆ’å™¨ï¼Œä»¥æé«˜å¯¹ç½•è§äº‹ä»¶çš„é€šç”¨æ€§ã€‚ç„¶è€Œï¼Œåœ¨æµ‹è¯•æ—¶ä½¿ç”¨LLMä¼šå¼•å…¥è¾ƒé«˜çš„è®¡ç®—æˆæœ¬ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†DiMAï¼Œè¿™æ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯çš„è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿï¼Œå®ƒèƒ½åœ¨ä¸ä¾èµ–LLMï¼ˆæˆ–åŸºäºè§†è§‰ï¼‰çš„è§„åˆ’å™¨çš„æƒ…å†µä¸‹ä¿æŒæ•ˆç‡ï¼ŒåŒæ—¶åˆ©ç”¨LLMçš„ä¸–ç•ŒçŸ¥è¯†ã€‚DiMAé€šè¿‡ä¸€ç³»åˆ—ä¸“é—¨è®¾è®¡çš„æ›¿ä»£ä»»åŠ¡ï¼Œå°†ä»å¤šæ¨¡æ€LLMä¸­æå–çš„ä¿¡æ¯è’¸é¦åˆ°åŸºäºè§†è§‰çš„ç«¯åˆ°ç«¯è§„åˆ’å™¨ä¸­ã€‚åœ¨è”åˆè®­ç»ƒç­–ç•¥ä¸‹ï¼Œä¸¤ä¸ªç½‘ç»œå…±ç”¨çš„åœºæ™¯ç¼–ç å™¨äº§ç”Ÿç»“æ„åŒ–è¡¨ç¤ºï¼Œè¿™äº›è¡¨ç¤ºåœ¨è¯­ä¹‰ä¸Šæ˜¯å›ºå®šçš„ï¼Œå¹¶ä¸æœ€ç»ˆçš„è§„åˆ’ç›®æ ‡å¯¹é½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæ¨ç†è¿‡ç¨‹ä¸­LLMæ˜¯å¯é€‰çš„ï¼Œèƒ½å¤Ÿåœ¨ä¸æŸå®³æ•ˆç‡çš„æƒ…å†µä¸‹å®ç°ç¨³å¥çš„è§„åˆ’ã€‚é€šè¿‡DiMAè¿›è¡Œè®­ç»ƒï¼Œå®ç°äº†åŸºäºè§†è§‰çš„è§„åˆ’å™¨çš„L2è½¨è¿¹è¯¯å·®é™ä½37%ï¼Œç¢°æ’ç‡é™ä½80%ï¼Œä»¥åŠåœ¨é•¿å°¾åœºæ™¯ä¸­çš„è½¨è¿¹è¯¯å·®é™ä½44%ã€‚DiMAè¿˜åœ¨nuScenesè§„åˆ’åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.09757v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>è‡ªä¸»é©¾é©¶éœ€è¦å®‰å…¨è¿åŠ¨è§„åˆ’ï¼Œç‰¹åˆ«æ˜¯åœ¨å…³é”®çš„â€œé•¿å°¾â€åœºæ™¯ä¸­ã€‚è¿‘æœŸç«¯åˆ°ç«¯çš„è‡ªä¸»é©¾é©¶ç³»ç»Ÿåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºè§„åˆ’å™¨ï¼Œä»¥æé«˜å¯¹ç½•è§äº‹ä»¶çš„æ³›åŒ–èƒ½åŠ›ã€‚ç„¶è€Œï¼Œä½¿ç”¨LLMè¿›è¡Œæµ‹è¯•æ—¶ä¼šäº§ç”Ÿé«˜è®¡ç®—æˆæœ¬ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†DiMAç³»ç»Ÿï¼Œå®ƒæ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯çš„è‡ªä¸»é©¾é©¶ç³»ç»Ÿï¼Œåœ¨ä¿æŒæ— LLMï¼ˆæˆ–åŸºäºè§†è§‰ï¼‰è§„åˆ’å™¨çš„æ•ˆç‡çš„åŒæ—¶ï¼Œåˆ©ç”¨LLMçš„ä¸–ç•ŒçŸ¥è¯†ã€‚DiMAé€šè¿‡ä¸€ç³»åˆ—ä¸“é—¨è®¾è®¡çš„æ›¿ä»£ä»»åŠ¡ï¼Œä»å¤šæ¨¡æ€LLMä¸­æç‚¼ä¿¡æ¯åˆ°åŸºäºè§†è§‰çš„ç«¯åˆ°ç«¯è§„åˆ’å™¨ã€‚åœ¨è”åˆè®­ç»ƒç­–ç•¥ä¸‹ï¼Œä¸€ä¸ªç”¨äºä¸¤è€…çš„åœºæ™¯ç¼–ç å™¨äº§ç”Ÿç»“æ„åŒ–çš„è¡¨ç¤ºå½¢å¼ï¼Œè¿™äº›è¡¨ç¤ºæ—¢è¯­ä¹‰ä¸Šåœ°æ ¹æ¤äºçŸ¥è¯†åˆç¬¦åˆæœ€ç»ˆçš„è§„åˆ’ç›®æ ‡ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒLLMåœ¨æ¨æ–­ä¸­æ˜¯å¯é€‰çš„ï¼Œèƒ½å¤Ÿå®ç°é«˜æ•ˆè€Œç¨³å¥çš„è§„åˆ’ã€‚ç»è¿‡DiMAè®­ç»ƒåï¼ŒåŸºäºè§†è§‰çš„è§„åˆ’å™¨çš„L2è½¨è¿¹è¯¯å·®é™ä½äº†37%ï¼Œç¢°æ’ç‡é™ä½äº†80%ï¼Œé•¿å°¾åœºæ™¯ä¸­çš„è½¨è¿¹è¯¯å·®é™ä½äº†44%ã€‚DiMAè¿˜åœ¨nuScenesè§„åˆ’åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†ä¸šç•Œé¢†å…ˆæ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>è‡ªä¸»é©¾é©¶éœ€è¦å®‰å…¨è¿åŠ¨è§„åˆ’ï¼Œç‰¹åˆ«æ˜¯åœ¨é•¿å°¾åœºæ™¯ä¸­ã€‚</li>
<li>ç«¯åˆ°ç«¯çš„è‡ªä¸»é©¾é©¶ç³»ç»Ÿåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æé«˜æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>ä½¿ç”¨LLMè¿›è¡Œæµ‹è¯•ä¼šå¯¼è‡´é«˜è®¡ç®—æˆæœ¬ã€‚</li>
<li>DiMAç³»ç»Ÿèƒ½å¤Ÿåœ¨æ— LLMçš„æƒ…å†µä¸‹ä¿æŒè§„åˆ’æ•ˆç‡å¹¶å¼•å…¥LLMçš„ä¸–ç•ŒçŸ¥è¯†ã€‚</li>
<li>DiMAé€šè¿‡ä¸“é—¨è®¾è®¡çš„æ›¿ä»£ä»»åŠ¡å°†LLMä¿¡æ¯æç‚¼å¹¶åº”ç”¨åˆ°åŸºäºè§†è§‰çš„è§„åˆ’å™¨ä¸­ã€‚</li>
<li>DiMAè®­ç»ƒåæ˜¾è‘—æé«˜åŸºäºè§†è§‰çš„è§„åˆ’å™¨æ€§èƒ½ï¼ŒåŒ…æ‹¬å‡å°‘è½¨è¿¹è¯¯å·®å’Œç¢°æ’ç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.09757">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a190b935718895415022ec0a583d3165.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ba88a0517c3d6d77a42eeae9fa4f5b65.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9227c617802f3a4ff25dfae781018582.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f4fa68d5acf5aae544036e34c7a792e4.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Lost-in-Translation-Found-in-Context-Sign-Language-Translation-with-Contextual-Cues"><a href="#Lost-in-Translation-Found-in-Context-Sign-Language-Translation-with-Contextual-Cues" class="headerlink" title="Lost in Translation, Found in Context: Sign Language Translation with   Contextual Cues"></a>Lost in Translation, Found in Context: Sign Language Translation with   Contextual Cues</h2><p><strong>Authors:Youngjoon Jang, Haran Raajesh, Liliane Momeni, GÃ¼l Varol, Andrew Zisserman</strong></p>
<p>Our objective is to translate continuous sign language into spoken language text. Inspired by the way human interpreters rely on context for accurate translation, we incorporate additional contextual cues together with the signing video, into a new translation framework. Specifically, besides visual sign recognition features that encode the input video, we integrate complementary textual information from (i) captions describing the background show, (ii) translation of previous sentences, as well as (iii) pseudo-glosses transcribing the signing. These are automatically extracted and inputted along with the visual features to a pre-trained large language model (LLM), which we fine-tune to generate spoken language translations in text form. Through extensive ablation studies, we show the positive contribution of each input cue to the translation performance. We train and evaluate our approach on BOBSL â€“ the largest British Sign Language dataset currently available. We show that our contextual approach significantly enhances the quality of the translations compared to previously reported results on BOBSL, and also to state-of-the-art methods that we implement as baselines. Furthermore, we demonstrate the generality of our approach by applying it also to How2Sign, an American Sign Language dataset, and achieve competitive results. </p>
<blockquote>
<p>æˆ‘ä»¬çš„ç›®æ ‡æ˜¯å°†è¿ç»­çš„æ‰‹è¯­ç¿»è¯‘æˆå£è¯­æ–‡æœ¬ã€‚å—äººç±»ç¿»è¯‘è€…ä¾èµ–ä¸Šä¸‹æ–‡è¿›è¡Œå‡†ç¡®ç¿»è¯‘çš„å¯å‘ï¼Œæˆ‘ä»¬å°†é¢å¤–çš„ä¸Šä¸‹æ–‡çº¿ç´¢ä¸æ‰‹è¯­è§†é¢‘ç»“åˆï¼Œèå…¥æ–°çš„ç¿»è¯‘æ¡†æ¶ã€‚å…·ä½“æ¥è¯´ï¼Œé™¤äº†ç¼–ç è¾“å…¥è§†é¢‘çš„è§†è§‰æ‰‹è¯­è¯†åˆ«ç‰¹å¾å¤–ï¼Œæˆ‘ä»¬è¿˜æ•´åˆäº†ï¼ˆiï¼‰æè¿°èƒŒæ™¯æ˜¾ç¤ºçš„å­—å¹•ã€ï¼ˆiiï¼‰ä¹‹å‰å¥å­çš„ç¿»è¯‘ä»¥åŠï¼ˆiiiï¼‰æ‰‹è¯­è½¬å½•çš„ä¼ªè¯­éŸ³ç­‰è¡¥å……æ–‡æœ¬ä¿¡æ¯ã€‚è¿™äº›è‡ªåŠ¨æå–å¹¶ä¸è§†è§‰ç‰¹å¾ä¸€èµ·è¾“å…¥åˆ°é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­ï¼Œæˆ‘ä»¬å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒä»¥ç”Ÿæˆå£è¯­å½¢å¼çš„æ–‡æœ¬ç¿»è¯‘ã€‚é€šè¿‡å¹¿æ³›çš„æ¶ˆèç ”ç©¶ï¼Œæˆ‘ä»¬è¯æ˜äº†æ¯ä¸ªè¾“å…¥çº¿ç´¢å¯¹ç¿»è¯‘æ€§èƒ½çš„ç§¯æè´¡çŒ®ã€‚æˆ‘ä»¬åœ¨BOBSLï¼ˆå½“å‰å¯ç”¨çš„æœ€å¤§è‹±å›½æ‰‹è¯­æ•°æ®é›†ï¼‰ä¸Šå¯¹æˆ‘ä»¬çš„æ–¹æ³•è¿›è¡Œäº†è®­ç»ƒå’Œè¯„ä¼°ã€‚æˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬çš„ä¸Šä¸‹æ–‡æ–¹æ³•åœ¨BOBSLä¸Šçš„ç¿»è¯‘è´¨é‡ç›¸æ¯”ä»¥å‰æŠ¥å‘Šçš„ç»“æœä»¥åŠæˆ‘ä»¬å®ç°çš„æœ€æ–°æ–¹æ³•ä½œä¸ºåŸºå‡†çº¿æœ‰æ˜¾è‘—æå‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†è¯¥æ–¹æ³•åº”ç”¨äºHow2Signï¼ˆç¾å›½æ‰‹è¯­æ•°æ®é›†ï¼‰ï¼Œå¹¶è·å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„ç»“æœï¼Œå±•ç¤ºäº†å…¶é€šç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.09754v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦ä»‹ç»äº†ä¸€ç§å°†è¿ç»­æ‰‹è¯­å®æ—¶ç¿»è¯‘æˆæ–‡å­—çš„æ–°æ–¹æ³•ã€‚è¯¥æ–¹æ³•å€Ÿé‰´äº†äººç±»ç¿»è¯‘ä¾èµ–è¯­å¢ƒè¿›è¡Œå‡†ç¡®ç¿»è¯‘çš„æ–¹å¼ï¼Œç»“åˆæ‰‹è¯­è§†é¢‘ä¸­çš„é¢å¤–è¯­å¢ƒçº¿ç´¢ï¼Œæ„å»ºäº†ä¸€ä¸ªæ–°çš„ç¿»è¯‘æ¡†æ¶ã€‚é€šè¿‡æ•´åˆæ‰‹è¯­è¯†åˆ«ç‰¹å¾ã€èƒŒæ™¯æè¿°å­—å¹•ã€å…ˆå‰å¥å­çš„ç¿»è¯‘ä»¥åŠæ‰‹è¯­è½¬å½•çš„ä¼ªæœ¯è¯­ç­‰æ–‡æœ¬ä¿¡æ¯ï¼Œè¾“å…¥åˆ°é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ï¼Œç”Ÿæˆå£è¯­å½¢å¼çš„ç¿»è¯‘ã€‚åœ¨BOBSLæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æé«˜äº†ç¿»è¯‘è´¨é‡ï¼Œå¹¶åœ¨How2Signæ•°æ®é›†ä¸Šå–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„ç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶ç›®æ ‡æ˜¯å°†è¿ç»­æ‰‹è¯­å®æ—¶ç¿»è¯‘æˆæ–‡å­—ã€‚</li>
<li>å€Ÿé‰´äººç±»ç¿»è¯‘ä¾èµ–è¯­å¢ƒçš„æ–¹å¼ï¼Œç»“åˆæ‰‹è¯­è§†é¢‘çš„é¢å¤–è¯­å¢ƒçº¿ç´¢æ„å»ºæ–°çš„ç¿»è¯‘æ¡†æ¶ã€‚</li>
<li>é€šè¿‡æ•´åˆå¤šç§æ–‡æœ¬ä¿¡æ¯ï¼ˆæ‰‹è¯­è¯†åˆ«ç‰¹å¾ã€èƒŒæ™¯æè¿°å­—å¹•ç­‰ï¼‰æ¥æé«˜ç¿»è¯‘å‡†ç¡®æ€§ã€‚</li>
<li>ä½¿ç”¨é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œå¹¶è¿›è¡Œå¾®è°ƒä»¥ç”Ÿæˆå£è¯­å½¢å¼çš„ç¿»è¯‘ã€‚</li>
<li>åœ¨BOBSLæ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œæé«˜äº†ç¿»è¯‘è´¨é‡ã€‚</li>
<li>å°†è¯¥æ–¹æ³•åº”ç”¨äºHow2Signæ•°æ®é›†ï¼Œå–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„ç»“æœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.09754">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d3fa6c22b0695c7556f700427b7366a4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0f4ff25d2c354490eee8d63a7df8b1e1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3c22852dc98eabead8edd925fc48a176.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Enhancing-Lexicon-Based-Text-Embeddings-with-Large-Language-Models"><a href="#Enhancing-Lexicon-Based-Text-Embeddings-with-Large-Language-Models" class="headerlink" title="Enhancing Lexicon-Based Text Embeddings with Large Language Models"></a>Enhancing Lexicon-Based Text Embeddings with Large Language Models</h2><p><strong>Authors:Yibin Lei, Tao Shen, Yu Cao, Andrew Yates</strong></p>
<p>Recent large language models (LLMs) have demonstrated exceptional performance on general-purpose text embedding tasks. While dense embeddings have dominated related research, we introduce the first Lexicon-based EmbeddiNgS (LENS) leveraging LLMs that achieve competitive performance on these tasks. Regarding the inherent tokenization redundancy issue and unidirectional attention limitations in traditional causal LLMs, LENS consolidates the vocabulary space through token embedding clustering, and investigates bidirectional attention and various pooling strategies. Specifically, LENS simplifies lexicon matching by assigning each dimension to a specific token cluster, where semantically similar tokens are grouped together, and unlocking the full potential of LLMs through bidirectional attention. Extensive experiments demonstrate that LENS outperforms dense embeddings on the Massive Text Embedding Benchmark (MTEB), delivering compact feature representations that match the sizes of dense counterparts. Notably, combining LENSE with dense embeddings achieves state-of-the-art performance on the retrieval subset of MTEB (i.e. BEIR). </p>
<blockquote>
<p>æœ€è¿‘çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é€šç”¨æ–‡æœ¬åµŒå…¥ä»»åŠ¡ä¸Šè¡¨ç°å‡ºäº†å“è¶Šçš„æ€§èƒ½ã€‚è™½ç„¶å¯†é›†åµŒå…¥åœ¨ç›¸å…³ç ”ç©¶ä¸­é•¿æœŸå æ®ä¸»å¯¼åœ°ä½ï¼Œä½†æˆ‘ä»¬å¼•å…¥äº†é¦–æ¬¾åŸºäºè¯å…¸çš„åµŒå…¥ï¼ˆLENSï¼‰ï¼Œåˆ©ç”¨LLMåœ¨è¿™äº›ä»»åŠ¡ä¸Šå®ç°äº†ç«äº‰æ€§çš„æ€§èƒ½ã€‚é’ˆå¯¹ä¼ ç»Ÿå› æœLLMä¸­çš„å›ºæœ‰åˆ†è¯å†—ä½™é—®é¢˜å’Œå•å‘æ³¨æ„åŠ›é™åˆ¶ï¼ŒLENSé€šè¿‡ä»¤ç‰ŒåµŒå…¥èšç±»æ•´åˆè¯æ±‡ç©ºé—´ï¼Œå¹¶ç ”ç©¶åŒå‘æ³¨æ„åŠ›å’Œå„ç§æ± ç­–ç•¥ã€‚å…·ä½“æ¥è¯´ï¼ŒLENSé€šè¿‡ä¸ºæ¯ä¸ªç»´åº¦åˆ†é…ç‰¹å®šçš„ä»¤ç‰Œé›†ç¾¤æ¥ç®€åŒ–è¯å…¸åŒ¹é…ï¼Œå…¶ä¸­è¯­ä¹‰ç›¸ä¼¼çš„ä»¤ç‰Œè¢«ç»„åˆåœ¨ä¸€èµ·ï¼Œå¹¶é€šè¿‡åŒå‘æ³¨æ„åŠ›é‡Šæ”¾LLMçš„å…¨éƒ¨æ½œåŠ›ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒLENSåœ¨å¤§å‹æ–‡æœ¬åµŒå…¥åŸºå‡†æµ‹è¯•ï¼ˆMTEBï¼‰ä¸Šçš„æ€§èƒ½ä¼˜äºå¯†é›†åµŒå…¥ï¼Œæä¾›ç´§å‡‘çš„ç‰¹å¾è¡¨ç¤ºï¼Œä¸å¯†é›†å¯¹åº”çš„è§„æ¨¡ç›¸åŒ¹é…ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå°†LENSä¸å¯†é›†åµŒå…¥ç›¸ç»“åˆï¼Œåœ¨MTEBçš„æ£€ç´¢å­é›†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ˆå³BEIRï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.09749v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é€šç”¨æ–‡æœ¬åµŒå…¥ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚å°½ç®¡å¯†é›†åµŒå…¥ç›¸å…³ç ”ç©¶å æ®ä¸»å¯¼ï¼Œæˆ‘ä»¬é¦–æ¬¡å¼•å…¥åˆ©ç”¨LLMçš„Lexicon-based EmbeddiNgSï¼ˆLENSï¼‰ï¼Œåœ¨è¿™äº›ä»»åŠ¡ä¸Šå®ç°å…·æœ‰ç«äº‰åŠ›çš„è¡¨ç°ã€‚é’ˆå¯¹ä¼ ç»Ÿå› æœLLMä¸­çš„å†…åœ¨ä»¤ç‰ŒåŒ–å†—ä½™é—®é¢˜å’Œå•å‘æ³¨æ„åŠ›é™åˆ¶ï¼ŒLENSé€šè¿‡ä»¤ç‰ŒåµŒå…¥èšç±»æ•´åˆè¯æ±‡ç©ºé—´ï¼Œå¹¶ç ”ç©¶åŒå‘æ³¨æ„åŠ›ä»¥åŠå„ç§æ± ç­–ç•¥ã€‚å…·ä½“æ¥è¯´ï¼ŒLENSç®€åŒ–äº†è¯æ±‡åŒ¹é…è¿‡ç¨‹ï¼Œå°†æ¯ä¸ªç»´åº¦åˆ†é…ç»™ç‰¹å®šçš„ä»¤ç‰Œç°‡ï¼Œå…¶ä¸­è¯­ä¹‰ç›¸ä¼¼çš„ä»¤ç‰Œä¼šè¢«ç»„åˆåœ¨ä¸€èµ·ï¼Œä»è€Œè§£é”LLMçš„æ½œåŠ›é€šè¿‡åŒå‘æ³¨æ„åŠ›ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒLENSåœ¨å¤§å‹æ–‡æœ¬åµŒå…¥åŸºå‡†æµ‹è¯•ï¼ˆMTEBï¼‰ä¸Šçš„æ€§èƒ½ä¼˜äºå¯†é›†åµŒå…¥ï¼Œäº§ç”Ÿç´§å‡‘çš„ç‰¹å¾è¡¨ç¤ºï¼Œå¯ä¸å¯†é›†å¯¹åº”çš„è§„æ¨¡ç›¸åŒ¹é…ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå°†LENSä¸å¯†é›†åµŒå…¥ç›¸ç»“åˆåœ¨MTEBæ£€ç´¢å­é›†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ˆå³BEIRï¼‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsåœ¨é€šç”¨æ–‡æœ¬åµŒå…¥ä»»åŠ¡ä¸Šè¡¨ç°å“è¶Šã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°çš„åŸºäºLLMçš„æ–‡æœ¬åµŒå…¥æ–¹æ³•â€”â€”Lexicon-based EmbeddiNgSï¼ˆLENSï¼‰ã€‚</li>
<li>LENSè§£å†³äº†ä¼ ç»ŸLLMä¸­çš„ä»¤ç‰ŒåŒ–å†—ä½™å’Œå•å‘æ³¨æ„åŠ›é™åˆ¶é—®é¢˜ã€‚</li>
<li>LENSé€šè¿‡ä»¤ç‰ŒåµŒå…¥èšç±»æ•´åˆè¯æ±‡ç©ºé—´ï¼Œå¹¶é‡‡ç”¨åŒå‘æ³¨æ„åŠ›å’Œæ± ç­–ç•¥ã€‚</li>
<li>LENSç®€åŒ–äº†è¯æ±‡åŒ¹é…è¿‡ç¨‹ï¼Œå°†è¯­ä¹‰ç›¸ä¼¼çš„ä»¤ç‰Œç»„åˆåœ¨ä¸€èµ·ã€‚</li>
<li>LENSåœ¨å¤§å‹æ–‡æœ¬åµŒå…¥åŸºå‡†æµ‹è¯•ï¼ˆMTEBï¼‰ä¸Šçš„æ€§èƒ½ä¼˜äºå¯†é›†åµŒå…¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.09749">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d6a1faf3407aeb18a7e4efaf7637a204.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-17a9385a7e749c7e053d81f2c189ef44.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d7f1b8b997b61ae7c3f306432a33310c.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Generating-particle-physics-Lagrangians-with-transformers"><a href="#Generating-particle-physics-Lagrangians-with-transformers" class="headerlink" title="Generating particle physics Lagrangians with transformers"></a>Generating particle physics Lagrangians with transformers</h2><p><strong>Authors:Yong Sheng Koay, Rikard Enberg, Stefano Moretti, Eliel Camargo-Molina</strong></p>
<p>In physics, Lagrangians provide a systematic way to describe laws governing physical systems. In the context of particle physics, they encode the interactions and behavior of the fundamental building blocks of our universe. By treating Lagrangians as complex, rule-based constructs similar to linguistic expressions, we trained a transformer model â€“ proven to be effective in natural language tasks â€“ to predict the Lagrangian corresponding to a given list of particles. We report on the transformerâ€™s performance in constructing Lagrangians respecting the Standard Model $\mathrm{SU}(3)\times \mathrm{SU}(2)\times \mathrm{U}(1)$ gauge symmetries. The resulting model is shown to achieve high accuracies (over 90%) with Lagrangians up to six matter fields, with the capacity to generalize beyond the training distribution, albeit within architectural constraints. We show through an analysis of input embeddings that the model has internalized concepts such as group representations and conjugation operations as it learned to generate Lagrangians. We make the model and training datasets available to the community. An interactive demonstration can be found at: \url{<a target="_blank" rel="noopener" href="https://huggingface.co/spaces/JoseEliel/generate-lagrangians%7D">https://huggingface.co/spaces/JoseEliel/generate-lagrangians}</a>. </p>
<blockquote>
<p>åœ¨ç‰©ç†å­¦ä¸­ï¼Œæ‹‰æ ¼æœ—æ—¥é‡ä¸ºæè¿°ç‰©ç†ç³»ç»Ÿçš„è§„å¾‹æä¾›äº†ä¸€ç§ç³»ç»Ÿçš„æ–¹æ³•ã€‚åœ¨ç²’å­ç‰©ç†å­¦çš„èƒŒæ™¯ä¸‹ï¼Œå®ƒä»¬ç¼–ç äº†æˆ‘ä»¬å®‡å®™åŸºæœ¬æ„æˆè¦ç´ çš„ç›¸äº’ä½œç”¨å’Œè¡Œä¸ºã€‚é€šè¿‡æŠŠæ‹‰æ ¼æœ—æ—¥é‡è§†ä¸ºå¤æ‚çš„ã€åŸºäºè§„åˆ™çš„ç»“æ„ï¼Œç±»ä¼¼äºè¯­è¨€è¡¨è¾¾ï¼Œæˆ‘ä»¬è®­ç»ƒäº†ä¸€ç§å˜å‹å™¨æ¨¡å‹â€”â€”åœ¨è‡ªç„¶è¯­è¨€ä»»åŠ¡ä¸­è¢«è¯æ˜æ˜¯æœ‰æ•ˆçš„â€”â€”æ¥é¢„æµ‹å¯¹åº”äºç»™å®šç²’å­åˆ—è¡¨çš„æ‹‰æ ¼æœ—æ—¥é‡ã€‚æˆ‘ä»¬æŠ¥å‘Šäº†å˜å‹å™¨åœ¨æ„å»ºå°Šé‡æ ‡å‡†æ¨¡å‹$\mathrm{SU}(3)\times \mathrm{SU}(2)\times \mathrm{U}(1)$è§„èŒƒå¯¹ç§°æ€§çš„æ‹‰æ ¼æœ—æ—¥é‡æ—¶çš„æ€§èƒ½ã€‚ç»“æœæ¨¡å‹æ˜¾ç¤ºï¼Œåœ¨æ‹‰æ ¼æœ—æ—¥é‡è¾¾åˆ°å…­ä¸ªç‰©è´¨åœºçš„æƒ…å†µä¸‹ï¼Œå…¶å‡†ç¡®ç‡é«˜è¾¾90%ä»¥ä¸Šï¼Œè™½ç„¶åœ¨æ¶æ„çº¦æŸå†…ï¼Œä½†å…¶èƒ½åŠ›å¯ä»¥æ¨å¹¿åˆ°è®­ç»ƒåˆ†å¸ƒä¹‹å¤–ã€‚é€šè¿‡å¯¹è¾“å…¥åµŒå…¥çš„åˆ†æï¼Œæˆ‘ä»¬è¡¨æ˜è¯¥æ¨¡å‹å·²ç»å†…åŒ–äº†è¯¸å¦‚ç¾¤è¡¨ç¤ºå’Œå…±è½­è¿ç®—ç­‰æ¦‚å¿µï¼Œå› ä¸ºå®ƒå­¦ä¼šäº†ç”Ÿæˆæ‹‰æ ¼æœ—æ—¥é‡ã€‚æˆ‘ä»¬å‘ç¤¾åŒºæä¾›æ¨¡å‹å’Œè®­ç»ƒæ•°æ®é›†ã€‚ä¸€ä¸ªäº¤äº’å¼æ¼”ç¤ºå¯ä»¥åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://huggingface.co/spaces/JoseEliel/generate-lagrangians">https://huggingface.co/spaces/JoseEliel/generate-lagrangians</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.09729v1">PDF</a> 32 pages, 11 figues, 18 tables</p>
<p><strong>Summary</strong></p>
<p>ç‰©ç†å­¦ä¸­ï¼Œæ‹‰æ ¼æœ—æ—¥å‡½æ•°æä¾›äº†ä¸€ç§æè¿°ç‰©ç†ç³»ç»Ÿè§„å¾‹çš„ç³»ç»Ÿæ–¹æ³•ã€‚åœ¨ç²’å­ç‰©ç†å­¦èƒŒæ™¯ä¸‹ï¼Œå®ƒä»¬ç¼–ç äº†å®‡å®™åŸºæœ¬æ„å»ºå—çš„ç›¸äº’ä½œç”¨å’Œè¡Œä¸ºã€‚æœ¬æ–‡æå‡ºå°†æ‹‰æ ¼æœ—æ—¥å‡½æ•°è§†ä¸ºå¤æ‚çš„è§„åˆ™ç»“æ„ï¼Œç±»ä¼¼äºè¯­è¨€è¡¨è¾¾å¼ï¼Œå¹¶è®­ç»ƒäº†ä¸€ç§å·²åœ¨è‡ªç„¶è¯­è¨€ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚çš„å˜å‹å™¨æ¨¡å‹æ¥é¢„æµ‹ç»™å®šç²’å­åˆ—è¡¨å¯¹åº”çš„æ‹‰æ ¼æœ—æ—¥å‡½æ•°ã€‚æœ¬æ–‡æŠ¥é“äº†è¯¥æ¨¡å‹åœ¨éµå¾ªæ ‡å‡†æ¨¡å‹SU(3)Ã—SU(2)Ã—U(1)è§„èŒƒå¯¹ç§°æ€§çš„å‰æä¸‹æ„å»ºæ‹‰æ ¼æœ—æ—¥çš„æ€§èƒ½ã€‚ç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨ç”Ÿæˆæ¶‰åŠå¤šè¾¾å…­ä¸ªç‰©è´¨åœºçš„æ‹‰æ ¼æœ—æ—¥å‡½æ•°æ—¶ï¼Œå‡†ç¡®ç‡è¶…è¿‡90%ï¼Œå¹¶åœ¨å»ºç­‘çº¦æŸå†…å…·æœ‰è¶…è¶Šè®­ç»ƒåˆ†å¸ƒçš„æ¦‚æ‹¬èƒ½åŠ›ã€‚é€šè¿‡å¯¹è¾“å…¥åµŒå…¥çš„åˆ†æï¼Œæˆ‘ä»¬å±•ç¤ºäº†è¯¥æ¨¡å‹å·²ç»æŒæ¡äº†è¯¸å¦‚ç¾¤è¡¨ç¤ºå’Œå…±è½­è¿ç®—ç­‰æ¦‚å¿µã€‚æˆ‘ä»¬å‘å…¬ä¼—æä¾›äº†æ¨¡å‹å’Œè®­ç»ƒæ•°æ®é›†ã€‚ä¸€ä¸ªäº¤äº’å¼æ¼”ç¤ºå¯ä»¥åœ¨é“¾æ¥ä¸­æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://huggingface.co/spaces/JoseEliel/generate-lagrangians">https://huggingface.co/spaces/JoseEliel/generate-lagrangians</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‹‰æ ¼æœ—æ—¥å‡½æ•°åœ¨ç‰©ç†å­¦ä¸­ç”¨äºæè¿°ç‰©ç†ç³»ç»Ÿçš„è§„å¾‹ï¼Œå°¤å…¶æ˜¯åœ¨ç²’å­ç‰©ç†å­¦ä¸­æè¿°åŸºæœ¬ç²’å­çš„ç›¸äº’ä½œç”¨å’Œè¡Œä¸ºã€‚</li>
<li>é€šè¿‡å°†æ‹‰æ ¼æœ—æ—¥å‡½æ•°è§†ä¸ºè¯­è¨€è¡¨è¾¾å¼å¹¶åº”ç”¨å˜å‹å™¨æ¨¡å‹ï¼Œå¯ä»¥å®ç°ç»™å®šç²’å­åˆ—è¡¨çš„æ‹‰æ ¼æœ—æ—¥é¢„æµ‹ã€‚</li>
<li>æ¨¡å‹åœ¨éµå¾ªæ ‡å‡†æ¨¡å‹SU(3)Ã—SU(2)Ã—U(1)è§„èŒƒå¯¹ç§°æ€§çš„æ¡ä»¶ä¸‹è¡¨ç°å‡ºé«˜æ€§èƒ½ã€‚</li>
<li>æ¨¡å‹åœ¨ç”Ÿæˆæ¶‰åŠå¤šè¾¾å…­ä¸ªç‰©è´¨åœºçš„æ‹‰æ ¼æœ—æ—¥å‡½æ•°æ—¶å‡†ç¡®ç‡è¶…è¿‡90%ã€‚</li>
<li>æ¨¡å‹å…·æœ‰æ¦‚æ‹¬èƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨å»ºç­‘çº¦æŸå†…è¶…è¶Šè®­ç»ƒåˆ†å¸ƒã€‚</li>
<li>è¾“å…¥åµŒå…¥åˆ†ææ˜¾ç¤ºæ¨¡å‹æŒæ¡äº†ç¾¤è¡¨ç¤ºå’Œå…±è½­è¿ç®—ç­‰æ¦‚å¿µã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.09729">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a05d1675bd4a28c614c41a4a85325707.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="LLM-Based-Routing-in-Mixture-of-Experts-A-Novel-Framework-for-Trading"><a href="#LLM-Based-Routing-in-Mixture-of-Experts-A-Novel-Framework-for-Trading" class="headerlink" title="LLM-Based Routing in Mixture of Experts: A Novel Framework for Trading"></a>LLM-Based Routing in Mixture of Experts: A Novel Framework for Trading</h2><p><strong>Authors:Kuan-Ming Liu, Ming-Chih Lo</strong></p>
<p>Recent advances in deep learning and large language models (LLMs) have facilitated the deployment of the mixture-of-experts (MoE) mechanism in the stock investment domain. While these models have demonstrated promising trading performance, they are often unimodal, neglecting the wealth of information available in other modalities, such as textual data. Moreover, the traditional neural network-based router selection mechanism fails to consider contextual and real-world nuances, resulting in suboptimal expert selection. To address these limitations, we propose LLMoE, a novel framework that employs LLMs as the router within the MoE architecture. Specifically, we replace the conventional neural network-based router with LLMs, leveraging their extensive world knowledge and reasoning capabilities to select experts based on historical price data and stock news. This approach provides a more effective and interpretable selection mechanism. Our experiments on multimodal real-world stock datasets demonstrate that LLMoE outperforms state-of-the-art MoE models and other deep neural network approaches. Additionally, the flexible architecture of LLMoE allows for easy adaptation to various downstream tasks. </p>
<blockquote>
<p>æœ€è¿‘æ·±åº¦å­¦ä¹ å’Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥ä¿ƒè¿›äº†ä¸“å®¶æ··åˆï¼ˆMoEï¼‰æœºåˆ¶åœ¨è‚¡ç¥¨æŠ•èµ„é¢†åŸŸçš„åº”ç”¨ã€‚è™½ç„¶è¿™äº›æ¨¡å‹å·²ç»è¡¨ç°å‡ºäº†æœ‰å‰æ™¯çš„äº¤æ˜“æ€§èƒ½ï¼Œä½†å®ƒä»¬é€šå¸¸æ˜¯å•æ¨¡æ€çš„ï¼Œå¿½ç•¥äº†å…¶ä»–æ¨¡æ€ï¼ˆå¦‚æ–‡æœ¬æ•°æ®ï¼‰ä¸­ä¸°å¯Œçš„ä¿¡æ¯ã€‚æ­¤å¤–ï¼ŒåŸºäºä¼ ç»Ÿç¥ç»ç½‘ç»œçš„è·¯ç”±å™¨é€‰æ‹©æœºåˆ¶æ²¡æœ‰è€ƒè™‘åˆ°ä¸Šä¸‹æ–‡å’Œç°å®ä¸–ç•Œä¸­çš„ç»†å¾®å·®åˆ«ï¼Œå¯¼è‡´ä¸“å®¶é€‰æ‹©ä¸ä½³ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†LLMoEè¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œå®ƒé‡‡ç”¨LLMä½œä¸ºMoEæ¶æ„ä¸­çš„è·¯ç”±å™¨ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ç”¨LLMæ›¿ä»£äº†åŸºäºå¸¸è§„ç¥ç»ç½‘ç»œçš„è·¯ç”±å™¨ï¼Œåˆ©ç”¨å…¶ä¸°å¯Œçš„ä¸–ç•ŒçŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ï¼Œæ ¹æ®å†å²ä»·æ ¼æ•°æ®å’Œè‚¡ç¥¨æ–°é—»é€‰æ‹©ä¸“å®¶ã€‚è¿™ç§æ–¹æ³•æä¾›äº†ä¸€ä¸ªæ›´æœ‰æ•ˆå’Œå¯è§£é‡Šçš„é€‰æ‹©æœºåˆ¶ã€‚æˆ‘ä»¬åœ¨å¤šæ¨¡æ€ç°å®ä¸–ç•Œè‚¡ç¥¨æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒLLMoEä¼˜äºæœ€æ–°çš„MoEæ¨¡å‹å’Œå…¶ä»–æ·±åº¦ç¥ç»ç½‘ç»œæ–¹æ³•ã€‚æ­¤å¤–ï¼ŒLLMoEçš„çµæ´»æ¶æ„å¯è½»æ¾é€‚åº”å„ç§ä¸‹æ¸¸ä»»åŠ¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.09636v1">PDF</a> Accepted by AAAI 2025 Workshop on AI for Social Impact - Bridging   Innovations in Finance, Social Media, and Crime Prevention</p>
<p><strong>Summary</strong><br>     æ·±åº¦å­¦ä¹ ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ€æ–°è¿›å±•ä¿ƒè¿›äº†æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æœºåˆ¶åœ¨è‚¡ç¥¨æŠ•èµ„é¢†åŸŸçš„åº”ç”¨ã€‚ä¼ ç»Ÿç¥ç»ç½‘ç»œè·¯ç”±é€‰æ‹©æœºåˆ¶ç¼ºä¹ä¸Šä¸‹æ–‡ä¸ç°å®ä¸–ç•Œçš„ç»†å¾®å·®åˆ«ï¼Œå¯¼è‡´ä¸“å®¶é€‰æ‹©ä¸å°½äººæ„ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºLLMoEæ¡†æ¶ï¼Œé‡‡ç”¨LLMä½œä¸ºMoEæ¶æ„ä¸­çš„è·¯ç”±å™¨ï¼ŒåŸºäºå†å²ä»·æ ¼æ•°æ®ä¸è‚¡ç¥¨æ–°é—»é€‰æ‹©ä¸“å®¶ã€‚å®éªŒè¯æ˜ï¼ŒLLMoEåœ¨æ¨¡æ€ç°å®è‚¡ç¥¨æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºæœ€å…ˆè¿›MoEæ¨¡å‹å’Œå…¶ä»–æ·±åº¦ç¥ç»ç½‘ç»œæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ åŠå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›å±•æ¨åŠ¨äº†æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æœºåˆ¶åœ¨è‚¡ç¥¨æŠ•èµ„ä¸­çš„åº”ç”¨ã€‚</li>
<li>ä¼ ç»Ÿç¥ç»ç½‘ç»œè·¯ç”±é€‰æ‹©æœºåˆ¶å­˜åœ¨å±€é™æ€§ï¼Œæ— æ³•å……åˆ†è€ƒè™‘ä¸Šä¸‹æ–‡ä¸ç°å®ä¸–ç•Œçš„ç»†å¾®å·®åˆ«ã€‚</li>
<li>LLMoEæ¡†æ¶é‡‡ç”¨LLMä½œä¸ºè·¯ç”±å™¨ï¼ŒåŸºäºå†å²ä»·æ ¼æ•°æ®ä¸è‚¡ç¥¨æ–°é—»é€‰æ‹©ä¸“å®¶ã€‚</li>
<li>LLMoEæ¡†æ¶åœ¨æ¨¡æ€ç°å®è‚¡ç¥¨æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</li>
<li>LLMoEæ¡†æ¶å…·æœ‰çµæ´»æ€§å’Œæ˜“é€‚åº”æ€§ï¼Œå¯è½»æ¾é€‚åº”å„ç§ä¸‹æ¸¸ä»»åŠ¡ã€‚</li>
<li>LLMsçš„ä¸°å¯ŒçŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›åœ¨è‚¡ç¥¨æŠ•èµ„é¢†åŸŸå…·æœ‰åº”ç”¨ä»·å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.09636">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cac5ea9a6cfb32ecc1e56317b879882f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c3a42e88b738b756ac1e6357b982a80a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-70c476004962a7c3351e42d261325042.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e532199298767c7cd68ed582970b40fe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-237f0e21d645cca6f05c865b282dd8ce.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="From-Scarcity-to-Capability-Empowering-Fake-News-Detection-in-Low-Resource-Languages-with-LLMs"><a href="#From-Scarcity-to-Capability-Empowering-Fake-News-Detection-in-Low-Resource-Languages-with-LLMs" class="headerlink" title="From Scarcity to Capability: Empowering Fake News Detection in   Low-Resource Languages with LLMs"></a>From Scarcity to Capability: Empowering Fake News Detection in   Low-Resource Languages with LLMs</h2><p><strong>Authors:Hrithik Majumdar Shibu, Shrestha Datta, Md. Sumon Miah, Nasrullah Sami, Mahruba Sharmin Chowdhury, Md. Saiful Islam</strong></p>
<p>The rapid spread of fake news presents a significant global challenge, particularly in low-resource languages like Bangla, which lack adequate datasets and detection tools. Although manual fact-checking is accurate, it is expensive and slow to prevent the dissemination of fake news. Addressing this gap, we introduce BanFakeNews-2.0, a robust dataset to enhance Bangla fake news detection. This version includes 11,700 additional, meticulously curated fake news articles validated from credible sources, creating a proportional dataset of 47,000 authentic and 13,000 fake news items across 13 categories. In addition, we created a manually curated independent test set of 460 fake and 540 authentic news items for rigorous evaluation. We invest efforts in collecting fake news from credible sources and manually verified while preserving the linguistic richness. We develop a benchmark system utilizing transformer-based architectures, including fine-tuned Bidirectional Encoder Representations from Transformers variants (F1-87%) and Large Language Models with Quantized Low-Rank Approximation (F1-89%), that significantly outperforms traditional methods. BanFakeNews-2.0 offers a valuable resource to advance research and application in fake news detection for low-resourced languages. We publicly release our dataset and model on Github to foster research in this direction. </p>
<blockquote>
<p>è™šå‡æ–°é—»çš„è¿…é€Ÿä¼ æ’­ç»™å…¨çƒå¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨åƒå­ŸåŠ æ‹‰è¯­è¿™æ ·çš„èµ„æºåŒ®ä¹çš„è¯­è¨€ä¸­ï¼Œç¼ºä¹è¶³å¤Ÿçš„æ•°æ®é›†å’Œæ£€æµ‹å·¥å…·ã€‚è™½ç„¶æ‰‹åŠ¨äº‹å®æ ¸æŸ¥æ˜¯å‡†ç¡®çš„ï¼Œä½†å…¶åœ¨é˜»æ­¢è™šå‡æ–°é—»çš„ä¼ æ’­æ–¹é¢æ˜¯æ˜‚è´µä¸”ç¼“æ…¢çš„ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†BanFakeNews-2.0ï¼Œè¿™æ˜¯ä¸€ä¸ªå¢å¼ºå­ŸåŠ æ‹‰è¯­è™šå‡æ–°é—»æ£€æµ‹èƒ½åŠ›çš„ç¨³å¥æ•°æ®é›†ã€‚æ­¤ç‰ˆæœ¬æ–°å¢äº†11,700ç¯‡ç²¾å¿ƒç­–åˆ’çš„è™šå‡æ–°é—»æ–‡ç« ï¼Œè¿™äº›æ–‡ç« æ¥è‡ªå¯é æ¥æºå¹¶ç»è¿‡äº†éªŒè¯ï¼Œåˆ›å»ºäº†åŒ…å«47,000ç¯‡çœŸå®å’Œ13,000ç¯‡è™šå‡æ–°é—»æ–‡ç« çš„å‡è¡¡æ•°æ®é›†ï¼Œæ¶µç›–13ä¸ªç±»åˆ«ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜åˆ›å»ºäº†åŒ…å«460ç¯‡è™šå‡æ–°é—»å’Œ540ç¯‡çœŸå®æ–°é—»çš„ç‹¬ç«‹æµ‹è¯•é›†ï¼Œä»¥ä¾¿è¿›è¡Œä¸¥æ ¼è¯„ä¼°ã€‚æˆ‘ä»¬åŠªåŠ›ä»å¯é æ¥æºæ”¶é›†è™šå‡æ–°é—»ï¼Œå¹¶è¿›è¡Œæ‰‹åŠ¨éªŒè¯ï¼ŒåŒæ—¶ä¿ç•™è¯­è¨€çš„ä¸°å¯Œæ€§ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªåŸºäºTransformeræ¶æ„çš„åŸºå‡†ç³»ç»Ÿï¼ŒåŒ…æ‹¬å¾®è°ƒåçš„åŒå‘ç¼–ç å™¨è¡¨ç¤ºï¼ˆF1-87ï¼…ï¼‰å’Œå…·æœ‰é‡åŒ–ä½ç§©é€¼è¿‘çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆF1-89ï¼…ï¼‰ï¼Œè¯¥ç³»ç»Ÿæ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚BanFakeNews-2.0ä¸ºæ¨è¿›è™šå‡æ–°é—»æ£€æµ‹ç ”ç©¶ä¸åº”ç”¨æä¾›äº†å®è´µèµ„æºï¼Œç‰¹åˆ«æ˜¯å¯¹äºèµ„æºåŒ®ä¹çš„è¯­è¨€ã€‚æˆ‘ä»¬åœ¨GitHubä¸Šå…¬å¼€å‘å¸ƒäº†æˆ‘ä»¬çš„æ•°æ®é›†å’Œæ¨¡å‹ï¼Œä»¥ä¿ƒè¿›è¿™ä¸€æ–¹å‘çš„ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.09604v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>Banglaè¯­ç”±äºç¼ºä¹è¶³å¤Ÿçš„è¯­æ–™åº“å’Œæ£€æµ‹å·¥å…·ï¼Œå‡æ–°é—»çš„å¿«é€Ÿä¼ æ’­æˆä¸ºä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ã€‚å°½ç®¡æ‰‹åŠ¨äº‹å®æ ¸æŸ¥å¾ˆå‡†ç¡®ï¼Œä½†å…¶é«˜æ˜‚çš„æˆæœ¬å’Œç¼“æ…¢çš„è¿›åº¦æ— æ³•é˜»æ­¢å‡æ–°é—»çš„æ•£å¸ƒã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæ¨å‡ºBanFakeNews-2.0æ•°æ®é›†ï¼Œæ—¨åœ¨å¢å¼ºBanglaå‡æ–°é—»çš„æ£€æµ‹èƒ½åŠ›ã€‚æ­¤ç‰ˆæœ¬æ–°å¢äº†æ¥è‡ªå¯é æ¥æºçš„11700ç¯‡ç²¾å¿ƒç­–åˆ’çš„å‡æ–°é—»æ–‡ç« ï¼Œåˆ›å»ºäº†åŒ…å«æ¯”ä¾‹é€‚å½“çš„è¯­æ–™åº“ï¼Œæ¶µç›–çœŸå®æ–°é—»47000ç¯‡å’Œå‡æ–°é—»13000ç¯‡ï¼Œå…±æ¶‰åŠ13ä¸ªç±»åˆ«ã€‚åŒæ—¶åˆ›å»ºäº†ä¸€ä¸ªç‹¬ç«‹çš„æ‰‹åŠ¨éªŒè¯æµ‹è¯•é›†ï¼ŒåŒ…æ‹¬460ç¯‡å‡æ–°é—»å’Œ540ç¯‡çœŸå®æ–°é—»ï¼Œç”¨äºä¸¥æ ¼çš„è¯„ä¼°ã€‚æˆ‘ä»¬è‡´åŠ›äºæ”¶é›†æ¥è‡ªå¯é æ¥æºçš„å‡æ–°é—»å¹¶è¿›è¡Œæ‰‹åŠ¨éªŒè¯ï¼ŒåŒæ—¶ä¿ç•™è¯­è¨€çš„ä¸°å¯Œæ€§ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªåŸºäºTransformeræ¶æ„çš„åŸºå‡†ç³»ç»Ÿï¼ŒåŒ…æ‹¬å¾®è°ƒåçš„åŒå‘ç¼–ç å™¨è¡¨ç¤ºï¼ˆF1-87%ï¼‰å’Œä½ç§©è¿‘ä¼¼é‡åŒ–çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆF1-89%ï¼‰ï¼Œæ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚BanFakeNews-2.0å¯¹äºæ¨åŠ¨ä½èµ„æºè¯­è¨€å‡æ–°é—»æ£€æµ‹çš„ç ”ç©¶å’Œåº”ç”¨å…·æœ‰é‡è¦æ„ä¹‰ã€‚æˆ‘ä»¬åœ¨GitHubä¸Šå…¬å¼€å‘å¸ƒæ•°æ®é›†å’Œæ¨¡å‹ï¼Œä»¥ä¿ƒè¿›è¿™ä¸€æ–¹å‘çš„ç ”ç©¶å‘å±•ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å‡æ–°é—»åœ¨å…¨çƒèŒƒå›´å†…çš„ä¼ æ’­æ„æˆé‡å¤§æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨ç¼ºä¹æ•°æ®å’Œæ£€æµ‹å·¥å…·çš„Banglaç­‰ä½èµ„æºè¯­è¨€ä¸­å°¤ä¸ºæ˜æ˜¾ã€‚</li>
<li>æ‰‹åŠ¨äº‹å®æ ¸æŸ¥è™½ç„¶å‡†ç¡®ä½†æˆæœ¬é«˜æ˜‚ä¸”è¿›åº¦ç¼“æ…¢ï¼Œéš¾ä»¥æ»¡è¶³é˜»æ­¢å‡æ–°é—»ä¼ æ’­çš„éœ€æ±‚ã€‚</li>
<li>æ¨å‡ºBanFakeNews-2.0æ•°æ®é›†ï¼ŒåŒ…å«çœŸå®æ–°é—»å’Œå‡æ–°é—»æ–‡ç« æ•°é‡å¾—åˆ°æ˜¾è‘—æ‰©å……ï¼Œä¾¿äºæ„å»ºæ›´ä¸ºå®Œå–„çš„æ£€æµ‹æ¨¡å‹ã€‚</li>
<li>é‡‡ç”¨åŸºäºTransformeræ¶æ„å¼€å‘åŸºå‡†ç³»ç»Ÿï¼ŒåŒ…æ‹¬å¾®è°ƒåŒå‘ç¼–ç å™¨è¡¨ç¤ºå’Œä½ç§©è¿‘ä¼¼é‡åŒ–çš„è¯­è¨€æ¨¡å‹ç­‰å…ˆè¿›æŠ€æœ¯ï¼Œæ˜¾è‘—æé«˜å‡æ–°é—»æ£€æµ‹å‡†ç¡®ç‡ã€‚</li>
<li>æ‰‹åŠ¨éªŒè¯ç¡®ä¿æ•°æ®é›†å’Œæ¨¡å‹çš„æœ‰æ•ˆæ€§å’Œå‡†ç¡®æ€§ï¼Œå¼ºè°ƒè¯­è¨€å­¦ä¸Šçš„ä¸°å¯Œæ€§å¯¹æ¨¡å‹è®­ç»ƒçš„é‡è¦æ€§ã€‚</li>
<li>æ•°æ®é›†åŠæ¨¡å‹çš„å…¬å¼€æä¾›æœ‰åˆ©äºä½èµ„æºè¯­è¨€å‡æ–°é—»æ£€æµ‹ç ”ç©¶çš„è¿›æ­¥ä¸åº”ç”¨å¼€å‘ã€‚æ­¤ä¸¾å¯é€šè¿‡å…±äº«å¹³å°å’Œèµ„æºåŠ å¼ºè¯¥é¢†åŸŸçš„åä½œä¸åˆ›æ–°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.09604">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d3c295b8a4447045bd2d6a56d1b816fb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e522a19e59c550cd46c787aa2ad8b83e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d5985ad83a99bc70f3ae4b808a4298ac.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9cb4d7872b4e78b9f2f6bf2ece85b4a7.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Atleus-Accelerating-Transformers-on-the-Edge-Enabled-by-3D-Heterogeneous-Manycore-Architectures"><a href="#Atleus-Accelerating-Transformers-on-the-Edge-Enabled-by-3D-Heterogeneous-Manycore-Architectures" class="headerlink" title="Atleus: Accelerating Transformers on the Edge Enabled by 3D   Heterogeneous Manycore Architectures"></a>Atleus: Accelerating Transformers on the Edge Enabled by 3D   Heterogeneous Manycore Architectures</h2><p><strong>Authors:Pratyush Dhingra, Janardhan Rao Doppa, Partha Pratim Pande</strong></p>
<p>Transformer architectures have become the standard neural network model for various machine learning applications including natural language processing and computer vision. However, the compute and memory requirements introduced by transformer models make them challenging to adopt for edge applications. Furthermore, fine-tuning pre-trained transformers (e.g., foundation models) is a common task to enhance the modelâ€™s predictive performance on specific tasks&#x2F;applications. Existing transformer accelerators are oblivious to complexities introduced by fine-tuning. In this paper, we propose the design of a three-dimensional (3D) heterogeneous architecture referred to as Atleus that incorporates heterogeneous computing resources specifically optimized to accelerate transformer models for the dual purposes of fine-tuning and inference. Specifically, Atleus utilizes non-volatile memory and systolic array for accelerating transformer computational kernels using an integrated 3D platform. Moreover, we design a suitable NoC to achieve high performance and energy efficiency. Finally, Atleus adopts an effective quantization scheme to support model compression. Experimental results demonstrate that Atleus outperforms existing state-of-the-art by up to 56x and 64.5x in terms of performance and energy efficiency respectively </p>
<blockquote>
<p>Transformeræ¶æ„å·²æˆä¸ºå„ç§æœºå™¨å­¦ä¹ åº”ç”¨çš„æ ‡å‡†ç¥ç»ç½‘ç»œæ¨¡å‹ï¼ŒåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†å’Œè®¡ç®—æœºè§†è§‰ã€‚ç„¶è€Œï¼ŒTransformeræ¨¡å‹å¸¦æ¥çš„è®¡ç®—å’Œå†…å­˜è¦æ±‚ï¼Œä½¿å¾—å…¶åœ¨è¾¹ç¼˜åº”ç”¨çš„é‡‡ç”¨é¢ä¸´æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œå¾®è°ƒé¢„è®­ç»ƒçš„Transformerï¼ˆä¾‹å¦‚åŸºç¡€æ¨¡å‹ï¼‰æ˜¯ä¸€ä¸ªå¸¸è§ä»»åŠ¡ï¼Œæ—¨åœ¨æé«˜æ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡&#x2F;åº”ç”¨ä¸Šçš„é¢„æµ‹æ€§èƒ½ã€‚ç°æœ‰çš„TransformeråŠ é€Ÿå™¨å¯¹å¾®è°ƒå¼•å…¥çš„å¤æ‚æ€§ç¼ºä¹è®¤è¯†ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºAtleusçš„ä¸‰ç»´å¼‚æ„æ¶æ„è®¾è®¡ï¼Œè¯¥æ¶æ„ç»“åˆäº†å¼‚æ„è®¡ç®—èµ„æºï¼Œä¸“é—¨ç”¨äºåŠ é€ŸTransformeræ¨¡å‹çš„å¾®è°ƒä¸æ¨ç†ã€‚å…·ä½“æ¥è¯´ï¼ŒAtleusåˆ©ç”¨éæ˜“å¤±æ€§å†…å­˜å’Œè„‰åŠ¨é˜µåˆ—ï¼Œé€šè¿‡é›†æˆä¸‰ç»´å¹³å°æ¥åŠ é€ŸTransformerè®¡ç®—å†…æ ¸ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§åˆé€‚çš„ç½‘ç»œèŠ¯ç‰‡äº’è”æŠ€æœ¯ï¼ˆNoCï¼‰ï¼Œä»¥å®ç°é«˜æ€§èƒ½å’Œèƒ½æ•ˆã€‚æœ€åï¼ŒAtleusé‡‡ç”¨æœ‰æ•ˆçš„é‡åŒ–æ–¹æ¡ˆæ¥æ”¯æŒæ¨¡å‹å‹ç¼©ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨æ€§èƒ½å’Œèƒ½æ•ˆæ–¹é¢ï¼ŒAtleusåˆ†åˆ«è¾¾åˆ°äº†æœ€é«˜56å€å’Œæœ€é«˜è‡³64.5å€çš„æœ€ä½³çŠ¶æ€</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.09588v1">PDF</a> Accepted for Publication in IEEE Transactions on Computer-Aided   Design of Integrated Circuits and Systems (TCAD)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºAtleusçš„ä¸‰ç»´å¼‚æ„æ¶æ„ï¼Œè¯¥æ¶æ„ä¸“é—¨ä¼˜åŒ–äº†ç”¨äºåŠ é€Ÿè½¬æ¢å™¨æ¨¡å‹çš„å¾®è°ƒä¸æ¨æ–­ä»»åŠ¡ã€‚é€šè¿‡åˆ©ç”¨éæ˜“å¤±æ€§å†…å­˜å’Œè„‰åŠ¨é˜µåˆ—ï¼ŒAtleusåœ¨ä¸€ä¸ªé›†æˆåŒ–çš„ä¸‰ç»´å¹³å°ä¸ŠåŠ é€Ÿè½¬æ¢å™¨è®¡ç®—å†…æ ¸ã€‚æ­¤å¤–ï¼Œè®¾è®¡äº†ä¸€ç§é«˜æ•ˆçš„NoCï¼Œä»¥å®ç°é«˜æ€§èƒ½å’Œèƒ½æºæ•ˆç‡ã€‚Atleusè¿˜é‡‡ç”¨æœ‰æ•ˆçš„é‡åŒ–æ–¹æ¡ˆä»¥æ”¯æŒæ¨¡å‹å‹ç¼©ï¼Œå¹¶ä¸”å®éªŒç»“æœè¡¨æ˜å…¶æ€§èƒ½ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œæ€§èƒ½æå‡æœ€é«˜è¾¾56å€ï¼Œèƒ½æ•ˆæå‡æœ€é«˜è¾¾64.5å€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Transformeræ¶æ„å·²æˆä¸ºå„ç§æœºå™¨å­¦ä¹ åº”ç”¨çš„æ ‡å‡†ç¥ç»ç½‘ç»œæ¨¡å‹ï¼ŒåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†å’Œè®¡ç®—æœºè§†è§‰ã€‚</li>
<li>è½¬æ¢å™¨æ¨¡å‹å¯¹è®¡ç®—å’Œå†…å­˜çš„éœ€æ±‚ä½¿å…¶éš¾ä»¥åœ¨è¾¹ç¼˜åº”ç”¨ä¸­ä½¿ç”¨ã€‚</li>
<li>å¯¹é¢„è®­ç»ƒçš„è½¬æ¢å™¨è¿›è¡Œå¾®è°ƒæ˜¯å¢å¼ºæ¨¡å‹å¯¹ç‰¹å®šä»»åŠ¡&#x2F;åº”ç”¨é¢„æµ‹æ€§èƒ½çš„ä¸€ç§å¸¸è§ä»»åŠ¡ã€‚ç°æœ‰çš„è½¬æ¢å™¨åŠ é€Ÿå™¨å¿½ç•¥äº†å¾®è°ƒå¸¦æ¥çš„å¤æ‚æ€§ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºAtleusçš„ä¸‰ç»´å¼‚æ„æ¶æ„ï¼Œæ—¨åœ¨åŠ é€Ÿè½¬æ¢å™¨æ¨¡å‹çš„å¾®è°ƒä¸æ¨æ–­ã€‚</li>
<li>Atleusåˆ©ç”¨éæ˜“å¤±æ€§å†…å­˜å’Œè„‰åŠ¨é˜µåˆ—æ¥åŠ é€Ÿè½¬æ¢å™¨è®¡ç®—å†…æ ¸ã€‚</li>
<li>Atleusè®¾è®¡äº†ä¸€ç§é«˜æ•ˆçš„NoCï¼Œä»¥å®ç°é«˜æ€§èƒ½å’Œèƒ½æºæ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.09588">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-37e0d474b6e42ed44c48f968b00acc96.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ca75bda4b44efd596708fc25793fc318.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-38828c5f78085617cca12dd2e5cd763a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2b6ae7fa95ba4a876ca7a26f6689b52e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0272fc103a38ce4eb48c30bbe6db9e55.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7528a67c943a01071c7cad6742d824f9.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Confidence-Estimation-for-Error-Detection-in-Text-to-SQL-Systems"><a href="#Confidence-Estimation-for-Error-Detection-in-Text-to-SQL-Systems" class="headerlink" title="Confidence Estimation for Error Detection in Text-to-SQL Systems"></a>Confidence Estimation for Error Detection in Text-to-SQL Systems</h2><p><strong>Authors:Oleg Somov, Elena Tutubalina</strong></p>
<p>Text-to-SQL enables users to interact with databases through natural language, simplifying the retrieval and synthesis of information. Despite the success of large language models (LLMs) in converting natural language questions into SQL queries, their broader adoption is limited by two main challenges: achieving robust generalization across diverse queries and ensuring interpretative confidence in their predictions. To tackle these issues, our research investigates the integration of selective classifiers into Text-to-SQL systems. We analyse the trade-off between coverage and risk using entropy based confidence estimation with selective classifiers and assess its impact on the overall performance of Text-to-SQL models. Additionally, we explore the modelsâ€™ initial calibration and improve it with calibration techniques for better model alignment between confidence and accuracy. Our experimental results show that encoder-decoder T5 is better calibrated than in-context-learning GPT 4 and decoder-only Llama 3, thus the designated external entropy-based selective classifier has better performance. The study also reveal that, in terms of error detection, selective classifier with a higher probability detects errors associated with irrelevant questions rather than incorrect query generations. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°SQLä½¿ç”¨æˆ·èƒ½å¤Ÿé€šè¿‡è‡ªç„¶è¯­è¨€ä¸æ•°æ®åº“è¿›è¡Œäº¤äº’ï¼Œç®€åŒ–äº†ä¿¡æ¯çš„æ£€ç´¢å’Œåˆæˆã€‚å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å°†è‡ªç„¶è¯­è¨€é—®é¢˜è½¬æ¢ä¸ºSQLæŸ¥è¯¢æ–¹é¢å–å¾—äº†æˆåŠŸï¼Œä½†å®ƒä»¬æ›´å¹¿æ³›çš„é‡‡ç”¨å—åˆ°ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜çš„é™åˆ¶ï¼šå®ç°è·¨ä¸åŒæŸ¥è¯¢çš„ç¨³å¥æ³›åŒ–ï¼Œä»¥åŠç¡®ä¿å¯¹å…¶é¢„æµ‹çš„è§£é‡Šä¿¡å¿ƒã€‚ä¸ºäº†åº”å¯¹è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬çš„ç ”ç©¶æ¢è®¨äº†é€‰æ‹©æ€§åˆ†ç±»å™¨åœ¨æ–‡æœ¬åˆ°SQLç³»ç»Ÿä¸­çš„é›†æˆã€‚æˆ‘ä»¬ä½¿ç”¨åŸºäºç†µçš„ç½®ä¿¡ä¼°è®¡æ¥åˆ†æè¦†ç›–ç‡å’Œé£é™©ä¹‹é—´çš„æƒè¡¡ï¼Œå¹¶è¯„ä¼°å…¶å¯¹æ–‡æœ¬åˆ°SQLæ¨¡å‹æ•´ä½“æ€§èƒ½çš„å½±å“ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ¢ç´¢äº†æ¨¡å‹çš„åˆå§‹æ ¡å‡†ï¼Œå¹¶é€šè¿‡æ ¡å‡†æŠ€æœ¯å¯¹å…¶è¿›è¡Œæ”¹è¿›ï¼Œä»¥æ›´å¥½åœ°å®ç°ç½®ä¿¡å’Œå‡†ç¡®æ€§ä¹‹é—´çš„æ¨¡å‹å¯¹é½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç¼–ç å™¨-è§£ç å™¨T5çš„æ ¡å‡†æ•ˆæœä¼˜äºä¸Šä¸‹æ–‡å­¦ä¹ GPT 4å’Œä»…è§£ç å™¨Llama 3ï¼Œå› æ­¤æŒ‡å®šçš„å¤–éƒ¨åŸºäºç†µçš„é€‰æ‹©æ€§åˆ†ç±»å™¨å…·æœ‰æ›´å¥½çš„æ€§èƒ½ã€‚è¯¥ç ”ç©¶è¿˜è¡¨æ˜ï¼Œå°±é”™è¯¯æ£€æµ‹è€Œè¨€ï¼Œå…·æœ‰è¾ƒé«˜æ¦‚ç‡çš„é€‰æ‹©æ€§åˆ†ç±»å™¨æ›´æ“…é•¿æ£€æµ‹ä¸æ— å…³é—®é¢˜ç›¸å…³çš„é”™è¯¯ï¼Œè€Œä¸æ˜¯ä¸æ­£ç¡®çš„æŸ¥è¯¢ç”Ÿæˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.09527v1">PDF</a> 15 pages, 11 figures, to be published in AAAI 2025 Proceedings</p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬ä»‹ç»äº†å¦‚ä½•é€šè¿‡é›†æˆé€‰æ‹©æ€§åˆ†ç±»å™¨æ¥è§£å†³æ–‡æœ¬åˆ°SQLè½¬æ¢ç³»ç»Ÿä¸­çš„ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šå®ç°è·¨ä¸åŒæŸ¥è¯¢çš„ç¨³å¥æ³›åŒ–å’Œç¡®ä¿é¢„æµ‹çš„å¯è§£é‡Šæ€§ä¿¡å¿ƒã€‚ç ”ç©¶æ¢è®¨äº†åŸºäºç†µçš„ç½®ä¿¡ä¼°è®¡çš„æƒè¡¡ï¼Œå¹¶è¯„ä¼°äº†å…¶å¯¹æ–‡æœ¬åˆ°SQLæ¨¡å‹æ•´ä½“æ€§èƒ½çš„å½±å“ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜æ¢ç´¢äº†æ¨¡å‹çš„åˆå§‹æ ¡å‡†é—®é¢˜ï¼Œå¹¶ä½¿ç”¨æ ¡å‡†æŠ€æœ¯æé«˜å…¶æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç¼–ç å™¨-è§£ç å™¨T5ç›¸è¾ƒäºä¸Šä¸‹æ–‡å­¦ä¹ GPT 4å’Œä»…è§£ç å™¨Llama 3å…·æœ‰æ›´å¥½çš„æ ¡å‡†æ€§èƒ½ï¼Œæ‰€è®¾è®¡çš„å¤–ç½®åŸºäºç†µçš„é€‰æ‹©æ€§åˆ†ç±»å™¨è¡¨ç°æ›´ä½³ã€‚åŒæ—¶å‘ç°ï¼Œé€‰æ‹©æ€§åˆ†ç±»å™¨åœ¨æ£€æµ‹åˆ°ä¸é—®é¢˜ä¸ç›¸å…³é”™è¯¯æ–¹é¢çš„æ¦‚ç‡æ›´é«˜ï¼Œè€Œéé”™è¯¯çš„æŸ¥è¯¢ç”Ÿæˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬åˆ°SQLæŠ€æœ¯ç®€åŒ–äº†æ•°æ®åº“çš„ä¿¡æ¯æ£€ç´¢ä¸åˆæˆï¼Œè®©ç”¨æˆ·é€šè¿‡è‡ªç„¶è¯­è¨€ä¸æ•°æ®åº“è¿›è¡Œäº¤äº’ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å°†è‡ªç„¶è¯­è¨€é—®é¢˜è½¬æ¢ä¸ºSQLæŸ¥è¯¢æ–¹é¢å–å¾—äº†æˆåŠŸï¼Œä½†ä»é¢ä¸´ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šæ³›åŒ–èƒ½åŠ›å’Œé¢„æµ‹çš„å¯è§£é‡Šæ€§ã€‚</li>
<li>ç ”ç©¶é€šè¿‡é›†æˆé€‰æ‹©æ€§åˆ†ç±»å™¨æ¥è§£å†³ä¸Šè¿°æŒ‘æˆ˜ï¼Œä»¥æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œé¢„æµ‹çš„å‡†ç¡®æ€§åŠå¯è§£é‡Šæ€§ã€‚</li>
<li>åŸºäºç†µçš„ç½®ä¿¡ä¼°è®¡è¢«ç”¨äºåˆ†æè¦†ç›–é¢ä¸é£é™©ä¹‹é—´çš„æƒè¡¡ï¼Œä»¥è¯„ä¼°å¯¹æ¨¡å‹æ•´ä½“æ€§èƒ½çš„å½±å“ã€‚</li>
<li>ç ”ç©¶å‘ç°ç¼–ç å™¨-è§£ç å™¨T5ç›¸è¾ƒäºå…¶ä»–æ¨¡å‹å…·æœ‰æ›´å¥½çš„æ ¡å‡†æ€§èƒ½ã€‚</li>
<li>å¤–éƒ¨åŸºäºç†µçš„é€‰æ‹©æ€§åˆ†ç±»å™¨è¡¨ç°æ›´ä½³ï¼Œç‰¹åˆ«æ˜¯åœ¨æ£€æµ‹ä¸é—®é¢˜ä¸ç›¸å…³çš„é”™è¯¯æ–¹é¢ã€‚</li>
<li>é”™è¯¯æ›´å¤šåœ°ä¸é—®é¢˜ä¸ç›¸å…³æ€§ç›¸å…³ï¼Œè€Œéé”™è¯¯çš„æŸ¥è¯¢ç”Ÿæˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.09527">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-19b2d4d38a49b64a440b7688a1467047.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-886aea6744df2c764b08a92a4eef9eb0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fd39c23078a013290e15f7dee7416d12.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eba93da7721d3c3119b4f210f248ef5f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f0293a9152fb205c936f8b0785dc0ef7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-17b567f0ae4ec2a48736463988d8493b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-98f5cf97419b20cf8c7c21757c86786b.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Omni-Emotion-Extending-Video-MLLM-with-Detailed-Face-and-Audio-Modeling-for-Multimodal-Emotion-Analysis"><a href="#Omni-Emotion-Extending-Video-MLLM-with-Detailed-Face-and-Audio-Modeling-for-Multimodal-Emotion-Analysis" class="headerlink" title="Omni-Emotion: Extending Video MLLM with Detailed Face and Audio Modeling   for Multimodal Emotion Analysis"></a>Omni-Emotion: Extending Video MLLM with Detailed Face and Audio Modeling   for Multimodal Emotion Analysis</h2><p><strong>Authors:Qize Yang, Detao Bai, Yi-Xing Peng, Xihan Wei</strong></p>
<p>Understanding emotions accurately is essential for fields like human-computer interaction. Due to the complexity of emotions and their multi-modal nature (e.g., emotions are influenced by facial expressions and audio), researchers have turned to using multi-modal models to understand human emotions rather than single-modality. However, current video multi-modal large language models (MLLMs) encounter difficulties in effectively integrating audio and identifying subtle facial micro-expressions. Furthermore, the lack of detailed emotion analysis datasets also limits the development of multimodal emotion analysis. To address these issues, we introduce a self-reviewed dataset and a human-reviewed dataset, comprising 24,137 coarse-grained samples and 3,500 manually annotated samples with detailed emotion annotations, respectively. These datasets allow models to learn from diverse scenarios and better generalize to real-world applications. Moreover, in addition to the audio modeling, we propose to explicitly integrate facial encoding models into the existing advanced Video MLLM, enabling the MLLM to effectively unify audio and the subtle facial cues for emotion understanding. By aligning these features within a unified space and employing instruction tuning in our proposed datasets, our Omni-Emotion achieves state-of-the-art performance in both emotion recognition and reasoning tasks. </p>
<blockquote>
<p>å‡†ç¡®åœ°ç†è§£æƒ…ç»ªå¯¹äºäººæœºäº¤äº’ç­‰é¢†åŸŸè‡³å…³é‡è¦ã€‚ç”±äºæƒ…ç»ªçš„å¤æ‚æ€§å’Œå…¶å¤šæ¨¡æ€æ€§è´¨ï¼ˆä¾‹å¦‚ï¼Œæƒ…ç»ªå—åˆ°é¢éƒ¨è¡¨æƒ…å’ŒéŸ³é¢‘çš„å½±å“ï¼‰ï¼Œç ”ç©¶äººå‘˜å·²ç»è½¬å‘ä½¿ç”¨å¤šæ¨¡æ€æ¨¡å‹æ¥ç†è§£äººç±»æƒ…ç»ªï¼Œè€Œä¸æ˜¯å•æ¨¡æ€æ¨¡å‹ã€‚ç„¶è€Œï¼Œå½“å‰çš„è§†é¢‘å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰åœ¨æœ‰æ•ˆæ•´åˆéŸ³é¢‘å’Œè¯†åˆ«å¾®å¦™çš„é¢éƒ¨è¡¨æƒ…å¾®åŠ¨ä½œæ–¹é¢é‡åˆ°å›°éš¾ã€‚æ­¤å¤–ï¼Œç¼ºä¹è¯¦ç»†çš„æƒ…æ„Ÿåˆ†ææ•°æ®é›†ä¹Ÿé™åˆ¶äº†å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æçš„å‘å±•ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªè‡ªæˆ‘å®¡æŸ¥çš„æ•°æ®é›†å’Œä¸€ä¸ªäººå·¥å®¡æŸ¥çš„æ•°æ®é›†ï¼Œåˆ†åˆ«åŒ…å«24,137ä¸ªç²—ç²’åº¦æ ·æœ¬å’Œ3,500ä¸ªå¸¦æœ‰è¯¦ç»†æƒ…æ„Ÿæ³¨é‡Šçš„æ‰‹åŠ¨æ³¨é‡Šæ ·æœ¬ã€‚è¿™äº›æ•°æ®é›†ä½¿æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ å„ç§åœºæ™¯ï¼Œå¹¶æ›´å¥½åœ°æ¨å¹¿ç°å®ä¸–ç•Œçš„å®é™…åº”ç”¨ã€‚é™¤äº†éŸ³é¢‘å»ºæ¨¡ä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜æè®®å°†é¢éƒ¨ç¼–ç æ¨¡å‹æ˜¾å¼åœ°é›†æˆåˆ°ç°æœ‰çš„å…ˆè¿›è§†é¢‘MLLMä¸­ï¼Œä½¿MLLMèƒ½å¤Ÿæœ‰æ•ˆåœ°ç»Ÿä¸€éŸ³é¢‘å’Œå¾®å¦™çš„é¢éƒ¨çº¿ç´¢æ¥è¿›è¡Œæƒ…æ„Ÿç†è§£ã€‚é€šè¿‡åœ¨ä¸€ä¸ªç»Ÿä¸€çš„ç©ºé—´å†…å¯¹é½è¿™äº›ç‰¹å¾ï¼Œå¹¶åœ¨æˆ‘ä»¬æå‡ºçš„æ•°æ®é›†ä¸­è¿›è¡ŒæŒ‡ä»¤è°ƒæ•´ï¼Œæˆ‘ä»¬çš„Omni-Emotionåœ¨æƒ…æ„Ÿè¯†åˆ«å’Œæ¨ç†ä»»åŠ¡ä¸Šå‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.09502v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æçš„é‡è¦æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†äººç±»ä¸è®¡ç®—æœºäº¤äº’æ—¶ã€‚å½“å‰è§†é¢‘å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å¤„ç†éŸ³é¢‘å’Œè¯†åˆ«å¾®å¦™çš„é¢éƒ¨è¡¨æƒ…æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæ–‡ç« å¼•å…¥äº†ä¸€ä¸ªè‡ªæˆ‘å®¡æŸ¥çš„æ•°æ®é›†å’Œä¸€ä¸ªç»è¿‡äººå·¥å®¡æŸ¥çš„æ•°æ®é›†ï¼Œå¹¶å»ºè®®å°†é¢éƒ¨ç¼–ç æ¨¡å‹æ˜¾å¼åœ°é›†æˆåˆ°ç°æœ‰çš„é«˜çº§è§†é¢‘å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ä¸­ï¼Œä»¥æé«˜æƒ…æ„Ÿç†è§£çš„å‡†ç¡®æ€§ã€‚é€šè¿‡åœ¨è¿™äº›æ•°æ®é›†ä¸­é‡‡ç”¨æŒ‡ä»¤å¾®è°ƒçš„æ–¹æ³•ï¼ŒOmni-Emotionæ¨¡å‹åœ¨æƒ…æ„Ÿè¯†åˆ«å’Œæ¨ç†ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æå¯¹äºäººç±»ä¸è®¡ç®—æœºäº¤äº’è‡³å…³é‡è¦ã€‚</li>
<li>å½“å‰è§†é¢‘å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å¤„ç†éŸ³é¢‘å’Œè¯†åˆ«å¾®å¦™çš„é¢éƒ¨è¡¨æƒ…æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>å¼•å…¥è‡ªæˆ‘å®¡æŸ¥çš„æ•°æ®é›†å’Œç»è¿‡äººå·¥å®¡æŸ¥çš„æ•°æ®é›†ï¼Œä»¥æä¾›æ›´è¯¦ç»†çš„æƒ…æ„Ÿæ³¨é‡Šã€‚</li>
<li>å»ºè®®å°†é¢éƒ¨ç¼–ç æ¨¡å‹é›†æˆåˆ°ç°æœ‰çš„é«˜çº§è§†é¢‘å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ä¸­ã€‚</li>
<li>é€šè¿‡åœ¨æ•°æ®é›†ä¸­é‡‡ç”¨æŒ‡ä»¤å¾®è°ƒçš„æ–¹æ³•ï¼ŒOmni-Emotionæ¨¡å‹å¯ä»¥æé«˜æƒ…æ„Ÿç†è§£çš„å‡†ç¡®æ€§ã€‚</li>
<li>Omni-Emotionæ¨¡å‹åœ¨æƒ…æ„Ÿè¯†åˆ«å’Œæ¨ç†ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€ä½³æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.09502">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-5f828c668f0ca27558841cc1b0debd93.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-00bc06da24d713f02e787d3edd58313b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aef77c49ab3cdf4b68ccccef952c60e5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6a88dc9b5861a1c697e78979926b7972.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3861a3139b9abb88278a0327c59ee103.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Double-Visual-Defense-Adversarial-Pre-training-and-Instruction-Tuning-for-Improving-Vision-Language-Model-Robustness"><a href="#Double-Visual-Defense-Adversarial-Pre-training-and-Instruction-Tuning-for-Improving-Vision-Language-Model-Robustness" class="headerlink" title="Double Visual Defense: Adversarial Pre-training and Instruction Tuning   for Improving Vision-Language Model Robustness"></a>Double Visual Defense: Adversarial Pre-training and Instruction Tuning   for Improving Vision-Language Model Robustness</h2><p><strong>Authors:Zeyu Wang, Cihang Xie, Brian Bartoldson, Bhavya Kailkhura</strong></p>
<p>This paper investigates the robustness of vision-language models against adversarial visual perturbations and introduces a novel &#96;&#96;double visual defenseâ€ to enhance this robustness. Unlike previous approaches that resort to lightweight adversarial fine-tuning of a pre-trained CLIP model, we perform large-scale adversarial vision-language pre-training from scratch using web-scale data. We then strengthen the defense by incorporating adversarial visual instruction tuning. The resulting models from each stage, $\Delta$CLIP and $\Delta^2$LLaVA, show substantially enhanced zero-shot robustness and set a new state-of-the-art in adversarial defense for vision-language models. For example, the adversarial robustness of $\Delta$CLIP surpasses that of the previous best models on ImageNet-1k by ~20%. %For example, $\Delta$CLIP surpasses the previous best models on ImageNet-1k by ~20% in terms of adversarial robustness. Similarly, compared to prior art, $\Delta^2$LLaVA brings a ~30% robustness improvement to image captioning task and a ~20% robustness improvement to visual question answering task. Furthermore, our models exhibit stronger zero-shot recognition capability, fewer hallucinations, and superior reasoning performance compared to baselines. Our project page is <a target="_blank" rel="noopener" href="https://doublevisualdefense.github.io/">https://doublevisualdefense.github.io/</a>. </p>
<blockquote>
<p>æœ¬æ–‡ç ”ç©¶äº†è§†è§‰è¯­è¨€æ¨¡å‹å¯¹æŠ—å¯¹è§†è§‰å¹²æ‰°çš„é²æ£’æ€§ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§æ–°å‹çš„â€œåŒé‡è§†è§‰é˜²å¾¡â€æ–¹æ³•æ¥æé«˜è¿™ç§é²æ£’æ€§ã€‚ä¸å…¶ä»–æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬æ²¡æœ‰ä½¿ç”¨é¢„è®­ç»ƒçš„CLIPæ¨¡å‹è¿›è¡Œè½»é‡çº§çš„å¯¹æŠ—å¾®è°ƒï¼Œè€Œæ˜¯ä½¿ç”¨å¤§è§„æ¨¡ç½‘ç»œæ•°æ®ä»å¤´å¼€å§‹è¿›è¡Œå¤§è§„æ¨¡çš„å¯¹æŠ—æ€§è§†è§‰è¯­è¨€é¢„è®­ç»ƒã€‚ç„¶åæˆ‘ä»¬é€šè¿‡èå…¥å¯¹æŠ—è§†è§‰æŒ‡ä»¤å¾®è°ƒæ¥åŠ å¼ºé˜²å¾¡ã€‚å„é˜¶æ®µå¾—å‡ºçš„æ¨¡å‹ï¼ŒåŒ…æ‹¬Î”CLIPå’ŒÎ”Â²LLaVAï¼Œè¡¨ç°å‡ºå¤§å¹…å¢å¼ºçš„é›¶æ ·æœ¬é²æ£’æ€§ï¼Œå¹¶åœ¨è§†è§‰è¯­è¨€æ¨¡å‹çš„å¯¹æŠ—é˜²å¾¡æ–¹é¢åˆ›é€ äº†æ–°çš„æŠ€æœ¯è®°å½•ã€‚ä¾‹å¦‚ï¼ŒÎ”CLIPåœ¨ImageNet-1kä¸Šçš„å¯¹æŠ—é²æ£’æ€§è¶…è¿‡äº†ä»¥å‰æœ€ä¼˜ç§€çš„æ¨¡å‹çº¦20%ã€‚ä¸ç°æœ‰æŠ€æœ¯ç›¸æ¯”ï¼ŒÎ”Â²LLaVAä¸ºå›¾åƒæè¿°ä»»åŠ¡å¸¦æ¥äº†çº¦30%çš„é²æ£’æ€§æå‡ï¼Œå¹¶ä¸ºè§†è§‰é—®ç­”ä»»åŠ¡å¸¦æ¥äº†çº¦20%çš„é²æ£’æ€§æå‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ¨¡å‹è¿˜è¡¨ç°å‡ºæ›´å¼ºçš„é›¶æ ·æœ¬è¯†åˆ«èƒ½åŠ›ã€æ›´å°‘çš„å¹»è§‰å’Œä¼˜äºåŸºå‡†çº¿çš„æ¨ç†æ€§èƒ½ã€‚æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢æ˜¯ï¼š[<a target="_blank" rel="noopener" href="https://doublevisualdefense.github.io/]">https://doublevisualdefense.github.io/]</a> ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.09446v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†è§†è§‰è¯­è¨€æ¨¡å‹å¯¹æŠ—è§†è§‰æ‰°åŠ¨æ”»å‡»çš„é²æ£’æ€§ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°çš„â€œåŒé‡è§†è§‰é˜²å¾¡â€æ–¹æ³•æ¥å¢å¼ºæ¨¡å‹çš„é²æ£’æ€§ã€‚ä¸åŒäºä¹‹å‰çš„æ–¹æ³•ï¼Œæˆ‘ä»¬é‡‡ç”¨å¤§è§„æ¨¡çš„å¯¹æŠ—æ€§è§†è§‰è¯­è¨€é¢„è®­ç»ƒï¼Œä»åŸå§‹æ•°æ®å¼€å§‹ä½¿ç”¨ç½‘ç»œè§„æ¨¡æ•°æ®è¿›è¡Œè®­ç»ƒã€‚éšåï¼Œé€šè¿‡å¼•å…¥å¯¹æŠ—æ€§è§†è§‰æŒ‡ä»¤è°ƒæ•´æ¥åŠ å¼ºé˜²å¾¡ã€‚ç”±æ­¤äº§ç”Ÿçš„æ¨¡å‹ï¼Œå¦‚$\Delta$CLIPå’Œ$\Delta^2$LLaVAï¼Œåœ¨é›¶æ ·æœ¬é²æ£’æ€§ä¸Šæœ‰æ˜¾è‘—å¢å¼ºï¼Œå¹¶åœ¨è§†è§‰è¯­è¨€æ¨¡å‹çš„å¯¹æŠ—æ€§é˜²å¾¡æ–¹é¢åˆ›é€ äº†æ–°çš„æœ€ä½³çŠ¶æ€ã€‚ä¾‹å¦‚ï¼Œ$\Delta$CLIPåœ¨ImageNet-1kä¸Šçš„å¯¹æŠ—æ€§é²æ£’æ€§è¶…è¿‡äº†ä»¥å‰çš„æœ€ä½³æ¨¡å‹çº¦20%ã€‚æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢æ˜¯<a target="_blank" rel="noopener" href="https://doublevisualdefense.github.io/%E3%80%82">https://doublevisualdefense.github.io/ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥è®ºæ–‡ç ”ç©¶äº†è§†è§‰è¯­è¨€æ¨¡å‹é¢å¯¹å¯¹æŠ—æ€§è§†è§‰æ‰°åŠ¨çš„é²æ£’æ€§ã€‚</li>
<li>è®ºæ–‡æå‡ºäº†â€œåŒé‡è§†è§‰é˜²å¾¡â€ç­–ç•¥ä»¥å¢å¼ºæ¨¡å‹çš„é²æ£’æ€§ã€‚</li>
<li>ä¸ä¹‹å‰çš„æ–¹æ³•ä¸åŒï¼Œè¯¥ç ”ç©¶é‡‡ç”¨å¤§è§„æ¨¡çš„å¯¹æŠ—æ€§è§†è§‰è¯­è¨€é¢„è®­ç»ƒã€‚</li>
<li>é€šè¿‡å¼•å…¥å¯¹æŠ—æ€§è§†è§‰æŒ‡ä»¤è°ƒæ•´ï¼Œè¿›ä¸€æ­¥åŠ å¼ºäº†æ¨¡å‹çš„é˜²å¾¡èƒ½åŠ›ã€‚</li>
<li>$\Delta$CLIPå’Œ$\Delta^2$LLaVAæ¨¡å‹æ˜¾è‘—æé«˜äº†é›¶æ ·æœ¬é²æ£’æ€§ï¼Œå¹¶åœ¨å¯¹æŠ—æ€§é˜²å¾¡æ–¹é¢è¾¾åˆ°æ–°çš„æœ€ä½³æ°´å¹³ã€‚</li>
<li>$\Delta$CLIPåœ¨ImageNet-1kä¸Šçš„å¯¹æŠ—æ€§é²æ£’æ€§ä¼˜äºä¹‹å‰çš„æœ€ä½³æ¨¡å‹çº¦20%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.09446">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6665ed36043f278859a6a55830b05e6c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-767b185b59c3c4eed70baa9c5a9e8322.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7c166aae33ad0dab36200cbe9f4001ca.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Monte-Carlo-Tree-Search-for-Comprehensive-Exploration-in-LLM-Based-Automatic-Heuristic-Design"><a href="#Monte-Carlo-Tree-Search-for-Comprehensive-Exploration-in-LLM-Based-Automatic-Heuristic-Design" class="headerlink" title="Monte Carlo Tree Search for Comprehensive Exploration in LLM-Based   Automatic Heuristic Design"></a>Monte Carlo Tree Search for Comprehensive Exploration in LLM-Based   Automatic Heuristic Design</h2><p><strong>Authors:Zhi Zheng, Zhuoliang Xie, Zhenkun Wang, Bryan Hooi</strong></p>
<p>Handcrafting heuristics for solving complex planning tasks (e.g., NP-hard combinatorial optimization (CO) problems) is a common practice but requires extensive domain knowledge. Recently, Large Language Model (LLM)-based automatic heuristics design (AHD) methods have shown promise in generating high-quality heuristics without manual intervention. Existing LLM-based AHD methods employ a population to maintain a fixed number of top-performing LLM-generated heuristics and introduce evolutionary computation (EC) to enhance the population iteratively. However, the population-based procedure brings greedy properties, often resulting in convergence to local optima. Instead, to more comprehensively explore the space of heuristics, we propose using Monte Carlo Tree Search (MCTS) for LLM-based heuristic evolution while preserving all LLM-generated heuristics in a tree structure. With a novel thought-alignment process and an exploration-decay technique, the proposed MCTS-AHD method delivers significantly higher-quality heuristics on various complex tasks. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/zz1358m/MCTS-AHD-master">https://github.com/zz1358m/MCTS-AHD-master</a>. </p>
<blockquote>
<p>æ‰‹åŠ¨è®¾è®¡å¯å‘å¼æ–¹æ³•æ¥è§£å†³å¤æ‚çš„è§„åˆ’ä»»åŠ¡ï¼ˆä¾‹å¦‚NPéš¾çš„ç»„åˆä¼˜åŒ–é—®é¢˜ï¼‰æ˜¯ä¸€ç§å¸¸è§çš„åšæ³•ï¼Œä½†éœ€è¦å¹¿æ³›çš„é¢†åŸŸçŸ¥è¯†ã€‚æœ€è¿‘ï¼ŒåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è‡ªåŠ¨å¯å‘å¼è®¾è®¡ï¼ˆAHDï¼‰æ–¹æ³•åœ¨ç”Ÿæˆé«˜è´¨é‡å¯å‘å¼æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ï¼Œæ— éœ€äººå·¥å¹²é¢„ã€‚ç°æœ‰çš„åŸºäºLLMçš„AHDæ–¹æ³•é‡‡ç”¨ç§ç¾¤æ–¹å¼ç»´æŒæ€§èƒ½æœ€ä½³çš„LLMç”Ÿæˆå¯å‘å¼æ–¹æ³•çš„æ•°é‡ï¼Œå¹¶å¼•å…¥è¿›åŒ–è®¡ç®—ï¼ˆECï¼‰æ¥é€æ­¥å¢å¼ºç§ç¾¤ã€‚ç„¶è€Œï¼ŒåŸºäºç§ç¾¤çš„æ–¹æ³•å…·æœ‰è´ªå©ªæ€§ï¼Œå¾€å¾€ä¼šå¯¼è‡´æ”¶æ•›åˆ°å±€éƒ¨æœ€ä¼˜è§£ã€‚ç›¸åï¼Œä¸ºäº†æ›´å…¨é¢åœ°æ¢ç´¢å¯å‘å¼æ–¹æ³•çš„ç©ºé—´ï¼Œæˆ‘ä»¬æå‡ºä½¿ç”¨è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰æ¥è¿›è¡ŒåŸºäºLLMçš„å¯å‘å¼æ–¹æ³•æ¼”åŒ–ï¼ŒåŒæ—¶å°†æ‰€æœ‰ç”±LLMç”Ÿæˆçš„å¯å‘å¼æ–¹æ³•ä¿å­˜åœ¨æ ‘ç»“æ„ä¸­ã€‚é€šè¿‡ä¸€ç§æ–°çš„æ€æƒ³å¯¹é½è¿‡ç¨‹å’Œæ¢ç´¢è¡°å‡æŠ€æœ¯ï¼Œæ‰€æå‡ºçš„MCTS-AHDæ–¹æ³•åœ¨å¤šç§å¤æ‚ä»»åŠ¡ä¸Šæä¾›äº†æ›´é«˜è´¨é‡çš„å¯å‘å¼æ–¹æ³•ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/zz1358m/MCTS-AHD-master%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/zz1358m/MCTS-AHD-masteræ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.08603v2">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è‡ªåŠ¨å¯å‘å¼è®¾è®¡ï¼ˆAHDï¼‰æ˜¯è§£å†³å¤æ‚è§„åˆ’ä»»åŠ¡çš„æœ‰æ•ˆæ–¹æ³•ã€‚è™½ç„¶ä¼ ç»Ÿæ–¹æ³•é‡‡ç”¨ç§ç¾¤æ¥ç»´æŠ¤æœ€ä½³å¯å‘å¼ï¼Œä½†å­˜åœ¨å±€é™æ€§ã€‚æœ¬æ–‡æå‡ºä½¿ç”¨è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰è¿›è¡ŒLLMå¯å‘å¼è¿›åŒ–ï¼Œå¹¶å°†æ‰€æœ‰ç”Ÿæˆçš„å¯å‘å¼ä¿å­˜åœ¨æ ‘ç»“æ„ä¸­ã€‚æ–°æ–¹æ³•æé«˜äº†å¯å‘å¼è´¨é‡ï¼Œé€‚ç”¨äºå¤šç§å¤æ‚ä»»åŠ¡ã€‚ç›¸å…³ä»£ç å·²å…¬å¼€åˆ†äº«ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LLMåœ¨å¯å‘å¼è®¾è®¡ï¼ˆAHDï¼‰é¢†åŸŸå±•ç°å‡ºæ½œåŠ›ï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡å¯å‘å¼è€Œæ— éœ€äººå·¥å¹²é¢„ã€‚</li>
<li>ä¼ ç»ŸLLM-based AHDæ–¹æ³•ä½¿ç”¨ç§ç¾¤ç»´æŠ¤æœ€ä½³å¯å‘å¼ï¼Œä½†å¯èƒ½é™·å…¥å±€éƒ¨æœ€ä¼˜è§£ã€‚</li>
<li>æå‡ºä½¿ç”¨è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰è¿›è¡ŒLLM-basedå¯å‘å¼è¿›åŒ–ï¼Œä»¥æ›´å…¨é¢åœ°æ¢ç´¢å¯å‘å¼ç©ºé—´ã€‚</li>
<li>æ–°æ–¹æ³•é€šè¿‡æ€æƒ³å¯¹é½è¿‡ç¨‹å’Œæ¢ç´¢è¡°å‡æŠ€æœ¯æé«˜äº†å¯å‘å¼è´¨é‡ã€‚</li>
<li>MCTS-AHDæ–¹æ³•åœ¨å¤šç§å¤æ‚ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>ç›¸å…³ç ”ç©¶ä»£ç å·²å…¬å¼€åˆ†äº«ï¼Œä¾¿äºè¿›ä¸€æ­¥ç ”ç©¶å’Œåº”ç”¨ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.08603">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-df4e7f274144f09d5a746f466013951e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3b7bad3523aadde1fcaa44e9ae50ebbc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d958ce1c774f9c8f44f9a14d8e876c42.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5216657749c61699431096c9b46f9d69.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Super-class-guided-Transformer-for-Zero-Shot-Attribute-Classification"><a href="#Super-class-guided-Transformer-for-Zero-Shot-Attribute-Classification" class="headerlink" title="Super-class guided Transformer for Zero-Shot Attribute Classification"></a>Super-class guided Transformer for Zero-Shot Attribute Classification</h2><p><strong>Authors:Sehyung Kim, Chanhyeong Yang, Jihwan Park, Taehoon Song, Hyunwoo J. Kim</strong></p>
<p>Attribute classification is crucial for identifying specific characteristics within image regions. Vision-Language Models (VLMs) have been effective in zero-shot tasks by leveraging their general knowledge from large-scale datasets. Recent studies demonstrate that transformer-based models with class-wise queries can effectively address zero-shot multi-label classification. However, poor utilization of the relationship between seen and unseen attributes makes the model lack generalizability. Additionally, attribute classification generally involves many attributes, making maintaining the modelâ€™s scalability difficult. To address these issues, we propose Super-class guided transFormer (SugaFormer), a novel framework that leverages super-classes to enhance scalability and generalizability for zero-shot attribute classification. SugaFormer employs Super-class Query Initialization (SQI) to reduce the number of queries, utilizing common semantic information from super-classes, and incorporates Multi-context Decoding (MD) to handle diverse visual cues. To strengthen generalizability, we introduce two knowledge transfer strategies that utilize VLMs. During training, Super-class guided Consistency Regularization (SCR) aligns modelâ€™s features with VLMs using super-class guided prompts, and during inference, Zero-shot Retrieval-based Score Enhancement (ZRSE) refines predictions for unseen attributes. Extensive experiments demonstrate that SugaFormer achieves state-of-the-art performance across three widely-used attribute classification benchmarks under zero-shot, and cross-dataset transfer settings. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/mlvlab/SugaFormer">https://github.com/mlvlab/SugaFormer</a>. </p>
<blockquote>
<p>å±æ€§åˆ†ç±»å¯¹äºè¯†åˆ«å›¾åƒåŒºåŸŸå†…çš„ç‰¹å®šç‰¹å¾è‡³å…³é‡è¦ã€‚è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰é€šè¿‡åˆ©ç”¨å¤§è§„æ¨¡æ•°æ®é›†çš„ä¸€èˆ¬çŸ¥è¯†ï¼Œåœ¨é›¶æ ·æœ¬ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼ŒåŸºäºtransformerçš„å…·æœ‰ç±»åˆ«æŸ¥è¯¢çš„æ¨¡å‹å¯ä»¥æœ‰æ•ˆåœ°è§£å†³é›¶æ ·æœ¬å¤šæ ‡ç­¾åˆ†ç±»é—®é¢˜ã€‚ç„¶è€Œï¼Œå¯è§å’Œä¸å¯è§å±æ€§ä¹‹é—´å…³ç³»åˆ©ç”¨ä¸è¶³ä½¿å¾—æ¨¡å‹ç¼ºä¹æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œå±æ€§åˆ†ç±»é€šå¸¸æ¶‰åŠè®¸å¤šå±æ€§ï¼Œä½¿å¾—ä¿æŒæ¨¡å‹çš„å¯æ‰©å±•æ€§å˜å¾—å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Super-class guided transFormerï¼ˆSugaFormerï¼‰è¿™ä¸€æ–°é¢–æ¡†æ¶ï¼Œåˆ©ç”¨è¶…ç±»å¢å¼ºé›¶æ ·æœ¬å±æ€§åˆ†ç±»çš„å¯æ‰©å±•æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚SugaFormeré‡‡ç”¨Super-class Query Initializationï¼ˆSQIï¼‰å‡å°‘æŸ¥è¯¢æ•°é‡ï¼Œåˆ©ç”¨è¶…ç±»çš„é€šç”¨è¯­ä¹‰ä¿¡æ¯ï¼Œå¹¶ç»“åˆMulti-context Decodingï¼ˆMDï¼‰å¤„ç†å¤šæ ·çš„è§†è§‰çº¿ç´¢ã€‚ä¸ºäº†åŠ å¼ºæ³›åŒ–èƒ½åŠ›ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸¤ç§åˆ©ç”¨VLMsçš„çŸ¥è¯†è½¬ç§»ç­–ç•¥ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼ŒSuper-class guided Consistency Regularizationï¼ˆSCRï¼‰é€šè¿‡è¶…ç±»å¼•å¯¼æç¤ºå°†æ¨¡å‹ç‰¹å¾ä¸VLMså¯¹é½ï¼Œè€Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼ŒZero-shot Retrieval-based Score Enhancementï¼ˆZRSEï¼‰å¯¹æœªè§è¿‡çš„å±æ€§é¢„æµ‹è¿›è¡Œç»†åŒ–ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSugaFormeråœ¨ä¸‰ä¸ªå¹¿æ³›ä½¿ç”¨çš„å±æ€§åˆ†ç±»åŸºå‡†æµ‹è¯•ä¸‹å®ç°äº†é›¶æ ·æœ¬å’Œè·¨æ•°æ®é›†è¿ç§»è®¾ç½®çš„æœ€æ–°æ€§èƒ½ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/mlvlab/SugaFormer%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/mlvlab/SugaFormeræ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.05728v2">PDF</a> AAAI25</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å±æ€§åˆ†ç±»åœ¨è¯†åˆ«å›¾åƒåŒºåŸŸç‰¹å®šç‰¹å¾ä¸­çš„é‡è¦æ€§ã€‚å°½ç®¡è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨é›¶æ ·æœ¬ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†ç°æœ‰ç ”ç©¶åœ¨è§£å†³é›¶æ ·æœ¬å¤šæ ‡ç­¾åˆ†ç±»æ—¶ï¼Œæœªèƒ½å……åˆ†åˆ©ç”¨å·²è§å’Œæœªè§å±æ€§ä¹‹é—´çš„å…³ç³»ï¼Œå¯¼è‡´æ¨¡å‹æ³›åŒ–æ€§ä¸è¶³ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºäº†Super-class guided transFormerï¼ˆSugaFormerï¼‰æ¡†æ¶ï¼Œé€šè¿‡åˆ©ç”¨è¶…ç±»å¢å¼ºé›¶æ ·æœ¬å±æ€§åˆ†ç±»çš„æ‰©å±•æ€§å’Œæ³›åŒ–æ€§ã€‚è¯¥æ¡†æ¶é‡‡ç”¨è¶…çº§ç±»æŸ¥è¯¢åˆå§‹åŒ–ï¼ˆSQIï¼‰å‡å°‘æŸ¥è¯¢æ•°é‡ï¼Œåˆ©ç”¨è¶…çº§ç±»çš„é€šç”¨è¯­ä¹‰ä¿¡æ¯ï¼Œå¹¶ç»“åˆå¤šä¸Šä¸‹æ–‡è§£ç ï¼ˆMDï¼‰å¤„ç†å¤šæ ·çš„è§†è§‰çº¿ç´¢ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥ä¸¤ç§åˆ©ç”¨VLMsçš„çŸ¥è¯†è½¬ç§»ç­–ç•¥ä»¥å¢å¼ºæ³›åŒ–èƒ½åŠ›ã€‚åœ¨ä¸‰ä¸ªå¹¿æ³›ä½¿ç”¨çš„å±æ€§åˆ†ç±»åŸºå‡†æµ‹è¯•ä¸‹ï¼ŒSugaFormeråœ¨é›¶æ ·æœ¬å’Œè·¨æ•°æ®é›†è½¬ç§»è®¾ç½®ä¸­å®ç°äº†æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å±æ€§åˆ†ç±»æ˜¯è¯†åˆ«å›¾åƒåŒºåŸŸç‰¹å®šç‰¹å¾çš„å…³é”®ã€‚</li>
<li>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨é›¶æ ·æœ¬ä»»åŠ¡ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å¤šæ ‡ç­¾åˆ†ç±»ä¸­æ³›åŒ–æ€§ä¸è¶³ã€‚</li>
<li>SugaFormeræ¡†æ¶é€šè¿‡åˆ©ç”¨è¶…ç±»å¢å¼ºé›¶æ ·æœ¬å±æ€§åˆ†ç±»çš„æ³›åŒ–æ€§å’Œæ‰©å±•æ€§ã€‚</li>
<li>SugaFormeré‡‡ç”¨è¶…çº§ç±»æŸ¥è¯¢åˆå§‹åŒ–å’Œå¤šä¸Šä¸‹æ–‡è§£ç å¤„ç†è§†è§‰ä¿¡æ¯ã€‚</li>
<li>SugaFormerå¼•å…¥ä¸¤ç§çŸ¥è¯†è½¬ç§»ç­–ç•¥ï¼Œåˆ©ç”¨VLMså¢å¼ºæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>SugaFormeråœ¨å¤šä¸ªå±æ€§åˆ†ç±»åŸºå‡†æµ‹è¯•ä¸­å®ç°æœ€ä½³æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.05728">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c23148e0133900a7bf7d14b6132c838c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ce1ec599dd84fdfc626d4a840a458d31.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8a253ebcf799571b261cbe3fa9594210.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e74b5349d2ab95debfe9e4d33c8e888b.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="VITA-1-5-Towards-GPT-4o-Level-Real-Time-Vision-and-Speech-Interaction"><a href="#VITA-1-5-Towards-GPT-4o-Level-Real-Time-Vision-and-Speech-Interaction" class="headerlink" title="VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction"></a>VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction</h2><p><strong>Authors:Chaoyou Fu, Haojia Lin, Xiong Wang, Yi-Fan Zhang, Yunhang Shen, Xiaoyu Liu, Yangze Li, Zuwei Long, Heting Gao, Ke Li, Long Ma, Xiawu Zheng, Rongrong Ji, Xing Sun, Caifeng Shan, Ran He</strong></p>
<p>Recent Multimodal Large Language Models (MLLMs) have typically focused on integrating visual and textual modalities, with less emphasis placed on the role of speech in enhancing interaction. However, speech plays a crucial role in multimodal dialogue systems, and implementing high-performance in both vision and speech tasks remains a significant challenge due to the fundamental modality differences. In this paper, we propose a carefully designed multi-stage training methodology that progressively trains LLM to understand both visual and speech information, ultimately enabling fluent vision and speech interaction. Our approach not only preserves strong vision-language capacity, but also enables efficient speech-to-speech dialogue capabilities without separate ASR and TTS modules, significantly accelerating multimodal end-to-end response speed. By comparing our method against state-of-the-art counterparts across benchmarks for image, video, and speech tasks, we demonstrate that our model is equipped with both strong visual and speech capabilities, making near real-time vision and speech interaction. </p>
<blockquote>
<p>æœ€è¿‘çš„è·¨æ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ä¸»è¦é›†ä¸­äºæ•´åˆè§†è§‰å’Œæ–‡æœ¬æ¨¡æ€ï¼Œå¯¹è¯­éŸ³åœ¨å¢å¼ºäº¤äº’ä¸­çš„ä½œç”¨é‡è§†ä¸å¤Ÿã€‚ç„¶è€Œï¼Œè¯­éŸ³åœ¨å¤šæ¨¡æ€å¯¹è¯ç³»ç»Ÿä¸­èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ï¼Œç”±äºåŸºæœ¬æ¨¡æ€å·®å¼‚ï¼Œåœ¨è§†è§‰å’Œè¯­éŸ³ä»»åŠ¡ä¸­å®ç°é«˜æ€§èƒ½ä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç²¾å¿ƒè®¾è®¡çš„å¤šé˜¶æ®µè®­ç»ƒæ–¹æ³•è®ºï¼Œé€æ­¥è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ä»¥ç†è§£è§†è§‰å’Œè¯­éŸ³ä¿¡æ¯ï¼Œæœ€ç»ˆå®ç°æµç•…çš„è§†è§‰å’Œè¯­éŸ³äº¤äº’ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…ä¿ç•™äº†å¼ºå¤§çš„è§†è§‰è¯­è¨€åŠŸèƒ½ï¼Œè¿˜å®ç°äº†é«˜æ•ˆçš„è¯­éŸ³å¯¹è¯åŠŸèƒ½ï¼Œæ— éœ€ä½¿ç”¨å•ç‹¬çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰å’Œæ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å—ï¼Œä»è€Œæ˜¾è‘—åŠ å¿«äº†å¤šæ¨¡æ€ç«¯åˆ°ç«¯çš„å“åº”é€Ÿåº¦ã€‚é€šè¿‡ä¸å›¾åƒã€è§†é¢‘å’Œè¯­éŸ³ä»»åŠ¡çš„æœ€æ–°åŸºå‡†è¿›è¡Œæ¯”è¾ƒï¼Œæˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬çš„æ¨¡å‹æ‹¥æœ‰å¼ºå¤§çš„è§†è§‰å’Œè¯­éŸ³åŠŸèƒ½ï¼Œèƒ½å¤Ÿå®ç°æ¥è¿‘å®æ—¶çš„è§†è§‰å’Œè¯­éŸ³äº¤äº’ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.01957v2">PDF</a> <a target="_blank" rel="noopener" href="https://github.com/VITA-MLLM/VITA">https://github.com/VITA-MLLM/VITA</a></p>
<p><strong>Summary</strong>ï¼š<br>è¿‘æœŸå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸»è¦èšç„¦äºæ•´åˆè§†è§‰å’Œæ–‡æœ¬æ¨¡æ€ï¼Œè¾ƒå°‘å…³æ³¨è¯­éŸ³åœ¨å¢å¼ºäº¤äº’ä¸­çš„ä½œç”¨ã€‚ç„¶è€Œï¼Œè¯­éŸ³åœ¨å¤šæ¨¡æ€å¯¹è¯ç³»ç»Ÿä¸­æ‰®æ¼”å…³é”®è§’è‰²ï¼Œå®ç°åœ¨è§†è§‰å’Œè¯­éŸ³ä»»åŠ¡ä¸­çš„é«˜æ€§èƒ½ä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ï¼Œå› ä¸ºå­˜åœ¨åŸºæœ¬çš„æ¨¡æ€å·®å¼‚ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç²¾å¿ƒè®¾è®¡çš„å¤šé˜¶æ®µè®­ç»ƒæ–¹æ³•è®ºï¼Œé€æ­¥è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ä»¥ç†è§£è§†è§‰å’Œè¯­éŸ³ä¿¡æ¯ï¼Œæœ€ç»ˆå®ç°æµç•…çš„è§†è§‰å’Œè¯­éŸ³äº¤äº’ã€‚è¯¥æ–¹æ³•ä¸ä»…ä¿ç•™äº†å¼ºå¤§çš„è§†è§‰è¯­è¨€åŠŸèƒ½ï¼Œè¿˜èƒ½å®ç°é«˜æ•ˆçš„è¯­éŸ³å¯¹è¯èƒ½åŠ›ï¼Œæ— éœ€é¢å¤–çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«å’Œæ–‡æœ¬è½¬è¯­éŸ³æ¨¡å—ï¼Œæ˜¾è‘—åŠ å¿«å¤šæ¨¡æ€ç«¯åˆ°ç«¯çš„å“åº”é€Ÿåº¦ã€‚é€šè¿‡ä¸å›¾åƒã€è§†é¢‘å’Œè¯­éŸ³ä»»åŠ¡çš„æœ€æ–°å‰æ²¿æ¨¡å‹è¿›è¡Œå¯¹æ¯”ï¼Œæœ¬æ–‡è¯æ˜äº†è¯¥æ¨¡å‹å…·å¤‡å¼ºå¤§çš„è§†è§‰å’Œè¯­éŸ³åŠŸèƒ½ï¼Œå¯å®ç°è¿‘ä¹å®æ—¶çš„è§†è§‰å’Œè¯­éŸ³äº¤äº’ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>MLLMs åœ¨æ•´åˆè§†è§‰å’Œæ–‡æœ¬æ¨¡æ€æ—¶è¾ƒå°‘å…³æ³¨è¯­éŸ³çš„é‡è¦æ€§ã€‚</li>
<li>å®ç°è§†è§‰å’Œè¯­éŸ³ä»»åŠ¡çš„é«˜æ€§èƒ½æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ï¼Œå› ä¸ºå­˜åœ¨åŸºæœ¬çš„æ¨¡æ€å·®å¼‚ã€‚</li>
<li>æå‡ºäº†ä¸€ç§å¤šé˜¶æ®µè®­ç»ƒæ–¹æ³•è®ºæ¥è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä»¥ç†è§£è§†è§‰å’Œè¯­éŸ³ä¿¡æ¯ã€‚</li>
<li>è¯¥æ–¹æ³•å®ç°äº†æµç•…çš„è§†è§‰å’Œè¯­éŸ³äº¤äº’ã€‚</li>
<li>æ¨¡å‹ä¸ä»…å…·å¤‡å¼ºå¤§çš„è§†è§‰è¯­è¨€åŠŸèƒ½ï¼Œè¿˜èƒ½å®ç°é«˜æ•ˆçš„è¯­éŸ³å¯¹è¯èƒ½åŠ›ã€‚</li>
<li>æ¨¡å‹æ— éœ€é¢å¤–çš„ASRå’ŒTTSæ¨¡å—å³å¯å®ç°è¯­éŸ³äº¤äº’ï¼ŒåŠ å¿«å“åº”é€Ÿåº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.01957">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-58744d62eb0fbf66540b32115e33c365.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-458784e265fee8832f68789bf9d105f9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e262945a140a0fbcdb7ddae1a7741766.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-396c9589614e6d25affa510578046020.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-200583adacfa33383773d2e9f3835530.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="SPRec-Leveraging-Self-Play-to-Debias-Preference-Alignment-for-Large-Language-Model-based-Recommendations"><a href="#SPRec-Leveraging-Self-Play-to-Debias-Preference-Alignment-for-Large-Language-Model-based-Recommendations" class="headerlink" title="SPRec: Leveraging Self-Play to Debias Preference Alignment for Large   Language Model-based Recommendations"></a>SPRec: Leveraging Self-Play to Debias Preference Alignment for Large   Language Model-based Recommendations</h2><p><strong>Authors:Chongming Gao, Ruijun Chen, Shuai Yuan, Kexin Huang, Yuanqing Yu, Xiangnan He</strong></p>
<p>Large language models (LLMs) have attracted significant attention in recommendation systems. Current LLM-based recommender systems primarily rely on supervised fine-tuning (SFT) to train the model for recommendation tasks. However, relying solely on positive samples limits the modelâ€™s ability to align with user satisfaction and expectations. To address this, researchers have introduced Direct Preference Optimization (DPO), which explicitly aligns recommendations with user preferences using offline preference ranking data. Despite its advantages, our theoretical analysis reveals that DPO inherently biases the model towards a few items, exacerbating the filter bubble issue and ultimately degrading user experience. In this paper, we propose SPRec, a novel self-play recommendation framework designed to mitigate over-recommendation and improve fairness without requiring additional data or manual intervention. In each self-play iteration, the model undergoes an SFT step followed by a DPO step, treating offline interaction data as positive samples and the predicted outputs from the previous iteration as negative samples. This effectively re-weights the DPO loss function using the modelâ€™s logits, adaptively suppressing biased items. Extensive experiments on multiple real-world datasets demonstrate SPRecâ€™s effectiveness in enhancing recommendation accuracy and addressing fairness concerns. The implementation is available via <a target="_blank" rel="noopener" href="https://github.com/RegionCh/SPRec">https://github.com/RegionCh/SPRec</a> </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ¨èç³»ç»Ÿä¸­å¼•èµ·äº†å¹¿æ³›å…³æ³¨ã€‚å½“å‰çš„åŸºäºLLMçš„æ¨èç³»ç»Ÿä¸»è¦ä¾èµ–äºç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ¥è®­ç»ƒæ¨èä»»åŠ¡æ¨¡å‹ã€‚ç„¶è€Œï¼Œä»…ä¾èµ–æ­£é¢æ ·æœ¬é™åˆ¶äº†æ¨¡å‹ä¸ç”¨æˆ·æ»¡æ„åº¦å’ŒæœŸæœ›å¯¹é½çš„èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œç ”ç©¶äººå‘˜å¼•å…¥äº†ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ï¼Œå®ƒåˆ©ç”¨ç¦»çº¿åå¥½æ’åæ•°æ®æ˜¾å¼åœ°å°†æ¨èä¸ç”¨æˆ·åå¥½å¯¹é½ã€‚å°½ç®¡DPOæœ‰å…¶ä¼˜åŠ¿ï¼Œä½†æˆ‘ä»¬çš„ç†è®ºåˆ†æè¡¨æ˜ï¼Œå®ƒå¤©ç”Ÿä¼šä½¿æ¨¡å‹åå‘äºå°‘æ•°ç‰©å“ï¼ŒåŠ å‰§äº†è¿‡æ»¤æ³¡æ²«é—®é¢˜ï¼Œæœ€ç»ˆé™ä½äº†ç”¨æˆ·ä½“éªŒã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†SPRecï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„è‡ªç©æ¨èæ¡†æ¶ï¼Œæ—¨åœ¨ç¼“è§£è¿‡åº¦æ¨èé—®é¢˜ï¼Œæé«˜å…¬å¹³æ€§ï¼Œè€Œæ— éœ€é¢å¤–æ•°æ®æˆ–äººå·¥å¹²é¢„ã€‚åœ¨æ¯æ¬¡è‡ªç©è¿­ä»£ä¸­ï¼Œæ¨¡å‹ä¼šç»å†ä¸€ä¸ªSFTæ­¥éª¤ï¼Œç„¶åæ˜¯ä¸€ä¸ªDPOæ­¥éª¤ï¼Œå°†ç¦»çº¿äº¤äº’æ•°æ®è§†ä¸ºæ­£é¢æ ·æœ¬ï¼Œè€Œä¸Šä¸€è½®çš„é¢„æµ‹è¾“å‡ºè§†ä¸ºè´Ÿé¢æ ·æœ¬ã€‚è¿™æœ‰æ•ˆåœ°ä½¿ç”¨æ¨¡å‹çš„é€»è¾‘é‡æ–°åŠ æƒDPOæŸå¤±å‡½æ•°ï¼Œè‡ªé€‚åº”åœ°æŠ‘åˆ¶åè§ç‰©å“ã€‚åœ¨å¤šä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¯æ˜äº†SPRecåœ¨æé«˜æ¨èå‡†ç¡®æ€§å’Œè§£å†³å…¬å¹³æ€§é—®é¢˜æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚å®ç°å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/RegionCh/SPRec%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/RegionCh/SPRecè·å¾—ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.09243v2">PDF</a> </p>
<p><strong>Summary</strong>ï¼šLLMåœ¨æ¨èç³»ç»Ÿä¸­çš„åº”ç”¨å¼•èµ·å¹¿æ³›å…³æ³¨ã€‚ç°æœ‰LLMæ¨èç³»ç»Ÿä¸»è¦ä¾èµ–ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰è¿›è¡Œè®­ç»ƒï¼Œä½†ä»…ä¾èµ–æ­£æ ·æœ¬é™åˆ¶äº†æ¨¡å‹ä¸ç”¨æˆ·æ»¡æ„åº¦å’ŒæœŸæœ›çš„å¥‘åˆåº¦ã€‚ç ”ç©¶è€…å¼•å…¥ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ä»¥åˆ©ç”¨ç¦»çº¿åå¥½æ’åæ•°æ®æ˜¾å¼åŒ¹é…ç”¨æˆ·åå¥½ã€‚ç„¶è€Œï¼Œç†è®ºåˆ†æè¡¨æ˜DPOä¼šå¯¼è‡´æ¨¡å‹åå‘å°‘æ•°ç‰©å“ï¼ŒåŠ å‰§æ»¤é•œæ³¡æ²«é—®é¢˜å¹¶é™ä½ç”¨æˆ·ä½“éªŒã€‚æœ¬æ–‡æå‡ºSPRecï¼Œä¸€ç§æ–°å‹è‡ªæˆ‘åšå¼ˆæ¨èæ¡†æ¶ï¼Œæ—¨åœ¨ç¼“è§£è¿‡åº¦æ¨èå¹¶æ”¹å–„å…¬å¹³æ€§ï¼Œæ— éœ€é¢å¤–æ•°æ®æˆ–äººå·¥å¹²é¢„ã€‚SPRecé€šè¿‡è¿­ä»£ä½¿ç”¨ç¦»çº¿äº¤äº’æ•°æ®ä½œä¸ºæ­£æ ·æœ¬ï¼Œä»¥åŠå‰ä¸€æ¬¡è¿­ä»£çš„é¢„æµ‹è¾“å‡ºä½œä¸ºè´Ÿæ ·æœ¬ï¼Œè‡ªé€‚åº”è°ƒæ•´DPOæŸå¤±å‡½æ•°æƒé‡ã€‚å®éªŒè¯æ˜SPRecåœ¨æé«˜æ¨èå‡†ç¡®æ€§å’Œè§£å†³å…¬å¹³æ€§é—®é¢˜æ–¹é¢æœ‰æ•ˆã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>LLMåœ¨æ¨èç³»ç»Ÿä¸­çš„åº”ç”¨å—åˆ°å…³æ³¨ï¼Œä½†ä¾èµ–ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰èƒ½å¤Ÿåˆ©ç”¨ç”¨æˆ·åå¥½æ•°æ®æé«˜æ¨èæ•ˆæœã€‚</li>
<li>DPOå¯èƒ½å¯¼è‡´æ¨¡å‹åå‘å°‘æ•°ç‰©å“ï¼ŒåŠ å‰§æ»¤é•œæ³¡æ²«é—®é¢˜ã€‚</li>
<li>SPRecæ¡†æ¶æ—¨åœ¨é€šè¿‡è‡ªæˆ‘åšå¼ˆæ–¹å¼ç¼“è§£è¿‡åº¦æ¨èé—®é¢˜å¹¶æ”¹å–„å…¬å¹³æ€§ã€‚</li>
<li>SPRecåˆ©ç”¨ç¦»çº¿äº¤äº’æ•°æ®è¿­ä»£æ›´æ–°æ¨¡å‹ï¼Œä½¿ç”¨å‰ä¸€æ¬¡è¿­ä»£çš„é¢„æµ‹è¾“å‡ºä½œä¸ºè´Ÿæ ·æœ¬ã€‚</li>
<li>SPRecé€šè¿‡è‡ªé€‚åº”è°ƒæ•´DPOæŸå¤±å‡½æ•°æƒé‡ï¼Œæé«˜æ¨èå‡†ç¡®æ€§å¹¶è§£å†³å…¬å¹³æ€§é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.09243">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0c23541395fc6e121e54ad49de88e83c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-86508638f5e3135e666ca5fdf6a4e600.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-062d936c9217d1cfd2f47fe65f7ed4cf.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Can-ChatGPT-Overcome-Behavioral-Biases-in-the-Financial-Sector-Classify-and-Rethink-Multi-Step-Zero-Shot-Reasoning-in-the-Gold-Investment"><a href="#Can-ChatGPT-Overcome-Behavioral-Biases-in-the-Financial-Sector-Classify-and-Rethink-Multi-Step-Zero-Shot-Reasoning-in-the-Gold-Investment" class="headerlink" title="Can ChatGPT Overcome Behavioral Biases in the Financial Sector?   Classify-and-Rethink: Multi-Step Zero-Shot Reasoning in the Gold Investment"></a>Can ChatGPT Overcome Behavioral Biases in the Financial Sector?   Classify-and-Rethink: Multi-Step Zero-Shot Reasoning in the Gold Investment</h2><p><strong>Authors:Shuoling Liu, Gaoguo Jia, Yuhang Jiang, Liyuan Chen, Qiang Yang</strong></p>
<p>Large Language Models (LLMs) have achieved remarkable success recently, displaying exceptional capabilities in creating understandable and organized text. These LLMs have been utilized in diverse fields, such as clinical research, where domain-specific models like Med-Palm have achieved human-level performance. Recently, researchers have employed advanced prompt engineering to enhance the general reasoning ability of LLMs. Despite the remarkable success of zero-shot Chain-of-Thoughts (CoT) in solving general reasoning tasks, the potential of these methods still remains paid limited attention in the financial reasoning task.To address this issue, we explore multiple prompt strategies and incorporated semantic news information to improve LLMsâ€™ performance on financial reasoning tasks.To the best of our knowledge, we are the first to explore this important issue by applying ChatGPT to the gold investment.In this work, our aim is to investigate the financial reasoning capabilities of LLMs and their capacity to generate logical and persuasive investment opinions. We will use ChatGPT, one of the most powerful LLMs recently, and prompt engineering to achieve this goal. Our research will focus on understanding the ability of LLMs in sophisticated analysis and reasoning within the context of investment decision-making. Our study finds that ChatGPT with CoT prompt can provide more explainable predictions and overcome behavioral biases, which is crucial in finance-related tasks and can achieve higher investment returns. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æœ€è¿‘å–å¾—äº†æ˜¾è‘—çš„æˆå°±ï¼Œå±•ç°å‡ºåœ¨åˆ›å»ºå¯ç†è§£å’Œæœ‰ç»„ç»‡çš„æ–‡æœ¬æ–¹é¢çš„å“è¶Šèƒ½åŠ›ã€‚è¿™äº›LLMå·²è¢«åº”ç”¨äºå¤šä¸ªé¢†åŸŸï¼Œå¦‚ä¸´åºŠç ”ç©¶ï¼Œå…¶ä¸­ç‰¹å®šé¢†åŸŸçš„æ¨¡å‹ï¼ˆå¦‚Med-Palmï¼‰å·²è¾¾åˆ°äº†äººç±»æ°´å¹³çš„æ€§èƒ½ã€‚æœ€è¿‘ï¼Œç ”ç©¶äººå‘˜é‡‡ç”¨å…ˆè¿›çš„æç¤ºå·¥ç¨‹æ¥å¢å¼ºLLMçš„ä¸€èˆ¬æ¨ç†èƒ½åŠ›ã€‚å°½ç®¡é›¶é•œå¤´é“¾æ€ç»´ï¼ˆCoTï¼‰åœ¨è§£å†³ä¸€èˆ¬æ¨ç†ä»»åŠ¡æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†è¿™äº›æ–¹æ³•åœ¨è´¢åŠ¡æ¨ç†ä»»åŠ¡ä¸­çš„æ½œåŠ›ä»ç„¶å—åˆ°çš„å…³æ³¨æœ‰é™ã€‚</p>
</blockquote>
<p>ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ¢ç´¢äº†å¤šç§æç¤ºç­–ç•¥å¹¶èå…¥äº†è¯­ä¹‰æ–°é—»ä¿¡æ¯ï¼Œä»¥æé«˜LLMåœ¨è´¢åŠ¡æ¨ç†ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œæˆ‘ä»¬æ˜¯ç¬¬ä¸€ä¸ªå°†ChatGPTåº”ç”¨äºé»„é‡‘æŠ•èµ„æ¥æ¢ç´¢è¿™ä¸€é‡è¦é—®é¢˜ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.13599v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨åˆ›å»ºå¯ç†è§£å’Œç»„ç»‡åŒ–çš„æ–‡æœ¬æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œå¹¶åœ¨ä¸´åºŠç ”åŸŸç­‰é¢†åŸŸå¾—åˆ°åº”ç”¨ã€‚æœ€è¿‘ï¼Œç ”ç©¶äººå‘˜é€šè¿‡å…ˆè¿›çš„æç¤ºå·¥ç¨‹æŠ€æœ¯å¢å¼ºäº†LLMçš„ä¸€èˆ¬æ¨ç†èƒ½åŠ›ã€‚å°½ç®¡é›¶æ ·æœ¬é“¾æ€ç»´ï¼ˆCoTï¼‰åœ¨è§£å†³ä¸€èˆ¬æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å…¶åœ¨é‡‘èæ¨ç†ä»»åŠ¡ä¸­çš„æ½œåŠ›ä»æœªå¾—åˆ°å……åˆ†å…³æ³¨ã€‚æœ¬ç ”ç©¶æ—¨åœ¨æ¢ç´¢å°†ChatGPTç­‰LLMåº”ç”¨äºé‡‘èæ¨ç†çš„èƒ½åŠ›ï¼Œå¹¶å…³æ³¨å…¶åœ¨æŠ•èµ„å†³ç­–ä¸­çš„å¤æ‚åˆ†æå’Œæ¨ç†èƒ½åŠ›ã€‚ç ”ç©¶å‘ç°ï¼Œå¸¦æœ‰CoTæç¤ºçš„ChatGPTå¯ä»¥æä¾›æ›´å¯è§£é‡Šçš„é¢„æµ‹ï¼Œå¹¶å…‹æœè¡Œä¸ºåè§ï¼Œå®ç°æ›´é«˜çš„æŠ•èµ„å›æŠ¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMså·²åœ¨å¤šä¸ªé¢†åŸŸå–å¾—æ˜¾è‘—æˆåŠŸï¼ŒåŒ…æ‹¬ä¸´åºŠç ”ç©¶å’Œé‡‘èé¢†åŸŸã€‚</li>
<li>é€šè¿‡å…ˆè¿›çš„æç¤ºå·¥ç¨‹æŠ€æœ¯ï¼ŒLLMçš„æ¨ç†èƒ½åŠ›å¾—åˆ°äº†å¢å¼ºã€‚</li>
<li>é›¶æ ·æœ¬é“¾æ€ç»´ï¼ˆCoTï¼‰åœ¨ä¸€èˆ¬æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ï¼Œä½†åœ¨é‡‘èæ¨ç†ä»»åŠ¡ä¸­çš„æ½œåŠ›å°šæœªå¾—åˆ°å……åˆ†ç ”ç©¶ã€‚</li>
<li>æœ¬ç ”ç©¶é¦–æ¬¡å°†ChatGPTåº”ç”¨äºé‡‘èæŠ•èµ„é¢†åŸŸï¼Œæ¢ç´¢å…¶åœ¨é‡‘èæ¨ç†æ–¹é¢çš„èƒ½åŠ›ã€‚</li>
<li>ä½¿ç”¨ChatGPTå’ŒCoTæç¤ºå¯ä»¥æä¾›æ›´å¯è§£é‡Šçš„é¢„æµ‹ï¼Œè¿™åœ¨é‡‘èä»»åŠ¡ä¸­è‡³å…³é‡è¦ã€‚</li>
<li>LLMsèƒ½å¤Ÿå…‹æœäººç±»è¡Œä¸ºåè§ï¼Œå®ç°æ›´é«˜çš„æŠ•èµ„å›æŠ¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.13599">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-a5635b174330008441c9ce6d11c8356b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ffa358b2985c6bf76eaba2f479473c0a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7ad0959ed2f0923617aaa302d06acc7d.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Crafting-Customisable-Characters-with-LLMs-Introducing-SimsChat-a-Persona-Driven-Role-Playing-Agent-Framework"><a href="#Crafting-Customisable-Characters-with-LLMs-Introducing-SimsChat-a-Persona-Driven-Role-Playing-Agent-Framework" class="headerlink" title="Crafting Customisable Characters with LLMs: Introducing SimsChat, a   Persona-Driven Role-Playing Agent Framework"></a>Crafting Customisable Characters with LLMs: Introducing SimsChat, a   Persona-Driven Role-Playing Agent Framework</h2><p><strong>Authors:Bohao Yang, Dong Liu, Chenghao Xiao, Kun Zhao, Chen Tang, Chao Li, Lin Yuan, Guang Yang, Lanxiao Huang, Chenghua Lin</strong></p>
<p>Large Language Models (LLMs) demonstrate remarkable ability to comprehend instructions and generate human-like text, enabling sophisticated agent simulation beyond basic behavior replication. However, the potential for creating freely customisable characters remains underexplored. We introduce the Customisable Conversation Agent Framework, which employs LLMs to simulate real-world characters through personalised characteristic feature injection, enabling diverse character creation according to user preferences. We propose the SimsConv dataset, comprising 68 customised characters and 13,971 multi-turn role-playing dialogues across 1,360 real-world scenes. Characters are initially customised using pre-defined elements (career, aspiration, traits, skills), then expanded through personal and social profiles. Building on this, we present SimsChat, a freely customisable role-playing agent incorporating various realistic settings and topic-specified character interactions. Experimental results on both SimsConv and WikiRoleEval datasets demonstrate SimsChatâ€™s superior performance in maintaining character consistency, knowledge accuracy, and appropriate question rejection compared to existing models. Our framework provides valuable insights for developing more accurate and customisable human simulacra. Our data and code are publicly available at <a target="_blank" rel="noopener" href="https://github.com/Bernard-Yang/SimsChat">https://github.com/Bernard-Yang/SimsChat</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¡¨ç°å‡ºä»¤äººç©ç›®çš„ç†è§£å’Œæ‰§è¡ŒæŒ‡ä»¤çš„èƒ½åŠ›ï¼Œä»¥åŠç”Ÿæˆç±»ä¼¼äººç±»çš„æ–‡æœ¬ï¼Œèƒ½å¤Ÿå®ç°è¶…è¶ŠåŸºæœ¬è¡Œä¸ºå¤åˆ¶çš„å¤æ‚ä»£ç†æ¨¡æ‹Ÿã€‚ç„¶è€Œï¼Œåˆ›å»ºå¯è‡ªç”±å®šåˆ¶è§’è‰²çš„æ½œåŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æˆ‘ä»¬å¼•å…¥äº†å¯å®šåˆ¶å¯¹è¯ä»£ç†æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨LLMé€šè¿‡ä¸ªæ€§åŒ–ç‰¹å¾æ³¨å…¥æ¨¡æ‹Ÿç°å®ä¸–ç•Œè§’è‰²ï¼Œå¹¶æ ¹æ®ç”¨æˆ·åå¥½å®ç°å¤šæ ·åŒ–è§’è‰²åˆ›å»ºã€‚æˆ‘ä»¬æå‡ºäº†SimsConvæ•°æ®é›†ï¼ŒåŒ…å«68ä¸ªè‡ªå®šä¹‰è§’è‰²å’Œ13971ä¸ªè·¨1360ä¸ªç°å®åœºæ™¯çš„å¤šè½®è§’è‰²æ‰®æ¼”å¯¹è¯ã€‚è§’è‰²æœ€åˆä½¿ç”¨é¢„å®šä¹‰å…ƒç´ ï¼ˆèŒä¸šã€å¿—å‘ã€ç‰¹è´¨ã€æŠ€èƒ½ï¼‰è¿›è¡Œå®šåˆ¶ï¼Œç„¶åé€šè¿‡ä¸ªäººå’Œç¤¾ä¼šæ¦‚å†µè¿›è¡Œæ‰©å±•ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æ¨å‡ºäº†SimsChatï¼Œä¸€ä¸ªå¯è‡ªç”±å®šåˆ¶çš„è§’è‰²æ‰®æ¼”ä»£ç†ï¼ŒåŒ…å«å„ç§ç°å®è®¾ç½®å’Œä¸»é¢˜ç‰¹å®šçš„è§’è‰²äº¤äº’ã€‚åœ¨SimsConvå’ŒWikiRoleEvalæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¯æ˜äº†SimsChatåœ¨ä¿æŒè§’è‰²ä¸€è‡´æ€§ã€çŸ¥è¯†å‡†ç¡®æ€§å’Œé€‚å½“é—®é¢˜æ‹’ç»æ–¹é¢çš„æ€§èƒ½ä¼˜äºç°æœ‰æ¨¡å‹ã€‚æˆ‘ä»¬çš„æ¡†æ¶ä¸ºå¼€å‘æ›´å‡†ç¡®ã€å¯å®šåˆ¶çš„äººç±»æ¨¡æ‹Ÿç‰©æä¾›äº†å®è´µçš„è§è§£ã€‚æˆ‘ä»¬çš„æ•°æ®å’Œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Bernard-Yang/SimsChat%E5%85%AC%E5%BC%BA%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/Bernard-Yang/SimsChatå…¬å¼€è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.17962v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å…·å¤‡ç†è§£æŒ‡ä»¤å’Œç”Ÿæˆç±»ä¼¼äººç±»æ–‡æœ¬çš„èƒ½åŠ›ï¼Œå¯ä»¥å®ç°è¶…è¶ŠåŸºæœ¬è¡Œä¸ºå¤åˆ¶çš„å¤æ‚ä»£ç†æ¨¡æ‹Ÿã€‚ç„¶è€Œï¼Œåˆ›å»ºå¯è‡ªç”±å®šåˆ¶è§’è‰²çš„æ½œåŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æœ¬æ–‡ä»‹ç»äº†å¯å®šåˆ¶å¯¹è¯ä»£ç†æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨LLMæ¨¡æ‹ŸçœŸå®ä¸–ç•Œè§’è‰²ï¼Œé€šè¿‡ä¸ªæ€§åŒ–ç‰¹å¾æ³¨å…¥å®ç°æ ¹æ®ç”¨æˆ·åå¥½åˆ›å»ºå¤šæ ·åŒ–è§’è‰²ã€‚æœ¬æ–‡è¿˜æå‡ºäº†SimsConvæ•°æ®é›†ï¼ŒåŒ…å«68ä¸ªè‡ªå®šä¹‰è§’è‰²å’Œ13971ä¸ªè·¨1360ä¸ªçœŸå®åœºæ™¯çš„å¤šè½®è§’è‰²æ‰®æ¼”å¯¹è¯ã€‚é€šè¿‡SimsChatè¿™ä¸€å¯è‡ªç”±å®šåˆ¶çš„è§’è‰²æ‰®æ¼”ä»£ç†ï¼Œç»“åˆå„ç§ç°å®åœºæ™¯å’Œä¸»é¢˜ç‰¹å®šçš„è§’è‰²äº¤äº’ï¼Œå®éªŒç»“æœè¡¨æ˜SimsChatåœ¨ä¿æŒè§’è‰²ä¸€è‡´æ€§ã€çŸ¥è¯†å‡†ç¡®æ€§å’Œé€‚å½“çš„é—®é¢˜æ‹’ç»æ–¹é¢ä¼˜äºç°æœ‰æ¨¡å‹ã€‚è¯¥æ¡†æ¶ä¸ºå¼€å‘æ›´å‡†ç¡®å’Œå¯å®šåˆ¶çš„äººç±»æ¨¡æ‹Ÿä½“æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMå…·å¤‡ç†è§£å’Œç”Ÿæˆç±»ä¼¼äººç±»æ–‡æœ¬çš„èƒ½åŠ›ï¼Œå¯å®ç°å¤æ‚ä»£ç†æ¨¡æ‹Ÿã€‚</li>
<li>åˆ›å»ºå¯è‡ªç”±å®šåˆ¶è§’è‰²çš„æ½œåŠ›å°šæœªå……åˆ†æ¢ç´¢ã€‚</li>
<li>ä»‹ç»äº†å¯å®šåˆ¶å¯¹è¯ä»£ç†æ¡†æ¶ï¼Œåˆ©ç”¨LLMæ¨¡æ‹ŸçœŸå®ä¸–ç•Œè§’è‰²ï¼Œå®ç°è§’è‰²ä¸ªæ€§åŒ–å®šåˆ¶ã€‚</li>
<li>æå‡ºäº†SimsConvæ•°æ®é›†ï¼ŒåŒ…å«è‡ªå®šä¹‰è§’è‰²å’Œè·¨çœŸå®åœºæ™¯çš„å¤šè½®è§’è‰²æ‰®æ¼”å¯¹è¯ã€‚</li>
<li>SimsChatæ˜¯è‡ªç”±å¯å®šåˆ¶çš„è§’è‰²æ‰®æ¼”ä»£ç†ï¼Œç»“åˆç°å®åœºæ™¯å’Œä¸»é¢˜ç‰¹å®šçš„è§’è‰²äº¤äº’ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜SimsChatåœ¨è§’è‰²ä¸€è‡´æ€§ã€çŸ¥è¯†å‡†ç¡®æ€§å’Œé—®é¢˜æ‹’ç»æ–¹é¢ä¼˜äºç°æœ‰æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.17962">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-19857b36c718308e59a54843c5b65dd4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b4c5244b4a9d50e72b6e1124bc10a55a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7ae555b04f297b9ac22b3048a7fa2d40.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-64c3adf27271332468cf886dff829d8c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d57663d9a037c9951e23a8da2f3d00e6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6bffccd2ed6bf4ab389d3547d8e32180.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3396c571b913e3b5a7e4a18a54ebe250.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Lean-Attention-Hardware-Aware-Scalable-Attention-Mechanism-for-the-Decode-Phase-of-Transformers"><a href="#Lean-Attention-Hardware-Aware-Scalable-Attention-Mechanism-for-the-Decode-Phase-of-Transformers" class="headerlink" title="Lean Attention: Hardware-Aware Scalable Attention Mechanism for the   Decode-Phase of Transformers"></a>Lean Attention: Hardware-Aware Scalable Attention Mechanism for the   Decode-Phase of Transformers</h2><p><strong>Authors:Rya Sanovar, Srikant Bharadwaj, Renee St. Amant, Victor RÃ¼hle, Saravan Rajmohan</strong></p>
<p>Transformer-based models have emerged as one of the most widely used architectures for natural language processing, natural language generation, and image generation. The size of the state-of-the-art models has increased steadily reaching billions of parameters. These huge models are memory hungry and incur significant inference latency even on cutting edge AI-accelerators, such as GPUs. Specifically, the time and memory complexity of the attention operation is quadratic in terms of the total context length, i.e., prompt and output tokens. Thus, several optimizations such as key-value tensor caching and FlashAttention computation have been proposed to deliver the low latency demands of applications relying on such large models. However, these techniques do not cater to the computationally distinct nature of different phases during inference.   To that end, we propose LeanAttention, a scalable technique of computing self-attention for the token-generation phase (decode-phase) of decoder-only transformer models. LeanAttention enables scaling the attention mechanism implementation for the challenging case of long context lengths by re-designing the execution flow for the decode-phase. We identify that the associative property of online softmax can be treated as a reduction operation thus allowing us to parallelize the attention computation over these large context lengths. We extend the â€œstream-Kâ€ style reduction of tiled calculation to self-attention to enable parallel computation resulting in an average of 2.6x attention execution speedup over FlashAttention-2 and up to 8.33x speedup for 512k context lengths. </p>
<blockquote>
<p>åŸºäºTransformerçš„æ¨¡å‹å·²æˆä¸ºè‡ªç„¶è¯­è¨€å¤„ç†ã€è‡ªç„¶è¯­è¨€ç”Ÿæˆå’Œå›¾åƒç”Ÿæˆé¢†åŸŸæœ€å¹¿æ³›ä½¿ç”¨çš„æ¶æ„ä¹‹ä¸€ã€‚æœ€å…ˆè¿›çš„æ¨¡å‹è§„æ¨¡æŒç»­å¢å¤§ï¼Œå·²è¾¾åˆ°æ•°åäº¿å‚æ•°ã€‚è¿™äº›å¤§å‹æ¨¡å‹éœ€è¦å¤§é‡å†…å­˜ï¼Œå³ä½¿åœ¨å…ˆè¿›çš„AIåŠ é€Ÿå™¨ï¼ˆå¦‚GPUï¼‰ä¸Šä¹Ÿä¼šäº§ç”Ÿæ˜¾è‘—çš„æ¨ç†å»¶è¿Ÿã€‚å…·ä½“æ¥è¯´ï¼Œæ³¨æ„åŠ›æ“ä½œçš„æ—¶é—´å’Œå†…å­˜å¤æ‚åº¦ä¸æ€»ä¸Šä¸‹æ–‡é•¿åº¦æˆäºŒæ¬¡æ–¹å…³ç³»ï¼Œå³æç¤ºå’Œè¾“å‡ºä»¤ç‰Œã€‚å› æ­¤ï¼Œå·²ç»æå‡ºäº†é”®å€¼å¼ é‡ç¼“å­˜å’ŒFlashAttentionè®¡ç®—ç­‰ä¼˜åŒ–æªæ–½ï¼Œä»¥æ»¡è¶³ä¾èµ–è¿™äº›å¤§å‹æ¨¡å‹çš„åº”ç”¨ç¨‹åºçš„ä½å»¶è¿Ÿéœ€æ±‚ã€‚ç„¶è€Œï¼Œè¿™äº›æŠ€æœ¯å¹¶ä¸é€‚åº”æ¨ç†è¿‡ç¨‹ä¸­ä¸åŒé˜¶æ®µçš„è®¡ç®—ç‰¹æ€§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†LeanAttentionï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºè§£ç å™¨ä»…è§£ç å™¨transformeræ¨¡å‹çš„ä»¤ç‰Œç”Ÿæˆé˜¶æ®µï¼ˆè§£ç é˜¶æ®µï¼‰çš„è‡ªæ³¨æ„åŠ›è®¡ç®—çš„å¯æ‰©å±•æŠ€æœ¯ã€‚LeanAttentioné€šè¿‡é‡æ–°è®¾è®¡è§£ç é˜¶æ®µçš„æ‰§è¡Œæµç¨‹ï¼Œå®ç°äº†å¯¹é•¿ä¸Šä¸‹æ–‡æƒ…å†µä¸‹æ³¨æ„åŠ›æœºåˆ¶çš„ç¼©æ”¾ã€‚æˆ‘ä»¬å‘ç°åœ¨çº¿softmaxçš„å…³è”å±æ€§å¯ä»¥è§†ä¸ºå½’çº¦æ“ä½œï¼Œä»è€Œå…è®¸æˆ‘ä»¬åœ¨è¿™äº›é•¿ä¸Šä¸‹æ–‡ä¸Šå¹¶è¡Œè®¡ç®—æ³¨æ„åŠ›ã€‚æˆ‘ä»¬å°†â€œæµKâ€é£æ ¼çš„å¹³é“ºè®¡ç®—å½’çº¦æ‰©å±•åˆ°è‡ªæ³¨æ„åŠ›ï¼Œä»¥å®ç°å¹¶è¡Œè®¡ç®—ï¼Œç›¸å¯¹äºFlashAttention-2å®ç°å¹³å‡2.6å€çš„æ³¨æ„åŠ›æ‰§è¡Œé€Ÿåº¦æå‡ï¼Œå¯¹äº512kçš„ä¸Šä¸‹æ–‡é•¿åº¦ç”šè‡³è¾¾åˆ°8.33å€çš„é€Ÿåº¦æå‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.10480v2">PDF</a> 13 pages, 10 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Transformeræ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ã€è‡ªç„¶è¯­è¨€ç”Ÿæˆå’Œå›¾åƒç”Ÿæˆç­‰é¢†åŸŸä¸­çš„å¹¿æ³›åº”ç”¨ã€‚é’ˆå¯¹å¤§å‹Transformeræ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­é¢ä¸´çš„å†…å­˜æ¶ˆè€—å¤§ã€å»¶è¿Ÿé«˜çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºLeanAttentionçš„å¯æ‰©å±•è‡ªæ³¨æ„åŠ›è®¡ç®—æ–¹æ³•ã€‚è¯¥æ–¹æ³•é’ˆå¯¹è§£ç é˜¶æ®µçš„tokenç”Ÿæˆé˜¶æ®µè¿›è¡Œä¼˜åŒ–ï¼Œé€šè¿‡é‡æ–°è®¾è®¡æ‰§è¡Œæµç¨‹ï¼Œå®ç°å¯¹é•¿ä¸Šä¸‹æ–‡æƒ…å†µä¸‹æ³¨æ„åŠ›æœºåˆ¶çš„æ‰©å±•ã€‚åˆ©ç”¨åœ¨çº¿softmaxçš„å…³è”å±æ€§ï¼Œå°†å…¶è§†ä¸ºå½’çº¦æ“ä½œï¼Œä»è€Œå…è®¸åœ¨å¤§ä¸Šä¸‹æ–‡é•¿åº¦ä¸Šå¹¶è¡Œè®¡ç®—æ³¨æ„åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLeanAttentionåœ¨å¤§å‹ä¸Šä¸‹æ–‡é•¿åº¦ä¸‹å®ç°äº†æ˜¾è‘—çš„é€Ÿåº¦æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Transformeræ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†å’Œå›¾åƒç”Ÿæˆé¢†åŸŸå¹¿æ³›åº”ç”¨ï¼Œä½†å¤§å‹æ¨¡å‹é¢ä¸´å†…å­˜æ¶ˆè€—å¤§ã€å»¶è¿Ÿé«˜çš„é—®é¢˜ã€‚</li>
<li>ç°æœ‰ä¼˜åŒ–æŠ€æœ¯å¦‚é”®å€¼å¼ é‡ç¼“å­˜å’ŒFlashAttentionè®¡ç®—ä¸èƒ½æ»¡è¶³æ¨ç†è¿‡ç¨‹ä¸­ä¸åŒé˜¶æ®µçš„è®¡ç®—ç‰¹ç‚¹ã€‚</li>
<li>LeanAttentionæå‡ºäº†ä¸€ç§å¯æ‰©å±•çš„è‡ªæ³¨æ„åŠ›è®¡ç®—æ–¹æ³•ï¼Œé’ˆå¯¹è§£ç é˜¶æ®µçš„tokenç”Ÿæˆè¿›è¡Œä¼˜åŒ–ã€‚</li>
<li>LeanAttentioné€šè¿‡é‡æ–°è®¾è®¡æ‰§è¡Œæµç¨‹ï¼Œå®ç°å¯¹é•¿ä¸Šä¸‹æ–‡æƒ…å†µä¸‹æ³¨æ„åŠ›æœºåˆ¶çš„æ‰©å±•ã€‚</li>
<li>åˆ©ç”¨åœ¨çº¿softmaxçš„å…³è”å±æ€§ï¼Œå°†å…¶è§†ä¸ºå½’çº¦æ“ä½œï¼Œå…è®¸åœ¨å¤§ä¸Šä¸‹æ–‡é•¿åº¦ä¸Šå¹¶è¡Œè®¡ç®—æ³¨æ„åŠ›ã€‚</li>
<li>LeanAttentionç›¸æ¯”FlashAttention-2å®ç°äº†å¹³å‡2.6å€çš„é€Ÿåº¦æå‡ï¼Œå¯¹äº512kçš„ä¸Šä¸‹æ–‡é•¿åº¦å®ç°äº†æœ€é«˜8.33å€çš„é€Ÿåº¦æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.10480">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-49450984dd6e0517cc4eef139b2fe67d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-76cb39e5d16114a6ba9558735df70166.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-21113e20aa1daab0c14f43943ce83ea5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bd59eacc91f666c6df4a3c0368220d76.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-09ae430c78a162aa835825422bad5cad.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Meaning-Typed-Programming-Language-level-Abstractions-and-Runtime-for-GenAI-Applications"><a href="#Meaning-Typed-Programming-Language-level-Abstractions-and-Runtime-for-GenAI-Applications" class="headerlink" title="Meaning-Typed Programming: Language-level Abstractions and Runtime for   GenAI Applications"></a>Meaning-Typed Programming: Language-level Abstractions and Runtime for   GenAI Applications</h2><p><strong>Authors:Jason Mars, Yiping Kang, Jayanaka L. Dantanarayana, Kugesan Sivasothynathan, Christopher Clarke, Baichuan Li, Krisztian Flautner, Lingjia Tang</strong></p>
<p>Software is rapidly evolving from being programmed with traditional logical code, to neuro-integrated applications that leverage generative AI and large language models (LLMs) for application functionality. This shift increases the complexity of building applications, as developers now must reasoning about, program, and prompt LLMs. Despite efforts to create tools to assist with prompt engineering, these solutions often introduce additional layers of complexity to the development of neuro-integrated applications. This paper proposes meaning-typed programming (MTP), a novel approach to simplify the creation of neuro-integrated applications by introducing new language-level abstractions that hide the complexities of LLM integration. Our key insight is that typical conventional code already possesses a high level of semantic richness that can be automatically reasoned about, as it is designed to be readable and maintainable by humans. Leveraging this insight, we conceptualize LLMs as meaning-typed code constructs and introduce a by abstraction at the language level, MT-IR, a new meaning-based intermediate representation at the compiler level, and MT Runtime, an automated run-time engine for LLM integration and operations. We implement MTP in a production-grade Python super-set language called Jac and perform an extensive evaluation. Our results demonstrate that MTP not only simplifies the development process but also meets or exceeds the efficacy of state-of-the-art manual and tool-assisted prompt engineering techniques in terms of accuracy and usability. </p>
<blockquote>
<p>è½¯ä»¶æ­£ä»ä½¿ç”¨ä¼ ç»Ÿé€»è¾‘ä»£ç ç¼–ç¨‹è¿…é€Ÿæ¼”å˜ä¸ºåˆ©ç”¨ç”Ÿæˆå¼äººå·¥æ™ºèƒ½å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç¥ç»é›†æˆåº”ç”¨ç¨‹åºã€‚è¿™ä¸€è½¬å˜å¢åŠ äº†æ„å»ºåº”ç”¨ç¨‹åºçš„å¤æ‚æ€§ï¼Œå› ä¸ºå¼€å‘è€…ç°åœ¨å¿…é¡»ç†è§£ã€ç¼–ç¨‹å’Œæç¤ºLLMã€‚å°½ç®¡æœ‰åˆ›å»ºè¾…åŠ©æç¤ºå·¥ç¨‹çš„å·¥å…·çš„åŠªåŠ›ï¼Œä½†è¿™äº›è§£å†³æ–¹æ¡ˆå¾€å¾€ä¸ºç¥ç»é›†æˆåº”ç”¨ç¨‹åºçš„å¼€å‘å¢åŠ äº†é¢å¤–çš„å¤æ‚å±‚ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.08965v3">PDF</a> </p>
<p><strong>Summary</strong><br>åœ¨ç°ä»£è½¯ä»¶å¼€å‘ä¸­ï¼Œé€»è¾‘ç¼–ç¨‹æ­£é€æ¸è½¬å‘ä»¥ç”Ÿæˆå¼äººå·¥æ™ºèƒ½å’Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸ºæ ¸å¿ƒçš„ç¥ç»é›†æˆåº”ç”¨ã€‚ç„¶è€Œï¼Œè¿™å¢åŠ äº†å¼€å‘å¤æ‚æ€§ã€‚æœ¬æ–‡æå‡ºæ„ä¹‰ç±»å‹ç¼–ç¨‹ï¼ˆMTPï¼‰ï¼Œé€šè¿‡å¼•å…¥æ–°çš„è¯­è¨€çº§æŠ½è±¡æ¥ç®€åŒ–ç¥ç»é›†æˆåº”ç”¨çš„åˆ›å»ºï¼Œéšè—LLMé›†æˆçš„å¤æ‚æ€§ã€‚ç ”ç©¶çš„å…³é”®è§è§£åœ¨äºåˆ©ç”¨ä¼ ç»Ÿä»£ç çš„é«˜è¯­ä¹‰ä¸°å¯Œæ€§ï¼Œå°†LLMè§†ä¸ºæ„ä¹‰ç±»å‹ä»£ç æ„é€ ã€‚é€šè¿‡ç¼–è¯‘å™¨çº§åˆ«çš„æ„ä¹‰åŸºç¡€ä¸­é—´è¡¨ç¤ºï¼ˆMT-IRï¼‰å’Œè‡ªåŠ¨åŒ–è¿è¡Œæ—¶å¼•æ“ï¼ˆMT Runtimeï¼‰ï¼Œå®ç°äº†MTPã€‚åœ¨Pythonè¶…é›†è¯­è¨€Jacä¸­çš„å®ç°å¹¶è¿›è¡Œè¯„ä¼°è¡¨æ˜ï¼ŒMTPä¸ä»…ç®€åŒ–äº†å¼€å‘è¿‡ç¨‹ï¼Œè€Œä¸”åœ¨å‡†ç¡®æ€§å’Œå¯ç”¨æ€§æ–¹é¢è¾¾åˆ°æˆ–è¶…è¿‡äº†æœ€æ–°æ‰‹åŠ¨å’Œå·¥å…·è¾…åŠ©æç¤ºå·¥ç¨‹æŠ€æœ¯çš„æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è½¯ä»¶æ­£åœ¨ä»é€»è¾‘ç¼–ç¨‹å‘åˆ©ç”¨ç”Ÿæˆå¼äººå·¥æ™ºèƒ½å’Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç¥ç»é›†æˆåº”ç”¨è½¬å˜ã€‚</li>
<li>è¿™ç§è½¬å˜å¢åŠ äº†å¼€å‘å¤æ‚æ€§ï¼Œéœ€è¦å¼€å‘è€…ç†è§£å’Œæ“ä½œLLMã€‚</li>
<li>å½“å‰å·¥å…·åœ¨è¾…åŠ©æç¤ºå·¥ç¨‹æ—¶ï¼Œå¯èƒ½å¼•å…¥é¢å¤–çš„å¤æ‚æ€§ã€‚</li>
<li>æå‡ºæ„ä¹‰ç±»å‹ç¼–ç¨‹ï¼ˆMTPï¼‰æ¥ç®€åŒ–ç¥ç»é›†æˆåº”ç”¨çš„å¼€å‘ã€‚</li>
<li>MTPåˆ©ç”¨ä¼ ç»Ÿä»£ç çš„é«˜è¯­ä¹‰ä¸°å¯Œæ€§ï¼Œå°†LLMè§†ä¸ºæ„ä¹‰ç±»å‹ä»£ç æ„é€ ã€‚</li>
<li>é€šè¿‡å¼•å…¥æ–°çš„è¯­è¨€çº§æŠ½è±¡ï¼ˆå¦‚MT-IRå’ŒMT Runtimeï¼‰ï¼Œå®ç°äº†MTPã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.08965">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9ec759b4da04a5269d4a302651f65a13.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-518ff93827e21d21ab732342e3e925b6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-062a0bbfe73720ba927969ba7b6f1e2b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ad3e3cb331bb3144ef8eb5d42d10c4f8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-58b237005ef27571bc957d52ce2ccb4e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8eec6f1839a168ab7c12d31c94debdac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5758327b5c8f843c4f9d81af4e79a2ab.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="SelectIT-Selective-Instruction-Tuning-for-LLMs-via-Uncertainty-Aware-Self-Reflection"><a href="#SelectIT-Selective-Instruction-Tuning-for-LLMs-via-Uncertainty-Aware-Self-Reflection" class="headerlink" title="SelectIT: Selective Instruction Tuning for LLMs via Uncertainty-Aware   Self-Reflection"></a>SelectIT: Selective Instruction Tuning for LLMs via Uncertainty-Aware   Self-Reflection</h2><p><strong>Authors:Liangxin Liu, Xuebo Liu, Derek F. Wong, Dongfang Li, Ziyi Wang, Baotian Hu, Min Zhang</strong></p>
<p>Instruction tuning (IT) is crucial to tailoring large language models (LLMs) towards human-centric interactions. Recent advancements have shown that the careful selection of a small, high-quality subset of IT data can significantly enhance the performance of LLMs. Despite this, common approaches often rely on additional models or data, which increases costs and limits widespread adoption. In this work, we propose a novel approach, termed SelectIT, that capitalizes on the foundational capabilities of the LLM itself. Specifically, we exploit the intrinsic uncertainty present in LLMs to more effectively select high-quality IT data, without the need for extra resources. Furthermore, we introduce a curated IT dataset, the Selective Alpaca, created by applying SelectIT to the Alpaca-GPT4 dataset. Empirical results demonstrate that IT using Selective Alpaca leads to substantial model ability enhancement. The robustness of SelectIT has also been corroborated in various foundation models and domain-specific tasks. Our findings suggest that longer and more computationally intensive IT data may serve as superior sources of IT, offering valuable insights for future research in this area. Data, code, and scripts are freely available at <a target="_blank" rel="noopener" href="https://github.com/Blue-Raincoat/SelectIT">https://github.com/Blue-Raincoat/SelectIT</a>. </p>
<blockquote>
<p>æŒ‡ä»¤å¾®è°ƒï¼ˆITï¼‰å¯¹äºé’ˆå¯¹äººç±»ä¸­å¿ƒäº¤äº’çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å®šåˆ¶è‡³å…³é‡è¦ã€‚æœ€è¿‘çš„è¿›å±•è¡¨æ˜ï¼Œä»”ç»†é€‰æ‹©ä¸€å°éƒ¨åˆ†é«˜è´¨é‡çš„ITæ•°æ®å¯ä»¥æ˜¾è‘—æé«˜LLMçš„æ€§èƒ½ã€‚å°½ç®¡å¦‚æ­¤ï¼Œå¸¸è§çš„æ–¹æ³•å¾€å¾€ä¾èµ–äºé¢å¤–çš„æ¨¡å‹æˆ–æ•°æ®ï¼Œè¿™å¢åŠ äº†æˆæœ¬å¹¶é™åˆ¶äº†å¹¿æ³›åº”ç”¨ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œç§°ä¸ºSelectITï¼Œå®ƒåˆ©ç”¨LLMæœ¬èº«çš„åŸºç¡€èƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åˆ©ç”¨LLMä¸­å­˜åœ¨çš„å†…åœ¨ä¸ç¡®å®šæ€§æ¥æ›´æœ‰æ•ˆåœ°é€‰æ‹©é«˜è´¨é‡çš„ITæ•°æ®ï¼Œæ— éœ€é¢å¤–çš„èµ„æºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡å°†SelectITåº”ç”¨äºAlpaca-GPT4æ•°æ®é›†ï¼Œå¼•å…¥äº†ä¸€ä¸ªç²¾é€‰çš„ITæ•°æ®é›†Selective Alpacaã€‚ç»éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨Selective Alpacaçš„ITå¤§å¤§æé«˜äº†æ¨¡å‹çš„èƒ½åŠ›ã€‚SelectITåœ¨å„ç§åŸºç¡€æ¨¡å‹å’Œç‰¹å®šé¢†åŸŸä»»åŠ¡ä¸­çš„ç¨³å¥æ€§ä¹Ÿå¾—åˆ°äº†è¯å®ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæ›´é•¿å’Œè®¡ç®—ä¸Šæ›´å¯†é›†çš„ITæ•°æ®å¯èƒ½ä½œä¸ºITçš„ä¼˜è´¨æ¥æºï¼Œä¸ºè¿™ä¸€é¢†åŸŸçš„æœªæ¥ç ”ç©¶æä¾›äº†å®è´µçš„è§è§£ã€‚æ•°æ®ã€ä»£ç å’Œè„šæœ¬å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Blue-Raincoat/SelectIT%E5%85%8D%E8%B4%B9%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/Blue-Raincoat/SelectITå…è´¹è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2402.16705v2">PDF</a> Accepted to NeurIPS 2024</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æŒ‡ä»¤å¾®è°ƒï¼ˆITï¼‰ä¸‹èƒ½æ›´å¥½åœ°è¿›è¡Œäººç±»äº¤äº’ã€‚ç ”ç©¶è¡¨æ˜ï¼Œé€‰æ‹©é«˜è´¨é‡çš„å°å‹ITæ•°æ®é›†èƒ½æœ‰æ•ˆæå‡LLMæ€§èƒ½ã€‚ä½†ç°æœ‰æ–¹æ³•å¸¸ä¾èµ–é¢å¤–æ¨¡å‹æˆ–æ•°æ®ï¼Œå¯¼è‡´æˆæœ¬å¢åŠ å¹¶é™åˆ¶äº†æ™®åŠã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•SelectITï¼Œåˆ©ç”¨LLMçš„åŸºç¡€èƒ½åŠ›æ¥é€‰æ‹©é«˜è´¨é‡çš„ITæ•°æ®ï¼Œæ— éœ€é¢å¤–èµ„æºã€‚æˆ‘ä»¬è¿˜é€šè¿‡SelectITæ–¹æ³•åˆ›å»ºäº†Selective Alpacaæ•°æ®é›†ï¼Œå¹¶è¯æ˜å…¶èƒ½æ˜¾è‘—æå‡æ¨¡å‹èƒ½åŠ›ã€‚SelectITåœ¨å¤šç§åŸºç¡€æ¨¡å‹å’Œé¢†åŸŸç‰¹å®šä»»åŠ¡ä¸­è¡¨ç°å‡ºç¨³å¥æ€§ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œé•¿ä¸”è®¡ç®—å¯†é›†çš„ITæ•°æ®å¯èƒ½æ˜¯æ›´ä¼˜çš„ITæ¥æºï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†å®è´µå¯ç¤ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æŒ‡ä»¤å¾®è°ƒï¼ˆITï¼‰å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œäººç±»äº¤äº’è‡³å…³é‡è¦ã€‚</li>
<li>é€‰æ‹©é«˜è´¨é‡çš„å°å‹ITæ•°æ®é›†èƒ½æ˜¾è‘—æå‡LLMæ€§èƒ½ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¸¸ä¾èµ–é¢å¤–èµ„æºï¼ŒSelectITæ–¹æ³•åˆ©ç”¨LLMçš„åŸºç¡€èƒ½åŠ›é€‰æ‹©ITæ•°æ®ï¼Œæ— éœ€é¢å¤–èµ„æºã€‚</li>
<li>é€šè¿‡SelectITæ–¹æ³•åˆ›å»ºäº†Selective Alpacaæ•°æ®é›†ï¼Œè¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>SelectITåœ¨å¤šç§æ¨¡å‹å’Œä»»åŠ¡ä¸­è¡¨ç°å‡ºç¨³å¥æ€§ã€‚</li>
<li>æ›´é•¿ã€è®¡ç®—å¯†é›†çš„ITæ•°æ®å¯èƒ½æ˜¯æ›´ä¼˜çš„ITæ¥æºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2402.16705">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f548f150fa0410f19dc77ae0b3ec6876.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7a8a5d0b279a13966d2d549f2326c8e3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-351c68912052b4967394f66ef602196a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c18c0cfde75b62c5e789a610e647184d.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-18/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-18/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-18/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-b4c5244b4a9d50e72b6e1124bc10a55a.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-18  Crafting Customisable Characters with LLMs Introducing SimsChat, a   Persona-Driven Role-Playing Agent Framework
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-18
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-17/TTS/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-43adc801f1d3aa8a7f76510ab47e37b1.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-17  Speech Synthesis along Perceptual Voice Quality Dimensions
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-17
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">31180k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
