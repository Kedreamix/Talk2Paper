<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-18  SynthLight Portrait Relighting with Diffusion Model by Learning to   Re-render Synthetic Faces">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-fb67e12998d31bb3ee872b18aaab809f.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-18
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-18
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    13.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    54 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-01-18-æ›´æ–°"><a href="#2025-01-18-æ›´æ–°" class="headerlink" title="2025-01-18 æ›´æ–°"></a>2025-01-18 æ›´æ–°</h1><h2 id="SynthLight-Portrait-Relighting-with-Diffusion-Model-by-Learning-to-Re-render-Synthetic-Faces"><a href="#SynthLight-Portrait-Relighting-with-Diffusion-Model-by-Learning-to-Re-render-Synthetic-Faces" class="headerlink" title="SynthLight: Portrait Relighting with Diffusion Model by Learning to   Re-render Synthetic Faces"></a>SynthLight: Portrait Relighting with Diffusion Model by Learning to   Re-render Synthetic Faces</h2><p><strong>Authors:Sumit Chaturvedi, Mengwei Ren, Yannick Hold-Geoffroy, Jingyuan Liu, Julie Dorsey, Zhixin Shu</strong></p>
<p>We introduce SynthLight, a diffusion model for portrait relighting. Our approach frames image relighting as a re-rendering problem, where pixels are transformed in response to changes in environmental lighting conditions. Using a physically-based rendering engine, we synthesize a dataset to simulate this lighting-conditioned transformation with 3D head assets under varying lighting. We propose two training and inference strategies to bridge the gap between the synthetic and real image domains: (1) multi-task training that takes advantage of real human portraits without lighting labels; (2) an inference time diffusion sampling procedure based on classifier-free guidance that leverages the input portrait to better preserve details. Our method generalizes to diverse real photographs and produces realistic illumination effects, including specular highlights and cast shadows, while preserving the subjectâ€™s identity. Our quantitative experiments on Light Stage data demonstrate results comparable to state-of-the-art relighting methods. Our qualitative results on in-the-wild images showcase rich and unprecedented illumination effects. Project Page: \url{<a target="_blank" rel="noopener" href="https://vrroom.github.io/synthlight/%7D">https://vrroom.github.io/synthlight/}</a> </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†SynthLightï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè‚–åƒé‡æ–°æ‰“å…‰çš„æ‰©æ•£æ¨¡å‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†å›¾åƒé‡æ–°æ‰“å…‰è§†ä¸ºé‡æ–°æ¸²æŸ“é—®é¢˜ï¼Œåƒç´ ä¼šéšç¯å¢ƒç…§æ˜æ¡ä»¶çš„æ”¹å˜è€Œå‘ç”Ÿå˜åŒ–ã€‚æˆ‘ä»¬ä½¿ç”¨åŸºäºç‰©ç†çš„æ¸²æŸ“å¼•æ“ï¼Œåˆæˆäº†ä¸€ä¸ªæ•°æ®é›†ï¼Œä»¥æ¨¡æ‹Ÿåœ¨ä¸åŒç…§æ˜æ¡ä»¶ä¸‹ä½¿ç”¨3Då¤´éƒ¨èµ„äº§è¿›è¡Œè¿™ç§å…‰ç…§è°ƒæ•´è½¬æ¢ã€‚æˆ‘ä»¬æå‡ºäº†ä¸¤ç§è®­ç»ƒå’Œæ¨ç†ç­–ç•¥ï¼Œä»¥ç¼©å°åˆæˆå›¾åƒå’ŒçœŸå®å›¾åƒé¢†åŸŸä¹‹é—´çš„å·®è·ï¼šï¼ˆ1ï¼‰å¤šä»»åŠ¡è®­ç»ƒï¼Œåˆ©ç”¨æ²¡æœ‰å…‰ç…§æ ‡ç­¾çš„çœŸå®äººåƒï¼›ï¼ˆ2ï¼‰åŸºäºåˆ†ç±»å™¨å¼•å¯¼çš„è‡ªç”±é‡‡æ ·è¿‡ç¨‹çš„æ¨ç†æ—¶é—´æ‰©æ•£ï¼Œåˆ©ç”¨è¾“å…¥çš„äººåƒæ¥æ›´å¥½åœ°ä¿ç•™ç»†èŠ‚ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€‚ç”¨äºå„ç§çœŸå®ç…§ç‰‡ï¼Œäº§ç”Ÿé€¼çœŸçš„ç…§æ˜æ•ˆæœï¼ŒåŒ…æ‹¬é«˜å…‰å’Œé˜´å½±ï¼ŒåŒæ—¶ä¿æŒä¸»ä½“çš„èº«ä»½ã€‚æˆ‘ä»¬åœ¨Light Stageæ•°æ®ä¸Šçš„å®šé‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ç»“æœä¸æœ€å…ˆè¿›çš„é‡æ–°æ‰“å…‰æ–¹æ³•ç›¸å½“ã€‚æˆ‘ä»¬åœ¨é‡å¤–å›¾åƒä¸Šçš„å®šæ€§ç»“æœå±•ç¤ºäº†ä¸°å¯Œä¸”å‰æ‰€æœªæœ‰çš„ç…§æ˜æ•ˆæœã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://vrroom.github.io/synthlight/">https://vrroom.github.io/synthlight/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.09756v1">PDF</a> 27 pages, 25 figures, Project Page   <a target="_blank" rel="noopener" href="https://vrroom.github.io/synthlight/">https://vrroom.github.io/synthlight/</a></p>
<p><strong>Summary</strong></p>
<p>ä»‹ç»äº†ä¸€ç§åä¸ºSynthLightçš„è‚–åƒé‡ç…§æ˜æ‰©æ•£æ¨¡å‹ã€‚è¯¥æ–¹æ³•å°†å›¾åƒé‡ç…§æ˜è§†ä¸ºé‡æ¸²æŸ“é—®é¢˜ï¼Œé€šè¿‡ç‰©ç†æ¸²æŸ“å¼•æ“åˆæˆæ•°æ®é›†ï¼Œæ¨¡æ‹Ÿä¸åŒå…‰ç…§æ¡ä»¶ä¸‹çš„åƒç´ å˜åŒ–ã€‚æå‡ºä¸¤ç§è®­ç»ƒå’Œæ¨ç†ç­–ç•¥ï¼Œä»¥ç¼©å°åˆæˆå›¾åƒå’ŒçœŸå®å›¾åƒä¹‹é—´çš„å·®è·ï¼šä¸€æ˜¯åˆ©ç”¨æ— å…‰ç…§æ ‡ç­¾çš„çœŸå®äººåƒè¿›è¡Œå¤šä»»åŠ¡è®­ç»ƒï¼›äºŒæ˜¯åœ¨æ¨ç†æ—¶é—´é‡‡ç”¨åŸºäºæ— åˆ†ç±»å™¨å¼•å¯¼æ‰©æ•£é‡‡æ ·ç¨‹åºï¼Œåˆ©ç”¨è¾“å…¥è‚–åƒæ›´å¥½åœ°ä¿ç•™ç»†èŠ‚ã€‚è¯¥æ–¹æ³•å¯å¹¿æ³›åº”ç”¨äºå„ç§çœŸå®ç…§ç‰‡ï¼Œäº§ç”Ÿé€¼çœŸçš„ç…§æ˜æ•ˆæœï¼ŒåŒ…æ‹¬é«˜å…‰å’Œé˜´å½±ï¼ŒåŒæ—¶ä¿æŒä¸»ä½“èº«ä»½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SynthLightæ˜¯ä¸€ç§ç”¨äºè‚–åƒé‡å…‰çš„æ‰©æ•£æ¨¡å‹ã€‚</li>
<li>è¯¥æ–¹æ³•å°†å›¾åƒé‡ç…§æ˜è§†ä¸ºé‡æ¸²æŸ“é—®é¢˜ï¼Œå¹¶æ¨¡æ‹Ÿå…‰ç…§æ¡ä»¶ä¸‹çš„åƒç´ å˜åŒ–ã€‚</li>
<li>ä½¿ç”¨ç‰©ç†æ¸²æŸ“å¼•æ“åˆæˆæ•°æ®é›†ã€‚</li>
<li>æå‡ºä¸¤ç§è®­ç»ƒå’Œæ¨ç†ç­–ç•¥æ¥ç¼©å°åˆæˆå›¾åƒå’ŒçœŸå®å›¾åƒä¹‹é—´çš„å·®è·ã€‚</li>
<li>å¤šä»»åŠ¡è®­ç»ƒåˆ©ç”¨æ— å…‰ç…§æ ‡ç­¾çš„çœŸå®äººåƒã€‚</li>
<li>æ¨ç†æ—¶é—´é‡‡ç”¨åŸºäºæ— åˆ†ç±»å™¨å¼•å¯¼æ‰©æ•£é‡‡æ ·ç¨‹åºï¼Œåˆ©ç”¨è¾“å…¥è‚–åƒä¿ç•™ç»†èŠ‚ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½å¹¿æ³›åº”ç”¨äºå„ç§çœŸå®ç…§ç‰‡ï¼Œäº§ç”Ÿé€¼çœŸçš„ç…§æ˜æ•ˆæœï¼Œå¹¶ä¿æŒä¸»ä½“èº«ä»½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.09756">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7d2af9c02ee2bae2a7fe82145276367f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-594d45b8a1fbd696f956e03aa6b99d5e.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-18\./crop_Diffusion Models/2501.09756v1/page_3_0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ea70621a8cb2f4e7582a651aaee323e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f01bfe90a02ec7231d88c183672e6267.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="PATCHEDSERVE-A-Patch-Management-Framework-for-SLO-Optimized-Hybrid-Resolution-Diffusion-Serving"><a href="#PATCHEDSERVE-A-Patch-Management-Framework-for-SLO-Optimized-Hybrid-Resolution-Diffusion-Serving" class="headerlink" title="PATCHEDSERVE: A Patch Management Framework for SLO-Optimized Hybrid   Resolution Diffusion Serving"></a>PATCHEDSERVE: A Patch Management Framework for SLO-Optimized Hybrid   Resolution Diffusion Serving</h2><p><strong>Authors:Desen Sun, Zepeng Zhao, Yuke Wang</strong></p>
<p>The Text-to-Image (T2I) diffusion model is one of the most popular models in the world. However, serving diffusion models at the entire image level faces several problems, especially when there are multiple candidate resolutions. First, image based serving system prevents requests with different resolutions from batching together. On the other hand, requests with hybrid resolutions also indicate diverse locality features, which makes it hard to apply the same cache policy to all of them. To this end, we propose PATCHEDSERVE, A Patch Management Framework for SLO-Optimized Hybrid Resolution Diffusion Serving that provides a patch-level management strategy to gather hybrid resolution requests into batches. Specifically, PATCHEDSERVE incorporates a novel patch-based processing workflow, significantly enhancing throughput for hybrid resolution inputs. Furthermore, PATCHEDSERVE designs a patch-level cache reuse policy to fully exploit the redundancy in diffusion. In addition, PATCHEDSERVE features an SLO-aware scheduling algorithm with lightweight online latency prediction, achieving higher SLO satisfaction rates. We show that PATCHEDSERVE can achieve 30.1 % higher SLO satisfaction compared to SOTA diffusion serving system while not hurt the image quality. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹æ˜¯ä¸–ç•Œä¸Šæœ€å—æ¬¢è¿æ¨¡å‹ä¹‹ä¸€ã€‚ç„¶è€Œï¼Œåœ¨æ•´ä¸ªå›¾åƒå±‚é¢æœåŠ¡æ‰©æ•£æ¨¡å‹é¢ä¸´å‡ ä¸ªé—®é¢˜ï¼Œå°¤å…¶æ˜¯å½“å­˜åœ¨å¤šä¸ªå€™é€‰åˆ†è¾¨ç‡æ—¶ã€‚é¦–å…ˆï¼ŒåŸºäºå›¾åƒçš„æœåŠ¡ç³»ç»Ÿé˜»æ­¢å…·æœ‰ä¸åŒåˆ†è¾¨ç‡çš„è¯·æ±‚è¿›è¡Œæ‰¹å¤„ç†ã€‚å¦ä¸€æ–¹é¢ï¼Œæ··åˆåˆ†è¾¨ç‡çš„è¯·æ±‚ä¹Ÿè¡¨ç°å‡ºä¸åŒçš„å±€éƒ¨ç‰¹å¾ï¼Œè¿™ä½¿å¾—éš¾ä»¥å¯¹æ‰€æœ‰è¯·æ±‚åº”ç”¨ç›¸åŒçš„ç¼“å­˜ç­–ç•¥ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºPATCHEDSERVEï¼Œä¸€ä¸ªé¢å‘SLOä¼˜åŒ–çš„æ··åˆåˆ†è¾¨ç‡æ‰©æ•£æœåŠ¡çš„è¡¥ä¸ç®¡ç†æ¡†æ¶ï¼Œå®ƒæä¾›äº†ä¸€ç§è¡¥ä¸çº§çš„ç®¡ç†ç­–ç•¥ï¼Œå°†æ··åˆåˆ†è¾¨ç‡çš„è¯·æ±‚èšé›†åˆ°æ‰¹æ¬¡ä¸­ã€‚å…·ä½“æ¥è¯´ï¼ŒPATCHEDSERVEå¼•å…¥äº†ä¸€ç§æ–°é¢–çš„åŸºäºè¡¥ä¸çš„å¤„ç†å·¥ä½œæµç¨‹ï¼Œæ˜¾è‘—æé«˜äº†æ··åˆåˆ†è¾¨ç‡è¾“å…¥çš„ååé‡ã€‚æ­¤å¤–ï¼ŒPATCHEDSERVEè®¾è®¡äº†ä¸€ç§è¡¥ä¸çº§çš„ç¼“å­˜é‡ç”¨ç­–ç•¥ï¼Œä»¥å……åˆ†åˆ©ç”¨æ‰©æ•£ä¸­çš„å†—ä½™ä¿¡æ¯ã€‚è€Œä¸”ï¼ŒPATCHEDSERVEè¿˜é‡‡ç”¨äº†å…·æœ‰è½»é‡çº§åœ¨çº¿å»¶è¿Ÿé¢„æµ‹åŠŸèƒ½çš„SLOæ„ŸçŸ¥è°ƒåº¦ç®—æ³•ï¼Œä»¥å®ç°æ›´é«˜çš„SLOæ»¡æ„åº¦ã€‚æˆ‘ä»¬å±•ç¤ºï¼Œç›¸è¾ƒäºæœ€å…ˆè¿›ï¼ˆSOTAï¼‰çš„æ‰©æ•£æœåŠ¡ç³»ç»Ÿï¼ŒPATCHEDSERVEå¯ä»¥å®ç°é«˜å‡º30.1%çš„SLOæ»¡æ„åº¦ï¼ŒåŒæ—¶ä¸æŸå®³å›¾åƒè´¨é‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.09253v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    æ–‡æœ¬è½¬å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹é¢ä¸´å¤šç§åˆ†è¾¨ç‡æœåŠ¡çš„é—®é¢˜ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæå‡ºPATCHDESERVEæ¡†æ¶ï¼Œé‡‡ç”¨è¡¥ä¸çº§ç®¡ç†ç­–ç•¥ï¼Œèšé›†æ··åˆåˆ†è¾¨ç‡è¯·æ±‚è¿›è¡Œæ‰¹é‡å¤„ç†ï¼Œå¢å¼ºæ··åˆåˆ†è¾¨ç‡è¾“å…¥çš„ååé‡ï¼Œå¹¶è®¾è®¡è¡¥ä¸çº§ç¼“å­˜é‡ç”¨ç­–ç•¥ï¼Œå……åˆ†åˆ©ç”¨æ‰©æ•£ä¸­çš„å†—ä½™ä¿¡æ¯ã€‚æ­¤å¤–ï¼ŒPATCHEDSERVEè¿˜å…·å¤‡SLOæ„ŸçŸ¥è°ƒåº¦ç®—æ³•å’Œè½»é‡çº§åœ¨çº¿å»¶è¿Ÿé¢„æµ‹åŠŸèƒ½ï¼Œä»¥æé«˜SLOæ»¡æ„åº¦ã€‚ç›¸æ¯”ç°æœ‰æ‰©æ•£æœåŠ¡ç³»ç»Ÿï¼ŒPATCHEDSERVEèƒ½æé«˜30.1%çš„SLOæ»¡æ„åº¦ï¼ŒåŒæ—¶ä¸å½±å“å›¾åƒè´¨é‡ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ–‡æœ¬è½¬å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹åœ¨æœåŠ¡å¤šä¸ªå€™é€‰åˆ†è¾¨ç‡æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰å›¾åƒæœåŠ¡ç³»ç»Ÿæ— æ³•æœ‰æ•ˆå¤„ç†æ··åˆåˆ†è¾¨ç‡è¯·æ±‚ã€‚</li>
<li>PATCHEDSERVEæ¡†æ¶é‡‡ç”¨è¡¥ä¸çº§ç®¡ç†ç­–ç•¥ï¼Œèšé›†æ··åˆåˆ†è¾¨ç‡è¯·æ±‚è¿›è¡Œæ‰¹é‡å¤„ç†ã€‚</li>
<li>PATCHEDSERVEé€šè¿‡è¡¥ä¸çº§ç¼“å­˜ç­–ç•¥ï¼Œå……åˆ†åˆ©ç”¨æ‰©æ•£ä¸­çš„å†—ä½™ä¿¡æ¯ã€‚</li>
<li>PATCHEDSERVEå…·å¤‡SLOæ„ŸçŸ¥è°ƒåº¦ç®—æ³•ï¼Œä»¥æé«˜æœåŠ¡ç­‰çº§åè®®ï¼ˆSLOï¼‰çš„æ»¡æ„åº¦ã€‚</li>
<li>PATCHEDSERVEé€šè¿‡è½»é‡çº§åœ¨çº¿å»¶è¿Ÿé¢„æµ‹åŠŸèƒ½ä¼˜åŒ–æ€§èƒ½ã€‚</li>
<li>ä¸ç°æœ‰ç³»ç»Ÿç›¸æ¯”ï¼ŒPATCHEDSERVEåœ¨ä¸å½±å“å›¾åƒè´¨é‡çš„æƒ…å†µä¸‹ï¼Œæé«˜äº†SLOæ»¡æ„åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.09253">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-cc984ae4e15cdb0e258f4ea1f3a5a6a6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-733cbb02604e4754a35046aae470cf59.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4be66bf58d88606c1d76c4a55e51d482.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f267e4ce6498703fc4d3384ef134fdc4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6784a0e5e112ecd112b2dc0217c54e27.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Grounding-Text-To-Image-Diffusion-Models-For-Controlled-High-Quality-Image-Generation"><a href="#Grounding-Text-To-Image-Diffusion-Models-For-Controlled-High-Quality-Image-Generation" class="headerlink" title="Grounding Text-To-Image Diffusion Models For Controlled High-Quality   Image Generation"></a>Grounding Text-To-Image Diffusion Models For Controlled High-Quality   Image Generation</h2><p><strong>Authors:Ahmad SÃ¼leyman, GÃ¶ksel Biricik</strong></p>
<p>Large-scale text-to-image (T2I) diffusion models have demonstrated an outstanding performance in synthesizing diverse high-quality visuals from natural language text captions. Multiple layout-to-image models have been developed to control the generation process by utilizing a broad array of layouts such as segmentation maps, edges, and human keypoints. In this work, we present ObjectDiffusion, a model that takes inspirations from the top cutting-edge image generative frameworks to seamlessly condition T2I models with new bounding boxes capabilities. Specifically, we make substantial modifications to the network architecture introduced in ContorlNet to integrate it with the condition processing and injection techniques proposed in GLIGEN. ObjectDiffusion is initialized with pretraining parameters to leverage the generation knowledge obtained from training on large-scale datasets. We fine-tune ObjectDiffusion on the COCO2017 training dataset and evaluate it on the COCO2017 validation dataset. Our model achieves an AP$_{50}$ of 46.6, an AR of 44.5, and a FID of 19.8 outperforming the current SOTA model trained on open-source datasets in all of the three metrics. ObjectDiffusion demonstrates a distinctive capability in synthesizing diverse, high-quality, high-fidelity images that seamlessly conform to the semantic and spatial control layout. Evaluated in qualitative and quantitative tests, ObjectDiffusion exhibits remarkable grounding abilities on closed-set and open-set settings across a wide variety of contexts. The qualitative assessment verifies the ability of ObjectDiffusion to generate multiple objects of different sizes and locations. </p>
<blockquote>
<p>å¤§è§„æ¨¡æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹å·²ç»æ˜¾ç¤ºå‡ºä»è‡ªç„¶è¯­è¨€æ–‡æœ¬æè¿°ä¸­åˆæˆå¤šæ ·é«˜è´¨é‡å›¾åƒçš„å‡ºè‰²æ€§èƒ½ã€‚å·²ç»å¼€å‘å‡ºäº†å¤šç§å¸ƒå±€åˆ°å›¾åƒæ¨¡å‹ï¼Œé€šè¿‡åˆ©ç”¨å¹¿æ³›çš„å¸ƒå±€ï¼ˆå¦‚åˆ†å‰²å›¾ã€è¾¹ç¼˜å’Œäººä½“å…³é”®ç‚¹ï¼‰æ¥æ§åˆ¶ç”Ÿæˆè¿‡ç¨‹ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ObjectDiffusionæ¨¡å‹ï¼Œè¯¥æ¨¡å‹å—åˆ°æœ€æ–°å›¾åƒç”Ÿæˆæ¡†æ¶çš„å¯å‘ï¼Œèƒ½å¤Ÿæ— ç¼åœ°å°†T2Iæ¨¡å‹ä¸æ–°çš„è¾¹ç•Œæ¡†åŠŸèƒ½ç›¸ç»“åˆã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¯¹ControlNetä¸­å¼•å…¥çš„ç½‘ç»œæ¶æ„è¿›è¡Œäº†é‡å¤§ä¿®æ”¹ï¼Œå°†å…¶ä¸GLIGENä¸­æå‡ºçš„æ¡ä»¶å¤„ç†æŠ€æœ¯å’Œæ³¨å…¥æŠ€æœ¯ç›¸ç»“åˆã€‚ObjectDiffusionä½¿ç”¨é¢„è®­ç»ƒå‚æ•°è¿›è¡Œåˆå§‹åŒ–ï¼Œä»¥åˆ©ç”¨åœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šè®­ç»ƒå¾—åˆ°çš„ç”ŸæˆçŸ¥è¯†ã€‚æˆ‘ä»¬åœ¨COCO2017è®­ç»ƒæ•°æ®é›†ä¸Šå¯¹ObjectDiffusionè¿›è¡Œäº†å¾®è°ƒï¼Œå¹¶åœ¨COCO2017éªŒè¯æ•°æ®é›†ä¸Šå¯¹å…¶è¿›è¡Œäº†è¯„ä¼°ã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨AP50ã€ARå’ŒFIDä¸‰ä¸ªæŒ‡æ ‡ä¸Šåˆ†åˆ«è¾¾åˆ°äº†46.6ã€44.5å’Œ19.8ï¼Œä¼˜äºå½“å‰åœ¨å¼€æºæ•°æ®é›†ä¸Šè®­ç»ƒçš„æœ€æ–°æ¨¡å‹ã€‚ObjectDiffusionåœ¨åˆæˆå¤šæ ·ã€é«˜è´¨é‡ã€é«˜ä¿çœŸå›¾åƒæ–¹é¢è¡¨ç°å‡ºç‹¬ç‰¹çš„èƒ½åŠ›ï¼Œè¿™äº›å›¾åƒæ— ç¼åœ°ç¬¦åˆè¯­ä¹‰å’Œç©ºé—´æ§åˆ¶å¸ƒå±€ã€‚åœ¨å®šæ€§å’Œå®šé‡æµ‹è¯•ä¸­ï¼ŒObjectDiffusionåœ¨å°é—­é›†å’Œå¼€æ”¾é›†è®¾ç½®ä¸‹ï¼Œåœ¨å„ç§ä¸Šä¸‹æ–‡ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ¥åœ°èƒ½åŠ›ã€‚å®šæ€§è¯„ä¼°éªŒè¯äº†ObjectDiffusionç”Ÿæˆä¸åŒå¤§å°å’Œä½ç½®å¤šä¸ªå¯¹è±¡çš„èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.09194v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºObjectDiffusionçš„å¤§å‹æ–‡æœ¬è½¬å›¾åƒæ‰©æ•£æ¨¡å‹ã€‚è¯¥æ¨¡å‹èåˆäº†é¡¶çº§å›¾åƒç”Ÿæˆæ¡†æ¶çš„çµæ„Ÿï¼Œä¸ºæ–‡æœ¬è½¬å›¾åƒæ¨¡å‹å¢åŠ äº†æ–°çš„è¾¹ç•Œæ¡†åŠŸèƒ½ã€‚ObjectDiffusionåœ¨COCO2017æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒå¹¶è¯„ä¼°ï¼Œå®ç°äº†å‡ºè‰²çš„æ€§èƒ½ï¼Œä¼˜äºåœ¨å¼€æ”¾æºä»£ç æ•°æ®é›†ä¸Šè®­ç»ƒçš„å½“å‰æœ€ä½³æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ObjectDiffusionæ˜¯ä¸€ä¸ªå¤§å‹æ–‡æœ¬è½¬å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œèƒ½å¤Ÿä»è‡ªç„¶è¯­è¨€æ–‡æœ¬ç”Ÿæˆå¤šæ ·åŒ–çš„é«˜è´¨é‡å›¾åƒã€‚</li>
<li>è¯¥æ¨¡å‹é€šè¿‡å¼•å…¥è¾¹ç•Œæ¡†åŠŸèƒ½ï¼Œå®ç°äº†å¯¹ç”Ÿæˆè¿‡ç¨‹çš„ç²¾ç»†æ§åˆ¶ã€‚</li>
<li>ObjectDiffusioné‡‡ç”¨é¢„è®­ç»ƒå‚æ•°è¿›è¡Œåˆå§‹åŒ–ï¼Œåˆ©ç”¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šçš„ç”ŸæˆçŸ¥è¯†ã€‚</li>
<li>åœ¨COCO2017æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒåï¼ŒObjectDiffusionå®ç°äº†é«˜æ€§èƒ½æŒ‡æ ‡ï¼ŒåŒ…æ‹¬AP50ä¸º46.6ï¼ŒARä¸º44.5ï¼ŒFIDä¸º19.8ã€‚</li>
<li>ObjectDiffusionåœ¨åˆæˆä¸è¯­ä¹‰å’Œç©ºé—´æ§åˆ¶å¸ƒå±€ç›¸åŒ¹é…çš„å¤šæ ·åŒ–ã€é«˜è´¨é‡ã€é«˜ä¿çœŸå›¾åƒæ–¹é¢è¡¨ç°å‡ºç‹¬ç‰¹çš„èƒ½åŠ›ã€‚</li>
<li>åœ¨å®šæ€§æµ‹è¯•å’Œå®šé‡æµ‹è¯•ä¸­ï¼ŒObjectDiffusionåœ¨å°é—­é›†å’Œå¼€æ”¾é›†è®¾ç½®ä¸‹è¡¨ç°å‡ºå“è¶Šçš„æ¥åœ°èƒ½åŠ›ï¼Œé€‚ç”¨äºå„ç§ä¸Šä¸‹æ–‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.09194">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c3211c9c0c0ef8029c747f1edaff97b1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2157b11baf5f36314e2f720b41fe7283.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-debb6fb491d45a8fe2a8ce728200e4aa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3f866af1537628ec32d450d051ada726.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Generative-diffusion-model-with-inverse-renormalization-group-flows"><a href="#Generative-diffusion-model-with-inverse-renormalization-group-flows" class="headerlink" title="Generative diffusion model with inverse renormalization group flows"></a>Generative diffusion model with inverse renormalization group flows</h2><p><strong>Authors:Kanta Masuki, Yuto Ashida</strong></p>
<p>Diffusion models represent a class of generative models that produce data by denoising a sample corrupted by white noise. Despite the success of diffusion models in computer vision, audio synthesis, and point cloud generation, so far they overlook inherent multiscale structures in data and have a slow generation process due to many iteration steps. In physics, the renormalization group offers a fundamental framework for linking different scales and giving an accurate coarse-grained model. Here we introduce a renormalization group-based diffusion model that leverages multiscale nature of data distributions for realizing a high-quality data generation. In the spirit of renormalization group procedures, we define a flow equation that progressively erases data information from fine-scale details to coarse-grained structures. Through reversing the renormalization group flows, our model is able to generate high-quality samples in a coarse-to-fine manner. We validate the versatility of the model through applications to protein structure prediction and image generation. Our model consistently outperforms conventional diffusion models across standard evaluation metrics, enhancing sample quality and&#x2F;or accelerating sampling speed by an order of magnitude. The proposed method alleviates the need for data-dependent tuning of hyperparameters in the generative diffusion models, showing promise for systematically increasing sample efficiency based on the concept of the renormalization group. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹æ˜¯ä¸€ç±»ç”Ÿæˆæ¨¡å‹ï¼Œé€šè¿‡æ¶ˆé™¤ç”±ç™½å™ªå£°ç ´åçš„æ ·æœ¬ä¸­çš„å™ªå£°æ¥äº§ç”Ÿæ•°æ®ã€‚å°½ç®¡æ‰©æ•£æ¨¡å‹åœ¨è®¡ç®—æœºè§†è§‰ã€éŸ³é¢‘åˆæˆå’Œç‚¹äº‘ç”Ÿæˆæ–¹é¢å–å¾—äº†æˆåŠŸï¼Œä½†å®ƒä»¬å¿½è§†äº†æ•°æ®ä¸­çš„å†…åœ¨å¤šå°ºåº¦ç»“æ„ï¼Œå¹¶ä¸”ç”±äºè®¸å¤šè¿­ä»£æ­¥éª¤ï¼Œç”Ÿæˆè¿‡ç¨‹ç¼“æ…¢ã€‚åœ¨ç‰©ç†å­¦ä¸­ï¼Œé‡æ•´åŒ–ç»„æä¾›äº†ä¸€ä¸ªè”ç³»ä¸åŒå°ºåº¦å¹¶ç»™å‡ºç²¾ç¡®ç²—ç²’åº¦æ¨¡å‹çš„åŸºæœ¬æ¡†æ¶ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§åŸºäºé‡æ•´åŒ–ç»„çš„æ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åˆ©ç”¨æ•°æ®åˆ†å¸ƒçš„å¤šå°ºåº¦ç‰¹æ€§æ¥å®ç°é«˜è´¨é‡çš„æ•°æ®ç”Ÿæˆã€‚ç§‰æ‰¿é‡æ•´åŒ–ç»„ç¨‹åºçš„ç²¾ç¥ï¼Œæˆ‘ä»¬å®šä¹‰äº†ä¸€ä¸ªæµæ–¹ç¨‹ï¼Œè¯¥æ–¹ç¨‹ä»ç²¾ç»†å°ºåº¦ç»†èŠ‚åˆ°ç²—ç²’åº¦ç»“æ„é€æ­¥æ¶ˆé™¤æ•°æ®ä¿¡æ¯ã€‚é€šè¿‡åè½¬é‡æ•´åŒ–ç»„æµï¼Œæˆ‘ä»¬çš„æ¨¡å‹èƒ½å¤Ÿä»¥ç²—åˆ°ç»†çš„æ–¹å¼ç”Ÿæˆé«˜è´¨é‡æ ·æœ¬ã€‚æˆ‘ä»¬é€šè¿‡å°†æ¨¡å‹åº”ç”¨äºè›‹ç™½è´¨ç»“æ„é¢„æµ‹å’Œå›¾åƒç”Ÿæˆæ¥éªŒè¯å…¶é€šç”¨æ€§ã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨æ ‡å‡†è¯„ä¼°æŒ‡æ ‡ä¸Šä¸€è‡´åœ°ä¼˜äºä¼ ç»Ÿæ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡æé«˜æ ·æœ¬è´¨é‡æˆ–åŠ é€Ÿé‡‡æ ·é€Ÿåº¦æ¥æé«˜ä¸€ä¸ªæ•°é‡çº§çš„æ€§èƒ½ã€‚æ‰€æå‡ºçš„æ–¹æ³•å‡è½»äº†ç”Ÿæˆæ‰©æ•£æ¨¡å‹ä¸­æ•°æ®ä¾èµ–çš„è¶…å‚æ•°è°ƒæ•´éœ€æ±‚ï¼Œæ˜¾ç¤ºå‡ºæ ¹æ®é‡æ•´åŒ–ç»„çš„ç†å¿µç³»ç»Ÿåœ°æé«˜æ ·æœ¬æ•ˆç‡çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.09064v1">PDF</a> 9+21 pages, 4+11 figures. The code and trained models are available   at <a target="_blank" rel="noopener" href="https://github.com/kantamasuki/RGDM">https://github.com/kantamasuki/RGDM</a></p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹é€šè¿‡å»å™ªè¢«ç™½å™ªå£°ç ´åçš„æ ·æœ¬äº§ç”Ÿæ•°æ®ã€‚è™½ç„¶æ‰©æ•£æ¨¡å‹åœ¨è®¡ç®—æœºè§†è§‰ã€éŸ³é¢‘åˆæˆå’Œç‚¹äº‘ç”Ÿæˆæ–¹é¢å–å¾—äº†æˆåŠŸï¼Œä½†å®ƒä»¬å¿½ç•¥äº†æ•°æ®å†…åœ¨çš„å¤šå°ºåº¦ç»“æ„ï¼Œå¹¶ä¸”ç”±äºè®¸å¤šè¿­ä»£æ­¥éª¤è€Œå¯¼è‡´ç”Ÿæˆè¿‡ç¨‹ç¼“æ…¢ã€‚ç»“åˆç‰©ç†ä¸­çš„é‡æ•´åŒ–ç»„ç†è®ºï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºé‡æ•´åŒ–ç»„çš„æ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åˆ©ç”¨æ•°æ®åˆ†å¸ƒçš„å¤šå°ºåº¦ç‰¹æ€§å®ç°é«˜è´¨é‡çš„æ•°æ®ç”Ÿæˆã€‚é€šè¿‡é€†è½¬é‡æ•´åŒ–ç»„æµç¨‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿä»¥ä»ç²—åˆ°ç»†çš„æ–¹å¼ç”Ÿæˆé«˜è´¨é‡æ ·æœ¬ã€‚åœ¨è›‹ç™½è´¨ç»“æ„é¢„æµ‹å’Œå›¾åƒç”Ÿæˆæ–¹é¢çš„åº”ç”¨éªŒè¯äº†è¯¥æ¨¡å‹çš„é€šç”¨æ€§ã€‚è¯¥æ¨¡å‹åœ¨æ ‡å‡†è¯„ä¼°æŒ‡æ ‡ä¸Šä¸€è‡´åœ°ä¼˜äºä¼ ç»Ÿæ‰©æ•£æ¨¡å‹ï¼Œæé«˜äº†æ ·æœ¬è´¨é‡ï¼Œå¹¶å°†é‡‡æ ·é€Ÿåº¦æé«˜äº†æ•°å€ã€‚æ‰€æå‡ºçš„æ–¹æ³•å‡è½»äº†ç”Ÿæˆæ‰©æ•£æ¨¡å‹ä¸­æ•°æ®ç›¸å…³è¶…å‚æ•°è°ƒæ•´çš„éœ€æ±‚ï¼Œæ˜¾ç¤ºå‡ºåŸºäºé‡æ•´åŒ–ç»„æ¦‚å¿µçš„ç³»ç»Ÿæé«˜æ ·æœ¬æ•ˆç‡çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹é€šè¿‡å»å™ªè¢«å™ªå£°ç ´åçš„æ ·æœ¬äº§ç”Ÿæ•°æ®ã€‚</li>
<li>ä¼ ç»Ÿçš„æ‰©æ•£æ¨¡å‹å¿½ç•¥äº†æ•°æ®çš„å¤šå°ºåº¦ç»“æ„ï¼Œå¹¶ä¸”ç”Ÿæˆè¿‡ç¨‹è¾ƒæ…¢ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†åŸºäºé‡æ•´åŒ–ç»„çš„æ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½åˆ©ç”¨æ•°æ®åˆ†å¸ƒçš„å¤šå°ºåº¦ç‰¹æ€§ã€‚</li>
<li>é€šè¿‡é€†è½¬é‡æ•´åŒ–ç»„æµç¨‹ï¼Œè¯¥æ¨¡å‹èƒ½ä»¥ä»ç²—åˆ°ç»†çš„æ–¹å¼ç”Ÿæˆé«˜è´¨é‡æ ·æœ¬ã€‚</li>
<li>æ¨¡å‹åœ¨è›‹ç™½è´¨ç»“æ„é¢„æµ‹å’Œå›¾åƒç”Ÿæˆæ–¹é¢æœ‰ç€å¹¿æ³›çš„åº”ç”¨ã€‚</li>
<li>ä¸ä¼ ç»Ÿæ‰©æ•£æ¨¡å‹ç›¸æ¯”ï¼Œè¯¥æ¨¡å‹åœ¨è¯„ä¼°æŒ‡æ ‡ä¸Šè¡¨ç°æ›´ä¼˜ç§€ï¼Œæé«˜äº†æ ·æœ¬è´¨é‡å¹¶åŠ é€Ÿäº†é‡‡æ ·é€Ÿåº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.09064">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-54d8746326b7a3a1b4de7737a6035182.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-98ffb9254139a04d920a166fb16f5d42.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb67e12998d31bb3ee872b18aaab809f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a9ae6871f2ce34eca77a0b466f46402d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-790b693b81367070cc13fd6282b5b2f8.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="NeurOp-Diff-Continuous-Remote-Sensing-Image-Super-Resolution-via-Neural-Operator-Diffusion"><a href="#NeurOp-Diff-Continuous-Remote-Sensing-Image-Super-Resolution-via-Neural-Operator-Diffusion" class="headerlink" title="NeurOp-Diff:Continuous Remote Sensing Image Super-Resolution via Neural   Operator Diffusion"></a>NeurOp-Diff:Continuous Remote Sensing Image Super-Resolution via Neural   Operator Diffusion</h2><p><strong>Authors:Zihao Xu, Yuzhi Tang, Bowen Xu, Qingquan Li</strong></p>
<p>Most publicly accessible remote sensing data suffer from low resolution, limiting their practical applications. To address this, we propose a diffusion model guided by neural operators for continuous remote sensing image super-resolution (NeurOp-Diff). Neural operators are used to learn resolution representations at arbitrary scales, encoding low-resolution (LR) images into high-dimensional features, which are then used as prior conditions to guide the diffusion model for denoising. This effectively addresses the artifacts and excessive smoothing issues present in existing super-resolution (SR) methods, enabling the generation of high-quality, continuous super-resolution images. Specifically, we adjust the super-resolution scale by a scaling factor s, allowing the model to adapt to different super-resolution magnifications. Furthermore, experiments on multiple datasets demonstrate the effectiveness of NeurOp-Diff. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/zerono000/NeurOp-Diff">https://github.com/zerono000/NeurOp-Diff</a>. </p>
<blockquote>
<p>å¤§éƒ¨åˆ†å…¬å¼€å¯è®¿é—®çš„é¥æ„Ÿæ•°æ®å­˜åœ¨åˆ†è¾¨ç‡ä½çš„é—®é¢˜ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬çš„å®é™…åº”ç”¨ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”±ç¥ç»ç®—å­å¼•å¯¼çš„æ‰©æ•£æ¨¡å‹ï¼Œç”¨äºè¿ç»­é¥æ„Ÿå›¾åƒè¶…åˆ†è¾¨ç‡ï¼ˆNeurOp-Diffï¼‰ã€‚ç¥ç»ç®—å­ç”¨äºå­¦ä¹ ä»»æ„å°ºåº¦çš„åˆ†è¾¨ç‡è¡¨ç¤ºï¼Œå°†ä½åˆ†è¾¨ç‡ï¼ˆLRï¼‰å›¾åƒç¼–ç ä¸ºé«˜ç»´ç‰¹å¾ï¼Œç„¶åç”¨ä½œå…ˆéªŒæ¡ä»¶ï¼Œå¼•å¯¼æ‰©æ•£æ¨¡å‹è¿›è¡Œå»å™ªã€‚è¿™æœ‰æ•ˆåœ°è§£å†³äº†ç°æœ‰è¶…åˆ†è¾¨ç‡ï¼ˆSRï¼‰æ–¹æ³•ä¸­å‡ºç°çš„ä¼ªå½±å’Œè¿‡åº¦å¹³æ»‘é—®é¢˜ï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡ã€è¿ç»­çš„è¶…é«˜åˆ†è¾¨ç‡å›¾åƒã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é€šè¿‡ç¼©æ”¾å› å­sè°ƒæ•´è¶…åˆ†è¾¨ç‡å°ºåº¦ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿé€‚åº”ä¸åŒçš„è¶…åˆ†è¾¨ç‡æ”¾å¤§å€æ•°ã€‚æ­¤å¤–ï¼Œå¯¹å¤šä¸ªæ•°æ®é›†çš„å®éªŒè¯æ˜äº†NeurOp-Diffçš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨[<a target="_blank" rel="noopener" href="https://github.com/zerono000/NeurOp-Diff%E6%89%BE%E5%88%B0%E3%80%82]">https://github.com/zerono000/NeurOp-Diffæ‰¾åˆ°ã€‚]</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.09054v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>é’ˆå¯¹å¤§å¤šæ•°å…¬å¼€å¯ç”¨çš„é¥æ„Ÿæ•°æ®åˆ†è¾¨ç‡ä½çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºç¥ç»ç®—å­å’Œæ‰©æ•£æ¨¡å‹çš„è¿ç»­é¥æ„Ÿå›¾åƒè¶…åˆ†è¾¨ç‡æ–¹æ³•ï¼ˆNeurOp-Diffï¼‰ã€‚ç¥ç»ç®—å­ç”¨äºå­¦ä¹ ä»»æ„å°ºåº¦çš„åˆ†è¾¨ç‡è¡¨ç¤ºï¼Œå°†ä½åˆ†è¾¨ç‡ï¼ˆLRï¼‰å›¾åƒç¼–ç ä¸ºé«˜ç»´ç‰¹å¾ï¼Œä½œä¸ºå…ˆéªŒæ¡ä»¶å¼•å¯¼æ‰©æ•£æ¨¡å‹è¿›è¡Œå»å™ªã€‚è¿™æœ‰æ•ˆè§£å†³äº†ä¸€å€çš„è¶…åˆ†è¾¨ç‡æ–¹æ³•ä¸­å­˜åœ¨çš„ä¼ªå½±å’Œè¿‡åº¦å¹³æ»‘é—®é¢˜ï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡ã€è¿ç»­çš„è¶…é«˜åˆ†è¾¨ç‡å›¾åƒã€‚é€šè¿‡ç¼©æ”¾å› å­sè°ƒæ•´è¶…åˆ†è¾¨ç‡å°ºåº¦ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿé€‚åº”ä¸åŒçš„è¶…åˆ†è¾¨ç‡æ”¾å¤§å€æ•°ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯äº†NeurOp-Diffçš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>ç¥ç»ç®—å­ç”¨äºå­¦ä¹ ä»»æ„å°ºåº¦çš„åˆ†è¾¨ç‡è¡¨ç¤ºã€‚</li>
<li>ä½åˆ†è¾¨ç‡å›¾åƒè¢«ç¼–ç ä¸ºé«˜ç»´ç‰¹å¾ä½œä¸ºå…ˆéªŒæ¡ä»¶ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹åœ¨è¶…åˆ†è¾¨ç‡è¿‡ç¨‹ä¸­ç”¨äºå»å™ªã€‚</li>
<li>æ–¹æ³•è§£å†³äº†ç°æœ‰è¶…åˆ†è¾¨ç‡æ–¹æ³•çš„ä¼ªå½±å’Œè¿‡åº¦å¹³æ»‘é—®é¢˜ã€‚</li>
<li>é€šè¿‡ç¼©æ”¾å› å­è°ƒæ•´è¶…åˆ†è¾¨ç‡å°ºåº¦ï¼Œé€‚åº”ä¸åŒçš„æ”¾å¤§éœ€æ±‚ã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯äº†NeurOp-Diffçš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.09054">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6b09f1e8afca0b0421b4283b275997f2.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-18\./crop_Diffusion Models/2501.09054v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-18\./crop_Diffusion Models/2501.09054v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="FlexiClip-Locality-Preserving-Free-Form-Character-Animation"><a href="#FlexiClip-Locality-Preserving-Free-Form-Character-Animation" class="headerlink" title="FlexiClip: Locality-Preserving Free-Form Character Animation"></a>FlexiClip: Locality-Preserving Free-Form Character Animation</h2><p><strong>Authors:Anant Khandelwal</strong></p>
<p>Animating clipart images with seamless motion while maintaining visual fidelity and temporal coherence presents significant challenges. Existing methods, such as AniClipart, effectively model spatial deformations but often fail to ensure smooth temporal transitions, resulting in artifacts like abrupt motions and geometric distortions. Similarly, text-to-video (T2V) and image-to-video (I2V) models struggle to handle clipart due to the mismatch in statistical properties between natural video and clipart styles. This paper introduces FlexiClip, a novel approach designed to overcome these limitations by addressing the intertwined challenges of temporal consistency and geometric integrity. FlexiClip extends traditional B&#39;ezier curve-based trajectory modeling with key innovations: temporal Jacobians to correct motion dynamics incrementally, continuous-time modeling via probability flow ODEs (pfODEs) to mitigate temporal noise, and a flow matching loss inspired by GFlowNet principles to optimize smooth motion transitions. These enhancements ensure coherent animations across complex scenarios involving rapid movements and non-rigid deformations. Extensive experiments validate the effectiveness of FlexiClip in generating animations that are not only smooth and natural but also structurally consistent across diverse clipart types, including humans and animals. By integrating spatial and temporal modeling with pre-trained video diffusion models, FlexiClip sets a new standard for high-quality clipart animation, offering robust performance across a wide range of visual content. Project Page: <a target="_blank" rel="noopener" href="https://creative-gen.github.io/flexiclip.github.io/">https://creative-gen.github.io/flexiclip.github.io/</a> </p>
<blockquote>
<p>å°†å‰ªè´´ç”»å›¾åƒä»¥æ— ç¼è¿åŠ¨è¿›è¡ŒåŠ¨ç”»å¤„ç†ï¼ŒåŒæ—¶ä¿æŒè§†è§‰ä¿çœŸåº¦å’Œæ—¶é—´è¿è´¯æ€§ï¼Œè¿™å¸¦æ¥äº†ç›¸å½“å¤§çš„æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•ï¼Œå¦‚AniClipartï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°æ¨¡æ‹Ÿç©ºé—´å˜å½¢ï¼Œä½†å¾€å¾€ä¸èƒ½ä¿è¯å¹³æ»‘çš„æ—¶é—´è¿‡æ¸¡ï¼Œå¯¼è‡´è¿åŠ¨çªå…€å’Œå‡ ä½•å¤±çœŸç­‰ä¼ªå½±ã€‚åŒæ ·ï¼Œæ–‡æœ¬åˆ°è§†é¢‘ï¼ˆT2Vï¼‰å’Œå›¾åƒåˆ°è§†é¢‘ï¼ˆI2Vï¼‰çš„æ¨¡å‹åœ¨å¤„ç†å‰ªè´´ç”»æ—¶ä¹Ÿé‡åˆ°äº†å›°éš¾ï¼Œå› ä¸ºè‡ªç„¶è§†é¢‘å’Œå‰ªè´´ç”»é£æ ¼ä¹‹é—´çš„ç»Ÿè®¡å±æ€§ä¸åŒ¹é…ã€‚æœ¬æ–‡ä»‹ç»äº†FlexiClipï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨å…‹æœè¿™äº›é™åˆ¶çš„æ–°æ–¹æ³•ï¼Œé€šè¿‡è§£å†³æ—¶é—´ä¸€è‡´æ€§å’Œå‡ ä½•å®Œæ•´æ€§çš„äº¤ç»‡æŒ‘æˆ˜æ¥åº”å¯¹ã€‚FlexiClipå¯¹ä¼ ç»ŸåŸºäºBÃ©zieræ›²çº¿çš„è½¨è¿¹å»ºæ¨¡è¿›è¡Œäº†å…³é”®åˆ›æ–°ï¼šä½¿ç”¨æ—¶é—´é›…å¯æ¯”çŸ©é˜µé€æ­¥çº æ­£è¿åŠ¨åŠ¨åŠ›å­¦ï¼Œé€šè¿‡æ¦‚ç‡æµODEï¼ˆpfODEsï¼‰è¿›è¡Œè¿ç»­æ—¶é—´å»ºæ¨¡ä»¥å‡è½»æ—¶é—´å™ªå£°ï¼Œå¹¶å—åˆ°GFlowNetåŸç†å¯å‘çš„æµåŒ¹é…æŸå¤±ä»¥ä¼˜åŒ–å¹³æ»‘è¿åŠ¨è¿‡æ¸¡ã€‚è¿™äº›å¢å¼ºåŠŸèƒ½ç¡®ä¿äº†æ¶‰åŠå¿«é€Ÿè¿åŠ¨å’Œéåˆšæ€§å˜å½¢çš„å¤æ‚åœºæ™¯ä¸­çš„è¿è´¯åŠ¨ç”»ã€‚å¤§é‡å®éªŒéªŒè¯äº†FlexiClipåœ¨ç”ŸæˆåŠ¨ç”»æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œè¿™äº›åŠ¨ç”»ä¸ä»…å¹³æ»‘è‡ªç„¶ï¼Œè€Œä¸”åœ¨ä¸åŒç±»å‹å‰ªè´´ç”»ï¼ˆåŒ…æ‹¬äººç‰©å’ŒåŠ¨ç‰©ï¼‰ä¹‹é—´ç»“æ„ä¸€è‡´ã€‚é€šè¿‡ç»“åˆé¢„è®­ç»ƒçš„è§†é¢‘æ‰©æ•£æ¨¡å‹è¿›è¡Œç©ºé—´å’Œæ—¶é—´å»ºæ¨¡ï¼ŒFlexiClipä¸ºé«˜è´¨é‡å‰ªè´´åŠ¨ç”»è®¾å®šäº†æ–°æ ‡å‡†ï¼Œåœ¨å¹¿æ³›çš„è§†è§‰å†…å®¹ä¸­è¡¨ç°å‡ºç¨³å¥çš„æ€§èƒ½ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://creative-gen.github.io/flexiclip.github.io/">https://creative-gen.github.io/flexiclip.github.io/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.08676v1">PDF</a> 13 pages, 4 figures, 7 tables</p>
<p><strong>Summary</strong></p>
<p>FlexiClipæ˜¯ä¸€ç§é’ˆå¯¹clipartå›¾åƒåŠ¨ç”»çš„æ–°æ–¹æ³•ï¼Œæ—¨åœ¨å®ç°å¹³æ»‘æµç•…çš„è¿åŠ¨ï¼ŒåŒæ—¶ä¿æŒè§†è§‰ä¿çœŸåº¦å’Œæ—¶é—´è¿è´¯æ€§ã€‚å®ƒç»“åˆäº†åˆ›æ–°çš„è½¨è¿¹å»ºæ¨¡æŠ€æœ¯ï¼ŒåŒ…æ‹¬ä½¿ç”¨ä¸´æ—¶é›…å¯æ¯”çŸ©é˜µè¿›è¡Œå¢é‡è¿åŠ¨æ ¡æ­£ã€æ¦‚ç‡æµODEsè¿›è¡Œè¿ç»­æ—¶é—´å»ºæ¨¡ä»¥åŠåŸºäºGFlowNetåŸç†çš„æµåŒ¹é…æŸå¤±æ¥ä¼˜åŒ–å¹³æ»‘è¿åŠ¨è¿‡æ¸¡ã€‚FlexiClipé€šè¿‡æ•´åˆç©ºé—´å’Œæ—¶é—´å»ºæ¨¡ä¸é¢„è®­ç»ƒçš„è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œä¸ºé«˜è´¨é‡çš„clipartåŠ¨ç”»è®¾å®šäº†æ–°çš„æ ‡å‡†ï¼Œèƒ½åœ¨å„ç§è§†è§‰å†…å®¹ä¸­è¡¨ç°ç¨³å¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FlexiClipè§£å†³äº†clipartå›¾åƒåŠ¨ç”»ä¸­çš„æ˜¾è‘—æŒ‘æˆ˜ï¼Œå¦‚ä¿æŒè§†è§‰ä¿çœŸåº¦å’Œæ—¶é—´è¿è´¯æ€§ã€‚</li>
<li>æ–¹æ³•ç»“åˆäº†åˆ›æ–°çš„è½¨è¿¹å»ºæ¨¡æŠ€æœ¯ï¼Œç¡®ä¿è¿åŠ¨å¹³æ»‘ä¸”è‡ªç„¶ã€‚</li>
<li>é€šè¿‡ä½¿ç”¨ä¸´æ—¶é›…å¯æ¯”çŸ©é˜µå’Œæ¦‚ç‡æµODEsç­‰æŠ€æœ¯ï¼ŒFlexiClipå®ç°äº†å¤æ‚åœºæ™¯ä¸‹çš„è¿è´¯åŠ¨ç”»ã€‚</li>
<li>FlexiClipé›†æˆäº†ç©ºé—´å’Œæ—¶é—´å»ºæ¨¡ï¼Œä¸é¢„è®­ç»ƒçš„è§†é¢‘æ‰©æ•£æ¨¡å‹ç›¸ç»“åˆï¼Œä¸ºé«˜è´¨é‡clipartåŠ¨ç”»æ ‘ç«‹äº†æ–°æ ‡å‡†ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å„ç§è§†è§‰å†…å®¹ä¸­è¡¨ç°ç¨³å¥ï¼ŒåŒ…æ‹¬äººç±»å’ŒåŠ¨ç‰©ç­‰å¤šç§clipartç±»å‹ã€‚</li>
<li>FlexiClipèƒ½æœ‰æ•ˆå¤„ç†å¿«é€Ÿè¿åŠ¨å’Œéåˆšæ€§å˜å½¢ç­‰æŒ‘æˆ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.08676">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-18\./crop_Diffusion Models/2501.08676v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-18\./crop_Diffusion Models/2501.08676v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-18\./crop_Diffusion Models/2501.08676v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-18\./crop_Diffusion Models/2501.08676v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="TimeFlow-Longitudinal-Brain-Image-Registration-and-Aging-Progression-Analysis"><a href="#TimeFlow-Longitudinal-Brain-Image-Registration-and-Aging-Progression-Analysis" class="headerlink" title="TimeFlow: Longitudinal Brain Image Registration and Aging Progression   Analysis"></a>TimeFlow: Longitudinal Brain Image Registration and Aging Progression   Analysis</h2><p><strong>Authors:Bailiang Jian, Jiazhen Pan, Yitong Li, Fabian Bongratz, Ruochen Li, Daniel Rueckert, Benedikt Wiestler, Christian Wachinger</strong></p>
<p>Predicting future brain states is crucial for understanding healthy aging and neurodegenerative diseases. Longitudinal brain MRI registration, a cornerstone for such analyses, has long been limited by its inability to forecast future developments, reliance on extensive, dense longitudinal data, and the need to balance registration accuracy with temporal smoothness. In this work, we present \emph{TimeFlow}, a novel framework for longitudinal brain MRI registration that overcomes all these challenges. Leveraging a U-Net architecture with temporal conditioning inspired by diffusion models, TimeFlow enables accurate longitudinal registration and facilitates prospective analyses through future image prediction. Unlike traditional methods that depend on explicit smoothness regularizers and dense sequential data, TimeFlow achieves temporal consistency and continuity without these constraints. Experimental results highlight its superior performance in both future timepoint prediction and registration accuracy compared to state-of-the-art methods. Additionally, TimeFlow supports novel biological brain aging analyses, effectively differentiating neurodegenerative conditions from healthy aging. It eliminates the need for segmentation, thereby avoiding the challenges of non-trivial annotation and inconsistent segmentation errors. TimeFlow paves the way for accurate, data-efficient, and annotation-free prospective analyses of brain aging and chronic diseases. </p>
<blockquote>
<p>é¢„æµ‹æœªæ¥çš„å¤§è„‘çŠ¶æ€å¯¹äºç†è§£å¥åº·è€é¾„åŒ–å’Œç¥ç»é€€è¡Œæ€§ç–¾ç—…è‡³å…³é‡è¦ã€‚çºµå‘è„‘MRIæ³¨å†Œæ˜¯æ­¤ç±»åˆ†æçš„æ ¸å¿ƒï¼Œé•¿æœŸä»¥æ¥ä¸€ç›´å—åˆ°æ— æ³•é¢„æµ‹æœªæ¥å‘å±•ã€ä¾èµ–å¹¿æ³›è€Œå¯†é›†çš„çºµå‘æ•°æ®ä»¥åŠéœ€è¦åœ¨æ³¨å†Œç²¾åº¦å’Œæ—¶é—´å¹³æ»‘ä¹‹é—´å–å¾—å¹³è¡¡çš„é™åˆ¶ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†\emph{TimeFlow}ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„çºµå‘è„‘MRIæ³¨å†Œæ¡†æ¶ï¼Œå…‹æœäº†æ‰€æœ‰è¿™äº›æŒ‘æˆ˜ã€‚TimeFlowåˆ©ç”¨å—æ‰©æ•£æ¨¡å‹å¯å‘çš„å…·æœ‰æ—¶é—´æ¡ä»¶çš„U-Netæ¶æ„ï¼Œå®ç°äº†å‡†ç¡®çš„çºµå‘æ³¨å†Œï¼Œå¹¶é€šè¿‡æœªæ¥å›¾åƒé¢„æµ‹ä¿ƒè¿›äº†å‰ç»æ€§åˆ†æã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼Œè¿™äº›æ–¹æ³•ä¾èµ–äºæ˜ç¡®çš„å¹³æ»‘æ­£åˆ™å™¨å’Œå¯†é›†åºåˆ—æ•°æ®ï¼ŒTimeFlowåœ¨ä¸ä½¿ç”¨è¿™äº›çº¦æŸçš„æƒ…å†µä¸‹å®ç°äº†æ—¶é—´ä¸€è‡´æ€§å’Œè¿ç»­æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼Œå®ƒåœ¨æœªæ¥æ—¶é—´ç‚¹çš„é¢„æµ‹å’Œæ³¨å†Œç²¾åº¦æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒTimeFlowæ”¯æŒæ–°å‹ç”Ÿç‰©è„‘è¡°è€åˆ†æï¼Œå¯æœ‰æ•ˆåŒºåˆ†ç¥ç»é€€è¡Œæ€§ç–¾ç—…å’Œå¥åº·è¡°è€ã€‚å®ƒæ¶ˆé™¤äº†å¯¹åˆ†å‰²çš„éœ€æ±‚ï¼Œä»è€Œé¿å…äº†éå¹³å‡¡æ³¨é‡Šå’Œä¸ä¸€è‡´çš„åˆ†å‰²é”™è¯¯å¸¦æ¥çš„æŒ‘æˆ˜ã€‚TimeFlowä¸ºå‡†ç¡®ã€é«˜æ•ˆä¸”æ— éœ€æ³¨é‡Šçš„å‰ç»æ€§åˆ†æè„‘è¡°è€å’Œæ…¢æ€§ç–¾ç—…å¼€è¾Ÿäº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.08667v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ‰©æ•£æ¨¡å‹çš„æ—¶é—´æµæ¡†æ¶ï¼Œèƒ½æœ‰æ•ˆè§£å†³é•¿æœŸå¤§è„‘MRIæ³¨å†Œçš„æŒ‘æˆ˜ã€‚å®ƒä½¿ç”¨Uå‹ç½‘ç»œæ¶æ„ï¼Œé€šè¿‡æ‰©æ•£æ¨¡å‹å¯å‘çš„æ—¶é—´æ¡ä»¶ï¼Œå®ç°å‡†ç¡®çš„é•¿å‘¨æœŸæ³¨å†Œå’Œæœªæ¥å›¾åƒé¢„æµ‹ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œæ—¶é—´æµåœ¨æ— éœ€æ˜¾å¼å¹³æ»‘æ­£åˆ™åŒ–å’Œå¯†é›†åºåˆ—æ•°æ®çš„æƒ…å†µä¸‹ï¼Œå®ç°äº†æ—¶é—´ä¸€è‡´æ€§å’Œè¿ç»­æ€§ã€‚å…¶æ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶æ”¯æŒæ–°å‹è„‘è¡°è€åˆ†æï¼Œæœ‰æ•ˆåŒºåˆ†ç¥ç»é€€è¡Œæ€§ç–¾ç—…ä¸å¥åº·è¡°è€ã€‚æ—¶é—´æµæ¶ˆé™¤äº†å¯¹åˆ†å‰²çš„éœ€æ±‚ï¼Œé¿å…äº†éå¹³å‡¡æ ‡æ³¨å’Œåˆ†å‰²é”™è¯¯çš„ä¸ä¸€è‡´æ€§ã€‚å®ƒä¸ºå‡†ç¡®ã€é«˜æ•ˆä¸”æ— æ ‡æ³¨çš„è„‘è¡°è€å’Œæ…¢æ€§ç—…å‰ç»æ€§åˆ†æé“ºå¹³äº†é“è·¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æ—¶é—´æµæ¡†æ¶è§£å†³äº†é•¿æœŸå¤§è„‘MRIæ³¨å†Œçš„å¤šä¸ªæŒ‘æˆ˜ã€‚</li>
<li>åˆ©ç”¨Uå‹ç½‘ç»œæ¶æ„å’Œæ‰©æ•£æ¨¡å‹çš„å¯å‘ï¼Œå®ç°äº†å‡†ç¡®çš„é•¿å‘¨æœŸæ³¨å†Œå’Œæœªæ¥å›¾åƒé¢„æµ‹ã€‚</li>
<li>æ—¶é—´æµåœ¨ä¸ä¾èµ–æ˜¾å¼å¹³æ»‘æ­£åˆ™åŒ–å’Œå¯†é›†åºåˆ—æ•°æ®çš„æƒ…å†µä¸‹å®ç°äº†æ—¶é—´ä¸€è‡´æ€§å’Œè¿ç»­æ€§ã€‚</li>
<li>ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæ—¶é—´æµåœ¨æœªæ¥æ—¶é—´ç‚¹é¢„æµ‹å’Œæ³¨å†Œå‡†ç¡®æ€§æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>æ—¶é—´æµæ”¯æŒæ–°å‹è„‘è¡°è€åˆ†æï¼Œå¹¶èƒ½æœ‰æ•ˆåŒºåˆ†ç¥ç»é€€è¡Œæ€§ç–¾ç—…ä¸å¥åº·è¡°è€ã€‚</li>
<li>æ—¶é—´æµæ¶ˆé™¤äº†å¯¹åˆ†å‰²çš„éœ€æ±‚ï¼Œé¿å…äº†æ ‡æ³¨å’Œåˆ†å‰²é”™è¯¯çš„é—®é¢˜ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.08667">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-18\./crop_Diffusion Models/2501.08667v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-18\./crop_Diffusion Models/2501.08667v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-18\./crop_Diffusion Models/2501.08667v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="DynamicFace-High-Quality-and-Consistent-Video-Face-Swapping-using-Composable-3D-Facial-Priors"><a href="#DynamicFace-High-Quality-and-Consistent-Video-Face-Swapping-using-Composable-3D-Facial-Priors" class="headerlink" title="DynamicFace: High-Quality and Consistent Video Face Swapping using   Composable 3D Facial Priors"></a>DynamicFace: High-Quality and Consistent Video Face Swapping using   Composable 3D Facial Priors</h2><p><strong>Authors:Runqi Wang, Sijie Xu, Tianyao He, Yang Chen, Wei Zhu, Dejia Song, Nemo Chen, Xu Tang, Yao Hu</strong></p>
<p>Face swapping transfers the identity of a source face to a target face while retaining the attributes like expression, pose, hair, and background of the target face. Advanced face swapping methods have achieved attractive results. However, these methods often inadvertently transfer identity information from the target face, compromising expression-related details and accurate identity. We propose a novel method DynamicFace that leverages the power of diffusion model and plug-and-play temporal layers for video face swapping. First, we introduce four fine-grained face conditions using 3D facial priors. All conditions are designed to be disentangled from each other for precise and unique control. Then, we adopt Face Former and ReferenceNet for high-level and detailed identity injection. Through experiments on the FF++ dataset, we demonstrate that our method achieves state-of-the-art results in face swapping, showcasing superior image quality, identity preservation, and expression accuracy. Besides, our method could be easily transferred to video domain with temporal attention layer. Our code and results will be available on the project page: <a target="_blank" rel="noopener" href="https://dynamic-face.github.io/">https://dynamic-face.github.io/</a> </p>
<blockquote>
<p>é¢éƒ¨æ›¿æ¢æŠ€æœ¯èƒ½å¤Ÿå°†æºé¢éƒ¨çš„èº«ä»½è½¬ç§»åˆ°ç›®æ ‡é¢éƒ¨ï¼ŒåŒæ—¶ä¿ç•™ç›®æ ‡é¢éƒ¨çš„è¡¨æƒ…ã€å§¿æ€ã€å‘å‹å’ŒèƒŒæ™¯ç­‰å±æ€§ã€‚å…ˆè¿›çš„é¢éƒ¨æ›¿æ¢æ–¹æ³•å·²ç»å–å¾—äº†å¸å¼•äººçš„æˆæœã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å¸¸å¸¸ä¼šåœ¨ä¸ç»æ„é—´ä»ç›®æ ‡é¢éƒ¨è½¬ç§»èº«ä»½ä¿¡æ¯ï¼Œä»è€Œå½±å“åˆ°è¡¨æƒ…ç›¸å…³çš„ç»†èŠ‚å’Œå‡†ç¡®çš„èº«ä»½è¡¨è¾¾ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹æ–¹æ³•DynamicFaceï¼Œå®ƒåˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„å¼ºå¤§åŠŸèƒ½å’Œå³æ’å³ç”¨çš„æ—¶é—´å±‚æ¥å®ç°è§†é¢‘é¢éƒ¨æ›¿æ¢ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼•å…¥å››ä¸ªç²¾ç»†çš„é¢éƒ¨æ¡ä»¶ï¼Œä½¿ç”¨3Dé¢éƒ¨å…ˆéªŒã€‚æ‰€æœ‰æ¡ä»¶éƒ½è¢«è®¾è®¡ä¸ºç›¸äº’ç‹¬ç«‹ï¼Œä»¥å®ç°ç²¾ç¡®å’Œç‹¬ç‰¹çš„æ§åˆ¶ã€‚ç„¶åï¼Œæˆ‘ä»¬é‡‡ç”¨Face Formerå’ŒReferenceNetè¿›è¡Œé«˜çº§å’Œè¯¦ç»†çš„èº«ä»½æ³¨å…¥ã€‚åœ¨FF++æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨é¢éƒ¨æ›¿æ¢æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼Œå±•ç°äº†å“è¶Šçš„å›¾ç‰‡è´¨é‡ã€èº«ä»½ä¿ç•™å’Œè¡¨æƒ…å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿè½»æ¾åœ°é€šè¿‡æ—¶é—´æ³¨æ„åŠ›å±‚è½¬ç§»åˆ°è§†é¢‘é¢†åŸŸã€‚æˆ‘ä»¬çš„ä»£ç å’Œç»“æœå°†åœ¨é¡¹ç›®é¡µé¢å…¬å¸ƒï¼š<a target="_blank" rel="noopener" href="https://dynamic-face.github.io/">https://dynamic-face.github.io/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.08553v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„æ–°å‹åŠ¨æ€é¢éƒ¨æ›¿æ¢æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨å››ç»´ç²¾ç»†é¢éƒ¨æ¡ä»¶å’ŒFace Formerä¸ReferenceNetè¿›è¡Œé«˜çº§å’Œè¯¦ç»†èº«ä»½æ³¨å…¥ï¼Œå®ç°è§†é¢‘é¢éƒ¨æ›¿æ¢ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é¢éƒ¨æ›¿æ¢æ–¹é¢è¾¾åˆ°æœ€æ–°æ°´å¹³ï¼Œå…·æœ‰å‡ºè‰²çš„å›¾åƒè´¨é‡ã€èº«ä»½ä¿ç•™å’Œè¡¨æƒ…å‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¢éƒ¨æ›¿æ¢æŠ€æœ¯èƒ½å¤Ÿå°†æºé¢éƒ¨çš„èº«ä»½è½¬ç§»åˆ°ç›®æ ‡é¢éƒ¨ï¼ŒåŒæ—¶ä¿ç•™ç›®æ ‡é¢éƒ¨çš„è¡¨æƒ…ã€å§¿æ€ã€å‘å‹å’ŒèƒŒæ™¯ç­‰å±æ€§ã€‚</li>
<li>å½“å‰å…ˆè¿›çš„é¢éƒ¨æ›¿æ¢æ–¹æ³•é¢ä¸´åœ¨è½¬ç§»èº«ä»½ä¿¡æ¯æ—¶æŸå¤±è¡¨æƒ…ç»†èŠ‚å’Œå‡†ç¡®èº«ä»½çš„é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹åŠ¨æ€é¢éƒ¨æ›¿æ¢æ–¹æ³•DynamicFaceï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„å¼ºå¤§åŠŸèƒ½ä»¥åŠå³æ’å³ç”¨çš„æ—¶é—´å±‚ï¼Œå®ç°è§†é¢‘é¢éƒ¨æ›¿æ¢ã€‚</li>
<li>DynamicFaceé€šè¿‡å¼•å…¥å››ä¸ªç²¾ç»†é¢éƒ¨æ¡ä»¶ï¼Œåˆ©ç”¨ä¸‰ç»´é¢éƒ¨å…ˆéªŒè¿›è¡Œç²¾ç¡®è€Œç‹¬ç‰¹çš„æ§åˆ¶ã€‚</li>
<li>é‡‡ç”¨Face Formerå’ŒReferenceNetè¿›è¡Œé«˜çº§å’Œè¯¦ç»†çš„èº«ä»½æ³¨å…¥ï¼Œä»¥æé«˜é¢éƒ¨æ›¿æ¢çš„æ•ˆæœã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒDynamicFaceæ–¹æ³•åœ¨é¢éƒ¨æ›¿æ¢æ–¹é¢è¾¾åˆ°æœ€æ–°æ°´å¹³ï¼Œå…·æœ‰ä¼˜ç§€çš„å›¾åƒè´¨é‡ã€èº«ä»½ä¿ç•™å’Œè¡¨æƒ…å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.08553">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-18\./crop_Diffusion Models/2501.08553v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-18\./crop_Diffusion Models/2501.08553v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-18\./crop_Diffusion Models/2501.08553v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-18\./crop_Diffusion Models/2501.08553v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-18\./crop_Diffusion Models/2501.08553v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="A-General-Framework-for-Inference-time-Scaling-and-Steering-of-Diffusion-Models"><a href="#A-General-Framework-for-Inference-time-Scaling-and-Steering-of-Diffusion-Models" class="headerlink" title="A General Framework for Inference-time Scaling and Steering of Diffusion   Models"></a>A General Framework for Inference-time Scaling and Steering of Diffusion   Models</h2><p><strong>Authors:Raghav Singhal, Zachary Horvitz, Ryan Teehan, Mengye Ren, Zhou Yu, Kathleen McKeown, Rajesh Ranganath</strong></p>
<p>Diffusion models produce impressive results in modalities ranging from images and video to protein design and text. However, generating samples with user-specified properties remains a challenge. Recent research proposes fine-tuning models to maximize rewards that capture desired properties, but these methods require expensive training and are prone to mode collapse. In this work, we propose Feynman Kac (FK) steering, an inference-time framework for steering diffusion models with reward functions. FK steering works by sampling a system of multiple interacting diffusion processes, called particles, and resampling particles at intermediate steps based on scores computed using functions called potentials. Potentials are defined using rewards for intermediate states and are selected such that a high value indicates that the particle will yield a high-reward sample. We explore various choices of potentials, intermediate rewards, and samplers. We evaluate FK steering on text-to-image and text diffusion models. For steering text-to-image models with a human preference reward, we find that FK steering a 0.8B parameter model outperforms a 2.6B parameter fine-tuned model on prompt fidelity, with faster sampling and no training. For steering text diffusion models with rewards for text quality and specific text attributes, we find that FK steering generates lower perplexity, more linguistically acceptable outputs and enables gradient-free control of attributes like toxicity. Our results demonstrate that inference-time scaling and steering of diffusion models, even with off-the-shelf rewards, can provide significant sample quality gains and controllability benefits. Code is available at <a target="_blank" rel="noopener" href="https://github.com/zacharyhorvitz/Fk-Diffusion-Steering">https://github.com/zacharyhorvitz/Fk-Diffusion-Steering</a> . </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒã€è§†é¢‘ã€è›‹ç™½è´¨è®¾è®¡å’Œæ–‡æœ¬ç­‰å¤šç§æ¨¡æ€ä¸­å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„æ•ˆæœã€‚ç„¶è€Œï¼Œç”Ÿæˆå…·æœ‰ç”¨æˆ·æŒ‡å®šå±æ€§çš„æ ·æœ¬ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æœ€è¿‘çš„ç ”ç©¶æå‡ºå¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»¥æœ€å¤§åŒ–æ•æ‰æ‰€éœ€å±æ€§çš„å¥–åŠ±ï¼Œä½†è¿™äº›æ–¹æ³•éœ€è¦æ˜‚è´µçš„è®­ç»ƒä¸”å®¹æ˜“å‡ºç°æ¨¡å¼å´©æºƒã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†è´¹æ›¼Â·å¡å…‹ï¼ˆFKï¼‰è½¬å‘ï¼Œè¿™æ˜¯ä¸€ç§åœ¨æ¨ç†æ—¶é—´æ¡†æ¶ä¸­ä½¿ç”¨å¥–åŠ±å‡½æ•°å¼•å¯¼æ‰©æ•£æ¨¡å‹çš„æ¡†æ¶ã€‚FKè½¬å‘é€šè¿‡é‡‡æ ·å¤šä¸ªç›¸äº’ä½œç”¨çš„æ‰©æ•£è¿‡ç¨‹ç³»ç»Ÿï¼ˆç§°ä¸ºç²’å­ï¼‰è¿›è¡Œå·¥ä½œï¼Œå¹¶åœ¨ä¸­é—´æ­¥éª¤åŸºäºä½¿ç”¨ç§°ä¸ºæ½œåŠ›çš„å‡½æ•°è®¡ç®—å¾—åˆ†è¿›è¡Œç²’å­é‡é‡‡æ ·ã€‚æ½œåŠ›æ˜¯é€šè¿‡ä¸­é—´çŠ¶æ€çš„å¥–åŠ±å®šä¹‰çš„ï¼Œå¹¶é€‰æ‹©ä½¿å¾—é«˜å€¼è¡¨ç¤ºç²’å­å°†äº§ç”Ÿé«˜å¥–åŠ±æ ·æœ¬ã€‚æˆ‘ä»¬æ¢è®¨äº†å„ç§æ½œåŠ›çš„é€‰æ‹©ã€ä¸­é—´å¥–åŠ±å’Œé‡‡æ ·å™¨ã€‚æˆ‘ä»¬å¯¹æ–‡æœ¬åˆ°å›¾åƒå’Œæ–‡æœ¬æ‰©æ•£æ¨¡å‹è¯„ä¼°äº†FKè½¬å‘ã€‚å¯¹äºä½¿ç”¨äººç±»åå¥½å¥–åŠ±å¼•å¯¼æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ï¼Œæˆ‘ä»¬å‘ç°FKè½¬å‘ä¸€ä¸ª0.8Bå‚æ•°æ¨¡å‹çš„æç¤ºä¿çœŸåº¦è¶…è¿‡äº†2.6Bå‚æ•°å¾®è°ƒæ¨¡å‹ï¼Œå…·æœ‰æ›´å¿«çš„é‡‡æ ·é€Ÿåº¦å’Œæ— éœ€è®­ç»ƒã€‚å¯¹äºä½¿ç”¨æ–‡æœ¬è´¨é‡å’Œç‰¹å®šæ–‡æœ¬å±æ€§çš„å¥–åŠ±å¼•å¯¼æ–‡æœ¬æ‰©æ•£æ¨¡å‹ï¼Œæˆ‘ä»¬å‘ç°FKè½¬å‘äº§ç”Ÿäº†æ›´ä½çš„å›°æƒ‘åº¦ï¼Œæ›´åœ¨è¯­è¨€ä¸Šå¯æ¥å—çš„è¾“å‡ºï¼Œå¹¶å®ç°äº†å¯¹è¯¸å¦‚æ¯’æ€§ç­‰å±æ€§çš„æ— æ¢¯åº¦æ§åˆ¶ã€‚æˆ‘ä»¬çš„ç»“æœè¯æ˜äº†å³ä½¿åœ¨ç¦»çº¿å¥–åŠ±çš„æƒ…å†µä¸‹ï¼Œæ¨ç†æ—¶é—´å°ºåº¦å’Œå¼•å¯¼æ‰©æ•£æ¨¡å‹ä¹Ÿå¯ä»¥æä¾›æ˜¾è‘—çš„æ ·æœ¬è´¨é‡æå‡å’Œå¯æ§æ€§ä¼˜åŠ¿ã€‚ç›¸å…³ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/zacharyhorvitz/Fk-Diffusion-Steering%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/zacharyhorvitz/Fk-Diffusion-Steeringä¸­æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.06848v3">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ‰©æ•£æ¨¡å‹å¹¿æ³›åº”ç”¨äºå›¾åƒã€è§†é¢‘ã€è›‹ç™½è´¨è®¾è®¡å’Œæ–‡æœ¬ç­‰å¤šä¸ªé¢†åŸŸï¼Œç”Ÿæˆå…·æœ‰ç”¨æˆ·æŒ‡å®šå±æ€§çš„æ ·æœ¬ä»æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•é€šè¿‡å¾®è°ƒæ¨¡å‹ä»¥æœ€å¤§åŒ–å¥–åŠ±æ¥æ•æ‰æ‰€éœ€å±æ€§ï¼Œä½†è¿™ç§æ–¹æ³•è®­ç»ƒæˆæœ¬é«˜æ˜‚ä¸”å®¹æ˜“å‡ºç°æ¨¡å¼å´©æºƒã€‚æœ¬ç ”ç©¶æå‡ºFeynman Kacï¼ˆFKï¼‰è½¬å‘ï¼Œè¿™æ˜¯ä¸€ç§åœ¨æ¨ç†æ—¶é—´æ¡†æ¶å†…ä½¿ç”¨å¥–åŠ±å‡½æ•°æ§åˆ¶æ‰©æ•£æ¨¡å‹çš„æ¡†æ¶ã€‚FKè½¬å‘é€šè¿‡é‡‡æ ·å¤šä¸ªç›¸äº’ä½œç”¨çš„æ‰©æ•£è¿‡ç¨‹ï¼ˆç²’å­ï¼‰çš„ç³»ç»Ÿï¼Œå¹¶åœ¨ä¸­é—´æ­¥éª¤åŸºäºä½¿ç”¨åŠ¿èƒ½è®¡ç®—çš„åˆ†æ•°é‡æ–°é‡‡æ ·ç²’å­ã€‚åŠ¿èƒ½åˆ©ç”¨ä¸­é—´çŠ¶æ€çš„å¥–åŠ±å®šä¹‰ï¼Œé€‰æ‹©é«˜å€¼çš„ç²’å­å°†äº§ç”Ÿé«˜å¥–åŠ±æ ·æœ¬ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†ä¸åŒçš„åŠ¿èƒ½ã€ä¸­é—´å¥–åŠ±å’Œé‡‡æ ·å™¨é€‰æ‹©ã€‚åœ¨æ–‡æœ¬åˆ°å›¾åƒå’Œæ–‡æœ¬æ‰©æ•£æ¨¡å‹ä¸Šè¯„ä¼°FKè½¬å‘æ•ˆæœã€‚å¯¹äºåˆ©ç”¨äººç±»åå¥½å¥–åŠ±æ§åˆ¶æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ï¼Œå‘ç°FKè½¬å‘çš„0.8Bå‚æ•°æ¨¡å‹åœ¨æç¤ºä¿çœŸåº¦ä¸Šä¼˜äº2.6Bå‚æ•°å¾®è°ƒæ¨¡å‹ï¼Œé‡‡æ ·é€Ÿåº¦æ›´å¿«ä¸”æ— éœ€è®­ç»ƒã€‚å¯¹äºæ§åˆ¶æ–‡æœ¬è´¨é‡å’Œç‰¹å®šæ–‡æœ¬å±æ€§çš„æ–‡æœ¬æ‰©æ•£æ¨¡å‹ï¼ŒFKè½¬å‘ç”Ÿæˆäº†æ›´ä½å›°æƒ‘åº¦ã€æ›´ç¬¦åˆè¯­è¨€å­¦çš„è¾“å‡ºï¼Œå¹¶å®ç°äº†æ— æ¯’æ€§çš„æ¢¯åº¦æ§åˆ¶ã€‚ç»“æœè¡¨æ˜ï¼Œå³ä½¿åœ¨ç¦»çº¿å¥–åŠ±ä¸‹ï¼Œæ‰©æ•£æ¨¡å‹çš„æ¨ç†æ—¶é—´å°ºåº¦å’Œè½¬å‘ä¹Ÿèƒ½æ˜¾è‘—æé«˜æ ·æœ¬è´¨é‡å’Œå¯æ§æ€§ã€‚ç›¸å…³ä»£ç å¯è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/zacharyhorvitz/Fk-Diffusion-Steering">https://github.com/zacharyhorvitz/Fk-Diffusion-Steering</a>ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹å¹¿æ³›åº”ç”¨äºå¤šä¸ªé¢†åŸŸï¼Œç”Ÿæˆå…·æœ‰ç”¨æˆ·æŒ‡å®šå±æ€§çš„æ ·æœ¬æ˜¯å½“å‰çš„æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¾®è°ƒæ¨¡å‹ä»¥æœ€å¤§åŒ–å¥–åŠ±ï¼Œä½†è¿™ç§æ–¹æ³•æˆæœ¬é«˜æ˜‚ä¸”æ˜“é™·å…¥æ¨¡å¼å´©æºƒã€‚</li>
<li>æå‡ºFeynman Kacï¼ˆFKï¼‰è½¬å‘ï¼Œä¸€ç§åœ¨æ¨ç†æ—¶é—´ä½¿ç”¨å¥–åŠ±å‡½æ•°æ§åˆ¶æ‰©æ•£æ¨¡å‹çš„æ¡†æ¶ã€‚</li>
<li>FKè½¬å‘é€šè¿‡é‡‡æ ·å¤šä¸ªç›¸äº’ä½œç”¨çš„æ‰©æ•£è¿‡ç¨‹ï¼ˆç²’å­ï¼‰å¹¶åœ¨ä¸­é—´æ­¥éª¤é‡æ–°é‡‡æ ·æ¥å®ç°æ§åˆ¶ã€‚</li>
<li>åˆ©ç”¨ä¸åŒçš„åŠ¿èƒ½å’Œå¥–åŠ±å‡½æ•°é€‰æ‹©è¿›è¡Œè¯•éªŒï¼Œä»¥ä¼˜åŒ–ç”Ÿæˆæ ·æœ¬çš„è´¨é‡ã€‚</li>
<li>åœ¨æ–‡æœ¬åˆ°å›¾åƒå’Œæ–‡æœ¬æ‰©æ•£æ¨¡å‹ä¸Šè¯„ä¼°FKè½¬å‘ï¼Œæ•ˆæœæ˜¾è‘—ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.06848">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-18\./crop_Diffusion Models/2501.06848v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-18\./crop_Diffusion Models/2501.06848v3/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-18\./crop_Diffusion Models/2501.06848v3/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-18\./crop_Diffusion Models/2501.06848v3/page_3_0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="CDDIP-Constrained-Diffusion-Driven-Deep-Image-Prior-for-Seismic-Image-Reconstruction"><a href="#CDDIP-Constrained-Diffusion-Driven-Deep-Image-Prior-for-Seismic-Image-Reconstruction" class="headerlink" title="CDDIP: Constrained Diffusion-Driven Deep Image Prior for Seismic Image   Reconstruction"></a>CDDIP: Constrained Diffusion-Driven Deep Image Prior for Seismic Image   Reconstruction</h2><p><strong>Authors:Paul Goyes-PeÃ±afiel, Ulugbek Kamilov, Henry Arguello</strong></p>
<p>Seismic data frequently exhibits missing traces, substantially affecting subsequent seismic processing and interpretation. Deep learning-based approaches have demonstrated significant advancements in reconstructing irregularly missing seismic data through supervised and unsupervised methods. Nonetheless, substantial challenges remain, such as generalization capacity and computation time cost during the inference. Our work introduces a reconstruction method that uses a pre-trained generative diffusion model for image synthesis and incorporates Deep Image Prior to enforce data consistency when reconstructing missing traces in seismic data. The proposed method has demonstrated strong robustness and high reconstruction capability of post-stack and pre-stack data with different levels of structural complexity, even in field and synthetic scenarios where test data were outside the training domain. This indicates that our method can handle the high geological variability of different exploration targets. Additionally, compared to other state-of-the-art seismic reconstruction methods using diffusion models. During inference, our approach reduces the number of sampling timesteps by up to 4x. </p>
<blockquote>
<p>åœ°éœ‡æ•°æ®ç»å¸¸å­˜åœ¨ç¼ºå¤±çš„è½¨è¿¹ï¼Œè¿™ä¼šå¯¹åç»­çš„åœ°éœ‡å¤„ç†å’Œè§£é‡Šäº§ç”Ÿé‡å¤§å½±å“ã€‚åŸºäºæ·±åº¦å­¦ä¹ çš„æ–¹æ³•åœ¨é€šè¿‡æœ‰ç›‘ç£å’Œæ— ç›‘ç£æ–¹æ³•é‡å»ºä¸è§„åˆ™ç¼ºå¤±çš„åœ°éœ‡æ•°æ®æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ã€‚ç„¶è€Œï¼Œä»ç„¶å­˜åœ¨é‡å¤§æŒ‘æˆ˜ï¼Œä¾‹å¦‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­çš„æ³›åŒ–èƒ½åŠ›å’Œè®¡ç®—æ—¶é—´æˆæœ¬ã€‚æˆ‘ä»¬çš„å·¥ä½œå¼•å…¥äº†ä¸€ç§é‡å»ºæ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä½¿ç”¨é¢„è®­ç»ƒçš„ç”Ÿæˆæ‰©æ•£æ¨¡å‹è¿›è¡Œå›¾åƒåˆæˆï¼Œå¹¶ç»“åˆæ·±åº¦å›¾åƒå…ˆéªŒåœ¨é‡å»ºåœ°éœ‡æ•°æ®ä¸­ç¼ºå¤±è½¨è¿¹æ—¶å¼ºåˆ¶æ•°æ®ä¸€è‡´æ€§ã€‚æ‰€æå‡ºçš„æ–¹æ³•è¡¨ç°å‡ºå¼ºå¤§çš„ç¨³å¥æ€§å’Œå¯¹ä¸åŒç»“æ„å¤æ‚æ€§çº§åˆ«çš„å åŠ åæ•°æ®å’Œå åŠ å‰æ•°æ®çš„é«˜é‡å»ºèƒ½åŠ›ï¼Œå³ä½¿åœ¨æµ‹è¯•æ•°æ®è¶…å‡ºè®­ç»ƒé¢†åŸŸçš„ç°åœºå’Œåˆæˆåœºæ™¯ä¸­äº¦æ˜¯å¦‚æ­¤ã€‚è¿™è¡¨æ˜æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥å¤„ç†ä¸åŒå‹˜æ¢ç›®æ ‡çš„é«˜åœ°è´¨å˜å¼‚æ€§ã€‚æ­¤å¤–ï¼Œä¸å…¶ä»–ä½¿ç”¨æ‰©æ•£æ¨¡å‹çš„æœ€æ–°åœ°éœ‡é‡å»ºæ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ¨ç†è¿‡ç¨‹ä¸­å‡å°‘äº†é«˜è¾¾4å€çš„é‡‡æ ·æ—¶é—´æ­¥æ•°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.17402v2">PDF</a> 5 pages, 4 figures, 3 tables. Submitted to geoscience and remote   sensing letters</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºé¢„è®­ç»ƒç”Ÿæˆæ‰©æ•£æ¨¡å‹çš„é‡å»ºæ–¹æ³•ï¼Œç”¨äºåˆæˆå›¾åƒå¹¶é‡å»ºåœ°éœ‡æ•°æ®ä¸­ç¼ºå¤±çš„è½¨è¿¹ã€‚è¯¥æ–¹æ³•ç»“åˆäº†æ·±åº¦å›¾åƒå…ˆéªŒæŠ€æœ¯ï¼Œä»¥ç¡®ä¿åœ¨é‡å»ºè¿‡ç¨‹ä¸­æ•°æ®çš„ä¸€è‡´æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åº”å¯¹ä¸åŒç»“æ„å¤æ‚åº¦çš„åå †æ ˆå’Œé¢„å †æ ˆæ•°æ®æ—¶è¡¨ç°å‡ºå¼ºå¤§çš„ç¨³å¥æ€§å’Œé«˜é‡å»ºèƒ½åŠ›ï¼Œå³ä½¿åœ¨è¶…å‡ºè®­ç»ƒåŸŸçš„åœºæ™¯ä¸‹ä¹Ÿèƒ½å¤„ç†å„ç§å‹˜æ¢ç›®æ ‡çš„é«˜åœ°è´¨å˜å¼‚æ€§ã€‚ç›¸è¾ƒäºå…¶ä»–ä½¿ç”¨æ‰©æ•£æ¨¡å‹çš„å…ˆè¿›åœ°éœ‡é‡å»ºæ–¹æ³•ï¼Œæ­¤æ–¹æ³•åœ¨æ¨ç†é˜¶æ®µå‡å°‘äº†é«˜è¾¾4å€çš„é‡‡æ ·æ—¶é—´æ­¥æ•°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åœ°éœ‡æ•°æ®ä¸­çš„ç¼ºå¤±è½¨è¿¹å¯¹åç»­çš„åœ°éœ‡å¤„ç†å’Œè§£é‡Šäº§ç”Ÿäº†å½±å“ã€‚</li>
<li>æ·±åº¦å­¦ä¹ åœ¨é‡å»ºä¸è§„åˆ™ç¼ºå¤±çš„åœ°éœ‡æ•°æ®æ–¹é¢å·²æ˜¾ç¤ºå‡ºæ˜¾è‘—è¿›å±•ã€‚</li>
<li>æœ¬æ–‡å¼•å…¥äº†ä¸€ç§åŸºäºé¢„è®­ç»ƒç”Ÿæˆæ‰©æ•£æ¨¡å‹çš„é‡å»ºæ–¹æ³•ï¼Œç”¨äºåˆæˆå›¾åƒã€‚</li>
<li>ç»“åˆæ·±åº¦å›¾åƒå…ˆéªŒæŠ€æœ¯ï¼Œè¯¥æ–¹æ³•åœ¨é‡å»ºç¼ºå¤±è½¨è¿¹æ—¶ä¿è¯äº†æ•°æ®çš„ä¸€è‡´æ€§ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨åº”å¯¹ä¸åŒç»“æ„å¤æ‚åº¦çš„åœ°éœ‡æ•°æ®ä»¥åŠä¸åŒåœ°è´¨ç¯å¢ƒä¸‹è¡¨ç°å‡ºå¼ºå¤§çš„ç¨³å¥æ€§å’Œé«˜é‡å»ºèƒ½åŠ›ã€‚</li>
<li>ä¸å…¶ä»–ä½¿ç”¨æ‰©æ•£æ¨¡å‹çš„å…ˆè¿›åœ°éœ‡é‡å»ºæ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨æ¨ç†é˜¶æ®µçš„é‡‡æ ·æ—¶é—´æ­¥æ•°å‡å°‘äº†é«˜è¾¾4å€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.17402">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-18\./crop_Diffusion Models/2407.17402v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-18\./crop_Diffusion Models/2407.17402v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-18\./crop_Diffusion Models/2407.17402v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-18\./crop_Diffusion Models/2407.17402v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-18\./crop_Diffusion Models/2407.17402v2/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-18\./crop_Diffusion Models/2407.17402v2/page_4_0.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Zero-shot-Video-Restoration-and-Enhancement-Using-Pre-Trained-Image-Diffusion-Model"><a href="#Zero-shot-Video-Restoration-and-Enhancement-Using-Pre-Trained-Image-Diffusion-Model" class="headerlink" title="Zero-shot Video Restoration and Enhancement Using Pre-Trained Image   Diffusion Model"></a>Zero-shot Video Restoration and Enhancement Using Pre-Trained Image   Diffusion Model</h2><p><strong>Authors:Cong Cao, Huanjing Yue, Xin Liu, Jingyu Yang</strong></p>
<p>Diffusion-based zero-shot image restoration and enhancement models have achieved great success in various tasks of image restoration and enhancement. However, directly applying them to video restoration and enhancement results in severe temporal flickering artifacts. In this paper, we propose the first framework for zero-shot video restoration and enhancement based on the pre-trained image diffusion model. By replacing the spatial self-attention layer with the proposed short-long-range (SLR) temporal attention layer, the pre-trained image diffusion model can take advantage of the temporal correlation between frames. We further propose temporal consistency guidance, spatial-temporal noise sharing, and an early stopping sampling strategy to improve temporally consistent sampling. Our method is a plug-and-play module that can be inserted into any diffusion-based image restoration or enhancement methods to further improve their performance. Experimental results demonstrate the superiority of our proposed method. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/cao-cong/ZVRD">https://github.com/cao-cong/ZVRD</a>. </p>
<blockquote>
<p>åŸºäºæ‰©æ•£çš„é›¶æ ·æœ¬å›¾åƒä¿®å¤å’Œå¢å¼ºæ¨¡å‹åœ¨å›¾åƒä¿®å¤å’Œå¢å¼ºçš„å„ç§ä»»åŠ¡ä¸­å–å¾—äº†å·¨å¤§çš„æˆåŠŸã€‚ç„¶è€Œï¼Œå°†å…¶ç›´æ¥åº”ç”¨äºè§†é¢‘ä¿®å¤å’Œå¢å¼ºä¼šå¯¼è‡´ä¸¥é‡çš„æ—¶ç©ºé—ªçƒä¼ªå½±ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºé¢„è®­ç»ƒå›¾åƒæ‰©æ•£æ¨¡å‹çš„é›¶æ ·æœ¬è§†é¢‘ä¿®å¤å’Œå¢å¼ºçš„ç¬¬ä¸€ä¸ªæ¡†æ¶ã€‚é€šè¿‡ç”¨æ‰€æå‡ºçš„çŸ­é•¿ç¨‹ï¼ˆSLRï¼‰æ—¶é—´æ³¨æ„åŠ›å±‚æ›¿æ¢ç©ºé—´è‡ªæ³¨æ„åŠ›å±‚ï¼Œé¢„è®­ç»ƒçš„å›¾åƒæ‰©æ•£æ¨¡å‹å¯ä»¥åˆ©ç”¨å¸§ä¹‹é—´çš„æ—¶é—´ç›¸å…³æ€§ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†æ—¶é—´ä¸€è‡´æ€§æŒ‡å¯¼ã€æ—¶ç©ºå™ªå£°å…±äº«å’Œæ—©æœŸåœæ­¢é‡‡æ ·ç­–ç•¥ï¼Œä»¥æ”¹å–„æ—¶é—´ä¸€è‡´æ€§çš„é‡‡æ ·ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ˜¯ä¸€ä¸ªå³æ’å³ç”¨çš„æ¨¡å—ï¼Œå¯ä»¥æ’å…¥åˆ°ä»»ä½•åŸºäºæ‰©æ•£çš„å›¾åƒä¿®å¤æˆ–å¢å¼ºæ–¹æ³•ä¸­ï¼Œä»¥è¿›ä¸€æ­¥æé«˜å…¶æ€§èƒ½ã€‚å®éªŒç»“æœè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•ä¼˜è¶Šæ€§ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/cao-cong/ZVRD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/cao-cong/ZVRDæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.01960v2">PDF</a> Accepted by AAAI 2025</p>
<p><strong>Summary</strong><br>æ‰©æ•£æ¨¡å‹åœ¨é›¶æ ·æœ¬å›¾åƒä¿®å¤å’Œå¢å¼ºä»»åŠ¡ä¸­å–å¾—äº†å·¨å¤§æˆåŠŸï¼Œä½†ç›´æ¥åº”ç”¨äºè§†é¢‘ä¿®å¤å’Œå¢å¼ºä¼šå¯¼è‡´ä¸¥é‡çš„æ—¶åºé—ªçƒä¼ªå½±ã€‚æœ¬æ–‡é¦–æ¬¡æå‡ºåŸºäºé¢„è®­ç»ƒå›¾åƒæ‰©æ•£æ¨¡å‹çš„é›¶æ ·æœ¬è§†é¢‘ä¿®å¤å’Œå¢å¼ºæ¡†æ¶ã€‚é€šè¿‡ç”¨æ‰€æå‡ºçš„é•¿çŸ­ç¨‹æ—¶åºæ³¨æ„åŠ›å±‚æ›¿æ¢ç©ºé—´è‡ªæ³¨æ„åŠ›å±‚ï¼Œåˆ©ç”¨å¸§é—´çš„æ—¶åºç›¸å…³æ€§ï¼Œé¢„è®­ç»ƒçš„å›¾åƒæ‰©æ•£æ¨¡å‹èƒ½å¤Ÿæ”¹å–„è§†é¢‘ä¿®å¤å’Œå¢å¼ºçš„æ•ˆæœã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†æ—¶åºä¸€è‡´æ€§å¼•å¯¼ã€æ—¶ç©ºå™ªå£°å…±äº«å’Œæ—©æœŸåœæ­¢é‡‡æ ·ç­–ç•¥æ¥æé«˜æ—¶åºä¸€è‡´æ€§é‡‡æ ·æ•ˆæœã€‚æœ¬æ–‡æ–¹æ³•æ˜¯ä¸€ä¸ªå³æ’å³ç”¨æ¨¡å—ï¼Œå¯æ’å…¥ä»»ä½•æ‰©æ•£å¼å›¾åƒä¿®å¤æˆ–å¢å¼ºæ–¹æ³•ä¸­ï¼Œè¿›ä¸€æ­¥æé«˜å…¶æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæœ¬æ–‡æå‡ºçš„æ–¹æ³•å…·æœ‰ä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨é›¶æ ·æœ¬å›¾åƒä¿®å¤å’Œå¢å¼ºä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½†åº”ç”¨äºè§†é¢‘ä¿®å¤å’Œå¢å¼ºæ—¶å­˜åœ¨æ—¶åºé—ªçƒé—®é¢˜ã€‚</li>
<li>æœ¬æ–‡é¦–æ¬¡æå‡ºäº†åŸºäºé¢„è®­ç»ƒå›¾åƒæ‰©æ•£æ¨¡å‹çš„é›¶æ ·æœ¬è§†é¢‘ä¿®å¤å’Œå¢å¼ºæ¡†æ¶ã€‚</li>
<li>é€šè¿‡ç”¨é•¿çŸ­ç¨‹æ—¶åºæ³¨æ„åŠ›å±‚æ›¿æ¢ç©ºé—´è‡ªæ³¨æ„åŠ›å±‚ï¼Œåˆ©ç”¨å¸§é—´æ—¶åºç›¸å…³æ€§ã€‚</li>
<li>æå‡ºäº†æ—¶åºä¸€è‡´æ€§å¼•å¯¼ã€æ—¶ç©ºå™ªå£°å…±äº«å’Œæ—©æœŸåœæ­¢é‡‡æ ·ç­–ç•¥æ¥æé«˜æ—¶åºä¸€è‡´æ€§é‡‡æ ·ã€‚</li>
<li>æœ¬æ–‡æ–¹æ³•æ˜¯ä¸€ä¸ªé€šç”¨çš„æ¨¡å—ï¼Œå¯ä»¥æ–¹ä¾¿åœ°é›†æˆåˆ°ç°æœ‰çš„æ‰©æ•£æ¨¡å‹ä¸­ï¼Œæé«˜å›¾åƒä¿®å¤å’Œå¢å¼ºçš„æ€§èƒ½ã€‚</li>
<li>å®éªŒç»“æœè¯æ˜äº†è¯¥æ–¹æ³•åœ¨è§†é¢‘ä¿®å¤å’Œå¢å¼ºä»»åŠ¡ä¸Šçš„ä¼˜è¶Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.01960">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-18\./crop_Diffusion Models/2407.01960v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-18\./crop_Diffusion Models/2407.01960v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-18\./crop_Diffusion Models/2407.01960v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-18\./crop_Diffusion Models/2407.01960v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-18\./crop_Diffusion Models/2407.01960v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Simplified-and-Generalized-Masked-Diffusion-for-Discrete-Data"><a href="#Simplified-and-Generalized-Masked-Diffusion-for-Discrete-Data" class="headerlink" title="Simplified and Generalized Masked Diffusion for Discrete Data"></a>Simplified and Generalized Masked Diffusion for Discrete Data</h2><p><strong>Authors:Jiaxin Shi, Kehang Han, Zhe Wang, Arnaud Doucet, Michalis K. Titsias</strong></p>
<p>Masked (or absorbing) diffusion is actively explored as an alternative to autoregressive models for generative modeling of discrete data. However, existing work in this area has been hindered by unnecessarily complex model formulations and unclear relationships between different perspectives, leading to suboptimal parameterization, training objectives, and ad hoc adjustments to counteract these issues. In this work, we aim to provide a simple and general framework that unlocks the full potential of masked diffusion models. We show that the continuous-time variational objective of masked diffusion models is a simple weighted integral of cross-entropy losses. Our framework also enables training generalized masked diffusion models with state-dependent masking schedules. When evaluated by perplexity, our models trained on OpenWebText surpass prior diffusion language models at GPT-2 scale and demonstrate superior performance on 4 out of 5 zero-shot language modeling tasks. Furthermore, our models vastly outperform previous discrete diffusion models on pixel-level image modeling, achieving 2.75 (CIFAR-10) and 3.40 (ImageNet 64x64) bits per dimension that are better than autoregressive models of similar sizes. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/google-deepmind/md4">https://github.com/google-deepmind/md4</a>. </p>
<blockquote>
<p>é®è”½ï¼ˆæˆ–å¸æ”¶ï¼‰æ‰©æ•£ä½œä¸ºä¸€ç§ç¦»æ•£æ•°æ®ç”Ÿæˆæ¨¡å‹çš„è‡ªå›å½’æ¨¡å‹çš„æ›¿ä»£æ–¹æ¡ˆï¼Œæ­£åœ¨å¾—åˆ°ç§¯æçš„ç ”ç©¶ã€‚ç„¶è€Œï¼Œè¯¥é¢†åŸŸç°æœ‰çš„å·¥ä½œå—åˆ°äº†æ¨¡å‹å…¬å¼ä¸å¿…è¦åœ°å¤æ‚å’Œä¸åŒè§‚ç‚¹ä¹‹é—´å…³ç³»ä¸æ˜ç¡®çš„å½±å“ï¼Œå¯¼è‡´äº†å‚æ•°åŒ–ã€è®­ç»ƒç›®æ ‡å’Œä¸´æ—¶è°ƒæ•´æ•ˆæœä¸ä½³ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯æä¾›ä¸€ä¸ªç®€å•ä¸”é€šç”¨çš„æ¡†æ¶ï¼Œä»¥å……åˆ†å‘æŒ¥é®è”½æ‰©æ•£æ¨¡å‹çš„æ½œåŠ›ã€‚æˆ‘ä»¬è¯æ˜äº†é®è”½æ‰©æ•£æ¨¡å‹çš„è¿ç»­æ—¶é—´å˜åˆ†ç›®æ ‡æ˜¯äº¤å‰ç†µæŸå¤±çš„ç®€å•åŠ æƒç§¯åˆ†ã€‚æˆ‘ä»¬çš„æ¡†æ¶è¿˜å…è®¸ä½¿ç”¨çŠ¶æ€ä¾èµ–çš„é®è”½æ—¶é—´è¡¨æ¥è®­ç»ƒå¹¿ä¹‰é®è”½æ‰©æ•£æ¨¡å‹ã€‚ä»¥å›°æƒ‘åº¦è¯„ä¼°æ—¶ï¼Œæˆ‘ä»¬åœ¨OpenWebTextä¸Šè®­ç»ƒçš„æ¨¡å‹è¶…è¶Šäº†GPT-2è§„æ¨¡çš„å…ˆå‰æ‰©æ•£è¯­è¨€æ¨¡å‹ï¼Œå¹¶åœ¨äº”é¡¹é›¶é•œå¤´è¯­è¨€å»ºæ¨¡ä»»åŠ¡ä¸­çš„å››é¡¹ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨åƒç´ çº§å›¾åƒå»ºæ¨¡ä¸Šå¤§å¤§ä¼˜äºå…ˆå‰çš„ç¦»æ•£æ‰©æ•£æ¨¡å‹ï¼Œåœ¨CIFAR-10å’ŒImageNet 64x64ä¸Šå®ç°äº†æ¯ç»´åº¦åˆ†åˆ«ä¸º2.75æ¯”ç‰¹å’Œ3.40æ¯”ç‰¹çš„ç»“æœï¼Œä¼˜äºç±»ä¼¼è§„æ¨¡çš„è‡ªå›å½’æ¨¡å‹ã€‚æˆ‘ä»¬çš„ä»£ç ä½äº<a target="_blank" rel="noopener" href="https://github.com/google-deepmind/md4%E3%80%82">https://github.com/google-deepmind/md4ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.04329v4">PDF</a> NeurIPS 2024. Code is available at:   <a target="_blank" rel="noopener" href="https://github.com/google-deepmind/md4">https://github.com/google-deepmind/md4</a></p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æ¢ç´¢äº†æ©è†œæ‰©æ•£æ¨¡å‹ä½œä¸ºç”Ÿæˆç¦»æ•£æ•°æ®çš„æ›¿ä»£æ–¹æ¡ˆã€‚æ–‡ç« æå‡ºäº†ä¸€ç§ç®€å•é€šç”¨çš„æ©è†œæ‰©æ•£æ¨¡å‹æ¡†æ¶ï¼Œè§£å†³äº†ç°æœ‰ç ”ç©¶ä¸­æ¨¡å‹å…¬å¼å¤æ‚ã€ä¸åŒè§†è§’å…³ç³»ä¸æ˜ç¡®çš„é—®é¢˜ï¼Œå®ç°äº†å‚æ•°åŒ–ã€è®­ç»ƒç›®æ ‡çš„æœ€ä¼˜åŒ–ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡å±•ç¤ºäº†æ©è†œæ‰©æ•£æ¨¡å‹çš„è¿ç»­æ—¶é—´å˜åˆ†ç›®æ ‡ä¸äº¤å‰ç†µæŸå¤±çš„åŠ æƒç§¯åˆ†å…³ç³»ã€‚æ¨¡å‹èƒ½åœ¨OpenWebTextæ•°æ®é›†ä¸Šè¾¾åˆ°GPT-2è§„æ¨¡çš„æœ€ä½å›°æƒ‘åº¦ï¼Œå¹¶åœ¨é›¶æ ·æœ¬è¯­è¨€å»ºæ¨¡ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚åŒæ—¶ï¼Œåœ¨åƒç´ çº§å›¾åƒå»ºæ¨¡æ–¹é¢ï¼Œæ¨¡å‹è¡¨ç°ä¼˜äºå…ˆå‰çš„ç¦»æ•£æ‰©æ•£æ¨¡å‹ï¼Œå®ç°äº†CIFAR-10çš„2.75æ¯”ç‰¹å’ŒImageNet 64x64çš„3.4æ¯”ç‰¹æ¯ç»´åº¦çš„é«˜æ€§èƒ½è¡¨ç°ã€‚ä»£ç å·²å…¬å¼€äºGitHubã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ©è†œæ‰©æ•£æ¨¡å‹ä½œä¸ºä¸€ç§ç”Ÿæˆç¦»æ•£æ•°æ®çš„æ›¿ä»£æ–¹æ¡ˆæ­£å—åˆ°å…³æ³¨ã€‚</li>
<li>æå‡ºäº†ä¸€ç§ç®€å•é€šç”¨çš„æ©è†œæ‰©æ•£æ¨¡å‹æ¡†æ¶ï¼Œè§£å†³äº†ç°æœ‰ç ”ç©¶ä¸­æ¨¡å‹å¤æ‚å’Œå…³ç³»ä¸æ˜ç¡®çš„é—®é¢˜ã€‚</li>
<li>æ­ç¤ºäº†æ©è†œæ‰©æ•£æ¨¡å‹çš„è¿ç»­æ—¶é—´å˜åˆ†ç›®æ ‡ä¸äº¤å‰ç†µæŸå¤±çš„åŠ æƒç§¯åˆ†å…³ç³»ã€‚</li>
<li>æ¨¡å‹åœ¨GPT-2è§„æ¨¡çš„OpenWebTextæ•°æ®é›†ä¸Šå®ç°äº†æœ€ä½çš„å›°æƒ‘åº¦ã€‚</li>
<li>æ¨¡å‹åœ¨é›¶æ ·æœ¬è¯­è¨€å»ºæ¨¡ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨æŸäº›ç‰¹å®šä»»åŠ¡ä¸Šè¡¨ç°ä¼˜äºå…¶ä»–æ¨¡å‹ã€‚</li>
<li>æ¨¡å‹åœ¨åƒç´ çº§å›¾åƒå»ºæ¨¡æ–¹é¢æ˜¾è‘—ä¼˜äºå…ˆå‰çš„ç¦»æ•£æ‰©æ•£æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.04329">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-18\./crop_Diffusion Models/2406.04329v4/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-18\./crop_Diffusion Models/2406.04329v4/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-18\./crop_Diffusion Models/2406.04329v4/page_4_0.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Learning-Discrete-Concepts-in-Latent-Hierarchical-Models"><a href="#Learning-Discrete-Concepts-in-Latent-Hierarchical-Models" class="headerlink" title="Learning Discrete Concepts in Latent Hierarchical Models"></a>Learning Discrete Concepts in Latent Hierarchical Models</h2><p><strong>Authors:Lingjing Kong, Guangyi Chen, Biwei Huang, Eric P. Xing, Yuejie Chi, Kun Zhang</strong></p>
<p>Learning concepts from natural high-dimensional data (e.g., images) holds potential in building human-aligned and interpretable machine learning models. Despite its encouraging prospect, formalization and theoretical insights into this crucial task are still lacking. In this work, we formalize concepts as discrete latent causal variables that are related via a hierarchical causal model that encodes different abstraction levels of concepts embedded in high-dimensional data (e.g., a dog breed and its eye shapes in natural images). We formulate conditions to facilitate the identification of the proposed causal model, which reveals when learning such concepts from unsupervised data is possible. Our conditions permit complex causal hierarchical structures beyond latent trees and multi-level directed acyclic graphs in prior work and can handle high-dimensional, continuous observed variables, which is well-suited for unstructured data modalities such as images. We substantiate our theoretical claims with synthetic data experiments. Further, we discuss our theoryâ€™s implications for understanding the underlying mechanisms of latent diffusion models and provide corresponding empirical evidence for our theoretical insights. </p>
<blockquote>
<p>ä»è‡ªç„¶çš„é«˜ç»´æ•°æ®ï¼ˆä¾‹å¦‚å›¾åƒï¼‰ä¸­å­¦ä¹ æ¦‚å¿µï¼Œåœ¨æ„å»ºä¸äººç±»å¯¹é½å’Œå¯è§£é‡Šçš„æœºå™¨å­¦ä¹ æ¨¡å‹æ–¹é¢å…·æ½œåŠ›ã€‚å°½ç®¡å…¶å‰æ™¯é¼“èˆäººå¿ƒï¼Œä½†å…³äºè¿™ä¸€å…³é”®ä»»åŠ¡çš„å½¢å¼åŒ–å’Œç†è®ºæ´å¯Ÿä»ç„¶ç¼ºä¹ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å°†æ¦‚å¿µå½¢å¼åŒ–ä¸ºç¦»æ•£çš„æ½œåœ¨å› æœå˜é‡ï¼Œè¿™äº›å˜é‡é€šè¿‡å±‚æ¬¡åŒ–å› æœæ¨¡å‹ç›¸å…³è”ï¼Œè¯¥æ¨¡å‹å¯¹é«˜ç»´æ•°æ®ä¸­åµŒå…¥æ¦‚å¿µçš„ä¸åŒæŠ½è±¡å±‚æ¬¡è¿›è¡Œç¼–ç ï¼ˆä¾‹å¦‚ï¼Œè‡ªç„¶å›¾åƒä¸­çš„çŠ¬ç§åŠå…¶çœ¼ç›å½¢çŠ¶ï¼‰ã€‚æˆ‘ä»¬åˆ¶å®šæ¡ä»¶ï¼Œä»¥ä¿ƒè¿›æ‰€æå‡ºå› æœæ¨¡å‹çš„è¯†åˆ«ï¼Œæ­ç¤ºä»éç›‘ç£æ•°æ®ä¸­å­¦ä¹ æ­¤ç±»æ¦‚å¿µçš„å¯èƒ½æ€§ã€‚æˆ‘ä»¬çš„æ¡ä»¶å…è®¸è¶…å‡ºå…ˆå‰å·¥ä½œä¸­çš„æ½œåœ¨æ ‘å’Œå¤šçº§æœ‰å‘æ— ç¯å›¾çš„å¤æ‚å› æœå±‚æ¬¡ç»“æ„ï¼Œå¹¶èƒ½å¤Ÿå¤„ç†é«˜ç»´ã€è¿ç»­çš„è§‚æµ‹å˜é‡ï¼Œè¿™éå¸¸é€‚åˆå›¾åƒç­‰æ— ç»“æ„çš„æ•°æ®æ¨¡å¼ã€‚æˆ‘ä»¬é€šè¿‡åˆæˆæ•°æ®å®éªŒæ¥è¯å®æˆ‘ä»¬çš„ç†è®ºä¸»å¼ ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¨è®ºäº†æˆ‘ä»¬çš„ç†è®ºå¯¹ç†è§£æ½œåœ¨æ‰©æ•£æ¨¡å‹çš„å†…åœ¨æœºåˆ¶çš„å½±å“ï¼Œå¹¶ä¸ºæˆ‘ä»¬çš„ç†è®ºè§è§£æä¾›ç›¸åº”çš„å®è¯è¯æ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.00519v2">PDF</a> NeurIPS 2024</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨ä»è‡ªç„¶é«˜ç»´æ•°æ®ï¼ˆå¦‚å›¾åƒï¼‰ä¸­å­¦ä¹ æ¦‚å¿µåœ¨æ„å»ºäººç±»å¯¹é½å’Œå¯è§£é‡Šçš„æœºå™¨å­¦ä¹ æ¨¡å‹ä¸­çš„æ½œåŠ›ã€‚æ–‡ç« å°†æ¦‚å¿µå½¢å¼åŒ–ä¸ºç¦»æ•£æ½œåœ¨å› æœå˜é‡ï¼Œå¹¶é€šè¿‡å±‚æ¬¡å› æœæ¨¡å‹å…³è”ï¼Œè¯¥æ¨¡å‹ç¼–ç åµŒå…¥åœ¨é«˜ç»´æ•°æ®ä¸­çš„ä¸åŒæŠ½è±¡å±‚æ¬¡çš„æ¦‚å¿µã€‚æ–‡ç« åˆ¶å®šäº†æ¡ä»¶ä»¥ä¿ƒè¿›æ‰€æå‡ºçš„å› æœæ¨¡å‹çš„è¯†åˆ«ï¼Œæ­ç¤ºä»éç›‘ç£æ•°æ®ä¸­å­¦ä¹ æ­¤ç±»æ¦‚å¿µçš„å¯èƒ½æ€§ã€‚è¯¥æ¡ä»¶èƒ½å¤Ÿå¤„ç†è¶…è¶Šå…ˆå‰å·¥ä½œä¸­æ½œåœ¨æ ‘å’Œå¤šçº§æœ‰å‘æ— ç¯å›¾çš„å¤æ‚å› æœå±‚æ¬¡ç»“æ„ï¼Œä»¥åŠé€‚åˆå›¾åƒç­‰éç»“æ„åŒ–æ•°æ®çš„é«˜ç»´è¿ç»­è§‚æµ‹å˜é‡ã€‚æ–‡ç« é€šè¿‡åˆæˆæ•°æ®å®éªŒè¯å®äº†ç†è®ºä¸»å¼ ï¼Œå¹¶æ¢è®¨äº†ç†è®ºå¯¹æ½œåœ¨æ‰©æ•£æ¨¡å‹çš„å†…åœ¨æœºåˆ¶çš„å½±å“ï¼Œä¸ºç†è®ºè§è§£æä¾›äº†ç›¸åº”çš„å®è¯è¯æ®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡ç« å°†æ¦‚å¿µå½¢å¼åŒ–ä¸ºç¦»æ•£æ½œåœ¨å› æœå˜é‡ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªå±‚æ¬¡åŒ–çš„å› æœæ¨¡å‹æ¥è¡¨ç¤ºé«˜ç»´æ•°æ®ä¸­çš„ä¸åŒæ¦‚å¿µé—´çš„å…³ç³»ã€‚</li>
<li>æ–‡ç« åˆ¶å®šäº†æ¡ä»¶ä»¥ä¿ƒè¿›å› æœæ¨¡å‹çš„è¯†åˆ«ï¼Œå¹¶æ­ç¤ºäº†ä»éç›‘ç£æ•°æ®ä¸­å­¦ä¹ æ¦‚å¿µçš„å¯èƒ½æ€§ã€‚</li>
<li>è¯¥ç†è®ºæ¡†æ¶èƒ½å¤Ÿå¤„ç†å¤æ‚çš„å› æœå±‚æ¬¡ç»“æ„ï¼ŒåŒ…æ‹¬é«˜ç»´è¿ç»­è§‚æµ‹å˜é‡ï¼Œé€‚ç”¨äºéç»“æ„åŒ–æ•°æ®ï¼Œå¦‚å›¾åƒã€‚</li>
<li>æ–‡ç« é€šè¿‡åˆæˆæ•°æ®å®éªŒè¯å®äº†ç†è®ºä¸»å¼ ã€‚</li>
<li>æ–‡ç« è®¨è®ºäº†ç†è®ºå¯¹ç†è§£æ½œåœ¨æ‰©æ•£æ¨¡å‹çš„å†…åœ¨æœºåˆ¶çš„å½±å“ï¼Œå¹¶ä¸ºç†è®ºè§è§£æä¾›äº†å®è¯è¯æ®ã€‚</li>
<li>å­¦ä¹ ä»è‡ªç„¶é«˜ç»´æ•°æ®ä¸­æå–æ¦‚å¿µå¯¹äºæ„å»ºä¸äººç±»å¯¹é½å’Œå¯è§£é‡Šçš„æœºå™¨å­¦ä¹ æ¨¡å‹å…·æœ‰æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.00519">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-18\./crop_Diffusion Models/2406.00519v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-18\./crop_Diffusion Models/2406.00519v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-18\./crop_Diffusion Models/2406.00519v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="DiffMesh-A-Motion-aware-Diffusion-Framework-for-Human-Mesh-Recovery-from-Videos"><a href="#DiffMesh-A-Motion-aware-Diffusion-Framework-for-Human-Mesh-Recovery-from-Videos" class="headerlink" title="DiffMesh: A Motion-aware Diffusion Framework for Human Mesh Recovery   from Videos"></a>DiffMesh: A Motion-aware Diffusion Framework for Human Mesh Recovery   from Videos</h2><p><strong>Authors:Ce Zheng, Xianpeng Liu, Qucheng Peng, Tianfu Wu, Pu Wang, Chen Chen</strong></p>
<p>Human mesh recovery (HMR) provides rich human body information for various real-world applications. While image-based HMR methods have achieved impressive results, they often struggle to recover humans in dynamic scenarios, leading to temporal inconsistencies and non-smooth 3D motion predictions due to the absence of human motion. In contrast, video-based approaches leverage temporal information to mitigate this issue. In this paper, we present DiffMesh, an innovative motion-aware Diffusion-like framework for video-based HMR. DiffMesh establishes a bridge between diffusion models and human motion, efficiently generating accurate and smooth output mesh sequences by incorporating human motion within the forward process and reverse process in the diffusion model. Extensive experiments are conducted on the widely used datasets (Human3.6M \cite{h36m_pami} and 3DPW \cite{pw3d2018}), which demonstrate the effectiveness and efficiency of our DiffMesh. Visual comparisons in real-world scenarios further highlight DiffMeshâ€™s suitability for practical applications. </p>
<blockquote>
<p>äººä½“ç½‘æ ¼æ¢å¤ï¼ˆHMRï¼‰ä¸ºå„ç§ç°å®ä¸–ç•Œåº”ç”¨æä¾›äº†ä¸°å¯Œçš„äººä½“ä¿¡æ¯ã€‚è™½ç„¶åŸºäºå›¾åƒçš„HMRæ–¹æ³•å·²ç»å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„ç»“æœï¼Œä½†åœ¨åŠ¨æ€åœºæ™¯ä¸­æ¢å¤äººä½“æ—¶å¸¸å¸¸é‡åˆ°å›°éš¾ï¼Œç”±äºç¼ºä¹äººä½“è¿åŠ¨å¯¼è‡´æ—¶é—´ä¸ä¸€è‡´å’Œéå¹³æ»‘çš„3Dè¿åŠ¨é¢„æµ‹ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒåŸºäºè§†é¢‘çš„æ–¹æ³•åˆ©ç”¨æ—¶é—´ä¿¡æ¯æ¥ç¼“è§£è¿™ä¸ªé—®é¢˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†DiffMeshï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ›æ–°çš„è¿åŠ¨æ„ŸçŸ¥æ‰©æ•£å¼æ¡†æ¶ï¼Œç”¨äºåŸºäºè§†é¢‘çš„HMRã€‚DiffMeshåœ¨æ‰©æ•£æ¨¡å‹å’Œäººä½“è¿åŠ¨ä¹‹é—´å»ºç«‹äº†æ¡¥æ¢ï¼Œé€šè¿‡å°†åœ¨æ‰©æ•£æ¨¡å‹çš„å‰å‘è¿‡ç¨‹å’Œåå‘è¿‡ç¨‹ä¸­èå…¥äººä½“è¿åŠ¨ï¼Œæœ‰æ•ˆåœ°ç”Ÿæˆå‡†ç¡®å’Œå¹³æ»‘çš„è¾“å‡ºç½‘æ ¼åºåˆ—ã€‚åœ¨å¹¿æ³›ä½¿ç”¨çš„æ•°æ®é›†ï¼ˆHuman3.6M \cite{h36m_pami}å’Œ3DPW \cite{pw3d2018}ï¼‰ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒï¼Œè¯æ˜äº†æˆ‘ä»¬çš„DiffMeshçš„æœ‰æ•ˆæ€§å’Œæ•ˆç‡ã€‚åœ¨çœŸå®åœºæ™¯ä¸­çš„è§†è§‰å¯¹æ¯”è¿›ä¸€æ­¥çªå‡ºäº†DiffMeshåœ¨å®é™…åº”ç”¨ä¸­çš„é€‚ç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2303.13397v6">PDF</a> WACV 2025</p>
<p><strong>Summary</strong><br>äººç±»ç½‘æ ¼æ¢å¤ï¼ˆHMRï¼‰ä¸ºå„ç§å®é™…åº”ç”¨æä¾›äº†ä¸°å¯Œçš„äººä½“ä¿¡æ¯ã€‚å°½ç®¡åŸºäºå›¾åƒçš„æ–¹æ³•å·²å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„ç»“æœï¼Œä½†å®ƒä»¬é€šå¸¸åœ¨åŠ¨æ€åœºæ™¯ä¸­éš¾ä»¥æ¢å¤äººä½“ä¿¡æ¯ï¼Œå¯¼è‡´æ—¶é—´ä¸ä¸€è‡´å’Œéå¹³æ»‘çš„3Dè¿åŠ¨é¢„æµ‹ã€‚ç›¸åï¼ŒåŸºäºè§†é¢‘çš„æ–¹æ³•åˆ©ç”¨æ—¶é—´ä¿¡æ¯æ¥ç¼“è§£è¿™ä¸ªé—®é¢˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åˆ›æ–°çš„è¿åŠ¨æ„ŸçŸ¥æ‰©æ•£æ¡†æ¶DiffMeshï¼Œç”¨äºåŸºäºè§†é¢‘çš„HMRã€‚DiffMeshåœ¨æ‰©æ•£æ¨¡å‹ä¸­å»ºç«‹äº†æ‰©æ•£æ¨¡å‹ä¸äººç±»è¿åŠ¨çš„æ¡¥æ¢ï¼Œé€šè¿‡åœ¨å‰å‘è¿‡ç¨‹å’Œåå‘è¿‡ç¨‹ä¸­èå…¥äººç±»è¿åŠ¨ï¼Œæœ‰æ•ˆåœ°ç”Ÿæˆå‡†ç¡®å’Œå¹³æ»‘çš„è¾“å‡ºç½‘æ ¼åºåˆ—ã€‚åœ¨å¹¿æ³›ä½¿ç”¨æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¯æ˜äº†DiffMeshçš„æœ‰æ•ˆæ€§å’Œæ•ˆç‡ã€‚åœ¨çœŸå®åœºæ™¯ä¸­çš„è§†è§‰å¯¹æ¯”è¿›ä¸€æ­¥çªå‡ºäº†DiffMeshåœ¨å®é™…åº”ç”¨ä¸­çš„é€‚ç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>HMRä¸ºå¤šç§å®é™…åº”ç”¨æä¾›äº†ä¸°å¯Œçš„äººä½“ä¿¡æ¯ã€‚</li>
<li>åŸºäºå›¾åƒçš„æ–¹æ³•åœ¨åŠ¨æ€åœºæ™¯ä¸­æ¢å¤äººä½“ä¿¡æ¯å­˜åœ¨æŒ‘æˆ˜ï¼Œå¯¼è‡´æ—¶é—´ä¸ä¸€è‡´å’Œéå¹³æ»‘çš„3Dè¿åŠ¨é¢„æµ‹ã€‚</li>
<li>åŸºäºè§†é¢‘çš„æ–¹æ³•åˆ©ç”¨æ—¶é—´ä¿¡æ¯ç¼“è§£ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>DiffMeshæ˜¯åˆ›æ–°çš„è¿åŠ¨æ„ŸçŸ¥æ‰©æ•£æ¡†æ¶ï¼Œç”¨äºåŸºäºè§†é¢‘çš„HMRã€‚</li>
<li>DiffMeshåœ¨æ‰©æ•£æ¨¡å‹ä¸­èå…¥äººç±»è¿åŠ¨ï¼Œç”Ÿæˆå‡†ç¡®ã€å¹³æ»‘çš„è¾“å‡ºç½‘æ ¼åºåˆ—ã€‚</li>
<li>åœ¨å¹¿æ³›ä½¿ç”¨æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜äº†DiffMeshçš„æœ‰æ•ˆæ€§å’Œæ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2303.13397">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-18\./crop_Diffusion Models/2303.13397v6/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-18\./crop_Diffusion Models/2303.13397v6/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-18\./crop_Diffusion Models/2303.13397v6/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-18\./crop_Diffusion Models/2303.13397v6/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-18\./crop_Diffusion Models/2303.13397v6/page_5_0.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-18/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-18/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-18/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-6ad5cbba534ecd3ac040c1cbe6809bd1.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-18  SRE-Conv Symmetric Rotation Equivariant Convolution for Biomedical   Image Classification
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-18
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-18/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-b8ba69eab24fadcf9a82f4a069c47087.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-18  Normal-NeRF Ambiguity-Robust Normal Estimation for Highly Reflective   Scenes
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-18
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">26384.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
