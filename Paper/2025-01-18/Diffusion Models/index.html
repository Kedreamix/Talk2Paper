<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-01-18  SynthLight Portrait Relighting with Diffusion Model by Learning to   Re-render Synthetic Faces">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-fb67e12998d31bb3ee872b18aaab809f.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-01-18
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-01-18
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    13.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    54 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-01-18-更新"><a href="#2025-01-18-更新" class="headerlink" title="2025-01-18 更新"></a>2025-01-18 更新</h1><h2 id="SynthLight-Portrait-Relighting-with-Diffusion-Model-by-Learning-to-Re-render-Synthetic-Faces"><a href="#SynthLight-Portrait-Relighting-with-Diffusion-Model-by-Learning-to-Re-render-Synthetic-Faces" class="headerlink" title="SynthLight: Portrait Relighting with Diffusion Model by Learning to   Re-render Synthetic Faces"></a>SynthLight: Portrait Relighting with Diffusion Model by Learning to   Re-render Synthetic Faces</h2><p><strong>Authors:Sumit Chaturvedi, Mengwei Ren, Yannick Hold-Geoffroy, Jingyuan Liu, Julie Dorsey, Zhixin Shu</strong></p>
<p>We introduce SynthLight, a diffusion model for portrait relighting. Our approach frames image relighting as a re-rendering problem, where pixels are transformed in response to changes in environmental lighting conditions. Using a physically-based rendering engine, we synthesize a dataset to simulate this lighting-conditioned transformation with 3D head assets under varying lighting. We propose two training and inference strategies to bridge the gap between the synthetic and real image domains: (1) multi-task training that takes advantage of real human portraits without lighting labels; (2) an inference time diffusion sampling procedure based on classifier-free guidance that leverages the input portrait to better preserve details. Our method generalizes to diverse real photographs and produces realistic illumination effects, including specular highlights and cast shadows, while preserving the subject’s identity. Our quantitative experiments on Light Stage data demonstrate results comparable to state-of-the-art relighting methods. Our qualitative results on in-the-wild images showcase rich and unprecedented illumination effects. Project Page: \url{<a target="_blank" rel="noopener" href="https://vrroom.github.io/synthlight/%7D">https://vrroom.github.io/synthlight/}</a> </p>
<blockquote>
<p>我们介绍了SynthLight，这是一个用于肖像重新打光的扩散模型。我们的方法将图像重新打光视为重新渲染问题，像素会随环境照明条件的改变而发生变化。我们使用基于物理的渲染引擎，合成了一个数据集，以模拟在不同照明条件下使用3D头部资产进行这种光照调整转换。我们提出了两种训练和推理策略，以缩小合成图像和真实图像领域之间的差距：（1）多任务训练，利用没有光照标签的真实人像；（2）基于分类器引导的自由采样过程的推理时间扩散，利用输入的人像来更好地保留细节。我们的方法适用于各种真实照片，产生逼真的照明效果，包括高光和阴影，同时保持主体的身份。我们在Light Stage数据上的定量实验表明，我们的结果与最先进的重新打光方法相当。我们在野外图像上的定性结果展示了丰富且前所未有的照明效果。项目页面：<a target="_blank" rel="noopener" href="https://vrroom.github.io/synthlight/">https://vrroom.github.io/synthlight/</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.09756v1">PDF</a> 27 pages, 25 figures, Project Page   <a target="_blank" rel="noopener" href="https://vrroom.github.io/synthlight/">https://vrroom.github.io/synthlight/</a></p>
<p><strong>Summary</strong></p>
<p>介绍了一种名为SynthLight的肖像重照明扩散模型。该方法将图像重照明视为重渲染问题，通过物理渲染引擎合成数据集，模拟不同光照条件下的像素变化。提出两种训练和推理策略，以缩小合成图像和真实图像之间的差距：一是利用无光照标签的真实人像进行多任务训练；二是在推理时间采用基于无分类器引导扩散采样程序，利用输入肖像更好地保留细节。该方法可广泛应用于各种真实照片，产生逼真的照明效果，包括高光和阴影，同时保持主体身份。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SynthLight是一种用于肖像重光的扩散模型。</li>
<li>该方法将图像重照明视为重渲染问题，并模拟光照条件下的像素变化。</li>
<li>使用物理渲染引擎合成数据集。</li>
<li>提出两种训练和推理策略来缩小合成图像和真实图像之间的差距。</li>
<li>多任务训练利用无光照标签的真实人像。</li>
<li>推理时间采用基于无分类器引导扩散采样程序，利用输入肖像保留细节。</li>
<li>该方法能广泛应用于各种真实照片，产生逼真的照明效果，并保持主体身份。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.09756">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-7d2af9c02ee2bae2a7fe82145276367f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-594d45b8a1fbd696f956e03aa6b99d5e.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-18\./crop_Diffusion Models/2501.09756v1/page_3_0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ea70621a8cb2f4e7582a651aaee323e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f01bfe90a02ec7231d88c183672e6267.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="PATCHEDSERVE-A-Patch-Management-Framework-for-SLO-Optimized-Hybrid-Resolution-Diffusion-Serving"><a href="#PATCHEDSERVE-A-Patch-Management-Framework-for-SLO-Optimized-Hybrid-Resolution-Diffusion-Serving" class="headerlink" title="PATCHEDSERVE: A Patch Management Framework for SLO-Optimized Hybrid   Resolution Diffusion Serving"></a>PATCHEDSERVE: A Patch Management Framework for SLO-Optimized Hybrid   Resolution Diffusion Serving</h2><p><strong>Authors:Desen Sun, Zepeng Zhao, Yuke Wang</strong></p>
<p>The Text-to-Image (T2I) diffusion model is one of the most popular models in the world. However, serving diffusion models at the entire image level faces several problems, especially when there are multiple candidate resolutions. First, image based serving system prevents requests with different resolutions from batching together. On the other hand, requests with hybrid resolutions also indicate diverse locality features, which makes it hard to apply the same cache policy to all of them. To this end, we propose PATCHEDSERVE, A Patch Management Framework for SLO-Optimized Hybrid Resolution Diffusion Serving that provides a patch-level management strategy to gather hybrid resolution requests into batches. Specifically, PATCHEDSERVE incorporates a novel patch-based processing workflow, significantly enhancing throughput for hybrid resolution inputs. Furthermore, PATCHEDSERVE designs a patch-level cache reuse policy to fully exploit the redundancy in diffusion. In addition, PATCHEDSERVE features an SLO-aware scheduling algorithm with lightweight online latency prediction, achieving higher SLO satisfaction rates. We show that PATCHEDSERVE can achieve 30.1 % higher SLO satisfaction compared to SOTA diffusion serving system while not hurt the image quality. </p>
<blockquote>
<p>文本到图像（T2I）扩散模型是世界上最受欢迎模型之一。然而，在整个图像层面服务扩散模型面临几个问题，尤其是当存在多个候选分辨率时。首先，基于图像的服务系统阻止具有不同分辨率的请求进行批处理。另一方面，混合分辨率的请求也表现出不同的局部特征，这使得难以对所有请求应用相同的缓存策略。为此，我们提出PATCHEDSERVE，一个面向SLO优化的混合分辨率扩散服务的补丁管理框架，它提供了一种补丁级的管理策略，将混合分辨率的请求聚集到批次中。具体来说，PATCHEDSERVE引入了一种新颖的基于补丁的处理工作流程，显著提高了混合分辨率输入的吞吐量。此外，PATCHEDSERVE设计了一种补丁级的缓存重用策略，以充分利用扩散中的冗余信息。而且，PATCHEDSERVE还采用了具有轻量级在线延迟预测功能的SLO感知调度算法，以实现更高的SLO满意度。我们展示，相较于最先进（SOTA）的扩散服务系统，PATCHEDSERVE可以实现高出30.1%的SLO满意度，同时不损害图像质量。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.09253v1">PDF</a> </p>
<p><strong>摘要</strong><br>    文本转图像（T2I）扩散模型面临多种分辨率服务的问题。为解决此问题，提出PATCHDESERVE框架，采用补丁级管理策略，聚集混合分辨率请求进行批量处理，增强混合分辨率输入的吞吐量，并设计补丁级缓存重用策略，充分利用扩散中的冗余信息。此外，PATCHEDSERVE还具备SLO感知调度算法和轻量级在线延迟预测功能，以提高SLO满意度。相比现有扩散服务系统，PATCHEDSERVE能提高30.1%的SLO满意度，同时不影响图像质量。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>文本转图像（T2I）扩散模型在服务多个候选分辨率时面临挑战。</li>
<li>现有图像服务系统无法有效处理混合分辨率请求。</li>
<li>PATCHEDSERVE框架采用补丁级管理策略，聚集混合分辨率请求进行批量处理。</li>
<li>PATCHEDSERVE通过补丁级缓存策略，充分利用扩散中的冗余信息。</li>
<li>PATCHEDSERVE具备SLO感知调度算法，以提高服务等级协议（SLO）的满意度。</li>
<li>PATCHEDSERVE通过轻量级在线延迟预测功能优化性能。</li>
<li>与现有系统相比，PATCHEDSERVE在不影响图像质量的情况下，提高了SLO满意度。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.09253">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-cc984ae4e15cdb0e258f4ea1f3a5a6a6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-733cbb02604e4754a35046aae470cf59.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4be66bf58d88606c1d76c4a55e51d482.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f267e4ce6498703fc4d3384ef134fdc4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6784a0e5e112ecd112b2dc0217c54e27.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Grounding-Text-To-Image-Diffusion-Models-For-Controlled-High-Quality-Image-Generation"><a href="#Grounding-Text-To-Image-Diffusion-Models-For-Controlled-High-Quality-Image-Generation" class="headerlink" title="Grounding Text-To-Image Diffusion Models For Controlled High-Quality   Image Generation"></a>Grounding Text-To-Image Diffusion Models For Controlled High-Quality   Image Generation</h2><p><strong>Authors:Ahmad Süleyman, Göksel Biricik</strong></p>
<p>Large-scale text-to-image (T2I) diffusion models have demonstrated an outstanding performance in synthesizing diverse high-quality visuals from natural language text captions. Multiple layout-to-image models have been developed to control the generation process by utilizing a broad array of layouts such as segmentation maps, edges, and human keypoints. In this work, we present ObjectDiffusion, a model that takes inspirations from the top cutting-edge image generative frameworks to seamlessly condition T2I models with new bounding boxes capabilities. Specifically, we make substantial modifications to the network architecture introduced in ContorlNet to integrate it with the condition processing and injection techniques proposed in GLIGEN. ObjectDiffusion is initialized with pretraining parameters to leverage the generation knowledge obtained from training on large-scale datasets. We fine-tune ObjectDiffusion on the COCO2017 training dataset and evaluate it on the COCO2017 validation dataset. Our model achieves an AP$_{50}$ of 46.6, an AR of 44.5, and a FID of 19.8 outperforming the current SOTA model trained on open-source datasets in all of the three metrics. ObjectDiffusion demonstrates a distinctive capability in synthesizing diverse, high-quality, high-fidelity images that seamlessly conform to the semantic and spatial control layout. Evaluated in qualitative and quantitative tests, ObjectDiffusion exhibits remarkable grounding abilities on closed-set and open-set settings across a wide variety of contexts. The qualitative assessment verifies the ability of ObjectDiffusion to generate multiple objects of different sizes and locations. </p>
<blockquote>
<p>大规模文本到图像（T2I）扩散模型已经显示出从自然语言文本描述中合成多样高质量图像的出色性能。已经开发出了多种布局到图像模型，通过利用广泛的布局（如分割图、边缘和人体关键点）来控制生成过程。在这项工作中，我们提出了ObjectDiffusion模型，该模型受到最新图像生成框架的启发，能够无缝地将T2I模型与新的边界框功能相结合。具体来说，我们对ControlNet中引入的网络架构进行了重大修改，将其与GLIGEN中提出的条件处理技术和注入技术相结合。ObjectDiffusion使用预训练参数进行初始化，以利用在大规模数据集上训练得到的生成知识。我们在COCO2017训练数据集上对ObjectDiffusion进行了微调，并在COCO2017验证数据集上对其进行了评估。我们的模型在AP50、AR和FID三个指标上分别达到了46.6、44.5和19.8，优于当前在开源数据集上训练的最新模型。ObjectDiffusion在合成多样、高质量、高保真图像方面表现出独特的能力，这些图像无缝地符合语义和空间控制布局。在定性和定量测试中，ObjectDiffusion在封闭集和开放集设置下，在各种上下文中表现出卓越的接地能力。定性评估验证了ObjectDiffusion生成不同大小和位置多个对象的能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.09194v1">PDF</a> </p>
<p><strong>Summary</strong><br>     本文介绍了一种名为ObjectDiffusion的大型文本转图像扩散模型。该模型融合了顶级图像生成框架的灵感，为文本转图像模型增加了新的边界框功能。ObjectDiffusion在COCO2017数据集上进行微调并评估，实现了出色的性能，优于在开放源代码数据集上训练的当前最佳模型。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ObjectDiffusion是一个大型文本转图像扩散模型，能够从自然语言文本生成多样化的高质量图像。</li>
<li>该模型通过引入边界框功能，实现了对生成过程的精细控制。</li>
<li>ObjectDiffusion采用预训练参数进行初始化，利用大规模数据集上的生成知识。</li>
<li>在COCO2017数据集上进行微调后，ObjectDiffusion实现了高性能指标，包括AP50为46.6，AR为44.5，FID为19.8。</li>
<li>ObjectDiffusion在合成与语义和空间控制布局相匹配的多样化、高质量、高保真图像方面表现出独特的能力。</li>
<li>在定性测试和定量测试中，ObjectDiffusion在封闭集和开放集设置下表现出卓越的接地能力，适用于各种上下文。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.09194">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c3211c9c0c0ef8029c747f1edaff97b1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2157b11baf5f36314e2f720b41fe7283.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-debb6fb491d45a8fe2a8ce728200e4aa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3f866af1537628ec32d450d051ada726.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Generative-diffusion-model-with-inverse-renormalization-group-flows"><a href="#Generative-diffusion-model-with-inverse-renormalization-group-flows" class="headerlink" title="Generative diffusion model with inverse renormalization group flows"></a>Generative diffusion model with inverse renormalization group flows</h2><p><strong>Authors:Kanta Masuki, Yuto Ashida</strong></p>
<p>Diffusion models represent a class of generative models that produce data by denoising a sample corrupted by white noise. Despite the success of diffusion models in computer vision, audio synthesis, and point cloud generation, so far they overlook inherent multiscale structures in data and have a slow generation process due to many iteration steps. In physics, the renormalization group offers a fundamental framework for linking different scales and giving an accurate coarse-grained model. Here we introduce a renormalization group-based diffusion model that leverages multiscale nature of data distributions for realizing a high-quality data generation. In the spirit of renormalization group procedures, we define a flow equation that progressively erases data information from fine-scale details to coarse-grained structures. Through reversing the renormalization group flows, our model is able to generate high-quality samples in a coarse-to-fine manner. We validate the versatility of the model through applications to protein structure prediction and image generation. Our model consistently outperforms conventional diffusion models across standard evaluation metrics, enhancing sample quality and&#x2F;or accelerating sampling speed by an order of magnitude. The proposed method alleviates the need for data-dependent tuning of hyperparameters in the generative diffusion models, showing promise for systematically increasing sample efficiency based on the concept of the renormalization group. </p>
<blockquote>
<p>扩散模型是一类生成模型，通过消除由白噪声破坏的样本中的噪声来产生数据。尽管扩散模型在计算机视觉、音频合成和点云生成方面取得了成功，但它们忽视了数据中的内在多尺度结构，并且由于许多迭代步骤，生成过程缓慢。在物理学中，重整化组提供了一个联系不同尺度并给出精确粗粒度模型的基本框架。在这里，我们介绍了一种基于重整化组的扩散模型，该模型利用数据分布的多尺度特性来实现高质量的数据生成。秉承重整化组程序的精神，我们定义了一个流方程，该方程从精细尺度细节到粗粒度结构逐步消除数据信息。通过反转重整化组流，我们的模型能够以粗到细的方式生成高质量样本。我们通过将模型应用于蛋白质结构预测和图像生成来验证其通用性。我们的模型在标准评估指标上一致地优于传统扩散模型，通过提高样本质量或加速采样速度来提高一个数量级的性能。所提出的方法减轻了生成扩散模型中数据依赖的超参数调整需求，显示出根据重整化组的理念系统地提高样本效率的潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.09064v1">PDF</a> 9+21 pages, 4+11 figures. The code and trained models are available   at <a target="_blank" rel="noopener" href="https://github.com/kantamasuki/RGDM">https://github.com/kantamasuki/RGDM</a></p>
<p><strong>Summary</strong></p>
<p>扩散模型通过去噪被白噪声破坏的样本产生数据。虽然扩散模型在计算机视觉、音频合成和点云生成方面取得了成功，但它们忽略了数据内在的多尺度结构，并且由于许多迭代步骤而导致生成过程缓慢。结合物理中的重整化组理论，我们提出了一种基于重整化组的扩散模型，该模型利用数据分布的多尺度特性实现高质量的数据生成。通过逆转重整化组流程，该模型能够以从粗到细的方式生成高质量样本。在蛋白质结构预测和图像生成方面的应用验证了该模型的通用性。该模型在标准评估指标上一致地优于传统扩散模型，提高了样本质量，并将采样速度提高了数倍。所提出的方法减轻了生成扩散模型中数据相关超参数调整的需求，显示出基于重整化组概念的系统提高样本效率的潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型通过去噪被噪声破坏的样本产生数据。</li>
<li>传统的扩散模型忽略了数据的多尺度结构，并且生成过程较慢。</li>
<li>本文提出了基于重整化组的扩散模型，该模型能利用数据分布的多尺度特性。</li>
<li>通过逆转重整化组流程，该模型能以从粗到细的方式生成高质量样本。</li>
<li>模型在蛋白质结构预测和图像生成方面有着广泛的应用。</li>
<li>与传统扩散模型相比，该模型在评估指标上表现更优秀，提高了样本质量并加速了采样速度。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.09064">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-54d8746326b7a3a1b4de7737a6035182.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-98ffb9254139a04d920a166fb16f5d42.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb67e12998d31bb3ee872b18aaab809f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a9ae6871f2ce34eca77a0b466f46402d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-790b693b81367070cc13fd6282b5b2f8.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="NeurOp-Diff-Continuous-Remote-Sensing-Image-Super-Resolution-via-Neural-Operator-Diffusion"><a href="#NeurOp-Diff-Continuous-Remote-Sensing-Image-Super-Resolution-via-Neural-Operator-Diffusion" class="headerlink" title="NeurOp-Diff:Continuous Remote Sensing Image Super-Resolution via Neural   Operator Diffusion"></a>NeurOp-Diff:Continuous Remote Sensing Image Super-Resolution via Neural   Operator Diffusion</h2><p><strong>Authors:Zihao Xu, Yuzhi Tang, Bowen Xu, Qingquan Li</strong></p>
<p>Most publicly accessible remote sensing data suffer from low resolution, limiting their practical applications. To address this, we propose a diffusion model guided by neural operators for continuous remote sensing image super-resolution (NeurOp-Diff). Neural operators are used to learn resolution representations at arbitrary scales, encoding low-resolution (LR) images into high-dimensional features, which are then used as prior conditions to guide the diffusion model for denoising. This effectively addresses the artifacts and excessive smoothing issues present in existing super-resolution (SR) methods, enabling the generation of high-quality, continuous super-resolution images. Specifically, we adjust the super-resolution scale by a scaling factor s, allowing the model to adapt to different super-resolution magnifications. Furthermore, experiments on multiple datasets demonstrate the effectiveness of NeurOp-Diff. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/zerono000/NeurOp-Diff">https://github.com/zerono000/NeurOp-Diff</a>. </p>
<blockquote>
<p>大部分公开可访问的遥感数据存在分辨率低的问题，这限制了它们的实际应用。为解决这一问题，我们提出了一种由神经算子引导的扩散模型，用于连续遥感图像超分辨率（NeurOp-Diff）。神经算子用于学习任意尺度的分辨率表示，将低分辨率（LR）图像编码为高维特征，然后用作先验条件，引导扩散模型进行去噪。这有效地解决了现有超分辨率（SR）方法中出现的伪影和过度平滑问题，能够生成高质量、连续的超高分辨率图像。具体来说，我们通过缩放因子s调整超分辨率尺度，使模型能够适应不同的超分辨率放大倍数。此外，对多个数据集的实验证明了NeurOp-Diff的有效性。我们的代码可在[<a target="_blank" rel="noopener" href="https://github.com/zerono000/NeurOp-Diff%E6%89%BE%E5%88%B0%E3%80%82]">https://github.com/zerono000/NeurOp-Diff找到。]</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.09054v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>针对大多数公开可用的遥感数据分辨率低的问题，提出了一种基于神经算子和扩散模型的连续遥感图像超分辨率方法（NeurOp-Diff）。神经算子用于学习任意尺度的分辨率表示，将低分辨率（LR）图像编码为高维特征，作为先验条件引导扩散模型进行去噪。这有效解决了一倍的超分辨率方法中存在的伪影和过度平滑问题，能够生成高质量、连续的超高分辨率图像。通过缩放因子s调整超分辨率尺度，使模型能够适应不同的超分辨率放大倍数。在多个数据集上的实验验证了NeurOp-Diff的有效性。</p>
<p><strong>要点</strong></p>
<ol>
<li>神经算子用于学习任意尺度的分辨率表示。</li>
<li>低分辨率图像被编码为高维特征作为先验条件。</li>
<li>扩散模型在超分辨率过程中用于去噪。</li>
<li>方法解决了现有超分辨率方法的伪影和过度平滑问题。</li>
<li>通过缩放因子调整超分辨率尺度，适应不同的放大需求。</li>
<li>在多个数据集上的实验验证了NeurOp-Diff的有效性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.09054">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-6b09f1e8afca0b0421b4283b275997f2.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-18\./crop_Diffusion Models/2501.09054v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-18\./crop_Diffusion Models/2501.09054v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="FlexiClip-Locality-Preserving-Free-Form-Character-Animation"><a href="#FlexiClip-Locality-Preserving-Free-Form-Character-Animation" class="headerlink" title="FlexiClip: Locality-Preserving Free-Form Character Animation"></a>FlexiClip: Locality-Preserving Free-Form Character Animation</h2><p><strong>Authors:Anant Khandelwal</strong></p>
<p>Animating clipart images with seamless motion while maintaining visual fidelity and temporal coherence presents significant challenges. Existing methods, such as AniClipart, effectively model spatial deformations but often fail to ensure smooth temporal transitions, resulting in artifacts like abrupt motions and geometric distortions. Similarly, text-to-video (T2V) and image-to-video (I2V) models struggle to handle clipart due to the mismatch in statistical properties between natural video and clipart styles. This paper introduces FlexiClip, a novel approach designed to overcome these limitations by addressing the intertwined challenges of temporal consistency and geometric integrity. FlexiClip extends traditional B&#39;ezier curve-based trajectory modeling with key innovations: temporal Jacobians to correct motion dynamics incrementally, continuous-time modeling via probability flow ODEs (pfODEs) to mitigate temporal noise, and a flow matching loss inspired by GFlowNet principles to optimize smooth motion transitions. These enhancements ensure coherent animations across complex scenarios involving rapid movements and non-rigid deformations. Extensive experiments validate the effectiveness of FlexiClip in generating animations that are not only smooth and natural but also structurally consistent across diverse clipart types, including humans and animals. By integrating spatial and temporal modeling with pre-trained video diffusion models, FlexiClip sets a new standard for high-quality clipart animation, offering robust performance across a wide range of visual content. Project Page: <a target="_blank" rel="noopener" href="https://creative-gen.github.io/flexiclip.github.io/">https://creative-gen.github.io/flexiclip.github.io/</a> </p>
<blockquote>
<p>将剪贴画图像以无缝运动进行动画处理，同时保持视觉保真度和时间连贯性，这带来了相当大的挑战。现有方法，如AniClipart，能够有效地模拟空间变形，但往往不能保证平滑的时间过渡，导致运动突兀和几何失真等伪影。同样，文本到视频（T2V）和图像到视频（I2V）的模型在处理剪贴画时也遇到了困难，因为自然视频和剪贴画风格之间的统计属性不匹配。本文介绍了FlexiClip，这是一种旨在克服这些限制的新方法，通过解决时间一致性和几何完整性的交织挑战来应对。FlexiClip对传统基于Bézier曲线的轨迹建模进行了关键创新：使用时间雅可比矩阵逐步纠正运动动力学，通过概率流ODE（pfODEs）进行连续时间建模以减轻时间噪声，并受到GFlowNet原理启发的流匹配损失以优化平滑运动过渡。这些增强功能确保了涉及快速运动和非刚性变形的复杂场景中的连贯动画。大量实验验证了FlexiClip在生成动画方面的有效性，这些动画不仅平滑自然，而且在不同类型剪贴画（包括人物和动物）之间结构一致。通过结合预训练的视频扩散模型进行空间和时间建模，FlexiClip为高质量剪贴动画设定了新标准，在广泛的视觉内容中表现出稳健的性能。项目页面：<a target="_blank" rel="noopener" href="https://creative-gen.github.io/flexiclip.github.io/">https://creative-gen.github.io/flexiclip.github.io/</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.08676v1">PDF</a> 13 pages, 4 figures, 7 tables</p>
<p><strong>Summary</strong></p>
<p>FlexiClip是一种针对clipart图像动画的新方法，旨在实现平滑流畅的运动，同时保持视觉保真度和时间连贯性。它结合了创新的轨迹建模技术，包括使用临时雅可比矩阵进行增量运动校正、概率流ODEs进行连续时间建模以及基于GFlowNet原理的流匹配损失来优化平滑运动过渡。FlexiClip通过整合空间和时间建模与预训练的视频扩散模型，为高质量的clipart动画设定了新的标准，能在各种视觉内容中表现稳健。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FlexiClip解决了clipart图像动画中的显著挑战，如保持视觉保真度和时间连贯性。</li>
<li>方法结合了创新的轨迹建模技术，确保运动平滑且自然。</li>
<li>通过使用临时雅可比矩阵和概率流ODEs等技术，FlexiClip实现了复杂场景下的连贯动画。</li>
<li>FlexiClip集成了空间和时间建模，与预训练的视频扩散模型相结合，为高质量clipart动画树立了新标准。</li>
<li>该方法在各种视觉内容中表现稳健，包括人类和动物等多种clipart类型。</li>
<li>FlexiClip能有效处理快速运动和非刚性变形等挑战。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.08676">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-18\./crop_Diffusion Models/2501.08676v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-18\./crop_Diffusion Models/2501.08676v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-18\./crop_Diffusion Models/2501.08676v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-18\./crop_Diffusion Models/2501.08676v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="TimeFlow-Longitudinal-Brain-Image-Registration-and-Aging-Progression-Analysis"><a href="#TimeFlow-Longitudinal-Brain-Image-Registration-and-Aging-Progression-Analysis" class="headerlink" title="TimeFlow: Longitudinal Brain Image Registration and Aging Progression   Analysis"></a>TimeFlow: Longitudinal Brain Image Registration and Aging Progression   Analysis</h2><p><strong>Authors:Bailiang Jian, Jiazhen Pan, Yitong Li, Fabian Bongratz, Ruochen Li, Daniel Rueckert, Benedikt Wiestler, Christian Wachinger</strong></p>
<p>Predicting future brain states is crucial for understanding healthy aging and neurodegenerative diseases. Longitudinal brain MRI registration, a cornerstone for such analyses, has long been limited by its inability to forecast future developments, reliance on extensive, dense longitudinal data, and the need to balance registration accuracy with temporal smoothness. In this work, we present \emph{TimeFlow}, a novel framework for longitudinal brain MRI registration that overcomes all these challenges. Leveraging a U-Net architecture with temporal conditioning inspired by diffusion models, TimeFlow enables accurate longitudinal registration and facilitates prospective analyses through future image prediction. Unlike traditional methods that depend on explicit smoothness regularizers and dense sequential data, TimeFlow achieves temporal consistency and continuity without these constraints. Experimental results highlight its superior performance in both future timepoint prediction and registration accuracy compared to state-of-the-art methods. Additionally, TimeFlow supports novel biological brain aging analyses, effectively differentiating neurodegenerative conditions from healthy aging. It eliminates the need for segmentation, thereby avoiding the challenges of non-trivial annotation and inconsistent segmentation errors. TimeFlow paves the way for accurate, data-efficient, and annotation-free prospective analyses of brain aging and chronic diseases. </p>
<blockquote>
<p>预测未来的大脑状态对于理解健康老龄化和神经退行性疾病至关重要。纵向脑MRI注册是此类分析的核心，长期以来一直受到无法预测未来发展、依赖广泛而密集的纵向数据以及需要在注册精度和时间平滑之间取得平衡的限制。在这项工作中，我们提出了\emph{TimeFlow}，这是一种新型的纵向脑MRI注册框架，克服了所有这些挑战。TimeFlow利用受扩散模型启发的具有时间条件的U-Net架构，实现了准确的纵向注册，并通过未来图像预测促进了前瞻性分析。与传统方法不同，这些方法依赖于明确的平滑正则器和密集序列数据，TimeFlow在不使用这些约束的情况下实现了时间一致性和连续性。实验结果表明，与最先进的方法相比，它在未来时间点的预测和注册精度方面表现出卓越的性能。此外，TimeFlow支持新型生物脑衰老分析，可有效区分神经退行性疾病和健康衰老。它消除了对分割的需求，从而避免了非平凡注释和不一致的分割错误带来的挑战。TimeFlow为准确、高效且无需注释的前瞻性分析脑衰老和慢性疾病开辟了道路。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.08667v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于扩散模型的时间流框架，能有效解决长期大脑MRI注册的挑战。它使用U型网络架构，通过扩散模型启发的时间条件，实现准确的长周期注册和未来图像预测。与传统方法相比，时间流在无需显式平滑正则化和密集序列数据的情况下，实现了时间一致性和连续性。其性能优于现有方法，并支持新型脑衰老分析，有效区分神经退行性疾病与健康衰老。时间流消除了对分割的需求，避免了非平凡标注和分割错误的不一致性。它为准确、高效且无标注的脑衰老和慢性病前瞻性分析铺平了道路。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>时间流框架解决了长期大脑MRI注册的多个挑战。</li>
<li>利用U型网络架构和扩散模型的启发，实现了准确的长周期注册和未来图像预测。</li>
<li>时间流在不依赖显式平滑正则化和密集序列数据的情况下实现了时间一致性和连续性。</li>
<li>与现有方法相比，时间流在未来时间点预测和注册准确性方面表现出卓越性能。</li>
<li>时间流支持新型脑衰老分析，并能有效区分神经退行性疾病与健康衰老。</li>
<li>时间流消除了对分割的需求，避免了标注和分割错误的问题。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.08667">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-18\./crop_Diffusion Models/2501.08667v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-18\./crop_Diffusion Models/2501.08667v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-18\./crop_Diffusion Models/2501.08667v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="DynamicFace-High-Quality-and-Consistent-Video-Face-Swapping-using-Composable-3D-Facial-Priors"><a href="#DynamicFace-High-Quality-and-Consistent-Video-Face-Swapping-using-Composable-3D-Facial-Priors" class="headerlink" title="DynamicFace: High-Quality and Consistent Video Face Swapping using   Composable 3D Facial Priors"></a>DynamicFace: High-Quality and Consistent Video Face Swapping using   Composable 3D Facial Priors</h2><p><strong>Authors:Runqi Wang, Sijie Xu, Tianyao He, Yang Chen, Wei Zhu, Dejia Song, Nemo Chen, Xu Tang, Yao Hu</strong></p>
<p>Face swapping transfers the identity of a source face to a target face while retaining the attributes like expression, pose, hair, and background of the target face. Advanced face swapping methods have achieved attractive results. However, these methods often inadvertently transfer identity information from the target face, compromising expression-related details and accurate identity. We propose a novel method DynamicFace that leverages the power of diffusion model and plug-and-play temporal layers for video face swapping. First, we introduce four fine-grained face conditions using 3D facial priors. All conditions are designed to be disentangled from each other for precise and unique control. Then, we adopt Face Former and ReferenceNet for high-level and detailed identity injection. Through experiments on the FF++ dataset, we demonstrate that our method achieves state-of-the-art results in face swapping, showcasing superior image quality, identity preservation, and expression accuracy. Besides, our method could be easily transferred to video domain with temporal attention layer. Our code and results will be available on the project page: <a target="_blank" rel="noopener" href="https://dynamic-face.github.io/">https://dynamic-face.github.io/</a> </p>
<blockquote>
<p>面部替换技术能够将源面部的身份转移到目标面部，同时保留目标面部的表情、姿态、发型和背景等属性。先进的面部替换方法已经取得了吸引人的成果。然而，这些方法常常会在不经意间从目标面部转移身份信息，从而影响到表情相关的细节和准确的身份表达。我们提出了一种新型方法DynamicFace，它利用扩散模型的强大功能和即插即用的时间层来实现视频面部替换。首先，我们引入四个精细的面部条件，使用3D面部先验。所有条件都被设计为相互独立，以实现精确和独特的控制。然后，我们采用Face Former和ReferenceNet进行高级和详细的身份注入。在FF++数据集上的实验表明，我们的方法在面部替换方面达到了最新水平，展现了卓越的图片质量、身份保留和表情准确性。此外，我们的方法能够轻松地通过时间注意力层转移到视频领域。我们的代码和结果将在项目页面公布：<a target="_blank" rel="noopener" href="https://dynamic-face.github.io/">https://dynamic-face.github.io/</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.08553v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种基于扩散模型的新型动态面部替换方法，该方法利用四维精细面部条件和Face Former与ReferenceNet进行高级和详细身份注入，实现视频面部替换。实验结果表明，该方法在面部替换方面达到最新水平，具有出色的图像质量、身份保留和表情准确性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>面部替换技术能够将源面部的身份转移到目标面部，同时保留目标面部的表情、姿态、发型和背景等属性。</li>
<li>当前先进的面部替换方法面临在转移身份信息时损失表情细节和准确身份的问题。</li>
<li>提出了一种新型动态面部替换方法DynamicFace，利用扩散模型的强大功能以及即插即用的时间层，实现视频面部替换。</li>
<li>DynamicFace通过引入四个精细面部条件，利用三维面部先验进行精确而独特的控制。</li>
<li>采用Face Former和ReferenceNet进行高级和详细的身份注入，以提高面部替换的效果。</li>
<li>实验结果表明，DynamicFace方法在面部替换方面达到最新水平，具有优秀的图像质量、身份保留和表情准确性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.08553">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-18\./crop_Diffusion Models/2501.08553v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-18\./crop_Diffusion Models/2501.08553v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-18\./crop_Diffusion Models/2501.08553v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-18\./crop_Diffusion Models/2501.08553v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-18\./crop_Diffusion Models/2501.08553v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="A-General-Framework-for-Inference-time-Scaling-and-Steering-of-Diffusion-Models"><a href="#A-General-Framework-for-Inference-time-Scaling-and-Steering-of-Diffusion-Models" class="headerlink" title="A General Framework for Inference-time Scaling and Steering of Diffusion   Models"></a>A General Framework for Inference-time Scaling and Steering of Diffusion   Models</h2><p><strong>Authors:Raghav Singhal, Zachary Horvitz, Ryan Teehan, Mengye Ren, Zhou Yu, Kathleen McKeown, Rajesh Ranganath</strong></p>
<p>Diffusion models produce impressive results in modalities ranging from images and video to protein design and text. However, generating samples with user-specified properties remains a challenge. Recent research proposes fine-tuning models to maximize rewards that capture desired properties, but these methods require expensive training and are prone to mode collapse. In this work, we propose Feynman Kac (FK) steering, an inference-time framework for steering diffusion models with reward functions. FK steering works by sampling a system of multiple interacting diffusion processes, called particles, and resampling particles at intermediate steps based on scores computed using functions called potentials. Potentials are defined using rewards for intermediate states and are selected such that a high value indicates that the particle will yield a high-reward sample. We explore various choices of potentials, intermediate rewards, and samplers. We evaluate FK steering on text-to-image and text diffusion models. For steering text-to-image models with a human preference reward, we find that FK steering a 0.8B parameter model outperforms a 2.6B parameter fine-tuned model on prompt fidelity, with faster sampling and no training. For steering text diffusion models with rewards for text quality and specific text attributes, we find that FK steering generates lower perplexity, more linguistically acceptable outputs and enables gradient-free control of attributes like toxicity. Our results demonstrate that inference-time scaling and steering of diffusion models, even with off-the-shelf rewards, can provide significant sample quality gains and controllability benefits. Code is available at <a target="_blank" rel="noopener" href="https://github.com/zacharyhorvitz/Fk-Diffusion-Steering">https://github.com/zacharyhorvitz/Fk-Diffusion-Steering</a> . </p>
<blockquote>
<p>扩散模型在图像、视频、蛋白质设计和文本等多种模态中取得了令人印象深刻的效果。然而，生成具有用户指定属性的样本仍然是一个挑战。最近的研究提出对模型进行微调，以最大化捕捉所需属性的奖励，但这些方法需要昂贵的训练且容易出现模式崩溃。在这项工作中，我们提出了费曼·卡克（FK）转向，这是一种在推理时间框架中使用奖励函数引导扩散模型的框架。FK转向通过采样多个相互作用的扩散过程系统（称为粒子）进行工作，并在中间步骤基于使用称为潜力的函数计算得分进行粒子重采样。潜力是通过中间状态的奖励定义的，并选择使得高值表示粒子将产生高奖励样本。我们探讨了各种潜力的选择、中间奖励和采样器。我们对文本到图像和文本扩散模型评估了FK转向。对于使用人类偏好奖励引导文本到图像模型，我们发现FK转向一个0.8B参数模型的提示保真度超过了2.6B参数微调模型，具有更快的采样速度和无需训练。对于使用文本质量和特定文本属性的奖励引导文本扩散模型，我们发现FK转向产生了更低的困惑度，更在语言上可接受的输出，并实现了对诸如毒性等属性的无梯度控制。我们的结果证明了即使在离线奖励的情况下，推理时间尺度和引导扩散模型也可以提供显著的样本质量提升和可控性优势。相关代码可在<a target="_blank" rel="noopener" href="https://github.com/zacharyhorvitz/Fk-Diffusion-Steering%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/zacharyhorvitz/Fk-Diffusion-Steering中找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.06848v3">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>扩散模型广泛应用于图像、视频、蛋白质设计和文本等多个领域，生成具有用户指定属性的样本仍是一个挑战。现有方法通过微调模型以最大化奖励来捕捉所需属性，但这种方法训练成本高昂且容易出现模式崩溃。本研究提出Feynman Kac（FK）转向，这是一种在推理时间框架内使用奖励函数控制扩散模型的框架。FK转向通过采样多个相互作用的扩散过程（粒子）的系统，并在中间步骤基于使用势能计算的分数重新采样粒子。势能利用中间状态的奖励定义，选择高值的粒子将产生高奖励样本。本研究探讨了不同的势能、中间奖励和采样器选择。在文本到图像和文本扩散模型上评估FK转向效果。对于利用人类偏好奖励控制文本到图像模型，发现FK转向的0.8B参数模型在提示保真度上优于2.6B参数微调模型，采样速度更快且无需训练。对于控制文本质量和特定文本属性的文本扩散模型，FK转向生成了更低困惑度、更符合语言学的输出，并实现了无毒性的梯度控制。结果表明，即使在离线奖励下，扩散模型的推理时间尺度和转向也能显著提高样本质量和可控性。相关代码可访问：<a target="_blank" rel="noopener" href="https://github.com/zacharyhorvitz/Fk-Diffusion-Steering">https://github.com/zacharyhorvitz/Fk-Diffusion-Steering</a>。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>扩散模型广泛应用于多个领域，生成具有用户指定属性的样本是当前的挑战。</li>
<li>现有方法微调模型以最大化奖励，但这种方法成本高昂且易陷入模式崩溃。</li>
<li>提出Feynman Kac（FK）转向，一种在推理时间使用奖励函数控制扩散模型的框架。</li>
<li>FK转向通过采样多个相互作用的扩散过程（粒子）并在中间步骤重新采样来实现控制。</li>
<li>利用不同的势能和奖励函数选择进行试验，以优化生成样本的质量。</li>
<li>在文本到图像和文本扩散模型上评估FK转向，效果显著。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.06848">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-18\./crop_Diffusion Models/2501.06848v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-18\./crop_Diffusion Models/2501.06848v3/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-18\./crop_Diffusion Models/2501.06848v3/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-18\./crop_Diffusion Models/2501.06848v3/page_3_0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="CDDIP-Constrained-Diffusion-Driven-Deep-Image-Prior-for-Seismic-Image-Reconstruction"><a href="#CDDIP-Constrained-Diffusion-Driven-Deep-Image-Prior-for-Seismic-Image-Reconstruction" class="headerlink" title="CDDIP: Constrained Diffusion-Driven Deep Image Prior for Seismic Image   Reconstruction"></a>CDDIP: Constrained Diffusion-Driven Deep Image Prior for Seismic Image   Reconstruction</h2><p><strong>Authors:Paul Goyes-Peñafiel, Ulugbek Kamilov, Henry Arguello</strong></p>
<p>Seismic data frequently exhibits missing traces, substantially affecting subsequent seismic processing and interpretation. Deep learning-based approaches have demonstrated significant advancements in reconstructing irregularly missing seismic data through supervised and unsupervised methods. Nonetheless, substantial challenges remain, such as generalization capacity and computation time cost during the inference. Our work introduces a reconstruction method that uses a pre-trained generative diffusion model for image synthesis and incorporates Deep Image Prior to enforce data consistency when reconstructing missing traces in seismic data. The proposed method has demonstrated strong robustness and high reconstruction capability of post-stack and pre-stack data with different levels of structural complexity, even in field and synthetic scenarios where test data were outside the training domain. This indicates that our method can handle the high geological variability of different exploration targets. Additionally, compared to other state-of-the-art seismic reconstruction methods using diffusion models. During inference, our approach reduces the number of sampling timesteps by up to 4x. </p>
<blockquote>
<p>地震数据经常存在缺失的轨迹，这会对后续的地震处理和解释产生重大影响。基于深度学习的方法在通过有监督和无监督方法重建不规则缺失的地震数据方面取得了显著的进步。然而，仍然存在重大挑战，例如在推理过程中的泛化能力和计算时间成本。我们的工作引入了一种重建方法，该方法使用预训练的生成扩散模型进行图像合成，并结合深度图像先验在重建地震数据中缺失轨迹时强制数据一致性。所提出的方法表现出强大的稳健性和对不同结构复杂性级别的叠加后数据和叠加前数据的高重建能力，即使在测试数据超出训练领域的现场和合成场景中亦是如此。这表明我们的方法可以处理不同勘探目标的高地质变异性。此外，与其他使用扩散模型的最新地震重建方法相比，我们的方法在推理过程中减少了高达4倍的采样时间步数。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.17402v2">PDF</a> 5 pages, 4 figures, 3 tables. Submitted to geoscience and remote   sensing letters</p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种基于预训练生成扩散模型的重建方法，用于合成图像并重建地震数据中缺失的轨迹。该方法结合了深度图像先验技术，以确保在重建过程中数据的一致性。实验表明，该方法在应对不同结构复杂度的后堆栈和预堆栈数据时表现出强大的稳健性和高重建能力，即使在超出训练域的场景下也能处理各种勘探目标的高地质变异性。相较于其他使用扩散模型的先进地震重建方法，此方法在推理阶段减少了高达4倍的采样时间步数。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>地震数据中的缺失轨迹对后续的地震处理和解释产生了影响。</li>
<li>深度学习在重建不规则缺失的地震数据方面已显示出显著进展。</li>
<li>本文引入了一种基于预训练生成扩散模型的重建方法，用于合成图像。</li>
<li>结合深度图像先验技术，该方法在重建缺失轨迹时保证了数据的一致性。</li>
<li>该方法在应对不同结构复杂度的地震数据以及不同地质环境下表现出强大的稳健性和高重建能力。</li>
<li>与其他使用扩散模型的先进地震重建方法相比，该方法在推理阶段的采样时间步数减少了高达4倍。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.17402">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-18\./crop_Diffusion Models/2407.17402v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-18\./crop_Diffusion Models/2407.17402v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-18\./crop_Diffusion Models/2407.17402v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-18\./crop_Diffusion Models/2407.17402v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-18\./crop_Diffusion Models/2407.17402v2/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-18\./crop_Diffusion Models/2407.17402v2/page_4_0.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Zero-shot-Video-Restoration-and-Enhancement-Using-Pre-Trained-Image-Diffusion-Model"><a href="#Zero-shot-Video-Restoration-and-Enhancement-Using-Pre-Trained-Image-Diffusion-Model" class="headerlink" title="Zero-shot Video Restoration and Enhancement Using Pre-Trained Image   Diffusion Model"></a>Zero-shot Video Restoration and Enhancement Using Pre-Trained Image   Diffusion Model</h2><p><strong>Authors:Cong Cao, Huanjing Yue, Xin Liu, Jingyu Yang</strong></p>
<p>Diffusion-based zero-shot image restoration and enhancement models have achieved great success in various tasks of image restoration and enhancement. However, directly applying them to video restoration and enhancement results in severe temporal flickering artifacts. In this paper, we propose the first framework for zero-shot video restoration and enhancement based on the pre-trained image diffusion model. By replacing the spatial self-attention layer with the proposed short-long-range (SLR) temporal attention layer, the pre-trained image diffusion model can take advantage of the temporal correlation between frames. We further propose temporal consistency guidance, spatial-temporal noise sharing, and an early stopping sampling strategy to improve temporally consistent sampling. Our method is a plug-and-play module that can be inserted into any diffusion-based image restoration or enhancement methods to further improve their performance. Experimental results demonstrate the superiority of our proposed method. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/cao-cong/ZVRD">https://github.com/cao-cong/ZVRD</a>. </p>
<blockquote>
<p>基于扩散的零样本图像修复和增强模型在图像修复和增强的各种任务中取得了巨大的成功。然而，将其直接应用于视频修复和增强会导致严重的时空闪烁伪影。在本文中，我们提出了基于预训练图像扩散模型的零样本视频修复和增强的第一个框架。通过用所提出的短长程（SLR）时间注意力层替换空间自注意力层，预训练的图像扩散模型可以利用帧之间的时间相关性。我们还提出了时间一致性指导、时空噪声共享和早期停止采样策略，以改善时间一致性的采样。我们的方法是一个即插即用的模块，可以插入到任何基于扩散的图像修复或增强方法中，以进一步提高其性能。实验结果证明了我们的方法优越性。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/cao-cong/ZVRD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/cao-cong/ZVRD找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.01960v2">PDF</a> Accepted by AAAI 2025</p>
<p><strong>Summary</strong><br>扩散模型在零样本图像修复和增强任务中取得了巨大成功，但直接应用于视频修复和增强会导致严重的时序闪烁伪影。本文首次提出基于预训练图像扩散模型的零样本视频修复和增强框架。通过用所提出的长短程时序注意力层替换空间自注意力层，利用帧间的时序相关性，预训练的图像扩散模型能够改善视频修复和增强的效果。此外，本文还提出了时序一致性引导、时空噪声共享和早期停止采样策略来提高时序一致性采样效果。本文方法是一个即插即用模块，可插入任何扩散式图像修复或增强方法中，进一步提高其性能。实验结果表明，本文提出的方法具有优越性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型在零样本图像修复和增强任务中表现优异，但应用于视频修复和增强时存在时序闪烁问题。</li>
<li>本文首次提出了基于预训练图像扩散模型的零样本视频修复和增强框架。</li>
<li>通过用长短程时序注意力层替换空间自注意力层，利用帧间时序相关性。</li>
<li>提出了时序一致性引导、时空噪声共享和早期停止采样策略来提高时序一致性采样。</li>
<li>本文方法是一个通用的模块，可以方便地集成到现有的扩散模型中，提高图像修复和增强的性能。</li>
<li>实验结果证明了该方法在视频修复和增强任务上的优越性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.01960">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-18\./crop_Diffusion Models/2407.01960v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-18\./crop_Diffusion Models/2407.01960v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-18\./crop_Diffusion Models/2407.01960v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-18\./crop_Diffusion Models/2407.01960v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-18\./crop_Diffusion Models/2407.01960v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Simplified-and-Generalized-Masked-Diffusion-for-Discrete-Data"><a href="#Simplified-and-Generalized-Masked-Diffusion-for-Discrete-Data" class="headerlink" title="Simplified and Generalized Masked Diffusion for Discrete Data"></a>Simplified and Generalized Masked Diffusion for Discrete Data</h2><p><strong>Authors:Jiaxin Shi, Kehang Han, Zhe Wang, Arnaud Doucet, Michalis K. Titsias</strong></p>
<p>Masked (or absorbing) diffusion is actively explored as an alternative to autoregressive models for generative modeling of discrete data. However, existing work in this area has been hindered by unnecessarily complex model formulations and unclear relationships between different perspectives, leading to suboptimal parameterization, training objectives, and ad hoc adjustments to counteract these issues. In this work, we aim to provide a simple and general framework that unlocks the full potential of masked diffusion models. We show that the continuous-time variational objective of masked diffusion models is a simple weighted integral of cross-entropy losses. Our framework also enables training generalized masked diffusion models with state-dependent masking schedules. When evaluated by perplexity, our models trained on OpenWebText surpass prior diffusion language models at GPT-2 scale and demonstrate superior performance on 4 out of 5 zero-shot language modeling tasks. Furthermore, our models vastly outperform previous discrete diffusion models on pixel-level image modeling, achieving 2.75 (CIFAR-10) and 3.40 (ImageNet 64x64) bits per dimension that are better than autoregressive models of similar sizes. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/google-deepmind/md4">https://github.com/google-deepmind/md4</a>. </p>
<blockquote>
<p>遮蔽（或吸收）扩散作为一种离散数据生成模型的自回归模型的替代方案，正在得到积极的研究。然而，该领域现有的工作受到了模型公式不必要地复杂和不同观点之间关系不明确的影响，导致了参数化、训练目标和临时调整效果不佳。在这项工作中，我们的目标是提供一个简单且通用的框架，以充分发挥遮蔽扩散模型的潜力。我们证明了遮蔽扩散模型的连续时间变分目标是交叉熵损失的简单加权积分。我们的框架还允许使用状态依赖的遮蔽时间表来训练广义遮蔽扩散模型。以困惑度评估时，我们在OpenWebText上训练的模型超越了GPT-2规模的先前扩散语言模型，并在五项零镜头语言建模任务中的四项上表现出卓越性能。此外，我们的模型在像素级图像建模上大大优于先前的离散扩散模型，在CIFAR-10和ImageNet 64x64上实现了每维度分别为2.75比特和3.40比特的结果，优于类似规模的自回归模型。我们的代码位于<a target="_blank" rel="noopener" href="https://github.com/google-deepmind/md4%E3%80%82">https://github.com/google-deepmind/md4。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.04329v4">PDF</a> NeurIPS 2024. Code is available at:   <a target="_blank" rel="noopener" href="https://github.com/google-deepmind/md4">https://github.com/google-deepmind/md4</a></p>
<p><strong>Summary</strong><br>     本文探索了掩膜扩散模型作为生成离散数据的替代方案。文章提出了一种简单通用的掩膜扩散模型框架，解决了现有研究中模型公式复杂、不同视角关系不明确的问题，实现了参数化、训练目标的最优化。此外，本文展示了掩膜扩散模型的连续时间变分目标与交叉熵损失的加权积分关系。模型能在OpenWebText数据集上达到GPT-2规模的最低困惑度，并在零样本语言建模任务中表现出卓越性能。同时，在像素级图像建模方面，模型表现优于先前的离散扩散模型，实现了CIFAR-10的2.75比特和ImageNet 64x64的3.4比特每维度的高性能表现。代码已公开于GitHub。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>掩膜扩散模型作为一种生成离散数据的替代方案正受到关注。</li>
<li>提出了一种简单通用的掩膜扩散模型框架，解决了现有研究中模型复杂和关系不明确的问题。</li>
<li>揭示了掩膜扩散模型的连续时间变分目标与交叉熵损失的加权积分关系。</li>
<li>模型在GPT-2规模的OpenWebText数据集上实现了最低的困惑度。</li>
<li>模型在零样本语言建模任务中表现出卓越性能，特别是在某些特定任务上表现优于其他模型。</li>
<li>模型在像素级图像建模方面显著优于先前的离散扩散模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.04329">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-18\./crop_Diffusion Models/2406.04329v4/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-18\./crop_Diffusion Models/2406.04329v4/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-18\./crop_Diffusion Models/2406.04329v4/page_4_0.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Learning-Discrete-Concepts-in-Latent-Hierarchical-Models"><a href="#Learning-Discrete-Concepts-in-Latent-Hierarchical-Models" class="headerlink" title="Learning Discrete Concepts in Latent Hierarchical Models"></a>Learning Discrete Concepts in Latent Hierarchical Models</h2><p><strong>Authors:Lingjing Kong, Guangyi Chen, Biwei Huang, Eric P. Xing, Yuejie Chi, Kun Zhang</strong></p>
<p>Learning concepts from natural high-dimensional data (e.g., images) holds potential in building human-aligned and interpretable machine learning models. Despite its encouraging prospect, formalization and theoretical insights into this crucial task are still lacking. In this work, we formalize concepts as discrete latent causal variables that are related via a hierarchical causal model that encodes different abstraction levels of concepts embedded in high-dimensional data (e.g., a dog breed and its eye shapes in natural images). We formulate conditions to facilitate the identification of the proposed causal model, which reveals when learning such concepts from unsupervised data is possible. Our conditions permit complex causal hierarchical structures beyond latent trees and multi-level directed acyclic graphs in prior work and can handle high-dimensional, continuous observed variables, which is well-suited for unstructured data modalities such as images. We substantiate our theoretical claims with synthetic data experiments. Further, we discuss our theory’s implications for understanding the underlying mechanisms of latent diffusion models and provide corresponding empirical evidence for our theoretical insights. </p>
<blockquote>
<p>从自然的高维数据（例如图像）中学习概念，在构建与人类对齐和可解释的机器学习模型方面具潜力。尽管其前景鼓舞人心，但关于这一关键任务的形式化和理论洞察仍然缺乏。在这项工作中，我们将概念形式化为离散的潜在因果变量，这些变量通过层次化因果模型相关联，该模型对高维数据中嵌入概念的不同抽象层次进行编码（例如，自然图像中的犬种及其眼睛形状）。我们制定条件，以促进所提出因果模型的识别，揭示从非监督数据中学习此类概念的可能性。我们的条件允许超出先前工作中的潜在树和多级有向无环图的复杂因果层次结构，并能够处理高维、连续的观测变量，这非常适合图像等无结构的数据模式。我们通过合成数据实验来证实我们的理论主张。此外，我们讨论了我们的理论对理解潜在扩散模型的内在机制的影响，并为我们的理论见解提供相应的实证证据。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.00519v2">PDF</a> NeurIPS 2024</p>
<p><strong>Summary</strong></p>
<p>本文探讨从自然高维数据（如图像）中学习概念在构建人类对齐和可解释的机器学习模型中的潜力。文章将概念形式化为离散潜在因果变量，并通过层次因果模型关联，该模型编码嵌入在高维数据中的不同抽象层次的概念。文章制定了条件以促进所提出的因果模型的识别，揭示从非监督数据中学习此类概念的可能性。该条件能够处理超越先前工作中潜在树和多级有向无环图的复杂因果层次结构，以及适合图像等非结构化数据的高维连续观测变量。文章通过合成数据实验证实了理论主张，并探讨了理论对潜在扩散模型的内在机制的影响，为理论见解提供了相应的实证证据。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文章将概念形式化为离散潜在因果变量，并提出了一个层次化的因果模型来表示高维数据中的不同概念间的关系。</li>
<li>文章制定了条件以促进因果模型的识别，并揭示了从非监督数据中学习概念的可能性。</li>
<li>该理论框架能够处理复杂的因果层次结构，包括高维连续观测变量，适用于非结构化数据，如图像。</li>
<li>文章通过合成数据实验证实了理论主张。</li>
<li>文章讨论了理论对理解潜在扩散模型的内在机制的影响，并为理论见解提供了实证证据。</li>
<li>学习从自然高维数据中提取概念对于构建与人类对齐和可解释的机器学习模型具有潜力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.00519">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-18\./crop_Diffusion Models/2406.00519v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-18\./crop_Diffusion Models/2406.00519v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-18\./crop_Diffusion Models/2406.00519v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="DiffMesh-A-Motion-aware-Diffusion-Framework-for-Human-Mesh-Recovery-from-Videos"><a href="#DiffMesh-A-Motion-aware-Diffusion-Framework-for-Human-Mesh-Recovery-from-Videos" class="headerlink" title="DiffMesh: A Motion-aware Diffusion Framework for Human Mesh Recovery   from Videos"></a>DiffMesh: A Motion-aware Diffusion Framework for Human Mesh Recovery   from Videos</h2><p><strong>Authors:Ce Zheng, Xianpeng Liu, Qucheng Peng, Tianfu Wu, Pu Wang, Chen Chen</strong></p>
<p>Human mesh recovery (HMR) provides rich human body information for various real-world applications. While image-based HMR methods have achieved impressive results, they often struggle to recover humans in dynamic scenarios, leading to temporal inconsistencies and non-smooth 3D motion predictions due to the absence of human motion. In contrast, video-based approaches leverage temporal information to mitigate this issue. In this paper, we present DiffMesh, an innovative motion-aware Diffusion-like framework for video-based HMR. DiffMesh establishes a bridge between diffusion models and human motion, efficiently generating accurate and smooth output mesh sequences by incorporating human motion within the forward process and reverse process in the diffusion model. Extensive experiments are conducted on the widely used datasets (Human3.6M \cite{h36m_pami} and 3DPW \cite{pw3d2018}), which demonstrate the effectiveness and efficiency of our DiffMesh. Visual comparisons in real-world scenarios further highlight DiffMesh’s suitability for practical applications. </p>
<blockquote>
<p>人体网格恢复（HMR）为各种现实世界应用提供了丰富的人体信息。虽然基于图像的HMR方法已经取得了令人印象深刻的结果，但在动态场景中恢复人体时常常遇到困难，由于缺乏人体运动导致时间不一致和非平滑的3D运动预测。相比之下，基于视频的方法利用时间信息来缓解这个问题。在本文中，我们提出了DiffMesh，这是一个创新的运动感知扩散式框架，用于基于视频的HMR。DiffMesh在扩散模型和人体运动之间建立了桥梁，通过将在扩散模型的前向过程和反向过程中融入人体运动，有效地生成准确和平滑的输出网格序列。在广泛使用的数据集（Human3.6M \cite{h36m_pami}和3DPW \cite{pw3d2018}）上进行了大量实验，证明了我们的DiffMesh的有效性和效率。在真实场景中的视觉对比进一步突出了DiffMesh在实际应用中的适用性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2303.13397v6">PDF</a> WACV 2025</p>
<p><strong>Summary</strong><br>人类网格恢复（HMR）为各种实际应用提供了丰富的人体信息。尽管基于图像的方法已取得了令人印象深刻的结果，但它们通常在动态场景中难以恢复人体信息，导致时间不一致和非平滑的3D运动预测。相反，基于视频的方法利用时间信息来缓解这个问题。本文提出了一种创新的运动感知扩散框架DiffMesh，用于基于视频的HMR。DiffMesh在扩散模型中建立了扩散模型与人类运动的桥梁，通过在前向过程和反向过程中融入人类运动，有效地生成准确和平滑的输出网格序列。在广泛使用数据集上的大量实验证明了DiffMesh的有效性和效率。在真实场景中的视觉对比进一步突出了DiffMesh在实际应用中的适用性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>HMR为多种实际应用提供了丰富的人体信息。</li>
<li>基于图像的方法在动态场景中恢复人体信息存在挑战，导致时间不一致和非平滑的3D运动预测。</li>
<li>基于视频的方法利用时间信息缓解上述问题。</li>
<li>DiffMesh是创新的运动感知扩散框架，用于基于视频的HMR。</li>
<li>DiffMesh在扩散模型中融入人类运动，生成准确、平滑的输出网格序列。</li>
<li>在广泛使用数据集上的实验证明了DiffMesh的有效性和效率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2303.13397">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-18\./crop_Diffusion Models/2303.13397v6/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-18\./crop_Diffusion Models/2303.13397v6/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-18\./crop_Diffusion Models/2303.13397v6/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-18\./crop_Diffusion Models/2303.13397v6/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-18\./crop_Diffusion Models/2303.13397v6/page_5_0.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-18/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-18/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-18/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-6ad5cbba534ecd3ac040c1cbe6809bd1.jpg" class="responsive-img" alt="医学图像">
                        
                        <span class="card-title">医学图像</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学图像 方向最新论文已更新，请持续关注 Update in 2025-01-18  SRE-Conv Symmetric Rotation Equivariant Convolution for Biomedical   Image Classification
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-18
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    医学图像
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">医学图像</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-18/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-b8ba69eab24fadcf9a82f4a069c47087.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF 方向最新论文已更新，请持续关注 Update in 2025-01-18  Normal-NeRF Ambiguity-Robust Normal Estimation for Highly Reflective   Scenes
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-18
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">26384.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
