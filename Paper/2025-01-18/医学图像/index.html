<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-18  SRE-Conv Symmetric Rotation Equivariant Convolution for Biomedical   Image Classification">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-6ad5cbba534ecd3ac040c1cbe6809bd1.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-18
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-18
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    80 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-01-18-æ›´æ–°"><a href="#2025-01-18-æ›´æ–°" class="headerlink" title="2025-01-18 æ›´æ–°"></a>2025-01-18 æ›´æ–°</h1><h2 id="SRE-Conv-Symmetric-Rotation-Equivariant-Convolution-for-Biomedical-Image-Classification"><a href="#SRE-Conv-Symmetric-Rotation-Equivariant-Convolution-for-Biomedical-Image-Classification" class="headerlink" title="SRE-Conv: Symmetric Rotation Equivariant Convolution for Biomedical   Image Classification"></a>SRE-Conv: Symmetric Rotation Equivariant Convolution for Biomedical   Image Classification</h2><p><strong>Authors:Yuexi Du, Jiazhen Zhang, Tal Zeevi, Nicha C. Dvornek, John A. Onofrey</strong></p>
<p>Convolutional neural networks (CNNs) are essential tools for computer vision tasks, but they lack traditionally desired properties of extracted features that could further improve model performance, e.g., rotational equivariance. Such properties are ubiquitous in biomedical images, which often lack explicit orientation. While current work largely relies on data augmentation or explicit modules to capture orientation information, this comes at the expense of increased training costs or ineffective approximations of the desired equivariance. To overcome these challenges, we propose a novel and efficient implementation of the Symmetric Rotation-Equivariant (SRE) Convolution (SRE-Conv) kernel, designed to learn rotation-invariant features while simultaneously compressing the model size. The SRE-Conv kernel can easily be incorporated into any CNN backbone. We validate the ability of a deep SRE-CNN to capture equivariance to rotation using the public MedMNISTv2 dataset (16 total tasks). SRE-Conv-CNN demonstrated improved rotated image classification performance accuracy on all 16 test datasets in both 2D and 3D images, all while increasing efficiency with fewer parameters and reduced memory footprint. The code is available at <a target="_blank" rel="noopener" href="https://github.com/XYPB/SRE-Conv">https://github.com/XYPB/SRE-Conv</a>. </p>
<blockquote>
<p>å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æ˜¯è®¡ç®—æœºè§†è§‰ä»»åŠ¡çš„é‡è¦å·¥å…·ï¼Œä½†å®ƒä»¬ç¼ºä¹èƒ½å¤Ÿè¿›ä¸€æ­¥æé«˜æ¨¡å‹æ€§èƒ½çš„æ‰€æå–ç‰¹å¾çš„ä¼ ç»Ÿæ‰€éœ€å±æ€§ï¼Œä¾‹å¦‚æ—‹è½¬ç­‰å˜æ€§ã€‚è¿™ç§å±æ€§åœ¨ç”Ÿç‰©åŒ»å­¦å›¾åƒä¸­æ™®éå­˜åœ¨ï¼Œè€Œç”Ÿç‰©åŒ»å­¦å›¾åƒå¾€å¾€ç¼ºä¹æ˜ç¡®çš„æ–¹ä½ã€‚å°½ç®¡å½“å‰çš„å·¥ä½œå¤§å¤šä¾èµ–äºæ•°æ®å¢å¼ºæˆ–æ˜¾å¼æ¨¡å—æ¥æ•è·æ–¹ä½ä¿¡æ¯ï¼Œä½†è¿™ä¼šå¢åŠ è®­ç»ƒæˆæœ¬æˆ–å¯¹æ‰€éœ€çš„ç­‰å˜æ€§çš„è¿‘ä¼¼æ— æ•ˆã€‚ä¸ºäº†å…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹é«˜æ•ˆçš„å¯¹ç§°æ—‹è½¬ç­‰å˜ï¼ˆSREï¼‰å·ç§¯ï¼ˆSRE-Convï¼‰å†…æ ¸çš„å®ç°æ–¹å¼ï¼Œæ—¨åœ¨å­¦ä¹ æ—‹è½¬ä¸å˜ç‰¹å¾çš„åŒæ—¶å‹ç¼©æ¨¡å‹å¤§å°ã€‚SRE-Convå†…æ ¸å¯ä»¥è½»æ¾é›†æˆåˆ°ä»»ä½•CNNä¸»å¹²ä¸­ã€‚æˆ‘ä»¬ä½¿ç”¨å…¬å…±MedMNISTv2æ•°æ®é›†ï¼ˆå…±16é¡¹ä»»åŠ¡ï¼‰éªŒè¯äº†æ·±åº¦SRE-CNNæ•è·æ—‹è½¬ç­‰å˜æ€§çš„èƒ½åŠ›ã€‚åœ¨16ä¸ªæµ‹è¯•æ•°æ®é›†çš„äºŒç»´å’Œä¸‰ç»´æ—‹è½¬å›¾åƒåˆ†ç±»ä¸­ï¼ŒSRE-Conv-CNNå‡è¡¨ç°å‡ºæ›´é«˜çš„åˆ†ç±»æ€§èƒ½å‡†ç¡®æ€§ï¼ŒåŒæ—¶æé«˜äº†æ•ˆç‡ï¼Œå‡å°‘äº†å‚æ•°å’Œå†…å­˜å ç”¨ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/XYPB/SRE-Conv%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/XYPB/SRE-Convæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.09753v1">PDF</a> Accepted by IEEE ISBI 2025 4-page paper</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„å¯¹ç§°æ—‹è½¬ç­‰å˜å·ç§¯ï¼ˆSRE-Convï¼‰æ ¸ï¼Œè¯¥æ ¸å¯è½»æ¾èå…¥ä»»ä½•CNNéª¨å¹²ç½‘ç»œï¼Œç”¨äºå­¦ä¹ æ—‹è½¬ä¸å˜ç‰¹å¾å¹¶å‹ç¼©æ¨¡å‹å¤§å°ã€‚åœ¨å…¬å…±MedMNISTv2æ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒSRE-Conv-CNNåœ¨æ—‹è½¬å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼ŒåŒæ—¶å‡å°‘äº†å‚æ•°å’Œå†…å­˜å ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬æ–‡ä»‹ç»äº†å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰åœ¨åŒ»å­¦å›¾åƒå¤„ç†ä¸­çš„å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†æ—‹è½¬ç­‰å˜æ€§é—®é¢˜æ—¶é¢ä¸´çš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹çš„å¯¹ç§°æ—‹è½¬ç­‰å˜å·ç§¯ï¼ˆSRE-Convï¼‰æ ¸ï¼Œæ—¨åœ¨è§£å†³è¿™äº›é—®é¢˜å¹¶æå‡æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>SRE-Convæ ¸å…·æœ‰å­¦ä¹ æ—‹è½¬ä¸å˜ç‰¹å¾çš„èƒ½åŠ›ï¼ŒåŒæ—¶å¯å‹ç¼©æ¨¡å‹å¤§å°ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSRE-Conv-CNNåœ¨æ—‹è½¬å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜äºä¼ ç»ŸCNNã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨æ‰€æœ‰æµ‹è¯•çš„16ä¸ªæ•°æ®é›†ä¸Šå‡è¡¨ç°å‡ºè¾ƒå¥½çš„æ€§èƒ½æå‡ï¼Œé€‚ç”¨äºäºŒç»´å’Œä¸‰ç»´å›¾åƒã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡å‡å°‘å‚æ•°å’Œå†…å­˜å ç”¨æé«˜äº†æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.09753">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-6bf7c9a3330d9b4f3649fe60b862d091.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ddbb06b5a8cceb28841b792d8ab889d2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-19991e4b8dc4f99a839d7cc3cb9f59ea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d5b3df35225b03268fda538cdec81847.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Unveiling-the-origin-of-XMM-Newton-soft-proton-flares-I-Design-and-validation-of-a-response-matrix-for-proton-spectral-analysis"><a href="#Unveiling-the-origin-of-XMM-Newton-soft-proton-flares-I-Design-and-validation-of-a-response-matrix-for-proton-spectral-analysis" class="headerlink" title="Unveiling the origin of XMM-Newton soft proton flares: I. Design and   validation of a response matrix for proton spectral analysis"></a>Unveiling the origin of XMM-Newton soft proton flares: I. Design and   validation of a response matrix for proton spectral analysis</h2><p><strong>Authors:Valentina Fioretti, Teresa Mineo, Simone Lotti, Silvano Molendi, Giorgio Lanzuisi, Roberta Amato, Claudio Macculi, Massimo Cappi, Mauro Dadina, Stefano Ettori, Fabio Gastaldello</strong></p>
<p>Low-energy (&lt;300 keV) protons entering the field of view of XMM-Newton are observed in the form of a sudden increase in the background level, the so-called soft proton flares, affecting up to 40% of the mission observing time. In-flight XMM-Newtonâ€™s observations of soft protons represent a unique laboratory to validate and improve our understanding of their interaction with the mirror, optical filters, and X-ray instruments. At the same time, such models would link the observed background flares to the primary proton population encountered by the telescope, converting XMM-Newton into a monitor for soft protons. We built a Geant4 simulation of XMM-Newton, including a verified mass model of the X-ray mirror, the focal plane assembly, and the EPIC MOS and pn-CCDs. We encoded the energy redistribution and proton transmission efficiency into a redistribution matrix file (RMF) and an auxiliary response file (ARF). For the validation, three averaged soft proton spectra, one for each filter configuration, were extracted from a collection of 13 years of MOS observations of the focused non X-ray background and analysed with Xspec. The best-fit model is in agreement with the power-law distribution predicted from independent measurements for the XMM-Newton orbit, spent mostly in the magnetosheath and nearby regions. For the first time we are able to link detected soft proton flares with the proton radiation environment in the Earthâ€™s magnetosphere, while proving the validity of the simulation chain in predicting the background of future missions. Benefiting from this work and contributions from the Athena instrument consortia, we also present the response files for the Athena mission and updated estimates for its focused charged background. </p>
<blockquote>
<p>ä½èƒ½é‡ï¼ˆ&lt;300 keVï¼‰è´¨å­è¿›å…¥XMM-Newtonçš„è§†é‡æ—¶ï¼Œä»¥èƒŒæ™¯æ°´å¹³çš„çªç„¶å¢åŠ çš„å½¢å¼è¢«è§‚å¯Ÿåˆ°ï¼Œè¿™å°±æ˜¯æ‰€è°“çš„è½¯è´¨å­è€€æ–‘ï¼Œå½±å“é«˜è¾¾40%çš„ä»»åŠ¡è§‚æµ‹æ—¶é—´ã€‚é£è¡Œä¸­çš„XMM-Newtonå¯¹è½¯è´¨å­çš„è§‚æµ‹ä¸ºæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªç‹¬ç‰¹çš„å®éªŒå®¤ï¼Œä»¥éªŒè¯å’Œæ”¹è¿›æˆ‘ä»¬å¯¹å®ƒä»¬ä¸é•œå­ã€å…‰å­¦è¿‡æ»¤å™¨ä»¥åŠXå°„çº¿ä»ªå™¨ç›¸äº’ä½œç”¨çš„äº†è§£ã€‚åŒæ—¶ï¼Œè¿™æ ·çš„æ¨¡å‹å°†è§‚å¯Ÿåˆ°çš„èƒŒæ™¯è€€æ–‘ä¸æœ›è¿œé•œé‡åˆ°çš„ä¸»è¦è´¨å­ç¾¤ä½“è”ç³»èµ·æ¥ï¼Œå°†XMM-Newtonè½¬åŒ–ä¸ºè½¯è´¨å­ç›‘æµ‹å™¨ã€‚æˆ‘ä»¬å»ºç«‹äº†Geant4æ¨¡æ‹Ÿçš„XMM-Newtonï¼ŒåŒ…æ‹¬ç»è¿‡éªŒè¯çš„Xå°„çº¿é•œå­çš„è´¨é‡æ¨¡å‹ã€ç„¦å¹³é¢ç»„ä»¶ä»¥åŠEPIC MOSå’Œpn-CCDã€‚æˆ‘ä»¬å°†èƒ½é‡å†åˆ†é…å’Œè´¨å­ä¼ è¾“æ•ˆç‡ç¼–ç ä¸ºå†åˆ†é…çŸ©é˜µæ–‡ä»¶ï¼ˆRMFï¼‰å’Œè¾…åŠ©å“åº”æ–‡ä»¶ï¼ˆARFï¼‰ã€‚ä¸ºäº†éªŒè¯ï¼Œæˆ‘ä»¬ä»13å¹´çš„MOSè§‚æµ‹ä¸­æŠ½å–äº†ä¸‰ç§å¹³å‡è½¯è´¨å­å…‰è°±ï¼ˆæ¯ç§è¿‡æ»¤å™¨é…ç½®ä¸€ç§ï¼‰ï¼Œå¹¶å¯¹å…¶è¿›è¡Œäº†åˆ†æã€‚æœ€ä½³æ‹Ÿåˆæ¨¡å‹ä¸XMM-Newtonè½¨é“çš„ç‹¬ç«‹æµ‹é‡æ‰€é¢„æµ‹å‡ºçš„å¹‚å¾‹åˆ†å¸ƒä¸€è‡´ï¼Œå…¶å¤§éƒ¨åˆ†æ—¶é—´éƒ½åœ¨ç£é˜å’Œé™„è¿‘åŒºåŸŸåº¦è¿‡ã€‚æˆ‘ä»¬é¦–æ¬¡èƒ½å¤Ÿå°†æ£€æµ‹åˆ°çš„è½¯è´¨å­è€€æ–‘ä¸åœ°çƒç£å±‚ä¸­çš„è´¨å­è¾å°„ç¯å¢ƒè”ç³»èµ·æ¥ï¼ŒåŒæ—¶è¯æ˜äº†æ¨¡æ‹Ÿé“¾åœ¨é¢„æµ‹æœªæ¥ä»»åŠ¡èƒŒæ™¯æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚å¾—ç›Šäºè¿™é¡¹å·¥ä½œä»¥åŠé›…å…¸å¨œä»ªå™¨è”åˆä¼šçš„è´¡çŒ®ï¼Œæˆ‘ä»¬è¿˜ä¸ºé›…å…¸å¨œä»»åŠ¡æä¾›äº†å“åº”æ–‡ä»¶ï¼Œå¹¶æ›´æ–°äº†å…¶èšç„¦å¸¦ç”µèƒŒæ™¯çš„ä¼°è®¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.09724v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>ä½èƒ½è´¨å­è¿›å…¥XMM-ç‰›é¡¿æœ›è¿œé•œè§†åœºæ—¶ï¼Œä¼šè§‚å¯Ÿåˆ°èƒŒæ™¯æ°´å¹³çªç„¶å¢åŠ ï¼Œå³æ‰€è°“çš„è½¯è´¨å­è€€æ–‘ï¼Œå½±å“é«˜è¾¾40%çš„ä»»åŠ¡è§‚æµ‹æ—¶é—´ã€‚åœ¨é£è¡Œè¿‡ç¨‹ä¸­ï¼ŒXMM-ç‰›é¡¿å¯¹è½¯è´¨å­çš„è§‚æµ‹ä¸ºæˆ‘ä»¬éªŒè¯å’Œæ”¹è¿›å¯¹é•œå­ã€å…‰å­¦è¿‡æ»¤å™¨ä»¥åŠXå°„çº¿ä»ªå™¨ä¸è´¨å­çš„ç›¸äº’ä½œç”¨çš„ç†è§£æä¾›äº†ç‹¬ç‰¹çš„å®éªŒå®¤ã€‚åŒæ—¶ï¼Œè¯¥æ¨¡å‹å°†è§‚æµ‹åˆ°çš„èƒŒæ™¯è€€æ–‘ä¸æœ›è¿œé•œé‡åˆ°çš„ä¸»è¦è´¨å­äººå£è”ç³»èµ·æ¥ï¼Œä½¿XMM-ç‰›é¡¿æˆä¸ºè½¯è´¨å­çš„ç›‘æµ‹å™¨ã€‚æˆ‘ä»¬å»ºç«‹äº†Geant4æ¨¡æ‹Ÿçš„XMM-ç‰›é¡¿æ¨¡å‹ï¼ŒåŒ…æ‹¬ç»éªŒè¯çš„Xå°„çº¿é•œã€ç„¦å¹³é¢ç»„ä»¶ä»¥åŠEPIC MOSå’Œpn-CCDçš„è´¨é‡æ¨¡å‹ã€‚æˆ‘ä»¬å°†èƒ½é‡å†åˆ†é…å’Œè´¨å­ä¼ è¾“æ•ˆç‡ç¼–ç ä¸ºå†åˆ†é…çŸ©é˜µæ–‡ä»¶ï¼ˆRMFï¼‰å’Œè¾…åŠ©å“åº”æ–‡ä»¶ï¼ˆARFï¼‰ã€‚ä¸ºäº†éªŒè¯ï¼Œæˆ‘ä»¬ä»13å¹´çš„MOSè§‚æµ‹ä¸­æŠ½å–äº†ä¸‰ç§å¹³å‡è½¯è´¨å­å…‰è°±ï¼Œæ¯ç§è¿‡æ»¤å™¨é…ç½®ä¸€ç§ï¼Œå¹¶ç”¨Xspecè¿›è¡Œäº†åˆ†æã€‚æœ€ä½³æ‹Ÿåˆæ¨¡å‹ä¸XMM-ç‰›é¡¿è½¨é“çš„ç‹¬ç«‹æµ‹é‡æ‰€é¢„æµ‹å‡ºçš„å¹‚å¾‹åˆ†å¸ƒä¸€è‡´ï¼Œå…¶å¤§éƒ¨åˆ†æ—¶é—´éƒ½åœ¨ç£é˜å’Œé™„è¿‘åŒºåŸŸåº¦è¿‡ã€‚æˆ‘ä»¬é¦–æ¬¡èƒ½å¤Ÿå°†æ£€æµ‹åˆ°çš„è½¯è´¨å­è€€æ–‘ä¸åœ°çƒç£å±‚ä¸­çš„è´¨å­è¾å°„ç¯å¢ƒè”ç³»èµ·æ¥ï¼ŒåŒæ—¶è¯æ˜äº†æ¨¡æ‹Ÿé“¾åœ¨é¢„æµ‹æœªæ¥ä»»åŠ¡èƒŒæ™¯æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚å¾—ç›Šäºè¿™é¡¹å·¥ä½œä»¥åŠé›…å…¸å¨œä»ªå™¨è”åˆä¼šçš„è´¡çŒ®ï¼Œæˆ‘ä»¬è¿˜ä¸ºé›…å…¸å¨œä»»åŠ¡æä¾›äº†å“åº”æ–‡ä»¶ï¼Œå¹¶æ›´æ–°äº†å…¶èšç„¦å¸¦ç”µèƒŒæ™¯çš„ä¼°è®¡ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ä½èƒ½è´¨å­è¿›å…¥XMM-ç‰›é¡¿æœ›è¿œé•œä¼šå¼•èµ·èƒŒæ™¯æ°´å¹³çªç„¶å¢åŠ ï¼Œå³è½¯è´¨å­è€€æ–‘ï¼Œå½±å“çº¦40%çš„è§‚æµ‹æ—¶é—´ã€‚</li>
<li>åœ¨è½¨XMM-ç‰›é¡¿å¯¹è½¯è´¨å­çš„è§‚æµ‹æä¾›äº†ä¸€ä¸ªéªŒè¯å’Œæ”¹è¿›è´¨å­ä¸ä»ªå™¨ç›¸äº’ä½œç”¨ç†è§£çš„å¹³å°ã€‚</li>
<li>æ„å»ºäº†åŒ…æ‹¬Xå°„çº¿é•œã€ç„¦å¹³é¢ç»„ä»¶ç­‰çš„Geant4æ¨¡æ‹ŸXMM-ç‰›é¡¿æ¨¡å‹ã€‚</li>
<li>é€šè¿‡ç¼–ç èƒ½é‡å†åˆ†é…å’Œè´¨å­ä¼ è¾“æ•ˆç‡ï¼Œåˆ›å»ºäº†RMFå’ŒARFæ–‡ä»¶ã€‚</li>
<li>é€šè¿‡Xspecåˆ†æï¼Œæœ€ä½³æ‹Ÿåˆæ¨¡å‹ä¸ç‹¬ç«‹æµ‹é‡é¢„æµ‹çš„å¹‚å¾‹åˆ†å¸ƒä¸€è‡´ã€‚</li>
<li>é¦–æ¬¡å°†è½¯è´¨å­è€€æ–‘ä¸åœ°çƒç£å±‚ä¸­çš„è´¨å­è¾å°„ç¯å¢ƒè”ç³»èµ·æ¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.09724">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c0b70c4e0dd8512fb59694b7b93e4ca3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-927197294308876fd24eca1c296049a8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c1225e4847944ad2a72953f5d0811701.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6f82bfa2f3ea6b5dc60be2feb2c41689.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-976559f53a2a5108f7297299596f52a4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a8c766c3718bda1b53143c655e9358ef.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b4751425cfbe3226687ec3b6b87f42a6.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Exploring-AI-based-System-Design-for-Pixel-level-Protected-Health-Information-Detection-in-Medical-Images"><a href="#Exploring-AI-based-System-Design-for-Pixel-level-Protected-Health-Information-Detection-in-Medical-Images" class="headerlink" title="Exploring AI-based System Design for Pixel-level Protected Health   Information Detection in Medical Images"></a>Exploring AI-based System Design for Pixel-level Protected Health   Information Detection in Medical Images</h2><p><strong>Authors:Tuan Truong, Ivo M. Baltruschat, Mark Klemens, Grit Werner, Matthias Lenga</strong></p>
<p>De-identification of medical images is a critical step to ensure privacy during data sharing in research and clinical settings. The initial step in this process involves detecting Protected Health Information (PHI), which can be found in image metadata or imprinted within image pixels. Despite the importance of such systems, there has been limited evaluation of existing AI-based solutions, creating barriers to the development of reliable and robust tools. In this study, we present an AI-based pipeline for PHI detection, comprising three key components: text detection, text extraction, and analysis of PHI content in medical images. By experimenting with exchanging roles of vision and language models within the pipeline, we evaluate the performance and recommend the best setup for the PHI detection task. </p>
<blockquote>
<p>åŒ»ç–—å›¾åƒçš„åŒ¿ååŒ–æ˜¯åœ¨ç ”ç©¶å’Œä¸´åºŠç¯å¢ƒä¸­è¿›è¡Œæ•°æ®å…±äº«æ—¶ç¡®ä¿éšç§çš„å…³é”®æ­¥éª¤ã€‚è¯¥è¿‡ç¨‹çš„åˆæ­¥æ­¥éª¤æ˜¯æ£€æµ‹å—ä¿æŠ¤çš„å«ç”Ÿä¿¡æ¯ï¼ˆPHIï¼‰ï¼Œè¿™äº›ä¿¡æ¯å¯èƒ½å­˜åœ¨äºå›¾åƒå…ƒæ•°æ®æˆ–å°åœ¨å›¾åƒåƒç´ ä¸­ã€‚å°½ç®¡è¿™äº›ç³»ç»Ÿå¾ˆé‡è¦ï¼Œä½†å¯¹ç°æœ‰çš„åŸºäºäººå·¥æ™ºèƒ½çš„è§£å†³æ–¹æ¡ˆçš„è¯„ä¼°ä»ç„¶æœ‰é™ï¼Œè¿™é˜»ç¢äº†å¯é å’Œç¨³å¥å·¥å…·çš„å¼€å‘ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºäººå·¥æ™ºèƒ½çš„PHIæ£€æµ‹æµæ°´çº¿ï¼ŒåŒ…å«ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šæ–‡æœ¬æ£€æµ‹ã€æ–‡æœ¬æå–å’ŒåŒ»ç–—å›¾åƒä¸­PHIå†…å®¹çš„åˆ†æã€‚é€šè¿‡å°è¯•åœ¨æµæ°´çº¿ä¸­äº¤æ¢è§†è§‰å’Œè¯­è¨€æ¨¡å‹çš„è§’è‰²ï¼Œæˆ‘ä»¬è¯„ä¼°äº†æ€§èƒ½å¹¶æ¨èäº†æœ€é€‚åˆPHIæ£€æµ‹ä»»åŠ¡çš„æœ€ä½³è®¾ç½®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.09552v1">PDF</a> In progress</p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒå»æ ‡è¯†åŒ–æ˜¯ç¡®ä¿ç ”ç©¶å’Œä¸´åºŠç¯å¢ƒä¸­æ•°æ®å…±äº«æ—¶éšç§å®‰å…¨çš„å…³é”®æ­¥éª¤ã€‚å…¶é¦–è¦æ­¥éª¤æ˜¯æ£€æµ‹å›¾åƒä¸­çš„å—ä¿æŠ¤å¥åº·ä¿¡æ¯ï¼ˆPHIï¼‰ï¼Œè¿™äº›ä¿¡æ¯å¯èƒ½éšè—åœ¨å›¾åƒå…ƒæ•°æ®æˆ–åƒç´ ä¸­ã€‚å°½ç®¡è¿™äº›ç³»ç»Ÿè‡³å…³é‡è¦ï¼Œä½†å¯¹ç°æœ‰åŸºäºäººå·¥æ™ºèƒ½çš„è§£å†³æ–¹æ¡ˆçš„è¯„ä¼°ä»æœ‰é™ï¼Œé˜»ç¢äº†å¯é ä¸”ç¨³å¥å·¥å…·çš„å¼€å‘ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºäººå·¥æ™ºèƒ½çš„PHIæ£€æµ‹æµç¨‹ï¼ŒåŒ…æ‹¬æ–‡æœ¬æ£€æµ‹ã€æ–‡æœ¬æå–å’ŒåŒ»å­¦å›¾åƒä¸­çš„PHIå†…å®¹åˆ†æä¸‰ä¸ªå…³é”®éƒ¨åˆ†ã€‚æˆ‘ä»¬é€šè¿‡å®éªŒéªŒè¯äº†åœ¨æµç¨‹ä¸­äº¤æ¢è§†è§‰å’Œè¯­è¨€æ¨¡å‹è§’è‰²çš„æ•ˆæœï¼Œè¯„ä¼°äº†æ€§èƒ½ï¼Œå¹¶æ¨èäº†æœ€é€‚åˆPHIæ£€æµ‹ä»»åŠ¡çš„æœ€ä½³é…ç½®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒå»æ ‡è¯†åŒ–æ˜¯ä¿æŠ¤éšç§çš„å…³é”®æ­¥éª¤ï¼Œç‰¹åˆ«æ˜¯åœ¨ç ”ç©¶å’Œä¸´åºŠç¯å¢ƒä¸­çš„æ•°æ®å…±äº«ä¸­ã€‚</li>
<li>å—ä¿æŠ¤å¥åº·ä¿¡æ¯ï¼ˆPHIï¼‰å¯éšè—åœ¨å›¾åƒå…ƒæ•°æ®æˆ–åƒç´ ä¸­ã€‚</li>
<li>å½“å‰å¯¹åŸºäºäººå·¥æ™ºèƒ½çš„PHIæ£€æµ‹è§£å†³æ–¹æ¡ˆçš„è¯„ä¼°æœ‰é™ï¼Œé˜»ç¢äº†å¯é å·¥å…·çš„å¼€å‘ã€‚</li>
<li>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºäººå·¥æ™ºèƒ½çš„PHIæ£€æµ‹æµç¨‹ï¼ŒåŒ…æ‹¬æ–‡æœ¬æ£€æµ‹ã€æ–‡æœ¬æå–å’ŒPHIå†…å®¹åˆ†æã€‚</li>
<li>é€šè¿‡å®éªŒéªŒè¯äº†ä¸åŒæ¨¡å‹è§’è‰²äº¤æ¢å¯¹PHIæ£€æµ‹æ€§èƒ½çš„å½±å“ã€‚</li>
<li>è¯„ä¼°äº†å„æµç¨‹é…ç½®çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.09552">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-5cb0864785a24714e1e198b7a117d78e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0de015dd590da0349d942aeac1ed126f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0e3f068566df9f668bbcbe6ccb322142.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ef429468edfdcc707a27c0047117d624.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-61fec017a206435e8b5834f66a7f4b77.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-72c9a398e949322b554e47b569f0994a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-43db639f07dc60f2afc9e62c337181bf.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="PISCO-Self-Supervised-k-Space-Regularization-for-Improved-Neural-Implicit-k-Space-Representations-of-Dynamic-MRI"><a href="#PISCO-Self-Supervised-k-Space-Regularization-for-Improved-Neural-Implicit-k-Space-Representations-of-Dynamic-MRI" class="headerlink" title="PISCO: Self-Supervised k-Space Regularization for Improved Neural   Implicit k-Space Representations of Dynamic MRI"></a>PISCO: Self-Supervised k-Space Regularization for Improved Neural   Implicit k-Space Representations of Dynamic MRI</h2><p><strong>Authors:Veronika Spieker, Hannah Eichhorn, Wenqi Huang, Jonathan K. Stelter, Tabita Catalan, Rickmer F. Braren, Daniel Rueckert, Francisco Sahli Costabal, Kerstin Hammernik, Dimitrios C. Karampinos, Claudia Prieto, Julia A. Schnabel</strong></p>
<p>Neural implicit k-space representations (NIK) have shown promising results for dynamic magnetic resonance imaging (MRI) at high temporal resolutions. Yet, reducing acquisition time, and thereby available training data, results in severe performance drops due to overfitting. To address this, we introduce a novel self-supervised k-space loss function $\mathcal{L}_\mathrm{PISCO}$, applicable for regularization of NIK-based reconstructions. The proposed loss function is based on the concept of parallel imaging-inspired self-consistency (PISCO), enforcing a consistent global k-space neighborhood relationship without requiring additional data. Quantitative and qualitative evaluations on static and dynamic MR reconstructions show that integrating PISCO significantly improves NIK representations. Particularly for high acceleration factors (R$\geq$54), NIK with PISCO achieves superior spatio-temporal reconstruction quality compared to state-of-the-art methods. Furthermore, an extensive analysis of the loss assumptions and stability shows PISCOâ€™s potential as versatile self-supervised k-space loss function for further applications and architectures. Code is available at: <a target="_blank" rel="noopener" href="https://github.com/compai-lab/2025-pisco-spieker">https://github.com/compai-lab/2025-pisco-spieker</a> </p>
<blockquote>
<p>ç¥ç»éšå¼kç©ºé—´è¡¨ç¤ºï¼ˆNIKï¼‰åœ¨åŠ¨æ€ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰çš„é«˜æ—¶é—´åˆ†è¾¨ç‡æ–¹é¢å±•ç°å‡ºæœ‰å‰æ™¯çš„ç»“æœã€‚ç„¶è€Œï¼Œå‡å°‘é‡‡é›†æ—¶é—´ä»¥åŠå¯ç”¨çš„è®­ç»ƒæ•°æ®å¯¼è‡´æ€§èƒ½å¤§å¹…åº¦ä¸‹é™ï¼Œå‡ºç°è¿‡åº¦æ‹Ÿåˆçš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹è‡ªç›‘ç£kç©ºé—´æŸå¤±å‡½æ•°$\mathcal{L}_\mathrm{PISCO}$ï¼Œå¯ç”¨äºNIKé‡å»ºçš„æ­£åˆ™åŒ–ã€‚æ‰€æå‡ºçš„æŸå¤±å‡½æ•°åŸºäºå¹¶è¡Œæˆåƒå¯å‘å¼çš„è‡ªä¸€è‡´æ€§ï¼ˆPISCOï¼‰æ¦‚å¿µï¼Œåœ¨ä¸è¦æ±‚é¢å¤–æ•°æ®çš„æƒ…å†µä¸‹å¼ºåˆ¶å®æ–½ä¸€è‡´çš„å…¨çƒkç©ºé—´é‚»åŸŸå…³ç³»ã€‚å¯¹é™æ€å’ŒåŠ¨æ€MRé‡å»ºçš„å®šé‡å’Œå®šæ€§è¯„ä¼°è¡¨æ˜ï¼Œæ•´åˆPISCOèƒ½æ˜¾è‘—æ”¹å–„NIKè¡¨ç¤ºã€‚ç‰¹åˆ«æ˜¯åœ¨é«˜åŠ é€Ÿå› å­ï¼ˆRâ‰¥54ï¼‰çš„æƒ…å†µä¸‹ï¼Œä½¿ç”¨PISCOçš„NIKåœ¨æ—¶ç©ºé‡å»ºè´¨é‡æ–¹é¢è¾¾åˆ°äº†ä¼˜äºæœ€æ–°æŠ€æœ¯çš„æ–¹æ³•ã€‚æ­¤å¤–ï¼Œå¯¹æŸå¤±å‡è®¾å’Œç¨³å®šæ€§çš„æ·±å…¥åˆ†æè¡¨æ˜ï¼ŒPISCOä½œä¸ºä¸€ç§é€šç”¨çš„è‡ªç›‘ç£kç©ºé—´æŸå¤±å‡½æ•°å…·æœ‰è¿›ä¸€æ­¥çš„ç”¨é€”å’Œæ¶æ„åº”ç”¨çš„æ½œåŠ›ã€‚ä»£ç å¯ä»ä»¥ä¸‹ç½‘ç«™è·å–ï¼š<a target="_blank" rel="noopener" href="https://github.com/compai-lab/2025-pisco-spieker">https://github.com/compai-lab/2025-pisco-spieker</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.09403v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºç¥ç»éšå¼kç©ºé—´è¡¨ç¤ºï¼ˆNIKï¼‰åœ¨åŠ¨æ€ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰çš„é«˜æ—¶é—´åˆ†è¾¨ç‡ä¸Šå±•ç°å‡ºè‰¯å¥½æ•ˆæœï¼Œä½†ç”±äºè®­ç»ƒæ•°æ®ä¸è¶³å¯¼è‡´çš„è¿‡æ‹Ÿåˆé—®é¢˜é™åˆ¶äº†å…¶æ€§èƒ½ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œå¼•å…¥äº†ä¸€ç§æ–°å‹è‡ªç›‘ç£kç©ºé—´æŸå¤±å‡½æ•°$\mathcal{L}_\mathrm{PISCO}$ï¼Œç”¨äºNIKé‡å»ºçš„æ­£åˆ™åŒ–ã€‚è¯¥æŸå¤±å‡½æ•°åŸºäºå¹¶è¡Œæˆåƒçš„è‡ªä¸€è‡´æ€§ï¼ˆPISCOï¼‰æ¦‚å¿µï¼Œæ— éœ€é¢å¤–æ•°æ®å³å¯å¼ºåˆ¶å®æ–½ä¸€è‡´çš„kç©ºé—´å…¨å±€é‚»åŸŸå…³ç³»ã€‚å®šé‡å’Œå®šæ€§è¯„ä¼°è¡¨æ˜ï¼Œé›†æˆPISCOå¯æ˜¾è‘—æ”¹å–„NIKè¡¨ç¤ºï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜åŠ é€Ÿå› å­ä¸‹ï¼ŒNIKä¸PISCOç»“åˆä½¿ç”¨å¯å®ç°ä¼˜äºç°æœ‰æ–¹æ³•çš„æ—¶ç©ºé‡å»ºè´¨é‡ã€‚æ­¤å¤–ï¼Œå¯¹æŸå¤±å‡è®¾å’Œç¨³å®šæ€§çš„æ·±å…¥åˆ†æè¡¨æ˜ï¼ŒPISCOä½œä¸ºé€šç”¨çš„è‡ªç›‘ç£kç©ºé—´æŸå¤±å‡½æ•°åœ¨å…¶ä»–åº”ç”¨å’Œæ¶æ„ä¸­å…·æœ‰æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>NIKåœ¨åŠ¨æ€MRIé«˜æ—¶é—´åˆ†è¾¨ç‡ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†è®­ç»ƒæ•°æ®ä¸è¶³ä¼šå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚</li>
<li>å¼•å…¥è‡ªç›‘ç£kç©ºé—´æŸå¤±å‡½æ•°$\mathcal{L}_\mathrm{PISCO}$ä»¥æ”¹å–„NIKçš„æ€§èƒ½ã€‚</li>
<li>PISCOåŸºäºå¹¶è¡Œæˆåƒçš„è‡ªä¸€è‡´æ€§æ¦‚å¿µï¼Œå¼ºåˆ¶å®æ–½kç©ºé—´å…¨å±€é‚»åŸŸå…³ç³»çš„ä¸€è‡´æ€§ã€‚</li>
<li>é›†æˆPISCOå¯ä»¥æ˜¾è‘—æé«˜NIKçš„é‡å»ºè´¨é‡ï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜åŠ é€Ÿå› å­ä¸‹ã€‚</li>
<li>ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒNIKç»“åˆPISCOå¯ä»¥å®ç°æ›´å¥½çš„æ—¶ç©ºé‡å»ºè´¨é‡ã€‚</li>
<li>PISCOä½œä¸ºé€šç”¨çš„è‡ªç›‘ç£kç©ºé—´æŸå¤±å‡½æ•°åœ¨å…¶ä»–åº”ç”¨å’Œæ¶æ„ä¸­å…·æœ‰æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.09403">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5f391add4c0cb59525041f6bd0cfe239.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2cf8b00a79c28841b75170143f8a1717.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ee6f038ac13b7309e0e267c9ba79d567.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-313c487f3a868019c31f0b0f6f3e0479.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0dd8c38e69f1a1fec8c9787dcae2104c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fc15061b6796de9a1cfb5ebba3ee6106.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Image-Segmentation-with-transformers-An-Overview-Challenges-and-Future"><a href="#Image-Segmentation-with-transformers-An-Overview-Challenges-and-Future" class="headerlink" title="Image Segmentation with transformers: An Overview, Challenges and Future"></a>Image Segmentation with transformers: An Overview, Challenges and Future</h2><p><strong>Authors:Deepjyoti Chetia, Debasish Dutta, Sanjib Kr Kalita</strong></p>
<p>Image segmentation, a key task in computer vision, has traditionally relied on convolutional neural networks (CNNs), yet these models struggle with capturing complex spatial dependencies, objects with varying scales, need for manually crafted architecture components and contextual information. This paper explores the shortcomings of CNN-based models and the shift towards transformer architectures -to overcome those limitations. This work reviews state-of-the-art transformer-based segmentation models, addressing segmentation-specific challenges and their solutions. The paper discusses current challenges in transformer-based segmentation and outlines promising future trends, such as lightweight architectures and enhanced data efficiency. This survey serves as a guide for understanding the impact of transformers in advancing segmentation capabilities and overcoming the limitations of traditional models. </p>
<blockquote>
<p>å›¾åƒåˆ†å‰²æ˜¯è®¡ç®—æœºè§†è§‰ä¸­çš„ä¸€é¡¹å…³é”®ä»»åŠ¡ï¼Œä¼ ç»Ÿä¸Šä¾èµ–äºå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨æ•æ‰å¤æ‚çš„ç©ºé—´ä¾èµ–æ€§ã€å¤„ç†ä¸åŒå°ºåº¦çš„ç‰©ä½“ã€éœ€è¦æ‰‹åŠ¨æ„å»ºæ¶æ„ç»„ä»¶å’Œä¸Šä¸‹æ–‡ä¿¡æ¯æ–¹é¢å­˜åœ¨å›°éš¾ã€‚æœ¬æ–‡æ¢è®¨äº†åŸºäºCNNçš„æ¨¡å‹çš„ä¸è¶³ï¼Œä»¥åŠè½¬å‘transformeræ¶æ„ä»¥å…‹æœè¿™äº›é™åˆ¶çš„è¶‹åŠ¿ã€‚æœ¬æ–‡å›é¡¾äº†æœ€å…ˆè¿›çš„åŸºäºtransformerçš„åˆ†å‰²æ¨¡å‹ï¼Œè§£å†³ç‰¹å®šçš„åˆ†å‰²æŒ‘æˆ˜åŠå…¶è§£å†³æ–¹æ¡ˆã€‚è®ºæ–‡è®¨è®ºäº†åŸºäºtransformerçš„åˆ†å‰²çš„å½“å‰æŒ‘æˆ˜ï¼Œå¹¶æ¦‚è¿°äº†æœ‰å‰é€”çš„æœªæ¥è¶‹åŠ¿ï¼Œä¾‹å¦‚è½»é‡çº§æ¶æ„å’Œå¢å¼ºçš„æ•°æ®æ•ˆç‡ã€‚è¿™ç¯‡ç»¼è¿°æœ‰åŠ©äºäº†è§£transformeråœ¨æé«˜åˆ†å‰²èƒ½åŠ›å’Œå…‹æœä¼ ç»Ÿæ¨¡å‹å±€é™æ€§æ–¹é¢çš„å½±å“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.09372v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†è®¡ç®—æœºè§†è§‰ä¸­å›¾åƒåˆ†å‰²ä»»åŠ¡çš„ä¼ ç»Ÿæ–¹æ³•ä¾èµ–çš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰çš„å±€é™æ€§ï¼ŒåŒ…æ‹¬éš¾ä»¥æ•æ‰å¤æ‚çš„ç©ºé—´ä¾èµ–æ€§ã€å¤„ç†ä¸åŒå°ºåº¦çš„å¯¹è±¡ã€éœ€è¦æ‰‹åŠ¨æ„å»ºæ¶æ„ç»„ä»¶å’Œä¸Šä¸‹æ–‡ä¿¡æ¯ç­‰é—®é¢˜ã€‚æœ¬æ–‡è½¬å‘æ¢ç´¢åŸºäºtransformerçš„æ¶æ„æ¥å…‹æœè¿™äº›é™åˆ¶ï¼Œè¯„è¿°äº†æœ€æ–°çš„åŸºäºtransformerçš„åˆ†å‰²æ¨¡å‹ï¼Œå¹¶é’ˆå¯¹åˆ†å‰²ç‰¹å®šæŒ‘æˆ˜åŠå…¶è§£å†³æ–¹æ¡ˆè¿›è¡Œäº†è®¨è®ºã€‚æœ¬æ–‡è¿˜è®¨è®ºäº†å½“å‰åŸºäºtransformerçš„åˆ†å‰²æ‰€é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¹¶æ¦‚è¿°äº†æœªæ¥æœ‰å‰æ™¯çš„è¶‹åŠ¿ï¼Œå¦‚è½»é‡åŒ–æ¶æ„å’Œæ•°æ®æ•ˆç‡çš„æé«˜ã€‚æœ¬æ–‡æ—¨åœ¨äº†è§£transformeråœ¨æå‡åˆ†å‰²èƒ½åŠ›æ–¹é¢çš„å½±å“å’Œå…‹æœä¼ ç»Ÿæ¨¡å‹çš„å±€é™æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å›¾åƒåˆ†å‰²æ˜¯è®¡ç®—æœºè§†è§‰ä¸­çš„å…³é”®ä»»åŠ¡ï¼Œä¼ ç»Ÿä¸Šä¾èµ–äºå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰ã€‚</li>
<li>CNNsåœ¨å¤„ç†å¤æ‚ç©ºé—´ä¾èµ–æ€§ã€ä¸åŒå°ºåº¦å¯¹è±¡å’Œä¸Šä¸‹æ–‡ä¿¡æ¯æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>åŸºäºtransformerçš„æ¶æ„è¢«æ¢ç´¢ä»¥å…‹æœCNNçš„å±€é™æ€§ã€‚</li>
<li>è®ºæ–‡è¯„è¿°äº†æœ€æ–°çš„åŸºäºtransformerçš„åˆ†å‰²æ¨¡å‹ï¼Œå¹¶è®¨è®ºäº†å®ƒä»¬å¦‚ä½•è§£å†³åˆ†å‰²ç‰¹å®šçš„æŒ‘æˆ˜ã€‚</li>
<li>å½“å‰åŸºäºtransformerçš„åˆ†å‰²ä»é¢ä¸´ä¸€äº›æŒ‘æˆ˜ã€‚</li>
<li>è®ºæ–‡æ¦‚è¿°äº†åŸºäºtransformerçš„æœªæ¥è¶‹åŠ¿ï¼ŒåŒ…æ‹¬è½»é‡åŒ–æ¶æ„å’Œæ•°æ®æ•ˆç‡çš„æé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.09372">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c4096607d904f77a6fa122b55567459a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-24f6587d527b76f00e0cd83aaa126227.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-59f612c5631cf86b95dafe587d97c7f8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8c8604c1b86c2ea49a6f40200178160e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fddd25890c35ec5a382e798edc337b90.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Shape-Based-Single-Object-Classification-Using-Ensemble-Method-Classifiers"><a href="#Shape-Based-Single-Object-Classification-Using-Ensemble-Method-Classifiers" class="headerlink" title="Shape-Based Single Object Classification Using Ensemble Method   Classifiers"></a>Shape-Based Single Object Classification Using Ensemble Method   Classifiers</h2><p><strong>Authors:Nur Shazwani Kamarudin, Mokhairi Makhtar, Syadiah Nor Wan Shamsuddin, Syed Abdullah Fadzli</strong></p>
<p>Nowadays, more and more images are available. Annotation and retrieval of the images pose classification problems, where each class is defined as the group of database images labelled with a common semantic label. Various systems have been proposed for content-based retrieval, as well as for image classification and indexing. In this paper, a hierarchical classification framework has been proposed for bridging the semantic gap effectively and achieving multi-category image classification. A well known pre-processing and post-processing method was used and applied to three problems; image segmentation, object identification and image classification. The method was applied to classify single object images from Amazon and Google datasets. The classification was tested for four different classifiers; BayesNetwork (BN), Random Forest (RF), Bagging and Vote. The estimated classification accuracies ranged from 20% to 99% (using 10-fold cross validation). The Bagging classifier presents the best performance, followed by the Random Forest classifier. </p>
<blockquote>
<p>å¦‚ä»Šï¼Œè¶Šæ¥è¶Šå¤šçš„å›¾åƒå¯ä¾›ä½¿ç”¨ã€‚å›¾åƒçš„æ ‡æ³¨å’Œæ£€ç´¢æ„æˆäº†åˆ†ç±»é—®é¢˜ï¼Œå…¶ä¸­æ¯ä¸ªç±»åˆ«è¢«å®šä¹‰ä¸ºæ•°æ®åº“ä¸­å…·æœ‰å…±åŒè¯­ä¹‰æ ‡ç­¾çš„å›¾åƒç»„ã€‚é’ˆå¯¹åŸºäºå†…å®¹çš„æ£€ç´¢ä»¥åŠå›¾åƒåˆ†ç±»å’Œç´¢å¼•ï¼Œå·²ç»æå‡ºäº†å„ç§ç³»ç»Ÿã€‚æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªå±‚æ¬¡åˆ†ç±»æ¡†æ¶ï¼Œä»¥æœ‰æ•ˆåœ°å¼¥åˆè¯­ä¹‰å·®è·å¹¶å®ç°å¤šç±»åˆ«å›¾åƒåˆ†ç±»ã€‚ä½¿ç”¨äº†ä¸€ç§ä¼—æ‰€å‘¨çŸ¥çš„é¢„å¤„ç†å’Œåå¤„ç†æ–¹æ³•ï¼Œå¹¶åº”ç”¨äºä¸‰ä¸ªé—®é¢˜ï¼šå›¾åƒåˆ†å‰²ã€å¯¹è±¡è¯†åˆ«å’Œå›¾åƒåˆ†ç±»ã€‚è¯¥æ–¹æ³•åº”ç”¨äºå¯¹äºšé©¬é€Šå’Œè°·æ­Œæ•°æ®é›†çš„å•ç›®æ ‡å›¾åƒè¿›è¡Œåˆ†ç±»ã€‚å¯¹å››ç§ä¸åŒçš„åˆ†ç±»å™¨è¿›è¡Œäº†æµ‹è¯•ï¼šBayesNetworkï¼ˆBNï¼‰ã€Random Forestï¼ˆRFï¼‰ã€Baggingå’ŒæŠ•ç¥¨ã€‚ä¼°è®¡çš„åˆ†ç±»å‡†ç¡®ç‡åœ¨20%åˆ°99%ä¹‹é—´ï¼ˆä½¿ç”¨10å€äº¤å‰éªŒè¯ï¼‰ã€‚Baggingåˆ†ç±»å™¨è¡¨ç°æœ€ä½³ï¼Œå…¶æ¬¡æ˜¯éšæœºæ£®æ—åˆ†ç±»å™¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.09311v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§å±‚æ¬¡åˆ†ç±»æ¡†æ¶ï¼Œç”¨äºæœ‰æ•ˆåœ°å¼¥åˆè¯­ä¹‰å·®è·å¹¶å®ç°å¤šç±»åˆ«å›¾åƒåˆ†ç±»ã€‚è¯¥æ–¹æ³•åº”ç”¨äºå›¾åƒåˆ†å‰²ã€å¯¹è±¡è¯†åˆ«å’Œå›¾åƒåˆ†ç±»ä¸‰ä¸ªé—®é¢˜ï¼Œå¹¶åº”ç”¨äºäºšé©¬é€Šå’Œè°·æ­Œæ•°æ®é›†çš„å•å¯¹è±¡å›¾åƒåˆ†ç±»ã€‚æµ‹è¯•äº†å››ç§ä¸åŒçš„åˆ†ç±»å™¨ï¼ŒåŒ…æ‹¬BayesNetworkã€Random Forestã€Baggingå’ŒVoteã€‚å…¶ä¸­Baggingåˆ†ç±»å™¨è¡¨ç°æœ€ä½³ï¼Œå…¶æ¬¡æ˜¯Random Foreståˆ†ç±»å™¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å±‚æ¬¡åˆ†ç±»æ¡†æ¶ç”¨äºå¤šç±»åˆ«å›¾åƒåˆ†ç±»ï¼Œæœ‰æ•ˆç¼©å°è¯­ä¹‰å·®è·ã€‚</li>
<li>è¯¥æ–¹æ³•åº”ç”¨äºå›¾åƒåˆ†å‰²ã€å¯¹è±¡è¯†åˆ«å’Œå›¾åƒåˆ†ç±»ã€‚</li>
<li>Baggingåˆ†ç±»å™¨åœ¨æµ‹è¯•ä¸­è¡¨ç°æœ€ä½³ã€‚</li>
<li>Random Foreståˆ†ç±»å™¨ç´§éšå…¶åã€‚</li>
<li>ä½¿ç”¨äº†é¢„å¤„ç†å’Œåå¤„ç†æ–¹æ³•ã€‚</li>
<li>åˆ†ç±»æµ‹è¯•æ˜¯åœ¨äºšé©¬é€Šå’Œè°·æ­Œæ•°æ®é›†çš„å•å¯¹è±¡å›¾åƒä¸Šè¿›è¡Œçš„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.09311">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1a4bdd2973ead2c6638230b449c4a3dc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-079e5cebe99b9dd3b7b4c8e43865153a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6b3e8802bc6ed8431fb0a5f6cbf6a85c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a588bb408296d899de1ea83931cc1736.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f1f92d13d4ff06bd19ad60581e3b1076.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-70eeca55d55de0b89e40cc6fbe7bd278.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9761d06867c1fd87780ec5beb82f5e84.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e6d7dcb442c6aaf6bced729cadab533d.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Efficient-Few-Shot-Medical-Image-Analysis-via-Hierarchical-Contrastive-Vision-Language-Learning"><a href="#Efficient-Few-Shot-Medical-Image-Analysis-via-Hierarchical-Contrastive-Vision-Language-Learning" class="headerlink" title="Efficient Few-Shot Medical Image Analysis via Hierarchical Contrastive   Vision-Language Learning"></a>Efficient Few-Shot Medical Image Analysis via Hierarchical Contrastive   Vision-Language Learning</h2><p><strong>Authors:Harrison Fuller, Fernando Gabriela Garcia, Victor Flores</strong></p>
<p>Few-shot learning in medical image classification presents a significant challenge due to the limited availability of annotated data and the complex nature of medical imagery. In this work, we propose Adaptive Vision-Language Fine-tuning with Hierarchical Contrastive Alignment (HiCA), a novel framework that leverages the capabilities of Large Vision-Language Models (LVLMs) for medical image analysis. HiCA introduces a two-stage fine-tuning strategy, combining domain-specific pretraining and hierarchical contrastive learning to align visual and textual representations at multiple levels. We evaluate our approach on two benchmark datasets, Chest X-ray and Breast Ultrasound, achieving state-of-the-art performance in both few-shot and zero-shot settings. Further analyses demonstrate the robustness, generalizability, and interpretability of our method, with substantial improvements in performance compared to existing baselines. Our work highlights the potential of hierarchical contrastive strategies in adapting LVLMs to the unique challenges of medical imaging tasks. </p>
<blockquote>
<p>åœ¨åŒ»å­¦å›¾åƒåˆ†ç±»ä¸­ï¼Œå°æ ·æœ¬å­¦ä¹ é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼Œå› ä¸ºæ ‡æ³¨æ•°æ®æœ‰é™ï¼Œä¸”åŒ»å­¦å›¾åƒå…·æœ‰å¤æ‚æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºå±‚æ¬¡å¯¹æ¯”å¯¹é½çš„é€‚åº”æ€§è§†è§‰è¯­è¨€å¾®è°ƒï¼ˆHiCAï¼‰æ–°å‹æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰è¿›è¡ŒåŒ»å­¦å›¾åƒåˆ†æã€‚HiCAå¼•å…¥äº†ä¸¤é˜¶æ®µçš„å¾®è°ƒç­–ç•¥ï¼Œç»“åˆé¢†åŸŸç‰¹å®šé¢„è®­ç»ƒå’Œå±‚æ¬¡å¯¹æ¯”å­¦ä¹ ï¼Œåœ¨å¤šçº§åˆ«å¯¹é½è§†è§‰å’Œæ–‡æœ¬è¡¨ç¤ºã€‚æˆ‘ä»¬åœ¨Chest Xå…‰ç‰‡å’Œä¹³è…ºè¶…å£°ä¸¤ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œåœ¨å°‘é‡æ ·æœ¬å’Œé›¶æ ·æœ¬è®¾ç½®ä¸­éƒ½è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è¿›ä¸€æ­¥çš„åˆ†æè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç¨³å¥æ€§ã€é€šç”¨æ€§å’Œå¯è§£é‡Šæ€§æ–¹é¢å…·æœ‰ä¼˜åŠ¿ï¼Œä¸ç°æœ‰åŸºçº¿ç›¸æ¯”åœ¨æ€§èƒ½ä¸Šæœ‰æ˜¾è‘—æ”¹è¿›ã€‚æˆ‘ä»¬çš„ç ”ç©¶çªå‡ºäº†å±‚æ¬¡å¯¹æ¯”ç­–ç•¥åœ¨é€‚åº”åŒ»å­¦æˆåƒä»»åŠ¡çš„ç‹¬ç‰¹æŒ‘æˆ˜ä¸­å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.09294v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†åŸºäºå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰çš„åŒ»å­¦å›¾åƒåˆ†ææ–°æ¡†æ¶â€”â€”è‡ªé€‚åº”è§†è§‰è¯­è¨€å¾®è°ƒä¸å±‚æ¬¡å¯¹æ¯”å¯¹é½ï¼ˆHiCAï¼‰ã€‚è¯¥æ¡†æ¶é‡‡ç”¨ä¸¤é˜¶æ®µå¾®è°ƒç­–ç•¥ï¼Œç»“åˆé¢†åŸŸç‰¹å®šé¢„è®­ç»ƒå’Œå±‚æ¬¡å¯¹æ¯”å­¦ä¹ ï¼Œå®ç°è§†è§‰å’Œæ–‡å­—çš„å¤šå±‚æ¬¡å¯¹é½ã€‚åœ¨Chest X-rayå’ŒBreast Ultrasoundä¸¤ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œè¯¥æ¡†æ¶åœ¨å°‘æ ·æœ¬å’Œæ— æ ·æœ¬åœºæ™¯ä¸‹å‡è¾¾åˆ°æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ã€‚è¿›ä¸€æ­¥çš„åˆ†æè¯æ˜äº†è¯¥æ–¹æ³•çš„ç¨³å¥æ€§ã€é€šç”¨æ€§å’Œå¯è§£é‡Šæ€§ï¼Œç›¸è¾ƒäºç°æœ‰åŸºçº¿æ–¹æ³•æœ‰æ˜æ˜¾çš„æ€§èƒ½æå‡ã€‚æœ¬ç ”ç©¶çªæ˜¾äº†å±‚æ¬¡å¯¹æ¯”ç­–ç•¥åœ¨é€‚åº”åŒ»å­¦æˆåƒä»»åŠ¡çš„ç‹¬ç‰¹æŒ‘æˆ˜æ–¹é¢çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†ç±»ä¸­çš„å°æ ·æœ¬å­¦ä¹ å› æ ‡æ³¨æ•°æ®æœ‰é™å’ŒåŒ»å­¦å›¾åƒå¤æ‚æ€§è€Œé¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†è‡ªé€‚åº”è§†è§‰è¯­è¨€å¾®è°ƒä¸å±‚æ¬¡å¯¹æ¯”å¯¹é½ï¼ˆHiCAï¼‰çš„æ–°æ¡†æ¶ã€‚</li>
<li>HiCAåˆ©ç”¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰è¿›è¡ŒåŒ»å­¦å›¾åƒåˆ†æã€‚</li>
<li>HiCAé‡‡ç”¨ä¸¤é˜¶æ®µå¾®è°ƒç­–ç•¥ï¼Œç»“åˆé¢†åŸŸç‰¹å®šé¢„è®­ç»ƒå’Œå±‚æ¬¡å¯¹æ¯”å­¦ä¹ ã€‚</li>
<li>åœ¨Chest X-rayå’ŒBreast Ultrasoundæ•°æ®é›†ä¸Šå®ç°äº†å…ˆè¿›æ€§èƒ½ã€‚</li>
<li>è¯¥æ–¹æ³•å…·æœ‰ç¨³å¥æ€§ã€é€šç”¨æ€§å’Œå¯è§£é‡Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.09294">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-feb68adb274eb01e606b2bd8c5e3883d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0a3da1b58e72be318fe279152124be15.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-61e80427d3f5d52d57807dcad1155940.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b6c2ba963e95be82e2f6516a94027b2b.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Knowledge-Distillation-for-Image-Restoration-Simultaneous-Learning-from-Degraded-and-Clean-Images"><a href="#Knowledge-Distillation-for-Image-Restoration-Simultaneous-Learning-from-Degraded-and-Clean-Images" class="headerlink" title="Knowledge Distillation for Image Restoration : Simultaneous Learning   from Degraded and Clean Images"></a>Knowledge Distillation for Image Restoration : Simultaneous Learning   from Degraded and Clean Images</h2><p><strong>Authors:Yongheng Zhang, Danfeng Yan</strong></p>
<p>Model compression through knowledge distillation has seen extensive application in classification and segmentation tasks. However, its potential in image-to-image translation, particularly in image restoration, remains underexplored. To address this gap, we propose a Simultaneous Learning Knowledge Distillation (SLKD) framework tailored for model compression in image restoration tasks. SLKD employs a dual-teacher, single-student architecture with two distinct learning strategies: Degradation Removal Learning (DRL) and Image Reconstruction Learning (IRL), simultaneously. In DRL, the student encoder learns from Teacher A to focus on removing degradation factors, guided by a novel BRISQUE extractor. In IRL, the student decoder learns from Teacher B to reconstruct clean images, with the assistance of a proposed PIQE extractor. These strategies enable the student to learn from degraded and clean images simultaneously, ensuring high-quality compression of image restoration models. Experimental results across five datasets and three tasks demonstrate that SLKD achieves substantial reductions in FLOPs and parameters, exceeding 80%, while maintaining strong image restoration performance. </p>
<blockquote>
<p>é€šè¿‡çŸ¥è¯†è’¸é¦è¿›è¡Œæ¨¡å‹å‹ç¼©åœ¨åˆ†ç±»å’Œåˆ†å‰²ä»»åŠ¡ä¸­å·²å¾—åˆ°å¹¿æ³›åº”ç”¨ã€‚ç„¶è€Œï¼Œå…¶åœ¨å›¾åƒåˆ°å›¾åƒçš„ç¿»è¯‘ï¼Œç‰¹åˆ«æ˜¯åœ¨å›¾åƒæ¢å¤ä¸­çš„æ½œåŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é’ˆå¯¹å›¾åƒæ¢å¤ä»»åŠ¡æ¨¡å‹å‹ç¼©çš„åŒæ­¥å­¦ä¹ çŸ¥è¯†è’¸é¦ï¼ˆSLKDï¼‰æ¡†æ¶ã€‚SLKDé‡‡ç”¨åŒæ•™å¸ˆã€å•å­¦ç”Ÿçš„æ¶æ„ï¼ŒåŒæ—¶é‡‡ç”¨ä¸¤ç§ç‹¬ç‰¹çš„å­¦ä¹ ç­–ç•¥ï¼šé€€åŒ–å»é™¤å­¦ä¹ ï¼ˆDRLï¼‰å’Œå›¾åƒé‡å»ºå­¦ä¹ ï¼ˆIRLï¼‰ã€‚åœ¨DRLä¸­ï¼Œå­¦ç”Ÿç¼–ç å™¨ä»æ•™å¸ˆAèº«ä¸Šå­¦ä¹ ï¼Œä¸“æ³¨äºå»é™¤é€€åŒ–å› ç´ ï¼Œç”±æ–°å‹BRISQUEæå–å™¨å¼•å¯¼ã€‚åœ¨IRLä¸­ï¼Œå­¦ç”Ÿè§£ç å™¨ä»æ•™å¸ˆBèº«ä¸Šå­¦ä¹ é‡å»ºæ¸…æ™°å›¾åƒï¼Œè¾…ä»¥æå‡ºçš„PIQEæå–å™¨ã€‚è¿™äº›ç­–ç•¥ä½¿å­¦ç”Ÿèƒ½å¤Ÿä»é€€åŒ–å’Œæ¸…æ™°çš„å›¾åƒä¸­åŒæ—¶å­¦ä¹ ï¼Œç¡®ä¿å›¾åƒæ¢å¤æ¨¡å‹çš„é«˜è´¨é‡å‹ç¼©ã€‚åœ¨äº”ä¸ªæ•°æ®é›†å’Œä¸‰ä¸ªä»»åŠ¡ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒSLKDåœ¨FLOPså’Œå‚æ•°æ–¹é¢å®ç°äº†è¶…è¿‡80%çš„å¤§å¹…å‡å°‘ï¼ŒåŒæ—¶ä¿æŒäº†å¼ºå¤§çš„å›¾åƒæ¢å¤æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.09268v1">PDF</a> Accepted by ICASSP2025</p>
<p><strong>Summary</strong></p>
<p>æ¨¡å‹å‹ç¼©çŸ¥è¯†è’¸é¦åœ¨åˆ†ç±»å’Œåˆ†å‰²ä»»åŠ¡ä¸­å¹¿æ³›åº”ç”¨ï¼Œä½†åœ¨å›¾åƒç¿»è¯‘ï¼Œç‰¹åˆ«æ˜¯å›¾åƒæ¢å¤æ–¹é¢çš„æ½œåŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºä¸€ç§é’ˆå¯¹å›¾åƒæ¢å¤ä»»åŠ¡çš„æ¨¡å‹å‹ç¼©çŸ¥è¯†è’¸é¦æ¡†æ¶â€”â€”åŒæ­¥å­¦ä¹ çŸ¥è¯†è’¸é¦ï¼ˆSLKDï¼‰ã€‚SLKDé‡‡ç”¨åŒæ•™å¸ˆã€å•å­¦ç”Ÿçš„æ¶æ„ï¼Œå¹¶èåˆä¸¤ç§ç‹¬ç‰¹å­¦ä¹ ç­–ç•¥ï¼šå»å™ªå­¦ä¹ ï¼ˆDRLï¼‰å’Œå›¾åƒé‡å»ºå­¦ä¹ ï¼ˆIRLï¼‰ã€‚DRLä½¿å­¦ç”Ÿç¼–ç å™¨ä»æ•™å¸ˆAä¸­å­¦ä¹ å»é™¤é€€åŒ–å› ç´ ï¼Œç”±æ–°å‹BRISQUEæå–å™¨å¼•å¯¼ï¼›è€ŒIRLä½¿å­¦ç”Ÿè§£ç å™¨ä»æ•™å¸ˆBä¸­å­¦ä¹ é‡å»ºæ¸…æ™°å›¾åƒï¼Œè¾…ä»¥æ–°æå‡ºçš„PIQEæå–å™¨ã€‚æ­¤ç­–ç•¥ä½¿å­¦ç”Ÿæ¨¡å‹å¯ä»é€€åŒ–åŠæ¸…æ™°å›¾åƒä¸­å­¦ä¹ ï¼Œç¡®ä¿å›¾åƒæ¢å¤æ¨¡å‹çš„é«˜è´¨é‡å‹ç¼©ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSLKDåœ¨äº”ä¸ªæ•°æ®é›†ã€ä¸‰é¡¹ä»»åŠ¡ä¸­å®ç°äº†è¶…è¿‡80%çš„FLOPså’Œå‚æ•°å¤§å¹…å‡å°‘ï¼ŒåŒæ—¶ä¿æŒå‡ºè‰²çš„å›¾åƒæ¢å¤æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>çŸ¥è¯†è’¸é¦åœ¨å›¾åƒæ¢å¤é¢†åŸŸçš„æ¨¡å‹å‹ç¼©å…·æœ‰è¾ƒå¤§æ½œåŠ›ï¼Œå°šæœªè¢«å……åˆ†æ¢ç´¢ã€‚</li>
<li>æå‡ºçš„SLKDæ¡†æ¶èåˆåŒæ•™å¸ˆã€å•å­¦ç”Ÿæ¶æ„ï¼Œä¸“ä¸ºå›¾åƒæ¢å¤ä»»åŠ¡è®¾è®¡ã€‚</li>
<li>SLKDåŒ…å«ä¸¤ç§å­¦ä¹ ç­–ç•¥ï¼šå»å™ªå­¦ä¹ ï¼ˆDRLï¼‰å’Œå›¾åƒé‡å»ºå­¦ä¹ ï¼ˆIRLï¼‰ã€‚</li>
<li>DRLç­–ç•¥ä½¿å­¦ç”Ÿç¼–ç å™¨ä»æ•™å¸ˆAå­¦ä¹ å»é™¤é€€åŒ–å› ç´ ï¼Œå€ŸåŠ©BRISQUEæå–å™¨ã€‚</li>
<li>IRLç­–ç•¥ä½¿å­¦ç”Ÿè§£ç å™¨ä»æ•™å¸ˆBå­¦ä¹ é‡å»ºæ¸…æ™°å›¾åƒï¼Œè¾…ä»¥PIQEæå–å™¨ã€‚</li>
<li>å­¦ç”Ÿæ¨¡å‹å¯ä»é€€åŒ–åŠæ¸…æ™°å›¾åƒä¸­å­¦ä¹ ï¼Œç¡®ä¿é«˜è´¨é‡å‹ç¼©å›¾åƒæ¢å¤æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.09268">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7faa231336060060aba7879d7c5d220b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8c6162544423f30898f3f787f3a1880a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ad5cbba534ecd3ac040c1cbe6809bd1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4af0fc3e390771d7a5fd225592bc73d9.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="XMM-HST-monitoring-of-the-ultra-soft-highly-accreting-Narrow-Line-Seyfert-1-RBS-1332"><a href="#XMM-HST-monitoring-of-the-ultra-soft-highly-accreting-Narrow-Line-Seyfert-1-RBS-1332" class="headerlink" title="XMM&#x2F;HST monitoring of the ultra-soft highly accreting Narrow Line   Seyfert 1 RBS 1332"></a>XMM&#x2F;HST monitoring of the ultra-soft highly accreting Narrow Line   Seyfert 1 RBS 1332</h2><p><strong>Authors:R. Middei, S. Barnier, F. G. Saturni, F. Ursini, P. -O. Petrucci, S. Bianchi, M. Cappi, M. Clavel, B. De Marco, A. De Rosa, G. Matt, G. A. Matzeu, M. Perri</strong></p>
<p>Ultra-soft narrow line Seyfert 1 (US-NLSy) are a poorly observed class of active galactic nuclei characterized by significant flux changes and an extreme soft X-ray excess. This peculiar spectral shape represents a golden opportunity to test whether the standard framework commonly adopted for modelling local AGN is still valid. We thus present the results on the joint XMM-Newton and HST monitoring campaign of the highly accreting US-NLSy RBS 1332. The optical-to-UV spectrum of RBS 1332 exhibits evidence of both a stratified narrow-line region and an ionized outflow, that produces absorption troughs over a wide range of velocities (from ~1500 km s-1 to ~1700 km s-1) in several high-ionization transitions (Lyalpha, N V, C IV). From a spectroscopic point of view, the optical&#x2F;UV&#x2F;FUV&#x2F;X-rays emission of this source is due to the superposition of three distinct components which are best modelled in the context of the two-coronae framework in which the radiation of RBS 1332 can be ascribed to a standard outer disk, a warm Comptonization region and a soft coronal continuum. The present dataset is not compatible with a pure relativistic reflection scenario. Finally, the adoption of the novel model reXcor allowed us to determine that the soft X-ray excess in RBS 1332 is dominated by the emission of the optically thick and warm Comptonizing medium, and only marginal contribution is expected from relativistic reflection from a lamppost-like corona. </p>
<blockquote>
<p>è¶…è½¯çª„çº¿å¡å¼—ç‰¹1å‹ï¼ˆUS-NLSyï¼‰æ˜¯ä¸€ç±»æ´»åŠ¨æ˜Ÿç³»æ ¸ï¼Œå…¶ç‰¹å¾ä¸ºæ˜¾è‘—çš„æµé‡å˜åŒ–å’Œæç«¯çš„è½¯Xå°„çº¿è¿‡å‰©ï¼Œè¿™ä¸€ç‰¹æ®Šå…‰è°±å½¢çŠ¶ä¸ºæµ‹è¯•æœ¬åœ°AGNå»ºæ¨¡çš„æ ‡å‡†æ¡†æ¶æ˜¯å¦ä»ç„¶æœ‰æ•ˆæä¾›äº†é»„é‡‘æœºä¼šã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯¹é«˜åº¦å¸æ”¶çš„US-NLSy RBS 1332è¿›è¡Œäº†è”åˆXMM-ç‰›é¡¿å’ŒHSTç›‘æµ‹æ´»åŠ¨çš„ç ”ç©¶ç»“æœè¿›è¡Œäº†ä»‹ç»ã€‚RBS 1332çš„å…‰å­¦è‡³ç´«å¤–çº¿å…‰è°±æ˜¾ç¤ºåˆ†å±‚çª„çº¿åŒºåŸŸå’Œç”µç¦»æµå‡ºçš„è¯æ®ï¼Œè¿™åœ¨å¤šç§é«˜ç”µç¦»è¿‡æ¸¡ä¸­äº§ç”Ÿäº†ä¸€ç³»åˆ—é€Ÿåº¦èŒƒå›´çš„å¸æ”¶è°·ï¼ˆä»çº¦1500å…¬é‡Œæ¯ç§’è‡³çº¦1700å…¬é‡Œæ¯ç§’ï¼‰ï¼ˆLyalphaï¼ŒNVï¼ŒCIVï¼‰ã€‚ä»å…‰è°±å­¦çš„è§’åº¦æ¥çœ‹ï¼Œè¯¥æºçš„å…‰å­¦&#x2F;ç´«å¤–çº¿&#x2F;è¿œç´«å¤–çº¿&#x2F;Xå°„çº¿å‘å°„æ˜¯ç”±äºä¸‰ä¸ªä¸åŒæˆåˆ†çš„å åŠ è€Œäº§ç”Ÿçš„ï¼Œæœ€å¥½åœ¨ä¸¤å±‚å† çŠ¶ç‰©çš„èƒŒæ™¯ä¸‹è¿›è¡Œå»ºæ¨¡ï¼Œå…¶ä¸­RBS 1332çš„è¾å°„å¯å½’å› äºå¤–éƒ¨ç£ç›˜çš„æ ‡å‡†éƒ¨åˆ†ã€æ¸©æš–çš„åº·æ™®é¡¿åŒ–åŒºåŸŸå’Œè½¯å† çŠ¶è¿ç»­ä½“ã€‚å½“å‰æ•°æ®é›†ä¸çº¯ç²¹çš„ç›¸å¯¹è®ºåå°„åœºæ™¯ä¸å…¼å®¹ã€‚æœ€åï¼Œé‡‡ç”¨æ–°å‹æ¨¡å‹ReXcorï¼Œæˆ‘ä»¬ç¡®å®šäº†RBS 1332ä¸­çš„è½¯Xå°„çº¿è¿‡å‰©ä¸»è¦ç”±å…‰å­¦åšä¸”æ¸©æš–çš„åº·æ™®é¡¿åŒ–ä»‹è´¨çš„å‘å°„å¼•èµ·ï¼Œä»…é¢„æœŸæ¥è‡ªç±»ä¼¼ç¯æŸ±çš„å† çŠ¶ç‰©çš„ç›¸å¯¹è®ºåå°„æœ‰è½»å¾®è´¡çŒ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.09220v1">PDF</a> 13 pages, 9 figures, accepted in A&amp;A</p>
<p><strong>Summary</strong><br>     ç ”ç©¶ç»“æœè¡¨æ˜RBS 1332çš„å…‰è°±ç”±ä¸‰éƒ¨åˆ†ç»„æˆï¼ŒåŒ…æ‹¬å¤–å±‚ç›˜ã€æš–åº·æ™®é¡¿åŒ–åŒºåŸŸå’Œè½¯å†•è¿ç»­è°±çš„è´¡çŒ®ã€‚æ­¤å¤–ï¼Œæ•°æ®æ’é™¤äº†çº¯ç›¸å¯¹è®ºåå°„æ¨¡å‹çš„å¯èƒ½æ€§ï¼Œå¹¶é‡‡ç”¨æ–°æ¨¡å‹ç¡®å®šè½¯Xå°„çº¿è¿‡å‰©ä¸»è¦ç”±åšçƒ­ä¸”æš–åŒ–çš„åº·æ™®é¡¿åŒ–ä»‹è´¨å‘å°„å¼•èµ·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RBS 1332æ˜¯ä¸€ç§é«˜åº¦å¸ç§¯çš„US-NLSyæ˜Ÿç³»æ ¸ï¼Œå…¶å…‰è°±è¡¨ç°å‡ºæ˜¾è‘—çš„å˜åŒ–å’Œæç«¯çš„è½¯Xå°„çº¿è¿‡å‰©ã€‚</li>
<li>RBS 1332çš„å…‰è°±ç”±ä¸‰éƒ¨åˆ†ç»„æˆï¼šå¤–å±‚ç›˜ã€æš–åº·æ™®é¡¿åŒ–åŒºåŸŸå’Œè½¯å†•è¿ç»­è°±ï¼Œè¿™åœ¨å…‰å­¦è‡³ç´«å¤–æ³¢æ®µéƒ½æœ‰ä½“ç°ã€‚</li>
<li>è¯¥æºè¡¨ç°å‡ºåˆ†å±‚çª„çº¿åŒºå’Œç¦»å­åŒ–æµå‡ºç‰©çš„è¯æ®ï¼Œå¸æ”¶æ§½åœ¨å®½èŒƒå›´é€Ÿåº¦ï¼ˆä»çº¦1500å…¬é‡Œæ¯ç§’åˆ°çº¦1700å…¬é‡Œæ¯ç§’ï¼‰å†…å‡ºç°å¤šä¸ªé«˜ç”µç¦»è¿‡æ¸¡ï¼ˆå¦‚Lyalphaã€NVã€CIVï¼‰ã€‚</li>
<li>æ•°æ®æ’é™¤äº†çº¯ç›¸å¯¹è®ºåå°„æ¨¡å‹çš„å¯èƒ½æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.09220">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-b36726b866ef7773987a54a339771f3f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-766dda76858fe09d798e173e48fe87b9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f5d41c43eded4a78171b8e2112ca011e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9414d0427d5b15f657a96a5cbf023e21.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-02450e979f21e0017eee1ec0f1b5cb4d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5be26dfa098152987e9c7a444059a4a7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ab97b16d94cde38321570d068975420b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fc1eca4d5c367b3526a8e7096c162912.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Cancer-Net-PCa-Seg-Benchmarking-Deep-Learning-Models-for-Prostate-Cancer-Segmentation-Using-Synthetic-Correlated-Diffusion-Imaging"><a href="#Cancer-Net-PCa-Seg-Benchmarking-Deep-Learning-Models-for-Prostate-Cancer-Segmentation-Using-Synthetic-Correlated-Diffusion-Imaging" class="headerlink" title="Cancer-Net PCa-Seg: Benchmarking Deep Learning Models for Prostate   Cancer Segmentation Using Synthetic Correlated Diffusion Imaging"></a>Cancer-Net PCa-Seg: Benchmarking Deep Learning Models for Prostate   Cancer Segmentation Using Synthetic Correlated Diffusion Imaging</h2><p><strong>Authors:Jarett Dewbury, Chi-en Amy Tai, Alexander Wong</strong></p>
<p>Prostate cancer (PCa) is the most prevalent cancer among men in the United States, accounting for nearly 300,000 cases, 29% of all diagnoses and 35,000 total deaths in 2024. Traditional screening methods such as prostate-specific antigen (PSA) testing and magnetic resonance imaging (MRI) have been pivotal in diagnosis, but have faced limitations in specificity and generalizability. In this paper, we explore the potential of enhancing PCa lesion segmentation using a novel MRI modality called synthetic correlated diffusion imaging (CDI$^s$). We employ several state-of-the-art deep learning models, including U-Net, SegResNet, Swin UNETR, Attention U-Net, and LightM-UNet, to segment PCa lesions from a 200 CDI$^s$ patient cohort. We find that SegResNet achieved superior segmentation performance with a Dice-Sorensen coefficient (DSC) of $76.68 \pm 0.8$. Notably, the Attention U-Net, while slightly less accurate (DSC $74.82 \pm 2.0$), offered a favorable balance between accuracy and computational efficiency. Our findings demonstrate the potential of deep learning models in improving PCa lesion segmentation using CDI$^s$ to enhance PCa management and clinical support. </p>
<blockquote>
<p>å‰åˆ—è…ºç™Œï¼ˆPCaï¼‰æ˜¯ç¾å›½ç”·æ€§ä¸­æœ€å¸¸è§çš„ç™Œç—‡ï¼Œ2024å¹´å°†è¿‘æœ‰30ä¸‡ä¾‹ç—…ä¾‹ï¼Œå æ‰€æœ‰è¯Šæ–­çš„29%ï¼Œä»¥åŠ3.5ä¸‡ä¾‹æ­»äº¡ç—…ä¾‹ã€‚ä¼ ç»Ÿçš„ç­›æŸ¥æ–¹æ³•ï¼Œå¦‚å‰åˆ—è…ºç‰¹å¼‚æ€§æŠ—åŸï¼ˆPSAï¼‰æ£€æµ‹å’Œç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰ï¼Œåœ¨è¯Šæ–­ä¸­å‘æŒ¥äº†å…³é”®ä½œç”¨ï¼Œä½†åœ¨ç‰¹å¼‚æ€§å’Œæ™®éæ€§æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ¢ç´¢äº†ä½¿ç”¨ä¸€ç§ç§°ä¸ºåˆæˆç›¸å…³æ‰©æ•£æˆåƒï¼ˆCDI$^s$ï¼‰çš„æ–°å‹MRIæ¨¡æ€æé«˜å‰åˆ—è…ºç™Œç—…å˜åˆ†å‰²çš„æ½œåŠ›ã€‚æˆ‘ä»¬é‡‡ç”¨äº†æœ€å…ˆè¿›çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼ŒåŒ…æ‹¬U-Netã€SegResNetã€Swin UNETRã€Attention U-Netå’ŒLightM-UNetï¼Œå¯¹200ä¾‹CDI$^s$æ‚£è€…çš„å‰åˆ—è…ºç™Œç—…å˜è¿›è¡Œåˆ†å‰²ã€‚æˆ‘ä»¬å‘ç°SegResNetçš„åˆ†å‰²æ€§èƒ½è¾ƒä¼˜ï¼ŒDice-Sorensenç³»æ•°ï¼ˆDSCï¼‰ä¸º$76.68 \pm 0.8$ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè™½ç„¶Attention U-Netçš„å‡†ç¡®åº¦ç¨ä½ï¼ˆDSC $74.82 \pm 2.0$ï¼‰ï¼Œä½†åœ¨å‡†ç¡®æ€§å’Œè®¡ç®—æ•ˆç‡ä¹‹é—´è¾¾åˆ°äº†æœ‰åˆ©çš„å¹³è¡¡ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨åˆ©ç”¨CDI$^s$æé«˜å‰åˆ—è…ºç™Œç—…å˜åˆ†å‰²æ–¹é¢å…·æœ‰æ½œåŠ›ï¼Œå¯æ”¹å–„å‰åˆ—è…ºç™Œç®¡ç†å’Œä¸´åºŠæ”¯æŒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.09185v1">PDF</a> 8 pages, 2 figures, to be published in Studies in Computational   Intelligence. This paper introduces Cancer-Net PCa-Seg, a comprehensive   evaluation of deep learning models for prostate cancer segmentation using   synthetic correlated diffusion imaging (CDI$^s$). We benchmark five   state-of-the-art architectures: U-Net, SegResNet, Swin UNETR, Attention   U-Net, and LightM-UNet</p>
<p><strong>Summary</strong></p>
<p>å‰åˆ—è…ºç™Œï¼ˆPCaï¼‰æ˜¯ç¾å›½ç”·æ€§æœ€å¸¸è§çš„ç™Œç—‡ä¹‹ä¸€ï¼Œé¢„è®¡åˆ°2024å¹´å°†å¯¼è‡´è¿‘30ä¸‡ä¾‹ç—…ä¾‹å’Œçº¦3ä¸‡äº”åƒäººæ­»äº¡ã€‚ä¼ ç»Ÿç­›æŸ¥æ–¹æ³•å¦‚å‰åˆ—è…ºç‰¹å¼‚æ€§æŠ—åŸï¼ˆPSAï¼‰æ£€æµ‹å’Œç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰å¯¹è¯Šæ–­è‡³å…³é‡è¦ï¼Œä½†åœ¨ç‰¹å¼‚æ€§å’Œé€šç”¨æ€§æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚æœ¬æ–‡æ¢è®¨äº†ä½¿ç”¨åˆæˆç›¸å…³æ‰©æ•£æˆåƒï¼ˆCDI$^s$ï¼‰è¿™ä¸€æ–°å‹MRIæ¨¡æ€æé«˜PCaç—…ç¶åˆ†å‰²çš„æ½œåŠ›ã€‚åˆ©ç”¨å¤šç§å…ˆè¿›çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼ŒåŒ…æ‹¬U-Netã€SegResNetã€Swin UNETRã€Attention U-Netå’ŒLightM-UNetï¼Œä»ä¸€ç»„åŒ…å«200ä¸ªCDI$^s$æ‚£è€…çš„é˜Ÿåˆ—ä¸­åˆ†å‰²PCaç—…ç¶ã€‚ç ”ç©¶å‘ç°ï¼ŒSegResNetçš„åˆ†å‰²æ€§èƒ½æœ€ä½³ï¼ŒDice-SÃ¸rensenç³»æ•°ï¼ˆDSCï¼‰ä¸º$76.68 \pm 0.8$ã€‚å°½ç®¡Attention U-Netçš„å‡†ç¡®åº¦ç¨ä½ï¼ˆDSC $74.82 \pm 2.0$ï¼‰ï¼Œä½†å…¶å‡†ç¡®æ€§å’Œè®¡ç®—æ•ˆç‡ä¹‹é—´çš„å¹³è¡¡è¾ƒä¸ºç†æƒ³ã€‚ç ”ç©¶ç»“æœè¡¨æ˜æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨åˆ©ç”¨CDI$^s$æé«˜PCaç—…ç¶åˆ†å‰²æ–¹é¢å…·æœ‰æ½œåŠ›ï¼Œæœ‰æœ›æ”¹å–„PCaç®¡ç†å’Œä¸´åºŠæ”¯æŒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å‰åˆ—è…ºç™Œæ˜¯ç¾å›½ç”·æ€§æœ€å¸¸è§çš„ç™Œç—‡ä¹‹ä¸€ï¼Œé¢„è®¡åˆ°2024å¹´å°†å¯¼è‡´å¤§é‡æ–°å¢ç—…ä¾‹å’Œæ­»äº¡ã€‚</li>
<li>ä¼ ç»Ÿç­›æŸ¥æ–¹æ³•å¦‚PSAæ£€æµ‹å’ŒMRIåœ¨å‰åˆ—è…ºç™Œè¯Šæ–­ä¸­å…·æœ‰é‡è¦æ„ä¹‰ï¼Œä½†å­˜åœ¨ç‰¹å¼‚æ€§å’Œé€šç”¨æ€§æ–¹é¢çš„å±€é™æ€§ã€‚</li>
<li>åˆæˆç›¸å…³æ‰©æ•£æˆåƒï¼ˆCDI$^s$ï¼‰æ˜¯ä¸€ç§æ–°å‹MRIæ¨¡æ€ï¼Œåœ¨å‰åˆ—è…ºç™Œç—…ç¶åˆ†å‰²æ–¹é¢å…·æœ‰æ½œåœ¨åº”ç”¨ä»·å€¼ã€‚</li>
<li>æ·±åº¦å­¦ä¹ æ¨¡å‹å¦‚SegResNetå’ŒAttention U-Netåœ¨CDI$^s$å›¾åƒä¸Šè¡¨ç°å‡ºè¾ƒå¥½çš„å‰åˆ—è…ºç™Œç—…ç¶åˆ†å‰²æ€§èƒ½ã€‚</li>
<li>SegResNetåœ¨åˆ†å‰²æ€§èƒ½ä¸Šè¾¾åˆ°è¾ƒé«˜çš„Dice-SÃ¸rensenç³»æ•°ï¼ˆDSC $76.68 \pm 0.8$ï¼‰ã€‚</li>
<li>Attention U-Netåœ¨å‡†ç¡®æ€§å’Œè®¡ç®—æ•ˆç‡ä¹‹é—´è¾¾åˆ°å¹³è¡¡ï¼Œè™½ç„¶å…¶DSCç•¥ä½ä½†ä»å…·æœ‰å®é™…åº”ç”¨ä»·å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.09185">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6176e6e3fdfa39b3ded11cbd2752219a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3526610fb289407b6f4c408bc40e2ebd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4b0a92e1b9cf1747c055587488a4d17e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7f65fbf10aa50a644eb3bcf9282019d0.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="A-Vessel-Bifurcation-Landmark-Pair-Dataset-for-Abdominal-CT-Deformable-Image-Registration-DIR-Validation"><a href="#A-Vessel-Bifurcation-Landmark-Pair-Dataset-for-Abdominal-CT-Deformable-Image-Registration-DIR-Validation" class="headerlink" title="A Vessel Bifurcation Landmark Pair Dataset for Abdominal CT Deformable   Image Registration (DIR) Validation"></a>A Vessel Bifurcation Landmark Pair Dataset for Abdominal CT Deformable   Image Registration (DIR) Validation</h2><p><strong>Authors:Edward R Criscuolo, Yao Hao, Zhendong Zhang, Trevor McKeown, Deshan Yang</strong></p>
<p>Deformable image registration (DIR) is an enabling technology in many diagnostic and therapeutic tasks. Despite this, DIR algorithms have limited clinical use, largely due to a lack of benchmark datasets for quality assurance during development. To support future algorithm development, here we introduce our first-of-its-kind abdominal CT DIR benchmark dataset, comprising large numbers of highly accurate landmark pairs on matching blood vessel bifurcations. Abdominal CT image pairs of 30 patients were acquired from several public repositories as well as the authorsâ€™ institution with IRB approval. The two CTs of each pair were originally acquired for the same patient on different days. An image processing workflow was developed and applied to each image pair: 1) Abdominal organs were segmented with a deep learning model, and image intensity within organ masks was overwritten. 2) Matching image patches were manually identified between two CTs of each image pair 3) Vessel bifurcation landmarks were labeled on one image of each image patch pair. 4) Image patches were deformably registered, and landmarks were projected onto the second image. 5) Landmark pair locations were refined manually or with an automated process. This workflow resulted in 1895 total landmark pairs, or 63 per case on average. Estimates of the landmark pair accuracy using digital phantoms were 0.7+&#x2F;-1.2mm. The data is published in Zenodo at <a target="_blank" rel="noopener" href="https://doi.org/10.5281/zenodo.14362785">https://doi.org/10.5281/zenodo.14362785</a>. Instructions for use can be found at <a target="_blank" rel="noopener" href="https://github.com/deshanyang/Abdominal-DIR-QA">https://github.com/deshanyang/Abdominal-DIR-QA</a>. This dataset is a first-of-its-kind for abdominal DIR validation. The number, accuracy, and distribution of landmark pairs will allow for robust validation of DIR algorithms with precision beyond what is currently available. </p>
<blockquote>
<p>å¯å˜å½¢å›¾åƒé…å‡†ï¼ˆDIRï¼‰æ˜¯è®¸å¤šè¯Šæ–­å’Œæ²»ç–—ä»»åŠ¡ä¸­çš„å…³é”®æŠ€æœ¯ã€‚å°½ç®¡å¦‚æ­¤ï¼ŒDIRç®—æ³•åœ¨ä¸´åºŠåº”ç”¨ä¸­çš„ä½¿ç”¨ä»ç„¶æœ‰é™ï¼Œè¿™ä¸»è¦æ˜¯å› ä¸ºå¼€å‘è¿‡ç¨‹ä¸­ç¼ºä¹ç”¨äºè´¨é‡ä¿è¯çš„åŸºå‡†æ•°æ®é›†ã€‚ä¸ºäº†æ”¯æŒæœªæ¥çš„ç®—æ³•å¼€å‘ï¼Œæˆ‘ä»¬åœ¨æ­¤ä»‹ç»äº†æˆ‘ä»¬é¦–åˆ›çš„è…¹éƒ¨CT DIRåŸºå‡†æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«å¤§é‡å…³äºåŒ¹é…è¡€ç®¡åˆ†å‰çš„å‡†ç¡®åœ°æ ‡å¯¹ã€‚30åæ‚£è€…çš„è…¹éƒ¨CTå›¾åƒå¯¹æ˜¯ä»ä¸€äº›å…¬å…±ä»“åº“ä»¥åŠä½œè€…æœºæ„è·å¾—ï¼Œå¹¶ä¸”è·å¾—äº†IRBæ‰¹å‡†ã€‚æ¯å¯¹ä¸­çš„ä¸¤ä¸ªCTæœ€åˆæ˜¯åœ¨ä¸åŒçš„æ—¥å­ä¸ºåŒä¸€æ‚£è€…è·å–çš„ã€‚é’ˆå¯¹æ¯å¯¹å›¾åƒå¼€å‘å¹¶åº”ç”¨äº†ä¸€ä¸ªå›¾åƒå¤„ç†å·¥ä½œæµç¨‹ï¼š1) ä½¿ç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹åˆ†å‰²è…¹éƒ¨å™¨å®˜ï¼Œå¹¶è¦†ç›–å™¨å®˜æ©è†œå†…çš„å›¾åƒå¼ºåº¦ã€‚2)åœ¨ä¸¤ä¸ªCTä¹‹é—´æ‰‹åŠ¨è¯†åˆ«åŒ¹é…å›¾åƒå—ã€‚3)åœ¨æ¯ä¸ªå›¾åƒå—å¯¹ä¸Šçš„ä¸€å¹…å›¾åƒä¸Šæ ‡æ³¨è¡€ç®¡åˆ†å‰åœ°æ ‡ã€‚4)å¯å˜å½¢åœ°é…å‡†å›¾åƒå—ï¼Œå¹¶å°†åœ°æ ‡æŠ•å½±åˆ°ç¬¬äºŒå¹…å›¾åƒä¸Šã€‚5)é€šè¿‡æ‰‹åŠ¨æˆ–è‡ªåŠ¨åŒ–è¿‡ç¨‹å¯¹åœ°æ ‡å¯¹ä½ç½®è¿›è¡Œå¾®è°ƒã€‚è¿™ä¸€å·¥ä½œæµç¨‹äº§ç”Ÿäº†æ€»å…±1895ä¸ªåœ°æ ‡å¯¹ï¼Œå¹³å‡æ¯ä¸ªç—…ä¾‹æœ‰63ä¸ªã€‚ä½¿ç”¨æ•°å­—å¹»å½±ä¼°è®¡åœ°æ ‡å¯¹çš„ç²¾åº¦ä¸º0.7Â±1.2æ¯«ç±³ã€‚æ•°æ®å·²åœ¨Zenodoä¸Šå‘è¡¨ï¼Œå¯é€šè¿‡<a target="_blank" rel="noopener" href="https://doi.org/10.5281/zenodo.14362785%E8%AE%BF%E9%97%AE%E3%80%82%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E%E5%8F%AF%E5%9C%A8https://github.com/deshanyang/Abdominal-DIR-QA%E6%89%BE%E5%88%B0%E3%80%82%E8%AF%A5%E6%95%B0%E6%8D%AE%E9%9B%86%E6%98%AF%E8%85%B9%E9%83%A8DIR%E9%AA%8C%E8%AF%81%E7%9A%84%E9%A6%96%E5%88%9B%EF%BC%8C%E5%9C%B0%E6%A0%87%E5%AF%B9%E7%9A%84%E6%95%B0%E9%87%8F%E3%80%81%E5%87%86%E7%A1%AE%E6%80%A7%E5%92%8C%E5%88%86%E5%B8%83%E5%B0%86%E5%85%81%E8%AE%B8%E5%AF%B9DIR%E7%AE%97%E6%B3%95%E8%BF%9B%E8%A1%8C%E7%A8%B3%E5%81%A5%E9%AA%8C%E8%AF%81%EF%BC%8C%E5%85%B6%E7%B2%BE%E7%A1%AE%E5%BA%A6%E8%B6%85%E8%B6%8A%E4%BA%86%E5%BD%93%E5%89%8D%E5%8F%AF%E7%94%A8%E7%9A%84%E6%B0%B4%E5%B9%B3%E3%80%82">https://doi.org/10.5281/zenodo.14362785è®¿é—®ã€‚ä½¿ç”¨è¯´æ˜å¯åœ¨https://github.com/deshanyang/Abdominal-DIR-QAæ‰¾åˆ°ã€‚è¯¥æ•°æ®é›†æ˜¯è…¹éƒ¨DIRéªŒè¯çš„é¦–åˆ›ï¼Œåœ°æ ‡å¯¹çš„æ•°é‡ã€å‡†ç¡®æ€§å’Œåˆ†å¸ƒå°†å…è®¸å¯¹DIRç®—æ³•è¿›è¡Œç¨³å¥éªŒè¯ï¼Œå…¶ç²¾ç¡®åº¦è¶…è¶Šäº†å½“å‰å¯ç”¨çš„æ°´å¹³ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.09162v1">PDF</a> 19 pages, 3 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªç”¨äºè¯„ä¼°è…¹éƒ¨å¯å˜å½¢å›¾åƒé…å‡†ï¼ˆDIRï¼‰ç®—æ³•çš„æ–°é¢–åŸºå‡†æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†åŒ…å«äº†å¤§é‡æ‚£è€…çš„é«˜ç²¾åº¦åœ°æ ‡é…å¯¹ï¼Œè¿™äº›åœ°æ ‡ä½äºåŒ¹é…çš„è¡€ç®¡åˆ†å‰å¤„ã€‚æ•°æ®é›†çš„åˆ›å»ºè¿‡ç¨‹åŒ…æ‹¬å›¾åƒåˆ†å‰²ã€å›¾åƒå¼ºåº¦å¤„ç†ã€æ‰‹åŠ¨æ ‡è¯†åŒ¹é…å›¾åƒå—ã€æ ‡æ³¨è¡€ç®¡åˆ†å‰åœ°æ ‡ã€å¯å˜å½¢å›¾åƒé…å‡†ä»¥åŠåœ°æ ‡æŠ•å½±ç­‰æ­¥éª¤ã€‚è¯¥æ•°æ®é›†ä¸ºè…¹éƒ¨DIRéªŒè¯æä¾›äº†ç‹¬ç‰¹çš„èµ„æºï¼Œå…¶åœ°æ ‡å¯¹çš„æ•°é‡ã€å‡†ç¡®æ€§å’Œåˆ†å¸ƒä¸ºDIRç®—æ³•çš„ç¨³å¥éªŒè¯æä¾›äº†è¶…è¶Šç°æœ‰æ°´å¹³çš„æ–°å·¥å…·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†ç”¨äºå¼€å‘è…¹éƒ¨CTå¯å˜å½¢å›¾åƒé…å‡†ï¼ˆDIRï¼‰åŸºå‡†æ•°æ®é›†çš„ç ”ç©¶ã€‚</li>
<li>æ•°æ®é›†åŒ…å«é«˜åº¦å‡†ç¡®çš„è¡€ç®¡åˆ†å‰åœ°æ ‡é…å¯¹ã€‚</li>
<li>æ•°æ®é›†åˆ›å»ºè¿‡ç¨‹åŒ…æ‹¬å›¾åƒåˆ†å‰²ã€åŒ¹é…å›¾åƒå—æ ‡è¯†ã€åœ°æ ‡æ ‡æ³¨ã€å›¾åƒé…å‡†å’Œåœ°æ ‡æŠ•å½±ç­‰æ­¥éª¤ã€‚</li>
<li>è¯¥æ•°æ®é›†æ˜¯é¦–ä¸ªç”¨äºè…¹éƒ¨DIRéªŒè¯çš„æ•°æ®é›†ã€‚</li>
<li>åœ°æ ‡å¯¹çš„æ•°é‡ã€å‡†ç¡®æ€§å’Œåˆ†å¸ƒå…è®¸å¯¹DIRç®—æ³•è¿›è¡Œç¨³å¥éªŒè¯ã€‚</li>
<li>æ•°æ®é›†å·²åœ¨Zenodoä¸Šå‘å¸ƒï¼Œå¹¶æä¾›ä½¿ç”¨è¯´æ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.09162">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-48ff151b0d1f42db46ab362a26ba6723.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Cosmology-from-UNIONS-weak-lensing-profiles-of-galaxy-clusters"><a href="#Cosmology-from-UNIONS-weak-lensing-profiles-of-galaxy-clusters" class="headerlink" title="Cosmology from UNIONS weak lensing profiles of galaxy clusters"></a>Cosmology from UNIONS weak lensing profiles of galaxy clusters</h2><p><strong>Authors:Charlie T. Mpetha, James E. Taylor, Yuba Amoura, Roan Haggar, Thomas de Boer, Sacha Guerrini, Axel Guinot, Fabian Hervas Peters, Hendrik Hildebrandt, Michael J. Hudson, Martin Kilbinger, Tobias Liaudat, Alan McConnachie, Ludovic Van Waerbeke, Anna Wittje</strong></p>
<p>Cosmological information is encoded in the structure of galaxy clusters. In Universes with less matter and larger initial density perturbations, clusters form earlier and have more time to accrete material, leading to a more extended infall region. Thus, measuring the mean mass distribution in the infall region provides a novel cosmological test. The infall region is largely insensitive to baryonic physics, and provides a cleaner structural test than other measures of cluster assembly time such as concentration. We consider cluster samples from three publicly available galaxy cluster catalogues: the Spectrsopic Identification of eROSITA Sources (SPIDERS) catalogue, the X-ray and Sunyaev-Zeldovich effect selected clusters in the meta-catalogue M2C, and clusters identified in the Dark Energy Spectroscopic Instrument (DESI) Legacy Imaging Survey. Using a preliminary shape catalogue from the Ultraviolet Near Infrared Optical Northern Survey (UNIONS), we derive excess surface mass density profiles for each sample. We then compare the mean profile for the DESI Legacy sample, which is the most complete, to predictions from a suite of simulations covering a range of $\Omega_{\rm m}$ and $\sigma_8$, obtaining constraints of $\Omega_{\rm m}&#x3D;0.29\pm 0.05$ and $\sigma_8&#x3D;0.80 \pm 0.04$. We also measure mean (comoving) splashback radii for SPIDERS, M2C and DESI Legacy Imaging Survey clusters of $1.59^{+0.16}<em>{-0.13} {\rm cMpc}&#x2F;h$, $1.30^{+0.25}</em>{-0.13} {\rm cMpc}&#x2F;h$ and $1.45\pm0.11 {\rm cMpc}&#x2F;h$ respectively. Performing this analysis with the final UNIONS shape catalogue and the full sample of spectroscopically observed clusters in DESI, we can expect to improve on the best current constraints from cluster abundance studies by a factor of 2 or more. </p>
<blockquote>
<p>å®‡å®™å­¦ä¿¡æ¯è¢«ç¼–ç åœ¨æ˜Ÿç³»å›¢çš„ç»“æ„ä¸­ã€‚åœ¨ç‰©è´¨è¾ƒå°‘ã€åˆå§‹å¯†åº¦æ‰°åŠ¨è¾ƒå¤§çš„å®‡å®™ä¸­ï¼Œæ˜Ÿç³»å›¢å½¢æˆè¾ƒæ—©ï¼Œæœ‰æ›´å¤šçš„æ—¶é—´ç´¯ç§¯ç‰©è´¨ï¼Œä»è€Œå¯¼è‡´å è½åŒºåŸŸæ›´åŠ å¹¿é˜”ã€‚å› æ­¤ï¼Œæµ‹é‡å è½åŒºåŸŸçš„å¹³å‡è´¨é‡åˆ†å¸ƒæä¾›äº†ä¸€ç§æ–°çš„å®‡å®™å­¦æµ‹è¯•æ–¹æ³•ã€‚å è½åŒºåŸŸå¯¹é‡å­ç‰©ç†åŸºæœ¬ä¸å—å½±å“ï¼Œç›¸è¾ƒäºå…¶ä»–è¡¡é‡æ˜Ÿç³»å›¢ç»„è£…æ—¶é—´çš„æŒ‡æ ‡ï¼ˆå¦‚æµ“åº¦ï¼‰ï¼Œå®ƒæä¾›äº†ä¸€ä¸ªæ›´æ¸…æ´çš„ç»“æ„æµ‹è¯•ã€‚æˆ‘ä»¬è€ƒè™‘äº†ä¸‰ä¸ªå…¬å¼€å¯ç”¨çš„æ˜Ÿç³»å›¢ç›®å½•ä¸­çš„æ˜Ÿç³»å›¢æ ·æœ¬ï¼šå…‰è°±è¯†åˆ«eROSITAæºçš„SPIDERSç›®å½•ã€å…ƒç›®å½•M2Cä¸­çš„Xå°„çº¿å’ŒSunyaev-Zeldovichæ•ˆåº”é€‰æ‹©çš„é›†ç¾¤ä»¥åŠåœ¨æš—èƒ½é‡å…‰è°±ä»ªï¼ˆDESIï¼‰é—äº§æˆåƒè°ƒæŸ¥ä¸­è¯†åˆ«çš„é›†ç¾¤ã€‚æˆ‘ä»¬ä½¿ç”¨ç´«å¤–çº¿è¿‘çº¢å¤–å…‰å­¦åŒ—éƒ¨è°ƒæŸ¥ï¼ˆUNIONSï¼‰çš„åˆæ­¥å½¢æ€ç›®å½•ï¼Œä¸ºæ¯ä¸ªæ ·æœ¬æ¨å¯¼å‡ºå¤šä½™è¡¨é¢è´¨é‡å¯†åº¦åˆ†å¸ƒæ›²çº¿ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†æœ€å®Œæ•´çš„DESIé—äº§æ ·æœ¬çš„å¹³å‡åˆ†å¸ƒæ›²çº¿ä¸ä¸€ç³»åˆ—æ¨¡æ‹Ÿé¢„æµ‹å€¼è¿›è¡Œæ¯”è¾ƒï¼Œè¿™äº›æ¨¡æ‹Ÿè¦†ç›–äº†Î©må’ŒÏƒ8çš„èŒƒå›´ï¼Œå¾—åˆ°çº¦æŸÎ©m&#x3D;0.29Â±0.05å’ŒÏƒ8&#x3D;0.80Â±0.04ã€‚æˆ‘ä»¬è¿˜æµ‹é‡äº†SPIDERSã€M2Cå’ŒDESIé—äº§æˆåƒè°ƒæŸ¥é›†ç¾¤çš„å¹³å‡ï¼ˆç§»åŠ¨ï¼‰æº…å°„åŠå¾„åˆ†åˆ«ä¸º$1.59^{+0.16}<em>{-0.13} {\rm cMpc}&#x2F;h$ã€$1.30^{+0.25}</em>{-0.13} {\rm cMpc}&#x2F;h$å’Œ$1.45\pm0.11 {\rm cMpc}&#x2F;h$ã€‚ä½¿ç”¨æœ€ç»ˆçš„UNIONSå½¢æ€ç›®å½•å’ŒDESIä¸­å…‰è°±è§‚æµ‹çš„å®Œæ•´æ ·æœ¬è¿›è¡Œæ­¤åˆ†æï¼Œæˆ‘ä»¬æœ‰æœ›å°†ç›®å‰åŸºäºé›†ç¾¤ä¸°åº¦çš„ç ”ç©¶çš„æœ€ä½³çº¦æŸæé«˜ä¸€å€æˆ–æ›´å¤šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.09147v1">PDF</a> 16 pages, 10 figures. Submitted to MNRAS</p>
<p><strong>Summary</strong></p>
<p>å®‡å®™å­¦ä¿¡æ¯è¢«ç¼–ç åœ¨æ˜Ÿç³»å›¢çš„ç»“æ„ä¸­ã€‚åœ¨ç‰©è´¨è¾ƒå°‘ã€åˆå§‹å¯†åº¦æ‰°åŠ¨è¾ƒå¤§çš„å®‡å®™ä¸­ï¼Œæ˜Ÿç³»å›¢å½¢æˆè¾ƒæ—©ï¼Œæœ‰æ›´å¤šçš„æ—¶é—´ç§¯ç´¯ç‰©è´¨ï¼Œå¯¼è‡´å è½åŒºåŸŸæ›´ä¸ºå¹¿é˜”ã€‚å› æ­¤ï¼Œæµ‹é‡å è½åŒºåŸŸçš„å¹³å‡è´¨é‡åˆ†å¸ƒæä¾›äº†ä¸€é¡¹æ–°çš„å®‡å®™å­¦æµ‹è¯•ã€‚æœ¬ç ”ç©¶åˆ©ç”¨ä¸‰ä¸ªå…¬å¼€çš„æ˜Ÿç³»å›¢ç›®å½•ï¼Œå³SPIDERSç›®å½•ã€M2Cå…ƒç›®å½•ä»¥åŠDESIæˆåƒè°ƒæŸ¥ä¸­çš„æ˜Ÿç³»å›¢ï¼Œé€šè¿‡UNIONSåˆæ­¥å½¢æ€ç›®å½•æ¨å¯¼å‡ºå„æ ·æœ¬çš„è¿‡é‡è¡¨é¢è´¨é‡å¯†åº¦åˆ†å¸ƒã€‚ç„¶åï¼Œæˆ‘ä»¬å°†DESIé—äº§æ ·æœ¬çš„å¹³å‡åˆ†å¸ƒä¸ä¸€ç³»åˆ—æ¨¡æ‹Ÿé¢„æµ‹è¿›è¡Œæ¯”è¾ƒï¼Œå¾—å‡ºç‰©è´¨å¯†åº¦å‚æ•°Î©må’ŒÏƒ8çš„é™åˆ¶å€¼ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æµ‹é‡äº†SPIDERSã€M2Cå’ŒDESIæˆåƒè°ƒæŸ¥æ˜Ÿç³»å›¢çš„å¹³å‡ï¼ˆç§»åŠ¨ï¼‰æº…å°„åŠå¾„ã€‚éšç€UNIONSå½¢æ€ç›®å½•å’ŒDESIå…‰è°±è§‚æµ‹æ˜Ÿç³»å›¢æ ·æœ¬çš„å®Œå–„ï¼Œæˆ‘ä»¬æœ‰æœ›å°†å½“å‰åŸºäºé›†ç¾¤ä¸°åº¦çš„æœ€ä½³çº¦æŸæé«˜ä¸€å€ä»¥ä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å®‡å®™å­¦ä¿¡æ¯è¢«ç¼–ç åœ¨æ˜Ÿç³»å›¢çš„ç»“æ„ä¸­ã€‚</li>
<li>æ˜Ÿç³»å›¢å½¢æˆçš„æ—¶é—´å’Œæ–¹å¼å½±å“å è½åŒºåŸŸçš„å¤§å°å’Œç»“æ„ã€‚</li>
<li>æµ‹é‡å è½åŒºåŸŸçš„å¹³å‡è´¨é‡åˆ†å¸ƒæ˜¯ä¸€ç§æ–°çš„å®‡å®™å­¦æµ‹è¯•æ–¹æ³•ã€‚</li>
<li>æ­¤æ–¹æ³•ä¸»è¦å…³æ³¨å è½åŒºåŸŸï¼Œè¾ƒå°‘å—åˆ°é‡å­ç‰©ç†çš„å½±å“ã€‚</li>
<li>ç ”ç©¶ä½¿ç”¨äº†ä¸‰ä¸ªå…¬å¼€çš„æ˜Ÿç³»å›¢ç›®å½•ä¸­çš„æ ·æœ¬æ•°æ®è¿›è¡Œåˆ†æã€‚</li>
<li>é€šè¿‡å¯¹æ¯”æ¨¡æ‹Ÿé¢„æµ‹å’Œå®æµ‹æ•°æ®ï¼Œå¾—å‡ºäº†ç‰©è´¨å¯†åº¦å‚æ•°Î©må’ŒÏƒ8çš„é™åˆ¶å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.09147">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c0a4f712aea8fdc72198f8be8a94c0a0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-84a19ef234c64d4346ebf22ff1a25ee4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1cd00a8bc18dac4e3334f0c3d433e108.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9e72dfcf4a88e84def9afbd063824c7b.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Benchmarking-Robustness-of-Contrastive-Learning-Models-for-Medical-Image-Report-Retrieval"><a href="#Benchmarking-Robustness-of-Contrastive-Learning-Models-for-Medical-Image-Report-Retrieval" class="headerlink" title="Benchmarking Robustness of Contrastive Learning Models for Medical   Image-Report Retrieval"></a>Benchmarking Robustness of Contrastive Learning Models for Medical   Image-Report Retrieval</h2><p><strong>Authors:Demetrio Deanda, Yuktha Priya Masupalli, Jeong Yang, Young Lee, Zechun Cao, Gongbo Liang</strong></p>
<p>Medical images and reports offer invaluable insights into patient health. The heterogeneity and complexity of these data hinder effective analysis. To bridge this gap, we investigate contrastive learning models for cross-domain retrieval, which associates medical images with their corresponding clinical reports. This study benchmarks the robustness of four state-of-the-art contrastive learning models: CLIP, CXR-RePaiR, MedCLIP, and CXR-CLIP. We introduce an occlusion retrieval task to evaluate model performance under varying levels of image corruption. Our findings reveal that all evaluated models are highly sensitive to out-of-distribution data, as evidenced by the proportional decrease in performance with increasing occlusion levels. While MedCLIP exhibits slightly more robustness, its overall performance remains significantly behind CXR-CLIP and CXR-RePaiR. CLIP, trained on a general-purpose dataset, struggles with medical image-report retrieval, highlighting the importance of domain-specific training data. The evaluation of this work suggests that more effort needs to be spent on improving the robustness of these models. By addressing these limitations, we can develop more reliable cross-domain retrieval models for medical applications. </p>
<blockquote>
<p>åŒ»ç–—å›¾åƒå’ŒæŠ¥å‘Šä¸ºæ‚£è€…å¥åº·æä¾›äº†å®è´µçš„è§è§£ã€‚è¿™äº›æ•°æ®çš„å¼‚è´¨æ€§å’Œå¤æ‚æ€§é˜»ç¢äº†æœ‰æ•ˆçš„åˆ†æã€‚ä¸ºäº†å¼¥å·®è·ï¼Œæˆ‘ä»¬ç ”ç©¶äº†è·¨åŸŸæ£€ç´¢çš„å¯¹æ¯”å­¦ä¹ æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å°†åŒ»å­¦å›¾åƒä¸å…¶ç›¸åº”çš„ä¸´åºŠæŠ¥å‘Šç›¸å…³è”ã€‚æœ¬ç ”ç©¶å¯¹å››ç§æœ€å…ˆè¿›çš„å¯¹æ¯”å­¦ä¹ æ¨¡å‹è¿›è¡Œäº†ç¨³å¥æ€§è¯„ä¼°ï¼šCLIPã€CXR-RePaiRã€MedCLIPå’ŒCXR-CLIPã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªé®æŒ¡æ£€ç´¢ä»»åŠ¡æ¥è¯„ä¼°æ¨¡å‹åœ¨ä¸åŒç¨‹åº¦çš„å›¾åƒæŸåä¸‹çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæ‰€æœ‰è¯„ä¼°æ¨¡å‹å¯¹å¼‚å¸¸æ•°æ®çš„æ•æ„Ÿæ€§éƒ½å¾ˆé«˜ï¼Œéšç€é®æŒ¡ç¨‹åº¦çš„å¢åŠ ï¼Œæ€§èƒ½æœ‰æ‰€ä¸‹é™ã€‚è™½ç„¶MedCLIPæ˜¾ç¤ºå‡ºè½»å¾®çš„æ›´å¤šç¨³å¥æ€§ï¼Œä½†å…¶æ€»ä½“æ€§èƒ½ä»ç„¶è¿œè¿œè½åäºCXR-CLIPå’ŒCXR-RePaiRã€‚CLIPåœ¨é€šç”¨æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå› æ­¤åœ¨åŒ»å­¦å›¾åƒæŠ¥å‘Šæ£€ç´¢æ–¹é¢è¡¨ç°æŒ£æ‰ï¼Œè¿™çªå‡ºäº†ç‰¹å®šé¢†åŸŸè®­ç»ƒæ•°æ®çš„é‡è¦æ€§ã€‚å¯¹æ­¤å·¥ä½œçš„è¯„ä¼°è¡¨æ˜ï¼Œè¿˜éœ€è¦æ›´å¤šåŠªåŠ›æ¥æé«˜è¿™äº›æ¨¡å‹çš„ç¨³å¥æ€§ã€‚é€šè¿‡è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬å¯ä»¥å¼€å‘æ›´å¯é çš„åŒ»å­¦åº”ç”¨è·¨åŸŸæ£€ç´¢æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.09134v1">PDF</a> This work is accepted to AAAI 2025 Workshop â€“ the 9th International   Workshop on Health Intelligence</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†åŒ»å­¦å›¾åƒä¸ä¸´åºŠæŠ¥å‘Šä¹‹é—´çš„è·¨åŸŸæ£€ç´¢é—®é¢˜ï¼Œå¼•å…¥å¯¹æ¯”å­¦ä¹ æ¨¡å‹æ¥è§£å†³åŒ»å­¦å›¾åƒæ•°æ®çš„å¼‚è´¨æ€§å’Œå¤æ‚æ€§ã€‚é€šè¿‡å¯¹å››ç§å…ˆè¿›çš„å¯¹æ¯”å­¦ä¹ æ¨¡å‹ï¼ˆCLIPã€CXR-RePaiRã€MedCLIPå’ŒCXR-CLIPï¼‰çš„è¯„ä¼°å’Œæ¯”è¾ƒï¼Œå‘ç°è¿™äº›æ¨¡å‹å¯¹é®æŒ¡ç­‰å¼‚å¸¸æ•°æ®æ•æ„Ÿï¼Œä¸”æ€§èƒ½ä¸‹é™å¹…åº¦è¾ƒå¤§ã€‚MedCLIPè™½ç¨å…·ç¨³å¥æ€§ï¼Œä½†æ€»ä½“æ€§èƒ½ä»è½åäºCXR-CLIPå’ŒCXR-RePaiRã€‚ç ”ç©¶å¼ºè°ƒéœ€è¦æ›´å¤šåŠªåŠ›æ”¹è¿›æ¨¡å‹çš„ç¨³å¥æ€§ï¼Œä»¥å¼€å‘æ›´å¯é çš„åŒ»å­¦åº”ç”¨è·¨åŸŸæ£€ç´¢æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¯¹æ¯”å­¦ä¹ æ¨¡å‹è¢«ç”¨äºè§£å†³åŒ»å­¦å›¾åƒä¸ä¸´åºŠæŠ¥å‘Šçš„è·¨åŸŸæ£€ç´¢é—®é¢˜ã€‚</li>
<li>å¼•å…¥é®æŒ¡æ£€ç´¢ä»»åŠ¡æ¥è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼Œä»¥åº”å¯¹ä¸åŒé®æŒ¡ç¨‹åº¦çš„æŒ‘æˆ˜ã€‚</li>
<li>ç ”ç©¶çš„æ¨¡å‹å¯¹å¼‚å¸¸æ•°æ®æ•æ„Ÿï¼Œæ€§èƒ½éšé®æŒ¡ç¨‹åº¦å¢åŠ è€Œä¸‹é™ã€‚</li>
<li>MedCLIPæ¨¡å‹è™½ç¨³å¥æ€§ç¨å¼ºï¼Œä½†æ€»ä½“æ€§èƒ½è½åäºç‰¹å®šé¢†åŸŸè®­ç»ƒçš„æ¨¡å‹å¦‚CXR-CLIPå’ŒCXR-RePaiRã€‚</li>
<li>CLIPæ¨¡å‹åœ¨åŒ»å­¦å›¾åƒæŠ¥å‘Šæ£€ç´¢æ–¹é¢çš„è¡¨ç°è¾ƒå¼±ï¼Œçªæ˜¾å‡ºé¢†åŸŸç‰¹å®šè®­ç»ƒæ•°æ®çš„é‡è¦æ€§ã€‚</li>
<li>ç ”ç©¶è¡¨æ˜éœ€è¦æ”¹è¿›æ¨¡å‹çš„ç¨³å¥æ€§ï¼Œä»¥æé«˜å…¶åœ¨åŒ»å­¦åº”ç”¨ä¸­çš„å¯é æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.09134">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-24d39930fb34e8cd2019d4d674c6f84f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a80f5585de45e0a19e953dd10dc2e843.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-230b4b527ba37c5a2e890f768c6ea168.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-456f367a301a3bc8bc8aaabc00d3385d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7988556f02c292e4e9e4212e92df0a00.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Deep-Distance-Map-Regression-Network-with-Shape-aware-Loss-for-Imbalanced-Medical-Image-Segmentation"><a href="#Deep-Distance-Map-Regression-Network-with-Shape-aware-Loss-for-Imbalanced-Medical-Image-Segmentation" class="headerlink" title="Deep Distance Map Regression Network with Shape-aware Loss for   Imbalanced Medical Image Segmentation"></a>Deep Distance Map Regression Network with Shape-aware Loss for   Imbalanced Medical Image Segmentation</h2><p><strong>Authors:Huiyu Li, Xiabi Liu, Said Boumaraf, Xiaopeng Gong, Donghai Liao, Xiaohong Ma</strong></p>
<p>Small object segmentation, like tumor segmentation, is a difficult and critical task in the field of medical image analysis. Although deep learning based methods have achieved promising performance, they are restricted to the use of binary segmentation mask. Inspired by the rigorous mapping between binary segmentation mask and distance map, we adopt distance map as a novel ground truth and employ a network to fulfill the computation of distance map. Specially, we propose a new segmentation framework that incorporates the existing binary segmentation network and a light weight regression network (dubbed as LR-Net). Thus, the LR-Net can convert the distance map computation into a regression task and leverage the rich information of distance maps. Additionally, we derive a shape-aware loss by employing distance maps as penalty map to infer the complete shape of an object. We evaluated our approach on MICCAI 2017 Liver Tumor Segmentation (LiTS) Challenge dataset and a clinical dataset. Experimental results show that our approach outperforms the classification-based methods as well as other existing state-of-the-arts. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒåˆ†æä¸­ï¼Œå°ç‰©ä½“åˆ†å‰²ï¼ˆå¦‚è‚¿ç˜¤åˆ†å‰²ï¼‰æ˜¯ä¸€é¡¹æ—¢å›°éš¾åˆå…³é”®çš„ä»»åŠ¡ã€‚å°½ç®¡åŸºäºæ·±åº¦å­¦ä¹ çš„æ–¹æ³•å·²ç»å–å¾—äº†æœ‰å‰æ™¯çš„æ•ˆæœï¼Œä½†å®ƒä»¬å—é™äºä½¿ç”¨äºŒå…ƒåˆ†å‰²æ©è†œã€‚å—äºŒå…ƒåˆ†å‰²æ©è†œä¸è·ç¦»æ˜ å°„ä¹‹é—´ä¸¥æ ¼æ˜ å°„å…³ç³»çš„å¯å‘ï¼Œæˆ‘ä»¬é‡‡ç”¨è·ç¦»æ˜ å°„ä½œä¸ºæ–°çš„åŸºå‡†çœŸå®å€¼ï¼Œå¹¶ä½¿ç”¨ç½‘ç»œå®Œæˆè·ç¦»æ˜ å°„çš„è®¡ç®—ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„åˆ†å‰²æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†ç°æœ‰çš„äºŒå…ƒåˆ†å‰²ç½‘ç»œå’Œè½»é‡çº§å›å½’ç½‘ç»œï¼ˆç§°ä¸ºLR-Netï¼‰ã€‚å› æ­¤ï¼ŒLR-Netå¯ä»¥å°†è·ç¦»æ˜ å°„è®¡ç®—è½¬æ¢ä¸ºå›å½’ä»»åŠ¡ï¼Œå¹¶åˆ©ç”¨è·ç¦»æ˜ å°„çš„ä¸°å¯Œä¿¡æ¯ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡ä½¿ç”¨è·ç¦»æ˜ å°„ä½œä¸ºæƒ©ç½šæ˜ å°„ï¼Œæ¨å¯¼äº†ä¸€ç§å½¢çŠ¶æ„ŸçŸ¥æŸå¤±æ¥æ¨æ–­å¯¹è±¡çš„å®Œæ•´å½¢çŠ¶ã€‚æˆ‘ä»¬åœ¨MICCAI 2017è‚è„è‚¿ç˜¤åˆ†å‰²æŒ‘æˆ˜èµ›æ•°æ®é›†å’Œä¸´åºŠæ•°æ®é›†ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºåŸºäºåˆ†ç±»çš„æ–¹æ³•ä»¥åŠå…¶ä»–ç°æœ‰æœ€æ–°æŠ€æœ¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.09116v1">PDF</a> Conference</p>
<p><strong>Summary</strong><br>     åŸºäºæ·±åº¦å­¦ä¹ çš„åŒ»å­¦å›¾åƒå°ç›®æ ‡åˆ†å‰²ï¼Œå¦‚è‚¿ç˜¤åˆ†å‰²ï¼Œæ˜¯åŒ»å­¦å›¾åƒåˆ†æä¸­çš„å…³é”®ä»»åŠ¡ã€‚æœ¬ç ”ç©¶é‡‡ç”¨è·ç¦»å›¾ä½œä¸ºæ–°çš„çœŸå®æ ‡ç­¾ï¼Œå¹¶æå‡ºä¸€ä¸ªç»“åˆäºŒå…ƒåˆ†å‰²ç½‘ç»œå’Œè½»é‡çº§å›å½’ç½‘ç»œï¼ˆç§°ä¸ºLR-Netï¼‰çš„åˆ†å‰²æ¡†æ¶ã€‚LR-Netå°†è·ç¦»å›¾è®¡ç®—è½¬åŒ–ä¸ºå›å½’ä»»åŠ¡ï¼Œå¹¶åˆ©ç”¨è·ç¦»å›¾çš„ä¸°å¯Œä¿¡æ¯ã€‚æ­¤å¤–ï¼Œé€šè¿‡è·ç¦»å›¾ä½œä¸ºæƒ©ç½šå›¾ï¼Œæˆ‘ä»¬æ¨å¯¼å‡ºå½¢çŠ¶æ„ŸçŸ¥æŸå¤±æ¥æ¨æ–­å¯¹è±¡çš„å®Œæ•´å½¢çŠ¶ã€‚åœ¨MICCAI 2017å¹´è‚è„è‚¿ç˜¤åˆ†å‰²æŒ‘æˆ˜èµ›æ•°æ®é›†å’Œä¸´åºŠæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºåŸºäºåˆ†ç±»çš„æ–¹æ³•å’Œå…¶ä»–ç°æœ‰å…ˆè¿›æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒä¸­çš„å°ç›®æ ‡åˆ†å‰²ï¼Œå¦‚è‚¿ç˜¤åˆ†å‰²ï¼Œæ˜¯æ·±åº¦å­¦ä¹ æ–¹æ³•çš„é‡è¦åº”ç”¨é¢†åŸŸã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨è·ç¦»å›¾ä½œä¸ºæ–°çš„çœŸå®æ ‡ç­¾ï¼Œä»¥æ›´ç²¾ç¡®åœ°æè¿°ç›®æ ‡ç‰©ä½“çš„è¾¹ç•Œä¿¡æ¯ã€‚</li>
<li>æå‡ºç»“åˆäºŒå…ƒåˆ†å‰²ç½‘ç»œå’Œè½»é‡çº§å›å½’ç½‘ç»œï¼ˆLR-Netï¼‰çš„æ–°åˆ†å‰²æ¡†æ¶ã€‚</li>
<li>LR-Netèƒ½å°†è·ç¦»å›¾çš„è®¡ç®—è½¬åŒ–ä¸ºå›å½’ä»»åŠ¡ï¼Œåˆ©ç”¨è·ç¦»å›¾ä¸­ä¸°å¯Œçš„ä¿¡æ¯ã€‚</li>
<li>é€šè¿‡è·ç¦»å›¾æ¨å¯¼å‡ºçš„å½¢çŠ¶æ„ŸçŸ¥æŸå¤±ï¼Œæœ‰åŠ©äºæ›´å‡†ç¡®åœ°æ¨æ–­ç›®æ ‡çš„å®Œæ•´å½¢çŠ¶ã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†åŸºäºåˆ†ç±»çš„æ–¹æ³•å’Œå…¶ä»–å…ˆè¿›æŠ€æœ¯ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºåŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.09116">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1862271f779dde2309875975c5b495f8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e71efc6ffdff06bc0173d6a619565f4c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b360b3f7150ae5f9828bd26cd7079d84.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Generative-Medical-Image-Anonymization-Based-on-Latent-Code-Projection-and-Optimization"><a href="#Generative-Medical-Image-Anonymization-Based-on-Latent-Code-Projection-and-Optimization" class="headerlink" title="Generative Medical Image Anonymization Based on Latent Code Projection   and Optimization"></a>Generative Medical Image Anonymization Based on Latent Code Projection   and Optimization</h2><p><strong>Authors:Huiyu Li, Nicholas Ayache, HervÃ© Delingette</strong></p>
<p>Medical image anonymization aims to protect patient privacy by removing identifying information, while preserving the data utility to solve downstream tasks. In this paper, we address the medical image anonymization problem with a two-stage solution: latent code projection and optimization. In the projection stage, we design a streamlined encoder to project input images into a latent space and propose a co-training scheme to enhance the projection process. In the optimization stage, we refine the latent code using two deep loss functions designed to address the trade-off between identity protection and data utility dedicated to medical images. Through a comprehensive set of qualitative and quantitative experiments, we showcase the effectiveness of our approach on the MIMIC-CXR chest X-ray dataset by generating anonymized synthetic images that can serve as training set for detecting lung pathologies. Source codes are available at <a target="_blank" rel="noopener" href="https://github.com/Huiyu-Li/GMIA">https://github.com/Huiyu-Li/GMIA</a>. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒåŒ¿ååŒ–çš„ç›®çš„æ˜¯é€šè¿‡ç§»é™¤è¯†åˆ«ä¿¡æ¯æ¥ä¿æŠ¤æ‚£è€…éšç§ï¼ŒåŒæ—¶ä¿ç•™æ•°æ®æ•ˆç”¨ä»¥è§£å†³ä¸‹æ¸¸ä»»åŠ¡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é‡‡ç”¨ä¸¤é˜¶æ®µè§£å†³æ–¹æ¡ˆæ¥è§£å†³åŒ»å­¦å›¾åƒåŒ¿ååŒ–é—®é¢˜ï¼šæ½œåœ¨ä»£ç æŠ•å½±å’Œä¼˜åŒ–ã€‚åœ¨æŠ•å½±é˜¶æ®µï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªç®€åŒ–çš„ç¼–ç å™¨ï¼Œå°†è¾“å…¥å›¾åƒæŠ•å½±åˆ°æ½œåœ¨ç©ºé—´ï¼Œå¹¶æå‡ºä¸€ç§è”åˆè®­ç»ƒæ–¹æ¡ˆæ¥å¢å¼ºæŠ•å½±è¿‡ç¨‹ã€‚åœ¨ä¼˜åŒ–é˜¶æ®µï¼Œæˆ‘ä»¬åˆ©ç”¨ä¸¤ç§æ·±åº¦æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–æ½œåœ¨ä»£ç ï¼Œè¿™äº›å‡½æ•°æ—¨åœ¨è§£å†³èº«ä»½ä¿æŠ¤ä¸æ•°æ®æ•ˆç”¨ä¹‹é—´çš„æƒè¡¡ï¼Œä¸“é—¨ç”¨äºåŒ»å­¦å›¾åƒã€‚é€šè¿‡ä¸€ç³»åˆ—ç»¼åˆçš„å®šæ€§å’Œå®šé‡å®éªŒï¼Œæˆ‘ä»¬åœ¨MIMIC-CXRèƒ¸éƒ¨Xå°„çº¿æ•°æ®é›†ä¸Šå±•ç¤ºäº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œé€šè¿‡ç”ŸæˆåŒ¿ååˆæˆå›¾åƒä½œä¸ºè®­ç»ƒé›†ï¼Œå¯ç”¨äºæ£€æµ‹è‚ºéƒ¨ç—…ç†ã€‚ç›¸å…³æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Huiyu-Li/GMIA%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Huiyu-Li/GMIAä¸­æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.09114v1">PDF</a> Conference</p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒåŒ¿ååŒ–æ—¨åœ¨å»é™¤æ‚£è€…èº«ä»½è¯†åˆ«ä¿¡æ¯ä»¥ä¿æŠ¤æ‚£è€…éšç§ï¼ŒåŒæ—¶ä¿ç•™æ•°æ®ç”¨äºè§£å†³ä¸‹æ¸¸ä»»åŠ¡ã€‚æœ¬æ–‡æå‡ºä¸€ä¸ªä¸¤é˜¶æ®µçš„åŒ»å­¦å›¾åƒåŒ¿ååŒ–è§£å†³æ–¹æ¡ˆï¼šæ½œåœ¨ä»£ç æŠ•å½±å’Œä¼˜åŒ–ã€‚æŠ•å½±é˜¶æ®µè®¾è®¡ç®€æ´çš„ç¼–ç å™¨å°†è¾“å…¥å›¾åƒæŠ•å½±åˆ°æ½œåœ¨ç©ºé—´ï¼Œå¹¶æå‡ºååŒè®­ç»ƒæ–¹æ¡ˆä»¥å¢å¼ºæŠ•å½±è¿‡ç¨‹ã€‚ä¼˜åŒ–é˜¶æ®µä½¿ç”¨ä¸¤ä¸ªé’ˆå¯¹åŒ»å­¦å›¾åƒèº«ä»½ä¿æŠ¤å’Œæ•°æ®æ•ˆç”¨æƒè¡¡çš„æ·±åº¦æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–æ½œåœ¨ä»£ç ã€‚é€šè¿‡ä¸€ç³»åˆ—å®šæ€§å’Œå®šé‡å®éªŒï¼Œåœ¨MIMIC-CXRèƒ¸éƒ¨Xå°„çº¿æ•°æ®é›†ä¸Šå±•ç¤ºäº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œç”ŸæˆåŒ¿ååˆæˆå›¾åƒå¯ä½œä¸ºæ£€æµ‹è‚ºéƒ¨ç–¾ç—…çš„è®­ç»ƒé›†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåŒ¿ååŒ–çš„ç›®æ ‡æ˜¯å»é™¤æ‚£è€…èº«ä»½è¯†åˆ«ä¿¡æ¯ï¼ŒåŒæ—¶ä¿ç•™æ•°æ®ç”¨äºè§£å†³ä¸‹æ¸¸ä»»åŠ¡ã€‚</li>
<li>æœ¬æ–‡æå‡ºä¸€ä¸ªä¸¤é˜¶æ®µçš„åŒ»å­¦å›¾åƒåŒ¿ååŒ–è§£å†³æ–¹æ¡ˆï¼šæ½œåœ¨ä»£ç æŠ•å½±å’Œä¼˜åŒ–ã€‚</li>
<li>æŠ•å½±é˜¶æ®µåˆ©ç”¨ç®€æ´çš„ç¼–ç å™¨å’ŒååŒè®­ç»ƒæ–¹æ¡ˆè¿›è¡Œå›¾åƒæŠ•å½±ã€‚</li>
<li>ä¼˜åŒ–é˜¶æ®µä½¿ç”¨æ·±åº¦æŸå¤±å‡½æ•°æ¥å¹³è¡¡èº«ä»½ä¿æŠ¤å’Œæ•°æ®æ•ˆç”¨ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨MIMIC-CXRæ•°æ®é›†ä¸Šç”Ÿæˆäº†æœ‰æ•ˆçš„åŒ¿ååˆæˆå›¾åƒã€‚</li>
<li>è¿™äº›åˆæˆå›¾åƒå¯ä»¥ä½œä¸ºåŒ»å­¦å›¾åƒåˆ†æä¸­æ£€æµ‹è‚ºéƒ¨ç–¾ç—…çš„è®­ç»ƒé›†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.09114">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-30e5102b357ff542b65344bb1f61cf48.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8e3efd89c65d53aef81180576df7e3ae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d429dc31de3fd22e18be171501519e71.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7ccbb70ba0609ccbc28a93ce01fa45ed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-16dc6f73e5ce5f47bf90802974280ff9.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Probing-the-low-energy-particle-content-of-blazar-jets-through-MeV-observations"><a href="#Probing-the-low-energy-particle-content-of-blazar-jets-through-MeV-observations" class="headerlink" title="Probing the low-energy particle content of blazar jets through MeV   observations"></a>Probing the low-energy particle content of blazar jets through MeV   observations</h2><p><strong>Authors:F. Tavecchio, L. Nava, A. Sciaccaluga, P. Coppi</strong></p>
<p>Many of the blazars observed by Fermi actually have the peak of their time-averaged gamma-ray emission outside the $\sim$ GeV Fermi energy range, at $\sim$ MeV energies. The detailed shape of the emission spectrum around the $\sim$ MeV peak places important constraints on acceleration and radiation mechanisms in the blazar jet and may not be the simple broken power law obtained by extrapolating from the observed X-ray and GeV gamma-ray spectra. In particular, state-of-the-art simulations of particle acceleration by shocks show that a significant fraction (possibly up to $\approx 90%$) of the available energy may go into bulk, quasi-thermal heating of the plasma crossing the shock rather than producing a non-thermal power law tail. Other <code>gentler&quot; but possibly more pervasive acceleration mechanisms such as shear acceleration at the jet boundary may result in a further build-up of the low-energy ($\gamma \lesssim 10^&#123;2&#125;$) electron/positron population in the jet. As already discussed for the case of gamma-ray bursts, the presence of a low-energy, Maxwellian-like </code>bumpâ€™â€™ in the jet particle energy distribution can strongly affect the spectrum of the emitted radiation, e.g., producing an excess over the emission expected from a power-law extrapolation of a blazarâ€™s GeV-TeV spectrum. We explore the potential detectability of the spectral component ascribable to a hot, quasi-thermal population of electrons in the high-energy emission of flat-spectrum radio quasars (FSRQ). We show that for typical FSRQ physical parameters, the expected spectral signature is located at $\sim$ MeV energies. For the brightest Fermi FSRQ sources, the presence of such a component will be constrained by the upcoming MeV Compton Spectrometer and Imager (COSI) satellite. </p>
<blockquote>
<p>ç”±è´¹ç±³è§‚æµ‹åˆ°çš„è®¸å¤šè€€æ–‘å®é™…ä¸Šå…¶æ—¶é—´å¹³å‡çš„ä¼½é©¬å°„çº¿å‘å°„å³°å€¼ä½äºçº¦MeVèƒ½é‡ï¼Œè€Œéè´¹ç±³èƒ½é‡èŒƒå›´çš„GeVå†…ã€‚MeVå³°å€¼å‘¨å›´çš„å‘å°„å…‰è°±çš„è¯¦ç»†å½¢çŠ¶å¯¹è€€æ–‘å–·æµä¸­çš„åŠ é€Ÿå’Œè¾å°„æœºåˆ¶æ–½åŠ äº†é‡è¦çš„çº¦æŸï¼Œå¯èƒ½å¹¶éé€šè¿‡è§‚æµ‹åˆ°çš„Xå°„çº¿å’ŒGeVä¼½é©¬å°„çº¿å…‰è°±æ¨æ–­å‡ºçš„ç®€å•åˆ†æ®µå¹‚å¾‹ã€‚ç‰¹åˆ«æ˜¯ï¼Œå…³äºå†²å‡»æ³¢ç²’å­åŠ é€Ÿçš„æœ€æ–°æ¨¡æ‹Ÿæ˜¾ç¤ºï¼Œç›¸å½“ä¸€éƒ¨åˆ†ï¼ˆå¯èƒ½é«˜è¾¾çº¦90%ï¼‰çš„å¯ç”¨èƒ½é‡å¯èƒ½ç”¨äºé€šè¿‡å†²å‡»åŒºåŸŸçš„ç­‰ç¦»å­ä½“çš„å¤§è§„æ¨¡å‡†çƒ­åŠ çƒ­ï¼Œè€Œéäº§ç”Ÿéçƒ­å¹‚å¾‹å°¾å·´ã€‚å…¶ä»–è¾ƒä¸ºæ¸©å’Œçš„ä½†å¯èƒ½æ›´ä¸ºæ™®éçš„åŠ é€Ÿæœºåˆ¶ï¼Œå¦‚åœ¨å–·æµè¾¹ç•Œå¤„çš„å‰ªåˆ‡åŠ é€Ÿï¼Œå¯èƒ½å¯¼è‡´å–·æµä¸­ä½èƒ½ï¼ˆÎ³â‰¤10Â²ï¼‰çš„ç”µå­&#x2F;æ­£ç”µå­ç§ç¾¤è¿›ä¸€æ­¥å¢åŠ ã€‚å¦‚å·²é’ˆå¯¹ä¼½é©¬å°„çº¿çˆ†å‘çš„æƒ…å†µè¿›è¡Œè®¨è®ºï¼Œå–·æµç²’å­èƒ½é‡åˆ†å¸ƒä¸­å­˜åœ¨ç±»ä¼¼äºéº¦å…‹æ–¯éŸ¦åˆ†å¸ƒçš„ä½èƒ½â€œå‡¸èµ·â€ä¼šå¼ºçƒˆå½±å“å‘å°„çš„è¾å°„å…‰è°±ï¼Œä¾‹å¦‚ï¼Œåœ¨è€€æ–‘çš„GeV-TeVå…‰è°±çš„å¹‚å¾‹å¤–æ¨é¢„æœŸä¹‹ä¸Šäº§ç”Ÿè¿‡é‡è¾å°„ã€‚æˆ‘ä»¬æ¢è®¨äº†é«˜æ¸©ã€å‡†çƒ­ç”µå­ç§ç¾¤æ‰€äº§ç”Ÿçš„é«˜èƒ½å‘å°„åœ¨å¹³è°±å°„ç”µç±»æ˜Ÿï¼ˆFSRQï¼‰ä¸­çš„æ½œåœ¨å¯æ¢æµ‹æ€§ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œå¯¹äºå…¸å‹çš„FSRQç‰©ç†å‚æ•°ï¼Œé¢„æœŸçš„è°±ç‰¹å¾ä½äºçº¦MeVèƒ½é‡ã€‚å¯¹äºè´¹ç±³æœ€æ˜äº®çš„FSRQæºï¼Œè¿™ç§æˆåˆ†çš„å­˜åœ¨å°†å—åˆ°å³å°†åˆ°æ¥çš„MeVåº·æ™®é¡¿å…‰è°±ä»ªå’Œæˆåƒä»ªï¼ˆCOSIï¼‰å«æ˜Ÿçš„çº¦æŸã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.09089v2">PDF</a> 5 pages, 2 figures, accepted for publication in A&amp;A Letters</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æŒ‡å‡ºï¼Œè®¸å¤šè´¹ç±³è§‚æµ‹åˆ°çš„è€€å˜ä½“åœ¨å¹³å‡Î³å°„çº¿å‘å°„å³°å€¼æ—¶ï¼Œå…¶èƒ½é‡å¹¶éåœ¨è´¹ç±³çš„$\sim$gevèƒ½é‡èŒƒå›´å†…ï¼Œè€Œæ˜¯åœ¨$\sim$MeVèƒ½é‡èŒƒå›´å†…ã€‚è€€å˜ä½“å‘å°„å…‰è°±çš„è¯¦ç»†å½¢çŠ¶åœ¨$\sim$MeVå³°å€¼å¤„å¯¹å–·æµä¸­çš„åŠ é€Ÿå’Œè¾å°„æœºåˆ¶æœ‰é‡è¦çš„é™åˆ¶ï¼Œå¯èƒ½å¹¶ä¸åªæ˜¯é€šè¿‡è§‚æµ‹åˆ°çš„Xå°„çº¿å’ŒgevÎ³å°„çº¿å…‰è°±æ¨æ–­å‡ºçš„ç®€å•åˆ†æ®µå¹‚å¾‹ã€‚å…ˆè¿›çš„ç²’å­åŠ é€Ÿæ¨¡æ‹Ÿæ˜¾ç¤ºï¼Œå¯ç”¨èƒ½é‡çš„å¾ˆå¤§ä¸€éƒ¨åˆ†ï¼ˆå¯èƒ½é«˜è¾¾90%ï¼‰å¯èƒ½ç”¨äºç©¿è¿‡å†²å‡»æ³¢çš„ç­‰ç¦»å­ä½“æ•´ä½“ã€å‡†çƒ­åŠ çƒ­ï¼Œè€Œä¸æ˜¯äº§ç”Ÿéçƒ­å¹‚å¾‹å°¾ã€‚å…¶ä»–è¾ƒä¸ºæ¸©å’Œä½†å¯èƒ½æ›´æ™®éçš„åŠ é€Ÿæœºåˆ¶ï¼Œå¦‚å–·å°„è¾¹ç•Œå¤„çš„å‰ªåˆ‡åŠ é€Ÿï¼Œå¯èƒ½å¯¼è‡´å–·å°„ä¸­èƒ½é‡è¾ƒä½ï¼ˆÎ³â‰¤10Â²ï¼‰çš„ç”µå­&#x2F;æ­£ç”µå­ç¾¤ä½“è¿›ä¸€æ­¥å¢åŠ ã€‚å¯¹äºä¼½é©¬å°„çº¿æš´çš„æƒ…å†µå·²ç»è¿›è¡Œäº†è®¨è®ºï¼Œå–·å°„ç²’å­èƒ½é‡åˆ†å¸ƒä¸­å­˜åœ¨ä½èƒ½ã€ç±»ä¼¼éº¦å…‹æ–¯éŸ¦çš„â€œå‡¸èµ·â€ä¼šå¼ºçƒˆå½±å“å‘å°„çš„è¾å°„å…‰è°±ï¼Œä¾‹å¦‚ï¼Œåœ¨è€€å˜ä½“çš„gev-tevå…‰è°±çš„å¹‚å¾‹å¤–æ¨ä¸­é¢„æœŸä¼šå‡ºç°è¿‡å¤šçš„å‘å°„ã€‚æˆ‘ä»¬æ¢è®¨äº†å¯å½’å› äºçƒ­ã€å‡†çƒ­ç”µå­ç¾¤ä½“çš„å…‰è°±æˆåˆ†åœ¨é«˜èƒ½å‘å°„ä¸­çš„æ½œåœ¨å¯æ¢æµ‹æ€§ã€‚å¯¹äºå…¸å‹çš„å¹³è°±å°„ç”µç±»æ˜Ÿç‰©ç†å‚æ•°ï¼Œé¢„æœŸçš„å…‰è°±ç‰¹å¾ä½äºMeVèƒ½é‡èŒƒå›´å†…ã€‚å¯¹äºæœ€äº®çš„è´¹ç±³å¹³è°±å°„ç”µç±»æ˜Ÿæºï¼Œè¯¥æˆåˆ†çš„å­˜åœ¨å°†å—åˆ°å³å°†å‘å°„çš„MeVåº·æ™®é¡¿å…‰è°±ä»ªå’Œæˆåƒå™¨ï¼ˆCOSIï¼‰å«æ˜Ÿçš„é™åˆ¶ã€‚ </p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è®¸å¤šè€€å˜ä½“çš„å¹³å‡Î³å°„çº¿å‘å°„å³°å€¼åœ¨MeVèƒ½é‡èŒƒå›´å†…ï¼Œä¸åœ¨è´¹ç±³çš„GEVèƒ½é‡èŒƒå›´å†…ã€‚</li>
<li>è€€å˜ä½“å‘å°„å…‰è°±çš„è¯¦ç»†å½¢çŠ¶å¯¹å–·æµä¸­çš„åŠ é€Ÿå’Œè¾å°„æœºåˆ¶æœ‰é‡è¦çš„é™åˆ¶ï¼Œå¯èƒ½ä¸åŒäºç®€å•çš„åˆ†æ®µå¹‚å¾‹ã€‚</li>
<li>å…ˆè¿›çš„æ¨¡æ‹Ÿæ˜¾ç¤ºï¼Œå¤§éƒ¨åˆ†èƒ½é‡å¯èƒ½ç”¨äºç­‰ç¦»å­ä½“æ•´ä½“åŠ çƒ­è€Œéäº§ç”Ÿéçƒ­å¹‚å¾‹å°¾ã€‚</li>
<li>å­˜åœ¨æ¸©å’Œä½†æ™®éçš„åŠ é€Ÿæœºåˆ¶å¦‚å‰ªåˆ‡åŠ é€Ÿå¯èƒ½å¯¼è‡´å–·å°„ä¸­ä½èƒ½ç”µå­&#x2F;æ­£ç”µå­ç¾¤ä½“å¢åŠ ã€‚</li>
<li>ä½èƒ½Maxwellianå‹ç²’å­èƒ½é‡åˆ†å¸ƒå¯èƒ½å½±å“å‘å°„çš„è¾å°„å…‰è°±ï¼Œå¯¼è‡´ä¸é¢„æœŸçš„å¹‚å¾‹å¤–æ¨ä¸ç¬¦çš„å‘å°„ç»“æœã€‚</li>
<li>å¯¹äºå¹³è°±å°„ç”µç±»æ˜Ÿæºçš„é«˜èƒ½å‘å°„ä¸­ï¼Œå¯èƒ½å­˜åœ¨ä¸€ä¸ªå½’å› äºçƒ­ã€å‡†çƒ­ç”µå­ç¾¤ä½“çš„å…‰è°±æˆåˆ†ã€‚è¿™ä¸€æˆåˆ†ä½äºMeVèƒ½é‡èŒƒå›´å†…ä¸”å¯¹ç‰©ç†å‚æ•°æ•æ„Ÿã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.09089">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-47b63881a5eed6ccb203e28e3eef7963.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-298c70a90cdf4f2b3e21894d2e0b5d86.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ccec326b92a0f0a12dad2a182ff64ec4.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Knowledge-Transfer-and-Domain-Adaptation-for-Fine-Grained-Remote-Sensing-Image-Segmentation"><a href="#Knowledge-Transfer-and-Domain-Adaptation-for-Fine-Grained-Remote-Sensing-Image-Segmentation" class="headerlink" title="Knowledge Transfer and Domain Adaptation for Fine-Grained Remote Sensing   Image Segmentation"></a>Knowledge Transfer and Domain Adaptation for Fine-Grained Remote Sensing   Image Segmentation</h2><p><strong>Authors:Shun Zhang, Xuechao Zou, Kai Li, Congyan Lang, Shiying Wang, Pin Tao, Tengfei Cao</strong></p>
<p>Fine-grained remote sensing image segmentation is essential for accurately identifying detailed objects in remote sensing images. Recently, vision transformer models (VTMs) pre-trained on large-scale datasets have demonstrated strong zero-shot generalization. However, directly applying them to specific tasks may lead to domain shift. We introduce a novel end-to-end learning paradigm combining knowledge guidance with domain refinement to enhance performance. We present two key components: the Feature Alignment Module (FAM) and the Feature Modulation Module (FMM). FAM aligns features from a CNN-based backbone with those from the pretrained VTMâ€™s encoder using channel transformation and spatial interpolation, and transfers knowledge via KL divergence and L2 normalization constraint. FMM further adapts the knowledge to the specific domain to address domain shift. We also introduce a fine-grained grass segmentation dataset and demonstrate, through experiments on two datasets, that our method achieves a significant improvement of 2.57 mIoU on the grass dataset and 3.73 mIoU on the cloud dataset. The results highlight the potential of combining knowledge transfer and domain adaptation to overcome domain-related challenges and data limitations. The project page is available at <a target="_blank" rel="noopener" href="https://xavierjiezou.github.io/KTDA/">https://xavierjiezou.github.io/KTDA/</a>. </p>
<blockquote>
<p>ç²¾ç»†é¥æ„Ÿå›¾åƒåˆ†å‰²å¯¹äºå‡†ç¡®è¯†åˆ«é¥æ„Ÿå›¾åƒä¸­çš„è¯¦ç»†å¯¹è±¡è‡³å…³é‡è¦ã€‚æœ€è¿‘ï¼Œåœ¨å¤§å‹æ•°æ®é›†ä¸Šé¢„è®­ç»ƒçš„è§†è§‰è½¬æ¢å™¨æ¨¡å‹ï¼ˆVTMï¼‰è¡¨ç°å‡ºäº†å¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç›´æ¥å°†å…¶åº”ç”¨äºç‰¹å®šä»»åŠ¡å¯èƒ½ä¼šå¯¼è‡´é¢†åŸŸåç§»ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ç»“åˆçŸ¥è¯†å¼•å¯¼å’Œé¢†åŸŸç²¾ç‚¼çš„æ–°å‹ç«¯åˆ°ç«¯å­¦ä¹ èŒƒå¼ï¼Œä»¥æé«˜æ€§èƒ½ã€‚æˆ‘ä»¬æå‡ºäº†ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šç‰¹å¾å¯¹é½æ¨¡å—ï¼ˆFAMï¼‰å’Œç‰¹å¾è°ƒåˆ¶æ¨¡å—ï¼ˆFMMï¼‰ã€‚FAMé€šè¿‡å¯¹é€šé“å˜æ¢å’Œç©ºé—´æ’å€¼ï¼Œå°†å¯¹æ¥è‡ªåŸºäºCNNçš„éª¨å¹²ç½‘çš„ç‰¹å¾ä¸é¢„è®­ç»ƒVTMç¼–ç å™¨çš„ç‰¹å¾è¿›è¡Œå¯¹é½ï¼Œå¹¶é€šè¿‡KLæ•£åº¦å’ŒL2å½’ä¸€åŒ–çº¦æŸè¿›è¡ŒçŸ¥è¯†è½¬ç§»ã€‚FMMè¿›ä¸€æ­¥å°†çŸ¥è¯†é€‚åº”åˆ°ç‰¹å®šé¢†åŸŸï¼Œä»¥è§£å†³é¢†åŸŸåç§»é—®é¢˜ã€‚æˆ‘ä»¬è¿˜ä»‹ç»äº†ä¸€ä¸ªç²¾ç»†çš„è‰åˆ†å‰²æ•°æ®é›†ï¼Œå¹¶é€šè¿‡ä¸¤ä¸ªæ•°æ®é›†çš„å®éªŒè¯æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è‰æ•°æ®é›†ä¸Šå®ç°äº†2.57 mIoUçš„æ˜¾è‘—æ”¹è¿›ï¼Œåœ¨äº‘æ•°æ®é›†ä¸Šå®ç°äº†3.73 mIoUçš„æ”¹è¿›ã€‚ç»“æœçªå‡ºäº†ç»“åˆçŸ¥è¯†è½¬ç§»å’Œé¢†åŸŸé€‚åº”çš„æ½œåŠ›ï¼Œå¯ä»¥å…‹æœä¸é¢†åŸŸç›¸å…³çš„æŒ‘æˆ˜å’Œæ•°æ®é™åˆ¶ã€‚é¡¹ç›®é¡µé¢å¯åœ¨[<a target="_blank" rel="noopener" href="https://xavierjiezou.github.io/KTDA/]%E8%AE%BF%E9%97%AE%E3%80%82">https://xavierjiezou.github.io/KTDA/]è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06664v3">PDF</a> 6 pages, 3 figures, 6 tables</p>
<p><strong>Summary</strong></p>
<p>åŸºäºé¥æ„Ÿå›¾åƒçš„ç‰¹ç‚¹å’Œéœ€æ±‚ï¼Œæœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ç»“åˆçŸ¥è¯†æŒ‡å¯¼å’Œé¢†åŸŸç²¾ç‚¼çš„ç«¯åˆ°ç«¯å­¦ä¹ èŒƒå¼ï¼Œç”¨äºç²¾ç»†é¥æ„Ÿå›¾åƒåˆ†å‰²ã€‚ç ”ç©¶åˆ›æ–°æ€§åœ°è®¾è®¡äº†ç‰¹å¾å¯¹é½æ¨¡å—ï¼ˆFAMï¼‰å’Œç‰¹å¾è°ƒåˆ¶æ¨¡å—ï¼ˆFMMï¼‰ï¼Œå¹¶é€šè¿‡å®éªŒéªŒè¯äº†åœ¨ç‰¹å®šæ•°æ®é›†ä¸Šçš„æ˜¾è‘—æ€§èƒ½æå‡ã€‚æ­¤ç ”ç©¶ä¸ºé¥æ„Ÿå›¾åƒåˆ†ææä¾›äº†æ–°æ€è·¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†ç²¾ç»†é¥æ„Ÿå›¾åƒåˆ†å‰²çš„é‡è¦æ€§ä»¥åŠè§†è§‰è½¬æ¢å™¨æ¨¡å‹ï¼ˆVTMsï¼‰åœ¨é›¶æ ·æœ¬è¿ç§»å­¦ä¹ ä¸­çš„ä¼˜åŠ¿ã€‚</li>
<li>æå‡ºäº†ä¸€ç§ç»“åˆçŸ¥è¯†æŒ‡å¯¼å’Œé¢†åŸŸç²¾ç‚¼çš„ç«¯åˆ°ç«¯å­¦ä¹ èŒƒå¼ï¼Œæ—¨åœ¨è§£å†³ç›´æ¥åº”ç”¨VTMsäºç‰¹å®šä»»åŠ¡æ—¶å¯èƒ½å‡ºç°çš„é¢†åŸŸåç§»é—®é¢˜ã€‚</li>
<li>ä»‹ç»äº†ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šç‰¹å¾å¯¹é½æ¨¡å—ï¼ˆFAMï¼‰å’Œç‰¹å¾è°ƒåˆ¶æ¨¡å—ï¼ˆFMMï¼‰ã€‚FAMé€šè¿‡å¯¹é½æ¥è‡ªCNNåŸºç¡€æ¶æ„å’Œé¢„è®­ç»ƒVTMç¼–ç å™¨çš„ç‰¹å¾æ¥å®ç°çŸ¥è¯†è½¬ç§»ã€‚FMMè¿›ä¸€æ­¥è°ƒæ•´çŸ¥è¯†ä»¥é€‚åº”ç‰¹å®šé¢†åŸŸï¼Œä»¥è§£å†³é¢†åŸŸåç§»é—®é¢˜ã€‚</li>
<li>å¼•å…¥äº†ä¸€ä¸ªç²¾ç»†çš„è‰åœ°åˆ†å‰²æ•°æ®é›†ï¼Œå¹¶é€šè¿‡åœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨è‰åœ°æ•°æ®é›†ä¸Šçš„mIoUæé«˜äº†2.57ï¼Œåœ¨äº‘æ•°æ®é›†ä¸Šæé«˜äº†3.73ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•çš„æ½œåŠ›ã€‚</li>
<li>ç ”ç©¶å¼ºè°ƒäº†ç»“åˆçŸ¥è¯†è½¬ç§»å’Œé¢†åŸŸé€‚åº”ä»¥å…‹æœä¸é¢†åŸŸç›¸å…³çš„æŒ‘æˆ˜å’Œæ•°æ®é™åˆ¶çš„é‡è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.06664">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-2c07f3a25844fc5402749997aba4c3eb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-72230074e4fabea54949d15fe0c98c06.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0ba22b25df8231db65d6a7f88f3dfc30.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aa4e7464f02406170b966d2bd8df31fb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a1b729ce65630b757bee9ab399fee7bc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ce9ec2299419671ee56177593d07d844.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e8b2fd1466c02125be44e6c2c16e9695.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-18\./crop_åŒ»å­¦å›¾åƒ/2412.06664v3/page_4_2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-881a29ae763d8aff44085823f6e7f0ff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1f1e5e22a81ad979be1a060511a5e5e9.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="RobustEMD-Domain-Robust-Matching-for-Cross-domain-Few-shot-Medical-Image-Segmentation"><a href="#RobustEMD-Domain-Robust-Matching-for-Cross-domain-Few-shot-Medical-Image-Segmentation" class="headerlink" title="RobustEMD: Domain Robust Matching for Cross-domain Few-shot Medical   Image Segmentation"></a>RobustEMD: Domain Robust Matching for Cross-domain Few-shot Medical   Image Segmentation</h2><p><strong>Authors:Yazhou Zhu, Minxian Li, Qiaolin Ye, Shidong Wang, Tong Xin, Haofeng Zhang</strong></p>
<p>Few-shot medical image segmentation (FSMIS) aims to perform the limited annotated data learning in the medical image analysis scope. Despite the progress has been achieved, current FSMIS models are all trained and deployed on the same data domain, as is not consistent with the clinical reality that medical imaging data is always across different data domains (e.g. imaging modalities, institutions and equipment sequences). How to enhance the FSMIS models to generalize well across the different specific medical imaging domains? In this paper, we focus on the matching mechanism of the few-shot semantic segmentation models and introduce an Earth Moverâ€™s Distance (EMD) calculation based domain robust matching mechanism for the cross-domain scenario. Specifically, we formulate the EMD transportation process between the foreground support-query features, the texture structure aware weights generation method, which proposes to perform the sobel based image gradient calculation over the nodes, is introduced in the EMD matching flow to restrain the domain relevant nodes. Besides, the point set level distance measurement metric is introduced to calculated the cost for the transportation from support set nodes to query set nodes. To evaluate the performance of our model, we conduct experiments on three scenarios (i.e., cross-modal, cross-sequence and cross-institution), which includes eight medical datasets and involves three body regions, and the results demonstrate that our model achieves the SoTA performance against the compared models. </p>
<blockquote>
<p>å°‘æ•°æ ·æœ¬åŒ»å­¦å›¾åƒåˆ†å‰²ï¼ˆFSMISï¼‰æ—¨åœ¨å®ç°åŒ»å­¦å›¾åƒåˆ†æèŒƒå›´å†…çš„æœ‰é™æ ‡æ³¨æ•°æ®å­¦ä¹ ã€‚å°½ç®¡å·²ç»å–å¾—äº†ä¸€äº›è¿›å±•ï¼Œä½†å½“å‰çš„FSMISæ¨¡å‹éƒ½åœ¨åŒä¸€æ•°æ®åŸŸä¸­è¿›è¡Œè®­ç»ƒå’Œéƒ¨ç½²ï¼Œè¿™ä¸åŒ»å­¦æˆåƒæ•°æ®åœ¨ä¸´åºŠç°å®ä¸­è·¨ä¸åŒæ•°æ®åŸŸï¼ˆä¾‹å¦‚æˆåƒæ¨¡å¼ã€æœºæ„å’Œè®¾å¤‡åºåˆ—ï¼‰çš„æƒ…å†µä¸ä¸€è‡´ã€‚å¦‚ä½•å¢å¼ºFSMISæ¨¡å‹åœ¨ä¸åŒç‰¹å®šåŒ»å­¦æˆåƒåŸŸä¹‹é—´çš„æ³›åŒ–èƒ½åŠ›ï¼Ÿåœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å…³æ³¨å°‘æ•°æ ·æœ¬è¯­ä¹‰åˆ†å‰²æ¨¡å‹çš„åŒ¹é…æœºåˆ¶ï¼Œå¹¶å¼•å…¥ä¸€ç§åŸºäºåœ°çƒç§»åŠ¨è·ç¦»ï¼ˆEMDï¼‰è®¡ç®—çš„åŸŸç¨³å¥åŒ¹é…æœºåˆ¶ï¼Œç”¨äºè·¨åŸŸåœºæ™¯ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åˆ¶å®šäº†å‰æ™¯æ”¯æŒæŸ¥è¯¢ç‰¹å¾ä¹‹é—´çš„EMDä¼ è¾“è¿‡ç¨‹ï¼Œå¹¶å¼•å…¥äº†çº¹ç†ç»“æ„æ„ŸçŸ¥æƒé‡ç”Ÿæˆæ–¹æ³•ï¼Œè¯¥æ–¹æ³•å»ºè®®åœ¨èŠ‚ç‚¹ä¸Šè¿›è¡ŒåŸºäºSobelçš„å›¾åƒæ¢¯åº¦è®¡ç®—ï¼Œè¢«å¼•å…¥åˆ°EMDåŒ¹é…æµä¸­ä»¥çº¦æŸåŸŸç›¸å…³èŠ‚ç‚¹ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†ç‚¹é›†çº§è·ç¦»æµ‹é‡æŒ‡æ ‡ï¼Œä»¥è®¡ç®—ä»æ”¯æŒé›†èŠ‚ç‚¹åˆ°æŸ¥è¯¢é›†èŠ‚ç‚¹çš„ä¼ è¾“æˆæœ¬ã€‚ä¸ºäº†è¯„ä¼°æˆ‘ä»¬æ¨¡å‹çš„æ€§èƒ½ï¼Œæˆ‘ä»¬åœ¨ä¸‰ç§åœºæ™¯ï¼ˆå³è·¨æ¨¡æ€ã€è·¨åºåˆ—å’Œè·¨æœºæ„ï¼‰è¿›è¡Œäº†å®éªŒï¼ŒåŒ…æ‹¬å…«ä¸ªåŒ»å­¦æ•°æ®é›†å’Œä¸‰ä¸ªèº«ä½“åŒºåŸŸï¼Œç»“æœè¡¨æ˜æˆ‘ä»¬çš„æ¨¡å‹åœ¨å¯¹æ¯”æ¨¡å‹ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.01110v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†å°æ ·æœ¬åŒ»å­¦å›¾åƒåˆ†å‰²ï¼ˆFSMISï¼‰æŠ€æœ¯åœ¨è·¨ä¸åŒåŒ»å­¦æˆåƒé¢†åŸŸä¸­çš„é€šç”¨åŒ–é—®é¢˜ã€‚é’ˆå¯¹ç°æœ‰FSMISæ¨¡å‹æ— æ³•é€‚åº”ä¸´åºŠå®é™…ä¸­è·¨é¢†åŸŸåŒ»å­¦å›¾åƒæ•°æ®çš„æƒ…å†µï¼Œæå‡ºäº†ä¸€ç§åŸºäºåœ°çƒç§»åŠ¨è·ç¦»ï¼ˆEMDï¼‰è®¡ç®—çš„æ–°å‹ç¨³å¥åŒ¹é…æœºåˆ¶ã€‚è¯¥æœºåˆ¶é€šè¿‡å‰æ™¯æ”¯æŒæŸ¥è¯¢ç‰¹å¾ä¹‹é—´çš„EMDè¿è¾“è¿‡ç¨‹è¿›è¡Œå½¢å¼åŒ–ï¼Œå¼•å…¥çº¹ç†ç»“æ„æ„ŸçŸ¥æƒé‡ç”Ÿæˆæ–¹æ³•ï¼Œå¹¶æå‡ºåœ¨EMDåŒ¹é…æµä¸­è¿›è¡ŒåŸºäºSobelçš„å›¾åƒæ¢¯åº¦è®¡ç®—ï¼Œä»¥é™åˆ¶ä¸é¢†åŸŸç›¸å…³çš„èŠ‚ç‚¹ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†ç‚¹é›†çº§åˆ«è·ç¦»æµ‹é‡æŒ‡æ ‡æ¥è®¡ç®—æ”¯æŒé›†èŠ‚ç‚¹åˆ°æŸ¥è¯¢é›†èŠ‚ç‚¹çš„è¿è¾“æˆæœ¬ã€‚å®éªŒåœ¨ä¸‰ç§åœºæ™¯ï¼ˆè·¨æ¨¡æ€ã€è·¨åºåˆ—å’Œè·¨æœºæ„ï¼‰ä¸‹è¿›è¡Œï¼Œæ¶‰åŠå…«ä¸ªåŒ»å­¦æ•°æ®é›†å’Œä¸‰ä¸ªèº«ä½“åŒºåŸŸï¼Œç»“æœè¡¨æ˜è¯¥æ¨¡å‹ç›¸è¾ƒäºå…¶ä»–æ¨¡å‹è¾¾åˆ°äº†æœ€æ–°æ€§èƒ½æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å°æ ·æœ¬åŒ»å­¦å›¾åƒåˆ†å‰²ï¼ˆFSMISï¼‰é¢ä¸´è·¨é¢†åŸŸæ•°æ®æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰FSMISæ¨¡å‹æ— æ³•é€‚åº”ä¸´åºŠå®é™…ä¸­çš„è·¨é¢†åŸŸåŒ»å­¦å›¾åƒæ•°æ®ã€‚</li>
<li>å¼•å…¥åŸºäºåœ°çƒç§»åŠ¨è·ç¦»ï¼ˆEMDï¼‰è®¡ç®—çš„ç¨³å¥åŒ¹é…æœºåˆ¶ï¼Œä»¥æ”¹è¿›FSMISæ¨¡å‹çš„é€šç”¨æ€§ã€‚</li>
<li>è¯¥æœºåˆ¶é€šè¿‡EMDè¿è¾“è¿‡ç¨‹å½¢å¼åŒ–å‰æ™¯æ”¯æŒæŸ¥è¯¢ç‰¹å¾çš„åŒ¹é…ã€‚</li>
<li>å¼•å…¥çº¹ç†ç»“æ„æ„ŸçŸ¥æƒé‡ç”Ÿæˆæ–¹æ³•å’ŒåŸºäºSobelçš„å›¾åƒæ¢¯åº¦è®¡ç®—ï¼Œä»¥é™åˆ¶ä¸é¢†åŸŸç›¸å…³çš„èŠ‚ç‚¹ã€‚</li>
<li>å®éªŒåœ¨å¤šç§è·¨é¢†åŸŸåœºæ™¯ä¸‹è¿›è¡Œï¼Œæ¶‰åŠå¤šä¸ªåŒ»å­¦æ•°æ®é›†å’Œèº«ä½“åŒºåŸŸã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.01110">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6ead464cd01cc60a5a1bc36b7226bc26.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9694b36b6f2c838aa4e62edcc696614d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9ca9d0e32e4b4033b35a3a671a4b8c4c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2d79e93e1b52bcd63795303434c3a06e.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="PointSAM-Pointly-Supervised-Segment-Anything-Model-for-Remote-Sensing-Images"><a href="#PointSAM-Pointly-Supervised-Segment-Anything-Model-for-Remote-Sensing-Images" class="headerlink" title="PointSAM: Pointly-Supervised Segment Anything Model for Remote Sensing   Images"></a>PointSAM: Pointly-Supervised Segment Anything Model for Remote Sensing   Images</h2><p><strong>Authors:Nanqing Liu, Xun Xu, Yongyi Su, Haojie Zhang, Heng-Chao Li</strong></p>
<p>Segment Anything Model (SAM) is an advanced foundational model for image segmentation, which is gradually being applied to remote sensing images (RSIs). Due to the domain gap between RSIs and natural images, traditional methods typically use SAM as a source pre-trained model and fine-tune it with fully supervised masks. Unlike these methods, our work focuses on fine-tuning SAM using more convenient and challenging point annotations. Leveraging SAMâ€™s zero-shot capabilities, we adopt a self-training framework that iteratively generates pseudo-labels for training. However, if the pseudo-labels contain noisy labels, there is a risk of error accumulation. To address this issue, we extract target prototypes from the target dataset and use the Hungarian algorithm to match them with prediction prototypes, preventing the model from learning in the wrong direction. Additionally, due to the complex backgrounds and dense distribution of objects in RSI, using point prompts may result in multiple objects being recognized as one. To solve this problem, we propose a negative prompt calibration method based on the non-overlapping nature of instance masks. In brief, we use the prompts of overlapping masks as corresponding negative signals, resulting in refined masks. Combining the above methods, we propose a novel Pointly-supervised Segment Anything Model named PointSAM. We conduct experiments on RSI datasets, including WHU, HRSID, and NWPU VHR-10, and the results show that our method significantly outperforms direct testing with SAM, SAM2, and other comparison methods. Furthermore, we introduce PointSAM as a point-to-box converter and achieve encouraging results, suggesting that this method can be extended to other point-supervised tasks. The code is available at <a target="_blank" rel="noopener" href="https://github.com/Lans1ng/PointSAM">https://github.com/Lans1ng/PointSAM</a>. </p>
<blockquote>
<p>åˆ†æ®µä»»ä½•äº‹æ¨¡å‹ï¼ˆSAMï¼‰æ˜¯ä¸€ä¸ªå…ˆè¿›çš„å›¾åƒåˆ†å‰²åŸºç¡€æ¨¡å‹ï¼Œæ­£é€æ¸åº”ç”¨äºé¥æ„Ÿå›¾åƒï¼ˆRSIï¼‰ã€‚ç”±äºé¥æ„Ÿå›¾åƒä¸è‡ªç„¶å›¾åƒé¢†åŸŸä¹‹é—´çš„å·®è·ï¼Œä¼ ç»Ÿæ–¹æ³•é€šå¸¸å°†SAMç”¨ä½œæºé¢„è®­ç»ƒæ¨¡å‹ï¼Œå¹¶ä½¿ç”¨å…¨ç›‘ç£æ©è†œå¯¹å…¶è¿›è¡Œå¾®è°ƒã€‚ä¸åŒäºè¿™äº›æ–¹æ³•ï¼Œæˆ‘ä»¬çš„å·¥ä½œä¸“æ³¨äºä½¿ç”¨æ›´æ–¹ä¾¿ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„ç‚¹æ³¨é‡Šæ¥å¾®è°ƒSAMã€‚æˆ‘ä»¬åˆ©ç”¨SAMçš„é›¶æ ·æœ¬èƒ½åŠ›ï¼Œé‡‡ç”¨è‡ªè®­ç»ƒæ¡†æ¶è¿­ä»£ç”Ÿæˆä¼ªæ ‡ç­¾ç”¨äºè®­ç»ƒã€‚ç„¶è€Œï¼Œå¦‚æœä¼ªæ ‡ç­¾åŒ…å«å™ªå£°æ ‡ç­¾ï¼Œåˆ™å­˜åœ¨è¯¯å·®ç´¯ç§¯çš„é£é™©ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬ä»ç›®æ ‡æ•°æ®é›†ä¸­æå–ç›®æ ‡åŸå‹ï¼Œå¹¶ä½¿ç”¨åŒˆç‰™åˆ©ç®—æ³•å°†å®ƒä»¬ä¸é¢„æµ‹åŸå‹åŒ¹é…ï¼Œé˜²æ­¢æ¨¡å‹å‘é”™è¯¯æ–¹å‘å­¦ä¹ ã€‚æ­¤å¤–ï¼Œç”±äºé¥æ„Ÿå›¾åƒçš„å¤æ‚èƒŒæ™¯å’Œç›®æ ‡å¯¹è±¡çš„å¯†é›†åˆ†å¸ƒï¼Œä½¿ç”¨ç‚¹æç¤ºå¯èƒ½ä¼šå¯¼è‡´å¤šä¸ªå¯¹è±¡è¢«è¯†åˆ«ä¸ºä¸€ä¸ªå¯¹è±¡ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå®ä¾‹æ©è†œéé‡å ç‰¹æ€§çš„è´Ÿæç¤ºæ ¡å‡†æ–¹æ³•ã€‚ç®€è€Œè¨€ä¹‹ï¼Œæˆ‘ä»¬ä½¿ç”¨é‡å æ©è†œçš„æç¤ºä½œä¸ºç›¸åº”çš„è´Ÿä¿¡å·ï¼Œä»è€Œè·å¾—æ›´ç²¾ç»†çš„æ©è†œã€‚ç»“åˆä¸Šè¿°æ–¹æ³•ï¼Œæˆ‘ä»¬æå‡ºäº†åä¸ºPointSAMçš„æ–°å‹ç‚¹ç›‘ç£åˆ†æ®µä»»ä½•äº‹æ¨¡å‹ã€‚æˆ‘ä»¬åœ¨WHUã€HRSIDå’ŒNWPU VHR-10ç­‰RSIæ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Œç»“æœè¡¨æ˜æˆ‘ä»¬çš„æ–¹æ³•æ˜æ˜¾ä¼˜äºç›´æ¥ä½¿ç”¨SAMã€SAM2å’Œå…¶ä»–å¯¹æ¯”æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†PointSAMä½œä¸ºç‚¹åˆ°æ¡†çš„è½¬æ¢å™¨ï¼Œå¹¶è·å¾—äº†ä»¤äººé¼“èˆçš„ç»“æœï¼Œè¿™è¡¨æ˜è¯¥æ–¹æ³•å¯æ‰©å±•åˆ°å…¶ä»–ç‚¹ç›‘ç£ä»»åŠ¡ã€‚ä»£ç å¯åœ¨[<a target="_blank" rel="noopener" href="https://github.com/Lans1ng/PointSAM%E6%89%BE%E5%88%B0%E3%80%82]">https://github.com/Lans1ng/PointSAMæ‰¾åˆ°ã€‚]</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.13401v2">PDF</a> Accepted by IEEE TGRS</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†PointSAMæ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ç§å…ˆè¿›çš„å›¾åƒåˆ†å‰²åŸºç¡€æ¨¡å‹ï¼Œå¹¶åº”ç”¨äºé¥æ„Ÿå›¾åƒã€‚ç ”ç©¶é‡‡ç”¨è‡ªæˆ‘è®­ç»ƒæ¡†æ¶ï¼Œåˆ©ç”¨SAMçš„é›¶æ ·æœ¬èƒ½åŠ›è¿›è¡Œå¾®è°ƒï¼Œè§£å†³é¥æ„Ÿå›¾åƒä¸è‡ªç„¶å›¾åƒé¢†åŸŸé—´çš„å·®è·é—®é¢˜ã€‚ä¸ºè§£å†³ä¼ªæ ‡ç­¾ä¸­çš„å™ªå£°æ ‡ç­¾é—®é¢˜ï¼Œç ”ç©¶é‡‡ç”¨ç›®æ ‡åŸå‹ä¸é¢„æµ‹åŸå‹åŒ¹é…çš„æ–¹æ³•ã€‚åŒæ—¶ï¼Œé’ˆå¯¹é¥æ„Ÿå›¾åƒå¤æ‚èƒŒæ™¯å’Œç‰©ä½“å¯†é›†åˆ†å¸ƒçš„é—®é¢˜ï¼Œç ”ç©¶æå‡ºäº†åŸºäºéé‡å å®ä¾‹æ©è†œçš„è´Ÿæç¤ºæ ¡å‡†æ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPointSAMåœ¨é¥æ„Ÿå›¾åƒæ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚æ­¤å¤–ï¼ŒPointSAMä½œä¸ºç‚¹è‡³ç›’çš„è½¬æ¢å™¨ï¼Œåœ¨ç‚¹ç›‘ç£ä»»åŠ¡ä¸­å–å¾—äº†ä»¤äººé¼“èˆçš„ç»“æœã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>PointSAMæ˜¯ä¸€ä¸ªåŸºäºå›¾åƒåˆ†å‰²çš„å…ˆè¿›æ¨¡å‹ï¼Œç‰¹åˆ«é€‚ç”¨äºé¥æ„Ÿå›¾åƒã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨è‡ªæˆ‘è®­ç»ƒæ¡†æ¶ï¼Œåˆ©ç”¨SAMçš„é›¶æ ·æœ¬èƒ½åŠ›è¿›è¡Œå¾®è°ƒï¼Œä»¥ç¼©å°é¥æ„Ÿå›¾åƒä¸è‡ªç„¶å›¾åƒä¹‹é—´çš„å·®è·ã€‚</li>
<li>é€šè¿‡é‡‡ç”¨ç›®æ ‡åŸå‹ä¸é¢„æµ‹åŸå‹åŒ¹é…çš„æ–¹æ³•ï¼Œè§£å†³äº†ä¼ªæ ‡ç­¾ä¸­çš„å™ªå£°æ ‡ç­¾é—®é¢˜ã€‚</li>
<li>é’ˆå¯¹é¥æ„Ÿå›¾åƒå¤æ‚èƒŒæ™¯å’Œç‰©ä½“å¯†é›†åˆ†å¸ƒçš„é—®é¢˜ï¼Œæå‡ºäº†åŸºäºéé‡å å®ä¾‹æ©è†œçš„è´Ÿæç¤ºæ ¡å‡†æ–¹æ³•ã€‚</li>
<li>PointSAMåœ¨å¤šä¸ªé¥æ„Ÿå›¾åƒæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾è‘—ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</li>
<li>PointSAMè¢«å¼•å…¥ä½œä¸ºç‚¹è‡³ç›’çš„è½¬æ¢å™¨ï¼Œä¸ºç‚¹ç›‘ç£ä»»åŠ¡æä¾›äº†æ–°çš„è§†è§’ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.13401">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6d1dee816a0977db656d064dc3baab23.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bf672564281d1f2749ddcb9f31de4ce3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-408971e00c0619608988cdf5418243a6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-557c093b3d09e7a26e89652314ec5d0b.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="CDDIP-Constrained-Diffusion-Driven-Deep-Image-Prior-for-Seismic-Image-Reconstruction"><a href="#CDDIP-Constrained-Diffusion-Driven-Deep-Image-Prior-for-Seismic-Image-Reconstruction" class="headerlink" title="CDDIP: Constrained Diffusion-Driven Deep Image Prior for Seismic Image   Reconstruction"></a>CDDIP: Constrained Diffusion-Driven Deep Image Prior for Seismic Image   Reconstruction</h2><p><strong>Authors:Paul Goyes-PeÃ±afiel, Ulugbek Kamilov, Henry Arguello</strong></p>
<p>Seismic data frequently exhibits missing traces, substantially affecting subsequent seismic processing and interpretation. Deep learning-based approaches have demonstrated significant advancements in reconstructing irregularly missing seismic data through supervised and unsupervised methods. Nonetheless, substantial challenges remain, such as generalization capacity and computation time cost during the inference. Our work introduces a reconstruction method that uses a pre-trained generative diffusion model for image synthesis and incorporates Deep Image Prior to enforce data consistency when reconstructing missing traces in seismic data. The proposed method has demonstrated strong robustness and high reconstruction capability of post-stack and pre-stack data with different levels of structural complexity, even in field and synthetic scenarios where test data were outside the training domain. This indicates that our method can handle the high geological variability of different exploration targets. Additionally, compared to other state-of-the-art seismic reconstruction methods using diffusion models. During inference, our approach reduces the number of sampling timesteps by up to 4x. </p>
<blockquote>
<p>åœ°éœ‡æ•°æ®ç»å¸¸å­˜åœ¨ç¼ºå¤±é“é›†ï¼Œå¯¹åç»­çš„åœ°éœ‡å¤„ç†å’Œè§£é‡Šäº§ç”Ÿå¾ˆå¤§å½±å“ã€‚åŸºäºæ·±åº¦å­¦ä¹ çš„æ–¹æ³•åœ¨é€šè¿‡æœ‰ç›‘ç£å’Œæ— ç›‘ç£æ–¹æ³•é‡å»ºä¸è§„åˆ™ç¼ºå¤±çš„åœ°éœ‡æ•°æ®æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ã€‚ç„¶è€Œï¼Œä»å­˜åœ¨è¯¸å¤šæŒ‘æˆ˜ï¼Œå¦‚æ¨ç†è¿‡ç¨‹ä¸­çš„æ³›åŒ–èƒ½åŠ›å’Œè®¡ç®—æ—¶é—´æˆæœ¬ã€‚æˆ‘ä»¬çš„å·¥ä½œå¼•å…¥äº†ä¸€ç§é‡å»ºæ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä½¿ç”¨é¢„è®­ç»ƒçš„ç”Ÿæˆæ‰©æ•£æ¨¡å‹è¿›è¡Œå›¾åƒåˆæˆï¼Œå¹¶ç»“åˆæ·±åº¦å›¾åƒå…ˆéªŒåœ¨é‡å»ºåœ°éœ‡æ•°æ®ä¸­ç¼ºå¤±é“é›†æ—¶å¼ºåˆ¶æ•°æ®ä¸€è‡´æ€§ã€‚æ‰€æå‡ºçš„æ–¹æ³•è¡¨ç°å‡ºå¼ºå¤§çš„ç¨³å¥æ€§å’Œé«˜é‡å»ºèƒ½åŠ›ï¼Œèƒ½å¤Ÿå¤„ç†ä¸åŒç»“æ„å¤æ‚æ€§çš„å åŠ å’Œæœªå åŠ æ•°æ®ï¼Œå³ä½¿åœ¨æµ‹è¯•æ•°æ®è¶…å‡ºè®­ç»ƒé¢†åŸŸçš„å®é™…å’Œåˆæˆåœºæ™¯ä¸­äº¦æ˜¯å¦‚æ­¤ã€‚è¿™è¡¨æ˜æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥åº”å¯¹ä¸åŒå‹˜æ¢ç›®æ ‡çš„é«˜åœ°è´¨å˜åŒ–æ€§ã€‚æ­¤å¤–ï¼Œä¸å…¶ä»–ä½¿ç”¨æ‰©æ•£æ¨¡å‹çš„å…ˆè¿›åœ°éœ‡é‡å»ºæ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ¨ç†è¿‡ç¨‹ä¸­å‡å°‘äº†é«˜è¾¾4å€çš„é‡‡æ ·æ—¶é—´æ­¥æ•°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.17402v2">PDF</a> 5 pages, 4 figures, 3 tables. Submitted to geoscience and remote   sensing letters</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åˆ©ç”¨é¢„è®­ç»ƒç”Ÿæˆæ‰©æ•£æ¨¡å‹è¿›è¡Œåœ°éœ‡æ•°æ®é‡å»ºçš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç»“åˆäº†æ·±åº¦å›¾åƒå…ˆéªŒæ¥å¼ºåˆ¶æ•°æ®ä¸€è‡´æ€§ã€‚è¯¥æ–¹æ³•åœ¨ä¸åŒç»“æ„å¤æ‚åº¦çš„åå å‰å æ•°æ®ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„é²æ£’æ€§å’Œé«˜é‡å»ºèƒ½åŠ›ï¼Œå³ä½¿æµ‹è¯•æ•°æ®è¶…å‡ºè®­ç»ƒåŸŸï¼Œä¹Ÿèƒ½å¤„ç†ä¸åŒå‹˜æ¢ç›®æ ‡çš„é«˜åœ°è´¨å˜åŒ–æ€§ã€‚ä¸å…¶ä»–ä½¿ç”¨æ‰©æ•£æ¨¡å‹çš„åœ°éœ‡é‡å»ºæ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨æ¨ç†è¿‡ç¨‹ä¸­å‡å°‘äº†é«˜è¾¾4å€çš„é‡‡æ ·æ—¶é—´æ­¥æ•°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åœ°éœ‡æ•°æ®ç»å¸¸ç¼ºå¤±ç—•è¿¹ï¼Œå½±å“åç»­çš„åœ°éœ‡å¤„ç†å’Œè§£é‡Šã€‚</li>
<li>æ·±åº¦å­¦ä¹ å·²åœ¨é‡å»ºä¸è§„åˆ™ç¼ºå¤±çš„åœ°éœ‡æ•°æ®æ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ï¼ŒåŒ…æ‹¬ç›‘ç£å’Œæ— ç›‘ç£æ–¹æ³•ã€‚</li>
<li>æœ¬æ–‡å¼•å…¥äº†ä¸€ç§åˆ©ç”¨é¢„è®­ç»ƒç”Ÿæˆæ‰©æ•£æ¨¡å‹è¿›è¡Œå›¾åƒåˆæˆçš„æ–¹æ³•ï¼Œå¹¶ç»“åˆæ·±åº¦å›¾åƒå…ˆéªŒæ¥é‡å»ºåœ°éœ‡æ•°æ®ä¸­ç¼ºå¤±çš„ç—•è¿¹ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨ç»“æ„å¤æ‚çš„åå å‰å æ•°æ®ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„é²æ£’æ€§å’Œé«˜é‡å»ºèƒ½åŠ›ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½å¤Ÿå¤„ç†ä¸åŒå‹˜æ¢ç›®æ ‡çš„é«˜åœ°è´¨å˜åŒ–æ€§ï¼Œå³ä½¿åœ¨æµ‹è¯•æ•°æ®è¶…å‡ºè®­ç»ƒåŸŸçš„æƒ…å†µä¸‹ã€‚</li>
<li>ä¸å…¶ä»–ä½¿ç”¨æ‰©æ•£æ¨¡å‹çš„åœ°éœ‡é‡å»ºæ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨æ¨ç†è¿‡ç¨‹ä¸­å‡å°‘äº†é‡‡æ ·æ—¶é—´æ­¥æ•°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.17402">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-44b53855d15cf8f1e6cb7a432a8907ac.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-39c3fd663841fc1aecb6cb78a1e0ffa1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0d4bb5537a0db944a21bf3b5cd300eec.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0fd840dc0bb73af1fecdf68a1ae6ca37.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-18\./crop_åŒ»å­¦å›¾åƒ/2407.17402v2/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-18\./crop_åŒ»å­¦å›¾åƒ/2407.17402v2/page_4_0.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-18/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-18/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-18/TTS/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-e262945a140a0fbcdb7ddae1a7741766.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-18  VITA-1.5 Towards GPT-4o Level Real-Time Vision and Speech Interaction
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-18
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-18/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-fb67e12998d31bb3ee872b18aaab809f.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-18  SynthLight Portrait Relighting with Diffusion Model by Learning to   Re-render Synthetic Faces
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-18
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">31879.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
