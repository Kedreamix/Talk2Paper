<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-20  AutoRule Reasoning Chain-of-thought Extracted Rule-based Rewards   Improve Preference Learning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-dab5064d282966610b2cee845500a27b.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-20
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    86 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-20-æ›´æ–°"><a href="#2025-06-20-æ›´æ–°" class="headerlink" title="2025-06-20 æ›´æ–°"></a>2025-06-20 æ›´æ–°</h1><h2 id="AutoRule-Reasoning-Chain-of-thought-Extracted-Rule-based-Rewards-Improve-Preference-Learning"><a href="#AutoRule-Reasoning-Chain-of-thought-Extracted-Rule-based-Rewards-Improve-Preference-Learning" class="headerlink" title="AutoRule: Reasoning Chain-of-thought Extracted Rule-based Rewards   Improve Preference Learning"></a>AutoRule: Reasoning Chain-of-thought Extracted Rule-based Rewards   Improve Preference Learning</h2><p><strong>Authors:Tevin Wang, Chenyan Xiong</strong></p>
<p>Rule-based rewards offer a promising strategy for improving reinforcement learning from human feedback (RLHF), but current approaches often rely on manual rule engineering. We present AutoRule, a fully automated method for extracting rules from preference feedback and formulating them into rule-based rewards. AutoRule extraction operates in three stages: it leverages a reasoning model to interpret user preferences, identifies candidate rules from the reasoning chain of these interpretations, and synthesizes them into a unified rule set. Leveraging the finalized rule set, we employ language-model verifiers to compute the fraction of rules satisfied by each output, using this metric as an auxiliary reward alongside the learned reward model during policy optimization. Training a Llama-3-8B model with AutoRule results in a 28.6% relative improvement in length-controlled win rate on AlpacaEval2.0, and a 6.1% relative gain in second-turn performance on a held-out MT-Bench subset, compared to a GRPO baseline trained with the same learned reward model but without the rule-based auxiliary reward. Our analysis confirms that the extracted rules exhibit good agreement with dataset preference. We find that AutoRule demonstrates reduced reward hacking compared to a learned reward model when run over two episodes. Finally, our case study suggests that the extracted rules capture unique qualities valued in different datasets. The extracted rules are provided in the appendix, and the code is open-sourced at <a target="_blank" rel="noopener" href="https://github.com/cxcscmu/AutoRule">https://github.com/cxcscmu/AutoRule</a>. </p>
<blockquote>
<p>åŸºäºè§„åˆ™çš„å¥–åŠ±å¯¹äºé€šè¿‡äººç±»åé¦ˆæ”¹è¿›å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰æ˜¯ä¸€ä¸ªå‰æ™¯å…‰æ˜çš„ç­–ç•¥ï¼Œä½†å½“å‰çš„æ–¹æ³•å¾€å¾€ä¾èµ–äºæ‰‹åŠ¨è§„åˆ™å·¥ç¨‹ã€‚æˆ‘ä»¬æå‡ºäº†AutoRuleï¼Œè¿™æ˜¯ä¸€ç§å…¨è‡ªåŠ¨çš„æ–¹æ³•ï¼Œå¯ä»¥ä»åå¥½åé¦ˆä¸­æå–è§„åˆ™ï¼Œå¹¶å°†å®ƒä»¬è½¬åŒ–ä¸ºåŸºäºè§„åˆ™çš„å¥–åŠ±ã€‚AutoRuleçš„æå–æ“ä½œåˆ†ä¸ºä¸‰ä¸ªé˜¶æ®µï¼šå®ƒåˆ©ç”¨æ¨ç†æ¨¡å‹æ¥è§£é‡Šç”¨æˆ·åå¥½ï¼Œä»è¿™äº›è§£é‡Šçš„æ¨ç†é“¾ä¸­è¯†åˆ«å€™é€‰è§„åˆ™ï¼Œå¹¶å°†å®ƒä»¬åˆæˆä¸€ä¸ªç»Ÿä¸€çš„è§„åˆ™é›†ã€‚åˆ©ç”¨æœ€ç»ˆç¡®å®šçš„è§„åˆ™é›†ï¼Œæˆ‘ä»¬é‡‡ç”¨è¯­è¨€æ¨¡å‹éªŒè¯å™¨æ¥è®¡ç®—æ¯ä¸ªè¾“å‡ºæ»¡è¶³çš„è§„åˆ™æ¯”ä¾‹ï¼Œå°†æ­¤æŒ‡æ ‡ä½œä¸ºç­–ç•¥ä¼˜åŒ–æœŸé—´ä¸å­¦åˆ°çš„å¥–åŠ±æ¨¡å‹å¹¶è¡Œçš„è¾…åŠ©å¥–åŠ±ã€‚ä½¿ç”¨AutoRuleè®­ç»ƒLlama-3-8Bæ¨¡å‹çš„ç»“æœæ˜¯ï¼Œåœ¨AlpacaEval2.0ä¸Šé•¿åº¦æ§åˆ¶èƒœç‡ç›¸å¯¹æé«˜äº†28.6%ï¼Œåœ¨ä¸€ä¸ªæœªè®­ç»ƒçš„MT-Benchå­é›†ä¸Šç¬¬äºŒè½®æ€§èƒ½ç›¸å¯¹æé«˜äº†6.1%ï¼Œç›¸å¯¹äºç”¨ç›¸åŒçš„å­¦åˆ°å¥–åŠ±æ¨¡å‹è®­ç»ƒä½†æ— åŸºäºè§„åˆ™çš„è¾…åŠ©å¥–åŠ±çš„GRPOåŸºçº¿ã€‚æˆ‘ä»¬çš„åˆ†æè¯å®ï¼Œæå–çš„è§„åˆ™ä¸æ•°æ®é›†åå¥½è¡¨ç°å‡ºè‰¯å¥½çš„ä¸€è‡´æ€§ã€‚æˆ‘ä»¬å‘ç°ï¼Œä¸å•é›†è¿è¡Œå­¦åˆ°çš„å¥–åŠ±æ¨¡å‹ç›¸æ¯”ï¼ŒAutoRuleæ˜¾ç¤ºå‡ºè¾ƒå°‘çš„å¥–åŠ±æ“çºµã€‚æœ€åï¼Œæˆ‘ä»¬çš„æ¡ˆä¾‹ç ”ç©¶è¡¨æ˜ï¼Œæå–çš„è§„åˆ™æ•æ‰åˆ°äº†ä¸åŒæ•°æ®é›†ä¸­é‡è§†çš„ç‹¬ç‰¹å“è´¨ã€‚æå–çš„è§„åˆ™è¢«åˆ—åœ¨é™„å½•ä¸­ï¼Œä»£ç å·²åœ¨<a target="_blank" rel="noopener" href="https://github.com/cxcscmu/AutoRule%E5%BC%80%E6%BA%90%E3%80%82">https://github.com/cxcscmu/AutoRuleå¼€æºã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.15651v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>è‡ªåŠ¨è§„åˆ™æå–ï¼ˆAutoRuleï¼‰æ˜¯ä¸€ç§ä»åå¥½åé¦ˆä¸­æå–è§„åˆ™å¹¶å°†å…¶è½¬åŒ–ä¸ºåŸºäºè§„åˆ™çš„å¥–åŠ±çš„å®Œå…¨è‡ªåŠ¨åŒ–æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡è§£é‡Šç”¨æˆ·åå¥½ã€è¯†åˆ«å€™é€‰è§„åˆ™å¹¶åˆæˆç»Ÿä¸€è§„åˆ™é›†ä¸‰ä¸ªé˜¶æ®µè¿›è¡Œæ“ä½œã€‚åœ¨ç­–ç•¥ä¼˜åŒ–è¿‡ç¨‹ä¸­ï¼Œé™¤å­¦ä¹ å¥–åŠ±æ¨¡å‹å¤–ï¼Œè¿˜åˆ©ç”¨æœ€ç»ˆè§„åˆ™é›†å’Œè¯­è¨€æ¨¡å‹éªŒè¯å™¨è®¡ç®—è¾“å‡ºæ»¡è¶³çš„è§„åˆ™æ¯”ä¾‹ä½œä¸ºè¾…åŠ©å¥–åŠ±ã€‚ä½¿ç”¨AutoRuleè®­ç»ƒLlama-3-8Bæ¨¡å‹ï¼Œåœ¨AlpacaEval2.0ä¸Šç›¸å¯¹æ”¹è¿›äº†28.6%çš„é•¿åº¦æ§åˆ¶èƒœç‡ï¼Œåœ¨MT-Benchå­é›†çš„ç¬¬äºŒå›åˆæ€§èƒ½ç›¸å¯¹æé«˜äº†6.1%ã€‚åˆ†ææ˜¾ç¤ºï¼Œæå–çš„è§„åˆ™ä¸æ•°æ®é›†åå¥½ä¸€è‡´ï¼Œç›¸è¾ƒäºå­¦ä¹ å¥–åŠ±æ¨¡å‹ï¼ŒAutoRuleçš„å¥–åŠ±ç ´è§£è¡Œä¸ºæœ‰æ‰€å‡å°‘ã€‚æ­¤å¤–ï¼Œæ¡ˆä¾‹ç ”ç©¶è¡¨æ˜ï¼Œæå–çš„è§„åˆ™èƒ½å¤Ÿæ•æ‰åˆ°ä¸åŒæ•°æ®é›†ä¸­é‡è§†çš„ç‹¬ç‰¹å“è´¨ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>AutoRuleæ˜¯ä¸€ç§è‡ªåŠ¨åŒ–æ–¹æ³•ï¼Œå¯ä»¥ä»åå¥½åé¦ˆä¸­æå–è§„åˆ™å¹¶è½¬åŒ–ä¸ºåŸºäºè§„åˆ™çš„å¥–åŠ±ã€‚</li>
<li>AutoRuleé€šè¿‡è§£é‡Šç”¨æˆ·åå¥½ã€è¯†åˆ«å€™é€‰è§„åˆ™å’Œåˆæˆç»Ÿä¸€è§„åˆ™é›†ä¸‰ä¸ªé˜¶æ®µæ“ä½œã€‚</li>
<li>AutoRuleåˆ©ç”¨æœ€ç»ˆè§„åˆ™é›†å’Œè¯­è¨€æ¨¡å‹éªŒè¯å™¨è®¡ç®—è¾“å‡ºæ»¡è¶³çš„è§„åˆ™æ¯”ä¾‹ä½œä¸ºè¾…åŠ©å¥–åŠ±ã€‚</li>
<li>ä½¿ç”¨AutoRuleè®­ç»ƒçš„æ¨¡å‹åœ¨ç›¸å…³æ•°æ®é›†ä¸Šçš„æ€§èƒ½æœ‰æ˜æ˜¾æå‡ã€‚</li>
<li>åˆ†ææ˜¾ç¤ºæå–çš„è§„åˆ™ä¸æ•°æ®é›†åå¥½ä¸€è‡´ã€‚</li>
<li>ç›¸è¾ƒäºå­¦ä¹ å¥–åŠ±æ¨¡å‹ï¼ŒAutoRuleçš„å¥–åŠ±ç ´è§£è¡Œä¸ºæœ‰æ‰€å‡å°‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.15651">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-19d0082f58c0fb0e3dcf13f3653f89c8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6924713df8d3020395849eedfac2dfc4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f26642ef9a115ab9771ce37a5b7171bf.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="RE-IMAGINE-Symbolic-Benchmark-Synthesis-for-Reasoning-Evaluation"><a href="#RE-IMAGINE-Symbolic-Benchmark-Synthesis-for-Reasoning-Evaluation" class="headerlink" title="RE-IMAGINE: Symbolic Benchmark Synthesis for Reasoning Evaluation"></a>RE-IMAGINE: Symbolic Benchmark Synthesis for Reasoning Evaluation</h2><p><strong>Authors:Xinnuo Xu, Rachel Lawrence, Kshitij Dubey, Atharva Pandey, Risa Ueno, Fabian Falck, Aditya V. Nori, Rahul Sharma, Amit Sharma, Javier Gonzalez</strong></p>
<p>Recent Large Language Models (LLMs) have reported high accuracy on reasoning benchmarks. However, it is still unclear whether the observed results arise from true reasoning or from statistical recall of the training set. Inspired by the ladder of causation (Pearl, 2009) and its three levels (associations, interventions and counterfactuals), this paper introduces RE-IMAGINE, a framework to characterize a hierarchy of reasoning ability in LLMs, alongside an automated pipeline to generate problem variations at different levels of the hierarchy. By altering problems in an intermediate symbolic representation, RE-IMAGINE generates arbitrarily many problems that are not solvable using memorization alone. Moreover, the framework is general and can work across reasoning domains, including math, code, and logic. We demonstrate our framework on four widely-used benchmarks to evaluate several families of LLMs, and observe reductions in performance when the models are queried with problem variations. These assessments indicate a degree of reliance on statistical recall for past performance, and open the door to further research targeting skills across the reasoning hierarchy. </p>
<blockquote>
<p>æœ€è¿‘çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ¨ç†åŸºå‡†æµ‹è¯•ä¸ŠæŠ¥å‘Šäº†é«˜å‡†ç¡®ç‡ã€‚ç„¶è€Œï¼Œå°šä¸æ¸…æ¥šè§‚å¯Ÿåˆ°çš„ç»“æœæ˜¯ç”±äºçœŸæ­£çš„æ¨ç†èƒ½åŠ›è¿˜æ˜¯ç”±äºå¯¹è®­ç»ƒé›†çš„ç»Ÿè®¡å›å¿†ã€‚å—å› æœé˜¶æ¢¯ï¼ˆPearlï¼Œ2009ï¼‰åŠå…¶ä¸‰ä¸ªå±‚æ¬¡ï¼ˆå…³è”ã€å¹²é¢„å’Œåäº‹å®ï¼‰çš„å¯å‘ï¼Œæœ¬æ–‡ä»‹ç»äº†RE-IMAGINEï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè¡¨å¾LLMä¸­æ¨ç†èƒ½åŠ›çš„å±‚æ¬¡ç»“æ„çš„æ¡†æ¶ï¼Œä»¥åŠä¸€ä¸ªè‡ªåŠ¨åŒ–ç®¡é“ï¼Œç”¨äºåœ¨ä¸åŒå±‚æ¬¡çš„å±‚æ¬¡ç»“æ„ä¸­ç”Ÿæˆé—®é¢˜å˜ä½“ã€‚RE-IMAGINEé€šè¿‡æ”¹å˜ä¸­é—´ç¬¦å·è¡¨ç¤ºä¸­çš„é—®é¢˜æ¥ç”Ÿæˆä»»æ„å¤šä¸ªé—®é¢˜ï¼Œè¿™äº›é—®é¢˜ä»…å‡­è®°å¿†æ˜¯æ— æ³•è§£å†³çš„ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶æ˜¯é€šç”¨çš„ï¼Œå¯ä»¥åœ¨æ¨ç†é¢†åŸŸï¼ˆåŒ…æ‹¬æ•°å­¦ã€ä»£ç å’Œé€»è¾‘ï¼‰ä¸­å‘æŒ¥ä½œç”¨ã€‚æˆ‘ä»¬åœ¨å››ä¸ªå¹¿æ³›ä½¿ç”¨çš„åŸºå‡†æµ‹è¯•ä¸Šå±•ç¤ºäº†æˆ‘ä»¬çš„æ¡†æ¶ï¼Œå¯¹å¤šä¸ªLLMå®¶æ—è¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶è§‚å¯Ÿåˆ°åœ¨é—®é¢˜å˜ä½“æŸ¥è¯¢æ—¶æ€§èƒ½ä¸‹é™ã€‚è¿™äº›è¯„ä¼°è¡¨æ˜äº†å¯¹è¿‡å»è¡¨ç°çš„ç»Ÿè®¡å›å¿†æœ‰ä¸€å®šçš„ä¾èµ–ï¼Œå¹¶ä¸ºè¿›ä¸€æ­¥ç ”ç©¶é¢å‘æ•´ä¸ªæ¨ç†å±‚æ¬¡ç»“æ„çš„æŠ€èƒ½æ‰“å¼€äº†å¤§é—¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.15455v1">PDF</a> ICML 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºé«˜å‡†ç¡®æ€§ï¼Œä½†å…¶æ˜¯å¦çœŸæ­£å…·å¤‡æ¨ç†èƒ½åŠ›ï¼Œè¿˜æ˜¯ä»…é€šè¿‡ç»Ÿè®¡å›å¿†è®­ç»ƒé›†æ¥å¾—å‡ºç»“æœï¼Œä»ä¸æ˜ç¡®ã€‚æœ¬æ–‡å€Ÿé‰´å› æœé˜¶æ¢¯ï¼ˆPearlï¼Œ2009ï¼‰çš„ä¸‰çº§ï¼ˆå…³è”ã€å¹²é¢„å’Œåäº‹å®ï¼‰ç†è®ºï¼Œæå‡ºRE-IMAGINEæ¡†æ¶ï¼Œç”¨äºåˆ»ç”»LLMä¸­çš„æ¨ç†èƒ½åŠ›å±‚æ¬¡ç»“æ„ï¼Œå¹¶é…ä»¥è‡ªåŠ¨åŒ–ç®¡é“ï¼Œåœ¨ä¸åŒå±‚æ¬¡ç”Ÿæˆé—®é¢˜å˜ä½“ã€‚é€šè¿‡ä¸­é—´ç¬¦å·è¡¨ç¤ºæ³•æ”¹å˜é—®é¢˜ï¼ŒRE-IMAGINEèƒ½å¤Ÿç”Ÿæˆä»…å‡­è®°å¿†æ— æ³•è§£å†³çš„é—®é¢˜ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶å…·æœ‰é€šç”¨æ€§ï¼Œå¯åº”ç”¨äºæ•°å­¦ã€ä»£ç å’Œé€»è¾‘ç­‰é¢†åŸŸçš„æ¨ç†ã€‚åœ¨å››ä¸ªå¹¿æ³›ä½¿ç”¨çš„åŸºå‡†æµ‹è¯•ä¸Šï¼Œæˆ‘ä»¬å¯¹å¤šä¸ªLLMå®¶æ—è¿›è¡Œäº†è¯„ä¼°ï¼Œå‘ç°æ¨¡å‹åœ¨é¢å¯¹é—®é¢˜å˜ä½“æ—¶çš„æ€§èƒ½æœ‰æ‰€ä¸‹é™ã€‚è¿™è¡¨æ˜åœ¨ä¸€å®šç¨‹åº¦ä¸Šä¾èµ–äºç»Ÿè®¡å›å¿†æ¥è·å¾—è¿‡å»çš„è¡¨ç°ï¼Œå¹¶ä¸ºè¿›ä¸€æ­¥ç ”ç©¶å’Œæé«˜ä¸åŒæ¨ç†å±‚æ¬¡ä¸Šçš„æŠ€èƒ½æ‰“å¼€äº†å¤§é—¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°å¯èƒ½å¹¶éçœŸæ­£å…·å¤‡æ¨ç†èƒ½åŠ›ï¼Œè€Œæ˜¯ä¾èµ–äºç»Ÿè®¡å›å¿†è®­ç»ƒé›†çš„ç»“æœã€‚</li>
<li>RE-IMAGINEæ¡†æ¶ç”¨äºåˆ»ç”»LLMä¸­çš„æ¨ç†èƒ½åŠ›å±‚æ¬¡ç»“æ„ï¼ŒåŒ…æ‹¬å…³è”ã€å¹²é¢„å’Œåäº‹å®ä¸‰ä¸ªå±‚æ¬¡ã€‚</li>
<li>RE-IMAGINEé€šè¿‡æ”¹å˜é—®é¢˜çš„ä¸­é—´ç¬¦å·è¡¨ç¤ºæ³•ï¼Œèƒ½å¤Ÿç”Ÿæˆä»…å‡­è®°å¿†æ— æ³•è§£å†³çš„é—®é¢˜å˜ä½“ã€‚</li>
<li>RE-IMAGINEæ¡†æ¶å…·æœ‰é€šç”¨æ€§ï¼Œé€‚ç”¨äºä¸åŒé¢†åŸŸçš„æ¨ç†ï¼Œå¦‚æ•°å­¦ã€ä»£ç å’Œé€»è¾‘ã€‚</li>
<li>åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¯„ä¼°LLMæ—¶ï¼Œå‘ç°æ¨¡å‹åœ¨é¢å¯¹é—®é¢˜å˜ä½“æ—¶çš„æ€§èƒ½ä¸‹é™ã€‚</li>
<li>è¯„ä¼°ç»“æœæŒ‡ç¤ºLLMåœ¨ä¸€å®šç¨‹åº¦ä¸Šä¾èµ–äºç»Ÿè®¡å›å¿†æ¥è·å¾—è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.15455">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-4a75621f12d233087c7aae40ef4a486d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5f27f5da5d3fc5e5e445f8fa19c2683a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3a6dcfa5a4bbe08e20a1ba495a2d7feb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5c33503dd88418547b464021b85ac463.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-15ae70f613dc1f458d5b9873f55eafb1.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Semantically-Aware-Rewards-for-Open-Ended-R1-Training-in-Free-Form-Generation"><a href="#Semantically-Aware-Rewards-for-Open-Ended-R1-Training-in-Free-Form-Generation" class="headerlink" title="Semantically-Aware Rewards for Open-Ended R1 Training in Free-Form   Generation"></a>Semantically-Aware Rewards for Open-Ended R1 Training in Free-Form   Generation</h2><p><strong>Authors:Zongxia Li, Yapei Chang, Yuhang Zhou, Xiyang Wu, Zichao Liang, Yoo Yeon Sung, Jordan Lee Boyd-Graber</strong></p>
<p>Evaluating open-ended long-form generation is challenging because it is hard to define what clearly separates good from bad outputs. Existing methods often miss key aspects like coherence, style, or relevance, or are biased by pretraining data, making open-ended long-form evaluation an underexplored problem. To address this gap, we propose PrefBERT, a scoring model for evaluating open-ended long-form generation in GRPO and guiding its training with distinct rewards for good and bad outputs. Trained on two response evaluation datasets with diverse long-form styles and Likert-rated quality, PrefBERT effectively supports GRPO by offering better semantic reward feedback than traditional metrics ROUGE-L and BERTScore do. Through comprehensive evaluations, including LLM-as-a-judge, human ratings, and qualitative analysis, we show that PrefBERT, trained on multi-sentence and paragraph-length responses, remains reliable across varied long passages and aligns well with the verifiable rewards GRPO needs. Human evaluations confirm that using PrefBERT as the reward signal to train policy models yields responses better aligned with human preferences than those trained with traditional metrics. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/zli12321/long_form_rl">https://github.com/zli12321/long_form_rl</a>. </p>
<blockquote>
<p>è¯„ä¼°å¼€æ”¾å¼é•¿æ–‡æœ¬ç”Ÿæˆå…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºå¾ˆéš¾æ˜ç¡®ç•Œå®šè‰¯å¥½è¾“å‡ºä¸ä¸è‰¯è¾“å‡ºä¹‹é—´çš„åŒºåˆ«ã€‚ç°æœ‰æ–¹æ³•å¸¸å¸¸å¿½ç•¥è¿è´¯æ€§ã€é£æ ¼æˆ–ç›¸å…³æ€§ç­‰é‡è¦æ–¹é¢ï¼Œæˆ–å—åˆ°é¢„è®­ç»ƒæ•°æ®çš„å½±å“ï¼Œä½¿å¾—å¼€æ”¾å¼é•¿æ–‡æœ¬è¯„ä¼°æˆä¸ºä¸€ä¸ªå°šæœªå……åˆ†æ¢ç´¢çš„é—®é¢˜ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†PrefBERTï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºè¯„ä¼°å¼€æ”¾å¼é•¿æ–‡æœ¬ç”Ÿæˆçš„è¯„åˆ†æ¨¡å‹ï¼Œæ—¨åœ¨åœ¨GRPOä¸­å¯¹å…¶è¿›è¡Œè¯„ä¼°ï¼Œå¹¶ä¸ºè‰¯å¥½å’Œä¸è‰¯è¾“å‡ºæä¾›ä¸åŒçš„å¥–åŠ±æ¥æŒ‡å¯¼å…¶è®­ç»ƒã€‚PrefBERTç»è¿‡ä¸¤ä¸ªå“åº”è¯„ä¼°æ•°æ®é›†çš„è®­ç»ƒï¼Œè¿™äº›æ•°æ®é›†å…·æœ‰å¤šæ ·åŒ–çš„é•¿æ–‡æœ¬é£æ ¼å’Œåˆ©å…‹ç‰¹é‡è¡¨è´¨é‡è¯„ä¼°ï¼Œæœ‰æ•ˆåœ°æ”¯æŒGRPOï¼Œæä¾›æ¯”ä¼ ç»ŸæŒ‡æ ‡ROUGE-Lå’ŒBERTScoreæ›´å¥½çš„è¯­ä¹‰å¥–åŠ±åé¦ˆã€‚é€šè¿‡åŒ…æ‹¬LLMä½œä¸ºæ³•å®˜ã€äººç±»è¯„åˆ†å’Œå®šæ€§åˆ†æåœ¨å†…çš„å…¨é¢è¯„ä¼°ï¼Œæˆ‘ä»¬è¯æ˜ï¼ŒPrefBERTåœ¨è®­ç»ƒå¤šå¥å’Œæ®µè½é•¿åº¦çš„å“åº”æ—¶ï¼Œåœ¨å¤šç§é•¿æ®µè½ä¸­è¡¨ç°å¯é ï¼Œå¹¶ä¸”éå¸¸ç¬¦åˆGRPOæ‰€éœ€çš„å¯éªŒè¯å¥–åŠ±ã€‚äººç±»è¯„ä¼°è¯å®ï¼Œä½¿ç”¨PrefBERTä½œä¸ºå¥–åŠ±ä¿¡å·æ¥è®­ç»ƒç­–ç•¥æ¨¡å‹äº§ç”Ÿçš„å“åº”æ›´ç¬¦åˆäººç±»åå¥½ï¼Œè€Œä¸æ˜¯ä½¿ç”¨ä¼ ç»ŸæŒ‡æ ‡è¿›è¡Œè®­ç»ƒã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/zli12321/long_form_rl">https://github.com/zli12321/long_form_rl</a>æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.15068v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºPrefBERTçš„è¯„åˆ†æ¨¡å‹ï¼Œç”¨äºè¯„ä¼°å¼€æ”¾å¼çš„é•¿æ–‡æœ¬ç”Ÿæˆã€‚è¯¥æ¨¡å‹æ—¨åœ¨è§£å†³é•¿æ–‡æœ¬ç”Ÿæˆçš„è¯„ä»·é—®é¢˜ï¼Œèƒ½åŒºåˆ†å¥½åè¾“å‡ºå¹¶ä¸ºå…¶æä¾›ç‹¬ç‰¹çš„å¥–åŠ±ï¼Œä»¥æŒ‡å¯¼è®­ç»ƒè¿‡ç¨‹ã€‚é€šè¿‡å¤šæ•°æ®é›†è®­ç»ƒå’Œå¤šæ–¹é¢çš„è¯„ä¼°ï¼Œè¯å®PrefBERTå¯¹å¤šç§é•¿æ–‡æœ¬éƒ½èƒ½æä¾›å¯é çš„è¯­ä¹‰å¥–åŠ±åé¦ˆï¼Œä¸”ä¸äººç±»åå¥½é«˜åº¦ä¸€è‡´ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼€æ”¾å¼çš„é•¿æ–‡æœ¬ç”Ÿæˆè¯„ä»·å­˜åœ¨æŒ‘æˆ˜ï¼Œéš¾ä»¥æ˜ç¡®åŒºåˆ†ä¼˜è´¨ä¸åŠ£è´¨è¾“å‡ºã€‚</li>
<li>ç°å­˜çš„è¯„ä»·æ–¹æ³•å¸¸å¸¸å¿½ç•¥è¿è´¯æ€§ã€é£æ ¼ã€ç›¸å…³æ€§ç­‰é‡è¦æ–¹é¢ï¼Œæˆ–å—åˆ°é¢„è®­ç»ƒæ•°æ®çš„å½±å“ã€‚</li>
<li>PrefBERTæ¨¡å‹è¢«æå‡ºï¼Œæ—¨åœ¨è§£å†³é•¿æ–‡æœ¬ç”Ÿæˆçš„è¯„ä»·é—®é¢˜ã€‚</li>
<li>PrefBERTé€šè¿‡åŒºåˆ†å¥½åè¾“å‡ºå¹¶æä¾›ç‹¬ç‰¹å¥–åŠ±æ¥æŒ‡å¯¼è®­ç»ƒè¿‡ç¨‹ã€‚</li>
<li>PrefBERTåœ¨å¤šæ•°æ®é›†è®­ç»ƒåï¼Œå¯¹å¤šç§é•¿æ–‡æœ¬éƒ½èƒ½æä¾›å¯é çš„è¯­ä¹‰å¥–åŠ±åé¦ˆã€‚</li>
<li>PrefBERTä¸äººç±»åå¥½é«˜åº¦ä¸€è‡´ï¼Œç»è¿‡å…¶è®­ç»ƒçš„æ”¿ç­–æ¨¡å‹ç”Ÿæˆçš„å“åº”æ›´ç¬¦åˆäººç±»éœ€æ±‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.15068">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-082d1a2de67d4110e009dd9a5ea9d49b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-92e93c8081ab63e21903e834245fad00.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e6462248e7b2dea3f40617de3021075a.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Truncated-Proximal-Policy-Optimization"><a href="#Truncated-Proximal-Policy-Optimization" class="headerlink" title="Truncated Proximal Policy Optimization"></a>Truncated Proximal Policy Optimization</h2><p><strong>Authors:Tiantian Fan, Lingjun Liu, Yu Yue, Jiaze Chen, Chengyi Wang, Qiying Yu, Chi Zhang, Zhiqi Lin, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Bole Ma, Mofan Zhang, Gaohong Liu, Ru Zhang, Haotian Zhou, Cong Xie, Ruidong Zhu, Zhi Zhang, Xin Liu, Mingxuan Wang, Lin Yan, Yonghui Wu</strong></p>
<p>Recently, test-time scaling Large Language Models (LLMs) have demonstrated exceptional reasoning capabilities across scientific and professional tasks by generating long chains-of-thought (CoT). As a crucial component for developing these reasoning models, reinforcement learning (RL), exemplified by Proximal Policy Optimization (PPO) and its variants, allows models to learn through trial and error. However, PPO can be time-consuming due to its inherent on-policy nature, which is further exacerbated by increasing response lengths. In this work, we propose Truncated Proximal Policy Optimization (T-PPO), a novel extension to PPO that improves training efficiency by streamlining policy update and length-restricted response generation. T-PPO mitigates the issue of low hardware utilization, an inherent drawback of fully synchronized long-generation procedures, where resources often sit idle during the waiting periods for complete rollouts. Our contributions are two-folds. First, we propose Extended Generalized Advantage Estimation (EGAE) for advantage estimation derived from incomplete responses while maintaining the integrity of policy learning. Second, we devise a computationally optimized mechanism that allows for the independent optimization of the policy and value models. By selectively filtering prompt and truncated tokens, this mechanism reduces redundant computations and accelerates the training process without sacrificing convergence performance. We demonstrate the effectiveness and efficacy of T-PPO on AIME 2024 with a 32B base model. The experimental results show that T-PPO improves the training efficiency of reasoning LLMs by up to 2.5x and outperforms its existing competitors. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œæµ‹è¯•è§„æ¨¡çš„å·¨å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡ç”Ÿæˆé•¿é“¾æ€ç»´ï¼ˆCoTï¼‰å±•ç¤ºäº†åœ¨ç§‘å­¦å’Œä¸“ä¸šä»»åŠ¡æ–¹é¢çš„å‡ºè‰²æ¨ç†èƒ½åŠ›ã€‚ä½œä¸ºå¼€å‘è¿™äº›æ¨ç†æ¨¡å‹çš„å…³é”®ç»„æˆéƒ¨åˆ†ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰é€šè¿‡è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰åŠå…¶å˜ä½“å±•ç¤ºäº†å…¶é€šè¿‡è¯•é”™å­¦ä¹ çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç”±äºPPOçš„å›ºæœ‰åœ¨çº¿ç­–ç•¥æ€§è´¨ï¼Œéšç€å“åº”é•¿åº¦çš„å¢åŠ ï¼Œå®ƒå¯èƒ½ä¼šå¾ˆè€—æ—¶ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†æˆªæ–­è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆT-PPOï¼‰ï¼Œè¿™æ˜¯PPOçš„ä¸€ç§æ–°å‹æ‰©å±•ï¼Œé€šè¿‡ç®€åŒ–ç­–ç•¥æ›´æ–°å’Œé•¿åº¦å—é™çš„å“åº”ç”Ÿæˆæ¥æé«˜è®­ç»ƒæ•ˆç‡ã€‚T-PPOç¼“è§£äº†ä½ç¡¬ä»¶åˆ©ç”¨ç‡çš„é—®é¢˜ï¼Œè¿™æ˜¯å®Œå…¨åŒæ­¥çš„é•¿ç”Ÿæˆç¨‹åºå›ºæœ‰çš„ç¼ºç‚¹ï¼Œå…¶ä¸­èµ„æºç»å¸¸åœ¨ç­‰å¾…å®Œæ•´rolloutsçš„æœŸé—´å¤„äºç©ºé—²çŠ¶æ€ã€‚æˆ‘ä»¬çš„è´¡çŒ®æœ‰ä¸¤æ–¹é¢ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æå‡ºäº†æ‰©å±•å¹¿ä¹‰ä¼˜åŠ¿ä¼°è®¡ï¼ˆEGAEï¼‰ï¼Œç”¨äºä»ä¸å®Œå…¨å“åº”ä¸­è¿›è¡Œä¼˜åŠ¿ä¼°è®¡ï¼ŒåŒæ—¶ä¿æŒç­–ç•¥å­¦ä¹ çš„å®Œæ•´æ€§ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§è®¡ç®—ä¼˜åŒ–æœºåˆ¶ï¼Œå…è®¸ç­–ç•¥å’Œå€¼æ¨¡å‹çš„ç‹¬ç«‹ä¼˜åŒ–ã€‚é€šè¿‡æœ‰é€‰æ‹©åœ°è¿‡æ»¤æç¤ºå’Œæˆªæ–­ä»¤ç‰Œï¼Œè¯¥æœºåˆ¶å‡å°‘äº†å†—ä½™è®¡ç®—ï¼ŒåŠ é€Ÿäº†è®­ç»ƒè¿‡ç¨‹ï¼ŒåŒæ—¶ä¸ç‰ºç‰²æ”¶æ•›æ€§èƒ½ã€‚æˆ‘ä»¬åœ¨AIME 2024çš„32BåŸºå‡†æ¨¡å‹ä¸Šå±•ç¤ºäº†T-PPOçš„æœ‰æ•ˆæ€§å’Œæ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒT-PPOæé«˜äº†æ¨ç†LLMçš„è®­ç»ƒæ•ˆç‡ï¼Œæœ€é«˜å¯è¾¾2.5å€ï¼Œå¹¶ä¼˜äºç°æœ‰ç«äº‰å¯¹æ‰‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.15050v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºæˆªæ–­è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆT-PPOï¼‰çš„æ–°æ–¹æ³•ï¼Œç”¨äºæ”¹è¿›å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æµ‹è¯•æ—¶é—´ç¼©æ”¾æ¨ç†æ•ˆç‡ã€‚é€šè¿‡ç®€åŒ–ç­–ç•¥æ›´æ–°å’Œé™åˆ¶é•¿åº¦å“åº”ç”Ÿæˆï¼ŒT-PPOè§£å†³äº†è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰åœ¨é•¿æ—¶é—´å“åº”æ—¶çš„ç¡¬ä»¶åˆ©ç”¨ç‡ä½å’Œæ—¶é—´æ¶ˆè€—é—®é¢˜ã€‚æœ¬æ–‡çš„è´¡çŒ®åŒ…æ‹¬æå‡ºæ‰©å±•å¹¿ä¹‰ä¼˜åŠ¿ä¼°è®¡ï¼ˆEGAEï¼‰å’Œè®¡ç®—ä¼˜åŒ–æœºåˆ¶ï¼Œä»¥æé«˜è®­ç»ƒæ•ˆç‡å¹¶åŠ é€Ÿæ”¶æ•›è¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒT-PPOåœ¨AIME 2024æ•°æ®é›†ä¸Šçš„è®­ç»ƒæ•ˆç‡æé«˜äº†é«˜è¾¾2.5å€å¹¶ä¼˜äºç°æœ‰ç«äº‰å¯¹æ‰‹ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>T-PPOæ˜¯PPOçš„ä¸€ç§æ”¹è¿›æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜LLMåœ¨æµ‹è¯•æ—¶çš„æ¨ç†æ•ˆç‡ã€‚</li>
<li>T-PPOé€šè¿‡ç®€åŒ–ç­–ç•¥æ›´æ–°å’Œé™åˆ¶å“åº”é•¿åº¦æ¥è§£å†³PPOåœ¨é•¿æ—¶é—´å“åº”æ—¶çš„ç¼ºé™·ã€‚</li>
<li>EGAEçš„æå‡ºæ˜¯ä¸ºäº†åœ¨ä¸å®Œæ•´å“åº”çš„æƒ…å†µä¸‹è¿›è¡Œä¼˜åŠ¿ä¼°è®¡ï¼ŒåŒæ—¶ä¿æŒç­–ç•¥å­¦ä¹ çš„å®Œæ•´æ€§ã€‚</li>
<li>è®¡ç®—ä¼˜åŒ–æœºåˆ¶å…è®¸ç‹¬ç«‹ä¼˜åŒ–ç­–ç•¥å’Œä»·å€¼æ¨¡å‹ï¼Œå‡å°‘å†—ä½™è®¡ç®—å¹¶åŠ é€Ÿè®­ç»ƒè¿‡ç¨‹ã€‚</li>
<li>T-PPOåœ¨AIME 2024æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒéªŒè¯ï¼Œæ˜¾ç¤ºå…¶è®­ç»ƒæ•ˆç‡æé«˜äº†é«˜è¾¾2.5å€ã€‚</li>
<li>T-PPOç›¸è¾ƒäºç°æœ‰æ–¹æ³•è¡¨ç°å‡ºæ›´å¥½çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.15050">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-29c4e8927a72f11faa37c15d33fc157d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-13b6a95e396f888b009a1d89deed5d98.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-42d780ad13b745ff8ae48d131a2e66ee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-88fb96bcbe8ba0796659266c6c60239f.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Revisiting-Reinforcement-Learning-for-LLM-Reasoning-from-A-Cross-Domain-Perspective"><a href="#Revisiting-Reinforcement-Learning-for-LLM-Reasoning-from-A-Cross-Domain-Perspective" class="headerlink" title="Revisiting Reinforcement Learning for LLM Reasoning from A Cross-Domain   Perspective"></a>Revisiting Reinforcement Learning for LLM Reasoning from A Cross-Domain   Perspective</h2><p><strong>Authors:Zhoujun Cheng, Shibo Hao, Tianyang Liu, Fan Zhou, Yutao Xie, Feng Yao, Yuexin Bian, Yonghao Zhuang, Nilabjo Dey, Yuheng Zha, Yi Gu, Kun Zhou, Yuqi Wang, Yuan Li, Richard Fan, Jianshu She, Chengqian Gao, Abulhair Saparov, Haonan Li, Taylor W. Killian, Mikhail Yurochkin, Zhengzhong Liu, Eric P. Xing, Zhiting Hu</strong></p>
<p>Reinforcement learning (RL) has emerged as a promising approach to improve large language model (LLM) reasoning, yet most open efforts focus narrowly on math and code, limiting our understanding of its broader applicability to general reasoning. A key challenge lies in the lack of reliable, scalable RL reward signals across diverse reasoning domains. We introduce Guru, a curated RL reasoning corpus of 92K verifiable examples spanning six reasoning domainsâ€“Math, Code, Science, Logic, Simulation, and Tabularâ€“each built through domain-specific reward design, deduplication, and filtering to ensure reliability and effectiveness for RL training. Based on Guru, we systematically revisit established findings in RL for LLM reasoning and observe significant variation across domains. For example, while prior work suggests that RL primarily elicits existing knowledge from pretrained models, our results reveal a more nuanced pattern: domains frequently seen during pretraining (Math, Code, Science) easily benefit from cross-domain RL training, while domains with limited pretraining exposure (Logic, Simulation, and Tabular) require in-domain training to achieve meaningful performance gains, suggesting that RL is likely to facilitate genuine skill acquisition. Finally, we present Guru-7B and Guru-32B, two models that achieve state-of-the-art performance among open models RL-trained with publicly available data, outperforming best baselines by 7.9% and 6.7% on our 17-task evaluation suite across six reasoning domains. We also show that our models effectively improve the Pass@k performance of their base models, particularly on complex tasks less likely to appear in pretraining data. We release data, models, training and evaluation code to facilitate general-purpose reasoning at: <a target="_blank" rel="noopener" href="https://github.com/LLM360/Reasoning360">https://github.com/LLM360/Reasoning360</a> </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å·²æˆä¸ºæé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†èƒ½åŠ›çš„ä¸€ç§æœ‰å‰é€”çš„æ–¹æ³•ï¼Œç„¶è€Œå¤§å¤šæ•°å¼€æ”¾çš„åŠªåŠ›éƒ½é›†ä¸­åœ¨æ•°å­¦å’Œä»£ç ä¸Šï¼Œè¿™é™åˆ¶äº†æˆ‘ä»¬å¯¹å…¶åœ¨æ›´å¹¿æ³›æ¨ç†ä¸­é€‚ç”¨æ€§çš„ç†è§£ã€‚ä¸€ä¸ªå…³é”®æŒ‘æˆ˜åœ¨äºç¼ºä¹å¯é ã€å¯æ‰©å±•çš„RLå¥–åŠ±ä¿¡å·æ¥æ¶µç›–ä¸åŒçš„æ¨ç†é¢†åŸŸã€‚æˆ‘ä»¬æ¨å‡ºäº†Guruï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«92Kä¸ªå¯éªŒè¯ç¤ºä¾‹çš„å®šåˆ¶RLæ¨ç†è¯­æ–™åº“ï¼Œæ¶µç›–å…­ä¸ªæ¨ç†é¢†åŸŸâ€”â€”æ•°å­¦ã€ä»£ç ã€ç§‘å­¦ã€é€»è¾‘ã€æ¨¡æ‹Ÿå’Œè¡¨æ ¼ï¼Œæ¯ä¸ªé¢†åŸŸéƒ½é€šè¿‡ç‰¹å®šçš„å¥–åŠ±è®¾è®¡ã€å»é‡å’Œè¿‡æ»¤æ¥ç¡®ä¿RLè®­ç»ƒçš„å¯é æ€§å’Œæœ‰æ•ˆæ€§ã€‚åŸºäºGuruï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°å›é¡¾äº†LLMæ¨ç†ä¸­RLçš„æ—¢æœ‰å‘ç°ï¼Œå¹¶è§‚å¯Ÿåˆ°å„é¢†åŸŸä¹‹é—´çš„æ˜¾è‘—å·®å¼‚ã€‚ä¾‹å¦‚ï¼Œè™½ç„¶ä¹‹å‰çš„ç ”ç©¶è¡¨æ˜RLä¸»è¦æ¿€å‘é¢„è®­ç»ƒæ¨¡å‹ä¸­çš„ç°æœ‰çŸ¥è¯†ï¼Œä½†æˆ‘ä»¬çš„ç»“æœæ­ç¤ºäº†ä¸€ä¸ªæ›´å¾®å¦™çš„æ¨¡å¼ï¼šåœ¨é¢„è®­ç»ƒæœŸé—´ç»å¸¸é‡åˆ°çš„é¢†åŸŸï¼ˆæ•°å­¦ã€ä»£ç ã€ç§‘å­¦ï¼‰å¾ˆå®¹æ˜“ä»è·¨åŸŸRLè®­ç»ƒä¸­å—ç›Šï¼Œè€Œé¢„è®­ç»ƒæš´éœ²æœ‰é™çš„é¢†åŸŸï¼ˆé€»è¾‘ã€æ¨¡æ‹Ÿå’Œè¡¨æ ¼ï¼‰éœ€è¦é¢†åŸŸå†…çš„è®­ç»ƒæ‰èƒ½å®ç°æœ‰æ„ä¹‰çš„æ€§èƒ½æå‡ï¼Œè¿™è¡¨æ˜RLå¯èƒ½æœ‰åŠ©äºçœŸæ­£çš„æŠ€èƒ½è·å–ã€‚æœ€åï¼Œæˆ‘ä»¬æ¨å‡ºäº†Guru-7Bå’ŒGuru-32Bä¸¤ä¸ªæ¨¡å‹ï¼Œè¿™ä¸¤ä¸ªæ¨¡å‹åœ¨å…¬å¼€æ•°æ®ä¸Šç»è¿‡RLè®­ç»ƒä¸”è¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œåœ¨æˆ‘ä»¬çš„æ¶µç›–å…­ä¸ªæ¨ç†é¢†åŸŸçš„17é¡¹ä»»åŠ¡è¯„ä¼°å¥—ä»¶ä¸­ï¼Œç›¸è¾ƒäºæœ€ä½³åŸºçº¿æ¨¡å‹åˆ†åˆ«æå‡äº†7.9%å’Œ6.7%ã€‚æˆ‘ä»¬è¿˜è¯æ˜äº†æˆ‘ä»¬çš„æ¨¡å‹æœ‰æ•ˆåœ°æé«˜äº†å…¶åŸºç¡€æ¨¡å‹çš„Pass@kæ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨é¢„è®­ç»ƒæ•°æ®ä¸­ä¸å¤ªå¯èƒ½å‡ºç°çš„å¤æ‚ä»»åŠ¡ä¸Šã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/LLM360/Reasoning360">https://github.com/LLM360/Reasoning360</a>ä¸Šå‘å¸ƒäº†æ•°æ®ã€æ¨¡å‹å’Œè®­ç»ƒè¯„ä¼°ä»£ç ï¼Œä»¥ä¿ƒè¿›é€šç”¨æ¨ç†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.14965v1">PDF</a> 38 pages, 9 figures. Under review</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†ä¸­çš„åº”ç”¨ã€‚é’ˆå¯¹ç°æœ‰ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨æ•°å­¦å’Œä»£ç é¢†åŸŸçš„å±€é™æ€§ï¼Œæå‡ºäº†Guruï¼Œä¸€ä¸ªæ¶µç›–å…­å¤§æ¨ç†é¢†åŸŸçš„å¯é å¯æ‰©å±•çš„RLæ¨ç†è¯­æ–™åº“ã€‚åŸºäºGuruï¼Œæœ¬æ–‡ç³»ç»Ÿåœ°å›é¡¾äº†RLåœ¨LLMæ¨ç†ä¸­çš„æ—¢æœ‰å‘ç°ï¼Œå¹¶è§‚å¯Ÿåˆ°ä¸åŒé¢†åŸŸé—´çš„æ˜¾è‘—å˜åŒ–ã€‚ç ”ç©¶å‘ç°ï¼ŒRLä¸ä»…èƒ½æ¿€å‘é¢„è®­ç»ƒæ¨¡å‹ä¸­çš„ç°æœ‰çŸ¥è¯†ï¼Œè€Œä¸”åœ¨é¢„è®­ç»ƒå¸¸è§çš„é¢†åŸŸï¼ˆå¦‚æ•°å­¦ã€ä»£ç ã€ç§‘å­¦ï¼‰ä¸­å®¹æ˜“è·å¾—è·¨åŸŸRLè®­ç»ƒçš„å¥½å¤„ã€‚å¯¹äºé¢„è®­ç»ƒæš´éœ²è¾ƒå°‘çš„é¢†åŸŸï¼ˆå¦‚é€»è¾‘ã€æ¨¡æ‹Ÿå’Œè¡¨æ ¼ï¼‰ï¼Œåˆ™éœ€è¦é¢†åŸŸå†…çš„è®­ç»ƒæ‰èƒ½å®ç°æœ‰æ„ä¹‰çš„æ€§èƒ½æå‡ã€‚æœ€åï¼Œä»‹ç»äº†ä½¿ç”¨å…¬å¼€æ•°æ®è®­ç»ƒçš„Guru-7Bå’ŒGuru-32Bæ¨¡å‹ï¼Œè¿™ä¸¤ä¸ªæ¨¡å‹åœ¨å…­ä¸ªæ¨ç†é¢†åŸŸçš„17é¡¹ä»»åŠ¡è¯„ä¼°ä¸­è¾¾åˆ°äº†æœ€æ–°çš„æŠ€æœ¯æ°´å¹³ï¼Œå¹¶æœ‰æ•ˆåœ°æé«˜äº†åŸºçº¿æ¨¡å‹çš„Pass@kæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç”¨äºæé«˜å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›å…·æœ‰å¹¿é˜”å‰æ™¯ã€‚</li>
<li>å½“å‰ç ”ç©¶ä¸»è¦å…³æ³¨æ•°å­¦å’Œä»£ç é¢†åŸŸçš„RLåº”ç”¨ï¼Œç¼ºä¹å¯¹æ›´å¹¿æ³›é¢†åŸŸçš„ç†è§£ã€‚</li>
<li>Guruæ˜¯ä¸€ä¸ªæ¶µç›–å…­å¤§æ¨ç†é¢†åŸŸçš„å¯é å¯æ‰©å±•çš„RLæ¨ç†è¯­æ–™åº“ã€‚</li>
<li>åŸºäºGuruçš„ç ”ç©¶å‘ç°ï¼ŒRLåœ¨ä¸åŒæ¨ç†é¢†åŸŸä¸­çš„æ•ˆæœå­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚</li>
<li>é¢„è®­ç»ƒå¸¸è§çš„é¢†åŸŸï¼ˆæ•°å­¦ã€ä»£ç ã€ç§‘å­¦ï¼‰å®¹æ˜“ä»RLè®­ç»ƒä¸­å—ç›Šï¼Œè€Œé¢„è®­ç»ƒæš´éœ²è¾ƒå°‘çš„é¢†åŸŸåˆ™éœ€è¦æ›´é’ˆå¯¹æ€§çš„è®­ç»ƒã€‚</li>
<li>Guru-7Bå’ŒGuru-32Bæ¨¡å‹åœ¨å¤šä¸ªä»»åŠ¡è¯„ä¼°ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œæœ‰æ•ˆæé«˜äº†åŸºçº¿æ¨¡å‹çš„Pass@kæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.14965">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-2e6f8d45d0801dbc7332b250cfcaf4b1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c84dd04c20fe8388210ff3274468b95b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1466b8b9df436c36205203a55b45ffd2.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="PeRL-Permutation-Enhanced-Reinforcement-Learning-for-Interleaved-Vision-Language-Reasoning"><a href="#PeRL-Permutation-Enhanced-Reinforcement-Learning-for-Interleaved-Vision-Language-Reasoning" class="headerlink" title="PeRL: Permutation-Enhanced Reinforcement Learning for Interleaved   Vision-Language Reasoning"></a>PeRL: Permutation-Enhanced Reinforcement Learning for Interleaved   Vision-Language Reasoning</h2><p><strong>Authors:Yizhen Zhang, Yang Ding, Shuoshuo Zhang, Xinchen Zhang, Haoling Li, Zhong-zhi Li, Peijie Wang, Jie Wu, Lei Ji, Yelong Shen, Yujiu Yang, Yeyun Gong</strong></p>
<p>Inspired by the impressive reasoning capabilities demonstrated by reinforcement learning approaches like DeepSeek-R1, recent emerging research has begun exploring the use of reinforcement learning (RL) to enhance vision-language models (VLMs) for multimodal reasoning tasks. However, most existing multimodal reinforcement learning approaches remain limited to spatial reasoning within single-image contexts, yet still struggle to generalize to more complex and real-world scenarios involving multi-image positional reasoning, where understanding the relationships across images is crucial. To address this challenge, we propose a general reinforcement learning approach PeRL tailored for interleaved multimodal tasks, and a multi-stage strategy designed to enhance the exploration-exploitation trade-off, thereby improving learning efficiency and task performance. Specifically, we introduce permutation of image sequences to simulate varied positional relationships to explore more spatial and positional diversity. Furthermore, we design a rollout filtering mechanism for resampling to focus on trajectories that contribute most to learning optimal behaviors to exploit learned policies effectively. We evaluate our model on 5 widely-used multi-image benchmarks and 3 single-image benchmarks. Our experiments confirm that PeRL trained model consistently surpasses R1-related and interleaved VLM baselines by a large margin, achieving state-of-the-art performance on multi-image benchmarks, while preserving comparable performance on single-image tasks. </p>
<blockquote>
<p>å—DeepSeek-R1ç­‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•çš„å‡ºè‰²æ¨ç†èƒ½åŠ›å¯å‘ï¼Œæœ€æ–°çš„ç ”ç©¶å¼€å§‹æ¢ç´¢ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¥å¢å¼ºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰çš„å¤šæ¨¡æ€å¼ºåŒ–å­¦ä¹ æ–¹æ³•ä»ç„¶ä»…é™äºå•å›¾åƒä¸Šä¸‹æ–‡ä¸­çš„ç©ºé—´æ¨ç†ï¼Œä½†åœ¨æ¶‰åŠå¤šå›¾åƒä½ç½®æ¨ç†çš„æ›´å¤æ‚å’Œç°å®ä¸–ç•Œåœºæ™¯ä¸­ï¼Œå®ƒä»¬ä»ç„¶éš¾ä»¥æ¨å¹¿ï¼Œç†è§£è·¨å›¾åƒçš„å…³ç³»æ˜¯è‡³å…³é‡è¦çš„ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é’ˆå¯¹äº¤ç»‡å¤šæ¨¡æ€ä»»åŠ¡çš„é€šç”¨å¼ºåŒ–å­¦ä¹ æ–¹æ³•PeRLï¼Œä»¥åŠä¸€ç§æ—¨åœ¨å¢å¼ºæ¢ç´¢ä¸åˆ©ç”¨ä¹‹é—´æƒè¡¡çš„å¤šé˜¶æ®µç­–ç•¥ï¼Œä»è€Œæé«˜å­¦ä¹ æ•ˆç‡å¹¶æ”¹å–„ä»»åŠ¡æ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥å›¾åƒåºåˆ—çš„æ’åˆ—æ¥æ¨¡æ‹Ÿä¸åŒçš„ä½ç½®å…³ç³»ï¼Œä»¥æ¢ç´¢æ›´å¤šçš„ç©ºé—´å’Œä½ç½®å¤šæ ·æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§é‡é‡‡æ ·æ»šåŠ¨è¿‡æ»¤æœºåˆ¶ï¼Œä¸“æ³¨äºå¯¹å­¦ä¹ æœ€ä¼˜è¡Œä¸ºè´¡çŒ®æœ€å¤§çš„è½¨è¿¹ï¼Œä»¥æœ‰æ•ˆåˆ©ç”¨å­¦ä¹ åˆ°çš„ç­–ç•¥ã€‚æˆ‘ä»¬åœ¨äº”ä¸ªå¸¸ç”¨çš„å¤šå›¾åƒåŸºå‡†æµ‹è¯•å’Œä¸‰ä¸ªå•å›¾åƒåŸºå‡†æµ‹è¯•ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ¨¡å‹ã€‚å®éªŒè¯å®ï¼ŒPeRLè®­ç»ƒå‡ºçš„æ¨¡å‹å§‹ç»ˆå¤§å¹…åº¦è¶…è¶Šäº†ä¸R1ç›¸å…³çš„äº¤ç»‡VLMåŸºå‡†æµ‹è¯•ï¼Œåœ¨å¤šå›¾åƒåŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶åœ¨å•å›¾åƒä»»åŠ¡ä¸Šä¿æŒäº†ç›¸å½“çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.14907v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨æå‡è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡èƒ½åŠ›æ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•çš„å±€é™æ€§ï¼Œæå‡ºä¸€ç§é€‚ç”¨äºåµŒå¥—å¤šæ¨¡æ€ä»»åŠ¡çš„é€šç”¨å¼ºåŒ–å­¦ä¹ æ¡†æ¶PeRLå’Œå¤šé˜¶æ®µç­–ç•¥ï¼Œé€šè¿‡æ¨¡æ‹Ÿä¸åŒçš„å›¾åƒåºåˆ—æ’åˆ—å…³ç³»ï¼Œå¢å¼ºç©ºé—´ä¸ä½ç½®å¤šæ ·æ€§ï¼Œå¹¶è®¾è®¡é‡é‡‡æ ·ä¸­çš„æ»šåŠ¨è¿‡æ»¤æœºåˆ¶ï¼Œä¸“æ³¨äºå­¦ä¹ æœ€ä¼˜è¡Œä¸ºçš„è½¨è¿¹ã€‚å®éªŒè¯æ˜ï¼ŒPeRLæ¨¡å‹åœ¨å¤šå›¾åƒåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å“è¶Šï¼Œè¿œè¶…åŒç±»æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ åœ¨æå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡æ–¹é¢å­˜åœ¨æ½œåŠ›ã€‚</li>
<li>æå‡ºé€‚ç”¨äºåµŒå¥—å¤šæ¨¡æ€ä»»åŠ¡çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶PeRLå’Œå¤šé˜¶æ®µç­–ç•¥ï¼Œä»¥æå‡å­¦ä¹ æ•ˆç‡å’Œä»»åŠ¡æ€§èƒ½ã€‚</li>
<li>é€šè¿‡æ¨¡æ‹Ÿå›¾åƒåºåˆ—çš„ä¸åŒæ’åˆ—å…³ç³»æ¥å¢å¼ºç©ºé—´ä¸ä½ç½®å¤šæ ·æ€§ã€‚</li>
<li>è®¾è®¡é‡é‡‡æ ·ä¸­çš„æ»šåŠ¨è¿‡æ»¤æœºåˆ¶ï¼Œèšç„¦äºå¯¹å­¦ä¹ æœ€ä¼˜è¡Œä¸ºæœ‰ç›Šçš„è½¨è¿¹ã€‚</li>
<li>PeRLæ¨¡å‹åœ¨å¤šå›¾åƒåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜è¶Šï¼Œè¶…è¶Šç°æœ‰æ–¹æ³•ã€‚</li>
<li>åœ¨å•å›¾åƒä»»åŠ¡ä¸Šä¿æŒä¸ç°æœ‰æ–¹æ³•ç›¸å½“çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.14907">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-11452b2b339b55c26f004157f1a69a4b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-68f6dadc1c41b6309dd7bcb63f237fc1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dbe7fc0b164ab7ba8f5209ac4314fcba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a74d796dec0cbf741b9e9b3c275b4213.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-809871376760bd52ee9036d771a0999a.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Optimizing-Length-Compression-in-Large-Reasoning-Models"><a href="#Optimizing-Length-Compression-in-Large-Reasoning-Models" class="headerlink" title="Optimizing Length Compression in Large Reasoning Models"></a>Optimizing Length Compression in Large Reasoning Models</h2><p><strong>Authors:Zhengxiang Cheng, Dongping Chen, Mingyang Fu, Tianyi Zhou</strong></p>
<p>Large Reasoning Models (LRMs) have achieved remarkable success, yet they often suffer from producing unnecessary and verbose reasoning chains. We identify a core aspect of this issue as â€œinvalid thinkingâ€ â€“ models tend to repeatedly double-check their work after having derived the correct answer. To address this specific inefficiency, we move beyond the general principles of Efficacy and Efficiency to propose two new, fine-grained principles: Brevity, which advocates for eliminating redundancy, and Sufficiency, which ensures critical reasoning steps are preserved. Guided by these principles, we introduce LC-R1, a post-training method based on Group Relative Policy Optimization (GRPO). LC-R1 employs a novel combination of a Length Reward for overall conciseness and a Compress Reward that is specifically designed to remove the invalid portion of the thinking process. Extensive experiments on multiple reasoning benchmarks demonstrate that LC-R1 achieves a significant reduction in sequence length (<del>50%) with only a marginal (</del>2%) drop in accuracy, achieving a favorable trade-off point on the Pareto frontier that prioritizes high compression. Our analysis further validates the robustness of LC-R1 and provides valuable insights for developing more powerful yet computationally efficient LRMs. Our code is released at <a target="_blank" rel="noopener" href="https://github.com/zxiangx/LC-R1">https://github.com/zxiangx/LC-R1</a>. </p>
<blockquote>
<p>å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰å·²ç»å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œä½†å®ƒä»¬å¸¸å¸¸ä¼šäº§ç”Ÿä¸å¿…è¦ä¸”å†—é•¿çš„æ¨ç†é“¾ã€‚æˆ‘ä»¬ç¡®å®šè¿™ä¸ªé—®é¢˜çš„æ ¸å¿ƒåœ¨äºâ€œæ— æ•ˆæ€è€ƒâ€â€”â€”åœ¨å¾—å‡ºæ­£ç¡®ç­”æ¡ˆåï¼Œæ¨¡å‹å¾€å¾€å€¾å‘äºåå¤æ ¡éªŒè‡ªå·±çš„å·¥ä½œã€‚ä¸ºäº†è§£å†³è¿™ç§ç‰¹å®šçš„ä½æ•ˆé—®é¢˜ï¼Œæˆ‘ä»¬è¶…è¶Šäº†æ•ˆèƒ½å’Œæ•ˆç‡çš„ä¸€èˆ¬åŸåˆ™ï¼Œæå‡ºäº†ä¸¤ä¸ªæ–°çš„ç²¾ç»†åŸåˆ™ï¼šç®€æ´æ€§ï¼Œä¸»å¼ æ¶ˆé™¤å†—ä½™ï¼›å……è¶³æ€§ï¼Œç¡®ä¿å…³é”®çš„æ¨ç†æ­¥éª¤å¾—åˆ°ä¿ç•™ã€‚åœ¨è¿™äº›åŸåˆ™çš„æŒ‡å¯¼ä¸‹ï¼Œæˆ‘ä»¬å¼•å…¥äº†LC-R1ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰çš„åè®­ç»ƒæ–¹æ³•ã€‚LC-R1é‡‡ç”¨äº†ä¸€ç§æ–°å‹çš„é•¿åº¦å¥–åŠ±æ¥é¼“åŠ±æ•´ä½“ç®€æ´æ€§ï¼Œä»¥åŠä¸€ç§ä¸“é—¨è®¾è®¡çš„å‹ç¼©å¥–åŠ±ï¼Œä»¥æ¶ˆé™¤æ€è€ƒè¿‡ç¨‹ä¸­çš„æ— æ•ˆéƒ¨åˆ†ã€‚åœ¨å¤šä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒLC-R1å®ç°äº†åºåˆ—é•¿åº¦æ˜¾è‘—å‡å°‘ï¼ˆ<del>50%ï¼‰ï¼ŒåŒæ—¶ä»…ä¼´æœ‰è½»å¾®ï¼ˆ</del>2%ï¼‰çš„å‡†ç¡®åº¦ä¸‹é™ï¼Œåœ¨ä¼˜å…ˆé«˜å‹ç¼©ç‡çš„å¸•ç´¯æ‰˜å‰æ²¿è¾¾åˆ°äº†æœ‰åˆ©çš„å¹³è¡¡ç‚¹ã€‚æˆ‘ä»¬çš„è¿›ä¸€æ­¥åˆ†æéªŒè¯äº†LC-R1çš„ç¨³å¥æ€§ï¼Œå¹¶ä¸ºå¼€å‘æ›´å¼ºå¤§ä¸”è®¡ç®—æ•ˆç‡æ›´é«˜çš„LRMsæä¾›äº†æœ‰ä»·å€¼çš„è§è§£ã€‚æˆ‘ä»¬çš„ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/zxiangx/LC-R1%E3%80%82">https://github.com/zxiangx/LC-R1ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.14755v1">PDF</a> 16 pages, 7 figures, 4 tables</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œä½†å¸¸å¸¸ä¼šäº§ç”Ÿä¸å¿…è¦ä¸”å†—é•¿çš„æ¨ç†é“¾ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸¤ä¸ªæ–°çš„ç²¾ç»†åŸåˆ™ï¼šç®€æ´æ€§ï¼Œæ—¨åœ¨æ¶ˆé™¤å†—ä½™ï¼›å……è¶³æ€§ï¼Œç¡®ä¿å…³é”®æ¨ç†æ­¥éª¤å¾—åˆ°ä¿ç•™ã€‚åŸºäºè¿™äº›åŸåˆ™ï¼Œæœ¬æ–‡å¼•å…¥äº†ä¸€ç§åŸºäºç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰çš„LC-R1åè®­ç»ƒæ–¹æ³•ã€‚å®éªŒè¯æ˜ï¼ŒLC-R1åœ¨ä¿æŒè¾ƒä½ç²¾åº¦æŸå¤±çš„åŒæ—¶ï¼Œå®ç°äº†æ˜¾è‘—çš„åºåˆ—é•¿åº¦ç¼©å‡ï¼Œå®ç°äº†å¸•ç´¯æ‰˜å‰æ²¿ä¸Šçš„æœ‰åˆ©æƒè¡¡ç‚¹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰å­˜åœ¨äº§ç”Ÿä¸å¿…è¦å’Œå†—é•¿æ¨ç†é“¾çš„é—®é¢˜ã€‚</li>
<li>é—®é¢˜çš„æ ¸å¿ƒåœ¨äºâ€œæ— æ•ˆæ€è€ƒâ€â€”â€”æ¨¡å‹åœ¨å¾—å‡ºæ­£ç¡®ç­”æ¡ˆåï¼Œå¾€å¾€å€¾å‘äºåå¤æ£€æŸ¥è‡ªå·±çš„å·¥ä½œã€‚</li>
<li>ä¸ºè§£å†³è¿™ä¸€ç‰¹å®šé—®é¢˜ï¼Œæå‡ºäº†ä¸¤ä¸ªæ–°çš„ç²¾ç»†åŸåˆ™ï¼šç®€æ´æ€§å’Œå……è¶³æ€§ã€‚</li>
<li>LC-R1æ˜¯ä¸€ç§åŸºäºç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰çš„åè®­ç»ƒæ–¹æ³•ï¼Œæ—¨åœ¨æ¶ˆé™¤å†—ä½™å¹¶ä¼˜åŒ–æ¨ç†è¿‡ç¨‹ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒLC-R1åœ¨å¤šä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æ˜¾è‘—åºåˆ—é•¿åº¦ç¼©å‡ï¼ŒåŒæ—¶ä¿æŒè¾ƒä½çš„ç²¾åº¦æŸå¤±ã€‚</li>
<li>LC-R1åœ¨å¸•ç´¯æ‰˜å‰æ²¿ä¸Šå®ç°äº†æœ‰åˆ©çš„æƒè¡¡ç‚¹ï¼Œå³åœ¨å‹ç¼©å’Œæ€§èƒ½ä¹‹é—´å–å¾—äº†è‰¯å¥½çš„å¹³è¡¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.14755">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e49c6539055be5e96fd678ced316f943.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0a46093bcdb9ce4d927a778251df3bbb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8e6f968127eb22ed59bb3aabf5b43827.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9681904358f386b504450272605d5192.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-10c071dda87714b3bf400682e04eb54e.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Ring-lite-Scalable-Reasoning-via-C3PO-Stabilized-Reinforcement-Learning-for-LLMs"><a href="#Ring-lite-Scalable-Reasoning-via-C3PO-Stabilized-Reinforcement-Learning-for-LLMs" class="headerlink" title="Ring-lite: Scalable Reasoning via C3PO-Stabilized Reinforcement Learning   for LLMs"></a>Ring-lite: Scalable Reasoning via C3PO-Stabilized Reinforcement Learning   for LLMs</h2><p><strong>Authors: Ling Team, Bin Hu, Cai Chen, Deng Zhao, Ding Liu, Dingnan Jin, Feng Zhu, Hao Dai, Hongzhi Luan, Jia Guo, Jiaming Liu, Jiewei Wu, Jun Mei, Jun Zhou, Junbo Zhao, Junwu Xiong, Kaihong Zhang, Kuan Xu, Lei Liang, Liang Jiang, Liangcheng Fu, Longfei Zheng, Qiang Gao, Qing Cui, Quan Wan, Shaomian Zheng, Shuaicheng Li, Tongkai Yang, Wang Ren, Xiaodong Yan, Xiaopei Wan, Xiaoyun Feng, Xin Zhao, Xinxing Yang, Xinyu Kong, Xuemin Yang, Yang Li, Yingting Wu, Yongkang Liu, Zhankai Xu, Zhenduo Zhang, Zhenglei Zhou, Zhenyu Huang, Zhiqiang Zhang, Zihao Wang, Zujie Wen</strong></p>
<p>We present Ring-lite, a Mixture-of-Experts (MoE)-based large language model optimized via reinforcement learning (RL) to achieve efficient and robust reasoning capabilities. Built upon the publicly available Ling-lite model, a 16.8 billion parameter model with 2.75 billion activated parameters, our approach matches the performance of state-of-the-art (SOTA) small-scale reasoning models on challenging benchmarks (e.g., AIME, LiveCodeBench, GPQA-Diamond) while activating only one-third of the parameters required by comparable models. To accomplish this, we introduce a joint training pipeline integrating distillation with RL, revealing undocumented challenges in MoE RL training. First, we identify optimization instability during RL training, and we propose Constrained Contextual Computation Policy Optimization(C3PO), a novel approach that enhances training stability and improves computational throughput via algorithm-system co-design methodology. Second, we empirically demonstrate that selecting distillation checkpoints based on entropy loss for RL training, rather than validation metrics, yields superior performance-efficiency trade-offs in subsequent RL training. Finally, we develop a two-stage training paradigm to harmonize multi-domain data integration, addressing domain conflicts that arise in training with mixed dataset. We will release the model, dataset, and code. </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºäº†Ring-liteï¼Œè¿™æ˜¯ä¸€æ¬¾åŸºäºä¸“å®¶æ··åˆï¼ˆMoEï¼‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¿›è¡Œä¼˜åŒ–ï¼Œä»¥å®ç°é«˜æ•ˆä¸”ç¨³å¥çš„æ¨ç†èƒ½åŠ›ã€‚è¯¥æ¨¡å‹å»ºç«‹åœ¨å…¬ä¼—å¯è®¿é—®çš„Ling-liteæ¨¡å‹ä¹‹ä¸Šï¼ŒLing-liteæ¨¡å‹æ˜¯ä¸€ä¸ªæ‹¥æœ‰16.8äº¿å‚æ•°ã€2.75äº¿ä¸ªæ¿€æ´»å‚æ•°çš„æ¨¡å‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸æœ€æ–°æŠ€æœ¯ï¼ˆSOTAï¼‰çš„å°è§„æ¨¡æ¨ç†æ¨¡å‹è¿›è¡Œå…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ï¼ˆä¾‹å¦‚AIMEã€LiveCodeBenchã€GPQA-Diamondï¼‰æ—¶ï¼Œä»…æ¿€æ´»äº†ç›¸å½“äºå…¶ä»–æ¨¡å‹ä¸‰åˆ†ä¹‹ä¸€çš„å‚æ•°ï¼Œå³å¯åŒ¹é…å…¶æ€§èƒ½ã€‚ä¸ºäº†å®Œæˆè¿™ä¸€ç›®æ ‡ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªè”åˆè®­ç»ƒç®¡é“ï¼Œå°†è’¸é¦æ³•ä¸RLç›¸ç»“åˆï¼Œæ­ç¤ºäº†MoE RLè®­ç»ƒä¸­çš„æœªè®°å½•æŒ‘æˆ˜ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ç¡®å®šäº†RLè®­ç»ƒè¿‡ç¨‹ä¸­çš„ä¼˜åŒ–ä¸ç¨³å®šæ€§é—®é¢˜ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°çš„çº¦æŸä¸Šä¸‹æ–‡è®¡ç®—ç­–ç•¥ä¼˜åŒ–ï¼ˆC3POï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡ç®—æ³•ç³»ç»ŸååŒè®¾è®¡æ–¹æ³•å¢å¼ºè®­ç»ƒç¨³å®šæ€§å¹¶æ”¹å–„è®¡ç®—ååé‡ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬é€šè¿‡å®è¯è¡¨æ˜ï¼Œåœ¨é€‰æ‹©è’¸é¦æ£€æŸ¥ç‚¹æ—¶ï¼ŒåŸºäºç†µæŸå¤±è€Œä¸æ˜¯éªŒè¯æŒ‡æ ‡æ¥è¿›è¡ŒRLè®­ç»ƒï¼Œå¯ä»¥åœ¨åç»­çš„RLè®­ç»ƒä¸­äº§ç”Ÿæ›´å¥½çš„æ€§èƒ½æ•ˆç‡æƒè¡¡ã€‚æœ€åï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§ä¸¤é˜¶æ®µè®­ç»ƒèŒƒå¼ï¼Œä»¥åè°ƒå¤šåŸŸæ•°æ®é›†æˆï¼Œè§£å†³åœ¨æ··åˆæ•°æ®é›†è®­ç»ƒä¸­å‡ºç°çš„é¢†åŸŸå†²çªé—®é¢˜ã€‚æˆ‘ä»¬å°†å‘å¸ƒè¯¥æ¨¡å‹ã€æ•°æ®é›†å’Œä»£ç ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.14731v2">PDF</a> Technical Report</p>
<p><strong>Summary</strong></p>
<p>åŸºäºMoEï¼ˆMixture-of-Expertsï¼‰çš„å¤§å‹è¯­è¨€æ¨¡å‹Ring-liteé€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¿›è¡Œä¼˜åŒ–ï¼Œä»¥å®ç°é«˜æ•ˆä¸”ç¨³å¥çš„æ¨ç†èƒ½åŠ›ã€‚è¯¥æ¨¡å‹ä»¥å…¬å¼€çš„Ling-liteæ¨¡å‹ä¸ºåŸºç¡€ï¼Œé€šè¿‡ç»“åˆè’¸é¦ä¸å¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„è”åˆè®­ç»ƒç®¡é“ï¼Œä»¥è¾ƒå°çš„å‚æ•°æ¿€æ´»é‡å®ç°äº†åœ¨å¤šä¸ªæŒ‘æˆ˜åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ä¸æœ€å…ˆè¿›çš„æ¨ç†æ¨¡å‹ç›¸åŒ¹é…ã€‚æ¨¡å‹è¿˜å¼•å…¥äº†C3POè®­ç»ƒç­–ç•¥æ¥è§£å†³è®­ç»ƒä¸­çš„ä¼˜åŒ–ä¸ç¨³å®šé—®é¢˜ï¼Œå¹¶æå‡ºåŸºäºç†µæŸå¤±çš„è’¸é¦æ£€æŸ¥ç‚¹é€‰æ‹©ç­–ç•¥ï¼Œä»¥è·å¾—æ›´å¥½çš„æ€§èƒ½æ•ˆç‡æƒè¡¡ã€‚æ­¤å¤–ï¼Œæ¨¡å‹é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒèŒƒå¼è§£å†³å¤šåŸŸæ•°æ®é›†æˆä¸­çš„åŸŸå†²çªé—®é¢˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Ring-liteæ˜¯ä¸€ä¸ªåŸºäºMoEçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä½¿ç”¨å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–ï¼Œä»¥å®ç°é«˜æ•ˆç¨³å¥çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>æ¨¡å‹ä»¥Ling-liteä¸ºåŸºç¡€ï¼Œé€šè¿‡è”åˆè®­ç»ƒç®¡é“ç»“åˆè’¸é¦ä¸å¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼Œå®ç°äº†åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ä¸æœ€å…ˆè¿›çš„æ¨ç†æ¨¡å‹ç›¸åŒ¹é…ã€‚</li>
<li>å¼•å…¥C3POè®­ç»ƒç­–ç•¥è§£å†³MoE RLè®­ç»ƒä¸­çš„ä¼˜åŒ–ä¸ç¨³å®šé—®é¢˜ï¼Œæé«˜è®­ç»ƒç¨³å®šæ€§å’Œè®¡ç®—æ•ˆç‡ã€‚</li>
<li>æå‡ºåŸºäºç†µæŸå¤±çš„è’¸é¦æ£€æŸ¥ç‚¹é€‰æ‹©ç­–ç•¥ï¼Œä¼˜åŒ–åç»­å¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„æ€§èƒ½æ•ˆç‡æƒè¡¡ã€‚</li>
<li>é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒèŒƒå¼è§£å†³å¤šåŸŸæ•°æ®é›†æˆä¸­çš„åŸŸå†²çªé—®é¢˜ã€‚</li>
<li>æ¨¡å‹å°†å…¬å¼€æä¾›ï¼ŒåŒ…æ‹¬æ•°æ®é›†å’Œä»£ç ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.14731">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-79fc13c79642f90f0db3455131ce6bdf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-00d56867ecf7770bf44ffd33c8b0e7c5.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Recognition-through-Reasoning-Reinforcing-Image-Geo-localization-with-Large-Vision-Language-Models"><a href="#Recognition-through-Reasoning-Reinforcing-Image-Geo-localization-with-Large-Vision-Language-Models" class="headerlink" title="Recognition through Reasoning: Reinforcing Image Geo-localization with   Large Vision-Language Models"></a>Recognition through Reasoning: Reinforcing Image Geo-localization with   Large Vision-Language Models</h2><p><strong>Authors:Ling Li, Yao Zhou, Yuxuan Liang, Fugee Tsung, Jiaheng Wei</strong></p>
<p>Previous methods for image geo-localization have typically treated the task as either classification or retrieval, often relying on black-box decisions that lack interpretability. The rise of large vision-language models (LVLMs) has enabled a rethinking of geo-localization as a reasoning-driven task grounded in visual cues. However, two major challenges persist. On the data side, existing reasoning-focused datasets are primarily based on street-view imagery, offering limited scene diversity and constrained viewpoints. On the modeling side, current approaches predominantly rely on supervised fine-tuning, which yields only marginal improvements in reasoning capabilities. To address these challenges, we propose a novel pipeline that constructs a reasoning-oriented geo-localization dataset, MP16-Reason, using diverse social media images. We introduce GLOBE, Group-relative policy optimization for Locatability assessment and Optimized visual-clue reasoning, yielding Bi-objective geo-Enhancement for the VLM in recognition and reasoning. GLOBE incorporates task-specific rewards that jointly enhance locatability assessment, visual clue reasoning, and geolocation accuracy. Both qualitative and quantitative results demonstrate that GLOBE outperforms state-of-the-art open-source LVLMs on geo-localization tasks, particularly in diverse visual scenes, while also generating more insightful and interpretable reasoning trajectories. </p>
<blockquote>
<p>ä¹‹å‰çš„æ–¹æ³•é€šå¸¸å°†å›¾åƒåœ°ç†å®šä½ä»»åŠ¡è§†ä¸ºåˆ†ç±»æˆ–æ£€ç´¢ä»»åŠ¡ï¼Œç»å¸¸ä¾èµ–äºç¼ºä¹å¯è§£é‡Šæ€§çš„é»‘ç®±å†³ç­–ã€‚å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰çš„å…´èµ·ä½¿äººä»¬é‡æ–°æ€è€ƒå°†åœ°ç†å®šä½ä½œä¸ºåŸºäºè§†è§‰çº¿ç´¢çš„æ¨ç†é©±åŠ¨ä»»åŠ¡ã€‚ç„¶è€Œï¼Œä»ç„¶å­˜åœ¨ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ã€‚åœ¨æ•°æ®æ–¹é¢ï¼Œç°æœ‰çš„ä¾§é‡äºæ¨ç†çš„æ•°æ®é›†ä¸»è¦åŸºäºè¡—é“è§†å›¾å›¾åƒï¼Œæä¾›çš„åœºæ™¯å¤šæ ·æ€§æœ‰é™ï¼Œè§†è§’ä¹Ÿå—åˆ°é™åˆ¶ã€‚åœ¨å»ºæ¨¡æ–¹é¢ï¼Œå½“å‰çš„æ–¹æ³•ä¸»è¦ä¾èµ–äºç›‘ç£å¾®è°ƒï¼Œè¿™åªèƒ½å¸¦æ¥æ¨ç†èƒ½åŠ›ä¸Šçš„å¾®å°æ”¹è¿›ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æµç¨‹ï¼Œä½¿ç”¨å¤šæ ·åŒ–çš„ç¤¾äº¤åª’ä½“å›¾åƒæ„å»ºäº†ä¸€ä¸ªé¢å‘æ¨ç†çš„åœ°ç†å®šä½æ•°æ®é›†MP16-Reasonã€‚æˆ‘ä»¬å¼•å…¥äº†GLOBEï¼Œå³ç”¨äºå®šä½è¯„ä¼°çš„ç›¸å¯¹ç»„ç­–ç•¥ä¼˜åŒ–å’Œä¼˜åŒ–çš„è§†è§‰çº¿ç´¢æ¨ç†ï¼Œä¸ºVLMåœ¨è¯†åˆ«å’Œæ¨ç†æ–¹é¢äº§ç”ŸåŒç›®æ ‡åœ°ç†å¢å¼ºã€‚GLOBEç»“åˆäº†ç‰¹å®šä»»åŠ¡å¥–åŠ±ï¼Œå…±åŒæé«˜å®šä½è¯„ä¼°ã€è§†è§‰çº¿ç´¢æ¨ç†å’Œåœ°ç†ä½ç½®å‡†ç¡®æ€§ã€‚å®šæ€§å’Œå®šé‡ç»“æœå‡è¡¨æ˜ï¼ŒGLOBEåœ¨åœ°ç†å®šä½ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºæœ€å…ˆè¿›çš„å¼€æºLVLMsï¼Œç‰¹åˆ«æ˜¯åœ¨å¤šæ ·åŒ–çš„è§†è§‰åœºæ™¯ä¸­ï¼ŒåŒæ—¶ç”Ÿæˆäº†æ›´æ·±åˆ»ä¸”å¯è§£é‡Šçš„æ¨ç†è½¨è¿¹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.14674v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§æ–°é¢–çš„åœ°ç†å®šä½æ–¹æ³•ï¼Œé€šè¿‡æ„å»ºä»¥æ¨ç†ä¸ºå¯¼å‘çš„åœ°ç†å®šä½æ•°æ®é›†MP16-Reasonï¼Œä½¿ç”¨ç¤¾äº¤åª’ä½“å›¾åƒå¢å¼ºè§†è§‰çº¿ç´¢æ¨ç†èƒ½åŠ›ã€‚æ–°æ–¹æ³•GLOBEé‡‡ç”¨ç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ä½ç½®è¯„ä¼°ï¼Œæå‡äº†è§†è§‰çº¿ç´¢æ¨ç†å’Œåœ°ç†ä½ç½®å‡†ç¡®åº¦ã€‚æ­¤æ–¹æ³•è¶…è¶Šç°æœ‰æŠ€æœ¯ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤šæ ·è§†è§‰åœºæ™¯ä¸‹çš„åœ°ç†å®šä½ä»»åŠ¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å›¾åƒåœ°ç†å®šä½ä»»åŠ¡è¢«é‡æ–°æ€è€ƒä¸ºåŸºäºè§†è§‰çº¿ç´¢çš„æ¨ç†ä»»åŠ¡ã€‚</li>
<li>å½“å‰åœ°ç†å®šä½æ–¹æ³•é¢ä¸´æ•°æ®å¤šæ ·æ€§å’Œæ¨¡å‹æ¨ç†èƒ½åŠ›ä¸¤æ–¹é¢çš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºæ„å»ºMP16-Reasonæ•°æ®é›†ï¼Œåˆ©ç”¨ç¤¾äº¤åª’ä½“å›¾åƒä¸°å¯Œåœºæ™¯å¤šæ ·æ€§ã€‚</li>
<li>ä»‹ç»GLOBEæ–¹æ³•ï¼Œé€šè¿‡ç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ä½ç½®è¯„ä¼°ï¼Œå¢å¼ºè§†è§‰çº¿ç´¢æ¨ç†èƒ½åŠ›ã€‚</li>
<li>GLOBEç»“åˆç‰¹å®šä»»åŠ¡å¥–åŠ±ï¼Œæé«˜å®šä½è¯„ä¼°ã€è§†è§‰çº¿ç´¢æ¨ç†å’Œåœ°ç†ä½ç½®å‡†ç¡®åº¦ã€‚</li>
<li>GLOBEåœ¨åœ°ç†å®šä½ä»»åŠ¡ä¸Šè¶…è¶Šç°æœ‰æŠ€æœ¯ï¼Œå°¤å…¶åœ¨å¤šæ ·è§†è§‰åœºæ™¯ä¸­è¡¨ç°çªå‡ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.14674">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-ddc50d45e774e023c09b8160f4e7e76f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4f860333e682e6d75378dbc791a45ab0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1e860f43892c0a218441f1faeb963517.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0a03acf562b862cfb84cc02667e05448.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-10ec6d6b3818c3306b9d399b41f3b64e.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="SIRI-Bench-Challenging-VLMsâ€™-Spatial-Intelligence-through-Complex-Reasoning-Tasks"><a href="#SIRI-Bench-Challenging-VLMsâ€™-Spatial-Intelligence-through-Complex-Reasoning-Tasks" class="headerlink" title="SIRI-Bench: Challenging VLMsâ€™ Spatial Intelligence through Complex   Reasoning Tasks"></a>SIRI-Bench: Challenging VLMsâ€™ Spatial Intelligence through Complex   Reasoning Tasks</h2><p><strong>Authors:Zijian Song, Xiaoxin Lin, Qiuming Huang, Guangrun Wang, Liang Lin</strong></p>
<p>Large Language Models (LLMs) are experiencing rapid advancements in complex reasoning, exhibiting remarkable generalization in mathematics and programming. In contrast, while spatial intelligence is fundamental for Vision-Language Models (VLMs) in real-world interaction, the systematic evaluation of their complex reasoning ability within spatial contexts remains underexplored. To bridge this gap, we introduce SIRI-Bench, a benchmark designed to evaluate VLMsâ€™ spatial intelligence through video-based reasoning tasks. SIRI-Bench comprises nearly 1K video-question-answer triplets, where each problem is embedded in a realistic 3D scene and captured by video. By carefully designing questions and corresponding 3D scenes, our benchmark ensures that solving the questions requires both spatial comprehension for extracting information and high-level reasoning for deriving solutions, making it a challenging benchmark for evaluating VLMs. To facilitate large-scale data synthesis, we develop an Automatic Scene Creation Engine. This engine, leveraging multiple specialized LLM agents, can generate realistic 3D scenes from abstract math problems, ensuring faithfulness to the original descriptions. Experimental results reveal that state-of-the-art VLMs struggle significantly on SIRI-Bench, underscoring the challenge of spatial reasoning. We hope that our study will bring researchersâ€™ attention to spatially grounded reasoning and advance VLMs in visual problem-solving. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤æ‚æ¨ç†æ–¹é¢å–å¾—äº†å¿«é€Ÿå‘å±•ï¼Œåœ¨æ•°å­¦å’Œç¼–ç¨‹æ–¹é¢å±•ç°å‡ºå“è¶Šçš„æ³›åŒ–èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç©ºé—´æ™ºèƒ½å¯¹äºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åœ¨ç°å®ä¸–ç•Œçš„äº¤äº’è‡³å…³é‡è¦ï¼Œè€Œå¯¹å…¶åœ¨ç©ºé—´ä¸Šä¸‹æ–‡ä¸­çš„å¤æ‚æ¨ç†èƒ½åŠ›è¿›è¡Œç³»ç»Ÿæ€§è¯„ä»·ä»ç„¶è¢«å¿½è§†ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¼•å…¥äº†SIRI-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨é€šè¿‡è§†é¢‘æ¨ç†ä»»åŠ¡è¯„ä¼°VLMç©ºé—´æ™ºèƒ½çš„åŸºå‡†æµ‹è¯•ã€‚SIRI-BenchåŒ…å«è¿‘1000ä¸ªè§†é¢‘é—®ç­”å¯¹ï¼Œæ¯ä¸ªé—®é¢˜éƒ½åµŒå…¥ä¸€ä¸ªçœŸå®çš„3Dåœºæ™¯ä¸­å¹¶é€šè¿‡è§†é¢‘æ•æ‰ã€‚é€šè¿‡ç²¾å¿ƒè®¾è®¡é—®é¢˜å’Œç›¸åº”çš„3Dåœºæ™¯ï¼Œæˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•ç¡®ä¿è§£å†³é—®é¢˜æ—¢éœ€è¦ç©ºé—´ç†è§£æ¥æå–ä¿¡æ¯ï¼Œåˆéœ€è¦é«˜çº§æ¨ç†æ¥æ¨å¯¼è§£å†³æ–¹æ¡ˆï¼Œä½¿å…¶æˆä¸ºè¯„ä¼°VLMçš„ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ã€‚ä¸ºäº†ä¿ƒè¿›å¤§è§„æ¨¡æ•°æ®åˆæˆï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªè‡ªåŠ¨åœºæ™¯åˆ›å»ºå¼•æ“ã€‚è¯¥å¼•æ“åˆ©ç”¨å¤šä¸ªä¸“ä¸šLLMä»£ç†ï¼Œå¯ä»¥ä»æŠ½è±¡çš„æ•°å­¦é—®é¢˜ç”ŸæˆçœŸå®çš„3Dåœºæ™¯ï¼Œç¡®ä¿å¿ äºåŸå§‹æè¿°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæœ€å…ˆè¿›çš„VLMåœ¨SIRI-Benchä¸Šè¡¨ç°æ˜¾è‘—æŒ£æ‰ï¼Œçªæ˜¾äº†ç©ºé—´æ¨ç†çš„æŒ‘æˆ˜æ€§ã€‚æˆ‘ä»¬å¸Œæœ›æœ¬ç ”ç©¶èƒ½å¼•èµ·ç ”ç©¶äººå‘˜å¯¹ç©ºé—´åŸºç¡€æ¨ç†çš„å…³æ³¨ï¼Œå¹¶æ¨åŠ¨VLMåœ¨è§†è§‰é—®é¢˜è§£å†³æ–¹é¢çš„è¿›å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.14512v1">PDF</a> 16 pages, 9 figures</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤æ‚æ¨ç†æ–¹é¢è¿…é€Ÿè¿›æ­¥ï¼Œå±•ç°å‡ºä»¤äººç©ç›®çš„æ•°å­¦å’Œç¼–ç¨‹æ³›åŒ–èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå¯¹äºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åœ¨ç°å®ä¸–ç•Œäº¤äº’ä¸­çš„ç©ºé—´æ™ºèƒ½è¯„ä¼°ä»ç„¶ä¸è¶³ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºSIRI-BenchåŸºå‡†æµ‹è¯•ï¼Œé€šè¿‡è§†é¢‘æ¨ç†ä»»åŠ¡è¯„ä¼°VLMçš„ç©ºé—´æ™ºèƒ½ã€‚SIRI-BenchåŒ…å«è¿‘ä¸€åƒä¸ªè§†é¢‘é—®ç­”å¯¹ï¼Œæ¯ä¸ªé—®é¢˜éƒ½åµŒå…¥ç°å®3Dåœºæ™¯ä¸­å¹¶ä»¥è§†é¢‘å½¢å¼å‘ˆç°ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•ç²¾å¿ƒè®¾è®¡é—®é¢˜å’Œå¯¹åº”çš„3Dåœºæ™¯ï¼Œç¡®ä¿è§£å†³é—®é¢˜éœ€è¦ç©ºé—´ç†è§£å’Œé«˜çº§æ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœæ­ç¤ºï¼Œæœ€å…ˆè¿›çš„VLMåœ¨SIRI-Benchä¸Šè¡¨ç°æ˜¾è‘—æŒ£æ‰ï¼Œçªæ˜¾ç©ºé—´æ¨ç†çš„æŒ‘æˆ˜æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤æ‚æ¨ç†æ–¹é¢å–å¾—å¿«é€Ÿè¿›å±•ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°å­¦å’Œç¼–ç¨‹é¢†åŸŸã€‚</li>
<li>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åœ¨ç°å®ä¸–ç•Œäº¤äº’ä¸­çš„ç©ºé—´æ™ºèƒ½è¯„ä¼°ä»ç„¶ä¸è¶³ã€‚</li>
<li>æ¨å‡ºSIRI-BenchåŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°VLMçš„ç©ºé—´æ™ºèƒ½ï¼ŒåŒ…å«è¿‘ä¸€åƒä¸ªè§†é¢‘é—®ç­”å¯¹ã€‚</li>
<li>SIRI-Benché€šè¿‡ç°å®3Dåœºæ™¯ä¸­çš„è§†é¢‘é—®é¢˜æµ‹è¯•ç©ºé—´ç†è§£å’Œé«˜çº§æ¨ç†èƒ½åŠ›ã€‚</li>
<li>æœ€å…ˆè¿›çš„VLMåœ¨SIRI-Benchä¸Šçš„è¡¨ç°æ˜¾è‘—æŒ£æ‰ï¼Œçªæ˜¾ç©ºé—´æ¨ç†çš„é‡è¦æ€§ã€‚</li>
<li>è‡ªåŠ¨åœºæ™¯åˆ›å»ºå¼•æ“ç”¨äºå¤§è§„æ¨¡æ•°æ®åˆæˆï¼Œå¯ä»æŠ½è±¡æ•°å­¦é—®é¢˜ç”Ÿæˆç°å®3Dåœºæ™¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.14512">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4bea855b27e5457a94a6aad8eae60d79.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e659f01e60bd8f1f939b499d2b080e4f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-324baddac659b885f711e8e70bad4645.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Discrete-JEPA-Learning-Discrete-Token-Representations-without-Reconstruction"><a href="#Discrete-JEPA-Learning-Discrete-Token-Representations-without-Reconstruction" class="headerlink" title="Discrete JEPA: Learning Discrete Token Representations without   Reconstruction"></a>Discrete JEPA: Learning Discrete Token Representations without   Reconstruction</h2><p><strong>Authors:Junyeob Baek, Hosung Lee, Christopher Hoang, Mengye Ren, Sungjin Ahn</strong></p>
<p>The cornerstone of cognitive intelligence lies in extracting hidden patterns from observations and leveraging these principles to systematically predict future outcomes. However, current image tokenization methods demonstrate significant limitations in tasks requiring symbolic abstraction and logical reasoning capabilities essential for systematic inference. To address this challenge, we propose Discrete-JEPA, extending the latent predictive coding framework with semantic tokenization and novel complementary objectives to create robust tokenization for symbolic reasoning tasks. Discrete-JEPA dramatically outperforms baselines on visual symbolic prediction tasks, while striking visual evidence reveals the spontaneous emergence of deliberate systematic patterns within the learned semantic token space. Though an initial model, our approach promises a significant impact for advancing Symbolic world modeling and planning capabilities in artificial intelligence systems. </p>
<blockquote>
<p>è®¤çŸ¥æ™ºèƒ½çš„æ ¸å¿ƒåœ¨äºä»è§‚å¯Ÿä¸­æå–éšè—æ¨¡å¼ï¼Œå¹¶åˆ©ç”¨è¿™äº›åŸåˆ™ç³»ç»Ÿåœ°é¢„æµ‹æœªæ¥ç»“æœã€‚ç„¶è€Œï¼Œç°æœ‰çš„å›¾åƒä»¤ç‰ŒåŒ–æ–¹æ³•åœ¨éœ€è¦ç¬¦å·æŠ½è±¡å’Œé€»è¾‘æ¨ç†èƒ½åŠ›çš„ä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºæ˜¾è‘—å±€é™æ€§ï¼Œè¿™å¯¹äºç³»ç»Ÿæ¨ç†è‡³å…³é‡è¦ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†Discrete-JEPAï¼Œå®ƒåœ¨æ½œåœ¨é¢„æµ‹ç¼–ç æ¡†æ¶çš„åŸºç¡€ä¸Šï¼Œé€šè¿‡è¯­ä¹‰ä»¤ç‰ŒåŒ–å’Œæ–°çš„è¾…åŠ©ç›®æ ‡ï¼Œä¸ºç¬¦å·æ¨ç†ä»»åŠ¡åˆ›å»ºäº†ç¨³å¥çš„ä»¤ç‰ŒåŒ–ã€‚åœ¨è§†è§‰ç¬¦å·é¢„æµ‹ä»»åŠ¡ä¸Šï¼ŒDiscrete-JEPAæ˜¾è‘—ä¼˜äºåŸºå‡†çº¿ï¼Œè€Œå¼•äººæ³¨ç›®çš„è§†è§‰è¯æ®è¡¨æ˜ï¼Œåœ¨å­¦ä¹ çš„è¯­ä¹‰ä»¤ç‰Œç©ºé—´å†…å‡ºç°äº†è‡ªå‘ç³»ç»Ÿçš„æ¨¡å¼ã€‚å°½ç®¡è¿™æ˜¯ä¸€ä¸ªåˆæ­¥æ¨¡å‹ï¼Œä½†æˆ‘ä»¬çš„æ–¹æ³•ä¸ºæ¨è¿›äººå·¥æ™ºèƒ½ç³»ç»Ÿä¸­çš„ç¬¦å·ä¸–ç•Œå»ºæ¨¡å’Œè§„åˆ’èƒ½åŠ›å¸¦æ¥äº†é‡å¤§å½±å“çš„æ‰¿è¯ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.14373v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šè®¤çŸ¥æ™ºèƒ½çš„æ ¸å¿ƒåœ¨äºä»è§‚å¯Ÿä¸­æå–éšè—æ¨¡å¼ï¼Œå¹¶åˆ©ç”¨è¿™äº›åŸåˆ™ç³»ç»Ÿåœ°é¢„æµ‹æœªæ¥ç»“æœã€‚ç„¶è€Œï¼Œå½“å‰å›¾åƒä»¤ç‰ŒåŒ–æ–¹æ³•åœ¨éœ€è¦ç¬¦å·æŠ½è±¡å’Œé€»è¾‘æ¨ç†èƒ½åŠ›çš„ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ˜¾è‘—å±€é™æ€§ï¼Œè¿™å¯¹äºç³»ç»Ÿæ¨ç†è‡³å…³é‡è¦ã€‚ä¸ºè§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†Discrete-JEPAï¼Œå®ƒé€šè¿‡è¯­ä¹‰ä»¤ç‰ŒåŒ–å’Œæ–°çš„è¾…åŠ©ç›®æ ‡æ‰©å±•æ½œåœ¨é¢„æµ‹ç¼–ç æ¡†æ¶ï¼Œä¸ºç¬¦å·æ¨ç†ä»»åŠ¡åˆ›å»ºç¨³å¥çš„ä»¤ç‰ŒåŒ–ã€‚Discrete-JEPAåœ¨è§†è§‰ç¬¦å·é¢„æµ‹ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºåŸºçº¿ï¼Œè€Œæœ‰åŠ›çš„è§†è§‰è¯æ®è¡¨æ˜ï¼Œåœ¨æ‰€å­¦çš„è¯­ä¹‰ä»¤ç‰Œç©ºé—´å†…å‡ºç°äº†è‡ªå‘ç³»ç»Ÿçš„æ¨¡å¼ã€‚å°½ç®¡æ˜¯ä¸€ä¸ªåˆæ­¥æ¨¡å‹ï¼Œä½†æˆ‘ä»¬çš„æ–¹æ³•ä¸ºç¬¦å·ä¸–ç•Œå»ºæ¨¡å’Œäººå·¥æ™ºèƒ½ç³»ç»Ÿçš„è§„åˆ’èƒ½åŠ›çš„å‘å±•å¸¦æ¥äº†æ˜¾è‘—å½±å“ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>è®¤çŸ¥æ™ºèƒ½åŸºäºä»è§‚å¯Ÿä¸­æå–éšè—æ¨¡å¼å¹¶é¢„æµ‹æœªæ¥ç»“æœã€‚</li>
<li>å½“å‰å›¾åƒä»¤ç‰ŒåŒ–æ–¹æ³•åœ¨ç¬¦å·æŠ½è±¡å’Œé€»è¾‘æ¨ç†ä»»åŠ¡ä¸­æœ‰å±€é™æ€§ã€‚</li>
<li>Discrete-JEPAé€šè¿‡ç»“åˆè¯­ä¹‰ä»¤ç‰ŒåŒ–å’Œæ–°çš„è¾…åŠ©ç›®æ ‡æ¥è§£å†³æ­¤æŒ‘æˆ˜ã€‚</li>
<li>Discrete-JEPAåœ¨è§†è§‰ç¬¦å·é¢„æµ‹ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>æ‰€å­¦è¯­ä¹‰ä»¤ç‰Œç©ºé—´å†…å‡ºç°äº†è‡ªå‘ç³»ç»Ÿçš„æ¨¡å¼ã€‚</li>
<li>Discrete-JEPAå¯¹ç¬¦å·ä¸–ç•Œå»ºæ¨¡æœ‰é‡è¦å½±å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.14373">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dc9e00656312cc53f89e4fd7219a7604.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c1fe1f872b2bd84d3719c7c990f8358d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ae76f10bbb3b312c8866f26d6a9f2d7a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7c83a4ed70085832f20b077e52d4174f.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="ADRD-LLM-Driven-Autonomous-Driving-Based-on-Rule-based-Decision-Systems"><a href="#ADRD-LLM-Driven-Autonomous-Driving-Based-on-Rule-based-Decision-Systems" class="headerlink" title="ADRD: LLM-Driven Autonomous Driving Based on Rule-based Decision Systems"></a>ADRD: LLM-Driven Autonomous Driving Based on Rule-based Decision Systems</h2><p><strong>Authors:Fanzhi Zeng, Siqi Wang, Chuzhao Zhu, Li Li</strong></p>
<p>How to construct an interpretable autonomous driving decision-making system has become a focal point in academic research. In this study, we propose a novel approach that leverages large language models (LLMs) to generate executable, rule-based decision systems to address this challenge. Specifically, harnessing the strong reasoning and programming capabilities of LLMs, we introduce the ADRD(LLM-Driven Autonomous Driving Based on Rule-based Decision Systems) framework, which integrates three core modules: the Information Module, the Agents Module, and the Testing Module. The framework operates by first aggregating contextual driving scenario information through the Information Module, then utilizing the Agents Module to generate rule-based driving tactics. These tactics are iteratively refined through continuous interaction with the Testing Module. Extensive experimental evaluations demonstrate that ADRD exhibits superior performance in autonomous driving decision tasks. Compared to traditional reinforcement learning approaches and the most advanced LLM-based methods, ADRD shows significant advantages in terms of interpretability, response speed, and driving performance. These results highlight the frameworkâ€™s ability to achieve comprehensive and accurate understanding of complex driving scenarios, and underscore the promising future of transparent, rule-based decision systems that are easily modifiable and broadly applicable. To the best of our knowledge, this is the first work that integrates large language models with rule-based systems for autonomous driving decision-making, and our findings validate its potential for real-world deployment. </p>
<blockquote>
<p>å¦‚ä½•æ„å»ºä¸€ä¸ªå¯è§£é‡Šçš„è‡ªåŠ¨é©¾é©¶å†³ç­–ç³»ç»Ÿå·²æˆä¸ºå­¦æœ¯ç ”ç©¶çš„ç„¦ç‚¹ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥ç”ŸæˆåŸºäºè§„åˆ™çš„å¯æ‰§è¡Œå†³ç­–ç³»ç»Ÿï¼Œä»¥åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ã€‚å…·ä½“æ¥è¯´ï¼Œé€šè¿‡åˆ©ç”¨LLMçš„å¼ºå¤§æ¨ç†å’Œç¼–ç¨‹èƒ½åŠ›ï¼Œæˆ‘ä»¬å¼•å…¥äº†ADRDï¼ˆåŸºäºè§„åˆ™å†³ç­–ç³»ç»Ÿçš„LLMé©±åŠ¨è‡ªåŠ¨é©¾é©¶ï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é›†æˆäº†ä¸‰ä¸ªæ ¸å¿ƒæ¨¡å—ï¼šä¿¡æ¯æ¨¡å—ã€ä»£ç†æ¨¡å—å’Œæµ‹è¯•æ¨¡å—ã€‚è¯¥æ¡†æ¶é€šè¿‡ä¿¡æ¯æ¨¡å—é¦–å…ˆèšåˆä¸Šä¸‹æ–‡é©¾é©¶åœºæ™¯ä¿¡æ¯ï¼Œç„¶ååˆ©ç”¨ä»£ç†æ¨¡å—ç”ŸæˆåŸºäºè§„åˆ™çš„é©¾é©¶ç­–ç•¥ã€‚è¿™äº›ç­–ç•¥é€šè¿‡ä¸æµ‹è¯•æ¨¡å—çš„æŒç»­äº¤äº’è¿›è¡Œè¿­ä»£ä¼˜åŒ–ã€‚å¹¿æ³›çš„å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒADRDåœ¨è‡ªåŠ¨é©¾é©¶å†³ç­–ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚ä¸ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ä»¥åŠæœ€å…ˆè¿›çš„åŸºäºLLMçš„æ–¹æ³•ç›¸æ¯”ï¼ŒADRDåœ¨å¯è§£é‡Šæ€§ã€å“åº”é€Ÿåº¦å’Œé©¾é©¶æ€§èƒ½æ–¹é¢æ˜¾ç¤ºå‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚è¿™äº›ç»“æœçªæ˜¾äº†æ¡†æ¶å¯¹å¤æ‚é©¾é©¶åœºæ™¯çš„å…¨é¢å’Œå‡†ç¡®ç†è§£ï¼Œå¹¶å¼ºè°ƒäº†åŸºäºè§„åˆ™çš„å†³ç­–ç³»ç»Ÿåœ¨æœªæ¥å…·æœ‰é€æ˜çš„ã€æ˜“äºä¿®æ”¹å’Œå¹¿æ³›é€‚ç”¨çš„å‰æ™¯ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯é¦–æ¬¡å°†å¤§å‹è¯­è¨€æ¨¡å‹ä¸åŸºäºè§„åˆ™çš„ç³»ç»Ÿç›¸ç»“åˆï¼Œç”¨äºè‡ªåŠ¨é©¾é©¶å†³ç­–ï¼Œæˆ‘ä»¬çš„ç ”ç©¶ç»“æœéªŒè¯äº†å…¶åœ¨ç°å®ä¸–ç•Œéƒ¨ç½²çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.14299v1">PDF</a> </p>
<p><strong>Summary</strong><br>è‡ªä¸»é©¾é©¶å†³ç­–ç³»ç»Ÿçš„æ„å»ºå·²æˆä¸ºå­¦æœ¯ç ”ç©¶çš„é‡è¦è¯¾é¢˜ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ–¹æ³•ï¼Œç”Ÿæˆå¯æ‰§è¡Œã€åŸºäºè§„åˆ™çš„å†³ç­–ç³»ç»Ÿæ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶å¼•å…¥äº†ADRDï¼ˆåŸºäºè§„åˆ™å†³ç­–ç³»ç»Ÿçš„LLMé©±åŠ¨è‡ªåŠ¨é©¾é©¶ï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åŒ…æ‹¬ä¿¡æ¯æ¨¡å—ã€ä»£ç†æ¨¡å—å’Œæµ‹è¯•æ¨¡å—ä¸‰ä¸ªæ ¸å¿ƒæ¨¡å—ã€‚é€šè¿‡ä¿¡æ¯æ¨¡å—æ”¶é›†é©¾é©¶åœºæ™¯ä¿¡æ¯ï¼Œåˆ©ç”¨ä»£ç†æ¨¡å—ç”ŸæˆåŸºäºè§„åˆ™çš„é©¾é©¶ç­–ç•¥ï¼Œå¹¶é€šè¿‡æµ‹è¯•æ¨¡å—è¿›è¡Œè¿­ä»£ä¼˜åŒ–ã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒADRDåœ¨è‡ªåŠ¨é©¾é©¶å†³ç­–ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œç›¸è¾ƒäºä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•å’Œæœ€å…ˆè¿›çš„LLMæ–¹æ³•ï¼Œåœ¨è§£é‡Šæ€§ã€å“åº”é€Ÿåº¦å’Œé©¾é©¶æ€§èƒ½ä¸Šå‡æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚è¿™çªæ˜¾äº†æ¡†æ¶å¯¹å¤æ‚é©¾é©¶åœºæ™¯çš„å…¨é¢å‡†ç¡®ç†è§£ï¼Œå¹¶å¼ºè°ƒäº†åŸºäºè§„åˆ™çš„å†³ç­–ç³»ç»Ÿåœ¨æœªæ¥å…·æœ‰é€æ˜åº¦é«˜ã€å¯ä¿®æ”¹æ€§å¼ºå’Œå¹¿æ³›åº”ç”¨çš„å‰æ™¯ã€‚æœ¬ç ”ç©¶æ˜¯é¦–ä¸ªå°†å¤§è¯­è¨€æ¨¡å‹ä¸åŸºäºè§„åˆ™çš„è‡ªä¸»é©¾é©¶å†³ç­–ç³»ç»Ÿç›¸ç»“åˆçš„å·¥ä½œï¼Œå…¶å®éªŒç»“æœéªŒè¯äº†å…¶åœ¨ç°å®ä¸–ç•Œéƒ¨ç½²çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶æå‡ºäº†åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ„å»ºè‡ªä¸»é©¾é©¶å†³ç­–ç³»ç»Ÿçš„åˆ›æ–°æ–¹æ³•ã€‚</li>
<li>å¼•å…¥ADRDæ¡†æ¶ï¼Œé›†æˆäº†ä¿¡æ¯æ¨¡å—ã€ä»£ç†æ¨¡å—å’Œæµ‹è¯•æ¨¡å—ä¸‰ä¸ªæ ¸å¿ƒéƒ¨åˆ†ã€‚</li>
<li>é€šè¿‡å®éªŒè¯„ä¼°ï¼Œè¯æ˜äº†ADRDæ¡†æ¶åœ¨è‡ªåŠ¨é©¾é©¶å†³ç­–ä»»åŠ¡ä¸­çš„å“è¶Šæ€§èƒ½ã€‚</li>
<li>ADRDæ¡†æ¶åœ¨è§£é‡Šæ€§ã€å“åº”é€Ÿåº¦å’Œé©¾é©¶æ€§èƒ½ç­‰æ–¹é¢ç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•å’Œæœ€å…ˆè¿›çš„LLMæ–¹æ³•å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚</li>
<li>æ¡†æ¶èƒ½å…¨é¢å‡†ç¡®åœ°ç†è§£å¤æ‚é©¾é©¶åœºæ™¯ï¼Œæ˜¾ç¤ºå‡ºå¹¿é˜”çš„åº”ç”¨å‰æ™¯ã€‚</li>
<li>è¿™æ˜¯é¦–ä¸ªå°†å¤§è¯­è¨€æ¨¡å‹ä¸åŸºäºè§„åˆ™çš„è‡ªä¸»é©¾é©¶å†³ç­–ç³»ç»Ÿç»“åˆçš„ç ”ç©¶å·¥ä½œã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.14299">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d77bc3ed9c2ef61f8e0b27a2a6b898b5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-39c900314e840b4d20d3a9aeb967b8ae.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f487db2abe3d97189d9fc06a1e93accc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3bfd857ebec11e4be388ef874842756e.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Xolver-Multi-Agent-Reasoning-with-Holistic-Experience-Learning-Just-Like-an-Olympiad-Team"><a href="#Xolver-Multi-Agent-Reasoning-with-Holistic-Experience-Learning-Just-Like-an-Olympiad-Team" class="headerlink" title="Xolver: Multi-Agent Reasoning with Holistic Experience Learning Just   Like an Olympiad Team"></a>Xolver: Multi-Agent Reasoning with Holistic Experience Learning Just   Like an Olympiad Team</h2><p><strong>Authors:Md Tanzib Hosain, Salman Rahman, Md Kishor Morol, Md Rizwan Parvez</strong></p>
<p>Despite impressive progress on complex reasoning, current large language models (LLMs) typically operate in isolation - treating each problem as an independent attempt, without accumulating or integrating experiential knowledge. In contrast, expert problem solvers - such as Olympiad or programming contest teams - leverage a rich tapestry of experiences: absorbing mentorship from coaches, developing intuition from past problems, leveraging knowledge of tool usage and library functionality, adapting strategies based on the expertise and experiences of peers, continuously refining their reasoning through trial and error, and learning from other related problems even during competition. We introduce Xolver, a training-free multi-agent reasoning framework that equips a black-box LLM with a persistent, evolving memory of holistic experience. Xolver integrates diverse experience modalities, including external and self-retrieval, tool use, collaborative interactions, agent-driven evaluation, and iterative refinement. By learning from relevant strategies, code fragments, and abstract reasoning patterns at inference time, Xolver avoids generating solutions from scratch - marking a transition from isolated inference toward experience-aware language agents. Built on both open-weight and proprietary models, Xolver consistently outperforms specialized reasoning agents. Even with lightweight backbones (e.g., QWQ-32B), it often surpasses advanced models including Qwen3-235B, Gemini 2.5 Pro, o3, and o4-mini-high. With o3-mini-high, it achieves new best results on GSM8K (98.1%), AIMEâ€™24 (94.4%), AIMEâ€™25 (93.7%), Math-500 (99.8%), and LiveCodeBench-V5 (91.6%) - highlighting holistic experience learning as a key step toward generalist agents capable of expert-level reasoning. Code and data are available at <a target="_blank" rel="noopener" href="https://kagnlp.github.io/xolver.github.io/">https://kagnlp.github.io/xolver.github.io/</a>. </p>
<blockquote>
<p>å°½ç®¡åœ¨å¤æ‚æ¨ç†æ–¹é¢å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„è¿›å±•ï¼Œä½†å½“å‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šå¸¸å­¤ç«‹è¿è¡Œï¼Œå°†æ¯ä¸ªé—®é¢˜è§†ä¸ºç‹¬ç«‹çš„å°è¯•ï¼Œæ²¡æœ‰ç§¯ç´¯æˆ–æ•´åˆç»éªŒçŸ¥è¯†ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œä¸“å®¶é—®é¢˜è§£å†³è€…ï¼ˆå¦‚å¥¥æ—åŒ¹å…‹ç«èµ›æˆ–ç¼–ç¨‹ç«èµ›å›¢é˜Ÿï¼‰åˆ™åˆ©ç”¨ä¸°å¯Œçš„ç»éªŒï¼šä»æ•™ç»ƒé‚£é‡Œå¸æ”¶æŒ‡å¯¼ï¼Œä»è¿‡å»çš„é—®é¢˜ä¸­åŸ¹å…»ç›´è§‰ï¼Œåˆ©ç”¨å·¥å…·å’Œåº“çš„åŠŸèƒ½çŸ¥è¯†ï¼Œæ ¹æ®åŒä¼´çš„ä¸“ä¸šçŸ¥è¯†å’Œç»éªŒè°ƒæ•´ç­–ç•¥ï¼Œé€šè¿‡åå¤è¯•éªŒå’Œé”™è¯¯ä¸æ–­ç£¨ç»ƒä»–ä»¬çš„æ¨ç†èƒ½åŠ›ï¼Œç”šè‡³åœ¨ç«èµ›æœŸé—´å­¦ä¹ å…¶ä»–ç›¸å…³çš„é—®é¢˜ã€‚æˆ‘ä»¬ä»‹ç»äº†Xolverï¼Œè¿™æ˜¯ä¸€ä¸ªæ— éœ€è®­ç»ƒçš„å¤šæ™ºèƒ½ä½“æ¨ç†æ¡†æ¶ï¼Œå®ƒä¸ºé»‘ç›’LLMé…å¤‡äº†æŒä¹…çš„ã€ä¸æ–­å‘å±•çš„æ•´ä½“ç»éªŒè®°å¿†ã€‚Xolverèåˆäº†å¤šæ ·åŒ–çš„ç»éªŒæ¨¡å¼ï¼ŒåŒ…æ‹¬å¤–éƒ¨å’Œè‡ªæˆ‘æ£€ç´¢ã€å·¥å…·ä½¿ç”¨ã€åä½œäº¤äº’ã€æ™ºèƒ½ä½“é©±åŠ¨è¯„ä¼°ä»¥åŠè¿­ä»£ä¼˜åŒ–ã€‚é€šè¿‡åœ¨æ¨ç†æ—¶é—´å­¦ä¹ ç›¸å…³ç­–ç•¥ã€ä»£ç ç‰‡æ®µå’ŒæŠ½è±¡æ¨ç†æ¨¡å¼ï¼ŒXolveré¿å…äº†ä»é›¶å¼€å§‹ç”Ÿæˆè§£å†³æ–¹æ¡ˆâ€”â€”æ ‡å¿—ç€ä»å­¤ç«‹æ¨ç†å‘ç»éªŒæ„ŸçŸ¥è¯­è¨€æ™ºèƒ½ä½“çš„è½¬å˜ã€‚Xolverå»ºç«‹åœ¨å¼€æºå’Œä¸“æœ‰æ¨¡å‹ä¹‹ä¸Šï¼Œå§‹ç»ˆä¼˜äºä¸“ä¸šæ¨ç†æ™ºèƒ½ä½“ã€‚å³ä½¿ä½¿ç”¨è½»é‡çº§éª¨å¹²ï¼ˆä¾‹å¦‚QWQ-32Bï¼‰ï¼Œå®ƒä¹Ÿç»å¸¸è¶…è¶Šé«˜çº§æ¨¡å‹ï¼ŒåŒ…æ‹¬Qwen3-235Bã€Gemini 2.5 Proã€o3å’Œo4-mini-highç­‰ã€‚å‡­å€Ÿo3-mini-highï¼Œå®ƒåœ¨GSM8Kï¼ˆ98.1%ï¼‰ã€AIMEâ€™24ï¼ˆ94.4%ï¼‰ã€AIMEâ€™25ï¼ˆ93.7%ï¼‰ã€Math-500ï¼ˆ99.8%ï¼‰å’ŒLiveCodeBench-V5ï¼ˆ91.6%ï¼‰ä¸Šå–å¾—äº†æœ€ä½³æ–°æˆç»©â€”â€”è¿™å‡¸æ˜¾å‡ºæ•´ä½“ç»éªŒå­¦ä¹ æ˜¯æœç€èƒ½å¤Ÿæ‰§è¡Œä¸“å®¶çº§æ¨ç†çš„é€šç”¨æ™ºèƒ½ä½“è¿ˆå‡ºçš„å…³é”®ä¸€æ­¥ã€‚ä»£ç å’Œæ•°æ®å¯åœ¨<a target="_blank" rel="noopener" href="https://kagnlp.github.io/xolver.github.io/%E6%89%BE%E5%88%B0%E3%80%82">https://kagnlp.github.io/xolver.github.io/æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.14234v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å½“å‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤„ç†å¤æ‚æ¨ç†æ—¶è™½ç„¶å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å®ƒä»¬é€šå¸¸å­¤ç«‹åœ°æ“ä½œï¼Œæ²¡æœ‰ç§¯ç´¯æˆ–æ•´åˆç»éªŒçŸ¥è¯†ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œä¸“å®¶é—®é¢˜è§£ç®—å™¨ï¼ˆå¦‚å¥¥æ—åŒ¹å…‹ç«èµ›æˆ–ç¼–ç¨‹ç«èµ›å›¢é˜Ÿï¼‰ä¼šåˆ©ç”¨ä¸°å¯Œçš„ç»éªŒï¼Œå¦‚ä»æ•™ç»ƒçš„æ•™å¯¼ä¸­å­¦ä¹ ã€ä»è¿‡å»çš„é—®é¢˜ä¸­åŸ¹å…»ç›´è§‰ç­‰ã€‚æˆ‘ä»¬å¼•å…¥äº†Xolverï¼Œè¿™æ˜¯ä¸€ä¸ªæ— éœ€è®­ç»ƒçš„å¤šæ™ºèƒ½ä½“æ¨ç†æ¡†æ¶ï¼Œå®ƒä¸ºé»‘ç®±LLMé…å¤‡äº†æŒä¹…çš„ã€ä¸æ–­å‘å±•çš„æ•´ä½“ç»éªŒè®°å¿†ã€‚Xolveræ•´åˆäº†å¤šç§ç»éªŒæ¨¡å¼ï¼ŒåŒ…æ‹¬å¤–éƒ¨å’Œè‡ªæˆ‘æ£€ç´¢ã€å·¥å…·ä½¿ç”¨ã€åä½œäº¤äº’ç­‰ã€‚é€šè¿‡ä»ç›¸å…³ç­–ç•¥ã€ä»£ç ç‰‡æ®µå’ŒæŠ½è±¡æ¨ç†æ¨¡å¼ä¸­å­¦ä¹ ï¼ŒXolveré¿å…äº†ä»é›¶å¼€å§‹ç”Ÿæˆè§£å†³æ–¹æ¡ˆï¼Œæ ‡å¿—ç€ä»å­¤ç«‹æ¨ç†å‘ç»éªŒæ„ŸçŸ¥è¯­è¨€ä»£ç†çš„è½¬å˜ã€‚Xolveråœ¨å¤šç§æ¨¡å‹ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå³ä½¿æ˜¯è½»é‡çº§æ¨¡å‹ä¹Ÿèƒ½è¶…è¶Šå…ˆè¿›çš„å¤§å‹æ¨¡å‹ã€‚å…¶ä¸»è¦æˆæœåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æ–°çš„æœ€ä½³æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤æ‚æ¨ç†æ–¹é¢è™½ç„¶æœ‰æ‰€è¿›å±•ï¼Œä½†é€šå¸¸ç¼ºä¹ç»éªŒçŸ¥è¯†çš„ç§¯ç´¯ä¸æ•´åˆã€‚</li>
<li>ä¸“å®¶é—®é¢˜è§£ç®—å™¨åˆ©ç”¨ä¸°å¯Œçš„ç»éªŒï¼ŒåŒ…æ‹¬æ•™ç»ƒæ•™å¯¼ã€è¿‡å»é—®é¢˜ç›´è§‰ç­‰ã€‚</li>
<li>Xolveræ˜¯ä¸€ä¸ªå¤šæ™ºèƒ½ä½“æ¨ç†æ¡†æ¶ï¼Œä¸ºLLMé…å¤‡äº†æ•´ä½“ç»éªŒè®°å¿†ã€‚</li>
<li>Xolveræ•´åˆäº†å¤šç§ç»éªŒæ¨¡å¼ï¼ŒåŒ…æ‹¬å¤–éƒ¨å’Œè‡ªæˆ‘æ£€ç´¢ã€å·¥å…·ä½¿ç”¨ç­‰ã€‚</li>
<li>Xolveré€šè¿‡ä»ç›¸å…³ç­–ç•¥ã€ä»£ç ç‰‡æ®µå’ŒæŠ½è±¡æ¨ç†æ¨¡å¼å­¦ä¹ ï¼Œé¿å…ä»é›¶å¼€å§‹ç”Ÿæˆè§£å†³æ–¹æ¡ˆã€‚</li>
<li>Xolveråœ¨å¤šç§æ¨¡å‹ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå³ä½¿æ˜¯è½»é‡çº§æ¨¡å‹ä¹Ÿèƒ½è¶…è¶Šå¤§å‹æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.14234">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9aeb572ad215ea025e5ffdc4a30751d3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1418ebf463650bd6433e874fea8e5b3b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ab88991496c1cec1891a26c59d790480.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="GRAM-A-Generative-Foundation-Reward-Model-for-Reward-Generalization"><a href="#GRAM-A-Generative-Foundation-Reward-Model-for-Reward-Generalization" class="headerlink" title="GRAM: A Generative Foundation Reward Model for Reward Generalization"></a>GRAM: A Generative Foundation Reward Model for Reward Generalization</h2><p><strong>Authors:Chenglong Wang, Yang Gan, Yifu Huo, Yongyu Mu, Qiaozhi He, Murun Yang, Bei Li, Tong Xiao, Chunliang Zhang, Tongran Liu, Jingbo Zhu</strong></p>
<p>In aligning large language models (LLMs), reward models have played an important role, but are standardly trained as discriminative models and rely only on labeled human preference data. In this paper, we explore methods that train reward models using both unlabeled and labeled data. Building on the generative models in LLMs, we develop a generative reward model that is first trained via large-scale unsupervised learning and then fine-tuned via supervised learning. We also show that by using label smoothing, we are in fact optimizing a regularized pairwise ranking loss. This result, in turn, provides a new view of training reward models, which links generative models and discriminative models under the same class of training objectives. The outcome of these techniques is a foundation reward model, which can be applied to a wide range of tasks with little or no further fine-tuning effort. Extensive experiments show that this model generalizes well across several tasks, including response ranking, reinforcement learning from human feedback, and task adaptation with fine-tuning, achieving significant performance improvements over several strong baseline models. </p>
<blockquote>
<p>åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¯¹é½è¿‡ç¨‹ä¸­ï¼Œå¥–åŠ±æ¨¡å‹å‘æŒ¥äº†é‡è¦ä½œç”¨ï¼Œä½†é€šå¸¸ä½œä¸ºåˆ¤åˆ«æ¨¡å‹è¿›è¡Œè®­ç»ƒï¼Œä»…ä¾èµ–äºæ ‡è®°çš„äººç±»åå¥½æ•°æ®ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ¢ç´¢äº†ä½¿ç”¨æ— æ ‡ç­¾å’Œæ ‡è®°æ•°æ®è®­ç»ƒå¥–åŠ±æ¨¡å‹çš„æ–¹æ³•ã€‚æˆ‘ä»¬åŸºäºLLMä¸­çš„ç”Ÿæˆæ¨¡å‹ï¼Œå¼€å‘äº†ä¸€ç§é¦–å…ˆé€šè¿‡å¤§è§„æ¨¡æ— ç›‘ç£å­¦ä¹ è¿›è¡Œè®­ç»ƒï¼Œç„¶åé€šè¿‡ç›‘ç£å­¦ä¹ è¿›è¡Œå¾®è°ƒç”Ÿæˆå¥–åŠ±æ¨¡å‹ã€‚æˆ‘ä»¬è¿˜è¡¨æ˜ï¼Œé€šè¿‡ä½¿ç”¨æ ‡ç­¾å¹³æ»‘ï¼Œæˆ‘ä»¬å®é™…ä¸Šæ˜¯åœ¨ä¼˜åŒ–æ­£åˆ™åŒ–é…å¯¹æ’åæŸå¤±ã€‚è¿™ä¸€ç»“æœåè¿‡æ¥ä¸ºè®­ç»ƒå¥–åŠ±æ¨¡å‹æä¾›äº†æ–°çš„è§†è§’ï¼Œå°†ç”Ÿæˆæ¨¡å‹å’Œåˆ¤åˆ«æ¨¡å‹è”ç³»åœ¨åŒä¸€ç±»è®­ç»ƒç›®æ ‡ä¹‹ä¸‹ã€‚è¿™äº›æŠ€æœ¯çš„ç»“æœæ˜¯åŸºç¡€å¥–åŠ±æ¨¡å‹ï¼Œå¯ä»¥å¹¿æ³›åº”ç”¨äºå„ç§ä»»åŠ¡ï¼Œå‡ ä¹ä¸éœ€è¦æˆ–æ ¹æœ¬ä¸éœ€è¦è¿›ä¸€æ­¥çš„å¾®è°ƒã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å¤šä¸ªä»»åŠ¡ä¸Šçš„è¡¨ç°è‰¯å¥½ï¼ŒåŒ…æ‹¬å“åº”æ’åã€äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ å’Œä»»åŠ¡å¾®è°ƒç­‰ï¼Œç›¸è¾ƒäºå¤šä¸ªå¼ºå¤§çš„åŸºçº¿æ¨¡å‹å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.14175v2">PDF</a> Accepted by ICML 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„å¥–åŠ±æ¨¡å‹é€šå¸¸åªä¾èµ–æ ‡è®°çš„äººç±»åå¥½æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œæœ¬æ–‡æ¢ç´¢äº†åŒæ—¶ä½¿ç”¨æœªæ ‡è®°å’Œå·²æ ‡è®°æ•°æ®è®­ç»ƒå¥–åŠ±æ¨¡å‹çš„æ–¹æ³•ã€‚é€šè¿‡åœ¨å¤§è§„æ¨¡æ— ç›‘ç£å­¦ä¹ åŸºç¡€ä¸Šæ„å»ºç”Ÿæˆå¥–åŠ±æ¨¡å‹ï¼Œå¹¶å¼•å…¥æ ‡ç­¾å¹³æ»‘æŠ€æœ¯è¿›è¡Œä¼˜åŒ–ï¼Œå®ç°äº†ç”Ÿæˆæ¨¡å‹å’Œåˆ¤åˆ«æ¨¡å‹çš„ç»Ÿä¸€è®­ç»ƒç›®æ ‡ã€‚è¯¥æ¨¡å‹å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œé€‚ç”¨äºå¤šç§ä»»åŠ¡ï¼Œæ— éœ€è¿›ä¸€æ­¥å¾®è°ƒå³å¯åº”ç”¨ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å¤šä¸ªä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¥–åŠ±æ¨¡å‹åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­æ‰®æ¼”é‡è¦è§’è‰²ï¼Œé€šå¸¸ä½¿ç”¨åˆ¤åˆ«å¼æ¨¡å‹è¿›è¡Œè®­ç»ƒã€‚</li>
<li>æœ¬æ–‡æ¢ç´¢äº†ç»“åˆæœªæ ‡è®°å’Œå·²æ ‡è®°æ•°æ®è®­ç»ƒå¥–åŠ±æ¨¡å‹çš„æ–¹æ³•ã€‚</li>
<li>ç”Ÿæˆå¥–åŠ±æ¨¡å‹é¦–å…ˆé€šè¿‡å¤§è§„æ¨¡æ— ç›‘ç£å­¦ä¹ è¿›è¡Œè®­ç»ƒï¼Œç„¶åé€šè¿‡ç›‘ç£å­¦ä¹ è¿›è¡Œå¾®è°ƒã€‚</li>
<li>æ ‡ç­¾å¹³æ»‘æŠ€æœ¯ç”¨äºä¼˜åŒ–å¥–åŠ±æ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹ï¼Œå®ç°äº†ç”Ÿæˆæ¨¡å‹å’Œåˆ¤åˆ«æ¨¡å‹çš„ç»Ÿä¸€è®­ç»ƒç›®æ ‡ã€‚</li>
<li>è¯¥å¥–åŠ±æ¨¡å‹å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œé€‚ç”¨äºå¤šç§ä»»åŠ¡ï¼Œæ— éœ€è¿›ä¸€æ­¥å¾®è°ƒå³å¯åº”ç”¨ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å¤šä¸ªä»»åŠ¡ä¸Šå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.14175">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c83157fccf0692bbcfdca2962e0fb5d7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-78fd587ee85215df6e0f041ec60929b9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-32b21a15db37e8d183fabbc40b9d4493.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-95f948d0b502c6a0324aeb97bc91184f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e4074e7ad6f27c8bc312b5642b16b882.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="MIST-Towards-Multi-dimensional-Implicit-Bias-and-Stereotype-Evaluation-of-LLMs-via-Theory-of-Mind"><a href="#MIST-Towards-Multi-dimensional-Implicit-Bias-and-Stereotype-Evaluation-of-LLMs-via-Theory-of-Mind" class="headerlink" title="MIST: Towards Multi-dimensional Implicit Bias and Stereotype Evaluation   of LLMs via Theory of Mind"></a>MIST: Towards Multi-dimensional Implicit Bias and Stereotype Evaluation   of LLMs via Theory of Mind</h2><p><strong>Authors:Yanlin Li, Hao Liu, Huimin Liu, Yinwei Wei, Yupeng Hu</strong></p>
<p>Theory of Mind (ToM) in Large Language Models (LLMs) refers to their capacity for reasoning about mental states, yet failures in this capacity often manifest as systematic implicit bias. Evaluating this bias is challenging, as conventional direct-query methods are susceptible to social desirability effects and fail to capture its subtle, multi-dimensional nature. To this end, we propose an evaluation framework that leverages the Stereotype Content Model (SCM) to reconceptualize bias as a multi-dimensional failure in ToM across Competence, Sociability, and Morality. The framework introduces two indirect tasks: the Word Association Bias Test (WABT) to assess implicit lexical associations and the Affective Attribution Test (AAT) to measure covert affective leanings, both designed to probe latent stereotypes without triggering model avoidance. Extensive experiments on 8 State-of-the-Art LLMs demonstrate our frameworkâ€™s capacity to reveal complex bias structures, including pervasive sociability bias, multi-dimensional divergence, and asymmetric stereotype amplification, thereby providing a more robust methodology for identifying the structural nature of implicit bias. </p>
<blockquote>
<p>å¿ƒæ™ºç†è®ºï¼ˆToMï¼‰åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­æŒ‡çš„æ˜¯å®ƒä»¬å¯¹å¿ƒç†çŠ¶æ€è¿›è¡Œæ¨ç†çš„èƒ½åŠ›ï¼Œç„¶è€Œè¿™ç§èƒ½åŠ›çš„ç¼ºå¤±é€šå¸¸è¡¨ç°ä¸ºç³»ç»Ÿæ€§çš„éšå«åè§ã€‚è¯„ä¼°è¿™ç§åè§å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºä¼ ç»Ÿçš„ç›´æ¥æŸ¥è¯¢æ–¹æ³•å®¹æ˜“å—åˆ°ç¤¾ä¼šæœŸæœ›æ•ˆåº”çš„å½±å“ï¼Œå¹¶ä¸”æ— æ³•æ•æ‰åˆ°å…¶å¾®å¦™çš„å¤šç»´æ€§è´¨ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªè¯„ä¼°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨åˆ»æ¿å°è±¡å†…å®¹æ¨¡å‹ï¼ˆSCMï¼‰æ¥é‡æ–°æ¦‚å¿µåŒ–åè§ï¼Œå°†å…¶è§†ä¸ºåœ¨èƒ½åŠ›ã€ç¤¾äº¤å’Œé“å¾·æ–¹é¢å¿ƒæ™ºç†è®ºçš„å¤šç»´å¤±è´¥ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†ä¸¤ä¸ªé—´æ¥ä»»åŠ¡ï¼šè¯æ±‡å…³è”åå·®æµ‹è¯•ï¼ˆWABTï¼‰ï¼Œç”¨äºè¯„ä¼°éšå«çš„è¯æ±‡å…³è”ï¼›æƒ…æ„Ÿå½’å±æµ‹è¯•ï¼ˆAATï¼‰ï¼Œç”¨äºæµ‹é‡éšè”½çš„æƒ…æ„Ÿå€¾å‘ï¼Œè¿™ä¸¤ä¸ªæµ‹è¯•å‡è®¾è®¡ç”¨äºæ¢æµ‹æ½œåœ¨åˆ»æ¿å°è±¡ï¼Œè€Œä¸ä¼šå¼•å‘æ¨¡å‹çš„è§„é¿è¡Œä¸ºã€‚åœ¨8ä¸ªæœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶èƒ½å¤Ÿæ­ç¤ºå¤æ‚çš„åè§ç»“æ„ï¼ŒåŒ…æ‹¬æ™®éçš„ç¤¾äº¤åè§ã€å¤šç»´å‘æ•£å’Œä¸å¯¹ç§°çš„åˆ»æ¿å°è±¡æ”¾å¤§ï¼Œä»è€Œä¸ºè¯†åˆ«éšå«åè§çš„ç»“æ„æ€§è´¨æä¾›äº†æ›´ç¨³å¥çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.14161v1">PDF</a> </p>
<p><strong>Summary</strong><br>ç†è®ºå¿ƒçµï¼ˆToMï¼‰åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„èƒ½åŠ›æŒ‡çš„æ˜¯å¯¹å¿ƒç†çŠ¶æ€è¿›è¡Œæ¨ç†çš„èƒ½åŠ›ï¼Œç„¶è€Œè¿™ç§èƒ½åŠ›çš„ç¼ºå¤±é€šå¸¸è¡¨ç°ä¸ºç³»ç»Ÿæ€§éšå«åè§ã€‚è¯„ä¼°è¿™ç§åè§å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºä¼ ç»Ÿçš„ç›´æ¥æŸ¥è¯¢æ–¹æ³•å®¹æ˜“å—åˆ°ç¤¾ä¼šæœŸæœ›æ•ˆåº”çš„å½±å“ï¼Œå¹¶ä¸”æ— æ³•æ•æ‰å…¶å¾®å¦™çš„å¤šç»´æ€§è´¨ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªè¯„ä¼°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨åˆ»æ¿å°è±¡å†…å®¹æ¨¡å‹ï¼ˆSCMï¼‰é‡æ–°æ„å»ºåè§ä½œä¸ºå¿ƒæ™ºç†è®ºè·¨èƒ½åŠ›ã€ç¤¾äº¤å’Œé“å¾·çš„å¤šç»´å¤±è´¥ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†ä¸¤ä¸ªé—´æ¥ä»»åŠ¡ï¼šå•è¯å…³è”åè§æµ‹è¯•ï¼ˆWABTï¼‰ä»¥è¯„ä¼°éšå«çš„è¯æ±‡å…³è”å’Œæƒ…æ„Ÿå½’å±æµ‹è¯•ï¼ˆAATï¼‰ä»¥è¡¡é‡éšè”½çš„æƒ…æ„Ÿå€¾å‘ï¼Œè¿™ä¸¤ä¸ªä»»åŠ¡æ—¨åœ¨æ¢æµ‹æ½œåœ¨åˆ»æ¿å°è±¡è€Œä¸ä¼šè§¦å‘æ¨¡å‹å›é¿ã€‚å¯¹å…«ç§æœ€æ–°çš„å¤§å‹è¯­è¨€æ¨¡å‹çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶èƒ½å¤Ÿæ­ç¤ºå¤æ‚çš„åè§ç»“æ„ï¼ŒåŒ…æ‹¬æ™®éçš„ç¤¾äº¤åè§ã€å¤šç»´åˆ†æ­§å’Œä¸å¯¹ç§°åˆ»æ¿å°è±¡æ”¾å¤§ï¼Œä»è€Œä¸ºè¯†åˆ«éšå«åè§çš„ç»“æ„æ€§è´¨æä¾›äº†æ›´ç¨³å¥çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ç†è®ºå¿ƒçµï¼ˆToMï¼‰åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­æŒ‡æ¨ç†å¿ƒç†çŠ¶æ€çš„å®¹é‡ï¼Œç¼ºå¤±è¡¨ç°ä¸ºç³»ç»Ÿæ€§éšå«åè§ã€‚</li>
<li>è¯„ä¼°åè§å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºä¼ ç»Ÿæ–¹æ³•æ˜“å—ç¤¾ä¼šæœŸæœ›æ•ˆåº”å½±å“ä¸”éš¾ä»¥æ•æ‰å¤šç»´æ€§è´¨ã€‚</li>
<li>æå‡ºæ–°çš„è¯„ä¼°æ¡†æ¶ï¼Œåˆ©ç”¨åˆ»æ¿å°è±¡å†…å®¹æ¨¡å‹ï¼ˆSCMï¼‰é‡æ„åè§ä¸ºè·¨èƒ½åŠ›ã€ç¤¾äº¤å’Œé“å¾·çš„å¤šç»´å¤±è´¥ã€‚</li>
<li>å¼•å…¥ä¸¤ä¸ªé—´æ¥ä»»åŠ¡ï¼šå•è¯å…³è”åè§æµ‹è¯•ï¼ˆWABTï¼‰å’Œæƒ…æ„Ÿå½’å±æµ‹è¯•ï¼ˆAATï¼‰ï¼Œç”¨äºæ¢æµ‹æ½œåœ¨åˆ»æ¿å°è±¡è€Œä¸è§¦å‘æ¨¡å‹å›é¿ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºæ–°æ¡†æ¶èƒ½æ­ç¤ºå¤æ‚åè§ç»“æ„ï¼ŒåŒ…æ‹¬ç¤¾äº¤åè§ã€å¤šç»´åˆ†æ­§å’Œä¸å¯¹ç§°åˆ»æ¿å°è±¡æ”¾å¤§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.14161">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a7d142447628d81e4fb19d2f240bc04c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f215c0acabac92846855ea1c89bcf86f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-da289b5f586c36d449b54b1c61daa8be.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ad79bd8c072d88eeeaf16be526cf433d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c474826dbd2f8971662c6523f230ce07.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9342434871577cd84420b23e197453db.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Innovating-Chinaâ€™s-Intangible-Cultural-Heritage-with-DeepSeek-MidJourney-The-Case-of-Yangliuqing-theme-Woodblock-Prints"><a href="#Innovating-Chinaâ€™s-Intangible-Cultural-Heritage-with-DeepSeek-MidJourney-The-Case-of-Yangliuqing-theme-Woodblock-Prints" class="headerlink" title="Innovating Chinaâ€™s Intangible Cultural Heritage with DeepSeek +   MidJourney: The Case of Yangliuqing theme Woodblock Prints"></a>Innovating Chinaâ€™s Intangible Cultural Heritage with DeepSeek +   MidJourney: The Case of Yangliuqing theme Woodblock Prints</h2><p><strong>Authors:RuiKun Yang, ZhongLiang Wei, Longdi Xian</strong></p>
<p>Yangliuqing woodblock prints, a cornerstone of Chinaâ€™s intangible cultural heritage, are celebrated for their intricate designs and vibrant colors. However, preserving these traditional art forms while fostering innovation presents significant challenges. This study explores the DeepSeek + MidJourney approach to generating creative, themed Yangliuqing woodblock prints focused on the fight against COVID-19 and depicting joyous winners. Using Fr&#39;echet Inception Distance (FID) scores for evaluation, the method that combined DeepSeek-generated thematic prompts, MidJourney-generated thematic images, original Yangliuqing prints, and DeepSeek-generated key prompts in MidJourney-generated outputs achieved the lowest mean FID score (150.2) with minimal variability ({\sigma} &#x3D; 4.9). Additionally, feedback from 62 participants, collected via questionnaires, confirmed that this hybrid approach produced the most representative results. Moreover, the questionnaire data revealed that participants demonstrated the highest willingness to promote traditional culture and the strongest interest in consuming the AI-generated images produced through this method. These findings underscore the effectiveness of an innovative approach that seamlessly blends traditional artistic elements with modern AI-driven creativity, ensuring both cultural preservation and contemporary relevance. </p>
<blockquote>
<p>æ¨æŸ³é’æœ¨ç‰ˆå¹´ç”»ä½œä¸ºä¸­å›½éç‰©è´¨æ–‡åŒ–é—äº§çš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼Œä»¥å…¶å¤æ‚çš„è®¾è®¡å’Œé²œè‰³çš„è‰²å½©è€Œå¹¿å—èµèª‰ã€‚ç„¶è€Œï¼Œåœ¨ä¿æŒè¿™äº›ä¼ ç»Ÿè‰ºæœ¯å½¢å¼çš„åŒæ—¶æ¿€å‘åˆ›æ–°ï¼Œé¢ä¸´ç€å·¨å¤§çš„æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†DeepSeekä¸MidJourneyç›¸ç»“åˆçš„æ–¹æ³•ï¼Œç”Ÿæˆä»¥æŠ—å‡»COVID-19ä¸ºä¸»é¢˜ã€å±•ç°æ¬¢ä¹èƒœåˆ©çš„æ¨æŸ³é’æœ¨ç‰ˆå¹´ç”»ã€‚ç ”ç©¶é‡‡ç”¨FrÃ©chet Inception Distance (FID)å¾—åˆ†è¿›è¡Œè¯„ä¼°ï¼Œå°†DeepSeekç”Ÿæˆçš„ä¸»é¢˜æç¤ºä¸MidJourneyç”Ÿæˆçš„ä¸»é¢˜å›¾åƒç›¸ç»“åˆï¼Œä»¥åŠåŸå§‹çš„æ¨æŸ³é’å¹´ç”»å’ŒDeepSeekç”Ÿæˆçš„å…³é”®æç¤ºåœ¨MidJourneyè¾“å‡ºä¸­çš„ç»„åˆï¼Œè·å¾—äº†æœ€ä½çš„å¹³å‡FIDå¾—åˆ†ï¼ˆ150.2ï¼‰ï¼Œä¸”å˜å¼‚æ€§æœ€å°ï¼ˆÏƒ&#x3D;4.9ï¼‰ã€‚æ­¤å¤–ï¼Œé€šè¿‡é—®å·è°ƒæŸ¥æ”¶é›†äº†62åå‚ä¸è€…çš„åé¦ˆï¼Œè¯å®è¿™ç§æ··åˆæ–¹æ³•äº§ç”Ÿäº†æœ€å…·ä»£è¡¨æ€§çš„ç»“æœã€‚è€Œä¸”ï¼Œé—®å·æ•°æ®è¿˜æ˜¾ç¤ºï¼Œå‚ä¸è€…å¯¹æ¨å¹¿ä¼ ç»Ÿæ–‡åŒ–çš„æ„æ„¿æœ€é«˜ï¼Œå¹¶å¯¹é€šè¿‡æ­¤æ–¹æ³•äº§ç”Ÿçš„AIç”Ÿæˆå›¾åƒæœ‰æœ€å¤§çš„å…´è¶£ã€‚è¿™äº›å‘ç°å‡¸æ˜¾äº†ä¸€ç§åˆ›æ–°æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œè¯¥æ–¹æ³•æ— ç¼èåˆäº†ä¼ ç»Ÿè‰ºæœ¯å…ƒç´ ä¸ç°ä»£AIé©±åŠ¨çš„åˆ›é€ åŠ›ï¼Œç¡®ä¿äº†æ–‡åŒ–çš„ä¼ æ‰¿å’Œå½“ä»£çš„ç›¸å…³æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.14104v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ¨æŸ³é’æœ¨ç‰ˆå¹´ç”»æ˜¯ä¸­å›½éç‰©è´¨æ–‡åŒ–é—äº§çš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼Œä»¥å…¶ç²¾ç»†çš„è®¾è®¡å’Œç”ŸåŠ¨çš„è‰²å½©è€Œè‘—ç§°ã€‚ç„¶è€Œï¼Œå¦‚ä½•åœ¨ä¿æŠ¤è¿™äº›ä¼ ç»Ÿè‰ºæœ¯å½¢å¼çš„åŒæ—¶ä¿ƒè¿›åˆ›æ–°ï¼Œæ˜¯ä¸€å¤§æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶æ¢ç´¢äº†DeepSeekå’ŒMidJourneyç›¸ç»“åˆçš„æ–¹æ³•ï¼Œç”Ÿæˆä»¥æŠ—å‡»COVID-19ä¸ºä¸»é¢˜çš„æ¨æŸ³é’æœ¨ç‰ˆå¹´ç”»ï¼Œå¹¶æç»˜äº†æ¬¢ä¹çš„èƒœæ™¯ã€‚é€šè¿‡FrÃ©chet Inception Distance (FID)åˆ†æ•°è¿›è¡Œè¯„ä¼°ï¼Œç»“åˆDeepSeekç”Ÿæˆçš„ä¸»é¢˜æç¤ºå’ŒMidJourneyç”Ÿæˆçš„å›¾åƒï¼Œä»¥åŠæ¨æŸ³é’åŸç‰ˆçš„å¹´ç”»å’ŒDeepSeekç”Ÿæˆçš„å…³é”®æç¤ºåœ¨MidJourneyè¾“å‡ºä¸­çš„æ–¹æ³•ï¼Œè·å¾—äº†æœ€ä½çš„å¹³å‡FIDåˆ†æ•°ï¼ˆ150.2ï¼‰ï¼Œä¸”å˜å¼‚æ€§æœ€å°ï¼ˆÏƒ&#x3D;4.9ï¼‰ã€‚æ­¤å¤–ï¼Œé€šè¿‡é—®å·è°ƒæŸ¥æ”¶é›†çš„62åå‚ä¸è€…çš„åé¦ˆè¯å®ï¼Œè¿™ç§æ··åˆæ–¹æ³•äº§ç”Ÿäº†æœ€å…·ä»£è¡¨æ€§çš„ç»“æœã€‚åŒæ—¶ï¼Œé—®å·æ•°æ®è¿˜æ˜¾ç¤ºï¼Œå‚ä¸è€…å¯¹ä¿ƒè¿›ä¼ ç»Ÿæ–‡åŒ–çš„æ„æ„¿æœ€é«˜ï¼Œå¹¶å¯¹é€šè¿‡æ­¤æ–¹æ³•ç”Ÿæˆçš„AIå›¾åƒæœ€æ„Ÿå…´è¶£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¨æŸ³é’æœ¨ç‰ˆå¹´ç”»æ˜¯ä¸­å›½éç‰©è´¨æ–‡åŒ–é—äº§çš„é‡è¦ä»£è¡¨ã€‚</li>
<li>ä¿æŠ¤ä¼ ç»Ÿè‰ºæœ¯å½¢å¼çš„åŒæ—¶ä¿ƒè¿›åˆ›æ–°é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>DeepSeekå’ŒMidJourneyç›¸ç»“åˆçš„æ–¹æ³•è¢«ç”¨äºç”Ÿæˆä»¥æŠ—å‡»COVID-19ä¸ºä¸»é¢˜çš„æ¨æŸ³é’æœ¨ç‰ˆå¹´ç”»ã€‚</li>
<li>ç»“åˆDeepSeekç”Ÿæˆçš„ä¸»é¢˜æç¤ºå’ŒMidJourneyç”Ÿæˆçš„å›¾åƒçš„æ–¹æ³•è·å¾—äº†æœ€ä½çš„å¹³å‡FIDåˆ†æ•°ã€‚</li>
<li>é—®å·è°ƒæŸ¥æ˜¾ç¤ºæ··åˆæ–¹æ³•äº§ç”Ÿçš„ç»“æœæœ€å…·ä»£è¡¨æ€§ã€‚</li>
<li>å‚ä¸è€…å¯¹ä¿ƒè¿›ä¼ ç»Ÿæ–‡åŒ–çš„æ„æ„¿æœ€é«˜ï¼Œå¯¹AIç”Ÿæˆçš„å›¾åƒå…´è¶£æµ“åšã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.14104">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6d398caf2f8a98ed5b6ccbed71dc3d48.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f2159a970129ea913d1b137feade46b7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7e3900c0cace0c33bb2780b4a3af2ba4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4a7ed7c86d53a352e15d6ec466eef805.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3a870e756d9761de6bff995fabbe59a7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-69e49c2200b5c726fe1ad56898915273.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Discovering-Temporal-Structure-An-Overview-of-Hierarchical-Reinforcement-Learning"><a href="#Discovering-Temporal-Structure-An-Overview-of-Hierarchical-Reinforcement-Learning" class="headerlink" title="Discovering Temporal Structure: An Overview of Hierarchical   Reinforcement Learning"></a>Discovering Temporal Structure: An Overview of Hierarchical   Reinforcement Learning</h2><p><strong>Authors:Martin Klissarov, Akhil Bagaria, Ziyan Luo, George Konidaris, Doina Precup, Marlos C. Machado</strong></p>
<p>Developing agents capable of exploring, planning and learning in complex open-ended environments is a grand challenge in artificial intelligence (AI). Hierarchical reinforcement learning (HRL) offers a promising solution to this challenge by discovering and exploiting the temporal structure within a stream of experience. The strong appeal of the HRL framework has led to a rich and diverse body of literature attempting to discover a useful structure. However, it is still not clear how one might define what constitutes good structure in the first place, or the kind of problems in which identifying it may be helpful. This work aims to identify the benefits of HRL from the perspective of the fundamental challenges in decision-making, as well as highlight its impact on the performance trade-offs of AI agents. Through these benefits, we then cover the families of methods that discover temporal structure in HRL, ranging from learning directly from online experience to offline datasets, to leveraging large language models (LLMs). Finally, we highlight the challenges of temporal structure discovery and the domains that are particularly well-suited for such endeavours. </p>
<blockquote>
<p>åœ¨å¤æ‚å¼€æ”¾ç¯å¢ƒä¸­å®ç°å…·å¤‡æ¢ç´¢ã€è§„åˆ’å’Œå­¦ä¹ èƒ½åŠ›çš„äººå·¥æ™ºèƒ½ä»£ç†æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚åˆ†å±‚å¼ºåŒ–å­¦ä¹ ï¼ˆHRLï¼‰é€šè¿‡å‘ç°å¹¶åˆ©ç”¨ç»éªŒæµä¸­çš„æ—¶é—´ç»“æ„ï¼Œä¸ºè§£å†³è¿™ä¸€æŒ‘æˆ˜æä¾›äº†æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚HRLæ¡†æ¶çš„å¼ºå¤§å¸å¼•åŠ›å‚¬ç”Ÿäº†ä¸€ç³»åˆ—ä¸°å¯Œä¸”å¤šæ ·çš„æ–‡çŒ®ï¼Œè¯•å›¾å‘ç°æœ‰ç”¨çš„ç»“æ„ã€‚ç„¶è€Œï¼Œä»ç„¶ä¸æ¸…æ¥šå¦‚ä½•å®šä¹‰ä»€ä¹ˆæ˜¯è‰¯å¥½çš„ç»“æ„ï¼Œæˆ–è€…åœ¨ç¡®å®šå“ªç§é—®é¢˜å¯¹å®ƒæœ‰æ‰€å¸®åŠ©çš„æƒ…å†µä¸‹å“ªäº›ç»“æ„æ˜¯å¥½ç”¨çš„ã€‚æœ¬ç ”ç©¶æ—¨åœ¨ä»å†³ç­–çš„åŸºæœ¬æŒ‘æˆ˜è§’åº¦ç¡®å®šHRLçš„ä¼˜åŠ¿ï¼Œå¹¶çªå‡ºå…¶å¯¹äººå·¥æ™ºèƒ½ä»£ç†æ€§èƒ½æƒè¡¡çš„å½±å“ã€‚é€šè¿‡è¿™äº›ä¼˜åŠ¿ï¼Œæˆ‘ä»¬æ¶µç›–äº†åˆ†å±‚å¼ºåŒ–å­¦ä¹ ä¸­å‘ç°æ—¶é—´ç»“æ„çš„å„ç§æ–¹æ³•ï¼ŒåŒ…æ‹¬ç›´æ¥ä»åœ¨çº¿ç»éªŒä¸­å­¦ä¹ ã€ç¦»çº¿æ•°æ®é›†ä»¥åŠåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚æœ€åï¼Œæˆ‘ä»¬å¼ºè°ƒäº†å‘ç°æ—¶é—´ç»“æ„çš„æŒ‘æˆ˜ä»¥åŠç‰¹åˆ«é€‚åˆäºæ­¤ç±»ä»»åŠ¡çš„é¢†åŸŸã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.14045v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šå±‚æ¬¡åŒ–å¼ºåŒ–å­¦ä¹ ä¸ºè§£å†³äººå·¥æ™ºèƒ½åœ¨å¤„ç†å¤æ‚å¼€æ”¾ç¯å¢ƒä¸­çš„æ¢ç´¢ã€è§„åˆ’å’Œå­¦ä¹ éš¾é¢˜æä¾›äº†æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆï¼Œå®ƒé€šè¿‡å‘ç°å¹¶åˆ©ç”¨ç»éªŒæµä¸­çš„æ—¶é—´ç»“æ„æ¥å®ç°è¿™ä¸€ç›®æ ‡ã€‚æœ¬æ–‡æ—¨åœ¨ä»å†³ç­–åˆ¶å®šçš„åŸºæœ¬æŒ‘æˆ˜è§’åº¦æ¢è®¨å±‚æ¬¡åŒ–å¼ºåŒ–å­¦ä¹ çš„ä¼˜åŠ¿ï¼Œå¹¶å¼ºè°ƒå…¶å¯¹äººå·¥æ™ºèƒ½ä»£ç†æ€§èƒ½çš„å½±å“ã€‚åŒæ—¶ä»‹ç»äº†å‘ç°å±‚æ¬¡åŒ–å¼ºåŒ–å­¦ä¹ ä¸­æ—¶é—´ç»“æ„çš„å„ç§æ–¹æ³•ï¼ŒåŒ…æ‹¬ä»åœ¨çº¿ç»éªŒä¸­å­¦ä¹ ã€ç¦»çº¿æ•°æ®é›†ä»¥åŠåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„æ–¹æ³•ã€‚æœ€åï¼Œæœ¬æ–‡æŒ‡å‡ºäº†å‘ç°æ—¶é—´ç»“æ„çš„æŒ‘æˆ˜ä»¥åŠç‰¹åˆ«é€‚åˆäºæ­¤ç±»åŠªåŠ›çš„é¢†åŸŸã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å±‚æ¬¡åŒ–å¼ºåŒ–å­¦ä¹ æ˜¯è§£å†³äººå·¥æ™ºèƒ½åœ¨å¤æ‚å¼€æ”¾ç¯å¢ƒä¸­æ¢ç´¢ã€è§„åˆ’å’Œå­¦ä¹ éš¾é¢˜çš„æœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>HRLé€šè¿‡å‘ç°å¹¶åˆ©ç”¨ç»éªŒæµä¸­çš„æ—¶é—´ç»“æ„æ¥å®ç°å…¶ç›®æ ‡ã€‚</li>
<li>å±‚æ¬¡åŒ–å¼ºåŒ–å­¦ä¹ æœ‰åŠ©äºæé«˜äººå·¥æ™ºèƒ½ä»£ç†çš„æ€§èƒ½ã€‚</li>
<li>ä»åœ¨çº¿ç»éªŒã€ç¦»çº¿æ•°æ®é›†ä»¥åŠå¤§å‹è¯­è¨€æ¨¡å‹ä¸­å‘ç°æ—¶é—´ç»“æ„çš„æ–¹æ³•æ˜¯å±‚æ¬¡åŒ–å¼ºåŒ–å­¦ä¹ çš„å…³é”®ã€‚</li>
<li>ç›®å‰å±‚æ¬¡åŒ–å¼ºåŒ–å­¦ä¹ é¢ä¸´çš„æŒ‘æˆ˜ä¹‹ä¸€æ˜¯å®šä¹‰è‰¯å¥½çš„ç»“æ„ä»¥åŠå¦‚ä½•ç¡®å®šå“ªäº›é—®é¢˜éœ€è¦åˆ©ç”¨è¿™ç§ç»“æ„ã€‚</li>
<li>åœ¨ä¸€äº›ç‰¹å®šçš„é¢†åŸŸä¸­ï¼Œå¦‚è‡ªç„¶è¯­è¨€å¤„ç†å’Œå¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œå±‚æ¬¡åŒ–å¼ºåŒ–å­¦ä¹ ç‰¹åˆ«é€‚ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.14045">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dab5064d282966610b2cee845500a27b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2a52693becba4807dd64ba6c83f9d6ed.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-029afda5cd6b8811390b4c6b550f060d.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Touch-begins-where-vision-ends-Generalizable-policies-for-contact-rich-manipulation"><a href="#Touch-begins-where-vision-ends-Generalizable-policies-for-contact-rich-manipulation" class="headerlink" title="Touch begins where vision ends: Generalizable policies for contact-rich   manipulation"></a>Touch begins where vision ends: Generalizable policies for contact-rich   manipulation</h2><p><strong>Authors:Zifan Zhao, Siddhant Haldar, Jinda Cui, Lerrel Pinto, Raunaq Bhirangi</strong></p>
<p>Data-driven approaches struggle with precise manipulation; imitation learning requires many hard-to-obtain demonstrations, while reinforcement learning yields brittle, non-generalizable policies. We introduce VisuoTactile Local (ViTaL) policy learning, a framework that solves fine-grained manipulation tasks by decomposing them into two phases: a reaching phase, where a vision-language model (VLM) enables scene-level reasoning to localize the object of interest, and a local interaction phase, where a reusable, scene-agnostic ViTaL policy performs contact-rich manipulation using egocentric vision and tactile sensing. This approach is motivated by the observation that while scene context varies, the low-level interaction remains consistent across task instances. By training local policies once in a canonical setting, they can generalize via a localize-then-execute strategy. ViTaL achieves around 90% success on contact-rich tasks in unseen environments and is robust to distractors. ViTaLâ€™s effectiveness stems from three key insights: (1) foundation models for segmentation enable training robust visual encoders via behavior cloning; (2) these encoders improve the generalizability of policies learned using residual RL; and (3) tactile sensing significantly boosts performance in contact-rich tasks. Ablation studies validate each of these insights, and we demonstrate that ViTaL integrates well with high-level VLMs, enabling robust, reusable low-level skills. Results and videos are available at <a target="_blank" rel="noopener" href="https://vitalprecise.github.io/">https://vitalprecise.github.io</a>. </p>
<blockquote>
<p>æ•°æ®é©±åŠ¨çš„æ–¹æ³•åœ¨ç²¾ç¡®æ“ä½œæ–¹é¢å­˜åœ¨å›°éš¾ï¼›æ¨¡ä»¿å­¦ä¹ éœ€è¦å¤§é‡çš„éš¾ä»¥è·å¾—çš„æ¼”ç¤ºï¼Œè€Œå¼ºåŒ–å­¦ä¹ äº§ç”Ÿçš„ç­–ç•¥æ˜¯è„†å¼±çš„ï¼Œæ— æ³•æ¨å¹¿ã€‚æˆ‘ä»¬å¼•å…¥äº†VisuoTactile Localï¼ˆViTaLï¼‰ç­–ç•¥å­¦ä¹ æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡ä¸¤ä¸ªé˜¶æ®µè§£å†³ç²¾ç»†æ“ä½œä»»åŠ¡ï¼šä¸€ä¸ªåˆ°è¾¾é˜¶æ®µï¼Œå…¶ä¸­è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å®ç°åœºæ™¯çº§æ¨ç†ï¼Œå®šä½æ„Ÿå…´è¶£çš„å¯¹è±¡ï¼›ä¸€ä¸ªå±€éƒ¨äº¤äº’é˜¶æ®µï¼Œå…¶ä¸­å¯é‡ç”¨ã€åœºæ™¯æ— å…³çš„ViTaLç­–ç•¥ä½¿ç”¨ç¬¬ä¸€äººç§°è§†è§‰å’Œè§¦è§‰æ„ŸçŸ¥æ‰§è¡Œä¸°å¯Œçš„æ¥è§¦æ“ä½œã€‚è¿™ä¸€æ–¹æ³•çš„çµæ„Ÿæ¥æºäºè¿™æ ·çš„è§‚å¯Ÿï¼šè™½ç„¶åœºæ™¯ä¸Šä¸‹æ–‡ä¼šå‘ç”Ÿå˜åŒ–ï¼Œä½†ä½çº§åˆ«çš„äº¤äº’åœ¨ä»»åŠ¡å®ä¾‹ä¹‹é—´æ˜¯ä¿æŒä¸€è‡´çš„ã€‚é€šè¿‡åœ¨æ ‡å‡†ç¯å¢ƒä¸­è®­ç»ƒä¸€æ¬¡å±€éƒ¨ç­–ç•¥ï¼Œå®ƒä»¬å¯ä»¥é€šè¿‡å®šä½ç„¶åæ‰§è¡Œç­–ç•¥è¿›è¡Œæ¨å¹¿ã€‚ViTaLåœ¨æœªè§è¿‡çš„ç¯å¢ƒä¸­æ¥è§¦ä¸°å¯Œä»»åŠ¡ä¸Šçš„æˆåŠŸç‡çº¦ä¸º90%ï¼Œå¹¶ä¸”å¯¹äºå¹²æ‰°å› ç´ å…·æœ‰é²æ£’æ€§ã€‚ViTaLçš„æœ‰æ•ˆæ€§æºäºä¸‰ä¸ªå…³é”®è§è§£ï¼šï¼ˆ1ï¼‰åˆ†å‰²åŸºç¡€æ¨¡å‹èƒ½å¤Ÿé€šè¿‡è¡Œä¸ºå…‹éš†è®­ç»ƒé²æ£’çš„è§†è§‰ç¼–ç å™¨ï¼›ï¼ˆ2ï¼‰è¿™äº›ç¼–ç å™¨æé«˜äº†ä½¿ç”¨æ®‹å·®å¼ºåŒ–å­¦ä¹ å­¦åˆ°çš„ç­–ç•¥çš„æ³›åŒ–èƒ½åŠ›ï¼›ï¼ˆ3ï¼‰è§¦è§‰æ„ŸçŸ¥åœ¨ä¸°å¯Œçš„æ¥è§¦ä»»åŠ¡ä¸­æ˜¾è‘—æé«˜äº†æ€§èƒ½ã€‚æ¶ˆèç ”ç©¶éªŒè¯äº†è¿™äº›è§è§£ï¼Œæˆ‘ä»¬è¯æ˜ViTaLèƒ½ä¸é«˜çº§VLMså¾ˆå¥½åœ°é›†æˆï¼Œå®ç°ç¨³å¥ã€å¯é‡å¤ä½¿ç”¨çš„ä½çº§æŠ€èƒ½ã€‚ç»“æœå’Œè§†é¢‘å¯åœ¨<a target="_blank" rel="noopener" href="https://vitalprecise.github.ioæŸ¥çœ‹./">https://vitalprecise.github.ioæŸ¥çœ‹ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.13762v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>æœ¬æ–‡ä»‹ç»äº†VisuoTactile Localï¼ˆViTaLï¼‰ç­–ç•¥å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºè§£å†³ç²¾ç»†æ“ä½œä»»åŠ¡ã€‚è¯¥æ¡†æ¶åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šå®šä½é˜¶æ®µï¼Œä½¿ç”¨è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œåœºæ™¯çº§æ¨ç†ä»¥å®šä½ç›®æ ‡ç‰©ä½“ï¼›å±€éƒ¨äº¤äº’é˜¶æ®µï¼Œä½¿ç”¨å¯é‡ç”¨ã€åœºæ™¯æ— å…³çš„ViTaLç­–ç•¥ï¼Œç»“åˆä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒï¼ˆegocentricï¼‰çš„è§†è§‰å’Œè§¦è§‰æ„ŸçŸ¥æ‰§è¡Œæ¥è§¦ä¸°å¯Œçš„æ“ä½œã€‚è¯¥æ¡†æ¶é€šè¿‡ä¸€æ¬¡åœ¨è§„èŒƒç¯å¢ƒä¸‹çš„æœ¬åœ°ç­–ç•¥è®­ç»ƒå³å¯å®ç°é€šç”¨åŒ–ç­–ç•¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒViTaLåœ¨å¤„ç†æ¥è§¦ä¸°å¯Œä»»åŠ¡æ—¶çš„æˆåŠŸç‡çº¦ä¸º90%ï¼Œèƒ½å¤Ÿåœ¨æœªè§è¿‡çš„ç¯å¢ƒä¸­è¿›è¡Œæ“ä½œå¹¶æŠµå¾¡å¹²æ‰°ç‰©çš„å½±å“ã€‚å…¶æ ¸å¿ƒä¼˜åŠ¿æºäºä¸‰ä¸ªå…³é”®è§è§£ï¼šåˆ†å‰²åŸºç¡€æ¨¡å‹å¯å®ç°é€šè¿‡è¡Œä¸ºå…‹éš†è®­ç»ƒé²æ£’è§†è§‰ç¼–ç å™¨ï¼›è¿™äº›ç¼–ç å™¨å¯æé«˜ä½¿ç”¨æ®‹å·®å¼ºåŒ–å­¦ä¹ å­¦ä¹ ç­–ç•¥çš„é€šç”¨æ€§ï¼›è§¦è§‰æ„ŸçŸ¥åœ¨æ¥è§¦ä¸°å¯Œä»»åŠ¡ä¸­å¯æ˜¾è‘—æé«˜æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>VisuoTactile Localï¼ˆViTaLï¼‰ç­–ç•¥å­¦ä¹ æ¡†æ¶ç”¨äºè§£å†³ç²¾ç»†æ“ä½œä»»åŠ¡ï¼Œåˆ†ä¸ºå®šä½é˜¶æ®µå’Œå±€éƒ¨äº¤äº’é˜¶æ®µã€‚</li>
<li>å®šä½é˜¶æ®µåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰è¿›è¡Œåœºæ™¯çº§æ¨ç†ï¼Œä»¥å®šä½ç›®æ ‡ç‰©ä½“ã€‚</li>
<li>å±€éƒ¨äº¤äº’é˜¶æ®µä½¿ç”¨å¯é‡ç”¨ã€åœºæ™¯æ— å…³çš„ViTaLç­–ç•¥ï¼Œç»“åˆä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„è§†è§‰å’Œè§¦è§‰æ„ŸçŸ¥æ‰§è¡Œæ“ä½œã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡ä¸€æ¬¡åœ¨è§„èŒƒç¯å¢ƒä¸‹çš„æœ¬åœ°ç­–ç•¥è®­ç»ƒå³å¯å®ç°é€šç”¨åŒ–ç­–ç•¥ï¼ŒæˆåŠŸç‡çº¦ä¸º90%ã€‚</li>
<li>ViTaLçš„æ ¸å¿ƒä¼˜åŠ¿åœ¨äºä¸‰ä¸ªå…³é”®è§è§£ï¼šåˆ†å‰²åŸºç¡€æ¨¡å‹ã€è¡Œä¸ºå…‹éš†è®­ç»ƒçš„è§†è§‰ç¼–ç å™¨ã€è§¦è§‰æ„ŸçŸ¥åœ¨æé«˜æ€§èƒ½ä¸­çš„ä½œç”¨ã€‚</li>
<li>å®éªŒç»“æœè¯æ˜äº†ViTaLåœ¨å¤„ç†æ¥è§¦ä¸°å¯Œä»»åŠ¡æ—¶çš„æœ‰æ•ˆæ€§ï¼Œå¹¶èƒ½å¤ŸæŠµå¾¡å¹²æ‰°ç‰©çš„å½±å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.13762">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d7b4b46f22a87fec0553a38328ce6d84.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d1541c3e79de7deb79fe3a67ac5d2982.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="TimeMaster-Training-Time-Series-Multimodal-LLMs-to-Reason-via-Reinforcement-Learning"><a href="#TimeMaster-Training-Time-Series-Multimodal-LLMs-to-Reason-via-Reinforcement-Learning" class="headerlink" title="TimeMaster: Training Time-Series Multimodal LLMs to Reason via   Reinforcement Learning"></a>TimeMaster: Training Time-Series Multimodal LLMs to Reason via   Reinforcement Learning</h2><p><strong>Authors:Junru Zhang, Lang Feng, Xu Guo, Yuhan Wu, Yabo Dong, Duanqing Xu</strong></p>
<p>Time-series reasoning remains a significant challenge in multimodal large language models (MLLMs) due to the dynamic temporal patterns, ambiguous semantics, and lack of temporal priors. In this work, we introduce TimeMaster, a reinforcement learning (RL)-based method that enables time-series MLLMs to perform structured, interpretable reasoning directly over visualized time-series inputs and task prompts. TimeMaster adopts a three-part structured output format, reasoning, classification, and domain-specific extension, and is optimized via a composite reward function that aligns format adherence, prediction accuracy, and open-ended insight quality. The model is trained using a two-stage pipeline: we first apply supervised fine-tuning (SFT) to establish a good initialization, followed by Group Relative Policy Optimization (GRPO) at the token level to enable stable and targeted reward-driven improvement in time-series reasoning. We evaluate TimeMaster on the TimerBed benchmark across six real-world classification tasks based on Qwen2.5-VL-3B-Instruct. TimeMaster achieves state-of-the-art performance, outperforming both classical time-series models and few-shot GPT-4o by over 14.6% and 7.3% performance gain, respectively. Notably, TimeMaster goes beyond time-series classification: it also exhibits expert-like reasoning behavior, generates context-aware explanations, and delivers domain-aligned insights. Our results highlight that reward-driven RL can be a scalable and promising path toward integrating temporal understanding into time-series MLLMs. </p>
<blockquote>
<p>æ—¶é—´åºåˆ—æ¨ç†åœ¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸­ä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ï¼ŒåŸå› åœ¨äºåŠ¨æ€çš„æ—¶é—´æ¨¡å¼ã€è¯­ä¹‰æ¨¡ç³Šä»¥åŠç¼ºä¹æ—¶é—´å…ˆéªŒçŸ¥è¯†ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†TimeMasterï¼Œä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ–¹æ³•ï¼Œä½¿æ—¶é—´åºåˆ—MLLMsèƒ½å¤Ÿç›´æ¥å¯¹å¯è§†åŒ–æ—¶é—´åºåˆ—è¾“å…¥å’Œä»»åŠ¡æç¤ºè¿›è¡Œç»“æ„åŒ–ã€å¯è§£é‡Šçš„æ¨ç†ã€‚TimeMasteré‡‡ç”¨ä¸‰éƒ¨åˆ†çš„ç»“æ„åŒ–è¾“å‡ºæ ¼å¼ï¼ŒåŒ…æ‹¬æ¨ç†ã€åˆ†ç±»å’Œé¢†åŸŸç‰¹å®šæ‰©å±•ï¼Œå¹¶é€šè¿‡ç»„åˆå¥–åŠ±å‡½æ•°è¿›è¡Œä¼˜åŒ–ï¼Œè¯¥å¥–åŠ±å‡½æ•°å°†æ ¼å¼éµå¾ªã€é¢„æµ‹å‡†ç¡®æ€§å’Œå¼€æ”¾å¼æ´å¯Ÿè´¨é‡å¯¹é½ã€‚è¯¥æ¨¡å‹é‡‡ç”¨ä¸¤é˜¶æ®µç®¡é“è¿›è¡Œè®­ç»ƒï¼šæˆ‘ä»¬é¦–å…ˆåº”ç”¨æœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ¥å»ºç«‹è‰¯å¥½çš„åˆå§‹åŒ–ï¼Œç„¶åé‡‡ç”¨ä»¤ç‰Œçº§åˆ«çš„ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œä»¥å®ç°æ—¶é—´åºåˆ—æ¨ç†ä¸­çš„ç¨³å®šå’Œç›®æ ‡å¥–åŠ±é©±åŠ¨æ”¹è¿›ã€‚æˆ‘ä»¬åœ¨åŸºäºQwen2.5-VL-3B-Instructçš„å…­ä¸ªç°å®ä¸–ç•Œåˆ†ç±»ä»»åŠ¡ä¸Šçš„TimerBedåŸºå‡†æµ‹è¯•äº†TimeMasterã€‚TimeMasterå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¶…è¶Šäº†ç»å…¸çš„æ—¶é—´åºåˆ—æ¨¡å‹å’Œå°‘é•œå¤´GPT-4oï¼Œæ€§èƒ½æå‡åˆ†åˆ«è¶…è¿‡14.6%å’Œ7.3%ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒTimeMasterè¶…è¶Šäº†æ—¶é—´åºåˆ—åˆ†ç±»ï¼šå®ƒè¿˜è¡¨ç°å‡ºä¸“å®¶çº§çš„æ¨ç†è¡Œä¸ºï¼Œç”Ÿæˆä¸Šä¸‹æ–‡ç›¸å…³çš„è§£é‡Šï¼Œå¹¶æä¾›ä¸é¢†åŸŸå¯¹é½çš„è§è§£ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœå¼ºè°ƒäº†å¥–åŠ±é©±åŠ¨çš„RLå¯ä»¥æˆä¸ºå°†æ—¶é—´ç†è§£æ•´åˆåˆ°æ—¶é—´åºåˆ—MLLMsä¸­çš„å¯æ‰©å±•å’Œå‰æ™¯å¹¿é˜”çš„é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.13705v1">PDF</a> Preprint</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†TimeMasterï¼Œä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ–¹æ³•ï¼Œèƒ½å¤Ÿä½¿æ—¶é—´åºåˆ—å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å¯¹å¯è§†åŒ–æ—¶é—´åºåˆ—è¾“å…¥å’Œä»»åŠ¡æç¤ºè¿›è¡Œç»“æ„åŒ–ã€å¯è§£é‡Šçš„æ¨ç†ã€‚TimeMasteré‡‡ç”¨ä¸‰éƒ¨åˆ†çš„ç»“æ„åŒ–è¾“å‡ºæ ¼å¼ï¼ŒåŒ…æ‹¬æ¨ç†ã€åˆ†ç±»å’Œé¢†åŸŸç‰¹å®šæ‰©å±•ï¼Œå¹¶é€šè¿‡ç»„åˆå¥–åŠ±å‡½æ•°è¿›è¡Œä¼˜åŒ–ï¼Œè¯¥å‡½æ•°å¯¹é½æ ¼å¼éµå®ˆã€é¢„æµ‹å‡†ç¡®æ€§å’Œå¼€æ”¾æ€§æ´å¯Ÿè´¨é‡ã€‚è¯¥æ¨¡å‹é‡‡ç”¨ä¸¤é˜¶æ®µç®¡é“è¿›è¡Œè®­ç»ƒï¼Œé¦–å…ˆé€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å»ºç«‹è‰¯å¥½çš„åˆå§‹åŒ–ï¼Œç„¶åé€šè¿‡Group Relative Policy Optimizationï¼ˆGRPOï¼‰åœ¨ä»¤ç‰Œçº§åˆ«è¿›è¡Œï¼Œä»¥å®ç°æ—¶é—´åºåˆ—æ¨ç†çš„ç¨³å®šå’Œç›®æ ‡å¥–åŠ±é©±åŠ¨æ”¹è¿›ã€‚åœ¨TimerBedåŸºå‡†æµ‹è¯•ä¸Šï¼ŒTimeMasteråœ¨å…­ä¸ªåŸºäºQwen2.5-VL-3B-Instructçš„çœŸå®ä¸–ç•Œåˆ†ç±»ä»»åŠ¡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ¯”ä¼ ç»Ÿçš„æ—¶é—´åºåˆ—æ¨¡å‹å’Œå°‘é•œå¤´GPT-4oåˆ†åˆ«é«˜å‡º14.6%å’Œ7.3%çš„æ€§èƒ½ã€‚TimeMasterä¸ä»…è¶…è¶Šæ—¶é—´åºåˆ—åˆ†ç±»ï¼Œè¿˜è¡¨ç°å‡ºä¸“å®¶çº§çš„æ¨ç†è¡Œä¸ºï¼Œç”Ÿæˆä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„è§£é‡Šï¼Œå¹¶æä¾›é¢†åŸŸå¯¹é½çš„è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TimeMasteræ˜¯ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ–¹æ³•ï¼Œç”¨äºæå‡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å¤„ç†æ—¶é—´åºåˆ—æ•°æ®çš„èƒ½åŠ›ã€‚</li>
<li>TimeMasterèƒ½å¤Ÿå¤„ç†åŠ¨æ€çš„æ—¶é—´æ¨¡å¼ã€æ¨¡ç³Šçš„è¯­ä¹‰å’Œç¼ºä¹æ—¶é—´å…ˆéªŒçŸ¥è¯†çš„é—®é¢˜ã€‚</li>
<li>TimeMasteré‡‡ç”¨ç»“æ„åŒ–è¾“å‡ºæ ¼å¼ï¼ŒåŒ…æ‹¬æ¨ç†ã€åˆ†ç±»å’Œé¢†åŸŸç‰¹å®šæ‰©å±•ã€‚</li>
<li>é€šè¿‡ç»„åˆå¥–åŠ±å‡½æ•°ä¼˜åŒ–æ¨¡å‹ï¼Œè¯¥å‡½æ•°è€ƒè™‘æ ¼å¼éµå®ˆã€é¢„æµ‹å‡†ç¡®æ€§å’Œå¼€æ”¾æ€§æ´å¯Ÿè´¨é‡ã€‚</li>
<li>æ¨¡å‹è®­ç»ƒé‡‡ç”¨ä¸¤é˜¶æ®µç®¡é“ï¼šç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’ŒGroup Relative Policy Optimizationï¼ˆGRPOï¼‰ã€‚</li>
<li>TimeMasteråœ¨TimerBedåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œè¶…è¿‡å…¶ä»–æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.13705">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d7e3a8ffa7f76ded1a1ce418de9e788e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-653f022d516ce712bb1a28c7eccc2dd3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9f608a58011de70a08635568edff806d.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="MiniMax-M1-Scaling-Test-Time-Compute-Efficiently-with-Lightning-Attention"><a href="#MiniMax-M1-Scaling-Test-Time-Compute-Efficiently-with-Lightning-Attention" class="headerlink" title="MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning   Attention"></a>MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning   Attention</h2><p><strong>Authors: MiniMax,  :, Aili Chen, Aonian Li, Bangwei Gong, Binyang Jiang, Bo Fei, Bo Yang, Boji Shan, Changqing Yu, Chao Wang, Cheng Zhu, Chengjun Xiao, Chengyu Du, Chi Zhang, Chu Qiao, Chunhao Zhang, Chunhui Du, Congchao Guo, Da Chen, Deming Ding, Dianjun Sun, Dong Li, Enwei Jiao, Haigang Zhou, Haimo Zhang, Han Ding, Haohai Sun, Haoyu Feng, Huaiguang Cai, Haichao Zhu, Jian Sun, Jiaqi Zhuang, Jiaren Cai, Jiayuan Song, Jin Zhu, Jingyang Li, Jinhao Tian, Jinli Liu, Junhao Xu, Junjie Yan, Junteng Liu, Junxian He, Kaiyi Feng, Ke Yang, Kecheng Xiao, Le Han, Leyang Wang, Lianfei Yu, Liheng Feng, Lin Li, Lin Zheng, Linge Du, Lingyu Yang, Lunbin Zeng, Minghui Yu, Mingliang Tao, Mingyuan Chi, Mozhi Zhang, Mujie Lin, Nan Hu, Nongyu Di, Peng Gao, Pengfei Li, Pengyu Zhao, Qibing Ren, Qidi Xu, Qile Li, Qin Wang, Rong Tian, Ruitao Leng, Shaoxiang Chen, Shaoyu Chen, Shengmin Shi, Shitong Weng, Shuchang Guan, Shuqi Yu, Sichen Li, Songquan Zhu, Tengfei Li, Tianchi Cai, Tianrun Liang, Weiyu Cheng, Weize Kong, Wenkai Li, Xiancai Chen, Xiangjun Song, Xiao Luo, Xiao Su, Xiaobo Li, Xiaodong Han, Xinzhu Hou, Xuan Lu, Xun Zou, Xuyang Shen, Yan Gong, Yan Ma, Yang Wang, Yiqi Shi, Yiran Zhong, Yonghong Duan, Yongxiang Fu, Yongyi Hu, Yu Gao, Yuanxiang Fan, Yufeng Yang, Yuhao Li, Yulin Hu, Yunan Huang, Yunji Li, Yunzhi Xu, Yuxin Mao, Yuxuan Shi, Yuze Wenren, Zehan Li, Zelin Li, Zhanxu Tian, Zhengmao Zhu, Zhenhua Fan, Zhenzhen Wu, Zhichao Xu, Zhihang Yu, Zhiheng Lyu, Zhuo Jiang, Zibo Gao, Zijia Wu, Zijian Song, Zijun Sun</strong></p>
<p>We introduce MiniMax-M1, the worldâ€™s first open-weight, large-scale hybrid-attention reasoning model. MiniMax-M1 is powered by a hybrid Mixture-of-Experts (MoE) architecture combined with a lightning attention mechanism. The model is developed based on our previous MiniMax-Text-01 model, which contains a total of 456 billion parameters with 45.9 billion parameters activated per token. The M1 model natively supports a context length of 1 million tokens, 8x the context size of DeepSeek R1. Furthermore, the lightning attention mechanism in MiniMax-M1 enables efficient scaling of test-time compute. These properties make M1 particularly suitable for complex tasks that require processing long inputs and thinking extensively. MiniMax-M1 is trained using large-scale reinforcement learning (RL) on diverse problems including sandbox-based, real-world software engineering environments. In addition to M1â€™s inherent efficiency advantage for RL training, we propose CISPO, a novel RL algorithm to further enhance RL efficiency. CISPO clips importance sampling weights rather than token updates, outperforming other competitive RL variants. Combining hybrid-attention and CISPO enables MiniMax-M1â€™s full RL training on 512 H800 GPUs to complete in only three weeks, with a rental cost of just $534,700. We release two versions of MiniMax-M1 models with 40K and 80K thinking budgets respectively, where the 40K model represents an intermediate phase of the 80K training. Experiments on standard benchmarks show that our models are comparable or superior to strong open-weight models such as the original DeepSeek-R1 and Qwen3-235B, with particular strengths in complex software engineering, tool utilization, and long-context tasks. We publicly release MiniMax-M1 at <a target="_blank" rel="noopener" href="https://github.com/MiniMax-AI/MiniMax-M1">https://github.com/MiniMax-AI/MiniMax-M1</a>. </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºäº†MiniMax-M1ï¼Œè¿™æ˜¯ä¸–ç•Œä¸Šé¦–ä¸ªå¼€æ”¾æƒé‡ã€å¤§è§„æ¨¡æ··åˆæ³¨æ„åŠ›æ¨ç†æ¨¡å‹ã€‚MiniMax-M1é‡‡ç”¨æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¶æ„ä¸é—ªç”µæ³¨æ„åŠ›æœºåˆ¶ç›¸ç»“åˆã€‚è¯¥æ¨¡å‹åŸºäºæˆ‘ä»¬ä¹‹å‰çš„MiniMax-Text-01æ¨¡å‹å¼€å‘ï¼ŒåŒ…å«æ€»å…±456äº¿ä¸ªå‚æ•°ï¼Œæ¯ä¸ªä»¤ç‰Œæ¿€æ´»45.9äº¿ä¸ªå‚æ•°ã€‚M1æ¨¡å‹åŸç”Ÿæ”¯æŒ1ç™¾ä¸‡ä¸ªä»¤ç‰Œçš„ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œæ˜¯DeepSeek R1çš„8å€ã€‚æ­¤å¤–ï¼ŒMiniMax-M1ä¸­çš„é—ªç”µæ³¨æ„åŠ›æœºåˆ¶ä½¿æµ‹è¯•æ—¶çš„è®¡ç®—æ•ˆç‡å¾—ä»¥æœ‰æ•ˆæé«˜ã€‚è¿™äº›ç‰¹æ€§ä½¿M1ç‰¹åˆ«é€‚ç”¨äºéœ€è¦å¤„ç†é•¿è¾“å…¥å’Œå¹¿æ³›æ€è€ƒçš„å¤æ‚ä»»åŠ¡ã€‚MiniMax-M1é€šè¿‡å¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å¤šç§é—®é¢˜ä¸Šè¿›è¡Œäº†è®­ç»ƒï¼ŒåŒ…æ‹¬åŸºäºæ²™ç›’çš„ã€ç°å®ä¸–ç•Œçš„è½¯ä»¶å·¥ç¨‹ç¯å¢ƒã€‚é™¤äº†M1åœ¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒä¸­çš„å›ºæœ‰æ•ˆç‡ä¼˜åŠ¿å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†CISPOè¿™ä¸€æ–°å‹å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œä»¥è¿›ä¸€æ­¥æé«˜å¼ºåŒ–å­¦ä¹ æ•ˆç‡ã€‚CISPOé€šè¿‡å‰ªè¾‘é‡è¦æ€§é‡‡æ ·æƒé‡è€Œä¸æ˜¯ä»¤ç‰Œæ›´æ–°ï¼Œè¶…è¶Šäº†å…¶ä»–æœ‰ç«äº‰åŠ›çš„å¼ºåŒ–å­¦ä¹ å˜ä½“ã€‚æ··åˆæ³¨æ„åŠ›å’ŒCISPOçš„ç»“åˆä½¿å¾—MiniMax-M1åœ¨512ä¸ªH800 GPUä¸Šè¿›è¡Œå®Œæ•´çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒåªéœ€ä¸‰å‘¨æ—¶é—´ï¼Œç§Ÿèµæˆæœ¬ä»…ä¸º534,700ç¾å…ƒã€‚æˆ‘ä»¬å‘å¸ƒäº†ä¸¤ä¸ªç‰ˆæœ¬çš„MiniMax-M1æ¨¡å‹ï¼Œåˆ†åˆ«å…·æœ‰4ä¸‡å’Œ8ä¸‡çš„æ€è€ƒé¢„ç®—ï¼Œå…¶ä¸­4ä¸‡æ¨¡å‹ä»£è¡¨8ä¸‡è®­ç»ƒçš„ä¸­é—´é˜¶æ®µã€‚åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹ä¸å¼ºå¤§çš„å¼€æ”¾æƒé‡æ¨¡å‹ï¼ˆå¦‚åŸå§‹çš„DeepSeek-R1å’ŒQwen3-235Bï¼‰ç›¸å½“æˆ–æ›´ä¼˜ç§€ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚çš„è½¯ä»¶å·¥ç¨‹ã€å·¥å…·åˆ©ç”¨å’Œé•¿ä¸Šä¸‹æ–‡ä»»åŠ¡æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚æˆ‘ä»¬å…¬å¼€å‘å¸ƒäº†MiniMax-M1ï¼š<a target="_blank" rel="noopener" href="https://github.com/MiniMax-AI/MiniMax-M1%E3%80%82">https://github.com/MiniMax-AI/MiniMax-M1ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.13585v1">PDF</a> A technical report from MiniMax. The authors are listed in   alphabetical order. We open-source our MiniMax-M1 at   <a target="_blank" rel="noopener" href="https://github.com/MiniMax-AI/MiniMax-M1">https://github.com/MiniMax-AI/MiniMax-M1</a></p>
<p><strong>Summary</strong></p>
<p>MiniMax-M1æ˜¯å…¨çƒé¦–æ¬¾å¼€æ”¾æƒé‡çš„å¤§å‹æ··åˆæ³¨æ„åŠ›æ¨ç†æ¨¡å‹ã€‚å®ƒé‡‡ç”¨æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¶æ„ä¸é—ªç”µæ³¨æ„åŠ›æœºåˆ¶ï¼Œæ”¯æŒé•¿è¾¾ç™¾ä¸‡ä»¤ç‰Œä¸Šä¸‹æ–‡å¤„ç†ï¼Œé€‚ç”¨äºå¤æ‚çš„é•¿è¾“å…¥å’Œå¹¿æ³›æ€è€ƒçš„ä»»åŠ¡ã€‚æ¨¡å‹é€šè¿‡å¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒï¼Œå¹¶æå‡ºCISPOç®—æ³•æå‡æ•ˆç‡ã€‚MiniMax-M1åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜ç§€ï¼Œå¹¶åœ¨è½¯ä»¶å·¥ç¨‹ã€å·¥å…·åˆ©ç”¨å’Œé•¿ä¸Šä¸‹æ–‡ä»»åŠ¡ä¸Šè¡¨ç°çªå‡ºã€‚æ¨¡å‹å·²åœ¨GitHubä¸Šå…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MiniMax-M1æ˜¯å…¨çƒé¦–ä¸ªå¼€æ”¾æƒé‡çš„å¤§å‹æ··åˆæ³¨æ„åŠ›æ¨ç†æ¨¡å‹ã€‚</li>
<li>é‡‡ç”¨æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¶æ„ä¸é—ªç”µæ³¨æ„åŠ›æœºåˆ¶ï¼Œä½¿æ¨¡å‹æ•ˆç‡æ›´é«˜ã€‚</li>
<li>æ”¯æŒé•¿è¾¾ç™¾ä¸‡ä»¤ç‰Œçš„ä¸Šä¸‹æ–‡å¤„ç†ã€‚</li>
<li>é€šè¿‡å¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒï¼Œå¹¶å¼•å…¥CISPOç®—æ³•è¿›ä¸€æ­¥æå‡æ•ˆç‡ã€‚</li>
<li>MiniMax-M1åœ¨å¤æ‚ä»»åŠ¡ï¼Œç‰¹åˆ«æ˜¯è½¯ä»¶å·¥ç¨‹å’Œé•¿ä¸Šä¸‹æ–‡ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜ç§€ã€‚</li>
<li>å…¬å¼€ä¸¤ä¸ªç‰ˆæœ¬æ¨¡å‹ï¼Œåˆ†åˆ«å…·æœ‰40Kå’Œ80Kçš„æ€è€ƒé¢„ç®—ã€‚</li>
<li>æ¨¡å‹å·²åœ¨GitHubä¸Šå…¬å¼€ï¼Œä¾¿äºå…¬ä¼—è®¿é—®å’Œä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.13585">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dc9d64737805bb73061d895457177798.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d46bef6aa221f48eb8092e41130ef0a0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6b7c43715705877efb0dd8b97d6ecb9c.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-20/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-20/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-21/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-bacceaa5b739488680cd4a31fc507c6f.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-21  Seewo's Submission to MLC-SLM Lessons learned from Speech Reasoning   Language Models
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-21
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-17/Text-to-Motion/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-0106ae2049aba135f37814fd5207b9e6.jpg" class="responsive-img" alt="Text-to-Motion">
                        
                        <span class="card-title">Text-to-Motion</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Text-to-Motion æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-17  Motion-R1 Chain-of-Thought Reasoning and Reinforcement Learning for   Human Motion Generation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-17
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Text-to-Motion/" class="post-category">
                                    Text-to-Motion
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Text-to-Motion/">
                        <span class="chip bg-color">Text-to-Motion</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">31686.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
