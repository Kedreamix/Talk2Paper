<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-11  A Sober Look at Progress in Language Model Reasoning Pitfalls and Paths   to Reproducibility">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-1177e54fc01df5256fe6e456b3b66cf9.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-11
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-21
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    81 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-04-11-æ›´æ–°"><a href="#2025-04-11-æ›´æ–°" class="headerlink" title="2025-04-11 æ›´æ–°"></a>2025-04-11 æ›´æ–°</h1><h2 id="A-Sober-Look-at-Progress-in-Language-Model-Reasoning-Pitfalls-and-Paths-to-Reproducibility"><a href="#A-Sober-Look-at-Progress-in-Language-Model-Reasoning-Pitfalls-and-Paths-to-Reproducibility" class="headerlink" title="A Sober Look at Progress in Language Model Reasoning: Pitfalls and Paths   to Reproducibility"></a>A Sober Look at Progress in Language Model Reasoning: Pitfalls and Paths   to Reproducibility</h2><p><strong>Authors:Andreas Hochlehnert, Hardik Bhatnagar, Vishaal Udandarao, Samuel Albanie, Ameya Prabhu, Matthias Bethge</strong></p>
<p>Reasoning has emerged as the next major frontier for language models (LMs), with rapid advances from both academic and industrial labs. However, this progress often outpaces methodological rigor, with many evaluations relying on benchmarking practices that lack transparency, robustness, or statistical grounding. In this work, we conduct a comprehensive empirical study and find that current mathematical reasoning benchmarks are highly sensitive to subtle implementation choices - including decoding parameters, random seeds, prompt formatting, and even hardware and software-framework configurations. Performance gains reported in recent studies frequently hinge on unclear comparisons or unreported sources of variance. To address these issues, we propose a standardized evaluation framework with clearly defined best practices and reporting standards. Using this framework, we reassess recent methods and find that reinforcement learning (RL) approaches yield only modest improvements - far below prior claims - and are prone to overfitting, especially on small-scale benchmarks like AIME24. In contrast, supervised finetuning (SFT) methods show consistently stronger generalization. To foster reproducibility, we release all code, prompts, and model outputs, for reasoning benchmarks, establishing more rigorous foundations for future work. </p>
<blockquote>
<p>æ¨ç†å·²æˆä¸ºè¯­è¨€æ¨¡å‹ï¼ˆLMsï¼‰çš„ä¸‹ä¸€ä¸ªä¸»è¦å‰æ²¿é¢†åŸŸï¼Œå­¦æœ¯å’Œå·¥ä¸šå®éªŒå®¤éƒ½å–å¾—äº†å¿«é€Ÿå‘å±•ã€‚ç„¶è€Œï¼Œè¿™ç§è¿›æ­¥å¾€å¾€è¶…å‡ºäº†æ–¹æ³•è®ºçš„ä¸¥è°¨æ€§ï¼Œè®¸å¤šè¯„ä¼°ä¾èµ–äºç¼ºä¹é€æ˜åº¦ã€ç¨³å¥æ€§æˆ–ç»Ÿè®¡åŸºç¡€çš„åŸºå‡†æµ‹è¯•å®è·µã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è¿›è¡Œäº†å…¨é¢çš„å®è¯ç ”ç©¶ï¼Œå‘ç°å½“å‰æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•å¯¹å¾®å¦™çš„å®ç°é€‰æ‹©é«˜åº¦æ•æ„Ÿï¼ŒåŒ…æ‹¬è§£ç å‚æ•°ã€éšæœºç§å­ã€æç¤ºæ ¼å¼ï¼Œç”šè‡³ç¡¬ä»¶å’Œè½¯ä»¶æ¡†æ¶é…ç½®ã€‚æœ€è¿‘ç ”ç©¶ä¸­æŠ¥å‘Šçš„æ€§èƒ½æå‡å¾€å¾€å–å†³äºä¸æ¸…æ™°çš„æ¯”è¾ƒæˆ–æœªæŠ¥å‘Šçš„å˜é‡æ¥æºã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ ‡å‡†åŒ–çš„è¯„ä¼°æ¡†æ¶ï¼Œå…¶ä¸­æ˜ç¡®äº†æœ€ä½³å®è·µå’ŒæŠ¥å‘Šæ ‡å‡†ã€‚ä½¿ç”¨è¿™ä¸ªæ¡†æ¶ï¼Œæˆ‘ä»¬é‡æ–°è¯„ä¼°äº†æœ€è¿‘çš„æ–¹æ³•ï¼Œå‘ç°å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ–¹æ³•åªäº§ç”Ÿäº†å¾®å°çš„æ”¹è¿›â€”â€”è¿œä½äºå…ˆå‰çš„è¯´æ³•ï¼Œå¹¶ä¸”å®¹æ˜“è¿‡åº¦æ‹Ÿåˆï¼Œå°¤å…¶æ˜¯åœ¨å°å‹åŸºå‡†æµ‹è¯•å¦‚AIME24ä¸Šã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ–¹æ³•è¡¨ç°å‡ºæ›´ä¸€è‡´çš„è‰¯å¥½æ³›åŒ–èƒ½åŠ›ã€‚ä¸ºäº†ä¿ƒè¿›å¯é‡å¤æ€§ï¼Œæˆ‘ä»¬å‘å¸ƒäº†æ‰€æœ‰ä»£ç ã€æç¤ºå’Œæ¨¡å‹è¾“å‡ºï¼Œä¸ºæœªæ¥çš„å·¥ä½œå»ºç«‹æ›´ä¸¥æ ¼çš„åŸºå‡†æµ‹è¯•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07086v1">PDF</a> Technical Report</p>
<p><strong>Summary</strong>ï¼š<br>éšç€è¯­è¨€æ¨¡å‹ï¼ˆLMsï¼‰åœ¨æ¨ç†é¢†åŸŸçš„å¿«é€Ÿå‘å±•ï¼Œå½“å‰å­˜åœ¨è¯¸å¤šç¼ºä¹é€æ˜åº¦ã€ç¨³å¥æ€§å’Œç»Ÿè®¡åŸºç¡€çš„è¯„ä¼°æ–¹æ³•ã€‚æœ¬ç ”ç©¶é€šè¿‡å®è¯ç ”ç©¶å‘ç°ï¼Œå½“å‰æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•å¯¹ç»†å¾®å®ç°é€‰æ‹©é«˜åº¦æ•æ„Ÿï¼ŒåŒ…æ‹¬è§£ç å‚æ•°ã€éšæœºç§å­ã€æç¤ºæ ¼å¼ç­‰ã€‚è¿‡å»ç ”ç©¶ä¸­çš„æ€§èƒ½æå‡å¸¸å¸¸å»ºç«‹åœ¨ä¸æ¸…æ™°æ¯”è¾ƒæˆ–æœªæŠ¥å‘Šçš„å˜å¼‚æºä¸Šã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬ç ”ç©¶æå‡ºæ ‡å‡†åŒ–è¯„ä¼°æ¡†æ¶å¹¶é‡æ–°è¯„ä¼°è¿‘æœŸæ–¹æ³•ï¼Œå‘ç°å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ–¹æ³•ä»…å–å¾—è½»å¾®æ”¹å–„ä¸”å®¹æ˜“è¿‡åº¦æ‹Ÿåˆï¼Œè€Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ–¹æ³•åˆ™è¡¨ç°å‡ºæ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸ºæå‡å¯é‡å¤æ€§ï¼Œæœ¬ç ”ç©¶å…¬å¼€æ‰€æœ‰ä»£ç ã€æç¤ºå’Œæ¨¡å‹è¾“å‡ºï¼Œä¸ºæœªæ¥çš„å·¥ä½œå»ºç«‹æ›´ä¸¥æ ¼çš„åŸºçŸ³ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æ¨ç†å·²æˆä¸ºè¯­è¨€æ¨¡å‹çš„ä¸‹ä¸€ä¸ªä¸»è¦å‰æ²¿é¢†åŸŸï¼Œä½†è¯„ä¼°æ–¹æ³•ç¼ºä¹é€æ˜åº¦å’Œç¨³å¥æ€§ã€‚</li>
<li>å½“å‰æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•å¯¹å®ç°ç»†èŠ‚é«˜åº¦æ•æ„Ÿï¼ŒåŒ…æ‹¬è§£ç å‚æ•°ã€éšæœºç§å­ç­‰ã€‚</li>
<li>è¿‡å»çš„ç ”ç©¶æŠ¥å‘Šä¸­çš„æ€§èƒ½æå‡å¸¸å¸¸æºäºä¸æ¸…æ™°çš„æ¯”è¾ƒæˆ–æœªæŠ¥å‘Šçš„å˜å¼‚æºã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ åœ¨æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­çš„æ”¹è¿›æœ‰é™ï¼Œä¸”å®¹æ˜“è¿‡åº¦æ‹Ÿåˆã€‚</li>
<li>ç›‘ç£å¾®è°ƒæ–¹æ³•åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†æ ‡å‡†åŒ–è¯„ä¼°æ¡†æ¶å’Œæ˜ç¡®çš„æœ€ä½³å®è·µåŠæŠ¥å‘Šæ ‡å‡†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07086">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-773aed401bf6531fc572217b1b9c8c97.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1177e54fc01df5256fe6e456b3b66cf9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-94b61f8a99519e78bdffaf06e3163e05.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c93b1ab661a074c96946930d369755bc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6e990a3fe9c7ae6f00e948087d4b041c.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="To-Backtrack-or-Not-to-Backtrack-When-Sequential-Search-Limits-Model-Reasoning"><a href="#To-Backtrack-or-Not-to-Backtrack-When-Sequential-Search-Limits-Model-Reasoning" class="headerlink" title="To Backtrack or Not to Backtrack: When Sequential Search Limits Model   Reasoning"></a>To Backtrack or Not to Backtrack: When Sequential Search Limits Model   Reasoning</h2><p><strong>Authors:Tian Qin, David Alvarez-Melis, Samy Jelassi, Eran Malach</strong></p>
<p>Recent advancements in large language models have significantly improved their reasoning abilities, particularly through techniques involving search and backtracking. Backtracking naturally scales test-time compute by enabling sequential, linearized exploration via long chain-of-thought (CoT) generation. However, this is not the only strategy for scaling test-time compute: parallel sampling with best-of-n selection provides an alternative that generates diverse solutions simultaneously. Despite the growing adoption of sequential search, its advantages over parallel samplingâ€“especially under a fixed compute budget remain poorly understood. In this paper, we systematically compare these two approaches on two challenging reasoning tasks: CountDown and Sudoku. Surprisingly, we find that sequential search underperforms parallel sampling on CountDown but outperforms it on Sudoku, suggesting that backtracking is not universally beneficial. We identify two factors that can cause backtracking to degrade performance: (1) training on fixed search traces can lock models into suboptimal strategies, and (2) explicit CoT supervision can discourage â€œimplicitâ€ (non-verbalized) reasoning. Extending our analysis to reinforcement learning (RL), we show that models with backtracking capabilities benefit significantly from RL fine-tuning, while models without backtracking see limited, mixed gains. Together, these findings challenge the assumption that backtracking universally enhances LLM reasoning, instead revealing a complex interaction between task structure, training data, model scale, and learning paradigm. </p>
<blockquote>
<p>è¿‘æœŸå¤§å‹è¯­è¨€æ¨¡å‹çš„è¿›æ­¥æ˜¾è‘—æé«˜äº†å…¶æ¨ç†èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯é€šè¿‡æ¶‰åŠæœç´¢å’Œå›æº¯çš„æŠ€æœ¯ã€‚å›æº¯é€šè¿‡å¯ç”¨é€šè¿‡é•¿é“¾æ¡æ€ç»´ï¼ˆCoTï¼‰ç”Ÿæˆçš„åºåˆ—åŒ–ã€çº¿æ€§åŒ–æ¢ç´¢ï¼Œè‡ªç„¶åœ°æ‰©å±•äº†æµ‹è¯•æ—¶é—´çš„è®¡ç®—ã€‚ç„¶è€Œï¼Œè¿™å¹¶ä¸æ˜¯æ‰©å±•æµ‹è¯•æ—¶é—´è®¡ç®—çš„å”¯ä¸€ç­–ç•¥ï¼šå¹¶è¡Œé‡‡æ ·ä¸né€‰æœ€ä½³æä¾›äº†åŒæ—¶ç”Ÿæˆå¤šç§è§£å†³æ–¹æ¡ˆçš„æ›¿ä»£æ–¹æ³•ã€‚å°½ç®¡åºè´¯æœç´¢çš„é‡‡ç”¨æ—¥ç›Šå¢å¤šï¼Œä½†å…¶ç›¸å¯¹äºå¹¶è¡Œé‡‡æ ·çš„ä¼˜åŠ¿â€”â€”ç‰¹åˆ«æ˜¯åœ¨å›ºå®šè®¡ç®—é¢„ç®—ä¸‹â€”â€”ä»é²œä¸ºäººçŸ¥ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°æ¯”è¾ƒäº†è¿™ä¸¤ç§æ–¹æ³•åœ¨ä¸¤é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„æ¨ç†ä»»åŠ¡ï¼šå€’è®¡æ—¶å’Œæ•°ç‹¬ä¸Šçš„è¡¨ç°ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç°åºè´¯æœç´¢åœ¨å€’è®¡æ—¶ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¸å¦‚å¹¶è¡Œé‡‡æ ·ï¼Œä½†åœ¨æ•°ç‹¬ä»»åŠ¡ä¸Šè¡¨ç°è¾ƒå¥½ï¼Œè¿™è¡¨æ˜å›æº¯å¹¶ä¸æ˜¯æ™®éæœ‰ç›Šçš„ã€‚æˆ‘ä»¬ç¡®å®šäº†å¯¼è‡´å›æº¯é™ä½æ€§èƒ½çš„å¦å¤–ä¸¤ä¸ªå› ç´ ï¼šï¼ˆ1ï¼‰åœ¨å›ºå®šæœç´¢è½¨è¿¹ä¸Šè¿›è¡Œè®­ç»ƒä¼šä½¿æ¨¡å‹é™·å…¥æ¬¡ä¼˜ç­–ç•¥ï¼›ï¼ˆ2ï¼‰æ˜ç¡®çš„æ€ç»´é“¾ç›‘ç£å¯èƒ½ä¼šé˜»ç¢â€œéšæ€§â€ï¼ˆæœªå£è¯­åŒ–ï¼‰æ¨ç†ã€‚å°†æˆ‘ä»¬çš„åˆ†ææ‰©å±•åˆ°å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ï¼Œæˆ‘ä»¬å‘ç°åœ¨å…·æœ‰å›æº¯èƒ½åŠ›çš„æ¨¡å‹ä¸­ï¼ŒRLå¾®è°ƒå¸¦æ¥äº†å·¨å¤§çš„å¥½å¤„ï¼Œè€Œæ²¡æœ‰å›æº¯çš„æ¨¡å‹åˆ™çœ‹åˆ°äº†æœ‰é™ä¸”å–œå¿§å‚åŠçš„æ”¶ç›Šã€‚æ€»ä¹‹ï¼Œè¿™äº›å‘ç°æŒ‘æˆ˜äº†å›æº¯æ™®éæé«˜LLMæ¨ç†èƒ½åŠ›çš„å‡è®¾ï¼Œç›¸åï¼Œæ­ç¤ºäº†ä»»åŠ¡ç»“æ„ã€è®­ç»ƒæ•°æ®ã€æ¨¡å‹è§„æ¨¡å’Œå­¦ä¹ èŒƒå¼ä¹‹é—´çš„å¤æ‚äº¤äº’ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07052v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›æœ€è¿‘æœ‰äº†æ˜¾è‘—æå‡ï¼Œä¸»è¦é€šè¿‡æœç´¢å’Œå›æº¯æŠ€æœ¯å®ç°ã€‚æœ¬æ–‡ç³»ç»Ÿåœ°æ¯”è¾ƒäº†é¡ºåºæœç´¢å’Œå¹¶è¡Œé‡‡æ ·ä¸¤ç§ç­–ç•¥ï¼Œåœ¨CountDownå’ŒSudokuä¸¤ä¸ªä»»åŠ¡ä¸­è¡¨ç°å‡ºä¸åŒçš„æ€§èƒ½è¡¨ç°ã€‚å‘ç°å›æº¯åœ¨æŸäº›ä»»åŠ¡ä¸Šå¹¶éæ™®éæœ‰ç›Šï¼Œç ”ç©¶è¿˜å‘ç°å¯¼è‡´å›æº¯æ€§èƒ½ä¸‹é™çš„ä¸¤ä¸ªå› ç´ ã€‚åŒæ—¶ï¼Œåœ¨å¼ºåŒ–å­¦ä¹ åœºæ™¯ä¸‹ï¼Œå…·æœ‰å›æº¯èƒ½åŠ›çš„æ¨¡å‹èƒ½è·å¾—æ˜¾è‘—æ”¶ç›Šã€‚æ€»ä¹‹ï¼Œæœ¬æ–‡æ­ç¤ºäº†ä»»åŠ¡ç»“æ„ã€è®­ç»ƒæ•°æ®ã€æ¨¡å‹è§„æ¨¡å’Œå­¦ä¹ èŒƒå¼ä¹‹é—´çš„å¤æ‚äº¤äº’å…³ç³»ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›æœ‰æ‰€æå‡ï¼Œä¸»è¦é€šè¿‡æœç´¢å’Œå›æº¯æŠ€æœ¯å®ç°ã€‚</li>
<li>é¡ºåºæœç´¢å’Œå¹¶è¡Œé‡‡æ ·æ˜¯ä¸¤ç§ä¸»è¦çš„æµ‹è¯•æ—¶é—´è®¡ç®—æ‰©å±•ç­–ç•¥ã€‚</li>
<li>åœ¨CountDownä»»åŠ¡ä¸­ï¼Œå¹¶è¡Œé‡‡æ ·è¡¨ç°ä¼˜äºé¡ºåºæœç´¢ï¼›è€Œåœ¨Sudokuä»»åŠ¡ä¸­ï¼Œé¡ºåºæœç´¢è¡¨ç°è¾ƒå¥½ã€‚</li>
<li>å›æº¯å¹¶éåœ¨æ‰€æœ‰ä»»åŠ¡ä¸­éƒ½æ™®éæœ‰ç›Šã€‚</li>
<li>è®­ç»ƒå›ºå®šæœç´¢è½¨è¿¹ä¼šé”å®šæ¨¡å‹è¿›å…¥æ¬¡ä¼˜ç­–ç•¥ï¼Œæ˜ç¡®æ€ç»´è½¨è¿¹ç›‘ç£ä¼šæŠ‘åˆ¶â€œéšæ€§â€æ¨ç†ã€‚</li>
<li>å…·æœ‰å›æº¯èƒ½åŠ›çš„æ¨¡å‹åœ¨å¼ºåŒ–å­¦ä¹ å¾®è°ƒä¸‹å¯è·å¾—æ˜¾è‘—æ”¶ç›Šï¼Œè€Œç¼ºä¹å›æº¯çš„æ¨¡å‹åˆ™è¡¨ç°æœ‰é™ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07052">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-798465083b46719d939d9bc7f255aa6a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f4822fc53d57bfd94c4767329ce7272e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7282154e24661a07398e218a4ed69737.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7dd5a45dc6963f32584010916e3a1b70.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="A-Unified-Agentic-Framework-for-Evaluating-Conditional-Image-Generation"><a href="#A-Unified-Agentic-Framework-for-Evaluating-Conditional-Image-Generation" class="headerlink" title="A Unified Agentic Framework for Evaluating Conditional Image Generation"></a>A Unified Agentic Framework for Evaluating Conditional Image Generation</h2><p><strong>Authors:Jifang Wang, Xue Yang, Longyue Wang, Zhenran Xu, Yiyu Wang, Yaowei Wang, Weihua Luo, Kaifu Zhang, Baotian Hu, Min Zhang</strong></p>
<p>Conditional image generation has gained significant attention for its ability to personalize content. However, the field faces challenges in developing task-agnostic, reliable, and explainable evaluation metrics. This paper introduces CIGEval, a unified agentic framework for comprehensive evaluation of conditional image generation tasks. CIGEval utilizes large multimodal models (LMMs) as its core, integrating a multi-functional toolbox and establishing a fine-grained evaluation framework. Additionally, we synthesize evaluation trajectories for fine-tuning, empowering smaller LMMs to autonomously select appropriate tools and conduct nuanced analyses based on tool outputs. Experiments across seven prominent conditional image generation tasks demonstrate that CIGEval (GPT-4o version) achieves a high correlation of 0.4625 with human assessments, closely matching the inter-annotator correlation of 0.47. Moreover, when implemented with 7B open-source LMMs using only 2.3K training trajectories, CIGEval surpasses the previous GPT-4o-based state-of-the-art method. Case studies on GPT-4o image generation highlight CIGEvalâ€™s capability in identifying subtle issues related to subject consistency and adherence to control guidance, indicating its great potential for automating evaluation of image generation tasks with human-level reliability. </p>
<blockquote>
<p>æ¡ä»¶å›¾åƒç”Ÿæˆå› å…¶ä¸ªæ€§åŒ–å†…å®¹çš„èƒ½åŠ›è€Œå—åˆ°å¹¿æ³›å…³æ³¨ã€‚ç„¶è€Œï¼Œè¯¥é¢†åŸŸåœ¨å¼€å‘ä»»åŠ¡æ— å…³ã€å¯é å’Œå¯è§£é‡Šçš„è¯„ä»·æŒ‡æ ‡æ–¹é¢é¢ä¸´æŒ‘æˆ˜ã€‚æœ¬æ–‡ä»‹ç»äº†CIGEvalï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„æ™ºèƒ½æ¡†æ¶ï¼Œç”¨äºå¯¹æ¡ä»¶å›¾åƒç”Ÿæˆä»»åŠ¡è¿›è¡Œå…¨é¢è¯„ä¼°ã€‚CIGEvalä»¥å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰ä¸ºæ ¸å¿ƒï¼Œé›†æˆå¤šåŠŸèƒ½å·¥å…·ç®±å¹¶å»ºç«‹ç²¾ç»†çš„è¯„ä»·æ¡†æ¶ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ç»¼åˆè¯„ä¼°è½¨è¿¹è¿›è¡Œå¾®è°ƒï¼Œä½¿è¾ƒå°çš„LMMèƒ½å¤Ÿè‡ªä¸»åœ°é€‰æ‹©åˆé€‚çš„å·¥å…·ï¼Œå¹¶æ ¹æ®å·¥å…·è¾“å‡ºè¿›è¡Œç»†å¾®åˆ†æã€‚åœ¨ä¸ƒä¸ªçªå‡ºçš„æ¡ä»¶å›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCIGEvalï¼ˆGPT-4oç‰ˆæœ¬ï¼‰ä¸äººç±»è¯„ä¼°è¾¾åˆ°0.4625çš„é«˜ç›¸å…³æ€§ï¼Œæ¥è¿‘äººå·¥è¯„ä¼°é—´äº’æ ‡æ³¨çš„ç›¸å…³ç³»æ•°0.47ã€‚è€Œä¸”ï¼Œå½“ä½¿ç”¨ä»…åŒ…å«2.3Kè®­ç»ƒè½¨è¿¹çš„7Bå¼€æºLMMå®ç°æ—¶ï¼ŒCIGEvalè¶…è¶Šäº†åŸºäºGPT-4oçš„å…ˆå‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚å…³äºGPT-4oå›¾åƒç”Ÿæˆçš„æ¡ˆä¾‹ç ”ç©¶çªå‡ºäº†CIGEvalåœ¨è¯†åˆ«ä¸ä¸»é¢˜ä¸€è‡´æ€§å’Œéµå¾ªæ§åˆ¶æŒ‡å¯¼ç›¸å…³çš„ç»†å¾®é—®é¢˜æ–¹é¢çš„èƒ½åŠ›ï¼Œè¡¨æ˜å…¶åœ¨è‡ªåŠ¨åŒ–å›¾åƒç”Ÿæˆä»»åŠ¡è¯„ä¼°æ–¹é¢å…·æœ‰æ¥è¿‘äººç±»å¯é æ€§çš„å·¨å¤§æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07046v1">PDF</a> Work in progress. GitHub:   <a target="_blank" rel="noopener" href="https://github.com/HITsz-TMG/Agentic-CIGEval">https://github.com/HITsz-TMG/Agentic-CIGEval</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªåä¸ºCIGEvalçš„ç»Ÿä¸€æ™ºèƒ½æ¡†æ¶ï¼Œç”¨äºå…¨é¢è¯„ä¼°æ¡ä»¶å›¾åƒç”Ÿæˆä»»åŠ¡ã€‚è¯¥æ¡†æ¶åˆ©ç”¨å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ä¸ºæ ¸å¿ƒï¼Œé›†æˆå¤šåŠŸèƒ½å·¥å…·ç®±å¹¶å»ºç«‹ç²¾ç»†çš„è¯„ä»·æ¡†æ¶ã€‚å®éªŒè¡¨æ˜ï¼ŒCIGEvalåœ¨ä¸ƒä¸ªä¸»æµæ¡ä»¶å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­çš„è¡¨ç°ä¸äººç±»è¯„ä¼°é«˜åº¦ç›¸å…³ï¼Œä¸”åœ¨æŸäº›æƒ…å†µä¸‹è¶…è¶Šäº†ä¹‹å‰çš„æœ€æ–°æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CIGEvalæ˜¯ä¸€ä¸ªç”¨äºæ¡ä»¶å›¾åƒç”Ÿæˆä»»åŠ¡çš„ç»¼åˆè¯„ä¼°æ¡†æ¶ã€‚</li>
<li>è¯¥æ¡†æ¶åˆ©ç”¨å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰ä¸ºæ ¸å¿ƒï¼Œæä¾›ç²¾ç»†çš„è¯„ä»·ä½“ç³»ã€‚</li>
<li>CIGEvalé€šè¿‡åˆæˆè¯„ä»·è½¨è¿¹è¿›è¡Œå¾®è°ƒï¼Œä½¿è¾ƒå°çš„LMMsèƒ½å¤Ÿè‡ªä¸»é€‰æ‹©é€‚å½“çš„å·¥å…·å¹¶è¿›è¡Œç»†è‡´çš„åˆ†æã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒCIGEvalåœ¨ä¸ƒä¸ªä¸»æµæ¡ä»¶å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­çš„è¡¨ç°ä¸äººç±»è¯„ä¼°é«˜åº¦ç›¸å…³ã€‚</li>
<li>CIGEvalåœ¨æŸäº›æƒ…å†µä¸‹è¶…è¶Šäº†ä¹‹å‰çš„æœ€æ–°æ–¹æ³•ï¼Œå¦‚ä½¿ç”¨7Bå¼€æºLMMsä»…2.3Kè®­ç»ƒè½¨è¿¹æ—¶ã€‚</li>
<li>æ¡ˆä¾‹ç ”ç©¶è¯æ˜äº†CIGEvalåœ¨è¯†åˆ«ä¸»é¢˜ä¸€è‡´æ€§å’Œéµå¾ªæ§åˆ¶æŒ‡å¯¼æ–¹é¢çš„å¾®å¦™é—®é¢˜æ–¹é¢çš„èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07046">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-59a97bf6dd1be2472272790a086f6dc1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-91f9c0b73fb5d39429ab63865d2543a0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f6b653bb28b3a2cff89583a92d333da4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-191a3b446e3897b1ebc1b675f45eb8b4.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="VideoChat-R1-Enhancing-Spatio-Temporal-Perception-via-Reinforcement-Fine-Tuning"><a href="#VideoChat-R1-Enhancing-Spatio-Temporal-Perception-via-Reinforcement-Fine-Tuning" class="headerlink" title="VideoChat-R1: Enhancing Spatio-Temporal Perception via Reinforcement   Fine-Tuning"></a>VideoChat-R1: Enhancing Spatio-Temporal Perception via Reinforcement   Fine-Tuning</h2><p><strong>Authors:Xinhao Li, Ziang Yan, Desen Meng, Lu Dong, Xiangyu Zeng, Yinan He, Yali Wang, Yu Qiao, Yi Wang, Limin Wang</strong></p>
<p>Recent advancements in reinforcement learning have significantly advanced the reasoning capabilities of multimodal large language models (MLLMs). While approaches such as Group Relative Policy Optimization (GRPO) and rule-based reward mechanisms demonstrate promise in text and image domains, their application to video understanding remains limited. This paper presents a systematic exploration of Reinforcement Fine-Tuning (RFT) with GRPO for video MLLMs, aiming to enhance spatio-temporal perception while maintaining general capabilities. Our experiments reveal that RFT is highly data-efficient for task-specific improvements. Through multi-task RFT on spatio-temporal perception objectives with limited samples, we develop VideoChat-R1, a powerful video MLLM that achieves state-of-the-art performance on spatio-temporal perception tasks without sacrificing chat ability, while exhibiting emerging spatio-temporal reasoning abilities. Compared to Qwen2.5-VL-7B, VideoChat-R1 boosts performance several-fold in tasks like temporal grounding (+31.8) and object tracking (+31.2). Additionally, it significantly improves on general QA benchmarks such as VideoMME (+0.9), MVBench (+1.0), and Perception Test (+0.9). Our findings underscore the potential of RFT for specialized task enhancement of Video MLLMs. We hope our work offers valuable insights for future RL research in video MLLMs. </p>
<blockquote>
<p>è¿‘æœŸå¼ºåŒ–å­¦ä¹ é¢†åŸŸçš„è¿›å±•æå¤§åœ°æå‡äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚è™½ç„¶è¯¸å¦‚é›†å›¢ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰å’ŒåŸºäºè§„åˆ™çš„å¥–åŠ±æœºåˆ¶ç­‰åœ¨æ–‡æœ¬å’Œå›¾åƒé¢†åŸŸè¡¨ç°å‡ºå¸Œæœ›ï¼Œä½†å®ƒä»¬åœ¨è§†é¢‘ç†è§£æ–¹é¢çš„åº”ç”¨ä»ç„¶æœ‰é™ã€‚æœ¬æ–‡ç³»ç»Ÿåœ°æ¢ç´¢äº†ç”¨äºè§†é¢‘MLLMçš„å¼ºåŒ–å¾®è°ƒï¼ˆRFTï¼‰ä¸GRPOçš„ç»“åˆï¼Œæ—¨åœ¨æé«˜æ—¶ç©ºæ„ŸçŸ¥èƒ½åŠ›çš„åŒæ—¶ä¿æŒä¸€èˆ¬èƒ½åŠ›ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒRFTå¯¹äºç‰¹å®šä»»åŠ¡çš„æ”¹è¿›éå¸¸æ•°æ®é«˜æ•ˆã€‚é€šè¿‡æœ‰é™æ ·æœ¬çš„æ—¶ç©ºæ„ŸçŸ¥ç›®æ ‡ä¸Šçš„å¤šä»»åŠ¡RFTï¼Œæˆ‘ä»¬å¼€å‘å‡ºäº†VideoChat-R1ï¼Œè¿™æ˜¯ä¸€æ¬¾å¼ºå¤§çš„è§†é¢‘MLLMï¼Œå®ƒåœ¨æ—¶ç©ºæ„ŸçŸ¥ä»»åŠ¡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¸ç‰ºç‰²èŠå¤©èƒ½åŠ›ï¼Œå¹¶å±•ç°å‡ºæ–°å…´çš„æ—¶ç©ºæ¨ç†èƒ½åŠ›ã€‚ä¸Qwen2.5-VL-7Bç›¸æ¯”ï¼ŒVideoChat-R1åœ¨è¯¸å¦‚æ—¶é—´å®šä½ï¼ˆ+31.8ï¼‰å’Œå¯¹è±¡è·Ÿè¸ªï¼ˆ+31.2ï¼‰ç­‰ä»»åŠ¡ä¸Šçš„æ€§èƒ½æé«˜äº†æ•°å€ã€‚æ­¤å¤–ï¼Œå®ƒåœ¨é€šç”¨é—®ç­”åŸºå‡†æµ‹è¯•ï¼ˆå¦‚VideoMME +0.9ã€MVBench +1.0å’Œæ„ŸçŸ¥æµ‹è¯•+ 0.9ï¼‰ä¸Šä¹Ÿå–å¾—äº†æ˜¾è‘—æ”¹å–„ã€‚æˆ‘ä»¬çš„ç ”ç©¶çªå‡ºäº†RFTåœ¨è§†é¢‘MLLMä¸“é¡¹ä»»åŠ¡å¢å¼ºæ–¹é¢çš„æ½œåŠ›ã€‚æˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„ç ”ç©¶èƒ½ä¸ºæœªæ¥è§†é¢‘MLLMçš„RLç ”ç©¶æä¾›æœ‰ä»·å€¼çš„è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.06958v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤šåª’ä½“æ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æ¨ç†èƒ½åŠ›åœ¨å¼ºåŒ–å­¦ä¹ é¢†åŸŸçš„æœ€æ–°è¿›å±•ä¸­å¾—åˆ°äº†æ˜¾è‘—æå‡ã€‚æœ¬ç ”ç©¶é‡‡ç”¨å¼ºåŒ–å¾®è°ƒï¼ˆRFTï¼‰ä¸ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ç›¸ç»“åˆçš„æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜è§†é¢‘MLLMsçš„æ—¶ç©ºæ„ŸçŸ¥èƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒå…¶é€šç”¨èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼ŒRFTåœ¨ç‰¹å®šä»»åŠ¡ä¸Šå…·æœ‰é«˜æ•ˆçš„æ•°æ®åˆ©ç”¨ç‡ï¼Œé€šè¿‡å¤šä»»åŠ¡RFTåœ¨æ—¶ç©ºæ„ŸçŸ¥ç›®æ ‡ä¸Šè¿›è¡Œæœ‰é™æ ·æœ¬è®­ç»ƒï¼ŒæˆåŠŸå¼€å‘å‡ºVideoChat-R1æ¨¡å‹ã€‚è¯¥æ¨¡å‹åœ¨æ—¶ç©ºæ„ŸçŸ¥ä»»åŠ¡ä¸Šå®ç°äº†å“è¶Šæ€§èƒ½ï¼ŒåŒæ—¶ä¸ç‰ºç‰²å¯¹è¯èƒ½åŠ›ï¼Œå±•ç°å‡ºæ–°å…´çš„æ—¶ç©ºæ¨ç†èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ åœ¨æå‡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æ¨ç†èƒ½åŠ›æ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ã€‚</li>
<li>Group Relative Policy Optimization (GRPO) ä¸å¼ºåŒ–å¾®è°ƒï¼ˆRFTï¼‰ç»“åˆåº”ç”¨ï¼Œæœ‰åŠ©äºå¢å¼ºè§†é¢‘MLLMsçš„æ—¶ç©ºæ„ŸçŸ¥èƒ½åŠ›ã€‚</li>
<li>RFTæ–¹æ³•åœ¨ç‰¹å®šä»»åŠ¡ä¸Šå…·æœ‰é«˜æ•ˆæ•°æ®åˆ©ç”¨ç‡ã€‚</li>
<li>VideoChat-R1æ¨¡å‹é€šè¿‡å¤šä»»åŠ¡RFTè®­ç»ƒï¼Œå®ç°äº†åœ¨æ—¶ç©ºæ„ŸçŸ¥ä»»åŠ¡ä¸Šçš„å“è¶Šæ€§èƒ½ã€‚</li>
<li>VideoChat-R1æ¨¡å‹åœ¨ä¸ç‰ºç‰²å¯¹è¯èƒ½åŠ›çš„å‰æä¸‹ï¼Œå±•ç°å‡ºæ–°å…´çš„æ—¶ç©ºæ¨ç†èƒ½åŠ›ã€‚</li>
<li>ä¸Qwen2.5-VL-7Bç›¸æ¯”ï¼ŒVideoChat-R1åœ¨æ—¶ç©ºæ„ŸçŸ¥ä»»åŠ¡ä¸Šçš„æ€§èƒ½æœ‰æ‰€æå‡ï¼Œå¦‚æ—¶é—´å®šä½æå‡31.8%ï¼Œç›®æ ‡è·Ÿè¸ªæå‡31.2%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.06958">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-276014d8a0684732775efd6b0c50e2c9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-89b3adc13557b37016357b42472fc020.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8c518c1061834b2dab5dbf91bfa54e1a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d0d3ab1d68f717f51aca88e4c25f7ced.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-247a56547855773e359a20635b8f470e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c392c0aeea22b73fb78f63e50e43e246.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="MovSAM-A-Single-image-Moving-Object-Segmentation-Framework-Based-on-Deep-Thinking"><a href="#MovSAM-A-Single-image-Moving-Object-Segmentation-Framework-Based-on-Deep-Thinking" class="headerlink" title="MovSAM: A Single-image Moving Object Segmentation Framework Based on   Deep Thinking"></a>MovSAM: A Single-image Moving Object Segmentation Framework Based on   Deep Thinking</h2><p><strong>Authors:Chang Nie, Yiqing Xu, Guangming Wang, Zhe Liu, Yanzi Miao, Hesheng Wang</strong></p>
<p>Moving object segmentation plays a vital role in understanding dynamic visual environments. While existing methods rely on multi-frame image sequences to identify moving objects, single-image MOS is critical for applications like motion intention prediction and handling camera frame drops. However, segmenting moving objects from a single image remains challenging for existing methods due to the absence of temporal cues. To address this gap, we propose MovSAM, the first framework for single-image moving object segmentation. MovSAM leverages a Multimodal Large Language Model (MLLM) enhanced with Chain-of-Thought (CoT) prompting to search the moving object and generate text prompts based on deep thinking for segmentation. These prompts are cross-fused with visual features from the Segment Anything Model (SAM) and a Vision-Language Model (VLM), enabling logic-driven moving object segmentation. The segmentation results then undergo a deep thinking refinement loop, allowing MovSAM to iteratively improve its understanding of the scene context and inter-object relationships with logical reasoning. This innovative approach enables MovSAM to segment moving objects in single images by considering scene understanding. We implement MovSAM in the real world to validate its practical application and effectiveness for autonomous driving scenarios where the multi-frame methods fail. Furthermore, despite the inherent advantage of multi-frame methods in utilizing temporal information, MovSAM achieves state-of-the-art performance across public MOS benchmarks, reaching 92.5% on J&amp;F. Our implementation will be available at <a target="_blank" rel="noopener" href="https://github.com/IRMVLab/MovSAM">https://github.com/IRMVLab/MovSAM</a>. </p>
<blockquote>
<p>åŠ¨æ€ç¯å¢ƒä¸­çš„ç§»åŠ¨å¯¹è±¡åˆ†å‰²åœ¨ç†è§£åŠ¨æ€è§†è§‰ç¯å¢ƒä¸­èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚è™½ç„¶ç°æœ‰æ–¹æ³•ä¾èµ–äºå¤šå¸§å›¾åƒåºåˆ—æ¥è¯†åˆ«ç§»åŠ¨å¯¹è±¡ï¼Œä½†å•å›¾åƒç§»åŠ¨å¯¹è±¡åˆ†å‰²ï¼ˆMOSï¼‰å¯¹äºè¿åŠ¨æ„å›¾é¢„æµ‹å’Œæ‘„åƒå¤´å¸§ä¸¢å¤±å¤„ç†ç­‰é¢†åŸŸçš„åº”ç”¨è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹æ—¶é—´çº¿ç´¢ï¼Œä»å•å¹…å›¾åƒä¸­åˆ†å‰²ç§»åŠ¨å¯¹è±¡å¯¹äºç°æœ‰æ–¹æ³•æ¥è¯´ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†å•å›¾åƒç§»åŠ¨å¯¹è±¡åˆ†å‰²çš„ç¬¬ä¸€ä¸ªæ¡†æ¶â€”â€”MovSAMã€‚MovSAMåˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œé€šè¿‡æ€ç»´é“¾ï¼ˆCoTï¼‰æç¤ºæ¥æœç´¢ç§»åŠ¨å¯¹è±¡ï¼Œå¹¶åŸºäºæ·±åº¦æ€è€ƒç”Ÿæˆæ–‡æœ¬æç¤ºè¿›è¡Œåˆ†å‰²ã€‚è¿™äº›æç¤ºä¸æ¥è‡ªåˆ†æ®µä»»ä½•æ¨¡å‹ï¼ˆSAMï¼‰å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„è§†è§‰ç‰¹å¾è¿›è¡Œäº¤å‰èåˆï¼Œå®ç°é€»è¾‘é©±åŠ¨çš„ç§»åŠ¨å¯¹è±¡åˆ†å‰²ã€‚åˆ†å‰²ç»“æœç„¶åç»è¿‡æ·±åº¦æ€è€ƒä¼˜åŒ–å¾ªç¯ï¼Œä½¿MovSamèƒ½å¤Ÿä¸æ–­è¿­ä»£åœ°æé«˜å¯¹åœºæ™¯ä¸Šä¸‹æ–‡å’Œå¯¹è±¡é—´å…³ç³»çš„ç†è§£ï¼Œè¿›è¡Œé€»è¾‘æ¨ç†ã€‚è¿™ç§åˆ›æ–°çš„æ–¹æ³•ä½¿MovSAMèƒ½å¤Ÿé€šè¿‡è€ƒè™‘åœºæ™¯ç†è§£æ¥åˆ†å‰²å•å¹…å›¾åƒä¸­çš„ç§»åŠ¨å¯¹è±¡ã€‚æˆ‘ä»¬åœ¨ç°å®ä¸–ç•Œä¸­å®ç°äº†MovSAMï¼Œä»¥éªŒè¯å…¶åœ¨è‡ªåŠ¨é©¾é©¶åœºæ™¯ä¸­çš„å®é™…åº”ç”¨å’Œæœ‰æ•ˆæ€§ï¼Œåœ¨è¿™äº›åœºæ™¯ä¸­ï¼Œå¤šå¸§æ–¹æ³•ä¼šå¤±æ•ˆã€‚æ­¤å¤–ï¼Œå°½ç®¡å¤šå¸§æ–¹æ³•åœ¨åˆ©ç”¨æ—¶é—´ä¿¡æ¯æ–¹é¢å…·æœ‰å›ºæœ‰ä¼˜åŠ¿ï¼Œä½†MovSAMåœ¨å…¬å…±MOSåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ï¼Œåœ¨J&amp;Fä¸Šè¾¾åˆ°äº†92.5%ã€‚æˆ‘ä»¬çš„å®ç°å°†å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/IRMVLab/MovSAM%E3%80%82">https://github.com/IRMVLab/MovSAMã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.06863v1">PDF</a> </p>
<p><strong>Summary</strong><br>å•å›¾åƒåŠ¨æ€ç›®æ ‡åˆ†å‰²åœ¨ç†è§£åŠ¨æ€è§†è§‰ç¯å¢ƒä¸­èµ·åˆ°é‡è¦ä½œç”¨ã€‚å½“å‰çš„æ–¹æ³•å¤šä¾èµ–äºå¤šå¸§å›¾åƒåºåˆ—æ¥è¯†åˆ«åŠ¨æ€ç›®æ ‡ï¼Œä½†åœ¨è¿åŠ¨æ„å›¾é¢„æµ‹å’Œåº”å¯¹æ‘„åƒæœºæ‰å¸§ç­‰åº”ç”¨ä¸­ï¼Œå•å›¾åƒåŠ¨æ€ç›®æ ‡åˆ†å‰²æ˜¯å…³é”®ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹æ—¶é—´çº¿ç´¢ï¼Œä»å•å¹…å›¾åƒä¸­åˆ†å‰²å‡ºåŠ¨æ€ç›®æ ‡å¯¹äºç°æœ‰æ–¹æ³•æ¥è¯´ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†é¦–ä¸ªå•å›¾åƒåŠ¨æ€ç›®æ ‡åˆ†å‰²æ¡†æ¶MovSAMã€‚MovSAMåˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ç»“åˆæ€ç»´é“¾ï¼ˆCoTï¼‰æç¤ºæ¥æœç´¢åŠ¨æ€ç›®æ ‡ï¼Œå¹¶åŸºäºæ·±åº¦æ€è€ƒç”Ÿæˆæ–‡æœ¬æç¤ºè¿›è¡Œåˆ†å‰²ã€‚è¿™äº›æç¤ºä¸åˆ†æ®µä»»ä½•äº‹æƒ…æ¨¡å‹ï¼ˆSAMï¼‰å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„è§†è§‰ç‰¹å¾ç›¸èåˆï¼Œå®ç°äº†é€»è¾‘é©±åŠ¨çš„åŠ¨æ€ç›®æ ‡åˆ†å‰²ã€‚MovSAMé€šè¿‡æ·±åº¦æ€è€ƒä¼˜åŒ–å¾ªç¯å¯¹åˆ†å‰²ç»“æœè¿›è¡Œä¼˜åŒ–ï¼Œèƒ½å¤Ÿä¸æ–­æ”¹å–„å¯¹åœºæ™¯ä¸Šä¸‹æ–‡å’Œç›®æ ‡é—´å…³ç³»çš„ç†è§£ã€‚è¿™ç§åˆ›æ–°æ–¹æ³•ä½¿å¾—MovSAMèƒ½å¤Ÿåœ¨è€ƒè™‘åœºæ™¯ç†è§£çš„æƒ…å†µä¸‹ï¼Œå¯¹å•å¹…å›¾åƒä¸­çš„åŠ¨æ€ç›®æ ‡è¿›è¡Œåˆ†å‰²ã€‚æˆ‘ä»¬åœ¨ç°å®ä¸–ç•Œå®ç°äº†MovSAMï¼ŒéªŒè¯äº†å…¶åœ¨è‡ªåŠ¨é©¾é©¶ç­‰åœºæ™¯ä¸­çš„å®é™…åº”ç”¨æ•ˆæœå’Œä¼˜è¶Šæ€§ï¼Œå³ä½¿åœ¨å¤šå¸§æ–¹æ³•å¤±æ•ˆçš„æƒ…å†µä¸‹ä¹Ÿèƒ½è¾¾åˆ°æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨J&amp;Fä¸Šè¾¾åˆ°92.5%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MovSAMæ˜¯é¦–ä¸ªé’ˆå¯¹å•å›¾åƒåŠ¨æ€ç›®æ ‡åˆ†å‰²çš„æ¡†æ¶ã€‚</li>
<li>MovSAMåˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ç»“åˆæ€ç»´é“¾ï¼ˆCoTï¼‰æç¤ºæ¥è¯†åˆ«åŠ¨æ€ç›®æ ‡ã€‚</li>
<li>é€šè¿‡æ·±åº¦æ€è€ƒç”Ÿæˆçš„æ–‡æœ¬æç¤ºä¸è§†è§‰ç‰¹å¾èåˆï¼Œå®ç°é€»è¾‘é©±åŠ¨çš„åŠ¨æ€ç›®æ ‡åˆ†å‰²ã€‚</li>
<li>MovSAMé€šè¿‡æ·±åº¦æ€è€ƒä¼˜åŒ–å¾ªç¯èƒ½å¤Ÿæ”¹å–„å¯¹åœºæ™¯ä¸Šä¸‹æ–‡å’Œç›®æ ‡é—´å…³ç³»çš„ç†è§£ã€‚</li>
<li>MovSAMåœ¨å•å›¾åƒåŠ¨æ€ç›®æ ‡åˆ†å‰²ä¸­è€ƒè™‘äº†åœºæ™¯ç†è§£ã€‚</li>
<li>åœ¨ç°å®ä¸–ç•Œçš„å®é™…åº”ç”¨ä¸­ï¼ŒMovSAMåœ¨è‡ªåŠ¨é©¾é©¶ç­‰åœºæ™¯è¡¨ç°å‡ºä¼˜è¶Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.06863">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-7548b00ea6e7b3cffc8b6d29f640cc85.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9ab3af68b20ca313254ded5f51703483.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-df9994fc5e8b40c5c1d5b7422b25a879.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1ae221583005626368b7a92633415557.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2cb734218e5fbcabedfb136dcd1cfd2f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d167d84989aad3247d3e91757b331da4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-03394e8e963fd583c5adb2fcf8d57f99.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="ThoughtProbe-Classifier-Guided-Thought-Space-Exploration-Leveraging-LLM-Intrinsic-Reasoning"><a href="#ThoughtProbe-Classifier-Guided-Thought-Space-Exploration-Leveraging-LLM-Intrinsic-Reasoning" class="headerlink" title="ThoughtProbe: Classifier-Guided Thought Space Exploration Leveraging LLM   Intrinsic Reasoning"></a>ThoughtProbe: Classifier-Guided Thought Space Exploration Leveraging LLM   Intrinsic Reasoning</h2><p><strong>Authors:Zijian Wang, Chang Xu</strong></p>
<p>Pre-trained large language models (LLMs) have been demonstrated to possess intrinsic reasoning capabilities that can emerge naturally when expanding the response space. However, the neural representation mechanisms underlying these intrinsic capabilities and approaches for their optimal utilization remain inadequately understood. In this work, we make the key discovery that a simple linear classifier can effectively detect intrinsic reasoning capabilities in LLMsâ€™ activation space, particularly within specific representation types and network layers. Based on this finding, we propose a classifier-guided search framework that strategically explore a tree-structured response space. In each node expansion, the classifier serves as a scoring and ranking mechanism that efficiently allocates computational resources by identifying and prioritizing more thoughtful reasoning directions for continuation. After completing the tree expansion, we collect answers from all branches to form a candidate answer pool. We propose a branch-aggregation selection method that marginalizes over all supporting branches by aggregating their thoughtfulness scores, thereby identifying the optimal answer from the pool. Experimental results show that our frameworkâ€™s comprehensive exploration not only covers valid reasoning chains but also effectively identifies them, achieving significant improvements across multiple arithmetic reasoning benchmarks. </p>
<blockquote>
<p>é¢„è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²è¡¨ç°å‡ºå†…åœ¨æ¨ç†èƒ½åŠ›ï¼Œåœ¨æ‰©å¤§å“åº”ç©ºé—´æ—¶è¿™äº›èƒ½åŠ›å¯ä»¥è‡ªç„¶å‡ºç°ã€‚ç„¶è€Œï¼Œè¿™äº›å†…åœ¨èƒ½åŠ›çš„åŸºç¡€ç¥ç»è¡¨å¾æœºåˆ¶å’Œå¦‚ä½•æœ€ä¼˜åˆ©ç”¨å®ƒä»¬çš„æ–¹æ³•ä»ç†è§£ä¸è¶³ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å…³é”®åœ°å‘ç°ä¸€ä¸ªç®€å•çš„çº¿æ€§åˆ†ç±»å™¨å¯ä»¥æœ‰æ•ˆåœ°æ£€æµ‹LLMæ¿€æ´»ç©ºé—´ä¸­çš„å†…åœ¨æ¨ç†èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨ç‰¹å®šçš„è¡¨å¾ç±»å‹å’Œç½‘ç»œå±‚å†…ã€‚åŸºäºè¿™ä¸€å‘ç°ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåˆ†ç±»å™¨å¼•å¯¼æœç´¢æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æœ‰ç­–ç•¥åœ°æ¢ç´¢æ ‘å½¢å“åº”ç©ºé—´ã€‚åœ¨æ¯æ¬¡èŠ‚ç‚¹æ‰©å±•ä¸­ï¼Œåˆ†ç±»å™¨å……å½“è¯„åˆ†å’Œæ’åæœºåˆ¶ï¼Œé€šè¿‡è¯†åˆ«å’Œä¼˜å…ˆè€ƒè™‘æ›´æœ‰æ·±åº¦çš„æ¨ç†æ–¹å‘æ¥ç»§ç»­ï¼Œä»è€Œæœ‰æ•ˆåœ°åˆ†é…è®¡ç®—èµ„æºã€‚å®Œæˆæ ‘æ‰©å±•åï¼Œæˆ‘ä»¬ä»æ‰€æœ‰åˆ†æ”¯æ”¶é›†ç­”æ¡ˆä»¥å½¢æˆå€™é€‰ç­”æ¡ˆæ± ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ†æ”¯èšåˆé€‰æ‹©æ–¹æ³•ï¼Œå®ƒé€šè¿‡èšåˆæ‰€æœ‰æ”¯æŒåˆ†æ”¯çš„æ€è€ƒå¾—åˆ†æ¥å¯¹å®ƒä»¬è¿›è¡Œæ•´ä½“è¯„ä¼°ï¼Œä»è€Œä»æ± ä¸­è¯†åˆ«æœ€ä½³ç­”æ¡ˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æ¡†æ¶çš„å…¨é¢æ¢ç´¢ä¸ä»…æ¶µç›–äº†æœ‰æ•ˆçš„æ¨ç†é“¾ï¼Œè€Œä¸”èƒ½å¤Ÿæœ‰æ•ˆåœ°è¯†åˆ«å®ƒä»¬ï¼Œåœ¨å¤šä¸ªç®—æœ¯æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æ˜¾è‘—æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.06650v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>é¢„è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å…·æœ‰å†…åœ¨æ¨ç†èƒ½åŠ›ï¼Œåœ¨æ‰©å¤§å“åº”ç©ºé—´æ—¶è¿™äº›èƒ½åŠ›å¯è‡ªç„¶æ˜¾ç°ã€‚ç„¶è€Œï¼Œè¿™äº›å†…åœ¨èƒ½åŠ›çš„ç¥ç»è¡¨å¾æœºåˆ¶åŠå…¶æœ€ä¼˜åˆ©ç”¨æ–¹æ³•å°šä¸å®Œå…¨æ¸…æ¥šã€‚æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬é‡è¦åœ°å‘ç°ä¸€ä¸ªç®€å•çš„çº¿æ€§åˆ†ç±»å™¨å¯æœ‰æ•ˆæ£€æµ‹LLMæ¿€æ´»ç©ºé—´ä¸­çš„å†…åœ¨æ¨ç†èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨ç‰¹å®šè¡¨å¾ç±»å‹å’Œç½‘ç»œå±‚å†…ã€‚åŸºäºæ­¤å‘ç°ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåˆ†ç±»å™¨å¼•å¯¼æœç´¢æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä»¥ç­–ç•¥æ–¹å¼æ¢ç´¢æ ‘çŠ¶å“åº”ç©ºé—´ã€‚åœ¨æ¯æ¬¡èŠ‚ç‚¹æ‰©å±•æ—¶ï¼Œåˆ†ç±»å™¨å……å½“è¯„åˆ†å’Œæ’åæœºåˆ¶ï¼Œé€šè¿‡è¯†åˆ«å¹¶ä¼˜å…ˆè€ƒè™‘æ›´æœ‰æ·±åº¦çš„æ¨ç†æ–¹å‘æ¥ç»§ç»­ï¼Œæœ‰æ•ˆåœ°åˆ†é…è®¡ç®—èµ„æºã€‚å®Œæˆæ ‘æ‰©å±•åï¼Œæˆ‘ä»¬ä»æ‰€æœ‰åˆ†æ”¯æ”¶é›†ç­”æ¡ˆä»¥å½¢æˆå€™é€‰ç­”æ¡ˆæ± ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ†æ”¯èšåˆé€‰æ‹©æ–¹æ³•ï¼Œé€šè¿‡å¯¹æ‰€æœ‰æ”¯æŒåˆ†æ”¯çš„ç½®ä¿¡åº¦å¾—åˆ†è¿›è¡Œæ±‚å’Œï¼Œä»è€Œä»æ± ä¸­è¯†åˆ«å‡ºæœ€ä½³ç­”æ¡ˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶çš„å…¨é¢æ¢ç´¢ä¸ä»…è¦†ç›–äº†æœ‰æ•ˆçš„æ¨ç†é“¾ï¼Œè¿˜èƒ½æœ‰æ•ˆè¯†åˆ«å®ƒä»¬ï¼Œåœ¨å¤šä¸ªç®—æœ¯æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æ˜¾è‘—æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¢„è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹å…·å¤‡å†…åœ¨æ¨ç†èƒ½åŠ›ï¼Œè¿™äº›èƒ½åŠ›åœ¨æ‰©å¤§å“åº”ç©ºé—´æ—¶è‡ªç„¶æ˜¾ç°ã€‚</li>
<li>çº¿æ€§åˆ†ç±»å™¨å¯æœ‰æ•ˆæ£€æµ‹LLMæ¿€æ´»ç©ºé—´ä¸­çš„å†…åœ¨æ¨ç†èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨ç‰¹å®šè¡¨å¾ç±»å‹å’Œç½‘ç»œå±‚å†…ã€‚</li>
<li>æå‡ºä¸€ä¸ªåˆ†ç±»å™¨å¼•å¯¼æœç´¢æ¡†æ¶ï¼Œä»¥ç­–ç•¥æ–¹å¼æ¢ç´¢æ ‘çŠ¶å“åº”ç©ºé—´ã€‚</li>
<li>åœ¨èŠ‚ç‚¹æ‰©å±•æ—¶ï¼Œåˆ†ç±»å™¨å……å½“è¯„åˆ†å’Œæ’åæœºåˆ¶ï¼Œæœ‰æ•ˆåˆ†é…è®¡ç®—èµ„æºã€‚</li>
<li>å®Œæˆæ ‘æ‰©å±•åå½¢æˆå€™é€‰ç­”æ¡ˆæ± ï¼Œé€šè¿‡åˆ†æ”¯èšåˆé€‰æ‹©æ–¹æ³•è¯†åˆ«æœ€ä½³ç­”æ¡ˆã€‚</li>
<li>æ¡†æ¶çš„å…¨é¢æ¢ç´¢è¦†ç›–äº†æœ‰æ•ˆçš„æ¨ç†é“¾ï¼Œå¹¶å®ç°äº†å¯¹å¤šä¸ªç®—æœ¯æ¨ç†åŸºå‡†æµ‹è¯•çš„æ˜¾è‘—æ”¹è¿›ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºLLMçš„å†…åœ¨æ¨ç†èƒ½åŠ›çš„ç†è§£å’Œä¼˜åŒ–åˆ©ç”¨æä¾›äº†æ–°è§†è§’å’Œæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.06650">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dc638b3ac724706d0eaf3adc43a3ed9a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6bb3b55ee9c45831763a43863f12787c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b12e4bc23d3d5c6b9bc85dc33049f202.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fbfa6d3fb65ea319aa1f7ca981ecd28b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0d806b66f181f0e5c04f32f5b7918ccc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d5bc64d99253ff390e5398c467c24009.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="SCI-Reason-A-Dataset-with-Chain-of-Thought-Rationales-for-Complex-Multimodal-Reasoning-in-Academic-Areas"><a href="#SCI-Reason-A-Dataset-with-Chain-of-Thought-Rationales-for-Complex-Multimodal-Reasoning-in-Academic-Areas" class="headerlink" title="SCI-Reason: A Dataset with Chain-of-Thought Rationales for Complex   Multimodal Reasoning in Academic Areas"></a>SCI-Reason: A Dataset with Chain-of-Thought Rationales for Complex   Multimodal Reasoning in Academic Areas</h2><p><strong>Authors:Chenghao Ma, Haihong E., Junpeng Ding, Jun Zhang, Ziyan Ma, Huang Qing, Bofei Gao, Liang Chen, Meina Song</strong></p>
<p>Large Language Models (LLMs) and Large Multimodal Models (LMMs) demonstrate impressive problem-solving skills in many tasks and domains. However, their ability to reason with complex images in academic domains has not been systematically investigated. To bridge this gap, we present SCI-Reason, a dataset for complex multimodel reasoning in academic areas. SCI-Reason aims to test and improve the reasoning ability of large multimodal models using real complex images in academic domains. The dataset contains 12,066 images and 12,626 question-answer pairs extracted from PubMed, divided into training, validation and test splits. Each question-answer pair also contains an accurate and efficient inference chain as a guide to improving the inference properties of the dataset. With SCI-Reason, we performed a comprehensive evaluation of 8 well-known models. The best performing model, Claude-3.7-Sonnet, only achieved an accuracy of 55.19%. Error analysis shows that more than half of the model failures are due to breakdowns in multi-step inference chains rather than errors in primary visual feature extraction. This finding underscores the inherent limitations in reasoning capabilities exhibited by current multimodal models when processing complex image analysis tasks within authentic academic contexts. Experiments on open-source models show that SCI-Reason not only enhances reasoning ability but also demonstrates cross-domain generalization in VQA tasks. We also explore future applications of model inference capabilities in this domain, highlighting its potential for future research. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œå¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨è®¸å¤šä»»åŠ¡å’Œé¢†åŸŸä¸­éƒ½å±•ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„è§£å†³é—®é¢˜çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨å­¦æœ¯é¢†åŸŸä¸­å¯¹å¤æ‚å›¾åƒè¿›è¡Œæ¨ç†çš„èƒ½åŠ›å°šæœªå¾—åˆ°ç³»ç»Ÿçš„ç ”ç©¶ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æ¨å‡ºäº†SCI-Reasonï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºå­¦æœ¯é¢†åŸŸå¤æ‚å¤šæ¨¡æ€æ¨ç†çš„æ•°æ®é›†ã€‚SCI-Reasonæ—¨åœ¨åˆ©ç”¨å­¦æœ¯é¢†åŸŸçš„çœŸå®å¤æ‚å›¾åƒæ¥æµ‹è¯•å’Œæ”¹è¿›å¤§å‹å¤šæ¨¡æ€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚è¯¥æ•°æ®é›†åŒ…å«ä»PubMedä¸­æå–çš„12,066å¼ å›¾åƒå’Œ12,626ä¸ªé—®é¢˜ç­”æ¡ˆå¯¹ï¼Œåˆ†ä¸ºè®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•ä¸‰éƒ¨åˆ†ã€‚æ¯ä¸ªé—®é¢˜ç­”æ¡ˆå¯¹è¿˜åŒ…å«ä¸€ä¸ªå‡†ç¡®é«˜æ•ˆçš„æ¨ç†é“¾ï¼Œä½œä¸ºæ”¹è¿›æ•°æ®é›†æ¨ç†å±æ€§çš„æŒ‡å—ã€‚ä½¿ç”¨SCI-Reasonï¼Œæˆ‘ä»¬å¯¹8ä¸ªçŸ¥åæ¨¡å‹è¿›è¡Œäº†å…¨é¢è¯„ä¼°ã€‚è¡¨ç°æœ€ä½³çš„æ¨¡å‹Claude-3.7-Sonnetçš„å‡†ç¡®ç‡ä»…ä¸º55.19%ã€‚é”™è¯¯åˆ†æè¡¨æ˜ï¼Œè¶…è¿‡ä¸€åŠçš„æ¨¡å‹å¤±è´¥æ˜¯ç”±äºå¤šæ­¥æ¨ç†é“¾çš„å´©æºƒï¼Œè€Œä¸æ˜¯åˆçº§è§†è§‰ç‰¹å¾æå–ä¸­çš„é”™è¯¯ã€‚è¿™ä¸€å‘ç°å¼ºè°ƒäº†å½“å‰å¤šæ¨¡æ€æ¨¡å‹åœ¨å¤„ç†çœŸå®å­¦æœ¯ç¯å¢ƒä¸­çš„å¤æ‚å›¾åƒåˆ†æä»»åŠ¡æ—¶æ‰€å±•ç°çš„æ¨ç†èƒ½åŠ›çš„å›ºæœ‰å±€é™æ€§ã€‚å¯¹å¼€æºæ¨¡å‹çš„å®éªŒè¡¨æ˜ï¼ŒSCI-Reasonä¸ä»…æé«˜äº†æ¨ç†èƒ½åŠ›ï¼Œè€Œä¸”åœ¨VQAä»»åŠ¡ä¸­å±•ç¤ºäº†è·¨åŸŸæ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬è¿˜æ¢è®¨äº†æ¨¡å‹æ¨ç†èƒ½åŠ›åœ¨æœªæ¥é¢†åŸŸçš„åº”ç”¨æ½œåŠ›ï¼Œçªæ˜¾äº†å…¶æœªæ¥ç ”ç©¶çš„é‡è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.06637v1">PDF</a> Submitted to ICCV 2025. 11 pages (including references)</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œå¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨ä¼—å¤šä»»åŠ¡å’Œé¢†åŸŸä¸­çš„é—®é¢˜è§£å†³èƒ½åŠ›ä»¤äººå°è±¡æ·±åˆ»ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨å­¦æœ¯é¢†åŸŸä¸­å¯¹å¤æ‚å›¾åƒè¿›è¡Œæ¨ç†çš„èƒ½åŠ›å°šæœªè¿›è¡Œç³»ç»Ÿæ€§çš„ç ”ç©¶ã€‚ä¸ºå¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æ¨å‡ºäº†SCI-Reasonæ•°æ®é›†ï¼Œç”¨äºæµ‹è¯•å’Œæé«˜å¤§å‹å¤šæ¨¡æ€æ¨¡å‹åœ¨å­¦æœ¯é¢†åŸŸå¯¹å¤æ‚å›¾åƒçš„æ¨ç†èƒ½åŠ›ã€‚è¯¥æ•°æ®é›†åŒ…å«ä»PubMedæå–çš„12,066å¼ å›¾åƒå’Œ12,626ç»„é—®ç­”å¯¹ï¼Œåˆ†ä¸ºè®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•ä¸‰éƒ¨åˆ†ã€‚é€šè¿‡SCI-Reasonæ•°æ®é›†ï¼Œæˆ‘ä»¬å¯¹8ä¸ªçŸ¥åæ¨¡å‹è¿›è¡Œäº†å…¨é¢è¯„ä¼°ã€‚æœ€ä½³æ¨¡å‹Claude-3.7-Sonnetçš„å‡†ç¡®ç‡ä»…ä¸º55.19%ã€‚é”™è¯¯åˆ†ææ˜¾ç¤ºï¼Œæ¨¡å‹å¤±è´¥çš„ä¸€åŠä»¥ä¸Šæ˜¯ç”±äºå¤šæ­¥æ¨ç†é“¾çš„å´©æºƒï¼Œè€Œä¸æ˜¯ä¸»è¦çš„è§†è§‰ç‰¹å¾æå–é”™è¯¯ã€‚è¿™è¡¨æ˜å½“å‰çš„å¤šæ¨¡æ€æ¨¡å‹åœ¨å¤„ç†çœŸå®çš„å­¦æœ¯ç¯å¢ƒä¸­çš„å¤æ‚å›¾åƒåˆ†æä»»åŠ¡æ—¶ï¼Œå…¶æ¨ç†èƒ½åŠ›å­˜åœ¨å›ºæœ‰å±€é™ã€‚SCI-Reasonä¸ä»…æé«˜äº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œè€Œä¸”åœ¨VQAä»»åŠ¡ä¸­è¡¨ç°å‡ºè·¨é¢†åŸŸçš„æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬è¿˜æ¢è®¨äº†æ¨¡å‹æ¨ç†èƒ½åŠ›åœ¨æœªæ¥åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œå¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨å­¦æœ¯é¢†åŸŸçš„å¤æ‚å›¾åƒæ¨ç†èƒ½åŠ›å°šæœªå¾—åˆ°å……åˆ†ç ”ç©¶ã€‚</li>
<li>SCI-Reasonæ•°æ®é›†æ—¨åœ¨æµ‹è¯•å’Œæé«˜å¤§å‹å¤šæ¨¡æ€æ¨¡å‹åœ¨å­¦æœ¯é¢†åŸŸçš„å¤æ‚å›¾åƒæ¨ç†èƒ½åŠ›ï¼ŒåŒ…å«ä»PubMedæå–çš„çœŸå®å›¾åƒå’Œé—®ç­”å¯¹ã€‚</li>
<li>æœ€ä½³æ¨¡å‹åœ¨SCI-Reasonæ•°æ®é›†ä¸Šçš„å‡†ç¡®ç‡ä»…ä¸º55.19%ï¼Œè¡¨æ˜å½“å‰å¤šæ¨¡æ€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å­˜åœ¨å±€é™ã€‚</li>
<li>æ¨¡å‹å¤±è´¥çš„ä¸»è¦åŸå› åœ¨äºå¤šæ­¥æ¨ç†é“¾çš„å´©æºƒï¼Œè€Œéè§†è§‰ç‰¹å¾æå–é”™è¯¯ã€‚</li>
<li>SCI-Reasonæ•°æ®é›†ä¸ä»…æé«˜äº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œè¿˜è¡¨ç°å‡ºè·¨é¢†åŸŸçš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>æ¨¡å‹æ¨ç†èƒ½åŠ›åœ¨æœªæ¥åº”ç”¨ä¸­å…·æœ‰æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†å¤æ‚å›¾åƒåˆ†æä»»åŠ¡æ–¹é¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.06637">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a87446f7258bb5254212f075c1504c2a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8cbb071e8ef779a47ecff07a9ad0ccf2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fe85fb1d451b3a7e67712ad365c1e9ff.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2ce2624363c2714a7d844d5f5bdd1e86.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8cc60da6c28f44aa4fdab36cc9e2f1c5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d167f844c354099282512e07adf5703d.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Right-Prediction-Wrong-Reasoning-Uncovering-LLM-Misalignment-in-RA-Disease-Diagnosis"><a href="#Right-Prediction-Wrong-Reasoning-Uncovering-LLM-Misalignment-in-RA-Disease-Diagnosis" class="headerlink" title="Right Prediction, Wrong Reasoning: Uncovering LLM Misalignment in RA   Disease Diagnosis"></a>Right Prediction, Wrong Reasoning: Uncovering LLM Misalignment in RA   Disease Diagnosis</h2><p><strong>Authors:Umakanta Maharana, Sarthak Verma, Avarna Agarwal, Prakashini Mruthyunjaya, Dwarikanath Mahapatra, Sakir Ahmed, Murari Mandal</strong></p>
<p>Large language models (LLMs) offer a promising pre-screening tool, improving early disease detection and providing enhanced healthcare access for underprivileged communities. The early diagnosis of various diseases continues to be a significant challenge in healthcare, primarily due to the nonspecific nature of early symptoms, the shortage of expert medical practitioners, and the need for prolonged clinical evaluations, all of which can delay treatment and adversely affect patient outcomes. With impressive accuracy in prediction across a range of diseases, LLMs have the potential to revolutionize clinical pre-screening and decision-making for various medical conditions. In this work, we study the diagnostic capability of LLMs for Rheumatoid Arthritis (RA) with real world patients data. Patient data was collected alongside diagnoses from medical experts, and the performance of LLMs was evaluated in comparison to expert diagnoses for RA disease prediction. We notice an interesting pattern in disease diagnosis and find an unexpected \textit{misalignment between prediction and explanation}. We conduct a series of multi-round analyses using different LLM agents. The best-performing model accurately predicts rheumatoid arthritis (RA) diseases approximately 95% of the time. However, when medical experts evaluated the reasoning generated by the model, they found that nearly 68% of the reasoning was incorrect. This study highlights a clear misalignment between LLMs high prediction accuracy and its flawed reasoning, raising important questions about relying on LLM explanations in clinical settings. \textbf{LLMs provide incorrect reasoning to arrive at the correct answer for RA disease diagnosis.} </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºä¸€ç§æœ‰å‰é€”çš„é¢„ç­›æŸ¥å·¥å…·ï¼Œåœ¨æ”¹å–„æ—©æœŸç–¾ç—…æ£€æµ‹å’Œæé«˜ç‰¹æƒç¾¤ä½“çš„åŒ»ç–—ä¿å¥æœåŠ¡æ–¹é¢å‘æŒ¥ç€é‡è¦ä½œç”¨ã€‚æ—©æœŸç–¾ç—…çš„è¯Šæ–­ä»æ˜¯åŒ»ç–—ä¿å¥é¢†åŸŸçš„ä¸€ä¸ªå·¨å¤§æŒ‘æˆ˜ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºæ—©æœŸç—‡çŠ¶çš„éç‰¹å¼‚æ€§ã€ä¸“ä¸šåŒ»ç–—äººå‘˜çš„çŸ­ç¼ºä»¥åŠéœ€è¦é•¿æœŸçš„ä¸´åºŠè¯„ä¼°ï¼Œæ‰€æœ‰è¿™äº›éƒ½å¯èƒ½å»¶è¿Ÿæ²»ç–—å¹¶å¯¹æ‚£è€…ç»“æœäº§ç”Ÿä¸åˆ©å½±å“ã€‚åœ¨å„ç§ç–¾ç—…çš„é¢„æµ‹ä¸­å…·æœ‰ä»¤äººå°è±¡æ·±åˆ»çš„å‡†ç¡®æ€§ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹æœ‰æ½œåŠ›å½»åº•æ”¹å˜ä¸´åºŠé¢„ç­›æŸ¥å’Œå¤šç§åŒ»ç–—çŠ¶å†µçš„ä¸´åºŠå†³ç­–ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†å¤§å‹è¯­è¨€æ¨¡å‹å¯¹ç±»é£æ¹¿æ€§å…³èŠ‚ç‚ï¼ˆRAï¼‰çš„è¯Šæ–­èƒ½åŠ›ä»¥åŠä¸ç°å®ä¸–ç•Œæ‚£è€…æ•°æ®çš„å¯¹æ¯”ã€‚æˆ‘ä»¬æ”¶é›†äº†æ‚£è€…çš„æ•°æ®ä»¥åŠä¸åŒ»ç–—ä¸“å®¶çš„è¯Šæ–­ç»“æœç›¸æ¯”è¾ƒè¯„ä¼°äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚æˆ‘ä»¬å‘ç°äº†ä¸€ç§æœ‰è¶£çš„ç–¾ç—…è¯Šæ–­æ¨¡å¼ï¼Œå¹¶å‘ç°äº†é¢„æµ‹å’Œè§£é‡Šä¹‹é—´çš„æ„å¤–ä¸åŒ¹é…ã€‚æˆ‘ä»¬ä½¿ç”¨ä¸åŒçš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œäº†å¤šè½®åˆ†æã€‚è¡¨ç°æœ€ä½³çš„æ¨¡å‹èƒ½å‡†ç¡®é¢„æµ‹ç±»é£æ¹¿æ€§å…³èŠ‚ç‚ç–¾ç—…çº¦95%çš„æ—¶é—´ã€‚ç„¶è€Œï¼Œå½“åŒ»å­¦ä¸“å®¶è¯„ä¼°æ¨¡å‹äº§ç”Ÿçš„æ¨ç†æ—¶ï¼Œä»–ä»¬å‘ç°è¿‘6.è¢«æ¨¡å‹çš„æ¨ç†é”™è¯¯çš„é¢‘ç‡è¾¾åˆ°è¿‘ä¸€åŠçš„æ¯”ä¾‹çš„æƒ…å†µè®©æˆ‘ä»¬æ›´ç›´è§‚çš„å‘ç°ä¸€ä¸ªå¤§è¯­è¨€æ¨¡å‹çš„ç°çŠ¶é—®é¢˜å°±æ˜¯è¿‡äºç‰‡é¢è¿½ç»¼ç²¾ç¡®è¯Šæ–­ç¨‹åº¦å¿½è§†äº†é”™è¯¯çš„è§£é‡Šæ¦‚ç‡çš„é£é™©åœ¨RAçš„ç–¾ç—…è¯Šæ–­ä¸­ç¡®å®å­˜åœ¨ç€è¯¸å¤šç–‘é—®æ˜¯å¦èƒ½ä¿¡èµ–å¤§è¯­è¨€æ¨¡å‹çš„è§£é‡Šã€‚è¿™ç§æ¨¡å‹çš„è¯¯è¯Šå¯èƒ½æ€§å¯¹æˆ‘ä»¬æå‡ºäº†æ›´é«˜çš„è¦æ±‚å’Œè­¦ç¤ºâ€”â€”åœ¨è¿›è¡Œè¯Šæ–­è¿‡ç¨‹ä¸­åº”ç»¼åˆè€ƒè™‘å„ç§å› ç´ è€Œéè¿‡åˆ†ä¾èµ–æ¨¡å‹çš„è§£é‡Šç»“æœã€‚â€</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.06581v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç–¾ç—…æ—©æœŸæ£€æµ‹ä¸­æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ï¼Œæœ‰åŠ©äºæé«˜åŒ»ç–—ä¿å¥æœåŠ¡å¯åŠæ€§å¹¶ä¸ºç‰¹æƒè¾ƒå°‘çš„ç¤¾åŒºæä¾›æ›´å¥½çš„æœåŠ¡ã€‚ä»¥ç±»é£æ¹¿æ€§å…³èŠ‚ç‚ï¼ˆRAï¼‰ä¸ºä¾‹ï¼ŒLLMæ¨¡å‹çš„é¢„æµ‹å‡†ç¡®æ€§é«˜ï¼Œä½†ç”Ÿæˆçš„è§£é‡Šå´å­˜åœ¨é”™è¯¯ã€‚å°½ç®¡å®ƒä»¬èƒ½å¤Ÿå‡†ç¡®é¢„æµ‹ç–¾ç—…ï¼Œä½†è¿‘68%çš„è§£é‡Šè¢«åŒ»å­¦ä¸“å®¶è®¤å®šä¸ºä¸æ­£ç¡®ã€‚å› æ­¤ï¼Œåœ¨ä¾èµ–LLMè§£é‡Šè¿›è¡Œä¸´åºŠå†³ç­–æ—¶éœ€è°¨æ…ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯ç”¨äºæ—©æœŸç–¾ç—…æ£€æµ‹ï¼Œæé«˜åŒ»ç–—ä¿å¥æœåŠ¡å¯åŠæ€§ã€‚</li>
<li>åœ¨ç±»é£æ¹¿æ€§å…³èŠ‚ç‚ï¼ˆRAï¼‰çš„é¢„æµ‹ä¸­ï¼ŒLLMsè¡¨ç°å‡ºé«˜é¢„æµ‹å‡†ç¡®æ€§ã€‚</li>
<li>LLMsåœ¨è§£é‡Šè¯Šæ–­ç»“æœæ—¶å­˜åœ¨è¯¯åŒºï¼Œè¿‘68%çš„è§£é‡Šè¢«åŒ»å­¦ä¸“å®¶è®¤ä¸ºä¸æ­£ç¡®ã€‚</li>
<li>LLMsåœ¨ç–¾ç—…è¯Šæ–­ä¸­æœ‰æ—¶èƒ½å¤Ÿæä¾›æ­£ç¡®çš„ç­”æ¡ˆï¼Œä½†ç†ç”±å´æ˜¯é”™è¯¯çš„ã€‚</li>
<li>LLMsçš„é¢„æµ‹å‡†ç¡®æ€§å’Œè§£é‡Šèƒ½åŠ›ä¹‹é—´å­˜åœ¨ä¸åŒ¹é…ï¼Œè¿™æé†’æˆ‘ä»¬åœ¨ä¸´åºŠç¯å¢ƒä¸­ä¾èµ–LLMè§£é‡Šæ—¶éœ€è°¨æ…ã€‚</li>
<li>çœŸå®ä¸–ç•Œçš„æ‚£è€…æ•°æ®å¯¹äºè¯„ä¼°LLMsåœ¨ç–¾ç—…è¯Šæ–­ä¸­çš„æ€§èƒ½è‡³å…³é‡è¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.06581">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9c80f98f1b26473f13aa74a8103b93ff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1eeb57dfe47d2134bb29d18891bbb061.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2e23a783532b4a53e83a2c0e62d882d6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-155fd2d4e7ad4cf09889357327648b91.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Polygon-Symbolic-Reasoning-for-SQL-using-Conflict-Driven-Under-Approximation-Search"><a href="#Polygon-Symbolic-Reasoning-for-SQL-using-Conflict-Driven-Under-Approximation-Search" class="headerlink" title="Polygon: Symbolic Reasoning for SQL using Conflict-Driven   Under-Approximation Search"></a>Polygon: Symbolic Reasoning for SQL using Conflict-Driven   Under-Approximation Search</h2><p><strong>Authors:Pinhan Zhao, Yuepeng Wang, Xinyu Wang</strong></p>
<p>We present a novel symbolic reasoning engine for SQL which can efficiently generate an input $I$ for $n$ queries $P_1, \cdots, P_n$, such that their outputs on $I$ satisfy a given property (expressed in SMT). This is useful in different contexts, such as disproving equivalence of two SQL queries and disambiguating a set of queries. Our first idea is to reason about an under-approximation of each $P_i$ â€“ that is, a subset of $P_i$â€™s input-output behaviors. While it makes our approach both semantics-aware and lightweight, this idea alone is incomplete (as a fixed under-approximation might miss some behaviors of interest). Therefore, our second idea is to perform search over an expressive family of under-approximations (which collectively cover all program behaviors of interest), thereby making our approach complete. We have implemented these ideas in a tool, Polygon, and evaluated it on over 30,000 benchmarks across two tasks (namely, SQL equivalence refutation and query disambiguation). Our evaluation results show that Polygon significantly outperforms all prior techniques. </p>
<blockquote>
<p>æˆ‘ä»¬ä¸ºSQLè®¾è®¡äº†ä¸€ç§æ–°å‹ç¬¦å·æ¨ç†å¼•æ“ï¼Œè¯¥å¼•æ“å¯ä»¥é«˜æ•ˆåœ°ç”Ÿæˆé’ˆå¯¹nä¸ªæŸ¥è¯¢P1ï¼Œâ€¦ï¼ŒPnçš„è¾“å…¥Iï¼Œä½¿å¾—å®ƒä»¬åœ¨Iä¸Šçš„è¾“å‡ºæ»¡è¶³ç»™å®šå±æ€§ï¼ˆä»¥SMTè¡¨ç¤ºï¼‰ã€‚è¿™åœ¨ä¸åŒçš„ä¸Šä¸‹æ–‡ç¯å¢ƒä¸­éå¸¸æœ‰ç”¨ï¼Œä¾‹å¦‚è¯æ˜ä¸¤ä¸ªSQLæŸ¥è¯¢ä¸ç­‰ä»·ä»¥åŠæ¶ˆé™¤ä¸€ç»„æŸ¥è¯¢çš„æ­§ä¹‰ã€‚æˆ‘ä»¬çš„åˆæ­¥æƒ³æ³•æ˜¯æ¨ç†å‡ºæ¯ä¸ªPiçš„ä¸€ä¸ªä¸‹è¿‘ä¼¼å€¼â€”â€”å³Piè¾“å…¥-è¾“å‡ºè¡Œä¸ºçš„ä¸€ä¸ªå­é›†ã€‚è¿™æ—¢ä½¿æˆ‘ä»¬çš„æ–¹æ³•å…·æœ‰è¯­ä¹‰æ„ŸçŸ¥èƒ½åŠ›ï¼Œåˆä¿æŒå…¶è½»é‡åŒ–ï¼Œä½†ä»…ä½¿ç”¨è¿™ä¸€æƒ³æ³•æ˜¯ä¸å®Œæ•´çš„ï¼ˆå› ä¸ºå›ºå®šçš„ä¸‹è¿‘ä¼¼å€¼å¯èƒ½ä¼šé—æ¼ä¸€äº›æœ‰è¶£çš„è¡Œä¸ºï¼‰ã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„ç¬¬äºŒä¸ªæƒ³æ³•æ˜¯åœ¨è¡¨ç°åŠ›ä¸°å¯Œçš„ä¸‹è¿‘ä¼¼å€¼å®¶æ—ä¸Šè¿›è¡Œæœç´¢ï¼ˆè¿™äº›ä¸‹è¿‘ä¼¼å€¼å…±åŒæ¶µç›–äº†æ‰€æœ‰ç¨‹åºè¡Œä¸ºçš„æœ‰è¶£æ–¹é¢ï¼‰ï¼Œä»è€Œä½¿æˆ‘ä»¬çš„æ–¹æ³•å¾—ä»¥å®Œå–„ã€‚æˆ‘ä»¬å°†è¿™äº›æƒ³æ³•å®ç°ä¸ºPolygonå·¥å…·ï¼Œå¹¶åœ¨ä¸¤é¡¹ä»»åŠ¡ï¼ˆå³SQLç­‰ä»·åé©³å’ŒæŸ¥è¯¢æ¶ˆæ­§ï¼‰çš„è¶…è¿‡3ä¸‡ä¸ªåŸºå‡†æµ‹è¯•ä¸Šå¯¹å…¶è¿›è¡Œäº†è¯„ä¼°ã€‚æˆ‘ä»¬çš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒPolygonåœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºæ‰€æœ‰å…ˆå‰æŠ€æœ¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.06542v1">PDF</a> PLDI 2025</p>
<p><strong>Summary</strong></p>
<p>è¿™æ˜¯ä¸€ç§æ–°å‹çš„SQLç¬¦å·æ¨ç†å¼•æ“ï¼Œèƒ½é«˜æ•ˆåœ°ä¸ºnä¸ªæŸ¥è¯¢ç”Ÿæˆè¾“å…¥ï¼Œä½¿å®ƒä»¬åœ¨è¾“å…¥ä¸Šçš„è¾“å‡ºæ»¡è¶³ç»™å®šå±æ€§ï¼ˆä»¥SMTè¡¨è¾¾ï¼‰ã€‚è¿™åœ¨ä¸åŒæƒ…å¢ƒä¸‹éƒ½å¾ˆæœ‰ç”¨ï¼Œå¦‚è¯æ˜ä¸¤ä¸ªSQLæŸ¥è¯¢ä¸ç­‰ä»·å’Œæ¶ˆé™¤ä¸€ç»„æŸ¥è¯¢çš„æ­§ä¹‰ã€‚è¯¥å¼•æ“é¦–å…ˆä¼šå¯¹æ¯ä¸ªæŸ¥è¯¢è¿›è¡Œä¸‹è¿‘ä¼¼æ¨ç†ï¼Œå³æŸ¥è¯¢è¾“å…¥è¾“å‡ºçš„å­é›†è¡Œä¸ºã€‚è™½ç„¶è¿™ä½¿å¾—æˆ‘ä»¬çš„æ–¹æ³•æ—¢è¯­ä¹‰æ„ŸçŸ¥åˆè½»ä¾¿ï¼Œä½†ä»…ä¾é è¿™ç§ç†å¿µæ˜¯ä¸å®Œæ•´çš„ï¼Œå› ä¸ºä¸€ä¸ªå›ºå®šçš„ä¸‹è¿‘ä¼¼å¯èƒ½ä¼šé—æ¼ä¸€äº›æ„Ÿå…´è¶£çš„è¡Œä¸ºã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„ç¬¬äºŒä¸ªç†å¿µæ˜¯åœ¨ä¸€ä¸ªå¯Œæœ‰è¡¨ç°åŠ›çš„ä¸‹è¿‘ä¼¼é›†åˆä¸Šè¿›è¡Œæœç´¢ï¼ˆè¿™äº›é›†åˆæ¶µç›–äº†æ‰€æœ‰ç¨‹åºè¡Œä¸ºå…´è¶£ï¼‰ï¼Œä»è€Œä½¿æˆ‘ä»¬çš„æ–¹æ³•å®Œæ•´ã€‚æˆ‘ä»¬å·²å°†è¿™äº›ç†å¿µå®ç°ä¸ºå·¥å…·Polygonï¼Œå¹¶åœ¨è¶…è¿‡3ä¸‡ä¸ªåŸºå‡†æµ‹è¯•ä¸Šå¯¹å…¶è¿›è¡Œäº†è¯„ä¼°ï¼ŒåŒ…æ‹¬SQLç­‰ä»·åé©³å’ŒæŸ¥è¯¢å»æ­§ä¹‰ä¸¤ä¸ªä»»åŠ¡ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒPolygonæ˜¾è‘—ä¼˜äºæ‰€æœ‰å…ˆå‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–°å‹SQLç¬¦å·æ¨ç†å¼•æ“èƒ½é«˜æ•ˆå¤„ç†å¤šä¸ªæŸ¥è¯¢è¾“å…¥ï¼Œæ»¡è¶³ç»™å®šå±æ€§è¦æ±‚ã€‚</li>
<li>ä¸‹è¿‘ä¼¼æ¨ç†ç”¨äºå¤„ç†æ¯ä¸ªæŸ¥è¯¢çš„è¯­ä¹‰æ„ŸçŸ¥å’Œè½»ä¾¿å¤„ç†ã€‚</li>
<li>ä»…ä¾é ä¸‹è¿‘ä¼¼æ˜¯ä¸å®Œæ•´çš„ï¼Œå› ä¸ºå¯èƒ½ä¼šé—æ¼ä¸€äº›é‡è¦è¡Œä¸ºã€‚</li>
<li>é€šè¿‡æœç´¢ä¸€ç³»åˆ—è¡¨è¾¾æ€§ä¸‹è¿‘ä¼¼æ¥å®Œå–„æ–¹æ³•ï¼Œæ¶µç›–æ‰€æœ‰ç¨‹åºè¡Œä¸ºå…´è¶£ã€‚</li>
<li>è¯¥æŠ€æœ¯å®ç°ä¸ºå·¥å…·Polygonã€‚</li>
<li>Polygonåœ¨è¶…è¿‡3ä¸‡ä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œè¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.06542">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-042f1acad5fd773279b5e3a2037c8726.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-146a56b46c2608000d3238ce5985a96c.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Leanabell-Prover-Posttraining-Scaling-in-Formal-Reasoning"><a href="#Leanabell-Prover-Posttraining-Scaling-in-Formal-Reasoning" class="headerlink" title="Leanabell-Prover: Posttraining Scaling in Formal Reasoning"></a>Leanabell-Prover: Posttraining Scaling in Formal Reasoning</h2><p><strong>Authors:Jingyuan Zhang, Qi Wang, Xingguang Ji, Yahui Liu, Yang Yue, Fuzheng Zhang, Di Zhang, Guorui Zhou, Kun Gai</strong></p>
<p>Recent advances in automated theorem proving (ATP) through LLMs have highlighted the potential of formal reasoning with Lean 4 codes. However, ATP has not yet be revolutionized by the recent posttraining scaling as demonstrated by Open AI O1&#x2F;O3 and Deepseek R1. In this work, we investigate the entire posttraining of ATP, aiming to align it with breakthroughs in reasoning models in natural languages. To begin, we continual train current ATP models with a hybrid dataset, which consists of numerous statement-proof pairs, and additional data aimed at incorporating cognitive behaviors that emulate human reasoning and hypothesis refinement. Next, we explore reinforcement learning with the use of outcome reward returned by Lean 4 compiler. Through our designed continual training and reinforcement learning processes, we have successfully improved existing formal provers, including both DeepSeek-Prover-v1.5 and Goedel-Prover, achieving state-of-the-art performance in the field of whole-proof generation. For example, we achieve a 59.8% pass rate (pass@32) on MiniF2F. This is an on-going project and we will progressively update our findings, release our data and training details. </p>
<blockquote>
<p>è¿‘æœŸé€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªåŠ¨åŒ–å®šç†è¯æ˜ï¼ˆATPï¼‰æ–¹é¢çš„è¿›å±•çªæ˜¾äº†ä½¿ç”¨Lean 4ä»£ç è¿›è¡Œå½¢å¼æ¨ç†çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œæ­£å¦‚Open AI O1&#x2F;O3å’ŒDeepseek R1æ‰€å±•ç¤ºçš„ï¼ŒATPå¹¶æœªå› æœ€è¿‘çš„è®­ç»ƒåæ‰©å±•è€Œå®ç°é©å‘½æ€§çš„è¿›å±•ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†ATPçš„æ•´ä¸ªè®­ç»ƒåè¿‡ç¨‹ï¼Œæ—¨åœ¨ä¸è‡ªç„¶è¯­è¨€æ¨ç†æ¨¡å‹æ–¹é¢çš„çªç ´ä¿æŒä¸€è‡´ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä½¿ç”¨åŒ…å«ä¼—å¤šè¯­å¥è¯æ˜å¯¹çš„æ··åˆæ•°æ®é›†ï¼Œä»¥åŠæ—¨åœ¨èå…¥æ¨¡æ‹Ÿäººç±»æ¨ç†å’Œå‡è®¾ç²¾åŒ–çš„è®¤çŸ¥è¡Œä¸ºçš„å…¶ä»–æ•°æ®ï¼Œå¯¹å½“å‰ATPæ¨¡å‹è¿›è¡ŒæŒç»­è®­ç»ƒã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æ¢ç´¢äº†ä½¿ç”¨Lean 4ç¼–è¯‘å™¨æä¾›çš„æˆæœå¥–åŠ±æ¥å¼ºåŒ–å­¦ä¹ ã€‚é€šè¿‡æˆ‘ä»¬è®¾è®¡çš„æŒç»­è®­ç»ƒå’Œå¼ºåŒ–å­¦ä¹ è¿‡ç¨‹ï¼Œæˆ‘ä»¬æˆåŠŸæ”¹è¿›äº†ç°æœ‰çš„å½¢å¼è¯æ˜å™¨ï¼ŒåŒ…æ‹¬DeepSeek-Prover-v1.5å’ŒGoedel-Proverï¼Œå¹¶åœ¨æ•´ä¸ªè¯æ˜ç”Ÿæˆé¢†åŸŸå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬åœ¨MiniF2Fä¸Šè¾¾åˆ°äº†59.8%ï¼ˆpass@32ï¼‰çš„é€šè¿‡ç‡ã€‚è¿™æ˜¯ä¸€ä¸ªæ­£åœ¨è¿›è¡Œä¸­çš„é¡¹ç›®ï¼Œæˆ‘ä»¬å°†é€æ­¥æ›´æ–°æˆ‘ä»¬çš„å‘ç°ï¼Œå¹¶å‘å¸ƒæˆ‘ä»¬çš„æ•°æ®å’Œè®­ç»ƒç»†èŠ‚ã€‚</p>
</blockquote>
<hr>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.06122v2">PDF</a> 23 pages, 6 figures</p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸï¼Œè‡ªåŠ¨åŒ–å®šç†è¯æ˜ï¼ˆATPï¼‰é€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥çªæ˜¾äº†å…¶åœ¨å½¢å¼æ¨ç†æ–¹é¢çš„æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½¿ç”¨Lean 4ä»£ç æ–¹é¢ã€‚ç„¶è€Œï¼ŒATPå°šæœªå› Open AIçš„O1&#x2F;O3å’ŒDeepseek R1æ‰€å±•ç¤ºçš„è¿‘æœŸåè®­ç»ƒæ‰©å±•è€Œå®ç°é©å‘½æ€§å˜é©ã€‚æœ¬ç ”ç©¶æ—¨åœ¨å°†ATPçš„åè®­ç»ƒè¿‡ç¨‹ä¸çªç ´æ€§çš„è‡ªç„¶è¯­è¨€æ¨ç†æ¨¡å‹ç›¸ç»“åˆã€‚æˆ‘ä»¬é€šè¿‡æŒç»­è®­ç»ƒå½“å‰ATPæ¨¡å‹ï¼Œä½¿ç”¨åŒ…å«å¤§é‡è¯­å¥è¯æ˜å¯¹å’Œæ—¨åœ¨æ¨¡ä»¿äººç±»æ¨ç†å’Œå‡è®¾æ”¹è¿›çš„è®¤çŸ¥è¡Œä¸ºé™„åŠ æ•°æ®çš„æ··åˆæ•°æ®é›†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¢ç´¢äº†ä½¿ç”¨Lean 4ç¼–è¯‘å™¨æä¾›çš„ç»“æœå¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ã€‚é€šè¿‡æˆ‘ä»¬çš„æŒç»­è®­ç»ƒä¸å¼ºåŒ–å­¦ä¹ è®¾è®¡è¿‡ç¨‹ï¼Œæˆ‘ä»¬æˆåŠŸæ”¹è¿›äº†ç°æœ‰çš„å½¢å¼è¯æ˜å™¨ï¼ŒåŒ…æ‹¬DeepSeek-Prover-v1.5å’ŒGoedel-Proverï¼Œå¹¶åœ¨æ•´ä¸ªè¯æ˜ç”Ÿæˆé¢†åŸŸå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œåœ¨MiniF2Fä¸Šè¾¾åˆ°äº†59.8%ï¼ˆpass@32ï¼‰çš„é€šè¿‡ç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªåŠ¨åŒ–å®šç†è¯æ˜ï¼ˆATPï¼‰é€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥å±•ç¤ºäº†å½¢å¼æ¨ç†çš„æ½œåŠ›ã€‚</li>
<li>å½“å‰ç ”ç©¶é‡ç‚¹åœ¨äºå°†ATPçš„åè®­ç»ƒè¿‡ç¨‹ä¸è‡ªç„¶è¯­è¨€æ¨ç†æ¨¡å‹çš„çªç ´ç›¸ç»“åˆã€‚</li>
<li>é€šè¿‡æ··åˆæ•°æ®é›†å’Œå¼ºåŒ–å­¦ä¹ æŠ€æœ¯æ”¹è¿›äº†ç°æœ‰çš„å½¢å¼è¯æ˜å™¨æ€§èƒ½ã€‚</li>
<li>åœ¨æ•´ä¸ªè¯æ˜ç”Ÿæˆé¢†åŸŸå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä¾‹å¦‚ï¼Œåœ¨MiniF2Fä¸Šè¾¾åˆ°äº†è¾ƒé«˜çš„é€šè¿‡ç‡ã€‚</li>
<li>æ­¤ç ”ç©¶ä¸ºæŒç»­è¿›è¡Œä¸­çš„é¡¹ç›®ï¼Œå°†ä¸æ–­æ›´æ–°å…¶è¿›å±•ã€æ•°æ®å’Œè®­ç»ƒç»†èŠ‚ã€‚</li>
<li>ä½¿ç”¨Lean 4ä»£ç è¿›è¡Œæ¨ç†æ˜¯å…¶å–å¾—è¿›å±•çš„å…³é”®æŠ€æœ¯ä¹‹ä¸€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.06122">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7c8a7f401593127492d63c69a340d296.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-488ecde7aa50f655a60b9deeefcb47bb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-736ba68fa87823813c71a8b46b444929.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Algorithm-Discovery-With-LLMs-Evolutionary-Search-Meets-Reinforcement-Learning"><a href="#Algorithm-Discovery-With-LLMs-Evolutionary-Search-Meets-Reinforcement-Learning" class="headerlink" title="Algorithm Discovery With LLMs: Evolutionary Search Meets Reinforcement   Learning"></a>Algorithm Discovery With LLMs: Evolutionary Search Meets Reinforcement   Learning</h2><p><strong>Authors:Anja Surina, Amin Mansouri, Lars Quaedvlieg, Amal Seddas, Maryna Viazovska, Emmanuel Abbe, Caglar Gulcehre</strong></p>
<p>Discovering efficient algorithms for solving complex problems has been an outstanding challenge in mathematics and computer science, requiring substantial human expertise over the years. Recent advancements in evolutionary search with large language models (LLMs) have shown promise in accelerating the discovery of algorithms across various domains, particularly in mathematics and optimization. However, existing approaches treat the LLM as a static generator, missing the opportunity to update the model with the signal obtained from evolutionary exploration. In this work, we propose to augment LLM-based evolutionary search by continuously refining the search operator - the LLM - through reinforcement learning (RL) fine-tuning. Our method leverages evolutionary search as an exploration strategy to discover improved algorithms, while RL optimizes the LLM policy based on these discoveries. Our experiments on three combinatorial optimization tasks - bin packing, traveling salesman, and the flatpack problem - show that combining RL and evolutionary search improves discovery efficiency of improved algorithms, showcasing the potential of RL-enhanced evolutionary strategies to assist computer scientists and mathematicians for more efficient algorithm design. </p>
<blockquote>
<p>å‘ç°è§£å†³å¤æ‚é—®é¢˜çš„æœ‰æ•ˆç®—æ³•ä¸€ç›´æ˜¯æ•°å­¦å’Œè®¡ç®—æœºç§‘å­¦é¢†åŸŸçš„ä¸€é¡¹é‡å¤§æŒ‘æˆ˜ï¼Œéœ€è¦å¤šå¹´çš„äººç±»ä¸“ä¸šçŸ¥è¯†ã€‚æœ€è¿‘ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œè¿›åŒ–æœç´¢çš„è¿›æ­¥ä¸ºåŠ é€Ÿè·¨å¤šä¸ªé¢†åŸŸçš„ç®—æ³•å‘ç°æä¾›äº†å¸Œæœ›ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°å­¦å’Œä¼˜åŒ–æ–¹é¢ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å°†LLMè§†ä¸ºé™æ€ç”Ÿæˆå™¨ï¼Œé”™è¿‡äº†ä½¿ç”¨è¿›åŒ–æ¢ç´¢è·å¾—çš„ä¿¡å·æ›´æ–°æ¨¡å‹çš„æœºä¼šã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºé€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¾®è°ƒæ¥è¿ç»­ä¼˜åŒ–æœç´¢è¿ç®—ç¬¦â€”â€”LLMï¼Œä»¥å¢å¼ºåŸºäºLLMçš„è¿›åŒ–æœç´¢ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨è¿›åŒ–æœç´¢ä½œä¸ºæ¢ç´¢ç­–ç•¥æ¥å‘ç°æ”¹è¿›åçš„ç®—æ³•ï¼Œè€Œå¼ºåŒ–å­¦ä¹ åˆ™åŸºäºè¿™äº›å‘ç°ä¼˜åŒ–LLMç­–ç•¥ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªç»„åˆä¼˜åŒ–ä»»åŠ¡â€”â€”è£…ç®±é—®é¢˜ã€æ—…è¡Œæ¨é”€å‘˜é—®é¢˜å’Œå¹³é“ºé—®é¢˜ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œå°†å¼ºåŒ–å­¦ä¹ ä¸è¿›åŒ–æœç´¢ç›¸ç»“åˆï¼Œæé«˜äº†å‘ç°æ”¹è¿›ç®—æ³•çš„æ•ˆç‡ï¼Œå±•ç¤ºäº†å¼ºåŒ–å­¦ä¹ å¢å¼ºå‹è¿›åŒ–ç­–ç•¥åœ¨å¸®åŠ©è®¡ç®—æœºç§‘å­¦å®¶å’Œæ•°å­¦å®¶è¿›è¡Œæ›´é«˜æ•ˆç®—æ³•è®¾è®¡æ–¹é¢çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05108v2">PDF</a> 30 pages</p>
<p><strong>Summary</strong>ï¼šè¿‘æœŸè¿›åŒ–æœç´¢ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç»“åˆï¼Œä¸ºç®—æ³•å‘ç°æä¾›äº†æ–°çš„å¯èƒ½ã€‚ä½†ç°æœ‰æ–¹æ³•å°†å…¶è§†ä½œé™æ€ç”Ÿæˆå™¨ï¼Œå¿½è§†äº†ä»è¿›åŒ–æ¢ç´¢ä¸­è·å¾—ä¿¡å·æ¥æ›´æ–°æ¨¡å‹çš„æœºä¼šã€‚æœ¬ç ”ç©¶é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¾®è°ƒï¼Œä¸æ–­æ”¹å–„æœç´¢è¿ç®—ç¬¦LLMï¼Œå®ç°è¿›åŒ–æœç´¢ä¸RLä¼˜åŒ–ç®—æ³•çš„èåˆã€‚å®éªŒè¯æ˜ï¼Œè¯¥èåˆæ–¹æ³•åœ¨ä¸‰é¡¹ç»„åˆä¼˜åŒ–ä»»åŠ¡ä¸Šæé«˜äº†ç®—æ³•å‘ç°çš„æ•ˆç‡ï¼Œå±•ç°å‡ºåŠ©åŠ›è®¡ç®—æœºç§‘å­¦å®¶å’Œæ•°å­¦å®¶è¿›è¡Œæ›´é«˜æ•ˆç®—æ³•è®¾è®¡çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç®—æ³•å‘ç°ä¸­æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨æ•°å­¦å’Œä¼˜åŒ–é¢†åŸŸã€‚</li>
<li>å½“å‰LLMåœ¨è¿›åŒ–æœç´¢ä¸­çš„ä¸»è¦é™åˆ¶æ˜¯å°†å…¶ä½œä¸ºé™æ€ç”Ÿæˆå™¨ï¼Œç¼ºä¹æ ¹æ®è¿›åŒ–æ¢ç´¢æ›´æ–°æ¨¡å‹çš„èƒ½åŠ›ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¯ç”¨äºä¼˜åŒ–LLMç­–ç•¥ï¼ŒåŸºäºè¿›åŒ–æœç´¢å‘ç°çš„ç®—æ³•è¿›è¡Œå¾®è°ƒã€‚</li>
<li>ç»“åˆRLå’Œè¿›åŒ–æœç´¢æé«˜äº†ç®—æ³•å‘ç°çš„æ•ˆç‡ã€‚</li>
<li>åœ¨ä¸‰é¡¹ç»„åˆä¼˜åŒ–ä»»åŠ¡ï¼ˆbin packingã€traveling salesmanå’Œflatpacké—®é¢˜ï¼‰ä¸Šçš„å®éªŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>è¯¥æ–¹æ³•å…·æœ‰æ½œåŠ›ååŠ©è®¡ç®—æœºç§‘å­¦å®¶å’Œæ•°å­¦å®¶è¿›è¡Œæ›´é«˜æ•ˆçš„ç®—æ³•è®¾è®¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05108">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-03a0422f6d31d473742ed1ad77a84804.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5c5cdaad98384c81391c81f53987a5ba.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c4c088176936a53f67b25fe0509fd8ba.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Robust-Reinforcement-Learning-from-Human-Feedback-for-Large-Language-Models-Fine-Tuning"><a href="#Robust-Reinforcement-Learning-from-Human-Feedback-for-Large-Language-Models-Fine-Tuning" class="headerlink" title="Robust Reinforcement Learning from Human Feedback for Large Language   Models Fine-Tuning"></a>Robust Reinforcement Learning from Human Feedback for Large Language   Models Fine-Tuning</h2><p><strong>Authors:Kai Ye, Hongyi Zhou, Jin Zhu, Francesco Quinzan, Chengchung Shi</strong></p>
<p>Reinforcement learning from human feedback (RLHF) has emerged as a key technique for aligning the output of large language models (LLMs) with human preferences. To learn the reward function, most existing RLHF algorithms use the Bradley-Terry model, which relies on assumptions about human preferences that may not reflect the complexity and variability of real-world judgments. In this paper, we propose a robust algorithm to enhance the performance of existing approaches under such reward model misspecifications. Theoretically, our algorithm reduces the variance of reward and policy estimators, leading to improved regret bounds. Empirical evaluations on LLM benchmark datasets demonstrate that the proposed algorithm consistently outperforms existing methods, with 77-81% of responses being favored over baselines on the Anthropic Helpful and Harmless dataset. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰å·²ç»æˆä¸ºä¸€ç§å…³é”®æŠ€æœ¯ï¼Œç”¨äºå°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¾“å‡ºä¸äººç±»åå¥½å¯¹é½ã€‚ä¸ºäº†å­¦ä¹ å¥–åŠ±å‡½æ•°ï¼Œå¤§å¤šæ•°ç°æœ‰çš„RLHFç®—æ³•ä½¿ç”¨Bradley-Terryæ¨¡å‹ï¼Œè¯¥æ¨¡å‹ä¾èµ–äºå…³äºäººç±»åå¥½çš„å‡è®¾ï¼Œè¿™äº›å‡è®¾å¯èƒ½æ— æ³•åæ˜ ç°å®ä¸–ç•Œåˆ¤æ–­ä¸­çš„å¤æ‚æ€§å’Œå¯å˜æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é²æ£’æ€§ç®—æ³•ï¼Œä»¥æé«˜åœ¨å½“å‰å¥–åŠ±æ¨¡å‹è¯¯å·®ä¸‹æ–¹æ³•çš„æ€§èƒ½ã€‚ä»ç†è®ºä¸Šè®²ï¼Œæˆ‘ä»¬çš„ç®—æ³•å‡å°‘äº†å¥–åŠ±å’Œæ”¿ç­–ä¼°è®¡é‡çš„æ–¹å·®ï¼Œä»è€Œæé«˜äº†åæ‚”è¾¹ç•Œã€‚åœ¨LLMåŸºå‡†æ•°æ®é›†ä¸Šçš„ç»éªŒè¯„ä¼°è¡¨æ˜ï¼Œæ‰€æå‡ºçš„ç®—æ³•å§‹ç»ˆä¼˜äºç°æœ‰æ–¹æ³•ï¼Œåœ¨Anthropicæœ‰ç›Šå’Œæ— å®³æ•°æ®é›†ä¸Šï¼Œæœ‰77%-81%çš„å“åº”ä¼˜äºåŸºçº¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.03784v2">PDF</a> </p>
<p><strong>Summary</strong>ï¼šå¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰å·²æˆä¸ºå°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¾“å‡ºä¸äººç±»åå¥½å¯¹é½çš„å…³é”®æŠ€æœ¯ã€‚ç°æœ‰å¤§å¤šæ•°RLHFç®—æ³•ä½¿ç”¨Bradley-Terryæ¨¡å‹æ¥å­¦ä¹ å¥–åŠ±å‡½æ•°ï¼Œè¯¥æ¨¡å‹ä¾èµ–äºå¯èƒ½æ— æ³•åæ˜ ç°å®ä¸–ç•Œåˆ¤æ–­å¤æ‚æ€§å’Œå¯å˜æ€§çš„å‡è®¾ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§å¢å¼ºç®—æ³•ä»¥æé«˜åœ¨å¥–åŠ±æ¨¡å‹è¯¯æŒ‡å®šä¸‹çš„æ€§èƒ½ã€‚ç†è®ºä¸Šï¼Œè¯¥ç®—æ³•é™ä½äº†å¥–åŠ±å’Œæ”¿ç­–ä¼°è®¡é‡çš„æ–¹å·®ï¼Œä»è€Œæé«˜äº†åæ‚”ç•Œé™ã€‚åœ¨LLMåŸºå‡†æ•°æ®é›†ä¸Šçš„å®è¯è¯„ä¼°è¡¨æ˜ï¼Œæ‰€æå‡ºçš„ç®—æ³•å§‹ç»ˆä¼˜äºç°æœ‰æ–¹æ³•ï¼Œåœ¨Anthropic Helpfulå’ŒHarmlessæ•°æ®é›†ä¸Šï¼Œæœ‰77-81%çš„å“åº”ä¼˜äºåŸºçº¿ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰æŠ€æœ¯ç”¨äºå¯¹é½å¤§å‹è¯­è¨€æ¨¡å‹è¾“å‡ºä¸äººç±»åå¥½ã€‚</li>
<li>ç°æœ‰RLHFç®—æ³•ä¸»è¦ä½¿ç”¨Bradley-Terryæ¨¡å‹ï¼Œå­˜åœ¨å¯¹ç°å®ä¸–ç•Œåˆ¤æ–­çš„å¤æ‚æ€§å’Œå¯å˜æ€§çš„å‡è®¾ä¸è¶³çš„é—®é¢˜ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§å¢å¼ºç®—æ³•ä»¥æé«˜åœ¨å¥–åŠ±æ¨¡å‹è¯¯æŒ‡å®šä¸‹çš„æ€§èƒ½ã€‚</li>
<li>è¯¥ç®—æ³•ç†è®ºä¸Šé™ä½äº†å¥–åŠ±å’Œæ”¿ç­–ä¼°è®¡é‡çš„æ–¹å·®ï¼Œæé«˜äº†åæ‚”ç•Œé™ã€‚</li>
<li>åœ¨LLMåŸºå‡†æ•°æ®é›†ä¸Šçš„å®è¯è¯„ä¼°è¡¨æ˜è¯¥ç®—æ³•æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>åœ¨Anthropic Helpfulå’ŒHarmlessæ•°æ®é›†ä¸Šï¼Œè¯¥ç®—æ³•çš„å“åº”ä¼˜äºåŸºçº¿ï¼Œæ¯”ä¾‹ä¸º77-81%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.03784">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-7de72d083afb502b20bf07556986755f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7f9b51ef57f8058197bf40683aa875f1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cce2ff1466f9e978a0ab1daaf67a66a3.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="F5R-TTS-Improving-Flow-Matching-based-Text-to-Speech-with-Group-Relative-Policy-Optimization"><a href="#F5R-TTS-Improving-Flow-Matching-based-Text-to-Speech-with-Group-Relative-Policy-Optimization" class="headerlink" title="F5R-TTS: Improving Flow-Matching based Text-to-Speech with Group   Relative Policy Optimization"></a>F5R-TTS: Improving Flow-Matching based Text-to-Speech with Group   Relative Policy Optimization</h2><p><strong>Authors:Xiaohui Sun, Ruitong Xiao, Jianye Mo, Bowen Wu, Qun Yu, Baoxun Wang</strong></p>
<p>We present F5R-TTS, a novel text-to-speech (TTS) system that integrates Gradient Reward Policy Optimization (GRPO) into a flow-matching based architecture. By reformulating the deterministic outputs of flow-matching TTS into probabilistic Gaussian distributions, our approach enables seamless integration of reinforcement learning algorithms. During pretraining, we train a probabilistically reformulated flow-matching based model which is derived from F5-TTS with an open-source dataset. In the subsequent reinforcement learning (RL) phase, we employ a GRPO-driven enhancement stage that leverages dual reward metrics: word error rate (WER) computed via automatic speech recognition and speaker similarity (SIM) assessed by verification models. Experimental results on zero-shot voice cloning demonstrate that F5R-TTS achieves significant improvements in both speech intelligibility (a 29.5% relative reduction in WER) and speaker similarity (a 4.6% relative increase in SIM score) compared to conventional flow-matching based TTS systems. Audio samples are available at <a target="_blank" rel="noopener" href="https://frontierlabs.github.io/F5R">https://frontierlabs.github.io/F5R</a>. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†F5R-TTSï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿï¼Œå®ƒå°†æ¢¯åº¦å¥–åŠ±ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰é›†æˆåˆ°åŸºäºæµåŒ¹é…çš„æ¶æ„ä¸­ã€‚æˆ‘ä»¬é€šè¿‡å°†åŸºäºæµåŒ¹é…çš„TTSçš„ç¡®å®šæ€§è¾“å‡ºé‡æ–°è¡¨è¿°ä¸ºæ¦‚ç‡é«˜æ–¯åˆ†å¸ƒï¼Œä½¿æˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿå®ç°å¼ºåŒ–å­¦ä¹ ç®—æ³•çš„æ— ç¼é›†æˆã€‚åœ¨é¢„è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨å…¬å¼€æ•°æ®é›†å¯¹æ¥è‡ªF5-TTSçš„åŸºäºæµåŒ¹é…çš„æ¦‚ç‡é‡æ„æ¨¡å‹è¿›è¡Œè®­ç»ƒã€‚åœ¨éšåçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰é˜¶æ®µï¼Œæˆ‘ä»¬é‡‡ç”¨GRPOé©±åŠ¨çš„å¢å¼ºé˜¶æ®µï¼Œåˆ©ç”¨åŒé‡å¥–åŠ±æŒ‡æ ‡ï¼šé€šè¿‡è‡ªåŠ¨è¯­éŸ³è¯†åˆ«è®¡ç®—çš„è¯é”™è¯¯ç‡ï¼ˆWERï¼‰å’Œé€šè¿‡éªŒè¯æ¨¡å‹è¯„ä¼°çš„è¯´è¯äººç›¸ä¼¼æ€§ï¼ˆSIMï¼‰ã€‚åœ¨é›¶æ ·æœ¬è¯­éŸ³å…‹éš†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ä¼ ç»Ÿçš„åŸºäºæµåŒ¹é…çš„TTSç³»ç»Ÿç›¸æ¯”ï¼ŒF5R-TTSåœ¨è¯­éŸ³æ¸…æ™°åº¦ï¼ˆç›¸å¯¹é™ä½29.5%çš„WERï¼‰å’Œè¯´è¯äººç›¸ä¼¼æ€§ï¼ˆSIMå¾—åˆ†ç›¸å¯¹æé«˜4.6%ï¼‰æ–¹é¢å–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚éŸ³é¢‘æ ·æœ¬å¯åœ¨<a target="_blank" rel="noopener" href="https://frontierlabs.github.io/F5R%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://frontierlabs.github.io/F5Rä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.02407v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>F5R-TTSæ˜¯ä¸€ä¸ªé›†æˆæ¢¯åº¦å¥–åŠ±ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰çš„åŸºäºæµåŒ¹é…æ¶æ„çš„æ–°å‹æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿã€‚å®ƒé€šè¿‡æ¦‚ç‡åŒ–é‡æ„æµåŒ¹é…TTSçš„è¾“å‡ºå¹¶å®ç°ä¸å¼ºåŒ–å­¦ä¹ ç®—æ³•çš„æ— ç¼é›†æˆã€‚åœ¨é¢„è®­ç»ƒé˜¶æ®µï¼Œä½¿ç”¨å…¬å¼€æ•°æ®é›†è®­ç»ƒæ¦‚ç‡é‡æ„çš„æµåŒ¹é…æ¨¡å‹ã€‚åœ¨éšåçš„å¼ºåŒ–å­¦ä¹ é˜¶æ®µï¼Œé‡‡ç”¨GRPOé©±åŠ¨çš„å¢å¼ºé˜¶æ®µï¼Œåˆ©ç”¨åŒé‡å¥–åŠ±æŒ‡æ ‡ï¼ˆé€šè¿‡è‡ªåŠ¨è¯­éŸ³è¯†åˆ«è®¡ç®—çš„è¯é”™è¯¯ç‡å’Œé€šè¿‡éªŒè¯æ¨¡å‹è¯„ä¼°çš„è¯´è¯äººç›¸ä¼¼æ€§ï¼‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒF5R-TTSåœ¨é›¶æ ·æœ¬è¯­éŸ³å…‹éš†ä¸­å®ç°äº†è¯­éŸ³æ¸…æ™°åº¦å’Œè¯´è¯äººç›¸ä¼¼æ€§çš„æ˜¾è‘—æé«˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>F5R-TTSæ˜¯ä¸€ä¸ªæ–°å‹æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿï¼Œé›†æˆæ¢¯åº¦å¥–åŠ±ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ã€‚</li>
<li>F5R-TTSå°†ç¡®å®šæ€§è¾“å‡ºè½¬åŒ–ä¸ºæ¦‚ç‡é«˜æ–¯åˆ†å¸ƒä»¥å®ç°ä¸å¼ºåŒ–å­¦ä¹ ç®—æ³•çš„æ— ç¼é›†æˆã€‚</li>
<li>åœ¨é¢„è®­ç»ƒé˜¶æ®µï¼Œä½¿ç”¨å…¬å¼€æ•°æ®é›†è®­ç»ƒæ¦‚ç‡é‡æ„çš„æµåŒ¹é…æ¨¡å‹ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ é˜¶æ®µé‡‡ç”¨GRPOé©±åŠ¨çš„å¢å¼ºé˜¶æ®µã€‚</li>
<li>F5R-TTSé‡‡ç”¨åŒé‡å¥–åŠ±æŒ‡æ ‡ï¼šè¯é”™è¯¯ç‡ï¼ˆWERï¼‰å’Œè¯´è¯äººç›¸ä¼¼æ€§ï¼ˆSIMï¼‰ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒF5R-TTSåœ¨é›¶æ ·æœ¬è¯­éŸ³å…‹éš†ä¸­æ˜¾è‘—æé«˜è¯­éŸ³æ¸…æ™°åº¦å’Œè¯´è¯äººç›¸ä¼¼æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.02407">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-60cc5601f6dcdfdb07b95743b66a5172.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7fadc489444d8b07a9c1126e55aec6bf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9219b3f94034443a15eaf5a1e73ff82b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-23ad0ad70d5d46832db0d11e5dfb6ea0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d12144657fdb0de65af673114315066a.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="DeepSeek-V3-GPT-4-Phi-4-and-LLaMA-3-3-generate-correct-code-for-LoRaWAN-related-engineering-tasks"><a href="#DeepSeek-V3-GPT-4-Phi-4-and-LLaMA-3-3-generate-correct-code-for-LoRaWAN-related-engineering-tasks" class="headerlink" title="DeepSeek-V3, GPT-4, Phi-4, and LLaMA-3.3 generate correct code for   LoRaWAN-related engineering tasks"></a>DeepSeek-V3, GPT-4, Phi-4, and LLaMA-3.3 generate correct code for   LoRaWAN-related engineering tasks</h2><p><strong>Authors:Daniel Fernandes, JoÃ£o P. Matos-Carvalho, Carlos M. Fernandes, Nuno Fachada</strong></p>
<p>This paper investigates the performance of 16 Large Language Models (LLMs) in automating LoRaWAN-related engineering tasks involving optimal placement of drones and received power calculation under progressively complex zero-shot, natural language prompts. The primary research question is whether lightweight, locally executed LLMs can generate correct Python code for these tasks. To assess this, we compared locally run models against state-of-the-art alternatives, such as GPT-4 and DeepSeek-V3, which served as reference points. By extracting and executing the Python functions generated by each model, we evaluated their outputs on a zero-to-five scale. Results show that while DeepSeek-V3 and GPT-4 consistently provided accurate solutions, certain smaller models â€“ particularly Phi-4 and LLaMA-3.3 â€“ also demonstrated strong performance, underscoring the viability of lightweight alternatives. Other models exhibited errors stemming from incomplete understanding or syntactic issues. These findings illustrate the potential of LLM-based approaches for specialized engineering applications while highlighting the need for careful model selection, rigorous prompt design, and targeted domain fine-tuning to achieve reliable outcomes. </p>
<blockquote>
<p>æœ¬æ–‡ç ”ç©¶äº†16ç§å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªåŠ¨åŒ–LoRaWANç›¸å…³å·¥ç¨‹ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œè¿™äº›ä»»åŠ¡æ¶‰åŠæ— äººæœºçš„æœ€ä½³æ”¾ç½®ä½ç½®ä»¥åŠåœ¨é€æ¸å¤æ‚çš„é›¶æ ·æœ¬è‡ªç„¶è¯­è¨€æç¤ºä¸‹æ¥æ”¶åŠŸç‡çš„è®¡ç®—ã€‚ä¸»è¦çš„ç ”ç©¶é—®é¢˜æ˜¯ï¼Œè½»é‡çº§çš„æœ¬åœ°æ‰§è¡ŒLLMæ˜¯å¦èƒ½å¤Ÿç”Ÿæˆå®Œæˆè¿™äº›ä»»åŠ¡çš„æ­£ç¡®Pythonä»£ç ã€‚ä¸ºäº†è¯„ä¼°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å°†æœ¬åœ°è¿è¡Œæ¨¡å‹ä¸æœ€æ–°æŠ€æœ¯æ›¿ä»£æ–¹æ¡ˆï¼ˆå¦‚GPT-4å’ŒDeepSeek-V3ï¼‰è¿›è¡Œäº†æ¯”è¾ƒï¼Œåè€…ä½œä¸ºå‚è€ƒç‚¹ã€‚é€šè¿‡æå–å¹¶æ‰§è¡Œæ¯ä¸ªæ¨¡å‹ç”Ÿæˆçš„Pythonå‡½æ•°ï¼Œæˆ‘ä»¬å¯¹å®ƒä»¬çš„è¾“å‡ºè¿›è¡Œäº†é›¶åˆ°äº”çº§çš„è¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼Œè™½ç„¶DeepSeek-V3å’ŒGPT-4å§‹ç»ˆæä¾›å‡†ç¡®çš„è§£å†³æ–¹æ¡ˆï¼Œä½†æŸäº›è¾ƒå°çš„æ¨¡å‹ï¼ˆç‰¹åˆ«æ˜¯Phi-4å’ŒLLaMA-3.3ï¼‰ä¹Ÿè¡¨ç°å‡ºå¼ºåŠ²çš„æ€§èƒ½ï¼Œè¯æ˜äº†è½»é‡çº§æ›¿ä»£æ–¹æ¡ˆçš„å¯è¡Œæ€§ã€‚å…¶ä»–æ¨¡å‹å‡ºç°çš„é”™è¯¯æºäºç†è§£ä¸å…¨é¢æˆ–å¥æ³•é—®é¢˜ã€‚è¿™äº›å‘ç°è¡¨æ˜äº†åŸºäºLLMçš„æ–¹æ³•åœ¨ç‰¹å®šå·¥ç¨‹åº”ç”¨ä¸­çš„æ½œåŠ›ï¼ŒåŒæ—¶ä¹Ÿå¼ºè°ƒäº†å®ç°å¯é ç»“æœéœ€è¦ä»”ç»†é€‰æ‹©æ¨¡å‹ã€ä¸¥æ ¼è®¾è®¡æç¤ºä»¥åŠæœ‰é’ˆå¯¹æ€§çš„é¢†åŸŸå¾®è°ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.14926v3">PDF</a> The peer-reviewed version of this paper is published in Electronics   at <a target="_blank" rel="noopener" href="https://doi.org/10.3390/electronics14071428">https://doi.org/10.3390/electronics14071428</a>. This version is typeset by   the authors and differs only in pagination and typographical detail</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†16ç§å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªåŠ¨åŒ–LoRaWANç›¸å…³çš„å·¥ç¨‹ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼ŒåŒ…æ‹¬æ— äººæœºçš„æœ€ä½³æ”¾ç½®å’Œæ¥æ”¶åŠŸç‡è®¡ç®—çš„é›¶æ ·æœ¬è‡ªç„¶è¯­è¨€æç¤ºã€‚ç ”ç©¶çš„æ ¸å¿ƒé—®é¢˜æ˜¯ï¼Œè½»é‡çº§çš„æœ¬åœ°æ‰§è¡ŒLLMsæ˜¯å¦èƒ½ç”Ÿæˆè¿™äº›ä»»åŠ¡çš„æ­£ç¡®Pythonä»£ç ã€‚é€šè¿‡å¯¹æ¯”æœ¬åœ°æ¨¡å‹ä¸GPT-4å’ŒDeepSeek-V3ç­‰å‰æ²¿æ¨¡å‹çš„è¡¨ç°ï¼Œå‘ç°DeepSeek-V3å’ŒGPT-4æä¾›äº†ä¸€è‡´çš„å‡†ç¡®è§£å†³æ–¹æ¡ˆï¼Œè€ŒæŸäº›å°å‹æ¨¡å‹å¦‚Phi-4å’ŒLLaMA-3.3ä¹Ÿè¡¨ç°å‡ºå¼ºåŠ²æ€§èƒ½ã€‚å…¶ä»–æ¨¡å‹åˆ™å› ç†è§£ä¸å…¨æˆ–å¥æ³•é—®é¢˜è€ŒçŠ¯é”™ã€‚ç ”ç©¶è¡¨æ˜LLMåœ¨ç‰¹å®šå·¥ç¨‹åº”ç”¨ä¸­æœ‰æ½œåŠ›ï¼Œä½†éœ€æ…é‡é€‰æ‹©æ¨¡å‹ã€ç²¾å¿ƒè®¾è®¡æç¤ºåŠé’ˆå¯¹æ€§çš„é¢†åŸŸå¾®è°ƒä»¥ç¡®ä¿å¯é ç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>16ç§å¤§å‹è¯­è¨€æ¨¡å‹åœ¨LoRaWANç›¸å…³å·¥ç¨‹ä»»åŠ¡ä¸­çš„æ€§èƒ½è¢«è¯„ä¼°ã€‚</li>
<li>ç ”ç©¶é‡ç‚¹ä¸ºè½»é‡çº§æœ¬åœ°æ‰§è¡Œçš„å¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¦ç”Ÿæˆæ­£ç¡®çš„Pythonä»£ç ã€‚</li>
<li>GPT-4å’ŒDeepSeek-V3ä½œä¸ºå‰æ²¿æ¨¡å‹ï¼Œè¡¨ç°ä¼˜ç§€ä¸”æä¾›äº†å‡†ç¡®çš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>éƒ¨åˆ†å°å‹æ¨¡å‹å¦‚Phi-4å’ŒLLaMA-3.3åœ¨ç ”ç©¶ä¸­å±•ç¤ºäº†å¼ºåŠ²æ€§èƒ½ã€‚</li>
<li>å…¶ä»–æ¨¡å‹å› ç†è§£ä¸å…¨æˆ–å¥æ³•é—®é¢˜è€ŒçŠ¯é”™ï¼Œè¿™å¼ºè°ƒäº†æ¨¡å‹é€‰æ‹©çš„é‡è¦æ€§ã€‚</li>
<li>ç ”ç©¶è¡¨æ˜LLMåœ¨ç‰¹å®šå·¥ç¨‹åº”ç”¨ä¸­å…·æœ‰æ½œåŠ›ï¼Œä½†æˆåŠŸçš„å®ç°éœ€è¦å¤šæ–¹é¢çš„è€ƒé‡å¦‚æ¨¡å‹é€‰æ‹©ã€æç¤ºè®¾è®¡å’Œé¢†åŸŸå¾®è°ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.14926">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-55890d0bb86a03ad5ec08f55522290bc.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="LogiDynamics-Unraveling-the-Dynamics-of-Logical-Inference-in-Large-Language-Model-Reasoning"><a href="#LogiDynamics-Unraveling-the-Dynamics-of-Logical-Inference-in-Large-Language-Model-Reasoning" class="headerlink" title="LogiDynamics: Unraveling the Dynamics of Logical Inference in Large   Language Model Reasoning"></a>LogiDynamics: Unraveling the Dynamics of Logical Inference in Large   Language Model Reasoning</h2><p><strong>Authors:Tianshi Zheng, Jiayang Cheng, Chunyang Li, Haochen Shi, Zihao Wang, Jiaxin Bai, Yangqiu Song, Ginny Y. Wong, Simon See</strong></p>
<p>Modern large language models (LLMs) employ various forms of logical inference, both implicitly and explicitly, when addressing reasoning tasks. Understanding how to optimally leverage these inference paradigms is critical for advancing LLMsâ€™ reasoning capabilities. This paper adopts an exploratory approach by introducing a controlled evaluation environment for analogical reasoning â€“ a fundamental cognitive task â€“ that is systematically parameterized across three dimensions: modality (textual, visual, symbolic), difficulty (easy, medium, hard), and task format (multiple-choice or free-text generation). We analyze the comparative dynamics of inductive, abductive, and deductive inference pipelines across these dimensions, and demonstrate that our findings generalize to broader in-context learning tasks. Additionally, we investigate advanced paradigms such as hypothesis selection, verification, and refinement, revealing their potential to scale up logical inference in LLM reasoning. This exploratory study provides a foundation for future research in enhancing LLM reasoning through systematic logical inference strategies. Resources are available at <a target="_blank" rel="noopener" href="https://github.com/HKUST-KnowComp/LogiDynamics">https://github.com/HKUST-KnowComp/LogiDynamics</a>. </p>
<blockquote>
<p>ç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è§£å†³æ¨ç†ä»»åŠ¡æ—¶ï¼Œä¼šéšå¼å’Œæ˜¾å¼åœ°ä½¿ç”¨å„ç§å½¢å¼çš„é€»è¾‘æ¨ç†ã€‚äº†è§£å¦‚ä½•æœ€ä¼˜åœ°åˆ©ç”¨è¿™äº›æ¨ç†èŒƒå¼æ˜¯æå‡LLMæ¨ç†èƒ½åŠ›çš„å…³é”®ã€‚æœ¬æ–‡é‡‡ç”¨ä¸€ç§æ¢ç´¢æ€§æ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥ç±»æ¯”æ¨ç†çš„æ§åˆ¶è¯„ä¼°ç¯å¢ƒæ¥è¿›è¡Œç ”ç©¶â€”â€”è¿™æ˜¯ä¸€ç§åŸºæœ¬çš„è®¤çŸ¥ä»»åŠ¡ï¼Œç³»ç»Ÿæ€§åœ°å‚æ•°åŒ–æ¶‰åŠä¸‰ä¸ªç»´åº¦ï¼šæ¨¡æ€ï¼ˆæ–‡æœ¬ã€è§†è§‰ã€ç¬¦å·ï¼‰ã€éš¾åº¦ï¼ˆå®¹æ˜“ã€ä¸­ç­‰ã€å›°éš¾ï¼‰å’Œä»»åŠ¡æ ¼å¼ï¼ˆå¤šé¡¹é€‰æ‹©æˆ–è‡ªç”±æ–‡æœ¬ç”Ÿæˆï¼‰ã€‚æˆ‘ä»¬åˆ†æäº†å½’çº³ã€å‡è®¾å’Œæ¼”ç»æ¨ç†ç®¡çº¿åœ¨è¿™äº›ç»´åº¦ä¸Šçš„æ¯”è¾ƒåŠ¨æ€ï¼Œå¹¶è¯æ˜æˆ‘ä»¬çš„å‘ç°å¯ä»¥æ¨å¹¿åˆ°æ›´å¹¿æ³›çš„ä¸Šä¸‹æ–‡å­¦ä¹ ä»»åŠ¡ä¸­ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ç ”ç©¶äº†å‡è®¾é€‰æ‹©ã€éªŒè¯å’Œç»†åŒ–ç­‰é«˜çº§èŒƒå¼ï¼Œæ­ç¤ºäº†å®ƒä»¬åœ¨æ‰©å¤§LLMæ¨ç†ä¸­çš„é€»è¾‘æ¨æ–­æ–¹é¢çš„æ½œåŠ›ã€‚è¿™é¡¹æ¢ç´¢æ€§ç ”ç©¶ä¸ºæœªæ¥é€šè¿‡ç³»ç»Ÿé€»è¾‘æ¨ç†ç­–ç•¥æé«˜LLMæ¨ç†èƒ½åŠ›çš„ç ”ç©¶å¥ å®šäº†åŸºç¡€ã€‚ç›¸å…³èµ„æºå¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/HKUST-KnowComp/LogiDynamics%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/HKUST-KnowComp/LogiDynamicsä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.11176v2">PDF</a> 21 pages</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤„ç†æ¨ç†ä»»åŠ¡æ—¶ï¼Œä¼šé‡‡ç”¨å„ç§å½¢å¼çš„é€»è¾‘æ¨ç†ï¼ŒåŒ…æ‹¬éšæ€§å’Œæ˜¾æ€§æ¨ç†ã€‚ç†è§£å¦‚ä½•æœ€ä¼˜åœ°åˆ©ç”¨è¿™äº›æ¨ç†æ¨¡å¼å¯¹äºæå‡LLMçš„æ¨ç†èƒ½åŠ›è‡³å…³é‡è¦ã€‚æœ¬æ–‡é‡‡ç”¨äº†ä¸€ç§æ¢ç´¢æ€§æ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥ä¸€ä¸ªå¯æ§çš„è¯„ä¼°ç¯å¢ƒï¼Œå¯¹ç±»æ¯”æ¨ç†è¿™ä¸€åŸºæœ¬è®¤çŸ¥ä»»åŠ¡è¿›è¡Œäº†ç³»ç»ŸåŒ–å‚æ•°åŒ–ã€‚åˆ†æå½’çº³ã€å‡è®¾å’Œæ¼”ç»æ¨ç†ç®¡é“åœ¨è¿™äº›ç»´åº¦ä¸Šçš„æ¯”è¾ƒåŠ¨æ€ï¼Œå¹¶è¯æ˜æˆ‘ä»¬çš„å‘ç°é€‚ç”¨äºæ›´å¹¿æ³›çš„ä¸Šä¸‹æ–‡å­¦ä¹ ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œè¿˜æ¢è®¨äº†å‡è®¾é€‰æ‹©ã€éªŒè¯å’Œä¿®æ­£ç­‰å…ˆè¿›èŒƒå¼åœ¨æå‡LLMé€»è¾‘æ¨æ–­ä¸­çš„æ½œåŠ›ã€‚è¿™é¡¹æ¢ç´¢æ€§ç ”ç©¶ä¸ºæœªæ¥é€šè¿‡ç³»ç»Ÿé€»è¾‘æ¨æ–­ç­–ç•¥æå‡LLMæ¨ç†èƒ½åŠ›çš„ç ”ç©¶å¥ å®šäº†åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤„ç†æ¨ç†ä»»åŠ¡æ—¶è¿ç”¨å¤šç§å½¢å¼çš„é€»è¾‘æ¨ç†ã€‚</li>
<li>ç†è§£å’Œåˆ©ç”¨è¿™äº›æ¨ç†æ¨¡å¼æ˜¯æå‡LLMæ¨ç†èƒ½åŠ›çš„å…³é”®ã€‚</li>
<li>è®ºæ–‡é€šè¿‡å¼•å…¥å¯æ§è¯„ä¼°ç¯å¢ƒï¼Œå¯¹ç±»æ¯”æ¨ç†è¿›è¡Œç³»ç»ŸåŒ–å‚æ•°åŒ–ç ”ç©¶ã€‚</li>
<li>åˆ†æäº†å½’çº³ã€å‡è®¾å’Œæ¼”ç»æ¨ç†åœ¨ä¸åŒç»´åº¦ä¸Šçš„åŠ¨æ€æ¯”è¾ƒã€‚</li>
<li>ç ”ç©¶å‘ç°é€‚ç”¨äºæ›´å¹¿æ³›çš„ä¸Šä¸‹æ–‡å­¦ä¹ ä»»åŠ¡ã€‚</li>
<li>æ¢è®¨äº†å‡è®¾é€‰æ‹©ã€éªŒè¯å’Œä¿®æ­£ç­‰å…ˆè¿›èŒƒå¼åœ¨LLMé€»è¾‘æ¨æ–­ä¸­çš„æ½œåŠ›ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºæœªæ¥æå‡LLMæ¨ç†èƒ½åŠ›çš„ç³»ç»Ÿé€»è¾‘æ¨æ–­ç­–ç•¥ç ”ç©¶å¥ å®šäº†åŸºç¡€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.11176">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-74050413fd0d9178dfec50c5f6e0166a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fef0b8ade205d74a391f743841fcbe5d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-621468b979fb63de248a29720fd968e5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-75fe903401a1163015d87838aaa3a1fa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-23e40f58d2ecc119d36408c0640d2eef.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Double-Visual-Defense-Adversarial-Pre-training-and-Instruction-Tuning-for-Improving-Vision-Language-Model-Robustness"><a href="#Double-Visual-Defense-Adversarial-Pre-training-and-Instruction-Tuning-for-Improving-Vision-Language-Model-Robustness" class="headerlink" title="Double Visual Defense: Adversarial Pre-training and Instruction Tuning   for Improving Vision-Language Model Robustness"></a>Double Visual Defense: Adversarial Pre-training and Instruction Tuning   for Improving Vision-Language Model Robustness</h2><p><strong>Authors:Zeyu Wang, Cihang Xie, Brian Bartoldson, Bhavya Kailkhura</strong></p>
<p>This paper investigates the robustness of vision-language models against adversarial visual perturbations and introduces a novel &#96;&#96;double visual defenseâ€ to enhance this robustness. Unlike previous approaches that resort to lightweight adversarial fine-tuning of a pre-trained CLIP model, we perform large-scale adversarial vision-language pre-training from scratch using web-scale data. We then strengthen the defense by incorporating adversarial visual instruction tuning. The resulting models from each stage, $\Delta$CLIP and $\Delta^2$LLaVA, show substantially enhanced zero-shot robustness and set a new state-of-the-art in adversarial defense for vision-language models. For example, the adversarial robustness of $\Delta$CLIP surpasses that of the previous best models on ImageNet-1k by ~20%. %For example, $\Delta$CLIP surpasses the previous best models on ImageNet-1k by ~20% in terms of adversarial robustness. Similarly, compared to prior art, $\Delta^2$LLaVA brings a ~30% robustness improvement to image captioning task and a ~20% robustness improvement to visual question answering task. Furthermore, our models exhibit stronger zero-shot recognition capability, fewer hallucinations, and superior reasoning performance compared to baselines. Our project page is <a target="_blank" rel="noopener" href="https://doublevisualdefense.github.io/">https://doublevisualdefense.github.io/</a>. </p>
<blockquote>
<p>æœ¬æ–‡ç ”ç©¶äº†è§†è§‰è¯­è¨€æ¨¡å‹å¯¹æŠ—å¯¹æŠ—æ€§è§†è§‰æ‰°åŠ¨ï¼ˆadversarial visual perturbationsï¼‰çš„é²æ£’æ€§ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§æ–°å‹çš„â€œåŒé‡è§†è§‰é˜²å¾¡â€ï¼ˆdouble visual defenseï¼‰æœºåˆ¶æ¥å¢å¼ºè¿™ç§é²æ£’æ€§ã€‚ä¸åŒäºä¹‹å‰çš„æ–¹æ³•ï¼Œæˆ‘ä»¬é‡‡ç”¨ä»ç½‘ç»œè§„æ¨¡æ•°æ®å¼€å§‹çš„å¤§è§„æ¨¡å¯¹æŠ—æ€§è§†è§‰è¯­è¨€é¢„è®­ç»ƒï¼Œè€Œä¸æ˜¯å¯¹é¢„è®­ç»ƒçš„CLIPæ¨¡å‹è¿›è¡Œè½»é‡çº§çš„å¯¹æŠ—å¾®è°ƒã€‚ç„¶åï¼Œæˆ‘ä»¬é€šè¿‡ç»“åˆå¯¹æŠ—æ€§è§†è§‰æŒ‡ä»¤è°ƒæ•´æ¥åŠ å¼ºé˜²å¾¡ã€‚æ¯ä¸ªé˜¶æ®µçš„æ¨¡å‹ï¼Œå³$\Delta$CLIPå’Œ$\Delta^2$LLaVAï¼Œéƒ½æ˜¾ç¤ºå‡ºå¤§å¹…å¢å¼ºçš„é›¶æ ·æœ¬é²æ£’æ€§ï¼Œå¹¶åœ¨è§†è§‰è¯­è¨€æ¨¡å‹çš„å¯¹æŠ—é˜²å¾¡æ–¹é¢åˆ›é€ äº†æ–°çš„æŠ€æœ¯æ°´å¹³ã€‚ä¾‹å¦‚ï¼Œ$\Delta$CLIPåœ¨ImageNet-1kä¸Šçš„å¯¹æŠ—é²æ£’æ€§è¶…è¿‡äº†ä¹‹å‰çš„æœ€ä½³æ¨¡å‹ï¼Œæé«˜äº†çº¦20%ã€‚åŒæ ·åœ°ï¼Œä¸å…ˆå‰æŠ€æœ¯ç›¸æ¯”ï¼Œ$\Delta^2$LLaVAåœ¨å›¾åƒæè¿°ä»»åŠ¡ä¸Šæé«˜äº†çº¦30%çš„é²æ£’æ€§ï¼Œåœ¨è§†è§‰é—®ç­”ä»»åŠ¡ä¸Šæé«˜äº†çº¦20%çš„é²æ£’æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ¨¡å‹è¿˜è¡¨ç°å‡ºæ›´å¼ºçš„é›¶æ ·æœ¬è¯†åˆ«èƒ½åŠ›ã€æ›´å°‘çš„å¹»è§‰å’Œä¼˜äºåŸºå‡†çº¿çš„æ¨ç†æ€§èƒ½ã€‚æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢æ˜¯ï¼š<a target="_blank" rel="noopener" href="https://doublevisualdefense.github.io/">https://doublevisualdefense.github.io/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.09446v2">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æ¢è®¨äº†è§†è§‰è¯­è¨€æ¨¡å‹å¯¹æŠ—è§†è§‰æ‰°åŠ¨æ”»å‡»çš„é²æ£’æ€§ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°çš„â€œåŒé‡è§†è§‰é˜²å¾¡â€ç­–ç•¥æ¥æå‡æ¨¡å‹çš„é²æ£’æ€§ã€‚è¯¥ç ”ç©¶ä¸ä»…è¿›è¡Œå°è§„æ¨¡çš„å¯¹æŠ—æ€§å¾®è°ƒé¢„è®­ç»ƒCLIPæ¨¡å‹ï¼Œè€Œä¸”ä»ç½‘ç»œè§„æ¨¡æ•°æ®å¼€å§‹å¤§è§„æ¨¡å¯¹æŠ—è§†è§‰è¯­è¨€é¢„è®­ç»ƒã€‚é€šè¿‡ç»“åˆå¯¹æŠ—è§†è§‰æŒ‡ä»¤å¾®è°ƒæ¥åŠ å¼ºé˜²å¾¡ã€‚æ‰€å¾—æ¨¡å‹Î”CLIPå’ŒÎ”Â²LLaVAçš„é›¶æ ·æœ¬é²æ£’æ€§å¾—åˆ°å®è´¨æ€§æå‡ï¼Œåœ¨è§†è§‰è¯­è¨€æ¨¡å‹çš„å¯¹æŠ—é˜²å¾¡æ–¹é¢åˆ›é€ äº†æ–°çš„æŠ€æœ¯æ°´å‡†ã€‚ä¾‹å¦‚ï¼ŒÎ”CLIPåœ¨ImageNet-1kä¸Šçš„å¯¹æŠ—é²æ£’æ€§è¶…è¿‡äº†ä»¥å‰æœ€ä¼˜ç§€çš„æ¨¡å‹çº¦20%ã€‚æ­¤å¤–ï¼Œä¸åŸºçº¿ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å±•ç°å‡ºæ›´å¼ºçš„é›¶æ ·æœ¬è¯†åˆ«èƒ½åŠ›ã€æ›´å°‘çš„å¹»è§‰å’Œæ›´å‡ºè‰²çš„æ¨ç†æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¢è®¨äº†è§†è§‰è¯­è¨€æ¨¡å‹å¯¹äºå¯¹æŠ—è§†è§‰æ‰°åŠ¨çš„é²æ£’æ€§æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„â€œåŒé‡è§†è§‰é˜²å¾¡â€ç­–ç•¥ï¼ŒåŒ…æ‹¬å¤§è§„æ¨¡çš„å¯¹æŠ—è§†è§‰è¯­è¨€é¢„è®­ç»ƒä»¥åŠå¯¹æŠ—è§†è§‰æŒ‡ä»¤å¾®è°ƒã€‚</li>
<li>Î”CLIPæ¨¡å‹åœ¨ImageNet-1kä¸Šçš„å¯¹æŠ—é²æ£’æ€§ç›¸æ¯”ä¹‹å‰æœ€ä¼˜ç§€çš„æ¨¡å‹æå‡äº†çº¦20%ã€‚</li>
<li>Î”Â²LLaVAæ¨¡å‹åœ¨å›¾åƒæè¿°å’Œè§†è§‰é—®ç­”ä»»åŠ¡ä¸Šåˆ†åˆ«å®ç°äº†çº¦30%å’Œçº¦20%çš„é²æ£’æ€§æå‡ã€‚</li>
<li>æ¨¡å‹å±•ç°å‡ºæ›´å¼ºçš„é›¶æ ·æœ¬è¯†åˆ«èƒ½åŠ›ã€‚</li>
<li>æ¨¡å‹å‡å°‘äº†å¹»è§‰ç°è±¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.09446">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-06e359ce86298d9604dc1e221b6cf5cc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-767b185b59c3c4eed70baa9c5a9e8322.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7c166aae33ad0dab36200cbe9f4001ca.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="REINFORCE-An-Efficient-RLHF-Algorithm-with-Robustness-to-Both-Prompt-and-Reward-Models"><a href="#REINFORCE-An-Efficient-RLHF-Algorithm-with-Robustness-to-Both-Prompt-and-Reward-Models" class="headerlink" title="REINFORCE++: An Efficient RLHF Algorithm with Robustness to Both Prompt   and Reward Models"></a>REINFORCE++: An Efficient RLHF Algorithm with Robustness to Both Prompt   and Reward Models</h2><p><strong>Authors:Jian Hu, Jason Klein Liu, Wei Shen</strong></p>
<p>Reinforcement Learning from Human Feedback (RLHF) plays a crucial role in aligning large language models (LLMs) with human values and preferences. While state-of-the-art applications like ChatGPT&#x2F;GPT-4 commonly employ Proximal Policy Optimization (PPO), the inclusion of a critic network introduces significant computational overhead. REINFORCE-based methods, such as REINFORCE Leave One-Out (RLOO), ReMax, and Group Relative Policy Optimization (GRPO), address this limitation by eliminating the critic network. However, these approaches face challenges in accurate advantage estimation. Specifically, they estimate advantages independently for responses to each prompt, which can lead to overfitting on simpler prompts and vulnerability to reward hacking. To address these challenges, we introduce REINFORCE++, a novel approach that removes the critic model while using the normalized reward of a batch as the baseline. Our empirical evaluation demonstrates that REINFORCE++ exhibits robust performance across various reward models without requiring prompt set truncation. Furthermore, it achieves superior generalization in both RLHF and long chain-of-thought (CoT) settings compared to existing REINFORCE-based methods. The implementation is available at <a target="_blank" rel="noopener" href="https://github.com/OpenRLHF/OpenRLHF">https://github.com/OpenRLHF/OpenRLHF</a>. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰åœ¨å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸äººç±»ä»·å€¼è§‚å’Œåå¥½å¯¹é½æ–¹é¢å‘æŒ¥ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚è™½ç„¶æœ€å…ˆè¿›çš„åº”ç”¨å¦‚ChatGPT&#x2F;GPT-4é€šå¸¸é‡‡ç”¨è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰ï¼Œä½†åŠ å…¥è¯„è®ºå®¶ç½‘ç»œä¼šå¼•å…¥å¤§é‡çš„è®¡ç®—å¼€é”€ã€‚åŸºäºREINFORCEçš„æ–¹æ³•ï¼Œå¦‚REINFORCE Leave One-Outï¼ˆRLOOï¼‰ã€ReMaxå’Œé›†å›¢ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œé€šè¿‡æ¶ˆé™¤è¯„è®ºå®¶ç½‘ç»œæ¥è§£å†³è¿™ä¸€é™åˆ¶ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åœ¨å‡†ç¡®ä¼°ç®—ä¼˜åŠ¿æ–¹é¢é¢ä¸´æŒ‘æˆ˜ã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒä»¬ç‹¬ç«‹åœ°ä¸ºæ¯ä¸ªæç¤ºçš„å›åº”ä¼°è®¡ä¼˜åŠ¿ï¼Œè¿™å¯èƒ½å¯¼è‡´åœ¨ç®€å•æç¤ºä¸Šè¿‡æ‹Ÿåˆï¼Œå¹¶å®¹æ˜“å—åˆ°å¥–åŠ±é»‘å®¢æ”»å‡»ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†REINFORCE++ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æ–¹æ³•ï¼Œå®ƒå»é™¤äº†è¯„è®ºå®¶æ¨¡å‹ï¼ŒåŒæ—¶ä½¿ç”¨ä¸€æ‰¹æ¬¡çš„æ ‡å‡†åŒ–å¥–åŠ±ä½œä¸ºåŸºçº¿ã€‚æˆ‘ä»¬çš„ç»éªŒè¯„ä¼°è¡¨æ˜ï¼ŒREINFORCE++åœ¨å„ç§å¥–åŠ±æ¨¡å‹ä¸­å…·æœ‰ç¨³å¥çš„æ€§èƒ½è¡¨ç°ï¼Œæ— éœ€æˆªæ–­æç¤ºé›†ã€‚æ­¤å¤–ï¼Œä¸ç°æœ‰çš„åŸºäºREINFORCEçš„æ–¹æ³•ç›¸æ¯”ï¼Œå®ƒåœ¨RLHFå’Œé•¿é“¾æ€ç»´ï¼ˆCoTï¼‰è®¾ç½®ä¸­å®ç°äº†æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚ç›¸å…³å®ç°å¯è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/OpenRLHF/OpenRLHF%E3%80%82">https://github.com/OpenRLHF/OpenRLHFã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.03262v3">PDF</a> fix typo</p>
<p><strong>Summary</strong></p>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨äººç±»åé¦ˆï¼ˆRLHFï¼‰ä¸­çš„ä½œç”¨å¯¹äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸äººç±»ä»·å€¼è§‚å’Œåå¥½çš„å¯¹é½è‡³å…³é‡è¦ã€‚ChatGPTå’ŒGPT-4ç­‰å°–ç«¯åº”ç”¨æ™®éé‡‡ç”¨è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰ã€‚æ¶ˆé™¤è¯„è®ºå®¶ç½‘ç»œçš„REINFORCEå¼ºåŒ–å­¦ä¹ ä¸ºåŸºç¡€çš„æ–¹æ³•èƒ½æ˜¾è‘—æé«˜æ•ˆç‡ã€‚ä½†æ˜¯é¢ä¸´å‡†ç¡®ä¼°ç®—ä¼˜åŠ¿çš„æŒ‘æˆ˜ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºREINFORCE++ç®—æ³•ï¼Œä½¿ç”¨ä¸€æ‰¹æ ‡å‡†åŒ–å¥–åŠ±ä½œä¸ºåŸºçº¿æ¶ˆé™¤è¯„è®ºå®¶æ¨¡å‹ã€‚å®éªŒè¡¨æ˜ï¼ŒREINFORCE++åœ¨ä¸åŒå¥–åŠ±æ¨¡å‹ä¸­å…·æœ‰ç¨³å¥æ€§èƒ½ï¼Œä¸”å…·å¤‡å‡ºè‰²æ³›åŒ–èƒ½åŠ›ã€‚è¯¦ç»†ä¿¡æ¯å¯é€šè¿‡é“¾æ¥è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/OpenRLHF/OpenRLHF%E3%80%82">https://github.com/OpenRLHF/OpenRLHFã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ åœ¨äººç±»åé¦ˆï¼ˆRLHFï¼‰å¯¹é½å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸äººç±»ä»·å€¼è§‚å’Œåå¥½æ–¹é¢èµ·åˆ°å…³é”®ä½œç”¨ã€‚</li>
<li>ç›®å‰å°–ç«¯åº”ç”¨å¦‚ChatGPTå’ŒGPT-4ä½¿ç”¨è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰ï¼Œä½†å¼•å…¥è¯„è®ºå®¶ç½‘ç»œå¸¦æ¥è®¡ç®—è´Ÿæ‹…ã€‚</li>
<li>REINFORCEå¼ºåŒ–å­¦ä¹ ä¸ºåŸºç¡€çš„æ–¹æ³•æ¶ˆé™¤è¯„è®ºå®¶ç½‘ç»œä»¥æé«˜æ•ˆç‡ï¼Œä½†é¢ä¸´å‡†ç¡®ä¼°ç®—ä¼˜åŠ¿çš„æŒ‘æˆ˜ã€‚</li>
<li>REINFORCEæ–¹æ³•ç‹¬ç«‹ä¼°ç®—æ¯ä¸ªæç¤ºçš„å“åº”ä¼˜åŠ¿ï¼Œå¯èƒ½å¯¼è‡´å¯¹ç®€å•æç¤ºçš„è¿‡æ‹Ÿåˆå’Œæ˜“å—å¥–åŠ±æ“çºµçš„å½±å“ã€‚</li>
<li>REINFORCE++ç®—æ³•æå‡ºæ¶ˆé™¤è¯„è®ºå®¶æ¨¡å‹åŒæ—¶ä½¿ç”¨ä¸€æ‰¹æ ‡å‡†åŒ–å¥–åŠ±ä½œä¸ºåŸºçº¿æ¥è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºREINFORCE++åœ¨ä¸åŒå¥–åŠ±æ¨¡å‹ä¸­è¡¨ç°ç¨³å¥ï¼Œä¸éœ€è¦æˆªæ–­æç¤ºé›†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.03262">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-db524866a6bdf508490367995c0f3fa7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5e5f062d8502344e71c2bee8aeb8c88c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dfd65a62636d841d3a6c83b1c94e1de6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-580aaa40e526cd8d93ef332eaea408ea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8949dc33799e30fa4426ed0d2658c0cb.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Large-Language-Model-Can-Be-a-Foundation-for-Hidden-Rationale-Based-Retrieval"><a href="#Large-Language-Model-Can-Be-a-Foundation-for-Hidden-Rationale-Based-Retrieval" class="headerlink" title="Large Language Model Can Be a Foundation for Hidden Rationale-Based   Retrieval"></a>Large Language Model Can Be a Foundation for Hidden Rationale-Based   Retrieval</h2><p><strong>Authors:Luo Ji, Feixiang Guo, Teng Chen, Qingqing Gu, Xiaoyu Wang, Ningyuan Xi, Yihong Wang, Peng Yu, Yue Zhao, Hongyang Lei, Zhonglin Jiang, Yong Chen</strong></p>
<p>Despite the recent advancement in Retrieval-Augmented Generation (RAG) systems, most retrieval methodologies are often developed for factual retrieval, which assumes query and positive documents are semantically similar. In this paper, we instead propose and study a more challenging type of retrieval task, called hidden rationale retrieval, in which query and document are not similar but can be inferred by reasoning chains, logic relationships, or empirical experiences. To address such problems, an instruction-tuned Large language model (LLM) with a cross-encoder architecture could be a reasonable choice. To further strengthen pioneering LLM-based retrievers, we design a special instruction that transforms the retrieval task into a generative task by prompting LLM to answer a binary-choice question. The model can be fine-tuned with direct preference optimization (DPO). The framework is also optimized for computational efficiency with no performance degradation. We name this retrieval framework by RaHoRe and verify its zero-shot and fine-tuned performance superiority on Emotional Support Conversation (ESC), compared with previous retrieval works. Our study suggests the potential to employ LLM as a foundation for a wider scope of retrieval tasks. Our codes, models, and datasets are available on <a target="_blank" rel="noopener" href="https://github.com/flyfree5/LaHoRe">https://github.com/flyfree5/LaHoRe</a>. </p>
<blockquote>
<p>å°½ç®¡æœ€è¿‘æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿæœ‰æ‰€è¿›å±•ï¼Œä½†å¤§å¤šæ•°æ£€ç´¢æ–¹æ³•å¾€å¾€é’ˆå¯¹äº‹å®æ£€ç´¢è€Œå¼€å‘ï¼Œè¿™å‡è®¾æŸ¥è¯¢å’Œæ­£é¢æ–‡æ¡£åœ¨è¯­ä¹‰ä¸Šæ˜¯ç›¸ä¼¼çš„ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºå¹¶ç ”ç©¶äº†ä¸€ç§æ›´å…·æŒ‘æˆ˜æ€§çš„æ£€ç´¢ä»»åŠ¡ï¼Œç§°ä¸ºéšè—é€»è¾‘æ£€ç´¢ï¼Œå…¶ä¸­æŸ¥è¯¢å’Œæ–‡æ¡£å¹¶ä¸ç›¸ä¼¼ï¼Œä½†å¯ä»¥é€šè¿‡æ¨ç†é“¾ã€é€»è¾‘å…³ç³»æˆ–ç»éªŒæ¨æ–­å¾—å‡ºã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œé‡‡ç”¨å¸¦æœ‰è·¨ç¼–ç å™¨æ¶æ„çš„æŒ‡ä»¤ä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ˜¯ä¸€ä¸ªåˆç†çš„é€‰æ‹©ã€‚ä¸ºäº†è¿›ä¸€æ­¥åŠ å¼ºåŸºäºLLMçš„æ£€ç´¢å™¨ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªç‰¹æ®ŠæŒ‡ä»¤ï¼Œé€šè¿‡æç¤ºLLMå›ç­”äºŒå…ƒé€‰æ‹©é—®é¢˜ï¼Œå°†æ£€ç´¢ä»»åŠ¡è½¬å˜ä¸ºç”Ÿæˆä»»åŠ¡ã€‚è¯¥æ¨¡å‹å¯ä»¥é€šè¿‡ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰è¿›è¡Œå¾®è°ƒã€‚è¯¥æ¡†æ¶åœ¨è®¡ç®—æ•ˆç‡æ–¹é¢ä¹Ÿè¿›è¡Œäº†ä¼˜åŒ–ï¼Œä¸”ä¸ä¼šé™ä½æ€§èƒ½ã€‚æˆ‘ä»¬å°†è¿™ç§æ£€ç´¢æ¡†æ¶å‘½åä¸ºRaHoReï¼Œå¹¶åœ¨æƒ…æ„Ÿæ”¯æŒå¯¹è¯ï¼ˆESCï¼‰ä¸ŠéªŒè¯äº†å…¶é›¶æ ·æœ¬å’Œå¾®è°ƒåçš„æ€§èƒ½ä¼˜è¶Šæ€§ï¼Œä¸ä¹‹å‰çš„ç ”ç©¶ç›¸æ¯”è¡¨ç°æ›´ä½³ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œæœ‰æ½œåŠ›å°†LLMä½œä¸ºæ›´å¹¿æ³›æ£€ç´¢ä»»åŠ¡çš„åŸºç¡€ã€‚æˆ‘ä»¬çš„ä»£ç ã€æ¨¡å‹å’Œæ•°æ®é›†å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/flyfree5/LaHoRe%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/flyfree5/LaHoReè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.16615v2">PDF</a> 10 pages, 3 figures, ECIR 2025</p>
<p><strong>Summary</strong></p>
<p>è¿™æ˜¯ä¸€ç¯‡å…³äºéšè—é€»è¾‘æ£€ç´¢çš„ç ”ç©¶è®ºæ–‡ï¼Œæå‡ºäº†ä¸€ç§æ›´å…·æŒ‘æˆ˜æ€§çš„æ£€ç´¢ä»»åŠ¡ã€‚è¯¥ç ”ç©¶æŒ‘æˆ˜äº†ä¼ ç»Ÿçš„åŸºäºäº‹å®æ£€ç´¢çš„å‡è®¾ï¼Œå³æŸ¥è¯¢å’Œæ­£é¢æ–‡æ¡£è¯­ä¹‰ç›¸ä¼¼ã€‚è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºRaHoReçš„æ£€ç´¢æ¡†æ¶ï¼Œåˆ©ç”¨æŒ‡ä»¤ä¼˜åŒ–çš„è·¨ç¼–ç å™¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥è§£å†³éšè—é€»è¾‘æ£€ç´¢é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡æç¤ºLLMå›ç­”äºŒé€‰ä¸€é—®é¢˜å°†æ£€ç´¢ä»»åŠ¡è½¬åŒ–ä¸ºç”Ÿæˆä»»åŠ¡ï¼Œå¹¶åˆ©ç”¨ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰è¿›è¡Œå¾®è°ƒã€‚ç ”ç©¶è¡¨æ˜ï¼ŒRaHoReåœ¨æƒ…æ„Ÿæ”¯æŒå¯¹è¯ï¼ˆESCï¼‰ç­‰ä»»åŠ¡ä¸Šçš„é›¶æ ·æœ¬å’Œå¾®è°ƒæ€§èƒ½å‡ä¼˜äºå…ˆå‰çš„ç ”ç©¶å·¥ä½œï¼Œå±•ç°äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ›´å¹¿èŒƒå›´æ£€ç´¢ä»»åŠ¡ä¸­çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ£€ç´¢ä»»åŠ¡â€”â€”éšè—é€»è¾‘æ£€ç´¢ï¼ŒæŒ‘æˆ˜äº†ä¼ ç»Ÿçš„åŸºäºäº‹å®æ£€ç´¢çš„å‡è®¾ã€‚</li>
<li>éšè—é€»è¾‘æ£€ç´¢ä¸­ï¼ŒæŸ¥è¯¢å’Œæ–‡æ¡£å¹¶ä¸ç›¸ä¼¼ï¼Œä½†å¯ä»¥é€šè¿‡æ¨ç†é“¾ã€é€»è¾‘å…³ç³»æˆ–ç»éªŒæ¨æ–­æ¥è”ç³»ã€‚</li>
<li>ç ”ç©¶è€…å»ºè®®ä½¿ç”¨æŒ‡ä»¤ä¼˜åŒ–çš„è·¨ç¼–ç å™¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥è§£å†³éšè—é€»è¾‘æ£€ç´¢é—®é¢˜ã€‚</li>
<li>é€šè¿‡å°†æ£€ç´¢ä»»åŠ¡è½¬åŒ–ä¸ºç”Ÿæˆä»»åŠ¡ï¼Œåˆ©ç”¨LLMå›ç­”äºŒé€‰ä¸€é—®é¢˜çš„ç‰¹æ®ŠæŒ‡ä»¤æ¥å¼ºåŒ–æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ç”¨äºå¾®è°ƒæ¨¡å‹ï¼Œæé«˜æ€§èƒ½ã€‚</li>
<li>RaHoReæ£€ç´¢æ¡†æ¶åœ¨æƒ…æ„Ÿæ”¯æŒå¯¹è¯ï¼ˆESCï¼‰ç­‰ä»»åŠ¡ä¸Šè¡¨ç°å‡ºä¼˜è¶Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.16615">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-71c315138e1bee7b6bc0652fd8382214.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a27a7c22ac71a7dc1f64ecbbefa4ff8d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4b6185f0732452917dcd1ca26de4153c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d2de7cc6ecb79282f279f85364276089.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-11592773aa0f5fd6b6eefb4ccf68da0b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-eda4d61130366799ddca2f6270caaabc.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="EarthDial-Turning-Multi-sensory-Earth-Observations-to-Interactive-Dialogues"><a href="#EarthDial-Turning-Multi-sensory-Earth-Observations-to-Interactive-Dialogues" class="headerlink" title="EarthDial: Turning Multi-sensory Earth Observations to Interactive   Dialogues"></a>EarthDial: Turning Multi-sensory Earth Observations to Interactive   Dialogues</h2><p><strong>Authors:Sagar Soni, Akshay Dudhane, Hiyam Debary, Mustansar Fiaz, Muhammad Akhtar Munir, Muhammad Sohail Danish, Paolo Fraccaro, Campbell D Watson, Levente J Klein, Fahad Shahbaz Khan, Salman Khan</strong></p>
<p>Automated analysis of vast Earth observation data via interactive Vision-Language Models (VLMs) can unlock new opportunities for environmental monitoring, disaster response, and {resource management}. Existing generic VLMs do not perform well on Remote Sensing data, while the recent Geo-spatial VLMs remain restricted to a fixed resolution and few sensor modalities. In this paper, we introduce EarthDial, a conversational assistant specifically designed for Earth Observation (EO) data, transforming complex, multi-sensory Earth observations into interactive, natural language dialogues. EarthDial supports multi-spectral, multi-temporal, and multi-resolution imagery, enabling a wide range of remote sensing tasks, including classification, detection, captioning, question answering, visual reasoning, and visual grounding. To achieve this, we introduce an extensive instruction tuning dataset comprising over 11.11M instruction pairs covering RGB, Synthetic Aperture Radar (SAR), and multispectral modalities such as Near-Infrared (NIR) and infrared. Furthermore, EarthDial handles bi-temporal and multi-temporal sequence analysis for applications like change detection. Our extensive experimental results on 44 downstream datasets demonstrate that EarthDial outperforms existing generic and domain-specific models, achieving better generalization across various EO tasks. Our source codes and pre-trained models are at <a target="_blank" rel="noopener" href="https://github.com/hiyamdebary/EarthDial">https://github.com/hiyamdebary/EarthDial</a>. </p>
<blockquote>
<p>é€šè¿‡äº¤äº’å¼çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å¯¹å¤§é‡çš„åœ°çƒè§‚æµ‹æ•°æ®è¿›è¡Œè‡ªåŠ¨åŒ–åˆ†æï¼Œå¯ä»¥ä¸ºç¯å¢ƒç›‘æµ‹ã€ç¾å®³å“åº”å’Œ{èµ„æºç®¡ç†}ç­‰é¢†åŸŸå¸¦æ¥æ–°çš„æœºé‡ã€‚ç°æœ‰çš„é€šç”¨VLMsåœ¨é¥æ„Ÿæ•°æ®ä¸Šçš„è¡¨ç°å¹¶ä¸ç†æƒ³ï¼Œè€Œæœ€è¿‘çš„åœ°ç†ç©ºé—´VLMsä»ç„¶å±€é™äºå›ºå®šçš„åˆ†è¾¨ç‡å’Œå°‘é‡çš„ä¼ æ„Ÿå™¨æ¨¡æ€ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†EarthDialï¼Œè¿™æ˜¯ä¸€æ¬¾ä¸“é—¨ä¸ºåœ°çƒè§‚æµ‹ï¼ˆEOï¼‰æ•°æ®è®¾è®¡çš„å¯¹è¯åŠ©æ‰‹ï¼Œå°†å¤æ‚çš„å¤šæ„Ÿå®˜åœ°çƒè§‚æµ‹è½¬åŒ–ä¸ºäº¤äº’å¼çš„è‡ªç„¶è¯­è¨€å¯¹è¯ã€‚EarthDialæ”¯æŒå¤šå…‰è°±ã€å¤šæ—¶ç›¸å’Œå¤šåˆ†è¾¨ç‡çš„å½±åƒï¼Œèƒ½å¤Ÿè¿›è¡Œå¹¿æ³›çš„é¥æ„Ÿä»»åŠ¡ï¼ŒåŒ…æ‹¬åˆ†ç±»ã€æ£€æµ‹ã€æè¿°ã€é—®ç­”ã€è§†è§‰æ¨ç†å’Œè§†è§‰å®šä½ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåŒ…å«è¶…è¿‡1111ä¸‡ä¸ªæŒ‡ä»¤å¯¹çš„åºå¤§æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†ï¼Œæ¶µç›–RGBã€åˆæˆå­”å¾„é›·è¾¾ï¼ˆSARï¼‰å’Œè¿‘çº¢å¤–ï¼ˆNIRï¼‰å’Œçº¢å¤–ç­‰å¤šå…‰è°±æ¨¡æ€ã€‚æ­¤å¤–ï¼ŒEarthDialè¿˜å¤„ç†åŒæ—¶æ€å’Œå¤šæ—¶æ€åºåˆ—åˆ†æï¼Œç”¨äºå˜åŒ–æ£€æµ‹ç­‰åº”ç”¨ã€‚æˆ‘ä»¬åœ¨44ä¸ªä¸‹æ¸¸æ•°æ®é›†ä¸Šè¿›è¡Œçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒEarthDialä¼˜äºç°æœ‰çš„é€šç”¨å’Œç‰¹å®šé¢†åŸŸæ¨¡å‹ï¼Œåœ¨å„ç§EOä»»åŠ¡ä¸­å®ç°äº†æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æºä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹ä½äº<a target="_blank" rel="noopener" href="https://github.com/hiyamdebary/EarthDial%E3%80%82">https://github.com/hiyamdebary/EarthDialã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15190v2">PDF</a> </p>
<p><strong>Summary</strong>ï¼šé€šè¿‡äº¤äº’å¼è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å¯¹æµ·é‡çš„åœ°çƒè§‚æµ‹æ•°æ®è¿›è¡Œè‡ªåŠ¨åŒ–åˆ†æï¼Œä¸ºç¯å¢ƒç›‘æµ‹ã€ç¾å®³å“åº”å’Œ{èµ„æºç®¡ç†}ç­‰é¢†åŸŸå¸¦æ¥äº†æ–°çš„æœºé‡ã€‚é’ˆå¯¹ç°æœ‰é€šç”¨VLMsåœ¨é¥æ„Ÿæ•°æ®ä¸Šçš„è¡¨ç°ä¸ä½³åŠåœ°ç†ç©ºé—´VLMså­˜åœ¨çš„åˆ†è¾¨ç‡å’Œä¼ æ„Ÿå™¨æ¨¡å¼é™åˆ¶ï¼Œæœ¬æ–‡å¼•å…¥äº†EarthDialï¼Œä¸€æ¬¾ä¸“ä¸ºåœ°çƒè§‚æµ‹ï¼ˆEOï¼‰æ•°æ®è®¾è®¡çš„å¯¹è¯åŠ©æ‰‹ã€‚EarthDialå¯å°†å¤æ‚çš„ã€å¤šæ„Ÿå®˜çš„åœ°çƒè§‚æµ‹æ•°æ®è½¬åŒ–ä¸ºäº¤äº’å¼è‡ªç„¶è¯­è¨€å¯¹è¯ï¼Œæ”¯æŒå¤šå…‰è°±ã€å¤šæ—¶ç›¸å’Œå¤šåˆ†è¾¨ç‡å½±åƒï¼Œèƒ½å®Œæˆé¥æ„Ÿåˆ†ç±»ã€æ£€æµ‹ã€æè¿°ã€é—®ç­”ã€è§†è§‰æ¨ç†å’Œè§†è§‰å®šä½ç­‰ä»»åŠ¡ã€‚ä¸ºè¾¾æˆè¿™ä¸€ç›®æ ‡ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåŒ…å«è¶…è¿‡111ä¸‡æŒ‡ä»¤å¯¹çš„å¤§å‹æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†ï¼Œæ¶µç›–RGBã€åˆæˆå­”å¾„é›·è¾¾ï¼ˆSARï¼‰å’Œå¤šå…‰è°±æ¨¡å¼ï¼Œå¦‚è¿‘çº¢å¤–å’Œçº¢å¤–ã€‚EarthDialè¿˜èƒ½å¤„ç†åŒæ—¶ç›¸å’Œå¤šæ—¶ç›¸åºåˆ—åˆ†æï¼Œç”¨äºå˜åŒ–æ£€æµ‹ç­‰åº”ç”¨ã€‚åœ¨å¤šä¸ªä¸‹æ¸¸æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒEarthDialåœ¨å¤šç§åœ°çƒè§‚æµ‹ä»»åŠ¡ä¸Šä¼˜äºç°æœ‰é€šç”¨å’Œç‰¹å®šé¢†åŸŸçš„æ¨¡å‹ï¼Œå…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>EarthDialæ˜¯ä¸€æ¬¾ä¸“ä¸ºåœ°çƒè§‚æµ‹æ•°æ®è®¾è®¡çš„äº¤äº’å¼å¯¹è¯åŠ©æ‰‹ï¼Œå¯å°†å¤æ‚çš„é¥æ„Ÿæ•°æ®è½¬åŒ–ä¸ºè‡ªç„¶è¯­è¨€å¯¹è¯ã€‚</li>
<li>EarthDialæ”¯æŒå¤šå…‰è°±ã€å¤šæ—¶ç›¸å’Œå¤šåˆ†è¾¨ç‡çš„å½±åƒå¤„ç†ã€‚</li>
<li>EarthDialèƒ½å®Œæˆå¤šç§é¥æ„Ÿä»»åŠ¡ï¼ŒåŒ…æ‹¬åˆ†ç±»ã€æ£€æµ‹ã€æè¿°ã€é—®ç­”ã€è§†è§‰æ¨ç†å’Œè§†è§‰å®šä½ã€‚</li>
<li>ä¸ºè®­ç»ƒEarthDialï¼Œç ”ç©¶å›¢é˜Ÿæ„å»ºäº†ä¸€ä¸ªåŒ…å«å¤šç§æŒ‡ä»¤å’Œé¥æ„Ÿæ•°æ®çš„å¤§å‹æ•°æ®é›†ã€‚</li>
<li>EarthDialèƒ½å¤„ç†åŒæ—¶ç›¸å’Œå¤šæ—¶ç›¸åºåˆ—åˆ†æï¼Œé€‚ç”¨äºå˜åŒ–æ£€æµ‹ç­‰åº”ç”¨ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒEarthDialåœ¨å¤šç§åœ°çƒè§‚æµ‹ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜äºç°æœ‰æ¨¡å‹ï¼Œå…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15190">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-25793f7de58c65d5c2c17255ce86db71.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-52e8696cfd7e236fbdae32706e2ab6e5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-63f8c3d4223882f779e76f50ea872089.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-032964f5ee4415fbf4ba61c49b63ef61.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0f92d0aa3d0e003e776638f3d9c3cee5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8f6e2a331988a050ce4c14158d45db1b.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="ClarityEthic-Explainable-Moral-Judgment-Utilizing-Contrastive-Ethical-Insights-from-Large-Language-Models"><a href="#ClarityEthic-Explainable-Moral-Judgment-Utilizing-Contrastive-Ethical-Insights-from-Large-Language-Models" class="headerlink" title="ClarityEthic: Explainable Moral Judgment Utilizing Contrastive Ethical   Insights from Large Language Models"></a>ClarityEthic: Explainable Moral Judgment Utilizing Contrastive Ethical   Insights from Large Language Models</h2><p><strong>Authors:Yuxi Sun, Wei Gao, Jing Ma, Hongzhan Lin, Ziyang Luo, Wenxuan Zhang</strong></p>
<p>With the rise and widespread use of Large Language Models (LLMs), ensuring their safety is crucial to prevent harm to humans and promote ethical behaviors. However, directly assessing value valence (i.e., support or oppose) by leveraging large-scale data training is untrustworthy and inexplainable. We assume that emulating humans to rely on social norms to make moral decisions can help LLMs understand and predict moral judgment. However, capturing human values remains a challenge, as multiple related norms might conflict in specific contexts. Consider norms that are upheld by the majority and promote the well-being of society are more likely to be accepted and widely adopted (e.g., â€œdonâ€™t cheat,â€). Therefore, it is essential for LLM to identify the appropriate norms for a given scenario before making moral decisions. To this end, we introduce a novel moral judgment approach called \textit{ClarityEthic} that leverages LLMsâ€™ reasoning ability and contrastive learning to uncover relevant social norms for human actions from different perspectives and select the most reliable one to enhance judgment accuracy. Extensive experiments demonstrate that our method outperforms state-of-the-art approaches in moral judgment tasks. Moreover, human evaluations confirm that the generated social norms provide plausible explanations that support the judgments. This suggests that modeling human moral judgment with the emulating humans moral strategy is promising for improving the ethical behaviors of LLMs. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å…´èµ·å’Œå¹¿æ³›åº”ç”¨ï¼Œç¡®ä¿å…¶å®‰å…¨æ€§å¯¹äºé˜²æ­¢å¯¹äººç±»é€ æˆä¼¤å®³å’Œä¿ƒè¿›é“å¾·è¡Œä¸ºè‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç›´æ¥åˆ©ç”¨å¤§è§„æ¨¡æ•°æ®è¿›è¡Œè®­ç»ƒæ¥è¯„ä¼°ä»·å€¼å€¾å‘ï¼ˆå³æ”¯æŒæˆ–åå¯¹ï¼‰æ˜¯ä¸å¯é ä¸”ä¸å¯è§£é‡Šçš„ã€‚æˆ‘ä»¬å‡è®¾é€šè¿‡æ¨¡ä»¿äººç±»ä¾èµ–ç¤¾ä¼šè§„èŒƒæ¥åšå‡ºé“å¾·å†³ç­–ï¼Œå¯ä»¥å¸®åŠ©LLMç†è§£å’Œé¢„æµ‹é“å¾·åˆ¤æ–­ã€‚ç„¶è€Œï¼Œæ•æ‰äººç±»ä»·å€¼è§‚ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œå› ä¸ºåœ¨ç‰¹å®šçš„ä¸Šä¸‹æ–‡ä¸­ï¼Œå¤šä¸ªç›¸å…³è§„èŒƒå¯èƒ½ä¼šå‘ç”Ÿå†²çªã€‚è¢«è®¤ä¸ºæ˜¯ç”±å¤šæ•°äººæ”¯æŒå¹¶ä¿ƒè¿›ç¤¾ä¼šç¦ç¥‰çš„è§„èŒƒæ›´æœ‰å¯èƒ½è¢«æ¥å—å’Œå¹¿æ³›é‡‡ç”¨ï¼ˆä¾‹å¦‚ï¼Œâ€œä¸è¦æ¬ºéª—â€ï¼‰ã€‚å› æ­¤ï¼Œå¯¹äºLLMæ¥è¯´ï¼Œåœ¨åšå‡ºé“å¾·å†³ç­–ä¹‹å‰ï¼Œè¯†åˆ«ç»™å®šåœºæ™¯ä¸­çš„é€‚å½“è§„èŒƒè‡³å…³é‡è¦ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹çš„é“å¾·åˆ¤æ–­æ–¹æ³•ï¼Œç§°ä¸ºClarityEthicã€‚è¯¥æ–¹æ³•åˆ©ç”¨LLMçš„æ¨ç†èƒ½åŠ›å’Œå¯¹æ¯”å­¦ä¹ ï¼Œä»ä¸åŒè§’åº¦æ­ç¤ºä¸äººç±»è¡Œä¸ºç›¸å…³çš„ç¤¾ä¼šè§„èŒƒï¼Œå¹¶é€‰æ‹©æœ€å¯é çš„ä¸€ä¸ªæ¥æé«˜åˆ¤æ–­çš„å‡†ç¡®æ€§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨é“å¾·åˆ¤æ–­ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚æ­¤å¤–ï¼Œäººç±»è¯„ä¼°è¯å®ï¼Œç”Ÿæˆçš„ç¤¾ä¼šè§„èŒƒæä¾›äº†æ”¯æŒåˆ¤æ–­çš„åˆç†è§£é‡Šã€‚è¿™è¡¨æ˜ç”¨æ¨¡ä»¿äººç±»çš„é“å¾·ç­–ç•¥æ¥æ¨¡æ‹Ÿäººç±»é“å¾·åˆ¤æ–­å¯¹äºæé«˜LLMçš„é“å¾·è¡Œä¸ºæ˜¯æœ‰å¸Œæœ›çš„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.12848v2">PDF</a> We have noticed that this version of our experiment and method   description isnâ€™t quite complete or accurate. To make sure we present our   best work, we think it would be a good idea to withdraw the manuscript for   now and take some time to revise and reformat it</p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¹¿æ³›åº”ç”¨å¸¦æ¥äº†ä¿éšœå…¶å®‰å…¨æ€§çš„é‡è¦æ€§ï¼Œä»¥é˜²æ­¢å¯¹äººç±»é€ æˆå±å®³å¹¶æ¨åŠ¨é“å¾·è¡Œä¸ºã€‚ç›´æ¥åˆ©ç”¨å¤§è§„æ¨¡æ•°æ®è¿›è¡Œä»·å€¼å€¾å‘è¯„ä¼°æ˜¯ä¸å€¼å¾—ä¿¡èµ–å’Œä¸å¯è§£é‡Šçš„ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºç¤¾ä¼šè§„èŒƒçš„æ¨¡æ‹Ÿäººç±»é“å¾·åˆ¤æ–­æ–¹æ³•ClarityEthicï¼Œå®ƒé€šè¿‡LLMsçš„æ¨ç†èƒ½åŠ›å’Œå¯¹æ¯”å­¦ä¹ ï¼Œæ­ç¤ºäººç±»è¡Œä¸ºç›¸å…³çš„ç¤¾ä¼šè§„èŒƒï¼Œå¹¶é€‰æ‹©æœ€å¯é çš„ç¤¾ä¼šè§„èŒƒæ¥æé«˜åˆ¤æ–­å‡†ç¡®æ€§ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é“å¾·åˆ¤æ–­ä»»åŠ¡ä¸Šä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œå¹¶å¾—åˆ°äººç±»è¯„ä¼°çš„æ”¯æŒã€‚è¿™è¡¨æ˜é‡‡ç”¨æ¨¡æ‹Ÿäººç±»é“å¾·ç­–ç•¥çš„å»ºæ¨¡äººç±»é“å¾·åˆ¤æ–­å¯¹äºæé«˜LLMsçš„ä¼¦ç†è¡Œä¸ºæ˜¯æœ‰å‰æ™¯çš„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¹¿æ³›åº”ç”¨éœ€è¦ç¡®ä¿å®‰å…¨æ€§ï¼Œä»¥é˜²æ­¢å¯¹äººç±»é€ æˆå±å®³å¹¶æ¨åŠ¨é“å¾·è¡Œä¸ºã€‚</li>
<li>ç›´æ¥åˆ©ç”¨å¤§è§„æ¨¡æ•°æ®è¿›è¡Œä»·å€¼å€¾å‘è¯„ä¼°æ˜¯ä¸å€¼å¾—ä¿¡èµ–å’Œä¸å¯è§£é‡Šçš„ã€‚</li>
<li>æ¨¡æ‹Ÿäººç±»ä¾é ç¤¾ä¼šè§„èŒƒè¿›è¡Œé“å¾·å†³ç­–çš„æ–¹æ³•å¯ä»¥å¸®åŠ©LLMsç†è§£å’Œé¢„æµ‹é“å¾·åˆ¤æ–­ã€‚</li>
<li>è¯†åˆ«ç»™å®šæƒ…å¢ƒä¸‹çš„é€‚å½“ç¤¾ä¼šè§„èŒƒå¯¹äºLLMsåšå‡ºé“å¾·å†³ç­–è‡³å…³é‡è¦ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°çš„é“å¾·åˆ¤æ–­æ–¹æ³•ClarityEthicï¼Œåˆ©ç”¨LLMsçš„æ¨ç†èƒ½åŠ›å’Œå¯¹æ¯”å­¦ä¹ æ¥æ­ç¤ºç›¸å…³çš„ç¤¾ä¼šè§„èŒƒã€‚</li>
<li>ClarityEthicæ–¹æ³•åœ¨é“å¾·åˆ¤æ–­ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.12848">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2c73758d99285e8580467182914c2166.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-682bc31311f341115ea25982502a8ea0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-faa0ea5a4128dc1343c9583135d4dc0e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9e646f4bbc2e575e58d0e9edfc115dad.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-11/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-11/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-11/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-b95a2e33a71f82a8db2e1dc2aad5c785.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-11  Sculpting Subspaces Constrained Full Fine-Tuning in LLMs for Continual   Learning
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-11
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-10/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-42b6f28679a49004e487f820b308ab23.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-10  SE4Lip Speech-Lip Encoder for Talking Head Synthesis to Solve   Phoneme-Viseme Alignment Ambiguity
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-10
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">17124.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
