<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-11  Sculpting Subspaces Constrained Full Fine-Tuning in LLMs for Continual   Learning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-b95a2e33a71f82a8db2e1dc2aad5c785.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-11
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    82 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-04-11-æ›´æ–°"><a href="#2025-04-11-æ›´æ–°" class="headerlink" title="2025-04-11 æ›´æ–°"></a>2025-04-11 æ›´æ–°</h1><h2 id="Sculpting-Subspaces-Constrained-Full-Fine-Tuning-in-LLMs-for-Continual-Learning"><a href="#Sculpting-Subspaces-Constrained-Full-Fine-Tuning-in-LLMs-for-Continual-Learning" class="headerlink" title="Sculpting Subspaces: Constrained Full Fine-Tuning in LLMs for Continual   Learning"></a>Sculpting Subspaces: Constrained Full Fine-Tuning in LLMs for Continual   Learning</h2><p><strong>Authors:Nikhil Shivakumar Nayak, Krishnateja Killamsetty, Ligong Han, Abhishek Bhandwaldar, Prateek Chanda, Kai Xu, Hao Wang, Aldo Pareja, Oleg Silkin, Mustafa Eyceoz, Akash Srivastava</strong></p>
<p>Continual learning in large language models (LLMs) is prone to catastrophic forgetting, where adapting to new tasks significantly degrades performance on previously learned ones. Existing methods typically rely on low-rank, parameter-efficient updates that limit the modelâ€™s expressivity and introduce additional parameters per task, leading to scalability issues. To address these limitations, we propose a novel continual full fine-tuning approach leveraging adaptive singular value decomposition (SVD). Our method dynamically identifies task-specific low-rank parameter subspaces and constrains updates to be orthogonal to critical directions associated with prior tasks, thus effectively minimizing interference without additional parameter overhead or storing previous task gradients. We evaluate our approach extensively on standard continual learning benchmarks using both encoder-decoder (T5-Large) and decoder-only (LLaMA-2 7B) models, spanning diverse tasks including classification, generation, and reasoning. Empirically, our method achieves state-of-the-art results, up to 7% higher average accuracy than recent baselines like O-LoRA, and notably maintains the modelâ€™s general linguistic capabilities, instruction-following accuracy, and safety throughout the continual learning process by reducing forgetting to near-negligible levels. Our adaptive SVD framework effectively balances model plasticity and knowledge retention, providing a practical, theoretically grounded, and computationally scalable solution for continual learning scenarios in large language models. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æŒç»­å­¦ä¹ å®¹æ˜“å—åˆ°ç¾éš¾æ€§é—å¿˜çš„å½±å“ï¼Œå³é€‚åº”æ–°ä»»åŠ¡ä¼šæ˜¾è‘—é™ä½å¯¹å…ˆå‰å­¦ä¹ ä»»åŠ¡çš„æ€§èƒ½ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºä½é˜¶ã€å‚æ•°é«˜æ•ˆçš„æ›´æ–°ï¼Œè¿™é™åˆ¶äº†æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ï¼Œå¹¶ä¸ºæ¯ä¸ªä»»åŠ¡å¼•å…¥äº†é¢å¤–çš„å‚æ•°ï¼Œå¯¼è‡´å¯æ‰©å±•æ€§é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ©ç”¨è‡ªé€‚åº”å¥‡å¼‚å€¼åˆ†è§£ï¼ˆSVDï¼‰çš„æ–°å‹æŒç»­å…¨å¾®è°ƒæ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŠ¨æ€åœ°è¯†åˆ«ç‰¹å®šä»»åŠ¡çš„ä½é˜¶å‚æ•°å­ç©ºé—´ï¼Œå¹¶çº¦æŸæ›´æ–°ä¸å…ˆå‰ä»»åŠ¡ç›¸å…³çš„å…³é”®æ–¹å‘æ­£äº¤ï¼Œä»è€Œæœ‰æ•ˆåœ°æœ€å°åŒ–å¹²æ‰°ï¼Œè€Œæ— éœ€é¢å¤–çš„å‚æ•°å¼€é”€æˆ–å­˜å‚¨å…ˆå‰ä»»åŠ¡çš„æ¢¯åº¦ã€‚æˆ‘ä»¬åœ¨ä½¿ç”¨ç¼–ç å™¨-è§£ç å™¨ï¼ˆT5-Largeï¼‰å’Œä»…è§£ç å™¨ï¼ˆLLaMA-2 7Bï¼‰æ¨¡å‹çš„æ ‡å‡†æŒç»­å­¦ä¹ åŸºå‡†æµ‹è¯•é›†ä¸Šå¯¹æˆ‘ä»¬çš„æ–¹æ³•è¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ï¼Œæ¶µç›–åˆ†ç±»ã€ç”Ÿæˆå’Œæ¨ç†ç­‰å¤šæ ·åŒ–ä»»åŠ¡ã€‚ç»éªŒä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æˆæœï¼Œå¹³å‡å‡†ç¡®ç‡æ¯”æœ€è¿‘çš„åŸºçº¿ï¼ˆå¦‚O-LoRAï¼‰é«˜å‡ºé«˜è¾¾7%ï¼Œå¹¶ä¸”æ˜¾è‘—åœ°ä¿æŒäº†æ¨¡å‹çš„ä¸€èˆ¬è¯­è¨€èƒ½åŠ›ã€éµå¾ªæŒ‡ä»¤çš„å‡†ç¡®æ€§å’Œå®‰å…¨æ€§ï¼Œé€šè¿‡å‡å°‘å‡ ä¹å¯ä»¥å¿½ç•¥çš„é—å¿˜æ°´å¹³æ¥å®ç°æŒç»­å­¦ä¹ è¿‡ç¨‹ã€‚æˆ‘ä»¬çš„è‡ªé€‚åº”SVDæ¡†æ¶æœ‰æ•ˆåœ°å¹³è¡¡äº†æ¨¡å‹çš„é€‚åº”æ€§å’ŒçŸ¥è¯†ä¿ç•™èƒ½åŠ›ï¼Œä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„æŒç»­å­¦ä¹ åœºæ™¯æä¾›äº†å®ç”¨ã€ç†è®ºæ‰å®ä¸”è®¡ç®—ä¸Šå¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07097v1">PDF</a> 25 pages, 13 figures, 6 tables</p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æŒç»­å­¦ä¹ é¢ä¸´ç¾éš¾æ€§é—å¿˜é—®é¢˜ï¼Œå³é€‚åº”æ–°ä»»åŠ¡æ—¶ä¼šå¯¼è‡´å¯¹å…ˆå‰å­¦ä¹ ä»»åŠ¡çš„æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºä½é˜¶ã€å‚æ•°é«˜æ•ˆçš„æ›´æ–°ï¼Œè¿™é™åˆ¶äº†æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ï¼Œå¹¶ä¸ºæ¯ä¸ªä»»åŠ¡å¼•å…¥äº†é¢å¤–çš„å‚æ•°ï¼Œå¯¼è‡´å¯æ‰©å±•æ€§é—®é¢˜ã€‚é’ˆå¯¹è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ©ç”¨è‡ªé€‚åº”å¥‡å¼‚å€¼åˆ†è§£ï¼ˆSVDï¼‰çš„æ–°çš„æŒç»­å…¨å¾®è°ƒæ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŠ¨æ€åœ°è¯†åˆ«ç‰¹å®šä»»åŠ¡çš„ä½é˜¶å‚æ•°å­ç©ºé—´ï¼Œå¹¶çº¦æŸæ›´æ–°ä¸å…ˆå‰ä»»åŠ¡çš„å…³é”®æ–¹å‘æ­£äº¤ï¼Œä»è€Œæœ‰æ•ˆåœ°æœ€å°åŒ–å¹²æ‰°ï¼Œæ— éœ€é¢å¤–çš„å‚æ•°å¼€é”€æˆ–å­˜å‚¨å…ˆå‰ä»»åŠ¡çš„æ¢¯åº¦ã€‚æˆ‘ä»¬åœ¨æ ‡å‡†çš„æŒç»­å­¦ä¹ åŸºå‡†æµ‹è¯•ä¸Šå¯¹ä½¿ç”¨ç¼–ç å™¨-è§£ç å™¨ï¼ˆT5-Largeï¼‰å’Œä»…è§£ç å™¨ï¼ˆLLaMA-2 7Bï¼‰çš„æ¨¡å‹è¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ï¼Œæ¶µç›–åˆ†ç±»ã€ç”Ÿæˆå’Œæ¨ç†ç­‰å¤šæ ·åŒ–ä»»åŠ¡ã€‚ç»éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†æœ€æ–°çš„ç»“æœï¼Œæ¯”æœ€è¿‘çš„åŸºçº¿ï¼ˆå¦‚O-LoRAï¼‰é«˜å‡ºé«˜è¾¾7%çš„å¹³å‡å‡†ç¡®ç‡ï¼Œå¹¶æ˜¾è‘—åœ°ä¿æŒäº†æ¨¡å‹çš„ä¸€èˆ¬è¯­è¨€èƒ½åŠ›ã€æŒ‡ä»¤éµå¾ªå‡†ç¡®æ€§å’Œå®‰å…¨æ€§ã€‚æˆ‘ä»¬çš„è‡ªé€‚åº”SVDæ¡†æ¶æœ‰æ•ˆåœ°å¹³è¡¡äº†æ¨¡å‹çš„é€‚åº”æ€§å’ŒçŸ¥è¯†ä¿ç•™èƒ½åŠ›ï¼Œä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„æŒç»­å­¦ä¹ åœºæ™¯æä¾›äº†å®ç”¨ã€ç†è®ºæ‰å®å’Œè®¡ç®—å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æŒç»­å­¦ä¹ ä¸­é¢ä¸´ç¾éš¾æ€§é—å¿˜é—®é¢˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¾èµ–ä½é˜¶ã€å‚æ•°é«˜æ•ˆçš„æ›´æ–°ï¼Œè¿™é™åˆ¶äº†æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›å’Œå¸¦æ¥äº†å¯æ‰©å±•æ€§é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æŒç»­å…¨å¾®è°ƒæ–¹æ³•ï¼Œåˆ©ç”¨è‡ªé€‚åº”å¥‡å¼‚å€¼åˆ†è§£ï¼ˆSVDï¼‰ã€‚</li>
<li>åŠ¨æ€è¯†åˆ«ç‰¹å®šä»»åŠ¡çš„ä½é˜¶å‚æ•°å­ç©ºé—´ï¼Œå¹¶çº¦æŸæ›´æ–°ä¸å…ˆå‰ä»»åŠ¡çš„å…³é”®æ–¹å‘æ­£äº¤ã€‚</li>
<li>æ–¹æ³•æ— éœ€é¢å¤–çš„å‚æ•°å¼€é”€æˆ–å­˜å‚¨å…ˆå‰ä»»åŠ¡çš„æ¢¯åº¦ï¼Œæœ‰æ•ˆåœ°æœ€å°åŒ–å¹²æ‰°ã€‚</li>
<li>åœ¨å¤šç§ä»»åŠ¡å’Œæ ‡å‡†åŸºå‡†æµ‹è¯•ä¸Šå¯¹æ¨¡å‹è¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ï¼Œç»éªŒç»“æœè¡¨æ˜è¯¥æ–¹æ³•è¾¾åˆ°äº†æœ€æ–°çš„ç»“æœï¼Œå¹¶æ˜¾è‘—åœ°ä¿æŒäº†æ¨¡å‹çš„å„ç§èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07097">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6e3c55b3147d51bc81f7b9bb38d19161.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bcda3edbbe279e0ed1f29940631aa14b.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="TASTE-Text-Aligned-Speech-Tokenization-and-Embedding-for-Spoken-Language-Modeling"><a href="#TASTE-Text-Aligned-Speech-Tokenization-and-Embedding-for-Spoken-Language-Modeling" class="headerlink" title="TASTE: Text-Aligned Speech Tokenization and Embedding for Spoken   Language Modeling"></a>TASTE: Text-Aligned Speech Tokenization and Embedding for Spoken   Language Modeling</h2><p><strong>Authors:Liang-Hsuan Tseng, Yi-Chang Chen, Kuan-Yi Lee, Da-Shan Shiu, Hung-yi Lee</strong></p>
<p>Large Language Models (LLMs) excel in text-based natural language processing tasks but remain constrained by their reliance on textual inputs and outputs. To enable more natural human-LLM interaction, recent progress have focused on deriving a spoken language model (SLM) that can not only listen but also generate speech. To achieve this, a promising direction is to conduct speech-text joint modeling. However, recent SLM still lag behind text LLM due to the modality mismatch. One significant mismatch can be the sequence lengths between speech and text tokens. To address this, we introduce Text-Aligned Speech Tokenization and Embedding (TASTE), a method that directly addresses the modality gap by aligning speech token with the corresponding text transcription during the tokenization stage. We propose a method that can achieve this through the special aggregation mechanism and with speech reconstruction as the training objective. We conduct extensive experiments and show that TASTE can preserve essential paralinguistic information while dramatically reducing the token sequence length. Furthermore, by leveraging TASTE, we can adapt text-based LLMs into effective SLMs with parameter-efficient fine-tuning techniques such as Low-Rank Adaptation (LoRA). Experimental results on benchmark tasks, including SALMON and StoryCloze, demonstrate that TASTE-based SLMs perform similarly to previous full-finetuning methods. To our knowledge, TASTE is the first end-to-end approach that utilizes a reconstruction objective to automatically learn a text-aligned speech tokenization and embedding suitable for spoken language modeling. Our demo, code, and models are publicly available at <a target="_blank" rel="noopener" href="https://github.com/mtkresearch/TASTE-SpokenLM">https://github.com/mtkresearch/TASTE-SpokenLM</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨åŸºäºæ–‡æœ¬çš„è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†å—é™äºå…¶å¯¹æ–‡æœ¬è¾“å…¥å’Œè¾“å‡ºçš„ä¾èµ–ã€‚ä¸ºäº†ä¿ƒè¿›æ›´è‡ªç„¶çš„äººæœºäº¤äº’ï¼Œæœ€è¿‘çš„è¿›å±•ä¸»è¦é›†ä¸­åœ¨å¼€å‘ä¸€ç§å£è¯­è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰ï¼Œè¿™ç§æ¨¡å‹ä¸ä»…è¦èƒ½å¬ï¼Œè¿˜è¦èƒ½ç”Ÿæˆè¯­éŸ³ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œä¸€ä¸ªå……æ»¡å¸Œæœ›çš„é€”å¾„æ˜¯è¿›è¡Œè¯­éŸ³æ–‡æœ¬çš„è”åˆå»ºæ¨¡ã€‚ç„¶è€Œï¼Œç”±äºæ¨¡æ€ä¸åŒ¹é…çš„é—®é¢˜ï¼Œæœ€è¿‘çš„SLMä»ç„¶è½åäºæ–‡æœ¬LLMã€‚ä¸€ä¸ªé‡è¦çš„ä¸åŒ¹é…åœ¨äºè¯­éŸ³å’Œæ–‡æœ¬æ ‡è®°ä¹‹é—´çš„åºåˆ—é•¿åº¦ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ–‡æœ¬å¯¹é½è¯­éŸ³æ ‡è®°åŒ–ä¸åµŒå…¥ï¼ˆTASTEï¼‰ï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡æ ‡è®°åŒ–é˜¶æ®µå°†è¯­éŸ³æ ‡è®°ä¸ç›¸åº”çš„æ–‡æœ¬è½¬å½•è¿›è¡Œå¯¹é½ï¼Œç›´æ¥è§£å†³æ¨¡æ€å·®å¼‚çš„æ–¹æ³•ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§é€šè¿‡ç‰¹æ®Šçš„èšåˆæœºåˆ¶å’Œä»¥è¯­éŸ³é‡å»ºä¸ºè®­ç»ƒç›®æ ‡çš„æ–¹æ³•æ¥å®ç°è¿™ä¸€ç‚¹ã€‚æˆ‘ä»¬è¿›è¡Œäº†å¤§é‡å®éªŒï¼Œç»“æœè¡¨æ˜TASTEå¯ä»¥ä¿ç•™å…³é”®çš„å‰¯è¯­è¨€ä¿¡æ¯ï¼ŒåŒæ—¶æ˜¾è‘—ç¼©çŸ­æ ‡è®°åºåˆ—é•¿åº¦ã€‚æ­¤å¤–ï¼Œé€šè¿‡åˆ©ç”¨TASTEï¼Œæˆ‘ä»¬å¯ä»¥å°†åŸºäºæ–‡æœ¬çš„LLMé€‚åº”ä¸ºæœ‰æ•ˆçš„SLMï¼Œé‡‡ç”¨å‚æ•°é«˜æ•ˆçš„å¾®è°ƒæŠ€æœ¯ï¼Œå¦‚ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰ã€‚åœ¨åŒ…æ‹¬SALMONå’ŒStoryClozeåœ¨å†…çš„åŸºå‡†ä»»åŠ¡ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºTASTEçš„SLMä¸ä¹‹å‰çš„å®Œå…¨å¾®è°ƒæ–¹æ³•è¡¨ç°ç›¸ä¼¼ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼ŒTASTEæ˜¯åˆ©ç”¨é‡å»ºç›®æ ‡è‡ªåŠ¨å­¦ä¹ é€‚åˆå£è¯­å»ºæ¨¡çš„æ–‡æœ¬å¯¹é½è¯­éŸ³æ ‡è®°åŒ–ä¸åµŒå…¥çš„é¦–ä¸ªç«¯åˆ°ç«¯æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ¼”ç¤ºã€ä»£ç å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/mtkresearch/TASTE-SpokenLM%E4%B8%8A%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/mtkresearch/TASTE-SpokenLMä¸Šå…¬å¼€è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07053v1">PDF</a> Preprint. Work in progress</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä¸»è¦ä»‹ç»äº†å¦‚ä½•é€šè¿‡å¼•å…¥TASTEæŠ€æœ¯å®ç°ä»æ–‡æœ¬å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å‘å£è¯­åŒ–è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰çš„è½¬å˜ã€‚TASTEæŠ€æœ¯èƒ½å¤Ÿåœ¨è¯­éŸ³è¯†åˆ«é˜¶æ®µå®ç°è¯­éŸ³å’Œæ–‡å­—çš„å¯¹é½ï¼Œå¹¶é€šè¿‡ç‰¹æ®Šèšåˆæœºåˆ¶å’Œè¯­éŸ³é‡å»ºè®­ç»ƒç›®æ ‡æ¥è§£å†³è¯­éŸ³å’Œæ–‡å­—æ¨¡æ€ä¹‹é—´çš„å·®è·é—®é¢˜ã€‚å®éªŒè¡¨æ˜ï¼ŒTASTEæŠ€æœ¯èƒ½å¤Ÿä¿ç•™é‡è¦çš„è¯­è¨€ä¿¡æ¯ï¼ŒåŒæ—¶æ˜¾è‘—å‡å°‘ä»¤ç‰Œåºåˆ—é•¿åº¦ã€‚åˆ©ç”¨TASTEæŠ€æœ¯å’Œä½ç§©é€‚åº”ï¼ˆLoRAï¼‰ç­‰å‚æ•°é«˜æ•ˆå¾®è°ƒæŠ€æœ¯ï¼Œå¯ä»¥å°†åŸºäºæ–‡æœ¬çš„LLMé€‚åº”ä¸ºæœ‰æ•ˆçš„SLMã€‚åœ¨åŸºå‡†ä»»åŠ¡ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºTASTEçš„SLMæ€§èƒ½ä¸ä¹‹å‰çš„å®Œå…¨å¾®è°ƒæ–¹æ³•ç›¸ä¼¼ã€‚TASTEæ˜¯é¦–ä¸ªåˆ©ç”¨é‡å»ºç›®æ ‡è‡ªåŠ¨å­¦ä¹ é€‚åˆå£è¯­å»ºæ¨¡çš„æ–‡æœ¬å¯¹é½è¯­éŸ³ä»¤ç‰ŒåŒ–å’ŒåµŒå…¥çš„ç«¯åˆ°ç«¯æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è™½ç„¶æ“…é•¿æ–‡æœ¬å¤„ç†ä»»åŠ¡ï¼Œä½†ç¼ºä¹å¤„ç†å£è¯­ä»»åŠ¡çš„èƒ½åŠ›ã€‚ä¸ºäº†ä¿ƒè¿›æ›´è‡ªç„¶çš„äººæœºäº¤äº’ï¼Œç ”ç©¶é›†ä¸­åœ¨å¼€å‘å£è¯­è¯­è¨€æ¨¡å‹ï¼ˆSLMsï¼‰ã€‚</li>
<li>è¯­éŸ³å’Œæ–‡å­—æ¨¡æ€ä¹‹é—´çš„å·®è·æ˜¯é˜»ç¢SLMå‘å±•çš„å…³é”®å› ç´ ä¹‹ä¸€ï¼Œå…¶ä¸­åºåˆ—é•¿åº¦ä¸åŒ¹é…æ˜¯æ˜¾è‘—çš„é—®é¢˜ã€‚</li>
<li>TASTEæŠ€æœ¯é€šè¿‡è¯­éŸ³å’Œæ–‡å­—çš„å¯¹é½è§£å†³äº†æ¨¡æ€å·®è·é—®é¢˜ï¼Œå®ç°äº†è¯­éŸ³ä»¤ç‰Œä¸ç›¸åº”æ–‡æœ¬è½¬å½•çš„å¯¹é½ã€‚</li>
<li>TASTEæŠ€æœ¯é€šè¿‡ç‰¹æ®Šèšåˆæœºåˆ¶å’Œè¯­éŸ³é‡å»ºè®­ç»ƒç›®æ ‡å®ç°æ¨¡æ€å¯¹é½ã€‚</li>
<li>TASTEèƒ½å¤Ÿä¿ç•™é‡è¦çš„è¯­è¨€ä¿¡æ¯ï¼ŒåŒæ—¶æ˜¾è‘—å‡å°‘ä»¤ç‰Œåºåˆ—é•¿åº¦ã€‚</li>
<li>åˆ©ç”¨TASTEå’Œå‚æ•°é«˜æ•ˆå¾®è°ƒæŠ€æœ¯ï¼ˆå¦‚Low-Rank Adaptationï¼‰ï¼Œå¯ä»¥å°†æ–‡æœ¬LLMè½¬åŒ–ä¸ºæœ‰æ•ˆçš„SLMã€‚</li>
<li>åœ¨åŸºå‡†ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒåŸºäºTASTEçš„SLMæ€§èƒ½ä¸å®Œå…¨å¾®è°ƒæ–¹æ³•ç›¸å½“ï¼Œè€ŒTASTEæ˜¯é¦–ä¸ªé‡‡ç”¨é‡å»ºç›®æ ‡è‡ªåŠ¨å­¦ä¹ é€‚åˆå£è¯­å»ºæ¨¡çš„ç«¯åˆ°ç«¯æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07053">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-6c6511d395991bc4ec90d303d2d95d70.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3a9dfe8e138cf0d8c8806763a4cf922a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5ce9ce39295a4d6610623530342063f6.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="To-Backtrack-or-Not-to-Backtrack-When-Sequential-Search-Limits-Model-Reasoning"><a href="#To-Backtrack-or-Not-to-Backtrack-When-Sequential-Search-Limits-Model-Reasoning" class="headerlink" title="To Backtrack or Not to Backtrack: When Sequential Search Limits Model   Reasoning"></a>To Backtrack or Not to Backtrack: When Sequential Search Limits Model   Reasoning</h2><p><strong>Authors:Tian Qin, David Alvarez-Melis, Samy Jelassi, Eran Malach</strong></p>
<p>Recent advancements in large language models have significantly improved their reasoning abilities, particularly through techniques involving search and backtracking. Backtracking naturally scales test-time compute by enabling sequential, linearized exploration via long chain-of-thought (CoT) generation. However, this is not the only strategy for scaling test-time compute: parallel sampling with best-of-n selection provides an alternative that generates diverse solutions simultaneously. Despite the growing adoption of sequential search, its advantages over parallel samplingâ€“especially under a fixed compute budget remain poorly understood. In this paper, we systematically compare these two approaches on two challenging reasoning tasks: CountDown and Sudoku. Surprisingly, we find that sequential search underperforms parallel sampling on CountDown but outperforms it on Sudoku, suggesting that backtracking is not universally beneficial. We identify two factors that can cause backtracking to degrade performance: (1) training on fixed search traces can lock models into suboptimal strategies, and (2) explicit CoT supervision can discourage â€œimplicitâ€ (non-verbalized) reasoning. Extending our analysis to reinforcement learning (RL), we show that models with backtracking capabilities benefit significantly from RL fine-tuning, while models without backtracking see limited, mixed gains. Together, these findings challenge the assumption that backtracking universally enhances LLM reasoning, instead revealing a complex interaction between task structure, training data, model scale, and learning paradigm. </p>
<blockquote>
<p>æœ€è¿‘çš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›å±•é€šè¿‡æœç´¢å’Œå›æº¯ç­‰æŠ€æœ¯æ˜¾è‘—æé«˜äº†å…¶æ¨ç†èƒ½åŠ›ã€‚å›æº¯é€šè¿‡çº¿æ€§åŒ–æ¢ç´¢åºåˆ—ï¼ˆé€šè¿‡é•¿é“¾æ€ç»´ç”Ÿæˆï¼‰ï¼Œè‡ªç„¶åœ°æ‰©å¤§äº†æµ‹è¯•æ—¶é—´è®¡ç®—ã€‚ç„¶è€Œï¼Œè¿™å¹¶ä¸æ˜¯æ‰©å¤§æµ‹è¯•æ—¶é—´è®¡ç®—è§„æ¨¡çš„å”¯ä¸€ç­–ç•¥ï¼šä½¿ç”¨æœ€ä½³nå€¼é€‰æ‹©è¿›è¡Œå¹¶è¡Œé‡‡æ ·æä¾›äº†ä¸€ç§èƒ½å¤ŸåŒæ—¶ç”Ÿæˆå¤šç§è§£å†³æ–¹æ¡ˆçš„æ–¹æ³•ã€‚å°½ç®¡é¡ºåºæœç´¢çš„é‡‡ç”¨è¶Šæ¥è¶Šå¤šï¼Œä½†å…¶ç›¸å¯¹äºå¹¶è¡Œé‡‡æ ·çš„ä¼˜åŠ¿â€”â€”ç‰¹åˆ«æ˜¯åœ¨å›ºå®šè®¡ç®—é¢„ç®—ä¸‹â€”â€”ä»ç„¶é²œä¸ºäººçŸ¥ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬åœ¨ä¸¤ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ¨ç†ä»»åŠ¡CountDownå’Œæ•°ç‹¬ä¸Šç³»ç»Ÿåœ°æ¯”è¾ƒäº†è¿™ä¸¤ç§æ–¹æ³•ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç°é¡ºåºæœç´¢åœ¨CountDownä¸Šçš„è¡¨ç°ä¸å¦‚å¹¶è¡Œé‡‡æ ·ï¼Œä½†åœ¨æ•°ç‹¬ä¸Šè¡¨ç°æ›´å¥½ï¼Œè¿™è¡¨æ˜å›æº¯å¹¶éæ™®éæœ‰ç›Šã€‚æˆ‘ä»¬ç¡®å®šäº†å¯¼è‡´å›æº¯é™ä½æ€§èƒ½çš„å¦å¤–ä¸¤ä¸ªå› ç´ ï¼šï¼ˆ1ï¼‰åœ¨å›ºå®šæœç´¢è½¨è¿¹ä¸Šè¿›è¡Œè®­ç»ƒä¼šä½¿æ¨¡å‹é™·å…¥æ¬¡ä¼˜ç­–ç•¥ï¼›ï¼ˆ2ï¼‰æ˜ç¡®çš„æ€ç»´è½¨è¿¹ç›‘ç£å¯èƒ½ä¼šæŠ‘åˆ¶â€œéšæ€§â€ï¼ˆæœªè¨€æ˜çš„ï¼‰æ¨ç†ã€‚æˆ‘ä»¬å°†åˆ†ææ‰©å±•åˆ°å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ï¼Œå‘ç°å…·æœ‰å›æº¯èƒ½åŠ›çš„æ¨¡å‹ä»RLå¾®è°ƒä¸­å—ç›ŠåŒªæµ…ï¼Œè€Œæ²¡æœ‰å›æº¯çš„æ¨¡å‹åˆ™æ”¶ç›Šæœ‰é™ä¸”è¡¨ç°ä¸ä¸€ã€‚æ€»çš„æ¥è¯´ï¼Œè¿™äº›å‘ç°æŒ‘æˆ˜äº†å›æº¯æ™®éæé«˜å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„å‡è®¾ï¼Œç›¸åæ­ç¤ºäº†ä»»åŠ¡ç»“æ„ã€è®­ç»ƒæ•°æ®ã€æ¨¡å‹è§„æ¨¡å’Œå­¦ä¹ èŒƒå¼ä¹‹é—´çš„å¤æ‚äº¤äº’ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07052v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹çš„æœ€æ–°è¿›å±•é€šè¿‡æœç´¢å’Œå›æº¯ç­‰æŠ€æœ¯æ˜¾è‘—æé«˜äº†å…¶æ¨ç†èƒ½åŠ›ã€‚æœ¬æ–‡ç³»ç»Ÿåœ°æ¯”è¾ƒäº†å›æº¯çš„ä¸²è¡Œæœç´¢ä¸å¹¶è¡Œé‡‡æ ·çš„æ–¹æ³•ï¼Œåœ¨å€’è®¡æ—¶å’Œæ•°ç‹¬ä¸¤é¡¹æŒ‘æˆ˜æ€§ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œå‘ç°æ²¡æœ‰ä¸€ç§æ–¹æ³•æ˜¯æ™®éæœ‰ç›Šçš„ã€‚ç ”ç©¶å‘ç°è®­ç»ƒå›ºå®šçš„æœç´¢ç—•è¿¹ä¼šä½¿æ¨¡å‹é™·å…¥æ¬¡ä¼˜ç­–ç•¥ï¼Œè€Œæ˜ç¡®çš„æ€ç»´é“¾ç›‘ç£å¯èƒ½ä¼šé˜»ç¢â€œéšæ€§â€æ¨ç†ã€‚å¼ºåŒ–å­¦ä¹ å¾®è°ƒå…·æœ‰å›æº¯èƒ½åŠ›çš„æ¨¡å‹æœ‰æ˜¾è‘—å¥½å¤„ï¼Œè€Œç¼ºå°‘å›æº¯çš„æ¨¡å‹åˆ™æ”¶æ•ˆç”šå¾®ã€‚è¿™äº›å‘ç°æŒ‘æˆ˜äº†å›æº¯æ™®éæé«˜å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„å‡è®¾ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹çš„æœ€æ–°è¿›å±•æé«˜äº†å…¶æ¨ç†èƒ½åŠ›ï¼Œä¸»è¦é€šè¿‡æœç´¢å’Œå›æº¯ç­‰æŠ€æœ¯å®ç°ã€‚</li>
<li>æ–‡ä¸­å¯¹æ¯”äº†ä¸²è¡Œæœç´¢çš„å›æº¯æ–¹æ³•ä¸å¹¶è¡Œé‡‡æ ·çš„æ•ˆæœï¼Œå‘ç°åœ¨ä¸åŒä»»åŠ¡ä¸­è¡¨ç°ä¸ä¸€ã€‚</li>
<li>è®­ç»ƒå›ºå®šçš„æœç´¢ç—•è¿¹å¯èƒ½å¯¼è‡´æ¨¡å‹é™·å…¥æ¬¡ä¼˜ç­–ç•¥ã€‚</li>
<li>æ˜ç¡®çš„æ€ç»´é“¾ç›‘ç£å¯èƒ½é˜»ç¢â€œéšæ€§â€æ¨ç†ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ å¾®è°ƒå¯¹å…·æœ‰å›æº¯èƒ½åŠ›çš„æ¨¡å‹æœ‰æ˜¾è‘—å¥½å¤„ã€‚</li>
<li>æ²¡æœ‰ä¸€ç§æ–¹æ³•æ˜¯æ™®éæœ‰ç›Šçš„ï¼Œä»»åŠ¡ç»“æ„ã€è®­ç»ƒæ•°æ®ã€æ¨¡å‹è§„æ¨¡å’Œå­¦ä¹ èŒƒå¼ä¹‹é—´æœ‰ç€å¤æ‚çš„äº¤äº’ä½œç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07052">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-798465083b46719d939d9bc7f255aa6a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f4822fc53d57bfd94c4767329ce7272e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7282154e24661a07398e218a4ed69737.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7dd5a45dc6963f32584010916e3a1b70.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Distilling-Textual-Priors-from-LLM-to-Efficient-Image-Fusion"><a href="#Distilling-Textual-Priors-from-LLM-to-Efficient-Image-Fusion" class="headerlink" title="Distilling Textual Priors from LLM to Efficient Image Fusion"></a>Distilling Textual Priors from LLM to Efficient Image Fusion</h2><p><strong>Authors:Ran Zhang, Xuanhua He, Ke Cao, Liu Liu, Li Zhang, Man Zhou, Jie Zhang</strong></p>
<p>Multi-modality image fusion aims to synthesize a single, comprehensive image from multiple source inputs. Traditional approaches, such as CNNs and GANs, offer efficiency but struggle to handle low-quality or complex inputs. Recent advances in text-guided methods leverage large model priors to overcome these limitations, but at the cost of significant computational overhead, both in memory and inference time. To address this challenge, we propose a novel framework for distilling large model priors, eliminating the need for text guidance during inference while dramatically reducing model size. Our framework utilizes a teacher-student architecture, where the teacher network incorporates large model priors and transfers this knowledge to a smaller student network via a tailored distillation process. Additionally, we introduce spatial-channel cross-fusion module to enhance the modelâ€™s ability to leverage textual priors across both spatial and channel dimensions. Our method achieves a favorable trade-off between computational efficiency and fusion quality. The distilled network, requiring only 10% of the parameters and inference time of the teacher network, retains 90% of its performance and outperforms existing SOTA methods. Extensive experiments demonstrate the effectiveness of our approach. The implementation will be made publicly available as an open-source resource. </p>
<blockquote>
<p>å¤šæ¨¡æ€å›¾åƒèåˆæ—¨åœ¨ä»å¤šä¸ªæºè¾“å…¥ä¸­åˆæˆå•ä¸€ã€å…¨é¢çš„å›¾åƒã€‚ä¼ ç»Ÿæ–¹æ³•ï¼Œå¦‚å·ç§¯ç¥ç»ç½‘ç»œå’Œç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼Œè™½ç„¶æ•ˆç‡é«˜ï¼Œä½†éš¾ä»¥å¤„ç†ä½è´¨é‡æˆ–å¤æ‚çš„è¾“å…¥ã€‚æœ€è¿‘çš„æ–‡æœ¬å¼•å¯¼æ–¹æ³•åˆ©ç”¨å¤§å‹æ¨¡å‹å…ˆéªŒçŸ¥è¯†æ¥å…‹æœè¿™äº›é™åˆ¶ï¼Œä½†è¿™åœ¨å†…å­˜å’Œæ¨ç†æ—¶é—´æ–¹é¢é€ æˆäº†å·¨å¤§çš„è®¡ç®—å¼€é”€ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„å¤§å‹æ¨¡å‹å…ˆéªŒçŸ¥è¯†æç‚¼æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åœ¨æ¨ç†è¿‡ç¨‹ä¸­æ— éœ€æ–‡æœ¬å¼•å¯¼ï¼ŒåŒæ—¶å¤§å¹…å‡å°äº†æ¨¡å‹ä½“ç§¯ã€‚æˆ‘ä»¬çš„æ¡†æ¶é‡‡ç”¨å¸ˆå¾’æ¶æ„ï¼Œå…¶ä¸­æ•™å¸ˆç½‘ç»œèå…¥å¤§å‹æ¨¡å‹å…ˆéªŒçŸ¥è¯†ï¼Œå¹¶é€šè¿‡å®šåˆ¶æç‚¼è¿‡ç¨‹å°†è¿™äº›çŸ¥è¯†è½¬ç§»ç»™è¾ƒå°çš„å­¦ç”Ÿç½‘ç»œã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç©ºé—´é€šé“äº¤å‰èåˆæ¨¡å—ï¼Œä»¥æé«˜æ¨¡å‹åœ¨ç©ºé—´å’Œé€šé“ç»´åº¦ä¸Šåˆ©ç”¨æ–‡æœ¬å…ˆéªŒçŸ¥è¯†çš„èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨è®¡ç®—æ•ˆç‡å’Œèåˆè´¨é‡ä¹‹é—´å–å¾—äº†æœ‰åˆ©çš„å¹³è¡¡ã€‚ç²¾ç‚¼çš„ç½‘ç»œä»…éœ€æ•™å¸ˆç½‘ç»œçš„10%çš„å‚æ•°å’Œæ¨ç†æ—¶é—´ï¼Œå³å¯ä¿ç•™90%çš„æ€§èƒ½å¹¶è¶…è¶Šç°æœ‰çš„æœ€ä½³æ–¹æ³•ã€‚å¤§é‡å®éªŒè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚å®ç°å°†ä½œä¸ºå¼€æºèµ„æºå…¬å¼€å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07029v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œç”¨äºè’¸é¦å¤§å‹æ¨¡å‹å…ˆéªŒçŸ¥è¯†ï¼Œç”¨äºå¤šæ¨¡æ€å›¾åƒèåˆä»»åŠ¡ã€‚è¯¥æ¡†æ¶é‡‡ç”¨æ•™å¸ˆ-å­¦ç”Ÿæ¶æ„ï¼Œé€šè¿‡å®šåˆ¶è’¸é¦è¿‡ç¨‹å°†å¤§å‹æ¨¡å‹çš„çŸ¥è¯†è½¬ç§»åˆ°å°å‹å­¦ç”Ÿç½‘ç»œã€‚å¼•å…¥ç©ºé—´é€šé“äº¤å‰èåˆæ¨¡å—ï¼Œæé«˜æ¨¡å‹åœ¨ç©ºé—´å’Œé€šé“ç»´åº¦ä¸Šåˆ©ç”¨æ–‡æœ¬å…ˆéªŒçŸ¥è¯†çš„èƒ½åŠ›ã€‚è¯¥æ–¹æ³•åœ¨è®¡ç®—æ•ˆç‡å’Œèåˆè´¨é‡ä¹‹é—´å®ç°äº†æœ‰åˆ©çš„æƒè¡¡ï¼Œè’¸é¦ç½‘ç»œä»…éœ€è¦æ•™å¸ˆç½‘ç»œ10%çš„å‚æ•°å’Œæ¨ç†æ—¶é—´ï¼Œä¿ç•™90%çš„æ€§èƒ½ï¼Œå¹¶ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºä¸€ç§æ–°é¢–çš„æ¡†æ¶ç”¨äºå¤šæ¨¡æ€å›¾åƒèåˆä¸­çš„æ¨¡å‹è’¸é¦ã€‚</li>
<li>é‡‡ç”¨æ•™å¸ˆ-å­¦ç”Ÿç½‘ç»œæ¶æ„è¿›è¡ŒçŸ¥è¯†è½¬ç§»ã€‚</li>
<li>å¼•å…¥ç©ºé—´é€šé“äº¤å‰èåˆæ¨¡å—ï¼Œæé«˜åˆ©ç”¨æ–‡æœ¬å…ˆéªŒçŸ¥è¯†çš„èƒ½åŠ›ã€‚</li>
<li>å®ç°äº†è®¡ç®—æ•ˆç‡å’Œèåˆè´¨é‡ä¹‹é—´çš„æœ‰åˆ©æƒè¡¡ã€‚</li>
<li>è’¸é¦ç½‘ç»œæ€§èƒ½ä¼˜å¼‚ï¼Œä»…éœ€è¦æ•™å¸ˆç½‘ç»œçš„10%å‚æ•°å’Œæ¨ç†æ—¶é—´ã€‚</li>
<li>æ¡†æ¶è¶…è¶Šäº†ç°æœ‰æœ€å…ˆè¿›çš„å›¾åƒèåˆæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07029">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ab5948f8a8a264f376244c8456993363.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b5c536e45c73588cc9081fc2252eadca.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-24ae04eb524c549cb7c5de7e2babab92.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-589549acfca3d7482690b0caf4fd5d30.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Evaluating-Retrieval-Augmented-Generative-Models-for-Document-Queries-in-Transportation-Safety"><a href="#Evaluating-Retrieval-Augmented-Generative-Models-for-Document-Queries-in-Transportation-Safety" class="headerlink" title="Evaluating Retrieval Augmented Generative Models for Document Queries in   Transportation Safety"></a>Evaluating Retrieval Augmented Generative Models for Document Queries in   Transportation Safety</h2><p><strong>Authors:Chad Melton, Alex Sorokine, Steve Peterson</strong></p>
<p>Applications of generative Large Language Models LLMs are rapidly expanding across various domains, promising significant improvements in workflow efficiency and information retrieval. However, their implementation in specialized, high-stakes domains such as hazardous materials transportation is challenging due to accuracy and reliability concerns. This study evaluates the performance of three fine-tuned generative models, ChatGPT, Googleâ€™s Vertex AI, and ORNL Retrieval Augmented Generation augmented LLaMA 2 and LLaMA in retrieving regulatory information essential for hazardous material transportation compliance in the United States. Utilizing approximately 40 publicly available federal and state regulatory documents, we developed 100 realistic queries relevant to route planning and permitting requirements. Responses were qualitatively rated based on accuracy, detail, and relevance, complemented by quantitative assessments of semantic similarity between model outputs. Results demonstrated that the RAG-augmented LLaMA models significantly outperformed Vertex AI and ChatGPT, providing more detailed and generally accurate information, despite occasional inconsistencies. This research introduces the first known application of RAG in transportation safety, emphasizing the need for domain-specific fine-tuning and rigorous evaluation methodologies to ensure reliability and minimize the risk of inaccuracies in high-stakes environments. </p>
<blockquote>
<p>ç”Ÿæˆå¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„åº”ç”¨æ­£åœ¨å„ä¸ªé¢†åŸŸè¿…é€Ÿæ‰©å±•ï¼Œä¸ºå·¥ä½œæµç¨‹æ•ˆç‡å’Œä¿¡æ¯æ£€ç´¢å¸¦æ¥äº†é‡å¤§æ”¹è¿›çš„å¸Œæœ›ã€‚ç„¶è€Œï¼Œç”±äºå…¶å‡†ç¡®æ€§å’Œå¯é æ€§é—®é¢˜ï¼Œåœ¨å±é™©å“è¿è¾“ç­‰ç‰¹å®šé«˜é£é™©é¢†åŸŸçš„å®æ–½å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æœ¬ç ”ç©¶è¯„ä¼°äº†ä¸‰ç§ç»è¿‡ç²¾ç»†è°ƒæ•´çš„ç”Ÿæˆæ¨¡å‹â€”â€”ChatGPTã€è°·æ­Œçš„Vertex AIå’ŒORNLæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å¢å¼ºçš„LLaMA 2å’ŒLLaMAåœ¨æ£€ç´¢å¯¹ç¾å›½å±é™©å“è¿è¾“åˆè§„è‡³å…³é‡è¦çš„æ³•è§„ä¿¡æ¯æ–¹é¢çš„è¡¨ç°ã€‚æˆ‘ä»¬åˆ©ç”¨çº¦40ä»½å…¬å¼€å¯ç”¨çš„è”é‚¦å’Œå·ç›‘ç®¡æ–‡ä»¶ï¼Œåˆ¶å®šäº†ä¸è·¯çº¿è§„åˆ’å’Œè®¸å¯è¦æ±‚ç›¸å…³çš„100ä¸ªç°å®æŸ¥è¯¢ã€‚æ ¹æ®å‡†ç¡®æ€§ã€è¯¦ç»†æ€§å’Œç›¸å…³æ€§å¯¹ç­”æ¡ˆè¿›è¡Œå®šæ€§è¯„ä»·ï¼Œè¾…ä»¥æ¨¡å‹è¾“å‡ºä¹‹é—´çš„è¯­ä¹‰ç›¸ä¼¼æ€§çš„å®šé‡è¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼ŒRAGå¢å¼ºçš„LLaMAæ¨¡å‹åœ¨æ€»ä½“ä¸Šæ˜¾è‘—ä¼˜äºVertex AIå’ŒChatGPTï¼Œæä¾›äº†æ›´è¯¦ç»†å’Œæ›´å‡†ç¡®çš„ä¿¡æ¯ï¼Œå°½ç®¡å¶å°”ä¼šå‡ºç°ä¸ä¸€è‡´çš„æƒ…å†µã€‚è¯¥ç ”ç©¶ä»‹ç»äº†è¿è¾“å®‰å…¨é¢†åŸŸä¸­RAGçš„ç¬¬ä¸€ä¸ªå·²çŸ¥åº”ç”¨ï¼Œå¼ºè°ƒäº†åœ¨ç¡®ä¿å¯é æ€§å’Œå‡å°‘é«˜é£é™©ç¯å¢ƒä¸­çš„ä¸å‡†ç¡®é£é™©æ–¹é¢ï¼Œéœ€è¦è¿›è¡Œç‰¹å®šçš„é¢†åŸŸç²¾ç»†è°ƒæ•´å’Œä¸¥æ ¼è¯„ä¼°æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07022v1">PDF</a> 14 pages, 3 Figures, 3 tables</p>
<p><strong>Summary</strong></p>
<p>åŸºäºç”Ÿæˆå¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å±é™©ç‰©èµ„è¿è¾“ç­‰é«˜è¦æ±‚é¢†åŸŸåº”ç”¨çš„æŒ‘æˆ˜ï¼Œæœ¬ç ”ç©¶è¯„ä¼°äº†ä¸‰ç§ç²¾ç»†è°ƒæ•´çš„ç”Ÿæˆæ¨¡å‹åœ¨æ£€ç´¢å¯¹ç¾å›½å±é™©ææ–™è¿è¾“åˆè§„è‡³å…³é‡è¦çš„æ³•è§„ä¿¡æ¯æ–¹é¢çš„æ€§èƒ½ã€‚ç»“æœæ˜¾ç¤ºï¼Œé‡‡ç”¨RAGå¢å¼ºçš„LLaMAæ¨¡å‹åœ¨å‡†ç¡®æ€§ã€è¯¦ç»†æ€§å’Œç›¸å…³æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºVertex AIå’ŒChatGPTï¼Œå°½ç®¡å¶æœ‰ä¸ä¸€è‡´æ€§ã€‚æœ¬ç ”ç©¶é¦–æ¬¡å°†RAGåº”ç”¨äºè¿è¾“å®‰å…¨é¢†åŸŸï¼Œå¼ºè°ƒé«˜è¦æ±‚ç¯å¢ƒä¸­éœ€è¦é¢†åŸŸç‰¹å®šçš„ç²¾ç»†è°ƒæ•´åŠä¸¥æ ¼è¯„ä¼°æ–¹æ³•ä»¥ç¡®ä¿å¯é æ€§å’Œæœ€å°åŒ–ä¸å‡†ç¡®çš„é£é™©ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆå¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šä¸ªé¢†åŸŸçš„åº”ç”¨è¿…é€Ÿæ‰©å±•ï¼Œæå‡äº†å·¥ä½œæ•ˆç‡å’Œä¿¡æ¯æ£€ç´¢èƒ½åŠ›ã€‚</li>
<li>åœ¨å±é™©ç‰©èµ„è¿è¾“ç­‰é«˜è¦æ±‚é¢†åŸŸï¼ŒLLMsçš„å®æ–½é¢ä¸´å‡†ç¡®æ€§ä¸å¯é æ€§æŒ‘æˆ˜ã€‚</li>
<li>ç ”ç©¶è¯„ä¼°äº†ä¸‰ç§ç²¾ç»†è°ƒæ•´çš„ç”Ÿæˆæ¨¡å‹åœ¨æ£€ç´¢å±é™©ææ–™è¿è¾“ç›¸å…³æ³•è§„ä¿¡æ¯æ–¹é¢çš„æ€§èƒ½ã€‚</li>
<li>RAGå¢å¼ºçš„LLaMAæ¨¡å‹åœ¨å‡†ç¡®æ€§ã€è¯¦ç»†æ€§å’Œç›¸å…³æ€§æ–¹é¢è¡¨ç°æœ€ä½³ã€‚</li>
<li>LLaMAæ¨¡å‹æ˜¾è‘—ä¼˜äºå…¶ä»–è¯„ä¼°çš„æ¨¡å‹ï¼Œæä¾›äº†æ›´è¯¦ç»†å’Œå‡†ç¡®çš„ä¿¡æ¯ã€‚</li>
<li>åœ¨é«˜è¦æ±‚ç¯å¢ƒä¸­ï¼Œéœ€è¦é¢†åŸŸç‰¹å®šçš„ç²¾ç»†è°ƒæ•´å’Œä¸¥æ ¼è¯„ä¼°æ–¹æ³•æ¥ç¡®ä¿æ¨¡å‹çš„å¯é æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07022">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ac37940f810c34cd770ee57ba03fe36b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-732194d6310ec4fcd493246255584942.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a7cedfc906cd5e66755785e8b8303197.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="VideoChat-R1-Enhancing-Spatio-Temporal-Perception-via-Reinforcement-Fine-Tuning"><a href="#VideoChat-R1-Enhancing-Spatio-Temporal-Perception-via-Reinforcement-Fine-Tuning" class="headerlink" title="VideoChat-R1: Enhancing Spatio-Temporal Perception via Reinforcement   Fine-Tuning"></a>VideoChat-R1: Enhancing Spatio-Temporal Perception via Reinforcement   Fine-Tuning</h2><p><strong>Authors:Xinhao Li, Ziang Yan, Desen Meng, Lu Dong, Xiangyu Zeng, Yinan He, Yali Wang, Yu Qiao, Yi Wang, Limin Wang</strong></p>
<p>Recent advancements in reinforcement learning have significantly advanced the reasoning capabilities of multimodal large language models (MLLMs). While approaches such as Group Relative Policy Optimization (GRPO) and rule-based reward mechanisms demonstrate promise in text and image domains, their application to video understanding remains limited. This paper presents a systematic exploration of Reinforcement Fine-Tuning (RFT) with GRPO for video MLLMs, aiming to enhance spatio-temporal perception while maintaining general capabilities. Our experiments reveal that RFT is highly data-efficient for task-specific improvements. Through multi-task RFT on spatio-temporal perception objectives with limited samples, we develop VideoChat-R1, a powerful video MLLM that achieves state-of-the-art performance on spatio-temporal perception tasks without sacrificing chat ability, while exhibiting emerging spatio-temporal reasoning abilities. Compared to Qwen2.5-VL-7B, VideoChat-R1 boosts performance several-fold in tasks like temporal grounding (+31.8) and object tracking (+31.2). Additionally, it significantly improves on general QA benchmarks such as VideoMME (+0.9), MVBench (+1.0), and Perception Test (+0.9). Our findings underscore the potential of RFT for specialized task enhancement of Video MLLMs. We hope our work offers valuable insights for future RL research in video MLLMs. </p>
<blockquote>
<p>æœ€è¿‘å¼ºåŒ–å­¦ä¹ é¢†åŸŸçš„è¿›å±•æå¤§åœ°æé«˜äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚è™½ç„¶é›†å›¢ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰å’ŒåŸºäºè§„åˆ™çš„å¥–åŠ±æœºåˆ¶ç­‰æ–¹æ³•åœ¨æ–‡æœ¬å’Œå›¾åƒé¢†åŸŸæ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å®ƒä»¬åœ¨è§†é¢‘ç†è§£æ–¹é¢çš„åº”ç”¨ä»ç„¶æœ‰é™ã€‚æœ¬æ–‡ç³»ç»Ÿåœ°æ¢è®¨äº†ç”¨äºè§†é¢‘MLLMçš„å¼ºåŒ–å¾®è°ƒï¼ˆRFTï¼‰ä¸GRPOçš„ç»“åˆï¼Œæ—¨åœ¨æé«˜æ—¶ç©ºæ„ŸçŸ¥èƒ½åŠ›çš„åŒæ—¶ä¿æŒé€šç”¨èƒ½åŠ›ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒRFTå¯¹äºç‰¹å®šä»»åŠ¡çš„æ”¹è¿›éå¸¸æ•°æ®é«˜æ•ˆã€‚é€šè¿‡æœ‰é™æ ·æœ¬çš„æ—¶ç©ºæ„ŸçŸ¥ç›®æ ‡ä¸Šçš„å¤šä»»åŠ¡RFTï¼Œæˆ‘ä»¬å¼€å‘å‡ºäº†VideoChat-R1ï¼Œè¿™æ˜¯ä¸€æ¬¾å¼ºå¤§çš„è§†é¢‘MLLMï¼Œåœ¨æ—¶ç©ºæ„ŸçŸ¥ä»»åŠ¡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¸ç‰ºç‰²èŠå¤©èƒ½åŠ›ï¼Œå¹¶å±•ç°å‡ºæ–°å…´çš„æ—¶ç©ºæ¨ç†èƒ½åŠ›ã€‚ä¸Qwen2.5-VL-7Bç›¸æ¯”ï¼ŒVideoChat-R1åœ¨è¯¸å¦‚æ—¶é—´å®šä½ï¼ˆ+31.8ï¼‰å’Œå¯¹è±¡è·Ÿè¸ªï¼ˆ+31.2ï¼‰çš„ä»»åŠ¡ä¸Šæå‡äº†æ•°å€çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œå®ƒåœ¨é€šç”¨é—®ç­”åŸºå‡†æµ‹è¯•ï¼ˆå¦‚VideoMMEï¼ˆ+0.9ï¼‰ã€MVBenchï¼ˆ+1.0ï¼‰å’Œæ„ŸçŸ¥æµ‹è¯•ï¼ˆ+0.9ï¼‰ï¼‰ä¸Šä¹Ÿå–å¾—äº†æ˜¾è‘—æ”¹å–„ã€‚æˆ‘ä»¬çš„ç ”ç©¶çªå‡ºäº†RFTåœ¨è§†é¢‘MLLMç‰¹å®šä»»åŠ¡å¢å¼ºæ–¹é¢çš„æ½œåŠ›ã€‚æˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„ç ”ç©¶ä¸ºæœªæ¥è§†é¢‘MLLMçš„å¼ºåŒ–å­¦ä¹ ç ”ç©¶æä¾›æœ‰ä»·å€¼çš„è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.06958v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸå¼ºåŒ–å­¦ä¹ åœ¨æå‡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æ¨ç†èƒ½åŠ›æ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ã€‚æœ¬æ–‡æ¢ç´¢äº†ä½¿ç”¨å¼ºåŒ–ç²¾ç»†è°ƒæ•´ï¼ˆRFTï¼‰å’Œé›†å›¢ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰çš„è§†é¢‘MLLMsï¼Œæ—¨åœ¨æé«˜æ—¶ç©ºæ„ŸçŸ¥èƒ½åŠ›çš„åŒæ—¶ä¿æŒä¸€èˆ¬èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼ŒRFTåœ¨ç‰¹å®šä»»åŠ¡æ”¹è¿›æ–¹é¢éå¸¸æ³¨é‡æ•°æ®æ•ˆç‡ã€‚é€šè¿‡æœ‰é™æ ·æœ¬çš„æ—¶ç©ºæ„ŸçŸ¥ç›®æ ‡ä¸Šçš„å¤šä»»åŠ¡RFTï¼Œå¼€å‘å‡ºVideoChat-R1ï¼Œè¯¥è§†é¢‘MLLMåœ¨æ—¶ç©ºæ„ŸçŸ¥ä»»åŠ¡ä¸Šå®ç°æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä¸ç‰ºç‰²èŠå¤©èƒ½åŠ›ï¼Œå¹¶å±•ç°å‡ºæ–°å…´çš„æ—¶ç©ºæ¨ç†èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ åœ¨æå‡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›æ–¹é¢å–å¾—è¿›å±•ã€‚</li>
<li>Group Relative Policy Optimization (GRPO) å’ŒåŸºäºè§„åˆ™çš„å¥–åŠ±æœºåˆ¶åœ¨æ–‡æœ¬å’Œå›¾åƒé¢†åŸŸå…·æœ‰åº”ç”¨å‰æ™¯ã€‚</li>
<li>æœ¬ç ”ç©¶æ¢ç´¢äº†Reinforcement Fine-Tuning (RFT) ä¸ GRPO åœ¨è§†é¢‘MLLMsä¸­çš„åº”ç”¨ã€‚</li>
<li>RFTé«˜åº¦æ³¨é‡æ•°æ®æ•ˆç‡ï¼Œç”¨äºç‰¹å®šä»»åŠ¡çš„æ”¹è¿›ã€‚</li>
<li>é€šè¿‡å¤šä»»åŠ¡RFTï¼ŒæˆåŠŸå¼€å‘å‡ºVideoChat-R1ï¼Œè¯¥æ¨¡å‹åœ¨æ—¶ç©ºæ„ŸçŸ¥ä»»åŠ¡ä¸Šè¡¨ç°å“è¶Šã€‚</li>
<li>VideoChat-R1ç›¸æ¯”Qwen2.5-VL-7Bï¼Œåœ¨æ—¶ç©ºæ„ŸçŸ¥ä»»åŠ¡ä¸Šçš„æ€§èƒ½å¤§å¹…åº¦æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.06958">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-276014d8a0684732775efd6b0c50e2c9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-89b3adc13557b37016357b42472fc020.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8c518c1061834b2dab5dbf91bfa54e1a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d0d3ab1d68f717f51aca88e4c25f7ced.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-247a56547855773e359a20635b8f470e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c392c0aeea22b73fb78f63e50e43e246.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Adaptive-Computation-Pruning-for-the-Forgetting-Transformer"><a href="#Adaptive-Computation-Pruning-for-the-Forgetting-Transformer" class="headerlink" title="Adaptive Computation Pruning for the Forgetting Transformer"></a>Adaptive Computation Pruning for the Forgetting Transformer</h2><p><strong>Authors:Zhixuan Lin, Johan Obando-Ceron, Xu Owen He, Aaron Courville</strong></p>
<p>The recently proposed Forgetting Transformer (FoX) incorporates a forget gate into softmax attention and has shown consistently better or on-par performance compared to the standard RoPE-based Transformer. Notably, many attention heads in FoX tend to forget quickly, causing their output at each timestep to rely primarily on the local context. Based on this observation, we propose Adaptive Computation Pruning (ACP) for FoX, a method that dynamically prunes computations involving input-output dependencies that are strongly decayed by the forget gate. This is achieved using a dynamically set pruning threshold that ensures that the pruned attention weights remain negligible. We apply ACP to language model pretraining with FoX and show it consistently reduces the number of FLOPs in softmax attention by around 70% across different model sizes and context lengths, resulting in a roughly 10% to 35% improvement in training throughput. Furthermore, longer context lengths yield greater computational savings. All these speed improvements are achieved without any performance degradation. We also perform several analyses to provide deeper insights into our method, such as examining the pruning patterns and analyzing the distribution of FLOP savings across different attention heads. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/zhixuan-lin/arctic-fox">https://github.com/zhixuan-lin/arctic-fox</a>. </p>
<blockquote>
<p>æœ€è¿‘æå‡ºçš„é—å¿˜è½¬æ¢å™¨ï¼ˆFoXï¼‰å°†é—å¿˜é—¨çº³å…¥softmaxæ³¨æ„åŠ›æœºåˆ¶ï¼Œä¸åŸºäºRoPEçš„æ ‡å‡†è½¬æ¢å™¨ç›¸æ¯”ï¼Œå…¶æ€§èƒ½ä¸€ç›´è¡¨ç°æ›´å¥½æˆ–ç›¸å½“ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒFoXä¸­çš„è®¸å¤šæ³¨æ„åŠ›å¤´å€¾å‘äºå¿«é€Ÿé—å¿˜ï¼Œå¯¼è‡´å®ƒä»¬åœ¨æ¯ä¸ªæ—¶é—´æ­¥çš„è¾“å‡ºä¸»è¦ä¾èµ–äºå±€éƒ¨ä¸Šä¸‹æ–‡ã€‚åŸºäºè¿™ä¸€è§‚å¯Ÿï¼Œæˆ‘ä»¬ä¸ºFoXæå‡ºäº†è‡ªé€‚åº”è®¡ç®—å‰ªæï¼ˆACPï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åŠ¨æ€å‰ªææ¶‰åŠè¢«é—å¿˜é—¨å¼ºçƒˆè¡°å‡çš„è¾“å…¥-è¾“å‡ºä¾èµ–å…³ç³»çš„è®¡ç®—ã€‚è¿™æ˜¯é€šè¿‡åŠ¨æ€è®¾ç½®å‰ªæé˜ˆå€¼æ¥å®ç°çš„ï¼Œè¯¥é˜ˆå€¼ç¡®ä¿è¢«å‰ªæçš„æ³¨æ„åŠ›æƒé‡ä¿æŒå¾®ä¸è¶³é“ã€‚æˆ‘ä»¬å°†ACPåº”ç”¨äºä½¿ç”¨FoXçš„è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒï¼Œå¹¶è¯æ˜å®ƒèƒ½åœ¨ä¸åŒæ¨¡å‹å¤§å°å’Œä¸Šä¸‹æ–‡é•¿åº¦çš„æƒ…å†µä¸‹ï¼Œå°†softmaxæ³¨æ„åŠ›çš„FLOPsæ•°é‡å‡å°‘çº¦70%ï¼Œä»è€Œå¯¼è‡´è®­ç»ƒååé‡æé«˜çº¦10%è‡³35%ã€‚æ­¤å¤–ï¼Œè¾ƒé•¿çš„ä¸Šä¸‹æ–‡é•¿åº¦å¯ä»¥äº§ç”Ÿæ›´å¤§çš„è®¡ç®—èŠ‚çœã€‚æ‰€æœ‰è¿™äº›é€Ÿåº¦æå‡éƒ½æ²¡æœ‰é€ æˆæ€§èƒ½ä¸‹é™ã€‚æˆ‘ä»¬è¿˜è¿›è¡Œäº†å‡ é¡¹åˆ†æï¼Œä»¥æ›´æ·±å…¥çš„æ–¹å¼äº†è§£æˆ‘ä»¬çš„æ–¹æ³•ï¼Œå¦‚æ£€æŸ¥å‰ªææ¨¡å¼å’Œåˆ†æä¸åŒæ³¨æ„åŠ›å¤´ä¹‹é—´FLOPsèŠ‚çœçš„åˆ†å¸ƒã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/zhixuan-lin/arctic-fox%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/zhixuan-lin/arctic-foxä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.06949v1">PDF</a> Preprint. Under review</p>
<p><strong>Summary</strong></p>
<p>æœ€è¿‘æå‡ºçš„é—å¿˜å˜å‹å™¨ï¼ˆFoXï¼‰åœ¨softmaxæ³¨æ„åŠ›ä¸­èå…¥äº†é—å¿˜é—¨ï¼Œå…¶æ€§èƒ½è¾ƒæ ‡å‡†çš„åŸºäºRoPEçš„Transformerè¡¨ç°æ›´ä¼˜å¼‚æˆ–ç›¸å½“ã€‚è§‚å¯Ÿåˆ°FoXä¸­è®¸å¤šæ³¨æ„åŠ›å¤´å¿«é€Ÿé—å¿˜ï¼Œå¯¼è‡´è¾“å‡ºä¸»è¦ä¾èµ–å±€éƒ¨ä¸Šä¸‹æ–‡ï¼Œå› æ­¤æå‡ºè‡ªé€‚åº”è®¡ç®—å‰ªæï¼ˆACPï¼‰æ–¹æ³•ã€‚ACPé€šè¿‡åŠ¨æ€è®¾å®šå‰ªæé˜ˆå€¼ï¼Œç¡®ä¿è¢«å‰ªæçš„æ³¨æ„åŠ›æƒé‡ä¿æŒå¾®å°ï¼Œå®ç°äº†åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¯¹è®¡ç®—èµ„æºçš„åŠ¨æ€ç®¡ç†ã€‚å°†ACPåº”ç”¨äºè¯­è¨€æ¨¡å‹é¢„è®­ç»ƒçš„FoXä¸Šï¼Œèƒ½æœ‰æ•ˆå‡å°‘softmaxæ³¨æ„åŠ›çš„æµ®ç‚¹è¿ç®—æ¬¡æ•°çº¦70%ï¼ŒåŒæ—¶æé«˜è®­ç»ƒæ•ˆç‡çº¦10%è‡³35%ã€‚æ›´é•¿çš„ä¸Šä¸‹æ–‡é•¿åº¦èƒ½å¸¦æ¥æ›´å¤§çš„è®¡ç®—èŠ‚çœã€‚è¿™äº›æå‡å‡æœªå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚æˆ‘ä»¬è¿˜è¿›è¡Œäº†æ·±å…¥åˆ†æï¼Œå¦‚ç ”ç©¶å‰ªææ¨¡å¼å’Œåˆ†æä¸åŒæ³¨æ„åŠ›å¤´çš„æµ®ç‚¹è¿ç®—èŠ‚çœåˆ†å¸ƒç­‰ã€‚ä»£ç å·²å…¬å¼€åœ¨GitHubä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é—å¿˜å˜å‹å™¨ï¼ˆFoXï¼‰ç»“åˆäº†é—å¿˜é—¨å’Œsoftmaxæ³¨æ„åŠ›æœºåˆ¶ï¼Œå±•ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½è¡¨ç°ã€‚</li>
<li>è®¸å¤šFoXçš„æ³¨æ„åŠ›å¤´å­˜åœ¨å¿«é€Ÿé—å¿˜ç°è±¡ï¼Œå¯¼è‡´è¾“å‡ºä¸»è¦ä¾èµ–å±€éƒ¨ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚</li>
<li>æå‡ºè‡ªé€‚åº”è®¡ç®—å‰ªæï¼ˆACPï¼‰æ–¹æ³•ï¼Œé€šè¿‡åŠ¨æ€ç®¡ç†è®¡ç®—èµ„æºä»¥æé«˜è®­ç»ƒæ•ˆç‡ã€‚</li>
<li>ACPåœ¨FoXè¯­è¨€æ¨¡å‹é¢„è®­ç»ƒä¸­åº”ç”¨å¹¿æ³›ï¼Œèƒ½æœ‰æ•ˆå‡å°‘æµ®ç‚¹è¿ç®—æ¬¡æ•°çº¦70%ï¼Œæé«˜è®­ç»ƒæ•ˆç‡è¾¾10%è‡³35%ã€‚</li>
<li>æ›´é•¿çš„ä¸Šä¸‹æ–‡é•¿åº¦èƒ½å¸¦æ¥æ›´å¤§çš„è®¡ç®—èŠ‚çœã€‚</li>
<li>ACPæ–¹æ³•æœªå¯¼è‡´æ€§èƒ½ä¸‹é™ï¼Œè¯´æ˜å…¶æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.06949">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-65a0ab6bfb9c52c21f1e5c80c4b237eb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a0f2503244bb83957605c9dfa1eb6bd1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-21a54384328ffe2be35d31fe723b5867.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b95a2e33a71f82a8db2e1dc2aad5c785.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1ad215b083f86cc55b17ec795892b430.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c3d6fb39bca5c7fa6c736002d23d507d.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="RuOpinionNE-2024-Extraction-of-Opinion-Tuples-from-Russian-News-Texts"><a href="#RuOpinionNE-2024-Extraction-of-Opinion-Tuples-from-Russian-News-Texts" class="headerlink" title="RuOpinionNE-2024: Extraction of Opinion Tuples from Russian News Texts"></a>RuOpinionNE-2024: Extraction of Opinion Tuples from Russian News Texts</h2><p><strong>Authors:Natalia Loukachevitch, Natalia Tkachenko, Anna Lapanitsyna, Mikhail Tikhomirov, Nicolay Rusnachenko</strong></p>
<p>In this paper, we introduce the Dialogue Evaluation shared task on extraction of structured opinions from Russian news texts. The task of the contest is to extract opinion tuples for a given sentence; the tuples are composed of a sentiment holder, its target, an expression and sentiment from the holder to the target. In total, the task received more than 100 submissions. The participants experimented mainly with large language models in zero-shot, few-shot and fine-tuning formats. The best result on the test set was obtained with fine-tuning of a large language model. We also compared 30 prompts and 11 open source language models with 3-32 billion parameters in the 1-shot and 10-shot settings and found the best models and prompts. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†ä»ä¿„ç½—æ–¯æ–°é—»æ–‡æœ¬ä¸­æå–ç»“æ„åŒ–æ„è§çš„å¯¹è¯è¯„ä»·å…±äº«ä»»åŠ¡ã€‚è¯¥ä»»åŠ¡çš„ç›®æ ‡æ˜¯æå–ç»™å®šå¥å­ä¸­çš„æ„è§å…ƒç»„ï¼Œè¿™äº›å…ƒç»„ç”±æƒ…æ„ŸæŒæœ‰è€…ã€ç›®æ ‡ã€è¡¨è¾¾å’ŒæŒæœ‰è€…å¯¹ç›®æ ‡çš„æƒ…æ„Ÿç»„æˆã€‚è¯¥ä»»åŠ¡å…±æ”¶åˆ°è¶…è¿‡100ä»½æäº¤ã€‚å‚èµ›è€…ä¸»è¦å°è¯•ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œé›¶æ ·æœ¬ã€å°æ ·æœ¬å’Œå¾®è°ƒæ ¼å¼çš„å®éªŒã€‚åœ¨æµ‹è¯•é›†ä¸Šè·å¾—æœ€ä½³ç»“æœçš„æ˜¯å¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„å¾®è°ƒã€‚æˆ‘ä»¬è¿˜æ¯”è¾ƒäº†30ä¸ªæç¤ºå’Œ11ä¸ªå¼€æºè¯­è¨€æ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹åœ¨1åˆ°32äº¿å‚æ•°èŒƒå›´å†…ï¼Œåœ¨å•æ ·æœ¬å’Œåæ ·æœ¬è®¾ç½®ä¸‹ï¼Œæ‰¾åˆ°äº†æœ€ä½³çš„æ¨¡å‹å’Œæç¤ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.06947v1">PDF</a> RuOpinionNE-2024 represent a proceeding of RuSentNE-2023. It   contributes with extraction and evaluation of factual statements that support   the assigned sentiment</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å…³äºä¿„ç½—æ–¯æ–°é—»æ–‡æœ¬ä¸­æå–ç»“æ„åŒ–æ„è§çš„å¯¹è¯è¯„ä»·å…±äº«ä»»åŠ¡ã€‚è¯¥ä»»åŠ¡æ—¨åœ¨ä»ç»™å®šå¥å­ä¸­æå–æ„è§å…ƒç»„ï¼Œè¿™äº›å…ƒç»„ç”±æƒ…æ„ŸæŒæœ‰è€…ã€ç›®æ ‡å¯¹è±¡ã€è¡¨è¾¾æ–¹å¼å’Œæƒ…æ„Ÿç»„æˆã€‚ä»»åŠ¡å…±æ”¶åˆ°è¶…è¿‡100ä»½æäº¤ï¼Œå‚èµ›è€…ä¸»è¦å°è¯•ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œé›¶æ ·æœ¬ã€å°æ ·æœ¬å’Œå¾®è°ƒæ ¼å¼çš„å®éªŒã€‚æœ€ç»ˆï¼Œé€šè¿‡å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šå–å¾—äº†æœ€ä½³ç»“æœã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å¯¹æ¯”äº†ä¸åŒæç¤ºå’Œå¼€æºè¯­è¨€æ¨¡å‹çš„è¡¨ç°ï¼Œæ‰¾åˆ°äº†æœ€ä½³æ¨¡å‹å’Œæç¤ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¯¹è¯è¯„ä»·å…±äº«ä»»åŠ¡æ—¨åœ¨ä»ä¿„ç½—æ–¯æ–°é—»æ–‡æœ¬ä¸­æå–ç»“æ„åŒ–æ„è§ã€‚</li>
<li>ä»»åŠ¡è¦æ±‚æå–æ„è§å…ƒç»„ï¼ŒåŒ…æ‹¬æƒ…æ„ŸæŒæœ‰è€…ã€ç›®æ ‡å¯¹è±¡ã€è¡¨è¾¾æ–¹å¼å’Œæƒ…æ„Ÿã€‚</li>
<li>ä»»åŠ¡å…±æ”¶åˆ°è¶…è¿‡100ä»½æäº¤ã€‚</li>
<li>å‚èµ›è€…ä¸»è¦ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå®éªŒï¼ŒåŒ…æ‹¬é›¶æ ·æœ¬ã€å°æ ·æœ¬å’Œå¾®è°ƒæ ¼å¼ã€‚</li>
<li>é€šè¿‡å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šè·å¾—æœ€ä½³ç»“æœã€‚</li>
<li>å¯¹æ¯”äº†ä¸åŒæç¤ºå’Œå¼€æºè¯­è¨€æ¨¡å‹çš„è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.06947">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-71a543f11957170fa7fb0b1288073517.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e23f8e3c6005e590ac5d09b4d2b1f44d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-511c699b49ec8ea9021ed69967010356.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-90be2db8fa7a1c4c8cb3812d2b3bf2b4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-185739968acdc353220b25f2b9a4cd6c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2fd522424b612a55fb2909f0eedab390.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-55fbe4af20b480a13722e8f4b2b9c118.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="FeedbackEval-A-Benchmark-for-Evaluating-Large-Language-Models-in-Feedback-Driven-Code-Repair-Tasks"><a href="#FeedbackEval-A-Benchmark-for-Evaluating-Large-Language-Models-in-Feedback-Driven-Code-Repair-Tasks" class="headerlink" title="FeedbackEval: A Benchmark for Evaluating Large Language Models in   Feedback-Driven Code Repair Tasks"></a>FeedbackEval: A Benchmark for Evaluating Large Language Models in   Feedback-Driven Code Repair Tasks</h2><p><strong>Authors:Dekun Dai, MingWei Liu, Anji Li, Jialun Cao, Yanlin Wang, Chong Wang, Xin Peng, Zibin Zheng</strong></p>
<p>Code repair is a fundamental task in software development, facilitating efficient bug resolution and software maintenance. Although large language models (LLMs) have demonstrated considerable potential in automated code repair, their ability to comprehend and effectively leverage diverse types of feedback remains insufficiently understood. To bridge this gap, we introduce FeedbackEval, a systematic benchmark for evaluating LLMsâ€™ feedback comprehension and performance in code repair tasks. We conduct a comprehensive empirical study on five state-of-the-art LLMs, including GPT-4o, Claude-3.5, Gemini-1.5, GLM-4, and Qwen2.5, to evaluate their behavior under both single-iteration and iterative code repair settings. Our results show that structured feedback, particularly in the form of test feedback, leads to the highest repair success rates, while unstructured feedback proves significantly less effective. Iterative feedback further enhances repair performance, though the marginal benefit diminishes after two or three rounds. Moreover, prompt structure is shown to be critical: incorporating docstrings, contextual information, and explicit guidelines substantially improves outcomes, whereas persona-based, chain-of-thought, and few-shot prompting strategies offer limited benefits in single-iteration scenarios. This work introduces a robust benchmark and delivers practical insights to advance the understanding and development of feedback-driven code repair using LLMs. </p>
<blockquote>
<p>ä»£ç ä¿®å¤æ˜¯è½¯ä»¶å¼€å‘ä¸­çš„ä¸€é¡¹åŸºæœ¬ä»»åŠ¡ï¼Œèƒ½å¤Ÿä¿ƒè¿›é«˜æ•ˆçš„é”™è¯¯è§£å†³å’Œè½¯ä»¶ç»´æŠ¤ã€‚å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªåŠ¨ä»£ç ä¿®å¤ä¸­å±•ç¤ºäº†å·¨å¤§çš„æ½œåŠ›ï¼Œä½†å®ƒä»¬ç†è§£å’Œæœ‰æ•ˆåˆ©ç”¨å„ç§åé¦ˆçš„èƒ½åŠ›ä»æœªèƒ½å¾—åˆ°å……åˆ†ç†è§£ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†FeedbackEvalï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°LLMåœ¨ä»£ç ä¿®å¤ä»»åŠ¡ä¸­çš„åé¦ˆç†è§£å’Œæ€§èƒ½çš„ç³»ç»Ÿæ€§åŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬å¯¹äº”æ¬¾æœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œäº†å…¨é¢çš„å®è¯ç ”ç©¶ï¼ŒåŒ…æ‹¬GPT-4oã€Claude-3.5ã€Gemini-1.5ã€GLM-4å’ŒQwen2.5ï¼Œä»¥è¯„ä¼°å®ƒä»¬åœ¨å•è½®å’Œè¿­ä»£ä»£ç ä¿®å¤è®¾ç½®ä¸‹çš„è¡Œä¸ºã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œç»“æ„åŒ–çš„åé¦ˆï¼Œå°¤å…¶æ˜¯æµ‹è¯•å½¢å¼çš„åé¦ˆï¼Œä¼šå¯¼è‡´æœ€é«˜çš„ä¿®å¤æˆåŠŸç‡ï¼Œè€Œç»“æ„ä¸è‰¯çš„åé¦ˆè¯æ˜æ•ˆæœè¾ƒå·®ã€‚è¿­ä»£åé¦ˆè¿›ä¸€æ­¥æé«˜äº†ä¿®å¤æ€§èƒ½ï¼Œä½†ä¸¤è½®æˆ–ä¸‰è½®åçš„è¾¹é™…æ•ˆç›Šä¼šé€’å‡ã€‚æ­¤å¤–ï¼Œæç¤ºç»“æ„ä¹Ÿè¢«è¯æ˜æ˜¯å…³é”®çš„ï¼šèå…¥æ–‡æ¡£å­—ç¬¦ä¸²ã€ä¸Šä¸‹æ–‡ä¿¡æ¯å’Œæ˜ç¡®çš„æŒ‡å¯¼å¯ä»¥å¤§å¹…åº¦æ”¹å–„ç»“æœï¼Œè€ŒåŸºäºè§’è‰²çš„æ€è€ƒã€æ€ç»´é“¾å’Œå°‘é‡æç¤ºç­–ç•¥åœ¨å•è½®åœºæ™¯ä¸­æä¾›æœ‰é™çš„ç›Šå¤„ã€‚è¿™é¡¹å·¥ä½œå¼•å…¥äº†ä¸€ä¸ªç¨³å¥çš„åŸºå‡†æµ‹è¯•ï¼Œå¹¶ä¸ºæ¨åŠ¨ä½¿ç”¨LLMè¿›è¡Œåé¦ˆé©±åŠ¨çš„ä»£ç ä¿®å¤çš„ç†è§£å’Œå‘å±•æä¾›äº†å®ç”¨çš„è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.06939v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ä»£ç ä¿®å¤æ˜¯è½¯ä»¶å¼€å‘ä¸­çš„ä¸€é¡¹åŸºæœ¬ä»»åŠ¡ï¼Œèƒ½å¤Ÿä¿ƒè¿›æœ‰æ•ˆçš„é”™è¯¯è§£å†³å’Œè½¯ä»¶ç»´æŠ¤ã€‚å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªåŠ¨ä»£ç ä¿®å¤ä¸­å±•ç¤ºäº†å·¨å¤§çš„æ½œåŠ›ï¼Œä½†å®ƒä»¬ç†è§£å’Œæœ‰æ•ˆåˆ©ç”¨å„ç§åé¦ˆçš„èƒ½åŠ›ä»ä¸å¤Ÿæ˜ç¡®ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†FeedbackEvalï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°LLMåœ¨ä»£ç ä¿®å¤ä»»åŠ¡ä¸­ç†è§£å’Œè¡¨ç°çš„ç³»ç»Ÿæ€§åŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬å¯¹äº”æ¬¾æœ€æ–°LLMè¿›è¡Œäº†å…¨é¢çš„å®è¯ç ”ç©¶ï¼ŒåŒ…æ‹¬GPT-4oã€Claude-3.5ã€Gemini-1.5ã€GLM-4å’ŒQwen2.5ï¼Œä»¥è¯„ä¼°å®ƒä»¬åœ¨å•è½®å’Œè¿­ä»£ä»£ç ä¿®å¤è®¾ç½®ä¸‹çš„è¡¨ç°ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œç»“æ„åŒ–åé¦ˆï¼Œå°¤å…¶æ˜¯æµ‹è¯•åé¦ˆï¼Œå¯¼è‡´æœ€é«˜çš„ä¿®å¤æˆåŠŸç‡ï¼Œè€Œéç»“æ„åŒ–åé¦ˆçš„è¯æ˜æ•ˆæœç”šå¾®ã€‚è¿­ä»£åé¦ˆè¿›ä¸€æ­¥æé«˜äº†ä¿®å¤æ€§èƒ½ï¼Œä½†ä¸¤è½®æˆ–ä¸‰è½®ä¹‹åçš„è¾¹é™…æ•ˆç›Šé€æ¸å‡å°ã€‚æ­¤å¤–ï¼Œæç¤ºç»“æ„è‡³å…³é‡è¦ï¼šèå…¥æ–‡æ¡£å­—ç¬¦ä¸²ã€ä¸Šä¸‹æ–‡ä¿¡æ¯å’Œæ˜ç¡®æŒ‡å¯¼å¤§å¹…æ”¹å–„äº†ç»“æœï¼Œè€ŒåŸºäºä¸ªæ€§ã€æ€ç»´é“¾å’Œå°‘é‡æç¤ºçš„ç­–ç•¥åœ¨å•è½®åœºæ™¯ä¸­æä¾›äº†æœ‰é™çš„æ•ˆç›Šã€‚æœ¬ç ”ç©¶å¼•å…¥äº†ä¸€ä¸ªç¨³å¥çš„åŸºå‡†æµ‹è¯•ï¼Œä¸ºæ¨è¿›åˆ©ç”¨LLMçš„åé¦ˆé©±åŠ¨ä»£ç ä¿®å¤çš„ç†è§£å’Œå¼€å‘æä¾›äº†å®é™…è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªåŠ¨ä»£ç ä¿®å¤ä¸­æœ‰å·¨å¤§æ½œåŠ›ã€‚</li>
<li>åé¦ˆç±»å‹å¯¹LLMåœ¨ä»£ç ä¿®å¤ä¸­çš„è¡¨ç°æœ‰æ˜¾è‘—å½±å“ï¼Œç»“æ„åŒ–åé¦ˆï¼ˆå°¤å…¶æ˜¯æµ‹è¯•åé¦ˆï¼‰æœ€ä¸ºæœ‰æ•ˆã€‚</li>
<li>è¿­ä»£åé¦ˆèƒ½æé«˜LLMçš„ä¿®å¤æ€§èƒ½ï¼Œä½†è¾¹é™…æ•ˆç›Šæœ‰é™ã€‚</li>
<li>æç¤ºç»“æ„å¯¹LLMçš„ä¿®å¤æ•ˆæœè‡³å…³é‡è¦ï¼Œèå…¥æ–‡æ¡£å­—ç¬¦ä¸²å’Œä¸Šä¸‹æ–‡ä¿¡æ¯èƒ½å¤§å¹…æ”¹å–„ç»“æœã€‚</li>
<li>åŸºäºä¸ªæ€§ã€æ€ç»´é“¾å’Œå°‘é‡æç¤ºçš„ç­–ç•¥åœ¨å•è½®ä»£ç ä¿®å¤åœºæ™¯ä¸­æ•ˆæœæœ‰é™ã€‚</li>
<li>FeedbackEvalæ˜¯ä¸€ä¸ªè¯„ä¼°LLMåœ¨ä»£ç ä¿®å¤ä¸­ç†è§£å’Œè¡¨ç°çš„æœ‰æ•ˆåŸºå‡†æµ‹è¯•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.06939">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0809324b1218de11e3b1de190ae9863a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fd4a1f1156e426f6fc59e7cd267d65d8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-856a9ccf86ca892c8cadbb714987eaec.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0d312b96c85d0fe85082c58615ce2c5c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3977ad098c6c31a75ca07a4c3b8d07f3.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Unifying-Search-and-Recommendation-A-Generative-Paradigm-Inspired-by-Information-Theory"><a href="#Unifying-Search-and-Recommendation-A-Generative-Paradigm-Inspired-by-Information-Theory" class="headerlink" title="Unifying Search and Recommendation: A Generative Paradigm Inspired by   Information Theory"></a>Unifying Search and Recommendation: A Generative Paradigm Inspired by   Information Theory</h2><p><strong>Authors:Jujia Zhao, Wenjie Wang, Chen Xu, Xiuying Wang, Zhaochun Ren, Suzan Verberne</strong></p>
<p>Recommender systems and search engines serve as foundational elements of online platforms, with the former delivering information proactively and the latter enabling users to seek information actively. Unifying both tasks in a shared model is promising since it can enhance user modeling and item understanding. Previous approaches mainly follow a discriminative paradigm, utilizing shared encoders to process input features and task-specific heads to perform each task. However, this paradigm encounters two key challenges: gradient conflict and manual design complexity. From the information theory perspective, these challenges potentially both stem from the same issue â€“ low mutual information between the input features and task-specific outputs during the optimization process.   To tackle these issues, we propose GenSR, a novel generative paradigm for unifying search and recommendation (S&amp;R), which leverages task-specific prompts to partition the modelâ€™s parameter space into subspaces, thereby enhancing mutual information. To construct effective subspaces for each task, GenSR first prepares informative representations for each subspace and then optimizes both subspaces in one unified model. Specifically, GenSR consists of two main modules: (1) Dual Representation Learning, which independently models collaborative and semantic historical information to derive expressive item representations; and (2) S&amp;R Task Unifying, which utilizes contrastive learning together with instruction tuning to generate task-specific outputs effectively. Extensive experiments on two public datasets show GenSR outperforms state-of-the-art methods across S&amp;R tasks. Our work introduces a new generative paradigm compared with previous discriminative methods and establishes its superiority from the mutual information perspective. </p>
<blockquote>
<p>æ¨èç³»ç»Ÿå’Œæœç´¢å¼•æ“ä½œä¸ºåœ¨çº¿å¹³å°çš„åŸºç¡€å…ƒç´ ï¼Œå‰è€…ä¸»åŠ¨æä¾›ä¿¡æ¯ï¼Œåè€…ä½¿ç”¨æˆ·èƒ½å¤Ÿä¸»åŠ¨å¯»æ‰¾ä¿¡æ¯ã€‚åœ¨å…±äº«æ¨¡å‹ä¸­ç»Ÿä¸€è¿™ä¸¤é¡¹ä»»åŠ¡æ˜¯æœ‰å‰é€”çš„ï¼Œå› ä¸ºå®ƒå¯ä»¥å¢å¼ºç”¨æˆ·å»ºæ¨¡å’Œç‰©å“ç†è§£ã€‚ä¹‹å‰çš„æ–¹æ³•ä¸»è¦éµå¾ªåˆ¤åˆ«èŒƒå¼ï¼Œä½¿ç”¨å…±äº«ç¼–ç å™¨å¤„ç†è¾“å…¥ç‰¹å¾å’Œä»»åŠ¡ç‰¹å®šå¤´æ¥æ‰§è¡Œæ¯ä¸ªä»»åŠ¡ã€‚ç„¶è€Œï¼Œè¿™ç§èŒƒå¼é¢ä¸´ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ï¼šæ¢¯åº¦å†²çªå’Œæ‰‹åŠ¨è®¾è®¡å¤æ‚æ€§ã€‚ä»ä¿¡æ¯è®ºçš„è§’åº¦æ¥çœ‹ï¼Œè¿™äº›æŒ‘æˆ˜éƒ½å¯èƒ½æºäºåŒæ ·çš„é—®é¢˜â€”â€”åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­ï¼Œè¾“å…¥ç‰¹å¾å’Œä»»åŠ¡ç‰¹å®šè¾“å‡ºä¹‹é—´çš„äº’ä¿¡æ¯è¾ƒä½ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†GenSRï¼Œè¿™æ˜¯ä¸€ç§ç»Ÿä¸€æœç´¢å’Œæ¨èï¼ˆS&amp;Rï¼‰çš„æ–°å‹ç”ŸæˆèŒƒå¼ã€‚GenSRåˆ©ç”¨ä»»åŠ¡ç‰¹å®šæç¤ºå°†æ¨¡å‹å‚æ•°ç©ºé—´åˆ†å‰²æˆå­ç©ºé—´ï¼Œä»è€Œæé«˜äº’ä¿¡æ¯ã€‚è¦ä¸ºæ¯ä¸ªä»»åŠ¡æ„å»ºæœ‰æ•ˆçš„å­ç©ºé—´ï¼ŒGenSRé¦–å…ˆä¸ºæ¯ä¸ªå­ç©ºé—´å‡†å¤‡ä¿¡æ¯è¡¨ç¤ºï¼Œç„¶ååœ¨ç»Ÿä¸€æ¨¡å‹ä¸­ä¼˜åŒ–è¿™ä¸¤ä¸ªå­ç©ºé—´ã€‚å…·ä½“æ¥è¯´ï¼ŒGenSRç”±ä¸¤ä¸ªä¸»è¦æ¨¡å—ç»„æˆï¼šï¼ˆ1ï¼‰åŒè¡¨ç¤ºå­¦ä¹ ï¼Œå®ƒç‹¬ç«‹åœ°å»ºæ¨¡åä½œå’Œè¯­ä¹‰å†å²ä¿¡æ¯ï¼Œä»¥å¯¼å‡ºè¡¨è¾¾æ€§çš„ç‰©å“è¡¨ç¤ºï¼›ï¼ˆ2ï¼‰S&amp;Rä»»åŠ¡ç»Ÿä¸€ï¼Œå®ƒåˆ©ç”¨å¯¹æ¯”å­¦ä¹ ä¸æŒ‡ä»¤è°ƒæ•´æ¥æœ‰æ•ˆåœ°ç”Ÿæˆä»»åŠ¡ç‰¹å®šè¾“å‡ºã€‚åœ¨ä¸¤ä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒGenSRåœ¨S&amp;Rä»»åŠ¡ä¸Šä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„å·¥ä½œä¸ä¹‹å‰çš„åˆ¤åˆ«æ–¹æ³•ç›¸æ¯”ï¼Œå¼•å…¥äº†ä¸€ç§æ–°çš„ç”ŸæˆèŒƒå¼ï¼Œå¹¶ä»äº’ä¿¡æ¯è§’åº¦è¯æ˜äº†å…¶ä¼˜è¶Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.06714v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æ¢è®¨äº†æ¨èç³»ç»Ÿå’Œæœç´¢å¼•æ“çš„ç»Ÿä¸€æ¨¡å‹ï¼ŒæŒ‡å‡ºä¼ ç»Ÿæ–¹æ³•é¢ä¸´çš„æŒ‘æˆ˜å¹¶æå‡ºäº†GenSRè¿™ä¸€æ–°çš„ç”Ÿæˆå¼æ¡†æ¶æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚GenSRé€šè¿‡ä»»åŠ¡ç‰¹å®šæç¤ºå°†æ¨¡å‹å‚æ•°ç©ºé—´åˆ’åˆ†ä¸ºå­ç©ºé—´ï¼Œæé«˜äº’ä¿¡æ¯ï¼ŒåŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šåŒè¡¨ç¤ºå­¦ä¹ å’ŒS&amp;Rä»»åŠ¡ç»Ÿä¸€ã€‚å®éªŒè¯æ˜ï¼ŒGenSRåœ¨å…¬å¼€æ•°æ®é›†ä¸Šä¼˜äºS&amp;Rä»»åŠ¡çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¨èç³»ç»Ÿå’Œæœç´¢å¼•æ“æ˜¯ä¿¡æ¯å¹³å°çš„åŸºç¡€ç»„ä»¶ï¼Œå‰è€…ä¸»åŠ¨æä¾›ä¿¡æ¯ï¼Œåè€…ä½¿ç”¨æˆ·èƒ½å¤Ÿä¸»åŠ¨å¯»æ‰¾ä¿¡æ¯ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦éµå¾ªåˆ¤åˆ«å¼èŒƒå¼ï¼Œä½¿ç”¨å…±äº«ç¼–ç å™¨å¤„ç†è¾“å…¥ç‰¹å¾å’Œä»»åŠ¡ç‰¹å®šå¤´æ‰§è¡Œæ¯ä¸ªä»»åŠ¡ï¼Œä½†é¢ä¸´æ¢¯åº¦å†²çªå’Œæ‰‹åŠ¨è®¾è®¡å¤æ‚æ€§ç­‰æŒ‘æˆ˜ã€‚</li>
<li>è¿™äº›æŒ‘æˆ˜å¯èƒ½æºäºä¼˜åŒ–è¿‡ç¨‹ä¸­çš„ä½äº’ä¿¡æ¯ã€‚</li>
<li>GenSRæ˜¯ä¸€ä¸ªæ–°çš„ç”Ÿæˆå¼æ¡†æ¶ï¼Œé€šè¿‡ä»»åŠ¡ç‰¹å®šæç¤ºåˆ’åˆ†æ¨¡å‹å‚æ•°ç©ºé—´æ¥æé«˜äº’ä¿¡æ¯ã€‚</li>
<li>GenSRåŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šåŒè¡¨ç¤ºå­¦ä¹ å’ŒS&amp;Rä»»åŠ¡ç»Ÿä¸€ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒGenSRåœ¨å…¬å¼€æ•°æ®é›†ä¸Šä¼˜äºå…¶ä»–S&amp;Rä»»åŠ¡çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.06714">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-fd3ee97ee377104c4d470caca6d561d3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-964e592eb0aa59da1889cff8d0d32957.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3a1e4445c7e2b20190079b37a3ee9434.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b9fa667f693c21c9a9b333e2ca3a5ec.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Hogwild-Inference-Parallel-LLM-Generation-via-Concurrent-Attention"><a href="#Hogwild-Inference-Parallel-LLM-Generation-via-Concurrent-Attention" class="headerlink" title="Hogwild! Inference: Parallel LLM Generation via Concurrent Attention"></a>Hogwild! Inference: Parallel LLM Generation via Concurrent Attention</h2><p><strong>Authors:Gleb Rodionov, Roman Garipov, Alina Shutova, George Yakushev, Vage Egiazarian, Anton Sinitsin, Denis Kuznedelev, Dan Alistarh</strong></p>
<p>Large Language Models (LLMs) have demonstrated the ability to tackle increasingly complex tasks through advanced reasoning, long-form content generation, and tool use. Solving these tasks often involves long inference-time computations. In human problem solving, a common strategy to expedite work is collaboration: by dividing the problem into sub-tasks, exploring different strategies concurrently, etc. Recent research has shown that LLMs can also operate in parallel by implementing explicit cooperation frameworks, such as voting mechanisms or the explicit creation of independent sub-tasks that can be executed in parallel. However, each of these frameworks may not be suitable for all types of tasks, which can hinder their applicability. In this work, we propose a different design approach: we run LLM â€œworkersâ€ in parallel , allowing them to synchronize via a concurrently-updated attention cache and prompt these workers to decide how best to collaborate. Our approach allows the instances to come up with their own collaboration strategy for the problem at hand, all the while â€œseeingâ€ each otherâ€™s partial progress in the concurrent cache. We implement this approach via Hogwild! Inference: a parallel LLM inference engine where multiple instances of the same LLM run in parallel with the same attention cache, with â€œinstantâ€ access to each otherâ€™s generated tokens. Hogwild! inference takes advantage of Rotary Position Embeddings (RoPE) to avoid recomputation while improving parallel hardware utilization. We find that modern reasoning-capable LLMs can perform inference with shared Key-Value cache out of the box, without additional fine-tuning. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²å±•ç°å‡ºé€šè¿‡é«˜çº§æ¨ç†ã€é•¿å½¢å¼å†…å®¹ç”Ÿæˆå’Œå·¥å…·ä½¿ç”¨æ¥å¤„ç†æ—¥ç›Šå¤æ‚ä»»åŠ¡çš„èƒ½åŠ›ã€‚è§£å†³è¿™äº›ä»»åŠ¡é€šå¸¸æ¶‰åŠé•¿æ—¶é—´çš„æ¨ç†è®¡ç®—ã€‚åœ¨äººç±»çš„é—®é¢˜è§£å†³ä¸­ï¼ŒåŠ é€Ÿå·¥ä½œçš„å¸¸ç”¨ç­–ç•¥æ˜¯åä½œï¼šå°†é—®é¢˜åˆ’åˆ†ä¸ºå­ä»»åŠ¡ï¼ŒåŒæ—¶æ¢ç´¢ä¸åŒçš„ç­–ç•¥ç­‰ã€‚æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼ŒLLMä¹Ÿå¯ä»¥é€šè¿‡å®æ–½æ˜ç¡®çš„åˆä½œæ¡†æ¶è¿›è¡Œå¹¶è¡Œæ“ä½œï¼Œå¦‚æŠ•ç¥¨æœºåˆ¶æˆ–åˆ›å»ºå¯ä»¥å¹¶è¡Œæ‰§è¡Œçš„ç‹¬ç«‹å­ä»»åŠ¡ã€‚ç„¶è€Œï¼Œè¿™äº›æ¡†æ¶å¯èƒ½å¹¶ä¸é€‚ç”¨äºæ‰€æœ‰ç±»å‹çš„ä»»åŠ¡ï¼Œè¿™ä¼šé˜»ç¢å…¶é€‚ç”¨æ€§ã€‚åœ¨æ­¤å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸åŒçš„è®¾è®¡æ€è·¯ï¼šæˆ‘ä»¬å¹¶è¡Œè¿è¡ŒLLMâ€œå·¥ä½œè€…â€ï¼Œå…è®¸å®ƒä»¬é€šè¿‡å®æ—¶æ›´æ–°çš„å…³æ³¨ç¼“å­˜è¿›è¡ŒåŒæ­¥ï¼Œå¹¶æç¤ºè¿™äº›å·¥ä½œè€…å†³å®šå¦‚ä½•æœ€ä½³åä½œã€‚æˆ‘ä»¬çš„æ–¹æ³•å…è®¸å®ä¾‹ä¸ºæ‰‹å¤´é—®é¢˜åˆ¶å®šè‡ªå·±çš„åä½œç­–ç•¥ï¼ŒåŒæ—¶â€œæŸ¥çœ‹â€å½¼æ­¤åœ¨å¹¶å‘ç¼“å­˜ä¸­çš„éƒ¨åˆ†è¿›åº¦ã€‚æˆ‘ä»¬é€šè¿‡â€œHogwildâ€å®ç°è¿™ç§æ–¹æ³•ï¼šä¸€ç§å¹¶è¡ŒLLMæ¨ç†å¼•æ“ï¼Œå…¶ä¸­åŒä¸€LLMçš„å¤šä¸ªå®ä¾‹å¯ä»¥å¹¶è¡Œè¿è¡Œå¹¶å…±äº«å…³æ³¨ç¼“å­˜ï¼Œå½¼æ­¤ä¹‹é—´å¯ä»¥å³æ—¶è®¿é—®ç”Ÿæˆçš„ä»¤ç‰Œã€‚â€œHogwildâ€æ¨ç†åˆ©ç”¨æ—‹è½¬ä½ç½®åµŒå…¥ï¼ˆRoPEï¼‰é¿å…é‡æ–°è®¡ç®—ï¼ŒåŒæ—¶æé«˜å¹¶è¡Œç¡¬ä»¶çš„åˆ©ç”¨ç‡ã€‚æˆ‘ä»¬å‘ç°ï¼Œç°ä»£å…·æœ‰æ¨ç†èƒ½åŠ›çš„å¤§å‹è¯­è¨€æ¨¡å‹å¯ä»¥åœ¨å…±äº«é”®å€¼ç¼“å­˜çš„æƒ…å†µä¸‹è¿›è¡Œæ¨ç†ï¼Œæ— éœ€é¢å¤–çš„å¾®è°ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.06261v2">PDF</a> Preprint, work in progress</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å…·å¤‡å¤„ç†å¤æ‚ä»»åŠ¡çš„èƒ½åŠ›ï¼Œå¦‚é«˜çº§æ¨ç†ã€é•¿æ–‡æœ¬å†…å®¹ç”Ÿæˆå’Œå·¥å…·ä½¿ç”¨ã€‚é€šè¿‡å®æ–½æ˜ç¡®çš„åˆä½œæ¡†æ¶ï¼Œå¦‚æŠ•ç¥¨æœºåˆ¶å’Œç‹¬ç«‹å­ä»»åŠ¡çš„åˆ›å»ºï¼ŒLLMå¯å¹¶è¡Œæ“ä½œä»¥åŠ å¿«ä»»åŠ¡å¤„ç†é€Ÿåº¦ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„è®¾è®¡æ–¹æ³•ï¼Œå³å¹¶è¡Œè¿è¡ŒLLMâ€œå·¥ä½œè€…â€ï¼Œé€šè¿‡åŒæ­¥æ›´æ–°çš„å…³æ³¨ç¼“å­˜æ¥æç¤ºå·¥ä½œè€…å¦‚ä½•æœ€ä½³åä½œã€‚è¿™ç§æ–¹æ³•å…è®¸å®ä¾‹ä¸ºæ‰‹å¤´é—®é¢˜åˆ¶å®šè‡ªå·±çš„åä½œç­–ç•¥ï¼ŒåŒæ—¶â€œæŸ¥çœ‹â€å½¼æ­¤çš„éƒ¨åˆ†è¿›åº¦ã€‚æœ¬ç ”ç©¶é€šè¿‡éœæ ¼å¨å°”ï¼ˆHogwildï¼‰æ¨ç†å®ç°è¿™ä¸€æ–¹æ³•ï¼šä¸€ç§å¹¶è¡ŒLLMæ¨ç†å¼•æ“ï¼Œå…¶ä¸­åŒä¸€LLMçš„å¤šä¸ªå®ä¾‹å¯ä»¥å¹¶è¡Œè¿è¡Œå¹¶å…±äº«å…³æ³¨ç¼“å­˜ï¼Œå¯å³æ—¶è®¿é—®å½¼æ­¤çš„ç”Ÿæˆæ ‡è®°ã€‚éœæ ¼å¨å°”æ¨ç†åˆ©ç”¨æ—‹è½¬ä½ç½®åµŒå…¥ï¼ˆRoPEï¼‰é¿å…é‡æ–°è®¡ç®—ï¼Œæé«˜å¹¶è¡Œç¡¬ä»¶åˆ©ç”¨ç‡ã€‚ç ”ç©¶å‘ç°ï¼Œç°ä»£å…·å¤‡æ¨ç†èƒ½åŠ›çš„LLMå¯åœ¨æ— éœ€é¢å¤–å¾®è°ƒçš„æƒ…å†µä¸‹ä½¿ç”¨å…±äº«é”®å€¼ç¼“å­˜è¿›è¡Œæ¨ç†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èƒ½å¤Ÿå¤„ç†å¤æ‚çš„ä»»åŠ¡ï¼ŒåŒ…æ‹¬é«˜çº§æ¨ç†ã€é•¿æ–‡æœ¬å†…å®¹ç”Ÿæˆå’Œå·¥å…·ä½¿ç”¨ã€‚</li>
<li>LLMå¯ä»¥é€šè¿‡å®æ–½æ˜ç¡®çš„åˆä½œæ¡†æ¶è¿›è¡Œå¹¶è¡Œæ“ä½œä»¥åŠ å¿«ä»»åŠ¡å¤„ç†é€Ÿåº¦ã€‚</li>
<li>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„è®¾è®¡æ–¹æ³•æ¥ä¿ƒè¿›LLMä¹‹é—´çš„åä½œï¼Œå³é€šè¿‡å¹¶è¡Œè¿è¡ŒLLMâ€œå·¥ä½œè€…â€ï¼Œå¹¶å…è®¸å®ƒä»¬é€šè¿‡å…³æ³¨ç¼“å­˜è¿›è¡ŒåŒæ­¥ã€‚</li>
<li>è¿™ç§æ–°æ–¹æ³•å…è®¸LLMå®ä¾‹ä¸ºç‰¹å®šé—®é¢˜åˆ¶å®šè‡ªå·±çš„åä½œç­–ç•¥ï¼Œå¹¶å¯ä»¥å®æ—¶æŸ¥çœ‹å…¶ä»–å®ä¾‹çš„è¿›åº¦ã€‚</li>
<li>éœæ ¼å¨å°”ï¼ˆHogwildï¼‰æ¨ç†æ˜¯å®ç°è¿™ä¸€æ–¹æ³•çš„ä¸€ç§æ‰‹æ®µï¼Œå®ƒå…è®¸å¤šä¸ªLLMå®ä¾‹å¹¶è¡Œè¿è¡Œå¹¶å…±äº«å…³æ³¨ç¼“å­˜ã€‚</li>
<li>æ—‹è½¬ä½ç½®åµŒå…¥ï¼ˆRoPEï¼‰è¢«ç”¨äºæé«˜ç¡¬ä»¶åˆ©ç”¨ç‡å¹¶é¿å…åœ¨æ¨ç†è¿‡ç¨‹ä¸­çš„é‡æ–°è®¡ç®—ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.06261">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-adf37f954f32f748594bc8568486f52e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ce12e9318b74af831a7c646939f395c0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-883e2750dec1b9a3764f4f91374f4060.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="STAGE-Stemmed-Accompaniment-Generation-through-Prefix-Based-Conditioning"><a href="#STAGE-Stemmed-Accompaniment-Generation-through-Prefix-Based-Conditioning" class="headerlink" title="STAGE: Stemmed Accompaniment Generation through Prefix-Based   Conditioning"></a>STAGE: Stemmed Accompaniment Generation through Prefix-Based   Conditioning</h2><p><strong>Authors:Giorgio Strano, Chiara Ballanti, Donato Crisostomi, Michele Mancusi, Luca Cosmo, Emanuele RodolÃ </strong></p>
<p>Recent advances in generative models have made it possible to create high-quality, coherent music, with some systems delivering production-level output. Yet, most existing models focus solely on generating music from scratch, limiting their usefulness for musicians who want to integrate such models into a human, iterative composition workflow. In this paper we introduce STAGE, our STemmed Accompaniment GEneration model, fine-tuned from the state-of-the-art MusicGen to generate single-stem instrumental accompaniments conditioned on a given mixture. Inspired by instruction-tuning methods for language models, we extend the transformerâ€™s embedding matrix with a context token, enabling the model to attend to a musical context through prefix-based conditioning. Compared to the baselines, STAGE yields accompaniments that exhibit stronger coherence with the input mixture, higher audio quality, and closer alignment with textual prompts. Moreover, by conditioning on a metronome-like track, our framework naturally supports tempo-constrained generation, achieving state-of-the-art alignment with the target rhythmic structureâ€“all without requiring any additional tempo-specific module. As a result, STAGE offers a practical, versatile tool for interactive music creation that can be readily adopted by musicians in real-world workflows. </p>
<blockquote>
<p>æœ€è¿‘ç”Ÿæˆæ¨¡å‹çš„è¿›æ­¥ä½¿å¾—åˆ›å»ºé«˜è´¨é‡ã€è¿è´¯çš„éŸ³ä¹æˆä¸ºå¯èƒ½ï¼Œä¸€äº›ç³»ç»Ÿç”šè‡³èƒ½å¤Ÿäº§ç”Ÿä¸“ä¸šçº§åˆ«çš„è¾“å‡ºã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰æ¨¡å‹åªä¸“æ³¨äºä»é›¶å¼€å§‹ç”ŸæˆéŸ³ä¹ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬å¯¹äºå¸Œæœ›å°†è¿™ç§æ¨¡å‹èå…¥äººç±»è¿­ä»£ä½œæ›²å·¥ä½œæµç¨‹çš„éŸ³ä¹å®¶çš„å®ç”¨æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†STAGEï¼Œæˆ‘ä»¬çš„åŸºäºèŒçš„ä¼´å¥ç”Ÿæˆæ¨¡å‹ï¼ˆSTAGEï¼‰ï¼Œå®ƒåŸºäºæœ€å…ˆè¿›çš„MusicGenè¿›è¡Œå¾®è°ƒï¼Œä»¥åœ¨ç»™å®šçš„æ··åˆä¸‹ç”Ÿæˆå•èŒä¹å™¨ä¼´å¥ã€‚å—è¯­è¨€æ¨¡å‹çš„æŒ‡ä»¤è°ƒæ•´æ–¹æ³•çš„å¯å‘ï¼Œæˆ‘ä»¬æ‰©å±•äº†è½¬æ¢å™¨çš„åµŒå…¥çŸ©é˜µï¼Œå¢åŠ äº†ä¸€ä¸ªä¸Šä¸‹æ–‡æ ‡è®°ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿé€šè¿‡åŸºäºå‰ç¼€çš„æ¡ä»¶æ¥å…³æ³¨éŸ³ä¹ä¸Šä¸‹æ–‡ã€‚ä¸åŸºçº¿ç›¸æ¯”ï¼ŒSTAGEç”Ÿæˆçš„ä¼´å¥ä¸è¾“å…¥æ··åˆçš„è¿è´¯æ€§æ›´å¼ºï¼ŒéŸ³é¢‘è´¨é‡æ›´é«˜ï¼Œä¸æ–‡æœ¬æç¤ºçš„å¯¹é½ç¨‹åº¦æ›´é«˜ã€‚æ­¤å¤–ï¼Œé€šè¿‡åŸºäºèŠ‚æ‹å™¨è½¨é“çš„æ¡ä»¶è®¾ç½®ï¼Œæˆ‘ä»¬çš„æ¡†æ¶è‡ªç„¶åœ°æ”¯æŒèŠ‚å¥çº¦æŸç”Ÿæˆï¼Œå®ç°äº†ä¸ç›®æ ‡èŠ‚å¥ç»“æ„çš„ä¸€æµå¯¹é½â€”â€”æ‰€æœ‰è¿™ä¸€åˆ‡éƒ½ä¸éœ€è¦ä»»ä½•é¢å¤–çš„ç‰¹å®šèŠ‚å¥çš„æ¨¡å—ã€‚å› æ­¤ï¼Œä½œä¸ºä¸€ç§ç»“æœï¼Œæ— è®ºåœ¨ç°å®ä¸­æˆ–æ˜¯åœ¨æ•°å­—ä¸–ç•Œé‡Œä½¿ç”¨ä½•ç§å·¥å…·çš„æƒ…å†µä¸‹è¿›è¡Œäº¤äº’å¼éŸ³ä¹åˆ›ä½œçš„æƒ…å†µä¸‹éƒ½èƒ½ä¸ºæˆ‘ä»¬æä¾›ä¸€ä¸ªç»ä½³çš„åˆ›ä½œé€”å¾„ä¸å®ç”¨çš„å·¥å…·æ¥ä½¿ç”¨ä»¥å®ç°è‡ªå·±çš„æƒ³è±¡åŠ›æƒ³æ³•ç­‰ã€‚ã€‚æ€»çš„æ¥è¯´ï¼Œæ— è®ºæ˜¯åœ¨ä¸šä½™éŸ³ä¹åˆ›ä½œè¿˜æ˜¯åœ¨éŸ³ä¹ç”Ÿäº§å·¥ä½œä¸­å®ƒéƒ½æ˜¯ä¸€ä¸ªçµæ´»å¤šå˜çš„å·¥å…·æ¥ä¸ºæˆ‘ä»¬çš„åˆ›ä½œæä¾›æ”¯æŒä¸å¸®åŠ©ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05690v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ç”Ÿæˆæ¨¡å‹è¿‘æœŸå–å¾—äº†é‡å¤§è¿›å±•ï¼Œç°åœ¨èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡ã€è¿è´¯çš„éŸ³ä¹ï¼ŒæŸäº›ç³»ç»Ÿç”šè‡³èƒ½äº§å‡ºä¸“ä¸šçº§åˆ«çš„ä½œå“ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰æ¨¡å‹ä¸“æ³¨äºä»é›¶å¼€å§‹ç”ŸæˆéŸ³ä¹ï¼Œå¿½ç•¥äº†éŸ³ä¹å®¶ä»¬æƒ³è¦å°†è¿™ç±»æ¨¡å‹èå…¥äººç±»è¿­ä»£åˆ›ä½œæµç¨‹çš„éœ€æ±‚ã€‚æœ¬æ–‡ä»‹ç»äº†STAGEæ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºMusicGençš„ç²¾ç»†åŒ–ä¼´å¥ç”Ÿæˆæ¨¡å‹ï¼Œèƒ½å¤Ÿé€šè¿‡ç»™å®šçš„æ—‹å¾‹ç”Ÿæˆå•ä¹å™¨ä¼´å¥ã€‚é€šè¿‡å€Ÿé‰´è¯­è¨€æ¨¡å‹çš„æŒ‡ä»¤å¾®è°ƒæ–¹æ³•ï¼Œæˆ‘ä»¬åœ¨transformerçš„åµŒå…¥çŸ©é˜µä¸­å¢åŠ äº†ä¸€ä¸ªä¸Šä¸‹æ–‡æ ‡è®°ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿé€šè¿‡å‰ç¼€æ¡ä»¶å…³æ³¨éŸ³ä¹ä¸Šä¸‹æ–‡ã€‚ç›¸è¾ƒäºå…¶ä»–æ¨¡å‹ï¼ŒSTAGEç”Ÿæˆçš„ä¼´å¥ä¸è¾“å…¥æ—‹å¾‹æ›´å…·è¿è´¯æ€§ã€éŸ³é¢‘è´¨é‡æ›´é«˜ï¼Œå¹¶èƒ½æ›´ç´§å¯†åœ°è´´åˆæ–‡æœ¬æç¤ºã€‚æ­¤å¤–ï¼Œé€šè¿‡ä»¥èŠ‚æ‹å™¨èˆ¬çš„è½¨è¿¹ä½œä¸ºæ¡ä»¶ï¼Œæˆ‘ä»¬çš„æ¡†æ¶èƒ½å¤Ÿè‡ªç„¶åœ°æ”¯æŒèŠ‚å¥çº¦æŸç”Ÿæˆï¼Œæ— éœ€ä»»ä½•é¢å¤–çš„èŠ‚å¥ç‰¹å®šæ¨¡å—ï¼Œå³å¯å®ç°ä¸ç›®æ ‡èŠ‚å¥ç»“æ„çš„æœ€ä½³å¯¹é½ã€‚å› æ­¤ï¼ŒSTAGEä¸ºéŸ³ä¹å®¶ä»¬æä¾›äº†ä¸€ä¸ªå®ç”¨çš„ã€å¤šåŠŸèƒ½å·¥å…·ï¼Œå¯è½»æ¾èå…¥ç°å®ä¸–ç•Œçš„éŸ³ä¹åˆ›ä½œæµç¨‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆæ¨¡å‹èƒ½ç”Ÿæˆé«˜è´¨é‡ã€è¿è´¯çš„éŸ³ä¹ï¼Œä½†å¤šæ•°æ¨¡å‹ä»…å…³æ³¨ä»é›¶å¼€å§‹ç”ŸæˆéŸ³ä¹ã€‚</li>
<li>STAGEæ¨¡å‹æ˜¯åŸºäºMusicGençš„ç²¾ç»†åŒ–ä¼´å¥ç”Ÿæˆæ¨¡å‹ï¼Œèƒ½å¤Ÿç”Ÿæˆä¸ç»™å®šæ—‹å¾‹åŒ¹é…çš„ä¼´å¥ã€‚</li>
<li>STAGEé€šè¿‡å¢åŠ ä¸Šä¸‹æ–‡æ ‡è®°çš„æ–¹å¼ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå…³æ³¨éŸ³ä¹ä¸Šä¸‹æ–‡ã€‚</li>
<li>ä¸å…¶ä»–æ¨¡å‹ç›¸æ¯”ï¼ŒSTAGEç”Ÿæˆçš„ä¼´å¥ä¸è¾“å…¥æ—‹å¾‹æ›´è¿è´¯ï¼ŒéŸ³é¢‘è´¨é‡æ›´é«˜ï¼Œå¹¶æ›´ç´§å¯†åœ°è´´åˆæ–‡æœ¬æç¤ºã€‚</li>
<li>STAGEæ”¯æŒä»¥èŠ‚æ‹å™¨è½¨è¿¹ä½œä¸ºæ¡ä»¶è¿›è¡ŒèŠ‚å¥çº¦æŸç”Ÿæˆï¼Œæ— éœ€é¢å¤–çš„èŠ‚å¥ç‰¹å®šæ¨¡å—ã€‚</li>
<li>STAGEæ¡†æ¶ä¸ºéŸ³ä¹å®¶æä¾›äº†ä¸€ä¸ªå®ç”¨çš„å·¥å…·ï¼Œå¯èå…¥ç°å®ä¸–ç•Œçš„éŸ³ä¹åˆ›ä½œæµç¨‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05690">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-244523c752a66abb9461372270eb02dd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5d20ceef31ac5f7ebd985f18d938fa17.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8d7ee14a5404d19e655608f82e46228c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d84691c7200e7602459b6b2a7fa477ab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d291bc873cbce5295c82cf82fc65c0d6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ecd4bfe0bcc82a404d9500c62a8e820c.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Studying-and-Understanding-the-Effectiveness-and-Failures-of-Conversational-LLM-Based-Repair"><a href="#Studying-and-Understanding-the-Effectiveness-and-Failures-of-Conversational-LLM-Based-Repair" class="headerlink" title="Studying and Understanding the Effectiveness and Failures of   Conversational LLM-Based Repair"></a>Studying and Understanding the Effectiveness and Failures of   Conversational LLM-Based Repair</h2><p><strong>Authors:Aolin Chen, Haojun Wu, Qi Xin, Steven P. Reiss, Jifeng Xuan</strong></p>
<p>Automated program repair (APR) is designed to automate the process of bug-fixing. In recent years, thanks to the rapid development of large language models (LLMs), automated repair has achieved remarkable progress. Advanced APR techniques powered by conversational LLMs, most notably ChatGPT, have exhibited impressive repair abilities and gained increasing popularity due to the capabilities of the underlying LLMs in providing repair feedback and performing iterative patch improvement. Despite the superiority, conversational APR techniques still fail to repair a large number of bugs. For example, a state-of-the-art conversational technique ChatRepair does not correctly repair over half of the single-function bugs in the Defects4J dataset. To understand the effectiveness and failures of conversational LLM-based repair and provide possible directions for improvement, we studied the exemplary ChatRepair with a focus on comparing the effectiveness of its cloze-style and full function repair strategies, assessing its key iterative component for patch improvement, and analyzing the repair failures. Our study has led to a series of findings, which we believe provide key implications for future research. </p>
<blockquote>
<p>è‡ªåŠ¨åŒ–ç¨‹åºä¿®å¤ï¼ˆAPRï¼‰æ—¨åœ¨è‡ªåŠ¨è¿›è¡Œæ•…éšœä¿®å¤ã€‚è¿‘å¹´æ¥ï¼Œç”±äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œè‡ªåŠ¨åŒ–ä¿®å¤å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ã€‚ç”±å¯¹è¯å¼LLMæ”¯æŒçš„é«˜çº§APRæŠ€æœ¯ï¼Œå°¤å…¶æ˜¯ChatGPTï¼Œå±•ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„ä¿®å¤èƒ½åŠ›ï¼Œå¹¶å› åº•å±‚LLMæä¾›ä¿®å¤åé¦ˆå’Œè¿›è¡Œè¿­ä»£è¡¥ä¸æ”¹è¿›çš„èƒ½åŠ›è€Œè¶Šæ¥è¶Šå—æ¬¢è¿ã€‚å°½ç®¡å¦‚æ­¤ï¼Œå¯¹è¯å¼APRæŠ€æœ¯ä»ç„¶æ— æ³•ä¿®å¤å¤§é‡æ¼æ´ã€‚ä¾‹å¦‚ï¼Œæœ€å…ˆè¿›çš„å¯¹è¯æŠ€æœ¯ChatRepairæ— æ³•æ­£ç¡®ä¿®å¤Defects4Jæ•°æ®é›†ä¸­è¶…è¿‡ä¸€åŠçš„å•åŠŸèƒ½æ¼æ´ã€‚ä¸ºäº†ç†è§£åŸºäºå¯¹è¯çš„LLMä¿®å¤çš„æœ‰æ•ˆæ€§å’Œå¤±è´¥åŸå› ï¼Œå¹¶æä¾›å¯èƒ½çš„æ”¹è¿›æ–¹å‘ï¼Œæˆ‘ä»¬ç ”ç©¶äº†å…·æœ‰ç¤ºèŒƒæ€§çš„ChatRepairï¼Œé‡ç‚¹å…³æ³¨æ¯”è¾ƒå…¶å¡«ç©ºå¼ä¸å…¨åŠŸèƒ½ä¿®å¤ç­–ç•¥çš„æœ‰æ•ˆæ€§ï¼Œè¯„ä¼°å…¶ç”¨äºè¡¥ä¸æ”¹è¿›çš„å…³é”®è¿­ä»£ç»„ä»¶ï¼Œå¹¶åˆ†æä¿®å¤å¤±è´¥çš„åŸå› ã€‚æˆ‘ä»¬çš„ç ”ç©¶å¾—åˆ°äº†ä¸€ç³»åˆ—å‘ç°ï¼Œæˆ‘ä»¬ç›¸ä¿¡è¿™äº›å‘ç°å¯¹æœªæ¥ç ”ç©¶å…·æœ‰å…³é”®å¯ç¤ºæ„ä¹‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.15050v2">PDF</a> </p>
<p><strong>Summary</strong><br>     åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è‡ªåŠ¨åŒ–ä¿®å¤ç¨‹åºï¼ˆAPRï¼‰åœ¨è¿‘å¹´æ¥å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç‰¹åˆ«æ˜¯ChatGPTç­‰å¯¹è¯å¼LLMæŠ€æœ¯çš„å‡ºç°ï¼Œä½¿å¾—è‡ªåŠ¨åŒ–ä¿®å¤èƒ½åŠ›å¾—åˆ°äº†æå¤§çš„æå‡ã€‚ç„¶è€Œï¼Œå°½ç®¡å…·æœ‰ä¼˜è¶Šæ€§ï¼Œå¯¹è¯å¼APRæŠ€æœ¯ä»ç„¶æ— æ³•ä¿®å¤å¤§é‡çš„bugã€‚ä¾‹å¦‚ï¼Œæœ€å…ˆè¿›çš„å¯¹è¯æŠ€æœ¯ChatRepairå¹¶ä¸èƒ½æ­£ç¡®ä¿®å¤Defects4Jæ•°æ®é›†ä¸­ä¸€åŠä»¥ä¸Šçš„å•åŠŸèƒ½bugã€‚ä¸ºäº†æ›´å¥½åœ°ç†è§£å¯¹è¯å¼LLMä¿®å¤æŠ€æœ¯çš„æœ‰æ•ˆæ€§å’Œå¤±è´¥åŸå› ï¼Œå¹¶ä¸ºæ”¹è¿›æä¾›æ–¹å‘ï¼Œæˆ‘ä»¬å¯¹ChatRepairè¿›è¡Œäº†æ·±å…¥ç ”ç©¶ï¼Œé‡ç‚¹å…³æ³¨å…¶å¡«ç©ºå¼ä¿®å¤ç­–ç•¥å’Œå…¨åŠŸèƒ½ä¿®å¤ç­–ç•¥çš„æœ‰æ•ˆæ€§å¯¹æ¯”ã€å…³é”®è¿­ä»£ç»„ä»¶çš„è¯„ä¼°ä»¥åŠä¿®å¤å¤±è´¥çš„æ·±å…¥åˆ†æã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªåŠ¨åŒ–ç¨‹åºä¿®å¤ï¼ˆAPRï¼‰æ—¨åœ¨è‡ªåŠ¨ä¿®å¤bugï¼Œè¿‘å¹´æ¥å› å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•è€Œå–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚</li>
<li>å¯¹è¯å¼APRæŠ€æœ¯ï¼Œå¦‚ChatGPTå’ŒChatRepairï¼Œè™½å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œä½†ä»å­˜åœ¨å¤§é‡ä¿®å¤å¤±è´¥çš„bugã€‚</li>
<li>ChatRepairçš„å¡«ç©ºå¼ä¿®å¤ç­–ç•¥å’Œå…¨åŠŸèƒ½ä¿®å¤ç­–ç•¥çš„æœ‰æ•ˆæ€§å¯¹æ¯”æ˜¯ç ”ç©¶çš„é‡ç‚¹ã€‚</li>
<li>è¯„ä¼°è¿­ä»£ç»„ä»¶å¯¹äºä¿®å¤æ•ˆæœçš„æå‡æ˜¯å…³é”®ã€‚</li>
<li>å¯¹ä¿®å¤å¤±è´¥çš„æ·±å…¥åˆ†ææœ‰åŠ©äºç†è§£ç°æœ‰æŠ€æœ¯çš„å±€é™å¹¶ä¸ºæœªæ¥ç ”ç©¶æä¾›æ–¹å‘ã€‚</li>
<li>æœ€å…ˆè¿›çš„å¯¹è¯æŠ€æœ¯ChatRepairä»æœ‰ä¸€åŠä»¥ä¸Šçš„å•åŠŸèƒ½bugæ— æ³•æ­£ç¡®ä¿®å¤ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.15050">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-468b66cf6499b352be642c393aafd3bc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7a62336ea6f54a64f31d0517f083ee0d.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="LogiDynamics-Unraveling-the-Dynamics-of-Logical-Inference-in-Large-Language-Model-Reasoning"><a href="#LogiDynamics-Unraveling-the-Dynamics-of-Logical-Inference-in-Large-Language-Model-Reasoning" class="headerlink" title="LogiDynamics: Unraveling the Dynamics of Logical Inference in Large   Language Model Reasoning"></a>LogiDynamics: Unraveling the Dynamics of Logical Inference in Large   Language Model Reasoning</h2><p><strong>Authors:Tianshi Zheng, Jiayang Cheng, Chunyang Li, Haochen Shi, Zihao Wang, Jiaxin Bai, Yangqiu Song, Ginny Y. Wong, Simon See</strong></p>
<p>Modern large language models (LLMs) employ various forms of logical inference, both implicitly and explicitly, when addressing reasoning tasks. Understanding how to optimally leverage these inference paradigms is critical for advancing LLMsâ€™ reasoning capabilities. This paper adopts an exploratory approach by introducing a controlled evaluation environment for analogical reasoning â€“ a fundamental cognitive task â€“ that is systematically parameterized across three dimensions: modality (textual, visual, symbolic), difficulty (easy, medium, hard), and task format (multiple-choice or free-text generation). We analyze the comparative dynamics of inductive, abductive, and deductive inference pipelines across these dimensions, and demonstrate that our findings generalize to broader in-context learning tasks. Additionally, we investigate advanced paradigms such as hypothesis selection, verification, and refinement, revealing their potential to scale up logical inference in LLM reasoning. This exploratory study provides a foundation for future research in enhancing LLM reasoning through systematic logical inference strategies. Resources are available at <a target="_blank" rel="noopener" href="https://github.com/HKUST-KnowComp/LogiDynamics">https://github.com/HKUST-KnowComp/LogiDynamics</a>. </p>
<blockquote>
<p>ç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤„ç†æ¨ç†ä»»åŠ¡æ—¶ï¼Œä¼šéšå¼å’Œæ˜¾å¼åœ°ä½¿ç”¨å„ç§å½¢å¼çš„é€»è¾‘æ¨ç†ã€‚äº†è§£å¦‚ä½•æœ€æœ‰æ•ˆåœ°åˆ©ç”¨è¿™äº›æ¨ç†èŒƒå¼å¯¹äºæé«˜LLMçš„æ¨ç†èƒ½åŠ›è‡³å…³é‡è¦ã€‚æœ¬æ–‡é‡‡ç”¨äº†ä¸€ç§æ¢ç´¢æ€§æ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥ç±»æ¯”æ¨ç†çš„æ§åˆ¶è¯„ä¼°ç¯å¢ƒæ¥è¿›è¡Œç ”ç©¶â€”â€”è¿™æ˜¯ä¸€ç§åŸºæœ¬çš„è®¤çŸ¥ä»»åŠ¡â€”â€”è¯¥ç³»ç»Ÿåœ°åœ¨ä¸‰ä¸ªç»´åº¦ä¸Šè¿›è¡Œå‚æ•°åŒ–ï¼šæ¨¡æ€ï¼ˆæ–‡æœ¬ã€è§†è§‰ã€ç¬¦å·ï¼‰ã€éš¾åº¦ï¼ˆå®¹æ˜“ã€ä¸­ç­‰ã€å›°éš¾ï¼‰å’Œä»»åŠ¡æ ¼å¼ï¼ˆå¤šé¡¹é€‰æ‹©æˆ–è‡ªç”±æ–‡æœ¬ç”Ÿæˆï¼‰ã€‚æˆ‘ä»¬åˆ†æäº†å½’çº³ã€å‡è®¾å’Œæ¼”ç»æ¨ç†ç®¡é“åœ¨è¿™äº›ç»´åº¦ä¸Šçš„æ¯”è¾ƒåŠ¨æ€ï¼Œå¹¶è¯æ˜æˆ‘ä»¬çš„å‘ç°å¯ä»¥æ¨å¹¿åˆ°æ›´å¹¿æ³›çš„ä¸Šä¸‹æ–‡å­¦ä¹ ä»»åŠ¡ä¸­ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ç ”ç©¶äº†å‡è®¾é€‰æ‹©ã€éªŒè¯å’Œä¿®æ­£ç­‰é«˜çº§èŒƒå¼ï¼Œæ­ç¤ºäº†å®ƒä»¬åœ¨æ‰©å¤§LLMæ¨ç†ä¸­çš„é€»è¾‘æ¨æ–­æ½œåŠ›ã€‚è¿™é¡¹æ¢ç´¢æ€§ç ”ç©¶ä¸ºæœªæ¥é€šè¿‡ç³»ç»Ÿé€»è¾‘æ¨ç†ç­–ç•¥æé«˜LLMæ¨ç†èƒ½åŠ›çš„ç ”ç©¶å¥ å®šäº†åŸºç¡€ã€‚èµ„æºå¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/HKUST-KnowComp/LogiDynamics">https://github.com/HKUST-KnowComp/LogiDynamics</a>è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.11176v2">PDF</a> 21 pages</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è§£å†³æ¨ç†ä»»åŠ¡æ—¶ï¼Œä¼šé‡‡ç”¨å¤šç§å½¢å¼çš„é€»è¾‘æ¨ç†ï¼ŒåŒ…æ‹¬éšæ€§å’Œæ˜¾æ€§æ¨ç†ã€‚äº†è§£å¦‚ä½•æœ€ä¼˜åœ°åˆ©ç”¨è¿™äº›æ¨ç†æ¨¡å¼å¯¹äºæé«˜LLMçš„æ¨ç†èƒ½åŠ›è‡³å…³é‡è¦ã€‚æœ¬æ–‡é‡‡ç”¨æ¢ç´¢æ€§æ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥ç±»æ¯”æ¨ç†çš„æ§åˆ¶è¯„ä¼°ç¯å¢ƒï¼Œç³»ç»Ÿåœ°å‚æ•°åŒ–ä¸‰ä¸ªç»´åº¦ï¼šæ¨¡æ€ï¼ˆæ–‡æœ¬ã€è§†è§‰ã€ç¬¦å·ï¼‰ã€éš¾åº¦ï¼ˆå®¹æ˜“ã€ä¸­ç­‰ã€å›°éš¾ï¼‰å’Œä»»åŠ¡æ ¼å¼ï¼ˆå¤šé¡¹é€‰æ‹©æˆ–è‡ªç”±æ–‡æœ¬ç”Ÿæˆï¼‰ã€‚æœ¬æ–‡åˆ†æäº†å½’çº³ã€å‡è®¾å’Œæ¼”ç»æ¨ç†ç®¡é“çš„æ¯”è¾ƒåŠ¨æ€ï¼Œå¹¶è¯æ˜æˆ‘ä»¬çš„å‘ç°å¯ä»¥æ¨å¹¿åˆ°æ›´å¹¿æ³›çš„ä¸Šä¸‹æ–‡å­¦ä¹ ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ç ”ç©¶äº†å‡è®¾é€‰æ‹©ã€éªŒè¯å’Œä¿®æ­£ç­‰é«˜çº§èŒƒå¼ï¼Œæ­ç¤ºäº†å®ƒä»¬åœ¨æ‰©å¤§é€»è¾‘æ¨ç†ä¸­çš„æ½œåŠ›ã€‚è¿™é¡¹æ¢ç´¢æ€§ç ”ç©¶ä¸ºæœªæ¥é€šè¿‡ç³»ç»Ÿé€»è¾‘æ¨ç†ç­–ç•¥æé«˜LLMæ¨ç†èƒ½åŠ›çš„ç ”ç©¶å¥ å®šäº†åŸºç¡€ã€‚ç›¸å…³èµ„æºå¯åœ¨é¦™æ¸¯ç§‘æŠ€å¤§å­¦çŸ¥è¯†è®¡ç®—å°ç»„çš„ç›¸å…³ç½‘ç«™ä¸Šè·å–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è§£å†³æ¨ç†ä»»åŠ¡æ—¶è¿ç”¨å¤šç§å½¢å¼çš„é€»è¾‘æ¨ç†ã€‚</li>
<li>ç†è§£å’Œåˆ©ç”¨è¿™äº›æ¨ç†æ¨¡å¼å¯¹æé«˜LLMçš„æ¨ç†èƒ½åŠ›è‡³å…³é‡è¦ã€‚</li>
<li>è®ºæ–‡é€šè¿‡æ§åˆ¶è¯„ä¼°ç¯å¢ƒï¼Œç³»ç»Ÿåœ°ç ”ç©¶æ¨¡æ€ã€éš¾åº¦å’Œä»»åŠ¡æ ¼å¼å¯¹é€»è¾‘æ¨ç†çš„å½±å“ã€‚</li>
<li>å½’çº³ã€å‡è®¾å’Œæ¼”ç»æ¨ç†ç®¡é“çš„æ¯”è¾ƒåˆ†ææ­ç¤ºäº†å®ƒä»¬åœ¨ä¸åŒæ¡ä»¶ä¸‹çš„è¡¨ç°ã€‚</li>
<li>é«˜çº§èŒƒå¼å¦‚å‡è®¾é€‰æ‹©ã€éªŒè¯å’Œä¿®æ­£å¯¹æ‰©å¤§é€»è¾‘æ¨ç†æœ‰æ½œåŠ›ã€‚</li>
<li>è®ºæ–‡ç ”ç©¶ä¸ºé€šè¿‡ç³»ç»Ÿé€»è¾‘ç­–ç•¥æé«˜LLMæ¨ç†èƒ½åŠ›çš„æœªæ¥ç ”ç©¶å¥ å®šäº†åŸºç¡€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.11176">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-74050413fd0d9178dfec50c5f6e0166a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fef0b8ade205d74a391f743841fcbe5d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-621468b979fb63de248a29720fd968e5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-75fe903401a1163015d87838aaa3a1fa.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-23e40f58d2ecc119d36408c0640d2eef.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Large-Language-Model-Can-Be-a-Foundation-for-Hidden-Rationale-Based-Retrieval"><a href="#Large-Language-Model-Can-Be-a-Foundation-for-Hidden-Rationale-Based-Retrieval" class="headerlink" title="Large Language Model Can Be a Foundation for Hidden Rationale-Based   Retrieval"></a>Large Language Model Can Be a Foundation for Hidden Rationale-Based   Retrieval</h2><p><strong>Authors:Luo Ji, Feixiang Guo, Teng Chen, Qingqing Gu, Xiaoyu Wang, Ningyuan Xi, Yihong Wang, Peng Yu, Yue Zhao, Hongyang Lei, Zhonglin Jiang, Yong Chen</strong></p>
<p>Despite the recent advancement in Retrieval-Augmented Generation (RAG) systems, most retrieval methodologies are often developed for factual retrieval, which assumes query and positive documents are semantically similar. In this paper, we instead propose and study a more challenging type of retrieval task, called hidden rationale retrieval, in which query and document are not similar but can be inferred by reasoning chains, logic relationships, or empirical experiences. To address such problems, an instruction-tuned Large language model (LLM) with a cross-encoder architecture could be a reasonable choice. To further strengthen pioneering LLM-based retrievers, we design a special instruction that transforms the retrieval task into a generative task by prompting LLM to answer a binary-choice question. The model can be fine-tuned with direct preference optimization (DPO). The framework is also optimized for computational efficiency with no performance degradation. We name this retrieval framework by RaHoRe and verify its zero-shot and fine-tuned performance superiority on Emotional Support Conversation (ESC), compared with previous retrieval works. Our study suggests the potential to employ LLM as a foundation for a wider scope of retrieval tasks. Our codes, models, and datasets are available on <a target="_blank" rel="noopener" href="https://github.com/flyfree5/LaHoRe">https://github.com/flyfree5/LaHoRe</a>. </p>
<blockquote>
<p>å°½ç®¡æœ€è¿‘åœ¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿæ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†å¤§å¤šæ•°æ£€ç´¢æ–¹æ³•å¾€å¾€é’ˆå¯¹äº‹å®æ£€ç´¢è€Œå¼€å‘ï¼Œè¿™å‡è®¾æŸ¥è¯¢å’Œæ­£é¢æ–‡æ¡£åœ¨è¯­ä¹‰ä¸Šæ˜¯ç›¸ä¼¼çš„ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºå¹¶ç ”ç©¶äº†ä¸€ç§æ›´å…·æŒ‘æˆ˜æ€§çš„æ£€ç´¢ä»»åŠ¡ï¼Œå³éšè—é€»è¾‘æ£€ç´¢ï¼Œå…¶ä¸­æŸ¥è¯¢å’Œæ–‡æ¡£å¹¶ä¸ç›¸ä¼¼ï¼Œä½†å¯ä»¥é€šè¿‡æ¨ç†é“¾ã€é€»è¾‘å…³ç³»æˆ–ç»éªŒæ¨æ–­å¾—å‡ºã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œé‡‡ç”¨æŒ‡ä»¤è°ƒæ•´çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œè·¨ç¼–ç å™¨æ¶æ„å¯èƒ½æ˜¯ä¸€ä¸ªåˆç†çš„é€‰æ‹©ã€‚ä¸ºäº†è¿›ä¸€æ­¥åŠ å¼ºåŸºäºLLMçš„æ£€ç´¢å™¨ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªç‰¹æ®ŠæŒ‡ä»¤ï¼Œé€šè¿‡å°†æ£€ç´¢ä»»åŠ¡è½¬åŒ–ä¸ºç”Ÿæˆä»»åŠ¡ï¼Œæç¤ºLLMå›ç­”äºŒè¿›åˆ¶é€‰æ‹©é—®é¢˜ã€‚è¯¥æ¨¡å‹å¯é€šè¿‡ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰è¿›è¡Œå¾®è°ƒã€‚è¯¥æ¡†æ¶åœ¨è®¡ç®—æ•ˆç‡æ–¹é¢ä¹Ÿè¿›è¡Œäº†ä¼˜åŒ–ï¼Œä¸”ä¸ä¼šé™ä½æ€§èƒ½ã€‚æˆ‘ä»¬å°†è¿™ç§æ£€ç´¢æ¡†æ¶å‘½åä¸ºRaHoReï¼Œå¹¶åœ¨æƒ…æ„Ÿæ”¯æŒå¯¹è¯ï¼ˆESCï¼‰ä¸ŠéªŒè¯äº†å…¶é›¶æ ·æœ¬å’Œå¾®è°ƒåçš„æ€§èƒ½ä¼˜åŠ¿ï¼Œä¸ä¹‹å‰çš„æ£€ç´¢å·¥ä½œç›¸æ¯”ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œæœ‰æ½œåŠ›å°†LLMä½œä¸ºæ›´å¹¿æ³›æ£€ç´¢ä»»åŠ¡åŸºç¡€ã€‚æˆ‘ä»¬çš„ä»£ç ã€æ¨¡å‹å’Œæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/flyfree5/LaHoRe">https://github.com/flyfree5/LaHoRe</a>è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.16615v2">PDF</a> 10 pages, 3 figures, ECIR 2025</p>
<p><strong>Summary</strong><br>éšè—é€»è¾‘æ£€ç´¢ä»»åŠ¡åœ¨æŸ¥è¯¢å’Œæ–‡æ¡£ä¹‹é—´ä¸å­˜åœ¨ç›¸ä¼¼æ€§ï¼Œéœ€è¦é€šè¿‡æ¨ç†é“¾ã€é€»è¾‘å…³ç³»æˆ–ç»éªŒæ¥æ¨æ–­ã€‚æœ¬æ–‡æå‡ºä¸€ç§åŸºäºæŒ‡ä»¤ä¼˜åŒ–çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ£€ç´¢æ¡†æ¶ï¼Œå°†æ£€ç´¢ä»»åŠ¡è½¬åŒ–ä¸ºç”Ÿæˆä»»åŠ¡ï¼Œé€šè¿‡æç¤ºLLMå›ç­”äºŒé€‰ä¸€é—®é¢˜æ¥å®ç°ã€‚æ¡†æ¶ç»è¿‡ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰çš„å¾®è°ƒï¼Œè®¡ç®—æ•ˆç‡ä¼˜åŒ–ä¸”æ€§èƒ½æ— æŸå¤±ã€‚åœ¨æƒ…æ„Ÿæ”¯æŒå¯¹è¯ï¼ˆESCï¼‰ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œæ˜¾ç¤ºLLMåœ¨æ›´å¹¿æ³›æ£€ç´¢ä»»åŠ¡ä¸­çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>éšè—é€»è¾‘æ£€ç´¢ä»»åŠ¡ä¸­æŸ¥è¯¢å’Œæ–‡æ¡£ä¹‹é—´ä¸å­˜åœ¨ç›´æ¥è¯­ä¹‰ç›¸ä¼¼æ€§ï¼Œéœ€è¦é€šè¿‡æ¨ç†ã€é€»è¾‘æˆ–ç»éªŒæ¥å‘ç°å…³è”ã€‚</li>
<li>æå‡ºä½¿ç”¨æŒ‡ä»¤ä¼˜åŒ–çš„LLMæ¥è§£å†³éšè—é€»è¾‘æ£€ç´¢é—®é¢˜ï¼Œå°†æ£€ç´¢è½¬åŒ–ä¸ºç”Ÿæˆä»»åŠ¡ã€‚</li>
<li>é€šè¿‡æç¤ºLLMå›ç­”äºŒé€‰ä¸€é—®é¢˜æ¥å®ç°æ£€ç´¢ï¼Œå¢å¼ºæ¨¡å‹çš„é€‚ç”¨æ€§ã€‚</li>
<li>æ¡†æ¶é‡‡ç”¨ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰è¿›è¡Œå¾®è°ƒï¼Œæé«˜æ€§èƒ½ã€‚</li>
<li>æ¡†æ¶è®¡ç®—æ•ˆç‡é«˜ä¸”æ€§èƒ½æ— æŸå¤±ã€‚</li>
<li>åœ¨æƒ…æ„Ÿæ”¯æŒå¯¹è¯ï¼ˆESCï¼‰ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œè¯æ˜è¯¥æ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.16615">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-71c315138e1bee7b6bc0652fd8382214.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a27a7c22ac71a7dc1f64ecbbefa4ff8d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4b6185f0732452917dcd1ca26de4153c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d2de7cc6ecb79282f279f85364276089.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-11592773aa0f5fd6b6eefb4ccf68da0b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eda4d61130366799ddca2f6270caaabc.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Quantized-symbolic-time-series-approximation"><a href="#Quantized-symbolic-time-series-approximation" class="headerlink" title="Quantized symbolic time series approximation"></a>Quantized symbolic time series approximation</h2><p><strong>Authors:Erin Carson, Xinye Chen, Cheng Kang</strong></p>
<p>Time series are ubiquitous in numerous science and engineering domains, e.g., signal processing, bioinformatics, and astronomy. Previous work has verified the efficacy of symbolic time series representation in a variety of engineering applications due to its storage efficiency and numerosity reduction. The most recent symbolic aggregate approximation technique, ABBA, has been shown to preserve essential shape information of time series and improve downstream applications, e.g., neural network inference regarding prediction and anomaly detection in time series.   Motivated by the emergence of high-performance hardware which enables efficient computation for low bit-width representations, we present a new quantization-based ABBA symbolic approximation technique, QABBA, which exhibits improved storage efficiency while retaining the original speed and accuracy of symbolic reconstruction. We prove an upper bound for the error arising from quantization and discuss how the number of bits should be chosen to balance this with other errors.   An application of QABBA with large language models (LLMs) for time series regression is also presented, and its utility is investigated. By representing the symbolic chain of patterns on time series, QABBA not only avoids the training of embedding from scratch, but also achieves a new state-of-the-art on Monash regression dataset. The symbolic approximation to the time series offers a more efficient way to fine-tune LLMs on the time series regression task which contains various application domains. We further present a set of extensive experiments performed across various well-established datasets to demonstrate the advantages of the QABBA method for symbolic approximation. </p>
<blockquote>
<p>æ—¶é—´åºåˆ—åœ¨ä¿¡å·å¤„ç†ã€ç”Ÿç‰©ä¿¡æ¯å­¦å’Œå¤©æ–‡å­¦ç­‰ä¼—å¤šç§‘å­¦å’Œå·¥ç¨‹é¢†åŸŸæ— å¤„ä¸åœ¨ã€‚ä»¥å¾€çš„ç ”ç©¶å·¥ä½œå·²ç»éªŒè¯äº†ç¬¦å·æ—¶é—´åºåˆ—è¡¨ç¤ºåœ¨å„ç§å·¥ç¨‹åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ï¼Œå› ä¸ºå®ƒå…·æœ‰å­˜å‚¨æ•ˆç‡é«˜å’Œæ•°é‡å‡å°‘çš„ä¼˜ç‚¹ã€‚æœ€æ–°çš„ç¬¦å·èšåˆè¿‘ä¼¼æŠ€æœ¯ABBAå·²ç»è¯æ˜èƒ½å¤Ÿä¿ç•™æ—¶é—´åºåˆ—çš„åŸºæœ¬å½¢çŠ¶ä¿¡æ¯ï¼Œå¹¶æ”¹è¿›ä¸‹æ¸¸åº”ç”¨ï¼Œä¾‹å¦‚ç¥ç»ç½‘ç»œæ¨ç†å…³äºæ—¶é—´åºåˆ—çš„é¢„æµ‹å’Œå¼‚å¸¸æ£€æµ‹ã€‚</p>
</blockquote>
<p>éšç€é«˜æ€§èƒ½ç¡¬ä»¶çš„å‡ºç°ï¼Œä½¿å¾—ä½ä½å®½è¡¨ç¤ºçš„é«˜æ•ˆè®¡ç®—æˆä¸ºå¯èƒ½ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„åŸºäºé‡åŒ–çš„ABBAç¬¦å·è¿‘ä¼¼æŠ€æœ¯ï¼Œç§°ä¸ºQABBAã€‚å®ƒåœ¨æé«˜å­˜å‚¨æ•ˆç‡çš„åŒæ—¶ï¼Œä¿æŒäº†ç¬¦å·é‡å»ºçš„åŸå§‹é€Ÿåº¦å’Œå‡†ç¡®æ€§ã€‚æˆ‘ä»¬è¯æ˜äº†é‡åŒ–æ‰€äº§ç”Ÿçš„è¯¯å·®ä¸Šé™ï¼Œå¹¶è®¨è®ºäº†å¦‚ä½•é€‰æ‹©ä½æ•°æ¥å¹³è¡¡ä¸å…¶ä»–è¯¯å·®çš„å…³ç³»ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.15209v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æ—¶é—´åºåˆ—åœ¨å¤šä¸ªç§‘å­¦å’Œå·¥ç¨‹é¢†åŸŸä¸­çš„å¹¿æ³›åº”ç”¨ï¼Œä»¥åŠç¬¦å·æ—¶é—´åºåˆ—è¡¨ç¤ºæ³•åœ¨å·¥ç¨‹åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ã€‚æœ€æ–°ç¬¦å·èšåˆè¿‘ä¼¼æŠ€æœ¯ABBAèƒ½å¤Ÿä¿ç•™æ—¶é—´åºåˆ—çš„é‡è¦å½¢çŠ¶ä¿¡æ¯ï¼Œå¹¶æ”¹è¿›ä¸‹æ¸¸åº”ç”¨ï¼Œå¦‚ç¥ç»ç½‘ç»œæ¨ç†ä¸­çš„æ—¶é—´åºåˆ—é¢„æµ‹å’Œå¼‚å¸¸æ£€æµ‹ã€‚å—é«˜æ€§èƒ½ç¡¬ä»¶çš„å¯å‘ï¼Œæå‡ºäº†ä¸€ç§åŸºäºé‡åŒ–çš„ABBAç¬¦å·è¿‘ä¼¼æŠ€æœ¯QABBAï¼Œå®ƒåœ¨ä¿æŒç¬¦å·é‡å»ºçš„åŸå§‹é€Ÿåº¦å’Œå‡†ç¡®æ€§çš„åŒæ—¶ï¼Œæé«˜äº†å­˜å‚¨æ•ˆç‡ã€‚æ–‡ç« è¿˜è¯æ˜äº†é‡åŒ–è¯¯å·®çš„ä¸Šç•Œï¼Œå¹¶è®¨è®ºäº†å¦‚ä½•é€‰æ‹©ä½æ•°æ¥å¹³è¡¡è¯¯å·®ã€‚å°†QABBAåº”ç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œæ—¶é—´åºåˆ—å›å½’ï¼Œä¸ä»…é¿å…äº†ä»å¤´å¼€å§‹è®­ç»ƒåµŒå…¥çš„éœ€è¦ï¼Œè€Œä¸”åœ¨Monashå›å½’æ•°æ®é›†ä¸Šå®ç°äº†æœ€æ–°æ°´å¹³ã€‚ç¬¦å·è¿‘ä¼¼ä¸ºåœ¨åŒ…å«å„ç§åº”ç”¨åŸŸçš„å›å½’ä»»åŠ¡ä¸Šå¾®è°ƒLLMæä¾›äº†æ›´æœ‰æ•ˆçš„æ–¹æ³•ã€‚é€šè¿‡ä¸€ç³»åˆ—å¹¿æ³›å®éªŒè¯æ˜äº†QABBAæ–¹æ³•åœ¨ç¬¦å·è¿‘ä¼¼æ–¹é¢çš„ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¬¦å·æ—¶é—´åºåˆ—è¡¨ç¤ºæ³•å…·æœ‰å­˜å‚¨æ•ˆç‡é«˜å’Œæ•°é‡å‡å°‘çš„ä¼˜ç‚¹ï¼Œå·²åœ¨å¤šç§å·¥ç¨‹åº”ç”¨ä¸­å¾—åˆ°éªŒè¯ã€‚</li>
<li>ABBAæŠ€æœ¯èƒ½å¤Ÿä¿ç•™æ—¶é—´åºåˆ—çš„é‡è¦å½¢çŠ¶ä¿¡æ¯ï¼Œå¹¶æ”¹è¿›é¢„æµ‹å’Œå¼‚å¸¸æ£€æµ‹ç­‰ä¸‹æ¸¸åº”ç”¨ã€‚</li>
<li>QABBAæ˜¯ä¸€ç§æ–°çš„åŸºäºé‡åŒ–çš„ç¬¦å·è¿‘ä¼¼æŠ€æœ¯ï¼Œæé«˜äº†å­˜å‚¨æ•ˆç‡ï¼ŒåŒæ—¶ä¿æŒäº†ç¬¦å·é‡å»ºçš„åŸå§‹é€Ÿåº¦å’Œå‡†ç¡®æ€§ã€‚</li>
<li>QABBAåœ¨æ—¶é—´åºåˆ—å›å½’ä»»åŠ¡ä¸­åº”ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œå®ç°äº†Monashå›å½’æ•°æ®é›†ä¸Šçš„æœ€æ–°æ°´å¹³ã€‚</li>
<li>QABBAé¿å…äº†ä»å¤´å¼€å§‹è®­ç»ƒåµŒå…¥çš„éœ€è¦ï¼Œä¸ºåœ¨å›å½’ä»»åŠ¡ä¸Šå¾®è°ƒLLMæä¾›äº†æ›´æœ‰æ•ˆçš„æ–¹æ³•ã€‚</li>
<li>æ–‡ç« é€šè¿‡ä¸€ç³»åˆ—å¹¿æ³›å®éªŒè¯æ˜äº†QABBAæ–¹æ³•åœ¨ç¬¦å·è¿‘ä¼¼æ–¹é¢çš„ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.15209">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-22e87587537a5751c544de7a061c7fda.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-751dd611ae2775f7f0072b45e579efe8.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="UFT-Unifying-Fine-Tuning-of-SFT-and-RLHF-DPO-UNA-through-a-Generalized-Implicit-Reward-Function"><a href="#UFT-Unifying-Fine-Tuning-of-SFT-and-RLHF-DPO-UNA-through-a-Generalized-Implicit-Reward-Function" class="headerlink" title="UFT: Unifying Fine-Tuning of SFT and RLHF&#x2F;DPO&#x2F;UNA through a Generalized   Implicit Reward Function"></a>UFT: Unifying Fine-Tuning of SFT and RLHF&#x2F;DPO&#x2F;UNA through a Generalized   Implicit Reward Function</h2><p><strong>Authors:Zhichao Wang, Bin Bi, Zixu Zhu, Xiangbo Mao, Jun Wang, Shiyu Wang</strong></p>
<p>By pretraining on trillions of tokens, an LLM gains the capability of text generation. However, to enhance its utility and reduce potential harm, SFT and alignment are applied sequentially to the pretrained model. Due to the differing nature and objective functions of SFT and alignment, catastrophic forgetting has become a significant issue. To address this, we introduce Unified Fine-Tuning (UFT), which integrates SFT and alignment into a single training stage using the same objective and loss functions through an implicit reward function. Our experimental results demonstrate that UFT outperforms SFT on instruction-tuning data alone. Moreover, when combining instruction-tuning data with alignment data, UFT effectively prevents catastrophic forgetting across these two stages and shows a clear advantage over sequentially applying SFT and alignment. This is evident in the significant improvements observed in the \textbf{ifeval} task for instruction-following and the \textbf{truthful-qa} task for factuality. The proposed general fine-tuning framework UFT establishes an effective and efficient pretraining-UFT paradigm for LLM training. </p>
<blockquote>
<p>é€šè¿‡é¢„è®­ç»ƒå¤§é‡çš„è¯­è¨€æ ‡è®°ï¼ˆtokensï¼‰ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è·å¾—äº†æ–‡æœ¬ç”Ÿæˆçš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œä¸ºäº†æé«˜å…¶å®ç”¨æ€§å’Œå‡å°‘æ½œåœ¨é£é™©ï¼Œåºåˆ—å¾®è°ƒï¼ˆSFTï¼‰å’Œå¯¹é½æŠ€æœ¯è¢«è¿ç»­åº”ç”¨äºé¢„è®­ç»ƒæ¨¡å‹ã€‚ç”±äºåºåˆ—å¾®è°ƒå’Œå¯¹é½åœ¨æœ¬è´¨å’Œç›®çš„å‡½æ•°ä¸Šçš„ä¸åŒï¼Œç¾éš¾æ€§é—å¿˜å·²ç»æˆä¸ºäº†ä¸€ä¸ªä¸»è¦é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç»Ÿä¸€å¾®è°ƒï¼ˆUFTï¼‰çš„æ–¹æ³•ã€‚ç»Ÿä¸€å¾®è°ƒé€šè¿‡å°†åºåˆ—å¾®è°ƒå’Œå¯¹é½æŠ€æœ¯æ•´åˆåˆ°ä¸€ä¸ªè®­ç»ƒé˜¶æ®µä¸­ï¼Œä½¿ç”¨ç›¸åŒçš„ç›®æ ‡å’ŒæŸå¤±å‡½æ•°ä»¥åŠä¸€ä¸ªéšå¼å¥–åŠ±å‡½æ•°ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼Œç»Ÿä¸€å¾®è°ƒåœ¨ä»…ä½¿ç”¨æŒ‡ä»¤å¾®è°ƒæ•°æ®çš„æƒ…å†µä¸‹ï¼Œè¡¨ç°ä¼˜äºåºåˆ—å¾®è°ƒã€‚æ­¤å¤–ï¼Œå½“å°†æŒ‡ä»¤å¾®è°ƒæ•°æ®å’Œå¯¹é½æ•°æ®ç›¸ç»“åˆæ—¶ï¼Œç»Ÿä¸€å¾®è°ƒå¯ä»¥æœ‰æ•ˆåœ°é˜²æ­¢è¿™ä¸¤ä¸ªé˜¶æ®µçš„ç¾éš¾æ€§é—å¿˜ï¼Œå¹¶ä¸”åœ¨åº”ç”¨åºåˆ—å¾®è°ƒå’Œå¯¹é½æ—¶è¡¨ç°å‡ºæ˜æ˜¾çš„ä¼˜åŠ¿ã€‚è¿™åœ¨æŒ‡ä»¤è·Ÿéšçš„\textbf{ifeval}ä»»åŠ¡å’Œäº‹å®æ€§çš„\textbf{truthful-qa}ä»»åŠ¡ä¸­å¾—åˆ°äº†æ˜¾è‘—çš„æ”¹å–„ã€‚æå‡ºçš„é€šç”¨å¾®è°ƒæ¡†æ¶UFTä¸ºLLMè®­ç»ƒå»ºç«‹äº†ä¸€ç§æœ‰æ•ˆä¸”é«˜æ•ˆçš„é¢„è®­ç»ƒ-UFTèŒƒå¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.21438v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>é¢„è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡è®­ç»ƒå¤§é‡æ–‡æœ¬æ•°æ®è·å¾—æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›ã€‚ä¸ºæé«˜å…¶å®ç”¨æ€§å’Œå‡å°‘æ½œåœ¨é£é™©ï¼Œåºè´¯åº”ç”¨å¾®è°ƒï¼ˆSFTï¼‰å’Œå¯¹é½ï¼Œä½†ç”±äºä¸¤è€…æ€§è´¨å’Œç›®æ ‡çš„å·®å¼‚ï¼Œç¾éš¾æ€§é—å¿˜æˆä¸ºæ˜¾è‘—é—®é¢˜ã€‚æœ¬ç ”ç©¶æå‡ºç»Ÿä¸€å¾®è°ƒï¼ˆUFTï¼‰ï¼Œå°†SFTå’Œå¯¹é½é›†æˆåˆ°ä¸€ä¸ªè®­ç»ƒé˜¶æ®µï¼Œä½¿ç”¨ç›¸åŒçš„ç›®æ ‡å’ŒæŸå¤±å‡½æ•°ï¼Œé€šè¿‡éšå¼å¥–åŠ±å‡½æ•°å®ç°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒUFTåœ¨ä»…ä½¿ç”¨æŒ‡ä»¤å¾®è°ƒæ•°æ®ä¸Šä¼˜äºSFTï¼Œä¸”åœ¨ç»“åˆæŒ‡ä»¤å¾®è°ƒæ•°æ®å’Œå¯¹é½æ•°æ®æ—¶ï¼Œèƒ½æœ‰æ•ˆé˜²æ­¢ç¾éš¾æ€§é—å¿˜ï¼Œå¹¶åœ¨æŒ‡ä»¤è·Ÿéšçš„ifevalä»»åŠ¡å’Œäº‹å®æ€§çš„truthful-qaä»»åŠ¡ä¸­è¡¨ç°å‡ºæ˜æ˜¾ä¼˜åŠ¿ã€‚UFTå»ºç«‹äº†ä¸€ä¸ªæœ‰æ•ˆä¸”é«˜æ•ˆçš„é¢„è®­ç»ƒ-UFTèŒƒå¼ï¼Œä¸ºLLMè®­ç»ƒæä¾›äº†æ–°çš„æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMé€šè¿‡é¢„è®­ç»ƒè·å¾—æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›ã€‚</li>
<li>SFTå’Œå¯¹é½åºè´¯åº”ç”¨äºLLMä»¥æé«˜å®ç”¨æ€§å’Œå‡å°‘é£é™©ã€‚</li>
<li>ç¾éš¾æ€§é—å¿˜æ˜¯SFTå’Œå¯¹é½åº”ç”¨ä¸­çš„ä¸»è¦é—®é¢˜ã€‚</li>
<li>UFTå°†SFTå’Œå¯¹é½é›†æˆåˆ°ä¸€ä¸ªè®­ç»ƒé˜¶æ®µï¼Œä½¿ç”¨ç›¸åŒçš„ç›®æ ‡å’ŒæŸå¤±å‡½æ•°ã€‚</li>
<li>UFTé€šè¿‡éšå¼å¥–åŠ±å‡½æ•°å®ç°é›†æˆã€‚</li>
<li>å®éªŒæ˜¾ç¤ºUFTåœ¨æŒ‡ä»¤å¾®è°ƒæ•°æ®å’Œä»»åŠ¡è¡¨ç°ä¸Šä¼˜äºSFTã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.21438">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8a941d9f2db063b4f30bf5e3eed084a5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-704e9494880916d2ee04205352724def.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fb42ab327f9b867b71b2bb188fb4ccdd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5d35e71db259bde74a30ee6ebe1357ac.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fd7af33abc9335a67f389343a5a03879.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0c748e0e6cc1559118ed34c0413532b0.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="AdvBDGen-Adversarially-Fortified-Prompt-Specific-Fuzzy-Backdoor-Generator-Against-LLM-Alignment"><a href="#AdvBDGen-Adversarially-Fortified-Prompt-Specific-Fuzzy-Backdoor-Generator-Against-LLM-Alignment" class="headerlink" title="AdvBDGen: Adversarially Fortified Prompt-Specific Fuzzy Backdoor   Generator Against LLM Alignment"></a>AdvBDGen: Adversarially Fortified Prompt-Specific Fuzzy Backdoor   Generator Against LLM Alignment</h2><p><strong>Authors:Pankayaraj Pathmanathan, Udari Madhushani Sehwag, Michael-Andrei Panaitescu-Liess, Furong Huang</strong></p>
<p>With the growing adoption of reinforcement learning with human feedback (RLHF) for aligning large language models (LLMs), the risk of backdoor installation during alignment has increased, leading to unintended and harmful behaviors. Existing backdoor triggers are typically limited to fixed word patterns, making them detectable during data cleaning and easily removable post-poisoning. In this work, we explore the use of prompt-specific paraphrases as backdoor triggers, enhancing their stealth and resistance to removal during LLM alignment. We propose AdvBDGen, an adversarially fortified generative fine-tuning framework that automatically generates prompt-specific backdoors that are effective, stealthy, and transferable across models. AdvBDGen employs a generator-discriminator pair, fortified by an adversary, to ensure the installability and stealthiness of backdoors. It enables the crafting and successful installation of complex triggers using as little as 3% of the fine-tuning data. Once installed, these backdoors can jailbreak LLMs during inference, demonstrate improved stability against perturbations compared to traditional constant triggers, and are more challenging to remove. These findings underscore an urgent need for the research community to develop more robust defenses against adversarial backdoor threats in LLM alignment. </p>
<blockquote>
<p>éšç€å¼ºåŒ–å­¦ä¹ ç»“åˆäººç±»åé¦ˆï¼ˆRLHFï¼‰åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯¹é½ä¸­çš„æ—¥ç›Šæ™®åŠï¼Œå¯¹é½è¿‡ç¨‹ä¸­çš„åé—¨å®‰è£…é£é™©ä¹Ÿéšä¹‹å¢åŠ ï¼Œå¯èƒ½å¯¼è‡´å‡ºç°æ— æ„ä¸­çš„æœ‰å®³è¡Œä¸ºã€‚ç°æœ‰çš„åé—¨è§¦å‘é€šå¸¸ä»…é™äºå›ºå®šè¯è¯­æ¨¡å¼ï¼Œå› æ­¤åœ¨æ•°æ®æ¸…æ´—æ—¶å¯æ£€æµ‹åˆ°ï¼Œå¹¶ä¸”åœ¨ä¸­æ¯’åæ˜“äºç§»é™¤ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ¢ç´¢ä½¿ç”¨ç‰¹å®šæç¤ºçš„åŒä¹‰æ›¿æ¢ä½œä¸ºåé—¨è§¦å‘ï¼Œæé«˜å…¶éšè”½æ€§ï¼Œå¹¶å¢å¼ºå…¶åœ¨LLMå¯¹é½è¿‡ç¨‹ä¸­å¯¹ç§»é™¤çš„æŠµæŠ—æ€§ã€‚æˆ‘ä»¬æå‡ºAdvBDGenï¼Œè¿™æ˜¯ä¸€ä¸ªç»è¿‡å¯¹æŠ—æ€§å¼ºåŒ–è®­ç»ƒçš„ç”Ÿæˆå¾®è°ƒæ¡†æ¶ï¼Œå¯è‡ªåŠ¨ç”Ÿæˆç‰¹å®šæç¤ºçš„åé—¨ï¼Œè¿™äº›åé—¨æœ‰æ•ˆã€éšè”½ï¼Œå¹¶åœ¨æ¨¡å‹ä¹‹é—´å…·æœ‰å¯è½¬ç§»æ€§ã€‚AdvBDGené‡‡ç”¨ç”Ÿæˆå™¨-é‰´åˆ«å™¨å¯¹ï¼Œé€šè¿‡å¯¹æ‰‹ä¿éšœåé—¨å®‰è£…å’Œéšè”½æ€§ã€‚å®ƒèƒ½å¤Ÿåœ¨ä»…ä½¿ç”¨å¾®è°ƒæ•°æ®çš„3%çš„æƒ…å†µä¸‹ï¼Œç²¾å¿ƒåˆ¶ä½œå¹¶æˆåŠŸå®‰è£…å¤æ‚çš„è§¦å‘å™¨ã€‚ä¸€æ—¦å®‰è£…å®Œæˆï¼Œè¿™äº›åé—¨å¯åœ¨æ¨ç†è¿‡ç¨‹ä¸­ç ´è§£LLMï¼Œä¸ä¼ ç»Ÿæ’å®šè§¦å‘å™¨ç›¸æ¯”ï¼Œå®ƒä»¬è¡¨ç°å‡ºæ›´å¼ºçš„å¯¹æŠ—æ‰°åŠ¨çš„èƒ½åŠ›ï¼Œå¹¶ä¸”æ›´éš¾è¢«ç§»é™¤ã€‚è¿™äº›å‘ç°çªæ˜¾å‡ºç ”ç©¶ç•Œè¿«åˆ‡éœ€è¦å¯¹LLMå¯¹é½ä¸­çš„å¯¹æŠ—æ€§åé—¨å¨èƒå¼€å‘æ›´å¼ºå¤§çš„é˜²å¾¡æ‰‹æ®µã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.11283v2">PDF</a> Published at the Neurips Safe Generative AI Workshop 2024</p>
<p><strong>Summary</strong><br>å¼ºåŒ–å­¦ä¹ ç»“åˆäººç±»åé¦ˆï¼ˆRLHFï¼‰åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„åº”ç”¨æ—¥ç›Šæ™®åŠï¼Œå¢åŠ äº†åé—¨å®‰è£…çš„é£é™©ï¼Œå¯èƒ½å¯¼è‡´æ„å¤–å’Œæœ‰å®³çš„è¡Œä¸ºã€‚ç°æœ‰çš„åé—¨è§¦å‘é€šå¸¸å±€é™äºå›ºå®šçš„è¯æ¨¡å¼ï¼Œä½¿å¾—å®ƒä»¬åœ¨æ•°æ®æ¸…ç†è¿‡ç¨‹ä¸­å¯ä»¥è¢«æ£€æµ‹åˆ°å¹¶ä¸”å®¹æ˜“è¢«ä¸­æ¯’åç§»é™¤ã€‚æœ¬ç ”ç©¶æ¢ç´¢ä½¿ç”¨ç‰¹å®šæç¤ºçš„æ”¹è¿°ä½œä¸ºåé—¨è§¦å‘ï¼Œä»¥æé«˜å…¶éšè”½æ€§å’Œåœ¨LLMå¯¹é½è¿‡ç¨‹ä¸­çš„æŠ—ç§»é™¤æ€§ã€‚æˆ‘ä»¬æå‡ºäº†AdvBDGenï¼Œè¿™æ˜¯ä¸€ç§ç»è¿‡å¯¹æŠ—åŠ å›ºçš„ç”Ÿæˆå¾®è°ƒæ¡†æ¶ï¼Œå¯è‡ªåŠ¨ç”Ÿæˆæœ‰æ•ˆã€éšè”½ä¸”è·¨æ¨¡å‹å¯è¿ç§»çš„ç‰¹å®šæç¤ºåé—¨ã€‚AdvBDGené‡‡ç”¨ç”Ÿæˆå™¨-é‰´åˆ«å™¨å¯¹ï¼Œå¹¶ç”±å¯¹æ‰‹åŠ å›ºï¼Œä»¥ç¡®ä¿åé—¨çš„å¯å®‰è£…æ€§å’Œéšè”½æ€§ã€‚å®ƒèƒ½åœ¨ä»…ä½¿ç”¨3%çš„å¾®è°ƒæ•°æ®çš„æƒ…å†µä¸‹æˆåŠŸå®‰è£…å¤æ‚è§¦å‘å™¨ã€‚å®‰è£…åï¼Œè¿™äº›åé—¨åœ¨æ¨ç†è¿‡ç¨‹ä¸­èƒ½å¤Ÿçªç ´LLMçš„é™åˆ¶ï¼Œç›¸è¾ƒäºä¼ ç»Ÿçš„æ’å®šè§¦å‘å™¨è¡¨ç°å‡ºæ›´é«˜çš„æŠ—å¹²æ‰°æ€§ï¼Œå¹¶ä¸”æ›´éš¾è¢«ç§»é™¤ã€‚è¿™äº›å‘ç°çªæ˜¾äº†ç ”ç©¶ç•Œè¿«åˆ‡éœ€è¦å¼€å‘æ›´å¼ºå¤§çš„é˜²å¾¡æªæ–½æ¥å¯¹æŠ—LLMå¯¹é½ä¸­çš„å¯¹æŠ—æ€§åé—¨å¨èƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ ç»“åˆäººç±»åé¦ˆåœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ä¸­çš„åº”ç”¨å¸¦æ¥äº†åé—¨å®‰è£…é£é™©çš„å¢åŠ ã€‚</li>
<li>ç°æœ‰åé—¨è§¦å‘ä¸»è¦å±€é™äºå›ºå®šè¯æ¨¡å¼ï¼Œç›¸å¯¹å®¹æ˜“è¢«æ£€æµ‹å’Œç§»é™¤ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä½¿ç”¨ç‰¹å®šæç¤ºçš„æ”¹è¿°ä½œä¸ºåé—¨è§¦å‘ï¼Œä»¥æé«˜å…¶éšè”½æ€§å’ŒæŠ—ç§»é™¤æ€§ã€‚</li>
<li>AdvBDGenæ¡†æ¶èƒ½è‡ªåŠ¨ç”Ÿæˆæœ‰æ•ˆã€éšè”½ä¸”è·¨æ¨¡å‹å¯è¿ç§»çš„ç‰¹å®šæç¤ºåé—¨ã€‚</li>
<li>AdvBDGené‡‡ç”¨ç”Ÿæˆå™¨-é‰´åˆ«å™¨å¯¹å¹¶ç”±å¯¹æ‰‹åŠ å›ºï¼Œç¡®ä¿åé—¨çš„å¯å®‰è£…æ€§å’Œéšè”½æ€§ã€‚</li>
<li>åé—¨èƒ½åœ¨å¾®è°ƒæ•°æ®å¾ˆå°‘çš„æƒ…å†µä¸‹æˆåŠŸå®‰è£…å¤æ‚è§¦å‘å™¨ï¼Œä¸”æ›´éš¾è¢«ç§»é™¤ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.11283">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-df76ef79bda4d4f86462144d8d284faa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ae95a851b80ddb9edee8cea3bfe07b67.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3691db0a2668d8bf92dad42a9e47bcb6.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Differential-Transformer"><a href="#Differential-Transformer" class="headerlink" title="Differential Transformer"></a>Differential Transformer</h2><p><strong>Authors:Tianzhu Ye, Li Dong, Yuqing Xia, Yutao Sun, Yi Zhu, Gao Huang, Furu Wei</strong></p>
<p>Transformer tends to overallocate attention to irrelevant context. In this work, we introduce Diff Transformer, which amplifies attention to the relevant context while canceling noise. Specifically, the differential attention mechanism calculates attention scores as the difference between two separate softmax attention maps. The subtraction cancels noise, promoting the emergence of sparse attention patterns. Experimental results on language modeling show that Diff Transformer outperforms Transformer in various settings of scaling up model size and training tokens. More intriguingly, it offers notable advantages in practical applications, such as long-context modeling, key information retrieval, hallucination mitigation, in-context learning, and reduction of activation outliers. By being less distracted by irrelevant context, Diff Transformer can mitigate hallucination in question answering and text summarization. For in-context learning, Diff Transformer not only enhances accuracy but is also more robust to order permutation, which was considered as a chronic robustness issue. The results position Diff Transformer as a highly effective and promising architecture to advance large language models. </p>
<blockquote>
<p>Transformeræ¨¡å‹å¾€å¾€ä¼šå¯¹æ— å…³ä¸Šä¸‹æ–‡è¿‡åº¦åˆ†é…æ³¨æ„åŠ›ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†Diff Transformerï¼Œå®ƒé€šè¿‡æ”¾å¤§å¯¹æœ‰å…³ä¸Šä¸‹æ–‡çš„æ³¨æ„åŠ›åŒæ—¶å–æ¶ˆå™ªå£°æ¥æ”¹è¿›ã€‚å…·ä½“æ¥è¯´ï¼Œå·®åˆ†æ³¨æ„åŠ›æœºåˆ¶å°†æ³¨æ„åŠ›å¾—åˆ†è®¡ç®—ä¸ºä¸¤ä¸ªå•ç‹¬çš„softmaxæ³¨æ„åŠ›å›¾ä¹‹é—´çš„å·®å¼‚ã€‚å‡æ³•è¿ç®—å–æ¶ˆäº†å™ªå£°ï¼Œä¿ƒè¿›äº†ç¨€ç–æ³¨æ„åŠ›æ¨¡å¼çš„å‡ºç°ã€‚åœ¨è¯­è¨€å»ºæ¨¡æ–¹é¢çš„å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å„ç§æ‰©å¤§æ¨¡å‹è§„æ¨¡å’Œè®­ç»ƒä»¤ç‰Œçš„è®¾ç½®ä¸­ï¼ŒDiff Transformerçš„è¡¨ç°éƒ½ä¼˜äºTransformerã€‚æ›´å¼•äººæ³¨ç›®çš„æ˜¯ï¼Œå®ƒåœ¨å®é™…åº”ç”¨ä¸­è¡¨ç°å‡ºæ˜¾è‘—çš„ä¼˜åŠ¿ï¼Œå¦‚é•¿ä¸Šä¸‹æ–‡å»ºæ¨¡ã€å…³é”®ä¿¡æ¯æ£€ç´¢ã€å‡è½»å¹»æƒ³ç°è±¡ã€ä¸Šä¸‹æ–‡å†…å­¦ä¹ å’Œå‡å°‘æ¿€æ´»å¼‚å¸¸å€¼ç­‰ã€‚ç”±äºè¾ƒå°‘å—åˆ°æ— å…³ä¸Šä¸‹æ–‡çš„å¹²æ‰°ï¼ŒDiff Transformerå¯ä»¥å‡è½»é—®ç­”å’Œæ–‡æœ¬æ‘˜è¦ä¸­çš„å¹»æƒ³ç°è±¡ã€‚åœ¨ä¸Šä¸‹æ–‡å­¦ä¹ ä¸­ï¼ŒDiff Transformerä¸ä»…æé«˜äº†å‡†ç¡®æ€§ï¼Œè€Œä¸”å¯¹é¡ºåºæ’åˆ—çš„é²æ£’æ€§æ›´å¼ºï¼Œåè€…è¢«è§†ä¸ºé•¿æœŸçš„ç¨³å¥æ€§é—®é¢˜ã€‚è¿™äº›ç»“æœä½¿Diff Transformeræˆä¸ºä¸€ç§æœ‰æ•ˆä¸”å‰æ™¯å¹¿é˜”çš„æ¶æ„ï¼Œå¯æ¨åŠ¨å¤§å‹è¯­è¨€æ¨¡å‹çš„å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.05258v2">PDF</a> Accepted as an Oral Presentation at ICLR 2025</p>
<p><strong>Summary</strong><br>     å·®åˆ†æ³¨æ„åŠ›æœºåˆ¶èƒ½å¤Ÿæœ‰æ•ˆæŠ‘åˆ¶Transformeræ¨¡å‹ä¸­æ³¨æ„åŠ›åˆ†é…å¯¹æ— å…³è¯­å¢ƒçš„è¿‡åº¦å…³æ³¨ï¼Œæå‡å¯¹å…³é”®è¯­å¢ƒçš„æ³¨æ„åŠ›åˆ†é…ã€‚å·®åˆ†æ³¨æ„åŠ›æœºåˆ¶é€šè¿‡è®¡ç®—ä¸¤ä¸ªç‹¬ç«‹softmaxæ³¨æ„åŠ›åœ°å›¾çš„å·®å€¼æ¥å¾—åˆ°æ³¨æ„åŠ›åˆ†æ•°ï¼Œä»¥æ­¤å®ç°é™ä½å™ªéŸ³å’Œå‡¸æ˜¾ç¨€ç–æ³¨æ„åŠ›æ¨¡å¼çš„æ•ˆæœã€‚å®éªŒç»“æœè¯æ˜ï¼Œæ— è®ºæ˜¯åœ¨è¯­è¨€å»ºæ¨¡ï¼Œè¿˜æ˜¯åœ¨å®é™…åº”ç”¨ä¸­ï¼Œå·®åˆ†Transformeréƒ½åœ¨å¤šç§è®¾ç½®ä¸­å±•ç°å‡ºè¶…è¶ŠTransformerçš„æ€§èƒ½ä¼˜åŠ¿ï¼ŒåŒ…æ‹¬é•¿è¯­å¢ƒå»ºæ¨¡ã€å…³é”®ä¿¡æ¯æ£€ç´¢ã€å¹»è§‰ç¼“è§£ã€ä¸Šä¸‹æ–‡å­¦ä¹ å’Œæ¿€æ´»å¼‚å¸¸å€¼å‡å°‘ç­‰ã€‚å·®åˆ†Transformeré€šè¿‡å‡å°‘å¯¹æ— å…³è¯­å¢ƒçš„å¹²æ‰°ï¼Œæå‡äº†é—®ç­”å’Œæ–‡æœ¬æ‘˜è¦ä¸­çš„å¹»è§‰ç¼“è§£èƒ½åŠ›ã€‚å¯¹äºä¸Šä¸‹æ–‡å­¦ä¹ ï¼Œå·®åˆ†Transformerä¸ä»…æé«˜äº†å‡†ç¡®æ€§ï¼Œè€Œä¸”å¯¹é¡ºåºæ’åˆ—çš„é²æ£’æ€§æ›´å¼ºã€‚è¿™äº›ç»“æœä½¿å¾—å·®åˆ†Transformeræˆä¸ºä¸€ç§æœ‰æ•ˆä¸”æœ‰å‰é€”çš„æ¶æ„ï¼Œæœ‰æœ›æ¨åŠ¨å¤§å‹è¯­è¨€æ¨¡å‹çš„å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å·®åˆ†Transformeré€šè¿‡è®¡ç®—ä¸¤ä¸ªç‹¬ç«‹softmaxæ³¨æ„åŠ›åœ°å›¾çš„å·®å€¼æ¥è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°ï¼Œå®ç°å¯¹æ— å…³è¯­å¢ƒæ³¨æ„åŠ›çš„æŠ‘åˆ¶å’Œå¯¹å…³é”®è¯­å¢ƒæ³¨æ„åŠ›çš„æå‡ã€‚</li>
<li>å·®åˆ†æ³¨æ„åŠ›æœºåˆ¶æœ‰åŠ©äºé™ä½æ¨¡å‹å¯¹å™ªéŸ³çš„æ•æ„Ÿåº¦ï¼Œå‡¸æ˜¾ç¨€ç–æ³¨æ„åŠ›æ¨¡å¼ã€‚</li>
<li>å·®åˆ†Transformeråœ¨å„ç§è¯­è¨€å»ºæ¨¡ç¯å¢ƒä¸­è¡¨ç°å‡ºä¼˜è¶Šæ€§èƒ½ï¼ŒåŒ…æ‹¬é•¿è¯­å¢ƒå»ºæ¨¡ã€å…³é”®ä¿¡æ¯æ£€ç´¢ç­‰ã€‚</li>
<li>åœ¨å®é™…åº”ç”¨ä¸­ï¼Œå·®åˆ†Transformerå…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œå¦‚ç¼“è§£é—®ç­”å’Œæ–‡æœ¬æ‘˜è¦ä¸­çš„å¹»è§‰é—®é¢˜ï¼Œæé«˜ä¸Šä¸‹æ–‡å­¦ä¹ çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚</li>
<li>å·®åˆ†Transformerå¯¹äºæ¨¡å‹è§„æ¨¡æ‰©å¤§å’Œè®­ç»ƒæ ‡è®°æ‰©å±•ç­‰è®¾ç½®ä¹Ÿæœ‰å‡ºè‰²çš„è¡¨ç°ã€‚</li>
<li>ä¸ä¼ ç»ŸTransformerç›¸æ¯”ï¼Œå·®åˆ†Transformerå…·æœ‰æ›´é«˜çš„æ•ˆç‡å’Œæ›´å¥½çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.05258">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-b157975708759de43872ccc02df65c73.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c7ef01abb8bd54b615ceba054af26504.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef81e115400aae1490646491f06f563f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-582d2d0ac86321b8300f026c015adfa0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ecf3991d10e8a41ba22944167cf65eea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bdd6758a8ac509463c22302393493a7d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d2375bcff472d06636d229606260481b.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="CodeMMLU-A-Multi-Task-Benchmark-for-Assessing-Code-Understanding-Reasoning-Capabilities-of-CodeLLMs"><a href="#CodeMMLU-A-Multi-Task-Benchmark-for-Assessing-Code-Understanding-Reasoning-Capabilities-of-CodeLLMs" class="headerlink" title="CodeMMLU: A Multi-Task Benchmark for Assessing Code Understanding &amp;   Reasoning Capabilities of CodeLLMs"></a>CodeMMLU: A Multi-Task Benchmark for Assessing Code Understanding &amp;   Reasoning Capabilities of CodeLLMs</h2><p><strong>Authors:Dung Nguyen Manh, Thang Phan Chau, Nam Le Hai, Thong T. Doan, Nam V. Nguyen, Quang Pham, Nghi D. Q. Bui</strong></p>
<p>Recent advances in Code Large Language Models (CodeLLMs) have primarily focused on open-ended code generation, often overlooking the crucial aspect of code understanding and reasoning. To bridge this gap, we introduce CodeMMLU, a comprehensive multiple-choice benchmark designed to evaluate the depth of software and code comprehension in LLMs. CodeMMLU includes nearly 20,000 questions spanning diverse domains, including code analysis, defect detection, and software engineering principles across multiple programming languages. Unlike traditional benchmarks that emphasize code generation, CodeMMLU assesses a modelâ€™s ability to reason about programs across a wide-range of tasks such as code repair, execution reasoning, and fill-in-the-blank challenges. Our extensive evaluation reveals that even state-of-the-art models struggle with CodeMMLU, highlighting significant gaps in comprehension beyond generation. By emphasizing the essential connection between code understanding and effective AI-assisted development, CodeMMLU provides a critical resource for advancing more reliable and capable coding assistants. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œä»£ç å¤§è¯­è¨€æ¨¡å‹ï¼ˆCodeLLMï¼‰çš„è¿›æ­¥ä¸»è¦é›†ä¸­åœ¨å¼€æ”¾å¼ä»£ç ç”Ÿæˆä¸Šï¼Œå¾€å¾€å¿½è§†äº†ä»£ç ç†è§£å’Œæ¨ç†è¿™ä¸€å…³é”®æ–¹é¢ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†CodeMMLUï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„å¤šé¡¹é€‰æ‹©é¢˜åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°LLMå¯¹è½¯ä»¶å’Œä»£ç çš„æ·±å…¥ç†è§£ç¨‹åº¦ã€‚CodeMMLUåŒ…å«è¿‘2ä¸‡é“é—®é¢˜ï¼Œæ¶µç›–å¤šä¸ªé¢†åŸŸï¼ŒåŒ…æ‹¬ä»£ç åˆ†æã€ç¼ºé™·æ£€æµ‹ä»¥åŠè·¨å¤šç§ç¼–ç¨‹è¯­è¨€çš„è½¯ä»¶å·¥ç¨‹åŸåˆ™ã€‚ä¸ä¼ ç»Ÿçš„ä¾§é‡äºä»£ç ç”Ÿæˆçš„åŸºå‡†æµ‹è¯•ä¸åŒï¼ŒCodeMMLUè¯„ä¼°çš„æ˜¯æ¨¡å‹åœ¨å„ç§ä»»åŠ¡ä¸Šçš„ç¨‹åºæ¨ç†èƒ½åŠ›ï¼Œå¦‚ä»£ç ä¿®å¤ã€æ‰§è¡Œæ¨ç†å’Œå¡«ç©ºæŒ‘æˆ˜ç­‰ã€‚æˆ‘ä»¬çš„å…¨é¢è¯„ä¼°è¡¨æ˜ï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„æ¨¡å‹åœ¨CodeMMLUä¸Šä¹Ÿå­˜åœ¨å›°éš¾ï¼Œè¿™çªæ˜¾äº†åœ¨ç”Ÿæˆä¹‹å¤–çš„ç†è§£æ–¹é¢çš„é‡å¤§å·®è·ã€‚CodeMMLUå¼ºè°ƒä»£ç ç†è§£ä¸æœ‰æ•ˆçš„AIè¾…åŠ©å¼€å‘ä¹‹é—´çš„ç´§å¯†è”ç³»ï¼Œä¸ºå¼€å‘æ›´å¯é ã€æ›´å¼ºå¤§çš„ç¼–ç åŠ©æ‰‹æä¾›äº†å…³é”®èµ„æºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.01999v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>CodeLLMçš„è¿›æ­¥ä¸»è¦é›†ä¸­åœ¨å¼€æ”¾å¼ä»£ç ç”Ÿæˆä¸Šï¼Œå¿½è§†äº†ä»£ç ç†è§£å’Œæ¨ç†çš„é‡è¦æ€§ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ä¸è¶³ï¼Œå¼•å…¥äº†CodeMMLUï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„å¤šé€‰æ‹©åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°LLMåœ¨è½¯ä»¶å’Œä»£ç ç†è§£æ–¹é¢çš„æ·±åº¦ã€‚CodeMMLUåŒ…æ‹¬è·¨è¶Šä¸åŒé¢†åŸŸã€æ¶µç›–å¤šç§ç¼–ç¨‹è¯­è¨€çš„è¿‘2ä¸‡ä¸ªé—®é¢˜ã€‚ä¸ä¼ ç»Ÿå¼ºè°ƒä»£ç ç”Ÿæˆçš„åŸºå‡†æµ‹è¯•ä¸åŒï¼ŒCodeMMLUè¯„ä¼°æ¨¡å‹åœ¨ç¨‹åºä¿®å¤ã€æ‰§è¡Œæ¨ç†å’Œå¡«ç©ºæŒ‘æˆ˜ç­‰æ–¹é¢çš„èƒ½åŠ›ã€‚è¯„ä¼°æ˜¾ç¤ºï¼Œå³ä½¿æ˜¯æœ€æ–°æŠ€æœ¯æ°´å¹³çš„æ¨¡å‹åœ¨CodeMMLUä¸Šä¹Ÿé¢ä¸´å›°éš¾ï¼Œè¿™çªæ˜¾äº†ç”Ÿæˆä¹‹å¤–çš„ç†è§£é¸¿æ²Ÿã€‚CodeMMLUå¯¹äºæ¨è¿›æ›´å¯é ã€åŠŸèƒ½æ›´å¼ºå¤§çš„ç¼–ç åŠ©æ‰‹è‡³å…³é‡è¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CodeLLMè¿›æ­¥é›†ä¸­åœ¨å¼€æ”¾å¼ä»£ç ç”Ÿæˆï¼Œä½†å¿½è§†äº†ä»£ç ç†è§£å’Œæ¨ç†çš„é‡è¦æ€§ã€‚</li>
<li>CodeMMLUæ˜¯ä¸€ä¸ªå…¨é¢çš„å¤šé€‰æ‹©åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°LLMåœ¨è½¯ä»¶å’Œä»£ç ç†è§£æ–¹é¢çš„æ·±åº¦ã€‚</li>
<li>CodeMMLUåŒ…å«æ¶µç›–å¤šç§ç¼–ç¨‹è¯­è¨€å’Œé¢†åŸŸçš„è¿‘2ä¸‡ä¸ªé—®é¢˜ã€‚</li>
<li>CodeMMLUè¯„ä¼°æ¨¡å‹åœ¨ç¨‹åºä¿®å¤ã€æ‰§è¡Œæ¨ç†å’Œå¡«ç©ºæŒ‘æˆ˜ç­‰æ–¹é¢çš„èƒ½åŠ›ã€‚</li>
<li>å³ä½¿æ˜¯å…ˆè¿›çš„æ¨¡å‹åœ¨CodeMMLUä¸Šä¹Ÿé¢ä¸´å›°éš¾ï¼Œçªæ˜¾äº†ç†è§£èƒ½åŠ›çš„å·®è·ã€‚</li>
<li>CodeMMLUå¯¹äºæ¨è¿›æ›´å¯é ã€åŠŸèƒ½å¼ºå¤§çš„ç¼–ç åŠ©æ‰‹è‡³å…³é‡è¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.01999">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-776e149b9783d8813ff472364c10c715.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2de876807c66cb40b2d70a7a2af14ca0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-217a0925ac18b16666f90156305492e0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f013fcad0fb709f2c3502eaadb112134.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a4577721b0a5623ba207b570d5b97613.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-11/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-11/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-11/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-f7b868fc29d6c17d14a37f56fc038d64.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-11  A Unified Agentic Framework for Evaluating Conditional Image Generation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-11
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-11/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-1177e54fc01df5256fe6e456b3b66cf9.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-11  A Sober Look at Progress in Language Model Reasoning Pitfalls and Paths   to Reproducibility
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-11
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">31373.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
