<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-04  KaVa Latent Reasoning via Compressed KV-Cache Distillation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic-private.zhihu.com/v2-63ae7b6ee03dbfa2b2acbdd981c58429~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029169&auth_key=1760029169-0-0-3e1aeb2254bc3739a7d758567d009b49&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-04
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-23
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    84 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-10-04-æ›´æ–°"><a href="#2025-10-04-æ›´æ–°" class="headerlink" title="2025-10-04 æ›´æ–°"></a>2025-10-04 æ›´æ–°</h1><h2 id="KaVa-Latent-Reasoning-via-Compressed-KV-Cache-Distillation"><a href="#KaVa-Latent-Reasoning-via-Compressed-KV-Cache-Distillation" class="headerlink" title="KaVa: Latent Reasoning via Compressed KV-Cache Distillation"></a>KaVa: Latent Reasoning via Compressed KV-Cache Distillation</h2><p><strong>Authors:Anna Kuzina, Maciej Pioro, Paul N. Whatmough, Babak Ehteshami Bejnordi</strong></p>
<p>Large Language Models (LLMs) excel at multi-step reasoning problems with explicit chain-of-thought (CoT), but verbose traces incur significant computational costs and memory overhead, and often carry redundant, stylistic artifacts. Latent reasoning has emerged as an efficient alternative that internalizes the thought process, but it suffers from a critical lack of supervision, limiting its effectiveness on complex, natural-language reasoning traces. In this work, we propose KaVa, the first framework that bridges this gap by distilling knowledge directly from a compressed KV-cache of the teacher into a latent-reasoning student via self-distillation, leveraging the representational flexibility of continuous latent tokens to align stepwise KV trajectories. We show that the abstract, unstructured knowledge within compressed KV-cache, which lacks direct token correspondence, can serve as a rich supervisory signal for a latent reasoning student. Empirically, the approach consistently outperforms strong latent baselines, exhibits markedly smaller degradation from equation-only to natural-language traces, and scales to larger backbones while preserving efficiency. These results establish compressed KV-cache distillation as a scalable supervision signal for latent reasoning, combining the accuracy of CoT-trained teachers with the efficiency and deployability of latent inference. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ“…é•¿å…·æœ‰æ˜ç¡®æ€ç»´é“¾ï¼ˆCoTï¼‰çš„å¤šæ­¥æ¨ç†é—®é¢˜ï¼Œä½†å†—é•¿çš„è·Ÿè¸ªä¼šå¼•å‘å·¨å¤§çš„è®¡ç®—æˆæœ¬å’Œå†…å­˜å¼€é”€ï¼Œå¹¶ä¸”ç»å¸¸å¸¦æœ‰å†—ä½™ã€é£æ ¼åŒ–çš„ä¼ªè¿¹ã€‚éšå¼æ¨ç†ä½œä¸ºä¸€ç§æœ‰æ•ˆçš„æ›¿ä»£æ–¹æ³•ï¼Œèƒ½å¤Ÿå†…åŒ–æ€ç»´è¿‡ç¨‹ï¼Œä½†åœ¨å¤æ‚ã€è‡ªç„¶è¯­è¨€æ¨ç†è½¨è¿¹ä¸Šï¼Œç”±äºç¼ºä¹ç›‘ç£è€Œä¸¥é‡é™åˆ¶äº†å…¶æœ‰æ•ˆæ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†KaVaæ¡†æ¶ï¼Œå®ƒæ˜¯ç¬¬ä¸€ä¸ªé€šè¿‡è‡ªæˆ‘è’¸é¦ç›´æ¥ä»å‹ç¼©çš„KVç¼“å­˜ä¸­æå–çŸ¥è¯†å¹¶å°†å…¶çŒè¾“åˆ°éšæ€§æ¨ç†å­¦ç”Ÿä¸­çš„æ¡†æ¶ã€‚æˆ‘ä»¬åˆ©ç”¨è¿ç»­æ½œåœ¨ç¬¦å·çš„è¡¨ç¤ºçµæ´»æ€§æ¥å¯¹é½é€æ­¥KVè½¨è¿¹ã€‚æˆ‘ä»¬è¯æ˜äº†å‹ç¼©KVç¼“å­˜ä¸­çš„æŠ½è±¡ã€éç»“æ„åŒ–çŸ¥è¯†å¯ä»¥ä½œä¸ºéšæ€§æ¨ç†å­¦ç”Ÿçš„ä¸°å¯Œç›‘ç£ä¿¡å·ï¼Œå°½ç®¡å®ƒç¼ºä¹ç›´æ¥çš„ç¬¦å·å¯¹åº”å…³ç³»ã€‚ä»å®è¯ä¸Šçœ‹ï¼Œè¯¥æ–¹æ³•å§‹ç»ˆä¼˜äºå¼ºå¤§çš„éšæ€§åŸºçº¿ï¼Œä»æ–¹ç¨‹å¼åˆ°è‡ªç„¶è¯­è¨€è½¨è¿¹çš„é€€åŒ–æ˜æ˜¾è¾ƒå°ï¼Œå¹¶ä¸”åœ¨ä¿ç•™æ•ˆç‡çš„åŒæ—¶æ‰©å±•åˆ°äº†æ›´å¤§çš„ä¸»å¹²ç½‘ã€‚è¿™äº›ç»“æœå°†å‹ç¼©KVç¼“å­˜è’¸é¦ç¡®ç«‹ä¸ºä¸€ç§å¯æ‰©å±•çš„ç›‘ç£ä¿¡å·ï¼Œç”¨äºéšæ€§æ¨ç†ï¼Œç»“åˆäº†ç»è¿‡CoTè®­ç»ƒçš„æ•™å¸ˆçš„å‡†ç¡®æ€§ä»¥åŠæ½œåœ¨æ¨ç†çš„æ•ˆç‡éƒ¨ç½²èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.02312v1">PDF</a> Preprint. Under Review</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šæ­¥æ¨ç†é—®é¢˜ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†æ˜¾å¼æ€ç»´é“¾ï¼ˆCoTï¼‰è·¯å¾„ä¼šå¯¼è‡´è®¡ç®—æˆæœ¬å¢åŠ å’Œå†…å­˜æ¶ˆè€—è¿‡å¤§ï¼Œä¸”å¸¸æœ‰å†—ä½™çš„é£æ ¼åŒ–ç—•è¿¹ã€‚æ½œæ¨ç†æˆä¸ºäº†ä¸€ç§é«˜æ•ˆçš„æ›¿ä»£æ–¹æ¡ˆï¼Œä½†å…¶ç¼ºä¹å…³é”®ç›‘ç£é™åˆ¶äº†å¤æ‚è‡ªç„¶è¯­è¨€æ¨ç†çš„å‡†ç¡®æ€§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ¡†æ¶KaVaï¼Œå®ƒé€šè¿‡æ•™å¸ˆæ¨¡å‹çš„KVç¼“å­˜å‹ç¼©ä¿¡æ¯è’¸é¦æŠ€æœ¯å¯¹å­¦ç”Ÿè¿›è¡Œç›‘ç£ï¼Œåˆ©ç”¨è¿ç»­æ½œåœ¨æ ‡è®°çš„ä»£è¡¨æ€§çµæ´»æ€§å¯¹é½é€æ­¥KVè½¨è¿¹ã€‚å®éªŒè¡¨æ˜ï¼Œå‹ç¼©çš„KVç¼“å­˜ä¸­çš„æŠ½è±¡ã€éç»“æ„åŒ–çŸ¥è¯†å¯ä»¥ä½œä¸ºå­¦ç”Ÿæ¨¡å‹çš„ä¸°å¯Œç›‘ç£ä¿¡å·ã€‚æ­¤æ–¹æ³•åœ¨åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜è¶Šï¼Œè‡ªç„¶è¯­è¨€çš„æ¨ç†å‡†ç¡®æ€§æœ‰æ˜æ˜¾æå‡ï¼Œä¸”èƒ½åœ¨å¤§è§„æ¨¡éª¨å¹²ç½‘ç»œä¸Šä¿æŒé«˜æ•ˆæ€§ã€‚è¿™ä¸ºæ½œæ¨ç†çš„å‹ç¼©KVç¼“å­˜è’¸é¦æä¾›äº†ä¸€ä¸ªå¯æ‰©å±•çš„ç›‘ç£ä¿¡å·ï¼Œç»“åˆäº†æ•™å¸ˆçš„æ€ç»´é“¾è®­ç»ƒå’Œå­¦ç”Ÿçš„æ½œæ¨ç†æ•ˆç‡ä¸éƒ¨ç½²æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å¤šæ­¥æ¨ç†é—®é¢˜æ—¶è¡¨ç°å‡ºè‰²ï¼Œä½†å­˜åœ¨è®¡ç®—æˆæœ¬é«˜å’Œå†…å­˜æ¶ˆè€—å¤§çš„é—®é¢˜ã€‚</li>
<li>æ½œæ¨ç†ä½œä¸ºä¸€ç§é«˜æ•ˆçš„æ›¿ä»£æ–¹æ¡ˆï¼Œç”±äºç¼ºä¹å…³é”®ç›‘ç£è€Œé¢ä¸´å¤æ‚è‡ªç„¶è¯­è¨€æ¨ç†çš„æŒ‘æˆ˜ã€‚</li>
<li>KaVaæ¡†æ¶é€šè¿‡æ•™å¸ˆæ¨¡å‹çš„KVç¼“å­˜å‹ç¼©ä¿¡æ¯è’¸é¦æŠ€æœ¯å¯¹å­¦ç”Ÿè¿›è¡Œç›‘ç£ï¼Œå®ç°äº†æ€ç»´é“¾çš„æ½œå†…åŒ–ã€‚</li>
<li>å‹ç¼©çš„KVç¼“å­˜ä¸­çš„æŠ½è±¡ã€éç»“æ„åŒ–çŸ¥è¯†å¯ä»¥ä½œä¸ºå­¦ç”Ÿæ¨¡å‹çš„ä¸°å¯Œç›‘ç£ä¿¡å·ã€‚</li>
<li>KaVaæ¡†æ¶åœ¨åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜è¶Šï¼Œæ˜¾è‘—æé«˜äº†è‡ªç„¶è¯­è¨€çš„æ¨ç†å‡†ç¡®æ€§ã€‚</li>
<li>KaVaæ¡†æ¶èƒ½å¤Ÿåœ¨å¤§è§„æ¨¡éª¨å¹²ç½‘ç»œä¸Šä¿æŒé«˜æ•ˆæ€§ï¼Œå…·æœ‰å¯æ‰©å±•æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.02312">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-61968473dd761e8796ba6cb881020913~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029177&auth_key=1760029177-0-0-ea6198211b5c2ba594673281347104cb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ae403e039291b562ae639092033ccf99~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029185&auth_key=1760029185-0-0-8230bdceabb203fe153b2f6f17381b37&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9ed344d2b9ce756ee7a36b287d08c41c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029191&auth_key=1760029191-0-0-c3566d230be995c208b967c2449bc0d6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9134f09f57f00a7addd1b553d56ab994~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029197&auth_key=1760029197-0-0-96c4f1d679f8959b44842a2205dd5ca0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-91dab4dbb419ef140739cf5d4b2d9013~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029204&auth_key=1760029204-0-0-07c1d1429a5d96dc6547d6c9236d6e7e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Tree-based-Dialogue-Reinforced-Policy-Optimization-for-Red-Teaming-Attacks"><a href="#Tree-based-Dialogue-Reinforced-Policy-Optimization-for-Red-Teaming-Attacks" class="headerlink" title="Tree-based Dialogue Reinforced Policy Optimization for Red-Teaming   Attacks"></a>Tree-based Dialogue Reinforced Policy Optimization for Red-Teaming   Attacks</h2><p><strong>Authors:Ruohao Guo, Afshin Oroojlooy, Roshan Sridhar, Miguel Ballesteros, Alan Ritter, Dan Roth</strong></p>
<p>Despite recent rapid progress in AI safety, current large language models remain vulnerable to adversarial attacks in multi-turn interaction settings, where attackers strategically adapt their prompts across conversation turns and pose a more critical yet realistic challenge. Existing approaches that discover safety vulnerabilities either rely on manual red-teaming with human experts or employ automated methods using pre-defined templates and human-curated attack data, with most focusing on single-turn attacks. However, these methods did not explore the vast space of possible multi-turn attacks, failing to consider novel attack trajectories that emerge from complex dialogue dynamics and strategic conversation planning. This gap is particularly critical given recent findings that LLMs exhibit significantly higher vulnerability to multi-turn attacks compared to single-turn attacks. We propose DialTree-RPO, an on-policy reinforcement learning framework integrated with tree search that autonomously discovers diverse multi-turn attack strategies by treating the dialogue as a sequential decision-making problem, enabling systematic exploration without manually curated data. Through extensive experiments, our approach not only achieves more than 25.9% higher ASR across 10 target models compared to previous state-of-the-art approaches, but also effectively uncovers new attack strategies by learning optimal dialogue policies that maximize attack success across multiple turns. </p>
<blockquote>
<p>å°½ç®¡äººå·¥æ™ºèƒ½å®‰å…¨é¢†åŸŸæœ€è¿‘å–å¾—äº†å¿«é€Ÿè¿›å±•ï¼Œä½†å½“å‰çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šè½®äº¤äº’è®¾ç½®ä¸­ä»ç„¶å®¹æ˜“å—åˆ°å¯¹æŠ—æ€§æ”»å‡»çš„å½±å“ã€‚åœ¨æ­¤ç±»è®¾ç½®ä¸­ï¼Œæ”»å‡»è€…ä¼šæˆ˜ç•¥æ€§åœ°è°ƒæ•´ä»–ä»¬çš„æç¤ºå¹¶è·¨å¯¹è¯è½®æ¬¡è¿›è¡Œé€‚åº”ï¼Œä»è€Œæ„æˆæ›´åŠ ç°å®ä¸”å…³é”®çš„æŒ‘æˆ˜ã€‚ç°æœ‰çš„å‘ç°å®‰å…¨æ¼æ´çš„æ–¹æ³•è¦ä¹ˆä¾èµ–äºä¸äººç±»ä¸“å®¶çš„æ‰‹åŠ¨çº¢é˜Ÿå¯¹æŠ—ï¼Œè¦ä¹ˆä½¿ç”¨åŸºäºé¢„å®šä¹‰æ¨¡æ¿å’Œäººç±»æ•´ç†çš„æ”»å‡»æ•°æ®çš„è‡ªåŠ¨åŒ–æ–¹æ³•ï¼Œå…¶ä¸­å¤§å¤šæ•°æ–¹æ³•ä¸»è¦å…³æ³¨å•è½®æ”»å‡»ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å¹¶æœªæ¢ç´¢åºå¤§çš„å¤šè½®æ”»å‡»å¯èƒ½æ€§ç©ºé—´ï¼Œæœªèƒ½è€ƒè™‘ç”±å¤æ‚çš„å¯¹è¯åŠ¨æ€å’Œæˆ˜ç•¥å¯¹è¯è§„åˆ’æ‰€æ¶Œç°å‡ºçš„æ–°å‹æ”»å‡»è½¨è¿¹ã€‚è€ƒè™‘åˆ°æœ€è¿‘çš„ç ”ç©¶å‘ç°å¤§å‹è¯­è¨€æ¨¡å‹ç›¸è¾ƒäºå•è½®æ”»å‡»è€Œè¨€å¯¹å¤šè½®æ”»å‡»å…·æœ‰æ›´é«˜çš„è„†å¼±æ€§ï¼Œè¿™ä¸€å·®è·å°¤ä¸ºå…³é”®ã€‚æˆ‘ä»¬æå‡ºäº†DialTree-RPOæ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸æ ‘æœç´¢ç›¸ç»“åˆçš„ç­–ç•¥å‹å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œå¯è‡ªä¸»å‘ç°å¤šæ ·åŒ–çš„å¤šè½®æ”»å‡»ç­–ç•¥ï¼Œå°†å¯¹è¯è§†ä¸ºä¸€ç§é¡ºåºå†³ç­–é—®é¢˜ï¼Œå¯ä»¥åœ¨ä¸ä¾èµ–æ‰‹åŠ¨æ•´ç†æ•°æ®çš„æƒ…å†µä¸‹è¿›è¡Œç³»ç»Ÿæ¢ç´¢ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒéªŒè¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…ç›¸è¾ƒäºå…ˆå‰æœ€å…ˆè¿›çš„æ–¹æ³•åœ¨åä¸ªç›®æ ‡æ¨¡å‹ä¸Šå®ç°äº†è¶…è¿‡25.9%çš„æ”»å‡»æˆåŠŸç‡æå‡ï¼Œè€Œä¸”é€šè¿‡å­¦ä¹ æœ€å¤§åŒ–å¤šè½®æ”»å‡»æˆåŠŸçš„æœ€ä½³å¯¹è¯ç­–ç•¥æ¥æœ‰æ•ˆåœ°å‘ç°æ–°çš„æ”»å‡»ç­–ç•¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.02286v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å½“å‰çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šè½®äº’åŠ¨åœºæ™¯ä¸­ä»æ˜“å—å¯¹æŠ—æ€§æ”»å‡»çš„å½±å“ã€‚ç°æœ‰çš„å‘ç°å®‰å…¨æ¼æ´çš„æ–¹æ³•ä¾èµ–äºäººå·¥ä¸“å®¶å›¢é˜Ÿæˆ–è‡ªåŠ¨åŒ–æ–¹æ³•ï¼Œä½†æ— æ³•ç³»ç»Ÿåœ°æ¢ç´¢å¤æ‚çš„å¤šè½®æ”»å‡»ç­–ç•¥ã€‚å› æ­¤ï¼Œæå‡ºäº†DialTree-RPOæ¡†æ¶ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ å’Œæ ‘æœç´¢æŠ€æœ¯è‡ªä¸»å‘ç°å¤šæ ·åŒ–çš„å¤šè½®æ”»å‡»ç­–ç•¥ï¼Œå®ç°æ— éœ€æ‰‹åŠ¨æ•´ç†æ•°æ®å°±èƒ½ç³»ç»Ÿåœ°æ¢ç´¢å¯¹è¯ç©ºé—´ã€‚ç»è¿‡å¤§é‡å®éªŒéªŒè¯ï¼ŒDialTree-RPOä¸ä»…èƒ½å®ç°å¯¹ç›®æ ‡æ¨¡å‹çš„æ”»å‡»æˆåŠŸç‡æå‡è¶…è¿‡25.9%ï¼Œè€Œä¸”èƒ½é€šè¿‡å­¦ä¹ æœ€ä½³å¯¹è¯ç­–ç•¥æ¥æœ‰æ•ˆåœ°å‘æ˜æ–°çš„æ”»å‡»ç­–ç•¥ã€‚è¿™ä¸€çªç ´æœ‰æœ›è¿›ä¸€æ­¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å®é™…ç¯å¢ƒä¸­çš„å®‰å…¨æ€§å’Œç¨³å®šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šè½®äº’åŠ¨åœºæ™¯ä¸­ä»å­˜åœ¨å®‰å…¨æ¼æ´ï¼Œå®¹æ˜“å—åˆ°å¯¹æŠ—æ€§æ”»å‡»çš„å½±å“ã€‚</li>
<li>ç°æœ‰çš„å‘ç°å®‰å…¨æ¼æ´çš„æ–¹æ³•ä¸»è¦ä¾èµ–äºäººå·¥ä¸“å®¶å›¢é˜Ÿæˆ–è‡ªåŠ¨åŒ–æ–¹æ³•ï¼Œä½†æ— æ³•ç³»ç»Ÿåœ°æ¢ç´¢å¤æ‚çš„å¤šè½®æ”»å‡»ç­–ç•¥ã€‚</li>
<li>DialTree-RPOæ¡†æ¶é€šè¿‡å¼ºåŒ–å­¦ä¹ å’Œæ ‘æœç´¢æŠ€æœ¯ï¼Œèƒ½å¤Ÿè‡ªä¸»å‘ç°å¤šæ ·åŒ–çš„å¤šè½®æ”»å‡»ç­–ç•¥ï¼Œå®ç°ç³»ç»Ÿçš„å¯¹è¯ç©ºé—´æ¢ç´¢è€Œæ— éœ€æ‰‹åŠ¨æ•´ç†æ•°æ®ã€‚</li>
<li>DialTree-RPOåœ¨å¤§é‡å®éªŒéªŒè¯ä¸‹è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œæ”»å‡»æˆåŠŸç‡æå‡è¶…è¿‡ç°æœ‰æ–¹æ³•25.9%ã€‚</li>
<li>DialTree-RPOèƒ½æœ‰æ•ˆå‘æ˜æ–°çš„æ”»å‡»ç­–ç•¥ï¼Œé€šè¿‡ä¼˜åŒ–å¯¹è¯ç­–ç•¥æ¥æé«˜æ”»å‡»æ•ˆæœã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºè§£å†³å¤§å‹è¯­è¨€æ¨¡å‹çš„å®‰å…¨æ€§é—®é¢˜æä¾›äº†æ–°æ€è·¯å’Œæ–°æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.02286">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-7701ebd5a6bb2ccbf40d9db9b4bfa4d1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029213&auth_key=1760029213-0-0-49c5210eab68e8678501eb63f47001e2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-04e539af233bd3807629d09b37ba34c8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029221&auth_key=1760029221-0-0-c0b88d2387ae22db465d6df76aeb3eed&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3736827f0a7bff363c7a850484b633bc~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029229&auth_key=1760029229-0-0-ad0563feb3641ade144d1e0d83bbdba8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="VidGuard-R1-AI-Generated-Video-Detection-and-Explanation-via-Reasoning-MLLMs-and-RL"><a href="#VidGuard-R1-AI-Generated-Video-Detection-and-Explanation-via-Reasoning-MLLMs-and-RL" class="headerlink" title="VidGuard-R1: AI-Generated Video Detection and Explanation via Reasoning   MLLMs and RL"></a>VidGuard-R1: AI-Generated Video Detection and Explanation via Reasoning   MLLMs and RL</h2><p><strong>Authors:Kyoungjun Park, Yifan Yang, Juheon Yi, Shicheng Zheng, Yifei Shen, Dongqi Han, Caihua Shan, Muhammad Muaz, Lili Qiu</strong></p>
<p>With the rapid advancement of AI-generated videos, there is an urgent need for effective detection tools to mitigate societal risks such as misinformation and reputational harm. In addition to accurate classification, it is essential that detection models provide interpretable explanations to ensure transparency for regulators and end users. To address these challenges, we introduce VidGuard-R1, the first video authenticity detector that fine-tunes a multi-modal large language model (MLLM) using group relative policy optimization (GRPO). Our model delivers both highly accurate judgments and insightful reasoning. We curate a challenging dataset of 140k real and AI-generated videos produced by state-of-the-art generation models, carefully designing the generation process to maximize discrimination difficulty. We then fine-tune Qwen-VL using GRPO with two specialized reward models that target temporal artifacts and generation complexity. Extensive experiments demonstrate that VidGuard-R1 achieves state-of-the-art zero-shot performance on existing benchmarks, with additional training pushing accuracy above 95%. Case studies further show that VidGuard-R1 produces precise and interpretable rationales behind its predictions. The code is publicly available at <a target="_blank" rel="noopener" href="https://vidguard-r1.github.io/">https://VidGuard-R1.github.io</a>. </p>
<blockquote>
<p>éšç€äººå·¥æ™ºèƒ½ç”Ÿæˆè§†é¢‘çš„å¿«é€Ÿå‘å±•ï¼Œå¯¹äºæœ‰æ•ˆæ£€æµ‹å·¥å…·çš„éœ€æ±‚ä¹Ÿæ—¥ç›Šè¿«åˆ‡ï¼Œä»¥å‡è½»è¯¸å¦‚è™šå‡ä¿¡æ¯å’Œå£°èª‰æŸå®³ç­‰ç¤¾ä¼šé£é™©ã€‚é™¤äº†å‡†ç¡®çš„åˆ†ç±»ä¹‹å¤–ï¼Œæ£€æµ‹æ¨¡å‹è¿˜å¿…é¡»æä¾›å¯è§£é‡Šçš„è§£é‡Šï¼Œä»¥ç¡®ä¿ç›‘ç®¡æœºæ„å’Œæœ€ç»ˆç”¨æˆ·ä¹‹é—´çš„é€æ˜åº¦ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†VidGuard-R1ï¼Œå®ƒæ˜¯é¦–æ¬¾ä½¿ç”¨åŸºäºç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰è¿›è¡Œå¾®è°ƒçš„è§†é¢‘çœŸå®æ€§æ£€æµ‹å™¨ã€‚æˆ‘ä»¬çš„æ¨¡å‹æ—¢æä¾›äº†é«˜åº¦å‡†ç¡®çš„åˆ¤æ–­ï¼Œåˆæä¾›äº†æ·±åˆ»çš„æ¨ç†ã€‚æˆ‘ä»¬ç²¾å¿ƒåˆ›å»ºäº†ä¸€ä¸ªç”±æœ€æ–°ç”Ÿæˆæ¨¡å‹äº§ç”Ÿçš„åŒ…å«14ä¸‡æ¡çœŸå®å’Œäººå·¥æ™ºèƒ½ç”Ÿæˆçš„è§†é¢‘æ•°æ®é›†ï¼Œè°¨æ…è®¾è®¡ç”Ÿæˆè¿‡ç¨‹ä»¥æœ€å¤§åŒ–åŒºåˆ†éš¾åº¦ã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨GRPOå¯¹Qwen-VLè¿›è¡Œå¾®è°ƒï¼Œé‡‡ç”¨ä¸¤ä¸ªä¸“é—¨ç”¨äºå¥–åŠ±æ¨¡å‹çš„ä¸´æ—¶ä¼ªè¿¹å’Œç”Ÿæˆå¤æ‚æ€§ç›®æ ‡ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒVidGuard-R1åœ¨ç°æœ‰åŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„é›¶æ ·æœ¬æ€§èƒ½ï¼Œé€šè¿‡é¢å¤–è®­ç»ƒå¯å°†å‡†ç¡®ç‡æé«˜åˆ°95%ä»¥ä¸Šã€‚æ¡ˆä¾‹ç ”ç©¶è¿›ä¸€æ­¥è¡¨æ˜ï¼ŒVidGuard-R1èƒ½å¤Ÿäº§ç”Ÿç²¾ç¡®ä¸”å¯è§£é‡Šçš„åˆ¤æ–­ä¾æ®ã€‚ä»£ç å…¬å¼€äº<a target="_blank" rel="noopener" href="https://vidguard-r1.github.io/">https://VidGuard-R1.github.io</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.02282v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>éšç€AIç”Ÿæˆè§†é¢‘çš„å¿«é€Ÿå‘å±•ï¼Œç¤¾ä¼šå¯¹é˜²æ­¢è¯¯ä¿¡å’Œå£°èª‰æŸå®³ç­‰é£é™©çš„æ£€æµ‹å·¥å…·çš„éœ€æ±‚æ—¥ç›Šè¿«åˆ‡ã€‚ä¸ºåº”å¯¹æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†VidGuard-R1ï¼Œé¦–æ¬¾é‡‡ç”¨ç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰è°ƒæ•´å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„è§†é¢‘çœŸå®æ€§æ£€æµ‹å™¨ã€‚è¯¥æ¨¡å‹æ—¢èƒ½æä¾›é«˜åº¦å‡†ç¡®çš„åˆ¤æ–­ï¼Œåˆèƒ½ç»™å‡ºæ·±å…¥çš„ç†ç”±ã€‚æˆ‘ä»¬ä½¿ç”¨14ä¸‡ä¸ªçœŸå®å’ŒAIç”Ÿæˆçš„è§†é¢‘æ•°æ®é›†è¿›è¡Œå¾®è°ƒï¼Œå¹¶ä½¿ç”¨ä¸“æ³¨äºæ—¶é—´ç—•è¿¹å’Œç”Ÿæˆå¤æ‚åº¦çš„ä¸¤ä¸ªå¥–åŠ±æ¨¡å‹ï¼Œè¿›ä¸€æ­¥å¼ºåŒ–æ¨¡å‹çš„æ€§èƒ½ã€‚å®éªŒè¯æ˜ï¼ŒVidGuard-R1åœ¨ç°æœ‰åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†é›¶æ ·æœ¬çš„æœ€ä½³æ€§èƒ½ï¼Œç»è¿‡é¢å¤–è®­ç»ƒï¼Œå‡†ç¡®ç‡è¶…è¿‡95%ã€‚å…¶é¢„æµ‹èƒŒåçš„ç†ç”±ç²¾ç¡®ä¸”å¯è§£é‡Šã€‚ç›¸å…³ä»£ç å·²å…¬å¼€åœ¨ï¼š<a target="_blank" rel="noopener" href="https://vidguard-r1.github.io./">https://VidGuard-R1.github.ioã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VidGuard-R1æ˜¯é¦–æ¬¾è§†é¢‘çœŸå®æ€§æ£€æµ‹å™¨ï¼Œé‡‡ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹å’Œç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ã€‚</li>
<li>è¯¥æ¨¡å‹èƒ½åŒæ—¶æä¾›å‡†ç¡®åˆ¤æ–­å’Œæ·±å…¥çš„è§£é‡Šï¼Œç¡®ä¿é€æ˜æ€§ã€‚</li>
<li>ä½¿ç”¨åŒ…å«çœŸå®å’ŒAIç”Ÿæˆè§†é¢‘çš„å¤æ‚æ•°æ®é›†è¿›è¡Œè®­ç»ƒå’Œå¾®è°ƒã€‚</li>
<li>VidGuard-R1ä½¿ç”¨ä¸“é—¨çš„å¥–åŠ±æ¨¡å‹é’ˆå¯¹æ—¶é—´ç—•è¿¹å’Œç”Ÿæˆå¤æ‚åº¦è¿›è¡Œä¼˜åŒ–ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒVidGuard-R1åœ¨é›¶æ ·æœ¬æƒ…å†µä¸‹è¾¾åˆ°æœ€ä½³æ€§èƒ½ï¼Œä¸”ç»è¿‡é¢å¤–è®­ç»ƒåå‡†ç¡®ç‡è¶…è¿‡95%ã€‚</li>
<li>æ¡ˆä¾‹ç ”ç©¶æ˜¾ç¤ºï¼ŒVidGuard-R1æä¾›çš„é¢„æµ‹ç†ç”±æ—¢ç²¾ç¡®åˆå…·å¤‡å¯è§£é‡Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.02282">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-4a1a60196af223f34b56fb1fecf25574~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029238&auth_key=1760029238-0-0-a81555cd2772bfe1cfdf2a1935851506&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f15b28aee3d004aa3a054acc788a6ef3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029247&auth_key=1760029247-0-0-13f27b9d2ecaa1139f9b1e8c86a7418b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="RewardMap-Tackling-Sparse-Rewards-in-Fine-grained-Visual-Reasoning-via-Multi-Stage-Reinforcement-Learning"><a href="#RewardMap-Tackling-Sparse-Rewards-in-Fine-grained-Visual-Reasoning-via-Multi-Stage-Reinforcement-Learning" class="headerlink" title="RewardMap: Tackling Sparse Rewards in Fine-grained Visual Reasoning via   Multi-Stage Reinforcement Learning"></a>RewardMap: Tackling Sparse Rewards in Fine-grained Visual Reasoning via   Multi-Stage Reinforcement Learning</h2><p><strong>Authors:Sicheng Feng, Kaiwen Tuo, Song Wang, Lingdong Kong, Jianke Zhu, Huan Wang</strong></p>
<p>Fine-grained visual reasoning remains a core challenge for multimodal large language models (MLLMs). The recently introduced ReasonMap highlights this gap by showing that even advanced MLLMs struggle with spatial reasoning in structured and information-rich settings such as transit maps, a task of clear practical and scientific importance. However, standard reinforcement learning (RL) on such tasks is impeded by sparse rewards and unstable optimization. To address this, we first construct ReasonMap-Plus, an extended dataset that introduces dense reward signals through Visual Question Answering (VQA) tasks, enabling effective cold-start training of fine-grained visual understanding skills. Next, we propose RewardMap, a multi-stage RL framework designed to improve both visual understanding and reasoning capabilities of MLLMs. RewardMap incorporates two key designs. First, we introduce a difficulty-aware reward design that incorporates detail rewards, directly tackling the sparse rewards while providing richer supervision. Second, we propose a multi-stage RL scheme that bootstraps training from simple perception to complex reasoning tasks, offering a more effective cold-start strategy than conventional Supervised Fine-Tuning (SFT). Experiments on ReasonMap and ReasonMap-Plus demonstrate that each component of RewardMap contributes to consistent performance gains, while their combination yields the best results. Moreover, models trained with RewardMap achieve an average improvement of 3.47% across 6 benchmarks spanning spatial reasoning, fine-grained visual reasoning, and general tasks beyond transit maps, underscoring enhanced visual understanding and reasoning capabilities. </p>
<blockquote>
<p>ç²¾ç»†è§†è§‰æ¨ç†å¯¹äºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ¥è¯´ä»ç„¶æ˜¯ä¸€ä¸ªæ ¸å¿ƒæŒ‘æˆ˜ã€‚æœ€è¿‘æ¨å‡ºçš„ReasonMapé€šè¿‡æ˜¾ç¤ºå³ä½¿åœ¨ç»“æ„åŒ–ä¸”ä¿¡æ¯ä¸°å¯Œçš„ç¯å¢ƒä¸­ï¼Œå…ˆè¿›çš„MLLMsåœ¨ç©ºé—´æ¨ç†æ–¹é¢ä¹Ÿé¢ä¸´å›°éš¾ï¼Œå¦‚äº¤é€šåœ°å›¾ç­‰ä»»åŠ¡æ˜ç¡®å…·æœ‰é‡è¦çš„å®é™…å’Œç§‘å­¦æ„ä¹‰ï¼Œä»è€Œçªå‡ºäº†è¿™ä¸€å·®è·ã€‚ç„¶è€Œï¼Œåœ¨è¿™ç§ä»»åŠ¡ä¸Šè¿›è¡Œæ ‡å‡†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç”±äºå¥–åŠ±ç¨€ç–å’Œä¼˜åŒ–ä¸ç¨³å®šè€Œå—åˆ°é˜»ç¢ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬é¦–å…ˆæ„å»ºäº†ReasonMap-Plusè¿™ä¸€æ‰©å±•æ•°æ®é›†ï¼Œé€šè¿‡è§†è§‰é—®ç­”ï¼ˆVQAï¼‰ä»»åŠ¡å¼•å…¥å¯†é›†çš„å¥–åŠ±ä¿¡å·ï¼Œä»è€Œå®ç°å¯¹ç²¾ç»†è§†è§‰ç†è§£æŠ€èƒ½çš„æœ‰æ•ˆå†·å¯åŠ¨è®­ç»ƒã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æå‡ºäº†RewardMapè¿™ä¸€å¤šé˜¶æ®µå¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜MLLMsçš„è§†è§‰ç†è§£å’Œæ¨ç†èƒ½åŠ›ã€‚RewardMapæœ‰ä¸¤ä¸ªå…³é”®è®¾è®¡ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼•å…¥äº†éš¾åº¦æ„ŸçŸ¥å¥–åŠ±è®¾è®¡ï¼Œè¯¥è®¾è®¡ç»“åˆäº†ç»†èŠ‚å¥–åŠ±ï¼Œç›´æ¥è§£å†³äº†å¥–åŠ±ç¨€ç–çš„é—®é¢˜ï¼ŒåŒæ—¶æä¾›äº†æ›´ä¸°å¯Œçš„ç›‘ç£ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¤šé˜¶æ®µå¼ºåŒ–å­¦ä¹ æ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆä»ç®€å•çš„æ„ŸçŸ¥ä»»åŠ¡å¼•å¯¼åˆ°å¤æ‚çš„æ¨ç†ä»»åŠ¡è¿›è¡Œè®­ç»ƒï¼Œç›¸æ¯”äºä¼ ç»Ÿçš„æœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æä¾›äº†ä¸€ç§æ›´æœ‰æ•ˆçš„å†·å¯åŠ¨ç­–ç•¥ã€‚åœ¨ReasonMapå’ŒReasonMap-Plusä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒRewardMapçš„æ¯ä¸ªç»„ä»¶éƒ½å®ç°äº†æŒç»­çš„æ€§èƒ½æå‡ï¼Œå®ƒä»¬çš„ç»„åˆåˆ™å–å¾—äº†æœ€ä½³æ•ˆæœã€‚æ­¤å¤–ï¼Œä½¿ç”¨RewardMapè®­ç»ƒçš„æ¨¡å‹åœ¨è·¨è¶Šç©ºé—´æ¨ç†ã€ç²¾ç»†è§†è§‰æ¨ç†å’Œä¸€èˆ¬ä»»åŠ¡çš„å…­ä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†å¹³å‡æé«˜3.47%çš„æ•ˆæœï¼Œçªæ˜¾äº†å¢å¼ºçš„è§†è§‰ç†è§£å’Œæ¨ç†èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.02240v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä¸»è¦è®¨è®ºäº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤„ç†ç²¾ç»†è§†è§‰æ¨ç†æ–¹é¢çš„æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³ç°æœ‰æ–¹æ³•åœ¨è¯¥ä»»åŠ¡ä¸Šçš„ä¸è¶³ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ•°æ®é›†RewardMap-Pluså’Œä¸€ä¸ªå¤šé˜¶æ®µå¼ºåŒ–å­¦ä¹ æ¡†æ¶RewardMapã€‚RewardMapé€šè¿‡å¼•å…¥éš¾åº¦æ„ŸçŸ¥å¥–åŠ±è®¾è®¡å’Œå¤šé˜¶æ®µRLæ–¹æ¡ˆï¼Œæé«˜äº†MLLMsçš„è§†è§‰ç†è§£å’Œæ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRewardMapçš„å„ä¸ªç»„æˆéƒ¨åˆ†éƒ½èƒ½å¸¦æ¥æ€§èƒ½çš„æå‡ï¼Œä¸”åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„æ¨¡å‹å¹³å‡æ”¹è¿›äº†3.47%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤„ç†ç²¾ç»†è§†è§‰æ¨ç†æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>ReasonMapæ•°æ®é›†å±•ç¤ºäº†MLLMsåœ¨ç»“æ„åŒ–ã€ä¿¡æ¯ä¸°å¯Œçš„åœºæ™¯ï¼ˆå¦‚äº¤é€šåœ°å›¾ï¼‰ä¸­çš„ç©ºé—´æ¨ç†å›°éš¾ã€‚</li>
<li>æ ‡å‡†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨æ­¤ç±»ä»»åŠ¡ä¸Šå—åˆ°ç¨€ç–å¥–åŠ±å’Œä¸ç¨³å®šä¼˜åŒ–çš„é™åˆ¶ã€‚</li>
<li>å¼•å…¥ReasonMap-Plusæ•°æ®é›†ï¼Œé€šè¿‡è§†è§‰é—®ç­”ï¼ˆVQAï¼‰ä»»åŠ¡æä¾›å¯†é›†å¥–åŠ±ä¿¡å·ï¼Œå®ç°ç²¾ç»†è§†è§‰ç†è§£æŠ€èƒ½çš„æœ‰æ•ˆå†·å¯åŠ¨è®­ç»ƒã€‚</li>
<li>æå‡ºRewardMapå¤šé˜¶æ®µRLæ¡†æ¶ï¼Œæé«˜MLLMsçš„è§†è§‰ç†è§£å’Œæ¨ç†èƒ½åŠ›ã€‚</li>
<li>RewardMapåŒ…æ‹¬éš¾åº¦æ„ŸçŸ¥å¥–åŠ±è®¾è®¡å’Œå¤šé˜¶æ®µRLæ–¹æ¡ˆï¼Œè§£å†³ç¨€ç–å¥–åŠ±é—®é¢˜å¹¶æä¾›æ›´ä¸°å¯Œç›‘ç£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.02240">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-ed6287e708318fd64bb2acc0cb540967~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029256&auth_key=1760029256-0-0-c3bc42b6407c2bdf381c4d5555a389a3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2abaac35664999357803835d0a21a6e6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029264&auth_key=1760029264-0-0-6e762ecbe3231b458f8bca9eb683b7ec&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-70b0efb12c99fc462ff60e5068ba49e6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029270&auth_key=1760029270-0-0-c7d973f087b097a5da76b154ad7ed660&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="The-Reasoning-Boundary-Paradox-How-Reinforcement-Learning-Constrains-Language-Models"><a href="#The-Reasoning-Boundary-Paradox-How-Reinforcement-Learning-Constrains-Language-Models" class="headerlink" title="The Reasoning Boundary Paradox: How Reinforcement Learning Constrains   Language Models"></a>The Reasoning Boundary Paradox: How Reinforcement Learning Constrains   Language Models</h2><p><strong>Authors:Phuc Minh Nguyen, Chinh D. La, Duy M. H. Nguyen, Nitesh V. Chawla, Binh T. Nguyen, Khoa D. Doan</strong></p>
<p>Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a key method for improving Large Language Modelsâ€™ reasoning capabilities, yet recent evidence suggests it may paradoxically shrink the reasoning boundary rather than expand it. This paper investigates the shrinkage issue of RLVR by analyzing its learning dynamics and reveals two critical phenomena that explain this failure. First, we expose negative interference in RLVR, where learning to solve certain training problems actively reduces the likelihood of correct solutions for others, leading to the decline of Pass@$k$ performance, or the probability of generating a correct solution within $k$ attempts. Second, we uncover the winner-take-all phenomenon: RLVR disproportionately reinforces problems with high likelihood, correct solutions, under the base model, while suppressing other initially low-likelihood ones. Through extensive theoretical and empirical analysis on multiple mathematical reasoning benchmarks, we show that this effect arises from the inherent on-policy sampling in standard RL objectives, causing the model to converge toward narrow solution strategies. Based on these insights, we propose a simple yet effective data curation algorithm that focuses RLVR learning on low-likelihood problems, achieving notable improvement in Pass@$k$ performance. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/mail-research/SELF-llm-interference">https://github.com/mail-research/SELF-llm-interference</a>. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰å·²æˆä¸ºæé«˜å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„ä¸€ç§å…³é”®æ–¹æ³•ï¼Œä½†æœ€è¿‘çš„è¯æ®è¡¨æ˜ï¼Œå®ƒå¯èƒ½ä¼šäº§ç”Ÿæ‚–è®ºï¼Œç¼©å°æ¨ç†è¾¹ç•Œè€Œä¸æ˜¯æ‰©å¤§å®ƒã€‚æœ¬æ–‡ç ”ç©¶äº†RLVRçš„æ”¶ç¼©é—®é¢˜ï¼Œé€šè¿‡åˆ†æå…¶å­¦ä¹ åŠ¨æ€æ­ç¤ºäº†ä¸¤ç§å…³é”®ç°è±¡æ¥è§£é‡Šè¿™ç§å¤±è´¥ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æ­ç¤ºäº†RLVRä¸­çš„è´Ÿå¹²æ‰°ç°è±¡ï¼Œå³å­¦ä¹ è§£å†³æŸäº›è®­ç»ƒé—®é¢˜å®é™…ä¸Šå‡å°‘äº†è§£å†³å…¶ä»–é—®é¢˜çš„æ­£ç¡®è§£å†³æ–¹æ¡ˆçš„å¯èƒ½æ€§ï¼Œå¯¼è‡´Pass@kæ€§èƒ½ä¸‹é™ï¼Œå³åœ¨kæ¬¡å°è¯•ä¸­äº§ç”Ÿæ­£ç¡®è§£å†³æ–¹æ¡ˆçš„æ¦‚ç‡é™ä½ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å‘ç°äº†èµ¢å®¶é€šåƒç°è±¡ï¼šRLVRä¼šä¸æˆæ¯”ä¾‹åœ°å¼ºåŒ–åŸºç¡€æ¨¡å‹ä¸‹å…·æœ‰é«˜å¯èƒ½æ€§æ­£ç¡®è§£å†³æ–¹æ¡ˆçš„é—®é¢˜ï¼ŒåŒæ—¶å‹åˆ¶å…¶ä»–æœ€åˆä½å¯èƒ½æ€§çš„é—®é¢˜ã€‚é€šè¿‡å¯¹å¤šä¸ªæ•°å­¦æ¨ç†åŸºå‡†çš„å¹¿æ³›ç†è®ºå’Œå®è¯åˆ†æï¼Œæˆ‘ä»¬è¡¨æ˜è¿™ç§æ•ˆæœæ¥è‡ªäºæ ‡å‡†å¼ºåŒ–å­¦ä¹ ç›®æ ‡ä¸­çš„å›ºæœ‰ç­–ç•¥é‡‡æ ·ï¼Œå¯¼è‡´æ¨¡å‹æ”¶æ•›äºç‹­çª„çš„è§£å†³æ–¹æ¡ˆç­–ç•¥ã€‚åŸºäºè¿™äº›è§è§£ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„æ•°æ®æ•´ç†ç®—æ³•ï¼Œè¯¥ç®—æ³•å°†RLVRçš„å­¦ä¹ é‡ç‚¹æ”¾åœ¨äº†ä½æ¦‚ç‡é—®é¢˜ä¸Šï¼Œåœ¨Pass@kæ€§èƒ½ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚æˆ‘ä»¬çš„ä»£ç ä½äº[<a target="_blank" rel="noopener" href="https://github.com/mail-research/SELF-llm-interference%E3%80%82]">https://github.com/mail-research/SELF-llm-interferenceã€‚]</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.02230v1">PDF</a> 23 pages, 15 figures</p>
<p><strong>Summary</strong><br>å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰æ˜¯æå‡å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„é‡è¦æ–¹æ³•ï¼Œä½†æœ€æ–°è¯æ®è¡¨æ˜å®ƒå¯èƒ½æ”¶ç¼©æ¨ç†è¾¹ç•Œè€Œéæ‰©å±•ã€‚æœ¬æ–‡è°ƒæŸ¥äº†RLVRçš„æ”¶ç¼©é—®é¢˜ï¼Œé€šè¿‡åˆ†æå…¶å­¦ä¹ åŠ¨æ€æ­ç¤ºäº†ä¸¤ä¸ªå…³é”®ç°è±¡ã€‚ä¸€æ˜¯è´Ÿå¹²æ‰°ç°è±¡ï¼Œè§£å†³æŸäº›è®­ç»ƒé—®é¢˜ä¼šå‡å°‘å¯¹å…¶å®ƒé—®é¢˜çš„æ­£ç¡®è§£ç­”å¯èƒ½æ€§ï¼Œå¯¼è‡´Pass@kæ€§èƒ½ä¸‹é™ã€‚äºŒæ˜¯èµ¢å®¶é€šåƒç°è±¡ï¼ŒRLVRä¼šè¿‡åº¦å¼ºåŒ–é«˜æ¦‚ç‡æ­£ç¡®è§£ç­”çš„é—®é¢˜ï¼ŒåŒæ—¶å‹åˆ¶å…¶å®ƒåˆå§‹ä½æ¦‚ç‡çš„é—®é¢˜ã€‚åŸºäºè¿™äº›è§è§£ï¼Œæˆ‘ä»¬æå‡ºä¸€ä¸ªç®€å•çš„æ•°æ®æ•´ç†ç®—æ³•ï¼Œä¸“æ³¨äºRLVRå­¦ä¹ ä½æ¦‚ç‡é—®é¢˜ï¼Œåœ¨Pass@kæ€§èƒ½ä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰åœ¨æå‡è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›ä¸Šæ‰®æ¼”é‡è¦è§’è‰²ï¼Œä½†å­˜åœ¨æ”¶ç¼©æ¨ç†è¾¹ç•Œçš„é—®é¢˜ã€‚</li>
<li>è´Ÿå¹²æ‰°ç°è±¡ï¼šè§£å†³æŸäº›è®­ç»ƒé—®é¢˜ä¼šå‡å°‘å¯¹å…¶å®ƒé—®é¢˜çš„æ­£ç¡®è§£ç­”å¯èƒ½æ€§ã€‚</li>
<li>èµ¢å®¶é€šåƒç°è±¡ï¼šRLVRä¼šè¿‡åº¦å¼ºåŒ–é«˜æ¦‚ç‡æ­£ç¡®è§£ç­”çš„é—®é¢˜ï¼Œå‹åˆ¶ä½æ¦‚ç‡é—®é¢˜ã€‚</li>
<li>é—®é¢˜æ¥æºäºæ ‡å‡†å¼ºåŒ–å­¦ä¹ ç›®æ ‡ä¸­çš„å†…åœ¨ç­–ç•¥é‡‡æ ·ï¼Œå¯¼è‡´æ¨¡å‹æ”¶æ•›äºç‹­çª„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>æå‡ºçš„æ•°æ®æ•´ç†ç®—æ³•ä¸“æ³¨äºRLVRå­¦ä¹ ä½æ¦‚ç‡é—®é¢˜ï¼Œæœ‰æ•ˆæå‡Pass@kæ€§èƒ½ã€‚</li>
<li>è®ºæ–‡é€šè¿‡å¹¿æ³›çš„ç†è®ºå’Œå®è¯ç ”ç©¶è¯æ˜äº†è¿™äº›è§‚ç‚¹çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.02230">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-09181bf624db85794f2aaf6d807a7161~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029278&auth_key=1760029278-0-0-89ff9f4c0a4c0afa72868b2ffc2427ce&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-56868382f5355584231ee370870e9bee~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029286&auth_key=1760029286-0-0-507e62fa869e1c0ca96757a1486e4174&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d52ebda66fdb5f5077a7c27103ab5206~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029293&auth_key=1760029293-0-0-de1309f3c8320a1060a478dc01fd3b68&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6d9ab69fe3a12a9b062a37daaf74c221~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029300&auth_key=1760029300-0-0-5e6438ecd9e3941a0d19c07c65f45102&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="More-Than-One-Teacher-Adaptive-Multi-Guidance-Policy-Optimization-for-Diverse-Exploration"><a href="#More-Than-One-Teacher-Adaptive-Multi-Guidance-Policy-Optimization-for-Diverse-Exploration" class="headerlink" title="More Than One Teacher: Adaptive Multi-Guidance Policy Optimization for   Diverse Exploration"></a>More Than One Teacher: Adaptive Multi-Guidance Policy Optimization for   Diverse Exploration</h2><p><strong>Authors:Xiaoyang Yuan, Yujuan Ding, Yi Bin, Wenqi Shao, Jinyu Cai, Jingkuan Song, Yang Yang, Hengtao Shen</strong></p>
<p>Reinforcement Learning with Verifiable Rewards (RLVR) is a promising paradigm for enhancing the reasoning ability in Large Language Models (LLMs). However, prevailing methods primarily rely on self-exploration or a single off-policy teacher to elicit long chain-of-thought (LongCoT) reasoning, which may introduce intrinsic model biases and restrict exploration, ultimately limiting reasoning diversity and performance. Drawing inspiration from multi-teacher strategies in knowledge distillation, we introduce Adaptive Multi-Guidance Policy Optimization (AMPO), a novel framework that adaptively leverages guidance from multiple proficient teacher models, but only when the on-policy model fails to generate correct solutions. This â€œguidance-on-demandâ€ approach expands exploration while preserving the value of self-discovery. Moreover, AMPO incorporates a comprehension-based selection mechanism, prompting the student to learn from the reasoning paths that it is most likely to comprehend, thus balancing broad exploration with effective exploitation. Extensive experiments show AMPO substantially outperforms a strong baseline (GRPO), with a 4.3% improvement on mathematical reasoning tasks and 12.2% on out-of-distribution tasks, while significantly boosting Pass@k performance and enabling more diverse exploration. Notably, using four peer-sized teachers, our method achieves comparable results to approaches that leverage a single, more powerful teacher (e.g., DeepSeek-R1) with more data. These results demonstrate a more efficient and scalable path to superior reasoning and generalizability. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/SII-Enigma/AMPO">https://github.com/SII-Enigma/AMPO</a>. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰æ˜¯ä¸€ä¸ªå¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†èƒ½åŠ›çš„æœ‰å‰é€”çš„èŒƒå¼ã€‚ç„¶è€Œï¼Œç›®å‰çš„æ–¹æ³•ä¸»è¦ä¾èµ–äºè‡ªæˆ‘æ¢ç´¢æˆ–å•ä¸€çš„éç­–ç•¥æ•™å¸ˆæ¥æ¿€å‘é•¿é“¾æ€ç»´ï¼ˆLongCoTï¼‰æ¨ç†ï¼Œè¿™å¯èƒ½ä¼šå¼•å…¥å†…åœ¨æ¨¡å‹åè§å¹¶é™åˆ¶æ¢ç´¢ï¼Œæœ€ç»ˆé™åˆ¶æ¨ç†çš„å¤šæ ·æ€§å’Œæ€§èƒ½ã€‚æˆ‘ä»¬ä»çŸ¥è¯†è’¸é¦ä¸­çš„å¤šæ•™å¸ˆç­–ç•¥ä¸­æ±²å–çµæ„Ÿï¼Œå¼•å…¥äº†è‡ªé€‚åº”å¤šæŒ‡å¯¼ç­–ç•¥ä¼˜åŒ–ï¼ˆAMPOï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„æ¡†æ¶ï¼Œè‡ªé€‚åº”åœ°åˆ©ç”¨å¤šä¸ªç†Ÿç»ƒæ•™å¸ˆæ¨¡å‹çš„æŒ‡å¯¼ï¼Œä½†ä»…åœ¨ç­–ç•¥å†…æ¨¡å‹æ— æ³•ç”Ÿæˆæ­£ç¡®è§£å†³æ–¹æ¡ˆæ—¶æ‰è¿™æ ·åšã€‚è¿™ç§â€œæŒ‰éœ€æŒ‡å¯¼â€çš„æ–¹æ³•æ‰©å¤§äº†æ¢ç´¢èŒƒå›´ï¼ŒåŒæ—¶ä¿ç•™äº†è‡ªæˆ‘å‘ç°çš„ä»·å€¼ã€‚æ­¤å¤–ï¼ŒAMPOè¿˜é‡‡ç”¨äº†ä¸€ç§åŸºäºç†è§£çš„é€‰æ‹©æœºåˆ¶ï¼Œä¿ƒä½¿å­¦ç”Ÿä»å®ƒæœ€å¯èƒ½ç†è§£çš„æ¨ç†è·¯å¾„ä¸­å­¦ä¹ ï¼Œä»è€Œåœ¨å¹¿æ³›çš„æ¢ç´¢ä¸æœ‰æ•ˆçš„åˆ©ç”¨ä¹‹é—´å–å¾—å¹³è¡¡ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒAMPOæ˜¾è‘—ä¼˜äºå¼ºå¤§çš„åŸºçº¿ï¼ˆGRPOï¼‰ï¼Œåœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šæé«˜äº†4.3%ï¼Œåœ¨åˆ†å¸ƒå¼å¤–ä»»åŠ¡ä¸Šæé«˜äº†12.2%ï¼ŒåŒæ—¶æ˜¾è‘—æé«˜äº†Pass@kæ€§èƒ½å¹¶å®ç°äº†æ›´å¤šæ ·åŒ–çš„æ¢ç´¢ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä½¿ç”¨å››ä½åŒçº§æ•™å¸ˆï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†ä¸åˆ©ç”¨å•ä¸€æ›´å¼ºå¤§æ•™å¸ˆçš„æ–¹æ³•ï¼ˆä¾‹å¦‚ï¼ŒDeepSeek-R1ï¼‰ç›¸å½“çš„ç»“æœï¼Œä¸”æˆ‘ä»¬çš„æ–¹æ³•ä½¿ç”¨äº†æ›´å¤šçš„æ•°æ®ã€‚è¿™äº›ç»“æœè¯æ˜äº†ä¸€æ¡æ›´é«˜æ•ˆã€æ›´å¯æ‰©å±•çš„é€šå¾€å“è¶Šæ¨ç†å’Œæ³›åŒ–èƒ½åŠ›çš„é“è·¯ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/SII-Enigma/AMPO%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/SII-Enigma/AMPOæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.02227v1">PDF</a> 20 pages, 5 figures</p>
<p><strong>Summary</strong><br>å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰èŒƒå¼ä¸ºæå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›å¸¦æ¥äº†å¸Œæœ›ã€‚ç„¶è€Œï¼Œå½“å‰æ–¹æ³•ä¸»è¦ä¾èµ–è‡ªæˆ‘æ¢ç´¢æˆ–å•ä¸€çš„éç­–ç•¥æ•™å¸ˆæ¥æ¿€å‘é•¿é“¾æ€ç»´æ¨ç†ï¼Œè¿™å¯èƒ½å¼•å…¥å†…åœ¨æ¨¡å‹åè§å¹¶é™åˆ¶æ¢ç´¢ï¼Œæœ€ç»ˆé™åˆ¶æ¨ç†çš„å¤šæ ·æ€§å’Œæ€§èƒ½ã€‚å—çŸ¥è¯†è’¸é¦ä¸­å¤šæ•™å¸ˆç­–ç•¥çš„å¯å‘ï¼Œæˆ‘ä»¬å¼•å…¥äº†è‡ªé€‚åº”å¤šæŒ‡å¯¼ç­–ç•¥ä¼˜åŒ–ï¼ˆAMPOï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æ¡†æ¶ï¼Œèƒ½å¤Ÿè‡ªé€‚åº”åœ°åˆ©ç”¨å¤šä¸ªç†Ÿç»ƒæ•™å¸ˆæ¨¡å‹çš„æŒ‡å¯¼ï¼Œä½†ä»…åœ¨ç­–ç•¥å†…æ¨¡å‹æ— æ³•ç”Ÿæˆæ­£ç¡®è§£å†³æ–¹æ¡ˆæ—¶ã€‚è¿™ç§â€œæŒ‰éœ€æŒ‡å¯¼â€çš„æ–¹æ³•åœ¨æ‰©å¤§æ¢ç´¢çš„åŒæ—¶ä¿ç•™äº†è‡ªæˆ‘å‘ç°çš„ä»·å€¼ã€‚AMPOè¿˜ç»“åˆäº†åŸºäºç†è§£çš„é€‰æ‹©æœºåˆ¶ï¼Œä¿ƒä½¿å­¦ç”Ÿä»å®ƒæœ€å¯èƒ½ç†è§£çš„æ¨ç†è·¯å¾„ä¸­å­¦ä¹ ï¼Œä»è€Œåœ¨å¹¿æ³›çš„æ¢ç´¢ä¸æœ‰æ•ˆçš„åˆ©ç”¨ä¹‹é—´å–å¾—å¹³è¡¡ã€‚å®éªŒè¡¨æ˜ï¼ŒAMPOåœ¨å¼ºå¤§çš„åŸºçº¿ï¼ˆGRPOï¼‰ä¸Šå–å¾—äº†å®è´¨æ€§çªç ´ï¼Œåœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šæé«˜äº†4.3%ï¼Œåœ¨åˆ†å¸ƒå¼å¤–ä»»åŠ¡ä¸Šæé«˜äº†12.2%ï¼ŒåŒæ—¶åœ¨æé«˜Pass@kæ€§èƒ½çš„åŒæ—¶å®ç°äº†æ›´å¤šæ ·åŒ–çš„æ¢ç´¢ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä½¿ç”¨å››ä½åŒè¡Œå¤§å°çš„æ•™å¸ˆï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†ä¸åˆ©ç”¨å•ä¸€æ›´å¼ºå¤§æ•™å¸ˆçš„æ–¹æ³•ç›¸å½“çš„ç»“æœï¼ˆä¾‹å¦‚ä½¿ç”¨æ›´å¤šæ•°æ®çš„DeepSeek-R1ï¼‰ã€‚è¿™äº›ç»“æœè¯æ˜äº†å®ç°å“è¶Šæ¨ç†å’Œæ³›åŒ–èƒ½åŠ›çš„æ›´é«˜æ•ˆå’Œå¯æ‰©å±•çš„é€”å¾„ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/SII-Enigma/AMPO">https://github.com/SII-Enigma/AMPO</a>è·å–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RLVRèŒƒå¼å¢å¼ºLLMæ¨ç†èƒ½åŠ›ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¾èµ–è‡ªæˆ‘æ¢ç´¢æˆ–å•ä¸€æ•™å¸ˆï¼Œå¯èƒ½å¼•å…¥åè§å¹¶é™åˆ¶æ¢ç´¢ã€‚</li>
<li>AMPOæ¡†æ¶é€šè¿‡è‡ªé€‚åº”åˆ©ç”¨å¤šä¸ªç†Ÿç»ƒæ•™å¸ˆçš„æŒ‡å¯¼æ¥æ”¹è¿›æ­¤æƒ…å†µã€‚</li>
<li>AMPOåœ¨é€‚å½“æ—¶å€™æä¾›æŒ‡å¯¼ï¼Œä¿æŒè‡ªæˆ‘å‘ç°çš„æ¢ç´¢ä»·å€¼ã€‚</li>
<li>AMPOç»“åˆäº†åŸºäºç†è§£çš„é€‰æ‹©æœºåˆ¶ï¼Œä¿ƒè¿›æœ‰æ•ˆå­¦ä¹ ã€‚</li>
<li>å®éªŒè¡¨æ˜AMPOåœ¨æ•°å­¦å’Œåˆ†å¸ƒå¼å¤–ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜è¶Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.02227">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-6b74133a3bd3d913c3433b3f36b91091~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029307&auth_key=1760029307-0-0-2eada87a280b7238f2aa5499bf29947f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-80b65aedb635e441d5057037ef27720a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029314&auth_key=1760029314-0-0-451af53eded953434bf7228b69b25820&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1570915f2e22666acbf89398f0dc69ea~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029321&auth_key=1760029321-0-0-332139d6dd030db45a334798f4031a67&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="DiFFPO-Training-Diffusion-LLMs-to-Reason-Fast-and-Furious-via-Reinforcement-Learning"><a href="#DiFFPO-Training-Diffusion-LLMs-to-Reason-Fast-and-Furious-via-Reinforcement-Learning" class="headerlink" title="DiFFPO: Training Diffusion LLMs to Reason Fast and Furious via   Reinforcement Learning"></a>DiFFPO: Training Diffusion LLMs to Reason Fast and Furious via   Reinforcement Learning</h2><p><strong>Authors:Hanyang Zhao, Dawen Liang, Wenpin Tang, David Yao, Nathan Kallus</strong></p>
<p>We propose DiFFPO, Diffusion Fast and Furious Policy Optimization, a unified framework for training masked diffusion large language models (dLLMs) to reason not only better (furious), but also faster via reinforcement learning (RL). We first unify the existing baseline approach such as d1 by proposing to train surrogate policies via off-policy RL, whose likelihood is much more tractable as an approximation to the true dLLM policy. This naturally motivates a more accurate and informative two-stage likelihood approximation combined with importance sampling correction, which leads to generalized RL algorithms with better sample efficiency and superior task performance. Second, we propose a new direction of joint training efficient samplers&#x2F;controllers of dLLMs policy. Via RL, we incentivize dLLMsâ€™ natural multi-token prediction capabilities by letting the model learn to adaptively allocate an inference threshold for each prompt. By jointly training the sampler, we yield better accuracies with lower number of function evaluations (NFEs) compared to training the model only, obtaining the best performance in improving the Pareto frontier of the inference-time compute of dLLMs. We showcase the effectiveness of our pipeline by training open source large diffusion language models over benchmark math and planning tasks. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºDiFFPOï¼Œå³æ‰©æ•£å¿«é€Ÿä¸æ¿€çƒˆç­–ç•¥ä¼˜åŒ–ï¼ˆDiffusion Fast and Furious Policy Optimizationï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œç”¨äºè®­ç»ƒæ©ç æ‰©æ•£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆdLLMsï¼‰ï¼Œä½¿å…¶ä¸ä»…æ¨ç†èƒ½åŠ›æ›´å¼ºï¼ˆæ¿€çƒˆï¼‰ï¼Œè€Œä¸”é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ›´å¿«åœ°è¿›è¡Œæ¨ç†ã€‚æˆ‘ä»¬é¦–å…ˆé€šè¿‡æå‡ºé€šè¿‡ç¦»çº¿å¼ºåŒ–å­¦ä¹ è®­ç»ƒä»£ç†ç­–ç•¥æ¥ç»Ÿä¸€ç°æœ‰çš„åŸºçº¿æ–¹æ³•ï¼Œå¦‚d1ï¼Œè¯¥ç­–ç•¥çš„ä¼¼ç„¶æ€§ä½œä¸ºå¯¹çœŸå®dLLMç­–ç•¥çš„è¿‘ä¼¼è¦æ›´å®¹æ˜“å¤„ç†ã€‚è¿™è‡ªç„¶åœ°ä¿ƒä½¿æˆ‘ä»¬ç»“åˆé‡è¦æ€§é‡‡æ ·æ ¡æ­£ï¼Œé‡‡ç”¨æ›´å‡†ç¡®ã€æ›´å¯Œæœ‰ä¿¡æ¯é‡çš„ä¸¤é˜¶æ®µä¼¼ç„¶è¿‘ä¼¼ï¼Œä»è€Œå¾—åˆ°å…·æœ‰æ›´å¥½æ ·æœ¬æ•ˆç‡å’Œä»»åŠ¡æ€§èƒ½çš„å¹¿ä¹‰å¼ºåŒ–å­¦ä¹ ç®—æ³•ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æå‡ºäº†è”åˆè®­ç»ƒdLLMsç­–ç•¥çš„é«˜æ•ˆé‡‡æ ·å™¨&#x2F;æ§åˆ¶å™¨çš„æ–°æ–¹å‘ã€‚é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼Œæˆ‘ä»¬æ¿€åŠ±dLLMsçš„è‡ªç„¶å¤šä»¤ç‰Œé¢„æµ‹èƒ½åŠ›ï¼Œè®©æ¨¡å‹å­¦ä¹ è‡ªé€‚åº”åœ°ä¸ºæ¯ä¸ªæç¤ºåˆ†é…æ¨ç†é˜ˆå€¼ã€‚é€šè¿‡è”åˆè®­ç»ƒé‡‡æ ·å™¨ï¼Œä¸ä»…è®­ç»ƒæ¨¡å‹ç›¸æ¯”ï¼Œæˆ‘ä»¬åœ¨è¿›è¡Œæ›´å°‘æ¬¡æ•°çš„å‡½æ•°è¯„ä¼°ï¼ˆNFEsï¼‰çš„æƒ…å†µä¸‹è·å¾—äº†æ›´é«˜çš„å‡†ç¡®ç‡ï¼Œå¹¶åœ¨æ”¹å–„dLLMsæ¨ç†æ—¶é—´è®¡ç®—æ–¹é¢å–å¾—äº†æœ€ä½³çš„å¸•ç´¯æ‰˜å‰æ²¿ã€‚æˆ‘ä»¬é€šè¿‡åœ¨æ•°å­¦å’Œè§„åˆ’ä»»åŠ¡çš„åŸºå‡†æµ‹è¯•ä¸Šè®­ç»ƒå¼€æºçš„å¤§å‹æ‰©æ•£è¯­è¨€æ¨¡å‹æ¥å±•ç¤ºæˆ‘ä»¬ç®¡é“çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.02212v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†DiFFPOï¼ˆDiffusion Fast and Furious Policy Optimizationï¼‰æ¡†æ¶ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ä»¥å®ç°æ›´å¿«çš„æ¨ç†é€Ÿåº¦å’Œæé«˜æ€§èƒ½ã€‚å®ƒé€šè¿‡æå‡ºé€šè¿‡ç¦»çº¿å¼ºåŒ–å­¦ä¹ è®­ç»ƒä»£ç†ç­–ç•¥çš„æ–¹å¼ç»Ÿä¸€ç°æœ‰åŸºçº¿æ–¹æ³•ï¼Œå¹¶æå‡ºä¸€ä¸ªæ›´ç²¾ç¡®å’Œæ›´å…¨é¢çš„ä¸¤é˜¶æ®µå¯èƒ½æ€§è¿‘ä¼¼ä¸é‡è¦æ€§é‡‡æ ·ä¿®æ­£æ–¹æ³•ï¼Œä»è€Œæé«˜æ ·æœ¬æ•ˆç‡å’Œä»»åŠ¡æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æ¢ç´¢äº†è”åˆè®­ç»ƒè¯­è¨€æ¨¡å‹çš„é‡‡æ ·å™¨&#x2F;æ§åˆ¶å™¨çš„æ–°æ–¹å‘ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ æ¿€åŠ±æ¨¡å‹è‡ªé€‚åº”åˆ†é…æ¯ä¸ªæç¤ºçš„æ¨ç†é˜ˆå€¼ï¼Œå®ç°æ›´å¥½çš„å‡†ç¡®æ€§å¹¶é™ä½å‡½æ•°è¯„ä¼°æ¬¡æ•°ã€‚æœ€åï¼Œé€šè¿‡è®­ç»ƒå¤§å‹æ‰©æ•£è¯­è¨€æ¨¡å‹è¿›è¡ŒåŸºå‡†æ•°å­¦å’Œè§„åˆ’ä»»åŠ¡ï¼ŒéªŒè¯äº†è¯¥ç®¡é“çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºDiFFPOæ¡†æ¶ï¼Œæ•´åˆå¼ºåŒ–å­¦ä¹ è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ã€‚</li>
<li>é€šè¿‡ç¦»çº¿å¼ºåŒ–å­¦ä¹ è®­ç»ƒä»£ç†ç­–ç•¥ï¼Œç»Ÿä¸€ç°æœ‰åŸºçº¿æ–¹æ³•ã€‚</li>
<li>æå‡ºä¸¤é˜¶æ®µå¯èƒ½æ€§è¿‘ä¼¼ä¸é‡è¦æ€§é‡‡æ ·ä¿®æ­£æ–¹æ³•ï¼Œæé«˜æ ·æœ¬æ•ˆç‡å’Œä»»åŠ¡æ€§èƒ½ã€‚</li>
<li>æ¢ç´¢è”åˆè®­ç»ƒè¯­è¨€æ¨¡å‹çš„é‡‡æ ·å™¨&#x2F;æ§åˆ¶å™¨çš„æ–°æ–¹å‘ã€‚</li>
<li>é€šè¿‡å¼ºåŒ–å­¦ä¹ æ¿€åŠ±æ¨¡å‹è‡ªé€‚åº”åˆ†é…æ¯ä¸ªæç¤ºçš„æ¨ç†é˜ˆå€¼ã€‚</li>
<li>åœ¨åŸºå‡†æ•°å­¦å’Œè§„åˆ’ä»»åŠ¡ä¸Šè®­ç»ƒå¤§å‹æ‰©æ•£è¯­è¨€æ¨¡å‹ï¼ŒéªŒè¯ç®¡é“æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.02212">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-3126e18a37c4da8eb59295735cd84279~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029328&auth_key=1760029328-0-0-e85bb9f2b3db4e29a3cd1ee08a7d95e0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6eef0ba78c9c1230f1eb40a3f628a6c4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029336&auth_key=1760029336-0-0-f6e2366aba0ab46b1e0e0f7f7a078cbe&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ca7fe65ac380fa0703a93c355e5bf47b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029342&auth_key=1760029342-0-0-f2d369846fc385df6fab15f8a41a4088&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Say-One-Thing-Do-Another-Diagnosing-Reasoning-Execution-Gaps-in-VLM-Powered-Mobile-Use-Agents"><a href="#Say-One-Thing-Do-Another-Diagnosing-Reasoning-Execution-Gaps-in-VLM-Powered-Mobile-Use-Agents" class="headerlink" title="Say One Thing, Do Another? Diagnosing Reasoning-Execution Gaps in   VLM-Powered Mobile-Use Agents"></a>Say One Thing, Do Another? Diagnosing Reasoning-Execution Gaps in   VLM-Powered Mobile-Use Agents</h2><p><strong>Authors:Lingzhong Dong, Ziqi Zhou, Shuaibo Yang, Haiyue Sheng, Pengzhou Cheng, Zongru Wu, Zheng Wu, Gongshen Liu, Zhuosheng Zhang</strong></p>
<p>Mobile-use agents powered by vision-language models (VLMs) have shown great potential in interpreting natural language instructions and generating corresponding actions based on mobile graphical user interface. Recent studies suggest that incorporating chain-of-thought (CoT) reasoning tends to improve the execution accuracy. However, existing evaluations emphasize execution accuracy while neglecting whether CoT reasoning aligns with ground-truth actions. This oversight fails to assess potential reasoning-execution gaps, which in turn foster over-trust: users relying on seemingly plausible CoTs may unknowingly authorize harmful actions, potentially resulting in financial loss or trust crisis. In this work, we introduce a new evaluation framework to diagnose reasoning-execution gaps. At its core lies Ground-Truth Alignment (GTA), which measures whether the action implied by a CoT matches the ground-truth action. By combining GTA with the standard Exact Match (EM) metric, we jointly assess both the reasoning accuracy and execution accuracy. This joint perspective reveals two types of reasoning-execution gaps: (i) Execution Gap (EG), where the reasoning correctly identifies the correct action but execution fails, and (ii) Reasoning Gap (RG), where execution succeeds but reasoning process conflicts with the actual execution. Experimental results across a wide range of mobile interaction tasks reveal that reasoning-execution gaps are prevalent, with execution gaps occurring more frequently than reasoning gaps. Moreover, while scaling up model size reduces the overall gap, sizable execution gaps persist even in the largest models. Further analysis shows that our framework reliably reflects systematic EG&#x2F;RG patterns in state-of-the-art models. These findings offer concrete diagnostics and support the development of more trustworthy mobile-use agents. </p>
<blockquote>
<p>ç”±è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰é©±åŠ¨çš„ç§»åŠ¨ä½¿ç”¨ä»£ç†åœ¨è§£é‡Šè‡ªç„¶è¯­è¨€æŒ‡ä»¤å’ŒåŸºäºç§»åŠ¨å›¾å½¢ç”¨æˆ·ç•Œé¢ç”Ÿæˆç›¸åº”åŠ¨ä½œæ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ã€‚æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œèå…¥æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†æœ‰åŠ©äºæé«˜æ‰§è¡Œç²¾åº¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è¯„ä¼°å¼ºè°ƒæ‰§è¡Œå‡†ç¡®æ€§ï¼Œå´å¿½è§†äº†CoTæ¨ç†æ˜¯å¦ä¸çœŸå®åŠ¨ä½œç›¸ç¬¦ã€‚è¿™ç§ç–å¿½æœªèƒ½è¯„ä¼°æ½œåœ¨çš„æ¨ç†æ‰§è¡Œå·®è·ï¼Œè¿™åè¿‡æ¥ä¼šå¯¼è‡´è¿‡åº¦ä¿¡ä»»ï¼šç”¨æˆ·å¯èƒ½ä¼šä¾èµ–çœ‹ä¼¼åˆç†çš„CoTè€Œæ— æ„ä¸­æˆæƒæœ‰å®³è¡Œä¸ºï¼Œå¯èƒ½å¯¼è‡´è´¢åŠ¡æŸå¤±æˆ–ä¿¡ä»»å±æœºã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°çš„è¯„ä¼°æ¡†æ¶æ¥è¯Šæ–­æ¨ç†æ‰§è¡Œå·®è·ã€‚å…¶æ ¸å¿ƒåœ¨äºåœ°é¢çœŸå®å¯¹é½ï¼ˆGTAï¼‰ï¼Œå®ƒè¡¡é‡çš„æ˜¯ç”±CoTéšå«çš„åŠ¨ä½œæ˜¯å¦ä¸åœ°é¢çœŸå®åŠ¨ä½œç›¸åŒ¹é…ã€‚æˆ‘ä»¬å°†GTAä¸æ ‡å‡†çš„ç²¾ç¡®åŒ¹é…ï¼ˆEMï¼‰æŒ‡æ ‡ç›¸ç»“åˆï¼Œå…±åŒè¯„ä¼°æ¨ç†å‡†ç¡®æ€§å’Œæ‰§è¡Œå‡†ç¡®æ€§ã€‚è¿™ç§è”åˆè§†è§’æ­ç¤ºäº†ä¸¤ç§ç±»å‹çš„æ¨ç†æ‰§è¡Œå·®è·ï¼šï¼ˆiï¼‰æ‰§è¡Œå·®è·ï¼ˆEGï¼‰ï¼Œå³æ¨ç†æ­£ç¡®åœ°è¯†åˆ«äº†æ­£ç¡®çš„åŠ¨ä½œï¼Œä½†æ‰§è¡Œå¤±è´¥ï¼›ï¼ˆiiï¼‰æ¨ç†å·®è·ï¼ˆRGï¼‰ï¼Œå³æ‰§è¡ŒæˆåŠŸï¼Œä½†æ¨ç†è¿‡ç¨‹ä¸å®é™…æ‰§è¡Œç›¸å†²çªã€‚åœ¨å¹¿æ³›çš„ç§»åŠ¨äº¤äº’ä»»åŠ¡ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæ¨ç†æ‰§è¡Œå·®è·æ™®éå­˜åœ¨ï¼Œæ‰§è¡Œå·®è·æ¯”æ¨ç†å·®è·æ›´é¢‘ç¹åœ°å‘ç”Ÿã€‚æ­¤å¤–ï¼Œè™½ç„¶æ‰©å¤§æ¨¡å‹è§„æ¨¡å‡å°‘äº†æ€»ä½“å·®è·ï¼Œä½†åœ¨æœ€å¤§çš„æ¨¡å‹ä¸­ä»ç„¶å­˜åœ¨ç›¸å½“å¤§çš„æ‰§è¡Œå·®è·ã€‚è¿›ä¸€æ­¥çš„åˆ†æè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶å¯é åœ°åæ˜ äº†æœ€å…ˆè¿›æ¨¡å‹ä¸­çš„ç³»ç»Ÿæ€§EG&#x2F;RGæ¨¡å¼ã€‚è¿™äº›å‘ç°æä¾›äº†å…·ä½“çš„è¯Šæ–­æ”¯æŒï¼Œå¹¶æœ‰åŠ©äºå¼€å‘æ›´å€¼å¾—ä¿¡èµ–çš„ç§»åŠ¨ä½¿ç”¨ä»£ç†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.02204v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„ç§»åŠ¨ä½¿ç”¨ä»£ç†åœ¨è§£è¯»è‡ªç„¶è¯­è¨€æŒ‡ä»¤å’ŒåŸºäºç§»åŠ¨å›¾å½¢ç”¨æˆ·ç•Œé¢ç”Ÿæˆç›¸åº”åŠ¨ä½œæ–¹é¢è¡¨ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚æœ€æ–°ç ”ç©¶è¡¨æ˜ï¼Œèå…¥æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†å¯ä»¥æé«˜æ‰§è¡Œç²¾åº¦ï¼Œä½†ç°æœ‰è¯„ä¼°ä¸»è¦å…³æ³¨æ‰§è¡Œç²¾åº¦ï¼Œå¿½è§†CoTæ¨ç†æ˜¯å¦ä¸çœŸå®åŠ¨ä½œç›¸ç¬¦ã€‚æœ¬ç ”ç©¶å¼•å…¥æ–°çš„è¯„ä¼°æ¡†æ¶æ¥è¯Šæ–­æ¨ç†-æ‰§è¡Œå·®è·ï¼Œå…¶æ ¸å¿ƒæ˜¯åœ°é¢çœŸå®å¯¹é½ï¼ˆGTAï¼‰ï¼Œè¡¡é‡CoTæ‰€éšå«çš„åŠ¨ä½œæ˜¯å¦ä¸åœ°é¢çœŸå®åŠ¨ä½œåŒ¹é…ã€‚ç»“åˆGTAå’Œæ ‡å‡†ç²¾ç¡®åŒ¹é…ï¼ˆEMï¼‰æŒ‡æ ‡ï¼Œå…±åŒè¯„ä¼°æ¨ç†å‡†ç¡®æ€§ã€‚å‘ç°ä¸¤ç§ç±»å‹çš„æ¨ç†-æ‰§è¡Œå·®è·ï¼šæ‰§è¡Œå·®è·ï¼ˆEGï¼‰å’Œæ¨ç†å·®è·ï¼ˆRGï¼‰ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ¨ç†-æ‰§è¡Œå·®è·æ™®éå­˜åœ¨ï¼Œæ‰§è¡Œå·®è·æ¯”æ¨ç†å·®è·æ›´é¢‘ç¹ã€‚æ‰©å¤§æ¨¡å‹è§„æ¨¡è™½èƒ½å‡å°‘æ€»ä½“å·®è·ï¼Œä½†æ‰§è¡Œå·®è·ä»æ˜¾è‘—å­˜åœ¨ã€‚åˆ†ææ˜¾ç¤ºï¼Œè¯¥æ¡†æ¶å¯é åœ°åæ˜ äº†æœ€æ–°æ¨¡å‹ä¸­çš„ç³»ç»Ÿæ€§EG&#x2F;RGæ¨¡å¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç§»åŠ¨ä½¿ç”¨ä»£ç†é€šè¿‡è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰è§£è¯»è‡ªç„¶è¯­è¨€æŒ‡ä»¤å¹¶ç”Ÿæˆå¯¹åº”åŠ¨ä½œï¼Œå…·æœ‰å·¨å¤§æ½œåŠ›ã€‚</li>
<li>èå…¥æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†èƒ½æé«˜æ‰§è¡Œç²¾åº¦ï¼Œä½†ç°æœ‰è¯„ä¼°ä¸»è¦å…³æ³¨æ‰§è¡Œç²¾åº¦ï¼Œå¿½è§†æ¨ç†ä¸çœŸå®åŠ¨ä½œçš„ç¬¦åˆç¨‹åº¦ã€‚</li>
<li>å¼•å…¥æ–°çš„è¯„ä¼°æ¡†æ¶ï¼ŒåŒ…æ‹¬åœ°é¢çœŸå®å¯¹é½ï¼ˆGTAï¼‰æ¥è¡¡é‡æ¨ç†ä¸åŠ¨ä½œçš„åŒ¹é…åº¦ã€‚</li>
<li>å‘ç°ä¸¤ç§æ¨ç†-æ‰§è¡Œå·®è·ï¼šæ‰§è¡Œå·®è·ï¼ˆEGï¼‰å’Œæ¨ç†å·®è·ï¼ˆRGï¼‰ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºï¼Œæ¨ç†-æ‰§è¡Œå·®è·æ™®éå­˜åœ¨ï¼Œä¸”æ‰§è¡Œå·®è·æ¯”æ¨ç†å·®è·æ›´å¸¸è§ã€‚</li>
<li>æ‰©å¤§æ¨¡å‹è§„æ¨¡èƒ½å‡å°‘æ€»ä½“å·®è·ï¼Œä½†æ‰§è¡Œå·®è·ä¾ç„¶æ˜¾è‘—ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.02204">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-08688fb4a8362fedbd2860e9c59badb0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029350&auth_key=1760029350-0-0-c20c6d2089fc4e29fcdfb9886d88c8d1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-78d7da33d00864dd02de320964a05f4d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029357&auth_key=1760029357-0-0-33ef6c35dc47531a66a6cd441dcdba8f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-cd89e8d9331923e94f99fbca7e3e1c15~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029363&auth_key=1760029363-0-0-47febe76759002a8224b6ad74e4757fe&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Agentic-Reasoning-and-Refinement-through-Semantic-Interaction"><a href="#Agentic-Reasoning-and-Refinement-through-Semantic-Interaction" class="headerlink" title="Agentic Reasoning and Refinement through Semantic Interaction"></a>Agentic Reasoning and Refinement through Semantic Interaction</h2><p><strong>Authors:Xuxin Tang, Rehema Abulikemu, Eric Krokos, Kirsten Whitley, Xuan Wang, Chris North</strong></p>
<p>Sensemaking report writing often requires multiple refinements in the iterative process. While Large Language Models (LLMs) have shown promise in generating initial reports based on human visual workspace representations, they struggle to precisely incorporate sequential semantic interactions during the refinement process. We introduce VIS-ReAct, a framework that reasons about newly-added semantic interactions in visual workspaces to steer the LLM for report refinement.   VIS-ReAct is a two-agent framework: a primary LLM analysis agent interprets new semantic interactions to infer user intentions and generate refinement planning, followed by an LLM refinement agent that updates reports accordingly. Through case study, VIS-ReAct outperforms baseline and VIS-ReAct (without LLM analysis) on targeted refinement, semantic fidelity, and transparent inference. Results demonstrate that VIS-ReAct better handles various interaction types and granularities while enhancing the transparency of human-LLM collaboration. </p>
<blockquote>
<p>åœ¨æŠ¥å‘Šå†™ä½œè¿‡ç¨‹ä¸­å¾€å¾€éœ€è¦å¤šæ¬¡ä¿®æ”¹å’Œå®Œå–„ã€‚è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨åŸºäºäººç±»è§†è§‰å·¥ä½œç©ºé—´è¡¨ç¤ºç”Ÿæˆåˆæ­¥æŠ¥å‘Šæ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†åœ¨ç»†åŒ–è¿‡ç¨‹ä¸­ç²¾ç¡®åœ°èå…¥è¿ç»­çš„è¯­ä¹‰äº’åŠ¨ä»ç„¶æ˜¯ä¸€å¤§æŒ‘æˆ˜ã€‚æˆ‘ä»¬å¼•å…¥äº†VIS-ReActæ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯ä»¥æ¨ç†è§†è§‰å·¥ä½œç©ºé—´ä¸­æ–°å¢çš„è¯­ä¹‰äº’åŠ¨ï¼Œä»¥å¼•å¯¼LLMè¿›è¡ŒæŠ¥å‘Šä¼˜åŒ–ã€‚VIS-ReActæ˜¯ä¸€ä¸ªä¸¤agentæ¡†æ¶ï¼šä¸»LLMåˆ†æagentè§£è¯»æ–°çš„è¯­ä¹‰äº’åŠ¨ä»¥æ¨æ–­ç”¨æˆ·æ„å›¾å¹¶ç”Ÿæˆä¼˜åŒ–è®¡åˆ’ï¼Œç„¶åæ˜¯LLMä¼˜åŒ–agentç›¸åº”åœ°æ›´æ–°æŠ¥å‘Šã€‚é€šè¿‡æ¡ˆä¾‹ç ”ç©¶ï¼ŒVIS-ReActåœ¨ç›®æ ‡ä¼˜åŒ–ã€è¯­ä¹‰ä¿çœŸå’Œé€æ˜æ¨ç†æ–¹é¢ä¼˜äºåŸºçº¿ä»¥åŠæ²¡æœ‰LLMåˆ†æçš„VIS-ReActã€‚ç»“æœè¡¨æ˜ï¼ŒVIS-ReActæ›´å¥½åœ°å¤„ç†äº†å„ç§äº’åŠ¨ç±»å‹å’Œç²’åº¦ï¼ŒåŒæ—¶æé«˜äº†äºº-LLMåä½œçš„é€æ˜åº¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.02157v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºäººç±»è§†è§‰å·¥ä½œåŒºè¡¨ç¤ºï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç”Ÿæˆåˆæ­¥æŠ¥å‘Šæ–¹é¢è¡¨ç°å‡ºæ½œåŠ›ï¼Œä½†åœ¨è¿­ä»£è¿‡ç¨‹çš„ç»†åŒ–é˜¶æ®µèå…¥è¿ç»­çš„è¯­ä¹‰äº¤äº’æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚æˆ‘ä»¬æ¨å‡ºäº†VIS-ReActæ¡†æ¶ï¼Œå®ƒèƒ½å¤Ÿè§£æè§†è§‰å·¥ä½œåŒºä¸­æ–°å¢çš„è¯­ä¹‰äº¤äº’ï¼Œä»¥æŒ‡å¯¼LLMè¿›è¡ŒæŠ¥å‘Šç»†åŒ–ã€‚VIS-ReActæ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µæ¡†æ¶ï¼Œé¦–å…ˆç”±ä¸»è¦LLMåˆ†æä»£ç†è§£é‡Šæ–°è¯­ä¹‰äº¤äº’æ¥æ¨æ–­ç”¨æˆ·æ„å›¾å¹¶ç”Ÿæˆç»†åŒ–è®¡åˆ’ï¼Œç„¶åç”±LLMç»†åŒ–ä»£ç†ç›¸åº”åœ°æ›´æ–°æŠ¥å‘Šã€‚é€šè¿‡æ¡ˆä¾‹ç ”ç©¶ï¼ŒVIS-ReActåœ¨ç›®æ ‡ç»†åŒ–ã€è¯­ä¹‰ä¿çœŸå’Œé€æ˜æ¨ç†æ–¹é¢ä¼˜äºåŸºçº¿ä»¥åŠæ²¡æœ‰LLMåˆ†æçš„VIS-ReActã€‚ç»“æœè¡¨æ˜ï¼ŒVIS-ReActèƒ½æ›´å¥½åœ°å¤„ç†å„ç§äº¤äº’ç±»å‹å’Œç²’åº¦ï¼ŒåŒæ—¶æé«˜äººæœºåä½œçš„é€æ˜åº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç”Ÿæˆåˆæ­¥æŠ¥å‘Šæ–¹é¢è¡¨ç°å‡ºæ½œåŠ›ã€‚</li>
<li>LLMsåœ¨ç»†åŒ–é˜¶æ®µèå…¥è¿ç»­çš„è¯­ä¹‰äº¤äº’æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>VIS-ReActæ¡†æ¶é€šè¿‡è§£æè§†è§‰å·¥ä½œåŒºä¸­çš„æ–°è¯­ä¹‰äº¤äº’æ¥æŒ‡å¯¼LLMè¿›è¡ŒæŠ¥å‘Šç»†åŒ–ã€‚</li>
<li>VIS-ReActæ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µæ¡†æ¶ï¼ŒåŒ…æ‹¬LLMåˆ†æä»£ç†å’ŒLLMç»†åŒ–ä»£ç†ã€‚</li>
<li>VIS-ReActåœ¨ç›®æ ‡ç»†åŒ–ã€è¯­ä¹‰ä¿çœŸå’Œé€æ˜æ¨ç†æ–¹é¢ä¼˜äºåŸºçº¿ã€‚</li>
<li>VIS-ReActèƒ½æ›´å¥½åœ°å¤„ç†å„ç§äº¤äº’ç±»å‹å’Œç²’åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.02157">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-faeabddfe89ef2e0b49f52279c099a7b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029371&auth_key=1760029371-0-0-eae936f1ac59911f275e8680c8e6f42e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-bf6b28a1c5cbfc7eb125a4c48cb6cda5~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029378&auth_key=1760029378-0-0-be37f26b8d816b88c818c50f250cd977&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6e73c4ad0e91ca78c8eb10e069361e57~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029384&auth_key=1760029384-0-0-a980c8084e704b8d493a410393f6b98f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e5aa5c17796d9409f3e5f6f5d6ca2bf2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029391&auth_key=1760029391-0-0-e0ddb4534993308da43770fabcb6f490&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-88dbd28e135068b23abaa45f0d30b380~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029397&auth_key=1760029397-0-0-6fad03aeb2c1d3e1047fb35905b98fc1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Do-AI-Models-Perform-Human-like-Abstract-Reasoning-Across-Modalities"><a href="#Do-AI-Models-Perform-Human-like-Abstract-Reasoning-Across-Modalities" class="headerlink" title="Do AI Models Perform Human-like Abstract Reasoning Across Modalities?"></a>Do AI Models Perform Human-like Abstract Reasoning Across Modalities?</h2><p><strong>Authors:Claas Beger, Ryan Yi, Shuhao Fu, Arseny Moskvichev, Sarah W. Tsai, Sivasankaran Rajamanickam, Melanie Mitchell</strong></p>
<p>OpenAIâ€™s o3-preview reasoning model exceeded human accuracy on the ARC-AGI benchmark, but does that mean state-of-the-art models recognize and reason with the abstractions that the task creators intended? We investigate modelsâ€™ abstraction abilities on ConceptARC. We evaluate models under settings that vary the input modality (textual vs. visual), whether the model is permitted to use external Python tools, and, for reasoning models, the amount of reasoning effort. In addition to measuring output accuracy, we perform fine-grained evaluation of the natural-language rules that models generate to explain their solutions. This dual evaluation lets us assess whether models solve tasks using the abstractions ConceptARC was designed to elicit, rather than relying on surface-level patterns. Our results show that, while some models using text-based representations match human output accuracy, the best modelsâ€™ rules are often based on surface-level &#96;&#96;shortcutsâ€™â€™ and capture intended abstractions far less often than humans. Thus their capabilities for general abstract reasoning may be overestimated by evaluations based on accuracy alone. In the visual modality, AI modelsâ€™ output accuracy drops sharply, yet our rule-level analysis reveals that models might be underestimated, as they still exhibit a substantial share of rules that capture intended abstractions, but are often unable to correctly apply these rules. In short, our results show that models still lag humans in abstract reasoning, and that using accuracy alone to evaluate abstract reasoning on ARC-like tasks may overestimate abstract-reasoning capabilities in textual modalities and underestimate it in visual modalities. We believe that our evaluation framework offers a more faithful picture of multimodal modelsâ€™ abstract reasoning abilities and a more principled way to track progress toward human-like, abstraction-centered intelligence. </p>
<blockquote>
<p>OpenAIçš„o3-previewæ¨ç†æ¨¡å‹åœ¨ARC-AGIåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†äººç±»çš„å‡†ç¡®åº¦ã€‚ä½†è¿™æ˜¯å¦æ„å‘³ç€æœ€å…ˆè¿›çš„æ¨¡å‹èƒ½å¤Ÿè¯†åˆ«å¹¶ç†è§£ä»»åŠ¡åˆ›å»ºè€…æ‰€æ„å›¾çš„æŠ½è±¡æ¦‚å¿µå‘¢ï¼Ÿæˆ‘ä»¬åœ¨ConceptARCä¸Šç ”ç©¶äº†æ¨¡å‹çš„æŠ½è±¡èƒ½åŠ›ã€‚æˆ‘ä»¬è¯„ä¼°äº†åœ¨ä¸åŒè®¾ç½®ä¸‹çš„æ¨¡å‹è¡¨ç°ï¼ŒåŒ…æ‹¬è¾“å…¥æ¨¡å¼ï¼ˆæ–‡æœ¬ä¸è§†è§‰ï¼‰ã€æ¨¡å‹æ˜¯å¦å¯ä»¥ä½¿ç”¨å¤–éƒ¨Pythonå·¥å…·ï¼Œä»¥åŠå¯¹äºæ¨ç†æ¨¡å‹æ¥è¯´ï¼Œæ¨ç†åŠªåŠ›çš„ç¨‹åº¦ã€‚é™¤äº†æµ‹é‡è¾“å‡ºå‡†ç¡®åº¦å¤–ï¼Œæˆ‘ä»¬è¿˜å¯¹æ¨¡å‹ç”Ÿæˆçš„ç”¨äºè§£é‡Šå…¶è§£å†³æ–¹æ¡ˆçš„è‡ªç„¶è¯­è¨€è§„åˆ™è¿›è¡Œäº†ç²¾ç»†è¯„ä¼°ã€‚è¿™ç§åŒé‡è¯„ä¼°è®©æˆ‘ä»¬èƒ½å¤Ÿè¯„ä¼°æ¨¡å‹æ˜¯å¦ä½¿ç”¨ConceptARCè®¾è®¡çš„æŠ½è±¡æ¦‚å¿µæ¥è§£å†³é—®é¢˜ï¼Œè€Œä¸æ˜¯ä¾èµ–è¡¨é¢æ¨¡å¼ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œè™½ç„¶ä¸€äº›ä½¿ç”¨æ–‡æœ¬è¡¨ç¤ºçš„æ¨¡å‹ä¸äººç±»è¾“å‡ºå‡†ç¡®åº¦ç›¸åŒ¹é…ï¼Œä½†æœ€ä½³æ¨¡å‹çš„è§„åˆ™é€šå¸¸åŸºäºè¡¨é¢å±‚æ¬¡çš„â€œæ·å¾„â€ï¼Œå¹¶ä¸”æ•è·çš„æ„å›¾æŠ½è±¡è¿œä¸åŠäººç±»é¢‘ç¹ã€‚å› æ­¤ï¼Œä»…é€šè¿‡å‡†ç¡®æ€§è¯„ä¼°å¯èƒ½ä¼šé«˜ä¼°è¿™äº›æ¨¡å‹åœ¨ä¸€èˆ¬æŠ½è±¡æ¨ç†æ–¹é¢çš„èƒ½åŠ›ã€‚åœ¨è§†è§‰æ¨¡å¼ä¸‹ï¼ŒAIæ¨¡å‹çš„è¾“å‡ºå‡†ç¡®åº¦æ€¥å‰§ä¸‹é™ï¼Œä½†æˆ‘ä»¬çš„è§„åˆ™å±‚é¢åˆ†æè¡¨æ˜ï¼Œæ¨¡å‹å¯èƒ½è¢«ä½ä¼°äº†ï¼Œå› ä¸ºå®ƒä»¬ä»ç„¶è¡¨ç°å‡ºç›¸å½“æ•°é‡çš„è§„åˆ™èƒ½å¤Ÿæ•è·æ„å›¾çš„æŠ½è±¡ï¼Œä½†å¾€å¾€æ— æ³•æ­£ç¡®åº”ç”¨è¿™äº›è§„åˆ™ã€‚æ€»ä¹‹ï¼Œæˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œè¿™äº›æ¨¡å‹åœ¨æŠ½è±¡æ¨ç†æ–¹é¢ä»ç„¶è½åäºäººç±»ï¼Œå¹¶ä¸”ä»…ä½¿ç”¨å‡†ç¡®æ€§æ¥è¯„ä¼°ARCç±»ä»»åŠ¡ä¸Šçš„æŠ½è±¡æ¨ç†å¯èƒ½ä¼šé«˜ä¼°æ–‡æœ¬æ¨¡å¼ä¸­çš„æŠ½è±¡æ¨ç†èƒ½åŠ›å¹¶ä½ä¼°è§†è§‰æ¨¡å¼ä¸­çš„èƒ½åŠ›ã€‚æˆ‘ä»¬ç›¸ä¿¡ï¼Œæˆ‘ä»¬çš„è¯„ä¼°æ¡†æ¶æä¾›äº†å¯¹å¤šæ¨¡å¼æ¨¡å‹çš„æŠ½è±¡æ¨ç†èƒ½åŠ›æ›´çœŸå®çš„æè¿°ï¼Œä»¥åŠä¸€ä¸ªæ›´åŸåˆ™æ€§çš„æ–¹æ³•æ¥è·Ÿè¸ªæœç€äººç±»èˆ¬çš„ã€ä»¥æŠ½è±¡ä¸ºä¸­å¿ƒçš„æ™ºåŠ›çš„è¿›æ­¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.02125v1">PDF</a> 10 pages, 4 figures</p>
<p><strong>Summary</strong></p>
<p>åœ¨ARC-AGIåŸºå‡†æµ‹è¯•ä¸­ï¼ŒOpenAIçš„o3-previewæ¨ç†æ¨¡å‹è¶…è¶Šäº†äººç±»çš„å‡†ç¡®æ€§ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å¯¹å…¶åœ¨ConceptARCä¸Šçš„æŠ½è±¡èƒ½åŠ›è¿›è¡Œäº†è°ƒæŸ¥ï¼Œå‘ç°åœ¨ä¸åŒè¾“å…¥æ¨¡å¼ï¼ˆæ–‡æœ¬ä¸è§†è§‰ï¼‰ã€æ˜¯å¦å…è®¸ä½¿ç”¨å¤–éƒ¨Pythonå·¥å…·ä»¥åŠæ¨ç†æ¨¡å‹çš„æ¨ç†åŠªåŠ›ç¨‹åº¦ä¸‹ï¼Œæ¨¡å‹çš„æ€§èƒ½å­˜åœ¨å·®å¼‚ã€‚å°½ç®¡ä¸€äº›åŸºäºæ–‡æœ¬è¡¨ç¤ºçš„æ¨¡å‹åŒ¹é…äº†äººç±»çš„è¾“å‡ºå‡†ç¡®æ€§ï¼Œä½†å®ƒä»¬çš„è§„åˆ™å¾€å¾€åŸºäºè¡¨é¢å±‚æ¬¡çš„â€œæ·å¾„â€ï¼Œå¹¶ä¸”æ•è·çš„æŠ½è±¡æ„å›¾è¿œä¸åŠäººç±»ã€‚åœ¨è§†è§‰æ¨¡å¼ä¸‹ï¼ŒAIæ¨¡å‹çš„è¾“å‡ºå‡†ç¡®æ€§æ€¥å‰§ä¸‹é™ï¼Œä½†åœ¨è§„åˆ™å±‚é¢çš„åˆ†ææ˜¾ç¤ºï¼Œæ¨¡å‹å¯èƒ½ä»ç„¶ä½ä¼°äº†å…¶èƒ½åŠ›ï¼Œå› ä¸ºå®ƒä»¬è¡¨ç°å‡ºç›¸å½“å¤§çš„è§„åˆ™èƒ½å¤Ÿæ•è·é¢„æœŸçš„æŠ½è±¡æ„å›¾ï¼Œä½†å¾€å¾€æ— æ³•æ­£ç¡®åº”ç”¨è¿™äº›è§„åˆ™ã€‚æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œåœ¨æŠ½è±¡æ¨ç†æ–¹é¢ï¼Œæ¨¡å‹ä»ç„¶è½åäºäººç±»ï¼Œä»…ä½¿ç”¨å‡†ç¡®æ€§æ¥è¯„ä¼°ARCç±»ä¼¼ä»»åŠ¡ä¸Šçš„æŠ½è±¡æ¨ç†å¯èƒ½ä¼šé«˜ä¼°æ–‡æœ¬æ¨¡æ€ä¸‹çš„æŠ½è±¡æ¨ç†èƒ½åŠ›å¹¶ä½ä¼°è§†è§‰æ¨¡æ€ä¸‹çš„èƒ½åŠ›ã€‚æˆ‘ä»¬ç›¸ä¿¡æˆ‘ä»¬çš„è¯„ä¼°æ¡†æ¶æ›´çœŸå®åœ°åæ˜ äº†å¤šæ¨¡æ€æ¨¡å‹çš„æŠ½è±¡æ¨ç†èƒ½åŠ›ï¼Œå¹¶ä¸ºè¿½è¸ªäººç±»æŠ½è±¡æ™ºèƒ½çš„è¿›æ­¥æä¾›äº†æ›´åŸåˆ™æ€§çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>OpenAIçš„o3-previewæ¨¡å‹åœ¨ARC-AGIåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°è¶…è¶Šäººç±»ã€‚</li>
<li>åœ¨ConceptARCä¸Šè¯„ä¼°äº†æ¨¡å‹çš„æŠ½è±¡èƒ½åŠ›ï¼Œæ¶‰åŠä¸åŒè¾“å…¥æ¨¡å¼å’Œæ¨¡å‹ä½¿ç”¨å¤–éƒ¨å·¥å…·çš„æƒ…å¢ƒã€‚</li>
<li>åœ¨æ–‡æœ¬æ¨¡å¼ä¸‹ï¼Œä¸€äº›æ¨¡å‹çš„è¾“å‡ºå‡†ç¡®æ€§åŒ¹é…äººç±»ï¼Œä½†å®ƒä»¬çš„è§„åˆ™æ›´å¤šåŸºäºè¡¨é¢å±‚æ¬¡çš„â€œæ·å¾„â€ã€‚</li>
<li>åœ¨è§†è§‰æ¨¡å¼ä¸‹ï¼ŒAIæ¨¡å‹çš„è¾“å‡ºå‡†ç¡®æ€§æ˜¾è‘—ä¸‹é™ã€‚</li>
<li>è§„åˆ™å±‚é¢çš„åˆ†ææ˜¾ç¤ºæ¨¡å‹åœ¨è§†è§‰æ¨¡æ€ä¸‹å¯èƒ½ä»ç„¶å…·å¤‡ä¸€äº›æŠ½è±¡èƒ½åŠ›ï¼Œä½†æ— æ³•æ­£ç¡®åº”ç”¨è¿™äº›è§„åˆ™ã€‚</li>
<li>ä»…ä½¿ç”¨å‡†ç¡®æ€§è¯„ä¼°æŠ½è±¡æ¨ç†å¯èƒ½é«˜ä¼°æ–‡æœ¬æ¨¡æ€ä¸‹çš„èƒ½åŠ›å¹¶ä½ä¼°è§†è§‰æ¨¡æ€ä¸‹çš„èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.02125">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-e023ad11b74f83eb982836cd946b3431~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029405&auth_key=1760029405-0-0-b32f27912c6e1d7cc283638bd37088ac&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d6a67c2e9ba4ad8c0b4aaa931fa4ba9f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029412&auth_key=1760029412-0-0-97299b863119ff977b933493785348b5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c14f05c7dfb19643906017eed4dbb8f0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029419&auth_key=1760029419-0-0-1b91c854a6c58bbb4888066cab5c3c1c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Demystifying-the-Roles-of-LLM-Layers-in-Retrieval-Knowledge-and-Reasoning"><a href="#Demystifying-the-Roles-of-LLM-Layers-in-Retrieval-Knowledge-and-Reasoning" class="headerlink" title="Demystifying the Roles of LLM Layers in Retrieval, Knowledge, and   Reasoning"></a>Demystifying the Roles of LLM Layers in Retrieval, Knowledge, and   Reasoning</h2><p><strong>Authors:Xinyuan Song, Keyu Wang, PengXiang Li, Lu Yin, Shiwei Liu</strong></p>
<p>Recent studies suggest that the deeper layers of Large Language Models (LLMs) contribute little to representation learning and can often be removed without significant performance loss. However, such claims are typically drawn from narrow evaluations and may overlook important aspects of model behavior. In this work, we present a systematic study of depth utilization across diverse dimensions, including evaluation protocols, task categories, and model architectures. Our analysis confirms that very deep layers are generally less effective than earlier ones, but their contributions vary substantially with the evaluation setting. Under likelihood-based metrics without generation, pruning most layers preserves performance, with only the initial few being critical. By contrast, generation-based evaluation uncovers indispensable roles for middle and deeper layers in enabling reasoning and maintaining long-range coherence. We further find that knowledge and retrieval are concentrated in shallow components, whereas reasoning accuracy relies heavily on deeper layers â€“ yet can be reshaped through distillation. These results highlight that depth usage in LLMs is highly heterogeneous and context-dependent, underscoring the need for task-, metric-, and model-aware perspectives in both interpreting and compressing large models. </p>
<blockquote>
<p>æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ·±å±‚å¯¹è¡¨ç¤ºå­¦ä¹ è´¡çŒ®ç”šå¾®ï¼Œé€šå¸¸å¯ä»¥å»é™¤è€Œä¸ä¼šå¯¹æ€§èƒ½é€ æˆæ˜¾è‘—æŸå¤±ã€‚ç„¶è€Œï¼Œè¿™äº›è¯´æ³•é€šå¸¸æ¥è‡ªäºç‹­éš˜çš„è¯„ä¼°ï¼Œå¯èƒ½ä¼šå¿½ç•¥æ¨¡å‹è¡Œä¸ºçš„é‡è¦æ–¹é¢ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¯¹æ·±åº¦åˆ©ç”¨è¿›è¡Œäº†ç³»ç»Ÿçš„ç ”ç©¶ï¼Œæ¶‰åŠå¤šä¸ªç»´åº¦ï¼ŒåŒ…æ‹¬è¯„ä¼°åè®®ã€ä»»åŠ¡ç±»åˆ«å’Œæ¨¡å‹æ¶æ„ã€‚æˆ‘ä»¬çš„åˆ†æè¯å®ï¼Œæ·±å±‚é€šå¸¸ä¸å¦‚æµ…å±‚æœ‰æ•ˆï¼Œä½†å®ƒä»¬çš„è´¡çŒ®åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå–å†³äºè¯„ä¼°ç¯å¢ƒã€‚åœ¨æ²¡æœ‰ç”Ÿæˆçš„åŸºäºå¯èƒ½æ€§çš„æŒ‡æ ‡ä¸‹ï¼Œåˆ é™¤å¤§éƒ¨åˆ†å±‚èƒ½å¤Ÿä¿ç•™æ€§èƒ½ï¼Œåªæœ‰å‰å‡ å±‚æœ€ä¸ºå…³é”®ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒåŸºäºç”Ÿæˆçš„è¯„ä¼°æ­ç¤ºäº†ä¸­å±‚å’Œæ·±å±‚åœ¨æ¨ç†å’Œä¿æŒé•¿æœŸè¿è´¯æ€§ä¸­çš„ä¸å¯æˆ–ç¼ºä½œç”¨ã€‚æˆ‘ä»¬è¿˜å‘ç°ï¼ŒçŸ¥è¯†å’Œæ£€ç´¢é›†ä¸­åœ¨æµ…å±‚ç»„ä»¶ä¸­ï¼Œè€Œæ¨ç†å‡†ç¡®æ€§åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä¾èµ–äºæ·±å±‚ï¼Œä½†å¯ä»¥é€šè¿‡è’¸é¦æ¥é‡å¡‘ã€‚è¿™äº›ç»“æœå¼ºè°ƒï¼Œåœ¨LLMä¸­ä½¿ç”¨æ·±åº¦çš„é«˜åº¦å¼‚è´¨æ€§å’Œä¸Šä¸‹æ–‡ä¾èµ–æ€§ï¼Œå¹¶å¼ºè°ƒäº†åœ¨ä½¿ç”¨ä»»åŠ¡ã€æŒ‡æ ‡å’Œæ¨¡å‹ç†è§£å¤§å‹æ¨¡å‹å‹ç¼©çš„åŒæ—¶ï¼Œè¿›è¡Œè§£é‡Šçš„éœ€è¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.02091v1">PDF</a> ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸç ”ç©¶è¡¨æ˜ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ·±å±‚å¯¹è¡¨ç¤ºå­¦ä¹ è´¡çŒ®è¾ƒå°ï¼Œä¸”é€šå¸¸å¯ç§»é™¤è€Œä¸ä¼šå¯¹æ€§èƒ½é€ æˆæ˜¾è‘—æŸå¤±ã€‚ç„¶è€Œï¼Œè¿™äº›ç»“è®ºé€šå¸¸æ¥è‡ªç‹­çª„çš„è¯„ä¼°ï¼Œå¯èƒ½å¿½ç•¥äº†æ¨¡å‹è¡Œä¸ºçš„é‡è¦æ–¹é¢ã€‚æœ¬ç ”ç©¶ç³»ç»Ÿåœ°æ¢è®¨äº†æ·±åº¦åˆ©ç”¨çš„ä¸åŒç»´åº¦ï¼ŒåŒ…æ‹¬è¯„ä¼°åè®®ã€ä»»åŠ¡ç±»åˆ«å’Œæ¨¡å‹æ¶æ„ã€‚åˆ†æè¡¨æ˜ï¼Œæ·±å±‚é€šå¸¸ä¸å¦‚æµ…å±‚æœ‰æ•ˆï¼Œä½†å…¶åœ¨ä¸åŒè¯„ä¼°è®¾ç½®ä¸­çš„è´¡çŒ®å·®å¼‚å¾ˆå¤§ã€‚åœ¨ä¸ç”Ÿæˆçš„æƒ…å†µä¸‹ï¼ŒåŸºäºå¯èƒ½æ€§çš„åº¦é‡æŒ‡æ ‡æ˜¾ç¤ºä¿®å‰ªå¤§éƒ¨åˆ†å±‚å¯ä¿æŒæ€§èƒ½ï¼Œåªæœ‰æœ€åˆçš„å‡ å±‚è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼ŒåŸºäºç”Ÿæˆçš„è¯„ä¼°æ­ç¤ºäº†ä¸­å±‚å’Œæ·±å±‚åœ¨æ¨ç†å’Œç»´æŒé•¿ç¨‹è¿è´¯æ€§ä¸­çš„ä¸å¯æˆ–ç¼ºä½œç”¨ã€‚è¿˜å‘ç°çŸ¥è¯†å’Œæ£€ç´¢é›†ä¸­åœ¨æµ…å±‚ç»„ä»¶ä¸­ï¼Œè€Œæ¨ç†å‡†ç¡®æ€§åˆ™é«˜åº¦ä¾èµ–äºæ·±å±‚ï¼Œä½†å¯é€šè¿‡è’¸é¦æ¥æ”¹å˜ã€‚è¿™äº›ç»“æœå¼ºè°ƒï¼Œåœ¨å¤§å‹æ¨¡å‹ä¸­åˆ©ç”¨æ·±åº¦æ˜¯éå¸¸å¤æ‚å’Œå¤šå˜çš„ï¼Œéœ€è¦åœ¨è§£é‡Šå’Œå‹ç¼©æ¨¡å‹æ—¶è€ƒè™‘ä»»åŠ¡ã€æŒ‡æ ‡å’Œæ¨¡å‹æ„è¯†çš„è§’åº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹çš„æ·±å±‚å¯¹è¡¨ç¤ºå­¦ä¹ çš„è´¡çŒ®ç›¸å¯¹è¾ƒå°ï¼Œå¯ç§»é™¤è€Œä¸å¯¹æ€§èƒ½é€ æˆæ˜¾è‘—æŸå¤±ã€‚</li>
<li>ä¸åŒè¯„ä¼°è®¾ç½®ä¸‹ï¼Œæ·±å±‚æ¨¡å‹çš„è´¡çŒ®å·®å¼‚æ˜¾è‘—ã€‚</li>
<li>åŸºäºå¯èƒ½æ€§çš„åº¦é‡æŒ‡æ ‡æ˜¾ç¤ºï¼Œåœ¨ä¸éœ€è¦ç”Ÿæˆçš„æƒ…å†µä¸‹ï¼Œä¿®å‰ªå¤§éƒ¨åˆ†æ¨¡å‹å±‚å¯ä»¥ä¿æŒæ€§èƒ½ã€‚</li>
<li>åŸºäºç”Ÿæˆçš„è¯„ä¼°è¡¨æ˜ä¸­å±‚å’Œæ·±å±‚åœ¨æ¨ç†å’Œç»´æŒé•¿ç¨‹è¿è´¯æ€§ä¸­èµ·é‡è¦ä½œç”¨ã€‚</li>
<li>çŸ¥è¯†å’Œæ£€ç´¢åŠŸèƒ½ä¸»è¦é›†ä¸­åœ¨æµ…å±‚ç»„ä»¶ä¸­ã€‚</li>
<li>æ¨ç†å‡†ç¡®æ€§é«˜åº¦ä¾èµ–äºæ·±å±‚æ¨¡å‹ï¼Œä½†å¯é€šè¿‡è’¸é¦æ¥æ”¹å˜è¿™ä¸€ä¾èµ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.02091">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-041b37c4d2f70ab55f5ccca305c86359~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029426&auth_key=1760029426-0-0-67d08ed8cda20466dbb91f151d622cbc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6e069402149270c563834b8280b054df~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029434&auth_key=1760029434-0-0-d9e51113ac6736ab0517aae5669a72d8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b48fdd495a841660ebb705613c2c6bc8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029442&auth_key=1760029442-0-0-1301f2b9669451c5884dce9252eede05&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-91a86266f7561f4ca797544f56c1f459~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029448&auth_key=1760029448-0-0-679b6cca7da0b30bc220a5362a076262&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ef3d49f8e432b27f75308c707704d8da~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029455&auth_key=1760029455-0-0-b906a37f1a8ee6c8601e09ac5e678e5e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a14053299ab2ea786a3be95606864bff~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029462&auth_key=1760029462-0-0-19e0948f777314f45aaf0450b0667dbf&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Enhancing-Large-Language-Model-Reasoning-with-Reward-Models-An-Analytical-Survey"><a href="#Enhancing-Large-Language-Model-Reasoning-with-Reward-Models-An-Analytical-Survey" class="headerlink" title="Enhancing Large Language Model Reasoning with Reward Models: An   Analytical Survey"></a>Enhancing Large Language Model Reasoning with Reward Models: An   Analytical Survey</h2><p><strong>Authors:Qiyuan Liu, Hao Xu, Xuhong Chen, Wei Chen, Yee Whye Teh, Ning Miao</strong></p>
<p>Reward models (RMs) play a critical role in enhancing the reasoning performance of LLMs. For example, they can provide training signals to finetune LLMs during reinforcement learning (RL) and help select the best answer from multiple candidates during inference. In this paper, we provide a systematic introduction to RMs, along with a comprehensive survey of their applications in LLM reasoning. We first review fundamental concepts of RMs, including their architectures, training methodologies, and evaluation techniques. Then, we explore their key applications: (1) guiding generation and selecting optimal outputs during LLM inference, (2) facilitating data synthesis and iterative self-improvement for LLMs, and (3) providing training signals in RL-based finetuning. Finally, we address critical open questions regarding the selection, generalization, evaluation, and enhancement of RMs, based on existing research and our own empirical findings. Our analysis aims to provide actionable insights for the effective deployment and advancement of RMs for LLM reasoning. </p>
<blockquote>
<p>å¥–åŠ±æ¨¡å‹ï¼ˆRMsï¼‰åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†æ€§èƒ½ä¸­å‘æŒ¥ç€å…³é”®ä½œç”¨ã€‚ä¾‹å¦‚ï¼Œå®ƒä»¬å¯ä»¥åœ¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¿‡ç¨‹ä¸­ä¸ºLLMsæä¾›è®­ç»ƒä¿¡å·ï¼Œå¹¶åœ¨æ¨ç†è¿‡ç¨‹ä¸­å¸®åŠ©ä»å¤šä¸ªå€™é€‰ç­”æ¡ˆä¸­é€‰æ‹©æœ€ä½³ç­”æ¡ˆã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¯¹RMsè¿›è¡Œäº†ç³»ç»Ÿä»‹ç»ï¼Œå¹¶å…¨é¢æ¦‚è¿°äº†å®ƒä»¬åœ¨LLMæ¨ç†ä¸­çš„åº”ç”¨ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å›é¡¾äº†RMsçš„åŸºæœ¬æ¦‚å¿µï¼ŒåŒ…æ‹¬å…¶æ¶æ„ã€è®­ç»ƒæ–¹æ³•å’Œè¯„ä¼°æŠ€æœ¯ã€‚ç„¶åï¼Œæˆ‘ä»¬æ¢è®¨äº†å…¶å…³é”®åº”ç”¨ï¼šï¼ˆ1ï¼‰åœ¨LLMæ¨ç†è¿‡ç¨‹ä¸­æŒ‡å¯¼ç”Ÿæˆå¹¶é€‰æ‹©æœ€ä½³è¾“å‡ºï¼Œï¼ˆ2ï¼‰ä¿ƒè¿›æ•°æ®åˆæˆå’ŒLLMçš„è¿­ä»£è‡ªæˆ‘æ”¹è¿›ï¼Œï¼ˆ3ï¼‰åœ¨åŸºäºRLçš„å¾®è°ƒä¸­æä¾›è®­ç»ƒä¿¡å·ã€‚æœ€åï¼Œæˆ‘ä»¬åŸºäºç°æœ‰ç ”ç©¶å’Œæˆ‘ä»¬çš„å®è¯å‘ç°ï¼Œæ¢è®¨äº†å…³äºRMsçš„é€‰æ‹©ã€æ³›åŒ–ã€è¯„ä¼°å’Œæ”¹è¿›çš„å…³é”®å¼€æ”¾é—®é¢˜ã€‚æˆ‘ä»¬çš„åˆ†ææ—¨åœ¨æä¾›æœ‰å…³RMsåœ¨LLMæ¨ç†ä¸­æœ‰æ•ˆéƒ¨ç½²å’Œè¿›æ­¥çš„å¯è¡Œè§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.01925v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¥–åŠ±æ¨¡å‹ï¼ˆRMsï¼‰åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†æ€§èƒ½ä¸­å‘æŒ¥ç€å…³é”®ä½œç”¨ã€‚æœ¬æ–‡é€šè¿‡ç³»ç»Ÿæ€§çš„ä»‹ç»å’Œå…¨é¢çš„è°ƒç ”ï¼Œé˜è¿°äº†RMsåœ¨LLMæ¨ç†ä¸­çš„åº”ç”¨ã€‚æ–‡ç« é¦–å…ˆå›é¡¾äº†RMsçš„åŸºæœ¬æ¦‚å¿µï¼ŒåŒ…æ‹¬å…¶æ¶æ„ã€è®­ç»ƒæ–¹æ³•å’Œè¯„ä¼°æŠ€æœ¯ã€‚ç„¶åï¼Œæ¢è®¨äº†RMsåœ¨LLMæ¨ç†ä¸­çš„å…³é”®åº”ç”¨ï¼ŒåŒ…æ‹¬å¼•å¯¼ç”Ÿæˆå’Œé€‰æ‹©æœ€ä½³è¾“å‡ºã€ä¿ƒè¿›æ•°æ®åˆæˆå’Œè¿­ä»£è‡ªæˆ‘æ”¹è¿›ä»¥åŠæä¾›å¼ºåŒ–å­¦ä¹ å¾®è°ƒä¸­çš„è®­ç»ƒä¿¡å·ã€‚æœ€åï¼ŒåŸºäºç°æœ‰ç ”ç©¶å’Œå®è¯å‘ç°ï¼Œå¯¹RMsçš„é€‰æ‹©ã€æ³›åŒ–ã€è¯„ä¼°å’Œæ”¹è¿›ç­‰å…³é”®é—®é¢˜è¿›è¡Œäº†æ¢è®¨ã€‚æœ¬æ–‡æ—¨åœ¨æä¾›å¯¹RMsåœ¨LLMæ¨ç†ä¸­æœ‰æ•ˆéƒ¨ç½²å’Œå‘å±•çš„å¯è¡Œè§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¥–åŠ±æ¨¡å‹ï¼ˆRMsï¼‰åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†æ€§èƒ½ä¸­èµ·å…³é”®ä½œç”¨ã€‚</li>
<li>RMså¯ä»¥é€šè¿‡æä¾›è®­ç»ƒä¿¡å·æ¥å¾®è°ƒLLMsï¼Œå¹¶åœ¨æ¨ç†è¿‡ç¨‹ä¸­å¸®åŠ©é€‰æ‹©æœ€ä½³ç­”æ¡ˆã€‚</li>
<li>æœ¬æ–‡ç³»ç»Ÿä»‹ç»äº†RMsçš„åŸºæœ¬æ¦‚å¿µï¼ŒåŒ…æ‹¬æ¶æ„ã€è®­ç»ƒæ–¹æ³•å’Œè¯„ä¼°æŠ€æœ¯ã€‚</li>
<li>RMsåœ¨LLMæ¨ç†ä¸­çš„å…³é”®åº”ç”¨åŒ…æ‹¬å¼•å¯¼ç”Ÿæˆå’Œé€‰æ‹©æœ€ä½³è¾“å‡ºã€ä¿ƒè¿›æ•°æ®åˆæˆå’Œè¿­ä»£è‡ªæˆ‘æ”¹è¿›ã€‚</li>
<li>RMsåœ¨å¼ºåŒ–å­¦ä¹ å¾®è°ƒä¸­æä¾›è®­ç»ƒä¿¡å·ï¼Œå¯¹LLMæ€§èƒ½æœ‰é‡è¦å½±å“ã€‚</li>
<li>åœ¨RMsçš„é€‰æ‹©ã€æ³›åŒ–ã€è¯„ä¼°å’Œæ”¹è¿›ç­‰æ–¹é¢ä»å­˜åœ¨è®¸å¤šå…³é”®é—®é¢˜æœ‰å¾…è§£å†³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.01925">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-a2658456aca2de392c1a6e7727a531ab~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029470&auth_key=1760029470-0-0-787570b2016a03782cf9d58df1573243&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-091b5852c251f8586dcc4ee3a1608347~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029478&auth_key=1760029478-0-0-29b1dca8ce0b5d08d8393299b6478b27&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1cbe98134daf875113e7934c1623f055~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029484&auth_key=1760029484-0-0-48d726a6c3c20153de496562cf6c374e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-252c3ddf2a9ae84874334a5c42b852cc~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029491&auth_key=1760029491-0-0-55824ad550f481bc37b79cd7015743de&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Plan-Then-Action-High-Level-Planning-Guidance-Reinforcement-Learning-for-LLM-Reasoning"><a href="#Plan-Then-Action-High-Level-Planning-Guidance-Reinforcement-Learning-for-LLM-Reasoning" class="headerlink" title="Plan Then Action:High-Level Planning Guidance Reinforcement Learning for   LLM Reasoning"></a>Plan Then Action:High-Level Planning Guidance Reinforcement Learning for   LLM Reasoning</h2><p><strong>Authors:Zhihao Dou, Qinjian Zhao, Zhongwei Wan, Dinggen Zhang, Weida Wang, Towsif Raiyan, Benteng Chen, Qingtao Pan, Yang Ouyang, Zhiqiang Gao, Shufei Zhang, Sumon Biswas</strong></p>
<p>Large language models (LLMs) have demonstrated remarkable reasoning abilities in complex tasks, often relying on Chain-of-Thought (CoT) reasoning. However, due to their autoregressive token-level generation, the reasoning process is largely constrained to local decision-making and lacks global planning. This limitation frequently results in redundant, incoherent, or inaccurate reasoning, which significantly degrades overall performance. Existing approaches, such as tree-based algorithms and reinforcement learning (RL), attempt to address this issue but suffer from high computational costs and often fail to produce optimal reasoning trajectories. To tackle this challenge, we propose Plan-Then-Action Enhanced Reasoning with Group Relative Policy Optimization PTA-GRPO, a two-stage framework designed to improve both high-level planning and fine-grained CoT reasoning. In the first stage, we leverage advanced LLMs to distill CoT into compact high-level guidance, which is then used for supervised fine-tuning (SFT). In the second stage, we introduce a guidance-aware RL method that jointly optimizes the final output and the quality of high-level guidance, thereby enhancing reasoning effectiveness. We conduct extensive experiments on multiple mathematical reasoning benchmarks, including MATH, AIME2024, AIME2025, and AMC, across diverse base models such as Qwen2.5-7B-Instruct, Qwen3-8B, Qwen3-14B, and LLaMA3.2-3B. Experimental results demonstrate that PTA-GRPO consistently achieves stable and significant improvements across different models and tasks, validating its effectiveness and generalization. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤æ‚ä»»åŠ¡ä¸­å±•ç°å‡ºäº†æƒŠäººçš„æ¨ç†èƒ½åŠ›ï¼Œé€šå¸¸ä¾èµ–äºé“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†ã€‚ç„¶è€Œï¼Œç”±äºå…¶è‡ªå›å½’çš„tokençº§ç”Ÿæˆï¼Œæ¨ç†è¿‡ç¨‹åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå—åˆ°å±€éƒ¨å†³ç­–çš„é™åˆ¶ï¼Œç¼ºä¹å…¨å±€è§„åˆ’ã€‚è¿™ç§å±€é™æ€§ç»å¸¸å¯¼è‡´å†—ä½™ã€ä¸è¿è´¯æˆ–é”™è¯¯çš„æ¨ç†ï¼Œä»è€Œæ˜¾è‘—é™ä½äº†æ•´ä½“æ€§èƒ½ã€‚ç°æœ‰æ–¹æ³•ï¼Œå¦‚åŸºäºæ ‘ç®—æ³•å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ï¼Œè¯•å›¾è§£å†³è¿™ä¸€é—®é¢˜ï¼Œä½†è®¡ç®—æˆæœ¬é«˜æ˜‚ï¼Œä¸”å¾€å¾€æ— æ³•äº§ç”Ÿæœ€ä½³çš„æ¨ç†è½¨è¿¹ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†â€œè®¡åˆ’ç„¶åè¡ŒåŠ¨å¢å¼ºæ¨ç†ä¸ç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–â€ï¼ˆPTA-GRPOï¼‰çš„ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œæ—¨åœ¨åŒæ—¶æé«˜é«˜çº§è§„åˆ’å’Œç²¾ç»†çš„CoTæ¨ç†ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œæˆ‘ä»¬åˆ©ç”¨å…ˆè¿›çš„LLMå°†CoTæç‚¼æˆç´§å‡‘çš„é«˜çº§æŒ‡å¯¼ï¼Œç„¶åç”¨äºç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æŒ‡å¯¼æ„ŸçŸ¥çš„RLæ–¹æ³•ï¼Œè¯¥æ–¹æ³•è”åˆä¼˜åŒ–æœ€ç»ˆè¾“å‡ºå’Œé«˜çº§æŒ‡å¯¼çš„è´¨é‡ï¼Œä»è€Œæé«˜æ¨ç†çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªæ•°å­¦æ¨ç†åŸºå‡†ä¸Šè¿›è¡Œäº†å¹¿æ³›å®éªŒï¼ŒåŒ…æ‹¬MATHã€AIME2024ã€AIME2025å’ŒAMCç­‰ä»»åŠ¡åŸºå‡†æµ‹è¯•æ•°æ®é›†å’ŒåŒ…æ‹¬Qwen 2.5-7B-Instructç­‰åœ¨å†…çš„å¤šä¸ªåŸºç¡€æ¨¡å‹ä¸Šè¿›è¡Œäº†å®éªŒéªŒè¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPTA-GRPOåœ¨ä¸åŒæ¨¡å‹å’Œä»»åŠ¡ä¸Šå‡å®ç°äº†ç¨³å®šå’Œæ˜¾è‘—çš„æ”¹è¿›ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.01833v1">PDF</a> 19 pages and 5 figures</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚ä»»åŠ¡ä¸­å±•ç°å‡ºæƒŠäººçš„æ¨ç†èƒ½åŠ›ï¼Œä¸»è¦ä¾èµ–äºé“¾å¼æ€ç»´æ¨ç†ã€‚ç„¶è€Œï¼Œç”±äºåŸºäºtokençš„è‡ªå›å½’ç”Ÿæˆæ–¹å¼ï¼Œå…¶æ¨ç†è¿‡ç¨‹å¤§å¤šå±€é™äºæœ¬åœ°å†³ç­–åˆ¶å®šï¼Œç¼ºä¹å…¨å±€è§„åˆ’ã€‚è¿™å¯¼è‡´é¢‘ç¹å‡ºç°å†—ä½™ã€ä¸è¿è´¯æˆ–é”™è¯¯çš„æ¨ç†ï¼Œä¸¥é‡å½±å“æ•´ä½“æ€§èƒ½ã€‚ä¸ºåº”å¯¹æ­¤æŒ‘æˆ˜ï¼Œæå‡ºPlan-Then-Actionå¢å¼ºæ¨ç†ä¸ç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆPTA-GRPOï¼‰çš„ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œæ—¨åœ¨æ”¹å–„é«˜çº§è§„åˆ’ä¸ç²¾ç»†é“¾å¼æ€ç»´æ¨ç†ã€‚ç¬¬ä¸€é˜¶æ®µåˆ©ç”¨å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹æç‚¼ç´§å‡‘çš„é«˜çº§æŒ‡å¯¼ï¼›ç¬¬äºŒé˜¶æ®µå¼•å…¥æŒ‡å¯¼æ„ŸçŸ¥çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œè”åˆä¼˜åŒ–æœ€ç»ˆè¾“å‡ºä¸é«˜çº§æŒ‡å¯¼è´¨é‡ï¼Œæå‡æ¨ç†æ•ˆæœã€‚å®éªŒè¯æ˜ï¼ŒPTA-GRPOåœ¨ä¸åŒæ¨¡å‹ä¸ä»»åŠ¡ä¸Šå‡å®ç°ç¨³å®šä¸”æ˜¾è‘—çš„æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚ä»»åŠ¡ä¸­å±•ç°å‡ºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œä½†å­˜åœ¨å†—ä½™ã€ä¸è¿è´¯æˆ–é”™è¯¯çš„æ¨ç†é—®é¢˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¦‚æ ‘çŠ¶ç®—æ³•å’Œå¼ºåŒ–å­¦ä¹ å°è¯•è§£å†³æ­¤é—®é¢˜ï¼Œä½†è®¡ç®—æˆæœ¬é«˜ä¸”æ•ˆæœä¸ä½³ã€‚</li>
<li>PTA-GRPOæ¡†æ¶åˆ†ä¸ºä¸¤é˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µæç‚¼é«˜çº§æŒ‡å¯¼ï¼Œç¬¬äºŒé˜¶æ®µè”åˆä¼˜åŒ–è¾“å‡ºå’ŒæŒ‡å¯¼è´¨é‡ã€‚</li>
<li>PTA-GRPOåœ¨ä¸åŒæ¨¡å‹å’Œä»»åŠ¡ä¸Šå®ç°ç¨³å®šä¸”æ˜¾è‘—çš„æå‡ã€‚</li>
<li>è¯¥æ¡†æ¶å¯åº”ç”¨äºå¤šä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬MATHã€AIME2024ã€AIME2025å’ŒAMCã€‚</li>
<li>ä½¿ç”¨å¤šç§åŸºç¡€æ¨¡å‹è¿›è¡Œå®éªŒéªŒè¯ï¼Œå¦‚Qwen2.5-7B-Instructã€Qwen3-8Bã€Qwen3-14Bå’ŒLLaMA3.2-3Bã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.01833">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-e00a39a6637fe6e01722557f952017a3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029499&auth_key=1760029499-0-0-ac404383dfeac28cb35c1dedfeccaf0b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a4e526fc727e3bae45d18a0bfd85d7cb~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029506&auth_key=1760029506-0-0-8cb2646e40110c8ebf23592d5ef90d87&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b23159b7c5aae63d1e3c1d8d3b8a39cf~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029514&auth_key=1760029514-0-0-72f56b393d37415a06895f4f05e83c08&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="What-Matters-in-RL-Based-Methods-for-Object-Goal-Navigation-An-Empirical-Study-and-A-Unified-Framework"><a href="#What-Matters-in-RL-Based-Methods-for-Object-Goal-Navigation-An-Empirical-Study-and-A-Unified-Framework" class="headerlink" title="What Matters in RL-Based Methods for Object-Goal Navigation? An   Empirical Study and A Unified Framework"></a>What Matters in RL-Based Methods for Object-Goal Navigation? An   Empirical Study and A Unified Framework</h2><p><strong>Authors:Hongze Wang, Boyang Sun, Jiaxu Xing, Fan Yang, Marco Hutter, Dhruv Shah, Davide Scaramuzza, Marc Pollefeys</strong></p>
<p>Object-Goal Navigation (ObjectNav) is a critical component toward deploying mobile robots in everyday, uncontrolled environments such as homes, schools, and workplaces. In this context, a robot must locate target objects in previously unseen environments using only its onboard perception. Success requires the integration of semantic understanding, spatial reasoning, and long-horizon planning, which is a combination that remains extremely challenging. While reinforcement learning (RL) has become the dominant paradigm, progress has spanned a wide range of design choices, yet the field still lacks a unifying analysis to determine which components truly drive performance. In this work, we conduct a large-scale empirical study of modular RL-based ObjectNav systems, decomposing them into three key components: perception, policy, and test-time enhancement. Through extensive controlled experiments, we isolate the contribution of each and uncover clear trends: perception quality and test-time strategies are decisive drivers of performance, whereas policy improvements with current methods yield only marginal gains. Building on these insights, we propose practical design guidelines and demonstrate an enhanced modular system that surpasses State-of-the-Art (SotA) methods by 6.6% on SPL and by a 2.7% success rate. We also introduce a human baseline under identical conditions, where experts achieve an average 98% success, underscoring the gap between RL agents and human-level navigation. Our study not only sets the SotA performance but also provides principled guidance for future ObjectNav development and evaluation. </p>
<blockquote>
<p>å¯¹è±¡ç›®æ ‡å¯¼èˆªï¼ˆObjectNavï¼‰æ˜¯éƒ¨ç½²ç§»åŠ¨æœºå™¨äººåœ¨å®¶åº­ã€å­¦æ ¡å’Œå·¥ä½œç¯å¢ƒç­‰æ—¥å¸¸éå—æ§ç¯å¢ƒä¸­çš„å…³é”®ç»„ä»¶ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæœºå™¨äººå¿…é¡»ä»…ä½¿ç”¨å…¶å†…ç½®æ„ŸçŸ¥ç³»ç»Ÿåœ¨ä¹‹å‰æœªè§è¿‡çš„ç¯å¢ƒä¸­å®šä½ç›®æ ‡å¯¹è±¡ã€‚æˆåŠŸéœ€è¦å°†è¯­ä¹‰ç†è§£ã€ç©ºé—´æ¨ç†å’Œé•¿æœŸè§„åˆ’æ•´åˆåœ¨ä¸€èµ·ï¼Œè¿™æ˜¯ä¸€ç§æå…·æŒ‘æˆ˜æ€§çš„ç»„åˆã€‚è™½ç„¶å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å·²æˆä¸ºä¸»æµèŒƒå¼ï¼Œä¸”è¿›å±•æ¶µç›–äº†å¹¿æ³›çš„è®¾è®¡é€‰æ‹©ï¼Œä½†è¯¥é¢†åŸŸä»ç¼ºä¹ç»Ÿä¸€çš„åˆ†ææ¥ç¡®å®šå“ªäº›ç»„ä»¶çœŸæ­£æ¨åŠ¨äº†æ€§èƒ½ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å¯¹åŸºäºæ¨¡å—åŒ–RLçš„ObjectNavç³»ç»Ÿè¿›è¡Œäº†å¤§è§„æ¨¡å®è¯ç ”ç©¶ï¼Œå°†å…¶åˆ†è§£ä¸ºä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šæ„ŸçŸ¥ã€ç­–ç•¥å’Œæµ‹è¯•æ—¶é—´å¢å¼ºã€‚é€šè¿‡å¹¿æ³›çš„å—æ§å®éªŒï¼Œæˆ‘ä»¬éš”ç¦»äº†æ¯ä¸ªç»„ä»¶çš„è´¡çŒ®å¹¶å‘ç°äº†æ˜ç¡®è¶‹åŠ¿ï¼šæ„ŸçŸ¥è´¨é‡å’Œæµ‹è¯•æ—¶é—´ç­–ç•¥æ˜¯å†³å®šæ€§èƒ½çš„å…³é”®å› ç´ ï¼Œè€Œå½“å‰æ–¹æ³•çš„ç­–ç•¥æ”¹è¿›åªäº§ç”Ÿå¾®å°çš„æ”¶ç›Šã€‚åŸºäºè¿™äº›è§è§£ï¼Œæˆ‘ä»¬æå‡ºäº†å®ç”¨çš„è®¾è®¡æŒ‡å—ï¼Œå¹¶å±•ç¤ºäº†ä¸€ä¸ªå¢å¼ºçš„æ¨¡å—åŒ–ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿåœ¨SPLä¸Šè¶…è¶Šäº†æœ€æ–°æ–¹æ³•6.6%ï¼ŒæˆåŠŸç‡æé«˜äº†2.7%ã€‚æˆ‘ä»¬è¿˜ä»‹ç»äº†åœ¨ç›¸åŒæ¡ä»¶ä¸‹çš„äººç±»åŸºå‡†çº¿ï¼Œä¸“å®¶å¹³å‡æˆåŠŸç‡ä¸º98%ï¼Œå¼ºè°ƒäº†RLä»£ç†å’Œäººç±»æ°´å¹³å¯¼èˆªä¹‹é—´çš„å·®è·ã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ä»…è¾¾åˆ°äº†æœ€æ–°æ€§èƒ½æ°´å¹³ï¼Œè€Œä¸”è¿˜ä¸ºæœªæ¥ObjectNavçš„å‘å±•å’Œè¯„ä¼°æä¾›äº†åŸåˆ™æ€§çš„æŒ‡å¯¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.01830v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¯¹è±¡ç›®æ ‡å¯¼èˆªï¼ˆObjectNavï¼‰æ˜¯éƒ¨ç½²ç§»åŠ¨æœºå™¨äººåœ¨å®¶åº­ã€å­¦æ ¡å’Œå·¥ä½œç¯å¢ƒç­‰æ—¥å¸¸éå—æ§ç¯å¢ƒä¸­çš„å…³é”®ç»„ä»¶ã€‚æœºå™¨äººéœ€ä»…ä¾é è‡ªèº«æ„ŸçŸ¥åœ¨æœªçŸ¥ç¯å¢ƒä¸­å®šä½ç›®æ ‡å¯¹è±¡ã€‚æˆåŠŸéœ€è¦æ•´åˆè¯­ä¹‰ç†è§£ã€ç©ºé—´æ¨ç†å’Œé•¿æœŸè§„åˆ’ï¼Œè¿™æ˜¯ä¸€é¡¹æå…·æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚è™½ç„¶å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å·²æˆä¸ºä¸»æµèŒƒå¼ï¼Œä¸”è®¾è®¡é€‰æ‹©èŒƒå›´å¹¿æ³›ï¼Œä½†é¢†åŸŸä»ç¼ºä¹ç»Ÿä¸€åˆ†ææ¥ç¡®å®šçœŸæ­£é©±åŠ¨æ€§èƒ½çš„ç»„ä»¶ã€‚åœ¨ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å¯¹æ¨¡å—åŒ–RL-based ObjectNavç³»ç»Ÿè¿›è¡Œäº†å¤§è§„æ¨¡å®è¯ç ”ç©¶ï¼Œå°†å…¶åˆ†è§£ä¸ºæ„ŸçŸ¥ã€æ”¿ç­–å’Œæµ‹è¯•æ—¶é—´å¢å¼ºä¸‰ä¸ªå…³é”®ç»„ä»¶ã€‚é€šè¿‡å¹¿æ³›çš„å—æ§å®éªŒï¼Œæˆ‘ä»¬ç¡®å®šäº†æ¯ä¸ªç»„ä»¶çš„è´¡çŒ®å¹¶å‘ç°äº†æ¸…æ™°è¶‹åŠ¿ï¼šæ„ŸçŸ¥è´¨é‡å’Œæµ‹è¯•æ—¶é—´ç­–ç•¥æ˜¯å†³å®šæ€§èƒ½çš„å…³é”®å› ç´ ï¼Œè€Œæ”¿ç­–æ”¹è¿›åœ¨å½“å‰æ–¹æ³•ä¸­åªäº§ç”Ÿå¾®å°æ”¶ç›Šã€‚åŸºäºè¿™äº›è§è§£ï¼Œæˆ‘ä»¬æå‡ºäº†å®ç”¨çš„è®¾è®¡æŒ‡å—ï¼Œå¹¶å±•ç¤ºäº†ä¸€ä¸ªå¢å¼ºå‹æ¨¡å—åŒ–ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿåœ¨æˆåŠŸç‡ä¸Šè¶…è¶Šç°æœ‰æŠ€æœ¯çŠ¶æ€ï¼ˆSotAï¼‰æ–¹æ³•6.6%ï¼Œè·¯å¾„æ•ˆç‡æé«˜2.7%ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ç›¸åŒæ¡ä»¶ä¸‹çš„ä¸€ä¸ªäººåŸºå‡†çº¿ï¼Œä¸“å®¶å¹³å‡æˆåŠŸç‡è¾¾åˆ°98%ï¼Œçªæ˜¾å‡ºå¼ºåŒ–å­¦ä¹ ä»£ç†ä¸äººç±»æ°´å¹³çš„å¯¼èˆªä¹‹é—´çš„å·®è·ã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ä»…è®¾å®šäº†å½“å‰æŠ€æœ¯æ°´å¹³ï¼Œè¿˜ä¸ºæœªæ¥çš„ObjectNavå‘å±•å’Œè¯„ä¼°æä¾›äº†åŸåˆ™æ€§æŒ‡å¯¼ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ObjectNavæ˜¯ç§»åŠ¨æœºå™¨äººåœ¨æ—¥å¸¸éå—æ§ç¯å¢ƒä¸­éƒ¨ç½²çš„å…³é”®ç»„æˆéƒ¨åˆ†ã€‚</li>
<li>æœºå™¨äººéœ€è¦åœ¨æœªçŸ¥ç¯å¢ƒä¸­åˆ©ç”¨è‡ªèº«æ„ŸçŸ¥å®šä½ç›®æ ‡å¯¹è±¡ã€‚</li>
<li>æˆåŠŸå®ç°ObjectNavéœ€è¦æ•´åˆè¯­ä¹‰ç†è§£ã€ç©ºé—´æ¨ç†å’Œé•¿æœŸè§„åˆ’ã€‚</li>
<li>æ„ŸçŸ¥è´¨é‡å’Œæµ‹è¯•æ—¶é—´ç­–ç•¥æ˜¯å†³å®šObjectNavæ€§èƒ½çš„å…³é”®å› ç´ ã€‚</li>
<li>æ”¿ç­–æ”¹è¿›åœ¨å½“å‰æ–¹æ³•ä¸­åªäº§ç”Ÿå¾®å°æ”¶ç›Šã€‚</li>
<li>å¢å¼ºå‹æ¨¡å—åŒ–ç³»ç»Ÿè¶…è¶Šäº†ç°æœ‰æŠ€æœ¯çŠ¶æ€ï¼ˆSotAï¼‰æ–¹æ³•ã€‚</li>
<li>ä¸äººç±»ä¸“å®¶ç›¸æ¯”ï¼Œå½“å‰å¼ºåŒ–å­¦ä¹ ä»£ç†çš„å¯¼èˆªèƒ½åŠ›ä»å­˜åœ¨å·®è·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.01830">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-1852f85ac05519fef07adf6b434fc270~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029521&auth_key=1760029521-0-0-fe503b08c0a8a86524dba200cff3bf2f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-eef51a48fb6680bb74c23201b33b5c4e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029528&auth_key=1760029528-0-0-83cb251d63b34b664703eb6ea66140e4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-165dfa1b5515ef4d9d7021723911e49d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029535&auth_key=1760029535-0-0-b4b6fffd45a4e9ba38a78fa6462d758e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-66b57213e94e68711cec66ab3fcd316e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029541&auth_key=1760029541-0-0-f3f66726d848b2bb1dbeae7798512b37&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-47ee57029850fb119f7699595025f67d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029549&auth_key=1760029549-0-0-0420d179df62b26268db0fc851485bd3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Black-Box-Combinatorial-Optimization-with-Order-Invariant-Reinforcement-Learning"><a href="#Black-Box-Combinatorial-Optimization-with-Order-Invariant-Reinforcement-Learning" class="headerlink" title="Black-Box Combinatorial Optimization with Order-Invariant Reinforcement   Learning"></a>Black-Box Combinatorial Optimization with Order-Invariant Reinforcement   Learning</h2><p><strong>Authors:Olivier Goudet, Quentin Suire, Adrien GoÃ«ffon, FrÃ©dÃ©ric Saubion, Sylvain Lamprier</strong></p>
<p>We introduce an order-invariant reinforcement learning framework for black-box combinatorial optimization. Classical estimation-of-distribution algorithms (EDAs) often rely on learning explicit variable dependency graphs, which can be costly and fail to capture complex interactions efficiently. In contrast, we parameterize a multivariate autoregressive generative model trained without a fixed variable ordering. By sampling random generation orders during training - a form of information-preserving dropout - the model is encouraged to be invariant to variable order, promoting search-space diversity and shaping the model to focus on the most relevant variable dependencies, improving sample efficiency. We adapt Generalized Reinforcement Policy Optimization (GRPO) to this setting, providing stable policy-gradient updates from scale-invariant advantages. Across a wide range of benchmark algorithms and problem instances of varying sizes, our method frequently achieves the best performance and consistently avoids catastrophic failures. </p>
<blockquote>
<p>æˆ‘ä»¬ä¸ºé»‘ç›’ç»„åˆä¼˜åŒ–å¼•å…¥äº†ä¸€ç§é¡ºåºä¸å˜å¼ºåŒ–å­¦ä¹ æ¡†æ¶ã€‚ç»å…¸çš„åˆ†å¸ƒä¼°è®¡ç®—æ³•ï¼ˆEDAï¼‰é€šå¸¸ä¾èµ–äºå­¦ä¹ æ˜ç¡®çš„å˜é‡ä¾èµ–å›¾ï¼Œè¿™å¯èƒ½ä¼šå¾ˆæ˜‚è´µï¼Œå¹¶ä¸”ä¸èƒ½æœ‰æ•ˆåœ°æ•è·å¤æ‚çš„äº¤äº’ã€‚ç›¸åï¼Œæˆ‘ä»¬å¯¹ä¸€ä¸ªå¤šå…ƒè‡ªå›å½’ç”Ÿæˆæ¨¡å‹è¿›è¡Œäº†å‚æ•°åŒ–è®­ç»ƒï¼Œè¯¥æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¸ä½¿ç”¨å›ºå®šçš„å˜é‡é¡ºåºã€‚é€šè¿‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­éšæœºé‡‡æ ·ç”Ÿæˆé¡ºåºâ€”â€”ä¸€ç§ä¿¡æ¯ä¿ç•™çš„ä¸¢å¼ƒç­–ç•¥â€”â€”æ¨¡å‹è¢«é¼“åŠ±å¯¹å˜é‡é¡ºåºä¿æŒä¸å˜ï¼Œä»è€Œä¿ƒè¿›æœç´¢ç©ºé—´çš„å¤šæ ·æ€§ï¼Œå¹¶ä½¿æ¨¡å‹ä¸“æ³¨äºæœ€ç›¸å…³çš„å˜é‡ä¾èµ–å…³ç³»ï¼Œä»è€Œæé«˜æ ·æœ¬æ•ˆç‡ã€‚æˆ‘ä»¬å°†å¹¿ä¹‰å¼ºåŒ–ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰é€‚åº”äºè¿™ç§è®¾ç½®ï¼Œé€šè¿‡å°ºåº¦ä¸å˜ä¼˜åŠ¿æä¾›ç¨³å®šçš„ç­–ç•¥æ¢¯åº¦æ›´æ–°ã€‚åœ¨å¹¿æ³›çš„åŸºå‡†ç®—æ³•å’Œå„ç§ä¸åŒè§„æ¨¡çš„å®ä¾‹é—®é¢˜ä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•é€šå¸¸å–å¾—æœ€ä½³æ€§èƒ½ï¼Œå¹¶ä¸”æŒç»­é¿å…ç¾éš¾æ€§å¤±è´¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.01824v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§é¡ºåºä¸å˜å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºé»‘ç®±ç»„åˆä¼˜åŒ–ã€‚è¯¥æ¡†æ¶é€šè¿‡å‚æ•°åŒ–å¤šå…ƒè‡ªå›å½’ç”Ÿæˆæ¨¡å‹ï¼Œé¿å…äº†ä¼ ç»Ÿä¼°ç®—åˆ†å¸ƒç®—æ³•ä¸­å¯¹å˜é‡ä¾èµ–å›¾çš„æ˜¾å¼å­¦ä¹ ï¼Œå¯ä»¥åœ¨ä¸å›ºå®šå˜é‡é¡ºåºçš„æƒ…å†µä¸‹è¿›è¡Œè®­ç»ƒã€‚é€šè¿‡è®­ç»ƒè¿‡ç¨‹ä¸­çš„éšæœºç”Ÿæˆé¡ºåºé‡‡æ ·ï¼Œæ¨¡å‹è¢«é¼“åŠ±å¯¹å˜é‡é¡ºåºä¿æŒä¸å˜ï¼Œä»è€Œä¿ƒè¿›æœç´¢ç©ºé—´å¤šæ ·æ€§ï¼Œä½¿æ¨¡å‹å…³æ³¨æœ€é‡è¦çš„å˜é‡ä¾èµ–å…³ç³»ï¼Œæé«˜æ ·æœ¬æ•ˆç‡ã€‚æœ¬ç ”ç©¶è¿˜é€‚åº”å¹¿ä¹‰å¼ºåŒ–ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œæä¾›ç¨³å®šçš„ç­–ç•¥æ¢¯åº¦æ›´æ–°å’Œè§„æ¨¡ä¸å˜ä¼˜åŠ¿ã€‚åœ¨å¹¿æ³›çš„åŸºå‡†ç®—æ³•å’Œè§„æ¨¡ä¸åŒçš„é—®é¢˜å®ä¾‹ä¸­ï¼Œè¯¥æ–¹æ³•ç»å¸¸è¾¾åˆ°æœ€ä½³æ€§èƒ½ï¼Œå¹¶å§‹ç»ˆé¿å…ç¾éš¾æ€§å¤±è´¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„é¡ºåºä¸å˜å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºé»‘ç®±ç»„åˆä¼˜åŒ–é—®é¢˜çš„æ±‚è§£ã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡å¤šå…ƒè‡ªå›å½’ç”Ÿæˆæ¨¡å‹è¿›è¡Œå‚æ•°åŒ–ï¼Œæ— éœ€æ˜¾å¼å­¦ä¹ å˜é‡ä¾èµ–å›¾ã€‚</li>
<li>æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é€šè¿‡éšæœºç”Ÿæˆé¡ºåºé‡‡æ ·ï¼Œå®ç°å¯¹å˜é‡é¡ºåºçš„ä¸å˜æ€§ï¼Œæé«˜æœç´¢ç©ºé—´å¤šæ ·æ€§å’Œæ ·æœ¬æ•ˆç‡ã€‚</li>
<li>ç ”ç©¶æ”¹è¿›äº†å¹¿ä¹‰å¼ºåŒ–ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œæä¾›ç¨³å®šçš„ç­–ç•¥æ¢¯åº¦æ›´æ–°å’Œè§„æ¨¡ä¸å˜ä¼˜åŠ¿ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å¹¿æ³›çš„åŸºå‡†ç®—æ³•ä¸­è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ï¼Œç»å¸¸è¾¾åˆ°æœ€ä½³æ•ˆæœã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½å¤Ÿé¿å…ç¾éš¾æ€§å¤±è´¥ï¼Œæ˜¾ç¤ºå‡ºå…¶ç¨³å®šæ€§å’Œå¯é æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.01824">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-45ec48d870419a811106a110f0adfffa~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029557&auth_key=1760029557-0-0-c244a6ddd8104c9adfc3e771e57bf789&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Quagmires-in-SFT-RL-Post-Training-When-High-SFT-Scores-Mislead-and-What-to-Use-Instead"><a href="#Quagmires-in-SFT-RL-Post-Training-When-High-SFT-Scores-Mislead-and-What-to-Use-Instead" class="headerlink" title="Quagmires in SFT-RL Post-Training: When High SFT Scores Mislead and What   to Use Instead"></a>Quagmires in SFT-RL Post-Training: When High SFT Scores Mislead and What   to Use Instead</h2><p><strong>Authors:Feiyang Kang, Michael Kuchnik, Karthik Padthe, Marin Vlastelica, Ruoxi Jia, Carole-Jean Wu, Newsha Ardalani</strong></p>
<p>In post-training for reasoning Large Language Models (LLMs), the current state of practice trains LLMs in two independent stages: Supervised Fine-Tuning (SFT) and Reinforcement Learning with Verifiable Rewards (RLVR, shortened as &#96;&#96;RLâ€™â€™ below). In this work, we challenge whether high SFT scores translate to improved performance after RL. We provide extensive counter-examples where this is not true. We find high SFT scores can be biased toward simpler or more homogeneous data and are not reliably predictive of subsequent RL gains or scaled-up post-training effectiveness. In some cases, RL training on models with improved SFT performance could lead to substantially worse outcome compared to RL on the base model without SFT. We study alternative metrics and identify generalization loss on held-out reasoning examples and Pass@large k performance to provide strong proxies for the RL outcome. We trained hundreds of models up to 12B-parameter with SFT and RLVR via GRPO and ran extensive evaluations on 7 math benchmarks with up to 256 repetitions, spending $&gt;$1M GPU hours. Experiments include models from Llama3, Mistral-Nemo, Qwen3 and multiple state-of-the-art SFT&#x2F;RL datasets. Compared to directly predicting from pre-RL performance, prediction based on generalization loss and Pass@large k achieves substantial higher precision, improving $R^2$ coefficient and Spearmanâ€™s rank correlation coefficient by up to 0.5 (2x). This provides strong utility for broad use cases. For example, in most experiments, we find SFT training on unique examples for a one epoch underperforms training on half examples for two epochs, either after SFT or SFT-then-RL; With the same SFT budget, training only on short examples may lead to better SFT performance, though, it often leads to worse outcome after RL compared to training on examples with varying lengths. Evaluation tool will be open-sourced. </p>
<blockquote>
<p>åœ¨é’ˆå¯¹æ¨ç†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œåè®­ç»ƒçš„è¿‡ç¨‹ä¸­ï¼Œç›®å‰çš„å®è·µæ˜¯å°†LLMåˆ†ä¸ºä¸¤ä¸ªç‹¬ç«‹é˜¶æ®µè¿›è¡Œè®­ç»ƒï¼šç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¯éªŒè¯å¥–åŠ±å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼Œä»¥ä¸‹ç®€ç§°â€œRLâ€ï¼‰ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è´¨ç–‘é«˜SFTåˆ†æ•°æ˜¯å¦èƒ½åœ¨RLåè½¬åŒ–ä¸ºæ›´å¥½çš„æ€§èƒ½ã€‚æˆ‘ä»¬æä¾›äº†å¤§é‡åä¾‹æ¥è¯æ˜è¿™ä¸€ç‚¹ã€‚æˆ‘ä»¬å‘ç°é«˜SFTåˆ†æ•°å¯èƒ½åå‘äºæ›´ç®€å•æˆ–æ›´åŒè´¨çš„æ•°æ®ï¼Œå¹¶ä¸èƒ½å¯é åœ°é¢„æµ‹éšåçš„RLæ”¶ç›Šæˆ–æ‰©å¤§åçš„åè®­ç»ƒæ•ˆæœã€‚åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œå¯¹å…·æœ‰æ”¹è¿›SFTæ€§èƒ½çš„æ¨¡å‹è¿›è¡ŒRLè®­ç»ƒå¯èƒ½ä¼šä¸åœ¨åŸºç¡€æ¨¡å‹ä¸Šè¿›è¡Œæ— SFTçš„RLç›¸æ¯”äº§ç”Ÿæ›´ç³Ÿç³•çš„ç»“æœã€‚æˆ‘ä»¬ç ”ç©¶äº†å…¶ä»–æŒ‡æ ‡ï¼Œå¹¶ç¡®å®šäº†åœ¨ä¿ç•™å‡ºæ¥çš„æ¨ç†ç¤ºä¾‹ä¸Šçš„æ³›åŒ–æŸå¤±å’ŒPass@large kæ€§èƒ½ï¼Œä»¥ä½œä¸ºRLç»“æœçš„å¼ºçƒˆä»£ç†ã€‚æˆ‘ä»¬ä½¿ç”¨SFTå’ŒRLVRé€šè¿‡GRPOè®­ç»ƒäº†æ•°ç™¾ä¸ªé«˜è¾¾12Bå‚æ•°çš„æ¨¡å‹ï¼Œå¹¶åœ¨7ä¸ªæ•°å­¦åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„è¯„ä¼°ï¼Œæ¯ä¸ªæµ‹è¯•æœ€å¤šé‡å¤äº†256æ¬¡ï¼Œè€—èµ„è¶…è¿‡100ä¸‡ç¾å…ƒçš„GPUå°æ—¶ã€‚å®éªŒåŒ…æ‹¬æ¥è‡ªLlama3ã€Mistral-Nemoã€Qwen3çš„æ¨¡å‹ä»¥åŠå¤šä¸ªæœ€å…ˆè¿›çš„SFT&#x2F;RLæ•°æ®é›†ã€‚ä¸ç›´æ¥ä»é¢„RLæ€§èƒ½è¿›è¡Œé¢„æµ‹ç›¸æ¯”ï¼ŒåŸºäºæ³›åŒ–æŸå¤±å’ŒPass@large kçš„é¢„æµ‹å…·æœ‰æ›´é«˜çš„ç²¾åº¦ï¼Œå°†R^2ç³»æ•°å’Œæ–¯çš®å°”æ›¼ç§©ç›¸å…³ç³»æ•°æé«˜äº†é«˜è¾¾0.5ï¼ˆä¸¤å€ï¼‰ã€‚è¿™ä¸ºå¹¿æ³›çš„åº”ç”¨åœºæ™¯æä¾›äº†å¼ºå¤§çš„å®ç”¨æ€§ã€‚ä¾‹å¦‚ï¼Œåœ¨å¤§å¤šæ•°å®éªŒä¸­ï¼Œæˆ‘ä»¬å‘ç°ä½¿ç”¨ç‹¬ç‰¹ç¤ºä¾‹è¿›è¡Œä¸ºæœŸä¸€ä¸ªå›åˆçš„SFTè®­ç»ƒçš„è¡¨ç°ä¸å¦‚ä½¿ç”¨åŠä¾‹è¿›è¡Œä¸ºæœŸä¸¤ä¸ªå›åˆçš„è®­ç»ƒï¼Œæ— è®ºæ˜¯åœ¨SFTä¹‹åè¿˜æ˜¯SFT-then-RLï¼›åœ¨ç›¸åŒçš„SFTé¢„ç®—ä¸‹ï¼Œä»…å¯¹çŸ­ä¾‹è¿›è¡Œè®­ç»ƒå¯èƒ½ä¼šå¯¼è‡´æ›´å¥½çš„SFTæ€§èƒ½ï¼Œç„¶è€Œï¼Œä¸è®­ç»ƒä¸åŒé•¿åº¦çš„ä¾‹å­ç›¸æ¯”ï¼Œè¿™å¾€å¾€ä¼šå¯¼è‡´RLä¹‹åçš„ç»“æœæ›´ç³Ÿã€‚è¯„ä¼°å·¥å…·å°†å¼€æºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.01624v1">PDF</a> Preprint. Under Review</p>
<p><strong>Summary</strong></p>
<p>åŸºäºè®­ç»ƒåçš„æ¨ç†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œç›®å‰å®è·µä¸­å­˜åœ¨ä¸¤ä¸ªé˜¶æ®µï¼šç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å­¦ä¹ å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰ã€‚æœ¬æ–‡æŒ‘æˆ˜äº†é«˜SFTåˆ†æ•°æ˜¯å¦ç­‰åŒäºRLåçš„æ€§èƒ½æå‡ã€‚æˆ‘ä»¬æä¾›äº†å¤§é‡åä¾‹æ¥è¯æ˜è¿™ä¸€ç‚¹ä¸æˆç«‹ã€‚ç ”ç©¶å‘ç°ï¼Œé«˜SFTåˆ†æ•°å¯èƒ½åå‘äºæ›´ç®€å•æˆ–æ›´å‡åŒ€çš„æ•°æ®ï¼Œå¹¶ä¸èƒ½å¯é åœ°é¢„æµ‹åç»­çš„RLæ”¶ç›Šæˆ–æ‰©å±•åçš„è®­ç»ƒåæ•ˆæœã€‚å› æ­¤ï¼Œå¯»æ‰¾èƒ½å¤Ÿé¢„æµ‹RLç»“æœçš„æ›¿ä»£æŒ‡æ ‡å°¤ä¸ºé‡è¦ã€‚æœ¬ç ”ç©¶è¯†åˆ«å‡ºæ³›åŒ–æŸå¤±å’ŒPass@large kæ€§èƒ½æ˜¯é¢„æµ‹RLç»“æœçš„å¼ºä»£ç†æŒ‡æ ‡ã€‚ç»è¿‡å¹¿æ³›çš„å®éªŒéªŒè¯ï¼Œä¸ç›´æ¥åŸºäºé¢„RLæ€§èƒ½é¢„æµ‹ç›¸æ¯”ï¼ŒåŸºäºæ³›åŒ–æŸå¤±å’ŒPass@large kçš„é¢„æµ‹ç²¾åº¦æ›´é«˜ï¼Œæé«˜äº†RÂ²ç³»æ•°å’Œæ–¯çš®å°”æ›¼ç­‰çº§ç›¸å…³ç³»æ•°ï¼Œæ˜¾ç¤ºå‡ºå¼ºå¤§çš„å®ç”¨ä»·å€¼ã€‚ä¾‹å¦‚ï¼Œåœ¨å¤§å¤šæ•°å®éªŒä¸­ï¼Œæˆ‘ä»¬å‘ç°ä½¿ç”¨ç‹¬ç‰¹ä¾‹å­è¿›è¡ŒSFTè®­ç»ƒä¸€ä¸ªå‘¨æœŸçš„è¡¨ç°ä¸å¦‚ä½¿ç”¨åŠä¾‹è¿›è¡Œä¸¤ä¸ªå‘¨æœŸçš„è®­ç»ƒï¼›åœ¨ç›¸åŒçš„SFTé¢„ç®—ä¸‹ï¼Œåªè®­ç»ƒçŸ­ä¾‹å­å¯èƒ½ä¼šå¯¼è‡´æ›´å¥½çš„SFTè¡¨ç°ï¼Œä½†åœ¨RLåå¾€å¾€è¡¨ç°è¾ƒå·®ã€‚è¯„ä¼°å·¥å…·å°†å¼€æºæä¾›ã€‚æ€»çš„æ¥è¯´ï¼Œæœ¬ç ”ç©¶æä¾›äº†é‡è¦çš„è§è§£å’Œæ”¹è¿›å®è·µæŒ‡å¯¼ã€‚åœ¨åŸºäºé«˜å¼ºåº¦è®­ç»ƒå’Œè¯„ä¼°çš„ç¯å¢ƒä¼˜åŒ–æ¨ç†æ¨¡å‹çš„åœºæ™¯ä¸‹æœ‰ç€æå¤§çš„åº”ç”¨æ½œåŠ›ã€‚ä¸­æ–‡æ¦‚è¿°ç»“æŸã€‚å…·ä½“è§è§£åˆ†æè§ä¸‹æ–‡çš„Key Takeawayséƒ¨åˆ†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯æœ¬æ–‡æœ¬ä¸­çš„å…³é”®æ´å¯Ÿç‚¹ï¼š</p>
<ul>
<li>åœ¨LLMçš„è®­ç»ƒåæ¨ç†é˜¶æ®µä¸­ï¼Œç›®å‰å®è·µå­˜åœ¨ä¸¤ä¸ªé˜¶æ®µï¼šç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å­¦ä¹ å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰ã€‚æœ¬ç ”ç©¶å¯¹å…¶å…³ç³»æå‡ºäº†è´¨ç–‘å’ŒæŒ‘æˆ˜ã€‚</li>
<li>å‘ç°é«˜SFTåˆ†æ•°ä¸èƒ½å¯é é¢„æµ‹RLä¹‹åçš„æ€§èƒ½æå‡ï¼Œå¹¶æå‡ºè¿™ä¸€è¯„ä»·å¯èƒ½ä¼šåå‘äºæ›´ç®€å•æˆ–æ›´å‡åŒ€çš„æ•°æ®ç¯å¢ƒã€‚è¿™ä¸€è§‚å¯Ÿå¸¦æ¥äº†å¯¹ç°æœ‰è¯„ä¼°æŒ‡æ ‡çš„åæ€å’Œæ”¹è¿›éœ€æ±‚ã€‚</li>
<li>ç ”ç©¶ç¡®å®šäº†æ–°çš„è¯„ä¼°æŒ‡æ ‡â€”â€”æ³›åŒ–æŸå¤±å’ŒPass@large kæ€§èƒ½ï¼Œè¿™ä¸¤ä¸ªæŒ‡æ ‡ä½œä¸ºé¢„æµ‹RLç»“æœçš„å¼ºä»£ç†ï¼Œç›¸è¾ƒäºä¼ ç»Ÿçš„é¢„RLæ€§èƒ½é¢„æµ‹æ›´ä¸ºå‡†ç¡®æœ‰æ•ˆã€‚</li>
<li>é€šè¿‡å¹¿æ³›çš„å®éªŒéªŒè¯ï¼ŒåŒ…æ‹¬ä½¿ç”¨å¤šç§æ¨¡å‹å’Œæ•°æ®é›†ï¼Œå‘ç°ä½¿ç”¨ç‹¬ç‰¹ä¾‹å­è¿›è¡ŒSFTè®­ç»ƒä¸ä¸€å®šä¼˜äºå…¶ä»–è®­ç»ƒç­–ç•¥ï¼Œå¹¶ä¸”éœ€è¦è°¨æ…é€‰æ‹©è®­ç»ƒæ•°æ®é‡å’Œç±»å‹ä»¥è¾¾åˆ°æœ€ä½³æ•ˆæœã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.01624">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-63ae7b6ee03dbfa2b2acbdd981c58429~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029565&auth_key=1760029565-0-0-44ca84dab00df90997cf8c3f4a80fb0a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-68ddf95faa7da3c75503349c1f964ec8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029573&auth_key=1760029573-0-0-c45a982f8051b61a0e3d01833ee9ab85&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-dbabe63888b254dc1ce41e6edb5cc48b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029579&auth_key=1760029579-0-0-2ddc4cfd0636aead0f88a9f0444ecadd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ba65d62ec76d7ac5ba68b881e8251651~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029586&auth_key=1760029586-0-0-c0be5c0b64695cd9aca0df3d139f33d6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="VLA-R1-Enhancing-Reasoning-in-Vision-Language-Action-Models"><a href="#VLA-R1-Enhancing-Reasoning-in-Vision-Language-Action-Models" class="headerlink" title="VLA-R1: Enhancing Reasoning in Vision-Language-Action Models"></a>VLA-R1: Enhancing Reasoning in Vision-Language-Action Models</h2><p><strong>Authors:Angen Ye, Zeyu Zhang, Boyuan Wang, Xiaofeng Wang, Dapeng Zhang, Zheng Zhu</strong></p>
<p>Vision-Language-Action (VLA) models aim to unify perception, language understanding, and action generation, offering strong cross-task and cross-scene generalization with broad impact on embodied AI. However, current VLA models often lack explicit step-by-step reasoning, instead emitting final actions without considering affordance constraints or geometric relations. Their post-training pipelines also rarely reinforce reasoning quality, relying primarily on supervised fine-tuning with weak reward design. To address these challenges, we present VLA-R1, a reasoning-enhanced VLA that integrates Reinforcement Learning from Verifiable Rewards (RLVR) with Group Relative Policy Optimization (GRPO) to systematically optimize both reasoning and execution. Specifically, we design an RLVR-based post-training strategy with verifiable rewards for region alignment, trajectory consistency, and output formatting, thereby strengthening reasoning robustness and execution accuracy. Moreover, we develop VLA-CoT-13K, a high-quality dataset that provides chain-of-thought supervision explicitly aligned with affordance and trajectory annotations. Furthermore, extensive evaluations on in-domain, out-of-domain, simulation, and real-robot platforms demonstrate that VLA-R1 achieves superior generalization and real-world performance compared to prior VLA methods. We plan to release the model, code, and dataset following the publication of this work. Code: <a target="_blank" rel="noopener" href="https://github.com/GigaAI-research/VLA-R1">https://github.com/GigaAI-research/VLA-R1</a>. Website: <a target="_blank" rel="noopener" href="https://gigaai-research.github.io/VLA-R1">https://gigaai-research.github.io/VLA-R1</a>. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹æ—¨åœ¨ç»Ÿä¸€æ„ŸçŸ¥ã€è¯­è¨€ç†è§£å’ŒåŠ¨ä½œç”Ÿæˆï¼Œæä¾›å¼ºå¤§çš„è·¨ä»»åŠ¡å’Œè·¨åœºæ™¯æ³›åŒ–èƒ½åŠ›ï¼Œå¯¹åµŒå…¥å¼äººå·¥æ™ºèƒ½æœ‰å¹¿æ³›å½±å“ã€‚ç„¶è€Œï¼Œå½“å‰çš„VLAæ¨¡å‹é€šå¸¸ç¼ºä¹æ˜ç¡®çš„é€æ­¥æ¨ç†ï¼Œè€Œæ˜¯å‘å‡ºæœ€ç»ˆåŠ¨ä½œï¼Œè€Œæ²¡æœ‰è€ƒè™‘å¯åŠæ€§çº¦æŸæˆ–å‡ ä½•å…³ç³»ã€‚ä»–ä»¬çš„åè®­ç»ƒç®¡é“ä¹Ÿå¾ˆå°‘åŠ å¼ºæ¨ç†è´¨é‡ï¼Œä¸»è¦ä¾èµ–äºç›‘ç£å¾®è°ƒä»¥åŠè®¾è®¡è¾ƒå¼±çš„å¥–åŠ±ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†å¢å¼ºæ¨ç†çš„VLA-R1ï¼Œå®ƒç»“åˆäº†å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰å’Œç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œä»¥ç³»ç»Ÿåœ°ä¼˜åŒ–æ¨ç†å’Œæ‰§è¡Œã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§åŸºäºRLVRçš„åè®­ç»ƒç­–ç•¥ï¼Œé€šè¿‡å¯éªŒè¯çš„å¥–åŠ±æ¥è¿›è¡ŒåŒºåŸŸå¯¹é½ã€è½¨è¿¹ä¸€è‡´æ€§å’Œè¾“å‡ºæ ¼å¼åŒ–ï¼Œä»è€Œå¢å¼ºæ¨ç†çš„ç¨³å¥æ€§å’Œæ‰§è¡Œçš„å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼€å‘äº†VLA-CoT-13Kè¿™ä¸€é«˜è´¨é‡æ•°æ®é›†ï¼Œå®ƒæä¾›äº†ä¸å¯åŠæ€§å’Œè½¨è¿¹æ³¨é‡Šæ˜ç¡®å¯¹é½çš„æ€ç»´é“¾ç›‘ç£ã€‚æ­¤å¤–ï¼Œé’ˆå¯¹é¢†åŸŸå†…ã€é¢†åŸŸå¤–ã€æ¨¡æ‹Ÿå’ŒçœŸå®æœºå™¨äººå¹³å°çš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼Œä¸å…ˆå‰çš„VLAæ–¹æ³•ç›¸æ¯”ï¼ŒVLA-R1å®ç°äº†æ›´å¥½çš„æ³›åŒ–å’Œç°å®ä¸–ç•Œæ€§èƒ½ã€‚æˆ‘ä»¬è®¡åˆ’åœ¨ä½œå“å‘è¡¨åå…¬å¼€æ¨¡å‹ã€ä»£ç å’Œæ•°æ®é›†ã€‚ä»£ç ï¼š<a target="_blank" rel="noopener" href="https://github.com/GigaAI-research/VLA-R1%E3%80%82%E7%BD%91%E7%AB%99%EF%BC%9Ahttps://gigaai-research.github.io/VLA-R1%E3%80%82">https://github.com/GigaAI-research/VLA-R1ã€‚ç½‘ç«™ï¼šhttps://gigaai-research.github.io/VLA-R1ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.01623v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†VLA-R1æ¨¡å‹ï¼Œå®ƒæ˜¯ä¸€ä¸ªå¢å¼ºæ¨ç†çš„Vision-Language-Actionï¼ˆVLAï¼‰æ¨¡å‹ã€‚è¯¥æ¨¡å‹é€šè¿‡ç»“åˆå¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰å’Œç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰æ¥ä¼˜åŒ–æ¨ç†å’Œæ‰§è¡Œã€‚è®¾è®¡äº†ä¸€ç§åŸºäºRLVRçš„åè®­ç»ƒç­–ç•¥ï¼Œé€šè¿‡å¯éªŒè¯å¥–åŠ±å¼ºåŒ–åŒºåŸŸå¯¹é½ã€è½¨è¿¹ä¸€è‡´æ€§å’Œè¾“å‡ºæ ¼å¼ï¼Œæé«˜æ¨ç†çš„é²æ£’æ€§å’Œæ‰§è¡Œçš„å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œè¿˜å¼€å‘äº†VLA-CoT-13Kæ•°æ®é›†ï¼Œæä¾›ä¸è´Ÿæ‹…èƒ½åŠ›å’Œè½¨è¿¹æ³¨é‡Šæ˜ç¡®å¯¹é½çš„æ€ç»´é“¾ç›‘ç£ã€‚è¯„ä¼°è¡¨æ˜ï¼ŒVLA-R1åœ¨åŸŸå†…ã€åŸŸå¤–ã€æ¨¡æ‹Ÿå’ŒçœŸå®æœºå™¨äººå¹³å°ä¸Šçš„æ³›åŒ–èƒ½åŠ›å’Œå®é™…æ€§èƒ½ä¼˜äºå…ˆå‰çš„VLAæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VLA-R1æ˜¯ä¸€ä¸ªå¢å¼ºæ¨ç†çš„Vision-Language-Actionï¼ˆVLAï¼‰æ¨¡å‹ï¼Œæ—¨åœ¨ç»Ÿä¸€æ„ŸçŸ¥ã€è¯­è¨€ç†è§£å’Œè¡ŒåŠ¨ç”Ÿæˆã€‚</li>
<li>VLA-R1é€šè¿‡ç»“åˆå¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰å’Œç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰æ¥ä¼˜åŒ–æ¨ç†å’Œæ‰§è¡Œã€‚</li>
<li>RLVRåŸºäºçš„åè®­ç»ƒç­–ç•¥é€šè¿‡å¯éªŒè¯å¥–åŠ±å¼ºåŒ–åŒºåŸŸå¯¹é½ã€è½¨è¿¹ä¸€è‡´æ€§å’Œè¾“å‡ºæ ¼å¼ã€‚</li>
<li>VLA-R1æé«˜äº†æ¨ç†çš„é²æ£’æ€§å’Œæ‰§è¡Œçš„å‡†ç¡®æ€§ã€‚</li>
<li>å¼€å‘äº†VLA-CoT-13Kæ•°æ®é›†ï¼Œæä¾›ä¸è´Ÿæ‹…èƒ½åŠ›å’Œè½¨è¿¹æ³¨é‡Šæ˜ç¡®å¯¹é½çš„æ€ç»´é“¾ç›‘ç£ã€‚</li>
<li>VLA-R1åœ¨å¤šä¸ªè¯„ä¼°å¹³å°ä¸Šè¡¨ç°å‡ºä¼˜äºå…ˆå‰VLAæ–¹æ³•çš„æ³›åŒ–èƒ½åŠ›å’Œå®é™…æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.01623">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-3add4837770b42411a8ddc68df0cd0be~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029593&auth_key=1760029593-0-0-180d921fc22b0a6e26633837146c1f24&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e944f3affdc56f5081abb410561df104~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029601&auth_key=1760029601-0-0-b195ab4b9500f14c9ad7fefdf1bfe002&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d21f22e31501b2dd6e650162b5a955e9~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029607&auth_key=1760029607-0-0-6fe9069bef4aee82033aca0b0ac80786&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-68c4a5daa18888a9cb66c56a14966578~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029614&auth_key=1760029614-0-0-939e24b65d666bdf6bae24ca1cc16020&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f3c68f66a90afd12abbb9b8000611f3a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029621&auth_key=1760029621-0-0-e6ef21172ab0cbddb7824b1d79c321ac&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-10a1f4a97d78677bc4e6bdb0f17b131d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029628&auth_key=1760029628-0-0-7fa15890b35f01b4c8832d1e47cecaed&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Executable-Counterfactuals-Improving-LLMsâ€™-Causal-Reasoning-Through-Code"><a href="#Executable-Counterfactuals-Improving-LLMsâ€™-Causal-Reasoning-Through-Code" class="headerlink" title="Executable Counterfactuals: Improving LLMsâ€™ Causal Reasoning Through   Code"></a>Executable Counterfactuals: Improving LLMsâ€™ Causal Reasoning Through   Code</h2><p><strong>Authors:Aniket Vashishtha, Qirun Dai, Hongyuan Mei, Amit Sharma, Chenhao Tan, Hao Peng</strong></p>
<p>Counterfactual reasoning, a hallmark of intelligence, consists of three steps: inferring latent variables from observations (abduction), constructing alternatives (interventions), and predicting their outcomes (prediction). This skill is essential for advancing LLMsâ€™ causal understanding and expanding their applications in high-stakes domains such as scientific research. However, existing efforts in assessing LLMâ€™s counterfactual reasoning capabilities tend to skip the abduction step, effectively reducing to interventional reasoning and leading to overestimation of LLM performance. To address this, we introduce executable counterfactuals, a novel framework that operationalizes causal reasoning through code and math problems. Our framework explicitly requires all three steps of counterfactual reasoning and enables scalable synthetic data creation with varying difficulty, creating a frontier for evaluating and improving LLMâ€™s reasoning. Our results reveal substantial drop in accuracy (25-40%) from interventional to counterfactual reasoning for SOTA models like o4-mini and Claude-4-Sonnet. To address this gap, we construct a training set comprising counterfactual code problems having if-else condition and test on out-of-domain code structures (e.g. having while-loop); we also test whether a model trained on code would generalize to counterfactual math word problems. While supervised finetuning on stronger modelsâ€™ reasoning traces improves in-domain performance of Qwen models, it leads to a decrease in accuracy on OOD tasks such as counterfactual math problems. In contrast, reinforcement learning induces the core cognitive behaviors and generalizes to new domains, yielding gains over the base model on both code (improvement of 1.5x-2x) and math problems. Analysis of the reasoning traces reinforces these findings and highlights the promise of RL for improving LLMsâ€™ counterfactual reasoning. </p>
<blockquote>
<p>åäº‹å®æ¨ç†æ˜¯æ™ºåŠ›çš„æ ‡å¿—ï¼ŒåŒ…å«ä¸‰ä¸ªæ­¥éª¤ï¼šä»è§‚å¯Ÿä¸­æ¨æ–­æ½œåœ¨å˜é‡ï¼ˆå‡è®¾ï¼‰ã€æ„å»ºæ›¿ä»£æ–¹æ¡ˆï¼ˆå¹²é¢„ï¼‰ï¼Œä»¥åŠé¢„æµ‹å…¶ç»“æœï¼ˆé¢„æµ‹ï¼‰ã€‚å¯¹äºæ¨è¿›å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å› æœç†è§£å¹¶åœ¨ç§‘å­¦ç ”ç©¶ç­‰é«˜é£é™©é¢†åŸŸæ‰©å¤§å…¶åº”ç”¨ï¼Œè¿™é¡¹æŠ€èƒ½è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰è¯„ä¼°LLMåäº‹å®æ¨ç†èƒ½åŠ›çš„æ–¹æ³•å¾€å¾€çœç•¥äº†å‡è®¾è¿™ä¸€æ­¥ï¼Œå®é™…ä¸Šç®€åŒ–ä¸ºå¹²é¢„æ¨ç†ï¼Œå¯¼è‡´å¯¹LLMæ€§èƒ½çš„è¿‡åº¦ä¼°è®¡ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¯æ‰§è¡Œçš„åäº‹å®ï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡ä»£ç å’Œæ•°å­¦é—®é¢˜æ“ä½œåŒ–å› æœæ¨ç†çš„æ–°æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ¡†æ¶æ˜ç¡®è¦æ±‚åäº‹å®æ¨ç†çš„æ‰€æœ‰ä¸‰ä¸ªæ­¥éª¤ï¼Œå¹¶èƒ½å¤Ÿåˆ›å»ºä¸åŒéš¾åº¦çš„å¤§è§„æ¨¡åˆæˆæ•°æ®ï¼Œä¸ºè¯„ä¼°å’Œæé«˜LLMçš„æ¨ç†èƒ½åŠ›å¼€è¾Ÿå‰æ²¿ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œä»å¹²é¢„æ¨ç†åˆ°åäº‹å®æ¨ç†ï¼ŒSOTAæ¨¡å‹ï¼ˆå¦‚o4-miniå’ŒClaude-4-Sonnetï¼‰çš„å‡†ç¡®åº¦å¤§å¹…ä¸‹é™ï¼ˆ25-40%ï¼‰ã€‚ä¸ºè§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªè®­ç»ƒé›†ï¼Œå…¶ä¸­åŒ…å«å…·æœ‰if-elseæ¡ä»¶çš„åäº‹å®ä»£ç é—®é¢˜ï¼Œå¹¶åœ¨è¶…å‡ºåŸŸçš„ä»£ç ç»“æ„ï¼ˆä¾‹å¦‚åŒ…å«while-loopçš„ï¼‰ä¸Šè¿›è¡Œæµ‹è¯•ï¼›æˆ‘ä»¬è¿˜æµ‹è¯•äº†æ¨¡å‹åœ¨ä»£ç ä¸Šè®­ç»ƒåæ˜¯å¦é€‚ç”¨äºåäº‹å®æ•°å­¦æ–‡å­—é—®é¢˜ã€‚è™½ç„¶å¯¹æ›´å¼ºæ¨¡å‹æ¨ç†è½¨è¿¹çš„ç›‘ç£å¾®è°ƒæé«˜äº†é¢†åŸŸå†…çš„æ€§èƒ½ï¼Œä½†å®ƒå¯¼è‡´åœ¨åŸŸå¤–ä»»åŠ¡ï¼ˆå¦‚åäº‹å®æ•°å­¦é—®é¢˜ï¼‰ä¸Šçš„å‡†ç¡®æ€§ä¸‹é™ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå¼ºåŒ–å­¦ä¹ å¼•å‘äº†æ ¸å¿ƒè®¤çŸ¥è¡Œä¸ºå¹¶æ¨å¹¿åˆ°æ–°çš„é¢†åŸŸï¼Œåœ¨ä»£ç ï¼ˆæ”¹è¿›1.5å€è‡³ä¸¤å€ï¼‰å’Œæ•°å­¦é—®é¢˜ä¸Šéƒ½å®ç°äº†å¯¹åŸºç¡€æ¨¡å‹çš„æ”¶ç›Šã€‚å¯¹æ¨ç†è½¨è¿¹çš„åˆ†æè¯å®äº†è¿™äº›å‘ç°ï¼Œå¹¶çªå‡ºäº†å¼ºåŒ–å­¦ä¹ åœ¨æ”¹å–„LLMåäº‹å®æ¨ç†æ–¹é¢çš„å‰æ™¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.01539v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å› æœæ¨ç†ä¸­çš„åäº‹å®æ¨ç†çš„ä¸‰ä¸ªæ­¥éª¤ï¼šä»è§‚å¯Ÿä¸­æ¨æ–­æ½œåœ¨å˜é‡ï¼ˆé€†å‘æ¨ç†ï¼‰ã€æ„å»ºæ›¿ä»£æ–¹æ¡ˆï¼ˆå¹²é¢„ï¼‰ï¼Œä»¥åŠé¢„æµ‹å…¶åæœã€‚æ–‡ç« æŒ‡å‡ºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„åäº‹å®æ¨ç†èƒ½åŠ›æ—¶ï¼Œå¸¸å¸¸å¿½ç•¥é€†å‘æ¨ç†æ­¥éª¤ï¼Œå¯¼è‡´å¯¹LLMæ€§èƒ½çš„è¿‡åº¦ä¼°è®¡ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œä½œè€…å¼•å…¥äº†å¯æ‰§è¡Œçš„åäº‹å®æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡ä»£ç å’Œæ•°å­¦é—®é¢˜æ“ä½œåŒ–å› æœæ¨ç†ï¼Œæ˜ç¡®è¦æ±‚åäº‹å®æ¨ç†çš„ä¸‰ä¸ªæ­¥éª¤ï¼Œå¹¶èƒ½åˆ›å»ºä¸åŒéš¾åº¦çš„åˆæˆæ•°æ®ï¼Œä¸ºè¯„ä¼°å’Œæ”¹è¿›LLMçš„æ¨ç†èƒ½åŠ›æä¾›äº†å‰æ²¿ã€‚ç ”ç©¶å‘ç°ï¼Œä»å¹²é¢„æ€§æ¨ç†è½¬å‘åäº‹å®æ¨ç†æ—¶ï¼Œå‡†ç¡®ç‡å¤§å¹…ä¸‹é™ã€‚ä¸ºè§£å†³è¿™ä¸€å·®è·ï¼Œä½œè€…æ„å»ºäº†ä¸€ä¸ªè®­ç»ƒé›†ï¼ŒåŒ…å«å…·æœ‰if-elseæ¡ä»¶çš„åäº‹å®ä»£ç é—®é¢˜ï¼Œå¹¶åœ¨éåŸŸä»£ç ç»“æ„ä¸Šè¿›è¡Œäº†æµ‹è¯•ã€‚åŒæ—¶ï¼Œæ–‡ç« æ¢è®¨äº†å¼ºåŒ–å­¦ä¹ åœ¨æ”¹å–„LLMåäº‹å®æ¨ç†ä¸­çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åäº‹å®æ¨ç†åŒ…å«ä¸‰ä¸ªå…³é”®æ­¥éª¤ï¼šé€†å‘æ¨ç†ã€æ„å»ºæ›¿ä»£æ–¹æ¡ˆå’Œé¢„æµ‹åæœã€‚</li>
<li>è¯„ä¼°LLMçš„åäº‹å®æ¨ç†èƒ½åŠ›æ—¶å¸¸å¸¸å¿½ç•¥é€†å‘æ¨ç†æ­¥éª¤ï¼Œå¯¼è‡´æ€§èƒ½è¯„ä¼°çš„è¿‡åº¦ä¹è§‚ã€‚</li>
<li>å¯æ‰§è¡Œçš„åäº‹å®æ¡†æ¶æ“ä½œåŒ–å› æœæ¨ç†ï¼Œæ˜ç¡®è¦æ±‚ä¸Šè¿°ä¸‰ä¸ªæ­¥éª¤ï¼Œä¿ƒè¿›æ•°æ®åˆ›å»ºçš„å‰æ²¿æ€§è¯„ä¼°å’Œæ”¹è¿›LLMçš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>åäº‹å®æ¨ç†å¯¹LLMæ¥è¯´å­˜åœ¨å‡†ç¡®ç‡çš„æ˜¾è‘—ä¸‹é™ã€‚</li>
<li>æ„å»ºåŒ…å«if-elseæ¡ä»¶çš„åäº‹å®ä»£ç é—®é¢˜è®­ç»ƒé›†ç”¨äºè§£å†³è¿™ä¸€å·®è·ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ åœ¨æ”¹å–„LLMåäº‹å®æ¨ç†æ–¹é¢å…·æœ‰æ½œåŠ›ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ èƒ½å¤Ÿæé«˜æ¨¡å‹åœ¨éåŸŸä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼ŒåŒ…æ‹¬æ•°å­¦é—®é¢˜å’Œä»£ç é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.01539">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-73d297d50b868d77877e92f9b20d7384~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029635&auth_key=1760029635-0-0-2eaefb471267d24ee5fe22e0aa63b500&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-fd5d3ac3284bd8a8ddb0f695714307fe~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029642&auth_key=1760029642-0-0-04fdcc3c2d0a4b184c0e315bd6e46105&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-eed7e6bc5b661e3d4cc8aadf63708134~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029649&auth_key=1760029649-0-0-3973c18af20c40f2648c57a85bac0dbf&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="DisCo-Reinforcement-with-Diversity-Constraints-for-Multi-Human-Generation"><a href="#DisCo-Reinforcement-with-Diversity-Constraints-for-Multi-Human-Generation" class="headerlink" title="DisCo: Reinforcement with Diversity Constraints for Multi-Human   Generation"></a>DisCo: Reinforcement with Diversity Constraints for Multi-Human   Generation</h2><p><strong>Authors:Shubhankar Borse, Farzad Farhadzadeh, Munawar Hayat, Fatih Porikli</strong></p>
<p>State-of-the-art text-to-image models excel at realism but collapse on multi-human prompts - duplicating faces, merging identities, and miscounting individuals. We introduce DisCo (Reinforcement with Diversity Constraints), the first RL-based framework to directly optimize identity diversity in multi-human generation. DisCo fine-tunes flow-matching models via Group-Relative Policy Optimization (GRPO) with a compositional reward that (i) penalizes intra-image facial similarity, (ii) discourages cross-sample identity repetition, (iii) enforces accurate person counts, and (iv) preserves visual fidelity through human preference scores. A single-stage curriculum stabilizes training as complexity scales, requiring no extra annotations. On the DiverseHumans Testset, DisCo achieves 98.6 Unique Face Accuracy and near-perfect Global Identity Spread - surpassing both open-source and proprietary methods (e.g., Gemini, GPT-Image) while maintaining competitive perceptual quality. Our results establish DisCo as a scalable, annotation-free solution that resolves the long-standing identity crisis in generative models and sets a new benchmark for compositional multi-human generation. </p>
<blockquote>
<p>å½“å‰æœ€å…ˆè¿›çš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹åœ¨çœŸå®æ„Ÿæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¤šäººç‰©æç¤ºæ–¹é¢å­˜åœ¨ç¼ºé™·ï¼Œä¾‹å¦‚é‡å¤é¢å­”ã€èº«ä»½èåˆå’Œäººæ•°è®¡ç®—é”™è¯¯ã€‚æˆ‘ä»¬å¼•å…¥äº†DisCoï¼ˆé€šè¿‡å¤šæ ·æ€§çº¦æŸè¿›è¡Œå¼ºåŒ–ï¼‰ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªåŸºäºå¼ºåŒ–å­¦ä¹ çš„æ¡†æ¶ï¼Œèƒ½å¤Ÿç›´æ¥åœ¨å¤šäººç‰©ç”Ÿæˆä¸­ä¼˜åŒ–èº«ä»½å¤šæ ·æ€§ã€‚DisCoé€šè¿‡ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰å¯¹æµç¨‹åŒ¹é…æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œé‡‡ç”¨ç»„åˆå¥–åŠ±ï¼ŒåŒ…æ‹¬ï¼ˆiï¼‰æƒ©ç½šå›¾åƒå†…é¢éƒ¨ç›¸ä¼¼æ€§ï¼Œï¼ˆiiï¼‰æŠ‘åˆ¶è·¨æ ·æœ¬èº«ä»½é‡å¤ï¼Œï¼ˆiiiï¼‰å¼ºåˆ¶å‡†ç¡®çš„äººæ•°è®¡æ•°ï¼Œä»¥åŠï¼ˆivï¼‰é€šè¿‡äººç±»åå¥½åˆ†æ•°ä¿æŒè§†è§‰ä¿çœŸåº¦ã€‚éšç€å¤æ‚æ€§çš„å¢åŠ ï¼Œå•é˜¶æ®µè¯¾ç¨‹èƒ½å¤Ÿç¨³å®šè®­ç»ƒï¼Œæ— éœ€é¢å¤–çš„æ³¨é‡Šã€‚åœ¨DiverseHumansæµ‹è¯•é›†ä¸Šï¼ŒDisCoå®ç°äº†98.6%çš„ç‹¬ç‰¹é¢éƒ¨å‡†ç¡®ç‡ï¼Œå…¨çƒèº«ä»½æ‰©æ•£è¿‘ä¹å®Œç¾ï¼Œè¶…è¶Šäº†å¼€æºå’Œä¸“æœ‰æ–¹æ³•ï¼ˆä¾‹å¦‚Geminiã€GPT-Imageï¼‰ï¼ŒåŒæ—¶ä¿æŒäº†ç«äº‰æ€§çš„æ„ŸçŸ¥è´¨é‡ã€‚æˆ‘ä»¬çš„ç»“æœè¯æ˜äº†DisCoä½œä¸ºä¸€ç§å¯æ‰©å±•ã€æ— éœ€æ³¨é‡Šçš„è§£å†³æ–¹æ¡ˆï¼Œè§£å†³äº†ç”Ÿæˆæ¨¡å‹ä¸­é•¿æœŸå­˜åœ¨çš„èº«ä»½å±æœºï¼Œå¹¶ä¸ºç»„åˆå¤šäººç‰©ç”Ÿæˆæ ‘ç«‹äº†æ–°åŸºå‡†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.01399v1">PDF</a> </p>
<p><strong>Summary</strong><br>æ–‡æœ¬ä¸»è¦ä»‹ç»äº†é’ˆå¯¹å¤šäººç±»ç”Ÿæˆä»»åŠ¡ä¸­çš„èº«ä»½å¤šæ ·æ€§é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ–°æ¡†æ¶DisCoã€‚DisCoé‡‡ç”¨é›†å›¢ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰æŠ€æœ¯ï¼Œé€šè¿‡ç»„åˆå¥–åŠ±æ¥ä¼˜åŒ–èº«ä»½å¤šæ ·æ€§ï¼ŒåŒ…æ‹¬æƒ©ç½šå›¾åƒå†…éƒ¨é¢éƒ¨ç›¸ä¼¼æ€§ã€æŠ‘åˆ¶è·¨æ ·æœ¬èº«ä»½é‡å¤ã€å¼ºåˆ¶å‡†ç¡®çš„äººæ•°è®¡æ•°ä»¥åŠé€šè¿‡äººç±»åå¥½åˆ†æ•°ä¿æŒè§†è§‰ä¿çœŸåº¦ã€‚åœ¨DiverseHumansæµ‹è¯•é›†ä¸Šï¼ŒDisCoå®ç°äº†é«˜ç‹¬ç‰¹æ€§é¢éƒ¨å‡†ç¡®ç‡å’Œè¿‘å®Œç¾çš„å…¨å±€èº«ä»½åˆ†å¸ƒï¼ŒåŒæ—¶ä¿æŒäº†æ„ŸçŸ¥è´¨é‡ç«äº‰åŠ›ã€‚å®ƒä¸ºè§£å†³ç”Ÿæˆæ¨¡å‹ä¸­çš„é•¿æœŸèº«ä»½å±æœºæä¾›äº†å¯æ‰©å±•ã€æ— éœ€æ³¨é‡Šçš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶ä¸ºç»„åˆå¤šäººç±»ç”Ÿæˆä»»åŠ¡è®¾å®šäº†æ–°çš„åŸºå‡†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰å…ˆè¿›çš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹åœ¨é¢å¯¹å¤šäººç±»æç¤ºæ—¶ä¼šå‡ºç°èº«ä»½å¤šæ ·æ€§é—®é¢˜ï¼Œå¦‚é‡å¤é¢å­”ã€åˆå¹¶èº«ä»½å’Œäººæ•°è®¡ç®—é”™è¯¯ã€‚</li>
<li>DisCoæ˜¯é¦–ä¸ªé’ˆå¯¹å¤šäººç±»ç”Ÿæˆä»»åŠ¡ä¸­èº«ä»½å¤šæ ·æ€§é—®é¢˜çš„åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ¡†æ¶ã€‚</li>
<li>DisCoé€šè¿‡é›†å›¢ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰æŠ€æœ¯ï¼Œä½¿ç”¨ç»„åˆå¥–åŠ±æ¥ä¼˜åŒ–èº«ä»½å¤šæ ·æ€§ã€‚</li>
<li>DisCoçš„å¥–åŠ±åŒ…æ‹¬æƒ©ç½šå›¾åƒå†…éƒ¨é¢éƒ¨ç›¸ä¼¼æ€§ã€æŠ‘åˆ¶è·¨æ ·æœ¬èº«ä»½é‡å¤ã€å¼ºåˆ¶å‡†ç¡®çš„äººæ•°è®¡æ•°ä»¥åŠä¿æŒè§†è§‰ä¿çœŸåº¦ã€‚</li>
<li>åœ¨DiverseHumansæµ‹è¯•é›†ä¸Šï¼ŒDisCoå®ç°äº†é«˜ç‹¬ç‰¹æ€§é¢éƒ¨å‡†ç¡®ç‡å’Œå…¨å±€èº«ä»½åˆ†å¸ƒæ€§èƒ½ã€‚</li>
<li>DisCoåœ¨ä¿æŒæ„ŸçŸ¥è´¨é‡ç«äº‰åŠ›çš„åŒæ—¶ï¼Œè¶…è¶Šäº†å¼€æºå’Œä¸“æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.01399">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-c3dfdced65d0e2f19fade629e51c78a0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029656&auth_key=1760029656-0-0-0863d259819cd9e349f52716e2993c59&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f7332aed47781fc34069ca50d35c5175~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029663&auth_key=1760029663-0-0-eef155166d18162f9100c165e47c7205&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-04/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-04/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-04/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-09181bf624db85794f2aaf6d807a7161~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029671&auth_key=1760029671-0-0-1bc5f142c5a73476480fb05310bb8fed&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-04  KaVa Latent Reasoning via Compressed KV-Cache Distillation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-04
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-04/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-c7ef9389de8cd96fd29c6ffd9f4417cb~resize:0:q75.jpg?source=1f5c5e47&expiration=1760083943&auth_key=1760083943-0-0-a938e9719b773e634b3783fc0842aa78&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-04  Audio Driven Real-Time Facial Animation for Social Telepresence
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-04
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">31180k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
