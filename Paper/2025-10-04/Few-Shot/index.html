<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot 方向最新论文已更新，请持续关注 Update in 2025-10-04  Taking a SEAT Predicting Value Interpretations from Sentiment, Emotion,   Argument, and Topic Annotations">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_Few-Shot/2510.01165v1/page_1_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-10-04
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-10-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    10.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    40 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-10-04-更新"><a href="#2025-10-04-更新" class="headerlink" title="2025-10-04 更新"></a>2025-10-04 更新</h1><h2 id="Taking-a-SEAT-Predicting-Value-Interpretations-from-Sentiment-Emotion-Argument-and-Topic-Annotations"><a href="#Taking-a-SEAT-Predicting-Value-Interpretations-from-Sentiment-Emotion-Argument-and-Topic-Annotations" class="headerlink" title="Taking a SEAT: Predicting Value Interpretations from Sentiment, Emotion,   Argument, and Topic Annotations"></a>Taking a SEAT: Predicting Value Interpretations from Sentiment, Emotion,   Argument, and Topic Annotations</h2><p><strong>Authors:Adina Nicola Dobrinoiu, Ana Cristiana Marcu, Amir Homayounirad, Luciano Cavalcante Siebert, Enrico Liscio</strong></p>
<p>Our interpretation of value concepts is shaped by our sociocultural background and lived experiences, and is thus subjective. Recognizing individual value interpretations is important for developing AI systems that can align with diverse human perspectives and avoid bias toward majority viewpoints. To this end, we investigate whether a language model can predict individual value interpretations by leveraging multi-dimensional subjective annotations as a proxy for their interpretive lens. That is, we evaluate whether providing examples of how an individual annotates Sentiment, Emotion, Argument, and Topics (SEAT dimensions) helps a language model in predicting their value interpretations. Our experiment across different zero- and few-shot settings demonstrates that providing all SEAT dimensions simultaneously yields superior performance compared to individual dimensions and a baseline where no information about the individual is provided. Furthermore, individual variations across annotators highlight the importance of accounting for the incorporation of individual subjective annotators. To the best of our knowledge, this controlled setting, although small in size, is the first attempt to go beyond demographics and investigate the impact of annotation behavior on value prediction, providing a solid foundation for future large-scale validation. </p>
<blockquote>
<p>我们对价值概念的理解受到我们的社会文化背景和生活经历的影响，因此是主观的。识别个体的价值解读对于开发能够与多样化的人类视角相吻合并避免偏向多数观点的AI系统很重要。为此，我们调查语言模型是否可以利用多维主观注释作为解读透镜的代理来预测个体价值解读。也就是说，我们评估提供个人如何注释情感、情绪、观点和话题（SEAT维度）的例子是否有助于语言模型预测他们的价值解读。我们在不同的零样本和少样本设置下的实验表明，同时提供所有SEAT维度相比单独维度和一个不提供个体信息的基准线能带来更好的性能。此外，标注者之间的个体差异突显了需要考虑融入个体主观标注者的重要性。据我们所知，这个受控设置虽然规模较小，但是首次尝试超越人口统计学，研究标注行为对价值预测的影响，为未来的大规模验证提供了坚实的基础。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.01976v1">PDF</a> Accepted at VALE workshop (ECAI 2025)</p>
<p><strong>Summary</strong></p>
<p>本文探讨了个人价值观念的解读受到社会文化背景和生活经历的影响，具有主观性。为了发展能够符合多元人类视角并避免偏向主流观点的AI系统，研究是否可以通过利用多维主观注释作为个体解读的透镜来预测个体价值解读。实验表明，在不同零样本和少样本设置下，同时提供情感、情绪、论证和主题（SEAT维度）的注释信息能提高预测性能，且比仅提供个别维度或没有提供个体信息的基础模型表现更佳。研究还发现，不同注释者之间的个体差异凸显出考虑个体主观注释的重要性。该实验虽规模较小，但却是首次尝试超越人口统计学去探究注释行为对价值预测的影响，为未来大规模验证提供了坚实的基础。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>个人价值观念的解读具有主观性，受到社会文化背景和生活经历的影响。</li>
<li>通过利用多维主观注释作为个体解读价值的透镜，可以预测个体价值解读。</li>
<li>实验表明，同时提供情感、情绪、论证和主题的注释信息能提高预测性能。</li>
<li>个体差异在价值预测中很重要，需要考虑到不同注释者之间的差异性。</li>
<li>此研究是首次尝试超越人口统计学探究注释行为对价值预测的影响。</li>
<li>提供了一种基于少量数据验证的有效方法，为未来的大规模验证提供了基础。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.01976">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_Few-Shot/2510.01976v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_Few-Shot/2510.01976v1/page_5_0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="MPMAvatar-Learning-3D-Gaussian-Avatars-with-Accurate-and-Robust-Physics-Based-Dynamics"><a href="#MPMAvatar-Learning-3D-Gaussian-Avatars-with-Accurate-and-Robust-Physics-Based-Dynamics" class="headerlink" title="MPMAvatar: Learning 3D Gaussian Avatars with Accurate and Robust   Physics-Based Dynamics"></a>MPMAvatar: Learning 3D Gaussian Avatars with Accurate and Robust   Physics-Based Dynamics</h2><p><strong>Authors:Changmin Lee, Jihyun Lee, Tae-Kyun Kim</strong></p>
<p>While there has been significant progress in the field of 3D avatar creation from visual observations, modeling physically plausible dynamics of humans with loose garments remains a challenging problem. Although a few existing works address this problem by leveraging physical simulation, they suffer from limited accuracy or robustness to novel animation inputs. In this work, we present MPMAvatar, a framework for creating 3D human avatars from multi-view videos that supports highly realistic, robust animation, as well as photorealistic rendering from free viewpoints. For accurate and robust dynamics modeling, our key idea is to use a Material Point Method-based simulator, which we carefully tailor to model garments with complex deformations and contact with the underlying body by incorporating an anisotropic constitutive model and a novel collision handling algorithm. We combine this dynamics modeling scheme with our canonical avatar that can be rendered using 3D Gaussian Splatting with quasi-shadowing, enabling high-fidelity rendering for physically realistic animations. In our experiments, we demonstrate that MPMAvatar significantly outperforms the existing state-of-the-art physics-based avatar in terms of (1) dynamics modeling accuracy, (2) rendering accuracy, and (3) robustness and efficiency. Additionally, we present a novel application in which our avatar generalizes to unseen interactions in a zero-shot manner-which was not achievable with previous learning-based methods due to their limited simulation generalizability. Our project page is at: <a target="_blank" rel="noopener" href="https://kaistchangmin.github.io/MPMAvatar/">https://KAISTChangmin.github.io/MPMAvatar/</a> </p>
<blockquote>
<p>在3D阿凡达（avatar）从视觉观察进行创作的领域取得了显著进展的同时，用宽松衣物模拟人类物理上可行的动态仍然是一个具有挑战性的问题。虽然有一些现有工作通过利用物理模拟来解决这个问题，但它们在准确性和对新型动画输入的稳健性方面存在局限。在这项工作中，我们提出了MPMAvatar框架，它可以从多视角视频创建3D人类阿凡达，支持高度逼真、稳健的动画，以及从自由视角进行真实感渲染。为了进行精确而稳健的动态建模，我们的核心思想是使用基于物质点方法的模拟器，我们精心定制它来模拟衣物的复杂变形以及与底层身体的接触，通过引入一个各向异性本构模型和一种新型碰撞处理算法。我们将这种动态建模方案与我们的规范阿凡达相结合，该阿凡达可以使用带有准阴影的3D高斯贴图进行渲染，为物理真实的动画提供高保真渲染。在我们的实验中，我们证明了MPMAvatar在（1）动态建模准确性、（2）渲染准确性和（3）稳健性和效率方面显著优于现有的基于物理的阿凡达技术。此外，我们还展示了一项新颖的应用，即我们的阿凡达能够以零镜头的方式泛化到未见过的交互-这是以前基于学习的方法由于有限的模拟泛化能力而无法实现的。我们的项目页面是：<a target="_blank" rel="noopener" href="https://kaistchangmin.github.io/MPMAvatar/">https://KAISTChangmin.github.io/MPMAvatar/</a> 。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.01619v1">PDF</a> Accepted to NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了MPMAvatar框架，该框架支持从多视角视频创建3D人类角色动画，具有逼真、稳健的特点，并能从自由视角进行照片级渲染。关键思想是采用基于物质点方法的模拟器来精确模拟衣物的复杂变形以及与底层身体的接触碰撞。通过实验证明，MPMAvatar在动力学建模、渲染准确性以及稳健性和效率方面均优于现有技术。此外，还展示了其在新互动场景下的零样本泛化能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MPMAvatar框架支持从多视角视频创建高度逼真的3D人类角色动画。</li>
<li>采用物质点方法模拟器，能精确模拟衣物的复杂变形和与身体的接触碰撞。</li>
<li>MPMAvatar在动力学建模、渲染准确性以及稳健性和效率方面超越了现有技术。</li>
<li>展示了对未见互动的零样本泛化能力。</li>
<li>该框架能够处理复杂场景下的物理仿真，提高了动画的真实感和观看体验。</li>
<li>MPMAvatar通过结合物质点方法和典型角色模型，实现了物理现实主义的动画渲染。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.01619">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_Few-Shot/2510.01619v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_Few-Shot/2510.01619v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="One-More-Question-is-Enough-Expert-Question-Decomposition-EQD-Model-for-Domain-Quantitative-Reasoning"><a href="#One-More-Question-is-Enough-Expert-Question-Decomposition-EQD-Model-for-Domain-Quantitative-Reasoning" class="headerlink" title="One More Question is Enough, Expert Question Decomposition (EQD) Model   for Domain Quantitative Reasoning"></a>One More Question is Enough, Expert Question Decomposition (EQD) Model   for Domain Quantitative Reasoning</h2><p><strong>Authors:Mengyu Wang, Sotirios Sabanis, Miguel de Carvalho, Shay B. Cohen, Tiejun Ma</strong></p>
<p>Domain-specific quantitative reasoning remains a major challenge for large language models (LLMs), especially in fields requiring expert knowledge and complex question answering (QA). In this work, we propose Expert Question Decomposition (EQD), an approach designed to balance the use of domain knowledge with computational efficiency. EQD is built on a two-step fine-tuning framework and guided by a reward function that measures the effectiveness of generated sub-questions in improving QA outcomes. It requires only a few thousand training examples and a single A100 GPU for fine-tuning, with inference time comparable to zero-shot prompting. Beyond its efficiency, EQD outperforms state-of-the-art domain-tuned models and advanced prompting strategies. We evaluate EQD in the financial domain, characterized by specialized knowledge and complex quantitative reasoning, across four benchmark datasets. Our method consistently improves QA performance by 0.6% to 10.5% across different LLMs. Our analysis reveals an important insight: in domain-specific QA, a single supporting question often provides greater benefit than detailed guidance steps. </p>
<blockquote>
<p>领域特定的量化推理对于大型语言模型（LLM）来说仍然是一个主要挑战，特别是在需要专业知识和复杂问答（QA）的领域。在这项工作中，我们提出了专家问题分解（EQD）方法，旨在平衡领域知识的使用与计算效率。EQD建立在两步微调框架上，并受奖励函数的指导，该奖励函数衡量生成的子问题在提高问答结果方面的有效性。它只需要几千个训练样本和一个A100 GPU进行微调，推理时间与零样本提示相当。除了高效性外，EQD还优于最先进的领域调整模型和高级提示策略。我们在金融领域评估了EQD，该领域具有专业知识和复杂的量化推理，跨越四个基准数据集。我们的方法在不同的大型语言模型上始终提高了问答性能，幅度从0.6%到10.5%。我们的分析揭示了一个重要见解：在特定领域的问答中，一个支持问题往往比详细的指导步骤更能带来益处。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.01526v1">PDF</a> Accepted by EMNLP 2025</p>
<p><strong>Summary</strong></p>
<p>领域特定的量化推理对于大型语言模型（LLM）仍然是一个挑战，特别是在需要专业知识和复杂问答（QA）的领域。本研究提出了专家问题分解（EQD）方法，旨在平衡领域知识的使用与计算效率。EQD建立在两步微调框架上，由奖励函数引导，该奖励函数衡量生成的子问题在提高问答结果方面的有效性。它仅需要几千个训练样本和一个A100 GPU进行微调，推理时间与零样本提示相当。在财务领域，EQD的表现优于最先进的领域调整模型和高级提示策略。在四个基准数据集上的评估显示，我们的方法在跨不同LLM的QA性能方面提高了0.6%至10.5%。分析表明，在特定领域的问答中，一个支持问题往往比详细的指导步骤更能带来益处。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）在领域特定的量化推理方面存在挑战，尤其是在需要专业知识和复杂问答的领域。</li>
<li>专家问题分解（EQD）方法旨在平衡领域知识的使用与计算效率。</li>
<li>EQD通过两步微调框架和奖励函数引导，奖励函数衡量生成的子问题在提高问答结果方面的有效性。</li>
<li>EQD仅需要有限的训练样本和计算资源进行微调，并且推理时间与零样本提示相当。</li>
<li>在财务领域的评估中，EQD表现优于其他先进方法，提高了问答性能的0.6%至10.5%。</li>
<li>分析显示，在特定领域的问答中，一个关键的支持问题比详细的指导步骤更有益。</li>
<li>EQD方法为领域特定的量化推理提供了一个有效的解决方案，具有广泛的应用前景。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.01526">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_Few-Shot/2510.01526v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_Few-Shot/2510.01526v1/page_2_0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="LVTINO-LAtent-Video-consisTency-INverse-sOlver-for-High-Definition-Video-Restoration"><a href="#LVTINO-LAtent-Video-consisTency-INverse-sOlver-for-High-Definition-Video-Restoration" class="headerlink" title="LVTINO: LAtent Video consisTency INverse sOlver for High Definition   Video Restoration"></a>LVTINO: LAtent Video consisTency INverse sOlver for High Definition   Video Restoration</h2><p><strong>Authors:Alessio Spagnoletti, Andrés Almansa, Marcelo Pereyra</strong></p>
<p>Computational imaging methods increasingly rely on powerful generative diffusion models to tackle challenging image restoration tasks. In particular, state-of-the-art zero-shot image inverse solvers leverage distilled text-to-image latent diffusion models (LDMs) to achieve unprecedented accuracy and perceptual quality with high computational efficiency. However, extending these advances to high-definition video restoration remains a significant challenge, due to the need to recover fine spatial detail while capturing subtle temporal dependencies. Consequently, methods that naively apply image-based LDM priors on a frame-by-frame basis often result in temporally inconsistent reconstructions. We address this challenge by leveraging recent advances in Video Consistency Models (VCMs), which distill video latent diffusion models into fast generators that explicitly capture temporal causality. Building on this foundation, we propose LVTINO, the first zero-shot or plug-and-play inverse solver for high definition video restoration with priors encoded by VCMs. Our conditioning mechanism bypasses the need for automatic differentiation and achieves state-of-the-art video reconstruction quality with only a few neural function evaluations, while ensuring strong measurement consistency and smooth temporal transitions across frames. Extensive experiments on a diverse set of video inverse problems show significant perceptual improvements over current state-of-the-art methods that apply image LDMs frame by frame, establishing a new benchmark in both reconstruction fidelity and computational efficiency. </p>
<blockquote>
<p>计算成像方法越来越依赖于强大的生成扩散模型，以应对具有挑战性的图像恢复任务。特别是，最先进的零样本图像逆求解器利用提炼的文本到图像的潜在扩散模型（LDM）以极高的计算效率实现了前所未有的精度和感知质量。然而，将这些进展扩展到高清视频恢复仍然是一项重大挑战，因为需要在恢复精细空间细节的同时捕捉微妙的时间依赖性。因此，基于图像的方法（例如在逐帧基础上应用LDM先验的方法）常常会导致时间不一致的重构。我们通过利用视频一致性模型（VCM）的最新进展来解决这一挑战，该模型将视频潜在扩散模型提炼为快速生成器，能够显式捕获时间因果关系。在此基础上，我们提出了LVTINO，它是第一个用于高清视频恢复的零样本或即插即用逆求解器，通过VCM编码先验知识。我们的条件机制绕过自动微分，通过几次神经功能评估即可实现业界领先的视频重建质量，同时确保强大的测量一致性和跨帧的平滑时间过渡。在多种视频反问题的广泛实验表明，与当前最先进的方法相比（即在逐帧应用图像LDM的方法），感知性能得到了显著改善，在重建保真度和计算效率方面都树立了新的基准。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.01339v1">PDF</a> 23 pages, 12 figures</p>
<p><strong>Summary</strong></p>
<p>本文提出利用视频一致性模型（VCMs）蒸馏视频潜在扩散模型的方法来解决高清视频恢复问题。通过构建LVTINO模型，该模型利用VCMs编码先验知识，作为零样本或即插即用逆求解器，首次实现了高清晰度视频恢复。该方法避免了自动微分的需求，以较少的神经网络评估次数实现了领先的视频重建质量，确保了强大的测量一致性和平滑的帧间过渡。实验表明，与逐帧应用图像LDM的当前最先进方法相比，该方法在重建保真度和计算效率方面都取得了显著的感知改进。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型在处理图像恢复任务时表现出强大的生成能力。</li>
<li>高清视频恢复面临恢复精细空间细节和捕捉微妙时间依赖性的挑战。</li>
<li>单纯应用基于图像的LDM先验的帧-帧方法会导致时间上的不一致重建。</li>
<li>利用视频一致性模型（VCMs）来解决这一问题，显式捕捉时间因果关系。</li>
<li>LVTINO模型是首个利用VCMs先验知识的零样本或即插即用逆求解器，用于高清视频恢复。</li>
<li>LVTINO模型实现了领先的视频重建质量，并具有强大的测量一致性和平滑的帧间过渡。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.01339">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_Few-Shot/2510.01339v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_Few-Shot/2510.01339v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="GRAD-Generative-Retrieval-Aligned-Demonstration-Sampler-for-Efficient-Few-Shot-Reasoning"><a href="#GRAD-Generative-Retrieval-Aligned-Demonstration-Sampler-for-Efficient-Few-Shot-Reasoning" class="headerlink" title="GRAD: Generative Retrieval-Aligned Demonstration Sampler for Efficient   Few-Shot Reasoning"></a>GRAD: Generative Retrieval-Aligned Demonstration Sampler for Efficient   Few-Shot Reasoning</h2><p><strong>Authors:Oussama Gabouj, Kamel Charaf, Ivan Zakazov, Nicolas Baldwin, Robert West</strong></p>
<p>Large Language Models (LLMs) achieve strong performance across diverse tasks, but their effectiveness often depends on the quality of the provided context. Retrieval-Augmented Generation (RAG) enriches prompts with external information, but its reliance on static databases constrains adaptability and can result in irrelevant demonstrations. In this work, we propose a Generative Retrieval-Aligned Demonstrator (GRAD), a dynamic demonstration-based approach where an LLM model is trained to generate input-specific concise demonstrations. By tailoring demonstrations to each input, our method offers better contextual support than traditional RAG approaches. We demonstrate the superiority of GRAD under budget constraints, where we limit both the number of tokens used per demonstration and the number of tokens used for the final output. Trained solely on a math dataset, GRAD consistently outperforms strong baselines on Qwen2.5-14B across mathematical reasoning and advanced STEM questions, highlighting GRAD’s robust generalization to out-of-distribution (OOD) domains such as physics, chemistry, and computer science. Furthermore, we show that demonstrations generated by trained smaller models can effectively guide larger target models, reducing training costs while maintaining competitive accuracy. Overall, this work introduces a scalable demonstration generator model presenting the first step toward a dynamic few-shot learning paradigm in resource-constrained settings. We release the code used for the project. </p>
<blockquote>
<p>大型语言模型（LLM）在多种任务中表现出强大的性能，但其有效性往往取决于提供的上下文的质量。检索增强生成（RAG）通过外部信息丰富提示，但其对静态数据库的依赖限制了适应性，并可能导致出现不相关的演示。在这项工作中，我们提出了一种基于生成的检索对齐演示器（GRAD），这是一种动态演示方法，其中LLM模型经过训练以生成针对输入的简洁演示。通过为每个输入定制演示，我们的方法在提供上下文支持方面优于传统RAG方法。我们在预算约束下证明了GRAD的优越性，我们限制了每个演示和最终输出所使用的令牌数量。仅在数学数据集上进行训练，GRAD在Qwen2.5-14B上始终优于强大的基线，在数学推理和高级STEM问题方面表现突出，突显了GRAD对分布外（OOD）领域的稳健概括能力，如物理、化学和计算机科学。此外，我们还表明，由训练有素的小型模型生成的演示可以有效地指导大型目标模型，在保持竞争准确性的同时降低训练成本。总的来说，这项工作介绍了一种可扩展的演示生成器模型，朝着资源受限环境中的动态少样本学习模式迈出了第一步。我们发布了用于该项目的代码。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.01165v1">PDF</a> EMNLP 2025 (findings)</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLMs）在多种任务中表现出强大的性能，但其有效性往往取决于提供的上下文质量。检索增强生成（RAG）通过外部信息丰富提示，但其对静态数据库的依赖限制了适应性，并可能导致不相关的演示。在本文中，我们提出了一种基于生成的检索对齐演示者（GRAD）方法，这是一种动态演示方法，其中LLM模型经过训练以针对每个输入生成简洁的演示。通过为每个输入定制演示，我们的方法在上下文支持方面优于传统RAG方法。在预算有限的情况下，我们证明了GRAD的优越性，在演示和最终输出的令牌数量受到限制的情况下，仅凭数学数据集训练的GRAD在Qwen2.5-14B上始终优于强大的基线，在数学推理和高级STEM问题上表现出色。此外，我们还表明，由训练过的较小模型生成的演示可以有效地指导更大的目标模型，在保持竞争性的准确性的同时降低训练成本。总的来说，这项工作介绍了一种可扩展的演示生成器模型，朝着资源受限环境中的动态少样本学习范式迈出了第一步。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMs的效力依赖于提供的上下文质量。</li>
<li>检索增强生成（RAG）方法存在静态数据库依赖的问题。</li>
<li>GRAD是一种动态演示方法，针对每个输入生成特定的演示，提供更佳的上下文支持。</li>
<li>在预算有限和令牌数量受限的情况下，GRAD在数学数据集上的表现优于基线。</li>
<li>GRAD在跨数学推理和高级STEM问题上表现优越，尤其在物理、化学和计算机科学等OOD领域。</li>
<li>由较小模型生成的演示可以有效地指导较大的目标模型，降低训练成本。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.01165">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_Few-Shot/2510.01165v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_Few-Shot/2510.01165v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_Few-Shot/2510.01165v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="FusionAdapter-for-Few-Shot-Relation-Learning-in-Multimodal-Knowledge-Graphs"><a href="#FusionAdapter-for-Few-Shot-Relation-Learning-in-Multimodal-Knowledge-Graphs" class="headerlink" title="FusionAdapter for Few-Shot Relation Learning in Multimodal Knowledge   Graphs"></a>FusionAdapter for Few-Shot Relation Learning in Multimodal Knowledge   Graphs</h2><p><strong>Authors:Ran Liu, Yuan Fang, Xiaoli Li</strong></p>
<p>Multimodal Knowledge Graphs (MMKGs) incorporate various modalities, including text and images, to enhance entity and relation representations. Notably, different modalities for the same entity often present complementary and diverse information. However, existing MMKG methods primarily align modalities into a shared space, which tends to overlook the distinct contributions of specific modalities, limiting their performance particularly in low-resource settings. To address this challenge, we propose FusionAdapter for the learning of few-shot relationships (FSRL) in MMKG. FusionAdapter introduces (1) an adapter module that enables efficient adaptation of each modality to unseen relations and (2) a fusion strategy that integrates multimodal entity representations while preserving diverse modality-specific characteristics. By effectively adapting and fusing information from diverse modalities, FusionAdapter improves generalization to novel relations with minimal supervision. Extensive experiments on two benchmark MMKG datasets demonstrate that FusionAdapter achieves superior performance over state-of-the-art methods. </p>
<blockquote>
<p>多模态知识图谱（MMKGs）结合了多种模式，包括文本和图像，以增强实体和关系表示。值得注意的是，同一实体的不同模式通常呈现互补和多样化的信息。然而，现有的MMKG方法主要将模式对齐到共享空间，这往往忽视了特定模式的独特贡献，特别是在低资源环境中限制了其性能。为了应对这一挑战，我们为MMKG中的学习少样本关系（FSRL）提出了FusionAdapter。FusionAdapter引入了（1）一个适配器模块，使每个模式都能有效地适应未见过的关系；（2）一种融合策略，在保留多样化模式特定特性的同时，融合了多模式实体表示。通过有效地适应和融合来自多种模式的信息，FusionAdapter在少量监督下提高了对新型关系的泛化能力。在两个基准MMKG数据集上的广泛实验表明，FusionAdapter在最新技术方面实现了卓越的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.00894v1">PDF</a> Archived paper</p>
<p><strong>Summary</strong><br>    多模态知识图谱（MMKG）融合多种模态，如文本和图像，以增强实体和关系表示。现有方法主要将不同模态对齐到共享空间，这忽略了特定模态的独特贡献，特别是在资源有限的情况下限制了性能。为解决此挑战，我们提出用于多模态知识图谱中少样本关系学习的FusionAdapter方法。该方法引入适配器模块，使每个模态都能有效适应未见过的关系，并提出融合策略，整合多模态实体表示的同时保留不同模态的特定特征。通过适应和融合来自不同模态的信息，FusionAdapter在少量监督下提高了对新型关系的泛化能力。实验表明，FusionAdapter在基准MMKG数据集上的性能优于现有方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态知识图谱（MMKG）结合多种模态以增强实体和关系表示。</li>
<li>现有MMKG方法主要将不同模态对齐到共享空间，忽略了特定模态的独特贡献。</li>
<li>FusionAdapter通过引入适配器模块，使每个模态都能有效适应未见过的关系。</li>
<li>FusionAdapter提出融合策略，整合多模态实体表示的同时保留不同模态的特定特征。</li>
<li>FusionAdapter提高了对新型关系的泛化能力，特别是在资源有限的情况下。</li>
<li>实验证明，FusionAdapter在基准MMKG数据集上的性能优于现有方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.00894">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_Few-Shot/2510.00894v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_Few-Shot/2510.00894v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_Few-Shot/2510.00894v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_Few-Shot/2510.00894v1/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_Few-Shot/2510.00894v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Agent-Fine-tuning-through-Distillation-for-Domain-specific-LLMs-in-Microdomains"><a href="#Agent-Fine-tuning-through-Distillation-for-Domain-specific-LLMs-in-Microdomains" class="headerlink" title="Agent Fine-tuning through Distillation for Domain-specific LLMs in   Microdomains"></a>Agent Fine-tuning through Distillation for Domain-specific LLMs in   Microdomains</h2><p><strong>Authors:Yawen Xue, Masaya Tsunokake, Yuta Koreeda, Ekant Muljibhai Amin, Takashi Sumiyoshi, Yasuhiro Sogawa</strong></p>
<p>Agentic large language models (LLMs) have become prominent for autonomously interacting with external environments and performing multi-step reasoning tasks. Most approaches leverage these capabilities via in-context learning with few-shot prompts, but this often results in lengthy inputs and higher computational costs. Agent fine-tuning offers an alternative by enabling LLMs to internalize procedural reasoning and domain-specific knowledge through training on relevant data and demonstration trajectories. While prior studies have focused on general domains, their effectiveness in specialized technical microdomains remains unclear. This paper explores agent fine-tuning for domain adaptation within Hitachi’s JP1 middleware, a microdomain for specialized IT operations. We fine-tuned LLMs using JP1-specific datasets derived from domain manuals and distilled reasoning trajectories generated by LLMs themselves, enhancing decision making accuracy and search efficiency. During inference, we used an agentic prompt with retrieval-augmented generation and introduced a context-answer extractor to improve information relevance. On JP1 certification exam questions, our method achieved a 14% performance improvement over the base model, demonstrating the potential of agent fine-tuning for domain-specific reasoning in complex microdomains. </p>
<blockquote>
<p>大型语言模型（LLM）在自主与外部环境中交互并执行多步骤推理任务方面表现出显著优势。大多数方法通过利用上下文学习（context learning）和少量的提示（few-shot prompts）来发挥这些能力，但这通常会导致输入冗长并增加计算成本。通过针对相关数据和应用轨迹进行训练，Agent fine-tuning（模型微调）为LLM提供了一种替代方案，使其能够内化程序推理和特定领域的专业知识。虽然先前的研究主要集中在通用领域，但它们对特定技术微域的效用尚不清楚。本文探讨了针对日立JP1中间件的域适应的Agent fine-tuning方法，这是一个用于专业IT操作的微域。我们使用从域手册派生的JP1特定数据集对LLM进行微调，并使用LLM本身生成的推理轨迹进行训练，以提高决策准确性和搜索效率。在推理过程中，我们使用了带检索增强的生成式的代理提示，并引入了一个上下文答案提取器以提高信息的相关性。在JP1认证考试问题上，我们的方法相较于基础模型实现了14%的性能提升，证明了针对特定领域的复杂微域中的推理的Agent fine-tuning方法的潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.00482v1">PDF</a> Accepted by AIxB 2025</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在自主与外部交互和多步推理任务中表现突出。大多数方法通过少数示例提示进行上下文学习来利用这些能力，但这常常导致冗长的输入和更高的计算成本。Agent fine-tuning通过训练LLM在相关数据和演示轨迹上内化程序推理和领域特定知识，提供了一种替代方案。尽管先前的研究集中在一般领域，但在专业技术微领域内的有效性仍不明确。本研究探讨了针对日立公司JP1中间件的领域自适应的agent fine-tuning，这是一个用于专业IT操作的微领域。通过使用JP1特定数据集和由LLM本身生成的精炼推理轨迹对LLM进行微调，提高了决策准确性和搜索效率。在推理过程中，使用了带检索增强生成功能的agentic提示，并引入了上下文答案提取器以提高信息相关性。在JP1认证考试问题上，我们的方法相较于基础模型实现了14%的性能提升，证明了agent fine-tuning在复杂微领域中的领域特定推理潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）具备自主与外部交互及多步推理任务的能力。</li>
<li>通过少数示例提示进行上下文学习虽能利用这些能力，但会导致冗长输入和计算成本增加。</li>
<li>Agent fine-tuning使LLM能够在相关数据和演示轨迹上内化程序推理和领域知识。</li>
<li>在专业技术微领域的有效性尚未明确，本研究探索了针对JP1中间件的agent fine-tuning应用。</li>
<li>使用JP1特定数据集和LLM生成的推理轨迹微调模型，提高了决策准确性和搜索效率。</li>
<li>在推理过程中使用带检索增强生成功能的agentic提示和上下文答案提取器，增强了信息相关性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.00482">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_Few-Shot/2510.00482v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_Few-Shot/2510.00482v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_Few-Shot/2510.00482v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_Few-Shot/2510.00482v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_Few-Shot/2510.00482v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_Few-Shot/2510.00482v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="RS-OOD-A-Vision-Language-Augmented-Framework-for-Out-of-Distribution-Detection-in-Remote-Sensing"><a href="#RS-OOD-A-Vision-Language-Augmented-Framework-for-Out-of-Distribution-Detection-in-Remote-Sensing" class="headerlink" title="RS-OOD: A Vision-Language Augmented Framework for Out-of-Distribution   Detection in Remote Sensing"></a>RS-OOD: A Vision-Language Augmented Framework for Out-of-Distribution   Detection in Remote Sensing</h2><p><strong>Authors:Chenhao Wang, Yingrui Ji, Yu Meng, Yunjian Zhang, Yao Zhu</strong></p>
<p>Out-of-distribution (OOD) detection represents a critical challenge in remote sensing applications, where reliable identification of novel or anomalous patterns is essential for autonomous monitoring, disaster response, and environmental assessment. Despite remarkable progress in OOD detection for natural images, existing methods and benchmarks remain poorly suited to remote sensing imagery due to data scarcity, complex multi-scale scene structures, and pronounced distribution shifts. To this end, we propose RS-OOD, a novel framework that leverages remote sensing-specific vision-language modeling to enable robust few-shot OOD detection. Our approach introduces three key innovations: spatial feature enhancement that improved scene discrimination, a dual-prompt alignment mechanism that cross-verifies scene context against fine-grained semantics for spatial-semantic consistency, and a confidence-guided self-training loop that dynamically mines pseudo-labels to expand training data without manual annotation. RS-OOD consistently outperforms existing methods across multiple remote sensing benchmarks and enables efficient adaptation with minimal labeled data, demonstrating the critical value of spatial-semantic integration. </p>
<blockquote>
<p>在遥感应用中，异常分布（Out-of-Distribution，简称OOD）检测是一个关键挑战。在新的或异常模式的有效识别中，这对自主监控、灾害响应和环境评估至关重要。尽管在自然图像的OOD检测方面取得了显著进展，但由于数据稀缺、复杂的场景结构以及明显的分布转移等问题，现有的方法和基准测试并不适合遥感图像。为此，我们提出了RS-OOD，这是一个新的框架，它利用遥感特定的视觉语言建模来实现稳健的少量异常分布检测。我们的方法引入了三个关键的创新点：空间特征增强提高了场景鉴别能力；双提示对齐机制通过场景上下文与精细语义的交叉验证实现空间语义一致性；以及基于置信度的自训练循环可以动态挖掘伪标签以扩充训练数据，无需人工标注。在多个遥感基准测试中，RS-OOD始终优于现有方法，并且能够在最少的标记数据下实现高效适应，证明了空间语义集成的关键价值。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.02273v2">PDF</a> </p>
<p><strong>总结</strong></p>
<p>在遥感应用中，对新型或异常模式的可靠识别对于自主监控、灾害响应和环境评估至关重要，因此，新型分布外（OOD）检测是一项关键挑战。尽管在自然图像中的OOD检测取得了显著进展，但由于数据稀缺、复杂的多尺度场景结构和明显的分布偏移等问题，现有方法和基准测试对遥感图像并不适用。为此，我们提出了RS-OOD框架，利用遥感专用视觉语言建模，实现稳健的少数镜头OOD检测。我们的方法引入了三个关键创新点：空间特征增强提高了场景鉴别能力；双提示对齐机制通过场景上下文与精细语义对比验证空间语义一致性；信心引导的自我训练循环动态挖掘伪标签来扩展训练数据而无需手动注释。RS-OOD在多个遥感基准测试中均优于现有方法，并在少量标注数据的情况下实现了有效的适应，证明了空间语义整合的关键价值。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>OOD检测在遥感应用中至关重要，用于识别新型或异常模式，有助于自主监控、灾害响应和环境评估。</li>
<li>现有OOD检测方法和基准测试因数据稀缺、复杂场景结构和分布偏移等问题，在遥感图像上表现不佳。</li>
<li>RS-OOD框架利用遥感专用视觉语言建模实现稳健的少数镜头OOD检测。</li>
<li>RS-OOD引入空间特征增强，提高场景鉴别能力。</li>
<li>双提示对齐机制通过场景上下文与精细语义对比，验证空间语义一致性。</li>
<li>信心引导的自我训练循环能动态挖掘伪标签，扩展训练数据，减少对手动注释的依赖。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.02273">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_Few-Shot/2509.02273v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_Few-Shot/2509.02273v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_Few-Shot/2509.02273v2/page_3_0.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="One-Policy-to-Run-Them-All-an-End-to-end-Learning-Approach-to-Multi-Embodiment-Locomotion"><a href="#One-Policy-to-Run-Them-All-an-End-to-end-Learning-Approach-to-Multi-Embodiment-Locomotion" class="headerlink" title="One Policy to Run Them All: an End-to-end Learning Approach to   Multi-Embodiment Locomotion"></a>One Policy to Run Them All: an End-to-end Learning Approach to   Multi-Embodiment Locomotion</h2><p><strong>Authors:Nico Bohlinger, Grzegorz Czechmanowski, Maciej Krupka, Piotr Kicki, Krzysztof Walas, Jan Peters, Davide Tateo</strong></p>
<p>Deep Reinforcement Learning techniques are achieving state-of-the-art results in robust legged locomotion. While there exists a wide variety of legged platforms such as quadruped, humanoids, and hexapods, the field is still missing a single learning framework that can control all these different embodiments easily and effectively and possibly transfer, zero or few-shot, to unseen robot embodiments. We introduce URMA, the Unified Robot Morphology Architecture, to close this gap. Our framework brings the end-to-end Multi-Task Reinforcement Learning approach to the realm of legged robots, enabling the learned policy to control any type of robot morphology. The key idea of our method is to allow the network to learn an abstract locomotion controller that can be seamlessly shared between embodiments thanks to our morphology-agnostic encoders and decoders. This flexible architecture can be seen as a potential first step in building a foundation model for legged robot locomotion. Our experiments show that URMA can learn a locomotion policy on multiple embodiments that can be easily transferred to unseen robot platforms in simulation and the real world. </p>
<blockquote>
<p>深度强化学习技术正在实现最先进的稳健的腿足运动结果。尽管存在多种腿足平台，如四足、人形和六足等，但该领域仍然缺少一个单一的学习框架，能够轻松有效地控制所有这些不同的体现形式，并可能以零或少量样本的方式转移到未见过的机器人体现形式。我们引入URMA，即统一机器人形态架构，以弥补这一空白。我们的框架将端到端多任务强化学习方法引入到腿足机器人的领域，使学习到的策略能够控制任何类型的机器人形态。我们的方法的关键思想是让网络学习一个抽象的运动控制器，由于我们的形态无关编码器和解码器，该控制器可以在各种形态之间无缝共享。这种灵活架构可以被视为构建腿足机器人运动基础模型的一个潜在的第一步。我们的实验表明，URMA可以在多种形态上学习运动策略，并可以轻松将其转移到模拟和真实世界中的未知机器人平台。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.06366v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>统一机器人形态架构（URMA）的出现填补了不同形态腿式机器人学习控制的空白。URMA采用端到端多任务强化学习，使得学习到的策略可以控制任何类型的机器人形态。其关键思想在于允许网络学习一个抽象的步态控制器，借助形态无关的编码器和解码器在不同的形态之间无缝共享。该灵活的架构可被视为构建腿式机器人运动基础模型的第一步。实验表明，URMA可以在多种形态上学习步态策略，并轻松应用于仿真和现实世界中未见过的新平台。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>URMA为不同形态的腿式机器人提供了统一的学习框架。</li>
<li>该框架采用端到端的Multi-Task强化学习方法。</li>
<li>URMA允许网络学习抽象的步态控制器，适应多种机器人形态。</li>
<li>借助形态无关的编码器和解码器，URMA实现了在不同形态间的无缝共享控制策略。</li>
<li>URMA的灵活架构被视为构建腿式机器人运动基础模型的重要一步。</li>
<li>实验显示URMA在仿真和现实中均能有效应用于新平台。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.06366">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_Few-Shot/2409.06366v4/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_Few-Shot/2409.06366v4/page_3_0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Meta-Transfer-Derm-Diagnosis-Exploring-Few-Shot-Learning-and-Transfer-Learning-for-Skin-Disease-Classification-in-Long-Tail-Distribution"><a href="#Meta-Transfer-Derm-Diagnosis-Exploring-Few-Shot-Learning-and-Transfer-Learning-for-Skin-Disease-Classification-in-Long-Tail-Distribution" class="headerlink" title="Meta-Transfer Derm-Diagnosis: Exploring Few-Shot Learning and Transfer   Learning for Skin Disease Classification in Long-Tail Distribution"></a>Meta-Transfer Derm-Diagnosis: Exploring Few-Shot Learning and Transfer   Learning for Skin Disease Classification in Long-Tail Distribution</h2><p><strong>Authors:Zeynep Özdemir, Hacer Yalim Keles, Ömer Özgür Tanrıöver</strong></p>
<p>Building accurate models for rare skin diseases remains challenging due to the lack of sufficient labeled data and the inherently long-tailed distribution of available samples. These issues are further complicated by inconsistencies in how datasets are collected and their varying objectives. To address these challenges, we compare three learning strategies: episodic learning, supervised transfer learning, and contrastive self-supervised pretraining, within a few-shot learning framework. We evaluate five training setups on three benchmark datasets: ISIC2018, Derm7pt, and SD-198. Our findings show that traditional transfer learning approaches, particularly those based on MobileNetV2 and Vision Transformer (ViT) architectures, consistently outperform episodic and self-supervised methods as the number of training examples increases. When combined with batch-level data augmentation techniques such as MixUp, CutMix, and ResizeMix, these models achieve state-of-the-art performance on the SD-198 and Derm7pt datasets, and deliver highly competitive results on ISIC2018. All the source codes related to this work will be made publicly available soon at the provided URL. </p>
<blockquote>
<p>针对罕见皮肤疾病建立精确模型仍然是一个挑战，因为缺乏足够的标记数据以及可用样本的固有长尾分布。这些问题还因数据集收集方式的不一致和其不同目标而进一步复杂化。为了应对这些挑战，我们在小样本学习框架内比较了三种学习策略：情景学习、监督迁移学习和对比自监督预训练。我们在三个基准数据集（ISIC2018、Derm7pt和SD-198）上评估了五种训练设置。我们的研究结果表明，随着训练样本数量的增加，基于MobileNetV2和Vision Transformer（ViT）架构的传统迁移学习方法始终表现出色，超越了情景学习和自监督方法。结合批量级别的数据增强技术（如MixUp、CutMix和ResizeMix），这些模型在SD-198和Derm7pt数据集上达到了最先进的性能，并在ISIC2018上取得了具有竞争力的结果。与此工作相关的所有源代码很快将在提供的URL上公开发布。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.16814v2">PDF</a> This is the accepted version of the article to appear in IEEE Journal   of Biomedical and Health Informatics. DOI: 10.1109&#x2F;JBHI.2025.3615479</p>
<p><strong>Summary</strong></p>
<p>本文探讨了罕见皮肤疾病建模的挑战，包括缺乏足够的标记数据和样本分布长尾化的问题。为解决这些问题，作者在少样本学习框架下比较了三种学习策略：情景学习、监督迁移学习和对比自监督预训练。在三个基准数据集ISIC2018、Derm7pt和SD-198上进行实验，发现传统迁移学习方法，特别是基于MobileNetV2和Vision Transformer（ViT）架构的方法，在训练样本数量增加时表现最佳。结合批量级数据增强技术如MixUp、CutMix和ResizeMix，这些模型在SD-198和Derm7pt数据集上达到最新性能，并在ISIC2018上取得有竞争力的结果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>罕见皮肤疾病建模面临数据不足和样本分布长尾化挑战。</li>
<li>情景学习、监督迁移学习和对比自监督预训练是应对这些挑战的策略。</li>
<li>在基准数据集上的实验表明传统迁移学习方法表现最佳。</li>
<li>基于MobileNetV2和Vision Transformer（ViT）架构的方法表现尤其出色。</li>
<li>结合批量级数据增强技术可进一步提高模型性能。</li>
<li>模型在SD-198和Derm7pt数据集上达到最新性能，并在ISIC2018上表现有竞争力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.16814">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_Few-Shot/2404.16814v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_Few-Shot/2404.16814v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_Few-Shot/2404.16814v2/page_1_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_Few-Shot/2404.16814v2/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_Few-Shot/2404.16814v2/page_5_1.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-04/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-04/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-04/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_I2I Translation/2510.00665v2/page_4_0.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation 方向最新论文已更新，请持续关注 Update in 2025-10-04  Multi-Domain Brain Vessel Segmentation Through Feature Disentanglement
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-04
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-04/Agent/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_Agent/2510.02204v1/page_5_0.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2025-10-04  InfoMosaic-Bench Evaluating Multi-Source Information Seeking in   Tool-Augmented Agents
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-04
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">29774.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
