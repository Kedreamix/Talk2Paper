<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-04  KaVa Latent Reasoning via Compressed KV-Cache Distillation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_LLM/2510.02230v1/page_0_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-04
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-04
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    86 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-10-04-æ›´æ–°"><a href="#2025-10-04-æ›´æ–°" class="headerlink" title="2025-10-04 æ›´æ–°"></a>2025-10-04 æ›´æ–°</h1><h2 id="KaVa-Latent-Reasoning-via-Compressed-KV-Cache-Distillation"><a href="#KaVa-Latent-Reasoning-via-Compressed-KV-Cache-Distillation" class="headerlink" title="KaVa: Latent Reasoning via Compressed KV-Cache Distillation"></a>KaVa: Latent Reasoning via Compressed KV-Cache Distillation</h2><p><strong>Authors:Anna Kuzina, Maciej Pioro, Paul N. Whatmough, Babak Ehteshami Bejnordi</strong></p>
<p>Large Language Models (LLMs) excel at multi-step reasoning problems with explicit chain-of-thought (CoT), but verbose traces incur significant computational costs and memory overhead, and often carry redundant, stylistic artifacts. Latent reasoning has emerged as an efficient alternative that internalizes the thought process, but it suffers from a critical lack of supervision, limiting its effectiveness on complex, natural-language reasoning traces. In this work, we propose KaVa, the first framework that bridges this gap by distilling knowledge directly from a compressed KV-cache of the teacher into a latent-reasoning student via self-distillation, leveraging the representational flexibility of continuous latent tokens to align stepwise KV trajectories. We show that the abstract, unstructured knowledge within compressed KV-cache, which lacks direct token correspondence, can serve as a rich supervisory signal for a latent reasoning student. Empirically, the approach consistently outperforms strong latent baselines, exhibits markedly smaller degradation from equation-only to natural-language traces, and scales to larger backbones while preserving efficiency. These results establish compressed KV-cache distillation as a scalable supervision signal for latent reasoning, combining the accuracy of CoT-trained teachers with the efficiency and deployability of latent inference. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤„ç†å…·æœ‰æ˜ç¡®æ€ç»´é“¾ï¼ˆCoTï¼‰çš„å¤šæ­¥æ¨ç†é—®é¢˜æ—¶è¡¨ç°å‡ºè‰²ï¼Œä½†å†—é•¿çš„è·Ÿè¸ªä¼šäº§ç”Ÿæ˜¾è‘—çš„è®¡ç®—æˆæœ¬å’Œå†…å­˜å¼€é”€ï¼Œå¹¶ç»å¸¸å¸¦æœ‰å†—ä½™ã€é£æ ¼åŒ–çš„ç‰¹å¾ã€‚éšå¼æ¨ç†ä½œä¸ºä¸€ç§æœ‰æ•ˆçš„æ›¿ä»£æ–¹æ³•ï¼Œèƒ½å¤Ÿå†…åŒ–æ€ç»´è¿‡ç¨‹ï¼Œä½†å®ƒç¼ºä¹å…³é”®çš„ç›‘ç£ï¼Œåœ¨å¤æ‚ã€è‡ªç„¶è¯­è¨€æ¨ç†è½¨è¿¹ä¸Šçš„æ•ˆæœæœ‰é™ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†KaVaæ¡†æ¶ï¼Œå®ƒé€šè¿‡è‡ªè’¸é¦çš„æ–¹å¼ï¼Œä»å‹ç¼©çš„KVç¼“å­˜ä¸­æå–çŸ¥è¯†å¹¶ç›´æ¥ä¼ é€’ç»™è¿›è¡Œéšæ€§æ¨ç†çš„å­¦ç”Ÿæ¨¡å‹ï¼Œä»¥æ­¤å¡«è¡¥è¿™ä¸€ç©ºç™½ã€‚æˆ‘ä»¬åˆ©ç”¨è¿ç»­æ½œåœ¨ä»£å¸çš„ä»£è¡¨æ€§çµæ´»æ€§æ¥å¯¹é½é€æ­¥KVè½¨è¿¹ã€‚æˆ‘ä»¬è¯æ˜äº†å‹ç¼©KVç¼“å­˜ä¸­çš„æŠ½è±¡ã€éç»“æ„åŒ–çŸ¥è¯†å¯ä»¥ä½œä¸ºéšæ€§æ¨ç†å­¦ç”Ÿçš„ä¸°å¯Œç›‘ç£ä¿¡å·ï¼Œå³ä½¿æ²¡æœ‰ç›´æ¥çš„ä»£å¸å¯¹åº”å…³ç³»ã€‚ä»å®è¯ç»“æœæ¥çœ‹ï¼Œè¯¥æ–¹æ³•å§‹ç»ˆä¼˜äºå¼ºå¤§çš„éšæ€§åŸºå‡†æµ‹è¯•ï¼Œä»æ–¹ç¨‹å¼åˆ°è‡ªç„¶è¯­è¨€è½¨è¿¹çš„é€€åŒ–æ˜æ˜¾è¾ƒå°ï¼Œå¹¶ä¸”åœ¨ä¿æŒæ•ˆç‡çš„åŒæ—¶æ‰©å±•åˆ°æ›´å¤§çš„ä¸»å¹²ç½‘ç»œã€‚è¿™äº›ç»“æœç¡®ç«‹äº†å‹ç¼©KVç¼“å­˜è’¸é¦ä½œä¸ºéšæ€§æ¨ç†çš„å¯æ‰©å±•ç›‘ç£ä¿¡å·ï¼Œç»“åˆäº†æ•™å¸ˆCoTè®­ç»ƒçš„å‡†ç¡®æ€§å’Œå­¦ç”Ÿéšæ€§æ¨ç†çš„æ•ˆç‡ä¸å¯éƒ¨ç½²æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.02312v1">PDF</a> Preprint. Under Review</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šæ­¥æ¨ç†é—®é¢˜ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†æ˜ç¡®çš„æ€ç»´é“¾ï¼ˆCoTï¼‰è¿½è¸ªæ–¹å¼ä¼šå¸¦æ¥æ˜¾è‘—çš„è®¡ç®—æˆæœ¬å’Œå†…å­˜è´Ÿæ‹…ï¼Œå¹¶å¸¸å¸¦æœ‰å†—ä½™çš„é£æ ¼åŒ–äº§ç‰©ã€‚æ½œåœ¨æ¨ç†ä½œä¸ºä¸€ç§é«˜æ•ˆæ›¿ä»£æ–¹æ¡ˆï¼Œèƒ½å¤Ÿå†…åŒ–æ€ç»´è¿‡ç¨‹ï¼Œä½†ç¼ºä¹ç›‘ç£æ˜¯å…¶å…³é”®ç¼ºé™·ï¼Œé™åˆ¶äº†å…¶åœ¨å¤æ‚è‡ªç„¶è¯­è¨€æ¨ç†è¿½è¸ªä¸Šçš„æœ‰æ•ˆæ€§ã€‚æœ¬ç ”ç©¶æå‡ºKaVaæ¡†æ¶ï¼Œé¦–æ¬¡é€šè¿‡è‡ªæˆ‘è’¸é¦çš„æ–¹å¼ï¼Œä»æ•™å¸ˆæ¨¡å‹çš„å‹ç¼©é”®å€¼ç¼“å­˜ï¼ˆKV-cacheï¼‰ä¸­æ±²å–çŸ¥è¯†ï¼Œä¼ æˆç»™æ½œåœ¨æ¨ç†çš„å­¦ç”Ÿæ¨¡å‹ã€‚åˆ©ç”¨è¿ç»­æ½œåœ¨ç¬¦å·çš„è¡¨ç¤ºçµæ´»æ€§ï¼Œå¯¹é½é”®å€¼å¯¹çš„é€æ­¥è½¨è¿¹ã€‚ç ”ç©¶æ˜¾ç¤ºï¼Œå‹ç¼©KVç¼“å­˜ä¸­çš„æŠ½è±¡ã€éç»“æ„åŒ–çŸ¥è¯†ï¼Œå°½ç®¡ç¼ºä¹ç›´æ¥çš„ç¬¦å·å¯¹åº”å…³ç³»ï¼Œä½†å¯ä½œä¸ºæ½œåœ¨æ¨ç†å­¦ç”Ÿæ¨¡å‹çš„ä¸°å¯Œç›‘ç£ä¿¡å·ã€‚å®è¯ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¼ºåŸºçº¿æ½œåœ¨æ¨ç†æ¨¡å‹ä¸Šè¡¨ç°æ›´ä¼˜ç§€ï¼Œåœ¨è‡ªç„¶è¯­è¨€è¿½è¸ªåˆ°æ–¹ç¨‹è¿½è¸ªçš„é™è§£ç¨‹åº¦æ˜¾è‘—é™ä½ï¼Œå¹¶åœ¨å¤§å‹éª¨å¹²ç½‘ç»œä¸Šä¿æŒé«˜æ•ˆæ€§ã€‚è¿™äº›ç»“æœç¡®ç«‹äº†å‹ç¼©KVç¼“å­˜è’¸é¦ä½œä¸ºæ½œåœ¨æ¨ç†çš„å¯æ‰©å±•ç›‘ç£ä¿¡å·ï¼Œç»“åˆäº†æœ‰æ€ç»´é“¾æ•™å¸ˆæ¨¡å‹çš„å‡†ç¡®æ€§ã€æ½œåœ¨æ¨ç†çš„æ•ˆç‡å’Œå¯éƒ¨ç½²æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMså–„äºå¤„ç†å¤šæ­¥æ¨ç†é—®é¢˜ï¼Œä½†æ€ç»´é“¾è¿½è¸ªæ–¹å¼è®¡ç®—æˆæœ¬é«˜ä¸”å†—ä½™ã€‚</li>
<li>æ½œåœ¨æ¨ç†èƒ½å¤Ÿé«˜æ•ˆå¤„ç†è¿™äº›é—®é¢˜ï¼Œä½†ç¼ºä¹ç›‘ç£é™åˆ¶äº†å…¶å¤æ‚æ€§ã€‚</li>
<li>KaVaæ¡†æ¶é€šè¿‡è‡ªæˆ‘è’¸é¦çš„æ–¹å¼ï¼Œä»æ•™å¸ˆæ¨¡å‹çš„å‹ç¼©é”®å€¼ç¼“å­˜ä¸­æ±²å–çŸ¥è¯†å¹¶ä¼ æˆç»™å­¦ç”Ÿæ¨¡å‹ã€‚</li>
<li>å‹ç¼©KVç¼“å­˜ä¸­çš„çŸ¥è¯†å¯ä»¥ä½œä¸ºæ½œåœ¨æ¨ç†å­¦ç”Ÿæ¨¡å‹çš„ä¸°å¯Œç›‘ç£ä¿¡å·ã€‚</li>
<li>KaVaæ¡†æ¶åœ¨å¼ºåŸºçº¿æ½œåœ¨æ¨ç†æ¨¡å‹ä¸Šè¡¨ç°ä¼˜ç§€ï¼Œè‡ªç„¶è¯­è¨€å’Œæ–¹ç¨‹è¿½è¸ªä¹‹é—´çš„é™è§£ç¨‹åº¦é™ä½ã€‚</li>
<li>KaVaæ¡†æ¶åœ¨å¤§å‹éª¨å¹²ç½‘ç»œä¸Šä¿æŒé«˜æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.02312">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_LLM/2510.02312v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_LLM/2510.02312v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_LLM/2510.02312v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_LLM/2510.02312v1/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_LLM/2510.02312v1/page_5_0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="F2LLM-Technical-Report-Matching-SOTA-Embedding-Performance-with-6-Million-Open-Source-Data"><a href="#F2LLM-Technical-Report-Matching-SOTA-Embedding-Performance-with-6-Million-Open-Source-Data" class="headerlink" title="F2LLM Technical Report: Matching SOTA Embedding Performance with 6   Million Open-Source Data"></a>F2LLM Technical Report: Matching SOTA Embedding Performance with 6   Million Open-Source Data</h2><p><strong>Authors:Ziyin Zhang, Zihan Liao, Hang Yu, Peng Di, Rui Wang</strong></p>
<p>We introduce F2LLM - Foundation to Feature Large Language Models, a suite of state-of-the-art embedding models in three sizes: 0.6B, 1.7B, and 4B. Unlike previous top-ranking embedding models that require massive contrastive pretraining, sophisticated training pipelines, and costly synthetic training data, F2LLM is directly finetuned from foundation models on 6 million query-document-negative tuples curated from open-source, non-synthetic datasets, striking a strong balance between training cost, model size, and embedding performance. On the MTEB English leaderboard, F2LLM-4B ranks 2nd among models with approximately 4B parameters and 7th overall, while F2LLM-1.7B ranks 1st among models in the 1B-2B size range. To facilitate future research in the field, we release the models, training dataset, and code, positioning F2LLM as a strong, reproducible, and budget-friendly baseline for future works. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†F2LLMâ€”â€”ä»åŸºç¡€åˆ°ç‰¹æ€§å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼Œè¿™æ˜¯ä¸€å¥—æœ€å…ˆè¿›çš„åµŒå…¥æ¨¡å‹ï¼ŒåŒ…æ‹¬ä¸‰ç§è§„æ¨¡ï¼š0.6Bã€1.7Bå’Œ4Bã€‚ä¸åŒäºä¹‹å‰æ’åé å‰çš„åµŒå…¥æ¨¡å‹ï¼Œéœ€è¦å¤§è§„æ¨¡å¯¹æ¯”é¢„è®­ç»ƒã€å¤æ‚çš„è®­ç»ƒç®¡é“å’Œæ˜‚è´µçš„åˆæˆè®­ç»ƒæ•°æ®ï¼ŒF2LLMç›´æ¥ä»åŸºç¡€æ¨¡å‹ä¸Šè°ƒæ•´å‚æ•°è®­ç»ƒè€Œæˆï¼Œæ‰€ç”¨æ•°æ®æ˜¯ç²¾å¿ƒæŒ‘é€‰è‡ªå¼€æºéåˆæˆæ•°æ®é›†ä¸­çš„6ç™¾ä¸‡æŸ¥è¯¢æ–‡æ¡£å¦å®šå…ƒç»„ï¼Œåœ¨è®­ç»ƒæˆæœ¬ã€æ¨¡å‹å¤§å°å’ŒåµŒå…¥æ€§èƒ½ä¹‹é—´å–å¾—äº†å¾ˆå¥½çš„å¹³è¡¡ã€‚åœ¨MTEBè‹±è¯­æ’è¡Œæ¦œä¸Šï¼ŒF2LLM-4Båœ¨å‚æ•°çº¦ä¸º4Bçš„æ¨¡å‹ä¸­æ’åç¬¬2ï¼Œæ€»ä½“æ’åç¬¬7ï¼Œè€ŒF2LLM-1.7Båœ¨1B-2Bå¤§å°èŒƒå›´å†…çš„æ¨¡å‹ä¸­æ’åç¬¬1ã€‚ä¸ºäº†ä¿ƒè¿›è¯¥é¢†åŸŸæœªæ¥çš„ç ”ç©¶ï¼Œæˆ‘ä»¬å‘å¸ƒäº†æ¨¡å‹ã€è®­ç»ƒæ•°æ®é›†å’Œä»£ç ï¼Œå°†F2LLMå®šä½ä¸ºæœªæ¥å·¥ä½œçš„å¼ºå¤§ã€å¯é‡å¤ã€ç»æµå®æƒ çš„åŸºçº¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.02294v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>F2LLMæ˜¯ä¸€å¥—å…ˆè¿›çš„åµŒå…¥æ¨¡å‹å¥—ä»¶ï¼ŒåŒ…æ‹¬ä¸‰ç§è§„æ¨¡ï¼š0.6Bã€1.7Bå’Œ4Bã€‚ä¸å…¶ä»–éœ€è¦å¤§è§„æ¨¡å¯¹æ¯”é¢„è®­ç»ƒã€å¤æ‚è®­ç»ƒç®¡é“å’Œæ˜‚è´µåˆæˆè®­ç»ƒæ•°æ®çš„é¡¶çº§åµŒå…¥æ¨¡å‹ä¸åŒï¼ŒF2LLMç›´æ¥ä»åŸºç¡€æ¨¡å‹å¯¹ç²¾é€‰çš„å¼€æºéåˆæˆæ•°æ®é›†è¿›è¡Œå¾®è°ƒï¼Œåœ¨è®­ç»ƒæˆæœ¬ã€æ¨¡å‹å¤§å°å’ŒåµŒå…¥æ€§èƒ½ä¹‹é—´å–å¾—äº†å¹³è¡¡ã€‚F2LLMåœ¨MTEBè‹±è¯­æ’è¡Œæ¦œä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå…¶ä¸­F2LLM-4Båœ¨çº¦4Bå‚æ•°çš„æ¨¡å‹ä¸­æ’åç¬¬äºŒï¼Œæ€»ä½“æ’åç¬¬ä¸ƒï¼›F2LLM-1.7Båœ¨1B-2Bè§„æ¨¡èŒƒå›´å†…æ’åç¬¬ä¸€ã€‚ä¸ºä¾¿äºæœªæ¥ç ”ç©¶ï¼Œæˆ‘ä»¬å…¬å¼€äº†æ¨¡å‹ã€è®­ç»ƒæ•°æ®é›†å’Œä»£ç ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>F2LLMæ˜¯ä¸€ä¸ªåŒ…å«ä¸‰ç§è§„æ¨¡çš„å…ˆè¿›åµŒå…¥æ¨¡å‹å¥—ä»¶ã€‚</li>
<li>ä¸å…¶ä»–åµŒå…¥æ¨¡å‹ä¸åŒï¼ŒF2LLMé‡‡ç”¨ç›´æ¥ä»åŸºç¡€æ¨¡å‹å¾®è°ƒçš„æ–¹æ³•ï¼Œä½¿ç”¨ç²¾é€‰çš„å¼€æºéåˆæˆæ•°æ®é›†ã€‚</li>
<li>F2LLMåœ¨è®­ç»ƒæˆæœ¬ã€æ¨¡å‹å¤§å°å’ŒåµŒå…¥æ€§èƒ½ä¹‹é—´å–å¾—äº†å¹³è¡¡ã€‚</li>
<li>F2LLM-4Båœ¨MTEBè‹±è¯­æ’è¡Œæ¦œä¸Šæ’åç¬¬äºŒï¼Œæ€»ä½“æ’åç¬¬ä¸ƒã€‚</li>
<li>F2LLM-1.7Båœ¨ç‰¹å®šè§„æ¨¡èŒƒå›´å†…çš„æ¨¡å‹ä¸­æ’åç¬¬ä¸€ã€‚</li>
<li>F2LLMçš„å…¬å¼€åŒ…æ‹¬æ¨¡å‹ã€è®­ç»ƒæ•°æ®é›†å’Œä»£ç ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æ–¹ä¾¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.02294">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_LLM/2510.02294v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_LLM/2510.02294v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_LLM/2510.02294v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Tree-based-Dialogue-Reinforced-Policy-Optimization-for-Red-Teaming-Attacks"><a href="#Tree-based-Dialogue-Reinforced-Policy-Optimization-for-Red-Teaming-Attacks" class="headerlink" title="Tree-based Dialogue Reinforced Policy Optimization for Red-Teaming   Attacks"></a>Tree-based Dialogue Reinforced Policy Optimization for Red-Teaming   Attacks</h2><p><strong>Authors:Ruohao Guo, Afshin Oroojlooy, Roshan Sridhar, Miguel Ballesteros, Alan Ritter, Dan Roth</strong></p>
<p>Despite recent rapid progress in AI safety, current large language models remain vulnerable to adversarial attacks in multi-turn interaction settings, where attackers strategically adapt their prompts across conversation turns and pose a more critical yet realistic challenge. Existing approaches that discover safety vulnerabilities either rely on manual red-teaming with human experts or employ automated methods using pre-defined templates and human-curated attack data, with most focusing on single-turn attacks. However, these methods did not explore the vast space of possible multi-turn attacks, failing to consider novel attack trajectories that emerge from complex dialogue dynamics and strategic conversation planning. This gap is particularly critical given recent findings that LLMs exhibit significantly higher vulnerability to multi-turn attacks compared to single-turn attacks. We propose DialTree-RPO, an on-policy reinforcement learning framework integrated with tree search that autonomously discovers diverse multi-turn attack strategies by treating the dialogue as a sequential decision-making problem, enabling systematic exploration without manually curated data. Through extensive experiments, our approach not only achieves more than 25.9% higher ASR across 10 target models compared to previous state-of-the-art approaches, but also effectively uncovers new attack strategies by learning optimal dialogue policies that maximize attack success across multiple turns. </p>
<blockquote>
<p>å°½ç®¡äººå·¥æ™ºèƒ½å®‰å…¨æ€§é¢†åŸŸæœ€è¿‘å–å¾—äº†å¿«é€Ÿè¿›å±•ï¼Œä½†å½“å‰çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šè½®äº¤äº’ç¯å¢ƒä¸­ä»ç„¶å®¹æ˜“å—åˆ°å¯¹æŠ—æ€§æ”»å‡»çš„å¨èƒã€‚åœ¨è¿™ä¸ªç¯å¢ƒä¸­ï¼Œæ”»å‡»è€…ä¼šåœ¨å¯¹è¯è¿‡ç¨‹ä¸­æœ‰é’ˆå¯¹æ€§åœ°è°ƒæ•´ä»–ä»¬çš„æç¤ºï¼Œä»è€Œæ„æˆæ›´åŠ ç°å®ä¸”å…·æŒ‘æˆ˜æ€§çš„å¨èƒã€‚ç°æœ‰çš„å‘ç°å®‰å…¨æ¼æ´çš„æ–¹æ³•è¦ä¹ˆä¾èµ–äºä¸äººç±»ä¸“å®¶çš„æ‰‹åŠ¨çº¢é˜Ÿæµ‹è¯•ï¼Œè¦ä¹ˆä½¿ç”¨é¢„å®šä¹‰æ¨¡æ¿å’Œäººç±»æ•´ç†çš„æ”»å‡»æ•°æ®è¿›è¡Œè‡ªåŠ¨åŒ–æ–¹æ³•å¤„ç†ï¼Œå…¶ä¸­å¤§å¤šæ•°æ–¹æ³•ä¸»è¦å…³æ³¨å•è½®æ”»å‡»ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å¹¶æœªæ¢ç´¢å¤šè½®æ”»å‡»çš„å·¨å¤§å¯èƒ½ç©ºé—´ï¼Œæœªèƒ½è€ƒè™‘åˆ°ç”±å¤æ‚å¯¹è¯åŠ¨æ€å’Œç­–ç•¥æ€§å¯¹è¯è§„åˆ’æ‰€äº§ç”Ÿçš„æ–°å…´æ”»å‡»è½¨è¿¹ã€‚è€ƒè™‘åˆ°æœ€è¿‘çš„å‘ç°è¡¨æ˜ï¼Œä¸å•è½®æ”»å‡»ç›¸æ¯”ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šè½®æ”»å‡»ä¸­è¡¨ç°å‡ºæ›´é«˜çš„è„†å¼±æ€§ï¼Œè¿™ä¸€å·®è·å°¤ä¸ºé‡è¦ã€‚æˆ‘ä»¬æå‡ºäº†DialTree-RPOï¼Œè¿™æ˜¯ä¸€ä¸ªä¸æ ‘æœç´¢ç›¸ç»“åˆçš„ç­–ç•¥å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œèƒ½å¤Ÿè‡ªä¸»åœ°å‘ç°å¤šæ ·åŒ–çš„å¤šè½®æ”»å‡»ç­–ç•¥ï¼Œé€šè¿‡å°†å¯¹è¯è§†ä¸ºä¸€ä¸ªåºåˆ—å†³ç­–é—®é¢˜ï¼Œåœ¨ä¸ä¾èµ–æ‰‹åŠ¨æ•´ç†æ•°æ®çš„æƒ…å†µä¸‹è¿›è¡Œç³»ç»Ÿæ¢ç´¢ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…ä¸ä»¥å‰çš„æœ€å…ˆè¿›æ–¹æ³•ç›¸æ¯”ï¼Œåœ¨10ä¸ªç›®æ ‡æ¨¡å‹ä¸Šçš„æ”»å‡»æˆåŠŸç‡æé«˜äº†è¶…è¿‡2.5%ï¼Œè€Œä¸”è¿˜é€šè¿‡å­¦ä¹ æœ€ä½³å¯¹è¯ç­–ç•¥æœ‰æ•ˆåœ°å‘ç°äº†æ–°çš„æ”»å‡»ç­–ç•¥ï¼Œè¿™äº›ç­–ç•¥åœ¨å¤šè½®æ”»å‡»ä¸­æœ€å¤§åŒ–æ”»å‡»æˆåŠŸæ¬¡æ•°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.02286v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šè½®äº¤äº’åœºæ™¯ä¸­ä»å­˜åœ¨æ˜“å—æ”»å‡»çš„é—®é¢˜ã€‚ç°æœ‰å‘ç°å®‰å…¨æ¼æ´çš„æ–¹æ³•ä¸»è¦ä¾èµ–äººå·¥çº¢é˜Ÿæˆ–è‡ªåŠ¨åŒ–æ–¹æ³•ï¼Œä½†æœªèƒ½å…¨é¢æ¢ç´¢å¤šè½®æ”»å‡»çš„å·¨å¤§ç©ºé—´ã€‚æœ¬æ–‡æå‡ºçš„DialTree-RPOæ¡†æ¶ç»“åˆäº†æ ‘æœç´¢å’Œå¼ºåŒ–å­¦ä¹ ï¼Œèƒ½å¤Ÿè‡ªä¸»å‘ç°å¤šæ ·çš„å¤šè½®æ”»å‡»ç­–ç•¥ï¼Œé€šè¿‡å¯¹è¯ä½œä¸ºåºè´¯å†³ç­–é—®é¢˜æ¥ç³»ç»ŸåŒ–æ¢ç´¢ï¼Œæ— éœ€æ‰‹åŠ¨æ•´ç†æ•°æ®ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¸ä»…æé«˜äº†è¶…è¿‡25.9%çš„æ”»å‡»æˆåŠŸç‡ï¼Œè¿˜é€šè¿‡å­¦ä¹ ä¼˜åŒ–å¯¹è¯ç­–ç•¥å‘ç°äº†æ–°çš„æ”»å‡»ç­–ç•¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šè½®äº¤äº’åœºæ™¯ä¸­ä»é¢ä¸´å®‰å…¨æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰å‘ç°å®‰å…¨æ¼æ´çš„æ–¹æ³•ä¸»è¦ä¾èµ–äººå·¥çº¢é˜Ÿæˆ–è‡ªåŠ¨åŒ–æ–¹æ³•ï¼Œå­˜åœ¨å±€é™æ€§ã€‚</li>
<li>ç°æœ‰æ–¹æ³•æœªèƒ½å…¨é¢æ¢ç´¢å¤šè½®æ”»å‡»çš„å·¨å¤§ç©ºé—´ï¼Œå¿½ç•¥äº†å¤æ‚çš„å¯¹è¯åŠ¨æ€å’Œæˆ˜ç•¥å¯¹è¯è§„åˆ’ã€‚</li>
<li>DialTree-RPOæ¡†æ¶ç»“åˆäº†æ ‘æœç´¢å’Œå¼ºåŒ–å­¦ä¹ ï¼Œèƒ½å¤Ÿè‡ªä¸»å‘ç°å¤šæ ·çš„å¤šè½®æ”»å‡»ç­–ç•¥ã€‚</li>
<li>DialTree-RPOæ¡†æ¶é€šè¿‡å¯¹è¯ä½œä¸ºåºè´¯å†³ç­–é—®é¢˜æ¥ç³»ç»ŸåŒ–æ¢ç´¢ï¼Œæ— éœ€æ‰‹åŠ¨æ•´ç†æ•°æ®ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒDialTree-RPOæ¡†æ¶æé«˜äº†è¶…è¿‡25.9%çš„æ”»å‡»æˆåŠŸç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.02286">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_LLM/2510.02286v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_LLM/2510.02286v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_LLM/2510.02286v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="VidGuard-R1-AI-Generated-Video-Detection-and-Explanation-via-Reasoning-MLLMs-and-RL"><a href="#VidGuard-R1-AI-Generated-Video-Detection-and-Explanation-via-Reasoning-MLLMs-and-RL" class="headerlink" title="VidGuard-R1: AI-Generated Video Detection and Explanation via Reasoning   MLLMs and RL"></a>VidGuard-R1: AI-Generated Video Detection and Explanation via Reasoning   MLLMs and RL</h2><p><strong>Authors:Kyoungjun Park, Yifan Yang, Juheon Yi, Shicheng Zheng, Yifei Shen, Dongqi Han, Caihua Shan, Muhammad Muaz, Lili Qiu</strong></p>
<p>With the rapid advancement of AI-generated videos, there is an urgent need for effective detection tools to mitigate societal risks such as misinformation and reputational harm. In addition to accurate classification, it is essential that detection models provide interpretable explanations to ensure transparency for regulators and end users. To address these challenges, we introduce VidGuard-R1, the first video authenticity detector that fine-tunes a multi-modal large language model (MLLM) using group relative policy optimization (GRPO). Our model delivers both highly accurate judgments and insightful reasoning. We curate a challenging dataset of 140k real and AI-generated videos produced by state-of-the-art generation models, carefully designing the generation process to maximize discrimination difficulty. We then fine-tune Qwen-VL using GRPO with two specialized reward models that target temporal artifacts and generation complexity. Extensive experiments demonstrate that VidGuard-R1 achieves state-of-the-art zero-shot performance on existing benchmarks, with additional training pushing accuracy above 95%. Case studies further show that VidGuard-R1 produces precise and interpretable rationales behind its predictions. The code is publicly available at <a target="_blank" rel="noopener" href="https://vidguard-r1.github.io/">https://VidGuard-R1.github.io</a>. </p>
<blockquote>
<p>éšç€AIç”Ÿæˆè§†é¢‘çš„å¿«é€Ÿå‘å±•ï¼Œå¯¹äºæœ‰æ•ˆæ£€æµ‹å·¥å…·çš„éœ€æ±‚æ—¥ç›Šè¿«åˆ‡ï¼Œä»¥å‡è½»ç¤¾ä¼šé£é™©ï¼Œå¦‚è™šå‡ä¿¡æ¯å’Œå£°èª‰æŸå®³ã€‚é™¤äº†å‡†ç¡®åˆ†ç±»ä¹‹å¤–ï¼Œæ£€æµ‹æ¨¡å‹è¿˜å¿…é¡»æä¾›å¯è§£é‡Šçš„è§£é‡Šï¼Œä»¥ç¡®ä¿ç›‘ç®¡æœºæ„å’Œæœ€ç»ˆç”¨æˆ·é€æ˜åº¦ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†VidGuard-R1ï¼Œå®ƒæ˜¯ç¬¬ä¸€ä¸ªä½¿ç”¨ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰å¾®è°ƒå¤šæ¨¡å¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„è§†é¢‘çœŸå®æ€§æ£€æµ‹å™¨ã€‚æˆ‘ä»¬çš„æ¨¡å‹æ—¢æä¾›äº†é«˜åº¦å‡†ç¡®çš„åˆ¤æ–­ï¼Œåˆæä¾›äº†æ·±åˆ»çš„æ¨ç†ã€‚æˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªç”±14ä¸‡ä¸ªçœŸå®å’ŒAIç”Ÿæˆçš„è§†é¢‘ç»„æˆçš„æœ‰æŒ‘æˆ˜æ€§çš„æ•°æ®é›†ï¼Œè¿™äº›è§†é¢‘ç”±æœ€å…ˆè¿›çš„ç”Ÿæˆæ¨¡å‹ç”Ÿæˆï¼Œæˆ‘ä»¬ç²¾å¿ƒè®¾è®¡ç”Ÿæˆè¿‡ç¨‹ä»¥æœ€å¤§åŒ–åŒºåˆ†éš¾åº¦ã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨GRPOå¾®è°ƒQwen-VLï¼Œé‡‡ç”¨ä¸¤ä¸ªé’ˆå¯¹æ—¶é—´ä¼ªå½±å’Œç”Ÿæˆå¤æ‚æ€§çš„ä¸“ç”¨å¥–åŠ±æ¨¡å‹ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒVidGuard-R1åœ¨ç°æœ‰åŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„é›¶æ ·æœ¬æ€§èƒ½ï¼Œç»è¿‡é¢å¤–è®­ç»ƒåï¼Œå‡†ç¡®ç‡è¶…è¿‡95%ã€‚æ¡ˆä¾‹ç ”ç©¶è¿›ä¸€æ­¥è¡¨æ˜ï¼ŒVidGuard-R1äº§ç”Ÿçš„é¢„æµ‹èƒŒåçš„ç†ç”±ç²¾ç¡®ä¸”å¯è§£é‡Šã€‚ç›¸å…³ä»£ç å·²å…¬å¼€åœ¨ï¼š<a target="_blank" rel="noopener" href="https://vidguard-r1.github.ioä¸Š./">https://VidGuard-R1.github.ioä¸Šã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.02282v1">PDF</a> </p>
<p><strong>Summary</strong><br>åŸºäºäººå·¥æ™ºèƒ½ç”Ÿæˆè§†é¢‘çš„å¿«é€Ÿå‘å±•ï¼Œä¸ºåº”å¯¹è¯¸å¦‚è¯¯å¯¼ä¿¡æ¯å’Œå£°èª‰æŸå®³ç­‰ç¤¾ä¼šé£é™©ï¼Œæ€¥éœ€æœ‰æ•ˆçš„æ£€æµ‹å·¥å…·ã€‚ä¸ºè§£å†³æ­¤æŒ‘æˆ˜ï¼Œæ¨å‡ºé¦–æ¬¾è§†é¢‘çœŸå®æ€§æ£€æµ‹å™¨VidGuard-R1ï¼Œå…¶é€šè¿‡ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰å¯¹å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰è¿›è¡Œå¾®è°ƒï¼Œæ—¢æä¾›é«˜åº¦å‡†ç¡®çš„åˆ¤æ–­ï¼Œåˆæä¾›å¯è§£é‡Šçš„ç†ç”±ï¼Œç¡®ä¿å¯¹ç›‘ç®¡æœºæ„å’Œç»ˆç«¯ç”¨æˆ·çš„é€æ˜åº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>VidGuard-R1æ˜¯é¦–æ¬¾è§†é¢‘çœŸå®æ€§æ£€æµ‹å™¨ï¼Œèƒ½ç²¾ç»†åœ°åŒºåˆ†çœŸå®è§†é¢‘å’ŒAIç”Ÿæˆçš„è§†é¢‘ã€‚</li>
<li>è¯¥æ¨¡å‹ä½¿ç”¨ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰å’Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰è¿›è¡Œè®­ç»ƒï¼Œä»¥æé«˜å‡†ç¡®æ€§å’Œæ£€æµ‹æ•ˆç‡ã€‚</li>
<li>VidGuard-R1ä¸ä»…æä¾›å‡†ç¡®çš„åˆ¤æ–­ï¼Œè¿˜æä¾›å¯è§£é‡Šçš„ç†ç”±ï¼Œå¢å¼ºäº†å…¶é€æ˜åº¦ã€‚</li>
<li>è¯¥æ¨¡å‹ä½¿ç”¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼ŒåŒ…æ‹¬14ä¸‡ä¸ªçœŸå®å’ŒAIç”Ÿæˆçš„è§†é¢‘ã€‚</li>
<li>VidGuard-R1åœ¨ç°æœ‰åŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„é›¶æ ·æœ¬æ€§èƒ½ï¼Œç»è¿‡é¢å¤–è®­ç»ƒï¼Œå…¶å‡†ç¡®ç‡è¶…è¿‡95%ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.02282">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_LLM/2510.02282v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_LLM/2510.02282v1/page_2_0.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="microCLIP-Unsupervised-CLIP-Adaptation-via-Coarse-Fine-Token-Fusion-for-Fine-Grained-Image-Classification"><a href="#microCLIP-Unsupervised-CLIP-Adaptation-via-Coarse-Fine-Token-Fusion-for-Fine-Grained-Image-Classification" class="headerlink" title="microCLIP: Unsupervised CLIP Adaptation via Coarse-Fine Token Fusion for   Fine-Grained Image Classification"></a>microCLIP: Unsupervised CLIP Adaptation via Coarse-Fine Token Fusion for   Fine-Grained Image Classification</h2><p><strong>Authors:Sathira Silva, Eman Ali, Chetan Arora, Muhammad Haris Khan</strong></p>
<p>Unsupervised adaptation of CLIP-based vision-language models (VLMs) for fine-grained image classification requires sensitivity to microscopic local cues. While CLIP exhibits strong zero-shot transfer, its reliance on coarse global features restricts its performance on fine-grained classification tasks. Prior efforts inject fine-grained knowledge by aligning large language model (LLM) descriptions with the CLIP $\texttt{[CLS]}$ token; however, this approach overlooks spatial precision. We propose $\textbf{microCLIP}$, a self-training framework that jointly refines CLIPâ€™s visual and textual representations using fine-grained cues. At its core is Saliency-Oriented Attention Pooling (SOAP) within a lightweight TokenFusion module, which builds a saliency-guided $\texttt{[FG]}$ token from patch embeddings and fuses it with the global $\texttt{[CLS]}$ token for coarse-fine alignment. To stabilize adaptation, we introduce a two-headed LLM-derived classifier: a frozen classifier that, via multi-view alignment, provides a stable text-based prior for pseudo-labeling, and a learnable classifier initialized from LLM descriptions and fine-tuned with TokenFusion. We further develop Dynamic Knowledge Aggregation, which convexly combines fixed LLM&#x2F;CLIP priors with TokenFusionâ€™s evolving logits to iteratively refine pseudo-labels. Together, these components uncover latent fine-grained signals in CLIP, yielding a consistent $2.90%$ average accuracy gain across 13 fine-grained benchmarks while requiring only light adaptation. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/sathiiii/microCLIP">https://github.com/sathiiii/microCLIP</a>. </p>
<blockquote>
<p>åŸºäºCLIPçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„æ— ç›‘ç£é€‚åº”å¯¹äºç²¾ç»†å›¾åƒåˆ†ç±»éœ€è¦æ•æ„Ÿäºå¾®è§‚å±€éƒ¨çº¿ç´¢ã€‚è™½ç„¶CLIPè¡¨ç°å‡ºå¼ºå¤§çš„é›¶æ ·æœ¬è¿ç§»èƒ½åŠ›ï¼Œä½†å®ƒå¯¹ç²—ç³™å…¨å±€ç‰¹å¾çš„ä¾èµ–é™åˆ¶äº†å…¶åœ¨ç²¾ç»†åˆ†ç±»ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚ä¹‹å‰çš„åŠªåŠ›é€šè¿‡å°†ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æè¿°å¯¹é½æ¥æ³¨å…¥ç²¾ç»†çŸ¥è¯†ï¼Œè¿™å¿½ç•¥äº†ç©ºé—´ç²¾åº¦ã€‚æˆ‘ä»¬æå‡ºäº†microCLIPï¼Œè¿™æ˜¯ä¸€ä¸ªè‡ªè®­ç»ƒæ¡†æ¶ï¼Œå®ƒé€šè¿‡åˆ©ç”¨ç²¾ç»†çš„çº¿ç´¢æ¥å…±åŒå®Œå–„CLIPçš„è§†è§‰å’Œæ–‡æœ¬è¡¨ç¤ºã€‚å…¶æ ¸å¿ƒæ˜¯è½»é‡çº§çš„TokenFusionæ¨¡å—ä¸­çš„æ˜¾è‘—æ€§å¯¼å‘æ³¨æ„åŠ›æ± åŒ–ï¼ˆSOAPï¼‰ï¼Œè¯¥æ¨¡å—ä»è¡¥ä¸åµŒå…¥ä¸­æ„å»ºäº†ä¸€ä¸ªæ˜¾è‘—æ€§å¼•å¯¼çš„[FG]æ ‡è®°ï¼Œå¹¶å°†å…¶ä¸å…¨å±€[CLS]æ ‡è®°èåˆï¼Œä»¥å®ç°ç²—å¯¹é½å’Œç²¾ç»†å¯¹é½ã€‚ä¸ºäº†ç¨³å®šé€‚åº”è¿‡ç¨‹ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŒé‡LLMæ´¾ç”Ÿåˆ†ç±»å™¨ï¼šä¸€ä¸ªå†»ç»“åˆ†ç±»å™¨é€šè¿‡å¤šè§†å›¾å¯¹é½ä¸ºä¼ªæ ‡ç­¾æä¾›ç¨³å®šçš„æ–‡æœ¬åŸºç¡€å…ˆéªŒï¼Œä¸€ä¸ªå¯å­¦ä¹ åˆ†ç±»å™¨ä»LLMæè¿°åˆå§‹åŒ–å¹¶åœ¨TokenFusionä¸­è¿›è¡Œå¾®è°ƒã€‚æˆ‘ä»¬è¿˜å¼€å‘äº†åŠ¨æ€çŸ¥è¯†èšåˆï¼Œå®ƒé€šè¿‡å‡¸ç»„åˆå›ºå®šçš„LLM&#x2F;CLIPå…ˆéªŒå’ŒTokenFusionä¸æ–­å‘å±•çš„å¯¹æ•°å‡ ç‡æ¥è¿­ä»£åœ°æ”¹è¿›ä¼ªæ ‡ç­¾ã€‚è¿™äº›ç»„ä»¶å…±åŒä½œç”¨ï¼Œæ­ç¤ºäº†CLIPä¸­çš„æ½œåœ¨ç²¾ç»†ä¿¡å·ï¼Œåœ¨13ä¸ªç²¾ç»†åˆ†ç±»åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†å¹³å‡å‡†ç¡®åº¦æé«˜2.90%ï¼Œè€Œä»…éœ€è¦è½»é‡çº§çš„é€‚åº”ã€‚æˆ‘ä»¬çš„ä»£ç å¯ä»¥åœ¨ä»¥ä¸‹ç½‘ç«™æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/sathiiii/microCLIP">https://github.com/sathiiii/microCLIP</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.02270v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºmicroCLIPçš„è‡ªè®­ç»ƒæ¡†æ¶ï¼Œç”¨äºå¯¹CLIP-basedçš„è§†è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰è¿›è¡Œæ— ç›‘ç£å¾®è°ƒï¼Œæé«˜å…¶è¿›è¡Œç²¾ç»†å›¾åƒåˆ†ç±»çš„èƒ½åŠ›ã€‚é€šè¿‡ç»“åˆå±€éƒ¨æ˜¾è‘—æ€§æ£€æµ‹å’Œå…¨å±€ç‰¹å¾ï¼Œè¯¥æ¡†æ¶ä½¿ç”¨ç²¾ç»†ç‰¹å¾æ¥æé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚åŒæ—¶å¼•å…¥äº†åŒé‡LLMåˆ†ç±»å™¨ä»¥åŠåŠ¨æ€çŸ¥è¯†èšåˆç­–ç•¥æ¥ç¨³å®šæ¨¡å‹çš„é€‚åº”è¿‡ç¨‹ã€‚è¯¥æ¡†æ¶åœ¨å¤šä¸ªç²¾ç»†åˆ†ç±»åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†å¹³å‡å‡†ç¡®ç‡æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>microCLIPæ¡†æ¶åˆ©ç”¨å±€éƒ¨æ˜¾è‘—æ€§ä¿¡æ¯æ¥æé«˜CLIPæ¨¡å‹çš„æ€§èƒ½ï¼Œä»¥è§£å†³å…¶åœ¨ç²¾ç»†å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­çš„å±€é™æ€§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ˜¾è‘—æ€§å¯¼å‘æ³¨æ„åŠ›æ± åŒ–ï¼ˆSOAPï¼‰æ–¹æ³•ï¼Œç”¨äºæ„å»ºæ˜¾è‘—æ€§å¼•å¯¼çš„[FG]æ ‡è®°ã€‚</li>
<li>å¼•å…¥åŒé‡LLMåˆ†ç±»å™¨ç­–ç•¥ï¼ŒåŒ…æ‹¬ä¸€ä¸ªç”¨äºä¼ªæ ‡ç­¾çš„å†»ç»“åˆ†ç±»å™¨å’Œä¸€ä¸ªåŸºäºLLMæè¿°è¿›è¡Œåˆå§‹åŒ–å’Œå¾®è°ƒçš„å­¦ä¹ åˆ†ç±»å™¨ã€‚</li>
<li>åŠ¨æ€çŸ¥è¯†èšåˆç­–ç•¥èƒ½å¤Ÿç»“åˆå›ºå®šçš„LLM&#x2F;CLIPå…ˆéªŒçŸ¥è¯†å’ŒTokenFusionçš„æ¼”åŒ–é€»è¾‘ï¼Œä»¥è¿­ä»£åœ°ä¼˜åŒ–ä¼ªæ ‡ç­¾ã€‚</li>
<li>microCLIPæ¡†æ¶æé«˜äº†CLIPæ¨¡å‹åœ¨ç²¾ç»†åˆ†ç±»ä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼Œå¹³å‡å‡†ç¡®ç‡æå‡è¾¾åˆ°2.9%ã€‚</li>
<li>è¯¥æ¡†æ¶é€‚ç”¨äºå¤šç§ç²¾ç»†åˆ†ç±»åŸºå‡†æµ‹è¯•ï¼Œå±•ç¤ºäº†å…¶é€šç”¨æ€§å’Œæœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.02270">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_LLM/2510.02270v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_LLM/2510.02270v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_LLM/2510.02270v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_LLM/2510.02270v1/page_5_1.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="From-Frames-to-Clips-Efficient-Key-Clip-Selection-for-Long-Form-Video-Understanding"><a href="#From-Frames-to-Clips-Efficient-Key-Clip-Selection-for-Long-Form-Video-Understanding" class="headerlink" title="From Frames to Clips: Efficient Key Clip Selection for Long-Form Video   Understanding"></a>From Frames to Clips: Efficient Key Clip Selection for Long-Form Video   Understanding</h2><p><strong>Authors:Guangyu Sun, Archit Singhal, Burak Uzkent, Mubarak Shah, Chen Chen, Garin Kessler</strong></p>
<p>Video Large Language Models (VLMs) have achieved remarkable results on a variety of vision language tasks, yet their practical use is limited by the â€œneedle in a haystackâ€ problem: the massive number of visual tokens produced from raw video frames exhausts the modelâ€™s context window. Existing solutions alleviate this issue by selecting a sparse set of frames, thereby reducing token count, but such frame-wise selection discards essential temporal dynamics, leading to suboptimal reasoning about motion and event continuity. In this work we systematically explore the impact of temporal information and demonstrate that extending selection from isolated key frames to key clips, which are short, temporally coherent segments, improves video understanding. To maintain a fixed computational budget while accommodating the larger token footprint of clips, we propose an adaptive resolution strategy that dynamically balances spatial resolution and clip length, ensuring a constant token count per video. Experiments on three long-form video benchmarks demonstrate that our training-free approach, F2C, outperforms uniform sampling up to 8.1%, 5.6%, and 10.3% on Video-MME, LongVideoBench and MLVU benchmarks, respectively. These results highlight the importance of preserving temporal coherence in frame selection and provide a practical pathway for scaling Video LLMs to real world video understanding applications. Project webpage is available at <a target="_blank" rel="noopener" href="https://guangyusun.com/f2c">https://guangyusun.com/f2c</a> . </p>
<blockquote>
<p>è§†é¢‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å„ç§è§†è§‰è¯­è¨€ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—æˆæœï¼Œä½†å®ƒä»¬çš„å®é™…åº”ç”¨å—åˆ°â€œæµ·åº•æé’ˆâ€é—®é¢˜çš„é™åˆ¶ï¼šä»åŸå§‹è§†é¢‘å¸§ä¸­äº§ç”Ÿçš„å¤§é‡è§†è§‰æ ‡è®°è€—å°½äº†æ¨¡å‹çš„ä¸Šæ–‡çª—å£ã€‚ç°æœ‰è§£å†³æ–¹æ¡ˆé€šè¿‡é€‰æ‹©ç¨€ç–çš„å¸§é›†æ¥ç¼“è§£è¿™ä¸ªé—®é¢˜ï¼Œä»è€Œå‡å°‘æ ‡è®°æ•°é‡ï¼Œä½†è¿™æ ·çš„é€å¸§é€‰æ‹©ä¸¢å¼ƒäº†é‡è¦çš„æ—¶é—´åŠ¨æ€ï¼Œå¯¼è‡´å¯¹è¿åŠ¨å’Œäº‹ä»¶è¿ç»­æ€§çš„æ¨ç†ä¸ä½³ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°æ¢è®¨äº†æ—¶é—´ä¿¡æ¯çš„å½±å“ï¼Œå¹¶è¯æ˜å°†é€‰æ‹©èŒƒå›´ä»å­¤ç«‹çš„å…³é”®å¸§æ‰©å±•åˆ°å…³é”®ç‰‡æ®µï¼ˆçŸ­è€Œæ—¶é—´è¿è´¯çš„ç‰‡æ®µï¼‰å¯ä»¥æé«˜è§†é¢‘ç†è§£ã€‚ä¸ºäº†ä¿æŒå›ºå®šçš„è®¡ç®—é¢„ç®—å¹¶é€‚åº”ç‰‡æ®µæ›´å¤§çš„æ ‡è®°è¶³è¿¹ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è‡ªé€‚åº”åˆ†è¾¨ç‡ç­–ç•¥ï¼Œè¯¥ç­–ç•¥åŠ¨æ€å¹³è¡¡ç©ºé—´åˆ†è¾¨ç‡å’Œç‰‡æ®µé•¿åº¦ï¼Œç¡®ä¿æ¯ä¸ªè§†é¢‘çš„æ ‡è®°è®¡æ•°ä¿æŒä¸å˜ã€‚åœ¨ä¸‰ä¸ªé•¿æ ¼å¼è§†é¢‘åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ— è®­ç»ƒæ–¹æ³•F2Cåœ¨Video-MMEã€LongVideoBenchå’ŒMLVUåŸºå‡†æµ‹è¯•ä¸Šçš„æ€§èƒ½åˆ†åˆ«æ¯”å‡åŒ€é‡‡æ ·é«˜å‡º8.1%ã€5.6%å’Œ10.3%ã€‚è¿™äº›ç»“æœå¼ºè°ƒäº†ä¿æŒæ—¶é—´è¿è´¯æ€§åœ¨å¸§é€‰æ‹©ä¸­çš„é‡è¦æ€§ï¼Œå¹¶ä¸ºå°†è§†é¢‘LLMæ‰©å±•åˆ°ç°å®ä¸–ç•Œè§†é¢‘ç†è§£åº”ç”¨ç¨‹åºæä¾›äº†å®é™…é€”å¾„ã€‚é¡¹ç›®ç½‘é¡µå¯åœ¨<a target="_blank" rel="noopener" href="https://guangyusun.com/f2c%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://guangyusun.com/f2cä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.02262v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è§†é¢‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åœ¨å„ç§è§†è§‰è¯­è¨€ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—æˆæœï¼Œä½†å®é™…åº”ç”¨ä¸­å­˜åœ¨â€œæµ·é‡æ•°æ®ä¸­çš„é’ˆå¤´é—®é¢˜â€ï¼Œå³ä»åŸå§‹è§†é¢‘å¸§ç”Ÿæˆçš„å¤§é‡è§†è§‰æ ‡è®°è€—å°½äº†æ¨¡å‹çš„ä¸Šä¸‹æ–‡çª—å£ã€‚ç°æœ‰è§£å†³æ–¹æ¡ˆé€šè¿‡é€‰æ‹©ç¨€ç–çš„å¸§é›†æ¥å‡å°‘æ ‡è®°æ•°é‡ï¼Œä½†è¿™ç§æ–¹æ³•å¿½ç•¥äº†é‡è¦çš„æ—¶é—´åŠ¨æ€ï¼Œå¯¼è‡´å¯¹è¿åŠ¨å’Œäº‹ä»¶è¿ç»­æ€§çš„æ¨ç†ä¸å¤Ÿç†æƒ³ã€‚æœ¬æ–‡ç³»ç»Ÿåœ°æ¢è®¨äº†æ—¶é—´ä¿¡æ¯çš„å½±å“ï¼Œå¹¶è¯æ˜ä»å­¤ç«‹çš„å…³é”®å¸§æ‰©å±•åˆ°å…³é”®ç‰‡æ®µï¼ˆå³çŸ­æš‚è€Œè¿è´¯çš„æ—¶é—´æ®µï¼‰å¯ä»¥æé«˜è§†é¢‘ç†è§£ã€‚ä¸ºäº†ä¿æŒå›ºå®šçš„è®¡ç®—é¢„ç®—ï¼ŒåŒæ—¶é€‚åº”ç‰‡æ®µçš„æ›´å¤§æ ‡è®°è¶³è¿¹ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è‡ªé€‚åº”åˆ†è¾¨ç‡ç­–ç•¥ï¼Œè¯¥ç­–ç•¥åŠ¨æ€å¹³è¡¡ç©ºé—´åˆ†è¾¨ç‡å’Œç‰‡æ®µé•¿åº¦ï¼Œç¡®ä¿æ¯ä¸ªè§†é¢‘çš„æ ‡è®°è®¡æ•°ä¿æŒä¸å˜ã€‚åœ¨ä¸‰ä¸ªé•¿æ ¼å¼è§†é¢‘åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ— è®­ç»ƒæ–¹æ³•F2Cä¼˜äºå‡åŒ€é‡‡æ ·ï¼Œåœ¨Video-MMEã€LongVideoBenchå’ŒMLVUåŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°åˆ†åˆ«æé«˜äº†8.1%ã€5.6%å’Œ10.3%ã€‚è¿™äº›ç»“æœå¼ºè°ƒäº†ä¿æŒæ—¶é—´è¿è´¯æ€§åœ¨å¸§é€‰æ‹©ä¸­çš„é‡è¦æ€§ï¼Œå¹¶ä¸ºå°†è§†é¢‘LLMæ‰©å±•åˆ°ç°å®ä¸–ç•Œè§†é¢‘ç†è§£åº”ç”¨æä¾›äº†å®ç”¨é€”å¾„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†é¢‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åœ¨è§†è§‰è¯­è¨€ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä½†å­˜åœ¨â€œæµ·é‡æ•°æ®ä¸­çš„é’ˆå¤´é—®é¢˜â€ã€‚</li>
<li>ç°æœ‰è§£å†³æ–¹æ¡ˆé€šè¿‡é€‰æ‹©ç¨€ç–å¸§æ¥å‡å°‘æ ‡è®°æ•°é‡ï¼Œä½†å¿½ç•¥äº†æ—¶é—´åŠ¨æ€ï¼Œå¯¼è‡´è¿åŠ¨æ¨ç†ä¸å¤Ÿç†æƒ³ã€‚</li>
<li>æœ¬æ–‡æ¢ç´¢äº†æ—¶é—´ä¿¡æ¯å¯¹è§†é¢‘ç†è§£çš„å½±å“ï¼Œå¹¶è¯æ˜æ‰©å±•å…³é”®ç‰‡æ®µé€‰æ‹©å¯ä»¥æé«˜è§†é¢‘ç†è§£ã€‚</li>
<li>æå‡ºäº†ä¸€ç§è‡ªé€‚åº”åˆ†è¾¨ç‡ç­–ç•¥ï¼Œä»¥åŠ¨æ€å¹³è¡¡ç©ºé—´åˆ†è¾¨ç‡å’Œç‰‡æ®µé•¿åº¦ï¼Œç¡®ä¿æ¯ä¸ªè§†é¢‘çš„æ ‡è®°è®¡æ•°ä¿æŒä¸å˜ã€‚</li>
<li>F2Cæ–¹æ³•åœ¨ä¸‰ä¸ªé•¿æ ¼å¼è§†é¢‘åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè¯æ˜äº†ä¿æŒæ—¶é—´è¿è´¯æ€§åœ¨å¸§é€‰æ‹©ä¸­çš„é‡è¦æ€§ã€‚</li>
<li>F2Cæ–¹æ³•ä¸ºè§†é¢‘LLMæ‰©å±•åˆ°ç°å®ä¸–ç•Œè§†é¢‘ç†è§£åº”ç”¨æä¾›äº†å®ç”¨é€”å¾„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.02262">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_LLM/2510.02262v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_LLM/2510.02262v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_LLM/2510.02262v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_LLM/2510.02262v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_LLM/2510.02262v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="DragFlow-Unleashing-DiT-Priors-with-Region-Based-Supervision-for-Drag-Editing"><a href="#DragFlow-Unleashing-DiT-Priors-with-Region-Based-Supervision-for-Drag-Editing" class="headerlink" title="DragFlow: Unleashing DiT Priors with Region Based Supervision for Drag   Editing"></a>DragFlow: Unleashing DiT Priors with Region Based Supervision for Drag   Editing</h2><p><strong>Authors:Zihan Zhou, Shilin Lu, Shuli Leng, Shaocong Zhang, Zhuming Lian, Xinlei Yu, Adams Wai-Kin Kong</strong></p>
<p>Drag-based image editing has long suffered from distortions in the target region, largely because the priors of earlier base models, Stable Diffusion, are insufficient to project optimized latents back onto the natural image manifold. With the shift from UNet-based DDPMs to more scalable DiT with flow matching (e.g., SD3.5, FLUX), generative priors have become significantly stronger, enabling advances across diverse editing tasks. However, drag-based editing has yet to benefit from these stronger priors. This work proposes the first framework to effectively harness FLUXâ€™s rich prior for drag-based editing, dubbed DragFlow, achieving substantial gains over baselines. We first show that directly applying point-based drag editing to DiTs performs poorly: unlike the highly compressed features of UNets, DiT features are insufficiently structured to provide reliable guidance for point-wise motion supervision. To overcome this limitation, DragFlow introduces a region-based editing paradigm, where affine transformations enable richer and more consistent feature supervision. Additionally, we integrate pretrained open-domain personalization adapters (e.g., IP-Adapter) to enhance subject consistency, while preserving background fidelity through gradient mask-based hard constraints. Multimodal large language models (MLLMs) are further employed to resolve task ambiguities. For evaluation, we curate a novel Region-based Dragging benchmark (ReD Bench) featuring region-level dragging instructions. Extensive experiments on DragBench-DR and ReD Bench show that DragFlow surpasses both point-based and region-based baselines, setting a new state-of-the-art in drag-based image editing. Code and datasets will be publicly available upon publication. </p>
<blockquote>
<p>åŸºäºæ‹–æ”¾çš„å›¾åƒç¼–è¾‘é•¿æœŸå­˜åœ¨ç›®æ ‡åŒºåŸŸå¤±çœŸçš„é—®é¢˜ï¼Œè¿™ä¸»è¦æ˜¯å› ä¸ºæ—©æœŸåŸºç¡€æ¨¡å‹ï¼ˆå¦‚Stable Diffusionï¼‰çš„å…ˆéªŒçŸ¥è¯†ä¸è¶³ä»¥å°†ä¼˜åŒ–åçš„æ½œåœ¨å˜é‡æŠ•å½±å›è‡ªç„¶å›¾åƒæµå½¢ã€‚éšç€ä»åŸºäºUNetçš„DDPMsè½¬å‘æ›´å¯æ‰©å±•çš„DiTæµåŒ¹é…ï¼ˆä¾‹å¦‚SD3.5ã€FLUXï¼‰ï¼Œç”Ÿæˆå…ˆéªŒçŸ¥è¯†å˜å¾—æ›´ä¸ºå¼ºå¤§ï¼Œèƒ½å¤Ÿåœ¨å„ç§ç¼–è¾‘ä»»åŠ¡ä¸­å–å¾—è¿›å±•ã€‚ç„¶è€Œï¼ŒåŸºäºæ‹–æ”¾çš„å›¾åƒç¼–è¾‘å°šæœªä»è¿™äº›æ›´å¼ºçš„å…ˆéªŒçŸ¥è¯†ä¸­å—ç›Šã€‚è¿™é¡¹å·¥ä½œæå‡ºäº†ç¬¬ä¸€ä¸ªæœ‰æ•ˆåˆ©ç”¨FLUXä¸°å¯Œå…ˆéªŒçŸ¥è¯†çš„åŸºäºæ‹–æ”¾çš„ç¼–è¾‘æ¡†æ¶ï¼Œåä¸ºDragFlowï¼Œç›¸è¾ƒäºåŸºçº¿æ–¹æ³•å–å¾—äº†æ˜¾è‘—çš„æå‡ã€‚</p>
</blockquote>
<p>æˆ‘ä»¬é¦–å…ˆå±•ç¤ºç›´æ¥å°†åŸºäºç‚¹çš„æ‹–æ”¾ç¼–è¾‘åº”ç”¨äºDiTsçš„æ•ˆæœå¾ˆå·®ï¼šä¸UNetçš„é«˜åº¦å‹ç¼©ç‰¹å¾ä¸åŒï¼ŒDiTçš„ç‰¹å¾ç»“æ„ä¸å¤Ÿå……åˆ†ï¼Œæ— æ³•ä¸ºç‚¹å¼è¿åŠ¨ç›‘ç£æä¾›å¯é çš„æŒ‡å¯¼ã€‚ä¸ºäº†å…‹æœè¿™ä¸€å±€é™æ€§ï¼ŒDragFlowå¼•å…¥äº†ä¸€ç§åŸºäºåŒºåŸŸçš„ç¼–è¾‘æ¨¡å¼ï¼Œå…¶ä¸­ä»¿å°„å˜æ¢ä½¿ç‰¹å¾ç›‘ç£æ›´åŠ ä¸°å¯Œå’Œä¸€è‡´ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é›†æˆäº†é¢„è®­ç»ƒçš„å¼€æ”¾åŸŸä¸ªæ€§åŒ–é€‚é…å™¨ï¼ˆä¾‹å¦‚IP-Adapterï¼‰ä»¥å¢å¼ºä¸»é¢˜çš„ä¸€è‡´æ€§ï¼ŒåŒæ—¶é€šè¿‡åŸºäºæ¢¯åº¦æ©ç çš„ç¡¬çº¦æŸä¿ç•™èƒŒæ™¯ä¿çœŸåº¦ã€‚å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰è¿›ä¸€æ­¥ç”¨äºè§£å†³ä»»åŠ¡æ­§ä¹‰ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.02253v1">PDF</a> Preprint</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ä¸ªåŸºäºFluxä¸°å¯Œå…ˆéªŒçš„æ‹–æ”¾å¼å›¾åƒç¼–è¾‘æ¡†æ¶ï¼Œç§°ä¸ºDragFlowã€‚è¯¥æ¡†æ¶è§£å†³äº†åœ¨DiTæ¨¡å‹ä¸Šç›´æ¥åº”ç”¨ç‚¹å¼æ‹–æ”¾ç¼–è¾‘æ€§èƒ½ä¸ä½³çš„é—®é¢˜ï¼Œå¹¶å¼•å…¥åŒºåŸŸåŒ–ç¼–è¾‘æ¨¡å¼ä»¥æä¾›æ›´å¯é çš„ç‰¹å¾ç›‘ç£ã€‚ç»“åˆé¢„è®­ç»ƒçš„å¼€æ”¾åŸŸä¸ªæ€§åŒ–é€‚é…å™¨ï¼ˆå¦‚IP-Adapterï¼‰ï¼Œè§£å†³äº†èƒŒæ™¯ä¿æŒä¸ä¸»ä½“ä¸€è‡´æ€§çš„æŒ‘æˆ˜ã€‚é‡‡ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰è§£å†³ä»»åŠ¡æ¨¡ç³Šæ€§é—®é¢˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DragFlowæ¡†æ¶åˆ©ç”¨Fluxçš„ä¸°å¯Œå…ˆéªŒè¿›è¡Œæ‹–æ”¾å¼å›¾åƒç¼–è¾‘ï¼Œå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</li>
<li>ç›´æ¥åœ¨DiTæ¨¡å‹ä¸Šåº”ç”¨ç‚¹å¼æ‹–æ”¾ç¼–è¾‘æ•ˆæœä¸ä½³ï¼Œå› æ­¤DragFlowå¼•å…¥åŒºåŸŸåŒ–ç¼–è¾‘æ¨¡å¼ã€‚</li>
<li>åŒºåŸŸåŒ–ç¼–è¾‘æ¨¡å¼é€šè¿‡ä»¿å°„å˜æ¢æä¾›æ›´å¯é çš„ç‰¹å¾ç›‘ç£ï¼Œå®ç°æ›´ä¸°å¯Œä¸”æ›´ä¸€è‡´çš„ç¼–è¾‘æ•ˆæœã€‚</li>
<li>ç»“åˆé¢„è®­ç»ƒçš„å¼€æ”¾åŸŸä¸ªæ€§åŒ–é€‚é…å™¨ï¼Œå¢å¼ºä¸»ä½“ä¸€è‡´æ€§ï¼ŒåŒæ—¶ä¿æŒèƒŒæ™¯ä¿çœŸåº¦ã€‚</li>
<li>é‡‡ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹è§£å†³ä»»åŠ¡æ¨¡ç³Šæ€§ï¼Œæé«˜ç¼–è¾‘ä»»åŠ¡çš„å‡†ç¡®æ€§ã€‚</li>
<li>å»ºç«‹äº†åŸºäºåŒºåŸŸæ‹–æ‹½çš„æ–°å‹è¯„ä¼°åŸºå‡†ï¼ˆReD Benchï¼‰ï¼Œä»¥æ›´å‡†ç¡®åœ°è¯„ä¼°æ‹–æ”¾å¼å›¾åƒç¼–è¾‘æ–¹æ³•çš„æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.02253">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_LLM/2510.02253v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_LLM/2510.02253v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_LLM/2510.02253v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_LLM/2510.02253v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_LLM/2510.02253v1/page_5_1.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="RewardMap-Tackling-Sparse-Rewards-in-Fine-grained-Visual-Reasoning-via-Multi-Stage-Reinforcement-Learning"><a href="#RewardMap-Tackling-Sparse-Rewards-in-Fine-grained-Visual-Reasoning-via-Multi-Stage-Reinforcement-Learning" class="headerlink" title="RewardMap: Tackling Sparse Rewards in Fine-grained Visual Reasoning via   Multi-Stage Reinforcement Learning"></a>RewardMap: Tackling Sparse Rewards in Fine-grained Visual Reasoning via   Multi-Stage Reinforcement Learning</h2><p><strong>Authors:Sicheng Feng, Kaiwen Tuo, Song Wang, Lingdong Kong, Jianke Zhu, Huan Wang</strong></p>
<p>Fine-grained visual reasoning remains a core challenge for multimodal large language models (MLLMs). The recently introduced ReasonMap highlights this gap by showing that even advanced MLLMs struggle with spatial reasoning in structured and information-rich settings such as transit maps, a task of clear practical and scientific importance. However, standard reinforcement learning (RL) on such tasks is impeded by sparse rewards and unstable optimization. To address this, we first construct ReasonMap-Plus, an extended dataset that introduces dense reward signals through Visual Question Answering (VQA) tasks, enabling effective cold-start training of fine-grained visual understanding skills. Next, we propose RewardMap, a multi-stage RL framework designed to improve both visual understanding and reasoning capabilities of MLLMs. RewardMap incorporates two key designs. First, we introduce a difficulty-aware reward design that incorporates detail rewards, directly tackling the sparse rewards while providing richer supervision. Second, we propose a multi-stage RL scheme that bootstraps training from simple perception to complex reasoning tasks, offering a more effective cold-start strategy than conventional Supervised Fine-Tuning (SFT). Experiments on ReasonMap and ReasonMap-Plus demonstrate that each component of RewardMap contributes to consistent performance gains, while their combination yields the best results. Moreover, models trained with RewardMap achieve an average improvement of 3.47% across 6 benchmarks spanning spatial reasoning, fine-grained visual reasoning, and general tasks beyond transit maps, underscoring enhanced visual understanding and reasoning capabilities. </p>
<blockquote>
<p>ç²¾ç»†è§†è§‰æ¨ç†å¯¹äºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ¥è¯´ä»ç„¶æ˜¯ä¸€ä¸ªæ ¸å¿ƒæŒ‘æˆ˜ã€‚æœ€è¿‘æ¨å‡ºçš„ReasonMapé€šè¿‡æ˜¾ç¤ºå³ä½¿åœ¨é«˜çº§MLLMsä¸­ï¼Œåœ¨ç»“æ„åŒ–ä¸”ä¿¡æ¯ä¸°å¯Œçš„åœºæ™¯ï¼ˆå¦‚äº¤é€šåœ°å›¾ï¼‰ä¸­è¿›è¡Œç©ºé—´æ¨ç†ä¹Ÿå­˜åœ¨å›°éš¾ï¼Œä»è€Œçªå‡ºäº†è¿™ä¸€å·®è·ï¼Œè¿™æ˜¯ä¸€ä¸ªå…·æœ‰æ˜ç¡®å®ç”¨æ€§å’Œç§‘å­¦é‡è¦æ€§çš„ä»»åŠ¡ã€‚ç„¶è€Œï¼Œæ­¤ç±»ä»»åŠ¡çš„æ ‡å‡†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å—åˆ°å¥–åŠ±ç¨€ç–å’Œä¼˜åŒ–ä¸ç¨³å®šçš„å½±å“ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬é¦–å…ˆæ„å»ºReasonMap-Plusï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡è§†è§‰é—®ç­”ï¼ˆVQAï¼‰ä»»åŠ¡å¼•å…¥å¯†é›†å¥–åŠ±ä¿¡å·çš„æ‰©å±•æ•°æ®é›†ï¼Œèƒ½å¤Ÿå®ç°ç²¾ç»†è§†è§‰ç†è§£æŠ€èƒ½çš„æœ‰æ•ˆå†·å¯åŠ¨è®­ç»ƒã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æå‡ºäº†RewardMapï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šé˜¶æ®µå¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜MLLMsçš„è§†è§‰ç†è§£å’Œæ¨ç†èƒ½åŠ›ã€‚RewardMapæœ‰ä¸¤ä¸ªå…³é”®è®¾è®¡ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§éš¾åº¦æ„ŸçŸ¥å¥–åŠ±è®¾è®¡ï¼Œè¯¥è®¾è®¡ç»“åˆäº†ç»†èŠ‚å¥–åŠ±ï¼Œç›´æ¥è§£å†³äº†å¥–åŠ±ç¨€ç–çš„é—®é¢˜ï¼ŒåŒæ—¶æä¾›äº†æ›´ä¸°å¯Œçš„ç›‘ç£ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¤šé˜¶æ®µå¼ºåŒ–å­¦ä¹ æ–¹æ¡ˆï¼Œä»ç®€å•çš„æ„ŸçŸ¥åˆ°å¤æ‚çš„æ¨ç†ä»»åŠ¡è¿›è¡Œå¼•å¯¼è®­ç»ƒï¼Œç›¸è¾ƒäºä¼ ç»Ÿçš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æä¾›äº†æ›´æœ‰æ•ˆçš„å†·å¯åŠ¨ç­–ç•¥ã€‚åœ¨ReasonMapå’ŒReasonMap-Plusä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒRewardMapçš„æ¯ä¸ªç»„æˆéƒ¨åˆ†éƒ½å¸¦æ¥äº†æŒç»­çš„æ€§èƒ½æå‡ï¼Œå®ƒä»¬çš„ç»„åˆåˆ™å–å¾—äº†æœ€ä½³æ•ˆæœã€‚æ­¤å¤–ï¼Œä½¿ç”¨RewardMapè®­ç»ƒçš„æ¨¡å‹åœ¨è·¨è¶Šç©ºé—´æ¨ç†ã€ç²¾ç»†è§†è§‰æ¨ç†å’Œä¸€èˆ¬ä»»åŠ¡çš„å…­ä¸ªåŸºå‡†æµ‹è¯•ä¸­å¹³å‡æé«˜äº†3.47%ï¼Œçªæ˜¾äº†å¢å¼ºçš„è§†è§‰ç†è§£å’Œæ¨ç†èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.02240v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºè§†è§‰æ¨ç†åœ¨ç²¾ç»†çº§åˆ«ä¸Šä»æ˜¯å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æ ¸å¿ƒæŒ‘æˆ˜ä¹‹ä¸€ã€‚ReasonMapçš„å¼•å…¥çªå‡ºäº†è¿™ä¸€å·®è·ï¼Œè¡¨æ˜å³ä½¿åœ¨ç»“æ„å’Œä¿¡æ¯ä¸°å¯Œçš„è®¾ç½®ï¼ˆå¦‚äº¤é€šåœ°å›¾ï¼‰ä¸­ï¼Œå…ˆè¿›çš„MLLMsåœ¨ç©ºé—´æ¨ç†æ–¹é¢ä¹Ÿä¼šé‡åˆ°å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬é¦–å…ˆé€šè¿‡è§†è§‰é—®ç­”ï¼ˆVQAï¼‰ä»»åŠ¡æ„å»ºReasonMap-Plusæ•°æ®é›†ï¼Œå¼•å…¥å¯†é›†å¥–åŠ±ä¿¡å·ï¼Œä»¥å®ç°ç²¾ç»†ç²’åº¦è§†è§‰ç†è§£æŠ€èƒ½çš„å†·å¯åŠ¨è®­ç»ƒã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æå‡ºäº†RewardMapï¼Œä¸€ä¸ªæ—¨åœ¨æé«˜MLLMsçš„è§†è§‰ç†è§£å’Œæ¨ç†èƒ½åŠ›çš„å¤šé˜¶æ®µå¼ºåŒ–å­¦ä¹ æ¡†æ¶ã€‚RewardMapåŒ…æ‹¬ä¸¤ä¸ªå…³é”®è®¾è®¡ï¼šé¦–å…ˆï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§éš¾åº¦æ„ŸçŸ¥å¥–åŠ±è®¾è®¡ï¼Œé€šè¿‡è¯¦ç»†çš„å¥–åŠ±æ¥è§£å†³ç¨€ç–å¥–åŠ±é—®é¢˜ï¼Œå¹¶æä¾›æ›´ä¸°å¯Œçš„ç›‘ç£ï¼›å…¶æ¬¡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¤šé˜¶æ®µå¼ºåŒ–å­¦ä¹ æ–¹æ¡ˆï¼Œä»ç®€å•çš„æ„ŸçŸ¥åˆ°å¤æ‚çš„æ¨ç†ä»»åŠ¡è¿›è¡Œå¼•å¯¼è®­ç»ƒï¼Œç›¸æ¯”ä¼ ç»Ÿçš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æä¾›äº†æ›´æœ‰æ•ˆçš„å†·å¯åŠ¨ç­–ç•¥ã€‚å®éªŒè¡¨æ˜ï¼ŒRewardMapçš„æ¯ä¸ªç»„æˆéƒ¨åˆ†éƒ½å¯¹æ€§èƒ½æå‡åšå‡ºäº†è´¡çŒ®ï¼Œå®ƒä»¬çš„ç»„åˆå–å¾—äº†æœ€ä½³æ•ˆæœã€‚ä½¿ç”¨RewardMapè®­ç»ƒçš„æ¨¡å‹åœ¨è·¨è¶Šç©ºé—´æ¨ç†ã€ç²¾ç»†è§†è§‰æ¨ç†å’Œä¸€èˆ¬ä»»åŠ¡çš„å…­ä¸ªåŸºå‡†æµ‹è¯•ä¸­å¹³å‡æé«˜äº†3.47%ï¼Œè¯æ˜äº†å…¶å¢å¼ºçš„è§†è§‰ç†è§£å’Œæ¨ç†èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Fine-grained visual reasoning remains a core challenge for MLLMs.</li>
<li>ReasonMap highlights the gap in spatial reasoning for MLLMs in structured and information-rich settings.</li>
<li>Dense reward signals are introduced via VQA tasks in ReasonMap-Plus to enable effective cold-start training.</li>
<li>RewardMap framework improves visual understanding and reasoning capabilities of MLLMs through a difficulty-aware reward design and multi-stage RL scheme.</li>
<li>Experiments demonstrate that components of RewardMap contribute to performance gains, with the combination yielding the best results.</li>
<li>Models trained with RewardMap achieve an average improvement of 3.47% across multiple benchmarks, underscoring enhanced visual understanding and reasoning capabilities.</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.02240">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_LLM/2510.02240v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_LLM/2510.02240v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_LLM/2510.02240v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="The-Reasoning-Boundary-Paradox-How-Reinforcement-Learning-Constrains-Language-Models"><a href="#The-Reasoning-Boundary-Paradox-How-Reinforcement-Learning-Constrains-Language-Models" class="headerlink" title="The Reasoning Boundary Paradox: How Reinforcement Learning Constrains   Language Models"></a>The Reasoning Boundary Paradox: How Reinforcement Learning Constrains   Language Models</h2><p><strong>Authors:Phuc Minh Nguyen, Chinh D. La, Duy M. H. Nguyen, Nitesh V. Chawla, Binh T. Nguyen, Khoa D. Doan</strong></p>
<p>Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a key method for improving Large Language Modelsâ€™ reasoning capabilities, yet recent evidence suggests it may paradoxically shrink the reasoning boundary rather than expand it. This paper investigates the shrinkage issue of RLVR by analyzing its learning dynamics and reveals two critical phenomena that explain this failure. First, we expose negative interference in RLVR, where learning to solve certain training problems actively reduces the likelihood of correct solutions for others, leading to the decline of Pass@$k$ performance, or the probability of generating a correct solution within $k$ attempts. Second, we uncover the winner-take-all phenomenon: RLVR disproportionately reinforces problems with high likelihood, correct solutions, under the base model, while suppressing other initially low-likelihood ones. Through extensive theoretical and empirical analysis on multiple mathematical reasoning benchmarks, we show that this effect arises from the inherent on-policy sampling in standard RL objectives, causing the model to converge toward narrow solution strategies. Based on these insights, we propose a simple yet effective data curation algorithm that focuses RLVR learning on low-likelihood problems, achieving notable improvement in Pass@$k$ performance. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/mail-research/SELF-llm-interference">https://github.com/mail-research/SELF-llm-interference</a>. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰å·²æˆä¸ºæé«˜å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„ä¸€ç§å…³é”®æ–¹æ³•ï¼Œä½†æœ€è¿‘çš„è¯æ®è¡¨æ˜ï¼Œå®ƒå¯èƒ½ä¼šç¼©å°æ¨ç†è¾¹ç•Œè€Œä¸æ˜¯æ‰©å¤§ã€‚æœ¬æ–‡é€šè¿‡åˆ†æRLVRçš„å­¦ä¹ åŠ¨æ€æ¥ç ”ç©¶å…¶æ”¶ç¼©é—®é¢˜ï¼Œæ­ç¤ºäº†ä¸¤ç§å…³é”®ç°è±¡æ¥è§£é‡Šè¿™ç§å¤±è´¥ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æš´éœ²äº†RLVRä¸­çš„è´Ÿå¹²æ‰°ç°è±¡ï¼Œå…¶ä¸­å­¦ä¹ è§£å†³æŸäº›è®­ç»ƒé—®é¢˜å®é™…ä¸Šå‡å°‘äº†è§£å†³å…¶ä»–é—®é¢˜çš„æ­£ç¡®è§£å†³æ–¹æ¡ˆçš„å¯èƒ½æ€§ï¼Œå¯¼è‡´Pass@kæ€§èƒ½ä¸‹é™ï¼Œå³åœ¨kæ¬¡å°è¯•ä¸­äº§ç”Ÿæ­£ç¡®è§£å†³æ–¹æ¡ˆçš„æ¦‚ç‡ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å‘ç°äº†èµ¢å®¶é€šåƒç°è±¡ï¼šRLVRä¼šä¸æˆæ¯”ä¾‹åœ°å¼ºåŒ–åŸºç¡€æ¨¡å‹ä¸‹é«˜æ¦‚ç‡çš„æ­£ç¡®è§£å†³æ–¹æ¡ˆçš„é—®é¢˜ï¼ŒåŒæ—¶æŠ‘åˆ¶å…¶ä»–æœ€åˆä½æ¦‚ç‡çš„é—®é¢˜ã€‚é€šè¿‡å¯¹å¤šä¸ªæ•°å­¦æ¨ç†åŸºå‡†è¿›è¡Œå¹¿æ³›çš„ç†è®ºå’Œå®è¯åˆ†æï¼Œæˆ‘ä»¬è¡¨æ˜è¿™ç§æ•ˆæœæºäºæ ‡å‡†å¼ºåŒ–å­¦ä¹ ç›®æ ‡ä¸­çš„å›ºæœ‰ç­–ç•¥é‡‡æ ·ï¼Œå¯¼è‡´æ¨¡å‹æ”¶æ•›äºç‹­çª„çš„è§£å†³æ–¹æ¡ˆç­–ç•¥ã€‚åŸºäºè¿™äº›è§è§£ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„æ•°æ®æ•´ç†ç®—æ³•ï¼Œè¯¥ç®—æ³•ä¸“æ³¨äºRLVRå­¦ä¹ ä½æ¦‚ç‡é—®é¢˜ï¼Œåœ¨Pass@kæ€§èƒ½ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/mail-research/SELF-llm-interference%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/mail-research/SELF-llm-interferenceæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.02230v1">PDF</a> 23 pages, 15 figures</p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¼ºåŒ–å­¦ä¹ ç»“åˆå¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰æ˜¯æé«˜å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„ä¸€ç§å…³é”®æ–¹æ³•ï¼Œä½†æœ€æ–°è¯æ®è¡¨æ˜ï¼Œå®ƒå¯èƒ½ç¼©å°æ¨ç†è¾¹ç•Œè€Œä¸æ˜¯æ‰©å¤§ã€‚æœ¬æ–‡è°ƒæŸ¥äº†RLVRçš„æ”¶ç¼©é—®é¢˜ï¼Œé€šè¿‡åˆ†æå…¶å­¦ä¹ åŠ¨æ€æ­ç¤ºäº†ä¸¤ä¸ªå…³é”®ç°è±¡æ¥è§£é‡Šè¿™ç§å¤±è´¥çš„åŸå› ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æ­ç¤ºäº†RLVRä¸­çš„è´Ÿå¹²æ‰°ç°è±¡ï¼Œå³è§£å†³æŸäº›è®­ç»ƒé—®é¢˜ä¼šç§¯æå‡å°‘å…¶ä»–é—®é¢˜çš„æ­£ç¡®è§£å†³æ–¹æ¡ˆçš„å¯èƒ½æ€§ï¼Œå¯¼è‡´Pass@kæ€§èƒ½ä¸‹é™ï¼Œå³åœ¨kæ¬¡å°è¯•ä¸­äº§ç”Ÿæ­£ç¡®è§£å†³æ–¹æ¡ˆçš„æ¦‚ç‡ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å‘ç°äº†èµ¢å®¶é€šåƒç°è±¡ï¼šRLVRä¼šä¸æˆæ¯”ä¾‹åœ°å¼ºåŒ–åŸºç¡€æ¨¡å‹ä¸‹é«˜æ¦‚ç‡æ­£ç¡®è§£å†³æ–¹æ¡ˆçš„é—®é¢˜ï¼ŒåŒæ—¶å‹åˆ¶å…¶ä»–æœ€åˆä½æ¦‚ç‡çš„é—®é¢˜ã€‚é€šè¿‡å¤šä¸ªæ•°å­¦æ¨ç†åŸºå‡†çš„å¹¿æ³›ç†è®ºå’Œå®è¯åˆ†æï¼Œæˆ‘ä»¬è¡¨æ˜è¿™ç§æ•ˆåº”æºäºæ ‡å‡†å¼ºåŒ–å­¦ä¹ ç›®æ ‡ä¸­çš„å›ºæœ‰ç­–ç•¥é‡‡æ ·ï¼Œå¯¼è‡´æ¨¡å‹æ”¶æ•›åˆ°ç‹­çª„çš„è§£å†³æ–¹æ¡ˆç­–ç•¥ã€‚åŸºäºè¿™äº›è§è§£ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„æ•°æ®æ•´ç†ç®—æ³•ï¼Œé‡ç‚¹å…³æ³¨RLVRåœ¨ä½æ¦‚ç‡é—®é¢˜ä¸Šçš„å­¦ä¹ ï¼Œåœ¨Pass@kæ€§èƒ½ä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>RLVRè™½èƒ½æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œä½†å¯èƒ½ç¼©å°æ¨ç†è¾¹ç•Œã€‚</li>
<li>RLVRä¸­å­˜åœ¨è´Ÿå¹²æ‰°ç°è±¡ï¼Œå³è§£å†³æŸäº›è®­ç»ƒé—®é¢˜å¯èƒ½ä¼šå½±å“å…¶ä»–é—®é¢˜çš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>èµ¢å®¶é€šåƒç°è±¡åœ¨RLVRä¸­æ™®éå­˜åœ¨ï¼Œå³æŸäº›é—®é¢˜çš„æ­£ç¡®è§£å†³æ–¹æ¡ˆå¾—åˆ°å¼ºåŒ–ï¼Œè€Œå…¶ä»–ä½æ¦‚ç‡çš„é—®é¢˜è¢«å‹åˆ¶ã€‚</li>
<li>ä¸Šè¿°ç°è±¡æºäºæ ‡å‡†å¼ºåŒ–å­¦ä¹ ç›®æ ‡ä¸­çš„å†…åœ¨ç­–ç•¥é‡‡æ ·ï¼Œå¯¼è‡´æ¨¡å‹æ”¶æ•›åˆ°ç‹­çª„çš„è§£å†³æ–¹æ¡ˆç­–ç•¥ã€‚</li>
<li>é€šè¿‡ç†è®ºåˆ†æå’Œå®è¯ç ”ç©¶è¯æ˜ä¸Šè¿°è§‚ç‚¹ï¼Œæ¶‰åŠå¤šä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ã€‚</li>
<li>æå‡ºä¸€ç§æ•°æ®æ•´ç†ç®—æ³•ï¼Œä¸“æ³¨äºRLVRåœ¨ä½æ¦‚ç‡é—®é¢˜ä¸Šçš„å­¦ä¹ ã€‚</li>
<li>è¯¥ç®—æ³•æ˜¾è‘—æé«˜äº†Pass@kæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.02230">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_LLM/2510.02230v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_LLM/2510.02230v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_LLM/2510.02230v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_LLM/2510.02230v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="xLSTM-Scaling-Laws-Competitive-Performance-with-Linear-Time-Complexity"><a href="#xLSTM-Scaling-Laws-Competitive-Performance-with-Linear-Time-Complexity" class="headerlink" title="xLSTM Scaling Laws: Competitive Performance with Linear Time-Complexity"></a>xLSTM Scaling Laws: Competitive Performance with Linear Time-Complexity</h2><p><strong>Authors:Maximilian Beck, Kajetan Schweighofer, Sebastian BÃ¶ck, Sebastian Lehner, Sepp Hochreiter</strong></p>
<p>Scaling laws play a central role in the success of Large Language Models (LLMs), enabling the prediction of model performance relative to compute budgets prior to training. While Transformers have been the dominant architecture, recent alternatives such as xLSTM offer linear complexity with respect to context length while remaining competitive in the billion-parameter regime. We conduct a comparative investigation on the scaling behavior of Transformers and xLSTM along the following lines, providing insights to guide future model design and deployment. First, we study the scaling behavior for xLSTM in compute-optimal and over-training regimes using both IsoFLOP and parametric fit approaches on a wide range of model sizes (80M-7B) and number of training tokens (2B-2T). Second, we examine the dependence of optimal model sizes on context length, a pivotal aspect that was largely ignored in previous work. Finally, we analyze inference-time scaling characteristics. Our findings reveal that in typical LLM training and inference scenarios, xLSTM scales favorably compared to Transformers. Importantly, xLSTMâ€™s advantage widens as training and inference contexts grow. </p>
<blockquote>
<p>æ¨¡å‹ç¼©æ”¾æ³•åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æˆåŠŸä¸­èµ·åˆ°äº†æ ¸å¿ƒä½œç”¨ï¼Œèƒ½å¤Ÿåœ¨è®­ç»ƒå‰é¢„æµ‹æ¨¡å‹ç›¸å¯¹äºè®¡ç®—é¢„ç®—çš„æ€§èƒ½ã€‚è™½ç„¶Transformeræ˜¯ä¸»å¯¼æ¶æ„ï¼Œä½†æœ€è¿‘çš„æ›¿ä»£æ–¹æ¡ˆå¦‚xLSTMåœ¨äº¿å‚æ•°é¢†åŸŸä»ç„¶å…·æœ‰ç«äº‰åŠ›ï¼ŒåŒæ—¶å®ƒä»¬å¯¹äºä¸Šä¸‹æ–‡é•¿åº¦å…·æœ‰çº¿æ€§å¤æ‚æ€§ã€‚æˆ‘ä»¬å¯¹Transformerå’ŒxLSTMçš„ç¼©æ”¾è¡Œä¸ºè¿›è¡Œäº†æ¯”è¾ƒè°ƒæŸ¥ï¼Œä¸ºæœªæ¥çš„æ¨¡å‹è®¾è®¡å’Œéƒ¨ç½²æä¾›äº†è§è§£ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ç ”ç©¶äº†åœ¨æœ€ä¼˜è®¡ç®—å’Œè¿‡åº¦è®­ç»ƒçŠ¶æ€ä¸‹xLSTMçš„ç¼©æ”¾è¡Œä¸ºï¼Œä½¿ç”¨IsoFLOPå’Œå‚æ•°æ‹Ÿåˆæ–¹æ³•ï¼Œæ¶µç›–äº†å¹¿æ³›çš„æ¨¡å‹å¤§å°ï¼ˆ80M-7Bï¼‰å’Œè®­ç»ƒä»¤ç‰Œæ•°é‡ï¼ˆ2B-2Tï¼‰ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬ç ”ç©¶äº†æœ€ä¼˜æ¨¡å‹å¤§å°å¯¹ä¸Šä¸‹æ–‡é•¿åº¦çš„ä¾èµ–æ€§ï¼Œè¿™æ˜¯ä»¥å‰å·¥ä½œä¸­è¢«å¿½è§†çš„å…³é”®æ–¹é¢ã€‚æœ€åï¼Œæˆ‘ä»¬åˆ†æäº†æ¨ç†æ—¶é—´çš„ç¼©æ”¾ç‰¹æ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œåœ¨å…¸å‹çš„å¤§å‹è¯­è¨€æ¨¡å‹è®­ç»ƒå’Œæ¨ç†åœºæ™¯ä¸­ï¼ŒxLSTMç›¸å¯¹äºTransformerè¡¨ç°æ›´å¥½ã€‚é‡è¦çš„æ˜¯ï¼Œéšç€è®­ç»ƒå’Œæ¨ç†ä¸Šä¸‹æ–‡çš„å¢é•¿ï¼ŒxLSTMçš„ä¼˜åŠ¿ä¹Ÿåœ¨æ‰©å¤§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.02228v1">PDF</a> Code and data available at   <a target="_blank" rel="noopener" href="https://github.com/NX-AI/xlstm_scaling_laws">https://github.com/NX-AI/xlstm_scaling_laws</a></p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æˆåŠŸå…³é”®åœ¨äºå…¶ç¼©æ”¾å¾‹ï¼Œå®ƒèƒ½å¤Ÿåœ¨è®­ç»ƒå‰é¢„æµ‹æ¨¡å‹æ€§èƒ½ä¸è®¡ç®—é¢„ç®—çš„å…³ç³»ã€‚è™½ç„¶Transformeræ˜¯ç›®å‰ä¸»æµæ¶æ„ï¼Œä½†æœ€è¿‘çš„æ›¿ä»£å“xLSTMåœ¨ä¸Šä¸‹æ–‡é•¿åº¦æ–¹é¢æä¾›äº†çº¿æ€§å¤æ‚æ€§å¹¶ä¿æŒç«äº‰åŠ›ã€‚æœ¬æ–‡æ¯”è¾ƒäº†Transformerå’ŒxLSTMçš„ç¼©æ”¾è¡Œä¸ºï¼Œä¸ºæœªæ¥çš„æ¨¡å‹è®¾è®¡å’Œéƒ¨ç½²æä¾›äº†æŒ‡å¯¼ã€‚ç ”ç©¶å‘ç°ï¼Œåœ¨å…¸å‹çš„LLMè®­ç»ƒå’Œæ¨ç†åœºæ™¯ä¸­ï¼ŒxLSTMçš„ç¼©æ”¾æ€§èƒ½ä¼˜äºTransformerï¼Œå°¤å…¶æ˜¯åœ¨è®­ç»ƒå’Œæ¨ç†ä¸Šä¸‹æ–‡å¢é•¿æ—¶ï¼ŒxLSTMçš„ä¼˜åŠ¿æ›´ä¸ºæ˜æ˜¾ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¼©æ”¾å¾‹åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æˆåŠŸä¸­èµ·å…³é”®ä½œç”¨ï¼Œå¯é¢„æµ‹æ¨¡å‹æ€§èƒ½ä¸è®¡ç®—é¢„ç®—çš„å…³ç³»ã€‚</li>
<li>xLSTMä½œä¸ºä¸€ç§æ–°çš„æ¶æ„ï¼Œåœ¨ä¸Šä¸‹æ–‡é•¿åº¦æ–¹é¢æä¾›äº†çº¿æ€§å¤æ‚æ€§ï¼Œä¸Transformerç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ã€‚</li>
<li>æœ¬æ–‡æ¯”è¾ƒäº†Transformerå’ŒxLSTMçš„ç¼©æ”¾è¡Œä¸ºï¼Œæ¶µç›–è®¡ç®—æœ€ä¼˜å’Œè¿‡åº¦è®­ç»ƒçŠ¶æ€ä¸‹çš„ç ”ç©¶ã€‚</li>
<li>ç ”ç©¶å‘ç°ï¼Œåœ¨æœ€å…¸å‹çš„LLMè®­ç»ƒå’Œæ¨ç†åœºæ™¯ä¸­ï¼ŒxLSTMçš„ç¼©æ”¾æ€§èƒ½è¾ƒTransformeræ›´ä¸ºä¼˜è¶Šã€‚</li>
<li>åœ¨æ¨¡å‹å°ºå¯¸å’Œè®­ç»ƒä»¤ç‰Œæ•°é‡å¢åŠ æ—¶ï¼ŒxLSTMçš„ä¼˜åŠ¿æ›´ä¸ºæ˜æ˜¾ã€‚</li>
<li>ä»¥å¾€çš„ç ”ç©¶å¤§å¤šå¿½è§†äº†æœ€ä½³æ¨¡å‹å¤§å°ä¸ä¸Šä¸‹æ–‡é•¿åº¦çš„å…³ç³»ï¼Œè€Œæœ¬æ–‡å¯¹æ­¤è¿›è¡Œäº†æ·±å…¥çš„åˆ†æã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.02228">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_LLM/2510.02228v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_LLM/2510.02228v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_LLM/2510.02228v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_LLM/2510.02228v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_LLM/2510.02228v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="More-Than-One-Teacher-Adaptive-Multi-Guidance-Policy-Optimization-for-Diverse-Exploration"><a href="#More-Than-One-Teacher-Adaptive-Multi-Guidance-Policy-Optimization-for-Diverse-Exploration" class="headerlink" title="More Than One Teacher: Adaptive Multi-Guidance Policy Optimization for   Diverse Exploration"></a>More Than One Teacher: Adaptive Multi-Guidance Policy Optimization for   Diverse Exploration</h2><p><strong>Authors:Xiaoyang Yuan, Yujuan Ding, Yi Bin, Wenqi Shao, Jinyu Cai, Jingkuan Song, Yang Yang, Hengtao Shen</strong></p>
<p>Reinforcement Learning with Verifiable Rewards (RLVR) is a promising paradigm for enhancing the reasoning ability in Large Language Models (LLMs). However, prevailing methods primarily rely on self-exploration or a single off-policy teacher to elicit long chain-of-thought (LongCoT) reasoning, which may introduce intrinsic model biases and restrict exploration, ultimately limiting reasoning diversity and performance. Drawing inspiration from multi-teacher strategies in knowledge distillation, we introduce Adaptive Multi-Guidance Policy Optimization (AMPO), a novel framework that adaptively leverages guidance from multiple proficient teacher models, but only when the on-policy model fails to generate correct solutions. This â€œguidance-on-demandâ€ approach expands exploration while preserving the value of self-discovery. Moreover, AMPO incorporates a comprehension-based selection mechanism, prompting the student to learn from the reasoning paths that it is most likely to comprehend, thus balancing broad exploration with effective exploitation. Extensive experiments show AMPO substantially outperforms a strong baseline (GRPO), with a 4.3% improvement on mathematical reasoning tasks and 12.2% on out-of-distribution tasks, while significantly boosting Pass@k performance and enabling more diverse exploration. Notably, using four peer-sized teachers, our method achieves comparable results to approaches that leverage a single, more powerful teacher (e.g., DeepSeek-R1) with more data. These results demonstrate a more efficient and scalable path to superior reasoning and generalizability. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/SII-Enigma/AMPO">https://github.com/SII-Enigma/AMPO</a>. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰æ˜¯ä¸€ç§æœ‰æœ›å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†èƒ½åŠ›çš„èŒƒå¼ã€‚ç„¶è€Œï¼Œå½“å‰ä¸»æµæ–¹æ³•ä¸»è¦ä¾èµ–äºè‡ªæˆ‘æ¢ç´¢æˆ–å•ä¸€çš„éç­–ç•¥æ•™å¸ˆæ¥æ¿€å‘é•¿é“¾æ€ç»´ï¼ˆLongCoTï¼‰æ¨ç†ï¼Œè¿™å¯èƒ½ä¼šå¼•å…¥å†…åœ¨æ¨¡å‹åè§å¹¶é™åˆ¶æ¢ç´¢ï¼Œä»è€Œæœ€ç»ˆé™åˆ¶æ¨ç†çš„å¤šæ ·æ€§å’Œæ€§èƒ½ã€‚ä»çŸ¥è¯†è’¸é¦ä¸­çš„å¤šæ•™å¸ˆç­–ç•¥ä¸­æ±²å–çµæ„Ÿï¼Œæˆ‘ä»¬å¼•å…¥äº†è‡ªé€‚åº”å¤šå¼•å¯¼ç­–ç•¥ä¼˜åŒ–ï¼ˆAMPOï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æ¡†æ¶ï¼Œè‡ªé€‚åº”åœ°åˆ©ç”¨å¤šä¸ªç†Ÿç»ƒæ•™å¸ˆæ¨¡å‹çš„æŒ‡å¯¼ï¼Œä½†ä»…åœ¨ç­–ç•¥å†…æ¨¡å‹æ— æ³•ç”Ÿæˆæ­£ç¡®è§£å†³æ–¹æ¡ˆæ—¶æ‰è¿™æ ·åšã€‚è¿™ç§â€œæŒ‰éœ€æŒ‡å¯¼â€çš„æ–¹æ³•åœ¨ä¿æŒè‡ªæˆ‘å‘ç°ä»·å€¼çš„åŒæ—¶æ‰©å¤§äº†æ¢ç´¢èŒƒå›´ã€‚æ­¤å¤–ï¼ŒAMPOèå…¥äº†ä¸€ç§åŸºäºç†è§£çš„é€‰æ‹©æœºåˆ¶ï¼Œé¼“åŠ±å­¦ç”Ÿä»å®ƒæœ€å¯èƒ½ç†è§£çš„æ¨ç†è·¯å¾„ä¸­å­¦ä¹ ï¼Œä»è€Œåœ¨å¹¿æ³›æ¢ç´¢ä¸æœ‰æ•ˆåˆ©ç”¨ä¹‹é—´å–å¾—å¹³è¡¡ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒAMPOæ˜¾è‘—ä¼˜äºå¼ºå¤§çš„åŸºçº¿ï¼ˆGRPOï¼‰ï¼Œåœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šæé«˜äº†4.3%ï¼Œåœ¨åˆ†å¸ƒå¤–ä»»åŠ¡ä¸Šæé«˜äº†12.2%ï¼ŒåŒæ—¶æ˜¾è‘—æé«˜äº†Pass@kæ€§èƒ½å¹¶å®ç°äº†æ›´å¤šæ ·åŒ–çš„æ¢ç´¢ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä½¿ç”¨å››ä½åŒè¡Œå¤§å°çš„æ•™å¸ˆï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸åˆ©ç”¨å•ä¸ªæ›´å¼ºå¤§æ•™å¸ˆçš„æ–¹æ³•ï¼ˆä¾‹å¦‚ï¼ŒDeepSeek-R1ï¼‰çš„ç»“æœç›¸å½“ï¼Œè€Œä¸”æˆ‘ä»¬çš„æ–¹æ³•æ‰€éœ€æ•°æ®æ›´å°‘ã€‚è¿™äº›ç»“æœè¯æ˜äº†ä¸€æ¡æ›´é«˜æ•ˆã€æ›´å¯æ‰©å±•çš„é€šå¾€å“è¶Šæ¨ç†å’Œæ³›åŒ–çš„é“è·¯ã€‚æˆ‘ä»¬çš„ä»£ç ä½äº<a target="_blank" rel="noopener" href="https://github.com/SII-Enigma/AMPO%E3%80%82">https://github.com/SII-Enigma/AMPOã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.02227v1">PDF</a> 20 pages, 5 figures</p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰èŒƒå¼ä¸ºæå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›å¸¦æ¥äº†å¸Œæœ›ã€‚ç„¶è€Œï¼Œå½“å‰ä¸»æµæ–¹æ³•ä¸»è¦ä¾èµ–äºè‡ªæˆ‘æ¢ç´¢æˆ–å•ä¸€ç¦»ç­–ç•¥æ•™å¸ˆæ¥æ¿€å‘é•¿é“¾æ€ç»´æ¨ç†ï¼Œè¿™å¯èƒ½å¼•å…¥æ¨¡å‹å†…åœ¨åè§å¹¶é™åˆ¶æ¢ç´¢ï¼Œæœ€ç»ˆé™åˆ¶æ¨ç†çš„å¤šæ ·æ€§å’Œæ€§èƒ½ã€‚å—çŸ¥è¯†è’¸é¦ä¸­å¤šä»»åŠ¡æ•™å¸ˆç­–ç•¥çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†è‡ªé€‚åº”å¤šå¼•å¯¼ç­–ç•¥ä¼˜åŒ–ï¼ˆAMPOï¼‰è¿™ä¸€æ–°æ¡†æ¶ï¼Œå®ƒèƒ½è‡ªé€‚åº”åœ°åˆ©ç”¨å¤šä¸ªç†Ÿç»ƒæ•™å¸ˆæ¨¡å‹çš„æŒ‡å¯¼ï¼Œä½†ä»…åœ¨ç­–ç•¥å†…æ¨¡å‹æ— æ³•ç”Ÿæˆæ­£ç¡®è§£å†³æ–¹æ¡ˆæ—¶æ‰è¿™æ ·åšã€‚è¿™ç§â€œæŒ‰éœ€æŒ‡å¯¼â€çš„æ–¹æ³•åœ¨æ‰©å¤§æ¢ç´¢çš„åŒæ—¶ä¿æŒäº†è‡ªæˆ‘å‘ç°çš„ä»·å€¼ã€‚æ­¤å¤–ï¼ŒAMPOè¿˜ç»“åˆäº†åŸºäºç†è§£çš„é€‰æ‹©æœºåˆ¶ï¼Œä¿ƒä½¿å­¦ç”Ÿå­¦ä¹ æœ€å¯èƒ½ç†è§£çš„æ¨ç†è·¯å¾„ï¼Œä»è€Œåœ¨å¹¿æ³›çš„æ¢ç´¢ä¸æœ‰æ•ˆçš„åˆ©ç”¨ä¹‹é—´å–å¾—å¹³è¡¡ã€‚å®éªŒè¡¨æ˜ï¼ŒAMPOæ˜¾è‘—ä¼˜äºå¼ºåŸºçº¿ï¼ˆGRPOï¼‰ï¼Œåœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šæé«˜äº†4.3%ï¼Œåœ¨è¶…å‡ºåˆ†å¸ƒçš„ä»»åŠ¡ä¸Šæé«˜äº†12.2%ï¼ŒåŒæ—¶æ˜¾è‘—æé«˜äº†Pass@kæ€§èƒ½å¹¶å®ç°äº†æ›´å¤šæ ·åŒ–çš„æ¢ç´¢ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä½¿ç”¨å››ä½åŒè¡Œè§„æ¨¡çš„æ•™å¸ˆï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†ä¸åˆ©ç”¨å•ä¸€æ›´å¼ºå¤§æ•™å¸ˆï¼ˆä¾‹å¦‚DeepSeek-R1ï¼‰çš„æ–¹æ³•ç›¸å½“çš„ç»“æœï¼Œä¸”æ‰€éœ€æ•°æ®é‡æ›´å°‘ã€‚è¿™äº›ç»“æœè¯æ˜äº†æ›´é«˜æ•ˆã€æ›´å¯æ‰©å±•çš„å®ç°å“è¶Šæ¨ç†å’Œæ³›åŒ–èƒ½åŠ›çš„é€”å¾„ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/SII-Enigma/AMPO">https://github.com/SII-Enigma/AMPO</a>è·å–ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>RLVRèŒƒå¼å¢å¼ºäº†LLMçš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>å½“å‰æ–¹æ³•ä¸»è¦ä¾èµ–è‡ªæˆ‘æ¢ç´¢æˆ–å•ä¸€æ•™å¸ˆæ¨¡å‹ï¼Œå¯èƒ½å¼•å…¥åè§å¹¶é™åˆ¶æ¢ç´¢ã€‚</li>
<li>AMPOæ¡†æ¶è‡ªé€‚åº”åœ°åˆ©ç”¨å¤šä¸ªæ•™å¸ˆæ¨¡å‹çš„æŒ‡å¯¼ï¼Œç‰¹åˆ«æ˜¯å½“ç­–ç•¥å†…æ¨¡å‹å¤±è´¥æ—¶ã€‚</li>
<li>AMPOç»“åˆâ€œæŒ‰éœ€æŒ‡å¯¼â€å’Œè‡ªå‘ç°ä»·å€¼ï¼Œæ‰©å¤§æ¢ç´¢ã€‚</li>
<li>AMPOé‡‡ç”¨åŸºäºç†è§£çš„é€‰æ‹©æœºåˆ¶ï¼Œå¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨ã€‚</li>
<li>AMPOåœ¨æ•°å­¦å’Œè¶…å‡ºåˆ†å¸ƒçš„ä»»åŠ¡ä¸Šè¡¨ç°æ˜¾è‘—ä¼˜äºåŸºçº¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.02227">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_LLM/2510.02227v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_LLM/2510.02227v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_LLM/2510.02227v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Contrastive-Retrieval-Heads-Improve-Attention-Based-Re-Ranking"><a href="#Contrastive-Retrieval-Heads-Improve-Attention-Based-Re-Ranking" class="headerlink" title="Contrastive Retrieval Heads Improve Attention-Based Re-Ranking"></a>Contrastive Retrieval Heads Improve Attention-Based Re-Ranking</h2><p><strong>Authors:Linh Tran, Yulong Li, Radu Florian, Wei Sun</strong></p>
<p>The strong zero-shot and long-context capabilities of recent Large Language Models (LLMs) have paved the way for highly effective re-ranking systems. Attention-based re-rankers leverage attention weights from transformer heads to produce relevance scores, but not all heads are created equally: many contribute noise and redundancy, thus limiting performance. To address this, we introduce CoRe heads, a small set of retrieval heads identified via a contrastive scoring metric that explicitly rewards high attention heads that correlate with relevant documents, while downplaying nodes with higher attention that correlate with irrelevant documents. This relative ranking criterion isolates the most discriminative heads for re-ranking and yields a state-of-the-art list-wise re-ranker. Extensive experiments with three LLMs show that aggregated signals from CoRe heads, constituting less than 1% of all heads, substantially improve re-ranking accuracy over strong baselines. We further find that CoRe heads are concentrated in middle layers, and pruning the computation of final 50% of model layers preserves accuracy while significantly reducing inference time and memory usage. </p>
<blockquote>
<p>æœ€è¿‘çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¼ºå¤§é›¶æ ·æœ¬å’Œé•¿ä¸Šä¸‹æ–‡èƒ½åŠ›ä¸ºé«˜æ•ˆçš„é‡æ–°æ’åç³»ç»Ÿé“ºå¹³äº†é“è·¯ã€‚åŸºäºæ³¨æ„åŠ›çš„é‡æ–°æ’åå™¨åˆ©ç”¨å˜å‹å™¨å¤´çš„æ³¨æ„åŠ›æƒé‡æ¥äº§ç”Ÿç›¸å…³æ€§åˆ†æ•°ï¼Œä½†å¹¶éæ‰€æœ‰çš„å¤´éƒ½æ˜¯åŒç­‰åˆ›é€ çš„ï¼šè®¸å¤šå¤´ä¼šäº§ç”Ÿå™ªå£°å’Œå†—ä½™ï¼Œä»è€Œé™åˆ¶äº†æ€§èƒ½ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†CoReå¤´ï¼Œè¿™æ˜¯ä¸€ç»„é€šè¿‡å¯¹æ¯”è¯„åˆ†æŒ‡æ ‡è¯†åˆ«å‡ºçš„å°‘é‡æ£€ç´¢å¤´ï¼Œå®ƒæ˜ç¡®å¥–åŠ±é‚£äº›ä¸ç›¸å…³æ–‡æ¡£ç›¸å…³çš„é«˜æ³¨æ„åŠ›å¤´ï¼ŒåŒæ—¶æ·¡åŒ–ä¸æ— å…³æ–‡æ¡£ç›¸å…³çš„é«˜æ³¨æ„åŠ›èŠ‚ç‚¹ã€‚è¿™ç§ç›¸å¯¹æ’åæ ‡å‡†å°†æœ€å…·è¾¨åˆ«åŠ›çš„å¤´ç”¨äºé‡æ–°æ’åï¼Œå¹¶äº§ç”Ÿæœ€å…ˆè¿›çš„åˆ—è¡¨å¼é‡æ–°æ’åå™¨ã€‚åœ¨ä¸‰ä¸ªLLMä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œç”±ä¸åˆ°1%çš„CoReå¤´äº§ç”Ÿçš„èšåˆä¿¡å·åœ¨å¼ºåŸºçº¿çš„åŸºç¡€ä¸Šæ˜¾è‘—æé«˜äº†é‡æ–°æ’åçš„å‡†ç¡®æ€§ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥å‘ç°CoReå¤´é›†ä¸­åœ¨ä¸­é—´å±‚ï¼Œè€Œåˆ é™¤æ¨¡å‹å±‚æœ€å50%çš„è®¡ç®—å¯ä»¥åœ¨ä¿æŒå‡†ç¡®æ€§çš„åŒæ—¶æ˜¾è‘—å‡å°‘æ¨ç†æ—¶é—´å’Œå†…å­˜ä½¿ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.02219v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ€è¿‘çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å…·å¤‡å¼ºå¤§çš„é›¶å¯åŠ¨å’Œé•¿æ–‡æœ¬ä¸Šä¸‹æ–‡èƒ½åŠ›ï¼Œä¸ºé«˜æ•ˆçš„é‡æ–°æ’åç³»ç»Ÿé“ºå¹³äº†é“è·¯ã€‚åŸºäºæ³¨æ„åŠ›çš„é‡æ–°æ’åå™¨åˆ©ç”¨å˜å‹å™¨å¤´éƒ¨çš„æ³¨æ„åŠ›æƒé‡æ¥ç”Ÿæˆç›¸å…³æ€§å¾—åˆ†ï¼Œä½†å¹¶éæ‰€æœ‰å¤´éƒ¨éƒ½æ˜¯æœ‰ç”¨çš„ï¼šè®¸å¤šå¤´éƒ¨ä¼šäº§ç”Ÿå™ªå£°å’Œå†—ä½™ï¼Œä»è€Œé™åˆ¶äº†æ€§èƒ½ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†CoReå¤´éƒ¨ï¼Œé€šè¿‡å¯¹æ¯”è¯„åˆ†æŒ‡æ ‡è¯†åˆ«çš„ä¸€ç»„å°å‹æ£€ç´¢å¤´éƒ¨ï¼Œæ˜ç¡®å¥–åŠ±ä¸ç›¸å…³æ–‡æ¡£é«˜åº¦ç›¸å…³çš„é«˜æ³¨æ„åŠ›å¤´éƒ¨ï¼ŒåŒæ—¶å¿½ç•¥ä¸æ— å…³æ–‡æ¡£é«˜åº¦ç›¸å…³çš„èŠ‚ç‚¹ã€‚è¿™ç§ç›¸å¯¹æ’åæ ‡å‡†å°†æœ€å…·åŒºåˆ†èƒ½åŠ›çš„å¤´éƒ¨éš”ç¦»å¼€æ¥ï¼Œç”¨äºé‡æ–°æ’åï¼Œå¹¶äº§ç”Ÿäº†æœ€å…ˆè¿›çš„åˆ—è¡¨å¼é‡æ–°æ’åå™¨ã€‚åœ¨ä¸‰ä¸ªLLMä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæ„æˆä¸åˆ°1%çš„CoReå¤´éƒ¨çš„èšåˆä¿¡å·å¤§å¤§æé«˜äº†é‡æ–°æ’åçš„å‡†ç¡®æ€§ï¼Œè¶…è¿‡äº†å¼ºå¤§çš„åŸºå‡†æµ‹è¯•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMå…·å¤‡å¼ºå¤§çš„é›¶å¯åŠ¨å’Œé•¿æ–‡æœ¬ä¸Šä¸‹æ–‡èƒ½åŠ›ï¼Œä¸ºé‡æ–°æ’åç³»ç»Ÿæä¾›äº†æœ‰æ•ˆåŸºç¡€ã€‚</li>
<li>åŸºäºæ³¨æ„åŠ›çš„é‡æ–°æ’åå™¨åˆ©ç”¨å˜å‹å™¨å¤´éƒ¨çš„æ³¨æ„åŠ›æƒé‡ç”Ÿæˆç›¸å…³æ€§å¾—åˆ†ã€‚</li>
<li>å¹¶éæ‰€æœ‰å˜å‹å™¨å¤´éƒ¨éƒ½æ˜¯æœ‰ç”¨çš„ï¼Œè®¸å¤šä¼šäº§ç”Ÿå™ªå£°å’Œå†—ä½™ï¼Œå½±å“æ€§èƒ½ã€‚</li>
<li>CoReå¤´éƒ¨é€šè¿‡å¯¹æ¯”è¯„åˆ†æŒ‡æ ‡è¯†åˆ«çš„å°å‹æ£€ç´¢å¤´éƒ¨ï¼Œå¯ä»¥æé«˜æ³¨æ„åŠ›ä¸ç›¸å…³æ–‡æ¡£çš„åŒ¹é…åº¦ã€‚</li>
<li>CoReå¤´éƒ¨ä»…å æ‰€æœ‰å¤´éƒ¨çš„ä¸åˆ°1%ï¼Œå´èƒ½æ˜¾è‘—æé«˜é‡æ–°æ’åçš„å‡†ç¡®æ€§ã€‚</li>
<li>CoReå¤´éƒ¨ä¸»è¦é›†ä¸­åœ¨æ¨¡å‹çš„ä¸­å±‚ï¼Œé™ä½æœ€å50%çš„æ¨¡å‹å±‚è®¡ç®—èƒ½ä¿ç•™å‡†ç¡®æ€§ï¼ŒåŒæ—¶æ˜¾è‘—å‡å°‘æ¨ç†æ—¶é—´å’Œå†…å­˜ä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.02219">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_LLM/2510.02219v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_LLM/2510.02219v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_LLM/2510.02219v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="DiFFPO-Training-Diffusion-LLMs-to-Reason-Fast-and-Furious-via-Reinforcement-Learning"><a href="#DiFFPO-Training-Diffusion-LLMs-to-Reason-Fast-and-Furious-via-Reinforcement-Learning" class="headerlink" title="DiFFPO: Training Diffusion LLMs to Reason Fast and Furious via   Reinforcement Learning"></a>DiFFPO: Training Diffusion LLMs to Reason Fast and Furious via   Reinforcement Learning</h2><p><strong>Authors:Hanyang Zhao, Dawen Liang, Wenpin Tang, David Yao, Nathan Kallus</strong></p>
<p>We propose DiFFPO, Diffusion Fast and Furious Policy Optimization, a unified framework for training masked diffusion large language models (dLLMs) to reason not only better (furious), but also faster via reinforcement learning (RL). We first unify the existing baseline approach such as d1 by proposing to train surrogate policies via off-policy RL, whose likelihood is much more tractable as an approximation to the true dLLM policy. This naturally motivates a more accurate and informative two-stage likelihood approximation combined with importance sampling correction, which leads to generalized RL algorithms with better sample efficiency and superior task performance. Second, we propose a new direction of joint training efficient samplers&#x2F;controllers of dLLMs policy. Via RL, we incentivize dLLMsâ€™ natural multi-token prediction capabilities by letting the model learn to adaptively allocate an inference threshold for each prompt. By jointly training the sampler, we yield better accuracies with lower number of function evaluations (NFEs) compared to training the model only, obtaining the best performance in improving the Pareto frontier of the inference-time compute of dLLMs. We showcase the effectiveness of our pipeline by training open source large diffusion language models over benchmark math and planning tasks. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºDiFFPOï¼ˆæ‰©æ•£å¿«é€Ÿä¸é«˜æ•ˆç­–ç•¥ä¼˜åŒ–ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œç”¨äºè®­ç»ƒæ©ç æ‰©æ•£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆdLLMï¼‰ï¼Œä¸ä»…æ¨ç†æ•ˆæœæ›´å¥½ï¼ˆé«˜æ•ˆï¼‰ï¼Œè€Œä¸”é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¨ç†é€Ÿåº¦æ›´å¿«ã€‚æˆ‘ä»¬é¦–å…ˆé€šè¿‡æå‡ºé€šè¿‡ç¦»çº¿RLè®­ç»ƒä»£ç†ç­–ç•¥æ¥ç»Ÿä¸€ç°æœ‰çš„åŸºçº¿æ–¹æ³•ï¼Œå¦‚d1ï¼Œè¯¥ç­–ç•¥çš„ä¼¼ç„¶æ€§ä½œä¸ºå¯¹çœŸå®dLLMç­–ç•¥çš„è¿‘ä¼¼æ›´å®¹æ˜“å¤„ç†ã€‚è¿™è‡ªç„¶åœ°ä¿ƒä½¿æˆ‘ä»¬ç»“åˆé‡è¦æ€§é‡‡æ ·æ ¡æ­£ï¼Œé‡‡ç”¨æ›´å‡†ç¡®ã€æ›´å…¨é¢çš„ä¸¤é˜¶æ®µä¼¼ç„¶æ€§è¿‘ä¼¼ï¼Œä»è€Œå¾—åˆ°å…·æœ‰æ›´å¥½æ ·æœ¬æ•ˆç‡å’Œä»»åŠ¡æ€§èƒ½çš„å¹¿ä¹‰RLç®—æ³•ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æå‡ºäº†è”åˆè®­ç»ƒdLLMç­–ç•¥çš„é«˜æ•ˆé‡‡æ ·å™¨&#x2F;æ§åˆ¶å™¨çš„æ–°æ–¹å‘ã€‚é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼Œæˆ‘ä»¬æ¿€åŠ±dLLMçš„è‡ªç„¶å¤šä»¤ç‰Œé¢„æµ‹èƒ½åŠ›ï¼Œè®©æ¨¡å‹å­¦ä¹ è‡ªé€‚åº”åœ°ä¸ºæ¯ä¸ªæç¤ºåˆ†é…æ¨ç†é˜ˆå€¼ã€‚é€šè¿‡è”åˆè®­ç»ƒé‡‡æ ·å™¨ï¼Œæˆ‘ä»¬åœ¨å‡½æ•°è¯„ä¼°æ¬¡æ•°ï¼ˆNFEsï¼‰è¾ƒå°‘çš„æƒ…å†µä¸‹è·å¾—äº†æ›´é«˜çš„å‡†ç¡®ç‡ï¼Œä¸ä»…è®­ç»ƒæ¨¡å‹ç›¸æ¯”ï¼Œåœ¨æ”¹å–„dLLMæ¨ç†æ—¶é—´è®¡ç®—æ–¹é¢å–å¾—äº†æœ€ä½³çš„å¸•ç´¯æ‰˜å‰æ²¿æ•ˆæœã€‚æˆ‘ä»¬é€šè¿‡å…¬å¼€çš„å¤§å‹æ‰©æ•£è¯­è¨€æ¨¡å‹åœ¨åŸºå‡†æ•°å­¦å’Œè§„åˆ’ä»»åŠ¡ä¸Šçš„è®­ç»ƒå±•ç¤ºäº†æˆ‘ä»¬çš„ç®¡é“çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.02212v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†DiFFPOæ¡†æ¶ï¼Œå³æ‰©æ•£å¿«é€Ÿç‹‚æš´æ”¿ç­–ä¼˜åŒ–ï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œç”¨äºè®­ç»ƒæ©ç æ‰©æ•£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆdLLMsï¼‰ï¼Œä½¿å…¶ä¸ä»…æ›´å¥½åœ°æ¨ç†ï¼ˆç‹‚æš´ï¼‰ï¼Œè€Œä¸”é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ›´å¿«åœ°è¿›è¡Œæ¨ç†ã€‚æœ¬æ–‡é¦–å…ˆé€šè¿‡æå‡ºé€šè¿‡ç¦»çº¿å¼ºåŒ–å­¦ä¹ è®­ç»ƒä»£ç†æ”¿ç­–æ¥ç»Ÿä¸€ç°æœ‰çš„åŸºçº¿æ–¹æ³•ï¼Œä½œä¸ºå¯¹çœŸå®dLLMæ”¿ç­–çš„è¿‘ä¼¼ï¼Œå…¶å¯èƒ½æ€§æ›´åŠ æ˜“äºå¤„ç†ã€‚è¿™è‡ªç„¶åœ°ä¿ƒä½¿äº†ä¸€ä¸ªæ›´å‡†ç¡®å’Œæ›´å…·ä¿¡æ¯é‡çš„ä¸¤é˜¶æ®µå¯èƒ½æ€§è¿‘ä¼¼ï¼Œç»“åˆé‡è¦æ€§é‡‡æ ·æ ¡æ­£ï¼Œäº§ç”Ÿäº†å…·æœ‰æ›´é«˜æ ·æœ¬æ•ˆç‡å’Œæ›´å¥½ä»»åŠ¡æ€§èƒ½çš„å¹¿ä¹‰å¼ºåŒ–å­¦ä¹ ç®—æ³•ã€‚å…¶æ¬¡ï¼Œæœ¬æ–‡æå‡ºäº†è”åˆè®­ç»ƒdLLMsæ”¿ç­–çš„æ•ˆç‡é‡‡æ ·å™¨&#x2F;æ§åˆ¶å™¨çš„æ–°æ–¹å‘ã€‚é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼Œæˆ‘ä»¬æ¿€åŠ±dLLMsçš„è‡ªç„¶å¤šä»¤ç‰Œé¢„æµ‹èƒ½åŠ›ï¼Œè®©æ¨¡å‹å­¦ä¹ ä¸ºæ¯ä¸ªæç¤ºè‡ªé€‚åº”åœ°åˆ†é…æ¨ç†é˜ˆå€¼ã€‚é€šè¿‡è”åˆè®­ç»ƒé‡‡æ ·å™¨ï¼Œä¸ä»…è®­ç»ƒæ¨¡å‹ç›¸æ¯”ï¼Œæˆ‘ä»¬åœ¨å‡½æ•°è¯„ä¼°æ¬¡æ•°ï¼ˆNFEsï¼‰è¾ƒä½çš„æƒ…å†µä¸‹è·å¾—äº†æ›´å¥½çš„ç²¾åº¦ï¼Œåœ¨æ”¹å–„dLLMsæ¨ç†æ—¶é—´è®¡ç®—æ–¹é¢çš„å¸•ç´¯æ‰˜å‰æ²¿æ–¹é¢å–å¾—äº†æœ€ä½³æ€§èƒ½ã€‚æˆ‘ä»¬åœ¨åŸºå‡†æ•°å­¦å’Œè§„åˆ’ä»»åŠ¡ä¸Šå±•ç¤ºäº†æˆ‘ä»¬çš„ç®¡é“è®­ç»ƒå¼€æºå¤§å‹æ‰©æ•£è¯­è¨€æ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†DiFFPOæ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè®­ç»ƒæ©ç æ‰©æ•£å¤§å‹è¯­è¨€æ¨¡å‹çš„ç»Ÿä¸€æ¡†æ¶ï¼Œç»“åˆäº†å¼ºåŒ–å­¦ä¹ ï¼Œæé«˜æ¨ç†é€Ÿåº¦å’Œæ€§èƒ½ã€‚</li>
<li>é€šè¿‡ç¦»çº¿å¼ºåŒ–å­¦ä¹ è®­ç»ƒä»£ç†æ”¿ç­–ï¼Œä½œä¸ºå¯¹çœŸå®dLLMæ”¿ç­–çš„è¿‘ä¼¼ï¼Œä½¿å¯èƒ½æ€§æ›´åŠ æ˜“äºå¤„ç†ã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªæ›´å‡†ç¡®å’Œæ›´å…·ä¿¡æ¯é‡çš„ä¸¤é˜¶æ®µå¯èƒ½æ€§è¿‘ä¼¼æ–¹æ³•ï¼Œç»“åˆäº†é‡è¦æ€§é‡‡æ ·æ ¡æ­£ï¼Œæé«˜äº†å¼ºåŒ–å­¦ä¹ ç®—æ³•çš„æ ·æœ¬æ•ˆç‡å’Œä»»åŠ¡æ€§èƒ½ã€‚</li>
<li>ä»‹ç»äº†è”åˆè®­ç»ƒdLLMsæ”¿ç­–çš„æ•ˆç‡é‡‡æ ·å™¨&#x2F;æ§åˆ¶å™¨çš„æ–°æ–¹å‘ï¼Œæ¿€åŠ±æ¨¡å‹çš„è‡ªç„¶å¤šä»¤ç‰Œé¢„æµ‹èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡RLæ¿€åŠ±æ¨¡å‹è‡ªé€‚åº”åœ°åˆ†é…æ¨ç†é˜ˆå€¼ç»™æ¯ä¸ªæç¤ºï¼Œå®ç°äº†æ›´å¥½çš„ç²¾åº¦å’Œè¾ƒä½çš„å‡½æ•°è¯„ä¼°æ¬¡æ•°ï¼ˆNFEsï¼‰ã€‚</li>
<li>åœ¨å¸•ç´¯æ‰˜å‰æ²¿æ–¹é¢å–å¾—äº†æ”¹è¿›dLLMsæ¨ç†æ—¶é—´è®¡ç®—çš„æœ€ä½³æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.02212">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_LLM/2510.02212v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_LLM/2510.02212v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_LLM/2510.02212v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="StockBench-Can-LLM-Agents-Trade-Stocks-Profitably-In-Real-world-Markets"><a href="#StockBench-Can-LLM-Agents-Trade-Stocks-Profitably-In-Real-world-Markets" class="headerlink" title="StockBench: Can LLM Agents Trade Stocks Profitably In Real-world   Markets?"></a>StockBench: Can LLM Agents Trade Stocks Profitably In Real-world   Markets?</h2><p><strong>Authors:Yanxu Chen, Zijun Yao, Yantao Liu, Jin Ye, Jianing Yu, Lei Hou, Juanzi Li</strong></p>
<p>Large language models (LLMs) have recently demonstrated strong capabilities as autonomous agents, showing promise in reasoning, tool use, and sequential decision-making. While prior benchmarks have evaluated LLM agents in domains such as software engineering and scientific discovery, the finance domain remains underexplored, despite its direct relevance to economic value and high-stakes decision-making. Existing financial benchmarks primarily test static knowledge through question answering, but they fall short of capturing the dynamic and iterative nature of trading. To address this gap, we introduce StockBench, a contamination-free benchmark designed to evaluate LLM agents in realistic, multi-month stock trading environments. Agents receive daily market signals â€“ including prices, fundamentals, and news â€“ and must make sequential buy, sell, or hold decisions. Performance is assessed using financial metrics such as cumulative return, maximum drawdown, and the Sortino ratio. Our evaluation of state-of-the-art proprietary (e.g., GPT-5, Claude-4) and open-weight (e.g., Qwen3, Kimi-K2, GLM-4.5) models shows that while most LLM agents struggle to outperform the simple buy-and-hold baseline, several models demonstrate the potential to deliver higher returns and manage risk more effectively. These findings highlight both the challenges and opportunities in developing LLM-powered financial agents, showing that excelling at static financial knowledge tasks does not necessarily translate into successful trading strategies. We release StockBench as an open-source resource to support reproducibility and advance future research in this domain. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æœ€è¿‘å±•ç°å‡ºä½œä¸ºè‡ªä¸»ä»£ç†çš„å¼ºå¤§èƒ½åŠ›ï¼Œåœ¨æ¨ç†ã€å·¥å…·ä½¿ç”¨å’Œåºåˆ—å†³ç­–ç­‰æ–¹é¢è¡¨ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚è™½ç„¶ä¹‹å‰çš„åŸºå‡†æµ‹è¯•å·²ç»åœ¨è½¯ä»¶å·¥ç¨‹å’Œç§‘å­¦å‘ç°ç­‰é¢†åŸŸè¯„ä¼°äº†LLMä»£ç†ï¼Œä½†é‡‘èé¢†åŸŸä»ç„¶è¢«å¿½è§†ï¼Œå°½ç®¡å®ƒä¸ç»æµä»·å€¼å’Œé«˜é£é™©å†³ç­–ç›´æ¥ç›¸å…³ã€‚ç°æœ‰çš„é‡‘èåŸºå‡†æµ‹è¯•ä¸»è¦é€šè¿‡é—®ç­”æ¥æµ‹è¯•é™æ€çŸ¥è¯†ï¼Œä½†å®ƒä»¬æ— æ³•æ•æ‰äº¤æ˜“çš„åŠ¨æ€å’Œè¿­ä»£æ€§è´¨ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æ¨å‡ºäº†StockBenchï¼Œè¿™æ˜¯ä¸€ä¸ªæ— æ±¡æŸ“åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°å¤šä¸ªæœˆç°å®è‚¡ç¥¨äº¤æ˜“ç¯å¢ƒä¸­çš„LLMä»£ç†ã€‚ä»£ç†æ¯å¤©æ¥æ”¶å¸‚åœºä¿¡å·ï¼ŒåŒ…æ‹¬ä»·æ ¼ã€åŸºæœ¬é¢å’Œæ–°é—»ï¼Œå¹¶å¿…é¡»åšå‡ºè¿ç»­çš„ä¹°å…¥ã€å–å‡ºæˆ–æŒæœ‰å†³ç­–ã€‚æ€§èƒ½è¯„ä¼°ä½¿ç”¨é‡‘èæŒ‡æ ‡ï¼Œå¦‚ç´¯è®¡æ”¶ç›Šã€æœ€å¤§å›æ’¤å’Œç´¢æè¯ºæ¯”ç‡ç­‰ã€‚æˆ‘ä»¬å¯¹æœ€æ–°çš„ä¸“æœ‰ï¼ˆä¾‹å¦‚GPT-5ã€Claude-4ï¼‰å’Œå…¬å¼€æƒé‡ï¼ˆä¾‹å¦‚Qwen3ã€Kimi-K2ã€GLM-4.5ï¼‰æ¨¡å‹çš„è¯„ä¼°æ˜¾ç¤ºï¼Œè™½ç„¶å¤§å¤šæ•°LLMä»£ç†åœ¨è¶…è¶Šç®€å•çš„ä¹°å…¥å¹¶æŒæœ‰åŸºå‡†çº¿æ–¹é¢è¡¨ç°å›°éš¾ï¼Œä½†ä¸€äº›æ¨¡å‹æ˜¾ç¤ºå‡ºå®ç°æ›´é«˜å›æŠ¥å’Œæ›´æœ‰æ•ˆåœ°ç®¡ç†é£é™©çš„æ½œåŠ›ã€‚è¿™äº›å‘ç°æ—¢çªå‡ºäº†å¼€å‘LLMé‡‘èä»£ç†æ‰€é¢ä¸´çš„æŒ‘æˆ˜å’Œæœºé‡ï¼Œä¹Ÿè¡¨æ˜åœ¨é™æ€é‡‘èçŸ¥è¯†ä»»åŠ¡ä¸Šçš„å‡ºè‰²è¡¨ç°å¹¶ä¸ä¸€å®šè½¬åŒ–ä¸ºæˆåŠŸçš„äº¤æ˜“ç­–ç•¥ã€‚æˆ‘ä»¬å‘å¸ƒStockBenchä½œä¸ºå¼€æºèµ„æºï¼Œä»¥æ”¯æŒå¯é‡å¤æ€§å’Œæ¨åŠ¨è¿™ä¸€é¢†åŸŸçš„æœªæ¥ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.02209v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºè‡ªä¸»ä»£ç†å±•ç°äº†å¼ºå¤§çš„èƒ½åŠ›ï¼Œåœ¨æ¨ç†ã€å·¥å…·ä½¿ç”¨å’Œåºåˆ—å†³ç­–æ–¹é¢è¡¨ç°å‡ºæ½œåŠ›ã€‚å°½ç®¡å·²æœ‰åŸºå‡†æµ‹è¯•äº†LLMä»£ç†åœ¨è½¯ä»¶å·¥ç¨‹å’Œç§‘å­¦å‘ç°ç­‰é¢†åŸŸçš„èƒ½åŠ›ï¼Œä½†é‡‘èé¢†åŸŸä»è¢«å¿½è§†ã€‚ç°æœ‰é‡‘èåŸºå‡†æµ‹è¯•ä¸»è¦é€šè¿‡é—®ç­”æ¥æµ‹è¯•é™æ€çŸ¥è¯†ï¼Œä½†æ— æ³•æ•æ‰äº¤æ˜“çš„åŠ¨æ€å’Œè¿­ä»£æ€§è´¨ã€‚ä¸ºè§£å†³æ­¤ç©ºç™½ï¼Œæˆ‘ä»¬æ¨å‡ºStockBenchåŸºå‡†æµ‹è¯•ï¼Œä¸“ä¸ºè¯„ä¼°LLMä»£ç†åœ¨çœŸå®ã€å¤šæœˆçš„è‚¡ç¥¨äº¤æ˜“ç¯å¢ƒä¸­çš„èƒ½åŠ›è€Œè®¾è®¡ã€‚ä»£ç†æ¥æ”¶æ¯æ—¥å¸‚åœºä¿¡å·ï¼ŒåŒ…æ‹¬ä»·æ ¼ã€åŸºæœ¬é¢å’Œæ–°é—»ï¼Œå¹¶éœ€åšå‡ºè¿ç»­çš„ä¹°å…¥ã€å–å‡ºæˆ–æŒæœ‰å†³ç­–ã€‚æ€§èƒ½è¯„ä¼°é‡‡ç”¨é‡‘èæŒ‡æ ‡ï¼Œå¦‚ç´¯è®¡å›æŠ¥ã€æœ€å¤§å›æ’¤å’Œç´¢æè¯ºæ¯”ç‡ã€‚å¯¹æœ€æ–°ç§æœ‰ï¼ˆå¦‚GPT-5ã€Claude-4ï¼‰å’Œå…¬å¼€æƒé‡ï¼ˆå¦‚Qwen3ã€Kimi-K2ã€GLM-4.5ï¼‰æ¨¡å‹çš„è¯„ä¼°æ˜¾ç¤ºï¼Œå¤§å¤šæ•°LLMä»£ç†éš¾ä»¥è¶…è¶Šç®€å•çš„ä¹°å…¥å¹¶æŒæœ‰åŸºçº¿ï¼Œä½†ä¸€äº›æ¨¡å‹å±•ç°å‡ºæ›´é«˜çš„å›æŠ¥å’Œæ›´æœ‰æ•ˆçš„é£é™©ç®¡ç†æ½œåŠ›ã€‚è¿™äº›å‘ç°çªæ˜¾äº†å¼€å‘LLMé‡‘èä»£ç†çš„æŒ‘æˆ˜å’Œæœºé‡ï¼Œè¡¨æ˜åœ¨é™æ€é‡‘èçŸ¥è¯†ä»»åŠ¡ä¸Šçš„ä¼˜ç§€å¹¶ä¸ä¸€å®šèƒ½è½¬åŒ–ä¸ºæˆåŠŸçš„äº¤æ˜“ç­–ç•¥ã€‚æˆ‘ä»¬å‘å¸ƒStockBenchä½œä¸ºå¼€æºèµ„æºï¼Œä»¥æ”¯æŒé‡ç°å¹¶æ¨åŠ¨è¯¥é¢†åŸŸæœªæ¥çš„ç ”ç©¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMså±•ç°å‡ºå¼ºå¤§çš„è‡ªä¸»ä»£ç†èƒ½åŠ›ï¼Œåœ¨æ¨ç†ã€å·¥å…·ä½¿ç”¨å’Œåºåˆ—å†³ç­–æ–¹é¢å…·æ½œåŠ›ã€‚</li>
<li>é‡‘èé¢†åŸŸåœ¨LLMç ”ç©¶ä¸­ä»è¢«å¿½è§†ï¼Œå°½ç®¡å…¶åœ¨ç»æµä»·å€¼å’Œé«˜é£é™©å†³ç­–ä¸­çš„é‡è¦æ€§ã€‚</li>
<li>ç°æœ‰é‡‘èåŸºå‡†æµ‹è¯•ä¸»è¦é€šè¿‡é—®ç­”æµ‹è¯•é™æ€çŸ¥è¯†ï¼Œæ— æ³•æ•æ‰äº¤æ˜“çš„åŠ¨æ€æ€§è´¨ã€‚</li>
<li>æ¨å‡ºStockBenchåŸºå‡†æµ‹è¯•ï¼Œè¯„ä¼°LLMåœ¨çœŸå®è‚¡ç¥¨äº¤æ˜“ç¯å¢ƒä¸­çš„æ€§èƒ½ã€‚</li>
<li>LLMä»£ç†éœ€å¤„ç†æ¯æ—¥å¸‚åœºä¿¡å·ï¼ŒåŒ…æ‹¬ä»·æ ¼ã€åŸºæœ¬é¢å’Œæ–°é—»ï¼Œåšå‡ºä¹°å–å†³ç­–ã€‚</li>
<li>è¯„ä¼°æ˜¾ç¤ºï¼Œå¤§å¤šæ•°LLMéš¾ä»¥è¶…è¶Šç®€å•åŸºçº¿ï¼Œä½†éƒ¨åˆ†å±•ç°å‡ºé«˜å›æŠ¥å’Œæœ‰æ•ˆé£é™©ç®¡ç†æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.02209">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_LLM/2510.02209v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_LLM/2510.02209v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_LLM/2510.02209v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_LLM/2510.02209v1/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_LLM/2510.02209v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="CAT-Curvature-Adaptive-Transformers-for-Geometry-Aware-Learning"><a href="#CAT-Curvature-Adaptive-Transformers-for-Geometry-Aware-Learning" class="headerlink" title="CAT: Curvature-Adaptive Transformers for Geometry-Aware Learning"></a>CAT: Curvature-Adaptive Transformers for Geometry-Aware Learning</h2><p><strong>Authors:Ryan Y. Lin, Siddhartha Ojha, Nicholas Bai</strong></p>
<p>Transformers achieve strong performance across diverse domains but implicitly assume Euclidean geometry in their attention mechanisms, limiting their effectiveness on data with non-Euclidean structure. While recent extensions to hyperbolic and spherical spaces show promise for hierarchical and cyclical patterns, respectively, they require committing to a single geometry a priori, reducing flexibility when data exhibits mixed geometric properties. We introduce the Curvature-Adaptive Transformer (CAT), a novel architecture that dynamically learns per-token routing across three geometric attention branches through a lightweight, differentiable gating mechanism. Unlike fixed-geometry approaches, CAT enables adaptive geometric specialization, routing tokens to the appropriate curvature based on their local relational structure. The routing network provides interpretable curvature preferences while each branch employs geometry-specific operations optimized for its respective manifold. On knowledge graph completion benchmarks (FB15k-237, WN18RR), CAT achieves approximately 10% improvements in MRR and Hits@10 over fixed-geometry baselines with minimal overhead (5% parameter increase, comparable inference time). These results demonstrate that learned geometric adaptation outperforms any single fixed geometry for complex relational reasoning, establishing CAT as a scalable and interpretable foundation for mixture-of-geometry architectures across language, vision, and multimodal domains. </p>
<blockquote>
<p>Transformeråœ¨ä¸åŒé¢†åŸŸå–å¾—äº†å¼ºå¤§çš„æ€§èƒ½ï¼Œä½†åœ¨å…¶æ³¨æ„æœºåˆ¶ä¸­éšå«åœ°å‡è®¾äº†æ¬§å‡ é‡Œå¾—å‡ ä½•ï¼Œè¿™é™åˆ¶äº†å…¶åœ¨å…·æœ‰éæ¬§å‡ é‡Œå¾—ç»“æ„æ•°æ®ä¸Šçš„æœ‰æ•ˆæ€§ã€‚è™½ç„¶æœ€è¿‘æ‰©å±•åˆ°åŒæ›²ç©ºé—´å’Œçƒé¢ç©ºé—´çš„å°è¯•é’ˆå¯¹å±‚æ¬¡æ¨¡å¼å’Œå¾ªç¯æ¨¡å¼åˆ†åˆ«æ˜¾ç¤ºå‡ºå¸Œæœ›ï¼Œä½†å®ƒä»¬éœ€è¦äº‹å…ˆç¡®å®šä¸€ç§å•ä¸€çš„å‡ ä½•å½¢æ€ï¼Œå½“æ•°æ®è¡¨ç°å‡ºæ··åˆçš„å‡ ä½•å±æ€§æ—¶ï¼Œçµæ´»æ€§é™ä½ã€‚æˆ‘ä»¬å¼•å…¥äº†æ›²ç‡è‡ªé€‚åº”Transformerï¼ˆCATï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æ¶æ„ï¼Œå®ƒé€šè¿‡è½»é‡çº§ã€å¯å¾®åˆ†çš„é—¨æ§æœºåˆ¶åŠ¨æ€å­¦ä¹ æ¯ä¸ªç¬¦å·æ ‡è®°åœ¨ä¸‰ç§å‡ ä½•æ³¨æ„åˆ†æ”¯ä¸­çš„è·¯ç”±ã€‚ä¸å›ºå®šå‡ ä½•æ–¹æ³•ä¸åŒï¼ŒCATèƒ½å¤Ÿå®ç°è‡ªé€‚åº”çš„å‡ ä½•ä¸“ä¸šåŒ–ï¼Œæ ¹æ®ç¬¦å·æ ‡è®°çš„å±€éƒ¨å…³ç³»ç»“æ„å°†å…¶è·¯ç”±åˆ°é€‚å½“çš„æ›²ç‡ä¸Šã€‚è·¯ç”±ç½‘ç»œæä¾›äº†å¯è§£é‡Šçš„æ›²ç‡åå¥½ï¼Œæ¯ä¸ªåˆ†æ”¯åˆ™ä½¿ç”¨é’ˆå¯¹å…¶ç‰¹å®šæµå½¢ä¼˜åŒ–çš„ç‰¹å®šå‡ ä½•æ“ä½œã€‚åœ¨çŸ¥è¯†å›¾è°±å®ŒæˆåŸºå‡†æµ‹è¯•ï¼ˆFB15k-237ã€WN18RRï¼‰ä¸­ï¼ŒCATåœ¨MRRå’ŒHits@10ä¸Šç›¸è¾ƒäºå›ºå®šå‡ ä½•åŸºå‡†æµ‹è¯•æœ‰äº†å¤§çº¦10%çš„æ”¹è¿›ï¼ŒåŒæ—¶å¸¦æ¥æå°‘çš„é¢å¤–å¼€é”€ï¼ˆå‚æ•°å¢åŠ 5%ï¼Œæ¨ç†æ—¶é—´ç›¸å½“ï¼‰ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œå¯¹äºå¤æ‚çš„å…³è”æ¨ç†ï¼Œå­¦ä¹ åˆ°çš„å‡ ä½•é€‚åº”æ€§ä¼˜äºä»»ä½•å•ä¸€çš„å›ºå®šå‡ ä½•ç»“æ„ï¼Œè¯æ˜äº†CATä½œä¸ºè¯­è¨€ã€è§†è§‰å’Œå¤šæ¨¡å¼é¢†åŸŸä¸­æ··åˆå‡ ä½•æ¶æ„çš„å¯æ‰©å±•æ€§å’Œå¯è§£é‡Šæ€§åŸºç¡€çš„ä¼˜åŠ¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.01634v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>Transformeråœ¨ä¸åŒé¢†åŸŸè¡¨ç°ä¼˜å¼‚ï¼Œä½†å…¶åœ¨æ³¨æ„åŠ›æœºåˆ¶ä¸­éšå«äº†æ¬§å‡ é‡Œå¾—å‡ ä½•çš„å‡è®¾ï¼Œè¿™å¯¹äºå…·æœ‰éæ¬§å‡ é‡Œå¾—ç»“æ„çš„æ•°æ®å­˜åœ¨å±€é™æ€§ã€‚è™½ç„¶æœ€è¿‘å¯¹åŒæ›²ç©ºé—´å’Œçƒé¢ç©ºé—´çš„æ‰©å±•å¯¹å±‚æ¬¡æ¨¡å¼å’Œå¾ªç¯æ¨¡å¼æ˜¾ç¤ºå‡ºå¸Œæœ›ï¼Œä½†å®ƒä»¬éœ€è¦é¢„å…ˆç¡®å®šå•ä¸€å‡ ä½•å½¢çŠ¶ï¼Œå½“æ•°æ®å‘ˆç°æ··åˆå‡ ä½•å±æ€§æ—¶ï¼Œçµæ´»æ€§é™ä½ã€‚æœ¬æ–‡ä»‹ç»äº†æ›²ç‡è‡ªé€‚åº”Transformerï¼ˆCATï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æ¶æ„ï¼Œå®ƒé€šè¿‡è½»é‡çº§ã€å¯å¾®åˆ†çš„é—¨æ§æœºåˆ¶ï¼ŒåŠ¨æ€åœ°å­¦ä¹ æ¯ä¸ªç¬¦å·åœ¨ä¸‰ç§å‡ ä½•æ³¨æ„åŠ›åˆ†æ”¯ä¹‹é—´çš„è·¯ç”±ã€‚ä¸å›ºå®šå‡ ä½•æ–¹æ³•ä¸åŒï¼ŒCATèƒ½å¤Ÿå®ç°è‡ªé€‚åº”çš„å‡ ä½•ä¸“ä¸šåŒ–ï¼Œæ ¹æ®ç¬¦å·çš„å±€éƒ¨å…³ç³»ç»“æ„å°†å…¶è·¯ç”±åˆ°é€‚å½“çš„æ›²ç‡ã€‚è·¯ç”±ç½‘ç»œæä¾›å¯è§£é‡Šçš„æ›²ç‡åå¥½ï¼ŒåŒæ—¶æ¯ä¸ªåˆ†æ”¯ä¸ºå…¶ç‰¹å®šçš„æµå½¢ä½¿ç”¨ä¼˜åŒ–çš„å‡ ä½•ç‰¹å®šæ“ä½œã€‚åœ¨çŸ¥è¯†å›¾è°±å®ŒæˆåŸºå‡†æµ‹è¯•ï¼ˆFB15k-237ã€WN18RRï¼‰ä¸­ï¼Œä¸å›ºå®šå‡ ä½•åŸºå‡†æµ‹è¯•ç›¸æ¯”ï¼ŒCATåœ¨MRRå’ŒHits@10ä¸Šå®ç°äº†çº¦10%çš„æ”¹è¿›ï¼Œå¹¶ä¸”å…·æœ‰æå°çš„é¢å¤–è´Ÿæ‹…ï¼ˆå‚æ•°å¢åŠ 5%ï¼Œæ¨ç†æ—¶é—´ç›¸å½“ï¼‰ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œå¯¹äºå¤æ‚å…³ç³»æ¨ç†è€Œè¨€ï¼Œå­¦ä¹ åˆ°çš„å‡ ä½•é€‚åº”æ€§ä¼˜äºä»»ä½•å•ä¸€çš„å›ºå®šå‡ ä½•ã€‚å› æ­¤CATæˆä¸ºè¯­è¨€ã€è§†è§‰å’Œå¤šæ¨¡æ€é¢†åŸŸæ··åˆå‡ ä½•æ¶æ„çš„å¯æ‰©å±•å’Œå¯è§£é‡Šæ€§åŸºç¡€ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>Transformeråœ¨éæ¬§å‡ é‡Œå¾—æ•°æ®ç»“æ„ä¸Šå­˜åœ¨å±€é™æ€§ï¼Œå› ä¸ºå…¶æ³¨æ„åŠ›æœºåˆ¶éšå«äº†æ¬§å‡ é‡Œå¾—å‡ ä½•å‡è®¾ã€‚</li>
<li>ç°æœ‰æ‰©å±•å¦‚åŒæ›²ç©ºé—´å’Œçƒé¢ç©ºé—´è™½å¯¹ç‰¹å®šæ¨¡å¼æœ‰ä¼˜åŠ¿ï¼Œä½†ç¼ºä¹çµæ´»æ€§å¤„ç†æ··åˆå‡ ä½•æ•°æ®ã€‚</li>
<li>å¼•å…¥çš„æ›²ç‡è‡ªé€‚åº”Transformerï¼ˆCATï¼‰æ¶æ„èƒ½é€šè¿‡å­¦ä¹ åŠ¨æ€è·¯ç”±æœºåˆ¶æ¥é€‚åº”ä¸åŒçš„å‡ ä½•ç»“æ„ã€‚</li>
<li>CATé€šè¿‡è½»é‡çº§ã€å¯å¾®åˆ†çš„é—¨æ§æœºåˆ¶å®ç°ç¬¦å·åœ¨ä¸‰ç§å‡ ä½•æ³¨æ„åŠ›åˆ†æ”¯é—´çš„åŠ¨æ€è·¯ç”±ï¼Œæé«˜æ¨¡å‹çµæ´»æ€§å’Œæ•ˆç‡ã€‚</li>
<li>CATåœ¨çŸ¥è¯†å›¾è°±å®Œæˆä»»åŠ¡ä¸Šå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æ”¹è¿›ï¼Œä¸å›ºå®šå‡ ä½•æ¨¡å‹ç›¸æ¯”å…·æœ‰æ›´å¥½çš„å¤æ‚å…³ç³»æ¨ç†èƒ½åŠ›ã€‚</li>
<li>CATæ¶æ„å…·æœ‰å¯æ‰©å±•æ€§å’Œå¯è§£é‡Šæ€§ï¼Œä¸ºè¯­è¨€ã€è§†è§‰å’Œå¤šæ¨¡æ€é¢†åŸŸçš„æ··åˆå‡ ä½•æ¶æ„å¥ å®šäº†åŸºç¡€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.01634">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_LLM/2510.01634v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_LLM/2510.01634v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="ReSSFormer-A-Recursive-Sparse-Structured-Transformer-for-Scalable-and-Long-Context-Reasoning"><a href="#ReSSFormer-A-Recursive-Sparse-Structured-Transformer-for-Scalable-and-Long-Context-Reasoning" class="headerlink" title="ReSSFormer: A Recursive Sparse Structured Transformer for Scalable and   Long-Context Reasoning"></a>ReSSFormer: A Recursive Sparse Structured Transformer for Scalable and   Long-Context Reasoning</h2><p><strong>Authors:Haochen You, Baojing Liu</strong></p>
<p>While Transformer architectures have demonstrated impressive scalability across domains, they continue to face challenges in long-context reasoning, computational efficiency, and structural generalization - largely due to rigid layer stacking, dense attention, and reliance on positional encodings. We present ReSSFormer, a Recursive Sparse Structured Transformer that integrates three complementary innovations: Recurrent Reasoning &amp; Memory Unit (R2MU) for iterative reasoning with bounded depth, Adaptive Sparse Attention Module (ASAM) for efficient and focused context selection, and Self-Organizing Encoder Structure (SOES) for position-free structure induction. ReSSFormer replaces conventional depth stacking with recurrent inference, substitutes full attention with token- and expert-level sparsity, and models latent token topology directly from content. Across language modeling, multi-hop QA, and structure-sensitive tasks, ReSSFormer consistently outperforms strong baselines under comparable FLOPs and parameter budgets, highlighting its scalability, efficiency, and structural flexibility. </p>
<blockquote>
<p>è™½ç„¶Transformeræ¶æ„åœ¨å„ä¸ªé¢†åŸŸè¡¨ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„å¯æ‰©å±•æ€§ï¼Œä½†å®ƒä»¬ä»ç„¶é¢ä¸´ç€é•¿ä¸Šä¸‹æ–‡æ¨ç†ã€è®¡ç®—æ•ˆç‡å’Œç»“æ„æ³›åŒ–æ–¹é¢çš„æŒ‘æˆ˜ï¼Œè¿™ä¸»è¦æ˜¯å› ä¸ºå®ƒä»¬å­˜åœ¨åˆšæ€§å±‚å †å ã€å¯†é›†æ³¨æ„åŠ›å’Œä¾èµ–ä½ç½®ç¼–ç çš„é—®é¢˜ã€‚æˆ‘ä»¬æå‡ºäº†ReSSFormerï¼Œè¿™æ˜¯ä¸€ç§é€’å½’ç¨€ç–ç»“æ„Transformerï¼Œé›†æˆäº†ä¸‰ç§äº’è¡¥çš„åˆ›æ–°ï¼šç”¨äºå…·æœ‰æœ‰é™æ·±åº¦çš„è¿­ä»£æ¨ç†çš„å¾ªç¯æ¨ç†ä¸å†…å­˜å•å…ƒï¼ˆR2MUï¼‰ã€ç”¨äºé«˜æ•ˆå’Œæœ‰é’ˆå¯¹æ€§çš„ä¸Šä¸‹æ–‡é€‰æ‹©çš„è‡ªé€‚åº”ç¨€ç–æ³¨æ„åŠ›æ¨¡å—ï¼ˆASAMï¼‰ä»¥åŠç”¨äºæ— ä½ç½®ç»“æ„å½’çº³çš„è‡ªæˆ‘ç»„ç»‡ç¼–ç å™¨ç»“æ„ï¼ˆSOESï¼‰ã€‚ReSSFormerç”¨é€’å½’æ¨ç†æ›¿ä»£äº†ä¼ ç»Ÿçš„æ·±åº¦å †å ï¼Œç”¨ä»¤ç‰Œå’Œä¸“å®¶çº§çš„ç¨€ç–æ€§æ›¿ä»£äº†å…¨æ³¨æ„åŠ›ï¼Œå¹¶ç›´æ¥ä»å†…å®¹ä¸­å»ºæ¨¡æ½œåœ¨ä»¤ç‰Œæ‹“æ‰‘ã€‚åœ¨è¯­è¨€å»ºæ¨¡ã€å¤šè·³é—®ç­”å’Œç»“æ„æ•æ„Ÿä»»åŠ¡ä¸­ï¼ŒReSSFormeråœ¨å¯æ¯”çš„FLOPså’Œå‚æ•°é¢„ç®—ä¸‹å§‹ç»ˆä¼˜äºå¼ºå¤§çš„åŸºå‡†æµ‹è¯•ï¼Œçªæ˜¾äº†å…¶å¯æ‰©å±•æ€§ã€æ•ˆç‡å’Œç»“æ„çµæ´»æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.01585v1">PDF</a> Accepted as a short paper at ACM Multimedia Asia 2025</p>
<p><strong>Summary</strong>ï¼š</p>
<p>ReSSFormeræ˜¯ä¸€ä¸ªé€’å½’ç¨€ç–ç»“æ„åŒ–çš„Transformerï¼Œé€šè¿‡é›†æˆä¸‰é¡¹åˆ›æ–°æŠ€æœ¯â€”â€”é€’å½’æ¨ç†ä¸è®°å¿†å•å…ƒï¼ˆR2MUï¼‰ã€è‡ªé€‚åº”ç¨€ç–æ³¨æ„åŠ›æ¨¡å—ï¼ˆASAMï¼‰å’Œè‡ªæˆ‘ç»„ç»‡ç¼–ç å™¨ç»“æ„ï¼ˆSOESï¼‰â€”â€”è§£å†³äº†Transformeræ¶æ„åœ¨é•¿æ–‡æœ¬ä¸Šä¸‹æ–‡æ¨ç†ã€è®¡ç®—æ•ˆç‡å’Œç»“æ„æ³›åŒ–æ–¹é¢çš„æŒ‘æˆ˜ã€‚ReSSFormeré€šè¿‡é€’å½’æ¨ç†å®ç°æœ‰é™æ·±åº¦ï¼Œé€šè¿‡è‡ªé€‚åº”ç¨€ç–æ³¨æ„åŠ›è¿›è¡Œé«˜æ•ˆä¸Šä¸‹æ–‡é€‰æ‹©ï¼Œå¹¶é€šè¿‡è‡ªæˆ‘ç»„ç»‡ç¼–ç å™¨ç»“æ„å®ç°æ— ä½ç½®çš„ç»“æ„å½’çº³ã€‚å®ƒåœ¨è¯­è¨€å»ºæ¨¡ã€å¤šè·³é—®ç­”å’Œç»“æ„æ•æ„Ÿä»»åŠ¡ä¸Šçš„è¡¨ç°å‡ä¼˜äºåŸºå‡†æ¨¡å‹ï¼Œå±•ç°å‡ºè‰¯å¥½çš„å¯æ‰©å±•æ€§ã€æ•ˆç‡å’Œç»“æ„çµæ´»æ€§ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ReSSFormeræ˜¯ä¸€ä¸ªé’ˆå¯¹Transformeræ¶æ„çš„æ”¹è¿›æ¨¡å‹ï¼Œè§£å†³äº†å…¶åœ¨é•¿æ–‡æœ¬ä¸Šä¸‹æ–‡æ¨ç†ã€è®¡ç®—æ•ˆç‡å’Œç»“æ„æ³›åŒ–æ–¹é¢çš„æŒ‘æˆ˜ã€‚</li>
<li>ReSSFormeré€šè¿‡é€’å½’æ¨ç†ä¸è®°å¿†å•å…ƒå®ç°è¿­ä»£æ¨ç†å’Œæœ‰é™æ·±åº¦ã€‚</li>
<li>è‡ªé€‚åº”ç¨€ç–æ³¨æ„åŠ›æ¨¡å—æé«˜äº†ReSSFormerçš„æ•ˆç‡ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´ä¸“æ³¨äºä¸Šä¸‹æ–‡é€‰æ‹©ã€‚</li>
<li>è‡ªæˆ‘ç»„ç»‡ç¼–ç å™¨ç»“æ„ä½¿å¾—ReSSFormerèƒ½å¤Ÿç›´æ¥ä»å†…å®¹ä¸­å»ºæ¨¡æ½œåœ¨æ ‡è®°æ‹“æ‰‘ã€‚</li>
<li>ReSSFormeråœ¨è¯­è¨€å»ºæ¨¡ã€å¤šè·³é—®ç­”å’Œç»“æ„æ•æ„Ÿä»»åŠ¡ä¸Šçš„è¡¨ç°å‡ä¼˜äºåŸºå‡†æ¨¡å‹ã€‚</li>
<li>ReSSFormeré€šè¿‡é€’å½’ç»“æ„åŒ–è®¾è®¡å®ç°äº†è‰¯å¥½çš„å¯æ‰©å±•æ€§ã€æ•ˆç‡å’Œç»“æ„çµæ´»æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.01585">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_LLM/2510.01585v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_LLM/2510.01585v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_LLM/2510.01585v1/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_LLM/2510.01585v1/page_2_2.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_LLM/2510.01585v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_LLM/2510.01585v1/page_3_1.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Data-Selection-for-Fine-tuning-Vision-Language-Models-via-Cross-Modal-Alignment-Trajectories"><a href="#Data-Selection-for-Fine-tuning-Vision-Language-Models-via-Cross-Modal-Alignment-Trajectories" class="headerlink" title="Data Selection for Fine-tuning Vision Language Models via Cross Modal   Alignment Trajectories"></a>Data Selection for Fine-tuning Vision Language Models via Cross Modal   Alignment Trajectories</h2><p><strong>Authors:Nilay Naharas, Dang Nguyen, Nesihan Bulut, Mohammadhossein Bateni, Vahab Mirrokni, Baharan Mirzasoleiman</strong></p>
<p>Data-efficient learning aims to eliminate redundancy in large training datasets by training models on smaller subsets of the most informative examples. While data selection has been extensively explored for vision models and large language models (LLMs), it remains underexplored for Large Vision-Language Models (LVLMs). Notably, none of existing methods can outperform random selection at different subset sizes. In this work, we propose the first principled method for data-efficient instruction tuning of LVLMs. We prove that examples with similar cross-modal attention matrices during instruction tuning have similar gradients. Thus, they influence model parameters in a similar manner and convey the same information to the model during training. Building on this insight, we propose XMAS, which clusters examples based on the trajectories of the top singular values of their attention matrices obtained from fine-tuning a small proxy LVLM. By sampling a balanced subset from these clusters, XMAS effectively removes redundancy in large-scale LVLM training data. Extensive experiments show that XMAS can discard 50% of the LLaVA-665k dataset and 85% of the Vision-Flan dataset while fully preserving performance of LLaVA-1.5-7B on 10 downstream benchmarks and speeding up its training by 1.2x. This is 30% more data reduction compared to the best baseline for LLaVA-665k. The projectâ€™s website can be found at <a target="_blank" rel="noopener" href="https://bigml-cs-ucla.github.io/XMAS-project-page/">https://bigml-cs-ucla.github.io/XMAS-project-page/</a>. </p>
<blockquote>
<p>æ•°æ®é«˜æ•ˆå­¦ä¹ æ—¨åœ¨é€šè¿‡åœ¨æœ€å…·ä¿¡æ¯é‡çš„è¾ƒå°å­é›†ä¸Šè®­ç»ƒæ¨¡å‹æ¥æ¶ˆé™¤å¤§è§„æ¨¡è®­ç»ƒæ•°æ®é›†ä¸­çš„å†—ä½™ä¿¡æ¯ã€‚è™½ç„¶æ•°æ®é€‰æ‹©å·²ç»ä¸ºè§†è§‰æ¨¡å‹å’Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œäº†å¹¿æ³›æ¢ç´¢ï¼Œä½†å¯¹äºå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMï¼‰ä»ç„¶ç¼ºä¹æ¢ç´¢ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œç°æœ‰æ–¹æ³•éƒ½æ— æ³•åœ¨ä¸åŒå­é›†å¤§å°ä¸Šå®ç°éšæœºé€‰æ‹©çš„è¶…è¶Šã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†é’ˆå¯¹LVLMçš„æ•°æ®é«˜æ•ˆæŒ‡ä»¤å¾®è°ƒçš„é¦–ä¸ªåŸç†æ–¹æ³•ã€‚æˆ‘ä»¬è¯æ˜ï¼Œåœ¨æŒ‡ä»¤å¾®è°ƒè¿‡ç¨‹ä¸­å…·æœ‰ç›¸ä¼¼è·¨æ¨¡æ€æ³¨æ„åŠ›çŸ©é˜µçš„ç¤ºä¾‹å…·æœ‰ç›¸ä¼¼çš„æ¢¯åº¦ã€‚å› æ­¤ï¼Œå®ƒä»¬ä»¥ç›¸ä¼¼çš„æ–¹å¼å½±å“æ¨¡å‹å‚æ•°ï¼Œå¹¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å‘æ¨¡å‹ä¼ è¾¾ç›¸åŒçš„ä¿¡æ¯ã€‚åŸºäºè¿™ä¸€è§è§£ï¼Œæˆ‘ä»¬æå‡ºäº†XMASï¼Œå®ƒæ ¹æ®ä»å¾®è°ƒå°å‹ä»£ç†LVLMè·å¾—çš„æ³¨æ„åŠ›çŸ©é˜µçš„é¡¶éƒ¨å¥‡å¼‚å€¼çš„è½¨è¿¹å¯¹ç¤ºä¾‹è¿›è¡Œèšç±»ã€‚ä»è¿™äº›é›†ç¾¤ä¸­é‡‡æ ·å¹³è¡¡çš„å­é›†ï¼ŒXMASæœ‰æ•ˆåœ°æ¶ˆé™¤äº†å¤§è§„æ¨¡LVLMè®­ç»ƒæ•°æ®ä¸­çš„å†—ä½™ä¿¡æ¯ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒXMASå¯ä»¥ä¸¢å¼ƒLLaVA-665kæ•°æ®é›†çš„50%å’ŒVision-Flanæ•°æ®é›†çš„85%ï¼ŒåŒæ—¶å®Œå…¨ä¿ç•™LLaVA-1.5-7Båœ¨10ä¸ªä¸‹æ¸¸åŸºå‡†æµ‹è¯•ä¸Šçš„æ€§èƒ½ï¼Œå¹¶åŠ é€Ÿå…¶è®­ç»ƒ1.2å€ã€‚ä¸LLaVA-665kçš„æœ€ä½³åŸºå‡†ç›¸æ¯”ï¼Œè¿™å®ç°äº†30%æ›´å¤šçš„æ•°æ®ç¼©å‡ã€‚é¡¹ç›®ç½‘ç«™ä½äºï¼š<a target="_blank" rel="noopener" href="https://bigml-cs-ucla.github.io/XMAS-project-page/">https://bigml-cs-ucla.github.io/XMAS-project-page/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.01454v1">PDF</a> 30 pages, 10 figures, 5 tables, link:   <a target="_blank" rel="noopener" href="https://bigml-cs-ucla.github.io/XMAS-project-page/">https://bigml-cs-ucla.github.io/XMAS-project-page/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§é’ˆå¯¹å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰çš„æ•°æ®é«˜æ•ˆå­¦ä¹ ï¼ˆData-efficient learningï¼‰æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡èšç±»å…·æœ‰ç›¸ä¼¼è·¨æ¨¡æ€æ³¨æ„åŠ›çŸ©é˜µçš„ç¤ºä¾‹ï¼Œå»é™¤å¤§è§„æ¨¡LVLMè®­ç»ƒæ•°æ®ä¸­çš„å†—ä½™ä¿¡æ¯ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å¯åœ¨ä¿æŒæ¨¡å‹æ€§èƒ½çš„åŒæ—¶ï¼Œå‡å°‘è®­ç»ƒæ•°æ®é›†çš„å¤§å°å¹¶åŠ é€Ÿæ¨¡å‹è®­ç»ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ•°æ®é«˜æ•ˆå­¦ä¹ æ—¨åœ¨é€šè¿‡è®­ç»ƒæ¨¡å‹åœ¨æœ€å…·ä¿¡æ¯é‡çš„è¾ƒå°æ•°æ®å­é›†ä¸Šæ¥æ¶ˆé™¤å¤§è§„æ¨¡è®­ç»ƒæ•°æ®é›†ä¸­çš„å†—ä½™ä¿¡æ¯ã€‚</li>
<li>è™½ç„¶æ•°æ®é€‰æ‹©å·²ç»å¹¿æ³›åº”ç”¨äºè§†è§‰æ¨¡å‹å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œä½†å¯¹äºå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰ä»ç„¶ç¼ºä¹ç ”ç©¶ã€‚</li>
<li>ç°æœ‰æ–¹æ³•æ— æ³•åœ¨ä¸åŒå­é›†å¤§å°ä¸Šè¶…è¶Šéšæœºé€‰æ‹©ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè·¨æ¨¡æ€æ³¨æ„åŠ›çŸ©é˜µçš„èšç±»æ–¹æ³•ï¼Œç”¨äºæ•°æ®é«˜æ•ˆæŒ‡ä»¤è°ƒæ•´LVLMsã€‚</li>
<li>ç¤ºä¾‹çš„èšç±»æ˜¯åŸºäºå…¶æ³¨æ„åŠ›çŸ©é˜µçš„å¥‡å¼‚å€¼è½¨è¿¹ï¼Œè¿™äº›æ³¨æ„åŠ›çŸ©é˜µæ¥è‡ªäºå¯¹å°å‹ä»£ç†LVLMçš„å¾®è°ƒã€‚</li>
<li>é€šè¿‡ä»è¿™äº›é›†ç¾¤ä¸­æŠ½å–å¹³è¡¡çš„å­é›†ï¼ŒXMASæœ‰æ•ˆåœ°å»é™¤äº†å¤§è§„æ¨¡LVLMè®­ç»ƒæ•°æ®çš„å†—ä½™ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.01454">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_LLM/2510.01454v1/page_0_0.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="In-AI-Sweet-Harmony-Sociopragmatic-Guardrail-Bypasses-and-Evaluation-Awareness-in-OpenAI-gpt-oss-20b"><a href="#In-AI-Sweet-Harmony-Sociopragmatic-Guardrail-Bypasses-and-Evaluation-Awareness-in-OpenAI-gpt-oss-20b" class="headerlink" title="In AI Sweet Harmony: Sociopragmatic Guardrail Bypasses and   Evaluation-Awareness in OpenAI gpt-oss-20b"></a>In AI Sweet Harmony: Sociopragmatic Guardrail Bypasses and   Evaluation-Awareness in OpenAI gpt-oss-20b</h2><p><strong>Authors:Nils Durner</strong></p>
<p>We probe OpenAIâ€™s open-weights 20-billion-parameter model gpt-oss-20b to study how sociopragmatic framing, language choice, and instruction hierarchy affect refusal behavior. Across 80 seeded iterations per scenario, we test several harm domains including ZIP-bomb construction (cyber threat), synthetic card-number generation, minor-unsafe driving advice, drug-precursor indicators, and RAG context exfiltration. Composite prompts that combine an educator persona, a safety-pretext (â€œwhat to avoidâ€), and step-cue phrasing flip assistance rates from 0% to 97.5% on a ZIP-bomb task. On our grid, formal registers in German and French are often leakier than matched English prompts. A â€œLinux terminalâ€ role-play overrides a developer rule not to reveal context in a majority of runs with a naive developer prompt, and we introduce an AI-assisted hardening method that reduces leakage to 0% in several user-prompt variants. We further test evaluation awareness with a paired-track design and measure frame-conditioned differences between matched â€œhelpfulnessâ€ and â€œharmfulnessâ€ evaluation prompts; we observe inconsistent assistance in 13% of pairs. Finally, we find that the OpenAI Moderation API under-captures materially helpful outputs relative to a semantic grader, and that refusal rates differ by 5 to 10 percentage points across inference stacks, raising reproducibility concerns. We release prompts, seeds, outputs, and code for reproducible auditing at <a target="_blank" rel="noopener" href="https://github.com/ndurner/gpt-oss-rt-run">https://github.com/ndurner/gpt-oss-rt-run</a> . </p>
<blockquote>
<p>æˆ‘ä»¬æ¢ç©¶äº†OpenAIçš„å¼€æ”¾æƒé‡20äº¿å‚æ•°æ¨¡å‹gpt-oss-20bï¼Œä»¥ç ”ç©¶ç¤¾ä¼šè¯­ç”¨æ¡†æ¶ã€è¯­è¨€é€‰æ‹©å’ŒæŒ‡ä»¤å±‚æ¬¡ç»“æ„å¦‚ä½•å½±å“æ‹’ç»è¡Œä¸ºã€‚åœ¨æ¯ç§æƒ…æ™¯çš„80æ¬¡è¿­ä»£ä¸­ï¼Œæˆ‘ä»¬æµ‹è¯•äº†å‡ ä¸ªå±å®³é¢†åŸŸï¼ŒåŒ…æ‹¬ZIPç‚¸å¼¹æ„å»ºï¼ˆç½‘ç»œå¨èƒï¼‰ã€åˆæˆå¡å·ç”Ÿæˆã€è½»å¾®ä¸å®‰å…¨é©¾é©¶å»ºè®®ã€è¯ç‰©å‰é©±ä½“æŒ‡æ ‡å’ŒRAGä¸Šä¸‹æ–‡æå–ã€‚ç»“åˆæ•™è‚²è€…äººæ ¼ã€å®‰å…¨å‰æï¼ˆâ€œé¿å…ä»€ä¹ˆâ€ï¼‰å’Œæ­¥éª¤æç¤ºçš„å¤åˆæç¤ºï¼Œåœ¨ZIPç‚¸å¼¹ä»»åŠ¡ä¸­å°†æ´åŠ©ç‡ä»0%æé«˜åˆ°97.5%ã€‚åœ¨æˆ‘ä»¬çš„ç½‘æ ¼ä¸­ï¼Œå¾·è¯­å’Œæ³•è¯­çš„æ­£å¼è¯­ä½“å¾€å¾€æ¯”åŒ¹é…çš„è‹±è¯­æç¤ºæ›´å®¹æ˜“æ³„éœ²ã€‚ä¸€ä¸ªâ€œLinuxç»ˆç«¯â€è§’è‰²æ‰®æ¼”è¿èƒŒäº†å¼€å‘è€…çš„è§„åˆ™ï¼Œåœ¨å¤§å¤šæ•°è¿è¡Œä¸­æœªé€éœ²ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªå¤©çœŸçš„å¼€å‘è€…æç¤ºï¼Œå¹¶å¼•å…¥äº†ä¸€ç§AIè¾…åŠ©ç¡¬åŒ–æ–¹æ³•ï¼Œå°†å‡ ç§ç”¨æˆ·æç¤ºå˜ä½“ä¸­çš„æ³„æ¼ç‡é™ä½åˆ°0%ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥é€šè¿‡é…å¯¹è½¨è¿¹è®¾è®¡æµ‹è¯•è¯„ä¼°æ„è¯†ï¼Œå¹¶æµ‹é‡åŒ¹é…â€œæœ‰ç”¨æ€§â€å’Œâ€œå±å®³æ€§â€è¯„ä¼°æç¤ºä¹‹é—´çš„æ¡†æ¶æ¡ä»¶å·®å¼‚ï¼›æˆ‘ä»¬è§‚å¯Ÿåˆ°13%çš„é…å¯¹ä¸­å­˜åœ¨ä¸ä¸€è‡´çš„æ´åŠ©ã€‚æœ€åï¼Œæˆ‘ä»¬å‘ç°OpenAIçš„å®¡æ ¸APIç›¸å¯¹äºè¯­ä¹‰è¯„åˆ†è€…æœªèƒ½æ•è·åˆ°å®è´¨æ€§çš„æœ‰ç”¨è¾“å‡ºï¼Œå¹¶ä¸”æ‹’ç»ç‡åœ¨æ¨ç†å †æ ˆä¹‹é—´ç›¸å·®5åˆ°10ä¸ªç™¾åˆ†ç‚¹ï¼Œè¿™å¼•å‘äº†å¯é‡å¤æ€§çš„æ‹…å¿§ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/ndurner/gpt-oss-rt-run%E4%B8%8A%E5%8F%91%E5%B8%83%E4%BA%86%E6%8F%90%E7%A4%BA%E3%80%81%E7%A7%8D%E5%AD%90%E3%80%81%E8%BE%93%E5%87%BA%E5%92%8C%E4%BB%A3%E7%A0%81%EF%BC%8C%E4%BB%A5%E4%BE%BF%E8%BF%9B%E8%A1%8C%E5%8F%AF%E9%87%8D%E5%A4%8D%E7%9A%84%E5%AE%A1%E8%AE%A1%E3%80%82">https://github.com/ndurner/gpt-oss-rt-runä¸Šå‘å¸ƒäº†æç¤ºã€ç§å­ã€è¾“å‡ºå’Œä»£ç ï¼Œä»¥ä¾¿è¿›è¡Œå¯é‡å¤çš„å®¡è®¡ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.01259v1">PDF</a> 27 pages, 1 figure</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†OpenAIçš„å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹GPT-OSSåœ¨å¤„ç†ä¸åŒé¢†åŸŸä»»åŠ¡æ—¶å¦‚ä½•åº”å¯¹ç¤¾ä¼šè¯­å¢ƒæ¡†æ¶ã€è¯­è¨€é€‰æ‹©å’ŒæŒ‡ä»¤å±‚æ¬¡ç»“æ„å¯¹æ‹’ç»è¡Œä¸ºçš„å½±å“ã€‚é€šè¿‡å¹¿æ³›çš„æµ‹è¯•å‘ç°ï¼Œåœ¨æŸäº›æƒ…å¢ƒä¸‹ï¼Œè¯­è¨€æ¡†æ¶çš„å˜åŒ–å¯ä»¥å¯¼è‡´AIæä¾›çš„å¸®åŠ©ç‡äº§ç”Ÿæ˜¾è‘—å˜åŒ–ã€‚åŒæ—¶ï¼Œè¿˜ä»‹ç»äº†åœ¨ä¸åŒè¯­å¢ƒä¸‹æ¨¡å‹çš„å®‰å…¨æ€§è¡¨ç°åŠå…¶è¯„ä¼°æ–¹æ³•ã€‚æœ¬æ–‡è¿˜æåˆ°äº†OpenAIçš„å®¡æ ¸APIåœ¨è¯†åˆ«æœ‰ç”¨è¾“å‡ºæ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œå¹¶æŒ‡å‡ºåœ¨ä¸åŒæ¨ç†å †æ ˆä¹‹é—´æ‹’ç»ç‡å­˜åœ¨å·®å¼‚ã€‚æœ¬æ–‡å‘å¸ƒçš„å®éªŒæ•°æ®å’Œä»£ç å¯ä¾›ä»–äººè¿›è¡Œå¯é‡å¤æ€§çš„å®¡è®¡ç ”ç©¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GPT-OSSæ¨¡å‹åœ¨å¤„ç†ä¸åŒä»»åŠ¡æ—¶å—åˆ°ç¤¾ä¼šè¯­å¢ƒæ¡†æ¶ã€è¯­è¨€é€‰æ‹©å’ŒæŒ‡ä»¤å±‚æ¬¡ç»“æ„çš„å½±å“ã€‚</li>
<li>åœ¨ç‰¹å®šä»»åŠ¡ä¸­ï¼Œè¯­è¨€æ¡†æ¶çš„å˜åŒ–å¯ä»¥æ˜¾è‘—æ”¹å˜AIæä¾›çš„å¸®åŠ©ç‡ã€‚</li>
<li>æ¨¡å‹åœ¨ä¸åŒè¯­å¢ƒä¸‹çš„å®‰å…¨æ€§è¡¨ç°æœ‰æ‰€å·®å¼‚ã€‚</li>
<li>æ¨¡å‹åœ¨æ‹’ç»è¡Œä¸ºçš„æ‰§è¡Œä¸Šå­˜åœ¨å·®å¼‚æ€§ï¼Œä¸åŒæ¨ç†å †æ ˆçš„æ‹’ç»ç‡æœ‰æ‰€ä¸åŒã€‚</li>
<li>OpenAIçš„å®¡æ ¸APIåœ¨è¯†åˆ«æœ‰ç”¨è¾“å‡ºæ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œå¯èƒ½éœ€è¦æ”¹è¿›ã€‚</li>
<li>ä½œè€…å‘å¸ƒäº†ä¸€ç³»åˆ—å®éªŒæ•°æ®å’Œä»£ç ä»¥ä¾›ä»–äººè¿›è¡Œå®¡è®¡ç ”ç©¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.01259">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_LLM/2510.01259v1/page_0_0.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Train-on-Validation-ToV-Fast-data-selection-with-applications-to-fine-tuning"><a href="#Train-on-Validation-ToV-Fast-data-selection-with-applications-to-fine-tuning" class="headerlink" title="Train on Validation (ToV): Fast data selection with applications to   fine-tuning"></a>Train on Validation (ToV): Fast data selection with applications to   fine-tuning</h2><p><strong>Authors:Ayush Jain, Andrea Montanari, Eren Sasoglu</strong></p>
<p>State-of-the-art machine learning often follows a two-stage process: $(i)$<del>pre-training on large, general-purpose datasets; $(ii)$</del>fine-tuning on task-specific data. In fine-tuning, selecting training examples that closely reflect the target distribution is crucial. However, it is often the case that only a few samples are available from the target distribution. Existing data selection methods treat these target samples as a validation set and estimate the effect of adding or removing a single sample from the training pool by performing inference on the validation set.   We propose a simpler and faster alternative that inverts the usual role of train and validation: we perform inference on the training pool before and after fine-tuning on the validation set. We then select samples whose predictions change the most. Our key insight is that the training samples most affected by fine-tuning on a small validation set tend to be the most beneficial for reducing test loss on the target distribution. Experiments on instruction tuning and named entity recognition tasks show that, in most cases, our method achieves lower test log-loss than state-of-the-art approaches. We support our findings with theoretical analysis. </p>
<blockquote>
<p>æœ€å…ˆè¿›çš„æœºå™¨å­¦ä¹ é€šå¸¸éµå¾ªä¸¤é˜¶æ®µè¿‡ç¨‹ï¼š$(i)$åœ¨å¤§è§„æ¨¡é€šç”¨æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼›$(ii)$åœ¨ç‰¹å®šä»»åŠ¡çš„æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒã€‚åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­ï¼Œé€‰æ‹©èƒ½ç´§å¯†åæ˜ ç›®æ ‡åˆ†å¸ƒçš„è®­ç»ƒæ ·æœ¬è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç›®æ ‡åˆ†å¸ƒå¾€å¾€åªæœ‰å°‘æ•°æ ·æœ¬å¯ç”¨ã€‚ç°æœ‰çš„æ•°æ®é€‰æ‹©æ–¹æ³•å°†è¿™äº›ç›®æ ‡æ ·æœ¬è§†ä¸ºéªŒè¯é›†ï¼Œå¹¶é€šè¿‡åœ¨éªŒè¯é›†ä¸Šè¿›è¡Œæ¨ç†æ¥ä¼°è®¡æ·»åŠ æˆ–åˆ é™¤å•ä¸ªæ ·æœ¬å¯¹è®­ç»ƒæ± çš„å½±å“ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ›´ç®€å•ã€æ›´å¿«çš„æ–¹æ³•ï¼Œé¢ å€’äº†è®­ç»ƒå’ŒéªŒè¯çš„å¸¸è§„è§’è‰²ï¼šæˆ‘ä»¬åœ¨å¾®è°ƒéªŒè¯é›†ä¹‹å‰å’Œä¹‹ååœ¨è®­ç»ƒæ± ä¸Šè¿›è¡Œæ¨ç†ã€‚ç„¶åï¼Œæˆ‘ä»¬é€‰æ‹©é¢„æµ‹å˜åŒ–æœ€å¤§çš„æ ·æœ¬ã€‚æˆ‘ä»¬çš„å…³é”®è§è§£æ˜¯ï¼Œå—å°è§„æ¨¡éªŒè¯é›†å¾®è°ƒå½±å“æœ€å¤§çš„è®­ç»ƒæ ·æœ¬å¾€å¾€æœ€æœ‰åˆ©äºå‡å°‘ç›®æ ‡åˆ†å¸ƒä¸Šçš„æµ‹è¯•æŸå¤±ã€‚åœ¨æŒ‡ä»¤è°ƒæ•´å’Œå‘½åå®ä½“è¯†åˆ«ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œåœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†æ¯”æœ€å…ˆè¿›æ–¹æ³•æ›´ä½çš„æµ‹è¯•å¯¹æ•°æŸå¤±ã€‚æˆ‘ä»¬ä»¥ç†è®ºåˆ†ææ”¯æŒæˆ‘ä»¬çš„å‘ç°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.00386v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æœºå™¨å­¦ä¹ çš„ä¸¤é˜¶æ®µè¿‡ç¨‹ï¼šé¢„è®­ç»ƒå’Œå¾®è°ƒã€‚åœ¨å¾®è°ƒé˜¶æ®µï¼Œé€‰æ‹©åæ˜ ç›®æ ‡åˆ†å¸ƒçš„æ ·æœ¬è‡³å…³é‡è¦ã€‚é’ˆå¯¹ç›®æ ‡åˆ†å¸ƒåªæœ‰å°‘é‡æ ·æœ¬å¯ç”¨çš„æƒ…å†µï¼Œç°æœ‰æ•°æ®é€‰æ‹©æ–¹æ³•å°†ç›®æ ‡æ ·æœ¬è§†ä¸ºéªŒè¯é›†ï¼Œé€šè¿‡éªŒè¯é›†ä¸Šçš„æ¨ç†æ¥ä¼°è®¡æ·»åŠ æˆ–åˆ é™¤å•ä¸ªæ ·æœ¬å¯¹è®­ç»ƒæ± çš„å½±å“ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ›´ç®€å•å¿«é€Ÿçš„æ–¹æ³•ï¼Œåè½¬äº†è®­ç»ƒå’ŒéªŒè¯çš„å¸¸è§„è§’è‰²ï¼šåœ¨å¾®è°ƒéªŒè¯é›†ä¹‹å‰å’Œä¹‹åå¯¹è®­ç»ƒæ± è¿›è¡Œæ¨ç†ï¼Œå¹¶é€‰æ‹©é¢„æµ‹å˜åŒ–æœ€å¤§çš„æ ·æœ¬ã€‚å…³é”®è§è§£æ˜¯ï¼Œç»è¿‡å°éªŒè¯é›†å¾®è°ƒåå—åˆ°æœ€å¤§å½±å“çš„è®­ç»ƒæ ·æœ¬å¾€å¾€å¯¹å‡å°‘ç›®æ ‡åˆ†å¸ƒä¸Šçš„æµ‹è¯•æŸå¤±æœ€æœ‰åˆ©ã€‚åœ¨æŒ‡ä»¤è°ƒæ•´å’Œå‘½åå®ä½“è¯†åˆ«ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œå¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ¯”ç°æœ‰æ–¹æ³•å®ç°äº†æ›´ä½çš„æµ‹è¯•å¯¹æ•°æŸå¤±ã€‚æˆ‘ä»¬çš„å‘ç°å¾—åˆ°äº†ç†è®ºæ”¯æŒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœºå™¨å­¦ä¹ é€šå¸¸åŒ…å«é¢„è®­ç»ƒå’Œå¾®è°ƒä¸¤ä¸ªé˜¶æ®µã€‚å¾®è°ƒé˜¶æ®µé€‰æ‹©åæ˜ ç›®æ ‡åˆ†å¸ƒçš„æ ·æœ¬è‡³å…³é‡è¦ã€‚</li>
<li>é’ˆå¯¹ç›®æ ‡æ ·æœ¬ç¨€ç¼ºçš„æƒ…å†µï¼Œç°æœ‰æ•°æ®é€‰æ‹©æ–¹æ³•ä½¿ç”¨éªŒè¯é›†è¯„ä¼°æ ·æœ¬å½±å“ã€‚</li>
<li>æœ¬æ–‡æå‡ºä¸€ç§æ–°æ–¹æ³•ï¼Œé€šè¿‡å¯¹æ¯”å¾®è°ƒå‰åè®­ç»ƒæ± ä¸Šçš„é¢„æµ‹å˜åŒ–æ¥é€‰æ‹©æ ·æœ¬ã€‚</li>
<li>å…³é”®è§è§£æ˜¯ï¼šå—å¾®è°ƒå½±å“æœ€å¤§çš„è®­ç»ƒæ ·æœ¬å¯¹å‡å°‘ç›®æ ‡åˆ†å¸ƒä¸Šçš„æµ‹è¯•æŸå¤±æœ€ä¸ºæœ‰ç›Šã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œåœ¨æŒ‡ä»¤è°ƒæ•´å’Œå‘½åå®ä½“è¯†åˆ«ä»»åŠ¡ä¸Šï¼Œæ–°æ–¹æ³•å®ç°äº†æ›´ä½çš„æµ‹è¯•å¯¹æ•°æŸå¤±ã€‚</li>
<li>è¯¥æ–¹æ³•ä¸ä»…æœ‰æ•ˆï¼Œè€Œä¸”æ“ä½œç®€å•ï¼Œå…·æœ‰å®é™…åº”ç”¨ä»·å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.00386">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_LLM/2510.00386v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_LLM/2510.00386v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_LLM/2510.00386v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Why-Canâ€™t-Transformers-Learn-Multiplication-Reverse-Engineering-Reveals-Long-Range-Dependency-Pitfalls"><a href="#Why-Canâ€™t-Transformers-Learn-Multiplication-Reverse-Engineering-Reveals-Long-Range-Dependency-Pitfalls" class="headerlink" title="Why Canâ€™t Transformers Learn Multiplication? Reverse-Engineering Reveals   Long-Range Dependency Pitfalls"></a>Why Canâ€™t Transformers Learn Multiplication? Reverse-Engineering Reveals   Long-Range Dependency Pitfalls</h2><p><strong>Authors:Xiaoyan Bai, Itamar Pres, Yuntian Deng, Chenhao Tan, Stuart Shieber, Fernanda ViÃ©gas, Martin Wattenberg, Andrew Lee</strong></p>
<p>Language models are increasingly capable, yet still fail at a seemingly simple task of multi-digit multiplication. In this work, we study why, by reverse-engineering a model that successfully learns multiplication via \emph{implicit chain-of-thought}, and report three findings: (1) Evidence of long-range structure: Logit attributions and linear probes indicate that the model encodes the necessary long-range dependencies for multi-digit multiplication. (2) Mechanism: the model encodes long-range dependencies using attention to construct a directed acyclic graph to <code>cache&#39;&#39; and </code>retrieveâ€™â€™ pairwise partial products. (3) Geometry: the model implements partial products in attention heads by forming Minkowski sums between pairs of digits, and digits are represented using a Fourier basis, both of which are intuitive and efficient representations that the standard fine-tuning model lacks. With these insights, we revisit the learning dynamics of standard fine-tuning and find that the model converges to a local optimum that lacks the required long-range dependencies. We further validate this understanding by introducing an auxiliary loss that predicts the &#96;&#96;running sumâ€™â€™ via a linear regression probe, which provides an inductive bias that enables the model to successfully learn multi-digit multiplication. In summary, by reverse-engineering the mechanisms of an implicit chain-of-thought model we uncover a pitfall for learning long-range dependencies in Transformers and provide an example of how the correct inductive bias can address this issue. </p>
<blockquote>
<p>è¯­è¨€æ¨¡å‹çš„æ€§èƒ½æ—¥ç›Šå¢å¼ºï¼Œä½†åœ¨çœ‹ä¼¼ç®€å•çš„å¤šä½æ•°ä¹˜æ³•ä»»åŠ¡ä¸Šä»ç„¶ä¼šå¤±è´¥ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡é€†å‘å·¥ç¨‹ä¸€ä¸ªé€šè¿‡â€œéšå¼æ€ç»´é“¾â€æˆåŠŸå­¦ä¹ ä¹˜æ³•çš„æ¨¡å‹æ¥ç ”ç©¶åŸå› ï¼Œå¹¶æŠ¥å‘Šäº†ä¸‰ä¸ªå‘ç°ï¼š(1) é•¿ç¨‹ç»“æ„çš„è¯æ®ï¼šLogitå½’å±å’Œçº¿æ€§æ¢é’ˆè¡¨æ˜ï¼Œæ¨¡å‹ç¼–ç äº†å¤šä½æ•°ä¹˜æ³•æ‰€éœ€çš„é•¿ç¨‹ä¾èµ–å…³ç³»ã€‚(2) æœºåˆ¶ï¼šæ¨¡å‹åˆ©ç”¨æ³¨æ„åŠ›æ¥æ„å»ºæœ‰å‘æ— ç¯å›¾ï¼Œä»¥â€œç¼“å­˜â€å’Œâ€œæ£€ç´¢â€æˆå¯¹çš„éƒ¨åˆ†ä¹˜ç§¯ï¼Œä»è€Œç¼–ç é•¿ç¨‹ä¾èµ–å…³ç³»ã€‚(3) å‡ ä½•ï¼šæ¨¡å‹é€šè¿‡åœ¨æ•°å­—å¯¹ä¹‹é—´å½¢æˆé—µå¯å¤«æ–¯åŸºå’Œå¹¶åˆ©ç”¨å‚…é‡Œå¶åŸºè¡¨ç¤ºæ•°å­—ï¼Œåœ¨æ³¨æ„åŠ›å¤´ä¸­å®ç°éƒ¨åˆ†ä¹˜ç§¯ã€‚è¿™ä¸¤ç§è¡¨ç¤ºéƒ½æ˜¯ç›´è§‚å’Œé«˜æ•ˆçš„ï¼Œè€Œæ ‡å‡†å¾®è°ƒæ¨¡å‹åˆ™ç¼ºä¹è¿™äº›ã€‚å€ŸåŠ©è¿™äº›è§è§£ï¼Œæˆ‘ä»¬é‡æ–°å®¡è§†äº†æ ‡å‡†å¾®è°ƒçš„å­¦ä¹ åŠ¨æ€ï¼Œå‘ç°æ¨¡å‹æ”¶æ•›åˆ°ç¼ºä¹æ‰€éœ€é•¿ç¨‹ä¾èµ–å…³ç³»çš„å±€éƒ¨æœ€ä¼˜è§£ã€‚æˆ‘ä»¬é€šè¿‡å¼•å…¥ä¸€ä¸ªè¾…åŠ©æŸå¤±æ¥éªŒè¯è¿™ä¸€ç†è§£ï¼Œè¯¥æŸå¤±é€šè¿‡çº¿æ€§å›å½’æ¢é’ˆé¢„æµ‹â€œè¿è¡Œæ€»å’Œâ€ï¼Œä¸ºæ¨¡å‹æä¾›äº†ä¸€ä¸ªå½’çº³åç½®ï¼Œä½¿å…¶èƒ½å¤ŸæˆåŠŸå­¦ä¹ å¤šä½æ•°ä¹˜æ³•ã€‚æ€»ä¹‹ï¼Œé€šè¿‡é€†å‘å·¥ç¨‹éšå¼æ€ç»´é“¾æ¨¡å‹çš„æœºåˆ¶ï¼Œæˆ‘ä»¬å‘ç°äº†Transformerä¸­å­¦ä¹ é•¿ç¨‹ä¾èµ–å…³ç³»çš„é™·é˜±ï¼Œå¹¶æä¾›äº†å¦‚ä½•æ­£ç¡®åº”ç”¨å½’çº³åç½®æ¥è§£å†³è¿™ä¸ªé—®é¢˜çš„ä¾‹å­ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.00184v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹è™½åŠŸèƒ½æ—¥ç›Šå¼ºå¤§ï¼Œä½†åœ¨è¿›è¡Œå¤šä½æ•°ä¹˜æ³•è¿ç®—çš„ç®€å•ä»»åŠ¡æ—¶ä»ä¼šå‡ºç°å›°éš¾ã€‚æœ¬ç ”ç©¶é€šè¿‡é€†å‘å·¥ç¨‹ç ”ç©¶äº†ä¸€ä¸ªæˆåŠŸå­¦ä¹ ä¹˜æ³•è¿ç®—çš„æ¨¡å‹ï¼Œå‘ç°æ¨¡å‹é€šè¿‡éšå¼æ€ç»´é“¾ç¼–ç äº†å¿…è¦çš„é•¿ç¨‹ä¾èµ–å…³ç³»ï¼Œå¹¶åˆ©ç”¨æ³¨æ„åŠ›æœºåˆ¶æ„å»ºæœ‰å‘æ— ç¯å›¾ä»¥ç¼“å­˜å’Œæ£€ç´¢éƒ¨åˆ†ä¹˜ç§¯ã€‚æ¨¡å‹çš„ä¹˜æ³•å‡ ä½•è¿ç®—ä¸­åŒ…æ‹¬é—µå¯å¤«æ–¯åŸºå’Œä»¥åŠå‚…é‡Œå¶åŸºåº•ç­‰ç›´è§‚å’Œé«˜æ•ˆçš„è¡¨ç¤ºå½¢å¼ã€‚ç¼ºä¹è¿™ç§éšå¼æ€ç»´é“¾æœºåˆ¶çš„æ¨¡å‹å®¹æ˜“é™·å…¥ç¼ºä¹å¿…è¦é•¿ç¨‹ä¾èµ–çš„å±€éƒ¨æœ€ä¼˜è§£ã€‚é€šè¿‡å¼•å…¥é¢„æµ‹è¿è¡Œå’Œçš„è¾…åŠ©æŸå¤±å‡½æ•°ï¼Œæä¾›äº†å­¦ä¹ é•¿ç¨‹ä¾èµ–çš„å½’çº³åç½®è§£å†³æ–¹æ¡ˆã€‚æ€»ä¹‹ï¼Œæœ¬ç ”ç©¶æ­ç¤ºäº†Transformerä¸­å­¦ä¹ é•¿ç¨‹ä¾èµ–çš„é™·é˜±ï¼Œå¹¶å±•ç¤ºäº†å¦‚ä½•å€ŸåŠ©å½’çº³åç½®æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šä½æ•°ä¹˜æ³•ä»»åŠ¡ä¸Šä»æœ‰æŒ‘æˆ˜ã€‚</li>
<li>é€šè¿‡é€†å‘å·¥ç¨‹ï¼Œå‘ç°æ¨¡å‹æˆåŠŸå­¦ä¹ ä¹˜æ³•çš„å…³é”®åœ¨äºéšå¼æ€ç»´é“¾æœºåˆ¶ã€‚</li>
<li>æ¨¡å‹é€šè¿‡ç¼–ç é•¿ç¨‹ä¾èµ–å…³ç³»è¿›è¡Œä¹˜æ³•è¿ç®—ï¼Œåˆ©ç”¨æ³¨æ„åŠ›æœºåˆ¶ç¼“å­˜å’Œæ£€ç´¢éƒ¨åˆ†ä¹˜ç§¯ã€‚</li>
<li>æ¨¡å‹é‡‡ç”¨ç›´è§‚å’Œé«˜æ•ˆçš„ä¹˜æ³•å‡ ä½•è¿ç®—å½¢å¼ï¼Œå¦‚é—µå¯å¤«æ–¯åŸºå’Œä»¥åŠå‚…é‡Œå¶åŸºåº•è¡¨ç¤ºã€‚</li>
<li>ç¼ºä¹éšå¼æ€ç»´é“¾æœºåˆ¶çš„æ¨¡å‹æ˜“é™·å…¥ç¼ºä¹å¿…è¦é•¿ç¨‹ä¾èµ–çš„å±€éƒ¨æœ€ä¼˜è§£ã€‚</li>
<li>é€šè¿‡å¼•å…¥é¢„æµ‹è¿è¡Œå’Œçš„è¾…åŠ©æŸå¤±å‡½æ•°ï¼Œæä¾›äº†å­¦ä¹ é•¿ç¨‹ä¾èµ–çš„å½’çº³åç½®è§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.00184">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_LLM/2510.00184v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_LLM/2510.00184v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_LLM/2510.00184v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_LLM/2510.00184v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_LLM/2510.00184v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-04/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-04/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-04/Agent/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_Agent/2510.02204v1/page_5_0.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-04  InfoMosaic-Bench Evaluating Multi-Source Information Seeking in   Tool-Augmented Agents
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-04
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-04/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-10-04\./crop_R1_Reasoning/2510.01624v1/page_0_0.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-04  KaVa Latent Reasoning via Compressed KV-Cache Distillation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-04
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">29739.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
