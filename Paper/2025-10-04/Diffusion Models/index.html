<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-10-04  Optimal Control Meets Flow Matching A Principled Route to Multi-Subject   Fidelity">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic-private.zhihu.com/v2-4fd7c05622c6bb8b0f574ec8ddaec0c9~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030463&auth_key=1760030463-0-0-793ca779bce6d66c56c6da9122d0f847&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-10-04
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-10-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    22.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    89 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-10-04-更新"><a href="#2025-10-04-更新" class="headerlink" title="2025-10-04 更新"></a>2025-10-04 更新</h1><h2 id="Optimal-Control-Meets-Flow-Matching-A-Principled-Route-to-Multi-Subject-Fidelity"><a href="#Optimal-Control-Meets-Flow-Matching-A-Principled-Route-to-Multi-Subject-Fidelity" class="headerlink" title="Optimal Control Meets Flow Matching: A Principled Route to Multi-Subject   Fidelity"></a>Optimal Control Meets Flow Matching: A Principled Route to Multi-Subject   Fidelity</h2><p><strong>Authors:Eric Tillmann Bill, Enis Simsar, Thomas Hofmann</strong></p>
<p>Text-to-image (T2I) models excel on single-entity prompts but struggle with multi-subject descriptions, often showing attribute leakage, identity entanglement, and subject omissions. We introduce the first theoretical framework with a principled, optimizable objective for steering sampling dynamics toward multi-subject fidelity. Viewing flow matching (FM) through stochastic optimal control (SOC), we formulate subject disentanglement as control over a trained FM sampler. This yields two architecture-agnostic algorithms: (i) a training-free test-time controller that perturbs the base velocity with a single-pass update, and (ii) Adjoint Matching, a lightweight fine-tuning rule that regresses a control network to a backward adjoint signal while preserving base-model capabilities. The same formulation unifies prior attention heuristics, extends to diffusion models via a flow-diffusion correspondence, and provides the first fine-tuning route explicitly designed for multi-subject fidelity. Empirically, on Stable Diffusion 3.5, FLUX, and Stable Diffusion XL, both algorithms consistently improve multi-subject alignment while maintaining base-model style. Test-time control runs efficiently on commodity GPUs, and fine-tuned controllers trained on limited prompts generalize to unseen ones. We further highlight FOCUS (Flow Optimal Control for Unentangled Subjects), which achieves state-of-the-art multi-subject fidelity across models. </p>
<blockquote>
<p>文本到图像（T2I）模型在单实体提示方面表现出色，但在多主题描述方面却表现挣扎，常常出现属性泄露、身份纠缠和主题遗漏的问题。我们引入了第一个理论框架，该框架具有原则性且可优化的目标，旨在引导采样动态实现多主题保真。通过随机最优控制（SOC）来观察流匹配（FM），我们将主题解耦公式化为对训练好的FM采样器的控制。这产生了两种与架构无关的算法：（i）一种无训练测试时间控制器，它可以在单次更新中扰动基本速度；（ii）伴随匹配是一种轻量级的微调规则，它回归一个控制网络以产生向后伴随信号，同时保留基础模型的能力。同样的公式统一了先前的注意力启发式方法，通过流扩散对应关系扩展到扩散模型，并为多主题保真提供了第一个明确设计的微调路径。在Stable Diffusion 3.5、FLUX和Stable Diffusion XL上的经验表明，两种算法都能一致地提高多主题对齐性，同时保持基础模型风格。测试时间控制可在商品GPU上高效运行，而在有限提示上训练的微调控制器可以推广到未见过的提示。我们进一步强调了FOCUS（用于无纠缠主题的最优流控制），它在各种模型中实现了最先进的主题保真度。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.02315v1">PDF</a> Code: <a target="_blank" rel="noopener" href="https://github.com/ericbill21/FOCUS/">https://github.com/ericbill21/FOCUS/</a></p>
<p><strong>摘要</strong></p>
<p>文本至图像（T2I）模型在单实体提示上表现优异，但在多主题描述方面存在属性泄露、身份混淆及主题遗漏等问题。本文首次提出了一个理论框架，通过优化目标来引导采样动态以实现多主题保真。通过随机最优控制（SOC）来观察流匹配（FM），我们将主题分离表现为对训练过的FM采样器的控制，从而得到两种与架构无关的算法：一是无需训练的检测时间控制器，它可以通过单次更新来扰动基本速度；二是回归控制网络的反向伴随信号的同时保留基础模型能力的Adjoint Matching轻量级微调规则。同一表述统一了先前的注意力启发式方法，通过流动扩散对应关系扩展到了扩散模型，并为多主题保真提供了第一条精心设计过的微调路线。在Stable Diffusion 3.5、FLUX和Stable Diffusion XL上的实验表明，两种算法均能在保持基础模型风格的同时，不断提高多主题对齐性。检测时间控制可在商品GPU上高效运行，经过有限提示训练的控制器可推广至未见过的提示。本文进一步强调了FOCUS（用于无纠缠主题的流动最优控制），它在各种模型中实现了最佳的多主题保真度。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>T2I模型在单实体提示上表现良好，但在多主题描述方面存在挑战，如属性泄露、身份混淆和主题遗漏。</li>
<li>引入了一个理论框架，通过优化目标来引导采样动态，以实现多主题保真。</li>
<li>提出了两种与架构无关的算法：测试时间控制器和Adjoint Matching，以提高多主题对齐性并保留基础模型风格。</li>
<li>首次为扩散模型提供了专门设计的微调路线。</li>
<li>实证结果显示，所提方法在多种模型上实现了卓越的多主题保真度。</li>
<li>测试时间控制可在商品GPU上高效运行，且经过有限提示训练的控制器具有良好的泛化能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.02315">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-18ed38116073d4b48c82a9ce8cc39d28~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030470&auth_key=1760030470-0-0-d56bac89008c4f918047020ca432a21c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2dbbc0a898872c9d99a7386f9aa142c2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030478&auth_key=1760030478-0-0-d35550c97e38c9ea7fc5cefd0c8fe86f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6326edb5b51fb5e1acb79b5ec79dd9c4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030485&auth_key=1760030485-0-0-437e329dd06b5fbad9b3cb6e7b09e075&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Continual-Personalization-for-Diffusion-Models"><a href="#Continual-Personalization-for-Diffusion-Models" class="headerlink" title="Continual Personalization for Diffusion Models"></a>Continual Personalization for Diffusion Models</h2><p><strong>Authors:Yu-Chien Liao, Jr-Jen Chen, Chi-Pin Huang, Ci-Siang Lin, Meng-Lin Wu, Yu-Chiang Frank Wang</strong></p>
<p>Updating diffusion models in an incremental setting would be practical in real-world applications yet computationally challenging. We present a novel learning strategy of Concept Neuron Selection (CNS), a simple yet effective approach to perform personalization in a continual learning scheme. CNS uniquely identifies neurons in diffusion models that are closely related to the target concepts. In order to mitigate catastrophic forgetting problems while preserving zero-shot text-to-image generation ability, CNS finetunes concept neurons in an incremental manner and jointly preserves knowledge learned of previous concepts. Evaluation of real-world datasets demonstrates that CNS achieves state-of-the-art performance with minimal parameter adjustments, outperforming previous methods in both single and multi-concept personalization works. CNS also achieves fusion-free operation, reducing memory storage and processing time for continual personalization. </p>
<blockquote>
<p>在现实世界的应用中，逐步更新扩散模型是实用的，但在计算上存在挑战。我们提出了一种新的学习策略——概念神经元选择（CNS），这是一种简单而有效的持续学习方案进行个性化方法。CNS能够独特地识别与扩散模型中的目标概念密切相关的神经元。为了缓解灾难性遗忘问题并保留零样本文本到图像生成能力，CNS以增量方式微调概念神经元并联合保留先前概念的知识。对真实数据集的评价表明，CNS以最小的参数调整实现了最先进的性能，在单概念和多概念个性化工作中都优于以前的方法。CNS还实现了无融合操作，减少了持续个性化所需的内存存储和处理时间。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.02296v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种新颖的学习策略——概念神经元选择（CNS），用于增量更新扩散模型。CNS能准确识别与目标概念紧密相关的神经元，并在增量学习方案中实现对模型的个性化调整。为了缓解灾难性遗忘问题并保持零样本文本到图像生成能力，CNS以增量方式微调概念神经元并联合保留先前概念的知识。在真实数据集上的评估表明，CNS通过最小的参数调整实现了卓越性能，在单概念和多概念个性化工作中均优于先前方法。此外，CNS实现了无融合操作，降低了连续个性化任务的内存存储和处理时间。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型在现实世界应用中的增量更新具有实践意义且计算具有挑战性。</li>
<li>概念神经元选择（CNS）是一种简单而有效的方法，可在持续学习方案中进行个性化调整。</li>
<li>CNS能识别与目标概念相关的神经元。</li>
<li>CNS通过微调概念神经元缓解灾难性遗忘问题，并保留先前知识。</li>
<li>在真实数据集上的评估显示CNS性能卓越，优于其他方法。</li>
<li>CNS实现无融合操作，降低内存存储和处理时间。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.02296">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-15e3b19584fba84acd1a9d19a4ab04a0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030492&auth_key=1760030492-0-0-1590bf9e82f58687e0a64e642c3b3b24&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-26aea0e703637b4bd9a4ccaa75b4aa2f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030500&auth_key=1760030500-0-0-e79d91b25602faa69519e7c5f538637b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d737bd013e827212e65ae0baa3bd5e16~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030506&auth_key=1760030506-0-0-9a2e48173fb526cb94aeef2c4bd529c6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8f52d64f662e8f70a48a22f031fc2334~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030513&auth_key=1760030513-0-0-24d9dc84cfa989842e8df4747bb312b0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Test-Time-Anchoring-for-Discrete-Diffusion-Posterior-Sampling"><a href="#Test-Time-Anchoring-for-Discrete-Diffusion-Posterior-Sampling" class="headerlink" title="Test-Time Anchoring for Discrete Diffusion Posterior Sampling"></a>Test-Time Anchoring for Discrete Diffusion Posterior Sampling</h2><p><strong>Authors:Litu Rout, Andreas Lugmayr, Yasamin Jafarian, Srivatsan Varadharajan, Constantine Caramanis, Sanjay Shakkottai, Ira Kemelmacher-Shlizerman</strong></p>
<p>We study the problem of posterior sampling using pretrained discrete diffusion foundation models, aiming to recover images from noisy measurements without retraining task-specific models. While diffusion models have achieved remarkable success in generative modeling, most advances rely on continuous Gaussian diffusion. In contrast, discrete diffusion offers a unified framework for jointly modeling categorical data such as text and images. Beyond unification, discrete diffusion provides faster inference, finer control, and principled training-free Bayesian inference, making it particularly well-suited for posterior sampling. However, existing approaches to discrete diffusion posterior sampling face severe challenges: derivative-free guidance yields sparse signals, continuous relaxations limit applicability, and split Gibbs samplers suffer from the curse of dimensionality. To overcome these limitations, we introduce Anchored Posterior Sampling (APS) for masked diffusion foundation models, built on two key innovations – quantized expectation for gradient-like guidance in discrete embedding space, and anchored remasking for adaptive decoding. Our approach achieves state-of-the-art performance among discrete diffusion samplers across linear and nonlinear inverse problems on the standard benchmarks. We further demonstrate the benefits of our approach in training-free stylization and text-guided editing. </p>
<blockquote>
<p>我们研究了使用后向采样的问题，利用了预训练的离散扩散基础模型，旨在从噪声测量中恢复图像，而无需重新训练特定任务模型。虽然扩散模型在生成建模方面取得了显著的成功，但大多数进展都依赖于连续的高斯扩散。相比之下，离散扩散提供了一个统一的框架，可以联合对文本和图像等分类数据进行建模。除了统一建模，离散扩散还提供了更快的推理速度、更精细的控制和基于原理的无训练贝叶斯推理，使其成为后向采样的理想选择。然而，现有的离散扩散后向采样方法面临严重的挑战：无导数指导产生稀疏信号、连续松弛限制了适用性，而分裂吉布斯采样器受到维数诅咒的影响。为了克服这些局限性，我们为掩膜扩散基础模型引入了锚点后向采样（APS），这是基于两项关键创新——用于离散嵌入空间中梯度引导的量化的期望和用于自适应解码的锚点重新掩蔽。我们的方法在所有标准基准测试中的线性和非线性反问题上均实现了离散扩散采样器的最佳性能。我们还进一步展示了我们的方法在无需训练的风格化和文本引导编辑中的优势。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.02291v1">PDF</a> Preprint</p>
<p><strong>摘要</strong></p>
<p>研究使用预训练的离散扩散基础模型进行后采样的问题，旨在从噪声测量中恢复图像，而无需重新训练特定任务模型。虽然扩散模型在生成建模方面取得了显著的成功，但大多数进展都依赖于连续高斯扩散。相比之下，离散扩散提供了一个统一的框架，可以联合对文本和图像等分类数据进行建模。离散扩散除了统一建模外，还提供了更快的推理速度、更精细的控制和基于原则的无训练贝叶斯推理，使其特别适用于后采样。然而，现有的离散扩散后采样方法面临严重挑战：无导数指导产生稀疏信号、连续放松限制应用，分裂吉布斯采样器受到维度诅咒的影响。为了克服这些限制，我们引入了锚定后采样（APS）用于掩模扩散基础模型，基于两项关键创新——量化期望用于离散嵌入空间中的梯度引导，以及锚定遮罩用于自适应解码。我们的方法在标准基准测试上的离散扩散采样器中的线性和非线性反问题方面实现了最佳性能。我们还进一步展示了我们的方法在无需训练和文本引导编辑方面的优势。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>研究使用预训练离散扩散基础模型进行后采样，旨在从噪声中恢复图像，无需针对特定任务重新训练模型。</li>
<li>离散扩散提供了一个统一的框架，可以联合对分类数据进行建模，如文本和图像。</li>
<li>现有离散扩散后采样方法面临挑战，如无导数指导产生的稀疏信号、连续放松的限制和分裂吉布斯采样器的维度诅咒。</li>
<li>引入的锚定后采样（APS）方法基于两项关键创新：量化期望用于梯度引导，以及锚定遮罩用于自适应解码。</li>
<li>APS方法在线性和非线性反问题上实现了最佳性能，在标准基准测试中优于其他离散扩散采样器。</li>
<li>APS方法在无需训练的样式化和文本引导编辑方面显示出优势。</li>
<li>研究表明，离散扩散模型在生成建模领域具有广阔的应用前景，特别是在后采样和基于文本的图像编辑等方面。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.02291">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-00b466a9e04cdda7ca8f0a9fd3ffb478~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030521&auth_key=1760030521-0-0-1532792cd8a7d5408fd2d440a2ffba41&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b470577238d43915553216f980e61134~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030529&auth_key=1760030529-0-0-ead113b7b5ef9d842ac42b5e6283dfdf&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f5859238b5a6d552eee6689277238615~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030536&auth_key=1760030536-0-0-b21c428471c79818d3037f4430ac0149&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Learning-to-Generate-Object-Interactions-with-Physics-Guided-Video-Diffusion"><a href="#Learning-to-Generate-Object-Interactions-with-Physics-Guided-Video-Diffusion" class="headerlink" title="Learning to Generate Object Interactions with Physics-Guided Video   Diffusion"></a>Learning to Generate Object Interactions with Physics-Guided Video   Diffusion</h2><p><strong>Authors:David Romero, Ariana Bermudez, Hao Li, Fabio Pizzati, Ivan Laptev</strong></p>
<p>Recent models for video generation have achieved remarkable progress and are now deployed in film, social media production, and advertising. Beyond their creative potential, such models also hold promise as world simulators for robotics and embodied decision making. Despite strong advances, however, current approaches still struggle to generate physically plausible object interactions and lack physics-grounded control mechanisms. To address this limitation, we introduce KineMask, an approach for physics-guided video generation that enables realistic rigid body control, interactions, and effects. Given a single image and a specified object velocity, our method generates videos with inferred motions and future object interactions. We propose a two-stage training strategy that gradually removes future motion supervision via object masks. Using this strategy we train video diffusion models (VDMs) on synthetic scenes of simple interactions and demonstrate significant improvements of object interactions in real scenes. Furthermore, KineMask integrates low-level motion control with high-level textual conditioning via predictive scene descriptions, leading to effective support for synthesis of complex dynamical phenomena. Extensive experiments show that KineMask achieves strong improvements over recent models of comparable size. Ablation studies further highlight the complementary roles of low- and high-level conditioning in VDMs. Our code, model, and data will be made publicly available. </p>
<blockquote>
<p>视频生成模型最近取得了显著的进步，并已应用于电影、社交媒体生产和广告中。除了创造性潜力之外，这些模型在机器人和实体决策制定的世界模拟器方面也表现出希望。然而，尽管取得了重大进展，但当前的方法仍然难以生成物理上合理的物体交互，并且缺乏基于物理的控制机制。为了解决这个问题，我们引入了KineMask，这是一种物理指导的视频生成方法，能够实现现实的刚体控制、交互和效果。给定单个图像和指定的对象速度，我们的方法可以生成具有推断运动和未来对象交互的视频。我们提出了一种两阶段训练策略，通过对象掩膜逐步消除未来运动监督。使用这种策略，我们在简单交互的合成场景上训练视频扩散模型（VDMs），并在实际场景中展示了物体交互的重大改进。此外，KineMask通过预测场景描述将低级运动控制与高级文本条件相结合，有效支持了复杂动态现象的综合。大量实验表明，KineMask在同类规模的最新模型上取得了显著改进。消融研究进一步突出了低级和高级条件在VDMs中的互补作用。我们的代码、模型和数据将公开可用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.02284v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>近期视频生成模型在电影、社交媒体制作和广告等领域取得了显著进展，并在机器人和实体决策模拟方面展现出潜力。然而，当前方法难以生成物理上合理的物体互动，并缺乏物理基础的控制机制。为此，我们提出KineMask方法，这是一种物理指导的视频生成方法，能够实现现实的刚体控制、互动和效果。给定单张图像和指定物体速度，我们的方法可以生成具有推断运动和未来物体互动的视频。我们提出了一种两阶段训练策略，通过物体掩膜逐步消除未来运动监督。使用这种策略，我们在合成简单互动场景的视频扩散模型（VDMs）训练上取得了显著改进，并在实际场景中展示了物体互动的改善。此外，KineMask通过将低级运动控制与高级文本条件相结合，实现了复杂动态现象的合成支持。实验表明，KineMask在同类规模模型中取得了显著改进。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>视频生成模型在多个领域取得显著进展，并展现出在机器人和实体决策模拟方面的潜力。</li>
<li>当前模型难以生成物理上合理的物体互动，缺乏物理基础的控制机制。</li>
<li>KineMask是一种物理指导的视频生成方法，能实现现实的刚体控制、互动和效果。</li>
<li>KineMask可以通过给定的单张图像和物体速度生成具有推断运动和未来物体互动的视频。</li>
<li>两阶段训练策略逐步消除未来运动监督，改进了视频扩散模型（VDMs）的训练。</li>
<li>KineMask集成了低级运动控制和高级文本条件，支持复杂动态现象的合成。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.02284">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-4ed0496045562a6400b3086b83fcc111~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030544&auth_key=1760030544-0-0-7945cdb3a9358e45ba2f25b7e8e5b28f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ba534e22e45077133ee420ee4508e152~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030552&auth_key=1760030552-0-0-15bf6c20273b5594918476c9d710fe51&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a5a774bc547e2a4d4931115f337151b5~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030559&auth_key=1760030559-0-0-44b91e6679edd9ee70db399300fed41c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5bd43d41bb05a4458c97c3c08b177bcc~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030565&auth_key=1760030565-0-0-078f153e113917fd20666551783ae3a1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-39bf0325c46f7a7872e107d388926f32~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030572&auth_key=1760030572-0-0-cd81276d0e9bd274373865eac3aa0d8e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Self-Forcing-Towards-Minute-Scale-High-Quality-Video-Generation"><a href="#Self-Forcing-Towards-Minute-Scale-High-Quality-Video-Generation" class="headerlink" title="Self-Forcing++: Towards Minute-Scale High-Quality Video Generation"></a>Self-Forcing++: Towards Minute-Scale High-Quality Video Generation</h2><p><strong>Authors:Justin Cui, Jie Wu, Ming Li, Tao Yang, Xiaojie Li, Rui Wang, Andrew Bai, Yuanhao Ban, Cho-Jui Hsieh</strong></p>
<p>Diffusion models have revolutionized image and video generation, achieving unprecedented visual quality. However, their reliance on transformer architectures incurs prohibitively high computational costs, particularly when extending generation to long videos. Recent work has explored autoregressive formulations for long video generation, typically by distilling from short-horizon bidirectional teachers. Nevertheless, given that teacher models cannot synthesize long videos, the extrapolation of student models beyond their training horizon often leads to pronounced quality degradation, arising from the compounding of errors within the continuous latent space. In this paper, we propose a simple yet effective approach to mitigate quality degradation in long-horizon video generation without requiring supervision from long-video teachers or retraining on long video datasets. Our approach centers on exploiting the rich knowledge of teacher models to provide guidance for the student model through sampled segments drawn from self-generated long videos. Our method maintains temporal consistency while scaling video length by up to 20x beyond teacher’s capability, avoiding common issues such as over-exposure and error-accumulation without recomputing overlapping frames like previous methods. When scaling up the computation, our method shows the capability of generating videos up to 4 minutes and 15 seconds, equivalent to 99.9% of the maximum span supported by our base model’s position embedding and more than 50x longer than that of our baseline model. Experiments on standard benchmarks and our proposed improved benchmark demonstrate that our approach substantially outperforms baseline methods in both fidelity and consistency. Our long-horizon videos demo can be found at <a target="_blank" rel="noopener" href="https://self-forcing-plus-plus.github.io/">https://self-forcing-plus-plus.github.io/</a> </p>
<blockquote>
<p>扩散模型已经彻底改变了图像和视频生成领域，实现了前所未有的视觉质量。然而，它们依赖于变压器架构，产生了高昂的计算成本，特别是在将生成扩展到长视频时。近期的工作已经探索了长视频生成的自回归公式，通常是通过从短视野双向教师模型中进行蒸馏。然而，由于教师模型无法合成长视频，学生模型在超出其训练视野的推广往往会导致质量显著下降，这是由于连续潜在空间内错误累积导致的。在本文中，我们提出了一种简单而有效的方法来缓解长视野视频生成中的质量下降问题，而无需从长视频教师那里获得监督或重新训练长视频数据集。我们的方法主要利用教师模型的丰富知识，通过从学生模型自我生成的长视频中抽取的片段来提供指导。我们的方法在保持时间一致性的同时，通过将视频长度扩展到教师能力的20倍，避免了过度曝光和错误累积等常见问题，而且无需像之前的方法那样重新计算重叠帧。在计算规模扩大时，我们的方法能够生成长达4分钟15秒的视频，相当于我们基础模型位置嵌入所支持的最大跨度的99.9%，并且比我们基线模型长50倍以上。在标准基准测试和我们提出的改进基准测试上的实验表明，我们的方法在保真度和一致性方面显著优于基线方法。您可以在 <a target="_blank" rel="noopener" href="https://self-forcing-plus-plus.github.io/">https://self-forcing-plus-plus.github.io/</a> 找到我们的长视野视频演示。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.02283v1">PDF</a> preprint</p>
<p><strong>Summary</strong></p>
<p>本文介绍了扩散模型在图像和视频生成方面的革命性进展，其视觉质量达到了前所未有的水平。然而，由于依赖变压器架构，扩散模型计算成本高昂，尤其是在长视频生成方面的扩展性受限。最近的研究尝试通过从短期双向教师模型中提炼出自回归公式进行长视频生成。然而，由于教师模型无法合成长视频，学生模型在超出训练范围的情况下会产生明显的质量下降问题。本文提出了一种简单有效的方法，在不依赖长视频教师模型的监督或重新训练长视频数据集的情况下，减轻长周期视频生成中的质量下降问题。该方法通过利用教师模型的丰富知识，通过从自我生成的长视频中抽取片段来指导模型，在保持时间一致性的同时，将视频长度扩大了高达教师能力的20倍。实验证明，该方法在保真度和一致性方面显著优于基线方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型在图像和视频生成中取得了显著进展，但存在高计算成本问题，特别是在长视频生成方面。</li>
<li>现有方法尝试通过教师模型指导学生模型进行长视频生成，但存在质量下降问题。</li>
<li>本文提出了一种简单有效的方法，通过利用教师模型的丰富知识，从自我生成的长视频中抽取片段来指导模型，提高了长周期视频生成的质量。</li>
<li>该方法能够在不重新训练或依赖长视频教师模型的情况下，将视频长度扩大高达教师能力的20倍。</li>
<li>实验证明，该方法在标准基准测试和我们改进后的基准测试中均显著优于基线方法。</li>
<li>该方法能够生成长达4分钟15秒的视频，相当于基础模型位置嵌入所支持的最大跨度的99.9%，并且比基线模型长出50倍以上。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.02283">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-7f7630f0a973200a5b135bb4f796dc34~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030579&auth_key=1760030579-0-0-e5577a98fe826ec094b789ba684f4ce9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ef5363f6d53d41c680e88654968dc009~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030586&auth_key=1760030586-0-0-f6351645878c7f5e36c581c7f5ecb6dd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-bbda4f8101deffc9c86c78b07f487fe3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030593&auth_key=1760030593-0-0-8f60434e472711aee6dc62ddd7048b97&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="VGDM-Vision-Guided-Diffusion-Model-for-Brain-Tumor-Detection-and-Segmentation"><a href="#VGDM-Vision-Guided-Diffusion-Model-for-Brain-Tumor-Detection-and-Segmentation" class="headerlink" title="VGDM: Vision-Guided Diffusion Model for Brain Tumor Detection and   Segmentation"></a>VGDM: Vision-Guided Diffusion Model for Brain Tumor Detection and   Segmentation</h2><p><strong>Authors:Arman Behnam</strong></p>
<p>Accurate detection and segmentation of brain tumors from magnetic resonance imaging (MRI) are essential for diagnosis, treatment planning, and clinical monitoring. While convolutional architectures such as U-Net have long been the backbone of medical image segmentation, their limited capacity to capture long-range dependencies constrains performance on complex tumor structures. Recent advances in diffusion models have demonstrated strong potential for generating high-fidelity medical images and refining segmentation boundaries.   In this work, we propose VGDM: Vision-Guided Diffusion Model for Brain Tumor Detection and Segmentation framework, a transformer-driven diffusion framework for brain tumor detection and segmentation. By embedding a vision transformer at the core of the diffusion process, the model leverages global contextual reasoning together with iterative denoising to enhance both volumetric accuracy and boundary precision. The transformer backbone enables more effective modeling of spatial relationships across entire MRI volumes, while diffusion refinement mitigates voxel-level errors and recovers fine-grained tumor details.   This hybrid design provides a pathway toward improved robustness and scalability in neuro-oncology, moving beyond conventional U-Net baselines. Experimental validation on MRI brain tumor datasets demonstrates consistent gains in Dice similarity and Hausdorff distance, underscoring the potential of transformer-guided diffusion models to advance the state of the art in tumor segmentation. </p>
<blockquote>
<p>从磁共振成像（MRI）准确检测和分割脑肿瘤对于诊断、治疗计划和临床监测至关重要。虽然U-Net等卷积架构长期以来一直是医学图像分割的支柱，但其捕捉长距离依赖关系的有限能力在复杂肿瘤结构上的表现受到限制。扩散模型的最新进展显示出生成高保真医学图像和细化分割边界方面的巨大潜力。在这项工作中，我们提出了VGDM：用于脑肿瘤检测和分割的基于视觉引导的扩散模型框架，这是一个用于脑肿瘤检测和分割的基于变换驱动的扩散框架。通过在扩散过程的核心中嵌入视觉变换器，该模型利用全局上下文推理和迭代去噪来提高体积精度和边界精度。变换骨干网可以更有效地对整个MRI体积的空间关系进行建模，而扩散细化则减轻了体素级别的错误并恢复了精细的肿瘤细节。这种混合设计提供了在神经肿瘤学中提高稳健性和可扩展性的途径，超越了传统的U-Net基准线。在MRI脑肿瘤数据集上的实验验证证明了Dice相似度和Hausdorff距离的持续提高，突显了基于变换的扩散模型在肿瘤分割方面推动最新技术的潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.02086v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种基于扩散模型的脑肿瘤检测与分割框架VGDM，该框架融合了卷积网络和扩散模型的优点。通过使用核心中的视觉变压器并结合全局上下文推理和迭代去噪，模型在三维体积准确性和边界精度上都有所提升。在MRI脑肿瘤数据集上的实验验证证明了该混合设计相较于传统U-Net基准线的优越性和潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>脑肿瘤的准确检测和分割对诊断、治疗计划和临床监测至关重要。</li>
<li>现有方法如U-Net在复杂肿瘤结构上的性能受到限制，因为它们的长期依赖捕捉能力有限。</li>
<li>扩散模型具有生成高质量医学图像和细化分割边界的潜力。</li>
<li>提出的VGDM框架结合了卷积网络和扩散模型的优点，使用视觉变压器为核心。</li>
<li>该模型利用全局上下文推理和迭代去噪，提高了体积准确性和边界精度。</li>
<li>视觉变压器能有效建模整个MRI体积的空间关系。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.02086">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-b1bbd52fe85d59156603ba5ce1e53fd2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030601&auth_key=1760030601-0-0-b908436a37d41e8325e1d2df3ddea8be&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9c06905b4edc087b85564e79ea3894d3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030608&auth_key=1760030608-0-0-f2f088a856ca9c26e601d6088a98e8a9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="MSRepaint-Multiple-Sclerosis-Repaint-with-Conditional-Denoising-Diffusion-Implicit-Model-for-Bidirectional-Lesion-Filling-and-Synthesis"><a href="#MSRepaint-Multiple-Sclerosis-Repaint-with-Conditional-Denoising-Diffusion-Implicit-Model-for-Bidirectional-Lesion-Filling-and-Synthesis" class="headerlink" title="MSRepaint: Multiple Sclerosis Repaint with Conditional Denoising   Diffusion Implicit Model for Bidirectional Lesion Filling and Synthesis"></a>MSRepaint: Multiple Sclerosis Repaint with Conditional Denoising   Diffusion Implicit Model for Bidirectional Lesion Filling and Synthesis</h2><p><strong>Authors:Jinwei Zhang, Lianrui Zuo, Yihao Liu, Hang Zhang, Samuel W. Remedios, Bennett A. Landman, Peter A. Calabresi, Shiv Saidha, Scott D. Newsome, Dzung L. Pham, Jerry L. Prince, Ellen M. Mowry, Aaron Carass</strong></p>
<p>In multiple sclerosis, lesions interfere with automated magnetic resonance imaging analyses such as brain parcellation and deformable registration, while lesion segmentation models are hindered by the limited availability of annotated training data. To address both issues, we propose MSRepaint, a unified diffusion-based generative model for bidirectional lesion filling and synthesis that restores anatomical continuity for downstream analyses and augments segmentation through realistic data generation. MSRepaint conditions on spatial lesion masks for voxel-level control, incorporates contrast dropout to handle missing inputs, integrates a repainting mechanism to preserve surrounding anatomy during lesion filling and synthesis, and employs a multi-view DDIM inversion and fusion pipeline for 3D consistency with fast inference. Extensive evaluations demonstrate the effectiveness of MSRepaint across multiple tasks. For lesion filling, we evaluate both the accuracy within the filled regions and the impact on downstream tasks including brain parcellation and deformable registration. MSRepaint outperforms the traditional lesion filling methods FSL and NiftySeg, and achieves accuracy on par with FastSurfer-LIT, a recent diffusion model-based inpainting method, while offering over 20 times faster inference. For lesion synthesis, state-of-the-art MS lesion segmentation models trained on MSRepaint-synthesized data outperform those trained on CarveMix-synthesized data or real ISBI challenge training data across multiple benchmarks, including the MICCAI 2016 and UMCL datasets. Additionally, we demonstrate that MSRepaint’s unified bidirectional filling and synthesis capability, with full spatial control over lesion appearance, enables high-fidelity simulation of lesion evolution in longitudinal MS progression. </p>
<blockquote>
<p>在多发性硬化症（MS）中，病灶会干扰自动磁共振成像分析，如脑部分割和可变形配准，而病灶分割模型则受到注释训练数据有限性的阻碍。为了解决这两个问题，我们提出了MSRepaint，这是一个基于扩散的统一生成模型，用于双向病灶填充和合成，它恢复了下游分析的解剖连续性，并通过现实数据的生成增强了分割。MSRepaint根据空间病灶掩膜进行体素级控制，采用对比丢失处理缺失输入，融入重绘机制在病灶填充和合成过程中保留周围解剖结构，并采用多视图DDIM反演和融合管道实现3D一致性以加快推理速度。广泛评估表明MSRepaint在多个任务中的有效性。对于病灶填充，我们评估了填充区域内的准确性以及对下游任务（包括脑部分割和可变形配准）的影响。MSRepaint优于传统的FSL和NiftySeg病灶填充方法，与基于扩散模型的FastSurfer-LIT方法精度相当，但推理速度超过其20倍。对于病灶合成，使用MSRepaint合成数据训练的最新多发性硬化病灶分割模型在多个基准测试上表现优于使用CarveMix合成数据或真实ISBI挑战赛训练数据训练的模型，包括MICCAI 2016和UMCL数据集。此外，我们证明了MSRepaint的统一双向填充和合成能力，以及对病灶外观的完全空间控制，能够高质量地模拟多发性硬化症纵向进展中的病灶演变。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.02063v1">PDF</a> </p>
<p><strong>Summary</strong><br>    本文提出一种基于扩散模型的双向病变填充与合成方法MSRepaint，用于解决多发性硬化症（MS）中的病灶干扰问题。该方法可恢复解剖连续性，促进下游分析和分割，并通过逼真的数据生成增强分割效果。实验证明，MSRepaint在多任务评估中表现优异，不仅提高了病灶填充的准确性，并提高了下游任务如脑分区和可变形注册的准确性。此外，基于MSRepaint合成的数据训练的MS病灶分割模型表现优于其他方法，并展示了高保真模拟病灶演变的潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MSRepaint是一种基于扩散模型的双向病变填充与合成方法，用于解决多发性硬化症中的病灶干扰问题。</li>
<li>MSRepaint可恢复解剖连续性，促进下游分析任务。</li>
<li>MSRepaint通过数据生成增强分割效果，提高病灶填充的准确性。</li>
<li>与传统方法相比，MSRepaint在病灶填充任务上表现出更高的准确性。</li>
<li>基于MSRepaint合成的数据训练的MS病灶分割模型表现优于其他训练方法。</li>
<li>MSRepaint具备高保真模拟病灶演变的潜力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.02063">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-3becfdae6470483030792001027301e9~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030615&auth_key=1760030615-0-0-a4f6a9c5e6b5adfdfdb7157a91cfa033&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-72821693b54fe474a229dfcffda5ed72~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030623&auth_key=1760030623-0-0-9212e96e8db75754662311b8af96ff6c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-22209115d17448c7073443a6aff3bd79~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030630&auth_key=1760030630-0-0-0eaba00a04dfef40a177c1e0ab416f0a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="ZK-WAGON-Imperceptible-Watermark-for-Image-Generation-Models-using-ZK-SNARKs"><a href="#ZK-WAGON-Imperceptible-Watermark-for-Image-Generation-Models-using-ZK-SNARKs" class="headerlink" title="ZK-WAGON: Imperceptible Watermark for Image Generation Models using   ZK-SNARKs"></a>ZK-WAGON: Imperceptible Watermark for Image Generation Models using   ZK-SNARKs</h2><p><strong>Authors:Aadarsh Anantha Ramakrishnan, Shubham Agarwal, Selvanayagam S, Kunwar Singh</strong></p>
<p>As image generation models grow increasingly powerful and accessible, concerns around authenticity, ownership, and misuse of synthetic media have become critical. The ability to generate lifelike images indistinguishable from real ones introduces risks such as misinformation, deepfakes, and intellectual property violations. Traditional watermarking methods either degrade image quality, are easily removed, or require access to confidential model internals - making them unsuitable for secure and scalable deployment. We are the first to introduce ZK-WAGON, a novel system for watermarking image generation models using the Zero-Knowledge Succinct Non Interactive Argument of Knowledge (ZK-SNARKs). Our approach enables verifiable proof of origin without exposing model weights, generation prompts, or any sensitive internal information. We propose Selective Layer ZK-Circuit Creation (SL-ZKCC), a method to selectively convert key layers of an image generation model into a circuit, reducing proof generation time significantly. Generated ZK-SNARK proofs are imperceptibly embedded into a generated image via Least Significant Bit (LSB) steganography. We demonstrate this system on both GAN and Diffusion models, providing a secure, model-agnostic pipeline for trustworthy AI image generation. </p>
<blockquote>
<p>随着图像生成模型的日益强大和普及，关于合成媒体的真实性、所有权和误用的担忧已经变得至关重要。能够生成与真实图像无法区分的逼真图像，引入了诸如虚假信息、深度伪造和知识产权侵犯等风险。传统的水印方法要么会降低图像质量，要么容易被移除，要么需要访问模型的内部机密信息，因此它们不适合进行安全和可扩展的部署。我们首次引入了ZK-WAGON系统，这是一种使用零知识简洁非交互证明知识（ZK-SNARKs）为图像生成模型添加水印的新系统。我们的方法能够在不暴露模型权重、生成提示或任何敏感内部信息的情况下，实现可验证的起源证明。我们提出了选择性层ZK电路创建（SL-ZKCC）方法，该方法能够有选择地将图像生成模型的关键层转换为电路，从而显著减少证明生成时间。生成的ZK-SNARK证明通过最低有效位（LSB）隐写术嵌入到生成的图像中，几乎无法被察觉。我们在生成对抗网络（GAN）和扩散模型上都展示了这一系统，为可信的AI图像生成提供了一个安全、模型无关的流程。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.01967v1">PDF</a> Accepted at AI-ML Systems 2025, Bangalore, India,   <a target="_blank" rel="noopener" href="https://www.aimlsystems.org/2025/">https://www.aimlsystems.org/2025/</a></p>
<p><strong>Summary</strong></p>
<p>图像生成模型的力量日渐强大与普及，引发了关于合成媒体真实性、所有权及误用的关注。生成逼真图像的能力引入了诸如错误信息、深度伪造和知识产权侵犯等风险。传统水印方法要么降低图像质量，要么易于移除，要么需要访问保密模型内部，因此不适合安全和可扩展的部署。我们首次引入ZK-WAGON，一种使用零知识简洁非交互论证知识（ZK-SNARKs）为图像生成模型加水印的新型系统。我们的方法能够在不暴露模型权重、生成提示或任何敏感内部信息的情况下，进行可验证的起源证明。我们提出选择性层ZK电路创建（SL-ZKCC），一种将图像生成模型的关键层选择性转换为电路的方法，显著减少证明生成时间。生成的ZK-SNARK证明通过最低有效位（LSB）隐写术嵌入到生成的图像中。我们在GAN和Diffusion模型上展示了这一系统，为可信AI图像生成提供了一个安全、模型无关的流程。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>图像生成模型的强大与普及带来了关于合成媒体真实性、所有权和误用的关键关注点。</li>
<li>生成逼真图像的能力可能导致错误信息、深度伪造和知识产权侵犯等风险。</li>
<li>传统水印方法存在缺陷，不适合用于图像生成模型的部署。</li>
<li>ZK-WAGON系统利用ZK-SNARKs为图像生成模型加水印，实现可验证的起源证明。</li>
<li>ZK-WAGON方法无需暴露模型权重、生成提示或敏感内部信息。</li>
<li>SL-ZKCC方法能选择性转换图像生成模型的关键层为电路，减少证明生成时间。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.01967">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-3c95d7808604a34bbe13c05652132f21~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030638&auth_key=1760030638-0-0-f6b4e496fa847e4f005f8dbb10a879dd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-97d43a2762892e8e021f104e56d3f04d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030645&auth_key=1760030645-0-0-7e1487e6efabb23317367873ce67a3bb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-75fa0e9a260478b1929ee6a63d84ad4d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030652&auth_key=1760030652-0-0-0152c780c5a5effe6e635135477e55d7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Leveraging-Prior-Knowledge-of-Diffusion-Model-for-Person-Search"><a href="#Leveraging-Prior-Knowledge-of-Diffusion-Model-for-Person-Search" class="headerlink" title="Leveraging Prior Knowledge of Diffusion Model for Person Search"></a>Leveraging Prior Knowledge of Diffusion Model for Person Search</h2><p><strong>Authors:Giyeol Kim, Sooyoung Yang, Jihyong Oh, Myungjoo Kang, Chanho Eom</strong></p>
<p>Person search aims to jointly perform person detection and re-identification by localizing and identifying a query person within a gallery of uncropped scene images. Existing methods predominantly utilize ImageNet pre-trained backbones, which may be suboptimal for capturing the complex spatial context and fine-grained identity cues necessary for person search. Moreover, they rely on a shared backbone feature for both person detection and re-identification, leading to suboptimal features due to conflicting optimization objectives. In this paper, we propose DiffPS (Diffusion Prior Knowledge for Person Search), a novel framework that leverages a pre-trained diffusion model while eliminating the optimization conflict between two sub-tasks. We analyze key properties of diffusion priors and propose three specialized modules: (i) Diffusion-Guided Region Proposal Network (DGRPN) for enhanced person localization, (ii) Multi-Scale Frequency Refinement Network (MSFRN) to mitigate shape bias, and (iii) Semantic-Adaptive Feature Aggregation Network (SFAN) to leverage text-aligned diffusion features. DiffPS sets a new state-of-the-art on CUHK-SYSU and PRW. </p>
<blockquote>
<p>人物搜索旨在在未裁剪的场景图像库中定位并识别查询人物，从而联合执行人物检测和再识别。现有方法主要使用在ImageNet上预训练过的主干网络，这可能不利于捕捉人物搜索所需复杂的空间上下文和精细的身份线索。此外，它们依赖共享主干特征进行人物检测和再识别，由于优化目标之间的冲突，导致特征次优。在本文中，我们提出了DiffPS（用于人物搜索的扩散先验知识），这是一个利用预训练的扩散模型的新框架，同时消除了两个子任务之间的优化冲突。我们分析了扩散先验的关键属性，并提出了三个专用模块：（i）扩散引导区域提议网络（DGRPN），用于增强人物定位；（ii）多尺度频率细化网络（MSFRN），以减轻形状偏见；（iii）语义自适应特征聚合网络（SFAN），以利用文本对齐的扩散特征。DiffPS在CUHK-SYSU和PRW上达到了最新技术水平。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.01841v1">PDF</a> </p>
<p><strong>Summary</strong><br>扩散模型先验知识在人物搜索中的应用。该论文提出了一种新的框架DiffPS，利用预训练的扩散模型，解决了人物检测和再识别之间的优化冲突问题。通过三个专门模块，提高了人物定位的准确性，缓解了形状偏见，并充分利用了文本对齐的扩散特征。DiffPS在CUHK-SYSU和PRW数据集上取得了最新 state-of-the-art 的成绩。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>人物搜索需要同时进行人物检测和再识别，在一个未裁剪的场景图像库中定位并识别查询人物。</li>
<li>现有方法主要使用ImageNet预训练骨干网，可能不利于捕捉复杂的空间上下文和精细的身份线索。</li>
<li>现有方法使用共享骨干网特征进行人物检测和再识别，导致特征因优化目标冲突而次优。</li>
<li>DiffPS框架利用预训练的扩散模型，解决了两个子任务之间的优化冲突。</li>
<li>DiffPS通过三个专门模块提高了人物定位的准确性，缓解了形状偏见，并充分利用了文本对齐的扩散特征。</li>
<li>DiffPS在CUHK-SYSU和PRW数据集上取得了最新的 state-of-the-art 成绩。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.01841">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-8f4a06521d0a0c4592e73b3e5ee37633~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030659&auth_key=1760030659-0-0-97d39c65db83d6fab9077ce590d4f024&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-fb26a86451f2c90aee06601811c3de47~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030666&auth_key=1760030666-0-0-21f084fa8ea6f7b40aee036ef0b6c12c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-cb3370b9694fef5bb90e662519e9eebe~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030673&auth_key=1760030673-0-0-4fe37b72320f2110e704b687cad9a9d5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4fd7c05622c6bb8b0f574ec8ddaec0c9~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030680&auth_key=1760030680-0-0-c2651972fa55e13094dd8de1ae1fa723&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="UniVerse-Unleashing-the-Scene-Prior-of-Video-Diffusion-Models-for-Robust-Radiance-Field-Reconstruction"><a href="#UniVerse-Unleashing-the-Scene-Prior-of-Video-Diffusion-Models-for-Robust-Radiance-Field-Reconstruction" class="headerlink" title="UniVerse: Unleashing the Scene Prior of Video Diffusion Models for   Robust Radiance Field Reconstruction"></a>UniVerse: Unleashing the Scene Prior of Video Diffusion Models for   Robust Radiance Field Reconstruction</h2><p><strong>Authors:Jin Cao, Hongrui Wu, Ziyong Feng, Hujun Bao, Xiaowei Zhou, Sida Peng</strong></p>
<p>This paper tackles the challenge of robust reconstruction, i.e., the task of reconstructing a 3D scene from a set of inconsistent multi-view images. Some recent works have attempted to simultaneously remove image inconsistencies and perform reconstruction by integrating image degradation modeling into neural 3D scene representations.However, these methods rely heavily on dense observations for robustly optimizing model parameters.To address this issue, we propose to decouple robust reconstruction into two subtasks: restoration and reconstruction, which naturally simplifies the optimization process.To this end, we introduce UniVerse, a unified framework for robust reconstruction based on a video diffusion model. Specifically, UniVerse first converts inconsistent images into initial videos, then uses a specially designed video diffusion model to restore them into consistent images, and finally reconstructs the 3D scenes from these restored images.Compared with case-by-case per-view degradation modeling, the diffusion model learns a general scene prior from large-scale data, making it applicable to diverse image inconsistencies.Extensive experiments on both synthetic and real-world datasets demonstrate the strong generalization capability and superior performance of our method in robust reconstruction. Moreover, UniVerse can control the style of the reconstructed 3D scene. Project page: <a target="_blank" rel="noopener" href="https://jin-cao-tma.github.io/UniVerse.github.io/">https://jin-cao-tma.github.io/UniVerse.github.io/</a> </p>
<blockquote>
<p>本文面临稳健重建的挑战，即如何从一组不一致的多视角图像重建一个三维场景。近期的一些工作尝试通过整合图像退化建模到神经三维场景表示中，同时消除图像的不一致性和进行重建。然而，这些方法很大程度上依赖于密集观测来稳健地优化模型参数。为了解决这一问题，我们提议将稳健重建解耦为两个子任务：恢复和重建，这自然地简化了优化过程。为此，我们引入了基于视频扩散模型的稳健重建的统一框架UniVerse。具体来说，UniVerse首先把不一致的图像转换成初始视频，然后使用专门设计的视频扩散模型将它们恢复成一致图像，最后从这些恢复后的图像重建三维场景。与逐个视图进行退化建模相比，扩散模型从大规模数据中学习了一般场景先验，使其适用于多种图像不一致性。在合成和真实世界数据集上的大量实验证明了我们方法在稳健重建中的强大通用性和卓越性能。此外，UniVerse可以控制重建的三维场景的风格。项目页面：<a target="_blank" rel="noopener" href="https://jin-cao-tma.github.io/UniVerse.github.io/%E3%80%82">https://jin-cao-tma.github.io/UniVerse.github.io/。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.01669v1">PDF</a> page: <a target="_blank" rel="noopener" href="https://jin-cao-tma.github.io/UniVerse.github.io/">https://jin-cao-tma.github.io/UniVerse.github.io/</a> code:   <a target="_blank" rel="noopener" href="https://github.com/zju3dv/UniVerse">https://github.com/zju3dv/UniVerse</a></p>
<p><strong>Summary</strong></p>
<p>本文提出一种基于视频扩散模型的统一框架UniVerse，用于从多视角的不一致图像中重建3D场景。该框架通过将恢复和重建任务解耦，简化了优化过程。它通过转换不一致图像为初始视频，使用专门设计的视频扩散模型进行图像恢复，并从恢复的图像中重建3D场景。相较于针对个案的每视角退化建模，扩散模型从大规模数据中学习场景的一般先验，适用于多种图像不一致性。实验表明，该方法在鲁棒重建中具有强大的泛化能力和优越性能，并可控制重建的3D场景的风格。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>该论文解决了从多视角不一致图像中鲁棒重建3D场景的挑战。</li>
<li>提出了一种基于视频扩散模型的统一框架UniVerse。</li>
<li>UniVerse通过解耦恢复和重建任务，简化了优化过程。</li>
<li>使用视频扩散模型从不一致图像中恢复图像。</li>
<li>扩散模型能从大规模数据中学习场景的一般先验，适用于多种图像不一致性。</li>
<li>实验证明UniVerse在鲁棒重建中具有强大的泛化能力和优越性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.01669">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-65d372703458d0b113068ee810f93461~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030687&auth_key=1760030687-0-0-9a7ae0c3e9b3943981383ea8f02fb316&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5501dfb87dc50804467653c7e96b4c1c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030694&auth_key=1760030694-0-0-e4e7afe77200c97bdecb9c813539c80b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ab4ce4622f5a6100842c5ee32d7cbe5e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030702&auth_key=1760030702-0-0-fafac6c04191daa2f51c768a75f96bf3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a4e918d3000208f12a9d78ed9099a266~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030709&auth_key=1760030709-0-0-aafb6f922fa3db3ddfbcacb67023d045&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="FideDiff-Efficient-Diffusion-Model-for-High-Fidelity-Image-Motion-Deblurring"><a href="#FideDiff-Efficient-Diffusion-Model-for-High-Fidelity-Image-Motion-Deblurring" class="headerlink" title="FideDiff: Efficient Diffusion Model for High-Fidelity Image Motion   Deblurring"></a>FideDiff: Efficient Diffusion Model for High-Fidelity Image Motion   Deblurring</h2><p><strong>Authors:Xiaoyang Liu, Zhengyan Zhou, Zihang Xu, Jiezhang Cao, Zheng Chen, Yulun Zhang</strong></p>
<p>Recent advancements in image motion deblurring, driven by CNNs and transformers, have made significant progress. Large-scale pre-trained diffusion models, which are rich in true-world modeling, have shown great promise for high-quality image restoration tasks such as deblurring, demonstrating stronger generative capabilities than CNN and transformer-based methods. However, challenges such as unbearable inference time and compromised fidelity still limit the full potential of the diffusion models. To address this, we introduce FideDiff, a novel single-step diffusion model designed for high-fidelity deblurring. We reformulate motion deblurring as a diffusion-like process where each timestep represents a progressively blurred image, and we train a consistency model that aligns all timesteps to the same clean image. By reconstructing training data with matched blur trajectories, the model learns temporal consistency, enabling accurate one-step deblurring. We further enhance model performance by integrating Kernel ControlNet for blur kernel estimation and introducing adaptive timestep prediction. Our model achieves superior performance on full-reference metrics, surpassing previous diffusion-based methods and matching the performance of other state-of-the-art models. FideDiff offers a new direction for applying pre-trained diffusion models to high-fidelity image restoration tasks, establishing a robust baseline for further advancing diffusion models in real-world industrial applications. Our dataset and code will be available at <a target="_blank" rel="noopener" href="https://github.com/xyLiu339/FideDiff">https://github.com/xyLiu339/FideDiff</a>. </p>
<blockquote>
<p>最近，由卷积神经网络（CNN）和变压器驱动的图像运动去模糊技术取得了显著进展。大规模预训练扩散模型在真实世界建模方面表现出丰富的特性，对于高质量图像恢复任务（如去模糊）具有巨大的潜力，显示出比CNN和基于变压器的方法更强的生成能力。然而，挑战仍然存在，例如难以忍受的推理时间以及保真度的妥协仍然限制了扩散模型的全部潜力。为了解决这一问题，我们引入了FideDiff，这是一种为高性能去模糊而设计的新型单步扩散模型。我们将运动去模糊重新表述为类似扩散的过程，其中每个时间步代表一个逐渐模糊的图像，并训练了一个一致性模型，将所有时间步对齐到同一清洁图像。通过用匹配的模糊轨迹重建训练数据，模型学习时间一致性，从而实现了一步去模糊。我们进一步通过整合Kernel ControlNet进行模糊核估计和引入自适应时间步预测，增强了模型性能。我们的模型在全参考指标上实现了卓越的性能，超越了之前的基于扩散的方法，并与其他最先进的模型性能相匹配。FideDiff为将预训练的扩散模型应用于高质量图像恢复任务提供了新的方向，为在现实世界工业应用中进一步推进扩散模型建立了稳健的基线。我们的数据集和代码将在<a target="_blank" rel="noopener" href="https://github.com/xyLiu339/FideDiff%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/xyLiu339/FideDiff上提供。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.01641v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>近期，基于CNN和transformer的图像运动去模糊技术取得了显著进展。大型预训练扩散模型在高保真图像恢复任务中展现出巨大潜力，如去模糊。然而，存在推理时间过长和保真度不足等挑战。为解决这些问题，我们推出FideDiff，一种用于高保真去模糊的单步扩散模型。该模型将运动去模糊重新构建为扩散过程，每个时间步代表一张逐渐模糊的图片，并训练一致性模型将所有时间步对齐到同一清晰图像。通过重建具有匹配模糊轨迹的训练数据，模型学习时间一致性，实现一次去模糊。通过集成Kernel ControlNet进行模糊核估计和引入自适应时间步预测，模型性能进一步提升。FideDiff在参考指标上实现卓越性能，超越先前的扩散方法，并与其他顶尖模型性能相匹配。它为将预训练扩散模型应用于高保真图像恢复任务提供了新方向，为扩散模型在现实世界工业应用中的进一步发展建立了稳健基准。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型预训练扩散模型在高保真图像恢复任务中表现出巨大潜力。</li>
<li>扩散模型在处理图像去模糊任务时存在推理时间长和保真度不足的挑战。</li>
<li>FideDiff是一种新型单步扩散模型，用于高保真去模糊，将运动去模糊重新构建为扩散过程。</li>
<li>FideDiff通过训练一致性模型，使所有时间步对齐到同一清晰图像，实现一次去模糊。</li>
<li>FideDiff集成了Kernel ControlNet进行模糊核估计，并引入自适应时间步预测，进一步提升了模型性能。</li>
<li>FideDiff在参考指标上实现了卓越性能，超越了先前的扩散方法，并与其他顶尖模型相当。</li>
<li>FideDiff为将预训练扩散模型应用于高保真图像恢复任务提供了新方向，并为扩散模型在现实世界工业应用中的进一步发展建立了基准。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.01641">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-ad443cce71103cffc6b9d92514b69019~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030718&auth_key=1760030718-0-0-596538d2779f10279038e5bba805a97b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-fce960a9f7224196089bb47a13b26145~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030726&auth_key=1760030726-0-0-014176aeff609fc44e8df42714d5fc3a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d4ee87c5f4e21d4afd8b36158154a1f8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030733&auth_key=1760030733-0-0-06399b588922751946fb1fa789eb7a0a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c52789eb17dcf2b65c5ff87be2c986cc~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030739&auth_key=1760030739-0-0-0c509b0fcd964b4f6042b2136640442b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-fa3b8d57fddb39185bbd9c09c2b83575~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030746&auth_key=1760030746-0-0-129c98dac91a47dc2a7c9511c38dd3b0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="NPN-Non-Linear-Projections-of-the-Null-Space-for-Imaging-Inverse-Problems"><a href="#NPN-Non-Linear-Projections-of-the-Null-Space-for-Imaging-Inverse-Problems" class="headerlink" title="NPN: Non-Linear Projections of the Null-Space for Imaging Inverse   Problems"></a>NPN: Non-Linear Projections of the Null-Space for Imaging Inverse   Problems</h2><p><strong>Authors:Roman Jacome, Romario Gualdrón-Hurtado, Leon Suarez, Henry Arguello</strong></p>
<p>Imaging inverse problems aims to recover high-dimensional signals from undersampled, noisy measurements, a fundamentally ill-posed task with infinite solutions in the null-space of the sensing operator. To resolve this ambiguity, prior information is typically incorporated through handcrafted regularizers or learned models that constrain the solution space. However, these priors typically ignore the task-specific structure of that null-space. In this work, we propose \textit{Non-Linear Projections of the Null-Space} (NPN), a novel class of regularization that, instead of enforcing structural constraints in the image domain, promotes solutions that lie in a low-dimensional projection of the sensing matrix’s null-space with a neural network. Our approach has two key advantages: (1) Interpretability: by focusing on the structure of the null-space, we design sensing-matrix-specific priors that capture information orthogonal to the signal components that are fundamentally blind to the sensing process. (2) Flexibility: NPN is adaptable to various inverse problems, compatible with existing reconstruction frameworks, and complementary to conventional image-domain priors. We provide theoretical guarantees on convergence and reconstruction accuracy when used within plug-and-play methods. Empirical results across diverse sensing matrices demonstrate that NPN priors consistently enhance reconstruction fidelity in various imaging inverse problems, such as compressive sensing, deblurring, super-resolution, computed tomography, and magnetic resonance imaging, with plug-and-play methods, unrolling networks, deep image prior, and diffusion models. </p>
<blockquote>
<p>成像反问题旨在从欠采样、嘈杂的测量中恢复高维信号，这是一个根本上的不适定任务，其解空间在感知算子的零空间中存在无限解。为了解决这种模糊性，通常通过手工正则化器或学习模型来融入先验信息，以约束解空间。然而，这些先验通常忽略了任务特定结构的零空间。在这项工作中，我们提出了“零空间非线性投影”（NPN），这是一类新型正则化方法，它不在图像域中强制结构约束，而是通过在神经网络中使用感知矩阵零空间的一个低维投影来优化解。我们的方法有两个主要优点：（1）可解释性：通过关注零空间的结构，我们设计了针对感知矩阵的特定先验，这些先验信息捕捉到了与信号成分正交的信息，这些信号成分对感知过程本质上是盲性的。（2）灵活性：NPN能够适应各种反问题，与现有的重建框架兼容，并且是传统图像域先验的补充。当用于插件和播放方法时，我们提供了关于收敛和重建准确性的理论保证。在多种感知矩阵上的实证结果表明，NPN先验在多种成像反问题中一致地提高了重建保真度，如压缩感知、去模糊、超分辨率、计算机断层扫描和磁共振成像等，适用于插件和播放方法、展开网络、深度图像先验和扩散模型。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.01608v1">PDF</a> 25 pages, 12 tables, 10 figures. Accepted to NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>本文提出一种名为“非线性的零空间投影”（NPN）的新型正则化方法，用于解决成像反问题。该方法专注于零空间的结构，通过神经网络在低维投影中寻找解决方案，从而提高了解的可解释性和灵活性。在多种成像反问题中，NPN先验可增强重建的保真度。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>成像反问题旨在从欠采样、嘈杂的测量中恢复高维信号，这是一个根本上的不适定任务。</li>
<li>传统的先验信息通常通过手工正则化器或学习模型来约束解空间，但往往忽略了任务特定的结构。</li>
<li>NPN方法专注于零空间的结构，设计感应矩阵特定的先验，捕获与信号成分正交的信息。</li>
<li>NPN方法通过神经网络在低维投影中寻找解决方案，提高了解决方案的可解释性和灵活性。</li>
<li>NPN先验在各种成像反问题中都能提高重建的保真度，如压缩感知、去模糊、超分辨率、计算机断层扫描和磁共振成像等。</li>
<li>NPN方法可与现有的重建框架互补，适用于多种逆问题。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.01608">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-04c7ec5cf032affab78d04563a7967a4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030753&auth_key=1760030753-0-0-deea11098bde3aab42a36da855dee85e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7b759a443f7a05400992db6fe113bc84~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030760&auth_key=1760030760-0-0-a696b2c42aa9e42304b30ebcfa71bc8b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Towards-Better-Optimization-For-Listwise-Preference-in-Diffusion-Models"><a href="#Towards-Better-Optimization-For-Listwise-Preference-in-Diffusion-Models" class="headerlink" title="Towards Better Optimization For Listwise Preference in Diffusion Models"></a>Towards Better Optimization For Listwise Preference in Diffusion Models</h2><p><strong>Authors:Jiamu Bai, Xin Yu, Meilong Xu, Weitao Lu, Xin Pan, Kiwan Maeng, Daniel Kifer, Jian Wang, Yu Wang</strong></p>
<p>Reinforcement learning from human feedback (RLHF) has proven effectiveness for aligning text-to-image (T2I) diffusion models with human preferences. Although Direct Preference Optimization (DPO) is widely adopted for its computational efficiency and avoidance of explicit reward modeling, its applications to diffusion models have primarily relied on pairwise preferences. The precise optimization of listwise preferences remains largely unaddressed. In practice, human feedback on image preferences often contains implicit ranked information, which conveys more precise human preferences than pairwise comparisons. In this work, we propose Diffusion-LPO, a simple and effective framework for Listwise Preference Optimization in diffusion models with listwise data. Given a caption, we aggregate user feedback into a ranked list of images and derive a listwise extension of the DPO objective under the Plackett-Luce model. Diffusion-LPO enforces consistency across the entire ranking by encouraging each sample to be preferred over all of its lower-ranked alternatives. We empirically demonstrate the effectiveness of Diffusion-LPO across various tasks, including text-to-image generation, image editing, and personalized preference alignment. Diffusion-LPO consistently outperforms pairwise DPO baselines on visual quality and preference alignment. </p>
<blockquote>
<p>强化学习从人类反馈（RLHF）已经证明在使文本到图像（T2I）扩散模型与人类偏好对齐方面的有效性。尽管Direct Preference Optimization（DPO）因其计算效率高且避免了显式奖励建模而得到广泛应用，但其在扩散模型中的应用主要依赖于成对偏好。对列表级偏好的精确优化仍然未得到很好的解决。在实践中，人类对图像偏好的反馈通常包含隐性的排名信息，这传达了比成对比较更精确的人类偏好。在这项工作中，我们提出了Diffusion-LPO，这是一个在带有列表数据的扩散模型中用于列表级偏好优化的简单有效的框架。给定一个标题，我们将用户反馈聚合为图像排名列表，并在Plackett-Luce模型下推导出DPO目标的列表级扩展。Diffusion-LPO通过鼓励每个样本在所有排名较低的替代品中保持优先，从而在整个排名中强制执行一致性。我们在各种任务上实证了Diffusion-LPO的有效性，包括文本到图像生成、图像编辑和个人偏好对齐。Diffusion-LPO在视觉质量和偏好对齐方面始终优于成对的DPO基准测试。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.01540v1">PDF</a> </p>
<p><strong>Summary</strong><br>     基于人类反馈的强化学习已证明在调整文本到图像（T2I）扩散模型以符合人类偏好方面非常有效。尽管直接偏好优化（DPO）因其计算效率和高避免显式奖励建模的适用性而受到广泛采用，但其对扩散模型的应用主要依赖于成对偏好。列表偏好的精确优化仍未得到广泛研究。在实践中，关于图像偏好的人类反馈通常包含隐含的排名信息，这比成对比较更能准确反映人类偏好。本工作提出了一个简单有效的Diffusion-LPO框架，用于处理扩散模型中的列表偏好优化问题。给定描述，我们整合用户反馈生成图像排名列表，并在Plackett-Luce模型下推导DPO目标的列表扩展。Diffusion-LPO通过鼓励每个样本在所有排名较低的替代品中占据优势来确保整个排名的连贯性。我们实证地证明了Diffusion-LPO在各种任务上的有效性，包括文本到图像生成、图像编辑和个性化偏好对齐。Diffusion-LPO在视觉质量和偏好对齐方面均优于成对DPO基准测试。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>强化学习从人类反馈（RLHF）在提高文本到图像（T2I）扩散模型与人类偏好对齐方面展现出有效性。</li>
<li>直接偏好优化（DPO）已广泛应用于扩散模型，但主要依赖于成对偏好，忽视了列表偏好的精确优化。</li>
<li>人类反馈通常包含隐含的排名信息，能更精确地反映人类偏好。</li>
<li>本研究提出了Diffusion-LPO框架，该框架利用列表数据在扩散模型中进行列表偏好优化。</li>
<li>Diffusion-LPO通过鼓励每个样本在所有较低排名的替代品中占据优势，确保排名的连贯性。</li>
<li>Diffusion-LPO在多种任务上表现优异，包括文本到图像生成、图像编辑和个性化偏好对齐。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.01540">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-363c368ed37547908d525c67063f0ce4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030767&auth_key=1760030767-0-0-98a19ed619b06713bb170a866349c842&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-302a128e849f36b26045c531b66ab8ee~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030774&auth_key=1760030774-0-0-8d4ee02e9f02c4f8c5358257c68ebe88&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="AortaDiff-A-Unified-Multitask-Diffusion-Framework-For-Contrast-Free-AAA-Imaging"><a href="#AortaDiff-A-Unified-Multitask-Diffusion-Framework-For-Contrast-Free-AAA-Imaging" class="headerlink" title="AortaDiff: A Unified Multitask Diffusion Framework For Contrast-Free AAA   Imaging"></a>AortaDiff: A Unified Multitask Diffusion Framework For Contrast-Free AAA   Imaging</h2><p><strong>Authors:Yuxuan Ou, Ning Bi, Jiazhen Pan, Jiancheng Yang, Boliang Yu, Usama Zidan, Regent Lee, Vicente Grau</strong></p>
<p>While contrast-enhanced CT (CECT) is standard for assessing abdominal aortic aneurysms (AAA), the required iodinated contrast agents pose significant risks, including nephrotoxicity, patient allergies, and environmental harm. To reduce contrast agent use, recent deep learning methods have focused on generating synthetic CECT from non-contrast CT (NCCT) scans. However, most adopt a multi-stage pipeline that first generates images and then performs segmentation, which leads to error accumulation and fails to leverage shared semantic and anatomical structures. To address this, we propose a unified deep learning framework that generates synthetic CECT images from NCCT scans while simultaneously segmenting the aortic lumen and thrombus. Our approach integrates conditional diffusion models (CDM) with multi-task learning, enabling end-to-end joint optimization of image synthesis and anatomical segmentation. Unlike previous multitask diffusion models, our approach requires no initial predictions (e.g., a coarse segmentation mask), shares both encoder and decoder parameters across tasks, and employs a semi-supervised training strategy to learn from scans with missing segmentation labels, a common constraint in real-world clinical data. We evaluated our method on a cohort of 264 patients, where it consistently outperformed state-of-the-art single-task and multi-stage models. For image synthesis, our model achieved a PSNR of 25.61 dB, compared to 23.80 dB from a single-task CDM. For anatomical segmentation, it improved the lumen Dice score to 0.89 from 0.87 and the challenging thrombus Dice score to 0.53 from 0.48 (nnU-Net). These segmentation enhancements led to more accurate clinical measurements, reducing the lumen diameter MAE to 4.19 mm from 5.78 mm and the thrombus area error to 33.85% from 41.45% when compared to nnU-Net. Code is available at <a target="_blank" rel="noopener" href="https://github.com/yuxuanou623/AortaDiff.git">https://github.com/yuxuanou623/AortaDiff.git</a>. </p>
<blockquote>
<p>在评估腹主动脉瘤（AAA）时，虽然增强型计算机断层扫描（CECT）是标准方法，但所需的碘造影剂存在重大风险，包括肾毒性、患者过敏反应和环境危害。为了减少造影剂的使用，最近的深度学习方法专注于从非对比计算机断层扫描（NCCT）生成合成CECT。然而，大多数方法采用多阶段管道，首先生成图像，然后进行分割，这会导致误差累积，并且未能利用共享语义和解剖结构。针对这一问题，我们提出了一种统一的深度学习框架，该框架可从NCCT扫描生成合成CECT图像，同时分割主动脉腔和血栓。我们的方法将条件扩散模型（CDM）与多任务学习相结合，实现对图像合成和解剖结构分割的端到端联合优化。与之前的多任务扩散模型不同，我们的方法无需初始预测（例如粗略分割掩膜），共享编码器和解码器参数跨任务，并采用半监督训练策略从缺失分割标签的扫描中学习，这是现实世界临床数据中常见的约束。我们在264名患者队列中评估了我们的方法，其性能始终优于最新单任务和多阶段模型。在图像合成方面，我们的模型达到了25.61分贝的峰值信噪比（PSNR），高于单任务CDM的23.80分贝。在解剖结构分割方面，它将腔道Dice系数从0.87提高到0.89，并将具有挑战性的血栓Dice系数从0.48提高到0.53（nnU-Net）。这些分割改进带来了更准确的临床测量值，将腔道直径平均绝对误差（MAE）从nnU-Net的5.78毫米减少到4.19毫米，并将血栓面积误差从41.45%减少到33.85%。代码可在<a target="_blank" rel="noopener" href="https://github.com/yuxuanou623/AortaDiff.git%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/yuxuanou623/AortaDiff.git找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.01498v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>本研究针对腹部主动脉瘤的评估，提出一种基于深度学习的统一框架，能够从非对比CT扫描中生成合成对比增强CT图像，并同时分割主动脉腔和血栓。该方法结合了条件扩散模型和多任务学习，实现了图像合成和解剖分割的端到端联合优化。相较于先前的多任务扩散模型，本研究的方法无需初始预测，共享编码器和解码器参数，并采用半监督训练策略应对缺乏分割标签的扫描数据。在264名患者队列中的评估显示，该方法在图像合成和解剖分割方面均表现出卓越性能，超过单项任务和多阶段模型。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>碘化对比剂在评估腹部主动脉瘤中虽为标准，但存在肾毒性、患者过敏和环境危害等风险。</li>
<li>研究采用深度学习方法从非对比CT扫描生成合成对比增强CT图像。</li>
<li>提出一种统一深度学习框架，同时分割主动脉腔和血栓，减少误差累积。</li>
<li>结合条件扩散模型和多任务学习，实现图像合成和解剖分割的联合优化。</li>
<li>与先前的多任务扩散模型相比，该方法无需初始预测，参数共享，并采用半监督训练策略。</li>
<li>方法在264名患者队列中表现优异，超过单项任务和多阶段模型。</li>
<li>精确的解剖分割有助于更准确的临床测量。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.01498">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-519359d89438ab321d1c2b52424a17e3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030781&auth_key=1760030781-0-0-7f9784457ee5b0790720e8ffd80d43b0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-32d0ef3d1aeb4d304651a4260201dfe9~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030789&auth_key=1760030789-0-0-c9aef2314e9016a6a80cc769beaf0e54&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-cf2c4f35daafc0bb59e8abcb4cfefba3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030795&auth_key=1760030795-0-0-ea58d7b015d4af87520a52e71ff0c33a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-297ccfe8560eecebfd5971823ce7ace3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030802&auth_key=1760030802-0-0-04a4fe2ea498a94fb6385c2a9a44a8f4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="VENTURA-Adapting-Image-Diffusion-Models-for-Unified-Task-Conditioned-Navigation"><a href="#VENTURA-Adapting-Image-Diffusion-Models-for-Unified-Task-Conditioned-Navigation" class="headerlink" title="VENTURA: Adapting Image Diffusion Models for Unified Task Conditioned   Navigation"></a>VENTURA: Adapting Image Diffusion Models for Unified Task Conditioned   Navigation</h2><p><strong>Authors:Arthur Zhang, Xiangyun Meng, Luca Calliari, Dong-Ki Kim, Shayegan Omidshafiei, Joydeep Biswas, Ali Agha, Amirreza Shaban</strong></p>
<p>Robots must adapt to diverse human instructions and operate safely in unstructured, open-world environments. Recent Vision-Language models (VLMs) offer strong priors for grounding language and perception, but remain difficult to steer for navigation due to differences in action spaces and pretraining objectives that hamper transferability to robotics tasks. Towards addressing this, we introduce VENTURA, a vision-language navigation system that finetunes internet-pretrained image diffusion models for path planning. Instead of directly predicting low-level actions, VENTURA generates a path mask (i.e. a visual plan) in image space that captures fine-grained, context-aware navigation behaviors. A lightweight behavior-cloning policy grounds these visual plans into executable trajectories, yielding an interface that follows natural language instructions to generate diverse robot behaviors. To scale training, we supervise on path masks derived from self-supervised tracking models paired with VLM-augmented captions, avoiding manual pixel-level annotation or highly engineered data collection setups. In extensive real-world evaluations, VENTURA outperforms state-of-the-art foundation model baselines on object reaching, obstacle avoidance, and terrain preference tasks, improving success rates by 33% and reducing collisions by 54% across both seen and unseen scenarios. Notably, we find that VENTURA generalizes to unseen combinations of distinct tasks, revealing emergent compositional capabilities. Videos, code, and additional materials: <a target="_blank" rel="noopener" href="https://venturapath.github.io/">https://venturapath.github.io</a> </p>
<blockquote>
<p>机器人必须适应多样化的人类指令，并在非结构化、开放世界环境中安全操作。最近的视觉语言模型（VLM）为语言接地和感知提供了强有力的先验知识，但由于动作空间上的差异和预训练目标与机器人任务的可转移性受阻，使得其在导航方面仍然难以控制。为了解决这个问题，我们引入了VENTURA，一个视觉语言导航系统，它通过微调互联网预训练的图像扩散模型来进行路径规划。VENTURA不同于直接预测低层次动作，而是生成图像空间中的路径掩码（即视觉计划），捕捉精细的、上下文感知的导航行为。一个轻量级的模仿行为策略将这些视觉计划转化为可执行的轨迹，从而产生一个遵循自然语言指令的接口，以生成多样化的机器人行为。为了扩大训练规模，我们在自我监督跟踪模型生成的路径掩码上进行监督，这些模型与VLM增强的字幕配对，避免了手动像素级的标注或高度工程化的数据采集设置。在广泛的实际世界评估中，VENTURA在物体抓取、避障和地形偏好任务上的表现优于最先进的基准模型，在已知和未知场景中的成功率提高了33%，碰撞减少了54%。值得注意的是，我们发现VENTURA能够泛化到未见过的不同任务的组合，显示出新兴的组合能力。视频、代码和其他材料请参见：[<a target="_blank" rel="noopener" href="https://venturapath.github.io/]">https://venturapath.github.io/]</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.01388v1">PDF</a> 9 pages, 6 figures, 3 tables</p>
<p><strong>Summary</strong></p>
<p>本文介绍了VENTURA系统，该系统结合了视觉和语言模型，用于机器人导航。它通过微调互联网预训练的图像扩散模型来规划路径，生成路径掩码（即视觉计划），该掩码能捕捉精细、上下文感知的导航行为。该系统使用轻量级的行为克隆策略将视觉计划转化为可执行轨迹，并通过自然语言指令生成多样化的机器人行为。通过自我监督跟踪模型和语言模型的结合，实现了无需手动像素级标注和高度定制的数据采集设置的大规模训练。在真实世界环境中，VENTURA在物体抓取、避障和地形选择任务上超越了最先进的基线模型，在已知和未知场景中的成功率提高了33%，碰撞减少了54%。此外，它还能够推广到未见过的组合任务，展现出组合的潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VENTURA系统结合了视觉和语言模型，用于机器人导航。</li>
<li>通过微调互联网预训练的图像扩散模型进行路径规划。</li>
<li>生成路径掩码（视觉计划），捕捉精细、上下文感知的导航行为。</li>
<li>借助轻量级行为克隆策略将视觉计划转化为可执行轨迹。</li>
<li>通过自然语言指令生成多样化的机器人行为。</li>
<li>使用自我监督跟踪模型和语言模型的结合进行训练，无需手动像素级标注和高度定制的数据采集设置。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.01388">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-0b73489da6f26d0dcc21d7053e981787~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030809&auth_key=1760030809-0-0-fe4f3358b4150b507ba70ec3e8ef9e68&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f1d90c1eeca335ee887af48960952e48~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030817&auth_key=1760030817-0-0-b5ffe785df449c2be9a5a659d165f1bd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-11869348900d0df20cdf85720c701c44~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030824&auth_key=1760030824-0-0-7760be6d999dc7f7adb23556273349ff&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d00455d6cbcf965ac0087a8c2e943bc1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030831&auth_key=1760030831-0-0-4025dd71189a9c05953bf346e56d8b87&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4e16ecdc4feb8851f323ea4e65e12a81~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030839&auth_key=1760030839-0-0-932cc01abccb9a8b3754ea533726a9d6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="LVTINO-LAtent-Video-consisTency-INverse-sOlver-for-High-Definition-Video-Restoration"><a href="#LVTINO-LAtent-Video-consisTency-INverse-sOlver-for-High-Definition-Video-Restoration" class="headerlink" title="LVTINO: LAtent Video consisTency INverse sOlver for High Definition   Video Restoration"></a>LVTINO: LAtent Video consisTency INverse sOlver for High Definition   Video Restoration</h2><p><strong>Authors:Alessio Spagnoletti, Andrés Almansa, Marcelo Pereyra</strong></p>
<p>Computational imaging methods increasingly rely on powerful generative diffusion models to tackle challenging image restoration tasks. In particular, state-of-the-art zero-shot image inverse solvers leverage distilled text-to-image latent diffusion models (LDMs) to achieve unprecedented accuracy and perceptual quality with high computational efficiency. However, extending these advances to high-definition video restoration remains a significant challenge, due to the need to recover fine spatial detail while capturing subtle temporal dependencies. Consequently, methods that naively apply image-based LDM priors on a frame-by-frame basis often result in temporally inconsistent reconstructions. We address this challenge by leveraging recent advances in Video Consistency Models (VCMs), which distill video latent diffusion models into fast generators that explicitly capture temporal causality. Building on this foundation, we propose LVTINO, the first zero-shot or plug-and-play inverse solver for high definition video restoration with priors encoded by VCMs. Our conditioning mechanism bypasses the need for automatic differentiation and achieves state-of-the-art video reconstruction quality with only a few neural function evaluations, while ensuring strong measurement consistency and smooth temporal transitions across frames. Extensive experiments on a diverse set of video inverse problems show significant perceptual improvements over current state-of-the-art methods that apply image LDMs frame by frame, establishing a new benchmark in both reconstruction fidelity and computational efficiency. </p>
<blockquote>
<p>计算成像方法越来越依赖于强大的生成扩散模型，以解决具有挑战性的图像恢复任务。特别是，最先进的零样本图像逆求解器利用提炼的文本到图像潜在扩散模型（LDM）以极高的计算效率实现了前所未有的准确性和感知质量。然而，将这些进展扩展到高清视频恢复仍然是一个重大挑战，因为需要在恢复精细空间细节的同时捕捉微妙的时间依赖性。因此，那些直接按帧应用基于图像的LDM先验的方法通常会导致时间上不一致的重建。我们通过利用视频一致性模型（VCM）的最新进展来解决这一挑战，它将视频潜在扩散模型提炼成能够快速生成且能明确捕捉时间因果关系的模型。在此基础上，我们提出了LVTINO，这是第一个用于高清视频恢复的零样本或即插即用逆求解器，其先验由VCM编码。我们的调节机制跳过了自动分化的需求，仅通过几次神经网络功能评估就实现了最先进的视频重建质量，同时确保了强大的测量一致性和跨帧之间的平滑时间过渡。在多种视频逆问题上的广泛实验表明，与当前将图像LDM逐帧应用的先进方法相比，感知效果显著提升，在重建保真度和计算效率方面都树立了新的基准。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.01339v1">PDF</a> 23 pages, 12 figures</p>
<p><strong>Summary</strong></p>
<p>该文介绍了计算成像方法如何利用先进的生成扩散模型来处理图像恢复任务。最新一代的零样本图像逆求解器通过蒸馏文本到图像潜在扩散模型（LDMs）实现了前所未有的准确性和感知质量，同时保持了高计算效率。然而，将这些进展扩展到高清视频恢复仍然是一个重大挑战，因为需要在恢复精细空间细节的同时捕捉微妙的时间依赖性。为此，本文利用视频一致性模型（VCMs）的最新进展，将视频潜在扩散模型蒸馏成快速生成器，显式捕捉时间因果关系。在此基础上，提出了LVTINO，这是一种零样本或即插即用的高清视频恢复逆求解器，以VCMs编码的先验知识为基础。其调节机制无需自动微分，即可实现视频重建质量的最新水平，仅通过几次神经网络功能评估就能达到效果，同时确保强大的测量一致性和跨帧的平滑时间过渡。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>计算成像方法依赖生成扩散模型处理图像恢复任务。</li>
<li>先进的零样本图像逆求解器利用蒸馏文本到图像潜在扩散模型（LDMs）实现高准确性和感知质量。</li>
<li>将这些技术扩展到高清视频恢复存在挑战，需要恢复精细空间细节并捕捉时间依赖性。</li>
<li>视频一致性模型（VCMs）用于蒸馏视频潜在扩散模型，以显式捕捉时间因果关系。</li>
<li>LVTINO是首个利用VCMs先验知识的零样本或即插即用的高清视频恢复逆求解器。</li>
<li>LVTINO调节机制无需自动微分，能实现视频重建的先进质量，并确保测量一致性和平滑时间过渡。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.01339">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-b7394084e848b1883eae5f1b43008346~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030846&auth_key=1760030846-0-0-5519240679356798c9f257a5efbed5cd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d32d94dea63f1f19a89659dbe5a73530~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030853&auth_key=1760030853-0-0-e0144d67501ab1fe3ac722b5190ffb66&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Temporal-Score-Rescaling-for-Temperature-Sampling-in-Diffusion-and-Flow-Models"><a href="#Temporal-Score-Rescaling-for-Temperature-Sampling-in-Diffusion-and-Flow-Models" class="headerlink" title="Temporal Score Rescaling for Temperature Sampling in Diffusion and Flow   Models"></a>Temporal Score Rescaling for Temperature Sampling in Diffusion and Flow   Models</h2><p><strong>Authors:Yanbo Xu, Yu Wu, Sungjae Park, Zhizhuo Zhou, Shubham Tulsiani</strong></p>
<p>We present a mechanism to steer the sampling diversity of denoising diffusion and flow matching models, allowing users to sample from a sharper or broader distribution than the training distribution. We build on the observation that these models leverage (learned) score functions of noisy data distributions for sampling and show that rescaling these allows one to effectively control a &#96;local’ sampling temperature. Notably, this approach does not require any finetuning or alterations to training strategy, and can be applied to any off-the-shelf model and is compatible with both deterministic and stochastic samplers. We first validate our framework on toy 2D data, and then demonstrate its application for diffusion models trained across five disparate tasks – image generation, pose estimation, depth prediction, robot manipulation, and protein design. We find that across these tasks, our approach allows sampling from sharper (or flatter) distributions, yielding performance gains e.g., depth prediction models benefit from sampling more likely depth estimates, whereas image generation models perform better when sampling a slightly flatter distribution. Project page: <a target="_blank" rel="noopener" href="https://temporalscorerescaling.github.io/">https://temporalscorerescaling.github.io</a> </p>
<blockquote>
<p>我们提出了一种控制降噪扩散和流量匹配模型的采样多样性的机制，允许用户从比训练分布更尖锐或更广泛的分布中进行采样。我们的观察基础是，这些模型利用噪声数据的（学习）分数函数进行采样，我们表明重新缩放这些分数函数可以有效地控制“局部”采样温度。值得注意的是，这种方法不需要对训练策略进行微调或更改，可以应用于任何现成的模型，并且与确定性采样器和随机采样器兼容。我们首先在玩具2D数据上验证我们的框架，然后演示其在五个不同任务上训练的扩散模型的应用，包括图像生成、姿态估计、深度预测、机器人操作和蛋白质设计。我们发现，在这些任务中，我们的方法允许从更尖锐（或更平坦）的分布中进行采样，从而带来性能提升，例如深度预测模型受益于更可能的深度估计的采样，而图像生成模型在稍微平坦的分布中采样时表现更好。项目页面：<a target="_blank" rel="noopener" href="https://temporalscorerescaling.github.io/">https://temporalscorerescaling.github.io</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.01184v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了控制去噪扩散和流匹配模型的采样多样性的机制，使用户可以从比训练分布更尖锐或更广泛的分布中进行采样。文章基于这些模型利用噪声数据的评分函数进行采样的观察，展示通过调整评分函数的尺度可以有效地控制局部采样温度。该方法无需对训练策略进行微调或更改，可应用于任何现成的模型，并且与确定性采样器和随机采样器兼容。该框架在玩具2D数据上进行了验证，并展示了在五个不同任务（图像生成、姿态估计、深度预测、机器人操作和蛋白质设计）的扩散模型中的应用。实验表明，该方法可以在不同任务中实现从更尖锐或更平坦的分布进行采样，从而提高性能，例如在深度预测模型中，从更可能的深度估计中进行采样受益较大，而在图像生成模型中，从稍微平坦的分布中进行采样效果更好。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>介绍了一种控制去噪扩散和流匹配模型的采样多样性的新机制。</li>
<li>通过调整噪声数据的评分函数的尺度，可以控制局部采样温度。</li>
<li>该方法无需微调或更改训练策略，具有广泛的应用性，可应用于任何现成的模型。</li>
<li>该方法与确定性采样器和随机采样器兼容。</li>
<li>框架在玩具2D数据上进行了验证，并成功应用于图像生成、姿态估计、深度预测、机器人操作和蛋白质设计等五个任务的扩散模型。</li>
<li>实验显示，该方法能使模型从更尖锐或更平坦的分布中采样，从而提高性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.01184">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-6964f50787e4df4aa3910924d5a5081e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030860&auth_key=1760030860-0-0-2e789a3bfcd4063447b42f61a6a20f25&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4c687bb548c721d6188ee6b6bcfb11d1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030869&auth_key=1760030869-0-0-0beabf9d79296fbae2d65e2827bea829&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Semantic-and-Visual-Crop-Guided-Diffusion-Models-for-Heterogeneous-Tissue-Synthesis-in-Histopathology"><a href="#Semantic-and-Visual-Crop-Guided-Diffusion-Models-for-Heterogeneous-Tissue-Synthesis-in-Histopathology" class="headerlink" title="Semantic and Visual Crop-Guided Diffusion Models for Heterogeneous   Tissue Synthesis in Histopathology"></a>Semantic and Visual Crop-Guided Diffusion Models for Heterogeneous   Tissue Synthesis in Histopathology</h2><p><strong>Authors:Saghir Alfasly, Wataru Uegami, MD Enamul Hoq, Ghazal Alabtah, H. R. Tizhoosh</strong></p>
<p>Synthetic data generation in histopathology faces unique challenges: preserving tissue heterogeneity, capturing subtle morphological features, and scaling to unannotated datasets. We present a latent diffusion model that generates realistic heterogeneous histopathology images through a novel dual-conditioning approach combining semantic segmentation maps with tissue-specific visual crops. Unlike existing methods that rely on text prompts or abstract visual embeddings, our approach preserves critical morphological details by directly incorporating raw tissue crops from corresponding semantic regions. For annotated datasets (i.e., Camelyon16, Panda), we extract patches ensuring 20-80% tissue heterogeneity. For unannotated data (i.e., TCGA), we introduce a self-supervised extension that clusters whole-slide images into 100 tissue types using foundation model embeddings, automatically generating pseudo-semantic maps for training. Our method synthesizes high-fidelity images with precise region-wise annotations, achieving superior performance on downstream segmentation tasks. When evaluated on annotated datasets, models trained on our synthetic data show competitive performance to those trained on real data, demonstrating the utility of controlled heterogeneous tissue generation. In quantitative evaluation, prompt-guided synthesis reduces Frechet Distance by up to 6X on Camelyon16 (from 430.1 to 72.0) and yields 2-3x lower FD across Panda and TCGA. Downstream DeepLabv3+ models trained solely on synthetic data attain test IoU of 0.71 and 0.95 on Camelyon16 and Panda, within 1-2% of real-data baselines (0.72 and 0.96). By scaling to 11,765 TCGA whole-slide images without manual annotations, our framework offers a practical solution for an urgent need for generating diverse, annotated histopathology data, addressing a critical bottleneck in computational pathology. </p>
<blockquote>
<p>病理组织学的合成数据生成面临一些独特的挑战：保留组织异质性、捕捉微妙的形态学特征以及扩展到未标注的数据集。我们提出了一种潜在扩散模型，通过一种新颖的双重条件方法结合语义分割图和组织特异性视觉裁剪来生成真实的异质性病理组织学图像。不同于依赖文本提示或抽象视觉嵌入的现有方法，我们的方法通过直接融入来自相应语义区域的原始组织裁剪来保留关键的形态学细节。对于已标注的数据集（例如Camelyon16、Panda），我们提取斑块，确保组织异质性占20-80%。对于未标注的数据（例如TCGA），我们引入了一种自监督扩展，使用基础模型嵌入将整个幻灯片图像聚类成100种组织类型，自动生成伪语义图进行训练。我们的方法合成高保真图像，具有精确的区域注释，并在下游分割任务上实现卓越性能。在已标注数据集上进行评估，经过我们的合成数据训练的模型表现出与真实数据训练的模型相当的性能，证明了可控的异质性组织生成的实用性。在定量评估中，提示引导的合成将Camelyon16上的Frechet距离减少了高达6倍（从430.1降至72.0），并且在Panda和TCGA上的FD降低了2-3倍。仅经过合成数据训练的下游DeepLabv3+模型在Camelyon16和Panda上的测试交集mIoU达到0.71和0.95，与真实数据基准线相差1-2%（分别为0.72和0.96）。通过扩展到11765张没有手动标注的TCGA全幻灯片图像，我们的框架为解决生成多样、标注的病理组织学数据的紧迫需求提供了切实可行的解决方案，解决了计算病理学中的一个关键瓶颈。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.17847v2">PDF</a> NeurIPS 2025</p>
<p><strong>摘要</strong></p>
<p>本文介绍了针对病理图像合成数据生成所面临的挑战，如保持组织异质性、捕捉微妙的形态特征和扩展到未标注数据集的问题。提出了一种基于潜在扩散模型的病理图像合成方法，通过结合语义分割图和特定组织视觉裁剪的双重条件策略生成逼真的异质性病理图像。与依赖文本提示或抽象视觉嵌入的现有方法不同，该方法通过直接结合来自相应语义区域的原始组织裁剪片段，保留了关键的形态细节。对于已标注的数据集（如Camelyon16和Panda），通过提取确保20-80％组织异质性的补丁。对于未标注的数据（如TCGA），引入了一种自监督扩展，使用基础模型嵌入将全幻灯片图像聚类为100种组织类型，自动生成伪语义图进行训练。该方法合成的图像具有高保真度和精确的区域标注，在下游分割任务上表现优越。在已标注数据集上的评估表明，经合成数据训练的模型与真实数据训练的模型表现相当，证明了可控异质性组织生成的实用性。定量评估结果显示，提示引导的合成数据在Camelyon16上的Frechet距离降低了6倍（从430.1降至72.0），并且在Panda和TCGA上的FD降低了2-3倍。仅使用合成数据训练的DeepLabv3+模型在Camelyon16和Panda上的测试IoU达到0.71和0.95，与真实数据基准测试（分别为0.72和0.96）相差仅1-2％。通过扩展到未标注的TCGA全幻灯片图像（共11，765张），该框架为解决生成多样化和标注的病理数据的迫切需求提供了实用解决方案，解决了计算病理学中的关键瓶颈。</p>
<p><strong>要点解析</strong></p>
<p>以下是文本的七个主要洞察点：</p>
<ol>
<li>合成数据在病理学中的挑战包括保持组织异质性、精细的形态特征捕捉和扩展到大规模未标注数据集的能力。</li>
<li>介绍了一种基于潜在扩散模型的合成方法，该方法能够通过双重条件策略生成逼真的异质性病理图像。</li>
<li>与现有方法不同，该方法结合语义分割图和特定组织视觉裁剪来保留形态细节。</li>
<li>对于已标注数据集，通过提取确保一定组织异质性的图像块进行训练。</li>
<li>对于未标注数据集，引入了自监督扩展方法，使用基础模型嵌入自动为训练生成伪语义图。</li>
<li>合成的高保真图像具有精确的区域标注，并在下游分割任务上表现优越。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.17847">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-976e022aa4042493ae225b4999548fe4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030878&auth_key=1760030878-0-0-37385e4158b8f29245e7794c830bf10a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-06c5798d1f9122617b9ee39e0d86dcc3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030885&auth_key=1760030885-0-0-a6b8ddf59525a9ccc1a49988ffd069a8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e67d0364e8693b036501da13253d51ab~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030893&auth_key=1760030893-0-0-a46e7ee49323f21748c12f5ff29b0c15&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-77237d8a6c556c16c9852f7e00af8af5~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030901&auth_key=1760030901-0-0-874e81202d6df2cb65b29b54ece1a66a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Concept-Unlearning-by-Modeling-Key-Steps-of-Diffusion-Process"><a href="#Concept-Unlearning-by-Modeling-Key-Steps-of-Diffusion-Process" class="headerlink" title="Concept Unlearning by Modeling Key Steps of Diffusion Process"></a>Concept Unlearning by Modeling Key Steps of Diffusion Process</h2><p><strong>Authors:Chaoshuo Zhang, Chenhao Lin, Zhengyu Zhao, Le Yang, Qian Wang, Chao Shen</strong></p>
<p>Text-to-image diffusion models (T2I DMs), represented by Stable Diffusion, which generate highly realistic images based on textual input, have been widely used, but their flexibility also makes them prone to misuse for producing harmful or unsafe content. Concept unlearning has been used to prevent text-to-image diffusion models from being misused to generate undesirable visual content. However, existing methods struggle to trade off unlearning effectiveness with the preservation of generation quality. To address this limitation, we propose Key Step Concept Unlearning (KSCU), which selectively fine-tunes the model at key steps to the target concept. KSCU is inspired by the fact that different diffusion denoising steps contribute unequally to the final generation. Compared to previous approaches, which treat all denoising steps uniformly, KSCU avoids over-optimization of unnecessary steps for higher effectiveness and reduces the number of parameter updates for higher efficiency. For example, on the I2P dataset, KSCU outperforms ESD by 8.3% in nudity unlearning accuracy while improving FID by 8.4%, and achieves a high overall score of 0.92, substantially surpassing all other SOTA methods. </p>
<blockquote>
<p>文本转图像扩散模型（T2I DM），以Stable Diffusion为代表，能够根据文本输入生成高度逼真的图像，已经得到了广泛应用。然而，它们的灵活性也使得它们容易被误用于产生有害或不安全的内容。概念遗忘技术已被用于防止文本转图像扩散模型被误用生成不希望的视觉内容。然而，现有方法在平衡遗忘效果与保持生成质量方面存在困难。为了解决这一局限，我们提出了关键步骤概念遗忘（KSCU），它选择性地微调模型的关键步骤以针对目标概念。KSCU的灵感来源于不同扩散去噪步骤对最终生成的贡献不等这一事实。与以往对所有去噪步骤进行统一处理的方法不同，KSCU避免了不必要步骤的过度优化，以提高效果和效率。例如，在I2P数据集上，KSCU在裸露度遗忘准确性方面比ESD高出8.3%，同时改进了FID指标8.4%，并获得了0.92的高综合得分，显著超越了所有其他最先进的方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.06526v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>文本到图像扩散模型（T2I DM），如Stable Diffusion，能根据文本输入生成高度逼真的图像，但其灵活性也使其易于产生有害或不安全的内容。为预防模型滥用，研究者提出了概念去学习的策略。然而，现有方法难以在去除学习效果与保持生成质量之间取得平衡。为解决此问题，我们提出了关键步骤概念去学习（KSCU），该方法有选择性地在对目标概念的关键步骤上对模型进行微调。KSCU的灵感来源于不同扩散去噪步骤对最终生成的贡献不平等。与以往对所有去噪步骤一视同仁的方法相比，KSCU避免了不必要步骤的过度优化，提高了效率和效果。在I2P数据集上，KSCU在裸体去除准确性方面较ESD高出8.3%，同时提高了FID评分8.4%，总体得分高达0.92，远超其他最先进的方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>T2I DMs如Stable Diffusion虽能生成高度逼真的图像，但其灵活性导致易于产生有害或不安全的内容。</li>
<li>概念去学习方法被用来防止T2I DMs的滥用。</li>
<li>现有方法在平衡去学习效果和保持生成质量方面存在困难。</li>
<li>提出了一种新的方法KSCU，该方法有选择性地在对目标概念的关键步骤上对模型进行微调。</li>
<li>KSCU基于不同扩散去噪步骤对最终生成的贡献不平等这一事实。</li>
<li>KSCU避免了不必要去噪步骤的过度优化，提高了去学习的效率和效果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.06526">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-4966e0db9eaf54c5f72d1d76df2d116b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030908&auth_key=1760030908-0-0-3873eecf0840a15fa686e47dad3daf03&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ae75385ed52056fe7bb5c9970bd88944~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030916&auth_key=1760030916-0-0-f4705635d51ce1d06c44e28ec53c7b94&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-77674de7fe84930be39afbd48716f5da~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030923&auth_key=1760030923-0-0-c3b4f45b928ec9fafe51bd663b9c6de9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-cf251c5bace7b5f8d11e4addb74b7766~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030930&auth_key=1760030930-0-0-2453641beaf0c199518de39d8e7f08ba&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-76f9cc6d2b93f9416e3083e54a95fa19~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030937&auth_key=1760030937-0-0-952c1fe5b73b8866306f5b94f04963a1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="One-Step-Residual-Shifting-Diffusion-for-Image-Super-Resolution-via-Distillation"><a href="#One-Step-Residual-Shifting-Diffusion-for-Image-Super-Resolution-via-Distillation" class="headerlink" title="One-Step Residual Shifting Diffusion for Image Super-Resolution via   Distillation"></a>One-Step Residual Shifting Diffusion for Image Super-Resolution via   Distillation</h2><p><strong>Authors:Daniil Selikhanovych, David Li, Aleksei Leonov, Nikita Gushchin, Sergei Kushneriuk, Alexander Filippov, Evgeny Burnaev, Iaroslav Koshelev, Alexander Korotin</strong></p>
<p>Diffusion models for super-resolution (SR) produce high-quality visual results but require expensive computational costs. Despite the development of several methods to accelerate diffusion-based SR models, some (e.g., SinSR) fail to produce realistic perceptual details, while others (e.g., OSEDiff) may hallucinate non-existent structures. To overcome these issues, we present RSD, a new distillation method for ResShift, one of the top diffusion-based SR models. Our method is based on training the student network to produce such images that a new fake ResShift model trained on them will coincide with the teacher model. RSD achieves single-step restoration and outperforms the teacher by a large margin. We show that our distillation method can surpass the other distillation-based method for ResShift - SinSR - making it on par with state-of-the-art diffusion-based SR distillation methods. Compared to SR methods based on pre-trained text-to-image models, RSD produces competitive perceptual quality, provides images with better alignment to degraded input images, and requires fewer parameters and GPU memory. We provide experimental results on various real-world and synthetic datasets, including RealSR, RealSet65, DRealSR, ImageNet, and DIV2K. </p>
<blockquote>
<p>扩散模型在超分辨率（SR）应用中生成了高质量视觉效果，但需要昂贵的计算成本。尽管已经开发了几种方法来加速基于扩散的SR模型，但某些方法（例如SinSR）无法生成逼真的感知细节，而其他方法（例如OSEDiff）可能会虚构不存在的结构。为了克服这些问题，我们提出了RSD，这是一种用于ResShift的新型蒸馏方法，ResShift是顶级的基于扩散的SR模型之一。我们的方法基于训练学生网络来生成图像，这些图像能够使在新假ResShift模型上训练的模型与教师的模型相吻合。RSD实现了单步恢复，并在教师模型的基础上大幅超越。我们展示了我们的蒸馏方法可以超越ResShift的其他蒸馏方法SinSR，使其与最先进的基于扩散的SR蒸馏方法相媲美。与基于预训练文本到图像模型的SR方法相比，RSD产生的感知质量具有竞争力，为退化输入图像提供了更好的对齐图像，并且需要更少的参数和GPU内存。我们在各种真实和合成数据集上提供了实验结果，包括RealSR、RealSet65、DRealSR、ImageNet和DIV2K。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13358v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>扩散模型在超分辨率（SR）应用中能生成高质量图像，但计算成本较高。尽管有多种方法加速扩散模型在SR领域的应用，但一些模型（如SinSR）无法生成逼真的感知细节，而其他模型（如OSEDiff）可能会虚构不存在的结构。为解决这些问题，我们提出了RSD，这是一种针对ResShift的新蒸馏方法。该方法基于训练学生网络生成图像，使新的假ResShift模型在它们上训练的图像与原始模型一致。RSD实现了单步恢复，并在很大程度上超越了原始模型。实验结果表明，我们的蒸馏方法能超越ResShift的另一种蒸馏方法SinSR，使其与最先进的扩散模型SR蒸馏方法相当。相较于基于预训练文本到图像的SR方法，RSD在感知质量上具有竞争力，生成的图像与退化输入图像对齐度更高，同时需要的参数和GPU内存更少。我们在RealSR、RealSet65、DRealSR、ImageNet和DIV2K等多个真实和合成数据集上进行了实验验证。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型在超分辨率应用中能生成高质量图像，但计算成本较高。</li>
<li>当前存在的加速扩散模型在SR领域的方法存在生成不逼真感知细节和虚构不存在结构的问题。</li>
<li>RSD是一种针对ResShift的新蒸馏方法，通过训练学生网络生成图像，使假ResShift模型的训练结果与原始模型一致。</li>
<li>RSD实现了单步恢复，并大幅超越了原始模型性能。</li>
<li>RSD在蒸馏方法上超越了SinSR，使其性能与最先进的扩散模型SR蒸馏方法相当。</li>
<li>相较于其他SR方法，RSD在感知质量上具有竞争力，且生成的图像与退化输入图像对齐度更高。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13358">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-ef443f27cd4175c8fcb1442ad3d5305c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030944&auth_key=1760030944-0-0-0764d1c7542126c7634f3dc344a763a6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c519519873a0a15e1bcb407b8b8ab666~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030951&auth_key=1760030951-0-0-f7cd2b58eaef747641e8c7f111c04049&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-df83c1e39ffcdcf8c80eb3c4c71fda7d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030958&auth_key=1760030958-0-0-d5c088a16034b4ebed62e46035fc58ca&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-04/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-04/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-04/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-4f44294221847e7a520cb94c4dcb3ecd~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030965&auth_key=1760030965-0-0-26495b717ed77098283e7fdd207d5c84&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="医学图像">
                        
                        <span class="card-title">医学图像</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学图像 方向最新论文已更新，请持续关注 Update in 2025-10-04  Noisy Timing Behavior is a Feature of Central Compact Object Pulsars
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-04
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    医学图像
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">医学图像</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-04/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-f6a90ec4d3b1bf751fe3910e7a57ee39~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030292&auth_key=1760030292-0-0-701e7c9c7bb1e093f806fe89b0c2a362&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF 方向最新论文已更新，请持续关注 Update in 2025-10-04  StealthAttack Robust 3D Gaussian Splatting Poisoning via Density-Guided   Illusions
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-04
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">30341.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
