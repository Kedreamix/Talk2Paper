<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-04  Optimal Control Meets Flow Matching A Principled Route to Multi-Subject   Fidelity">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-4fd7c05622c6bb8b0f574ec8ddaec0c9')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-04
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-15
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    22.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    89 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-10-04-æ›´æ–°"><a href="#2025-10-04-æ›´æ–°" class="headerlink" title="2025-10-04 æ›´æ–°"></a>2025-10-04 æ›´æ–°</h1><h2 id="Optimal-Control-Meets-Flow-Matching-A-Principled-Route-to-Multi-Subject-Fidelity"><a href="#Optimal-Control-Meets-Flow-Matching-A-Principled-Route-to-Multi-Subject-Fidelity" class="headerlink" title="Optimal Control Meets Flow Matching: A Principled Route to Multi-Subject   Fidelity"></a>Optimal Control Meets Flow Matching: A Principled Route to Multi-Subject   Fidelity</h2><p><strong>Authors:Eric Tillmann Bill, Enis Simsar, Thomas Hofmann</strong></p>
<p>Text-to-image (T2I) models excel on single-entity prompts but struggle with multi-subject descriptions, often showing attribute leakage, identity entanglement, and subject omissions. We introduce the first theoretical framework with a principled, optimizable objective for steering sampling dynamics toward multi-subject fidelity. Viewing flow matching (FM) through stochastic optimal control (SOC), we formulate subject disentanglement as control over a trained FM sampler. This yields two architecture-agnostic algorithms: (i) a training-free test-time controller that perturbs the base velocity with a single-pass update, and (ii) Adjoint Matching, a lightweight fine-tuning rule that regresses a control network to a backward adjoint signal while preserving base-model capabilities. The same formulation unifies prior attention heuristics, extends to diffusion models via a flow-diffusion correspondence, and provides the first fine-tuning route explicitly designed for multi-subject fidelity. Empirically, on Stable Diffusion 3.5, FLUX, and Stable Diffusion XL, both algorithms consistently improve multi-subject alignment while maintaining base-model style. Test-time control runs efficiently on commodity GPUs, and fine-tuned controllers trained on limited prompts generalize to unseen ones. We further highlight FOCUS (Flow Optimal Control for Unentangled Subjects), which achieves state-of-the-art multi-subject fidelity across models. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ¨¡å‹åœ¨å•å®ä½“æç¤ºæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¤šä¸»é¢˜æè¿°æ–¹é¢å´è¡¨ç°æŒ£æ‰ï¼Œå¸¸å¸¸å‡ºç°å±æ€§æ³„éœ²ã€èº«ä»½çº ç¼ å’Œä¸»é¢˜é—æ¼çš„é—®é¢˜ã€‚æˆ‘ä»¬å¼•å…¥äº†ç¬¬ä¸€ä¸ªç†è®ºæ¡†æ¶ï¼Œè¯¥æ¡†æ¶å…·æœ‰åŸåˆ™æ€§ä¸”å¯ä¼˜åŒ–çš„ç›®æ ‡ï¼Œæ—¨åœ¨å¼•å¯¼é‡‡æ ·åŠ¨æ€å®ç°å¤šä¸»é¢˜ä¿çœŸã€‚é€šè¿‡éšæœºæœ€ä¼˜æ§åˆ¶ï¼ˆSOCï¼‰æ¥è§‚å¯ŸæµåŒ¹é…ï¼ˆFMï¼‰ï¼Œæˆ‘ä»¬å°†ä¸»é¢˜è§£è€¦å…¬å¼åŒ–ä¸ºå¯¹è®­ç»ƒå¥½çš„FMé‡‡æ ·å™¨çš„æ§åˆ¶ã€‚è¿™äº§ç”Ÿäº†ä¸¤ç§ä¸æ¶æ„æ— å…³çš„ç®—æ³•ï¼šï¼ˆiï¼‰ä¸€ç§æ— è®­ç»ƒæµ‹è¯•æ—¶é—´æ§åˆ¶å™¨ï¼Œå®ƒå¯ä»¥åœ¨å•æ¬¡æ›´æ–°ä¸­æ‰°åŠ¨åŸºæœ¬é€Ÿåº¦ï¼›ï¼ˆiiï¼‰ä¼´éšåŒ¹é…æ˜¯ä¸€ç§è½»é‡çº§çš„å¾®è°ƒè§„åˆ™ï¼Œå®ƒå›å½’ä¸€ä¸ªæ§åˆ¶ç½‘ç»œä»¥äº§ç”Ÿå‘åä¼´éšä¿¡å·ï¼ŒåŒæ—¶ä¿ç•™åŸºç¡€æ¨¡å‹çš„èƒ½åŠ›ã€‚åŒæ ·çš„å…¬å¼ç»Ÿä¸€äº†å…ˆå‰çš„æ³¨æ„åŠ›å¯å‘å¼æ–¹æ³•ï¼Œé€šè¿‡æµæ‰©æ•£å¯¹åº”å…³ç³»æ‰©å±•åˆ°æ‰©æ•£æ¨¡å‹ï¼Œå¹¶ä¸ºå¤šä¸»é¢˜ä¿çœŸæä¾›äº†ç¬¬ä¸€ä¸ªæ˜ç¡®è®¾è®¡çš„å¾®è°ƒè·¯å¾„ã€‚åœ¨Stable Diffusion 3.5ã€FLUXå’ŒStable Diffusion XLä¸Šçš„ç»éªŒè¡¨æ˜ï¼Œä¸¤ç§ç®—æ³•éƒ½èƒ½ä¸€è‡´åœ°æé«˜å¤šä¸»é¢˜å¯¹é½æ€§ï¼ŒåŒæ—¶ä¿æŒåŸºç¡€æ¨¡å‹é£æ ¼ã€‚æµ‹è¯•æ—¶é—´æ§åˆ¶å¯åœ¨å•†å“GPUä¸Šé«˜æ•ˆè¿è¡Œï¼Œè€Œåœ¨æœ‰é™æç¤ºä¸Šè®­ç»ƒçš„å¾®è°ƒæ§åˆ¶å™¨å¯ä»¥æ¨å¹¿åˆ°æœªè§è¿‡çš„æç¤ºã€‚æˆ‘ä»¬è¿›ä¸€æ­¥å¼ºè°ƒäº†FOCUSï¼ˆç”¨äºæ— çº ç¼ ä¸»é¢˜çš„æœ€ä¼˜æµæ§åˆ¶ï¼‰ï¼Œå®ƒåœ¨å„ç§æ¨¡å‹ä¸­å®ç°äº†æœ€å…ˆè¿›çš„ä¸»é¢˜ä¿çœŸåº¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.02315v1">PDF</a> Code: <a target="_blank" rel="noopener" href="https://github.com/ericbill21/FOCUS/">https://github.com/ericbill21/FOCUS/</a></p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ–‡æœ¬è‡³å›¾åƒï¼ˆT2Iï¼‰æ¨¡å‹åœ¨å•å®ä½“æç¤ºä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨å¤šä¸»é¢˜æè¿°æ–¹é¢å­˜åœ¨å±æ€§æ³„éœ²ã€èº«ä»½æ··æ·†åŠä¸»é¢˜é—æ¼ç­‰é—®é¢˜ã€‚æœ¬æ–‡é¦–æ¬¡æå‡ºäº†ä¸€ä¸ªç†è®ºæ¡†æ¶ï¼Œé€šè¿‡ä¼˜åŒ–ç›®æ ‡æ¥å¼•å¯¼é‡‡æ ·åŠ¨æ€ä»¥å®ç°å¤šä¸»é¢˜ä¿çœŸã€‚é€šè¿‡éšæœºæœ€ä¼˜æ§åˆ¶ï¼ˆSOCï¼‰æ¥è§‚å¯ŸæµåŒ¹é…ï¼ˆFMï¼‰ï¼Œæˆ‘ä»¬å°†ä¸»é¢˜åˆ†ç¦»è¡¨ç°ä¸ºå¯¹è®­ç»ƒè¿‡çš„FMé‡‡æ ·å™¨çš„æ§åˆ¶ï¼Œä»è€Œå¾—åˆ°ä¸¤ç§ä¸æ¶æ„æ— å…³çš„ç®—æ³•ï¼šä¸€æ˜¯æ— éœ€è®­ç»ƒçš„æ£€æµ‹æ—¶é—´æ§åˆ¶å™¨ï¼Œå®ƒå¯ä»¥é€šè¿‡å•æ¬¡æ›´æ–°æ¥æ‰°åŠ¨åŸºæœ¬é€Ÿåº¦ï¼›äºŒæ˜¯å›å½’æ§åˆ¶ç½‘ç»œçš„åå‘ä¼´éšä¿¡å·çš„åŒæ—¶ä¿ç•™åŸºç¡€æ¨¡å‹èƒ½åŠ›çš„Adjoint Matchingè½»é‡çº§å¾®è°ƒè§„åˆ™ã€‚åŒä¸€è¡¨è¿°ç»Ÿä¸€äº†å…ˆå‰çš„æ³¨æ„åŠ›å¯å‘å¼æ–¹æ³•ï¼Œé€šè¿‡æµåŠ¨æ‰©æ•£å¯¹åº”å…³ç³»æ‰©å±•åˆ°äº†æ‰©æ•£æ¨¡å‹ï¼Œå¹¶ä¸ºå¤šä¸»é¢˜ä¿çœŸæä¾›äº†ç¬¬ä¸€æ¡ç²¾å¿ƒè®¾è®¡è¿‡çš„å¾®è°ƒè·¯çº¿ã€‚åœ¨Stable Diffusion 3.5ã€FLUXå’ŒStable Diffusion XLä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä¸¤ç§ç®—æ³•å‡èƒ½åœ¨ä¿æŒåŸºç¡€æ¨¡å‹é£æ ¼çš„åŒæ—¶ï¼Œä¸æ–­æé«˜å¤šä¸»é¢˜å¯¹é½æ€§ã€‚æ£€æµ‹æ—¶é—´æ§åˆ¶å¯åœ¨å•†å“GPUä¸Šé«˜æ•ˆè¿è¡Œï¼Œç»è¿‡æœ‰é™æç¤ºè®­ç»ƒçš„æ§åˆ¶å™¨å¯æ¨å¹¿è‡³æœªè§è¿‡çš„æç¤ºã€‚æœ¬æ–‡è¿›ä¸€æ­¥å¼ºè°ƒäº†FOCUSï¼ˆç”¨äºæ— çº ç¼ ä¸»é¢˜çš„æµåŠ¨æœ€ä¼˜æ§åˆ¶ï¼‰ï¼Œå®ƒåœ¨å„ç§æ¨¡å‹ä¸­å®ç°äº†æœ€ä½³çš„å¤šä¸»é¢˜ä¿çœŸåº¦ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>T2Iæ¨¡å‹åœ¨å•å®ä½“æç¤ºä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å¤šä¸»é¢˜æè¿°æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œå¦‚å±æ€§æ³„éœ²ã€èº«ä»½æ··æ·†å’Œä¸»é¢˜é—æ¼ã€‚</li>
<li>å¼•å…¥äº†ä¸€ä¸ªç†è®ºæ¡†æ¶ï¼Œé€šè¿‡ä¼˜åŒ–ç›®æ ‡æ¥å¼•å¯¼é‡‡æ ·åŠ¨æ€ï¼Œä»¥å®ç°å¤šä¸»é¢˜ä¿çœŸã€‚</li>
<li>æå‡ºäº†ä¸¤ç§ä¸æ¶æ„æ— å…³çš„ç®—æ³•ï¼šæµ‹è¯•æ—¶é—´æ§åˆ¶å™¨å’ŒAdjoint Matchingï¼Œä»¥æé«˜å¤šä¸»é¢˜å¯¹é½æ€§å¹¶ä¿ç•™åŸºç¡€æ¨¡å‹é£æ ¼ã€‚</li>
<li>é¦–æ¬¡ä¸ºæ‰©æ•£æ¨¡å‹æä¾›äº†ä¸“é—¨è®¾è®¡çš„å¾®è°ƒè·¯çº¿ã€‚</li>
<li>å®è¯ç»“æœæ˜¾ç¤ºï¼Œæ‰€ææ–¹æ³•åœ¨å¤šç§æ¨¡å‹ä¸Šå®ç°äº†å“è¶Šçš„å¤šä¸»é¢˜ä¿çœŸåº¦ã€‚</li>
<li>æµ‹è¯•æ—¶é—´æ§åˆ¶å¯åœ¨å•†å“GPUä¸Šé«˜æ•ˆè¿è¡Œï¼Œä¸”ç»è¿‡æœ‰é™æç¤ºè®­ç»ƒçš„æ§åˆ¶å™¨å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.02315">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-18ed38116073d4b48c82a9ce8cc39d28" align="middle">
<img src="https://picx.zhimg.com/v2-2dbbc0a898872c9d99a7386f9aa142c2" align="middle">
<img src="https://picx.zhimg.com/v2-6326edb5b51fb5e1acb79b5ec79dd9c4" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Continual-Personalization-for-Diffusion-Models"><a href="#Continual-Personalization-for-Diffusion-Models" class="headerlink" title="Continual Personalization for Diffusion Models"></a>Continual Personalization for Diffusion Models</h2><p><strong>Authors:Yu-Chien Liao, Jr-Jen Chen, Chi-Pin Huang, Ci-Siang Lin, Meng-Lin Wu, Yu-Chiang Frank Wang</strong></p>
<p>Updating diffusion models in an incremental setting would be practical in real-world applications yet computationally challenging. We present a novel learning strategy of Concept Neuron Selection (CNS), a simple yet effective approach to perform personalization in a continual learning scheme. CNS uniquely identifies neurons in diffusion models that are closely related to the target concepts. In order to mitigate catastrophic forgetting problems while preserving zero-shot text-to-image generation ability, CNS finetunes concept neurons in an incremental manner and jointly preserves knowledge learned of previous concepts. Evaluation of real-world datasets demonstrates that CNS achieves state-of-the-art performance with minimal parameter adjustments, outperforming previous methods in both single and multi-concept personalization works. CNS also achieves fusion-free operation, reducing memory storage and processing time for continual personalization. </p>
<blockquote>
<p>åœ¨ç°å®ä¸–ç•Œçš„åº”ç”¨ä¸­ï¼Œé€æ­¥æ›´æ–°æ‰©æ•£æ¨¡å‹æ˜¯å®ç”¨çš„ï¼Œä½†åœ¨è®¡ç®—ä¸Šå­˜åœ¨æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„å­¦ä¹ ç­–ç•¥â€”â€”æ¦‚å¿µç¥ç»å…ƒé€‰æ‹©ï¼ˆCNSï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æŒç»­å­¦ä¹ æ–¹æ¡ˆè¿›è¡Œä¸ªæ€§åŒ–æ–¹æ³•ã€‚CNSèƒ½å¤Ÿç‹¬ç‰¹åœ°è¯†åˆ«ä¸æ‰©æ•£æ¨¡å‹ä¸­çš„ç›®æ ‡æ¦‚å¿µå¯†åˆ‡ç›¸å…³çš„ç¥ç»å…ƒã€‚ä¸ºäº†ç¼“è§£ç¾éš¾æ€§é—å¿˜é—®é¢˜å¹¶ä¿ç•™é›¶æ ·æœ¬æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆèƒ½åŠ›ï¼ŒCNSä»¥å¢é‡æ–¹å¼å¾®è°ƒæ¦‚å¿µç¥ç»å…ƒå¹¶è”åˆä¿ç•™å…ˆå‰æ¦‚å¿µçš„çŸ¥è¯†ã€‚å¯¹çœŸå®æ•°æ®é›†çš„è¯„ä»·è¡¨æ˜ï¼ŒCNSä»¥æœ€å°çš„å‚æ•°è°ƒæ•´å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨å•æ¦‚å¿µå’Œå¤šæ¦‚å¿µä¸ªæ€§åŒ–å·¥ä½œä¸­éƒ½ä¼˜äºä»¥å‰çš„æ–¹æ³•ã€‚CNSè¿˜å®ç°äº†æ— èåˆæ“ä½œï¼Œå‡å°‘äº†æŒç»­ä¸ªæ€§åŒ–æ‰€éœ€çš„å†…å­˜å­˜å‚¨å’Œå¤„ç†æ—¶é—´ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.02296v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§æ–°é¢–çš„å­¦ä¹ ç­–ç•¥â€”â€”æ¦‚å¿µç¥ç»å…ƒé€‰æ‹©ï¼ˆCNSï¼‰ï¼Œç”¨äºå¢é‡æ›´æ–°æ‰©æ•£æ¨¡å‹ã€‚CNSèƒ½å‡†ç¡®è¯†åˆ«ä¸ç›®æ ‡æ¦‚å¿µç´§å¯†ç›¸å…³çš„ç¥ç»å…ƒï¼Œå¹¶åœ¨å¢é‡å­¦ä¹ æ–¹æ¡ˆä¸­å®ç°å¯¹æ¨¡å‹çš„ä¸ªæ€§åŒ–è°ƒæ•´ã€‚ä¸ºäº†ç¼“è§£ç¾éš¾æ€§é—å¿˜é—®é¢˜å¹¶ä¿æŒé›¶æ ·æœ¬æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆèƒ½åŠ›ï¼ŒCNSä»¥å¢é‡æ–¹å¼å¾®è°ƒæ¦‚å¿µç¥ç»å…ƒå¹¶è”åˆä¿ç•™å…ˆå‰æ¦‚å¿µçš„çŸ¥è¯†ã€‚åœ¨çœŸå®æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒCNSé€šè¿‡æœ€å°çš„å‚æ•°è°ƒæ•´å®ç°äº†å“è¶Šæ€§èƒ½ï¼Œåœ¨å•æ¦‚å¿µå’Œå¤šæ¦‚å¿µä¸ªæ€§åŒ–å·¥ä½œä¸­å‡ä¼˜äºå…ˆå‰æ–¹æ³•ã€‚æ­¤å¤–ï¼ŒCNSå®ç°äº†æ— èåˆæ“ä½œï¼Œé™ä½äº†è¿ç»­ä¸ªæ€§åŒ–ä»»åŠ¡çš„å†…å­˜å­˜å‚¨å’Œå¤„ç†æ—¶é—´ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„å¢é‡æ›´æ–°å…·æœ‰å®è·µæ„ä¹‰ä¸”è®¡ç®—å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>æ¦‚å¿µç¥ç»å…ƒé€‰æ‹©ï¼ˆCNSï¼‰æ˜¯ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æ–¹æ³•ï¼Œå¯åœ¨æŒç»­å­¦ä¹ æ–¹æ¡ˆä¸­è¿›è¡Œä¸ªæ€§åŒ–è°ƒæ•´ã€‚</li>
<li>CNSèƒ½è¯†åˆ«ä¸ç›®æ ‡æ¦‚å¿µç›¸å…³çš„ç¥ç»å…ƒã€‚</li>
<li>CNSé€šè¿‡å¾®è°ƒæ¦‚å¿µç¥ç»å…ƒç¼“è§£ç¾éš¾æ€§é—å¿˜é—®é¢˜ï¼Œå¹¶ä¿ç•™å…ˆå‰çŸ¥è¯†ã€‚</li>
<li>åœ¨çœŸå®æ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºCNSæ€§èƒ½å“è¶Šï¼Œä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</li>
<li>CNSå®ç°æ— èåˆæ“ä½œï¼Œé™ä½å†…å­˜å­˜å‚¨å’Œå¤„ç†æ—¶é—´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.02296">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-15e3b19584fba84acd1a9d19a4ab04a0" align="middle">
<img src="https://picx.zhimg.com/v2-26aea0e703637b4bd9a4ccaa75b4aa2f" align="middle">
<img src="https://picx.zhimg.com/v2-d737bd013e827212e65ae0baa3bd5e16" align="middle">
<img src="https://picx.zhimg.com/v2-8f52d64f662e8f70a48a22f031fc2334" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Test-Time-Anchoring-for-Discrete-Diffusion-Posterior-Sampling"><a href="#Test-Time-Anchoring-for-Discrete-Diffusion-Posterior-Sampling" class="headerlink" title="Test-Time Anchoring for Discrete Diffusion Posterior Sampling"></a>Test-Time Anchoring for Discrete Diffusion Posterior Sampling</h2><p><strong>Authors:Litu Rout, Andreas Lugmayr, Yasamin Jafarian, Srivatsan Varadharajan, Constantine Caramanis, Sanjay Shakkottai, Ira Kemelmacher-Shlizerman</strong></p>
<p>We study the problem of posterior sampling using pretrained discrete diffusion foundation models, aiming to recover images from noisy measurements without retraining task-specific models. While diffusion models have achieved remarkable success in generative modeling, most advances rely on continuous Gaussian diffusion. In contrast, discrete diffusion offers a unified framework for jointly modeling categorical data such as text and images. Beyond unification, discrete diffusion provides faster inference, finer control, and principled training-free Bayesian inference, making it particularly well-suited for posterior sampling. However, existing approaches to discrete diffusion posterior sampling face severe challenges: derivative-free guidance yields sparse signals, continuous relaxations limit applicability, and split Gibbs samplers suffer from the curse of dimensionality. To overcome these limitations, we introduce Anchored Posterior Sampling (APS) for masked diffusion foundation models, built on two key innovations â€“ quantized expectation for gradient-like guidance in discrete embedding space, and anchored remasking for adaptive decoding. Our approach achieves state-of-the-art performance among discrete diffusion samplers across linear and nonlinear inverse problems on the standard benchmarks. We further demonstrate the benefits of our approach in training-free stylization and text-guided editing. </p>
<blockquote>
<p>æˆ‘ä»¬ç ”ç©¶äº†ä½¿ç”¨åå‘é‡‡æ ·çš„é—®é¢˜ï¼Œåˆ©ç”¨äº†é¢„è®­ç»ƒçš„ç¦»æ•£æ‰©æ•£åŸºç¡€æ¨¡å‹ï¼Œæ—¨åœ¨ä»å™ªå£°æµ‹é‡ä¸­æ¢å¤å›¾åƒï¼Œè€Œæ— éœ€é‡æ–°è®­ç»ƒç‰¹å®šä»»åŠ¡æ¨¡å‹ã€‚è™½ç„¶æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆå»ºæ¨¡æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œä½†å¤§å¤šæ•°è¿›å±•éƒ½ä¾èµ–äºè¿ç»­çš„é«˜æ–¯æ‰©æ•£ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œç¦»æ•£æ‰©æ•£æä¾›äº†ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œå¯ä»¥è”åˆå¯¹æ–‡æœ¬å’Œå›¾åƒç­‰åˆ†ç±»æ•°æ®è¿›è¡Œå»ºæ¨¡ã€‚é™¤äº†ç»Ÿä¸€å»ºæ¨¡ï¼Œç¦»æ•£æ‰©æ•£è¿˜æä¾›äº†æ›´å¿«çš„æ¨ç†é€Ÿåº¦ã€æ›´ç²¾ç»†çš„æ§åˆ¶å’ŒåŸºäºåŸç†çš„æ— è®­ç»ƒè´å¶æ–¯æ¨ç†ï¼Œä½¿å…¶æˆä¸ºåå‘é‡‡æ ·çš„ç†æƒ³é€‰æ‹©ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ç¦»æ•£æ‰©æ•£åå‘é‡‡æ ·æ–¹æ³•é¢ä¸´ä¸¥é‡çš„æŒ‘æˆ˜ï¼šæ— å¯¼æ•°æŒ‡å¯¼äº§ç”Ÿç¨€ç–ä¿¡å·ã€è¿ç»­æ¾å¼›é™åˆ¶äº†é€‚ç”¨æ€§ï¼Œè€Œåˆ†è£‚å‰å¸ƒæ–¯é‡‡æ ·å™¨å—åˆ°ç»´æ•°è¯…å’’çš„å½±å“ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬ä¸ºæ©è†œæ‰©æ•£åŸºç¡€æ¨¡å‹å¼•å…¥äº†é”šç‚¹åå‘é‡‡æ ·ï¼ˆAPSï¼‰ï¼Œè¿™æ˜¯åŸºäºä¸¤é¡¹å…³é”®åˆ›æ–°â€”â€”ç”¨äºç¦»æ•£åµŒå…¥ç©ºé—´ä¸­æ¢¯åº¦å¼•å¯¼çš„é‡åŒ–çš„æœŸæœ›å’Œç”¨äºè‡ªé€‚åº”è§£ç çš„é”šç‚¹é‡æ–°æ©è”½ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨æ‰€æœ‰æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­çš„çº¿æ€§å’Œéçº¿æ€§åé—®é¢˜ä¸Šå‡å®ç°äº†ç¦»æ•£æ‰©æ•£é‡‡æ ·å™¨çš„æœ€ä½³æ€§èƒ½ã€‚æˆ‘ä»¬è¿˜è¿›ä¸€æ­¥å±•ç¤ºäº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨æ— éœ€è®­ç»ƒçš„é£æ ¼åŒ–å’Œæ–‡æœ¬å¼•å¯¼ç¼–è¾‘ä¸­çš„ä¼˜åŠ¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.02291v1">PDF</a> Preprint</p>
<p><strong>æ‘˜è¦</strong></p>
<p>ç ”ç©¶ä½¿ç”¨é¢„è®­ç»ƒçš„ç¦»æ•£æ‰©æ•£åŸºç¡€æ¨¡å‹è¿›è¡Œåé‡‡æ ·çš„é—®é¢˜ï¼Œæ—¨åœ¨ä»å™ªå£°æµ‹é‡ä¸­æ¢å¤å›¾åƒï¼Œè€Œæ— éœ€é‡æ–°è®­ç»ƒç‰¹å®šä»»åŠ¡æ¨¡å‹ã€‚è™½ç„¶æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆå»ºæ¨¡æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œä½†å¤§å¤šæ•°è¿›å±•éƒ½ä¾èµ–äºè¿ç»­é«˜æ–¯æ‰©æ•£ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œç¦»æ•£æ‰©æ•£æä¾›äº†ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œå¯ä»¥è”åˆå¯¹æ–‡æœ¬å’Œå›¾åƒç­‰åˆ†ç±»æ•°æ®è¿›è¡Œå»ºæ¨¡ã€‚ç¦»æ•£æ‰©æ•£é™¤äº†ç»Ÿä¸€å»ºæ¨¡å¤–ï¼Œè¿˜æä¾›äº†æ›´å¿«çš„æ¨ç†é€Ÿåº¦ã€æ›´ç²¾ç»†çš„æ§åˆ¶å’ŒåŸºäºåŸåˆ™çš„æ— è®­ç»ƒè´å¶æ–¯æ¨ç†ï¼Œä½¿å…¶ç‰¹åˆ«é€‚ç”¨äºåé‡‡æ ·ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ç¦»æ•£æ‰©æ•£åé‡‡æ ·æ–¹æ³•é¢ä¸´ä¸¥é‡æŒ‘æˆ˜ï¼šæ— å¯¼æ•°æŒ‡å¯¼äº§ç”Ÿç¨€ç–ä¿¡å·ã€è¿ç»­æ”¾æ¾é™åˆ¶åº”ç”¨ï¼Œåˆ†è£‚å‰å¸ƒæ–¯é‡‡æ ·å™¨å—åˆ°ç»´åº¦è¯…å’’çš„å½±å“ã€‚ä¸ºäº†å…‹æœè¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬å¼•å…¥äº†é”šå®šåé‡‡æ ·ï¼ˆAPSï¼‰ç”¨äºæ©æ¨¡æ‰©æ•£åŸºç¡€æ¨¡å‹ï¼ŒåŸºäºä¸¤é¡¹å…³é”®åˆ›æ–°â€”â€”é‡åŒ–æœŸæœ›ç”¨äºç¦»æ•£åµŒå…¥ç©ºé—´ä¸­çš„æ¢¯åº¦å¼•å¯¼ï¼Œä»¥åŠé”šå®šé®ç½©ç”¨äºè‡ªé€‚åº”è§£ç ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸Šçš„ç¦»æ•£æ‰©æ•£é‡‡æ ·å™¨ä¸­çš„çº¿æ€§å’Œéçº¿æ€§åé—®é¢˜æ–¹é¢å®ç°äº†æœ€ä½³æ€§èƒ½ã€‚æˆ‘ä»¬è¿˜è¿›ä¸€æ­¥å±•ç¤ºäº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨æ— éœ€è®­ç»ƒå’Œæ–‡æœ¬å¼•å¯¼ç¼–è¾‘æ–¹é¢çš„ä¼˜åŠ¿ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç ”ç©¶ä½¿ç”¨é¢„è®­ç»ƒç¦»æ•£æ‰©æ•£åŸºç¡€æ¨¡å‹è¿›è¡Œåé‡‡æ ·ï¼Œæ—¨åœ¨ä»å™ªå£°ä¸­æ¢å¤å›¾åƒï¼Œæ— éœ€é’ˆå¯¹ç‰¹å®šä»»åŠ¡é‡æ–°è®­ç»ƒæ¨¡å‹ã€‚</li>
<li>ç¦»æ•£æ‰©æ•£æä¾›äº†ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œå¯ä»¥è”åˆå¯¹åˆ†ç±»æ•°æ®è¿›è¡Œå»ºæ¨¡ï¼Œå¦‚æ–‡æœ¬å’Œå›¾åƒã€‚</li>
<li>ç°æœ‰ç¦»æ•£æ‰©æ•£åé‡‡æ ·æ–¹æ³•é¢ä¸´æŒ‘æˆ˜ï¼Œå¦‚æ— å¯¼æ•°æŒ‡å¯¼äº§ç”Ÿçš„ç¨€ç–ä¿¡å·ã€è¿ç»­æ”¾æ¾çš„é™åˆ¶å’Œåˆ†è£‚å‰å¸ƒæ–¯é‡‡æ ·å™¨çš„ç»´åº¦è¯…å’’ã€‚</li>
<li>å¼•å…¥çš„é”šå®šåé‡‡æ ·ï¼ˆAPSï¼‰æ–¹æ³•åŸºäºä¸¤é¡¹å…³é”®åˆ›æ–°ï¼šé‡åŒ–æœŸæœ›ç”¨äºæ¢¯åº¦å¼•å¯¼ï¼Œä»¥åŠé”šå®šé®ç½©ç”¨äºè‡ªé€‚åº”è§£ç ã€‚</li>
<li>APSæ–¹æ³•åœ¨çº¿æ€§å’Œéçº¿æ€§åé—®é¢˜ä¸Šå®ç°äº†æœ€ä½³æ€§èƒ½ï¼Œåœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­ä¼˜äºå…¶ä»–ç¦»æ•£æ‰©æ•£é‡‡æ ·å™¨ã€‚</li>
<li>APSæ–¹æ³•åœ¨æ— éœ€è®­ç»ƒçš„æ ·å¼åŒ–å’Œæ–‡æœ¬å¼•å¯¼ç¼–è¾‘æ–¹é¢æ˜¾ç¤ºå‡ºä¼˜åŠ¿ã€‚</li>
<li>ç ”ç©¶è¡¨æ˜ï¼Œç¦»æ•£æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆå»ºæ¨¡é¢†åŸŸå…·æœ‰å¹¿é˜”çš„åº”ç”¨å‰æ™¯ï¼Œç‰¹åˆ«æ˜¯åœ¨åé‡‡æ ·å’ŒåŸºäºæ–‡æœ¬çš„å›¾åƒç¼–è¾‘ç­‰æ–¹é¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.02291">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-00b466a9e04cdda7ca8f0a9fd3ffb478" align="middle">
<img src="https://picx.zhimg.com/v2-b470577238d43915553216f980e61134" align="middle">
<img src="https://picx.zhimg.com/v2-f5859238b5a6d552eee6689277238615" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Learning-to-Generate-Object-Interactions-with-Physics-Guided-Video-Diffusion"><a href="#Learning-to-Generate-Object-Interactions-with-Physics-Guided-Video-Diffusion" class="headerlink" title="Learning to Generate Object Interactions with Physics-Guided Video   Diffusion"></a>Learning to Generate Object Interactions with Physics-Guided Video   Diffusion</h2><p><strong>Authors:David Romero, Ariana Bermudez, Hao Li, Fabio Pizzati, Ivan Laptev</strong></p>
<p>Recent models for video generation have achieved remarkable progress and are now deployed in film, social media production, and advertising. Beyond their creative potential, such models also hold promise as world simulators for robotics and embodied decision making. Despite strong advances, however, current approaches still struggle to generate physically plausible object interactions and lack physics-grounded control mechanisms. To address this limitation, we introduce KineMask, an approach for physics-guided video generation that enables realistic rigid body control, interactions, and effects. Given a single image and a specified object velocity, our method generates videos with inferred motions and future object interactions. We propose a two-stage training strategy that gradually removes future motion supervision via object masks. Using this strategy we train video diffusion models (VDMs) on synthetic scenes of simple interactions and demonstrate significant improvements of object interactions in real scenes. Furthermore, KineMask integrates low-level motion control with high-level textual conditioning via predictive scene descriptions, leading to effective support for synthesis of complex dynamical phenomena. Extensive experiments show that KineMask achieves strong improvements over recent models of comparable size. Ablation studies further highlight the complementary roles of low- and high-level conditioning in VDMs. Our code, model, and data will be made publicly available. </p>
<blockquote>
<p>è§†é¢‘ç”Ÿæˆæ¨¡å‹æœ€è¿‘å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œå¹¶å·²åº”ç”¨äºç”µå½±ã€ç¤¾äº¤åª’ä½“ç”Ÿäº§å’Œå¹¿å‘Šä¸­ã€‚é™¤äº†åˆ›é€ æ€§æ½œåŠ›ä¹‹å¤–ï¼Œè¿™äº›æ¨¡å‹åœ¨æœºå™¨äººå’Œå®ä½“å†³ç­–åˆ¶å®šçš„ä¸–ç•Œæ¨¡æ‹Ÿå™¨æ–¹é¢ä¹Ÿè¡¨ç°å‡ºå¸Œæœ›ã€‚ç„¶è€Œï¼Œå°½ç®¡å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†å½“å‰çš„æ–¹æ³•ä»ç„¶éš¾ä»¥ç”Ÿæˆç‰©ç†ä¸Šåˆç†çš„ç‰©ä½“äº¤äº’ï¼Œå¹¶ä¸”ç¼ºä¹åŸºäºç‰©ç†çš„æ§åˆ¶æœºåˆ¶ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†KineMaskï¼Œè¿™æ˜¯ä¸€ç§ç‰©ç†æŒ‡å¯¼çš„è§†é¢‘ç”Ÿæˆæ–¹æ³•ï¼Œèƒ½å¤Ÿå®ç°ç°å®çš„åˆšä½“æ§åˆ¶ã€äº¤äº’å’Œæ•ˆæœã€‚ç»™å®šå•ä¸ªå›¾åƒå’ŒæŒ‡å®šçš„å¯¹è±¡é€Ÿåº¦ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥ç”Ÿæˆå…·æœ‰æ¨æ–­è¿åŠ¨å’Œæœªæ¥å¯¹è±¡äº¤äº’çš„è§†é¢‘ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œé€šè¿‡å¯¹è±¡æ©è†œé€æ­¥æ¶ˆé™¤æœªæ¥è¿åŠ¨ç›‘ç£ã€‚ä½¿ç”¨è¿™ç§ç­–ç•¥ï¼Œæˆ‘ä»¬åœ¨ç®€å•äº¤äº’çš„åˆæˆåœºæ™¯ä¸Šè®­ç»ƒè§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ˆVDMsï¼‰ï¼Œå¹¶åœ¨å®é™…åœºæ™¯ä¸­å±•ç¤ºäº†ç‰©ä½“äº¤äº’çš„é‡å¤§æ”¹è¿›ã€‚æ­¤å¤–ï¼ŒKineMaské€šè¿‡é¢„æµ‹åœºæ™¯æè¿°å°†ä½çº§è¿åŠ¨æ§åˆ¶ä¸é«˜çº§æ–‡æœ¬æ¡ä»¶ç›¸ç»“åˆï¼Œæœ‰æ•ˆæ”¯æŒäº†å¤æ‚åŠ¨æ€ç°è±¡çš„ç»¼åˆã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒKineMaskåœ¨åŒç±»è§„æ¨¡çš„æœ€æ–°æ¨¡å‹ä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥çªå‡ºäº†ä½çº§å’Œé«˜çº§æ¡ä»¶åœ¨VDMsä¸­çš„äº’è¡¥ä½œç”¨ã€‚æˆ‘ä»¬çš„ä»£ç ã€æ¨¡å‹å’Œæ•°æ®å°†å…¬å¼€å¯ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.02284v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸè§†é¢‘ç”Ÿæˆæ¨¡å‹åœ¨ç”µå½±ã€ç¤¾äº¤åª’ä½“åˆ¶ä½œå’Œå¹¿å‘Šç­‰é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œå¹¶åœ¨æœºå™¨äººå’Œå®ä½“å†³ç­–æ¨¡æ‹Ÿæ–¹é¢å±•ç°å‡ºæ½œåŠ›ã€‚ç„¶è€Œï¼Œå½“å‰æ–¹æ³•éš¾ä»¥ç”Ÿæˆç‰©ç†ä¸Šåˆç†çš„ç‰©ä½“äº’åŠ¨ï¼Œå¹¶ç¼ºä¹ç‰©ç†åŸºç¡€çš„æ§åˆ¶æœºåˆ¶ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºKineMaskæ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§ç‰©ç†æŒ‡å¯¼çš„è§†é¢‘ç”Ÿæˆæ–¹æ³•ï¼Œèƒ½å¤Ÿå®ç°ç°å®çš„åˆšä½“æ§åˆ¶ã€äº’åŠ¨å’Œæ•ˆæœã€‚ç»™å®šå•å¼ å›¾åƒå’ŒæŒ‡å®šç‰©ä½“é€Ÿåº¦ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥ç”Ÿæˆå…·æœ‰æ¨æ–­è¿åŠ¨å’Œæœªæ¥ç‰©ä½“äº’åŠ¨çš„è§†é¢‘ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œé€šè¿‡ç‰©ä½“æ©è†œé€æ­¥æ¶ˆé™¤æœªæ¥è¿åŠ¨ç›‘ç£ã€‚ä½¿ç”¨è¿™ç§ç­–ç•¥ï¼Œæˆ‘ä»¬åœ¨åˆæˆç®€å•äº’åŠ¨åœºæ™¯çš„è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ˆVDMsï¼‰è®­ç»ƒä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼Œå¹¶åœ¨å®é™…åœºæ™¯ä¸­å±•ç¤ºäº†ç‰©ä½“äº’åŠ¨çš„æ”¹å–„ã€‚æ­¤å¤–ï¼ŒKineMaské€šè¿‡å°†ä½çº§è¿åŠ¨æ§åˆ¶ä¸é«˜çº§æ–‡æœ¬æ¡ä»¶ç›¸ç»“åˆï¼Œå®ç°äº†å¤æ‚åŠ¨æ€ç°è±¡çš„åˆæˆæ”¯æŒã€‚å®éªŒè¡¨æ˜ï¼ŒKineMaskåœ¨åŒç±»è§„æ¨¡æ¨¡å‹ä¸­å–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†é¢‘ç”Ÿæˆæ¨¡å‹åœ¨å¤šä¸ªé¢†åŸŸå–å¾—æ˜¾è‘—è¿›å±•ï¼Œå¹¶å±•ç°å‡ºåœ¨æœºå™¨äººå’Œå®ä½“å†³ç­–æ¨¡æ‹Ÿæ–¹é¢çš„æ½œåŠ›ã€‚</li>
<li>å½“å‰æ¨¡å‹éš¾ä»¥ç”Ÿæˆç‰©ç†ä¸Šåˆç†çš„ç‰©ä½“äº’åŠ¨ï¼Œç¼ºä¹ç‰©ç†åŸºç¡€çš„æ§åˆ¶æœºåˆ¶ã€‚</li>
<li>KineMaskæ˜¯ä¸€ç§ç‰©ç†æŒ‡å¯¼çš„è§†é¢‘ç”Ÿæˆæ–¹æ³•ï¼Œèƒ½å®ç°ç°å®çš„åˆšä½“æ§åˆ¶ã€äº’åŠ¨å’Œæ•ˆæœã€‚</li>
<li>KineMaskå¯ä»¥é€šè¿‡ç»™å®šçš„å•å¼ å›¾åƒå’Œç‰©ä½“é€Ÿåº¦ç”Ÿæˆå…·æœ‰æ¨æ–­è¿åŠ¨å’Œæœªæ¥ç‰©ä½“äº’åŠ¨çš„è§†é¢‘ã€‚</li>
<li>ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥é€æ­¥æ¶ˆé™¤æœªæ¥è¿åŠ¨ç›‘ç£ï¼Œæ”¹è¿›äº†è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ˆVDMsï¼‰çš„è®­ç»ƒã€‚</li>
<li>KineMaské›†æˆäº†ä½çº§è¿åŠ¨æ§åˆ¶å’Œé«˜çº§æ–‡æœ¬æ¡ä»¶ï¼Œæ”¯æŒå¤æ‚åŠ¨æ€ç°è±¡çš„åˆæˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.02284">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4ed0496045562a6400b3086b83fcc111" align="middle">
<img src="https://picx.zhimg.com/v2-ba534e22e45077133ee420ee4508e152" align="middle">
<img src="https://picx.zhimg.com/v2-a5a774bc547e2a4d4931115f337151b5" align="middle">
<img src="https://picx.zhimg.com/v2-5bd43d41bb05a4458c97c3c08b177bcc" align="middle">
<img src="https://picx.zhimg.com/v2-39bf0325c46f7a7872e107d388926f32" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Self-Forcing-Towards-Minute-Scale-High-Quality-Video-Generation"><a href="#Self-Forcing-Towards-Minute-Scale-High-Quality-Video-Generation" class="headerlink" title="Self-Forcing++: Towards Minute-Scale High-Quality Video Generation"></a>Self-Forcing++: Towards Minute-Scale High-Quality Video Generation</h2><p><strong>Authors:Justin Cui, Jie Wu, Ming Li, Tao Yang, Xiaojie Li, Rui Wang, Andrew Bai, Yuanhao Ban, Cho-Jui Hsieh</strong></p>
<p>Diffusion models have revolutionized image and video generation, achieving unprecedented visual quality. However, their reliance on transformer architectures incurs prohibitively high computational costs, particularly when extending generation to long videos. Recent work has explored autoregressive formulations for long video generation, typically by distilling from short-horizon bidirectional teachers. Nevertheless, given that teacher models cannot synthesize long videos, the extrapolation of student models beyond their training horizon often leads to pronounced quality degradation, arising from the compounding of errors within the continuous latent space. In this paper, we propose a simple yet effective approach to mitigate quality degradation in long-horizon video generation without requiring supervision from long-video teachers or retraining on long video datasets. Our approach centers on exploiting the rich knowledge of teacher models to provide guidance for the student model through sampled segments drawn from self-generated long videos. Our method maintains temporal consistency while scaling video length by up to 20x beyond teacherâ€™s capability, avoiding common issues such as over-exposure and error-accumulation without recomputing overlapping frames like previous methods. When scaling up the computation, our method shows the capability of generating videos up to 4 minutes and 15 seconds, equivalent to 99.9% of the maximum span supported by our base modelâ€™s position embedding and more than 50x longer than that of our baseline model. Experiments on standard benchmarks and our proposed improved benchmark demonstrate that our approach substantially outperforms baseline methods in both fidelity and consistency. Our long-horizon videos demo can be found at <a target="_blank" rel="noopener" href="https://self-forcing-plus-plus.github.io/">https://self-forcing-plus-plus.github.io/</a> </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹å·²ç»å½»åº•æ”¹å˜äº†å›¾åƒå’Œè§†é¢‘ç”Ÿæˆé¢†åŸŸï¼Œå®ç°äº†å‰æ‰€æœªæœ‰çš„è§†è§‰è´¨é‡ã€‚ç„¶è€Œï¼Œå®ƒä»¬ä¾èµ–äºå˜å‹å™¨æ¶æ„ï¼Œäº§ç”Ÿäº†é«˜æ˜‚çš„è®¡ç®—æˆæœ¬ï¼Œç‰¹åˆ«æ˜¯åœ¨å°†ç”Ÿæˆæ‰©å±•åˆ°é•¿è§†é¢‘æ—¶ã€‚è¿‘æœŸçš„å·¥ä½œå·²ç»æ¢ç´¢äº†é•¿è§†é¢‘ç”Ÿæˆçš„è‡ªå›å½’å…¬å¼ï¼Œé€šå¸¸æ˜¯é€šè¿‡ä»çŸ­è§†é‡åŒå‘æ•™å¸ˆæ¨¡å‹ä¸­è¿›è¡Œè’¸é¦ã€‚ç„¶è€Œï¼Œç”±äºæ•™å¸ˆæ¨¡å‹æ— æ³•åˆæˆé•¿è§†é¢‘ï¼Œå­¦ç”Ÿæ¨¡å‹åœ¨è¶…å‡ºå…¶è®­ç»ƒè§†é‡çš„æ¨å¹¿å¾€å¾€ä¼šå¯¼è‡´è´¨é‡æ˜¾è‘—ä¸‹é™ï¼Œè¿™æ˜¯ç”±äºè¿ç»­æ½œåœ¨ç©ºé—´å†…é”™è¯¯ç´¯ç§¯å¯¼è‡´çš„ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æ–¹æ³•æ¥ç¼“è§£é•¿è§†é‡è§†é¢‘ç”Ÿæˆä¸­çš„è´¨é‡ä¸‹é™é—®é¢˜ï¼Œè€Œæ— éœ€ä»é•¿è§†é¢‘æ•™å¸ˆé‚£é‡Œè·å¾—ç›‘ç£æˆ–é‡æ–°è®­ç»ƒé•¿è§†é¢‘æ•°æ®é›†ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸»è¦åˆ©ç”¨æ•™å¸ˆæ¨¡å‹çš„ä¸°å¯ŒçŸ¥è¯†ï¼Œé€šè¿‡ä»å­¦ç”Ÿæ¨¡å‹è‡ªæˆ‘ç”Ÿæˆçš„é•¿è§†é¢‘ä¸­æŠ½å–çš„ç‰‡æ®µæ¥æä¾›æŒ‡å¯¼ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¿æŒæ—¶é—´ä¸€è‡´æ€§çš„åŒæ—¶ï¼Œé€šè¿‡å°†è§†é¢‘é•¿åº¦æ‰©å±•åˆ°æ•™å¸ˆèƒ½åŠ›çš„20å€ï¼Œé¿å…äº†è¿‡åº¦æ›å…‰å’Œé”™è¯¯ç´¯ç§¯ç­‰å¸¸è§é—®é¢˜ï¼Œè€Œä¸”æ— éœ€åƒä¹‹å‰çš„æ–¹æ³•é‚£æ ·é‡æ–°è®¡ç®—é‡å å¸§ã€‚åœ¨è®¡ç®—è§„æ¨¡æ‰©å¤§æ—¶ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿç”Ÿæˆé•¿è¾¾4åˆ†é’Ÿ15ç§’çš„è§†é¢‘ï¼Œç›¸å½“äºæˆ‘ä»¬åŸºç¡€æ¨¡å‹ä½ç½®åµŒå…¥æ‰€æ”¯æŒçš„æœ€å¤§è·¨åº¦çš„99.9%ï¼Œå¹¶ä¸”æ¯”æˆ‘ä»¬åŸºçº¿æ¨¡å‹é•¿50å€ä»¥ä¸Šã€‚åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•å’Œæˆ‘ä»¬æå‡ºçš„æ”¹è¿›åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¿çœŸåº¦å’Œä¸€è‡´æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚æ‚¨å¯ä»¥åœ¨ <a target="_blank" rel="noopener" href="https://self-forcing-plus-plus.github.io/">https://self-forcing-plus-plus.github.io/</a> æ‰¾åˆ°æˆ‘ä»¬çš„é•¿è§†é‡è§†é¢‘æ¼”ç¤ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.02283v1">PDF</a> preprint</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒå’Œè§†é¢‘ç”Ÿæˆæ–¹é¢çš„é©å‘½æ€§è¿›å±•ï¼Œå…¶è§†è§‰è´¨é‡è¾¾åˆ°äº†å‰æ‰€æœªæœ‰çš„æ°´å¹³ã€‚ç„¶è€Œï¼Œç”±äºä¾èµ–å˜å‹å™¨æ¶æ„ï¼Œæ‰©æ•£æ¨¡å‹è®¡ç®—æˆæœ¬é«˜æ˜‚ï¼Œå°¤å…¶æ˜¯åœ¨é•¿è§†é¢‘ç”Ÿæˆæ–¹é¢çš„æ‰©å±•æ€§å—é™ã€‚æœ€è¿‘çš„ç ”ç©¶å°è¯•é€šè¿‡ä»çŸ­æœŸåŒå‘æ•™å¸ˆæ¨¡å‹ä¸­æç‚¼å‡ºè‡ªå›å½’å…¬å¼è¿›è¡Œé•¿è§†é¢‘ç”Ÿæˆã€‚ç„¶è€Œï¼Œç”±äºæ•™å¸ˆæ¨¡å‹æ— æ³•åˆæˆé•¿è§†é¢‘ï¼Œå­¦ç”Ÿæ¨¡å‹åœ¨è¶…å‡ºè®­ç»ƒèŒƒå›´çš„æƒ…å†µä¸‹ä¼šäº§ç”Ÿæ˜æ˜¾çš„è´¨é‡ä¸‹é™é—®é¢˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„æ–¹æ³•ï¼Œåœ¨ä¸ä¾èµ–é•¿è§†é¢‘æ•™å¸ˆæ¨¡å‹çš„ç›‘ç£æˆ–é‡æ–°è®­ç»ƒé•¿è§†é¢‘æ•°æ®é›†çš„æƒ…å†µä¸‹ï¼Œå‡è½»é•¿å‘¨æœŸè§†é¢‘ç”Ÿæˆä¸­çš„è´¨é‡ä¸‹é™é—®é¢˜ã€‚è¯¥æ–¹æ³•é€šè¿‡åˆ©ç”¨æ•™å¸ˆæ¨¡å‹çš„ä¸°å¯ŒçŸ¥è¯†ï¼Œé€šè¿‡ä»è‡ªæˆ‘ç”Ÿæˆçš„é•¿è§†é¢‘ä¸­æŠ½å–ç‰‡æ®µæ¥æŒ‡å¯¼æ¨¡å‹ï¼Œåœ¨ä¿æŒæ—¶é—´ä¸€è‡´æ€§çš„åŒæ—¶ï¼Œå°†è§†é¢‘é•¿åº¦æ‰©å¤§äº†é«˜è¾¾æ•™å¸ˆèƒ½åŠ›çš„20å€ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿çœŸåº¦å’Œä¸€è‡´æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒå’Œè§†é¢‘ç”Ÿæˆä¸­å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å­˜åœ¨é«˜è®¡ç®—æˆæœ¬é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨é•¿è§†é¢‘ç”Ÿæˆæ–¹é¢ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å°è¯•é€šè¿‡æ•™å¸ˆæ¨¡å‹æŒ‡å¯¼å­¦ç”Ÿæ¨¡å‹è¿›è¡Œé•¿è§†é¢‘ç”Ÿæˆï¼Œä½†å­˜åœ¨è´¨é‡ä¸‹é™é—®é¢˜ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„æ–¹æ³•ï¼Œé€šè¿‡åˆ©ç”¨æ•™å¸ˆæ¨¡å‹çš„ä¸°å¯ŒçŸ¥è¯†ï¼Œä»è‡ªæˆ‘ç”Ÿæˆçš„é•¿è§†é¢‘ä¸­æŠ½å–ç‰‡æ®µæ¥æŒ‡å¯¼æ¨¡å‹ï¼Œæé«˜äº†é•¿å‘¨æœŸè§†é¢‘ç”Ÿæˆçš„è´¨é‡ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨ä¸é‡æ–°è®­ç»ƒæˆ–ä¾èµ–é•¿è§†é¢‘æ•™å¸ˆæ¨¡å‹çš„æƒ…å†µä¸‹ï¼Œå°†è§†é¢‘é•¿åº¦æ‰©å¤§é«˜è¾¾æ•™å¸ˆèƒ½åŠ›çš„20å€ã€‚</li>
<li>å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•å’Œæˆ‘ä»¬æ”¹è¿›åçš„åŸºå‡†æµ‹è¯•ä¸­å‡æ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½å¤Ÿç”Ÿæˆé•¿è¾¾4åˆ†é’Ÿ15ç§’çš„è§†é¢‘ï¼Œç›¸å½“äºåŸºç¡€æ¨¡å‹ä½ç½®åµŒå…¥æ‰€æ”¯æŒçš„æœ€å¤§è·¨åº¦çš„99.9%ï¼Œå¹¶ä¸”æ¯”åŸºçº¿æ¨¡å‹é•¿å‡º50å€ä»¥ä¸Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.02283">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7f7630f0a973200a5b135bb4f796dc34" align="middle">
<img src="https://picx.zhimg.com/v2-ef5363f6d53d41c680e88654968dc009" align="middle">
<img src="https://picx.zhimg.com/v2-bbda4f8101deffc9c86c78b07f487fe3" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="VGDM-Vision-Guided-Diffusion-Model-for-Brain-Tumor-Detection-and-Segmentation"><a href="#VGDM-Vision-Guided-Diffusion-Model-for-Brain-Tumor-Detection-and-Segmentation" class="headerlink" title="VGDM: Vision-Guided Diffusion Model for Brain Tumor Detection and   Segmentation"></a>VGDM: Vision-Guided Diffusion Model for Brain Tumor Detection and   Segmentation</h2><p><strong>Authors:Arman Behnam</strong></p>
<p>Accurate detection and segmentation of brain tumors from magnetic resonance imaging (MRI) are essential for diagnosis, treatment planning, and clinical monitoring. While convolutional architectures such as U-Net have long been the backbone of medical image segmentation, their limited capacity to capture long-range dependencies constrains performance on complex tumor structures. Recent advances in diffusion models have demonstrated strong potential for generating high-fidelity medical images and refining segmentation boundaries.   In this work, we propose VGDM: Vision-Guided Diffusion Model for Brain Tumor Detection and Segmentation framework, a transformer-driven diffusion framework for brain tumor detection and segmentation. By embedding a vision transformer at the core of the diffusion process, the model leverages global contextual reasoning together with iterative denoising to enhance both volumetric accuracy and boundary precision. The transformer backbone enables more effective modeling of spatial relationships across entire MRI volumes, while diffusion refinement mitigates voxel-level errors and recovers fine-grained tumor details.   This hybrid design provides a pathway toward improved robustness and scalability in neuro-oncology, moving beyond conventional U-Net baselines. Experimental validation on MRI brain tumor datasets demonstrates consistent gains in Dice similarity and Hausdorff distance, underscoring the potential of transformer-guided diffusion models to advance the state of the art in tumor segmentation. </p>
<blockquote>
<p>ä»ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰å‡†ç¡®æ£€æµ‹å’Œåˆ†å‰²è„‘è‚¿ç˜¤å¯¹äºè¯Šæ–­ã€æ²»ç–—è®¡åˆ’å’Œä¸´åºŠç›‘æµ‹è‡³å…³é‡è¦ã€‚è™½ç„¶U-Netç­‰å·ç§¯æ¶æ„é•¿æœŸä»¥æ¥ä¸€ç›´æ˜¯åŒ»å­¦å›¾åƒåˆ†å‰²çš„æ”¯æŸ±ï¼Œä½†å…¶æ•æ‰é•¿è·ç¦»ä¾èµ–å…³ç³»çš„æœ‰é™èƒ½åŠ›åœ¨å¤æ‚è‚¿ç˜¤ç»“æ„ä¸Šçš„è¡¨ç°å—åˆ°é™åˆ¶ã€‚æ‰©æ•£æ¨¡å‹çš„æœ€æ–°è¿›å±•æ˜¾ç¤ºå‡ºç”Ÿæˆé«˜ä¿çœŸåŒ»å­¦å›¾åƒå’Œç»†åŒ–åˆ†å‰²è¾¹ç•Œæ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†VGDMï¼šç”¨äºè„‘è‚¿ç˜¤æ£€æµ‹å’Œåˆ†å‰²çš„åŸºäºè§†è§‰å¼•å¯¼çš„æ‰©æ•£æ¨¡å‹æ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè„‘è‚¿ç˜¤æ£€æµ‹å’Œåˆ†å‰²çš„åŸºäºå˜æ¢é©±åŠ¨çš„æ‰©æ•£æ¡†æ¶ã€‚é€šè¿‡åœ¨æ‰©æ•£è¿‡ç¨‹çš„æ ¸å¿ƒä¸­åµŒå…¥è§†è§‰å˜æ¢å™¨ï¼Œè¯¥æ¨¡å‹åˆ©ç”¨å…¨å±€ä¸Šä¸‹æ–‡æ¨ç†å’Œè¿­ä»£å»å™ªæ¥æé«˜ä½“ç§¯ç²¾åº¦å’Œè¾¹ç•Œç²¾åº¦ã€‚å˜æ¢éª¨å¹²ç½‘å¯ä»¥æ›´æœ‰æ•ˆåœ°å¯¹æ•´ä¸ªMRIä½“ç§¯çš„ç©ºé—´å…³ç³»è¿›è¡Œå»ºæ¨¡ï¼Œè€Œæ‰©æ•£ç»†åŒ–åˆ™å‡è½»äº†ä½“ç´ çº§åˆ«çš„é”™è¯¯å¹¶æ¢å¤äº†ç²¾ç»†çš„è‚¿ç˜¤ç»†èŠ‚ã€‚è¿™ç§æ··åˆè®¾è®¡æä¾›äº†åœ¨ç¥ç»è‚¿ç˜¤å­¦ä¸­æé«˜ç¨³å¥æ€§å’Œå¯æ‰©å±•æ€§çš„é€”å¾„ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿçš„U-NetåŸºå‡†çº¿ã€‚åœ¨MRIè„‘è‚¿ç˜¤æ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯è¯æ˜äº†Diceç›¸ä¼¼åº¦å’ŒHausdorffè·ç¦»çš„æŒç»­æé«˜ï¼Œçªæ˜¾äº†åŸºäºå˜æ¢çš„æ‰©æ•£æ¨¡å‹åœ¨è‚¿ç˜¤åˆ†å‰²æ–¹é¢æ¨åŠ¨æœ€æ–°æŠ€æœ¯çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.02086v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„è„‘è‚¿ç˜¤æ£€æµ‹ä¸åˆ†å‰²æ¡†æ¶VGDMï¼Œè¯¥æ¡†æ¶èåˆäº†å·ç§¯ç½‘ç»œå’Œæ‰©æ•£æ¨¡å‹çš„ä¼˜ç‚¹ã€‚é€šè¿‡ä½¿ç”¨æ ¸å¿ƒä¸­çš„è§†è§‰å˜å‹å™¨å¹¶ç»“åˆå…¨å±€ä¸Šä¸‹æ–‡æ¨ç†å’Œè¿­ä»£å»å™ªï¼Œæ¨¡å‹åœ¨ä¸‰ç»´ä½“ç§¯å‡†ç¡®æ€§å’Œè¾¹ç•Œç²¾åº¦ä¸Šéƒ½æœ‰æ‰€æå‡ã€‚åœ¨MRIè„‘è‚¿ç˜¤æ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯è¯æ˜äº†è¯¥æ··åˆè®¾è®¡ç›¸è¾ƒäºä¼ ç»ŸU-NetåŸºå‡†çº¿çš„ä¼˜è¶Šæ€§å’Œæ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è„‘è‚¿ç˜¤çš„å‡†ç¡®æ£€æµ‹å’Œåˆ†å‰²å¯¹è¯Šæ–­ã€æ²»ç–—è®¡åˆ’å’Œä¸´åºŠç›‘æµ‹è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¦‚U-Netåœ¨å¤æ‚è‚¿ç˜¤ç»“æ„ä¸Šçš„æ€§èƒ½å—åˆ°é™åˆ¶ï¼Œå› ä¸ºå®ƒä»¬çš„é•¿æœŸä¾èµ–æ•æ‰èƒ½åŠ›æœ‰é™ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹å…·æœ‰ç”Ÿæˆé«˜è´¨é‡åŒ»å­¦å›¾åƒå’Œç»†åŒ–åˆ†å‰²è¾¹ç•Œçš„æ½œåŠ›ã€‚</li>
<li>æå‡ºçš„VGDMæ¡†æ¶ç»“åˆäº†å·ç§¯ç½‘ç»œå’Œæ‰©æ•£æ¨¡å‹çš„ä¼˜ç‚¹ï¼Œä½¿ç”¨è§†è§‰å˜å‹å™¨ä¸ºæ ¸å¿ƒã€‚</li>
<li>è¯¥æ¨¡å‹åˆ©ç”¨å…¨å±€ä¸Šä¸‹æ–‡æ¨ç†å’Œè¿­ä»£å»å™ªï¼Œæé«˜äº†ä½“ç§¯å‡†ç¡®æ€§å’Œè¾¹ç•Œç²¾åº¦ã€‚</li>
<li>è§†è§‰å˜å‹å™¨èƒ½æœ‰æ•ˆå»ºæ¨¡æ•´ä¸ªMRIä½“ç§¯çš„ç©ºé—´å…³ç³»ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.02086">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b1bbd52fe85d59156603ba5ce1e53fd2" align="middle">
<img src="https://picx.zhimg.com/v2-9c06905b4edc087b85564e79ea3894d3" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="MSRepaint-Multiple-Sclerosis-Repaint-with-Conditional-Denoising-Diffusion-Implicit-Model-for-Bidirectional-Lesion-Filling-and-Synthesis"><a href="#MSRepaint-Multiple-Sclerosis-Repaint-with-Conditional-Denoising-Diffusion-Implicit-Model-for-Bidirectional-Lesion-Filling-and-Synthesis" class="headerlink" title="MSRepaint: Multiple Sclerosis Repaint with Conditional Denoising   Diffusion Implicit Model for Bidirectional Lesion Filling and Synthesis"></a>MSRepaint: Multiple Sclerosis Repaint with Conditional Denoising   Diffusion Implicit Model for Bidirectional Lesion Filling and Synthesis</h2><p><strong>Authors:Jinwei Zhang, Lianrui Zuo, Yihao Liu, Hang Zhang, Samuel W. Remedios, Bennett A. Landman, Peter A. Calabresi, Shiv Saidha, Scott D. Newsome, Dzung L. Pham, Jerry L. Prince, Ellen M. Mowry, Aaron Carass</strong></p>
<p>In multiple sclerosis, lesions interfere with automated magnetic resonance imaging analyses such as brain parcellation and deformable registration, while lesion segmentation models are hindered by the limited availability of annotated training data. To address both issues, we propose MSRepaint, a unified diffusion-based generative model for bidirectional lesion filling and synthesis that restores anatomical continuity for downstream analyses and augments segmentation through realistic data generation. MSRepaint conditions on spatial lesion masks for voxel-level control, incorporates contrast dropout to handle missing inputs, integrates a repainting mechanism to preserve surrounding anatomy during lesion filling and synthesis, and employs a multi-view DDIM inversion and fusion pipeline for 3D consistency with fast inference. Extensive evaluations demonstrate the effectiveness of MSRepaint across multiple tasks. For lesion filling, we evaluate both the accuracy within the filled regions and the impact on downstream tasks including brain parcellation and deformable registration. MSRepaint outperforms the traditional lesion filling methods FSL and NiftySeg, and achieves accuracy on par with FastSurfer-LIT, a recent diffusion model-based inpainting method, while offering over 20 times faster inference. For lesion synthesis, state-of-the-art MS lesion segmentation models trained on MSRepaint-synthesized data outperform those trained on CarveMix-synthesized data or real ISBI challenge training data across multiple benchmarks, including the MICCAI 2016 and UMCL datasets. Additionally, we demonstrate that MSRepaintâ€™s unified bidirectional filling and synthesis capability, with full spatial control over lesion appearance, enables high-fidelity simulation of lesion evolution in longitudinal MS progression. </p>
<blockquote>
<p>åœ¨å¤šå‘æ€§ç¡¬åŒ–ç—‡ï¼ˆMSï¼‰ä¸­ï¼Œç—…ç¶ä¼šå¹²æ‰°è‡ªåŠ¨ç£å…±æŒ¯æˆåƒåˆ†æï¼Œå¦‚è„‘éƒ¨åˆ†å‰²å’Œå¯å˜å½¢é…å‡†ï¼Œè€Œç—…ç¶åˆ†å‰²æ¨¡å‹åˆ™å—åˆ°æ³¨é‡Šè®­ç»ƒæ•°æ®æœ‰é™æ€§çš„é˜»ç¢ã€‚ä¸ºäº†è§£å†³è¿™ä¸¤ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†MSRepaintï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºæ‰©æ•£çš„ç»Ÿä¸€ç”Ÿæˆæ¨¡å‹ï¼Œç”¨äºåŒå‘ç—…ç¶å¡«å……å’Œåˆæˆï¼Œå®ƒæ¢å¤äº†ä¸‹æ¸¸åˆ†æçš„è§£å‰–è¿ç»­æ€§ï¼Œå¹¶é€šè¿‡ç°å®æ•°æ®çš„ç”Ÿæˆå¢å¼ºäº†åˆ†å‰²ã€‚MSRepaintæ ¹æ®ç©ºé—´ç—…ç¶æ©è†œè¿›è¡Œä½“ç´ çº§æ§åˆ¶ï¼Œé‡‡ç”¨å¯¹æ¯”ä¸¢å¤±å¤„ç†ç¼ºå¤±è¾“å…¥ï¼Œèå…¥é‡ç»˜æœºåˆ¶åœ¨ç—…ç¶å¡«å……å’Œåˆæˆè¿‡ç¨‹ä¸­ä¿ç•™å‘¨å›´è§£å‰–ç»“æ„ï¼Œå¹¶é‡‡ç”¨å¤šè§†å›¾DDIMåæ¼”å’Œèåˆç®¡é“å®ç°3Dä¸€è‡´æ€§ä»¥åŠ å¿«æ¨ç†é€Ÿåº¦ã€‚å¹¿æ³›è¯„ä¼°è¡¨æ˜MSRepaintåœ¨å¤šä¸ªä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚å¯¹äºç—…ç¶å¡«å……ï¼Œæˆ‘ä»¬è¯„ä¼°äº†å¡«å……åŒºåŸŸå†…çš„å‡†ç¡®æ€§ä»¥åŠå¯¹ä¸‹æ¸¸ä»»åŠ¡ï¼ˆåŒ…æ‹¬è„‘éƒ¨åˆ†å‰²å’Œå¯å˜å½¢é…å‡†ï¼‰çš„å½±å“ã€‚MSRepaintä¼˜äºä¼ ç»Ÿçš„FSLå’ŒNiftySegç—…ç¶å¡«å……æ–¹æ³•ï¼Œä¸åŸºäºæ‰©æ•£æ¨¡å‹çš„FastSurfer-LITæ–¹æ³•ç²¾åº¦ç›¸å½“ï¼Œä½†æ¨ç†é€Ÿåº¦è¶…è¿‡å…¶20å€ã€‚å¯¹äºç—…ç¶åˆæˆï¼Œä½¿ç”¨MSRepaintåˆæˆæ•°æ®è®­ç»ƒçš„æœ€æ–°å¤šå‘æ€§ç¡¬åŒ–ç—…ç¶åˆ†å‰²æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜äºä½¿ç”¨CarveMixåˆæˆæ•°æ®æˆ–çœŸå®ISBIæŒ‘æˆ˜èµ›è®­ç»ƒæ•°æ®è®­ç»ƒçš„æ¨¡å‹ï¼ŒåŒ…æ‹¬MICCAI 2016å’ŒUMCLæ•°æ®é›†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¯æ˜äº†MSRepaintçš„ç»Ÿä¸€åŒå‘å¡«å……å’Œåˆæˆèƒ½åŠ›ï¼Œä»¥åŠå¯¹ç—…ç¶å¤–è§‚çš„å®Œå…¨ç©ºé—´æ§åˆ¶ï¼Œèƒ½å¤Ÿé«˜è´¨é‡åœ°æ¨¡æ‹Ÿå¤šå‘æ€§ç¡¬åŒ–ç—‡çºµå‘è¿›å±•ä¸­çš„ç—…ç¶æ¼”å˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.02063v1">PDF</a> </p>
<p><strong>Summary</strong><br>    æœ¬æ–‡æå‡ºä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„åŒå‘ç—…å˜å¡«å……ä¸åˆæˆæ–¹æ³•MSRepaintï¼Œç”¨äºè§£å†³å¤šå‘æ€§ç¡¬åŒ–ç—‡ï¼ˆMSï¼‰ä¸­çš„ç—…ç¶å¹²æ‰°é—®é¢˜ã€‚è¯¥æ–¹æ³•å¯æ¢å¤è§£å‰–è¿ç»­æ€§ï¼Œä¿ƒè¿›ä¸‹æ¸¸åˆ†æå’Œåˆ†å‰²ï¼Œå¹¶é€šè¿‡é€¼çœŸçš„æ•°æ®ç”Ÿæˆå¢å¼ºåˆ†å‰²æ•ˆæœã€‚å®éªŒè¯æ˜ï¼ŒMSRepaintåœ¨å¤šä»»åŠ¡è¯„ä¼°ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä¸ä»…æé«˜äº†ç—…ç¶å¡«å……çš„å‡†ç¡®æ€§ï¼Œå¹¶æé«˜äº†ä¸‹æ¸¸ä»»åŠ¡å¦‚è„‘åˆ†åŒºå’Œå¯å˜å½¢æ³¨å†Œçš„å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼ŒåŸºäºMSRepaintåˆæˆçš„æ•°æ®è®­ç»ƒçš„MSç—…ç¶åˆ†å‰²æ¨¡å‹è¡¨ç°ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œå¹¶å±•ç¤ºäº†é«˜ä¿çœŸæ¨¡æ‹Ÿç—…ç¶æ¼”å˜çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MSRepaintæ˜¯ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„åŒå‘ç—…å˜å¡«å……ä¸åˆæˆæ–¹æ³•ï¼Œç”¨äºè§£å†³å¤šå‘æ€§ç¡¬åŒ–ç—‡ä¸­çš„ç—…ç¶å¹²æ‰°é—®é¢˜ã€‚</li>
<li>MSRepaintå¯æ¢å¤è§£å‰–è¿ç»­æ€§ï¼Œä¿ƒè¿›ä¸‹æ¸¸åˆ†æä»»åŠ¡ã€‚</li>
<li>MSRepainté€šè¿‡æ•°æ®ç”Ÿæˆå¢å¼ºåˆ†å‰²æ•ˆæœï¼Œæé«˜ç—…ç¶å¡«å……çš„å‡†ç¡®æ€§ã€‚</li>
<li>ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼ŒMSRepaintåœ¨ç—…ç¶å¡«å……ä»»åŠ¡ä¸Šè¡¨ç°å‡ºæ›´é«˜çš„å‡†ç¡®æ€§ã€‚</li>
<li>åŸºäºMSRepaintåˆæˆçš„æ•°æ®è®­ç»ƒçš„MSç—…ç¶åˆ†å‰²æ¨¡å‹è¡¨ç°ä¼˜äºå…¶ä»–è®­ç»ƒæ–¹æ³•ã€‚</li>
<li>MSRepaintå…·å¤‡é«˜ä¿çœŸæ¨¡æ‹Ÿç—…ç¶æ¼”å˜çš„æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.02063">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3becfdae6470483030792001027301e9" align="middle">
<img src="https://picx.zhimg.com/v2-72821693b54fe474a229dfcffda5ed72" align="middle">
<img src="https://picx.zhimg.com/v2-22209115d17448c7073443a6aff3bd79" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="ZK-WAGON-Imperceptible-Watermark-for-Image-Generation-Models-using-ZK-SNARKs"><a href="#ZK-WAGON-Imperceptible-Watermark-for-Image-Generation-Models-using-ZK-SNARKs" class="headerlink" title="ZK-WAGON: Imperceptible Watermark for Image Generation Models using   ZK-SNARKs"></a>ZK-WAGON: Imperceptible Watermark for Image Generation Models using   ZK-SNARKs</h2><p><strong>Authors:Aadarsh Anantha Ramakrishnan, Shubham Agarwal, Selvanayagam S, Kunwar Singh</strong></p>
<p>As image generation models grow increasingly powerful and accessible, concerns around authenticity, ownership, and misuse of synthetic media have become critical. The ability to generate lifelike images indistinguishable from real ones introduces risks such as misinformation, deepfakes, and intellectual property violations. Traditional watermarking methods either degrade image quality, are easily removed, or require access to confidential model internals - making them unsuitable for secure and scalable deployment. We are the first to introduce ZK-WAGON, a novel system for watermarking image generation models using the Zero-Knowledge Succinct Non Interactive Argument of Knowledge (ZK-SNARKs). Our approach enables verifiable proof of origin without exposing model weights, generation prompts, or any sensitive internal information. We propose Selective Layer ZK-Circuit Creation (SL-ZKCC), a method to selectively convert key layers of an image generation model into a circuit, reducing proof generation time significantly. Generated ZK-SNARK proofs are imperceptibly embedded into a generated image via Least Significant Bit (LSB) steganography. We demonstrate this system on both GAN and Diffusion models, providing a secure, model-agnostic pipeline for trustworthy AI image generation. </p>
<blockquote>
<p>éšç€å›¾åƒç”Ÿæˆæ¨¡å‹çš„æ—¥ç›Šå¼ºå¤§å’Œæ™®åŠï¼Œå…³äºåˆæˆåª’ä½“çš„çœŸå®æ€§ã€æ‰€æœ‰æƒå’Œè¯¯ç”¨çš„æ‹…å¿§å·²ç»å˜å¾—è‡³å…³é‡è¦ã€‚èƒ½å¤Ÿç”Ÿæˆä¸çœŸå®å›¾åƒæ— æ³•åŒºåˆ†çš„é€¼çœŸå›¾åƒï¼Œå¼•å…¥äº†è¯¸å¦‚è™šå‡ä¿¡æ¯ã€æ·±åº¦ä¼ªé€ å’ŒçŸ¥è¯†äº§æƒä¾µçŠ¯ç­‰é£é™©ã€‚ä¼ ç»Ÿçš„æ°´å°æ–¹æ³•è¦ä¹ˆä¼šé™ä½å›¾åƒè´¨é‡ï¼Œè¦ä¹ˆå®¹æ˜“è¢«ç§»é™¤ï¼Œè¦ä¹ˆéœ€è¦è®¿é—®æ¨¡å‹çš„å†…éƒ¨æœºå¯†ä¿¡æ¯ï¼Œå› æ­¤å®ƒä»¬ä¸é€‚åˆè¿›è¡Œå®‰å…¨å’Œå¯æ‰©å±•çš„éƒ¨ç½²ã€‚æˆ‘ä»¬é¦–æ¬¡å¼•å…¥äº†ZK-WAGONç³»ç»Ÿï¼Œè¿™æ˜¯ä¸€ç§ä½¿ç”¨é›¶çŸ¥è¯†ç®€æ´éäº¤äº’è¯æ˜çŸ¥è¯†ï¼ˆZK-SNARKsï¼‰ä¸ºå›¾åƒç”Ÿæˆæ¨¡å‹æ·»åŠ æ°´å°çš„æ–°ç³»ç»Ÿã€‚æˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿåœ¨ä¸æš´éœ²æ¨¡å‹æƒé‡ã€ç”Ÿæˆæç¤ºæˆ–ä»»ä½•æ•æ„Ÿå†…éƒ¨ä¿¡æ¯çš„æƒ…å†µä¸‹ï¼Œå®ç°å¯éªŒè¯çš„èµ·æºè¯æ˜ã€‚æˆ‘ä»¬æå‡ºäº†é€‰æ‹©æ€§å±‚ZKç”µè·¯åˆ›å»ºï¼ˆSL-ZKCCï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰é€‰æ‹©åœ°å°†å›¾åƒç”Ÿæˆæ¨¡å‹çš„å…³é”®å±‚è½¬æ¢ä¸ºç”µè·¯ï¼Œä»è€Œæ˜¾è‘—å‡å°‘è¯æ˜ç”Ÿæˆæ—¶é—´ã€‚ç”Ÿæˆçš„ZK-SNARKè¯æ˜é€šè¿‡æœ€ä½æœ‰æ•ˆä½ï¼ˆLSBï¼‰éšå†™æœ¯åµŒå…¥åˆ°ç”Ÿæˆçš„å›¾åƒä¸­ï¼Œå‡ ä¹æ— æ³•è¢«å¯Ÿè§‰ã€‚æˆ‘ä»¬åœ¨ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰å’Œæ‰©æ•£æ¨¡å‹ä¸Šéƒ½å±•ç¤ºäº†è¿™ä¸€ç³»ç»Ÿï¼Œä¸ºå¯ä¿¡çš„AIå›¾åƒç”Ÿæˆæä¾›äº†ä¸€ä¸ªå®‰å…¨ã€æ¨¡å‹æ— å…³çš„æµç¨‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.01967v1">PDF</a> Accepted at AI-ML Systems 2025, Bangalore, India,   <a target="_blank" rel="noopener" href="https://www.aimlsystems.org/2025/">https://www.aimlsystems.org/2025/</a></p>
<p><strong>Summary</strong></p>
<p>å›¾åƒç”Ÿæˆæ¨¡å‹çš„åŠ›é‡æ—¥æ¸å¼ºå¤§ä¸æ™®åŠï¼Œå¼•å‘äº†å…³äºåˆæˆåª’ä½“çœŸå®æ€§ã€æ‰€æœ‰æƒåŠè¯¯ç”¨çš„å…³æ³¨ã€‚ç”Ÿæˆé€¼çœŸå›¾åƒçš„èƒ½åŠ›å¼•å…¥äº†è¯¸å¦‚é”™è¯¯ä¿¡æ¯ã€æ·±åº¦ä¼ªé€ å’ŒçŸ¥è¯†äº§æƒä¾µçŠ¯ç­‰é£é™©ã€‚ä¼ ç»Ÿæ°´å°æ–¹æ³•è¦ä¹ˆé™ä½å›¾åƒè´¨é‡ï¼Œè¦ä¹ˆæ˜“äºç§»é™¤ï¼Œè¦ä¹ˆéœ€è¦è®¿é—®ä¿å¯†æ¨¡å‹å†…éƒ¨ï¼Œå› æ­¤ä¸é€‚åˆå®‰å…¨å’Œå¯æ‰©å±•çš„éƒ¨ç½²ã€‚æˆ‘ä»¬é¦–æ¬¡å¼•å…¥ZK-WAGONï¼Œä¸€ç§ä½¿ç”¨é›¶çŸ¥è¯†ç®€æ´éäº¤äº’è®ºè¯çŸ¥è¯†ï¼ˆZK-SNARKsï¼‰ä¸ºå›¾åƒç”Ÿæˆæ¨¡å‹åŠ æ°´å°çš„æ–°å‹ç³»ç»Ÿã€‚æˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿåœ¨ä¸æš´éœ²æ¨¡å‹æƒé‡ã€ç”Ÿæˆæç¤ºæˆ–ä»»ä½•æ•æ„Ÿå†…éƒ¨ä¿¡æ¯çš„æƒ…å†µä¸‹ï¼Œè¿›è¡Œå¯éªŒè¯çš„èµ·æºè¯æ˜ã€‚æˆ‘ä»¬æå‡ºé€‰æ‹©æ€§å±‚ZKç”µè·¯åˆ›å»ºï¼ˆSL-ZKCCï¼‰ï¼Œä¸€ç§å°†å›¾åƒç”Ÿæˆæ¨¡å‹çš„å…³é”®å±‚é€‰æ‹©æ€§è½¬æ¢ä¸ºç”µè·¯çš„æ–¹æ³•ï¼Œæ˜¾è‘—å‡å°‘è¯æ˜ç”Ÿæˆæ—¶é—´ã€‚ç”Ÿæˆçš„ZK-SNARKè¯æ˜é€šè¿‡æœ€ä½æœ‰æ•ˆä½ï¼ˆLSBï¼‰éšå†™æœ¯åµŒå…¥åˆ°ç”Ÿæˆçš„å›¾åƒä¸­ã€‚æˆ‘ä»¬åœ¨GANå’ŒDiffusionæ¨¡å‹ä¸Šå±•ç¤ºäº†è¿™ä¸€ç³»ç»Ÿï¼Œä¸ºå¯ä¿¡AIå›¾åƒç”Ÿæˆæä¾›äº†ä¸€ä¸ªå®‰å…¨ã€æ¨¡å‹æ— å…³çš„æµç¨‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å›¾åƒç”Ÿæˆæ¨¡å‹çš„å¼ºå¤§ä¸æ™®åŠå¸¦æ¥äº†å…³äºåˆæˆåª’ä½“çœŸå®æ€§ã€æ‰€æœ‰æƒå’Œè¯¯ç”¨çš„å…³é”®å…³æ³¨ç‚¹ã€‚</li>
<li>ç”Ÿæˆé€¼çœŸå›¾åƒçš„èƒ½åŠ›å¯èƒ½å¯¼è‡´é”™è¯¯ä¿¡æ¯ã€æ·±åº¦ä¼ªé€ å’ŒçŸ¥è¯†äº§æƒä¾µçŠ¯ç­‰é£é™©ã€‚</li>
<li>ä¼ ç»Ÿæ°´å°æ–¹æ³•å­˜åœ¨ç¼ºé™·ï¼Œä¸é€‚åˆç”¨äºå›¾åƒç”Ÿæˆæ¨¡å‹çš„éƒ¨ç½²ã€‚</li>
<li>ZK-WAGONç³»ç»Ÿåˆ©ç”¨ZK-SNARKsä¸ºå›¾åƒç”Ÿæˆæ¨¡å‹åŠ æ°´å°ï¼Œå®ç°å¯éªŒè¯çš„èµ·æºè¯æ˜ã€‚</li>
<li>ZK-WAGONæ–¹æ³•æ— éœ€æš´éœ²æ¨¡å‹æƒé‡ã€ç”Ÿæˆæç¤ºæˆ–æ•æ„Ÿå†…éƒ¨ä¿¡æ¯ã€‚</li>
<li>SL-ZKCCæ–¹æ³•èƒ½é€‰æ‹©æ€§è½¬æ¢å›¾åƒç”Ÿæˆæ¨¡å‹çš„å…³é”®å±‚ä¸ºç”µè·¯ï¼Œå‡å°‘è¯æ˜ç”Ÿæˆæ—¶é—´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.01967">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3c95d7808604a34bbe13c05652132f21" align="middle">
<img src="https://picx.zhimg.com/v2-97d43a2762892e8e021f104e56d3f04d" align="middle">
<img src="https://picx.zhimg.com/v2-75fa0e9a260478b1929ee6a63d84ad4d" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Leveraging-Prior-Knowledge-of-Diffusion-Model-for-Person-Search"><a href="#Leveraging-Prior-Knowledge-of-Diffusion-Model-for-Person-Search" class="headerlink" title="Leveraging Prior Knowledge of Diffusion Model for Person Search"></a>Leveraging Prior Knowledge of Diffusion Model for Person Search</h2><p><strong>Authors:Giyeol Kim, Sooyoung Yang, Jihyong Oh, Myungjoo Kang, Chanho Eom</strong></p>
<p>Person search aims to jointly perform person detection and re-identification by localizing and identifying a query person within a gallery of uncropped scene images. Existing methods predominantly utilize ImageNet pre-trained backbones, which may be suboptimal for capturing the complex spatial context and fine-grained identity cues necessary for person search. Moreover, they rely on a shared backbone feature for both person detection and re-identification, leading to suboptimal features due to conflicting optimization objectives. In this paper, we propose DiffPS (Diffusion Prior Knowledge for Person Search), a novel framework that leverages a pre-trained diffusion model while eliminating the optimization conflict between two sub-tasks. We analyze key properties of diffusion priors and propose three specialized modules: (i) Diffusion-Guided Region Proposal Network (DGRPN) for enhanced person localization, (ii) Multi-Scale Frequency Refinement Network (MSFRN) to mitigate shape bias, and (iii) Semantic-Adaptive Feature Aggregation Network (SFAN) to leverage text-aligned diffusion features. DiffPS sets a new state-of-the-art on CUHK-SYSU and PRW. </p>
<blockquote>
<p>äººç‰©æœç´¢æ—¨åœ¨åœ¨æœªè£å‰ªçš„åœºæ™¯å›¾åƒåº“ä¸­å®šä½å¹¶è¯†åˆ«æŸ¥è¯¢äººç‰©ï¼Œä»è€Œè”åˆæ‰§è¡Œäººç‰©æ£€æµ‹å’Œå†è¯†åˆ«ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦ä½¿ç”¨åœ¨ImageNetä¸Šé¢„è®­ç»ƒè¿‡çš„ä¸»å¹²ç½‘ç»œï¼Œè¿™å¯èƒ½ä¸åˆ©äºæ•æ‰äººç‰©æœç´¢æ‰€éœ€å¤æ‚çš„ç©ºé—´ä¸Šä¸‹æ–‡å’Œç²¾ç»†çš„èº«ä»½çº¿ç´¢ã€‚æ­¤å¤–ï¼Œå®ƒä»¬ä¾èµ–å…±äº«ä¸»å¹²ç‰¹å¾è¿›è¡Œäººç‰©æ£€æµ‹å’Œå†è¯†åˆ«ï¼Œç”±äºä¼˜åŒ–ç›®æ ‡ä¹‹é—´çš„å†²çªï¼Œå¯¼è‡´ç‰¹å¾æ¬¡ä¼˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†DiffPSï¼ˆç”¨äºäººç‰©æœç´¢çš„æ‰©æ•£å…ˆéªŒçŸ¥è¯†ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹çš„æ–°æ¡†æ¶ï¼ŒåŒæ—¶æ¶ˆé™¤äº†ä¸¤ä¸ªå­ä»»åŠ¡ä¹‹é—´çš„ä¼˜åŒ–å†²çªã€‚æˆ‘ä»¬åˆ†æäº†æ‰©æ•£å…ˆéªŒçš„å…³é”®å±æ€§ï¼Œå¹¶æå‡ºäº†ä¸‰ä¸ªä¸“ç”¨æ¨¡å—ï¼šï¼ˆiï¼‰æ‰©æ•£å¼•å¯¼åŒºåŸŸæè®®ç½‘ç»œï¼ˆDGRPNï¼‰ï¼Œç”¨äºå¢å¼ºäººç‰©å®šä½ï¼›ï¼ˆiiï¼‰å¤šå°ºåº¦é¢‘ç‡ç»†åŒ–ç½‘ç»œï¼ˆMSFRNï¼‰ï¼Œä»¥å‡è½»å½¢çŠ¶åè§ï¼›ï¼ˆiiiï¼‰è¯­ä¹‰è‡ªé€‚åº”ç‰¹å¾èšåˆç½‘ç»œï¼ˆSFANï¼‰ï¼Œä»¥åˆ©ç”¨æ–‡æœ¬å¯¹é½çš„æ‰©æ•£ç‰¹å¾ã€‚DiffPSåœ¨CUHK-SYSUå’ŒPRWä¸Šè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.01841v1">PDF</a> </p>
<p><strong>Summary</strong><br>æ‰©æ•£æ¨¡å‹å…ˆéªŒçŸ¥è¯†åœ¨äººç‰©æœç´¢ä¸­çš„åº”ç”¨ã€‚è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶DiffPSï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ï¼Œè§£å†³äº†äººç‰©æ£€æµ‹å’Œå†è¯†åˆ«ä¹‹é—´çš„ä¼˜åŒ–å†²çªé—®é¢˜ã€‚é€šè¿‡ä¸‰ä¸ªä¸“é—¨æ¨¡å—ï¼Œæé«˜äº†äººç‰©å®šä½çš„å‡†ç¡®æ€§ï¼Œç¼“è§£äº†å½¢çŠ¶åè§ï¼Œå¹¶å……åˆ†åˆ©ç”¨äº†æ–‡æœ¬å¯¹é½çš„æ‰©æ•£ç‰¹å¾ã€‚DiffPSåœ¨CUHK-SYSUå’ŒPRWæ•°æ®é›†ä¸Šå–å¾—äº†æœ€æ–° state-of-the-art çš„æˆç»©ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äººç‰©æœç´¢éœ€è¦åŒæ—¶è¿›è¡Œäººç‰©æ£€æµ‹å’Œå†è¯†åˆ«ï¼Œåœ¨ä¸€ä¸ªæœªè£å‰ªçš„åœºæ™¯å›¾åƒåº“ä¸­å®šä½å¹¶è¯†åˆ«æŸ¥è¯¢äººç‰©ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦ä½¿ç”¨ImageNeté¢„è®­ç»ƒéª¨å¹²ç½‘ï¼Œå¯èƒ½ä¸åˆ©äºæ•æ‰å¤æ‚çš„ç©ºé—´ä¸Šä¸‹æ–‡å’Œç²¾ç»†çš„èº«ä»½çº¿ç´¢ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä½¿ç”¨å…±äº«éª¨å¹²ç½‘ç‰¹å¾è¿›è¡Œäººç‰©æ£€æµ‹å’Œå†è¯†åˆ«ï¼Œå¯¼è‡´ç‰¹å¾å› ä¼˜åŒ–ç›®æ ‡å†²çªè€Œæ¬¡ä¼˜ã€‚</li>
<li>DiffPSæ¡†æ¶åˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ï¼Œè§£å†³äº†ä¸¤ä¸ªå­ä»»åŠ¡ä¹‹é—´çš„ä¼˜åŒ–å†²çªã€‚</li>
<li>DiffPSé€šè¿‡ä¸‰ä¸ªä¸“é—¨æ¨¡å—æé«˜äº†äººç‰©å®šä½çš„å‡†ç¡®æ€§ï¼Œç¼“è§£äº†å½¢çŠ¶åè§ï¼Œå¹¶å……åˆ†åˆ©ç”¨äº†æ–‡æœ¬å¯¹é½çš„æ‰©æ•£ç‰¹å¾ã€‚</li>
<li>DiffPSåœ¨CUHK-SYSUå’ŒPRWæ•°æ®é›†ä¸Šå–å¾—äº†æœ€æ–°çš„ state-of-the-art æˆç»©ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.01841">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8f4a06521d0a0c4592e73b3e5ee37633" align="middle">
<img src="https://picx.zhimg.com/v2-fb26a86451f2c90aee06601811c3de47" align="middle">
<img src="https://picx.zhimg.com/v2-cb3370b9694fef5bb90e662519e9eebe" align="middle">
<img src="https://picx.zhimg.com/v2-4fd7c05622c6bb8b0f574ec8ddaec0c9" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="UniVerse-Unleashing-the-Scene-Prior-of-Video-Diffusion-Models-for-Robust-Radiance-Field-Reconstruction"><a href="#UniVerse-Unleashing-the-Scene-Prior-of-Video-Diffusion-Models-for-Robust-Radiance-Field-Reconstruction" class="headerlink" title="UniVerse: Unleashing the Scene Prior of Video Diffusion Models for   Robust Radiance Field Reconstruction"></a>UniVerse: Unleashing the Scene Prior of Video Diffusion Models for   Robust Radiance Field Reconstruction</h2><p><strong>Authors:Jin Cao, Hongrui Wu, Ziyong Feng, Hujun Bao, Xiaowei Zhou, Sida Peng</strong></p>
<p>This paper tackles the challenge of robust reconstruction, i.e., the task of reconstructing a 3D scene from a set of inconsistent multi-view images. Some recent works have attempted to simultaneously remove image inconsistencies and perform reconstruction by integrating image degradation modeling into neural 3D scene representations.However, these methods rely heavily on dense observations for robustly optimizing model parameters.To address this issue, we propose to decouple robust reconstruction into two subtasks: restoration and reconstruction, which naturally simplifies the optimization process.To this end, we introduce UniVerse, a unified framework for robust reconstruction based on a video diffusion model. Specifically, UniVerse first converts inconsistent images into initial videos, then uses a specially designed video diffusion model to restore them into consistent images, and finally reconstructs the 3D scenes from these restored images.Compared with case-by-case per-view degradation modeling, the diffusion model learns a general scene prior from large-scale data, making it applicable to diverse image inconsistencies.Extensive experiments on both synthetic and real-world datasets demonstrate the strong generalization capability and superior performance of our method in robust reconstruction. Moreover, UniVerse can control the style of the reconstructed 3D scene. Project page: <a target="_blank" rel="noopener" href="https://jin-cao-tma.github.io/UniVerse.github.io/">https://jin-cao-tma.github.io/UniVerse.github.io/</a> </p>
<blockquote>
<p>æœ¬æ–‡é¢ä¸´ç¨³å¥é‡å»ºçš„æŒ‘æˆ˜ï¼Œå³å¦‚ä½•ä»ä¸€ç»„ä¸ä¸€è‡´çš„å¤šè§†è§’å›¾åƒé‡å»ºä¸€ä¸ªä¸‰ç»´åœºæ™¯ã€‚è¿‘æœŸçš„ä¸€äº›å·¥ä½œå°è¯•é€šè¿‡æ•´åˆå›¾åƒé€€åŒ–å»ºæ¨¡åˆ°ç¥ç»ä¸‰ç»´åœºæ™¯è¡¨ç¤ºä¸­ï¼ŒåŒæ—¶æ¶ˆé™¤å›¾åƒçš„ä¸ä¸€è‡´æ€§å’Œè¿›è¡Œé‡å»ºã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å¾ˆå¤§ç¨‹åº¦ä¸Šä¾èµ–äºå¯†é›†è§‚æµ‹æ¥ç¨³å¥åœ°ä¼˜åŒ–æ¨¡å‹å‚æ•°ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æè®®å°†ç¨³å¥é‡å»ºè§£è€¦ä¸ºä¸¤ä¸ªå­ä»»åŠ¡ï¼šæ¢å¤å’Œé‡å»ºï¼Œè¿™è‡ªç„¶åœ°ç®€åŒ–äº†ä¼˜åŒ–è¿‡ç¨‹ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºäºè§†é¢‘æ‰©æ•£æ¨¡å‹çš„ç¨³å¥é‡å»ºçš„ç»Ÿä¸€æ¡†æ¶UniVerseã€‚å…·ä½“æ¥è¯´ï¼ŒUniVerseé¦–å…ˆæŠŠä¸ä¸€è‡´çš„å›¾åƒè½¬æ¢æˆåˆå§‹è§†é¢‘ï¼Œç„¶åä½¿ç”¨ä¸“é—¨è®¾è®¡çš„è§†é¢‘æ‰©æ•£æ¨¡å‹å°†å®ƒä»¬æ¢å¤æˆä¸€è‡´å›¾åƒï¼Œæœ€åä»è¿™äº›æ¢å¤åçš„å›¾åƒé‡å»ºä¸‰ç»´åœºæ™¯ã€‚ä¸é€ä¸ªè§†å›¾è¿›è¡Œé€€åŒ–å»ºæ¨¡ç›¸æ¯”ï¼Œæ‰©æ•£æ¨¡å‹ä»å¤§è§„æ¨¡æ•°æ®ä¸­å­¦ä¹ äº†ä¸€èˆ¬åœºæ™¯å…ˆéªŒï¼Œä½¿å…¶é€‚ç”¨äºå¤šç§å›¾åƒä¸ä¸€è‡´æ€§ã€‚åœ¨åˆæˆå’ŒçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¯æ˜äº†æˆ‘ä»¬æ–¹æ³•åœ¨ç¨³å¥é‡å»ºä¸­çš„å¼ºå¤§é€šç”¨æ€§å’Œå“è¶Šæ€§èƒ½ã€‚æ­¤å¤–ï¼ŒUniVerseå¯ä»¥æ§åˆ¶é‡å»ºçš„ä¸‰ç»´åœºæ™¯çš„é£æ ¼ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://jin-cao-tma.github.io/UniVerse.github.io/%E3%80%82">https://jin-cao-tma.github.io/UniVerse.github.io/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.01669v1">PDF</a> page: <a target="_blank" rel="noopener" href="https://jin-cao-tma.github.io/UniVerse.github.io/">https://jin-cao-tma.github.io/UniVerse.github.io/</a> code:   <a target="_blank" rel="noopener" href="https://github.com/zju3dv/UniVerse">https://github.com/zju3dv/UniVerse</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºè§†é¢‘æ‰©æ•£æ¨¡å‹çš„ç»Ÿä¸€æ¡†æ¶UniVerseï¼Œç”¨äºä»å¤šè§†è§’çš„ä¸ä¸€è‡´å›¾åƒä¸­é‡å»º3Dåœºæ™¯ã€‚è¯¥æ¡†æ¶é€šè¿‡å°†æ¢å¤å’Œé‡å»ºä»»åŠ¡è§£è€¦ï¼Œç®€åŒ–äº†ä¼˜åŒ–è¿‡ç¨‹ã€‚å®ƒé€šè¿‡è½¬æ¢ä¸ä¸€è‡´å›¾åƒä¸ºåˆå§‹è§†é¢‘ï¼Œä½¿ç”¨ä¸“é—¨è®¾è®¡çš„è§†é¢‘æ‰©æ•£æ¨¡å‹è¿›è¡Œå›¾åƒæ¢å¤ï¼Œå¹¶ä»æ¢å¤çš„å›¾åƒä¸­é‡å»º3Dåœºæ™¯ã€‚ç›¸è¾ƒäºé’ˆå¯¹ä¸ªæ¡ˆçš„æ¯è§†è§’é€€åŒ–å»ºæ¨¡ï¼Œæ‰©æ•£æ¨¡å‹ä»å¤§è§„æ¨¡æ•°æ®ä¸­å­¦ä¹ åœºæ™¯çš„ä¸€èˆ¬å…ˆéªŒï¼Œé€‚ç”¨äºå¤šç§å›¾åƒä¸ä¸€è‡´æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é²æ£’é‡å»ºä¸­å…·æœ‰å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›å’Œä¼˜è¶Šæ€§èƒ½ï¼Œå¹¶å¯æ§åˆ¶é‡å»ºçš„3Dåœºæ™¯çš„é£æ ¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥è®ºæ–‡è§£å†³äº†ä»å¤šè§†è§’ä¸ä¸€è‡´å›¾åƒä¸­é²æ£’é‡å»º3Dåœºæ™¯çš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºè§†é¢‘æ‰©æ•£æ¨¡å‹çš„ç»Ÿä¸€æ¡†æ¶UniVerseã€‚</li>
<li>UniVerseé€šè¿‡è§£è€¦æ¢å¤å’Œé‡å»ºä»»åŠ¡ï¼Œç®€åŒ–äº†ä¼˜åŒ–è¿‡ç¨‹ã€‚</li>
<li>ä½¿ç”¨è§†é¢‘æ‰©æ•£æ¨¡å‹ä»ä¸ä¸€è‡´å›¾åƒä¸­æ¢å¤å›¾åƒã€‚</li>
<li>æ‰©æ•£æ¨¡å‹èƒ½ä»å¤§è§„æ¨¡æ•°æ®ä¸­å­¦ä¹ åœºæ™¯çš„ä¸€èˆ¬å…ˆéªŒï¼Œé€‚ç”¨äºå¤šç§å›¾åƒä¸ä¸€è‡´æ€§ã€‚</li>
<li>å®éªŒè¯æ˜UniVerseåœ¨é²æ£’é‡å»ºä¸­å…·æœ‰å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›å’Œä¼˜è¶Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.01669">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-65d372703458d0b113068ee810f93461" align="middle">
<img src="https://picx.zhimg.com/v2-5501dfb87dc50804467653c7e96b4c1c" align="middle">
<img src="https://picx.zhimg.com/v2-ab4ce4622f5a6100842c5ee32d7cbe5e" align="middle">
<img src="https://picx.zhimg.com/v2-a4e918d3000208f12a9d78ed9099a266" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="FideDiff-Efficient-Diffusion-Model-for-High-Fidelity-Image-Motion-Deblurring"><a href="#FideDiff-Efficient-Diffusion-Model-for-High-Fidelity-Image-Motion-Deblurring" class="headerlink" title="FideDiff: Efficient Diffusion Model for High-Fidelity Image Motion   Deblurring"></a>FideDiff: Efficient Diffusion Model for High-Fidelity Image Motion   Deblurring</h2><p><strong>Authors:Xiaoyang Liu, Zhengyan Zhou, Zihang Xu, Jiezhang Cao, Zheng Chen, Yulun Zhang</strong></p>
<p>Recent advancements in image motion deblurring, driven by CNNs and transformers, have made significant progress. Large-scale pre-trained diffusion models, which are rich in true-world modeling, have shown great promise for high-quality image restoration tasks such as deblurring, demonstrating stronger generative capabilities than CNN and transformer-based methods. However, challenges such as unbearable inference time and compromised fidelity still limit the full potential of the diffusion models. To address this, we introduce FideDiff, a novel single-step diffusion model designed for high-fidelity deblurring. We reformulate motion deblurring as a diffusion-like process where each timestep represents a progressively blurred image, and we train a consistency model that aligns all timesteps to the same clean image. By reconstructing training data with matched blur trajectories, the model learns temporal consistency, enabling accurate one-step deblurring. We further enhance model performance by integrating Kernel ControlNet for blur kernel estimation and introducing adaptive timestep prediction. Our model achieves superior performance on full-reference metrics, surpassing previous diffusion-based methods and matching the performance of other state-of-the-art models. FideDiff offers a new direction for applying pre-trained diffusion models to high-fidelity image restoration tasks, establishing a robust baseline for further advancing diffusion models in real-world industrial applications. Our dataset and code will be available at <a target="_blank" rel="noopener" href="https://github.com/xyLiu339/FideDiff">https://github.com/xyLiu339/FideDiff</a>. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œç”±å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å’Œå˜å‹å™¨é©±åŠ¨çš„å›¾åƒè¿åŠ¨å»æ¨¡ç³ŠæŠ€æœ¯å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚å¤§è§„æ¨¡é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹åœ¨çœŸå®ä¸–ç•Œå»ºæ¨¡æ–¹é¢è¡¨ç°å‡ºä¸°å¯Œçš„ç‰¹æ€§ï¼Œå¯¹äºé«˜è´¨é‡å›¾åƒæ¢å¤ä»»åŠ¡ï¼ˆå¦‚å»æ¨¡ç³Šï¼‰å…·æœ‰å·¨å¤§çš„æ½œåŠ›ï¼Œæ˜¾ç¤ºå‡ºæ¯”CNNå’ŒåŸºäºå˜å‹å™¨çš„æ–¹æ³•æ›´å¼ºçš„ç”Ÿæˆèƒ½åŠ›ã€‚ç„¶è€Œï¼ŒæŒ‘æˆ˜ä»ç„¶å­˜åœ¨ï¼Œä¾‹å¦‚éš¾ä»¥å¿å—çš„æ¨ç†æ—¶é—´ä»¥åŠä¿çœŸåº¦çš„å¦¥åä»ç„¶é™åˆ¶äº†æ‰©æ•£æ¨¡å‹çš„å…¨éƒ¨æ½œåŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†FideDiffï¼Œè¿™æ˜¯ä¸€ç§ä¸ºé«˜æ€§èƒ½å»æ¨¡ç³Šè€Œè®¾è®¡çš„æ–°å‹å•æ­¥æ‰©æ•£æ¨¡å‹ã€‚æˆ‘ä»¬å°†è¿åŠ¨å»æ¨¡ç³Šé‡æ–°è¡¨è¿°ä¸ºç±»ä¼¼æ‰©æ•£çš„è¿‡ç¨‹ï¼Œå…¶ä¸­æ¯ä¸ªæ—¶é—´æ­¥ä»£è¡¨ä¸€ä¸ªé€æ¸æ¨¡ç³Šçš„å›¾åƒï¼Œå¹¶è®­ç»ƒäº†ä¸€ä¸ªä¸€è‡´æ€§æ¨¡å‹ï¼Œå°†æ‰€æœ‰æ—¶é—´æ­¥å¯¹é½åˆ°åŒä¸€æ¸…æ´å›¾åƒã€‚é€šè¿‡ç”¨åŒ¹é…çš„æ¨¡ç³Šè½¨è¿¹é‡å»ºè®­ç»ƒæ•°æ®ï¼Œæ¨¡å‹å­¦ä¹ æ—¶é—´ä¸€è‡´æ€§ï¼Œä»è€Œå®ç°äº†ä¸€æ­¥å»æ¨¡ç³Šã€‚æˆ‘ä»¬è¿›ä¸€æ­¥é€šè¿‡æ•´åˆKernel ControlNetè¿›è¡Œæ¨¡ç³Šæ ¸ä¼°è®¡å’Œå¼•å…¥è‡ªé€‚åº”æ—¶é—´æ­¥é¢„æµ‹ï¼Œå¢å¼ºäº†æ¨¡å‹æ€§èƒ½ã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨å…¨å‚è€ƒæŒ‡æ ‡ä¸Šå®ç°äº†å“è¶Šçš„æ€§èƒ½ï¼Œè¶…è¶Šäº†ä¹‹å‰çš„åŸºäºæ‰©æ•£çš„æ–¹æ³•ï¼Œå¹¶ä¸å…¶ä»–æœ€å…ˆè¿›çš„æ¨¡å‹æ€§èƒ½ç›¸åŒ¹é…ã€‚FideDiffä¸ºå°†é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹åº”ç”¨äºé«˜è´¨é‡å›¾åƒæ¢å¤ä»»åŠ¡æä¾›äº†æ–°çš„æ–¹å‘ï¼Œä¸ºåœ¨ç°å®ä¸–ç•Œå·¥ä¸šåº”ç”¨ä¸­è¿›ä¸€æ­¥æ¨è¿›æ‰©æ•£æ¨¡å‹å»ºç«‹äº†ç¨³å¥çš„åŸºçº¿ã€‚æˆ‘ä»¬çš„æ•°æ®é›†å’Œä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/xyLiu339/FideDiff%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/xyLiu339/FideDiffä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.01641v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸï¼ŒåŸºäºCNNå’Œtransformerçš„å›¾åƒè¿åŠ¨å»æ¨¡ç³ŠæŠ€æœ¯å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚å¤§å‹é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹åœ¨é«˜ä¿çœŸå›¾åƒæ¢å¤ä»»åŠ¡ä¸­å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œå¦‚å»æ¨¡ç³Šã€‚ç„¶è€Œï¼Œå­˜åœ¨æ¨ç†æ—¶é—´è¿‡é•¿å’Œä¿çœŸåº¦ä¸è¶³ç­‰æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºFideDiffï¼Œä¸€ç§ç”¨äºé«˜ä¿çœŸå»æ¨¡ç³Šçš„å•æ­¥æ‰©æ•£æ¨¡å‹ã€‚è¯¥æ¨¡å‹å°†è¿åŠ¨å»æ¨¡ç³Šé‡æ–°æ„å»ºä¸ºæ‰©æ•£è¿‡ç¨‹ï¼Œæ¯ä¸ªæ—¶é—´æ­¥ä»£è¡¨ä¸€å¼ é€æ¸æ¨¡ç³Šçš„å›¾ç‰‡ï¼Œå¹¶è®­ç»ƒä¸€è‡´æ€§æ¨¡å‹å°†æ‰€æœ‰æ—¶é—´æ­¥å¯¹é½åˆ°åŒä¸€æ¸…æ™°å›¾åƒã€‚é€šè¿‡é‡å»ºå…·æœ‰åŒ¹é…æ¨¡ç³Šè½¨è¿¹çš„è®­ç»ƒæ•°æ®ï¼Œæ¨¡å‹å­¦ä¹ æ—¶é—´ä¸€è‡´æ€§ï¼Œå®ç°ä¸€æ¬¡å»æ¨¡ç³Šã€‚é€šè¿‡é›†æˆKernel ControlNetè¿›è¡Œæ¨¡ç³Šæ ¸ä¼°è®¡å’Œå¼•å…¥è‡ªé€‚åº”æ—¶é—´æ­¥é¢„æµ‹ï¼Œæ¨¡å‹æ€§èƒ½è¿›ä¸€æ­¥æå‡ã€‚FideDiffåœ¨å‚è€ƒæŒ‡æ ‡ä¸Šå®ç°å“è¶Šæ€§èƒ½ï¼Œè¶…è¶Šå…ˆå‰çš„æ‰©æ•£æ–¹æ³•ï¼Œå¹¶ä¸å…¶ä»–é¡¶å°–æ¨¡å‹æ€§èƒ½ç›¸åŒ¹é…ã€‚å®ƒä¸ºå°†é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹åº”ç”¨äºé«˜ä¿çœŸå›¾åƒæ¢å¤ä»»åŠ¡æä¾›äº†æ–°æ–¹å‘ï¼Œä¸ºæ‰©æ•£æ¨¡å‹åœ¨ç°å®ä¸–ç•Œå·¥ä¸šåº”ç”¨ä¸­çš„è¿›ä¸€æ­¥å‘å±•å»ºç«‹äº†ç¨³å¥åŸºå‡†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹åœ¨é«˜ä¿çœŸå›¾åƒæ¢å¤ä»»åŠ¡ä¸­è¡¨ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹åœ¨å¤„ç†å›¾åƒå»æ¨¡ç³Šä»»åŠ¡æ—¶å­˜åœ¨æ¨ç†æ—¶é—´é•¿å’Œä¿çœŸåº¦ä¸è¶³çš„æŒ‘æˆ˜ã€‚</li>
<li>FideDiffæ˜¯ä¸€ç§æ–°å‹å•æ­¥æ‰©æ•£æ¨¡å‹ï¼Œç”¨äºé«˜ä¿çœŸå»æ¨¡ç³Šï¼Œå°†è¿åŠ¨å»æ¨¡ç³Šé‡æ–°æ„å»ºä¸ºæ‰©æ•£è¿‡ç¨‹ã€‚</li>
<li>FideDiffé€šè¿‡è®­ç»ƒä¸€è‡´æ€§æ¨¡å‹ï¼Œä½¿æ‰€æœ‰æ—¶é—´æ­¥å¯¹é½åˆ°åŒä¸€æ¸…æ™°å›¾åƒï¼Œå®ç°ä¸€æ¬¡å»æ¨¡ç³Šã€‚</li>
<li>FideDiffé›†æˆäº†Kernel ControlNetè¿›è¡Œæ¨¡ç³Šæ ¸ä¼°è®¡ï¼Œå¹¶å¼•å…¥è‡ªé€‚åº”æ—¶é—´æ­¥é¢„æµ‹ï¼Œè¿›ä¸€æ­¥æå‡äº†æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>FideDiffåœ¨å‚è€ƒæŒ‡æ ‡ä¸Šå®ç°äº†å“è¶Šæ€§èƒ½ï¼Œè¶…è¶Šäº†å…ˆå‰çš„æ‰©æ•£æ–¹æ³•ï¼Œå¹¶ä¸å…¶ä»–é¡¶å°–æ¨¡å‹ç›¸å½“ã€‚</li>
<li>FideDiffä¸ºå°†é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹åº”ç”¨äºé«˜ä¿çœŸå›¾åƒæ¢å¤ä»»åŠ¡æä¾›äº†æ–°æ–¹å‘ï¼Œå¹¶ä¸ºæ‰©æ•£æ¨¡å‹åœ¨ç°å®ä¸–ç•Œå·¥ä¸šåº”ç”¨ä¸­çš„è¿›ä¸€æ­¥å‘å±•å»ºç«‹äº†åŸºå‡†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.01641">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ad443cce71103cffc6b9d92514b69019" align="middle">
<img src="https://picx.zhimg.com/v2-fce960a9f7224196089bb47a13b26145" align="middle">
<img src="https://picx.zhimg.com/v2-d4ee87c5f4e21d4afd8b36158154a1f8" align="middle">
<img src="https://picx.zhimg.com/v2-c52789eb17dcf2b65c5ff87be2c986cc" align="middle">
<img src="https://picx.zhimg.com/v2-fa3b8d57fddb39185bbd9c09c2b83575" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="NPN-Non-Linear-Projections-of-the-Null-Space-for-Imaging-Inverse-Problems"><a href="#NPN-Non-Linear-Projections-of-the-Null-Space-for-Imaging-Inverse-Problems" class="headerlink" title="NPN: Non-Linear Projections of the Null-Space for Imaging Inverse   Problems"></a>NPN: Non-Linear Projections of the Null-Space for Imaging Inverse   Problems</h2><p><strong>Authors:Roman Jacome, Romario GualdrÃ³n-Hurtado, Leon Suarez, Henry Arguello</strong></p>
<p>Imaging inverse problems aims to recover high-dimensional signals from undersampled, noisy measurements, a fundamentally ill-posed task with infinite solutions in the null-space of the sensing operator. To resolve this ambiguity, prior information is typically incorporated through handcrafted regularizers or learned models that constrain the solution space. However, these priors typically ignore the task-specific structure of that null-space. In this work, we propose \textit{Non-Linear Projections of the Null-Space} (NPN), a novel class of regularization that, instead of enforcing structural constraints in the image domain, promotes solutions that lie in a low-dimensional projection of the sensing matrixâ€™s null-space with a neural network. Our approach has two key advantages: (1) Interpretability: by focusing on the structure of the null-space, we design sensing-matrix-specific priors that capture information orthogonal to the signal components that are fundamentally blind to the sensing process. (2) Flexibility: NPN is adaptable to various inverse problems, compatible with existing reconstruction frameworks, and complementary to conventional image-domain priors. We provide theoretical guarantees on convergence and reconstruction accuracy when used within plug-and-play methods. Empirical results across diverse sensing matrices demonstrate that NPN priors consistently enhance reconstruction fidelity in various imaging inverse problems, such as compressive sensing, deblurring, super-resolution, computed tomography, and magnetic resonance imaging, with plug-and-play methods, unrolling networks, deep image prior, and diffusion models. </p>
<blockquote>
<p>æˆåƒåé—®é¢˜æ—¨åœ¨ä»æ¬ é‡‡æ ·ã€å˜ˆæ‚çš„æµ‹é‡ä¸­æ¢å¤é«˜ç»´ä¿¡å·ï¼Œè¿™æ˜¯ä¸€ä¸ªæ ¹æœ¬ä¸Šçš„ä¸é€‚å®šä»»åŠ¡ï¼Œå…¶è§£ç©ºé—´åœ¨æ„ŸçŸ¥ç®—å­çš„é›¶ç©ºé—´ä¸­å­˜åœ¨æ— é™è§£ã€‚ä¸ºäº†è§£å†³è¿™ç§æ¨¡ç³Šæ€§ï¼Œé€šå¸¸é€šè¿‡æ‰‹å·¥æ­£åˆ™åŒ–å™¨æˆ–å­¦ä¹ æ¨¡å‹æ¥èå…¥å…ˆéªŒä¿¡æ¯ï¼Œä»¥çº¦æŸè§£ç©ºé—´ã€‚ç„¶è€Œï¼Œè¿™äº›å…ˆéªŒé€šå¸¸å¿½ç•¥äº†ä»»åŠ¡ç‰¹å®šç»“æ„çš„é›¶ç©ºé—´ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†â€œé›¶ç©ºé—´éçº¿æ€§æŠ•å½±â€ï¼ˆNPNï¼‰ï¼Œè¿™æ˜¯ä¸€ç±»æ–°å‹æ­£åˆ™åŒ–æ–¹æ³•ï¼Œå®ƒä¸åœ¨å›¾åƒåŸŸä¸­å¼ºåˆ¶ç»“æ„çº¦æŸï¼Œè€Œæ˜¯é€šè¿‡åœ¨ç¥ç»ç½‘ç»œä¸­ä½¿ç”¨æ„ŸçŸ¥çŸ©é˜µé›¶ç©ºé—´çš„ä¸€ä¸ªä½ç»´æŠ•å½±æ¥ä¼˜åŒ–è§£ã€‚æˆ‘ä»¬çš„æ–¹æ³•æœ‰ä¸¤ä¸ªä¸»è¦ä¼˜ç‚¹ï¼šï¼ˆ1ï¼‰å¯è§£é‡Šæ€§ï¼šé€šè¿‡å…³æ³¨é›¶ç©ºé—´çš„ç»“æ„ï¼Œæˆ‘ä»¬è®¾è®¡äº†é’ˆå¯¹æ„ŸçŸ¥çŸ©é˜µçš„ç‰¹å®šå…ˆéªŒï¼Œè¿™äº›å…ˆéªŒä¿¡æ¯æ•æ‰åˆ°äº†ä¸ä¿¡å·æˆåˆ†æ­£äº¤çš„ä¿¡æ¯ï¼Œè¿™äº›ä¿¡å·æˆåˆ†å¯¹æ„ŸçŸ¥è¿‡ç¨‹æœ¬è´¨ä¸Šæ˜¯ç›²æ€§çš„ã€‚ï¼ˆ2ï¼‰çµæ´»æ€§ï¼šNPNèƒ½å¤Ÿé€‚åº”å„ç§åé—®é¢˜ï¼Œä¸ç°æœ‰çš„é‡å»ºæ¡†æ¶å…¼å®¹ï¼Œå¹¶ä¸”æ˜¯ä¼ ç»Ÿå›¾åƒåŸŸå…ˆéªŒçš„è¡¥å……ã€‚å½“ç”¨äºæ’ä»¶å’Œæ’­æ”¾æ–¹æ³•æ—¶ï¼Œæˆ‘ä»¬æä¾›äº†å…³äºæ”¶æ•›å’Œé‡å»ºå‡†ç¡®æ€§çš„ç†è®ºä¿è¯ã€‚åœ¨å¤šç§æ„ŸçŸ¥çŸ©é˜µä¸Šçš„å®è¯ç»“æœè¡¨æ˜ï¼ŒNPNå…ˆéªŒåœ¨å¤šç§æˆåƒåé—®é¢˜ä¸­ä¸€è‡´åœ°æé«˜äº†é‡å»ºä¿çœŸåº¦ï¼Œå¦‚å‹ç¼©æ„ŸçŸ¥ã€å»æ¨¡ç³Šã€è¶…åˆ†è¾¨ç‡ã€è®¡ç®—æœºæ–­å±‚æ‰«æå’Œç£å…±æŒ¯æˆåƒç­‰ï¼Œé€‚ç”¨äºæ’ä»¶å’Œæ’­æ”¾æ–¹æ³•ã€å±•å¼€ç½‘ç»œã€æ·±åº¦å›¾åƒå…ˆéªŒå’Œæ‰©æ•£æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.01608v1">PDF</a> 25 pages, 12 tables, 10 figures. Accepted to NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºâ€œéçº¿æ€§çš„é›¶ç©ºé—´æŠ•å½±â€ï¼ˆNPNï¼‰çš„æ–°å‹æ­£åˆ™åŒ–æ–¹æ³•ï¼Œç”¨äºè§£å†³æˆåƒåé—®é¢˜ã€‚è¯¥æ–¹æ³•ä¸“æ³¨äºé›¶ç©ºé—´çš„ç»“æ„ï¼Œé€šè¿‡ç¥ç»ç½‘ç»œåœ¨ä½ç»´æŠ•å½±ä¸­å¯»æ‰¾è§£å†³æ–¹æ¡ˆï¼Œä»è€Œæé«˜äº†è§£çš„å¯è§£é‡Šæ€§å’Œçµæ´»æ€§ã€‚åœ¨å¤šç§æˆåƒåé—®é¢˜ä¸­ï¼ŒNPNå…ˆéªŒå¯å¢å¼ºé‡å»ºçš„ä¿çœŸåº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æˆåƒåé—®é¢˜æ—¨åœ¨ä»æ¬ é‡‡æ ·ã€å˜ˆæ‚çš„æµ‹é‡ä¸­æ¢å¤é«˜ç»´ä¿¡å·ï¼Œè¿™æ˜¯ä¸€ä¸ªæ ¹æœ¬ä¸Šçš„ä¸é€‚å®šä»»åŠ¡ã€‚</li>
<li>ä¼ ç»Ÿçš„å…ˆéªŒä¿¡æ¯é€šå¸¸é€šè¿‡æ‰‹å·¥æ­£åˆ™åŒ–å™¨æˆ–å­¦ä¹ æ¨¡å‹æ¥çº¦æŸè§£ç©ºé—´ï¼Œä½†å¾€å¾€å¿½ç•¥äº†ä»»åŠ¡ç‰¹å®šçš„ç»“æ„ã€‚</li>
<li>NPNæ–¹æ³•ä¸“æ³¨äºé›¶ç©ºé—´çš„ç»“æ„ï¼Œè®¾è®¡æ„Ÿåº”çŸ©é˜µç‰¹å®šçš„å…ˆéªŒï¼Œæ•è·ä¸ä¿¡å·æˆåˆ†æ­£äº¤çš„ä¿¡æ¯ã€‚</li>
<li>NPNæ–¹æ³•é€šè¿‡ç¥ç»ç½‘ç»œåœ¨ä½ç»´æŠ•å½±ä¸­å¯»æ‰¾è§£å†³æ–¹æ¡ˆï¼Œæé«˜äº†è§£å†³æ–¹æ¡ˆçš„å¯è§£é‡Šæ€§å’Œçµæ´»æ€§ã€‚</li>
<li>NPNå…ˆéªŒåœ¨å„ç§æˆåƒåé—®é¢˜ä¸­éƒ½èƒ½æé«˜é‡å»ºçš„ä¿çœŸåº¦ï¼Œå¦‚å‹ç¼©æ„ŸçŸ¥ã€å»æ¨¡ç³Šã€è¶…åˆ†è¾¨ç‡ã€è®¡ç®—æœºæ–­å±‚æ‰«æå’Œç£å…±æŒ¯æˆåƒç­‰ã€‚</li>
<li>NPNæ–¹æ³•å¯ä¸ç°æœ‰çš„é‡å»ºæ¡†æ¶äº’è¡¥ï¼Œé€‚ç”¨äºå¤šç§é€†é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.01608">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-04c7ec5cf032affab78d04563a7967a4" align="middle">
<img src="https://picx.zhimg.com/v2-7b759a443f7a05400992db6fe113bc84" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Towards-Better-Optimization-For-Listwise-Preference-in-Diffusion-Models"><a href="#Towards-Better-Optimization-For-Listwise-Preference-in-Diffusion-Models" class="headerlink" title="Towards Better Optimization For Listwise Preference in Diffusion Models"></a>Towards Better Optimization For Listwise Preference in Diffusion Models</h2><p><strong>Authors:Jiamu Bai, Xin Yu, Meilong Xu, Weitao Lu, Xin Pan, Kiwan Maeng, Daniel Kifer, Jian Wang, Yu Wang</strong></p>
<p>Reinforcement learning from human feedback (RLHF) has proven effectiveness for aligning text-to-image (T2I) diffusion models with human preferences. Although Direct Preference Optimization (DPO) is widely adopted for its computational efficiency and avoidance of explicit reward modeling, its applications to diffusion models have primarily relied on pairwise preferences. The precise optimization of listwise preferences remains largely unaddressed. In practice, human feedback on image preferences often contains implicit ranked information, which conveys more precise human preferences than pairwise comparisons. In this work, we propose Diffusion-LPO, a simple and effective framework for Listwise Preference Optimization in diffusion models with listwise data. Given a caption, we aggregate user feedback into a ranked list of images and derive a listwise extension of the DPO objective under the Plackett-Luce model. Diffusion-LPO enforces consistency across the entire ranking by encouraging each sample to be preferred over all of its lower-ranked alternatives. We empirically demonstrate the effectiveness of Diffusion-LPO across various tasks, including text-to-image generation, image editing, and personalized preference alignment. Diffusion-LPO consistently outperforms pairwise DPO baselines on visual quality and preference alignment. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰å·²ç»è¯æ˜åœ¨ä½¿æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹ä¸äººç±»åå¥½å¯¹é½æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚å°½ç®¡Direct Preference Optimizationï¼ˆDPOï¼‰å› å…¶è®¡ç®—æ•ˆç‡é«˜ä¸”é¿å…äº†æ˜¾å¼å¥–åŠ±å»ºæ¨¡è€Œå¾—åˆ°å¹¿æ³›åº”ç”¨ï¼Œä½†å…¶åœ¨æ‰©æ•£æ¨¡å‹ä¸­çš„åº”ç”¨ä¸»è¦ä¾èµ–äºæˆå¯¹åå¥½ã€‚å¯¹åˆ—è¡¨çº§åå¥½çš„ç²¾ç¡®ä¼˜åŒ–ä»ç„¶æœªå¾—åˆ°å¾ˆå¥½çš„è§£å†³ã€‚åœ¨å®è·µä¸­ï¼Œäººç±»å¯¹å›¾åƒåå¥½çš„åé¦ˆé€šå¸¸åŒ…å«éšæ€§çš„æ’åä¿¡æ¯ï¼Œè¿™ä¼ è¾¾äº†æ¯”æˆå¯¹æ¯”è¾ƒæ›´ç²¾ç¡®çš„äººç±»åå¥½ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Diffusion-LPOï¼Œè¿™æ˜¯ä¸€ä¸ªåœ¨å¸¦æœ‰åˆ—è¡¨æ•°æ®çš„æ‰©æ•£æ¨¡å‹ä¸­ç”¨äºåˆ—è¡¨çº§åå¥½ä¼˜åŒ–çš„ç®€å•æœ‰æ•ˆçš„æ¡†æ¶ã€‚ç»™å®šä¸€ä¸ªæ ‡é¢˜ï¼Œæˆ‘ä»¬å°†ç”¨æˆ·åé¦ˆèšåˆä¸ºå›¾åƒæ’ååˆ—è¡¨ï¼Œå¹¶åœ¨Plackett-Luceæ¨¡å‹ä¸‹æ¨å¯¼å‡ºDPOç›®æ ‡çš„åˆ—è¡¨çº§æ‰©å±•ã€‚Diffusion-LPOé€šè¿‡é¼“åŠ±æ¯ä¸ªæ ·æœ¬åœ¨æ‰€æœ‰æ’åè¾ƒä½çš„æ›¿ä»£å“ä¸­ä¿æŒä¼˜å…ˆï¼Œä»è€Œåœ¨æ•´ä¸ªæ’åä¸­å¼ºåˆ¶æ‰§è¡Œä¸€è‡´æ€§ã€‚æˆ‘ä»¬åœ¨å„ç§ä»»åŠ¡ä¸Šå®è¯äº†Diffusion-LPOçš„æœ‰æ•ˆæ€§ï¼ŒåŒ…æ‹¬æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆã€å›¾åƒç¼–è¾‘å’Œä¸ªäººåå¥½å¯¹é½ã€‚Diffusion-LPOåœ¨è§†è§‰è´¨é‡å’Œåå¥½å¯¹é½æ–¹é¢å§‹ç»ˆä¼˜äºæˆå¯¹çš„DPOåŸºå‡†æµ‹è¯•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.01540v1">PDF</a> </p>
<p><strong>Summary</strong><br>     åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ å·²è¯æ˜åœ¨è°ƒæ•´æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹ä»¥ç¬¦åˆäººç±»åå¥½æ–¹é¢éå¸¸æœ‰æ•ˆã€‚å°½ç®¡ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰å› å…¶è®¡ç®—æ•ˆç‡å’Œé«˜é¿å…æ˜¾å¼å¥–åŠ±å»ºæ¨¡çš„é€‚ç”¨æ€§è€Œå—åˆ°å¹¿æ³›é‡‡ç”¨ï¼Œä½†å…¶å¯¹æ‰©æ•£æ¨¡å‹çš„åº”ç”¨ä¸»è¦ä¾èµ–äºæˆå¯¹åå¥½ã€‚åˆ—è¡¨åå¥½çš„ç²¾ç¡®ä¼˜åŒ–ä»æœªå¾—åˆ°å¹¿æ³›ç ”ç©¶ã€‚åœ¨å®è·µä¸­ï¼Œå…³äºå›¾åƒåå¥½çš„äººç±»åé¦ˆé€šå¸¸åŒ…å«éšå«çš„æ’åä¿¡æ¯ï¼Œè¿™æ¯”æˆå¯¹æ¯”è¾ƒæ›´èƒ½å‡†ç¡®åæ˜ äººç±»åå¥½ã€‚æœ¬å·¥ä½œæå‡ºäº†ä¸€ä¸ªç®€å•æœ‰æ•ˆçš„Diffusion-LPOæ¡†æ¶ï¼Œç”¨äºå¤„ç†æ‰©æ•£æ¨¡å‹ä¸­çš„åˆ—è¡¨åå¥½ä¼˜åŒ–é—®é¢˜ã€‚ç»™å®šæè¿°ï¼Œæˆ‘ä»¬æ•´åˆç”¨æˆ·åé¦ˆç”Ÿæˆå›¾åƒæ’ååˆ—è¡¨ï¼Œå¹¶åœ¨Plackett-Luceæ¨¡å‹ä¸‹æ¨å¯¼DPOç›®æ ‡çš„åˆ—è¡¨æ‰©å±•ã€‚Diffusion-LPOé€šè¿‡é¼“åŠ±æ¯ä¸ªæ ·æœ¬åœ¨æ‰€æœ‰æ’åè¾ƒä½çš„æ›¿ä»£å“ä¸­å æ®ä¼˜åŠ¿æ¥ç¡®ä¿æ•´ä¸ªæ’åçš„è¿è´¯æ€§ã€‚æˆ‘ä»¬å®è¯åœ°è¯æ˜äº†Diffusion-LPOåœ¨å„ç§ä»»åŠ¡ä¸Šçš„æœ‰æ•ˆæ€§ï¼ŒåŒ…æ‹¬æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆã€å›¾åƒç¼–è¾‘å’Œä¸ªæ€§åŒ–åå¥½å¯¹é½ã€‚Diffusion-LPOåœ¨è§†è§‰è´¨é‡å’Œåå¥½å¯¹é½æ–¹é¢å‡ä¼˜äºæˆå¯¹DPOåŸºå‡†æµ‹è¯•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰åœ¨æé«˜æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹ä¸äººç±»åå¥½å¯¹é½æ–¹é¢å±•ç°å‡ºæœ‰æ•ˆæ€§ã€‚</li>
<li>ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰å·²å¹¿æ³›åº”ç”¨äºæ‰©æ•£æ¨¡å‹ï¼Œä½†ä¸»è¦ä¾èµ–äºæˆå¯¹åå¥½ï¼Œå¿½è§†äº†åˆ—è¡¨åå¥½çš„ç²¾ç¡®ä¼˜åŒ–ã€‚</li>
<li>äººç±»åé¦ˆé€šå¸¸åŒ…å«éšå«çš„æ’åä¿¡æ¯ï¼Œèƒ½æ›´ç²¾ç¡®åœ°åæ˜ äººç±»åå¥½ã€‚</li>
<li>æœ¬ç ”ç©¶æå‡ºäº†Diffusion-LPOæ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨åˆ—è¡¨æ•°æ®åœ¨æ‰©æ•£æ¨¡å‹ä¸­è¿›è¡Œåˆ—è¡¨åå¥½ä¼˜åŒ–ã€‚</li>
<li>Diffusion-LPOé€šè¿‡é¼“åŠ±æ¯ä¸ªæ ·æœ¬åœ¨æ‰€æœ‰è¾ƒä½æ’åçš„æ›¿ä»£å“ä¸­å æ®ä¼˜åŠ¿ï¼Œç¡®ä¿æ’åçš„è¿è´¯æ€§ã€‚</li>
<li>Diffusion-LPOåœ¨å¤šç§ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼ŒåŒ…æ‹¬æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆã€å›¾åƒç¼–è¾‘å’Œä¸ªæ€§åŒ–åå¥½å¯¹é½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.01540">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-363c368ed37547908d525c67063f0ce4" align="middle">
<img src="https://picx.zhimg.com/v2-302a128e849f36b26045c531b66ab8ee" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="AortaDiff-A-Unified-Multitask-Diffusion-Framework-For-Contrast-Free-AAA-Imaging"><a href="#AortaDiff-A-Unified-Multitask-Diffusion-Framework-For-Contrast-Free-AAA-Imaging" class="headerlink" title="AortaDiff: A Unified Multitask Diffusion Framework For Contrast-Free AAA   Imaging"></a>AortaDiff: A Unified Multitask Diffusion Framework For Contrast-Free AAA   Imaging</h2><p><strong>Authors:Yuxuan Ou, Ning Bi, Jiazhen Pan, Jiancheng Yang, Boliang Yu, Usama Zidan, Regent Lee, Vicente Grau</strong></p>
<p>While contrast-enhanced CT (CECT) is standard for assessing abdominal aortic aneurysms (AAA), the required iodinated contrast agents pose significant risks, including nephrotoxicity, patient allergies, and environmental harm. To reduce contrast agent use, recent deep learning methods have focused on generating synthetic CECT from non-contrast CT (NCCT) scans. However, most adopt a multi-stage pipeline that first generates images and then performs segmentation, which leads to error accumulation and fails to leverage shared semantic and anatomical structures. To address this, we propose a unified deep learning framework that generates synthetic CECT images from NCCT scans while simultaneously segmenting the aortic lumen and thrombus. Our approach integrates conditional diffusion models (CDM) with multi-task learning, enabling end-to-end joint optimization of image synthesis and anatomical segmentation. Unlike previous multitask diffusion models, our approach requires no initial predictions (e.g., a coarse segmentation mask), shares both encoder and decoder parameters across tasks, and employs a semi-supervised training strategy to learn from scans with missing segmentation labels, a common constraint in real-world clinical data. We evaluated our method on a cohort of 264 patients, where it consistently outperformed state-of-the-art single-task and multi-stage models. For image synthesis, our model achieved a PSNR of 25.61 dB, compared to 23.80 dB from a single-task CDM. For anatomical segmentation, it improved the lumen Dice score to 0.89 from 0.87 and the challenging thrombus Dice score to 0.53 from 0.48 (nnU-Net). These segmentation enhancements led to more accurate clinical measurements, reducing the lumen diameter MAE to 4.19 mm from 5.78 mm and the thrombus area error to 33.85% from 41.45% when compared to nnU-Net. Code is available at <a target="_blank" rel="noopener" href="https://github.com/yuxuanou623/AortaDiff.git">https://github.com/yuxuanou623/AortaDiff.git</a>. </p>
<blockquote>
<p>åœ¨è¯„ä¼°è…¹ä¸»åŠ¨è„‰ç˜¤ï¼ˆAAAï¼‰æ—¶ï¼Œè™½ç„¶å¢å¼ºå‹è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCECTï¼‰æ˜¯æ ‡å‡†æ–¹æ³•ï¼Œä½†æ‰€éœ€çš„ç¢˜é€ å½±å‰‚å­˜åœ¨é‡å¤§é£é™©ï¼ŒåŒ…æ‹¬è‚¾æ¯’æ€§ã€æ‚£è€…è¿‡æ•ååº”å’Œç¯å¢ƒå±å®³ã€‚ä¸ºäº†å‡å°‘é€ å½±å‰‚çš„ä½¿ç”¨ï¼Œæœ€è¿‘çš„æ·±åº¦å­¦ä¹ æ–¹æ³•ä¸“æ³¨äºä»éå¯¹æ¯”è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆNCCTï¼‰ç”ŸæˆåˆæˆCECTã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°æ–¹æ³•é‡‡ç”¨å¤šé˜¶æ®µç®¡é“ï¼Œé¦–å…ˆç”Ÿæˆå›¾åƒï¼Œç„¶åè¿›è¡Œåˆ†å‰²ï¼Œè¿™ä¼šå¯¼è‡´è¯¯å·®ç´¯ç§¯ï¼Œå¹¶ä¸”æœªèƒ½åˆ©ç”¨å…±äº«è¯­ä¹‰å’Œè§£å‰–ç»“æ„ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯ä»NCCTæ‰«æç”ŸæˆåˆæˆCECTå›¾åƒï¼ŒåŒæ—¶åˆ†å‰²ä¸»åŠ¨è„‰è…”å’Œè¡€æ “ã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼ˆCDMï¼‰ä¸å¤šä»»åŠ¡å­¦ä¹ ç›¸ç»“åˆï¼Œå®ç°å¯¹å›¾åƒåˆæˆå’Œè§£å‰–ç»“æ„åˆ†å‰²çš„ç«¯åˆ°ç«¯è”åˆä¼˜åŒ–ã€‚ä¸ä¹‹å‰çš„å¤šä»»åŠ¡æ‰©æ•£æ¨¡å‹ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ— éœ€åˆå§‹é¢„æµ‹ï¼ˆä¾‹å¦‚ç²—ç•¥åˆ†å‰²æ©è†œï¼‰ï¼Œå…±äº«ç¼–ç å™¨å’Œè§£ç å™¨å‚æ•°è·¨ä»»åŠ¡ï¼Œå¹¶é‡‡ç”¨åŠç›‘ç£è®­ç»ƒç­–ç•¥ä»ç¼ºå¤±åˆ†å‰²æ ‡ç­¾çš„æ‰«æä¸­å­¦ä¹ ï¼Œè¿™æ˜¯ç°å®ä¸–ç•Œä¸´åºŠæ•°æ®ä¸­å¸¸è§çš„çº¦æŸã€‚æˆ‘ä»¬åœ¨264åæ‚£è€…é˜Ÿåˆ—ä¸­è¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œå…¶æ€§èƒ½å§‹ç»ˆä¼˜äºæœ€æ–°å•ä»»åŠ¡å’Œå¤šé˜¶æ®µæ¨¡å‹ã€‚åœ¨å›¾åƒåˆæˆæ–¹é¢ï¼Œæˆ‘ä»¬çš„æ¨¡å‹è¾¾åˆ°äº†25.61åˆ†è´çš„å³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰ï¼Œé«˜äºå•ä»»åŠ¡CDMçš„23.80åˆ†è´ã€‚åœ¨è§£å‰–ç»“æ„åˆ†å‰²æ–¹é¢ï¼Œå®ƒå°†è…”é“Diceç³»æ•°ä»0.87æé«˜åˆ°0.89ï¼Œå¹¶å°†å…·æœ‰æŒ‘æˆ˜æ€§çš„è¡€æ “Diceç³»æ•°ä»0.48æé«˜åˆ°0.53ï¼ˆnnU-Netï¼‰ã€‚è¿™äº›åˆ†å‰²æ”¹è¿›å¸¦æ¥äº†æ›´å‡†ç¡®çš„ä¸´åºŠæµ‹é‡å€¼ï¼Œå°†è…”é“ç›´å¾„å¹³å‡ç»å¯¹è¯¯å·®ï¼ˆMAEï¼‰ä»nnU-Netçš„5.78æ¯«ç±³å‡å°‘åˆ°4.19æ¯«ç±³ï¼Œå¹¶å°†è¡€æ “é¢ç§¯è¯¯å·®ä»41.45%å‡å°‘åˆ°33.85%ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/yuxuanou623/AortaDiff.git%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/yuxuanou623/AortaDiff.gitæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.01498v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬ç ”ç©¶é’ˆå¯¹è…¹éƒ¨ä¸»åŠ¨è„‰ç˜¤çš„è¯„ä¼°ï¼Œæå‡ºä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„ç»Ÿä¸€æ¡†æ¶ï¼Œèƒ½å¤Ÿä»éå¯¹æ¯”CTæ‰«æä¸­ç”Ÿæˆåˆæˆå¯¹æ¯”å¢å¼ºCTå›¾åƒï¼Œå¹¶åŒæ—¶åˆ†å‰²ä¸»åŠ¨è„‰è…”å’Œè¡€æ “ã€‚è¯¥æ–¹æ³•ç»“åˆäº†æ¡ä»¶æ‰©æ•£æ¨¡å‹å’Œå¤šä»»åŠ¡å­¦ä¹ ï¼Œå®ç°äº†å›¾åƒåˆæˆå’Œè§£å‰–åˆ†å‰²çš„ç«¯åˆ°ç«¯è”åˆä¼˜åŒ–ã€‚ç›¸è¾ƒäºå…ˆå‰çš„å¤šä»»åŠ¡æ‰©æ•£æ¨¡å‹ï¼Œæœ¬ç ”ç©¶çš„æ–¹æ³•æ— éœ€åˆå§‹é¢„æµ‹ï¼Œå…±äº«ç¼–ç å™¨å’Œè§£ç å™¨å‚æ•°ï¼Œå¹¶é‡‡ç”¨åŠç›‘ç£è®­ç»ƒç­–ç•¥åº”å¯¹ç¼ºä¹åˆ†å‰²æ ‡ç­¾çš„æ‰«ææ•°æ®ã€‚åœ¨264åæ‚£è€…é˜Ÿåˆ—ä¸­çš„è¯„ä¼°æ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å›¾åƒåˆæˆå’Œè§£å‰–åˆ†å‰²æ–¹é¢å‡è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œè¶…è¿‡å•é¡¹ä»»åŠ¡å’Œå¤šé˜¶æ®µæ¨¡å‹ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç¢˜åŒ–å¯¹æ¯”å‰‚åœ¨è¯„ä¼°è…¹éƒ¨ä¸»åŠ¨è„‰ç˜¤ä¸­è™½ä¸ºæ ‡å‡†ï¼Œä½†å­˜åœ¨è‚¾æ¯’æ€§ã€æ‚£è€…è¿‡æ•å’Œç¯å¢ƒå±å®³ç­‰é£é™©ã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨æ·±åº¦å­¦ä¹ æ–¹æ³•ä»éå¯¹æ¯”CTæ‰«æç”Ÿæˆåˆæˆå¯¹æ¯”å¢å¼ºCTå›¾åƒã€‚</li>
<li>æå‡ºä¸€ç§ç»Ÿä¸€æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼ŒåŒæ—¶åˆ†å‰²ä¸»åŠ¨è„‰è…”å’Œè¡€æ “ï¼Œå‡å°‘è¯¯å·®ç´¯ç§¯ã€‚</li>
<li>ç»“åˆæ¡ä»¶æ‰©æ•£æ¨¡å‹å’Œå¤šä»»åŠ¡å­¦ä¹ ï¼Œå®ç°å›¾åƒåˆæˆå’Œè§£å‰–åˆ†å‰²çš„è”åˆä¼˜åŒ–ã€‚</li>
<li>ä¸å…ˆå‰çš„å¤šä»»åŠ¡æ‰©æ•£æ¨¡å‹ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•æ— éœ€åˆå§‹é¢„æµ‹ï¼Œå‚æ•°å…±äº«ï¼Œå¹¶é‡‡ç”¨åŠç›‘ç£è®­ç»ƒç­–ç•¥ã€‚</li>
<li>æ–¹æ³•åœ¨264åæ‚£è€…é˜Ÿåˆ—ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¿‡å•é¡¹ä»»åŠ¡å’Œå¤šé˜¶æ®µæ¨¡å‹ã€‚</li>
<li>ç²¾ç¡®çš„è§£å‰–åˆ†å‰²æœ‰åŠ©äºæ›´å‡†ç¡®çš„ä¸´åºŠæµ‹é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.01498">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-519359d89438ab321d1c2b52424a17e3" align="middle">
<img src="https://picx.zhimg.com/v2-32d0ef3d1aeb4d304651a4260201dfe9" align="middle">
<img src="https://picx.zhimg.com/v2-cf2c4f35daafc0bb59e8abcb4cfefba3" align="middle">
<img src="https://picx.zhimg.com/v2-297ccfe8560eecebfd5971823ce7ace3" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="VENTURA-Adapting-Image-Diffusion-Models-for-Unified-Task-Conditioned-Navigation"><a href="#VENTURA-Adapting-Image-Diffusion-Models-for-Unified-Task-Conditioned-Navigation" class="headerlink" title="VENTURA: Adapting Image Diffusion Models for Unified Task Conditioned   Navigation"></a>VENTURA: Adapting Image Diffusion Models for Unified Task Conditioned   Navigation</h2><p><strong>Authors:Arthur Zhang, Xiangyun Meng, Luca Calliari, Dong-Ki Kim, Shayegan Omidshafiei, Joydeep Biswas, Ali Agha, Amirreza Shaban</strong></p>
<p>Robots must adapt to diverse human instructions and operate safely in unstructured, open-world environments. Recent Vision-Language models (VLMs) offer strong priors for grounding language and perception, but remain difficult to steer for navigation due to differences in action spaces and pretraining objectives that hamper transferability to robotics tasks. Towards addressing this, we introduce VENTURA, a vision-language navigation system that finetunes internet-pretrained image diffusion models for path planning. Instead of directly predicting low-level actions, VENTURA generates a path mask (i.e. a visual plan) in image space that captures fine-grained, context-aware navigation behaviors. A lightweight behavior-cloning policy grounds these visual plans into executable trajectories, yielding an interface that follows natural language instructions to generate diverse robot behaviors. To scale training, we supervise on path masks derived from self-supervised tracking models paired with VLM-augmented captions, avoiding manual pixel-level annotation or highly engineered data collection setups. In extensive real-world evaluations, VENTURA outperforms state-of-the-art foundation model baselines on object reaching, obstacle avoidance, and terrain preference tasks, improving success rates by 33% and reducing collisions by 54% across both seen and unseen scenarios. Notably, we find that VENTURA generalizes to unseen combinations of distinct tasks, revealing emergent compositional capabilities. Videos, code, and additional materials: <a target="_blank" rel="noopener" href="https://venturapath.github.io/">https://venturapath.github.io</a> </p>
<blockquote>
<p>æœºå™¨äººå¿…é¡»é€‚åº”å¤šæ ·åŒ–çš„äººç±»æŒ‡ä»¤ï¼Œå¹¶åœ¨éç»“æ„åŒ–ã€å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­å®‰å…¨æ“ä½œã€‚æœ€è¿‘çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ä¸ºè¯­è¨€æ¥åœ°å’Œæ„ŸçŸ¥æä¾›äº†å¼ºæœ‰åŠ›çš„å…ˆéªŒçŸ¥è¯†ï¼Œä½†ç”±äºåŠ¨ä½œç©ºé—´ä¸Šçš„å·®å¼‚å’Œé¢„è®­ç»ƒç›®æ ‡ä¸æœºå™¨äººä»»åŠ¡çš„å¯è½¬ç§»æ€§å—é˜»ï¼Œä½¿å¾—å…¶åœ¨å¯¼èˆªæ–¹é¢ä»ç„¶éš¾ä»¥æ§åˆ¶ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†VENTURAï¼Œä¸€ä¸ªè§†è§‰è¯­è¨€å¯¼èˆªç³»ç»Ÿï¼Œå®ƒé€šè¿‡å¾®è°ƒäº’è”ç½‘é¢„è®­ç»ƒçš„å›¾åƒæ‰©æ•£æ¨¡å‹æ¥è¿›è¡Œè·¯å¾„è§„åˆ’ã€‚VENTURAä¸åŒäºç›´æ¥é¢„æµ‹ä½å±‚æ¬¡åŠ¨ä½œï¼Œè€Œæ˜¯ç”Ÿæˆå›¾åƒç©ºé—´ä¸­çš„è·¯å¾„æ©ç ï¼ˆå³è§†è§‰è®¡åˆ’ï¼‰ï¼Œæ•æ‰ç²¾ç»†çš„ã€ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„å¯¼èˆªè¡Œä¸ºã€‚ä¸€ä¸ªè½»é‡çº§çš„æ¨¡ä»¿è¡Œä¸ºç­–ç•¥å°†è¿™äº›è§†è§‰è®¡åˆ’è½¬åŒ–ä¸ºå¯æ‰§è¡Œçš„è½¨è¿¹ï¼Œä»è€Œäº§ç”Ÿä¸€ä¸ªéµå¾ªè‡ªç„¶è¯­è¨€æŒ‡ä»¤çš„æ¥å£ï¼Œä»¥ç”Ÿæˆå¤šæ ·åŒ–çš„æœºå™¨äººè¡Œä¸ºã€‚ä¸ºäº†æ‰©å¤§è®­ç»ƒè§„æ¨¡ï¼Œæˆ‘ä»¬åœ¨è‡ªæˆ‘ç›‘ç£è·Ÿè¸ªæ¨¡å‹ç”Ÿæˆçš„è·¯å¾„æ©ç ä¸Šè¿›è¡Œç›‘ç£ï¼Œè¿™äº›æ¨¡å‹ä¸VLMå¢å¼ºçš„å­—å¹•é…å¯¹ï¼Œé¿å…äº†æ‰‹åŠ¨åƒç´ çº§çš„æ ‡æ³¨æˆ–é«˜åº¦å·¥ç¨‹åŒ–çš„æ•°æ®é‡‡é›†è®¾ç½®ã€‚åœ¨å¹¿æ³›çš„å®é™…ä¸–ç•Œè¯„ä¼°ä¸­ï¼ŒVENTURAåœ¨ç‰©ä½“æŠ“å–ã€é¿éšœå’Œåœ°å½¢åå¥½ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºæœ€å…ˆè¿›çš„åŸºå‡†æ¨¡å‹ï¼Œåœ¨å·²çŸ¥å’ŒæœªçŸ¥åœºæ™¯ä¸­çš„æˆåŠŸç‡æé«˜äº†33%ï¼Œç¢°æ’å‡å°‘äº†54%ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç°VENTURAèƒ½å¤Ÿæ³›åŒ–åˆ°æœªè§è¿‡çš„ä¸åŒä»»åŠ¡çš„ç»„åˆï¼Œæ˜¾ç¤ºå‡ºæ–°å…´çš„ç»„åˆèƒ½åŠ›ã€‚è§†é¢‘ã€ä»£ç å’Œå…¶ä»–ææ–™è¯·å‚è§ï¼š[<a target="_blank" rel="noopener" href="https://venturapath.github.io/]">https://venturapath.github.io/]</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.01388v1">PDF</a> 9 pages, 6 figures, 3 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†VENTURAç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿç»“åˆäº†è§†è§‰å’Œè¯­è¨€æ¨¡å‹ï¼Œç”¨äºæœºå™¨äººå¯¼èˆªã€‚å®ƒé€šè¿‡å¾®è°ƒäº’è”ç½‘é¢„è®­ç»ƒçš„å›¾åƒæ‰©æ•£æ¨¡å‹æ¥è§„åˆ’è·¯å¾„ï¼Œç”Ÿæˆè·¯å¾„æ©ç ï¼ˆå³è§†è§‰è®¡åˆ’ï¼‰ï¼Œè¯¥æ©ç èƒ½æ•æ‰ç²¾ç»†ã€ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„å¯¼èˆªè¡Œä¸ºã€‚è¯¥ç³»ç»Ÿä½¿ç”¨è½»é‡çº§çš„è¡Œä¸ºå…‹éš†ç­–ç•¥å°†è§†è§‰è®¡åˆ’è½¬åŒ–ä¸ºå¯æ‰§è¡Œè½¨è¿¹ï¼Œå¹¶é€šè¿‡è‡ªç„¶è¯­è¨€æŒ‡ä»¤ç”Ÿæˆå¤šæ ·åŒ–çš„æœºå™¨äººè¡Œä¸ºã€‚é€šè¿‡è‡ªæˆ‘ç›‘ç£è·Ÿè¸ªæ¨¡å‹å’Œè¯­è¨€æ¨¡å‹çš„ç»“åˆï¼Œå®ç°äº†æ— éœ€æ‰‹åŠ¨åƒç´ çº§æ ‡æ³¨å’Œé«˜åº¦å®šåˆ¶çš„æ•°æ®é‡‡é›†è®¾ç½®çš„å¤§è§„æ¨¡è®­ç»ƒã€‚åœ¨çœŸå®ä¸–ç•Œç¯å¢ƒä¸­ï¼ŒVENTURAåœ¨ç‰©ä½“æŠ“å–ã€é¿éšœå’Œåœ°å½¢é€‰æ‹©ä»»åŠ¡ä¸Šè¶…è¶Šäº†æœ€å…ˆè¿›çš„åŸºçº¿æ¨¡å‹ï¼Œåœ¨å·²çŸ¥å’ŒæœªçŸ¥åœºæ™¯ä¸­çš„æˆåŠŸç‡æé«˜äº†33%ï¼Œç¢°æ’å‡å°‘äº†54%ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜èƒ½å¤Ÿæ¨å¹¿åˆ°æœªè§è¿‡çš„ç»„åˆä»»åŠ¡ï¼Œå±•ç°å‡ºç»„åˆçš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VENTURAç³»ç»Ÿç»“åˆäº†è§†è§‰å’Œè¯­è¨€æ¨¡å‹ï¼Œç”¨äºæœºå™¨äººå¯¼èˆªã€‚</li>
<li>é€šè¿‡å¾®è°ƒäº’è”ç½‘é¢„è®­ç»ƒçš„å›¾åƒæ‰©æ•£æ¨¡å‹è¿›è¡Œè·¯å¾„è§„åˆ’ã€‚</li>
<li>ç”Ÿæˆè·¯å¾„æ©ç ï¼ˆè§†è§‰è®¡åˆ’ï¼‰ï¼Œæ•æ‰ç²¾ç»†ã€ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„å¯¼èˆªè¡Œä¸ºã€‚</li>
<li>å€ŸåŠ©è½»é‡çº§è¡Œä¸ºå…‹éš†ç­–ç•¥å°†è§†è§‰è®¡åˆ’è½¬åŒ–ä¸ºå¯æ‰§è¡Œè½¨è¿¹ã€‚</li>
<li>é€šè¿‡è‡ªç„¶è¯­è¨€æŒ‡ä»¤ç”Ÿæˆå¤šæ ·åŒ–çš„æœºå™¨äººè¡Œä¸ºã€‚</li>
<li>ä½¿ç”¨è‡ªæˆ‘ç›‘ç£è·Ÿè¸ªæ¨¡å‹å’Œè¯­è¨€æ¨¡å‹çš„ç»“åˆè¿›è¡Œè®­ç»ƒï¼Œæ— éœ€æ‰‹åŠ¨åƒç´ çº§æ ‡æ³¨å’Œé«˜åº¦å®šåˆ¶çš„æ•°æ®é‡‡é›†è®¾ç½®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.01388">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0b73489da6f26d0dcc21d7053e981787" align="middle">
<img src="https://picx.zhimg.com/v2-f1d90c1eeca335ee887af48960952e48" align="middle">
<img src="https://picx.zhimg.com/v2-11869348900d0df20cdf85720c701c44" align="middle">
<img src="https://picx.zhimg.com/v2-d00455d6cbcf965ac0087a8c2e943bc1" align="middle">
<img src="https://picx.zhimg.com/v2-4e16ecdc4feb8851f323ea4e65e12a81" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="LVTINO-LAtent-Video-consisTency-INverse-sOlver-for-High-Definition-Video-Restoration"><a href="#LVTINO-LAtent-Video-consisTency-INverse-sOlver-for-High-Definition-Video-Restoration" class="headerlink" title="LVTINO: LAtent Video consisTency INverse sOlver for High Definition   Video Restoration"></a>LVTINO: LAtent Video consisTency INverse sOlver for High Definition   Video Restoration</h2><p><strong>Authors:Alessio Spagnoletti, AndrÃ©s Almansa, Marcelo Pereyra</strong></p>
<p>Computational imaging methods increasingly rely on powerful generative diffusion models to tackle challenging image restoration tasks. In particular, state-of-the-art zero-shot image inverse solvers leverage distilled text-to-image latent diffusion models (LDMs) to achieve unprecedented accuracy and perceptual quality with high computational efficiency. However, extending these advances to high-definition video restoration remains a significant challenge, due to the need to recover fine spatial detail while capturing subtle temporal dependencies. Consequently, methods that naively apply image-based LDM priors on a frame-by-frame basis often result in temporally inconsistent reconstructions. We address this challenge by leveraging recent advances in Video Consistency Models (VCMs), which distill video latent diffusion models into fast generators that explicitly capture temporal causality. Building on this foundation, we propose LVTINO, the first zero-shot or plug-and-play inverse solver for high definition video restoration with priors encoded by VCMs. Our conditioning mechanism bypasses the need for automatic differentiation and achieves state-of-the-art video reconstruction quality with only a few neural function evaluations, while ensuring strong measurement consistency and smooth temporal transitions across frames. Extensive experiments on a diverse set of video inverse problems show significant perceptual improvements over current state-of-the-art methods that apply image LDMs frame by frame, establishing a new benchmark in both reconstruction fidelity and computational efficiency. </p>
<blockquote>
<p>è®¡ç®—æˆåƒæ–¹æ³•è¶Šæ¥è¶Šä¾èµ–äºå¼ºå¤§çš„ç”Ÿæˆæ‰©æ•£æ¨¡å‹ï¼Œä»¥è§£å†³å…·æœ‰æŒ‘æˆ˜æ€§çš„å›¾åƒæ¢å¤ä»»åŠ¡ã€‚ç‰¹åˆ«æ˜¯ï¼Œæœ€å…ˆè¿›çš„é›¶æ ·æœ¬å›¾åƒé€†æ±‚è§£å™¨åˆ©ç”¨æç‚¼çš„æ–‡æœ¬åˆ°å›¾åƒæ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰ä»¥æé«˜çš„è®¡ç®—æ•ˆç‡å®ç°äº†å‰æ‰€æœªæœ‰çš„å‡†ç¡®æ€§å’Œæ„ŸçŸ¥è´¨é‡ã€‚ç„¶è€Œï¼Œå°†è¿™äº›è¿›å±•æ‰©å±•åˆ°é«˜æ¸…è§†é¢‘æ¢å¤ä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ï¼Œå› ä¸ºéœ€è¦åœ¨æ¢å¤ç²¾ç»†ç©ºé—´ç»†èŠ‚çš„åŒæ—¶æ•æ‰å¾®å¦™çš„æ—¶é—´ä¾èµ–æ€§ã€‚å› æ­¤ï¼Œé‚£äº›ç›´æ¥æŒ‰å¸§åº”ç”¨åŸºäºå›¾åƒçš„LDMå…ˆéªŒçš„æ–¹æ³•é€šå¸¸ä¼šå¯¼è‡´æ—¶é—´ä¸Šä¸ä¸€è‡´çš„é‡å»ºã€‚æˆ‘ä»¬é€šè¿‡åˆ©ç”¨è§†é¢‘ä¸€è‡´æ€§æ¨¡å‹ï¼ˆVCMï¼‰çš„æœ€æ–°è¿›å±•æ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œå®ƒå°†è§†é¢‘æ½œåœ¨æ‰©æ•£æ¨¡å‹æç‚¼æˆèƒ½å¤Ÿå¿«é€Ÿç”Ÿæˆä¸”èƒ½æ˜ç¡®æ•æ‰æ—¶é—´å› æœå…³ç³»çš„æ¨¡å‹ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æå‡ºäº†LVTINOï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç”¨äºé«˜æ¸…è§†é¢‘æ¢å¤çš„é›¶æ ·æœ¬æˆ–å³æ’å³ç”¨é€†æ±‚è§£å™¨ï¼Œå…¶å…ˆéªŒç”±VCMç¼–ç ã€‚æˆ‘ä»¬çš„è°ƒèŠ‚æœºåˆ¶è·³è¿‡äº†è‡ªåŠ¨åˆ†åŒ–çš„éœ€æ±‚ï¼Œä»…é€šè¿‡å‡ æ¬¡ç¥ç»ç½‘ç»œåŠŸèƒ½è¯„ä¼°å°±å®ç°äº†æœ€å…ˆè¿›çš„è§†é¢‘é‡å»ºè´¨é‡ï¼ŒåŒæ—¶ç¡®ä¿äº†å¼ºå¤§çš„æµ‹é‡ä¸€è‡´æ€§å’Œè·¨å¸§ä¹‹é—´çš„å¹³æ»‘æ—¶é—´è¿‡æ¸¡ã€‚åœ¨å¤šç§è§†é¢‘é€†é—®é¢˜ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œä¸å½“å‰å°†å›¾åƒLDMé€å¸§åº”ç”¨çš„å…ˆè¿›æ–¹æ³•ç›¸æ¯”ï¼Œæ„ŸçŸ¥æ•ˆæœæ˜¾è‘—æå‡ï¼Œåœ¨é‡å»ºä¿çœŸåº¦å’Œè®¡ç®—æ•ˆç‡æ–¹é¢éƒ½æ ‘ç«‹äº†æ–°çš„åŸºå‡†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.01339v1">PDF</a> 23 pages, 12 figures</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡ä»‹ç»äº†è®¡ç®—æˆåƒæ–¹æ³•å¦‚ä½•åˆ©ç”¨å…ˆè¿›çš„ç”Ÿæˆæ‰©æ•£æ¨¡å‹æ¥å¤„ç†å›¾åƒæ¢å¤ä»»åŠ¡ã€‚æœ€æ–°ä¸€ä»£çš„é›¶æ ·æœ¬å›¾åƒé€†æ±‚è§£å™¨é€šè¿‡è’¸é¦æ–‡æœ¬åˆ°å›¾åƒæ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMsï¼‰å®ç°äº†å‰æ‰€æœªæœ‰çš„å‡†ç¡®æ€§å’Œæ„ŸçŸ¥è´¨é‡ï¼ŒåŒæ—¶ä¿æŒäº†é«˜è®¡ç®—æ•ˆç‡ã€‚ç„¶è€Œï¼Œå°†è¿™äº›è¿›å±•æ‰©å±•åˆ°é«˜æ¸…è§†é¢‘æ¢å¤ä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ï¼Œå› ä¸ºéœ€è¦åœ¨æ¢å¤ç²¾ç»†ç©ºé—´ç»†èŠ‚çš„åŒæ—¶æ•æ‰å¾®å¦™çš„æ—¶é—´ä¾èµ–æ€§ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡åˆ©ç”¨è§†é¢‘ä¸€è‡´æ€§æ¨¡å‹ï¼ˆVCMsï¼‰çš„æœ€æ–°è¿›å±•ï¼Œå°†è§†é¢‘æ½œåœ¨æ‰©æ•£æ¨¡å‹è’¸é¦æˆå¿«é€Ÿç”Ÿæˆå™¨ï¼Œæ˜¾å¼æ•æ‰æ—¶é—´å› æœå…³ç³»ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæå‡ºäº†LVTINOï¼Œè¿™æ˜¯ä¸€ç§é›¶æ ·æœ¬æˆ–å³æ’å³ç”¨çš„é«˜æ¸…è§†é¢‘æ¢å¤é€†æ±‚è§£å™¨ï¼Œä»¥VCMsç¼–ç çš„å…ˆéªŒçŸ¥è¯†ä¸ºåŸºç¡€ã€‚å…¶è°ƒèŠ‚æœºåˆ¶æ— éœ€è‡ªåŠ¨å¾®åˆ†ï¼Œå³å¯å®ç°è§†é¢‘é‡å»ºè´¨é‡çš„æœ€æ–°æ°´å¹³ï¼Œä»…é€šè¿‡å‡ æ¬¡ç¥ç»ç½‘ç»œåŠŸèƒ½è¯„ä¼°å°±èƒ½è¾¾åˆ°æ•ˆæœï¼ŒåŒæ—¶ç¡®ä¿å¼ºå¤§çš„æµ‹é‡ä¸€è‡´æ€§å’Œè·¨å¸§çš„å¹³æ»‘æ—¶é—´è¿‡æ¸¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®¡ç®—æˆåƒæ–¹æ³•ä¾èµ–ç”Ÿæˆæ‰©æ•£æ¨¡å‹å¤„ç†å›¾åƒæ¢å¤ä»»åŠ¡ã€‚</li>
<li>å…ˆè¿›çš„é›¶æ ·æœ¬å›¾åƒé€†æ±‚è§£å™¨åˆ©ç”¨è’¸é¦æ–‡æœ¬åˆ°å›¾åƒæ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMsï¼‰å®ç°é«˜å‡†ç¡®æ€§å’Œæ„ŸçŸ¥è´¨é‡ã€‚</li>
<li>å°†è¿™äº›æŠ€æœ¯æ‰©å±•åˆ°é«˜æ¸…è§†é¢‘æ¢å¤å­˜åœ¨æŒ‘æˆ˜ï¼Œéœ€è¦æ¢å¤ç²¾ç»†ç©ºé—´ç»†èŠ‚å¹¶æ•æ‰æ—¶é—´ä¾èµ–æ€§ã€‚</li>
<li>è§†é¢‘ä¸€è‡´æ€§æ¨¡å‹ï¼ˆVCMsï¼‰ç”¨äºè’¸é¦è§†é¢‘æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼Œä»¥æ˜¾å¼æ•æ‰æ—¶é—´å› æœå…³ç³»ã€‚</li>
<li>LVTINOæ˜¯é¦–ä¸ªåˆ©ç”¨VCMså…ˆéªŒçŸ¥è¯†çš„é›¶æ ·æœ¬æˆ–å³æ’å³ç”¨çš„é«˜æ¸…è§†é¢‘æ¢å¤é€†æ±‚è§£å™¨ã€‚</li>
<li>LVTINOè°ƒèŠ‚æœºåˆ¶æ— éœ€è‡ªåŠ¨å¾®åˆ†ï¼Œèƒ½å®ç°è§†é¢‘é‡å»ºçš„å…ˆè¿›è´¨é‡ï¼Œå¹¶ç¡®ä¿æµ‹é‡ä¸€è‡´æ€§å’Œå¹³æ»‘æ—¶é—´è¿‡æ¸¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.01339">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b7394084e848b1883eae5f1b43008346" align="middle">
<img src="https://picx.zhimg.com/v2-d32d94dea63f1f19a89659dbe5a73530" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Temporal-Score-Rescaling-for-Temperature-Sampling-in-Diffusion-and-Flow-Models"><a href="#Temporal-Score-Rescaling-for-Temperature-Sampling-in-Diffusion-and-Flow-Models" class="headerlink" title="Temporal Score Rescaling for Temperature Sampling in Diffusion and Flow   Models"></a>Temporal Score Rescaling for Temperature Sampling in Diffusion and Flow   Models</h2><p><strong>Authors:Yanbo Xu, Yu Wu, Sungjae Park, Zhizhuo Zhou, Shubham Tulsiani</strong></p>
<p>We present a mechanism to steer the sampling diversity of denoising diffusion and flow matching models, allowing users to sample from a sharper or broader distribution than the training distribution. We build on the observation that these models leverage (learned) score functions of noisy data distributions for sampling and show that rescaling these allows one to effectively control a &#96;localâ€™ sampling temperature. Notably, this approach does not require any finetuning or alterations to training strategy, and can be applied to any off-the-shelf model and is compatible with both deterministic and stochastic samplers. We first validate our framework on toy 2D data, and then demonstrate its application for diffusion models trained across five disparate tasks â€“ image generation, pose estimation, depth prediction, robot manipulation, and protein design. We find that across these tasks, our approach allows sampling from sharper (or flatter) distributions, yielding performance gains e.g., depth prediction models benefit from sampling more likely depth estimates, whereas image generation models perform better when sampling a slightly flatter distribution. Project page: <a target="_blank" rel="noopener" href="https://temporalscorerescaling.github.io/">https://temporalscorerescaling.github.io</a> </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ§åˆ¶é™å™ªæ‰©æ•£å’Œæµé‡åŒ¹é…æ¨¡å‹çš„é‡‡æ ·å¤šæ ·æ€§çš„æœºåˆ¶ï¼Œå…è®¸ç”¨æˆ·ä»æ¯”è®­ç»ƒåˆ†å¸ƒæ›´å°–é”æˆ–æ›´å¹¿æ³›çš„åˆ†å¸ƒä¸­è¿›è¡Œé‡‡æ ·ã€‚æˆ‘ä»¬çš„è§‚å¯ŸåŸºç¡€æ˜¯ï¼Œè¿™äº›æ¨¡å‹åˆ©ç”¨å™ªå£°æ•°æ®çš„ï¼ˆå­¦ä¹ ï¼‰åˆ†æ•°å‡½æ•°è¿›è¡Œé‡‡æ ·ï¼Œæˆ‘ä»¬è¡¨æ˜é‡æ–°ç¼©æ”¾è¿™äº›åˆ†æ•°å‡½æ•°å¯ä»¥æœ‰æ•ˆåœ°æ§åˆ¶â€œå±€éƒ¨â€é‡‡æ ·æ¸©åº¦ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¿™ç§æ–¹æ³•ä¸éœ€è¦å¯¹è®­ç»ƒç­–ç•¥è¿›è¡Œå¾®è°ƒæˆ–æ›´æ”¹ï¼Œå¯ä»¥åº”ç”¨äºä»»ä½•ç°æˆçš„æ¨¡å‹ï¼Œå¹¶ä¸”ä¸ç¡®å®šæ€§é‡‡æ ·å™¨å’Œéšæœºé‡‡æ ·å™¨å…¼å®¹ã€‚æˆ‘ä»¬é¦–å…ˆåœ¨ç©å…·2Dæ•°æ®ä¸ŠéªŒè¯æˆ‘ä»¬çš„æ¡†æ¶ï¼Œç„¶åæ¼”ç¤ºå…¶åœ¨äº”ä¸ªä¸åŒä»»åŠ¡ä¸Šè®­ç»ƒçš„æ‰©æ•£æ¨¡å‹çš„åº”ç”¨ï¼ŒåŒ…æ‹¬å›¾åƒç”Ÿæˆã€å§¿æ€ä¼°è®¡ã€æ·±åº¦é¢„æµ‹ã€æœºå™¨äººæ“ä½œå’Œè›‹ç™½è´¨è®¾è®¡ã€‚æˆ‘ä»¬å‘ç°ï¼Œåœ¨è¿™äº›ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å…è®¸ä»æ›´å°–é”ï¼ˆæˆ–æ›´å¹³å¦ï¼‰çš„åˆ†å¸ƒä¸­è¿›è¡Œé‡‡æ ·ï¼Œä»è€Œå¸¦æ¥æ€§èƒ½æå‡ï¼Œä¾‹å¦‚æ·±åº¦é¢„æµ‹æ¨¡å‹å—ç›Šäºæ›´å¯èƒ½çš„æ·±åº¦ä¼°è®¡çš„é‡‡æ ·ï¼Œè€Œå›¾åƒç”Ÿæˆæ¨¡å‹åœ¨ç¨å¾®å¹³å¦çš„åˆ†å¸ƒä¸­é‡‡æ ·æ—¶è¡¨ç°æ›´å¥½ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://temporalscorerescaling.github.io/">https://temporalscorerescaling.github.io</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.01184v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æ§åˆ¶å»å™ªæ‰©æ•£å’ŒæµåŒ¹é…æ¨¡å‹çš„é‡‡æ ·å¤šæ ·æ€§çš„æœºåˆ¶ï¼Œä½¿ç”¨æˆ·å¯ä»¥ä»æ¯”è®­ç»ƒåˆ†å¸ƒæ›´å°–é”æˆ–æ›´å¹¿æ³›çš„åˆ†å¸ƒä¸­è¿›è¡Œé‡‡æ ·ã€‚æ–‡ç« åŸºäºè¿™äº›æ¨¡å‹åˆ©ç”¨å™ªå£°æ•°æ®çš„è¯„åˆ†å‡½æ•°è¿›è¡Œé‡‡æ ·çš„è§‚å¯Ÿï¼Œå±•ç¤ºé€šè¿‡è°ƒæ•´è¯„åˆ†å‡½æ•°çš„å°ºåº¦å¯ä»¥æœ‰æ•ˆåœ°æ§åˆ¶å±€éƒ¨é‡‡æ ·æ¸©åº¦ã€‚è¯¥æ–¹æ³•æ— éœ€å¯¹è®­ç»ƒç­–ç•¥è¿›è¡Œå¾®è°ƒæˆ–æ›´æ”¹ï¼Œå¯åº”ç”¨äºä»»ä½•ç°æˆçš„æ¨¡å‹ï¼Œå¹¶ä¸”ä¸ç¡®å®šæ€§é‡‡æ ·å™¨å’Œéšæœºé‡‡æ ·å™¨å…¼å®¹ã€‚è¯¥æ¡†æ¶åœ¨ç©å…·2Dæ•°æ®ä¸Šè¿›è¡Œäº†éªŒè¯ï¼Œå¹¶å±•ç¤ºäº†åœ¨äº”ä¸ªä¸åŒä»»åŠ¡ï¼ˆå›¾åƒç”Ÿæˆã€å§¿æ€ä¼°è®¡ã€æ·±åº¦é¢„æµ‹ã€æœºå™¨äººæ“ä½œå’Œè›‹ç™½è´¨è®¾è®¡ï¼‰çš„æ‰©æ•£æ¨¡å‹ä¸­çš„åº”ç”¨ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å¯ä»¥åœ¨ä¸åŒä»»åŠ¡ä¸­å®ç°ä»æ›´å°–é”æˆ–æ›´å¹³å¦çš„åˆ†å¸ƒè¿›è¡Œé‡‡æ ·ï¼Œä»è€Œæé«˜æ€§èƒ½ï¼Œä¾‹å¦‚åœ¨æ·±åº¦é¢„æµ‹æ¨¡å‹ä¸­ï¼Œä»æ›´å¯èƒ½çš„æ·±åº¦ä¼°è®¡ä¸­è¿›è¡Œé‡‡æ ·å—ç›Šè¾ƒå¤§ï¼Œè€Œåœ¨å›¾åƒç”Ÿæˆæ¨¡å‹ä¸­ï¼Œä»ç¨å¾®å¹³å¦çš„åˆ†å¸ƒä¸­è¿›è¡Œé‡‡æ ·æ•ˆæœæ›´å¥½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†ä¸€ç§æ§åˆ¶å»å™ªæ‰©æ•£å’ŒæµåŒ¹é…æ¨¡å‹çš„é‡‡æ ·å¤šæ ·æ€§çš„æ–°æœºåˆ¶ã€‚</li>
<li>é€šè¿‡è°ƒæ•´å™ªå£°æ•°æ®çš„è¯„åˆ†å‡½æ•°çš„å°ºåº¦ï¼Œå¯ä»¥æ§åˆ¶å±€éƒ¨é‡‡æ ·æ¸©åº¦ã€‚</li>
<li>è¯¥æ–¹æ³•æ— éœ€å¾®è°ƒæˆ–æ›´æ”¹è®­ç»ƒç­–ç•¥ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ€§ï¼Œå¯åº”ç”¨äºä»»ä½•ç°æˆçš„æ¨¡å‹ã€‚</li>
<li>è¯¥æ–¹æ³•ä¸ç¡®å®šæ€§é‡‡æ ·å™¨å’Œéšæœºé‡‡æ ·å™¨å…¼å®¹ã€‚</li>
<li>æ¡†æ¶åœ¨ç©å…·2Dæ•°æ®ä¸Šè¿›è¡Œäº†éªŒè¯ï¼Œå¹¶æˆåŠŸåº”ç”¨äºå›¾åƒç”Ÿæˆã€å§¿æ€ä¼°è®¡ã€æ·±åº¦é¢„æµ‹ã€æœºå™¨äººæ“ä½œå’Œè›‹ç™½è´¨è®¾è®¡ç­‰äº”ä¸ªä»»åŠ¡çš„æ‰©æ•£æ¨¡å‹ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•èƒ½ä½¿æ¨¡å‹ä»æ›´å°–é”æˆ–æ›´å¹³å¦çš„åˆ†å¸ƒä¸­é‡‡æ ·ï¼Œä»è€Œæé«˜æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.01184">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6964f50787e4df4aa3910924d5a5081e" align="middle">
<img src="https://picx.zhimg.com/v2-4c687bb548c721d6188ee6b6bcfb11d1" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Semantic-and-Visual-Crop-Guided-Diffusion-Models-for-Heterogeneous-Tissue-Synthesis-in-Histopathology"><a href="#Semantic-and-Visual-Crop-Guided-Diffusion-Models-for-Heterogeneous-Tissue-Synthesis-in-Histopathology" class="headerlink" title="Semantic and Visual Crop-Guided Diffusion Models for Heterogeneous   Tissue Synthesis in Histopathology"></a>Semantic and Visual Crop-Guided Diffusion Models for Heterogeneous   Tissue Synthesis in Histopathology</h2><p><strong>Authors:Saghir Alfasly, Wataru Uegami, MD Enamul Hoq, Ghazal Alabtah, H. R. Tizhoosh</strong></p>
<p>Synthetic data generation in histopathology faces unique challenges: preserving tissue heterogeneity, capturing subtle morphological features, and scaling to unannotated datasets. We present a latent diffusion model that generates realistic heterogeneous histopathology images through a novel dual-conditioning approach combining semantic segmentation maps with tissue-specific visual crops. Unlike existing methods that rely on text prompts or abstract visual embeddings, our approach preserves critical morphological details by directly incorporating raw tissue crops from corresponding semantic regions. For annotated datasets (i.e., Camelyon16, Panda), we extract patches ensuring 20-80% tissue heterogeneity. For unannotated data (i.e., TCGA), we introduce a self-supervised extension that clusters whole-slide images into 100 tissue types using foundation model embeddings, automatically generating pseudo-semantic maps for training. Our method synthesizes high-fidelity images with precise region-wise annotations, achieving superior performance on downstream segmentation tasks. When evaluated on annotated datasets, models trained on our synthetic data show competitive performance to those trained on real data, demonstrating the utility of controlled heterogeneous tissue generation. In quantitative evaluation, prompt-guided synthesis reduces Frechet Distance by up to 6X on Camelyon16 (from 430.1 to 72.0) and yields 2-3x lower FD across Panda and TCGA. Downstream DeepLabv3+ models trained solely on synthetic data attain test IoU of 0.71 and 0.95 on Camelyon16 and Panda, within 1-2% of real-data baselines (0.72 and 0.96). By scaling to 11,765 TCGA whole-slide images without manual annotations, our framework offers a practical solution for an urgent need for generating diverse, annotated histopathology data, addressing a critical bottleneck in computational pathology. </p>
<blockquote>
<p>ç—…ç†ç»„ç»‡å­¦çš„åˆæˆæ•°æ®ç”Ÿæˆé¢ä¸´ä¸€äº›ç‹¬ç‰¹çš„æŒ‘æˆ˜ï¼šä¿ç•™ç»„ç»‡å¼‚è´¨æ€§ã€æ•æ‰å¾®å¦™çš„å½¢æ€å­¦ç‰¹å¾ä»¥åŠæ‰©å±•åˆ°æœªæ ‡æ³¨çš„æ•°æ®é›†ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡ä¸€ç§æ–°é¢–çš„åŒé‡æ¡ä»¶æ–¹æ³•ç»“åˆè¯­ä¹‰åˆ†å‰²å›¾å’Œç»„ç»‡ç‰¹å¼‚æ€§è§†è§‰è£å‰ªæ¥ç”ŸæˆçœŸå®çš„å¼‚è´¨æ€§ç—…ç†ç»„ç»‡å­¦å›¾åƒã€‚ä¸åŒäºä¾èµ–æ–‡æœ¬æç¤ºæˆ–æŠ½è±¡è§†è§‰åµŒå…¥çš„ç°æœ‰æ–¹æ³•ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡ç›´æ¥èå…¥æ¥è‡ªç›¸åº”è¯­ä¹‰åŒºåŸŸçš„åŸå§‹ç»„ç»‡è£å‰ªæ¥ä¿ç•™å…³é”®çš„å½¢æ€å­¦ç»†èŠ‚ã€‚å¯¹äºå·²æ ‡æ³¨çš„æ•°æ®é›†ï¼ˆä¾‹å¦‚Camelyon16ã€Pandaï¼‰ï¼Œæˆ‘ä»¬æå–æ–‘å—ï¼Œç¡®ä¿ç»„ç»‡å¼‚è´¨æ€§å 20-80%ã€‚å¯¹äºæœªæ ‡æ³¨çš„æ•°æ®ï¼ˆä¾‹å¦‚TCGAï¼‰ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§è‡ªç›‘ç£æ‰©å±•ï¼Œä½¿ç”¨åŸºç¡€æ¨¡å‹åµŒå…¥å°†æ•´ä¸ªå¹»ç¯ç‰‡å›¾åƒèšç±»æˆ100ç§ç»„ç»‡ç±»å‹ï¼Œè‡ªåŠ¨ç”Ÿæˆä¼ªè¯­ä¹‰å›¾è¿›è¡Œè®­ç»ƒã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆæˆé«˜ä¿çœŸå›¾åƒï¼Œå…·æœ‰ç²¾ç¡®çš„åŒºåŸŸæ³¨é‡Šï¼Œå¹¶åœ¨ä¸‹æ¸¸åˆ†å‰²ä»»åŠ¡ä¸Šå®ç°å“è¶Šæ€§èƒ½ã€‚åœ¨å·²æ ‡æ³¨æ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œç»è¿‡æˆ‘ä»¬çš„åˆæˆæ•°æ®è®­ç»ƒçš„æ¨¡å‹è¡¨ç°å‡ºä¸çœŸå®æ•°æ®è®­ç»ƒçš„æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ï¼Œè¯æ˜äº†å¯æ§çš„å¼‚è´¨æ€§ç»„ç»‡ç”Ÿæˆçš„å®ç”¨æ€§ã€‚åœ¨å®šé‡è¯„ä¼°ä¸­ï¼Œæç¤ºå¼•å¯¼çš„åˆæˆå°†Camelyon16ä¸Šçš„Frechetè·ç¦»å‡å°‘äº†é«˜è¾¾6å€ï¼ˆä»430.1é™è‡³72.0ï¼‰ï¼Œå¹¶ä¸”åœ¨Pandaå’ŒTCGAä¸Šçš„FDé™ä½äº†2-3å€ã€‚ä»…ç»è¿‡åˆæˆæ•°æ®è®­ç»ƒçš„ä¸‹æ¸¸DeepLabv3+æ¨¡å‹åœ¨Camelyon16å’ŒPandaä¸Šçš„æµ‹è¯•äº¤é›†mIoUè¾¾åˆ°0.71å’Œ0.95ï¼Œä¸çœŸå®æ•°æ®åŸºå‡†çº¿ç›¸å·®1-2%ï¼ˆåˆ†åˆ«ä¸º0.72å’Œ0.96ï¼‰ã€‚é€šè¿‡æ‰©å±•åˆ°11765å¼ æ²¡æœ‰æ‰‹åŠ¨æ ‡æ³¨çš„TCGAå…¨å¹»ç¯ç‰‡å›¾åƒï¼Œæˆ‘ä»¬çš„æ¡†æ¶ä¸ºè§£å†³ç”Ÿæˆå¤šæ ·ã€æ ‡æ³¨çš„ç—…ç†ç»„ç»‡å­¦æ•°æ®çš„ç´§è¿«éœ€æ±‚æä¾›äº†åˆ‡å®å¯è¡Œçš„è§£å†³æ–¹æ¡ˆï¼Œè§£å†³äº†è®¡ç®—ç—…ç†å­¦ä¸­çš„ä¸€ä¸ªå…³é”®ç“¶é¢ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.17847v2">PDF</a> NeurIPS 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹ç—…ç†å›¾åƒåˆæˆæ•°æ®ç”Ÿæˆæ‰€é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¦‚ä¿æŒç»„ç»‡å¼‚è´¨æ€§ã€æ•æ‰å¾®å¦™çš„å½¢æ€ç‰¹å¾å’Œæ‰©å±•åˆ°æœªæ ‡æ³¨æ•°æ®é›†çš„é—®é¢˜ã€‚æå‡ºäº†ä¸€ç§åŸºäºæ½œåœ¨æ‰©æ•£æ¨¡å‹çš„ç—…ç†å›¾åƒåˆæˆæ–¹æ³•ï¼Œé€šè¿‡ç»“åˆè¯­ä¹‰åˆ†å‰²å›¾å’Œç‰¹å®šç»„ç»‡è§†è§‰è£å‰ªçš„åŒé‡æ¡ä»¶ç­–ç•¥ç”Ÿæˆé€¼çœŸçš„å¼‚è´¨æ€§ç—…ç†å›¾åƒã€‚ä¸ä¾èµ–æ–‡æœ¬æç¤ºæˆ–æŠ½è±¡è§†è§‰åµŒå…¥çš„ç°æœ‰æ–¹æ³•ä¸åŒï¼Œè¯¥æ–¹æ³•é€šè¿‡ç›´æ¥ç»“åˆæ¥è‡ªç›¸åº”è¯­ä¹‰åŒºåŸŸçš„åŸå§‹ç»„ç»‡è£å‰ªç‰‡æ®µï¼Œä¿ç•™äº†å…³é”®çš„å½¢æ€ç»†èŠ‚ã€‚å¯¹äºå·²æ ‡æ³¨çš„æ•°æ®é›†ï¼ˆå¦‚Camelyon16å’ŒPandaï¼‰ï¼Œé€šè¿‡æå–ç¡®ä¿20-80ï¼…ç»„ç»‡å¼‚è´¨æ€§çš„è¡¥ä¸ã€‚å¯¹äºæœªæ ‡æ³¨çš„æ•°æ®ï¼ˆå¦‚TCGAï¼‰ï¼Œå¼•å…¥äº†ä¸€ç§è‡ªç›‘ç£æ‰©å±•ï¼Œä½¿ç”¨åŸºç¡€æ¨¡å‹åµŒå…¥å°†å…¨å¹»ç¯ç‰‡å›¾åƒèšç±»ä¸º100ç§ç»„ç»‡ç±»å‹ï¼Œè‡ªåŠ¨ç”Ÿæˆä¼ªè¯­ä¹‰å›¾è¿›è¡Œè®­ç»ƒã€‚è¯¥æ–¹æ³•åˆæˆçš„å›¾åƒå…·æœ‰é«˜ä¿çœŸåº¦å’Œç²¾ç¡®çš„åŒºåŸŸæ ‡æ³¨ï¼Œåœ¨ä¸‹æ¸¸åˆ†å‰²ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜è¶Šã€‚åœ¨å·²æ ‡æ³¨æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œç»åˆæˆæ•°æ®è®­ç»ƒçš„æ¨¡å‹ä¸çœŸå®æ•°æ®è®­ç»ƒçš„æ¨¡å‹è¡¨ç°ç›¸å½“ï¼Œè¯æ˜äº†å¯æ§å¼‚è´¨æ€§ç»„ç»‡ç”Ÿæˆçš„å®ç”¨æ€§ã€‚å®šé‡è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œæç¤ºå¼•å¯¼çš„åˆæˆæ•°æ®åœ¨Camelyon16ä¸Šçš„Frechetè·ç¦»é™ä½äº†6å€ï¼ˆä»430.1é™è‡³72.0ï¼‰ï¼Œå¹¶ä¸”åœ¨Pandaå’ŒTCGAä¸Šçš„FDé™ä½äº†2-3å€ã€‚ä»…ä½¿ç”¨åˆæˆæ•°æ®è®­ç»ƒçš„DeepLabv3+æ¨¡å‹åœ¨Camelyon16å’ŒPandaä¸Šçš„æµ‹è¯•IoUè¾¾åˆ°0.71å’Œ0.95ï¼Œä¸çœŸå®æ•°æ®åŸºå‡†æµ‹è¯•ï¼ˆåˆ†åˆ«ä¸º0.72å’Œ0.96ï¼‰ç›¸å·®ä»…1-2ï¼…ã€‚é€šè¿‡æ‰©å±•åˆ°æœªæ ‡æ³¨çš„TCGAå…¨å¹»ç¯ç‰‡å›¾åƒï¼ˆå…±11ï¼Œ765å¼ ï¼‰ï¼Œè¯¥æ¡†æ¶ä¸ºè§£å†³ç”Ÿæˆå¤šæ ·åŒ–å’Œæ ‡æ³¨çš„ç—…ç†æ•°æ®çš„è¿«åˆ‡éœ€æ±‚æä¾›äº†å®ç”¨è§£å†³æ–¹æ¡ˆï¼Œè§£å†³äº†è®¡ç®—ç—…ç†å­¦ä¸­çš„å…³é”®ç“¶é¢ˆã€‚</p>
<p><strong>è¦ç‚¹è§£æ</strong></p>
<p>ä»¥ä¸‹æ˜¯æ–‡æœ¬çš„ä¸ƒä¸ªä¸»è¦æ´å¯Ÿç‚¹ï¼š</p>
<ol>
<li>åˆæˆæ•°æ®åœ¨ç—…ç†å­¦ä¸­çš„æŒ‘æˆ˜åŒ…æ‹¬ä¿æŒç»„ç»‡å¼‚è´¨æ€§ã€ç²¾ç»†çš„å½¢æ€ç‰¹å¾æ•æ‰å’Œæ‰©å±•åˆ°å¤§è§„æ¨¡æœªæ ‡æ³¨æ•°æ®é›†çš„èƒ½åŠ›ã€‚</li>
<li>ä»‹ç»äº†ä¸€ç§åŸºäºæ½œåœ¨æ‰©æ•£æ¨¡å‹çš„åˆæˆæ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿé€šè¿‡åŒé‡æ¡ä»¶ç­–ç•¥ç”Ÿæˆé€¼çœŸçš„å¼‚è´¨æ€§ç—…ç†å›¾åƒã€‚</li>
<li>ä¸ç°æœ‰æ–¹æ³•ä¸åŒï¼Œè¯¥æ–¹æ³•ç»“åˆè¯­ä¹‰åˆ†å‰²å›¾å’Œç‰¹å®šç»„ç»‡è§†è§‰è£å‰ªæ¥ä¿ç•™å½¢æ€ç»†èŠ‚ã€‚</li>
<li>å¯¹äºå·²æ ‡æ³¨æ•°æ®é›†ï¼Œé€šè¿‡æå–ç¡®ä¿ä¸€å®šç»„ç»‡å¼‚è´¨æ€§çš„å›¾åƒå—è¿›è¡Œè®­ç»ƒã€‚</li>
<li>å¯¹äºæœªæ ‡æ³¨æ•°æ®é›†ï¼Œå¼•å…¥äº†è‡ªç›‘ç£æ‰©å±•æ–¹æ³•ï¼Œä½¿ç”¨åŸºç¡€æ¨¡å‹åµŒå…¥è‡ªåŠ¨ä¸ºè®­ç»ƒç”Ÿæˆä¼ªè¯­ä¹‰å›¾ã€‚</li>
<li>åˆæˆçš„é«˜ä¿çœŸå›¾åƒå…·æœ‰ç²¾ç¡®çš„åŒºåŸŸæ ‡æ³¨ï¼Œå¹¶åœ¨ä¸‹æ¸¸åˆ†å‰²ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜è¶Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.17847">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-976e022aa4042493ae225b4999548fe4" align="middle">
<img src="https://picx.zhimg.com/v2-06c5798d1f9122617b9ee39e0d86dcc3" align="middle">
<img src="https://picx.zhimg.com/v2-e67d0364e8693b036501da13253d51ab" align="middle">
<img src="https://picx.zhimg.com/v2-77237d8a6c556c16c9852f7e00af8af5" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Concept-Unlearning-by-Modeling-Key-Steps-of-Diffusion-Process"><a href="#Concept-Unlearning-by-Modeling-Key-Steps-of-Diffusion-Process" class="headerlink" title="Concept Unlearning by Modeling Key Steps of Diffusion Process"></a>Concept Unlearning by Modeling Key Steps of Diffusion Process</h2><p><strong>Authors:Chaoshuo Zhang, Chenhao Lin, Zhengyu Zhao, Le Yang, Qian Wang, Chao Shen</strong></p>
<p>Text-to-image diffusion models (T2I DMs), represented by Stable Diffusion, which generate highly realistic images based on textual input, have been widely used, but their flexibility also makes them prone to misuse for producing harmful or unsafe content. Concept unlearning has been used to prevent text-to-image diffusion models from being misused to generate undesirable visual content. However, existing methods struggle to trade off unlearning effectiveness with the preservation of generation quality. To address this limitation, we propose Key Step Concept Unlearning (KSCU), which selectively fine-tunes the model at key steps to the target concept. KSCU is inspired by the fact that different diffusion denoising steps contribute unequally to the final generation. Compared to previous approaches, which treat all denoising steps uniformly, KSCU avoids over-optimization of unnecessary steps for higher effectiveness and reduces the number of parameter updates for higher efficiency. For example, on the I2P dataset, KSCU outperforms ESD by 8.3% in nudity unlearning accuracy while improving FID by 8.4%, and achieves a high overall score of 0.92, substantially surpassing all other SOTA methods. </p>
<blockquote>
<p>æ–‡æœ¬è½¬å›¾åƒæ‰©æ•£æ¨¡å‹ï¼ˆT2I DMï¼‰ï¼Œä»¥Stable Diffusionä¸ºä»£è¡¨ï¼Œèƒ½å¤Ÿæ ¹æ®æ–‡æœ¬è¾“å…¥ç”Ÿæˆé«˜åº¦é€¼çœŸçš„å›¾åƒï¼Œå·²ç»å¾—åˆ°äº†å¹¿æ³›åº”ç”¨ã€‚ç„¶è€Œï¼Œå®ƒä»¬çš„çµæ´»æ€§ä¹Ÿä½¿å¾—å®ƒä»¬å®¹æ˜“è¢«è¯¯ç”¨äºäº§ç”Ÿæœ‰å®³æˆ–ä¸å®‰å…¨çš„å†…å®¹ã€‚æ¦‚å¿µé—å¿˜æŠ€æœ¯å·²è¢«ç”¨äºé˜²æ­¢æ–‡æœ¬è½¬å›¾åƒæ‰©æ•£æ¨¡å‹è¢«è¯¯ç”¨ç”Ÿæˆä¸å¸Œæœ›çš„è§†è§‰å†…å®¹ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•åœ¨å¹³è¡¡é—å¿˜æ•ˆæœä¸ä¿æŒç”Ÿæˆè´¨é‡æ–¹é¢å­˜åœ¨å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™ï¼Œæˆ‘ä»¬æå‡ºäº†å…³é”®æ­¥éª¤æ¦‚å¿µé—å¿˜ï¼ˆKSCUï¼‰ï¼Œå®ƒé€‰æ‹©æ€§åœ°å¾®è°ƒæ¨¡å‹çš„å…³é”®æ­¥éª¤ä»¥é’ˆå¯¹ç›®æ ‡æ¦‚å¿µã€‚KSCUçš„çµæ„Ÿæ¥æºäºä¸åŒæ‰©æ•£å»å™ªæ­¥éª¤å¯¹æœ€ç»ˆç”Ÿæˆçš„è´¡çŒ®ä¸ç­‰è¿™ä¸€äº‹å®ã€‚ä¸ä»¥å¾€å¯¹æ‰€æœ‰å»å™ªæ­¥éª¤è¿›è¡Œç»Ÿä¸€å¤„ç†çš„æ–¹æ³•ä¸åŒï¼ŒKSCUé¿å…äº†ä¸å¿…è¦æ­¥éª¤çš„è¿‡åº¦ä¼˜åŒ–ï¼Œä»¥æé«˜æ•ˆæœå’Œæ•ˆç‡ã€‚ä¾‹å¦‚ï¼Œåœ¨I2Pæ•°æ®é›†ä¸Šï¼ŒKSCUåœ¨è£¸éœ²åº¦é—å¿˜å‡†ç¡®æ€§æ–¹é¢æ¯”ESDé«˜å‡º8.3%ï¼ŒåŒæ—¶æ”¹è¿›äº†FIDæŒ‡æ ‡8.4%ï¼Œå¹¶è·å¾—äº†0.92çš„é«˜ç»¼åˆå¾—åˆ†ï¼Œæ˜¾è‘—è¶…è¶Šäº†æ‰€æœ‰å…¶ä»–æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.06526v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼ˆT2I DMï¼‰ï¼Œå¦‚Stable Diffusionï¼Œèƒ½æ ¹æ®æ–‡æœ¬è¾“å…¥ç”Ÿæˆé«˜åº¦é€¼çœŸçš„å›¾åƒï¼Œä½†å…¶çµæ´»æ€§ä¹Ÿä½¿å…¶æ˜“äºäº§ç”Ÿæœ‰å®³æˆ–ä¸å®‰å…¨çš„å†…å®¹ã€‚ä¸ºé¢„é˜²æ¨¡å‹æ»¥ç”¨ï¼Œç ”ç©¶è€…æå‡ºäº†æ¦‚å¿µå»å­¦ä¹ çš„ç­–ç•¥ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥åœ¨å»é™¤å­¦ä¹ æ•ˆæœä¸ä¿æŒç”Ÿæˆè´¨é‡ä¹‹é—´å–å¾—å¹³è¡¡ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†å…³é”®æ­¥éª¤æ¦‚å¿µå»å­¦ä¹ ï¼ˆKSCUï¼‰ï¼Œè¯¥æ–¹æ³•æœ‰é€‰æ‹©æ€§åœ°åœ¨å¯¹ç›®æ ‡æ¦‚å¿µçš„å…³é”®æ­¥éª¤ä¸Šå¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚KSCUçš„çµæ„Ÿæ¥æºäºä¸åŒæ‰©æ•£å»å™ªæ­¥éª¤å¯¹æœ€ç»ˆç”Ÿæˆçš„è´¡çŒ®ä¸å¹³ç­‰ã€‚ä¸ä»¥å¾€å¯¹æ‰€æœ‰å»å™ªæ­¥éª¤ä¸€è§†åŒä»çš„æ–¹æ³•ç›¸æ¯”ï¼ŒKSCUé¿å…äº†ä¸å¿…è¦æ­¥éª¤çš„è¿‡åº¦ä¼˜åŒ–ï¼Œæé«˜äº†æ•ˆç‡å’Œæ•ˆæœã€‚åœ¨I2Pæ•°æ®é›†ä¸Šï¼ŒKSCUåœ¨è£¸ä½“å»é™¤å‡†ç¡®æ€§æ–¹é¢è¾ƒESDé«˜å‡º8.3%ï¼ŒåŒæ—¶æé«˜äº†FIDè¯„åˆ†8.4%ï¼Œæ€»ä½“å¾—åˆ†é«˜è¾¾0.92ï¼Œè¿œè¶…å…¶ä»–æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>T2I DMså¦‚Stable Diffusionè™½èƒ½ç”Ÿæˆé«˜åº¦é€¼çœŸçš„å›¾åƒï¼Œä½†å…¶çµæ´»æ€§å¯¼è‡´æ˜“äºäº§ç”Ÿæœ‰å®³æˆ–ä¸å®‰å…¨çš„å†…å®¹ã€‚</li>
<li>æ¦‚å¿µå»å­¦ä¹ æ–¹æ³•è¢«ç”¨æ¥é˜²æ­¢T2I DMsçš„æ»¥ç”¨ã€‚</li>
<li>ç°æœ‰æ–¹æ³•åœ¨å¹³è¡¡å»å­¦ä¹ æ•ˆæœå’Œä¿æŒç”Ÿæˆè´¨é‡æ–¹é¢å­˜åœ¨å›°éš¾ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•KSCUï¼Œè¯¥æ–¹æ³•æœ‰é€‰æ‹©æ€§åœ°åœ¨å¯¹ç›®æ ‡æ¦‚å¿µçš„å…³é”®æ­¥éª¤ä¸Šå¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚</li>
<li>KSCUåŸºäºä¸åŒæ‰©æ•£å»å™ªæ­¥éª¤å¯¹æœ€ç»ˆç”Ÿæˆçš„è´¡çŒ®ä¸å¹³ç­‰è¿™ä¸€äº‹å®ã€‚</li>
<li>KSCUé¿å…äº†ä¸å¿…è¦å»å™ªæ­¥éª¤çš„è¿‡åº¦ä¼˜åŒ–ï¼Œæé«˜äº†å»å­¦ä¹ çš„æ•ˆç‡å’Œæ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.06526">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4966e0db9eaf54c5f72d1d76df2d116b" align="middle">
<img src="https://picx.zhimg.com/v2-ae75385ed52056fe7bb5c9970bd88944" align="middle">
<img src="https://picx.zhimg.com/v2-77674de7fe84930be39afbd48716f5da" align="middle">
<img src="https://picx.zhimg.com/v2-cf251c5bace7b5f8d11e4addb74b7766" align="middle">
<img src="https://picx.zhimg.com/v2-76f9cc6d2b93f9416e3083e54a95fa19" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="One-Step-Residual-Shifting-Diffusion-for-Image-Super-Resolution-via-Distillation"><a href="#One-Step-Residual-Shifting-Diffusion-for-Image-Super-Resolution-via-Distillation" class="headerlink" title="One-Step Residual Shifting Diffusion for Image Super-Resolution via   Distillation"></a>One-Step Residual Shifting Diffusion for Image Super-Resolution via   Distillation</h2><p><strong>Authors:Daniil Selikhanovych, David Li, Aleksei Leonov, Nikita Gushchin, Sergei Kushneriuk, Alexander Filippov, Evgeny Burnaev, Iaroslav Koshelev, Alexander Korotin</strong></p>
<p>Diffusion models for super-resolution (SR) produce high-quality visual results but require expensive computational costs. Despite the development of several methods to accelerate diffusion-based SR models, some (e.g., SinSR) fail to produce realistic perceptual details, while others (e.g., OSEDiff) may hallucinate non-existent structures. To overcome these issues, we present RSD, a new distillation method for ResShift, one of the top diffusion-based SR models. Our method is based on training the student network to produce such images that a new fake ResShift model trained on them will coincide with the teacher model. RSD achieves single-step restoration and outperforms the teacher by a large margin. We show that our distillation method can surpass the other distillation-based method for ResShift - SinSR - making it on par with state-of-the-art diffusion-based SR distillation methods. Compared to SR methods based on pre-trained text-to-image models, RSD produces competitive perceptual quality, provides images with better alignment to degraded input images, and requires fewer parameters and GPU memory. We provide experimental results on various real-world and synthetic datasets, including RealSR, RealSet65, DRealSR, ImageNet, and DIV2K. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹åœ¨è¶…åˆ†è¾¨ç‡ï¼ˆSRï¼‰åº”ç”¨ä¸­ç”Ÿæˆäº†é«˜è´¨é‡è§†è§‰æ•ˆæœï¼Œä½†éœ€è¦æ˜‚è´µçš„è®¡ç®—æˆæœ¬ã€‚å°½ç®¡å·²ç»å¼€å‘äº†å‡ ç§æ–¹æ³•æ¥åŠ é€ŸåŸºäºæ‰©æ•£çš„SRæ¨¡å‹ï¼Œä½†æŸäº›æ–¹æ³•ï¼ˆä¾‹å¦‚SinSRï¼‰æ— æ³•ç”Ÿæˆé€¼çœŸçš„æ„ŸçŸ¥ç»†èŠ‚ï¼Œè€Œå…¶ä»–æ–¹æ³•ï¼ˆä¾‹å¦‚OSEDiffï¼‰å¯èƒ½ä¼šè™šæ„ä¸å­˜åœ¨çš„ç»“æ„ã€‚ä¸ºäº†å…‹æœè¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†RSDï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºResShiftçš„æ–°å‹è’¸é¦æ–¹æ³•ï¼ŒResShiftæ˜¯é¡¶çº§çš„åŸºäºæ‰©æ•£çš„SRæ¨¡å‹ä¹‹ä¸€ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŸºäºè®­ç»ƒå­¦ç”Ÿç½‘ç»œæ¥ç”Ÿæˆå›¾åƒï¼Œè¿™äº›å›¾åƒèƒ½å¤Ÿä½¿åœ¨æ–°å‡ResShiftæ¨¡å‹ä¸Šè®­ç»ƒçš„æ¨¡å‹ä¸æ•™å¸ˆçš„æ¨¡å‹ç›¸å»åˆã€‚RSDå®ç°äº†å•æ­¥æ¢å¤ï¼Œå¹¶åœ¨æ•™å¸ˆæ¨¡å‹çš„åŸºç¡€ä¸Šå¤§å¹…è¶…è¶Šã€‚æˆ‘ä»¬å±•ç¤ºäº†æˆ‘ä»¬çš„è’¸é¦æ–¹æ³•å¯ä»¥è¶…è¶ŠResShiftçš„å…¶ä»–è’¸é¦æ–¹æ³•SinSRï¼Œä½¿å…¶ä¸æœ€å…ˆè¿›çš„åŸºäºæ‰©æ•£çš„SRè’¸é¦æ–¹æ³•ç›¸åª²ç¾ã€‚ä¸åŸºäºé¢„è®­ç»ƒæ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„SRæ–¹æ³•ç›¸æ¯”ï¼ŒRSDäº§ç”Ÿçš„æ„ŸçŸ¥è´¨é‡å…·æœ‰ç«äº‰åŠ›ï¼Œä¸ºé€€åŒ–è¾“å…¥å›¾åƒæä¾›äº†æ›´å¥½çš„å¯¹é½å›¾åƒï¼Œå¹¶ä¸”éœ€è¦æ›´å°‘çš„å‚æ•°å’ŒGPUå†…å­˜ã€‚æˆ‘ä»¬åœ¨å„ç§çœŸå®å’Œåˆæˆæ•°æ®é›†ä¸Šæä¾›äº†å®éªŒç»“æœï¼ŒåŒ…æ‹¬RealSRã€RealSet65ã€DRealSRã€ImageNetå’ŒDIV2Kã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13358v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹åœ¨è¶…åˆ†è¾¨ç‡ï¼ˆSRï¼‰åº”ç”¨ä¸­èƒ½ç”Ÿæˆé«˜è´¨é‡å›¾åƒï¼Œä½†è®¡ç®—æˆæœ¬è¾ƒé«˜ã€‚å°½ç®¡æœ‰å¤šç§æ–¹æ³•åŠ é€Ÿæ‰©æ•£æ¨¡å‹åœ¨SRé¢†åŸŸçš„åº”ç”¨ï¼Œä½†ä¸€äº›æ¨¡å‹ï¼ˆå¦‚SinSRï¼‰æ— æ³•ç”Ÿæˆé€¼çœŸçš„æ„ŸçŸ¥ç»†èŠ‚ï¼Œè€Œå…¶ä»–æ¨¡å‹ï¼ˆå¦‚OSEDiffï¼‰å¯èƒ½ä¼šè™šæ„ä¸å­˜åœ¨çš„ç»“æ„ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†RSDï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹ResShiftçš„æ–°è’¸é¦æ–¹æ³•ã€‚è¯¥æ–¹æ³•åŸºäºè®­ç»ƒå­¦ç”Ÿç½‘ç»œç”Ÿæˆå›¾åƒï¼Œä½¿æ–°çš„å‡ResShiftæ¨¡å‹åœ¨å®ƒä»¬ä¸Šè®­ç»ƒçš„å›¾åƒä¸åŸå§‹æ¨¡å‹ä¸€è‡´ã€‚RSDå®ç°äº†å•æ­¥æ¢å¤ï¼Œå¹¶åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šè¶…è¶Šäº†åŸå§‹æ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„è’¸é¦æ–¹æ³•èƒ½è¶…è¶ŠResShiftçš„å¦ä¸€ç§è’¸é¦æ–¹æ³•SinSRï¼Œä½¿å…¶ä¸æœ€å…ˆè¿›çš„æ‰©æ•£æ¨¡å‹SRè’¸é¦æ–¹æ³•ç›¸å½“ã€‚ç›¸è¾ƒäºåŸºäºé¢„è®­ç»ƒæ–‡æœ¬åˆ°å›¾åƒçš„SRæ–¹æ³•ï¼ŒRSDåœ¨æ„ŸçŸ¥è´¨é‡ä¸Šå…·æœ‰ç«äº‰åŠ›ï¼Œç”Ÿæˆçš„å›¾åƒä¸é€€åŒ–è¾“å…¥å›¾åƒå¯¹é½åº¦æ›´é«˜ï¼ŒåŒæ—¶éœ€è¦çš„å‚æ•°å’ŒGPUå†…å­˜æ›´å°‘ã€‚æˆ‘ä»¬åœ¨RealSRã€RealSet65ã€DRealSRã€ImageNetå’ŒDIV2Kç­‰å¤šä¸ªçœŸå®å’Œåˆæˆæ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒéªŒè¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨è¶…åˆ†è¾¨ç‡åº”ç”¨ä¸­èƒ½ç”Ÿæˆé«˜è´¨é‡å›¾åƒï¼Œä½†è®¡ç®—æˆæœ¬è¾ƒé«˜ã€‚</li>
<li>å½“å‰å­˜åœ¨çš„åŠ é€Ÿæ‰©æ•£æ¨¡å‹åœ¨SRé¢†åŸŸçš„æ–¹æ³•å­˜åœ¨ç”Ÿæˆä¸é€¼çœŸæ„ŸçŸ¥ç»†èŠ‚å’Œè™šæ„ä¸å­˜åœ¨ç»“æ„çš„é—®é¢˜ã€‚</li>
<li>RSDæ˜¯ä¸€ç§é’ˆå¯¹ResShiftçš„æ–°è’¸é¦æ–¹æ³•ï¼Œé€šè¿‡è®­ç»ƒå­¦ç”Ÿç½‘ç»œç”Ÿæˆå›¾åƒï¼Œä½¿å‡ResShiftæ¨¡å‹çš„è®­ç»ƒç»“æœä¸åŸå§‹æ¨¡å‹ä¸€è‡´ã€‚</li>
<li>RSDå®ç°äº†å•æ­¥æ¢å¤ï¼Œå¹¶å¤§å¹…è¶…è¶Šäº†åŸå§‹æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>RSDåœ¨è’¸é¦æ–¹æ³•ä¸Šè¶…è¶Šäº†SinSRï¼Œä½¿å…¶æ€§èƒ½ä¸æœ€å…ˆè¿›çš„æ‰©æ•£æ¨¡å‹SRè’¸é¦æ–¹æ³•ç›¸å½“ã€‚</li>
<li>ç›¸è¾ƒäºå…¶ä»–SRæ–¹æ³•ï¼ŒRSDåœ¨æ„ŸçŸ¥è´¨é‡ä¸Šå…·æœ‰ç«äº‰åŠ›ï¼Œä¸”ç”Ÿæˆçš„å›¾åƒä¸é€€åŒ–è¾“å…¥å›¾åƒå¯¹é½åº¦æ›´é«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13358">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ef443f27cd4175c8fcb1442ad3d5305c" align="middle">
<img src="https://picx.zhimg.com/v2-c519519873a0a15e1bcb407b8b8ab666" align="middle">
<img src="https://picx.zhimg.com/v2-df83c1e39ffcdcf8c80eb3c4c71fda7d" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-04/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-04/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-04/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-4f44294221847e7a520cb94c4dcb3ecd" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-04  Noisy Timing Behavior is a Feature of Central Compact Object Pulsars
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-04
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-04/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-f6a90ec4d3b1bf751fe3910e7a57ee39" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-04  StealthAttack Robust 3D Gaussian Splatting Poisoning via Density-Guided   Illusions
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-04
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32306k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
