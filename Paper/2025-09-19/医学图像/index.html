<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="医学图像">
    <meta name="description" content="医学图像 方向最新论文已更新，请持续关注 Update in 2025-09-19  LamiGauss Pitching Radiative Gaussian for Sparse-View X-ray   Laminography Reconstruction">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>医学图像 | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_医学图像/2509.13629v1/page_0_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">医学图像</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">医学图像</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                医学图像
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-09-19
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-10-07
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    16.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    65 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-09-19-更新"><a href="#2025-09-19-更新" class="headerlink" title="2025-09-19 更新"></a>2025-09-19 更新</h1><h2 id="LamiGauss-Pitching-Radiative-Gaussian-for-Sparse-View-X-ray-Laminography-Reconstruction"><a href="#LamiGauss-Pitching-Radiative-Gaussian-for-Sparse-View-X-ray-Laminography-Reconstruction" class="headerlink" title="LamiGauss: Pitching Radiative Gaussian for Sparse-View X-ray   Laminography Reconstruction"></a>LamiGauss: Pitching Radiative Gaussian for Sparse-View X-ray   Laminography Reconstruction</h2><p><strong>Authors:Chu Chen, Ander Biguri, Jean-Michel Morel, Raymond H. Chan, Carola-Bibiane Schönlieb, Jizhou Li</strong></p>
<p>X-ray Computed Laminography (CL) is essential for non-destructive inspection of plate-like structures in applications such as microchips and composite battery materials, where traditional computed tomography (CT) struggles due to geometric constraints. However, reconstructing high-quality volumes from laminographic projections remains challenging, particularly under highly sparse-view acquisition conditions. In this paper, we propose a reconstruction algorithm, namely LamiGauss, that combines Gaussian Splatting radiative rasterization with a dedicated detector-to-world transformation model incorporating the laminographic tilt angle. LamiGauss leverages an initialization strategy that explicitly filters out common laminographic artifacts from the preliminary reconstruction, preventing redundant Gaussians from being allocated to false structures and thereby concentrating model capacity on representing the genuine object. Our approach effectively optimizes directly from sparse projections, enabling accurate and efficient reconstruction with limited data. Extensive experiments on both synthetic and real datasets demonstrate the effectiveness and superiority of the proposed method over existing techniques. LamiGauss uses only 3$%$ of full views to achieve superior performance over the iterative method optimized on a full dataset. </p>
<blockquote>
<p>X射线计算层析成像（CL）对于微芯片和复合电池材料等板状结构的非破坏性检测至关重要。由于几何约束，传统计算机断层扫描（CT）在这些应用中面临困难。然而，从层析投影重建高质量体积仍然是一个挑战，特别是在高度稀疏视图采集条件下。在本文中，我们提出了一种重建算法，即LamiGauss，它将高斯Splatting辐射光栅化与结合层析倾斜角的专用检测器到世界转换模型相结合。LamiGauss利用初始化策略，明确过滤出常见的层析伪影，防止冗余高斯值被分配到错误的结构上，从而将模型容量集中在表示真实物体上。我们的方法直接从稀疏投影中进行优化，可用有限的数据实现准确高效的重建。在合成和真实数据集上的大量实验证明了所提方法的有效性和优越性。LamiGauss仅使用3%的全视图即可实现优于全数据集优化迭代方法的高性能表现。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13863v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>X光计算层析成像（CL）对于微芯片和复合电池材料等板状结构的非破坏性检测至关重要，传统计算机断层扫描（CT）因几何约束而难以应用。本文提出一种结合高斯溅射辐射光栅化和包含层析倾斜角的专用检测器到世界转换模型的重建算法LamiGauss。该算法通过初步重建过滤掉常见的层析成像伪影，避免冗余高斯分配于错误结构，从而专注于真实物体的表示。此算法直接从稀疏投影进行优化，在有限数据下实现准确高效的重建。在合成和真实数据集上的大量实验证明，该方法在现有技术上的效果和优越性。仅使用全部视图的3%，便可超越在全数据集上优化的迭代方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>X-ray Computed Laminography (CL)对于非破坏性检测板状结构非常重要，尤其适用于微芯片和复合电池材料等应用。</li>
<li>传统计算机断层扫描（CT）由于几何约束在某些领域难以应用。</li>
<li>LamiGauss算法结合了Gaussian Splatting和检测器到世界的转换模型。</li>
<li>LamiGauss通过过滤层析成像伪影，优化模型容量以表示真实物体。</li>
<li>该算法可直接从稀疏投影进行优化，实现有限数据下的准确高效重建。</li>
<li>在合成和真实数据集上的实验证明LamiGauss方法和现有技术的优越性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13863">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_医学图像/2509.13863v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_医学图像/2509.13863v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_医学图像/2509.13863v1/page_1_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_医学图像/2509.13863v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_医学图像/2509.13863v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_医学图像/2509.13863v1/page_3_1.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Consistent-View-Alignment-Improves-Foundation-Models-for-3D-Medical-Image-Segmentation"><a href="#Consistent-View-Alignment-Improves-Foundation-Models-for-3D-Medical-Image-Segmentation" class="headerlink" title="Consistent View Alignment Improves Foundation Models for 3D Medical   Image Segmentation"></a>Consistent View Alignment Improves Foundation Models for 3D Medical   Image Segmentation</h2><p><strong>Authors:Puru Vaish, Felix Meister, Tobias Heimann, Christoph Brune, Jelmer M. Wolterink</strong></p>
<p>Many recent approaches in representation learning implicitly assume that uncorrelated views of a data point are sufficient to learn meaningful representations for various downstream tasks. In this work, we challenge this assumption and demonstrate that meaningful structure in the latent space does not emerge naturally. Instead, it must be explicitly induced. We propose a method that aligns representations from different views of the data to align complementary information without inducing false positives. Our experiments show that our proposed self-supervised learning method, Consistent View Alignment, improves performance for downstream tasks, highlighting the critical role of structured view alignment in learning effective representations. Our method achieved first and second place in the MICCAI 2025 SSL3D challenge when using a Primus vision transformer and ResEnc convolutional neural network, respectively. The code and pretrained model weights are released at <a target="_blank" rel="noopener" href="https://github.com/Tenbatsu24/LatentCampus">https://github.com/Tenbatsu24/LatentCampus</a>. </p>
<blockquote>
<p>在表示学习的诸多最新方法中，隐含地假设一个数据点的非相关视图足以为各种下游任务学习有意义的表示。在这项工作中，我们质疑这一假设，并证明潜在空间中的有意义结构并不会自然出现。相反，它必须显式地诱导产生。我们提出了一种方法，通过对数据不同视图的表示进行对齐，以对齐互补信息，而不会产生误报。我们的实验表明，我们提出的自监督学习方法——一致视图对齐，提高了下游任务的性能，凸显了结构化视图对齐在学习有效表示中的关键作用。当使用Primus视觉变压器和ResEnc卷积神经网络时，我们的方法在MICCAI 2025 SSL3D挑战中分别取得第一和第二名。相关代码和预训练模型权重已发布在<a target="_blank" rel="noopener" href="https://github.com/Tenbatsu24/LatentCampus">https://github.com/Tenbatsu24/LatentCampus</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13846v1">PDF</a> MICCAI 2025: 1st Place in Transformer track and 2nd Place in   Convolution track of SSL3D-OpenMind challenge</p>
<p><strong>Summary</strong><br>     本文挑战了现有表示学习方法中无关数据点视图能自然形成有意义结构的假设，并提出一种方法，通过对不同视图的数据表示进行对齐，以获取互补信息，避免产生误报。实验证明，其提出的自监督学习方法Consistent View Alignment对下游任务性能有所提升。在MICCAI 2025 SSL3D挑战赛中，使用Primus Vision Transformer和ResEnc卷积神经网络的方法分别获得第一和第二名。代码和预训练模型权重已发布在LatentCampus上。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>现有表示学习方法存在假设：无关数据点视图足以形成有意义结构，本文对此提出质疑。</li>
<li>本文提出一种方法，通过显式诱导形成有意义结构，而非假设其会自然出现。</li>
<li>方法名为Consistent View Alignment，旨在对齐不同视图的数据表示，获取互补信息并避免误报。</li>
<li>实验证明，该方法能提高下游任务性能。</li>
<li>在MICCAI 2025 SSL3D挑战赛中，使用此方法在两种不同网络架构上取得优异成绩。</li>
<li>代码和预训练模型权重已公开发布在LatentCampus上，方便研究者和开发者使用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13846">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_医学图像/2509.13846v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_医学图像/2509.13846v1/page_2_0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Semi-MoE-Mixture-of-Experts-meets-Semi-Supervised-Histopathology-Segmentation"><a href="#Semi-MoE-Mixture-of-Experts-meets-Semi-Supervised-Histopathology-Segmentation" class="headerlink" title="Semi-MoE: Mixture-of-Experts meets Semi-Supervised Histopathology   Segmentation"></a>Semi-MoE: Mixture-of-Experts meets Semi-Supervised Histopathology   Segmentation</h2><p><strong>Authors:Nguyen Lan Vi Vu, Thanh-Huy Nguyen, Thien Nguyen, Daisuke Kihara, Tianyang Wang, Xingjian Li, Min Xu</strong></p>
<p>Semi-supervised learning has been employed to alleviate the need for extensive labeled data for histopathology image segmentation, but existing methods struggle with noisy pseudo-labels due to ambiguous gland boundaries and morphological misclassification. This paper introduces Semi-MOE, to the best of our knowledge, the first multi-task Mixture-of-Experts framework for semi-supervised histopathology image segmentation. Our approach leverages three specialized expert networks: A main segmentation expert, a signed distance field regression expert, and a boundary prediction expert, each dedicated to capturing distinct morphological features. Subsequently, the Multi-Gating Pseudo-labeling module dynamically aggregates expert features, enabling a robust fuse-and-refine pseudo-labeling mechanism. Furthermore, to eliminate manual tuning while dynamically balancing multiple learning objectives, we propose an Adaptive Multi-Objective Loss. Extensive experiments on GlaS and CRAG benchmarks show that our method outperforms state-of-the-art approaches in low-label settings, highlighting the potential of MoE-based architectures in advancing semi-supervised segmentation. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/vnlvi2k3/Semi-MoE">https://github.com/vnlvi2k3/Semi-MoE</a>. </p>
<blockquote>
<p>半监督学习已被应用于缓解病理学图像分割对大量标注数据的需求，但现有方法在处理由于腺体边界模糊和形态误分类而产生的噪声伪标签时遇到了困难。本文引入了Semi-MOE，据我们所知，这是第一个用于半监督病理学图像分割的多任务混合专家框架。我们的方法利用三个专业的专家网络：主要分割专家、带符号距离场回归专家和边界预测专家，每个专家网络都专注于捕获不同的形态特征。随后，多门控伪标签模块动态聚合专家特征，实现稳健的融合和细化伪标签机制。此外，为了消除手动调整，同时动态平衡多个学习目标，我们提出了自适应多目标损失。在GlaS和CRAG基准测试上的大量实验表明，我们的方法在低标签设置下优于最新方法，突显了基于MoE架构的潜力在推进半监督分割方面的优势。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/vnlvi2k3/Semi-MoE%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/vnlvi2k3/Semi-MoE获得。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13834v1">PDF</a> Accepted to BMVC 2025</p>
<p><strong>Summary</strong></p>
<p>半监督学习被应用于缓解医学图像分割中对大量标注数据的需求，但现有方法因腺体边界模糊和形态误分类而面临噪声伪标签的问题。本文引入Semi-MOE，据我们所知，它是第一个用于半监督医学图像分割的多任务混合专家框架。该方法利用三个专业专家网络：主分割专家、符号距离场回归专家和边界预测专家，分别专注于捕捉不同的形态特征。随后，多门控伪标签模块动态聚合专家特征，实现稳健的融合和细化伪标签机制。此外，为了消除手动调整并动态平衡多个学习目标，我们提出了自适应多目标损失。在GlaS和CRAG基准测试上的实验表明，我们的方法在低标签设置下优于最新方法，突显了基于MoE架构的潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>本研究解决半监督学习在医学图像分割中面临噪声伪标签的问题。</li>
<li>引入Semi-MOE，据称首个多任务混合专家框架用于半监督医学图像分割。</li>
<li>利用三个专业专家网络进行特征捕捉：主分割、符号距离场回归和边界预测。</li>
<li>多门控伪标签模块动态聚合专家特征，实现稳健伪标签机制。</li>
<li>提出自适应多目标损失以消除手动调整并动态平衡多个学习目标。</li>
<li>在GlaS和CRAG基准测试上表现优于其他方法，特别是在低标签设置下。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13834">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_医学图像/2509.13834v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_医学图像/2509.13834v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_医学图像/2509.13834v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="VocSegMRI-Multimodal-Learning-for-Precise-Vocal-Tract-Segmentation-in-Real-time-MRI"><a href="#VocSegMRI-Multimodal-Learning-for-Precise-Vocal-Tract-Segmentation-in-Real-time-MRI" class="headerlink" title="VocSegMRI: Multimodal Learning for Precise Vocal Tract Segmentation in   Real-time MRI"></a>VocSegMRI: Multimodal Learning for Precise Vocal Tract Segmentation in   Real-time MRI</h2><p><strong>Authors:Daiqi Liu, Tomás Arias-Vergara, Johannes Enk, Fangxu Xing, Maureen Stone, Jerry L. Prince, Jana Hutter, Andreas Maier, Jonghye Woo, Paula Andrea Pérez-Toro</strong></p>
<p>Accurately segmenting articulatory structures in real-time magnetic resonance imaging (rtMRI) remains challenging, as most existing methods rely almost entirely on visual cues. Yet synchronized acoustic and phonological signals provide complementary context that can enrich visual information and improve precision. In this paper, we introduce VocSegMRI, a multimodal framework that integrates video, audio, and phonological inputs through cross-attention fusion for dynamic feature alignment. To further enhance cross-modal representation, we incorporate a contrastive learning objective that improves segmentation performance even when the audio modality is unavailable at inference. Evaluated on a sub-set of USC-75 rtMRI dataset, our approach achieves state-of-the-art performance, with a Dice score of 0.95 and a 95th percentile Hausdorff Distance (HD_95) of 4.20 mm, outperforming both unimodal and multimodal baselines. Ablation studies confirm the contributions of cross-attention and contrastive learning to segmentation precision and robustness. These results highlight the value of integrative multimodal modeling for accurate vocal tract analysis. </p>
<blockquote>
<p>实时磁共振成像（rtMRI）中精确分割发音结构仍然是一个挑战，因为大多数现有方法几乎完全依赖于视觉线索。然而，同步的声学和语音信号提供了丰富的上下文信息，可以丰富视觉信息并提高精度。在本文中，我们介绍了VocSegMRI，这是一个多模式框架，通过跨注意融合整合视频、音频和语音输入，实现动态特征对齐。为了进一步增强跨模式表示，我们引入了对比学习目标，即使在推理时音频模式不可用，也能提高分割性能。在USC-75 rtMRI数据集的一个子集上进行了评估，我们的方法达到了最先进的性能，Dice得分为0.95，95th百分位Hausdorff距离（HD_95）为4.20毫米，超过了单模态和多模态基线。消融研究证实了跨注意力和对比学习对分割精度和稳健性的贡献。这些结果突显了整合多模式建模在准确分析声带结构中的重要性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13767v1">PDF</a> Preprint submitted to ICASSP</p>
<p><strong>Summary</strong><br>     本论文提出VocSegMRI的多模态框架，通过融合视频、音频和语音信号，利用跨注意力融合实现动态特征对齐，提高实时磁共振成像（rtMRI）中对发音结构的准确分割。即使在没有音频模态的情况下，通过对比学习目标，该框架也能提高分割性能。在USC-75 rtMRI数据集上评估，该方法达到领先水平，Dice系数为0.95，Hausdorff Distance（HD_95）为4.20mm。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VocSegMRI是一个多模态框架，能够整合视频、音频和语音信号。</li>
<li>该框架利用跨注意力融合实现动态特征对齐。</li>
<li>对比学习目标的引入提高了分割性能的鲁棒性，即使在没有音频模态的情况下。</li>
<li>在USC-75 rtMRI数据集上评估，VocSegMRI达到领先水平。</li>
<li>该方法的Dice系数达到0.95，显示出较高的分割准确性。</li>
<li>框架的Ablation研究证实了跨注意力和对比学习对分割精度和稳健性的贡献。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13767">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_医学图像/2509.13767v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_医学图像/2509.13767v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_医学图像/2509.13767v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_医学图像/2509.13767v1/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_医学图像/2509.13767v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="SAMIR-an-efficient-registration-framework-via-robust-feature-learning-from-SAM"><a href="#SAMIR-an-efficient-registration-framework-via-robust-feature-learning-from-SAM" class="headerlink" title="SAMIR, an efficient registration framework via robust feature learning   from SAM"></a>SAMIR, an efficient registration framework via robust feature learning   from SAM</h2><p><strong>Authors:Yue He, Min Liu, Qinghao Liu, Jiazheng Wang, Yaonan Wang, Hang Zhang, Xiang Chen</strong></p>
<p>Image registration is a fundamental task in medical image analysis. Deformations are often closely related to the morphological characteristics of tissues, making accurate feature extraction crucial. Recent weakly supervised methods improve registration by incorporating anatomical priors such as segmentation masks or landmarks, either as inputs or in the loss function. However, such weak labels are often not readily available, limiting their practical use. Motivated by the strong representation learning ability of visual foundation models, this paper introduces SAMIR, an efficient medical image registration framework that utilizes the Segment Anything Model (SAM) to enhance feature extraction. SAM is pretrained on large-scale natural image datasets and can learn robust, general-purpose visual representations. Rather than using raw input images, we design a task-specific adaptation pipeline using SAM’s image encoder to extract structure-aware feature embeddings, enabling more accurate modeling of anatomical consistency and deformation patterns. We further design a lightweight 3D head to refine features within the embedding space, adapting to local deformations in medical images. Additionally, we introduce a Hierarchical Feature Consistency Loss to guide coarse-to-fine feature matching and improve anatomical alignment. Extensive experiments demonstrate that SAMIR significantly outperforms state-of-the-art methods on benchmark datasets for both intra-subject cardiac image registration and inter-subject abdomen CT image registration, achieving performance improvements of 2.68% on ACDC and 6.44% on the abdomen dataset. The source code will be publicly available on GitHub following the acceptance of this paper. </p>
<blockquote>
<p>图像配准是医学图像分析中的基本任务。变形通常与组织的形态特征密切相关，因此准确的特征提取至关重要。最近出现的弱监督方法通过融入解剖学先验（如分割掩膜或标记点）来改善配准效果，无论是作为输入还是用于损失函数。然而，此类弱标签通常难以获取，从而限制了其实际应用。受视觉基础模型强大表示学习能力的影响，本文介绍了SAMIR，这是一个高效的医学图像配准框架，它利用“任意分割模型”（SAM）增强特征提取。SAM在大规模自然图像数据集上进行预训练，可学习稳健的通用视觉表示。我们没有使用原始输入图像，而是设计了一个特定任务的适应管道，使用SAM的图像编码器来提取结构感知特征嵌入，从而更准确地模拟解剖一致性和变形模式。我们进一步设计了一个轻量级的3D头部，在嵌入空间内细化特征，以适应医学图像中的局部变形。此外，我们引入了分层特征一致性损失，以引导从粗糙到精细的特征匹配并改善解剖学对齐。大量实验表明，在基准数据集上，SAMIR在受试者内部心脏图像配准和受试者之间腹部CT图像配准方面都显著优于最新方法，在ACDC上实现了2.68%的性能提升，在腹部数据集上实现了6.44%的性能提升。论文被接受后，源代码将在GitHub上公开提供。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13629v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了一种高效的医学图像注册框架SAMIR，它利用预训练在大型自然图像数据集上的Segment Anything Model（SAM）增强特征提取。通过设计任务特定的适应管道和3D头，SAMIR能更准确地建模解剖结构的一致性和变形模式。此外，还引入了分层特征一致性损失，以指导粗到细的特征匹配，提高解剖对齐。实验表明，SAMIR在心脏图像注册的基准数据集上比最新技术领先了高达2.68%，在腹部CT图像注册的基准数据集上领先了高达6.44%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>医学图像注册中，特征提取的重要性在于其与组织形态的紧密关联。</li>
<li>预训练的Segment Anything Model（SAM）用于增强医学图像的特征提取。</li>
<li>SAMIR框架通过任务特定适应管道和3D头设计，实现更准确的结构感知特征嵌入提取。</li>
<li>引入分层特征一致性损失以改善粗到细的特征匹配和解剖对齐。</li>
<li>SAMIR在心脏和腹部CT图像注册的基准数据集上实现了显著的性能提升。</li>
<li>SAMIR框架源代码将在论文被接受后公开在GitHub上。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13629">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_医学图像/2509.13629v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_医学图像/2509.13629v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_医学图像/2509.13629v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_医学图像/2509.13629v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_医学图像/2509.13629v1/page_5_1.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Cross-Distribution-Diffusion-Priors-Driven-Iterative-Reconstruction-for-Sparse-View-CT"><a href="#Cross-Distribution-Diffusion-Priors-Driven-Iterative-Reconstruction-for-Sparse-View-CT" class="headerlink" title="Cross-Distribution Diffusion Priors-Driven Iterative Reconstruction for   Sparse-View CT"></a>Cross-Distribution Diffusion Priors-Driven Iterative Reconstruction for   Sparse-View CT</h2><p><strong>Authors:Haodong Li, Shuo Han, Haiyang Mao, Yu Shi, Changsheng Fang, Jianjia Zhang, Weiwen Wu, Hengyong Yu</strong></p>
<p>Sparse-View CT (SVCT) reconstruction enhances temporal resolution and reduces radiation dose, yet its clinical use is hindered by artifacts due to view reduction and domain shifts from scanner, protocol, or anatomical variations, leading to performance degradation in out-of-distribution (OOD) scenarios. In this work, we propose a Cross-Distribution Diffusion Priors-Driven Iterative Reconstruction (CDPIR) framework to tackle the OOD problem in SVCT. CDPIR integrates cross-distribution diffusion priors, derived from a Scalable Interpolant Transformer (SiT), with model-based iterative reconstruction methods. Specifically, we train a SiT backbone, an extension of the Diffusion Transformer (DiT) architecture, to establish a unified stochastic interpolant framework, leveraging Classifier-Free Guidance (CFG) across multiple datasets. By randomly dropping the conditioning with a null embedding during training, the model learns both domain-specific and domain-invariant priors, enhancing generalizability. During sampling, the globally sensitive transformer-based diffusion model exploits the cross-distribution prior within the unified stochastic interpolant framework, enabling flexible and stable control over multi-distribution-to-noise interpolation paths and decoupled sampling strategies, thereby improving adaptation to OOD reconstruction. By alternating between data fidelity and sampling updates, our model achieves state-of-the-art performance with superior detail preservation in SVCT reconstructions. Extensive experiments demonstrate that CDPIR significantly outperforms existing approaches, particularly under OOD conditions, highlighting its robustness and potential clinical value in challenging imaging scenarios. </p>
<blockquote>
<p>稀疏视图CT（SVCT）重建提高了时间分辨率并降低了辐射剂量，但其临床应用受到视图减少和来自扫描仪、协议或解剖结构变化所导致的域偏移所产生的伪影的阻碍，这导致在分布外（OOD）场景中性能下降。在这项工作中，我们提出了一个跨分布扩散先验驱动迭代重建（CDPIR）框架，以解决SVCT中的OOD问题。CDPIR将跨分布扩散先验与基于模型的迭代重建方法相结合，这些先验是由可扩展插值转换器（SiT）得出的。具体来说，我们训练了一个SiT骨干网，这是扩散转换器（DiT）架构的扩展，以建立一个统一的随机插值框架，利用多个数据集之间的无分类指导（CFG）。通过训练过程中随机丢弃空嵌入作为条件，模型学会了特定领域和跨领域的先验知识，增强了其通用性。在采样过程中，全局敏感性的基于变压器的扩散模型在统一的随机插值框架内利用跨分布先验，实现对多分布到噪声插值路径和解耦采样策略的灵活稳定控制，从而提高了对OOD重建的适应性。通过数据保真度和采样更新之间的交替，我们的模型在SVCT重建中实现了最先进的性能，并出色地保留了细节。大量实验表明，CDPIR显著优于现有方法，尤其是在OOD条件下，突显其在具有挑战性的成像场景中的稳健性和潜在的临床价值。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13576v1">PDF</a> 11 pages, 8 figures, under reviewing of IEEE TMI</p>
<p><strong>Summary</strong></p>
<p>本文提出一种名为CDPIR的跨分布扩散先验驱动迭代重建框架，以解决稀疏视图CT（SVCT）中的跨分布问题。CDPIR结合了来自可扩展插值变换器（SiT）的跨分布扩散先验与模型驱动的迭代重建方法。通过训练SiT骨干——一个基于扩散转换器（DiT）架构的扩展，建立统一随机插值框架，利用多个数据集的无分类器引导（CFG）。模型在训练过程中通过随机丢弃条件并使用空嵌入，学习特定领域和跨领域的先验知识，提高泛化能力。采样时，基于全局敏感变换的扩散模型在统一随机插值框架内利用跨分布先验，实现多分布到噪声插值路径的灵活稳定控制和解耦采样策略，适应跨分布重建。通过交替进行数据保真度和采样更新，我们的模型在SVCT重建中实现了最先进的性能，尤其在细节保留方面表现出卓越的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CDPIR框架被提出以解决稀疏视图CT（SVCT）中的跨分布问题。</li>
<li>跨分布扩散先验与模型驱动的迭代重建方法的结合提高了SVCT的性能。</li>
<li>SiT骨干的建立结合了扩散转换器（DiT）架构，并引入了无分类器引导（CFG）。</li>
<li>训练过程中，模型学习特定领域和跨领域的先验知识，提高泛化能力。</li>
<li>在采样过程中，模型利用跨分布先验实现灵活稳定的插值路径和采样策略。</li>
<li>CDPIR框架通过交替进行数据保真度和采样更新，实现了SVCT重建中的最佳性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13576">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_医学图像/2509.13576v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_医学图像/2509.13576v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_医学图像/2509.13576v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Semantic-3D-Reconstructions-with-SLAM-for-Central-Airway-Obstruction"><a href="#Semantic-3D-Reconstructions-with-SLAM-for-Central-Airway-Obstruction" class="headerlink" title="Semantic 3D Reconstructions with SLAM for Central Airway Obstruction"></a>Semantic 3D Reconstructions with SLAM for Central Airway Obstruction</h2><p><strong>Authors:Ayberk Acar, Fangjie Li, Hao Li, Lidia Al-Zogbi, Kanyifeechukwu Jane Oguine, Susheela Sharma Stern, Jesse F. d’Almeida, Robert J. Webster III, Ipek Oguz, Jie Ying Wu</strong></p>
<p>Central airway obstruction (CAO) is a life-threatening condition with increasing incidence, caused by tumors in and outside of the airway. Traditional treatment methods such as bronchoscopy and electrocautery can be used to remove the tumor completely; however, these methods carry a high risk of complications. Recent advances allow robotic interventions with lesser risk. The combination of robot interventions with scene understanding and mapping also opens up the possibilities for automation. We present a novel pipeline that enables real-time, semantically informed 3D reconstructions of the central airway using monocular endoscopic video.   Our approach combines DROID-SLAM with a segmentation model trained to identify obstructive tissues. The SLAM module reconstructs the 3D geometry of the airway in real time, while the segmentation masks guide the annotation of obstruction regions within the reconstructed point cloud. To validate our pipeline, we evaluate the reconstruction quality using ex vivo models.   Qualitative and quantitative results show high similarity between ground truth CT scans and the 3D reconstructions (0.62 mm Chamfer distance). By integrating segmentation directly into the SLAM workflow, our system produces annotated 3D maps that highlight clinically relevant regions in real time. High-speed capabilities of the pipeline allows quicker reconstructions compared to previous work, reflecting the surgical scene more accurately.   To the best of our knowledge, this is the first work to integrate semantic segmentation with real-time monocular SLAM for endoscopic CAO scenarios. Our framework is modular and can generalize to other anatomies or procedures with minimal changes, offering a promising step toward autonomous robotic interventions. </p>
<blockquote>
<p>中央气道梗阻（CAO）是一种生命威胁性疾病，发病率不断上升，由气道内外肿瘤引起。传统治疗方法如支气管镜检和电烙术可完全切除肿瘤；然而，这些方法并发症风险较高。最近的进展使得机器人干预的风险降低。机器人干预与场景理解和映射的结合也为自动化打开了可能性。我们提出了一种新型管道，能够实现使用单眼内窥镜视频进行中央气道的实时、语义信息3D重建。我们的方法结合了DROID-SLAM和一个经过训练用于识别阻塞性组织的分割模型。SLAM模块实时重建气道的3D几何结构，而分割掩膜引导重建点云中梗阻区域的注释。为了验证我们的管道，我们使用离体模型评估重建质量。定性和定量结果表明，地面真实CT扫描和3D重建之间具有高度相似性（0.62毫米Chamfer距离）。通过将分割直接集成到SLAM工作流程中，我们的系统产生注释的3D地图，可实时突出显示临床上相关的区域。管道的高速能力可实现与先前工作相比更快的重建，更准确地反映手术场景。据我们所知，这是第一个将语义分割与实时单眼SLAM相结合用于内窥镜CAO场景的工作。我们的框架是模块化的，并且可以通过最小的更改概括到其他解剖结构或程序，是朝着自主机器人干预的有希望的一步。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13541v1">PDF</a> 5 pages, 2 figures, 1 table</p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种基于单目内窥镜视频进行中央气道实时、语义信息丰富的三维重建的新型管道。该管道结合了DROID-SLAM和一个用于识别阻塞性组织的分割模型，实现了气道的实时三维几何重建，并通过分割掩膜指导阻塞区域的标注。实验验证显示，该管道重建结果与真实CT扫描高度相似。此外，该管道还具有模块化特点，可广泛应用于其他解剖结构或手术场景，为自主机器人干预提供了有前途的一步。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>中央气道阻塞（CAO）是威胁生命的疾病，其发病率正在上升，通常由气道内外肿瘤引起。</li>
<li>传统治疗方法如支气管镜和电烙术可以完全去除肿瘤，但并发症风险较高。</li>
<li>新型机器人干预技术风险较低，并与场景理解和映射结合，为自动化提供了可能性。</li>
<li>提出了一种基于单目内窥镜视频进行中央气道实时三维重建的管道，结合DROID-SLAM和分割模型实现精准重建。</li>
<li>重建结果与真实CT扫描高度相似，定量评估显示Chamfer距离为0.62毫米。</li>
<li>该管道通过将分割直接集成到SLAM工作流程中，产生注释的三维地图，实时突出显示临床相关区域。</li>
<li>管道的高速特性允许更快的重建，更准确地反映手术场景，并且是模块化的，可轻松应用于其他解剖结构或手术程序。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13541">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_医学图像/2509.13541v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_医学图像/2509.13541v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_医学图像/2509.13541v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_医学图像/2509.13541v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="FunKAN-Functional-Kolmogorov-Arnold-Network-for-Medical-Image-Enhancement-and-Segmentation"><a href="#FunKAN-Functional-Kolmogorov-Arnold-Network-for-Medical-Image-Enhancement-and-Segmentation" class="headerlink" title="FunKAN: Functional Kolmogorov-Arnold Network for Medical Image   Enhancement and Segmentation"></a>FunKAN: Functional Kolmogorov-Arnold Network for Medical Image   Enhancement and Segmentation</h2><p><strong>Authors:Maksim Penkin, Andrey Krylov</strong></p>
<p>Medical image enhancement and segmentation are critical yet challenging tasks in modern clinical practice, constrained by artifacts and complex anatomical variations. Traditional deep learning approaches often rely on complex architectures with limited interpretability. While Kolmogorov-Arnold networks offer interpretable solutions, their reliance on flattened feature representations fundamentally disrupts the intrinsic spatial structure of imaging data. To address this issue we propose a Functional Kolmogorov-Arnold Network (FunKAN) – a novel interpretable neural framework, designed specifically for image processing, that formally generalizes the Kolmogorov-Arnold representation theorem onto functional spaces and learns inner functions using Fourier decomposition over the basis Hermite functions. We explore FunKAN on several medical image processing tasks, including Gibbs ringing suppression in magnetic resonance images, benchmarking on IXI dataset. We also propose U-FunKAN as state-of-the-art binary medical segmentation model with benchmarks on three medical datasets: BUSI (ultrasound images), GlaS (histological structures) and CVC-ClinicDB (colonoscopy videos), detecting breast cancer, glands and polyps, respectively. Experiments on those diverse datasets demonstrate that our approach outperforms other KAN-based backbones in both medical image enhancement (PSNR, TV) and segmentation (IoU, F1). Our work bridges the gap between theoretical function approximation and medical image analysis, offering a robust, interpretable solution for clinical applications. </p>
<blockquote>
<p>医学图像增强和分割是现代临床实践中既重要又具挑战性的任务，受到伪影和复杂解剖结构变化的影响。传统的深度学习方法往往依赖于具有有限解释性的复杂架构。尽管Kolmogorov-Arnold网络提供了可解释的解决方案，但它们对扁平特征表示的依赖从根本上破坏了成像数据的内在空间结构。为了解决这个问题，我们提出了一种功能Kolmogorov-Arnold网络（FunKAN）——一种专门用于图像处理的全新可解释神经网络框架，它正式将Kolmogorov-Arnold表示定理推广到功能空间，并使用Hermite函数的基函数的傅里叶分解来学习内部函数。我们在多个医学图像处理任务上探索了FunKAN，包括对磁共振图像中的Gibbs振铃进行抑制，并在IXI数据集上进行基准测试。我们还提出了U-FunKAN，这是最先进的二元医学分割模型，在三个医学数据集上进行基准测试：BUSI（超声图像）、GlaS（组织结构）和CVC-ClinicDB（结肠镜检查视频），分别检测乳腺癌、腺体和多发性息肉。在这些不同数据集上的实验表明，我们的方法在医学图像增强（PSNR，TV）和分割（IoU，F1）方面都优于其他KAN基础方法。我们的工作缩小了理论函数逼近和医学图像分析之间的差距，为临床应用提供了稳健、可解释的解决方案。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13508v1">PDF</a> 9 pages, 5 figures, submitted to the Fortieth AAAI Conference on   Artificial Intelligence (AAAI-26)</p>
<p><strong>Summary</strong></p>
<p>医学图像增强与分割是现代临床实践中重要且具有挑战性的任务，受到伪影和复杂解剖结构变化的影响。为解决这个问题，本文提出了一种名为Functional Kolmogorov-Arnold Network（FunKAN）的新型神经网络框架，该框架在功能空间上推广了Kolmogorov-Arnold表示定理，并利用Hermite函数的傅立叶分解来学习内在函数。该框架用于磁共振图像中的Gibbs伪影抑制等医学图像处理任务，并在IXI数据集上进行基准测试。同时，本文还提出了U-FunKAN作为最先进的二元医学分割模型，并在BUSI（超声图像）、GlaS（组织结构）和CVC-ClinicDB（结肠镜检查视频）三个医学数据集上进行基准测试，分别检测乳腺癌、腺体和多发性息肉。实验结果表明，该方法在医学图像增强（PSNR，TV）和分割（IoU，F1）方面优于其他KAN基础模型。本文缩小了理论函数逼近与医学图像分析之间的差距，为临床应用提供了稳健、可解释的解决方案。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>医学图像增强与分割是临床实践中重要的任务，面临伪影和复杂解剖结构变化的挑战。</li>
<li>传统深度学习方法依赖复杂架构，而Kolmogorov-Arnold网络虽然具有解释性，但会破坏成像数据的内在空间结构。</li>
<li>提出了Functional Kolmogorov-Arnold Network (FunKAN) 新型神经网络框架，旨在解决医学图像处理问题。</li>
<li>FunKAN能够在磁共振图像中抑制Gibbs伪影，并在IXI数据集上进行基准测试。</li>
<li>U-FunKAN作为最先进的二元医学分割模型提出，用于检测乳腺癌、腺体和多发性息肉。</li>
<li>U-FunKAN在BUSI、GlaS和CVC-ClinicDB三个医学数据集上进行了基准测试。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13508">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_医学图像/2509.13508v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_医学图像/2509.13508v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_医学图像/2509.13508v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_医学图像/2509.13508v1/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_医学图像/2509.13508v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_医学图像/2509.13508v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="PREDICT-GBM-Platform-for-Robust-Evaluation-and-Development-of-Individualized-Computational-Tumor-Models-in-Glioblastoma"><a href="#PREDICT-GBM-Platform-for-Robust-Evaluation-and-Development-of-Individualized-Computational-Tumor-Models-in-Glioblastoma" class="headerlink" title="PREDICT-GBM: Platform for Robust Evaluation and Development of   Individualized Computational Tumor Models in Glioblastoma"></a>PREDICT-GBM: Platform for Robust Evaluation and Development of   Individualized Computational Tumor Models in Glioblastoma</h2><p><strong>Authors:L. Zimmer, J. Weidner, M. Balcerak, F. Kofler, I. Ezhov, B. Menze, B. Wiestler</strong></p>
<p>Glioblastoma is the most prevalent primary brain malignancy, distinguished by its highly invasive behavior and exceptionally high rates of recurrence. Conventional radiation therapy, which employs uniform treatment margins, fails to account for patient-specific anatomical and biological factors that critically influence tumor cell migration. To address this limitation, numerous computational models of glioblastoma growth have been developed, enabling generation of tumor cell distribution maps extending beyond radiographically visible regions and thus informing more precise treatment strategies. However, despite encouraging preliminary findings, the clinical adoption of these growth models remains limited. To bridge this translational gap and accelerate both model development and clinical validation, we introduce PREDICT-GBM, a comprehensive integrated pipeline and dataset for modeling and evaluation. This platform enables systematic benchmarking of state-of-the-art tumor growth models using an expert-curated clinical dataset comprising 255 subjects with complete tumor segmentations and tissue characterization maps. Our analysis demonstrates that personalized radiation treatment plans derived from tumor growth predictions achieved superior recurrence coverage compared to conventional uniform margin approaches for two of the evaluated models. This work establishes a robust platform for advancing and systematically evaluating cutting-edge tumor growth modeling approaches, with the ultimate goal of facilitating clinical translation and improving patient outcomes. </p>
<blockquote>
<p>胶质母细胞瘤是最常见的主要脑恶性肿瘤，其特点是具有高度侵袭性和极高的复发率。传统的放射治疗采用均匀的治疗边界，忽视了特定患者的解剖和生物学因素，这些因素对肿瘤细胞迁移有重要影响。为了解决这一局限性，已经开发了许多胶质母细胞瘤生长的计算模型，能够生成超越放射学可见区域的肿瘤细胞分布图，从而为更精确的治疗策略提供信息。然而，尽管初步发现令人鼓舞，但这些生长模型的临床应用仍然有限。为了弥合这一转化差距并加速模型开发和临床验证，我们引入了PREDICT-GBM，这是一个用于建模和评估的综合集成管道和数据集。该平台使用专家编制的临床数据集，其中包括255名患者的完整肿瘤分割和组织特征图，能够对最先进的肿瘤生长模型进行系统性的基准测试。我们的分析表明，根据肿瘤生长预测制定的个性化放射治疗计划，对于所评估的两个模型来说，相较于传统均匀边界方法实现了更高的复发覆盖。这项工作建立了一个稳健的平台，用于推进和系统评估前沿的肿瘤生长建模方法，最终目标是通过改善临床翻译和提高患者疗效来实现突破。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13360v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了胶质母细胞瘤是一种常见且易于复发的原发性脑恶性肿瘤。传统的放射治疗无法考虑到患者特定的解剖和生物学因素，影响治疗效果。为了解决这个问题，已经开发了许多胶质母细胞瘤生长的计算模型，以生成肿瘤细胞的分布图并预测肿瘤生长，从而为更精确的治疗策略提供依据。然而，这些模型的临床应用仍然有限。为了缩小差距并加速模型开发和临床验证，引入了一个名为PREDICT-GBM的综合管道和数据集。此平台能够系统地评估最先进的肿瘤生长模型，并使用包含255名患者的专家分类临床数据集进行验证。分析表明，基于肿瘤生长预测制定的个性化放射治疗计划在某些模型中实现了优于传统均匀边界方法的复发覆盖率。这为推进和系统评估前沿的肿瘤生长建模方法提供了一个稳健的平台，最终目标是为临床翻译和改善患者预后提供便利。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>胶质母细胞瘤是常见且易复发的原发性脑恶性肿瘤。</li>
<li>传统放射治疗未考虑患者特定的解剖和生物学因素。</li>
<li>计算模型用于预测胶质母细胞瘤的生长，以提供更精确的治疗策略。</li>
<li>PREDICT-GBM平台旨在推进和系统评估肿瘤生长建模方法。</li>
<li>该平台使用了包含255名患者的专家分类临床数据集进行验证。</li>
<li>基于肿瘤生长预测制定的个性化放射治疗计划在某些模型中显示出更高的复发覆盖率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13360">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_医学图像/2509.13360v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_医学图像/2509.13360v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_医学图像/2509.13360v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Data-Efficient-Fine-Tuning-of-Vision-Language-Models-for-Diagnosis-of-Alzheimer’s-Disease"><a href="#Data-Efficient-Fine-Tuning-of-Vision-Language-Models-for-Diagnosis-of-Alzheimer’s-Disease" class="headerlink" title="Data-Efficient Fine-Tuning of Vision-Language Models for Diagnosis of   Alzheimer’s Disease"></a>Data-Efficient Fine-Tuning of Vision-Language Models for Diagnosis of   Alzheimer’s Disease</h2><p><strong>Authors:Fangqi Cheng, Surajit Ray, Xiaochen Yang</strong></p>
<p>Medical vision-language models (Med-VLMs) have shown impressive results in tasks such as report generation and visual question answering, but they still face several limitations. Most notably, they underutilize patient metadata and lack integration of clinical diagnostic knowledge. Moreover, most existing models are typically trained from scratch or fine-tuned on large-scale 2D image-text pairs, requiring extensive computational resources, and their effectiveness on 3D medical imaging is often limited due to the absence of structural information. To address these gaps, we propose a data-efficient fine-tuning pipeline to adapt 3D CT-based Med-VLMs for 3D MRI and demonstrate its application in Alzheimer’s disease (AD) diagnosis. Our system introduces two key innovations. First, we convert structured metadata into synthetic reports, enriching textual input for improved image-text alignment. Second, we add an auxiliary token trained to predict the mini-mental state examination (MMSE) score, a widely used clinical measure of cognitive function that correlates with AD severity. This provides additional supervision for fine-tuning. Applying lightweight prompt tuning to both image and text modalities, our approach achieves state-of-the-art performance on two AD datasets using 1,500 training images, outperforming existing methods fine-tuned on 10,000 images. Code will be released upon publication. </p>
<blockquote>
<p>医疗视觉语言模型（Med-VLMs）在报告生成和视觉问答等任务中展现出了令人印象深刻的结果，但它们仍然面临一些局限性。最显著的是，它们对病人元数据的利用不足，且缺乏临床诊断知识的整合。此外，大多数现有模型通常是从头开始训练或在大量2D图像文本对上微调，需要大量的计算资源，它们在3D医学成像上的应用效果往往由于缺少结构信息而受限。为了解决这些差距，我们提出了一种数据高效微调管道，以适应基于3D CT的Med-VLMs用于3D MRI，并展示了其在阿尔茨海默病（AD）诊断中的应用。我们的系统引入了两个关键创新点。首先，我们将结构化元数据转换为合成报告，丰富文本输入，以改进图像文本对齐。其次，我们添加了一个辅助令牌，用于预测迷你精神状态检查表（MMSE）得分，这是一个广泛应用于临床的认知功能测量表，与AD严重程度相关。这为微调提供了额外的监督。通过对图像和文本模态应用轻量级提示调整，我们的方法在两个AD数据集上使用1500张训练图像达到了最先进的性能，优于在10000张图像上微调的现有方法。代码将在发布时公布。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.07613v2">PDF</a> </p>
<p><strong>Summary</strong><br>医学视觉语言模型（Med-VLMs）在报告生成和视觉问答等任务中表现出令人印象深刻的结果，但仍存在几个局限性。最主要的是，它们未充分利用患者元数据并缺乏临床诊断知识的整合。针对这些不足，我们提出了一种数据高效的微调管道，以适应基于3D CT的Med-VLMs用于3D MRI，并展示了其在阿尔茨海默病（AD）诊断中的应用。我们的系统包含两个关键创新点：首先，我们将结构化元数据转换为合成报告，丰富文本输入以提高图像文本对齐；其次，我们添加了一个用于预测微小精神状态检查（MMSE）得分的辅助标记符号进行微调监督，作为衡量认知功能的常用指标，它与AD的严重程度密切相关。这种方法仅在数千个训练图像上实现了一项应用方面的最前沿成果，可预测更高级的图像处理技术要求精细推理的数据集的表现，超越在现有方法的十倍训练数据上的表现。该方法的实现代码将在出版时公开发布。对于注意力机制和结构化的元数据的价值有了更深入的见解。这种精细调整的语言模型能更准确地解释和理解复杂的医学图像数据，这对于诊断和医疗决策具有至关重要的意义。它提供了更大的可能性，即通过机器学习模型更精确地分析阿尔茨海默病等神经性疾病的早期症状。总体而言，该模型显著提高了对医学图像信息的理解准确性并减少了训练成本。因此它有助于更有效地利用资源并提供更好的医疗服务。这开启了人工智能在医学领域的新的里程碑。医学图像理解和报告生成等任务的精确度和效率都获得了显著的提升和前景明朗的应用发展轨迹都指明了我们的进步对未来精确诊断和相关临床应用产生的影响意义重大值得期待通过减轻其面对的各种限制因素的措施将其实际应用得到进一步扩大是我们后续的研究方向将充分发挥出其更大的潜力推进医疗保健技术的进步从而为未来的患者提供更高效的服务减少治疗成本和提高治疗效率。同时我们还将致力于开发更加智能的算法以应对未来可能出现的挑战和机遇提高诊断的准确性同时也进一步拓展其应用范围以实现更加广泛的医疗服务从而为广大患者带来实实在在的利益这也是我们的目标所在。”这是对我们未来研究和工作的一种强有力的鞭策。”接下来我将根据这些信息提取关键要点并进行简明扼要的解释以帮助理解。虽然这个过程非常复杂需要各种技能和知识背景知识的支撑但我仍然能够确保分析和解释的准确性和清晰性以满足你的需求并努力达成你的期望目标。<strong>Key Takeaways</strong>:</p>
<ol>
<li>Med-VLMs在医学图像相关任务中表现出色，但在利用患者元数据和整合临床知识方面存在不足。</li>
<li>提出的微调管道适应了基于3D CT的Med-VLMs用于处理3D MRI数据，用于阿尔茨海默病的诊断。</li>
<li>系统创新点包括利用结构化元数据生成合成报告以增强图像文本对齐，以及通过预测MMSE得分提供微调监督。</li>
<li>该方法在阿尔茨海默病数据集上实现了卓越性能，使用较少的训练图像超越了使用更多图像进行训练的方法。</li>
<li>该方法有助于更准确地理解和解释医学图像数据，对于精确诊断和医疗决策至关重要。</li>
<li>未来的研究方向包括扩大模型的实际应用范围和开发更智能的算法以提高诊断准确性及拓展应用范围。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.07613">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_医学图像/2509.07613v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_医学图像/2509.07613v2/page_3_0.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Sample-Aware-Test-Time-Adaptation-for-Medical-Image-to-Image-Translation"><a href="#Sample-Aware-Test-Time-Adaptation-for-Medical-Image-to-Image-Translation" class="headerlink" title="Sample-Aware Test-Time Adaptation for Medical Image-to-Image Translation"></a>Sample-Aware Test-Time Adaptation for Medical Image-to-Image Translation</h2><p><strong>Authors:Irene Iele, Francesco Di Feola, Valerio Guarrasi, Paolo Soda</strong></p>
<p>Image-to-image translation has emerged as a powerful technique in medical imaging, enabling tasks such as image denoising and cross-modality conversion. However, it suffers from limitations in handling out-of-distribution samples without causing performance degradation. To address this limitation, we propose a novel Test-Time Adaptation (TTA) framework that dynamically adjusts the translation process based on the characteristics of each test sample. Our method introduces a Reconstruction Module to quantify the domain shift and a Dynamic Adaptation Block that selectively modifies the internal features of a pretrained translation model to mitigate the shift without compromising the performance on in-distribution samples that do not require adaptation. We evaluate our approach on two medical image-to-image translation tasks: low-dose CT denoising and T1 to T2 MRI translation, showing consistent improvements over both the baseline translation model without TTA and prior TTA methods. Our analysis highlights the limitations of the state-of-the-art that uniformly apply the adaptation to both out-of-distribution and in-distribution samples, demonstrating that dynamic, sample-specific adjustment offers a promising path to improve model resilience in real-world scenarios. The code is available at: <a target="_blank" rel="noopener" href="https://github.com/Sample-Aware-TTA/Code">https://github.com/Sample-Aware-TTA/Code</a>. </p>
<blockquote>
<p>图像到图像的转换技术已成为医学成像中的一项强大技术，能够完成图像去噪和跨模态转换等任务。然而，它在处理离群样本时存在局限，可能导致性能下降。为了解决这一局限，我们提出了一种新颖的测试时间适应（TTA）框架，该框架根据每个测试样本的特性动态调整翻译过程。我们的方法引入了一个重建模块来量化域差异和一个动态适应块，该块有选择地修改预训练翻译模型的内部特征，以减轻域差异的影响，同时不损害对不需要适应的离群样本的性能。我们在两个医学图像到图像的翻译任务上评估了我们的方法：低剂量CT去噪和T1到T2 MRI翻译，显示出相较于没有TTA的基线翻译模型和先前的TTA方法的一致改进。我们的分析强调了当前主流技术的局限性，即对离群样本和离群样本统一应用适应策略，表明动态、针对样本的特定调整是提高模型在现实场景中的稳健性的有前途的途径。代码可在：<a target="_blank" rel="noopener" href="https://github.com/Sample-Aware-TTA/Code%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/Sample-Aware-TTA/Code获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.00766v2">PDF</a> </p>
<p><strong>Summary</strong><br>     医学图像领域中的图像到图像翻译技术虽可实现图像去噪和跨模态转换等任务，但在处理离群样本时存在性能下降的局限性。为此，我们提出一种新颖的Test-Time Adaptation（TTA）框架，可基于每个测试样本的特性动态调整翻译过程。通过引入重建模块来衡量域偏移，并使用动态适配块选择性修改预训练翻译模型的内部特征，以减轻偏移影响，同时不损害对不需要适配的离群样本的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>医学图像翻译技术面临处理离群样本时的性能下降问题。</li>
<li>提出Test-Time Adaptation（TTA）框架，可动态调整翻译过程以适应不同测试样本。</li>
<li>引入重建模块来衡量域偏移。</li>
<li>通过动态适配块选择性修改预训练翻译模型的内部特征，以减轻偏移影响。</li>
<li>在低剂量CT去噪和T1到T2 MRI翻译两个任务上进行了评估，较基线翻译模型和现有TTA方法有所改进。</li>
<li>分析指出统一适配离群样本和离群样本的现有方法存在局限性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.00766">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_医学图像/2508.00766v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_医学图像/2508.00766v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="MEGANet-W-A-Wavelet-Driven-Edge-Guided-Attention-Framework-for-Weak-Boundary-Polyp-Detection"><a href="#MEGANet-W-A-Wavelet-Driven-Edge-Guided-Attention-Framework-for-Weak-Boundary-Polyp-Detection" class="headerlink" title="MEGANet-W: A Wavelet-Driven Edge-Guided Attention Framework for Weak   Boundary Polyp Detection"></a>MEGANet-W: A Wavelet-Driven Edge-Guided Attention Framework for Weak   Boundary Polyp Detection</h2><p><strong>Authors:Zhe Yee Tan, Ashwaq Qasem</strong></p>
<p>Colorectal polyp segmentation is critical for early detection of colorectal cancer, yet weak and low contrast boundaries significantly limit automated accuracy. Existing deep models either blur fine edge details or rely on handcrafted filters that perform poorly under variable imaging conditions. We propose MEGANet-W, a Wavelet Driven Edge Guided Attention Network that injects directional, parameter free Haar wavelet edge maps into each decoder stage to recalibrate semantic features. The key novelties of MEGANet-W include a two-level Haar wavelet head for multi-orientation edge extraction; and Wavelet Edge Guided Attention (W-EGA) modules that fuse wavelet cues with boundary and input branches. On five public polyp datasets, MEGANet-W consistently outperforms existing methods, improving mIoU by up to 2.3% and mDice by 1.2%, while introducing no additional learnable parameters. This approach improves reliability in difficult cases and offers a robust solution for medical image segmentation tasks requiring precise boundary detection. </p>
<blockquote>
<p>结肠息肉分割对结直肠癌的早期检测至关重要，但弱边界和低对比度边界显著限制了自动化精度。现有的深度模型要么模糊精细边缘细节，要么依赖于在可变成像条件下表现不佳的手工过滤器。我们提出了MEGANet-W，这是一种基于小波驱动的边缘引导注意力网络，它将方向性、无参数的Haar小波边缘图注入到每个解码器阶段以重新校准语义特征。MEGANet-W的主要新颖性包括用于多方向边缘提取的两级Haar小波头；以及小波边缘引导注意力（W-EGA）模块，该模块将小波线索与边界和输入分支融合。在五个公共息肉数据集上，MEGANet-W始终优于现有方法，mIoU提高最多达2.3%，mDice提高1.2%，同时没有引入任何额外的可学习参数。该方法在困难情况下提高了可靠性，并为需要精确边界检测的医学图像分割任务提供了稳健的解决方案。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.02668v4">PDF</a> This work has been submitted to the IEEE for possible publication</p>
<p><strong>Summary</strong><br>医学图像中结肠息肉分割对于早期检测结直肠癌至关重要，但边界模糊和对比度低影响自动化分割精度。现有深度模型存在边缘细节模糊或在不同成像条件下表现不佳的问题。本研究提出MEGANet-W模型，通过小波驱动的边引导注意力机制来改进分割效果。该模型采用两级Haar小波头进行多方向边缘提取，以及小波边引导注意力模块融合小波线索与边界和输入分支。在五个公共息肉数据集上的实验表明，MEGANet-W显著提高了现有方法的性能，平均交并比（mIoU）提高最多达2.3%，平均Dice系数提高1.2%，且没有增加额外的可学习参数。此方法提高了困难病例的可靠性，并为需要精确边界检测的医学图像分割任务提供了稳健解决方案。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>医学图像中结肠息肉的自动分割对早期结直肠癌检测至关重要，但边界模糊和对比度低限制了自动化分割的准确性。</li>
<li>现有深度模型在应对模糊边界和多变成像条件时表现欠佳。</li>
<li>MEGANet-W模型通过结合小波理论和边引导注意力机制来提高医学图像分割的精度和可靠性。</li>
<li>MEGANet-W采用两级Haar小波头进行多方向边缘提取，增强模型对边缘细节的捕捉能力。</li>
<li>小波边引导注意力模块融合小波线索、边界信息和输入数据，提高了模型的性能。</li>
<li>在多个公共数据集上的实验表明，MEGANet-W在医学图像分割任务上表现出优异的性能，相较于现有方法有所提高。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.02668">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_医学图像/2507.02668v4/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_医学图像/2507.02668v4/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_医学图像/2507.02668v4/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_医学图像/2507.02668v4/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_医学图像/2507.02668v4/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_医学图像/2507.02668v4/page_5_1.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="A-Unified-Benchmark-of-Federated-Learning-with-Kolmogorov-Arnold-Networks-for-Medical-Imaging"><a href="#A-Unified-Benchmark-of-Federated-Learning-with-Kolmogorov-Arnold-Networks-for-Medical-Imaging" class="headerlink" title="A Unified Benchmark of Federated Learning with Kolmogorov-Arnold   Networks for Medical Imaging"></a>A Unified Benchmark of Federated Learning with Kolmogorov-Arnold   Networks for Medical Imaging</h2><p><strong>Authors:Youngjoon Lee, Jinu Gong, Joonhyuk Kang</strong></p>
<p>Federated Learning (FL) enables model training across decentralized devices without sharing raw data, thereby preserving privacy in sensitive domains like healthcare. In this paper, we evaluate Kolmogorov-Arnold Networks (KAN) architectures against traditional MLP across six state-of-the-art FL algorithms on a blood cell classification dataset. Notably, our experiments demonstrate that KAN can effectively replace MLP in federated environments, achieving superior performance with simpler architectures. Furthermore, we analyze the impact of key hyperparameters-grid size and network architecture-on KAN performance under varying degrees of Non-IID data distribution. In addition, our ablation studies reveal that optimizing KAN width while maintaining minimal depth yields the best performance in federated settings. As a result, these findings establish KAN as a promising alternative for privacy-preserving medical imaging applications in distributed healthcare. To the best of our knowledge, this is the first comprehensive benchmark of KAN in FL settings for medical imaging task. </p>
<blockquote>
<p>联邦学习（FL）能够在分散的设备上进行模型训练，无需共享原始数据，从而保护医疗等敏感领域的隐私。在本文中，我们评估了Kolmogorov-Arnold网络（KAN）架构与传统多层感知机（MLP）在六种最先进的联邦学习算法上的表现，这些算法是基于血细胞分类数据集。值得注意的是，我们的实验表明，在联邦环境中，KAN可以有效地替代MLP，在架构更简单的情况下实现卓越的性能。此外，我们分析了关键超参数——网格大小和网络架构在不同程度的非独立同分布（Non-IID）数据分布下对KAN性能的影响。另外，我们的消融研究表明，在保持深度最小的情况下优化KAN的宽度，在联邦环境中可以获得最佳性能。因此，这些发现证明了KAN在分布式医疗护理的隐私保护医学影像应用中的巨大潜力。据我们所知，这是首次在联邦学习环境下对KAN进行医学影像任务的全面基准测试。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.19639v2">PDF</a> Accepted to AI&#x2F;ML for Edge&#x2F;Fog Networks Workshop - IEEE GLOBECOM 2025</p>
<p><strong>Summary</strong></p>
<p>基于联邦学习（FL）的Kolmogorov-Arnold网络（KAN）在血液细胞分类数据集上进行了评估。实验表明，与传统多层感知机（MLP）相比，KAN在联邦环境中表现更优，且架构更简单。研究还分析了关键超参数对KAN性能的影响，并发现优化KAN宽度同时保持深度最小能在联邦环境中获得最佳性能。这些发现使KAN成为分布式医疗保健中隐私保护医学影像应用的有前途的替代方案。据我们所知，这是首次针对医学成像任务在联邦学习环境中全面评估KAN。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>联邦学习（FL）允许在不共享原始数据的情况下进行模型训练，保护隐私。<br>2.Kolmogorov-Arnold网络（KAN）在血液细胞分类数据集上的性能优于传统多层感知机（MLP）。</li>
<li>在联邦环境中，KAN架构更简单，可实现优越性能。</li>
<li>关键超参数如网格大小和网络架构对KAN性能有影响。</li>
<li>在非独立同分布（Non-IID）数据环境下，优化KAN宽度并保持深度最小可获得最佳性能。</li>
<li>KAN在隐私保护的医学影像应用中有潜力成为分布式医疗保健的替代方案。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.19639">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_医学图像/2504.19639v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_医学图像/2504.19639v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_医学图像/2504.19639v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_医学图像/2504.19639v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_医学图像/2504.19639v2/page_4_0.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Y-AR-A-Mixed-Reality-CAD-Tool-for-3D-Wire-Bending"><a href="#Y-AR-A-Mixed-Reality-CAD-Tool-for-3D-Wire-Bending" class="headerlink" title="Y-AR: A Mixed Reality CAD Tool for 3D Wire Bending"></a>Y-AR: A Mixed Reality CAD Tool for 3D Wire Bending</h2><p><strong>Authors:Shuo Feng, Bo Liu,  Yifan,  Shan, Roy Zunder, Wei-Che Lin, Tri Dinh, Harald Haraldsson, Ofer Berman, Thijs Roumen</strong></p>
<p>Wire bending is a technique used in manufacturing to mass-produce items such as clips, mounts, and braces. Recent advances in programmable wire bending have made this process increasingly accessible for custom fabrication. However, CNC wire benders are controlled using Computer Aided Manufacturing (CAM) software, without design tools, making custom designs challenging to produce. We present Y-AR, a Computer Aided Design (CAD) interface for 3D wire bending. Y-AR uses mixed reality to let designers create clips, mounts, and braces to physically connect objects to their surrounding environment. The interface incorporates springs as design primitives which (1) apply forces to hold objects, and (2) counter-act dimensional inaccuracies inherently caused by mid-air modeling and measurement errors in AR. Springs are a natural design element when working with metal wire-bending given its specific material properties. We demonstrate workflows to design and fabricate a range of mechanisms in Y-AR as well as structures made using free-hand design tools. We found that combining gesture-based interaction with fabrication-aware design principles allowed novice users to create functional wire connectors, even when using imprecise XR-based input. In our usability evaluation, all 12 participants successfully designed and fabricated a functional bottle holder using Y-AR. </p>
<blockquote>
<p>弯曲金属线制造是一种在制造业中用于批量生产夹子、支架和箍的技术。随着可编程弯曲金属线技术的最新发展，这一过程对于定制制造变得越来越容易接近。然而，CNC金属线弯曲机是通过计算机辅助制造（CAM）软件控制的，没有设计工具，使得定制设计的生产具有挑战性。我们提出了Y-AR，这是一个用于3D金属线弯曲的计算机辅助设计（CAD）界面。Y-AR使用混合现实技术，让设计师能够创建夹子、支架和箍，以将物体物理连接到其周围环境。该界面采用弹簧作为设计元素，弹簧（1）施加力来固定物体，（2）抵消由于空中建模和测量误差固有产生的尺寸误差。在与金属线弯曲相结合工作时，弹簧作为一种自然设计元素因其特定的材料特性而发挥作用。我们展示了在Y-AR中设计和制作一系列机制的工作流程，以及使用自由手绘工具制作的结构。我们发现，将基于手势的交互与面向制造的设计原则相结合，即使使用不精确的XR输入，新手用户也能创建出功能性的金属线连接器。在我们的可用性评估中，所有12名参与者都成功使用Y-AR设计和制作了一个功能性的瓶架。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.23540v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该文章介绍了用于制造业的大规模生产物品（如夹子、支架和撑架）的一种弯曲金属丝技术——Y-AR。近年来，可编程的金属丝弯曲技术让这一工艺对于定制制造变得更具可访问性。然而，数控金属丝弯曲器通过计算机辅助制造软件进行控制，没有设计工具，因此制作定制设计具有挑战性。因此文章提出了一个名为Y-AR的计算机辅助设计界面，该界面结合了增强现实技术，允许设计师创建用于将物体与其周围环境连接的夹子、支架和撑杆。该界面采用弹簧作为设计元素，通过弹簧力来固定物体并抵消因空中建模和测量误差造成的尺寸误差。此外，文章展示了在Y-AR中进行设计和制作的一系列工作流程，以及使用自由手绘工具构建的结构。最后，通过可用性评估发现，结合了基于手势的交互和制造意识的设计原则后，即使是新手用户也能创造出实用的金属丝连接器。所有参与者都成功使用Y-AR设计和制作了一个实用的瓶架。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Y-AR是一个结合了增强现实技术的计算机辅助设计界面，主要用于创建用于物体连接的金属丝产品（如夹子、支架和撑杆）。</li>
<li>该界面利用弹簧作为设计元素，利用其物理特性应对制作过程中可能出现的误差。</li>
<li>Y-AR允许设计者进行自由手绘设计，并展示了多种工作流程来创建各种机制结构。</li>
<li>文章强调了可编程金属丝弯曲技术的最新进展及其在定制制造中的应用。</li>
<li>结合手势交互和制造意识的设计原则，新手用户也能使用Y-AR创建实用的金属丝连接器。</li>
<li>Y-AR的可用性得到了验证，所有参与者均成功设计和制作了一个瓶架。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.23540">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_医学图像/2410.23540v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_医学图像/2410.23540v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_医学图像/2410.23540v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_医学图像/2410.23540v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_医学图像/2410.23540v2/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_医学图像/2410.23540v2/page_5_1.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="DiffGAN-A-Test-Generation-Approach-for-Differential-Testing-of-Deep-Neural-Networks-for-Image-Analysis"><a href="#DiffGAN-A-Test-Generation-Approach-for-Differential-Testing-of-Deep-Neural-Networks-for-Image-Analysis" class="headerlink" title="DiffGAN: A Test Generation Approach for Differential Testing of Deep   Neural Networks for Image Analysis"></a>DiffGAN: A Test Generation Approach for Differential Testing of Deep   Neural Networks for Image Analysis</h2><p><strong>Authors:Zohreh Aghababaeyan, Manel Abdellatif, Lionel Briand, Ramesh S</strong></p>
<p>Deep Neural Networks (DNNs) are increasingly deployed across applications. However, ensuring their reliability remains a challenge, and in many situations, alternative models with similar functionality and accuracy are available. Traditional accuracy-based evaluations often fail to capture behavioral differences between models, especially with limited test datasets, making it difficult to select or combine models effectively. Differential testing addresses this by generating test inputs that expose discrepancies in DNN model behavior. However, existing approaches face significant limitations: many rely on model internals or are constrained by available seed inputs. To address these challenges, we propose DiffGAN, a black-box test image generation approach for differential testing of DNN models. DiffGAN leverages a Generative Adversarial Network (GAN) and the Non-dominated Sorting Genetic Algorithm II to generate diverse and valid triggering inputs that reveal behavioral discrepancies between models. DiffGAN employs two custom fitness functions, focusing on diversity and divergence, to guide the exploration of the GAN input space and identify discrepancies between models’ outputs. By strategically searching this space, DiffGAN generates inputs with specific features that trigger differences in model behavior. DiffGAN is black-box, making it applicable in more situations. We evaluate DiffGAN on eight DNN model pairs trained on widely used image datasets. Our results show DiffGAN significantly outperforms a SOTA baseline, generating four times more triggering inputs, with greater diversity and validity, within the same budget. Additionally, the generated inputs improve the accuracy of a machine learning-based model selection mechanism, which selects the best-performing model based on input characteristics and can serve as a smart output voting mechanism when using alternative models. </p>
<blockquote>
<p>深度神经网络（DNNs）在各种应用中部署得越来越广泛。然而，确保它们的可靠性仍然是一个挑战，而且在许多情况下，存在具有相似功能和准确性的替代模型。传统的基于准确性的评估往往无法捕捉模型之间的行为差异，尤其是在有限的测试数据集下，这使得有效选择或组合模型变得困难。差异测试通过生成暴露DNN模型行为差异的测试输入来解决这个问题。然而，现有方法存在重大局限性：许多方法依赖于模型内部，或受到可用种子输入的约束。为了解决这些挑战，我们提出了DiffGAN，这是一种用于DNN模型差异测试的黑盒测试图像生成方法。DiffGAN利用生成对抗网络（GAN）和非支配排序遗传算法II来生成多样且有效的触发输入，这些输入揭示了模型之间的行为差异。DiffGAN采用两个自定义的适应度函数，专注于多样性和发散性，以指导GAN输入空间的探索，并识别模型输出之间的差异。通过有针对性地搜索这个空间，DiffGAN生成具有特定特征的输入，这些特征会引发模型行为的差异。DiffGAN是黑盒式的，使其适用于更多场景。我们在广泛使用的图像数据集上训练的8对DNN模型上评估DiffGAN。结果表明，DiffGAN显著优于最新基线，在相同的预算下，生成了触发输入的数量是基线方法的四倍，并且具有更大的多样性和有效性。此外，生成的输入提高了基于机器学习模型的选型机制的准确性，该机制根据输入特性选择性能最佳的模型，当使用替代模型时，它可以作为智能输出投票机制。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.19794v4">PDF</a> Accepted into IEEE Transactions on Software Engineering</p>
<p><strong>Summary</strong></p>
<p>本文提出一种基于生成对抗网络（GAN）和非支配排序遗传算法II的差分测试方法DiffGAN，用于对深度神经网络（DNN）模型进行黑盒测试图像生成。DiffGAN旨在通过生成具有特定特征的输入来揭示不同模型间的行为差异，适用于多种场景下的模型可靠性验证。在广泛使用的图像数据集上进行的实验表明，相较于当前最佳实践方法，DiffGAN显著提高了性能，能生成更丰富多样的触发输入。同时，DiffGAN有助于更精准地选择和组合模型，进而优化模型选择机制与智能输出投票机制。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>DiffGAN方法旨在解决深度神经网络模型可靠性的验证问题，通过生成对抗网络和非支配排序遗传算法II生成测试图像。</li>
<li>DiffGAN聚焦于揭示模型间行为差异，在有限的测试数据集条件下具有重要应用价值。</li>
<li>方法可应用于黑盒测试，适用范围更广。</li>
<li>实验结果表明，相较于现有方法，DiffGAN能生成更多样化的触发输入。</li>
<li>DiffGAN能优化模型选择机制，提高模型性能评估的准确性。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.19794">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_医学图像/2410.19794v4/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_医学图像/2410.19794v4/page_5_0.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-19/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-19/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">医学图像</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-19/TTS/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_TTS/2509.14161v1/page_2_1.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS 方向最新论文已更新，请持续关注 Update in 2025-09-19  CS-FLEURS A Massively Multilingual and Code-Switched Speech Dataset
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-19
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-19/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_Diffusion Models/2509.13576v1/page_4_0.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-09-19  Noise-Level Diffusion Guidance Well Begun is Half Done
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-19
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">29997.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
