<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning 方向最新论文已更新，请持续关注 Update in 2025-09-19  TGPO Tree-Guided Preference Optimization for Robust Web Agent   Reinforcement Learning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13758v1/page_0_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-09-19
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-09-29
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    22.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    92 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-09-19-更新"><a href="#2025-09-19-更新" class="headerlink" title="2025-09-19 更新"></a>2025-09-19 更新</h1><h2 id="TGPO-Tree-Guided-Preference-Optimization-for-Robust-Web-Agent-Reinforcement-Learning"><a href="#TGPO-Tree-Guided-Preference-Optimization-for-Robust-Web-Agent-Reinforcement-Learning" class="headerlink" title="TGPO: Tree-Guided Preference Optimization for Robust Web Agent   Reinforcement Learning"></a>TGPO: Tree-Guided Preference Optimization for Robust Web Agent   Reinforcement Learning</h2><p><strong>Authors:Ziyuan Chen, Zhenghui Zhao, Zhangye Han, Miancan Liu, Xianhang Ye, Yiqing Li, Hongbo Min, Jinkui Ren, Xiantao Zhang, Guitao Cao</strong></p>
<p>With the rapid advancement of large language models and vision-language models, employing large models as Web Agents has become essential for automated web interaction. However, training Web Agents with reinforcement learning faces critical challenges including credit assignment misallocation, prohibitively high annotation costs, and reward sparsity. To address these issues, we propose Tree-Guided Preference Optimization (TGPO), an offline reinforcement learning framework that proposes a tree-structured trajectory representation merging semantically identical states across trajectories to eliminate label conflicts. Our framework incorporates a Process Reward Model that automatically generates fine-grained rewards through subgoal progress, redundancy detection, and action verification. Additionally, a dynamic weighting mechanism prioritizes high-impact decision points during training. Experiments on Online-Mind2Web and our self-constructed C-WebShop datasets demonstrate that TGPO significantly outperforms existing methods, achieving higher success rates with fewer redundant steps. </p>
<blockquote>
<p>随着大型语言模型和视觉语言模型的快速发展，将大型模型作为Web代理进行自动化Web交互变得至关重要。然而，使用强化学习训练Web代理面临着关键的挑战，包括信用分配的误分配、高昂的标注成本和奖励稀疏性。为了解决这些问题，我们提出了Tree-Guided Preference Optimization（TGPO）这一离线强化学习框架。该框架通过提出树形结构轨迹表示，将轨迹中语义相同的状态合并，以消除标签冲突。我们的框架结合了过程奖励模型，该模型通过子目标进度、冗余检测以及动作验证自动生成精细奖励。此外，动态加权机制在训练过程中优先考虑高影响力的决策点。在Online-Mind2Web和我们自行构建的C-WebShop数据集上的实验表明，TGPO显著优于现有方法，实现了更高的成功率并减少了冗余步骤。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.14172v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型和视觉语言模型的快速发展使得将大型模型作为Web代理进行自动化Web交互变得至关重要。然而，使用强化学习训练Web代理面临关键挑战，如信用分配不当、标注成本过高和奖励稀疏等。为解决这些问题，我们提出了树结构指导的偏好优化（TGPO）这一离线强化学习框架，通过树状结构轨迹表示消除标签冲突并合并语义上相同的轨迹状态。我们的框架结合了过程奖励模型，通过子目标进展、冗余检测以及动作验证自动生成精细奖励。此外，动态加权机制在训练过程中优先处理高影响力的决策点。在线实验表明，TGPO在在线和自建数据集上的成功率显著优于现有方法。通过优化核心功能并使用优化的机器学习技术（包括低成本精准定制数据集和分布式并行计算架构），我们能够有效地简化网络操作并提高准确性。我们的模型显示出处理大量数据的潜力，未来可广泛应用于Web代理和自动化任务的训练。TGPO为Web代理的发展提供了强大的支持，开启了自动化Web交互的新篇章。简言之，我们的工作极大地推动了强化学习在Web代理训练中的应用，为构建更智能的Web交互系统提供了强大的工具。简而言之，强化学习技术通过融合多源数据对复杂的用户行为进行建模，从而推动自动化Web交互的进步。随着技术的不断进步，未来的Web代理将更加智能、高效和人性化。我们相信这一领域的发展将为人工智能研究注入新的活力。希望我们能以此工作为基础进一步推进该领域的发展并解决更多的挑战性问题。此外值得一提的是通过对这一模型的进一步研究人们可以对当前的NLP技术以及NLP理论中的理解缺口有更好的把握这对于创建人工智能以及真正理解的计算机接口的应用领域是至关重要的为大规模数据处理提供了一个有效解决方案为我们探索更多智能网络应用开辟了道路提供了机会对计算机和网络的使用将变得越来越容易智能化越来越高且不再需要耗费大量的时间和资源来解决某些任务它带来了全新的机遇并有望在未来引发巨大的变化和改进有望成为我们构建现代世界不可或缺的桥梁技术同时也提供了一个深入了解强化学习复杂系统的窗口便于探索如何在日常生活中更加广泛地运用它开启了智能化发展的全新阶段将为全球技术领域带来无限机遇和发展潜力提升了相关领域的应用能力和竞争力这是面向全球的研究人员和技术人员的宝贵资源。这不仅是技术的飞跃更是对人类智慧的致敬和赞美。我们相信随着研究的深入这一领域将不断取得新的突破为人类带来更加美好的未来。我们相信该模型将在未来引领人工智能领域的革命性进步。总之我们期待着未来基于强化学习的Web代理在智能人机交互领域的广阔前景及其对人类社会的深远影响为未来研究指明了方向也给我们带来了无限的好奇与期待开启了新时代的技术探索之旅展现出其独特的价值和潜力为该领域的研究者提供了全新的视角和挑战我们相信这个领域的未来将更加广阔展现出令人期待的可能性也意味着一个新的时代已经来临能够带给我们更为丰富的应用体验和感知提升技术的发展让人们未来的生活越来越智能化这是一个充满希望且值得期待的领域方向在未来将为我们的生活带来深远的影响和创新可能正在塑造一个全新的人工智能时代世界为未来带来更便捷高效的解决方案展示出无限的机遇和挑战未来值得期待我们相信该领域的未来发展将会带来更多的惊喜和挑战让我们共同期待这一领域的持续发展和未来的创新成果它将为我们开启一个全新的世界不断为人类创造新的奇迹并为社会带来巨大的进步推动整个社会向更高的智能时代迈进带来新的挑战同时也充满了新的机遇是我们共同努力和期待的全新里程碑也是我们迈向未来的重要一步朝着更加智能化和便捷化的方向发展并引领着人工智能领域的创新和发展方向引领着人类社会迈向一个全新的时代展示了人工智能技术的无限潜力和未来发展方向以及我们对于未来的期待和希望为该领域的发展注入新的活力和希望朝着更加智能高效的未来迈进为我们的社会发展注入新的动力。<strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型和视觉语言模型在Web代理自动化交互中的重要性。</li>
<li>强化学习在训练Web代理时面临的挑战，如信用分配、高标注成本和奖励稀疏性。</li>
<li>Tree-Guided Preference Optimization (TGPO)框架解决了上述挑战，通过树状结构轨迹表示和过程奖励模型提高训练效率。</li>
<li>TGPO在在线和自建数据集上的表现优于现有方法，实现了较高的成功率和较少的冗余步骤。</li>
<li>TGPO为Web代理的发展提供了强大的支持，开启了自动化Web交互的新篇章。</li>
<li>强化学习技术在处理大量数据、建模复杂用户行为方面的潜力，以及其在未来的广泛应用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14172">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.14172v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.14172v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.14172v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.14172v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.14172v1/page_3_1.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="MARS2-2025-Challenge-on-Multimodal-Reasoning-Datasets-Methods-Results-Discussion-and-Outlook"><a href="#MARS2-2025-Challenge-on-Multimodal-Reasoning-Datasets-Methods-Results-Discussion-and-Outlook" class="headerlink" title="MARS2 2025 Challenge on Multimodal Reasoning: Datasets, Methods,   Results, Discussion, and Outlook"></a>MARS2 2025 Challenge on Multimodal Reasoning: Datasets, Methods,   Results, Discussion, and Outlook</h2><p><strong>Authors:Peng Xu, Shengwu Xiong, Jiajun Zhang, Yaxiong Chen, Bowen Zhou, Chen Change Loy, David A. Clifton, Kyoung Mu Lee, Luc Van Gool, Ruiming He, Ruilin Yao, Xinwei Long, Jirui Huang, Kai Tian, Sa Yang, Yihua Shao, Jin Feng, Yue Zhong, Jiakai Zhou, Cheng Tang, Tianyu Zou, Yifang Zhang, Junming Liang, Guoyou Li, Zhaoxiang Wang, Qiang Zhou, Yichen Zhao, Shili Xiong, Hyeongjin Nam, Jaerin Lee, Jaeyoung Chung, JoonKyu Park, Junghun Oh, Kanggeon Lee, Wooseok Lee, Juneyoung Ro, Turghun Osman, Can Hu, Chaoyang Liao, Cheng Chen, Chengcheng Han, Chenhao Qiu, Chong Peng, Cong Xu, Dailin Li, Feiyu Wang, Feng Gao, Guibo Zhu, Guopeng Tang, Haibo Lu, Han Fang, Han Qi, Hanxiao Wu, Haobo Cheng, Hongbo Sun, Hongyao Chen, Huayong Hu, Hui Li, Jiaheng Ma, Jiang Yu, Jianing Wang, Jie Yang, Jing He, Jinglin Zhou, Jingxuan Li, Josef Kittler, Lihao Zheng, Linnan Zhao, Mengxi Jia, Muyang Yan, Nguyen Thanh Thien, Pu Luo, Qi Li, Shien Song, Shijie Dong, Shuai Shao, Shutao Li, Taofeng Xue, Tianyang Xu, Tianyi Gao, Tingting Li, Wei Zhang, Weiyang Su, Xiaodong Dong, Xiao-Jun Wu, Xiaopeng Zhou, Xin Chen, Xin Wei, Xinyi You, Xudong Kang, Xujie Zhou, Xusheng Liu, Yanan Wang, Yanbin Huang, Yang Liu, Yang Yang, Yanglin Deng, Yashu Kang, Ye Yuan, Yi Wen, Yicen Tian, Yilin Tao, Yin Tang, Yipeng Lin, Yiqing Wang, Yiting Xi, Yongkang Yu, Yumei Li, Yuxin Qin, Yuying Chen, Yuzhe Cen, Zhaofan Zou, Zhaohong Liu, Zhehao Shen, Zhenglin Du, Zhengyang Li, Zhenni Huang, Zhenwei Shao, Zhilong Song, Zhiyong Feng, Zhiyu Wang, Zhou Yu, Ziang Li, Zihan Zhai, Zijian Zhang, Ziyang Peng, Ziyun Xiao, Zongshu Li</strong></p>
<p>This paper reviews the MARS2 2025 Challenge on Multimodal Reasoning. We aim to bring together different approaches in multimodal machine learning and LLMs via a large benchmark. We hope it better allows researchers to follow the state-of-the-art in this very dynamic area. Meanwhile, a growing number of testbeds have boosted the evolution of general-purpose large language models. Thus, this year’s MARS2 focuses on real-world and specialized scenarios to broaden the multimodal reasoning applications of MLLMs. Our organizing team released two tailored datasets Lens and AdsQA as test sets, which support general reasoning in 12 daily scenarios and domain-specific reasoning in advertisement videos, respectively. We evaluated 40+ baselines that include both generalist MLLMs and task-specific models, and opened up three competition tracks, i.e., Visual Grounding in Real-world Scenarios (VG-RS), Visual Question Answering with Spatial Awareness (VQA-SA), and Visual Reasoning in Creative Advertisement Videos (VR-Ads). Finally, 76 teams from the renowned academic and industrial institutions have registered and 40+ valid submissions (out of 1200+) have been included in our ranking lists. Our datasets, code sets (40+ baselines and 15+ participants’ methods), and rankings are publicly available on the MARS2 workshop website and our GitHub organization page <a target="_blank" rel="noopener" href="https://github.com/mars2workshop/">https://github.com/mars2workshop/</a>, where our updates and announcements of upcoming events will be continuously provided. </p>
<blockquote>
<p>本文回顾了关于多模态推理的MARS2 2025挑战赛。我们的目标是通过大规模基准测试，汇集多种多模态机器学习和大型语言模型的方法。我们希望这能更好地让研究人员在这个充满动态变化的领域追踪最新进展。与此同时，越来越多的测试平台推动了通用大型语言模型的进化。因此，今年的MARS2重点关注现实和特定场景，以拓展多模态推理在大型语言模型的应用领域。我们的组织团队发布了两个定制数据集Lens和AdsQA作为测试集，分别支持日常场景中的通用推理和广告视频中的领域特定推理。我们评估了超过40个基线模型，包括通用的大型语言模型和特定任务的模型，并开设了三个竞赛赛道，即现实世界场景中的视觉定位（VG-RS）、具有空间意识的视觉问答（VQA-SA）和在创意广告视频中的视觉推理（VR-Ads）。最后，来自著名学术和工业机构的76支队伍已经注册，超过40个有效提交（总计超过1200份）已被纳入我们的排名列表。我们的数据集、代码集（包含超过四十种基线和多种参赛者的方法）、以及排名均公开可在MARS2研讨会网站和我们的GitHub组织页面查看：<a target="_blank" rel="noopener" href="https://github.com/mars2workshop/">https://github.com/mars2workshop/</a>，我们将不断更新并公布即将发布的活动信息。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.14142v1">PDF</a> ICCV 2025 MARS2 Workshop and Challenge “Multimodal Reasoning and Slow   Thinking in the Large Model Era: Towards System 2 and Beyond’’</p>
<p><strong>Summary</strong>：<br>此文章介绍了MARS2 2025挑战赛的多模态推理内容。该挑战旨在通过大型基准测试，汇集不同的多模态机器学习和大型语言模型方法。今年，MARS2的重点在于现实和特定场景，以拓展多模态推理的应用范围。此外，该挑战释放了两个数据集Lens和AdsQA作为测试集，并评估了超过四十种基线模型，开辟了三大竞赛赛道。来自知名学术和工业机构的七十六支团队已经注册参与，已有四十多次有效提交记录于排名榜单。数据集、代码集和排名均公开于MARS2研讨会网站和GitHub组织页面。</p>
<p><strong>Key Takeaways</strong>：</p>
<ul>
<li>MARS2挑战致力于推动多模态机器学习和大型语言模型的发展和应用。</li>
<li>通过大型基准测试，该挑战集结了多种多模态机器学习的方法。</li>
<li>MARS2专注于现实和特定场景的多模态推理应用拓展。</li>
<li>释放了两个数据集Lens和AdsQA以支持通用和特定领域的推理任务。</li>
<li>对超过四十种基线模型进行了评估，包括通用MLLMs和任务特定模型。</li>
<li>竞赛包括三大赛道：现实世界场景中的视觉定位、具有空间意识的视觉问答和创意广告视频中的视觉推理。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14142">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.14142v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.14142v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.14142v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Reasoning-Efficiently-Through-Adaptive-Chain-of-Thought-Compression-A-Self-Optimizing-Framework"><a href="#Reasoning-Efficiently-Through-Adaptive-Chain-of-Thought-Compression-A-Self-Optimizing-Framework" class="headerlink" title="Reasoning Efficiently Through Adaptive Chain-of-Thought Compression: A   Self-Optimizing Framework"></a>Reasoning Efficiently Through Adaptive Chain-of-Thought Compression: A   Self-Optimizing Framework</h2><p><strong>Authors:Kerui Huang, Shuhan Liu, Xing Hu, Tongtong Xu, Lingfeng Bao, Xin Xia</strong></p>
<p>Chain-of-Thought (CoT) reasoning enhances Large Language Models (LLMs) by prompting intermediate steps, improving accuracy and robustness in arithmetic, logic, and commonsense tasks. However, this benefit comes with high computational costs: longer outputs increase latency, memory usage, and KV-cache demands. These issues are especially critical in software engineering tasks where concise and deterministic outputs are required. To investigate these trade-offs, we conduct an empirical study based on code generation benchmarks. The results reveal that longer CoT does not always help. Excessive reasoning often causes truncation, accuracy drops, and latency up to five times higher, with failed outputs consistently longer than successful ones. These findings challenge the assumption that longer reasoning is inherently better and highlight the need for adaptive CoT control. Motivated by this, we propose SEER (Self-Enhancing Efficient Reasoning), an adaptive framework that compresses CoT while preserving accuracy. SEER combines Best-of-N sampling with task-aware adaptive filtering, dynamically adjusting thresholds based on pre-inference outputs to reduce verbosity and computational overhead. We then evaluate SEER on three software engineering tasks and one math task. On average, SEER shortens CoT by 42.1%, improves accuracy by reducing truncation, and eliminates most infinite loops. These results demonstrate SEER as a practical method to make CoT-enhanced LLMs more efficient and robust, even under resource constraints. </p>
<blockquote>
<p>“链式思维（Chain-of-Thought，简称CoT）通过提示中间步骤增强了大语言模型（LLM）的功能，提高了算术、逻辑和常识任务的准确性和稳健性。然而，这一优势带来了较高的计算成本：较长的输出增加了延迟、内存使用和KV缓存需求。这些问题在需要简洁和确定性输出的软件工程任务中尤其关键。为了调查这些权衡，我们基于代码生成基准测试进行了实证研究。结果显示，过长的CoT并不总是有帮助。过多的推理往往会导致截断、准确性下降和高达五倍的延迟，失败的输出始终比成功的输出长。这些发现挑战了“更长的推理本质上更好”的假设，并强调了自适应CoT控制的必要性。受此启发，我们提出了SEER（自我增强高效推理），这是一种自适应框架，可以在保持准确性的同时压缩CoT。SEER结合了最佳N采样和基于任务的自适应过滤，根据预推理输出动态调整阈值，以减少冗余和计算开销。然后我们在三个软件工程任务和一个数学任务上评估了SEER。平均而言，SEER将CoT缩短了42.1%，通过减少截断提高了准确性，并消除了大多数无限循环。这些结果表明，SEER是一种实用的方法，可以使CoT增强型LLM更加高效和稳健，即使在资源受限的情况下也是如此。”</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.14093v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于Chain-of-Thought（CoT）推理的大型语言模型（LLM）在算术、逻辑和常识任务中通过提示中间步骤提高了准确性和稳健性。然而，这一优势伴随着较高的计算成本：更长的输出增加了延迟、内存使用量和KV缓存需求。在软件工程任务中，这些问题尤为关键，需要简洁和确定的输出。通过实证研究发现，过长的CoT并不总是有帮助。过多的推理会导致截断、准确性下降和高达五倍的延迟，失败的输出通常比成功的输出更长。因此，需要自适应的CoT控制。受此启发，提出了SEER（自适应高效推理），一个压缩CoT同时保持准确性的自适应框架。SEER结合Best-of-N采样和任务感知自适应过滤，根据预推理输出动态调整阈值，减少冗长和计算开销。在三项软件工程任务和一项数学任务上的评估表明，SEER平均缩短CoT 42.1%，提高了准确性，减少了截断，并消除了大多数无限循环。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>CoT推理在LLM中提高了算术、逻辑和常识任务的准确性和稳健性。</li>
<li>过长的CoT会增加计算成本，包括延迟、内存和KV缓存需求。</li>
<li>过多的推理可能导致输出截断、准确性下降和更高的延迟。</li>
<li>需要自适应的CoT控制以平衡推理长度和模型效率。</li>
<li>SEER框架通过压缩CoT同时保持准确性来解决这些问题。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14093">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.14093v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.14093v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.14093v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="SAIL-VL2-Technical-Report"><a href="#SAIL-VL2-Technical-Report" class="headerlink" title="SAIL-VL2 Technical Report"></a>SAIL-VL2 Technical Report</h2><p><strong>Authors:Weijie Yin, Yongjie Ye, Fangxun Shu, Yue Liao, Zijian Kang, Hongyuan Dong, Haiyang Yu, Dingkang Yang, Jiacong Wang, Han Wang, Wenzhuo Liu, Xiao Liang, Shuicheng Yan, Chao Feng</strong></p>
<p>We introduce SAIL-VL2, an open-suite vision-language foundation model (LVM) for comprehensive multimodal understanding and reasoning. As the successor to SAIL-VL, SAIL-VL2 achieves state-of-the-art performance at the 2B and 8B parameter scales across diverse image and video benchmarks, demonstrating strong capabilities from fine-grained perception to complex reasoning. Three core innovations drive its effectiveness. First, a large-scale data curation pipeline with scoring and filtering strategies enhances both quality and distribution across captioning, OCR, QA, and video data, improving training efficiency. Second, a progressive training framework begins with a powerful pre-trained vision encoder (SAIL-ViT), advances through multimodal pre-training, and culminates in a thinking-fusion SFT-RL hybrid paradigm that systematically strengthens model capabilities. Third, architectural advances extend beyond dense LLMs to efficient sparse Mixture-of-Experts (MoE) designs. With these contributions, SAIL-VL2 demonstrates competitive performance across 106 datasets and achieves state-of-the-art results on challenging reasoning benchmarks such as MMMU and MathVista. Furthermore, on the OpenCompass leaderboard, SAIL-VL2-2B ranks first among officially released open-source models under the 4B parameter scale, while serving as an efficient and extensible foundation for the open-source multimodal community. </p>
<blockquote>
<p>我们推出SAIL-VL2，这是一个用于全面多模态理解和推理的开放视觉语言基础模型（LVM）。作为SAIL-VL的继任者，SAIL-VL2在不同的图像和视频基准测试中达到了2B和8B参数规模的最新技术水平，表现出从精细粒度感知到复杂推理的强大能力。三个核心创新驱动了其有效性。首先，采用大规模数据整理管道，结合评分和过滤策略，提高了描述、OCR、问答和视频数据的质量和分布，提高了训练效率。其次，渐进式训练框架从一个强大的预训练视觉编码器（SAIL-ViT）开始，通过多模态预训练，最终形成一个思考融合SFT-RL混合范式，系统地增强了模型的能力。第三，架构的改进超越了密集的LLM，采用了高效的稀疏混合专家（MoE）设计。通过这些贡献，SAIL-VL2在106个数据集上表现出竞争力，并在具有挑战性的推理基准测试（如MMMU和MathVista）上达到了最新技术水平。此外，在OpenCompass排行榜上，SAIL-VL2-2B在官方发布的开源模型中位列4B参数规模之首，成为开源多模态社区的效率和可扩展性的基础。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.14033v1">PDF</a> Technical Report</p>
<p><strong>摘要</strong></p>
<p>SAIL-VL2是一款用于全面多模式理解和推理的开放视觉语言基础模型（LVM）。作为SAIL-VL的继任者，SAIL-VL2在2B和8B参数规模上实现了跨图像和视频基准测试的最佳性能，展现出从精细粒度感知到复杂推理的强劲能力。其三大核心创新驱动了模型的有效性。首先，大规模数据收集管道通过评分和过滤策略提高了字幕、OCR、QA和视频数据的质量和分布，提高了训练效率。其次，渐进式训练框架以强大的预训练视觉编码器（SAIL-ViT）开始，通过多模式预训练，最终采用思考融合SFT-RL混合范式，系统性地增强了模型能力。最后，架构的改进超越了密集的LLMs，采用了高效的稀疏混合专家（MoE）设计。通过这些贡献，SAIL-VL2在106个数据集上表现出竞争力，并在MMMU和MathVista等具有挑战性的推理基准测试上达到了最佳状态。此外，在OpenCompass排行榜上，SAIL-VL2-2B在官方发布的开源模型中排名第一，成为开源多模式社区的有效和可扩展基础。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>SAIL-VL2是一款先进的视觉语言基础模型（LVM），用于多模式理解和推理。</li>
<li>作为SAIL-VL的升级版，它在多种图像和视频基准测试上表现出卓越性能。</li>
<li>SAIL-VL2的三大核心创新包括大规模数据收集管道、渐进式训练框架和架构改进。</li>
<li>数据收集管道通过提高数据和训练效率的策略，增强了模型性能。</li>
<li>渐进式训练框架通过结合预训练、多模式预训练和思考融合，增强了模型的推理能力。</li>
<li>SAIL-VL2采用了混合专家设计，提高了模型的效率和性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14033">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.14033v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.14033v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.14033v1/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.14033v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.14033v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Large-Language-Model-Empowered-Decision-Transformer-for-UAV-Enabled-Data-Collection"><a href="#Large-Language-Model-Empowered-Decision-Transformer-for-UAV-Enabled-Data-Collection" class="headerlink" title="Large Language Model-Empowered Decision Transformer for UAV-Enabled Data   Collection"></a>Large Language Model-Empowered Decision Transformer for UAV-Enabled Data   Collection</h2><p><strong>Authors:Zhixion Chen, Jiangzhou Wang, and Hyundong Shin, Arumugam Nallanathan</strong></p>
<p>The deployment of unmanned aerial vehicles (UAVs) for reliable and energy-efficient data collection from spatially distributed devices holds great promise in supporting diverse Internet of Things (IoT) applications. Nevertheless, the limited endurance and communication range of UAVs necessitate intelligent trajectory planning. While reinforcement learning (RL) has been extensively explored for UAV trajectory optimization, its interactive nature entails high costs and risks in real-world environments. Offline RL mitigates these issues but remains susceptible to unstable training and heavily rely on expert-quality datasets. To address these challenges, we formulate a joint UAV trajectory planning and resource allocation problem to maximize energy efficiency of data collection. The resource allocation subproblem is first transformed into an equivalent linear programming formulation and solved optimally with polynomial-time complexity. Then, we propose a large language model (LLM)-empowered critic-regularized decision transformer (DT) framework, termed LLM-CRDT, to learn effective UAV control policies. In LLM-CRDT, we incorporate critic networks to regularize the DT model training, thereby integrating the sequence modeling capabilities of DT with critic-based value guidance to enable learning effective policies from suboptimal datasets. Furthermore, to mitigate the data-hungry nature of transformer models, we employ a pre-trained LLM as the transformer backbone of the DT model and adopt a parameter-efficient fine-tuning strategy, i.e., LoRA, enabling rapid adaptation to UAV control tasks with small-scale dataset and low computational overhead. Extensive simulations demonstrate that LLM-CRDT outperforms benchmark online and offline RL methods, achieving up to 36.7% higher energy efficiency than the current state-of-the-art DT approaches. </p>
<blockquote>
<p>在物联网（IoT）的众多应用中，利用无人机（UAV）进行可靠且能源高效的数据收集有着巨大的潜力。然而，无人机的续航和通信范围有限，需要进行智能轨迹规划。强化学习（RL）在无人机轨迹优化方面已得到广泛研究，但其交互性质导致在实际环境中的成本高和风险大。离线强化学习缓解了这些问题，但仍易受到训练不稳定的影响，并严重依赖于专家级数据集。为解决这些挑战，我们制定了一个联合无人机轨迹规划和资源分配问题，以最大化数据收集的能量效率。首先，我们将资源分配子问题转化为等效的线性规划形式，并以多项式时间复杂度最优地解决。然后，我们提出了一种由大型语言模型（LLM）赋能的评论家正则化决策转换器（DT）框架，称为LLM-CRDT，用于学习有效的无人机控制策略。在LLM-CRDT中，我们结合评论家网络来规范DT模型的训练，从而将DT的序列建模能力与基于评论家的价值指导相结合，使能够从次优数据集中学习有效策略。此外，为了缓解转换器模型对数据的需求，我们采用预训练的大型语言模型作为DT模型的转换器骨干，并采用参数高效的微调策略（即LoRA），使模型能够以小规模数据集和低计算开销快速适应无人机控制任务。大量模拟实验表明，LLM-CRDT在在线和离线强化学习方法上的表现均优于基准方法，与当前最先进的决策树方法相比，能源效率提高了高达36.7％。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13934v1">PDF</a> 14pages, 8 figures</p>
<p><strong>Summary</strong></p>
<p>本文探讨了利用无人机（UAVs）进行可靠且能源高效的数据采集以支持物联网（IoT）应用的前景。针对无人机的有限续航和通信范围问题，提出了联合无人机轨迹规划和资源分配的策略，以最大化数据收集的能量效率。为解决不稳定训练和依赖专家级数据集的问题，结合线性规划和决策变压器（DT）框架进行优化。引入大型语言模型（LLM）赋能的批评正则化决策变压器（LLM-CRDT），有效学习无人机控制策略，从次优数据集中整合序列建模和基于批评的价值导向。采用预训练LLM作为DT模型的变压器背景，并采用参数高效的微调策略，即LoRA，能在小规模数据集和低计算开销下快速适应无人机控制任务。模拟结果显示，LLM-CRDT相较于基准在线和离线强化学习方法表现出更高的能量效率，最高提升达36.7%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>UAVs在IoT数据收集中的应用及其面临的挑战：有限的续航和通信范围需要智能轨迹规划。</li>
<li>强化学习（RL）在UAV轨迹优化中的局限性，以及离线RL的易受到不稳定训练和依赖专家数据集的问题。</li>
<li>提出联合UAV轨迹规划和资源分配问题以最大化能量效率，其中资源分配子问题通过线性规划解决。</li>
<li>引入LLM-CRDT框架，结合决策变压器和批评网络，从次优数据中学习有效的UAV控制策略。</li>
<li>利用预训练的大型语言模型（LLM）作为决策变压器的核心，并采用参数高效的微调策略以适应小规模数据集和低计算需求。</li>
<li>模拟实验显示LLM-CRDT相较于其他方法显著提高能量效率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13934">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13934v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13934v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13934v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="DSpAST-Disentangled-Representations-for-Spatial-Audio-Reasoning-with-Large-Language-Models"><a href="#DSpAST-Disentangled-Representations-for-Spatial-Audio-Reasoning-with-Large-Language-Models" class="headerlink" title="DSpAST: Disentangled Representations for Spatial Audio Reasoning with   Large Language Models"></a>DSpAST: Disentangled Representations for Spatial Audio Reasoning with   Large Language Models</h2><p><strong>Authors:Kevin Wilkinghoff, Zheng-Hua Tan</strong></p>
<p>Reasoning about spatial audio with large language models requires a spatial audio encoder as an acoustic front-end to obtain audio embeddings for further processing. Such an encoder needs to capture all information required to detect the type of sound events, as well as the direction and distance of their corresponding sources. Accomplishing this with a single audio encoder is demanding as the information required for each of these tasks is mostly independent of each other. As a result, the performance obtained with a single encoder is often worse than when using task-specific audio encoders. In this work, we present DSpAST, a novel audio encoder based on SpatialAST that learns disentangled representations of spatial audio while having only 0.2% additional parameters. Experiments on SpatialSoundQA with the spatial audio reasoning system BAT demonstrate that DSpAST significantly outperforms SpatialAST. </p>
<blockquote>
<p>关于空间音频的推理需要空间音频编码器作为声学前端来获得用于进一步处理的音频嵌入。这样的编码器需要捕获所有检测声音事件类型所需的信息，以及相应音源的方向和距离。使用单个音频编码器完成这些任务是困难的，因为这些任务所需的信息大多相互独立。因此，使用单个编码器的性能往往比使用特定任务的音频编码器差。在这项工作中，我们提出了DSpAST，这是一种基于SpatialAST的新型音频编码器，它学习空间音频的解纠缠表示，并且仅增加了0.2%的参数。在带有空间音频推理系统BAT的SpatialSoundQA上的实验表明，DSpAST显著优于SpatialAST。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13927v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>空间音频推理需要使用空间音频编码器作为声学前端获取音频嵌入以供进一步处理。单一编码器难以满足同时检测声音事件类型及音源方向和距离的需求。因此，任务特定的音频编码器往往更为有效。本研究提出了基于SpatialAST的新型音频编码器DSpAST，其学习空间音频的分离表示并具有仅0.2%的额外参数。在SpatialSoundQA上的实验显示，DSpAST显著优于SpatialAST。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>空间音频推理需要声学前端的空间音频编码器获取音频嵌入。</li>
<li>单一编码器难以满足检测声音事件类型和音源方向及距离的需求。</li>
<li>任务特定的音频编码器通常表现更好。</li>
<li>本研究提出了新型音频编码器DSpAST，基于SpatialAST并具有仅0.2%的额外参数。</li>
<li>DSpAST能够学习空间音频的分离表示。</li>
<li>在SpatialSoundQA的实验中，DSpAST显著优于SpatialAST。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13927">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13927v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13927v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13927v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13927v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Combining-Evidence-and-Reasoning-for-Biomedical-Fact-Checking"><a href="#Combining-Evidence-and-Reasoning-for-Biomedical-Fact-Checking" class="headerlink" title="Combining Evidence and Reasoning for Biomedical Fact-Checking"></a>Combining Evidence and Reasoning for Biomedical Fact-Checking</h2><p><strong>Authors:Mariano Barone, Antonio Romano, Giuseppe Riccio, Marco Postiglione, Vincenzo Moscato</strong></p>
<p>Misinformation in healthcare, from vaccine hesitancy to unproven treatments, poses risks to public health and trust in medical systems. While machine learning and natural language processing have advanced automated fact-checking, validating biomedical claims remains uniquely challenging due to complex terminology, the need for domain expertise, and the critical importance of grounding in scientific evidence. We introduce CER (Combining Evidence and Reasoning), a novel framework for biomedical fact-checking that integrates scientific evidence retrieval, reasoning via large language models, and supervised veracity prediction. By integrating the text-generation capabilities of large language models with advanced retrieval techniques for high-quality biomedical scientific evidence, CER effectively mitigates the risk of hallucinations, ensuring that generated outputs are grounded in verifiable, evidence-based sources. Evaluations on expert-annotated datasets (HealthFC, BioASQ-7b, SciFact) demonstrate state-of-the-art performance and promising cross-dataset generalization. Code and data are released for transparency and reproducibility: https: &#x2F;&#x2F;github.com&#x2F;PRAISELab-PicusLab&#x2F;CER. </p>
<blockquote>
<p>医疗健康领域的错误信息，从疫苗犹豫到未经证实的治疗方法，对公共健康和医疗系统的信任构成了风险。尽管机器学习和自然语言处理已经推动了自动事实核查的发展，但由于复杂的术语、需要领域专业知识和对科学证据的基础至关重要，验证生物医学声明仍然具有独特挑战性。我们引入了CER（结合证据和推理），这是一个用于生物医学事实核查的新型框架，它结合了科学证据检索、通过大型语言模型的推理和受监督的真实性预测。通过将大型语言模型的文本生成能力与先进的生物医学科学证据检索技术相结合，CER有效地减轻了幻觉的风险，确保生成的输出基于可验证的证据来源。在专家注释数据集（HealthFC、BioASQ-7b、SciFact）上的评估证明了其处于行业前沿的性能和跨数据集的通用性。为了透明度和可重复性，我们公开了代码和数据：<a target="_blank" rel="noopener" href="https://github.com/PRAISELab-PicusLab/CER">https://github.com/PRAISELab-PicusLab/CER</a> 。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13879v1">PDF</a> Proceedings of the 48th International ACM SIGIR Conference on   Research and Development in Information Retrieval, 2025</p>
<p><strong>Summary</strong>：医疗健康领域中的错误信息，从疫苗犹豫到未经证实的治疗方法，对公共健康和医疗系统的信任构成风险。虽然机器学习和自然语言处理已经推动了自动事实核查的进展，但由于专业术语的复杂性、需要专业领域的知识和对科学证据的基础性需求，生物医学声明验证仍然面临独特挑战。本文介绍了一种新型的事实核查框架CER（结合证据和推理），该框架融合了科学证据检索、大型语言模型的推理和受控真实预测，以验证生物医学事实。通过结合大型语言模型的文本生成能力与高质量生物医学科学证据的先进检索技术，CER有效降低了幻想的风险，确保生成的输出基于可验证的证据来源。在专家注释数据集上的评估显示出了该框架的最佳性能和跨数据集的广泛概括前景。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>医疗健康领域的错误信息对公共健康及医疗系统信任度构成风险。</li>
<li>自动化事实核查在生物医学领域面临挑战，包括复杂术语、需要专业知识和对科学证据的需求。</li>
<li>CER框架结合了科学证据检索、大型语言模型的推理和受控真实预测来进行生物医学事实核查。</li>
<li>CER框架融合了文本生成能力和高质量生物医学科学证据的检索技术。</li>
<li>CER有效降低了生成错误信息（即“幻想”）的风险，确保输出基于可验证的证据来源。</li>
<li>在专家注释数据集上的评估显示，CER框架性能达到最新水平，具有良好的跨数据集概括能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13879">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13879v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13879v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13879v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13879v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13879v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13879v1/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13879v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13879v1/page_5_1.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="AdaThinkDrive-Adaptive-Thinking-via-Reinforcement-Learning-for-Autonomous-Driving"><a href="#AdaThinkDrive-Adaptive-Thinking-via-Reinforcement-Learning-for-Autonomous-Driving" class="headerlink" title="AdaThinkDrive: Adaptive Thinking via Reinforcement Learning for   Autonomous Driving"></a>AdaThinkDrive: Adaptive Thinking via Reinforcement Learning for   Autonomous Driving</h2><p><strong>Authors:Yuechen Luo, Fang Li, Shaoqing Xu, Zhiyi Lai, Lei Yang, Qimao Chen, Ziang Luo, Zixun Xie, Shengyin Jiang, Jiaxin Liu, Long Chen, Bing Wang, Zhi-xin Yang</strong></p>
<p>While reasoning technology like Chain of Thought (CoT) has been widely adopted in Vision Language Action (VLA) models, it demonstrates promising capabilities in end to end autonomous driving. However, recent efforts to integrate CoT reasoning often fall short in simple scenarios, introducing unnecessary computational overhead without improving decision quality. To address this, we propose AdaThinkDrive, a novel VLA framework with a dual mode reasoning mechanism inspired by fast and slow thinking. First, our framework is pretrained on large scale autonomous driving (AD) scenarios using both question answering (QA) and trajectory datasets to acquire world knowledge and driving commonsense. During supervised fine tuning (SFT), we introduce a two mode dataset, fast answering (w&#x2F;o CoT) and slow thinking (with CoT), enabling the model to distinguish between scenarios that require reasoning. Furthermore, an Adaptive Think Reward strategy is proposed in conjunction with the Group Relative Policy Optimization (GRPO), which rewards the model for selectively applying CoT by comparing trajectory quality across different reasoning modes. Extensive experiments on the Navsim benchmark show that AdaThinkDrive achieves a PDMS of 90.3, surpassing the best vision only baseline by 1.7 points. Moreover, ablations show that AdaThinkDrive surpasses both the never Think and always Think baselines, improving PDMS by 2.0 and 1.4, respectively. It also reduces inference time by 14% compared to the always Think baseline, demonstrating its ability to balance accuracy and efficiency through adaptive reasoning. </p>
<blockquote>
<p>虽然像思维链（CoT）这样的推理技术已在视觉语言行动（VLA）模型中得到广泛应用，并在端到端自动驾驶中展现出有前景的能力。然而，最近将CoT推理技术集成在一起的努力在简单场景中往往达不到预期效果，引入了不必要的计算开销，并未能提高决策质量。为了解决这一问题，我们提出了AdaThinkDrive，这是一种新型VLA框架，具有双模式推理机制，灵感来自快速和慢速思考。首先，我们的框架使用问答（QA）和轨迹数据集在大规模自动驾驶（AD）场景上进行预训练，以获取世界知识和驾驶常识。在监督微调（SFT）过程中，我们引入了一个双模式数据集，包括快速回答（无CoT）和慢速思考（有CoT），使模型能够区分需要推理的场景。此外，结合群体相对策略优化（GRPO），提出了一种自适应思考奖励策略，该策略通过比较不同推理模式下的轨迹质量，奖励模型有选择地应用CoT。在Navsim基准测试上的广泛实验表明，AdaThinkDrive的PDMS达到90.3，超越了仅使用视觉的最佳基准1.7分。此外，消融实验表明，AdaThinkDrive超越了从不思考（never Think）和总是思考（always Think）的基线模型，PDMS分别提高了2.0和1.4。与始终思考的基线模型相比，它还将推理时间减少了14%，证明了其通过自适应推理平衡准确性和效率的能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13769v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该文介绍了在端到端自动驾驶中运用Chain of Thought（CoT）推理技术的潜力，并指出近期尝试在简单场景中整合CoT推理往往引入不必要的计算负担，并未提高决策质量。为此，提出了AdaThinkDrive，一个具有快慢思维启发的新型VLA框架。该框架首先在大规模自动驾驶（AD）场景上进行预训练，获取世界知识和驾驶常识。在监督微调（SFT）阶段，引入快慢思维数据集，使模型能区分需要推理的场景。同时，提出自适应思考奖励策略，结合群体相对策略优化（GRPO），通过比较不同推理模式的轨迹质量来奖励模型选择性应用CoT。在Navsim基准测试上的实验表明，AdaThinkDrive的PDMS达到90.3，超越最佳视觉基准1.7个点。此外，与始终思考和从不思考基线相比，AdaThinkDrive分别提高PDMS 2.0和1.4。相较于始终思考基线，推理时间减少14%，展现了其在准确性与效率间的平衡能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AdaThinkDrive是一个结合了快慢思维启发的VLA框架，用于端到端自动驾驶。</li>
<li>框架通过在大规模自动驾驶场景上预训练，获取世界知识和驾驶常识。</li>
<li>引入快慢思维数据集，使模型能区分需要推理的场景。</li>
<li>提出自适应思考奖励策略，结合群体相对策略优化，鼓励模型选择性应用CoT。</li>
<li>AdaThinkDrive在Navsim基准测试上表现优异，PDMS达到90.3，超越视觉基准。</li>
<li>与其他基线相比，AdaThinkDrive在PDMS上有显著提高，并减少推理时间。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13769">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13769v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13769v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13769v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13769v1/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13769v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13769v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13769v1/page_5_1.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="THOR-Tool-Integrated-Hierarchical-Optimization-via-RL-for-Mathematical-Reasoning"><a href="#THOR-Tool-Integrated-Hierarchical-Optimization-via-RL-for-Mathematical-Reasoning" class="headerlink" title="THOR: Tool-Integrated Hierarchical Optimization via RL for Mathematical   Reasoning"></a>THOR: Tool-Integrated Hierarchical Optimization via RL for Mathematical   Reasoning</h2><p><strong>Authors:Qikai Chang, Zhenrong Zhang, Pengfei Hu, Jiefeng Ma, Yicheng Pan, Jianshu Zhang, Jun Du, Quan Liu, Jianqing Gao</strong></p>
<p>Large Language Models (LLMs) have made remarkable progress in mathematical reasoning, but still continue to struggle with high-precision tasks like numerical computation and formal symbolic manipulation. Integrating external tools has emerged as a promising approach to bridge this gap. Despite recent advances, existing methods struggle with three key challenges: constructing tool-integrated reasoning data, performing fine-grained optimization, and enhancing inference. To overcome these limitations, we propose THOR (Tool-Integrated Hierarchical Optimization via RL). First, we introduce TIRGen, a multi-agent actor-critic-based pipeline for constructing high-quality datasets of tool-integrated reasoning paths, aligning with the policy and generalizing well across diverse models. Second, to perform fine-grained hierarchical optimization, we introduce an RL strategy that jointly optimizes for both trajectory-level problem solving and step-level code generation. This is motivated by our key insight that the success of an intermediate tool call is a strong predictor of the final answer’s correctness. Finally, THOR incorporates a self-correction mechanism that leverages immediate tool feedback to dynamically revise erroneous reasoning paths during inference. Our approach demonstrates strong generalization across diverse models, performing effectively in both reasoning and non-reasoning models. It further achieves state-of-the-art performance for models of a similar scale on multiple mathematical benchmarks, while also delivering consistent improvements on code benchmarks. Our code will be publicly available at <a target="_blank" rel="noopener" href="https://github.com/JingMog/THOR">https://github.com/JingMog/THOR</a>. </p>
<blockquote>
<p>大型语言模型（LLM）在数学推理方面取得了显著进展，但仍在数值计算和形式符号操作等高精度任务上遇到困难。集成外部工具已成为弥补这一差距的有前途的方法。尽管最近有进展，但现有方法仍面临三大挑战：构建工具集成推理数据、进行精细优化和提高推理。为了克服这些限制，我们提出了THOR（通过强化学习进行工具集成分层优化）。首先，我们介绍了TIRGen，这是一个基于多智能体演员评论家（actor-critic）的管道，用于构建高质量的工具集成推理路径数据集，与策略相符并在不同的模型之间具有良好的通用性。其次，为了进行精细的分层优化，我们引入了一种强化学习策略，该策略同时对轨迹级的问题解决和步骤级的代码生成进行优化。我们的关键见解是，中间工具调用的成功是预测最终答案正确性的强烈指标。最后，THOR融入了一种自我修正机制，利用即时工具反馈在推理过程中动态修正错误的推理路径。我们的方法在不同的模型之间表现出强大的通用性，在推理和非推理模型中都能有效运行。此外，在多个数学基准测试上，我们的方法在同类规模模型中实现了最先进的性能，同时在代码基准测试上实现了持续的改进。我们的代码将在<a target="_blank" rel="noopener" href="https://github.com/JingMog/THOR">https://github.com/JingMog/THOR</a>上公开可用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13761v1">PDF</a> 22 pages, 13 figures</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLMs）在数学推理方面取得显著进展，但在数值计算和形式符号操作等高精度任务上仍面临挑战。整合外部工具成为弥补这一差距的有前途的方法。为克服现有方法的挑战，提出THOR（通过强化学习实现工具集成分层优化）。THOR引入TIRGen，基于多智能体actor-critic的管道，构建高质量的工具集成推理路径数据集，很好地与策略对齐并在多种模型上实现泛化。此外，采用强化学习策略进行精细分层优化，联合优化轨迹级问题解决和步骤级代码生成。最后，THOR利用即时工具反馈在推理过程中动态修正错误路径。THOR在多个数学基准测试上实现了卓越性能，同时在类似规模的模型上表现优异。代码公开于：<a target="_blank" rel="noopener" href="https://github.com/JingMog/THOR%E3%80%82">https://github.com/JingMog/THOR。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型(LLMs)在数学推理任务中虽有所进展，但在高精度任务上仍面临挑战。</li>
<li>整合外部工具是一种有效的策略，以弥补大型语言模型在处理复杂数学任务时的不足。</li>
<li>THOR通过引入TIRGen，构建高质量的工具集成推理路径数据集，实现了策略与模型的良好对齐和泛化。</li>
<li>THOR采用强化学习策略进行精细的分层优化，同时优化轨迹级问题解决和步骤级代码生成。</li>
<li>THOR利用即时工具反馈，在推理过程中动态修正错误路径，增强了模型的自我纠正能力。</li>
<li>THOR在多个数学基准测试上实现了卓越性能，表明其在复杂数学任务上的有效性。</li>
<li>THOR的代码将公开发布，以便其他研究者使用和改进。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13761">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13761v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13761v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13761v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13761v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="A-Study-on-Thinking-Patterns-of-Large-Reasoning-Models-in-Code-Generation"><a href="#A-Study-on-Thinking-Patterns-of-Large-Reasoning-Models-in-Code-Generation" class="headerlink" title="A Study on Thinking Patterns of Large Reasoning Models in Code   Generation"></a>A Study on Thinking Patterns of Large Reasoning Models in Code   Generation</h2><p><strong>Authors:Kevin Halim, Sin G. Teo, Ruitao Feng, Zhenpeng Chen, Yang Gu, Chong Wang, Yang Liu</strong></p>
<p>Currently, many large language models (LLMs) are utilized for software engineering tasks such as code generation. The emergence of more advanced models known as large reasoning models (LRMs), such as OpenAI’s o3, DeepSeek R1, and Qwen3. They have demonstrated the capability of performing multi-step reasoning. Despite the advancement in LRMs, little attention has been paid to systematically analyzing the reasoning patterns these models exhibit and how such patterns influence the generated code. This paper presents a comprehensive study aimed at investigating and uncovering the reasoning behavior of LRMs during code generation. We prompted several state-of-the-art LRMs of varying sizes with code generation tasks and applied open coding to manually annotate the reasoning traces. From this analysis, we derive a taxonomy of LRM reasoning behaviors, encompassing 15 reasoning actions across four phases.   Our empirical study based on the taxonomy reveals a series of findings. First, we identify common reasoning patterns, showing that LRMs generally follow a human-like coding workflow, with more complex tasks eliciting additional actions such as scaffolding, flaw detection, and style checks. Second, we compare reasoning across models, finding that Qwen3 exhibits iterative reasoning while DeepSeek-R1-7B follows a more linear, waterfall-like approach. Third, we analyze the relationship between reasoning and code correctness, showing that actions such as unit test creation and scaffold generation strongly support functional outcomes, with LRMs adapting strategies based on task context. Finally, we evaluate lightweight prompting strategies informed by these findings, demonstrating the potential of context- and reasoning-oriented prompts to improve LRM-generated code. Our results offer insights and practical implications for advancing automatic code generation. </p>
<blockquote>
<p>当前，许多大型语言模型（LLM）被用于软件工程任务，如代码生成。随着被称为大型推理模型（LRM）的更先进模型的出现，如OpenAI的o3、DeepSeek R1和Qwen3，它们已经表现出了进行多步骤推理的能力。尽管LRM有所发展，但很少有人关注系统地分析这些模型所表现出的推理模式以及这些模式如何影响生成的代码。本文旨在全面研究调查LRM在代码生成过程中的推理行为。我们向几种最新的大型推理模型分配了代码生成任务，并通过开放编码对推理痕迹进行了手动注释。根据分析，我们得出了LRM推理行为的分类，包括四个阶段中的15种推理行动。基于分类的实证研究揭示了一系列发现。首先，我们确定了常见的推理模式，表明LRM通常遵循类似于人类的编码工作流程，更复杂的任务会引发额外的行动，如脚手架、缺陷检测和风格检查。其次，我们对不同模型的推理进行了比较，发现Qwen3表现出迭代推理，而DeepSeek-R1-7B则采用更线性、瀑布式的方法。第三，我们分析了推理与代码正确性之间的关系，显示创建单元测试、生成脚手架等行动有力地支持了功能结果，LRM根据任务上下文调整策略。最后，我们根据这些发现评估了轻量级提示策略，证明了上下文和推理导向提示改善LRM生成代码的潜力。我们的研究结果为自动代码生成的发展提供了见解和实际应用的启示。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13758v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型推理模型（LRMs）如OpenAI的o3、DeepSeek R1和Qwen3等，在代码生成等软件工程任务中展现出强大的多步推理能力。本文对LRMs在代码生成过程中的推理行为进行了全面的研究，通过手动注解推理痕迹，构建了涵盖15种推理行为的分类体系。研究发现LRMs遵循类似人类编码的工作流程，不同模型展现出不同的推理模式，且推理行为与代码正确性紧密相关。本文结果为改进LRM生成的代码提供了语境和推理导向的提示策略。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LRMs展现出强大的多步推理能力，可用于代码生成等软件工程任务。</li>
<li>通过手动注解推理痕迹，构建了LRM推理行为的分类体系，涵盖15种推理行为。</li>
<li>LRMs一般遵循类似人类编码的工作流程，复杂任务会触发更多推理行为。</li>
<li>不同LRM模型展现出不同的推理模式，如Qwen3采用迭代推理，DeepSeek-R1-7B采用更线性的瀑布式方法。</li>
<li>推理行为与代码正确性紧密相关，某些推理行为如单元测试创建和脚手架生成对功能结果有强支持。</li>
<li>语境和推理导向的提示策略可改善LRM生成的代码。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13758">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13758v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13758v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13758v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13758v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="FSR-VLN-Fast-and-Slow-Reasoning-for-Vision-Language-Navigation-with-Hierarchical-Multi-modal-Scene-Graph"><a href="#FSR-VLN-Fast-and-Slow-Reasoning-for-Vision-Language-Navigation-with-Hierarchical-Multi-modal-Scene-Graph" class="headerlink" title="FSR-VLN: Fast and Slow Reasoning for Vision-Language Navigation with   Hierarchical Multi-modal Scene Graph"></a>FSR-VLN: Fast and Slow Reasoning for Vision-Language Navigation with   Hierarchical Multi-modal Scene Graph</h2><p><strong>Authors:Xiaolin Zhou, Tingyang Xiao, Liu Liu, Yucheng Wang, Maiyue Chen, Xinrui Meng, Xinjie Wang, Wei Feng, Wei Sui, Zhizhong Su</strong></p>
<p>Visual-Language Navigation (VLN) is a fundamental challenge in robotic systems, with broad applications for the deployment of embodied agents in real-world environments. Despite recent advances, existing approaches are limited in long-range spatial reasoning, often exhibiting low success rates and high inference latency, particularly in long-range navigation tasks. To address these limitations, we propose FSR-VLN, a vision-language navigation system that combines a Hierarchical Multi-modal Scene Graph (HMSG) with Fast-to-Slow Navigation Reasoning (FSR). The HMSG provides a multi-modal map representation supporting progressive retrieval, from coarse room-level localization to fine-grained goal view and object identification. Building on HMSG, FSR first performs fast matching to efficiently select candidate rooms, views, and objects, then applies VLM-driven refinement for final goal selection. We evaluated FSR-VLN across four comprehensive indoor datasets collected by humanoid robots, utilizing 87 instructions that encompass a diverse range of object categories. FSR-VLN achieves state-of-the-art (SOTA) performance in all datasets, measured by the retrieval success rate (RSR), while reducing the response time by 82% compared to VLM-based methods on tour videos by activating slow reasoning only when fast intuition fails. Furthermore, we integrate FSR-VLN with speech interaction, planning, and control modules on a Unitree-G1 humanoid robot, enabling natural language interaction and real-time navigation. </p>
<blockquote>
<p>视觉语言导航（VLN）是机器人系统中的一个基本挑战，它在现实世界的环境中部署实体代理方面具有广泛的应用。尽管最近有进展，但现有方法在远程空间推理方面存在局限性，通常表现出较低的成功率和较高的推理延迟，特别是在远程导航任务中。为了解决这些局限性，我们提出了FSR-VLN，这是一种结合分层多模式场景图（HMSG）和快慢导航推理（FSR）的视觉语言导航系统。HMSG提供了一种多模式地图表示，支持从粗略的房间级别定位到精细的目标视图和对象识别的渐进检索。建立在HMSG之上，FSR首先进行快速匹配，以有效地选择候选房间、视图和对象，然后应用VLM驱动的精炼来进行最终目标选择。我们在由人形机器人收集的四个综合室内数据集上评估了FSR-VLN，使用了87条指令，涵盖了各种对象类别。FSR-VLN在所有数据集上的检索成功率（RSR）方面达到了最新水平，并且在激活慢速推理仅在快速直觉失败时，将响应时间减少了82%，与基于VLM的方法相比，用于游览视频的响应速度更快。此外，我们将FSR-VLN与Unitree-G1人形机器人上的语音交互、规划和控制模块集成在一起，实现自然语言交互和实时导航。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13733v1">PDF</a> 8 pages</p>
<p><strong>Summary</strong><br>视觉语言导航（VLN）是机器人系统中的一项基本挑战，广泛应用于真实世界环境中的智能体部署。针对现有方法在空间推理方面存在的局限性，我们提出了FSR-VLN系统，它结合了分层多模式场景图（HMSG）和快慢导航推理（FSR）。HMSG提供了多模态地图表示，支持从粗略的房间级别定位到精细的目标视图和对象识别的渐进检索。在此基础上，FSR首先进行快速匹配，以有效地选择候选房间、视图和对象，然后应用VLM驱动的细化进行最终目标选择。我们在由人形机器人收集的四个综合室内数据集上评估了FSR-VLN，使用87条指令涵盖了各种对象类别。FSR-VLN在所有数据集上的检索成功率（RSR）达到了最先进的水平，同时响应时间与基于VLM的方法相比减少了82%，它通过仅在快速直觉失败时激活慢速推理来实现这一点。此外，我们将FSR-VLN与人形机器人的语音交互、规划和控制模块集成在一起，实现自然语言交互和实时导航。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VLN是机器人系统中的一个基本挑战，具有广泛的应用于真实世界环境中的智能体部署。</li>
<li>现有方法存在长距离空间推理的局限性，常常表现出较低的成功率和较高的推理延迟。</li>
<li>提出了一种新的视觉语言导航系统FSR-VLN，结合了分层多模式场景图（HMSG）和快慢导航推理（FSR）。</li>
<li>HMSG提供了多模态地图表示，支持从粗略到精细的渐进检索。</li>
<li>FSR通过快速匹配选择候选目标，然后应用VLM驱动的细化进行最终目标选择。</li>
<li>在四个室内数据集上的评估显示，FSR-VLN达到了最先进的性能，并显著减少了响应时间。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13733">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13733v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13733v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13733v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13733v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13733v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13733v1/page_5_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13733v1/page_5_2.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="DSPC-Dual-Stage-Progressive-Compression-Framework-for-Efficient-Long-Context-Reasoning"><a href="#DSPC-Dual-Stage-Progressive-Compression-Framework-for-Efficient-Long-Context-Reasoning" class="headerlink" title="DSPC: Dual-Stage Progressive Compression Framework for Efficient   Long-Context Reasoning"></a>DSPC: Dual-Stage Progressive Compression Framework for Efficient   Long-Context Reasoning</h2><p><strong>Authors:Yaxin Gao, Yao Lu, Zongfei Zhang, Jiaqi Nie, Shanqing Yu, Qi Xuan</strong></p>
<p>Large language models (LLMs) have achieved remarkable success in many natural language processing (NLP) tasks. To achieve more accurate output, the prompts used to drive LLMs have become increasingly longer, which incurs higher computational costs. To address this prompt inflation problem, prompt compression has been proposed. However, most existing methods require training a small auxiliary model for compression, incurring a significant amount of additional computation. To avoid this, we propose a two-stage, training-free approach, called Dual-Stage Progressive Compression (DSPC). In the coarse-grained stage, semantic-related sentence filtering removes sentences with low semantic value based on TF-IDF. In the fine-grained stage, token importance is assessed using attention contribution, cross-model loss difference, and positional importance, enabling the pruning of low-utility tokens while preserving semantics. We validate DSPC on LLaMA-3.1-8B-Instruct and GPT-3.5-Turbo under a constrained token budget and observe consistent improvements. For instance, in the FewShot task of the Longbench dataset, DSPC achieves a performance of 49.17 by using only 3x fewer tokens, outperforming the best state-of-the-art baseline LongLLMLingua by 7.76. </p>
<blockquote>
<p>大型语言模型（LLM）在许多自然语言处理（NLP）任务中取得了显著的成功。为了获得更准确的输出，用于驱动LLM的提示变得越来越长，这导致了更高的计算成本。为了解决提示膨胀问题，已经提出了提示压缩。然而，大多数现有方法需要训练一个用于压缩的辅助模型，这产生了大量的额外计算。为了避免这一点，我们提出了一种两阶段的无训练方法，称为双阶段渐进压缩（DSPC）。在粗粒度阶段，通过语义相关句子过滤，基于TF-IDF去除低语义价值的句子。在细粒度阶段，通过注意力贡献、跨模型损失差异和位置重要性来评估令牌的重要性，从而能够在保留语义的同时删除低效用令牌。我们在有限的令牌预算下对LLaMA-3.1-8B-Instruct和GPT-3.5-Turbo进行了DSPC验证，并观察到了一致性的改进。例如，在Longbench数据集的FewShot任务中，DSPC仅使用3倍较少的令牌就实现了49.17的性能，优于当前最佳基线LongLLMLingua 7.76。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13723v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLMs）在自然语言处理（NLP）任务中取得了显著的成功。为了得到更准确的输出，用于驱动LLMs的提示越来越长，这带来了更高的计算成本。为解决提示膨胀问题，提出了提示压缩技术。然而，大多数现有方法需要训练一个用于压缩的辅助模型，这增加了大量的计算负担。为避免这一问题，我们提出了一种名为Dual-Stage Progressive Compression（DSPC）的两阶段、无需训练的方法。第一阶段通过语义相关句子过滤去除低语义价值的句子；第二阶段评估标记的重要性，通过注意力贡献、跨模型损失差异和位置重要性等方法，能够在保留语义的同时删除低效用标记。我们在LLaMA-3.1-8B-Instruct和GPT-3.5-Turbo上验证了DSPC的效果，并在有限的标记预算下观察到了一致的改进。例如，在Longbench数据集的FewShot任务中，DSPC仅使用3倍较少的标记就达到了49.17的性能，优于当前最佳基线LongLLMLingua的7.76。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLMs）在自然语言处理（NLP）任务中广泛应用，但提示膨胀问题导致计算成本增加。</li>
<li>现有的提示压缩方法通常需要训练辅助模型，增加了计算负担。</li>
<li>本文提出了一种名为Dual-Stage Progressive Compression（DSPC）的两阶段、无需训练的方法来解决提示膨胀问题。</li>
<li>DSPC包括两个阶段：粗粒度阶段通过语义相关句子过滤去除低语义价值的句子。</li>
<li>在细粒度阶段，通过评估标记的重要性来删除低效用标记，同时保留语义。</li>
<li>实验验证表明，DSPC在有限的标记预算下实现了性能改进。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13723">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13723v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13723v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13723v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13723v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="DSCC-HS-A-Dynamic-Self-Reinforcing-Framework-for-Hallucination-Suppression-in-Large-Language-Models"><a href="#DSCC-HS-A-Dynamic-Self-Reinforcing-Framework-for-Hallucination-Suppression-in-Large-Language-Models" class="headerlink" title="DSCC-HS: A Dynamic Self-Reinforcing Framework for Hallucination   Suppression in Large Language Models"></a>DSCC-HS: A Dynamic Self-Reinforcing Framework for Hallucination   Suppression in Large Language Models</h2><p><strong>Authors:Xiao Zheng</strong></p>
<p>Large Language Model (LLM) hallucination is a significant barrier to their reliable deployment. Current methods like Retrieval-Augmented Generation (RAG) are often reactive. We introduce <strong>Dynamic Self-reinforcing Calibration for Hallucination Suppression (DSCC-HS)</strong>, a novel, proactive framework that intervenes during autoregressive decoding. Inspired by dual-process cognitive theory, DSCC-HS uses a compact proxy model, trained in adversarial roles as a Factual Alignment Proxy (FAP) and a Hallucination Detection Proxy (HDP). During inference, these proxies dynamically steer a large target model by injecting a real-time steering vector, which is the difference between FAP and HDP logits, at each decoding step. This plug-and-play approach requires no modification to the target model. Our experiments on TruthfulQA and BioGEN show DSCC-HS achieves state-of-the-art performance. On TruthfulQA, it reached a 99.2% Factual Consistency Rate (FCR). On the long-form BioGEN benchmark, it attained the highest FActScore of 46.50. These results validate DSCC-HS as a principled and efficient solution for enhancing LLM factuality. </p>
<blockquote>
<p>大型语言模型（LLM）的幻觉是可靠部署的重大障碍。当前的方法如检索增强生成（RAG）通常是反应性的。我们引入了<strong>用于幻觉抑制的动态自强化校准（DSCC-HS）</strong>，这是一种新型主动框架，可在自回归解码过程中进行干预。受双过程认知理论的启发，DSCC-HS使用一个紧凑的代理模型，以对抗性角色接受训练，作为事实对齐代理（FAP）和幻觉检测代理（HDP）。在推理过程中，这些代理通过注入实时控制向量（即FAP和HDP日志之间的差值）来动态引导大型目标模型，该向量在每个解码步骤中都会被注入。这种即插即用的方法无需修改目标模型。我们在TruthfulQA和BioGEN上的实验表明，DSCC-HS达到了最新技术水平。在TruthfulQA上，它达到了99.2%的事实一致性率（FCR）。在长篇BioGEN基准测试中，它获得了最高的FActScore为46.50。这些结果验证了DSCC-HS作为一种有原则且高效的解决方案，可以增强LLM的真实性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13702v1">PDF</a> </p>
<p><strong>Summary</strong>：大型语言模型（LLM）的幻觉是一个可靠部署的障碍。目前的方法通常是反应性的，如检索增强生成（RAG）。本文介绍了一种名为动态自我强化校准抑制幻觉（DSCC-HS）的新型主动框架，它在自动回归解码过程中进行干预。受双过程认知理论的启发，DSCC-HS使用一个紧凑的代理模型，以对抗性角色作为事实对齐代理（FAP）和幻觉检测代理（HDP）进行训练。在推理过程中，这些代理通过注入实时转向向量（即FAP和HDP日志之间的差异），动态引导大型目标模型进行解码。这种即插即用方法无需修改目标模型。在TruthfulQA和BioGEN上的实验表明，DSCC-HS达到了最先进的性能。在TruthfulQA上，它达到了99.2%的事实一致性率（FCR）。在长文本BioGEN基准测试中，它获得了最高的FActScore，得分为46.50。这些结果表明，DSCC-HS是一种提高LLM事实性的有原则和有效率的解决方案。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>大型语言模型（LLM）的幻觉是其可靠部署的主要障碍之一。</li>
<li>目前的方法如检索增强生成（RAG）通常是反应性的。</li>
<li>本文提出了动态自我强化校准抑制幻觉（DSCC-HS）框架，这是一种新型的主动干预方法。</li>
<li>DSCC-HS使用双过程认知理论启发，通过紧凑的代理模型进行训练。</li>
<li>该框架通过注入实时转向向量，在自动回归解码过程中动态引导大型目标模型。</li>
<li>DSCC-HS在TruthfulQA和BioGEN基准测试上表现出卓越性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13702">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13702v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13702v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13702v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Improving-Context-Fidelity-via-Native-Retrieval-Augmented-Reasoning"><a href="#Improving-Context-Fidelity-via-Native-Retrieval-Augmented-Reasoning" class="headerlink" title="Improving Context Fidelity via Native Retrieval-Augmented Reasoning"></a>Improving Context Fidelity via Native Retrieval-Augmented Reasoning</h2><p><strong>Authors:Suyuchen Wang, Jinlin Wang, Xinyu Wang, Shiqi Li, Xiangru Tang, Sirui Hong, Xiao-Wen Chang, Chenglin Wu, Bang Liu</strong></p>
<p>Large language models (LLMs) often struggle with context fidelity, producing inconsistent answers when responding to questions based on provided information. Existing approaches either rely on expensive supervised fine-tuning to generate evidence post-answer or train models to perform web searches without necessarily improving utilization of the given context. We propose CARE, a novel native retrieval-augmented reasoning framework that teaches LLMs to explicitly integrate in-context evidence within their reasoning process with the model’s own retrieval capabilities. Our method requires limited labeled evidence data while significantly enhancing both retrieval accuracy and answer generation performance through strategically retrieved in-context tokens in the reasoning chain. Extensive experiments on multiple real-world and counterfactual QA benchmarks demonstrate that our approach substantially outperforms supervised fine-tuning, traditional retrieval-augmented generation methods, and external retrieval solutions. This work represents a fundamental advancement in making LLMs more accurate, reliable, and efficient for knowledge-intensive tasks. </p>
<blockquote>
<p>大型语言模型（LLM）通常在上下文忠实度方面遇到困难，在根据提供的信息回答问题时，会产生不一致的答案。现有方法要么依赖于昂贵的监督微调来在回答问题后生成证据，要么训练模型执行网络搜索，而不一定提高给定上下文的使用效率。我们提出了CARE，这是一种新型的本土检索增强推理框架，它能教会LLM在其推理过程中明确整合上下文证据和模型自身的检索能力。我们的方法需要有限的标记证据数据，同时通过战略性地检索推理链中的上下文标记，显著提高检索准确性和答案生成性能。在多个现实和虚构问答基准测试上的广泛实验表明，我们的方法显著优于监督微调、传统的检索增强生成方法和外部检索解决方案。这项工作使LLM在知识密集型任务中更加准确、可靠和高效，代表了一项基本进步。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13683v1">PDF</a> Accepted as a main conference paper at EMNLP 2025</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在处理上下文一致性时存在困难，对基于提供信息的提问会产生不一致的答案。现有方法依赖昂贵的监督微调来生成答案后的证据，或训练模型执行网络搜索，但不一定能提高对给定上下文的利用。我们提出CARE，一种新型的本地检索增强推理框架，旨在教授LLM在推理过程中明确整合上下文证据的能力，并利用模型的自身检索功能。我们的方法只需要有限的标记证据数据，同时通过战略性地检索推理链中的上下文令牌，显著提高了检索准确性和答案生成性能。在多个现实和虚构问答基准测试上的实验表明，我们的方法大幅优于监督微调、传统的检索增强生成方法和外部检索解决方案。这项工作标志着让LLM在知识密集型任务中更准确、可靠和高效的重要进步。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）在上下文一致性方面存在挑战，导致对基于提供信息的提问产生不一致的答案。</li>
<li>现有方法主要依赖监督微调或训练模型执行网络搜索来改善模型性能，但不一定能提高对给定上下文的利用。</li>
<li>CARE是一种新型的本地检索增强推理框架，整合了上下文证据到LLM的推理过程中。</li>
<li>CARE利用模型的自身检索功能，仅需要有限的标记证据数据。</li>
<li>通过战略性地检索推理链中的上下文令牌，CARE显著提高了检索准确性和答案生成性能。</li>
<li>在多个基准测试上，CARE大幅优于其他方法，包括监督微调、传统的检索增强生成方法和外部检索解决方案。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13683">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13683v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13683v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13683v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13683v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="DREAM-Domain-aware-Reasoning-for-Efficient-Autonomous-Underwater-Monitoring"><a href="#DREAM-Domain-aware-Reasoning-for-Efficient-Autonomous-Underwater-Monitoring" class="headerlink" title="DREAM: Domain-aware Reasoning for Efficient Autonomous Underwater   Monitoring"></a>DREAM: Domain-aware Reasoning for Efficient Autonomous Underwater   Monitoring</h2><p><strong>Authors:Zhenqi Wu, Abhinav Modi, Angelos Mavrogiannis, Kaustubh Joshi, Nikhil Chopra, Yiannis Aloimonos, Nare Karapetyan, Ioannis Rekleitis, Xiaomin Lin</strong></p>
<p>The ocean is warming and acidifying, increasing the risk of mass mortality events for temperature-sensitive shellfish such as oysters. This motivates the development of long-term monitoring systems. However, human labor is costly and long-duration underwater work is highly hazardous, thus favoring robotic solutions as a safer and more efficient option. To enable underwater robots to make real-time, environment-aware decisions without human intervention, we must equip them with an intelligent “brain.” This highlights the need for persistent,wide-area, and low-cost benthic monitoring. To this end, we present DREAM, a Vision Language Model (VLM)-guided autonomy framework for long-term underwater exploration and habitat monitoring. The results show that our framework is highly efficient in finding and exploring target objects (e.g., oysters, shipwrecks) without prior location information. In the oyster-monitoring task, our framework takes 31.5% less time than the previous baseline with the same amount of oysters. Compared to the vanilla VLM, it uses 23% fewer steps while covering 8.88% more oysters. In shipwreck scenes, our framework successfully explores and maps the wreck without collisions, requiring 27.5% fewer steps than the vanilla model and achieving 100% coverage, while the vanilla model achieves 60.23% average coverage in our shipwreck environments. </p>
<blockquote>
<p>海洋正在变暖和酸化，增加了对温度敏感的双壳类动物（如牡蛎）大规模死亡事件的风险。这促使了长期监测系统的开发。然而，人工劳动力成本高昂，长时间的水下工作极为危险，因此倾向于使用机器人解决方案作为更安全、更高效的替代方案。为了能够让水下机器人做出实时、了解环境的决策而无需人工干预，我们必须为他们配备一个智能“大脑”。这凸显了对持久、广泛和低成本的底栖监测的需求。为此，我们推出了DREAM，这是一个由视觉语言模型（VLM）引导的长期水下探索和栖息地监测自主性框架。结果表明，我们的框架在寻找和探索目标对象（例如牡蛎、沉船）方面非常高效，无需预先的位置信息。在牡蛎监测任务中，我们的框架比之前的基线减少了31.5%的时间，同时发现了相同数量的牡蛎。与普通的VLM相比，它减少了23%的步骤，同时覆盖了更多的牡蛎（增加了8.88%）。在沉船场景中，我们的框架成功地探索并绘制了沉船地图，没有发生碰撞，所需的步骤比常规模型减少了27.5%，并实现了100%的覆盖。而常规模型在我们的沉船环境中平均覆盖率仅为60.23%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13666v1">PDF</a> submitted to ICRA 2026</p>
<p><strong>Summary</strong><br>海洋变暖与酸化威胁了敏感贝壳类动物如生蚝的生存，促使长期监控系统的开发变得重要。考虑到人工成本的昂贵及长期水下工作的危险，机器人成为更安全高效的选择。为使水下机器人能进行实时环境感知并自主决策，需要为其配备智能“大脑”。因此强调了对持久、广泛区域、低成本底栖生物监测的需求。为此，我们推出DREAM项目，采用视觉语言模型（VLM）引导的水下自主框架，用于长期水下探索和栖息地监测。结果显示，该框架在寻找和探索目标物体（如生蚝、沉船）方面非常高效，无需预先位置信息。在生蚝监测任务中，我们的框架比之前的基线方法节省31.5%的时间，同时使用较少的步骤覆盖更多的生蚝区域。在沉船场景中，我们的框架能够成功探索并绘制沉船地图，且不会碰撞船只。相比普通模型，我们的框架探索步骤减少27.5%，并实现了全覆盖，而普通模型只能达到平均覆盖率为60.23%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>海洋变暖与酸化导致对温度敏感的海洋生物（如生蚝）面临大规模死亡的风险。</li>
<li>长期水下监测系统的开发至关重要，但人工成本高且工作危险，因此机器人解决方案更受欢迎。</li>
<li>水下机器人需要配备智能“大脑”以实现实时环境感知和自主决策。</li>
<li>需要持久、广泛区域和低成本的水下监测方法。</li>
<li>提出的DREAM框架采用视觉语言模型引导，用于长期水下探索和栖息地监测。</li>
<li>在目标物体监测方面，DREAM框架表现出高效率，显著优于基线方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13666">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13666v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13666v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13666v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13666v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13666v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="LLM-I-LLMs-are-Naturally-Interleaved-Multimodal-Creators"><a href="#LLM-I-LLMs-are-Naturally-Interleaved-Multimodal-Creators" class="headerlink" title="LLM-I: LLMs are Naturally Interleaved Multimodal Creators"></a>LLM-I: LLMs are Naturally Interleaved Multimodal Creators</h2><p><strong>Authors:Zirun Guo, Feng Zhang, Kai Jia, Tao Jin</strong></p>
<p>We propose LLM-Interleaved (LLM-I), a flexible and dynamic framework that reframes interleaved image-text generation as a tool-use problem. LLM-I is designed to overcome the “one-tool” bottleneck of current unified models, which are limited to synthetic imagery and struggle with tasks requiring factual grounding or programmatic precision. Our framework empowers a central LLM or MLLM agent to intelligently orchestrate a diverse toolkit of specialized visual tools, including online image search, diffusion-based generation, code execution, and image editing. The agent is trained to select and apply these tools proficiently via a Reinforcement Learning (RL) framework that features a hybrid reward system combining rule-based logic with judgments from LLM and MLLM evaluators. Trained on a diverse new dataset using four different model backbones, LLM-I demonstrates state-of-the-art performance, outperforming existing methods by a large margin across four benchmarks. We also introduce a novel test-time scaling strategy that provides further performance gains. Project Page: <a target="_blank" rel="noopener" href="https://github.com/ByteDance-BandAI/LLM-I">https://github.com/ByteDance-BandAI/LLM-I</a>. </p>
<blockquote>
<p>我们提出了LLM-Interleaved（LLM-I）这一灵活且动态框架，它将交错式图像文本生成重新构建为一种工具使用问题。LLM-I的设计旨在克服当前统一模型的“单一工具”瓶颈，这些模型仅限于合成图像，并且在需要事实依据或程序精确的任务方面表现挣扎。我们的框架使中央LLM或MLLM代理能够智能地协调各种专业视觉工具套件，包括在线图像搜索、基于扩散的生成、代码执行和图像编辑。该代理通过强化学习（RL）框架接受培训，该框架能够熟练地选择和应用这些工具，其特色在于结合基于规则的逻辑与LLM和MLLM评估者的判断力的混合奖励系统。LLM-I在新型多样数据集上进行训练，使用四种不同的模型骨干，展现出卓越的性能，在四项基准测试中大幅超越现有方法。我们还引入了一种新型测试时缩放策略，可提供进一步的性能提升。项目页面：<a target="_blank" rel="noopener" href="https://github.com/ByteDance-BandAI/LLM-I%E3%80%82">https://github.com/ByteDance-BandAI/LLM-I。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13642v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>LLM-Interleaved框架通过将其视为工具使用问题，实现了图像文本生成的重新构想。该框架旨在克服当前统一模型“单一工具”的瓶颈，这些模型仅限于合成图像，难以应对需要事实依据或程序精确性的任务。LLM-I框架允许中央LLM或MLLM代理智能地协调各种专业视觉工具，包括在线图像搜索、基于扩散的生成、代码执行和图像编辑等。代理通过结合规则逻辑与LLM和MLLM评估者的判断的强化学习（RL）框架，接受培训以熟练选择和运用这些工具。在采用四种不同模型主干的新数据集上进行训练后，LLM-I表现出卓越的性能，在四个基准测试中大幅超越现有方法。我们还引入了一种新型测试时缩放策略，可提供额外的性能提升。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM-Interleaved是一个灵活且动态的框架，将图像文本生成重新构想为工具使用问题。</li>
<li>该框架旨在克服当前统一模型的“单一工具”瓶颈，适应更多样化的任务需求。</li>
<li>LLM-I框架允许中央LLM或MLLM代理智能协调多种专业视觉工具。</li>
<li>代理通过强化学习框架接受培训，熟练选择和应用这些工具。</li>
<li>LLM-I在多种数据集上表现出卓越性能，大幅超越现有方法。</li>
<li>新型测试时缩放策略可提供额外的性能提升。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13642">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13642v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13642v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13642v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="AQUA-LLM-Evaluating-Accuracy-Quantization-and-Adversarial-Robustness-Trade-offs-in-LLMs-for-Cybersecurity-Question-Answering"><a href="#AQUA-LLM-Evaluating-Accuracy-Quantization-and-Adversarial-Robustness-Trade-offs-in-LLMs-for-Cybersecurity-Question-Answering" class="headerlink" title="AQUA-LLM: Evaluating Accuracy, Quantization, and Adversarial Robustness   Trade-offs in LLMs for Cybersecurity Question Answering"></a>AQUA-LLM: Evaluating Accuracy, Quantization, and Adversarial Robustness   Trade-offs in LLMs for Cybersecurity Question Answering</h2><p><strong>Authors:Onat Gungor, Roshan Sood, Harold Wang, Tajana Rosing</strong></p>
<p>Large Language Models (LLMs) have recently demonstrated strong potential for cybersecurity question answering (QA), supporting decision-making in real-time threat detection and response workflows. However, their substantial computational demands pose significant challenges for deployment on resource-constrained edge devices. Quantization, a widely adopted model compression technique, can alleviate these constraints. Nevertheless, quantization may degrade model accuracy and increase susceptibility to adversarial attacks. Fine-tuning offers a potential means to mitigate these limitations, but its effectiveness when combined with quantization remains insufficiently explored. Hence, it is essential to understand the trade-offs among accuracy, efficiency, and robustness. We propose AQUA-LLM, an evaluation framework designed to benchmark several state-of-the-art small LLMs under four distinct configurations: base, quantized-only, fine-tuned, and fine-tuned combined with quantization, specifically for cybersecurity QA. Our results demonstrate that quantization alone yields the lowest accuracy and robustness despite improving efficiency. In contrast, combining quantization with fine-tuning enhances both LLM robustness and predictive performance, achieving an optimal balance of accuracy, robustness, and efficiency. These findings highlight the critical need for quantization-aware, robustness-preserving fine-tuning methodologies to enable the robust and efficient deployment of LLMs for cybersecurity QA. </p>
<blockquote>
<p>大型语言模型（LLM）在网络安全问答（QA）方面显示出强大的潜力，支持实时威胁检测和响应工作流程中的决策制定。然而，它们巨大的计算需求对资源受限的边缘设备的部署构成了重大挑战。量化是一种广泛采用的模型压缩技术，可以缓解这些约束。然而，量化可能会降低模型精度并增加遭受对抗性攻击的风险。微调提供了一种缓解这些局限性的潜在手段，但其与量化的结合效果尚未得到充分探索。因此，了解精度、效率和稳健性之间的权衡至关重要。我们提出了AQUA-LLM，这是一个评估框架，旨在针对四种不同配置下的几种最新小型LLM进行基准测试：基础、仅量化、微调以及与量化相结合的微调，专门用于网络安全QA。我们的结果表明，仅量化虽然提高了效率，但会导致最低的准确性和稳健性。相反，将量化和微调相结合，可以提高LLM的稳健性和预测性能，实现准确性、稳健性和效率之间的最佳平衡。这些发现凸显了量化感知、保持稳健性的微调方法的迫切需求，以实现网络安全QA中LLM的稳健和高效部署。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13514v1">PDF</a> Accepted by the 24th IEEE International Conference on Machine   Learning and Applications (ICMLA’25)</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在网络安全问答（QA）中展现出强潜力，支持实时威胁检测与响应流程中的决策制定。然而，其巨大的计算需求对资源有限的边缘设备部署带来挑战。量化（一种广泛采用的模型压缩技术）可缓解这些约束，但可能降低模型精度并增加遭受攻击的风险。微调可缓解这些局限性，但其与量化的结合效果尚未得到充分探索。因此，理解精度、效率与稳健性之间的权衡至关重要。我们提出AQUA-LLM评估框架，旨在针对网络安全QA，对几种最新小型LLM在四种不同配置下进行基准测试：基础、仅量化、微调、以及结合量化的微调。结果显示，仅量化在精度和稳健性方面表现最低，尽管效率有所提升。相反，结合量化与微调可提升LLM的稳健性和预测性能，实现精度、稳健性和效率的最佳平衡。这表明需要采用兼顾量化、保持稳健性的微调方法，以实现LLM在网络安全QA中的稳健高效部署。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLMs）在网络安全问答（QA）中表现出色，但计算需求大，难以在资源受限的边缘设备上部署。</li>
<li>量化技术可压缩模型以减轻计算负担，但可能导致模型精度下降和易受攻击。</li>
<li>量化与微调相结合可提高LLM的稳健性和预测性能。</li>
<li>AQUA-LLM评估框架用于基准测试不同配置的最新小型LLMs在网络安全QA中的表现。</li>
<li>仅量化在精度和稳健性方面效果有限，而结合量化与微调可实现精度、稳健性和效率之间的平衡。</li>
<li>需要开发兼顾量化并保持稳健性的微调方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13514">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13514v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13514v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13514v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13514v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13514v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="SteeringControl-Holistic-Evaluation-of-Alignment-Steering-in-LLMs"><a href="#SteeringControl-Holistic-Evaluation-of-Alignment-Steering-in-LLMs" class="headerlink" title="SteeringControl: Holistic Evaluation of Alignment Steering in LLMs"></a>SteeringControl: Holistic Evaluation of Alignment Steering in LLMs</h2><p><strong>Authors:Vincent Siu, Nicholas Crispino, David Park, Nathan W. Henry, Zhun Wang, Yang Liu, Dawn Song, Chenguang Wang</strong></p>
<p>We introduce SteeringControl, a benchmark for evaluating representation steering methods across core alignment objectives–bias, harmful generation, and hallucination–and their effects on secondary behaviors such as sycophancy and commonsense morality. While prior alignment work often highlights truthfulness or reasoning ability to demonstrate the side effects of representation steering, we find there are many unexplored tradeoffs not yet understood in a systematic way. We collect a dataset of safety-relevant primary and secondary behaviors to evaluate steering effectiveness and behavioral entanglement centered around five popular steering methods. To enable this, we craft a modular steering framework based on unique components that serve as the building blocks of many existing methods. Our results on Qwen-2.5-7B and Llama-3.1-8B find that strong steering performance is dependent on the specific combination of steering method, model, and targeted behavior, and that severe concept entanglement can result from poor combinations of these three as well. We release our code here: <a target="_blank" rel="noopener" href="https://github.com/wang-research-lab/SteeringControl.git">https://github.com/wang-research-lab/SteeringControl.git</a>. </p>
<blockquote>
<p>我们介绍了SteeringControl，这是一个评估核心对齐目标（偏见、有害生成和幻觉）下的表示转向方法及其在对次行为（如谄媚和常识道德）的影响方面的基准测试。虽然先前的对齐工作通常强调真实性或推理能力来展示表示转向的副作用，但我们发现还有许多尚未系统理解的未探索权衡。我们收集了一个与安全相关的主行为和次行为数据集，以评估围绕五种流行的转向方法的转向效果和行为纠缠。为此，我们基于独特组件构建了一个模块化转向框架，这些组件作为许多现有方法的构建块。我们在Qwen-2.5-7B和Llama-3.1-8B上的结果发现，强大的转向性能取决于转向方法、模型和目标行为的特定组合，这三种不良组合也可能导致严重的概念纠缠。我们在此发布我们的代码：<a target="_blank" rel="noopener" href="https://github.com/wang-research-lab/SteeringControl.git">https://github.com/wang-research-lab/SteeringControl.git</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13450v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>介绍了一个名为SteeringControl的基准测试平台，用于评估表征转向方法在不同核心对齐目标（如偏见、有害生成和虚构）上的表现，及其对对齐次要行为（如奉承和常识道德）的影响。文章通过收集安全相关的主次行为数据集来评估转向效果和行为纠缠，并围绕五种流行的转向方法进行研究。提出了一种基于独特组件的模块化转向框架，该框架可作为许多现有方法的基本构件。对Qwen-2.5-7B和Llama-3.1-8B的测试结果表明，强大的转向性能取决于转向方法、模型和目标行为的特定组合，而这三者的不良组合可能导致严重的概念纠缠。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SteeringControl是一个用于评估表征转向方法的基准测试平台，涵盖核心对齐目标和次要行为。</li>
<li>文章收集了一个安全相关的数据集来评估转向效果和行为纠缠。</li>
<li>提出了一个模块化转向框架，基于独特组件作为现有方法的基本构件。</li>
<li>测试结果强调了转向方法、模型和特定目标行为组合的重要性。</li>
<li>转向性能受到转向方法、模型和次要行为之间概念纠缠的影响。</li>
<li>不同转向方法的组合可能会导致不同的概念纠缠程度。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13450">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13450v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13450v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13450v1/page_2_0.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="EdiVal-Agent-An-Object-Centric-Framework-for-Automated-Scalable-Fine-Grained-Evaluation-of-Multi-Turn-Editing"><a href="#EdiVal-Agent-An-Object-Centric-Framework-for-Automated-Scalable-Fine-Grained-Evaluation-of-Multi-Turn-Editing" class="headerlink" title="EdiVal-Agent: An Object-Centric Framework for Automated, Scalable,   Fine-Grained Evaluation of Multi-Turn Editing"></a>EdiVal-Agent: An Object-Centric Framework for Automated, Scalable,   Fine-Grained Evaluation of Multi-Turn Editing</h2><p><strong>Authors:Tianyu Chen, Yasi Zhang, Zhi Zhang, Peiyu Yu, Shu Wang, Zhendong Wang, Kevin Lin, Xiaofei Wang, Zhengyuan Yang, Linjie Li, Chung-Ching Lin, Jianwen Xie, Oscar Leong, Lijuan Wang, Ying Nian Wu, Mingyuan Zhou</strong></p>
<p>Instruction-based image editing has advanced rapidly, yet reliable and interpretable evaluation remains a bottleneck. Current protocols either (i) depend on paired reference images – resulting in limited coverage and inheriting biases from prior generative models – or (ii) rely solely on zero-shot vision-language models (VLMs), whose prompt-based assessments of instruction following, content consistency, and visual quality are often imprecise.   To address this, we introduce EdiVal-Agent, an automated, scalable, and fine-grained evaluation framework for multi-turn instruction-based editing from an object-centric perspective, supported by a suite of expert tools. Given an image, EdiVal-Agent first decomposes it into semantically meaningful objects, then synthesizes diverse, context-aware editing instructions. For evaluation, it integrates VLMs with open-vocabulary object detectors to assess instruction following, uses semantic-level feature extractors to evaluate content consistency, and leverages human preference models to judge visual quality. We show that combining VLMs with object detectors yields stronger agreement with human judgments in instruction-following evaluation compared to using VLMs alone and CLIP-based metrics. Furthermore, the pipeline’s modular design allows future tools to be seamlessly integrated, enhancing evaluation accuracy over time.   Instantiating this pipeline, we build EdiVal-Bench, a multi-turn editing benchmark covering 9 instruction types and 11 state-of-the-art editing models spanning autoregressive (AR) (including Nano Banana, GPT-Image-1), flow-matching, and diffusion paradigms. We demonstrate that EdiVal-Agent can be used to identify existing failure modes, thereby informing the development of the next generation of editing models. Project page: <a target="_blank" rel="noopener" href="https://tianyucodings.github.io/EdiVAL-page/">https://tianyucodings.github.io/EdiVAL-page/</a>. </p>
<blockquote>
<p>基于指令的图像编辑技术已经迅速发展，但可靠且可解释的评价仍然是瓶颈。当前协议要么（i）依赖于配对参考图像——导致覆盖有限并继承先前生成模型的偏见——要么（ii）完全依赖于零样本视觉语言模型（VLMs），其基于提示的指令遵循、内容一致性和视觉质量评估往往不精确。为了解决这一问题，我们引入了EdiVal-Agent，这是一个自动化、可扩展的针对多轮基于指令编辑的精细评价框架，得到一系列专家工具的支持。给定一个图像，EdiVal-Agent首先将其分解为语义上有意义的对象，然后合成多样且上下文感知的编辑指令。在评估过程中，它将VLMs与开放词汇对象检测器相结合来评估指令遵循情况，使用语义级特征提取器来评估内容一致性，并利用人类偏好模型来判断视觉质量。我们表明，与单独使用VLMs和基于CLIP的指标相比，将VLMs与对象检测器相结合在指令遵循评估中与人类判断的一致性更强。此外，该管道模块化设计允许未来工具无缝集成，随着时间的推移提高评估准确性。通过实例化此管道，我们构建了EdiVal-Bench，这是一个多轮编辑基准测试，涵盖9种指令类型和11种最新编辑模型，包括自回归（AR）（包括Nano Banana、GPT-Image-1）、流匹配和扩散范式。我们证明EdiVal-Agent可用于识别现有失败模式，从而为下一代编辑模型的开发提供信息。项目页面：<a target="_blank" rel="noopener" href="https://tianyucodings.github.io/EdiVAL-page/%E3%80%82">https://tianyucodings.github.io/EdiVAL-page/。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13399v1">PDF</a> Tianyu Chen and Yasi Zhang contributed equally; Oscar Leong, Lijuan   Wang, Ying Nian Wu, and Mingyuan Zhou advised equally</p>
<p><strong>Summary</strong></p>
<p>本文介绍了针对基于指令的图像编辑的评估瓶颈，提出一种自动化、可规模化、精细化的评估框架EdiVal-Agent，支持从对象中心的角度进行多轮指令编辑评估。该框架结合VLMs与开放词汇对象检测器，评估指令遵循情况、内容一致性及视觉质量。文章还展示了EdiVal-Bench的建立情况，这是一个涵盖多种指令类型和先进编辑模型的多轮编辑基准测试集。通过实例展示，EdiVal-Agent可识别现有模型的失败模式，为未来编辑模型的发展提供参考。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>当前基于指令的图像编辑在评估上遇到困难，缺少可靠且可解释的评估方法。</li>
<li>提出的EdiVal-Agent框架旨在解决这一问题，结合VLMs和开放词汇对象检测器进行精细化评估。</li>
<li>EdiVal-Agent能从对象中心的角度进行多轮指令编辑评估，提高评估的可靠性和准确性。</li>
<li>该框架还包括一个名为EdiVal-Bench的多轮编辑基准测试集，涵盖多种指令类型和先进的编辑模型。</li>
<li>通过实例展示，EdiVal-Agent有助于识别现有模型的失败模式，为未来模型的发展提供指导。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13399">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13399v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13399v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13399v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13399v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="BranchGRPO-Stable-and-Efficient-GRPO-with-Structured-Branching-in-Diffusion-Models"><a href="#BranchGRPO-Stable-and-Efficient-GRPO-with-Structured-Branching-in-Diffusion-Models" class="headerlink" title="BranchGRPO: Stable and Efficient GRPO with Structured Branching in   Diffusion Models"></a>BranchGRPO: Stable and Efficient GRPO with Structured Branching in   Diffusion Models</h2><p><strong>Authors:Yuming Li, Yikai Wang, Yuying Zhu, Zhongyu Zhao, Ming Lu, Qi She, Shanghang Zhang</strong></p>
<p>Recent progress in aligning image and video generative models with Group Relative Policy Optimization (GRPO) has improved human preference alignment, but existing variants remain inefficient due to sequential rollouts and large numbers of sampling steps, unreliable credit assignment: sparse terminal rewards are uniformly propagated across timesteps, failing to capture the varying criticality of decisions during denoising. In this paper, we present BranchGRPO, a method that restructures the rollout process into a branching tree, where shared prefixes amortize computation and pruning removes low-value paths and redundant depths. BranchGRPO introduces three contributions: (1) a branching scheme that amortizes rollout cost through shared prefixes while preserving exploration diversity; (2) a reward fusion and depth-wise advantage estimator that transforms sparse terminal rewards into dense step-level signals; and (3) pruning strategies that cut gradient computation but leave forward rollouts and exploration unaffected. On HPDv2.1 image alignment, BranchGRPO improves alignment scores by up to \textbf{16%} over DanceGRPO, while reducing per-iteration training time by nearly \textbf{55%}. A hybrid variant, BranchGRPO-Mix, further accelerates training to 4.7x faster than DanceGRPO without degrading alignment. On WanX video generation, it further achieves higher Video-Align scores with sharper and temporally consistent frames compared to DanceGRPO. Codes are available at \href{<a target="_blank" rel="noopener" href="https://fredreic1849.github.io/BranchGRPO-Webpage/%7D%7BBranchGRPO%7D">https://fredreic1849.github.io/BranchGRPO-Webpage/}{BranchGRPO}</a>. </p>
<blockquote>
<p>近期在图像和视频生成模型与组相对策略优化（GRPO）对齐方面的进展已经提高了人类偏好对齐的效果。然而，由于序列推演和大量采样步骤的存在，现有变体仍然效率低下，信用分配不可靠：稀疏的终端奖励被均匀传播到各个时间点，未能捕捉到去噪过程中决策的不同关键性。在本文中，我们提出了BranchGRPO方法，该方法将推演过程重构为一颗分支树，其中共享前缀摊销了计算成本，修剪则移除了低价值路径和冗余深度。BranchGRPO引入了三项贡献：（1）一种通过共享前缀来摊销推演成本的同时保持探索多样性的分支方案；（2）一种将稀疏终端奖励转化为密集步骤级信号的奖励融合和深度优势估计器；（3）修剪策略减少了梯度计算，但不影响正向推演和探索。在HPDv2.1图像对齐方面，BranchGRPO在DanceGRPO的基础上提高了对齐分数，最高可达16%，同时每迭代一次的训练时间减少了近55%。一种混合变体BranchGRPO-Mix进一步加速了训练，速度比DanceGRPO快4.7倍，同时不降低对齐效果。在WanX视频生成方面，它进一步实现了更高的视频对齐分数，与DanceGRPO相比，生成的帧更加清晰且时间上一致。相关代码可在<a target="_blank" rel="noopener" href="https://fredreic1849.github.io/BranchGRPO-Webpage/">BranchGRPO</a>获取。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.06040v4">PDF</a> 12 pages, 6 figures</p>
<p><strong>Summary</strong><br>     近期图像和视频生成模型通过采用Group Relative Policy Optimization（GRPO）进行对齐，提升了与人类偏好对齐的效果。但现有方法因序列展开、大量采样步骤和不可靠的信用分配（稀疏终端奖励无法反映决策过程中的不同重要性）而导致效率较低。本文提出的BranchGRPO方法通过构建分支树重组展开过程，利用共享前缀降低展开成本并实现计算成本分摊。此外，BranchGRPO引入三项贡献：分支方案、奖励融合和深度优势估计器以及剪枝策略。在HPDv2.1图像对齐任务上，BranchGRPO相较于DanceGRPO提升了对齐得分达16%，并将每迭代训练时间减少近55%。同时，其混合变体BranchGRPO-Mix在加速训练的同时不降低对齐效果。在WanX视频生成任务上，相较于DanceGRPO实现了更高的Video-Align得分并呈现出更清晰连贯的帧。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>BranchGRPO通过构建分支树改善了图像和视频生成模型的效率问题。</li>
<li>分支方案利用共享前缀降低了展开成本，同时保证了探索多样性。</li>
<li>奖励融合和深度优势估计器能将稀疏的终端奖励转化为密集的步骤级信号。</li>
<li>通过剪枝策略，BranchGRPO减少了梯度计算但不影响前向滚动和探索。</li>
<li>在HPDv2.1图像对齐任务上，BranchGRPO显著提高对齐得分并降低了训练时间。</li>
<li>BranchGRPO-Mix混合变体在加速训练的同时保持了良好的对齐效果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.06040">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.06040v4/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.06040v4/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.06040v4/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.06040v4/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.06040v4/page_5_0.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-19/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-19/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-19/LLM/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_LLM/2509.13905v1/page_2_0.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM 方向最新论文已更新，请持续关注 Update in 2025-09-19  Apertus Democratizing Open and Compliant LLMs for Global Language   Environments
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-19
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-19/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_Talking Head Generation/2509.13774v1/page_2_0.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2025-09-19  Dual-Actor Fine-Tuning of VLA Models A Talk-and-Tweak Human-in-the-Loop   Approach
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-19
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">28879.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
