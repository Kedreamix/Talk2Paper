<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-19  TGPO Tree-Guided Preference Optimization for Robust Web Agent   Reinforcement Learning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13758v1/page_0_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-19
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-07
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    22.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    92 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-09-19-æ›´æ–°"><a href="#2025-09-19-æ›´æ–°" class="headerlink" title="2025-09-19 æ›´æ–°"></a>2025-09-19 æ›´æ–°</h1><h2 id="TGPO-Tree-Guided-Preference-Optimization-for-Robust-Web-Agent-Reinforcement-Learning"><a href="#TGPO-Tree-Guided-Preference-Optimization-for-Robust-Web-Agent-Reinforcement-Learning" class="headerlink" title="TGPO: Tree-Guided Preference Optimization for Robust Web Agent   Reinforcement Learning"></a>TGPO: Tree-Guided Preference Optimization for Robust Web Agent   Reinforcement Learning</h2><p><strong>Authors:Ziyuan Chen, Zhenghui Zhao, Zhangye Han, Miancan Liu, Xianhang Ye, Yiqing Li, Hongbo Min, Jinkui Ren, Xiantao Zhang, Guitao Cao</strong></p>
<p>With the rapid advancement of large language models and vision-language models, employing large models as Web Agents has become essential for automated web interaction. However, training Web Agents with reinforcement learning faces critical challenges including credit assignment misallocation, prohibitively high annotation costs, and reward sparsity. To address these issues, we propose Tree-Guided Preference Optimization (TGPO), an offline reinforcement learning framework that proposes a tree-structured trajectory representation merging semantically identical states across trajectories to eliminate label conflicts. Our framework incorporates a Process Reward Model that automatically generates fine-grained rewards through subgoal progress, redundancy detection, and action verification. Additionally, a dynamic weighting mechanism prioritizes high-impact decision points during training. Experiments on Online-Mind2Web and our self-constructed C-WebShop datasets demonstrate that TGPO significantly outperforms existing methods, achieving higher success rates with fewer redundant steps. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹å’Œè§†è§‰è¯­è¨€æ¨¡å‹çš„å¿«é€Ÿå‘å±•ï¼Œå°†å¤§å‹æ¨¡å‹ä½œä¸ºWebä»£ç†è¿›è¡Œè‡ªåŠ¨åŒ–Webäº¤äº’å˜å¾—è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œä½¿ç”¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒWebä»£ç†é¢ä¸´ç€å…³é”®çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ä¿¡ç”¨åˆ†é…çš„è¯¯åˆ†é…ã€é«˜æ˜‚çš„æ ‡æ³¨æˆæœ¬å’Œå¥–åŠ±ç¨€ç–æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Tree-Guided Preference Optimizationï¼ˆTGPOï¼‰è¿™ä¸€ç¦»çº¿å¼ºåŒ–å­¦ä¹ æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡æå‡ºæ ‘å½¢ç»“æ„è½¨è¿¹è¡¨ç¤ºï¼Œå°†è½¨è¿¹ä¸­è¯­ä¹‰ç›¸åŒçš„çŠ¶æ€åˆå¹¶ï¼Œä»¥æ¶ˆé™¤æ ‡ç­¾å†²çªã€‚æˆ‘ä»¬çš„æ¡†æ¶ç»“åˆäº†è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼Œè¯¥æ¨¡å‹é€šè¿‡å­ç›®æ ‡è¿›åº¦ã€å†—ä½™æ£€æµ‹ä»¥åŠåŠ¨ä½œéªŒè¯è‡ªåŠ¨ç”Ÿæˆç²¾ç»†å¥–åŠ±ã€‚æ­¤å¤–ï¼ŒåŠ¨æ€åŠ æƒæœºåˆ¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¼˜å…ˆè€ƒè™‘é«˜å½±å“åŠ›çš„å†³ç­–ç‚¹ã€‚åœ¨Online-Mind2Webå’Œæˆ‘ä»¬è‡ªè¡Œæ„å»ºçš„C-WebShopæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒTGPOæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå®ç°äº†æ›´é«˜çš„æˆåŠŸç‡å¹¶å‡å°‘äº†å†—ä½™æ­¥éª¤ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.14172v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹å’Œè§†è§‰è¯­è¨€æ¨¡å‹çš„å¿«é€Ÿå‘å±•ä½¿å¾—å°†å¤§å‹æ¨¡å‹ä½œä¸ºWebä»£ç†è¿›è¡Œè‡ªåŠ¨åŒ–Webäº¤äº’å˜å¾—è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œä½¿ç”¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒWebä»£ç†é¢ä¸´å…³é”®æŒ‘æˆ˜ï¼Œå¦‚ä¿¡ç”¨åˆ†é…ä¸å½“ã€æ ‡æ³¨æˆæœ¬è¿‡é«˜å’Œå¥–åŠ±ç¨€ç–ç­‰ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†æ ‘ç»“æ„æŒ‡å¯¼çš„åå¥½ä¼˜åŒ–ï¼ˆTGPOï¼‰è¿™ä¸€ç¦»çº¿å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡æ ‘çŠ¶ç»“æ„è½¨è¿¹è¡¨ç¤ºæ¶ˆé™¤æ ‡ç­¾å†²çªå¹¶åˆå¹¶è¯­ä¹‰ä¸Šç›¸åŒçš„è½¨è¿¹çŠ¶æ€ã€‚æˆ‘ä»¬çš„æ¡†æ¶ç»“åˆäº†è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼Œé€šè¿‡å­ç›®æ ‡è¿›å±•ã€å†—ä½™æ£€æµ‹ä»¥åŠåŠ¨ä½œéªŒè¯è‡ªåŠ¨ç”Ÿæˆç²¾ç»†å¥–åŠ±ã€‚æ­¤å¤–ï¼ŒåŠ¨æ€åŠ æƒæœºåˆ¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¼˜å…ˆå¤„ç†é«˜å½±å“åŠ›çš„å†³ç­–ç‚¹ã€‚åœ¨çº¿å®éªŒè¡¨æ˜ï¼ŒTGPOåœ¨åœ¨çº¿å’Œè‡ªå»ºæ•°æ®é›†ä¸Šçš„æˆåŠŸç‡æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚é€šè¿‡ä¼˜åŒ–æ ¸å¿ƒåŠŸèƒ½å¹¶ä½¿ç”¨ä¼˜åŒ–çš„æœºå™¨å­¦ä¹ æŠ€æœ¯ï¼ˆåŒ…æ‹¬ä½æˆæœ¬ç²¾å‡†å®šåˆ¶æ•°æ®é›†å’Œåˆ†å¸ƒå¼å¹¶è¡Œè®¡ç®—æ¶æ„ï¼‰ï¼Œæˆ‘ä»¬èƒ½å¤Ÿæœ‰æ•ˆåœ°ç®€åŒ–ç½‘ç»œæ“ä½œå¹¶æé«˜å‡†ç¡®æ€§ã€‚æˆ‘ä»¬çš„æ¨¡å‹æ˜¾ç¤ºå‡ºå¤„ç†å¤§é‡æ•°æ®çš„æ½œåŠ›ï¼Œæœªæ¥å¯å¹¿æ³›åº”ç”¨äºWebä»£ç†å’Œè‡ªåŠ¨åŒ–ä»»åŠ¡çš„è®­ç»ƒã€‚TGPOä¸ºWebä»£ç†çš„å‘å±•æä¾›äº†å¼ºå¤§çš„æ”¯æŒï¼Œå¼€å¯äº†è‡ªåŠ¨åŒ–Webäº¤äº’çš„æ–°ç¯‡ç« ã€‚ç®€è¨€ä¹‹ï¼Œæˆ‘ä»¬çš„å·¥ä½œæå¤§åœ°æ¨åŠ¨äº†å¼ºåŒ–å­¦ä¹ åœ¨Webä»£ç†è®­ç»ƒä¸­çš„åº”ç”¨ï¼Œä¸ºæ„å»ºæ›´æ™ºèƒ½çš„Webäº¤äº’ç³»ç»Ÿæä¾›äº†å¼ºå¤§çš„å·¥å…·ã€‚ç®€è€Œè¨€ä¹‹ï¼Œå¼ºåŒ–å­¦ä¹ æŠ€æœ¯é€šè¿‡èåˆå¤šæºæ•°æ®å¯¹å¤æ‚çš„ç”¨æˆ·è¡Œä¸ºè¿›è¡Œå»ºæ¨¡ï¼Œä»è€Œæ¨åŠ¨è‡ªåŠ¨åŒ–Webäº¤äº’çš„è¿›æ­¥ã€‚éšç€æŠ€æœ¯çš„ä¸æ–­è¿›æ­¥ï¼Œæœªæ¥çš„Webä»£ç†å°†æ›´åŠ æ™ºèƒ½ã€é«˜æ•ˆå’Œäººæ€§åŒ–ã€‚æˆ‘ä»¬ç›¸ä¿¡è¿™ä¸€é¢†åŸŸçš„å‘å±•å°†ä¸ºäººå·¥æ™ºèƒ½ç ”ç©¶æ³¨å…¥æ–°çš„æ´»åŠ›ã€‚å¸Œæœ›æˆ‘ä»¬èƒ½ä»¥æ­¤å·¥ä½œä¸ºåŸºç¡€è¿›ä¸€æ­¥æ¨è¿›è¯¥é¢†åŸŸçš„å‘å±•å¹¶è§£å†³æ›´å¤šçš„æŒ‘æˆ˜æ€§é—®é¢˜ã€‚æ­¤å¤–å€¼å¾—ä¸€æçš„æ˜¯é€šè¿‡å¯¹è¿™ä¸€æ¨¡å‹çš„è¿›ä¸€æ­¥ç ”ç©¶äººä»¬å¯ä»¥å¯¹å½“å‰çš„NLPæŠ€æœ¯ä»¥åŠNLPç†è®ºä¸­çš„ç†è§£ç¼ºå£æœ‰æ›´å¥½çš„æŠŠæ¡è¿™å¯¹äºåˆ›å»ºäººå·¥æ™ºèƒ½ä»¥åŠçœŸæ­£ç†è§£çš„è®¡ç®—æœºæ¥å£çš„åº”ç”¨é¢†åŸŸæ˜¯è‡³å…³é‡è¦çš„ä¸ºå¤§è§„æ¨¡æ•°æ®å¤„ç†æä¾›äº†ä¸€ä¸ªæœ‰æ•ˆè§£å†³æ–¹æ¡ˆä¸ºæˆ‘ä»¬æ¢ç´¢æ›´å¤šæ™ºèƒ½ç½‘ç»œåº”ç”¨å¼€è¾Ÿäº†é“è·¯æä¾›äº†æœºä¼šå¯¹è®¡ç®—æœºå’Œç½‘ç»œçš„ä½¿ç”¨å°†å˜å¾—è¶Šæ¥è¶Šå®¹æ˜“æ™ºèƒ½åŒ–è¶Šæ¥è¶Šé«˜ä¸”ä¸å†éœ€è¦è€—è´¹å¤§é‡çš„æ—¶é—´å’Œèµ„æºæ¥è§£å†³æŸäº›ä»»åŠ¡å®ƒå¸¦æ¥äº†å…¨æ–°çš„æœºé‡å¹¶æœ‰æœ›åœ¨æœªæ¥å¼•å‘å·¨å¤§çš„å˜åŒ–å’Œæ”¹è¿›æœ‰æœ›æˆä¸ºæˆ‘ä»¬æ„å»ºç°ä»£ä¸–ç•Œä¸å¯æˆ–ç¼ºçš„æ¡¥æ¢æŠ€æœ¯åŒæ—¶ä¹Ÿæä¾›äº†ä¸€ä¸ªæ·±å…¥äº†è§£å¼ºåŒ–å­¦ä¹ å¤æ‚ç³»ç»Ÿçš„çª—å£ä¾¿äºæ¢ç´¢å¦‚ä½•åœ¨æ—¥å¸¸ç”Ÿæ´»ä¸­æ›´åŠ å¹¿æ³›åœ°è¿ç”¨å®ƒå¼€å¯äº†æ™ºèƒ½åŒ–å‘å±•çš„å…¨æ–°é˜¶æ®µå°†ä¸ºå…¨çƒæŠ€æœ¯é¢†åŸŸå¸¦æ¥æ— é™æœºé‡å’Œå‘å±•æ½œåŠ›æå‡äº†ç›¸å…³é¢†åŸŸçš„åº”ç”¨èƒ½åŠ›å’Œç«äº‰åŠ›è¿™æ˜¯é¢å‘å…¨çƒçš„ç ”ç©¶äººå‘˜å’ŒæŠ€æœ¯äººå‘˜çš„å®è´µèµ„æºã€‚è¿™ä¸ä»…æ˜¯æŠ€æœ¯çš„é£è·ƒæ›´æ˜¯å¯¹äººç±»æ™ºæ…§çš„è‡´æ•¬å’Œèµç¾ã€‚æˆ‘ä»¬ç›¸ä¿¡éšç€ç ”ç©¶çš„æ·±å…¥è¿™ä¸€é¢†åŸŸå°†ä¸æ–­å–å¾—æ–°çš„çªç ´ä¸ºäººç±»å¸¦æ¥æ›´åŠ ç¾å¥½çš„æœªæ¥ã€‚æˆ‘ä»¬ç›¸ä¿¡è¯¥æ¨¡å‹å°†åœ¨æœªæ¥å¼•é¢†äººå·¥æ™ºèƒ½é¢†åŸŸçš„é©å‘½æ€§è¿›æ­¥ã€‚æ€»ä¹‹æˆ‘ä»¬æœŸå¾…ç€æœªæ¥åŸºäºå¼ºåŒ–å­¦ä¹ çš„Webä»£ç†åœ¨æ™ºèƒ½äººæœºäº¤äº’é¢†åŸŸçš„å¹¿é˜”å‰æ™¯åŠå…¶å¯¹äººç±»ç¤¾ä¼šçš„æ·±è¿œå½±å“ä¸ºæœªæ¥ç ”ç©¶æŒ‡æ˜äº†æ–¹å‘ä¹Ÿç»™æˆ‘ä»¬å¸¦æ¥äº†æ— é™çš„å¥½å¥‡ä¸æœŸå¾…å¼€å¯äº†æ–°æ—¶ä»£çš„æŠ€æœ¯æ¢ç´¢ä¹‹æ—…å±•ç°å‡ºå…¶ç‹¬ç‰¹çš„ä»·å€¼å’Œæ½œåŠ›ä¸ºè¯¥é¢†åŸŸçš„ç ”ç©¶è€…æä¾›äº†å…¨æ–°çš„è§†è§’å’ŒæŒ‘æˆ˜æˆ‘ä»¬ç›¸ä¿¡è¿™ä¸ªé¢†åŸŸçš„æœªæ¥å°†æ›´åŠ å¹¿é˜”å±•ç°å‡ºä»¤äººæœŸå¾…çš„å¯èƒ½æ€§ä¹Ÿæ„å‘³ç€ä¸€ä¸ªæ–°çš„æ—¶ä»£å·²ç»æ¥ä¸´èƒ½å¤Ÿå¸¦ç»™æˆ‘ä»¬æ›´ä¸ºä¸°å¯Œçš„åº”ç”¨ä½“éªŒå’Œæ„ŸçŸ¥æå‡æŠ€æœ¯çš„å‘å±•è®©äººä»¬æœªæ¥çš„ç”Ÿæ´»è¶Šæ¥è¶Šæ™ºèƒ½åŒ–è¿™æ˜¯ä¸€ä¸ªå……æ»¡å¸Œæœ›ä¸”å€¼å¾—æœŸå¾…çš„é¢†åŸŸæ–¹å‘åœ¨æœªæ¥å°†ä¸ºæˆ‘ä»¬çš„ç”Ÿæ´»å¸¦æ¥æ·±è¿œçš„å½±å“å’Œåˆ›æ–°å¯èƒ½æ­£åœ¨å¡‘é€ ä¸€ä¸ªå…¨æ–°çš„äººå·¥æ™ºèƒ½æ—¶ä»£ä¸–ç•Œä¸ºæœªæ¥å¸¦æ¥æ›´ä¾¿æ·é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆå±•ç¤ºå‡ºæ— é™çš„æœºé‡å’ŒæŒ‘æˆ˜æœªæ¥å€¼å¾—æœŸå¾…æˆ‘ä»¬ç›¸ä¿¡è¯¥é¢†åŸŸçš„æœªæ¥å‘å±•å°†ä¼šå¸¦æ¥æ›´å¤šçš„æƒŠå–œå’ŒæŒ‘æˆ˜è®©æˆ‘ä»¬å…±åŒæœŸå¾…è¿™ä¸€é¢†åŸŸçš„æŒç»­å‘å±•å’Œæœªæ¥çš„åˆ›æ–°æˆæœå®ƒå°†ä¸ºæˆ‘ä»¬å¼€å¯ä¸€ä¸ªå…¨æ–°çš„ä¸–ç•Œä¸æ–­ä¸ºäººç±»åˆ›é€ æ–°çš„å¥‡è¿¹å¹¶ä¸ºç¤¾ä¼šå¸¦æ¥å·¨å¤§çš„è¿›æ­¥æ¨åŠ¨æ•´ä¸ªç¤¾ä¼šå‘æ›´é«˜çš„æ™ºèƒ½æ—¶ä»£è¿ˆè¿›å¸¦æ¥æ–°çš„æŒ‘æˆ˜åŒæ—¶ä¹Ÿå……æ»¡äº†æ–°çš„æœºé‡æ˜¯æˆ‘ä»¬å…±åŒåŠªåŠ›å’ŒæœŸå¾…çš„å…¨æ–°é‡Œç¨‹ç¢‘ä¹Ÿæ˜¯æˆ‘ä»¬è¿ˆå‘æœªæ¥çš„é‡è¦ä¸€æ­¥æœç€æ›´åŠ æ™ºèƒ½åŒ–å’Œä¾¿æ·åŒ–çš„æ–¹å‘å‘å±•å¹¶å¼•é¢†ç€äººå·¥æ™ºèƒ½é¢†åŸŸçš„åˆ›æ–°å’Œå‘å±•æ–¹å‘å¼•é¢†ç€äººç±»ç¤¾ä¼šè¿ˆå‘ä¸€ä¸ªå…¨æ–°çš„æ—¶ä»£å±•ç¤ºäº†äººå·¥æ™ºèƒ½æŠ€æœ¯çš„æ— é™æ½œåŠ›å’Œæœªæ¥å‘å±•æ–¹å‘ä»¥åŠæˆ‘ä»¬å¯¹äºæœªæ¥çš„æœŸå¾…å’Œå¸Œæœ›ä¸ºè¯¥é¢†åŸŸçš„å‘å±•æ³¨å…¥æ–°çš„æ´»åŠ›å’Œå¸Œæœ›æœç€æ›´åŠ æ™ºèƒ½é«˜æ•ˆçš„æœªæ¥è¿ˆè¿›ä¸ºæˆ‘ä»¬çš„ç¤¾ä¼šå‘å±•æ³¨å…¥æ–°çš„åŠ¨åŠ›ã€‚<strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹å’Œè§†è§‰è¯­è¨€æ¨¡å‹åœ¨Webä»£ç†è‡ªåŠ¨åŒ–äº¤äº’ä¸­çš„é‡è¦æ€§ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ åœ¨è®­ç»ƒWebä»£ç†æ—¶é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¦‚ä¿¡ç”¨åˆ†é…ã€é«˜æ ‡æ³¨æˆæœ¬å’Œå¥–åŠ±ç¨€ç–æ€§ã€‚</li>
<li>Tree-Guided Preference Optimization (TGPO)æ¡†æ¶è§£å†³äº†ä¸Šè¿°æŒ‘æˆ˜ï¼Œé€šè¿‡æ ‘çŠ¶ç»“æ„è½¨è¿¹è¡¨ç¤ºå’Œè¿‡ç¨‹å¥–åŠ±æ¨¡å‹æé«˜è®­ç»ƒæ•ˆç‡ã€‚</li>
<li>TGPOåœ¨åœ¨çº¿å’Œè‡ªå»ºæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå®ç°äº†è¾ƒé«˜çš„æˆåŠŸç‡å’Œè¾ƒå°‘çš„å†—ä½™æ­¥éª¤ã€‚</li>
<li>TGPOä¸ºWebä»£ç†çš„å‘å±•æä¾›äº†å¼ºå¤§çš„æ”¯æŒï¼Œå¼€å¯äº†è‡ªåŠ¨åŒ–Webäº¤äº’çš„æ–°ç¯‡ç« ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ æŠ€æœ¯åœ¨å¤„ç†å¤§é‡æ•°æ®ã€å»ºæ¨¡å¤æ‚ç”¨æˆ·è¡Œä¸ºæ–¹é¢çš„æ½œåŠ›ï¼Œä»¥åŠå…¶åœ¨æœªæ¥çš„å¹¿æ³›åº”ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14172">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.14172v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.14172v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.14172v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.14172v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.14172v1/page_3_1.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="MARS2-2025-Challenge-on-Multimodal-Reasoning-Datasets-Methods-Results-Discussion-and-Outlook"><a href="#MARS2-2025-Challenge-on-Multimodal-Reasoning-Datasets-Methods-Results-Discussion-and-Outlook" class="headerlink" title="MARS2 2025 Challenge on Multimodal Reasoning: Datasets, Methods,   Results, Discussion, and Outlook"></a>MARS2 2025 Challenge on Multimodal Reasoning: Datasets, Methods,   Results, Discussion, and Outlook</h2><p><strong>Authors:Peng Xu, Shengwu Xiong, Jiajun Zhang, Yaxiong Chen, Bowen Zhou, Chen Change Loy, David A. Clifton, Kyoung Mu Lee, Luc Van Gool, Ruiming He, Ruilin Yao, Xinwei Long, Jirui Huang, Kai Tian, Sa Yang, Yihua Shao, Jin Feng, Yue Zhong, Jiakai Zhou, Cheng Tang, Tianyu Zou, Yifang Zhang, Junming Liang, Guoyou Li, Zhaoxiang Wang, Qiang Zhou, Yichen Zhao, Shili Xiong, Hyeongjin Nam, Jaerin Lee, Jaeyoung Chung, JoonKyu Park, Junghun Oh, Kanggeon Lee, Wooseok Lee, Juneyoung Ro, Turghun Osman, Can Hu, Chaoyang Liao, Cheng Chen, Chengcheng Han, Chenhao Qiu, Chong Peng, Cong Xu, Dailin Li, Feiyu Wang, Feng Gao, Guibo Zhu, Guopeng Tang, Haibo Lu, Han Fang, Han Qi, Hanxiao Wu, Haobo Cheng, Hongbo Sun, Hongyao Chen, Huayong Hu, Hui Li, Jiaheng Ma, Jiang Yu, Jianing Wang, Jie Yang, Jing He, Jinglin Zhou, Jingxuan Li, Josef Kittler, Lihao Zheng, Linnan Zhao, Mengxi Jia, Muyang Yan, Nguyen Thanh Thien, Pu Luo, Qi Li, Shien Song, Shijie Dong, Shuai Shao, Shutao Li, Taofeng Xue, Tianyang Xu, Tianyi Gao, Tingting Li, Wei Zhang, Weiyang Su, Xiaodong Dong, Xiao-Jun Wu, Xiaopeng Zhou, Xin Chen, Xin Wei, Xinyi You, Xudong Kang, Xujie Zhou, Xusheng Liu, Yanan Wang, Yanbin Huang, Yang Liu, Yang Yang, Yanglin Deng, Yashu Kang, Ye Yuan, Yi Wen, Yicen Tian, Yilin Tao, Yin Tang, Yipeng Lin, Yiqing Wang, Yiting Xi, Yongkang Yu, Yumei Li, Yuxin Qin, Yuying Chen, Yuzhe Cen, Zhaofan Zou, Zhaohong Liu, Zhehao Shen, Zhenglin Du, Zhengyang Li, Zhenni Huang, Zhenwei Shao, Zhilong Song, Zhiyong Feng, Zhiyu Wang, Zhou Yu, Ziang Li, Zihan Zhai, Zijian Zhang, Ziyang Peng, Ziyun Xiao, Zongshu Li</strong></p>
<p>This paper reviews the MARS2 2025 Challenge on Multimodal Reasoning. We aim to bring together different approaches in multimodal machine learning and LLMs via a large benchmark. We hope it better allows researchers to follow the state-of-the-art in this very dynamic area. Meanwhile, a growing number of testbeds have boosted the evolution of general-purpose large language models. Thus, this yearâ€™s MARS2 focuses on real-world and specialized scenarios to broaden the multimodal reasoning applications of MLLMs. Our organizing team released two tailored datasets Lens and AdsQA as test sets, which support general reasoning in 12 daily scenarios and domain-specific reasoning in advertisement videos, respectively. We evaluated 40+ baselines that include both generalist MLLMs and task-specific models, and opened up three competition tracks, i.e., Visual Grounding in Real-world Scenarios (VG-RS), Visual Question Answering with Spatial Awareness (VQA-SA), and Visual Reasoning in Creative Advertisement Videos (VR-Ads). Finally, 76 teams from the renowned academic and industrial institutions have registered and 40+ valid submissions (out of 1200+) have been included in our ranking lists. Our datasets, code sets (40+ baselines and 15+ participantsâ€™ methods), and rankings are publicly available on the MARS2 workshop website and our GitHub organization page <a target="_blank" rel="noopener" href="https://github.com/mars2workshop/">https://github.com/mars2workshop/</a>, where our updates and announcements of upcoming events will be continuously provided. </p>
<blockquote>
<p>æœ¬æ–‡å›é¡¾äº†å…³äºå¤šæ¨¡æ€æ¨ç†çš„MARS2 2025æŒ‘æˆ˜èµ›ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯é€šè¿‡å¤§è§„æ¨¡åŸºå‡†æµ‹è¯•ï¼Œæ±‡é›†å¤šç§å¤šæ¨¡æ€æœºå™¨å­¦ä¹ å’Œå¤§å‹è¯­è¨€æ¨¡å‹çš„æ–¹æ³•ã€‚æˆ‘ä»¬å¸Œæœ›è¿™èƒ½æ›´å¥½åœ°è®©ç ”ç©¶äººå‘˜åœ¨è¿™ä¸ªå……æ»¡åŠ¨æ€å˜åŒ–çš„é¢†åŸŸè¿½è¸ªæœ€æ–°è¿›å±•ã€‚ä¸æ­¤åŒæ—¶ï¼Œè¶Šæ¥è¶Šå¤šçš„æµ‹è¯•å¹³å°æ¨åŠ¨äº†é€šç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„è¿›åŒ–ã€‚å› æ­¤ï¼Œä»Šå¹´çš„MARS2é‡ç‚¹å…³æ³¨ç°å®å’Œç‰¹å®šåœºæ™¯ï¼Œä»¥æ‹“å±•å¤šæ¨¡æ€æ¨ç†åœ¨å¤§å‹è¯­è¨€æ¨¡å‹çš„åº”ç”¨é¢†åŸŸã€‚æˆ‘ä»¬çš„ç»„ç»‡å›¢é˜Ÿå‘å¸ƒäº†ä¸¤ä¸ªå®šåˆ¶æ•°æ®é›†Lenså’ŒAdsQAä½œä¸ºæµ‹è¯•é›†ï¼Œåˆ†åˆ«æ”¯æŒæ—¥å¸¸åœºæ™¯ä¸­çš„é€šç”¨æ¨ç†å’Œå¹¿å‘Šè§†é¢‘ä¸­çš„é¢†åŸŸç‰¹å®šæ¨ç†ã€‚æˆ‘ä»¬è¯„ä¼°äº†è¶…è¿‡40ä¸ªåŸºçº¿æ¨¡å‹ï¼ŒåŒ…æ‹¬é€šç”¨çš„å¤§å‹è¯­è¨€æ¨¡å‹å’Œç‰¹å®šä»»åŠ¡çš„æ¨¡å‹ï¼Œå¹¶å¼€è®¾äº†ä¸‰ä¸ªç«èµ›èµ›é“ï¼Œå³ç°å®ä¸–ç•Œåœºæ™¯ä¸­çš„è§†è§‰å®šä½ï¼ˆVG-RSï¼‰ã€å…·æœ‰ç©ºé—´æ„è¯†çš„è§†è§‰é—®ç­”ï¼ˆVQA-SAï¼‰å’Œåœ¨åˆ›æ„å¹¿å‘Šè§†é¢‘ä¸­çš„è§†è§‰æ¨ç†ï¼ˆVR-Adsï¼‰ã€‚æœ€åï¼Œæ¥è‡ªè‘—åå­¦æœ¯å’Œå·¥ä¸šæœºæ„çš„76æ”¯é˜Ÿä¼å·²ç»æ³¨å†Œï¼Œè¶…è¿‡40ä¸ªæœ‰æ•ˆæäº¤ï¼ˆæ€»è®¡è¶…è¿‡1200ä»½ï¼‰å·²è¢«çº³å…¥æˆ‘ä»¬çš„æ’ååˆ—è¡¨ã€‚æˆ‘ä»¬çš„æ•°æ®é›†ã€ä»£ç é›†ï¼ˆåŒ…å«è¶…è¿‡å››åç§åŸºçº¿å’Œå¤šç§å‚èµ›è€…çš„æ–¹æ³•ï¼‰ã€ä»¥åŠæ’åå‡å…¬å¼€å¯åœ¨MARS2ç ”è®¨ä¼šç½‘ç«™å’Œæˆ‘ä»¬çš„GitHubç»„ç»‡é¡µé¢æŸ¥çœ‹ï¼š<a target="_blank" rel="noopener" href="https://github.com/mars2workshop/">https://github.com/mars2workshop/</a>ï¼Œæˆ‘ä»¬å°†ä¸æ–­æ›´æ–°å¹¶å…¬å¸ƒå³å°†å‘å¸ƒçš„æ´»åŠ¨ä¿¡æ¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.14142v1">PDF</a> ICCV 2025 MARS2 Workshop and Challenge â€œMultimodal Reasoning and Slow   Thinking in the Large Model Era: Towards System 2 and Beyondâ€™â€™</p>
<p><strong>Summary</strong>ï¼š<br>æ­¤æ–‡ç« ä»‹ç»äº†MARS2 2025æŒ‘æˆ˜èµ›çš„å¤šæ¨¡æ€æ¨ç†å†…å®¹ã€‚è¯¥æŒ‘æˆ˜æ—¨åœ¨é€šè¿‡å¤§å‹åŸºå‡†æµ‹è¯•ï¼Œæ±‡é›†ä¸åŒçš„å¤šæ¨¡æ€æœºå™¨å­¦ä¹ å’Œå¤§å‹è¯­è¨€æ¨¡å‹æ–¹æ³•ã€‚ä»Šå¹´ï¼ŒMARS2çš„é‡ç‚¹åœ¨äºç°å®å’Œç‰¹å®šåœºæ™¯ï¼Œä»¥æ‹“å±•å¤šæ¨¡æ€æ¨ç†çš„åº”ç”¨èŒƒå›´ã€‚æ­¤å¤–ï¼Œè¯¥æŒ‘æˆ˜é‡Šæ”¾äº†ä¸¤ä¸ªæ•°æ®é›†Lenså’ŒAdsQAä½œä¸ºæµ‹è¯•é›†ï¼Œå¹¶è¯„ä¼°äº†è¶…è¿‡å››åç§åŸºçº¿æ¨¡å‹ï¼Œå¼€è¾Ÿäº†ä¸‰å¤§ç«èµ›èµ›é“ã€‚æ¥è‡ªçŸ¥åå­¦æœ¯å’Œå·¥ä¸šæœºæ„çš„ä¸ƒåå…­æ”¯å›¢é˜Ÿå·²ç»æ³¨å†Œå‚ä¸ï¼Œå·²æœ‰å››åå¤šæ¬¡æœ‰æ•ˆæäº¤è®°å½•äºæ’åæ¦œå•ã€‚æ•°æ®é›†ã€ä»£ç é›†å’Œæ’åå‡å…¬å¼€äºMARS2ç ”è®¨ä¼šç½‘ç«™å’ŒGitHubç»„ç»‡é¡µé¢ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ul>
<li>MARS2æŒ‘æˆ˜è‡´åŠ›äºæ¨åŠ¨å¤šæ¨¡æ€æœºå™¨å­¦ä¹ å’Œå¤§å‹è¯­è¨€æ¨¡å‹çš„å‘å±•å’Œåº”ç”¨ã€‚</li>
<li>é€šè¿‡å¤§å‹åŸºå‡†æµ‹è¯•ï¼Œè¯¥æŒ‘æˆ˜é›†ç»“äº†å¤šç§å¤šæ¨¡æ€æœºå™¨å­¦ä¹ çš„æ–¹æ³•ã€‚</li>
<li>MARS2ä¸“æ³¨äºç°å®å’Œç‰¹å®šåœºæ™¯çš„å¤šæ¨¡æ€æ¨ç†åº”ç”¨æ‹“å±•ã€‚</li>
<li>é‡Šæ”¾äº†ä¸¤ä¸ªæ•°æ®é›†Lenså’ŒAdsQAä»¥æ”¯æŒé€šç”¨å’Œç‰¹å®šé¢†åŸŸçš„æ¨ç†ä»»åŠ¡ã€‚</li>
<li>å¯¹è¶…è¿‡å››åç§åŸºçº¿æ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ï¼ŒåŒ…æ‹¬é€šç”¨MLLMså’Œä»»åŠ¡ç‰¹å®šæ¨¡å‹ã€‚</li>
<li>ç«èµ›åŒ…æ‹¬ä¸‰å¤§èµ›é“ï¼šç°å®ä¸–ç•Œåœºæ™¯ä¸­çš„è§†è§‰å®šä½ã€å…·æœ‰ç©ºé—´æ„è¯†çš„è§†è§‰é—®ç­”å’Œåˆ›æ„å¹¿å‘Šè§†é¢‘ä¸­çš„è§†è§‰æ¨ç†ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14142">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.14142v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.14142v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.14142v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Reasoning-Efficiently-Through-Adaptive-Chain-of-Thought-Compression-A-Self-Optimizing-Framework"><a href="#Reasoning-Efficiently-Through-Adaptive-Chain-of-Thought-Compression-A-Self-Optimizing-Framework" class="headerlink" title="Reasoning Efficiently Through Adaptive Chain-of-Thought Compression: A   Self-Optimizing Framework"></a>Reasoning Efficiently Through Adaptive Chain-of-Thought Compression: A   Self-Optimizing Framework</h2><p><strong>Authors:Kerui Huang, Shuhan Liu, Xing Hu, Tongtong Xu, Lingfeng Bao, Xin Xia</strong></p>
<p>Chain-of-Thought (CoT) reasoning enhances Large Language Models (LLMs) by prompting intermediate steps, improving accuracy and robustness in arithmetic, logic, and commonsense tasks. However, this benefit comes with high computational costs: longer outputs increase latency, memory usage, and KV-cache demands. These issues are especially critical in software engineering tasks where concise and deterministic outputs are required. To investigate these trade-offs, we conduct an empirical study based on code generation benchmarks. The results reveal that longer CoT does not always help. Excessive reasoning often causes truncation, accuracy drops, and latency up to five times higher, with failed outputs consistently longer than successful ones. These findings challenge the assumption that longer reasoning is inherently better and highlight the need for adaptive CoT control. Motivated by this, we propose SEER (Self-Enhancing Efficient Reasoning), an adaptive framework that compresses CoT while preserving accuracy. SEER combines Best-of-N sampling with task-aware adaptive filtering, dynamically adjusting thresholds based on pre-inference outputs to reduce verbosity and computational overhead. We then evaluate SEER on three software engineering tasks and one math task. On average, SEER shortens CoT by 42.1%, improves accuracy by reducing truncation, and eliminates most infinite loops. These results demonstrate SEER as a practical method to make CoT-enhanced LLMs more efficient and robust, even under resource constraints. </p>
<blockquote>
<p>â€œé“¾å¼æ€ç»´ï¼ˆChain-of-Thoughtï¼Œç®€ç§°CoTï¼‰é€šè¿‡æç¤ºä¸­é—´æ­¥éª¤å¢å¼ºäº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åŠŸèƒ½ï¼Œæé«˜äº†ç®—æœ¯ã€é€»è¾‘å’Œå¸¸è¯†ä»»åŠ¡çš„å‡†ç¡®æ€§å’Œç¨³å¥æ€§ã€‚ç„¶è€Œï¼Œè¿™ä¸€ä¼˜åŠ¿å¸¦æ¥äº†è¾ƒé«˜çš„è®¡ç®—æˆæœ¬ï¼šè¾ƒé•¿çš„è¾“å‡ºå¢åŠ äº†å»¶è¿Ÿã€å†…å­˜ä½¿ç”¨å’ŒKVç¼“å­˜éœ€æ±‚ã€‚è¿™äº›é—®é¢˜åœ¨éœ€è¦ç®€æ´å’Œç¡®å®šæ€§è¾“å‡ºçš„è½¯ä»¶å·¥ç¨‹ä»»åŠ¡ä¸­å°¤å…¶å…³é”®ã€‚ä¸ºäº†è°ƒæŸ¥è¿™äº›æƒè¡¡ï¼Œæˆ‘ä»¬åŸºäºä»£ç ç”ŸæˆåŸºå‡†æµ‹è¯•è¿›è¡Œäº†å®è¯ç ”ç©¶ã€‚ç»“æœæ˜¾ç¤ºï¼Œè¿‡é•¿çš„CoTå¹¶ä¸æ€»æ˜¯æœ‰å¸®åŠ©ã€‚è¿‡å¤šçš„æ¨ç†å¾€å¾€ä¼šå¯¼è‡´æˆªæ–­ã€å‡†ç¡®æ€§ä¸‹é™å’Œé«˜è¾¾äº”å€çš„å»¶è¿Ÿï¼Œå¤±è´¥çš„è¾“å‡ºå§‹ç»ˆæ¯”æˆåŠŸçš„è¾“å‡ºé•¿ã€‚è¿™äº›å‘ç°æŒ‘æˆ˜äº†â€œæ›´é•¿çš„æ¨ç†æœ¬è´¨ä¸Šæ›´å¥½â€çš„å‡è®¾ï¼Œå¹¶å¼ºè°ƒäº†è‡ªé€‚åº”CoTæ§åˆ¶çš„å¿…è¦æ€§ã€‚å—æ­¤å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†SEERï¼ˆè‡ªæˆ‘å¢å¼ºé«˜æ•ˆæ¨ç†ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§è‡ªé€‚åº”æ¡†æ¶ï¼Œå¯ä»¥åœ¨ä¿æŒå‡†ç¡®æ€§çš„åŒæ—¶å‹ç¼©CoTã€‚SEERç»“åˆäº†æœ€ä½³Né‡‡æ ·å’ŒåŸºäºä»»åŠ¡çš„è‡ªé€‚åº”è¿‡æ»¤ï¼Œæ ¹æ®é¢„æ¨ç†è¾“å‡ºåŠ¨æ€è°ƒæ•´é˜ˆå€¼ï¼Œä»¥å‡å°‘å†—ä½™å’Œè®¡ç®—å¼€é”€ã€‚ç„¶åæˆ‘ä»¬åœ¨ä¸‰ä¸ªè½¯ä»¶å·¥ç¨‹ä»»åŠ¡å’Œä¸€ä¸ªæ•°å­¦ä»»åŠ¡ä¸Šè¯„ä¼°äº†SEERã€‚å¹³å‡è€Œè¨€ï¼ŒSEERå°†CoTç¼©çŸ­äº†42.1%ï¼Œé€šè¿‡å‡å°‘æˆªæ–­æé«˜äº†å‡†ç¡®æ€§ï¼Œå¹¶æ¶ˆé™¤äº†å¤§å¤šæ•°æ— é™å¾ªç¯ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒSEERæ˜¯ä¸€ç§å®ç”¨çš„æ–¹æ³•ï¼Œå¯ä»¥ä½¿CoTå¢å¼ºå‹LLMæ›´åŠ é«˜æ•ˆå’Œç¨³å¥ï¼Œå³ä½¿åœ¨èµ„æºå—é™çš„æƒ…å†µä¸‹ä¹Ÿæ˜¯å¦‚æ­¤ã€‚â€</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.14093v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºChain-of-Thoughtï¼ˆCoTï¼‰æ¨ç†çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç®—æœ¯ã€é€»è¾‘å’Œå¸¸è¯†ä»»åŠ¡ä¸­é€šè¿‡æç¤ºä¸­é—´æ­¥éª¤æé«˜äº†å‡†ç¡®æ€§å’Œç¨³å¥æ€§ã€‚ç„¶è€Œï¼Œè¿™ä¸€ä¼˜åŠ¿ä¼´éšç€è¾ƒé«˜çš„è®¡ç®—æˆæœ¬ï¼šæ›´é•¿çš„è¾“å‡ºå¢åŠ äº†å»¶è¿Ÿã€å†…å­˜ä½¿ç”¨é‡å’ŒKVç¼“å­˜éœ€æ±‚ã€‚åœ¨è½¯ä»¶å·¥ç¨‹ä»»åŠ¡ä¸­ï¼Œè¿™äº›é—®é¢˜å°¤ä¸ºå…³é”®ï¼Œéœ€è¦ç®€æ´å’Œç¡®å®šçš„è¾“å‡ºã€‚é€šè¿‡å®è¯ç ”ç©¶å‘ç°ï¼Œè¿‡é•¿çš„CoTå¹¶ä¸æ€»æ˜¯æœ‰å¸®åŠ©ã€‚è¿‡å¤šçš„æ¨ç†ä¼šå¯¼è‡´æˆªæ–­ã€å‡†ç¡®æ€§ä¸‹é™å’Œé«˜è¾¾äº”å€çš„å»¶è¿Ÿï¼Œå¤±è´¥çš„è¾“å‡ºé€šå¸¸æ¯”æˆåŠŸçš„è¾“å‡ºæ›´é•¿ã€‚å› æ­¤ï¼Œéœ€è¦è‡ªé€‚åº”çš„CoTæ§åˆ¶ã€‚å—æ­¤å¯å‘ï¼Œæå‡ºäº†SEERï¼ˆè‡ªé€‚åº”é«˜æ•ˆæ¨ç†ï¼‰ï¼Œä¸€ä¸ªå‹ç¼©CoTåŒæ—¶ä¿æŒå‡†ç¡®æ€§çš„è‡ªé€‚åº”æ¡†æ¶ã€‚SEERç»“åˆBest-of-Né‡‡æ ·å’Œä»»åŠ¡æ„ŸçŸ¥è‡ªé€‚åº”è¿‡æ»¤ï¼Œæ ¹æ®é¢„æ¨ç†è¾“å‡ºåŠ¨æ€è°ƒæ•´é˜ˆå€¼ï¼Œå‡å°‘å†—é•¿å’Œè®¡ç®—å¼€é”€ã€‚åœ¨ä¸‰é¡¹è½¯ä»¶å·¥ç¨‹ä»»åŠ¡å’Œä¸€é¡¹æ•°å­¦ä»»åŠ¡ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒSEERå¹³å‡ç¼©çŸ­CoT 42.1%ï¼Œæé«˜äº†å‡†ç¡®æ€§ï¼Œå‡å°‘äº†æˆªæ–­ï¼Œå¹¶æ¶ˆé™¤äº†å¤§å¤šæ•°æ— é™å¾ªç¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>CoTæ¨ç†åœ¨LLMä¸­æé«˜äº†ç®—æœ¯ã€é€»è¾‘å’Œå¸¸è¯†ä»»åŠ¡çš„å‡†ç¡®æ€§å’Œç¨³å¥æ€§ã€‚</li>
<li>è¿‡é•¿çš„CoTä¼šå¢åŠ è®¡ç®—æˆæœ¬ï¼ŒåŒ…æ‹¬å»¶è¿Ÿã€å†…å­˜å’ŒKVç¼“å­˜éœ€æ±‚ã€‚</li>
<li>è¿‡å¤šçš„æ¨ç†å¯èƒ½å¯¼è‡´è¾“å‡ºæˆªæ–­ã€å‡†ç¡®æ€§ä¸‹é™å’Œæ›´é«˜çš„å»¶è¿Ÿã€‚</li>
<li>éœ€è¦è‡ªé€‚åº”çš„CoTæ§åˆ¶ä»¥å¹³è¡¡æ¨ç†é•¿åº¦å’Œæ¨¡å‹æ•ˆç‡ã€‚</li>
<li>SEERæ¡†æ¶é€šè¿‡å‹ç¼©CoTåŒæ—¶ä¿æŒå‡†ç¡®æ€§æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14093">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.14093v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.14093v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.14093v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="SAIL-VL2-Technical-Report"><a href="#SAIL-VL2-Technical-Report" class="headerlink" title="SAIL-VL2 Technical Report"></a>SAIL-VL2 Technical Report</h2><p><strong>Authors:Weijie Yin, Yongjie Ye, Fangxun Shu, Yue Liao, Zijian Kang, Hongyuan Dong, Haiyang Yu, Dingkang Yang, Jiacong Wang, Han Wang, Wenzhuo Liu, Xiao Liang, Shuicheng Yan, Chao Feng</strong></p>
<p>We introduce SAIL-VL2, an open-suite vision-language foundation model (LVM) for comprehensive multimodal understanding and reasoning. As the successor to SAIL-VL, SAIL-VL2 achieves state-of-the-art performance at the 2B and 8B parameter scales across diverse image and video benchmarks, demonstrating strong capabilities from fine-grained perception to complex reasoning. Three core innovations drive its effectiveness. First, a large-scale data curation pipeline with scoring and filtering strategies enhances both quality and distribution across captioning, OCR, QA, and video data, improving training efficiency. Second, a progressive training framework begins with a powerful pre-trained vision encoder (SAIL-ViT), advances through multimodal pre-training, and culminates in a thinking-fusion SFT-RL hybrid paradigm that systematically strengthens model capabilities. Third, architectural advances extend beyond dense LLMs to efficient sparse Mixture-of-Experts (MoE) designs. With these contributions, SAIL-VL2 demonstrates competitive performance across 106 datasets and achieves state-of-the-art results on challenging reasoning benchmarks such as MMMU and MathVista. Furthermore, on the OpenCompass leaderboard, SAIL-VL2-2B ranks first among officially released open-source models under the 4B parameter scale, while serving as an efficient and extensible foundation for the open-source multimodal community. </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºSAIL-VL2ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºå…¨é¢å¤šæ¨¡æ€ç†è§£å’Œæ¨ç†çš„å¼€æ”¾è§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹ï¼ˆLVMï¼‰ã€‚ä½œä¸ºSAIL-VLçš„ç»§ä»»è€…ï¼ŒSAIL-VL2åœ¨ä¸åŒçš„å›¾åƒå’Œè§†é¢‘åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†2Bå’Œ8Bå‚æ•°è§„æ¨¡çš„æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œè¡¨ç°å‡ºä»ç²¾ç»†ç²’åº¦æ„ŸçŸ¥åˆ°å¤æ‚æ¨ç†çš„å¼ºå¤§èƒ½åŠ›ã€‚ä¸‰ä¸ªæ ¸å¿ƒåˆ›æ–°é©±åŠ¨äº†å…¶æœ‰æ•ˆæ€§ã€‚é¦–å…ˆï¼Œé‡‡ç”¨å¤§è§„æ¨¡æ•°æ®æ•´ç†ç®¡é“ï¼Œç»“åˆè¯„åˆ†å’Œè¿‡æ»¤ç­–ç•¥ï¼Œæé«˜äº†æè¿°ã€OCRã€é—®ç­”å’Œè§†é¢‘æ•°æ®çš„è´¨é‡å’Œåˆ†å¸ƒï¼Œæé«˜äº†è®­ç»ƒæ•ˆç‡ã€‚å…¶æ¬¡ï¼Œæ¸è¿›å¼è®­ç»ƒæ¡†æ¶ä»ä¸€ä¸ªå¼ºå¤§çš„é¢„è®­ç»ƒè§†è§‰ç¼–ç å™¨ï¼ˆSAIL-ViTï¼‰å¼€å§‹ï¼Œé€šè¿‡å¤šæ¨¡æ€é¢„è®­ç»ƒï¼Œæœ€ç»ˆå½¢æˆä¸€ä¸ªæ€è€ƒèåˆSFT-RLæ··åˆèŒƒå¼ï¼Œç³»ç»Ÿåœ°å¢å¼ºäº†æ¨¡å‹çš„èƒ½åŠ›ã€‚ç¬¬ä¸‰ï¼Œæ¶æ„çš„æ”¹è¿›è¶…è¶Šäº†å¯†é›†çš„LLMï¼Œé‡‡ç”¨äº†é«˜æ•ˆçš„ç¨€ç–æ··åˆä¸“å®¶ï¼ˆMoEï¼‰è®¾è®¡ã€‚é€šè¿‡è¿™äº›è´¡çŒ®ï¼ŒSAIL-VL2åœ¨106ä¸ªæ•°æ®é›†ä¸Šè¡¨ç°å‡ºç«äº‰åŠ›ï¼Œå¹¶åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ¨ç†åŸºå‡†æµ‹è¯•ï¼ˆå¦‚MMMUå’ŒMathVistaï¼‰ä¸Šè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚æ­¤å¤–ï¼Œåœ¨OpenCompassæ’è¡Œæ¦œä¸Šï¼ŒSAIL-VL2-2Båœ¨å®˜æ–¹å‘å¸ƒçš„å¼€æºæ¨¡å‹ä¸­ä½åˆ—4Bå‚æ•°è§„æ¨¡ä¹‹é¦–ï¼Œæˆä¸ºå¼€æºå¤šæ¨¡æ€ç¤¾åŒºçš„æ•ˆç‡å’Œå¯æ‰©å±•æ€§çš„åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.14033v1">PDF</a> Technical Report</p>
<p><strong>æ‘˜è¦</strong></p>
<p>SAIL-VL2æ˜¯ä¸€æ¬¾ç”¨äºå…¨é¢å¤šæ¨¡å¼ç†è§£å’Œæ¨ç†çš„å¼€æ”¾è§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹ï¼ˆLVMï¼‰ã€‚ä½œä¸ºSAIL-VLçš„ç»§ä»»è€…ï¼ŒSAIL-VL2åœ¨2Bå’Œ8Bå‚æ•°è§„æ¨¡ä¸Šå®ç°äº†è·¨å›¾åƒå’Œè§†é¢‘åŸºå‡†æµ‹è¯•çš„æœ€ä½³æ€§èƒ½ï¼Œå±•ç°å‡ºä»ç²¾ç»†ç²’åº¦æ„ŸçŸ¥åˆ°å¤æ‚æ¨ç†çš„å¼ºåŠ²èƒ½åŠ›ã€‚å…¶ä¸‰å¤§æ ¸å¿ƒåˆ›æ–°é©±åŠ¨äº†æ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚é¦–å…ˆï¼Œå¤§è§„æ¨¡æ•°æ®æ”¶é›†ç®¡é“é€šè¿‡è¯„åˆ†å’Œè¿‡æ»¤ç­–ç•¥æé«˜äº†å­—å¹•ã€OCRã€QAå’Œè§†é¢‘æ•°æ®çš„è´¨é‡å’Œåˆ†å¸ƒï¼Œæé«˜äº†è®­ç»ƒæ•ˆç‡ã€‚å…¶æ¬¡ï¼Œæ¸è¿›å¼è®­ç»ƒæ¡†æ¶ä»¥å¼ºå¤§çš„é¢„è®­ç»ƒè§†è§‰ç¼–ç å™¨ï¼ˆSAIL-ViTï¼‰å¼€å§‹ï¼Œé€šè¿‡å¤šæ¨¡å¼é¢„è®­ç»ƒï¼Œæœ€ç»ˆé‡‡ç”¨æ€è€ƒèåˆSFT-RLæ··åˆèŒƒå¼ï¼Œç³»ç»Ÿæ€§åœ°å¢å¼ºäº†æ¨¡å‹èƒ½åŠ›ã€‚æœ€åï¼Œæ¶æ„çš„æ”¹è¿›è¶…è¶Šäº†å¯†é›†çš„LLMsï¼Œé‡‡ç”¨äº†é«˜æ•ˆçš„ç¨€ç–æ··åˆä¸“å®¶ï¼ˆMoEï¼‰è®¾è®¡ã€‚é€šè¿‡è¿™äº›è´¡çŒ®ï¼ŒSAIL-VL2åœ¨106ä¸ªæ•°æ®é›†ä¸Šè¡¨ç°å‡ºç«äº‰åŠ›ï¼Œå¹¶åœ¨MMMUå’ŒMathVistaç­‰å…·æœ‰æŒ‘æˆ˜æ€§çš„æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æœ€ä½³çŠ¶æ€ã€‚æ­¤å¤–ï¼Œåœ¨OpenCompassæ’è¡Œæ¦œä¸Šï¼ŒSAIL-VL2-2Båœ¨å®˜æ–¹å‘å¸ƒçš„å¼€æºæ¨¡å‹ä¸­æ’åç¬¬ä¸€ï¼Œæˆä¸ºå¼€æºå¤šæ¨¡å¼ç¤¾åŒºçš„æœ‰æ•ˆå’Œå¯æ‰©å±•åŸºç¡€ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>SAIL-VL2æ˜¯ä¸€æ¬¾å…ˆè¿›çš„è§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹ï¼ˆLVMï¼‰ï¼Œç”¨äºå¤šæ¨¡å¼ç†è§£å’Œæ¨ç†ã€‚</li>
<li>ä½œä¸ºSAIL-VLçš„å‡çº§ç‰ˆï¼Œå®ƒåœ¨å¤šç§å›¾åƒå’Œè§†é¢‘åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>SAIL-VL2çš„ä¸‰å¤§æ ¸å¿ƒåˆ›æ–°åŒ…æ‹¬å¤§è§„æ¨¡æ•°æ®æ”¶é›†ç®¡é“ã€æ¸è¿›å¼è®­ç»ƒæ¡†æ¶å’Œæ¶æ„æ”¹è¿›ã€‚</li>
<li>æ•°æ®æ”¶é›†ç®¡é“é€šè¿‡æé«˜æ•°æ®å’Œè®­ç»ƒæ•ˆç‡çš„ç­–ç•¥ï¼Œå¢å¼ºäº†æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>æ¸è¿›å¼è®­ç»ƒæ¡†æ¶é€šè¿‡ç»“åˆé¢„è®­ç»ƒã€å¤šæ¨¡å¼é¢„è®­ç»ƒå’Œæ€è€ƒèåˆï¼Œå¢å¼ºäº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>SAIL-VL2é‡‡ç”¨äº†æ··åˆä¸“å®¶è®¾è®¡ï¼Œæé«˜äº†æ¨¡å‹çš„æ•ˆç‡å’Œæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14033">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.14033v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.14033v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.14033v1/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.14033v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.14033v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Large-Language-Model-Empowered-Decision-Transformer-for-UAV-Enabled-Data-Collection"><a href="#Large-Language-Model-Empowered-Decision-Transformer-for-UAV-Enabled-Data-Collection" class="headerlink" title="Large Language Model-Empowered Decision Transformer for UAV-Enabled Data   Collection"></a>Large Language Model-Empowered Decision Transformer for UAV-Enabled Data   Collection</h2><p><strong>Authors:Zhixion Chen, Jiangzhou Wang, and Hyundong Shin, Arumugam Nallanathan</strong></p>
<p>The deployment of unmanned aerial vehicles (UAVs) for reliable and energy-efficient data collection from spatially distributed devices holds great promise in supporting diverse Internet of Things (IoT) applications. Nevertheless, the limited endurance and communication range of UAVs necessitate intelligent trajectory planning. While reinforcement learning (RL) has been extensively explored for UAV trajectory optimization, its interactive nature entails high costs and risks in real-world environments. Offline RL mitigates these issues but remains susceptible to unstable training and heavily rely on expert-quality datasets. To address these challenges, we formulate a joint UAV trajectory planning and resource allocation problem to maximize energy efficiency of data collection. The resource allocation subproblem is first transformed into an equivalent linear programming formulation and solved optimally with polynomial-time complexity. Then, we propose a large language model (LLM)-empowered critic-regularized decision transformer (DT) framework, termed LLM-CRDT, to learn effective UAV control policies. In LLM-CRDT, we incorporate critic networks to regularize the DT model training, thereby integrating the sequence modeling capabilities of DT with critic-based value guidance to enable learning effective policies from suboptimal datasets. Furthermore, to mitigate the data-hungry nature of transformer models, we employ a pre-trained LLM as the transformer backbone of the DT model and adopt a parameter-efficient fine-tuning strategy, i.e., LoRA, enabling rapid adaptation to UAV control tasks with small-scale dataset and low computational overhead. Extensive simulations demonstrate that LLM-CRDT outperforms benchmark online and offline RL methods, achieving up to 36.7% higher energy efficiency than the current state-of-the-art DT approaches. </p>
<blockquote>
<p>åœ¨ç‰©è”ç½‘ï¼ˆIoTï¼‰çš„ä¼—å¤šåº”ç”¨ä¸­ï¼Œåˆ©ç”¨æ— äººæœºï¼ˆUAVï¼‰è¿›è¡Œå¯é ä¸”èƒ½æºé«˜æ•ˆçš„æ•°æ®æ”¶é›†æœ‰ç€å·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œæ— äººæœºçš„ç»­èˆªå’Œé€šä¿¡èŒƒå›´æœ‰é™ï¼Œéœ€è¦è¿›è¡Œæ™ºèƒ½è½¨è¿¹è§„åˆ’ã€‚å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨æ— äººæœºè½¨è¿¹ä¼˜åŒ–æ–¹é¢å·²å¾—åˆ°å¹¿æ³›ç ”ç©¶ï¼Œä½†å…¶äº¤äº’æ€§è´¨å¯¼è‡´åœ¨å®é™…ç¯å¢ƒä¸­çš„æˆæœ¬é«˜å’Œé£é™©å¤§ã€‚ç¦»çº¿å¼ºåŒ–å­¦ä¹ ç¼“è§£äº†è¿™äº›é—®é¢˜ï¼Œä½†ä»æ˜“å—åˆ°è®­ç»ƒä¸ç¨³å®šçš„å½±å“ï¼Œå¹¶ä¸¥é‡ä¾èµ–äºä¸“å®¶çº§æ•°æ®é›†ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬åˆ¶å®šäº†ä¸€ä¸ªè”åˆæ— äººæœºè½¨è¿¹è§„åˆ’å’Œèµ„æºåˆ†é…é—®é¢˜ï¼Œä»¥æœ€å¤§åŒ–æ•°æ®æ”¶é›†çš„èƒ½é‡æ•ˆç‡ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å°†èµ„æºåˆ†é…å­é—®é¢˜è½¬åŒ–ä¸ºç­‰æ•ˆçš„çº¿æ€§è§„åˆ’å½¢å¼ï¼Œå¹¶ä»¥å¤šé¡¹å¼æ—¶é—´å¤æ‚åº¦æœ€ä¼˜åœ°è§£å†³ã€‚ç„¶åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”±å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èµ‹èƒ½çš„è¯„è®ºå®¶æ­£åˆ™åŒ–å†³ç­–è½¬æ¢å™¨ï¼ˆDTï¼‰æ¡†æ¶ï¼Œç§°ä¸ºLLM-CRDTï¼Œç”¨äºå­¦ä¹ æœ‰æ•ˆçš„æ— äººæœºæ§åˆ¶ç­–ç•¥ã€‚åœ¨LLM-CRDTä¸­ï¼Œæˆ‘ä»¬ç»“åˆè¯„è®ºå®¶ç½‘ç»œæ¥è§„èŒƒDTæ¨¡å‹çš„è®­ç»ƒï¼Œä»è€Œå°†DTçš„åºåˆ—å»ºæ¨¡èƒ½åŠ›ä¸åŸºäºè¯„è®ºå®¶çš„ä»·å€¼æŒ‡å¯¼ç›¸ç»“åˆï¼Œä½¿èƒ½å¤Ÿä»æ¬¡ä¼˜æ•°æ®é›†ä¸­å­¦ä¹ æœ‰æ•ˆç­–ç•¥ã€‚æ­¤å¤–ï¼Œä¸ºäº†ç¼“è§£è½¬æ¢å™¨æ¨¡å‹å¯¹æ•°æ®çš„éœ€æ±‚ï¼Œæˆ‘ä»¬é‡‡ç”¨é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºDTæ¨¡å‹çš„è½¬æ¢å™¨éª¨å¹²ï¼Œå¹¶é‡‡ç”¨å‚æ•°é«˜æ•ˆçš„å¾®è°ƒç­–ç•¥ï¼ˆå³LoRAï¼‰ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿä»¥å°è§„æ¨¡æ•°æ®é›†å’Œä½è®¡ç®—å¼€é”€å¿«é€Ÿé€‚åº”æ— äººæœºæ§åˆ¶ä»»åŠ¡ã€‚å¤§é‡æ¨¡æ‹Ÿå®éªŒè¡¨æ˜ï¼ŒLLM-CRDTåœ¨åœ¨çº¿å’Œç¦»çº¿å¼ºåŒ–å­¦ä¹ æ–¹æ³•ä¸Šçš„è¡¨ç°å‡ä¼˜äºåŸºå‡†æ–¹æ³•ï¼Œä¸å½“å‰æœ€å…ˆè¿›çš„å†³ç­–æ ‘æ–¹æ³•ç›¸æ¯”ï¼Œèƒ½æºæ•ˆç‡æé«˜äº†é«˜è¾¾36.7ï¼…ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13934v1">PDF</a> 14pages, 8 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†åˆ©ç”¨æ— äººæœºï¼ˆUAVsï¼‰è¿›è¡Œå¯é ä¸”èƒ½æºé«˜æ•ˆçš„æ•°æ®é‡‡é›†ä»¥æ”¯æŒç‰©è”ç½‘ï¼ˆIoTï¼‰åº”ç”¨çš„å‰æ™¯ã€‚é’ˆå¯¹æ— äººæœºçš„æœ‰é™ç»­èˆªå’Œé€šä¿¡èŒƒå›´é—®é¢˜ï¼Œæå‡ºäº†è”åˆæ— äººæœºè½¨è¿¹è§„åˆ’å’Œèµ„æºåˆ†é…çš„ç­–ç•¥ï¼Œä»¥æœ€å¤§åŒ–æ•°æ®æ”¶é›†çš„èƒ½é‡æ•ˆç‡ã€‚ä¸ºè§£å†³ä¸ç¨³å®šè®­ç»ƒå’Œä¾èµ–ä¸“å®¶çº§æ•°æ®é›†çš„é—®é¢˜ï¼Œç»“åˆçº¿æ€§è§„åˆ’å’Œå†³ç­–å˜å‹å™¨ï¼ˆDTï¼‰æ¡†æ¶è¿›è¡Œä¼˜åŒ–ã€‚å¼•å…¥å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èµ‹èƒ½çš„æ‰¹è¯„æ­£åˆ™åŒ–å†³ç­–å˜å‹å™¨ï¼ˆLLM-CRDTï¼‰ï¼Œæœ‰æ•ˆå­¦ä¹ æ— äººæœºæ§åˆ¶ç­–ç•¥ï¼Œä»æ¬¡ä¼˜æ•°æ®é›†ä¸­æ•´åˆåºåˆ—å»ºæ¨¡å’ŒåŸºäºæ‰¹è¯„çš„ä»·å€¼å¯¼å‘ã€‚é‡‡ç”¨é¢„è®­ç»ƒLLMä½œä¸ºDTæ¨¡å‹çš„å˜å‹å™¨èƒŒæ™¯ï¼Œå¹¶é‡‡ç”¨å‚æ•°é«˜æ•ˆçš„å¾®è°ƒç­–ç•¥ï¼Œå³LoRAï¼Œèƒ½åœ¨å°è§„æ¨¡æ•°æ®é›†å’Œä½è®¡ç®—å¼€é”€ä¸‹å¿«é€Ÿé€‚åº”æ— äººæœºæ§åˆ¶ä»»åŠ¡ã€‚æ¨¡æ‹Ÿç»“æœæ˜¾ç¤ºï¼ŒLLM-CRDTç›¸è¾ƒäºåŸºå‡†åœ¨çº¿å’Œç¦»çº¿å¼ºåŒ–å­¦ä¹ æ–¹æ³•è¡¨ç°å‡ºæ›´é«˜çš„èƒ½é‡æ•ˆç‡ï¼Œæœ€é«˜æå‡è¾¾36.7%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>UAVsåœ¨IoTæ•°æ®æ”¶é›†ä¸­çš„åº”ç”¨åŠå…¶é¢ä¸´çš„æŒ‘æˆ˜ï¼šæœ‰é™çš„ç»­èˆªå’Œé€šä¿¡èŒƒå›´éœ€è¦æ™ºèƒ½è½¨è¿¹è§„åˆ’ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨UAVè½¨è¿¹ä¼˜åŒ–ä¸­çš„å±€é™æ€§ï¼Œä»¥åŠç¦»çº¿RLçš„æ˜“å—åˆ°ä¸ç¨³å®šè®­ç»ƒå’Œä¾èµ–ä¸“å®¶æ•°æ®é›†çš„é—®é¢˜ã€‚</li>
<li>æå‡ºè”åˆUAVè½¨è¿¹è§„åˆ’å’Œèµ„æºåˆ†é…é—®é¢˜ä»¥æœ€å¤§åŒ–èƒ½é‡æ•ˆç‡ï¼Œå…¶ä¸­èµ„æºåˆ†é…å­é—®é¢˜é€šè¿‡çº¿æ€§è§„åˆ’è§£å†³ã€‚</li>
<li>å¼•å…¥LLM-CRDTæ¡†æ¶ï¼Œç»“åˆå†³ç­–å˜å‹å™¨å’Œæ‰¹è¯„ç½‘ç»œï¼Œä»æ¬¡ä¼˜æ•°æ®ä¸­å­¦ä¹ æœ‰æ•ˆçš„UAVæ§åˆ¶ç­–ç•¥ã€‚</li>
<li>åˆ©ç”¨é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºå†³ç­–å˜å‹å™¨çš„æ ¸å¿ƒï¼Œå¹¶é‡‡ç”¨å‚æ•°é«˜æ•ˆçš„å¾®è°ƒç­–ç•¥ä»¥é€‚åº”å°è§„æ¨¡æ•°æ®é›†å’Œä½è®¡ç®—éœ€æ±‚ã€‚</li>
<li>æ¨¡æ‹Ÿå®éªŒæ˜¾ç¤ºLLM-CRDTç›¸è¾ƒäºå…¶ä»–æ–¹æ³•æ˜¾è‘—æé«˜èƒ½é‡æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13934">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13934v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13934v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13934v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="DSpAST-Disentangled-Representations-for-Spatial-Audio-Reasoning-with-Large-Language-Models"><a href="#DSpAST-Disentangled-Representations-for-Spatial-Audio-Reasoning-with-Large-Language-Models" class="headerlink" title="DSpAST: Disentangled Representations for Spatial Audio Reasoning with   Large Language Models"></a>DSpAST: Disentangled Representations for Spatial Audio Reasoning with   Large Language Models</h2><p><strong>Authors:Kevin Wilkinghoff, Zheng-Hua Tan</strong></p>
<p>Reasoning about spatial audio with large language models requires a spatial audio encoder as an acoustic front-end to obtain audio embeddings for further processing. Such an encoder needs to capture all information required to detect the type of sound events, as well as the direction and distance of their corresponding sources. Accomplishing this with a single audio encoder is demanding as the information required for each of these tasks is mostly independent of each other. As a result, the performance obtained with a single encoder is often worse than when using task-specific audio encoders. In this work, we present DSpAST, a novel audio encoder based on SpatialAST that learns disentangled representations of spatial audio while having only 0.2% additional parameters. Experiments on SpatialSoundQA with the spatial audio reasoning system BAT demonstrate that DSpAST significantly outperforms SpatialAST. </p>
<blockquote>
<p>å…³äºç©ºé—´éŸ³é¢‘çš„æ¨ç†éœ€è¦ç©ºé—´éŸ³é¢‘ç¼–ç å™¨ä½œä¸ºå£°å­¦å‰ç«¯æ¥è·å¾—ç”¨äºè¿›ä¸€æ­¥å¤„ç†çš„éŸ³é¢‘åµŒå…¥ã€‚è¿™æ ·çš„ç¼–ç å™¨éœ€è¦æ•è·æ‰€æœ‰æ£€æµ‹å£°éŸ³äº‹ä»¶ç±»å‹æ‰€éœ€çš„ä¿¡æ¯ï¼Œä»¥åŠç›¸åº”éŸ³æºçš„æ–¹å‘å’Œè·ç¦»ã€‚ä½¿ç”¨å•ä¸ªéŸ³é¢‘ç¼–ç å™¨å®Œæˆè¿™äº›ä»»åŠ¡æ˜¯å›°éš¾çš„ï¼Œå› ä¸ºè¿™äº›ä»»åŠ¡æ‰€éœ€çš„ä¿¡æ¯å¤§å¤šç›¸äº’ç‹¬ç«‹ã€‚å› æ­¤ï¼Œä½¿ç”¨å•ä¸ªç¼–ç å™¨çš„æ€§èƒ½å¾€å¾€æ¯”ä½¿ç”¨ç‰¹å®šä»»åŠ¡çš„éŸ³é¢‘ç¼–ç å™¨å·®ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†DSpASTï¼Œè¿™æ˜¯ä¸€ç§åŸºäºSpatialASTçš„æ–°å‹éŸ³é¢‘ç¼–ç å™¨ï¼Œå®ƒå­¦ä¹ ç©ºé—´éŸ³é¢‘çš„è§£çº ç¼ è¡¨ç¤ºï¼Œå¹¶ä¸”ä»…å¢åŠ äº†0.2%çš„å‚æ•°ã€‚åœ¨å¸¦æœ‰ç©ºé—´éŸ³é¢‘æ¨ç†ç³»ç»ŸBATçš„SpatialSoundQAä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDSpASTæ˜¾è‘—ä¼˜äºSpatialASTã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13927v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ç©ºé—´éŸ³é¢‘æ¨ç†éœ€è¦ä½¿ç”¨ç©ºé—´éŸ³é¢‘ç¼–ç å™¨ä½œä¸ºå£°å­¦å‰ç«¯è·å–éŸ³é¢‘åµŒå…¥ä»¥ä¾›è¿›ä¸€æ­¥å¤„ç†ã€‚å•ä¸€ç¼–ç å™¨éš¾ä»¥æ»¡è¶³åŒæ—¶æ£€æµ‹å£°éŸ³äº‹ä»¶ç±»å‹åŠéŸ³æºæ–¹å‘å’Œè·ç¦»çš„éœ€æ±‚ã€‚å› æ­¤ï¼Œä»»åŠ¡ç‰¹å®šçš„éŸ³é¢‘ç¼–ç å™¨å¾€å¾€æ›´ä¸ºæœ‰æ•ˆã€‚æœ¬ç ”ç©¶æå‡ºäº†åŸºäºSpatialASTçš„æ–°å‹éŸ³é¢‘ç¼–ç å™¨DSpASTï¼Œå…¶å­¦ä¹ ç©ºé—´éŸ³é¢‘çš„åˆ†ç¦»è¡¨ç¤ºå¹¶å…·æœ‰ä»…0.2%çš„é¢å¤–å‚æ•°ã€‚åœ¨SpatialSoundQAä¸Šçš„å®éªŒæ˜¾ç¤ºï¼ŒDSpASTæ˜¾è‘—ä¼˜äºSpatialASTã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç©ºé—´éŸ³é¢‘æ¨ç†éœ€è¦å£°å­¦å‰ç«¯çš„ç©ºé—´éŸ³é¢‘ç¼–ç å™¨è·å–éŸ³é¢‘åµŒå…¥ã€‚</li>
<li>å•ä¸€ç¼–ç å™¨éš¾ä»¥æ»¡è¶³æ£€æµ‹å£°éŸ³äº‹ä»¶ç±»å‹å’ŒéŸ³æºæ–¹å‘åŠè·ç¦»çš„éœ€æ±‚ã€‚</li>
<li>ä»»åŠ¡ç‰¹å®šçš„éŸ³é¢‘ç¼–ç å™¨é€šå¸¸è¡¨ç°æ›´å¥½ã€‚</li>
<li>æœ¬ç ”ç©¶æå‡ºäº†æ–°å‹éŸ³é¢‘ç¼–ç å™¨DSpASTï¼ŒåŸºäºSpatialASTå¹¶å…·æœ‰ä»…0.2%çš„é¢å¤–å‚æ•°ã€‚</li>
<li>DSpASTèƒ½å¤Ÿå­¦ä¹ ç©ºé—´éŸ³é¢‘çš„åˆ†ç¦»è¡¨ç¤ºã€‚</li>
<li>åœ¨SpatialSoundQAçš„å®éªŒä¸­ï¼ŒDSpASTæ˜¾è‘—ä¼˜äºSpatialASTã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13927">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13927v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13927v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13927v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13927v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Combining-Evidence-and-Reasoning-for-Biomedical-Fact-Checking"><a href="#Combining-Evidence-and-Reasoning-for-Biomedical-Fact-Checking" class="headerlink" title="Combining Evidence and Reasoning for Biomedical Fact-Checking"></a>Combining Evidence and Reasoning for Biomedical Fact-Checking</h2><p><strong>Authors:Mariano Barone, Antonio Romano, Giuseppe Riccio, Marco Postiglione, Vincenzo Moscato</strong></p>
<p>Misinformation in healthcare, from vaccine hesitancy to unproven treatments, poses risks to public health and trust in medical systems. While machine learning and natural language processing have advanced automated fact-checking, validating biomedical claims remains uniquely challenging due to complex terminology, the need for domain expertise, and the critical importance of grounding in scientific evidence. We introduce CER (Combining Evidence and Reasoning), a novel framework for biomedical fact-checking that integrates scientific evidence retrieval, reasoning via large language models, and supervised veracity prediction. By integrating the text-generation capabilities of large language models with advanced retrieval techniques for high-quality biomedical scientific evidence, CER effectively mitigates the risk of hallucinations, ensuring that generated outputs are grounded in verifiable, evidence-based sources. Evaluations on expert-annotated datasets (HealthFC, BioASQ-7b, SciFact) demonstrate state-of-the-art performance and promising cross-dataset generalization. Code and data are released for transparency and reproducibility: https: &#x2F;&#x2F;github.com&#x2F;PRAISELab-PicusLab&#x2F;CER. </p>
<blockquote>
<p>åŒ»ç–—å¥åº·é¢†åŸŸçš„é”™è¯¯ä¿¡æ¯ï¼Œä»ç–«è‹—çŠ¹è±«åˆ°æœªç»è¯å®çš„æ²»ç–—æ–¹æ³•ï¼Œå¯¹å…¬å…±å¥åº·å’ŒåŒ»ç–—ç³»ç»Ÿçš„ä¿¡ä»»æ„æˆäº†é£é™©ã€‚å°½ç®¡æœºå™¨å­¦ä¹ å’Œè‡ªç„¶è¯­è¨€å¤„ç†å·²ç»æ¨åŠ¨äº†è‡ªåŠ¨äº‹å®æ ¸æŸ¥çš„å‘å±•ï¼Œä½†ç”±äºå¤æ‚çš„æœ¯è¯­ã€éœ€è¦é¢†åŸŸä¸“ä¸šçŸ¥è¯†å’Œå¯¹ç§‘å­¦è¯æ®çš„åŸºç¡€è‡³å…³é‡è¦ï¼ŒéªŒè¯ç”Ÿç‰©åŒ»å­¦å£°æ˜ä»ç„¶å…·æœ‰ç‹¬ç‰¹æŒ‘æˆ˜æ€§ã€‚æˆ‘ä»¬å¼•å…¥äº†CERï¼ˆç»“åˆè¯æ®å’Œæ¨ç†ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºç”Ÿç‰©åŒ»å­¦äº‹å®æ ¸æŸ¥çš„æ–°å‹æ¡†æ¶ï¼Œå®ƒç»“åˆäº†ç§‘å­¦è¯æ®æ£€ç´¢ã€é€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†å’Œå—ç›‘ç£çš„çœŸå®æ€§é¢„æµ‹ã€‚é€šè¿‡å°†å¤§å‹è¯­è¨€æ¨¡å‹çš„æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›ä¸å…ˆè¿›çš„ç”Ÿç‰©åŒ»å­¦ç§‘å­¦è¯æ®æ£€ç´¢æŠ€æœ¯ç›¸ç»“åˆï¼ŒCERæœ‰æ•ˆåœ°å‡è½»äº†å¹»è§‰çš„é£é™©ï¼Œç¡®ä¿ç”Ÿæˆçš„è¾“å‡ºåŸºäºå¯éªŒè¯çš„è¯æ®æ¥æºã€‚åœ¨ä¸“å®¶æ³¨é‡Šæ•°æ®é›†ï¼ˆHealthFCã€BioASQ-7bã€SciFactï¼‰ä¸Šçš„è¯„ä¼°è¯æ˜äº†å…¶å¤„äºè¡Œä¸šå‰æ²¿çš„æ€§èƒ½å’Œè·¨æ•°æ®é›†çš„é€šç”¨æ€§ã€‚ä¸ºäº†é€æ˜åº¦å’Œå¯é‡å¤æ€§ï¼Œæˆ‘ä»¬å…¬å¼€äº†ä»£ç å’Œæ•°æ®ï¼š<a target="_blank" rel="noopener" href="https://github.com/PRAISELab-PicusLab/CER">https://github.com/PRAISELab-PicusLab/CER</a> ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13879v1">PDF</a> Proceedings of the 48th International ACM SIGIR Conference on   Research and Development in Information Retrieval, 2025</p>
<p><strong>Summary</strong>ï¼šåŒ»ç–—å¥åº·é¢†åŸŸä¸­çš„é”™è¯¯ä¿¡æ¯ï¼Œä»ç–«è‹—çŠ¹è±«åˆ°æœªç»è¯å®çš„æ²»ç–—æ–¹æ³•ï¼Œå¯¹å…¬å…±å¥åº·å’ŒåŒ»ç–—ç³»ç»Ÿçš„ä¿¡ä»»æ„æˆé£é™©ã€‚è™½ç„¶æœºå™¨å­¦ä¹ å’Œè‡ªç„¶è¯­è¨€å¤„ç†å·²ç»æ¨åŠ¨äº†è‡ªåŠ¨äº‹å®æ ¸æŸ¥çš„è¿›å±•ï¼Œä½†ç”±äºä¸“ä¸šæœ¯è¯­çš„å¤æ‚æ€§ã€éœ€è¦ä¸“ä¸šé¢†åŸŸçš„çŸ¥è¯†å’Œå¯¹ç§‘å­¦è¯æ®çš„åŸºç¡€æ€§éœ€æ±‚ï¼Œç”Ÿç‰©åŒ»å­¦å£°æ˜éªŒè¯ä»ç„¶é¢ä¸´ç‹¬ç‰¹æŒ‘æˆ˜ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹çš„äº‹å®æ ¸æŸ¥æ¡†æ¶CERï¼ˆç»“åˆè¯æ®å’Œæ¨ç†ï¼‰ï¼Œè¯¥æ¡†æ¶èåˆäº†ç§‘å­¦è¯æ®æ£€ç´¢ã€å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†å’Œå—æ§çœŸå®é¢„æµ‹ï¼Œä»¥éªŒè¯ç”Ÿç‰©åŒ»å­¦äº‹å®ã€‚é€šè¿‡ç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹çš„æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›ä¸é«˜è´¨é‡ç”Ÿç‰©åŒ»å­¦ç§‘å­¦è¯æ®çš„å…ˆè¿›æ£€ç´¢æŠ€æœ¯ï¼ŒCERæœ‰æ•ˆé™ä½äº†å¹»æƒ³çš„é£é™©ï¼Œç¡®ä¿ç”Ÿæˆçš„è¾“å‡ºåŸºäºå¯éªŒè¯çš„è¯æ®æ¥æºã€‚åœ¨ä¸“å®¶æ³¨é‡Šæ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºå‡ºäº†è¯¥æ¡†æ¶çš„æœ€ä½³æ€§èƒ½å’Œè·¨æ•°æ®é›†çš„å¹¿æ³›æ¦‚æ‹¬å‰æ™¯ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>åŒ»ç–—å¥åº·é¢†åŸŸçš„é”™è¯¯ä¿¡æ¯å¯¹å…¬å…±å¥åº·åŠåŒ»ç–—ç³»ç»Ÿä¿¡ä»»åº¦æ„æˆé£é™©ã€‚</li>
<li>è‡ªåŠ¨åŒ–äº‹å®æ ¸æŸ¥åœ¨ç”Ÿç‰©åŒ»å­¦é¢†åŸŸé¢ä¸´æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬å¤æ‚æœ¯è¯­ã€éœ€è¦ä¸“ä¸šçŸ¥è¯†å’Œå¯¹ç§‘å­¦è¯æ®çš„éœ€æ±‚ã€‚</li>
<li>CERæ¡†æ¶ç»“åˆäº†ç§‘å­¦è¯æ®æ£€ç´¢ã€å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†å’Œå—æ§çœŸå®é¢„æµ‹æ¥è¿›è¡Œç”Ÿç‰©åŒ»å­¦äº‹å®æ ¸æŸ¥ã€‚</li>
<li>CERæ¡†æ¶èåˆäº†æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›å’Œé«˜è´¨é‡ç”Ÿç‰©åŒ»å­¦ç§‘å­¦è¯æ®çš„æ£€ç´¢æŠ€æœ¯ã€‚</li>
<li>CERæœ‰æ•ˆé™ä½äº†ç”Ÿæˆé”™è¯¯ä¿¡æ¯ï¼ˆå³â€œå¹»æƒ³â€ï¼‰çš„é£é™©ï¼Œç¡®ä¿è¾“å‡ºåŸºäºå¯éªŒè¯çš„è¯æ®æ¥æºã€‚</li>
<li>åœ¨ä¸“å®¶æ³¨é‡Šæ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒCERæ¡†æ¶æ€§èƒ½è¾¾åˆ°æœ€æ–°æ°´å¹³ï¼Œå…·æœ‰è‰¯å¥½çš„è·¨æ•°æ®é›†æ¦‚æ‹¬èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13879">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13879v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13879v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13879v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13879v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13879v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13879v1/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13879v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13879v1/page_5_1.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="AdaThinkDrive-Adaptive-Thinking-via-Reinforcement-Learning-for-Autonomous-Driving"><a href="#AdaThinkDrive-Adaptive-Thinking-via-Reinforcement-Learning-for-Autonomous-Driving" class="headerlink" title="AdaThinkDrive: Adaptive Thinking via Reinforcement Learning for   Autonomous Driving"></a>AdaThinkDrive: Adaptive Thinking via Reinforcement Learning for   Autonomous Driving</h2><p><strong>Authors:Yuechen Luo, Fang Li, Shaoqing Xu, Zhiyi Lai, Lei Yang, Qimao Chen, Ziang Luo, Zixun Xie, Shengyin Jiang, Jiaxin Liu, Long Chen, Bing Wang, Zhi-xin Yang</strong></p>
<p>While reasoning technology like Chain of Thought (CoT) has been widely adopted in Vision Language Action (VLA) models, it demonstrates promising capabilities in end to end autonomous driving. However, recent efforts to integrate CoT reasoning often fall short in simple scenarios, introducing unnecessary computational overhead without improving decision quality. To address this, we propose AdaThinkDrive, a novel VLA framework with a dual mode reasoning mechanism inspired by fast and slow thinking. First, our framework is pretrained on large scale autonomous driving (AD) scenarios using both question answering (QA) and trajectory datasets to acquire world knowledge and driving commonsense. During supervised fine tuning (SFT), we introduce a two mode dataset, fast answering (w&#x2F;o CoT) and slow thinking (with CoT), enabling the model to distinguish between scenarios that require reasoning. Furthermore, an Adaptive Think Reward strategy is proposed in conjunction with the Group Relative Policy Optimization (GRPO), which rewards the model for selectively applying CoT by comparing trajectory quality across different reasoning modes. Extensive experiments on the Navsim benchmark show that AdaThinkDrive achieves a PDMS of 90.3, surpassing the best vision only baseline by 1.7 points. Moreover, ablations show that AdaThinkDrive surpasses both the never Think and always Think baselines, improving PDMS by 2.0 and 1.4, respectively. It also reduces inference time by 14% compared to the always Think baseline, demonstrating its ability to balance accuracy and efficiency through adaptive reasoning. </p>
<blockquote>
<p>è™½ç„¶åƒæ€ç»´é“¾ï¼ˆCoTï¼‰è¿™æ ·çš„æ¨ç†æŠ€æœ¯å·²åœ¨è§†è§‰è¯­è¨€è¡ŒåŠ¨ï¼ˆVLAï¼‰æ¨¡å‹ä¸­å¾—åˆ°å¹¿æ³›åº”ç”¨ï¼Œå¹¶åœ¨ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶ä¸­å±•ç°å‡ºæœ‰å‰æ™¯çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œæœ€è¿‘å°†CoTæ¨ç†æŠ€æœ¯é›†æˆåœ¨ä¸€èµ·çš„åŠªåŠ›åœ¨ç®€å•åœºæ™¯ä¸­å¾€å¾€è¾¾ä¸åˆ°é¢„æœŸæ•ˆæœï¼Œå¼•å…¥äº†ä¸å¿…è¦çš„è®¡ç®—å¼€é”€ï¼Œå¹¶æœªèƒ½æé«˜å†³ç­–è´¨é‡ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†AdaThinkDriveï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹VLAæ¡†æ¶ï¼Œå…·æœ‰åŒæ¨¡å¼æ¨ç†æœºåˆ¶ï¼Œçµæ„Ÿæ¥è‡ªå¿«é€Ÿå’Œæ…¢é€Ÿæ€è€ƒã€‚é¦–å…ˆï¼Œæˆ‘ä»¬çš„æ¡†æ¶ä½¿ç”¨é—®ç­”ï¼ˆQAï¼‰å’Œè½¨è¿¹æ•°æ®é›†åœ¨å¤§è§„æ¨¡è‡ªåŠ¨é©¾é©¶ï¼ˆADï¼‰åœºæ™¯ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œä»¥è·å–ä¸–ç•ŒçŸ¥è¯†å’Œé©¾é©¶å¸¸è¯†ã€‚åœ¨ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåŒæ¨¡å¼æ•°æ®é›†ï¼ŒåŒ…æ‹¬å¿«é€Ÿå›ç­”ï¼ˆæ— CoTï¼‰å’Œæ…¢é€Ÿæ€è€ƒï¼ˆæœ‰CoTï¼‰ï¼Œä½¿æ¨¡å‹èƒ½å¤ŸåŒºåˆ†éœ€è¦æ¨ç†çš„åœºæ™¯ã€‚æ­¤å¤–ï¼Œç»“åˆç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œæå‡ºäº†ä¸€ç§è‡ªé€‚åº”æ€è€ƒå¥–åŠ±ç­–ç•¥ï¼Œè¯¥ç­–ç•¥é€šè¿‡æ¯”è¾ƒä¸åŒæ¨ç†æ¨¡å¼ä¸‹çš„è½¨è¿¹è´¨é‡ï¼Œå¥–åŠ±æ¨¡å‹æœ‰é€‰æ‹©åœ°åº”ç”¨CoTã€‚åœ¨NavsimåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒAdaThinkDriveçš„PDMSè¾¾åˆ°90.3ï¼Œè¶…è¶Šäº†ä»…ä½¿ç”¨è§†è§‰çš„æœ€ä½³åŸºå‡†1.7åˆ†ã€‚æ­¤å¤–ï¼Œæ¶ˆèå®éªŒè¡¨æ˜ï¼ŒAdaThinkDriveè¶…è¶Šäº†ä»ä¸æ€è€ƒï¼ˆnever Thinkï¼‰å’Œæ€»æ˜¯æ€è€ƒï¼ˆalways Thinkï¼‰çš„åŸºçº¿æ¨¡å‹ï¼ŒPDMSåˆ†åˆ«æé«˜äº†2.0å’Œ1.4ã€‚ä¸å§‹ç»ˆæ€è€ƒçš„åŸºçº¿æ¨¡å‹ç›¸æ¯”ï¼Œå®ƒè¿˜å°†æ¨ç†æ—¶é—´å‡å°‘äº†14%ï¼Œè¯æ˜äº†å…¶é€šè¿‡è‡ªé€‚åº”æ¨ç†å¹³è¡¡å‡†ç¡®æ€§å’Œæ•ˆç‡çš„èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13769v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡ä»‹ç»äº†åœ¨ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶ä¸­è¿ç”¨Chain of Thoughtï¼ˆCoTï¼‰æ¨ç†æŠ€æœ¯çš„æ½œåŠ›ï¼Œå¹¶æŒ‡å‡ºè¿‘æœŸå°è¯•åœ¨ç®€å•åœºæ™¯ä¸­æ•´åˆCoTæ¨ç†å¾€å¾€å¼•å…¥ä¸å¿…è¦çš„è®¡ç®—è´Ÿæ‹…ï¼Œå¹¶æœªæé«˜å†³ç­–è´¨é‡ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†AdaThinkDriveï¼Œä¸€ä¸ªå…·æœ‰å¿«æ…¢æ€ç»´å¯å‘çš„æ–°å‹VLAæ¡†æ¶ã€‚è¯¥æ¡†æ¶é¦–å…ˆåœ¨å¤§è§„æ¨¡è‡ªåŠ¨é©¾é©¶ï¼ˆADï¼‰åœºæ™¯ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œè·å–ä¸–ç•ŒçŸ¥è¯†å’Œé©¾é©¶å¸¸è¯†ã€‚åœ¨ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰é˜¶æ®µï¼Œå¼•å…¥å¿«æ…¢æ€ç»´æ•°æ®é›†ï¼Œä½¿æ¨¡å‹èƒ½åŒºåˆ†éœ€è¦æ¨ç†çš„åœºæ™¯ã€‚åŒæ—¶ï¼Œæå‡ºè‡ªé€‚åº”æ€è€ƒå¥–åŠ±ç­–ç•¥ï¼Œç»“åˆç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œé€šè¿‡æ¯”è¾ƒä¸åŒæ¨ç†æ¨¡å¼çš„è½¨è¿¹è´¨é‡æ¥å¥–åŠ±æ¨¡å‹é€‰æ‹©æ€§åº”ç”¨CoTã€‚åœ¨NavsimåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒAdaThinkDriveçš„PDMSè¾¾åˆ°90.3ï¼Œè¶…è¶Šæœ€ä½³è§†è§‰åŸºå‡†1.7ä¸ªç‚¹ã€‚æ­¤å¤–ï¼Œä¸å§‹ç»ˆæ€è€ƒå’Œä»ä¸æ€è€ƒåŸºçº¿ç›¸æ¯”ï¼ŒAdaThinkDriveåˆ†åˆ«æé«˜PDMS 2.0å’Œ1.4ã€‚ç›¸è¾ƒäºå§‹ç»ˆæ€è€ƒåŸºçº¿ï¼Œæ¨ç†æ—¶é—´å‡å°‘14%ï¼Œå±•ç°äº†å…¶åœ¨å‡†ç¡®æ€§ä¸æ•ˆç‡é—´çš„å¹³è¡¡èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AdaThinkDriveæ˜¯ä¸€ä¸ªç»“åˆäº†å¿«æ…¢æ€ç»´å¯å‘çš„VLAæ¡†æ¶ï¼Œç”¨äºç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶ã€‚</li>
<li>æ¡†æ¶é€šè¿‡åœ¨å¤§è§„æ¨¡è‡ªåŠ¨é©¾é©¶åœºæ™¯ä¸Šé¢„è®­ç»ƒï¼Œè·å–ä¸–ç•ŒçŸ¥è¯†å’Œé©¾é©¶å¸¸è¯†ã€‚</li>
<li>å¼•å…¥å¿«æ…¢æ€ç»´æ•°æ®é›†ï¼Œä½¿æ¨¡å‹èƒ½åŒºåˆ†éœ€è¦æ¨ç†çš„åœºæ™¯ã€‚</li>
<li>æå‡ºè‡ªé€‚åº”æ€è€ƒå¥–åŠ±ç­–ç•¥ï¼Œç»“åˆç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼Œé¼“åŠ±æ¨¡å‹é€‰æ‹©æ€§åº”ç”¨CoTã€‚</li>
<li>AdaThinkDriveåœ¨NavsimåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜å¼‚ï¼ŒPDMSè¾¾åˆ°90.3ï¼Œè¶…è¶Šè§†è§‰åŸºå‡†ã€‚</li>
<li>ä¸å…¶ä»–åŸºçº¿ç›¸æ¯”ï¼ŒAdaThinkDriveåœ¨PDMSä¸Šæœ‰æ˜¾è‘—æé«˜ï¼Œå¹¶å‡å°‘æ¨ç†æ—¶é—´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13769">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13769v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13769v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13769v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13769v1/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13769v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13769v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13769v1/page_5_1.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="THOR-Tool-Integrated-Hierarchical-Optimization-via-RL-for-Mathematical-Reasoning"><a href="#THOR-Tool-Integrated-Hierarchical-Optimization-via-RL-for-Mathematical-Reasoning" class="headerlink" title="THOR: Tool-Integrated Hierarchical Optimization via RL for Mathematical   Reasoning"></a>THOR: Tool-Integrated Hierarchical Optimization via RL for Mathematical   Reasoning</h2><p><strong>Authors:Qikai Chang, Zhenrong Zhang, Pengfei Hu, Jiefeng Ma, Yicheng Pan, Jianshu Zhang, Jun Du, Quan Liu, Jianqing Gao</strong></p>
<p>Large Language Models (LLMs) have made remarkable progress in mathematical reasoning, but still continue to struggle with high-precision tasks like numerical computation and formal symbolic manipulation. Integrating external tools has emerged as a promising approach to bridge this gap. Despite recent advances, existing methods struggle with three key challenges: constructing tool-integrated reasoning data, performing fine-grained optimization, and enhancing inference. To overcome these limitations, we propose THOR (Tool-Integrated Hierarchical Optimization via RL). First, we introduce TIRGen, a multi-agent actor-critic-based pipeline for constructing high-quality datasets of tool-integrated reasoning paths, aligning with the policy and generalizing well across diverse models. Second, to perform fine-grained hierarchical optimization, we introduce an RL strategy that jointly optimizes for both trajectory-level problem solving and step-level code generation. This is motivated by our key insight that the success of an intermediate tool call is a strong predictor of the final answerâ€™s correctness. Finally, THOR incorporates a self-correction mechanism that leverages immediate tool feedback to dynamically revise erroneous reasoning paths during inference. Our approach demonstrates strong generalization across diverse models, performing effectively in both reasoning and non-reasoning models. It further achieves state-of-the-art performance for models of a similar scale on multiple mathematical benchmarks, while also delivering consistent improvements on code benchmarks. Our code will be publicly available at <a target="_blank" rel="noopener" href="https://github.com/JingMog/THOR">https://github.com/JingMog/THOR</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ•°å­¦æ¨ç†æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ä»åœ¨æ•°å€¼è®¡ç®—å’Œå½¢å¼ç¬¦å·æ“ä½œç­‰é«˜ç²¾åº¦ä»»åŠ¡ä¸Šé‡åˆ°å›°éš¾ã€‚é›†æˆå¤–éƒ¨å·¥å…·å·²æˆä¸ºå¼¥è¡¥è¿™ä¸€å·®è·çš„æœ‰å‰é€”çš„æ–¹æ³•ã€‚å°½ç®¡æœ€è¿‘æœ‰è¿›å±•ï¼Œä½†ç°æœ‰æ–¹æ³•ä»é¢ä¸´ä¸‰å¤§æŒ‘æˆ˜ï¼šæ„å»ºå·¥å…·é›†æˆæ¨ç†æ•°æ®ã€è¿›è¡Œç²¾ç»†ä¼˜åŒ–å’Œæé«˜æ¨ç†ã€‚ä¸ºäº†å…‹æœè¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†THORï¼ˆé€šè¿‡å¼ºåŒ–å­¦ä¹ è¿›è¡Œå·¥å…·é›†æˆåˆ†å±‚ä¼˜åŒ–ï¼‰ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä»‹ç»äº†TIRGenï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºå¤šæ™ºèƒ½ä½“æ¼”å‘˜è¯„è®ºå®¶ï¼ˆactor-criticï¼‰çš„ç®¡é“ï¼Œç”¨äºæ„å»ºé«˜è´¨é‡çš„å·¥å…·é›†æˆæ¨ç†è·¯å¾„æ•°æ®é›†ï¼Œä¸ç­–ç•¥ç›¸ç¬¦å¹¶åœ¨ä¸åŒçš„æ¨¡å‹ä¹‹é—´å…·æœ‰è‰¯å¥½çš„é€šç”¨æ€§ã€‚å…¶æ¬¡ï¼Œä¸ºäº†è¿›è¡Œç²¾ç»†çš„åˆ†å±‚ä¼˜åŒ–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å¼ºåŒ–å­¦ä¹ ç­–ç•¥ï¼Œè¯¥ç­–ç•¥åŒæ—¶å¯¹è½¨è¿¹çº§çš„é—®é¢˜è§£å†³å’Œæ­¥éª¤çº§çš„ä»£ç ç”Ÿæˆè¿›è¡Œä¼˜åŒ–ã€‚æˆ‘ä»¬çš„å…³é”®è§è§£æ˜¯ï¼Œä¸­é—´å·¥å…·è°ƒç”¨çš„æˆåŠŸæ˜¯é¢„æµ‹æœ€ç»ˆç­”æ¡ˆæ­£ç¡®æ€§çš„å¼ºçƒˆæŒ‡æ ‡ã€‚æœ€åï¼ŒTHORèå…¥äº†ä¸€ç§è‡ªæˆ‘ä¿®æ­£æœºåˆ¶ï¼Œåˆ©ç”¨å³æ—¶å·¥å…·åé¦ˆåœ¨æ¨ç†è¿‡ç¨‹ä¸­åŠ¨æ€ä¿®æ­£é”™è¯¯çš„æ¨ç†è·¯å¾„ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸åŒçš„æ¨¡å‹ä¹‹é—´è¡¨ç°å‡ºå¼ºå¤§çš„é€šç”¨æ€§ï¼Œåœ¨æ¨ç†å’Œéæ¨ç†æ¨¡å‹ä¸­éƒ½èƒ½æœ‰æ•ˆè¿è¡Œã€‚æ­¤å¤–ï¼Œåœ¨å¤šä¸ªæ•°å­¦åŸºå‡†æµ‹è¯•ä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨åŒç±»è§„æ¨¡æ¨¡å‹ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶åœ¨ä»£ç åŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†æŒç»­çš„æ”¹è¿›ã€‚æˆ‘ä»¬çš„ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/JingMog/THOR">https://github.com/JingMog/THOR</a>ä¸Šå…¬å¼€å¯ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13761v1">PDF</a> 22 pages, 13 figures</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ•°å­¦æ¨ç†æ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨æ•°å€¼è®¡ç®—å’Œå½¢å¼ç¬¦å·æ“ä½œç­‰é«˜ç²¾åº¦ä»»åŠ¡ä¸Šä»é¢ä¸´æŒ‘æˆ˜ã€‚æ•´åˆå¤–éƒ¨å·¥å…·æˆä¸ºå¼¥è¡¥è¿™ä¸€å·®è·çš„æœ‰å‰é€”çš„æ–¹æ³•ã€‚ä¸ºå…‹æœç°æœ‰æ–¹æ³•çš„æŒ‘æˆ˜ï¼Œæå‡ºTHORï¼ˆé€šè¿‡å¼ºåŒ–å­¦ä¹ å®ç°å·¥å…·é›†æˆåˆ†å±‚ä¼˜åŒ–ï¼‰ã€‚THORå¼•å…¥TIRGenï¼ŒåŸºäºå¤šæ™ºèƒ½ä½“actor-criticçš„ç®¡é“ï¼Œæ„å»ºé«˜è´¨é‡çš„å·¥å…·é›†æˆæ¨ç†è·¯å¾„æ•°æ®é›†ï¼Œå¾ˆå¥½åœ°ä¸ç­–ç•¥å¯¹é½å¹¶åœ¨å¤šç§æ¨¡å‹ä¸Šå®ç°æ³›åŒ–ã€‚æ­¤å¤–ï¼Œé‡‡ç”¨å¼ºåŒ–å­¦ä¹ ç­–ç•¥è¿›è¡Œç²¾ç»†åˆ†å±‚ä¼˜åŒ–ï¼Œè”åˆä¼˜åŒ–è½¨è¿¹çº§é—®é¢˜è§£å†³å’Œæ­¥éª¤çº§ä»£ç ç”Ÿæˆã€‚æœ€åï¼ŒTHORåˆ©ç”¨å³æ—¶å·¥å…·åé¦ˆåœ¨æ¨ç†è¿‡ç¨‹ä¸­åŠ¨æ€ä¿®æ­£é”™è¯¯è·¯å¾„ã€‚THORåœ¨å¤šä¸ªæ•°å­¦åŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†å“è¶Šæ€§èƒ½ï¼ŒåŒæ—¶åœ¨ç±»ä¼¼è§„æ¨¡çš„æ¨¡å‹ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚ä»£ç å…¬å¼€äºï¼š<a target="_blank" rel="noopener" href="https://github.com/JingMog/THOR%E3%80%82">https://github.com/JingMog/THORã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­è™½æœ‰æ‰€è¿›å±•ï¼Œä½†åœ¨é«˜ç²¾åº¦ä»»åŠ¡ä¸Šä»é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>æ•´åˆå¤–éƒ¨å·¥å…·æ˜¯ä¸€ç§æœ‰æ•ˆçš„ç­–ç•¥ï¼Œä»¥å¼¥è¡¥å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å¤æ‚æ•°å­¦ä»»åŠ¡æ—¶çš„ä¸è¶³ã€‚</li>
<li>THORé€šè¿‡å¼•å…¥TIRGenï¼Œæ„å»ºé«˜è´¨é‡çš„å·¥å…·é›†æˆæ¨ç†è·¯å¾„æ•°æ®é›†ï¼Œå®ç°äº†ç­–ç•¥ä¸æ¨¡å‹çš„è‰¯å¥½å¯¹é½å’Œæ³›åŒ–ã€‚</li>
<li>THORé‡‡ç”¨å¼ºåŒ–å­¦ä¹ ç­–ç•¥è¿›è¡Œç²¾ç»†çš„åˆ†å±‚ä¼˜åŒ–ï¼ŒåŒæ—¶ä¼˜åŒ–è½¨è¿¹çº§é—®é¢˜è§£å†³å’Œæ­¥éª¤çº§ä»£ç ç”Ÿæˆã€‚</li>
<li>THORåˆ©ç”¨å³æ—¶å·¥å…·åé¦ˆï¼Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­åŠ¨æ€ä¿®æ­£é”™è¯¯è·¯å¾„ï¼Œå¢å¼ºäº†æ¨¡å‹çš„è‡ªæˆ‘çº æ­£èƒ½åŠ›ã€‚</li>
<li>THORåœ¨å¤šä¸ªæ•°å­¦åŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†å“è¶Šæ€§èƒ½ï¼Œè¡¨æ˜å…¶åœ¨å¤æ‚æ•°å­¦ä»»åŠ¡ä¸Šçš„æœ‰æ•ˆæ€§ã€‚</li>
<li>THORçš„ä»£ç å°†å…¬å¼€å‘å¸ƒï¼Œä»¥ä¾¿å…¶ä»–ç ”ç©¶è€…ä½¿ç”¨å’Œæ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13761">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13761v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13761v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13761v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13761v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="A-Study-on-Thinking-Patterns-of-Large-Reasoning-Models-in-Code-Generation"><a href="#A-Study-on-Thinking-Patterns-of-Large-Reasoning-Models-in-Code-Generation" class="headerlink" title="A Study on Thinking Patterns of Large Reasoning Models in Code   Generation"></a>A Study on Thinking Patterns of Large Reasoning Models in Code   Generation</h2><p><strong>Authors:Kevin Halim, Sin G. Teo, Ruitao Feng, Zhenpeng Chen, Yang Gu, Chong Wang, Yang Liu</strong></p>
<p>Currently, many large language models (LLMs) are utilized for software engineering tasks such as code generation. The emergence of more advanced models known as large reasoning models (LRMs), such as OpenAIâ€™s o3, DeepSeek R1, and Qwen3. They have demonstrated the capability of performing multi-step reasoning. Despite the advancement in LRMs, little attention has been paid to systematically analyzing the reasoning patterns these models exhibit and how such patterns influence the generated code. This paper presents a comprehensive study aimed at investigating and uncovering the reasoning behavior of LRMs during code generation. We prompted several state-of-the-art LRMs of varying sizes with code generation tasks and applied open coding to manually annotate the reasoning traces. From this analysis, we derive a taxonomy of LRM reasoning behaviors, encompassing 15 reasoning actions across four phases.   Our empirical study based on the taxonomy reveals a series of findings. First, we identify common reasoning patterns, showing that LRMs generally follow a human-like coding workflow, with more complex tasks eliciting additional actions such as scaffolding, flaw detection, and style checks. Second, we compare reasoning across models, finding that Qwen3 exhibits iterative reasoning while DeepSeek-R1-7B follows a more linear, waterfall-like approach. Third, we analyze the relationship between reasoning and code correctness, showing that actions such as unit test creation and scaffold generation strongly support functional outcomes, with LRMs adapting strategies based on task context. Finally, we evaluate lightweight prompting strategies informed by these findings, demonstrating the potential of context- and reasoning-oriented prompts to improve LRM-generated code. Our results offer insights and practical implications for advancing automatic code generation. </p>
<blockquote>
<p>å½“å‰ï¼Œè®¸å¤šå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¢«ç”¨äºè½¯ä»¶å·¥ç¨‹ä»»åŠ¡ï¼Œå¦‚ä»£ç ç”Ÿæˆã€‚éšç€è¢«ç§°ä¸ºå¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMï¼‰çš„æ›´å…ˆè¿›æ¨¡å‹çš„å‡ºç°ï¼Œå¦‚OpenAIçš„o3ã€DeepSeek R1å’ŒQwen3ï¼Œå®ƒä»¬å·²ç»è¡¨ç°å‡ºäº†è¿›è¡Œå¤šæ­¥éª¤æ¨ç†çš„èƒ½åŠ›ã€‚å°½ç®¡LRMæœ‰æ‰€å‘å±•ï¼Œä½†å¾ˆå°‘æœ‰äººå…³æ³¨ç³»ç»Ÿåœ°åˆ†æè¿™äº›æ¨¡å‹æ‰€è¡¨ç°å‡ºçš„æ¨ç†æ¨¡å¼ä»¥åŠè¿™äº›æ¨¡å¼å¦‚ä½•å½±å“ç”Ÿæˆçš„ä»£ç ã€‚æœ¬æ–‡æ—¨åœ¨å…¨é¢ç ”ç©¶è°ƒæŸ¥LRMåœ¨ä»£ç ç”Ÿæˆè¿‡ç¨‹ä¸­çš„æ¨ç†è¡Œä¸ºã€‚æˆ‘ä»¬å‘å‡ ç§æœ€æ–°çš„å¤§å‹æ¨ç†æ¨¡å‹åˆ†é…äº†ä»£ç ç”Ÿæˆä»»åŠ¡ï¼Œå¹¶é€šè¿‡å¼€æ”¾ç¼–ç å¯¹æ¨ç†ç—•è¿¹è¿›è¡Œäº†æ‰‹åŠ¨æ³¨é‡Šã€‚æ ¹æ®åˆ†æï¼Œæˆ‘ä»¬å¾—å‡ºäº†LRMæ¨ç†è¡Œä¸ºçš„åˆ†ç±»ï¼ŒåŒ…æ‹¬å››ä¸ªé˜¶æ®µä¸­çš„15ç§æ¨ç†è¡ŒåŠ¨ã€‚åŸºäºåˆ†ç±»çš„å®è¯ç ”ç©¶æ­ç¤ºäº†ä¸€ç³»åˆ—å‘ç°ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ç¡®å®šäº†å¸¸è§çš„æ¨ç†æ¨¡å¼ï¼Œè¡¨æ˜LRMé€šå¸¸éµå¾ªç±»ä¼¼äºäººç±»çš„ç¼–ç å·¥ä½œæµç¨‹ï¼Œæ›´å¤æ‚çš„ä»»åŠ¡ä¼šå¼•å‘é¢å¤–çš„è¡ŒåŠ¨ï¼Œå¦‚è„šæ‰‹æ¶ã€ç¼ºé™·æ£€æµ‹å’Œé£æ ¼æ£€æŸ¥ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å¯¹ä¸åŒæ¨¡å‹çš„æ¨ç†è¿›è¡Œäº†æ¯”è¾ƒï¼Œå‘ç°Qwen3è¡¨ç°å‡ºè¿­ä»£æ¨ç†ï¼Œè€ŒDeepSeek-R1-7Båˆ™é‡‡ç”¨æ›´çº¿æ€§ã€ç€‘å¸ƒå¼çš„æ–¹æ³•ã€‚ç¬¬ä¸‰ï¼Œæˆ‘ä»¬åˆ†æäº†æ¨ç†ä¸ä»£ç æ­£ç¡®æ€§ä¹‹é—´çš„å…³ç³»ï¼Œæ˜¾ç¤ºåˆ›å»ºå•å…ƒæµ‹è¯•ã€ç”Ÿæˆè„šæ‰‹æ¶ç­‰è¡ŒåŠ¨æœ‰åŠ›åœ°æ”¯æŒäº†åŠŸèƒ½ç»“æœï¼ŒLRMæ ¹æ®ä»»åŠ¡ä¸Šä¸‹æ–‡è°ƒæ•´ç­–ç•¥ã€‚æœ€åï¼Œæˆ‘ä»¬æ ¹æ®è¿™äº›å‘ç°è¯„ä¼°äº†è½»é‡çº§æç¤ºç­–ç•¥ï¼Œè¯æ˜äº†ä¸Šä¸‹æ–‡å’Œæ¨ç†å¯¼å‘æç¤ºæ”¹å–„LRMç”Ÿæˆä»£ç çš„æ½œåŠ›ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœä¸ºè‡ªåŠ¨ä»£ç ç”Ÿæˆçš„å‘å±•æä¾›äº†è§è§£å’Œå®é™…åº”ç”¨çš„å¯ç¤ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13758v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰å¦‚OpenAIçš„o3ã€DeepSeek R1å’ŒQwen3ç­‰ï¼Œåœ¨ä»£ç ç”Ÿæˆç­‰è½¯ä»¶å·¥ç¨‹ä»»åŠ¡ä¸­å±•ç°å‡ºå¼ºå¤§çš„å¤šæ­¥æ¨ç†èƒ½åŠ›ã€‚æœ¬æ–‡å¯¹LRMsåœ¨ä»£ç ç”Ÿæˆè¿‡ç¨‹ä¸­çš„æ¨ç†è¡Œä¸ºè¿›è¡Œäº†å…¨é¢çš„ç ”ç©¶ï¼Œé€šè¿‡æ‰‹åŠ¨æ³¨è§£æ¨ç†ç—•è¿¹ï¼Œæ„å»ºäº†æ¶µç›–15ç§æ¨ç†è¡Œä¸ºçš„åˆ†ç±»ä½“ç³»ã€‚ç ”ç©¶å‘ç°LRMséµå¾ªç±»ä¼¼äººç±»ç¼–ç çš„å·¥ä½œæµç¨‹ï¼Œä¸åŒæ¨¡å‹å±•ç°å‡ºä¸åŒçš„æ¨ç†æ¨¡å¼ï¼Œä¸”æ¨ç†è¡Œä¸ºä¸ä»£ç æ­£ç¡®æ€§ç´§å¯†ç›¸å…³ã€‚æœ¬æ–‡ç»“æœä¸ºæ”¹è¿›LRMç”Ÿæˆçš„ä»£ç æä¾›äº†è¯­å¢ƒå’Œæ¨ç†å¯¼å‘çš„æç¤ºç­–ç•¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LRMså±•ç°å‡ºå¼ºå¤§çš„å¤šæ­¥æ¨ç†èƒ½åŠ›ï¼Œå¯ç”¨äºä»£ç ç”Ÿæˆç­‰è½¯ä»¶å·¥ç¨‹ä»»åŠ¡ã€‚</li>
<li>é€šè¿‡æ‰‹åŠ¨æ³¨è§£æ¨ç†ç—•è¿¹ï¼Œæ„å»ºäº†LRMæ¨ç†è¡Œä¸ºçš„åˆ†ç±»ä½“ç³»ï¼Œæ¶µç›–15ç§æ¨ç†è¡Œä¸ºã€‚</li>
<li>LRMsä¸€èˆ¬éµå¾ªç±»ä¼¼äººç±»ç¼–ç çš„å·¥ä½œæµç¨‹ï¼Œå¤æ‚ä»»åŠ¡ä¼šè§¦å‘æ›´å¤šæ¨ç†è¡Œä¸ºã€‚</li>
<li>ä¸åŒLRMæ¨¡å‹å±•ç°å‡ºä¸åŒçš„æ¨ç†æ¨¡å¼ï¼Œå¦‚Qwen3é‡‡ç”¨è¿­ä»£æ¨ç†ï¼ŒDeepSeek-R1-7Bé‡‡ç”¨æ›´çº¿æ€§çš„ç€‘å¸ƒå¼æ–¹æ³•ã€‚</li>
<li>æ¨ç†è¡Œä¸ºä¸ä»£ç æ­£ç¡®æ€§ç´§å¯†ç›¸å…³ï¼ŒæŸäº›æ¨ç†è¡Œä¸ºå¦‚å•å…ƒæµ‹è¯•åˆ›å»ºå’Œè„šæ‰‹æ¶ç”Ÿæˆå¯¹åŠŸèƒ½ç»“æœæœ‰å¼ºæ”¯æŒã€‚</li>
<li>è¯­å¢ƒå’Œæ¨ç†å¯¼å‘çš„æç¤ºç­–ç•¥å¯æ”¹å–„LRMç”Ÿæˆçš„ä»£ç ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13758">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13758v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13758v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13758v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13758v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="FSR-VLN-Fast-and-Slow-Reasoning-for-Vision-Language-Navigation-with-Hierarchical-Multi-modal-Scene-Graph"><a href="#FSR-VLN-Fast-and-Slow-Reasoning-for-Vision-Language-Navigation-with-Hierarchical-Multi-modal-Scene-Graph" class="headerlink" title="FSR-VLN: Fast and Slow Reasoning for Vision-Language Navigation with   Hierarchical Multi-modal Scene Graph"></a>FSR-VLN: Fast and Slow Reasoning for Vision-Language Navigation with   Hierarchical Multi-modal Scene Graph</h2><p><strong>Authors:Xiaolin Zhou, Tingyang Xiao, Liu Liu, Yucheng Wang, Maiyue Chen, Xinrui Meng, Xinjie Wang, Wei Feng, Wei Sui, Zhizhong Su</strong></p>
<p>Visual-Language Navigation (VLN) is a fundamental challenge in robotic systems, with broad applications for the deployment of embodied agents in real-world environments. Despite recent advances, existing approaches are limited in long-range spatial reasoning, often exhibiting low success rates and high inference latency, particularly in long-range navigation tasks. To address these limitations, we propose FSR-VLN, a vision-language navigation system that combines a Hierarchical Multi-modal Scene Graph (HMSG) with Fast-to-Slow Navigation Reasoning (FSR). The HMSG provides a multi-modal map representation supporting progressive retrieval, from coarse room-level localization to fine-grained goal view and object identification. Building on HMSG, FSR first performs fast matching to efficiently select candidate rooms, views, and objects, then applies VLM-driven refinement for final goal selection. We evaluated FSR-VLN across four comprehensive indoor datasets collected by humanoid robots, utilizing 87 instructions that encompass a diverse range of object categories. FSR-VLN achieves state-of-the-art (SOTA) performance in all datasets, measured by the retrieval success rate (RSR), while reducing the response time by 82% compared to VLM-based methods on tour videos by activating slow reasoning only when fast intuition fails. Furthermore, we integrate FSR-VLN with speech interaction, planning, and control modules on a Unitree-G1 humanoid robot, enabling natural language interaction and real-time navigation. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€å¯¼èˆªï¼ˆVLNï¼‰æ˜¯æœºå™¨äººç³»ç»Ÿä¸­çš„ä¸€ä¸ªåŸºæœ¬æŒ‘æˆ˜ï¼Œå®ƒåœ¨ç°å®ä¸–ç•Œçš„ç¯å¢ƒä¸­éƒ¨ç½²å®ä½“ä»£ç†æ–¹é¢å…·æœ‰å¹¿æ³›çš„åº”ç”¨ã€‚å°½ç®¡æœ€è¿‘æœ‰è¿›å±•ï¼Œä½†ç°æœ‰æ–¹æ³•åœ¨è¿œç¨‹ç©ºé—´æ¨ç†æ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œé€šå¸¸è¡¨ç°å‡ºè¾ƒä½çš„æˆåŠŸç‡å’Œè¾ƒé«˜çš„æ¨ç†å»¶è¿Ÿï¼Œç‰¹åˆ«æ˜¯åœ¨è¿œç¨‹å¯¼èˆªä»»åŠ¡ä¸­ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†FSR-VLNï¼Œè¿™æ˜¯ä¸€ç§ç»“åˆåˆ†å±‚å¤šæ¨¡å¼åœºæ™¯å›¾ï¼ˆHMSGï¼‰å’Œå¿«æ…¢å¯¼èˆªæ¨ç†ï¼ˆFSRï¼‰çš„è§†è§‰è¯­è¨€å¯¼èˆªç³»ç»Ÿã€‚HMSGæä¾›äº†ä¸€ç§å¤šæ¨¡å¼åœ°å›¾è¡¨ç¤ºï¼Œæ”¯æŒä»ç²—ç•¥çš„æˆ¿é—´çº§åˆ«å®šä½åˆ°ç²¾ç»†çš„ç›®æ ‡è§†å›¾å’Œå¯¹è±¡è¯†åˆ«çš„æ¸è¿›æ£€ç´¢ã€‚å»ºç«‹åœ¨HMSGä¹‹ä¸Šï¼ŒFSRé¦–å…ˆè¿›è¡Œå¿«é€ŸåŒ¹é…ï¼Œä»¥æœ‰æ•ˆåœ°é€‰æ‹©å€™é€‰æˆ¿é—´ã€è§†å›¾å’Œå¯¹è±¡ï¼Œç„¶ååº”ç”¨VLMé©±åŠ¨çš„ç²¾ç‚¼æ¥è¿›è¡Œæœ€ç»ˆç›®æ ‡é€‰æ‹©ã€‚æˆ‘ä»¬åœ¨ç”±äººå½¢æœºå™¨äººæ”¶é›†çš„å››ä¸ªç»¼åˆå®¤å†…æ•°æ®é›†ä¸Šè¯„ä¼°äº†FSR-VLNï¼Œä½¿ç”¨äº†87æ¡æŒ‡ä»¤ï¼Œæ¶µç›–äº†å„ç§å¯¹è±¡ç±»åˆ«ã€‚FSR-VLNåœ¨æ‰€æœ‰æ•°æ®é›†ä¸Šçš„æ£€ç´¢æˆåŠŸç‡ï¼ˆRSRï¼‰æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼Œå¹¶ä¸”åœ¨æ¿€æ´»æ…¢é€Ÿæ¨ç†ä»…åœ¨å¿«é€Ÿç›´è§‰å¤±è´¥æ—¶ï¼Œå°†å“åº”æ—¶é—´å‡å°‘äº†82%ï¼Œä¸åŸºäºVLMçš„æ–¹æ³•ç›¸æ¯”ï¼Œç”¨äºæ¸¸è§ˆè§†é¢‘çš„å“åº”é€Ÿåº¦æ›´å¿«ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†FSR-VLNä¸Unitree-G1äººå½¢æœºå™¨äººä¸Šçš„è¯­éŸ³äº¤äº’ã€è§„åˆ’å’Œæ§åˆ¶æ¨¡å—é›†æˆåœ¨ä¸€èµ·ï¼Œå®ç°è‡ªç„¶è¯­è¨€äº¤äº’å’Œå®æ—¶å¯¼èˆªã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13733v1">PDF</a> 8 pages</p>
<p><strong>Summary</strong><br>è§†è§‰è¯­è¨€å¯¼èˆªï¼ˆVLNï¼‰æ˜¯æœºå™¨äººç³»ç»Ÿä¸­çš„ä¸€é¡¹åŸºæœ¬æŒ‘æˆ˜ï¼Œå¹¿æ³›åº”ç”¨äºçœŸå®ä¸–ç•Œç¯å¢ƒä¸­çš„æ™ºèƒ½ä½“éƒ¨ç½²ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•åœ¨ç©ºé—´æ¨ç†æ–¹é¢å­˜åœ¨çš„å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†FSR-VLNç³»ç»Ÿï¼Œå®ƒç»“åˆäº†åˆ†å±‚å¤šæ¨¡å¼åœºæ™¯å›¾ï¼ˆHMSGï¼‰å’Œå¿«æ…¢å¯¼èˆªæ¨ç†ï¼ˆFSRï¼‰ã€‚HMSGæä¾›äº†å¤šæ¨¡æ€åœ°å›¾è¡¨ç¤ºï¼Œæ”¯æŒä»ç²—ç•¥çš„æˆ¿é—´çº§åˆ«å®šä½åˆ°ç²¾ç»†çš„ç›®æ ‡è§†å›¾å’Œå¯¹è±¡è¯†åˆ«çš„æ¸è¿›æ£€ç´¢ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼ŒFSRé¦–å…ˆè¿›è¡Œå¿«é€ŸåŒ¹é…ï¼Œä»¥æœ‰æ•ˆåœ°é€‰æ‹©å€™é€‰æˆ¿é—´ã€è§†å›¾å’Œå¯¹è±¡ï¼Œç„¶ååº”ç”¨VLMé©±åŠ¨çš„ç»†åŒ–è¿›è¡Œæœ€ç»ˆç›®æ ‡é€‰æ‹©ã€‚æˆ‘ä»¬åœ¨ç”±äººå½¢æœºå™¨äººæ”¶é›†çš„å››ä¸ªç»¼åˆå®¤å†…æ•°æ®é›†ä¸Šè¯„ä¼°äº†FSR-VLNï¼Œä½¿ç”¨87æ¡æŒ‡ä»¤æ¶µç›–äº†å„ç§å¯¹è±¡ç±»åˆ«ã€‚FSR-VLNåœ¨æ‰€æœ‰æ•°æ®é›†ä¸Šçš„æ£€ç´¢æˆåŠŸç‡ï¼ˆRSRï¼‰è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ï¼ŒåŒæ—¶å“åº”æ—¶é—´ä¸åŸºäºVLMçš„æ–¹æ³•ç›¸æ¯”å‡å°‘äº†82%ï¼Œå®ƒé€šè¿‡ä»…åœ¨å¿«é€Ÿç›´è§‰å¤±è´¥æ—¶æ¿€æ´»æ…¢é€Ÿæ¨ç†æ¥å®ç°è¿™ä¸€ç‚¹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†FSR-VLNä¸äººå½¢æœºå™¨äººçš„è¯­éŸ³äº¤äº’ã€è§„åˆ’å’Œæ§åˆ¶æ¨¡å—é›†æˆåœ¨ä¸€èµ·ï¼Œå®ç°è‡ªç„¶è¯­è¨€äº¤äº’å’Œå®æ—¶å¯¼èˆªã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VLNæ˜¯æœºå™¨äººç³»ç»Ÿä¸­çš„ä¸€ä¸ªåŸºæœ¬æŒ‘æˆ˜ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨äºçœŸå®ä¸–ç•Œç¯å¢ƒä¸­çš„æ™ºèƒ½ä½“éƒ¨ç½²ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å­˜åœ¨é•¿è·ç¦»ç©ºé—´æ¨ç†çš„å±€é™æ€§ï¼Œå¸¸å¸¸è¡¨ç°å‡ºè¾ƒä½çš„æˆåŠŸç‡å’Œè¾ƒé«˜çš„æ¨ç†å»¶è¿Ÿã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„è§†è§‰è¯­è¨€å¯¼èˆªç³»ç»ŸFSR-VLNï¼Œç»“åˆäº†åˆ†å±‚å¤šæ¨¡å¼åœºæ™¯å›¾ï¼ˆHMSGï¼‰å’Œå¿«æ…¢å¯¼èˆªæ¨ç†ï¼ˆFSRï¼‰ã€‚</li>
<li>HMSGæä¾›äº†å¤šæ¨¡æ€åœ°å›¾è¡¨ç¤ºï¼Œæ”¯æŒä»ç²—ç•¥åˆ°ç²¾ç»†çš„æ¸è¿›æ£€ç´¢ã€‚</li>
<li>FSRé€šè¿‡å¿«é€ŸåŒ¹é…é€‰æ‹©å€™é€‰ç›®æ ‡ï¼Œç„¶ååº”ç”¨VLMé©±åŠ¨çš„ç»†åŒ–è¿›è¡Œæœ€ç»ˆç›®æ ‡é€‰æ‹©ã€‚</li>
<li>åœ¨å››ä¸ªå®¤å†…æ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒFSR-VLNè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶æ˜¾è‘—å‡å°‘äº†å“åº”æ—¶é—´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13733">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13733v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13733v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13733v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13733v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13733v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13733v1/page_5_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13733v1/page_5_2.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="DSPC-Dual-Stage-Progressive-Compression-Framework-for-Efficient-Long-Context-Reasoning"><a href="#DSPC-Dual-Stage-Progressive-Compression-Framework-for-Efficient-Long-Context-Reasoning" class="headerlink" title="DSPC: Dual-Stage Progressive Compression Framework for Efficient   Long-Context Reasoning"></a>DSPC: Dual-Stage Progressive Compression Framework for Efficient   Long-Context Reasoning</h2><p><strong>Authors:Yaxin Gao, Yao Lu, Zongfei Zhang, Jiaqi Nie, Shanqing Yu, Qi Xuan</strong></p>
<p>Large language models (LLMs) have achieved remarkable success in many natural language processing (NLP) tasks. To achieve more accurate output, the prompts used to drive LLMs have become increasingly longer, which incurs higher computational costs. To address this prompt inflation problem, prompt compression has been proposed. However, most existing methods require training a small auxiliary model for compression, incurring a significant amount of additional computation. To avoid this, we propose a two-stage, training-free approach, called Dual-Stage Progressive Compression (DSPC). In the coarse-grained stage, semantic-related sentence filtering removes sentences with low semantic value based on TF-IDF. In the fine-grained stage, token importance is assessed using attention contribution, cross-model loss difference, and positional importance, enabling the pruning of low-utility tokens while preserving semantics. We validate DSPC on LLaMA-3.1-8B-Instruct and GPT-3.5-Turbo under a constrained token budget and observe consistent improvements. For instance, in the FewShot task of the Longbench dataset, DSPC achieves a performance of 49.17 by using only 3x fewer tokens, outperforming the best state-of-the-art baseline LongLLMLingua by 7.76. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è®¸å¤šè‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸã€‚ä¸ºäº†è·å¾—æ›´å‡†ç¡®çš„è¾“å‡ºï¼Œç”¨äºé©±åŠ¨LLMçš„æç¤ºå˜å¾—è¶Šæ¥è¶Šé•¿ï¼Œè¿™å¯¼è‡´äº†æ›´é«˜çš„è®¡ç®—æˆæœ¬ã€‚ä¸ºäº†è§£å†³æç¤ºè†¨èƒ€é—®é¢˜ï¼Œå·²ç»æå‡ºäº†æç¤ºå‹ç¼©ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰æ–¹æ³•éœ€è¦è®­ç»ƒä¸€ä¸ªç”¨äºå‹ç¼©çš„è¾…åŠ©æ¨¡å‹ï¼Œè¿™äº§ç”Ÿäº†å¤§é‡çš„é¢å¤–è®¡ç®—ã€‚ä¸ºäº†é¿å…è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µçš„æ— è®­ç»ƒæ–¹æ³•ï¼Œç§°ä¸ºåŒé˜¶æ®µæ¸è¿›å‹ç¼©ï¼ˆDSPCï¼‰ã€‚åœ¨ç²—ç²’åº¦é˜¶æ®µï¼Œé€šè¿‡è¯­ä¹‰ç›¸å…³å¥å­è¿‡æ»¤ï¼ŒåŸºäºTF-IDFå»é™¤ä½è¯­ä¹‰ä»·å€¼çš„å¥å­ã€‚åœ¨ç»†ç²’åº¦é˜¶æ®µï¼Œé€šè¿‡æ³¨æ„åŠ›è´¡çŒ®ã€è·¨æ¨¡å‹æŸå¤±å·®å¼‚å’Œä½ç½®é‡è¦æ€§æ¥è¯„ä¼°ä»¤ç‰Œçš„é‡è¦æ€§ï¼Œä»è€Œèƒ½å¤Ÿåœ¨ä¿ç•™è¯­ä¹‰çš„åŒæ—¶åˆ é™¤ä½æ•ˆç”¨ä»¤ç‰Œã€‚æˆ‘ä»¬åœ¨æœ‰é™çš„ä»¤ç‰Œé¢„ç®—ä¸‹å¯¹LLaMA-3.1-8B-Instructå’ŒGPT-3.5-Turboè¿›è¡Œäº†DSPCéªŒè¯ï¼Œå¹¶è§‚å¯Ÿåˆ°äº†ä¸€è‡´æ€§çš„æ”¹è¿›ã€‚ä¾‹å¦‚ï¼Œåœ¨Longbenchæ•°æ®é›†çš„FewShotä»»åŠ¡ä¸­ï¼ŒDSPCä»…ä½¿ç”¨3å€è¾ƒå°‘çš„ä»¤ç‰Œå°±å®ç°äº†49.17çš„æ€§èƒ½ï¼Œä¼˜äºå½“å‰æœ€ä½³åŸºçº¿LongLLMLingua 7.76ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13723v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸã€‚ä¸ºäº†å¾—åˆ°æ›´å‡†ç¡®çš„è¾“å‡ºï¼Œç”¨äºé©±åŠ¨LLMsçš„æç¤ºè¶Šæ¥è¶Šé•¿ï¼Œè¿™å¸¦æ¥äº†æ›´é«˜çš„è®¡ç®—æˆæœ¬ã€‚ä¸ºè§£å†³æç¤ºè†¨èƒ€é—®é¢˜ï¼Œæå‡ºäº†æç¤ºå‹ç¼©æŠ€æœ¯ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰æ–¹æ³•éœ€è¦è®­ç»ƒä¸€ä¸ªç”¨äºå‹ç¼©çš„è¾…åŠ©æ¨¡å‹ï¼Œè¿™å¢åŠ äº†å¤§é‡çš„è®¡ç®—è´Ÿæ‹…ã€‚ä¸ºé¿å…è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºDual-Stage Progressive Compressionï¼ˆDSPCï¼‰çš„ä¸¤é˜¶æ®µã€æ— éœ€è®­ç»ƒçš„æ–¹æ³•ã€‚ç¬¬ä¸€é˜¶æ®µé€šè¿‡è¯­ä¹‰ç›¸å…³å¥å­è¿‡æ»¤å»é™¤ä½è¯­ä¹‰ä»·å€¼çš„å¥å­ï¼›ç¬¬äºŒé˜¶æ®µè¯„ä¼°æ ‡è®°çš„é‡è¦æ€§ï¼Œé€šè¿‡æ³¨æ„åŠ›è´¡çŒ®ã€è·¨æ¨¡å‹æŸå¤±å·®å¼‚å’Œä½ç½®é‡è¦æ€§ç­‰æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨ä¿ç•™è¯­ä¹‰çš„åŒæ—¶åˆ é™¤ä½æ•ˆç”¨æ ‡è®°ã€‚æˆ‘ä»¬åœ¨LLaMA-3.1-8B-Instructå’ŒGPT-3.5-Turboä¸ŠéªŒè¯äº†DSPCçš„æ•ˆæœï¼Œå¹¶åœ¨æœ‰é™çš„æ ‡è®°é¢„ç®—ä¸‹è§‚å¯Ÿåˆ°äº†ä¸€è‡´çš„æ”¹è¿›ã€‚ä¾‹å¦‚ï¼Œåœ¨Longbenchæ•°æ®é›†çš„FewShotä»»åŠ¡ä¸­ï¼ŒDSPCä»…ä½¿ç”¨3å€è¾ƒå°‘çš„æ ‡è®°å°±è¾¾åˆ°äº†49.17çš„æ€§èƒ½ï¼Œä¼˜äºå½“å‰æœ€ä½³åŸºçº¿LongLLMLinguaçš„7.76ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä»»åŠ¡ä¸­å¹¿æ³›åº”ç”¨ï¼Œä½†æç¤ºè†¨èƒ€é—®é¢˜å¯¼è‡´è®¡ç®—æˆæœ¬å¢åŠ ã€‚</li>
<li>ç°æœ‰çš„æç¤ºå‹ç¼©æ–¹æ³•é€šå¸¸éœ€è¦è®­ç»ƒè¾…åŠ©æ¨¡å‹ï¼Œå¢åŠ äº†è®¡ç®—è´Ÿæ‹…ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºDual-Stage Progressive Compressionï¼ˆDSPCï¼‰çš„ä¸¤é˜¶æ®µã€æ— éœ€è®­ç»ƒçš„æ–¹æ³•æ¥è§£å†³æç¤ºè†¨èƒ€é—®é¢˜ã€‚</li>
<li>DSPCåŒ…æ‹¬ä¸¤ä¸ªé˜¶æ®µï¼šç²—ç²’åº¦é˜¶æ®µé€šè¿‡è¯­ä¹‰ç›¸å…³å¥å­è¿‡æ»¤å»é™¤ä½è¯­ä¹‰ä»·å€¼çš„å¥å­ã€‚</li>
<li>åœ¨ç»†ç²’åº¦é˜¶æ®µï¼Œé€šè¿‡è¯„ä¼°æ ‡è®°çš„é‡è¦æ€§æ¥åˆ é™¤ä½æ•ˆç”¨æ ‡è®°ï¼ŒåŒæ—¶ä¿ç•™è¯­ä¹‰ã€‚</li>
<li>å®éªŒéªŒè¯è¡¨æ˜ï¼ŒDSPCåœ¨æœ‰é™çš„æ ‡è®°é¢„ç®—ä¸‹å®ç°äº†æ€§èƒ½æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13723">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13723v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13723v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13723v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13723v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="DSCC-HS-A-Dynamic-Self-Reinforcing-Framework-for-Hallucination-Suppression-in-Large-Language-Models"><a href="#DSCC-HS-A-Dynamic-Self-Reinforcing-Framework-for-Hallucination-Suppression-in-Large-Language-Models" class="headerlink" title="DSCC-HS: A Dynamic Self-Reinforcing Framework for Hallucination   Suppression in Large Language Models"></a>DSCC-HS: A Dynamic Self-Reinforcing Framework for Hallucination   Suppression in Large Language Models</h2><p><strong>Authors:Xiao Zheng</strong></p>
<p>Large Language Model (LLM) hallucination is a significant barrier to their reliable deployment. Current methods like Retrieval-Augmented Generation (RAG) are often reactive. We introduce <strong>Dynamic Self-reinforcing Calibration for Hallucination Suppression (DSCC-HS)</strong>, a novel, proactive framework that intervenes during autoregressive decoding. Inspired by dual-process cognitive theory, DSCC-HS uses a compact proxy model, trained in adversarial roles as a Factual Alignment Proxy (FAP) and a Hallucination Detection Proxy (HDP). During inference, these proxies dynamically steer a large target model by injecting a real-time steering vector, which is the difference between FAP and HDP logits, at each decoding step. This plug-and-play approach requires no modification to the target model. Our experiments on TruthfulQA and BioGEN show DSCC-HS achieves state-of-the-art performance. On TruthfulQA, it reached a 99.2% Factual Consistency Rate (FCR). On the long-form BioGEN benchmark, it attained the highest FActScore of 46.50. These results validate DSCC-HS as a principled and efficient solution for enhancing LLM factuality. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¹»è§‰æ˜¯å¯é éƒ¨ç½²çš„é‡å¤§éšœç¢ã€‚å½“å‰çš„æ–¹æ³•å¦‚æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰é€šå¸¸æ˜¯ååº”æ€§çš„ã€‚æˆ‘ä»¬å¼•å…¥äº†<strong>ç”¨äºå¹»è§‰æŠ‘åˆ¶çš„åŠ¨æ€è‡ªå¼ºåŒ–æ ¡å‡†ï¼ˆDSCC-HSï¼‰</strong>ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹ä¸»åŠ¨æ¡†æ¶ï¼Œå¯åœ¨è‡ªå›å½’è§£ç è¿‡ç¨‹ä¸­è¿›è¡Œå¹²é¢„ã€‚å—åŒè¿‡ç¨‹è®¤çŸ¥ç†è®ºçš„å¯å‘ï¼ŒDSCC-HSä½¿ç”¨ä¸€ä¸ªç´§å‡‘çš„ä»£ç†æ¨¡å‹ï¼Œä»¥å¯¹æŠ—æ€§è§’è‰²æ¥å—è®­ç»ƒï¼Œä½œä¸ºäº‹å®å¯¹é½ä»£ç†ï¼ˆFAPï¼‰å’Œå¹»è§‰æ£€æµ‹ä»£ç†ï¼ˆHDPï¼‰ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œè¿™äº›ä»£ç†é€šè¿‡æ³¨å…¥å®æ—¶æ§åˆ¶å‘é‡ï¼ˆå³FAPå’ŒHDPæ—¥å¿—ä¹‹é—´çš„å·®å€¼ï¼‰æ¥åŠ¨æ€å¼•å¯¼å¤§å‹ç›®æ ‡æ¨¡å‹ï¼Œè¯¥å‘é‡åœ¨æ¯ä¸ªè§£ç æ­¥éª¤ä¸­éƒ½ä¼šè¢«æ³¨å…¥ã€‚è¿™ç§å³æ’å³ç”¨çš„æ–¹æ³•æ— éœ€ä¿®æ”¹ç›®æ ‡æ¨¡å‹ã€‚æˆ‘ä»¬åœ¨TruthfulQAå’ŒBioGENä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDSCC-HSè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚åœ¨TruthfulQAä¸Šï¼Œå®ƒè¾¾åˆ°äº†99.2%çš„äº‹å®ä¸€è‡´æ€§ç‡ï¼ˆFCRï¼‰ã€‚åœ¨é•¿ç¯‡BioGENåŸºå‡†æµ‹è¯•ä¸­ï¼Œå®ƒè·å¾—äº†æœ€é«˜çš„FActScoreä¸º46.50ã€‚è¿™äº›ç»“æœéªŒè¯äº†DSCC-HSä½œä¸ºä¸€ç§æœ‰åŸåˆ™ä¸”é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œå¯ä»¥å¢å¼ºLLMçš„çœŸå®æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13702v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¹»è§‰æ˜¯ä¸€ä¸ªå¯é éƒ¨ç½²çš„éšœç¢ã€‚ç›®å‰çš„æ–¹æ³•é€šå¸¸æ˜¯ååº”æ€§çš„ï¼Œå¦‚æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºåŠ¨æ€è‡ªæˆ‘å¼ºåŒ–æ ¡å‡†æŠ‘åˆ¶å¹»è§‰ï¼ˆDSCC-HSï¼‰çš„æ–°å‹ä¸»åŠ¨æ¡†æ¶ï¼Œå®ƒåœ¨è‡ªåŠ¨å›å½’è§£ç è¿‡ç¨‹ä¸­è¿›è¡Œå¹²é¢„ã€‚å—åŒè¿‡ç¨‹è®¤çŸ¥ç†è®ºçš„å¯å‘ï¼ŒDSCC-HSä½¿ç”¨ä¸€ä¸ªç´§å‡‘çš„ä»£ç†æ¨¡å‹ï¼Œä»¥å¯¹æŠ—æ€§è§’è‰²ä½œä¸ºäº‹å®å¯¹é½ä»£ç†ï¼ˆFAPï¼‰å’Œå¹»è§‰æ£€æµ‹ä»£ç†ï¼ˆHDPï¼‰è¿›è¡Œè®­ç»ƒã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œè¿™äº›ä»£ç†é€šè¿‡æ³¨å…¥å®æ—¶è½¬å‘å‘é‡ï¼ˆå³FAPå’ŒHDPæ—¥å¿—ä¹‹é—´çš„å·®å¼‚ï¼‰ï¼ŒåŠ¨æ€å¼•å¯¼å¤§å‹ç›®æ ‡æ¨¡å‹è¿›è¡Œè§£ç ã€‚è¿™ç§å³æ’å³ç”¨æ–¹æ³•æ— éœ€ä¿®æ”¹ç›®æ ‡æ¨¡å‹ã€‚åœ¨TruthfulQAå’ŒBioGENä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDSCC-HSè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚åœ¨TruthfulQAä¸Šï¼Œå®ƒè¾¾åˆ°äº†99.2%çš„äº‹å®ä¸€è‡´æ€§ç‡ï¼ˆFCRï¼‰ã€‚åœ¨é•¿æ–‡æœ¬BioGENåŸºå‡†æµ‹è¯•ä¸­ï¼Œå®ƒè·å¾—äº†æœ€é«˜çš„FActScoreï¼Œå¾—åˆ†ä¸º46.50ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒDSCC-HSæ˜¯ä¸€ç§æé«˜LLMäº‹å®æ€§çš„æœ‰åŸåˆ™å’Œæœ‰æ•ˆç‡çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¹»è§‰æ˜¯å…¶å¯é éƒ¨ç½²çš„ä¸»è¦éšœç¢ä¹‹ä¸€ã€‚</li>
<li>ç›®å‰çš„æ–¹æ³•å¦‚æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰é€šå¸¸æ˜¯ååº”æ€§çš„ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†åŠ¨æ€è‡ªæˆ‘å¼ºåŒ–æ ¡å‡†æŠ‘åˆ¶å¹»è§‰ï¼ˆDSCC-HSï¼‰æ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„ä¸»åŠ¨å¹²é¢„æ–¹æ³•ã€‚</li>
<li>DSCC-HSä½¿ç”¨åŒè¿‡ç¨‹è®¤çŸ¥ç†è®ºå¯å‘ï¼Œé€šè¿‡ç´§å‡‘çš„ä»£ç†æ¨¡å‹è¿›è¡Œè®­ç»ƒã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡æ³¨å…¥å®æ—¶è½¬å‘å‘é‡ï¼Œåœ¨è‡ªåŠ¨å›å½’è§£ç è¿‡ç¨‹ä¸­åŠ¨æ€å¼•å¯¼å¤§å‹ç›®æ ‡æ¨¡å‹ã€‚</li>
<li>DSCC-HSåœ¨TruthfulQAå’ŒBioGENåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13702">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13702v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13702v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13702v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Improving-Context-Fidelity-via-Native-Retrieval-Augmented-Reasoning"><a href="#Improving-Context-Fidelity-via-Native-Retrieval-Augmented-Reasoning" class="headerlink" title="Improving Context Fidelity via Native Retrieval-Augmented Reasoning"></a>Improving Context Fidelity via Native Retrieval-Augmented Reasoning</h2><p><strong>Authors:Suyuchen Wang, Jinlin Wang, Xinyu Wang, Shiqi Li, Xiangru Tang, Sirui Hong, Xiao-Wen Chang, Chenglin Wu, Bang Liu</strong></p>
<p>Large language models (LLMs) often struggle with context fidelity, producing inconsistent answers when responding to questions based on provided information. Existing approaches either rely on expensive supervised fine-tuning to generate evidence post-answer or train models to perform web searches without necessarily improving utilization of the given context. We propose CARE, a novel native retrieval-augmented reasoning framework that teaches LLMs to explicitly integrate in-context evidence within their reasoning process with the modelâ€™s own retrieval capabilities. Our method requires limited labeled evidence data while significantly enhancing both retrieval accuracy and answer generation performance through strategically retrieved in-context tokens in the reasoning chain. Extensive experiments on multiple real-world and counterfactual QA benchmarks demonstrate that our approach substantially outperforms supervised fine-tuning, traditional retrieval-augmented generation methods, and external retrieval solutions. This work represents a fundamental advancement in making LLMs more accurate, reliable, and efficient for knowledge-intensive tasks. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šå¸¸åœ¨ä¸Šä¸‹æ–‡å¿ å®åº¦æ–¹é¢é‡åˆ°å›°éš¾ï¼Œåœ¨æ ¹æ®æä¾›çš„ä¿¡æ¯å›ç­”é—®é¢˜æ—¶ï¼Œä¼šäº§ç”Ÿä¸ä¸€è‡´çš„ç­”æ¡ˆã€‚ç°æœ‰æ–¹æ³•è¦ä¹ˆä¾èµ–äºæ˜‚è´µçš„ç›‘ç£å¾®è°ƒæ¥åœ¨å›ç­”é—®é¢˜åç”Ÿæˆè¯æ®ï¼Œè¦ä¹ˆè®­ç»ƒæ¨¡å‹æ‰§è¡Œç½‘ç»œæœç´¢ï¼Œè€Œä¸ä¸€å®šæé«˜ç»™å®šä¸Šä¸‹æ–‡çš„ä½¿ç”¨æ•ˆç‡ã€‚æˆ‘ä»¬æå‡ºäº†CAREï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„æœ¬åœŸæ£€ç´¢å¢å¼ºæ¨ç†æ¡†æ¶ï¼Œå®ƒèƒ½æ•™ä¼šLLMåœ¨å…¶æ¨ç†è¿‡ç¨‹ä¸­æ˜ç¡®æ•´åˆä¸Šä¸‹æ–‡è¯æ®å’Œæ¨¡å‹è‡ªèº«çš„æ£€ç´¢èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ–¹æ³•éœ€è¦æœ‰é™çš„æ ‡è®°è¯æ®æ•°æ®ï¼ŒåŒæ—¶é€šè¿‡æˆ˜ç•¥æ€§åœ°æ£€ç´¢æ¨ç†é“¾ä¸­çš„ä¸Šä¸‹æ–‡æ ‡è®°ï¼Œæ˜¾è‘—æé«˜æ£€ç´¢å‡†ç¡®æ€§å’Œç­”æ¡ˆç”Ÿæˆæ€§èƒ½ã€‚åœ¨å¤šä¸ªç°å®å’Œè™šæ„é—®ç­”åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—ä¼˜äºç›‘ç£å¾®è°ƒã€ä¼ ç»Ÿçš„æ£€ç´¢å¢å¼ºç”Ÿæˆæ–¹æ³•å’Œå¤–éƒ¨æ£€ç´¢è§£å†³æ–¹æ¡ˆã€‚è¿™é¡¹å·¥ä½œä½¿LLMåœ¨çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ä¸­æ›´åŠ å‡†ç¡®ã€å¯é å’Œé«˜æ•ˆï¼Œä»£è¡¨äº†ä¸€é¡¹åŸºæœ¬è¿›æ­¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13683v1">PDF</a> Accepted as a main conference paper at EMNLP 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤„ç†ä¸Šä¸‹æ–‡ä¸€è‡´æ€§æ—¶å­˜åœ¨å›°éš¾ï¼Œå¯¹åŸºäºæä¾›ä¿¡æ¯çš„æé—®ä¼šäº§ç”Ÿä¸ä¸€è‡´çš„ç­”æ¡ˆã€‚ç°æœ‰æ–¹æ³•ä¾èµ–æ˜‚è´µçš„ç›‘ç£å¾®è°ƒæ¥ç”Ÿæˆç­”æ¡ˆåçš„è¯æ®ï¼Œæˆ–è®­ç»ƒæ¨¡å‹æ‰§è¡Œç½‘ç»œæœç´¢ï¼Œä½†ä¸ä¸€å®šèƒ½æé«˜å¯¹ç»™å®šä¸Šä¸‹æ–‡çš„åˆ©ç”¨ã€‚æˆ‘ä»¬æå‡ºCAREï¼Œä¸€ç§æ–°å‹çš„æœ¬åœ°æ£€ç´¢å¢å¼ºæ¨ç†æ¡†æ¶ï¼Œæ—¨åœ¨æ•™æˆLLMåœ¨æ¨ç†è¿‡ç¨‹ä¸­æ˜ç¡®æ•´åˆä¸Šä¸‹æ–‡è¯æ®çš„èƒ½åŠ›ï¼Œå¹¶åˆ©ç”¨æ¨¡å‹çš„è‡ªèº«æ£€ç´¢åŠŸèƒ½ã€‚æˆ‘ä»¬çš„æ–¹æ³•åªéœ€è¦æœ‰é™çš„æ ‡è®°è¯æ®æ•°æ®ï¼ŒåŒæ—¶é€šè¿‡æˆ˜ç•¥æ€§åœ°æ£€ç´¢æ¨ç†é“¾ä¸­çš„ä¸Šä¸‹æ–‡ä»¤ç‰Œï¼Œæ˜¾è‘—æé«˜äº†æ£€ç´¢å‡†ç¡®æ€§å’Œç­”æ¡ˆç”Ÿæˆæ€§èƒ½ã€‚åœ¨å¤šä¸ªç°å®å’Œè™šæ„é—®ç­”åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¤§å¹…ä¼˜äºç›‘ç£å¾®è°ƒã€ä¼ ç»Ÿçš„æ£€ç´¢å¢å¼ºç”Ÿæˆæ–¹æ³•å’Œå¤–éƒ¨æ£€ç´¢è§£å†³æ–¹æ¡ˆã€‚è¿™é¡¹å·¥ä½œæ ‡å¿—ç€è®©LLMåœ¨çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ä¸­æ›´å‡†ç¡®ã€å¯é å’Œé«˜æ•ˆçš„é‡è¦è¿›æ­¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä¸Šä¸‹æ–‡ä¸€è‡´æ€§æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œå¯¼è‡´å¯¹åŸºäºæä¾›ä¿¡æ¯çš„æé—®äº§ç”Ÿä¸ä¸€è‡´çš„ç­”æ¡ˆã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–ç›‘ç£å¾®è°ƒæˆ–è®­ç»ƒæ¨¡å‹æ‰§è¡Œç½‘ç»œæœç´¢æ¥æ”¹å–„æ¨¡å‹æ€§èƒ½ï¼Œä½†ä¸ä¸€å®šèƒ½æé«˜å¯¹ç»™å®šä¸Šä¸‹æ–‡çš„åˆ©ç”¨ã€‚</li>
<li>CAREæ˜¯ä¸€ç§æ–°å‹çš„æœ¬åœ°æ£€ç´¢å¢å¼ºæ¨ç†æ¡†æ¶ï¼Œæ•´åˆäº†ä¸Šä¸‹æ–‡è¯æ®åˆ°LLMçš„æ¨ç†è¿‡ç¨‹ä¸­ã€‚</li>
<li>CAREåˆ©ç”¨æ¨¡å‹çš„è‡ªèº«æ£€ç´¢åŠŸèƒ½ï¼Œä»…éœ€è¦æœ‰é™çš„æ ‡è®°è¯æ®æ•°æ®ã€‚</li>
<li>é€šè¿‡æˆ˜ç•¥æ€§åœ°æ£€ç´¢æ¨ç†é“¾ä¸­çš„ä¸Šä¸‹æ–‡ä»¤ç‰Œï¼ŒCAREæ˜¾è‘—æé«˜äº†æ£€ç´¢å‡†ç¡®æ€§å’Œç­”æ¡ˆç”Ÿæˆæ€§èƒ½ã€‚</li>
<li>åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šï¼ŒCAREå¤§å¹…ä¼˜äºå…¶ä»–æ–¹æ³•ï¼ŒåŒ…æ‹¬ç›‘ç£å¾®è°ƒã€ä¼ ç»Ÿçš„æ£€ç´¢å¢å¼ºç”Ÿæˆæ–¹æ³•å’Œå¤–éƒ¨æ£€ç´¢è§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13683">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13683v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13683v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13683v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13683v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="DREAM-Domain-aware-Reasoning-for-Efficient-Autonomous-Underwater-Monitoring"><a href="#DREAM-Domain-aware-Reasoning-for-Efficient-Autonomous-Underwater-Monitoring" class="headerlink" title="DREAM: Domain-aware Reasoning for Efficient Autonomous Underwater   Monitoring"></a>DREAM: Domain-aware Reasoning for Efficient Autonomous Underwater   Monitoring</h2><p><strong>Authors:Zhenqi Wu, Abhinav Modi, Angelos Mavrogiannis, Kaustubh Joshi, Nikhil Chopra, Yiannis Aloimonos, Nare Karapetyan, Ioannis Rekleitis, Xiaomin Lin</strong></p>
<p>The ocean is warming and acidifying, increasing the risk of mass mortality events for temperature-sensitive shellfish such as oysters. This motivates the development of long-term monitoring systems. However, human labor is costly and long-duration underwater work is highly hazardous, thus favoring robotic solutions as a safer and more efficient option. To enable underwater robots to make real-time, environment-aware decisions without human intervention, we must equip them with an intelligent â€œbrain.â€ This highlights the need for persistent,wide-area, and low-cost benthic monitoring. To this end, we present DREAM, a Vision Language Model (VLM)-guided autonomy framework for long-term underwater exploration and habitat monitoring. The results show that our framework is highly efficient in finding and exploring target objects (e.g., oysters, shipwrecks) without prior location information. In the oyster-monitoring task, our framework takes 31.5% less time than the previous baseline with the same amount of oysters. Compared to the vanilla VLM, it uses 23% fewer steps while covering 8.88% more oysters. In shipwreck scenes, our framework successfully explores and maps the wreck without collisions, requiring 27.5% fewer steps than the vanilla model and achieving 100% coverage, while the vanilla model achieves 60.23% average coverage in our shipwreck environments. </p>
<blockquote>
<p>æµ·æ´‹æ­£åœ¨å˜æš–å’Œé…¸åŒ–ï¼Œå¢åŠ äº†å¯¹æ¸©åº¦æ•æ„Ÿçš„åŒå£³ç±»åŠ¨ç‰©ï¼ˆå¦‚ç‰¡è›ï¼‰å¤§è§„æ¨¡æ­»äº¡äº‹ä»¶çš„é£é™©ã€‚è¿™ä¿ƒä½¿äº†é•¿æœŸç›‘æµ‹ç³»ç»Ÿçš„å¼€å‘ã€‚ç„¶è€Œï¼Œäººå·¥åŠ³åŠ¨åŠ›æˆæœ¬é«˜æ˜‚ï¼Œé•¿æ—¶é—´çš„æ°´ä¸‹å·¥ä½œæä¸ºå±é™©ï¼Œå› æ­¤å€¾å‘äºä½¿ç”¨æœºå™¨äººè§£å†³æ–¹æ¡ˆä½œä¸ºæ›´å®‰å…¨ã€æ›´é«˜æ•ˆçš„æ›¿ä»£æ–¹æ¡ˆã€‚ä¸ºäº†èƒ½å¤Ÿè®©æ°´ä¸‹æœºå™¨äººåšå‡ºå®æ—¶ã€äº†è§£ç¯å¢ƒçš„å†³ç­–è€Œæ— éœ€äººå·¥å¹²é¢„ï¼Œæˆ‘ä»¬å¿…é¡»ä¸ºä»–ä»¬é…å¤‡ä¸€ä¸ªæ™ºèƒ½â€œå¤§è„‘â€ã€‚è¿™å‡¸æ˜¾äº†å¯¹æŒä¹…ã€å¹¿æ³›å’Œä½æˆæœ¬çš„åº•æ –ç›‘æµ‹çš„éœ€æ±‚ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºäº†DREAMï¼Œè¿™æ˜¯ä¸€ä¸ªç”±è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å¼•å¯¼çš„é•¿æœŸæ°´ä¸‹æ¢ç´¢å’Œæ –æ¯åœ°ç›‘æµ‹è‡ªä¸»æ€§æ¡†æ¶ã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶åœ¨å¯»æ‰¾å’Œæ¢ç´¢ç›®æ ‡å¯¹è±¡ï¼ˆä¾‹å¦‚ç‰¡è›ã€æ²‰èˆ¹ï¼‰æ–¹é¢éå¸¸é«˜æ•ˆï¼Œæ— éœ€é¢„å…ˆçš„ä½ç½®ä¿¡æ¯ã€‚åœ¨ç‰¡è›ç›‘æµ‹ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬çš„æ¡†æ¶æ¯”ä¹‹å‰çš„åŸºçº¿å‡å°‘äº†31.5%çš„æ—¶é—´ï¼ŒåŒæ—¶å‘ç°äº†ç›¸åŒæ•°é‡çš„ç‰¡è›ã€‚ä¸æ™®é€šçš„VLMç›¸æ¯”ï¼Œå®ƒå‡å°‘äº†23%çš„æ­¥éª¤ï¼ŒåŒæ—¶è¦†ç›–äº†æ›´å¤šçš„ç‰¡è›ï¼ˆå¢åŠ äº†8.88%ï¼‰ã€‚åœ¨æ²‰èˆ¹åœºæ™¯ä¸­ï¼Œæˆ‘ä»¬çš„æ¡†æ¶æˆåŠŸåœ°æ¢ç´¢å¹¶ç»˜åˆ¶äº†æ²‰èˆ¹åœ°å›¾ï¼Œæ²¡æœ‰å‘ç”Ÿç¢°æ’ï¼Œæ‰€éœ€çš„æ­¥éª¤æ¯”å¸¸è§„æ¨¡å‹å‡å°‘äº†27.5%ï¼Œå¹¶å®ç°äº†100%çš„è¦†ç›–ã€‚è€Œå¸¸è§„æ¨¡å‹åœ¨æˆ‘ä»¬çš„æ²‰èˆ¹ç¯å¢ƒä¸­å¹³å‡è¦†ç›–ç‡ä»…ä¸º60.23%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13666v1">PDF</a> submitted to ICRA 2026</p>
<p><strong>Summary</strong><br>æµ·æ´‹å˜æš–ä¸é…¸åŒ–å¨èƒäº†æ•æ„Ÿè´å£³ç±»åŠ¨ç‰©å¦‚ç”Ÿèšçš„ç”Ÿå­˜ï¼Œä¿ƒä½¿é•¿æœŸç›‘æ§ç³»ç»Ÿçš„å¼€å‘å˜å¾—é‡è¦ã€‚è€ƒè™‘åˆ°äººå·¥æˆæœ¬çš„æ˜‚è´µåŠé•¿æœŸæ°´ä¸‹å·¥ä½œçš„å±é™©ï¼Œæœºå™¨äººæˆä¸ºæ›´å®‰å…¨é«˜æ•ˆçš„é€‰æ‹©ã€‚ä¸ºä½¿æ°´ä¸‹æœºå™¨äººèƒ½è¿›è¡Œå®æ—¶ç¯å¢ƒæ„ŸçŸ¥å¹¶è‡ªä¸»å†³ç­–ï¼Œéœ€è¦ä¸ºå…¶é…å¤‡æ™ºèƒ½â€œå¤§è„‘â€ã€‚å› æ­¤å¼ºè°ƒäº†å¯¹æŒä¹…ã€å¹¿æ³›åŒºåŸŸã€ä½æˆæœ¬åº•æ –ç”Ÿç‰©ç›‘æµ‹çš„éœ€æ±‚ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºDREAMé¡¹ç›®ï¼Œé‡‡ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å¼•å¯¼çš„æ°´ä¸‹è‡ªä¸»æ¡†æ¶ï¼Œç”¨äºé•¿æœŸæ°´ä¸‹æ¢ç´¢å’Œæ –æ¯åœ°ç›‘æµ‹ã€‚ç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¡†æ¶åœ¨å¯»æ‰¾å’Œæ¢ç´¢ç›®æ ‡ç‰©ä½“ï¼ˆå¦‚ç”Ÿèšã€æ²‰èˆ¹ï¼‰æ–¹é¢éå¸¸é«˜æ•ˆï¼Œæ— éœ€é¢„å…ˆä½ç½®ä¿¡æ¯ã€‚åœ¨ç”Ÿèšç›‘æµ‹ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬çš„æ¡†æ¶æ¯”ä¹‹å‰çš„åŸºçº¿æ–¹æ³•èŠ‚çœ31.5%çš„æ—¶é—´ï¼ŒåŒæ—¶ä½¿ç”¨è¾ƒå°‘çš„æ­¥éª¤è¦†ç›–æ›´å¤šçš„ç”ŸèšåŒºåŸŸã€‚åœ¨æ²‰èˆ¹åœºæ™¯ä¸­ï¼Œæˆ‘ä»¬çš„æ¡†æ¶èƒ½å¤ŸæˆåŠŸæ¢ç´¢å¹¶ç»˜åˆ¶æ²‰èˆ¹åœ°å›¾ï¼Œä¸”ä¸ä¼šç¢°æ’èˆ¹åªã€‚ç›¸æ¯”æ™®é€šæ¨¡å‹ï¼Œæˆ‘ä»¬çš„æ¡†æ¶æ¢ç´¢æ­¥éª¤å‡å°‘27.5%ï¼Œå¹¶å®ç°äº†å…¨è¦†ç›–ï¼Œè€Œæ™®é€šæ¨¡å‹åªèƒ½è¾¾åˆ°å¹³å‡è¦†ç›–ç‡ä¸º60.23%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æµ·æ´‹å˜æš–ä¸é…¸åŒ–å¯¼è‡´å¯¹æ¸©åº¦æ•æ„Ÿçš„æµ·æ´‹ç”Ÿç‰©ï¼ˆå¦‚ç”Ÿèšï¼‰é¢ä¸´å¤§è§„æ¨¡æ­»äº¡çš„é£é™©ã€‚</li>
<li>é•¿æœŸæ°´ä¸‹ç›‘æµ‹ç³»ç»Ÿçš„å¼€å‘è‡³å…³é‡è¦ï¼Œä½†äººå·¥æˆæœ¬é«˜ä¸”å·¥ä½œå±é™©ï¼Œå› æ­¤æœºå™¨äººè§£å†³æ–¹æ¡ˆæ›´å—æ¬¢è¿ã€‚</li>
<li>æ°´ä¸‹æœºå™¨äººéœ€è¦é…å¤‡æ™ºèƒ½â€œå¤§è„‘â€ä»¥å®ç°å®æ—¶ç¯å¢ƒæ„ŸçŸ¥å’Œè‡ªä¸»å†³ç­–ã€‚</li>
<li>éœ€è¦æŒä¹…ã€å¹¿æ³›åŒºåŸŸå’Œä½æˆæœ¬çš„æ°´ä¸‹ç›‘æµ‹æ–¹æ³•ã€‚</li>
<li>æå‡ºçš„DREAMæ¡†æ¶é‡‡ç”¨è§†è§‰è¯­è¨€æ¨¡å‹å¼•å¯¼ï¼Œç”¨äºé•¿æœŸæ°´ä¸‹æ¢ç´¢å’Œæ –æ¯åœ°ç›‘æµ‹ã€‚</li>
<li>åœ¨ç›®æ ‡ç‰©ä½“ç›‘æµ‹æ–¹é¢ï¼ŒDREAMæ¡†æ¶è¡¨ç°å‡ºé«˜æ•ˆç‡ï¼Œæ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13666">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13666v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13666v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13666v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13666v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13666v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="LLM-I-LLMs-are-Naturally-Interleaved-Multimodal-Creators"><a href="#LLM-I-LLMs-are-Naturally-Interleaved-Multimodal-Creators" class="headerlink" title="LLM-I: LLMs are Naturally Interleaved Multimodal Creators"></a>LLM-I: LLMs are Naturally Interleaved Multimodal Creators</h2><p><strong>Authors:Zirun Guo, Feng Zhang, Kai Jia, Tao Jin</strong></p>
<p>We propose LLM-Interleaved (LLM-I), a flexible and dynamic framework that reframes interleaved image-text generation as a tool-use problem. LLM-I is designed to overcome the â€œone-toolâ€ bottleneck of current unified models, which are limited to synthetic imagery and struggle with tasks requiring factual grounding or programmatic precision. Our framework empowers a central LLM or MLLM agent to intelligently orchestrate a diverse toolkit of specialized visual tools, including online image search, diffusion-based generation, code execution, and image editing. The agent is trained to select and apply these tools proficiently via a Reinforcement Learning (RL) framework that features a hybrid reward system combining rule-based logic with judgments from LLM and MLLM evaluators. Trained on a diverse new dataset using four different model backbones, LLM-I demonstrates state-of-the-art performance, outperforming existing methods by a large margin across four benchmarks. We also introduce a novel test-time scaling strategy that provides further performance gains. Project Page: <a target="_blank" rel="noopener" href="https://github.com/ByteDance-BandAI/LLM-I">https://github.com/ByteDance-BandAI/LLM-I</a>. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†LLM-Interleavedï¼ˆLLM-Iï¼‰è¿™ä¸€çµæ´»ä¸”åŠ¨æ€æ¡†æ¶ï¼Œå®ƒå°†äº¤é”™å¼å›¾åƒæ–‡æœ¬ç”Ÿæˆé‡æ–°æ„å»ºä¸ºä¸€ç§å·¥å…·ä½¿ç”¨é—®é¢˜ã€‚LLM-Içš„è®¾è®¡æ—¨åœ¨å…‹æœå½“å‰ç»Ÿä¸€æ¨¡å‹çš„â€œå•ä¸€å·¥å…·â€ç“¶é¢ˆï¼Œè¿™äº›æ¨¡å‹ä»…é™äºåˆæˆå›¾åƒï¼Œå¹¶ä¸”åœ¨éœ€è¦äº‹å®ä¾æ®æˆ–ç¨‹åºç²¾ç¡®çš„ä»»åŠ¡æ–¹é¢è¡¨ç°æŒ£æ‰ã€‚æˆ‘ä»¬çš„æ¡†æ¶ä½¿ä¸­å¤®LLMæˆ–MLLMä»£ç†èƒ½å¤Ÿæ™ºèƒ½åœ°åè°ƒå„ç§ä¸“ä¸šè§†è§‰å·¥å…·å¥—ä»¶ï¼ŒåŒ…æ‹¬åœ¨çº¿å›¾åƒæœç´¢ã€åŸºäºæ‰©æ•£çš„ç”Ÿæˆã€ä»£ç æ‰§è¡Œå’Œå›¾åƒç¼–è¾‘ã€‚è¯¥ä»£ç†é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¡†æ¶æ¥å—åŸ¹è®­ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿç†Ÿç»ƒåœ°é€‰æ‹©å’Œåº”ç”¨è¿™äº›å·¥å…·ï¼Œå…¶ç‰¹è‰²åœ¨äºç»“åˆåŸºäºè§„åˆ™çš„é€»è¾‘ä¸LLMå’ŒMLLMè¯„ä¼°è€…çš„åˆ¤æ–­åŠ›çš„æ··åˆå¥–åŠ±ç³»ç»Ÿã€‚LLM-Iåœ¨æ–°å‹å¤šæ ·æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä½¿ç”¨å››ç§ä¸åŒçš„æ¨¡å‹éª¨å¹²ï¼Œå±•ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œåœ¨å››é¡¹åŸºå‡†æµ‹è¯•ä¸­å¤§å¹…è¶…è¶Šç°æœ‰æ–¹æ³•ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§æ–°å‹æµ‹è¯•æ—¶ç¼©æ”¾ç­–ç•¥ï¼Œå¯æä¾›è¿›ä¸€æ­¥çš„æ€§èƒ½æå‡ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://github.com/ByteDance-BandAI/LLM-I%E3%80%82">https://github.com/ByteDance-BandAI/LLM-Iã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13642v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>LLM-Interleavedæ¡†æ¶é€šè¿‡å°†å…¶è§†ä¸ºå·¥å…·ä½¿ç”¨é—®é¢˜ï¼Œå®ç°äº†å›¾åƒæ–‡æœ¬ç”Ÿæˆçš„é‡æ–°æ„æƒ³ã€‚è¯¥æ¡†æ¶æ—¨åœ¨å…‹æœå½“å‰ç»Ÿä¸€æ¨¡å‹â€œå•ä¸€å·¥å…·â€çš„ç“¶é¢ˆï¼Œè¿™äº›æ¨¡å‹ä»…é™äºåˆæˆå›¾åƒï¼Œéš¾ä»¥åº”å¯¹éœ€è¦äº‹å®ä¾æ®æˆ–ç¨‹åºç²¾ç¡®æ€§çš„ä»»åŠ¡ã€‚LLM-Iæ¡†æ¶å…è®¸ä¸­å¤®LLMæˆ–MLLMä»£ç†æ™ºèƒ½åœ°åè°ƒå„ç§ä¸“ä¸šè§†è§‰å·¥å…·ï¼ŒåŒ…æ‹¬åœ¨çº¿å›¾åƒæœç´¢ã€åŸºäºæ‰©æ•£çš„ç”Ÿæˆã€ä»£ç æ‰§è¡Œå’Œå›¾åƒç¼–è¾‘ç­‰ã€‚ä»£ç†é€šè¿‡ç»“åˆè§„åˆ™é€»è¾‘ä¸LLMå’ŒMLLMè¯„ä¼°è€…çš„åˆ¤æ–­çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¡†æ¶ï¼Œæ¥å—åŸ¹è®­ä»¥ç†Ÿç»ƒé€‰æ‹©å’Œè¿ç”¨è¿™äº›å·¥å…·ã€‚åœ¨é‡‡ç”¨å››ç§ä¸åŒæ¨¡å‹ä¸»å¹²çš„æ–°æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒåï¼ŒLLM-Iè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œåœ¨å››ä¸ªåŸºå‡†æµ‹è¯•ä¸­å¤§å¹…è¶…è¶Šç°æœ‰æ–¹æ³•ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§æ–°å‹æµ‹è¯•æ—¶ç¼©æ”¾ç­–ç•¥ï¼Œå¯æä¾›é¢å¤–çš„æ€§èƒ½æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM-Interleavedæ˜¯ä¸€ä¸ªçµæ´»ä¸”åŠ¨æ€çš„æ¡†æ¶ï¼Œå°†å›¾åƒæ–‡æœ¬ç”Ÿæˆé‡æ–°æ„æƒ³ä¸ºå·¥å…·ä½¿ç”¨é—®é¢˜ã€‚</li>
<li>è¯¥æ¡†æ¶æ—¨åœ¨å…‹æœå½“å‰ç»Ÿä¸€æ¨¡å‹çš„â€œå•ä¸€å·¥å…·â€ç“¶é¢ˆï¼Œé€‚åº”æ›´å¤šæ ·åŒ–çš„ä»»åŠ¡éœ€æ±‚ã€‚</li>
<li>LLM-Iæ¡†æ¶å…è®¸ä¸­å¤®LLMæˆ–MLLMä»£ç†æ™ºèƒ½åè°ƒå¤šç§ä¸“ä¸šè§†è§‰å·¥å…·ã€‚</li>
<li>ä»£ç†é€šè¿‡å¼ºåŒ–å­¦ä¹ æ¡†æ¶æ¥å—åŸ¹è®­ï¼Œç†Ÿç»ƒé€‰æ‹©å’Œåº”ç”¨è¿™äº›å·¥å…·ã€‚</li>
<li>LLM-Iåœ¨å¤šç§æ•°æ®é›†ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå¤§å¹…è¶…è¶Šç°æœ‰æ–¹æ³•ã€‚</li>
<li>æ–°å‹æµ‹è¯•æ—¶ç¼©æ”¾ç­–ç•¥å¯æä¾›é¢å¤–çš„æ€§èƒ½æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13642">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13642v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13642v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13642v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="AQUA-LLM-Evaluating-Accuracy-Quantization-and-Adversarial-Robustness-Trade-offs-in-LLMs-for-Cybersecurity-Question-Answering"><a href="#AQUA-LLM-Evaluating-Accuracy-Quantization-and-Adversarial-Robustness-Trade-offs-in-LLMs-for-Cybersecurity-Question-Answering" class="headerlink" title="AQUA-LLM: Evaluating Accuracy, Quantization, and Adversarial Robustness   Trade-offs in LLMs for Cybersecurity Question Answering"></a>AQUA-LLM: Evaluating Accuracy, Quantization, and Adversarial Robustness   Trade-offs in LLMs for Cybersecurity Question Answering</h2><p><strong>Authors:Onat Gungor, Roshan Sood, Harold Wang, Tajana Rosing</strong></p>
<p>Large Language Models (LLMs) have recently demonstrated strong potential for cybersecurity question answering (QA), supporting decision-making in real-time threat detection and response workflows. However, their substantial computational demands pose significant challenges for deployment on resource-constrained edge devices. Quantization, a widely adopted model compression technique, can alleviate these constraints. Nevertheless, quantization may degrade model accuracy and increase susceptibility to adversarial attacks. Fine-tuning offers a potential means to mitigate these limitations, but its effectiveness when combined with quantization remains insufficiently explored. Hence, it is essential to understand the trade-offs among accuracy, efficiency, and robustness. We propose AQUA-LLM, an evaluation framework designed to benchmark several state-of-the-art small LLMs under four distinct configurations: base, quantized-only, fine-tuned, and fine-tuned combined with quantization, specifically for cybersecurity QA. Our results demonstrate that quantization alone yields the lowest accuracy and robustness despite improving efficiency. In contrast, combining quantization with fine-tuning enhances both LLM robustness and predictive performance, achieving an optimal balance of accuracy, robustness, and efficiency. These findings highlight the critical need for quantization-aware, robustness-preserving fine-tuning methodologies to enable the robust and efficient deployment of LLMs for cybersecurity QA. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç½‘ç»œå®‰å…¨é—®ç­”ï¼ˆQAï¼‰æ–¹é¢æ˜¾ç¤ºå‡ºå¼ºå¤§çš„æ½œåŠ›ï¼Œæ”¯æŒå®æ—¶å¨èƒæ£€æµ‹å’Œå“åº”å·¥ä½œæµç¨‹ä¸­çš„å†³ç­–åˆ¶å®šã€‚ç„¶è€Œï¼Œå®ƒä»¬å·¨å¤§çš„è®¡ç®—éœ€æ±‚å¯¹èµ„æºå—é™çš„è¾¹ç¼˜è®¾å¤‡çš„éƒ¨ç½²æ„æˆäº†é‡å¤§æŒ‘æˆ˜ã€‚é‡åŒ–æ˜¯ä¸€ç§å¹¿æ³›é‡‡ç”¨çš„æ¨¡å‹å‹ç¼©æŠ€æœ¯ï¼Œå¯ä»¥ç¼“è§£è¿™äº›çº¦æŸã€‚ç„¶è€Œï¼Œé‡åŒ–å¯èƒ½ä¼šé™ä½æ¨¡å‹ç²¾åº¦å¹¶å¢åŠ é­å—å¯¹æŠ—æ€§æ”»å‡»çš„é£é™©ã€‚å¾®è°ƒæä¾›äº†ä¸€ç§ç¼“è§£è¿™äº›å±€é™æ€§çš„æ½œåœ¨æ‰‹æ®µï¼Œä½†å…¶ä¸é‡åŒ–çš„ç»“åˆæ•ˆæœå°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚å› æ­¤ï¼Œäº†è§£ç²¾åº¦ã€æ•ˆç‡å’Œç¨³å¥æ€§ä¹‹é—´çš„æƒè¡¡è‡³å…³é‡è¦ã€‚æˆ‘ä»¬æå‡ºäº†AQUA-LLMï¼Œè¿™æ˜¯ä¸€ä¸ªè¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨é’ˆå¯¹å››ç§ä¸åŒé…ç½®ä¸‹çš„å‡ ç§æœ€æ–°å°å‹LLMè¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼šåŸºç¡€ã€ä»…é‡åŒ–ã€å¾®è°ƒä»¥åŠä¸é‡åŒ–ç›¸ç»“åˆçš„å¾®è°ƒï¼Œä¸“é—¨ç”¨äºç½‘ç»œå®‰å…¨QAã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œä»…é‡åŒ–è™½ç„¶æé«˜äº†æ•ˆç‡ï¼Œä½†ä¼šå¯¼è‡´æœ€ä½çš„å‡†ç¡®æ€§å’Œç¨³å¥æ€§ã€‚ç›¸åï¼Œå°†é‡åŒ–å’Œå¾®è°ƒç›¸ç»“åˆï¼Œå¯ä»¥æé«˜LLMçš„ç¨³å¥æ€§å’Œé¢„æµ‹æ€§èƒ½ï¼Œå®ç°å‡†ç¡®æ€§ã€ç¨³å¥æ€§å’Œæ•ˆç‡ä¹‹é—´çš„æœ€ä½³å¹³è¡¡ã€‚è¿™äº›å‘ç°å‡¸æ˜¾äº†é‡åŒ–æ„ŸçŸ¥ã€ä¿æŒç¨³å¥æ€§çš„å¾®è°ƒæ–¹æ³•çš„è¿«åˆ‡éœ€æ±‚ï¼Œä»¥å®ç°ç½‘ç»œå®‰å…¨QAä¸­LLMçš„ç¨³å¥å’Œé«˜æ•ˆéƒ¨ç½²ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13514v1">PDF</a> Accepted by the 24th IEEE International Conference on Machine   Learning and Applications (ICMLAâ€™25)</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç½‘ç»œå®‰å…¨é—®ç­”ï¼ˆQAï¼‰ä¸­å±•ç°å‡ºå¼ºæ½œåŠ›ï¼Œæ”¯æŒå®æ—¶å¨èƒæ£€æµ‹ä¸å“åº”æµç¨‹ä¸­çš„å†³ç­–åˆ¶å®šã€‚ç„¶è€Œï¼Œå…¶å·¨å¤§çš„è®¡ç®—éœ€æ±‚å¯¹èµ„æºæœ‰é™çš„è¾¹ç¼˜è®¾å¤‡éƒ¨ç½²å¸¦æ¥æŒ‘æˆ˜ã€‚é‡åŒ–ï¼ˆä¸€ç§å¹¿æ³›é‡‡ç”¨çš„æ¨¡å‹å‹ç¼©æŠ€æœ¯ï¼‰å¯ç¼“è§£è¿™äº›çº¦æŸï¼Œä½†å¯èƒ½é™ä½æ¨¡å‹ç²¾åº¦å¹¶å¢åŠ é­å—æ”»å‡»çš„é£é™©ã€‚å¾®è°ƒå¯ç¼“è§£è¿™äº›å±€é™æ€§ï¼Œä½†å…¶ä¸é‡åŒ–çš„ç»“åˆæ•ˆæœå°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚å› æ­¤ï¼Œç†è§£ç²¾åº¦ã€æ•ˆç‡ä¸ç¨³å¥æ€§ä¹‹é—´çš„æƒè¡¡è‡³å…³é‡è¦ã€‚æˆ‘ä»¬æå‡ºAQUA-LLMè¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨é’ˆå¯¹ç½‘ç»œå®‰å…¨QAï¼Œå¯¹å‡ ç§æœ€æ–°å°å‹LLMåœ¨å››ç§ä¸åŒé…ç½®ä¸‹è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼šåŸºç¡€ã€ä»…é‡åŒ–ã€å¾®è°ƒã€ä»¥åŠç»“åˆé‡åŒ–çš„å¾®è°ƒã€‚ç»“æœæ˜¾ç¤ºï¼Œä»…é‡åŒ–åœ¨ç²¾åº¦å’Œç¨³å¥æ€§æ–¹é¢è¡¨ç°æœ€ä½ï¼Œå°½ç®¡æ•ˆç‡æœ‰æ‰€æå‡ã€‚ç›¸åï¼Œç»“åˆé‡åŒ–ä¸å¾®è°ƒå¯æå‡LLMçš„ç¨³å¥æ€§å’Œé¢„æµ‹æ€§èƒ½ï¼Œå®ç°ç²¾åº¦ã€ç¨³å¥æ€§å’Œæ•ˆç‡çš„æœ€ä½³å¹³è¡¡ã€‚è¿™è¡¨æ˜éœ€è¦é‡‡ç”¨å…¼é¡¾é‡åŒ–ã€ä¿æŒç¨³å¥æ€§çš„å¾®è°ƒæ–¹æ³•ï¼Œä»¥å®ç°LLMåœ¨ç½‘ç»œå®‰å…¨QAä¸­çš„ç¨³å¥é«˜æ•ˆéƒ¨ç½²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç½‘ç»œå®‰å…¨é—®ç­”ï¼ˆQAï¼‰ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†è®¡ç®—éœ€æ±‚å¤§ï¼Œéš¾ä»¥åœ¨èµ„æºå—é™çš„è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²ã€‚</li>
<li>é‡åŒ–æŠ€æœ¯å¯å‹ç¼©æ¨¡å‹ä»¥å‡è½»è®¡ç®—è´Ÿæ‹…ï¼Œä½†å¯èƒ½å¯¼è‡´æ¨¡å‹ç²¾åº¦ä¸‹é™å’Œæ˜“å—æ”»å‡»ã€‚</li>
<li>é‡åŒ–ä¸å¾®è°ƒç›¸ç»“åˆå¯æé«˜LLMçš„ç¨³å¥æ€§å’Œé¢„æµ‹æ€§èƒ½ã€‚</li>
<li>AQUA-LLMè¯„ä¼°æ¡†æ¶ç”¨äºåŸºå‡†æµ‹è¯•ä¸åŒé…ç½®çš„æœ€æ–°å°å‹LLMsåœ¨ç½‘ç»œå®‰å…¨QAä¸­çš„è¡¨ç°ã€‚</li>
<li>ä»…é‡åŒ–åœ¨ç²¾åº¦å’Œç¨³å¥æ€§æ–¹é¢æ•ˆæœæœ‰é™ï¼Œè€Œç»“åˆé‡åŒ–ä¸å¾®è°ƒå¯å®ç°ç²¾åº¦ã€ç¨³å¥æ€§å’Œæ•ˆç‡ä¹‹é—´çš„å¹³è¡¡ã€‚</li>
<li>éœ€è¦å¼€å‘å…¼é¡¾é‡åŒ–å¹¶ä¿æŒç¨³å¥æ€§çš„å¾®è°ƒæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13514">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13514v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13514v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13514v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13514v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13514v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="SteeringControl-Holistic-Evaluation-of-Alignment-Steering-in-LLMs"><a href="#SteeringControl-Holistic-Evaluation-of-Alignment-Steering-in-LLMs" class="headerlink" title="SteeringControl: Holistic Evaluation of Alignment Steering in LLMs"></a>SteeringControl: Holistic Evaluation of Alignment Steering in LLMs</h2><p><strong>Authors:Vincent Siu, Nicholas Crispino, David Park, Nathan W. Henry, Zhun Wang, Yang Liu, Dawn Song, Chenguang Wang</strong></p>
<p>We introduce SteeringControl, a benchmark for evaluating representation steering methods across core alignment objectivesâ€“bias, harmful generation, and hallucinationâ€“and their effects on secondary behaviors such as sycophancy and commonsense morality. While prior alignment work often highlights truthfulness or reasoning ability to demonstrate the side effects of representation steering, we find there are many unexplored tradeoffs not yet understood in a systematic way. We collect a dataset of safety-relevant primary and secondary behaviors to evaluate steering effectiveness and behavioral entanglement centered around five popular steering methods. To enable this, we craft a modular steering framework based on unique components that serve as the building blocks of many existing methods. Our results on Qwen-2.5-7B and Llama-3.1-8B find that strong steering performance is dependent on the specific combination of steering method, model, and targeted behavior, and that severe concept entanglement can result from poor combinations of these three as well. We release our code here: <a target="_blank" rel="noopener" href="https://github.com/wang-research-lab/SteeringControl.git">https://github.com/wang-research-lab/SteeringControl.git</a>. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†SteeringControlï¼Œè¿™æ˜¯ä¸€ä¸ªè¯„ä¼°æ ¸å¿ƒå¯¹é½ç›®æ ‡ï¼ˆåè§ã€æœ‰å®³ç”Ÿæˆå’Œå¹»è§‰ï¼‰ä¸‹çš„è¡¨ç¤ºè½¬å‘æ–¹æ³•åŠå…¶åœ¨å¯¹æ¬¡è¡Œä¸ºï¼ˆå¦‚è°„åªšå’Œå¸¸è¯†é“å¾·ï¼‰çš„å½±å“æ–¹é¢çš„åŸºå‡†æµ‹è¯•ã€‚è™½ç„¶å…ˆå‰çš„å¯¹é½å·¥ä½œé€šå¸¸å¼ºè°ƒçœŸå®æ€§æˆ–æ¨ç†èƒ½åŠ›æ¥å±•ç¤ºè¡¨ç¤ºè½¬å‘çš„å‰¯ä½œç”¨ï¼Œä½†æˆ‘ä»¬å‘ç°è¿˜æœ‰è®¸å¤šå°šæœªç³»ç»Ÿç†è§£çš„æœªæ¢ç´¢æƒè¡¡ã€‚æˆ‘ä»¬æ”¶é›†äº†ä¸€ä¸ªä¸å®‰å…¨ç›¸å…³çš„ä¸»è¡Œä¸ºå’Œæ¬¡è¡Œä¸ºæ•°æ®é›†ï¼Œä»¥è¯„ä¼°å›´ç»•äº”ç§æµè¡Œçš„è½¬å‘æ–¹æ³•çš„è½¬å‘æ•ˆæœå’Œè¡Œä¸ºçº ç¼ ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬åŸºäºç‹¬ç‰¹ç»„ä»¶æ„å»ºäº†ä¸€ä¸ªæ¨¡å—åŒ–è½¬å‘æ¡†æ¶ï¼Œè¿™äº›ç»„ä»¶ä½œä¸ºè®¸å¤šç°æœ‰æ–¹æ³•çš„æ„å»ºå—ã€‚æˆ‘ä»¬åœ¨Qwen-2.5-7Bå’ŒLlama-3.1-8Bä¸Šçš„ç»“æœå‘ç°ï¼Œå¼ºå¤§çš„è½¬å‘æ€§èƒ½å–å†³äºè½¬å‘æ–¹æ³•ã€æ¨¡å‹å’Œç›®æ ‡è¡Œä¸ºçš„ç‰¹å®šç»„åˆï¼Œè¿™ä¸‰ç§ä¸è‰¯ç»„åˆä¹Ÿå¯èƒ½å¯¼è‡´ä¸¥é‡çš„æ¦‚å¿µçº ç¼ ã€‚æˆ‘ä»¬åœ¨æ­¤å‘å¸ƒæˆ‘ä»¬çš„ä»£ç ï¼š<a target="_blank" rel="noopener" href="https://github.com/wang-research-lab/SteeringControl.git">https://github.com/wang-research-lab/SteeringControl.git</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13450v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ä»‹ç»äº†ä¸€ä¸ªåä¸ºSteeringControlçš„åŸºå‡†æµ‹è¯•å¹³å°ï¼Œç”¨äºè¯„ä¼°è¡¨å¾è½¬å‘æ–¹æ³•åœ¨ä¸åŒæ ¸å¿ƒå¯¹é½ç›®æ ‡ï¼ˆå¦‚åè§ã€æœ‰å®³ç”Ÿæˆå’Œè™šæ„ï¼‰ä¸Šçš„è¡¨ç°ï¼ŒåŠå…¶å¯¹å¯¹é½æ¬¡è¦è¡Œä¸ºï¼ˆå¦‚å¥‰æ‰¿å’Œå¸¸è¯†é“å¾·ï¼‰çš„å½±å“ã€‚æ–‡ç« é€šè¿‡æ”¶é›†å®‰å…¨ç›¸å…³çš„ä¸»æ¬¡è¡Œä¸ºæ•°æ®é›†æ¥è¯„ä¼°è½¬å‘æ•ˆæœå’Œè¡Œä¸ºçº ç¼ ï¼Œå¹¶å›´ç»•äº”ç§æµè¡Œçš„è½¬å‘æ–¹æ³•è¿›è¡Œç ”ç©¶ã€‚æå‡ºäº†ä¸€ç§åŸºäºç‹¬ç‰¹ç»„ä»¶çš„æ¨¡å—åŒ–è½¬å‘æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯ä½œä¸ºè®¸å¤šç°æœ‰æ–¹æ³•çš„åŸºæœ¬æ„ä»¶ã€‚å¯¹Qwen-2.5-7Bå’ŒLlama-3.1-8Bçš„æµ‹è¯•ç»“æœè¡¨æ˜ï¼Œå¼ºå¤§çš„è½¬å‘æ€§èƒ½å–å†³äºè½¬å‘æ–¹æ³•ã€æ¨¡å‹å’Œç›®æ ‡è¡Œä¸ºçš„ç‰¹å®šç»„åˆï¼Œè€Œè¿™ä¸‰è€…çš„ä¸è‰¯ç»„åˆå¯èƒ½å¯¼è‡´ä¸¥é‡çš„æ¦‚å¿µçº ç¼ ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SteeringControlæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°è¡¨å¾è½¬å‘æ–¹æ³•çš„åŸºå‡†æµ‹è¯•å¹³å°ï¼Œæ¶µç›–æ ¸å¿ƒå¯¹é½ç›®æ ‡å’Œæ¬¡è¦è¡Œä¸ºã€‚</li>
<li>æ–‡ç« æ”¶é›†äº†ä¸€ä¸ªå®‰å…¨ç›¸å…³çš„æ•°æ®é›†æ¥è¯„ä¼°è½¬å‘æ•ˆæœå’Œè¡Œä¸ºçº ç¼ ã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªæ¨¡å—åŒ–è½¬å‘æ¡†æ¶ï¼ŒåŸºäºç‹¬ç‰¹ç»„ä»¶ä½œä¸ºç°æœ‰æ–¹æ³•çš„åŸºæœ¬æ„ä»¶ã€‚</li>
<li>æµ‹è¯•ç»“æœå¼ºè°ƒäº†è½¬å‘æ–¹æ³•ã€æ¨¡å‹å’Œç‰¹å®šç›®æ ‡è¡Œä¸ºç»„åˆçš„é‡è¦æ€§ã€‚</li>
<li>è½¬å‘æ€§èƒ½å—åˆ°è½¬å‘æ–¹æ³•ã€æ¨¡å‹å’Œæ¬¡è¦è¡Œä¸ºä¹‹é—´æ¦‚å¿µçº ç¼ çš„å½±å“ã€‚</li>
<li>ä¸åŒè½¬å‘æ–¹æ³•çš„ç»„åˆå¯èƒ½ä¼šå¯¼è‡´ä¸åŒçš„æ¦‚å¿µçº ç¼ ç¨‹åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13450">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13450v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13450v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13450v1/page_2_0.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="EdiVal-Agent-An-Object-Centric-Framework-for-Automated-Scalable-Fine-Grained-Evaluation-of-Multi-Turn-Editing"><a href="#EdiVal-Agent-An-Object-Centric-Framework-for-Automated-Scalable-Fine-Grained-Evaluation-of-Multi-Turn-Editing" class="headerlink" title="EdiVal-Agent: An Object-Centric Framework for Automated, Scalable,   Fine-Grained Evaluation of Multi-Turn Editing"></a>EdiVal-Agent: An Object-Centric Framework for Automated, Scalable,   Fine-Grained Evaluation of Multi-Turn Editing</h2><p><strong>Authors:Tianyu Chen, Yasi Zhang, Zhi Zhang, Peiyu Yu, Shu Wang, Zhendong Wang, Kevin Lin, Xiaofei Wang, Zhengyuan Yang, Linjie Li, Chung-Ching Lin, Jianwen Xie, Oscar Leong, Lijuan Wang, Ying Nian Wu, Mingyuan Zhou</strong></p>
<p>Instruction-based image editing has advanced rapidly, yet reliable and interpretable evaluation remains a bottleneck. Current protocols either (i) depend on paired reference images â€“ resulting in limited coverage and inheriting biases from prior generative models â€“ or (ii) rely solely on zero-shot vision-language models (VLMs), whose prompt-based assessments of instruction following, content consistency, and visual quality are often imprecise.   To address this, we introduce EdiVal-Agent, an automated, scalable, and fine-grained evaluation framework for multi-turn instruction-based editing from an object-centric perspective, supported by a suite of expert tools. Given an image, EdiVal-Agent first decomposes it into semantically meaningful objects, then synthesizes diverse, context-aware editing instructions. For evaluation, it integrates VLMs with open-vocabulary object detectors to assess instruction following, uses semantic-level feature extractors to evaluate content consistency, and leverages human preference models to judge visual quality. We show that combining VLMs with object detectors yields stronger agreement with human judgments in instruction-following evaluation compared to using VLMs alone and CLIP-based metrics. Furthermore, the pipelineâ€™s modular design allows future tools to be seamlessly integrated, enhancing evaluation accuracy over time.   Instantiating this pipeline, we build EdiVal-Bench, a multi-turn editing benchmark covering 9 instruction types and 11 state-of-the-art editing models spanning autoregressive (AR) (including Nano Banana, GPT-Image-1), flow-matching, and diffusion paradigms. We demonstrate that EdiVal-Agent can be used to identify existing failure modes, thereby informing the development of the next generation of editing models. Project page: <a target="_blank" rel="noopener" href="https://tianyucodings.github.io/EdiVAL-page/">https://tianyucodings.github.io/EdiVAL-page/</a>. </p>
<blockquote>
<p>åŸºäºæŒ‡ä»¤çš„å›¾åƒç¼–è¾‘æŠ€æœ¯å·²ç»è¿…é€Ÿå‘å±•ï¼Œä½†å¯é ä¸”å¯è§£é‡Šçš„è¯„ä»·ä»ç„¶æ˜¯ç“¶é¢ˆã€‚å½“å‰åè®®è¦ä¹ˆï¼ˆiï¼‰ä¾èµ–äºé…å¯¹å‚è€ƒå›¾åƒâ€”â€”å¯¼è‡´è¦†ç›–æœ‰é™å¹¶ç»§æ‰¿å…ˆå‰ç”Ÿæˆæ¨¡å‹çš„åè§â€”â€”è¦ä¹ˆï¼ˆiiï¼‰å®Œå…¨ä¾èµ–äºé›¶æ ·æœ¬è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ï¼Œå…¶åŸºäºæç¤ºçš„æŒ‡ä»¤éµå¾ªã€å†…å®¹ä¸€è‡´æ€§å’Œè§†è§‰è´¨é‡è¯„ä¼°å¾€å¾€ä¸ç²¾ç¡®ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†EdiVal-Agentï¼Œè¿™æ˜¯ä¸€ä¸ªè‡ªåŠ¨åŒ–ã€å¯æ‰©å±•çš„é’ˆå¯¹å¤šè½®åŸºäºæŒ‡ä»¤ç¼–è¾‘çš„ç²¾ç»†è¯„ä»·æ¡†æ¶ï¼Œå¾—åˆ°ä¸€ç³»åˆ—ä¸“å®¶å·¥å…·çš„æ”¯æŒã€‚ç»™å®šä¸€ä¸ªå›¾åƒï¼ŒEdiVal-Agenté¦–å…ˆå°†å…¶åˆ†è§£ä¸ºè¯­ä¹‰ä¸Šæœ‰æ„ä¹‰çš„å¯¹è±¡ï¼Œç„¶ååˆæˆå¤šæ ·ä¸”ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„ç¼–è¾‘æŒ‡ä»¤ã€‚åœ¨è¯„ä¼°è¿‡ç¨‹ä¸­ï¼Œå®ƒå°†VLMsä¸å¼€æ”¾è¯æ±‡å¯¹è±¡æ£€æµ‹å™¨ç›¸ç»“åˆæ¥è¯„ä¼°æŒ‡ä»¤éµå¾ªæƒ…å†µï¼Œä½¿ç”¨è¯­ä¹‰çº§ç‰¹å¾æå–å™¨æ¥è¯„ä¼°å†…å®¹ä¸€è‡´æ€§ï¼Œå¹¶åˆ©ç”¨äººç±»åå¥½æ¨¡å‹æ¥åˆ¤æ–­è§†è§‰è´¨é‡ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œä¸å•ç‹¬ä½¿ç”¨VLMså’ŒåŸºäºCLIPçš„æŒ‡æ ‡ç›¸æ¯”ï¼Œå°†VLMsä¸å¯¹è±¡æ£€æµ‹å™¨ç›¸ç»“åˆåœ¨æŒ‡ä»¤éµå¾ªè¯„ä¼°ä¸­ä¸äººç±»åˆ¤æ–­çš„ä¸€è‡´æ€§æ›´å¼ºã€‚æ­¤å¤–ï¼Œè¯¥ç®¡é“æ¨¡å—åŒ–è®¾è®¡å…è®¸æœªæ¥å·¥å…·æ— ç¼é›†æˆï¼Œéšç€æ—¶é—´çš„æ¨ç§»æé«˜è¯„ä¼°å‡†ç¡®æ€§ã€‚é€šè¿‡å®ä¾‹åŒ–æ­¤ç®¡é“ï¼Œæˆ‘ä»¬æ„å»ºäº†EdiVal-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šè½®ç¼–è¾‘åŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–9ç§æŒ‡ä»¤ç±»å‹å’Œ11ç§æœ€æ–°ç¼–è¾‘æ¨¡å‹ï¼ŒåŒ…æ‹¬è‡ªå›å½’ï¼ˆARï¼‰ï¼ˆåŒ…æ‹¬Nano Bananaã€GPT-Image-1ï¼‰ã€æµåŒ¹é…å’Œæ‰©æ•£èŒƒå¼ã€‚æˆ‘ä»¬è¯æ˜EdiVal-Agentå¯ç”¨äºè¯†åˆ«ç°æœ‰å¤±è´¥æ¨¡å¼ï¼Œä»è€Œä¸ºä¸‹ä¸€ä»£ç¼–è¾‘æ¨¡å‹çš„å¼€å‘æä¾›ä¿¡æ¯ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://tianyucodings.github.io/EdiVAL-page/%E3%80%82">https://tianyucodings.github.io/EdiVAL-page/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13399v1">PDF</a> Tianyu Chen and Yasi Zhang contributed equally; Oscar Leong, Lijuan   Wang, Ying Nian Wu, and Mingyuan Zhou advised equally</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹åŸºäºæŒ‡ä»¤çš„å›¾åƒç¼–è¾‘çš„è¯„ä¼°ç“¶é¢ˆï¼Œæå‡ºä¸€ç§è‡ªåŠ¨åŒ–ã€å¯è§„æ¨¡åŒ–ã€ç²¾ç»†åŒ–çš„è¯„ä¼°æ¡†æ¶EdiVal-Agentï¼Œæ”¯æŒä»å¯¹è±¡ä¸­å¿ƒçš„è§’åº¦è¿›è¡Œå¤šè½®æŒ‡ä»¤ç¼–è¾‘è¯„ä¼°ã€‚è¯¥æ¡†æ¶ç»“åˆVLMsä¸å¼€æ”¾è¯æ±‡å¯¹è±¡æ£€æµ‹å™¨ï¼Œè¯„ä¼°æŒ‡ä»¤éµå¾ªæƒ…å†µã€å†…å®¹ä¸€è‡´æ€§åŠè§†è§‰è´¨é‡ã€‚æ–‡ç« è¿˜å±•ç¤ºäº†EdiVal-Benchçš„å»ºç«‹æƒ…å†µï¼Œè¿™æ˜¯ä¸€ä¸ªæ¶µç›–å¤šç§æŒ‡ä»¤ç±»å‹å’Œå…ˆè¿›ç¼–è¾‘æ¨¡å‹çš„å¤šè½®ç¼–è¾‘åŸºå‡†æµ‹è¯•é›†ã€‚é€šè¿‡å®ä¾‹å±•ç¤ºï¼ŒEdiVal-Agentå¯è¯†åˆ«ç°æœ‰æ¨¡å‹çš„å¤±è´¥æ¨¡å¼ï¼Œä¸ºæœªæ¥ç¼–è¾‘æ¨¡å‹çš„å‘å±•æä¾›å‚è€ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å½“å‰åŸºäºæŒ‡ä»¤çš„å›¾åƒç¼–è¾‘åœ¨è¯„ä¼°ä¸Šé‡åˆ°å›°éš¾ï¼Œç¼ºå°‘å¯é ä¸”å¯è§£é‡Šçš„è¯„ä¼°æ–¹æ³•ã€‚</li>
<li>æå‡ºçš„EdiVal-Agentæ¡†æ¶æ—¨åœ¨è§£å†³è¿™ä¸€é—®é¢˜ï¼Œç»“åˆVLMså’Œå¼€æ”¾è¯æ±‡å¯¹è±¡æ£€æµ‹å™¨è¿›è¡Œç²¾ç»†åŒ–è¯„ä¼°ã€‚</li>
<li>EdiVal-Agentèƒ½ä»å¯¹è±¡ä¸­å¿ƒçš„è§’åº¦è¿›è¡Œå¤šè½®æŒ‡ä»¤ç¼–è¾‘è¯„ä¼°ï¼Œæé«˜è¯„ä¼°çš„å¯é æ€§å’Œå‡†ç¡®æ€§ã€‚</li>
<li>è¯¥æ¡†æ¶è¿˜åŒ…æ‹¬ä¸€ä¸ªåä¸ºEdiVal-Benchçš„å¤šè½®ç¼–è¾‘åŸºå‡†æµ‹è¯•é›†ï¼Œæ¶µç›–å¤šç§æŒ‡ä»¤ç±»å‹å’Œå…ˆè¿›çš„ç¼–è¾‘æ¨¡å‹ã€‚</li>
<li>é€šè¿‡å®ä¾‹å±•ç¤ºï¼ŒEdiVal-Agentæœ‰åŠ©äºè¯†åˆ«ç°æœ‰æ¨¡å‹çš„å¤±è´¥æ¨¡å¼ï¼Œä¸ºæœªæ¥æ¨¡å‹çš„å‘å±•æä¾›æŒ‡å¯¼ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13399">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13399v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13399v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13399v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13399v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="BranchGRPO-Stable-and-Efficient-GRPO-with-Structured-Branching-in-Diffusion-Models"><a href="#BranchGRPO-Stable-and-Efficient-GRPO-with-Structured-Branching-in-Diffusion-Models" class="headerlink" title="BranchGRPO: Stable and Efficient GRPO with Structured Branching in   Diffusion Models"></a>BranchGRPO: Stable and Efficient GRPO with Structured Branching in   Diffusion Models</h2><p><strong>Authors:Yuming Li, Yikai Wang, Yuying Zhu, Zhongyu Zhao, Ming Lu, Qi She, Shanghang Zhang</strong></p>
<p>Recent progress in aligning image and video generative models with Group Relative Policy Optimization (GRPO) has improved human preference alignment, but existing variants remain inefficient due to sequential rollouts and large numbers of sampling steps, unreliable credit assignment: sparse terminal rewards are uniformly propagated across timesteps, failing to capture the varying criticality of decisions during denoising. In this paper, we present BranchGRPO, a method that restructures the rollout process into a branching tree, where shared prefixes amortize computation and pruning removes low-value paths and redundant depths. BranchGRPO introduces three contributions: (1) a branching scheme that amortizes rollout cost through shared prefixes while preserving exploration diversity; (2) a reward fusion and depth-wise advantage estimator that transforms sparse terminal rewards into dense step-level signals; and (3) pruning strategies that cut gradient computation but leave forward rollouts and exploration unaffected. On HPDv2.1 image alignment, BranchGRPO improves alignment scores by up to \textbf{16%} over DanceGRPO, while reducing per-iteration training time by nearly \textbf{55%}. A hybrid variant, BranchGRPO-Mix, further accelerates training to 4.7x faster than DanceGRPO without degrading alignment. On WanX video generation, it further achieves higher Video-Align scores with sharper and temporally consistent frames compared to DanceGRPO. Codes are available at \href{<a target="_blank" rel="noopener" href="https://fredreic1849.github.io/BranchGRPO-Webpage/%7D%7BBranchGRPO%7D">https://fredreic1849.github.io/BranchGRPO-Webpage/}{BranchGRPO}</a>. </p>
<blockquote>
<p>è¿‘æœŸåœ¨å›¾åƒå’Œè§†é¢‘ç”Ÿæˆæ¨¡å‹ä¸ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰å¯¹é½æ–¹é¢çš„è¿›å±•å·²ç»æé«˜äº†äººç±»åå¥½å¯¹é½çš„æ•ˆæœã€‚ç„¶è€Œï¼Œç”±äºåºåˆ—æ¨æ¼”å’Œå¤§é‡é‡‡æ ·æ­¥éª¤çš„å­˜åœ¨ï¼Œç°æœ‰å˜ä½“ä»ç„¶æ•ˆç‡ä½ä¸‹ï¼Œä¿¡ç”¨åˆ†é…ä¸å¯é ï¼šç¨€ç–çš„ç»ˆç«¯å¥–åŠ±è¢«å‡åŒ€ä¼ æ’­åˆ°å„ä¸ªæ—¶é—´ç‚¹ï¼Œæœªèƒ½æ•æ‰åˆ°å»å™ªè¿‡ç¨‹ä¸­å†³ç­–çš„ä¸åŒå…³é”®æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†BranchGRPOæ–¹æ³•ï¼Œè¯¥æ–¹æ³•å°†æ¨æ¼”è¿‡ç¨‹é‡æ„ä¸ºä¸€é¢—åˆ†æ”¯æ ‘ï¼Œå…¶ä¸­å…±äº«å‰ç¼€æ‘Šé”€äº†è®¡ç®—æˆæœ¬ï¼Œä¿®å‰ªåˆ™ç§»é™¤äº†ä½ä»·å€¼è·¯å¾„å’Œå†—ä½™æ·±åº¦ã€‚BranchGRPOå¼•å…¥äº†ä¸‰é¡¹è´¡çŒ®ï¼šï¼ˆ1ï¼‰ä¸€ç§é€šè¿‡å…±äº«å‰ç¼€æ¥æ‘Šé”€æ¨æ¼”æˆæœ¬çš„åŒæ—¶ä¿æŒæ¢ç´¢å¤šæ ·æ€§çš„åˆ†æ”¯æ–¹æ¡ˆï¼›ï¼ˆ2ï¼‰ä¸€ç§å°†ç¨€ç–ç»ˆç«¯å¥–åŠ±è½¬åŒ–ä¸ºå¯†é›†æ­¥éª¤çº§ä¿¡å·çš„å¥–åŠ±èåˆå’Œæ·±åº¦ä¼˜åŠ¿ä¼°è®¡å™¨ï¼›ï¼ˆ3ï¼‰ä¿®å‰ªç­–ç•¥å‡å°‘äº†æ¢¯åº¦è®¡ç®—ï¼Œä½†ä¸å½±å“æ­£å‘æ¨æ¼”å’Œæ¢ç´¢ã€‚åœ¨HPDv2.1å›¾åƒå¯¹é½æ–¹é¢ï¼ŒBranchGRPOåœ¨DanceGRPOçš„åŸºç¡€ä¸Šæé«˜äº†å¯¹é½åˆ†æ•°ï¼Œæœ€é«˜å¯è¾¾16%ï¼ŒåŒæ—¶æ¯è¿­ä»£ä¸€æ¬¡çš„è®­ç»ƒæ—¶é—´å‡å°‘äº†è¿‘55%ã€‚ä¸€ç§æ··åˆå˜ä½“BranchGRPO-Mixè¿›ä¸€æ­¥åŠ é€Ÿäº†è®­ç»ƒï¼Œé€Ÿåº¦æ¯”DanceGRPOå¿«4.7å€ï¼ŒåŒæ—¶ä¸é™ä½å¯¹é½æ•ˆæœã€‚åœ¨WanXè§†é¢‘ç”Ÿæˆæ–¹é¢ï¼Œå®ƒè¿›ä¸€æ­¥å®ç°äº†æ›´é«˜çš„è§†é¢‘å¯¹é½åˆ†æ•°ï¼Œä¸DanceGRPOç›¸æ¯”ï¼Œç”Ÿæˆçš„å¸§æ›´åŠ æ¸…æ™°ä¸”æ—¶é—´ä¸Šä¸€è‡´ã€‚ç›¸å…³ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://fredreic1849.github.io/BranchGRPO-Webpage/">BranchGRPO</a>è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.06040v4">PDF</a> 12 pages, 6 figures</p>
<p><strong>Summary</strong><br>     è¿‘æœŸå›¾åƒå’Œè§†é¢‘ç”Ÿæˆæ¨¡å‹é€šè¿‡é‡‡ç”¨Group Relative Policy Optimizationï¼ˆGRPOï¼‰è¿›è¡Œå¯¹é½ï¼Œæå‡äº†ä¸äººç±»åå¥½å¯¹é½çš„æ•ˆæœã€‚ä½†ç°æœ‰æ–¹æ³•å› åºåˆ—å±•å¼€ã€å¤§é‡é‡‡æ ·æ­¥éª¤å’Œä¸å¯é çš„ä¿¡ç”¨åˆ†é…ï¼ˆç¨€ç–ç»ˆç«¯å¥–åŠ±æ— æ³•åæ˜ å†³ç­–è¿‡ç¨‹ä¸­çš„ä¸åŒé‡è¦æ€§ï¼‰è€Œå¯¼è‡´æ•ˆç‡è¾ƒä½ã€‚æœ¬æ–‡æå‡ºçš„BranchGRPOæ–¹æ³•é€šè¿‡æ„å»ºåˆ†æ”¯æ ‘é‡ç»„å±•å¼€è¿‡ç¨‹ï¼Œåˆ©ç”¨å…±äº«å‰ç¼€é™ä½å±•å¼€æˆæœ¬å¹¶å®ç°è®¡ç®—æˆæœ¬åˆ†æ‘Šã€‚æ­¤å¤–ï¼ŒBranchGRPOå¼•å…¥ä¸‰é¡¹è´¡çŒ®ï¼šåˆ†æ”¯æ–¹æ¡ˆã€å¥–åŠ±èåˆå’Œæ·±åº¦ä¼˜åŠ¿ä¼°è®¡å™¨ä»¥åŠå‰ªæç­–ç•¥ã€‚åœ¨HPDv2.1å›¾åƒå¯¹é½ä»»åŠ¡ä¸Šï¼ŒBranchGRPOç›¸è¾ƒäºDanceGRPOæå‡äº†å¯¹é½å¾—åˆ†è¾¾16%ï¼Œå¹¶å°†æ¯è¿­ä»£è®­ç»ƒæ—¶é—´å‡å°‘è¿‘55%ã€‚åŒæ—¶ï¼Œå…¶æ··åˆå˜ä½“BranchGRPO-Mixåœ¨åŠ é€Ÿè®­ç»ƒçš„åŒæ—¶ä¸é™ä½å¯¹é½æ•ˆæœã€‚åœ¨WanXè§†é¢‘ç”Ÿæˆä»»åŠ¡ä¸Šï¼Œç›¸è¾ƒäºDanceGRPOå®ç°äº†æ›´é«˜çš„Video-Alignå¾—åˆ†å¹¶å‘ˆç°å‡ºæ›´æ¸…æ™°è¿è´¯çš„å¸§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>BranchGRPOé€šè¿‡æ„å»ºåˆ†æ”¯æ ‘æ”¹å–„äº†å›¾åƒå’Œè§†é¢‘ç”Ÿæˆæ¨¡å‹çš„æ•ˆç‡é—®é¢˜ã€‚</li>
<li>åˆ†æ”¯æ–¹æ¡ˆåˆ©ç”¨å…±äº«å‰ç¼€é™ä½äº†å±•å¼€æˆæœ¬ï¼ŒåŒæ—¶ä¿è¯äº†æ¢ç´¢å¤šæ ·æ€§ã€‚</li>
<li>å¥–åŠ±èåˆå’Œæ·±åº¦ä¼˜åŠ¿ä¼°è®¡å™¨èƒ½å°†ç¨€ç–çš„ç»ˆç«¯å¥–åŠ±è½¬åŒ–ä¸ºå¯†é›†çš„æ­¥éª¤çº§ä¿¡å·ã€‚</li>
<li>é€šè¿‡å‰ªæç­–ç•¥ï¼ŒBranchGRPOå‡å°‘äº†æ¢¯åº¦è®¡ç®—ä½†ä¸å½±å“å‰å‘æ»šåŠ¨å’Œæ¢ç´¢ã€‚</li>
<li>åœ¨HPDv2.1å›¾åƒå¯¹é½ä»»åŠ¡ä¸Šï¼ŒBranchGRPOæ˜¾è‘—æé«˜å¯¹é½å¾—åˆ†å¹¶é™ä½äº†è®­ç»ƒæ—¶é—´ã€‚</li>
<li>BranchGRPO-Mixæ··åˆå˜ä½“åœ¨åŠ é€Ÿè®­ç»ƒçš„åŒæ—¶ä¿æŒäº†è‰¯å¥½çš„å¯¹é½æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.06040">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.06040v4/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.06040v4/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.06040v4/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.06040v4/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.06040v4/page_5_0.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-19/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-19/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-19/LLM/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_LLM/2509.13905v1/page_2_0.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-19  Apertus Democratizing Open and Compliant LLMs for Global Language   Environments
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-19
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-19/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_Talking Head Generation/2509.13774v1/page_2_0.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-19  Dual-Actor Fine-Tuning of VLA Models A Talk-and-Tweak Human-in-the-Loop   Approach
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-19
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32562k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
