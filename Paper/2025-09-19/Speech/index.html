<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech 方向最新论文已更新，请持续关注 Update in 2025-09-19  CS-FLEURS A Massively Multilingual and Code-Switched Speech Dataset">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_Speech/2509.13581v1/page_0_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-09-19
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-10-02
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    8.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    35 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-09-19-更新"><a href="#2025-09-19-更新" class="headerlink" title="2025-09-19 更新"></a>2025-09-19 更新</h1><h2 id="CS-FLEURS-A-Massively-Multilingual-and-Code-Switched-Speech-Dataset"><a href="#CS-FLEURS-A-Massively-Multilingual-and-Code-Switched-Speech-Dataset" class="headerlink" title="CS-FLEURS: A Massively Multilingual and Code-Switched Speech Dataset"></a>CS-FLEURS: A Massively Multilingual and Code-Switched Speech Dataset</h2><p><strong>Authors:Brian Yan, Injy Hamed, Shuichiro Shimizu, Vasista Lodagala, William Chen, Olga Iakovenko, Bashar Talafha, Amir Hussein, Alexander Polok, Kalvin Chang, Dominik Klement, Sara Althubaiti, Puyuan Peng, Matthew Wiesner, Thamar Solorio, Ahmed Ali, Sanjeev Khudanpur, Shinji Watanabe, Chih-Chen Chen, Zhen Wu, Karim Benharrak, Anuj Diwan, Samuele Cornell, Eunjung Yeo, Kwanghee Choi, Carlos Carvalho, Karen Rosero</strong></p>
<p>We present CS-FLEURS, a new dataset for developing and evaluating code-switched speech recognition and translation systems beyond high-resourced languages. CS-FLEURS consists of 4 test sets which cover in total 113 unique code-switched language pairs across 52 languages: 1) a 14 X-English language pair set with real voices reading synthetically generated code-switched sentences, 2) a 16 X-English language pair set with generative text-to-speech 3) a 60 {Arabic, Mandarin, Hindi, Spanish}-X language pair set with the generative text-to-speech, and 4) a 45 X-English lower-resourced language pair test set with concatenative text-to-speech. Besides the four test sets, CS-FLEURS also provides a training set with 128 hours of generative text-to-speech data across 16 X-English language pairs. Our hope is that CS-FLEURS helps to broaden the scope of future code-switched speech research. Dataset link: <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/byan/cs-fleurs">https://huggingface.co/datasets/byan/cs-fleurs</a>. </p>
<blockquote>
<p>我们推出CS-FLEURS数据集，这是一个用于开发和评估跨高资源语言代码切换的语音识别和翻译系统的新数据集。CS-FLEURS包含四个测试集，总共覆盖113种独特的代码切换语言对和52种语言：1）包含真实语音朗读合成代码切换句子的14种英语对外的语言对集；2）生成性文本语音合成的具有创造性文本的带有语言倾向的口音英语的另一种测试集；包含具有生成性文本语音合成口音的阿拉伯、普通话、印度和西班牙语对外口音的测试集，以及一种带有拼接文本语音的口音英语的测试集。除了这四个测试集外，CS-FLEURS还提供包含具有创造性文本的口音英语的合成语音数据的训练集，共计时长为CS-FLEURS希望拓宽未来代码切换语音识别的研究范围。数据集链接：<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/byan/cs-fleurs">https://huggingface.co/datasets/byan/cs-fleurs</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.14161v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>CS-FLEURS是一个用于开发和评估跨语言代码切换语音识别和翻译系统的新数据集。它包含四个测试集，涵盖113种独特的代码切换语言对和52种语言。该数据集可用于支持多种语言的代码切换场景，并提供训练集以支持未来代码切换语音研究的拓宽。数据集链接：<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/byan/cs-fleurs%E3%80%82">https://huggingface.co/datasets/byan/cs-fleurs。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CS-FLEURS是一个用于开发评估跨语言代码切换语音识别和翻译系统的新数据集。</li>
<li>数据集包含四个测试集，涵盖113种独特的代码切换语言对和52种语言。</li>
<li>数据集支持多种场景的跨语言代码切换应用。</li>
<li>数据集包含128小时的生成式文本到语音数据的训练集。</li>
<li>CS-FLEURS提供了广泛的语种覆盖，包括英语与其他语言的组合。</li>
<li>数据集可以用于未来的代码切换语音研究拓宽领域。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14161">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_Speech/2509.14161v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_Speech/2509.14161v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_Speech/2509.14161v1/page_1_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_Speech/2509.14161v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_Speech/2509.14161v1/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_Speech/2509.14161v1/page_3_0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Canary-1B-v2-Parakeet-TDT-0-6B-v3-Efficient-and-High-Performance-Models-for-Multilingual-ASR-and-AST"><a href="#Canary-1B-v2-Parakeet-TDT-0-6B-v3-Efficient-and-High-Performance-Models-for-Multilingual-ASR-and-AST" class="headerlink" title="Canary-1B-v2 &amp; Parakeet-TDT-0.6B-v3: Efficient and High-Performance   Models for Multilingual ASR and AST"></a>Canary-1B-v2 &amp; Parakeet-TDT-0.6B-v3: Efficient and High-Performance   Models for Multilingual ASR and AST</h2><p><strong>Authors:Monica Sekoyan, Nithin Rao Koluguri, Nune Tadevosyan, Piotr Zelasko, Travis Bartley, Nick Karpov, Jagadeesh Balam, Boris Ginsburg</strong></p>
<p>This report introduces Canary-1B-v2, a fast, robust multilingual model for Automatic Speech Recognition (ASR) and Speech-to-Text Translation (AST). Built with a FastConformer encoder and Transformer decoder, it supports 25 languages primarily European. The model was trained on 1.7M hours of total data samples, including Granary and NeMo ASR Set 3.0, with non-speech audio added to reduce hallucinations for ASR and AST. We describe its two-stage pre-training and fine-tuning process with dynamic data balancing, as well as experiments with an nGPT encoder. Results show nGPT scales well with massive data, while FastConformer excels after fine-tuning. For timestamps, Canary-1B-v2 uses the NeMo Forced Aligner (NFA) with an auxiliary CTC model, providing reliable segment-level timestamps for ASR and AST. Evaluations show Canary-1B-v2 outperforms Whisper-large-v3 on English ASR while being 10x faster, and delivers competitive multilingual ASR and AST performance against larger models like Seamless-M4T-v2-large and LLM-based systems. We also release Parakeet-TDT-0.6B-v3, a successor to v2, offering multilingual ASR across the same 25 languages with just 600M parameters. </p>
<blockquote>
<p>这份报告介绍了Canary-1B-v2，这是一个快速、稳健的多语言模型，用于自动语音识别（ASR）和语音到文本的翻译（AST）。该模型采用FastConformer编码器和Transformer解码器构建，主要支持25种欧洲语言。该模型在170万小时的总数据样本上进行训练，包括Granary和NeMo ASR Set 3.0，并添加了非语音音频，以减少ASR和AST的幻觉。我们描述了其两阶段预训练和微调过程，采用动态数据平衡，以及使用nGPT编码器的实验。结果表明，nGPT在大量数据中表现良好，而FastConformer在微调后表现出色。对于时间戳，Canary-1B-v2使用NeMo强制对齐器（NFA）和辅助CTC模型，为ASR和AST提供可靠的分段级时间戳。评估表明，Canary-1B-v2在英语ASR方面超越了Whisper-large-v3，而且速度更快，同时在多语言ASR和AST方面与更大的模型（如无缝M4T-v2大型模型）和基于LLM的系统表现竞争。我们还发布了Parakeet-TDT-0.6B-v3，这是v2的后续版本，使用仅6亿个参数即可提供相同25种语言的多语言ASR。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.14128v1">PDF</a> Mini Version of it Submitted to ICASSP 2026</p>
<p><strong>Summary</strong></p>
<p>这份报告介绍了Canary-1B-v2模型，它是一个快速、稳健的多语言模型，主要用于语音识别（ASR）和语音到文本的翻译（AST）。该模型采用FastConformer编码器和Transformer解码器，支持25种主要欧洲语言。经过大规模数据样本的训练和两阶段预训练与微调，该模型表现出优秀的性能。Canary-1B-v2使用NeMo强制对齐器（NFA）提供可靠的时段级时间戳，并在英语ASR上表现优于Whisper-large-v3，同时处理速度更快。它还推出了Parakeet-TDT-0.6B-v3模型，具有更小的参数规模，但依然能进行多语言ASR。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Canary-1B-v2是一个针对自动语音识别（ASR）和语音到文本翻译（AST）的快速、稳健的多语言模型。</li>
<li>该模型采用FastConformer编码器和Transformer解码器，支持25种主要欧洲语言。</li>
<li>模型经过两阶段预训练和微调，使用动态数据平衡来提高性能。</li>
<li>模型使用NeMo强制对齐器（NFA）提供可靠的时间戳。</li>
<li>在英语ASR方面，Canary-1B-v2优于Whisper-large-v3模型，同时处理速度更快。</li>
<li>模型能够处理大规模数据，并且FastConformer在微调后表现出卓越性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14128">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_Speech/2509.14128v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_Speech/2509.14128v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_Speech/2509.14128v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_Speech/2509.14128v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_Speech/2509.14128v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="A-Lightweight-Fourier-based-Network-for-Binaural-Speech-Enhancement-with-Spatial-Cue-Preservation"><a href="#A-Lightweight-Fourier-based-Network-for-Binaural-Speech-Enhancement-with-Spatial-Cue-Preservation" class="headerlink" title="A Lightweight Fourier-based Network for Binaural Speech Enhancement with   Spatial Cue Preservation"></a>A Lightweight Fourier-based Network for Binaural Speech Enhancement with   Spatial Cue Preservation</h2><p><strong>Authors:Xikun Lu, Yujian Ma, Xianquan Jiang, Xuelong Wang, Jinqiu Sang</strong></p>
<p>Binaural speech enhancement faces a severe trade-off challenge, where state-of-the-art performance is achieved by computationally intensive architectures, while lightweight solutions often come at the cost of significant performance degradation. To bridge this gap, we propose the Global Adaptive Fourier Network (GAF-Net), a lightweight deep complex network that aims to establish a balance between performance and computational efficiency. The GAF-Net architecture consists of three components. First, a dual-feature encoder combining short-time Fourier transform and gammatone features enhances the robustness of acoustic representation. Second, a channel-independent globally adaptive Fourier modulator efficiently captures long-term temporal dependencies while preserving the spatial cues. Finally, a dynamic gating mechanism is implemented to reduce processing artifacts. Experimental results show that GAF-Net achieves competitive performance, particularly in terms of binaural cues (ILD and IPD error) and objective intelligibility (MBSTOI), with fewer parameters and computational cost. These results confirm that GAF-Net provides a feasible way to achieve high-fidelity binaural processing on resource-constrained devices. </p>
<blockquote>
<p>双耳语音增强面临着严重的权衡挑战，现有技术性能的实现依赖于计算密集型的架构，而轻量级解决方案往往以性能显著下降为代价。为了弥补这一差距，我们提出了全球自适应傅里叶网络（GAF-Net），这是一个轻量级的深度复数网络，旨在在性能和计算效率之间取得平衡。GAF-Net架构由三个部分组成。首先，结合短时傅里叶变换和伽马通特征的双重特征编码器增强了声音表示的鲁棒性。其次，通道独立的全局自适应傅里叶调制器有效地捕获了长期时间依赖性，同时保留了空间线索。最后，实现了动态门控机制以减少处理过程中的伪影。实验结果表明，GAF-Net在双耳线索（ILD和IPD误差）和客观清晰度（MBSTOI）方面取得了具有竞争力的性能表现，同时减少了参数和计算成本。这些结果证实，GAF-Net在资源受限的设备上实现高保真双耳处理提供了一种可行的方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.14076v1">PDF</a> Submitted to ICASSP 2026</p>
<p><strong>Summary</strong></p>
<p>GAF-Net是一种结合了全球自适应傅立叶网络架构的轻量级深度复杂网络，旨在平衡性能和计算效率。它采用双特征编码器、全局自适应傅立叶调制器和动态门控机制等技术，实现了高性能和轻量级的语音增强效果。实验结果表明，GAF-Net在双耳线索、客观可懂度和参数计算成本等方面均表现出竞争力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GAF-Net是一种针对语音增强的轻量级深度复杂网络。</li>
<li>该网络结合了双特征编码器、全局自适应傅立叶调制器和动态门控机制等技术。</li>
<li>GAF-Net旨在平衡高性能和计算效率。</li>
<li>实验结果显示，GAF-Net在语音增强方面表现出竞争力。</li>
<li>GAF-Net对于资源受限的设备实现高保真双耳处理具有可行性。</li>
<li>GAF-Net特别优化了双耳线索（ILD和IPD误差）和客观可懂度（MBSTOI）。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14076">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_Speech/2509.14076v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_Speech/2509.14076v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_Speech/2509.14076v1/page_1_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_Speech/2509.14076v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_Speech/2509.14076v1/page_3_1.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Mixture-of-Low-Rank-Adapter-Experts-in-Generalizable-Audio-Deepfake-Detection"><a href="#Mixture-of-Low-Rank-Adapter-Experts-in-Generalizable-Audio-Deepfake-Detection" class="headerlink" title="Mixture of Low-Rank Adapter Experts in Generalizable Audio Deepfake   Detection"></a>Mixture of Low-Rank Adapter Experts in Generalizable Audio Deepfake   Detection</h2><p><strong>Authors:Janne Laakkonen, Ivan Kukanov, Ville Hautamäki</strong></p>
<p>Foundation models such as Wav2Vec2 excel at representation learning in speech tasks, including audio deepfake detection. However, after being fine-tuned on a fixed set of bonafide and spoofed audio clips, they often fail to generalize to novel deepfake methods not represented in training. To address this, we propose a mixture-of-LoRA-experts approach that integrates multiple low-rank adapters (LoRA) into the model’s attention layers. A routing mechanism selectively activates specialized experts, enhancing adaptability to evolving deepfake attacks. Experimental results show that our method outperforms standard fine-tuning in both in-domain and out-of-domain scenarios, reducing equal error rates relative to baseline models. Notably, our best MoE-LoRA model lowers the average out-of-domain EER from 8.55% to 6.08%, demonstrating its effectiveness in achieving generalizable audio deepfake detection. </p>
<blockquote>
<p>Wave2Vec2等基础模型在语音任务（包括音频深度伪造检测）中擅长表示学习。然而，在对固定的真实和假冒音频片段进行微调后，它们通常无法推广到训练中没有出现的新型深度伪造方法。为了解决这个问题，我们提出了一种混合LoRA专家方法，该方法将多个低秩适配器（LoRA）集成到模型的注意力层中。路由机制有选择地激活专业专家，提高对不断发展的深度伪造攻击的适应性。实验结果表明，与标准微调相比，我们的方法在域内和域外场景中表现更好，相对于基线模型降低了等误码率。值得注意的是，我们最好的MoE-LoRA模型将域外平均EER从8.55%降低到6.08%，表明其在实现可泛化的音频深度伪造检测方面的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13878v1">PDF</a> 6 pages, 3 figures, 1 table</p>
<p><strong>Summary</strong></p>
<p>模型如Wav2Vec2在语音任务中的表示学习表现卓越，尤其在音频深度伪造检测方面。然而，在固定的一组真实和伪造音频片段上进行微调后，它们往往无法推广到训练中没有代表的新型深度伪造方法。为解决这一问题，我们提出了一种混合LoRA专家方法，该方法将多个低秩适配器（LoRA）集成到模型的注意力层中。路由机制能够选择性地激活专业专家，增强对不断发展的深度伪造攻击的适应性。实验结果表明，我们的方法在域内和域外场景中都优于标准微调，相对于基线模型降低了等错误率。特别是，我们最好的MoE-LoRA模型将平均域外EER从8.55%降低到6.08%，证明了其在实现可推广的音频深度伪造检测方面的有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Wav2Vec2等模型在语音表示学习，尤其是音频深度伪造检测方面表现出色。</li>
<li>现有模型在固定音频集上微调后，难以适应新型深度伪造方法。</li>
<li>提出的混合LoRA专家方法通过集成多个低秩适配器增强模型的适应性。</li>
<li>路由机制能选择性地激活特定专家，以应对不断演变的深度伪造攻击。</li>
<li>实验显示，该方法在域内和域外场景中都优于标准微调。</li>
<li>MoE-LoRA模型降低了平均域外等错误率，证明了其有效性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13878">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_Speech/2509.13878v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_Speech/2509.13878v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_Speech/2509.13878v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_Speech/2509.13878v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Conducting-Mission-Critical-Voice-Experiments-with-Automated-Speech-Recognition-and-Crowdsourcing"><a href="#Conducting-Mission-Critical-Voice-Experiments-with-Automated-Speech-Recognition-and-Crowdsourcing" class="headerlink" title="Conducting Mission-Critical Voice Experiments with Automated Speech   Recognition and Crowdsourcing"></a>Conducting Mission-Critical Voice Experiments with Automated Speech   Recognition and Crowdsourcing</h2><p><strong>Authors:Jan Janak, Kahlil Dozier, Lauren Berny, Liang Hu, Dan Rubenstein, Charles Jennings, Henning Schulzrinne</strong></p>
<p>Mission-critical voice (MCV) communications systems have been a critical tool for the public safety community for over eight decades. Public safety users expect MCV systems to operate reliably and consistently, particularly in challenging conditions. Because of these expectations, the Public Safety Communications Research (PSCR) Division of the National Institute of Standards and Technology (NIST) has been interested in correlating impairments in MCV communication systems and public safety user quality of experience (QoE). Previous research has studied MCV voice quality and intelligibility in a controlled environment. However, such research has been limited by the challenges inherent in emulating real-world environmental conditions. Additionally, there is the question of the best metric to use to reflect QoE accurately.   This paper describes our efforts to develop the methodology and tools for human-subject experiments with MCV. We illustrate their use in human-subject experiments in emulated real-world environments. The tools include a testbed for emulating real-world MCV systems and an automated speech recognition (ASR) robot approximating human subjects in transcription tasks. We evaluate QoE through a Levenshtein Distance-based metric, arguing it is a suitable proxy for measuring comprehension and the QoE. We conducted human-subject studies with Amazon MTurk volunteers to understand the influence of selected system parameters and impairments on human subject performance and end-user QoE. We also compare the performance of several ASR system configurations with human-subject performance. We find that humans generally perform better than ASR in accuracy-related MCV tasks and that the codec significantly influences the end-user QoE and ASR performance. </p>
<blockquote>
<p>任务关键语音（MCV）通信系统已作为公共安全社区的重要工具长达八十多年。公共安全用户期望MCV系统能够可靠且稳定地运行，特别是在具有挑战性的条件下。由于这些期望，美国国家技术标准研究所（NIST）的公共安全通信研究（PSCR）部门对MCV通信系统的缺陷与公共安全用户的质量体验（QoE）之间的相关性非常感兴趣。以往的研究已在受控环境中研究了MCV的语音质量和清晰度。然而，这类研究受限于模拟现实环境条件的挑战。此外，还存在用于准确反映QoE的最佳指标的问题。本文描述了我们在开发MCV人类主体实验的方法和工具方面的努力。我们在模拟现实环境的人类主体实验中说明了它们的使用。这些工具包括一个模拟现实MCV系统的测试平台和一个在转录任务中近似人类主体的自动语音识别（ASR）机器人。我们通过基于Levenshtein距离的度量来评估QoE，认为它是衡量理解和QoE的合适代理。我们使用亚马逊MTurk志愿者进行人类主体研究，以了解所选系统参数和缺陷对人类主体性能和最终用户QoE的影响。我们还比较了不同ASR系统配置与人类主体性能的表现。我们发现，在准确性相关的MCV任务中，人类的表现通常优于ASR，并且编码器对最终用户的QoE和ASR性能具有重大影响。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13724v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>本论文关注公共安全问题中至关重要的语音通信系统（MCV）。为解决MCV系统在实际环境中的挑战及其对公共安全用户体验（QoE）的影响，国家标准技术研究所公共研究部致力于探究MCV通信系统中的损害与QoE的相关性。此前的实验主要在受控环境中研究MCV的语音质量和清晰度，但在模拟真实环境时面临诸多挑战。同时，对最佳度量QoE的标准也存在疑问。本文描述了为MCV进行人体实验的方法和工具开发，包括模拟真实世界环境的测试平台和模拟人类转录任务的自动语音识别机器人。通过基于Levenshtein距离的度量评估QoE，认为其适合衡量理解和QoE。通过亚马逊MTurk志愿者进行人体实验，研究选定系统参数和损害对人体表现和用户体验的影响，并将多种ASR系统配置与人体性能进行比较。发现人类在准确性相关的MCV任务中通常表现优于ASR，并且编码器对用户体验和ASR性能有显著影响。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>MCV系统是公共安全社区的重要工具，其可靠性和一致性至关重要。</li>
<li>NIST对MCV系统损害与公共安全性用户体验（QoE）的相关性感兴趣。</li>
<li>之前的研究主要在受控环境中进行，难以模拟真实环境。</li>
<li>选择Levenshtein距离作为评估QoE的度量标准，能有效衡量理解和用户体验。</li>
<li>人体实验显示人类在准确性相关的MCV任务中表现优于ASR系统。</li>
<li>系统参数和损害影响人体表现和用户体验。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13724">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_Speech/2509.13724v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_Speech/2509.13724v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_Speech/2509.13724v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_Speech/2509.13724v1/page_2_1.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Invisible-Ears-at-Your-Fingertips-Acoustic-Eavesdropping-via-Mouse-Sensors"><a href="#Invisible-Ears-at-Your-Fingertips-Acoustic-Eavesdropping-via-Mouse-Sensors" class="headerlink" title="Invisible Ears at Your Fingertips: Acoustic Eavesdropping via Mouse   Sensors"></a>Invisible Ears at Your Fingertips: Acoustic Eavesdropping via Mouse   Sensors</h2><p><strong>Authors:Mohamad Fakih, Rahul Dharmaji, Youssef Mahmoud, Halima Bouzidi, Mohammad Abdullah Al Faruque</strong></p>
<p>Modern optical mouse sensors, with their advanced precision and high responsiveness, possess an often overlooked vulnerability: they can be exploited for side-channel attacks. This paper introduces Mic-E-Mouse, the first-ever side-channel attack that targets high-performance optical mouse sensors to covertly eavesdrop on users. We demonstrate that audio signals can induce subtle surface vibrations detectable by a mouse’s optical sensor. Remarkably, user-space software on popular operating systems can collect and broadcast this sensitive side channel, granting attackers access to raw mouse data without requiring direct system-level permissions. Initially, the vibration signals extracted from mouse data are of poor quality due to non-uniform sampling, a non-linear frequency response, and significant quantization. To overcome these limitations, Mic-E-Mouse employs a sophisticated end-to-end data filtering pipeline that combines Wiener filtering, resampling corrections, and an innovative encoder-only spectrogram neural filtering technique. We evaluate the attack’s efficacy across diverse conditions, including speaking volume, mouse polling rate and DPI, surface materials, speaker languages, and environmental noise. In controlled environments, Mic-E-Mouse improves the signal-to-noise ratio (SNR) by up to +19 dB for speech reconstruction. Furthermore, our results demonstrate a speech recognition accuracy of roughly 42% to 61% on the AudioMNIST and VCTK datasets. All our code and datasets are publicly accessible on <a target="_blank" rel="noopener" href="https://sites.google.com/view/mic-e-mouse">https://sites.google.com/view/mic-e-mouse</a>. </p>
<blockquote>
<p>现代光学鼠标传感器具有高精度和高响应性的特性，但往往会被忽略其存在的漏洞：它们可能被用于侧信道攻击。本文介绍了Mic-E-Mouse，这是一种针对高性能光学鼠标传感器的新型侧信道攻击，可以隐秘地窃听用户信息。我们证明了音频信号会引发鼠标光学传感器可检测到的细微表面振动。值得注意的是，流行操作系统上的用户空间软件可以收集和广播这种敏感的侧信道信息，使攻击者无需获得系统级权限即可访问原始鼠标数据。由于采样不均匀、频率响应非线性以及量化显著，最初从鼠标数据中提取的振动信号质量较差。为了克服这些局限性，Mic-E-Mouse采用了一种先进的端到端数据过滤管道，结合了Wiener过滤、重采样校正以及创新的仅编码器光谱神经过滤技术。我们评估了攻击在不同条件下的有效性，包括说话音量、鼠标轮询率和DPI、表面材料、说话人的语言和环境噪音。在受控环境中，Mic-E-Mouse提高了信号噪声比（SNR），语音重建提高了高达+19分贝。此外，我们的结果在AudioMNIST和VCTK数据集上达到了约42%至61%的语音识别准确率。所有代码和数据集均可在<a target="_blank" rel="noopener" href="https://sites.google.com/view/mic-e-mouse%E4%B8%8A%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://sites.google.com/view/mic-e-mouse上公开访问。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13581v1">PDF</a> Appearing in the Annual Computer Security Applications Conference   (ACSAC 2025)</p>
<p><strong>摘要</strong></p>
<p>现代高性能光学鼠标传感器存在一种常被忽视的漏洞：它们可能受到侧信道攻击的影响。本文介绍了Mic-E-Mouse，这是一种针对高性能光学鼠标传感器的新型侧信道攻击，可以隐秘地窃听用户信息。我们证明，音频信号会诱导鼠标光学传感器可检测到的微小表面振动。值得注意的是，流行操作系统上的用户空间软件可以收集和广播这种敏感侧信道信息，使攻击者无需获得系统级权限即可访问原始鼠标数据。为解决从鼠标数据中提取的振动信号质量差的问题（如非均匀采样、非线性频率响应和显著量化），Mic-E-Mouse采用了一种先进的端到端数据过滤管道，结合了维纳滤波、重采样校正和创新的仅编码器光谱神经过滤技术。我们在不同条件下评估了攻击的有效性，包括讲话音量、鼠标轮询率和DPI、表面材料、讲话语言和环境噪音。在控制环境下，Mic-E-Mouse将信噪比（SNR）提高了高达+19分贝，用于语音重建。此外，我们的结果在AudioMNIST和VCTK数据集上达到了约42%至61%的语音识别准确率。我们的所有代码和数据集均可在<a target="_blank" rel="noopener" href="https://sites.google.com/view/mic-e-mouse%E4%B8%8A%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://sites.google.com/view/mic-e-mouse上公开访问。</a></p>
<p><strong>要点掌握</strong></p>
<ol>
<li>现代光学鼠标传感器存在侧信道攻击漏洞，可通过Mic-E-Mouse进行隐秘窃听。</li>
<li>音频信号能诱导光学鼠标传感器检测微小表面振动。</li>
<li>用户空间软件能收集和广播鼠标的侧信道信息，使攻击者无需系统级权限即可获取数据。</li>
<li>Mic-E-Mouse采用先进的数据过滤管道处理振动信号的质量问题。</li>
<li>该攻击在多种条件下有效，包括讲话音量、鼠标参数、表面材料、语言和环境噪音。</li>
<li>在控制环境下，Mic-E-Mouse能提高语音重建的信噪比达+19分贝。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13581">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_Speech/2509.13581v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_Speech/2509.13581v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_Speech/2509.13581v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_Speech/2509.13581v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_Speech/2509.13581v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_Speech/2509.13581v1/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_Speech/2509.13581v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Enhancing-Speaker-Independent-Dysarthric-Speech-Severity-Classification-with-DSSCNet-and-Cross-Corpus-Adaptation"><a href="#Enhancing-Speaker-Independent-Dysarthric-Speech-Severity-Classification-with-DSSCNet-and-Cross-Corpus-Adaptation" class="headerlink" title="Enhancing Speaker-Independent Dysarthric Speech Severity Classification   with DSSCNet and Cross-Corpus Adaptation"></a>Enhancing Speaker-Independent Dysarthric Speech Severity Classification   with DSSCNet and Cross-Corpus Adaptation</h2><p><strong>Authors:Arnab Kumar Roy, Hemant Kumar Kathania, Paban Sapkota</strong></p>
<p>Dysarthric speech severity classification is crucial for objective clinical assessment and progress monitoring in individuals with motor speech disorders. Although prior methods have addressed this task, achieving robust generalization in speaker-independent (SID) scenarios remains challenging. This work introduces DSSCNet, a novel deep neural architecture that combines Convolutional, Squeeze-Excitation (SE), and Residual network, helping it extract discriminative representations of dysarthric speech from mel spectrograms. The addition of SE block selectively focuses on the important features of the dysarthric speech, thereby minimizing loss and enhancing overall model performance. We also propose a cross-corpus fine-tuning framework for severity classification, adapted from detection-based transfer learning approaches. DSSCNet is evaluated on two benchmark dysarthric speech corpora: TORGO and UA-Speech under speaker-independent evaluation protocols: One-Speaker-Per-Severity (OSPS) and Leave-One-Speaker-Out (LOSO) protocols. DSSCNet achieves accuracies of 56.84% and 62.62% under OSPS and 63.47% and 64.18% under LOSO setting on TORGO and UA-Speech respectively outperforming existing state-of-the-art methods. Upon fine-tuning, the performance improves substantially, with DSSCNet achieving up to 75.80% accuracy on TORGO and 68.25% on UA-Speech in OSPS, and up to 77.76% and 79.44%, respectively, in LOSO. These results demonstrate the effectiveness and generalizability of DSSCNet for fine-grained severity classification across diverse dysarthric speech datasets. </p>
<blockquote>
<p>语言障碍语音严重性分类对于患有运动性语言障碍者的客观临床评估和进度监测至关重要。尽管之前的方法已经解决了此任务，但在独立于说话人的场景中实现稳健的泛化仍然具有挑战性。本研究介绍了DSSCNet，这是一种新型深度神经网络架构，结合了卷积、挤压激励（SE）和残差网络，有助于从梅尔频谱图中提取语言障碍语音的判别表示。SE块的添加有选择地关注语言障碍语音的重要特征，从而减小损失并提高整体模型性能。我们还提出了基于检测迁移学习方法的严重性分类跨语料微调框架。DSSCNet在两个基准语言障碍语音语料库TORGO和UA-Speech上进行了评估，采用独立于说话人的评估协议：每严重性一位说话人（OSPS）和留一说话人出（LOSO）协议。DSSCNet在TORGO和UA-Speech上的OSPS协议下分别达到56.84%和62.62%的准确率，在LOSO设置下分别达到63.47%和64.18%，超过了现有最先进的方法。经过微调后，性能大大提高，在TORGO的OSPS中，DSSCNet的准确率高达75.80%，在UA-Speech中高达68.25%，而在LOSO中分别高达77.76%和79.44%。这些结果证明了DSSCNet在跨多种语言障碍语音数据集上进行精细严重性分类的有效性和泛化能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13442v1">PDF</a> Speaker-independent experiments on classification of dysarthric   speech severity</p>
<p><strong>摘要</strong></p>
<p>本文研究了运动性言语障碍患者的语言障碍程度分类问题。尽管已有方法对此进行了探索，但在独立于说话人的场景下实现稳健的泛化仍然具有挑战性。本研究提出了一种新型的深度神经网络架构DSSCNet，它结合了卷积神经网络、挤压激励网络和残差网络，能够从梅尔频谱图中提取运动性言语障碍的判别特征表示。挤压激励模块能够有选择地关注运动性言语障碍的重要特征，从而减少损失并提高模型的整体性能。同时，本文提出了基于检测迁移学习的跨语料库微调框架，用于严重程度分类。在TORGO和UA-Speech两个基准运动性言语障碍语料库中，对DSSCNet进行了说话人独立评估协议下的评估，包括单说话人单严重程度和留一说话人出协议。DSSCNet在TORGO和UA-Speech上的准确率分别在OPS和LOSO设置下达到56.84%和62.62%，以及63.47%和64.18%，超过了现有最新方法。经过微调后，DSSCNet的性能得到显著提高，在TORGO和UA-Speech上的OPS准确率分别提高到75.80%和68.25%，LOSO中的准确率分别提高到77.76%和79.44%。这些结果表明，DSSCNet在精细粒度的严重程度分类中对于不同的运动性言语障碍数据集具有良好的有效性和泛化能力。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>DSSCNet结合了卷积神经网络、挤压激励网络和残差网络，能有效提取运动性言语障碍的判别特征表示。</li>
<li>挤压激励模块能关注运动性言语障碍的重要特征，提高模型性能。</li>
<li>提出了基于检测迁移学习的跨语料库微调框架，用于严重程度分类。</li>
<li>DSSCNet在多个基准数据集上表现优于现有方法，显示其有效性和泛化能力。</li>
<li>通过微调，DSSCNet的性能得到显著提高。</li>
<li>研究结果对于开发更精确的语音识别和运动性言语障碍诊断工具具有重要意义。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13442">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_Speech/2509.13442v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_Speech/2509.13442v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_Speech/2509.13442v1/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_Speech/2509.13442v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_Speech/2509.13442v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="CAMEO-Collection-of-Multilingual-Emotional-Speech-Corpora"><a href="#CAMEO-Collection-of-Multilingual-Emotional-Speech-Corpora" class="headerlink" title="CAMEO: Collection of Multilingual Emotional Speech Corpora"></a>CAMEO: Collection of Multilingual Emotional Speech Corpora</h2><p><strong>Authors:Iwona Christop, Maciej Czajka</strong></p>
<p>This paper presents CAMEO – a curated collection of multilingual emotional speech datasets designed to facilitate research in emotion recognition and other speech-related tasks. The main objectives were to ensure easy access to the data, to allow reproducibility of the results, and to provide a standardized benchmark for evaluating speech emotion recognition (SER) systems across different emotional states and languages. The paper describes the dataset selection criteria, the curation and normalization process, and provides performance results for several models. The collection, along with metadata, and a leaderboard, is publicly available via the Hugging Face platform. </p>
<blockquote>
<p>本文介绍了CAMEO——一个精选的多语言情感语音数据集合集，旨在促进情感识别和其他相关语音任务的研究。主要目标是确保数据的轻松访问，以便结果的复现性，以及为评估不同情感状态和语言的语音情感识别（SER）系统提供一个标准化的基准。本文描述了数据集的选择标准、编纂和归一化过程，并为几个模型提供了性能结果。该集合与元数据以及排行榜可通过Hugging Face平台公开访问。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11051v2">PDF</a> Under review at ICASSP</p>
<p><strong>Summary</strong></p>
<p>本文介绍了CAMEO——一个精选的多语言情感语音数据集合集，旨在促进情感识别和其他语音相关任务的研究。其主要目标包括确保数据的轻松访问、允许结果的可重复性，以及为不同情感状态和语言的语音情感识别（SER）系统提供一个标准化的基准测试。本文描述了数据集的选择标准、整理和标准化流程，并提供了多个模型的表现结果。该合集与元数据以及排行榜已通过Hugging Face平台公开提供。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CAMEO是一个多语言情感语音数据集合集，用于促进情感识别研究。</li>
<li>数据集的主要目标是确保数据轻松访问、结果可重复，并为语音情感识别系统提供标准化基准测试。</li>
<li>数据集的选择标准、整理和标准化流程在文中得到详细描述。</li>
<li>提供了多个模型在CAMEO上的表现结果。</li>
<li>该数据集合集与元数据已公开在Hugging Face平台上。</li>
<li>有一个公开的排行榜，可以比较不同系统的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11051">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_Speech/2505.11051v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_Speech/2505.11051v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_Speech/2505.11051v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_Speech/2505.11051v2/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_Speech/2505.11051v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_Speech/2505.11051v2/page_3_1.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="COMI-LINGUA-Expert-Annotated-Large-Scale-Dataset-for-Multitask-NLP-in-Hindi-English-Code-Mixing"><a href="#COMI-LINGUA-Expert-Annotated-Large-Scale-Dataset-for-Multitask-NLP-in-Hindi-English-Code-Mixing" class="headerlink" title="COMI-LINGUA: Expert Annotated Large-Scale Dataset for Multitask NLP in   Hindi-English Code-Mixing"></a>COMI-LINGUA: Expert Annotated Large-Scale Dataset for Multitask NLP in   Hindi-English Code-Mixing</h2><p><strong>Authors:Rajvee Sheth, Himanshu Beniwal, Mayank Singh</strong></p>
<p>We introduce COMI-LINGUA, the largest manually annotated Hindi-English code-mixed dataset, comprising 125K+ high-quality instances across five core NLP tasks: Matrix Language Identification, Token-level Language Identification, Part-Of-Speech Tagging, Named Entity Recognition, and Machine Translation. Each instance is annotated by three bilingual annotators, yielding over 376K expert annotations with strong inter-annotator agreement (Fleiss’ Kappa $\geq$ 0.81). The rigorously preprocessed and filtered dataset covers both Devanagari and Roman scripts and spans diverse domains, ensuring real-world linguistic coverage. Evaluation reveals that closed-source LLMs significantly outperform traditional tools and open-source models in zero-shot settings. Notably, one-shot prompting consistently boosts performance across tasks, especially in structure-sensitive predictions like POS and NER. Fine-tuning state-of-the-art LLMs on COMI-LINGUA demonstrates substantial improvements, achieving up to 95.25 F1 in NER, 98.77 F1 in MLI, and competitive MT performance, setting new benchmarks for Hinglish code-mixed text. COMI-LINGUA is publicly available at this URL: <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/LingoIITGN/COMI-LINGUA">https://huggingface.co/datasets/LingoIITGN/COMI-LINGUA</a>. </p>
<blockquote>
<p>我们介绍COMI-LINGUA，这是最大的手动标注的印地语-英语混合代码数据集，包含超过12.5万个高质量实例，涉及五个核心NLP任务：矩阵语言识别、标记级语言识别、词性标注、命名实体识别和机器翻译。每个实例均由三名双语注释者进行标注，产生了超过37.6万个专家注释，注释者之间的强一致性（Fleiss’ Kappa $\geq$ 0.81）。该数据集经过严格预处理和过滤，涵盖Devanagari和Roman两种脚本，涉及多个领域，确保现实世界的语言覆盖。评估表明，封闭源代码的大型语言模型在零样本设置中显著优于传统工具和开源模型。值得注意的是，一次性提示任务始终提高了性能，特别是在结构敏感的预测中，如POS和NER。在COMI-LINGUA上对最新的大型语言模型进行微调，实现了显著的性能提升，命名实体识别的F1得分高达95.25，矩阵语言识别的F1得分高达98.77，机器翻译性能具有竞争力，为印地语混合文本树立了新基准。COMI-LINGUA可在以下网址公开访问：<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/LingoIITGN/COMI-LINGUA%E3%80%82">https://huggingface.co/datasets/LingoIITGN/COMI-LINGUA。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.21670v3">PDF</a> </p>
<p><strong>Summary</strong>：</p>
<p>COMI-LINGUA是最大规模的手动标注印地语-英语混合代码数据集，包含超过12.5万个高质量实例，涵盖五大核心NLP任务。该数据集由三位双语标注者进行标注，具有强大的标注者间一致性。数据集经过严格预处理和筛选，覆盖多种领域和脚本，确保真实世界语言覆盖度。评估显示，封闭源代码的大型语言模型在零样本设置下显著优于传统工具和开源模型。特别是一步提示法在各种任务上表现一致，特别是在结构敏感的预测如POS和NER上效果显著。对最先进的大型语言模型在COMI-LINGUA上进行微调，实现了显著的改进，在NER上达到95.25 F1，MLI达到98.77 F1，并且在机器翻译性能上具有竞争力，为印地语混合文本设定了新的基准。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>COMI-LINGUA是最大规模的手动标注印地语-英语混合代码数据集。</li>
<li>数据集涵盖五大核心NLP任务，包括矩阵语言识别、令牌级语言识别、词性标注、实体命名识别和机器翻译。</li>
<li>数据集由三位双语标注者进行标注，具有强大的标注者间一致性（Fleiss’ Kappa ≥ 0.81）。</li>
<li>数据集覆盖多种领域和脚本，确保真实世界语言覆盖度。</li>
<li>封闭源代码的大型语言模型在零样本设置下显著优于传统工具和开源模型。</li>
<li>一步提示法在各种任务上表现一致，特别是在结构敏感的预测如POS和NER上效果显著。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.21670">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_Speech/2503.21670v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_Speech/2503.21670v3/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_Speech/2503.21670v3/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_Speech/2503.21670v3/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_Speech/2503.21670v3/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_Speech/2503.21670v3/page_5_1.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-19/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-19/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-19/GAN/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_GAN/2410.19794v4/page_0_0.jpg" class="responsive-img" alt="GAN">
                        
                        <span class="card-title">GAN</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            GAN 方向最新论文已更新，请持续关注 Update in 2025-09-19  DiffGAN A Test Generation Approach for Differential Testing of Deep   Neural Networks for Image Analysis
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-19
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/GAN/" class="post-category">
                                    GAN
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/GAN/">
                        <span class="chip bg-color">GAN</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-19/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F_Breast%20Ultrasound/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_医学影像_Breast Ultrasound/2509.13508v1/page_3_0.jpg" class="responsive-img" alt="医学影像/Breast Ultrasound">
                        
                        <span class="card-title">医学影像/Breast Ultrasound</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学影像/Breast Ultrasound 方向最新论文已更新，请持续关注 Update in 2025-09-19  FunKAN Functional Kolmogorov-Arnold Network for Medical Image   Enhancement and Segmentation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-19
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/" class="post-category">
                                    医学影像/Breast Ultrasound
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/">
                        <span class="chip bg-color">医学影像/Breast Ultrasound</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">29474.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
