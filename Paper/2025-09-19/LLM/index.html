<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-19  Apertus Democratizing Open and Compliant LLMs for Global Language   Environments">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_LLM/2509.13905v1/page_2_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-19
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-07
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    18.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    74 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-09-19-æ›´æ–°"><a href="#2025-09-19-æ›´æ–°" class="headerlink" title="2025-09-19 æ›´æ–°"></a>2025-09-19 æ›´æ–°</h1><h2 id="Apertus-Democratizing-Open-and-Compliant-LLMs-for-Global-Language-Environments"><a href="#Apertus-Democratizing-Open-and-Compliant-LLMs-for-Global-Language-Environments" class="headerlink" title="Apertus: Democratizing Open and Compliant LLMs for Global Language   Environments"></a>Apertus: Democratizing Open and Compliant LLMs for Global Language   Environments</h2><p><strong>Authors:Alejandro HernÃ¡ndez-Cano, Alexander HÃ¤gele, Allen Hao Huang, Angelika Romanou, Antoni-Joan Solergibert, Barna Pasztor, Bettina Messmer, Dhia Garbaya, Eduard Frank Äurech, Ido Hakimi, Juan GarcÃ­a Giraldo, Mete Ismayilzada, Negar Foroutan, Skander Moalla, Tiancheng Chen, Vinko SabolÄec, Yixuan Xu, Michael Aerni, Badr AlKhamissi, Ines Altemir Marinas, Mohammad Hossein Amani, Matin Ansaripour, Ilia Badanin, Harold Benoit, Emanuela Boros, Nicholas Browning, Fabian BÃ¶sch, Maximilian BÃ¶ther, Niklas Canova, Camille Challier, Clement Charmillot, Jonathan Coles, Jan Deriu, Arnout Devos, Lukas Drescher, Daniil Dzenhaliou, Maud Ehrmann, Dongyang Fan, Simin Fan, Silin Gao, Miguel Gila, MarÃ­a Grandury, Diba Hashemi, Alexander Hoyle, Jiaming Jiang, Mark Klein, Andrei Kucharavy, Anastasiia Kucherenko, Frederike LÃ¼beck, Roman Machacek, Theofilos Manitaras, Andreas Marfurt, Kyle Matoba, Simon Matrenok, Henrique MendoncÃ§a, Fawzi Roberto Mohamed, Syrielle Montariol, Luca Mouchel, Sven Najem-Meyer, Jingwei Ni, Gennaro Oliva, Matteo Pagliardini, Elia Palme, Andrei Panferov, LÃ©o Paoletti, Marco Passerini, Ivan Pavlov, Auguste Poiroux, Kaustubh Ponkshe, Nathan Ranchin, Javi Rando, Mathieu Sauser, Jakhongir Saydaliev, Muhammad Ali Sayfiddinov, Marian Schneider, Stefano Schuppli, Marco Scialanga, Andrei Semenov, Kumar Shridhar, Raghav Singhal, Anna Sotnikova, Alexander Sternfeld, Ayush Kumar Tarun, Paul Teiletche, Jannis Vamvas, Xiaozhe Yao, Hao Zhao Alexander Ilic, Ana Klimovic, Andreas Krause, Caglar Gulcehre, David Rosenthal, Elliott Ash, Florian TramÃ¨r, Joost VandeVondele, Livio Veraldi, Martin Rajman, Thomas Schulthess, Torsten Hoefler, Antoine Bosselut, Martin Jaggi, Imanol Schlag</strong></p>
<p>We present Apertus, a fully open suite of large language models (LLMs) designed to address two systemic shortcomings in todayâ€™s open model ecosystem: data compliance and multilingual representation. Unlike many prior models that release weights without reproducible data pipelines or regard for content-owner rights, Apertus models are pretrained exclusively on openly available data, retroactively respecting robots.txt exclusions and filtering for non-permissive, toxic, and personally identifiable content. To mitigate risks of memorization, we adopt the Goldfish objective during pretraining, strongly suppressing verbatim recall of data while retaining downstream task performance. The Apertus models also expand multilingual coverage, training on 15T tokens from over 1800 languages, with ~40% of pretraining data allocated to non-English content. Released at 8B and 70B scales, Apertus approaches state-of-the-art results among fully open models on multilingual benchmarks, rivalling or surpassing open-weight counterparts. Beyond model weights, we release all scientific artifacts from our development cycle with a permissive license, including data preparation scripts, checkpoints, evaluation suites, and training code, enabling transparent audit and extension. </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºApertusï¼Œè¿™æ˜¯ä¸€å¥—å®Œå…¨å¼€æ”¾çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¥—ä»¶ï¼Œæ—¨åœ¨è§£å†³å½“å‰å¼€æ”¾æ¨¡å‹ç”Ÿæ€ç³»ç»Ÿä¸­çš„ä¸¤ä¸ªç³»ç»Ÿæ€§ç¼ºé™·ï¼šæ•°æ®åˆè§„æ€§å’Œå¤šè¯­è¨€è¡¨ç¤ºã€‚ä¸è®¸å¤šå…ˆå‰åªå‘å¸ƒæƒé‡æ•°æ®ç®¡é“ä¸å¯å¤ç°æˆ–å¿½è§†å†…å®¹æ‰€æœ‰è€…æƒåˆ©çš„æ¨¡å‹ä¸åŒï¼ŒApertusæ¨¡å‹ä»…åœ¨å…¬å¼€å¯ç”¨çš„æ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå°Šé‡robots.txtçš„æ’é™¤è§„å®šï¼Œå¹¶è¿‡æ»¤æ‰ä¸å…è®¸çš„ã€æœ‰æ¯’çš„å’Œå¯è¯†åˆ«ä¸ªäººèº«ä»½çš„å†…å®¹ã€‚ä¸ºäº†å‡è½»è®°å¿†é£é™©ï¼Œæˆ‘ä»¬åœ¨é¢„è®­ç»ƒè¿‡ç¨‹ä¸­é‡‡ç”¨äº†é‡‘é±¼è®°å¿†ç›®æ ‡ï¼Œåœ¨ä¿ç•™ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½çš„åŒæ—¶ï¼Œå¼ºçƒˆæŠ‘åˆ¶å¯¹æ•°æ®çš„é€å­—å›å¿†ã€‚Apertusæ¨¡å‹è¿˜æ‰©å¤§äº†å¤šè¯­è¨€è¦†ç›–ï¼Œåœ¨æ¥è‡ªè¶…è¿‡1800ç§è¯­è¨€çš„15Tæ ‡è®°ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå…¶ä¸­çº¦40%çš„é¢„è®­ç»ƒæ•°æ®ä¸ºéè‹±è¯­å†…å®¹ã€‚ä»¥8Bå’Œ70Bçš„è§„æ¨¡å‘å¸ƒæ—¶ï¼ŒApertusåœ¨å¤šè¯­è¨€åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„å®Œå…¨å¼€æ”¾æ¨¡å‹çš„ç»“æœï¼Œå¯ä¸å…¬å¼€æƒé‡çš„åŒç±»æ¨¡å‹ç›¸æŠ—è¡¡æˆ–è¶…è¶Šã€‚é™¤äº†æ¨¡å‹æƒé‡å¤–ï¼Œæˆ‘ä»¬è¿˜ä»¥è®¸å¯çš„æ–¹å¼å‘å¸ƒå¼€å‘å‘¨æœŸä¸­çš„æ‰€æœ‰ç§‘å­¦æˆæœï¼ŒåŒ…æ‹¬æ•°æ®å‡†å¤‡è„šæœ¬ã€æ£€æŸ¥ç‚¹ã€è¯„ä¼°å¥—ä»¶å’Œè®­ç»ƒä»£ç ï¼Œä»¥å®ç°é€æ˜çš„å®¡æ ¸å’Œæ‰©å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.14233v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>Apertusæ˜¯ä¸€å¥—å®Œå…¨å¼€æ”¾çš„çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¥—ä»¶ï¼Œæ—¨åœ¨è§£å†³å½“å‰å¼€æ”¾æ¨¡å‹ç”Ÿæ€ç³»ç»Ÿä¸­çš„æ•°æ®åˆè§„æ€§å’Œå¤šè¯­è¨€è¡¨ç¤ºçš„ä¸¤ä¸ªç³»ç»Ÿæ€§çŸ­æ¿ã€‚è¯¥æ¨¡å‹åœ¨å…¬å¼€å¯ç”¨çš„æ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå°Šé‡robots.txtçš„æ’é™¤è®¾ç½®ï¼Œè¿‡æ»¤æ‰éè®¸å¯ã€æœ‰æ¯’å’Œå¯è¯†åˆ«çš„å†…å®¹ã€‚é‡‡ç”¨Goldfishç›®æ ‡è¿›è¡Œé¢„è®­ç»ƒï¼Œä»¥å‡è½»è®°å¿†é£é™©ï¼ŒåŒæ—¶ä¿ç•™ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ã€‚Apertusæ¨¡å‹è¿˜æ‰©å±•äº†å¤šè¯­è¨€è¦†ç›–èƒ½åŠ›ï¼Œåœ¨è¶…è¿‡1800ç§è¯­è¨€çš„15Tæ ‡è®°ä¸Šè¿›è¡Œè®­ç»ƒï¼Œçº¦40%çš„é¢„è®­ç»ƒæ•°æ®ä¸ºéè‹±è¯­å†…å®¹ã€‚ä»¥8Bå’Œ70Bçš„è§„æ¨¡å‘å¸ƒï¼ŒApertusåœ¨å¤šè¯­è¨€åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°æ¥è¿‘æˆ–è¶…è¶Šå…¶ä»–å®Œå…¨å¼€æ”¾çš„æ¨¡å‹ã€‚é™¤äº†æ¨¡å‹æƒé‡å¤–ï¼Œæˆ‘ä»¬è¿˜ä»¥è®¸å¯çš„å½¢å¼å‘å¸ƒäº†å¼€å‘å‘¨æœŸä¸­çš„æ‰€æœ‰ç§‘å­¦æˆæœï¼ŒåŒ…æ‹¬æ•°æ®å‡†å¤‡è„šæœ¬ã€æ£€æŸ¥ç‚¹ã€è¯„ä¼°å¥—ä»¶å’Œè®­ç»ƒä»£ç ï¼Œä»¥å®ç°é€æ˜çš„å®¡è®¡å’Œæ‰©å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Apertusæ˜¯ä¸€ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹å¥—ä»¶ï¼Œä¸“æ³¨äºè§£å†³æ•°æ®åˆè§„æ€§å’Œå¤šè¯­è¨€è¡¨ç¤ºé—®é¢˜ã€‚</li>
<li>è¯¥æ¨¡å‹åœ¨å…¬å¼€å¯ç”¨æ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå°Šé‡robots.txtè®¾ç½®ï¼Œå¹¶è¿‡æ»¤ä¸è‰¯å†…å®¹ã€‚</li>
<li>é‡‡ç”¨Goldfishç›®æ ‡è¿›è¡Œé¢„è®­ç»ƒï¼Œä»¥é™ä½è®°å¿†é£é™©å¹¶ä¿ç•™ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ã€‚</li>
<li>Apertusæ¨¡å‹æ‰©å±•äº†å¤šè¯­è¨€è¦†ç›–èƒ½åŠ›ï¼ŒåŒ…å«è¶…è¿‡1800ç§è¯­è¨€çš„æ•°æ®ã€‚</li>
<li>æ¨¡å‹åˆ†ä¸º8Bå’Œ70Bä¸¤ä¸ªè§„æ¨¡ç‰ˆæœ¬ã€‚</li>
<li>è¯¥æ¨¡å‹åœ¨å¤šè¯­è¨€åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°æ¥è¿‘æˆ–è¶…è¶Šå…¶ä»–å®Œå…¨å¼€æ”¾çš„æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14233">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_LLM/2509.14233v1/page_0_0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="NIRVANA-Structured-pruning-reimagined-for-large-language-models-compression"><a href="#NIRVANA-Structured-pruning-reimagined-for-large-language-models-compression" class="headerlink" title="NIRVANA: Structured pruning reimagined for large language models   compression"></a>NIRVANA: Structured pruning reimagined for large language models   compression</h2><p><strong>Authors:Mengting Ai, Tianxin Wei, Sirui Chen, Jingrui He</strong></p>
<p>Structured pruning of large language models (LLMs) offers substantial efficiency improvements by removing entire hidden units, yet current approaches often suffer from significant performance degradation, particularly in zero-shot settings, and necessitate costly recovery techniques such as supervised fine-tuning (SFT) or adapter insertion. To address these critical shortcomings, we introduce NIRVANA, a novel pruning method explicitly designed to balance immediate zero-shot accuracy preservation with robust fine-tuning capability. Leveraging a first-order saliency criterion derived from the Neural Tangent Kernel under Adam optimization dynamics, NIRVANA provides a theoretically grounded pruning strategy that respects essential model training behaviors. To further address the unique challenges posed by structured pruning, NIRVANA incorporates an adaptive sparsity allocation mechanism across layers and modules (attention vs. MLP), which adjusts pruning intensity between modules in a globally balanced manner. Additionally, to mitigate the high sensitivity of pruning decisions to calibration data quality, we propose a simple yet effective KL divergence-based calibration data selection strategy, ensuring more reliable and task-agnostic pruning outcomes. Comprehensive experiments conducted on Llama3, Qwen, and T5 models demonstrate that NIRVANA outperforms existing structured pruning methods under equivalent sparsity constraints, providing a theoretically sound and practical approach to LLM compression. The code is available at <a target="_blank" rel="noopener" href="https://github.com/iDEA-iSAIL-Lab-UIUC/NIRVANA">https://github.com/iDEA-iSAIL-Lab-UIUC/NIRVANA</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç»“æ„åŒ–å‰ªæé€šè¿‡ç§»é™¤æ•´ä¸ªéšè—å•å…ƒï¼Œå¯ä»¥æ˜¾è‘—æé«˜æ•ˆç‡ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ–¹æ³•å¸¸å¸¸é¢ä¸´æ€§èƒ½æ˜¾è‘—ä¸‹é™çš„é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹ï¼Œå¹¶ä¸”éœ€è¦æ˜‚è´µçš„æ¢å¤æŠ€æœ¯ï¼Œå¦‚ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æˆ–é€‚é…å™¨æ’å…¥ã€‚ä¸ºäº†è§£å†³è¿™äº›å…³é”®ç¼ºç‚¹ï¼Œæˆ‘ä»¬å¼•å…¥äº†NIRVANAï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹å‰ªææ–¹æ³•ï¼Œä¸“é—¨è®¾è®¡ç”¨äºå¹³è¡¡å³æ—¶é›¶æ ·æœ¬å‡†ç¡®æ€§ä¿æŒä¸ç¨³å¥å¾®è°ƒèƒ½åŠ›ã€‚NIRVANAåˆ©ç”¨åŸºäºAdamä¼˜åŒ–åŠ¨åŠ›å­¦ä¸‹çš„ç¥ç»åˆ‡çº¿æ ¸å¯¼å‡ºçš„ä¸€é˜¶æ˜¾è‘—æ€§æ ‡å‡†ï¼Œæä¾›äº†ä¸€ä¸ªç†è®ºæ‰å®çš„å‰ªæç­–ç•¥ï¼Œå°Šé‡æ¨¡å‹çš„åŸºæœ¬è®­ç»ƒè¡Œä¸ºã€‚ä¸ºäº†è¿›ä¸€æ­¥è§£å†³ç»“æ„åŒ–å‰ªæå¸¦æ¥çš„ç‹¬ç‰¹æŒ‘æˆ˜ï¼ŒNIRVANAåœ¨å±‚å’Œæ¨¡å—ï¼ˆæ³¨æ„åŠ›ä¸MLPï¼‰ä¹‹é—´èå…¥äº†ä¸€ç§è‡ªé€‚åº”ç¨€ç–åˆ†é…æœºåˆ¶ï¼Œä»¥å…¨å±€å¹³è¡¡çš„æ–¹å¼åœ¨æ¨¡å—ä¹‹é—´è°ƒæ•´å‰ªæå¼ºåº¦ã€‚æ­¤å¤–ï¼Œä¸ºäº†å‡è½»å‰ªæå†³ç­–å¯¹æ ¡å‡†æ•°æ®è´¨é‡çš„é«˜æ•æ„Ÿæ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„åŸºäºKLæ•£åº¦çš„æ ¡å‡†æ•°æ®é€‰æ‹©ç­–ç•¥ï¼Œç¡®ä¿æ›´å¯é ã€ä¸ä»»åŠ¡æ— å…³çš„å‰ªæç»“æœã€‚åœ¨Llama3ã€Qwenå’ŒT5æ¨¡å‹ä¸Šè¿›è¡Œçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒNIRVANAåœ¨ç­‰æ•ˆç¨€ç–çº¦æŸä¸‹ä¼˜äºç°æœ‰çš„ç»“æ„åŒ–å‰ªææ–¹æ³•ï¼Œä¸ºLLMå‹ç¼©æä¾›äº†ç†è®ºæ‰å®ä¸”å®ç”¨çš„æ–¹æ³•ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/iDEA-iSAIL-Lab-UIUC/NIRVANA%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/iDEA-iSAIL-Lab-UIUC/NIRVANAæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.14230v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ç»“æ„åŒ–å‰ªæå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯ä»¥æ˜¾è‘—æå‡æ•ˆç‡ï¼Œç§»é™¤éšè—å•å…ƒæ¥å®ç°æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¸ºè§£å†³ç°æœ‰æ–¹æ³•å­˜åœ¨çš„é›¶æ ·æœ¬è®¾ç½®æ€§èƒ½ä¸‹é™é—®é¢˜ï¼Œæå‡ºNIRVANAæ–¹æ³•ï¼Œæ—¨åœ¨å¹³è¡¡å³æ—¶é›¶æ ·æœ¬ç²¾åº¦ä¿æŒä¸ç²¾ç»†è°ƒèŠ‚èƒ½åŠ›ã€‚åˆ©ç”¨åŸºäºAdamä¼˜åŒ–åŠ¨åŠ›å­¦çš„ç¥ç»ç½‘ç»œåˆ‡çº¿æ ¸ä¸€é˜¶æ˜¾è‘—æ ‡å‡†ä½œä¸ºå‰ªæç­–ç•¥çš„ç†è®ºåŸºç¡€ã€‚è¿›ä¸€æ­¥åº”å¯¹ç»“æ„åŒ–å‰ªæå¸¦æ¥çš„æŒ‘æˆ˜ï¼ŒNIRVANAé‡‡ç”¨è‡ªé€‚åº”ç¨€ç–åˆ†é…æœºåˆ¶ï¼Œè·¨å±‚ä¸æ¨¡å—è°ƒæ•´å‰ªæå¼ºåº¦ã€‚åŒæ—¶ï¼Œä¸ºç¼“è§£æ ¡å‡†æ•°æ®è´¨é‡å¯¹å‰ªæå†³ç­–çš„å½±å“ï¼Œæå‡ºåŸºäºKLæ•£åº¦çš„æ ¡å‡†æ•°æ®é€‰æ‹©ç­–ç•¥ï¼Œç¡®ä¿æ›´å¯é çš„ä»»åŠ¡æ— å…³å‰ªæç»“æœã€‚å®éªŒè¡¨æ˜ï¼ŒNIRVANAåœ¨ç›¸åŒç¨€ç–çº¦æŸä¸‹ä¼˜äºç°æœ‰ç»“æ„åŒ–å‰ªææ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ç»“æ„åŒ–å‰ªæå¯ä»¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ•ˆç‡ã€‚</li>
<li>å½“å‰å‰ªææ–¹æ³•é¢ä¸´é›¶æ ·æœ¬è®¾ç½®æ€§èƒ½ä¸‹é™çš„é—®é¢˜ã€‚</li>
<li>NIRVANAæ–¹æ³•æ—¨åœ¨å¹³è¡¡å³æ—¶é›¶æ ·æœ¬ç²¾åº¦ä¿æŒä¸ç²¾ç»†è°ƒèŠ‚èƒ½åŠ›ã€‚</li>
<li>åŸºäºAdamä¼˜åŒ–åŠ¨åŠ›å­¦çš„ç¥ç»ç½‘ç»œåˆ‡çº¿æ ¸ä½œä¸ºNIRVANAçš„ç†è®ºåŸºç¡€ã€‚</li>
<li>NIRVANAé‡‡ç”¨è‡ªé€‚åº”ç¨€ç–åˆ†é…æœºåˆ¶ï¼Œé’ˆå¯¹å±‚ä¸æ¨¡å—è¿›è¡Œä¼˜åŒ–è°ƒæ•´ã€‚</li>
<li>åŸºäºKLæ•£åº¦çš„æ ¡å‡†æ•°æ®é€‰æ‹©ç­–ç•¥æé«˜äº†å‰ªæå†³ç­–çš„å¯é æ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14230">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_LLM/2509.14230v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_LLM/2509.14230v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Dense-Video-Understanding-with-Gated-Residual-Tokenization"><a href="#Dense-Video-Understanding-with-Gated-Residual-Tokenization" class="headerlink" title="Dense Video Understanding with Gated Residual Tokenization"></a>Dense Video Understanding with Gated Residual Tokenization</h2><p><strong>Authors:Haichao Zhang, Wenhao Chai, Shwai He, Ang Li, Yun Fu</strong></p>
<p>High temporal resolution is essential for capturing fine-grained details in video understanding. However, current video large language models (VLLMs) and benchmarks mostly rely on low-frame-rate sampling, such as uniform sampling or keyframe selection, discarding dense temporal information. This compromise avoids the high cost of tokenizing every frame, which otherwise leads to redundant computation and linear token growth as video length increases. While this trade-off works for slowly changing content, it fails for tasks like lecture comprehension, where information appears in nearly every frame and requires precise temporal alignment. To address this gap, we introduce Dense Video Understanding (DVU), which enables high-FPS video comprehension by reducing both tokenization time and token overhead. Existing benchmarks are also limited, as their QA pairs focus on coarse content changes. We therefore propose DIVE (Dense Information Video Evaluation), the first benchmark designed for dense temporal reasoning. To make DVU practical, we present Gated Residual Tokenization (GRT), a two-stage framework: (1) Motion-Compensated Inter-Gated Tokenization uses pixel-level motion estimation to skip static regions during tokenization, achieving sub-linear growth in token count and compute. (2) Semantic-Scene Intra-Tokenization Merging fuses tokens across static regions within a scene, further reducing redundancy while preserving dynamic semantics. Experiments on DIVE show that GRT outperforms larger VLLM baselines and scales positively with FPS. These results highlight the importance of dense temporal information and demonstrate that GRT enables efficient, scalable high-FPS video understanding. </p>
<blockquote>
<p>é«˜æ—¶é—´åˆ†è¾¨ç‡å¯¹äºæ•æ‰è§†é¢‘ç†è§£ä¸­çš„ç²¾ç»†ç»†èŠ‚è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œå½“å‰çš„è§†é¢‘å¤§è¯­è¨€æ¨¡å‹ï¼ˆVLLMsï¼‰å’ŒåŸºå‡†æµ‹è¯•ä¸»è¦ä¾èµ–äºä½å¸§ç‡é‡‡æ ·ï¼Œå¦‚å‡åŒ€é‡‡æ ·æˆ–å…³é”®å¸§é€‰æ‹©ï¼Œä¸¢å¼ƒäº†å¯†é›†çš„æ—¶é—´ä¿¡æ¯ã€‚è¿™ç§æŠ˜è¡·é¿å…äº†å¯¹æ•´ä¸ªå¸§è¿›è¡Œåˆ†è¯çš„é«˜æˆæœ¬ï¼Œå¦åˆ™ä¼šå¯¼è‡´å†—ä½™è®¡ç®—å’Œçº¿æ€§ä»¤ç‰Œå¢é•¿éšç€è§†é¢‘é•¿åº¦çš„å¢åŠ ã€‚è™½ç„¶è¿™ç§æƒè¡¡å¯¹äºç¼“æ…¢å˜åŒ–çš„å†…å®¹æœ‰æ•ˆï¼Œä½†å¯¹äºè¯¸å¦‚è®²åº§ç†è§£ä¹‹ç±»çš„ä»»åŠ¡å´å¤±è´¥äº†ï¼Œåœ¨è¿™äº›ä»»åŠ¡ä¸­ï¼Œä¿¡æ¯å‡ ä¹å‡ºç°åœ¨æ¯ä¸€å¸§ä¸­ï¼Œå¹¶è¦æ±‚ç²¾ç¡®çš„æ—¶é—´å¯¹é½ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¯†é›†è§†é¢‘ç†è§£ï¼ˆDVUï¼‰ï¼Œé€šè¿‡å‡å°‘åˆ†è¯æ—¶é—´å’Œä»¤ç‰Œå¼€é”€æ¥å®ç°é«˜å¸§ç‡è§†é¢‘ç†è§£ã€‚ç°æœ‰çš„åŸºå‡†æµ‹è¯•ä¹Ÿæœ‰é™ï¼Œå®ƒä»¬çš„é—®ç­”å¯¹ä¾§é‡äºç²—ç•¥çš„å†…å®¹å˜åŒ–ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†DIVEï¼ˆå¯†é›†ä¿¡æ¯è§†é¢‘è¯„ä¼°ï¼‰ï¼Œè¿™æ˜¯ä¸ºå¯†é›†æ—¶é—´æ¨ç†è®¾è®¡çš„ç¬¬ä¸€ä¸ªåŸºå‡†æµ‹è¯•ã€‚ä¸ºäº†ä½¿DVUå®ç”¨ï¼Œæˆ‘ä»¬æå‡ºäº†é—¨æ§æ®‹å·®åˆ†è¯ï¼ˆGRTï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µæ¡†æ¶ï¼šï¼ˆ1ï¼‰è¿åŠ¨è¡¥å¿é—´éš”åˆ†è¯ä½¿ç”¨åƒç´ çº§è¿åŠ¨ä¼°è®¡åœ¨åˆ†è¯è¿‡ç¨‹ä¸­è·³è¿‡é™æ€åŒºåŸŸï¼Œå®ç°ä»¤ç‰Œè®¡æ•°å’Œè®¡ç®—çš„æ¬¡çº¿æ€§å¢é•¿ã€‚ï¼ˆ2ï¼‰è¯­ä¹‰åœºæ™¯å†…éƒ¨åˆ†è¯åˆå¹¶åˆå¹¶åœºæ™¯å†…é™æ€åŒºåŸŸå†…çš„ä»¤ç‰Œï¼Œè¿›ä¸€æ­¥å‡å°‘å†—ä½™ï¼ŒåŒæ—¶ä¿ç•™åŠ¨æ€è¯­ä¹‰ã€‚åœ¨DIVEä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒGRTä¼˜äºè¾ƒå¤§çš„VLLMåŸºçº¿ï¼Œå¹¶ä¸”ä¸FPSæ­£ç›¸å…³ã€‚è¿™äº›ç»“æœçªå‡ºäº†å¯†é›†æ—¶é—´ä¿¡æ¯çš„é‡è¦æ€§ï¼Œå¹¶è¯æ˜GRTèƒ½å¤Ÿå®ç°é«˜æ•ˆã€å¯æ‰©å±•çš„é«˜å¸§ç‡è§†é¢‘ç†è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.14199v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>é«˜æ—¶é—´åˆ†è¾¨ç‡å¯¹äºè§†é¢‘ç†è§£ä¸­çš„ç²¾ç»†ç»†èŠ‚æ•æ‰è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œå½“å‰çš„è§†é¢‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆVLLMsï¼‰å’ŒåŸºå‡†æµ‹è¯•ä¸»è¦ä¾èµ–äºä½å¸§ç‡é‡‡æ ·ï¼Œå¦‚å‡åŒ€é‡‡æ ·æˆ–å…³é”®å¸§é€‰æ‹©ï¼Œè¿™ä¸¢å¼ƒäº†å¯†é›†çš„æ—¶é—´ä¿¡æ¯ã€‚è¿™ç§æƒè¡¡é¿å…äº†æ¯ä¸€å¸§æ ‡è®°çš„é«˜æˆæœ¬ï¼Œé¿å…äº†è§†é¢‘é•¿åº¦å¢åŠ æ—¶çš„å†—ä½™è®¡ç®—å’Œçº¿æ€§æ ‡è®°å¢é•¿ã€‚è™½ç„¶è¿™ç§æƒè¡¡å¯¹äºç¼“æ…¢å˜åŒ–çš„å†…å®¹æœ‰æ•ˆï¼Œä½†å¯¹äºè®²åº§ç†è§£ç­‰ä»»åŠ¡å´å¤±è´¥äº†ï¼Œåœ¨è¿™äº›ä»»åŠ¡ä¸­ï¼Œä¿¡æ¯å‡ ä¹å‡ºç°åœ¨æ¯ä¸€å¸§ä¸­ï¼Œéœ€è¦ç²¾ç¡®çš„æ—¶é—´å¯¹é½ã€‚ä¸ºè§£å†³è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¯†é›†è§†é¢‘ç†è§£ï¼ˆDVUï¼‰ï¼Œé€šè¿‡å‡å°‘æ ‡è®°æ—¶é—´å’Œå¼€é”€ï¼Œå®ç°äº†é«˜å¸§ç‡è§†é¢‘ç†è§£ã€‚ç°æœ‰çš„åŸºå‡†æµ‹è¯•ä¹Ÿæœ‰å±€é™æ€§ï¼Œä»–ä»¬çš„é—®ç­”å¯¹ä¾§é‡äºç²—ç•¥çš„å†…å®¹å˜åŒ–ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ç”¨äºå¯†é›†æ—¶é—´æ¨ç†çš„ç¬¬ä¸€ä¸ªåŸºå‡†æµ‹è¯•â€”â€”å¯†é›†ä¿¡æ¯è§†é¢‘è¯„ä¼°ï¼ˆDIVEï¼‰ã€‚ä¸ºäº†ä½¿DVUå®ç”¨ï¼Œæˆ‘ä»¬æå‡ºäº†é—¨æ§æ®‹å·®æ ‡è®°åŒ–ï¼ˆGRTï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µæ¡†æ¶ï¼šï¼ˆ1ï¼‰è¿åŠ¨è¡¥å¿é—¨æ§æ ‡è®°åŒ–ä½¿ç”¨åƒç´ çº§è¿åŠ¨ä¼°è®¡åœ¨æ ‡è®°è¿‡ç¨‹ä¸­è·³è¿‡é™æ€åŒºåŸŸï¼Œå®ç°æ ‡è®°è®¡æ•°å’Œè®¡ç®—çš„æ¬¡çº¿æ€§å¢é•¿ã€‚ï¼ˆ2ï¼‰è¯­ä¹‰åœºæ™¯å†…æ ‡è®°åˆå¹¶åˆå¹¶åœºæ™¯å†…é™æ€åŒºåŸŸå†…çš„æ ‡è®°ï¼Œè¿›ä¸€æ­¥å‡å°‘å†—ä½™ï¼ŒåŒæ—¶ä¿ç•™åŠ¨æ€è¯­ä¹‰ã€‚åœ¨DIVEä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒGRTä¼˜äºè¾ƒå¤§çš„VLLMåŸºçº¿ï¼Œå¹¶ä¸”éšç€å¸§ç‡çš„æé«˜è€Œç§¯æå‘å±•ã€‚è¿™äº›ç»“æœçªæ˜¾äº†å¯†é›†æ—¶é—´ä¿¡æ¯çš„é‡è¦æ€§ï¼Œå¹¶è¯æ˜GRTèƒ½å¤Ÿå®ç°é«˜æ•ˆã€å¯æ‰©å±•çš„é«˜å¸§ç‡è§†é¢‘ç†è§£ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>é«˜æ—¶é—´åˆ†è¾¨ç‡å¯¹äºæ•æ‰è§†é¢‘ç†è§£ä¸­çš„ç²¾ç»†ç»†èŠ‚è‡³å…³é‡è¦ã€‚</li>
<li>å½“å‰VLLMså’ŒåŸºå‡†æµ‹è¯•ä¸»è¦ä¾èµ–ä½å¸§ç‡é‡‡æ ·ï¼Œä¸¢å¤±äº†å¯†é›†çš„æ—¶é—´ä¿¡æ¯ã€‚</li>
<li>å¼•å…¥å¯†é›†è§†é¢‘ç†è§£ï¼ˆDVUï¼‰æ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œå®ç°é«˜å¸§ç‡è§†é¢‘ç†è§£ã€‚</li>
<li>ç°æœ‰åŸºå‡†æµ‹è¯•æ— æ³•å……åˆ†è¯„ä¼°å¯†é›†æ—¶é—´ä¿¡æ¯çš„é‡è¦æ€§ã€‚</li>
<li>æå‡ºäº†å¯†é›†ä¿¡æ¯è§†é¢‘è¯„ä¼°ï¼ˆDIVEï¼‰ä½œä¸ºé¦–ä¸ªç”¨äºå¯†é›†æ—¶é—´æ¨ç†çš„åŸºå‡†æµ‹è¯•ã€‚</li>
<li>å¼•å…¥é—¨æ§æ®‹å·®æ ‡è®°åŒ–ï¼ˆGRTï¼‰æ¡†æ¶ä»¥å®ç°é«˜æ•ˆã€å¯æ‰©å±•çš„è§†é¢‘ç†è§£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14199">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_LLM/2509.14199v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_LLM/2509.14199v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_LLM/2509.14199v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_LLM/2509.14199v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="TGPO-Tree-Guided-Preference-Optimization-for-Robust-Web-Agent-Reinforcement-Learning"><a href="#TGPO-Tree-Guided-Preference-Optimization-for-Robust-Web-Agent-Reinforcement-Learning" class="headerlink" title="TGPO: Tree-Guided Preference Optimization for Robust Web Agent   Reinforcement Learning"></a>TGPO: Tree-Guided Preference Optimization for Robust Web Agent   Reinforcement Learning</h2><p><strong>Authors:Ziyuan Chen, Zhenghui Zhao, Zhangye Han, Miancan Liu, Xianhang Ye, Yiqing Li, Hongbo Min, Jinkui Ren, Xiantao Zhang, Guitao Cao</strong></p>
<p>With the rapid advancement of large language models and vision-language models, employing large models as Web Agents has become essential for automated web interaction. However, training Web Agents with reinforcement learning faces critical challenges including credit assignment misallocation, prohibitively high annotation costs, and reward sparsity. To address these issues, we propose Tree-Guided Preference Optimization (TGPO), an offline reinforcement learning framework that proposes a tree-structured trajectory representation merging semantically identical states across trajectories to eliminate label conflicts. Our framework incorporates a Process Reward Model that automatically generates fine-grained rewards through subgoal progress, redundancy detection, and action verification. Additionally, a dynamic weighting mechanism prioritizes high-impact decision points during training. Experiments on Online-Mind2Web and our self-constructed C-WebShop datasets demonstrate that TGPO significantly outperforms existing methods, achieving higher success rates with fewer redundant steps. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹å’Œè§†è§‰è¯­è¨€æ¨¡å‹çš„å¿«é€Ÿå‘å±•ï¼Œå°†å¤§å‹æ¨¡å‹ä½œä¸ºWebä»£ç†è¿›è¡Œè‡ªåŠ¨åŒ–ç½‘ç»œäº¤äº’å·²æˆä¸ºå¿…ç„¶è¶‹åŠ¿ã€‚ç„¶è€Œï¼Œä½¿ç”¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒWebä»£ç†é¢ä¸´ç€å…³é”®æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ä¿¡ç”¨åˆ†é…ä¸å½“ã€æ ‡æ³¨æˆæœ¬è¿‡é«˜å’Œå¥–åŠ±ç¨€ç–ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†æ ‘å¯¼å‘åå¥½ä¼˜åŒ–ï¼ˆTGPOï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç¦»çº¿å¼ºåŒ–å­¦ä¹ æ¡†æ¶ã€‚å®ƒé‡‡ç”¨æ ‘çŠ¶è½¨è¿¹è¡¨ç¤ºï¼Œé€šè¿‡åˆå¹¶è½¨è¿¹ä¹‹é—´è¯­ä¹‰ç›¸åŒçš„çŠ¶æ€æ¥æ¶ˆé™¤æ ‡ç­¾å†²çªã€‚æˆ‘ä»¬çš„æ¡†æ¶ç»“åˆäº†è¿›ç¨‹å¥–åŠ±æ¨¡å‹ï¼Œè¯¥æ¨¡å‹é€šè¿‡å­ç›®æ ‡è¿›åº¦ã€å†—ä½™æ£€æµ‹å’Œè¡Œä¸ºéªŒè¯è‡ªåŠ¨ç”Ÿæˆç²¾ç»†å¥–åŠ±ã€‚æ­¤å¤–ï¼ŒåŠ¨æ€åŠ æƒæœºåˆ¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¼˜å…ˆè€ƒè™‘é«˜å½±å“åŠ›çš„å†³ç­–ç‚¹ã€‚åœ¨çº¿Mind2Webå’Œæˆ‘ä»¬è‡ªè¡Œæ„å»ºçš„C-WebShopæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒTGPOæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œåœ¨æ›´å°‘çš„å†—ä½™æ­¥éª¤ä¸‹å®ç°äº†æ›´é«˜çš„æˆåŠŸç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.14172v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹å’Œè§†è§‰è¯­è¨€æ¨¡å‹çš„å¿«é€Ÿå‘å±•ï¼Œä½¿ç”¨å¤§å‹æ¨¡å‹ä½œä¸ºWebä»£ç†è¿›è¡Œè‡ªåŠ¨åŒ–ç½‘ç»œäº¤äº’è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œä½¿ç”¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒWebä»£ç†é¢ä¸´å…³é”®æŒ‘æˆ˜ï¼Œå¦‚ä¿¡ç”¨åˆ†é…ä¸å½“ã€æ³¨é‡Šæˆæœ¬è¿‡é«˜å’Œå¥–åŠ±ç¨€ç–æ€§ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†æ ‘ç»“æ„å¼•å¯¼åå¥½ä¼˜åŒ–ï¼ˆTGPOï¼‰çš„ç¦»çº¿å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡åˆå¹¶è½¨è¿¹ä¸­çš„è¯­ä¹‰ç›¸åŒçŠ¶æ€æ¶ˆé™¤æ ‡ç­¾å†²çªã€‚æˆ‘ä»¬çš„æ¡†æ¶ç»“åˆäº†è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼Œé€šè¿‡å­ç›®æ ‡è¿›åº¦ã€å†—ä½™æ£€æµ‹ä»¥åŠåŠ¨ä½œéªŒè¯è‡ªåŠ¨ç”Ÿæˆç²¾ç»†å¥–åŠ±ã€‚æ­¤å¤–ï¼ŒåŠ¨æ€åŠ æƒæœºåˆ¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¼˜å…ˆå¤„ç†é«˜å½±å“åŠ›çš„å†³ç­–ç‚¹ã€‚åœ¨çº¿å®éªŒè¡¨æ˜ï¼ŒTGPOåœ¨Online-Mind2Webå’Œæˆ‘ä»¬è‡ªå»ºçš„C-WebShopæ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå®ç°äº†æ›´é«˜çš„æˆåŠŸç‡å¹¶å‡å°‘äº†å†—ä½™æ­¥éª¤ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹å’Œè§†è§‰è¯­è¨€æ¨¡å‹çš„å¿«é€Ÿå‘å±•ä½¿å¾—ä½¿ç”¨å¤§å‹æ¨¡å‹ä½œä¸ºWebä»£ç†è¿›è¡Œè‡ªåŠ¨åŒ–ç½‘ç»œäº¤äº’å˜å¾—è‡³å…³é‡è¦ã€‚</li>
<li>ä½¿ç”¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒWebä»£ç†é¢ä¸´æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ä¿¡ç”¨åˆ†é…ä¸å½“ã€æ³¨é‡Šæˆæœ¬è¿‡é«˜å’Œå¥–åŠ±ç¨€ç–æ€§ã€‚</li>
<li>æ ‘ç»“æ„å¼•å¯¼åå¥½ä¼˜åŒ–ï¼ˆTGPOï¼‰æ˜¯ä¸€ä¸ªç¦»çº¿å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>TGPOé€šè¿‡åˆå¹¶è¯­ä¹‰ç›¸åŒçŠ¶æ€æ¶ˆé™¤æ ‡ç­¾å†²çªï¼Œå¹¶æå‡ºäº†æ ‘ç»“æ„è½¨è¿¹è¡¨ç¤ºæ³•ã€‚</li>
<li>è¿‡ç¨‹å¥–åŠ±æ¨¡å‹å¯ä»¥è‡ªåŠ¨é€šè¿‡å­ç›®æ ‡è¿›åº¦ã€å†—ä½™æ£€æµ‹ä»¥åŠåŠ¨ä½œéªŒè¯ç”Ÿæˆç²¾ç»†å¥–åŠ±ã€‚</li>
<li>åŠ¨æ€åŠ æƒæœºåˆ¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¼˜å…ˆå¤„ç†é«˜å½±å“åŠ›çš„å†³ç­–ç‚¹ï¼Œä»¥æé«˜å­¦ä¹ æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14172">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_LLM/2509.14172v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_LLM/2509.14172v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_LLM/2509.14172v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_LLM/2509.14172v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_LLM/2509.14172v1/page_3_1.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="MARS2-2025-Challenge-on-Multimodal-Reasoning-Datasets-Methods-Results-Discussion-and-Outlook"><a href="#MARS2-2025-Challenge-on-Multimodal-Reasoning-Datasets-Methods-Results-Discussion-and-Outlook" class="headerlink" title="MARS2 2025 Challenge on Multimodal Reasoning: Datasets, Methods,   Results, Discussion, and Outlook"></a>MARS2 2025 Challenge on Multimodal Reasoning: Datasets, Methods,   Results, Discussion, and Outlook</h2><p><strong>Authors:Peng Xu, Shengwu Xiong, Jiajun Zhang, Yaxiong Chen, Bowen Zhou, Chen Change Loy, David A. Clifton, Kyoung Mu Lee, Luc Van Gool, Ruiming He, Ruilin Yao, Xinwei Long, Jirui Huang, Kai Tian, Sa Yang, Yihua Shao, Jin Feng, Yue Zhong, Jiakai Zhou, Cheng Tang, Tianyu Zou, Yifang Zhang, Junming Liang, Guoyou Li, Zhaoxiang Wang, Qiang Zhou, Yichen Zhao, Shili Xiong, Hyeongjin Nam, Jaerin Lee, Jaeyoung Chung, JoonKyu Park, Junghun Oh, Kanggeon Lee, Wooseok Lee, Juneyoung Ro, Turghun Osman, Can Hu, Chaoyang Liao, Cheng Chen, Chengcheng Han, Chenhao Qiu, Chong Peng, Cong Xu, Dailin Li, Feiyu Wang, Feng Gao, Guibo Zhu, Guopeng Tang, Haibo Lu, Han Fang, Han Qi, Hanxiao Wu, Haobo Cheng, Hongbo Sun, Hongyao Chen, Huayong Hu, Hui Li, Jiaheng Ma, Jiang Yu, Jianing Wang, Jie Yang, Jing He, Jinglin Zhou, Jingxuan Li, Josef Kittler, Lihao Zheng, Linnan Zhao, Mengxi Jia, Muyang Yan, Nguyen Thanh Thien, Pu Luo, Qi Li, Shien Song, Shijie Dong, Shuai Shao, Shutao Li, Taofeng Xue, Tianyang Xu, Tianyi Gao, Tingting Li, Wei Zhang, Weiyang Su, Xiaodong Dong, Xiao-Jun Wu, Xiaopeng Zhou, Xin Chen, Xin Wei, Xinyi You, Xudong Kang, Xujie Zhou, Xusheng Liu, Yanan Wang, Yanbin Huang, Yang Liu, Yang Yang, Yanglin Deng, Yashu Kang, Ye Yuan, Yi Wen, Yicen Tian, Yilin Tao, Yin Tang, Yipeng Lin, Yiqing Wang, Yiting Xi, Yongkang Yu, Yumei Li, Yuxin Qin, Yuying Chen, Yuzhe Cen, Zhaofan Zou, Zhaohong Liu, Zhehao Shen, Zhenglin Du, Zhengyang Li, Zhenni Huang, Zhenwei Shao, Zhilong Song, Zhiyong Feng, Zhiyu Wang, Zhou Yu, Ziang Li, Zihan Zhai, Zijian Zhang, Ziyang Peng, Ziyun Xiao, Zongshu Li</strong></p>
<p>This paper reviews the MARS2 2025 Challenge on Multimodal Reasoning. We aim to bring together different approaches in multimodal machine learning and LLMs via a large benchmark. We hope it better allows researchers to follow the state-of-the-art in this very dynamic area. Meanwhile, a growing number of testbeds have boosted the evolution of general-purpose large language models. Thus, this yearâ€™s MARS2 focuses on real-world and specialized scenarios to broaden the multimodal reasoning applications of MLLMs. Our organizing team released two tailored datasets Lens and AdsQA as test sets, which support general reasoning in 12 daily scenarios and domain-specific reasoning in advertisement videos, respectively. We evaluated 40+ baselines that include both generalist MLLMs and task-specific models, and opened up three competition tracks, i.e., Visual Grounding in Real-world Scenarios (VG-RS), Visual Question Answering with Spatial Awareness (VQA-SA), and Visual Reasoning in Creative Advertisement Videos (VR-Ads). Finally, 76 teams from the renowned academic and industrial institutions have registered and 40+ valid submissions (out of 1200+) have been included in our ranking lists. Our datasets, code sets (40+ baselines and 15+ participantsâ€™ methods), and rankings are publicly available on the MARS2 workshop website and our GitHub organization page <a target="_blank" rel="noopener" href="https://github.com/mars2workshop/">https://github.com/mars2workshop/</a>, where our updates and announcements of upcoming events will be continuously provided. </p>
<blockquote>
<p>æœ¬æ–‡å›é¡¾äº†å…³äºå¤šæ¨¡æ€æ¨ç†çš„MARS2 2025æŒ‘æˆ˜èµ›ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯é€šè¿‡å¤§å‹åŸºå‡†æµ‹è¯•ï¼Œæ±‡é›†ä¸åŒæ¨¡æ€çš„æœºå™¨å­¦ä¹ å’Œå¤§å‹è¯­è¨€æ¨¡å‹çš„å¤šæ¨¡æ€æ–¹æ³•ã€‚æˆ‘ä»¬å¸Œæœ›è¿™èƒ½æ›´å¥½åœ°è®©ç ”ç©¶äººå‘˜åœ¨è¿™ä¸ªåŠ¨æ€é¢†åŸŸè¿½è¸ªæœ€æ–°çš„ç ”ç©¶å‰æ²¿ã€‚ä¸æ­¤åŒæ—¶ï¼Œè¶Šæ¥è¶Šå¤šçš„æµ‹è¯•å¹³å°æ¨åŠ¨äº†é€šç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„è¿›åŒ–ã€‚å› æ­¤ï¼Œä»Šå¹´çš„MARS2ä¸“æ³¨äºç°å®å’Œç‰¹å®šåœºæ™¯ï¼Œä»¥æ‹“å®½å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹çš„å¤šæ¨¡æ€æ¨ç†åº”ç”¨ã€‚æˆ‘ä»¬å›¢é˜Ÿå‘å¸ƒäº†ä¸¤ä¸ªå®šåˆ¶æ•°æ®é›†Lenså’ŒAdsQAä½œä¸ºæµ‹è¯•é›†ï¼Œå®ƒä»¬åˆ†åˆ«æ”¯æŒåœ¨12ä¸ªæ—¥å¸¸åœºæ™¯ä¸­çš„é€šç”¨æ¨ç†å’Œåœ¨å¹¿å‘Šè§†é¢‘ä¸­çš„ç‰¹å®šé¢†åŸŸæ¨ç†ã€‚æˆ‘ä»¬è¯„ä¼°äº†è¶…è¿‡40ä¸ªåŸºçº¿æ¨¡å‹ï¼ŒåŒ…æ‹¬é€šç”¨å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹å’Œç‰¹å®šä»»åŠ¡æ¨¡å‹ï¼Œå¹¶å¼€è®¾äº†ä¸‰ä¸ªç«èµ›èµ›é“ï¼Œå³ç°å®ä¸–ç•Œåœºæ™¯ä¸­çš„è§†è§‰å®šä½ï¼ˆVG-RSï¼‰ã€å…·æœ‰ç©ºé—´æ„è¯†çš„è§†è§‰é—®ç­”ï¼ˆVQA-SAï¼‰ä»¥åŠåœ¨åˆ›æ„å¹¿å‘Šè§†é¢‘ä¸­çš„è§†è§‰æ¨ç†ï¼ˆVR-Adsï¼‰ã€‚æœ€ç»ˆï¼Œæ¥è‡ªè‘—åå­¦æœ¯å’Œå·¥ä¸šæœºæ„çš„76æ”¯å›¢é˜Ÿå·²ç»æ³¨å†Œï¼Œè¶…è¿‡40ä¸ªæœ‰æ•ˆæäº¤ï¼ˆæ€»å…±è¶…è¿‡1200ä¸ªæäº¤ï¼‰å·²è¢«çº³å…¥æˆ‘ä»¬çš„æ’ååˆ—è¡¨ã€‚æˆ‘ä»¬çš„æ•°æ®é›†ã€ä»£ç é›†ï¼ˆè¶…è¿‡40ä¸ªåŸºçº¿å’Œè¶…è¿‡15ç§å‚ä¸è€…æ–¹æ³•ï¼‰ï¼Œä»¥åŠæ’åéƒ½å·²åœ¨MARS2ç ”è®¨ä¼šç½‘ç«™å’Œæˆ‘ä»¬çš„GitHubç»„ç»‡é¡µé¢å…¬å¼€ï¼š[<a target="_blank" rel="noopener" href="https://github.com/mars2workshop/]%E4%B8%8A%E5%8F%91%E5%B8%83%EF%BC%8C%E6%88%91%E4%BB%AC%E5%B0%86%E4%B8%8D%E6%96%AD%E6%9B%B4%E6%96%B0%E5%92%8C%E5%85%AC%E5%B8%83%E5%8D%B3%E5%B0%86%E5%8F%91%E5%B8%83%E7%9A%84%E4%BA%8B%E4%BB%B6%E3%80%82">https://github.com/mars2workshop/]ä¸Šå‘å¸ƒï¼Œæˆ‘ä»¬å°†ä¸æ–­æ›´æ–°å’Œå…¬å¸ƒå³å°†å‘å¸ƒçš„äº‹ä»¶ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.14142v1">PDF</a> ICCV 2025 MARS2 Workshop and Challenge â€œMultimodal Reasoning and Slow   Thinking in the Large Model Era: Towards System 2 and Beyondâ€™â€™</p>
<p><strong>æ‘˜è¦</strong><br>    æœ¬æ–‡å›é¡¾äº†MARS2 2025æŒ‘æˆ˜èµ›çš„è·¨æ¨¡æ€æ¨ç†ä»»åŠ¡ã€‚é€šè¿‡å¤§å‹åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨æ±‡é›†ä¸åŒè·¨æ¨¡æ€æœºå™¨å­¦ä¹ å’Œå¤§å‹è¯­è¨€æ¨¡å‹çš„æ–¹æ³•ã€‚æ¨åŠ¨ç ”ç©¶äººå‘˜åœ¨è¿™ä¸ªå……æ»¡å˜åŒ–çš„é¢†åŸŸè¿½è¸ªæœ€æ–°è¿›å±•ã€‚åŒæ—¶ï¼Œè¶Šæ¥è¶Šå¤šçš„æµ‹è¯•å¹³å°æ¨åŠ¨äº†é€šç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¼”å˜ã€‚å› æ­¤ï¼Œä»Šå¹´çš„MARS2èšç„¦äºç°å®å’Œç‰¹æ®Šåœºæ™¯ï¼Œä»¥æ‹“å±•å¤šæ¨¡æ€æ¨ç†ä¸­å¤§å‹è¯­è¨€æ¨¡å‹çš„åº”ç”¨ã€‚ç»„ç»‡å›¢é˜Ÿå‘å¸ƒäº†ä¸¤ä¸ªå®šåˆ¶æ•°æ®é›†Lenså’ŒAdsQAä½œä¸ºæµ‹è¯•é›†ï¼Œåˆ†åˆ«æ”¯æŒæ—¥å¸¸åœºæ™¯ä¸­çš„é€šç”¨æ¨ç†å’Œå¹¿å‘Šè§†é¢‘ä¸­çš„é¢†åŸŸç‰¹å®šæ¨ç†ã€‚è¯„ä¼°äº†è¶…è¿‡40ä¸ªåŸºçº¿æ¨¡å‹ï¼ŒåŒ…æ‹¬é€šç”¨å¤§å‹è¯­è¨€æ¨¡å‹å’Œç‰¹å®šä»»åŠ¡æ¨¡å‹ï¼Œå¹¶å¼€è®¾äº†ä¸‰ä¸ªç«èµ›èµ›é“ã€‚æœ€åï¼Œæ¥è‡ªè‘—åå­¦æœ¯å’Œå·¥ä¸šæœºæ„çš„76ä¸ªå›¢é˜Ÿå·²æ³¨å†Œå‚ä¸ï¼Œæœ‰40å¤šä¸ªæœ‰æ•ˆæäº¤è¿›å…¥äº†æ’ååˆ—è¡¨ã€‚æ•°æ®é›†ã€ä»£ç é›†å’Œæ’åå‡å¯åœ¨MARS2ç ”è®¨ä¼šç½‘ç«™å’ŒGitHubç»„ç»‡é¡µé¢ä¸Šå…¬å¼€è®¿é—®ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>MARS2 2025æŒ‘æˆ˜èµ›æ—¨åœ¨ä¿ƒè¿›è·¨æ¨¡æ€æœºå™¨å­¦ä¹ çš„å‘å±•ã€‚è¯¥åŸºå‡†æµ‹è¯•å¸®åŠ©é›†ç»“å¤šç§ä¸åŒæ–¹æ³•å’Œå¤§å‹è¯­è¨€æ¨¡å‹çš„ç ”ç©¶ã€‚</li>
<li>éšç€æµ‹è¯•å¹³å°çš„å¢é•¿ï¼Œé€šç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¼”å˜åŠ å¿«ã€‚è¿™è¡¨æ˜è·¨æ¨¡æ€æ¨ç†åº”ç”¨çš„å¹¿æ³›æ€§å’Œéœ€æ±‚å¢é•¿ã€‚</li>
<li>MARS2èšç„¦äºç°å®å’Œç‰¹æ®Šåœºæ™¯çš„åº”ç”¨ï¼Œæ¨å‡ºä¸¤ä¸ªå®šåˆ¶æ•°æ®é›†Lenså’ŒAdsQAï¼Œåˆ†åˆ«é’ˆå¯¹æ—¥å¸¸åœºæ™¯å’Œå¹¿å‘Šè§†é¢‘ä¸­çš„æ¨ç†ä»»åŠ¡ã€‚</li>
<li>è¯¥ç ”ç©¶è¯„ä¼°äº†è¶…è¿‡40ä¸ªåŸºçº¿æ¨¡å‹çš„è¡¨ç°ï¼ŒåŒ…æ‹¬é€šç”¨å’Œç‰¹å®šä»»åŠ¡çš„æ¨¡å‹ã€‚</li>
<li>ä¸‰ä¸ªç«èµ›èµ›é“æ¶µç›–äº†è§†è§‰æ¥åœ°ã€è§†è§‰é—®ç­”å’Œå¹¿å‘Šè§†é¢‘ä¸­çš„è§†è§‰æ¨ç†ç­‰ä»»åŠ¡ã€‚</li>
<li>æ¥è‡ªå¤šä¸ªå­¦æœ¯å’Œå·¥ä¸šæœºæ„çš„å›¢é˜Ÿå‚ä¸æŒ‘æˆ˜ï¼Œæœ‰æ•ˆæäº¤æ•°é‡ä¼—å¤šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14142">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_LLM/2509.14142v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_LLM/2509.14142v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_LLM/2509.14142v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Reasoning-Efficiently-Through-Adaptive-Chain-of-Thought-Compression-A-Self-Optimizing-Framework"><a href="#Reasoning-Efficiently-Through-Adaptive-Chain-of-Thought-Compression-A-Self-Optimizing-Framework" class="headerlink" title="Reasoning Efficiently Through Adaptive Chain-of-Thought Compression: A   Self-Optimizing Framework"></a>Reasoning Efficiently Through Adaptive Chain-of-Thought Compression: A   Self-Optimizing Framework</h2><p><strong>Authors:Kerui Huang, Shuhan Liu, Xing Hu, Tongtong Xu, Lingfeng Bao, Xin Xia</strong></p>
<p>Chain-of-Thought (CoT) reasoning enhances Large Language Models (LLMs) by prompting intermediate steps, improving accuracy and robustness in arithmetic, logic, and commonsense tasks. However, this benefit comes with high computational costs: longer outputs increase latency, memory usage, and KV-cache demands. These issues are especially critical in software engineering tasks where concise and deterministic outputs are required. To investigate these trade-offs, we conduct an empirical study based on code generation benchmarks. The results reveal that longer CoT does not always help. Excessive reasoning often causes truncation, accuracy drops, and latency up to five times higher, with failed outputs consistently longer than successful ones. These findings challenge the assumption that longer reasoning is inherently better and highlight the need for adaptive CoT control. Motivated by this, we propose SEER (Self-Enhancing Efficient Reasoning), an adaptive framework that compresses CoT while preserving accuracy. SEER combines Best-of-N sampling with task-aware adaptive filtering, dynamically adjusting thresholds based on pre-inference outputs to reduce verbosity and computational overhead. We then evaluate SEER on three software engineering tasks and one math task. On average, SEER shortens CoT by 42.1%, improves accuracy by reducing truncation, and eliminates most infinite loops. These results demonstrate SEER as a practical method to make CoT-enhanced LLMs more efficient and robust, even under resource constraints. </p>
<blockquote>
<p>é“¾å¼æ€ç»´ï¼ˆChain-of-Thoughtï¼Œç®€ç§°CoTï¼‰æ¨ç†é€šè¿‡æç¤ºä¸­é—´æ­¥éª¤å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åŠŸèƒ½ï¼Œåœ¨ç®—æœ¯ã€é€»è¾‘å’Œå¸¸è¯†ä»»åŠ¡ä¸­æé«˜å‡†ç¡®æ€§å’Œç¨³å¥æ€§ã€‚ç„¶è€Œï¼Œè¿™ä¸€ä¼˜åŠ¿ä¼´éšç€è¾ƒé«˜çš„è®¡ç®—æˆæœ¬ï¼šæ›´é•¿çš„è¾“å‡ºå¢åŠ äº†å»¶è¿Ÿã€å†…å­˜ä½¿ç”¨å’ŒKVç¼“å­˜éœ€æ±‚ã€‚è¿™äº›é—®é¢˜åœ¨éœ€è¦ç®€æ´å’Œç¡®å®šæ€§è¾“å‡ºçš„è½¯ä»¶å·¥ç¨‹ä»»åŠ¡ä¸­å°¤ä¸ºå…³é”®ã€‚ä¸ºäº†ç ”ç©¶è¿™äº›æƒè¡¡ï¼Œæˆ‘ä»¬åŸºäºä»£ç ç”ŸæˆåŸºå‡†æµ‹è¯•è¿›è¡Œäº†å®è¯ç ”ç©¶ã€‚ç»“æœè¡¨æ˜ï¼Œè¾ƒé•¿çš„CoTå¹¶ä¸æ€»æ˜¯æœ‰å¸®åŠ©ã€‚è¿‡å¤šçš„æ¨ç†å¸¸å¸¸å¯¼è‡´æˆªæ–­ã€å‡†ç¡®æ€§ä¸‹é™å’Œé«˜è¾¾äº”å€çš„å»¶è¿Ÿï¼Œå¹¶ä¸”å¤±è´¥è¾“å‡ºçš„é•¿åº¦å§‹ç»ˆè¶…è¿‡æˆåŠŸçš„è¾“å‡ºã€‚è¿™äº›å‘ç°è´¨ç–‘äº†è¾ƒé•¿æ¨ç†è¿‡ç¨‹æœ¬èº«æ›´å¥½çš„å‡è®¾ï¼Œå¹¶å¼ºè°ƒäº†è‡ªé€‚åº”CoTæ§åˆ¶çš„å¿…è¦æ€§ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†SEERï¼ˆè‡ªæˆ‘å¢å¼ºé«˜æ•ˆæ¨ç†ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªè‡ªé€‚åº”æ¡†æ¶ï¼Œå¯ä»¥åœ¨ä¿æŒå‡†ç¡®æ€§çš„åŒæ—¶å‹ç¼©CoTã€‚SEERç»“åˆäº†æœ€ä½³Né‡‡æ ·å’ŒåŸºäºä»»åŠ¡çš„è‡ªé€‚åº”è¿‡æ»¤ï¼Œæ ¹æ®é¢„æ¨ç†è¾“å‡ºåŠ¨æ€è°ƒæ•´é˜ˆå€¼ï¼Œä»¥å‡å°‘å†—é•¿å’Œè®¡ç®—å¼€é”€ã€‚ç„¶åæˆ‘ä»¬åœ¨ä¸‰ä¸ªè½¯ä»¶å·¥ç¨‹ä»»åŠ¡å’Œä¸€é¡¹æ•°å­¦ä»»åŠ¡ä¸Šè¯„ä¼°äº†SEERã€‚å¹³å‡è€Œè¨€ï¼ŒSEERå°†CoTç¼©çŸ­äº†42.1%ï¼Œé€šè¿‡å‡å°‘æˆªæ–­æé«˜äº†å‡†ç¡®æ€§ï¼Œå¹¶æ¶ˆé™¤äº†å¤§å¤šæ•°æ— é™å¾ªç¯ã€‚è¿™äº›ç»“æœè¡¨æ˜SEERæ˜¯ä¸€ç§å®ç”¨çš„æ–¹æ³•ï¼Œå¯ä»¥ä½¿CoTå¢å¼ºçš„LLMæ›´åŠ é«˜æ•ˆå’Œç¨³å¥ï¼Œå³ä½¿åœ¨èµ„æºå—é™çš„æƒ…å†µä¸‹ä¹Ÿæ˜¯å¦‚æ­¤ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.14093v1">PDF</a> </p>
<p><strong>Summary</strong><br>é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†é€šè¿‡æç¤ºä¸­é—´æ­¥éª¤å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„èƒ½åŠ›ï¼Œæé«˜ç®—æœ¯ã€é€»è¾‘å’Œå¸¸è¯†ä»»åŠ¡çš„å‡†ç¡®æ€§å’Œç¨³å¥æ€§ã€‚ç„¶è€Œï¼Œè¿™ä¸€ä¼˜åŠ¿å¸¦æ¥äº†è¾ƒé«˜çš„è®¡ç®—æˆæœ¬ï¼ŒåŒ…æ‹¬è¾“å‡ºå¢é•¿å¯¼è‡´çš„å»¶è¿Ÿã€å†…å­˜ä½¿ç”¨å’ŒKVç¼“å­˜éœ€æ±‚å¢åŠ ã€‚åœ¨è½¯ä»¶å·¥ç¨‹ä»»åŠ¡ä¸­ï¼Œè¿™äº›é—®é¢˜å°¤ä¸ºé‡è¦ï¼Œéœ€è¦ç®€æ´å’Œç¡®å®šçš„è¾“å‡ºã€‚ç ”ç©¶å‘ç°ï¼Œè¿‡åº¦çš„æ¨ç†å¸¸å¸¸å¯¼è‡´æˆªæ–­ã€å‡†ç¡®æ€§ä¸‹é™å’Œé«˜è¾¾äº”å€çš„å»¶è¿Ÿï¼Œè€Œä¸”å¤±è´¥çš„è¾“å‡ºé€šå¸¸æ¯”æˆåŠŸçš„è¾“å‡ºæ›´é•¿ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†è‡ªé€‚åº”çš„CoTæ§åˆ¶æ¡†æ¶SEERï¼ˆè‡ªæˆ‘å¢å¼ºæœ‰æ•ˆæ¨ç†ï¼‰ï¼Œå®ƒèƒ½åœ¨ä¿æŒå‡†ç¡®æ€§çš„åŒæ—¶å‹ç¼©CoTã€‚SEERç»“åˆæœ€ä½³Né‡‡æ ·å’Œä»»åŠ¡æ„ŸçŸ¥è‡ªé€‚åº”è¿‡æ»¤ï¼Œæ ¹æ®é¢„æ¨ç†è¾“å‡ºåŠ¨æ€è°ƒæ•´é˜ˆå€¼ï¼Œå‡å°‘å†—é•¿å’Œè®¡ç®—å¼€é”€ã€‚åœ¨ä¸‰ä¸ªè½¯ä»¶å·¥ç¨‹ä»»åŠ¡å’Œä¸€é¡¹æ•°å­¦ä»»åŠ¡ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒSEERå¹³å‡ç¼©çŸ­CoT 42.1%ï¼Œé€šè¿‡å‡å°‘æˆªæ–­æé«˜å‡†ç¡®æ€§ï¼Œå¹¶æ¶ˆé™¤å¤§å¤šæ•°æ— é™å¾ªç¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†èƒ½å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç®—æœ¯ã€é€»è¾‘å’Œå¸¸è¯†ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚</li>
<li>CoTæ¨ç†å¸¦æ¥çš„è®¡ç®—æˆæœ¬è¾ƒé«˜ï¼ŒåŒ…æ‹¬å¢åŠ å»¶è¿Ÿã€å†…å­˜ä½¿ç”¨å’ŒKVç¼“å­˜éœ€æ±‚ã€‚</li>
<li>åœ¨è½¯ä»¶å·¥ç¨‹ä»»åŠ¡ä¸­ï¼Œè¿‡é•¿å’Œå†—ä½™çš„æ¨ç†å¯èƒ½å¯¼è‡´æˆªæ–­ã€å‡†ç¡®æ€§ä¸‹é™å’Œå»¶è¿Ÿå¢åŠ ã€‚</li>
<li>SEERæ¡†æ¶èƒ½åœ¨ä¿æŒå‡†ç¡®æ€§çš„åŒæ—¶å‹ç¼©CoTï¼Œé€šè¿‡æœ€ä½³Né‡‡æ ·å’Œä»»åŠ¡æ„ŸçŸ¥è‡ªé€‚åº”è¿‡æ»¤å‡å°‘å†—é•¿å’Œè®¡ç®—å¼€é”€ã€‚</li>
<li>SEERåœ¨å¤šä¸ªä»»åŠ¡ä¸Šçš„è¯„ä¼°è¡¨æ˜å…¶èƒ½æœ‰æ•ˆç¼©çŸ­CoTï¼Œæé«˜å‡†ç¡®æ€§ï¼Œå¹¶æ¶ˆé™¤å¤§å¤šæ•°æ— é™å¾ªç¯ã€‚</li>
<li>SEERæ–¹æ³•å…·æœ‰å®è·µæ„ä¹‰ï¼Œèƒ½ä½¿CoTå¢å¼ºçš„LLMåœ¨èµ„æºå—é™çš„æƒ…å†µä¸‹æ›´åŠ é«˜æ•ˆå’Œç¨³å¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14093">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_LLM/2509.14093v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_LLM/2509.14093v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_LLM/2509.14093v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="CrowdAgent-Multi-Agent-Managed-Multi-Source-Annotation-System"><a href="#CrowdAgent-Multi-Agent-Managed-Multi-Source-Annotation-System" class="headerlink" title="CrowdAgent: Multi-Agent Managed Multi-Source Annotation System"></a>CrowdAgent: Multi-Agent Managed Multi-Source Annotation System</h2><p><strong>Authors:Maosheng Qin, Renyu Zhu, Mingxuan Xia, Chenkai Chen, Zhen Zhu, Minmin Lin, Junbo Zhao, Lu Xu, Changjie Fan, Runze Wu, Haobo Wang</strong></p>
<p>High-quality annotated data is a cornerstone of modern Natural Language Processing (NLP). While recent methods begin to leverage diverse annotation sources-including Large Language Models (LLMs), Small Language Models (SLMs), and human experts-they often focus narrowly on the labeling step itself. A critical gap remains in the holistic process control required to manage these sources dynamically, addressing complex scheduling and quality-cost trade-offs in a unified manner. Inspired by real-world crowdsourcing companies, we introduce CrowdAgent, a multi-agent system that provides end-to-end process control by integrating task assignment, data annotation, and quality&#x2F;cost management. It implements a novel methodology that rationally assigns tasks, enabling LLMs, SLMs, and human experts to advance synergistically in a collaborative annotation workflow. We demonstrate the effectiveness of CrowdAgent through extensive experiments on six diverse multimodal classification tasks. The source code and video demo are available at <a target="_blank" rel="noopener" href="https://github.com/QMMMS/CrowdAgent">https://github.com/QMMMS/CrowdAgent</a>. </p>
<blockquote>
<p>ç°ä»£è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä¸­ï¼Œé«˜è´¨é‡æ ‡æ³¨æ•°æ®æ˜¯æ ¸å¿ƒè¦ç´ ã€‚è™½ç„¶æœ€è¿‘çš„æ–¹æ³•å¼€å§‹åˆ©ç”¨å¤šç§æ ‡æ³¨æ¥æºï¼ŒåŒ…æ‹¬å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰å’Œäººç±»ä¸“å®¶ï¼Œä½†å®ƒä»¬å¾€å¾€åªå…³æ³¨æ ‡æ³¨æ­¥éª¤æœ¬èº«ã€‚åœ¨åŠ¨æ€ç®¡ç†è¿™äº›æ¥æºæ—¶ï¼Œä»ç„¶å­˜åœ¨æ•´ä½“æµç¨‹æ§åˆ¶çš„ç©ºç™½ï¼Œæ— æ³•ä»¥ç»Ÿä¸€çš„æ–¹å¼è§£å†³å¤æ‚çš„è°ƒåº¦å’Œè´¨é‡æ§åˆ¶æˆæœ¬ä¹‹é—´çš„æƒè¡¡é—®é¢˜ã€‚å—ç°å®ä¸–ç•Œçš„ä¼—åŒ…å…¬å¸å¯å‘ï¼Œæˆ‘ä»¬å¼•å…¥äº†CrowdAgentï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œé€šè¿‡ä»»åŠ¡åˆ†é…ã€æ•°æ®æ ‡æ³¨å’Œè´¨é‡&#x2F;æˆæœ¬ç®¡ç†æ¥æä¾›ç«¯åˆ°ç«¯çš„æµç¨‹æ§åˆ¶ã€‚å®ƒå®æ–½äº†ä¸€ç§åˆç†åˆ†é…ä»»åŠ¡çš„æ–°æ–¹æ³•ï¼Œä½¿LLMã€SLMå’Œäººç±»ä¸“å®¶èƒ½å¤Ÿåœ¨ååŒæ ‡æ³¨å·¥ä½œæµç¨‹ä¸­ååŒæ¨è¿›ã€‚æˆ‘ä»¬åœ¨å…­ä¸ªä¸åŒçš„å¤šæ¨¡æ€åˆ†ç±»ä»»åŠ¡ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒï¼Œè¯æ˜äº†CrowdAgentçš„æœ‰æ•ˆæ€§ã€‚ç›¸å…³æºä»£ç å’Œè§†é¢‘æ¼”ç¤ºå¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/QMMMS/CrowdAgent%E4%B8%8A%E6%9F%A5%E7%9C%8B%E3%80%82">https://github.com/QMMMS/CrowdAgentä¸ŠæŸ¥çœ‹ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.14030v1">PDF</a> </p>
<p><strong>Summary</strong><br>æ•°æ®æ ‡æ³¨æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†çš„æ ¸å¿ƒç¯èŠ‚ï¼Œå½“å‰æ–¹æ³•å¤šèšç„¦äºæ ‡æ³¨æœ¬èº«ï¼Œç¼ºä¹å¯¹ä¸åŒæ ‡æ³¨æºå¦‚å¤§å‹è¯­è¨€æ¨¡å‹ã€å°å‹è¯­è¨€æ¨¡å‹åŠäººç±»ä¸“å®¶çš„å…¨ç¨‹ç®¡æ§ã€‚æœ¬æ–‡æå‡ºCrowdAgentç³»ç»Ÿï¼Œé‡‡ç”¨å¤šæ™ºèƒ½ä½“æŠ€æœ¯å®ç°ä»»åŠ¡åˆ†é…ã€æ•°æ®æ ‡æ³¨å’Œè´¨é‡ç®¡ç†çš„ä¸€ä½“åŒ–ï¼Œé€šè¿‡ååŒæ ‡æ³¨å®ç°ä¸åŒæ™ºèƒ½ä½“çš„æœ‰æ•ˆåˆä½œï¼Œæå‡æ ‡æ³¨æ•ˆç‡å’Œè´¨é‡ã€‚å®éªŒç»“æœè¯æ˜äº†å…¶åœ¨å…­ä¸ªå¤šæ¨¡æ€åˆ†ç±»ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚æ›´å¤šè¯¦æƒ…å¯è®¿é—®å¼€æºå¹³å°ï¼š<a target="_blank" rel="noopener" href="https://github.com/QMMMS/CrowdAgent">é“¾æ¥</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æ•°æ®æ ‡æ³¨åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­è‡³å…³é‡è¦ã€‚</li>
<li>å½“å‰æ–¹æ³•ä¸»è¦å…³æ³¨æ ‡æ³¨æœ¬èº«ï¼Œç¼ºä¹å¯¹æ•´ä¸ªæ ‡æ³¨è¿‡ç¨‹çš„æ§åˆ¶ã€‚</li>
<li>CrowdAgentç³»ç»Ÿé‡‡ç”¨å¤šæ™ºèƒ½ä½“æŠ€æœ¯å®ç°ä¸€ä½“åŒ–çš„æ•°æ®æ ‡æ³¨è¿‡ç¨‹æ§åˆ¶ã€‚</li>
<li>ç³»ç»Ÿèƒ½ç†æ€§åˆ†é…ä»»åŠ¡ï¼Œå®ç°å¤§å‹è¯­è¨€æ¨¡å‹ã€å°å‹è¯­è¨€æ¨¡å‹å’Œäººç±»ä¸“å®¶çš„ååŒå·¥ä½œã€‚</li>
<li>é€šè¿‡å®éªŒéªŒè¯äº†CrowdAgentåœ¨å…­ä¸ªå¤šæ¨¡æ€åˆ†ç±»ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14030">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_LLM/2509.14030v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_LLM/2509.14030v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_LLM/2509.14030v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_LLM/2509.14030v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_LLM/2509.14030v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Large-Language-Model-Empowered-Decision-Transformer-for-UAV-Enabled-Data-Collection"><a href="#Large-Language-Model-Empowered-Decision-Transformer-for-UAV-Enabled-Data-Collection" class="headerlink" title="Large Language Model-Empowered Decision Transformer for UAV-Enabled Data   Collection"></a>Large Language Model-Empowered Decision Transformer for UAV-Enabled Data   Collection</h2><p><strong>Authors:Zhixion Chen, Jiangzhou Wang, and Hyundong Shin, Arumugam Nallanathan</strong></p>
<p>The deployment of unmanned aerial vehicles (UAVs) for reliable and energy-efficient data collection from spatially distributed devices holds great promise in supporting diverse Internet of Things (IoT) applications. Nevertheless, the limited endurance and communication range of UAVs necessitate intelligent trajectory planning. While reinforcement learning (RL) has been extensively explored for UAV trajectory optimization, its interactive nature entails high costs and risks in real-world environments. Offline RL mitigates these issues but remains susceptible to unstable training and heavily rely on expert-quality datasets. To address these challenges, we formulate a joint UAV trajectory planning and resource allocation problem to maximize energy efficiency of data collection. The resource allocation subproblem is first transformed into an equivalent linear programming formulation and solved optimally with polynomial-time complexity. Then, we propose a large language model (LLM)-empowered critic-regularized decision transformer (DT) framework, termed LLM-CRDT, to learn effective UAV control policies. In LLM-CRDT, we incorporate critic networks to regularize the DT model training, thereby integrating the sequence modeling capabilities of DT with critic-based value guidance to enable learning effective policies from suboptimal datasets. Furthermore, to mitigate the data-hungry nature of transformer models, we employ a pre-trained LLM as the transformer backbone of the DT model and adopt a parameter-efficient fine-tuning strategy, i.e., LoRA, enabling rapid adaptation to UAV control tasks with small-scale dataset and low computational overhead. Extensive simulations demonstrate that LLM-CRDT outperforms benchmark online and offline RL methods, achieving up to 36.7% higher energy efficiency than the current state-of-the-art DT approaches. </p>
<blockquote>
<p>æ— äººæœºï¼ˆUAVï¼‰çš„éƒ¨ç½²åœ¨æ”¯æŒå¤šç§ç‰©è”ç½‘ï¼ˆIoTï¼‰åº”ç”¨æ–¹é¢æœ‰ç€å·¨å¤§çš„æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨ä»ç©ºé—´åˆ†å¸ƒçš„è®¾å¤‡æ”¶é›†å¯é å’Œé«˜æ•ˆèƒ½æºæ•°æ®æ–¹é¢ã€‚ç„¶è€Œï¼Œæ— äººæœºçš„ç»­èˆªå’Œé€šä¿¡èŒƒå›´æœ‰é™ï¼Œéœ€è¦è¿›è¡Œæ™ºèƒ½è½¨è¿¹è§„åˆ’ã€‚å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨æ— äººæœºè½¨è¿¹ä¼˜åŒ–æ–¹é¢å¾—åˆ°äº†å¹¿æ³›çš„åº”ç”¨ï¼Œä½†å…¶äº¤äº’æ€§è´¨å¸¦æ¥äº†çœŸå®ç¯å¢ƒä¸­çš„é«˜æˆæœ¬å’Œé£é™©ã€‚ç¦»çº¿RLç¼“è§£äº†è¿™äº›é—®é¢˜ï¼Œä½†ä»ç„¶å®¹æ˜“å—åˆ°è®­ç»ƒä¸ç¨³å®šçš„å½±å“ï¼Œå¹¶ä¸”é«˜åº¦ä¾èµ–ä¸“å®¶çº§æ•°æ®é›†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13934v1">PDF</a> 14pages, 8 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†åˆ©ç”¨æ— äººæœºï¼ˆUAVsï¼‰è¿›è¡Œå¯é ã€èŠ‚èƒ½çš„æ•°æ®é‡‡é›†ï¼Œä»¥æ”¯æŒç‰©è”ç½‘ï¼ˆIoTï¼‰åº”ç”¨çš„å‰æ™¯ã€‚é’ˆå¯¹æ— äººæœºçš„æœ‰é™ç»­èˆªå’Œé€šä¿¡èŒƒå›´é—®é¢˜ï¼Œæå‡ºè”åˆæ— äººæœºè½¨è¿¹è§„åˆ’å’Œèµ„æºåˆ†é…çš„ç­–ç•¥ï¼Œä»¥æœ€å¤§åŒ–æ•°æ®é‡‡é›†çš„èƒ½æºæ•ˆç‡ã€‚é€šè¿‡çº¿æ€§è§„åˆ’è§£å†³èµ„æºåˆ†é…å­é—®é¢˜ï¼Œå¹¶å¼•å…¥å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èµ‹èƒ½çš„å†³ç­–å˜å‹å™¨ï¼ˆDTï¼‰æ¡†æ¶ï¼Œå³LLM-CRDTï¼Œå­¦ä¹ æœ‰æ•ˆçš„æ— äººæœºæ§åˆ¶ç­–ç•¥ã€‚åˆ©ç”¨é¢„è®­ç»ƒLLMä½œä¸ºDTæ¨¡å‹çš„å˜å‹å™¨éª¨å¹²ï¼Œå¹¶é‡‡ç”¨å‚æ•°é«˜æ•ˆçš„å¾®è°ƒç­–ç•¥ï¼Œå³LoRAï¼Œå®ç°å°è§„æ¨¡æ•°æ®é›†ä¸‹çš„å¿«é€Ÿé€‚åº”å’Œè¾ƒä½çš„è®¡ç®—å¼€é”€ã€‚æ¨¡æ‹Ÿç»“æœè¡¨æ˜ï¼ŒLLM-CRDTåœ¨åœ¨çº¿å’Œç¦»çº¿RLæ–¹æ³•ä¸Šè¡¨ç°å‡ºä¼˜åŠ¿ï¼Œä¸å½“å‰æœ€å…ˆè¿›çš„DTæ–¹æ³•ç›¸æ¯”ï¼Œèƒ½æºæ•ˆç‡æé«˜äº†é«˜è¾¾36.7%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>UAVsåœ¨IoTåº”ç”¨ä¸­å…·æœ‰å¯é ã€èŠ‚èƒ½çš„æ•°æ®é‡‡é›†æ½œåŠ›ã€‚</li>
<li>æ— äººæœºçš„ç»­èˆªå’Œé€šä¿¡èŒƒå›´é™åˆ¶éœ€è¦æ™ºèƒ½è½¨è¿¹è§„åˆ’ã€‚</li>
<li>æå‡ºè”åˆæ— äººæœºè½¨è¿¹è§„åˆ’å’Œèµ„æºåˆ†é…é—®é¢˜ä»¥æœ€å¤§åŒ–èƒ½æºæ•ˆç‡ã€‚</li>
<li>èµ„æºåˆ†é…å­é—®é¢˜å¯é€šè¿‡çº¿æ€§è§„åˆ’ä»¥å¤šé¡¹å¼æ—¶é—´å¤æ‚åº¦æœ€ä¼˜è§£å†³ã€‚</li>
<li>å¼•å…¥LLM-CRDTæ¡†æ¶ï¼Œç»“åˆå†³ç­–å˜å‹å™¨å’Œæ‰¹è¯„ç½‘ç»œï¼Œå­¦ä¹ æœ‰æ•ˆçš„æ— äººæœºæ§åˆ¶ç­–ç•¥ã€‚</li>
<li>åˆ©ç”¨é¢„è®­ç»ƒLLMå’ŒLoRAç­–ç•¥ï¼Œå®ç°å¿«é€Ÿé€‚åº”å°è§„æ¨¡æ•°æ®é›†å’Œä½è®¡ç®—å¼€é”€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13934">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_LLM/2509.13934v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_LLM/2509.13934v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_LLM/2509.13934v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Do-Large-Language-Models-Understand-Word-Senses"><a href="#Do-Large-Language-Models-Understand-Word-Senses" class="headerlink" title="Do Large Language Models Understand Word Senses?"></a>Do Large Language Models Understand Word Senses?</h2><p><strong>Authors:Domenico Meconi, Simone Stirpe, Federico Martelli, Leonardo Lavalle, Roberto Navigli</strong></p>
<p>Understanding the meaning of words in context is a fundamental capability for Large Language Models (LLMs). Despite extensive evaluation efforts, the extent to which LLMs show evidence that they truly grasp word senses remains underexplored. In this paper, we address this gap by evaluating both i) the Word Sense Disambiguation (WSD) capabilities of instruction-tuned LLMs, comparing their performance to state-of-the-art systems specifically designed for the task, and ii) the ability of two top-performing open- and closed-source LLMs to understand word senses in three generative settings: definition generation, free-form explanation, and example generation. Notably, we find that, in the WSD task, leading models such as GPT-4o and DeepSeek-V3 achieve performance on par with specialized WSD systems, while also demonstrating greater robustness across domains and levels of difficulty. In the generation tasks, results reveal that LLMs can explain the meaning of words in context up to 98% accuracy, with the highest performance observed in the free-form explanation task, which best aligns with their generative capabilities. </p>
<blockquote>
<p>ç†è§£ä¸Šä¸‹æ–‡ä¸­çš„è¯æ±‡å«ä¹‰å¯¹äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥è¯´æ˜¯ä¸€é¡¹åŸºæœ¬èƒ½åŠ›ã€‚å°½ç®¡å·²ç»è¿›è¡Œäº†å¤§é‡çš„è¯„ä¼°å·¥ä½œï¼Œä½†LLMåœ¨å¤šå¤§ç¨‹åº¦ä¸ŠçœŸæ­£æŒæ¡è¯æ±‡æ„ä¹‰ä»ç„¶ç¼ºä¹æ·±å…¥æ¢ç´¢ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡è¯„ä¼°iï¼‰æŒ‡ä»¤è°ƒä¼˜LLMçš„è¯ä¹‰æ¶ˆæ­§ï¼ˆWSDï¼‰èƒ½åŠ›ï¼Œå¹¶å°†å…¶æ€§èƒ½ä¸ä¸“é—¨ä¸ºæ­¤ä»»åŠ¡è®¾è®¡çš„æœ€æ–°ç³»ç»Ÿè¿›è¡Œæ¯”è¾ƒï¼Œä»¥åŠiiï¼‰ä¸¤ä¸ªè¡¨ç°æœ€ä½³çš„å¼€æºå’Œé—­æºLLMåœ¨ä¸‰ç§ç”Ÿæˆç¯å¢ƒä¸­ç†è§£è¯æ±‡å«ä¹‰çš„èƒ½åŠ›ï¼šå®šä¹‰ç”Ÿæˆã€è‡ªç”±å½¢å¼è§£é‡Šå’Œç¤ºä¾‹ç”Ÿæˆã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç°ï¼Œåœ¨WSDä»»åŠ¡ä¸­ï¼Œé¢†å…ˆæ¨¡å‹å¦‚GPT-4oå’ŒDeepSeek-V3ä¸ä¸“é—¨çš„WSDç³»ç»Ÿæ€§èƒ½ç›¸å½“ï¼ŒåŒæ—¶åœ¨è·¨é¢†åŸŸå’Œä¸åŒéš¾åº¦çº§åˆ«ä¸Šè¡¨ç°å‡ºæ›´å¤§çš„ç¨³å¥æ€§ã€‚åœ¨ç”Ÿæˆä»»åŠ¡ä¸­ï¼Œç»“æœæ˜¾ç¤ºLLMèƒ½å¤Ÿåœ¨ä¸Šä¸‹æ–‡ä¸­è§£é‡Šè¯æ±‡çš„å«ä¹‰ï¼Œå‡†ç¡®ç‡é«˜è¾¾98%ï¼Œå¹¶ä¸”åœ¨è‡ªç”±å½¢å¼è§£é‡Šä»»åŠ¡ä¸­è§‚å¯Ÿåˆ°æœ€ä½³æ€§èƒ½ï¼Œè¿™ä¸å®ƒä»¬çš„ç”Ÿæˆèƒ½åŠ›æœ€ä¸ºå»åˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13905v1">PDF</a> 20 pages, to be published in EMNLP2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è¯­å¢ƒä¸­ç†è§£è¯æ±‡æ„ä¹‰æ˜¯åŸºæœ¬èƒ½åŠ›ã€‚æœ¬æ–‡è¯„ä¼°äº†æŒ‡ä»¤è°ƒæ•´å‹LLMçš„è¯æ±‡æ„ŸçŸ¥æ¶ˆæ­§ï¼ˆWSDï¼‰èƒ½åŠ›ï¼Œå¹¶ä¸ä¸“é—¨ä¸ºè¯¥ä»»åŠ¡è®¾è®¡çš„æœ€å…ˆè¿›çš„ç³»ç»Ÿè¿›è¡Œäº†æ¯”è¾ƒã€‚åŒæ—¶ï¼Œè¿˜è¯„ä¼°äº†ä¸¤ä¸ªé¡¶å°–çš„å¼€æºå’Œé—­æºLLMåœ¨ä¸‰ç§ç”Ÿæˆç¯å¢ƒä¸­çš„è¯æ±‡æ„ŸçŸ¥èƒ½åŠ›ï¼ŒåŒ…æ‹¬å®šä¹‰ç”Ÿæˆã€è‡ªç”±å½¢å¼è§£é‡Šå’Œç¤ºä¾‹ç”Ÿæˆã€‚ç ”ç©¶å‘ç°ï¼Œåœ¨WSDä»»åŠ¡ä¸­ï¼ŒGPT-4oå’ŒDeepSeek-V3ç­‰é¢†å…ˆæ¨¡å‹çš„è¡¨ç°ä¸ä¸“ä¸šçš„WSDç³»ç»Ÿç›¸å½“ï¼Œå¹¶ä¸”åœ¨è·¨é¢†åŸŸå’Œä¸åŒéš¾åº¦çº§åˆ«ä¸Šè¡¨ç°å‡ºæ›´å¤§çš„ç¨³å¥æ€§ã€‚åœ¨ç”Ÿæˆä»»åŠ¡ä¸­ï¼ŒLLMèƒ½å¤Ÿä»¥é«˜è¾¾98%çš„å‡†ç¡®ç‡è§£é‡Šè¯­å¢ƒä¸­çš„è¯æ±‡æ„ä¹‰ï¼Œå…¶ä¸­è‡ªç”±å½¢å¼è§£é‡Šä»»åŠ¡çš„æ€§èƒ½æœ€é«˜ï¼Œè¿™ä¸å®ƒä»¬çš„ç”Ÿæˆèƒ½åŠ›æœ€ä¸ºå»åˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰éœ€è¦åœ¨è¯­å¢ƒä¸­ç†è§£è¯æ±‡æ„ä¹‰çš„åŸºæœ¬èƒ½åŠ›ã€‚</li>
<li>æœ¬æ–‡è¯„ä¼°äº†LLMçš„è¯æ±‡æ„ŸçŸ¥æ¶ˆæ­§ï¼ˆWSDï¼‰èƒ½åŠ›ï¼Œå¹¶ä¸ä¸“é—¨çš„WSDç³»ç»Ÿè¿›è¡Œäº†æ¯”è¾ƒã€‚</li>
<li>æŒ‡ä»¤è°ƒæ•´å‹LLMåœ¨WSDä»»åŠ¡ä¸­çš„è¡¨ç°ä¸æœ€å…ˆè¿›çš„WSDç³»ç»Ÿç›¸å½“ã€‚</li>
<li>LLMåœ¨è·¨é¢†åŸŸå’Œä¸åŒéš¾åº¦çº§åˆ«ä¸Šè¡¨ç°å‡ºç¨³å¥æ€§ã€‚</li>
<li>åœ¨å®šä¹‰ç”Ÿæˆã€è‡ªç”±å½¢å¼è§£é‡Šå’Œç¤ºä¾‹ç”Ÿæˆä¸‰ç§ç”Ÿæˆç¯å¢ƒä¸­è¯„ä¼°äº†LLMçš„è¯æ±‡æ„ŸçŸ¥èƒ½åŠ›ã€‚</li>
<li>LLMèƒ½å¤Ÿä»¥é«˜è¾¾98%çš„å‡†ç¡®ç‡è§£é‡Šè¯­å¢ƒä¸­çš„è¯æ±‡æ„ä¹‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13905">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_LLM/2509.13905v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_LLM/2509.13905v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_LLM/2509.13905v1/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_LLM/2509.13905v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_LLM/2509.13905v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_LLM/2509.13905v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_LLM/2509.13905v1/page_5_1.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Pre-Manipulation-Alignment-Prediction-with-Parallel-Deep-State-Space-and-Transformer-Models"><a href="#Pre-Manipulation-Alignment-Prediction-with-Parallel-Deep-State-Space-and-Transformer-Models" class="headerlink" title="Pre-Manipulation Alignment Prediction with Parallel Deep State-Space and   Transformer Models"></a>Pre-Manipulation Alignment Prediction with Parallel Deep State-Space and   Transformer Models</h2><p><strong>Authors:Motonari Kambara, Komei Sugiura</strong></p>
<p>In this work, we address the problem of predicting the future success of open-vocabulary object manipulation tasks. Conventional approaches typically determine success or failure after the action has been carried out. However, they make it difficult to prevent potential hazards and rely on failures to trigger replanning, thereby reducing the efficiency of object manipulation sequences. To overcome these challenges, we propose a model, which predicts the alignment between a pre-manipulation egocentric image with the planned trajectory and a given natural language instruction. We introduce a Multi-Level Trajectory Fusion module, which employs a state-of-the-art deep state-space model and a transformer encoder in parallel to capture multi-level time-series self-correlation within the end effector trajectory. Our experimental results indicate that the proposed method outperformed existing methods, including foundation models. </p>
<blockquote>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è§£å†³äº†é¢„æµ‹å¼€æ”¾è¯æ±‡å¯¹è±¡æ“ä½œä»»åŠ¡æœªæ¥æˆåŠŸçš„é—®é¢˜ã€‚ä¼ ç»Ÿçš„æ–¹æ³•é€šå¸¸åœ¨åŠ¨ä½œæ‰§è¡Œåæ‰ç¡®å®šæˆåŠŸä¸å¦ã€‚ç„¶è€Œï¼Œè¿™ä½¿å¾—é¢„é˜²æ½œåœ¨å±é™©å˜å¾—å›°éš¾ï¼Œå¹¶ä¸”ä¾èµ–äºå¤±è´¥æ¥è§¦å‘é‡æ–°è§„åˆ’ï¼Œä»è€Œé™ä½å¯¹è±¡æ“ä½œåºåˆ—çš„æ•ˆç‡ã€‚ä¸ºäº†å…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ¨¡å‹ï¼Œè¯¥æ¨¡å‹é¢„æµ‹æ“ä½œå‰çš„ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„å›¾åƒä¸è®¡åˆ’çš„è½¨è¿¹å’Œç»™å®šçš„è‡ªç„¶è¯­è¨€æŒ‡ä»¤ä¹‹é—´çš„å¯¹é½æƒ…å†µã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå¤šå±‚æ¬¡è½¨è¿¹èåˆæ¨¡å—ï¼Œè¯¥æ¨¡å—é‡‡ç”¨æœ€å…ˆè¿›çš„æ·±åº¦çŠ¶æ€ç©ºé—´æ¨¡å‹å’Œå¹¶è¡Œè½¬æ¢å™¨ç¼–ç å™¨æ¥æ•æ‰æœ«ç«¯æ‰§è¡Œå™¨è½¨è¿¹ä¸­çš„å¤šå±‚æ¬¡æ—¶é—´åºåˆ—è‡ªç›¸å…³æ€§ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒåŒ…æ‹¬åŸºç¡€æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13839v1">PDF</a> Published in Advanced Robotics</p>
<p><strong>Summary</strong></p>
<p>é¢„æµ‹å¼€æ”¾è¯æ±‡å¯¹è±¡æ“ä½œä»»åŠ¡çš„æœªæ¥æˆåŠŸæ˜¯ä¸€ä¸ªå…³é”®é—®é¢˜ã€‚ä¼ ç»Ÿæ–¹æ³•é€šå¸¸åœ¨æ“ä½œåè¿›è¡Œæˆè´¥åˆ¤æ–­ï¼Œéš¾ä»¥é¢„é˜²æ½œåœ¨å±é™©ï¼Œä¸”ä¾èµ–å¤±è´¥è§¦å‘é‡æ–°è§„åˆ’ï¼Œé™ä½äº†å¯¹è±¡æ“ä½œåºåˆ—çš„æ•ˆç‡ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯ä»¥é¢„æµ‹é¢„æ“ä½œç¬¬ä¸€äººç§°å›¾åƒä¸è®¡åˆ’è½¨è¿¹ä»¥åŠç»™å®šè‡ªç„¶è¯­è¨€æŒ‡ä»¤ä¹‹é—´çš„å¯¹é½ç¨‹åº¦ã€‚å¼•å…¥çš„å¤šå±‚æ¬¡è½¨è¿¹èåˆæ¨¡å—é‡‡ç”¨æœ€å…ˆè¿›çš„æ·±åº¦çŠ¶æ€ç©ºé—´æ¨¡å‹å’Œå˜å‹å™¨ç¼–ç å™¨ï¼Œä»¥æ•æ‰æœ«ç«¯æ‰§è¡Œå™¨è½¨è¿¹ä¸­çš„å¤šå±‚æ¬¡æ—¶é—´åºåˆ—è‡ªç›¸å…³æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒåŒ…æ‹¬åŸºç¡€æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¼ ç»Ÿæ–¹æ³•åœ¨è¿›è¡Œå¯¹è±¡æ“ä½œä»»åŠ¡æ—¶ï¼Œé€šå¸¸éš¾ä»¥é¢„é˜²æ½œåœ¨å±é™©ï¼Œéœ€è¦åœ¨æ“ä½œåè¿›è¡Œæˆè´¥åˆ¤æ–­ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§é¢„æµ‹å¼€æ”¾è¯æ±‡å¯¹è±¡æ“ä½œä»»åŠ¡æœªæ¥æˆåŠŸçš„æ–°æ¨¡å‹ã€‚</li>
<li>è¯¥æ¨¡å‹é€šè¿‡é¢„æµ‹é¢„æ“ä½œç¬¬ä¸€äººç§°å›¾åƒä¸è®¡åˆ’è½¨è¿¹ä»¥åŠè‡ªç„¶è¯­è¨€æŒ‡ä»¤ä¹‹é—´çš„å¯¹é½ç¨‹åº¦æ¥è§£å†³é—®é¢˜ã€‚</li>
<li>å¼•å…¥çš„å¤šå±‚æ¬¡è½¨è¿¹èåˆæ¨¡å—é‡‡ç”¨å…ˆè¿›çš„æ·±åº¦çŠ¶æ€ç©ºé—´æ¨¡å‹å’Œå˜å‹å™¨ç¼–ç å™¨ã€‚</li>
<li>è¯¥æ¨¡å—èƒ½å¤Ÿæ•æ‰æœ«ç«¯æ‰§è¡Œå™¨è½¨è¿¹ä¸­çš„å¤šå±‚æ¬¡æ—¶é—´åºåˆ—è‡ªç›¸å…³æ€§ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œæ–°æ¨¡å‹åœ¨å¯¹è±¡æ“ä½œä»»åŠ¡ä¸Šçš„æ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13839">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_LLM/2509.13839v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_LLM/2509.13839v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_LLM/2509.13839v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Teaching-According-to-Talents-Instruction-Tuning-LLMs-with-Competence-Aware-Curriculum-Learning"><a href="#Teaching-According-to-Talents-Instruction-Tuning-LLMs-with-Competence-Aware-Curriculum-Learning" class="headerlink" title="Teaching According to Talents! Instruction Tuning LLMs with   Competence-Aware Curriculum Learning"></a>Teaching According to Talents! Instruction Tuning LLMs with   Competence-Aware Curriculum Learning</h2><p><strong>Authors:Yangning Li, Tingwei Lu, Yinghui Li, Yankai Chen, Wei-Chieh Huang, Wenhao Jiang, Hui Wang, Hai-Tao Zheng, Philip S. Yu</strong></p>
<p>Efficient instruction tuning aims to enhance the ultimate performance of large language models (LLMs) trained on a given instruction dataset. Curriculum learning as a typical data organization strategy has shown preliminary effectiveness in instruction tuning. However, current curriculum tuning methods suffer from the curriculum rigidity, since they rely solely on static heuristic difficulty metrics. These methods fail to adapt to the evolving capabilities of models during training, resulting in a fixed and potentially sub-optimal learning trajectory. To address the issue, Competence-Aware Multi-Perspective cUrriculum inStruction tuning framework termed CAMPUS is proposed. CAMPUS offers several advantages: (1) Dynamic selection for sub-curriculum. (2) Competency-aware adjustment to the curriculum schedule. (3) Multiple difficulty-based scheduling. Extensive experiments prove the superior performance of CAMPUS, compared to other state-of-the-art baselines for efficient instruction tuning. </p>
<blockquote>
<p>é«˜æ•ˆæŒ‡ä»¤è°ƒæ•´æ—¨åœ¨æé«˜åœ¨ç»™å®šæŒ‡ä»¤æ•°æ®é›†ä¸Šè®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ€ç»ˆæ€§èƒ½ã€‚ä½œä¸ºå…¸å‹çš„æ•°æ®ç»„ç»‡ç­–ç•¥ï¼Œè¯¾ç¨‹å­¦ä¹ åœ¨æŒ‡ä»¤è°ƒæ•´ä¸­å·²æ˜¾ç¤ºå‡ºåˆæ­¥çš„æœ‰æ•ˆæ€§ã€‚ç„¶è€Œï¼Œå½“å‰çš„è¯¾ç¨‹è°ƒæ•´æ–¹æ³•å—åˆ°è¯¾ç¨‹åˆšæ€§çš„å›°æ‰°ï¼Œå› ä¸ºå®ƒä»¬å®Œå…¨ä¾èµ–äºé™æ€çš„å¯å‘å¼éš¾åº¦æŒ‡æ ‡ã€‚è¿™äº›æ–¹æ³•æœªèƒ½é€‚åº”æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„èƒ½åŠ›æ¼”å˜ï¼Œå¯¼è‡´å›ºå®šä¸”å¯èƒ½æ¬¡ä¼˜çš„å­¦ä¹ è½¨è¿¹ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†åä¸ºCAMPUSçš„èƒ½åŠ›æ„ŸçŸ¥å¤šè§†è§’è¯¾ç¨‹æŒ‡ä»¤è°ƒæ•´æ¡†æ¶ã€‚CAMPUSæä¾›äº†å‡ ä¸ªä¼˜ç‚¹ï¼šï¼ˆ1ï¼‰å­è¯¾ç¨‹çš„åŠ¨æ€é€‰æ‹©ã€‚ï¼ˆ2ï¼‰å¯¹è¯¾ç¨‹è¡¨çš„èƒ½åŠ›æ„ŸçŸ¥è°ƒæ•´ã€‚ï¼ˆ3ï¼‰åŸºäºéš¾åº¦çš„å¤šç§è°ƒåº¦ã€‚å¤§é‡å®éªŒè¯æ˜äº†CAMPUSåœ¨é«˜æ•ˆæŒ‡ä»¤è°ƒæ•´æ–¹é¢ä¼˜äºå…¶ä»–æœ€æ–°åŸºçº¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13790v1">PDF</a> EMNLP 2025 Findings</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è®­ç»ƒè¿‡ç¨‹ä¸­çš„æŒ‡ä»¤é›†ä¼˜åŒ–è‡´åŠ›äºæé«˜å…¶æœ€ç»ˆæ€§èƒ½è¡¨ç°ã€‚ç°æœ‰çš„è¯¾ç¨‹å­¦ä¹ ç­–ç•¥å·²è¢«è¯å®èƒ½å¤Ÿåœ¨ä¼˜åŒ–æŒ‡ä»¤é›†æ–¹é¢å–å¾—åˆæ­¥æˆæ•ˆã€‚ç„¶è€Œï¼Œå½“å‰è¯¾ç¨‹å¼è®­ç»ƒç­–ç•¥å­˜åœ¨è¯¾ç¨‹å®‰æ’åƒµåŒ–çš„é—®é¢˜ï¼Œå› ä¸ºå®ƒä»¬ä¾èµ–äºé™æ€çš„å¯å‘å¼éš¾åº¦åº¦é‡ï¼Œè€Œä¸æ˜¯æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„å®æ—¶é€‚åº”èƒ½åŠ›ï¼Œä»è€Œå¯èƒ½ç”Ÿæˆå›ºå®šçš„æ¬¡ä¼˜å­¦ä¹ è½¨è¿¹ã€‚é’ˆå¯¹ä¸Šè¿°é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºCOMPASSçš„å¤šè§†è§’ç«äº‰åº¦è®­ç»ƒç­–ç•¥æ¡†æ¶ã€‚COMPASSæä¾›äº†å‡ å¤§ä¼˜åŠ¿ï¼ŒåŒ…æ‹¬åŠ¨æ€çš„å­è¯¾ç¨‹é€‰æ‹©ã€åŸºäºèƒ½åŠ›çš„è¯¾ç¨‹å®‰æ’è°ƒæ•´ä»¥åŠå¤šç§éš¾åº¦ä¸ºåŸºç¡€çš„è°ƒåº¦ç­–ç•¥ã€‚å¤§é‡çš„å®éªŒè¯æ˜ï¼Œç›¸è¾ƒäºå…¶ä»–å…ˆè¿›çš„åŸºçº¿æ–¹æ³•ï¼ŒCOMPASSåœ¨æŒ‡ä»¤é›†ä¼˜åŒ–æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è®­ç»ƒè¿‡ç¨‹ä¸­æŒ‡ä»¤é›†ä¼˜åŒ–æ˜¯æé«˜å…¶æ€§èƒ½çš„å…³é”®ã€‚</li>
<li>è¯¾ç¨‹å­¦ä¹ ç­–ç•¥åœ¨æŒ‡ä»¤é›†ä¼˜åŒ–æ–¹é¢å·²å±•ç°å‡ºåˆæ­¥æˆæ•ˆã€‚</li>
<li>å½“å‰è¯¾ç¨‹å¼è®­ç»ƒç­–ç•¥å­˜åœ¨è¯¾ç¨‹å®‰æ’åƒµåŒ–çš„é—®é¢˜ï¼Œå› ä¸ºå®ƒä»¬ä¾èµ–äºé™æ€çš„å¯å‘å¼éš¾åº¦åº¦é‡ã€‚</li>
<li>åŠ¨æ€é€‰æ‹©å­è¯¾ç¨‹èƒ½å¤Ÿä¼˜åŒ–è®­ç»ƒè¿‡ç¨‹ï¼Œé¿å…ä½¿ç”¨å›ºå®šéš¾åº¦åº¦é‡å¯¼è‡´çš„å±€é™æ€§ã€‚</li>
<li>åŸºäºèƒ½åŠ›çš„è¯¾ç¨‹å®‰æ’è°ƒæ•´æœ‰åŠ©äºæé«˜æ¨¡å‹çš„é€‚åº”èƒ½åŠ›ã€‚</li>
<li>COMPASSé€šè¿‡é‡‡ç”¨å¤šç§éš¾åº¦ä¸ºåŸºç¡€çš„è°ƒåº¦ç­–ç•¥æ¥æå‡æ¨¡å‹æ€§èƒ½è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13790">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_LLM/2509.13790v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_LLM/2509.13790v1/page_2_0.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Is-GPT-4o-mini-Blinded-by-its-Own-Safety-Filters-Exposing-the-Multimodal-to-Unimodal-Bottleneck-in-Hate-Speech-Detection"><a href="#Is-GPT-4o-mini-Blinded-by-its-Own-Safety-Filters-Exposing-the-Multimodal-to-Unimodal-Bottleneck-in-Hate-Speech-Detection" class="headerlink" title="Is GPT-4o mini Blinded by its Own Safety Filters? Exposing the   Multimodal-to-Unimodal Bottleneck in Hate Speech Detection"></a>Is GPT-4o mini Blinded by its Own Safety Filters? Exposing the   Multimodal-to-Unimodal Bottleneck in Hate Speech Detection</h2><p><strong>Authors:Niruthiha Selvanayagam, Ted Kurti</strong></p>
<p>As Large Multimodal Models (LMMs) become integral to daily digital life, understanding their safety architectures is a critical problem for AI Alignment. This paper presents a systematic analysis of OpenAIâ€™s GPT-4o mini, a globally deployed model, on the difficult task of multimodal hate speech detection. Using the Hateful Memes Challenge dataset, we conduct a multi-phase investigation on 500 samples to probe the modelâ€™s reasoning and failure modes. Our central finding is the experimental identification of a â€œUnimodal Bottleneck,â€ an architectural flaw where the modelâ€™s advanced multimodal reasoning is systematically preempted by context-blind safety filters. A quantitative validation of 144 content policy refusals reveals that these overrides are triggered in equal measure by unimodal visual 50% and textual 50% content. We further demonstrate that this safety system is brittle, blocking not only high-risk imagery but also benign, common meme formats, leading to predictable false positives. These findings expose a fundamental tension between capability and safety in state-of-the-art LMMs, highlighting the need for more integrated, context-aware alignment strategies to ensure AI systems can be deployed both safely and effectively. </p>
<blockquote>
<p>éšç€å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMï¼‰åœ¨æ—¥å¸¸æ•°å­—ç”Ÿæ´»ä¸­å˜å¾—ä¸å¯æˆ–ç¼ºï¼Œç†è§£å…¶å®‰å…¨æ¶æ„å¯¹äºäººå·¥æ™ºèƒ½å¯¹é½ï¼ˆAI Alignmentï¼‰è€Œè¨€æ˜¯ä¸€ä¸ªå…³é”®é—®é¢˜ã€‚æœ¬æ–‡å¯¹å…¨çƒéƒ¨ç½²çš„æ¨¡å‹OpenAIçš„GPT-4o miniè¿›è¡Œäº†ç³»ç»Ÿåˆ†æï¼Œé’ˆå¯¹å¤šæ¨¡æ€ä»‡æ¨è¨€è®ºæ£€æµ‹è¿™ä¸€è‰°å·¨ä»»åŠ¡å±•å¼€ç ”ç©¶ã€‚æˆ‘ä»¬ä½¿ç”¨ä»‡æ¨memeæŒ‘æˆ˜æ•°æ®é›†ï¼Œå¯¹500ä¸ªæ ·æœ¬è¿›è¡Œå¤šé˜¶æ®µè°ƒæŸ¥ï¼Œä»¥æ¢ç´¢è¯¥æ¨¡å‹çš„æ¨ç†å’Œæ•…éšœæ¨¡å¼ã€‚æˆ‘ä»¬çš„ä¸»è¦å‘ç°æ˜¯å®éªŒç¡®å®šçš„â€œå•æ¨¡æ€ç“¶é¢ˆâ€ï¼Œè¿™æ˜¯ä¸€ç§æ¶æ„ç¼ºé™·ï¼Œæ¨¡å‹çš„å…ˆè¿›å¤šæ¨¡æ€æ¨ç†è¢«è¯­å¢ƒç›²å®‰å…¨è¿‡æ»¤å™¨ç³»ç»Ÿæ‰€é˜»ç¢ã€‚å¯¹144é¡¹å†…å®¹æ”¿ç­–æ‹’ç»è¿›è¡Œçš„å®šé‡éªŒè¯è¡¨æ˜ï¼Œè¿™äº›è¦†ç›–æ˜¯ç”±å•æ¨¡æ€è§†è§‰å†…å®¹å’Œæ–‡æœ¬å†…å®¹å„å ä¸€åŠè§¦å‘çš„ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è¯æ˜è¿™ä¸€å®‰å…¨ç³»ç»Ÿå¾ˆè„†å¼±ï¼Œä¸ä»…é˜»æŒ¡é«˜é£é™©å›¾åƒï¼Œè¿˜é˜»æŒ¡è‰¯æ€§ã€å¸¸è§çš„memeæ ¼å¼ï¼Œå¯¼è‡´å¯é¢„æµ‹çš„é”™è¯¯é˜³æ€§ç»“æœã€‚è¿™äº›å‘ç°æš´éœ²äº†æœ€å…ˆè¿›LMMsåœ¨èƒ½åŠ›å’Œå®‰å…¨ä¹‹é—´çš„åŸºæœ¬çŸ›ç›¾ï¼Œå¼ºè°ƒäº†éœ€è¦æ›´é›†æˆã€è¯­å¢ƒæ„ŸçŸ¥çš„å¯¹é½ç­–ç•¥ï¼Œä»¥ç¡®ä¿AIç³»ç»Ÿèƒ½å¤Ÿæ—¢å®‰å…¨åˆæœ‰æ•ˆåœ°éƒ¨ç½²ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13608v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†OpenAIçš„GPT-4oæ¨¡å‹åœ¨å¤šæ¨¡æ€ä»‡æ¨è¨€è®ºæ£€æµ‹ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚é€šè¿‡å¯¹æ¨¡å‹çš„åˆ†æï¼Œå‘ç°äº†â€œå•æ¨¡æ€ç“¶é¢ˆâ€é—®é¢˜ï¼Œå³æ¨¡å‹çš„å…ˆè¿›å¤šæ¨¡æ€æ¨ç†è¢«ä¸Šä¸‹æ–‡æ— å…³çš„å®‰å…¨è¿‡æ»¤å™¨ç³»ç»Ÿæ€§åœ°é˜»æ­¢ã€‚è¿™å¯¼è‡´æ¨¡å‹åœ¨æ£€æµ‹ä»‡æ¨è¨€è®ºæ—¶å­˜åœ¨ç¼ºé™·ï¼Œå®¹æ˜“è¯¯åˆ¤å›¾åƒå’Œæ–‡æœ¬å†…å®¹ã€‚è¿™è¡¨æ˜åœ¨æœ€æ–°çš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ä¸­ï¼Œèƒ½åŠ›å’Œå®‰å…¨ä¹‹é—´å­˜åœ¨æ ¹æœ¬æ€§å†²çªï¼Œéœ€è¦æ›´é›†æˆã€ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„å¯¹é½ç­–ç•¥æ¥ç¡®ä¿äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„å®‰å…¨å’Œæœ‰æ•ˆéƒ¨ç½²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GPT-4oæ¨¡å‹åœ¨å¤šæ¨¡æ€ä»‡æ¨è¨€è®ºæ£€æµ‹ä¸Šè¡¨ç°å‡ºé‡è¦æ€§ã€‚</li>
<li>å‘ç°äº†â€œå•æ¨¡æ€ç“¶é¢ˆâ€é—®é¢˜ï¼Œå³æ¨¡å‹çš„å…ˆè¿›å¤šæ¨¡æ€æ¨ç†è¢«å®‰å…¨è¿‡æ»¤å™¨é˜»æ­¢ã€‚</li>
<li>æ¨¡å‹åœ¨æ£€æµ‹ä»‡æ¨è¨€è®ºæ—¶å­˜åœ¨ç¼ºé™·ï¼Œå®¹æ˜“è¯¯åˆ¤å›¾åƒå’Œæ–‡æœ¬å†…å®¹ã€‚</li>
<li>å®‰å…¨è¿‡æ»¤å™¨å¯¹å•æ¨¡æ€è§†è§‰å†…å®¹å’Œæ–‡æœ¬å†…å®¹çš„è§¦å‘æ¯”ä¾‹ç›¸åŒã€‚</li>
<li>å®‰å…¨ç³»ç»Ÿè¿‡äºæ•æ„Ÿï¼Œä¸ä»…é˜»æ­¢é«˜é£é™©å†…å®¹ï¼Œä¹Ÿé˜»æ­¢å¸¸è§ã€æ— å®³çš„memeæ ¼å¼ã€‚</li>
<li>å¤§å‹å¤šæ¨¡æ€æ¨¡å‹çš„èƒ½åŠ›å’Œå®‰å…¨ä¹‹é—´å­˜åœ¨æ ¹æœ¬æ€§å†²çªã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13608">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_LLM/2509.13608v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_LLM/2509.13608v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_LLM/2509.13608v1/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_LLM/2509.13608v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Uni-cot-Towards-Unified-Chain-of-Thought-Reasoning-Across-Text-and-Vision"><a href="#Uni-cot-Towards-Unified-Chain-of-Thought-Reasoning-Across-Text-and-Vision" class="headerlink" title="Uni-cot: Towards Unified Chain-of-Thought Reasoning Across Text and   Vision"></a>Uni-cot: Towards Unified Chain-of-Thought Reasoning Across Text and   Vision</h2><p><strong>Authors:Luozheng Qin, Jia Gong, Yuqing Sun, Tianjiao Li, Mengping Yang, Xiaomeng Yang, Chao Qu, Zhiyu Tan, Hao Li</strong></p>
<p>Chain-of-Thought (CoT) reasoning has been widely adopted to enhance Large Language Models (LLMs) by decomposing complex tasks into simpler, sequential subtasks. However, extending CoT to vision-language reasoning tasks remains challenging, as it often requires interpreting transitions of visual states to support reasoning. Existing methods often struggle with this due to limited capacity of modeling visual state transitions or incoherent visual trajectories caused by fragmented architectures.   To overcome these limitations, we propose Uni-CoT, a Unified Chain-of-Thought framework that enables coherent and grounded multimodal reasoning within a single unified model. The key idea is to leverage a model capable of both image understanding and generation to reason over visual content and model evolving visual states. However, empowering a unified model to achieve that is non-trivial, given the high computational cost and the burden of training. To address this, Uni-CoT introduces a novel two-level reasoning paradigm: A Macro-Level CoT for high-level task planning and A Micro-Level CoT for subtask execution. This design significantly reduces the computational overhead. Furthermore, we introduce a structured training paradigm that combines interleaved image-text supervision for macro-level CoT with multi-task objectives for micro-level CoT. Together, these innovations allow Uni-CoT to perform scalable and coherent multi-modal reasoning. Furthermore, thanks to our design, all experiments can be efficiently completed using only 8 A100 GPUs with 80GB VRAM each. Experimental results on reasoning-driven image generation benchmark (WISE) and editing benchmarks (RISE and KRIS) indicates that Uni-CoT demonstrates SOTA performance and strong generalization, establishing Uni-CoT as a promising solution for multi-modal reasoning. Project Page and Code: <a target="_blank" rel="noopener" href="https://sais-fuxi.github.io/projects/uni-cot/">https://sais-fuxi.github.io/projects/uni-cot/</a> </p>
<blockquote>
<p>é“¾å¼æ€ç»´ï¼ˆChain-of-Thoughtï¼Œç®€ç§°CoTï¼‰æ¨ç†å·²è¢«å¹¿æ³›åº”ç”¨äºé€šè¿‡åˆ†è§£å¤æ‚ä»»åŠ¡ä¸ºæ›´ç®€å•çš„é¡ºåºå­ä»»åŠ¡æ¥æå‡å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå°†CoTæ‰©å±•åˆ°è§†è§‰è¯­è¨€æ¨ç†ä»»åŠ¡ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œå› ä¸ºå®ƒé€šå¸¸éœ€è¦è§£é‡Šè§†è§‰çŠ¶æ€çš„è¿‡æ¸¡æ¥æ”¯æŒæ¨ç†ã€‚ç°æœ‰æ–¹æ³•å¸¸å¸¸åœ¨è¿™æ–¹é¢æ„Ÿåˆ°å›°éš¾ï¼Œå› ä¸ºå®ƒä»¬åœ¨å»ºæ¨¡è§†è§‰çŠ¶æ€è¿‡æ¸¡çš„èƒ½åŠ›æœ‰é™ï¼Œæˆ–è€…ç”±äºç¢ç‰‡åŒ–æ¶æ„å¯¼è‡´è§†è§‰è½¨è¿¹ä¸ä¸€è‡´ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†Uni-CoTï¼Œä¸€ä¸ªç»Ÿä¸€çš„æ€ç»´é“¾æ¡†æ¶ï¼Œå®ƒåœ¨ä¸€ä¸ªå•ä¸€æ¨¡å‹ä¸­å®ç°äº†è¿è´¯å’ŒåŸºäºæƒ…å¢ƒçš„å¤šæ¨¡æ€æ¨ç†ã€‚å…³é”®æ˜¯åˆ©ç”¨ä¸€ä¸ªæ—¢èƒ½ç†è§£å›¾åƒåˆèƒ½ç”Ÿæˆå›¾åƒçš„æ¨¡å‹æ¥æ¨ç†è§†è§‰å†…å®¹å¹¶æ¨¡æ‹Ÿä¸æ–­æ¼”å˜çš„è§†è§‰çŠ¶æ€ã€‚ç„¶è€Œï¼Œè®©ä¸€ä¸ªç»Ÿä¸€çš„æ¨¡å‹å®ç°è¿™ä¸€ç‚¹å¹¶ä¸ç®€å•ï¼Œè€ƒè™‘åˆ°é«˜æ˜‚çš„è®¡ç®—æˆæœ¬å’ŒåŸ¹è®­è´Ÿæ‹…ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼ŒUni-CoTå¼•å…¥äº†ä¸€ç§æ–°å‹çš„ä¸¤çº§æ¨ç†æ¨¡å¼ï¼šç”¨äºé«˜çº§ä»»åŠ¡è§„åˆ’çš„å®è§‚å±‚é¢CoTå’Œç”¨äºå­ä»»åŠ¡æ‰§è¡Œçš„å¾®è§‚å±‚é¢CoTã€‚è¿™ç§è®¾è®¡æ˜¾è‘—å‡å°‘äº†è®¡ç®—å¼€é”€ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ç»“æ„åŒ–çš„è®­ç»ƒæ¨¡å¼ï¼Œè¯¥æ¨¡å¼ç»“åˆäº†å®è§‚å±‚é¢CoTçš„äº¤æ›¿å¼å›¾åƒæ–‡æœ¬ç›‘ç£ä¸å¾®è§‚å±‚é¢CoTçš„å¤šä»»åŠ¡ç›®æ ‡ã€‚è¿™äº›åˆ›æ–°ä½¿Uni-CoTèƒ½å¤Ÿè¿›è¡Œå¯æ‰©å±•å’Œè¿è´¯çš„å¤šæ¨¡æ€æ¨ç†ã€‚æ­¤å¤–ï¼Œç”±äºæˆ‘ä»¬çš„è®¾è®¡ï¼Œæ‰€æœ‰å®éªŒéƒ½å¯ä»¥ä»…ä½¿ç”¨8ä¸ªå¸¦æœ‰80GB VRAMçš„A100 GPUé«˜æ•ˆå®Œæˆã€‚åœ¨åŸºäºæ¨ç†çš„å›¾åƒç”ŸæˆåŸºå‡†æµ‹è¯•ï¼ˆWISEï¼‰å’Œç¼–è¾‘åŸºå‡†æµ‹è¯•ï¼ˆRISEå’ŒKRISï¼‰ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒUni-CoTå…·æœ‰å“è¶Šçš„æ€§èƒ½å’Œå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œç¡®ç«‹äº†å®ƒåœ¨å¤šæ¨¡æ€æ¨ç†é¢†åŸŸä¸­çš„å‰æ™¯ã€‚é¡¹ç›®é¡µé¢å’Œä»£ç ï¼š<a target="_blank" rel="noopener" href="https://sais-fuxi.github.io/projects/uni-cot/%E3%80%82">https://sais-fuxi.github.io/projects/uni-cot/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05606v2">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://sais-fuxi.github.io/projects/uni-cot/">https://sais-fuxi.github.io/projects/uni-cot/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¦‚ä½•å°†Chain-of-Thoughtï¼ˆCoTï¼‰æ¨ç†åº”ç”¨äºå¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¤æ‚ä»»åŠ¡ä¸Šï¼Œç‰¹åˆ«æ˜¯åœ¨è§†è§‰è¯­è¨€æ¨ç†ä»»åŠ¡ä¸­é¢ä¸´çš„æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†ä¸€ç§åä¸ºUni-CoTçš„ç»Ÿä¸€æ€ç»´é“¾æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿåœ¨å•ä¸€æ¨¡å‹ä¸­å®ç°è¿è´¯å’ŒåŸºäºåŸºç¡€çš„å¤šæ¨¡æ€æ¨ç†ã€‚å®ƒé€šè¿‡å¼•å…¥ä¸€ç§æ–°å‹çš„ä¸¤çº§æ¨ç†æ¨¡å¼ï¼ˆå®è§‚å±‚é¢çš„ä»»åŠ¡è§„åˆ’å’Œå¾®è§‚å±‚é¢çš„å­ä»»åŠ¡æ‰§è¡Œï¼‰å’Œç»“æ„åŒ–è®­ç»ƒæ¨¡å¼æ¥é™ä½è®¡ç®—å¼€é”€å¹¶ç®€åŒ–è®­ç»ƒéš¾åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒUni-CoTåœ¨å›¾åƒç”Ÿæˆå’Œç¼–è¾‘ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½å’Œå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Uni-CoTæ¡†æ¶æˆåŠŸå°†Chain-of-Thoughtï¼ˆCoTï¼‰æ¨ç†åº”ç”¨äºå¤šæ¨¡æ€ä»»åŠ¡ï¼Œé€šè¿‡åˆ†è§£å¤æ‚ä»»åŠ¡ä¸ºç®€å•å­ä»»åŠ¡æ¥å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚</li>
<li>Uni-CoTæ¡†æ¶é¢ä¸´è§†è§‰è¯­è¨€æ¨ç†ä»»åŠ¡çš„æŒ‘æˆ˜æ—¶ï¼Œåˆ©ç”¨æ—¢èƒ½ç†è§£å›¾åƒåˆèƒ½ç”Ÿæˆå›¾åƒçš„æ¨¡å‹è¿›è¡Œè§†è§‰å†…å®¹çš„æ¨ç†å’ŒçŠ¶æ€å»ºæ¨¡ã€‚</li>
<li>Uni-CoTå¼•å…¥äº†ä¸€ç§æ–°å‹çš„ä¸¤çº§æ¨ç†æ¨¡å¼ï¼šå®è§‚å±‚é¢çš„ä»»åŠ¡è§„åˆ’å’Œå¾®è§‚å±‚é¢çš„å­ä»»åŠ¡æ‰§è¡Œï¼Œä»¥é™ä½è®¡ç®—å¼€é”€ã€‚</li>
<li>Uni-CoTé‡‡ç”¨äº†ç»“åˆå›¾åƒæ–‡æœ¬ç›‘ç£çš„å®è§‚å±‚é¢CoTå’Œå¤šä»»åŠ¡ç›®æ ‡çš„å¾®è§‚å±‚é¢CoTçš„ç»“æ„åŒ–è®­ç»ƒæ¨¡å¼ã€‚</li>
<li>Uni-CoTåœ¨å›¾åƒç”Ÿæˆå’Œç¼–è¾‘ä»»åŠ¡ä¸Šå®ç°äº†å“è¶Šçš„æ€§èƒ½å’Œå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>Uni-CoTçš„è®¾è®¡ä½¿å¾—æ‰€æœ‰å®éªŒéƒ½èƒ½å¤Ÿåœ¨åªæœ‰8ä¸ªA100 GPUï¼ˆæ¯ä¸ªå…·æœ‰80GB VRAMï¼‰ä¸Šé«˜æ•ˆå®Œæˆã€‚</li>
<li>Uni-CoTæ¡†æ¶åœ¨ç†è®ºä¸Šå±•ç¤ºäº†å¼ºå¤§çš„æ½œåŠ›ï¼Œæœ‰æœ›æˆä¸ºå¤šæ¨¡æ€æ¨ç†é¢†åŸŸçš„ä¸€ç§æœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05606">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_LLM/2508.05606v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_LLM/2508.05606v2/page_4_0.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="NL-in-the-Middle-Code-Translation-with-LLMs-and-Intermediate-Representations"><a href="#NL-in-the-Middle-Code-Translation-with-LLMs-and-Intermediate-Representations" class="headerlink" title="NL in the Middle: Code Translation with LLMs and Intermediate   Representations"></a>NL in the Middle: Code Translation with LLMs and Intermediate   Representations</h2><p><strong>Authors:Chi-en Amy Tai, Pengyu Nie, Lukasz Golab, Alexander Wong</strong></p>
<p>Studies show that large language models (LLMs) produce buggy code translations. One promising avenue to improve translation accuracy is through intermediate representations, which provide structured guidance for the translation process. We investigate whether LLM-based code translation can benefit from intermediate representations, specifically in the form of natural language (NL) summaries and abstract syntax trees (ASTs). Since prompt engineering greatly affects LLM performance, we consider several ways to integrate these representations, from one-shot to chain-of-thought (CoT) prompting. Using Open GPT4 8X7B and specialized StarCoder and CodeGen models on popular code translation benchmarks (CodeNet and AVATAR), we find that CoT with an intermediate NL summary performs best, with an increase of 13.8% and 6.7%, respectively, in successful translations for the best-performing model (Open GPT4 8X7B) compared to the zero-shot prompt. </p>
<blockquote>
<p>ç ”ç©¶è¡¨æ˜ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰äº§ç”Ÿçš„ä»£ç ç¿»è¯‘å­˜åœ¨é”™è¯¯ã€‚æé«˜ç¿»è¯‘å‡†ç¡®æ€§çš„ä¸€ä¸ªæœ‰å‰é€”çš„é€”å¾„æ˜¯é€šè¿‡ä¸­é—´è¡¨ç¤ºï¼Œå®ƒä¸ºç¿»è¯‘è¿‡ç¨‹æä¾›ç»“æ„åŒ–æŒ‡å¯¼ã€‚æˆ‘ä»¬è°ƒæŸ¥äº†åŸºäºLLMçš„ä»£ç ç¿»è¯‘æ˜¯å¦å¯ä»¥ä»ä¸­é—´è¡¨ç¤ºä¸­å—ç›Šï¼Œå…·ä½“è¡¨ç°ä¸ºè‡ªç„¶è¯­è¨€ï¼ˆNLï¼‰æ‘˜è¦å’ŒæŠ½è±¡è¯­æ³•æ ‘ï¼ˆASTsï¼‰ã€‚ç”±äºæç¤ºå·¥ç¨‹æå¤§åœ°å½±å“äº†LLMçš„æ€§èƒ½ï¼Œæˆ‘ä»¬è€ƒè™‘äº†å‡ ç§å°†è¿™äº›è¡¨ç¤ºé›†æˆåœ¨ä¸€èµ·çš„æ–¹æ³•ï¼Œä»å•æ­¥åˆ°é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æç¤ºã€‚åœ¨æµè¡Œçš„ä»£ç ç¿»è¯‘åŸºå‡†æµ‹è¯•ï¼ˆCodeNetå’ŒAVATARï¼‰ä¸Šï¼Œä½¿ç”¨Open GPT4 8X7Bå’Œä¸“é—¨çš„StarCoderå’ŒCodeGenæ¨¡å‹ï¼Œæˆ‘ä»¬å‘ç°ä½¿ç”¨ä¸­é—´NLæ‘˜è¦çš„CoTè¡¨ç°æœ€ä½³ï¼Œå¯¹äºæ€§èƒ½æœ€ä½³çš„æ¨¡å‹ï¼ˆOpen GPT4 8X7Bï¼‰ï¼Œä¸é›¶æ­¥æç¤ºç›¸æ¯”ï¼ŒæˆåŠŸç¿»è¯‘çš„æ•°é‡åˆ†åˆ«å¢åŠ äº†13.8%å’Œ6.7%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.08627v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä»£ç ç¿»è¯‘æ–¹é¢å­˜åœ¨ç¼ºé™·ï¼Œå¯é€šè¿‡ä¸­é—´è¡¨ç¤ºæ³•æé«˜ç¿»è¯‘å‡†ç¡®æ€§ã€‚ç ”ç©¶å‘ç°ï¼Œè‡ªç„¶è¯­è¨€ï¼ˆNLï¼‰æ‘˜è¦å’ŒæŠ½è±¡è¯­æ³•æ ‘ï¼ˆASTsï¼‰ç­‰ä¸­é—´è¡¨ç¤ºå½¢å¼å¯¹LLMä»£ç ç¿»è¯‘æœ‰ç§¯æå½±å“ã€‚é€šè¿‡ä¸€ç³»åˆ—å®éªŒï¼Œå‘ç°ä½¿ç”¨Open GPT4 8X7Bå’Œç‰¹æ®Šè®¾è®¡çš„StarCoderå’ŒCodeGenæ¨¡å‹è¿›è¡Œæœ€ä½³æ¨¡å‹æ€§èƒ½å¯¹æ¯”ï¼Œä¸é›¶åŸºå‡†æç¤ºç›¸æ¯”ï¼Œå…·æœ‰ä¸­é—´NLæ‘˜è¦çš„æ€ç»´é“¾ï¼ˆCoTï¼‰æç¤ºæ•ˆæœæœ€ä½³ï¼ŒæˆåŠŸç¿»è¯‘æ¬¡æ•°å¢åŠ äº†æœ€é«˜è¾¾æœ€é«˜å¯è¾¾å¢åŠ ç™¾åˆ†ä¹‹åä¸‰ç‚¹å…«å’Œç™¾åˆ†ä¹‹å…­ç‚¹ä¸ƒã€‚è¿™è¡¨æ˜ä¸­é—´è¡¨ç¤ºæ³•å¯¹äºæé«˜LLMåœ¨ä»£ç ç¿»è¯‘æ–¹é¢çš„æ€§èƒ½å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LLMåœ¨ä»£ç ç¿»è¯‘æ–¹é¢å­˜åœ¨ç¼ºé™·ï¼Œéœ€è¦æ”¹è¿›ä»¥æé«˜å‡†ç¡®æ€§ã€‚</li>
<li>ä¸­é—´è¡¨ç¤ºæ³•æ˜¯æé«˜LLMä»£ç ç¿»è¯‘æ€§èƒ½çš„ä¸€ç§æœ‰å‰é€”çš„æ–¹æ³•ã€‚</li>
<li>è‡ªç„¶è¯­è¨€ï¼ˆNLï¼‰æ‘˜è¦å’ŒæŠ½è±¡è¯­æ³•æ ‘ï¼ˆASTsï¼‰æ˜¯æœ‰æ•ˆçš„ä¸­é—´è¡¨ç¤ºå½¢å¼ã€‚</li>
<li>æç¤ºå·¥ç¨‹å¯¹LLMæ€§èƒ½æœ‰å¾ˆå¤§å½±å“ã€‚</li>
<li>æ€ç»´é“¾ï¼ˆCoTï¼‰æç¤ºæ–¹æ³•ç»“åˆä¸­é—´NLæ‘˜è¦è¡¨ç°æœ€ä½³ã€‚</li>
<li>ä¸é›¶åŸºå‡†æç¤ºç›¸æ¯”ï¼Œæœ€ä½³æ¨¡å‹ï¼ˆOpen GPT4 8X7Bï¼‰ä½¿ç”¨CoTæç¤ºåï¼ŒæˆåŠŸç¿»è¯‘æ¬¡æ•°å¢åŠ äº†æœ€é«˜è¾¾ç™¾åˆ†ä¹‹åä¸‰ç‚¹å…«å’Œç™¾åˆ†ä¹‹å…­ç‚¹ä¸ƒã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.08627">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_LLM/2507.08627v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_LLM/2507.08627v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_LLM/2507.08627v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_LLM/2507.08627v2/page_3_0.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Benchmarking-Large-Language-Models-for-Cryptanalysis-and-Side-Channel-Vulnerabilities"><a href="#Benchmarking-Large-Language-Models-for-Cryptanalysis-and-Side-Channel-Vulnerabilities" class="headerlink" title="Benchmarking Large Language Models for Cryptanalysis and Side-Channel   Vulnerabilities"></a>Benchmarking Large Language Models for Cryptanalysis and Side-Channel   Vulnerabilities</h2><p><strong>Authors:Utsav Maskey, Chencheng Zhu, Usman Naseem</strong></p>
<p>Recent advancements in large language models (LLMs) have transformed natural language understanding and generation, leading to extensive benchmarking across diverse tasks. However, cryptanalysis - a critical area for data security and its connection to LLMsâ€™ generalization abilities - remains underexplored in LLM evaluations. To address this gap, we evaluate the cryptanalytic potential of state-of-the-art LLMs on ciphertexts produced by a range of cryptographic algorithms. We introduce a benchmark dataset of diverse plaintexts, spanning multiple domains, lengths, writing styles, and topics, paired with their encrypted versions. Using zero-shot and few-shot settings along with chain-of-thought prompting, we assess LLMsâ€™ decryption success rate and discuss their comprehension abilities. Our findings reveal key insights into LLMsâ€™ strengths and limitations in side-channel scenarios and raise concerns about their susceptibility to under-generalization-related attacks. This research highlights the dual-use nature of LLMs in security contexts and contributes to the ongoing discussion on AI safety and security. </p>
<blockquote>
<p>æœ€è¿‘çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥å·²ç»æ”¹å˜äº†è‡ªç„¶è¯­è¨€çš„ç†è§£å’Œç”Ÿæˆæ–¹å¼ï¼Œå¹¶åœ¨å„ç§ä»»åŠ¡ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„åŸºå‡†æµ‹è¯•ã€‚ç„¶è€Œï¼Œå¯†ç åˆ†ææ˜¯æ•°æ®å®‰å…¨çš„å…³é”®é¢†åŸŸï¼Œå…¶ä¸LLMçš„æ³›åŒ–èƒ½åŠ›ä¹‹é—´çš„è”ç³»åœ¨LLMè¯„ä¼°ä¸­ä»ç„¶è¢«æ¢ç´¢å¾—ä¸å¤Ÿæ·±å…¥ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬è¯„ä¼°äº†æœ€æ–°LLMå¯¹ç”±å¤šç§åŠ å¯†ç®—æ³•äº§ç”Ÿçš„å¯†æ–‡çš„åˆ†ææ½œåŠ›ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåŒ…å«å¤šç§é¢†åŸŸçš„å¤šæ ·åŒ–æ˜æ–‡åŸºå‡†æ•°æ®é›†ï¼Œä»¥åŠä¸å®ƒä»¬çš„åŠ å¯†ç‰ˆæœ¬é…å¯¹ã€‚é€šè¿‡é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬è®¾ç½®ä»¥åŠé“¾å¼æ€ç»´æç¤ºï¼Œæˆ‘ä»¬è¯„ä¼°äº†LLMçš„è§£å¯†æˆåŠŸç‡å¹¶è®¨è®ºäº†å®ƒä»¬çš„ç†è§£èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœæ­ç¤ºäº†LLMåœ¨ä¾§é€šé“åœºæ™¯ä¸­çš„ä¼˜åŠ¿å’Œå±€é™æ€§ï¼Œå¹¶å¯¹å®ƒä»¬å®¹æ˜“å—åˆ°æ³›åŒ–ä¸è¶³ç›¸å…³çš„æ”»å‡»è¡¨ç¤ºæ‹…å¿§ã€‚è¯¥ç ”ç©¶çªæ˜¾äº†LLMåœ¨å®‰å…¨ä¸Šä¸‹æ–‡ä¸­çš„åŒé‡ç”¨é€”æ€§è´¨ï¼Œå¹¶ä¸ºäººå·¥æ™ºèƒ½å®‰å…¨å’Œä¿å¯†æ€§çš„æŒç»­è®¨è®ºåšå‡ºäº†è´¡çŒ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24621v2">PDF</a> EMNLPâ€™25 Findings</p>
<p><strong>Summary</strong></p>
<p>æœ€æ–°çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›å±•åœ¨è‡ªç„¶è¯­è¨€ç†è§£å’Œç”Ÿæˆæ–¹é¢å¸¦æ¥äº†å˜é©ï¼Œå¹¶åœ¨å„ç§ä»»åŠ¡ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„åŸºå‡†æµ‹è¯•ã€‚ç„¶è€Œï¼Œåœ¨æ•°æ®å®‰å…¨å’ŒLLMçš„æ³›åŒ–èƒ½åŠ›ä¹‹é—´çš„è”ç³»ä¸­ï¼Œå¯†ç åˆ†æè¿™ä¸€å…³é”®é¢†åŸŸåœ¨LLMè¯„ä¼°ä¸­ä»è¢«å¿½è§†ã€‚æœ¬ç ”ç©¶æ—¨åœ¨è§£å†³è¿™ä¸€ç©ºç™½ï¼Œè¯„ä¼°æœ€å…ˆè¿›çš„LLMå¯¹å¤šç§åŠ å¯†ç®—æ³•äº§ç”Ÿçš„å¯†æ–‡çš„å¯†ç åˆ†ææ½œåŠ›ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåŒ…å«å¤šæ ·åŒ–æ˜æ–‡å’Œä¸å…¶åŠ å¯†ç‰ˆæœ¬é…å¯¹çš„åŸºå‡†æ•°æ®é›†ã€‚é€šè¿‡é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬è®¾ç½®ä»¥åŠé“¾å¼æ€ç»´æç¤ºï¼Œæˆ‘ä»¬è¯„ä¼°äº†LLMçš„è§£å¯†æˆåŠŸç‡å¹¶è®¨è®ºäº†å…¶ç†è§£èƒ½åŠ›ã€‚ç ”ç©¶å‘ç°äº†LLMåœ¨ä¾§ä¿¡é“åœºæ™¯ä¸‹çš„ä¼˜ç‚¹å’Œå±€é™æ€§ï¼Œæ­ç¤ºäº†å…¶å¯¹æ³›åŒ–æ”»å‡»çš„æ•æ„Ÿæ€§ï¼Œå¼ºè°ƒäº†åœ¨å®‰å…¨èƒŒæ™¯ä¸‹LLMçš„åŒé‡ç”¨é€”æ€§è´¨ï¼Œå¹¶ä¸ºäººå·¥æ™ºèƒ½å®‰å…¨å’Œå®‰å…¨æ€§çš„æŒç»­è®¨è®ºåšå‡ºäº†è´¡çŒ®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LLMçš„æœ€æ–°è¿›å±•åœ¨è‡ªç„¶è¯­è¨€ç†è§£å’Œç”Ÿæˆæ–¹é¢å¸¦æ¥äº†æ˜¾è‘—å˜é©ï¼Œå¹¶è¿›è¡Œäº†å¹¿æ³›çš„åŸºå‡†æµ‹è¯•ã€‚</li>
<li>å¯†ç åˆ†æåœ¨LLMè¯„ä¼°ä¸­ä»æ˜¯ä¸€ä¸ªè¢«å¿½è§†çš„å…³é”®é¢†åŸŸï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°æ®å®‰å…¨å’Œæ³›åŒ–èƒ½åŠ›ä¹‹é—´çš„è”ç³»æ–¹é¢ã€‚</li>
<li>è¯„ä¼°äº†LLMå¯¹å¤šç§åŠ å¯†ç®—æ³•äº§ç”Ÿçš„å¯†æ–‡çš„å¯†ç åˆ†ææ½œåŠ›ã€‚</li>
<li>é€šè¿‡å¤šæ ·åŒ–çš„æ˜æ–‡å’Œå…¶åŠ å¯†ç‰ˆæœ¬é…å¯¹çš„åŸºå‡†æ•°æ®é›†è¿›è¡Œè¯„ä¼°ã€‚</li>
<li>è¯„ä¼°äº†LLMçš„è§£å¯†æˆåŠŸç‡ï¼Œå¹¶è®¨è®ºäº†å…¶ç†è§£èƒ½åŠ›ï¼Œæ­ç¤ºäº†LLMåœ¨ä¾§ä¿¡é“åœºæ™¯ä¸‹çš„ä¼˜ç‚¹å’Œå±€é™æ€§ã€‚</li>
<li>LLMå¯èƒ½å®¹æ˜“å—åˆ°æ³›åŒ–æ”»å‡»çš„å½±å“ï¼Œè¿™å¼•å‘äº†å¯¹å…¶å®‰å…¨æ€§çš„å…³æ³¨ã€‚</li>
<li>ç ”ç©¶å¼ºè°ƒäº†LLMåœ¨å®‰å…¨æ€§èƒŒæ™¯ä¸‹çš„åŒé‡ç”¨é€”æ€§è´¨ï¼Œæ—¢å¯ç”¨äºå®‰å…¨ç›®çš„ä¹Ÿå¯èƒ½è¢«ç”¨äºå¨èƒå®‰å…¨çš„ç›®çš„ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24621">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_LLM/2505.24621v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_LLM/2505.24621v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_LLM/2505.24621v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_LLM/2505.24621v2/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_LLM/2505.24621v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_LLM/2505.24621v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Understanding-and-Mitigating-Overrefusal-in-LLMs-from-an-Unveiling-Perspective-of-Safety-Decision-Boundary"><a href="#Understanding-and-Mitigating-Overrefusal-in-LLMs-from-an-Unveiling-Perspective-of-Safety-Decision-Boundary" class="headerlink" title="Understanding and Mitigating Overrefusal in LLMs from an Unveiling   Perspective of Safety Decision Boundary"></a>Understanding and Mitigating Overrefusal in LLMs from an Unveiling   Perspective of Safety Decision Boundary</h2><p><strong>Authors:Licheng Pan, Yongqi Tong, Xin Zhang, Xiaolu Zhang, Jun Zhou, Zhixuan Chu</strong></p>
<p>Large language models (LLMs) have demonstrated remarkable capabilities across a wide range of tasks, yet they often refuse to answer legitimate queriesâ€“a phenomenon known as overrefusal. Overrefusal typically stems from over-conservative safety alignment, causing models to treat many reasonable prompts as potentially risky. To systematically understand this issue, we probe and leverage the modelsâ€™ safety decision boundaries to analyze and mitigate overrefusal. Our findings reveal that overrefusal is closely tied to misalignment at these boundary regions, where models struggle to distinguish subtle differences between benign and harmful content. Building on these insights, we present RASS, an automated framework for prompt generation and selection that strategically targets overrefusal prompts near the safety boundary. By harnessing steering vectors in the representation space, RASS efficiently identifies and curates boundary-aligned prompts, enabling more effective and targeted mitigation of overrefusal. This approach not only provides a more precise and interpretable view of model safety decisions but also seamlessly extends to multilingual scenarios. We have explored the safety decision boundaries of various LLMs and construct the MORBench evaluation set to facilitate robust assessment of model safety and helpfulness across multiple languages. Code and datasets are available at <a target="_blank" rel="noopener" href="https://github.com/Master-PLC/RASS">https://github.com/Master-PLC/RASS</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šç§ä»»åŠ¡ä¸­å±•ç°äº†å“è¶Šçš„èƒ½åŠ›ï¼Œä½†å®ƒä»¬ç»å¸¸æ‹’ç»å›ç­”åˆç†çš„æŸ¥è¯¢â€”â€”è¿™ä¸€ç°è±¡è¢«ç§°ä¸ºè¿‡åº¦æ‹’ç»ã€‚è¿‡åº¦æ‹’ç»é€šå¸¸æºäºè¿‡äºä¿å®ˆçš„å®‰å…¨å¯¹é½ï¼Œå¯¼è‡´æ¨¡å‹å°†è®¸å¤šåˆç†çš„æç¤ºè§†ä¸ºå¯èƒ½å­˜åœ¨é£é™©ã€‚ä¸ºäº†ç³»ç»Ÿåœ°äº†è§£è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æ¢ç´¢å’Œåˆ©ç”¨äº†æ¨¡å‹çš„å®‰å…¨å†³ç­–è¾¹ç•Œï¼Œä»¥åˆ†æå’Œå‡è½»è¿‡åº¦æ‹’ç»ç°è±¡ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œè¿‡åº¦æ‹’ç»ä¸è¿™äº›è¾¹ç•ŒåŒºåŸŸçš„é”™ä½ç´§å¯†ç›¸å…³ï¼Œåœ¨è¿™é‡Œï¼Œæ¨¡å‹å¾ˆéš¾åŒºåˆ†è‰¯æ€§å†…å®¹å’Œæœ‰å®³å†…å®¹ä¹‹é—´çš„ç»†å¾®å·®åˆ«ã€‚åŸºäºè¿™äº›è§è§£ï¼Œæˆ‘ä»¬æå‡ºäº†RASSï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºæç¤ºç”Ÿæˆå’Œé€‰æ‹©çš„è‡ªåŠ¨åŒ–æ¡†æ¶ï¼Œå®ƒæˆ˜ç•¥æ€§åœ°é’ˆå¯¹å®‰å…¨è¾¹ç•Œé™„è¿‘çš„è¿‡åº¦æ‹’ç»æç¤ºã€‚é€šè¿‡åˆ©ç”¨è¡¨ç¤ºç©ºé—´ä¸­çš„å¼•å¯¼å‘é‡ï¼ŒRASSæœ‰æ•ˆåœ°è¯†åˆ«å’Œç­›é€‰è¾¹ç•Œå¯¹é½çš„æç¤ºï¼Œä»è€Œæ›´æœ‰æ•ˆåœ°è§£å†³è¿‡åº¦æ‹’ç»é—®é¢˜ã€‚è¿™ç§æ–¹æ³•ä¸ä»…æä¾›äº†æ›´ç²¾ç¡®å’Œå¯è§£é‡Šçš„æ¨¡å‹å®‰å…¨å†³ç­–è§†å›¾ï¼Œè€Œä¸”æ— ç¼åœ°æ‰©å±•åˆ°å¤šè¯­è¨€åœºæ™¯ã€‚æˆ‘ä»¬æ¢ç´¢äº†å¤šç§LLMçš„å®‰å…¨å†³ç­–è¾¹ç•Œï¼Œå¹¶æ„å»ºäº†MORBenchè¯„ä¼°é›†ï¼Œä»¥ä¿ƒè¿›è·¨å¤šç§è¯­è¨€çš„æ¨¡å‹å®‰å…¨å’Œæœ‰ç”¨æ€§çš„ç¨³å¥è¯„ä¼°ã€‚ä»£ç å’Œæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Master-PLC/RASS%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Master-PLC/RASSæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.18325v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä¼—å¤šä»»åŠ¡ä¸­å±•ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œä½†å­˜åœ¨æ‹’ç»åˆæ³•æŸ¥è¯¢çš„é—®é¢˜ï¼Œç§°ä¸ºæ‹’ç»ç°è±¡ã€‚æ‹’ç»é€šå¸¸æºäºè¿‡äºä¿å®ˆçš„å®‰å…¨å¯¹é½ï¼Œå¯¼è‡´æ¨¡å‹å°†è®¸å¤šåˆç†çš„æç¤ºè§†ä¸ºæ½œåœ¨é£é™©ã€‚æœ¬æ–‡æ·±å…¥æ¢è®¨äº†è¿™ä¸€é—®é¢˜ï¼Œé€šè¿‡æ¢ç©¶æ¨¡å‹çš„å®‰å…¨å†³ç­–è¾¹ç•Œæ¥åˆ†æå’Œç¼“è§£æ‹’ç»ç°è±¡ã€‚ç ”ç©¶å‘ç°ï¼Œæ‹’ç»ç°è±¡ä¸å†³ç­–è¾¹ç•Œå¤„çš„å¯¹é½å¤±é…å¯†åˆ‡ç›¸å…³ï¼Œæ¨¡å‹åœ¨åŒºåˆ†è‰¯æ€§å†…å®¹å’Œæœ‰å®³å†…å®¹ä¹‹é—´çš„å¾®å¦™å·®å¼‚æ—¶é‡åˆ°å›°éš¾ã€‚åŸºäºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†RASSæ¡†æ¶ï¼Œç”¨äºç”Ÿæˆå’Œé€‰æ‹©æç¤ºï¼Œæœ‰é’ˆå¯¹æ€§åœ°é’ˆå¯¹å®‰å…¨è¾¹ç•Œå¤„çš„æ‹’ç»æç¤ºã€‚RASSåˆ©ç”¨è¡¨ç¤ºç©ºé—´ä¸­çš„å¼•å¯¼å‘é‡ï¼Œæœ‰æ•ˆè¯†åˆ«å¹¶ç­›é€‰è¾¹ç•Œå¯¹é½çš„æç¤ºï¼Œæ›´é’ˆå¯¹æ‹’ç»ç°è±¡è¿›è¡Œç¼“è§£ã€‚è¿™ä¸ä»…ä¸ºæ¨¡å‹å®‰å…¨å†³ç­–æä¾›äº†æ›´ç²¾ç¡®å’Œå¯è§£é‡Šçš„è§†è§’ï¼Œè¿˜èƒ½æ— ç¼æ‰©å±•åˆ°å¤šè¯­è¨€åœºæ™¯ã€‚æœ¬æ–‡æ¢è®¨äº†LLMçš„å®‰å…¨å†³ç­–è¾¹ç•Œï¼Œå¹¶æ„å»ºäº†MORBenchè¯„ä¼°é›†ï¼Œä»¥ä¿ƒè¿›è·¨å¤šç§è¯­è¨€çš„æ¨¡å‹å®‰å…¨å’Œæœ‰ç”¨æ€§çš„ç¨³å¥è¯„ä¼°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMså±•ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†å­˜åœ¨æ‹’ç»åˆæ³•æŸ¥è¯¢çš„é—®é¢˜ï¼Œç§°ä¸ºæ‹’ç»ç°è±¡ã€‚</li>
<li>æ‹’ç»ç°è±¡é€šå¸¸æºäºæ¨¡å‹çš„å®‰å…¨å¯¹é½è¿‡äºä¿å®ˆã€‚</li>
<li>æ‹’ç»ç°è±¡ä¸æ¨¡å‹å®‰å…¨å†³ç­–è¾¹ç•Œçš„å¯¹é½å¤±é…æœ‰å…³ã€‚</li>
<li>RASSæ¡†æ¶ç”¨äºç”Ÿæˆå’Œé€‰æ‹©æç¤ºï¼Œä»¥ç¼“è§£æ‹’ç»ç°è±¡ã€‚</li>
<li>RASSåˆ©ç”¨è¡¨ç¤ºç©ºé—´ä¸­çš„å¼•å¯¼å‘é‡ï¼Œæœ‰æ•ˆè¯†åˆ«è¾¹ç•Œå¯¹é½çš„æç¤ºã€‚</li>
<li>RASSæ¡†æ¶å¯æ‰©å±•åˆ°å¤šè¯­è¨€åœºæ™¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.18325">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_LLM/2505.18325v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_LLM/2505.18325v3/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_LLM/2505.18325v3/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_LLM/2505.18325v3/page_5_0.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="CoPL-Collaborative-Preference-Learning-for-Personalizing-LLMs"><a href="#CoPL-Collaborative-Preference-Learning-for-Personalizing-LLMs" class="headerlink" title="CoPL: Collaborative Preference Learning for Personalizing LLMs"></a>CoPL: Collaborative Preference Learning for Personalizing LLMs</h2><p><strong>Authors:Youngbin Choi, Seunghyuk Cho, Minjong Lee, MoonJeong Park, Yesong Ko, Jungseul Ok, Dongwoo Kim</strong></p>
<p>Personalizing large language models (LLMs) is important for aligning outputs with diverse user preferences, yet existing methods struggle with flexibility and generalization. We propose CoPL (Collaborative Preference Learning), a graph-based collaborative filtering framework that models user-response relationships to enhance preference estimation, particularly in sparse annotation settings. By integrating a mixture of LoRA experts, CoPL efficiently fine-tunes LLMs while dynamically balancing shared and user-specific preferences. Additionally, an optimization-free adaptation strategy enables generalization to unseen users without fine-tuning. Experiments on UltraFeedback-P demonstrate that CoPL outperforms existing personalized reward models, effectively capturing both common and controversial preferences, making it a scalable solution for personalized LLM alignment. The code is available at <a target="_blank" rel="noopener" href="https://github.com/ml-postech/CoPL">https://github.com/ml-postech/CoPL</a>. </p>
<blockquote>
<p>ä¸ªæ€§åŒ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯¹äºå°†è¾“å‡ºä¸å¤šæ ·åŒ–çš„ç”¨æˆ·åå¥½å¯¹é½éå¸¸é‡è¦ï¼Œä½†ç°æœ‰æ–¹æ³•åœ¨çµæ´»æ€§å’Œé€šç”¨æ€§æ–¹é¢é‡åˆ°äº†æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†CoPLï¼ˆååŒåå¥½å­¦ä¹ ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºå›¾çš„ååŒè¿‡æ»¤æ¡†æ¶ï¼Œé€šè¿‡å»ºæ¨¡ç”¨æˆ·å“åº”å…³ç³»æ¥æé«˜åå¥½ä¼°è®¡ï¼Œç‰¹åˆ«æ˜¯åœ¨ç¨€ç–æ³¨é‡Šè®¾ç½®ä¸‹ã€‚é€šè¿‡é›†æˆæ··åˆLoRAä¸“å®¶ï¼ŒCoPLèƒ½å¤Ÿé«˜æ•ˆåœ°å¾®è°ƒLLMï¼ŒåŒæ—¶åŠ¨æ€å¹³è¡¡å…±äº«å’Œç”¨æˆ·ç‰¹å®šåå¥½ã€‚æ­¤å¤–ï¼Œä¸€ç§æ— éœ€ä¼˜åŒ–çš„é€‚åº”ç­–ç•¥ä½¿å¾—èƒ½å¤Ÿæ¨å¹¿åˆ°æœªè§è¿‡çš„ç”¨æˆ·è€Œæ— éœ€å¾®è°ƒã€‚åœ¨UltraFeedback-Pä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCoPLä¼˜äºç°æœ‰çš„ä¸ªæ€§åŒ–å¥–åŠ±æ¨¡å‹ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ•æ‰å…±åŒå’Œäº‰è®®çš„åå¥½ï¼Œæˆä¸ºä¸ªæ€§åŒ–LLMå¯¹é½çš„å¯æ‰©å±•è§£å†³æ–¹æ¡ˆã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/ml-postech/CoPL%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/ml-postech/CoPLè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.01658v2">PDF</a> 19pages, 13 figures, 11 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸ªæ€§åŒ–éœ€æ±‚çš„æ–°æ–¹æ³•â€”â€”åä½œåå¥½å­¦ä¹ ï¼ˆCoPLï¼‰ã€‚åœ¨ç¨€ç–æ ‡æ³¨åœºæ™¯ä¸‹ï¼Œè¯¥æ–¹æ³•é€šè¿‡å»ºæ¨¡ç”¨æˆ·å“åº”å…³ç³»æ¥æé«˜åå¥½ä¼°è®¡çš„å‡†ç¡®æ€§ã€‚CoPLé‡‡ç”¨åŸºäºå›¾çš„åä½œè¿‡æ»¤æ¡†æ¶ï¼Œé›†æˆå¤šä¸ªLoRAä¸“å®¶ï¼Œèƒ½å¤ŸåŠ¨æ€å¹³è¡¡å…±äº«å’Œç”¨æˆ·ç‰¹å®šåå¥½ï¼Œå¹¶åœ¨æ— éœ€å¾®è°ƒçš„æƒ…å†µä¸‹é€‚åº”æ–°ç”¨æˆ·ã€‚åœ¨UltraFeedback-Pä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCoPLåœ¨æ•æ‰å…±åŒå’Œäº‰è®®æ€§åå¥½æ–¹é¢è¡¨ç°ä¼˜äºç°æœ‰ä¸ªæ€§åŒ–å¥–åŠ±æ¨¡å‹ï¼Œä¸ºä¸ªæ€§åŒ–LLMå¯¹é½æä¾›äº†å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CoPLæ˜¯ä¸€ç§é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸ªæ€§åŒ–æ–¹æ³•ï¼Œé€‚ç”¨äºå¤šæ ·ç”¨æˆ·åå¥½å¯¹é½ã€‚</li>
<li>CoPLé‡‡ç”¨åŸºäºå›¾çš„åä½œè¿‡æ»¤æ¡†æ¶ï¼Œèƒ½æœ‰æ•ˆæé«˜ç¨€ç–æ ‡æ³¨åœºæ™¯ä¸‹çš„åå¥½ä¼°è®¡å‡†ç¡®æ€§ã€‚</li>
<li>é›†æˆLoRAä¸“å®¶è¿›è¡ŒåŠ¨æ€å¹³è¡¡å…±äº«å’Œç”¨æˆ·ç‰¹å®šåå¥½ã€‚</li>
<li>ä¼˜åŒ–ç­–ç•¥ä½¿æ¨¡å‹èƒ½å¤Ÿé€‚åº”æœªè§ç”¨æˆ·è€Œæ— éœ€è¿›è¡Œå¾®è°ƒã€‚</li>
<li>åœ¨UltraFeedback-Pä¸Šçš„å®éªŒéªŒè¯äº†CoPLçš„æœ‰æ•ˆæ€§ï¼Œè¡¨æ˜å…¶èƒ½å¤Ÿæ•æ‰å…±åŒå’Œäº‰è®®æ€§åå¥½ã€‚</li>
<li>CoPLåœ¨ä¸ªæ€§åŒ–LLMå¯¹é½é—®é¢˜ä¸Šæä¾›äº†ä¸€ç§å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.01658">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_LLM/2503.01658v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_LLM/2503.01658v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_LLM/2503.01658v2/page_4_0.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="LLM-ABBA-Understanding-time-series-via-symbolic-approximation"><a href="#LLM-ABBA-Understanding-time-series-via-symbolic-approximation" class="headerlink" title="LLM-ABBA: Understanding time series via symbolic approximation"></a>LLM-ABBA: Understanding time series via symbolic approximation</h2><p><strong>Authors:Erin Carson, Xinye Chen, Cheng Kang</strong></p>
<p>The success of large language models (LLMs) for time series has been demonstrated in previous work. Utilizing a symbolic time series representation, one can efficiently bridge the gap between LLMs and time series. However, the remaining challenge is to exploit the semantic information hidden in time series by using symbols or existing tokens of LLMs, while aligning the embedding space of LLMs according to the hidden information of time series. The symbolic time series approximation (STSA) method called adaptive Brownian bridge-based symbolic aggregation (ABBA) shows outstanding efficacy in preserving salient time series features by modeling time series patterns in terms of amplitude and period while using existing tokens of LLMs.   In this paper, we introduce a method, called LLM-ABBA, that integrates ABBA into large language models for various downstream time series tasks. By symbolizing time series, LLM-ABBA compares favorably to the recent state-of-the-art (SOTA) in UCR and three medical time series classification tasks. Meanwhile, a fixed-polygonal chain trick in ABBA is introduced to \kc{avoid obvious drifting} during prediction tasks by significantly mitigating the effects of cumulative error arising from misused symbols during the transition from symbols to numerical values. In time series regression tasks, LLM-ABBA achieves the new SOTA on Time Series Extrinsic Regression (TSER) benchmarks. LLM-ABBA also shows competitive prediction capability compared to recent SOTA time series prediction results. We believe this framework can also seamlessly extend to other time series tasks. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ—¶é—´åºåˆ—ä¸Šçš„æˆåŠŸå·²åœ¨å…ˆå‰çš„ç ”ç©¶ä¸­å¾—åˆ°è¯æ˜ã€‚é€šè¿‡ç¬¦å·æ—¶é—´åºåˆ—è¡¨ç¤ºæ³•ï¼Œå¯ä»¥æœ‰æ•ˆåœ°å¼¥åˆLLMå’Œæ—¶é—´åºåˆ—ä¹‹é—´çš„å·®è·ã€‚ç„¶è€Œï¼Œå‰©ä½™çš„æŒ‘æˆ˜åœ¨äºåˆ©ç”¨LLMä¸­çš„ç¬¦å·æˆ–ç°æœ‰ä»¤ç‰Œæ¥æŒ–æ˜éšè—åœ¨æ—¶é—´åºåˆ—ä¸­çš„è¯­ä¹‰ä¿¡æ¯ï¼ŒåŒæ—¶æ ¹æ®æ—¶é—´åºåˆ—çš„éšè—ä¿¡æ¯å¯¹é½LLMçš„åµŒå…¥ç©ºé—´ã€‚ç¬¦å·æ—¶é—´åºåˆ—é€¼è¿‘ï¼ˆSTSAï¼‰æ–¹æ³•ä¸­çš„è‡ªé€‚åº”å¸ƒæœ—æ¡¥åŸºç¬¦å·èšåˆï¼ˆABBAï¼‰æ–¹æ³•é€šè¿‡åœ¨å¹…åº¦å’Œå‘¨æœŸæ–¹é¢å¯¹æ—¶é—´åºåˆ—æ¨¡å¼è¿›è¡Œå»ºæ¨¡ï¼ŒåŒæ—¶ä½¿ç”¨LLMçš„ç°æœ‰ä»¤ç‰Œï¼Œæ˜¾ç¤ºå‡ºåœ¨ä¿ç•™é‡è¦æ—¶é—´åºåˆ—ç‰¹å¾æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ•ˆæœã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§ç§°ä¸ºLLM-ABBAçš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å°†ABBAé›†æˆåˆ°å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ï¼Œç”¨äºå„ç§ä¸‹æ¸¸æ—¶é—´åºåˆ—ä»»åŠ¡ã€‚é€šè¿‡ç¬¦å·åŒ–æ—¶é—´åºåˆ—ï¼ŒLLM-ABBAåœ¨UCRå’Œä¸‰ä¸ªåŒ»ç–—æ—¶é—´åºåˆ—åˆ†ç±»ä»»åŠ¡ä¸Šæ¯”è¾ƒé¢†å…ˆå½“å‰æœ€æ–°æŠ€æœ¯ã€‚åŒæ—¶ï¼ŒABBAä¸­çš„å›ºå®šå¤šè¾¹å½¢é“¾æŠ€å·§è¢«å¼•å…¥ï¼Œä»¥é¿å…é¢„æµ‹ä»»åŠ¡æœŸé—´å‡ºç°æ˜æ˜¾çš„æ¼‚ç§»ç°è±¡ï¼Œé€šè¿‡æ˜¾ç€å‡è½»å› ç¬¦å·åˆ°æ•°å€¼è½¬æ¢è¿‡ç¨‹ä¸­è¯¯ç”¨ç¬¦å·æ‰€äº§ç”Ÿçš„ç´¯ç§¯è¯¯å·®çš„å½±å“ã€‚åœ¨æ—¶é—´åºåˆ—å›å½’ä»»åŠ¡ä¸­ï¼ŒLLM-ABBAåœ¨TimeSeriesExtrinsicRegressionï¼ˆTSERï¼‰åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æ–°çš„æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚LLM-ABBAè¿˜æ˜¾ç¤ºå‡ºä¸æœ€æ–°æ—¶é—´åºåˆ—é¢„æµ‹ç»“æœç›¸æ¯”å…·æœ‰ç«äº‰åŠ›çš„é¢„æµ‹èƒ½åŠ›ã€‚æˆ‘ä»¬ç›¸ä¿¡è¯¥æ¡†æ¶ä¹Ÿå¯ä»¥æ— ç¼åœ°æ‰©å±•åˆ°å…¶ä»–æ—¶é—´åºåˆ—ä»»åŠ¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.18506v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ—¶é—´åºåˆ—é¢†åŸŸçš„æˆåŠŸåº”ç”¨ã€‚é€šè¿‡ç¬¦å·æ—¶é—´åºåˆ—è¡¨ç¤ºæ³•ï¼Œå¯ä»¥æœ‰æ•ˆåœ°å¼¥åˆLLMå’Œæ—¶é—´åºåˆ—ä¹‹é—´çš„å·®è·ã€‚æ–‡ç« æå‡ºäº†ä¸€ç§åä¸ºLLM-ABBAçš„æ–¹æ³•ï¼Œå°†è‡ªé€‚åº”å¸ƒæœ—æ¡¥åŸºç¬¦å·èšåˆï¼ˆABBAï¼‰é›†æˆåˆ°å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ï¼Œç”¨äºå„ç§ä¸‹æ¸¸æ—¶é—´åºåˆ—ä»»åŠ¡ã€‚è¯¥æ–¹æ³•é€šè¿‡ç¬¦å·åŒ–æ—¶é—´åºåˆ—ï¼Œåœ¨UCRå’Œä¸‰ä¸ªåŒ»ç–—æ—¶é—´åºåˆ—åˆ†ç±»ä»»åŠ¡ä¸­ï¼Œä¸æœ€æ–°æŠ€æœ¯ç›¸æ¯”è¡¨ç°å‡ºä¼˜è¶Šçš„æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒABBAä¸­çš„å›ºå®šå¤šè¾¹å½¢é“¾æŠ€å·§é¿å…äº†é¢„æµ‹ä»»åŠ¡ä¸­æ˜æ˜¾çš„æ¼‚ç§»ç°è±¡ï¼Œé€šè¿‡æ˜¾è‘—å‡è½»ç¬¦å·åˆ°æ•°å€¼è½¬æ¢è¿‡ç¨‹ä¸­è¯¯ç”¨ç¬¦å·å¯¼è‡´çš„ç´¯ç§¯è¯¯å·®çš„å½±å“ã€‚åœ¨æ—¶é—´åºåˆ—å›å½’ä»»åŠ¡ä¸­ï¼ŒLLM-ABBAè¾¾åˆ°äº†Time Series Extrinsic Regressionï¼ˆTSERï¼‰åŸºå‡†æµ‹è¯•çš„æ–°æ°´å¹³ã€‚é¢„è®¡è¯¥æ¡†æ¶å¯ä»¥æ— ç¼æ‰©å±•åˆ°å…¶ä»–æ—¶é—´åºåˆ—ä»»åŠ¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²æˆåŠŸåº”ç”¨äºæ—¶é—´åºåˆ—é¢†åŸŸã€‚</li>
<li>é€šè¿‡ç¬¦å·æ—¶é—´åºåˆ—è¡¨ç¤ºæ³•ï¼Œå¯ä»¥å¼¥åˆLLMå’Œæ—¶é—´åºåˆ—ä¹‹é—´çš„å·®è·ã€‚</li>
<li>LLM-ABBAæ–¹æ³•ç»“åˆäº†è‡ªé€‚åº”å¸ƒæœ—æ¡¥åŸºç¬¦å·èšåˆï¼ˆABBAï¼‰å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œç”¨äºå¤šç§æ—¶é—´åºåˆ—ä»»åŠ¡ã€‚</li>
<li>LLM-ABBAåœ¨UCRå’ŒåŒ»ç–—æ—¶é—´åºåˆ—åˆ†ç±»ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>å›ºå®šå¤šè¾¹å½¢é“¾æŠ€å·§é¿å…äº†é¢„æµ‹ä»»åŠ¡ä¸­çš„æ˜æ˜¾æ¼‚ç§»ç°è±¡ã€‚</li>
<li>LLM-ABBAåœ¨TSERåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æ–°æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.18506">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_LLM/2411.18506v4/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_LLM/2411.18506v4/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_LLM/2411.18506v4/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_LLM/2411.18506v4/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_LLM/2411.18506v4/page_5_1.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-19/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-19/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-19/Agent/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_Agent/2504.09532v2/page_1_0.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-19  TGPO Tree-Guided Preference Optimization for Robust Web Agent   Reinforcement Learning
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-19
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-19/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_R1_Reasoning/2509.13758v1/page_0_0.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-19  TGPO Tree-Guided Preference Optimization for Robust Web Agent   Reinforcement Learning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-19
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32251.6k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
