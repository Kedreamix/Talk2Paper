<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-09-19  Noise-Level Diffusion Guidance Well Begun is Half Done">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_Diffusion Models/2509.13576v1/page_4_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-09-19
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-10-07
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    7.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    29 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-09-19-更新"><a href="#2025-09-19-更新" class="headerlink" title="2025-09-19 更新"></a>2025-09-19 更新</h1><h2 id="Noise-Level-Diffusion-Guidance-Well-Begun-is-Half-Done"><a href="#Noise-Level-Diffusion-Guidance-Well-Begun-is-Half-Done" class="headerlink" title="Noise-Level Diffusion Guidance: Well Begun is Half Done"></a>Noise-Level Diffusion Guidance: Well Begun is Half Done</h2><p><strong>Authors:Harvey Mannering, Zhiwu Huang, Adam Prugel-Bennett</strong></p>
<p>Diffusion models have achieved state-of-the-art image generation. However, the random Gaussian noise used to start the diffusion process influences the final output, causing variations in image quality and prompt adherence. Existing noise-level optimization approaches generally rely on extra dataset construction, additional networks, or backpropagation-based optimization, limiting their practicality. In this paper, we propose Noise Level Guidance (NLG), a simple, efficient, and general noise-level optimization approach that refines initial noise by increasing the likelihood of its alignment with general guidance - requiring no additional training data, auxiliary networks, or backpropagation. The proposed NLG approach provides a unified framework generalizable to both conditional and unconditional diffusion models, accommodating various forms of diffusion-level guidance. Extensive experiments on five standard benchmarks demonstrate that our approach enhances output generation quality and input condition adherence. By seamlessly integrating with existing guidance methods while maintaining computational efficiency, our method establishes NLG as a practical and scalable enhancement to diffusion models. Code can be found at <a target="_blank" rel="noopener" href="https://github.com/harveymannering/NoiseLevelGuidance">https://github.com/harveymannering/NoiseLevelGuidance</a>. </p>
<blockquote>
<p>扩散模型在图像生成方面已经达到了前沿水平。然而，用于启动扩散过程的随机高斯噪声会影响最终输出，导致图像质量和提示遵循方面的变化。现有的噪声水平优化方法一般依赖于额外数据集的建设、附加网络，或基于反向传播的优化，这限制了它们的实用性。在本文中，我们提出了噪声水平指导（NLG）方法，这是一种简单、高效且通用的噪声水平优化方法，它通过提高初始噪声与通用指导对齐的可能性来优化初始噪声，无需额外的训练数据、辅助网络或反向传播。所提出的NLG方法为有条件和无条件的扩散模型提供了一个统一的可推广框架，可适应各种扩散水平的指导。在五个标准基准测试上的大量实验表明，我们的方法提高了输出生成的质量和输入条件的遵循性。我们的方法能够无缝集成到现有的指导方法中，同时保持计算效率，这使得NLG成为扩散模型的实用和可扩展增强。代码可在<a target="_blank" rel="noopener" href="https://github.com/harveymannering/NoiseLevelGuidance%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/harveymannering/NoiseLevelGuidance找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13936v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了一种名为噪声水平引导（NLG）的简单、高效且通用的噪声水平优化方法，用于改进扩散过程的初始噪声。该方法无需额外的训练数据、辅助网络或反向传播，通过增加初始噪声与通用引导对齐的可能性来优化噪声水平。NLG方法适用于条件和无条件的扩散模型，可容纳各种扩散水平引导形式。在五个标准基准测试上的广泛实验表明，该方法提高了输出生成质量和输入条件遵守性，且计算效率高。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型在图像生成方面达到最新水平，但初始噪声影响最终输出，导致图像质量和提示遵守性有所差异。</li>
<li>现有噪声水平优化方法通常依赖于额外数据集构建、辅助网络或反向传播优化，限制了其实用性。</li>
<li>提出了噪声水平引导（NLG）方法，这是一种简单、高效且通用的噪声水平优化策略。</li>
<li>NLG方法无需额外的训练数据、辅助网络或反向传播，通过增加初始噪声与通用引导对齐的可能性来工作。</li>
<li>NLG方法适用于条件和无条件的扩散模型，并提供了广泛的扩散级别指导形式。</li>
<li>在五个标准基准测试上，NLG方法提高了输出生成质量和输入条件的遵守性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13936">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_Diffusion Models/2509.13936v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_Diffusion Models/2509.13936v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_Diffusion Models/2509.13936v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_Diffusion Models/2509.13936v1/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_Diffusion Models/2509.13936v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_Diffusion Models/2509.13936v1/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_Diffusion Models/2509.13936v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_Diffusion Models/2509.13936v1/page_5_1.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Towards-Robust-Defense-against-Customization-via-Protective-Perturbation-Resistant-to-Diffusion-based-Purification"><a href="#Towards-Robust-Defense-against-Customization-via-Protective-Perturbation-Resistant-to-Diffusion-based-Purification" class="headerlink" title="Towards Robust Defense against Customization via Protective Perturbation   Resistant to Diffusion-based Purification"></a>Towards Robust Defense against Customization via Protective Perturbation   Resistant to Diffusion-based Purification</h2><p><strong>Authors:Wenkui Yang, Jie Cao, Junxian Duan, Ran He</strong></p>
<p>Diffusion models like Stable Diffusion have become prominent in visual synthesis tasks due to their powerful customization capabilities, which also introduce significant security risks, including deepfakes and copyright infringement. In response, a class of methods known as protective perturbation emerged, which mitigates image misuse by injecting imperceptible adversarial noise. However, purification can remove protective perturbations, thereby exposing images again to the risk of malicious forgery. In this work, we formalize the anti-purification task, highlighting challenges that hinder existing approaches, and propose a simple diagnostic protective perturbation named AntiPure. AntiPure exposes vulnerabilities of purification within the “purification-customization” workflow, owing to two guidance mechanisms: 1) Patch-wise Frequency Guidance, which reduces the model’s influence over high-frequency components in the purified image, and 2) Erroneous Timestep Guidance, which disrupts the model’s denoising strategy across different timesteps. With additional guidance, AntiPure embeds imperceptible perturbations that persist under representative purification settings, achieving effective post-customization distortion. Experiments show that, as a stress test for purification, AntiPure achieves minimal perceptual discrepancy and maximal distortion, outperforming other protective perturbation methods within the purification-customization workflow. </p>
<blockquote>
<p>由于强大的定制化能力，像Stable Diffusion这样的扩散模型在视觉合成任务中脱颖而出，但同时也引入了巨大的安全风险，包括深度伪造和版权侵犯等。为了应对这一问题，一类名为保护扰动的方法应运而生，通过注入几乎无法察觉的对立噪声来减轻图像滥用问题。然而，净化方法可以消除保护性扰动，从而使图像再次面临恶意篡改的风险。在本研究中，我们对抗净化任务进行了形式化表述，强调了阻碍现有方法的各种挑战，并提出了一种简单的诊断性保护扰动方法AntiPure。AntiPure在“净化-定制”工作流中暴露了净化方法的漏洞，这得益于两种引导机制：1）斑块频率引导，降低了模型对净化图像中高频成分的影响；2）错误时间步引导，破坏了模型在不同时间步的降噪策略。通过额外的引导，AntiPure能够在典型的净化设置下嵌入几乎无法察觉的扰动，实现了有效的定制后失真。实验表明，作为对净化方法的压力测试，AntiPure实现了最小的感知差异和最大的失真，在净化定制工作流中优于其他保护扰动方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13922v1">PDF</a> Accepted to ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>Stable Diffusion等扩散模型在视觉合成任务中因强大的定制能力而备受瞩目，但也带来深伪、版权侵犯等安全风险。为应对这些问题，出现了保护扰动等方法来减轻图像滥用风险。然而，净化技术可以消除保护扰动，使图像再次面临恶意篡改风险。本文正式提出抗净化任务，强调当前方法面临的挑战，并提出一种名为AntiPure的简单诊断保护扰动。AntiPure通过两种引导机制揭露净化技术的漏洞：“净化定制”工作流程中的补丁频率引导和错误时序引导。它嵌入看不见的保护扰动，在代表性净化设置下持续存在，实现了有效的后定制失真。实验表明，作为净化压力测试，AntiPure实现了最小的感知差异和最大的失真，在净化定制工作流中优于其他保护扰动方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型如Stable Diffusion在视觉合成任务中表现出强大的定制化能力，但这也引发了深伪和版权侵犯等安全风险。</li>
<li>保护扰动方法被提出来减轻图像滥用风险，但现有的净化技术可以消除这些保护扰动。</li>
<li>AntiPure是一种新的诊断保护扰动，它通过两种引导机制揭露净化技术的漏洞。</li>
<li>AntiPure通过在净化后维持必要的失真来强化图像的保护能力。</li>
<li>作为一项针对净化的压力测试，AntiPure展现出优异性能，能够在最小感知差异下实现最大失真。</li>
<li>AntiPure在净化定制工作流中的表现优于其他保护扰动方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13922">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_Diffusion Models/2509.13922v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_Diffusion Models/2509.13922v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_Diffusion Models/2509.13922v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_Diffusion Models/2509.13922v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="EDITS-Enhancing-Dataset-Distillation-with-Implicit-Textual-Semantics"><a href="#EDITS-Enhancing-Dataset-Distillation-with-Implicit-Textual-Semantics" class="headerlink" title="EDITS: Enhancing Dataset Distillation with Implicit Textual Semantics"></a>EDITS: Enhancing Dataset Distillation with Implicit Textual Semantics</h2><p><strong>Authors:Qianxin Xia, Jiawei Du, Guoming Lu, Zhiyong Shu, Jielei Wang</strong></p>
<p>Dataset distillation aims to synthesize a compact dataset from the original large-scale one, enabling highly efficient learning while preserving competitive model performance. However, traditional techniques primarily capture low-level visual features, neglecting the high-level semantic and structural information inherent in images. In this paper, we propose EDITS, a novel framework that exploits the implicit textual semantics within the image data to achieve enhanced distillation. First, external texts generated by a Vision Language Model (VLM) are fused with image features through a Global Semantic Query module, forming the prior clustered buffer. Local Semantic Awareness then selects representative samples from the buffer to construct image and text prototypes, with the latter produced by guiding a Large Language Model (LLM) with meticulously crafted prompt. Ultimately, Dual Prototype Guidance strategy generates the final synthetic dataset through a diffusion model. Extensive experiments confirm the effectiveness of our method.Source code is available in: <a target="_blank" rel="noopener" href="https://github.com/einsteinxia/EDITS">https://github.com/einsteinxia/EDITS</a>. </p>
<blockquote>
<p>数据集蒸馏的目标是从原始的大规模数据集中合成一个紧凑的数据集，以实现高效学习，同时保持竞争性的模型性能。然而，传统技术主要捕捉图像的低级视觉特征，忽略了图像中固有的高级语义和结构信息。在本文中，我们提出了EDITS，这是一个利用图像数据中的隐含文本语义来实现增强蒸馏的新型框架。首先，通过全局语义查询模块，将视觉语言模型（VLM）生成的外部文本与图像特征相融合，形成先验聚类缓冲区。然后，局部语义意识从缓冲区中选择代表性样本，构建图像和文本原型，后者是通过精心设计的提示来引导大型语言模型（LLM）而产生的。最终，通过扩散模型采用双重原型指导策略生成最终的合成数据集。大量实验证实了我们的方法的有效性。源代码可在<a target="_blank" rel="noopener" href="https://github.com/einsteinxia/EDITS%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/einsteinxia/EDITS找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13858v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了一种名为EDITS的新型数据集蒸馏框架，该框架通过利用图像数据中的隐含文本语义来实现增强的蒸馏效果。通过全球语义查询模块将外部文本与图像特征融合形成先验聚类缓冲区，然后使用局部语义意识从中选择代表性样本构建图像和文本原型，最后通过扩散模型生成合成数据集。该方法不仅提高了学习效率，还保持了模型性能。源代码可在GitHub上找到。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>EDITS框架通过融合外部文本和图像特征实现数据集蒸馏。</li>
<li>全球语义查询模块是EDITS的核心部分，负责将外部文本与图像特征结合形成先验聚类缓冲区。</li>
<li>局部语义意识从缓冲区中选择代表性样本，构建图像和文本原型。</li>
<li>Large Language Model (LLM)在生成文本原型时起到关键作用。</li>
<li>EDITS使用扩散模型生成合成数据集，这有助于提高学习效率并维持模型性能。</li>
<li>EDITS在实验中表现出良好的效果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13858">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_Diffusion Models/2509.13858v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_Diffusion Models/2509.13858v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_Diffusion Models/2509.13858v1/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_Diffusion Models/2509.13858v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Generative-Image-Coding-with-Diffusion-Prior"><a href="#Generative-Image-Coding-with-Diffusion-Prior" class="headerlink" title="Generative Image Coding with Diffusion Prior"></a>Generative Image Coding with Diffusion Prior</h2><p><strong>Authors:Jianhui Chang</strong></p>
<p>As generative technologies advance, visual content has evolved into a complex mix of natural and AI-generated images, driving the need for more efficient coding techniques that prioritize perceptual quality. Traditional codecs and learned methods struggle to maintain subjective quality at high compression ratios, while existing generative approaches face challenges in visual fidelity and generalization. To this end, we propose a novel generative coding framework leveraging diffusion priors to enhance compression performance at low bitrates. Our approach employs a pre-optimized encoder to generate generalized compressed-domain representations, integrated with the pretrained model’s internal features via a lightweight adapter and an attentive fusion module. This framework effectively leverages existing pretrained diffusion models and enables efficient adaptation to different pretrained models for new requirements with minimal retraining costs. We also introduce a distribution renormalization method to further enhance reconstruction fidelity. Extensive experiments show that our method (1) outperforms existing methods in visual fidelity across low bitrates, (2) improves compression performance by up to 79% over H.266&#x2F;VVC, and (3) offers an efficient solution for AI-generated content while being adaptable to broader content types. </p>
<blockquote>
<p>随着生成技术的进步，视觉内容已经演变为自然图像和AI生成图像的复杂混合体，这推动了需要更有效的编码技术，这些技术需要优先重视感知质量。传统编码器和学习方法在较高的压缩率下很难保持主观质量，而现有的生成方法则在视觉保真度和通用性方面面临挑战。为此，我们提出了一种利用扩散先验知识的新型生成编码框架，以提高在低码率下的压缩性能。我们的方法采用预优化的编码器生成通用的压缩域表示，通过轻量级适配器和注意力融合模块与预训练模型的内部特征相结合。该框架有效地利用了现有的预训练扩散模型，并能够实现针对不同预训练模型的高效适配，以满足新的需求且最小化再训练成本。我们还引入了一种分布归一化方法，以进一步提高重建保真度。大量实验表明，我们的方法（1）在低码率下在视觉保真度方面优于现有方法，（2）与H.266&#x2F;VVC相比，压缩性能提高了高达79%，（3）为AI生成内容提供了有效的解决方案，同时可适应更广泛的内容类型。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13768v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>随着生成技术的不断进步，视觉内容已演变为自然图像和AI生成图像的综合体，这促使我们需要更高效的编码技术，特别是在重视感知质量方面。本文提出了一种利用扩散先验的新型生成编码框架，以提高低比特率下的压缩性能。该框架采用预优化编码器生成通用压缩域表示，通过轻量级适配器和注意力融合模块与预训练模型的内特征相融合。此外，引入分布重归一化方法进一步提高重建保真度。实验表明，该方法在视觉保真度方面优于现有方法，在较低比特率下表现尤为出色，并可有效适应AI生成内容和其他更广泛的内容类型。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>生成技术的进步促使视觉内容演变为自然和AI生成图像的综合体。</li>
<li>需要更高效的编码技术来适应复杂的视觉内容，特别是重视感知质量。</li>
<li>提出了一种新型的生成编码框架，利用扩散先验提高低比特率下的压缩性能。</li>
<li>该框架采用预优化编码器生成通用压缩域表示，并与预训练模型融合。</li>
<li>引入分布重归一化方法来提高重建图像的保真度。</li>
<li>该方法在视觉保真度方面优于现有方法，特别是在低比特率下表现更出色。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13768">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_Diffusion Models/2509.13768v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_Diffusion Models/2509.13768v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_Diffusion Models/2509.13768v1/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_Diffusion Models/2509.13768v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_Diffusion Models/2509.13768v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_Diffusion Models/2509.13768v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Cross-Distribution-Diffusion-Priors-Driven-Iterative-Reconstruction-for-Sparse-View-CT"><a href="#Cross-Distribution-Diffusion-Priors-Driven-Iterative-Reconstruction-for-Sparse-View-CT" class="headerlink" title="Cross-Distribution Diffusion Priors-Driven Iterative Reconstruction for   Sparse-View CT"></a>Cross-Distribution Diffusion Priors-Driven Iterative Reconstruction for   Sparse-View CT</h2><p><strong>Authors:Haodong Li, Shuo Han, Haiyang Mao, Yu Shi, Changsheng Fang, Jianjia Zhang, Weiwen Wu, Hengyong Yu</strong></p>
<p>Sparse-View CT (SVCT) reconstruction enhances temporal resolution and reduces radiation dose, yet its clinical use is hindered by artifacts due to view reduction and domain shifts from scanner, protocol, or anatomical variations, leading to performance degradation in out-of-distribution (OOD) scenarios. In this work, we propose a Cross-Distribution Diffusion Priors-Driven Iterative Reconstruction (CDPIR) framework to tackle the OOD problem in SVCT. CDPIR integrates cross-distribution diffusion priors, derived from a Scalable Interpolant Transformer (SiT), with model-based iterative reconstruction methods. Specifically, we train a SiT backbone, an extension of the Diffusion Transformer (DiT) architecture, to establish a unified stochastic interpolant framework, leveraging Classifier-Free Guidance (CFG) across multiple datasets. By randomly dropping the conditioning with a null embedding during training, the model learns both domain-specific and domain-invariant priors, enhancing generalizability. During sampling, the globally sensitive transformer-based diffusion model exploits the cross-distribution prior within the unified stochastic interpolant framework, enabling flexible and stable control over multi-distribution-to-noise interpolation paths and decoupled sampling strategies, thereby improving adaptation to OOD reconstruction. By alternating between data fidelity and sampling updates, our model achieves state-of-the-art performance with superior detail preservation in SVCT reconstructions. Extensive experiments demonstrate that CDPIR significantly outperforms existing approaches, particularly under OOD conditions, highlighting its robustness and potential clinical value in challenging imaging scenarios. </p>
<blockquote>
<p>稀疏视图CT（SVCT）重建提高了时间分辨率并降低了辐射剂量，但其临床应用受到由视图减少、扫描仪、协议或解剖结构变化引起的域漂移所导致的伪影的阻碍，这导致在超出分布（OOD）的场景中出现性能下降。在这项工作中，我们提出了一种跨分布扩散先验驱动迭代重建（CDPIR）框架，以解决SVCT中的OOD问题。CDPIR将源于可扩展插值转换器（SiT）的跨分布扩散先验与基于模型的迭代重建方法相结合。具体来说，我们训练了一个SiT骨干网，这是扩散转换器（DiT）架构的扩展，以建立统一的随机插值框架，利用多个数据集之间的无分类器指导（CFG）。通过训练过程中随机丢弃条件并使用空嵌入，模型学会了特定域和跨域的先验知识，增强了其泛化能力。在采样过程中，全局敏感性的基于转换器的扩散模型在统一的随机插值框架内利用跨分布先验，实现对多分布到噪声插值路径和独立采样策略的灵活稳定控制，从而提高了对OOD重建的适应性。通过交替进行数据保真度和采样更新，我们的模型在SVCT重建中实现了最先进的性能，并以卓越的细节保留而脱颖而出。大量实验表明，CDPIR显著优于现有方法，特别是在OOD条件下，凸显了其在具有挑战性的成像场景中的稳健性和潜在的的临床价值。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13576v1">PDF</a> 11 pages, 8 figures, under reviewing of IEEE TMI</p>
<p><strong>摘要</strong></p>
<p>稀疏视图CT（SVCT）重建提高了时间分辨率并降低了辐射剂量，但其临床应用受到视图减少和来自扫描仪、协议或解剖结构变化导致的域漂移所产生的伪影的阻碍，这在超出分布范围（OOD）的场景中会导致性能下降。在此工作中，我们提出了一个跨分布扩散先验驱动迭代重建（CDPIR）框架来解决SVCT中的OOD问题。CDPIR结合了来自可扩展插值转换器（SiT）的跨分布扩散先验与基于模型的迭代重建方法。具体来说，我们训练了一个SiT骨干网，这是扩散转换器（DiT）架构的扩展，以建立一个统一的随机插值框架，利用跨多个数据集的Classifier-Free Guidance（CFG）。通过在训练期间随机丢弃条件嵌入作为null嵌入，模型可以学习特定领域和不特定领域的先验知识，从而提高泛化能力。在采样过程中，全局敏感性的基于转换器的扩散模型在统一的随机插值框架内利用跨分布先验，实现灵活稳定的控制多分布到噪声的插值路径和独立的采样策略，从而提高了对OOD重建的适应性。通过交替进行数据保真度和采样更新，我们的模型在SVCT重建中实现了最先进的性能表现，并出色地保留了细节。大量实验表明，CDPIR显著优于现有方法，特别是在OOD条件下，突显了其稳健性和在具有挑战性的成像场景中的潜在临床价值。</p>
<p><strong>要点</strong></p>
<ol>
<li>CDPIR框架解决了稀疏视图CT（SVCT）重建中的OOD问题。</li>
<li>结合了跨分布扩散先验与迭代重建方法。</li>
<li>使用可扩展插值转换器（SiT）训练模型以学习领域特定和领域不变的先验知识。</li>
<li>模型通过结合数据保真度和采样更新实现了先进性能。</li>
<li>CDPIR在SVCT重建中表现出卓越的细节保留能力。</li>
<li>实验结果显示CDPIR在OOD条件下显著优于现有方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13576">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_Diffusion Models/2509.13576v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_Diffusion Models/2509.13576v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_Diffusion Models/2509.13576v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="DEFT-VTON-Efficient-Virtual-Try-On-with-Consistent-Generalised-H-Transform"><a href="#DEFT-VTON-Efficient-Virtual-Try-On-with-Consistent-Generalised-H-Transform" class="headerlink" title="DEFT-VTON: Efficient Virtual Try-On with Consistent Generalised   H-Transform"></a>DEFT-VTON: Efficient Virtual Try-On with Consistent Generalised   H-Transform</h2><p><strong>Authors:Xingzi Xu, Qi Li, Shuwen Qiu, Julien Han, Karim Bouyarmane</strong></p>
<p>Diffusion models enable high-quality virtual try-on (VTO) with their established image synthesis abilities. Despite the extensive end-to-end training of large pre-trained models involved in current VTO methods, real-world applications often prioritize limited training and inference, serving, and deployment budgets for VTO. To solve this obstacle, we apply Doob’s h-transform efficient fine-tuning (DEFT) for adapting large pre-trained unconditional models for downstream image-conditioned VTO abilities. DEFT freezes the pre-trained model’s parameters and trains a small h-transform network to learn a conditional h-transform. The h-transform network allows training only 1.42 percent of the frozen parameters, compared to a baseline of 5.52 percent in traditional parameter-efficient fine-tuning (PEFT).   To further improve DEFT’s performance and decrease existing models’ inference time, we additionally propose an adaptive consistency loss. Consistency training distills slow but high-performing diffusion models into a fast one while retaining performance by enforcing consistencies along the inference path. Inspired by constrained optimization, instead of distillation, we combine the consistency loss and the denoising score matching loss in a data-adaptive manner for fine-tuning existing VTO models at a low cost. Empirical results show the proposed DEFT-VTON method achieves state-of-the-art performance on VTO tasks, with as few as 15 denoising steps, while maintaining competitive results. </p>
<blockquote>
<p>扩散模型凭借其成熟的图像合成能力，能够实现高质量的虚拟试穿（VTO）。尽管目前的VTO方法涉及对大型预训练模型进行端到端的广泛训练，但现实世界的应用往往优先考虑有限的训练、推理、服务和部署预算，用于VTO。为了解决这个问题，我们应用Doob的h-transform高效微调（DEFT）技术，以适应大型预训练无条件模型进行下游图像调节的VTO能力。DEFT冻结预训练模型的参数，并训练一个小型的h-transform网络来学习条件h-transform。与传统参数有效微调（PEFT）的基线相比，h-transform网络只允许冻结参数的1.42%进行训练，而基线则需要训练5.52%的参数。为了进一步提高DEFT的性能并减少现有模型的推理时间，我们还提出了一种自适应一致性损失。一致性训练将缓慢但高性能的扩散模型蒸馏为快速模型，同时通过执行推理路径上的一致性来保留性能。受约束优化的启发，我们不是采用蒸馏的方法，而是将一致性损失和去噪评分匹配损失以数据自适应的方式结合起来，以低成本微调现有的VTO模型。经验结果表明，所提出的DEFT-VTON方法在VTO任务上达到了最新技术水平的性能，仅使用15个去噪步骤，同时保持了竞争力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13506v1">PDF</a> Published in 2025 CVPR Workshop</p>
<p><strong>Summary</strong></p>
<p>本文介绍了使用扩散模型实现高质量虚拟试穿（VTO）的方法。为解决实际应用中训练、推理、部署预算有限的问题，采用Doob的h-transform高效微调（DEFT）技术，使大型预训练无条件模型适应下游图像条件VTO能力。DEFT通过冻结预训练模型参数并训练小型h-transform网络学习条件h-transform来解决此问题，仅训练1.42%的冻结参数，相较于传统参数效率微调（PEFT）的5.52%有显著优势。为进一步改善DEFT性能并减少现有模型的推理时间，提出了自适应一致性损失。一致性训练将缓慢但高性能的扩散模型转化为快速模型，同时保留性能，通过在推理路径上强制执行一致性来实现。通过结合一致性损失和去噪分数匹配损失，以数据自适应方式微调现有VTO模型，成本低，且只需少数几步去噪即可达到最佳性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型能够实现高质量的虚拟试穿（VTO）。</li>
<li>当前VTO方法虽然涉及大型预训练模型的端到端训练，但实际应用中更强调有限的训练、推理、部署预算。</li>
<li>Doob的h-transform高效微调（DEFT）技术被应用于适应下游图像条件VTO能力，通过冻结预训练模型的大部分参数并仅训练小部分h-transform网络来解决这一问题。</li>
<li>DEFT相较于传统参数效率微调（PEFT）有更优的参数训练效率。</li>
<li>为提高DEFT性能并减少模型推理时间，提出了自适应一致性损失。</li>
<li>一致性训练能将慢但高性能的扩散模型转化为快速模型，同时保持性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13506">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_Diffusion Models/2509.13506v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_Diffusion Models/2509.13506v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_Diffusion Models/2509.13506v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_Diffusion Models/2509.13506v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_Diffusion Models/2509.13506v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="DPDEdit-Detail-Preserved-Diffusion-Models-for-Multimodal-Fashion-Image-Editing"><a href="#DPDEdit-Detail-Preserved-Diffusion-Models-for-Multimodal-Fashion-Image-Editing" class="headerlink" title="DPDEdit: Detail-Preserved Diffusion Models for Multimodal Fashion Image   Editing"></a>DPDEdit: Detail-Preserved Diffusion Models for Multimodal Fashion Image   Editing</h2><p><strong>Authors:Xiaolong Wang, Zhi-Qi Cheng, Jue Wang, Xiaojiang Peng</strong></p>
<p>Fashion image editing is a crucial tool for designers to convey their creative ideas by visualizing design concepts interactively. Current fashion image editing techniques, though advanced with multimodal prompts and powerful diffusion models, often struggle to accurately identify editing regions and preserve the desired garment texture detail. To address these challenges, we introduce a new multimodal fashion image editing architecture based on latent diffusion models, called Detail-Preserved Diffusion Models (DPDEdit). DPDEdit guides the fashion image generation of diffusion models by integrating text prompts, region masks, human pose images, and garment texture images. To precisely locate the editing region, we first introduce Grounded-SAM to predict the editing region based on the user’s textual description, and then combine it with other conditions to perform local editing. To transfer the detail of the given garment texture into the target fashion image, we propose a texture injection and refinement mechanism. Specifically, this mechanism employs a decoupled cross-attention layer to integrate textual descriptions and texture images, and incorporates an auxiliary U-Net to preserve the high-frequency details of generated garment texture. Additionally, we extend the VITON-HD dataset using a multimodal large language model to generate paired samples with texture images and textual descriptions. Extensive experiments show that our DPDEdit outperforms state-of-the-art methods in terms of image fidelity and coherence with the given multimodal inputs. </p>
<blockquote>
<p>时尚图像编辑是设计师通过交互式可视化设计概念来表达创意想法的重要工具。尽管当前的时尚图像编辑技术已经具备多模式提示和强大的扩散模型，但它们仍然难以准确识别编辑区域并保留所需的服装纹理细节。为了应对这些挑战，我们基于潜在扩散模型引入了一种新的多模式时尚图像编辑架构，称为细节保留扩散模型（DPDEdit）。DPDEdit通过整合文本提示、区域掩码、人体姿势图像和服装纹理图像来引导扩散模型的时尚图像生成。为了精确定位编辑区域，我们首先引入基于用户文本描述的Grounded-SAM进行预测编辑区域，然后结合其他条件进行局部编辑。为了将给定的服装纹理细节转移到目标时尚图像中，我们提出了纹理注入和细化机制。具体来说，该机制采用解耦的交叉注意层来整合文本描述和纹理图像，并引入辅助U-Net来保留生成服装纹理的高频细节。此外，我们使用多模式大型语言模型扩展了VITON-HD数据集，生成具有纹理图像和文本描述的双样本。大量实验表明，我们的DPDEdit在图像保真度和与给定多模式输入的连贯性方面优于现有先进技术。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.01086v3">PDF</a> 13 pages,12 figures</p>
<p><strong>Summary</strong><br>     基于潜在扩散模型的多模态时尚图像编辑架构DPDEdit，通过结合文本提示、区域掩膜、人体姿态图像和服装纹理图像，指导扩散模型的时尚图像生成。提出Grounded-SAM精确定位编辑区域，并结合其他条件进行局部编辑。通过纹理注入和细化机制，将给定的服装纹理细节转移到目标时尚图像中。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>时尚图像编辑是设计师传达创意的重要工具，当前技术面临准确识别编辑区域和保留服装纹理细节的挑战。</li>
<li>引入基于潜在扩散模型的多模态时尚图像编辑架构DPDEdit，结合多种输入进行时尚图像生成。</li>
<li>DPDEdit通过Grounded-SAM预测编辑区域，实现精确定位。</li>
<li>纹理注入和细化机制采用解耦交叉注意层来集成文本描述和纹理图像，并辅以U-Net保留生成服装纹理的高频细节。</li>
<li>DPDEdit扩展了VITON-HD数据集，利用多模态大型语言模型生成配对样本（纹理图像和文本描述）。</li>
<li>实验表明，DPDEdit在图像保真度和与给定多模态输入的一致性方面优于现有方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.01086">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_Diffusion Models/2409.01086v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_Diffusion Models/2409.01086v3/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_Diffusion Models/2409.01086v3/page_1_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_Diffusion Models/2409.01086v3/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_Diffusion Models/2409.01086v3/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_Diffusion Models/2409.01086v3/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_Diffusion Models/2409.01086v3/page_5_0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-19/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-19/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-19/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_医学图像/2509.13629v1/page_0_0.jpg" class="responsive-img" alt="医学图像">
                        
                        <span class="card-title">医学图像</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学图像 方向最新论文已更新，请持续关注 Update in 2025-09-19  LamiGauss Pitching Radiative Gaussian for Sparse-View X-ray   Laminography Reconstruction
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-19
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    医学图像
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">医学图像</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-19/NeRF/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_NeRF/2509.13686v1/page_0_0.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF 方向最新论文已更新，请持续关注 Update in 2025-09-19  RF-LSCM Pushing Radiance Fields to Multi-Domain Localized Statistical   Channel Modeling for Cellular Network Optimization
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-19
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">30191.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
