<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-05-09  Efficient Flow Matching using Latent Variables">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-de804d728ada58b8fa4a97244451761d.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-05-09
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-10
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    10k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    40 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-05-09-更新"><a href="#2025-05-09-更新" class="headerlink" title="2025-05-09 更新"></a>2025-05-09 更新</h1><h2 id="Efficient-Flow-Matching-using-Latent-Variables"><a href="#Efficient-Flow-Matching-using-Latent-Variables" class="headerlink" title="Efficient Flow Matching using Latent Variables"></a>Efficient Flow Matching using Latent Variables</h2><p><strong>Authors:Anirban Samaddar, Yixuan Sun, Viktor Nilsson, Sandeep Madireddy</strong></p>
<p>Flow matching models have shown great potential in image generation tasks among probabilistic generative models. Building upon the ideas of continuous normalizing flows, flow matching models generalize the transport path of the diffusion models from a simple prior distribution to the data. Most flow matching models in the literature do not explicitly model the underlying structure&#x2F;manifold in the target data when learning the flow from a simple source distribution like the standard Gaussian. This leads to inefficient learning, especially for many high-dimensional real-world datasets, which often reside in a low-dimensional manifold. Existing strategies of incorporating manifolds, including data with underlying multi-modal distribution, often require expensive training and hence frequently lead to suboptimal performance. To this end, we present \texttt{Latent-CFM}, which provides simplified training&#x2F;inference strategies to incorporate multi-modal data structures using pretrained deep latent variable models. Through experiments on multi-modal synthetic data and widely used image benchmark datasets, we show that \texttt{Latent-CFM} exhibits improved generation quality with significantly less training ($\sim 50%$ less in some cases) and computation than state-of-the-art flow matching models. Using a 2d Darcy flow dataset, we demonstrate that our approach generates more physically accurate samples than competitive approaches. In addition, through latent space analysis, we demonstrate that our approach can be used for conditional image generation conditioned on latent features. </p>
<blockquote>
<p>流体匹配模型在概率生成模型中的图像生成任务中表现出了巨大的潜力。基于连续归一化流的思想，流体匹配模型将扩散模型的传输路径从简单的先验分布推广到数据。文献中的大多数流体匹配模型在学习从标准高斯等简单源分布到目标的流体时，并没有显式地建模目标数据的底层结构&#x2F;流形。这导致了学习的不高效，特别是对于许多高维的现实世界数据集，它们经常居住在低维流形中。现有的结合流形的策略，包括具有潜在多元分布的数据，通常需要昂贵的训练成本，因此经常导致次优性能。为此，我们提出了“潜在CFM”，它提供了简化的训练&#x2F;推理策略，利用预训练的深度潜在变量模型来结合多模态数据结构。通过对多模态合成数据和广泛使用的图像基准数据集进行实验，我们证明了“潜在CFM”在生成质量上的提高，并且在训练量和计算量上大大减少了（在某些情况下减少了约50%）相比于最先进的流体匹配模型。使用二维达西流数据集，我们证明了我们的方法生成的样本比竞争方法更具有物理准确性。此外，通过潜在空间分析，我们证明了我们的方法可用于基于潜在特征的条件图像生成。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.04486v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>流匹配模型在概率生成模型中在图像生成任务中显示出巨大潜力。基于连续归一化流的思想，流匹配模型将数据从简单先验分布推广到扩散模型的传输路径。现有文献中的大多数流匹配模型在学习从简单源分布（如标准高斯分布）开始的流时，并没有显式地建模目标数据的底层结构&#x2F;流形。这导致了对许多高维现实数据集的无效学习，尤其是那些经常处于低维流形中的数据集。尽管现有的结合流形策略包括具有潜在多模态分布的数据，但其训练成本高昂且经常导致性能不佳。为此，我们提出了基于预训练的深度潜在变量模型的“Latent-CFM”，它可以通过简化训练和推理策略来结合多模态数据结构。通过实验表明，Latent-CFM在生成质量上有所提升，并且在某些情况下训练时间减少了约一半，计算效率也显著提高。此外，我们的方法还能用于基于潜在特征的条件图像生成。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>流匹配模型在图像生成任务中具有潜力。</li>
<li>流匹配模型基于连续归一化流推广数据传输路径。</li>
<li>现有流匹配模型未显式建模目标数据的底层结构，导致对高维现实数据集的学习效率低下。</li>
<li>Latent-CFM通过简化训练和推理策略，能够结合多模态数据结构。</li>
<li>Latent-CFM提高了生成质量，并显著减少了训练时间和计算成本。</li>
<li>Latent-CFM在物理样本生成方面表现出更准确的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.04486">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-1af039e694d9960d94c8d7f5dd208133.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-830c52f8cad95b8a4cefd06af99e3cb0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-90ce1230cc7e61483e4156bbd8a79f3e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a26bcb5af701278146d790add613e39d.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="CountDiffusion-Text-to-Image-Synthesis-with-Training-Free-Counting-Guidance-Diffusion"><a href="#CountDiffusion-Text-to-Image-Synthesis-with-Training-Free-Counting-Guidance-Diffusion" class="headerlink" title="CountDiffusion: Text-to-Image Synthesis with Training-Free   Counting-Guidance Diffusion"></a>CountDiffusion: Text-to-Image Synthesis with Training-Free   Counting-Guidance Diffusion</h2><p><strong>Authors:Yanyu Li, Pencheng Wan, Liang Han, Yaowei Wang, Liqiang Nie, Min Zhang</strong></p>
<p>Stable Diffusion has advanced text-to-image synthesis, but training models to generate images with accurate object quantity is still difficult due to the high computational cost and the challenge of teaching models the abstract concept of quantity. In this paper, we propose CountDiffusion, a training-free framework aiming at generating images with correct object quantity from textual descriptions. CountDiffusion consists of two stages. In the first stage, an intermediate denoising result is generated by the diffusion model to predict the final synthesized image with one-step denoising, and a counting model is used to count the number of objects in this image. In the second stage, a correction module is used to correct the object quantity by changing the attention map of the object with universal guidance. The proposed CountDiffusion can be plugged into any diffusion-based text-to-image (T2I) generation models without further training. Experiment results demonstrate the superiority of our proposed CountDiffusion, which improves the accurate object quantity generation ability of T2I models by a large margin. </p>
<blockquote>
<p>Stable Diffusion已经实现了先进的文本到图像合成技术，但由于计算成本高昂以及教授模型理解数量这一抽象概念具有挑战性，训练模型以生成具有准确对象数量的图像仍然很困难。在本文中，我们提出了CountDiffusion，这是一个无需训练即可生成正确对象数量的图像的目标框架。CountDiffusion分为两个阶段。在第一阶段，通过扩散模型生成中间去噪结果，以通过一步去噪预测最终的合成图像，并使用计数模型计算图像中的对象数量。在第二阶段，使用校正模块通过改变对象的注意力图进行通用指导来校正对象数量。所提出的CountDiffusion可以插入任何基于扩散的文本到图像（T2I）生成模型中而无需进一步训练。实验结果表明，我们所提出的CountDiffusion具有显著优势，能大幅度提高T2I模型生成准确对象数量的能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.04347v1">PDF</a> 8 pages, 9 figures, 3 tables</p>
<p><strong>Summary</strong></p>
<p>稳定扩散已提升了文本到图像的合成技术，但生成具有准确对象数量的图像仍存在困难，因涉及高计算成本及教授模型抽象数量概念的挑战。本文提出CountDiffusion，一种无需训练、旨在从文本描述生成具有正确对象数量的图像框架。CountDiffusion分两个阶段，第一阶段利用扩散模型生成中间去噪结果，通过一步去噪预测最终合成图像，并利用计数模型计算图像中对象的数量。第二阶段采用校正模块，通过改变对象的注意力图进行通用指导，以校正对象数量。CountDiffusion可无缝集成任何基于扩散的文本到图像生成模型，无需进一步训练。实验结果表明，CountDiffusion大幅提升了文本到图像模型的准确对象数量生成能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>稳定扩散虽已提升文本到图像的合成技术，但生成具有准确对象数量的图像仍然具有挑战。</li>
<li>CountDiffusion是一种无需训练的框架，可从文本描述生成具有正确对象数量的图像。</li>
<li>CountDiffusion分为两个阶段：第一阶段预测图像并计算对象数量，第二阶段校正对象数量。</li>
<li>CountDiffusion可无缝集成到任何扩散基础的文本到图像生成模型中。</li>
<li>CountDiffusion通过改变对象的注意力图进行通用指导以校正对象数量。</li>
<li>实验结果表明CountDiffusion显著提高了文本到图像模型的准确对象数量生成能力。</li>
<li>该方法面对的挑战包括高计算成本和教授模型抽象数量概念的困难。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.04347">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-851d1b1a21c2aeea5dce2bd7f6fb9e98.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-20c58d51d38062902f0153e89a79a4b0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f865d89608174b6488f53229f6863d73.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-77a91e3def149e958df452124849708d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8895e4dd82e8e902e13a6ecf57f8e0ec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-649f403c50bd4b6ca4078e5d656b67a6.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="TS-Diff-Two-Stage-Diffusion-Model-for-Low-Light-RAW-Image-Enhancement"><a href="#TS-Diff-Two-Stage-Diffusion-Model-for-Low-Light-RAW-Image-Enhancement" class="headerlink" title="TS-Diff: Two-Stage Diffusion Model for Low-Light RAW Image Enhancement"></a>TS-Diff: Two-Stage Diffusion Model for Low-Light RAW Image Enhancement</h2><p><strong>Authors:Yi Li, Zhiyuan Zhang, Jiangnan Xia, Jianghan Cheng, Qilong Wu, Junwei Li, Yibin Tian, Hui Kong</strong></p>
<p>This paper presents a novel Two-Stage Diffusion Model (TS-Diff) for enhancing extremely low-light RAW images. In the pre-training stage, TS-Diff synthesizes noisy images by constructing multiple virtual cameras based on a noise space. Camera Feature Integration (CFI) modules are then designed to enable the model to learn generalizable features across diverse virtual cameras. During the aligning stage, CFIs are averaged to create a target-specific CFI$^T$, which is fine-tuned using a small amount of real RAW data to adapt to the noise characteristics of specific cameras. A structural reparameterization technique further simplifies CFI$^T$ for efficient deployment. To address color shifts during the diffusion process, a color corrector is introduced to ensure color consistency by dynamically adjusting global color distributions. Additionally, a novel dataset, QID, is constructed, featuring quantifiable illumination levels and a wide dynamic range, providing a comprehensive benchmark for training and evaluation under extreme low-light conditions. Experimental results demonstrate that TS-Diff achieves state-of-the-art performance on multiple datasets, including QID, SID, and ELD, excelling in denoising, generalization, and color consistency across various cameras and illumination levels. These findings highlight the robustness and versatility of TS-Diff, making it a practical solution for low-light imaging applications. Source codes and models are available at <a target="_blank" rel="noopener" href="https://github.com/CircccleK/TS-Diff">https://github.com/CircccleK/TS-Diff</a> </p>
<blockquote>
<p>本文提出了一种新型的两阶段扩散模型（TS-Diff），用于增强极低光照的RAW图像。在预训练阶段，TS-Diff通过构建基于噪声空间的多个虚拟相机来合成噪声图像。随后设计了相机特征融合（CFI）模块，使模型能够在各种虚拟相机上学习可推广的特征。在对齐阶段，CFIs被平均化以创建目标特定的CFI$^T$，并使用少量真实RAW数据进行微调，以适应特定相机的噪声特性。一种结构再参数化技术进一步简化了CFI$^T$，以便有效部署。为了解决扩散过程中的颜色偏移问题，引入了一种颜色校正器，通过动态调整全局颜色分布来确保颜色一致性。此外，还构建了一个新型数据集QID，具有可量化的照明水平和宽动态范围，为极端低光照条件下的训练和评估提供了全面的基准测试。实验结果证明，TS-Diff在多数据集（包括QID、SID和ELD）上达到了最新技术水平，在降噪、通用性和跨不同相机和光照级别的颜色一致性方面表现出色。这些发现突出了TS-Diff的稳健性和多功能性，使其成为低光成像应用的实用解决方案。相关源代码和模型可在<a target="_blank" rel="noopener" href="https://github.com/CircccleK/TS-Diff">https://github.com/CircccleK/TS-Diff</a> 获得。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.04281v1">PDF</a> International Joint Conference on Neural Networks (IJCNN)</p>
<p><strong>摘要</strong></p>
<p>本文提出了一种新型的两阶段扩散模型（TS-Diff），用于增强极低光照下的RAW图像。在预训练阶段，TS-Diff通过构建多个基于噪声空间的虚拟相机合成噪声图像。设计了相机特征融合（CFI）模块，使模型能够在不同的虚拟相机上学习通用特征。在调整阶段，CFIs经过平均化处理以创建目标特定的CFI^T，并使用少量真实RAW数据进行微调，以适应特定相机的噪声特性。通过结构再参数化技术进一步简化CFI^T，以便有效部署。为解决扩散过程中的色彩偏移问题，引入了色彩校正器，通过动态调整全局色彩分布来确保色彩一致性。此外，还构建了一个新型数据集QID，具有可量化的照明水平和宽动态范围，为极端低光照条件下的训练和评估提供了全面的基准。实验结果证明，TS-Diff在多个数据集（包括QID、SID和ELD）上实现了最先进的性能，在降噪、通用性和色彩一致性方面表现出色，适用于各种相机和光照水平。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>TS-Diff模型通过构建虚拟相机合成噪声图像，为极低光照下的RAW图像增强提供了新方法。</li>
<li>引入相机特征融合（CFI）模块，使模型能在不同的虚拟相机上学习通用特征。</li>
<li>通过创建目标特定的CFI^T并微调，模型能适应特定相机的噪声特性。</li>
<li>结构再参数化技术简化了CFI^T的部署。</li>
<li>引入色彩校正器来解决扩散过程中的色彩偏移问题，确保色彩一致性。</li>
<li>新型数据集QID的构建为极端低光照条件下的训练和评估提供了基准。</li>
<li>TS-Diff在多个数据集上实现了最先进的性能，尤其在降噪、通用性和色彩一致性方面表现突出。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.04281">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-4ea664589c338e4de673d7da9f3b9ad8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e5650b5d018c0a9f982929d2ccdc5170.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6169a0f2068e61453e296091bf0eecab.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-43c4f8ae4bdb7cc37ca7e8f6c53f8afb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5f3db9ddd9821a6a9808992ed986cd88.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6073e4edd7b7b98722fce716c0d7950c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b7816f58b1588cba03dded873ec5c4a5.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Ming-Lite-Uni-Advancements-in-Unified-Architecture-for-Natural-Multimodal-Interaction"><a href="#Ming-Lite-Uni-Advancements-in-Unified-Architecture-for-Natural-Multimodal-Interaction" class="headerlink" title="Ming-Lite-Uni: Advancements in Unified Architecture for Natural   Multimodal Interaction"></a>Ming-Lite-Uni: Advancements in Unified Architecture for Natural   Multimodal Interaction</h2><p><strong>Authors:Inclusion AI, Biao Gong, Cheng Zou, Dandan Zheng, Hu Yu, Jingdong Chen, Jianxin Sun, Junbo Zhao, Jun Zhou, Kaixiang Ji, Lixiang Ru, Libin Wang, Qingpei Guo, Rui Liu, Weilong Chai, Xinyu Xiao, Ziyuan Huang</strong></p>
<p>We introduce Ming-Lite-Uni, an open-source multimodal framework featuring a newly designed unified visual generator and a native multimodal autoregressive model tailored for unifying vision and language. Specifically, this project provides an open-source implementation of the integrated MetaQueries and M2-omni framework, while introducing the novel multi-scale learnable tokens and multi-scale representation alignment strategy. By leveraging a fixed MLLM and a learnable diffusion model, Ming-Lite-Uni enables native multimodal AR models to perform both text-to-image generation and instruction based image editing tasks, expanding their capabilities beyond pure visual understanding. Our experimental results demonstrate the strong performance of Ming-Lite-Uni and illustrate the impressive fluid nature of its interactive process. All code and model weights are open-sourced to foster further exploration within the community. Notably, this work aligns with concurrent multimodal AI milestones - such as ChatGPT-4o with native image generation updated in March 25, 2025 - underscoring the broader significance of unified models like Ming-Lite-Uni on the path toward AGI. Ming-Lite-Uni is in alpha stage and will soon be further refined. </p>
<blockquote>
<p>我们介绍了Ming-Lite-Uni，这是一个开源的多模式框架，具有新设计的统一视觉生成器和针对视觉和语言的融合量身定制的本土多模式自回归模型。具体来说，此项目提供了集成MetaQueries和M2-omni框架的开源实现，同时引入了新型的多尺度可学习令牌和多尺度表示对齐策略。通过利用固定的MLLM和可学习的扩散模型，Ming-Lite-Uni使本土多模式自回归模型能够执行文本到图像生成和基于指令的图像编辑任务，扩展了其超越纯视觉理解的能力。我们的实验结果证明了Ming-Lite-Uni的强大性能，并展示了其交互过程的惊人流畅性。所有代码和模型权重均开源，以促进社区内的进一步探索。值得注意的是，这项工作与同期的多模式人工智能里程碑事件相符，如2025年3月25日更新的具有原生图像生成的ChatGPT-4o，突显出像Ming-Lite-Uni这样的统一模型在通往人工智能通用性（AGI）道路上的重要性。目前Ming-Lite-Uni处于Alpha阶段，未来将会进一步完善。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.02471v2">PDF</a> <a target="_blank" rel="noopener" href="https://github.com/inclusionAI/Ming/tree/main/Ming-unify">https://github.com/inclusionAI/Ming/tree/main/Ming-unify</a></p>
<p><strong>Summary</strong></p>
<p>本文介绍了Ming-Lite-Uni这一开源多模态框架，其特点为全新设计的统一视觉生成器以及针对视觉和语言统一化的本地多模态自回归模型。该项目实现了集成MetaQueries和M2-omni框架的开源实现，引入新型多尺度可学习令牌和多尺度表示对齐策略。利用固定的MLLM和可学习的扩散模型，Ming-Lite-Uni使本地多模态AR模型能够执行文本到图像生成和基于指令的图像编辑任务，超越了纯视觉理解的能力。实验结果证明了Ming-Lite-Uni的强大性能，其交互过程表现出令人印象深刻的流畅性。所有代码和模型权重均开源，以促进社区内的进一步探索。该工作与当前的多元模态人工智能里程碑（如ChatGPT-4o等）相吻合，显示出统一模型在迈向人工智能通用性过程中的重要性。Ming-Lite-Uni尚处于Alpha阶段，未来将有进一步的改进。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Ming-Lite-Uni是一个开源多模态框架，集成了视觉和语言处理功能。</li>
<li>它采用统一视觉生成器和本地多模态自回归模型设计。</li>
<li>该项目实现了MetaQueries和M2-omni框架的集成。</li>
<li>引入新型的多尺度可学习令牌和多尺度表示对齐策略是该框架的创新点。</li>
<li>Ming-Lite-Uni支持文本到图像生成以及基于指令的图像编辑任务。</li>
<li>该框架的实验结果表现优秀，与当前的多元模态人工智能里程碑相符。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.02471">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-06380fed336a1af539b00aeae9b6b812.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-161e89071d69ff9ea19ef921d96c9afe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4250c4cfea42b43946ce25636b1e44a1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a12669519da0fb4b4b9ba0b26683f33d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-29c7288bc0de2e28b2c88cd12d144e21.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d019031236945e165e3541d7f4f6b1a7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5d2bba3d34902481f35e1b20940b3e5c.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Probability-Density-Geodesics-in-Image-Diffusion-Latent-Space"><a href="#Probability-Density-Geodesics-in-Image-Diffusion-Latent-Space" class="headerlink" title="Probability Density Geodesics in Image Diffusion Latent Space"></a>Probability Density Geodesics in Image Diffusion Latent Space</h2><p><strong>Authors:Qingtao Yu, Jaskirat Singh, Zhaoyuan Yang, Peter Henry Tu, Jing Zhang, Hongdong Li, Richard Hartley, Dylan Campbell</strong></p>
<p>Diffusion models indirectly estimate the probability density over a data space, which can be used to study its structure. In this work, we show that geodesics can be computed in diffusion latent space, where the norm induced by the spatially-varying inner product is inversely proportional to the probability density. In this formulation, a path that traverses a high density (that is, probable) region of image latent space is shorter than the equivalent path through a low density region. We present algorithms for solving the associated initial and boundary value problems and show how to compute the probability density along the path and the geodesic distance between two points. Using these techniques, we analyze how closely video clips approximate geodesics in a pre-trained image diffusion space. Finally, we demonstrate how these techniques can be applied to training-free image sequence interpolation and extrapolation, given a pre-trained image diffusion model. </p>
<blockquote>
<p>扩散模型通过间接估计数据空间的概率密度，可用于研究其结构。在这项工作中，我们展示了如何在扩散潜在空间中计算测地线，其中由空间变化的内积引起的范数与概率密度成反比。在这种表述中，遍历图像潜在空间的高密度（即可能的）区域的路径比通过低密度区域的等效路径更短。我们提出了解决相关初始值和边界值问题的算法，并展示了如何计算路径上的概率密度以及两点之间的测地距离。使用这些技术，我们分析了视频剪辑在预训练的图像扩散空间中如何近似测地线。最后，我们展示了如何在给定预训练的图像扩散模型的情况下，将这些技术应用于无训练图像序列的插值和外推。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.06675v2">PDF</a> CVPR2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了扩散模型在数据空间概率密度估计方面的应用，并展示了如何在扩散潜在空间中进行测地线计算。研究发现，由空间变化内积诱导的范数与概率密度成反比，高密度区域的路径比低密度区域的路径短。本文提出了解决相关初始和边界值问题的算法，并展示了如何计算路径上的概率密度和两点之间的测地距离。此外，本文还分析了视频剪辑在预训练图像扩散空间中的近似测地线路情况，并展示了如何将该技术应用于训练图像序列的插值与外延预测。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型可估计数据空间的概率密度。</li>
<li>在扩散潜在空间中进行测地线计算。</li>
<li>空间变化内积诱导的范数与概率密度成反比。</li>
<li>高密度区域路径短于低密度区域路径。</li>
<li>提出了解决初始和边界值问题的算法。</li>
<li>分析了视频剪辑在预训练图像扩散空间中的近似测地线路情况。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.06675">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-c7330cbfd513e3c1cef2c59e69ded84d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1d06b122adb1825e79c3a16d4ba9b898.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b296e7c1379b1ae87ad2cd2a3b809087.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f34efe72965049724f3fa2baf72f5e30.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-652fbb387a0eea35ddddc517bb64e55a.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Generative-Detail-Enhancement-for-Physically-Based-Materials"><a href="#Generative-Detail-Enhancement-for-Physically-Based-Materials" class="headerlink" title="Generative Detail Enhancement for Physically Based Materials"></a>Generative Detail Enhancement for Physically Based Materials</h2><p><strong>Authors:Saeed Hadadan, Benedikt Bitterli, Tizian Zeltner, Jan Novák, Fabrice Rousselle, Jacob Munkberg, Jon Hasselgren, Bartlomiej Wronski, Matthias Zwicker</strong></p>
<p>We present a tool for enhancing the detail of physically based materials using an off-the-shelf diffusion model and inverse rendering. Our goal is to enhance the visual fidelity of materials with detail that is often tedious to author, by adding signs of wear, aging, weathering, etc. As these appearance details are often rooted in real-world processes, we leverage a generative image model trained on a large dataset of natural images with corresponding visuals in context. Starting with a given geometry, UV mapping, and basic appearance, we render multiple views of the object. We use these views, together with an appearance-defining text prompt, to condition a diffusion model. The details it generates are then backpropagated from the enhanced images to the material parameters via inverse differentiable rendering. For inverse rendering to be successful, the generated appearance has to be consistent across all the images. We propose two priors to address the multi-view consistency of the diffusion model. First, we ensure that the initial noise that seeds the diffusion process is itself consistent across views by integrating it from a view-independent UV space. Second, we enforce geometric consistency by biasing the attention mechanism via a projective constraint so that pixels attend strongly to their corresponding pixel locations in other views. Our approach does not require any training or finetuning of the diffusion model, is agnostic of the material model used, and the enhanced material properties, i.e., 2D PBR textures, can be further edited by artists. This project is available at <a target="_blank" rel="noopener" href="https://generative-detail.github.io/">https://generative-detail.github.io</a>. </p>
<blockquote>
<p>我们提出了一种利用现成的扩散模型和逆向渲染技术，提高基于物理材质细节的工具。我们的目标是通过添加磨损、老化、风化等迹象，提高材质的视觉保真度和细节。由于这些外观细节往往源于现实世界的过程，我们利用在大规模自然图像数据集上训练的生成图像模型，以及相应的上下文视觉。从给定的几何形状、UV贴图和基本外观开始，我们渲染对象的多个视图。我们使用这些视图以及定义外观的文本提示，对扩散模型进行条件化处理。它生成的细节然后会从增强图像反向传播到材质参数，通过逆向可微分渲染。为了逆向渲染成功，生成的外观必须在所有图像中保持一致。我们提出了两种先验知识来解决扩散模型的多视图一致性。首先，我们通过从与视图无关的UV空间进行积分，确保种子扩散过程的初始噪声本身在不同视图中是一致的。其次，我们通过投影约束来引导注意力机制，强制几何一致性，使像素强烈关注其他视图中的对应像素位置。我们的方法不需要对扩散模型进行任何训练或微调，不受所使用的材料模型的影响，并且增强的材料属性（即2D PBR纹理）可以进一步由艺术家进行编辑。此项目在<a target="_blank" rel="noopener" href="https://generative-detail.github.io上提供的./">https://generative-detail.github.io上提供。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.13994v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种利用现成的扩散模型和逆向渲染技术提升物理基础材料的细节丰富度的工具。其目标是通过添加磨损、老化、风化等迹象，提高材料的视觉逼真度，这些外观细节通常难以制作。该研究利用大型自然图像数据集训练的生成图像模型，结合物体的多个视图和定义外观的文本提示，来调控扩散模型。生成的细节通过逆向可微渲染从增强图像传播回材料参数。该研究提出了两种优先策略，以确保扩散模型在多视图一致性方面的成功。一是确保扩散过程初始噪声在视图间是一致，通过从与视图无关的UV空间进行整合实现；二是通过投影约束引导注意力机制，实现几何一致性。该方法无需对扩散模型进行任何训练或微调，对使用的材料模型具有通用性，且增强的材料属性（如2D PBR纹理）可进一步由艺术家编辑。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>介绍了一种利用扩散模型和逆向渲染增强物理基础材料细节的方法。</li>
<li>方法的目的是提高材料的视觉逼真度，特别是添加磨损、老化、风化等细节。</li>
<li>利用生成图像模型和对物体的多个视图以及定义外观的文本提示来调控扩散模型。</li>
<li>通过逆向可微渲染将生成的细节从增强图像传播回材料参数。</li>
<li>提出了两种策略确保多视图一致性：整合UV空间的初始噪声和通过投影约束引导注意力机制。</li>
<li>该方法无需对扩散模型进行训练或微调，适用于各种材料模型。</li>
<li>增强的材料属性（如2D PBR纹理）可由艺术家进一步编辑。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.13994">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b0c35c2cdedd5eb7bd674e56bdc63dbb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a17b6d434a27dd49478953ebe89c0df8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0ad98893ef6e615a8ab604040e3cf077.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-65e79c3d53d342349f547588fe3e0e31.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d7c4c877c2bbf02fd46d980426f0f050.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Efficiency-Meets-Fidelity-A-Novel-Quantization-Framework-for-Stable-Diffusion"><a href="#Efficiency-Meets-Fidelity-A-Novel-Quantization-Framework-for-Stable-Diffusion" class="headerlink" title="Efficiency Meets Fidelity: A Novel Quantization Framework for Stable   Diffusion"></a>Efficiency Meets Fidelity: A Novel Quantization Framework for Stable   Diffusion</h2><p><strong>Authors:Shuaiting Li, Juncan Deng, Zeyu Wang, Kedong Xu, Rongtao Deng, Hong Gu, Haibin Shen, Kejie Huang</strong></p>
<p>Text-to-image generation via Stable Diffusion models (SDM) have demonstrated remarkable capabilities. However, their computational intensity, particularly in the iterative denoising process, hinders real-time deployment in latency-sensitive applications. While Recent studies have explored post-training quantization (PTQ) and quantization-aware training (QAT) methods to compress Diffusion models, existing methods often overlook the consistency between results generated by quantized models and those from floating-point models. This consistency is paramount for professional applications where both efficiency and output reliability are essential. To ensure that quantized SDM generates high-quality and consistent images, we propose an efficient quantization framework for SDM. Our framework introduces a Serial-to-Parallel pipeline that simultaneously maintains training-inference consistency and ensures optimization stability. Building upon this foundation, we further develop several techniques including multi-timestep activation quantization, time information precalculation, inter-layer distillation, and selective freezing, to achieve high-fidelity generation in comparison to floating-point models while maintaining quantization efficiency.   Through comprehensive evaluation across multiple Stable Diffusion variants (v1-4, v2-1, XL 1.0, and v3), our method demonstrates superior performance over state-of-the-art approaches with shorter training times. Under W4A8 quantization settings, we achieve significant improvements in both distribution similarity and visual fidelity, while preserving a high image quality. </p>
<blockquote>
<p>通过稳定扩散模型（SDM）进行的文本到图像生成已经显示出显著的能力。然而，其计算强度，特别是在迭代去噪过程中，阻碍了其在延迟敏感应用中的实时部署。虽然最近的研究已经探索了训练后量化（PTQ）和量化感知训练（QAT）方法来压缩扩散模型，但现有方法往往忽视了量化模型生成的结果与浮点模型生成的结果之间的一致性。对于既需要效率又需要输出可靠性的专业应用而言，这种一致性至关重要。为了确保量化的SDM生成高质量且一致的图像，我们为SDM提出了一个高效的量化框架。我们的框架引入了一个串行到并行的流水线，同时保持训练推理的一致性和确保优化稳定性。在此基础上，我们进一步开发了几种技术，包括多时间步激活量化、时间信息预计算、层间蒸馏和选择性冻结，以实现与浮点模型相比的高保真生成，同时保持量化效率。通过对多个稳定扩散变体（v1-4、v2-1、XL 1.0和v3）的综合评估，我们的方法在训练时间上优于最新方法，表现出卓越的性能。在W4A8量化设置下，我们在分布相似性和视觉保真度方面取得了显著的改进，同时保持了高图像质量。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06661v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于Stable Diffusion模型（SDM）的文本转图像生成技术展现出卓越的能力，但其计算密集型的迭代去噪过程阻碍了其在延迟敏感场景中的实时部署。为确保量化后的SDM生成高质量且一致的图像，研究团队提出一个高效量化框架，采用串行转并行管道，保持训练推理一致性并优化稳定性。通过多项技术，包括多时间步激活量化、时间信息预计算、层间蒸馏和选择性冻结等，该框架相较于浮点模型在保持量化效率的同时实现了高保真生成。经过对多个Stable Diffusion版本的综合评估，该方法在性能和训练时间上均表现出超越现有技术的优势。在W4A8量化设置下，其在分布相似性和视觉保真度方面取得了显著改进，同时保持了高图像质量。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Stable Diffusion模型在文本转图像生成上表现卓越，但计算密集型的去噪过程导致实时部署受限。</li>
<li>量化框架用于提高SDM的效率并生成高质量、一致的图像。</li>
<li>框架采用串行转并行管道，旨在保持训练推理一致性并优化稳定性。</li>
<li>通过多项技术实现高保真生成，包括多时间步激活量化、时间信息预计算等。</li>
<li>框架在多个Stable Diffusion版本评估中表现优越，训练时间缩短。</li>
<li>在W4A8量化设置下，框架在分布相似性和视觉保真度上取得显著改进。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.06661">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-d3fa101b120d05b6303238761aad3620.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cbc4be7dc44772e90b4431ed6f422623.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d70f199936df6264c8c332589207ad86.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c5f78148f3c049d8f7dd834c359e812a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-de804d728ada58b8fa4a97244451761d.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="DynamicControl-Adaptive-Condition-Selection-for-Improved-Text-to-Image-Generation"><a href="#DynamicControl-Adaptive-Condition-Selection-for-Improved-Text-to-Image-Generation" class="headerlink" title="DynamicControl: Adaptive Condition Selection for Improved Text-to-Image   Generation"></a>DynamicControl: Adaptive Condition Selection for Improved Text-to-Image   Generation</h2><p><strong>Authors:Qingdong He, Jinlong Peng, Pengcheng Xu, Boyuan Jiang, Xiaobin Hu, Donghao Luo, Yong Liu, Yabiao Wang, Chengjie Wang, Xiangtai Li, Jiangning Zhang</strong></p>
<p>To enhance the controllability of text-to-image diffusion models, current ControlNet-like models have explored various control signals to dictate image attributes. However, existing methods either handle conditions inefficiently or use a fixed number of conditions, which does not fully address the complexity of multiple conditions and their potential conflicts. This underscores the need for innovative approaches to manage multiple conditions effectively for more reliable and detailed image synthesis. To address this issue, we propose a novel framework, DynamicControl, which supports dynamic combinations of diverse control signals, allowing adaptive selection of different numbers and types of conditions. Our approach begins with a double-cycle controller that generates an initial real score sorting for all input conditions by leveraging pre-trained conditional generation models and discriminative models. This controller evaluates the similarity between extracted conditions and input conditions, as well as the pixel-level similarity with the source image. Then, we integrate a Multimodal Large Language Model (MLLM) to build an efficient condition evaluator. This evaluator optimizes the ordering of conditions based on the double-cycle controller’s score ranking. Our method jointly optimizes MLLMs and diffusion models, utilizing MLLMs’ reasoning capabilities to facilitate multi-condition text-to-image (T2I) tasks. The final sorted conditions are fed into a parallel multi-control adapter, which learns feature maps from dynamic visual conditions and integrates them to modulate ControlNet, thereby enhancing control over generated images. Through both quantitative and qualitative comparisons, DynamicControl demonstrates its superiority over existing methods in terms of controllability, generation quality and composability under various conditional controls. </p>
<blockquote>
<p>为了增强文本到图像扩散模型的可控性，当前的ControlNet类模型已经探索了各种控制信号来指示图像属性。然而，现有方法要么处理条件效率低下，要么使用固定数量的条件，这并没有完全解决多个条件的复杂性及其潜在冲突。这强调了需要采用创新方法有效管理多个条件，以实现更可靠和更详细的图像合成。为了解决这一问题，我们提出了一种新型框架DynamicControl，它支持各种控制信号的动态组合，并允许自适应选择不同数量和类型的条件。我们的方法始于双重循环控制器，该控制器利用预训练的条件生成模型和判别模型，对所有输入条件生成初始真实分数排序。该控制器评估提取的条件与输入条件之间的相似性，以及与源图像的像素级相似性。然后，我们集成了多模态大型语言模型（MLLM），以构建高效的条件评估器。该评估器根据双重循环控制器的得分排名优化条件的顺序。我们的方法联合优化MLLMs和扩散模型，利用MLLMs的推理能力促进多条件文本到图像（T2I）任务。最终的排序条件被输入到并行多控制适配器中，该适配器从动态视觉条件中学习特征映射并将其集成，以调制ControlNet，从而提高对生成图像的控制能力。通过定量和定性比较，DynamicControl在各种条件控制下，在可控性、生成质量和组合性方面均表现出其优越性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.03255v2">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>为提高文本到图像扩散模型的可控性，当前ControlNet类模型已尝试使用各种控制信号来指示图像属性。然而，现有方法要么处理条件效率低下，要么使用固定数量的条件，这并未充分解决多个条件的复杂性及其潜在冲突。因此，需要创新方法有效管理多个条件，以实现更可靠和详细的图像合成。为解决这一问题，我们提出了DynamicControl框架，支持各种控制信号的动态组合，可实现不同数量和类型的条件的自适应选择。我们的方法首先通过利用预训练的条件生成模型和判别模型，构建一个双循环控制器，生成所有输入条件的初始真实分数排序。该控制器评估提取的条件与输入条件的相似性，以及与源图像的像素级相似性。然后，我们整合了一个多模态大型语言模型（MLLM）来构建一个高效的条件评估器。该评估器基于双循环控制器的分数排名优化条件的排序。我们的方法联合优化MLLMs和扩散模型，利用MLLMs的推理能力促进多条件文本到图像（T2I）任务。最终排序的条件被输入到并行多控制适配器中，该适配器从动态视觉条件中学习特征映射并将其集成以调制ControlNet，从而提高对生成图像的控制能力。通过定量和定性比较，DynamicControl在可控性、生成质量和各种条件控制的组合性方面均优于现有方法。</p>
<p><strong>要点摘要</strong></p>
<ol>
<li>当前文本到图像扩散模型面临可控性问题，需要有效管理多个条件的方法。</li>
<li>DynamicControl框架支持动态组合多种控制信号，实现条件数量和类型的自适应选择。</li>
<li>利用双循环控制器生成初始真实分数排序，评估条件相似性。</li>
<li>集成多模态大型语言模型（MLLM）构建高效条件评估器，优化条件排序。</li>
<li>联合优化MLLMs和扩散模型，利用MLLMs的推理能力促进多条件文本到图像任务。</li>
<li>最后的条件排序被输入到并行多控制适配器中，该适配器学习并集成动态视觉条件以调制ControlNet。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.03255">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ff48f17347fe8711f0d05bd19cac5566.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8dbb1d1ea24bdd3d7330f6a851fe0345.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5224713e0b2322ebfe4f1e385bfa9219.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6762a09abb139e5cf0b4091265ff8a8f.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Replace-Anyone-in-Videos"><a href="#Replace-Anyone-in-Videos" class="headerlink" title="Replace Anyone in Videos"></a>Replace Anyone in Videos</h2><p><strong>Authors:Xiang Wang, Shiwei Zhang, Haonan Qiu, Ruihang Chu, Zekun Li, Yingya Zhang, Changxin Gao, Yuehuan Wang, Chunhua Shen, Nong Sang</strong></p>
<p>The field of controllable human-centric video generation has witnessed remarkable progress, particularly with the advent of diffusion models. However, achieving precise and localized control over human motion in videos, such as replacing or inserting individuals while preserving desired motion patterns, still remains a formidable challenge. In this work, we present the ReplaceAnyone framework, which focuses on localized human replacement and insertion featuring intricate backgrounds. Specifically, we formulate this task as an image-conditioned video inpainting paradigm with pose guidance, utilizing a unified end-to-end video diffusion architecture that facilitates image-conditioned video inpainting within masked regions. To prevent shape leakage and enable granular local control, we introduce diverse mask forms involving both regular and irregular shapes. Furthermore, we implement an enriched visual guidance mechanism to enhance appearance alignment, a hybrid inpainting encoder to further preserve the detailed background information in the masked video, and a two-phase optimization methodology to simplify the training difficulty. ReplaceAnyone enables seamless replacement or insertion of characters while maintaining the desired pose motion and reference appearance within a single framework. Extensive experimental results demonstrate the effectiveness of our method in generating realistic and coherent video content. The proposed ReplaceAnyone can be seamlessly applied not only to traditional 3D-UNet base models but also to DiT-based video models such as Wan2.1. The code will be available at <a target="_blank" rel="noopener" href="https://github.com/ali-vilab/UniAnimate-DiT">https://github.com/ali-vilab/UniAnimate-DiT</a>. </p>
<blockquote>
<p>可控以人为中心的视频生成领域已经取得了显著的进步，特别是随着扩散模型的兴起。然而，在视频中对人类动作进行精确和局部化的控制，如替换或插入个体同时保持期望的动作模式，仍然是一个巨大的挑战。在这项工作中，我们提出了ReplaceAnyone框架，该框架专注于具有复杂背景的局部人类替换和插入。具体来说，我们将此任务制定为图像条件视频修复模式带有姿势指导，利用统一端到端视频扩散架构，便于在遮罩区域内进行图像条件视频修复。为了防止形状泄露并实现精细的局部控制，我们引入了多种形式的遮罩，包括规则和不规则形状。此外，我们实现了丰富的视觉引导机制以增强外观对齐，混合修复编码器以进一步保留遮罩视频中的详细背景信息，以及两阶段优化方法来简化训练难度。ReplaceAnyone能够在单一框架内无缝替换或插入角色，同时保持期望的姿势动作和参考外观。大量的实验结果证明了我们的方法在生成现实和连贯的视频内容方面的有效性。所提出的ReplaceAnyone不仅可以无缝应用于传统的3D-UNet基础模型，还可以应用于基于DiT的视频模型，如Wan 2.1。相关代码将发布在：<a target="_blank" rel="noopener" href="https://github.com/ali-vilab/UniAnimate-DiT%E3%80%82">https://github.com/ali-vilab/UniAnimate-DiT。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.19911v2">PDF</a> </p>
<p><strong>Summary</strong><br>     随着扩散模型的发展，以人为中心的可控视频生成领域取得了显著进步，但在视频中实现精确和局部化的运动控制，如在保留所需运动模式的同时替换或插入个体，仍是一项艰巨的挑战。本研究提出了ReplaceAnyone框架，专注于局部人物替换和插入，带有复杂背景。我们将此任务制定为图像条件视频修复范式，采用统一的端到端视频扩散架构，在掩码区域内进行图像条件视频修复。为预防形状泄露并实现精细局部控制，我们引入了多种掩码形式，包括规则和不规则形状。此外，我们还实施了丰富的视觉引导机制，以提高外观对齐度，混合修复编码器以进一步保留掩码视频中的详细背景信息，以及两阶段优化方法以简化训练难度。ReplaceAnyone能够在单一框架内无缝替换或插入角色，同时保持所需的姿势运动和参考外观。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型在以人为中心的可控视频生成领域取得显著进步。</li>
<li>实现精确和局部化的运动控制（如替换或插入视频中的个体）仍是一个挑战。</li>
<li>ReplaceAnyone框架专注于局部人物替换和插入，带有复杂背景。</li>
<li>该任务被制定为图像条件视频修复范式，利用统一的端到端视频扩散架构。</li>
<li>为实现精细局部控制，引入了多种掩码形式和丰富的视觉引导机制。</li>
<li>ReplaceAnyone能无缝替换或插入角色，同时保持姿势运动和外观。</li>
<li>该方法不仅适用于传统的3D-UNet基础模型，也适用于DiT-based视频模型，如Wan2.1。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.19911">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9f2de81fc88ef8bdd3059f073c63be93.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cc6ce7af988fa9e983a747c34e839ed4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ede94b47fc530380b5aed1cadab15da9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cfa5a7fbe79221e5b437f60379b972bd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-57189adf87590c97692f3c707a276f4f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-246e9b7a85f96adb40cc36219d2291b0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-09/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-09/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-09/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-aac96b3ebfbae988da12a816938a33ca.jpg" class="responsive-img" alt="医学图像">
                        
                        <span class="card-title">医学图像</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学图像 方向最新论文已更新，请持续关注 Update in 2025-05-09  Active Sampling for MRI-based Sequential Decision Making
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-09
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    医学图像
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">医学图像</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-09/GAN/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-2ad56f32c9dc60dd7bad97809a6d3e2d.jpg" class="responsive-img" alt="GAN">
                        
                        <span class="card-title">GAN</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            GAN 方向最新论文已更新，请持续关注 Update in 2025-05-09  MAISY Motion-Aware Image SYnthesis for MedicalImage Motion Correction
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-09
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/GAN/" class="post-category">
                                    GAN
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/GAN/">
                        <span class="chip bg-color">GAN</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">17663.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
