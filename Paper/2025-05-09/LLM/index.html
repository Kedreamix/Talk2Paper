<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-09  On Path to Multimodal Generalist General-Level and General-Bench">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-e390b395ffd3ef8a44030ead5123007d.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-09
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    16.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    69 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-09-æ›´æ–°"><a href="#2025-05-09-æ›´æ–°" class="headerlink" title="2025-05-09 æ›´æ–°"></a>2025-05-09 æ›´æ–°</h1><h2 id="On-Path-to-Multimodal-Generalist-General-Level-and-General-Bench"><a href="#On-Path-to-Multimodal-Generalist-General-Level-and-General-Bench" class="headerlink" title="On Path to Multimodal Generalist: General-Level and General-Bench"></a>On Path to Multimodal Generalist: General-Level and General-Bench</h2><p><strong>Authors:Hao Fei, Yuan Zhou, Juncheng Li, Xiangtai Li, Qingshan Xu, Bobo Li, Shengqiong Wu, Yaoting Wang, Junbao Zhou, Jiahao Meng, Qingyu Shi, Zhiyuan Zhou, Liangtao Shi, Minghe Gao, Daoan Zhang, Zhiqi Ge, Weiming Wu, Siliang Tang, Kaihang Pan, Yaobo Ye, Haobo Yuan, Tao Zhang, Tianjie Ju, Zixiang Meng, Shilin Xu, Liyu Jia, Wentao Hu, Meng Luo, Jiebo Luo, Tat-Seng Chua, Shuicheng Yan, Hanwang Zhang</strong></p>
<p>The Multimodal Large Language Model (MLLM) is currently experiencing rapid growth, driven by the advanced capabilities of LLMs. Unlike earlier specialists, existing MLLMs are evolving towards a Multimodal Generalist paradigm. Initially limited to understanding multiple modalities, these models have advanced to not only comprehend but also generate across modalities. Their capabilities have expanded from coarse-grained to fine-grained multimodal understanding and from supporting limited modalities to arbitrary ones. While many benchmarks exist to assess MLLMs, a critical question arises: Can we simply assume that higher performance across tasks indicates a stronger MLLM capability, bringing us closer to human-level AI? We argue that the answer is not as straightforward as it seems. This project introduces General-Level, an evaluation framework that defines 5-scale levels of MLLM performance and generality, offering a methodology to compare MLLMs and gauge the progress of existing systems towards more robust multimodal generalists and, ultimately, towards AGI. At the core of the framework is the concept of Synergy, which measures whether models maintain consistent capabilities across comprehension and generation, and across multiple modalities. To support this evaluation, we present General-Bench, which encompasses a broader spectrum of skills, modalities, formats, and capabilities, including over 700 tasks and 325,800 instances. The evaluation results that involve over 100 existing state-of-the-art MLLMs uncover the capability rankings of generalists, highlighting the challenges in reaching genuine AI. We expect this project to pave the way for future research on next-generation multimodal foundation models, providing a robust infrastructure to accelerate the realization of AGI. Project page: <a target="_blank" rel="noopener" href="https://generalist.top/">https://generalist.top/</a> </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ç›®å‰æ­£åœ¨ç»å†å¿«é€Ÿå‘å±•ï¼Œå…¶åŠ¨åŠ›æ¥æºäºLLMçš„å…ˆè¿›åŠŸèƒ½ã€‚ä¸æ—©æœŸçš„ä¸“å®¶ä¸åŒï¼Œç°æœ‰çš„MLLMæ­£åœ¨å‘å¤šæ¨¡æ€é€šæ‰æ¨¡å¼å‘å±•ã€‚è¿™äº›æ¨¡å‹æœ€åˆä»…é™äºç†è§£å¤šç§æ¨¡å¼ï¼Œç°å·²å‘å±•è¿›æ­¥ï¼Œä¸ä»…èƒ½ç†è§£ä¹Ÿèƒ½ç”Ÿæˆå¤šç§æ¨¡å¼çš„å†…å®¹ã€‚å®ƒä»¬çš„èƒ½åŠ›å·²ä»ç²—ç²’åº¦çš„å¤šæ¨¡æ€ç†è§£æ‰©å±•åˆ°ç²¾ç»†ç²’åº¦çš„å¤šæ¨¡æ€ç†è§£ï¼Œå¹¶ä¸”ä»æ”¯æŒæœ‰é™çš„æ¨¡å¼æ‰©å±•åˆ°ä»»æ„æ¨¡å¼ã€‚è™½ç„¶å­˜åœ¨è®¸å¤šåŸºå‡†æµ‹è¯•æ¥è¯„ä¼°MLLMï¼Œä½†æœ‰ä¸€ä¸ªå…³é”®é—®é¢˜ï¼šæˆ‘ä»¬èƒ½å¦ç®€å•åœ°å‡è®¾è·¨ä»»åŠ¡çš„é«˜æ€§èƒ½å°±è¡¨æ˜MLLMèƒ½åŠ›æ›´å¼ºï¼Œæ›´èƒ½å®ç°äººç±»æ°´å¹³çš„AIï¼Ÿæˆ‘ä»¬è®¤ä¸ºç­”æ¡ˆå¹¶ä¸åƒçœ‹èµ·æ¥é‚£ä¹ˆç®€å•ã€‚æ­¤é¡¹ç›®å¼•å…¥äº†é€šç”¨çº§åˆ«è¯„ä¼°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å®šä¹‰äº†MLLMæ€§èƒ½çš„äº”ä¸ªçº§åˆ«ä»¥åŠä¸€èˆ¬æ€§ï¼Œæä¾›äº†ä¸€ç§æ¯”è¾ƒMLLMå¹¶è¡¡é‡ç°æœ‰ç³»ç»Ÿæœç€æ›´ç¨³å¥çš„å¤šæ¨¡æ€é€šæ‰å’Œæœ€ç»ˆå®ç°AGIçš„è¿›å±•çš„æ–¹æ³•ã€‚æ¡†æ¶çš„æ ¸å¿ƒåœ¨äºååŒæ¦‚å¿µï¼Œå®ƒè¡¡é‡çš„æ˜¯æ¨¡å‹åœ¨ç†è§£å’Œç”Ÿæˆæ–¹é¢ä»¥åŠå¤šä¸ªæ¨¡å¼ä¹‹é—´æ˜¯å¦ä¿æŒäº†ä¸€è‡´çš„èƒ½åŠ›ã€‚ä¸ºäº†æ”¯æŒè¿™ä¸€è¯„ä¼°ï¼Œæˆ‘ä»¬æ¨å‡ºäº†General-Benchï¼Œå®ƒæ¶µç›–äº†æ›´å¹¿æ³›çš„æŠ€èƒ½ã€æ¨¡å¼ã€æ ¼å¼å’Œèƒ½åŠ›ï¼ŒåŒ…æ‹¬è¶…è¿‡700ä¸ªä»»åŠ¡å’Œ325,800ä¸ªå®ä¾‹ã€‚æ¶‰åŠè¶…è¿‡100ä¸ªç°æœ‰æœ€å…ˆè¿›çš„MLLMçš„è¯„ä¼°ç»“æœæ­ç¤ºäº†é€šæ‰çš„èƒ½åŠ›æ’åï¼Œçªå‡ºäº†å®ç°çœŸæ­£AIçš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬å¸Œæœ›è¿™ä¸ªé¡¹ç›®èƒ½ä¸ºä¸‹ä¸€ä»£å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹çš„è¿›ä¸€æ­¥ç ”ç©¶é“ºå¹³é“è·¯ï¼Œæä¾›åŠ é€Ÿå®ç°AGIçš„ç¨³å¥åŸºç¡€è®¾æ–½ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://generalist.top/">https://generalist.top/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.04620v1">PDF</a> ICMLâ€™25, 305 pages, 115 tables, 177 figures, project page:   <a target="_blank" rel="noopener" href="https://generalist.top/">https://generalist.top/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„æœ€æ–°è¿›å±•ã€‚MLLMæ­£æœç€å¤šæ¨¡æ€é€šç”¨ä¸»ä¹‰æ¨¡å¼å‘å±•ï¼Œä¸ä»…èƒ½ç†è§£å¤šç§æ¨¡æ€ï¼Œè¿˜èƒ½ç”Ÿæˆå¤šç§æ¨¡æ€çš„å†…å®¹ã€‚æ–‡ç« æå‡ºäº†ä¸€ä¸ªè¯„ä¼°MLLMæ€§èƒ½ä¸é€šç”¨æ€§çš„äº”å±‚æ¬¡è¯„ä»·æ¡†æ¶ï¼Œå¹¶å¼•å…¥äº†â€œååŒâ€è¿™ä¸€æ ¸å¿ƒæ¦‚å¿µæ¥æ¯”è¾ƒæ¨¡å‹åœ¨ä¸åŒç†è§£å’Œç”Ÿæˆä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œå¹¶è¯„ä»·ä¸åŒæ¨¡å‹ä¹‹é—´çš„ç»¼åˆèƒ½åŠ›å·®è·ã€‚é€šè¿‡æ„å»ºå¤§è§„æ¨¡çš„è¯„æµ‹åŸºå‡†æ•°æ®é›†General-Benchï¼ŒåŒ…æ‹¬å¹¿æ³›çš„æŠ€èƒ½ã€æ¨¡æ€ã€æ ¼å¼å’Œèƒ½åŠ›ä»¥åŠè¶…è¿‡ç™¾ä¸‡çº§åˆ«çš„æ•°æ®ç‚¹æ¥è®­ç»ƒå’ŒéªŒè¯MLLMæ€§èƒ½ç­‰çº§è¯„ä¼°æ¨¡å‹çš„èƒ½åŠ›æ°´å¹³æ’åå’ŒæŒ‘æˆ˜ï¼ŒæœŸæœ›ä¸ºä¸‹ä¸€ä»£å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹çš„å®ç°æä¾›ç¨³å¥çš„æ¡†æ¶å’ŒåŠ é€Ÿäººå·¥æ™ºèƒ½é€šç”¨æ™ºèƒ½ï¼ˆAGIï¼‰çš„å®ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>MLLMæ­£åœ¨è¿…é€Ÿå‘å±•å’Œæ¼”è¿›ï¼Œå±•ç°å‡ºäº†å¯¹å¤šç§æ¨¡æ€çš„ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ã€‚</li>
<li>æ–‡ç« æå‡ºäº†ä¸€ä¸ªè¯„ä¼°MLLMæ€§èƒ½ä¸é€šç”¨æ€§çš„äº”å±‚æ¬¡è¯„ä»·æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ—¨åœ¨è¯„ä¼°æ¨¡å‹åœ¨ä¸åŒä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œå¹¶è¡¡é‡å…¶å‘æ›´ç¨³å¥çš„å¤šæ¨¡æ€é€šç”¨ä¸»ä¹‰å’Œäººå·¥æ™ºèƒ½é€šç”¨æ™ºèƒ½ï¼ˆAGIï¼‰è¿ˆè¿›çš„è¿›åº¦ã€‚</li>
<li>å¼•å…¥äº†â€œååŒâ€è¿™ä¸€æ ¸å¿ƒè¯„ä¼°æ¦‚å¿µï¼Œæ—¨åœ¨æ¯”è¾ƒæ¨¡å‹åœ¨ä¸åŒç†è§£å’Œç”Ÿæˆä»»åŠ¡ä¸­çš„ä¸€è‡´æ€§è¡¨ç°ã€‚</li>
<li>æ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡çš„è¯„æµ‹åŸºå‡†æ•°æ®é›†General-Benchï¼Œç”¨äºè®­ç»ƒå’ŒéªŒè¯MLLMæ€§èƒ½ç­‰çº§è¯„ä¼°æ¨¡å‹çš„èƒ½åŠ›æ°´å¹³æ’åå’ŒæŒ‘æˆ˜ã€‚</li>
<li>é€šè¿‡è¶…è¿‡ç™¾ä¸‡çº§åˆ«çš„æ•°æ®ç‚¹æ¥è¯„ä¼°æ¨¡å‹åœ¨å„ç§æŠ€èƒ½å’Œæ¨¡æ€æ–¹é¢çš„è¡¨ç°ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.04620">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-69b2a18376d3518d367ef5dc3cbce6b9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-43d7956692b153fc6f51178d4187df4a.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="OmniGIRL-A-Multilingual-and-Multimodal-Benchmark-for-GitHub-Issue-Resolution"><a href="#OmniGIRL-A-Multilingual-and-Multimodal-Benchmark-for-GitHub-Issue-Resolution" class="headerlink" title="OmniGIRL: A Multilingual and Multimodal Benchmark for GitHub Issue   Resolution"></a>OmniGIRL: A Multilingual and Multimodal Benchmark for GitHub Issue   Resolution</h2><p><strong>Authors:Lianghong Guo, Wei Tao, Runhan Jiang, Yanlin Wang, Jiachi Chen, Xilin Liu, Yuchi Ma, Mingzhi Mao, Hongyu Zhang, Zibin Zheng</strong></p>
<p>The GitHub issue resolution task aims to resolve issues reported in repositories automatically. With advances in large language models (LLMs), this task has gained increasing attention, and several benchmarks are proposed to evaluate the issue resolution ability of LLMs. However, existing benchmarks have three main limitations. First, current benchmarks focus on a single programming language, limiting the evaluation of issues from repositories across different languages. Second, they usually cover a narrow range of domains, which may fail to represent the diversity of real-world issues. Third, existing benchmarks rely solely on textual information in issue descriptions, overlooking multimodal information such as images in issues. In this paper, we propose OmniGIRL, a GitHub Issue ResoLution benchmark that is multilingual, multimodal, and multi-domain. OmniGIRL includes 959 task instances, which are collected from repositories across four programming languages (i.e., Python, JavaScript, TypeScript, and Java) and eight different domains. Our evaluation shows that current LLMs show limited performances on OmniGIRL. Notably, the best-performing model, GPT-4o, resolves only 8.6% of the issues. Besides, we find that current LLMs struggle to resolve issues requiring understanding images. The best performance is achieved by Claude-3.5-Sonnet, which resolves only 10.5% of the issues with image information. Finally, we analyze the reasons behind current LLMsâ€™ failure on OmniGIRL, providing insights for future improvements. </p>
<blockquote>
<p>GitHubé—®é¢˜è§£ç­”ä»»åŠ¡æ—¨åœ¨è‡ªåŠ¨è§£å†³ä»“åº“ä¸­æŠ¥å‘Šçš„é—®é¢˜ã€‚éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥ï¼Œæ­¤ä»»åŠ¡è¶Šæ¥è¶Šå—åˆ°å…³æ³¨ï¼Œå¹¶æå‡ºäº†å¤šä¸ªåŸºå‡†æµ‹è¯•æ¥è¯„ä¼°LLMçš„é—®é¢˜è§£å†³èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºå‡†æµ‹è¯•å­˜åœ¨ä¸‰ä¸ªä¸»è¦å±€é™æ€§ã€‚é¦–å…ˆï¼Œå½“å‰çš„åŸºå‡†æµ‹è¯•ä¸“æ³¨äºå•ä¸€ç¼–ç¨‹è¯­è¨€ï¼Œè¿™é™åˆ¶äº†ä»ä¸åŒè¯­è¨€çš„ä»“åº“ä¸­è¯„ä¼°é—®é¢˜ã€‚å…¶æ¬¡ï¼Œå®ƒä»¬é€šå¸¸æ¶‰åŠèŒƒå›´ç‹­çª„çš„é¢†åŸŸï¼Œå¯èƒ½æ— æ³•ä»£è¡¨çœŸå®ä¸–ç•Œé—®é¢˜çš„å¤šæ ·æ€§ã€‚ç¬¬ä¸‰ï¼Œç°æœ‰åŸºå‡†æµ‹è¯•ä»…ä¾èµ–äºé—®é¢˜æè¿°ä¸­çš„æ–‡æœ¬ä¿¡æ¯ï¼Œè€Œå¿½ç•¥äº†é—®é¢˜ä¸­çš„å¤šæ¨¡æ€ä¿¡æ¯ï¼Œå¦‚å›¾åƒã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†OmniGIRLï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šè¯­è¨€ã€å¤šæ¨¡æ€å’Œå¤šé¢†åŸŸçš„GitHubé—®é¢˜è§£ç­”åŸºå‡†æµ‹è¯•ã€‚OmniGIRLåŒ…å«959ä¸ªä»»åŠ¡å®ä¾‹ï¼Œè¿™äº›å®ä¾‹æ˜¯ä»å››ç§ç¼–ç¨‹è¯­è¨€ï¼ˆå³Pythonã€JavaScriptã€TypeScriptå’ŒJavaï¼‰çš„ä»“åº“ä¸­ä»¥åŠå…«ä¸ªä¸åŒé¢†åŸŸæ”¶é›†çš„ã€‚æˆ‘ä»¬çš„è¯„ä¼°æ˜¾ç¤ºï¼Œå½“å‰çš„LLMåœ¨OmniGIRLä¸Šçš„è¡¨ç°æœ‰é™ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¡¨ç°æœ€ä½³çš„æ¨¡å‹GPT-4oåªèƒ½è§£å†³8.6%çš„é—®é¢˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°å½“å‰LLMåœ¨è§£å†³éœ€è¦ç†è§£å›¾åƒçš„é—®é¢˜æ—¶é¢ä¸´å›°éš¾ã€‚æˆç»©æœ€å¥½çš„æ˜¯Claude-3.5-Sonnetï¼Œå®ƒåªèƒ½è§£å†³å¸¦æœ‰å›¾åƒä¿¡æ¯çš„10.5%çš„é—®é¢˜ã€‚æœ€åï¼Œæˆ‘ä»¬åˆ†æäº†å½“å‰LLMåœ¨OmniGIRLä¸Šå¤±è´¥çš„åŸå› ï¼Œä¸ºæœªæ¥çš„æ”¹è¿›æä¾›äº†è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.04606v1">PDF</a> To appear at ISSTAâ€™25</p>
<p><strong>Summary</strong></p>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‘å±•ï¼ŒGitHubé—®é¢˜è‡ªåŠ¨è§£å†³ä»»åŠ¡å¾—åˆ°äº†è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ã€‚ç„¶è€Œï¼Œå½“å‰å­˜åœ¨å‡ ä¸ªä¸»è¦çš„å±€é™æ€§ã€‚æœ¬æ–‡æå‡ºäº†OmniGIRLåŸºå‡†æµ‹è¯•ï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šè¯­è¨€ã€å¤šæ¨¡æ€å’Œå¤šé¢†åŸŸçš„GitHubé—®é¢˜è§£å†³æ–¹æ¡ˆåŸºå‡†æµ‹è¯•ã€‚ç„¶è€Œï¼ŒLLMæ¨¡å‹åœ¨è¯¥æµ‹è¯•ä¸­çš„è¡¨ç°å—é™ï¼Œè§£å†³é—®é¢˜çš„æœ€ä½³æ¨¡å‹GPT-4oä»…å è§£å†³ç‡çš„8.6%ã€‚å¹¶ä¸”LLMåœ¨å¤„ç†æ¶‰åŠå›¾åƒç†è§£çš„é—®é¢˜æ—¶å°¤ä¸ºå›°éš¾ã€‚å› æ­¤ï¼Œéœ€è¦æ·±å…¥åˆ†æå’Œæ”¹è¿›æœªæ¥çš„LLMæ¨¡å‹ä»¥æé«˜è§£å†³ç‡ã€‚æœ¬æ–‡åŒæ—¶æ¢è®¨äº†åŸå› ï¼Œå¹¶æä¾›äº†å¯¹æœªæ¥æ”¹è¿›çš„å¯ç¤ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GitHubé—®é¢˜è‡ªåŠ¨è§£å†³ä»»åŠ¡æ—¨åœ¨è‡ªåŠ¨è§£å†³ä»“åº“ä¸­æŠ¥å‘Šçš„é—®é¢˜ã€‚</li>
<li>å½“å‰å­˜åœ¨çš„åŸºå‡†æµ‹è¯•ä¸»è¦å…³æ³¨å•ä¸€ç¼–ç¨‹è¯­è¨€å’Œç‹­çª„é¢†åŸŸçš„é—®é¢˜è¯„ä»·ï¼Œé™åˆ¶äº†é—®é¢˜çš„è¯„ä¼°èŒƒå›´ã€‚</li>
<li>OmniGIRLæ˜¯ä¸€ä¸ªå¤šè¯­è¨€ã€å¤šæ¨¡æ€å’Œå¤šé¢†åŸŸçš„GitHubé—®é¢˜è§£å†³æ–¹æ¡ˆåŸºå‡†æµ‹è¯•ã€‚å®ƒæ¶µç›–äº†å››ç§ç¼–ç¨‹è¯­è¨€å’Œå…«ä¸ªä¸åŒé¢†åŸŸï¼ŒåŒ…æ‹¬å›¾åƒåœ¨å†…çš„å¤šç§ä¿¡æ¯ã€‚</li>
<li>å½“å‰LLMåœ¨OmniGIRLä¸Šçš„è¡¨ç°å—é™ï¼Œæœ€ä½³æ¨¡å‹GPT-4oçš„è§£å†³ç‡ä»…ä¸º8.6%ã€‚</li>
<li>LLMåœ¨å¤„ç†æ¶‰åŠå›¾åƒç†è§£çš„é—®é¢˜æ—¶é¢ä¸´å›°éš¾ï¼Œæœ€ä½³æ¨¡å‹Claude-3.5-Sonnetçš„è§£å†³ç‡ä¸ºä»…10.5%ã€‚è¿™è¡¨æ˜LLMåœ¨å¤„ç†å¤æ‚é—®é¢˜æ—¶ä»éœ€è¦æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.04606">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3e4ea6799c4827336b7c53b378752b8f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0d965d191682f56dfbef86a92743fe15.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-44b32e27405df970c0dba71ea49660bc.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="MonoCoP-Chain-of-Prediction-for-Monocular-3D-Object-Detection"><a href="#MonoCoP-Chain-of-Prediction-for-Monocular-3D-Object-Detection" class="headerlink" title="MonoCoP: Chain-of-Prediction for Monocular 3D Object Detection"></a>MonoCoP: Chain-of-Prediction for Monocular 3D Object Detection</h2><p><strong>Authors:Zhihao Zhang, Abhinav Kumar, Girish Chandar Ganesan, Xiaoming Liu</strong></p>
<p>Accurately predicting 3D attributes is crucial for monocular 3D object detection (Mono3D), with depth estimation posing the greatest challenge due to the inherent ambiguity in mapping 2D images to 3D space. While existing methods leverage multiple depth cues (e.g., estimating depth uncertainty, modeling depth error) to improve depth accuracy, they overlook that accurate depth prediction requires conditioning on other 3D attributes, as these attributes are intrinsically inter-correlated through the 3D to 2D projection, which ultimately limits overall accuracy and stability. Inspired by Chain-of-Thought (CoT) in large language models (LLMs), this paper proposes MonoCoP, which leverages a Chain-of-Prediction (CoP) to predict attributes sequentially and conditionally via three key designs. First, it employs a lightweight AttributeNet (AN) for each 3D attribute to learn attribute-specific features. Next, MonoCoP constructs an explicit chain to propagate these learned features from one attribute to the next. Finally, MonoCoP uses a residual connection to aggregate features for each attribute along the chain, ensuring that later attribute predictions are conditioned on all previously processed attributes without forgetting the features of earlier ones. Experimental results show that our MonoCoP achieves state-of-the-art (SoTA) performance on the KITTI leaderboard without requiring additional data and further surpasses existing methods on the Waymo and nuScenes frontal datasets. </p>
<blockquote>
<p>å‡†ç¡®é¢„æµ‹3Då±æ€§å¯¹äºå•ç›®3Dç‰©ä½“æ£€æµ‹ï¼ˆMono3Dï¼‰è‡³å…³é‡è¦ï¼Œæ·±åº¦ä¼°è®¡ç”±äºå°†2Då›¾åƒæ˜ å°„åˆ°3Dç©ºé—´æ—¶å›ºæœ‰çš„æ¨¡ç³Šæ€§è€Œæ„æˆæœ€å¤§çš„æŒ‘æˆ˜ã€‚è™½ç„¶ç°æœ‰æ–¹æ³•åˆ©ç”¨å¤šç§æ·±åº¦çº¿ç´¢ï¼ˆä¾‹å¦‚ä¼°è®¡æ·±åº¦ä¸ç¡®å®šæ€§ã€å»ºæ¨¡æ·±åº¦è¯¯å·®ï¼‰æ¥æé«˜æ·±åº¦å‡†ç¡®æ€§ï¼Œä½†å®ƒä»¬å¿½ç•¥äº†å‡†ç¡®çš„æ·±åº¦é¢„æµ‹éœ€è¦ä¾èµ–äºå…¶ä»–3Då±æ€§ï¼Œå› ä¸ºè¿™äº›å±æ€§é€šè¿‡3Dåˆ°2Dçš„æŠ•å½±å›ºæœ‰åœ°ç›¸äº’å…³è”ï¼Œè¿™æœ€ç»ˆé™åˆ¶äº†æ•´ä½“å‡†ç¡®æ€§å’Œç¨³å®šæ€§ã€‚æœ¬æ–‡å—å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„æ€ç»´é“¾ï¼ˆCoTï¼‰å¯å‘ï¼Œæå‡ºäº†ä¸€ç§åä¸ºMonoCoPçš„æ–¹æ³•ï¼Œå®ƒåˆ©ç”¨é¢„æµ‹é“¾ï¼ˆCoPï¼‰æŒ‰é¡ºåºå’Œæœ‰æ¡ä»¶åœ°é¢„æµ‹å±æ€§ï¼Œä¸»è¦é€šè¿‡ä¸‰ä¸ªå…³é”®è®¾è®¡å®ç°ã€‚é¦–å…ˆï¼Œå®ƒé‡‡ç”¨è½»é‡çº§çš„AttributeNetï¼ˆANï¼‰æ¥å­¦ä¹ æ¯ä¸ª3Då±æ€§çš„ç‰¹å®šç‰¹å¾ã€‚æ¥ä¸‹æ¥ï¼ŒMonoCoPæ„å»ºäº†ä¸€ä¸ªæ˜ç¡®çš„é“¾æ¡ï¼Œä»¥ä¼ æ’­ä»ä¸€ä¸ªå±æ€§å­¦åˆ°çš„ç‰¹å¾åˆ°ä¸‹ä¸€ä¸ªå±æ€§ã€‚æœ€åï¼ŒMonoCoPä½¿ç”¨æ®‹å·®è¿æ¥æ¥æ²¿é“¾æ¡èšåˆæ¯ä¸ªå±æ€§çš„ç‰¹å¾ï¼Œç¡®ä¿åç»­çš„å±æ€§é¢„æµ‹ä¾èµ–äºæ‰€æœ‰å…ˆå‰å¤„ç†çš„å±æ€§ï¼ŒåŒæ—¶ä¸ä¼šå¿˜è®°è¾ƒæ—©å±æ€§çš„ç‰¹å¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„MonoCoPåœ¨KITTIæ’è¡Œæ¦œä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶ä¸”æ— éœ€é¢å¤–æ•°æ®ï¼Œåœ¨Waymoå’ŒnuScenesæ­£é¢æ•°æ®é›†ä¸Šè¶…è¶Šäº†ç°æœ‰æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.04594v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†åŸºäºChain-of-Predictionï¼ˆCoPï¼‰çš„å•çœ¼ä¸‰ç»´ç‰©ä½“æ£€æµ‹æ–°æ–¹æ³•MonoCoPã€‚è¯¥æ–¹æ³•é€šè¿‡æ„å»ºé¢„æµ‹é“¾ï¼Œåˆ©ç”¨å±æ€§ç‰¹å®šçš„ç‰¹å¾è¿›è¡Œä¸‰ç»´å±æ€§çš„é¡ºåºå’Œæ¡ä»¶é¢„æµ‹ã€‚é€šè¿‡å®éªŒç»“æœï¼Œè¯æ˜äº†MonoCoPåœ¨KITTIæ’è¡Œæ¦œä¸Šè¾¾åˆ°äº†é¢†å…ˆæ°´å¹³ï¼ŒåŒæ—¶åœ¨Waymoå’ŒnuScenesæ­£é¢æ•°æ®é›†ä¸Šè¶…è¶Šäº†ç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å•çœ¼ä¸‰ç»´ç‰©ä½“æ£€æµ‹ï¼ˆMono3Dï¼‰ä¸­ï¼Œå‡†ç¡®é¢„æµ‹ä¸‰ç»´å±æ€§è‡³å…³é‡è¦ï¼Œæ·±åº¦ä¼°è®¡æ˜¯ä¸€å¤§æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•è™½åˆ©ç”¨å¤šç§æ·±åº¦çº¿ç´¢æé«˜æ·±åº¦å‡†ç¡®æ€§ï¼Œä½†å¿½ç•¥äº†å‡†ç¡®æ·±åº¦é¢„æµ‹éœ€è¦ä¾èµ–å…¶ä»–ä¸‰ç»´å±æ€§çš„æ¡ä»¶ã€‚</li>
<li>æœ¬æ–‡å—å¤§å‹è¯­è¨€æ¨¡å‹ä¸­â€œChain-of-Thoughtâ€ï¼ˆCoTï¼‰çš„å¯å‘ï¼Œæå‡ºäº†MonoCoPæ–¹æ³•ã€‚</li>
<li>MonoCoPé€šè¿‡æ„å»ºé¢„æµ‹é“¾è¿›è¡Œä¸‰ç»´å±æ€§çš„é¡ºåºå’Œæ¡ä»¶é¢„æµ‹ï¼Œåˆ©ç”¨AttributeNetï¼ˆANï¼‰å­¦ä¹ å±æ€§ç‰¹å®šç‰¹å¾ã€‚</li>
<li>MonoCoPé€šè¿‡æ˜¾å¼é“¾ä¼ æ’­è¿™äº›å­¦ä¹ åˆ°çš„ç‰¹å¾ä»ä¸€ä¸ªå±æ€§åˆ°ä¸‹ä¸€ä¸ªå±æ€§ã€‚</li>
<li>MonoCoPä½¿ç”¨æ®‹å·®è¿æ¥æ¥èšåˆé¢„æµ‹é“¾ä¸­æ¯ä¸ªå±æ€§çš„ç‰¹å¾ï¼Œç¡®ä¿åæ¥çš„å±æ€§é¢„æµ‹ä¾èµ–äºæ‰€æœ‰å…ˆå‰å¤„ç†çš„å±æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.04594">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cf0ab01b2bd9120b424439c0b13f2464.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4656fa795d076aea34f6e55c4fc8abad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0d6169567c1dc4abeac209b7307d9785.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a4fbd105d5b65f5ad6dbead5d93a91e2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d4e1fb2b9073d0dc3996c41cd4d610d7.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="SlideItRight-Using-AI-to-Find-Relevant-Slides-and-Provide-Feedback-for-Open-Ended-Questions"><a href="#SlideItRight-Using-AI-to-Find-Relevant-Slides-and-Provide-Feedback-for-Open-Ended-Questions" class="headerlink" title="SlideItRight: Using AI to Find Relevant Slides and Provide Feedback for   Open-Ended Questions"></a>SlideItRight: Using AI to Find Relevant Slides and Provide Feedback for   Open-Ended Questions</h2><p><strong>Authors:Chloe Qianhui Zhao, Jie Cao, Eason Chen, Kenneth R. Koedinger, Jionghao Lin</strong></p>
<p>Feedback is important in supporting student learning. While various automated feedback systems have been implemented to make the feedback scalable, many existing solutions only focus on generating text-based feedback. As is indicated in the multimedia learning principle, learning with more modalities could help utilize more separate channels, reduce the cognitive load and facilitate studentsâ€™ learning. Hence, it is important to explore the potential of Artificial Intelligence (AI) in feedback generation from and to different modalities. Our study leverages Large Language Models (LLMs) for textual feedback with the supplementary guidance from other modality - relevant lecture slide retrieved from the slides hub. Through an online crowdsourcing study (N&#x3D;91), this study investigates learning gains and student perceptions using a 2x2 design (i.e., human feedback vs. AI feedback and with vs. without relevant slide), evaluating the clarity, engagement, perceived effectiveness, and reliability) of AI-facilitated multimodal feedback. We observed significant pre-to-post learning gains across all conditions. However, the differences in these gains were not statistically significant between conditions. The post-survey revealed that students found the slide feedback helpful in their learning process, though they reported difficulty in understanding it. Regarding the AI-generated open-ended feedback, students considered it personalized and relevant to their responses, but they expressed lower trust in the AI feedback compared to human-generated feedback. </p>
<blockquote>
<p>åé¦ˆå¯¹äºæ”¯æŒå­¦ç”Ÿå­¦ä¹ éå¸¸é‡è¦ã€‚è™½ç„¶å·²å®æ–½å„ç§è‡ªåŠ¨åŒ–åé¦ˆç³»ç»Ÿä»¥å®ç°åé¦ˆçš„è§„æ¨¡åŒ–ï¼Œä½†è®¸å¤šç°æœ‰è§£å†³æ–¹æ¡ˆä»…ä¸“æ³¨äºç”ŸæˆåŸºäºæ–‡æœ¬çš„åé¦ˆã€‚æ­£å¦‚å¤šåª’ä½“å­¦ä¹ åŸç†æ‰€æŒ‡ç¤ºçš„ï¼Œä½¿ç”¨æ›´å¤šæ¨¡å¼è¿›è¡Œå­¦ä¹ å¯ä»¥å¸®åŠ©åˆ©ç”¨æ›´å¤šç‹¬ç«‹é€šé“ï¼Œå‡å°‘è®¤çŸ¥è´Ÿè·å¹¶ä¿ƒè¿›å­¦ç”Ÿå­¦ä¹ ã€‚å› æ­¤ï¼Œæ¢ç´¢äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰åœ¨ä¸åŒæ¨¡å¼åé¦ˆç”Ÿæˆä¸­çš„æ½œåŠ›éå¸¸é‡è¦ã€‚æˆ‘ä»¬çš„ç ”ç©¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œæ–‡æœ¬åé¦ˆï¼Œå¹¶è¾…ä»¥å…¶ä»–æ¨¡å¼çš„æŒ‡å¯¼â€”â€”ä»å¹»ç¯ç‰‡ä¸­å¿ƒæ£€ç´¢çš„ç›¸å…³è®²ä¹‰å¹»ç¯ç‰‡ã€‚é€šè¿‡ä¸€é¡¹åœ¨çº¿ä¼—åŒ…ç ”ç©¶ï¼ˆN&#x3D;91ï¼‰ï¼Œæœ¬ç ”ç©¶é‡‡ç”¨2x2è®¾è®¡ï¼ˆå³äººå·¥åé¦ˆä¸AIåé¦ˆï¼Œæœ‰ä¸æ— ç›¸å…³å¹»ç¯ç‰‡ï¼‰ï¼Œå¯¹å­¦ä¹ æ”¶è·å’Œå­¦ç”Ÿæ„ŸçŸ¥è¿›è¡Œè°ƒæŸ¥ï¼Œè¯„ä¼°AIè¾…åŠ©å¤šæ¨¡å¼åé¦ˆçš„æ¸…æ™°åº¦ã€å‚ä¸åº¦ã€æ„ŸçŸ¥æœ‰æ•ˆæ€§å’Œå¯é æ€§ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°æ‰€æœ‰æ¡ä»¶ä¸‹çš„å­¦ä¹ æ”¶è·éƒ½æœ‰æ˜¾è‘—æé«˜ã€‚ç„¶è€Œï¼Œè¿™äº›æ”¶ç›Šä¹‹é—´çš„å·®å¼‚åœ¨æ¡ä»¶ä¹‹é—´å¹¶æ²¡æœ‰ç»Ÿè®¡å­¦ä¸Šçš„æ˜¾è‘—å·®å¼‚ã€‚è°ƒæŸ¥æ˜¾ç¤ºï¼Œå­¦ç”Ÿä»¬è®¤ä¸ºå¹»ç¯ç‰‡åé¦ˆå¯¹ä»–ä»¬çš„å­¦ä¹ è¿‡ç¨‹å¾ˆæœ‰å¸®åŠ©ï¼Œå°½ç®¡ä»–ä»¬è¡¨ç¤ºç†è§£èµ·æ¥æœ‰å›°éš¾ã€‚å…³äºAIç”Ÿæˆçš„å¼€æ”¾å¼åé¦ˆï¼Œå­¦ç”Ÿè®¤ä¸ºå®ƒé’ˆå¯¹ä»–ä»¬çš„å›ç­”è¿›è¡Œäº†ä¸ªæ€§åŒ–ä¸”ç›¸å…³çš„åé¦ˆï¼Œä½†ä»–ä»¬è¡¨ç¤ºå¯¹AIåé¦ˆçš„ä¿¡ä»»åº¦ä½äºäººç±»ç”Ÿæˆçš„åé¦ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.04584v1">PDF</a> 14 pages, to be published at the 26th International Conference on   Artificial Intelligence in Education (AIED â€˜25)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†äººå·¥æ™ºèƒ½åœ¨å¤šæ¨¡æ€åé¦ˆç”Ÿæˆä¸­çš„æ½œåŠ›ï¼Œç ”ç©¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æä¾›æ–‡æœ¬åé¦ˆï¼Œè¾…ä»¥ç›¸å…³è®²åº§å¹»ç¯ç‰‡ã€‚é€šè¿‡åœ¨çº¿ä¼—åŒ…ç ”ç©¶ï¼ˆN&#x3D;91ï¼‰ï¼Œæœ¬ç ”ç©¶è°ƒæŸ¥äº†ä½¿ç”¨äººå·¥æ™ºèƒ½ä¿ƒè¿›çš„å¤šæ¨¡å¼åé¦ˆçš„å­¦ä¹ æ”¶è·å’Œå­¦ç”Ÿæ„ŸçŸ¥ã€‚ç ”ç©¶è§‚å¯Ÿåˆ°æ‰€æœ‰æ¡ä»¶ä¸‹çš„å­¦ä¹ æ”¶è·éƒ½æœ‰æ˜¾è‘—æé«˜ï¼Œä½†ä¸åŒæ¡ä»¶ä¹‹é—´çš„å·®å¼‚å¹¶ä¸æ˜¾è‘—ã€‚å­¦ç”Ÿä»¬å‘ç°å¹»ç¯ç‰‡çš„åé¦ˆåœ¨ä»–ä»¬çš„å­¦ä¹ è¿‡ç¨‹ä¸­å¾ˆæœ‰å¸®åŠ©ï¼Œå°½ç®¡ä»–ä»¬æŠ¥å‘Šè¯´éš¾ä»¥ç†è§£ã€‚å¯¹äºAIç”Ÿæˆçš„å¼€æ”¾æ€§åé¦ˆï¼Œå­¦ç”Ÿä»¬è®¤ä¸ºå®ƒæ˜¯ä¸ªæ€§åŒ–çš„å¹¶ä¸”ä¸ä»–ä»¬çš„å›ç­”ç›¸å…³ï¼Œä½†ä»–ä»¬è¡¨ç¤ºå¯¹AIåé¦ˆçš„ä¿¡ä»»åº¦è¾ƒä½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åé¦ˆåœ¨å­¦ç”Ÿå­¦ä¹ ä¸­éå¸¸é‡è¦ï¼Œéœ€è¦æ¢ç´¢äººå·¥æ™ºèƒ½åœ¨å¤šæ¨¡æ€åé¦ˆç”Ÿæˆä¸­çš„æ½œåŠ›ã€‚</li>
<li>æœ¬ç ”ç©¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æä¾›æ–‡æœ¬åé¦ˆï¼Œå¹¶è¾…ä»¥ç›¸å…³è®²åº§å¹»ç¯ç‰‡ä»¥å¢å¼ºå­¦ä¹ æ•ˆæœã€‚</li>
<li>é€šè¿‡åœ¨çº¿ä¼—åŒ…ç ”ç©¶ï¼Œå‘ç°æ‰€æœ‰æ¡ä»¶ä¸‹çš„å­¦ä¹ æ”¶è·éƒ½æœ‰æ˜¾è‘—æé«˜ï¼Œä½†ä¸åŒæ¡ä»¶é—´çš„å·®å¼‚å¹¶ä¸æ˜¾è‘—ã€‚</li>
<li>å­¦ç”Ÿä»¬è®¤ä¸ºå¹»ç¯ç‰‡åé¦ˆå¯¹å­¦ä¹ è¿‡ç¨‹æœ‰å¸®åŠ©ï¼Œä½†å­˜åœ¨ç†è§£å›°éš¾çš„é—®é¢˜ã€‚</li>
<li>AIç”Ÿæˆçš„å¼€æ”¾æ€§åé¦ˆè¢«è®¤ä¸ºæ˜¯ä¸å›ç­”ç›¸å…³çš„ä¸ªæ€§åŒ–åé¦ˆï¼Œä½†å­¦ç”Ÿå¯¹AIåé¦ˆçš„ä¿¡ä»»åº¦è¾ƒä½ã€‚</li>
<li>å­¦ä¹ æ•ˆæœçš„è¯„ä»·åŒ…æ‹¬æ¸…æ™°åº¦ã€å‚ä¸åº¦ã€æ„ŸçŸ¥æ•ˆæœå’Œå¯é æ€§ç­‰æ–¹é¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.04584">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-873abd6135eb4ef9d36dfbe571118b32.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-92c66cb825f3f2edda76eedfae222e90.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f4e4a3e613297ef3f4ba709026fdc9bd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-40db775c0e0775959c34988030f0ccf7.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Pangu-Ultra-MoE-How-to-Train-Your-Big-MoE-on-Ascend-NPUs"><a href="#Pangu-Ultra-MoE-How-to-Train-Your-Big-MoE-on-Ascend-NPUs" class="headerlink" title="Pangu Ultra MoE: How to Train Your Big MoE on Ascend NPUs"></a>Pangu Ultra MoE: How to Train Your Big MoE on Ascend NPUs</h2><p><strong>Authors:Yehui Tang, Yichun Yin, Yaoyuan Wang, Hang Zhou, Yu Pan, Wei Guo, Ziyang Zhang, Miao Rang, Fangcheng Liu, Naifu Zhang, Binghan Li, Yonghan Dong, Xiaojun Meng, Yasheng Wang, Dong Li, Yin Li, Dandan Tu, Can Chen, Youliang Yan, Fisher Yu, Ruiming Tang, Yunhe Wang, Botian Huang, Bo Wang, Boxiao Liu, Changzheng Zhang, Da Kuang, Fei Liu, Gang Huang, Jiansheng Wei, Jiarui Qin, Jie Ran, Jinpeng Li, Jun Zhao, Liang Dai, Lin Li, Liqun Deng, Peifeng Qin, Pengyuan Zeng, Qiang Gu, Shaohua Tang, Shengjun Cheng, Tao Gao, Tao Yu, Tianshu Li, Tianyu Bi, Wei He, Weikai Mao, Wenyong Huang, Wulong Liu, Xiabing Li, Xianzhi Yu, Xueyu Wu, Xu He, Yangkai Du, Yan Xu, Ye Tian, Yimeng Wu, Yongbing Huang, Yong Tian, Yong Zhu, Yue Li, Yufei Wang, Yuhang Gai, Yujun Li, Yu Luo, Yunsheng Ni, Yusen Sun, Zelin Chen, Zhe Liu, Zhicheng Liu, Zhipeng Tu, Zilin Ding, Zongyuan Zhan</strong></p>
<p>Sparse large language models (LLMs) with Mixture of Experts (MoE) and close to a trillion parameters are dominating the realm of most capable language models. However, the massive model scale poses significant challenges for the underlying software and hardware systems. In this paper, we aim to uncover a recipe to harness such scale on Ascend NPUs. The key goals are better usage of the computing resources under the dynamic sparse model structures and materializing the expected performance gain on the actual hardware. To select model configurations suitable for Ascend NPUs without repeatedly running the expensive experiments, we leverage simulation to compare the trade-off of various model hyperparameters. This study led to Pangu Ultra MoE, a sparse LLM with 718 billion parameters, and we conducted experiments on the model to verify the simulation results. On the system side, we dig into Expert Parallelism to optimize the communication between NPU devices to reduce the synchronization overhead. We also optimize the memory efficiency within the devices to further reduce the parameter and activation management overhead. In the end, we achieve an MFU of 30.0% when training Pangu Ultra MoE, with performance comparable to that of DeepSeek R1, on 6K Ascend NPUs, and demonstrate that the Ascend system is capable of harnessing all the training stages of the state-of-the-art language models. Extensive experiments indicate that our recipe can lead to efficient training of large-scale sparse language models with MoE. We also study the behaviors of such models for future reference. </p>
<blockquote>
<p>å…·æœ‰ä¸“å®¶æ··åˆï¼ˆMoEï¼‰çš„ç¨€ç–å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥è¿‘ä¸‡äº¿å‚æ•°ï¼Œæ­£åœ¨æˆä¸ºæœ€å¼ºå¤§çš„è¯­è¨€æ¨¡å‹é¢†åŸŸçš„ä¸»å¯¼ã€‚ç„¶è€Œï¼Œå·¨å¤§çš„æ¨¡å‹è§„æ¨¡ç»™åº•å±‚è½¯ç¡¬ä»¶ç³»ç»Ÿå¸¦æ¥äº†å·¨å¤§çš„æŒ‘æˆ˜ã€‚æœ¬æ–‡æ—¨åœ¨æ­ç¤ºåœ¨Ascend NPUsä¸Šåˆ©ç”¨è¿™ç§è§„æ¨¡çš„ç§˜è¯€ã€‚æˆ‘ä»¬çš„ä¸»è¦ç›®æ ‡æ˜¯æ›´å¥½åœ°åˆ©ç”¨åŠ¨æ€ç¨€ç–æ¨¡å‹ç»“æ„ä¸‹çš„è®¡ç®—èµ„æºï¼Œå¹¶åœ¨å®é™…ç¡¬ä»¶ä¸Šå®ç°é¢„æœŸçš„æ€§èƒ½æå‡ã€‚ä¸ºäº†é€‰æ‹©é€‚åˆAscend NPUsçš„æ¨¡å‹é…ç½®ï¼Œè€Œé¿å…åå¤è¿è¡Œæ˜‚è´µçš„å®éªŒï¼Œæˆ‘ä»¬åˆ©ç”¨ä»¿çœŸæ¥æ¯”è¾ƒå„ç§æ¨¡å‹è¶…å‚æ•°çš„æƒè¡¡ã€‚è¿™é¡¹ç ”ç©¶ä¿ƒæˆäº†Pangu Ultra MoEçš„è¯ç”Ÿï¼Œè¿™æ˜¯ä¸€ä¸ªå…·æœ‰718äº¿å‚æ•°çš„ç¨€ç–LLMï¼Œæˆ‘ä»¬å¯¹è¯¥æ¨¡å‹è¿›è¡Œäº†å®éªŒä»¥éªŒè¯ä»¿çœŸç»“æœã€‚åœ¨ç³»ç»Ÿæ–¹é¢ï¼Œæˆ‘ä»¬æ·±å…¥æŒ–æ˜ä¸“å®¶å¹¶è¡Œæ€§ï¼Œä¼˜åŒ–NPUè®¾å¤‡ä¹‹é—´çš„é€šä¿¡ï¼Œä»¥å‡å°‘åŒæ­¥å¼€é”€ã€‚æˆ‘ä»¬è¿˜ä¼˜åŒ–è®¾å¤‡å†…çš„å†…å­˜æ•ˆç‡ï¼Œè¿›ä¸€æ­¥å‡å°‘å‚æ•°å’Œæ¿€æ´»ç®¡ç†å¼€é”€ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬åœ¨è®­ç»ƒPangu Ultra MoEæ—¶å®ç°äº†30.0%çš„MFUï¼Œåœ¨6K Ascend NPUsä¸Šçš„æ€§èƒ½ä¸DeepSeek R1ç›¸å½“ï¼Œè¯æ˜Ascendç³»ç»Ÿèƒ½å¤Ÿé©¾é©­æœ€æ–°è¯­è¨€æ¨¡å‹çš„æ‰€æœ‰è®­ç»ƒé˜¶æ®µã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ç§˜è¯€å¯ä»¥å¯¼è‡´å…·æœ‰MoEçš„å¤§è§„æ¨¡ç¨€ç–è¯­è¨€æ¨¡å‹çš„é«˜æ•ˆè®­ç»ƒã€‚æˆ‘ä»¬è¿˜ç ”ç©¶äº†æ­¤ç±»æ¨¡å‹çš„è¡Œä¸ºä»¥ä¾›æœªæ¥å‚è€ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.04519v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>ç¨€ç–å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç»“åˆä¸“å®¶æ··åˆï¼ˆMoEï¼‰æŠ€æœ¯ï¼Œæ‹¥æœ‰æ¥è¿‘ä¸‡äº¿å‚æ•°ï¼Œæˆä¸ºå½“å‰æœ€å¼ºå¤§çš„è¯­è¨€æ¨¡å‹é¢†åŸŸçš„ä¸»å¯¼è€…ã€‚ç„¶è€Œï¼Œå¤§è§„æ¨¡æ¨¡å‹å¯¹åº•å±‚è½¯ç¡¬ä»¶ç³»ç»Ÿæå‡ºäº†å·¨å¤§æŒ‘æˆ˜ã€‚æœ¬æ–‡æ—¨åœ¨æ­ç¤ºåœ¨Ascend NPUsä¸Šåˆ©ç”¨è¿™ç§è§„æ¨¡çš„æ¨¡å‹çš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„ä¸»è¦ç›®æ ‡æ˜¯åœ¨åŠ¨æ€ç¨€ç–æ¨¡å‹ç»“æ„ä¸‹æ›´å¥½åœ°åˆ©ç”¨è®¡ç®—èµ„æºï¼Œå¹¶åœ¨å®é™…ç¡¬ä»¶ä¸Šå®ç°é¢„æœŸçš„æ€§èƒ½æå‡ã€‚æˆ‘ä»¬é€šè¿‡æ¨¡æ‹Ÿæ¯”è¾ƒä¸åŒæ¨¡å‹è¶…å‚æ•°çš„æƒè¡¡ï¼Œä»¥é€‰æ‹©é€‚åˆAscend NPUsçš„æ¨¡å‹é…ç½®ã€‚è¿™é¡¹ç ”ç©¶è¯ç”Ÿäº†åºå¤Ultra MoEè¿™ä¸€ç¨€ç–LLMï¼Œæ‹¥æœ‰718äº¿å‚æ•°ã€‚æˆ‘ä»¬å¯¹è¯¥æ¨¡å‹è¿›è¡Œå®éªŒï¼ŒéªŒè¯äº†æ¨¡æ‹Ÿç»“æœã€‚åœ¨ç³»ç»Ÿå±‚é¢ï¼Œæˆ‘ä»¬æ·±å…¥ç ”ç©¶äº†ä¸“å®¶å¹¶è¡Œæ€§ä»¥ä¼˜åŒ–NPUè®¾å¤‡ä¹‹é—´çš„é€šä¿¡ï¼Œå‡å°‘åŒæ­¥å¼€é”€ã€‚æˆ‘ä»¬è¿˜ä¼˜åŒ–äº†è®¾å¤‡å†…çš„å†…å­˜æ•ˆç‡ï¼Œè¿›ä¸€æ­¥å‡å°‘äº†å‚æ•°å’Œæ¿€æ´»ç®¡ç†å¼€é”€ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬åœ¨è®­ç»ƒåºå¤Ultra MoEæ—¶å®ç°äº†30.0%çš„MFUï¼ˆä¸€ç§æ€§èƒ½æŒ‡æ ‡ï¼‰ï¼Œåœ¨6K Ascend NPUsä¸Šçš„æ€§èƒ½ä¸DeepSeek R1ç›¸å½“ï¼Œè¯æ˜äº†Ascendç³»ç»Ÿèƒ½å¤Ÿåº”å¯¹æœ€å…ˆè¿›è¯­è¨€æ¨¡å‹çš„æ‰€æœ‰è®­ç»ƒé˜¶æ®µã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿå®ç°å¤§è§„æ¨¡ç¨€ç–è¯­è¨€æ¨¡å‹çš„é«˜æ•ˆè®­ç»ƒã€‚æˆ‘ä»¬è¿˜ç ”ç©¶äº†æ­¤ç±»æ¨¡å‹çš„è¡Œä¸ºï¼Œä»¥ä¾›æœªæ¥å‚è€ƒã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç¨€ç–å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç»“åˆä¸“å®¶æ··åˆï¼ˆMoEï¼‰æˆä¸ºå½“å‰é¢†å…ˆçš„å¼ºå¤§è¯­è¨€æ¨¡å‹ã€‚</li>
<li>é¢ä¸´å¤§è§„æ¨¡æ¨¡å‹çš„æŒ‘æˆ˜ï¼Œéœ€è¦åœ¨Ascend NPUsä¸Šä¼˜åŒ–åˆ©ç”¨è®¡ç®—èµ„æºçš„æ–¹æ³•ã€‚</li>
<li>é€šè¿‡æ¨¡æ‹Ÿæ¯”è¾ƒä¸åŒæ¨¡å‹è¶…å‚æ•°çš„æƒè¡¡æ¥é€‰æ‹©é€‚åˆAscend NPUsçš„æ¨¡å‹é…ç½®ã€‚</li>
<li>åºå¤Ultra MoEæ˜¯ä¸€ä¸ªç¨€ç–LLMï¼Œæ‹¥æœ‰718äº¿å‚æ•°ï¼Œé€šè¿‡å®éªŒéªŒè¯äº†å…¶æ€§èƒ½ã€‚</li>
<li>ä¼˜åŒ–ä¸“å®¶å¹¶è¡Œæ€§ä»¥å‡å°‘NPUè®¾å¤‡é—´çš„åŒæ­¥å¼€é”€ï¼ŒåŒæ—¶ä¼˜åŒ–å†…å­˜æ•ˆç‡ã€‚</li>
<li>åœ¨è®­ç»ƒåºå¤Ultra MoEæ—¶å®ç°äº†è¾ƒé«˜çš„MFUæ€§èƒ½ï¼Œä¸DeepSeek R1ç›¸å½“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.04519">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-553d19d6b81ca9368dceaf9d8282419e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e390b395ffd3ef8a44030ead5123007d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-78e6b95c0d5127df86f9092ee335fe07.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8a9904f0a4a64ab4f431cc55d018692a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1e3fedd7592faa499fe64b68eb72a2bc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d443237ede60658776509c2d994eb74e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7a26c7e42c389e8e2b6d8b6e6f25943e.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="CAD-Llama-Leveraging-Large-Language-Models-for-Computer-Aided-Design-Parametric-3D-Model-Generation"><a href="#CAD-Llama-Leveraging-Large-Language-Models-for-Computer-Aided-Design-Parametric-3D-Model-Generation" class="headerlink" title="CAD-Llama: Leveraging Large Language Models for Computer-Aided Design   Parametric 3D Model Generation"></a>CAD-Llama: Leveraging Large Language Models for Computer-Aided Design   Parametric 3D Model Generation</h2><p><strong>Authors:Jiahao Li, Weijian Ma, Xueyang Li, Yunzhong Lou, Guichun Zhou, Xiangdong Zhou</strong></p>
<p>Recently, Large Language Models (LLMs) have achieved significant success, prompting increased interest in expanding their generative capabilities beyond general text into domain-specific areas. This study investigates the generation of parametric sequences for computer-aided design (CAD) models using LLMs. This endeavor represents an initial step towards creating parametric 3D shapes with LLMs, as CAD model parameters directly correlate with shapes in three-dimensional space. Despite the formidable generative capacities of LLMs, this task remains challenging, as these models neither encounter parametric sequences during their pretraining phase nor possess direct awareness of 3D structures. To address this, we present CAD-Llama, a framework designed to enhance pretrained LLMs for generating parametric 3D CAD models. Specifically, we develop a hierarchical annotation pipeline and a code-like format to translate parametric 3D CAD command sequences into Structured Parametric CAD Code (SPCC), incorporating hierarchical semantic descriptions. Furthermore, we propose an adaptive pretraining approach utilizing SPCC, followed by an instruction tuning process aligned with CAD-specific guidelines. This methodology aims to equip LLMs with the spatial knowledge inherent in parametric sequences. Experimental results demonstrate that our framework significantly outperforms prior autoregressive methods and existing LLM baselines. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å–å¾—äº†å·¨å¤§æˆåŠŸï¼Œå¼•å‘äº†å¯¹å…¶ç”Ÿæˆèƒ½åŠ›ä»ä¸€èˆ¬æ–‡æœ¬æ‰©å±•åˆ°ç‰¹å®šé¢†åŸŸå…´è¶£çš„å¢åŠ ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†ä½¿ç”¨LLMä¸ºè®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰æ¨¡å‹ç”Ÿæˆå‚æ•°åºåˆ—ã€‚è¿™ä¸€åŠªåŠ›æ˜¯æœç€ä½¿ç”¨LLMåˆ›å»ºå‚æ•°åŒ–3Då½¢çŠ¶çš„ç¬¬ä¸€æ­¥ï¼Œå› ä¸ºCADæ¨¡å‹å‚æ•°ç›´æ¥ä¸ä¸‰ç»´ç©ºé—´ä¸­çš„å½¢çŠ¶ç›¸å…³è”ã€‚å°½ç®¡LLMå…·æœ‰å¼ºå¤§çš„ç”Ÿæˆèƒ½åŠ›ï¼Œä½†è¿™ä¸€ä»»åŠ¡ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºè¿™äº›æ¨¡å‹åœ¨é¢„è®­ç»ƒé˜¶æ®µå¹¶æœªé‡åˆ°å‚æ•°åºåˆ—ï¼Œä¹Ÿä¸å…·å¤‡å¯¹ä¸‰ç»´ç»“æ„çš„ç›´æ¥æ„è¯†ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†CAD-Llamaæ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼ºé¢„è®­ç»ƒçš„LLMç”Ÿæˆå‚æ•°åŒ–3D CADæ¨¡å‹çš„èƒ½åŠ›ã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§åˆ†å±‚æ³¨é‡Šç®¡é“å’Œä¸€ç§ç±»ä¼¼äºä»£ç çš„æ ¼å¼ï¼Œå°†å‚æ•°åŒ–çš„ä¸‰ç»´CADå‘½ä»¤åºåˆ—ç¿»è¯‘æˆç»“æ„åŒ–å‚æ•°CADä»£ç ï¼ˆSPCCï¼‰ï¼Œå¹¶èå…¥åˆ†å±‚è¯­ä¹‰æè¿°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ©ç”¨SPCCçš„è‡ªé€‚åº”é¢„è®­ç»ƒæ–¹æ³•ï¼Œéšåæ˜¯ä¸€ä¸ªä¸CADç‰¹å®šå‡†åˆ™å¯¹é½çš„æŒ‡ä»¤è°ƒæ•´è¿‡ç¨‹ã€‚è¯¥æ–¹æ³•æ—¨åœ¨è®©LLMå…·å¤‡å‚æ•°åºåˆ—ä¸­å›ºæœ‰çš„ç©ºé—´çŸ¥è¯†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶æ˜¾è‘—ä¼˜äºå…ˆå‰çš„è‡ªå›å½’æ–¹æ³•å’Œç°æœ‰çš„LLMåŸºçº¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.04481v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç”Ÿæˆé€šç”¨æ–‡æœ¬æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œç°åœ¨æ­£æ‰©å±•åˆ°ç‰¹å®šé¢†åŸŸçš„ç”Ÿæˆèƒ½åŠ›ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†ä½¿ç”¨LLMç”Ÿæˆè®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰æ¨¡å‹çš„å‚æ•°åºåˆ—ã€‚è¿™æ˜¯å‘åˆ©ç”¨LLMåˆ›å»ºå‚æ•°åŒ–ä¸‰ç»´å½¢çŠ¶è¿ˆå‡ºçš„åˆæ­¥å°è¯•ï¼Œå› ä¸ºCADæ¨¡å‹å‚æ•°ä¸ä¸‰ç»´ç©ºé—´ä¸­çš„å½¢çŠ¶ç›´æ¥ç›¸å…³ã€‚å°½ç®¡LLMå…·æœ‰å¼ºå¤§çš„ç”Ÿæˆèƒ½åŠ›ï¼Œä½†æ­¤ä»»åŠ¡ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºè¿™äº›æ¨¡å‹åœ¨é¢„è®­ç»ƒé˜¶æ®µå¹¶æœªé‡åˆ°å‚æ•°åºåˆ—ï¼Œä¹Ÿç¼ºä¹å¯¹ä¸‰ç»´ç»“æ„çš„ç›´æ¥äº†è§£ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†CAD-Llamaæ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼ºé¢„è®­ç»ƒçš„LLMç”Ÿæˆå‚æ•°åŒ–ä¸‰ç»´CADæ¨¡å‹çš„èƒ½åŠ›ã€‚é€šè¿‡å¼€å‘å±‚æ¬¡åŒ–æ³¨é‡Šç®¡é“å’Œç±»ä¼¼ä»£ç çš„æ ¼å¼ï¼Œå°†å‚æ•°åŒ–ä¸‰ç»´CADå‘½ä»¤åºåˆ—ç¿»è¯‘æˆç»“æ„åŒ–å‚æ•°åŒ–CADä»£ç ï¼ˆSPCCï¼‰ï¼Œå¹¶ç»“åˆå±‚æ¬¡åŒ–è¯­ä¹‰æè¿°ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§åˆ©ç”¨SPCCçš„è‡ªé€‚åº”é¢„è®­ç»ƒæ–¹æ³•ï¼Œéšåæ˜¯ç¬¦åˆCADç‰¹å®šæŒ‡å¯¼çš„æŒ‡ä»¤è°ƒæ•´è¿‡ç¨‹ã€‚è¯¥æ–¹æ³•æ—¨åœ¨èµ‹äºˆLLMå‚æ•°åºåˆ—ä¸­çš„ç©ºé—´çŸ¥è¯†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶æ˜¾è‘—ä¼˜äºå…ˆå‰çš„è‡ªå›å½’æ–¹æ³•å’Œç°æœ‰çš„LLMåŸºçº¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç”Ÿæˆé€šç”¨æ–‡æœ¬æ–¹é¢çš„æˆåŠŸä¿ƒä½¿äº†å¯¹æ‰©å±•å…¶åœ¨ç‰¹å®šé¢†åŸŸç”Ÿæˆèƒ½åŠ›çš„ç ”ç©¶ã€‚</li>
<li>æœ¬ç ”ç©¶å…³æ³¨ä½¿ç”¨LLMç”Ÿæˆè®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰æ¨¡å‹çš„å‚æ•°åºåˆ—ï¼Œä¸ºåˆ›å»ºå‚æ•°åŒ–ä¸‰ç»´å½¢çŠ¶è¿ˆå‡ºäº†åˆæ­¥å°è¯•ã€‚</li>
<li>LLMåœ¨é¢„è®­ç»ƒé˜¶æ®µå¹¶æœªé‡åˆ°å‚æ•°åºåˆ—ï¼Œå› æ­¤æ­¤ä»»åŠ¡å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>CAD-Llamaæ¡†æ¶æ—¨åœ¨å¢å¼ºLLMç”Ÿæˆå‚æ•°åŒ–ä¸‰ç»´CADæ¨¡å‹çš„èƒ½åŠ›ã€‚</li>
<li>CAD-Llamaé€šè¿‡å¼€å‘å±‚æ¬¡åŒ–æ³¨é‡Šç®¡é“å’Œç±»ä¼¼ä»£ç çš„æ ¼å¼æ¥å®ç°å¯¹å‚æ•°åŒ–ä¸‰ç»´CADå‘½ä»¤åºåˆ—çš„ç¿»è¯‘ã€‚</li>
<li>è¯¥æ¡†æ¶ç»“åˆäº†è‡ªé€‚åº”é¢„è®­ç»ƒæ–¹æ³•å’ŒæŒ‡ä»¤è°ƒæ•´è¿‡ç¨‹ï¼Œä»¥èµ‹äºˆLLMå‚æ•°åºåˆ—ä¸­çš„ç©ºé—´çŸ¥è¯†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.04481">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-70e7bb5b8ac6769051cbe9e8d657f8a3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fcacddc0913d7cbcd6b76b78ec37cf32.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a3c0bdf1feeb665cfaec7359b2c7d4f9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-823a2fff5409310c69650c11412405ee.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="TrajEvo-Designing-Trajectory-Prediction-Heuristics-via-LLM-driven-Evolution"><a href="#TrajEvo-Designing-Trajectory-Prediction-Heuristics-via-LLM-driven-Evolution" class="headerlink" title="TrajEvo: Designing Trajectory Prediction Heuristics via LLM-driven   Evolution"></a>TrajEvo: Designing Trajectory Prediction Heuristics via LLM-driven   Evolution</h2><p><strong>Authors:Zhikai Zhao, Chuanbo Hua, Federico Berto, Kanghoon Lee, Zihan Ma, Jiachen Li, Jinkyoo Park</strong></p>
<p>Trajectory prediction is a crucial task in modeling human behavior, especially in fields as social robotics and autonomous vehicle navigation. Traditional heuristics based on handcrafted rules often lack accuracy, while recently proposed deep learning approaches suffer from computational cost, lack of explainability, and generalization issues that limit their practical adoption. In this paper, we introduce TrajEvo, a framework that leverages Large Language Models (LLMs) to automatically design trajectory prediction heuristics. TrajEvo employs an evolutionary algorithm to generate and refine prediction heuristics from past trajectory data. We introduce a Cross-Generation Elite Sampling to promote population diversity and a Statistics Feedback Loop allowing the LLM to analyze alternative predictions. Our evaluations show TrajEvo outperforms previous heuristic methods on the ETH-UCY datasets, and remarkably outperforms both heuristics and deep learning methods when generalizing to the unseen SDD dataset. TrajEvo represents a first step toward automated design of fast, explainable, and generalizable trajectory prediction heuristics. We make our source code publicly available to foster future research at <a target="_blank" rel="noopener" href="https://github.com/ai4co/trajevo">https://github.com/ai4co/trajevo</a>. </p>
<blockquote>
<p>è½¨è¿¹é¢„æµ‹æ˜¯æ¨¡æ‹Ÿäººç±»è¡Œä¸ºçš„é‡è¦ä»»åŠ¡ï¼Œç‰¹åˆ«æ˜¯åœ¨ç¤¾äº¤æœºå™¨äººå’Œè‡ªåŠ¨é©¾é©¶å¯¼èˆªç­‰é¢†åŸŸã€‚åŸºäºæ‰‹å·¥è§„åˆ™çš„ä¼ ç»Ÿå¯å‘å¼æ–¹æ³•é€šå¸¸ç¼ºä¹å‡†ç¡®æ€§ï¼Œè€Œæœ€è¿‘æå‡ºçš„æ·±åº¦å­¦ä¹ æ–¹æ³•åˆ™å­˜åœ¨è®¡ç®—æˆæœ¬é«˜ã€ç¼ºä¹å¯è§£é‡Šæ€§å’Œæ³›åŒ–é—®é¢˜ï¼Œè¿™äº›é—®é¢˜é™åˆ¶äº†å®ƒä»¬çš„å®é™…åº”ç”¨ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†TrajEvoæ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è‡ªåŠ¨è®¾è®¡è½¨è¿¹é¢„æµ‹å¯å‘å¼æ–¹æ³•ã€‚TrajEvoé‡‡ç”¨è¿›åŒ–ç®—æ³•ï¼Œæ ¹æ®è¿‡å»çš„è½¨è¿¹æ•°æ®ç”Ÿæˆå’Œç»†åŒ–é¢„æµ‹å¯å‘å¼æ–¹æ³•ã€‚æˆ‘ä»¬å¼•å…¥äº†è·¨ä»£ç²¾è‹±é‡‡æ ·æ³•ä»¥ä¿ƒè¿›ç§ç¾¤å¤šæ ·æ€§ï¼Œå¹¶å»ºç«‹äº†ç»Ÿè®¡åé¦ˆå¾ªç¯ï¼Œä½¿LLMèƒ½å¤Ÿåˆ†ææ›¿ä»£é¢„æµ‹ã€‚æˆ‘ä»¬çš„è¯„ä¼°æ˜¾ç¤ºï¼ŒTrajEvoåœ¨ETH-UCYæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå…ˆå‰çš„å¯å‘å¼æ–¹æ³•ï¼Œå¹¶ä¸”åœ¨æ¨å¹¿åˆ°æœªè§è¿‡çš„SDDæ•°æ®é›†æ—¶ï¼Œå®ƒæ˜¾è‘—ä¼˜äºå¯å‘å¼æ–¹æ³•å’Œæ·±åº¦å­¦ä¹ æ–¹æ³•ã€‚TrajEvoæœç€è‡ªåŠ¨è®¾è®¡å¿«é€Ÿã€å¯è§£é‡Šå’Œé€šç”¨çš„è½¨è¿¹é¢„æµ‹å¯å‘å¼æ–¹æ³•è¿ˆå‡ºäº†ç¬¬ä¸€æ­¥ã€‚ä¸ºäº†è®©æœªæ¥çš„ç ”ç©¶å¾—ä»¥å‘å±•ï¼Œæˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/ai4co/trajevo%E5%85%AC%E5%BC%80%E4%BA%86%E6%88%91%E4%BB%AC%E7%9A%84%E6%BA%90%E4%BB%A3%E7%A0%81%E3%80%82">https://github.com/ai4co/trajevoå…¬å¼€äº†æˆ‘ä»¬çš„æºä»£ç ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.04480v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è½¨è¿¹é¢„æµ‹æ˜¯æ¨¡æ‹Ÿäººç±»è¡Œä¸ºçš„é‡è¦ä»»åŠ¡ï¼Œå°¤å…¶åœ¨ç¤¾äº¤æœºå™¨äººå’Œè‡ªåŠ¨é©¾é©¶å¯¼èˆªç­‰é¢†åŸŸã€‚ä¼ ç»ŸåŸºäºæ‰‹å·¥è§„åˆ™çš„æ–¹æ³•å¸¸å¸¸ç¼ºä¹å‡†ç¡®æ€§ï¼Œè€Œæ–°è¿‘æå‡ºçš„æ·±åº¦å­¦ä¹ æ–¹æ³•åˆ™å­˜åœ¨è®¡ç®—æˆæœ¬é«˜ã€ç¼ºä¹å¯è§£é‡Šæ€§å’Œæ³›åŒ–é—®é¢˜ç­‰å®è·µåº”ç”¨ä¸­çš„å±€é™ã€‚æœ¬æ–‡ä»‹ç»TrajEvoæ¡†æ¶ï¼Œå®ƒåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è‡ªåŠ¨è®¾è®¡è½¨è¿¹é¢„æµ‹è§„åˆ™ã€‚TrajEvoé‡‡ç”¨è¿›åŒ–ç®—æ³•ä»è¿‡å»çš„è½¨è¿¹æ•°æ®ä¸­ç”Ÿæˆå’Œä¼˜åŒ–é¢„æµ‹è§„åˆ™ã€‚é€šè¿‡å¼•å…¥è·¨ä»£ç²¾è‹±é‡‡æ ·ä»¥ä¿ƒè¿›ç§ç¾¤å¤šæ ·æ€§å’Œç»Ÿè®¡åé¦ˆç¯ï¼Œä½¿LLMèƒ½å¤Ÿåˆ†ææ›¿ä»£é¢„æµ‹ã€‚è¯„ä¼°è¡¨æ˜ï¼ŒTrajEvoåœ¨ETH-UCYæ•°æ®é›†ä¸Šä¼˜äºä¼ ç»Ÿå¯å‘å¼æ–¹æ³•ï¼Œå¹¶ä¸”åœ¨æœªè§è¿‡çš„SDDæ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºå¯å‘å¼å’Œæ·±åº¦å­¦ä¹ æ–¹æ³•ã€‚TrajEvoæœç€è‡ªåŠ¨è®¾è®¡å¿«é€Ÿã€å¯è§£é‡Šå’Œé€šç”¨çš„è½¨è¿¹é¢„æµ‹è§„åˆ™è¿ˆå‡ºäº†ç¬¬ä¸€æ­¥ã€‚æˆ‘ä»¬çš„æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/ai4co/trajevo%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%EF%BC%8C%E4%BB%A5%E4%BF%83%E8%BF%9B%E6%9C%AA%E6%9D%A5%E7%A0%94%E7%A9%B6%E3%80%82">https://github.com/ai4co/trajevoå…¬å¼€è®¿é—®ï¼Œä»¥ä¿ƒè¿›æœªæ¥ç ”ç©¶ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è½¨è¿¹é¢„æµ‹æ˜¯æ¨¡æ‹Ÿäººç±»è¡Œä¸ºçš„å…³é”®ä»»åŠ¡ï¼Œç‰¹åˆ«æ˜¯åœ¨ç¤¾äº¤æœºå™¨äººå’Œè‡ªåŠ¨é©¾é©¶å¯¼èˆªé¢†åŸŸã€‚</li>
<li>ä¼ ç»ŸåŸºäºæ‰‹å·¥è§„åˆ™çš„æ–¹æ³•å‡†ç¡®æ€§ä¸è¶³ï¼Œè€Œæ·±åº¦å­¦ä¹ æ–¹æ³•å­˜åœ¨è®¡ç®—æˆæœ¬é«˜ã€ç¼ºä¹å¯è§£é‡Šæ€§å’Œæ³›åŒ–é—®é¢˜ã€‚</li>
<li>TrajEvoæ¡†æ¶åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è‡ªåŠ¨è®¾è®¡è½¨è¿¹é¢„æµ‹è§„åˆ™ï¼Œé€šè¿‡è¿›åŒ–ç®—æ³•ä¼˜åŒ–é¢„æµ‹è§„åˆ™ã€‚</li>
<li>TrajEvoé‡‡ç”¨è·¨ä»£ç²¾è‹±é‡‡æ ·å’Œç»Ÿè®¡åé¦ˆç¯æœºåˆ¶ï¼Œæé«˜é¢„æµ‹æ€§èƒ½å¹¶ä¿ƒè¿›LLMåˆ†ææ›¿ä»£é¢„æµ‹ã€‚</li>
<li>TrajEvoåœ¨ETH-UCYæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜äºä¼ ç»Ÿå¯å‘å¼æ–¹æ³•ã€‚</li>
<li>TrajEvoåœ¨æœªè§è¿‡çš„SDDæ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºå¯å‘å¼å’Œæ·±åº¦å­¦ä¹ æ–¹æ³•ï¼Œæ˜¾ç¤ºå‡ºå…¶è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.04480">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d39aba531f0059203348accd0697aaaa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7d3b663a59dd397f421c0ae21fd24ed9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e4b6f7ea902b0e03a0e786f3397f6b44.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8df9c86069080eacb5e74b2b90e7cd5b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d6eb3cabe531046ce3e07035bb3a1dbe.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2fef9841c2a4e5212335f0b214f17175.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="M2Rec-Multi-scale-Mamba-for-Efficient-Sequential-Recommendation"><a href="#M2Rec-Multi-scale-Mamba-for-Efficient-Sequential-Recommendation" class="headerlink" title="M2Rec: Multi-scale Mamba for Efficient Sequential Recommendation"></a>M2Rec: Multi-scale Mamba for Efficient Sequential Recommendation</h2><p><strong>Authors:Qianru Zhang, Liang Qu, Honggang Wen, Dong Huang, Siu-Ming Yiu, Nguyen Quoc Viet Hung, Hongzhi Yin</strong></p>
<p>Sequential recommendation systems aim to predict usersâ€™ next preferences based on their interaction histories, but existing approaches face critical limitations in efficiency and multi-scale pattern recognition. While Transformer-based methods struggle with quadratic computational complexity, recent Mamba-based models improve efficiency but fail to capture periodic user behaviors, leverage rich semantic information, or effectively fuse multimodal features. To address these challenges, we propose \model, a novel sequential recommendation framework that integrates multi-scale Mamba with Fourier analysis, Large Language Models (LLMs), and adaptive gating. First, we enhance Mamba with Fast Fourier Transform (FFT) to explicitly model periodic patterns in the frequency domain, separating meaningful trends from noise. Second, we incorporate LLM-based text embeddings to enrich sparse interaction data with semantic context from item descriptions. Finally, we introduce a learnable gate mechanism to dynamically balance temporal (Mamba), frequency (FFT), and semantic (LLM) features, ensuring harmonious multimodal fusion. Extensive experiments demonstrate that \model\ achieves state-of-the-art performance, improving Hit Rate@10 by 3.2% over existing Mamba-based models while maintaining 20% faster inference than Transformer baselines. Our results highlight the effectiveness of combining frequency analysis, semantic understanding, and adaptive fusion for sequential recommendation. Code and datasets are available at: <a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/M2Rec">https://anonymous.4open.science/r/M2Rec</a>. </p>
<blockquote>
<p>åºåˆ—æ¨èç³»ç»Ÿçš„ç›®æ ‡æ˜¯åŸºäºç”¨æˆ·çš„äº¤äº’å†å²é¢„æµ‹å…¶ä¸‹ä¸€ä¸ªåå¥½ï¼Œä½†ç°æœ‰æ–¹æ³•åœ¨æ•ˆç‡å’Œå¤šå°ºåº¦æ¨¡å¼è¯†åˆ«æ–¹é¢é¢ä¸´å…³é”®å±€é™ã€‚è™½ç„¶åŸºäºTransformerçš„æ–¹æ³•é¢ä¸´äºŒæ¬¡è®¡ç®—å¤æ‚åº¦çš„é—®é¢˜ï¼Œæœ€è¿‘çš„åŸºäºMambaçš„æ¨¡å‹æé«˜äº†æ•ˆç‡ï¼Œä½†æ— æ³•æ•æ‰ç”¨æˆ·å‘¨æœŸæ€§è¡Œä¸ºï¼Œåˆ©ç”¨ä¸°å¯Œçš„è¯­ä¹‰ä¿¡æ¯ï¼Œæˆ–æœ‰æ•ˆåœ°èåˆå¤šæ¨¡æ€ç‰¹å¾ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†M2Recæ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„åºåˆ—æ¨èæ¡†æ¶ï¼Œå®ƒå°†å¤šå°ºåº¦Mambaä¸å‚…é‡Œå¶åˆ†æã€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œè‡ªé€‚åº”é—¨æ§æœºåˆ¶ç›¸ç»“åˆã€‚é¦–å…ˆï¼Œæˆ‘ä»¬é€šè¿‡å¿«é€Ÿå‚…é‡Œå¶å˜æ¢ï¼ˆFFTï¼‰å¢å¼ºMambaï¼Œä»¥æ˜¾å¼åœ°æ¨¡æ‹Ÿé¢‘ç‡åŸŸä¸­çš„å‘¨æœŸæ€§æ¨¡å¼ï¼Œå°†æœ‰æ„ä¹‰çš„è¶‹åŠ¿ä¸å™ªå£°åŒºåˆ†å¼€ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬èå…¥åŸºäºLLMçš„æ–‡æœ¬åµŒå…¥ï¼Œä»¥ä¸°å¯Œç¨€ç–äº¤äº’æ•°æ®ï¼ŒåŠ å…¥é¡¹ç›®æè¿°çš„è¯­ä¹‰ä¸Šä¸‹æ–‡ã€‚æœ€åï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å¯å­¦ä¹ çš„é—¨æ§æœºåˆ¶ï¼Œä»¥åŠ¨æ€å¹³è¡¡æ—¶é—´ï¼ˆMambaï¼‰ã€é¢‘ç‡ï¼ˆFFTï¼‰å’Œè¯­ä¹‰ï¼ˆLLMï¼‰ç‰¹å¾ï¼Œç¡®ä¿å’Œè°çš„å¤šæ¨¡æ€èåˆã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒM2Recæ¨¡å‹è¾¾åˆ°äº†æœ€æ–°çš„æ€§èƒ½æ°´å¹³ï¼Œåœ¨ç°æœ‰åŸºäºMambaçš„æ¨¡å‹çš„åŸºç¡€ä¸Šï¼ŒHit Rate@10æé«˜äº†3.2%ï¼ŒåŒæ—¶ä¿æŒæ¯”TransformeråŸºå‡†æµ‹è¯•å¿«20%çš„æ¨ç†é€Ÿåº¦ã€‚æˆ‘ä»¬çš„ç»“æœçªå‡ºäº†ç»“åˆé¢‘ç‡åˆ†æã€è¯­ä¹‰ç†è§£å’Œè‡ªé€‚åº”èåˆåœ¨åºåˆ—æ¨èä¸­çš„æœ‰æ•ˆæ€§ã€‚ç›¸å…³ä»£ç å’Œæ•°æ®é›†å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/M2Rec%E8%8E%B7%E5%8F%96%E3%80%82">https://anonymous.4open.science/r/M2Recè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.04445v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹çš„é¡ºåºæ¨èæ¡†æ¶ï¼Œè¯¥æ¡†æ¶é›†æˆäº†å¤šå°ºåº¦Mambaã€å‚…é‡Œå¶åˆ†æã€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œè‡ªé€‚åº”é—¨æ§æœºåˆ¶ã€‚å®ƒé€šè¿‡å¿«é€Ÿå‚…é‡Œå¶å˜æ¢ï¼ˆFFTï¼‰å¢å¼ºMambaï¼Œä»¥æ˜¾å¼å»ºæ¨¡é¢‘ç‡åŸŸä¸­çš„å‘¨æœŸæ€§æ¨¡å¼ï¼ŒåŒæ—¶èå…¥LLMæ–‡æœ¬åµŒå…¥ï¼Œä¸°å¯Œç¨€ç–äº¤äº’æ•°æ®å¹¶åŠ å…¥é¡¹ç›®æè¿°çš„è¯­ä¹‰ä¸Šä¸‹æ–‡ã€‚æ­¤å¤–ï¼Œå¼•å…¥äº†ä¸€ç§å¯å­¦ä¹ çš„é—¨æ§æœºåˆ¶ï¼ŒåŠ¨æ€å¹³è¡¡æ—¶é—´ï¼ˆMambaï¼‰ã€é¢‘ç‡ï¼ˆFFTï¼‰å’Œè¯­ä¹‰ï¼ˆLLMï¼‰ç‰¹å¾ï¼Œç¡®ä¿å’Œè°çš„å¤šæ¨¡å¼èåˆã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨ç°æœ‰Mambaæ¨¡å‹çš„åŸºç¡€ä¸Šæé«˜äº†Hit Rate@10è¾¾3.2%ï¼ŒåŒæ—¶ä¿æŒæ¯”TransformeråŸºçº¿æ›´å¿«çš„æ¨ç†é€Ÿåº¦ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>é¡ºåºæ¨èç³»ç»ŸåŸºäºç”¨æˆ·äº¤äº’å†å²é¢„æµ‹å…¶ä¸‹ä¸€ä¸ªåå¥½ã€‚</li>
<li>ç°æœ‰æ–¹æ³•é¢ä¸´æ•ˆç‡å’Œå¤šå°ºåº¦æ¨¡å¼è¯†åˆ«æ–¹é¢çš„å…³é”®é™åˆ¶ã€‚</li>
<li>æå‡ºçš„æ¨¡å‹é›†æˆäº†å¤šå°ºåº¦Mambaã€å‚…é‡Œå¶åˆ†æã€LLMå’Œè‡ªé€‚åº”é—¨æ§æœºåˆ¶ã€‚</li>
<li>ä½¿ç”¨å¿«é€Ÿå‚…é‡Œå¶å˜æ¢ï¼ˆFFTï¼‰å¢å¼ºMambaï¼Œä»¥æ˜¾å¼å»ºæ¨¡é¢‘ç‡åŸŸä¸­çš„å‘¨æœŸæ€§æ¨¡å¼ã€‚</li>
<li>èå…¥LLMæ–‡æœ¬åµŒå…¥ï¼Œä¸°å¯Œç¨€ç–äº¤äº’æ•°æ®å¹¶åŠ å…¥é¡¹ç›®æè¿°çš„è¯­ä¹‰ä¸Šä¸‹æ–‡ã€‚</li>
<li>å¼•å…¥å¯å­¦ä¹ çš„é—¨æ§æœºåˆ¶ï¼ŒåŠ¨æ€å¹³è¡¡æ—¶é—´ã€é¢‘ç‡å’Œè¯­ä¹‰ç‰¹å¾ã€‚</li>
<li>æ¨¡å‹åœ¨æ€§èƒ½ä¸Šå®ç°äº†æ˜¾è‘—çš„æå‡ï¼Œä¸”åœ¨æ¨ç†é€Ÿåº¦ä¸Šä¹Ÿå…·æœ‰ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.04445">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-aed761271032cef45aed2f9bce64688c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d3ca2af4b02709aae482bdc73b11a42c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-447a8efc230dacf62073b3d5e8b590e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8865799589a393aa5db489c7dbac468f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d432acdd181f57c733b8fe16939c61ae.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="The-Aloe-Family-Recipe-for-Open-and-Specialized-Healthcare-LLMs"><a href="#The-Aloe-Family-Recipe-for-Open-and-Specialized-Healthcare-LLMs" class="headerlink" title="The Aloe Family Recipe for Open and Specialized Healthcare LLMs"></a>The Aloe Family Recipe for Open and Specialized Healthcare LLMs</h2><p><strong>Authors:Dario Garcia-Gasulla, Jordi Bayarri-Planas, Ashwin Kumar Gururajan, Enrique Lopez-Cuena, Adrian Tormos, Daniel Hinjos, Pablo Bernabeu-Perez, Anna Arias-Duart, Pablo Agustin Martin-Torres, Marta Gonzalez-Mallo, Sergio Alvarez-Napagao, Eduard AyguadÃ©-Parra, Ulises CortÃ©s</strong></p>
<p>Purpose: With advancements in Large Language Models (LLMs) for healthcare, the need arises for competitive open-source models to protect the public interest. This work contributes to the field of open medical LLMs by optimizing key stages of data preprocessing and training, while showing how to improve model safety (through DPO) and efficacy (through RAG). The evaluation methodology used, which includes four different types of tests, defines a new standard for the field. The resultant models, shown to be competitive with the best private alternatives, are released with a permisive license.   Methods: Building on top of strong base models like Llama 3.1 and Qwen 2.5, Aloe Beta uses a custom dataset to enhance public data with synthetic Chain of Thought examples. The models undergo alignment with Direct Preference Optimization, emphasizing ethical and policy-aligned performance in the presence of jailbreaking attacks. Evaluation includes close-ended, open-ended, safety and human assessments, to maximize the reliability of results.   Results: Recommendations are made across the entire pipeline, backed by the solid performance of the Aloe Family. These models deliver competitive performance across healthcare benchmarks and medical fields, and are often preferred by healthcare professionals. On bias and toxicity, the Aloe Beta models significantly improve safety, showing resilience to unseen jailbreaking attacks. For a responsible release, a detailed risk assessment specific to healthcare is attached to the Aloe Family models.   Conclusion: The Aloe Beta models, and the recipe that leads to them, are a significant contribution to the open-source medical LLM field, offering top-of-the-line performance while maintaining high ethical requirements. This work sets a new standard for developing and reporting aligned LLMs in healthcare. </p>
<blockquote>
<p>ç›®çš„ï¼šéšç€åŒ»ç–—é¢†åŸŸçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥ï¼Œå¯¹ä¿æŠ¤å…¬ä¼—åˆ©ç›Šçš„å¼€æºæ¨¡å‹çš„éœ€æ±‚åº”è¿è€Œç”Ÿã€‚è¿™é¡¹å·¥ä½œé€šè¿‡å¯¹æ•°æ®é¢„å¤„ç†å’Œè®­ç»ƒçš„å…³é”®é˜¶æ®µè¿›è¡Œä¼˜åŒ–ï¼Œä¸ºå¼€æºåŒ»ç–—LLMé¢†åŸŸåšå‡ºè´¡çŒ®ï¼ŒåŒæ—¶å±•ç¤ºäº†å¦‚ä½•é€šè¿‡DPOæé«˜æ¨¡å‹çš„å®‰å…¨æ€§å’Œé€šè¿‡RAGæé«˜æ¨¡å‹çš„æ•ˆåŠ›ã€‚æ‰€ä½¿ç”¨çš„è¯„ä¼°æ–¹æ³•ï¼ŒåŒ…æ‹¬å››ç§ä¸åŒç±»å‹çš„æµ‹è¯•ï¼Œä¸ºè¯¥é¢†åŸŸå®šä¹‰äº†æ–°çš„æ ‡å‡†ã€‚æ‰€å¾—åˆ°çš„æ¨¡å‹è¢«è¯æ˜ä¸æœ€ä½³ç§æœ‰æ›¿ä»£æ¨¡å‹å…·æœ‰ç«äº‰åŠ›ï¼Œå¹¶ä»¥è®¸å¯è®¸å¯çš„å½¢å¼å‘å¸ƒã€‚</p>
</blockquote>
<p>æ–¹æ³•ï¼šAloe Betaå»ºç«‹åœ¨å¼ºå¤§çš„åŸºç¡€æ¨¡å‹ä¸Šï¼Œå¦‚Llama 3.1å’ŒQwen 2. 5ï¼Œå¹¶ä½¿ç”¨è‡ªå®šä¹‰æ•°æ®é›†å¢å¼ºå…¬å…±æ•°æ®åˆæˆæ€ç»´é“¾ç¤ºä¾‹ã€‚æ¨¡å‹é€šè¿‡ç›´æ¥åå¥½ä¼˜åŒ–è¿›è¡Œå¯¹é½ï¼Œå¼ºè°ƒåœ¨è¶Šç‹±æ”»å‡»å­˜åœ¨çš„æƒ…å†µä¸‹é“å¾·å’Œæ”¿ç­–å¯¹é½çš„æ€§èƒ½ã€‚è¯„ä¼°åŒ…æ‹¬å°é—­å¼ã€å¼€æ”¾å¼ã€å®‰å…¨æ€§å’Œäººç±»è¯„ä¼°ï¼Œä»¥æœ€å¤§åŒ–ç»“æœçš„å¯é æ€§ã€‚</p>
<p>ç»“æœï¼šåœ¨æ•´ä¸ªç®¡é“ä¸­æå‡ºäº†å»ºè®®ï¼Œè¿™äº›å»ºè®®å¾—åˆ°äº†Aloeå®¶æ—ç¨³å¥æ€§èƒ½çš„æ”¯æ’‘ã€‚è¿™äº›æ¨¡å‹åœ¨åŒ»ç–—åŸºå‡†æµ‹è¯•å’ŒåŒ»ç–—é¢†åŸŸè¡¨ç°å‡ºç«äº‰åŠ›ï¼Œå¹¶ç»å¸¸å—åˆ°åŒ»ç–—ä¿å¥ä¸“ä¸šäººå£«çš„é’çã€‚åœ¨åè§å’Œæ¯’æ€§æ–¹é¢ï¼ŒAloe Betaæ¨¡å‹åœ¨å®‰å…¨æ–¹é¢å¾—åˆ°æ˜¾ç€æ”¹å–„ï¼Œæ˜¾ç¤ºå‡ºå¯¹çœ‹ä¸è§çš„è¶Šç‹±æ”»å‡»çš„éŸ§æ€§ã€‚ä¸ºäº†è´Ÿè´£ä»»åœ°å‘å¸ƒï¼ŒAloeå®¶æ—æ¨¡å‹é™„å¸¦äº†é’ˆå¯¹åŒ»ç–—çš„è¯¦ç»†é£é™©è¯„ä¼°ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.04388v1">PDF</a> arXiv admin note: substantial text overlap with arXiv:2405.01886</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Aloe Betaæ¨¡å‹åœ¨åŒ»ç–—é¢†åŸŸå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ–¹é¢çš„è´¡çŒ®ã€‚é€šè¿‡ä¼˜åŒ–æ•°æ®é¢„å¤„ç†å’Œè®­ç»ƒçš„å…³é”®é˜¶æ®µï¼Œæé«˜æ¨¡å‹å®‰å…¨æ€§å’Œæœ‰æ•ˆæ€§ã€‚ä½¿ç”¨å››ç§ä¸åŒç±»å‹çš„æµ‹è¯•æ¥è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼Œå¹¶è¯æ˜å…¶ä¸æœ€ä½³ç§æœ‰æ›¿ä»£å“çš„ç«äº‰åŠ›ã€‚æœ€ç»ˆæ¨¡å‹ä»¥è®¸å¯å½¢å¼å‘å¸ƒï¼Œä¸ºå…¬ä¼—åˆ©ç›Šåšå‡ºè´¡çŒ®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Aloe Betaæ¨¡å‹ä¼˜åŒ–äº†æ•°æ®é¢„å¤„ç†å’Œè®­ç»ƒçš„å…³é”®é˜¶æ®µï¼Œæé«˜äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨åŒ»ç–—é¢†åŸŸçš„æ€§èƒ½ã€‚</li>
<li>é€šè¿‡ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰æé«˜æ¨¡å‹å®‰å…¨æ€§ï¼Œé€šè¿‡RAGæé«˜æ¨¡å‹æœ‰æ•ˆæ€§ã€‚</li>
<li>ä½¿ç”¨å››ç§ä¸åŒç±»å‹çš„æµ‹è¯•æ¥è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼Œä¸ºåŒ»ç–—é¢†åŸŸçš„LLMè®¾å®šäº†æ–°çš„è¯„ä¼°æ ‡å‡†ã€‚</li>
<li>Aloe Betaæ¨¡å‹è¡¨ç°å‡ºä¸æœ€ä½³ç§æœ‰æ¨¡å‹ç›¸å½“çš„ç«äº‰åŠ›ï¼Œè¢«åŒ»ç–—ä¿å¥ä¸“ä¸šäººå£«å¹¿æ³›æ¥å—ã€‚</li>
<li>åœ¨åè§å’Œæ¯’æ€§æ–¹é¢ï¼ŒAloe Betaæ¨¡å‹æ˜¾è‘—æé«˜äº†å®‰å…¨æ€§ï¼Œå¹¶å¯¹æœªæ›¾é‡åˆ°çš„æ”»å‡»è¡¨ç°å‡ºéŸ§æ€§ã€‚</li>
<li>å‘å¸ƒæ¨¡å‹æ—¶è¿›è¡Œäº†è¯¦ç»†çš„é£é™©è¯„ä¼°ï¼Œä»¥ç¡®ä¿å¯¹å…¬ä¼—è´Ÿè´£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.04388">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9cfbd035379c02858cc5c8b949c75dc8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9d1ab7ef7d382858c6a659a6c1c686c4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0d27627dcf550198c2d1b37814194621.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-12eb508a0e7d5a669b89371b21dfb91b.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="DOTA-Deformable-Optimized-Transformer-Architecture-for-End-to-End-Text-Recognition-with-Retrieval-Augmented-Generation"><a href="#DOTA-Deformable-Optimized-Transformer-Architecture-for-End-to-End-Text-Recognition-with-Retrieval-Augmented-Generation" class="headerlink" title="DOTA: Deformable Optimized Transformer Architecture for End-to-End Text   Recognition with Retrieval-Augmented Generation"></a>DOTA: Deformable Optimized Transformer Architecture for End-to-End Text   Recognition with Retrieval-Augmented Generation</h2><p><strong>Authors:Naphat Nithisopa, Teerapong Panboonyuen</strong></p>
<p>Text recognition in natural images remains a challenging yet essential task, with broad applications spanning computer vision and natural language processing. This paper introduces a novel end-to-end framework that combines ResNet and Vision Transformer backbones with advanced methodologies, including Deformable Convolutions, Retrieval-Augmented Generation, and Conditional Random Fields (CRF). These innovations collectively enhance feature representation and improve Optical Character Recognition (OCR) performance. Specifically, the framework substitutes standard convolution layers in the third and fourth blocks with Deformable Convolutions, leverages adaptive dropout for regularization, and incorporates CRF for more refined sequence modeling. Extensive experiments conducted on six benchmark datasets IC13, IC15, SVT, IIIT5K, SVTP, and CUTE80 validate the proposed methodâ€™s efficacy, achieving notable accuracies: 97.32% on IC13, 58.26% on IC15, 88.10% on SVT, 74.13% on IIIT5K, 82.17% on SVTP, and 66.67% on CUTE80, resulting in an average accuracy of 77.77%. These results establish a new state-of-the-art for text recognition, demonstrating the robustness of the approach across diverse and challenging datasets. </p>
<blockquote>
<p>æ–‡æœ¬è¯†åˆ«åœ¨è‡ªç„¶å›¾åƒä¸­ä»ç„¶æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜ä½†å¿…ä¸å¯å°‘çš„ä»»åŠ¡ï¼Œå¹¿æ³›åº”ç”¨äºè®¡ç®—æœºè§†è§‰å’Œè‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹ç«¯åˆ°ç«¯æ¡†æ¶ï¼Œå®ƒç»“åˆäº†ResNetå’ŒVision Transformerçš„ä¸»å¹²ç½‘ï¼Œå¹¶é‡‡ç”¨äº†å…ˆè¿›çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬å¯å˜å½¢å·ç§¯ã€æ£€ç´¢å¢å¼ºç”Ÿæˆå’Œæ¡ä»¶éšæœºåœºï¼ˆCRFï¼‰ã€‚è¿™äº›åˆ›æ–°å…±åŒæé«˜äº†ç‰¹å¾è¡¨ç¤ºèƒ½åŠ›ï¼Œå¹¶æé«˜äº†å…‰å­¦å­—ç¬¦è¯†åˆ«ï¼ˆOCRï¼‰çš„æ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼Œè¯¥æ¡†æ¶åœ¨ç¬¬ä¸‰å’Œç¬¬å››å—ä¸­ç”¨å¯å˜å½¢å·ç§¯æ›¿ä»£äº†æ ‡å‡†å·ç§¯å±‚ï¼Œåˆ©ç”¨è‡ªé€‚åº”ä¸¢å¼ƒè¿›è¡Œæ­£åˆ™åŒ–ï¼Œå¹¶å¼•å…¥äº†CRFè¿›è¡Œæ›´ç²¾ç»†çš„åºåˆ—å»ºæ¨¡ã€‚åœ¨IC13ã€IC15ã€SVTã€IIIT5Kã€SVTPå’ŒCUTE80å…­ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒéªŒè¯äº†æ‰€æå‡ºæ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå–å¾—äº†æ˜¾è‘—çš„å‡†ç¡®ç‡ï¼šIC13ä¸Š97.32%ï¼ŒIC15ä¸Š58.26%ï¼ŒSVTä¸Š88.10%ï¼ŒIIIT5Kä¸Š74.13%ï¼ŒSVTPä¸Š82.17%ï¼ŒCUTE80ä¸Š66.67%ï¼Œå¹³å‡å‡†ç¡®ç‡ä¸º77.77%ã€‚è¿™äº›ç»“æœå»ºç«‹äº†æ–‡æœ¬è¯†åˆ«çš„æ–°ä¸–ç•Œçºªå½•ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•åœ¨å¤šæ ·åŒ–å’Œå…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°æ®é›†ä¸Šçš„ç¨³å¥æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.04175v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹ç«¯åˆ°ç«¯æ¡†æ¶ï¼Œç»“åˆäº†ResNetå’ŒVision TransformeræŠ€æœ¯ï¼Œå¹¶é‡‡ç”¨äº†å¯å˜å½¢å·ç§¯ã€æ£€ç´¢å¢å¼ºç”Ÿæˆå’Œæ¡ä»¶éšæœºåœºç­‰å…ˆè¿›æ–¹æ³•ï¼Œæé«˜äº†è‡ªç„¶å›¾åƒä¸­çš„æ–‡æœ¬è¯†åˆ«æ€§èƒ½ã€‚è¯¥æ¡†æ¶åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›å®éªŒï¼Œå–å¾—äº†æ˜¾è‘—æˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°å‹ç«¯åˆ°ç«¯æ¡†æ¶ï¼Œç”¨äºè‡ªç„¶å›¾åƒä¸­çš„æ–‡æœ¬è¯†åˆ«ã€‚</li>
<li>æ¡†æ¶ç»“åˆäº†ResNetå’ŒVision TransformeræŠ€æœ¯ï¼Œå¢å¼ºäº†ç‰¹å¾è¡¨ç¤ºèƒ½åŠ›ã€‚</li>
<li>é‡‡ç”¨äº†å¯å˜å½¢å·ç§¯ã€æ£€ç´¢å¢å¼ºç”Ÿæˆå’Œæ¡ä»¶éšæœºåœºç­‰æ–¹æ³•ï¼Œæé«˜äº†å…‰å­¦å­—ç¬¦è¯†åˆ«ï¼ˆOCRï¼‰æ€§èƒ½ã€‚</li>
<li>æ¡†æ¶åœ¨å…­ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›å®éªŒï¼Œå–å¾—äº†å¹³å‡å‡†ç¡®ç‡77.77%çš„æ˜¾è‘—æˆæœã€‚</li>
<li>è¯¥æ¡†æ¶åœ¨IC13ã€IC15ã€SVTã€IIIT5Kã€SVTPå’ŒCUTE80ç­‰æ•°æ®é›†ä¸Šçš„å‡†ç¡®ç‡åˆ†åˆ«ä¸º97.32%ã€58.26%ã€88.10%ã€74.13%ã€82.17%å’Œ66.67%ã€‚</li>
<li>è®ºæ–‡å®ç°äº†åœ¨è‡ªç„¶å›¾åƒæ–‡æœ¬è¯†åˆ«é¢†åŸŸçš„æœ€æ–°æˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.04175">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-80d0b11f9af8ff69e8b0808845692ee7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-04d65bf05b6e8b864820c8c852fbcb41.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3ba34125ec64f354cf6582eed658d2b0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8468289f2238d85bd2b5c19b459d64bc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-efea82d85ee9b6e7499bd1a4a1fbb3dd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4f305356d119ba20f252a00169fa48ed.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Absolute-Zero-Reinforced-Self-play-Reasoning-with-Zero-Data"><a href="#Absolute-Zero-Reinforced-Self-play-Reasoning-with-Zero-Data" class="headerlink" title="Absolute Zero: Reinforced Self-play Reasoning with Zero Data"></a>Absolute Zero: Reinforced Self-play Reasoning with Zero Data</h2><p><strong>Authors:Andrew Zhao, Yiran Wu, Yang Yue, Tong Wu, Quentin Xu, Yang Yue, Matthieu Lin, Shenzhi Wang, Qingyun Wu, Zilong Zheng, Gao Huang</strong></p>
<p>Reinforcement learning with verifiable rewards (RLVR) has shown promise in enhancing the reasoning capabilities of large language models by learning directly from outcome-based rewards. Recent RLVR works that operate under the zero setting avoid supervision in labeling the reasoning process, but still depend on manually curated collections of questions and answers for training. The scarcity of high-quality, human-produced examples raises concerns about the long-term scalability of relying on human supervision, a challenge already evident in the domain of language model pretraining. Furthermore, in a hypothetical future where AI surpasses human intelligence, tasks provided by humans may offer limited learning potential for a superintelligent system. To address these concerns, we propose a new RLVR paradigm called Absolute Zero, in which a single model learns to propose tasks that maximize its own learning progress and improves reasoning by solving them, without relying on any external data. Under this paradigm, we introduce the Absolute Zero Reasoner (AZR), a system that self-evolves its training curriculum and reasoning ability by using a code executor to both validate proposed code reasoning tasks and verify answers, serving as an unified source of verifiable reward to guide open-ended yet grounded learning. Despite being trained entirely without external data, AZR achieves overall SOTA performance on coding and mathematical reasoning tasks, outperforming existing zero-setting models that rely on tens of thousands of in-domain human-curated examples. Furthermore, we demonstrate that AZR can be effectively applied across different model scales and is compatible with various model classes. </p>
<blockquote>
<p>é€šè¿‡ä»ç»“æœå¥–åŠ±ä¸­ç›´æ¥å­¦ä¹ ï¼Œå¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›æ–¹é¢å±•ç°å‡ºå·¨å¤§çš„æ½œåŠ›ã€‚æœ€è¿‘çš„ä¸€äº›RLVRå·¥ä½œåœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹è¿è¡Œï¼Œé¿å…äº†ç›‘ç£æ ‡æ³¨æ¨ç†è¿‡ç¨‹ï¼Œä½†ä»ç„¶ä¾èµ–äºæ‰‹åŠ¨æ•´ç†çš„é—®é¢˜å’Œç­”æ¡ˆé›†åˆè¿›è¡Œè®­ç»ƒã€‚é«˜è´¨é‡ã€äººç±»ç”Ÿäº§æ ·æœ¬çš„ç¨€ç¼ºæ€§å¼•å‘äº†å¯¹é•¿æœŸä¾èµ–äººå·¥ç›‘ç£çš„å¯è¡Œæ€§çš„æ‹…å¿§ï¼Œè¿™ä¸€æŒ‘æˆ˜åœ¨è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒé¢†åŸŸå·²ç»æ˜¾è€Œæ˜“è§ã€‚æ­¤å¤–ï¼Œåœ¨äººå·¥æ™ºèƒ½è¶…è¶Šäººç±»æ™ºèƒ½çš„å‡è®¾æœªæ¥ä¸­ï¼Œäººç±»æä¾›çš„ä»»åŠ¡å¯èƒ½ä¸ºè¶…çº§æ™ºèƒ½ç³»ç»Ÿæä¾›æœ‰é™çš„å­¦ä¹ æ½œåŠ›ã€‚ä¸ºäº†è§£å†³è¿™äº›æ‹…å¿§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„RLVRèŒƒå¼ï¼Œç§°ä¸ºâ€œç»å¯¹é›¶â€ï¼Œå…¶ä¸­å•ä¸ªæ¨¡å‹å­¦ä¼šæå‡ºèƒ½æœ€å¤§åŒ–å…¶è‡ªèº«å­¦ä¹ è¿›åº¦çš„ä»»åŠ¡ï¼Œå¹¶é€šè¿‡è§£å†³è¿™äº›ä»»åŠ¡æé«˜æ¨ç†èƒ½åŠ›ï¼Œæ— éœ€ä¾èµ–ä»»ä½•å¤–éƒ¨æ•°æ®ã€‚åœ¨è¿™ç§èŒƒå¼ä¸‹ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç»å¯¹é›¶æ¨ç†å™¨ï¼ˆAZRï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡ä»£ç æ‰§è¡Œå™¨éªŒè¯æå‡ºçš„ä»£ç æ¨ç†ä»»åŠ¡å’Œç­”æ¡ˆçš„ç³»ç»Ÿï¼Œè¯¥æ‰§è¡Œå™¨ä½œä¸ºå¯éªŒè¯å¥–åŠ±çš„ç»Ÿä¸€æ¥æºï¼ŒæŒ‡å¯¼å¼€æ”¾ä½†åŸºäºå®é™…çš„å­¦ä¹ ï¼Œè‡ªæˆ‘è¿›åŒ–å…¶è®­ç»ƒè¯¾ç¨‹å’Œæ¨ç†èƒ½åŠ›ã€‚å°½ç®¡å®Œå…¨æœªç»å¤–éƒ¨æ•°æ®è®­ç»ƒï¼ŒAZRåœ¨ç¼–ç å’Œæ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šå®ç°äº†æ€»ä½“æœ€ä½³æ€§èƒ½ï¼Œä¼˜äºä¾èµ–æ•°ä¸‡é¢†åŸŸå†…äººå·¥æ•´ç†æ ·æœ¬çš„ç°æœ‰é›¶æ ·æœ¬æ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¯æ˜äº†AZRå¯ä»¥æœ‰æ•ˆåº”ç”¨äºä¸åŒçš„æ¨¡å‹è§„æ¨¡ï¼Œå¹¶ä¸å„ç§æ¨¡å‹ç±»åˆ«å…¼å®¹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.03335v2">PDF</a> </p>
<p><strong>Summary</strong><br>å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›æ–¹é¢å±•ç°å‡ºæ½œåŠ›ï¼Œé€šè¿‡ç›´æ¥ä»ç»“æœå¯¼å‘çš„å¥–åŠ±ä¸­å­¦ä¹ ã€‚æœ€æ–°çš„RLVRå·¥ä½œåœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹é¿å…äº†ç›‘ç£æ ‡æ³¨æ¨ç†è¿‡ç¨‹ï¼Œä½†ä»ä¾èµ–äºæ‰‹åŠ¨æ•´ç†çš„é—®é¢˜å’Œç­”æ¡ˆé›†åˆè¿›è¡Œè®­ç»ƒã€‚å¯¹é«˜è´¨é‡äººç±»ç”Ÿæˆä¾‹å­çš„ç¨€ç¼ºæ€§ï¼Œä»¥åŠå¯¹é•¿æœŸä¾èµ–äººç±»ç›‘ç£çš„å¯æŒç»­æ€§æ‹…å¿§ï¼Œä¿ƒä½¿æˆ‘ä»¬æå‡ºä¸€ç§æ–°çš„RLVRèŒƒå¼â€”â€”ç»å¯¹é›¶ï¼ˆAbsolute Zeroï¼‰ã€‚åœ¨æ­¤èŒƒå¼ä¸‹ï¼Œå•ä¸€æ¨¡å‹å­¦ä¹ æå‡ºä»»åŠ¡ä»¥æœ€å¤§åŒ–è‡ªèº«å­¦ä¹ è¿›åº¦å¹¶é€šè¿‡è§£å†³è¿™äº›ä»»åŠ¡æé«˜æ¨ç†èƒ½åŠ›ï¼Œæ— éœ€ä¾èµ–ä»»ä½•å¤–éƒ¨æ•°æ®ã€‚æˆ‘ä»¬ä»‹ç»äº†ç»å¯¹é›¶æ¨ç†å™¨ï¼ˆAZRï¼‰ï¼Œä¸€ä¸ªé€šè¿‡ä»£ç æ‰§è¡Œå™¨éªŒè¯æå‡ºçš„ä»£ç æ¨ç†ä»»åŠ¡å’Œç­”æ¡ˆçš„ç³»ç»Ÿï¼Œä½œä¸ºéªŒè¯å¥–åŠ±çš„ç»Ÿä¸€æ¥æºï¼ŒæŒ‡å¯¼å¼€æ”¾å¼ä½†åŸºäºç°å®çš„å­¦ä¹ ã€‚å°½ç®¡å®Œå…¨ä¸ä¾èµ–å¤–éƒ¨æ•°æ®è¿›è¡Œè®­ç»ƒï¼ŒAZRåœ¨ç¼–ç¨‹å’Œæ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šå®ç°äº†é¢†å…ˆåŒç±»ç ”ç©¶çš„æ€§èƒ½è¡¨ç°ï¼Œå¹¶èƒ½åœ¨ä¸åŒæ¨¡å‹è§„æ¨¡å’Œç±»åˆ«ä¸­æœ‰æ•ˆåº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰èƒ½å¤Ÿæé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>åœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹è®­ç»ƒçš„æ¨¡å‹å¯ä»¥é¿å…å¯¹ç›‘ç£æ ‡æ³¨çš„ä¾èµ–ï¼Œä½†ä»éœ€è¦æ‰‹åŠ¨æ•´ç†çš„ä»»åŠ¡æ•°æ®ã€‚</li>
<li>å¯¹é«˜è´¨é‡äººç±»ç”Ÿæˆä¾‹å­çš„ç¨€ç¼ºæ€§å’Œé•¿æœŸä¾èµ–äººç±»ç›‘ç£çš„å¯æŒç»­æ€§å­˜åœ¨æ‹…å¿§ã€‚</li>
<li>æå‡ºæ–°çš„RLVRèŒƒå¼â€”â€”ç»å¯¹é›¶ï¼ˆAbsolute Zeroï¼‰ï¼Œå…è®¸æ¨¡å‹è‡ªæˆ‘è¿›åŒ–å…¶è®­ç»ƒè¯¾ç¨‹å’Œæ¨ç†èƒ½åŠ›ï¼Œæ— éœ€ä¾èµ–å¤–éƒ¨æ•°æ®ã€‚</li>
<li>å¼•å…¥ç»å¯¹é›¶æ¨ç†å™¨ï¼ˆAZRï¼‰ï¼Œé€šè¿‡ä»£ç æ‰§è¡Œå™¨éªŒè¯ä»»åŠ¡å’Œç­”æ¡ˆï¼Œä½œä¸ºéªŒè¯å¥–åŠ±çš„ç»Ÿä¸€æ¥æºã€‚</li>
<li>AZRåœ¨ç¼–ç¨‹å’Œæ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šå®ç°äº†å“è¶Šæ€§èƒ½ï¼Œä¼˜äºä¾èµ–å¤§é‡äººç±»æ•´ç†æ•°æ®çš„æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.03335">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-36fec0814348995c0d811253c19339a9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-100471ed028d8370f5d09c8ba954c734.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-53990f3c8272ae038fb94dbd3dafdfd8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ea3ba310819fd4aff8e3d53a3e56fb34.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-746ff970b2e408d3c9774f657c02f292.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Grokking-in-the-Wild-Data-Augmentation-for-Real-World-Multi-Hop-Reasoning-with-Transformers"><a href="#Grokking-in-the-Wild-Data-Augmentation-for-Real-World-Multi-Hop-Reasoning-with-Transformers" class="headerlink" title="Grokking in the Wild: Data Augmentation for Real-World Multi-Hop   Reasoning with Transformers"></a>Grokking in the Wild: Data Augmentation for Real-World Multi-Hop   Reasoning with Transformers</h2><p><strong>Authors:Roman Abramov, Felix Steinbauer, Gjergji Kasneci</strong></p>
<p>Transformers have achieved great success in numerous NLP tasks but continue to exhibit notable gaps in multi-step factual reasoning, especially when real-world knowledge is sparse. Recent advances in grokking have demonstrated that neural networks can transition from memorizing to perfectly generalizing once they detect underlying logical patterns - yet these studies have primarily used small, synthetic tasks. In this paper, for the first time, we extend grokking to real-world factual data and address the challenge of dataset sparsity by augmenting existing knowledge graphs with carefully designed synthetic data to raise the ratio $\phi_r$ of inferred facts to atomic facts above the threshold required for grokking. Surprisingly, we find that even factually incorrect synthetic data can strengthen emergent reasoning circuits rather than degrade accuracy, as it forces the model to rely on relational structure rather than memorization. When evaluated on multi-hop reasoning benchmarks, our approach achieves up to 95-100% accuracy on 2WikiMultiHopQA - substantially improving over strong baselines and matching or exceeding current state-of-the-art results. We further provide an in-depth analysis of how increasing $\phi_r$ drives the formation of generalizing circuits inside Transformers. Our findings suggest that grokking-based data augmentation can unlock implicit multi-hop reasoning capabilities, opening the door to more robust and interpretable factual reasoning in large-scale language models. </p>
<blockquote>
<p>Transformeråœ¨è®¸å¤šNLPä»»åŠ¡ä¸­å–å¾—äº†å·¨å¤§æˆåŠŸï¼Œä½†åœ¨å¤šæ­¥éª¤äº‹å®æ¨ç†æ–¹é¢ä»å­˜åœ¨æ˜æ˜¾å·®è·ï¼Œç‰¹åˆ«æ˜¯åœ¨ç°å®ä¸–ç•ŒçŸ¥è¯†ç¨€ç¼ºçš„æƒ…å†µä¸‹ã€‚æœ€è¿‘çš„grokkingç ”ç©¶è¡¨æ˜ï¼Œä¸€æ—¦ç¥ç»ç½‘ç»œå‘ç°æ½œåœ¨çš„é€»è¾‘æ¨¡å¼ï¼Œå®ƒä»¬å°±å¯ä»¥ä»è®°å¿†è½¬å‘å®Œå…¨æ³›åŒ–ã€‚ç„¶è€Œï¼Œè¿™äº›ç ”ç©¶ä¸»è¦ä½¿ç”¨å°å‹åˆæˆä»»åŠ¡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é¦–æ¬¡å°†grokkingæ‰©å±•åˆ°ç°å®ä¸–ç•Œçš„äº‹å®æ•°æ®ï¼Œå¹¶é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„åˆæˆæ•°æ®å¢å¼ºç°æœ‰çŸ¥è¯†å›¾è°±æ¥è§£å†³æ•°æ®é›†ç¨€ç–æ€§çš„æŒ‘æˆ˜ï¼Œä»è€Œæé«˜æ¨æ–­äº‹å®ä¸åŸå­äº‹å®çš„æ¯”ç‡$\phi_r$ï¼Œä½¿å…¶è¶…è¿‡å®ç°grokkingæ‰€éœ€çš„é˜ˆå€¼ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç°å³ä½¿æ˜¯äº‹å®é”™è¯¯çš„åˆæˆæ•°æ®ä¹Ÿå¯ä»¥åŠ å¼ºæ–°å…´æ¨ç†ç”µè·¯ï¼Œè€Œä¸æ˜¯é™ä½å‡†ç¡®æ€§ï¼Œå› ä¸ºå®ƒè¿«ä½¿æ¨¡å‹ä¾èµ–å…³ç³»ç»“æ„è€Œä¸æ˜¯è®°å¿†ã€‚åœ¨è¯„ä¼°å¤šè·³æ¨ç†åŸºå‡†æµ‹è¯•æ—¶ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨2WikiMultiHopQAä¸Šè¾¾åˆ°äº†é«˜è¾¾95-100%çš„å‡†ç¡®ç‡ï¼Œå¤§å¹…è¶…è¶Šäº†å¼ºå¤§çš„åŸºçº¿å¹¶åŒ¹é…æˆ–è¶…è¿‡äº†å½“å‰æœ€æ–°çš„ç»“æœã€‚æˆ‘ä»¬è¿˜æ·±å…¥åˆ†æäº†å¦‚ä½•æé«˜$\phi_r$æ¥é©±åŠ¨Transformerå†…éƒ¨æ³›åŒ–ç”µè·¯çš„å½¢æˆã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒåŸºäºgrokkingçš„æ•°æ®å¢å¼ºå¯ä»¥è§£é”éšå¼çš„å¤šè·³æ¨ç†èƒ½åŠ›ï¼Œä¸ºå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ä¸­çš„æ›´ç¨³å¥å’Œå¯è§£é‡Šçš„äº‹å®æ¨ç†æ‰“å¼€äº†å¤§é—¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.20752v2">PDF</a> Accepted to the International Conference on Machine Learning (ICML)   2025</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æ‰©å±•äº†grokkingè‡³çœŸå®ä¸–ç•Œçš„äº‹å®æ•°æ®ï¼Œé€šè¿‡å¢å¼ºç°æœ‰çŸ¥è¯†å›¾è°±ä¸ç²¾å¿ƒè®¾è®¡åˆæˆæ•°æ®æ¥è§£å†³æ•°æ®é›†ç¨€ç–æ€§çš„æŒ‘æˆ˜ã€‚ç ”ç©¶å‘ç°ï¼Œå³ä½¿äº‹å®ä¸Šä¸æ­£ç¡®åˆæˆæ•°æ®ä¹Ÿèƒ½åŠ å¼ºæ–°å…´æ¨ç†ç”µè·¯ï¼Œè€Œéé™ä½å‡†ç¡®åº¦ï¼Œè¿™ä¿ƒä½¿æ¨¡å‹ä¾èµ–å…³ç³»ç»“æ„è€Œéè®°å¿†ã€‚åœ¨è·¨æ­¥æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šï¼Œè¯¥æ–¹æ³•åœ¨2WikiMultiHopQAä¸Šè¾¾åˆ°95-100%çš„å‡†ç¡®ç‡ï¼Œå¤§å¹…è¶…è¶Šå¼ºåŸºçº¿å¹¶åŒ¹é…æˆ–è¶…è¶Šå½“å‰æœ€æ–°ç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®ºæ–‡å°†grokkingæ‰©å±•åˆ°çœŸå®ä¸–ç•Œçš„äº‹å®æ•°æ®ï¼Œè§£å†³æ•°æ®é›†ç¨€ç–æ€§é—®é¢˜ã€‚</li>
<li>é€šè¿‡å¢å¼ºçŸ¥è¯†å›¾è°±ä¸åˆæˆæ•°æ®ï¼Œæé«˜æ¨æ–­äº‹å®ä¸åŸå­äº‹å®æ¯”ç‡$\phi_r$ï¼Œè¾¾åˆ°grokkingæ‰€éœ€çš„é˜ˆå€¼ã€‚</li>
<li>äº‹å®ä¸Šä¸æ­£ç¡®åˆæˆæ•°æ®èƒ½åŠ å¼ºæ¨¡å‹çš„æ–°å…´æ¨ç†ç”µè·¯ï¼Œä½¿å…¶æ›´ä¾èµ–å…³ç³»ç»“æ„è€Œéè®°å¿†ã€‚</li>
<li>åœ¨å¤šæ­¥æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šå–å¾—é«˜è¾¾95-100%çš„å‡†ç¡®ç‡ï¼Œè¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>è¯¥æ–¹æ³•åŒ¹é…æˆ–è¶…è¶Šå½“å‰æœ€æ–°æŠ€æœ¯ç»“æœã€‚</li>
<li>å¢åŠ $\phi_r$æœ‰åŠ©äºåœ¨Transformerä¸­å½¢æˆé€šç”¨ç”µè·¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.20752">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-406b431f663203c8959efb51332e98bd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-94a55d5cbe8ee683dc6e752d80e2a233.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0b9b7b6e7023119aa898965d466f8aab.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6e554fd2d70706b64cfe4ff222dedc21.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-76abdaf2dcffe85e82b856b4bbf98e46.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6591030219d9ec9b02801b6e8e3fc520.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Adaptive-Rank-Allocation-Speeding-Up-Modern-Transformers-with-RaNA-Adapters"><a href="#Adaptive-Rank-Allocation-Speeding-Up-Modern-Transformers-with-RaNA-Adapters" class="headerlink" title="Adaptive Rank Allocation: Speeding Up Modern Transformers with RaNA   Adapters"></a>Adaptive Rank Allocation: Speeding Up Modern Transformers with RaNA   Adapters</h2><p><strong>Authors:Roberto Garcia, Jerry Liu, Daniel Sorvisto, Sabri Eyuboglu</strong></p>
<p>Large Language Models (LLMs) are computationally intensive, particularly during inference. Neuron-adaptive techniques, which selectively activate neurons in Multi-Layer Perceptron (MLP) layers, offer some speedups but suffer from limitations in modern Transformers. These include reliance on sparse activations, incompatibility with attention layers, and the use of costly neuron masking techniques. To address these issues, we propose the Adaptive Rank Allocation framework and introduce the Rank and Neuron Allocator (RaNA) adapter. RaNA adapters leverage rank adapters, which operate on linear layers by applying both low-rank matrix decompositions and adaptive masking to efficiently allocate compute without depending on activation sparsity. This enables RaNA to be generally applied to MLPs and linear components of attention modules, while eliminating the need for expensive maskers found in neuron-adaptive methods. Notably, when compared to neuron adapters, RaNA improves perplexity by up to 7 points and increases accuracy by up to 8 percentage-points when reducing FLOPs by $\sim$44% in state-of-the-art Transformer architectures. These results position RaNA as a robust solution for improving inference efficiency in modern Transformer architectures. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è®¡ç®—å¯†é›†ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¨ç†é˜¶æ®µã€‚ç¥ç»å…ƒè‡ªé€‚åº”æŠ€æœ¯é€šè¿‡é€‰æ‹©æ€§æ¿€æ´»å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰å±‚ä¸­çš„ç¥ç»å…ƒæä¾›äº†ä¸€äº›åŠ é€Ÿï¼Œä½†åœ¨ç°ä»£Transformerä¸­å—åˆ°ä¸€äº›é™åˆ¶ã€‚è¿™äº›é™åˆ¶åŒ…æ‹¬ä¾èµ–äºç¨€ç–æ¿€æ´»ã€ä¸æ³¨æ„åŠ›å±‚ä¸å…¼å®¹ä»¥åŠä½¿ç”¨æˆæœ¬é«˜æ˜‚çš„ç¥ç»å…ƒæ©è”½æŠ€æœ¯ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†è‡ªé€‚åº”æ’ååˆ†é…æ¡†æ¶ï¼Œå¹¶å¼•å…¥äº†Rankå’ŒNeuron Allocatorï¼ˆRaNAï¼‰é€‚é…å™¨ã€‚RaNAé€‚é…å™¨åˆ©ç”¨æ’åé€‚é…å™¨ï¼Œé€šè¿‡ä½é˜¶çŸ©é˜µåˆ†è§£å’Œè‡ªé€‚åº”æ©è”½åœ¨çº¿æ€§å±‚ä¸Šè¿è¡Œï¼Œå¯ä»¥æœ‰æ•ˆåœ°åˆ†é…è®¡ç®—ï¼Œè€Œä¸ä¾èµ–äºæ¿€æ´»ç¨€ç–æ€§ã€‚è¿™ä½¿å¾—RaNAå¯ä»¥å¹¿æ³›åº”ç”¨äºMLPå’Œæ³¨æ„åŠ›æ¨¡å—çš„çº¿æ€§ç»„ä»¶ï¼ŒåŒæ—¶æ¶ˆé™¤äº†ç¥ç»å…ƒè‡ªé€‚åº”æ–¹æ³•ä¸­å‘ç°çš„æ˜‚è´µæ©è”½å™¨çš„éœ€æ±‚ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä¸ç¥ç»å…ƒé€‚é…å™¨ç›¸æ¯”ï¼ŒRaNAåœ¨å‡å°‘44%æµ®ç‚¹è¿ç®—ï¼ˆFLOPsï¼‰çš„æƒ…å†µä¸‹ï¼Œå›°æƒ‘åº¦é™ä½äº†é«˜è¾¾7ç‚¹ï¼Œå‡†ç¡®ç‡æé«˜äº†é«˜è¾¾8ä¸ªç™¾åˆ†ç‚¹ï¼Œè¿™åœ¨æœ€å…ˆè¿›çš„Transformeræ¶æ„ä¸­è¡¨ç°ä¼˜å¼‚ã€‚è¿™äº›ç»“æœå°†RaNAå®šä½ä¸ºæé«˜ç°ä»£Transformeræ¶æ„æ¨ç†æ•ˆç‡çš„ä¸€ç§ç¨³å¥è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18216v2">PDF</a> 16 pages, 5 figures. ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ¨ç†è¿‡ç¨‹ä¸­è®¡ç®—å¯†é›†ï¼Œç¥ç»å…ƒè‡ªé€‚åº”æŠ€æœ¯è™½èƒ½æä¾›ä¸€å®šçš„åŠ é€Ÿï¼Œä½†åœ¨ç°ä»£Transformeræ¶æ„ä¸­å­˜åœ¨å±€é™æ€§ã€‚é’ˆå¯¹è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†è‡ªé€‚åº”æ’ååˆ†é…æ¡†æ¶å’Œå¼•å…¥RaNAé€‚é…å™¨ã€‚RaNAé€‚é…å™¨åˆ©ç”¨æ’åé€‚é…å™¨ï¼Œé€šè¿‡ä½é˜¶çŸ©é˜µåˆ†è§£å’Œè‡ªé€‚åº”æ©ç åœ¨çº¿æ€§å±‚ä¸Šè¿è¡Œï¼Œä»¥æœ‰æ•ˆåœ°åˆ†é…è®¡ç®—èµ„æºï¼Œè€Œæ— éœ€ä¾èµ–æ¿€æ´»ç¨€ç–æ€§ã€‚è¿™ä½¿å¾—RaNAå¯å¹¿æ³›åº”ç”¨äºMLPå’Œæ³¨æ„åŠ›æ¨¡å—çš„çº¿æ€§ç»„ä»¶ï¼Œæ¶ˆé™¤äº†ç¥ç»å…ƒè‡ªé€‚åº”æ–¹æ³•ä¸­æ˜‚è´µçš„æ©ç å™¨çš„éœ€æ±‚ã€‚åœ¨å‡å°‘çº¦44%çš„æµ®ç‚¹è¿ç®—ï¼ˆFLOPsï¼‰çš„åŒæ—¶ï¼ŒRaNAåœ¨å…ˆè¿›çš„Transformeræ¶æ„ä¸­æ”¹å–„äº†å›°æƒ‘åº¦è¾¾7ç‚¹ï¼Œå‡†ç¡®ç‡æé«˜é«˜è¾¾8ä¸ªç™¾åˆ†ç‚¹ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨æé«˜ç°ä»£Transformeræ¶æ„æ¨ç†æ•ˆç‡æ–¹é¢çš„ç¨³å¥è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨æ¨ç†è¿‡ç¨‹ä¸­è®¡ç®—å¯†é›†ï¼Œéœ€è¦é«˜æ•ˆè®¡ç®—æ–¹æ³•ä¼˜åŒ–ã€‚</li>
<li>ç¥ç»å…ƒè‡ªé€‚åº”æŠ€æœ¯è™½ç„¶å¯ä»¥æä¾›åŠ é€Ÿï¼Œä½†åœ¨ç°ä»£Transformeræ¶æ„ä¸­å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>æå‡ºè‡ªé€‚åº”æ’ååˆ†é…æ¡†æ¶å’ŒRaNAé€‚é…å™¨ä»¥æ”¹è¿›æ­¤é—®é¢˜ã€‚</li>
<li>RaNAåˆ©ç”¨æ’åé€‚é…å™¨ï¼Œç»“åˆä½é˜¶çŸ©é˜µåˆ†è§£å’Œè‡ªé€‚åº”æ©ç ï¼Œæœ‰æ•ˆåˆ†é…è®¡ç®—èµ„æºã€‚</li>
<li>RaNAå¯å¹¿æ³›åº”ç”¨äºMLPå’Œæ³¨æ„åŠ›æ¨¡å—çš„çº¿æ€§ç»„ä»¶ã€‚</li>
<li>RaNAä¸éœ€è¦æ˜‚è´µçš„æ©ç å™¨ï¼Œä¸ç¥ç»å…ƒè‡ªé€‚åº”æ–¹æ³•ç›¸æ¯”å…·æœ‰ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18216">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-35c9c089c637c0b75d1d22e8d5a2b2a3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3b1a974f521b0b5bcaacd7f6a07f4a34.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="AffectGPT-A-New-Dataset-Model-and-Benchmark-for-Emotion-Understanding-with-Multimodal-Large-Language-Models"><a href="#AffectGPT-A-New-Dataset-Model-and-Benchmark-for-Emotion-Understanding-with-Multimodal-Large-Language-Models" class="headerlink" title="AffectGPT: A New Dataset, Model, and Benchmark for Emotion Understanding   with Multimodal Large Language Models"></a>AffectGPT: A New Dataset, Model, and Benchmark for Emotion Understanding   with Multimodal Large Language Models</h2><p><strong>Authors:Zheng Lian, Haoyu Chen, Lan Chen, Haiyang Sun, Licai Sun, Yong Ren, Zebang Cheng, Bin Liu, Rui Liu, Xiaojiang Peng, Jiangyan Yi, Jianhua Tao</strong></p>
<p>The emergence of multimodal large language models (MLLMs) advances multimodal emotion recognition (MER) to the next level, from naive discriminative tasks to complex emotion understanding with advanced video understanding abilities and natural language description. However, the current community suffers from a lack of large-scale datasets with intensive, descriptive emotion annotations, as well as a multimodal-centric framework to maximize the potential of MLLMs for emotion understanding. To address this, we establish a new benchmark for MLLM-based emotion understanding with a novel dataset (MER-Caption) and a new model (AffectGPT). Utilizing our model-based crowd-sourcing data collection strategy, we construct the largest descriptive emotion dataset to date (by far), featuring over 2K fine-grained emotion categories across 115K samples. We also introduce the AffectGPT model, designed with pre-fusion operations to enhance multimodal integration. Finally, we present MER-UniBench, a unified benchmark with evaluation metrics tailored for typical MER tasks and the free-form, natural language output style of MLLMs. Extensive experimental results show AffectGPTâ€™s robust performance across various MER tasks. We have released both the code and the dataset to advance research and development in emotion understanding: <a target="_blank" rel="noopener" href="https://github.com/zeroQiaoba/AffectGPT">https://github.com/zeroQiaoba/AffectGPT</a>. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„å‡ºç°å°†å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«ï¼ˆMERï¼‰æå‡åˆ°äº†ä¸€ä¸ªæ–°çš„æ°´å¹³ï¼Œä»ç®€å•çš„è¾¨åˆ«ä»»åŠ¡å‘å±•åˆ°äº†å…·æœ‰å…ˆè¿›è§†é¢‘ç†è§£èƒ½åŠ›å’Œè‡ªç„¶è¯­è¨€æè¿°åŠŸèƒ½çš„å¤æ‚æƒ…æ„Ÿç†è§£ã€‚ç„¶è€Œï¼Œå½“å‰ç¤¾åŒºç¼ºä¹å¤§è§„æ¨¡ã€æƒ…æ„Ÿæ ‡æ³¨å¯†é›†ä¸”æè¿°æ€§çš„æ•°æ®é›†ï¼Œä»¥åŠä»¥å¤šæ¨¡æ€ä¸ºä¸­å¿ƒã€èƒ½æœ€å¤§åŒ–MLLMåœ¨æƒ…æ„Ÿç†è§£æ½œåŠ›çš„æ¡†æ¶ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å»ºç«‹äº†åŸºäºMLLMçš„æƒ…æ„Ÿç†è§£æ–°åŸºå‡†ï¼ŒåŒ…æ‹¬ä¸€ä¸ªæ–°å‹æ•°æ®é›†ï¼ˆMER-Captionï¼‰å’Œä¸€ä¸ªæ–°æ¨¡å‹ï¼ˆAffectGPTï¼‰ã€‚é€šè¿‡åŸºäºæ¨¡å‹çš„ä¼—åŒ…æ•°æ®æ”¶é›†ç­–ç•¥ï¼Œæˆ‘ä»¬æ„å»ºäº†è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„æè¿°æ€§æƒ…æ„Ÿæ•°æ®é›†ï¼Œæ¶µç›–è¶…è¿‡2Kä¸ªç²¾ç»†æƒ…æ„Ÿç±»åˆ«ï¼Œè·¨è¶Š11.5ä¸‡æ ·æœ¬ã€‚æˆ‘ä»¬è¿˜ä»‹ç»äº†AffectGPTæ¨¡å‹çš„è®¾è®¡ï¼Œè¯¥æ¨¡å‹é€šè¿‡é¢„èåˆæ“ä½œå¢å¼ºå¤šæ¨¡æ€é›†æˆã€‚æœ€åï¼Œæˆ‘ä»¬æ¨å‡ºäº†MER-UniBenchç»Ÿä¸€åŸºå‡†æµ‹è¯•ï¼Œå®ƒåŒ…å«é’ˆå¯¹å…¸å‹MERä»»åŠ¡çš„è¯„ä¼°æŒ‡æ ‡å’ŒMLLMçš„è‡ªç”±å½¢å¼è‡ªç„¶è¯­è¨€è¾“å‡ºé£æ ¼ã€‚å¹¿æ³›çš„å®éªŒç»“æœè¡¨æ˜AffectGPTåœ¨å„ç§MERä»»åŠ¡ä¸­çš„ç¨³å¥æ€§èƒ½ã€‚æˆ‘ä»¬å·²ç»å‘å¸ƒäº†ä»£ç å’Œæ•°æ®é›†ï¼Œä»¥ä¿ƒè¿›æƒ…æ„Ÿç†è§£çš„ç ”ç©¶å’Œå‘å±•ï¼š<a target="_blank" rel="noopener" href="https://github.com/zeroQiaoba/AffectGPT%E3%80%82">https://github.com/zeroQiaoba/AffectGPTã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.16566v2">PDF</a> </p>
<p><strong>Summary</strong>ï¼šå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„å‡ºç°æ¨åŠ¨äº†å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«ï¼ˆMERï¼‰çš„å‘å±•ï¼Œä»ç®€å•çš„åˆ¤åˆ«ä»»åŠ¡åˆ°å¤æ‚çš„æƒ…æ„Ÿç†è§£ï¼Œå…·å¤‡äº†é«˜çº§è§†é¢‘ç†è§£èƒ½åŠ›å’Œè‡ªç„¶è¯­è¨€æè¿°èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹å…·æœ‰æè¿°æ€§æƒ…æ„Ÿæ³¨é‡Šçš„å¤§è§„æ¨¡æ•°æ®é›†ä»¥åŠå¤šæ¨¡æ€ä¸­å¿ƒæ¡†æ¶æ¥æœ€å¤§é™åº¦åœ°å‘æŒ¥MLLMsåœ¨æƒ…æ„Ÿç†è§£æ–¹é¢çš„æ½œåŠ›ï¼Œå½“å‰çš„ç ”ç©¶é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å»ºç«‹äº†åŸºäºMLLMçš„æƒ…æ„Ÿç†è§£æ–°åŸºå‡†ï¼Œå¹¶æå‡ºäº†æ–°çš„æ•°æ®é›†ï¼ˆMER-Captionï¼‰å’Œæ–°æ¨¡å‹ï¼ˆAffectGPTï¼‰ã€‚æˆ‘ä»¬åˆ©ç”¨åŸºäºæ¨¡å‹çš„ä¼—åŒ…æ•°æ®æ”¶é›†ç­–ç•¥ï¼Œæ„å»ºäº†è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„æè¿°æ€§æƒ…æ„Ÿæ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡2Kä¸ªç²¾ç»†ç²’åº¦çš„æƒ…æ„Ÿç±»åˆ«å’Œè¶…è¿‡11ä¸‡æ ·æœ¬ã€‚æˆ‘ä»¬è¿˜ä»‹ç»äº†ä¸“ä¸ºå¢å¼ºå¤šæ¨¡æ€èåˆè€Œè®¾è®¡çš„AffectGPTæ¨¡å‹ã€‚æœ€åï¼Œæˆ‘ä»¬æ¨å‡ºäº†MER-UniBenchç»Ÿä¸€åŸºå‡†ï¼Œå…¶è¯„ä¼°æŒ‡æ ‡é€‚ç”¨äºå…¸å‹çš„MERä»»åŠ¡ä»¥åŠMLLMsçš„è‡ªç„¶è¯­è¨€è¾“å‡ºé£æ ¼ã€‚å®éªŒç»“æœå¹¿æ³›è¯æ˜äº†AffectGPTåœ¨å„ç§MERä»»åŠ¡ä¸­çš„ç¨³å¥æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«ï¼ˆMERï¼‰é¢†åŸŸå±•ç°å‡ºä¼˜å¼‚æ½œåŠ›ï¼Œæ”¯æŒä»å¤æ‚è§†é¢‘ç†è§£åˆ°è‡ªç„¶è¯­è¨€æè¿°çš„å…ˆè¿›æƒ…æ„Ÿç†è§£ã€‚</li>
<li>å½“å‰ç¼ºä¹å…·å¤‡æè¿°æ€§æƒ…æ„Ÿæ³¨é‡Šçš„å¤§è§„æ¨¡æ•°æ®é›†å’Œå¤šæ¨¡æ€ä¸­å¿ƒæ¡†æ¶ï¼Œé™åˆ¶äº†MLLMsåœ¨æƒ…æ„Ÿç†è§£æ–¹é¢çš„åº”ç”¨ã€‚</li>
<li>å»ºç«‹äº†æ–°çš„åŸºäºMLLMçš„æƒ…æ„Ÿç†è§£åŸºå‡†ï¼ŒåŒ…æ‹¬æ–°çš„æ•°æ®é›†MER-Captionå’Œæ¨¡å‹AffectGPTã€‚</li>
<li>MER-Captionæ•°æ®é›†æ˜¯è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„æè¿°æ€§æƒ…æ„Ÿæ•°æ®é›†ï¼ŒåŒ…å«å¤§é‡ç²¾ç»†ç²’åº¦çš„æƒ…æ„Ÿç±»åˆ«å’Œæ ·æœ¬ã€‚</li>
<li>AffectGPTæ¨¡å‹è®¾è®¡æ—¨åœ¨å¢å¼ºå¤šæ¨¡æ€èåˆï¼Œæé«˜æƒ…æ„Ÿç†è§£çš„æ€§èƒ½ã€‚</li>
<li>æ¨å‡ºäº†MER-UniBenchç»Ÿä¸€åŸºå‡†ï¼Œé€‚åº”å…¸å‹çš„MERä»»åŠ¡ä»¥åŠMLLMsçš„è‡ªç„¶è¯­è¨€è¾“å‡ºé£æ ¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.16566">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-16726be445d37a249738f729c1d0e46b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3809c5d8973e06247bdafadab2f226ea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0f565352de7b39671297117a7f677d87.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1f98b1037557407aaaf7280eb7213ee0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-57a3e7f214a5a493da71a28677dd59e4.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="LLM2CLIP-Powerful-Language-Model-Unlocks-Richer-Visual-Representation"><a href="#LLM2CLIP-Powerful-Language-Model-Unlocks-Richer-Visual-Representation" class="headerlink" title="LLM2CLIP: Powerful Language Model Unlocks Richer Visual Representation"></a>LLM2CLIP: Powerful Language Model Unlocks Richer Visual Representation</h2><p><strong>Authors:Weiquan Huang, Aoqi Wu, Yifan Yang, Xufang Luo, Yuqing Yang, Liang Hu, Qi Dai, Chunyu Wang, Xiyang Dai, Dongdong Chen, Chong Luo, Lili Qiu</strong></p>
<p>CLIP is a foundational multimodal model that aligns image and text features into a shared representation space via contrastive learning on large-scale image-text pairs. Its effectiveness primarily stems from the use of natural language as rich supervision. Motivated by the remarkable advancements in large language models (LLMs), this work explores how LLMsâ€™ superior text understanding and extensive open-world knowledge can enhance CLIPâ€™s capability, especially for processing longer and more complex image captions. We propose an efficient post-training strategy that integrates LLMs into pretrained CLIP. To address the challenge posed by the autoregressive nature of LLMs, we introduce a caption-to-caption contrastive fine-tuning framework, significantly enhancing the discriminative quality of LLM outputs. Extensive experiments demonstrate that our approach outperforms LoRA-based methods, achieving nearly fourfold faster training with superior performance. Furthermore, we validate substantial improvements over state-of-the-art models such as CLIP, EVA02, and SigLip2 across various zero-shot multimodal retrieval tasks, cross-lingual retrieval tasks, and multimodal language model pretraining. </p>
<blockquote>
<p>CLIPæ˜¯ä¸€ç§åŸºç¡€çš„å¤šæ¨¡æ€æ¨¡å‹ï¼Œå®ƒé€šè¿‡å¤§è§„æ¨¡å›¾åƒæ–‡æœ¬å¯¹ä¸Šçš„å¯¹æ¯”å­¦ä¹ ï¼Œå°†å›¾åƒå’Œæ–‡æœ¬ç‰¹å¾å¯¹é½åˆ°å…±äº«è¡¨ç¤ºç©ºé—´ã€‚å…¶æœ‰æ•ˆæ€§ä¸»è¦æºäºä½¿ç”¨ä¸°å¯Œçš„è‡ªç„¶è¯­è¨€ç›‘ç£ã€‚å—å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ˜¾è‘—è¿›æ­¥çš„å¯å‘ï¼Œè¿™é¡¹å·¥ä½œæ¢ç´¢äº†LLMså‡ºè‰²çš„æ–‡æœ¬ç†è§£å’Œä¸°å¯Œçš„å¼€æ”¾ä¸–ç•ŒçŸ¥è¯†å¦‚ä½•å¢å¼ºCLIPçš„èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯å¤„ç†æ›´é•¿æ›´å¤æ‚çš„å›¾åƒæ ‡é¢˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„åè®­ç»ƒç­–ç•¥ï¼Œå°†LLMsé›†æˆåˆ°é¢„è®­ç»ƒçš„CLIPä¸­ã€‚ä¸ºäº†è§£å†³LLMsçš„è‡ªå›å½’æ€§è´¨æ‰€å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ ‡é¢˜åˆ°æ ‡é¢˜çš„å¯¹æ¯”å¾®è°ƒæ¡†æ¶ï¼Œæ˜¾è‘—æé«˜äº†LLMè¾“å‡ºçš„è¾¨åˆ«è´¨é‡ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºåŸºäºLoRAçš„æ–¹æ³•ï¼Œè®­ç»ƒé€Ÿåº¦è¿‘ä¹æé«˜äº†å››å€ï¼Œä¸”æ€§èƒ½æ›´ä¼˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨å„ç§é›¶æ ·æœ¬å¤šæ¨¡æ€æ£€ç´¢ä»»åŠ¡ã€è·¨è¯­è¨€æ£€ç´¢ä»»åŠ¡å’Œå¤šåª’ä½“è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒä»»åŠ¡ä¸Šï¼ŒéªŒè¯äº†ä¸æœ€æ–°æ¨¡å‹ï¼ˆå¦‚CLIPã€EVA02å’ŒSigLip2ï¼‰ç›¸æ¯”çš„å®è´¨æ€§æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.04997v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºCLIPæ¨¡å‹çš„å¤šæ¨¡æ€æ¨¡å‹é€šè¿‡å¯¹æ¯”å­¦ä¹ åœ¨å¤§è§„æ¨¡å›¾æ–‡å¯¹ä¸Šå¯¹é½å›¾åƒå’Œæ–‡æœ¬ç‰¹å¾ï¼Œå½¢æˆå…±äº«è¡¨ç¤ºç©ºé—´ã€‚æœ¬æ–‡æ¢ç´¢äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¦‚ä½•å¢å¼ºCLIPå¤„ç†è¾ƒé•¿ã€æ›´å¤æ‚å›¾åƒå­—å¹•çš„èƒ½åŠ›ï¼Œå¹¶æå‡ºå°†LLMsé›†æˆåˆ°é¢„è®­ç»ƒCLIPä¸­çš„é«˜æ•ˆåè®­ç»ƒç­–ç•¥ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é›¶æ ·æœ¬å¤šæ¨¡æ€æ£€ç´¢ä»»åŠ¡ã€è·¨è¯­è¨€æ£€ç´¢ä»»åŠ¡å’Œè·¨æ¨¡æ€è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒç­‰æ–¹é¢å®ç°äº†æ˜¾è‘—æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>CLIPæ˜¯ä¸€ä¸ªåŸºç¡€å¤šæ¨¡æ€æ¨¡å‹ï¼Œé€šè¿‡å°†å›¾åƒå’Œæ–‡æœ¬ç‰¹å¾æ˜ å°„åˆ°å…±äº«è¡¨ç¤ºç©ºé—´è¿›è¡Œå¯¹æ¯”å­¦ä¹ ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å…·æœ‰å‡ºè‰²çš„æ–‡æœ¬ç†è§£å’Œä¸°å¯Œçš„å¼€æ”¾ä¸–ç•ŒçŸ¥è¯†ï¼Œå¯ä»¥å¼ºåŒ–CLIPçš„å¤„ç†èƒ½åŠ›ã€‚</li>
<li>æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„åè®­ç»ƒç­–ç•¥ï¼Œå°†LLMsé›†æˆåˆ°é¢„è®­ç»ƒçš„CLIPæ¨¡å‹ä¸­ã€‚</li>
<li>å¼•å…¥äº†å­—å¹•å¯¹æ¯”å¾®è°ƒæ¡†æ¶æ¥è§£å†³LLMsçš„è‡ªå›å½’æ€§è´¨å¸¦æ¥çš„æŒ‘æˆ˜ã€‚</li>
<li>å®éªŒè¯æ˜è¯¥æ–¹æ³•ä¼˜äºåŸºäºLoRAçš„æ–¹æ³•ï¼Œå®ç°äº†æ›´å¿«çš„è®­ç»ƒå’Œæ›´é«˜çš„æ€§èƒ½ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.04997">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6fdf5378ab38ec6302ca06ce3b5595d7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9696ac22eaf727f99fafd990cdb0415f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-853256ecab48efeceb7a1bb911abad96.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1c4eb5518a39b45f4e05e4f6359fbccf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-46571eb67ad9015e1ce30207cbd2d591.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b5793a1e2b9bba162d7bd97e39342d92.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4d3292d7f78e6c262a513ff0a5dac395.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7fff0cd268e2f71eddde8408fb98f83a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-831249b959997b4092d35bc43e337128.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="XrayGPT-Chest-Radiographs-Summarization-using-Medical-Vision-Language-Models"><a href="#XrayGPT-Chest-Radiographs-Summarization-using-Medical-Vision-Language-Models" class="headerlink" title="XrayGPT: Chest Radiographs Summarization using Medical Vision-Language   Models"></a>XrayGPT: Chest Radiographs Summarization using Medical Vision-Language   Models</h2><p><strong>Authors:Omkar Thawakar, Abdelrahman Shaker, Sahal Shaji Mullappilly, Hisham Cholakkal, Rao Muhammad Anwer, Salman Khan, Jorma Laaksonen, Fahad Shahbaz Khan</strong></p>
<p>The latest breakthroughs in large vision-language models, such as Bard and GPT-4, have showcased extraordinary abilities in performing a wide range of tasks. Such models are trained on massive datasets comprising billions of public image-text pairs with diverse tasks. However, their performance on task-specific domains, such as radiology, is still under-investigated and potentially limited due to a lack of sophistication in understanding biomedical images. On the other hand, conversational medical models have exhibited remarkable success but have mainly focused on text-based analysis. In this paper, we introduce XrayGPT, a novel conversational medical vision-language model that can analyze and answer open-ended questions about chest radiographs. Specifically, we align both medical visual encoder (MedClip) with a fine-tuned large language model (Vicuna), using a simple linear transformation. This alignment enables our model to possess exceptional visual conversation abilities, grounded in a deep understanding of radiographs and medical domain knowledge. To enhance the performance of LLMs in the medical context, we generate ~217k interactive and high-quality summaries from free-text radiology reports. These summaries serve to enhance the performance of LLMs through the fine-tuning process. Our approach opens up new avenues the research for advancing the automated analysis of chest radiographs. Our open-source demos, models, and instruction sets are available at: <a target="_blank" rel="noopener" href="https://github.com/mbzuai-oryx/XrayGPT">https://github.com/mbzuai-oryx/XrayGPT</a>. </p>
<blockquote>
<p>æœ€æ–°çš„å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆå¦‚Bardå’ŒGPT-4ï¼‰çš„çªç ´å±•ç°å‡ºåœ¨å„ç§ä»»åŠ¡ä¸Šçš„è¶…å‡¡èƒ½åŠ›ã€‚è¿™äº›æ¨¡å‹ç»è¿‡å¤§è§„æ¨¡æ•°æ®é›†çš„è®­ç»ƒï¼ŒåŒ…å«æ•°åäº¿å¼ å…¬å…±å›¾åƒå’Œæ–‡æœ¬å¯¹ï¼Œæ¶µç›–å¤šæ ·åŒ–çš„ä»»åŠ¡ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨ç‰¹å®šä»»åŠ¡é¢†åŸŸï¼ˆå¦‚æ”¾å°„å­¦ï¼‰çš„è¡¨ç°ä»å¾…ç ”ç©¶ï¼Œå¹¶ä¸”ç”±äºç¼ºä¹å¤æ‚çš„ç”Ÿç‰©åŒ»å­¦å›¾åƒç†è§£èƒ½åŠ›ï¼Œå…¶è¡¨ç°å¯èƒ½å—åˆ°é™åˆ¶ã€‚å¦ä¸€æ–¹é¢ï¼Œå¯¹è¯å¼åŒ»ç–—æ¨¡å‹å·²ç»å–å¾—äº†æ˜¾è‘—çš„æˆå°±ï¼Œä½†ä¸»è¦ä¾§é‡äºåŸºäºæ–‡æœ¬çš„åˆ†æã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†XrayGPTï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹å¯¹è¯å¼åŒ»ç–—è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œèƒ½å¤Ÿåˆ†æå¹¶å›ç­”æœ‰å…³èƒ¸éƒ¨Xå…‰ç‰‡çš„å¼€æ”¾æ€§é—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é€šè¿‡ç®€å•çš„çº¿æ€§å˜æ¢ï¼Œå°†åŒ»ç–—è§†è§‰ç¼–ç å™¨ï¼ˆMedClipï¼‰ä¸ç»è¿‡å¾®è°ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆVicunaï¼‰è¿›è¡Œå¯¹é½ã€‚è¿™ç§å¯¹é½ä½¿æˆ‘ä»¬çš„æ¨¡å‹æ‹¥æœ‰å‡ºè‰²çš„è§†è§‰å¯¹è¯èƒ½åŠ›ï¼ŒåŸºäºå¯¹Xå…‰ç‰‡å’ŒåŒ»å­¦é¢†åŸŸçŸ¥è¯†çš„æ·±åˆ»ç†è§£ã€‚ä¸ºäº†æé«˜LLMsåœ¨åŒ»å­¦èƒŒæ™¯ä¸‹çš„æ€§èƒ½ï¼Œæˆ‘ä»¬ä»è‡ªç”±æ–‡æœ¬æ”¾å°„å­¦æŠ¥å‘Šä¸­ç”Ÿæˆäº†çº¦21.7ä¸‡æ¡äº¤äº’å¼å’Œé«˜è´¨é‡æ‘˜è¦ã€‚è¿™äº›æ‘˜è¦é€šè¿‡å¾®è°ƒè¿‡ç¨‹å¢å¼ºäº†LLMsçš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ºæ¨è¿›èƒ¸éƒ¨Xå…‰ç‰‡çš„è‡ªåŠ¨åŒ–åˆ†æç ”ç©¶å¼€è¾Ÿäº†æ–°é€”å¾„ã€‚æˆ‘ä»¬çš„å¼€æºæ¼”ç¤ºã€æ¨¡å‹å’ŒæŒ‡ä»¤é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/mbzuai-oryx/XrayGPT%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/mbzuai-oryx/XrayGPTè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2306.07971v2">PDF</a> Accepted at ACL 2024-BIONLP Workshop. Code:   <a target="_blank" rel="noopener" href="https://github.com/mbzuai-oryx/XrayGPT">https://github.com/mbzuai-oryx/XrayGPT</a></p>
<p><strong>Summary</strong>ï¼šæœ€æ–°å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹å¦‚Bardå’ŒGPT-4å±•ç°å‡ºå“è¶Šçš„å¤šä»»åŠ¡æ€§èƒ½ï¼Œä½†åœ¨ç‰¹å®šé¢†åŸŸå¦‚æ”¾å°„å­¦ä¸­çš„è¡¨ç°ä»å¾…ç ”ç©¶ã€‚æ–‡ç« ä»‹ç»äº†ä¸€ç§æ–°å‹å¯¹è¯åŒ»å­¦è§†è§‰è¯­è¨€æ¨¡å‹XrayGPTï¼Œè¯¥æ¨¡å‹é€šè¿‡èåˆåŒ»å­¦è§†è§‰ç¼–ç å™¨MedClipå’Œå¾®è°ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹Vicunaï¼Œå¯¹èƒ¸éƒ¨Xå°„çº¿å›¾åƒè¿›è¡Œå¼€æ”¾é—®ç­”åˆ†æã€‚ä¸ºæé«˜LLMsåœ¨åŒ»å­¦è¯­å¢ƒä¸‹çš„æ€§èƒ½ï¼Œç ”ç©¶å›¢é˜Ÿç”Ÿæˆäº†çº¦21.7ä¸‡ä»½æ¥è‡ªæ”¾å°„æŠ¥å‘Šçš„äº¤äº’å¼é«˜è´¨é‡æ‘˜è¦ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹å¦‚Bardå’ŒGPT-4å…·æœ‰å¹¿æ³›çš„ä»»å½¹èƒ½åŠ›ï¼Œä½†åœ¨åŒ»å­¦é¢†åŸŸçš„è¡¨ç°ä»éœ€æ·±å…¥ç ”ç©¶ã€‚</li>
<li>XrayGPTæ˜¯ä¸€ç§æ–°å‹çš„å¯¹è¯åŒ»å­¦è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œèƒ½åˆ†æå’Œå›ç­”å…³äºèƒ¸éƒ¨Xå°„çº¿å›¾åƒçš„å¼€æ”¾æ€§é—®é¢˜ã€‚</li>
<li>XrayGPTèåˆäº†åŒ»å­¦è§†è§‰ç¼–ç å™¨MedClipå’Œå¾®è°ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹Vicunaï¼Œå®ç°å‡ºè‰²çš„è§†è§‰å¯¹è¯èƒ½åŠ›ã€‚</li>
<li>ç ”ç©¶å›¢é˜Ÿé€šè¿‡ç”Ÿæˆæ¥è‡ªæ”¾å°„æŠ¥å‘Šçš„äº¤äº’å¼é«˜è´¨é‡æ‘˜è¦ï¼Œå¢å¼ºäº†LLMsåœ¨åŒ»å­¦è¯­å¢ƒä¸‹çš„æ€§èƒ½ã€‚</li>
<li>XrayGPTçš„ç ”ç©¶å¼€æ”¾äº†æ–°çš„é€”å¾„ï¼Œæ¨åŠ¨äº†èƒ¸éƒ¨Xå°„çº¿å›¾åƒçš„è‡ªåŠ¨åŒ–åˆ†æçš„å‘å±•ã€‚</li>
<li>XrayGPTçš„å¼€æºæ¼”ç¤ºã€æ¨¡å‹å’ŒæŒ‡ä»¤é›†å¯ä¾›å…¬ä¼—è®¿é—®å’Œä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2306.07971">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-20297ab478b2c096dab547fab17db330.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d68c5e712eae4e6c964eb0c15801a24a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c15aaa539060e8d84eef483f7a67ef9e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-52d79b981781c96adb3ccd26230ec486.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-09/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-09/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-09/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-a0f82ff74bc9ccf90bd75457b8757e02.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-09  Implicitly Aligning Humans and Autonomous Agents through Shared Task   Abstractions
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-09
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-09/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-100471ed028d8370f5d09c8ba954c734.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-09  On Path to Multimodal Generalist General-Level and General-Bench
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-09
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">17982k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
