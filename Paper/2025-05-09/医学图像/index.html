<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="医学图像">
    <meta name="description" content="医学图像 方向最新论文已更新，请持续关注 Update in 2025-05-09  Active Sampling for MRI-based Sequential Decision Making">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>医学图像 | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-aac96b3ebfbae988da12a816938a33ca.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">医学图像</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">医学图像</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                医学图像
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-05-09
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    22.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    91 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-05-09-更新"><a href="#2025-05-09-更新" class="headerlink" title="2025-05-09 更新"></a>2025-05-09 更新</h1><h2 id="Active-Sampling-for-MRI-based-Sequential-Decision-Making"><a href="#Active-Sampling-for-MRI-based-Sequential-Decision-Making" class="headerlink" title="Active Sampling for MRI-based Sequential Decision Making"></a>Active Sampling for MRI-based Sequential Decision Making</h2><p><strong>Authors:Yuning Du, Jingshuai Liu, Rohan Dharmakumar, Sotirios A. Tsaftaris</strong></p>
<p>Despite the superior diagnostic capability of Magnetic Resonance Imaging (MRI), its use as a Point-of-Care (PoC) device remains limited by high cost and complexity. To enable such a future by reducing the magnetic field strength, one key approach will be to improve sampling strategies. Previous work has shown that it is possible to make diagnostic decisions directly from k-space with fewer samples. Such work shows that single diagnostic decisions can be made, but if we aspire to see MRI as a true PoC, multiple and sequential decisions are necessary while minimizing the number of samples acquired. We present a novel multi-objective reinforcement learning framework enabling comprehensive, sequential, diagnostic evaluation from undersampled k-space data. Our approach during inference actively adapts to sequential decisions to optimally sample. To achieve this, we introduce a training methodology that identifies the samples that contribute the best to each diagnostic objective using a step-wise weighting reward function. We evaluate our approach in two sequential knee pathology assessment tasks: ACL sprain detection and cartilage thickness loss assessment. Our framework achieves diagnostic performance competitive with various policy-based benchmarks on disease detection, severity quantification, and overall sequential diagnosis, while substantially saving k-space samples. Our approach paves the way for the future of MRI as a comprehensive and affordable PoC device. Our code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/vios-s/MRI_Sequential_Active_Sampling">https://github.com/vios-s/MRI_Sequential_Active_Sampling</a> </p>
<blockquote>
<p>尽管磁共振成像（MRI）在诊断方面具有卓越的能力，但由于其高昂的成本和复杂性，作为即时检测（Point-of-Care，PoC）设备的使用仍然受到限制。通过降低磁场强度来实现未来技术的发展，关键方法之一是改进采样策略。先前的研究工作已经表明，从较少的样本中可以直接从k空间进行诊断决策。这样的工作证明了可以做出单一的诊断决策，但如果我们希望将MRI视为真正的即时检测工具，则需要在减少采集样本数量的同时，做出多次连续的诊断决策。我们提出了一种新型的多目标强化学习框架，能够从欠采样的k空间数据中实现全面、连续的诊断评估。在推理过程中，我们的方法能够主动适应连续决策以实现最优采样。为了实现这一点，我们引入了一种训练方法，该方法使用逐步加权奖励函数来确定对每个诊断目标贡献最大的样本。我们在两个连续的膝关节病理评估任务中评估了我们的方法：前交叉韧带扭伤检测和软骨厚度损失评估。我们的框架在疾病检测、严重程度量化和整体连续诊断方面的诊断性能与各种基于策略的标准相当，同时大幅减少了k空间样本的采集。我们的方法为磁共振成像作为全面且经济实惠的即时检测设备铺平了道路。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/vios-s/MRI_Sequential_Active_Sampling%E4%B8%8A%E5%85%AC%E5%BC%80%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/vios-s/MRI_Sequential_Active_Sampling上公开获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.04586v1">PDF</a> Under Review</p>
<p><strong>Summary</strong></p>
<p>本文探讨了磁共振成像（MRI）在点医疗护理（PoC）设备中的应用限制，如高成本和复杂性。为提高MRI在PoC设备中的潜力，研究者提出改进采样策略是关键。利用欠采样的k-space数据进行综合诊断评估，研究团队采用一种新型多目标强化学习框架，实现连续决策过程的最优化采样。该研究在膝部病理评估任务中表现出良好的诊断性能，并显著节省了k-space样本。这为MRI作为全面且可负担的PoC设备铺平了道路。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MRI在点医疗护理设备中的应用受限于高成本和复杂性。</li>
<li>改进采样策略是提高MRI在PoC设备中应用潜力的关键。</li>
<li>研究采用新型多目标强化学习框架，实现从欠采样的k-space数据中做出综合诊断评估。</li>
<li>该框架实现了连续决策过程的最优化采样。</li>
<li>在膝部病理评估任务中，该框架表现出良好的诊断性能，与各种政策基准相比具有竞争力。</li>
<li>该框架显著节省了k-space样本的采集。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.04586">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-5f8043abafbfb3037f706b308f2fe8d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c1f75e45ef95fbae2f3981c802b3b20a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c609c7549387967b2d699d78ebc8b37d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5ea647cd9749c874e9f8fd4f4e40a19b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a60fa77114b557ae2a3fcbcdabf65f54.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="RAFT-Robust-Augmentation-of-FeaTures-for-Image-Segmentation"><a href="#RAFT-Robust-Augmentation-of-FeaTures-for-Image-Segmentation" class="headerlink" title="RAFT: Robust Augmentation of FeaTures for Image Segmentation"></a>RAFT: Robust Augmentation of FeaTures for Image Segmentation</h2><p><strong>Authors:Edward Humes, Xiaomin Lin, Uttej Kallakuri, Tinoosh Mohsenin</strong></p>
<p>Image segmentation is a powerful computer vision technique for scene understanding. However, real-world deployment is stymied by the need for high-quality, meticulously labeled datasets. Synthetic data provides high-quality labels while reducing the need for manual data collection and annotation. However, deep neural networks trained on synthetic data often face the Syn2Real problem, leading to poor performance in real-world deployments.   To mitigate the aforementioned gap in image segmentation, we propose RAFT, a novel framework for adapting image segmentation models using minimal labeled real-world data through data and feature augmentations, as well as active learning. To validate RAFT, we perform experiments on the synthetic-to-real “SYNTHIA-&gt;Cityscapes” and “GTAV-&gt;Cityscapes” benchmarks. We managed to surpass the previous state of the art, HALO. SYNTHIA-&gt;Cityscapes experiences an improvement in mIoU* upon domain adaptation of 2.1%&#x2F;79.9%, and GTAV-&gt;Cityscapes experiences a 0.4%&#x2F;78.2% improvement in mIoU. Furthermore, we test our approach on the real-to-real benchmark of “Cityscapes-&gt;ACDC”, and again surpass HALO, with a gain in mIoU upon adaptation of 1.3%&#x2F;73.2%. Finally, we examine the effect of the allocated annotation budget and various components of RAFT upon the final transfer mIoU. </p>
<blockquote>
<p>图像分割是场景理解的一种强大的计算机视觉技术。然而，真实世界的应用部署受到需要高质量、精细标注数据集的限制。合成数据提供了高质量标签，同时减少了手动数据收集和注释的需求。然而，在合成数据上训练的深度神经网络经常面临Syn2Real问题，导致在真实世界部署中的性能不佳。</p>
</blockquote>
<p>为了缓解上述图像分割中的差距，我们提出了RAFT，这是一个新的框架，通过数据和特征增强以及主动学习，使用最少的标记真实世界数据来适应图像分割模型。为了验证RAFT，我们在合成到真实的“SYNTHIA-&gt;Cityscapes”和“GTAV-&gt;Cityscapes”基准测试集上进行了实验。我们超越了之前的先进技术HALO。SYNTHIA-&gt;Cityscapes在域适应方面提高了mIoU* 2.1%&#x2F;79.9%，GTAV-&gt;Cityscapes提高了mIoU 0.4%&#x2F;78.2%。此外，我们在“Cityscapes-&gt;ACDC”的真实到真实的基准测试集上测试了我们的方法，并再次超越了HALO，在适应后提高了mIoU 1.3%&#x2F;73.2%。最后，我们研究了分配注释预算对RAFT最终迁移mIoU的影响以及各种组件的影响。</p>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.04529v1">PDF</a> </p>
<p><strong>Summary</strong><br>     图像分割是一种强大的计算机视觉场景理解技术，但在实际部署中需要高质量、精细标注的数据集。合成数据提供了高质量标签，减少了手动收集和标注的需要。然而，在合成数据上训练的深度神经网络常常面临合成到现实（Syn2Real）问题，导致在现实部署中表现不佳。为缓解图像分割中的上述差距，我们提出了RAFT框架，通过最小限度的真实世界标注数据，利用数据和特征增强以及主动学习方法，对图像分割模型进行适应。实验验证显示，我们在合成到现实的“SYNTHIA→Cityscapes”和“GTAV→Cityscapes”基准测试中超过了之前的最新水平HALO，在mIoU*指标上实现了显著改进。此外，我们在真实到真实的“Cityscapes→ACDC”基准测试中也验证了我们的方法的有效性。最后，我们研究了标注预算分配和RAFT的不同组件对最终迁移mIoU的影响。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>图像分割是计算机视觉中的一项重要技术，用于场景理解，但在实际应用中受到高质量数据集的限制。</li>
<li>合成数据能减少手动数据收集和标注的工作量，但训练的模型在真实场景中表现不佳，存在合成到现实（Syn2Real）问题。</li>
<li>提出RAFT框架，通过数据增强、特征增强和主动学习策略，解决图像分割模型在现实场景部署中的性能差距。</li>
<li>在多个基准测试中验证了RAFT的有效性，包括“SYNTHIA→Cityscapes”，“GTAV→Cityscapes”，以及“Cityscapes→ACDC”，并超过了之前的最新水平HALO。</li>
<li>RAFT框架能有效利用最小限度的真实世界标注数据，提高模型在现实场景中的性能。</li>
<li>实验结果显示，在mIoU*指标上实现了显著改进，具体数值根据不同基准测试而有所不同。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.04529">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-259a34333abf28c411db929cac9c14b4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-59e54fa910c2a9674c019d868575b969.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-033804c9937cc5c85ea2a371eb2fa3fd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ebc51f6e8cc08d2798886eb33c70313.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Text2CT-Towards-3D-CT-Volume-Generation-from-Free-text-Descriptions-Using-Diffusion-Model"><a href="#Text2CT-Towards-3D-CT-Volume-Generation-from-Free-text-Descriptions-Using-Diffusion-Model" class="headerlink" title="Text2CT: Towards 3D CT Volume Generation from Free-text Descriptions   Using Diffusion Model"></a>Text2CT: Towards 3D CT Volume Generation from Free-text Descriptions   Using Diffusion Model</h2><p><strong>Authors:Pengfei Guo, Can Zhao, Dong Yang, Yufan He, Vishwesh Nath, Ziyue Xu, Pedro R. A. S. Bassi, Zongwei Zhou, Benjamin D. Simon, Stephanie Anne Harmon, Baris Turkbey, Daguang Xu</strong></p>
<p>Generating 3D CT volumes from descriptive free-text inputs presents a transformative opportunity in diagnostics and research. In this paper, we introduce Text2CT, a novel approach for synthesizing 3D CT volumes from textual descriptions using the diffusion model. Unlike previous methods that rely on fixed-format text input, Text2CT employs a novel prompt formulation that enables generation from diverse, free-text descriptions. The proposed framework encodes medical text into latent representations and decodes them into high-resolution 3D CT scans, effectively bridging the gap between semantic text inputs and detailed volumetric representations in a unified 3D framework. Our method demonstrates superior performance in preserving anatomical fidelity and capturing intricate structures as described in the input text. Extensive evaluations show that our approach achieves state-of-the-art results, offering promising potential applications in diagnostics, and data augmentation. </p>
<blockquote>
<p>从描述性自由文本输入生成3D CT体积在诊断和研究中提供了变革性的机会。在本文中，我们介绍了Text2CT，这是一种利用扩散模型从文本描述中合成3D CT体积的新方法。与之前依赖于固定格式文本输入的方法不同，Text2CT采用了一种新的提示制定方式，能够根据不同的自由文本描述进行生成。所提出的框架将医学文本编码为潜在表示，并将其解码为高分辨率的3D CT扫描，有效地在统一的3D框架中弥合了语义文本输入和详细体积表示之间的鸿沟。我们的方法在保持解剖保真度和捕捉输入文本中描述的复杂结构方面表现出卓越的性能。大量评估表明，我们的方法达到了最新水平的结果，在诊断和数据增强方面有着广阔的应用前景。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.04522v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种基于扩散模型从文本描述中生成3D CT体积的新方法Text2CT。与传统的依赖于固定格式文本输入的方法不同，Text2CT采用了一种新颖的提示形式，支持从各种自由文本描述中进行生成。该方法将医学文本编码为潜在表示，然后解码为高质量的三维CT扫描图像，有效桥接了语义文本输入和详细体积表示之间的鸿沟。实验表明，该方法在保持解剖真实性和捕捉复杂结构方面表现出卓越性能，为诊断和医学数据增强领域带来了应用前景。</p>
<p><strong>Key Takeaways</strong></p>
<p>以下是文本中关键的见解摘要：</p>
<ol>
<li>Text2CT是一种将文本描述转化为3D CT体积的新型方法。它利用扩散模型，能在诊断和医学研究领域实现转化性进步。</li>
<li>Text2CT不同于传统方法，支持多种自由文本描述作为输入，增强了生成的多样性和灵活性。</li>
<li>该方法通过将医学文本编码为潜在表示，再解码为高质量的三维CT扫描图像，有效融合了语义文本和体积表示。</li>
<li>Text2CT在保持解剖真实性和捕捉复杂结构方面表现出卓越性能，能够有效生成细致的CT体积图像。</li>
<li>广泛评估显示，Text2CT的方法处于领先水平，这为诊断应用提供了潜力。</li>
<li>该方法可用于医学数据增强，有助于丰富医学图像数据集。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.04522">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ddf81f9d13a7b8666f5a69baabe9af15.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e3c7b49feb9f96bc0244b8216c3a82e8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-67e3c88ea22e27211cb9eaaf774eccf8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-615ac0254561132db98fdf1b8c74fb58.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="4XMM-J175136-8-275858-A-New-Magnetar-Candidate"><a href="#4XMM-J175136-8-275858-A-New-Magnetar-Candidate" class="headerlink" title="4XMM J175136.8-275858: A New Magnetar Candidate?"></a>4XMM J175136.8-275858: A New Magnetar Candidate?</h2><p><strong>Authors:Robbie Webbe, Norman Khan, N. A. Webb, E. Quintin</strong></p>
<p>Magnetars are very rare astrophysical objects, with $\sim$31 known to date. They are best understood as highly magnetised neutron stars, but a greater number need to be found to constrain their role in stellar evolution pathways. We apply a novel approach for the detection of fast, transient X-ray sources, using a revised version of the EPIC XMM-Newton Outburst Detector (EXOD) with the aim of detecting and identifying new and rare variable compact objects. We detect a transient, variable source notable for its strong variability and hard spectrum. The emission from 4XMM J175136.8-275858 is well characterised by a blackbody, with temperatures between $\sim$1.8–5,keV during its lower luminosity phase. Its temperature is poorly constrained during its brightest phase, and we observe an increase in luminosity by two orders of magnitude over timescales of a few ks. This is driven by increased emission of X-rays at energies above 2,keV, with a luminosity decay potentially over weeks or months. Derived luminosities for 4XJ1751-2759 range up to $\sim10^{35} \text{,erg s}^{-1}$ at 8,kpc at the Galactic centre, but neutral hydrogen column densities are greater than predicted Galactic values possibly implying a greater distance to the source, still within our galaxy, further increasing its luminosity. A consideration of optical and IR information in combination with the X-ray observations allow us to exclude the possibility that 4XJ1751-2759 is a star, rotationally powered pulsar or supergiant fast X-ray transient. This rapid, hard, variability is closest to that of outbursts in magnetars than any other known class of X-ray transient. </p>
<blockquote>
<p>磁星是非常罕见的天体物理对象，至今已知约31个。它们最好被理解为高度磁化的中子星，但需要发现更多的磁星来限制它们在恒星演化途径中的作用。我们采用一种新颖的方法来检测快速、短暂的X射线源，使用修订后的EPICXMM-Newton爆发检测器（EXOD）版本，旨在检测和识别新的和罕见的可变紧凑型天体。我们检测到一个短暂、可变的源，以其强可变性以及硬谱而显著。来自4XMM J175136.8-275858的发射可以被黑体很好地表征，在其较低光度阶段时，温度约为~ 1.8至~ 5,keV之间。在其最亮的阶段，温度约束较差，我们观察到在几千秒内亮度增加了两个数量级。这是由高于~ 2,keV的X射线发射的增加所驱动的，亮度衰减可能在数周或数月内持续。对于位于银河系中心距离8kpc的磁星来说，其派生光度高达$\sim 10^{35} \text{ erg s}^{-1}$，但中性氢柱密度大于预测的银河系值，可能暗示该源的发射源距离更远。不过仍旧位于银河系内。这一事实连同与光学和红外观测结果综合考虑排除了它是一颗普通恒星、旋转脉冲星或超级巨星快速X射线瞬变的可能性。这种快速而强烈的可变性最接近磁星爆发的特征，与其他已知的X射线瞬变类型相比最为相似。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.04206v1">PDF</a> 14 pages, 10 figures. Accepted to MNRAS</p>
<p><strong>Summary</strong><br>    利用修订后的EPICXMM-Newton爆发检测器（EXOD）新方法检测到了一种短暂、可变的强变量硬谱源。该源（4XMM J175136.8-275858）在较低光度时期的温度约为1.8-5keV，亮度在两个数量级内上升，时间跨度为数千秒。其X射线发射在高于2keV的能量处增强，光度衰减可能持续数周或数月。结合光学和红外信息，排除了其为恒星、旋转驱动脉冲星或超级巨星快速X射线短暂事件的可能性，被认为是最接近磁星爆发的短暂硬变源。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Magnetars是罕见的天文现象，目前已知约31个，被认为是高度磁化的中子星，需要更多的发现来限制它们在恒星演化过程中的作用。</li>
<li>使用修订后的EXOD方法检测到新的短暂可变的强变量硬谱源。</li>
<li>源（4XMM J175136.8-275858）在较低光度时期的温度范围约为1.8-5keV。</li>
<li>源的亮度在短时间内增加了两个数量级。</li>
<li>源的X射线发射在较高能量处增强，光度衰减可能长达数周或数月。</li>
<li>结合光学和红外信息排除了该源为恒星、旋转驱动脉冲星的可能性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.04206">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-60de1fe4d91622f0261e675f19e65e96.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6cf9b14354a7fb569de8e54056e216d1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2dc0a0959f3d105af47185b42549c8fc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e5dd302315769891b3db73ec1ccdb982.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a7b88c44ba647bb1b5e881339271f9d0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1c296cef636a2f2853c4eb76e38d7cd7.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="MAISY-Motion-Aware-Image-SYnthesis-for-MedicalImage-Motion-Correction"><a href="#MAISY-Motion-Aware-Image-SYnthesis-for-MedicalImage-Motion-Correction" class="headerlink" title="MAISY: Motion-Aware Image SYnthesis for MedicalImage Motion Correction"></a>MAISY: Motion-Aware Image SYnthesis for MedicalImage Motion Correction</h2><p><strong>Authors:Andrew Zhang, Hao Wang, Shuchang Ye, Michael Fulham, Jinman Kim</strong></p>
<p>Patient motion during medical image acquisition causes blurring, ghosting, and distorts organs, which makes image interpretation challenging.Current state-of-the-art algorithms using Generative Adversarial Network (GAN)-based methods with their ability to learn the mappings between corrupted images and their ground truth via Structural Similarity Index Measure (SSIM) loss effectively generate motion-free images. However, we identified the following limitations: (i) they mainly focus on global structural characteristics and therefore overlook localized features that often carry critical pathological information, and (ii) the SSIM loss function struggles to handle images with varying pixel intensities, luminance factors, and variance. In this study, we propose Motion-Aware Image SYnthesis (MAISY) which initially characterize motion and then uses it for correction by: (a) leveraging the foundation model Segment Anything Model (SAM), to dynamically learn spatial patterns along anatomical boundaries where motion artifacts are most pronounced and, (b) introducing the Variance-Selective SSIM (VS-SSIM) loss which adaptively emphasizes spatial regions with high pixel variance to preserve essential anatomical details during artifact correction. Experiments on chest and head CT datasets demonstrate that our model outperformed the state-of-the-art counterparts, with Peak Signal-to-Noise Ratio (PSNR) increasing by 40%, SSIM by 10%, and Dice by 16%. </p>
<blockquote>
<p>患者在医学图像采集过程中的运动会导致图像模糊、鬼影和器官扭曲，这使得图像解读具有挑战性。当前最先进的算法使用基于生成对抗网络（GAN）的方法，通过学习被污染图像与真实图像之间的映射关系，通过结构相似性指数度量（SSIM）损失有效地生成无运动图像。然而，我们发现存在以下局限性：（i）它们主要关注全局结构特征，从而忽略了通常携带关键病理信息的局部特征；（ii）SSIM损失函数在处理像素强度、亮度因素和方差各异的图像时遇到困难。本研究提出了运动感知图像合成（MAISY），它首先表征运动，然后利用运动进行修正：（a）通过利用基础模型分割任何模型（SAM），动态学习解剖边界处的空间模式，这些边界处的运动伪影最为突出；（b）引入方差选择性SSIM（VS-SSIM）损失，在伪影修正过程中自适应地强调高像素方差的空间区域，以保留关键的解剖细节。在胸部和头部CT数据集上的实验表明，我们的模型超过了最先进的同行模型，峰值信噪比（PSNR）提高了40%，SSIM提高了10%，Dice系数提高了16%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.04105v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文探讨了医学图像获取过程中患者运动导致的图像模糊、鬼影和器官扭曲问题，使得图像解读变得困难。现有算法主要关注全局结构特征，忽略了携带关键病理信息的局部特征，且SSIM损失函数在处理像素强度、亮度因素和方差变化的图像时存在困难。本研究提出了Motion-Aware Image SYnthesis（MAISY），首先进行运动特征表征，然后利用运动特征进行校正。通过利用Segment Anything Model（SAM）基础模型动态学习解剖边界的空间模式，并引入Variance-Selective SSIM（VS-SSIM）损失，以在保留重要解剖细节的同时自适应地强调高像素方差的空间区域。实验表明，该模型在胸部和头部CT数据集上的表现优于现有技术，PSNR提高40%，SSIM提高10%，Dice提高16%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>医学图像获取过程中的患者运动会导致图像模糊、鬼影和器官扭曲，使得图像解读困难。</li>
<li>当前最先进的算法使用GAN和SSIM损失来生成无运动图像，但存在局限性：主要关注全局结构特征，忽略局部特征；SSIM损失函数在处理像素强度、亮度和方差变化的图像时表现不佳。</li>
<li>本研究提出了Motion-Aware Image SYnthesis（MAISY）模型，该模型首先进行运动特征表征并利用这些特征进行图像校正。</li>
<li>MAISY模型利用Segment Anything Model（SAM）学习解剖边界的空间模式，并强调高像素方差区域以保留重要解剖细节。</li>
<li>实验结果表明，MAISY模型在胸部和头部CT数据集上的表现优于现有技术。</li>
<li>MAISY模型提高了图像质量指标，如PSNR提高40%，SSIM提高10%，Dice提高16%。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.04105">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2ad56f32c9dc60dd7bad97809a6d3e2d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fa1cfcf6463d8b74bad08fcbe4efb97f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3aec9bc57c218e1014e2b1609ce0922f.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Device-Free-Localization-Using-Multi-Link-MIMO-Channels-in-Distributed-Antenna-Networks"><a href="#Device-Free-Localization-Using-Multi-Link-MIMO-Channels-in-Distributed-Antenna-Networks" class="headerlink" title="Device-Free Localization Using Multi-Link MIMO Channels in Distributed   Antenna Networks"></a>Device-Free Localization Using Multi-Link MIMO Channels in Distributed   Antenna Networks</h2><p><strong>Authors:Minseok Kim, Gesi Teng, Keita Nishi, Togo Ikegami, Masamune Sato</strong></p>
<p>This paper presented a novel device-free localization (DFL) framework based on distributed antenna networks (DANs), targeting integrated sensing and communication (ISAC) in future 6G radio access networks (RANs). In the proposed approach, radio tomographic imaging (RTI) leverages the spatial and temporal diversity of multi-link multiple-input multiple-output (MIMO) channels in DANs to improve localization accuracy. Furthermore, a prototype system was developed using software-defined radios (SDRs) operating in the sub-6 GHz band, and comprehensive evaluations were conducted under indoor conditions involving varying node densities and target types. The results demonstrate that the framework achieves sub-meter localization accuracy in most scenarios and maintains robust performance under complex multipath environments. In addition, the use of Bayesian optimization to fine-tune key parameters, such as sparsity and path thickness, led to significant improvements in image reconstruction quality and target estimation accuracy. These results demonstrate the feasibility and effectiveness of DAN-based DFL systems for accurate, robust, and scalable localization. </p>
<blockquote>
<p>本文提出了一种基于分布式天线网络（DANs）的新型无设备定位（DFL）框架，旨在用于未来6G无线接入网络（RANs）的综合感知和通信（ISAC）。在该方法中，通过利用分布天线网络的多个链接的多输入多输出（MIMO）通道的时空多样性，放射层析成像（RTI）提高了定位精度。此外，开发了一个采用工作在低于6 GHz频段的软件定义无线电（SDRs）的原型系统，并在涉及不同节点密度和目标类型的室内条件下进行了全面的评估。结果表明，该框架在大多数场景中实现了亚米级定位精度，并在复杂的多径环境中保持了稳健的性能。此外，使用贝叶斯优化对关键参数（如稀疏性和路径厚度）进行微调，显著提高了图像重建质量和目标估计精度。这些结果证明了基于DAN的DFL系统在实现准确、稳健和可扩展的定位方面的可行性和有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.04085v1">PDF</a> </p>
<p><strong>Summary</strong><br>     本文提出了一种基于分布式天线网络（DANs）的无设备定位（DFL）框架，适用于未来6G无线电接入网络（RANs）中的集成感知和通信（ISAC）。该框架利用无线电层析成像（RTI）技术，通过利用DANs中多链路多输入多输出（MIMO）通道的空间和时间多样性来提高定位精度。开发了一个采用软件定义无线电（SDRs）在低于6GHz频段运行的原型系统，并在室内条件下进行了全面的评估，包括不同的节点密度和目标类型。结果表明，该框架在大多数场景下实现了亚米级定位精度，并在复杂的多路径环境下保持了稳健的性能。此外，使用贝叶斯优化对稀疏性和路径厚度等关键参数进行微调，显著提高了图像重建质量和目标估计精度。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>该论文介绍了一种新型的基于分布式天线网络（DANs）的无设备定位（DFL）框架，旨在用于未来6G网络的集成感知和通信。</li>
<li>通过无线电层析成像（RTI）技术，利用MIMO通道的空间和时间多样性提高定位精度。</li>
<li>开发了采用软件定义无线电（SDRs）的原型系统，并在低于6GHz的频段进行试验。</li>
<li>在室内环境下进行了全面的评估，测试环境涵盖不同的节点密度和目标类型。</li>
<li>该框架实现了亚米级定位精度，且在复杂多路径环境中表现稳健。</li>
<li>使用贝叶斯优化对关键参数进行微调，显著提升了图像重建质量和目标估计的准确性。</li>
<li>整体结果表明，基于DAN的DFL系统为实现准确、稳健和可扩展的定位是可行的和有效的。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.04085">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c707755cea16d7d22e8d5b66873a1d03.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-35f8783af72419addb6fd19439ae5275.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-823e4a8dbf5bed70f70e9c48297327ef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-18c8c30932cc75c6b06d3c9a6a22c4b1.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Teleios-G305-4-2-2-–-the-mystery-of-a-perfectly-shaped-new-Galactic-supernova-remnant"><a href="#Teleios-G305-4-2-2-–-the-mystery-of-a-perfectly-shaped-new-Galactic-supernova-remnant" class="headerlink" title="Teleios (G305.4-2.2) – the mystery of a perfectly shaped new Galactic   supernova remnant"></a>Teleios (G305.4-2.2) – the mystery of a perfectly shaped new Galactic   supernova remnant</h2><p><strong>Authors:Miroslav D. Filipovic, Zachary J. Smeaton, Roland Kothes, Silvia Mantovanini, Petar Kostic, Denis Leahy, Adeel Ahmad, Gemma E. Anderson, Miguel Araya, Brianna Ball, Werner Becker, Cristobal Bordiu, Aaron C. Bradley, Robert Brose, Christopher Burger-Scheidlin, Shi Dai, Stefan Duchesne, Timothy J. Galvin, Andrew M. Hopkins, Natasha Hurley-Walker, Barbel S. Koribalski, Sanja Lazarevic, Peter Lundqvist, Jonathan Mackey, Pierrick Martin, Padric McGee, Ana Mitrasinovic, Jeffrey L. Payne, Simone Riggi, Kathryn Ross, Gavin Rowell, Lawrence Rudnick, Hidetoshi Sano, Manami Sasaki, Roberto Soria, Dejan Urosevic, Branislav Vukotic, Jennifer L. West</strong></p>
<p>We present the serendipitous radio-continuum discovery of a likely Galactic supernova remnant (SNR) G305.4-2.2. This object displays a remarkable circular symmetry in shape, making it one of the most circular Galactic SNRs known. Nicknamed Teleios due to its symmetry, it was detected in the new Australian Square Kilometre Array Pathfinder (ASKAP) Evolutionary Map of the Universe (EMU) radio-continuum images with an angular size of 1320”x1260” and PA &#x3D; 0 deg. While there is a hint of possible H$\alpha$ and gamma-ray emission, Teleios is exclusively seen at radio-continuum frequencies. Interestingly, Teleios is not only almost perfectly symmetric, but it also has one of the lowest surface brightnesses discovered among Galactic SNRs and a steep spectral index of $\alpha&#x3D;-0.6\pm 0.3$. Our estimates from HI studies and the Sigma-D relation place Teleios as a type Ia SNR at a distance of either ~2.2 kpc of ~7.7 kpc. This indicates two possible scenarios, either a young (under 1000 yr) or an older SNR (over 10000 yr). With a corresponding diameter of 14&#x2F;48 pc, our evolutionary studies place Teleios at the either early or late Sedov phase, depending on the distance estimate. However, our modelling also predicts X-ray emission, which we do not see in the present generation of eROSITA images. We also explored a type Iax explosion scenario that points to a much closer distance of &lt;1 kpc and Teleios size of only ~3.3 pc, which would be similar to the only known type Iax remnant SN1181. Unfortunately, all examined scenarios have their challenges, and no definitive supernova (SN) origin type can be established at this stage. Teleios’s symmetrical shape suggests expansion into a rarefied and isotropic ambient medium. The low radio surface brightness and the lack of pronounced polarisation can be explained by a high level of ambient rotation measure (RM), with the largest RM being observed at centre. </p>
<blockquote>
<p>我们意外发现了银河系超新星遗迹（SNR）G305.4-2.2的无线电连续谱发现。该对象呈现出显著的圆形对称性，使其成为已知最圆的银河系SNR之一。由于其对称性，它被命名为Teleios。它是在新的澳大利亚平方公里阵列探路者（ASKAP）宇宙演化图（EMU）无线电连续谱图像中检测到的，其角大小为1320”x1260”，位置角为0度。虽然有Hα和γ射线的可能发射提示，但Teleios仅在无线电连续谱频率下可见。有趣的是，Teleios不仅几乎完全对称，而且它还具有银河系SNR中发现的最低的表面亮度之一，以及α&#x3D;-0.6±0.3的陡峭光谱指数。我们通过HI研究和Sigma-D关系估计，Teleios是Ia型SNR，距离约为2.2kpc或7.7kpc。这表明了两种可能的情况，一是年轻的（不到1000岁）SNR，二是较老的SNR（超过10000岁）。相应的直径为14&#x2F;48秒差距，我们的进化研究将Teleios置于早期的塞多夫阶段或晚期塞多夫阶段，这取决于距离估计。然而，我们的模型还预测了X射线发射，这在当前的eROSITA图像中并未观察到。我们还探索了Iax型爆炸情景，这指向了小于1kpc的较近距离和Teleios只有约3.3秒差距的大小，这类似于已知的唯一Iax型遗迹SN1181。不幸的是，所有考察的情况都有其挑战，目前阶段无法确定超新星（SN）的起源类型。Teleios的对称形状表明其膨胀到一个稀疏的同位素环境中。低无线电表面亮度和缺乏明显的极化可以用高环境旋转度量（RM）来解释，最大的RM被观察到在中心。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.04041v1">PDF</a> Has been accepted for publication in PASA</p>
<p><strong>Summary</strong><br>     发现一个新的可能银河系超新星遗迹G305.4-2.2，呈现显著圆形对称性，命名为Teleios。通过ASKAP和EMU的射电连续图像检测，初步判断为年轻的或年老的超新星遗迹，距离约为2.2kpc或7.7kpc。早期或晚期Sedov阶段模型预测有X射线发射，但在现有eROSITA图像中未见。同时探索了可能的Ia型或Iax型超新星爆炸情景，但仍无法确定其起源类型。周围介质被认为具有高的旋转度量（RM）。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>发现新的银河系超新星遗迹G305.4-2.2（Teleios），具有显著的圆形对称性。</li>
<li>Teleios在射电连续谱上被发现，且具有非常低的表面亮度和陡峭的谱指数α&#x3D;-0.6±0.3。</li>
<li>基于HI研究和Sigma-D关系，推测Teleios的距离可能为约2.2kpc或约7.7kpc，分别对应年轻或年老的超新星遗迹。</li>
<li>Teleios可能处于早期或晚期Sedov阶段，具体取决于距离估计。</li>
<li>预测存在X射线发射，但在现有的eROSITA图像中未见。</li>
<li>对可能的Ia型或Iax型超新星爆炸情景进行了探索，但无法确定其确切的SN起源类型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.04041">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-5dbb312e969e97aecb1edb9bd08815cf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-01c7f608559c810257b0cf349897e78a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2766629cd16cf8fee162d90f42b15d76.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0d002064cca4dacee6b3ec4b8a504a46.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5a5a2a3a2b0b7e673d88cf66ddf7197c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b6c756f350ff3f62267ad94132295bc6.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="IntelliCardiac-An-Intelligent-Platform-for-Cardiac-Image-Segmentation-and-Classification"><a href="#IntelliCardiac-An-Intelligent-Platform-for-Cardiac-Image-Segmentation-and-Classification" class="headerlink" title="IntelliCardiac: An Intelligent Platform for Cardiac Image Segmentation   and Classification"></a>IntelliCardiac: An Intelligent Platform for Cardiac Image Segmentation   and Classification</h2><p><strong>Authors:Ting Yu Tsai, An Yu, Meghana Spurthi Maadugundu, Ishrat Jahan Mohima, Umme Habiba Barsha, Mei-Hwa F. Chen, Balakrishnan Prabhakaran, Ming-Ching Chang</strong></p>
<p>Precise and effective processing of cardiac imaging data is critical for the identification and management of the cardiovascular diseases. We introduce IntelliCardiac, a comprehensive, web-based medical image processing platform for the automatic segmentation of 4D cardiac images and disease classification, utilizing an AI model trained on the publicly accessible ACDC dataset. The system, intended for patients, cardiologists, and healthcare professionals, offers an intuitive interface and uses deep learning models to identify essential heart structures and categorize cardiac diseases. The system supports analysis of both the right and left ventricles as well as myocardium, and then classifies patient’s cardiac images into five diagnostic categories: dilated cardiomyopathy, myocardial infarction, hypertrophic cardiomyopathy, right ventricular abnormality, and no disease. IntelliCardiac combines a deep learning-based segmentation model with a two-step classification pipeline. The segmentation module gains an overall accuracy of 92.6%. The classification module, trained on characteristics taken from segmented heart structures, achieves 98% accuracy in five categories. These results exceed the performance of the existing state-of-the-art methods that integrate both segmentation and classification models. IntelliCardiac, which supports real-time visualization, workflow integration, and AI-assisted diagnostics, has great potential as a scalable, accurate tool for clinical decision assistance in cardiac imaging and diagnosis. </p>
<blockquote>
<p>精确且有效地处理心脏成像数据对于心血管疾病的识别和管理至关重要。我们推出IntelliCardiac，这是一款全面的基于网页的医学图像处理平台，用于自动分割4D心脏图像和疾病分类。它利用在公共可访问的ACDC数据集上训练的AI模型。此系统面向患者、心脏病专家以及医疗专业人士，提供直观界面，并使用深度学习模型来识别心脏的关键结构并对心脏疾病进行分类。系统支持对右心室和左心室以及心肌的分析，然后将患者的心脏图像分类为五种诊断类别：扩张型心肌病、心肌梗死、肥厚型心肌病、右心室异常以及无疾病。IntelliCardiac结合了基于深度学习的分割模型与两步分类流程。分割模块的整体准确率为92.6%。分类模块基于分割心脏结构的特征进行训练，在五个类别中达到98%的准确率。这些结果超过了现有最先进的集成分割和分类模型的方法的表现。IntelliCardiac支持实时可视化、工作流程集成和人工智能辅助诊断，作为心脏成像和诊断的临床决策辅助工具，具有可扩展性和准确性，具有巨大的潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.03838v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了IntelliCardiac这一基于网络的医学图像处理平台，用于自动分割四维心脏图像和疾病分类。它利用在公开可访问的ACDC数据集上训练的AI模型，提供对患者、心脏病专家和医疗保健专业人员的直观界面，并使用深度学习模型识别心脏结构并进行分类。该平台支持左右心室和心肌的分析，并将患者的心脏图像分为五个诊断类别。IntelliCardiac结合了深度学习分割模型和两步分类管道，其性能优于现有的最先进的集成分割和分类模型的方法。它具有良好的可扩展性和准确性，可作为心脏成像和诊断的临床决策辅助工具。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>IntelliCardiac是一个用于自动分割四维心脏图像和疾病分类的医学图像处理平台。</li>
<li>平台使用深度学习模型进行心脏结构识别和疾病分类。</li>
<li>支持左右心室和心肌的分析，分为五个诊断类别。</li>
<li>IntelliCardiac结合了深度学习分割模型和两步分类管道。</li>
<li>分割模型的准确度为92.6%，分类模型的准确度为98%。</li>
<li>与现有的集成分割和分类模型的方法相比，IntelliCardiac的性能更为优越。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.03838">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-5037aafe40bd4b469837b6807b986688.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a15755d537f500447dcf3fcc3c146cac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3608cd25951f0b2fc10788ff462a7257.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dfb431549855a8a85dd34c98bfac9704.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-752e3cd3412f80d43099d556cbf0ef54.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-56bfddbf7c70202ecd1577dbf4f0384f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5db69e6b4914d0f89deeefa779e0db63.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="mAIstro-an-open-source-multi-agentic-system-for-automated-end-to-end-development-of-radiomics-and-deep-learning-models-for-medical-imaging"><a href="#mAIstro-an-open-source-multi-agentic-system-for-automated-end-to-end-development-of-radiomics-and-deep-learning-models-for-medical-imaging" class="headerlink" title="mAIstro: an open-source multi-agentic system for automated end-to-end   development of radiomics and deep learning models for medical imaging"></a>mAIstro: an open-source multi-agentic system for automated end-to-end   development of radiomics and deep learning models for medical imaging</h2><p><strong>Authors:Eleftherios Tzanis, Michail E. Klontzas</strong></p>
<p>Agentic systems built on large language models (LLMs) offer promising capabilities for automating complex workflows in healthcare AI. We introduce mAIstro, an open-source, autonomous multi-agentic framework for end-to-end development and deployment of medical AI models. The system orchestrates exploratory data analysis, radiomic feature extraction, image segmentation, classification, and regression through a natural language interface, requiring no coding from the user. Built on a modular architecture, mAIstro supports both open- and closed-source LLMs, and was evaluated using a large and diverse set of prompts across 16 open-source datasets, covering a wide range of imaging modalities, anatomical regions, and data types. The agents successfully executed all tasks, producing interpretable outputs and validated models. This work presents the first agentic framework capable of unifying data analysis, AI model development, and inference across varied healthcare applications, offering a reproducible and extensible foundation for clinical and research AI integration. The code is available at: <a target="_blank" rel="noopener" href="https://github.com/eltzanis/mAIstro">https://github.com/eltzanis/mAIstro</a> </p>
<blockquote>
<p>基于大型语言模型（LLM）的Agentic系统为医疗保健AI中复杂工作流的自动化提供了有前途的能力。我们介绍了mAIstro，这是一个开源的、自主的多Agentic框架，用于端到端的医疗AI模型开发和部署。该系统通过自然语言接口协调探索性数据分析、放射学特征提取、图像分割、分类和回归，无需用户编写代码。mAIstro采用模块化架构，支持开源和闭源的LLM，并使用涵盖广泛成像模式、解剖部位和数据类型的16个开源数据集的大型和多样化的提示集进行评估。代理成功执行了所有任务，产生了可解释的输出和经过验证的模型。这项工作提出了第一个能够在各种医疗保健应用中统一数据分析、AI模型开发和推理的Agentic框架，为临床和研究AI集成提供了可复制和可扩展的基础。代码可在<a target="_blank" rel="noopener" href="https://github.com/eltzanis/mAIstro%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/eltzanis/mAIstro找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.03785v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于大型语言模型（LLM）的Agentic系统为医疗保健AI自动化复杂工作流程提供了有前景的能力。本文介绍了mAIstro，一个开源的、自主的多Agentic框架，用于端到端医疗AI模型的开发和部署。该系统通过自然语言接口协调探索性分析、放射特征提取、图像分割、分类和回归，无需用户编程。在跨多个开源数据集、涵盖多种成像模式、解剖部位和数据类型的广泛评估中，agents成功执行了所有任务，产生了可解释的输出和经过验证的模型。这项工作是第一个能够在各种医疗保健应用中统一数据分析、AI模型开发和推理的Agentic框架，为临床和研究AI的融合提供了可复制和可扩展的基础。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Agentic系统基于大型语言模型（LLM），为医疗保健AI自动化提供前景。</li>
<li>mAIstro是一个开源的、自主的多Agentic框架，支持医疗AI模型端到端的开发和部署。</li>
<li>mAIstro通过自然语言接口进行探索性分析、放射特征提取、图像分割等任务。</li>
<li>系统无需用户编程，可协调多种任务。</li>
<li>mAIstro支持开源和闭源的LLM。</li>
<li>Agents在广泛的数据集上成功执行任务，产生可解释的输出和经过验证的模型。</li>
<li>mAIstro为临床和研究AI的融合提供了可复制和可扩展的基础。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.03785">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-aac96b3ebfbae988da12a816938a33ca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0c61218385cef034302cdb39bae5efba.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="RESAnything-Attribute-Prompting-for-Arbitrary-Referring-Segmentation"><a href="#RESAnything-Attribute-Prompting-for-Arbitrary-Referring-Segmentation" class="headerlink" title="RESAnything: Attribute Prompting for Arbitrary Referring Segmentation"></a>RESAnything: Attribute Prompting for Arbitrary Referring Segmentation</h2><p><strong>Authors:Ruiqi Wang, Hao Zhang</strong></p>
<p>We present an open-vocabulary and zero-shot method for arbitrary referring expression segmentation (RES), targeting input expressions that are more general than what prior works were designed to handle. Specifically, our inputs encompass both object- and part-level labels as well as implicit references pointing to properties or qualities of object&#x2F;part function, design, style, material, etc. Our model, coined RESAnything, leverages Chain-of-Thoughts (CoT) reasoning, where the key idea is attribute prompting. We generate detailed descriptions of object&#x2F;part attributes including shape, color, and location for potential segment proposals through systematic prompting of a large language model (LLM), where the proposals are produced by a foundational image segmentation model. Our approach encourages deep reasoning about object or part attributes related to function, style, design, etc., enabling the system to handle implicit queries without any part annotations for training or fine-tuning. As the first zero-shot and LLM-based RES method, RESAnything achieves clearly superior performance among zero-shot methods on traditional RES benchmarks and significantly outperforms existing methods on challenging scenarios involving implicit queries and complex part-level relations. Finally, we contribute a new benchmark dataset to offer ~3K carefully curated RES instances to assess part-level, arbitrary RES solutions. </p>
<blockquote>
<p>我们提出了一种用于任意引用表达式分割（RES）的开放词汇和零样本方法，针对的输入表达式比以往工作设计的更为通用。具体来说，我们的输入涵盖了对象级和部件级标签，以及指向对象&#x2F;部件功能、设计、风格、材质等属性的隐含引用。我们的模型被称为RESAnything，它利用Chain-of-Thoughts（CoT）推理，关键思想是属性提示。我们通过系统提示大型语言模型（LLM）来生成对象&#x2F;部件属性的详细描述，包括形状、颜色和位置等，为潜在的分段提案提供支持。这些提案由基础图像分割模型生成。我们的方法鼓励对对象或部件属性（如功能、风格、设计等）进行深度推理，使系统能够处理隐式查询，而无需任何部分注释进行训练或微调。作为基于零样本和LLM的RES方法，RESAnything在传统的RES基准测试上实现了明显的优势性能，并且在涉及隐式查询和复杂部件级关系的挑战性场景中显著优于现有方法。最后，我们贡献了一个新的基准数据集，提供了约3K个精心策划的RES实例，以评估部件级、任意RES解决方案。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.02867v1">PDF</a> 42 pages, 31 figures. For more details:   <a target="_blank" rel="noopener" href="https://suikei-wang.github.io/RESAnything/">https://suikei-wang.github.io/RESAnything/</a></p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种开放词汇表和零样本方法，用于处理任意引用表达式分割（RES），该方法能够处理比以往研究更一般的输入表达式。通过Chain-of-Thoughts（CoT）推理和属性提示，模型RESAnything能够详细描述对象&#x2F;部分的属性，如形状、颜色和位置，以产生潜在的分段建议。该方法鼓励对对象或部分的属性进行深度推理，能够处理隐式查询，无需对训练或微调进行部分注释。作为第一个基于零样本和大型语言模型（LLM）的RES方法，RESAnything在传统的RES基准测试上的表现优于其他零样本方法，在处理涉及隐式查询和复杂部分级关系的挑战场景时表现出显著的优势。最后，本文贡献了一个新的基准数据集，包含约3K个精心挑选的RES实例，以评估部分级、任意的RES解决方案。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出了一种开放词汇表和零样本方法用于任意引用表达式分割（RES）。</li>
<li>模型RESAnything能够处理更一般的输入表达式，涵盖对象级和部分级的标签以及指向属性或功能的隐式引用。</li>
<li>采用Chain-of-Thoughts（CoT）推理和属性提示，生成详细的对象&#x2F;部分属性描述。</li>
<li>鼓励对对象或部分的属性进行深度推理，能处理隐式查询，无需对训练或微调进行部分注释。</li>
<li>RESAnything在RES基准测试上的表现优于其他零样本方法，特别擅长处理复杂场景和隐式查询。</li>
<li>引入了一个新的基准数据集，包含约3K个RES实例，用于评估部分级、任意的RES解决方案。</li>
<li>该方法推动了医学图像分割技术的发展，尤其是任意引用表达式分割的处理能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.02867">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-838c2f525b419bc4b84318c21c78fbce.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-82fb20b90fa5f9f9590597efc8399386.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ae62dc9b97c4f8f1dbf3643150b81bc5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f6335bf5ff638853d90a9afcefdce353.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a38f640a35f562c0dddee0c1cdf120e0.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Advances-in-Automated-Fetal-Brain-MRI-Segmentation-and-Biometry-Insights-from-the-FeTA-2024-Challenge"><a href="#Advances-in-Automated-Fetal-Brain-MRI-Segmentation-and-Biometry-Insights-from-the-FeTA-2024-Challenge" class="headerlink" title="Advances in Automated Fetal Brain MRI Segmentation and Biometry:   Insights from the FeTA 2024 Challenge"></a>Advances in Automated Fetal Brain MRI Segmentation and Biometry:   Insights from the FeTA 2024 Challenge</h2><p><strong>Authors:Vladyslav Zalevskyi, Thomas Sanchez, Misha Kaandorp, Margaux Roulet, Diego Fajardo-Rojas, Liu Li, Jana Hutter, Hongwei Bran Li, Matthew Barkovich, Hui Ji, Luca Wilhelmi, Aline Dändliker, Céline Steger, Mériam Koob, Yvan Gomez, Anton Jakovčić, Melita Klaić, Ana Adžić, Pavel Marković, Gracia Grabarić, Milan Rados, Jordina Aviles Verdera, Gregor Kasprian, Gregor Dovjak, Raphael Gaubert-Rachmühl, Maurice Aschwanden, Qi Zeng, Davood Karimi, Denis Peruzzo, Tommaso Ciceri, Giorgio Longari, Rachika E. Hamadache, Amina Bouzid, Xavier Lladó, Simone Chiarella, Gerard Martí-Juan, Miguel Ángel González Ballester, Marco Castellaro, Marco Pinamonti, Valentina Visani, Robin Cremese, Keïn Sam, Fleur Gaudfernau, Param Ahir, Mehul Parikh, Maximilian Zenk, Michael Baumgartner, Klaus Maier-Hein, Li Tianhong, Yang Hong, Zhao Longfei, Domen Preloznik, Žiga Špiclin, Jae Won Choi, Muyang Li, Jia Fu, Guotai Wang, Jingwen Jiang, Lyuyang Tong, Bo Du, Milton O. Candela-Leal, Andrea Gondova, Sungmin You, Abdul Qayyum, Moona Mazher, Steven A Niederer, Andras Jakab, Roxane Licandro, Kelly Payette, Meritxell Bach Cuadra</strong></p>
<p>Accurate fetal brain tissue segmentation and biometric analysis are essential for studying brain development in utero. The FeTA Challenge 2024 advanced automated fetal brain MRI analysis by introducing biometry prediction as a new task alongside tissue segmentation. For the first time, our diverse multi-centric test set included data from a new low-field (0.55T) MRI dataset. Evaluation metrics were also expanded to include the topology-specific Euler characteristic difference (ED). Sixteen teams submitted segmentation methods, most of which performed consistently across both high- and low-field scans. However, longitudinal trends indicate that segmentation accuracy may be reaching a plateau, with results now approaching inter-rater variability. The ED metric uncovered topological differences that were missed by conventional metrics, while the low-field dataset achieved the highest segmentation scores, highlighting the potential of affordable imaging systems when paired with high-quality reconstruction. Seven teams participated in the biometry task, but most methods failed to outperform a simple baseline that predicted measurements based solely on gestational age, underscoring the challenge of extracting reliable biometric estimates from image data alone. Domain shift analysis identified image quality as the most significant factor affecting model generalization, with super-resolution pipelines also playing a substantial role. Other factors, such as gestational age, pathology, and acquisition site, had smaller, though still measurable, effects. Overall, FeTA 2024 offers a comprehensive benchmark for multi-class segmentation and biometry estimation in fetal brain MRI, underscoring the need for data-centric approaches, improved topological evaluation, and greater dataset diversity to enable clinically robust and generalizable AI tools. </p>
<blockquote>
<p>准确胎儿脑组织分割和生物测定分析对于研究胎儿期的大脑发育至关重要。FeTA Challenge 2024通过引入生物测定预测作为新任务，推动了胎儿脑部MRI分析的自动化发展，同时辅以组织分割。我们的多样化多中心测试集首次包含来自新低场（0.55T）MRI数据集的数据。评估指标也得以扩充，包括针对拓扑特性的欧拉特征差异（ED）。有十六支队伍提交了分割方法，其中大多数在高场和低场扫描中都表现稳定。然而纵向趋势表明，分割精度可能已达到一个瓶颈期，其结果现已接近观察者间变异度。ED指标发现了传统指标未能覆盖的拓扑差异，同时低场数据集获得了最高的分割得分，这突显了当与高质量重建相结合时，平价成像系统的潜力。有七支队伍参与了生物测定任务，但大多数方法未能超越一个简单基线，该基线仅根据胎龄进行预测测量，这突显了仅从图像数据中提取可靠生物测定估计的挑战性。域偏移分析确定了图像质量是影响模型泛化的最重要因素，超分辨率流程也发挥了重要作用。其他因素，如胎龄、病理和采集地点的影响较小，但仍可衡量。总体而言，FeTA 2024为多类分割和胎儿脑部MRI生物测定估计提供了全面的基准测试，这突显了对数据中心方法、改进拓扑评估和更大数据集多样性的需求，以实现临床稳健且可推广的人工智能工具。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.02784v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了FeTA Challenge 2024在胎儿脑部MRI分析方面的进展，包括组织分割和生物计量预测两个任务。研究使用了多中心测试集，包括来自低场（0.55T）MRI数据集的数据。评价指标扩展到了拓扑特定的Euler特征差异（ED）。尽管大多数团队在高场和低场扫描中表现稳定，但分割精度可能已经达到一个平台期。低场数据集获得了最高的分割分数，突显出低成本成像系统与高质重建相结合时的潜力。对于生物计量任务，大多数方法未能超越基于胎龄的简单基线预测，表明仅从图像数据中提取可靠生物计量估计的挑战性。领域偏移分析表明，图像质量是影响模型泛化的最重要因素。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FeTA Challenge 2024扩展了胎儿脑部MRI分析，包括生物计量预测这一新任务。</li>
<li>多样化的多中心测试集包含来自低场MRI数据集的数据。</li>
<li>评价指标新增了Euler特征差异（ED），用于捕捉拓扑特性。</li>
<li>大多数团队在高场和低场扫描中表现稳定，但分割精度接近饱和。</li>
<li>低场数据集获得最高分割分数，显示低成本成像系统结合高质量重建的潜力。</li>
<li>生物计量预测任务面临挑战，简单基线模型基于胎龄的预测表现较好。</li>
<li>领域偏移分析显示，模型泛化受影响最大的因素是图像质量，其次是超分辨率管道、胎龄、病理和采集地点。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.02784">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-59abc2a5d7b365fdb6a3df4093a10af2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-832bc4077b2ac604381d11d72513ff93.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b33490215f6013f90bf3c62bc0035a8e.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Accelerating-Volumetric-Medical-Image-Annotation-via-Short-Long-Memory-SAM-2"><a href="#Accelerating-Volumetric-Medical-Image-Annotation-via-Short-Long-Memory-SAM-2" class="headerlink" title="Accelerating Volumetric Medical Image Annotation via Short-Long Memory   SAM 2"></a>Accelerating Volumetric Medical Image Annotation via Short-Long Memory   SAM 2</h2><p><strong>Authors:Yuwen Chen, Zafer Yildiz, Qihang Li, Yaqian Chen, Haoyu Dong, Hanxue Gu, Nicholas Konz, Maciej A. Mazurowski</strong></p>
<p>Manual annotation of volumetric medical images, such as magnetic resonance imaging (MRI) and computed tomography (CT), is a labor-intensive and time-consuming process. Recent advancements in foundation models for video object segmentation, such as Segment Anything Model 2 (SAM 2), offer a potential opportunity to significantly speed up the annotation process by manually annotating one or a few slices and then propagating target masks across the entire volume. However, the performance of SAM 2 in this context varies. Our experiments show that relying on a single memory bank and attention module is prone to error propagation, particularly at boundary regions where the target is present in the previous slice but absent in the current one. To address this problem, we propose Short-Long Memory SAM 2 (SLM-SAM 2), a novel architecture that integrates distinct short-term and long-term memory banks with separate attention modules to improve segmentation accuracy. We evaluate SLM-SAM 2 on three public datasets covering organs, bones, and muscles across MRI and CT modalities. We show that the proposed method markedly outperforms the default SAM 2, achieving average Dice Similarity Coefficient improvement of 0.14 and 0.11 in the scenarios when 5 volumes and 1 volume are available for the initial adaptation, respectively. SLM-SAM 2 also exhibits stronger resistance to over-propagation, making a notable step toward more accurate automated annotation of medical images for segmentation model development. </p>
<blockquote>
<p>医学图像（如磁共振成像（MRI）和计算机断层扫描（CT））的手动标注是一个劳动密集型且耗时的工作过程。近期视频对象分割基础模型（例如分割任何模型 2 （SAM 2））的最新进展提供了一个潜在的机会，可以通过手动标注一个或多个切片，然后将其传播到整个体积，从而显著加快标注过程。然而，SAM 2 在此背景下的表现有所不同。我们的实验表明，依赖单一内存银行和注意力模块容易出现误差传播，特别是在目标出现在前一个切片中而在当前切片中不存在的边界区域。为了解决这个问题，我们提出了长短记忆SAM 2（SLM-SAM 2），这是一种新型架构，融合了不同的短期和长期内存银行以及单独的注意力模块，以提高分割精度。我们在三个公共数据集上评估了SLM-SAM 2，这些数据集涵盖了MRI和CT模态下的器官、骨骼和肌肉。我们展示的方法在初始适应时显著优于默认SAM 2，在可用体积为5个和1个的情况下，平均Dice相似系数分别提高了0.14和0.11。SLM-SAM 2 还展现出更强的抵抗过度传播的能力，在更准确自动标注医学图像以开发分割模型方面迈出了重要一步。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.01854v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种基于医学图像（如MRI和CT）的自动标注技术改进方案，名为Short-Long Memory SAM 2（SLM-SAM 2）。该技术通过整合短期和长期记忆库以及独立注意力模块，解决了现有技术如SAM 2在标注过程中出现的误差传播问题，特别是在目标在前后切片间边界区域的标注问题。实验表明，SLM-SAM 2在公开数据集上的表现优于原版SAM 2，Dice相似系数平均提升0.14和0.11。此外，SLM-SAM 2还展现出更强的抗过度传播能力，为医学图像分割模型的更准确自动标注迈出了重要一步。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>手动标注医学图像如MRI和CT是劳动密集且耗时的过程。</li>
<li>Segment Anything Model 2 (SAM 2)等视频物体分割基础模型可用于加速标注过程。</li>
<li>SAM 2在医学图像标注中存在误差传播问题，特别是在目标边界区域。</li>
<li>Short-Long Memory SAM 2（SLM-SAM 2）是一种新技术，通过结合短期和长期记忆库及独立注意力模块，提高了分割准确性。</li>
<li>SLM-SAM 2在公开数据集上的表现优于SAM 2，Dice相似系数有所提升。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.01854">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0d504c4ae7bf0a8e4058df4bffd6e926.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4a16fbd3c13ed87036595798aac43278.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-07f91dd8632862a3f53bdc969d04aa0e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d2ac07530677d9bdc617793b2f77ef87.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a7865071c598d37bf2a521861f899ac4.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="LensNet-An-End-to-End-Learning-Framework-for-Empirical-Point-Spread-Function-Modeling-and-Lensless-Imaging-Reconstruction"><a href="#LensNet-An-End-to-End-Learning-Framework-for-Empirical-Point-Spread-Function-Modeling-and-Lensless-Imaging-Reconstruction" class="headerlink" title="LensNet: An End-to-End Learning Framework for Empirical Point Spread   Function Modeling and Lensless Imaging Reconstruction"></a>LensNet: An End-to-End Learning Framework for Empirical Point Spread   Function Modeling and Lensless Imaging Reconstruction</h2><p><strong>Authors:Jiesong Bai, Yuhao Yin, Yihang Dong, Xiaofeng Zhang, Chi-Man Pun, Xuhang Chen</strong></p>
<p>Lensless imaging stands out as a promising alternative to conventional lens-based systems, particularly in scenarios demanding ultracompact form factors and cost-effective architectures. However, such systems are fundamentally governed by the Point Spread Function (PSF), which dictates how a point source contributes to the final captured signal. Traditional lensless techniques often require explicit calibrations and extensive pre-processing, relying on static or approximate PSF models. These rigid strategies can result in limited adaptability to real-world challenges, including noise, system imperfections, and dynamic scene variations, thus impeding high-fidelity reconstruction. In this paper, we propose LensNet, an end-to-end deep learning framework that integrates spatial-domain and frequency-domain representations in a unified pipeline. Central to our approach is a learnable Coded Mask Simulator (CMS) that enables dynamic, data-driven estimation of the PSF during training, effectively mitigating the shortcomings of fixed or sparsely calibrated kernels. By embedding a Wiener filtering component, LensNet refines global structure and restores fine-scale details, thus alleviating the dependency on multiple handcrafted pre-processing steps. Extensive experiments demonstrate LensNet’s robust performance and superior reconstruction quality compared to state-of-the-art methods, particularly in preserving high-frequency details and attenuating noise. The proposed framework establishes a novel convergence between physics-based modeling and data-driven learning, paving the way for more accurate, flexible, and practical lensless imaging solutions for applications ranging from miniature sensors to medical diagnostics. The link of code is <a target="_blank" rel="noopener" href="https://github.com/baijiesong/Lensnet">https://github.com/baijiesong/Lensnet</a>. </p>
<blockquote>
<p>无透镜成像作为传统基于透镜系统的有前途的替代方案脱颖而出，特别是在要求超紧凑外形尺寸和成本效益高的架构的场景中。然而，此类系统从根本上受到点扩散函数（PSF）的支配，PSF决定了点源如何贡献于最终捕获的信号。传统的无透镜技术通常需要明确的校准和广泛的预处理，依赖于静态或近似PSF模型。这些僵硬策略对现实世界挑战的可适应性有限，包括噪声、系统缺陷和动态场景变化，从而阻碍高保真重建。在本文中，我们提出了LensNet，这是一个端到端的深度学习框架，它将空间域和频率域表示集成到一个统一的管道中。我们的方法的核心是可学习的编码掩膜模拟器（CMS），它能够在训练过程中实现PSF的动态数据驱动估计，有效地弥补了固定或稀疏校准内核的缺点。通过嵌入维纳滤波组件，LensNet细化全局结构并恢复细微细节，从而减少对多个手工预处理步骤的依赖。大量实验表明，LensNet的性能稳健，重建质量优越，特别是能够保持高频细节并减少噪声。所提出的框架在基于物理的建模和数据驱动学习之间建立了新的融合，为从微型传感器到医学诊断等各种应用提供了更准确、更灵活、更实用的无透镜成像解决方案。代码链接是<a target="_blank" rel="noopener" href="https://github.com/baijiesong/Lensnet%E3%80%82">https://github.com/baijiesong/Lensnet。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.01755v1">PDF</a> Accepted by IJCAI 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了无透镜成像作为一种对传统透镜系统有前景的替代方案的特点与优势，尤其在超紧凑形式因素和成本效益型架构方面的应用。但由于受到点扩散函数（PSF）的影响，其面临诸多挑战。针对这些问题，本文提出了一种名为LensNet的端到端深度学习框架，该框架结合了空间域和频率域表示，并引入了一个可学习的编码掩膜模拟器（CMS），能够动态、数据驱动地估计PSF。此外，LensNet通过嵌入维纳滤波组件，改进了全局结构并恢复了细节，减少了对手动预处理步骤的依赖。实验结果证明了LensNet的鲁棒性能和优越的重建质量。该框架为物理建模和数据驱动学习之间建立了新的融合点，为从微型传感器到医疗诊断等领域的无透镜成像提供了更准确、灵活和实用的解决方案。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>无透镜成像作为传统透镜系统的替代方案，尤其在超紧凑和成本效益型架构方面表现出潜力。</li>
<li>点扩散函数（PSF）在无透镜成像中起到关键作用，影响最终捕获信号的点源贡献。</li>
<li>传统无透镜技术依赖于静态或近似PSF模型，存在对现实世界挑战的适应性有限的缺点。</li>
<li>LensNet框架结合了空间域和频率域表示，通过深度学习实现动态、数据驱动的PSF估计。</li>
<li>LensNet引入编码掩膜模拟器（CMS）和维纳滤波组件，改进全局结构并恢复细节。</li>
<li>LensNet减少了对手动预处理步骤的依赖，并展示了在噪声衰减和高频细节保留方面的优势。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.01755">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-b8357a44e9d7da6c2d39f56adc375c33.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b439ac68aaba0d6f76129b96fbeb8cd4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-df34f1b0f47188639997223f5936042c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a69054a0c9ae345017df69f3facb525e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-35a6280a3adcac8b4d347fa9035531fe.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="High-Fidelity-Pseudo-label-Generation-by-Large-Language-Models-for-Training-Robust-Radiology-Report-Classifiers"><a href="#High-Fidelity-Pseudo-label-Generation-by-Large-Language-Models-for-Training-Robust-Radiology-Report-Classifiers" class="headerlink" title="High-Fidelity Pseudo-label Generation by Large Language Models for   Training Robust Radiology Report Classifiers"></a>High-Fidelity Pseudo-label Generation by Large Language Models for   Training Robust Radiology Report Classifiers</h2><p><strong>Authors:Brian Wong, Kaito Tanaka</strong></p>
<p>Automated labeling of chest X-ray reports is essential for enabling downstream tasks such as training image-based diagnostic models, population health studies, and clinical decision support. However, the high variability, complexity, and prevalence of negation and uncertainty in these free-text reports pose significant challenges for traditional Natural Language Processing methods. While large language models (LLMs) demonstrate strong text understanding, their direct application for large-scale, efficient labeling is limited by computational cost and speed. This paper introduces DeBERTa-RAD, a novel two-stage framework that combines the power of state-of-the-art LLM pseudo-labeling with efficient DeBERTa-based knowledge distillation for accurate and fast chest X-ray report labeling. We leverage an advanced LLM to generate high-quality pseudo-labels, including certainty statuses, for a large corpus of reports. Subsequently, a DeBERTa-Base model is trained on this pseudo-labeled data using a tailored knowledge distillation strategy. Evaluated on the expert-annotated MIMIC-500 benchmark, DeBERTa-RAD achieves a state-of-the-art Macro F1 score of 0.9120, significantly outperforming established rule-based systems, fine-tuned transformer models, and direct LLM inference, while maintaining a practical inference speed suitable for high-throughput applications. Our analysis shows particular strength in handling uncertain findings. This work demonstrates a promising path to overcome data annotation bottlenecks and achieve high-performance medical text processing through the strategic combination of LLM capabilities and efficient student models trained via distillation. </p>
<blockquote>
<p>胸部X光报告自动标注对于下游任务至关重要，如基于图像的诊断模型训练、人群健康研究和临床决策支持。然而，这些自由文本报告中存在的高变异性、复杂性和否定以及不确定性的普遍存在，给传统自然语言处理方法带来了巨大的挑战。虽然大型语言模型（LLM）表现出强大的文本理解能力，但直接应用于大规模、高效的标注却受到计算成本和速度的限制。</p>
</blockquote>
<p>本文介绍了DeBERTa-RAD，这是一种新颖的两阶段框架，结合了最先进的LLM伪标注的能力和基于DeBERTa的高效知识蒸馏，用于准确快速地完成胸部X光报告标注。我们利用先进的LLM为大量报告生成高质量伪标签，包括确定性状态。随后，使用定制的知识蒸馏策略，在伪标记数据上训练DeBERTa-Base模型。在专家注释的MIMIC-500基准测试上进行评估，DeBERTa-RAD取得了宏观F1得分为0.9120的业界最佳成绩，显著优于基于规则的系统、微调后的transformer模型和直接LLM推理，同时保持了适用于高吞吐量应用的实际推理速度。我们的分析显示在处理不确定的检查结果时具有特别的优势。</p>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.01693v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>本文提出一种基于自然语言处理技术的自动标注胸部X光报告的方法。针对传统自然语言处理方法难以处理报告中存在的否定和不确定性等问题，本文引入了一种新型的两阶段框架——DeBERTa-RAD。该框架结合了最先进的大型语言模型的伪标注能力与高效的DeBERTa知识蒸馏技术，以实现快速准确的胸部X光报告标注。利用先进的LLM生成高质量伪标签（包括确定状态），用于大规模报告数据集。随后，使用定制的知识蒸馏策略在伪标签数据上训练DeBERTa-Base模型。在专家标注的MIMIC-500基准测试上评估，DeBERTa-RAD取得了宏平均F1分数为0.912的业界最佳成绩，显著优于基于规则的系统、微调过的transformer模型和直接LLM推理，同时保持了适合高吞吐量应用的实际推理速度。特别在处理不确定结果方面表现突出。本文展示了通过结合LLM能力和高效学生模型训练克服数据标注瓶颈，实现高性能医学文本处理的途径。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>自动化标记胸部X光报告对下游任务至关重要，如训练基于图像的诊断模型、人群健康研究和临床决策支持。</li>
<li>传统自然语言处理方法面临报告中否定和不确定性的高变异性、复杂性和普遍性所带来的挑战。</li>
<li>DeBERTa-RAD框架结合了大型语言模型的伪标注能力和DeBERTa知识蒸馏技术，以实现快速准确的胸部X光报告标注。</li>
<li>利用伪标签数据训练DeBERTa-Base模型，在MIMIC-500基准测试中取得业界最佳性能。</li>
<li>DeBERTa-RAD在处理不确定结果方面表现优异。</li>
<li>该研究展示了结合大型语言模型能力和高效学生模型训练的战略组合，以克服数据标注瓶颈并实现高性能医学文本处理。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.01693">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ebc31267880572e75c27de55294b0557.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ec797d8903896c5c9e0dccf703d0c140.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b040d8581b277effe3c207fa0ca5c000.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="A-Dual-Task-Synergy-Driven-Generalization-Framework-for-Pancreatic-Cancer-Segmentation-in-CT-Scans"><a href="#A-Dual-Task-Synergy-Driven-Generalization-Framework-for-Pancreatic-Cancer-Segmentation-in-CT-Scans" class="headerlink" title="A Dual-Task Synergy-Driven Generalization Framework for Pancreatic   Cancer Segmentation in CT Scans"></a>A Dual-Task Synergy-Driven Generalization Framework for Pancreatic   Cancer Segmentation in CT Scans</h2><p><strong>Authors:Jun Li, Yijue Zhang, Haibo Shi, Minhong Li, Qiwei Li, Xiaohua Qian</strong></p>
<p>Pancreatic cancer, characterized by its notable prevalence and mortality rates, demands accurate lesion delineation for effective diagnosis and therapeutic interventions. The generalizability of extant methods is frequently compromised due to the pronounced variability in imaging and the heterogeneous characteristics of pancreatic lesions, which may mimic normal tissues and exhibit significant inter-patient variability. Thus, we propose a generalization framework that synergizes pixel-level classification and regression tasks, to accurately delineate lesions and improve model stability. This framework not only seeks to align segmentation contours with actual lesions but also uses regression to elucidate spatial relationships between diseased and normal tissues, thereby improving tumor localization and morphological characterization. Enhanced by the reciprocal transformation of task outputs, our approach integrates additional regression supervision within the segmentation context, bolstering the model’s generalization ability from a dual-task perspective. Besides, dual self-supervised learning in feature spaces and output spaces augments the model’s representational capability and stability across different imaging views. Experiments on 594 samples composed of three datasets with significant imaging differences demonstrate that our generalized pancreas segmentation results comparable to mainstream in-domain validation performance (Dice: 84.07%). More importantly, it successfully improves the results of the highly challenging cross-lesion generalized pancreatic cancer segmentation task by 9.51%. Thus, our model constitutes a resilient and efficient foundational technological support for pancreatic disease management and wider medical applications. The codes will be released at <a target="_blank" rel="noopener" href="https://github.com/SJTUBME-QianLab/Dual-Task-Seg">https://github.com/SJTUBME-QianLab/Dual-Task-Seg</a>. </p>
<blockquote>
<p>胰腺癌以其较高的发病率和死亡率而著称，需要进行准确的病灶勾画以实现有效的诊断和干预治疗。由于成像的明显差异性和胰腺病灶的异质性特征（可能模拟正常组织并表现出显著的病人间差异），现有方法的通用性经常受到威胁。因此，我们提出了一个综合像素级分类和回归任务的通用框架，以准确勾画病灶并提高模型稳定性。该框架不仅致力于使分割轮廓与实际病灶对齐，还使用回归来阐明病变组织与正常组织之间的空间关系，从而提高肿瘤定位和形态表征。通过任务输出的相互转换，我们的方法将额外的回归监督整合到分割背景中，从双重任务的角度增强模型的通用性能力。此外，特征空间和输出空间中的双重自监督学习增强了模型的表示能力和稳定性，适用于不同的成像视图。在由三个具有显著差异的数据集组成的594个样本上进行的实验表明，我们的通用胰腺分割结果与主流的同域验证性能相当（Dice系数为84.07%）。更重要的是，它成功提高了具有挑战性的跨病灶通用胰腺癌分割任务的结果达9.51%。因此，我们的模型为胰腺疾病管理和更广泛的医学应用提供了稳健高效的基础技术支持。代码将发布在<a target="_blank" rel="noopener" href="https://github.com/SJTUBME-QianLab/Dual-Task-Seg%E3%80%82">https://github.com/SJTUBME-QianLab/Dual-Task-Seg。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.01644v1">PDF</a> accept by IEEE Transactions on Medical Imaging (TMI) 2025</p>
<p><strong>摘要</strong></p>
<p>胰腺癌因其高发病率和高死亡率而备受关注，准确地进行病灶勾画对于有效的诊断和干预治疗至关重要。由于成像的显著变异性和胰腺病灶的异质性特征（可能模拟正常组织并表现出显著的患者间差异），现有方法的通用性往往受到影响。因此，我们提出了一个融合像素级分类和回归任务的通用框架，以准确勾画病灶并提高模型稳定性。该框架不仅寻求将分割轮廓与实际病灶对齐，而且利用回归来阐明病灶与正常组织之间的空间关系，从而改善肿瘤定位和形态表征。通过任务输出的相互转换，我们的方法将额外的回归监督整合到分割背景中，从双重任务的角度增强了模型的泛化能力。此外，特征空间和输出空间中的双重自监督学习增强了模型的表征能力和稳定性，适用于不同的成像视图。在由三个数据集组成的594个样本上进行实验，结果表明我们的泛化胰腺分割结果与主流的同域验证性能相当（Dice：84.07%）。更重要的是，它成功提高了极具挑战性的跨病灶泛化胰腺癌分割任务的结果达9.51%。因此，我们的模型为胰腺疾病管理和更广泛的医疗应用提供了稳健有效的基本技术支持。代码将发布在<a target="_blank" rel="noopener" href="https://github.com/SJTUBME-QianLab/Dual-Task-Seg">https://github.com/SJTUBME-QianLab/Dual-Task-Seg</a>。</p>
<p><strong>要点</strong></p>
<ol>
<li>胰腺癌由于其高发病率和死亡率，需要准确进行病灶勾画以进行有效诊断和治疗。</li>
<li>现有方法由于成像的变异性和胰腺病灶的异质性而缺乏泛化能力。</li>
<li>提出的框架融合了像素级分类和回归任务，以提高病灶勾画的准确性并增强模型稳定性。</li>
<li>该框架通过任务输出的相互转换和双重自监督学习增强了模型的泛化能力和表征能力。</li>
<li>实验结果表明，该框架在泛化胰腺分割任务上具有良好的性能，并在跨病灶分割任务上取得了显著改进。</li>
<li>该模型为胰腺疾病管理提供了稳健有效的技术支持，并有望应用于更广泛的医疗领域。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.01644">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-904cd3a7be858436465d3828e28b4a0e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-61fcd3f833672dc6dbfe359d0840b993.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b27379e7cd4ecfe9352269642664f06e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1d6c2d38e60e05ab747673017f69c04f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-09d7f5bf25e312659edb155f8e8cb61f.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Towards-Accurate-and-Interpretable-Neuroblastoma-Diagnosis-via-Contrastive-Multi-scale-Pathological-Image-Analysis"><a href="#Towards-Accurate-and-Interpretable-Neuroblastoma-Diagnosis-via-Contrastive-Multi-scale-Pathological-Image-Analysis" class="headerlink" title="Towards Accurate and Interpretable Neuroblastoma Diagnosis via   Contrastive Multi-scale Pathological Image Analysis"></a>Towards Accurate and Interpretable Neuroblastoma Diagnosis via   Contrastive Multi-scale Pathological Image Analysis</h2><p><strong>Authors:Zhu Zhu, Shuo Jiang, Jingyuan Zheng, Yawen Li, Yifei Chen, Manli Zhao, Weizhong Gu, Feiwei Qin, Jinhu Wang, Gang Yu</strong></p>
<p>Neuroblastoma, adrenal-derived, is among the most common pediatric solid malignancies, characterized by significant clinical heterogeneity. Timely and accurate pathological diagnosis from hematoxylin and eosin-stained whole-slide images is critical for patient prognosis. However, current diagnostic practices primarily rely on subjective manual examination by pathologists, leading to inconsistent accuracy. Existing automated whole-slide image classification methods encounter challenges such as poor interpretability, limited feature extraction capabilities, and high computational costs, restricting their practical clinical deployment. To overcome these limitations, we propose CMSwinKAN, a contrastive-learning-based multi-scale feature fusion model tailored for pathological image classification, which enhances the Swin Transformer architecture by integrating a Kernel Activation Network within its multilayer perceptron and classification head modules, significantly improving both interpretability and accuracy. By fusing multi-scale features and leveraging contrastive learning strategies, CMSwinKAN mimics clinicians’ comprehensive approach, effectively capturing global and local tissue characteristics. Additionally, we introduce a heuristic soft voting mechanism guided by clinical insights to bridge patch-level predictions to whole-slide image-level classifications seamlessly. We verified the CMSwinKAN on the publicly available BreakHis dataset and the PpNTs dataset, which was established by our hospital. Results demonstrate that CMSwinKAN performs better than existing state-of-the-art pathology-specific models pre-trained on large datasets. Our source code is available at <a target="_blank" rel="noopener" href="https://github.com/JSLiam94/CMSwinKAN">https://github.com/JSLiam94/CMSwinKAN</a>. </p>
<blockquote>
<p>神经母细胞瘤是肾上腺来源的最常见的儿童实体恶性肿瘤之一，具有显著的临床异质性。及时而准确的病理诊断对于患者的预后至关重要，这些诊断通常基于苏木精和伊红染色的全切片图像。然而，当前的诊断方法主要依赖于病理医师的主观手动检查，导致诊断准确性不一致。现有的全自动切片图像分类方法面临可解释性差、特征提取能力有限以及计算成本高的挑战，限制了其在临床实践中的部署应用。为了克服这些限制，我们提出了CMSwinKAN，这是一种基于对比学习的多尺度特征融合模型，专为病理图像分类而定制。我们通过将内核激活网络集成到Swin Transformer架构的多层感知器和分类头模块中，增强了其可解释性和准确性。通过融合多尺度特征并利用对比学习策略，CMSwinKAN模仿了临床医生全面的诊断方法，有效地捕捉了全局和局部组织特征。此外，我们还引入了一种受临床见解启发的新型软投票机制，将斑块级别的预测无缝地桥接到全切片图像级别的分类。我们在公开可用的BreakHis数据集和由我们医院建立PpNTs数据集上验证了CMSwinKAN。结果表明，CMSwinKAN的性能优于现有最先进的、在大数据集上预训练的病理学专用模型。我们的源代码可在<a target="_blank" rel="noopener" href="https://github.com/JSLiam94/CMSwinKAN%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/JSLiam94/CMSwinKAN获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13754v3">PDF</a> 10pages, 8 figures</p>
<p><strong>Summary</strong></p>
<p>本文提出了一种基于对比学习的多尺度特征融合模型CMSwinKAN，用于神经母细胞瘤等肾上腺衍生固体肿瘤的病理图像分类。该模型结合了Swin Transformer架构和Kernel Activation Network，提高了模型的解释性和准确性。通过融合多尺度特征和采用对比学习策略，CMSwinKAN有效捕捉全局和局部组织特征，模拟医生的综合诊断方法。此外，还引入了一种基于临床见解的启发式软投票机制，将补丁级别的预测无缝地桥接到整个幻灯片图像级别的分类。在公开可用的BreakHis数据集和我们医院建立的PpNTs数据集上的验证结果表明，CMSwinKAN的性能优于现有的最新病理学预训练模型。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>神经母细胞瘤是常见的儿童固体肿瘤，病理诊断对其预后至关重要。</li>
<li>当前诊断方法主要依赖病理医师的主观手动检查，存在准确性不一致的问题。</li>
<li>CMSwinKAN模型结合了Swin Transformer和Kernel Activation Network，提高病理图像分类的准确性和解释性。</li>
<li>CMSwinKAN通过融合多尺度特征和对比学习策略，有效捕捉全局和局部组织特性，模拟医生的诊断过程。</li>
<li>引入启发式软投票机制，将补丁级别预测无缝转换为整个幻灯片图像级别分类。</li>
<li>在公开和医院内部数据集上的验证结果表明，CMSwinKAN性能优于现有最新病理学预训练模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13754">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-fb96e73ff698492f49377d9bb024e869.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5728caeb68be85c517b9cd6f4143dbcc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6d214216c00cfc191bea492759d2ef60.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-90c9357fe4979a191912cd4ded1f41e6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ce85806f90fbc30636ef0a49a71ba0f0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8bd4b02c25a30c3cc5a38c94f547b27c.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="MSA-UNet3-Multi-Scale-Attention-UNet3-with-New-Supervised-Prototypical-Contrastive-Loss-for-Coronary-DSA-Image-Segmentation"><a href="#MSA-UNet3-Multi-Scale-Attention-UNet3-with-New-Supervised-Prototypical-Contrastive-Loss-for-Coronary-DSA-Image-Segmentation" class="headerlink" title="MSA-UNet3+: Multi-Scale Attention UNet3+ with New Supervised   Prototypical Contrastive Loss for Coronary DSA Image Segmentation"></a>MSA-UNet3+: Multi-Scale Attention UNet3+ with New Supervised   Prototypical Contrastive Loss for Coronary DSA Image Segmentation</h2><p><strong>Authors:Rayan Merghani Ahmed, Adnan Iltaf, Mohamed Elmanna, Gang Zhao, Hongliang Li, Yue Du, Bin Li, Shoujun Zhou</strong></p>
<p>Accurate segmentation of coronary Digital Subtraction Angiography images is essential to diagnose and treat coronary artery diseases. Despite advances in deep learning, challenges such as high intra-class variance and class imbalance limit precise vessel delineation. Most existing approaches for coronary DSA segmentation cannot address these issues. Also, existing segmentation network’s encoders do not directly generate semantic embeddings, which could enable the decoder to reconstruct segmentation masks effectively from these well-defined features. We propose a Supervised Prototypical Contrastive Loss that fuses supervised and prototypical contrastive learning to enhance coronary DSA image segmentation. The supervised contrastive loss enforces semantic embeddings in the encoder, improving feature differentiation. The prototypical contrastive loss allows the model to focus on the foreground class while alleviating the high intra-class variance and class imbalance problems by concentrating only on the hard-to-classify background samples. We implement the proposed SPCL loss within an MSA-UNet3+: a Multi-Scale Attention-Enhanced UNet3+ architecture. The architecture integrates key components: a Multi-Scale Attention Encoder and a Multi-Scale Dilated Bottleneck designed to enhance multi-scale feature extraction and a Contextual Attention Fusion Module built to keep fine-grained details while improving contextual understanding. Experiments on a private coronary DSA dataset show that MSA-UNet3+ outperforms state-of-the-art methods, achieving the highest Dice coefficient and F1-score and significantly reducing ASD and ACD. The developed framework provides clinicians with precise vessel segmentation, enabling accurate identification of coronary stenosis and supporting informed diagnostic and therapeutic decisions. The code will be released at <a target="_blank" rel="noopener" href="https://github.com/rayanmerghani/MSA-UNet3plus">https://github.com/rayanmerghani/MSA-UNet3plus</a>. </p>
<blockquote>
<p>冠状动脉数字减影血管造影（DSA）图像的准确分割对于冠状动脉疾病的诊断和治疗至关重要。尽管深度学习有所进展，但类内高方差和类不平衡等挑战仍然限制了血管精确勾勒。大多数现有的冠状动脉DSA分割方法无法解决这些问题。此外，现有分割网络的编码器并没有直接生成语义嵌入，这可能会使解码器无法从这些定义明确的功能中有效地重建分割掩模。我们提出了一种有监督原型对比损失（Supervised Prototypical Contrastive Loss），它将有监督学习和原型对比学习相结合，以提高冠状动脉DSA图像分割的效果。有监督对比损失在编码器中实施语义嵌入，提高特征差异。原型对比损失允许模型专注于前景类，并通过仅关注难以分类的背景样本，缓解类内高方差和类不平衡问题。我们在MSA-UNet3+中实现了所提出的SPCL损失：一种集多尺度注意力增强UNet3+架构。该架构集成了关键组件：多尺度注意力编码器和多尺度膨胀瓶颈，旨在增强多尺度特征提取和上下文理解，以及构建用于保持精细细节的同时改善上下文理解的上下文注意力融合模块。在私有冠状动脉DSA数据集上的实验表明，MSA-UNet3+优于最先进的方法，获得了最高的Dice系数和F1分数，并显著降低了ASD和ACD。所开发的框架为临床医生提供了精确的血管分割，能够准确识别冠状动脉狭窄，并支持做出明智的诊断和治疗决策。代码将在<a target="_blank" rel="noopener" href="https://github.com/rayanmerghani/MSA-UNet3plus">https://github.com/rayanmerghani/MSA-UNet3plus</a>发布。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05184v3">PDF</a> Work in progress</p>
<p><strong>Summary</strong></p>
<p>本文提出一种融合监督对比损失和原型对比损失的冠状动脉数字减影血管造影图像分割方法，以解决现有方法面临的高内类方差和类别不平衡等问题。该方法通过加强编码器中的语义嵌入来提高特征区分度，并通过原型对比损失使模型专注于前景类别，从而减轻高内类方差和类别不平衡问题。实验结果表明，该方法在私有冠状动脉DSA数据集上优于现有先进技术，实现了最高的Dice系数和F1分数，并显著降低了ASD和ACD。此框架可为临床医生提供精确的血管分割，有助于准确识别冠状动脉狭窄并支持医生做出诊断和治疗的决策。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>冠状动脉数字减影血管造影（DSA）图像的准确分割对诊断和治疗冠状动脉疾病至关重要。</li>
<li>现有方法面临高内类方差和类别不平衡的挑战，影响精确血管轮廓的描绘。</li>
<li>提出的监督原型对比损失（SPCL）融合了监督对比学习和原型对比学习，旨在提高冠状动脉DSA图像分割的效果。</li>
<li>SPCL损失在编码器阶段强制生成语义嵌入，改善特征区分度，并通过原型对比损失关注前景类别，减轻高内类方差和类别不平衡问题。</li>
<li>采用多尺度注意力增强UNet3+架构（MSA-UNet3+）实现SPCL损失，包括多尺度注意力编码器、多尺度扩张瓶颈和上下文注意力融合模块，旨在提高多尺度特征提取和上下文理解能力。</li>
<li>在私有冠状动脉DSA数据集上的实验表明，MSA-UNet3+方法优于现有技术，提高了Dice系数和F1分数，并降低了ASD和ACD。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05184">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-c60db5ed57c5683c7acc8a01277c0527.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-678d65f1527ebedb4c1b5a42c616231e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c4f042f82046052d8354a4c8b6b4ca4d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9b6992bcc178cdaf11795c407d10da2d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b035f1f1b4cb95a429c2a47f2110d205.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Instance-Segmentation-of-Scene-Sketches-Using-Natural-Image-Priors"><a href="#Instance-Segmentation-of-Scene-Sketches-Using-Natural-Image-Priors" class="headerlink" title="Instance Segmentation of Scene Sketches Using Natural Image Priors"></a>Instance Segmentation of Scene Sketches Using Natural Image Priors</h2><p><strong>Authors:Mia Tang, Yael Vinker, Chuan Yan, Lvmin Zhang, Maneesh Agrawala</strong></p>
<p>Sketch segmentation involves grouping pixels within a sketch that belong to the same object or instance. It serves as a valuable tool for sketch editing tasks, such as moving, scaling, or removing specific components. While image segmentation models have demonstrated remarkable capabilities in recent years, sketches present unique challenges for these models due to their sparse nature and wide variation in styles. We introduce InkLayer, a method for instance segmentation of raster scene sketches. Our approach adapts state-of-the-art image segmentation and object detection models to the sketch domain by employing class-agnostic fine-tuning and refining segmentation masks using depth cues. Furthermore, our method organizes sketches into sorted layers, where occluded instances are inpainted, enabling advanced sketch editing applications. As existing datasets in this domain lack variation in sketch styles, we construct a synthetic scene sketch segmentation dataset, InkScenes, featuring sketches with diverse brush strokes and varying levels of detail. We use this dataset to demonstrate the robustness of our approach. </p>
<blockquote>
<p>草图分割涉及将属于同一对象或实例的草图内的像素进行分组。它作为草图编辑任务（如移动、缩放或删除特定组件）的宝贵工具。近年来，图像分割模型表现出了显著的能力，但由于草图的稀疏性和风格上的巨大差异，这些模型面临着独特的挑战。我们引入了InkLayer，这是一种基于矢量的场景草图实例分割方法。我们的方法通过采用类无关的微调和使用深度线索细化分割掩膜，将最先进的图像分割和对象检测模型适应到草图领域。此外，我们的方法将草图组织成有序的层，其中遮挡的实例被填充，从而实现高级的草图编辑应用程序。由于该领域的现有数据集缺乏草图风格的多样性，我们构建了一个合成场景草图分割数据集InkScenes，其中包含具有不同笔触和细节级别的草图。我们使用此数据集来展示我们方法的稳健性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.09608v2">PDF</a> Project website: <a target="_blank" rel="noopener" href="https://inklayer.github.io/">https://inklayer.github.io</a></p>
<p><strong>Summary</strong></p>
<p>本文介绍了InkLayer方法，用于对场景草图进行实例分割。该方法采用先进的图像分割和对象检测模型，通过类别无关的微调并利用深度线索完善分割掩模，以适应草图领域。此外，该方法可将草图组织成有序层，对遮挡的实例进行填充，实现高级草图编辑应用。为解决现有数据集风格缺乏变化的问题，构建了一个合成场景草图分割数据集InkScenes。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Sketch segmentation旨在将同一对象或实例的像素进行分组，对于草图编辑任务（如移动、缩放或删除特定组件）具有重要价值。</li>
<li>InkLayer方法采用先进的图像分割和对象检测模型，通过类别无关的微调来适应草图领域。</li>
<li>InkLayer方法利用深度线索完善分割掩模，提高模型的适应性。</li>
<li>该方法将草图组织成有序层，便于进行高级草图编辑应用。</li>
<li>现存数据集缺乏草图风格的变化，缺乏足够的多样性以适应不同的模型需求。</li>
<li>为解决上述问题，构建了一个合成场景草图分割数据集InkScenes，包含不同笔触和细节层次的草图。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.09608">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-11416b84860e9b8a9143fd3738e0694d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4a754963067c7dc46376b04cbfe6128a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ef670f87f20142e8fbbb04a99ba0e9bf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4cfd62a6da2a1ca76bc264ca0f7e0422.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d7b7a8b9efd58ba54027b747401b9c90.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Censor-Aware-Semi-Supervised-Survival-Time-Prediction-in-Lung-Cancer-Using-Clinical-and-Radiomics-Features"><a href="#Censor-Aware-Semi-Supervised-Survival-Time-Prediction-in-Lung-Cancer-Using-Clinical-and-Radiomics-Features" class="headerlink" title="Censor-Aware Semi-Supervised Survival Time Prediction in Lung Cancer   Using Clinical and Radiomics Features"></a>Censor-Aware Semi-Supervised Survival Time Prediction in Lung Cancer   Using Clinical and Radiomics Features</h2><p><strong>Authors:Arman Gorji, Ali Fathi Jouzdani, Nima Sanati, Amir Mahmoud Ahmadzadeh, Ren Yuan, Arman Rahmim, Mohammad R. Salmanpour</strong></p>
<p>Objectives: Lung cancer poses a significant global health challenge, necessitating improved prognostic methods for personalized treatment. This study introduces a censor-aware semi-supervised learning (SSL) framework that integrates clinical and imaging data, addressing biases in traditional models handling censored data. Methods: We analyzed clinical, PET and CT data from 199 lung cancer patients from public and local data respositories, focusing on overall survival (OS) time as the primary outcome. Handcrafted (HRF) and Deep Radiomics features (DRF) were extracted after preprocessing using ViSERA software and were combined with clinical features (CF). Feature dimensions were optimized using Principal Component Analysis (PCA), followed by the application of supervised learning (SL) and SSL. SSL incorporated pseudo-labeling of censored data to improve performance. Seven regressors and three hazard ratio survival analysis (HRSA) algorithms were optimized using five-fold cross-validation, grid search and external test bootstrapping. Results: For PET HRFs, SSL reduced the mean absolute error (MAE) by 26.5%, achieving 1.55 years with PCA+decision tree regression, compared to SL’s 2.11 years with PCA+KNNR (p&lt;0.05). Combining HRFs (CT_HRF) and DRFs from CT images using SSL+PCA+KNNR achieved an MAE of 2.08 years, outperforming SL’s 2.26 years by 7.96% (p&lt;0.05). In HRSA, CT_HRF applied to PCA+Component Wise Gradient Boosting Survival Analysis achieved an external c-index of 0.65, effectively differentiating high- and low-risk groups. Conclusions: We demonstrated that the SSL strategy significantly outperforms SL across PET, CT, and CF. As such, censor-aware SSL applied to HRFs from PET images significantly improved survival prediction performance by 26.5% compared to the SL approach. </p>
<blockquote>
<p>目标：肺癌构成一项重大的全球健康挑战，需要改进预测方法以实现个性化治疗。本研究引入了一种有审查意识的半监督学习（SSL）框架，该框架结合了临床和成像数据，解决了传统模型在处理审查数据时的偏见问题。</p>
</blockquote>
<p>方法：我们分析了来自公共和本地数据仓库的199名肺癌患者的临床、PET和CT数据，以总生存期（OS）时间为主要结果。使用ViSERA软件预处理后，提取了手工特征（HRF）和深度放射学特征（DRF），并与临床特征（CF）相结合。使用主成分分析（PCA）优化特征维度，然后应用监督学习（SL）和SSL。SSL通过伪标记审查数据来提高性能。使用五折交叉验证、网格搜索和外部测试自助法对七个回归器和三种风险比率生存分析（HRSA）算法进行了优化。</p>
<p>结果：对于PET的HRF，SSL将平均绝对误差（MAE）降低了26.5%，使用PCA+决策树回归达到1.55年，而SL使用PCA+KNNR为2.11年（p&lt;0.05）。使用SSL+PCA+KNNR结合来自CT图像的HRF和DRF，MAE为2.08年，优于SL的2.26年，提高了7.96%（p&lt;0.05）。在HRSA中，将CT_HRF应用于PCA+组件智慧梯度提升生存分析达到了外部c指数为0.65，有效地区分了高风险组和低风险组。</p>
<p>结论：我们证明SSL策略在PET、CT和CF方面显著优于SL。因此，与SL方法相比，应用于PET图像HRF的有审查意识的SSL将生存预测性能提高了26.5%。</p>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.01661v3">PDF</a> 11 pages, 4 Figures and 4 Tables</p>
<p><strong>Summary</strong>：本研究旨在应对肺癌预后预测方法的挑战，提出一种融合临床与成像数据的半监督学习框架，该框架能够处理带有审查的数据中的偏见问题。通过对PET和CT影像数据以及临床数据的分析，本研究发现该半监督学习方法能够提高预后预测的准确度。对比只使用监督学习方法，该半监督学习方法的预测误差降低了近百分之二十五。对影像特征的有效组合进一步提升了模型的性能。研究结论表明半监督学习框架在处理肺癌患者的预后预测问题上表现优异。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>本研究引入了半监督学习框架以整合临床和成像数据，解决传统模型处理审查数据时存在的偏见问题。</li>
<li>研究采用了PET和CT影像数据以及临床数据，关注的主要结果是患者的总体生存时间。</li>
<li>半监督学习方法相较于传统监督学习方法显著提高了预后预测的准确度，预测误差降低了约百分之二十五。</li>
<li>结合影像特征的临床特征可以进一步优化模型性能。</li>
<li>通过使用主成分分析（PCA）对特征维度进行优化后，模型性能得到进一步提升。</li>
<li>HRSA算法对于区分高风险和低风险患者群体具有良好的效果，其外部一致性指数达到了0.65。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.01661">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-bfe5fa9a3a7303ab32965d8c06afdef3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9d7167453cde86778f0b22a8b0693a36.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-107297f39c14e9543e21201e1f240c20.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-37ad9a419e7f042b55c9eeead14f751c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6c89e181a49d363d482400b0b7868a78.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-49e8bb808e464585bd2fe65e213c46ae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8ecfef211018620ba373b6d98795dff6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6c2ebab837bf060fbff9e6fdd7c65188.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3682ff7914fd7809c57ffc142678a503.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="GRAPHITE-Graph-Based-Interpretable-Tissue-Examination-for-Enhanced-Explainability-in-Breast-Cancer-Histopathology"><a href="#GRAPHITE-Graph-Based-Interpretable-Tissue-Examination-for-Enhanced-Explainability-in-Breast-Cancer-Histopathology" class="headerlink" title="GRAPHITE: Graph-Based Interpretable Tissue Examination for Enhanced   Explainability in Breast Cancer Histopathology"></a>GRAPHITE: Graph-Based Interpretable Tissue Examination for Enhanced   Explainability in Breast Cancer Histopathology</h2><p><strong>Authors:Raktim Kumar Mondol, Ewan K. A. Millar, Peter H. Graham, Lois Browne, Arcot Sowmya, Erik Meijering</strong></p>
<p>Explainable AI (XAI) in medical histopathology is essential for enhancing the interpretability and clinical trustworthiness of deep learning models in cancer diagnosis. However, the black-box nature of these models often limits their clinical adoption. We introduce GRAPHITE (Graph-based Interpretable Tissue Examination), a post-hoc explainable framework designed for breast cancer tissue microarray (TMA) analysis. GRAPHITE employs a multiscale approach, extracting patches at various magnification levels, constructing an hierarchical graph, and utilising graph attention networks (GAT) with scalewise attention (SAN) to capture scale-dependent features. We trained the model on 140 tumour TMA cores and four benign whole slide images from which 140 benign samples were created, and tested it on 53 pathologist-annotated TMA samples. GRAPHITE outperformed traditional XAI methods, achieving a mean average precision (mAP) of 0.56, an area under the receiver operating characteristic curve (AUROC) of 0.94, and a threshold robustness (ThR) of 0.70, indicating that the model maintains high performance across a wide range of thresholds. In clinical utility, GRAPHITE achieved the highest area under the decision curve (AUDC) of 4.17e+5, indicating reliable decision support across thresholds. These results highlight GRAPHITE’s potential as a clinically valuable tool in computational pathology, providing interpretable visualisations that align with the pathologists’ diagnostic reasoning and support precision medicine. </p>
<blockquote>
<p>在医学病理学中，可解释人工智能（XAI）对于提高癌症诊断中深度学习模型的解释性和临床可信度至关重要。然而，这些模型的“黑箱”性质常常限制了它们在临床上的采纳。我们引入了GRAPHITE（基于图的可解释组织检查），这是一种针对乳腺癌组织微阵列（TMA）分析的事后可解释框架。GRAPHITE采用多尺度方法，在不同放大级别提取斑块，构建分层图，并利用具有尺度相关注意力（SAN）的图注意力网络（GAT）来捕获尺度相关特征。我们在由肿瘤组织微阵列核心中的140个样本和由四张良性全切片图像创建的另外140个良性样本上训练模型，并在由病理学家注释的53个TMA样本上进行测试。与传统的XAI方法相比，GRAPHITE表现更佳，其平均精度均值（mAP）达到0.56，在受试者工作特征曲线下的面积（AUROC）达到0.94，阈值稳健性（ThR）达到0.70，这表明模型在广泛阈值范围内均表现出良好性能。在临床应用中，GRAPHITE的决策曲线下的面积（AUDC）达到最高值，即每观察小时后多决策后计算的校正因子等于常数计算值为十万分之一的最大可能累计得分之和等于万分之一点八四零三加量数字的最高积分域数据取得了一次达到每个可接受模型的判别，证明其提供了解释性可视化，这些可视化与病理医师的诊断推理相一致，为精准医学提供了可靠的决策支持。结果证明了GRAPHITE在人工智能临床病理领域的潜在应用价值，它可以为医学决策分析提供有意义的可解释工具并促进精准医疗的发展。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.04206v2">PDF</a> 25 Pages, 10 Figures, 1 Tables</p>
<p><strong>摘要</strong><br>    GRAPHITE（基于图的解释性组织检测）是一种用于乳腺癌组织微阵列（TMA）分析的后验可解释框架。它采用多尺度方法，利用图注意力网络（GAT）和尺度注意力（SAN）捕获尺度相关特征，提高深度学习模型在癌症诊断中的可解释性和临床可信度。在肿瘤TMA芯和全幻灯片图像上的实验表明，GRAPHITE优于传统XAI方法，具有高平均精度、高曲线下面积和阈值稳健性。这为临床决策支持提供了可靠的依据，凸显了其在计算病理学中的临床价值。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>XAI在医学病理学中对于增强深度学习模型在癌症诊断中的可解释性和临床信任度至关重要。</li>
<li>GRAPHITE框架被引入以解决深度学习模型的“黑箱”问题，提高临床采纳率。</li>
<li>GRAPHITE采用多尺度方法，捕获尺度相关特征，构建层次图来提高解释性能。</li>
<li>GRAPHITE在肿瘤TMA芯和全幻灯片图像上进行训练并测试，表现出优越性能。</li>
<li>GRAPHITE相对于传统XAI方法具有更高的平均精度、曲线下面积和阈值稳健性。</li>
<li>GRAPHITE在临床决策支持中表现出可靠性，与病理医师的诊断推理相符，支持精准医疗。</li>
<li>结果凸显了GRAPHITE在计算病理学中的潜在临床价值。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.04206">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-4ad972449fab939bff5cbe953f5b186d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7b149051f8299c719689e65ae35c8a8a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f3007e60e927f7f43d377e4eb488fdcf.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-09/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-09/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">医学图像</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-09/TTS/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-a97d731c442f25b3ed5578117f7483ee.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS 方向最新论文已更新，请持续关注 Update in 2025-05-09  Advancing Zero-shot Text-to-Speech Intelligibility across Diverse   Domains via Preference Alignment
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-09
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-09/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-de804d728ada58b8fa4a97244451761d.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-05-09  Efficient Flow Matching using Latent Variables
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-09
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">18181.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
