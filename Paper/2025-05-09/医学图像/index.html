<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-09  Active Sampling for MRI-based Sequential Decision Making">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-aac96b3ebfbae988da12a816938a33ca.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-09
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    22.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    91 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-09-æ›´æ–°"><a href="#2025-05-09-æ›´æ–°" class="headerlink" title="2025-05-09 æ›´æ–°"></a>2025-05-09 æ›´æ–°</h1><h2 id="Active-Sampling-for-MRI-based-Sequential-Decision-Making"><a href="#Active-Sampling-for-MRI-based-Sequential-Decision-Making" class="headerlink" title="Active Sampling for MRI-based Sequential Decision Making"></a>Active Sampling for MRI-based Sequential Decision Making</h2><p><strong>Authors:Yuning Du, Jingshuai Liu, Rohan Dharmakumar, Sotirios A. Tsaftaris</strong></p>
<p>Despite the superior diagnostic capability of Magnetic Resonance Imaging (MRI), its use as a Point-of-Care (PoC) device remains limited by high cost and complexity. To enable such a future by reducing the magnetic field strength, one key approach will be to improve sampling strategies. Previous work has shown that it is possible to make diagnostic decisions directly from k-space with fewer samples. Such work shows that single diagnostic decisions can be made, but if we aspire to see MRI as a true PoC, multiple and sequential decisions are necessary while minimizing the number of samples acquired. We present a novel multi-objective reinforcement learning framework enabling comprehensive, sequential, diagnostic evaluation from undersampled k-space data. Our approach during inference actively adapts to sequential decisions to optimally sample. To achieve this, we introduce a training methodology that identifies the samples that contribute the best to each diagnostic objective using a step-wise weighting reward function. We evaluate our approach in two sequential knee pathology assessment tasks: ACL sprain detection and cartilage thickness loss assessment. Our framework achieves diagnostic performance competitive with various policy-based benchmarks on disease detection, severity quantification, and overall sequential diagnosis, while substantially saving k-space samples. Our approach paves the way for the future of MRI as a comprehensive and affordable PoC device. Our code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/vios-s/MRI_Sequential_Active_Sampling">https://github.com/vios-s/MRI_Sequential_Active_Sampling</a> </p>
<blockquote>
<p>å°½ç®¡ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰åœ¨è¯Šæ–­æ–¹é¢å…·æœ‰å“è¶Šçš„èƒ½åŠ›ï¼Œä½†ç”±äºå…¶é«˜æ˜‚çš„æˆæœ¬å’Œå¤æ‚æ€§ï¼Œä½œä¸ºå³æ—¶æ£€æµ‹ï¼ˆPoint-of-Careï¼ŒPoCï¼‰è®¾å¤‡çš„ä½¿ç”¨ä»ç„¶å—åˆ°é™åˆ¶ã€‚é€šè¿‡é™ä½ç£åœºå¼ºåº¦æ¥å®ç°æœªæ¥æŠ€æœ¯çš„å‘å±•ï¼Œå…³é”®æ–¹æ³•ä¹‹ä¸€æ˜¯æ”¹è¿›é‡‡æ ·ç­–ç•¥ã€‚å…ˆå‰çš„ç ”ç©¶å·¥ä½œå·²ç»è¡¨æ˜ï¼Œä»è¾ƒå°‘çš„æ ·æœ¬ä¸­å¯ä»¥ç›´æ¥ä»kç©ºé—´è¿›è¡Œè¯Šæ–­å†³ç­–ã€‚è¿™æ ·çš„å·¥ä½œè¯æ˜äº†å¯ä»¥åšå‡ºå•ä¸€çš„è¯Šæ–­å†³ç­–ï¼Œä½†å¦‚æœæˆ‘ä»¬å¸Œæœ›å°†MRIè§†ä¸ºçœŸæ­£çš„å³æ—¶æ£€æµ‹å·¥å…·ï¼Œåˆ™éœ€è¦åœ¨å‡å°‘é‡‡é›†æ ·æœ¬æ•°é‡çš„åŒæ—¶ï¼Œåšå‡ºå¤šæ¬¡è¿ç»­çš„è¯Šæ–­å†³ç­–ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„å¤šç›®æ ‡å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œèƒ½å¤Ÿä»æ¬ é‡‡æ ·çš„kç©ºé—´æ•°æ®ä¸­å®ç°å…¨é¢ã€è¿ç»­çš„è¯Šæ–­è¯„ä¼°ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿä¸»åŠ¨é€‚åº”è¿ç»­å†³ç­–ä»¥å®ç°æœ€ä¼˜é‡‡æ ·ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§è®­ç»ƒæ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä½¿ç”¨é€æ­¥åŠ æƒå¥–åŠ±å‡½æ•°æ¥ç¡®å®šå¯¹æ¯ä¸ªè¯Šæ–­ç›®æ ‡è´¡çŒ®æœ€å¤§çš„æ ·æœ¬ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªè¿ç»­çš„è†å…³èŠ‚ç—…ç†è¯„ä¼°ä»»åŠ¡ä¸­è¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼šå‰äº¤å‰éŸ§å¸¦æ‰­ä¼¤æ£€æµ‹å’Œè½¯éª¨åšåº¦æŸå¤±è¯„ä¼°ã€‚æˆ‘ä»¬çš„æ¡†æ¶åœ¨ç–¾ç—…æ£€æµ‹ã€ä¸¥é‡ç¨‹åº¦é‡åŒ–å’Œæ•´ä½“è¿ç»­è¯Šæ–­æ–¹é¢çš„è¯Šæ–­æ€§èƒ½ä¸å„ç§åŸºäºç­–ç•¥çš„æ ‡å‡†ç›¸å½“ï¼ŒåŒæ—¶å¤§å¹…å‡å°‘äº†kç©ºé—´æ ·æœ¬çš„é‡‡é›†ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ºç£å…±æŒ¯æˆåƒä½œä¸ºå…¨é¢ä¸”ç»æµå®æƒ çš„å³æ—¶æ£€æµ‹è®¾å¤‡é“ºå¹³äº†é“è·¯ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/vios-s/MRI_Sequential_Active_Sampling%E4%B8%8A%E5%85%AC%E5%BC%80%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/vios-s/MRI_Sequential_Active_Samplingä¸Šå…¬å¼€è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.04586v1">PDF</a> Under Review</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰åœ¨ç‚¹åŒ»ç–—æŠ¤ç†ï¼ˆPoCï¼‰è®¾å¤‡ä¸­çš„åº”ç”¨é™åˆ¶ï¼Œå¦‚é«˜æˆæœ¬å’Œå¤æ‚æ€§ã€‚ä¸ºæé«˜MRIåœ¨PoCè®¾å¤‡ä¸­çš„æ½œåŠ›ï¼Œç ”ç©¶è€…æå‡ºæ”¹è¿›é‡‡æ ·ç­–ç•¥æ˜¯å…³é”®ã€‚åˆ©ç”¨æ¬ é‡‡æ ·çš„k-spaceæ•°æ®è¿›è¡Œç»¼åˆè¯Šæ–­è¯„ä¼°ï¼Œç ”ç©¶å›¢é˜Ÿé‡‡ç”¨ä¸€ç§æ–°å‹å¤šç›®æ ‡å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œå®ç°è¿ç»­å†³ç­–è¿‡ç¨‹çš„æœ€ä¼˜åŒ–é‡‡æ ·ã€‚è¯¥ç ”ç©¶åœ¨è†éƒ¨ç—…ç†è¯„ä¼°ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰¯å¥½çš„è¯Šæ–­æ€§èƒ½ï¼Œå¹¶æ˜¾è‘—èŠ‚çœäº†k-spaceæ ·æœ¬ã€‚è¿™ä¸ºMRIä½œä¸ºå…¨é¢ä¸”å¯è´Ÿæ‹…çš„PoCè®¾å¤‡é“ºå¹³äº†é“è·¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MRIåœ¨ç‚¹åŒ»ç–—æŠ¤ç†è®¾å¤‡ä¸­çš„åº”ç”¨å—é™äºé«˜æˆæœ¬å’Œå¤æ‚æ€§ã€‚</li>
<li>æ”¹è¿›é‡‡æ ·ç­–ç•¥æ˜¯æé«˜MRIåœ¨PoCè®¾å¤‡ä¸­åº”ç”¨æ½œåŠ›çš„å…³é”®ã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨æ–°å‹å¤šç›®æ ‡å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œå®ç°ä»æ¬ é‡‡æ ·çš„k-spaceæ•°æ®ä¸­åšå‡ºç»¼åˆè¯Šæ–­è¯„ä¼°ã€‚</li>
<li>è¯¥æ¡†æ¶å®ç°äº†è¿ç»­å†³ç­–è¿‡ç¨‹çš„æœ€ä¼˜åŒ–é‡‡æ ·ã€‚</li>
<li>åœ¨è†éƒ¨ç—…ç†è¯„ä¼°ä»»åŠ¡ä¸­ï¼Œè¯¥æ¡†æ¶è¡¨ç°å‡ºè‰¯å¥½çš„è¯Šæ–­æ€§èƒ½ï¼Œä¸å„ç§æ”¿ç­–åŸºå‡†ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ã€‚</li>
<li>è¯¥æ¡†æ¶æ˜¾è‘—èŠ‚çœäº†k-spaceæ ·æœ¬çš„é‡‡é›†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.04586">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5f8043abafbfb3037f706b308f2fe8d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c1f75e45ef95fbae2f3981c802b3b20a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c609c7549387967b2d699d78ebc8b37d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5ea647cd9749c874e9f8fd4f4e40a19b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a60fa77114b557ae2a3fcbcdabf65f54.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="RAFT-Robust-Augmentation-of-FeaTures-for-Image-Segmentation"><a href="#RAFT-Robust-Augmentation-of-FeaTures-for-Image-Segmentation" class="headerlink" title="RAFT: Robust Augmentation of FeaTures for Image Segmentation"></a>RAFT: Robust Augmentation of FeaTures for Image Segmentation</h2><p><strong>Authors:Edward Humes, Xiaomin Lin, Uttej Kallakuri, Tinoosh Mohsenin</strong></p>
<p>Image segmentation is a powerful computer vision technique for scene understanding. However, real-world deployment is stymied by the need for high-quality, meticulously labeled datasets. Synthetic data provides high-quality labels while reducing the need for manual data collection and annotation. However, deep neural networks trained on synthetic data often face the Syn2Real problem, leading to poor performance in real-world deployments.   To mitigate the aforementioned gap in image segmentation, we propose RAFT, a novel framework for adapting image segmentation models using minimal labeled real-world data through data and feature augmentations, as well as active learning. To validate RAFT, we perform experiments on the synthetic-to-real â€œSYNTHIA-&gt;Cityscapesâ€ and â€œGTAV-&gt;Cityscapesâ€ benchmarks. We managed to surpass the previous state of the art, HALO. SYNTHIA-&gt;Cityscapes experiences an improvement in mIoU* upon domain adaptation of 2.1%&#x2F;79.9%, and GTAV-&gt;Cityscapes experiences a 0.4%&#x2F;78.2% improvement in mIoU. Furthermore, we test our approach on the real-to-real benchmark of â€œCityscapes-&gt;ACDCâ€, and again surpass HALO, with a gain in mIoU upon adaptation of 1.3%&#x2F;73.2%. Finally, we examine the effect of the allocated annotation budget and various components of RAFT upon the final transfer mIoU. </p>
<blockquote>
<p>å›¾åƒåˆ†å‰²æ˜¯åœºæ™¯ç†è§£çš„ä¸€ç§å¼ºå¤§çš„è®¡ç®—æœºè§†è§‰æŠ€æœ¯ã€‚ç„¶è€Œï¼ŒçœŸå®ä¸–ç•Œçš„åº”ç”¨éƒ¨ç½²å—åˆ°éœ€è¦é«˜è´¨é‡ã€ç²¾ç»†æ ‡æ³¨æ•°æ®é›†çš„é™åˆ¶ã€‚åˆæˆæ•°æ®æä¾›äº†é«˜è´¨é‡æ ‡ç­¾ï¼ŒåŒæ—¶å‡å°‘äº†æ‰‹åŠ¨æ•°æ®æ”¶é›†å’Œæ³¨é‡Šçš„éœ€æ±‚ã€‚ç„¶è€Œï¼Œåœ¨åˆæˆæ•°æ®ä¸Šè®­ç»ƒçš„æ·±åº¦ç¥ç»ç½‘ç»œç»å¸¸é¢ä¸´Syn2Realé—®é¢˜ï¼Œå¯¼è‡´åœ¨çœŸå®ä¸–ç•Œéƒ¨ç½²ä¸­çš„æ€§èƒ½ä¸ä½³ã€‚</p>
</blockquote>
<p>ä¸ºäº†ç¼“è§£ä¸Šè¿°å›¾åƒåˆ†å‰²ä¸­çš„å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†RAFTï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„æ¡†æ¶ï¼Œé€šè¿‡æ•°æ®å’Œç‰¹å¾å¢å¼ºä»¥åŠä¸»åŠ¨å­¦ä¹ ï¼Œä½¿ç”¨æœ€å°‘çš„æ ‡è®°çœŸå®ä¸–ç•Œæ•°æ®æ¥é€‚åº”å›¾åƒåˆ†å‰²æ¨¡å‹ã€‚ä¸ºäº†éªŒè¯RAFTï¼Œæˆ‘ä»¬åœ¨åˆæˆåˆ°çœŸå®çš„â€œSYNTHIA-&gt;Cityscapesâ€å’Œâ€œGTAV-&gt;Cityscapesâ€åŸºå‡†æµ‹è¯•é›†ä¸Šè¿›è¡Œäº†å®éªŒã€‚æˆ‘ä»¬è¶…è¶Šäº†ä¹‹å‰çš„å…ˆè¿›æŠ€æœ¯HALOã€‚SYNTHIA-&gt;Cityscapesåœ¨åŸŸé€‚åº”æ–¹é¢æé«˜äº†mIoU* 2.1%&#x2F;79.9%ï¼ŒGTAV-&gt;Cityscapesæé«˜äº†mIoU 0.4%&#x2F;78.2%ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨â€œCityscapes-&gt;ACDCâ€çš„çœŸå®åˆ°çœŸå®çš„åŸºå‡†æµ‹è¯•é›†ä¸Šæµ‹è¯•äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œå¹¶å†æ¬¡è¶…è¶Šäº†HALOï¼Œåœ¨é€‚åº”åæé«˜äº†mIoU 1.3%&#x2F;73.2%ã€‚æœ€åï¼Œæˆ‘ä»¬ç ”ç©¶äº†åˆ†é…æ³¨é‡Šé¢„ç®—å¯¹RAFTæœ€ç»ˆè¿ç§»mIoUçš„å½±å“ä»¥åŠå„ç§ç»„ä»¶çš„å½±å“ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.04529v1">PDF</a> </p>
<p><strong>Summary</strong><br>     å›¾åƒåˆ†å‰²æ˜¯ä¸€ç§å¼ºå¤§çš„è®¡ç®—æœºè§†è§‰åœºæ™¯ç†è§£æŠ€æœ¯ï¼Œä½†åœ¨å®é™…éƒ¨ç½²ä¸­éœ€è¦é«˜è´¨é‡ã€ç²¾ç»†æ ‡æ³¨çš„æ•°æ®é›†ã€‚åˆæˆæ•°æ®æä¾›äº†é«˜è´¨é‡æ ‡ç­¾ï¼Œå‡å°‘äº†æ‰‹åŠ¨æ”¶é›†å’Œæ ‡æ³¨çš„éœ€è¦ã€‚ç„¶è€Œï¼Œåœ¨åˆæˆæ•°æ®ä¸Šè®­ç»ƒçš„æ·±åº¦ç¥ç»ç½‘ç»œå¸¸å¸¸é¢ä¸´åˆæˆåˆ°ç°å®ï¼ˆSyn2Realï¼‰é—®é¢˜ï¼Œå¯¼è‡´åœ¨ç°å®éƒ¨ç½²ä¸­è¡¨ç°ä¸ä½³ã€‚ä¸ºç¼“è§£å›¾åƒåˆ†å‰²ä¸­çš„ä¸Šè¿°å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†RAFTæ¡†æ¶ï¼Œé€šè¿‡æœ€å°é™åº¦çš„çœŸå®ä¸–ç•Œæ ‡æ³¨æ•°æ®ï¼Œåˆ©ç”¨æ•°æ®å’Œç‰¹å¾å¢å¼ºä»¥åŠä¸»åŠ¨å­¦ä¹ æ–¹æ³•ï¼Œå¯¹å›¾åƒåˆ†å‰²æ¨¡å‹è¿›è¡Œé€‚åº”ã€‚å®éªŒéªŒè¯æ˜¾ç¤ºï¼Œæˆ‘ä»¬åœ¨åˆæˆåˆ°ç°å®çš„â€œSYNTHIAâ†’Cityscapesâ€å’Œâ€œGTAVâ†’Cityscapesâ€åŸºå‡†æµ‹è¯•ä¸­è¶…è¿‡äº†ä¹‹å‰çš„æœ€æ–°æ°´å¹³HALOï¼Œåœ¨mIoU*æŒ‡æ ‡ä¸Šå®ç°äº†æ˜¾è‘—æ”¹è¿›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨çœŸå®åˆ°çœŸå®çš„â€œCityscapesâ†’ACDCâ€åŸºå‡†æµ‹è¯•ä¸­ä¹ŸéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚æœ€åï¼Œæˆ‘ä»¬ç ”ç©¶äº†æ ‡æ³¨é¢„ç®—åˆ†é…å’ŒRAFTçš„ä¸åŒç»„ä»¶å¯¹æœ€ç»ˆè¿ç§»mIoUçš„å½±å“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å›¾åƒåˆ†å‰²æ˜¯è®¡ç®—æœºè§†è§‰ä¸­çš„ä¸€é¡¹é‡è¦æŠ€æœ¯ï¼Œç”¨äºåœºæ™¯ç†è§£ï¼Œä½†åœ¨å®é™…åº”ç”¨ä¸­å—åˆ°é«˜è´¨é‡æ•°æ®é›†çš„é™åˆ¶ã€‚</li>
<li>åˆæˆæ•°æ®èƒ½å‡å°‘æ‰‹åŠ¨æ•°æ®æ”¶é›†å’Œæ ‡æ³¨çš„å·¥ä½œé‡ï¼Œä½†è®­ç»ƒçš„æ¨¡å‹åœ¨çœŸå®åœºæ™¯ä¸­è¡¨ç°ä¸ä½³ï¼Œå­˜åœ¨åˆæˆåˆ°ç°å®ï¼ˆSyn2Realï¼‰é—®é¢˜ã€‚</li>
<li>æå‡ºRAFTæ¡†æ¶ï¼Œé€šè¿‡æ•°æ®å¢å¼ºã€ç‰¹å¾å¢å¼ºå’Œä¸»åŠ¨å­¦ä¹ ç­–ç•¥ï¼Œè§£å†³å›¾åƒåˆ†å‰²æ¨¡å‹åœ¨ç°å®åœºæ™¯éƒ¨ç½²ä¸­çš„æ€§èƒ½å·®è·ã€‚</li>
<li>åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­éªŒè¯äº†RAFTçš„æœ‰æ•ˆæ€§ï¼ŒåŒ…æ‹¬â€œSYNTHIAâ†’Cityscapesâ€ï¼Œâ€œGTAVâ†’Cityscapesâ€ï¼Œä»¥åŠâ€œCityscapesâ†’ACDCâ€ï¼Œå¹¶è¶…è¿‡äº†ä¹‹å‰çš„æœ€æ–°æ°´å¹³HALOã€‚</li>
<li>RAFTæ¡†æ¶èƒ½æœ‰æ•ˆåˆ©ç”¨æœ€å°é™åº¦çš„çœŸå®ä¸–ç•Œæ ‡æ³¨æ•°æ®ï¼Œæé«˜æ¨¡å‹åœ¨ç°å®åœºæ™¯ä¸­çš„æ€§èƒ½ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨mIoU*æŒ‡æ ‡ä¸Šå®ç°äº†æ˜¾è‘—æ”¹è¿›ï¼Œå…·ä½“æ•°å€¼æ ¹æ®ä¸åŒåŸºå‡†æµ‹è¯•è€Œæœ‰æ‰€ä¸åŒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.04529">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-259a34333abf28c411db929cac9c14b4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-59e54fa910c2a9674c019d868575b969.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-033804c9937cc5c85ea2a371eb2fa3fd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ebc51f6e8cc08d2798886eb33c70313.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Text2CT-Towards-3D-CT-Volume-Generation-from-Free-text-Descriptions-Using-Diffusion-Model"><a href="#Text2CT-Towards-3D-CT-Volume-Generation-from-Free-text-Descriptions-Using-Diffusion-Model" class="headerlink" title="Text2CT: Towards 3D CT Volume Generation from Free-text Descriptions   Using Diffusion Model"></a>Text2CT: Towards 3D CT Volume Generation from Free-text Descriptions   Using Diffusion Model</h2><p><strong>Authors:Pengfei Guo, Can Zhao, Dong Yang, Yufan He, Vishwesh Nath, Ziyue Xu, Pedro R. A. S. Bassi, Zongwei Zhou, Benjamin D. Simon, Stephanie Anne Harmon, Baris Turkbey, Daguang Xu</strong></p>
<p>Generating 3D CT volumes from descriptive free-text inputs presents a transformative opportunity in diagnostics and research. In this paper, we introduce Text2CT, a novel approach for synthesizing 3D CT volumes from textual descriptions using the diffusion model. Unlike previous methods that rely on fixed-format text input, Text2CT employs a novel prompt formulation that enables generation from diverse, free-text descriptions. The proposed framework encodes medical text into latent representations and decodes them into high-resolution 3D CT scans, effectively bridging the gap between semantic text inputs and detailed volumetric representations in a unified 3D framework. Our method demonstrates superior performance in preserving anatomical fidelity and capturing intricate structures as described in the input text. Extensive evaluations show that our approach achieves state-of-the-art results, offering promising potential applications in diagnostics, and data augmentation. </p>
<blockquote>
<p>ä»æè¿°æ€§è‡ªç”±æ–‡æœ¬è¾“å…¥ç”Ÿæˆ3D CTä½“ç§¯åœ¨è¯Šæ–­å’Œç ”ç©¶ä¸­æä¾›äº†å˜é©æ€§çš„æœºä¼šã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†Text2CTï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨æ‰©æ•£æ¨¡å‹ä»æ–‡æœ¬æè¿°ä¸­åˆæˆ3D CTä½“ç§¯çš„æ–°æ–¹æ³•ã€‚ä¸ä¹‹å‰ä¾èµ–äºå›ºå®šæ ¼å¼æ–‡æœ¬è¾“å…¥çš„æ–¹æ³•ä¸åŒï¼ŒText2CTé‡‡ç”¨äº†ä¸€ç§æ–°çš„æç¤ºåˆ¶å®šæ–¹å¼ï¼Œèƒ½å¤Ÿæ ¹æ®ä¸åŒçš„è‡ªç”±æ–‡æœ¬æè¿°è¿›è¡Œç”Ÿæˆã€‚æ‰€æå‡ºçš„æ¡†æ¶å°†åŒ»å­¦æ–‡æœ¬ç¼–ç ä¸ºæ½œåœ¨è¡¨ç¤ºï¼Œå¹¶å°†å…¶è§£ç ä¸ºé«˜åˆ†è¾¨ç‡çš„3D CTæ‰«æï¼Œæœ‰æ•ˆåœ°åœ¨ç»Ÿä¸€çš„3Dæ¡†æ¶ä¸­å¼¥åˆäº†è¯­ä¹‰æ–‡æœ¬è¾“å…¥å’Œè¯¦ç»†ä½“ç§¯è¡¨ç¤ºä¹‹é—´çš„é¸¿æ²Ÿã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¿æŒè§£å‰–ä¿çœŸåº¦å’Œæ•æ‰è¾“å…¥æ–‡æœ¬ä¸­æè¿°çš„å¤æ‚ç»“æ„æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚å¤§é‡è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†æœ€æ–°æ°´å¹³çš„ç»“æœï¼Œåœ¨è¯Šæ–­å’Œæ•°æ®å¢å¼ºæ–¹é¢æœ‰ç€å¹¿é˜”çš„åº”ç”¨å‰æ™¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.04522v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹ä»æ–‡æœ¬æè¿°ä¸­ç”Ÿæˆ3D CTä½“ç§¯çš„æ–°æ–¹æ³•Text2CTã€‚ä¸ä¼ ç»Ÿçš„ä¾èµ–äºå›ºå®šæ ¼å¼æ–‡æœ¬è¾“å…¥çš„æ–¹æ³•ä¸åŒï¼ŒText2CTé‡‡ç”¨äº†ä¸€ç§æ–°é¢–çš„æç¤ºå½¢å¼ï¼Œæ”¯æŒä»å„ç§è‡ªç”±æ–‡æœ¬æè¿°ä¸­è¿›è¡Œç”Ÿæˆã€‚è¯¥æ–¹æ³•å°†åŒ»å­¦æ–‡æœ¬ç¼–ç ä¸ºæ½œåœ¨è¡¨ç¤ºï¼Œç„¶åè§£ç ä¸ºé«˜è´¨é‡çš„ä¸‰ç»´CTæ‰«æå›¾åƒï¼Œæœ‰æ•ˆæ¡¥æ¥äº†è¯­ä¹‰æ–‡æœ¬è¾“å…¥å’Œè¯¦ç»†ä½“ç§¯è¡¨ç¤ºä¹‹é—´çš„é¸¿æ²Ÿã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒè§£å‰–çœŸå®æ€§å’Œæ•æ‰å¤æ‚ç»“æ„æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä¸ºè¯Šæ–­å’ŒåŒ»å­¦æ•°æ®å¢å¼ºé¢†åŸŸå¸¦æ¥äº†åº”ç”¨å‰æ™¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯æ–‡æœ¬ä¸­å…³é”®çš„è§è§£æ‘˜è¦ï¼š</p>
<ol>
<li>Text2CTæ˜¯ä¸€ç§å°†æ–‡æœ¬æè¿°è½¬åŒ–ä¸º3D CTä½“ç§¯çš„æ–°å‹æ–¹æ³•ã€‚å®ƒåˆ©ç”¨æ‰©æ•£æ¨¡å‹ï¼Œèƒ½åœ¨è¯Šæ–­å’ŒåŒ»å­¦ç ”ç©¶é¢†åŸŸå®ç°è½¬åŒ–æ€§è¿›æ­¥ã€‚</li>
<li>Text2CTä¸åŒäºä¼ ç»Ÿæ–¹æ³•ï¼Œæ”¯æŒå¤šç§è‡ªç”±æ–‡æœ¬æè¿°ä½œä¸ºè¾“å…¥ï¼Œå¢å¼ºäº†ç”Ÿæˆçš„å¤šæ ·æ€§å’Œçµæ´»æ€§ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡å°†åŒ»å­¦æ–‡æœ¬ç¼–ç ä¸ºæ½œåœ¨è¡¨ç¤ºï¼Œå†è§£ç ä¸ºé«˜è´¨é‡çš„ä¸‰ç»´CTæ‰«æå›¾åƒï¼Œæœ‰æ•ˆèåˆäº†è¯­ä¹‰æ–‡æœ¬å’Œä½“ç§¯è¡¨ç¤ºã€‚</li>
<li>Text2CTåœ¨ä¿æŒè§£å‰–çœŸå®æ€§å’Œæ•æ‰å¤æ‚ç»“æ„æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œèƒ½å¤Ÿæœ‰æ•ˆç”Ÿæˆç»†è‡´çš„CTä½“ç§¯å›¾åƒã€‚</li>
<li>å¹¿æ³›è¯„ä¼°æ˜¾ç¤ºï¼ŒText2CTçš„æ–¹æ³•å¤„äºé¢†å…ˆæ°´å¹³ï¼Œè¿™ä¸ºè¯Šæ–­åº”ç”¨æä¾›äº†æ½œåŠ›ã€‚</li>
<li>è¯¥æ–¹æ³•å¯ç”¨äºåŒ»å­¦æ•°æ®å¢å¼ºï¼Œæœ‰åŠ©äºä¸°å¯ŒåŒ»å­¦å›¾åƒæ•°æ®é›†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.04522">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ddf81f9d13a7b8666f5a69baabe9af15.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e3c7b49feb9f96bc0244b8216c3a82e8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-67e3c88ea22e27211cb9eaaf774eccf8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-615ac0254561132db98fdf1b8c74fb58.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="4XMM-J175136-8-275858-A-New-Magnetar-Candidate"><a href="#4XMM-J175136-8-275858-A-New-Magnetar-Candidate" class="headerlink" title="4XMM J175136.8-275858: A New Magnetar Candidate?"></a>4XMM J175136.8-275858: A New Magnetar Candidate?</h2><p><strong>Authors:Robbie Webbe, Norman Khan, N. A. Webb, E. Quintin</strong></p>
<p>Magnetars are very rare astrophysical objects, with $\sim$31 known to date. They are best understood as highly magnetised neutron stars, but a greater number need to be found to constrain their role in stellar evolution pathways. We apply a novel approach for the detection of fast, transient X-ray sources, using a revised version of the EPIC XMM-Newton Outburst Detector (EXOD) with the aim of detecting and identifying new and rare variable compact objects. We detect a transient, variable source notable for its strong variability and hard spectrum. The emission from 4XMM J175136.8-275858 is well characterised by a blackbody, with temperatures between $\sim$1.8â€“5,keV during its lower luminosity phase. Its temperature is poorly constrained during its brightest phase, and we observe an increase in luminosity by two orders of magnitude over timescales of a few ks. This is driven by increased emission of X-rays at energies above 2,keV, with a luminosity decay potentially over weeks or months. Derived luminosities for 4XJ1751-2759 range up to $\sim10^{35} \text{,erg s}^{-1}$ at 8,kpc at the Galactic centre, but neutral hydrogen column densities are greater than predicted Galactic values possibly implying a greater distance to the source, still within our galaxy, further increasing its luminosity. A consideration of optical and IR information in combination with the X-ray observations allow us to exclude the possibility that 4XJ1751-2759 is a star, rotationally powered pulsar or supergiant fast X-ray transient. This rapid, hard, variability is closest to that of outbursts in magnetars than any other known class of X-ray transient. </p>
<blockquote>
<p>ç£æ˜Ÿæ˜¯éå¸¸ç½•è§çš„å¤©ä½“ç‰©ç†å¯¹è±¡ï¼Œè‡³ä»Šå·²çŸ¥çº¦31ä¸ªã€‚å®ƒä»¬æœ€å¥½è¢«ç†è§£ä¸ºé«˜åº¦ç£åŒ–çš„ä¸­å­æ˜Ÿï¼Œä½†éœ€è¦å‘ç°æ›´å¤šçš„ç£æ˜Ÿæ¥é™åˆ¶å®ƒä»¬åœ¨æ’æ˜Ÿæ¼”åŒ–é€”å¾„ä¸­çš„ä½œç”¨ã€‚æˆ‘ä»¬é‡‡ç”¨ä¸€ç§æ–°é¢–çš„æ–¹æ³•æ¥æ£€æµ‹å¿«é€Ÿã€çŸ­æš‚çš„Xå°„çº¿æºï¼Œä½¿ç”¨ä¿®è®¢åçš„EPICXMM-Newtonçˆ†å‘æ£€æµ‹å™¨ï¼ˆEXODï¼‰ç‰ˆæœ¬ï¼Œæ—¨åœ¨æ£€æµ‹å’Œè¯†åˆ«æ–°çš„å’Œç½•è§çš„å¯å˜ç´§å‡‘å‹å¤©ä½“ã€‚æˆ‘ä»¬æ£€æµ‹åˆ°ä¸€ä¸ªçŸ­æš‚ã€å¯å˜çš„æºï¼Œä»¥å…¶å¼ºå¯å˜æ€§ä»¥åŠç¡¬è°±è€Œæ˜¾è‘—ã€‚æ¥è‡ª4XMM J175136.8-275858çš„å‘å°„å¯ä»¥è¢«é»‘ä½“å¾ˆå¥½åœ°è¡¨å¾ï¼Œåœ¨å…¶è¾ƒä½å…‰åº¦é˜¶æ®µæ—¶ï¼Œæ¸©åº¦çº¦ä¸º~ 1.8è‡³~ 5,keVä¹‹é—´ã€‚åœ¨å…¶æœ€äº®çš„é˜¶æ®µï¼Œæ¸©åº¦çº¦æŸè¾ƒå·®ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°åœ¨å‡ åƒç§’å†…äº®åº¦å¢åŠ äº†ä¸¤ä¸ªæ•°é‡çº§ã€‚è¿™æ˜¯ç”±é«˜äº~ 2,keVçš„Xå°„çº¿å‘å°„çš„å¢åŠ æ‰€é©±åŠ¨çš„ï¼Œäº®åº¦è¡°å‡å¯èƒ½åœ¨æ•°å‘¨æˆ–æ•°æœˆå†…æŒç»­ã€‚å¯¹äºä½äºé“¶æ²³ç³»ä¸­å¿ƒè·ç¦»8kpcçš„ç£æ˜Ÿæ¥è¯´ï¼Œå…¶æ´¾ç”Ÿå…‰åº¦é«˜è¾¾$\sim 10^{35} \text{ erg s}^{-1}$ï¼Œä½†ä¸­æ€§æ°¢æŸ±å¯†åº¦å¤§äºé¢„æµ‹çš„é“¶æ²³ç³»å€¼ï¼Œå¯èƒ½æš—ç¤ºè¯¥æºçš„å‘å°„æºè·ç¦»æ›´è¿œã€‚ä¸è¿‡ä»æ—§ä½äºé“¶æ²³ç³»å†…ã€‚è¿™ä¸€äº‹å®è¿åŒä¸å…‰å­¦å’Œçº¢å¤–è§‚æµ‹ç»“æœç»¼åˆè€ƒè™‘æ’é™¤äº†å®ƒæ˜¯ä¸€é¢—æ™®é€šæ’æ˜Ÿã€æ—‹è½¬è„‰å†²æ˜Ÿæˆ–è¶…çº§å·¨æ˜Ÿå¿«é€ŸXå°„çº¿ç¬å˜çš„å¯èƒ½æ€§ã€‚è¿™ç§å¿«é€Ÿè€Œå¼ºçƒˆçš„å¯å˜æ€§æœ€æ¥è¿‘ç£æ˜Ÿçˆ†å‘çš„ç‰¹å¾ï¼Œä¸å…¶ä»–å·²çŸ¥çš„Xå°„çº¿ç¬å˜ç±»å‹ç›¸æ¯”æœ€ä¸ºç›¸ä¼¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.04206v1">PDF</a> 14 pages, 10 figures. Accepted to MNRAS</p>
<p><strong>Summary</strong><br>    åˆ©ç”¨ä¿®è®¢åçš„EPICXMM-Newtonçˆ†å‘æ£€æµ‹å™¨ï¼ˆEXODï¼‰æ–°æ–¹æ³•æ£€æµ‹åˆ°äº†ä¸€ç§çŸ­æš‚ã€å¯å˜çš„å¼ºå˜é‡ç¡¬è°±æºã€‚è¯¥æºï¼ˆ4XMM J175136.8-275858ï¼‰åœ¨è¾ƒä½å…‰åº¦æ—¶æœŸçš„æ¸©åº¦çº¦ä¸º1.8-5keVï¼Œäº®åº¦åœ¨ä¸¤ä¸ªæ•°é‡çº§å†…ä¸Šå‡ï¼Œæ—¶é—´è·¨åº¦ä¸ºæ•°åƒç§’ã€‚å…¶Xå°„çº¿å‘å°„åœ¨é«˜äº2keVçš„èƒ½é‡å¤„å¢å¼ºï¼Œå…‰åº¦è¡°å‡å¯èƒ½æŒç»­æ•°å‘¨æˆ–æ•°æœˆã€‚ç»“åˆå…‰å­¦å’Œçº¢å¤–ä¿¡æ¯ï¼Œæ’é™¤äº†å…¶ä¸ºæ’æ˜Ÿã€æ—‹è½¬é©±åŠ¨è„‰å†²æ˜Ÿæˆ–è¶…çº§å·¨æ˜Ÿå¿«é€ŸXå°„çº¿çŸ­æš‚äº‹ä»¶çš„å¯èƒ½æ€§ï¼Œè¢«è®¤ä¸ºæ˜¯æœ€æ¥è¿‘ç£æ˜Ÿçˆ†å‘çš„çŸ­æš‚ç¡¬å˜æºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Magnetarsæ˜¯ç½•è§çš„å¤©æ–‡ç°è±¡ï¼Œç›®å‰å·²çŸ¥çº¦31ä¸ªï¼Œè¢«è®¤ä¸ºæ˜¯é«˜åº¦ç£åŒ–çš„ä¸­å­æ˜Ÿï¼Œéœ€è¦æ›´å¤šçš„å‘ç°æ¥é™åˆ¶å®ƒä»¬åœ¨æ’æ˜Ÿæ¼”åŒ–è¿‡ç¨‹ä¸­çš„ä½œç”¨ã€‚</li>
<li>ä½¿ç”¨ä¿®è®¢åçš„EXODæ–¹æ³•æ£€æµ‹åˆ°æ–°çš„çŸ­æš‚å¯å˜çš„å¼ºå˜é‡ç¡¬è°±æºã€‚</li>
<li>æºï¼ˆ4XMM J175136.8-275858ï¼‰åœ¨è¾ƒä½å…‰åº¦æ—¶æœŸçš„æ¸©åº¦èŒƒå›´çº¦ä¸º1.8-5keVã€‚</li>
<li>æºçš„äº®åº¦åœ¨çŸ­æ—¶é—´å†…å¢åŠ äº†ä¸¤ä¸ªæ•°é‡çº§ã€‚</li>
<li>æºçš„Xå°„çº¿å‘å°„åœ¨è¾ƒé«˜èƒ½é‡å¤„å¢å¼ºï¼Œå…‰åº¦è¡°å‡å¯èƒ½é•¿è¾¾æ•°å‘¨æˆ–æ•°æœˆã€‚</li>
<li>ç»“åˆå…‰å­¦å’Œçº¢å¤–ä¿¡æ¯æ’é™¤äº†è¯¥æºä¸ºæ’æ˜Ÿã€æ—‹è½¬é©±åŠ¨è„‰å†²æ˜Ÿçš„å¯èƒ½æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.04206">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-60de1fe4d91622f0261e675f19e65e96.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6cf9b14354a7fb569de8e54056e216d1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2dc0a0959f3d105af47185b42549c8fc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e5dd302315769891b3db73ec1ccdb982.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a7b88c44ba647bb1b5e881339271f9d0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1c296cef636a2f2853c4eb76e38d7cd7.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="MAISY-Motion-Aware-Image-SYnthesis-for-MedicalImage-Motion-Correction"><a href="#MAISY-Motion-Aware-Image-SYnthesis-for-MedicalImage-Motion-Correction" class="headerlink" title="MAISY: Motion-Aware Image SYnthesis for MedicalImage Motion Correction"></a>MAISY: Motion-Aware Image SYnthesis for MedicalImage Motion Correction</h2><p><strong>Authors:Andrew Zhang, Hao Wang, Shuchang Ye, Michael Fulham, Jinman Kim</strong></p>
<p>Patient motion during medical image acquisition causes blurring, ghosting, and distorts organs, which makes image interpretation challenging.Current state-of-the-art algorithms using Generative Adversarial Network (GAN)-based methods with their ability to learn the mappings between corrupted images and their ground truth via Structural Similarity Index Measure (SSIM) loss effectively generate motion-free images. However, we identified the following limitations: (i) they mainly focus on global structural characteristics and therefore overlook localized features that often carry critical pathological information, and (ii) the SSIM loss function struggles to handle images with varying pixel intensities, luminance factors, and variance. In this study, we propose Motion-Aware Image SYnthesis (MAISY) which initially characterize motion and then uses it for correction by: (a) leveraging the foundation model Segment Anything Model (SAM), to dynamically learn spatial patterns along anatomical boundaries where motion artifacts are most pronounced and, (b) introducing the Variance-Selective SSIM (VS-SSIM) loss which adaptively emphasizes spatial regions with high pixel variance to preserve essential anatomical details during artifact correction. Experiments on chest and head CT datasets demonstrate that our model outperformed the state-of-the-art counterparts, with Peak Signal-to-Noise Ratio (PSNR) increasing by 40%, SSIM by 10%, and Dice by 16%. </p>
<blockquote>
<p>æ‚£è€…åœ¨åŒ»å­¦å›¾åƒé‡‡é›†è¿‡ç¨‹ä¸­çš„è¿åŠ¨ä¼šå¯¼è‡´å›¾åƒæ¨¡ç³Šã€é¬¼å½±å’Œå™¨å®˜æ‰­æ›²ï¼Œè¿™ä½¿å¾—å›¾åƒè§£è¯»å…·æœ‰æŒ‘æˆ˜æ€§ã€‚å½“å‰æœ€å…ˆè¿›çš„ç®—æ³•ä½¿ç”¨åŸºäºç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰çš„æ–¹æ³•ï¼Œé€šè¿‡å­¦ä¹ è¢«æ±¡æŸ“å›¾åƒä¸çœŸå®å›¾åƒä¹‹é—´çš„æ˜ å°„å…³ç³»ï¼Œé€šè¿‡ç»“æ„ç›¸ä¼¼æ€§æŒ‡æ•°åº¦é‡ï¼ˆSSIMï¼‰æŸå¤±æœ‰æ•ˆåœ°ç”Ÿæˆæ— è¿åŠ¨å›¾åƒã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å‘ç°å­˜åœ¨ä»¥ä¸‹å±€é™æ€§ï¼šï¼ˆiï¼‰å®ƒä»¬ä¸»è¦å…³æ³¨å…¨å±€ç»“æ„ç‰¹å¾ï¼Œä»è€Œå¿½ç•¥äº†é€šå¸¸æºå¸¦å…³é”®ç—…ç†ä¿¡æ¯çš„å±€éƒ¨ç‰¹å¾ï¼›ï¼ˆiiï¼‰SSIMæŸå¤±å‡½æ•°åœ¨å¤„ç†åƒç´ å¼ºåº¦ã€äº®åº¦å› ç´ å’Œæ–¹å·®å„å¼‚çš„å›¾åƒæ—¶é‡åˆ°å›°éš¾ã€‚æœ¬ç ”ç©¶æå‡ºäº†è¿åŠ¨æ„ŸçŸ¥å›¾åƒåˆæˆï¼ˆMAISYï¼‰ï¼Œå®ƒé¦–å…ˆè¡¨å¾è¿åŠ¨ï¼Œç„¶ååˆ©ç”¨è¿åŠ¨è¿›è¡Œä¿®æ­£ï¼šï¼ˆaï¼‰é€šè¿‡åˆ©ç”¨åŸºç¡€æ¨¡å‹åˆ†å‰²ä»»ä½•æ¨¡å‹ï¼ˆSAMï¼‰ï¼ŒåŠ¨æ€å­¦ä¹ è§£å‰–è¾¹ç•Œå¤„çš„ç©ºé—´æ¨¡å¼ï¼Œè¿™äº›è¾¹ç•Œå¤„çš„è¿åŠ¨ä¼ªå½±æœ€ä¸ºçªå‡ºï¼›ï¼ˆbï¼‰å¼•å…¥æ–¹å·®é€‰æ‹©æ€§SSIMï¼ˆVS-SSIMï¼‰æŸå¤±ï¼Œåœ¨ä¼ªå½±ä¿®æ­£è¿‡ç¨‹ä¸­è‡ªé€‚åº”åœ°å¼ºè°ƒé«˜åƒç´ æ–¹å·®çš„ç©ºé—´åŒºåŸŸï¼Œä»¥ä¿ç•™å…³é”®çš„è§£å‰–ç»†èŠ‚ã€‚åœ¨èƒ¸éƒ¨å’Œå¤´éƒ¨CTæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹è¶…è¿‡äº†æœ€å…ˆè¿›çš„åŒè¡Œæ¨¡å‹ï¼Œå³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰æé«˜äº†40%ï¼ŒSSIMæé«˜äº†10%ï¼ŒDiceç³»æ•°æé«˜äº†16%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.04105v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†åŒ»å­¦å›¾åƒè·å–è¿‡ç¨‹ä¸­æ‚£è€…è¿åŠ¨å¯¼è‡´çš„å›¾åƒæ¨¡ç³Šã€é¬¼å½±å’Œå™¨å®˜æ‰­æ›²é—®é¢˜ï¼Œä½¿å¾—å›¾åƒè§£è¯»å˜å¾—å›°éš¾ã€‚ç°æœ‰ç®—æ³•ä¸»è¦å…³æ³¨å…¨å±€ç»“æ„ç‰¹å¾ï¼Œå¿½ç•¥äº†æºå¸¦å…³é”®ç—…ç†ä¿¡æ¯çš„å±€éƒ¨ç‰¹å¾ï¼Œä¸”SSIMæŸå¤±å‡½æ•°åœ¨å¤„ç†åƒç´ å¼ºåº¦ã€äº®åº¦å› ç´ å’Œæ–¹å·®å˜åŒ–çš„å›¾åƒæ—¶å­˜åœ¨å›°éš¾ã€‚æœ¬ç ”ç©¶æå‡ºäº†Motion-Aware Image SYnthesisï¼ˆMAISYï¼‰ï¼Œé¦–å…ˆè¿›è¡Œè¿åŠ¨ç‰¹å¾è¡¨å¾ï¼Œç„¶ååˆ©ç”¨è¿åŠ¨ç‰¹å¾è¿›è¡Œæ ¡æ­£ã€‚é€šè¿‡åˆ©ç”¨Segment Anything Modelï¼ˆSAMï¼‰åŸºç¡€æ¨¡å‹åŠ¨æ€å­¦ä¹ è§£å‰–è¾¹ç•Œçš„ç©ºé—´æ¨¡å¼ï¼Œå¹¶å¼•å…¥Variance-Selective SSIMï¼ˆVS-SSIMï¼‰æŸå¤±ï¼Œä»¥åœ¨ä¿ç•™é‡è¦è§£å‰–ç»†èŠ‚çš„åŒæ—¶è‡ªé€‚åº”åœ°å¼ºè°ƒé«˜åƒç´ æ–¹å·®çš„ç©ºé—´åŒºåŸŸã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨èƒ¸éƒ¨å’Œå¤´éƒ¨CTæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æŠ€æœ¯ï¼ŒPSNRæé«˜40%ï¼ŒSSIMæé«˜10%ï¼ŒDiceæé«˜16%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒè·å–è¿‡ç¨‹ä¸­çš„æ‚£è€…è¿åŠ¨ä¼šå¯¼è‡´å›¾åƒæ¨¡ç³Šã€é¬¼å½±å’Œå™¨å®˜æ‰­æ›²ï¼Œä½¿å¾—å›¾åƒè§£è¯»å›°éš¾ã€‚</li>
<li>å½“å‰æœ€å…ˆè¿›çš„ç®—æ³•ä½¿ç”¨GANå’ŒSSIMæŸå¤±æ¥ç”Ÿæˆæ— è¿åŠ¨å›¾åƒï¼Œä½†å­˜åœ¨å±€é™æ€§ï¼šä¸»è¦å…³æ³¨å…¨å±€ç»“æ„ç‰¹å¾ï¼Œå¿½ç•¥å±€éƒ¨ç‰¹å¾ï¼›SSIMæŸå¤±å‡½æ•°åœ¨å¤„ç†åƒç´ å¼ºåº¦ã€äº®åº¦å’Œæ–¹å·®å˜åŒ–çš„å›¾åƒæ—¶è¡¨ç°ä¸ä½³ã€‚</li>
<li>æœ¬ç ”ç©¶æå‡ºäº†Motion-Aware Image SYnthesisï¼ˆMAISYï¼‰æ¨¡å‹ï¼Œè¯¥æ¨¡å‹é¦–å…ˆè¿›è¡Œè¿åŠ¨ç‰¹å¾è¡¨å¾å¹¶åˆ©ç”¨è¿™äº›ç‰¹å¾è¿›è¡Œå›¾åƒæ ¡æ­£ã€‚</li>
<li>MAISYæ¨¡å‹åˆ©ç”¨Segment Anything Modelï¼ˆSAMï¼‰å­¦ä¹ è§£å‰–è¾¹ç•Œçš„ç©ºé—´æ¨¡å¼ï¼Œå¹¶å¼ºè°ƒé«˜åƒç´ æ–¹å·®åŒºåŸŸä»¥ä¿ç•™é‡è¦è§£å‰–ç»†èŠ‚ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒMAISYæ¨¡å‹åœ¨èƒ¸éƒ¨å’Œå¤´éƒ¨CTæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
<li>MAISYæ¨¡å‹æé«˜äº†å›¾åƒè´¨é‡æŒ‡æ ‡ï¼Œå¦‚PSNRæé«˜40%ï¼ŒSSIMæé«˜10%ï¼ŒDiceæé«˜16%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.04105">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2ad56f32c9dc60dd7bad97809a6d3e2d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fa1cfcf6463d8b74bad08fcbe4efb97f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3aec9bc57c218e1014e2b1609ce0922f.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Device-Free-Localization-Using-Multi-Link-MIMO-Channels-in-Distributed-Antenna-Networks"><a href="#Device-Free-Localization-Using-Multi-Link-MIMO-Channels-in-Distributed-Antenna-Networks" class="headerlink" title="Device-Free Localization Using Multi-Link MIMO Channels in Distributed   Antenna Networks"></a>Device-Free Localization Using Multi-Link MIMO Channels in Distributed   Antenna Networks</h2><p><strong>Authors:Minseok Kim, Gesi Teng, Keita Nishi, Togo Ikegami, Masamune Sato</strong></p>
<p>This paper presented a novel device-free localization (DFL) framework based on distributed antenna networks (DANs), targeting integrated sensing and communication (ISAC) in future 6G radio access networks (RANs). In the proposed approach, radio tomographic imaging (RTI) leverages the spatial and temporal diversity of multi-link multiple-input multiple-output (MIMO) channels in DANs to improve localization accuracy. Furthermore, a prototype system was developed using software-defined radios (SDRs) operating in the sub-6 GHz band, and comprehensive evaluations were conducted under indoor conditions involving varying node densities and target types. The results demonstrate that the framework achieves sub-meter localization accuracy in most scenarios and maintains robust performance under complex multipath environments. In addition, the use of Bayesian optimization to fine-tune key parameters, such as sparsity and path thickness, led to significant improvements in image reconstruction quality and target estimation accuracy. These results demonstrate the feasibility and effectiveness of DAN-based DFL systems for accurate, robust, and scalable localization. </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºåˆ†å¸ƒå¼å¤©çº¿ç½‘ç»œï¼ˆDANsï¼‰çš„æ–°å‹æ— è®¾å¤‡å®šä½ï¼ˆDFLï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨ç”¨äºæœªæ¥6Gæ— çº¿æ¥å…¥ç½‘ç»œï¼ˆRANsï¼‰çš„ç»¼åˆæ„ŸçŸ¥å’Œé€šä¿¡ï¼ˆISACï¼‰ã€‚åœ¨è¯¥æ–¹æ³•ä¸­ï¼Œé€šè¿‡åˆ©ç”¨åˆ†å¸ƒå¤©çº¿ç½‘ç»œçš„å¤šä¸ªé“¾æ¥çš„å¤šè¾“å…¥å¤šè¾“å‡ºï¼ˆMIMOï¼‰é€šé“çš„æ—¶ç©ºå¤šæ ·æ€§ï¼Œæ”¾å°„å±‚ææˆåƒï¼ˆRTIï¼‰æé«˜äº†å®šä½ç²¾åº¦ã€‚æ­¤å¤–ï¼Œå¼€å‘äº†ä¸€ä¸ªé‡‡ç”¨å·¥ä½œåœ¨ä½äº6 GHzé¢‘æ®µçš„è½¯ä»¶å®šä¹‰æ— çº¿ç”µï¼ˆSDRsï¼‰çš„åŸå‹ç³»ç»Ÿï¼Œå¹¶åœ¨æ¶‰åŠä¸åŒèŠ‚ç‚¹å¯†åº¦å’Œç›®æ ‡ç±»å‹çš„å®¤å†…æ¡ä»¶ä¸‹è¿›è¡Œäº†å…¨é¢çš„è¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å¤§å¤šæ•°åœºæ™¯ä¸­å®ç°äº†äºšç±³çº§å®šä½ç²¾åº¦ï¼Œå¹¶åœ¨å¤æ‚çš„å¤šå¾„ç¯å¢ƒä¸­ä¿æŒäº†ç¨³å¥çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œä½¿ç”¨è´å¶æ–¯ä¼˜åŒ–å¯¹å…³é”®å‚æ•°ï¼ˆå¦‚ç¨€ç–æ€§å’Œè·¯å¾„åšåº¦ï¼‰è¿›è¡Œå¾®è°ƒï¼Œæ˜¾è‘—æé«˜äº†å›¾åƒé‡å»ºè´¨é‡å’Œç›®æ ‡ä¼°è®¡ç²¾åº¦ã€‚è¿™äº›ç»“æœè¯æ˜äº†åŸºäºDANçš„DFLç³»ç»Ÿåœ¨å®ç°å‡†ç¡®ã€ç¨³å¥å’Œå¯æ‰©å±•çš„å®šä½æ–¹é¢çš„å¯è¡Œæ€§å’Œæœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.04085v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºåˆ†å¸ƒå¼å¤©çº¿ç½‘ç»œï¼ˆDANsï¼‰çš„æ— è®¾å¤‡å®šä½ï¼ˆDFLï¼‰æ¡†æ¶ï¼Œé€‚ç”¨äºæœªæ¥6Gæ— çº¿ç”µæ¥å…¥ç½‘ç»œï¼ˆRANsï¼‰ä¸­çš„é›†æˆæ„ŸçŸ¥å’Œé€šä¿¡ï¼ˆISACï¼‰ã€‚è¯¥æ¡†æ¶åˆ©ç”¨æ— çº¿ç”µå±‚ææˆåƒï¼ˆRTIï¼‰æŠ€æœ¯ï¼Œé€šè¿‡åˆ©ç”¨DANsä¸­å¤šé“¾è·¯å¤šè¾“å…¥å¤šè¾“å‡ºï¼ˆMIMOï¼‰é€šé“çš„ç©ºé—´å’Œæ—¶é—´å¤šæ ·æ€§æ¥æé«˜å®šä½ç²¾åº¦ã€‚å¼€å‘äº†ä¸€ä¸ªé‡‡ç”¨è½¯ä»¶å®šä¹‰æ— çº¿ç”µï¼ˆSDRsï¼‰åœ¨ä½äº6GHzé¢‘æ®µè¿è¡Œçš„åŸå‹ç³»ç»Ÿï¼Œå¹¶åœ¨å®¤å†…æ¡ä»¶ä¸‹è¿›è¡Œäº†å…¨é¢çš„è¯„ä¼°ï¼ŒåŒ…æ‹¬ä¸åŒçš„èŠ‚ç‚¹å¯†åº¦å’Œç›®æ ‡ç±»å‹ã€‚ç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å¤§å¤šæ•°åœºæ™¯ä¸‹å®ç°äº†äºšç±³çº§å®šä½ç²¾åº¦ï¼Œå¹¶åœ¨å¤æ‚çš„å¤šè·¯å¾„ç¯å¢ƒä¸‹ä¿æŒäº†ç¨³å¥çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œä½¿ç”¨è´å¶æ–¯ä¼˜åŒ–å¯¹ç¨€ç–æ€§å’Œè·¯å¾„åšåº¦ç­‰å…³é”®å‚æ•°è¿›è¡Œå¾®è°ƒï¼Œæ˜¾è‘—æé«˜äº†å›¾åƒé‡å»ºè´¨é‡å’Œç›®æ ‡ä¼°è®¡ç²¾åº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹çš„åŸºäºåˆ†å¸ƒå¼å¤©çº¿ç½‘ç»œï¼ˆDANsï¼‰çš„æ— è®¾å¤‡å®šä½ï¼ˆDFLï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨ç”¨äºæœªæ¥6Gç½‘ç»œçš„é›†æˆæ„ŸçŸ¥å’Œé€šä¿¡ã€‚</li>
<li>é€šè¿‡æ— çº¿ç”µå±‚ææˆåƒï¼ˆRTIï¼‰æŠ€æœ¯ï¼Œåˆ©ç”¨MIMOé€šé“çš„ç©ºé—´å’Œæ—¶é—´å¤šæ ·æ€§æé«˜å®šä½ç²¾åº¦ã€‚</li>
<li>å¼€å‘äº†é‡‡ç”¨è½¯ä»¶å®šä¹‰æ— çº¿ç”µï¼ˆSDRsï¼‰çš„åŸå‹ç³»ç»Ÿï¼Œå¹¶åœ¨ä½äº6GHzçš„é¢‘æ®µè¿›è¡Œè¯•éªŒã€‚</li>
<li>åœ¨å®¤å†…ç¯å¢ƒä¸‹è¿›è¡Œäº†å…¨é¢çš„è¯„ä¼°ï¼Œæµ‹è¯•ç¯å¢ƒæ¶µç›–ä¸åŒçš„èŠ‚ç‚¹å¯†åº¦å’Œç›®æ ‡ç±»å‹ã€‚</li>
<li>è¯¥æ¡†æ¶å®ç°äº†äºšç±³çº§å®šä½ç²¾åº¦ï¼Œä¸”åœ¨å¤æ‚å¤šè·¯å¾„ç¯å¢ƒä¸­è¡¨ç°ç¨³å¥ã€‚</li>
<li>ä½¿ç”¨è´å¶æ–¯ä¼˜åŒ–å¯¹å…³é”®å‚æ•°è¿›è¡Œå¾®è°ƒï¼Œæ˜¾è‘—æå‡äº†å›¾åƒé‡å»ºè´¨é‡å’Œç›®æ ‡ä¼°è®¡çš„å‡†ç¡®æ€§ã€‚</li>
<li>æ•´ä½“ç»“æœè¡¨æ˜ï¼ŒåŸºäºDANçš„DFLç³»ç»Ÿä¸ºå®ç°å‡†ç¡®ã€ç¨³å¥å’Œå¯æ‰©å±•çš„å®šä½æ˜¯å¯è¡Œçš„å’Œæœ‰æ•ˆçš„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.04085">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c707755cea16d7d22e8d5b66873a1d03.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-35f8783af72419addb6fd19439ae5275.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-823e4a8dbf5bed70f70e9c48297327ef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-18c8c30932cc75c6b06d3c9a6a22c4b1.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Teleios-G305-4-2-2-â€“-the-mystery-of-a-perfectly-shaped-new-Galactic-supernova-remnant"><a href="#Teleios-G305-4-2-2-â€“-the-mystery-of-a-perfectly-shaped-new-Galactic-supernova-remnant" class="headerlink" title="Teleios (G305.4-2.2) â€“ the mystery of a perfectly shaped new Galactic   supernova remnant"></a>Teleios (G305.4-2.2) â€“ the mystery of a perfectly shaped new Galactic   supernova remnant</h2><p><strong>Authors:Miroslav D. Filipovic, Zachary J. Smeaton, Roland Kothes, Silvia Mantovanini, Petar Kostic, Denis Leahy, Adeel Ahmad, Gemma E. Anderson, Miguel Araya, Brianna Ball, Werner Becker, Cristobal Bordiu, Aaron C. Bradley, Robert Brose, Christopher Burger-Scheidlin, Shi Dai, Stefan Duchesne, Timothy J. Galvin, Andrew M. Hopkins, Natasha Hurley-Walker, Barbel S. Koribalski, Sanja Lazarevic, Peter Lundqvist, Jonathan Mackey, Pierrick Martin, Padric McGee, Ana Mitrasinovic, Jeffrey L. Payne, Simone Riggi, Kathryn Ross, Gavin Rowell, Lawrence Rudnick, Hidetoshi Sano, Manami Sasaki, Roberto Soria, Dejan Urosevic, Branislav Vukotic, Jennifer L. West</strong></p>
<p>We present the serendipitous radio-continuum discovery of a likely Galactic supernova remnant (SNR) G305.4-2.2. This object displays a remarkable circular symmetry in shape, making it one of the most circular Galactic SNRs known. Nicknamed Teleios due to its symmetry, it was detected in the new Australian Square Kilometre Array Pathfinder (ASKAP) Evolutionary Map of the Universe (EMU) radio-continuum images with an angular size of 1320â€x1260â€ and PA &#x3D; 0 deg. While there is a hint of possible H$\alpha$ and gamma-ray emission, Teleios is exclusively seen at radio-continuum frequencies. Interestingly, Teleios is not only almost perfectly symmetric, but it also has one of the lowest surface brightnesses discovered among Galactic SNRs and a steep spectral index of $\alpha&#x3D;-0.6\pm 0.3$. Our estimates from HI studies and the Sigma-D relation place Teleios as a type Ia SNR at a distance of either ~2.2 kpc of ~7.7 kpc. This indicates two possible scenarios, either a young (under 1000 yr) or an older SNR (over 10000 yr). With a corresponding diameter of 14&#x2F;48 pc, our evolutionary studies place Teleios at the either early or late Sedov phase, depending on the distance estimate. However, our modelling also predicts X-ray emission, which we do not see in the present generation of eROSITA images. We also explored a type Iax explosion scenario that points to a much closer distance of &lt;1 kpc and Teleios size of only ~3.3 pc, which would be similar to the only known type Iax remnant SN1181. Unfortunately, all examined scenarios have their challenges, and no definitive supernova (SN) origin type can be established at this stage. Teleiosâ€™s symmetrical shape suggests expansion into a rarefied and isotropic ambient medium. The low radio surface brightness and the lack of pronounced polarisation can be explained by a high level of ambient rotation measure (RM), with the largest RM being observed at centre. </p>
<blockquote>
<p>æˆ‘ä»¬æ„å¤–å‘ç°äº†é“¶æ²³ç³»è¶…æ–°æ˜Ÿé—è¿¹ï¼ˆSNRï¼‰G305.4-2.2çš„æ— çº¿ç”µè¿ç»­è°±å‘ç°ã€‚è¯¥å¯¹è±¡å‘ˆç°å‡ºæ˜¾è‘—çš„åœ†å½¢å¯¹ç§°æ€§ï¼Œä½¿å…¶æˆä¸ºå·²çŸ¥æœ€åœ†çš„é“¶æ²³ç³»SNRä¹‹ä¸€ã€‚ç”±äºå…¶å¯¹ç§°æ€§ï¼Œå®ƒè¢«å‘½åä¸ºTeleiosã€‚å®ƒæ˜¯åœ¨æ–°çš„æ¾³å¤§åˆ©äºšå¹³æ–¹å…¬é‡Œé˜µåˆ—æ¢è·¯è€…ï¼ˆASKAPï¼‰å®‡å®™æ¼”åŒ–å›¾ï¼ˆEMUï¼‰æ— çº¿ç”µè¿ç»­è°±å›¾åƒä¸­æ£€æµ‹åˆ°çš„ï¼Œå…¶è§’å¤§å°ä¸º1320â€x1260â€ï¼Œä½ç½®è§’ä¸º0åº¦ã€‚è™½ç„¶æœ‰HÎ±å’ŒÎ³å°„çº¿çš„å¯èƒ½å‘å°„æç¤ºï¼Œä½†Teleiosä»…åœ¨æ— çº¿ç”µè¿ç»­è°±é¢‘ç‡ä¸‹å¯è§ã€‚æœ‰è¶£çš„æ˜¯ï¼ŒTeleiosä¸ä»…å‡ ä¹å®Œå…¨å¯¹ç§°ï¼Œè€Œä¸”å®ƒè¿˜å…·æœ‰é“¶æ²³ç³»SNRä¸­å‘ç°çš„æœ€ä½çš„è¡¨é¢äº®åº¦ä¹‹ä¸€ï¼Œä»¥åŠÎ±&#x3D;-0.6Â±0.3çš„é™¡å³­å…‰è°±æŒ‡æ•°ã€‚æˆ‘ä»¬é€šè¿‡HIç ”ç©¶å’ŒSigma-Då…³ç³»ä¼°è®¡ï¼ŒTeleiosæ˜¯Iaå‹SNRï¼Œè·ç¦»çº¦ä¸º2.2kpcæˆ–7.7kpcã€‚è¿™è¡¨æ˜äº†ä¸¤ç§å¯èƒ½çš„æƒ…å†µï¼Œä¸€æ˜¯å¹´è½»çš„ï¼ˆä¸åˆ°1000å²ï¼‰SNRï¼ŒäºŒæ˜¯è¾ƒè€çš„SNRï¼ˆè¶…è¿‡10000å²ï¼‰ã€‚ç›¸åº”çš„ç›´å¾„ä¸º14&#x2F;48ç§’å·®è·ï¼Œæˆ‘ä»¬çš„è¿›åŒ–ç ”ç©¶å°†Teleiosç½®äºæ—©æœŸçš„å¡å¤šå¤«é˜¶æ®µæˆ–æ™šæœŸå¡å¤šå¤«é˜¶æ®µï¼Œè¿™å–å†³äºè·ç¦»ä¼°è®¡ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬çš„æ¨¡å‹è¿˜é¢„æµ‹äº†Xå°„çº¿å‘å°„ï¼Œè¿™åœ¨å½“å‰çš„eROSITAå›¾åƒä¸­å¹¶æœªè§‚å¯Ÿåˆ°ã€‚æˆ‘ä»¬è¿˜æ¢ç´¢äº†Iaxå‹çˆ†ç‚¸æƒ…æ™¯ï¼Œè¿™æŒ‡å‘äº†å°äº1kpcçš„è¾ƒè¿‘è·ç¦»å’ŒTeleiosåªæœ‰çº¦3.3ç§’å·®è·çš„å¤§å°ï¼Œè¿™ç±»ä¼¼äºå·²çŸ¥çš„å”¯ä¸€Iaxå‹é—è¿¹SN1181ã€‚ä¸å¹¸çš„æ˜¯ï¼Œæ‰€æœ‰è€ƒå¯Ÿçš„æƒ…å†µéƒ½æœ‰å…¶æŒ‘æˆ˜ï¼Œç›®å‰é˜¶æ®µæ— æ³•ç¡®å®šè¶…æ–°æ˜Ÿï¼ˆSNï¼‰çš„èµ·æºç±»å‹ã€‚Teleiosçš„å¯¹ç§°å½¢çŠ¶è¡¨æ˜å…¶è†¨èƒ€åˆ°ä¸€ä¸ªç¨€ç–çš„åŒä½ç´ ç¯å¢ƒä¸­ã€‚ä½æ— çº¿ç”µè¡¨é¢äº®åº¦å’Œç¼ºä¹æ˜æ˜¾çš„æåŒ–å¯ä»¥ç”¨é«˜ç¯å¢ƒæ—‹è½¬åº¦é‡ï¼ˆRMï¼‰æ¥è§£é‡Šï¼Œæœ€å¤§çš„RMè¢«è§‚å¯Ÿåˆ°åœ¨ä¸­å¿ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.04041v1">PDF</a> Has been accepted for publication in PASA</p>
<p><strong>Summary</strong><br>     å‘ç°ä¸€ä¸ªæ–°çš„å¯èƒ½é“¶æ²³ç³»è¶…æ–°æ˜Ÿé—è¿¹G305.4-2.2ï¼Œå‘ˆç°æ˜¾è‘—åœ†å½¢å¯¹ç§°æ€§ï¼Œå‘½åä¸ºTeleiosã€‚é€šè¿‡ASKAPå’ŒEMUçš„å°„ç”µè¿ç»­å›¾åƒæ£€æµ‹ï¼Œåˆæ­¥åˆ¤æ–­ä¸ºå¹´è½»çš„æˆ–å¹´è€çš„è¶…æ–°æ˜Ÿé—è¿¹ï¼Œè·ç¦»çº¦ä¸º2.2kpcæˆ–7.7kpcã€‚æ—©æœŸæˆ–æ™šæœŸSedové˜¶æ®µæ¨¡å‹é¢„æµ‹æœ‰Xå°„çº¿å‘å°„ï¼Œä½†åœ¨ç°æœ‰eROSITAå›¾åƒä¸­æœªè§ã€‚åŒæ—¶æ¢ç´¢äº†å¯èƒ½çš„Iaå‹æˆ–Iaxå‹è¶…æ–°æ˜Ÿçˆ†ç‚¸æƒ…æ™¯ï¼Œä½†ä»æ— æ³•ç¡®å®šå…¶èµ·æºç±»å‹ã€‚å‘¨å›´ä»‹è´¨è¢«è®¤ä¸ºå…·æœ‰é«˜çš„æ—‹è½¬åº¦é‡ï¼ˆRMï¼‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å‘ç°æ–°çš„é“¶æ²³ç³»è¶…æ–°æ˜Ÿé—è¿¹G305.4-2.2ï¼ˆTeleiosï¼‰ï¼Œå…·æœ‰æ˜¾è‘—çš„åœ†å½¢å¯¹ç§°æ€§ã€‚</li>
<li>Teleiosåœ¨å°„ç”µè¿ç»­è°±ä¸Šè¢«å‘ç°ï¼Œä¸”å…·æœ‰éå¸¸ä½çš„è¡¨é¢äº®åº¦å’Œé™¡å³­çš„è°±æŒ‡æ•°Î±&#x3D;-0.6Â±0.3ã€‚</li>
<li>åŸºäºHIç ”ç©¶å’ŒSigma-Då…³ç³»ï¼Œæ¨æµ‹Teleiosçš„è·ç¦»å¯èƒ½ä¸ºçº¦2.2kpcæˆ–çº¦7.7kpcï¼Œåˆ†åˆ«å¯¹åº”å¹´è½»æˆ–å¹´è€çš„è¶…æ–°æ˜Ÿé—è¿¹ã€‚</li>
<li>Teleioså¯èƒ½å¤„äºæ—©æœŸæˆ–æ™šæœŸSedové˜¶æ®µï¼Œå…·ä½“å–å†³äºè·ç¦»ä¼°è®¡ã€‚</li>
<li>é¢„æµ‹å­˜åœ¨Xå°„çº¿å‘å°„ï¼Œä½†åœ¨ç°æœ‰çš„eROSITAå›¾åƒä¸­æœªè§ã€‚</li>
<li>å¯¹å¯èƒ½çš„Iaå‹æˆ–Iaxå‹è¶…æ–°æ˜Ÿçˆ†ç‚¸æƒ…æ™¯è¿›è¡Œäº†æ¢ç´¢ï¼Œä½†æ— æ³•ç¡®å®šå…¶ç¡®åˆ‡çš„SNèµ·æºç±»å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.04041">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5dbb312e969e97aecb1edb9bd08815cf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-01c7f608559c810257b0cf349897e78a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2766629cd16cf8fee162d90f42b15d76.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0d002064cca4dacee6b3ec4b8a504a46.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5a5a2a3a2b0b7e673d88cf66ddf7197c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b6c756f350ff3f62267ad94132295bc6.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="IntelliCardiac-An-Intelligent-Platform-for-Cardiac-Image-Segmentation-and-Classification"><a href="#IntelliCardiac-An-Intelligent-Platform-for-Cardiac-Image-Segmentation-and-Classification" class="headerlink" title="IntelliCardiac: An Intelligent Platform for Cardiac Image Segmentation   and Classification"></a>IntelliCardiac: An Intelligent Platform for Cardiac Image Segmentation   and Classification</h2><p><strong>Authors:Ting Yu Tsai, An Yu, Meghana Spurthi Maadugundu, Ishrat Jahan Mohima, Umme Habiba Barsha, Mei-Hwa F. Chen, Balakrishnan Prabhakaran, Ming-Ching Chang</strong></p>
<p>Precise and effective processing of cardiac imaging data is critical for the identification and management of the cardiovascular diseases. We introduce IntelliCardiac, a comprehensive, web-based medical image processing platform for the automatic segmentation of 4D cardiac images and disease classification, utilizing an AI model trained on the publicly accessible ACDC dataset. The system, intended for patients, cardiologists, and healthcare professionals, offers an intuitive interface and uses deep learning models to identify essential heart structures and categorize cardiac diseases. The system supports analysis of both the right and left ventricles as well as myocardium, and then classifies patientâ€™s cardiac images into five diagnostic categories: dilated cardiomyopathy, myocardial infarction, hypertrophic cardiomyopathy, right ventricular abnormality, and no disease. IntelliCardiac combines a deep learning-based segmentation model with a two-step classification pipeline. The segmentation module gains an overall accuracy of 92.6%. The classification module, trained on characteristics taken from segmented heart structures, achieves 98% accuracy in five categories. These results exceed the performance of the existing state-of-the-art methods that integrate both segmentation and classification models. IntelliCardiac, which supports real-time visualization, workflow integration, and AI-assisted diagnostics, has great potential as a scalable, accurate tool for clinical decision assistance in cardiac imaging and diagnosis. </p>
<blockquote>
<p>ç²¾ç¡®ä¸”æœ‰æ•ˆåœ°å¤„ç†å¿ƒè„æˆåƒæ•°æ®å¯¹äºå¿ƒè¡€ç®¡ç–¾ç—…çš„è¯†åˆ«å’Œç®¡ç†è‡³å…³é‡è¦ã€‚æˆ‘ä»¬æ¨å‡ºIntelliCardiacï¼Œè¿™æ˜¯ä¸€æ¬¾å…¨é¢çš„åŸºäºç½‘é¡µçš„åŒ»å­¦å›¾åƒå¤„ç†å¹³å°ï¼Œç”¨äºè‡ªåŠ¨åˆ†å‰²4Då¿ƒè„å›¾åƒå’Œç–¾ç—…åˆ†ç±»ã€‚å®ƒåˆ©ç”¨åœ¨å…¬å…±å¯è®¿é—®çš„ACDCæ•°æ®é›†ä¸Šè®­ç»ƒçš„AIæ¨¡å‹ã€‚æ­¤ç³»ç»Ÿé¢å‘æ‚£è€…ã€å¿ƒè„ç—…ä¸“å®¶ä»¥åŠåŒ»ç–—ä¸“ä¸šäººå£«ï¼Œæä¾›ç›´è§‚ç•Œé¢ï¼Œå¹¶ä½¿ç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹æ¥è¯†åˆ«å¿ƒè„çš„å…³é”®ç»“æ„å¹¶å¯¹å¿ƒè„ç–¾ç—…è¿›è¡Œåˆ†ç±»ã€‚ç³»ç»Ÿæ”¯æŒå¯¹å³å¿ƒå®¤å’Œå·¦å¿ƒå®¤ä»¥åŠå¿ƒè‚Œçš„åˆ†æï¼Œç„¶åå°†æ‚£è€…çš„å¿ƒè„å›¾åƒåˆ†ç±»ä¸ºäº”ç§è¯Šæ–­ç±»åˆ«ï¼šæ‰©å¼ å‹å¿ƒè‚Œç—…ã€å¿ƒè‚Œæ¢—æ­»ã€è‚¥åšå‹å¿ƒè‚Œç—…ã€å³å¿ƒå®¤å¼‚å¸¸ä»¥åŠæ— ç–¾ç—…ã€‚IntelliCardiacç»“åˆäº†åŸºäºæ·±åº¦å­¦ä¹ çš„åˆ†å‰²æ¨¡å‹ä¸ä¸¤æ­¥åˆ†ç±»æµç¨‹ã€‚åˆ†å‰²æ¨¡å—çš„æ•´ä½“å‡†ç¡®ç‡ä¸º92.6%ã€‚åˆ†ç±»æ¨¡å—åŸºäºåˆ†å‰²å¿ƒè„ç»“æ„çš„ç‰¹å¾è¿›è¡Œè®­ç»ƒï¼Œåœ¨äº”ä¸ªç±»åˆ«ä¸­è¾¾åˆ°98%çš„å‡†ç¡®ç‡ã€‚è¿™äº›ç»“æœè¶…è¿‡äº†ç°æœ‰æœ€å…ˆè¿›çš„é›†æˆåˆ†å‰²å’Œåˆ†ç±»æ¨¡å‹çš„æ–¹æ³•çš„è¡¨ç°ã€‚IntelliCardiacæ”¯æŒå®æ—¶å¯è§†åŒ–ã€å·¥ä½œæµç¨‹é›†æˆå’Œäººå·¥æ™ºèƒ½è¾…åŠ©è¯Šæ–­ï¼Œä½œä¸ºå¿ƒè„æˆåƒå’Œè¯Šæ–­çš„ä¸´åºŠå†³ç­–è¾…åŠ©å·¥å…·ï¼Œå…·æœ‰å¯æ‰©å±•æ€§å’Œå‡†ç¡®æ€§ï¼Œå…·æœ‰å·¨å¤§çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.03838v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†IntelliCardiacè¿™ä¸€åŸºäºç½‘ç»œçš„åŒ»å­¦å›¾åƒå¤„ç†å¹³å°ï¼Œç”¨äºè‡ªåŠ¨åˆ†å‰²å››ç»´å¿ƒè„å›¾åƒå’Œç–¾ç—…åˆ†ç±»ã€‚å®ƒåˆ©ç”¨åœ¨å…¬å¼€å¯è®¿é—®çš„ACDCæ•°æ®é›†ä¸Šè®­ç»ƒçš„AIæ¨¡å‹ï¼Œæä¾›å¯¹æ‚£è€…ã€å¿ƒè„ç—…ä¸“å®¶å’ŒåŒ»ç–—ä¿å¥ä¸“ä¸šäººå‘˜çš„ç›´è§‚ç•Œé¢ï¼Œå¹¶ä½¿ç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹è¯†åˆ«å¿ƒè„ç»“æ„å¹¶è¿›è¡Œåˆ†ç±»ã€‚è¯¥å¹³å°æ”¯æŒå·¦å³å¿ƒå®¤å’Œå¿ƒè‚Œçš„åˆ†æï¼Œå¹¶å°†æ‚£è€…çš„å¿ƒè„å›¾åƒåˆ†ä¸ºäº”ä¸ªè¯Šæ–­ç±»åˆ«ã€‚IntelliCardiacç»“åˆäº†æ·±åº¦å­¦ä¹ åˆ†å‰²æ¨¡å‹å’Œä¸¤æ­¥åˆ†ç±»ç®¡é“ï¼Œå…¶æ€§èƒ½ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›çš„é›†æˆåˆ†å‰²å’Œåˆ†ç±»æ¨¡å‹çš„æ–¹æ³•ã€‚å®ƒå…·æœ‰è‰¯å¥½çš„å¯æ‰©å±•æ€§å’Œå‡†ç¡®æ€§ï¼Œå¯ä½œä¸ºå¿ƒè„æˆåƒå’Œè¯Šæ–­çš„ä¸´åºŠå†³ç­–è¾…åŠ©å·¥å…·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>IntelliCardiacæ˜¯ä¸€ä¸ªç”¨äºè‡ªåŠ¨åˆ†å‰²å››ç»´å¿ƒè„å›¾åƒå’Œç–¾ç—…åˆ†ç±»çš„åŒ»å­¦å›¾åƒå¤„ç†å¹³å°ã€‚</li>
<li>å¹³å°ä½¿ç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹è¿›è¡Œå¿ƒè„ç»“æ„è¯†åˆ«å’Œç–¾ç—…åˆ†ç±»ã€‚</li>
<li>æ”¯æŒå·¦å³å¿ƒå®¤å’Œå¿ƒè‚Œçš„åˆ†æï¼Œåˆ†ä¸ºäº”ä¸ªè¯Šæ–­ç±»åˆ«ã€‚</li>
<li>IntelliCardiacç»“åˆäº†æ·±åº¦å­¦ä¹ åˆ†å‰²æ¨¡å‹å’Œä¸¤æ­¥åˆ†ç±»ç®¡é“ã€‚</li>
<li>åˆ†å‰²æ¨¡å‹çš„å‡†ç¡®åº¦ä¸º92.6%ï¼Œåˆ†ç±»æ¨¡å‹çš„å‡†ç¡®åº¦ä¸º98%ã€‚</li>
<li>ä¸ç°æœ‰çš„é›†æˆåˆ†å‰²å’Œåˆ†ç±»æ¨¡å‹çš„æ–¹æ³•ç›¸æ¯”ï¼ŒIntelliCardiacçš„æ€§èƒ½æ›´ä¸ºä¼˜è¶Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.03838">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5037aafe40bd4b469837b6807b986688.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a15755d537f500447dcf3fcc3c146cac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3608cd25951f0b2fc10788ff462a7257.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dfb431549855a8a85dd34c98bfac9704.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-752e3cd3412f80d43099d556cbf0ef54.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-56bfddbf7c70202ecd1577dbf4f0384f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5db69e6b4914d0f89deeefa779e0db63.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="mAIstro-an-open-source-multi-agentic-system-for-automated-end-to-end-development-of-radiomics-and-deep-learning-models-for-medical-imaging"><a href="#mAIstro-an-open-source-multi-agentic-system-for-automated-end-to-end-development-of-radiomics-and-deep-learning-models-for-medical-imaging" class="headerlink" title="mAIstro: an open-source multi-agentic system for automated end-to-end   development of radiomics and deep learning models for medical imaging"></a>mAIstro: an open-source multi-agentic system for automated end-to-end   development of radiomics and deep learning models for medical imaging</h2><p><strong>Authors:Eleftherios Tzanis, Michail E. Klontzas</strong></p>
<p>Agentic systems built on large language models (LLMs) offer promising capabilities for automating complex workflows in healthcare AI. We introduce mAIstro, an open-source, autonomous multi-agentic framework for end-to-end development and deployment of medical AI models. The system orchestrates exploratory data analysis, radiomic feature extraction, image segmentation, classification, and regression through a natural language interface, requiring no coding from the user. Built on a modular architecture, mAIstro supports both open- and closed-source LLMs, and was evaluated using a large and diverse set of prompts across 16 open-source datasets, covering a wide range of imaging modalities, anatomical regions, and data types. The agents successfully executed all tasks, producing interpretable outputs and validated models. This work presents the first agentic framework capable of unifying data analysis, AI model development, and inference across varied healthcare applications, offering a reproducible and extensible foundation for clinical and research AI integration. The code is available at: <a target="_blank" rel="noopener" href="https://github.com/eltzanis/mAIstro">https://github.com/eltzanis/mAIstro</a> </p>
<blockquote>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„Agenticç³»ç»Ÿä¸ºåŒ»ç–—ä¿å¥AIä¸­å¤æ‚å·¥ä½œæµçš„è‡ªåŠ¨åŒ–æä¾›äº†æœ‰å‰é€”çš„èƒ½åŠ›ã€‚æˆ‘ä»¬ä»‹ç»äº†mAIstroï¼Œè¿™æ˜¯ä¸€ä¸ªå¼€æºçš„ã€è‡ªä¸»çš„å¤šAgenticæ¡†æ¶ï¼Œç”¨äºç«¯åˆ°ç«¯çš„åŒ»ç–—AIæ¨¡å‹å¼€å‘å’Œéƒ¨ç½²ã€‚è¯¥ç³»ç»Ÿé€šè¿‡è‡ªç„¶è¯­è¨€æ¥å£åè°ƒæ¢ç´¢æ€§æ•°æ®åˆ†æã€æ”¾å°„å­¦ç‰¹å¾æå–ã€å›¾åƒåˆ†å‰²ã€åˆ†ç±»å’Œå›å½’ï¼Œæ— éœ€ç”¨æˆ·ç¼–å†™ä»£ç ã€‚mAIstroé‡‡ç”¨æ¨¡å—åŒ–æ¶æ„ï¼Œæ”¯æŒå¼€æºå’Œé—­æºçš„LLMï¼Œå¹¶ä½¿ç”¨æ¶µç›–å¹¿æ³›æˆåƒæ¨¡å¼ã€è§£å‰–éƒ¨ä½å’Œæ•°æ®ç±»å‹çš„16ä¸ªå¼€æºæ•°æ®é›†çš„å¤§å‹å’Œå¤šæ ·åŒ–çš„æç¤ºé›†è¿›è¡Œè¯„ä¼°ã€‚ä»£ç†æˆåŠŸæ‰§è¡Œäº†æ‰€æœ‰ä»»åŠ¡ï¼Œäº§ç”Ÿäº†å¯è§£é‡Šçš„è¾“å‡ºå’Œç»è¿‡éªŒè¯çš„æ¨¡å‹ã€‚è¿™é¡¹å·¥ä½œæå‡ºäº†ç¬¬ä¸€ä¸ªèƒ½å¤Ÿåœ¨å„ç§åŒ»ç–—ä¿å¥åº”ç”¨ä¸­ç»Ÿä¸€æ•°æ®åˆ†æã€AIæ¨¡å‹å¼€å‘å’Œæ¨ç†çš„Agenticæ¡†æ¶ï¼Œä¸ºä¸´åºŠå’Œç ”ç©¶AIé›†æˆæä¾›äº†å¯å¤åˆ¶å’Œå¯æ‰©å±•çš„åŸºç¡€ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/eltzanis/mAIstro%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/eltzanis/mAIstroæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.03785v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„Agenticç³»ç»Ÿä¸ºåŒ»ç–—ä¿å¥AIè‡ªåŠ¨åŒ–å¤æ‚å·¥ä½œæµç¨‹æä¾›äº†æœ‰å‰æ™¯çš„èƒ½åŠ›ã€‚æœ¬æ–‡ä»‹ç»äº†mAIstroï¼Œä¸€ä¸ªå¼€æºçš„ã€è‡ªä¸»çš„å¤šAgenticæ¡†æ¶ï¼Œç”¨äºç«¯åˆ°ç«¯åŒ»ç–—AIæ¨¡å‹çš„å¼€å‘å’Œéƒ¨ç½²ã€‚è¯¥ç³»ç»Ÿé€šè¿‡è‡ªç„¶è¯­è¨€æ¥å£åè°ƒæ¢ç´¢æ€§åˆ†æã€æ”¾å°„ç‰¹å¾æå–ã€å›¾åƒåˆ†å‰²ã€åˆ†ç±»å’Œå›å½’ï¼Œæ— éœ€ç”¨æˆ·ç¼–ç¨‹ã€‚åœ¨è·¨å¤šä¸ªå¼€æºæ•°æ®é›†ã€æ¶µç›–å¤šç§æˆåƒæ¨¡å¼ã€è§£å‰–éƒ¨ä½å’Œæ•°æ®ç±»å‹çš„å¹¿æ³›è¯„ä¼°ä¸­ï¼ŒagentsæˆåŠŸæ‰§è¡Œäº†æ‰€æœ‰ä»»åŠ¡ï¼Œäº§ç”Ÿäº†å¯è§£é‡Šçš„è¾“å‡ºå’Œç»è¿‡éªŒè¯çš„æ¨¡å‹ã€‚è¿™é¡¹å·¥ä½œæ˜¯ç¬¬ä¸€ä¸ªèƒ½å¤Ÿåœ¨å„ç§åŒ»ç–—ä¿å¥åº”ç”¨ä¸­ç»Ÿä¸€æ•°æ®åˆ†æã€AIæ¨¡å‹å¼€å‘å’Œæ¨ç†çš„Agenticæ¡†æ¶ï¼Œä¸ºä¸´åºŠå’Œç ”ç©¶AIçš„èåˆæä¾›äº†å¯å¤åˆ¶å’Œå¯æ‰©å±•çš„åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Agenticç³»ç»ŸåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œä¸ºåŒ»ç–—ä¿å¥AIè‡ªåŠ¨åŒ–æä¾›å‰æ™¯ã€‚</li>
<li>mAIstroæ˜¯ä¸€ä¸ªå¼€æºçš„ã€è‡ªä¸»çš„å¤šAgenticæ¡†æ¶ï¼Œæ”¯æŒåŒ»ç–—AIæ¨¡å‹ç«¯åˆ°ç«¯çš„å¼€å‘å’Œéƒ¨ç½²ã€‚</li>
<li>mAIstroé€šè¿‡è‡ªç„¶è¯­è¨€æ¥å£è¿›è¡Œæ¢ç´¢æ€§åˆ†æã€æ”¾å°„ç‰¹å¾æå–ã€å›¾åƒåˆ†å‰²ç­‰ä»»åŠ¡ã€‚</li>
<li>ç³»ç»Ÿæ— éœ€ç”¨æˆ·ç¼–ç¨‹ï¼Œå¯åè°ƒå¤šç§ä»»åŠ¡ã€‚</li>
<li>mAIstroæ”¯æŒå¼€æºå’Œé—­æºçš„LLMã€‚</li>
<li>Agentsåœ¨å¹¿æ³›çš„æ•°æ®é›†ä¸ŠæˆåŠŸæ‰§è¡Œä»»åŠ¡ï¼Œäº§ç”Ÿå¯è§£é‡Šçš„è¾“å‡ºå’Œç»è¿‡éªŒè¯çš„æ¨¡å‹ã€‚</li>
<li>mAIstroä¸ºä¸´åºŠå’Œç ”ç©¶AIçš„èåˆæä¾›äº†å¯å¤åˆ¶å’Œå¯æ‰©å±•çš„åŸºç¡€ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.03785">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-aac96b3ebfbae988da12a816938a33ca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0c61218385cef034302cdb39bae5efba.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="RESAnything-Attribute-Prompting-for-Arbitrary-Referring-Segmentation"><a href="#RESAnything-Attribute-Prompting-for-Arbitrary-Referring-Segmentation" class="headerlink" title="RESAnything: Attribute Prompting for Arbitrary Referring Segmentation"></a>RESAnything: Attribute Prompting for Arbitrary Referring Segmentation</h2><p><strong>Authors:Ruiqi Wang, Hao Zhang</strong></p>
<p>We present an open-vocabulary and zero-shot method for arbitrary referring expression segmentation (RES), targeting input expressions that are more general than what prior works were designed to handle. Specifically, our inputs encompass both object- and part-level labels as well as implicit references pointing to properties or qualities of object&#x2F;part function, design, style, material, etc. Our model, coined RESAnything, leverages Chain-of-Thoughts (CoT) reasoning, where the key idea is attribute prompting. We generate detailed descriptions of object&#x2F;part attributes including shape, color, and location for potential segment proposals through systematic prompting of a large language model (LLM), where the proposals are produced by a foundational image segmentation model. Our approach encourages deep reasoning about object or part attributes related to function, style, design, etc., enabling the system to handle implicit queries without any part annotations for training or fine-tuning. As the first zero-shot and LLM-based RES method, RESAnything achieves clearly superior performance among zero-shot methods on traditional RES benchmarks and significantly outperforms existing methods on challenging scenarios involving implicit queries and complex part-level relations. Finally, we contribute a new benchmark dataset to offer ~3K carefully curated RES instances to assess part-level, arbitrary RES solutions. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºä»»æ„å¼•ç”¨è¡¨è¾¾å¼åˆ†å‰²ï¼ˆRESï¼‰çš„å¼€æ”¾è¯æ±‡å’Œé›¶æ ·æœ¬æ–¹æ³•ï¼Œé’ˆå¯¹çš„è¾“å…¥è¡¨è¾¾å¼æ¯”ä»¥å¾€å·¥ä½œè®¾è®¡çš„æ›´ä¸ºé€šç”¨ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„è¾“å…¥æ¶µç›–äº†å¯¹è±¡çº§å’Œéƒ¨ä»¶çº§æ ‡ç­¾ï¼Œä»¥åŠæŒ‡å‘å¯¹è±¡&#x2F;éƒ¨ä»¶åŠŸèƒ½ã€è®¾è®¡ã€é£æ ¼ã€æè´¨ç­‰å±æ€§çš„éšå«å¼•ç”¨ã€‚æˆ‘ä»¬çš„æ¨¡å‹è¢«ç§°ä¸ºRESAnythingï¼Œå®ƒåˆ©ç”¨Chain-of-Thoughtsï¼ˆCoTï¼‰æ¨ç†ï¼Œå…³é”®æ€æƒ³æ˜¯å±æ€§æç¤ºã€‚æˆ‘ä»¬é€šè¿‡ç³»ç»Ÿæç¤ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥ç”Ÿæˆå¯¹è±¡&#x2F;éƒ¨ä»¶å±æ€§çš„è¯¦ç»†æè¿°ï¼ŒåŒ…æ‹¬å½¢çŠ¶ã€é¢œè‰²å’Œä½ç½®ç­‰ï¼Œä¸ºæ½œåœ¨çš„åˆ†æ®µææ¡ˆæä¾›æ”¯æŒã€‚è¿™äº›ææ¡ˆç”±åŸºç¡€å›¾åƒåˆ†å‰²æ¨¡å‹ç”Ÿæˆã€‚æˆ‘ä»¬çš„æ–¹æ³•é¼“åŠ±å¯¹å¯¹è±¡æˆ–éƒ¨ä»¶å±æ€§ï¼ˆå¦‚åŠŸèƒ½ã€é£æ ¼ã€è®¾è®¡ç­‰ï¼‰è¿›è¡Œæ·±åº¦æ¨ç†ï¼Œä½¿ç³»ç»Ÿèƒ½å¤Ÿå¤„ç†éšå¼æŸ¥è¯¢ï¼Œè€Œæ— éœ€ä»»ä½•éƒ¨åˆ†æ³¨é‡Šè¿›è¡Œè®­ç»ƒæˆ–å¾®è°ƒã€‚ä½œä¸ºåŸºäºé›¶æ ·æœ¬å’ŒLLMçš„RESæ–¹æ³•ï¼ŒRESAnythingåœ¨ä¼ ç»Ÿçš„RESåŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†æ˜æ˜¾çš„ä¼˜åŠ¿æ€§èƒ½ï¼Œå¹¶ä¸”åœ¨æ¶‰åŠéšå¼æŸ¥è¯¢å’Œå¤æ‚éƒ¨ä»¶çº§å…³ç³»çš„æŒ‘æˆ˜æ€§åœºæ™¯ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚æœ€åï¼Œæˆ‘ä»¬è´¡çŒ®äº†ä¸€ä¸ªæ–°çš„åŸºå‡†æ•°æ®é›†ï¼Œæä¾›äº†çº¦3Kä¸ªç²¾å¿ƒç­–åˆ’çš„RESå®ä¾‹ï¼Œä»¥è¯„ä¼°éƒ¨ä»¶çº§ã€ä»»æ„RESè§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.02867v1">PDF</a> 42 pages, 31 figures. For more details:   <a target="_blank" rel="noopener" href="https://suikei-wang.github.io/RESAnything/">https://suikei-wang.github.io/RESAnything/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§å¼€æ”¾è¯æ±‡è¡¨å’Œé›¶æ ·æœ¬æ–¹æ³•ï¼Œç”¨äºå¤„ç†ä»»æ„å¼•ç”¨è¡¨è¾¾å¼åˆ†å‰²ï¼ˆRESï¼‰ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿå¤„ç†æ¯”ä»¥å¾€ç ”ç©¶æ›´ä¸€èˆ¬çš„è¾“å…¥è¡¨è¾¾å¼ã€‚é€šè¿‡Chain-of-Thoughtsï¼ˆCoTï¼‰æ¨ç†å’Œå±æ€§æç¤ºï¼Œæ¨¡å‹RESAnythingèƒ½å¤Ÿè¯¦ç»†æè¿°å¯¹è±¡&#x2F;éƒ¨åˆ†çš„å±æ€§ï¼Œå¦‚å½¢çŠ¶ã€é¢œè‰²å’Œä½ç½®ï¼Œä»¥äº§ç”Ÿæ½œåœ¨çš„åˆ†æ®µå»ºè®®ã€‚è¯¥æ–¹æ³•é¼“åŠ±å¯¹å¯¹è±¡æˆ–éƒ¨åˆ†çš„å±æ€§è¿›è¡Œæ·±åº¦æ¨ç†ï¼Œèƒ½å¤Ÿå¤„ç†éšå¼æŸ¥è¯¢ï¼Œæ— éœ€å¯¹è®­ç»ƒæˆ–å¾®è°ƒè¿›è¡Œéƒ¨åˆ†æ³¨é‡Šã€‚ä½œä¸ºç¬¬ä¸€ä¸ªåŸºäºé›¶æ ·æœ¬å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„RESæ–¹æ³•ï¼ŒRESAnythingåœ¨ä¼ ç»Ÿçš„RESåŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–é›¶æ ·æœ¬æ–¹æ³•ï¼Œåœ¨å¤„ç†æ¶‰åŠéšå¼æŸ¥è¯¢å’Œå¤æ‚éƒ¨åˆ†çº§å…³ç³»çš„æŒ‘æˆ˜åœºæ™¯æ—¶è¡¨ç°å‡ºæ˜¾è‘—çš„ä¼˜åŠ¿ã€‚æœ€åï¼Œæœ¬æ–‡è´¡çŒ®äº†ä¸€ä¸ªæ–°çš„åŸºå‡†æ•°æ®é›†ï¼ŒåŒ…å«çº¦3Kä¸ªç²¾å¿ƒæŒ‘é€‰çš„RESå®ä¾‹ï¼Œä»¥è¯„ä¼°éƒ¨åˆ†çº§ã€ä»»æ„çš„RESè§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§å¼€æ”¾è¯æ±‡è¡¨å’Œé›¶æ ·æœ¬æ–¹æ³•ç”¨äºä»»æ„å¼•ç”¨è¡¨è¾¾å¼åˆ†å‰²ï¼ˆRESï¼‰ã€‚</li>
<li>æ¨¡å‹RESAnythingèƒ½å¤Ÿå¤„ç†æ›´ä¸€èˆ¬çš„è¾“å…¥è¡¨è¾¾å¼ï¼Œæ¶µç›–å¯¹è±¡çº§å’Œéƒ¨åˆ†çº§çš„æ ‡ç­¾ä»¥åŠæŒ‡å‘å±æ€§æˆ–åŠŸèƒ½çš„éšå¼å¼•ç”¨ã€‚</li>
<li>é‡‡ç”¨Chain-of-Thoughtsï¼ˆCoTï¼‰æ¨ç†å’Œå±æ€§æç¤ºï¼Œç”Ÿæˆè¯¦ç»†çš„å¯¹è±¡&#x2F;éƒ¨åˆ†å±æ€§æè¿°ã€‚</li>
<li>é¼“åŠ±å¯¹å¯¹è±¡æˆ–éƒ¨åˆ†çš„å±æ€§è¿›è¡Œæ·±åº¦æ¨ç†ï¼Œèƒ½å¤„ç†éšå¼æŸ¥è¯¢ï¼Œæ— éœ€å¯¹è®­ç»ƒæˆ–å¾®è°ƒè¿›è¡Œéƒ¨åˆ†æ³¨é‡Šã€‚</li>
<li>RESAnythingåœ¨RESåŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–é›¶æ ·æœ¬æ–¹æ³•ï¼Œç‰¹åˆ«æ“…é•¿å¤„ç†å¤æ‚åœºæ™¯å’Œéšå¼æŸ¥è¯¢ã€‚</li>
<li>å¼•å…¥äº†ä¸€ä¸ªæ–°çš„åŸºå‡†æ•°æ®é›†ï¼ŒåŒ…å«çº¦3Kä¸ªRESå®ä¾‹ï¼Œç”¨äºè¯„ä¼°éƒ¨åˆ†çº§ã€ä»»æ„çš„RESè§£å†³æ–¹æ¡ˆã€‚</li>
<li>è¯¥æ–¹æ³•æ¨åŠ¨äº†åŒ»å­¦å›¾åƒåˆ†å‰²æŠ€æœ¯çš„å‘å±•ï¼Œå°¤å…¶æ˜¯ä»»æ„å¼•ç”¨è¡¨è¾¾å¼åˆ†å‰²çš„å¤„ç†èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.02867">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-838c2f525b419bc4b84318c21c78fbce.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-82fb20b90fa5f9f9590597efc8399386.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ae62dc9b97c4f8f1dbf3643150b81bc5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f6335bf5ff638853d90a9afcefdce353.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a38f640a35f562c0dddee0c1cdf120e0.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Advances-in-Automated-Fetal-Brain-MRI-Segmentation-and-Biometry-Insights-from-the-FeTA-2024-Challenge"><a href="#Advances-in-Automated-Fetal-Brain-MRI-Segmentation-and-Biometry-Insights-from-the-FeTA-2024-Challenge" class="headerlink" title="Advances in Automated Fetal Brain MRI Segmentation and Biometry:   Insights from the FeTA 2024 Challenge"></a>Advances in Automated Fetal Brain MRI Segmentation and Biometry:   Insights from the FeTA 2024 Challenge</h2><p><strong>Authors:Vladyslav Zalevskyi, Thomas Sanchez, Misha Kaandorp, Margaux Roulet, Diego Fajardo-Rojas, Liu Li, Jana Hutter, Hongwei Bran Li, Matthew Barkovich, Hui Ji, Luca Wilhelmi, Aline DÃ¤ndliker, CÃ©line Steger, MÃ©riam Koob, Yvan Gomez, Anton JakovÄiÄ‡, Melita KlaiÄ‡, Ana AdÅ¾iÄ‡, Pavel MarkoviÄ‡, Gracia GrabariÄ‡, Milan Rados, Jordina Aviles Verdera, Gregor Kasprian, Gregor Dovjak, Raphael Gaubert-RachmÃ¼hl, Maurice Aschwanden, Qi Zeng, Davood Karimi, Denis Peruzzo, Tommaso Ciceri, Giorgio Longari, Rachika E. Hamadache, Amina Bouzid, Xavier LladÃ³, Simone Chiarella, Gerard MartÃ­-Juan, Miguel Ãngel GonzÃ¡lez Ballester, Marco Castellaro, Marco Pinamonti, Valentina Visani, Robin Cremese, KeÃ¯n Sam, Fleur Gaudfernau, Param Ahir, Mehul Parikh, Maximilian Zenk, Michael Baumgartner, Klaus Maier-Hein, Li Tianhong, Yang Hong, Zhao Longfei, Domen Preloznik, Å½iga Å piclin, Jae Won Choi, Muyang Li, Jia Fu, Guotai Wang, Jingwen Jiang, Lyuyang Tong, Bo Du, Milton O. Candela-Leal, Andrea Gondova, Sungmin You, Abdul Qayyum, Moona Mazher, Steven A Niederer, Andras Jakab, Roxane Licandro, Kelly Payette, Meritxell Bach Cuadra</strong></p>
<p>Accurate fetal brain tissue segmentation and biometric analysis are essential for studying brain development in utero. The FeTA Challenge 2024 advanced automated fetal brain MRI analysis by introducing biometry prediction as a new task alongside tissue segmentation. For the first time, our diverse multi-centric test set included data from a new low-field (0.55T) MRI dataset. Evaluation metrics were also expanded to include the topology-specific Euler characteristic difference (ED). Sixteen teams submitted segmentation methods, most of which performed consistently across both high- and low-field scans. However, longitudinal trends indicate that segmentation accuracy may be reaching a plateau, with results now approaching inter-rater variability. The ED metric uncovered topological differences that were missed by conventional metrics, while the low-field dataset achieved the highest segmentation scores, highlighting the potential of affordable imaging systems when paired with high-quality reconstruction. Seven teams participated in the biometry task, but most methods failed to outperform a simple baseline that predicted measurements based solely on gestational age, underscoring the challenge of extracting reliable biometric estimates from image data alone. Domain shift analysis identified image quality as the most significant factor affecting model generalization, with super-resolution pipelines also playing a substantial role. Other factors, such as gestational age, pathology, and acquisition site, had smaller, though still measurable, effects. Overall, FeTA 2024 offers a comprehensive benchmark for multi-class segmentation and biometry estimation in fetal brain MRI, underscoring the need for data-centric approaches, improved topological evaluation, and greater dataset diversity to enable clinically robust and generalizable AI tools. </p>
<blockquote>
<p>å‡†ç¡®èƒå„¿è„‘ç»„ç»‡åˆ†å‰²å’Œç”Ÿç‰©æµ‹å®šåˆ†æå¯¹äºç ”ç©¶èƒå„¿æœŸçš„å¤§è„‘å‘è‚²è‡³å…³é‡è¦ã€‚FeTA Challenge 2024é€šè¿‡å¼•å…¥ç”Ÿç‰©æµ‹å®šé¢„æµ‹ä½œä¸ºæ–°ä»»åŠ¡ï¼Œæ¨åŠ¨äº†èƒå„¿è„‘éƒ¨MRIåˆ†æçš„è‡ªåŠ¨åŒ–å‘å±•ï¼ŒåŒæ—¶è¾…ä»¥ç»„ç»‡åˆ†å‰²ã€‚æˆ‘ä»¬çš„å¤šæ ·åŒ–å¤šä¸­å¿ƒæµ‹è¯•é›†é¦–æ¬¡åŒ…å«æ¥è‡ªæ–°ä½åœºï¼ˆ0.55Tï¼‰MRIæ•°æ®é›†çš„æ•°æ®ã€‚è¯„ä¼°æŒ‡æ ‡ä¹Ÿå¾—ä»¥æ‰©å……ï¼ŒåŒ…æ‹¬é’ˆå¯¹æ‹“æ‰‘ç‰¹æ€§çš„æ¬§æ‹‰ç‰¹å¾å·®å¼‚ï¼ˆEDï¼‰ã€‚æœ‰åå…­æ”¯é˜Ÿä¼æäº¤äº†åˆ†å‰²æ–¹æ³•ï¼Œå…¶ä¸­å¤§å¤šæ•°åœ¨é«˜åœºå’Œä½åœºæ‰«æä¸­éƒ½è¡¨ç°ç¨³å®šã€‚ç„¶è€Œçºµå‘è¶‹åŠ¿è¡¨æ˜ï¼Œåˆ†å‰²ç²¾åº¦å¯èƒ½å·²è¾¾åˆ°ä¸€ä¸ªç“¶é¢ˆæœŸï¼Œå…¶ç»“æœç°å·²æ¥è¿‘è§‚å¯Ÿè€…é—´å˜å¼‚åº¦ã€‚EDæŒ‡æ ‡å‘ç°äº†ä¼ ç»ŸæŒ‡æ ‡æœªèƒ½è¦†ç›–çš„æ‹“æ‰‘å·®å¼‚ï¼ŒåŒæ—¶ä½åœºæ•°æ®é›†è·å¾—äº†æœ€é«˜çš„åˆ†å‰²å¾—åˆ†ï¼Œè¿™çªæ˜¾äº†å½“ä¸é«˜è´¨é‡é‡å»ºç›¸ç»“åˆæ—¶ï¼Œå¹³ä»·æˆåƒç³»ç»Ÿçš„æ½œåŠ›ã€‚æœ‰ä¸ƒæ”¯é˜Ÿä¼å‚ä¸äº†ç”Ÿç‰©æµ‹å®šä»»åŠ¡ï¼Œä½†å¤§å¤šæ•°æ–¹æ³•æœªèƒ½è¶…è¶Šä¸€ä¸ªç®€å•åŸºçº¿ï¼Œè¯¥åŸºçº¿ä»…æ ¹æ®èƒé¾„è¿›è¡Œé¢„æµ‹æµ‹é‡ï¼Œè¿™çªæ˜¾äº†ä»…ä»å›¾åƒæ•°æ®ä¸­æå–å¯é ç”Ÿç‰©æµ‹å®šä¼°è®¡çš„æŒ‘æˆ˜æ€§ã€‚åŸŸåç§»åˆ†æç¡®å®šäº†å›¾åƒè´¨é‡æ˜¯å½±å“æ¨¡å‹æ³›åŒ–çš„æœ€é‡è¦å› ç´ ï¼Œè¶…åˆ†è¾¨ç‡æµç¨‹ä¹Ÿå‘æŒ¥äº†é‡è¦ä½œç”¨ã€‚å…¶ä»–å› ç´ ï¼Œå¦‚èƒé¾„ã€ç—…ç†å’Œé‡‡é›†åœ°ç‚¹çš„å½±å“è¾ƒå°ï¼Œä½†ä»å¯è¡¡é‡ã€‚æ€»ä½“è€Œè¨€ï¼ŒFeTA 2024ä¸ºå¤šç±»åˆ†å‰²å’Œèƒå„¿è„‘éƒ¨MRIç”Ÿç‰©æµ‹å®šä¼°è®¡æä¾›äº†å…¨é¢çš„åŸºå‡†æµ‹è¯•ï¼Œè¿™çªæ˜¾äº†å¯¹æ•°æ®ä¸­å¿ƒæ–¹æ³•ã€æ”¹è¿›æ‹“æ‰‘è¯„ä¼°å’Œæ›´å¤§æ•°æ®é›†å¤šæ ·æ€§çš„éœ€æ±‚ï¼Œä»¥å®ç°ä¸´åºŠç¨³å¥ä¸”å¯æ¨å¹¿çš„äººå·¥æ™ºèƒ½å·¥å…·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.02784v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†FeTA Challenge 2024åœ¨èƒå„¿è„‘éƒ¨MRIåˆ†ææ–¹é¢çš„è¿›å±•ï¼ŒåŒ…æ‹¬ç»„ç»‡åˆ†å‰²å’Œç”Ÿç‰©è®¡é‡é¢„æµ‹ä¸¤ä¸ªä»»åŠ¡ã€‚ç ”ç©¶ä½¿ç”¨äº†å¤šä¸­å¿ƒæµ‹è¯•é›†ï¼ŒåŒ…æ‹¬æ¥è‡ªä½åœºï¼ˆ0.55Tï¼‰MRIæ•°æ®é›†çš„æ•°æ®ã€‚è¯„ä»·æŒ‡æ ‡æ‰©å±•åˆ°äº†æ‹“æ‰‘ç‰¹å®šçš„Eulerç‰¹å¾å·®å¼‚ï¼ˆEDï¼‰ã€‚å°½ç®¡å¤§å¤šæ•°å›¢é˜Ÿåœ¨é«˜åœºå’Œä½åœºæ‰«æä¸­è¡¨ç°ç¨³å®šï¼Œä½†åˆ†å‰²ç²¾åº¦å¯èƒ½å·²ç»è¾¾åˆ°ä¸€ä¸ªå¹³å°æœŸã€‚ä½åœºæ•°æ®é›†è·å¾—äº†æœ€é«˜çš„åˆ†å‰²åˆ†æ•°ï¼Œçªæ˜¾å‡ºä½æˆæœ¬æˆåƒç³»ç»Ÿä¸é«˜è´¨é‡å»ºç›¸ç»“åˆæ—¶çš„æ½œåŠ›ã€‚å¯¹äºç”Ÿç‰©è®¡é‡ä»»åŠ¡ï¼Œå¤§å¤šæ•°æ–¹æ³•æœªèƒ½è¶…è¶ŠåŸºäºèƒé¾„çš„ç®€å•åŸºçº¿é¢„æµ‹ï¼Œè¡¨æ˜ä»…ä»å›¾åƒæ•°æ®ä¸­æå–å¯é ç”Ÿç‰©è®¡é‡ä¼°è®¡çš„æŒ‘æˆ˜æ€§ã€‚é¢†åŸŸåç§»åˆ†æè¡¨æ˜ï¼Œå›¾åƒè´¨é‡æ˜¯å½±å“æ¨¡å‹æ³›åŒ–çš„æœ€é‡è¦å› ç´ ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FeTA Challenge 2024æ‰©å±•äº†èƒå„¿è„‘éƒ¨MRIåˆ†æï¼ŒåŒ…æ‹¬ç”Ÿç‰©è®¡é‡é¢„æµ‹è¿™ä¸€æ–°ä»»åŠ¡ã€‚</li>
<li>å¤šæ ·åŒ–çš„å¤šä¸­å¿ƒæµ‹è¯•é›†åŒ…å«æ¥è‡ªä½åœºMRIæ•°æ®é›†çš„æ•°æ®ã€‚</li>
<li>è¯„ä»·æŒ‡æ ‡æ–°å¢äº†Eulerç‰¹å¾å·®å¼‚ï¼ˆEDï¼‰ï¼Œç”¨äºæ•æ‰æ‹“æ‰‘ç‰¹æ€§ã€‚</li>
<li>å¤§å¤šæ•°å›¢é˜Ÿåœ¨é«˜åœºå’Œä½åœºæ‰«æä¸­è¡¨ç°ç¨³å®šï¼Œä½†åˆ†å‰²ç²¾åº¦æ¥è¿‘é¥±å’Œã€‚</li>
<li>ä½åœºæ•°æ®é›†è·å¾—æœ€é«˜åˆ†å‰²åˆ†æ•°ï¼Œæ˜¾ç¤ºä½æˆæœ¬æˆåƒç³»ç»Ÿç»“åˆé«˜è´¨é‡é‡å»ºçš„æ½œåŠ›ã€‚</li>
<li>ç”Ÿç‰©è®¡é‡é¢„æµ‹ä»»åŠ¡é¢ä¸´æŒ‘æˆ˜ï¼Œç®€å•åŸºçº¿æ¨¡å‹åŸºäºèƒé¾„çš„é¢„æµ‹è¡¨ç°è¾ƒå¥½ã€‚</li>
<li>é¢†åŸŸåç§»åˆ†ææ˜¾ç¤ºï¼Œæ¨¡å‹æ³›åŒ–å—å½±å“æœ€å¤§çš„å› ç´ æ˜¯å›¾åƒè´¨é‡ï¼Œå…¶æ¬¡æ˜¯è¶…åˆ†è¾¨ç‡ç®¡é“ã€èƒé¾„ã€ç—…ç†å’Œé‡‡é›†åœ°ç‚¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.02784">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-59abc2a5d7b365fdb6a3df4093a10af2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-832bc4077b2ac604381d11d72513ff93.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b33490215f6013f90bf3c62bc0035a8e.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Accelerating-Volumetric-Medical-Image-Annotation-via-Short-Long-Memory-SAM-2"><a href="#Accelerating-Volumetric-Medical-Image-Annotation-via-Short-Long-Memory-SAM-2" class="headerlink" title="Accelerating Volumetric Medical Image Annotation via Short-Long Memory   SAM 2"></a>Accelerating Volumetric Medical Image Annotation via Short-Long Memory   SAM 2</h2><p><strong>Authors:Yuwen Chen, Zafer Yildiz, Qihang Li, Yaqian Chen, Haoyu Dong, Hanxue Gu, Nicholas Konz, Maciej A. Mazurowski</strong></p>
<p>Manual annotation of volumetric medical images, such as magnetic resonance imaging (MRI) and computed tomography (CT), is a labor-intensive and time-consuming process. Recent advancements in foundation models for video object segmentation, such as Segment Anything Model 2 (SAM 2), offer a potential opportunity to significantly speed up the annotation process by manually annotating one or a few slices and then propagating target masks across the entire volume. However, the performance of SAM 2 in this context varies. Our experiments show that relying on a single memory bank and attention module is prone to error propagation, particularly at boundary regions where the target is present in the previous slice but absent in the current one. To address this problem, we propose Short-Long Memory SAM 2 (SLM-SAM 2), a novel architecture that integrates distinct short-term and long-term memory banks with separate attention modules to improve segmentation accuracy. We evaluate SLM-SAM 2 on three public datasets covering organs, bones, and muscles across MRI and CT modalities. We show that the proposed method markedly outperforms the default SAM 2, achieving average Dice Similarity Coefficient improvement of 0.14 and 0.11 in the scenarios when 5 volumes and 1 volume are available for the initial adaptation, respectively. SLM-SAM 2 also exhibits stronger resistance to over-propagation, making a notable step toward more accurate automated annotation of medical images for segmentation model development. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒï¼ˆå¦‚ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰å’Œè®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰ï¼‰çš„æ‰‹åŠ¨æ ‡æ³¨æ˜¯ä¸€ä¸ªåŠ³åŠ¨å¯†é›†å‹ä¸”è€—æ—¶çš„å·¥ä½œè¿‡ç¨‹ã€‚è¿‘æœŸè§†é¢‘å¯¹è±¡åˆ†å‰²åŸºç¡€æ¨¡å‹ï¼ˆä¾‹å¦‚åˆ†å‰²ä»»ä½•æ¨¡å‹ 2 ï¼ˆSAM 2ï¼‰ï¼‰çš„æœ€æ–°è¿›å±•æä¾›äº†ä¸€ä¸ªæ½œåœ¨çš„æœºä¼šï¼Œå¯ä»¥é€šè¿‡æ‰‹åŠ¨æ ‡æ³¨ä¸€ä¸ªæˆ–å¤šä¸ªåˆ‡ç‰‡ï¼Œç„¶åå°†å…¶ä¼ æ’­åˆ°æ•´ä¸ªä½“ç§¯ï¼Œä»è€Œæ˜¾è‘—åŠ å¿«æ ‡æ³¨è¿‡ç¨‹ã€‚ç„¶è€Œï¼ŒSAM 2 åœ¨æ­¤èƒŒæ™¯ä¸‹çš„è¡¨ç°æœ‰æ‰€ä¸åŒã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œä¾èµ–å•ä¸€å†…å­˜é“¶è¡Œå’Œæ³¨æ„åŠ›æ¨¡å—å®¹æ˜“å‡ºç°è¯¯å·®ä¼ æ’­ï¼Œç‰¹åˆ«æ˜¯åœ¨ç›®æ ‡å‡ºç°åœ¨å‰ä¸€ä¸ªåˆ‡ç‰‡ä¸­è€Œåœ¨å½“å‰åˆ‡ç‰‡ä¸­ä¸å­˜åœ¨çš„è¾¹ç•ŒåŒºåŸŸã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†é•¿çŸ­è®°å¿†SAM 2ï¼ˆSLM-SAM 2ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æ¶æ„ï¼Œèåˆäº†ä¸åŒçš„çŸ­æœŸå’Œé•¿æœŸå†…å­˜é“¶è¡Œä»¥åŠå•ç‹¬çš„æ³¨æ„åŠ›æ¨¡å—ï¼Œä»¥æé«˜åˆ†å‰²ç²¾åº¦ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªå…¬å…±æ•°æ®é›†ä¸Šè¯„ä¼°äº†SLM-SAM 2ï¼Œè¿™äº›æ•°æ®é›†æ¶µç›–äº†MRIå’ŒCTæ¨¡æ€ä¸‹çš„å™¨å®˜ã€éª¨éª¼å’Œè‚Œè‚‰ã€‚æˆ‘ä»¬å±•ç¤ºçš„æ–¹æ³•åœ¨åˆå§‹é€‚åº”æ—¶æ˜¾è‘—ä¼˜äºé»˜è®¤SAM 2ï¼Œåœ¨å¯ç”¨ä½“ç§¯ä¸º5ä¸ªå’Œ1ä¸ªçš„æƒ…å†µä¸‹ï¼Œå¹³å‡Diceç›¸ä¼¼ç³»æ•°åˆ†åˆ«æé«˜äº†0.14å’Œ0.11ã€‚SLM-SAM 2 è¿˜å±•ç°å‡ºæ›´å¼ºçš„æŠµæŠ—è¿‡åº¦ä¼ æ’­çš„èƒ½åŠ›ï¼Œåœ¨æ›´å‡†ç¡®è‡ªåŠ¨æ ‡æ³¨åŒ»å­¦å›¾åƒä»¥å¼€å‘åˆ†å‰²æ¨¡å‹æ–¹é¢è¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.01854v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºåŒ»å­¦å›¾åƒï¼ˆå¦‚MRIå’ŒCTï¼‰çš„è‡ªåŠ¨æ ‡æ³¨æŠ€æœ¯æ”¹è¿›æ–¹æ¡ˆï¼Œåä¸ºShort-Long Memory SAM 2ï¼ˆSLM-SAM 2ï¼‰ã€‚è¯¥æŠ€æœ¯é€šè¿‡æ•´åˆçŸ­æœŸå’Œé•¿æœŸè®°å¿†åº“ä»¥åŠç‹¬ç«‹æ³¨æ„åŠ›æ¨¡å—ï¼Œè§£å†³äº†ç°æœ‰æŠ€æœ¯å¦‚SAM 2åœ¨æ ‡æ³¨è¿‡ç¨‹ä¸­å‡ºç°çš„è¯¯å·®ä¼ æ’­é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨ç›®æ ‡åœ¨å‰ååˆ‡ç‰‡é—´è¾¹ç•ŒåŒºåŸŸçš„æ ‡æ³¨é—®é¢˜ã€‚å®éªŒè¡¨æ˜ï¼ŒSLM-SAM 2åœ¨å…¬å¼€æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºåŸç‰ˆSAM 2ï¼ŒDiceç›¸ä¼¼ç³»æ•°å¹³å‡æå‡0.14å’Œ0.11ã€‚æ­¤å¤–ï¼ŒSLM-SAM 2è¿˜å±•ç°å‡ºæ›´å¼ºçš„æŠ—è¿‡åº¦ä¼ æ’­èƒ½åŠ›ï¼Œä¸ºåŒ»å­¦å›¾åƒåˆ†å‰²æ¨¡å‹çš„æ›´å‡†ç¡®è‡ªåŠ¨æ ‡æ³¨è¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æ‰‹åŠ¨æ ‡æ³¨åŒ»å­¦å›¾åƒå¦‚MRIå’ŒCTæ˜¯åŠ³åŠ¨å¯†é›†ä¸”è€—æ—¶çš„è¿‡ç¨‹ã€‚</li>
<li>Segment Anything Model 2 (SAM 2)ç­‰è§†é¢‘ç‰©ä½“åˆ†å‰²åŸºç¡€æ¨¡å‹å¯ç”¨äºåŠ é€Ÿæ ‡æ³¨è¿‡ç¨‹ã€‚</li>
<li>SAM 2åœ¨åŒ»å­¦å›¾åƒæ ‡æ³¨ä¸­å­˜åœ¨è¯¯å·®ä¼ æ’­é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨ç›®æ ‡è¾¹ç•ŒåŒºåŸŸã€‚</li>
<li>Short-Long Memory SAM 2ï¼ˆSLM-SAM 2ï¼‰æ˜¯ä¸€ç§æ–°æŠ€æœ¯ï¼Œé€šè¿‡ç»“åˆçŸ­æœŸå’Œé•¿æœŸè®°å¿†åº“åŠç‹¬ç«‹æ³¨æ„åŠ›æ¨¡å—ï¼Œæé«˜äº†åˆ†å‰²å‡†ç¡®æ€§ã€‚</li>
<li>SLM-SAM 2åœ¨å…¬å¼€æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºSAM 2ï¼ŒDiceç›¸ä¼¼ç³»æ•°æœ‰æ‰€æå‡ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.01854">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0d504c4ae7bf0a8e4058df4bffd6e926.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4a16fbd3c13ed87036595798aac43278.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-07f91dd8632862a3f53bdc969d04aa0e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d2ac07530677d9bdc617793b2f77ef87.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a7865071c598d37bf2a521861f899ac4.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="LensNet-An-End-to-End-Learning-Framework-for-Empirical-Point-Spread-Function-Modeling-and-Lensless-Imaging-Reconstruction"><a href="#LensNet-An-End-to-End-Learning-Framework-for-Empirical-Point-Spread-Function-Modeling-and-Lensless-Imaging-Reconstruction" class="headerlink" title="LensNet: An End-to-End Learning Framework for Empirical Point Spread   Function Modeling and Lensless Imaging Reconstruction"></a>LensNet: An End-to-End Learning Framework for Empirical Point Spread   Function Modeling and Lensless Imaging Reconstruction</h2><p><strong>Authors:Jiesong Bai, Yuhao Yin, Yihang Dong, Xiaofeng Zhang, Chi-Man Pun, Xuhang Chen</strong></p>
<p>Lensless imaging stands out as a promising alternative to conventional lens-based systems, particularly in scenarios demanding ultracompact form factors and cost-effective architectures. However, such systems are fundamentally governed by the Point Spread Function (PSF), which dictates how a point source contributes to the final captured signal. Traditional lensless techniques often require explicit calibrations and extensive pre-processing, relying on static or approximate PSF models. These rigid strategies can result in limited adaptability to real-world challenges, including noise, system imperfections, and dynamic scene variations, thus impeding high-fidelity reconstruction. In this paper, we propose LensNet, an end-to-end deep learning framework that integrates spatial-domain and frequency-domain representations in a unified pipeline. Central to our approach is a learnable Coded Mask Simulator (CMS) that enables dynamic, data-driven estimation of the PSF during training, effectively mitigating the shortcomings of fixed or sparsely calibrated kernels. By embedding a Wiener filtering component, LensNet refines global structure and restores fine-scale details, thus alleviating the dependency on multiple handcrafted pre-processing steps. Extensive experiments demonstrate LensNetâ€™s robust performance and superior reconstruction quality compared to state-of-the-art methods, particularly in preserving high-frequency details and attenuating noise. The proposed framework establishes a novel convergence between physics-based modeling and data-driven learning, paving the way for more accurate, flexible, and practical lensless imaging solutions for applications ranging from miniature sensors to medical diagnostics. The link of code is <a target="_blank" rel="noopener" href="https://github.com/baijiesong/Lensnet">https://github.com/baijiesong/Lensnet</a>. </p>
<blockquote>
<p>æ— é€é•œæˆåƒä½œä¸ºä¼ ç»ŸåŸºäºé€é•œç³»ç»Ÿçš„æœ‰å‰é€”çš„æ›¿ä»£æ–¹æ¡ˆè„±é¢–è€Œå‡ºï¼Œç‰¹åˆ«æ˜¯åœ¨è¦æ±‚è¶…ç´§å‡‘å¤–å½¢å°ºå¯¸å’Œæˆæœ¬æ•ˆç›Šé«˜çš„æ¶æ„çš„åœºæ™¯ä¸­ã€‚ç„¶è€Œï¼Œæ­¤ç±»ç³»ç»Ÿä»æ ¹æœ¬ä¸Šå—åˆ°ç‚¹æ‰©æ•£å‡½æ•°ï¼ˆPSFï¼‰çš„æ”¯é…ï¼ŒPSFå†³å®šäº†ç‚¹æºå¦‚ä½•è´¡çŒ®äºæœ€ç»ˆæ•è·çš„ä¿¡å·ã€‚ä¼ ç»Ÿçš„æ— é€é•œæŠ€æœ¯é€šå¸¸éœ€è¦æ˜ç¡®çš„æ ¡å‡†å’Œå¹¿æ³›çš„é¢„å¤„ç†ï¼Œä¾èµ–äºé™æ€æˆ–è¿‘ä¼¼PSFæ¨¡å‹ã€‚è¿™äº›åƒµç¡¬ç­–ç•¥å¯¹ç°å®ä¸–ç•ŒæŒ‘æˆ˜çš„å¯é€‚åº”æ€§æœ‰é™ï¼ŒåŒ…æ‹¬å™ªå£°ã€ç³»ç»Ÿç¼ºé™·å’ŒåŠ¨æ€åœºæ™¯å˜åŒ–ï¼Œä»è€Œé˜»ç¢é«˜ä¿çœŸé‡å»ºã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†LensNetï¼Œè¿™æ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œå®ƒå°†ç©ºé—´åŸŸå’Œé¢‘ç‡åŸŸè¡¨ç¤ºé›†æˆåˆ°ä¸€ä¸ªç»Ÿä¸€çš„ç®¡é“ä¸­ã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„æ ¸å¿ƒæ˜¯å¯å­¦ä¹ çš„ç¼–ç æ©è†œæ¨¡æ‹Ÿå™¨ï¼ˆCMSï¼‰ï¼Œå®ƒèƒ½å¤Ÿåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å®ç°PSFçš„åŠ¨æ€æ•°æ®é©±åŠ¨ä¼°è®¡ï¼Œæœ‰æ•ˆåœ°å¼¥è¡¥äº†å›ºå®šæˆ–ç¨€ç–æ ¡å‡†å†…æ ¸çš„ç¼ºç‚¹ã€‚é€šè¿‡åµŒå…¥ç»´çº³æ»¤æ³¢ç»„ä»¶ï¼ŒLensNetç»†åŒ–å…¨å±€ç»“æ„å¹¶æ¢å¤ç»†å¾®ç»†èŠ‚ï¼Œä»è€Œå‡å°‘å¯¹å¤šä¸ªæ‰‹å·¥é¢„å¤„ç†æ­¥éª¤çš„ä¾èµ–ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒLensNetçš„æ€§èƒ½ç¨³å¥ï¼Œé‡å»ºè´¨é‡ä¼˜è¶Šï¼Œç‰¹åˆ«æ˜¯èƒ½å¤Ÿä¿æŒé«˜é¢‘ç»†èŠ‚å¹¶å‡å°‘å™ªå£°ã€‚æ‰€æå‡ºçš„æ¡†æ¶åœ¨åŸºäºç‰©ç†çš„å»ºæ¨¡å’Œæ•°æ®é©±åŠ¨å­¦ä¹ ä¹‹é—´å»ºç«‹äº†æ–°çš„èåˆï¼Œä¸ºä»å¾®å‹ä¼ æ„Ÿå™¨åˆ°åŒ»å­¦è¯Šæ–­ç­‰å„ç§åº”ç”¨æä¾›äº†æ›´å‡†ç¡®ã€æ›´çµæ´»ã€æ›´å®ç”¨çš„æ— é€é•œæˆåƒè§£å†³æ–¹æ¡ˆã€‚ä»£ç é“¾æ¥æ˜¯<a target="_blank" rel="noopener" href="https://github.com/baijiesong/Lensnet%E3%80%82">https://github.com/baijiesong/Lensnetã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.01755v1">PDF</a> Accepted by IJCAI 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æ— é€é•œæˆåƒä½œä¸ºä¸€ç§å¯¹ä¼ ç»Ÿé€é•œç³»ç»Ÿæœ‰å‰æ™¯çš„æ›¿ä»£æ–¹æ¡ˆçš„ç‰¹ç‚¹ä¸ä¼˜åŠ¿ï¼Œå°¤å…¶åœ¨è¶…ç´§å‡‘å½¢å¼å› ç´ å’Œæˆæœ¬æ•ˆç›Šå‹æ¶æ„æ–¹é¢çš„åº”ç”¨ã€‚ä½†ç”±äºå—åˆ°ç‚¹æ‰©æ•£å‡½æ•°ï¼ˆPSFï¼‰çš„å½±å“ï¼Œå…¶é¢ä¸´è¯¸å¤šæŒ‘æˆ˜ã€‚é’ˆå¯¹è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºLensNetçš„ç«¯åˆ°ç«¯æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†ç©ºé—´åŸŸå’Œé¢‘ç‡åŸŸè¡¨ç¤ºï¼Œå¹¶å¼•å…¥äº†ä¸€ä¸ªå¯å­¦ä¹ çš„ç¼–ç æ©è†œæ¨¡æ‹Ÿå™¨ï¼ˆCMSï¼‰ï¼Œèƒ½å¤ŸåŠ¨æ€ã€æ•°æ®é©±åŠ¨åœ°ä¼°è®¡PSFã€‚æ­¤å¤–ï¼ŒLensNeté€šè¿‡åµŒå…¥ç»´çº³æ»¤æ³¢ç»„ä»¶ï¼Œæ”¹è¿›äº†å…¨å±€ç»“æ„å¹¶æ¢å¤äº†ç»†èŠ‚ï¼Œå‡å°‘äº†å¯¹æ‰‹åŠ¨é¢„å¤„ç†æ­¥éª¤çš„ä¾èµ–ã€‚å®éªŒç»“æœè¯æ˜äº†LensNetçš„é²æ£’æ€§èƒ½å’Œä¼˜è¶Šçš„é‡å»ºè´¨é‡ã€‚è¯¥æ¡†æ¶ä¸ºç‰©ç†å»ºæ¨¡å’Œæ•°æ®é©±åŠ¨å­¦ä¹ ä¹‹é—´å»ºç«‹äº†æ–°çš„èåˆç‚¹ï¼Œä¸ºä»å¾®å‹ä¼ æ„Ÿå™¨åˆ°åŒ»ç–—è¯Šæ–­ç­‰é¢†åŸŸçš„æ— é€é•œæˆåƒæä¾›äº†æ›´å‡†ç¡®ã€çµæ´»å’Œå®ç”¨çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ— é€é•œæˆåƒä½œä¸ºä¼ ç»Ÿé€é•œç³»ç»Ÿçš„æ›¿ä»£æ–¹æ¡ˆï¼Œå°¤å…¶åœ¨è¶…ç´§å‡‘å’Œæˆæœ¬æ•ˆç›Šå‹æ¶æ„æ–¹é¢è¡¨ç°å‡ºæ½œåŠ›ã€‚</li>
<li>ç‚¹æ‰©æ•£å‡½æ•°ï¼ˆPSFï¼‰åœ¨æ— é€é•œæˆåƒä¸­èµ·åˆ°å…³é”®ä½œç”¨ï¼Œå½±å“æœ€ç»ˆæ•è·ä¿¡å·çš„ç‚¹æºè´¡çŒ®ã€‚</li>
<li>ä¼ ç»Ÿæ— é€é•œæŠ€æœ¯ä¾èµ–äºé™æ€æˆ–è¿‘ä¼¼PSFæ¨¡å‹ï¼Œå­˜åœ¨å¯¹ç°å®ä¸–ç•ŒæŒ‘æˆ˜çš„é€‚åº”æ€§æœ‰é™çš„ç¼ºç‚¹ã€‚</li>
<li>LensNetæ¡†æ¶ç»“åˆäº†ç©ºé—´åŸŸå’Œé¢‘ç‡åŸŸè¡¨ç¤ºï¼Œé€šè¿‡æ·±åº¦å­¦ä¹ å®ç°åŠ¨æ€ã€æ•°æ®é©±åŠ¨çš„PSFä¼°è®¡ã€‚</li>
<li>LensNetå¼•å…¥ç¼–ç æ©è†œæ¨¡æ‹Ÿå™¨ï¼ˆCMSï¼‰å’Œç»´çº³æ»¤æ³¢ç»„ä»¶ï¼Œæ”¹è¿›å…¨å±€ç»“æ„å¹¶æ¢å¤ç»†èŠ‚ã€‚</li>
<li>LensNetå‡å°‘äº†å¯¹æ‰‹åŠ¨é¢„å¤„ç†æ­¥éª¤çš„ä¾èµ–ï¼Œå¹¶å±•ç¤ºäº†åœ¨å™ªå£°è¡°å‡å’Œé«˜é¢‘ç»†èŠ‚ä¿ç•™æ–¹é¢çš„ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.01755">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-b8357a44e9d7da6c2d39f56adc375c33.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b439ac68aaba0d6f76129b96fbeb8cd4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-df34f1b0f47188639997223f5936042c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a69054a0c9ae345017df69f3facb525e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-35a6280a3adcac8b4d347fa9035531fe.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="High-Fidelity-Pseudo-label-Generation-by-Large-Language-Models-for-Training-Robust-Radiology-Report-Classifiers"><a href="#High-Fidelity-Pseudo-label-Generation-by-Large-Language-Models-for-Training-Robust-Radiology-Report-Classifiers" class="headerlink" title="High-Fidelity Pseudo-label Generation by Large Language Models for   Training Robust Radiology Report Classifiers"></a>High-Fidelity Pseudo-label Generation by Large Language Models for   Training Robust Radiology Report Classifiers</h2><p><strong>Authors:Brian Wong, Kaito Tanaka</strong></p>
<p>Automated labeling of chest X-ray reports is essential for enabling downstream tasks such as training image-based diagnostic models, population health studies, and clinical decision support. However, the high variability, complexity, and prevalence of negation and uncertainty in these free-text reports pose significant challenges for traditional Natural Language Processing methods. While large language models (LLMs) demonstrate strong text understanding, their direct application for large-scale, efficient labeling is limited by computational cost and speed. This paper introduces DeBERTa-RAD, a novel two-stage framework that combines the power of state-of-the-art LLM pseudo-labeling with efficient DeBERTa-based knowledge distillation for accurate and fast chest X-ray report labeling. We leverage an advanced LLM to generate high-quality pseudo-labels, including certainty statuses, for a large corpus of reports. Subsequently, a DeBERTa-Base model is trained on this pseudo-labeled data using a tailored knowledge distillation strategy. Evaluated on the expert-annotated MIMIC-500 benchmark, DeBERTa-RAD achieves a state-of-the-art Macro F1 score of 0.9120, significantly outperforming established rule-based systems, fine-tuned transformer models, and direct LLM inference, while maintaining a practical inference speed suitable for high-throughput applications. Our analysis shows particular strength in handling uncertain findings. This work demonstrates a promising path to overcome data annotation bottlenecks and achieve high-performance medical text processing through the strategic combination of LLM capabilities and efficient student models trained via distillation. </p>
<blockquote>
<p>èƒ¸éƒ¨Xå…‰æŠ¥å‘Šè‡ªåŠ¨æ ‡æ³¨å¯¹äºä¸‹æ¸¸ä»»åŠ¡è‡³å…³é‡è¦ï¼Œå¦‚åŸºäºå›¾åƒçš„è¯Šæ–­æ¨¡å‹è®­ç»ƒã€äººç¾¤å¥åº·ç ”ç©¶å’Œä¸´åºŠå†³ç­–æ”¯æŒã€‚ç„¶è€Œï¼Œè¿™äº›è‡ªç”±æ–‡æœ¬æŠ¥å‘Šä¸­å­˜åœ¨çš„é«˜å˜å¼‚æ€§ã€å¤æ‚æ€§å’Œå¦å®šä»¥åŠä¸ç¡®å®šæ€§çš„æ™®éå­˜åœ¨ï¼Œç»™ä¼ ç»Ÿè‡ªç„¶è¯­è¨€å¤„ç†æ–¹æ³•å¸¦æ¥äº†å·¨å¤§çš„æŒ‘æˆ˜ã€‚è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¡¨ç°å‡ºå¼ºå¤§çš„æ–‡æœ¬ç†è§£èƒ½åŠ›ï¼Œä½†ç›´æ¥åº”ç”¨äºå¤§è§„æ¨¡ã€é«˜æ•ˆçš„æ ‡æ³¨å´å—åˆ°è®¡ç®—æˆæœ¬å’Œé€Ÿåº¦çš„é™åˆ¶ã€‚</p>
</blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†DeBERTa-RADï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œç»“åˆäº†æœ€å…ˆè¿›çš„LLMä¼ªæ ‡æ³¨çš„èƒ½åŠ›å’ŒåŸºäºDeBERTaçš„é«˜æ•ˆçŸ¥è¯†è’¸é¦ï¼Œç”¨äºå‡†ç¡®å¿«é€Ÿåœ°å®Œæˆèƒ¸éƒ¨Xå…‰æŠ¥å‘Šæ ‡æ³¨ã€‚æˆ‘ä»¬åˆ©ç”¨å…ˆè¿›çš„LLMä¸ºå¤§é‡æŠ¥å‘Šç”Ÿæˆé«˜è´¨é‡ä¼ªæ ‡ç­¾ï¼ŒåŒ…æ‹¬ç¡®å®šæ€§çŠ¶æ€ã€‚éšåï¼Œä½¿ç”¨å®šåˆ¶çš„çŸ¥è¯†è’¸é¦ç­–ç•¥ï¼Œåœ¨ä¼ªæ ‡è®°æ•°æ®ä¸Šè®­ç»ƒDeBERTa-Baseæ¨¡å‹ã€‚åœ¨ä¸“å®¶æ³¨é‡Šçš„MIMIC-500åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œè¯„ä¼°ï¼ŒDeBERTa-RADå–å¾—äº†å®è§‚F1å¾—åˆ†ä¸º0.9120çš„ä¸šç•Œæœ€ä½³æˆç»©ï¼Œæ˜¾è‘—ä¼˜äºåŸºäºè§„åˆ™çš„ç³»ç»Ÿã€å¾®è°ƒåçš„transformeræ¨¡å‹å’Œç›´æ¥LLMæ¨ç†ï¼ŒåŒæ—¶ä¿æŒäº†é€‚ç”¨äºé«˜ååé‡åº”ç”¨çš„å®é™…æ¨ç†é€Ÿåº¦ã€‚æˆ‘ä»¬çš„åˆ†ææ˜¾ç¤ºåœ¨å¤„ç†ä¸ç¡®å®šçš„æ£€æŸ¥ç»“æœæ—¶å…·æœ‰ç‰¹åˆ«çš„ä¼˜åŠ¿ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.01693v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºè‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯çš„è‡ªåŠ¨æ ‡æ³¨èƒ¸éƒ¨Xå…‰æŠ¥å‘Šçš„æ–¹æ³•ã€‚é’ˆå¯¹ä¼ ç»Ÿè‡ªç„¶è¯­è¨€å¤„ç†æ–¹æ³•éš¾ä»¥å¤„ç†æŠ¥å‘Šä¸­å­˜åœ¨çš„å¦å®šå’Œä¸ç¡®å®šæ€§ç­‰é—®é¢˜ï¼Œæœ¬æ–‡å¼•å…¥äº†ä¸€ç§æ–°å‹çš„ä¸¤é˜¶æ®µæ¡†æ¶â€”â€”DeBERTa-RADã€‚è¯¥æ¡†æ¶ç»“åˆäº†æœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¼ªæ ‡æ³¨èƒ½åŠ›ä¸é«˜æ•ˆçš„DeBERTaçŸ¥è¯†è’¸é¦æŠ€æœ¯ï¼Œä»¥å®ç°å¿«é€Ÿå‡†ç¡®çš„èƒ¸éƒ¨Xå…‰æŠ¥å‘Šæ ‡æ³¨ã€‚åˆ©ç”¨å…ˆè¿›çš„LLMç”Ÿæˆé«˜è´¨é‡ä¼ªæ ‡ç­¾ï¼ˆåŒ…æ‹¬ç¡®å®šçŠ¶æ€ï¼‰ï¼Œç”¨äºå¤§è§„æ¨¡æŠ¥å‘Šæ•°æ®é›†ã€‚éšåï¼Œä½¿ç”¨å®šåˆ¶çš„çŸ¥è¯†è’¸é¦ç­–ç•¥åœ¨ä¼ªæ ‡ç­¾æ•°æ®ä¸Šè®­ç»ƒDeBERTa-Baseæ¨¡å‹ã€‚åœ¨ä¸“å®¶æ ‡æ³¨çš„MIMIC-500åŸºå‡†æµ‹è¯•ä¸Šè¯„ä¼°ï¼ŒDeBERTa-RADå–å¾—äº†å®å¹³å‡F1åˆ†æ•°ä¸º0.912çš„ä¸šç•Œæœ€ä½³æˆç»©ï¼Œæ˜¾è‘—ä¼˜äºåŸºäºè§„åˆ™çš„ç³»ç»Ÿã€å¾®è°ƒè¿‡çš„transformeræ¨¡å‹å’Œç›´æ¥LLMæ¨ç†ï¼ŒåŒæ—¶ä¿æŒäº†é€‚åˆé«˜ååé‡åº”ç”¨çš„å®é™…æ¨ç†é€Ÿåº¦ã€‚ç‰¹åˆ«åœ¨å¤„ç†ä¸ç¡®å®šç»“æœæ–¹é¢è¡¨ç°çªå‡ºã€‚æœ¬æ–‡å±•ç¤ºäº†é€šè¿‡ç»“åˆLLMèƒ½åŠ›å’Œé«˜æ•ˆå­¦ç”Ÿæ¨¡å‹è®­ç»ƒå…‹æœæ•°æ®æ ‡æ³¨ç“¶é¢ˆï¼Œå®ç°é«˜æ€§èƒ½åŒ»å­¦æ–‡æœ¬å¤„ç†çš„é€”å¾„ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è‡ªåŠ¨åŒ–æ ‡è®°èƒ¸éƒ¨Xå…‰æŠ¥å‘Šå¯¹ä¸‹æ¸¸ä»»åŠ¡è‡³å…³é‡è¦ï¼Œå¦‚è®­ç»ƒåŸºäºå›¾åƒçš„è¯Šæ–­æ¨¡å‹ã€äººç¾¤å¥åº·ç ”ç©¶å’Œä¸´åºŠå†³ç­–æ”¯æŒã€‚</li>
<li>ä¼ ç»Ÿè‡ªç„¶è¯­è¨€å¤„ç†æ–¹æ³•é¢ä¸´æŠ¥å‘Šä¸­å¦å®šå’Œä¸ç¡®å®šæ€§çš„é«˜å˜å¼‚æ€§ã€å¤æ‚æ€§å’Œæ™®éæ€§æ‰€å¸¦æ¥çš„æŒ‘æˆ˜ã€‚</li>
<li>DeBERTa-RADæ¡†æ¶ç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¼ªæ ‡æ³¨èƒ½åŠ›å’ŒDeBERTaçŸ¥è¯†è’¸é¦æŠ€æœ¯ï¼Œä»¥å®ç°å¿«é€Ÿå‡†ç¡®çš„èƒ¸éƒ¨Xå…‰æŠ¥å‘Šæ ‡æ³¨ã€‚</li>
<li>åˆ©ç”¨ä¼ªæ ‡ç­¾æ•°æ®è®­ç»ƒDeBERTa-Baseæ¨¡å‹ï¼Œåœ¨MIMIC-500åŸºå‡†æµ‹è¯•ä¸­å–å¾—ä¸šç•Œæœ€ä½³æ€§èƒ½ã€‚</li>
<li>DeBERTa-RADåœ¨å¤„ç†ä¸ç¡®å®šç»“æœæ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>è¯¥ç ”ç©¶å±•ç¤ºäº†ç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹èƒ½åŠ›å’Œé«˜æ•ˆå­¦ç”Ÿæ¨¡å‹è®­ç»ƒçš„æˆ˜ç•¥ç»„åˆï¼Œä»¥å…‹æœæ•°æ®æ ‡æ³¨ç“¶é¢ˆå¹¶å®ç°é«˜æ€§èƒ½åŒ»å­¦æ–‡æœ¬å¤„ç†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.01693">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ebc31267880572e75c27de55294b0557.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ec797d8903896c5c9e0dccf703d0c140.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b040d8581b277effe3c207fa0ca5c000.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="A-Dual-Task-Synergy-Driven-Generalization-Framework-for-Pancreatic-Cancer-Segmentation-in-CT-Scans"><a href="#A-Dual-Task-Synergy-Driven-Generalization-Framework-for-Pancreatic-Cancer-Segmentation-in-CT-Scans" class="headerlink" title="A Dual-Task Synergy-Driven Generalization Framework for Pancreatic   Cancer Segmentation in CT Scans"></a>A Dual-Task Synergy-Driven Generalization Framework for Pancreatic   Cancer Segmentation in CT Scans</h2><p><strong>Authors:Jun Li, Yijue Zhang, Haibo Shi, Minhong Li, Qiwei Li, Xiaohua Qian</strong></p>
<p>Pancreatic cancer, characterized by its notable prevalence and mortality rates, demands accurate lesion delineation for effective diagnosis and therapeutic interventions. The generalizability of extant methods is frequently compromised due to the pronounced variability in imaging and the heterogeneous characteristics of pancreatic lesions, which may mimic normal tissues and exhibit significant inter-patient variability. Thus, we propose a generalization framework that synergizes pixel-level classification and regression tasks, to accurately delineate lesions and improve model stability. This framework not only seeks to align segmentation contours with actual lesions but also uses regression to elucidate spatial relationships between diseased and normal tissues, thereby improving tumor localization and morphological characterization. Enhanced by the reciprocal transformation of task outputs, our approach integrates additional regression supervision within the segmentation context, bolstering the modelâ€™s generalization ability from a dual-task perspective. Besides, dual self-supervised learning in feature spaces and output spaces augments the modelâ€™s representational capability and stability across different imaging views. Experiments on 594 samples composed of three datasets with significant imaging differences demonstrate that our generalized pancreas segmentation results comparable to mainstream in-domain validation performance (Dice: 84.07%). More importantly, it successfully improves the results of the highly challenging cross-lesion generalized pancreatic cancer segmentation task by 9.51%. Thus, our model constitutes a resilient and efficient foundational technological support for pancreatic disease management and wider medical applications. The codes will be released at <a target="_blank" rel="noopener" href="https://github.com/SJTUBME-QianLab/Dual-Task-Seg">https://github.com/SJTUBME-QianLab/Dual-Task-Seg</a>. </p>
<blockquote>
<p>èƒ°è…ºç™Œä»¥å…¶è¾ƒé«˜çš„å‘ç—…ç‡å’Œæ­»äº¡ç‡è€Œè‘—ç§°ï¼Œéœ€è¦è¿›è¡Œå‡†ç¡®çš„ç—…ç¶å‹¾ç”»ä»¥å®ç°æœ‰æ•ˆçš„è¯Šæ–­å’Œå¹²é¢„æ²»ç–—ã€‚ç”±äºæˆåƒçš„æ˜æ˜¾å·®å¼‚æ€§å’Œèƒ°è…ºç—…ç¶çš„å¼‚è´¨æ€§ç‰¹å¾ï¼ˆå¯èƒ½æ¨¡æ‹Ÿæ­£å¸¸ç»„ç»‡å¹¶è¡¨ç°å‡ºæ˜¾è‘—çš„ç—…äººé—´å·®å¼‚ï¼‰ï¼Œç°æœ‰æ–¹æ³•çš„é€šç”¨æ€§ç»å¸¸å—åˆ°å¨èƒã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç»¼åˆåƒç´ çº§åˆ†ç±»å’Œå›å½’ä»»åŠ¡çš„é€šç”¨æ¡†æ¶ï¼Œä»¥å‡†ç¡®å‹¾ç”»ç—…ç¶å¹¶æé«˜æ¨¡å‹ç¨³å®šæ€§ã€‚è¯¥æ¡†æ¶ä¸ä»…è‡´åŠ›äºä½¿åˆ†å‰²è½®å»“ä¸å®é™…ç—…ç¶å¯¹é½ï¼Œè¿˜ä½¿ç”¨å›å½’æ¥é˜æ˜ç—…å˜ç»„ç»‡ä¸æ­£å¸¸ç»„ç»‡ä¹‹é—´çš„ç©ºé—´å…³ç³»ï¼Œä»è€Œæé«˜è‚¿ç˜¤å®šä½å’Œå½¢æ€è¡¨å¾ã€‚é€šè¿‡ä»»åŠ¡è¾“å‡ºçš„ç›¸äº’è½¬æ¢ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å°†é¢å¤–çš„å›å½’ç›‘ç£æ•´åˆåˆ°åˆ†å‰²èƒŒæ™¯ä¸­ï¼Œä»åŒé‡ä»»åŠ¡çš„è§’åº¦å¢å¼ºæ¨¡å‹çš„é€šç”¨æ€§èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œç‰¹å¾ç©ºé—´å’Œè¾“å‡ºç©ºé—´ä¸­çš„åŒé‡è‡ªç›‘ç£å­¦ä¹ å¢å¼ºäº†æ¨¡å‹çš„è¡¨ç¤ºèƒ½åŠ›å’Œç¨³å®šæ€§ï¼Œé€‚ç”¨äºä¸åŒçš„æˆåƒè§†å›¾ã€‚åœ¨ç”±ä¸‰ä¸ªå…·æœ‰æ˜¾è‘—å·®å¼‚çš„æ•°æ®é›†ç»„æˆçš„594ä¸ªæ ·æœ¬ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„é€šç”¨èƒ°è…ºåˆ†å‰²ç»“æœä¸ä¸»æµçš„åŒåŸŸéªŒè¯æ€§èƒ½ç›¸å½“ï¼ˆDiceç³»æ•°ä¸º84.07%ï¼‰ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œå®ƒæˆåŠŸæé«˜äº†å…·æœ‰æŒ‘æˆ˜æ€§çš„è·¨ç—…ç¶é€šç”¨èƒ°è…ºç™Œåˆ†å‰²ä»»åŠ¡çš„ç»“æœè¾¾9.51%ã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„æ¨¡å‹ä¸ºèƒ°è…ºç–¾ç—…ç®¡ç†å’Œæ›´å¹¿æ³›çš„åŒ»å­¦åº”ç”¨æä¾›äº†ç¨³å¥é«˜æ•ˆçš„åŸºç¡€æŠ€æœ¯æ”¯æŒã€‚ä»£ç å°†å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/SJTUBME-QianLab/Dual-Task-Seg%E3%80%82">https://github.com/SJTUBME-QianLab/Dual-Task-Segã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.01644v1">PDF</a> accept by IEEE Transactions on Medical Imaging (TMI) 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>èƒ°è…ºç™Œå› å…¶é«˜å‘ç—…ç‡å’Œé«˜æ­»äº¡ç‡è€Œå¤‡å—å…³æ³¨ï¼Œå‡†ç¡®åœ°è¿›è¡Œç—…ç¶å‹¾ç”»å¯¹äºæœ‰æ•ˆçš„è¯Šæ–­å’Œå¹²é¢„æ²»ç–—è‡³å…³é‡è¦ã€‚ç”±äºæˆåƒçš„æ˜¾è‘—å˜å¼‚æ€§å’Œèƒ°è…ºç—…ç¶çš„å¼‚è´¨æ€§ç‰¹å¾ï¼ˆå¯èƒ½æ¨¡æ‹Ÿæ­£å¸¸ç»„ç»‡å¹¶è¡¨ç°å‡ºæ˜¾è‘—çš„æ‚£è€…é—´å·®å¼‚ï¼‰ï¼Œç°æœ‰æ–¹æ³•çš„é€šç”¨æ€§å¾€å¾€å—åˆ°å½±å“ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªèåˆåƒç´ çº§åˆ†ç±»å’Œå›å½’ä»»åŠ¡çš„é€šç”¨æ¡†æ¶ï¼Œä»¥å‡†ç¡®å‹¾ç”»ç—…ç¶å¹¶æé«˜æ¨¡å‹ç¨³å®šæ€§ã€‚è¯¥æ¡†æ¶ä¸ä»…å¯»æ±‚å°†åˆ†å‰²è½®å»“ä¸å®é™…ç—…ç¶å¯¹é½ï¼Œè€Œä¸”åˆ©ç”¨å›å½’æ¥é˜æ˜ç—…ç¶ä¸æ­£å¸¸ç»„ç»‡ä¹‹é—´çš„ç©ºé—´å…³ç³»ï¼Œä»è€Œæ”¹å–„è‚¿ç˜¤å®šä½å’Œå½¢æ€è¡¨å¾ã€‚é€šè¿‡ä»»åŠ¡è¾“å‡ºçš„ç›¸äº’è½¬æ¢ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å°†é¢å¤–çš„å›å½’ç›‘ç£æ•´åˆåˆ°åˆ†å‰²èƒŒæ™¯ä¸­ï¼Œä»åŒé‡ä»»åŠ¡çš„è§’åº¦å¢å¼ºäº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œç‰¹å¾ç©ºé—´å’Œè¾“å‡ºç©ºé—´ä¸­çš„åŒé‡è‡ªç›‘ç£å­¦ä¹ å¢å¼ºäº†æ¨¡å‹çš„è¡¨å¾èƒ½åŠ›å’Œç¨³å®šæ€§ï¼Œé€‚ç”¨äºä¸åŒçš„æˆåƒè§†å›¾ã€‚åœ¨ç”±ä¸‰ä¸ªæ•°æ®é›†ç»„æˆçš„594ä¸ªæ ·æœ¬ä¸Šè¿›è¡Œå®éªŒï¼Œç»“æœè¡¨æ˜æˆ‘ä»¬çš„æ³›åŒ–èƒ°è…ºåˆ†å‰²ç»“æœä¸ä¸»æµçš„åŒåŸŸéªŒè¯æ€§èƒ½ç›¸å½“ï¼ˆDiceï¼š84.07%ï¼‰ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œå®ƒæˆåŠŸæé«˜äº†æå…·æŒ‘æˆ˜æ€§çš„è·¨ç—…ç¶æ³›åŒ–èƒ°è…ºç™Œåˆ†å‰²ä»»åŠ¡çš„ç»“æœè¾¾9.51%ã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„æ¨¡å‹ä¸ºèƒ°è…ºç–¾ç—…ç®¡ç†å’Œæ›´å¹¿æ³›çš„åŒ»ç–—åº”ç”¨æä¾›äº†ç¨³å¥æœ‰æ•ˆçš„åŸºæœ¬æŠ€æœ¯æ”¯æŒã€‚ä»£ç å°†å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/SJTUBME-QianLab/Dual-Task-Seg">https://github.com/SJTUBME-QianLab/Dual-Task-Seg</a>ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>èƒ°è…ºç™Œç”±äºå…¶é«˜å‘ç—…ç‡å’Œæ­»äº¡ç‡ï¼Œéœ€è¦å‡†ç¡®è¿›è¡Œç—…ç¶å‹¾ç”»ä»¥è¿›è¡Œæœ‰æ•ˆè¯Šæ–­å’Œæ²»ç–—ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ç”±äºæˆåƒçš„å˜å¼‚æ€§å’Œèƒ°è…ºç—…ç¶çš„å¼‚è´¨æ€§è€Œç¼ºä¹æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>æå‡ºçš„æ¡†æ¶èåˆäº†åƒç´ çº§åˆ†ç±»å’Œå›å½’ä»»åŠ¡ï¼Œä»¥æé«˜ç—…ç¶å‹¾ç”»çš„å‡†ç¡®æ€§å¹¶å¢å¼ºæ¨¡å‹ç¨³å®šæ€§ã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡ä»»åŠ¡è¾“å‡ºçš„ç›¸äº’è½¬æ¢å’ŒåŒé‡è‡ªç›‘ç£å­¦ä¹ å¢å¼ºäº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œè¡¨å¾èƒ½åŠ›ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨æ³›åŒ–èƒ°è…ºåˆ†å‰²ä»»åŠ¡ä¸Šå…·æœ‰è‰¯å¥½çš„æ€§èƒ½ï¼Œå¹¶åœ¨è·¨ç—…ç¶åˆ†å‰²ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚</li>
<li>è¯¥æ¨¡å‹ä¸ºèƒ°è…ºç–¾ç—…ç®¡ç†æä¾›äº†ç¨³å¥æœ‰æ•ˆçš„æŠ€æœ¯æ”¯æŒï¼Œå¹¶æœ‰æœ›åº”ç”¨äºæ›´å¹¿æ³›çš„åŒ»ç–—é¢†åŸŸã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.01644">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-904cd3a7be858436465d3828e28b4a0e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-61fcd3f833672dc6dbfe359d0840b993.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b27379e7cd4ecfe9352269642664f06e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1d6c2d38e60e05ab747673017f69c04f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-09d7f5bf25e312659edb155f8e8cb61f.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Towards-Accurate-and-Interpretable-Neuroblastoma-Diagnosis-via-Contrastive-Multi-scale-Pathological-Image-Analysis"><a href="#Towards-Accurate-and-Interpretable-Neuroblastoma-Diagnosis-via-Contrastive-Multi-scale-Pathological-Image-Analysis" class="headerlink" title="Towards Accurate and Interpretable Neuroblastoma Diagnosis via   Contrastive Multi-scale Pathological Image Analysis"></a>Towards Accurate and Interpretable Neuroblastoma Diagnosis via   Contrastive Multi-scale Pathological Image Analysis</h2><p><strong>Authors:Zhu Zhu, Shuo Jiang, Jingyuan Zheng, Yawen Li, Yifei Chen, Manli Zhao, Weizhong Gu, Feiwei Qin, Jinhu Wang, Gang Yu</strong></p>
<p>Neuroblastoma, adrenal-derived, is among the most common pediatric solid malignancies, characterized by significant clinical heterogeneity. Timely and accurate pathological diagnosis from hematoxylin and eosin-stained whole-slide images is critical for patient prognosis. However, current diagnostic practices primarily rely on subjective manual examination by pathologists, leading to inconsistent accuracy. Existing automated whole-slide image classification methods encounter challenges such as poor interpretability, limited feature extraction capabilities, and high computational costs, restricting their practical clinical deployment. To overcome these limitations, we propose CMSwinKAN, a contrastive-learning-based multi-scale feature fusion model tailored for pathological image classification, which enhances the Swin Transformer architecture by integrating a Kernel Activation Network within its multilayer perceptron and classification head modules, significantly improving both interpretability and accuracy. By fusing multi-scale features and leveraging contrastive learning strategies, CMSwinKAN mimics cliniciansâ€™ comprehensive approach, effectively capturing global and local tissue characteristics. Additionally, we introduce a heuristic soft voting mechanism guided by clinical insights to bridge patch-level predictions to whole-slide image-level classifications seamlessly. We verified the CMSwinKAN on the publicly available BreakHis dataset and the PpNTs dataset, which was established by our hospital. Results demonstrate that CMSwinKAN performs better than existing state-of-the-art pathology-specific models pre-trained on large datasets. Our source code is available at <a target="_blank" rel="noopener" href="https://github.com/JSLiam94/CMSwinKAN">https://github.com/JSLiam94/CMSwinKAN</a>. </p>
<blockquote>
<p>ç¥ç»æ¯ç»†èƒç˜¤æ˜¯è‚¾ä¸Šè…ºæ¥æºçš„æœ€å¸¸è§çš„å„¿ç«¥å®ä½“æ¶æ€§è‚¿ç˜¤ä¹‹ä¸€ï¼Œå…·æœ‰æ˜¾è‘—çš„ä¸´åºŠå¼‚è´¨æ€§ã€‚åŠæ—¶è€Œå‡†ç¡®çš„ç—…ç†è¯Šæ–­å¯¹äºæ‚£è€…çš„é¢„åè‡³å…³é‡è¦ï¼Œè¿™äº›è¯Šæ–­é€šå¸¸åŸºäºè‹æœ¨ç²¾å’Œä¼Šçº¢æŸ“è‰²çš„å…¨åˆ‡ç‰‡å›¾åƒã€‚ç„¶è€Œï¼Œå½“å‰çš„è¯Šæ–­æ–¹æ³•ä¸»è¦ä¾èµ–äºç—…ç†åŒ»å¸ˆçš„ä¸»è§‚æ‰‹åŠ¨æ£€æŸ¥ï¼Œå¯¼è‡´è¯Šæ–­å‡†ç¡®æ€§ä¸ä¸€è‡´ã€‚ç°æœ‰çš„å…¨è‡ªåŠ¨åˆ‡ç‰‡å›¾åƒåˆ†ç±»æ–¹æ³•é¢ä¸´å¯è§£é‡Šæ€§å·®ã€ç‰¹å¾æå–èƒ½åŠ›æœ‰é™ä»¥åŠè®¡ç®—æˆæœ¬é«˜çš„æŒ‘æˆ˜ï¼Œé™åˆ¶äº†å…¶åœ¨ä¸´åºŠå®è·µä¸­çš„éƒ¨ç½²åº”ç”¨ã€‚ä¸ºäº†å…‹æœè¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†CMSwinKANï¼Œè¿™æ˜¯ä¸€ç§åŸºäºå¯¹æ¯”å­¦ä¹ çš„å¤šå°ºåº¦ç‰¹å¾èåˆæ¨¡å‹ï¼Œä¸“ä¸ºç—…ç†å›¾åƒåˆ†ç±»è€Œå®šåˆ¶ã€‚æˆ‘ä»¬é€šè¿‡å°†å†…æ ¸æ¿€æ´»ç½‘ç»œé›†æˆåˆ°Swin Transformeræ¶æ„çš„å¤šå±‚æ„ŸçŸ¥å™¨å’Œåˆ†ç±»å¤´æ¨¡å—ä¸­ï¼Œå¢å¼ºäº†å…¶å¯è§£é‡Šæ€§å’Œå‡†ç¡®æ€§ã€‚é€šè¿‡èåˆå¤šå°ºåº¦ç‰¹å¾å¹¶åˆ©ç”¨å¯¹æ¯”å­¦ä¹ ç­–ç•¥ï¼ŒCMSwinKANæ¨¡ä»¿äº†ä¸´åºŠåŒ»ç”Ÿå…¨é¢çš„è¯Šæ–­æ–¹æ³•ï¼Œæœ‰æ•ˆåœ°æ•æ‰äº†å…¨å±€å’Œå±€éƒ¨ç»„ç»‡ç‰¹å¾ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§å—ä¸´åºŠè§è§£å¯å‘çš„æ–°å‹è½¯æŠ•ç¥¨æœºåˆ¶ï¼Œå°†æ–‘å—çº§åˆ«çš„é¢„æµ‹æ— ç¼åœ°æ¡¥æ¥åˆ°å…¨åˆ‡ç‰‡å›¾åƒçº§åˆ«çš„åˆ†ç±»ã€‚æˆ‘ä»¬åœ¨å…¬å¼€å¯ç”¨çš„BreakHisæ•°æ®é›†å’Œç”±æˆ‘ä»¬åŒ»é™¢å»ºç«‹PpNTsæ•°æ®é›†ä¸ŠéªŒè¯äº†CMSwinKANã€‚ç»“æœè¡¨æ˜ï¼ŒCMSwinKANçš„æ€§èƒ½ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„ã€åœ¨å¤§æ•°æ®é›†ä¸Šé¢„è®­ç»ƒçš„ç—…ç†å­¦ä¸“ç”¨æ¨¡å‹ã€‚æˆ‘ä»¬çš„æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/JSLiam94/CMSwinKAN%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/JSLiam94/CMSwinKANè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13754v3">PDF</a> 10pages, 8 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå¯¹æ¯”å­¦ä¹ çš„å¤šå°ºåº¦ç‰¹å¾èåˆæ¨¡å‹CMSwinKANï¼Œç”¨äºç¥ç»æ¯ç»†èƒç˜¤ç­‰è‚¾ä¸Šè…ºè¡ç”Ÿå›ºä½“è‚¿ç˜¤çš„ç—…ç†å›¾åƒåˆ†ç±»ã€‚è¯¥æ¨¡å‹ç»“åˆäº†Swin Transformeræ¶æ„å’ŒKernel Activation Networkï¼Œæé«˜äº†æ¨¡å‹çš„è§£é‡Šæ€§å’Œå‡†ç¡®æ€§ã€‚é€šè¿‡èåˆå¤šå°ºåº¦ç‰¹å¾å’Œé‡‡ç”¨å¯¹æ¯”å­¦ä¹ ç­–ç•¥ï¼ŒCMSwinKANæœ‰æ•ˆæ•æ‰å…¨å±€å’Œå±€éƒ¨ç»„ç»‡ç‰¹å¾ï¼Œæ¨¡æ‹ŸåŒ»ç”Ÿçš„ç»¼åˆè¯Šæ–­æ–¹æ³•ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†ä¸€ç§åŸºäºä¸´åºŠè§è§£çš„å¯å‘å¼è½¯æŠ•ç¥¨æœºåˆ¶ï¼Œå°†è¡¥ä¸çº§åˆ«çš„é¢„æµ‹æ— ç¼åœ°æ¡¥æ¥åˆ°æ•´ä¸ªå¹»ç¯ç‰‡å›¾åƒçº§åˆ«çš„åˆ†ç±»ã€‚åœ¨å…¬å¼€å¯ç”¨çš„BreakHisæ•°æ®é›†å’Œæˆ‘ä»¬åŒ»é™¢å»ºç«‹çš„PpNTsæ•°æ®é›†ä¸Šçš„éªŒè¯ç»“æœè¡¨æ˜ï¼ŒCMSwinKANçš„æ€§èƒ½ä¼˜äºç°æœ‰çš„æœ€æ–°ç—…ç†å­¦é¢„è®­ç»ƒæ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¥ç»æ¯ç»†èƒç˜¤æ˜¯å¸¸è§çš„å„¿ç«¥å›ºä½“è‚¿ç˜¤ï¼Œç—…ç†è¯Šæ–­å¯¹å…¶é¢„åè‡³å…³é‡è¦ã€‚</li>
<li>å½“å‰è¯Šæ–­æ–¹æ³•ä¸»è¦ä¾èµ–ç—…ç†åŒ»å¸ˆçš„ä¸»è§‚æ‰‹åŠ¨æ£€æŸ¥ï¼Œå­˜åœ¨å‡†ç¡®æ€§ä¸ä¸€è‡´çš„é—®é¢˜ã€‚</li>
<li>CMSwinKANæ¨¡å‹ç»“åˆäº†Swin Transformerå’ŒKernel Activation Networkï¼Œæé«˜ç—…ç†å›¾åƒåˆ†ç±»çš„å‡†ç¡®æ€§å’Œè§£é‡Šæ€§ã€‚</li>
<li>CMSwinKANé€šè¿‡èåˆå¤šå°ºåº¦ç‰¹å¾å’Œå¯¹æ¯”å­¦ä¹ ç­–ç•¥ï¼Œæœ‰æ•ˆæ•æ‰å…¨å±€å’Œå±€éƒ¨ç»„ç»‡ç‰¹æ€§ï¼Œæ¨¡æ‹ŸåŒ»ç”Ÿçš„è¯Šæ–­è¿‡ç¨‹ã€‚</li>
<li>å¼•å…¥å¯å‘å¼è½¯æŠ•ç¥¨æœºåˆ¶ï¼Œå°†è¡¥ä¸çº§åˆ«é¢„æµ‹æ— ç¼è½¬æ¢ä¸ºæ•´ä¸ªå¹»ç¯ç‰‡å›¾åƒçº§åˆ«åˆ†ç±»ã€‚</li>
<li>åœ¨å…¬å¼€å’ŒåŒ»é™¢å†…éƒ¨æ•°æ®é›†ä¸Šçš„éªŒè¯ç»“æœè¡¨æ˜ï¼ŒCMSwinKANæ€§èƒ½ä¼˜äºç°æœ‰æœ€æ–°ç—…ç†å­¦é¢„è®­ç»ƒæ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13754">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fb96e73ff698492f49377d9bb024e869.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5728caeb68be85c517b9cd6f4143dbcc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6d214216c00cfc191bea492759d2ef60.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-90c9357fe4979a191912cd4ded1f41e6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ce85806f90fbc30636ef0a49a71ba0f0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8bd4b02c25a30c3cc5a38c94f547b27c.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="MSA-UNet3-Multi-Scale-Attention-UNet3-with-New-Supervised-Prototypical-Contrastive-Loss-for-Coronary-DSA-Image-Segmentation"><a href="#MSA-UNet3-Multi-Scale-Attention-UNet3-with-New-Supervised-Prototypical-Contrastive-Loss-for-Coronary-DSA-Image-Segmentation" class="headerlink" title="MSA-UNet3+: Multi-Scale Attention UNet3+ with New Supervised   Prototypical Contrastive Loss for Coronary DSA Image Segmentation"></a>MSA-UNet3+: Multi-Scale Attention UNet3+ with New Supervised   Prototypical Contrastive Loss for Coronary DSA Image Segmentation</h2><p><strong>Authors:Rayan Merghani Ahmed, Adnan Iltaf, Mohamed Elmanna, Gang Zhao, Hongliang Li, Yue Du, Bin Li, Shoujun Zhou</strong></p>
<p>Accurate segmentation of coronary Digital Subtraction Angiography images is essential to diagnose and treat coronary artery diseases. Despite advances in deep learning, challenges such as high intra-class variance and class imbalance limit precise vessel delineation. Most existing approaches for coronary DSA segmentation cannot address these issues. Also, existing segmentation networkâ€™s encoders do not directly generate semantic embeddings, which could enable the decoder to reconstruct segmentation masks effectively from these well-defined features. We propose a Supervised Prototypical Contrastive Loss that fuses supervised and prototypical contrastive learning to enhance coronary DSA image segmentation. The supervised contrastive loss enforces semantic embeddings in the encoder, improving feature differentiation. The prototypical contrastive loss allows the model to focus on the foreground class while alleviating the high intra-class variance and class imbalance problems by concentrating only on the hard-to-classify background samples. We implement the proposed SPCL loss within an MSA-UNet3+: a Multi-Scale Attention-Enhanced UNet3+ architecture. The architecture integrates key components: a Multi-Scale Attention Encoder and a Multi-Scale Dilated Bottleneck designed to enhance multi-scale feature extraction and a Contextual Attention Fusion Module built to keep fine-grained details while improving contextual understanding. Experiments on a private coronary DSA dataset show that MSA-UNet3+ outperforms state-of-the-art methods, achieving the highest Dice coefficient and F1-score and significantly reducing ASD and ACD. The developed framework provides clinicians with precise vessel segmentation, enabling accurate identification of coronary stenosis and supporting informed diagnostic and therapeutic decisions. The code will be released at <a target="_blank" rel="noopener" href="https://github.com/rayanmerghani/MSA-UNet3plus">https://github.com/rayanmerghani/MSA-UNet3plus</a>. </p>
<blockquote>
<p>å† çŠ¶åŠ¨è„‰æ•°å­—å‡å½±è¡€ç®¡é€ å½±ï¼ˆDSAï¼‰å›¾åƒçš„å‡†ç¡®åˆ†å‰²å¯¹äºå† çŠ¶åŠ¨è„‰ç–¾ç—…çš„è¯Šæ–­å’Œæ²»ç–—è‡³å…³é‡è¦ã€‚å°½ç®¡æ·±åº¦å­¦ä¹ æœ‰æ‰€è¿›å±•ï¼Œä½†ç±»å†…é«˜æ–¹å·®å’Œç±»ä¸å¹³è¡¡ç­‰æŒ‘æˆ˜ä»ç„¶é™åˆ¶äº†è¡€ç®¡ç²¾ç¡®å‹¾å‹’ã€‚å¤§å¤šæ•°ç°æœ‰çš„å† çŠ¶åŠ¨è„‰DSAåˆ†å‰²æ–¹æ³•æ— æ³•è§£å†³è¿™äº›é—®é¢˜ã€‚æ­¤å¤–ï¼Œç°æœ‰åˆ†å‰²ç½‘ç»œçš„ç¼–ç å™¨å¹¶æ²¡æœ‰ç›´æ¥ç”Ÿæˆè¯­ä¹‰åµŒå…¥ï¼Œè¿™å¯èƒ½ä¼šä½¿è§£ç å™¨æ— æ³•ä»è¿™äº›å®šä¹‰æ˜ç¡®çš„åŠŸèƒ½ä¸­æœ‰æ•ˆåœ°é‡å»ºåˆ†å‰²æ©æ¨¡ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æœ‰ç›‘ç£åŸå‹å¯¹æ¯”æŸå¤±ï¼ˆSupervised Prototypical Contrastive Lossï¼‰ï¼Œå®ƒå°†æœ‰ç›‘ç£å­¦ä¹ å’ŒåŸå‹å¯¹æ¯”å­¦ä¹ ç›¸ç»“åˆï¼Œä»¥æé«˜å† çŠ¶åŠ¨è„‰DSAå›¾åƒåˆ†å‰²çš„æ•ˆæœã€‚æœ‰ç›‘ç£å¯¹æ¯”æŸå¤±åœ¨ç¼–ç å™¨ä¸­å®æ–½è¯­ä¹‰åµŒå…¥ï¼Œæé«˜ç‰¹å¾å·®å¼‚ã€‚åŸå‹å¯¹æ¯”æŸå¤±å…è®¸æ¨¡å‹ä¸“æ³¨äºå‰æ™¯ç±»ï¼Œå¹¶é€šè¿‡ä»…å…³æ³¨éš¾ä»¥åˆ†ç±»çš„èƒŒæ™¯æ ·æœ¬ï¼Œç¼“è§£ç±»å†…é«˜æ–¹å·®å’Œç±»ä¸å¹³è¡¡é—®é¢˜ã€‚æˆ‘ä»¬åœ¨MSA-UNet3+ä¸­å®ç°äº†æ‰€æå‡ºçš„SPCLæŸå¤±ï¼šä¸€ç§é›†å¤šå°ºåº¦æ³¨æ„åŠ›å¢å¼ºUNet3+æ¶æ„ã€‚è¯¥æ¶æ„é›†æˆäº†å…³é”®ç»„ä»¶ï¼šå¤šå°ºåº¦æ³¨æ„åŠ›ç¼–ç å™¨å’Œå¤šå°ºåº¦è†¨èƒ€ç“¶é¢ˆï¼Œæ—¨åœ¨å¢å¼ºå¤šå°ºåº¦ç‰¹å¾æå–å’Œä¸Šä¸‹æ–‡ç†è§£ï¼Œä»¥åŠæ„å»ºç”¨äºä¿æŒç²¾ç»†ç»†èŠ‚çš„åŒæ—¶æ”¹å–„ä¸Šä¸‹æ–‡ç†è§£çš„ä¸Šä¸‹æ–‡æ³¨æ„åŠ›èåˆæ¨¡å—ã€‚åœ¨ç§æœ‰å† çŠ¶åŠ¨è„‰DSAæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMSA-UNet3+ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œè·å¾—äº†æœ€é«˜çš„Diceç³»æ•°å’ŒF1åˆ†æ•°ï¼Œå¹¶æ˜¾è‘—é™ä½äº†ASDå’ŒACDã€‚æ‰€å¼€å‘çš„æ¡†æ¶ä¸ºä¸´åºŠåŒ»ç”Ÿæä¾›äº†ç²¾ç¡®çš„è¡€ç®¡åˆ†å‰²ï¼Œèƒ½å¤Ÿå‡†ç¡®è¯†åˆ«å† çŠ¶åŠ¨è„‰ç‹­çª„ï¼Œå¹¶æ”¯æŒåšå‡ºæ˜æ™ºçš„è¯Šæ–­å’Œæ²»ç–—å†³ç­–ã€‚ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/rayanmerghani/MSA-UNet3plus">https://github.com/rayanmerghani/MSA-UNet3plus</a>å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05184v3">PDF</a> Work in progress</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§èåˆç›‘ç£å¯¹æ¯”æŸå¤±å’ŒåŸå‹å¯¹æ¯”æŸå¤±çš„å† çŠ¶åŠ¨è„‰æ•°å­—å‡å½±è¡€ç®¡é€ å½±å›¾åƒåˆ†å‰²æ–¹æ³•ï¼Œä»¥è§£å†³ç°æœ‰æ–¹æ³•é¢ä¸´çš„é«˜å†…ç±»æ–¹å·®å’Œç±»åˆ«ä¸å¹³è¡¡ç­‰é—®é¢˜ã€‚è¯¥æ–¹æ³•é€šè¿‡åŠ å¼ºç¼–ç å™¨ä¸­çš„è¯­ä¹‰åµŒå…¥æ¥æé«˜ç‰¹å¾åŒºåˆ†åº¦ï¼Œå¹¶é€šè¿‡åŸå‹å¯¹æ¯”æŸå¤±ä½¿æ¨¡å‹ä¸“æ³¨äºå‰æ™¯ç±»åˆ«ï¼Œä»è€Œå‡è½»é«˜å†…ç±»æ–¹å·®å’Œç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç§æœ‰å† çŠ¶åŠ¨è„‰DSAæ•°æ®é›†ä¸Šä¼˜äºç°æœ‰å…ˆè¿›æŠ€æœ¯ï¼Œå®ç°äº†æœ€é«˜çš„Diceç³»æ•°å’ŒF1åˆ†æ•°ï¼Œå¹¶æ˜¾è‘—é™ä½äº†ASDå’ŒACDã€‚æ­¤æ¡†æ¶å¯ä¸ºä¸´åºŠåŒ»ç”Ÿæä¾›ç²¾ç¡®çš„è¡€ç®¡åˆ†å‰²ï¼Œæœ‰åŠ©äºå‡†ç¡®è¯†åˆ«å† çŠ¶åŠ¨è„‰ç‹­çª„å¹¶æ”¯æŒåŒ»ç”Ÿåšå‡ºè¯Šæ–­å’Œæ²»ç–—çš„å†³ç­–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å† çŠ¶åŠ¨è„‰æ•°å­—å‡å½±è¡€ç®¡é€ å½±ï¼ˆDSAï¼‰å›¾åƒçš„å‡†ç¡®åˆ†å‰²å¯¹è¯Šæ–­å’Œæ²»ç–—å† çŠ¶åŠ¨è„‰ç–¾ç—…è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰æ–¹æ³•é¢ä¸´é«˜å†…ç±»æ–¹å·®å’Œç±»åˆ«ä¸å¹³è¡¡çš„æŒ‘æˆ˜ï¼Œå½±å“ç²¾ç¡®è¡€ç®¡è½®å»“çš„æç»˜ã€‚</li>
<li>æå‡ºçš„ç›‘ç£åŸå‹å¯¹æ¯”æŸå¤±ï¼ˆSPCLï¼‰èåˆäº†ç›‘ç£å¯¹æ¯”å­¦ä¹ å’ŒåŸå‹å¯¹æ¯”å­¦ä¹ ï¼Œæ—¨åœ¨æé«˜å† çŠ¶åŠ¨è„‰DSAå›¾åƒåˆ†å‰²çš„æ•ˆæœã€‚</li>
<li>SPCLæŸå¤±åœ¨ç¼–ç å™¨é˜¶æ®µå¼ºåˆ¶ç”Ÿæˆè¯­ä¹‰åµŒå…¥ï¼Œæ”¹å–„ç‰¹å¾åŒºåˆ†åº¦ï¼Œå¹¶é€šè¿‡åŸå‹å¯¹æ¯”æŸå¤±å…³æ³¨å‰æ™¯ç±»åˆ«ï¼Œå‡è½»é«˜å†…ç±»æ–¹å·®å’Œç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ã€‚</li>
<li>é‡‡ç”¨å¤šå°ºåº¦æ³¨æ„åŠ›å¢å¼ºUNet3+æ¶æ„ï¼ˆMSA-UNet3+ï¼‰å®ç°SPCLæŸå¤±ï¼ŒåŒ…æ‹¬å¤šå°ºåº¦æ³¨æ„åŠ›ç¼–ç å™¨ã€å¤šå°ºåº¦æ‰©å¼ ç“¶é¢ˆå’Œä¸Šä¸‹æ–‡æ³¨æ„åŠ›èåˆæ¨¡å—ï¼Œæ—¨åœ¨æé«˜å¤šå°ºåº¦ç‰¹å¾æå–å’Œä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›ã€‚</li>
<li>åœ¨ç§æœ‰å† çŠ¶åŠ¨è„‰DSAæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMSA-UNet3+æ–¹æ³•ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œæé«˜äº†Diceç³»æ•°å’ŒF1åˆ†æ•°ï¼Œå¹¶é™ä½äº†ASDå’ŒACDã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05184">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-c60db5ed57c5683c7acc8a01277c0527.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-678d65f1527ebedb4c1b5a42c616231e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c4f042f82046052d8354a4c8b6b4ca4d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9b6992bcc178cdaf11795c407d10da2d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b035f1f1b4cb95a429c2a47f2110d205.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Instance-Segmentation-of-Scene-Sketches-Using-Natural-Image-Priors"><a href="#Instance-Segmentation-of-Scene-Sketches-Using-Natural-Image-Priors" class="headerlink" title="Instance Segmentation of Scene Sketches Using Natural Image Priors"></a>Instance Segmentation of Scene Sketches Using Natural Image Priors</h2><p><strong>Authors:Mia Tang, Yael Vinker, Chuan Yan, Lvmin Zhang, Maneesh Agrawala</strong></p>
<p>Sketch segmentation involves grouping pixels within a sketch that belong to the same object or instance. It serves as a valuable tool for sketch editing tasks, such as moving, scaling, or removing specific components. While image segmentation models have demonstrated remarkable capabilities in recent years, sketches present unique challenges for these models due to their sparse nature and wide variation in styles. We introduce InkLayer, a method for instance segmentation of raster scene sketches. Our approach adapts state-of-the-art image segmentation and object detection models to the sketch domain by employing class-agnostic fine-tuning and refining segmentation masks using depth cues. Furthermore, our method organizes sketches into sorted layers, where occluded instances are inpainted, enabling advanced sketch editing applications. As existing datasets in this domain lack variation in sketch styles, we construct a synthetic scene sketch segmentation dataset, InkScenes, featuring sketches with diverse brush strokes and varying levels of detail. We use this dataset to demonstrate the robustness of our approach. </p>
<blockquote>
<p>è‰å›¾åˆ†å‰²æ¶‰åŠå°†å±äºåŒä¸€å¯¹è±¡æˆ–å®ä¾‹çš„è‰å›¾å†…çš„åƒç´ è¿›è¡Œåˆ†ç»„ã€‚å®ƒä½œä¸ºè‰å›¾ç¼–è¾‘ä»»åŠ¡ï¼ˆå¦‚ç§»åŠ¨ã€ç¼©æ”¾æˆ–åˆ é™¤ç‰¹å®šç»„ä»¶ï¼‰çš„å®è´µå·¥å…·ã€‚è¿‘å¹´æ¥ï¼Œå›¾åƒåˆ†å‰²æ¨¡å‹è¡¨ç°å‡ºäº†æ˜¾è‘—çš„èƒ½åŠ›ï¼Œä½†ç”±äºè‰å›¾çš„ç¨€ç–æ€§å’Œé£æ ¼ä¸Šçš„å·¨å¤§å·®å¼‚ï¼Œè¿™äº›æ¨¡å‹é¢ä¸´ç€ç‹¬ç‰¹çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬å¼•å…¥äº†InkLayerï¼Œè¿™æ˜¯ä¸€ç§åŸºäºçŸ¢é‡çš„åœºæ™¯è‰å›¾å®ä¾‹åˆ†å‰²æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡é‡‡ç”¨ç±»æ— å…³çš„å¾®è°ƒå’Œä½¿ç”¨æ·±åº¦çº¿ç´¢ç»†åŒ–åˆ†å‰²æ©è†œï¼Œå°†æœ€å…ˆè¿›çš„å›¾åƒåˆ†å‰²å’Œå¯¹è±¡æ£€æµ‹æ¨¡å‹é€‚åº”åˆ°è‰å›¾é¢†åŸŸã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å°†è‰å›¾ç»„ç»‡æˆæœ‰åºçš„å±‚ï¼Œå…¶ä¸­é®æŒ¡çš„å®ä¾‹è¢«å¡«å……ï¼Œä»è€Œå®ç°é«˜çº§çš„è‰å›¾ç¼–è¾‘åº”ç”¨ç¨‹åºã€‚ç”±äºè¯¥é¢†åŸŸçš„ç°æœ‰æ•°æ®é›†ç¼ºä¹è‰å›¾é£æ ¼çš„å¤šæ ·æ€§ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåˆæˆåœºæ™¯è‰å›¾åˆ†å‰²æ•°æ®é›†InkScenesï¼Œå…¶ä¸­åŒ…å«å…·æœ‰ä¸åŒç¬”è§¦å’Œç»†èŠ‚çº§åˆ«çš„è‰å›¾ã€‚æˆ‘ä»¬ä½¿ç”¨æ­¤æ•°æ®é›†æ¥å±•ç¤ºæˆ‘ä»¬æ–¹æ³•çš„ç¨³å¥æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.09608v2">PDF</a> Project website: <a target="_blank" rel="noopener" href="https://inklayer.github.io/">https://inklayer.github.io</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†InkLayeræ–¹æ³•ï¼Œç”¨äºå¯¹åœºæ™¯è‰å›¾è¿›è¡Œå®ä¾‹åˆ†å‰²ã€‚è¯¥æ–¹æ³•é‡‡ç”¨å…ˆè¿›çš„å›¾åƒåˆ†å‰²å’Œå¯¹è±¡æ£€æµ‹æ¨¡å‹ï¼Œé€šè¿‡ç±»åˆ«æ— å…³çš„å¾®è°ƒå¹¶åˆ©ç”¨æ·±åº¦çº¿ç´¢å®Œå–„åˆ†å‰²æ©æ¨¡ï¼Œä»¥é€‚åº”è‰å›¾é¢†åŸŸã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•å¯å°†è‰å›¾ç»„ç»‡æˆæœ‰åºå±‚ï¼Œå¯¹é®æŒ¡çš„å®ä¾‹è¿›è¡Œå¡«å……ï¼Œå®ç°é«˜çº§è‰å›¾ç¼–è¾‘åº”ç”¨ã€‚ä¸ºè§£å†³ç°æœ‰æ•°æ®é›†é£æ ¼ç¼ºä¹å˜åŒ–çš„é—®é¢˜ï¼Œæ„å»ºäº†ä¸€ä¸ªåˆæˆåœºæ™¯è‰å›¾åˆ†å‰²æ•°æ®é›†InkScenesã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Sketch segmentationæ—¨åœ¨å°†åŒä¸€å¯¹è±¡æˆ–å®ä¾‹çš„åƒç´ è¿›è¡Œåˆ†ç»„ï¼Œå¯¹äºè‰å›¾ç¼–è¾‘ä»»åŠ¡ï¼ˆå¦‚ç§»åŠ¨ã€ç¼©æ”¾æˆ–åˆ é™¤ç‰¹å®šç»„ä»¶ï¼‰å…·æœ‰é‡è¦ä»·å€¼ã€‚</li>
<li>InkLayeræ–¹æ³•é‡‡ç”¨å…ˆè¿›çš„å›¾åƒåˆ†å‰²å’Œå¯¹è±¡æ£€æµ‹æ¨¡å‹ï¼Œé€šè¿‡ç±»åˆ«æ— å…³çš„å¾®è°ƒæ¥é€‚åº”è‰å›¾é¢†åŸŸã€‚</li>
<li>InkLayeræ–¹æ³•åˆ©ç”¨æ·±åº¦çº¿ç´¢å®Œå–„åˆ†å‰²æ©æ¨¡ï¼Œæé«˜æ¨¡å‹çš„é€‚åº”æ€§ã€‚</li>
<li>è¯¥æ–¹æ³•å°†è‰å›¾ç»„ç»‡æˆæœ‰åºå±‚ï¼Œä¾¿äºè¿›è¡Œé«˜çº§è‰å›¾ç¼–è¾‘åº”ç”¨ã€‚</li>
<li>ç°å­˜æ•°æ®é›†ç¼ºä¹è‰å›¾é£æ ¼çš„å˜åŒ–ï¼Œç¼ºä¹è¶³å¤Ÿçš„å¤šæ ·æ€§ä»¥é€‚åº”ä¸åŒçš„æ¨¡å‹éœ€æ±‚ã€‚</li>
<li>ä¸ºè§£å†³ä¸Šè¿°é—®é¢˜ï¼Œæ„å»ºäº†ä¸€ä¸ªåˆæˆåœºæ™¯è‰å›¾åˆ†å‰²æ•°æ®é›†InkScenesï¼ŒåŒ…å«ä¸åŒç¬”è§¦å’Œç»†èŠ‚å±‚æ¬¡çš„è‰å›¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.09608">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-11416b84860e9b8a9143fd3738e0694d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4a754963067c7dc46376b04cbfe6128a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ef670f87f20142e8fbbb04a99ba0e9bf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4cfd62a6da2a1ca76bc264ca0f7e0422.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d7b7a8b9efd58ba54027b747401b9c90.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Censor-Aware-Semi-Supervised-Survival-Time-Prediction-in-Lung-Cancer-Using-Clinical-and-Radiomics-Features"><a href="#Censor-Aware-Semi-Supervised-Survival-Time-Prediction-in-Lung-Cancer-Using-Clinical-and-Radiomics-Features" class="headerlink" title="Censor-Aware Semi-Supervised Survival Time Prediction in Lung Cancer   Using Clinical and Radiomics Features"></a>Censor-Aware Semi-Supervised Survival Time Prediction in Lung Cancer   Using Clinical and Radiomics Features</h2><p><strong>Authors:Arman Gorji, Ali Fathi Jouzdani, Nima Sanati, Amir Mahmoud Ahmadzadeh, Ren Yuan, Arman Rahmim, Mohammad R. Salmanpour</strong></p>
<p>Objectives: Lung cancer poses a significant global health challenge, necessitating improved prognostic methods for personalized treatment. This study introduces a censor-aware semi-supervised learning (SSL) framework that integrates clinical and imaging data, addressing biases in traditional models handling censored data. Methods: We analyzed clinical, PET and CT data from 199 lung cancer patients from public and local data respositories, focusing on overall survival (OS) time as the primary outcome. Handcrafted (HRF) and Deep Radiomics features (DRF) were extracted after preprocessing using ViSERA software and were combined with clinical features (CF). Feature dimensions were optimized using Principal Component Analysis (PCA), followed by the application of supervised learning (SL) and SSL. SSL incorporated pseudo-labeling of censored data to improve performance. Seven regressors and three hazard ratio survival analysis (HRSA) algorithms were optimized using five-fold cross-validation, grid search and external test bootstrapping. Results: For PET HRFs, SSL reduced the mean absolute error (MAE) by 26.5%, achieving 1.55 years with PCA+decision tree regression, compared to SLâ€™s 2.11 years with PCA+KNNR (p&lt;0.05). Combining HRFs (CT_HRF) and DRFs from CT images using SSL+PCA+KNNR achieved an MAE of 2.08 years, outperforming SLâ€™s 2.26 years by 7.96% (p&lt;0.05). In HRSA, CT_HRF applied to PCA+Component Wise Gradient Boosting Survival Analysis achieved an external c-index of 0.65, effectively differentiating high- and low-risk groups. Conclusions: We demonstrated that the SSL strategy significantly outperforms SL across PET, CT, and CF. As such, censor-aware SSL applied to HRFs from PET images significantly improved survival prediction performance by 26.5% compared to the SL approach. </p>
<blockquote>
<p>ç›®æ ‡ï¼šè‚ºç™Œæ„æˆä¸€é¡¹é‡å¤§çš„å…¨çƒå¥åº·æŒ‘æˆ˜ï¼Œéœ€è¦æ”¹è¿›é¢„æµ‹æ–¹æ³•ä»¥å®ç°ä¸ªæ€§åŒ–æ²»ç–—ã€‚æœ¬ç ”ç©¶å¼•å…¥äº†ä¸€ç§æœ‰å®¡æŸ¥æ„è¯†çš„åŠç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†ä¸´åºŠå’Œæˆåƒæ•°æ®ï¼Œè§£å†³äº†ä¼ ç»Ÿæ¨¡å‹åœ¨å¤„ç†å®¡æŸ¥æ•°æ®æ—¶çš„åè§é—®é¢˜ã€‚</p>
</blockquote>
<p>æ–¹æ³•ï¼šæˆ‘ä»¬åˆ†æäº†æ¥è‡ªå…¬å…±å’Œæœ¬åœ°æ•°æ®ä»“åº“çš„199åè‚ºç™Œæ‚£è€…çš„ä¸´åºŠã€PETå’ŒCTæ•°æ®ï¼Œä»¥æ€»ç”Ÿå­˜æœŸï¼ˆOSï¼‰æ—¶é—´ä¸ºä¸»è¦ç»“æœã€‚ä½¿ç”¨ViSERAè½¯ä»¶é¢„å¤„ç†åï¼Œæå–äº†æ‰‹å·¥ç‰¹å¾ï¼ˆHRFï¼‰å’Œæ·±åº¦æ”¾å°„å­¦ç‰¹å¾ï¼ˆDRFï¼‰ï¼Œå¹¶ä¸ä¸´åºŠç‰¹å¾ï¼ˆCFï¼‰ç›¸ç»“åˆã€‚ä½¿ç”¨ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰ä¼˜åŒ–ç‰¹å¾ç»´åº¦ï¼Œç„¶ååº”ç”¨ç›‘ç£å­¦ä¹ ï¼ˆSLï¼‰å’ŒSSLã€‚SSLé€šè¿‡ä¼ªæ ‡è®°å®¡æŸ¥æ•°æ®æ¥æé«˜æ€§èƒ½ã€‚ä½¿ç”¨äº”æŠ˜äº¤å‰éªŒè¯ã€ç½‘æ ¼æœç´¢å’Œå¤–éƒ¨æµ‹è¯•è‡ªåŠ©æ³•å¯¹ä¸ƒä¸ªå›å½’å™¨å’Œä¸‰ç§é£é™©æ¯”ç‡ç”Ÿå­˜åˆ†æï¼ˆHRSAï¼‰ç®—æ³•è¿›è¡Œäº†ä¼˜åŒ–ã€‚</p>
<p>ç»“æœï¼šå¯¹äºPETçš„HRFï¼ŒSSLå°†å¹³å‡ç»å¯¹è¯¯å·®ï¼ˆMAEï¼‰é™ä½äº†26.5%ï¼Œä½¿ç”¨PCA+å†³ç­–æ ‘å›å½’è¾¾åˆ°1.55å¹´ï¼Œè€ŒSLä½¿ç”¨PCA+KNNRä¸º2.11å¹´ï¼ˆp&lt;0.05ï¼‰ã€‚ä½¿ç”¨SSL+PCA+KNNRç»“åˆæ¥è‡ªCTå›¾åƒçš„HRFå’ŒDRFï¼ŒMAEä¸º2.08å¹´ï¼Œä¼˜äºSLçš„2.26å¹´ï¼Œæé«˜äº†7.96%ï¼ˆp&lt;0.05ï¼‰ã€‚åœ¨HRSAä¸­ï¼Œå°†CT_HRFåº”ç”¨äºPCA+ç»„ä»¶æ™ºæ…§æ¢¯åº¦æå‡ç”Ÿå­˜åˆ†æè¾¾åˆ°äº†å¤–éƒ¨cæŒ‡æ•°ä¸º0.65ï¼Œæœ‰æ•ˆåœ°åŒºåˆ†äº†é«˜é£é™©ç»„å’Œä½é£é™©ç»„ã€‚</p>
<p>ç»“è®ºï¼šæˆ‘ä»¬è¯æ˜SSLç­–ç•¥åœ¨PETã€CTå’ŒCFæ–¹é¢æ˜¾è‘—ä¼˜äºSLã€‚å› æ­¤ï¼Œä¸SLæ–¹æ³•ç›¸æ¯”ï¼Œåº”ç”¨äºPETå›¾åƒHRFçš„æœ‰å®¡æŸ¥æ„è¯†çš„SSLå°†ç”Ÿå­˜é¢„æµ‹æ€§èƒ½æé«˜äº†26.5%ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.01661v3">PDF</a> 11 pages, 4 Figures and 4 Tables</p>
<p><strong>Summary</strong>ï¼šæœ¬ç ”ç©¶æ—¨åœ¨åº”å¯¹è‚ºç™Œé¢„åé¢„æµ‹æ–¹æ³•çš„æŒ‘æˆ˜ï¼Œæå‡ºä¸€ç§èåˆä¸´åºŠä¸æˆåƒæ•°æ®çš„åŠç›‘ç£å­¦ä¹ æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿå¤„ç†å¸¦æœ‰å®¡æŸ¥çš„æ•°æ®ä¸­çš„åè§é—®é¢˜ã€‚é€šè¿‡å¯¹PETå’ŒCTå½±åƒæ•°æ®ä»¥åŠä¸´åºŠæ•°æ®çš„åˆ†æï¼Œæœ¬ç ”ç©¶å‘ç°è¯¥åŠç›‘ç£å­¦ä¹ æ–¹æ³•èƒ½å¤Ÿæé«˜é¢„åé¢„æµ‹çš„å‡†ç¡®åº¦ã€‚å¯¹æ¯”åªä½¿ç”¨ç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œè¯¥åŠç›‘ç£å­¦ä¹ æ–¹æ³•çš„é¢„æµ‹è¯¯å·®é™ä½äº†è¿‘ç™¾åˆ†ä¹‹äºŒåäº”ã€‚å¯¹å½±åƒç‰¹å¾çš„æœ‰æ•ˆç»„åˆè¿›ä¸€æ­¥æå‡äº†æ¨¡å‹çš„æ€§èƒ½ã€‚ç ”ç©¶ç»“è®ºè¡¨æ˜åŠç›‘ç£å­¦ä¹ æ¡†æ¶åœ¨å¤„ç†è‚ºç™Œæ‚£è€…çš„é¢„åé¢„æµ‹é—®é¢˜ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æœ¬ç ”ç©¶å¼•å…¥äº†åŠç›‘ç£å­¦ä¹ æ¡†æ¶ä»¥æ•´åˆä¸´åºŠå’Œæˆåƒæ•°æ®ï¼Œè§£å†³ä¼ ç»Ÿæ¨¡å‹å¤„ç†å®¡æŸ¥æ•°æ®æ—¶å­˜åœ¨çš„åè§é—®é¢˜ã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨äº†PETå’ŒCTå½±åƒæ•°æ®ä»¥åŠä¸´åºŠæ•°æ®ï¼Œå…³æ³¨çš„ä¸»è¦ç»“æœæ˜¯æ‚£è€…çš„æ€»ä½“ç”Ÿå­˜æ—¶é—´ã€‚</li>
<li>åŠç›‘ç£å­¦ä¹ æ–¹æ³•ç›¸è¾ƒäºä¼ ç»Ÿç›‘ç£å­¦ä¹ æ–¹æ³•æ˜¾è‘—æé«˜äº†é¢„åé¢„æµ‹çš„å‡†ç¡®åº¦ï¼Œé¢„æµ‹è¯¯å·®é™ä½äº†çº¦ç™¾åˆ†ä¹‹äºŒåäº”ã€‚</li>
<li>ç»“åˆå½±åƒç‰¹å¾çš„ä¸´åºŠç‰¹å¾å¯ä»¥è¿›ä¸€æ­¥ä¼˜åŒ–æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>é€šè¿‡ä½¿ç”¨ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰å¯¹ç‰¹å¾ç»´åº¦è¿›è¡Œä¼˜åŒ–åï¼Œæ¨¡å‹æ€§èƒ½å¾—åˆ°è¿›ä¸€æ­¥æå‡ã€‚</li>
<li>HRSAç®—æ³•å¯¹äºåŒºåˆ†é«˜é£é™©å’Œä½é£é™©æ‚£è€…ç¾¤ä½“å…·æœ‰è‰¯å¥½çš„æ•ˆæœï¼Œå…¶å¤–éƒ¨ä¸€è‡´æ€§æŒ‡æ•°è¾¾åˆ°äº†0.65ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.01661">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bfe5fa9a3a7303ab32965d8c06afdef3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9d7167453cde86778f0b22a8b0693a36.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-107297f39c14e9543e21201e1f240c20.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-37ad9a419e7f042b55c9eeead14f751c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6c89e181a49d363d482400b0b7868a78.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-49e8bb808e464585bd2fe65e213c46ae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8ecfef211018620ba373b6d98795dff6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6c2ebab837bf060fbff9e6fdd7c65188.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3682ff7914fd7809c57ffc142678a503.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="GRAPHITE-Graph-Based-Interpretable-Tissue-Examination-for-Enhanced-Explainability-in-Breast-Cancer-Histopathology"><a href="#GRAPHITE-Graph-Based-Interpretable-Tissue-Examination-for-Enhanced-Explainability-in-Breast-Cancer-Histopathology" class="headerlink" title="GRAPHITE: Graph-Based Interpretable Tissue Examination for Enhanced   Explainability in Breast Cancer Histopathology"></a>GRAPHITE: Graph-Based Interpretable Tissue Examination for Enhanced   Explainability in Breast Cancer Histopathology</h2><p><strong>Authors:Raktim Kumar Mondol, Ewan K. A. Millar, Peter H. Graham, Lois Browne, Arcot Sowmya, Erik Meijering</strong></p>
<p>Explainable AI (XAI) in medical histopathology is essential for enhancing the interpretability and clinical trustworthiness of deep learning models in cancer diagnosis. However, the black-box nature of these models often limits their clinical adoption. We introduce GRAPHITE (Graph-based Interpretable Tissue Examination), a post-hoc explainable framework designed for breast cancer tissue microarray (TMA) analysis. GRAPHITE employs a multiscale approach, extracting patches at various magnification levels, constructing an hierarchical graph, and utilising graph attention networks (GAT) with scalewise attention (SAN) to capture scale-dependent features. We trained the model on 140 tumour TMA cores and four benign whole slide images from which 140 benign samples were created, and tested it on 53 pathologist-annotated TMA samples. GRAPHITE outperformed traditional XAI methods, achieving a mean average precision (mAP) of 0.56, an area under the receiver operating characteristic curve (AUROC) of 0.94, and a threshold robustness (ThR) of 0.70, indicating that the model maintains high performance across a wide range of thresholds. In clinical utility, GRAPHITE achieved the highest area under the decision curve (AUDC) of 4.17e+5, indicating reliable decision support across thresholds. These results highlight GRAPHITEâ€™s potential as a clinically valuable tool in computational pathology, providing interpretable visualisations that align with the pathologistsâ€™ diagnostic reasoning and support precision medicine. </p>
<blockquote>
<p>åœ¨åŒ»å­¦ç—…ç†å­¦ä¸­ï¼Œå¯è§£é‡Šäººå·¥æ™ºèƒ½ï¼ˆXAIï¼‰å¯¹äºæé«˜ç™Œç—‡è¯Šæ–­ä¸­æ·±åº¦å­¦ä¹ æ¨¡å‹çš„è§£é‡Šæ€§å’Œä¸´åºŠå¯ä¿¡åº¦è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹çš„â€œé»‘ç®±â€æ€§è´¨å¸¸å¸¸é™åˆ¶äº†å®ƒä»¬åœ¨ä¸´åºŠä¸Šçš„é‡‡çº³ã€‚æˆ‘ä»¬å¼•å…¥äº†GRAPHITEï¼ˆåŸºäºå›¾çš„å¯è§£é‡Šç»„ç»‡æ£€æŸ¥ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹ä¹³è…ºç™Œç»„ç»‡å¾®é˜µåˆ—ï¼ˆTMAï¼‰åˆ†æçš„äº‹åå¯è§£é‡Šæ¡†æ¶ã€‚GRAPHITEé‡‡ç”¨å¤šå°ºåº¦æ–¹æ³•ï¼Œåœ¨ä¸åŒæ”¾å¤§çº§åˆ«æå–æ–‘å—ï¼Œæ„å»ºåˆ†å±‚å›¾ï¼Œå¹¶åˆ©ç”¨å…·æœ‰å°ºåº¦ç›¸å…³æ³¨æ„åŠ›ï¼ˆSANï¼‰çš„å›¾æ³¨æ„åŠ›ç½‘ç»œï¼ˆGATï¼‰æ¥æ•è·å°ºåº¦ç›¸å…³ç‰¹å¾ã€‚æˆ‘ä»¬åœ¨ç”±è‚¿ç˜¤ç»„ç»‡å¾®é˜µåˆ—æ ¸å¿ƒä¸­çš„140ä¸ªæ ·æœ¬å’Œç”±å››å¼ è‰¯æ€§å…¨åˆ‡ç‰‡å›¾åƒåˆ›å»ºçš„å¦å¤–140ä¸ªè‰¯æ€§æ ·æœ¬ä¸Šè®­ç»ƒæ¨¡å‹ï¼Œå¹¶åœ¨ç”±ç—…ç†å­¦å®¶æ³¨é‡Šçš„53ä¸ªTMAæ ·æœ¬ä¸Šè¿›è¡Œæµ‹è¯•ã€‚ä¸ä¼ ç»Ÿçš„XAIæ–¹æ³•ç›¸æ¯”ï¼ŒGRAPHITEè¡¨ç°æ›´ä½³ï¼Œå…¶å¹³å‡ç²¾åº¦å‡å€¼ï¼ˆmAPï¼‰è¾¾åˆ°0.56ï¼Œåœ¨å—è¯•è€…å·¥ä½œç‰¹å¾æ›²çº¿ä¸‹çš„é¢ç§¯ï¼ˆAUROCï¼‰è¾¾åˆ°0.94ï¼Œé˜ˆå€¼ç¨³å¥æ€§ï¼ˆThRï¼‰è¾¾åˆ°0.70ï¼Œè¿™è¡¨æ˜æ¨¡å‹åœ¨å¹¿æ³›é˜ˆå€¼èŒƒå›´å†…å‡è¡¨ç°å‡ºè‰¯å¥½æ€§èƒ½ã€‚åœ¨ä¸´åºŠåº”ç”¨ä¸­ï¼ŒGRAPHITEçš„å†³ç­–æ›²çº¿ä¸‹çš„é¢ç§¯ï¼ˆAUDCï¼‰è¾¾åˆ°æœ€é«˜å€¼ï¼Œå³æ¯è§‚å¯Ÿå°æ—¶åå¤šå†³ç­–åè®¡ç®—çš„æ ¡æ­£å› å­ç­‰äºå¸¸æ•°è®¡ç®—å€¼ä¸ºåä¸‡åˆ†ä¹‹ä¸€çš„æœ€å¤§å¯èƒ½ç´¯è®¡å¾—åˆ†ä¹‹å’Œç­‰äºä¸‡åˆ†ä¹‹ä¸€ç‚¹å…«å››é›¶ä¸‰åŠ é‡æ•°å­—çš„æœ€é«˜ç§¯åˆ†åŸŸæ•°æ®å–å¾—äº†ä¸€æ¬¡è¾¾åˆ°æ¯ä¸ªå¯æ¥å—æ¨¡å‹çš„åˆ¤åˆ«ï¼Œè¯æ˜å…¶æä¾›äº†è§£é‡Šæ€§å¯è§†åŒ–ï¼Œè¿™äº›å¯è§†åŒ–ä¸ç—…ç†åŒ»å¸ˆçš„è¯Šæ–­æ¨ç†ç›¸ä¸€è‡´ï¼Œä¸ºç²¾å‡†åŒ»å­¦æä¾›äº†å¯é çš„å†³ç­–æ”¯æŒã€‚ç»“æœè¯æ˜äº†GRAPHITEåœ¨äººå·¥æ™ºèƒ½ä¸´åºŠç—…ç†é¢†åŸŸçš„æ½œåœ¨åº”ç”¨ä»·å€¼ï¼Œå®ƒå¯ä»¥ä¸ºåŒ»å­¦å†³ç­–åˆ†ææä¾›æœ‰æ„ä¹‰çš„å¯è§£é‡Šå·¥å…·å¹¶ä¿ƒè¿›ç²¾å‡†åŒ»ç–—çš„å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.04206v2">PDF</a> 25 Pages, 10 Figures, 1 Tables</p>
<p><strong>æ‘˜è¦</strong><br>    GRAPHITEï¼ˆåŸºäºå›¾çš„è§£é‡Šæ€§ç»„ç»‡æ£€æµ‹ï¼‰æ˜¯ä¸€ç§ç”¨äºä¹³è…ºç™Œç»„ç»‡å¾®é˜µåˆ—ï¼ˆTMAï¼‰åˆ†æçš„åéªŒå¯è§£é‡Šæ¡†æ¶ã€‚å®ƒé‡‡ç”¨å¤šå°ºåº¦æ–¹æ³•ï¼Œåˆ©ç”¨å›¾æ³¨æ„åŠ›ç½‘ç»œï¼ˆGATï¼‰å’Œå°ºåº¦æ³¨æ„åŠ›ï¼ˆSANï¼‰æ•è·å°ºåº¦ç›¸å…³ç‰¹å¾ï¼Œæé«˜æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨ç™Œç—‡è¯Šæ–­ä¸­çš„å¯è§£é‡Šæ€§å’Œä¸´åºŠå¯ä¿¡åº¦ã€‚åœ¨è‚¿ç˜¤TMAèŠ¯å’Œå…¨å¹»ç¯ç‰‡å›¾åƒä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒGRAPHITEä¼˜äºä¼ ç»ŸXAIæ–¹æ³•ï¼Œå…·æœ‰é«˜å¹³å‡ç²¾åº¦ã€é«˜æ›²çº¿ä¸‹é¢ç§¯å’Œé˜ˆå€¼ç¨³å¥æ€§ã€‚è¿™ä¸ºä¸´åºŠå†³ç­–æ”¯æŒæä¾›äº†å¯é çš„ä¾æ®ï¼Œå‡¸æ˜¾äº†å…¶åœ¨è®¡ç®—ç—…ç†å­¦ä¸­çš„ä¸´åºŠä»·å€¼ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>XAIåœ¨åŒ»å­¦ç—…ç†å­¦ä¸­å¯¹äºå¢å¼ºæ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨ç™Œç—‡è¯Šæ–­ä¸­çš„å¯è§£é‡Šæ€§å’Œä¸´åºŠä¿¡ä»»åº¦è‡³å…³é‡è¦ã€‚</li>
<li>GRAPHITEæ¡†æ¶è¢«å¼•å…¥ä»¥è§£å†³æ·±åº¦å­¦ä¹ æ¨¡å‹çš„â€œé»‘ç®±â€é—®é¢˜ï¼Œæé«˜ä¸´åºŠé‡‡çº³ç‡ã€‚</li>
<li>GRAPHITEé‡‡ç”¨å¤šå°ºåº¦æ–¹æ³•ï¼Œæ•è·å°ºåº¦ç›¸å…³ç‰¹å¾ï¼Œæ„å»ºå±‚æ¬¡å›¾æ¥æé«˜è§£é‡Šæ€§èƒ½ã€‚</li>
<li>GRAPHITEåœ¨è‚¿ç˜¤TMAèŠ¯å’Œå…¨å¹»ç¯ç‰‡å›¾åƒä¸Šè¿›è¡Œè®­ç»ƒå¹¶æµ‹è¯•ï¼Œè¡¨ç°å‡ºä¼˜è¶Šæ€§èƒ½ã€‚</li>
<li>GRAPHITEç›¸å¯¹äºä¼ ç»ŸXAIæ–¹æ³•å…·æœ‰æ›´é«˜çš„å¹³å‡ç²¾åº¦ã€æ›²çº¿ä¸‹é¢ç§¯å’Œé˜ˆå€¼ç¨³å¥æ€§ã€‚</li>
<li>GRAPHITEåœ¨ä¸´åºŠå†³ç­–æ”¯æŒä¸­è¡¨ç°å‡ºå¯é æ€§ï¼Œä¸ç—…ç†åŒ»å¸ˆçš„è¯Šæ–­æ¨ç†ç›¸ç¬¦ï¼Œæ”¯æŒç²¾å‡†åŒ»ç–—ã€‚</li>
<li>ç»“æœå‡¸æ˜¾äº†GRAPHITEåœ¨è®¡ç®—ç—…ç†å­¦ä¸­çš„æ½œåœ¨ä¸´åºŠä»·å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.04206">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-4ad972449fab939bff5cbe953f5b186d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7b149051f8299c719689e65ae35c8a8a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f3007e60e927f7f43d377e4eb488fdcf.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-09/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-09/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-09/TTS/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-a97d731c442f25b3ed5578117f7483ee.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-09  Advancing Zero-shot Text-to-Speech Intelligibility across Diverse   Domains via Preference Alignment
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-09
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-09/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-de804d728ada58b8fa4a97244451761d.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-09  Efficient Flow Matching using Latent Variables
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-09
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">18181.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
