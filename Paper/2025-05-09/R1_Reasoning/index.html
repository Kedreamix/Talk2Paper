<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-09  On Path to Multimodal Generalist General-Level and General-Bench">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-100471ed028d8370f5d09c8ba954c734.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-09
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-26
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    88 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-09-æ›´æ–°"><a href="#2025-05-09-æ›´æ–°" class="headerlink" title="2025-05-09 æ›´æ–°"></a>2025-05-09 æ›´æ–°</h1><h2 id="On-Path-to-Multimodal-Generalist-General-Level-and-General-Bench"><a href="#On-Path-to-Multimodal-Generalist-General-Level-and-General-Bench" class="headerlink" title="On Path to Multimodal Generalist: General-Level and General-Bench"></a>On Path to Multimodal Generalist: General-Level and General-Bench</h2><p><strong>Authors:Hao Fei, Yuan Zhou, Juncheng Li, Xiangtai Li, Qingshan Xu, Bobo Li, Shengqiong Wu, Yaoting Wang, Junbao Zhou, Jiahao Meng, Qingyu Shi, Zhiyuan Zhou, Liangtao Shi, Minghe Gao, Daoan Zhang, Zhiqi Ge, Weiming Wu, Siliang Tang, Kaihang Pan, Yaobo Ye, Haobo Yuan, Tao Zhang, Tianjie Ju, Zixiang Meng, Shilin Xu, Liyu Jia, Wentao Hu, Meng Luo, Jiebo Luo, Tat-Seng Chua, Shuicheng Yan, Hanwang Zhang</strong></p>
<p>The Multimodal Large Language Model (MLLM) is currently experiencing rapid growth, driven by the advanced capabilities of LLMs. Unlike earlier specialists, existing MLLMs are evolving towards a Multimodal Generalist paradigm. Initially limited to understanding multiple modalities, these models have advanced to not only comprehend but also generate across modalities. Their capabilities have expanded from coarse-grained to fine-grained multimodal understanding and from supporting limited modalities to arbitrary ones. While many benchmarks exist to assess MLLMs, a critical question arises: Can we simply assume that higher performance across tasks indicates a stronger MLLM capability, bringing us closer to human-level AI? We argue that the answer is not as straightforward as it seems. This project introduces General-Level, an evaluation framework that defines 5-scale levels of MLLM performance and generality, offering a methodology to compare MLLMs and gauge the progress of existing systems towards more robust multimodal generalists and, ultimately, towards AGI. At the core of the framework is the concept of Synergy, which measures whether models maintain consistent capabilities across comprehension and generation, and across multiple modalities. To support this evaluation, we present General-Bench, which encompasses a broader spectrum of skills, modalities, formats, and capabilities, including over 700 tasks and 325,800 instances. The evaluation results that involve over 100 existing state-of-the-art MLLMs uncover the capability rankings of generalists, highlighting the challenges in reaching genuine AI. We expect this project to pave the way for future research on next-generation multimodal foundation models, providing a robust infrastructure to accelerate the realization of AGI. Project page: <a target="_blank" rel="noopener" href="https://generalist.top/">https://generalist.top/</a> </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ç›®å‰æ­£åœ¨ç»å†å¿«é€Ÿå¢é•¿ï¼Œå…¶é©±åŠ¨å› ç´ åœ¨äºå¤§å‹è¯­è¨€æ¨¡å‹çš„å…ˆè¿›åŠŸèƒ½ã€‚ä¸æ—©æœŸçš„ä¸“å®¶ä¸åŒï¼Œç°æœ‰çš„MLLMæ­£åœ¨æœç€å¤šæ¨¡æ€ç»¼åˆè€…çš„æ¨¡å¼å‘å±•ã€‚è¿™äº›æ¨¡å‹æœ€åˆä»…é™äºç†è§£å¤šç§æ¨¡å¼ï¼Œç°åœ¨å·²ç»å‘å±•åˆ°ä¸ä»…ç†è§£è€Œä¸”èƒ½å¤Ÿåœ¨å„ç§æ¨¡å¼ä¸‹ç”Ÿæˆå†…å®¹ã€‚å®ƒä»¬çš„èƒ½åŠ›å·²ç»ä»ç²—ç²’åº¦æ‰©å±•åˆ°ç»†ç²’åº¦å¤šæ¨¡å¼ç†è§£ï¼Œå¹¶ä¸”ä»æ”¯æŒæœ‰é™çš„æ¨¡å¼æ‰©å±•åˆ°ä»»æ„æ¨¡å¼ã€‚è™½ç„¶å­˜åœ¨è®¸å¤šåŸºå‡†æµ‹è¯•æ¥è¯„ä¼°MLLMï¼Œä½†ä¼šå‡ºç°ä¸€ä¸ªé—®é¢˜ï¼šæˆ‘ä»¬èƒ½å¦ç®€å•åœ°å‡è®¾è·¨ä»»åŠ¡çš„é«˜æ€§èƒ½è¡¨æ˜MLLMèƒ½åŠ›æ›´å¼ºï¼Œä½¿æˆ‘ä»¬æ›´æ¥è¿‘äººç±»æ°´å¹³çš„AIï¼Ÿæˆ‘ä»¬è®¤ä¸ºç­”æ¡ˆå¹¶ä¸åƒçœ‹èµ·æ¥é‚£ä¹ˆç®€å•ã€‚è¿™ä¸ªé¡¹ç›®å¼•å…¥äº†é€šç”¨çº§åˆ«è¯„ä¼°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å®šä¹‰äº†MLLMæ€§èƒ½å’Œé€šç”¨æ€§çš„5ä¸ªçº§åˆ«ï¼Œæä¾›äº†ä¸€ç§æ¯”è¾ƒMLLMå’Œè¡¡é‡ç°æœ‰ç³»ç»Ÿæœç€æ›´ç¨³å¥çš„å¤šæ¨¡å¼ç»¼åˆè€…å’Œæœ€ç»ˆæœç€é€šç”¨äººå·¥æ™ºèƒ½ï¼ˆAGIï¼‰å‘å±•è¿›æ­¥çš„æ–¹æ³•ã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒæ˜¯ååŒæ¦‚å¿µï¼Œå®ƒè¡¡é‡çš„æ˜¯æ¨¡å‹åœ¨ç†è§£å’Œç”Ÿæˆä¹‹é—´ä»¥åŠå¤šä¸ªæ¨¡å¼ä¹‹é—´æ˜¯å¦ä¿æŒä¸€è‡´çš„èƒ½åŠ›ã€‚ä¸ºäº†æ”¯æŒè¿™ä¸€è¯„ä¼°ï¼Œæˆ‘ä»¬æ¨å‡ºäº†é€šç”¨åŸºå‡†æµ‹è¯•ï¼Œå®ƒæ¶µç›–äº†æ›´å¹¿æ³›çš„æŠ€èƒ½ã€æ¨¡å¼ã€æ ¼å¼å’Œèƒ½åŠ›ï¼ŒåŒ…æ‹¬è¶…è¿‡700ä¸ªä»»åŠ¡å’Œ325,800ä¸ªå®ä¾‹ã€‚æ¶‰åŠè¶…è¿‡100ä¸ªç°æœ‰æœ€å…ˆè¿›çš„MLLMçš„è¯„ä¼°ç»“æœæ­ç¤ºäº†ç»¼åˆè€…çš„èƒ½åŠ›æ’åï¼Œå¹¶çªå‡ºäº†å®ç°çœŸæ­£äººå·¥æ™ºèƒ½çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬å¸Œæœ›è¿™ä¸ªé¡¹ç›®èƒ½ä¸ºä¸‹ä¸€ä»£å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹çš„ç ”ç©¶é“ºå¹³é“è·¯ï¼Œæä¾›åŠ é€Ÿå®ç°AGIçš„ç¨³å¥åŸºç¡€è®¾æ–½ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://generalist.top/">https://generalist.top/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.04620v1">PDF</a> ICMLâ€™25, 305 pages, 115 tables, 177 figures, project page:   <a target="_blank" rel="noopener" href="https://generalist.top/">https://generalist.top/</a></p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œç°æœ‰æ¨¡å‹æ­£æœç€å¤šæ¨¡æ€é€šæ‰æ¨¡å¼å‘å±•ã€‚å®ƒä»¬çš„èƒ½åŠ›å·²ä»ç²—ç²’åº¦æ‰©å±•åˆ°ç»†ç²’åº¦ï¼Œä»æ”¯æŒæœ‰é™æ¨¡æ€æ‰©å±•åˆ°ä»»æ„æ¨¡æ€ã€‚ç„¶è€Œï¼Œè¯„ä¼°MLLMæ—¶ä¸èƒ½ä»…ä¾èµ–ä»»åŠ¡æ€§èƒ½æ¥é¢„æµ‹å…¶æ•´ä½“èƒ½åŠ›ï¼Œå› æ­¤æå‡ºäº†General-Levelè¯„ä¼°æ¡†æ¶å’ŒGeneral-Benchè¯„ä¼°æ”¯æŒå·¥å…·ã€‚è¯¥æ¡†æ¶å®šä¹‰äº†MLLMæ€§èƒ½çš„äº”ä¸ªçº§åˆ«ï¼Œå¹¶å›´ç»•ååŒå·¥ä½œçš„æ¦‚å¿µè¯„ä¼°æ¨¡å‹åœ¨ç†è§£å’Œç”Ÿæˆæ–¹é¢ä»¥åŠè·¨å¤šä¸ªæ¨¡æ€çš„èƒ½åŠ›ã€‚æœŸæœ›æ­¤é¡¹ç›®ä¸ºæœªæ¥ç ”ç©¶ä¸‹ä¸€ä»£å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹é“ºå¹³é“è·¯ï¼Œå¹¶ä¸ºåŠ é€Ÿäººå·¥æ™ºèƒ½é€šç”¨æ€§ï¼ˆAGIï¼‰çš„å®ç°æä¾›ç¨³å¥çš„åŸºç¡€è®¾æ–½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MLLMæ­£ç»å†å¿«é€Ÿå‘å±•ï¼Œä»å¤šæ¨¡æ€ç†è§£è¿›åŒ–åˆ°å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆã€‚</li>
<li>MLLMçš„èƒ½åŠ›å·²ä»ç²—ç²’åº¦æ‰©å±•åˆ°ç»†ç²’åº¦ï¼Œæ”¯æŒä»»æ„æ¨¡æ€ã€‚</li>
<li>è¯„ä¼°MLLMçš„èƒ½åŠ›ä¸èƒ½ä»…ä¾èµ–ä»»åŠ¡æ€§èƒ½ï¼Œéœ€è¦æ›´å…¨é¢çš„è¯„ä¼°æ¡†æ¶å¦‚General-Levelã€‚</li>
<li>General-Levelè¯„ä¼°æ¡†æ¶å®šä¹‰MLLMæ€§èƒ½çš„äº”ä¸ªçº§åˆ«ï¼Œå¹¶å›´ç»•Synergyæ¦‚å¿µè¯„ä¼°æ¨¡å‹åœ¨å„ç§ä»»åŠ¡å’Œæ¨¡æ€ä¸­çš„ä¸€è‡´æ€§èƒ½åŠ›ã€‚</li>
<li>General-Benchæä¾›äº†å¹¿æ³›çš„æŠ€èƒ½ã€æ¨¡æ€ã€æ ¼å¼å’Œèƒ½åŠ›è¯„ä¼°æ”¯æŒï¼ŒåŒ…å«700å¤šä¸ªä»»åŠ¡å’Œ325,800ä¸ªå®ä¾‹ã€‚</li>
<li>è¯„ä¼°ç»“æœæ¶‰åŠ100å¤šä¸ªç°æœ‰æœ€å…ˆè¿›çš„MLLMï¼Œæ­ç¤ºäº†é€šæ‰çš„èƒ½åŠ›æ’åï¼Œå¹¶æŒ‡å‡ºäº†å®ç°çœŸæ­£äººå·¥æ™ºèƒ½çš„æŒ‘æˆ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.04620">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-69b2a18376d3518d367ef5dc3cbce6b9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-43d7956692b153fc6f51178d4187df4a.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="VideoPath-LLaVA-Pathology-Diagnostic-Reasoning-Through-Video-Instruction-Tuning"><a href="#VideoPath-LLaVA-Pathology-Diagnostic-Reasoning-Through-Video-Instruction-Tuning" class="headerlink" title="VideoPath-LLaVA: Pathology Diagnostic Reasoning Through Video   Instruction Tuning"></a>VideoPath-LLaVA: Pathology Diagnostic Reasoning Through Video   Instruction Tuning</h2><p><strong>Authors:Trinh T. L. Vuong, Jin Tae Kwak</strong></p>
<p>We present VideoPath-LLaVA, the first large multimodal model (LMM) in computational pathology that integrates three distinct image scenarios, single patch images, automatically keyframe-extracted clips, and manually segmented video pathology images, to mimic the natural diagnostic process of pathologists. By generating detailed histological descriptions and culminating in a definitive sign-out diagnosis, VideoPath-LLaVA bridges visual narratives with diagnostic reasoning.   Central to our approach is the VideoPath-Instruct dataset, comprising 4278 video and diagnosis-specific chain-of-thought instructional pairs sourced from educational histopathology videos on YouTube. Although high-quality data is critical for enhancing diagnostic reasoning, its creation is time-intensive and limited in volume. To overcome this challenge, we transfer knowledge from existing single-image instruction datasets to train on weakly annotated, keyframe-extracted clips, followed by fine-tuning on manually segmented videos. VideoPath-LLaVA establishes a new benchmark in pathology video analysis and offers a promising foundation for future AI systems that support clinical decision-making through integrated visual and diagnostic reasoning. Our code, data, and model are publicly available at <a target="_blank" rel="noopener" href="https://github.com/trinhvg/VideoPath-LLaVA">https://github.com/trinhvg/VideoPath-LLaVA</a>. </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºVideoPath-LLaVAï¼Œè¿™æ˜¯è®¡ç®—ç—…ç†å­¦é¢†åŸŸé¦–ä¸ªå¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMï¼‰ï¼Œå®ƒæ•´åˆäº†ä¸‰ç§ä¸åŒçš„å›¾åƒæƒ…æ™¯ï¼šå•è¡¥ä¸å›¾åƒã€è‡ªåŠ¨æå–å…³é”®å¸§çš„å‰ªè¾‘å’Œæ‰‹åŠ¨åˆ†å‰²çš„è§†é¢‘ç—…ç†å›¾åƒï¼Œä»¥æ¨¡æ‹Ÿç—…ç†åŒ»ç”Ÿçš„è‡ªç„¶è¯Šæ–­è¿‡ç¨‹ã€‚é€šè¿‡ç”Ÿæˆè¯¦ç»†çš„ç»„ç»‡ç—…ç†å­¦æè¿°ï¼Œå¹¶æœ€ç»ˆä½œå‡ºæ˜ç¡®çš„è¯Šæ–­ç»“è®ºï¼ŒVideoPath-LLaVAå°†è§†è§‰å™äº‹ä¸è¯Šæ–­æ¨ç†ç›¸ç»“åˆã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„æ ¸å¿ƒæ˜¯VideoPath-Instructæ•°æ®é›†ï¼ŒåŒ…å«4278ä¸ªè§†é¢‘å’Œé’ˆå¯¹è¯Šæ–­çš„ç‰¹å®šæ€ç»´é“¾æ•™å­¦å¯¹ï¼Œè¿™äº›æ•°æ®æ¥æºäºYouTubeä¸Šçš„æ•™è‚²ç—…ç†å­¦è§†é¢‘ã€‚è™½ç„¶é«˜è´¨é‡çš„æ•°æ®å¯¹äºæé«˜è¯Šæ–­æ¨ç†è‡³å…³é‡è¦ï¼Œä½†å…¶åˆ¶ä½œè€—æ—¶ä¸”æ•°é‡æœ‰é™ã€‚ä¸ºäº†å…‹æœè¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬ä»ç°æœ‰çš„å•å›¾åƒæŒ‡ä»¤æ•°æ®é›†è½¬ç§»çŸ¥è¯†ï¼Œå¯¹å¼±æ³¨é‡Šã€æå–å…³é”®å¸§çš„å‰ªè¾‘è¿›è¡Œè®­ç»ƒï¼Œç„¶ååœ¨æ‰‹åŠ¨åˆ†å‰²çš„è§†é¢‘ä¸Šè¿›è¡Œå¾®è°ƒã€‚VideoPath-LLaVAåœ¨ç—…ç†å­¦è§†é¢‘åˆ†ææ–¹é¢å»ºç«‹äº†æ–°çš„åŸºå‡†ï¼Œå¹¶ä¸ºæœªæ¥é€šè¿‡é›†æˆè§†è§‰å’Œè¯Šæ–­æ¨ç†æ”¯æŒä¸´åºŠå†³ç­–çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿæä¾›äº†æœ‰å¸Œæœ›çš„åŸºçŸ³ã€‚æˆ‘ä»¬çš„ä»£ç ã€æ•°æ®å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/trinhvg/VideoPath-LLaVA%E5%85%AC%E5%BC%BA%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/trinhvg/VideoPath-LLaVAå…¬å¼€è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.04192v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šæˆ‘ä»¬æå‡ºäº†VideoPath-LLaVAæ¨¡å‹ï¼Œè¿™æ˜¯è®¡ç®—ç—…ç†å­¦é¢†åŸŸé¦–ä¸ªå¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMï¼‰ï¼Œå®ƒæ•´åˆäº†ä¸‰ç§ä¸åŒçš„å›¾åƒæƒ…æ™¯ï¼šå•è¡¥ä¸å›¾åƒã€è‡ªåŠ¨æå–å…³é”®å¸§çš„å‰ªè¾‘å’Œæ‰‹åŠ¨åˆ†å‰²çš„è§†é¢‘ç—…ç†å›¾åƒï¼Œä»¥æ¨¡ä»¿ç—…ç†åŒ»ç”Ÿçš„è‡ªç„¶è¯Šæ–­è¿‡ç¨‹ã€‚é€šè¿‡ç”Ÿæˆè¯¦ç»†çš„ç»„ç»‡æè¿°å¹¶æœ€ç»ˆä½œå‡ºæ˜ç¡®çš„è¯Šæ–­ç»“è®ºï¼ŒVideoPath-LLaVAå°†è§†è§‰å™äº‹ä¸è¯Šæ–­æ¨ç†ç›¸ç»“åˆã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„æ ¸å¿ƒæ˜¯VideoPath-Instructæ•°æ®é›†ï¼ŒåŒ…å«ä»YouTubeä¸Šçš„æ•™è‚²ç—…ç†è§†é¢‘æ”¶é›†çš„4278ä¸ªè§†é¢‘å’Œé’ˆå¯¹è¯Šæ–­çš„æ€ç»´é“¾æŒ‡ä»¤å¯¹ã€‚å°½ç®¡é«˜è´¨é‡çš„æ•°æ®å¯¹äºæé«˜è¯Šæ–­æ¨ç†è‡³å…³é‡è¦ï¼Œä½†å…¶åˆ¶ä½œæ˜¯è€—æ—¶çš„ä¸”æ•°é‡æœ‰é™ã€‚ä¸ºäº†å…‹æœè¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬ä»ç°æœ‰çš„å•æŒ‡ä»¤å›¾åƒæ•°æ®é›†ä¸­è½¬ç§»çŸ¥è¯†ï¼Œå¯¹æå–çš„å…³é”®å¸§è¿›è¡Œå¼±æ ‡æ³¨è®­ç»ƒï¼Œç„¶åå¯¹æ‰‹åŠ¨åˆ†å‰²çš„è§†é¢‘è¿›è¡Œå¾®è°ƒã€‚VideoPath-LLaVAåœ¨ç—…ç†è§†é¢‘åˆ†æä¸­å»ºç«‹äº†æ–°çš„åŸºå‡†ï¼Œå¹¶ä¸ºæœªæ¥é€šè¿‡é›†æˆè§†è§‰å’Œè¯Šæ–­æ¨ç†æ”¯æŒä¸´åºŠå†³ç­–çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿæä¾›äº†æœ‰å¸Œæœ›çš„åŸºçŸ³ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>VideoPath-LLaVAæ˜¯é¦–ä¸ªåœ¨è®¡ç®—ç—…ç†å­¦é¢†åŸŸçš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMï¼‰ï¼Œæ¨¡æ‹Ÿç—…ç†åŒ»ç”Ÿçš„è‡ªç„¶è¯Šæ–­è¿‡ç¨‹ã€‚</li>
<li>è¯¥æ¨¡å‹é›†æˆäº†ä¸‰ç§ä¸åŒçš„å›¾åƒæƒ…æ™¯ï¼šå•è¡¥ä¸å›¾åƒã€è‡ªåŠ¨æå–å…³é”®å¸§çš„å‰ªè¾‘å’Œæ‰‹åŠ¨åˆ†å‰²çš„è§†é¢‘ç—…ç†å›¾åƒã€‚</li>
<li>VideoPath-LLaVAé€šè¿‡è¯¦ç»†çš„ç»„ç»‡æè¿°å’Œè¯Šæ–­ç»“è®ºï¼Œå°†è§†è§‰å™äº‹ä¸è¯Šæ–­æ¨ç†ç›¸ç»“åˆã€‚</li>
<li>VideoPath-Instructæ•°æ®é›†æ˜¯æ ¸å¿ƒï¼Œç”±æ¥è‡ªYouTubeæ•™è‚²ç—…ç†è§†é¢‘çš„è¯Šæ–­ç‰¹å®šæ€ç»´é“¾æŒ‡ä»¤å¯¹ç»„æˆã€‚</li>
<li>å°½ç®¡é«˜è´¨é‡æ•°æ®å¯¹è¯Šæ–­æ¨ç†è‡³å…³é‡è¦ï¼Œä½†å…¶åˆ¶ä½œæˆæœ¬é«˜æ˜‚ä¸”æ•°é‡æœ‰é™ã€‚</li>
<li>ä¸ºäº†è§£å†³æ•°æ®é™åˆ¶é—®é¢˜ï¼Œæ¨¡å‹ä»å•æŒ‡ä»¤å›¾åƒæ•°æ®é›†ä¸­è½¬ç§»çŸ¥è¯†ï¼Œå¹¶å¯¹å¼±æ ‡æ³¨çš„å…³é”®å¸§è¿›è¡Œè®­ç»ƒï¼Œå†è¿›è¡Œæ‰‹åŠ¨åˆ†å‰²è§†é¢‘çš„å¾®è°ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.04192">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3e62c63ba1323d03aa27df2a5a4d9ab2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-90fb6ce35c6f465f65a0e4a8a426377f.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="R-3-VQA-â€œRead-the-Roomâ€-by-Video-Social-Reasoning"><a href="#R-3-VQA-â€œRead-the-Roomâ€-by-Video-Social-Reasoning" class="headerlink" title="R^3-VQA: â€œRead the Roomâ€ by Video Social Reasoning"></a>R^3-VQA: â€œRead the Roomâ€ by Video Social Reasoning</h2><p><strong>Authors:Lixing Niu, Jiapeng Li, Xingping Yu, Shu Wang, Ruining Feng, Bo Wu, Ping Wei, Yisen Wang, Lifeng Fan</strong></p>
<p>â€œRead the roomâ€ is a significant social reasoning capability in human daily life. Humans can infer othersâ€™ mental states from subtle social cues. Previous social reasoning tasks and datasets lack complexity (e.g., simple scenes, basic interactions, incomplete mental state variables, single-step reasoning, etc.) and fall far short of the challenges present in real-life social interactions. In this paper, we contribute a valuable, high-quality, and comprehensive video dataset named R^3-VQA with precise and fine-grained annotations of social events and mental states (i.e., belief, intent, desire, and emotion) as well as corresponding social causal chains in complex social scenarios. Moreover, we include human-annotated and model-generated QAs. Our task R^3-VQA includes three aspects: Social Event Understanding, Mental State Estimation, and Social Causal Reasoning. As a benchmark, we comprehensively evaluate the social reasoning capabilities and consistencies of current state-of-the-art large vision-language models (LVLMs). Comprehensive experiments show that (i) LVLMs are still far from human-level consistent social reasoning in complex social scenarios; (ii) Theory of Mind (ToM) prompting can help LVLMs perform better on social reasoning tasks. We provide some of our dataset and codes in supplementary material and will release our full dataset and codes upon acceptance. </p>
<blockquote>
<p>åœ¨æ—¥å¸¸ç”Ÿæ´»ä¸­ï¼Œâ€è¯»æ‡‚æ°›å›´â€æ˜¯ä¸€ç§é‡è¦çš„ç¤¾ä¼šæ¨ç†èƒ½åŠ›ã€‚äººç±»å¯ä»¥ä»å¾®å¦™çš„ç¤¾äº¤çº¿ç´¢ä¸­æ¨æ–­å‡ºä»–äººçš„å¿ƒç†çŠ¶æ€ã€‚ä»¥å¾€çš„ç¤¾ä¼šæ¨ç†ä»»åŠ¡å’Œæ•°æ®é›†ç¼ºä¹å¤æ‚æ€§ï¼ˆä¾‹å¦‚ï¼Œç®€å•åœºæ™¯ã€åŸºæœ¬äº’åŠ¨ã€ä¸å®Œæ•´çš„å¿ƒæ€å˜é‡ã€å•æ­¥æ¨ç†ç­‰ï¼‰ï¼Œä¸ç°å®ç”Ÿæ´»ä¸­çš„ç¤¾äº¤äº’åŠ¨æŒ‘æˆ˜ç›¸å»ç”šè¿œã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è´¡çŒ®äº†ä¸€ä¸ªæœ‰ä»·å€¼ã€é«˜è´¨é‡ä¸”å…¨é¢çš„è§†é¢‘æ•°æ®é›†ï¼Œåä¸ºR^3-VQAï¼Œå…¶ä¸­åŒ…å«ç¤¾ä¼šäº‹ä»¶å’Œå¿ƒæ€ï¼ˆå³ä¿¡å¿µã€æ„å›¾ã€æ„¿æœ›å’Œæƒ…æ„Ÿï¼‰çš„ç²¾ç¡®å’Œç²¾ç»†æ³¨é‡Šï¼Œä»¥åŠå¤æ‚ç¤¾ä¼šåœºæ™¯ä¸­çš„ç›¸åº”ç¤¾ä¼šå› æœé“¾ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åŒ…å«äº†äººç±»æ³¨é‡Šå’Œæ¨¡å‹ç”Ÿæˆçš„QAã€‚æˆ‘ä»¬çš„ä»»åŠ¡R^3-VQAåŒ…æ‹¬ä¸‰ä¸ªæ–¹é¢ï¼šç¤¾ä¼šäº‹ä»¶ç†è§£ã€å¿ƒæ€ä¼°è®¡å’Œç¤¾ä¼šå› æœæ¨ç†ã€‚ä½œä¸ºä¸€ä¸ªåŸºå‡†æµ‹è¯•ï¼Œæˆ‘ä»¬å…¨é¢è¯„ä¼°äº†å½“å‰å…ˆè¿›çš„å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰çš„ç¤¾ä¼šæ¨ç†èƒ½åŠ›å’Œä¸€è‡´æ€§ã€‚ç»¼åˆå®éªŒè¡¨æ˜ï¼Œï¼ˆiï¼‰LVLMsåœ¨å¤æ‚ç¤¾ä¼šåœºæ™¯ä¸­çš„ç¤¾ä¼šæ¨ç†èƒ½åŠ›ä¸äººç±»æ°´å¹³çš„ä¸€è‡´æ€§ä»æœ‰å¾ˆå¤§å·®è·ï¼›ï¼ˆiiï¼‰å¿ƒæ™ºç†è®ºï¼ˆToMï¼‰æç¤ºå¯ä»¥å¸®åŠ©LVLMsåœ¨ç¤¾äº¤æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°æ›´å¥½ã€‚æˆ‘ä»¬åœ¨è¡¥å……ææ–™ä¸­æä¾›éƒ¨åˆ†æ•°æ®é›†å’Œä»£ç ï¼Œå¹¶åœ¨æ¥å—åå…¬å¼€æˆ‘ä»¬çš„å®Œæ•´æ•°æ®é›†å’Œä»£ç ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.04147v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>äººä»¬æ—¥å¸¸ç¤¾äº¤æ´»åŠ¨ä¸­ï¼Œé€šè¿‡é˜…è¯»ç¤¾äº¤çº¿ç´¢æ¨æ–­ä»–äººçš„å¿ƒç†çŠ¶æ€æ˜¯ä¸€ç§é‡è¦çš„ç¤¾ä¼šè®¤çŸ¥èƒ½åŠ›ã€‚ç°æœ‰ç¤¾ä¼šæ¨ç†ä»»åŠ¡å’Œæ•°æ®é›†ç¼ºä¹å¤æ‚æ€§ï¼Œæ— æ³•åº”å¯¹çœŸå®ç¤¾äº¤äº’åŠ¨ä¸­çš„æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶è´¡çŒ®äº†ä¸€ä¸ªé«˜è´¨é‡çš„ç»¼åˆè§†é¢‘æ•°æ®é›†R^3-VQAï¼Œå…¶ä¸­åŒ…å«å¯¹ç¤¾ä¼šäº‹ä»¶å’Œå†…å¿ƒçŠ¶æ€ï¼ˆä¿¡å¿µã€æ„å›¾ã€æ„¿æœ›å’Œæƒ…æ„Ÿï¼‰çš„ç²¾ç¡®å’Œç²¾ç»†æ³¨é‡Šï¼Œä»¥åŠå¤æ‚ç¤¾äº¤åœºæ™¯ä¸­çš„ç¤¾ä¼šå› æœé“¾ã€‚æ­¤å¤–ï¼Œè¿˜åŒ…æ‹¬äººç±»æ ‡æ³¨å’Œæ¨¡å‹ç”Ÿæˆçš„QAã€‚è¯¥ä»»åŠ¡åŒ…æ‹¬ä¸‰ä¸ªæ–¹é¢ï¼šç¤¾ä¼šäº‹ä»¶ç†è§£ã€å†…å¿ƒçŠ¶æ€ä¼°è®¡å’Œç¤¾ä¼šå› æœæ¨ç†ã€‚ä½œä¸ºåŸºå‡†æµ‹è¯•ï¼Œæˆ‘ä»¬å¯¹å½“å‰å…ˆè¿›çš„è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œäº†å…¨é¢çš„ç¤¾ä¼šæ¨ç†èƒ½åŠ›å’Œä¸€è‡´æ€§è¯„ä¼°ã€‚å®éªŒè¡¨æ˜ï¼Œè¿™äº›æ¨¡å‹åœ¨å¤æ‚ç¤¾äº¤åœºæ™¯ä¸­çš„ç¤¾ä¼šæ¨ç†èƒ½åŠ›ä¸äººç±»æ°´å¹³ç›¸å·®ç”šè¿œï¼Œè€Œå¿ƒæ™ºç†è®ºï¼ˆToMï¼‰æç¤ºæœ‰åŠ©äºæ”¹è¿›å…¶åœ¨ç¤¾ä¼šæ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚æˆ‘ä»¬å°†éƒ¨åˆ†æ•°æ®é›†å’Œä»£ç ä½œä¸ºè¡¥å……ææ–™æä¾›ï¼Œå¹¶åœ¨æ¥å—åå…¬å¼€å®Œæ•´æ•°æ®é›†å’Œä»£ç ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>â€œè¯»æ‡‚æˆ¿é—´â€æ˜¯äººç±»æ—¥å¸¸ç”Ÿæ´»ä¸­é‡è¦çš„ç¤¾ä¼šæ¨ç†èƒ½åŠ›ï¼Œèƒ½é€šè¿‡å¾®å¦™çš„ç¤¾äº¤çº¿ç´¢æ¨æ–­ä»–äººçš„å¿ƒç†çŠ¶æ€ã€‚</li>
<li>ç°æœ‰ç¤¾ä¼šæ¨ç†ä»»åŠ¡å’Œæ•°æ®é›†ç¼ºä¹å¤æ‚æ€§ï¼Œæ— æ³•åæ˜ çœŸå®ç¤¾äº¤äº’åŠ¨çš„æŒ‘æˆ˜ã€‚</li>
<li>R^3-VQAæ•°æ®é›†åŒ…å«å¯¹ç¤¾ä¼šäº‹ä»¶å’Œå†…å¿ƒçŠ¶æ€çš„ç²¾ç¡®å’Œç²¾ç»†æ³¨é‡Šï¼Œä»¥åŠå¤æ‚ç¤¾äº¤åœºæ™¯ä¸­çš„ç¤¾ä¼šå› æœé“¾ã€‚</li>
<li>R^3-VQAä»»åŠ¡åŒ…æ‹¬ç¤¾ä¼šäº‹ä»¶ç†è§£ã€å†…å¿ƒçŠ¶æ€ä¼°è®¡å’Œç¤¾ä¼šå› æœæ¨ç†ä¸‰ä¸ªæ–¹é¢ã€‚</li>
<li>å½“å‰å…ˆè¿›çš„è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ç¤¾ä¼šæ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°ä¸è¶³ï¼Œä¸äººç±»çš„å·®è·æ˜æ˜¾ã€‚</li>
<li>å¿ƒæ™ºç†è®ºï¼ˆToMï¼‰æç¤ºæœ‰åŠ©äºæ”¹è¿›æ¨¡å‹åœ¨ç¤¾ä¼šæ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.04147">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0754c7d788dfb3b2ce8cc513e4ceeeaf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2b3edf612a9c8ac2a14b611f26f2f9d7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d4430703aef17570bb42ee5237b38abd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-de994cc4a13b3c55da953f070094f907.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d167167e36c764ad01ffeaae9e6722ff.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="X-Reasoner-Towards-Generalizable-Reasoning-Across-Modalities-and-Domains"><a href="#X-Reasoner-Towards-Generalizable-Reasoning-Across-Modalities-and-Domains" class="headerlink" title="X-Reasoner: Towards Generalizable Reasoning Across Modalities and   Domains"></a>X-Reasoner: Towards Generalizable Reasoning Across Modalities and   Domains</h2><p><strong>Authors:Qianchu Liu, Sheng Zhang, Guanghui Qin, Timothy Ossowski, Yu Gu, Ying Jin, Sid Kiblawi, Sam Preston, Mu Wei, Paul Vozila, Tristan Naumann, Hoifung Poon</strong></p>
<p>Recent proprietary models (e.g., o3) have begun to demonstrate strong multimodal reasoning capabilities. Yet, most existing open-source research concentrates on training text-only reasoning models, with evaluations limited to mainly mathematical and general-domain tasks. Therefore, it remains unclear how to effectively extend reasoning capabilities beyond text input and general domains. This paper explores a fundamental research question: Is reasoning generalizable across modalities and domains? Our findings support an affirmative answer: General-domain text-based post-training can enable such strong generalizable reasoning. Leveraging this finding, we introduce X-Reasoner, a vision-language model post-trained solely on general-domain text for generalizable reasoning, using a two-stage approach: an initial supervised fine-tuning phase with distilled long chain-of-thoughts, followed by reinforcement learning with verifiable rewards. Experiments show that X-Reasoner successfully transfers reasoning capabilities to both multimodal and out-of-domain settings, outperforming existing state-of-the-art models trained with in-domain and multimodal data across various general and medical benchmarks (Figure 1). Additionally, we find that X-Reasonerâ€™s performance in specialized domains can be further enhanced through continued training on domain-specific text-only data. Building upon this, we introduce X-Reasoner-Med, a medical-specialized variant that achieves new state of the art on numerous text-only and multimodal medical benchmarks. </p>
<blockquote>
<p>æœ€è¿‘çš„ä¸“æœ‰æ¨¡å‹ï¼ˆä¾‹å¦‚o3ï¼‰å·²ç»å¼€å§‹å±•ç°å‡ºå¼ºå¤§çš„å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç›®å‰å¤§å¤šæ•°å¼€æºç ”ç©¶éƒ½é›†ä¸­åœ¨è®­ç»ƒçº¯æ–‡æœ¬æ¨ç†æ¨¡å‹ä¸Šï¼Œè¯„ä¼°ä¹Ÿä¸»è¦é™äºæ•°å­¦å’Œé€šç”¨é¢†åŸŸçš„ä»»åŠ¡ã€‚å› æ­¤ï¼Œå¦‚ä½•æœ‰æ•ˆåœ°æ‰©å±•è¶…å‡ºæ–‡æœ¬è¾“å…¥å’Œé€šç”¨é¢†åŸŸçš„æ¨ç†èƒ½åŠ›ä»ä¸æ˜ç¡®ã€‚æœ¬æ–‡æ¢è®¨äº†ä¸€ä¸ªåŸºæœ¬çš„ç ”ç©¶é—®é¢˜ï¼šæ¨ç†æ˜¯å¦å¯ä»¥åœ¨è·¨æ¨¡æ€å’Œé¢†åŸŸä¹‹é—´è¿›è¡Œæ³›åŒ–ï¼Ÿæˆ‘ä»¬çš„ç ”ç©¶ç»“æœæ”¯æŒè‚¯å®šçš„ç­”æ¡ˆï¼šåŸºäºé€šç”¨é¢†åŸŸçš„æ–‡æœ¬åè®­ç»ƒå¯ä»¥å®ç°å¼ºå¤§çš„å¯æ³›åŒ–æ¨ç†ã€‚åˆ©ç”¨è¿™ä¸€å‘ç°ï¼Œæˆ‘ä»¬å¼•å…¥äº†X-Reasonerï¼Œå®ƒæ˜¯ä¸€ä¸ªä»…åœ¨é€šç”¨æ–‡æœ¬åŸºç¡€ä¸Šè¿›è¡Œåè®­ç»ƒçš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼Œç”¨äºé€šç”¨æ¨ç†ã€‚å®ƒé‡‡ç”¨ä¸¤é˜¶æ®µçš„æ–¹æ³•ï¼šé¦–å…ˆç”¨è’¸é¦åçš„é•¿æ€è€ƒé“¾è¿›è¡Œæœ‰ç›‘ç£å¾®è°ƒï¼Œéšåç”¨å¯éªŒè¯çš„å¥–åŠ±è¿›è¡Œå¼ºåŒ–å­¦ä¹ ã€‚å®éªŒè¡¨æ˜ï¼ŒX-ReasoneræˆåŠŸå°†æ¨ç†èƒ½åŠ›è½¬ç§»åˆ°å¤šæ¨¡æ€å’Œè·¨é¢†åŸŸç¯å¢ƒï¼Œä¸”åœ¨å„ç§é€šç”¨å’ŒåŒ»ç–—åŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†ä½¿ç”¨é¢†åŸŸå†…å’Œå¤šæ¨¡æ€æ•°æ®è®­ç»ƒçš„ç°æœ‰æœ€å…ˆè¿›çš„æ¨¡å‹ï¼ˆå¦‚å›¾1æ‰€ç¤ºï¼‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°é€šè¿‡ç»§ç»­åœ¨ç‰¹å®šé¢†åŸŸçš„çº¯æ–‡æœ¬æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼ŒX-Reasoneråœ¨ç‰¹å®šé¢†åŸŸçš„æ€§èƒ½å¯ä»¥è¿›ä¸€æ­¥æé«˜ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æ¨å‡ºäº†ä¸“ä¸šåŒ–çš„åŒ»ç–—ç‰ˆX-Reasoner-Medï¼Œå®ƒåœ¨å¤šä¸ªçº¯æ–‡æœ¬å’Œå¤šæ¨¡æ€åŒ»ç–—åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æ–°çš„æŠ€æœ¯æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.03981v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢ç´¢äº†è·¨æ¨¡æ€å’Œé¢†åŸŸæ¨ç†èƒ½åŠ›æ˜¯å¦å¯æ³›åŒ–çš„é—®é¢˜ï¼Œå¹¶å‘ç°é€šè¿‡åŸºäºä¸€èˆ¬é¢†åŸŸæ–‡æœ¬çš„åè®­ç»ƒå¯ä»¥å®ç°å¯¹æ³›åŒ–æ¨ç†çš„å¼ºåŒ–ã€‚ç ”ç©¶æå‡ºäº†X-Reasoneræ¨¡å‹ï¼Œé‡‡ç”¨ä¸¤é˜¶æ®µæ–¹æ³•ï¼šåˆå§‹é˜¶æ®µä½¿ç”¨è’¸é¦é•¿æ€è€ƒé“¾è¿›è¡Œç²¾ç»†å¾®è°ƒï¼Œéšåä½¿ç”¨å¯éªŒè¯å¥–åŠ±è¿›è¡Œå¼ºåŒ–å­¦ä¹ ã€‚å®éªŒè¡¨æ˜ï¼ŒX-ReasoneræˆåŠŸå°†æ¨ç†èƒ½åŠ›è½¬ç§»åˆ°å¤šæ¨¡æ€å’Œè·¨é¢†åŸŸåœºæ™¯ï¼Œå¹¶åœ¨å„ç§é€šç”¨å’ŒåŒ»ç–—åŸºå‡†æµ‹è¯•ä¸­ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ¨¡å‹ã€‚æ­¤å¤–ï¼Œè¿˜å‘ç°é€šè¿‡ç»§ç»­è®­ç»ƒç‰¹å®šé¢†åŸŸçš„æ–‡æœ¬æ•°æ®ï¼ŒX-Reasoneråœ¨ç‰¹å®šé¢†åŸŸçš„æ€§èƒ½å¯ä»¥å¾—åˆ°è¿›ä¸€æ­¥æå‡ã€‚åŸºäºæ­¤ï¼Œç ”ç©¶è¿˜æ¨å‡ºäº†é’ˆå¯¹åŒ»ç–—é¢†åŸŸçš„ä¸“é—¨ç‰ˆæœ¬X-Reasoner-Medï¼Œåœ¨å¤šä¸ªçº¯æ–‡æœ¬å’Œå¤šæ¨¡æ€åŒ»ç–—åŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†æœ€æ–°æŠ€æœ¯æˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¿‘æœŸä¸“æœ‰æ¨¡å‹å¼€å§‹å±•ç¤ºå¼ºå¤§çš„å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ï¼Œä½†å¼€æºç ”ç©¶ä¸»è¦é›†ä¸­åœ¨è®­ç»ƒæ–‡æœ¬æ¨ç†æ¨¡å‹ä¸Šï¼Œè¯„ä¼°ä¸»è¦é™äºæ•°å­¦å’Œé€šç”¨é¢†åŸŸä»»åŠ¡ã€‚</li>
<li>æœ¬æ–‡æ¢ç´¢äº†è·¨æ¨¡æ€å’Œé¢†åŸŸæ¨ç†èƒ½åŠ›çš„æ³›åŒ–é—®é¢˜ï¼Œå¹¶å‘ç°åŸºäºä¸€èˆ¬é¢†åŸŸæ–‡æœ¬çš„åè®­ç»ƒæœ‰åŠ©äºå¼ºåŒ–æ³›åŒ–æ¨ç†èƒ½åŠ›ã€‚</li>
<li>å¼•å…¥X-Reasoneræ¨¡å‹ï¼Œé‡‡ç”¨ä¸¤é˜¶æ®µæ–¹æ³•è¿›è¡Œè®­ç»ƒï¼šç²¾ç»†å¾®è°ƒé˜¶æ®µå’Œå¼ºåŒ–å­¦ä¹ é˜¶æ®µã€‚</li>
<li>X-ReasoneræˆåŠŸå°†æ¨ç†èƒ½åŠ›è½¬ç§»åˆ°å¤šæ¨¡æ€å’Œè·¨é¢†åŸŸåœºæ™¯ï¼Œå¹¶åœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>X-Reasoneråœ¨ç‰¹å®šé¢†åŸŸçš„æ€§èƒ½å¯é€šè¿‡ç»§ç»­è®­ç»ƒç‰¹å®šé¢†åŸŸçš„æ–‡æœ¬æ•°æ®è¿›ä¸€æ­¥æå‡ã€‚</li>
<li>æ¨å‡ºé’ˆå¯¹åŒ»ç–—é¢†åŸŸçš„ä¸“é—¨ç‰ˆæœ¬X-Reasoner-Medï¼Œåœ¨åŒ»ç–—åŸºå‡†æµ‹è¯•ä¸Šå–å¾—æœ€æ–°æŠ€æœ¯æˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.03981">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-051beff3ad49cb2bc1f15e4651dba018.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6584117351abaff0ef4dc1c05a94f9d5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f5d732489d99e42d73242b6ce961c115.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5d4ca950f9855521a770dd6ed0b4e01c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b7bc5d18d1ae4ba5c3e0884947b954db.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5d8f85399550a4a3613735f43f733640.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Frog-Soup-Zero-Shot-In-Context-and-Sample-Efficient-Frogger-Agents"><a href="#Frog-Soup-Zero-Shot-In-Context-and-Sample-Efficient-Frogger-Agents" class="headerlink" title="Frog Soup: Zero-Shot, In-Context, and Sample-Efficient Frogger Agents"></a>Frog Soup: Zero-Shot, In-Context, and Sample-Efficient Frogger Agents</h2><p><strong>Authors:Xiang Li, Yiyang Hao, Doug Fulop</strong></p>
<p>One of the primary aspirations in reinforcement learning research is developing general-purpose agents capable of rapidly adapting to and mastering novel tasks. While RL gaming agents have mastered many Atari games, they remain slow and costly to train for each game. In this work, we demonstrate that latest reasoning LLMs with out-of-domain RL post-training can play a challenging Atari game called Frogger under a zero-shot setting. We then investigate the effect of in-context learning and the amount of reasoning effort on LLM performance. Lastly, we demonstrate a way to bootstrap traditional RL method with LLM demonstrations, which significantly improves their performance and sample efficiency. Our implementation is open sourced at <a target="_blank" rel="noopener" href="https://github.com/AlienKevin/frogger">https://github.com/AlienKevin/frogger</a>. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ç ”ç©¶çš„ä¸»è¦æ„¿æœ›ä¹‹ä¸€æ˜¯å¼€å‘èƒ½å¤Ÿè¿…é€Ÿé€‚åº”å¹¶æŒæ¡æ–°ä»»åŠ¡çš„é€šç”¨æ™ºèƒ½ä½“ã€‚è™½ç„¶å¼ºåŒ–å­¦ä¹ æ¸¸æˆæ™ºèƒ½ä½“å·²ç»æŒæ¡äº†è®¸å¤šé›…è¾¾åˆ©ï¼ˆAtariï¼‰æ¸¸æˆï¼Œä½†å®ƒä»¬ä»ç„¶å¯¹æ¯æ¬¾æ¸¸æˆçš„è®­ç»ƒç¼“æ…¢ä¸”æˆæœ¬é«˜æ˜‚ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†æœ€æ–°çš„æ¨ç†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åŸŸå¤–å¼ºåŒ–å­¦ä¹ åè®­ç»ƒå¯ä»¥åœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹ç©ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„é›…è¾¾åˆ©æ¸¸æˆã€Šé’è›™ç‹å­ã€‹(Frogger)ã€‚ç„¶åæˆ‘ä»¬ç ”ç©¶äº†ä¸Šä¸‹æ–‡å­¦ä¹ ä¸æ¨ç†åŠªåŠ›å¯¹å¤§å‹è¯­è¨€æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚æœ€åï¼Œæˆ‘ä»¬å±•ç¤ºäº†å¦‚ä½•å°†å¤§å‹è¯­è¨€æ¨¡å‹æ¼”ç¤ºä¸ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ æ–¹æ³•ç›¸ç»“åˆï¼Œè¿™æ˜¾è‘—æé«˜äº†å®ƒä»¬çš„æ€§èƒ½å’Œæ ·æœ¬æ•ˆç‡ã€‚æˆ‘ä»¬çš„å®ç°å·²å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/AlienKevin/frogger%E3%80%82">https://github.com/AlienKevin/froggerã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.03947v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨å¼ºåŒ–å­¦ä¹ é¢†åŸŸï¼Œå¼€å‘èƒ½å¤Ÿè¿…é€Ÿé€‚åº”å¹¶æŒæ¡æ–°ä»»åŠ¡çš„ä¸€èˆ¬æ€§æ™ºèƒ½ä½“æ˜¯ä¸»è¦ç›®æ ‡ä¹‹ä¸€ã€‚è™½ç„¶RLæ¸¸æˆæ™ºèƒ½ä½“å·²ç»æŒæ¡äº†ä¼—å¤šAtariæ¸¸æˆï¼Œä½†å®ƒä»¬å¯¹äºæ¯ä¸ªæ¸¸æˆçš„è®­ç»ƒé€Ÿåº¦è¾ƒæ…¢ä¸”æˆæœ¬è¾ƒé«˜ã€‚æœ¬ç ”ç©¶å±•ç¤ºäº†æœ€æ–°çš„æ¨ç†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è·¨é¢†åŸŸå¼ºåŒ–å­¦ä¹ åå¤„ç†è®­ç»ƒä¸‹ï¼Œèƒ½åœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹ç©æŒ‘æˆ˜æ€§Atariæ¸¸æˆã€Šè›™ç‹å­ã€‹ã€‚æ¥ç€ï¼Œæœ¬ç ”ç©¶æ¢è®¨äº†ä¸Šä¸‹æ–‡å­¦ä¹ çš„æ•ˆæœä»¥åŠæ¨ç†åŠªåŠ›å¯¹å¤§å‹è¯­è¨€æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚æœ€åï¼Œæœ¬ç ”ç©¶å±•ç¤ºäº†å¦‚ä½•å°†å¤§å‹è¯­è¨€æ¨¡å‹ç¤ºèŒƒä¸ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ ç›¸ç»“åˆï¼Œæ˜¾è‘—æé«˜æ€§èƒ½ä¸æ ·æœ¬æ•ˆç‡ã€‚ç›¸å…³å®ç°å·²å¼€æºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ è‡´åŠ›äºå¼€å‘èƒ½è¿…é€Ÿé€‚åº”å¹¶æŒæ¡æ–°ä»»åŠ¡çš„ä¸€èˆ¬æ€§æ™ºèƒ½ä½“ã€‚</li>
<li>RLæ¸¸æˆæ™ºèƒ½ä½“è™½ç„¶å·²ç»æŒæ¡äº†ä¼—å¤šAtariæ¸¸æˆï¼Œä½†è®­ç»ƒé€Ÿåº¦å’Œæˆæœ¬ä»éœ€æ”¹è¿›ã€‚</li>
<li>æ¨ç†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è·¨é¢†åŸŸå¼ºåŒ–å­¦ä¹ åå¤„ç†è®­ç»ƒä¸‹èƒ½åœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹ç©ã€Šè›™ç‹å­ã€‹ã€‚</li>
<li>ä¸Šä¸‹æ–‡å­¦ä¹ çš„æ•ˆæœä»¥åŠæ¨ç†åŠªåŠ›å¯¹å¤§å‹è¯­è¨€æ¨¡å‹æ€§èƒ½çš„å½±å“è¢«æ¢è®¨ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ç¤ºèŒƒä¸ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ ç›¸ç»“åˆå¯æ˜¾è‘—æé«˜æ€§èƒ½ä¸æ ·æœ¬æ•ˆç‡ã€‚</li>
<li>è¯¥ç ”ç©¶çš„å®ç°å·²ç»å¼€æºï¼Œä¾¿äºå…¶ä»–ç ”ç©¶è€…å‚è€ƒå’Œè¿›ä¸€æ­¥æ¢ç´¢ã€‚</li>
<li>ç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹ä¸ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ æ–¹æ³•æ˜¯æœªæ¥æ™ºèƒ½ä½“ç ”ç©¶çš„ä¸€ä¸ªé‡è¦æ–¹å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.03947">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a3f96ed255116d7dcbd3f66debf4701b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bfadafa064d82af923f72b896dac43f6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dfdf7599a1b0affa75136132126ae383.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-51a8acb9d9da2a710b592f8918d17bc3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c0c7b2eaa4805e66dbc264367e859021.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Absolute-Zero-Reinforced-Self-play-Reasoning-with-Zero-Data"><a href="#Absolute-Zero-Reinforced-Self-play-Reasoning-with-Zero-Data" class="headerlink" title="Absolute Zero: Reinforced Self-play Reasoning with Zero Data"></a>Absolute Zero: Reinforced Self-play Reasoning with Zero Data</h2><p><strong>Authors:Andrew Zhao, Yiran Wu, Yang Yue, Tong Wu, Quentin Xu, Yang Yue, Matthieu Lin, Shenzhi Wang, Qingyun Wu, Zilong Zheng, Gao Huang</strong></p>
<p>Reinforcement learning with verifiable rewards (RLVR) has shown promise in enhancing the reasoning capabilities of large language models by learning directly from outcome-based rewards. Recent RLVR works that operate under the zero setting avoid supervision in labeling the reasoning process, but still depend on manually curated collections of questions and answers for training. The scarcity of high-quality, human-produced examples raises concerns about the long-term scalability of relying on human supervision, a challenge already evident in the domain of language model pretraining. Furthermore, in a hypothetical future where AI surpasses human intelligence, tasks provided by humans may offer limited learning potential for a superintelligent system. To address these concerns, we propose a new RLVR paradigm called Absolute Zero, in which a single model learns to propose tasks that maximize its own learning progress and improves reasoning by solving them, without relying on any external data. Under this paradigm, we introduce the Absolute Zero Reasoner (AZR), a system that self-evolves its training curriculum and reasoning ability by using a code executor to both validate proposed code reasoning tasks and verify answers, serving as an unified source of verifiable reward to guide open-ended yet grounded learning. Despite being trained entirely without external data, AZR achieves overall SOTA performance on coding and mathematical reasoning tasks, outperforming existing zero-setting models that rely on tens of thousands of in-domain human-curated examples. Furthermore, we demonstrate that AZR can be effectively applied across different model scales and is compatible with various model classes. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰é€šè¿‡ç›´æ¥ä»ç»“æœå¯¼å‘çš„å¥–åŠ±ä¸­å­¦ä¹ ï¼Œåœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›æ–¹é¢å±•ç°å‡ºäº†å·¨å¤§çš„æ½œåŠ›ã€‚è¿‘æœŸåœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹è¿ä½œçš„RLVRä½œå“é¿å…äº†ç›‘ç£æ¨ç†è¿‡ç¨‹çš„æ ‡ç­¾ï¼Œä½†ä»ä¾èµ–äºæ‰‹åŠ¨æ•´ç†çš„é—®é¢˜å’Œç­”æ¡ˆé›†åˆè¿›è¡Œè®­ç»ƒã€‚é«˜è´¨é‡ã€äººå·¥ç”Ÿæˆæ¡ˆä¾‹çš„ç¨€ç¼ºæ€§å¼•å‘äº†å¯¹é•¿æœŸä¾èµ–äººå·¥ç›‘ç£çš„å¯æ‰©å±•æ€§çš„æ‹…å¿§ï¼Œè¿™ä¸€æŒ‘æˆ˜åœ¨è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒé¢†åŸŸå·²ç»æ˜¾è€Œæ˜“è§ã€‚æ­¤å¤–ï¼Œåœ¨äººå·¥æ™ºèƒ½è¶…è¶Šäººç±»æ™ºèƒ½çš„å‡è®¾æœªæ¥ä¸­ï¼Œäººç±»æä¾›çš„ä»»åŠ¡å¯èƒ½ä¸ºè¶…çº§æ™ºèƒ½ç³»ç»Ÿæä¾›æœ‰é™çš„å­¦ä¹ æ½œåŠ›ã€‚ä¸ºäº†è§£å†³è¿™äº›æ‹…å¿§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„RLVRèŒƒå¼ï¼Œç§°ä¸ºâ€œç»å¯¹é›¶â€ï¼Œå…¶ä¸­å•ä¸ªæ¨¡å‹å­¦ä¼šæå‡ºèƒ½æœ€å¤§åŒ–å…¶è‡ªèº«å­¦ä¹ è¿›åº¦çš„ä»»åŠ¡ï¼Œå¹¶é€šè¿‡è§£å†³è¿™äº›ä»»åŠ¡æ¥æé«˜æ¨ç†èƒ½åŠ›ï¼Œæ— éœ€ä¾èµ–ä»»ä½•å¤–éƒ¨æ•°æ®ã€‚åœ¨è¿™ç§èŒƒå¼ä¸‹ï¼Œæˆ‘ä»¬å¼•å…¥äº†â€œç»å¯¹é›¶æ¨ç†å™¨â€ï¼ˆAZRï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡ä»£ç æ‰§è¡Œå™¨éªŒè¯æ‰€æå‡ºçš„ä»£ç æ¨ç†ä»»åŠ¡å¹¶éªŒè¯ç­”æ¡ˆçš„ç³»ç»Ÿï¼Œå®ƒä½œä¸ºå¯éªŒè¯å¥–åŠ±çš„ç»Ÿä¸€æ¥æºï¼Œå¼•å¯¼å¼€æ”¾å¼ä½†åŸºäºç°å®çš„å­¦ä¹ ã€‚å°½ç®¡å®Œå…¨ä¸å—å¤–éƒ¨æ•°æ®è®­ç»ƒï¼ŒAZRåœ¨ç¼–ç å’Œæ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šå®ç°äº†æ•´ä½“æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¶…è¶Šäº†ä¾èµ–æ•°ä¸‡é¢†åŸŸå†…éƒ¨äººå·¥æ•´ç†æ ·æœ¬çš„é›¶è®¾ç½®æ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¯æ˜äº†AZRå¯ä»¥æœ‰æ•ˆåœ°åº”ç”¨äºä¸åŒçš„æ¨¡å‹è§„æ¨¡ï¼Œå¹¶ä¸”ä¸å„ç§æ¨¡å‹ç±»åˆ«å…¼å®¹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.03335v2">PDF</a> </p>
<p><strong>Summary</strong>ï¼šå¼ºåŒ–å­¦ä¹ é€šè¿‡å¯éªŒè¯çš„å¥–åŠ±ï¼ˆRLVRï¼‰æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œé€šè¿‡ç›´æ¥å­¦ä¹ ç»“æœå¯¼å‘çš„å¥–åŠ±æ¥å¢å¼ºæ¨ç†èƒ½åŠ›ã€‚æ–°çš„RLVRå·¥ä½œåœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹é¿å…äº†ç›‘ç£æ ‡æ³¨æ¨ç†è¿‡ç¨‹ï¼Œä½†ä»ä¾èµ–äºæ‰‹åŠ¨æ•´ç†çš„é—®é¢˜å’Œç­”æ¡ˆé›†åˆè¿›è¡Œè®­ç»ƒã€‚ä¸ºäº†å…‹æœå¯¹é«˜è´¨é‡äººç±»ç”Ÿäº§æ ·æœ¬çš„ä¾èµ–æ€§å’Œé•¿æœŸå¯æ‰©å±•æ€§çš„æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„RLVRèŒƒå¼ï¼Œç§°ä¸ºç»å¯¹é›¶å€¼ï¼Œä¸€ä¸ªæ¨¡å‹èƒ½å¤Ÿè‡ªè¡Œæå‡ºä»»åŠ¡å¹¶æœ€å¤§åŒ–è‡ªå·±çš„å­¦ä¹ è¿›æ­¥å’Œæ”¹å–„æ¨ç†èƒ½åŠ›ã€‚åœ¨æ­¤èŒƒå¼ä¸‹ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç»å¯¹é›¶æ¨ç†å™¨ï¼ˆAZRï¼‰ï¼Œè¯¥ç³»ç»Ÿä½¿ç”¨ä»£ç æ‰§è¡Œå™¨æ¥éªŒè¯æå‡ºçš„ä»£ç æ¨ç†ä»»åŠ¡å’Œç­”æ¡ˆï¼Œä½œä¸ºéªŒè¯å¥–åŠ±çš„ç»Ÿä¸€æ¥æºï¼ŒæŒ‡å¯¼å¼€æ”¾å¼ä½†åŸºäºå®é™…çš„å­¦ä¹ ã€‚å°½ç®¡å®Œå…¨åœ¨å¤–éƒ¨æ•°æ®è®­ç»ƒä¸‹ï¼ŒAZRåœ¨ç¼–ç å’Œæ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šå®ç°äº†æœ€ä½³æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¯æ˜äº†AZRåœ¨ä¸åŒæ¨¡å‹è§„æ¨¡ä¸‹å‡æœ‰æ•ˆï¼Œä¸”å…¼å®¹å„ç±»æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ é€šè¿‡å¯éªŒè¯çš„å¥–åŠ±ï¼ˆRLVRï¼‰èƒ½æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>æ–°çš„RLVRå·¥ä½œåœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹é¿å…äº†å¯¹ç›‘ç£æ ‡æ³¨æ¨ç†è¿‡ç¨‹çš„ä¾èµ–ï¼Œä½†ä»éœ€æ‰‹åŠ¨æ•´ç†çš„é—®é¢˜å’Œç­”æ¡ˆé›†åˆè¿›è¡Œè®­ç»ƒã€‚</li>
<li>å¯¹é«˜è´¨é‡äººç±»ç”Ÿäº§æ ·æœ¬çš„ä¾èµ–æ€§å’Œé•¿æœŸå¯æ‰©å±•æ€§å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºæ–°çš„RLVRèŒƒå¼â€œç»å¯¹é›¶â€ï¼Œæ¨¡å‹èƒ½è‡ªè¡Œæå‡ºä»»åŠ¡å¹¶æœ€å¤§åŒ–è‡ªå·±çš„å­¦ä¹ è¿›æ­¥ã€‚</li>
<li>å¼•å…¥ç»å¯¹é›¶æ¨ç†å™¨ï¼ˆAZRï¼‰ï¼Œç»“åˆä»£ç æ‰§è¡Œå™¨éªŒè¯ä»»åŠ¡å’Œç­”æ¡ˆï¼Œä¿ƒè¿›å¼€æ”¾å¼ä½†åŸºäºå®é™…çš„å­¦ä¹ ã€‚</li>
<li>AZRåœ¨ç¼–ç å’Œæ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šå®ç°æœ€ä½³æ€§èƒ½ï¼Œä¸”é€‚ç”¨äºä¸åŒæ¨¡å‹è§„æ¨¡å’Œå„ç§æ¨¡å‹ç±»å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.03335">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-36fec0814348995c0d811253c19339a9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-100471ed028d8370f5d09c8ba954c734.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-53990f3c8272ae038fb94dbd3dafdfd8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ea3ba310819fd4aff8e3d53a3e56fb34.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-746ff970b2e408d3c9774f657c02f292.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="RM-R1-Reward-Modeling-as-Reasoning"><a href="#RM-R1-Reward-Modeling-as-Reasoning" class="headerlink" title="RM-R1: Reward Modeling as Reasoning"></a>RM-R1: Reward Modeling as Reasoning</h2><p><strong>Authors:Xiusi Chen, Gaotang Li, Ziqi Wang, Bowen Jin, Cheng Qian, Yu Wang, Hongru Wang, Yu Zhang, Denghui Zhang, Tong Zhang, Hanghang Tong, Heng Ji</strong></p>
<p>Reward modeling is essential for aligning large language models (LLMs) with human preferences, especially through reinforcement learning from human feedback (RLHF). To provide accurate reward signals, a reward model (RM) should stimulate deep thinking and conduct interpretable reasoning before assigning a score or a judgment. However, existing RMs either produce opaque scalar scores or directly generate the prediction of a preferred answer, making them struggle to integrate natural language critiques, thus lacking interpretability. Inspired by recent advances of long chain-of-thought (CoT) on reasoning-intensive tasks, we hypothesize and validate that integrating reasoning capabilities into reward modeling significantly enhances RMâ€™s interpretability and performance. In this work, we introduce a new class of generative reward models â€“ Reasoning Reward Models (ReasRMs) â€“ which formulate reward modeling as a reasoning task. We propose a reasoning-oriented training pipeline and train a family of ReasRMs, RM-R1. The training consists of two key stages: (1) distillation of high-quality reasoning chains and (2) reinforcement learning with verifiable rewards. RM-R1 improves LLM rollouts by self-generating reasoning traces or chat-specific rubrics and evaluating candidate responses against them. Empirically, our models achieve state-of-the-art or near state-of-the-art performance of generative RMs across multiple comprehensive reward model benchmarks, outperforming much larger open-weight models (e.g., Llama3.1-405B) and proprietary ones (e.g., GPT-4o) by up to 13.8%. Beyond final performance, we perform thorough empirical analysis to understand the key ingredients of successful ReasRM training. To facilitate future research, we release six ReasRM models along with code and data at <a target="_blank" rel="noopener" href="https://github.com/RM-R1-UIUC/RM-R1">https://github.com/RM-R1-UIUC/RM-R1</a>. </p>
<blockquote>
<p>å¥–åŠ±å»ºæ¨¡å¯¹äºå°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸äººç±»åå¥½è¿›è¡Œå¯¹é½è‡³å…³é‡è¦ï¼Œå°¤å…¶æ˜¯é€šè¿‡ä»äººç±»åé¦ˆä¸­è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰ã€‚ä¸ºäº†æä¾›å‡†ç¡®çš„å¥–åŠ±ä¿¡å·ï¼Œå¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰åº”è¯¥åœ¨åˆ†é…åˆ†æ•°æˆ–åˆ¤æ–­ä¹‹å‰æ¿€å‘æ·±åº¦æ€è€ƒå¹¶è¿›è¡Œå¯è§£é‡Šçš„æ¨ç†ã€‚ç„¶è€Œï¼Œç°æœ‰çš„RMsè¦ä¹ˆäº§ç”Ÿä¸é€æ˜çš„æ ‡é‡åˆ†æ•°ï¼Œè¦ä¹ˆç›´æ¥ç”Ÿæˆé¦–é€‰ç­”æ¡ˆçš„é¢„æµ‹ï¼Œè¿™ä½¿å¾—å®ƒä»¬éš¾ä»¥æ•´åˆè‡ªç„¶è¯­è¨€æ‰¹è¯„ï¼Œå› æ­¤ç¼ºä¹å¯è§£é‡Šæ€§ã€‚å—è¿‘æœŸé•¿é“¾æ€ç»´ï¼ˆCoTï¼‰åœ¨æ¨ç†å¯†é›†å‹ä»»åŠ¡ä¸Šè¿›æ­¥çš„å¯å‘ï¼Œæˆ‘ä»¬å‡è®¾å¹¶å°†éªŒè¯å°†æ¨ç†èƒ½åŠ›èå…¥å¥–åŠ±å»ºæ¨¡ä¼šæ˜¾è‘—å¢å¼ºRMçš„å¯è§£é‡Šæ€§å’Œæ€§èƒ½ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹ç”Ÿæˆå¥–åŠ±æ¨¡å‹â€”â€”æ¨ç†å¥–åŠ±æ¨¡å‹ï¼ˆReasRMsï¼‰ï¼Œå®ƒå°†å¥–åŠ±å»ºæ¨¡åˆ¶å®šä¸ºæ¨ç†ä»»åŠ¡ã€‚æˆ‘ä»¬æå‡ºäº†é¢å‘æ¨ç†çš„è®­ç»ƒæµç¨‹ï¼Œå¹¶è®­ç»ƒäº†ä¸€ç³»åˆ—ReasRMsï¼Œå³RM-R1ã€‚è®­ç»ƒåŒ…æ‹¬ä¸¤ä¸ªå…³é”®é˜¶æ®µï¼šï¼ˆ1ï¼‰é«˜è´¨é‡æ¨ç†é“¾çš„æç‚¼ï¼›ï¼ˆ2ï¼‰å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ã€‚RM-R1é€šè¿‡è‡ªæˆ‘ç”Ÿæˆæ¨ç†è½¨è¿¹æˆ–é’ˆå¯¹èŠå¤©çš„ç‰¹å®šå‡†åˆ™ï¼Œå¹¶å¯¹å€™é€‰å“åº”è¿›è¡Œè¯„ä¼°ï¼Œä»è€Œæ”¹è¿›LLMçš„æ»šåŠ¨æ›´æ–°ã€‚ä»å®è¯è§’åº¦çœ‹ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨å¤šä¸ªç»¼åˆå¥–åŠ±æ¨¡å‹åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æˆ–æ¥è¿‘æœ€å…ˆè¿›çš„ç”ŸæˆRMæ€§èƒ½ï¼Œè¶…è¿‡äº†è®¸å¤šæ›´å¤§çš„å¼€æ”¾æƒé‡æ¨¡å‹ï¼ˆä¾‹å¦‚Llama3.1-405Bï¼‰å’Œä¸“æœ‰æ¨¡å‹ï¼ˆä¾‹å¦‚GPT-4oï¼‰ï¼Œæœ€å¤šé«˜å‡º13.8%ã€‚é™¤äº†æœ€ç»ˆæ€§èƒ½ä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜è¿›è¡Œäº†å½»åº•çš„ç»éªŒåˆ†æï¼Œä»¥äº†è§£æˆåŠŸçš„ReasRMè®­ç»ƒçš„å…³é”®è¦ç´ ã€‚ä¸ºäº†ä¾¿äºæœªæ¥çš„ç ”ç©¶ï¼Œæˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/RM-R1-UIUC/RM-R1%E4%B8%8A%E5%8F%91%E5%B8%83%E4%BA%86%E5%85%AD%E4%B8%AAReasRM%E6%A8%A1%E5%9E%8B%E4%BB%A5%E5%8F%8A%E7%9B%B8%E5%85%B3%E7%9A%84%E4%BB%A3%E7%A0%81%E5%92%8C%E6%95%B0%E6%8D%AE%E3%80%82">https://github.com/RM-R1-UIUC/RM-R1ä¸Šå‘å¸ƒäº†å…­ä¸ªReasRMæ¨¡å‹ä»¥åŠç›¸å…³çš„ä»£ç å’Œæ•°æ®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.02387v1">PDF</a> 23 pages, 7 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å¼ºè°ƒå¥–åŠ±æ¨¡å‹åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸äººç±»åå¥½å¯¹é½ä¸­çš„é‡è¦æ€§ï¼Œç‰¹åˆ«æ˜¯é€šè¿‡äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰ã€‚ä¸ºäº†æä¾›å‡†ç¡®çš„å¥–åŠ±ä¿¡å·ï¼Œå¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰åº”åœ¨åˆ†é…åˆ†æ•°æˆ–åˆ¤æ–­ä¹‹å‰è¿›è¡Œæ·±åº¦æ€è€ƒå’Œå¯è§£é‡Šæ¨ç†ã€‚ç„¶è€Œï¼Œç°æœ‰RMsäº§ç”Ÿçš„æ˜¯æ¨¡ç³Šçš„æ ‡é‡åˆ†æ•°æˆ–ç›´æ¥ç”Ÿæˆé¦–é€‰ç­”æ¡ˆçš„é¢„æµ‹ï¼Œéš¾ä»¥æ•´åˆè‡ªç„¶è¯­è¨€æ‰¹è¯„ï¼Œå› æ­¤ç¼ºä¹å¯è§£é‡Šæ€§ã€‚æœ¬ç ”ç©¶ç»“åˆé•¿é“¾æ€ç»´ï¼ˆCoTï¼‰åœ¨æ¨ç†å¯†é›†å‹ä»»åŠ¡ä¸Šçš„æœ€æ–°è¿›å±•ï¼Œå‡è®¾å¹¶éªŒè¯äº†å°†æ¨ç†èƒ½åŠ›èå…¥å¥–åŠ±æ¨¡å‹å¯ä»¥æ˜¾è‘—å¢å¼ºRMçš„å¯è§£é‡Šæ€§å’Œæ€§èƒ½ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç±»æ–°çš„ç”Ÿæˆå¥–åŠ±æ¨¡å‹â€”â€”æ¨ç†å¥–åŠ±æ¨¡å‹ï¼ˆReasRMsï¼‰ï¼Œå°†å¥–åŠ±å»ºæ¨¡è¡¨è¿°ä¸ºæ¨ç†ä»»åŠ¡ã€‚æœ¬ç ”ç©¶æå‡ºäº†é¢å‘æ¨ç†çš„è®­ç»ƒç®¡é“ï¼Œå¹¶è®­ç»ƒäº†ä¸€ç³»åˆ—ReasRMsï¼Œå³RM-R1ã€‚è®­ç»ƒåŒ…æ‹¬ä¸¤ä¸ªå…³é”®é˜¶æ®µï¼š1ï¼‰é«˜è´¨é‡æ¨ç†é“¾çš„æç‚¼å’Œ2ï¼‰å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ã€‚RM-R1é€šè¿‡è‡ªæˆ‘ç”Ÿæˆæ¨ç†è½¨è¿¹æˆ–é’ˆå¯¹èŠå¤©çš„ç‰¹å®šå‡†åˆ™ï¼Œè¯„ä¼°å€™é€‰å“åº”ï¼Œä»è€Œæ”¹è¿›å¤§å‹è¯­è¨€æ¨¡å‹çš„æ»šåŠ¨è¾“å‡ºã€‚ç»éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨å¤šä¸ªç»¼åˆå¥–åŠ±æ¨¡å‹åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æœ€æ–°æˆ–æ¥è¿‘æœ€æ–°çš„ç”ŸæˆRMæ€§èƒ½ï¼Œä¼˜äºæ›´å¤§çš„å¼€æ”¾æƒé‡æ¨¡å‹ï¼ˆå¦‚Llama3.1-405Bï¼‰å’Œä¸“æœ‰æ¨¡å‹ï¼ˆå¦‚GPT-4oï¼‰ï¼Œæœ€é«˜æå‡äº†13.8%ã€‚é™¤äº†æœ€ç»ˆæ€§èƒ½å¤–ï¼Œæˆ‘ä»¬è¿˜è¿›è¡Œäº†å½»åº•çš„ç»éªŒåˆ†æï¼Œä»¥äº†è§£æˆåŠŸçš„ReasRMè®­ç»ƒçš„å…³é”®æˆåˆ†ã€‚ä¸ºäº†ä¿ƒè¿›æœªæ¥ç ”ç©¶ï¼Œæˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/RM-R1-UIUC/RM-R1%E4%B8%8A%E5%8F%91%E5%B8%83%E4%BA%86%E5%85%AD%E4%B8%AAReasRM%E6%A8%A1%E5%9E%8B%E5%8F%8A%E4%BB%A3%E7%A0%81%E5%92%8C%E6%95%B0%E6%8D%AE%E3%80%82">https://github.com/RM-R1-UIUC/RM-R1ä¸Šå‘å¸ƒäº†å…­ä¸ªReasRMæ¨¡å‹åŠä»£ç å’Œæ•°æ®ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¥–åŠ±æ¨¡å‹åœ¨å¤§è¯­è¨€æ¨¡å‹ä¸äººç±»åå¥½å¯¹é½ä¸­èµ·å…³é”®ä½œç”¨ã€‚</li>
<li>ç°æœ‰å¥–åŠ±æ¨¡å‹ç¼ºä¹æ·±åº¦æ€è€ƒå’Œå¯è§£é‡Šæ¨ç†èƒ½åŠ›ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°å‹ç”Ÿæˆå¥–åŠ±æ¨¡å‹â€”â€”æ¨ç†å¥–åŠ±æ¨¡å‹ï¼ˆReasRMsï¼‰ï¼Œå°†å¥–åŠ±å»ºæ¨¡è§†ä¸ºæ¨ç†ä»»åŠ¡ã€‚</li>
<li>ReasRMsé€šè¿‡è‡ªæˆ‘ç”Ÿæˆæ¨ç†è½¨è¿¹æˆ–é’ˆå¯¹èŠå¤©çš„ç‰¹å®šå‡†åˆ™æ¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>RM-R1æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œç›¸æ¯”å…¶ä»–å¤§å‹æ¨¡å‹æœ‰æ‰€æå‡ã€‚</li>
<li>æˆåŠŸè®­ç»ƒReasRMçš„å…³é”®åœ¨äºé«˜è´¨é‡æ¨ç†é“¾çš„æç‚¼å’Œå¼ºåŒ–å­¦ä¹ ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.02387">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5a098b35312f00a46efca0a171a63eb9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9f78764be564d5b669c81a657c42cb07.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-75dc9f665d121652cf1860cc0697cba8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a4c6e91a538ff0fe9628c83c2172a2f2.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="HyperTree-Planning-Enhancing-LLM-Reasoning-via-Hierarchical-Thinking"><a href="#HyperTree-Planning-Enhancing-LLM-Reasoning-via-Hierarchical-Thinking" class="headerlink" title="HyperTree Planning: Enhancing LLM Reasoning via Hierarchical Thinking"></a>HyperTree Planning: Enhancing LLM Reasoning via Hierarchical Thinking</h2><p><strong>Authors:Runquan Gui, Zhihai Wang, Jie Wang, Chi Ma, Huiling Zhen, Mingxuan Yuan, Jianye Hao, Defu Lian, Enhong Chen, Feng Wu</strong></p>
<p>Recent advancements have significantly enhanced the performance of large language models (LLMs) in tackling complex reasoning tasks, achieving notable success in domains like mathematical and logical reasoning. However, these methods encounter challenges with complex planning tasks, primarily due to extended reasoning steps, diverse constraints, and the challenge of handling multiple distinct sub-tasks. To address these challenges, we propose HyperTree Planning (HTP), a novel reasoning paradigm that constructs hypertree-structured planning outlines for effective planning. The hypertree structure enables LLMs to engage in hierarchical thinking by flexibly employing the divide-and-conquer strategy, effectively breaking down intricate reasoning steps, accommodating diverse constraints, and managing multiple distinct sub-tasks in a well-organized manner. We further introduce an autonomous planning framework that completes the planning process by iteratively refining and expanding the hypertree-structured planning outlines. Experiments demonstrate the effectiveness of HTP, achieving state-of-the-art accuracy on the TravelPlanner benchmark with Gemini-1.5-Pro, resulting in a 3.6 times performance improvement over o1-preview. </p>
<blockquote>
<p>æœ€è¿‘çš„æŠ€æœ¯è¿›æ­¥æ˜¾è‘—æé«˜äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è§£å†³å¤æ‚æ¨ç†ä»»åŠ¡æ–¹é¢çš„æ€§èƒ½ï¼Œåœ¨æ•°å­¦å’Œé€»è¾‘æ¨ç†ç­‰é¢†åŸŸå–å¾—äº†æ˜¾è‘—çš„æˆåŠŸã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åœ¨å¤„ç†å¤æ‚çš„è§„åˆ’ä»»åŠ¡æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œä¸»è¦æ˜¯ç”±äºæ¨ç†æ­¥éª¤ç¹çã€çº¦æŸå¤šæ ·ä»¥åŠå¤„ç†å¤šä¸ªä¸åŒå­ä»»åŠ¡çš„æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†HyperTree Planningï¼ˆHTPï¼‰è¿™ä¸€æ–°çš„æ¨ç†èŒƒå¼ï¼Œä¸ºæœ‰æ•ˆè§„åˆ’æ„å»ºè¶…æ ‘ç»“æ„è§„åˆ’å¤§çº²ã€‚è¶…æ ‘ç»“æ„ä½¿LLMèƒ½å¤Ÿé€šè¿‡çµæ´»åœ°é‡‡ç”¨åˆ†è€Œæ²»ä¹‹çš„ç­–ç•¥æ¥è¿›è¡Œåˆ†å±‚æ€è€ƒï¼Œæœ‰æ•ˆåœ°åˆ†è§£å¤æ‚çš„æ¨ç†æ­¥éª¤ï¼Œé€‚åº”å„ç§çº¦æŸï¼Œå¹¶ä»¥æœ‰åºçš„æ–¹å¼ç®¡ç†å¤šä¸ªä¸åŒçš„å­ä»»åŠ¡ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªè‡ªä¸»è§„åˆ’æ¡†æ¶ï¼Œé€šè¿‡è¿­ä»£åœ°ç»†åŒ–å’Œæ‰©å±•è¶…æ ‘ç»“æ„è§„åˆ’å¤§çº²æ¥å®Œæˆè§„åˆ’è¿‡ç¨‹ã€‚å®éªŒè¡¨æ˜HTPçš„æœ‰æ•ˆæ€§ï¼Œåœ¨TravelPlanneråŸºå‡†æµ‹è¯•ä¸Šä½¿ç”¨Gemini-1.5-Proå®ç°äº†æœ€å…ˆè¿›çš„å‡†ç¡®æ€§ï¼Œç›¸è¾ƒäºo1-previewæ€§èƒ½æå‡äº†3.6å€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.02322v1">PDF</a> arXiv admin note: text overlap with arXiv:2406.14228 by other authors</p>
<p><strong>Summary</strong>ï¼šæœ€è¿‘å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å¤æ‚æ¨ç†ä»»åŠ¡æ–¹é¢çš„æ€§èƒ½å¾—åˆ°æ˜¾è‘—æé«˜ï¼Œä½†åœ¨å¤æ‚è§„åˆ’ä»»åŠ¡ä¸Šä»é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºHyperTree Planningï¼ˆHTPï¼‰è¿™ä¸€æ–°æ¨ç†èŒƒå¼ï¼Œé€šè¿‡æ„å»ºè¶…æ ‘ç»“æ„è§„åˆ’çº²è¦ï¼Œå®ç°æœ‰æ•ˆçš„è§„åˆ’ã€‚Hypertreeç»“æ„ä½¿LLMsèƒ½å¤Ÿé€šè¿‡çµæ´»åœ°é‡‡ç”¨åˆ†è€Œæ²»ä¹‹çš„ç­–ç•¥ï¼Œæœ‰æ•ˆåœ°æ‰“ç ´å¤æ‚çš„æ¨ç†æ­¥éª¤ï¼Œé€‚åº”å¤šç§çº¦æŸæ¡ä»¶ï¼Œä»¥æœ‰æ¡ä¸ç´Šçš„æ–¹å¼å¤„ç†å¤šä¸ªä¸åŒçš„å­ä»»åŠ¡ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥å¼•å…¥ä¸€ä¸ªè‡ªä¸»è§„åˆ’æ¡†æ¶ï¼Œé€šè¿‡è¿­ä»£åœ°æ”¹è¿›å’Œæ‰©å±•è¶…æ ‘ç»“æ„çš„è§„åˆ’æ¦‚è¦æ¥å®Œæˆè§„åˆ’è¿‡ç¨‹ã€‚å®éªŒè¡¨æ˜HTPçš„æœ‰æ•ˆæ€§ï¼Œåœ¨TravelPlanneråŸºå‡†æµ‹è¯•ä¸Šä½¿ç”¨Gemini-1.5-Proå–å¾—äº†æœ€å…ˆè¿›çš„å‡†ç¡®æ€§ï¼Œç›¸è¾ƒäºo1-previewå®ç°äº†3.6å€çš„æ€§èƒ½æå‡ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šçš„æ€§èƒ½æœ‰æ‰€æå‡ï¼Œä½†ä»é¢ä¸´å¤„ç†å¤æ‚è§„åˆ’ä»»åŠ¡çš„æŒ‘æˆ˜ã€‚</li>
<li>HyperTree Planningï¼ˆHTPï¼‰æ˜¯ä¸€ç§æ–°çš„æ¨ç†èŒƒå¼ï¼Œé€šè¿‡æ„å»ºè¶…æ ‘ç»“æ„è§„åˆ’çº²è¦å®ç°æœ‰æ•ˆè§„åˆ’ã€‚</li>
<li>Hypertreeç»“æ„æ”¯æŒLLMsé‡‡ç”¨åˆ†è€Œæ²»ä¹‹çš„ç­–ç•¥ï¼Œæœ‰æ•ˆåˆ†è§£å¤æ‚æ¨ç†æ­¥éª¤ï¼Œé€‚åº”å¤šç§çº¦æŸæ¡ä»¶ï¼Œå¹¶å¤„ç†å¤šä¸ªä¸åŒçš„å­ä»»åŠ¡ã€‚</li>
<li>å¼•å…¥è‡ªä¸»è§„åˆ’æ¡†æ¶ï¼Œé€šè¿‡è¿­ä»£æ”¹è¿›å’Œæ‰©å±•è¶…æ ‘ç»“æ„è§„åˆ’æ¦‚è¦å®Œæˆè§„åˆ’è¿‡ç¨‹ã€‚</li>
<li>HTPåœ¨TravelPlanneråŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œä½¿ç”¨Gemini-1.5-Proå–å¾—äº†æœ€å…ˆè¿›çš„å‡†ç¡®æ€§ã€‚</li>
<li>HTPç›¸è¾ƒäºç°æœ‰æ–¹æ³•å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.02322">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-537a148212a480294882cb1d27086b88.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c03920e4cc3cd29a180232ceffb0ab45.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-44f9d5cceba3ec751953ea8ff3b73c0e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2fe6b08e9b26f06938e3a39c138437f4.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Think-on-your-Feet-Adaptive-Thinking-via-Reinforcement-Learning-for-Social-Agents"><a href="#Think-on-your-Feet-Adaptive-Thinking-via-Reinforcement-Learning-for-Social-Agents" class="headerlink" title="Think on your Feet: Adaptive Thinking via Reinforcement Learning for   Social Agents"></a>Think on your Feet: Adaptive Thinking via Reinforcement Learning for   Social Agents</h2><p><strong>Authors:Minzheng Wang, Yongbin Li, Haobo Wang, Xinghua Zhang, Nan Xu, Bingli Wu, Fei Huang, Haiyang Yu, Wenji Mao</strong></p>
<p>Effective social intelligence simulation requires language agents to dynamically adjust reasoning depth, a capability notably absent in current approaches. While existing methods either lack this kind of reasoning capability or enforce uniform long chain-of-thought reasoning across all scenarios, resulting in excessive token usage and inappropriate social simulation. In this paper, we propose $\textbf{A}$daptive $\textbf{M}$ode $\textbf{L}$earning ($\textbf{AML}$) that strategically selects from four thinking modes (intuitive reaction $\rightarrow$ deep contemplation) based on real-time context. Our frameworkâ€™s core innovation, the $\textbf{A}$daptive $\textbf{M}$ode $\textbf{P}$olicy $\textbf{O}$ptimization ($\textbf{AMPO}$) algorithm, introduces three key advancements over existing methods: (1) Multi-granular thinking mode design, (2) Context-aware mode switching across social interaction, and (3) Token-efficient reasoning via depth-adaptive processing. Extensive experiments on social intelligence tasks confirm that AML achieves 15.6% higher task performance than state-of-the-art methods. Notably, our method outperforms GRPO by 7.0% with 32.8% shorter reasoning chains. These results demonstrate that context-sensitive thinking mode selection, as implemented in AMPO, enables more human-like adaptive reasoning than GRPOâ€™s fixed-depth approach. </p>
<blockquote>
<p>å®ç°æœ‰æ•ˆçš„ç¤¾ä¼šæ™ºèƒ½æ¨¡æ‹Ÿéœ€è¦è¯­è¨€ä»£ç†èƒ½å¤ŸåŠ¨æ€è°ƒæ•´æ¨ç†æ·±åº¦ï¼Œè€Œå½“å‰çš„æ–¹æ³•æ˜æ˜¾ç¼ºä¹è¿™ç§èƒ½åŠ›ã€‚ç°æœ‰çš„æ–¹æ³•è¦ä¹ˆç¼ºä¹è¿™ç§æ¨ç†èƒ½åŠ›ï¼Œè¦ä¹ˆåœ¨æ‰€æœ‰åœºæ™¯ä¸­å¼ºåˆ¶å®æ–½ç»Ÿä¸€çš„é•¿é“¾æ€ç»´æ¨ç†ï¼Œå¯¼è‡´è¿‡åº¦ä½¿ç”¨ä»¤ç‰Œå¹¶äº§ç”Ÿä¸æ°å½“çš„ç¤¾ä¼šæ¨¡æ‹Ÿã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºå®æ—¶ä¸Šä¸‹æ–‡ä»å››ç§æ€ç»´æ¨¡å¼ï¼ˆç›´è§‰ååº”åˆ°æ·±åº¦æ€è€ƒï¼‰è¿›è¡Œæˆ˜ç•¥é€‰æ‹©çš„å¯é€‚åº”æ¨¡å¼å­¦ä¹ ï¼ˆAMLï¼‰ã€‚æˆ‘ä»¬æ¡†æ¶çš„æ ¸å¿ƒåˆ›æ–°ä¹‹å¤„åœ¨äºè‡ªé€‚åº”æ¨¡å¼ä¼˜åŒ–ç­–ç•¥ï¼ˆAMPOç®—æ³•ï¼‰ï¼Œå®ƒåœ¨ç°æœ‰æ–¹æ³•çš„åŸºç¡€ä¸Šå¼•å…¥äº†ä¸‰ä¸ªå…³é”®è¿›å±•ï¼šï¼ˆ1ï¼‰å¤šç²’åº¦æ€ç»´æ¨¡å¼è®¾è®¡ï¼Œï¼ˆ2ï¼‰ç¤¾ä¼šäº’åŠ¨ä¸­çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥æ¨¡å¼åˆ‡æ¢ï¼Œä»¥åŠï¼ˆ3ï¼‰é€šè¿‡æ·±åº¦è‡ªé€‚åº”å¤„ç†çš„ä»¤ç‰Œé«˜æ•ˆæ¨ç†ã€‚åœ¨ç¤¾ä¼šæ™ºèƒ½ä»»åŠ¡ä¸Šçš„å¤§é‡å®éªŒè¯å®ï¼ŒAMLæ¯”æœ€æ–°æŠ€æœ¯çš„æ–¹æ³•å®ç°äº†é«˜å‡º15.6%çš„ä»»åŠ¡æ€§èƒ½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä»¥æ›´çŸ­çš„æ¨ç†é“¾ï¼ˆé«˜å‡ºGRPO 32.8%ï¼‰ä¼˜äºGRPO 7.0%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œå¦‚AMPOä¸­æ‰€å®ç°çš„ä¸Šä¸‹æ–‡æ•æ„Ÿçš„æ€ç»´æ¨¡å¼é€‰æ‹©ï¼Œèƒ½å¤Ÿå®ç°æ¯”GRPOçš„å›ºå®šæ·±åº¦æ–¹æ³•æ›´ç±»ä¼¼äºäººç±»çš„è‡ªé€‚åº”æ¨ç†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.02156v2">PDF</a> Work in Progress. The code and data are available, see   <a target="_blank" rel="noopener" href="https://github.com/MozerWang/AMPO">https://github.com/MozerWang/AMPO</a></p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æå‡ºäº†ä¸€ç§è‡ªé€‚åº”æ¨¡å¼å­¦ä¹ ï¼ˆAMLï¼‰æ–¹æ³•ï¼Œä½¿è¯­è¨€ä»£ç†èƒ½å¤Ÿæ ¹æ®ä¸åŒçš„å®æ—¶è¯­å¢ƒåŠ¨æ€è°ƒæ•´æ¨ç†æ·±åº¦ï¼Œè§£å†³äº†ç°æœ‰ç¤¾ä¼šæ™ºèƒ½æ¨¡æ‹Ÿæ–¹æ³•åœ¨æ¨ç†èƒ½åŠ›ä¸Šçš„ä¸è¶³ã€‚AMLçš„æ ¸å¿ƒåˆ›æ–°ç‚¹åœ¨äºè‡ªé€‚åº”æ¨¡å¼ä¼˜åŒ–ç®—æ³•ï¼ˆAMPOï¼‰ï¼Œè¯¥ç®—æ³•å…·æœ‰å¤šç²’åº¦æ€è€ƒæ¨¡å¼è®¾è®¡ã€è¯­å¢ƒæ„ŸçŸ¥æ¨¡å¼åœ¨ç¤¾ä¼šäº’åŠ¨ä¸­çš„åˆ‡æ¢ä»¥åŠé€šè¿‡æ·±åº¦è‡ªé€‚åº”å¤„ç†å®ç°çš„é«˜æ•ˆæ¨ç†ç­‰ç‰¹ç‚¹ã€‚å®éªŒè¡¨æ˜ï¼ŒAMLåœ¨ç¤¾äº¤æ™ºèƒ½ä»»åŠ¡ä¸Šçš„è¡¨ç°æ¯”ç°æœ‰æ–¹æ³•é«˜å‡º15.6%ï¼Œå¹¶ä¸”ç›¸æ¯”GRPOæ–¹æ³•ï¼ŒAMLçš„æ¨ç†é“¾æ›´çŸ­ï¼ˆçŸ­32.8%ï¼‰ï¼Œå±•ç°å‡ºæ›´å¼ºçš„é€‚åº”æ€§å’Œäººç±»åŒ–æ¨ç†èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AMLè§£å†³äº†ç°æœ‰ç¤¾ä¼šæ™ºèƒ½æ¨¡æ‹Ÿæ–¹æ³•åœ¨åŠ¨æ€è°ƒæ•´æ¨ç†æ·±åº¦ä¸Šçš„ä¸è¶³ã€‚</li>
<li>AMPOç®—æ³•æ˜¯AMLçš„æ ¸å¿ƒï¼Œå…·æœ‰å¤šç²’åº¦æ€è€ƒæ¨¡å¼è®¾è®¡ã€‚</li>
<li>AMPOèƒ½åŸºäºå®æ—¶è¯­å¢ƒè¿›è¡Œæ¨¡å¼åˆ‡æ¢ï¼Œå¢å¼ºç¤¾ä¼šäº¤äº’çš„é€‚åº”æ€§ã€‚</li>
<li>AMPOå®ç°äº†é«˜æ•ˆçš„token-efficientæ¨ç†ï¼Œé€šè¿‡æ·±åº¦è‡ªé€‚åº”å¤„ç†ä¼˜åŒ–æ¨ç†æ•ˆç‡ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒAMLåœ¨ç¤¾äº¤æ™ºèƒ½ä»»åŠ¡ä¸Šçš„æ€§èƒ½ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œä»»åŠ¡æ€§èƒ½æå‡15.6%ã€‚</li>
<li>ä¸GRPOç›¸æ¯”ï¼ŒAMLçš„æ¨ç†é“¾æ›´çŸ­ï¼Œå±•ç°å‡ºæ›´å¼ºçš„æ¨ç†èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.02156">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d0de66be52199adbd2e625a6a3b9b7ec.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c404ecfe71eefc58b729a6b7052a7e0f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-77f2993ab3d5c3a7b3c300f867258d89.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="RTV-Bench-Benchmarking-MLLM-Continuous-Perception-Understanding-and-Reasoning-through-Real-Time-Video"><a href="#RTV-Bench-Benchmarking-MLLM-Continuous-Perception-Understanding-and-Reasoning-through-Real-Time-Video" class="headerlink" title="RTV-Bench: Benchmarking MLLM Continuous Perception, Understanding and   Reasoning through Real-Time Video"></a>RTV-Bench: Benchmarking MLLM Continuous Perception, Understanding and   Reasoning through Real-Time Video</h2><p><strong>Authors:Shuhang Xun, Sicheng Tao, Jungang Li, Yibo Shi, Zhixin Lin, Zhanhui Zhu, Yibo Yan, Hanqian Li, Linghao Zhang, Shikang Wang, Yixin Liu, Hanbo Zhang, Ying Ma, Xuming Hu</strong></p>
<p>Multimodal Large Language Models (MLLMs) increasingly excel at perception, understanding, and reasoning. However, current benchmarks inadequately evaluate their ability to perform these tasks continuously in dynamic, real-world environments. To bridge this gap, we introduce RTV-Bench, a fine-grained benchmark for MLLM real-time video analysis. RTV-Bench uses three key principles: (1) Multi-Timestamp Question Answering (MTQA), where answers evolve with scene changes; (2) Hierarchical Question Structure, combining basic and advanced queries; and (3) Multi-dimensional Evaluation, assessing the ability of continuous perception, understanding, and reasoning. RTV-Bench contains 552 diverse videos (167.2 hours) and 4,631 high-quality QA pairs. We evaluated leading MLLMs, including proprietary (GPT-4o, Gemini 2.0), open-source offline (Qwen2.5-VL, VideoLLaMA3), and open-source real-time (VITA-1.5, InternLM-XComposer2.5-OmniLive) models. Experiment results show open-source real-time models largely outperform offline ones but still trail top proprietary models. Our analysis also reveals that larger model size or higher frame sampling rates do not significantly boost RTV-Bench performance, sometimes causing slight decreases. This underscores the need for better model architectures optimized for video stream processing and long sequences to advance real-time video analysis with MLLMs. Our benchmark toolkit is available at: <a target="_blank" rel="noopener" href="https://github.com/LJungang/RTV-Bench">https://github.com/LJungang/RTV-Bench</a>. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨æ„ŸçŸ¥ã€ç†è§£å’Œæ¨ç†æ–¹é¢è¶Šæ¥è¶Šå‡ºè‰²ã€‚ç„¶è€Œï¼Œå½“å‰çš„åŸºå‡†æµ‹è¯•ä¸è¶³ä»¥è¯„ä¼°å®ƒä»¬åœ¨åŠ¨æ€ã€çœŸå®ç¯å¢ƒçš„å®æ—¶ä»»åŠ¡ä¸­çš„è¿ç»­æ‰§è¡Œèƒ½åŠ›ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†RTV-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºMLLMå®æ—¶è§†é¢‘åˆ†æçš„ç²¾ç»†åŸºå‡†æµ‹è¯•ã€‚RTV-Benché‡‡ç”¨ä¸‰ä¸ªå…³é”®åŸåˆ™ï¼šï¼ˆ1ï¼‰å¤šæ—¶é—´æˆ³é—®ç­”ï¼ˆMTQAï¼‰ï¼Œç­”æ¡ˆéšåœºæ™¯å˜åŒ–è€Œæ¼”å˜ï¼›ï¼ˆ2ï¼‰åˆ†å±‚é—®é¢˜ç»“æ„ï¼Œç»“åˆåŸºæœ¬å’Œé«˜çº§æŸ¥è¯¢ï¼›ï¼ˆ3ï¼‰å¤šç»´è¯„ä¼°ï¼Œè¯„ä¼°æ„ŸçŸ¥ã€ç†è§£å’Œæ¨ç†çš„è¿ç»­èƒ½åŠ›ã€‚RTV-BenchåŒ…å«552ä¸ªå¤šæ ·åŒ–çš„è§†é¢‘ï¼ˆ167.2å°æ—¶ï¼‰å’Œ4631ä¸ªé«˜è´¨é‡çš„QAå¯¹ã€‚æˆ‘ä»¬è¯„ä¼°äº†é¢†å…ˆçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ŒåŒ…æ‹¬ä¸“æœ‰æ¨¡å‹ï¼ˆGPT-4oã€Gemini 2.0ï¼‰ã€å¼€æºç¦»çº¿æ¨¡å‹ï¼ˆQwen2.5-VLã€VideoLLaMA3ï¼‰å’Œå¼€æºå®æ—¶æ¨¡å‹ï¼ˆVITA-1.5ã€InternLM-XComposer2.5-OmniLiveï¼‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå¼€æºå®æ—¶æ¨¡å‹å¤§å¤šä¼˜äºç¦»çº¿æ¨¡å‹ï¼Œä½†ä»è½åäºé¡¶çº§ä¸“æœ‰æ¨¡å‹ã€‚æˆ‘ä»¬çš„åˆ†æè¿˜è¡¨æ˜ï¼Œæ›´å¤§çš„æ¨¡å‹è§„æ¨¡æˆ–æ›´é«˜çš„å¸§é‡‡æ ·ç‡å¹¶ä¸ä¼šæ˜¾è‘—æé«˜RTV-Benchçš„æ€§èƒ½ï¼Œæœ‰æ—¶ç”šè‡³ä¼šå¯¼è‡´è½»å¾®çš„æ€§èƒ½ä¸‹é™ã€‚è¿™å¼ºè°ƒäº†éœ€è¦æ›´å¥½çš„æ¨¡å‹æ¶æ„æ¥ä¼˜åŒ–è§†é¢‘æµå¤„ç†å’Œé•¿åºåˆ—ï¼Œä»¥æ¨åŠ¨ä½¿ç”¨MLLMsçš„å®æ—¶è§†é¢‘åˆ†æçš„å‘å±•ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•å·¥å…·åŒ…å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/LJungang/RTV-Bench%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/LJungang/RTV-Benchè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.02064v2">PDF</a> 13 pages, 4 figures, 5 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨æ„ŸçŸ¥ã€ç†è§£å’Œæ¨ç†æ–¹é¢çš„ä¼˜åŠ¿ï¼Œä½†ç°æœ‰åŸºå‡†æµ‹è¯•æ— æ³•å……åˆ†è¯„ä¼°å®ƒä»¬åœ¨åŠ¨æ€ã€çœŸå®ç¯å¢ƒä¸­çš„æŒç»­ä»»åŠ¡æ‰§è¡Œèƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†RTV-BenchåŸºå‡†æµ‹è¯•ï¼Œç”¨äºå¯¹MLLMå®æ—¶è§†é¢‘åˆ†æè¿›è¡Œç²¾ç»†è¯„ä¼°ã€‚è¯¥åŸºå‡†æµ‹è¯•åŒ…å«ä¸‰ä¸ªå…³é”®åŸåˆ™ï¼šå¤šæ—¶é—´æˆ³é—®ç­”ã€åˆ†å±‚é—®é¢˜ç»“æ„å’Œå¤šç»´åº¦è¯„ä¼°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå¼€æºå®æ—¶æ¨¡å‹åœ¨æ€§èƒ½ä¸Šä¼˜äºç¦»çº¿æ¨¡å‹ï¼Œä½†ä»è½åäºé¡¶çº§ä¸“æœ‰æ¨¡å‹ã€‚æ­¤å¤–ï¼Œæ¨¡å‹è§„æ¨¡æˆ–å¸§é‡‡æ ·ç‡çš„æé«˜å¹¶ä¸ä¸€å®šä¼šæ˜¾è‘—æé«˜RTV-Benchæ€§èƒ½ï¼Œæœ‰æ—¶ç”šè‡³ä¼šç•¥æœ‰ä¸‹é™ã€‚è¿™å¼ºè°ƒäº†éœ€è¦æ›´å¥½çš„é’ˆå¯¹è§†é¢‘æµå¤„ç†å’Œé•¿åºåˆ—ä¼˜åŒ–çš„æ¨¡å‹æ¶æ„ï¼Œä»¥æ¨åŠ¨MLLMçš„å®æ—¶è§†é¢‘åˆ†æå‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨æ„ŸçŸ¥ã€ç†è§£å’Œæ¨ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†éœ€è¦æ›´ç²¾ç»†çš„åŸºå‡†æµ‹è¯•æ¥è¯„ä¼°å…¶åœ¨çœŸå®ç¯å¢ƒä¸­çš„æŒç»­ä»»åŠ¡æ‰§è¡Œèƒ½åŠ›ã€‚</li>
<li>RTV-Benchæ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°MLLMåœ¨å®æ—¶è§†é¢‘åˆ†ææ–¹é¢çš„æ€§èƒ½ï¼ŒåŒ…å«å¤šæ—¶é—´æˆ³é—®ç­”ã€åˆ†å±‚é—®é¢˜ç»“æ„å’Œå¤šç»´åº¦è¯„ä¼°ç­‰å…³é”®åŸåˆ™ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œå¼€æºå®æ—¶æ¨¡å‹æ€§èƒ½ä¼˜äºç¦»çº¿æ¨¡å‹ï¼Œä½†ä»è½åäºä¸€äº›ä¸“æœ‰æ¨¡å‹ã€‚</li>
<li>æ¨¡å‹è§„æ¨¡å’Œå¸§é‡‡æ ·ç‡çš„æé«˜å¹¶ä¸ä¸€å®šèƒ½å¤Ÿæ˜¾è‘—æé«˜RTV-Benchæ€§èƒ½ï¼Œæœ‰æ—¶å¯èƒ½äº§ç”Ÿè´Ÿé¢å½±å“ã€‚</li>
<li>æ›´å¥½çš„é’ˆå¯¹è§†é¢‘æµå¤„ç†å’Œé•¿åºåˆ—ä¼˜åŒ–çš„æ¨¡å‹æ¶æ„æ˜¯æ¨åŠ¨MLLMå®æ—¶è§†é¢‘åˆ†æå‘å±•çš„å…³é”®ã€‚</li>
<li>RTV-BenchåŸºå‡†æµ‹è¯•å·¥å…·åŒ…å¯åœ¨ç½‘ä¸Šè·å–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.02064">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-6354e402ca0811ff973deaeca8ac682a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5e8e4c798605363ed765cce779a5b054.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-30f67bacaae88a35a3b6af1b8c63a0c8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ec10fc39365525610ca17d80234d0b89.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0bf242a1ed069e01427a0d63829db335.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="VideoHallu-Evaluating-and-Mitigating-Multi-modal-Hallucinations-for-Synthetic-Videos"><a href="#VideoHallu-Evaluating-and-Mitigating-Multi-modal-Hallucinations-for-Synthetic-Videos" class="headerlink" title="VideoHallu: Evaluating and Mitigating Multi-modal Hallucinations for   Synthetic Videos"></a>VideoHallu: Evaluating and Mitigating Multi-modal Hallucinations for   Synthetic Videos</h2><p><strong>Authors:Zongxia Li, Xiyang Wu, Yubin Qin, Guangyao Shi, Hongyang Du, Dinesh Manocha, Tianyi Zhou, Jordan Lee Boyd-Graber</strong></p>
<p>Synthetic video generation with foundation models has gained attention for its realism and wide applications. While these models produce high-quality frames, they often fail to respect common sense and physical laws, resulting in abnormal content. Existing metrics like VideoScore emphasize general quality but ignore such violations and lack interpretability. A more insightful approach is using multi-modal large language models (MLLMs) as interpretable evaluators, as seen in FactScore. Yet, MLLMsâ€™ ability to detect abnormalities in synthetic videos remains underexplored. To address this, we introduce VideoHallu, a benchmark featuring synthetic videos from models like Veo2, Sora, and Kling, paired with expert-designed QA tasks solvable via human-level reasoning across various categories. We assess several SoTA MLLMs, including GPT-4o, Gemini-2.5-Pro, Qwen-2.5-VL, and newer models like Video-R1 and VideoChat-R1. Despite strong real-world performance on MVBench and MovieChat, these models still hallucinate on basic commonsense and physics tasks in synthetic settings, underscoring the challenge of hallucination. We further fine-tune SoTA MLLMs using Group Relative Policy Optimization (GRPO) on real and synthetic commonsense&#x2F;physics data. Results show notable accuracy gains, especially with counterexample integration, advancing MLLMsâ€™ reasoning capabilities. Our data is available at <a target="_blank" rel="noopener" href="https://github.com/zli12321/VideoHallu">https://github.com/zli12321/VideoHallu</a>. </p>
<blockquote>
<p>åŸºäºç”Ÿæˆæ¨¡å‹çš„åˆæˆè§†é¢‘ç”Ÿæˆå› å…¶çœŸå®æ„Ÿå’Œå¹¿æ³›åº”ç”¨è€Œå¤‡å—å…³æ³¨ã€‚è™½ç„¶è¿™äº›æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„å¸§ï¼Œä½†å®ƒä»¬å¾€å¾€å¿½ç•¥äº†å¸¸è¯†å’Œç‰©ç†å®šå¾‹ï¼Œå¯¼è‡´å‡ºç°å¼‚å¸¸å†…å®¹ã€‚ç°æœ‰çš„æŒ‡æ ‡å¦‚VideoScoreå¼ºè°ƒæ•´ä½“è´¨é‡ï¼Œä½†å¿½è§†äº†è¿™äº›è¿è§„æƒ…å†µä¸”ç¼ºä¹å¯è§£é‡Šæ€§ã€‚ä¸€ç§æ›´æœ‰æ•ˆçš„æ–¹æ³•æ˜¯ä½¿ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä½œä¸ºå¯è§£é‡Šçš„è¯„ä»·å™¨ï¼Œå¦‚åœ¨FactScoreä¸­æ‰€è§ã€‚ç„¶è€Œï¼ŒMLLMsåœ¨æ£€æµ‹åˆæˆè§†é¢‘ä¸­çš„å¼‚å¸¸æ–¹é¢çš„èƒ½åŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†VideoHalluåŸºå‡†æµ‹è¯•ï¼Œå®ƒåŒ…å«æ¥è‡ªVeo2ã€Soraå’ŒKlingç­‰æ¨¡å‹çš„åˆæˆè§†é¢‘ï¼Œä»¥åŠå¯é€šè¿‡è·¨å„ç§ç±»åˆ«çš„äººç±»çº§æ¨ç†è§£å†³çš„ä¸“å®¶è®¾è®¡é—®ç­”ä»»åŠ¡ã€‚æˆ‘ä»¬è¯„ä¼°äº†å¤šä¸ªæœ€æ–°MLLMsï¼ŒåŒ…æ‹¬GPT-4oã€Gemini-2.5-Proã€Qwen-2.5-VLä»¥åŠVideo-R1å’ŒVideoChat-R1ç­‰è¾ƒæ–°æ¨¡å‹çš„è¡¨ç°ã€‚å°½ç®¡è¿™äº›æ¨¡å‹åœ¨MVBenchå’ŒMovieChatä¸Šçš„ç°å®ä¸–ç•Œè¡¨ç°å¼ºåŠ²ï¼Œä½†åœ¨åˆæˆç¯å¢ƒä¸­çš„åŸºæœ¬å¸¸è¯†å’Œç‰©ç†ä»»åŠ¡ä¸Šä»ä¼šå‡ºç°å¹»è§‰ç°è±¡ï¼Œè¿™çªæ˜¾äº†å¹»è§‰çš„æŒ‘æˆ˜æ€§ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥ä½¿ç”¨Group Relative Policy Optimizationï¼ˆGRPOï¼‰å¯¹æœ€æ–°MLLMsè¿›è¡Œå¾®è°ƒï¼Œç»“åˆçœŸå®å’Œåˆæˆå¸¸è¯†&#x2F;ç‰©ç†æ•°æ®ã€‚ç»“æœæ˜¾ç¤ºï¼Œå‡†ç¡®ç‡æœ‰äº†æ˜¾è‘—æé«˜ï¼Œå°¤å…¶æ˜¯é€šè¿‡åä¾‹é›†æˆåï¼ŒMLLMsçš„æ¨ç†èƒ½åŠ›å¾—åˆ°äº†æå‡ã€‚æˆ‘ä»¬çš„æ•°æ®å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/zli12321/VideoHallu%E4%B8%8A%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/zli12321/VideoHalluä¸Šè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.01481v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å…³æ³¨åˆæˆè§†é¢‘ç”Ÿæˆä¸­çš„ç°å®é—®é¢˜ï¼Œè™½ç„¶ç°æœ‰æ¨¡å‹å¦‚Veo2ã€Soraå’ŒKlingèƒ½ç”Ÿæˆé«˜è´¨é‡å¸§ï¼Œä½†å®ƒä»¬å¸¸å¸¸å¿½è§†å¸¸è¯†å’Œç‰©ç†å®šå¾‹ï¼Œå¯¼è‡´å†…å®¹å¼‚å¸¸ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œå¼•å…¥äº†VideoHalluåŸºå‡†æµ‹è¯•å¹³å°ï¼Œè¯¥å¹³å°åŒ…å«åˆæˆè§†é¢‘ä¸ä¸“å®¶è®¾è®¡çš„é—®ç­”ä»»åŠ¡ï¼Œç”¨ä»¥è¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¸¸è¯†å’Œç‰©ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œå°½ç®¡SoTA MLLMsåœ¨MVBenchå’ŒMovieChatç­‰ç°å®ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨åˆæˆç¯å¢ƒä¸‹çš„å¸¸è¯†å’Œç‰©ç†ä»»åŠ¡ä¸Šä»å­˜åœ¨å¹»è§‰ç°è±¡ã€‚é€šè¿‡Group Relative Policy Optimization (GRPO)å¯¹MLLMsè¿›è¡Œå¾®è°ƒå¹¶ç»“åˆåä¾‹ï¼Œèƒ½æœ‰æ•ˆæå‡æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆæˆè§†é¢‘ç”ŸæˆæŠ€æœ¯è™½ç„¶èƒ½ç”Ÿæˆé«˜è´¨é‡å¸§ï¼Œä½†å¸¸å¸¸å¿½è§†å¸¸è¯†å’Œç‰©ç†å®šå¾‹ï¼Œå¯¼è‡´å†…å®¹å¼‚å¸¸ã€‚</li>
<li>ç°æœ‰çš„è§†é¢‘è´¨é‡è¯„ä¼°æ–¹æ³•å¦‚VideoScoreå¿½ç•¥äº†å†…å®¹å¼‚å¸¸çš„é—®é¢˜ï¼Œç¼ºä¹å¯è§£é‡Šæ€§ã€‚</li>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å¯ä½œä¸ºæ›´æ·±å…¥çš„è¯„ä¼°è€…æ¥æ£€æµ‹åˆæˆè§†é¢‘ä¸­çš„å¼‚å¸¸æƒ…å†µã€‚</li>
<li>å¼•å…¥VideoHalluåŸºå‡†æµ‹è¯•å¹³å°ï¼Œè¯¥å¹³å°åŒ…å«åˆæˆè§†é¢‘ä¸ä¸“å®¶è®¾è®¡çš„é—®ç­”ä»»åŠ¡ï¼Œç”¨ä»¥è¯„ä¼°MLLMsåœ¨å¸¸è¯†å’Œç‰©ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚</li>
<li>SoTA MLLMsåœ¨åˆæˆç¯å¢ƒä¸‹çš„å¸¸è¯†å’Œç‰©ç†ä»»åŠ¡ä¸Šä»å­˜åœ¨å¹»è§‰ç°è±¡ã€‚</li>
<li>é€šè¿‡Group Relative Policy Optimization (GRPO)å¯¹MLLMsè¿›è¡Œå¾®è°ƒå¹¶ç»“åˆåä¾‹ï¼Œèƒ½æ˜¾è‘—æå‡æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.01481">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7c330be565d79efa41fec56dda066294.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-43f7b8ad9d401faba797fb120ca16092.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eab4d397a865b539b3bc2732d40f4eb6.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Llama-Nemotron-Efficient-Reasoning-Models"><a href="#Llama-Nemotron-Efficient-Reasoning-Models" class="headerlink" title="Llama-Nemotron: Efficient Reasoning Models"></a>Llama-Nemotron: Efficient Reasoning Models</h2><p><strong>Authors:Akhiad Bercovich, Itay Levy, Izik Golan, Mohammad Dabbah, Ran El-Yaniv, Omri Puny, Ido Galil, Zach Moshe, Tomer Ronen, Najeeb Nabwani, Ido Shahaf, Oren Tropp, Ehud Karpas, Ran Zilberstein, Jiaqi Zeng, Soumye Singhal, Alexander Bukharin, Yian Zhang, Tugrul Konuk, Gerald Shen, Ameya Sunil Mahabaleshwarkar, Bilal Kartal, Yoshi Suhara, Olivier Delalleau, Zijia Chen, Zhilin Wang, David Mosallanezhad, Adi Renduchintala, Haifeng Qian, Dima Rekesh, Fei Jia, Somshubra Majumdar, Vahid Noroozi, Wasi Uddin Ahmad, Sean Narenthiran, Aleksander Ficek, Mehrzad Samadi, Jocelyn Huang, Siddhartha Jain, Igor Gitman, Ivan Moshkov, Wei Du, Shubham Toshniwal, George Armstrong, Branislav Kisacanin, Matvei Novikov, Daria Gitman, Evelina Bakhturina, Jane Polak Scowcroft, John Kamalu, Dan Su, Kezhi Kong, Markus Kliegl, Rabeeh Karimi, Ying Lin, Sanjeev Satheesh, Jupinder Parmar, Pritam Gundecha, Brandon Norick, Joseph Jennings, Shrimai Prabhumoye, Syeda Nahida Akter, Mostofa Patwary, Abhinav Khattar, Deepak Narayanan, Roger Waleffe, Jimmy Zhang, Bor-Yiing Su, Guyue Huang, Terry Kong, Parth Chadha, Sahil Jain, Christine Harvey, Elad Segal, Jining Huang, Sergey Kashirsky, Robert McQueen, Izzy Putterman, George Lam, Arun Venkatesan, Sherry Wu, Vinh Nguyen, Manoj Kilaru, Andrew Wang, Anna Warno, Abhilash Somasamudramath, Sandip Bhaskar, Maka Dong, Nave Assaf, Shahar Mor, Omer Ullman Argov, Scot Junkin, Oleksandr Romanenko, Pedro Larroy, Monika Katariya, Marco Rovinelli, Viji Balas, Nicholas Edelman, Anahita Bhiwandiwalla, Muthu Subramaniam, Smita Ithape, Karthik Ramamoorthy, Yuting Wu, Suguna Varshini Velury, Omri Almog, Joyjit Daw, Denys Fridman, Erick Galinkin, Michael Evans, Katherine Luna, Leon Derczynski, Nikki Pope, Eileen Long, Seth Schneider, Guillermo Siman, Tomasz Grzegorzek, Pablo Ribalta, Monika Katariya, Joey Conway, Trisha Saar, Ann Guan, Krzysztof Pawelec, Shyamala Prayaga, Oleksii Kuchaiev, Boris Ginsburg, Oluwatobi Olabiyi, Kari Briski, Jonathan Cohen, Bryan Catanzaro, Jonah Alben, Yonatan Geifman, Eric Chung, Chris Alexiuk</strong></p>
<p>We introduce the Llama-Nemotron series of models, an open family of heterogeneous reasoning models that deliver exceptional reasoning capabilities, inference efficiency, and an open license for enterprise use. The family comes in three sizes â€“ Nano (8B), Super (49B), and Ultra (253B) â€“ and performs competitively with state-of-the-art reasoning models such as DeepSeek-R1 while offering superior inference throughput and memory efficiency. In this report, we discuss the training procedure for these models, which entails using neural architecture search from Llama 3 models for accelerated inference, knowledge distillation, and continued pretraining, followed by a reasoning-focused post-training stage consisting of two main parts: supervised fine-tuning and large scale reinforcement learning. Llama-Nemotron models are the first open-source models to support a dynamic reasoning toggle, allowing users to switch between standard chat and reasoning modes during inference. To further support open research and facilitate model development, we provide the following resources: 1. We release the Llama-Nemotron reasoning models â€“ LN-Nano, LN-Super, and LN-Ultra â€“ under the commercially permissive NVIDIA Open Model License Agreement. 2. We release the complete post-training dataset: Llama-Nemotron-Post-Training-Dataset. 3. We also release our training codebases: NeMo, NeMo-Aligner, and Megatron-LM. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†Llama-Nemotronç³»åˆ—æ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ä¸ªå¼€æ”¾çš„å¼‚è´¨æ¨ç†æ¨¡å‹å®¶æ—ï¼Œå…·æœ‰å‡ºè‰²çš„æ¨ç†èƒ½åŠ›ã€æ¨ç†æ•ˆç‡å’Œå¼€æ”¾çš„ä¼ä¸šä½¿ç”¨è®¸å¯ã€‚è¯¥ç³»åˆ—æœ‰ä¸‰ç§è§„æ¨¡ï¼šNanoï¼ˆ8Bï¼‰ã€Superï¼ˆ49Bï¼‰å’ŒUltraï¼ˆ253Bï¼‰ï¼Œä¸æœ€å…ˆè¿›çš„æ¨ç†æ¨¡å‹å¦‚DeepSeek-R1ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ï¼ŒåŒæ—¶æä¾›å“è¶Šçš„æ¨ç†ååé‡å’Œå†…å­˜æ•ˆç‡ã€‚åœ¨æœ¬æŠ¥å‘Šä¸­ï¼Œæˆ‘ä»¬è®¨è®ºäº†è¿™äº›æ¨¡å‹çš„è®­ç»ƒæµç¨‹ï¼ŒåŒ…æ‹¬ä½¿ç”¨Llama 3æ¨¡å‹çš„ç¥ç»ç½‘ç»œæ¶æ„æœç´¢ä»¥åŠ é€Ÿæ¨ç†ã€çŸ¥è¯†è’¸é¦å’ŒæŒç»­é¢„è®­ç»ƒï¼Œéšåæ˜¯ä¸“æ³¨äºæ¨ç†çš„åè®­ç»ƒé˜¶æ®µï¼Œä¸»è¦åŒ…æ‹¬ä¸¤éƒ¨åˆ†ï¼šç›‘ç£å¾®è°ƒå’Œå¤§è§„æ¨¡å¢å¼ºå­¦ä¹ ã€‚Llama-Nemotronæ¨¡å‹æ˜¯é¦–ä¸ªæ”¯æŒåŠ¨æ€æ¨ç†åˆ‡æ¢çš„å¼€æºæ¨¡å‹ï¼Œå…è®¸ç”¨æˆ·åœ¨æ¨ç†è¿‡ç¨‹ä¸­åˆ‡æ¢æ ‡å‡†èŠå¤©å’Œæ¨ç†æ¨¡å¼ã€‚ä¸ºäº†è¿›ä¸€æ­¥æ”¯æŒå¼€æ”¾ç ”ç©¶å’Œä¿ƒè¿›æ¨¡å‹å¼€å‘ï¼Œæˆ‘ä»¬æä¾›äº†ä»¥ä¸‹èµ„æºï¼š1.æˆ‘ä»¬åœ¨å•†ä¸šè®¸å¯çš„NVIDIAå¼€æ”¾æ¨¡å‹è®¸å¯åè®®ä¸‹å‘å¸ƒäº†Llama-Nemotronæ¨ç†æ¨¡å‹â€”â€”LN-Nanoã€LN-Superå’ŒLN-Ultraã€‚2.æˆ‘ä»¬å‘å¸ƒäº†å®Œæ•´çš„åè®­ç»ƒæ•°æ®é›†ï¼šLlama-Nemotron-Post-Training-Datasetã€‚3.æˆ‘ä»¬è¿˜å‘å¸ƒäº†æˆ‘ä»¬çš„è®­ç»ƒä»£ç åº“ï¼šNeMoã€NeMo-Alignerå’ŒMegatron-LMã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.00949v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºLlama 3æ¨¡å‹çš„ç¥ç»ç½‘ç»œæ¶æ„æœç´¢åŠ é€Ÿæ¨ç†ã€çŸ¥è¯†è’¸é¦å’ŒæŒç»­é¢„è®­ç»ƒç­‰æŠ€æœ¯ï¼Œæˆ‘ä»¬æ¨å‡ºäº†Llama-Nemotronç³»åˆ—æ¨¡å‹ï¼ŒåŒ…æ‹¬Nanoï¼ˆ8Bï¼‰ã€Superï¼ˆ49Bï¼‰å’ŒUltraï¼ˆ253Bï¼‰ä¸‰ç§è§„æ¨¡ã€‚è¯¥ç³»åˆ—æ¨¡å‹å…·æœ‰å‡ºè‰²çš„æ¨ç†èƒ½åŠ›ã€é«˜æ•ˆçš„æ¨ç†æ•ˆç‡ï¼Œå¹¶æ”¯æŒä¼ä¸šä½¿ç”¨å¼€æ”¾è®¸å¯ã€‚è¯¥æ¨¡å‹æ˜¯é¦–ä¸ªæ”¯æŒåŠ¨æ€æ¨ç†åˆ‡æ¢çš„å¼€æºæ¨¡å‹ï¼Œç”¨æˆ·å¯åœ¨æ¨ç†è¿‡ç¨‹ä¸­åˆ‡æ¢æ ‡å‡†èŠå¤©å’Œæ¨ç†æ¨¡å¼ã€‚ä¸ºæ”¯æŒå¼€æ”¾ç ”ç©¶å’Œä¿ƒè¿›æ¨¡å‹å‘å±•ï¼Œæˆ‘ä»¬æä¾›äº†ä»¥ä¸‹èµ„æºï¼š</p>
<ul>
<li>å‘å¸ƒäº†Llama-Nemotronæ¨ç†æ¨¡å‹ï¼ˆLN-Nanoã€LN-Superã€LN-Ultraï¼‰ï¼Œè®¸å¯åè®®ä¸ºå•†ä¸šè®¸å¯çš„NVIDIA Open Model License Agreementã€‚</li>
<li>å…¬å¼€äº†å®Œæ•´çš„åè®­ç»ƒæ•°æ®é›†Llama-Nemotron-Post-Training-Datasetã€‚</li>
<li>å‘å¸ƒäº†æˆ‘ä»¬çš„è®­ç»ƒä»£ç åº“NeMoã€NeMo-Alignerå’ŒMegatron-LMã€‚</li>
</ul>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Llama-Nemotronç³»åˆ—æ¨¡å‹æ˜¯ä¸€ä¸ªå¼€æºçš„å¼‚æ„æ¨ç†æ¨¡å‹å®¶æ—ï¼Œæä¾›ä¸‰ç§è§„æ¨¡ä¾›é€‰æ‹©ã€‚</li>
<li>è¯¥ç³»åˆ—æ¨¡å‹å…·å¤‡å…ˆè¿›çš„æ¨ç†èƒ½åŠ›å’Œé«˜æ•ˆçš„æ¨ç†æ•ˆç‡ï¼Œä¸ç°æœ‰æœ€å…ˆè¿›çš„æ¨ç†æ¨¡å‹å¦‚DeepSeek-R1ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ã€‚</li>
<li>æ¨¡å‹è®­ç»ƒè¿‡ç¨‹åŒ…æ‹¬ä½¿ç”¨ç¥ç»ç½‘ç»œæ¶æ„æœç´¢åŠ é€Ÿæ¨ç†ã€çŸ¥è¯†è’¸é¦å’ŒæŒç»­é¢„è®­ç»ƒç­‰æŠ€æœ¯ã€‚</li>
<li>æ¨¡å‹æ”¯æŒåŠ¨æ€æ¨ç†åˆ‡æ¢ï¼Œç”¨æˆ·å¯æ ¹æ®éœ€æ±‚åœ¨æ ‡å‡†èŠå¤©å’Œæ¨ç†æ¨¡å¼ä¹‹é—´åˆ‡æ¢ã€‚</li>
<li>æ¨¡å‹å‘å¸ƒåŒ…æ‹¬LN-Nanoã€LN-Superã€LN-Ultraä¸‰ç§è§„æ¨¡çš„æ¨ç†æ¨¡å‹ï¼Œå¹¶éµå¾ªNVIDIA Open Model License Agreementè®¸å¯åè®®ã€‚</li>
<li>å…¬å¼€äº†å®Œæ•´çš„åè®­ç»ƒæ•°æ®é›†Llama-Nemotron-Post-Training-Datasetï¼Œä»¥æ”¯æŒç ”ç©¶å’Œæ¨¡å‹å¼€å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.00949">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a1199fe9c081fc37f51ce7a4c8777269.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ffc8d12a6834b4c9e396a6053dddb5b5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d628edf153a5cada2c015b186a4980e5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5107c1e8e34460d4858de1099677bd03.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-955b8ad12e7bdca49170d8f4949d0583.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="SmallPlan-Leverage-Small-Language-Models-for-Sequential-Path-Planning-with-Simulation-Powered-LLM-Guided-Distillation"><a href="#SmallPlan-Leverage-Small-Language-Models-for-Sequential-Path-Planning-with-Simulation-Powered-LLM-Guided-Distillation" class="headerlink" title="SmallPlan: Leverage Small Language Models for Sequential Path Planning   with Simulation-Powered, LLM-Guided Distillation"></a>SmallPlan: Leverage Small Language Models for Sequential Path Planning   with Simulation-Powered, LLM-Guided Distillation</h2><p><strong>Authors:Quang P. M. Pham, Khoi T. N. Nguyen, Nhi H. Doan, Cuong A. Pham, Kentaro Inui, Dezhen Song</strong></p>
<p>Efficient path planning in robotics, particularly within large-scale, dynamic environments, remains a significant hurdle. While Large Language Models (LLMs) offer strong reasoning capabilities, their high computational cost and limited adaptability in dynamic scenarios hinder real-time deployment on edge devices. We present SmallPlan â€“ a novel framework leveraging LLMs as teacher models to train lightweight Small Language Models (SLMs) for high-level path planning tasks. In SmallPlan, the SLMs provide optimal action sequences to navigate across scene graphs that compactly represent full-scaled 3D scenes. The SLMs are trained in a simulation-powered, interleaved manner with LLM-guided supervised fine-tuning (SFT) and reinforcement learning (RL). This strategy not only enables SLMs to successfully complete navigation tasks but also makes them aware of important factors like travel distance and number of trials. Through experiments, we demonstrate that the fine-tuned SLMs perform competitively with larger models like GPT-4o on sequential path planning, without suffering from hallucination and overfitting. SmallPlan is resource-efficient, making it well-suited for edge-device deployment and advancing practical autonomous robotics. </p>
<blockquote>
<p>åœ¨æœºå™¨äººæŠ€æœ¯ä¸­ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤§è§„æ¨¡ã€åŠ¨æ€ç¯å¢ƒä¸­è¿›è¡Œé«˜æ•ˆè·¯å¾„è§„åˆ’ä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ã€‚è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æä¾›äº†å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œä½†å®ƒä»¬çš„é«˜è®¡ç®—æˆæœ¬å’Œåœ¨åŠ¨æ€åœºæ™¯ä¸­çš„æœ‰é™é€‚åº”æ€§é˜»ç¢äº†å…¶åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šçš„å®æ—¶éƒ¨ç½²ã€‚æˆ‘ä»¬æå‡ºäº†SmallPlanâ€”â€”ä¸€ä¸ªåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºæ•™å¸ˆæ¨¡å‹æ¥è®­ç»ƒç”¨äºé«˜çº§è·¯å¾„è§„åˆ’ä»»åŠ¡çš„å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰çš„æ–°å‹æ¡†æ¶ã€‚åœ¨SmallPlanä¸­ï¼ŒSLMæä¾›æœ€ä¼˜åŠ¨ä½œåºåˆ—ï¼Œä»¥åœ¨ç´§å‡‘åœ°ä»£è¡¨å…¨å°ºåº¦3Dåœºæ™¯çš„åœºæ™¯å›¾ä¸­è¿›è¡Œå¯¼èˆªã€‚SLMä»¥æ¨¡æ‹Ÿé©±åŠ¨çš„æ–¹å¼ï¼Œé€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹æŒ‡å¯¼çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¿›è¡Œè®­ç»ƒã€‚è¿™ç§ç­–ç•¥ä¸ä»…ä½¿SLMèƒ½å¤ŸæˆåŠŸå®Œæˆå¯¼èˆªä»»åŠ¡ï¼Œè¿˜ä½¿å®ƒä»¬æ„è¯†åˆ°æ—…è¡Œè·ç¦»å’Œè¯•éªŒæ¬¡æ•°ç­‰é‡è¦å› ç´ ã€‚é€šè¿‡å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†ç»è¿‡å¾®è°ƒçš„å°å‹è¯­è¨€æ¨¡å‹åœ¨åºåˆ—è·¯å¾„è§„åˆ’æ–¹é¢ä¸GPT-4ç­‰å¤§å‹æ¨¡å‹è¡¨ç°ç›¸å½“ï¼Œä¸ä¼šå‡ºç°å¹»è§‰å’Œè¿‡åº¦æ‹Ÿåˆçš„æƒ…å†µã€‚SmallPlanèµ„æºé«˜æ•ˆï¼Œéå¸¸é€‚åˆåœ¨è¾¹ç¼˜è®¾å¤‡è¿›è¡Œéƒ¨ç½²ï¼Œä¸ºæ¨åŠ¨å®é™…è‡ªä¸»æœºå™¨äººæŠ€æœ¯çš„å‘å±•åšå‡ºè´¡çŒ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.00831v2">PDF</a> Paper is under review</p>
<p><strong>Summary</strong><br>é«˜æ•ˆè·¯å¾„è§„åˆ’åœ¨æœºå™¨äººæŠ€æœ¯ä¸­æ˜¯ä¸€å¤§æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤§è§„æ¨¡åŠ¨æ€ç¯å¢ƒä¸­ã€‚å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å…·å¤‡å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œä½†å…¶é«˜è®¡ç®—æˆæœ¬å’Œåœ¨åŠ¨æ€åœºæ™¯ä¸­çš„æœ‰é™é€‚åº”æ€§é˜»ç¢äº†å…¶åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šçš„å®æ—¶éƒ¨ç½²ã€‚æˆ‘ä»¬æå‡ºäº†SmallPlanæ¡†æ¶ï¼Œåˆ©ç”¨LLMsä½œä¸ºæ•™å¸ˆæ¨¡å‹æ¥è®­ç»ƒè½»é‡çº§å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMsï¼‰ï¼Œç”¨äºé«˜çº§è·¯å¾„è§„åˆ’ä»»åŠ¡ã€‚SmallPlané€šè¿‡SLMæä¾›æœ€ä¼˜åŠ¨ä½œåºåˆ—æ¥åœ¨åœºæ™¯å›¾ä¸­å¯¼èˆªï¼Œåœºæ™¯å›¾ç´§å‡‘åœ°è¡¨ç¤ºå…¨å°ºå¯¸3Dåœºæ™¯ã€‚SLMé‡‡ç”¨ä»¿çœŸé©±åŠ¨çš„äº¤é”™æ–¹å¼è¿›è¡Œè®­ç»ƒï¼Œç»“åˆLLMæŒ‡å¯¼çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ã€‚è¿™ä¸€ç­–ç•¥ä¸ä»…ä½¿SLMæˆåŠŸå®Œæˆå¯¼èˆªä»»åŠ¡ï¼Œè¿˜ä½¿å…¶èƒ½å¤Ÿäº†è§£æ—…è¡Œè·ç¦»å’Œè¯•éªŒæ¬¡æ•°ç­‰é‡è¦å› ç´ ã€‚å®éªŒè¡¨æ˜ï¼Œç»è¿‡å¾®è°ƒåçš„SLMåœ¨åºåˆ—è·¯å¾„è§„åˆ’æ–¹é¢çš„è¡¨ç°ä¸GPT-4oç­‰å¤§å‹æ¨¡å‹ç›¸å½“ï¼Œä¸å­˜åœ¨å¹»è§‰å’Œè¿‡åº¦æ‹Ÿåˆé—®é¢˜ã€‚SmallPlanå…·æœ‰èµ„æºæ•ˆç‡é«˜çš„ç‰¹ç‚¹ï¼Œéå¸¸é€‚åˆç”¨äºè¾¹ç¼˜è®¾å¤‡éƒ¨ç½²ï¼Œæ¨åŠ¨å®ç”¨å‹è‡ªä¸»æœºå™¨äººæŠ€æœ¯çš„å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æœºå™¨äººè·¯å¾„è§„åˆ’ä¸­è™½å…·å¤‡å¼ºå¤§æ¨ç†èƒ½åŠ›ï¼Œä½†åœ¨åŠ¨æ€åœºæ™¯ä¸­çš„å®æ—¶éƒ¨ç½²å­˜åœ¨è®¡ç®—æˆæœ¬é«˜å’Œé€‚åº”æ€§æœ‰é™çš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åä¸ºSmallPlançš„æ–°å‹æ¡†æ¶ï¼Œåˆ©ç”¨LLMsä½œä¸ºæ•™å¸ˆæ¨¡å‹æ¥è®­ç»ƒè½»é‡çº§å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMsï¼‰ï¼Œç”¨äºé«˜çº§è·¯å¾„è§„åˆ’ä»»åŠ¡ã€‚</li>
<li>SLMé€šè¿‡æä¾›æœ€ä¼˜åŠ¨ä½œåºåˆ—åœ¨åœºæ™¯å›¾ä¸­è¿›è¡Œå¯¼èˆªï¼Œç´§å‡‘åœ°è¡¨ç¤ºå…¨å°ºå¯¸3Dåœºæ™¯ã€‚</li>
<li>SLMé‡‡ç”¨ä»¿çœŸé©±åŠ¨çš„äº¤é”™æ–¹å¼è¿›è¡Œè®­ç»ƒï¼Œç»“åˆç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ–¹æ³•ã€‚</li>
<li>æ­¤ç­–ç•¥ä½¿SLMä¸ä»…æˆåŠŸå®Œæˆå¯¼èˆªä»»åŠ¡ï¼Œè¿˜å­¦ä¹ åˆ°è€ƒè™‘æ—…è¡Œè·ç¦»å’Œè¯•éªŒæ¬¡æ•°ç­‰é‡è¦å› ç´ çš„èƒ½åŠ›ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºï¼Œå¾®è°ƒåçš„SLMåœ¨åºåˆ—è·¯å¾„è§„åˆ’ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä¸å¤§å‹æ¨¡å‹å¦‚GPT-4oç›¸å½“ï¼Œä¸å­˜åœ¨å¹»è§‰å’Œè¿‡åº¦æ‹Ÿåˆé—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.00831">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d92e990f561a01971a16fff7228035fd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-67d4bfd2544253742e0f454092cee4cc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-99e74c9601a2faff20a2aaa27abb4f69.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Grokking-in-the-Wild-Data-Augmentation-for-Real-World-Multi-Hop-Reasoning-with-Transformers"><a href="#Grokking-in-the-Wild-Data-Augmentation-for-Real-World-Multi-Hop-Reasoning-with-Transformers" class="headerlink" title="Grokking in the Wild: Data Augmentation for Real-World Multi-Hop   Reasoning with Transformers"></a>Grokking in the Wild: Data Augmentation for Real-World Multi-Hop   Reasoning with Transformers</h2><p><strong>Authors:Roman Abramov, Felix Steinbauer, Gjergji Kasneci</strong></p>
<p>Transformers have achieved great success in numerous NLP tasks but continue to exhibit notable gaps in multi-step factual reasoning, especially when real-world knowledge is sparse. Recent advances in grokking have demonstrated that neural networks can transition from memorizing to perfectly generalizing once they detect underlying logical patterns - yet these studies have primarily used small, synthetic tasks. In this paper, for the first time, we extend grokking to real-world factual data and address the challenge of dataset sparsity by augmenting existing knowledge graphs with carefully designed synthetic data to raise the ratio $\phi_r$ of inferred facts to atomic facts above the threshold required for grokking. Surprisingly, we find that even factually incorrect synthetic data can strengthen emergent reasoning circuits rather than degrade accuracy, as it forces the model to rely on relational structure rather than memorization. When evaluated on multi-hop reasoning benchmarks, our approach achieves up to 95-100% accuracy on 2WikiMultiHopQA - substantially improving over strong baselines and matching or exceeding current state-of-the-art results. We further provide an in-depth analysis of how increasing $\phi_r$ drives the formation of generalizing circuits inside Transformers. Our findings suggest that grokking-based data augmentation can unlock implicit multi-hop reasoning capabilities, opening the door to more robust and interpretable factual reasoning in large-scale language models. </p>
<blockquote>
<p>Transformeråœ¨ä¼—å¤šNLPä»»åŠ¡ä¸­å–å¾—äº†å·¨å¤§æˆåŠŸï¼Œä½†åœ¨å¤šæ­¥éª¤äº‹å®æ¨ç†æ–¹é¢ä»å­˜åœ¨æ˜æ˜¾å·®è·ï¼Œç‰¹åˆ«æ˜¯åœ¨ç°å®ä¸–ç•ŒçŸ¥è¯†ç¨€ç¼ºçš„æƒ…å†µä¸‹ã€‚æœ€è¿‘çš„é’»ç ”è¯æ˜ï¼Œä¸€æ—¦ç¥ç»ç½‘ç»œå‘ç°æ½œåœ¨çš„é€»è¾‘æ¨¡å¼ï¼Œå®ƒä»¬å°±å¯ä»¥ä»è®°å¿†è½¬å‘å®Œå…¨æ³›åŒ–ã€‚ç„¶è€Œï¼Œè¿™äº›ç ”ç©¶ä¸»è¦ä½¿ç”¨çš„æ˜¯å°å‹åˆæˆä»»åŠ¡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é¦–æ¬¡å°†é’»ç ”æ‰©å±•åˆ°ç°å®ä¸–ç•Œçš„äº‹å®æ•°æ®ï¼Œå¹¶é€šè¿‡å¢å¼ºç°æœ‰çŸ¥è¯†å›¾è°±å’Œç²¾å¿ƒè®¾è®¡åˆæˆæ•°æ®æ¥è§£å†³æ•°æ®é›†ç¨€ç–æ€§çš„æŒ‘æˆ˜ï¼Œä»¥æé«˜æ¨æ–­äº‹å®ä¸åŸå­äº‹å®çš„æ¯”ä¾‹$\phi_r$ï¼Œè¾¾åˆ°æ³›åŒ–æ‰€éœ€é˜ˆå€¼ä»¥ä¸Šã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç°å³ä½¿æ˜¯äº‹å®é”™è¯¯çš„åˆæˆæ•°æ®ä¹Ÿèƒ½åŠ å¼ºæ–°å…´æ¨ç†ç”µè·¯ï¼Œè€Œä¸æ˜¯é™ä½å‡†ç¡®æ€§ï¼Œå› ä¸ºå®ƒè¿«ä½¿æ¨¡å‹ä¾èµ–å…³ç³»ç»“æ„è€Œä¸æ˜¯è®°å¿†ã€‚åœ¨è¯„ä¼°å¤šè·³æ¨ç†åŸºå‡†æµ‹è¯•æ—¶ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨2WikiMultiHopQAä¸Šè¾¾åˆ°äº†é«˜è¾¾95-100%çš„å‡†ç¡®ç‡ï¼Œå¤§å¹…è¶…è¶Šäº†å¼ºå¤§çš„åŸºçº¿å¹¶åŒ¹é…æˆ–è¶…è¶Šäº†å½“å‰æœ€æ–°æŠ€æœ¯ç»“æœã€‚æˆ‘ä»¬è¿˜æ·±å…¥åˆ†æäº†å¦‚ä½•æé«˜$\phi_r$æ¥é©±åŠ¨Transformerå†…éƒ¨æ³›åŒ–ç”µè·¯çš„å½¢æˆã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒåŸºäºæ³›åŒ–çš„æ•°æ®å¢å¼ºå¯ä»¥è§£é”éšå¼å¤šè·³æ¨ç†èƒ½åŠ›ï¼Œä¸ºå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ä¸­çš„æ›´ç¨³å¥å’Œå¯è§£é‡Šçš„äº‹å®æ¨ç†æ‰“å¼€äº†å¤§é—¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.20752v2">PDF</a> Accepted to the International Conference on Machine Learning (ICML)   2025</p>
<p><strong>Summary</strong>ï¼šå˜å‹å™¨åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­å–å¾—äº†å·¨å¤§æˆåŠŸï¼Œä½†åœ¨å¤šæ­¥éª¤æ¨ç†æ–¹é¢ä»å­˜åœ¨æ˜æ˜¾å·®è·ï¼Œç‰¹åˆ«æ˜¯åœ¨ç°å®ä¸–ç•ŒçŸ¥è¯†ç¨€ç–çš„æƒ…å†µä¸‹ã€‚æœ€è¿‘å¯¹grokkingçš„ç ”ç©¶è¡¨æ˜ï¼Œç¥ç»ç½‘ç»œä¸€æ—¦æ£€æµ‹åˆ°æ½œåœ¨çš„é€»è¾‘æ¨¡å¼ï¼Œå°±å¯ä»¥ä»è®°å¿†è½¬å˜ä¸ºå®Œå…¨æ³›åŒ–ã€‚ç„¶è€Œï¼Œè¿™äº›ç ”ç©¶ä¸»è¦ä½¿ç”¨å°å‹åˆæˆä»»åŠ¡ã€‚æœ¬æ–‡é¦–æ¬¡å°†grokkingæ‰©å±•åˆ°ç°å®ä¸–ç•Œçš„å®é™…æ•°æ®ï¼Œå¹¶é€šè¿‡å¢å¼ºç°æœ‰çŸ¥è¯†å›¾è°±å’Œç²¾å¿ƒè®¾è®¡åˆæˆæ•°æ®æ¥è§£å†³æ•°æ®é›†ç¨€ç–æ€§çš„æŒ‘æˆ˜ã€‚ç ”ç©¶å‘ç°ï¼Œå³ä½¿æ˜¯äº‹å®é”™è¯¯çš„åˆæˆæ•°æ®ä¹Ÿèƒ½åŠ å¼ºæ–°å…´æ¨ç†ç”µè·¯è€Œä¸æ˜¯é™ä½å‡†ç¡®æ€§ï¼Œå› ä¸ºå®ƒè¿«ä½¿æ¨¡å‹ä¾èµ–å…³ç³»ç»“æ„è€Œä¸æ˜¯è®°å¿†ã€‚åœ¨è¯„ä¼°å¤šè·³æ¨ç†åŸºå‡†æµ‹è¯•æ—¶ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨2WikiMultiHopQAä¸Šè¾¾åˆ°äº†95-100%çš„å‡†ç¡®ç‡ï¼Œå¤§å¹…è¶…è¶Šäº†å¼ºå¤§çš„åŸºå‡†æµ‹è¯•å¹¶åŒ¹é…æˆ–è¶…è¿‡äº†å½“å‰æœ€æ–°ç»“æœã€‚æˆ‘ä»¬çš„åˆ†æè¿›ä¸€æ­¥è¡¨æ˜ï¼Œå¢åŠ $\phi_r$å¯æ¨åŠ¨å˜å‹å™¨å†…éƒ¨æ³›åŒ–ç”µè·¯çš„å½¢æˆã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒåŸºäºgrokkingçš„æ•°æ®å¢å¼ºå¯ä»¥è§£é”éšå¼å¤šè·³æ¨ç†èƒ½åŠ›ï¼Œä¸ºå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ä¸­çš„æ›´ç¨³å¥å’Œå¯è§£é‡Šçš„äº‹å®æ¨ç†æ‰“å¼€äº†å¤§é—¨ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>å˜å‹å™¨åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†åœ¨å¤šæ­¥éª¤æ¨ç†æ–¹é¢ä»æœ‰å·®è·ã€‚</li>
<li>Grokkingç ”ç©¶è¡¨æ˜ç¥ç»ç½‘ç»œå¯ä»¥ä»è®°å¿†è½¬å˜ä¸ºæ³›åŒ–ï¼Œå½“æ£€æµ‹åˆ°æ½œåœ¨é€»è¾‘æ¨¡å¼æ—¶ã€‚</li>
<li>æœ¬æ–‡é¦–æ¬¡å°†grokkingæ‰©å±•åˆ°ç°å®ä¸–ç•Œçš„å®é™…æ•°æ®ï¼Œè§£å†³æ•°æ®é›†ç¨€ç–æ€§é—®é¢˜ã€‚</li>
<li>é€šè¿‡å¢å¼ºçŸ¥è¯†å›¾è°±å’Œåˆæˆæ•°æ®æ¥æé«˜äº‹å®æ¨æ–­ä¸åŸå­äº‹å®ä¹‹æ¯”$\phi_r$ã€‚</li>
<li>äº‹å®ä¸Šé”™è¯¯çš„åˆæˆæ•°æ®ä¹Ÿèƒ½åŠ å¼ºæ¨ç†ç”µè·¯ï¼Œè¿™è¿«ä½¿æ¨¡å‹ä¾èµ–å…³ç³»ç»“æ„è€Œéè®°å¿†ã€‚</li>
<li>æ–¹æ³•çš„å‡†ç¡®ç‡åœ¨2WikiMultiHopQAä¸Šé«˜è¾¾95-100%ï¼Œè¶…è¶ŠåŸºå‡†æµ‹è¯•å¹¶åŒ¹é…æœ€æ–°ç»“æœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.20752">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-406b431f663203c8959efb51332e98bd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-94a55d5cbe8ee683dc6e752d80e2a233.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0b9b7b6e7023119aa898965d466f8aab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6e554fd2d70706b64cfe4ff222dedc21.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-76abdaf2dcffe85e82b856b4bbf98e46.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6591030219d9ec9b02801b6e8e3fc520.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Benchmarking-Multimodal-Mathematical-Reasoning-with-Explicit-Visual-Dependency"><a href="#Benchmarking-Multimodal-Mathematical-Reasoning-with-Explicit-Visual-Dependency" class="headerlink" title="Benchmarking Multimodal Mathematical Reasoning with Explicit Visual   Dependency"></a>Benchmarking Multimodal Mathematical Reasoning with Explicit Visual   Dependency</h2><p><strong>Authors:Zhikai Wang, Jiashuo Sun, Wenqi Zhang, Zhiqiang Hu, Xin Li, Fan Wang, Deli Zhao</strong></p>
<p>Recent advancements in Large Vision-Language Models (LVLMs) have significantly enhanced their ability to integrate visual and linguistic information, achieving near-human proficiency in tasks like object recognition, captioning, and visual question answering. However, current benchmarks typically focus on knowledge-centric evaluations that assess domain-specific expertise, often neglecting the core ability to reason about fundamental mathematical elements and visual concepts. We identify a gap in evaluating elementary-level math problems, which rely on explicit visual dependencies-requiring models to discern, integrate, and reason across multiple images while incorporating commonsense knowledge, all of which are crucial for advancing toward broader AGI capabilities. To address this gap, we introduce VCBENCH, a comprehensive benchmark for multimodal mathematical reasoning with explicit visual dependencies. VCBENCH includes 1,720 problems across six cognitive domains, featuring 6,697 images (averaging 3.9 per question) to ensure multi-image reasoning. We evaluate 26 state-of-the-art LVLMs on VCBENCH, revealing substantial performance disparities, with even the top models unable to exceed 50% accuracy. Our findings highlight the ongoing challenges in visual-mathematical integration and suggest avenues for future LVLM advancements.The project can be found at <a target="_blank" rel="noopener" href="https://alibaba-damo-academy.github.io/VCBench/">https://alibaba-damo-academy.github.io/VCBench/</a>. </p>
<blockquote>
<p>è¿‘æœŸå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰çš„è¿›æ­¥æ˜¾è‘—æå‡äº†å…¶æ•´åˆè§†è§‰å’Œè¯­è¨€ä¿¡æ¯çš„èƒ½åŠ›ï¼Œåœ¨ç›®æ ‡è¯†åˆ«ã€æ ‡é¢˜æè¿°å’Œè§†è§‰é—®ç­”ç­‰ä»»åŠ¡ä¸­å‡ ä¹è¾¾åˆ°äº†äººç±»æ°´å¹³ã€‚ç„¶è€Œï¼Œå½“å‰çš„åŸºå‡†æµ‹è¯•é€šå¸¸ä¾§é‡äºä»¥çŸ¥è¯†ä¸ºä¸­å¿ƒçš„è¯„ä»·ï¼Œè¯„ä¼°ç‰¹å®šé¢†åŸŸçš„ä¸“ä¸šçŸ¥è¯†ï¼Œå¾€å¾€å¿½è§†å¯¹åŸºæœ¬æ•°å­¦å…ƒç´ å’Œè§†è§‰æ¦‚å¿µçš„ç†è§£æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬å‘ç°è¯„ä¼°åˆçº§æ•°å­¦é—®é¢˜çš„å·®è·ï¼Œè¿™äº›é—®é¢˜ä¾èµ–äºæ˜ç¡®çš„è§†è§‰ä¾èµ–æ€§ï¼Œè¦æ±‚æ¨¡å‹åœ¨å¤šä¸ªå›¾åƒä¹‹é—´è¿›è¡Œè¾¨åˆ«ã€æ•´åˆå’Œæ¨ç†ï¼ŒåŒæ—¶ç»“åˆå¸¸è¯†çŸ¥è¯†ï¼Œè¿™äº›éƒ½æ˜¯æœç€æ›´å¹¿æ³›çš„AGIèƒ½åŠ›å‘å±•æ‰€å¿…éœ€çš„ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†VCBENCHï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«æ˜ç¡®è§†è§‰ä¾èµ–æ€§çš„å¤šæ¨¡æ€æ•°å­¦æ¨ç†çš„ç»¼åˆåŸºå‡†æµ‹è¯•ã€‚VCBENCHåŒ…å«1720ä¸ªé—®é¢˜ï¼Œè·¨è¶Šå…­ä¸ªè®¤çŸ¥é¢†åŸŸï¼ŒåŒ…å«6697å¼ å›¾åƒï¼ˆå¹³å‡æ¯é¢˜3.9å¼ ï¼‰ï¼Œä»¥ç¡®ä¿å¤šå›¾åƒæ¨ç†ã€‚æˆ‘ä»¬åœ¨VCBENCHä¸Šè¯„ä¼°äº†26ä¸ªæœ€æ–°çš„LVLMsï¼Œç»“æœæ˜¾ç¤ºæ€§èƒ½å­˜åœ¨å¾ˆå¤§å·®å¼‚ï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„æ¨¡å‹ä¹Ÿæ— æ³•è¶…è¿‡50%çš„å‡†ç¡®ç‡ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœå¼ºè°ƒäº†è§†è§‰æ•°å­¦æ•´åˆçš„æŒç»­æŒ‘æˆ˜ï¼Œå¹¶ä¸ºæœªæ¥çš„LVLMå‘å±•æä¾›äº†å»ºè®®ã€‚è¯¥é¡¹ç›®å¯åœ¨<a target="_blank" rel="noopener" href="https://alibaba-damo-academy.github.io/VCBench/%E6%89%BE%E5%88%B0%E3%80%82">https://alibaba-damo-academy.github.io/VCBench/æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.18589v3">PDF</a> Home page: <a target="_blank" rel="noopener" href="https://alibaba-damo-academy.github.io/VCBench/">https://alibaba-damo-academy.github.io/VCBench/</a></p>
<p><strong>Summary</strong>ï¼šè¿‘æœŸå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰çš„è¿›æ­¥æ˜¾è‘—æå‡äº†å…¶æ•´åˆè§†è§‰å’Œè¯­è¨€ä¿¡æ¯çš„èƒ½åŠ›ï¼Œå¹¶åœ¨å¯¹è±¡è¯†åˆ«ã€æè¿°å’Œè§†è§‰é—®ç­”ç­‰ä»»åŠ¡ä¸Šæ¥è¿‘äººç±»æ°´å¹³ã€‚ç„¶è€Œï¼Œå½“å‰è¯„ä¼°é€šå¸¸ä¾§é‡äºä»¥çŸ¥è¯†ä¸ºä¸­å¿ƒçš„è¯„ä¼°ï¼Œå¿½è§†äº†å¯¹åŸºæœ¬æ•°å­¦å…ƒç´ å’Œè§†è§‰æ¦‚å¿µçš„ç†è§£èƒ½åŠ›è¯„ä¼°ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºVCBENCHåŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«è·¨å¤šä¸ªè®¤çŸ¥é¢†åŸŸçš„æ•°å­¦é—®é¢˜å’Œå›¾åƒï¼Œä»¥è¯„ä¼°æ¨¡å‹åœ¨å¤šæ¨¡æ€æ•°å­¦æ¨ç†æ–¹é¢çš„è§†è§‰ä¾èµ–æ€§ã€‚è¯„ä¼°ç»“æœæ­ç¤ºï¼Œç°æœ‰é¡¶çº§LVLMsåœ¨è¯¥åŸºå‡†æµ‹è¯•ä¸Šçš„å‡†ç¡®ç‡ä»ä½äºä¸€åŠï¼Œè¡¨æ˜è§†è§‰æ•°å­¦æ•´åˆçš„æŒ‘æˆ˜ä»å¾…è§£å†³ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨è§†è§‰å’Œè¯­è¨€æ•´åˆæ–¹é¢å–å¾—æ˜¾è‘—è¿›æ­¥ã€‚</li>
<li>å½“å‰è¯„ä¼°ä¸»è¦ä¾§é‡äºçŸ¥è¯†é¢†åŸŸï¼Œå¿½è§†äº†å¯¹åŸºæœ¬æ•°å­¦å…ƒç´ å’Œè§†è§‰æ¦‚å¿µçš„æ¨ç†èƒ½åŠ›è¯„ä¼°ã€‚</li>
<li>æ–‡ä¸­å¼ºè°ƒäº†è§†è§‰ä¾èµ–æ€§åœ¨å¤šæ¨¡æ€æ•°å­¦æ¨ç†ä¸­çš„é‡è¦æ€§ã€‚</li>
<li>æå‡ºçš„VCBENCHåŸºå‡†æµ‹è¯•åŒ…å«å¤šè®¤çŸ¥é¢†åŸŸçš„æ•°å­¦é—®é¢˜å’Œå›¾åƒã€‚</li>
<li>ç°æœ‰é¡¶çº§LVLMsåœ¨VCBENCHä¸Šçš„å‡†ç¡®ç‡ä½äºä¸€åŠã€‚</li>
<li>è§†è§‰æ•°å­¦æ•´åˆä»å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.18589">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cad8fd1e6edee7a919455b7d86575ce6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-03ea371b038e74aa3cd6a793fe33ec4d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9380724875fc15624dbee3f518b4735c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6654ddd876af52433f6052eae10aef88.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-66bf577636fcf2af14e58ae27eda5103.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fa8e13171deec1a16b44eccce8c7c9f7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-86cb957a0f3f6d83baea66c368d183a0.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Pushing-the-boundary-on-Natural-Language-Inference"><a href="#Pushing-the-boundary-on-Natural-Language-Inference" class="headerlink" title="Pushing the boundary on Natural Language Inference"></a>Pushing the boundary on Natural Language Inference</h2><p><strong>Authors:Pablo Miralles-GonzÃ¡lez, Javier Huertas-Tato, Alejandro MartÃ­n, David Camacho</strong></p>
<p>Natural Language Inference (NLI) is a central task in natural language understanding with applications in fact-checking, question answering, and information retrieval. Despite its importance, current NLI systems heavily rely on supervised learning with datasets that often contain annotation artifacts and biases, limiting generalization and real-world applicability. In this work, we apply a reinforcement learning-based approach using Group Relative Policy Optimization (GRPO) for Chain-of-Thought (CoT) learning in NLI, eliminating the need for labeled rationales and enabling this type of training on more challenging datasets such as ANLI. We fine-tune 7B, 14B, and 32B language models using parameter-efficient techniques (LoRA and QLoRA), demonstrating strong performance across standard and adversarial NLI benchmarks. Our 32B AWQ-quantized model surpasses state-of-the-art results on 7 out of 11 adversarial sets$\unicode{x2013}$or on all of them considering our replication$\unicode{x2013}$within a 22GB memory footprint, showing that robust reasoning can be retained under aggressive quantization. This work provides a scalable and practical framework for building robust NLI systems without sacrificing inference quality. </p>
<blockquote>
<p>è‡ªç„¶è¯­è¨€æ¨ç†ï¼ˆNLIï¼‰æ˜¯è‡ªç„¶è¯­è¨€ç†è§£ä¸­çš„ä¸€é¡¹æ ¸å¿ƒä»»åŠ¡ï¼Œåº”ç”¨äºäº‹å®æ ¸æŸ¥ã€é—®ç­”å’Œä¿¡æ¯æ£€ç´¢ã€‚å°½ç®¡å®ƒå¾ˆé‡è¦ï¼Œä½†å½“å‰çš„NLIç³»ç»Ÿä¸¥é‡ä¾èµ–äºç›‘ç£å­¦ä¹ ï¼Œè€Œæ•°æ®é›†é€šå¸¸åŒ…å«æ³¨é‡Šä¼ªè¿¹å’Œåè§ï¼Œè¿™é™åˆ¶äº†å…¶æ¨å¹¿å’Œç°å®ä¸–ç•Œçš„åº”ç”¨ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é‡‡ç”¨åŸºäºå¼ºåŒ–å­¦ä¹ çš„Group Relative Policy Optimizationï¼ˆGRPOï¼‰æ–¹æ³•ï¼Œåº”ç”¨äºè‡ªç„¶è¯­è¨€æ¨ç†ä¸­çš„æ€ç»´é“¾ï¼ˆChain-of-Thoughtï¼‰ï¼ˆCoTï¼‰å­¦ä¹ ï¼Œæ¶ˆé™¤äº†å¯¹æ ‡è®°ç†æ®çš„éœ€æ±‚ï¼Œä½¿è¿™ç§ç±»å‹çš„è®­ç»ƒèƒ½å¤Ÿåœ¨æ›´å…·æŒ‘æˆ˜æ€§çš„æ•°æ®é›†ï¼ˆå¦‚ANLIï¼‰ä¸Šè¿›è¡Œã€‚æˆ‘ä»¬ä½¿ç”¨å‚æ•°é«˜æ•ˆæŠ€æœ¯ï¼ˆLoRAå’ŒQLoRAï¼‰å¾®è°ƒäº†7Bã€14Bå’Œ32Bçš„è¯­è¨€æ¨¡å‹ï¼Œåœ¨æ ‡å‡†å’Œå¯¹æŠ—æ€§NLIåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„32B AWQé‡åŒ–æ¨¡å‹åœ¨7ä¸ªå¯¹æŠ—é›†ä¸Šçš„è¡¨ç°è¶…è¿‡äº†æœ€å…ˆè¿›çš„æŠ€æœ¯ç»“æœâ€”â€”å¦‚æœæˆ‘ä»¬è¿›è¡Œå¤åˆ¶ï¼Œåˆ™åœ¨æ‰€æœ‰å¯¹æŠ—é›†ä¸Šçš„è¡¨ç°å‡ä¼˜äºæ­¤â€”â€”åœ¨ä»…22GBçš„å†…å­˜å ç”¨ä¸­ï¼Œè¯æ˜äº†åœ¨æ¿€è¿›é‡åŒ–ä¸‹ä»èƒ½ä¿ç•™ç¨³å¥çš„æ¨ç†èƒ½åŠ›ã€‚è¿™é¡¹å·¥ä½œæä¾›äº†ä¸€ä¸ªå¯æ‰©å±•ä¸”å®ç”¨çš„æ¡†æ¶ï¼Œç”¨äºæ„å»ºç¨³å¥çš„NLIç³»ç»Ÿï¼Œè€Œä¸ç‰ºç‰²æ¨ç†è´¨é‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.18376v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿™ç¯‡æ–‡æœ¬ä»‹ç»äº†è‡ªç„¶è¯­è¨€æ¨ç†ï¼ˆNLIï¼‰çš„é‡è¦æ€§å’Œç°æœ‰ç³»ç»Ÿçš„å±€é™ã€‚ä¸ºäº†æ”¹è¿›è¿™ä¸€é¢†åŸŸï¼Œç ”ç©¶äººå‘˜é‡‡ç”¨äº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„ç­–ç•¥ä¼˜åŒ–æ–¹æ³•ï¼Œå³Group Relative Policy Optimization (GRPO)ï¼Œç”¨äºChain-of-Thought (CoT) å­¦ä¹ ã€‚è¿™ç§æ–¹æ³•æ— éœ€æ ‡æ³¨è§£é‡Šï¼Œèƒ½åœ¨æ›´å…·æŒ‘æˆ˜æ€§çš„æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¦‚ANLIæ•°æ®é›†ã€‚é€šè¿‡å‚æ•°é«˜æ•ˆçš„å¾®è°ƒæŠ€æœ¯ï¼ˆLoRAå’ŒQLoRAï¼‰ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹å±•ç°äº†å¼ºå¤§çš„æ€§èƒ½ã€‚ç‰¹åˆ«æ˜¯ï¼Œä»–ä»¬çš„32Bé‡åŒ–æ¨¡å‹åœ¨å¤šä¸ªå¯¹æŠ—æµ‹è¯•é›†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ï¼Œå¹¶è¯æ˜äº†åœ¨æ¿€çƒˆçš„é‡åŒ–æ¡ä»¶ä¸‹ï¼Œä»ç„¶èƒ½ä¿ç•™ç¨³å¥çš„æ¨ç†èƒ½åŠ›ã€‚è¯¥ç ”ç©¶ä¸ºæ„å»ºä¸ç‰ºç‰²æ¨ç†è´¨é‡çš„ç¨³å¥NLIç³»ç»Ÿæä¾›äº†ä¸€ä¸ªå¯æ‰©å±•ä¸”å®ç”¨çš„æ¡†æ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯å…³äºæ–‡æœ¬çš„å…³é”®è§è§£ï¼š</p>
<ul>
<li>è‡ªç„¶è¯­è¨€æ¨ç†ï¼ˆNLIï¼‰æ˜¯è‡ªç„¶è¯­è¨€ç†è§£ä¸­çš„æ ¸å¿ƒä»»åŠ¡ï¼Œåœ¨äº‹å®æ ¸æŸ¥ã€é—®ç­”å’Œä¿¡æ¯æ£€ç´¢ç­‰æ–¹é¢æœ‰åº”ç”¨ã€‚ä½†ç°æœ‰çš„NLIç³»ç»Ÿä¾èµ–æ ‡æ³¨æ•°æ®è¿›è¡Œç›‘ç£å­¦ä¹ ï¼Œå­˜åœ¨æ ‡æ³¨äººå·¥äº§ç‰©å’Œåè§çš„é—®é¢˜ï¼Œé™åˆ¶äº†å…¶åœ¨ç°å®ä¸–ç•Œçš„åº”ç”¨ã€‚</li>
<li>ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œç ”ç©¶äººå‘˜æå‡ºäº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„ç­–ç•¥ä¼˜åŒ–æ–¹æ³•GRPOï¼Œå¹¶å°†å…¶åº”ç”¨äºCoTå­¦ä¹ ã€‚è¿™ç§æ–¹æ³•æ— éœ€æ ‡æ³¨è§£é‡Šï¼Œèƒ½åœ¨æ›´å…·æŒ‘æˆ˜æ€§çš„æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.18376">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-74645137b3246e3d4fa65c55b548108f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-73680a7526bf596884e3ec69175a79aa.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Chain-of-Thought-Textual-Reasoning-for-Few-shot-Temporal-Action-Localization"><a href="#Chain-of-Thought-Textual-Reasoning-for-Few-shot-Temporal-Action-Localization" class="headerlink" title="Chain-of-Thought Textual Reasoning for Few-shot Temporal Action   Localization"></a>Chain-of-Thought Textual Reasoning for Few-shot Temporal Action   Localization</h2><p><strong>Authors:Hongwei Ji, Wulian Yun, Mengshi Qi, Huadong Ma</strong></p>
<p>Traditional temporal action localization (TAL) methods rely on large amounts of detailed annotated data, whereas few-shot TAL reduces this dependence by using only a few training samples to identify unseen action categories. However, existing few-shot TAL methods typically focus solely on video-level information, neglecting textual information, which can provide valuable semantic support for the localization task. Therefore, we propose a new few-shot temporal action localization method by Chain-of-Thought textual reasoning to improve localization performance. Specifically, we design a novel few-shot learning framework that leverages textual semantic information to enhance the modelâ€™s ability to capture action commonalities and variations, which includes a semantic-aware text-visual alignment module designed to align the query and support videos at different levels. Meanwhile, to better express the temporal dependencies and causal relationships between actions at the textual level to assist action localization, we design a Chain of Thought (CoT)-like reasoning method that progressively guides the Vision Language Model (VLM) and Large Language Model (LLM) to generate CoT-like text descriptions for videos. The generated texts can capture more variance of action than visual features. We conduct extensive experiments on the publicly available ActivityNet1.3 and THUMOS14 datasets. We introduce the first dataset named Human-related Anomaly Localization and explore the application of the TAL task in human anomaly detection. The experimental results demonstrate that our proposed method significantly outperforms existing methods in single-instance and multi-instance scenarios. We will release our code, data and benchmark. </p>
<blockquote>
<p>ä¼ ç»Ÿçš„æ—¶é—´åŠ¨ä½œå®šä½ï¼ˆTALï¼‰æ–¹æ³•ä¾èµ–äºå¤§é‡çš„è¯¦ç»†æ ‡æ³¨æ•°æ®ï¼Œè€Œå°‘æ ·æœ¬TALåˆ™é€šè¿‡ä»…ä½¿ç”¨å°‘é‡çš„è®­ç»ƒæ ·æœ¬æ¥å‡å°‘è¿™ç§ä¾èµ–ï¼Œä»¥è¯†åˆ«æœªè§è¿‡çš„åŠ¨ä½œç±»åˆ«ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å°‘æ ·æœ¬TALæ–¹æ³•é€šå¸¸åªå…³æ³¨è§†é¢‘çº§åˆ«çš„ä¿¡æ¯ï¼Œå¿½ç•¥äº†æ–‡æœ¬ä¿¡æ¯ï¼Œè¿™å¯¹äºå®šä½ä»»åŠ¡å¯ä»¥æä¾›æœ‰ä»·å€¼çš„è¯­ä¹‰æ”¯æŒã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„åŸºäºæ€è€ƒé“¾æ–‡æœ¬æ¨ç†çš„å°‘æ ·æœ¬æ—¶é—´åŠ¨ä½œå®šä½æ–¹æ³•ï¼Œä»¥æé«˜å®šä½æ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§æ–°é¢–çš„åŸºäºæ–‡æœ¬è¯­ä¹‰ä¿¡æ¯çš„å°‘æ ·æœ¬å­¦ä¹ æ¡†æ¶ï¼Œä»¥æé«˜æ¨¡å‹æ•æ‰åŠ¨ä½œå…±æ€§å’Œå˜åŒ–çš„èƒ½åŠ›ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬ä¸€ä¸ªè¯­ä¹‰æ„ŸçŸ¥çš„æ–‡æœ¬è§†è§‰å¯¹é½æ¨¡å—ï¼Œæ—¨åœ¨ä»¥ä¸åŒå±‚çº§å¯¹é½æŸ¥è¯¢å’Œæ”¯æŒè§†é¢‘ã€‚åŒæ—¶ï¼Œä¸ºäº†æ›´å¥½åœ°åœ¨æ–‡æœ¬å±‚é¢è¡¨è¾¾åŠ¨ä½œçš„æ—¶ç©ºä¾èµ–å…³ç³»å’Œå› æœå…³ç³»ä»¥è¾…åŠ©åŠ¨ä½œå®šä½ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§ç±»ä¼¼æ€è€ƒé“¾ï¼ˆChain of Thoughtï¼Œç®€ç§°CoTï¼‰çš„æ¨ç†æ–¹æ³•ï¼Œé€æ­¥å¼•å¯¼è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆé’ˆå¯¹è§†é¢‘çš„ç±»ä¼¼CoTçš„æ–‡æœ¬æè¿°ã€‚ç”Ÿæˆçš„æ–‡æœ¬èƒ½å¤Ÿæ•æ‰åˆ°æ¯”è§†è§‰ç‰¹å¾æ›´å¤šçš„åŠ¨ä½œå˜åŒ–ã€‚æˆ‘ä»¬åœ¨å…¬å¼€å¯ç”¨çš„ActivityNet1.3å’ŒTHUMOS14æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†åä¸ºHuman-related Anomaly Localizationçš„æ–°æ•°æ®é›†ï¼Œå¹¶æ¢ç´¢äº†TALä»»åŠ¡åœ¨äººç±»å¼‚å¸¸æ£€æµ‹ä¸­çš„åº”ç”¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å•å®ä¾‹å’Œå¤šå®ä¾‹åœºæ™¯ä¸­å‡æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚æˆ‘ä»¬å°†å‘å¸ƒæˆ‘ä»¬çš„ä»£ç ã€æ•°æ®å’ŒåŸºå‡†æµ‹è¯•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13460v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºChain-of-Thoughtæ–‡æœ¬æ¨ç†çš„å°‘æ ·æœ¬æ—¶åºåŠ¨ä½œå®šä½æ–¹æ³•ã€‚è¯¥æ–¹æ³•åˆ©ç”¨æ–‡æœ¬è¯­ä¹‰ä¿¡æ¯æé«˜æ¨¡å‹æ•æ‰åŠ¨ä½œå…±æ€§å’Œå˜åŒ–çš„èƒ½åŠ›ï¼Œè®¾è®¡äº†è¯­ä¹‰æ„ŸçŸ¥çš„æ–‡æœ¬è§†è§‰å¯¹é½æ¨¡å—ï¼Œä»¥åœ¨æŸ¥è¯¢å’Œæ”¯æŒè§†é¢‘çš„ä¸åŒå±‚æ¬¡ä¸Šè¿›è¡Œå¯¹é½ã€‚åŒæ—¶ï¼Œä¸ºäº†æ›´å¥½åœ°åœ¨æ–‡æœ¬å±‚é¢è¡¨è¾¾åŠ¨ä½œé—´çš„æ—¶åºä¾èµ–å’Œå› æœå…³ç³»ï¼Œè®¾è®¡äº†ä¸€ç§ç±»ä¼¼äºChain of Thoughtï¼ˆCoTï¼‰çš„æ¨ç†æ–¹æ³•ï¼Œé€æ­¥å¼•å¯¼è§†è§‰è¯­è¨€æ¨¡å‹å’Œå¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆé’ˆå¯¹è§†é¢‘çš„CoTæ–‡æœ¬æè¿°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å•å®ä¾‹å’Œå¤šå®ä¾‹åœºæ™¯ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§åŸºäºChain-of-Thoughtæ–‡æœ¬æ¨ç†çš„å°‘æ ·æœ¬æ—¶åºåŠ¨ä½œå®šä½æ–¹æ³•ï¼Œå‡å°‘äº†å¯¹å¤§é‡è¯¦ç»†æ³¨é‡Šæ•°æ®çš„ä¾èµ–ã€‚</li>
<li>å¼•å…¥äº†æ–‡æœ¬è¯­ä¹‰ä¿¡æ¯æ¥æé«˜æ¨¡å‹æ•æ‰åŠ¨ä½œå…±æ€§å’Œå˜åŒ–çš„èƒ½åŠ›ã€‚</li>
<li>è®¾è®¡äº†è¯­ä¹‰æ„ŸçŸ¥çš„æ–‡æœ¬è§†è§‰å¯¹é½æ¨¡å—ï¼Œå®ç°æŸ¥è¯¢å’Œæ”¯æŒè§†é¢‘çš„ä¸åŒå±‚æ¬¡å¯¹é½ã€‚</li>
<li>é€šè¿‡Chain of Thoughtï¼ˆCoTï¼‰æ¨ç†æ–¹æ³•ï¼Œæ›´å¥½åœ°åœ¨æ–‡æœ¬å±‚é¢è¡¨è¾¾åŠ¨ä½œçš„æ—¶åºä¾èµ–å’Œå› æœå…³ç³»ã€‚</li>
<li>ç”Ÿæˆäº†é’ˆå¯¹è§†é¢‘çš„CoTæ–‡æœ¬æè¿°ï¼Œèƒ½æ•æ‰æ¯”è§†è§‰ç‰¹å¾æ›´ä¸°å¯Œçš„åŠ¨ä½œå˜åŒ–ã€‚</li>
<li>åœ¨ActivityNet1.3å’ŒTHUMOS14å…¬å¼€æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›å®éªŒï¼ŒéªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13460">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3c28b4d56e719be49e7b23beb9e51bb4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1a64c1df0179a3d48520ff1d1aed0ddc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9dc5ab6a2751031974916ba1a71c7ffd.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="VL-Rethinker-Incentivizing-Self-Reflection-of-Vision-Language-Models-with-Reinforcement-Learning"><a href="#VL-Rethinker-Incentivizing-Self-Reflection-of-Vision-Language-Models-with-Reinforcement-Learning" class="headerlink" title="VL-Rethinker: Incentivizing Self-Reflection of Vision-Language Models   with Reinforcement Learning"></a>VL-Rethinker: Incentivizing Self-Reflection of Vision-Language Models   with Reinforcement Learning</h2><p><strong>Authors:Haozhe Wang, Chao Qu, Zuming Huang, Wei Chu, Fangzhen Lin, Wenhu Chen</strong></p>
<p>Recently, slow-thinking systems like GPT-o1 and DeepSeek-R1 have demonstrated great potential in solving challenging problems through explicit reflection. They significantly outperform the best fast-thinking models, such as GPT-4o, on various math and science benchmarks. However, their multimodal reasoning capabilities remain on par with fast-thinking models. For instance, GPT-o1â€™s performance on benchmarks like MathVista, MathVerse, and MathVision is similar to fast-thinking models. In this paper, we aim to enhance the slow-thinking capabilities of vision-language models using reinforcement learning (without relying on distillation) to advance the state of the art. First, we adapt the GRPO algorithm with a novel technique called Selective Sample Replay (SSR) to address the vanishing advantages problem. While this approach yields strong performance, the resulting RL-trained models exhibit limited self-reflection or self-verification. To further encourage slow-thinking, we introduce Forced Rethinking, which appends a rethinking trigger token to the end of rollouts in RL training, explicitly enforcing a self-reflection reasoning step. By combining these two techniques, our model, VL-Rethinker, advances state-of-the-art scores on MathVista, MathVerse to achieve 80.4%, 63.5% respectively. VL-Rethinker also achieves open-source SoTA on multi-disciplinary benchmarks such as MathVision, MMMU-Pro, EMMA, and MEGA-Bench, narrowing the gap with OpenAI-o1. Our empirical results show the effectiveness of our approaches. </p>
<blockquote>
<p>æœ€è¿‘ï¼ŒåƒGPT-o1å’ŒDeepSeek-R1è¿™æ ·çš„æ…¢æ€è€ƒç³»ç»Ÿå·²ç»è¡¨ç°å‡ºé€šè¿‡æ˜ç¡®çš„åæ€æ¥è§£å†³å…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚åœ¨å„ç§æ•°å­¦å’Œç§‘å­¦åŸºå‡†æµ‹è¯•ä¸­ï¼Œå®ƒä»¬çš„è¡¨ç°è¿œè¶…æœ€å¥½çš„å¿«é€Ÿæ€è€ƒæ¨¡å‹ï¼Œå¦‚GPT-4oã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›æ–¹é¢ä¸å¿«é€Ÿæ€è€ƒæ¨¡å‹ä¸ç›¸ä¸Šä¸‹ã€‚ä¾‹å¦‚ï¼ŒGPT-o1åœ¨MathVistaã€MathVerseå’ŒMathVisionç­‰åŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°ä¸å¿«é€Ÿæ€è€ƒæ¨¡å‹ç›¸ä¼¼ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æ—¨åœ¨ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆä¸ä¾èµ–è’¸é¦ï¼‰æ¥æå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„æ…¢æ€è€ƒèƒ½åŠ›ï¼Œä»¥æ¨åŠ¨æœ€æ–°æŠ€æœ¯çš„è¿›å±•ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬é‡‡ç”¨GRPOç®—æ³•ï¼Œç»“åˆä¸€ç§åä¸ºé€‰æ‹©æ€§æ ·æœ¬å›æ”¾ï¼ˆSSRï¼‰çš„æ–°æŠ€æœ¯ï¼Œæ¥è§£å†³ä¼˜åŠ¿æ¶ˆå¤±çš„é—®é¢˜ã€‚è™½ç„¶è¿™ç§æ–¹æ³•è¡¨ç°è‰¯å¥½ï¼Œä½†å¾—åˆ°çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ¨¡å‹è¡¨ç°å‡ºæœ‰é™çš„è‡ªæˆ‘åæ€æˆ–è‡ªæˆ‘éªŒè¯èƒ½åŠ›ã€‚ä¸ºäº†è¿›ä¸€æ­¥é¼“åŠ±æ…¢æ€è€ƒï¼Œæˆ‘ä»¬å¼•å…¥äº†å¼ºåˆ¶åæ€ï¼Œé€šè¿‡åœ¨å¼ºåŒ–å­¦ä¹ çš„rolloutsç»“å°¾æ·»åŠ åæ€è§¦å‘ä»¤ç‰Œï¼Œæ˜ç¡®æ‰§è¡Œè‡ªæˆ‘åæ€æ¨ç†æ­¥éª¤ã€‚é€šè¿‡ç»“åˆè¿™ä¸¤ç§æŠ€æœ¯ï¼Œæˆ‘ä»¬çš„æ¨¡å‹VL-Rethinkeråœ¨MathVistaã€MathVerseä¸Šçš„è¡¨ç°è¾¾åˆ°äº†æœ€æ–°çš„æ°´å¹³ï¼Œåˆ†åˆ«è¾¾åˆ°äº†80.4%å’Œ63.5%ã€‚VL-Rethinkeråœ¨å¤šå­¦ç§‘åŸºå‡†æµ‹è¯•ï¼ˆå¦‚MathVisionã€MMMU-Proã€EMMAå’ŒMEGA-Benchï¼‰ä¸Šä¹Ÿè¾¾åˆ°äº†å¼€æºçš„é¡¶å°–æ°´å¹³ï¼Œç¼©å°äº†ä¸OpenAI-o1çš„å·®è·ã€‚æˆ‘ä»¬çš„å®è¯ç»“æœè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.08837v2">PDF</a> submitted to NeurIPS</p>
<p><strong>Summary</strong></p>
<p>è¿™ç¯‡è®ºæ–‡æ—¨åœ¨æå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„æ…¢æ€è€ƒèƒ½åŠ›ï¼Œé€šè¿‡ä½¿ç”¨å¼ºåŒ–å­¦ä¹ è€Œéè’¸é¦æŠ€æœ¯æ¥å®ç°ã€‚æ–‡ç« ä»‹ç»äº†å¦‚ä½•é€šè¿‡æ”¹è¿›GRPOç®—æ³•å¹¶ç»“åˆé€‰æ‹©æ€§æ ·æœ¬å›æ”¾ï¼ˆSSRï¼‰æŠ€æœ¯æ¥è§£å†³ä¼˜åŠ¿æ¶ˆå¤±é—®é¢˜ã€‚åŒæ—¶ï¼Œå¼•å…¥å¼ºåˆ¶åæ€æœºåˆ¶ï¼Œé€šè¿‡åœ¨å¼ºåŒ–è®­ç»ƒè¿‡ç¨‹ä¸­æ·»åŠ åæ€è§¦å‘ä»¤ç‰Œï¼Œæ˜ç¡®é¼“åŠ±æ¨¡å‹è¿›è¡Œè‡ªæˆ‘åæ€ã€‚ç»“åˆè¿™ä¸¤ç§æŠ€æœ¯ï¼Œè®ºæ–‡æå‡ºçš„VL-Rethinkeræ¨¡å‹åœ¨MathVistaã€MathVerseç­‰æ•°å­¦åŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†å…ˆè¿›æˆç»©ï¼Œå¹¶åœ¨å¤šå­¦ç§‘çš„åŸºå‡†æµ‹è¯•ä¸­ç¼©å°äº†ä¸OpenAI-o1çš„å·®è·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®ºæ–‡å…³æ³¨æå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„æ…¢æ€è€ƒèƒ½åŠ›ï¼Œé‡‡ç”¨å¼ºåŒ–å­¦ä¹ æŠ€æœ¯å®ç°ã€‚</li>
<li>é€šè¿‡æ”¹è¿›GRPOç®—æ³•å¹¶ç»“åˆé€‰æ‹©æ€§æ ·æœ¬å›æ”¾ï¼ˆSSRï¼‰è§£å†³ä¼˜åŠ¿æ¶ˆå¤±é—®é¢˜ã€‚</li>
<li>å¼•å…¥å¼ºåˆ¶åæ€æœºåˆ¶ï¼Œé¼“åŠ±æ¨¡å‹è¿›è¡Œè‡ªæˆ‘åæ€ã€‚</li>
<li>VL-Rethinkeræ¨¡å‹åœ¨å¤šä¸ªæ•°å­¦åŸºå‡†æµ‹è¯•ä¸­å–å¾—å…ˆè¿›æˆç»©ã€‚</li>
<li>VL-Rethinkeråœ¨å¤šå­¦ç§‘åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°æ¥è¿‘OpenAI-o1ã€‚</li>
<li>è®ºæ–‡é€šè¿‡å®è¯ç ”ç©¶éªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.08837">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0cf115f65e695d16b6d53275002744b2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2f6e398845f24a991ad9b3ceb9ff28de.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-17a73e965d07f8fa2171bfd6ebc521ee.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="SEAL-Steerable-Reasoning-Calibration-of-Large-Language-Models-for-Free"><a href="#SEAL-Steerable-Reasoning-Calibration-of-Large-Language-Models-for-Free" class="headerlink" title="SEAL: Steerable Reasoning Calibration of Large Language Models for Free"></a>SEAL: Steerable Reasoning Calibration of Large Language Models for Free</h2><p><strong>Authors:Runjin Chen, Zhenyu Zhang, Junyuan Hong, Souvik Kundu, Zhangyang Wang</strong></p>
<p>Large Language Models (LLMs), such as OpenAIâ€™s o1-series have demonstrated compelling capabilities for complex reasoning tasks via the extended chain-of-thought (CoT) reasoning mechanism. However, recent studies reveal substantial redundancy in the CoT reasoning traces, which not only increases inference latency but also negatively impacts model performance by diverting attention to unnecessary reasoning paths. To address this issue, we investigate the internal reasoning structures of LLMs and categorize them into three primary thought types: execution, reflection, and transition thoughts. Moreover, our analysis reveals that excessive reflection and transition thoughts are strongly correlated with failure cases and these thought categories exhibit clear separation in the latent space. Based on these, we introduce SEAL (Steerable reasoning calibration), a training-free approach that seamlessly calibrates the CoT process, improving accuracy while demonstrating significant efficiency gains. SEAL consists of an offline stage for extracting the reasoning steering vector in the latent space, followed by an on-the-fly calibration of the reasoning trace through representation intervention using the steering vector. Notably, the steering vector exhibits strong transferability across various tasks. Extensive experiments across multiple models (DeepSeek-R1-Distill and QwQ-32B-Preview) and benchmarks (Math500, GSM8K, LiveCodeBench) validate the effectiveness of SEAL, up to a 11% improvement in accuracy while reducing reasoning tokens by 11.8% to 50.4%. Our code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/VITA-Group/SEAL">https://github.com/VITA-Group/SEAL</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œå¦‚OpenAIçš„o1ç³»åˆ—ï¼Œå·²é€šè¿‡æ‰©å±•çš„é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†æœºåˆ¶å±•ç°å‡ºå¤„ç†å¤æ‚æ¨ç†ä»»åŠ¡çš„å¼ºå¤§èƒ½åŠ›ã€‚ç„¶è€Œï¼Œæœ€è¿‘çš„ç ”ç©¶æ˜¾ç¤ºCoTæ¨ç†è½¨è¿¹ä¸­å­˜åœ¨å¤§é‡å†—ä½™ï¼Œè¿™ä¸ä»…å¢åŠ äº†æ¨ç†å»¶è¿Ÿï¼Œè¿˜ä¼šé€šè¿‡å¼•å¯¼æ³¨æ„åŠ›èµ°å‘ä¸å¿…è¦çš„æ¨ç†è·¯å¾„æ¥è´Ÿé¢å½±å“æ¨¡å‹æ€§èƒ½ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬ç ”ç©¶äº†LLMçš„å†…éƒ¨æ¨ç†ç»“æ„ï¼Œå¹¶å°†å…¶ä¸»è¦åˆ’åˆ†ä¸ºä¸‰ç§æ€ç»´ç±»å‹ï¼šæ‰§è¡Œæ€ç»´ã€åæ€æ€ç»´å’Œè¿‡æ¸¡æ€ç»´ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œè¿‡åº¦çš„åæ€å’Œè¿‡æ¸¡æ€ç»´ä¸å¤±è´¥æ¡ˆä¾‹å¼ºçƒˆç›¸å…³ï¼Œè¿™äº›æ€ç»´ç±»å‹åœ¨æ½œåœ¨ç©ºé—´ä¸­æœ‰æ˜æ˜¾çš„åˆ†éš”ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†SEALï¼ˆå¯æ§åˆ¶æ¨ç†æ ¡å‡†ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„æ–¹æ³•ï¼Œå¯ä»¥æ— ç¼æ ¡å‡†CoTè¿‡ç¨‹ï¼Œæé«˜å‡†ç¡®æ€§ï¼ŒåŒæ—¶å®ç°æ˜¾è‘—çš„æ•ˆç‡æå‡ã€‚SEALåŒ…æ‹¬ä¸€ä¸ªç¦»çº¿é˜¶æ®µï¼Œç”¨äºåœ¨æ½œåœ¨ç©ºé—´ä¸­æå–æ¨ç†å¯¼å‘å‘é‡ï¼Œç„¶åå®æ—¶æ ¡å‡†æ¨ç†è½¨è¿¹ï¼Œé€šè¿‡è¡¨ç¤ºå¹²é¢„ä½¿ç”¨å¯¼å‘å‘é‡ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå¯¼å‘å‘é‡åœ¨å„ç§ä»»åŠ¡ä¹‹é—´è¡¨ç°å‡ºå¼ºå¤§çš„å¯è¿ç§»æ€§ã€‚åœ¨å¤šä¸ªæ¨¡å‹ï¼ˆDeepSeek-R1-Distillå’ŒQwQ-32B-Previewï¼‰å’ŒåŸºå‡†æµ‹è¯•ï¼ˆMath500ã€GSM8Kã€LiveCodeBenchï¼‰ä¸Šè¿›è¡Œçš„å¤§é‡å®éªŒéªŒè¯äº†SEALçš„æœ‰æ•ˆæ€§ï¼Œå…¶å‡†ç¡®æ€§æé«˜äº†é«˜è¾¾11%ï¼ŒåŒæ—¶å‡å°‘æ¨ç†ä»¤ç‰Œé«˜è¾¾11.8%è‡³50.4%ã€‚æˆ‘ä»¬çš„ä»£ç å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/VITA-Group/SEAL%E3%80%82">https://github.com/VITA-Group/SEALã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07986v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¦‚OpenAIçš„o1ç³»åˆ—é€šè¿‡æ‰©å±•é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†æœºåˆ¶å±•ç°å‡ºå¤„ç†å¤æ‚æ¨ç†ä»»åŠ¡çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç ”ç©¶å‘ç°CoTæ¨ç†è½¨è¿¹å­˜åœ¨å¤§é‡å†—ä½™ï¼Œä¸ä»…å¢åŠ æ¨ç†å»¶è¿Ÿï¼Œè¿˜å½±å“æ¨¡å‹æ€§èƒ½ã€‚æœ¬æ–‡åˆ†æäº†LLMçš„å†…éƒ¨æ¨ç†ç»“æ„ï¼Œå°†æ€ç»´åˆ†ä¸ºæ‰§è¡Œã€åæ€å’Œè¿‡æ¸¡ä¸‰ç±»ã€‚é€šè¿‡è¯†åˆ«ä¸å¤±è´¥æ¡ˆä¾‹ç›¸å…³çš„è¿‡åº¦åæ€å’Œè¿‡æ¸¡æ€ç»´ï¼Œå¹¶å‘ç°å®ƒä»¬åœ¨æ½œåœ¨ç©ºé—´ä¸­çš„æ˜ç¡®åˆ†ç¦»ï¼Œæå‡ºäº†SEALï¼ˆå¯æ§åˆ¶çš„æ¨ç†æ ¡å‡†ï¼‰æ–¹æ³•ã€‚SEALæ˜¯ä¸€ç§æ— éœ€è®­ç»ƒå³å¯æ ¡å‡†CoTè¿‡ç¨‹çš„æ–¹æ³•ï¼Œé€šè¿‡ç¦»çº¿é˜¶æ®µæå–æ¨ç†å¯¼å‘å‘é‡ï¼Œå®æ—¶æ ¡å‡†æ¨ç†è½¨è¿¹ã€‚å®éªŒè¯æ˜ï¼ŒSEALèƒ½æœ‰æ•ˆæé«˜å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¦‚OpenAIçš„o1ç³»åˆ—åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ã€‚</li>
<li>é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†æœºåˆ¶å­˜åœ¨å†—ä½™ï¼Œå½±å“æ¨¡å‹æ€§èƒ½å’Œæ¨ç†æ•ˆç‡ã€‚</li>
<li>LLMçš„å†…éƒ¨æ¨ç†ç»“æ„å¯åˆ†ä¸ºæ‰§è¡Œã€åæ€å’Œè¿‡æ¸¡ä¸‰ç§ä¸»è¦æ€ç»´ç±»å‹ã€‚</li>
<li>è¿‡åº¦åæ€å’Œè¿‡æ¸¡æ€ç»´ä¸å¤±è´¥æ¡ˆä¾‹é«˜åº¦ç›¸å…³ï¼Œåœ¨æ½œåœ¨ç©ºé—´ä¸­å­˜åœ¨æ˜ç¡®åˆ†ç¦»ã€‚</li>
<li>æå‡ºäº†SEALæ–¹æ³•ï¼Œä¸€ç§æ— éœ€è®­ç»ƒå³å¯æ ¡å‡†CoTè¿‡ç¨‹çš„æ ¡å‡†æ–¹æ³•ï¼Œæé«˜å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚</li>
<li>SEALé€šè¿‡æå–æ¨ç†å¯¼å‘å‘é‡å¹¶å®æ—¶æ ¡å‡†æ¨ç†è½¨è¿¹æ¥å®ç°æ€§èƒ½æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07986">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-496e7b2f7456cb2acf3e7b673cc5e533.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6087d6b0f1f3cb968e3df908a12184f9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e6233799e4810730025887af32869fc4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-46cae7169c9615ffcd70b2902c2744d9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-492fd62789577d2f26908e57a9503b2b.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="HAIR-Hardness-Aware-Inverse-Reinforcement-Learning-with-Introspective-Reasoning-for-LLM-Alignment"><a href="#HAIR-Hardness-Aware-Inverse-Reinforcement-Learning-with-Introspective-Reasoning-for-LLM-Alignment" class="headerlink" title="HAIR: Hardness-Aware Inverse Reinforcement Learning with Introspective   Reasoning for LLM Alignment"></a>HAIR: Hardness-Aware Inverse Reinforcement Learning with Introspective   Reasoning for LLM Alignment</h2><p><strong>Authors:Ruoxi Cheng, Haoxuan Ma, Weixin Wang</strong></p>
<p>The alignment of large language models (LLMs) with human values remains critical yet hindered by four key challenges: (1) scarcity of balanced safety datasets, (2) alignment tax, (3) vulnerability to jailbreak attacks due to shallow alignment, and (4) inability to dynamically adapt rewards according to task difficulty. To address these limitations, we introduce HAIR (Hardness-Aware Inverse Reinforcement Learning with Introspective Reasoning), a novel alignment approach inspired by shadow models in membership inference attacks. Our approach consists of two main components: (1) construction of a balanced safety Chain-of-Draft (CoD) dataset for seven harmful categories using structured prompts that leverage the introspective reasoning capabilities of LLMs; and (2) training of category-specific reward models with Group Relative Policy Optimization (GRPO), dynamically tuning optimization to task difficulty at both the data and model levels. Comprehensive experiments across four harmlessness and four usefulness benchmarks demonstrate that HAIR achieves state-of-the-art performance, outperforming all baseline methods in safety while maintaining high levels of usefulness. </p>
<blockquote>
<p>å¤§è¯­è¨€æ¨¡å‹ä¸äººç±»ä»·å€¼è§‚çš„åŒ¹é…ä»ç„¶è‡³å…³é‡è¦ï¼Œä½†å—åˆ°å››ä¸ªä¸»è¦æŒ‘æˆ˜çš„é™åˆ¶ï¼šï¼ˆ1ï¼‰å¹³è¡¡å®‰å…¨æ•°æ®é›†çš„ç¨€ç¼ºæ€§ï¼›ï¼ˆ2ï¼‰å¯¹é½ç¨ï¼›ï¼ˆ3ï¼‰ç”±äºæµ…å¯¹é½è€Œå®¹æ˜“å—åˆ°çªç ´æ”»å‡»ï¼›ï¼ˆ4ï¼‰æ— æ³•æ ¹æ®ä»»åŠ¡éš¾åº¦åŠ¨æ€è°ƒæ•´å¥–åŠ±ã€‚ä¸ºäº†åº”å¯¹è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†HAIRï¼ˆåŸºäºå†…åœ¨æ¨ç†çš„è€ƒè™‘éš¾åº¦é€†å‘å¼ºåŒ–å­¦ä¹ ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å—æˆå‘˜æ¨ç†æ”»å‡»ä¸­çš„å½±å­æ¨¡å‹å¯å‘çš„æ–°å‹å¯¹é½æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸»è¦åŒ…æ‹¬ä¸¤ä¸ªç»„æˆéƒ¨åˆ†ï¼šï¼ˆ1ï¼‰ä½¿ç”¨ç»“æ„åŒ–æç¤ºï¼Œæ„å»ºåŒ…å«ä¸ƒç§æœ‰å®³ç±»åˆ«çš„å¹³è¡¡å®‰å…¨è‰æ¡ˆé“¾ï¼ˆCoDï¼‰æ•°æ®é›†ï¼Œåˆ©ç”¨LLMçš„å†…åœ¨æ¨ç†èƒ½åŠ›ï¼›ï¼ˆ2ï¼‰ä½¿ç”¨ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰è®­ç»ƒç‰¹å®šç±»åˆ«çš„å¥–åŠ±æ¨¡å‹ï¼Œåœ¨æ•°æ®å’Œæ¨¡å‹å±‚é¢æ ¹æ®ä»»åŠ¡éš¾åº¦åŠ¨æ€è°ƒæ•´ä¼˜åŒ–ã€‚åœ¨å››ä¸ªæ— å®³æ€§å’Œå››ä¸ªæœ‰ç”¨æ€§åŸºå‡†æµ‹è¯•ä¸Šçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒHAIRè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨å®‰å…¨æ–¹é¢ä¼˜äºæ‰€æœ‰åŸºå‡†æ–¹æ³•ï¼ŒåŒæ—¶ä¿æŒé«˜æ°´å¹³çš„æœ‰ç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18991v2">PDF</a> The three authors contributed equally to this work</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ä¸äººç±»ä»·å€¼è§‚çš„åŒ¹é…é¢ä¸´å››ä¸ªå…³é”®æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºHAIRçš„æ–°å‹åŒ¹é…æ–¹æ³•ï¼Œé€šè¿‡æ„å»ºå¹³è¡¡çš„å®‰å…¨æ•°æ®é›†å’Œè®­ç»ƒç‰¹å®šç±»åˆ«çš„å¥–åŠ±æ¨¡å‹æ¥å®ç°åŠ¨æ€é€‚åº”ä»»åŠ¡éš¾åº¦çš„ä¼˜åŒ–ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å®‰å…¨æ€§å’Œå®ç”¨æ€§æ–¹é¢éƒ½è¾¾åˆ°äº†é¢†å…ˆæ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ä¸äººç±»ä»·å€¼è§‚çš„åŒ¹é…è‡³å…³é‡è¦ï¼Œä½†ä»é¢ä¸´å››ä¸ªä¸»è¦æŒ‘æˆ˜ã€‚</li>
<li>ç¼ºå°‘å¹³è¡¡çš„å®‰å…¨æ•°æ®é›†æ˜¯å…¶ä¸­çš„ä¸€å¤§æŒ‘æˆ˜ã€‚</li>
<li>HAIRæ–¹æ³•é€šè¿‡æ„å»ºå¹³è¡¡çš„å®‰å…¨Chain-of-Draftæ•°æ®é›†æ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œè¯¥æ•°æ®é›†åˆ©ç”¨ç»“æ„åŒ–æç¤ºæ¥åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„å†…çœæ¨ç†èƒ½åŠ›ã€‚</li>
<li>HAIRè¿˜é€šè¿‡è®­ç»ƒç‰¹å®šç±»åˆ«çš„å¥–åŠ±æ¨¡å‹ï¼Œé‡‡ç”¨Group Relative Policy Optimizationæ–¹æ³•ï¼ŒåŠ¨æ€é€‚åº”ä»»åŠ¡éš¾åº¦çš„ä¼˜åŒ–ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒHAIRåœ¨å®‰å…¨æ€§å’Œå®ç”¨æ€§æ–¹é¢éƒ½ä¼˜äºå…¶ä»–åŸºå‡†æ–¹æ³•ã€‚</li>
<li>HAIRæ–¹æ³•çš„å¼•å…¥å—åˆ°å½±å­æ¨¡å‹åœ¨æˆå‘˜æ¨æ–­æ”»å‡»ä¸­çš„å¯å‘çš„å¯å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18991">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-68a6b7ebba83fb2eb0b3433b17b89ea8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9f623c6569f3856aa402517ce429bcaf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1ed5d2d3ee8206eecd3314e332ba0ebc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-199d7f8f9b972eebe5f412b1925453c8.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-09/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-09/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-09/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-e390b395ffd3ef8a44030ead5123007d.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-09  On Path to Multimodal Generalist General-Level and General-Bench
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-09
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-08/Text-to-Motion/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-06e6530a21dbebc8afb9a187390137e8.jpg" class="responsive-img" alt="Text-to-Motion">
                        
                        <span class="card-title">Text-to-Motion</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Text-to-Motion æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-08  TSTMotion Training-free Scene-aware Text-to-motion Generation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-08
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Text-to-Motion/" class="post-category">
                                    Text-to-Motion
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Text-to-Motion/">
                        <span class="chip bg-color">Text-to-Motion</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">19758k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
