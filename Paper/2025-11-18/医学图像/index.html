<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-18  LARM A Large Articulated-Object Reconstruction Model">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-1d7606dfa305347887614c756f0bd25f')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-18
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    82 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-11-18-æ›´æ–°"><a href="#2025-11-18-æ›´æ–°" class="headerlink" title="2025-11-18 æ›´æ–°"></a>2025-11-18 æ›´æ–°</h1><h2 id="LARM-A-Large-Articulated-Object-Reconstruction-Model"><a href="#LARM-A-Large-Articulated-Object-Reconstruction-Model" class="headerlink" title="LARM: A Large Articulated-Object Reconstruction Model"></a>LARM: A Large Articulated-Object Reconstruction Model</h2><p><strong>Authors:Sylvia Yuan, Ruoxi Shi, Xinyue Wei, Xiaoshuai Zhang, Hao Su, Minghua Liu</strong></p>
<p>Modeling 3D articulated objects with realistic geometry, textures, and kinematics is essential for a wide range of applications. However, existing optimization-based reconstruction methods often require dense multi-view inputs and expensive per-instance optimization, limiting their scalability. Recent feedforward approaches offer faster alternatives but frequently produce coarse geometry, lack texture reconstruction, and rely on brittle, complex multi-stage pipelines. We introduce LARM, a unified feedforward framework that reconstructs 3D articulated objects from sparse-view images by jointly recovering detailed geometry, realistic textures, and accurate joint structures. LARM extends LVSM a recent novel view synthesis (NVS) approach for static 3D objects into the articulated setting by jointly reasoning over camera pose and articulation variation using a transformer-based architecture, enabling scalable and accurate novel view synthesis. In addition, LARM generates auxiliary outputs such as depth maps and part masks to facilitate explicit 3D mesh extraction and joint estimation. Our pipeline eliminates the need for dense supervision and supports high-fidelity reconstruction across diverse object categories. Extensive experiments demonstrate that LARM outperforms state-of-the-art methods in both novel view and state synthesis as well as 3D articulated object reconstruction, generating high-quality meshes that closely adhere to the input images. project page: <a target="_blank" rel="noopener" href="https://sylviayuan-sy.github.io/larm-site/">https://sylviayuan-sy.github.io/larm-site/</a></p>
<blockquote>
<p>å¯¹å…·æœ‰çœŸå®å‡ ä½•ã€çº¹ç†å’Œè¿åŠ¨å­¦çš„3Då…³èŠ‚å¯¹è±¡è¿›è¡Œå»ºæ¨¡å¯¹äºå„ç§åº”ç”¨è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºäºä¼˜åŒ–çš„é‡å»ºæ–¹æ³•é€šå¸¸éœ€è¦å¯†é›†çš„å¤šè§†è§’è¾“å…¥å’Œæ˜‚è´µçš„å•å®ä¾‹ä¼˜åŒ–ï¼Œè¿™é™åˆ¶äº†å…¶å¯æ‰©å±•æ€§ã€‚æœ€è¿‘çš„å‰é¦ˆæ–¹æ³•æä¾›äº†æ›´å¿«çš„æ›¿ä»£æ–¹æ¡ˆï¼Œä½†é€šå¸¸äº§ç”Ÿç²—ç³™çš„å‡ ä½•å½¢çŠ¶ï¼Œç¼ºä¹çº¹ç†é‡å»ºï¼Œå¹¶ä¸”ä¾èµ–äºè„†å¼±ã€å¤æ‚çš„å¤šé˜¶æ®µæµæ°´çº¿ã€‚æˆ‘ä»¬å¼•å…¥äº†LARMï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„å‰é¦ˆæ¡†æ¶ï¼Œå¯ä»¥ä»ç¨€ç–è§†è§’å›¾åƒé‡å»º3Då…³èŠ‚å¯¹è±¡ï¼Œé€šè¿‡è”åˆæ¢å¤è¯¦ç»†çš„å‡ ä½•å½¢çŠ¶ã€ç°å®çš„çº¹ç†å’Œç²¾ç¡®çš„å…³èŠ‚ç»“æ„ã€‚LARMå°†LVSMï¼ˆä¸€ç§é’ˆå¯¹é™æ€3Då¯¹è±¡çš„æ–°å‹è§†å›¾åˆæˆï¼ˆNVSï¼‰æ–¹æ³•ï¼‰æ‰©å±•åˆ°å…³èŠ‚è®¾å®šä¸­ï¼Œé€šè¿‡ä½¿ç”¨åŸºäºtransformerçš„æ¶æ„è”åˆæ¨ç†ç›¸æœºå§¿æ€å’Œå…³èŠ‚å˜åŒ–ï¼Œä»è€Œå®ç°å¯æ‰©å±•å’Œå‡†ç¡®çš„æ–°å‹è§†å›¾åˆæˆã€‚æ­¤å¤–ï¼ŒLARMç”Ÿæˆè¾…åŠ©è¾“å‡ºï¼Œå¦‚æ·±åº¦å›¾å’Œéƒ¨åˆ†æ©è†œï¼Œä»¥ä¿ƒè¿›æ˜ç¡®çš„3Dç½‘æ ¼æå–å’Œå…³èŠ‚ä¼°è®¡ã€‚æˆ‘ä»¬çš„æµæ°´çº¿æ¶ˆé™¤äº†å¯¹å¯†é›†ç›‘ç£çš„éœ€æ±‚ï¼Œæ”¯æŒè·¨ä¸åŒå¯¹è±¡ç±»åˆ«çš„é«˜ä¿çœŸé‡å»ºã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒLARMåœ¨æ–°å‹è§†å›¾ã€çŠ¶æ€åˆæˆä»¥åŠ3Då…³èŠ‚å¯¹è±¡é‡å»ºæ–¹é¢å‡ä¼˜äºæœ€æ–°æ–¹æ³•ï¼Œç”Ÿæˆçš„é«˜è´¨é‡ç½‘æ ¼ä¸è¾“å…¥å›¾åƒç´§å¯†è´´åˆã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://sylviayuan-sy.github.io/larm-site/">https://sylviayuan-sy.github.io/larm-site/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.11563v1">PDF</a> project page: <a target="_blank" rel="noopener" href="https://sylviayuan-sy.github.io/larm-site/">https://sylviayuan-sy.github.io/larm-site/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºLARMçš„ç»Ÿä¸€å‰é¦ˆæ¡†æ¶ï¼Œç”¨äºä»ç¨€ç–è§†è§’å›¾åƒé‡å»º3Då…³èŠ‚æ´»åŠ¨å¯¹è±¡ã€‚LARMé€šè¿‡è”åˆæ¢å¤è¯¦ç»†å‡ ä½•ã€çœŸå®çº¹ç†å’Œç²¾ç¡®å…³èŠ‚ç»“æ„ï¼Œæ‰©å±•äº†é’ˆå¯¹é™æ€3Då¯¹è±¡çš„æœ€æ–°æ–°å‹è§†å›¾åˆæˆæ–¹æ³•LVSMï¼Œè¿›å…¥å…³èŠ‚æ´»åŠ¨è®¾ç½®ã€‚LARMä½¿ç”¨åŸºäºå˜å‹å™¨çš„æ¶æ„è¿›è¡Œç›¸æœºå§¿æ€å’Œå…³èŠ‚æ´»åŠ¨å˜åŒ–çš„è”åˆæ¨ç†ï¼Œå®ç°äº†å¯æ‰©å±•å’Œå‡†ç¡®çš„æ–°å‹è§†å›¾åˆæˆã€‚æ­¤å¤–ï¼ŒLARMç”Ÿæˆæ·±åº¦å›¾å’Œéƒ¨åˆ†æ©è†œç­‰è¾…åŠ©è¾“å‡ºï¼Œä¿ƒè¿›æ˜ç¡®çš„3Dç½‘æ ¼æå–å’Œå…³èŠ‚ä¼°è®¡ã€‚è¯¥ç®¡é“æ— éœ€å¯†é›†ç›‘ç£ï¼Œæ”¯æŒä¸åŒå¯¹è±¡ç±»åˆ«çš„é«˜ä¿çœŸé‡å»ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LARMæ˜¯ä¸€ä¸ªç”¨äºä»ç¨€ç–è§†è§’å›¾åƒé‡å»º3Då…³èŠ‚æ´»åŠ¨å¯¹è±¡çš„å‰é¦ˆæ¡†æ¶ã€‚</li>
<li>LARMæ‰©å±•äº†é’ˆå¯¹é™æ€3Då¯¹è±¡çš„è§†å›¾åˆæˆæ–¹æ³•LVSMï¼Œä»¥å¤„ç†å…³èŠ‚æ´»åŠ¨å¯¹è±¡ã€‚</li>
<li>LARMé€šè¿‡è”åˆæ¢å¤è¯¦ç»†å‡ ä½•ã€çœŸå®çº¹ç†å’Œç²¾ç¡®å…³èŠ‚ç»“æ„ï¼Œæä¾›é«˜è´¨é‡çš„3Dé‡å»ºã€‚</li>
<li>LARMä½¿ç”¨åŸºäºå˜å‹å™¨çš„æ¶æ„è¿›è¡Œç›¸æœºå§¿æ€å’Œå…³èŠ‚æ´»åŠ¨çš„è”åˆæ¨ç†ã€‚</li>
<li>LARMæ”¯æŒä»ç¨€ç–è§†è§’å›¾åƒè¿›è¡Œæ–°å‹è§†å›¾åˆæˆï¼Œå…·æœ‰å¯æ‰©å±•æ€§å’Œå‡†ç¡®æ€§ã€‚</li>
<li>LARMç”Ÿæˆæ·±åº¦å›¾å’Œéƒ¨åˆ†æ©è†œç­‰è¾…åŠ©è¾“å‡ºï¼Œä¿ƒè¿›3Dç½‘æ ¼æå–å’Œå…³èŠ‚ä¼°è®¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.11563">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cad4ffe46c392ae1f6ed53e679f76164" align="middle">
<img src="https://picx.zhimg.com/v2-5aee9d4dd0728a3a3f7fd443fe44286e" align="middle">
<img src="https://picx.zhimg.com/v2-2eec0c00f283cadfede6e723caa7fc07" align="middle">
<img src="https://picx.zhimg.com/v2-7030d81058e02660bb60ed054a79a0b9" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="CVChess-A-Deep-Learning-Framework-for-Converting-Chessboard-Images-to-Forsyth-Edwards-Notation"><a href="#CVChess-A-Deep-Learning-Framework-for-Converting-Chessboard-Images-to-Forsyth-Edwards-Notation" class="headerlink" title="CVChess: A Deep Learning Framework for Converting Chessboard Images to Forsyth-Edwards Notation"></a>CVChess: A Deep Learning Framework for Converting Chessboard Images to Forsyth-Edwards Notation</h2><p><strong>Authors:Luthira Abeykoon, Ved Patel, Gawthaman Senthilvelan, Darshan Kasundra</strong></p>
<p>Chess has experienced a large increase in viewership since the pandemic, driven largely by the accessibility of online learning platforms. However, no equivalent assistance exists for physical chess games, creating a divide between analog and digital chess experiences. This paper presents CVChess, a deep learning framework for converting chessboard images to Forsyth-Edwards Notation (FEN), which is later input into online chess engines to provide you with the best next move. Our approach employs a convolutional neural network (CNN) with residual layers to perform piece recognition from smartphone camera images. The system processes RGB images of a physical chess board through a multistep process: image preprocessing using the Hough Line Transform for edge detection, projective transform to achieve a top-down board alignment, segmentation into 64 individual squares, and piece classification into 13 classes (6 unique white pieces, 6 unique black pieces and an empty square) using the residual CNN. Residual connections help retain low-level visual features while enabling deeper feature extraction, improving accuracy and stability during training. We train and evaluate our model using the Chess Recognition Dataset (ChessReD), containing 10,800 annotated smartphone images captured under diverse lighting conditions and angles. The resulting classifications are encoded as an FEN string, which can be fed into a chess engine to generate the most optimal move</p>
<blockquote>
<p>è‡ªç–«æƒ…ä»¥æ¥ï¼Œè±¡æ£‹çš„è§‚ä¼—æ•°é‡å¤§å¹…å¢åŠ ï¼Œè¿™ä¸»è¦å¾—ç›Šäºåœ¨çº¿å­¦ä¹ å¹³å°çš„å¯è®¿é—®æ€§ã€‚ç„¶è€Œï¼Œå¯¹äºå®ä½“è±¡æ£‹æ¸¸æˆï¼Œå°šæ²¡æœ‰ç±»ä¼¼çš„è¾…åŠ©å·¥å…·å­˜åœ¨ï¼Œè¿™å°±é€ æˆäº†æ¨¡æ‹Ÿè±¡æ£‹ä¸æ•°å­—è±¡æ£‹ä½“éªŒä¹‹é—´çš„é¸¿æ²Ÿã€‚æœ¬æ–‡æå‡ºäº†CVChessï¼Œè¿™æ˜¯ä¸€ä¸ªæ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºå°†æ£‹ç›˜å›¾åƒè½¬æ¢ä¸ºForsyth-Edwardsè®°å·æ³•ï¼ˆFENï¼‰ï¼Œç„¶åå°†å…¶è¾“å…¥åœ¨çº¿è±¡æ£‹å¼•æ“ï¼Œä»¥æä¾›æœ€ä½³çš„ä¸‹ä¸€æ­¥è¡ŒåŠ¨å»ºè®®ã€‚æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨å¸¦æœ‰æ®‹å·®å±‚çš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æ¥æ‰§è¡Œä»æ™ºèƒ½æ‰‹æœºæ‘„åƒå¤´å›¾åƒè¿›è¡Œæ£‹å­è¯†åˆ«ã€‚è¯¥ç³»ç»Ÿé€šè¿‡å¤šæ­¥éª¤å¤„ç†ç‰©ç†è±¡æ£‹æ¿çš„RGBå›¾åƒï¼šä½¿ç”¨éœå¤«çº¿å˜æ¢è¿›è¡Œè¾¹ç¼˜æ£€æµ‹çš„å›¾åƒé¢„å¤„ç†ï¼ŒæŠ•å½±å˜æ¢ä»¥å®ç°ä»ä¸Šåˆ°ä¸‹çš„æ£‹ç›˜å¯¹é½ï¼Œåˆ†å‰²æˆ64ä¸ªå•ç‹¬çš„æ–¹æ ¼ï¼Œå¹¶ä½¿ç”¨æ®‹å·®CNNå°†æ£‹å­åˆ†ç±»ä¸º13ç±»ï¼ˆ6ç§ç‹¬ç‰¹çš„ç™½æ£‹ï¼Œ6ç§ç‹¬ç‰¹çš„é»‘æ£‹å’Œä¸€ä¸ªç©ºæ–¹æ ¼ï¼‰ã€‚æ®‹å·®è¿æ¥æœ‰åŠ©äºä¿ç•™ä½çº§åˆ«çš„è§†è§‰ç‰¹å¾ï¼ŒåŒæ—¶å®ç°æ›´æ·±çš„ç‰¹å¾æå–ï¼Œæé«˜è®­ç»ƒå’Œé¢„æµ‹è¿‡ç¨‹ä¸­çš„å‡†ç¡®æ€§å’Œç¨³å®šæ€§ã€‚æˆ‘ä»¬ä½¿ç”¨åŒ…å«åœ¨å„ç§å…‰ç…§æ¡ä»¶å’Œè§’åº¦ä¸‹æ‹æ‘„çš„10800å¼ æ³¨é‡Šè¿‡çš„æ™ºèƒ½æ‰‹æœºå›¾åƒçš„è±¡æ£‹è¯†åˆ«æ•°æ®é›†ï¼ˆChessReDï¼‰æ¥è®­ç»ƒå’Œè¯„ä¼°æˆ‘ä»¬çš„æ¨¡å‹ã€‚æ‰€å¾—çš„åˆ†ç±»ç»“æœç¼–ç ä¸ºFENå­—ç¬¦ä¸²ï¼Œå¯ä»¥è¾“å…¥åˆ°è±¡æ£‹å¼•æ“ä¸­ï¼Œç”Ÿæˆæœ€ä¼˜è´¨çš„è¡ŒåŠ¨å»ºè®®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.11522v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ç–«æƒ…æœŸé—´ç½‘ç»œæ£‹ç‰Œæ¸¸æˆçš„è§‚ä¼—æ•°é‡æ¿€å¢ï¼Œä½†å®ä½“æ£‹ç›˜æ¸¸æˆç¼ºä¹ç›¸åº”çš„è¾…åŠ©å·¥å…·ï¼Œé€ æˆæ¨¡æ‹Ÿä¸æ•°å­—æ£‹ç‰Œä½“éªŒé—´çš„é¸¿æ²Ÿã€‚æœ¬æ–‡æå‡ºCVChessæ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œå¯å°†æ£‹ç›˜å›¾åƒè½¬æ¢ä¸ºForsyth-Edwardsè®°å·ï¼ˆFENï¼‰ï¼Œå†è¾“å…¥ç½‘ç»œæ£‹ç‰Œå¼•æ“ï¼Œç»™å‡ºæœ€ä½³ä¸‹ä¸€æ­¥è¡ŒåŠ¨ã€‚é‡‡ç”¨å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ç»“åˆæ®‹å·®å±‚è¿›è¡Œæ£‹å­è¯†åˆ«ï¼Œå¯é€šè¿‡æ™ºèƒ½æ‰‹æœºæ‘„åƒå¤´æ‹æ‘„çš„å›¾ç‰‡è¿›è¡Œã€‚ç³»ç»Ÿé€šè¿‡å¤šæ­¥éª¤å¤„ç†å›¾åƒï¼ŒåŒ…æ‹¬éœå¤«çº¿å˜æ¢è¾¹ç¼˜æ£€æµ‹ã€æŠ•å½±å˜æ¢å®ç°æ£‹ç›˜é¡¶éƒ¨å¯¹é½ã€åˆ†å‰²æˆ64ä¸ªç‹¬ç«‹æ–¹æ ¼ï¼Œå¹¶ä½¿ç”¨æ®‹å·®CNNå°†æ£‹å­åˆ†ç±»ä¸º13ç±»ã€‚æ®‹å·®è¿æ¥æœ‰åŠ©äºä¿ç•™ä½å±‚æ¬¡è§†è§‰ç‰¹å¾ï¼ŒåŒæ—¶å®ç°æ·±å±‚æ¬¡ç‰¹å¾æå–ï¼Œæé«˜è®­ç»ƒå’Œé¢„æµ‹æœŸé—´çš„å‡†ç¡®æ€§å’Œç¨³å®šæ€§ã€‚æ¨¡å‹åœ¨Chess Recognition Datasetæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒå’Œè¯„ä¼°ï¼ŒåŒ…å«10800å¼ æ ‡æ³¨çš„æ™ºèƒ½æ‰‹æœºå›¾åƒï¼Œå¯åœ¨ä¸åŒå…‰ç…§æ¡ä»¶å’Œè§’åº¦ä¸‹è¿›è¡Œæ‹æ‘„ã€‚ç»“æœç¼–ç ä¸ºFENå­—ç¬¦ä¸²ï¼Œå¯è¾“å…¥æ£‹ç‰Œå¼•æ“ç”Ÿæˆæœ€ä¼˜èµ°æ£‹æ­¥éª¤ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç–«æƒ…æœŸé—´ç½‘ç»œæ£‹ç‰Œæ¸¸æˆè§‚ä¼—å¢å¤šï¼Œå®ä½“æ£‹ç›˜æ¸¸æˆç¼ºä¹è¾…åŠ©å·¥å…·ã€‚</li>
<li>CVChessæ¡†æ¶å¯å°†æ£‹ç›˜å›¾åƒè½¬æ¢ä¸ºFENè®°å·ã€‚</li>
<li>é‡‡ç”¨CNNç»“åˆæ®‹å·®å±‚è¿›è¡Œæ£‹å­è¯†åˆ«ï¼Œå¯é€šè¿‡æ™ºèƒ½æ‰‹æœºæ‘„åƒå¤´å›¾ç‰‡è¿›è¡Œã€‚</li>
<li>ç³»ç»ŸåŒ…æ‹¬å›¾åƒé¢„å¤„ç†ã€æ£‹ç›˜å¯¹é½ã€åˆ†å‰²æˆæ–¹æ ¼å’Œæ£‹å­åˆ†ç±»ç­‰æ­¥éª¤ã€‚</li>
<li>æ®‹å·®è¿æ¥æœ‰åŠ©äºæé«˜æ¨¡å‹çš„å‡†ç¡®æ€§å’Œç¨³å®šæ€§ã€‚</li>
<li>æ¨¡å‹åœ¨Chess Recognition Datasetæ•°æ®é›†ä¸Šè®­ç»ƒå¹¶è¯„ä¼°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.11522">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-81f4495c707c1f9d9ebba10c319a418d" align="middle">
<img src="https://picx.zhimg.com/v2-01f67e97916ed97c74f6541b8f9b0e4b" align="middle">
<img src="https://picx.zhimg.com/v2-83d98356dddce15c272daac7fa3d6248" align="middle">
<img src="https://picx.zhimg.com/v2-5eed7604d85e629db88f2bfc37c8618e" align="middle">
<img src="https://picx.zhimg.com/v2-5c14674e0ec027bdf2be88497669b551" align="middle">
<img src="https://picx.zhimg.com/v2-b12897a8fb947cd87c778d2942aaa4a5" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="OpenUS-A-Fully-Open-Source-Foundation-Model-for-Ultrasound-Image-Analysis-via-Self-Adaptive-Masked-Contrastive-Learning"><a href="#OpenUS-A-Fully-Open-Source-Foundation-Model-for-Ultrasound-Image-Analysis-via-Self-Adaptive-Masked-Contrastive-Learning" class="headerlink" title="OpenUS: A Fully Open-Source Foundation Model for Ultrasound Image Analysis via Self-Adaptive Masked Contrastive Learning"></a>OpenUS: A Fully Open-Source Foundation Model for Ultrasound Image Analysis via Self-Adaptive Masked Contrastive Learning</h2><p><strong>Authors:Xiaoyu Zheng, Xu Chen, Awais Rauf, Qifan Fu, Benedetta Monosi, Felice Rivellese, Myles J. Lewis, Shaogang Gong, Gregory Slabaugh</strong></p>
<p>Ultrasound (US) is one of the most widely used medical imaging modalities, thanks to its low cost, portability, real-time feedback, and absence of ionizing radiation. However, US image interpretation remains highly operator-dependent and varies significantly across anatomical regions, acquisition protocols, and device types. These variations, along with unique challenges such as speckle, low contrast, and limited standardized annotations, hinder the development of generalizable, label-efficient ultrasound AI models. In this paper, we propose OpenUS, the first reproducible, open-source ultrasound foundation model built on a large collection of public data. OpenUS employs a vision Mamba backbone, capturing both local and global long-range dependencies across the image. To extract rich features during pre-training, we introduce a novel self-adaptive masking framework that combines contrastive learning with masked image modeling. This strategy integrates the teacherâ€™s attention map with student reconstruction loss, adaptively refining clinically-relevant masking to enhance pre-training effectiveness. OpenUS also applies a dynamic learning schedule to progressively adjust the difficulty of the pre-training process. To develop the foundation model, we compile the largest to-date public ultrasound dataset comprising over 308K images from 42 publicly available datasets, covering diverse anatomical regions, institutions, imaging devices, and disease types. Our pre-trained OpenUS model can be easily adapted to specific downstream tasks by serving as a backbone for label-efficient fine-tuning. Code is available at <a target="_blank" rel="noopener" href="https://github.com/XZheng0427/OpenUS">https://github.com/XZheng0427/OpenUS</a>.</p>
<blockquote>
<p>è¶…å£°ï¼ˆUSï¼‰æ˜¯æœ€å¸¸ç”¨çš„åŒ»å­¦æˆåƒæ–¹å¼ä¹‹ä¸€ï¼Œå¾—ç›Šäºå…¶ä½æˆæœ¬ã€ä¾¿æºæ€§ã€å®æ—¶åé¦ˆå’Œæ— ç”µç¦»è¾å°„çš„ç‰¹ç‚¹ã€‚ç„¶è€Œï¼Œè¶…å£°å›¾åƒè§£è¯»ä»ç„¶é«˜åº¦ä¾èµ–äºæ“ä½œå‘˜ï¼Œå¹¶ä¸”åœ¨ä¸åŒçš„è§£å‰–åŒºåŸŸã€é‡‡é›†åè®®å’Œè®¾å¤‡ç±»å‹ä¹‹é—´å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚è¿™äº›å·®å¼‚ï¼Œå†åŠ ä¸Šæ–‘ç‚¹ã€ä½å¯¹æ¯”åº¦å’Œç¼ºä¹æ ‡å‡†åŒ–æ³¨é‡Šç­‰ç‹¬ç‰¹æŒ‘æˆ˜ï¼Œé˜»ç¢äº†é€šç”¨ã€æ ‡ç­¾é«˜æ•ˆçš„è¶…å£°äººå·¥æ™ºèƒ½æ¨¡å‹çš„å‘å±•ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†OpenUSï¼Œè¿™æ˜¯åŸºäºå¤§é‡å…¬å…±æ•°æ®æ„å»ºçš„ã€å¯å¤ç°çš„å¼€æºè¶…å£°åŸºç¡€æ¨¡å‹ã€‚OpenUSé‡‡ç”¨è§†è§‰Mambaéª¨å¹²ç½‘ç»œï¼Œèƒ½å¤Ÿæ•æ‰å›¾åƒä¸­çš„å±€éƒ¨å’Œå…¨å±€é•¿è·ç¦»ä¾èµ–å…³ç³»ã€‚ä¸ºäº†åœ¨è¿›è¡Œé¢„è®­ç»ƒæ—¶æå–ä¸°å¯Œçš„ç‰¹å¾ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹çš„è‡ªé€‚åº”æ©ç æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†å¯¹æ¯”å­¦ä¹ ä¸æ©ç å›¾åƒå»ºæ¨¡ã€‚è¯¥ç­–ç•¥å°†æ•™å¸ˆçš„æ³¨æ„åŠ›å›¾ä¸å­¦ç”Ÿé‡å»ºæŸå¤±ç›¸ç»“åˆï¼Œè‡ªé€‚åº”åœ°ç²¾ç»†è°ƒæ•´ä¸ä¸´åºŠç›¸å…³çš„æ©ç ï¼Œä»¥æé«˜é¢„è®­ç»ƒæ•ˆæœã€‚OpenUSè¿˜é‡‡ç”¨åŠ¨æ€å­¦ä¹ æ—¶é—´è¡¨æ¥é€æ­¥è°ƒæ•´é¢„è®­ç»ƒè¿‡ç¨‹çš„éš¾åº¦ã€‚ä¸ºäº†å¼€å‘åŸºç¡€æ¨¡å‹ï¼Œæˆ‘ä»¬æ•´åˆäº†è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„å…¬å…±è¶…å£°æ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡30.8ä¸‡å¼ å›¾åƒï¼Œæ¥è‡ª42ä¸ªå…¬å¼€æ•°æ®é›†ï¼Œæ¶µç›–å¤šç§è§£å‰–åŒºåŸŸã€æœºæ„ã€æˆåƒè®¾å¤‡å’Œç–¾ç—…ç±»å‹ã€‚æˆ‘ä»¬çš„é¢„è®­ç»ƒOpenUSæ¨¡å‹å¯ä»¥å¾ˆå®¹æ˜“åœ°é€‚åº”ç‰¹å®šçš„ä¸‹æ¸¸ä»»åŠ¡ï¼Œä½œä¸ºæ ‡ç­¾é«˜æ•ˆå¾®è°ƒçš„ä¸»å¹²ã€‚ä»£ç å¯è®¿é—®äº <a target="_blank" rel="noopener" href="https://github.com/XZheng0427/OpenUS%E3%80%82">https://github.com/XZheng0427/OpenUSã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.11510v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶æå‡ºOpenUSâ€”â€”é¦–ä¸ªåŸºäºå¤§è§„æ¨¡å…¬å¼€æ•°æ®çš„å¼€æºè¶…å£°åŸºç¡€æ¨¡å‹ã€‚æ¨¡å‹ç»“åˆè§†è§‰æŠ€æœ¯å¹¶é‡‡ç”¨è‡ªé€‚åº”é®è”½æ¡†æ¶ï¼Œé€šè¿‡å¯¹æ¯”å­¦ä¹ ä¸é®ç½©å›¾åƒå»ºæ¨¡è¿›è¡Œç‰¹å¾æå–ã€‚è¯¥æ¨¡å‹åŠ¨æ€è°ƒæ•´é¢„è®­ç»ƒéš¾åº¦ï¼Œæ•´åˆäº†æ¥è‡ªå¤šä¸ªå…¬å¼€æ•°æ®é›†çš„å¤šæ ·åŒ–è¶…å£°å›¾åƒã€‚OpenUSèƒ½è½»æ¾é€‚åº”ä¸‹æ¸¸ä»»åŠ¡å¹¶æå‡æ ‡æ³¨æ•ˆç‡ã€‚ç›¸å…³ä»£ç å¯åœ¨æŒ‡å®šç½‘å€è·å–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ç ”ç©¶å¼•å…¥äº†è¶…å£°é¢†åŸŸçš„æ–°æ¨¡å‹â€”â€”OpenUSï¼Œåˆ©ç”¨å¤§é‡å…¬å¼€æ•°æ®é›†å»ºç«‹å¼€æ”¾æºç çš„æ¨¡å‹ï¼Œç®€åŒ–äº†è¯¥é¢†åŸŸAIçš„å¼€å‘ã€‚</li>
<li>OpenUSåŸºäºMambaæ¶æ„å®ç°æœ¬åœ°ä¸å…¨çƒèŒƒå›´å†…çš„é•¿æœŸä¾èµ–æ€§æ•è·ï¼Œè¿™ä¸€æŠ€æœ¯ç”¨äºæ”¹å–„å›¾åƒçš„æ•è·ä¸å¤„ç†æ•ˆç‡ã€‚</li>
<li>åˆ›æ–°æ€§ä½¿ç”¨è‡ªé€‚åº”é®è”½æ¡†æ¶ï¼ˆç»“åˆäº†å¯¹æ¯”å­¦ä¹ ä¸é®ç½©å›¾åƒå»ºæ¨¡ï¼‰ï¼Œæ—¨åœ¨æé«˜é¢„è®­ç»ƒæ•ˆæœå¹¶é€‚åº”ä¸´åºŠç›¸å…³é®è”½ã€‚</li>
<li>åŠ¨æ€å­¦ä¹ è°ƒåº¦ç­–ç•¥ç”¨äºé€æ­¥è°ƒæ•´é¢„è®­ç»ƒè¿‡ç¨‹çš„éš¾åº¦ï¼Œæå‡æ¨¡å‹çš„é€‚åº”æ€§ã€‚</li>
<li>æ¨¡å‹æ¶µç›–å¤šç§è§£å‰–å­¦åŒºåŸŸã€æœºæ„ã€æˆåƒè®¾å¤‡å’Œç–¾ç—…ç±»å‹çš„æ•°æ®é›†ç»„åˆä¸ºæœ€å¤§çš„å…¬å¼€è¶…å£°æ•°æ®é›†ï¼Œå…±è®¡è¶…è¿‡30ä¸‡å¼ å›¾åƒã€‚è¿™ä¸ºå»ºç«‹æ›´å…·æ³›åŒ–æ€§çš„æ¨¡å‹æä¾›äº†æ•°æ®åŸºç¡€ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.11510">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c284b7a4216a92a056321916567b85bc" align="middle">
<img src="https://picx.zhimg.com/v2-caa89ae7ddc9943ee032012be4f2d25b" align="middle">
<img src="https://picx.zhimg.com/v2-7faee1e08f35ac32c41636cb8e5ed604" align="middle">
<img src="https://picx.zhimg.com/v2-126b94c2a75fc776159148267c2ba61c" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Data-efficient-U-Net-for-Segmentation-of-Carbide-Microstructures-in-SEM-Images-of-Steel-Alloys"><a href="#Data-efficient-U-Net-for-Segmentation-of-Carbide-Microstructures-in-SEM-Images-of-Steel-Alloys" class="headerlink" title="Data-efficient U-Net for Segmentation of Carbide Microstructures in SEM Images of Steel Alloys"></a>Data-efficient U-Net for Segmentation of Carbide Microstructures in SEM Images of Steel Alloys</h2><p><strong>Authors:Alinda Ezgi GerÃ§ek, Till Korten, Paul Chekhonin, Maleeha Hassan, Peter Steinbach</strong></p>
<p>Understanding reactor-pressure-vessel steel microstructure is crucial for predicting mechanical properties, as carbide precipitates both strengthen the alloy and can initiate cracks. In scanning electron microscopy images, gray-value overlap between carbides and matrix makes simple thresholding ineffective. We present a data-efficient segmentation pipeline using a lightweight U-Net (30.7~M parameters) trained on just \textbf{10 annotated scanning electron microscopy images}. Despite limited data, our model achieves a \textbf{Dice-SÃ¸rensen coefficient of 0.98}, significantly outperforming the state-of-the-art in the field of metallurgy (classical image analysis: 0.85), while reducing annotation effort by one order of magnitude compared to the state-of-the-art data efficient segmentation model. This approach enables rapid, automated carbide quantification for alloy design and generalizes to other steel types, demonstrating the potential of data-efficient deep learning in reactor-pressure-vessel steel analysis.</p>
<blockquote>
<p>ç†è§£ååº”å †å‹åŠ›å®¹å™¨é’¢çš„å¾®ç»“æ„å¯¹äºé¢„æµ‹å…¶æœºæ¢°æ€§èƒ½è‡³å…³é‡è¦ï¼Œå› ä¸ºç¢³åŒ–ç‰©çš„æ²‰æ·€æ—¢èƒ½ä½¿åˆé‡‘ç¡¬åŒ–ä¹Ÿå¯èƒ½å¼•å‘è£‚çº¹ã€‚åœ¨æ‰«æç”µå­æ˜¾å¾®é•œå›¾åƒä¸­ï¼Œç¢³åŒ–ç‰©å’ŒåŸºè´¨ä¹‹é—´çš„ç°åº¦å€¼é‡å ä½¿å¾—ç®€å•çš„é˜ˆå€¼å¤„ç†æ— æ•ˆã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„æ•°æ®åˆ†å‰²ç®¡é“ï¼Œä»…ä½¿ç”¨ç»è¿‡æ ‡æ³¨çš„æ‰«æç”µå­æ˜¾å¾®é•œå›¾åƒè¿›è¡Œè®­ç»ƒçš„è½»é‡çº§U-Netï¼ˆ30.7~Må‚æ•°ï¼‰ã€‚å°½ç®¡æ•°æ®é‡æœ‰é™ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å®ç°äº†Dice-SÃ¸rensenç³»æ•°ä¸º0.98ï¼Œåœ¨å†¶é‡‘é¢†åŸŸæ˜¾è‘—ä¼˜äºæœ€å…ˆè¿›æŠ€æœ¯ï¼ˆç»å…¸å›¾åƒåˆ†æï¼š0.85ï¼‰ï¼Œå¹¶ä¸”ä¸ä¼ ç»Ÿçš„é«˜æ•ˆæ•°æ®åˆ†å‰²æ¨¡å‹ç›¸æ¯”ï¼Œå‡å°‘äº†ä¸€ä¸ªæ•°é‡çº§çš„æ ‡æ³¨å·¥ä½œé‡ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿå®ç°å¿«é€Ÿçš„è‡ªåŠ¨åŒ–ç¢³åŒ–ç‰©å®šé‡åˆ†æï¼Œç”¨äºåˆé‡‘è®¾è®¡ï¼Œå¹¶å¯æ¨å¹¿åˆ°å…¶ä»–ç±»å‹çš„é’¢æï¼Œå±•ç¤ºäº†æ•°æ®é«˜æ•ˆæ·±åº¦å­¦ä¹ åœ¨ååº”å †å‹åŠ›å®¹å™¨é’¢åˆ†æä¸­çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.11485v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¯¹ååº”å †å‹åŠ›å®¹å™¨é’¢å¾®è§‚ç»“æ„ä¸­ç¢³åŒ–ç‰©çš„è¯†åˆ«ä¸åˆ†å‰²æŠ€æœ¯ã€‚é‡‡ç”¨æ•°æ®é«˜æ•ˆçš„åˆ†å‰²ç®¡é“ï¼Œåˆ©ç”¨è½»é‡çº§U-Netæ¨¡å‹åœ¨å°‘é‡æ ‡æ³¨çš„æ‰«æç”µå­æ˜¾å¾®é•œå›¾åƒä¸Šè¿›è¡Œè®­ç»ƒï¼Œå®ç°äº†é«˜å‡†ç¡®ç‡çš„ç¢³åŒ–ç‰©åˆ†å‰²ï¼Œå¹¶å¤§å¹…å‡å°‘äº†æ ‡æ³¨å·¥ä½œé‡ã€‚è¯¥æŠ€æœ¯æœ‰åŠ©äºå¿«é€Ÿè‡ªåŠ¨åŒ–é‡åŒ–åˆé‡‘ä¸­çš„ç¢³åŒ–ç‰©ï¼Œä¸ºåˆé‡‘è®¾è®¡å’Œååº”å †å‹åŠ›å®¹å™¨é’¢åˆ†ææä¾›äº†æ½œåœ¨çš„æ•°æ®é«˜æ•ˆæ·±åº¦å­¦ä¹ åº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¢³åŒ–ç‰©åœ¨ååº”å †å‹åŠ›å®¹å™¨é’¢ä¸­çš„é‡è¦ä½œç”¨ï¼šå¼ºåŒ–åˆé‡‘ï¼Œä½†ä¹Ÿå¯èƒ½å¼•å‘è£‚çº¹ã€‚</li>
<li>æ‰«æç”µå­æ˜¾å¾®é•œå›¾åƒåˆ†æä¸­é¢ä¸´çš„æŒ‘æˆ˜ï¼šç¢³åŒ–ç‰©å’ŒåŸºè´¨ä¹‹é—´çš„ç°åº¦å€¼é‡å ã€‚</li>
<li>é‡‡ç”¨è½»é‡çº§U-Netæ¨¡å‹è¿›è¡Œå›¾åƒåˆ†å‰²ï¼šæ¨¡å‹å‚æ•°æ•ˆç‡ä¼˜åŒ–ï¼Œä»…ä½¿ç”¨å°‘é‡æ ‡æ³¨çš„æ‰«æç”µå­æ˜¾å¾®é•œå›¾åƒè¿›è¡Œè®­ç»ƒã€‚</li>
<li>æ¨¡å‹è¡¨ç°å“è¶Šï¼šè¾¾åˆ°Dice-SÃ¸rensenç³»æ•°0.98ï¼Œä¼˜äºå†¶é‡‘é¢†åŸŸçš„ç°æœ‰æŠ€æœ¯ï¼ˆä¼ ç»Ÿå›¾åƒåˆ†æï¼š0.85ï¼‰ã€‚</li>
<li>æ˜¾è‘—å‡å°‘æ ‡æ³¨å·¥ä½œé‡ï¼šç›¸æ¯”ç°æœ‰æ•°æ®é«˜æ•ˆåˆ†å‰²æ¨¡å‹ï¼Œé™ä½äº†æ ‡æ³¨å·¥ä½œé‡ä¸€ä¸ªæ•°é‡çº§ã€‚</li>
<li>æŠ€æœ¯åœ¨åˆé‡‘è®¾è®¡ä¸­çš„åº”ç”¨ï¼šå¿«é€Ÿè‡ªåŠ¨åŒ–é‡åŒ–ç¢³åŒ–ç‰©ï¼Œä¸ºåˆé‡‘è®¾è®¡æä¾›ä¾æ®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.11485">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bb4ebce7421f8969234086668ab29229" align="middle">
<img src="https://picx.zhimg.com/v2-b3f251196fce2c133a7ce3be5f3171c9" align="middle">
<img src="https://picx.zhimg.com/v2-85c33b4b5f5eb4e2aa2d2ef858e0c3ad" align="middle">
<img src="https://picx.zhimg.com/v2-1cddfc50e29920850a7b419be780dacf" align="middle">
<img src="https://picx.zhimg.com/v2-59f561835ecbc6fafbe09f3ec41f1c18" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="VoxTell-Free-Text-Promptable-Universal-3D-Medical-Image-Segmentation"><a href="#VoxTell-Free-Text-Promptable-Universal-3D-Medical-Image-Segmentation" class="headerlink" title="VoxTell: Free-Text Promptable Universal 3D Medical Image Segmentation"></a>VoxTell: Free-Text Promptable Universal 3D Medical Image Segmentation</h2><p><strong>Authors:Maximilian Rokuss, Moritz Langenberg, Yannick Kirchhoff, Fabian Isensee, Benjamin Hamm, Constantin Ulrich, Sebastian Regnery, Lukas Bauer, Efthimios Katsigiannopulos, Tobias Norajitra, Klaus Maier-Hein</strong></p>
<p>We introduce VoxTell, a vision-language model for text-prompted volumetric medical image segmentation. It maps free-form descriptions, from single words to full clinical sentences, to 3D masks. Trained on 62K+ CT, MRI, and PET volumes spanning over 1K anatomical and pathological classes, VoxTell uses multi-stage vision-language fusion across decoder layers to align textual and visual features at multiple scales. It achieves state-of-the-art zero-shot performance across modalities on unseen datasets, excelling on familiar concepts while generalizing to related unseen classes. Extensive experiments further demonstrate strong cross-modality transfer, robustness to linguistic variations and clinical language, as well as accurate instance-specific segmentation from real-world text. Code is available at: <a target="_blank" rel="noopener" href="https://www.github.com/MIC-DKFZ/VoxTell">https://www.github.com/MIC-DKFZ/VoxTell</a></p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†VoxTellï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºæ–‡æœ¬æç¤ºçš„ä½“ç§¯åŒ»å­¦å›¾åƒåˆ†å‰²çš„è§†è§‰è¯­è¨€æ¨¡å‹ã€‚å®ƒå°†è‡ªç”±å½¢å¼çš„æè¿°ï¼ˆä»å•ä¸ªå•è¯åˆ°å®Œæ•´çš„ä¸´åºŠå¥å­ï¼‰æ˜ å°„åˆ°3Dè’™ç‰ˆã€‚VoxTellåœ¨è¶…è¿‡1000ä¸ªè§£å‰–å’Œç—…ç†ç±»åˆ«çš„è¶…è¿‡62Kä¸ªCTã€MRIå’ŒPETä½“ç§¯ä¸Šè¿›è¡Œäº†è®­ç»ƒï¼Œåœ¨è§£ç å™¨å±‚ä¸Šä½¿ç”¨äº†å¤šé˜¶æ®µçš„è§†è§‰è¯­è¨€èåˆï¼Œä»¥åœ¨å¤šä¸ªå°ºåº¦ä¸Šå°†æ–‡æœ¬å’Œè§†è§‰ç‰¹å¾å¯¹é½ã€‚å®ƒåœ¨æœªè§æ•°æ®é›†ä¸Šå®ç°äº†è·¨æ¨¡æ€çš„é›¶æ ·æœ¬æ€§èƒ½å‰æ²¿æ°´å¹³ï¼Œåœ¨ç†Ÿæ‚‰çš„æ¦‚å¿µä¸Šè¡¨ç°å‡ºè‰²ï¼Œå¹¶æ¨å¹¿åˆ°ç›¸å…³çš„æœªè§ç±»åˆ«ã€‚è¿›ä¸€æ­¥çš„å®éªŒè¿˜è¯æ˜äº†å¼ºå¤§çš„è·¨æ¨¡æ€è¿ç§»èƒ½åŠ›ï¼Œå¯¹è¯­è¨€å˜åŒ–å’Œä¸´åºŠè¯­è¨€çš„ç¨³å¥æ€§ï¼Œä»¥åŠä»ç°å®ä¸–ç•Œçš„æ–‡æœ¬è¿›è¡Œå‡†ç¡®çš„å®ä¾‹ç‰¹å®šåˆ†å‰²ã€‚ä»£ç å¯ç”¨åœ¨ï¼š<a target="_blank" rel="noopener" href="https://www.github.com/MIC-DKFZ/VoxTell">https://www.github.com/MIC-DKFZ/VoxTell</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.11450v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>VoxTellæ˜¯ä¸€ç§ç”¨äºæ–‡æœ¬æç¤ºçš„ä½“ç§¯åŒ»å­¦å›¾åƒåˆ†å‰²çš„è§†è§‰è¯­è¨€æ¨¡å‹ã€‚å®ƒå°†è‡ªç”±å½¢å¼çš„æè¿°ï¼ˆä»å•ä¸ªå•è¯åˆ°å®Œæ•´çš„ä¸´åºŠå¥å­ï¼‰æ˜ å°„åˆ°3Dæ©è†œä¸Šã€‚è¯¥æ¨¡å‹åœ¨è¶…è¿‡62Kçš„CTã€MRIå’ŒPETä½“ç§¯æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œæ¶µç›–è¶…è¿‡ä¸€åƒç§è§£å‰–å’Œç—…ç†ç±»åˆ«ã€‚VoxTellä½¿ç”¨å¤šé˜¶æ®µè§†è§‰è¯­è¨€èåˆï¼Œåœ¨è§£ç å™¨å±‚å¯¹é½æ–‡æœ¬å’Œè§†è§‰ç‰¹å¾ï¼Œå®ç°è·¨æ¨¡æ€çš„é›¶æ ·æœ¬æ€§èƒ½æœ€ä¼˜ï¼Œå¹¶ä¸”åœ¨æœªè§è¿‡çš„ç±»åˆ«ä¸Šè¡¨ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜è¡¨ç°å‡ºå¼ºå¤§çš„è·¨æ¨¡æ€è½¬ç§»èƒ½åŠ›ï¼Œå¯¹è¯­è¨€å˜åŒ–å’Œä¸´åºŠè¯­è¨€çš„ç¨³å¥æ€§ï¼Œä»¥åŠä»çœŸå®ä¸–ç•Œæ–‡æœ¬ä¸­è¿›è¡Œå®ä¾‹ç‰¹å®šåˆ†å‰²çš„å‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VoxTellæ˜¯ä¸€ä¸ªè§†è§‰è¯­è¨€æ¨¡å‹ï¼Œç”¨äºæ–‡æœ¬æç¤ºçš„ä½“ç§¯åŒ»å­¦å›¾åƒåˆ†å‰²ã€‚</li>
<li>æ¨¡å‹å°†è‡ªç”±å½¢å¼çš„æ–‡æœ¬æè¿°æ˜ å°„åˆ°3Dæ©è†œä¸Šã€‚</li>
<li>VoxTellåœ¨å¤šç§åŒ»å­¦å›¾åƒæ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œæ¶µç›–å¹¿æ³›çš„è§£å‰–å’Œç—…ç†ç±»åˆ«ã€‚</li>
<li>ä½¿ç”¨å¤šé˜¶æ®µè§†è§‰è¯­è¨€èåˆï¼Œå¯¹é½æ–‡æœ¬å’Œè§†è§‰ç‰¹å¾ã€‚</li>
<li>å®ç°äº†è·¨æ¨¡æ€çš„é›¶æ ·æœ¬æ€§èƒ½æœ€ä¼˜ï¼Œå¹¶åœ¨æœªè§è¿‡çš„ç±»åˆ«ä¸Šè¡¨ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>VoxTellå…·æœ‰å¼ºå¤§çš„è·¨æ¨¡æ€è½¬ç§»èƒ½åŠ›ï¼Œå¯¹è¯­è¨€å˜åŒ–å’Œä¸´åºŠè¯­è¨€å…·æœ‰ç¨³å¥æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.11450">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-807c9b98c5182e92359c58daa9797f56" align="middle">
<img src="https://picx.zhimg.com/v2-7a4896a00360305e4352ce4580b93ef9" align="middle">
<img src="https://picx.zhimg.com/v2-e1f06d2cf0d7bda3f2e017e1abb65487" align="middle">
<img src="https://picx.zhimg.com/v2-3d91d0a6285cdcbe76e77951fe49bd74" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Hi-DREAM-Brain-Inspired-Hierarchical-Diffusion-for-fMRI-Reconstruction-via-ROI-Encoder-and-visuAl-Mapping"><a href="#Hi-DREAM-Brain-Inspired-Hierarchical-Diffusion-for-fMRI-Reconstruction-via-ROI-Encoder-and-visuAl-Mapping" class="headerlink" title="Hi-DREAM: Brain Inspired Hierarchical Diffusion for fMRI Reconstruction via ROI Encoder and visuAl Mapping"></a>Hi-DREAM: Brain Inspired Hierarchical Diffusion for fMRI Reconstruction via ROI Encoder and visuAl Mapping</h2><p><strong>Authors:Guowei Zhang, Yun Zhao, Moein Khajehnejad, Adeel Razi, Levin Kuhlmann</strong></p>
<p>Mapping human brain activity to natural images offers a new window into vision and cognition, yet current diffusion-based decoders face a core difficulty: most condition directly on fMRI features without analyzing how visual information is organized across the cortex. This overlooks the brainâ€™s hierarchical processing and blurs the roles of early, middle, and late visual areas. We propose Hi-DREAM, a brain-inspired conditional diffusion framework that makes the cortical organization explicit. A region-of-interest (ROI) adapter groups fMRI into early&#x2F;mid&#x2F;late streams and converts them into a multi-scale cortical pyramid aligned with the U-Net depth (shallow scales preserve layout and edges; deeper scales emphasize objects and semantics). A lightweight, depth-matched ControlNet injects these scale-specific hints during denoising. The result is an efficient and interpretable decoder in which each signal plays a brain-like role, allowing the model not only to reconstruct images but also to illuminate functional contributions of different visual areas. Experiments on the Natural Scenes Dataset (NSD) show that Hi-DREAM attains state-of-the-art performance on high-level semantic metrics while maintaining competitive low-level fidelity. These findings suggest that structuring conditioning by cortical hierarchy is a powerful alternative to purely data-driven embeddings and provides a useful lens for studying the visual cortex.</p>
<blockquote>
<p>å°†äººç±»å¤§è„‘æ´»åŠ¨ä¸å¤©ç„¶å›¾åƒç›¸æ˜ å°„ä¸ºè§†è§‰å’Œè®¤çŸ¥ç ”ç©¶æä¾›äº†æ–°çš„è§†è§’ï¼Œç„¶è€Œï¼Œå½“å‰çš„åŸºäºæ‰©æ•£çš„è§£ç å™¨é¢ä¸´ä¸€ä¸ªæ ¸å¿ƒéš¾é¢˜ï¼šå®ƒä»¬å¤§å¤šç›´æ¥ä»¥fMRIç‰¹å¾ä¸ºæ¡ä»¶ï¼Œè€Œæ²¡æœ‰åˆ†æè§†è§‰ä¿¡æ¯åœ¨å¤§è„‘çš®å±‚ä¸Šçš„ç»„ç»‡æ–¹å¼ã€‚è¿™å¿½ç•¥äº†å¤§è„‘çš„åˆ†å±‚å¤„ç†è¿‡ç¨‹ï¼Œå¹¶æ¨¡ç³Šäº†æ—©æœŸã€ä¸­æœŸå’Œæ™šæœŸè§†è§‰åŒºåŸŸçš„è§’è‰²ã€‚æˆ‘ä»¬æå‡ºäº†Hi-DREAMï¼Œè¿™æ˜¯ä¸€ä¸ªå—å¤§è„‘å¯å‘çš„æ¡ä»¶æ‰©æ•£æ¡†æ¶ï¼Œå®ƒæ˜ç¡®äº†çš®å±‚ç»„ç»‡ã€‚æ„Ÿå…´è¶£åŒºåŸŸï¼ˆROIï¼‰é€‚é…å™¨å°†fMRIåˆ†ä¸ºæ—©æœŸ&#x2F;ä¸­æœŸ&#x2F;æ™šæœŸæµï¼Œå¹¶å°†å…¶è½¬åŒ–ä¸ºä¸U-Netæ·±åº¦å¯¹é½çš„å¤šå°ºåº¦çš®å±‚é‡‘å­—å¡”ï¼ˆæµ…å±‚å°ºåº¦ä¿ç•™å¸ƒå±€å’Œè¾¹ç¼˜ï¼›æ·±å±‚å°ºåº¦å¼ºè°ƒå¯¹è±¡å’Œè¯­ä¹‰ï¼‰ã€‚ä¸€ä¸ªè½»ä¾¿çš„ã€æ·±åº¦åŒ¹é…çš„ControlNetåœ¨é™å™ªè¿‡ç¨‹ä¸­æ³¨å…¥è¿™äº›å°ºåº¦ç‰¹å®šçš„æç¤ºã€‚ç»“æœæ˜¯ä¸€ä¸ªé«˜æ•ˆä¸”å¯è§£é‡Šçš„è§£ç å™¨ï¼Œå…¶ä¸­æ¯ä¸ªä¿¡å·éƒ½æ‰®æ¼”ç€ç±»ä¼¼å¤§è„‘çš„è§’è‰²ï¼Œä½¿æ¨¡å‹ä¸ä»…èƒ½å¤Ÿé‡å»ºå›¾åƒï¼Œè¿˜èƒ½å¤Ÿé˜æ˜ä¸åŒè§†è§‰åŒºåŸŸçš„åŠŸèƒ½è´¡çŒ®ã€‚åœ¨è‡ªç„¶åœºæ™¯æ•°æ®é›†ï¼ˆNSDï¼‰ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒHi-DREAMåœ¨é«˜å±‚æ¬¡è¯­ä¹‰æŒ‡æ ‡ä¸Šè¾¾åˆ°äº†æœ€æ–°æ€§èƒ½ï¼ŒåŒæ—¶åœ¨ä½å±‚æ¬¡ä¿çœŸåº¦ä¸Šä¿æŒäº†ç«äº‰åŠ›ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œé€šè¿‡çš®å±‚ç»“æ„è¿›è¡Œç»“æ„åŒ–æ¡ä»¶è®¾ç½®æ˜¯çº¯ç²¹æ•°æ®é©±åŠ¨åµŒå…¥çš„æœ‰åŠ›æ›¿ä»£æ–¹æ¡ˆï¼Œä¸ºç ”ç©¶è§†è§‰çš®å±‚æä¾›äº†æœ‰ç”¨çš„è§†è§’ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.11437v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æå‡ºä¸€ç§åä¸ºHi-DREAMçš„è„‘å¯å‘æ¡ä»¶æ‰©æ•£æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†äººç±»å¤§è„‘æ´»åŠ¨æ˜ å°„åˆ°è‡ªç„¶å›¾åƒä¸Šï¼Œä»¥æ­ç¤ºè§†è§‰å’Œè®¤çŸ¥çš„æ–°è§†è§’ã€‚æ–‡ç« æŒ‡å‡ºå½“å‰æ‰©æ•£è§£ç å™¨ä¸»è¦ç›´æ¥ä¾èµ–äºfMRIç‰¹å¾è€Œå¿½è§†è§†è§‰ä¿¡æ¯åœ¨å¤§è„‘çš®å±‚çš„ç»„ç»‡æ–¹å¼ï¼Œå› æ­¤æå‡ºé€šè¿‡çš®å±‚ç»„ç»‡æ˜ç¡®çš„Hi-DREAMæ¡†æ¶æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡åŒºåŸŸå…´è¶£é€‚é…å™¨å°†fMRIæ•°æ®åˆ†ä¸ºæ—©æœŸã€ä¸­æœŸå’Œæ™šæœŸæµï¼Œå¹¶è½¬æ¢ä¸ºä¸U-Netæ·±åº¦ç›¸åŒ¹é…çš„å¤šå°ºåº¦çš®å±‚é‡‘å­—å¡”ã€‚æ·±åº¦åŒ¹é…çš„ControlNetåœ¨é™å™ªè¿‡ç¨‹ä¸­æ³¨å…¥å°ºåº¦ç‰¹å®šæç¤ºï¼Œä½¿å¾—è§£ç å™¨ä¸ä»…é«˜æ•ˆä¸”å¯è§£é‡Šï¼Œè€Œä¸”æ¯ä¸ªä¿¡å·éƒ½æ‰®æ¼”ç±»ä¼¼å¤§è„‘çš„è§’è‰²ï¼Œæ—¢èƒ½é‡å»ºå›¾åƒåˆèƒ½é˜æ˜ä¸åŒè§†è§‰åŒºåŸŸçš„è´¡çŒ®ã€‚åœ¨Natural Scenes Datasetä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒHi-DREAMåœ¨é«˜å±‚æ¬¡è¯­ä¹‰æŒ‡æ ‡ä¸Šè¾¾åˆ°æœ€ä½³æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒè¾ƒä½å±‚æ¬¡çš„ä¿çœŸç«äº‰åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰æ‰©æ•£è§£ç å™¨åœ¨æ˜ å°„äººè„‘æ´»åŠ¨åˆ°è‡ªç„¶å›¾åƒæ—¶å¿½ç•¥è§†è§‰ä¿¡æ¯åœ¨å¤§è„‘çš®å±‚çš„ç»„ç»‡æ–¹å¼ã€‚</li>
<li>Hi-DREAMæ¡†æ¶é€šè¿‡æ˜ç¡®çš®å±‚ç»„ç»‡æ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œå°†fMRIæ•°æ®åˆ†ä¸ºæ—©æœŸã€ä¸­æœŸå’Œæ™šæœŸæµã€‚</li>
<li>Hi-DREAMåˆ©ç”¨å¤šå°ºåº¦çš®å±‚é‡‘å­—å¡”ä¸U-Netæ·±åº¦ç›¸åŒ¹é…ï¼Œå®ç°é«˜æ•ˆä¸”å¯è§£é‡Šçš„è§£ç ã€‚</li>
<li>ControlNetæ³¨å…¥å°ºåº¦ç‰¹å®šæç¤ºï¼Œä½¿æ¯ä¸ªä¿¡å·æ‰®æ¼”ç±»ä¼¼å¤§è„‘çš„è§’è‰²ï¼Œæ—¢èƒ½é‡å»ºå›¾åƒåˆèƒ½é˜æ˜è§†è§‰åŒºåŸŸçš„è´¡çŒ®ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜Hi-DREAMåœ¨é«˜å±‚æ¬¡è¯­ä¹‰æŒ‡æ ‡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>Hi-DREAMæ¡†æ¶æä¾›äº†ä¸€ä¸ªå¼ºå¤§çš„å·¥å…·æ¥ç ”ç©¶è§†è§‰çš®å±‚çš„åŠŸèƒ½å’Œç»“æ„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.11437">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-44bdc1f7748c9f3e9df540c150743215" align="middle">
<img src="https://picx.zhimg.com/v2-baca6fab1b699735be5bbd3995ad4a3d" align="middle">
<img src="https://picx.zhimg.com/v2-a999b8c3eb8b34beb9aa4541460505bb" align="middle">
<img src="https://picx.zhimg.com/v2-035d7a4773e36a847b627588bcdcebaa" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Unsupervised-Motion-Compensated-Decomposition-for-Cardiac-MRI-Reconstruction-via-Neural-Representation"><a href="#Unsupervised-Motion-Compensated-Decomposition-for-Cardiac-MRI-Reconstruction-via-Neural-Representation" class="headerlink" title="Unsupervised Motion-Compensated Decomposition for Cardiac MRI Reconstruction via Neural Representation"></a>Unsupervised Motion-Compensated Decomposition for Cardiac MRI Reconstruction via Neural Representation</h2><p><strong>Authors:Xuanyu Tian, Lixuan Chen, Qing Wu, Xiao Wang, Jie Feng, Yuyao Zhang, Hongjiang Wei</strong></p>
<p>Cardiac magnetic resonance (CMR) imaging is widely used to characterize cardiac morphology and function. To accelerate CMR imaging, various methods have been proposed to recover high-quality spatiotemporal CMR images from highly undersampled k-t space data. However, current CMR reconstruction techniques either fail to achieve satisfactory image quality or are restricted by the scarcity of ground truth data, leading to limited applicability in clinical scenarios. In this work, we proposed MoCo-INR, a new unsupervised method that integrates implicit neural representations (INR) with the conventional motion-compensated (MoCo) framework. Using explicit motion modeling and the continuous prior of INRs, MoCo-INR can produce accurate cardiac motion decomposition and high-quality CMR reconstruction. Furthermore, we introduce a new INR network architecture tailored to the CMR problem, which significantly stabilizes model optimization. Experiments on retrospective (simulated) datasets demonstrate the superiority of MoCo-INR over state-of-the-art methods, achieving fast convergence and fine-detailed reconstructions at ultra-high acceleration factors (e.g., 20x in VISTA sampling). Additionally, evaluations on prospective (real-acquired) free-breathing CMR scans highlight the clinical practicality of MoCo-INR for real-time imaging. Several ablation studies further confirm the effectiveness of the critical components of MoCo-INR.</p>
<blockquote>
<p>å¿ƒè„ç£å…±æŒ¯ï¼ˆCMRï¼‰æˆåƒå¹¿æ³›ç”¨äºè¡¨å¾å¿ƒè„å½¢æ€å’ŒåŠŸèƒ½ã€‚ä¸ºåŠ é€ŸCMRæˆåƒï¼Œå·²ç»æå‡ºäº†å„ç§æ–¹æ³•ä»é«˜åº¦æ¬ é‡‡æ ·çš„k-tç©ºé—´æ•°æ®ä¸­æ¢å¤é«˜è´¨é‡çš„æ—¶ç©ºCMRå›¾åƒã€‚ç„¶è€Œï¼Œå½“å‰çš„CMRé‡å»ºæŠ€æœ¯è¦ä¹ˆæ— æ³•è¾¾åˆ°ä»¤äººæ»¡æ„çš„å›¾åƒè´¨é‡ï¼Œè¦ä¹ˆå—åˆ°çœŸå®æ•°æ®ç¼ºä¹çš„é™åˆ¶ï¼Œå¯¼è‡´åœ¨ä¸´åºŠåœºæ™¯ä¸­çš„åº”ç”¨æœ‰é™ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†MoCo-INRï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„æ— ç›‘ç£æ–¹æ³•ï¼Œå®ƒå°†éšå¼ç¥ç»è¡¨ç¤ºï¼ˆINRï¼‰ä¸ä¼ ç»Ÿçš„è¿åŠ¨è¡¥å¿ï¼ˆMoCoï¼‰æ¡†æ¶ç›¸ç»“åˆã€‚é€šè¿‡æ˜ç¡®çš„è¿åŠ¨å»ºæ¨¡å’ŒINRçš„è¿ç»­å…ˆéªŒï¼ŒMoCo-INRå¯ä»¥å®ç°å‡†ç¡®çš„å¿ƒè„è¿åŠ¨åˆ†è§£å’Œé«˜è´¨é‡çš„CMRé‡å»ºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é’ˆå¯¹CMRé—®é¢˜å¼•å…¥äº†ä¸€ç§æ–°çš„INRç½‘ç»œæ¶æ„ï¼Œè¿™å¯ä»¥æ˜¾è‘—ç¨³å®šæ¨¡å‹ä¼˜åŒ–ã€‚åœ¨å›é¡¾æ€§ï¼ˆæ¨¡æ‹Ÿï¼‰æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMoCo-INRä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå®ç°äº†å¿«é€Ÿæ”¶æ•›å’Œåœ¨è¶…é«˜åŠ é€Ÿå› å­ï¼ˆä¾‹å¦‚VISTAé‡‡æ ·ä¸­çš„20xï¼‰ä¸‹çš„ç²¾ç»†é‡å»ºã€‚æ­¤å¤–ï¼Œå¯¹å‰ç»æ€§ï¼ˆçœŸå®é‡‡é›†ï¼‰è‡ªç”±å‘¼å¸CMRæ‰«æçš„è¯„ä¼°çªå‡ºäº†MoCo-INRåœ¨å®æ—¶æˆåƒä¸­çš„ä¸´åºŠå®ç”¨æ€§ã€‚å‡ é¡¹æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥è¯å®äº†MoCo-INRå…³é”®ç»„ä»¶çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.11436v1">PDF</a> Accepted by AAAI-26</p>
<p><strong>Summary</strong><br>     æ­¤æ‘˜è¦ä»‹ç»äº†ä¸€ç§æ–°å‹çš„æ— ç›‘ç£æ–¹æ³•MoCo-INRï¼Œå®ƒå°†éšç¥ç»è¡¨ç¤ºï¼ˆINRï¼‰ä¸ä¼ ç»Ÿè¿åŠ¨è¡¥å¿ï¼ˆMoCoï¼‰æ¡†æ¶ç›¸ç»“åˆï¼Œç”¨äºåŠ é€Ÿå¿ƒè„ç£å…±æŒ¯ï¼ˆCMRï¼‰æˆåƒã€‚è¯¥æ–¹æ³•èƒ½å‡†ç¡®åˆ†è§£å¿ƒè„è¿åŠ¨å¹¶é‡å»ºé«˜è´¨é‡CMRå›¾åƒï¼Œå¹¶åœ¨å›é¡¾æ€§æ¨¡æ‹Ÿæ•°æ®é›†å’Œå‰ç»æ€§å®æ—¶CMRæ‰«æä¸Šå‡è¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MoCo-INRæ˜¯ä¸€ç§æ–°å‹æ— ç›‘ç£æ–¹æ³•ï¼Œç»“åˆäº†éšç¥ç»è¡¨ç¤ºï¼ˆINRï¼‰å’Œè¿åŠ¨è¡¥å¿ï¼ˆMoCoï¼‰æŠ€æœ¯ï¼Œç”¨äºåŠ é€Ÿå¿ƒè„ç£å…±æŒ¯ï¼ˆCMRï¼‰æˆåƒã€‚</li>
<li>MoCo-INRèƒ½å‡†ç¡®åˆ†è§£å¿ƒè„è¿åŠ¨å¹¶é‡å»ºé«˜è´¨é‡CMRå›¾åƒã€‚</li>
<li>åœ¨å›é¡¾æ€§æ¨¡æ‹Ÿæ•°æ®é›†ä¸Šï¼ŒMoCo-INRçš„è¡¨ç°ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œå®ç°äº†å¿«é€Ÿæ”¶æ•›å’Œç²¾ç»†çš„é‡å»ºï¼Œç”šè‡³åœ¨è¶…é«˜åŠ é€Ÿåº¦å› å­ï¼ˆä¾‹å¦‚20å€ï¼‰ä¸‹ä¹Ÿæ˜¯å¦‚æ­¤ã€‚</li>
<li>MoCo-INRåœ¨å‰æ™¯å®æ—¶CMRæ‰«æä¸Šçš„è¡¨ç°çªå‡ºäº†å…¶å®ç”¨æ€§ã€‚</li>
<li>MoCo-INRçš„æ–°å‹INRç½‘ç»œæ¶æ„é’ˆå¯¹CMRé—®é¢˜è¿›è¡Œäº†å®šåˆ¶ï¼Œæ˜¾è‘—ç¨³å®šäº†æ¨¡å‹ä¼˜åŒ–ã€‚</li>
<li>æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥è¯å®äº†MoCo-INRå…³é”®ç»„ä»¶çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.11436">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-eb5c10c65f0d0cfa02115de0e379d703" align="middle">
<img src="https://picx.zhimg.com/v2-c8afb714de35b4335344ce199e3bce18" align="middle">
<img src="https://picx.zhimg.com/v2-eba3010d89ad37c5b7c3dc98a27dcf02" align="middle">
<img src="https://picx.zhimg.com/v2-1f6b7a39d8a1d6d7bfdb75c4dbb60d05" align="middle">
<img src="https://picx.zhimg.com/v2-8ed2e4874384e76b8c52d989570c6874" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Unsupervised-Segmentation-of-Micro-CT-Scans-of-Polyurethane-Structures-By-Combining-Hidden-Markov-Random-Fields-and-a-U-Net"><a href="#Unsupervised-Segmentation-of-Micro-CT-Scans-of-Polyurethane-Structures-By-Combining-Hidden-Markov-Random-Fields-and-a-U-Net" class="headerlink" title="Unsupervised Segmentation of Micro-CT Scans of Polyurethane Structures By Combining Hidden-Markov-Random Fields and a U-Net"></a>Unsupervised Segmentation of Micro-CT Scans of Polyurethane Structures By Combining Hidden-Markov-Random Fields and a U-Net</h2><p><strong>Authors:Julian Grolig, Lars Griem, Michael Selzer, Hans-Ulrich Kauczor, Simon M. F. Triphan, Britta Nestler, Arnd Koeppe</strong></p>
<p>Extracting digital material representations from images is a necessary prerequisite for a quantitative analysis of material properties. Different segmentation approaches have been extensively studied in the past to achieve this task, but were often lacking accuracy or speed. With the advent of machine learning, supervised convolutional neural networks (CNNs) have achieved state-of-the-art performance for different segmentation tasks. However, these models are often trained in a supervised manner, which requires large labeled datasets. Unsupervised approaches do not require ground-truth data for learning, but suffer from long segmentation times and often worse segmentation accuracy. Hidden Markov Random Fields (HMRF) are an unsupervised segmentation approach that incorporates concepts of neighborhood and class distributions. We present a method that integrates HMRF theory and CNN segmentation, leveraging the advantages of both areas: unsupervised learning and fast segmentation times. We investigate the contribution of different neighborhood terms and components for the unsupervised HMRF loss. We demonstrate that the HMRF-UNet enables high segmentation accuracy without ground truth on a Micro-Computed Tomography ($Î¼$CT) image dataset of Polyurethane (PU) foam structures. Finally, we propose and demonstrate a pre-training strategy that considerably reduces the required amount of ground-truth data when training a segmentation model.</p>
<blockquote>
<p>ä»å›¾åƒä¸­æå–æ•°å­—ææ–™è¡¨ç¤ºæ˜¯å¯¹ææ–™å±æ€§è¿›è¡Œå®šé‡åˆ†æçš„å¿…è¦å‰æã€‚è¿‡å»å·²ç»ç ”ç©¶äº†ä¸åŒçš„åˆ†å‰²æ–¹æ³•æ¥å®ç°è¿™ä¸€ä»»åŠ¡ï¼Œä½†å¾€å¾€ç¼ºä¹å‡†ç¡®æ€§æˆ–é€Ÿåº¦ã€‚éšç€æœºå™¨å­¦ä¹ çš„å‘å±•ï¼Œæœ‰ç›‘ç£çš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰åœ¨ä¸åŒåˆ†å‰²ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹é€šå¸¸æ˜¯æœ‰ç›‘ç£è®­ç»ƒçš„ï¼Œéœ€è¦å¤§é‡æ ‡è®°æ•°æ®é›†ã€‚æ— ç›‘ç£çš„æ–¹æ³•ä¸éœ€è¦çœŸå®æ•°æ®è¿›è¡Œå­¦ä¹ ï¼Œä½†å­˜åœ¨åˆ†å‰²æ—¶é—´é•¿å’Œåˆ†å‰²å‡†ç¡®æ€§è¾ƒå·®çš„é—®é¢˜ã€‚éšé©¬å°”å¯å¤«éšæœºåœºï¼ˆHMRFï¼‰æ˜¯ä¸€ç§ç»“åˆäº†é‚»åŸŸå’Œç±»åˆ«åˆ†å¸ƒæ¦‚å¿µçš„æ— ç›‘ç£åˆ†å‰²æ–¹æ³•ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»“åˆHMRFç†è®ºå’ŒCNNåˆ†å‰²çš„æ–¹æ³•ï¼Œå……åˆ†åˆ©ç”¨äº†æ— ç›‘ç£å­¦ä¹ å’Œå¿«é€Ÿåˆ†å‰²æ—¶é—´ä¸¤ä¸ªé¢†åŸŸçš„ä¼˜åŠ¿ã€‚æˆ‘ä»¬ç ”ç©¶äº†ä¸åŒé‚»åŸŸé¡¹å’Œç»„ä»¶å¯¹æ— ç›‘ç£HMRFæŸå¤±çš„è´¡çŒ®ã€‚æˆ‘ä»¬è¯æ˜äº†HMRF-UNetåœ¨æ— Micro-Computed Tomographyï¼ˆÎ¼CTï¼‰å›¾åƒæ•°æ®é›†Polyurethaneï¼ˆPUï¼‰æ³¡æ²«ç»“æ„çš„æƒ…å†µä¸‹å¯å®ç°é«˜åˆ†å‰²ç²¾åº¦ã€‚æœ€åï¼Œæˆ‘ä»¬æå‡ºå¹¶å±•ç¤ºäº†ä¸€ç§é¢„è®­ç»ƒç­–ç•¥ï¼Œè¯¥ç­–ç•¥åœ¨è®­ç»ƒåˆ†å‰²æ¨¡å‹æ—¶æ˜¾è‘—å‡å°‘äº†æ‰€éœ€çš„çœŸå®æ•°æ®æ•°é‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.11378v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºæœºå™¨å­¦ä¹ çš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å·²ç”¨äºåŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ï¼Œå¹¶å–å¾—æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é€šå¸¸éœ€è¦å¤§é‡çš„æ ‡è®°æ•°æ®é›†è¿›è¡Œè®­ç»ƒã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ç»“åˆéšé©¬å°”å¯å¤«éšæœºåœºï¼ˆHMRFï¼‰ç†è®ºå’ŒCNNåˆ†å‰²çš„æ–¹æ³•ï¼Œæ—¨åœ¨å®ç°æ— ç›‘ç£å­¦ä¹ å’Œå¿«é€Ÿåˆ†å‰²ã€‚è¯¥ç ”ç©¶å±•ç¤ºäº†åœ¨æ— éœ€çœŸå®æ ‡æ³¨çš„æƒ…å†µä¸‹ï¼Œåˆ©ç”¨HMRF-UNetåœ¨èšæ°¨é…¯ï¼ˆPUï¼‰æ³¡æ²«ç»“æ„çš„Micro-Computed Tomographyï¼ˆÎ¼CTï¼‰å›¾åƒæ•°æ®é›†ä¸Šå®ç°é«˜åˆ†å‰²å‡†ç¡®æ€§çš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæœ¬ç ”ç©¶è¿˜æå‡ºäº†ä¸€ç§é¢„è®­ç»ƒç­–ç•¥ï¼Œæå¤§åœ°å‡å°‘äº†åˆ†å‰²æ¨¡å‹å¯¹çœŸå®æ ‡æ³¨æ•°æ®çš„éœ€æ±‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ•°å­—ææ–™è¡¨ç¤ºæ˜¯å®šé‡ææ–™æ€§è´¨åˆ†æçš„å¿…è¦å…ˆå†³æ¡ä»¶ï¼Œè¿‡å»çš„ç ”ç©¶æå‡ºäº†å¤šç§åˆ†å‰²æ–¹æ³•æ¥å®ç°è¿™ä¸€ä»»åŠ¡ï¼Œä½†å‡†ç¡®æ€§æˆ–é€Ÿåº¦æ–¹é¢å¸¸å­˜åœ¨ä¸è¶³ã€‚</li>
<li>éšç€æœºå™¨å­¦ä¹ çš„å‘å±•ï¼Œç›‘ç£å­¦ä¹ å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰åœ¨åˆ†å‰²ä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li>
<li>ä¼ ç»Ÿçš„ç›‘ç£å­¦ä¹ æ–¹æ³•éœ€è¦å¤§é‡çš„æ ‡è®°æ•°æ®é›†è¿›è¡Œæ¨¡å‹è®­ç»ƒã€‚</li>
<li>éšé©¬å°”å¯å¤«éšæœºåœºï¼ˆHMRFï¼‰æ˜¯ä¸€ç§æ— ç›‘ç£åˆ†å‰²æ–¹æ³•ï¼Œç»“åˆäº†é‚»åŸŸå’Œç±»åˆ«åˆ†å¸ƒçš„æ¦‚å¿µã€‚</li>
<li>æœ¬ç ”ç©¶ç»“åˆäº†HMRFç†è®ºå’ŒCNNåˆ†å‰²ï¼Œå®ç°äº†æ— ç›‘ç£å­¦ä¹ ä¸å¿«é€Ÿåˆ†å‰²ã€‚</li>
<li>åœ¨æ— éœ€çœŸå®æ ‡æ³¨çš„æƒ…å†µä¸‹ï¼ŒHMRF-UNetåœ¨èšæ°¨é…¯æ³¡æ²«ç»“æ„çš„Î¼CTå›¾åƒæ•°æ®é›†ä¸Šå®ç°äº†é«˜åˆ†å‰²å‡†ç¡®æ€§ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§é¢„è®­ç»ƒç­–ç•¥ï¼Œæ˜¾è‘—å‡å°‘äº†è®­ç»ƒåˆ†å‰²æ¨¡å‹å¯¹çœŸå®æ ‡æ³¨æ•°æ®çš„éœ€æ±‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.11378">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-88519daaafe97ae86c3c63c8d14cad43" align="middle">
<img src="https://picx.zhimg.com/v2-11efd8e134284bcd50ae9220918a1326" align="middle">
<img src="https://picx.zhimg.com/v2-bb3676bec7be01ed711f862de0f0b284" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Toward-Scalable-Early-Cancer-Detection-Evaluating-EHR-Based-Predictive-Models-Against-Traditional-Screening-Criteria"><a href="#Toward-Scalable-Early-Cancer-Detection-Evaluating-EHR-Based-Predictive-Models-Against-Traditional-Screening-Criteria" class="headerlink" title="Toward Scalable Early Cancer Detection: Evaluating EHR-Based Predictive Models Against Traditional Screening Criteria"></a>Toward Scalable Early Cancer Detection: Evaluating EHR-Based Predictive Models Against Traditional Screening Criteria</h2><p><strong>Authors:Jiheum Park, Chao Pang, Tristan Y. Lee, Jeong Yun Yang, Jacob Berkowitz, Alexander Z. Wei, Nicholas Tatonetti</strong></p>
<p>Current cancer screening guidelines cover only a few cancer types and rely on narrowly defined criteria such as age or a single risk factor like smoking history, to identify high-risk individuals. Predictive models using electronic health records (EHRs), which capture large-scale longitudinal patient-level health information, may provide a more effective tool for identifying high-risk groups by detecting subtle prediagnostic signals of cancer. Recent advances in large language and foundation models have further expanded this potential, yet evidence remains limited on how useful HER-based models are compared with traditional risk factors currently used in screening guidelines. We systematically evaluated the clinical utility of EHR-based predictive models against traditional risk factors, including gene mutations and family history of cancer, for identifying high-risk individuals across eight major cancers (breast, lung, colorectal, prostate, ovarian, liver, pancreatic, and stomach), using data from the All of Us Research Program, which integrates EHR, genomic, and survey data from over 865,000 participants. Even with a baseline modeling approach, EHR-based models achieved a 3- to 6-fold higher enrichment of true cancer cases among individuals identified as high risk compared with traditional risk factors alone, whether used as a standalone or complementary tool. The EHR foundation model, a state-of-the-art approach trained on comprehensive patient trajectories, further improved predictive performance across 26 cancer types, demonstrating the clinical potential of EHR-based predictive modeling to support more precise and scalable early detection strategies.</p>
<blockquote>
<p>å½“å‰ç™Œç—‡ç­›æŸ¥æŒ‡å—ä»…è¦†ç›–å‡ ç§ç™Œç—‡ç±»å‹ï¼Œå¹¶ä¾èµ–äºç‹­ä¹‰çš„æ ‡å‡†ï¼Œä¾‹å¦‚å¹´é¾„æˆ–å¸çƒŸå²ç­‰å•ä¸€é£é™©å› ç´ æ¥è¯†åˆ«é«˜å±ä¸ªä½“ã€‚ä½¿ç”¨ç”µå­å¥åº·è®°å½•ï¼ˆEHRsï¼‰çš„é¢„æµ‹æ¨¡å‹èƒ½å¤Ÿæ•æ‰å¤§è§„æ¨¡çºµå‘æ‚£è€…çº§å¥åº·ä¿¡æ¯ï¼Œé€šè¿‡æ£€æµ‹ç™Œç—‡çš„å¾®å¦™é¢„è¯Šæ–­ä¿¡å·ï¼Œæä¾›æ›´æœ‰æ•ˆçš„å·¥å…·æ¥è¯†åˆ«é«˜å±ç¾¤ä½“ã€‚éšç€å¤§å‹è¯­è¨€å’ŒåŸºç¡€æ¨¡å‹çš„æœ€æ–°è¿›å±•ï¼Œè¿™ä¸€æ½œåŠ›å¾—åˆ°äº†è¿›ä¸€æ­¥æ‹“å±•ï¼Œç„¶è€Œå…³äºä¸å½“å‰ç­›æŸ¥æŒ‡å—ä¸­ä½¿ç”¨çš„ä¼ ç»Ÿé£é™©å› ç´ ç›¸æ¯”ï¼ŒåŸºäºHERçš„æ¨¡å‹çš„å®é™…æ•ˆç”¨ä»è¯æ®æœ‰é™ã€‚æˆ‘ä»¬ç³»ç»Ÿåœ°è¯„ä¼°äº†åŸºäºEHRçš„é¢„æµ‹æ¨¡å‹ä¸ä¼ ç»Ÿé£é™©å› ç´ ï¼ˆåŒ…æ‹¬åŸºå› çªå˜å’Œç™Œç—‡å®¶æ—å²ï¼‰åœ¨ä¸´åºŠä¸Šçš„æ•ˆç”¨ï¼Œä»¥è¯†åˆ«å…«ç§ä¸»è¦ç™Œç—‡ï¼ˆä¹³è…ºç™Œã€è‚ºç™Œã€ç»“ç›´è‚ ç™Œã€å‰åˆ—è…ºç™Œã€åµå·¢ç™Œã€è‚ç™Œã€èƒ°è…ºç™Œå’Œèƒƒï¼‰ä¸­çš„é«˜å±ä¸ªä½“ã€‚æˆ‘ä»¬ä½¿ç”¨äº†â€œæˆ‘ä»¬æ‰€æœ‰äººç ”ç©¶è®¡åˆ’â€çš„æ•°æ®ï¼Œè¯¥è®¡åˆ’æ•´åˆäº†æ¥è‡ªè¶…è¿‡86ä¸‡äº”åƒåå‚ä¸è€…çš„EHRã€åŸºå› å’Œè°ƒæŸ¥æ•°æ®ã€‚å³ä½¿åœ¨åŸºçº¿å»ºæ¨¡æ–¹æ³•ä¸‹ï¼Œä¸ä»…ä½¿ç”¨ä¼ ç»Ÿé£é™©å› ç´ ç›¸æ¯”ï¼ŒåŸºäºEHRçš„æ¨¡å‹åœ¨è¢«è¯†åˆ«ä¸ºé«˜å±ä¸ªä½“çš„äººç¾¤ä¸­ï¼ŒçœŸå®ç™Œç—‡ç—…ä¾‹çš„å¯Œé›†ç‡æé«˜äº†ä¸‰åˆ°å…­å€ã€‚ä¸è®ºä½œä¸ºç‹¬ç«‹å·¥å…·è¿˜æ˜¯è¾…åŠ©å·¥å…·ï¼Œè¿™ä¸€æ¨¡å‹éƒ½å±•ç¤ºäº†å…¶ä¸´åºŠæ½œåŠ›ï¼Œç”¨äºæ”¯æŒæ›´ç²¾ç¡®å’Œå¯è§„æ¨¡åŒ–çš„æ—©æœŸæ£€æµ‹ç­–ç•¥ã€‚æ­¤å¤–ï¼ŒåŸºäºEHRåŸºç¡€æ¨¡å‹çš„æœ€æ–°æ–¹æ³•ç»è¿‡å…¨é¢çš„æ‚£è€…è½¨è¿¹è®­ç»ƒï¼Œè¿›ä¸€æ­¥æé«˜äº†åœ¨å¤šç§ç™Œç—‡ç±»å‹ä¸­çš„é¢„æµ‹æ€§èƒ½ã€‚è¿™æ˜¾ç¤ºäº†åŸºäºEHRçš„é¢„æµ‹æ¨¡å‹çš„ä¸´åºŠæ½œåŠ›ï¼Œæœ‰åŠ©äºæ”¯æŒæ›´ç²¾ç¡®å’Œå¯æ‰©å±•çš„æ—©æœŸæ£€æµ‹ç­–ç•¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.11293v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå½“å‰ç™Œç—‡ç­›æŸ¥æŒ‡å—è¦†ç›–é¢è¾ƒçª„ï¼Œä»…é’ˆå¯¹å°‘æ•°ç™Œç—‡ç±»å‹å’Œç‹­çª„å®šä¹‰çš„æ ‡å‡†ï¼ˆå¦‚å¹´é¾„æˆ–å•ä¸€é£é™©å› ç´ å¦‚å¸çƒŸå²ï¼‰ï¼Œæ¥è¯†åˆ«é«˜é£é™©ä¸ªä½“ã€‚ç ”ç©¶åˆ©ç”¨ç”µå­å¥åº·è®°å½•ï¼ˆEHRsï¼‰é¢„æµ‹æ¨¡å‹è¯†åˆ«é«˜é£é™©äººç¾¤å…·æœ‰è¾ƒå¤§æ½œåŠ›ï¼Œè¿™äº›æ¨¡å‹æ•æ‰å¤§è§„æ¨¡çš„çºµå‘æ‚£è€…å¥åº·ä¿¡æ¯ï¼Œå¯æ£€æµ‹å‡ºç™Œç—‡çš„å¾®å¦™é¢„è¯Šæ–­ä¿¡å·ã€‚æœ¬æ–‡é€šè¿‡å…¨é¢è¯„ä¼°ç”µå­å¥åº·è®°å½•åŸºç¡€é¢„æµ‹æ¨¡å‹ç›¸å¯¹äºä¼ ç»Ÿé£é™©å› ç´ çš„å®ç”¨æ€§ï¼Œç¡®è®¤äº†å…¶åœ¨è¯†åˆ«å…«å¤§ç™Œç—‡ï¼ˆä¹³è…ºç™Œã€è‚ºç™Œç­‰ï¼‰é«˜é£é™©ä¸ªä½“æ–¹é¢çš„ä¼˜åŠ¿ã€‚å³ä½¿åœ¨åŸºæœ¬çš„å»ºæ¨¡æ–¹æ³•ä¸­ï¼Œç”µå­å¥åº·è®°å½•æ¨¡å‹ä¹Ÿæ¯”ä¼ ç»Ÿé£é™©å› ç´ æ›´å…·ä¼˜åŠ¿ï¼Œå¯Œé›†äº†é«˜é£é™©ç¾¤ä½“ä¸­çš„çœŸå®ç™Œç—‡ç—…ä¾‹ã€‚æœ€å…ˆè¿›çš„ç”µå­å¥åº·è®°å½•åŸºç¡€æ¨¡å‹åœ¨é¢„æµ‹26ç§ç™Œç—‡æ–¹é¢çš„è¡¨ç°è¿›ä¸€æ­¥æå‡ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨æ”¯æŒæ›´ç²¾ç¡®å’Œå¯æ‰©å±•çš„æ—©æœŸæ£€æµ‹ç­–ç•¥æ–¹é¢çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰ç™Œç—‡ç­›æŸ¥æŒ‡å—è¦†ç›–èŒƒå›´æœ‰é™ï¼Œä¸»è¦ä¾èµ–å¹´é¾„å’Œå•ä¸€é£é™©å› ç´ æ¥è¯†åˆ«é«˜é£é™©äººç¾¤ã€‚</li>
<li>ç”µå­å¥åº·è®°å½•ï¼ˆEHRsï¼‰é¢„æµ‹æ¨¡å‹èƒ½å¤Ÿæ•æ‰å¤§è§„æ¨¡çºµå‘æ‚£è€…å¥åº·ä¿¡æ¯ï¼Œå…·æœ‰æ›´å¤§æ½œåŠ›æ¥è¯†åˆ«ç™Œç—‡é«˜é£é™©äººç¾¤ã€‚</li>
<li>EHRsé¢„æµ‹æ¨¡å‹ç›¸æ¯”ä¼ ç»Ÿé£é™©å› ç´ ï¼ˆå¦‚åŸºå› çªå˜å’Œå®¶æ—ç™Œç—‡å²ï¼‰åœ¨è¯†åˆ«å…«å¤§ç™Œç—‡é«˜é£é™©ä¸ªä½“æ–¹é¢è¡¨ç°å‡ºä¼˜åŠ¿ã€‚</li>
<li>ç”µå­å¥åº·è®°å½•åŸºç¡€æ¨¡å‹å¯Œé›†é«˜é£é™©ç¾¤ä½“ä¸­çš„çœŸå®ç™Œç—‡ç—…ä¾‹æ•°é‡æ˜¾è‘—é«˜äºä¼ ç»Ÿé£é™©å› ç´ ã€‚</li>
<li>æœ€å…ˆè¿›çš„ç”µå­å¥åº·è®°å½•åŸºç¡€æ¨¡å‹åœ¨é¢„æµ‹å¤šç§ç™Œç—‡æ–¹é¢çš„è¡¨ç°ä¼˜å¼‚ï¼Œè¿›ä¸€æ­¥è¯æ˜äº†å…¶æ½œåŠ›ã€‚</li>
<li>ç”µå­å¥åº·è®°å½•é¢„æµ‹æ¨¡å‹å¯ä½œä¸ºç‹¬ç«‹å·¥å…·æˆ–è¡¥å……å·¥å…·ä½¿ç”¨ï¼Œä»¥æé«˜ç™Œç—‡æ£€æµ‹çš„å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.11293">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d85ad7bb1ad609a9a24212931d3e0e95" align="middle">
<img src="https://picx.zhimg.com/v2-7120174f6369462d255ca8405bb749ce" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Coordinative-Learning-with-Ordinal-and-Relational-Priors-for-Volumetric-Medical-Image-Segmentation"><a href="#Coordinative-Learning-with-Ordinal-and-Relational-Priors-for-Volumetric-Medical-Image-Segmentation" class="headerlink" title="Coordinative Learning with Ordinal and Relational Priors for Volumetric Medical Image Segmentation"></a>Coordinative Learning with Ordinal and Relational Priors for Volumetric Medical Image Segmentation</h2><p><strong>Authors:Haoyi Wang</strong></p>
<p>Volumetric medical image segmentation presents unique challenges due to the inherent anatomical structure and limited availability of annotations. While recent methods have shown promise by contrasting spatial relationships between slices, they rely on hard binary thresholds to define positive and negative samples, thereby discarding valuable continuous information about anatomical similarity. Moreover, these methods overlook the global directional consistency of anatomical progression, resulting in distorted feature spaces that fail to capture the canonical anatomical manifold shared across patients. To address these limitations, we propose Coordinative Ordinal-Relational Anatomical Learning (CORAL) to capture both local and global structure in volumetric images. First, CORAL employs a contrastive ranking objective to leverage continuous anatomical similarity, ensuring relational feature distances between slices are proportional to their anatomical position differences. In addition, CORAL incorporates an ordinal objective to enforce global directional consistency, aligning the learned feature distribution with the canonical anatomical progression across patients. Learning these inter-slice relationships produces anatomically informed representations that benefit the downstream segmentation task. Through this coordinative learning framework, CORAL achieves state-of-the-art performance on benchmark datasets under limited-annotation settings while learning representations with meaningful anatomical structure. Code is available at <a target="_blank" rel="noopener" href="https://github.com/haoyiwang25/CORAL">https://github.com/haoyiwang25/CORAL</a>.</p>
<blockquote>
<p>åŒ»å­¦å›¾åƒä¸‰ç»´åˆ†å‰²é¢ä¸´ç€ç‹¬ç‰¹çš„æŒ‘æˆ˜ï¼Œä¸»è¦æ˜¯ç”±äºå…¶å›ºæœ‰çš„è§£å‰–ç»“æ„å’Œæ ‡æ³¨çš„æœ‰é™å¯ç”¨æ€§ã€‚è™½ç„¶æœ€è¿‘çš„æ–¹æ³•é€šè¿‡å¯¹æ¯”åˆ‡ç‰‡ä¹‹é—´çš„ç©ºé—´å…³ç³»æ˜¾ç¤ºå‡ºäº†ä¸€å®šçš„å‰æ™¯ï¼Œä½†å®ƒä»¬ä¾èµ–äºç¡¬äºŒè¿›åˆ¶é˜ˆå€¼æ¥å®šä¹‰æ­£æ ·æœ¬å’Œè´Ÿæ ·æœ¬ï¼Œä»è€Œä¸¢å¼ƒäº†å…³äºè§£å‰–ç›¸ä¼¼æ€§çš„å®è´µè¿ç»­ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œè¿™äº›æ–¹æ³•å¿½è§†äº†è§£å‰–è¿›å±•çš„å…¨å±€æ–¹å‘ä¸€è‡´æ€§ï¼Œå¯¼è‡´ç‰¹å¾ç©ºé—´å¤±çœŸï¼Œæ— æ³•æ•è·æ‚£è€…ä¹‹é—´å…±äº«çš„è§„èŒƒè§£å‰–æµå½¢ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†åè°ƒåºæ•°å…³ç³»è§£å‰–å­¦å­¦ä¹ ï¼ˆCORALï¼‰æ–¹æ³•ï¼Œä»¥æ•è·ä¸‰ç»´å›¾åƒä¸­çš„å±€éƒ¨å’Œå…¨å±€ç»“æ„ã€‚é¦–å…ˆï¼ŒCORALé‡‡ç”¨å¯¹æ¯”æ’åç›®æ ‡æ¥åˆ©ç”¨è¿ç»­çš„è§£å‰–ç›¸ä¼¼æ€§ï¼Œç¡®ä¿åˆ‡ç‰‡ä¹‹é—´çš„å…³ç³»ç‰¹å¾è·ç¦»ä¸å®ƒä»¬çš„è§£å‰–ä½ç½®å·®å¼‚æˆæ­£æ¯”ã€‚æ­¤å¤–ï¼ŒCORALè¿˜çº³å…¥äº†ä¸€ä¸ªåºæ•°ç›®æ ‡æ¥å¼ºåˆ¶æ‰§è¡Œå…¨å±€æ–¹å‘ä¸€è‡´æ€§ï¼Œä½¿å­¦ä¹ åˆ°çš„ç‰¹å¾åˆ†å¸ƒä¸è·¨æ‚£è€…çš„è§„èŒƒè§£å‰–è¿›å±•ä¿æŒä¸€è‡´ã€‚å­¦ä¹ è¿™äº›åˆ‡ç‰‡é—´çš„å…³ç³»äº§ç”Ÿäº†å¯¹ä¸‹æ¸¸åˆ†å‰²ä»»åŠ¡æœ‰ç›Šçš„è§£å‰–å­¦ä¿¡æ¯è¡¨ç¤ºã€‚é€šè¿‡è¿™ä¸€åè°ƒå­¦ä¹ æ¡†æ¶ï¼ŒCORALåœ¨æœ‰é™æ ‡æ³¨è®¾ç½®çš„åŸºå‡†æ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶å­¦ä¹ äº†å…·æœ‰æœ‰æ„ä¹‰çš„è§£å‰–ç»“æ„è¡¨ç¤ºã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/haoyiwang25/CORAL%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/haoyiwang25/CORALæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.11276v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŒ»å­¦å›¾åƒä½“ç§¯åˆ†å‰²é¢ä¸´ç‹¬ç‰¹æŒ‘æˆ˜ï¼Œå¦‚è§£å‰–ç»“æ„å†…åœ¨æ€§å’Œæ ‡æ³¨æ•°æ®æœ‰é™ã€‚ç°æœ‰æ–¹æ³•é€šè¿‡å¯¹æ¯”åˆ‡ç‰‡é—´çš„ç©ºé—´å…³ç³»å±•ç°å‡ºæ½œåŠ›ï¼Œä½†é‡‡ç”¨ç¡¬äºŒè¿›åˆ¶é˜ˆå€¼å®šä¹‰æ­£è´Ÿé¢æ ·æœ¬ï¼Œä¸¢å¤±äº†å…³äºè§£å‰–ç›¸ä¼¼æ€§çš„è¿ç»­ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œè¿™äº›æ–¹æ³•å¿½è§†äº†å…¨å±€æ–¹å‘ä¸€è‡´æ€§ï¼Œå¯¼è‡´ç‰¹å¾ç©ºé—´æ‰­æ›²ï¼Œæ— æ³•æ•æ‰è·¨æ‚£è€…çš„é€šç”¨è§£å‰–æµå½¢ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºåè°ƒåºå…³ç³»è§£å‰–å­¦å­¦ä¹ ï¼ˆCORALï¼‰æ–¹æ³•ï¼Œä»¥æ•æ‰ä½“ç§¯å›¾åƒä¸­çš„å±€éƒ¨å’Œå…¨å±€ç»“æ„ã€‚CORALé‡‡ç”¨å¯¹æ¯”æ’åç›®æ ‡ï¼Œåˆ©ç”¨è¿ç»­çš„è§£å‰–å­¦ç›¸ä¼¼æ€§ï¼Œç¡®ä¿åˆ‡ç‰‡ä¹‹é—´çš„ç›¸å¯¹è·ç¦»ä¸å®ƒä»¬çš„è§£å‰–ä½ç½®å·®å¼‚æˆæ¯”ä¾‹ã€‚æ­¤å¤–ï¼ŒCORALè¿˜åŒ…å«ä¸€ä¸ªåºè´¯ç›®æ ‡ï¼Œä»¥å¼ºåˆ¶æ‰§è¡Œå…¨å±€æ–¹å‘ä¸€è‡´æ€§ï¼Œä½¿å­¦ä¹ åˆ°çš„ç‰¹å¾åˆ†å¸ƒä¸è·¨æ‚£è€…çš„æ ‡å‡†è§£å‰–è¿›å±•å¯¹é½ã€‚è¿™ç§åè°ƒå­¦ä¹ æ¡†æ¶ä½¿CORALåœ¨æœ‰é™æ ‡æ³¨è®¾ç½®ä¸‹å®ç°äº†æœ€ä½³æ€§èƒ½ï¼Œå¹¶å­¦ä¹ äº†å…·æœ‰æœ‰æ„ä¹‰è§£å‰–ç»“æ„çš„è¡¨ç¤ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒä½“ç§¯åˆ†å‰²é¢ä¸´æ ‡æ³¨æ•°æ®æœ‰é™å’Œè§£å‰–ç»“æ„ç‹¬ç‰¹çš„æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä½¿ç”¨ç¡¬äºŒè¿›åˆ¶é˜ˆå€¼å®šä¹‰æ ·æœ¬ï¼Œä¸¢å¤±äº†è§£å‰–ç›¸ä¼¼æ€§çš„è¿ç»­ä¿¡æ¯ã€‚</li>
<li>CORALæ–¹æ³•é€šè¿‡å¯¹æ¯”æ’åç›®æ ‡å’Œåºè´¯ç›®æ ‡ï¼Œæ•æ‰ä½“ç§¯å›¾åƒä¸­çš„å±€éƒ¨å’Œå…¨å±€ç»“æ„ã€‚</li>
<li>CORALåˆ©ç”¨è¿ç»­çš„è§£å‰–å­¦ç›¸ä¼¼æ€§ï¼Œç¡®ä¿åˆ‡ç‰‡ä¹‹é—´çš„ç›¸å¯¹è·ç¦»ä¸è§£å‰–ä½ç½®å·®å¼‚æˆæ¯”ä¾‹ã€‚</li>
<li>CORALå¼ºåˆ¶æ‰§è¡Œå…¨å±€æ–¹å‘ä¸€è‡´æ€§ï¼Œä½¿å­¦ä¹ åˆ°çš„ç‰¹å¾åˆ†å¸ƒä¸è·¨æ‚£è€…çš„æ ‡å‡†è§£å‰–è¿›å±•å¯¹é½ã€‚</li>
<li>é€šè¿‡åè°ƒå­¦ä¹ æ¡†æ¶ï¼ŒCORALåœ¨æœ‰é™æ ‡æ³¨è®¾ç½®ä¸‹å®ç°äº†æœ€ä½³æ€§èƒ½ã€‚</li>
<li>CORALå­¦ä¹ çš„è¡¨ç¤ºå…·æœ‰æœ‰æ„ä¹‰çš„è§£å‰–ç»“æ„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.11276">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3960a72aa7c82bdaeedde12169202382" align="middle">
<img src="https://picx.zhimg.com/v2-17340cbe9862f102c2d7437730d31f77" align="middle">
<img src="https://picx.zhimg.com/v2-f7313a1752233446f4524fa4361c2d57" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="MAFM-3-Modular-Adaptation-of-Foundation-Models-for-Multi-Modal-Medical-AI"><a href="#MAFM-3-Modular-Adaptation-of-Foundation-Models-for-Multi-Modal-Medical-AI" class="headerlink" title="MAFM^3: Modular Adaptation of Foundation Models for Multi-Modal Medical AI"></a>MAFM^3: Modular Adaptation of Foundation Models for Multi-Modal Medical AI</h2><p><strong>Authors:Mohammad Areeb Qazi, Munachiso S Nwadike, Ibrahim Almakky, Mohammad Yaqub, Numan Saeed</strong></p>
<p>Foundational models are trained on extensive datasets to capture the general trends of a domain. However, in medical imaging, the scarcity of data makes pre-training for every domain, modality, or task challenging. Instead of building separate models, we propose MAFM^3 (Modular Adaptation of Foundation Models for Multi-Modal Medical AI), a framework that enables a single foundation model to expand into diverse domains, tasks, and modalities through lightweight modular components. These components serve as specialized skill sets that allow the system to flexibly activate the appropriate capability at the inference time, depending on the input type or clinical objective. Unlike conventional adaptation methods that treat each new task or modality in isolation, MAFM^3 provides a unified and expandable framework for efficient multitask and multimodality adaptation. Empirically, we validate our approach by adapting a chest CT foundation model initially trained for classification into prognosis and segmentation modules. Our results show improved performance on both tasks. Furthermore, by incorporating PET scans, MAFM^3 achieved an improvement in the Dice score 5% compared to the respective baselines. These findings establish that foundation models, when equipped with modular components, are not inherently constrained to their initial training scope but can evolve into multitask, multimodality systems for medical imaging. The code implementation of this work can be found at <a target="_blank" rel="noopener" href="https://github.com/Areeb2735/CTscan_prognosis_VLM">https://github.com/Areeb2735/CTscan_prognosis_VLM</a></p>
<blockquote>
<p>åŸºç¡€æ¨¡å‹åœ¨å¤§é‡æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä»¥æ•æ‰é¢†åŸŸçš„ä¸€èˆ¬è¶‹åŠ¿ã€‚ç„¶è€Œï¼Œåœ¨åŒ»å­¦æˆåƒé¢†åŸŸï¼Œæ•°æ®çš„ç¨€ç¼ºæ€§ä½¿å¾—é’ˆå¯¹æ¯ä¸ªé¢†åŸŸã€æ¨¡æ€æˆ–ä»»åŠ¡çš„é¢„è®­ç»ƒå˜å¾—å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æˆ‘ä»¬å¹¶ä¸ä¸»å¼ æ„å»ºå•ç‹¬æ¨¡å‹ï¼Œè€Œæ˜¯æå‡ºäº†MAFM^3ï¼ˆåŸºç¡€æ¨¡å‹çš„æ¨¡å—åŒ–é€‚åº”å¤šæ¨¡æ€åŒ»ç–—äººå·¥æ™ºèƒ½ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ¡†æ¶ï¼Œå®ƒå…è®¸å•ä¸ªåŸºç¡€æ¨¡å‹é€šè¿‡è½»é‡çº§æ¨¡å—åŒ–ç»„ä»¶æ‰©å±•åˆ°å¤šä¸ªé¢†åŸŸã€ä»»åŠ¡å’Œæ¨¡æ€ã€‚è¿™äº›ç»„ä»¶å……å½“äº†ä¸“é—¨æŠ€èƒ½é›†ï¼Œä½¿å¾—ç³»ç»Ÿå¯ä»¥æ ¹æ®è¾“å…¥ç±»å‹æˆ–ä¸´åºŠç›®æ ‡çµæ´»åœ°æ¿€æ´»ç›¸åº”èƒ½åŠ›åœ¨æ¨æ–­æ—¶ã€‚ä¸ä¼ ç»Ÿçš„éš”ç¦»å¤„ç†æ¯ä¸ªæ–°ä»»åŠ¡æˆ–æ¨¡æ€çš„é€‚åº”æ–¹æ³•ä¸åŒï¼ŒMAFM^3æä¾›äº†ä¸€ä¸ªç»Ÿä¸€ä¸”å¯æ‰©å±•çš„æ¡†æ¶ï¼Œå¯å®ç°é«˜æ•ˆçš„å¤šä»»åŠ¡å’Œå¤šæ¨¡æ€é€‚åº”ã€‚æˆ‘ä»¬é€šè¿‡å°†åˆå§‹ç”¨äºåˆ†ç±»çš„èƒ¸éƒ¨CTåŸºç¡€æ¨¡å‹æ”¹ç¼–ä¸ºé¢„åå’Œåˆ†å‰²æ¨¡å—æ¥éªŒè¯æˆ‘ä»¬çš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„ç»“æœæ˜¾ç¤ºè¿™ä¸¤ä¸ªä»»åŠ¡ä¸Šçš„æ€§èƒ½éƒ½æœ‰æ‰€æé«˜ã€‚æ­¤å¤–ï¼Œé€šè¿‡ç»“åˆPETæ‰«æï¼ŒMAFM^3åœ¨Diceå¾—åˆ†ä¸Šå®ç°äº†ä¸å„è‡ªåŸºçº¿ç›¸æ¯”æé«˜äº†5%ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œå½“é…å¤‡æ¨¡å—åŒ–ç»„ä»¶æ—¶ï¼ŒåŸºç¡€æ¨¡å‹å¹¶ä¸å±€é™äºå…¶åˆå§‹è®­ç»ƒèŒƒå›´ï¼Œè€Œæ˜¯å¯ä»¥æ¼”å˜ä¸ºå¤šä»»åŠ¡ã€å¤šæ¨¡æ€åŒ»å­¦æˆåƒç³»ç»Ÿã€‚è¯¥å·¥ä½œçš„ä»£ç å®ç°å¯ä»¥åœ¨<a target="_blank" rel="noopener" href="https://github.com/Areeb2735/CTscan_prognosis_VLM%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Areeb2735/CTscan_prognosis_VLMæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.11212v1">PDF</a> 2 figures, 3 tables</p>
<p><strong>Summary</strong><br>    åŒ»å­¦å›¾åƒé¢†åŸŸæå‡ºMAFM^3æ¡†æ¶ï¼Œé€šè¿‡å•ä¸€åŸºç¡€æ¨¡å‹ç»“åˆè½»é‡çº§æ¨¡å—åŒ–ç»„ä»¶ï¼Œå®ç°æ¨¡å‹åœ¨å¤šåŸŸã€å¤šä»»åŠ¡å’Œå¤šæ¨¡æ€ä¸‹çš„çµæ´»æ‰©å±•ã€‚æ¡†æ¶èƒ½ä½¿ç³»ç»Ÿæ ¹æ®è¾“å…¥ç±»å‹æˆ–ä¸´åºŠç›®æ ‡ï¼Œçµæ´»æ¿€æ´»ç›¸åº”èƒ½åŠ›ã€‚ç»éªŒè¯ï¼Œè¯¥æ¡†æ¶åœ¨èƒ¸éƒ¨CTå›¾åƒåˆ†ç±»åŸºç¡€æ¨¡å‹è½¬åŒ–ä¸ºé¢„åå’Œåˆ†å‰²æ¨¡å—æ—¶è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå¹¶åœ¨èåˆPETæ‰«ææ—¶è¿›ä¸€æ­¥æé«˜Diceå¾—åˆ†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŸºç¡€æ¨¡å‹åœ¨åŒ»å­¦æˆåƒé¢†åŸŸé¢ä¸´æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œéš¾ä»¥è¿›è¡Œæ¯ä¸ªåŸŸã€æ¨¡å¼æˆ–ä»»åŠ¡çš„é¢„è®­ç»ƒã€‚</li>
<li>æå‡ºMAFM^3æ¡†æ¶ï¼Œä½¿å•ä¸€åŸºç¡€æ¨¡å‹èƒ½æ‰©å±•åˆ°å¤šä¸ªåŸŸã€ä»»åŠ¡çš„æ¨¡å¼ã€‚</li>
<li>MAFM^3æ¡†æ¶åˆ©ç”¨è½»é‡çº§æ¨¡å—åŒ–ç»„ä»¶ï¼Œä½œä¸ºä¸“é—¨æŠ€èƒ½é›†ï¼Œä½¿ç³»ç»Ÿèƒ½çµæ´»é€‚åº”ä¸åŒä»»åŠ¡æˆ–æ¨¡æ€ã€‚</li>
<li>ä¸ä¼ ç»Ÿé€‚åº”æ–¹æ³•ä¸åŒï¼ŒMAFM^3æä¾›ç»Ÿä¸€ã€å¯æ‰©å±•çš„æ¡†æ¶ï¼Œå®ç°å¤šä»»åŠ¡å’Œå¤šæ¨¡æ€çš„æœ‰æ•ˆé€‚åº”ã€‚</li>
<li>é€šè¿‡å°†èƒ¸éƒ¨CTåˆ†ç±»åŸºç¡€æ¨¡å‹è½¬åŒ–ä¸ºé¢„åå’Œåˆ†å‰²æ¨¡å—ï¼ŒéªŒè¯äº†MAFM^3æ–¹æ³•çš„ä¼˜è¶Šæ€§ã€‚</li>
<li>ç»“åˆPETæ‰«æï¼ŒMAFM^3æ¡†æ¶åœ¨Diceå¾—åˆ†ä¸Šæé«˜äº†5%ï¼Œç›¸æ¯”åŸºçº¿æœ‰æ˜æ˜¾æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.11212">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-155ada9630978b82c521665965b631cf" align="middle">
<img src="https://picx.zhimg.com/v2-81c4ea0e36e96093dd37f56bc5602584" align="middle">
<img src="https://picx.zhimg.com/v2-00ee76d010274272f6052094e85f1b7e" align="middle">
<img src="https://picx.zhimg.com/v2-9e2af404f0e55fdba383ba50d73bacad" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Machine-Learning-Based-Detection-of-Coronary-Artery-Calcification-Using-Synthetic-Chest-X-Rays"><a href="#Machine-Learning-Based-Detection-of-Coronary-Artery-Calcification-Using-Synthetic-Chest-X-Rays" class="headerlink" title="Machine-Learning Based Detection of Coronary Artery Calcification Using Synthetic Chest X-Rays"></a>Machine-Learning Based Detection of Coronary Artery Calcification Using Synthetic Chest X-Rays</h2><p><strong>Authors:Dylan Saeed, Ramtin Gharleghi, Susann Bier, Sonit Singh</strong></p>
<p>Coronary artery calcification (CAC) is a strong predictor of cardiovascular events, with CT-based Agatston scoring widely regarded as the clinical gold standard. However, CT is costly and impractical for large-scale screening, while chest X-rays (CXRs) are inexpensive but lack reliable ground truth labels, constraining deep learning development. Digitally reconstructed radiographs (DRRs) offer a scalable alternative by projecting CT volumes into CXR-like images while inheriting precise labels. In this work, we provide the first systematic evaluation of DRRs as a surrogate training domain for CAC detection. Using 667 CT scans from the COCA dataset, we generate synthetic DRRs and assess model capacity, super-resolution fidelity enhancement, preprocessing, and training strategies. Lightweight CNNs trained from scratch outperform large pretrained networks; pairing super-resolution with contrast enhancement yields significant gains; and curriculum learning stabilises training under weak supervision. Our best configuration achieves a mean AUC of 0.754, comparable to or exceeding prior CXR-based studies. These results establish DRRs as a scalable, label-rich foundation for CAC detection, while laying the foundation for future transfer learning and domain adaptation to real CXRs.</p>
<blockquote>
<p>å† çŠ¶åŠ¨è„‰é’™åŒ–ï¼ˆCACï¼‰æ˜¯å¿ƒè¡€ç®¡ç–¾ç—…äº‹ä»¶çš„å¼ºçƒˆé¢„æµ‹å› å­ï¼ŒåŸºäºCTçš„Agatstonè¯„åˆ†è¢«å¹¿æ³›åº”ç”¨äºä¸´åºŠé»„é‡‘æ ‡å‡†ã€‚ç„¶è€Œï¼ŒCTæˆæœ¬é«˜æ˜‚ï¼Œä¸é€‚ç”¨äºå¤§è§„æ¨¡ç­›æŸ¥ï¼Œè€Œèƒ¸éƒ¨Xå°„çº¿ï¼ˆCXRsï¼‰ä»·æ ¼ä½å»‰ï¼Œä½†ç¼ºä¹å¯é çš„çœŸå®æ ‡ç­¾ï¼Œé™åˆ¶äº†æ·±åº¦å­¦ä¹ çš„å‘å±•ã€‚æ•°å­—é‡å»ºå›¾åƒï¼ˆDRRsï¼‰é€šè¿‡å°†ä»CTä½“ç§¯æŠ•å½±åˆ°ç±»ä¼¼CXRçš„å›¾åƒä¸­ï¼ŒåŒæ—¶ç»§æ‰¿ç²¾ç¡®æ ‡ç­¾ï¼Œæä¾›äº†ä¸€ç§å¯æ‰©å±•çš„æ›¿ä»£æ–¹æ¡ˆã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é¦–æ¬¡å¯¹DRRsä½œä¸ºCACæ£€æµ‹æ›¿ä»£è®­ç»ƒåŸŸè¿›è¡Œäº†ç³»ç»Ÿè¯„ä¼°ã€‚ä½¿ç”¨COCAæ•°æ®é›†çš„667ä¸ªCTæ‰«æï¼Œæˆ‘ä»¬ç”ŸæˆåˆæˆDRRsï¼Œå¹¶è¯„ä¼°æ¨¡å‹å®¹é‡ã€è¶…åˆ†è¾¨ç‡ä¿çœŸåº¦å¢å¼ºã€é¢„å¤„ç†å’Œè®­ç»ƒç­–ç•¥ã€‚ä»å¤´å¼€å§‹è®­ç»ƒçš„è½»å‹CNNä¼˜äºå¤§å‹é¢„è®­ç»ƒç½‘ç»œï¼›é…å¯¹è¶…åˆ†è¾¨ç‡ä¸å¯¹æ¯”åº¦å¢å¼ºå¯äº§ç”Ÿé‡å¤§æ”¶ç›Šï¼›è¯¾ç¨‹å­¦ä¹ å¯åœ¨å¼±ç›‘ç£ä¸‹ç¨³å®šè®­ç»ƒã€‚æˆ‘ä»¬çš„æœ€ä½³é…ç½®è¾¾åˆ°äº†å¹³å‡AUCä¸º0.754ï¼Œå¯ä¸æˆ–ä¼˜äºå…ˆå‰çš„åŸºäºCXRçš„ç ”ç©¶ã€‚è¿™äº›ç»“æœå°†DRRsç¡®ç«‹ä¸ºCACæ£€æµ‹çš„å¯æ‰©å±•ã€æ ‡ç­¾ä¸°å¯Œçš„åŸºçŸ³ï¼ŒåŒæ—¶ä¸ºæœªæ¥çš„è¿ç§»å­¦ä¹ å’Œå¯¹çœŸå®CXRsçš„é¢†åŸŸé€‚åº”å¥ å®šäº†åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.11093v1">PDF</a> 10 pages, 5 figures. Under review for MIDL 2026</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†æ•°å­—é‡å»ºæ”¾å°„å½±åƒï¼ˆDRRsï¼‰ä½œä¸ºå† çŠ¶åŠ¨è„‰é’™åŒ–ï¼ˆCACï¼‰æ£€æµ‹çš„æ›¿ä»£è®­ç»ƒé¢†åŸŸçš„åº”ç”¨ã€‚é€šè¿‡å¯¹COCAæ•°æ®é›†çš„667ä»½CTæ‰«æç”ŸæˆåˆæˆDRRsï¼Œè¯„ä¼°äº†æ¨¡å‹å®¹é‡ã€è¶…åˆ†è¾¨ç‡ä¿çœŸåº¦å¢å¼ºã€é¢„å¤„ç†å’Œè®­ç»ƒç­–ç•¥ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œè½»é‡çº§CNNè¡¨ç°ä¼˜äºå¤§å‹é¢„è®­ç»ƒç½‘ç»œï¼›ç»“åˆè¶…åˆ†è¾¨ç‡ä¸å¯¹æ¯”åº¦å¢å¼ºæœ‰æ˜æ˜¾æå‡ï¼›è¯¾ç¨‹å­¦ä¹ åœ¨å¼±ç›‘ç£ä¸‹ç¨³å®šè®­ç»ƒã€‚æœ€ä½³é…ç½®çš„å¹³å‡AUCè¾¾åˆ°0.754ï¼Œä¸å…ˆå‰çš„CXRç ”ç©¶ç›¸å½“æˆ–æ›´ä¼˜ç§€ã€‚DRRsä½œä¸ºä¸€ç§å¯ä¼¸ç¼©ã€æ ‡ç­¾ä¸°å¯Œçš„CACæ£€æµ‹åŸºç¡€å·²å¾—åˆ°éªŒè¯ï¼Œä¸ºæœªæ¥å‘çœŸå®CXRsçš„è¿ç§»å­¦ä¹ å’ŒåŸŸé€‚åº”å¥ å®šäº†åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å† çŠ¶åŠ¨è„‰é’™åŒ–ï¼ˆCACï¼‰æ˜¯å¿ƒè¡€ç®¡ç–¾ç—…äº‹ä»¶çš„å¼ºçƒˆé¢„æµ‹å› ç´ ï¼ŒCTåŸºç¡€ä¸Šçš„Agatstonè¯„åˆ†æ˜¯ä¸´åºŠé‡‘æ ‡å‡†ã€‚</li>
<li>CTæˆæœ¬é«˜æ˜‚ï¼Œä¸é€‚ç”¨äºå¤§è§„æ¨¡ç­›æŸ¥ï¼Œè€Œèƒ¸éƒ¨Xå…‰ç‰‡ï¼ˆCXRsï¼‰è™½ä»·æ ¼ä½å»‰ä½†ç¼ºä¹å¯é çš„åœ°é¢çœŸå®æ ‡ç­¾ï¼Œåˆ¶çº¦äº†æ·±åº¦å­¦ä¹ çš„å‘å±•ã€‚</li>
<li>æ•°å­—é‡å»ºæ”¾å°„å½±åƒï¼ˆDRRsï¼‰é€šè¿‡å°†CTä½“ç§¯æŠ•å½±æˆCXRæ ·å¼çš„å›¾åƒï¼ŒåŒæ—¶ç»§æ‰¿ç²¾ç¡®æ ‡ç­¾ï¼Œæä¾›äº†ä¸€ç§å¯æ‰©å±•çš„æ›¿ä»£æ–¹æ¡ˆã€‚</li>
<li>æœ¬ç ”ç©¶é¦–æ¬¡å¯¹DRRsä½œä¸ºCACæ£€æµ‹æ›¿ä»£è®­ç»ƒåŸŸè¿›è¡Œäº†ç³»ç»Ÿè¯„ä¼°ã€‚</li>
<li>ä½¿ç”¨è½»é‡çº§CNNä»åŸºç¡€å¼€å§‹è®­ç»ƒè¡¨ç°ä¼˜äºå¤§å‹é¢„è®­ç»ƒç½‘ç»œã€‚</li>
<li>ç»“åˆè¶…åˆ†è¾¨ç‡ä¸å¯¹æ¯”åº¦å¢å¼ºæŠ€æœ¯å¸¦æ¥äº†æ˜¾è‘—çš„æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.11093">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-808958abaf16e10842a284da21f9f24a" align="middle">
<img src="https://picx.zhimg.com/v2-e28f30738df022920807ef3b3fa75719" align="middle">
<img src="https://picx.zhimg.com/v2-9a6b701f238e88d1e12a364a98913004" align="middle">
<img src="https://picx.zhimg.com/v2-d8e64562a977abe524112cf8162a5511" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Highly-Polarized-Intrinsic-Emission-and-its-Orthogonal-Counterpart-in-Vela-X-1"><a href="#Highly-Polarized-Intrinsic-Emission-and-its-Orthogonal-Counterpart-in-Vela-X-1" class="headerlink" title="Highly Polarized Intrinsic Emission and its Orthogonal Counterpart in Vela X-1"></a>Highly Polarized Intrinsic Emission and its Orthogonal Counterpart in Vela X-1</h2><p><strong>Authors:WanYun Wu, Fei Xie, Long Ji, Mingyu Ge, Fabio La Monaca</strong></p>
<p>Vela X-1 is one of the most archetypal wind-fed X-ray pulsars (XRPs), and the emergence of its orthogonal polarization states reveals distinctive polarimetric properties. Using data from Imaging X-ray Polarimetry Explorer (IXPE) observations of Vela X-1, we perform a polarization analysis of Vela X-1 using a triple power-law spectral model absorbed by varying column densities, successfully isolating two physically distinct orthogonal polarized components. The first polarized component corresponds to emission from the accretion mound surface that is not obscured by the wind clumps, with its polarization degree (PD) exceeding 30%. In specific phase intervals, the PD reaches (50.9 \pm 10.7%). This marks the first detection of such highly polarized neutron star emission in an XRP. The second polarized component likely originates from complex physical processes within or near the accretion mound, with its PD showing a potential negative correlation with column density. Furthermore, by rotating the predicted polarization angle (PA) of the first polarized component by 90$^\circ$, we successfully achieve separate fitting and simultaneous fitting of the two orthogonal polarization states using the rotating vector model (RVM).</p>
<blockquote>
<p>Vela X-1æ˜¯æœ€å…¸å‹çš„å—é£é©±åŠ¨çš„Xå°„çº¿è„‰å†²æ˜Ÿä¹‹ä¸€ï¼Œå…¶æ­£äº¤åæŒ¯æ€çš„å‡ºç°æ­ç¤ºäº†ç‹¬ç‰¹çš„åæŒ¯ç‰¹æ€§ã€‚æˆ‘ä»¬ä½¿ç”¨æˆåƒXå°„çº¿åæŒ¯ä»ªï¼ˆIXPEï¼‰å¯¹Vela X-1çš„è§‚å¯Ÿæ•°æ®ï¼Œé‡‡ç”¨ä¸‰é‡å¹‚å¾‹è°±æ¨¡å‹ï¼ˆè¢«ä¸åŒæŸ±å¯†åº¦å¸æ”¶ï¼‰ï¼ŒæˆåŠŸåˆ†ç¦»å‡ºä¸¤ä¸ªç‰©ç†ä¸Šä¸åŒçš„æ­£äº¤åæŒ¯åˆ†é‡ã€‚ç¬¬ä¸€ä¸ªåæŒ¯åˆ†é‡å¯¹åº”äºæ¥è‡ªå †ç§¯å±±ä¸˜è¡¨é¢çš„å‘å°„ï¼Œè¯¥å‘å°„æœªè¢«é£å›¢é®æŒ¡ï¼Œå…¶åæŒ¯åº¦ï¼ˆPDï¼‰è¶…è¿‡30%ã€‚åœ¨ç‰¹å®šçš„ç›¸ä½é—´éš”å†…ï¼ŒPDè¾¾åˆ°(50.9 \pm 10.7%)ã€‚è¿™æ ‡å¿—ç€åœ¨Xå°„çº¿è„‰å†²æ˜Ÿä¸­é¦–æ¬¡æ£€æµ‹åˆ°å¦‚æ­¤é«˜åº¦åæŒ¯çš„ä¸­å­æ˜Ÿå‘å°„ã€‚ç¬¬äºŒä¸ªåæŒ¯åˆ†é‡å¯èƒ½æºäºå †ç§¯å±±ä¸˜å†…éƒ¨æˆ–è¿‘å¤„çš„å¤æ‚ç‰©ç†è¿‡ç¨‹ï¼Œå…¶PDä¸æŸ±å¯†åº¦å‘ˆæ½œåœ¨è´Ÿç›¸å…³ã€‚æ­¤å¤–ï¼Œé€šè¿‡å°†ç¬¬ä¸€ä¸ªåæŒ¯åˆ†é‡çš„é¢„æµ‹åæŒ¯è§’ï¼ˆPAï¼‰æ—‹è½¬90Â°ï¼Œæˆ‘ä»¬æˆåŠŸä½¿ç”¨æ—‹è½¬çŸ¢é‡æ¨¡å‹ï¼ˆRVMï¼‰å®ç°äº†ä¸¤ä¸ªæ­£äº¤åæŒ¯æ€çš„å•ç‹¬æ‹Ÿåˆå’ŒåŒæ—¶æ‹Ÿåˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.11075v1">PDF</a> 14 pages, 8 figures. Accepted for publication in APJ</p>
<p><strong>Summary</strong></p>
<p>Vela X-1çš„å…¸å‹é£å–‚Xå°„çº¿è„‰å†²æ˜Ÿï¼ˆXRPsï¼‰çš„æåŒ–ç‰¹æ€§è¢«ç ”ç©¶ã€‚åˆ©ç”¨æˆåƒXå°„çº¿åæŒ¯æ¢æµ‹å™¨ï¼ˆIXPEï¼‰è§‚æµ‹æ•°æ®ï¼Œé€šè¿‡ä¸‰é‡å¹‚å¾‹è°±æ¨¡å‹åˆ†æï¼ŒæˆåŠŸåˆ†ç¦»å‡ºä¸¤ä¸ªç‰©ç†ä¸Šæ­£äº¤åæŒ¯åˆ†é‡ã€‚é¦–ä¸ªåæŒ¯æˆåˆ†æ¥è‡ªæœªå—é£å›¢é®è”½çš„å¸ç§¯å †è¡¨é¢ï¼ŒåæŒ¯åº¦ï¼ˆPDï¼‰è¶…è¿‡30%ï¼Œç‰¹å®šåŒºé—´å¯è¾¾50.9Â±10.7%ã€‚è¿™æ˜¯XRPä¸­é¦–æ¬¡æ£€æµ‹åˆ°å¦‚æ­¤é«˜åæŒ¯çš„ä¸­å­æ˜Ÿå‘å°„ã€‚ç¬¬äºŒä¸ªåæŒ¯æˆåˆ†å¯èƒ½æºäºå¸ç§¯å †å†…éƒ¨æˆ–è¿‘å¤„çš„å¤æ‚ç‰©ç†è¿‡ç¨‹ï¼Œå…¶PDä¸æŸ±å¯†åº¦å‘ˆæ½œåœ¨è´Ÿç›¸å…³ã€‚é€šè¿‡æ—‹è½¬é¢„æµ‹çš„ç¬¬ä¸€åæŒ¯æˆåˆ†çš„åæŒ¯è§’ï¼ˆPAï¼‰90Â°ï¼ŒæˆåŠŸç”¨æ—‹è½¬çŸ¢é‡æ¨¡å‹ï¼ˆRVMï¼‰åˆ†åˆ«æ‹Ÿåˆå’ŒåŒæ—¶æ‹Ÿåˆä¸¤ä¸ªæ­£äº¤åæŒ¯çŠ¶æ€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Vela X-1æ˜¯å…¸å‹çš„é£å–‚Xå°„çº¿è„‰å†²æ˜Ÿï¼Œå…·æœ‰ç‹¬ç‰¹çš„åæŒ¯ç‰¹æ€§ã€‚</li>
<li>é€šè¿‡IXPEè§‚æµ‹æ•°æ®ï¼Œå‘ç°äº†ä¸¤ä¸ªç‰©ç†ä¸Šæ­£äº¤åæŒ¯åˆ†é‡ã€‚</li>
<li>é¦–ä¸ªåæŒ¯æˆåˆ†æ¥è‡ªå¸ç§¯å †è¡¨é¢ï¼ŒåæŒ¯åº¦è¶…è¿‡30%ï¼Œåœ¨ç‰¹å®šåŒºé—´å¯è¾¾50.9%ã€‚</li>
<li>è¿™æ˜¯XRPä¸­é¦–æ¬¡æ£€æµ‹åˆ°å¦‚æ­¤é«˜åæŒ¯çš„ä¸­å­æ˜Ÿå‘å°„ã€‚</li>
<li>ç¬¬äºŒä¸ªåæŒ¯æˆåˆ†å¯èƒ½æºäºå¤æ‚ç‰©ç†è¿‡ç¨‹ï¼Œå…¶åæŒ¯åº¦ä¸æŸ±å¯†åº¦å­˜åœ¨æ½œåœ¨è´Ÿç›¸å…³ã€‚</li>
<li>é€šè¿‡æ—‹è½¬é¢„æµ‹çš„ç¬¬ä¸€åæŒ¯æˆåˆ†çš„åæŒ¯è§’ï¼Œå®ç°äº†å¯¹ä¸¤ä¸ªæ­£äº¤åæŒ¯çŠ¶æ€çš„åˆ†åˆ«å’ŒåŒæ—¶æ‹Ÿåˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.11075">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-781106f1cf9798ac6646d66e95fb0853" align="middle">
<img src="https://picx.zhimg.com/v2-b2a4ee6b4289955c070877f00ed59c4a" align="middle">
<img src="https://picx.zhimg.com/v2-db79619b90bec66ca634d2542c56f3d9" align="middle">
<img src="https://picx.zhimg.com/v2-5e41b5692f27f1dd595b15d65f2cf169" align="middle">
<img src="https://picx.zhimg.com/v2-efb5c57e8ff61942d19cb3be29795c38" align="middle">
<img src="https://picx.zhimg.com/v2-85bb67dc75b7af70c1fbe3dabd564942" align="middle">
<img src="https://picx.zhimg.com/v2-336773961e8fefc8af4dce7340e8269f" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="S2D-ALIGN-Shallow-to-Deep-Auxiliary-Learning-for-Anatomically-Grounded-Radiology-Report-Generation"><a href="#S2D-ALIGN-Shallow-to-Deep-Auxiliary-Learning-for-Anatomically-Grounded-Radiology-Report-Generation" class="headerlink" title="S2D-ALIGN: Shallow-to-Deep Auxiliary Learning for Anatomically-Grounded Radiology Report Generation"></a>S2D-ALIGN: Shallow-to-Deep Auxiliary Learning for Anatomically-Grounded Radiology Report Generation</h2><p><strong>Authors:Jiechao Gao, Chang Liu, Yuangang Li</strong></p>
<p>Radiology Report Generation (RRG) aims to automatically generate diagnostic reports from radiology images. To achieve this, existing methods have leveraged the powerful cross-modal generation capabilities of Multimodal Large Language Models (MLLMs), primarily focusing on optimizing cross-modal alignment between radiographs and reports through Supervised Fine-Tuning (SFT). However, by only performing instance-level alignment with the image-text pairs, the standard SFT paradigm fails to establish anatomically-grounded alignment, where the templated nature of reports often leads to sub-optimal generation quality. To address this, we propose \textsc{S2D-Align}, a novel SFT paradigm that establishes anatomically-grounded alignment by leveraging auxiliary signals of varying granularities. \textsc{S2D-Align} implements a shallow-to-deep strategy, progressively enriching the alignment process: it begins with the coarse radiograph-report pairing, then introduces reference reports for instance-level guidance, and ultimately utilizes key phrases to ground the generation in specific anatomical details. To bridge the different alignment stages, we introduce a memory-based adapter that empowers feature sharing, thereby integrating coarse and fine-grained guidance. For evaluation, we conduct experiments on the public \textsc{MIMIC-CXR} and \textsc{IU X-Ray} benchmarks, where \textsc{S2D-Align} achieves state-of-the-art performance compared to existing methods. Ablation studies validate the effectiveness of our multi-stage, auxiliary-guided approach, highlighting a promising direction for enhancing grounding capabilities in complex, multi-modal generation tasks.</p>
<blockquote>
<p>æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆï¼ˆRRGï¼‰æ—¨åœ¨è‡ªåŠ¨æ ¹æ®æ”¾å°„å­¦å›¾åƒç”Ÿæˆè¯Šæ–­æŠ¥å‘Šã€‚ä¸ºå®ç°è¿™ä¸€ç›®æ ‡ï¼Œç°æœ‰æ–¹æ³•å·²ç»åˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„è·¨æ¨¡æ€ç”Ÿæˆèƒ½åŠ›ï¼Œä¸»è¦å…³æ³¨é€šè¿‡æœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ä¼˜åŒ–æ”¾å°„å›¾åƒä¸æŠ¥å‘Šä¹‹é—´çš„è·¨æ¨¡æ€å¯¹é½ã€‚ç„¶è€Œï¼Œä»…é€šè¿‡å®ä¾‹çº§å¯¹é½è¿›è¡Œå›¾åƒæ–‡æœ¬å¯¹çš„æ–¹æ³•ï¼Œæ ‡å‡†SFTèŒƒå¼æœªèƒ½å»ºç«‹åŸºäºè§£å‰–ç»“æ„çš„å¯¹é½ï¼ŒæŠ¥å‘Šçš„æ¨¡æ¿åŒ–æ€§è´¨å¾€å¾€å¯¼è‡´ç”Ÿæˆè´¨é‡ä¸ä½³ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„SFTèŒƒå¼â€”â€”S2D-Alignã€‚å®ƒé€šè¿‡åˆ©ç”¨ä¸åŒç²’åº¦çš„è¾…åŠ©ä¿¡å·ï¼Œå»ºç«‹åŸºäºè§£å‰–ç»“æ„çš„å¯¹é½ã€‚S2D-Aligné‡‡ç”¨ä»æµ…åˆ°æ·±çš„ç­–ç•¥ï¼Œé€æ­¥ä¸°å¯Œå¯¹é½è¿‡ç¨‹ï¼šå®ƒä»ç²—ç•¥çš„æ”¾å°„å›¾åƒæŠ¥å‘Šé…å¯¹å¼€å§‹ï¼Œç„¶åå¼•å…¥å‚è€ƒæŠ¥å‘Šè¿›è¡Œå®ä¾‹çº§æŒ‡å¯¼ï¼Œå¹¶æœ€ç»ˆåˆ©ç”¨å…³é”®çŸ­è¯­å°†ç”Ÿæˆç»†åŒ–åˆ°ç‰¹å®šçš„è§£å‰–ç»†èŠ‚ã€‚ä¸ºäº†æ¡¥æ¥ä¸åŒçš„å¯¹é½é˜¶æ®µï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºå†…å­˜çš„é€‚é…å™¨ï¼Œä»¥å®ç°ç‰¹å¾å…±äº«ï¼Œä»è€Œæ•´åˆç²—ç•¥å’Œç²¾ç»†çš„å¼•å¯¼ã€‚ä¸ºäº†è¿›è¡Œè¯„ä¼°ï¼Œæˆ‘ä»¬åœ¨å…¬å…±çš„MIMIC-CXRå’ŒIU X-RayåŸºå‡†ä¸Šè¿›è¡Œäº†å®éªŒï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒS2D-Alignå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æ¶ˆèç ”ç©¶éªŒè¯äº†æˆ‘ä»¬çš„å¤šé˜¶æ®µè¾…åŠ©å¼•å¯¼æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œä¸ºå¢å¼ºå¤æ‚å¤šæ¨¡æ€ç”Ÿæˆä»»åŠ¡çš„å®šä½èƒ½åŠ›æŒ‡å‡ºäº†æœ‰å¸Œæœ›çš„æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.11066v1">PDF</a> </p>
<p><strong>Summary</strong><br>    æœ¬ç ”ç©¶æå‡ºäº†ä¸€ä¸ªåŸºäºæ·±åº¦å­¦ä¹ çš„æ–°æ–¹æ³•â€”â€”S2D-Alignï¼Œæ—¨åœ¨æé«˜æ”¾å°„æŠ¥å‘Šç”Ÿæˆè´¨é‡ã€‚é€šè¿‡å¯¹å¤šæ¨¡æ€å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„è®­ç»ƒæ”¹è¿›ï¼Œè¯¥ç®—æ³•å¯å®ç°ä»ç²—åˆ°ç»†çš„å±‚æ¬¡æ€§å½±åƒæŠ¥å‘ŠåŒ¹é…å’Œè§£å‰–ç‰¹å¾æè¿°çš„å¯¹é½ï¼Œä»¥æå‡è‡ªåŠ¨ç”Ÿæˆçš„è¯Šæ–­æŠ¥å‘Šçš„å‡†ç¡®æ€§ã€‚åœ¨å…¬å¼€æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒS2D-Alignç›¸è¾ƒäºç°æœ‰æ–¹æ³•å–å¾—äº†æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Radiology Report Generation (RRG)ç›®æ ‡æ˜¯è‡ªåŠ¨ä»æ”¾å°„å­¦å›¾åƒç”Ÿæˆè¯Šæ–­æŠ¥å‘Šã€‚</li>
<li>å½“å‰æ–¹æ³•ä¸»è¦åˆ©ç”¨å¤šæ¨¡æ€å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„è·¨æ¨¡æ€ç”Ÿæˆèƒ½åŠ›ï¼Œé€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ä¼˜åŒ–æ”¾å°„å›¾åƒä¸æŠ¥å‘Šä¹‹é—´çš„è·¨æ¨¡æ€å¯¹é½ã€‚</li>
<li>ç°æœ‰SFTèŒƒå¼å­˜åœ¨ç¼ºé™·ï¼Œä»…è¿›è¡Œå®ä¾‹çº§å¯¹é½ï¼Œæœªèƒ½å»ºç«‹åŸºäºè§£å‰–ç»“æ„çš„å¯¹é½ï¼Œå¯¼è‡´æŠ¥å‘Šç”Ÿæˆè´¨é‡ä¸ä½³ã€‚</li>
<li>S2D-Alignæ˜¯ä¸€ç§æ–°å‹çš„SFTèŒƒå¼ï¼Œé€šè¿‡åˆ©ç”¨ä¸åŒç²’åº¦çš„è¾…åŠ©ä¿¡å·å»ºç«‹è§£å‰–ç»“æ„ä¸ºåŸºç¡€çš„å¯¹é½ã€‚</li>
<li>S2D-Aligné‡‡ç”¨ç”±æµ…åˆ°æ·±çš„ç­–ç•¥ï¼Œé€æ­¥ä¸°å¯Œå¯¹é½è¿‡ç¨‹ï¼ŒåŒ…æ‹¬ç²—çº§æ”¾å°„å›¾åƒæŠ¥å‘Šé…å¯¹ã€å¼•å…¥å‚è€ƒæŠ¥å‘Šè¿›è¡Œå®ä¾‹çº§æŒ‡å¯¼ä»¥åŠåˆ©ç”¨å…³é”®çŸ­è¯­å¯¹ç‰¹å®šè§£å‰–ç»†èŠ‚è¿›è¡Œå®šä½ã€‚</li>
<li>ä¸ºè¡”æ¥ä¸åŒçš„å¯¹é½é˜¶æ®µï¼Œå¼•å…¥äº†åŸºäºå†…å­˜çš„é€‚é…å™¨ï¼Œå®ç°ç‰¹å¾å…±äº«ï¼Œä»è€Œæ•´åˆç²—ç²’åº¦å’Œç»†ç²’åº¦æŒ‡å¯¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.11066">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8a276d16690612a5c14d400f8f53b9ff" align="middle">
<img src="https://picx.zhimg.com/v2-4a101e8eb2772c1daed8fbacf7f07964" align="middle">
<img src="https://picx.zhimg.com/v2-168e3f202d2a0a60b41cd5cd920b3a4b" align="middle">
<img src="https://picx.zhimg.com/v2-3fd42305d14ba9061dadb532afd561c2" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="PINGS-X-Physics-Informed-Normalized-Gaussian-Splatting-with-Axes-Alignment-for-Efficient-Super-Resolution-of-4D-Flow-MRI"><a href="#PINGS-X-Physics-Informed-Normalized-Gaussian-Splatting-with-Axes-Alignment-for-Efficient-Super-Resolution-of-4D-Flow-MRI" class="headerlink" title="PINGS-X: Physics-Informed Normalized Gaussian Splatting with Axes Alignment for Efficient Super-Resolution of 4D Flow MRI"></a>PINGS-X: Physics-Informed Normalized Gaussian Splatting with Axes Alignment for Efficient Super-Resolution of 4D Flow MRI</h2><p><strong>Authors:Sun Jo, Seok Young Hong, JinHyun Kim, Seungmin Kang, Ahjin Choi, Don-Gwan An, Simon Song, Je Hyeong Hong</strong></p>
<p>4D flow magnetic resonance imaging (MRI) is a reliable, non-invasive approach for estimating blood flow velocities, vital for cardiovascular diagnostics. Unlike conventional MRI focused on anatomical structures, 4D flow MRI requires high spatiotemporal resolution for early detection of critical conditions such as stenosis or aneurysms. However, achieving such resolution typically results in prolonged scan times, creating a trade-off between acquisition speed and prediction accuracy. Recent studies have leveraged physics-informed neural networks (PINNs) for super-resolution of MRI data, but their practical applicability is limited as the prohibitively slow training process must be performed for each patient. To overcome this limitation, we propose PINGS-X, a novel framework modeling high-resolution flow velocities using axes-aligned spatiotemporal Gaussian representations. Inspired by the effectiveness of 3D Gaussian splatting (3DGS) in novel view synthesis, PINGS-X extends this concept through several non-trivial novel innovations: (i) normalized Gaussian splatting with a formal convergence guarantee, (ii) axes-aligned Gaussians that simplify training for high-dimensional data while preserving accuracy and the convergence guarantee, and (iii) a Gaussian merging procedure to prevent degenerate solutions and boost computational efficiency. Experimental results on computational fluid dynamics (CFD) and real 4D flow MRI datasets demonstrate that PINGS-X substantially reduces training time while achieving superior super-resolution accuracy. Our code and datasets are available at <a target="_blank" rel="noopener" href="https://github.com/SpatialAILab/PINGS-X">https://github.com/SpatialAILab/PINGS-X</a>.</p>
<blockquote>
<p>å››ç»´è¡€æµç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰æ˜¯ä¸€ç§å¯é ã€æ— åˆ›çš„è¡€æµé€Ÿåº¦ä¼°è®¡æ–¹æ³•ï¼Œå¯¹å¿ƒè¡€ç®¡ç–¾ç—…çš„è¯Šæ–­è‡³å…³é‡è¦ã€‚ä¸ä¼ ç»Ÿçš„ä¸“æ³¨äºè§£å‰–ç»“æ„çš„MRIä¸åŒï¼Œå››ç»´è¡€æµMRIéœ€è¦é«˜çš„æ—¶ç©ºåˆ†è¾¨ç‡æ¥æ—©æœŸæ£€æµ‹å…³é”®ç–¾ç—…ï¼Œå¦‚ç‹­çª„æˆ–åŠ¨è„‰ç˜¤ã€‚ç„¶è€Œï¼Œå®ç°è¿™ç§åˆ†è¾¨ç‡é€šå¸¸ä¼šå¯¼è‡´æ‰«ææ—¶é—´å»¶é•¿ï¼Œéœ€è¦åœ¨é‡‡é›†é€Ÿåº¦å’Œé¢„æµ‹ç²¾åº¦ä¹‹é—´è¿›è¡Œæƒè¡¡ã€‚æœ€è¿‘çš„ç ”ç©¶å·²ç»åˆ©ç”¨ç‰©ç†ä¿¡æ¯ç¥ç»ç½‘ç»œï¼ˆPINNsï¼‰è¿›è¡ŒMRIæ•°æ®çš„è¶…åˆ†è¾¨ç‡å¤„ç†ï¼Œä½†å®ƒä»¬çš„å®é™…é€‚ç”¨æ€§å—åˆ°é™åˆ¶ï¼Œå› ä¸ºæ¯ä¸ªæ‚£è€…çš„è®­ç»ƒè¿‡ç¨‹æå…¶ç¼“æ…¢ã€‚ä¸ºäº†å…‹æœè¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†PINGS-Xï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹æ¡†æ¶ï¼Œåˆ©ç”¨è½´å¯¹é½çš„æ—¶ç©ºé«˜æ–¯è¡¨ç¤ºå¯¹é«˜åˆ†è¾¨ç‡è¡€æµé€Ÿåº¦è¿›è¡Œå»ºæ¨¡ã€‚PINGS-Xå—åˆ°ä¸‰ç»´é«˜æ–¯å–·æ¶‚ï¼ˆ3DGSï¼‰åœ¨æ–°å‹è§†å›¾åˆæˆä¸­çš„æœ‰æ•ˆæ€§çš„å¯å‘ï¼Œé€šè¿‡å‡ ä¸ªéå¹³å‡¡çš„åˆ›æ–°æ‰©å±•äº†è¿™ä¸€æ¦‚å¿µï¼šï¼ˆiï¼‰å…·æœ‰æ­£å¼æ”¶æ•›ä¿è¯çš„å½’ä¸€åŒ–é«˜æ–¯å–·æ¶‚ï¼›ï¼ˆiiï¼‰è½´å¯¹é½çš„é«˜æ–¯ç®€åŒ–äº†é«˜ç»´æ•°æ®çš„è®­ç»ƒè¿‡ç¨‹ï¼ŒåŒæ—¶ä¿ç•™äº†ç²¾åº¦å’Œæ”¶æ•›æ€§çš„ä¿è¯ï¼›ï¼ˆiiiï¼‰ä¸€ä¸ªé«˜æ–¯åˆå¹¶è¿‡ç¨‹ï¼Œä»¥é˜²æ­¢é€€åŒ–è§£å†³æ–¹æ¡ˆå¹¶æé«˜å·¥ä½œæ•ˆç‡ã€‚åœ¨è®¡ç®—æµä½“åŠ¨åŠ›å­¦ï¼ˆCFDï¼‰å’ŒçœŸå®å››ç»´è¡€æµMRIæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒPINGS-Xåœ¨å‡å°‘è®­ç»ƒæ—¶é—´çš„åŒæ—¶ï¼Œå®ç°äº†ä¼˜è¶Šçš„è¶…åˆ†è¾¨ç‡ç²¾åº¦ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/SpatialAILab/PINGS-X%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/SpatialAILab/PINGS-Xæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.11048v1">PDF</a> Accepted at AAAI 2026. Supplementary material included after references. 27 pages, 21 figures, 11 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†4Dæµç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰åœ¨ä¼°è®¡è¡€æµé€Ÿåº¦æ–¹é¢çš„å¯é æ€§å’Œé‡è¦æ€§ï¼Œè¿™å¯¹å¿ƒè¡€ç®¡è¯Šæ–­è‡³å…³é‡è¦ã€‚æ–‡ç« å¼ºè°ƒäº†å®ç°é«˜æ—¶ç©ºåˆ†è¾¨ç‡ä»¥æ—©æœŸæ£€æµ‹å…³é”®ç—…å˜å¦‚ç‹­çª„æˆ–åŠ¨è„‰ç˜¤çš„å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºPINGS-Xçš„æ–°å‹æ¡†æ¶ï¼Œå®ƒé€šè¿‡è½´å¯¹é½çš„æ—¶ç©ºé«˜æ–¯è¡¨ç¤ºå¯¹é«˜åˆ†è¾¨ç‡æµé€Ÿè¿›è¡Œå»ºæ¨¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPINGS-Xåœ¨å‡å°‘è®­ç»ƒæ—¶é—´çš„åŒæ—¶å®ç°äº†è¾ƒé«˜çš„è¶…åˆ†è¾¨ç‡ç²¾åº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>4DæµMRIå¯¹äºå¿ƒè¡€ç®¡è¯Šæ–­éå¸¸é‡è¦ï¼Œèƒ½å¯é ä¼°è®¡è¡€æµé€Ÿåº¦ã€‚</li>
<li>å®ç°é«˜æ—¶ç©ºåˆ†è¾¨ç‡æ˜¯æ—©æœŸæ£€æµ‹å…³é”®ç—…å˜å¦‚ç‹­çª„æˆ–åŠ¨è„‰ç˜¤çš„å…³é”®æŒ‘æˆ˜ã€‚</li>
<li>ç›®å‰å­˜åœ¨ä½¿ç”¨ç‰©ç†ä¿¡æ¯ç¥ç»ç½‘ç»œï¼ˆPINNsï¼‰è¿›è¡ŒMRIæ•°æ®è¶…åˆ†è¾¨ç‡å¤„ç†çš„æ–¹æ³•ï¼Œä½†å…¶è®­ç»ƒè¿‡ç¨‹ç¼“æ…¢ï¼Œé™åˆ¶äº†å®é™…åº”ç”¨ã€‚</li>
<li>PINGS-Xæ¡†æ¶åˆ©ç”¨è½´å¯¹é½çš„æ—¶ç©ºé«˜æ–¯è¡¨ç¤ºå»ºæ¨¡é«˜åˆ†è¾¨æµé€Ÿï¼Œæ˜¯æœ¬æ–‡çš„æ ¸å¿ƒåˆ›æ–°ç‚¹ã€‚</li>
<li>PINGS-Xå…·æœ‰è§„èŒƒåŒ–é«˜æ–¯è´´ç‰‡æŠ€æœ¯ã€è½´å¯¹é½çš„é«˜æ–¯å’Œåˆå¹¶ç¨‹åºä»¥é˜²æ­¢é€€åŒ–è§£å†³æ–¹æ¡ˆå¹¶æå‡è®¡ç®—æ•ˆç‡ç­‰ç‰¹ç‚¹ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒPINGS-Xåœ¨å‡å°‘è®­ç»ƒæ—¶é—´çš„åŒæ—¶å®ç°äº†è¾ƒé«˜çš„è¶…åˆ†è¾¨ç‡ç²¾åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.11048">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5d77ca12607b0790b0f4311d03f9b6c6" align="middle">
<img src="https://picx.zhimg.com/v2-100da6ea94e41f5df54ad23c0af8285f" align="middle">
<img src="https://picx.zhimg.com/v2-d6ee92e3d4eec526d787886b43186a5f" align="middle">
<img src="https://picx.zhimg.com/v2-9199444408640639bba3b011670b343d" align="middle">
<img src="https://picx.zhimg.com/v2-d299b4c1fdd6ec5cfd8bd60e4c5f7ceb" align="middle">
<img src="https://picx.zhimg.com/v2-540fe97b846f9e68cee5dba69de6ad00" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Algorithms-Trained-on-Normal-Chest-X-rays-Can-Predict-Health-Insurance-Types"><a href="#Algorithms-Trained-on-Normal-Chest-X-rays-Can-Predict-Health-Insurance-Types" class="headerlink" title="Algorithms Trained on Normal Chest X-rays Can Predict Health Insurance Types"></a>Algorithms Trained on Normal Chest X-rays Can Predict Health Insurance Types</h2><p><strong>Authors:Chi-Yu Chen, Rawan Abulibdeh, Arash Asgari, Leo Anthony Celi, Deirdre Goode, Hassan Hamidi, Laleh Seyyed-Kalantari, Po-Chih Kuo, Ned McCague, Thomas Sounack</strong></p>
<p>Artificial intelligence is revealing what medicine never intended to encode. Deep vision models, trained on chest X-rays, can now detect not only disease but also invisible traces of social inequality. In this study, we show that state-of-the-art architectures (DenseNet121, SwinV2-B, MedMamba) can predict a patientâ€™s health insurance type, a strong proxy for socioeconomic status, from normal chest X-rays with significant accuracy (AUC around 0.67 on MIMIC-CXR-JPG, 0.68 on CheXpert). The signal persists even when age, race, and sex are controlled for, and remains detectable when the model is trained exclusively on a single racial group. Patch-based occlusion reveals that the signal is diffuse rather than localized, embedded in the upper and mid-thoracic regions. This suggests that deep networks may be internalizing subtle traces of clinical environments, equipment differences, or care pathways; learning socioeconomic segregation itself. These findings challenge the assumption that medical images are neutral biological data. By uncovering how models perceive and exploit these hidden social signatures, this work reframes fairness in medical AI: the goal is no longer only to balance datasets or adjust thresholds, but to interrogate and disentangle the social fingerprints embedded in clinical data itself.</p>
<blockquote>
<p>äººå·¥æ™ºèƒ½æ­£åœ¨æ­ç¤ºåŒ»å­¦ä»æœªæ‰“ç®—ç¼–ç çš„ä¿¡æ¯ã€‚åœ¨èƒ¸éƒ¨Xå…‰ç‰‡ä¸Šè®­ç»ƒçš„æ·±åº¦è§†è§‰æ¨¡å‹ç°åœ¨ä¸ä»…èƒ½å¤Ÿæ£€æµ‹ç–¾ç—…ï¼Œè¿˜èƒ½å¤Ÿæ£€æµ‹ç¤¾ä¼šä¸å¹³ç­‰çš„éšå½¢ç—•è¿¹ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†æœ€å…ˆè¿›çš„æ¶æ„ï¼ˆDenseNet121ã€SwinV2-Bã€MedMambaï¼‰å¯ä»¥ä»æ­£å¸¸çš„èƒ¸éƒ¨Xå…‰ç‰‡ä¸­é¢„æµ‹æ‚£è€…çš„å¥åº·ä¿é™©ç±»å‹ï¼ˆä½œä¸ºç¤¾ä¼šç»æµåœ°ä½çš„æœ‰åŠ›ä»£ç†ï¼‰ï¼Œé¢„æµ‹çš„å‡†ç¡®æ€§ç›¸å½“é«˜ï¼ˆåœ¨MIMIC-CXR-JPGä¸Šä¸ºçº¦0.67çš„AUCï¼Œåœ¨CheXpertä¸Šä¸º0.68ï¼‰ã€‚å³ä½¿åœ¨æ§åˆ¶å¹´é¾„ã€ç§æ—å’Œæ€§åˆ«çš„æƒ…å†µä¸‹ï¼Œè¿™ä¸€ä¿¡å·ä¾ç„¶å­˜åœ¨ï¼Œå¹¶ä¸”åœ¨æ¨¡å‹ä»…é’ˆå¯¹å•ä¸€ç§æ—ç¾¤ä½“è¿›è¡Œè®­ç»ƒæ—¶ä»å¯æ£€æµ‹åˆ°ã€‚åŸºäºè¡¥ä¸çš„é®æŒ¡è¡¨æ˜ï¼Œè¿™ä¸€ä¿¡å·æ˜¯åˆ†æ•£çš„ï¼Œè€Œéå±€éƒ¨çš„ï¼ŒåµŒå…¥åœ¨èƒ¸éƒ¨ä¸ŠåŒºå’Œä¸­éƒ¨åŒºåŸŸã€‚è¿™è¡¨æ˜æ·±åº¦ç½‘ç»œå¯èƒ½æ­£åœ¨å†…åŒ–ä¸´åºŠç¯å¢ƒã€è®¾å¤‡å·®å¼‚æˆ–æŠ¤ç†è·¯å¾„çš„ç»†å¾®ç—•è¿¹ï¼›å­¦ä¹ ç¤¾ä¼šç»æµéš”ç¦»æœ¬èº«ã€‚è¿™äº›å‘ç°æŒ‘æˆ˜äº†åŒ»å­¦å›¾åƒæ˜¯ä¸­ç«‹ç”Ÿç‰©æ•°æ®çš„å‡è®¾ã€‚é€šè¿‡æ­ç¤ºæ¨¡å‹å¦‚ä½•æ„ŸçŸ¥å’Œåˆ©ç”¨è¿™äº›éšè—çš„ç¤¾ä¼šç‰¹å¾ï¼Œè¿™é¡¹å·¥ä½œé‡æ–°å®šä¹‰äº†åŒ»ç–—äººå·¥æ™ºèƒ½çš„å…¬å¹³æ€§ï¼šç›®æ ‡ä¸å†ä»…ä»…æ˜¯å¹³è¡¡æ•°æ®é›†æˆ–è°ƒæ•´é˜ˆå€¼ï¼Œè€Œæ˜¯æ¢ç´¢å’Œè§£å†³åµŒå…¥åœ¨ä¸´åºŠæ•°æ®ä¸­çš„ç¤¾ä¼šæŒ‡çº¹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.11030v1">PDF</a> Submitting to MIDL 2026</p>
<p><strong>Summary</strong></p>
<p>äººå·¥æ™ºèƒ½ä¸ä»…èƒ½åœ¨èƒ¸éƒ¨Xå…‰ç‰‡ä¸­æ£€æµ‹ç–¾ç—…ï¼Œè¿˜èƒ½æ­ç¤ºç¤¾ä¼šä¸å¹³ç­‰çš„éšå½¢ç—•è¿¹ã€‚æœ¬ç ”ç©¶å±•ç¤ºäº†æœ€å…ˆè¿›çš„æ¨¡å‹æ¶æ„å¯ä»¥é¢„æµ‹æ‚£è€…çš„å¥åº·ä¿é™©ç±»å‹ï¼Œä½œä¸ºç¤¾ä¼šç»æµåœ°ä½çš„å¼ºå¤§ä»£ç†æŒ‡æ ‡ï¼Œå‡†ç¡®ç‡æ˜¾è‘—ã€‚è¿™äº›ä¿¡å·åœ¨æ§åˆ¶å¹´é¾„ã€ç§æ—å’Œæ€§åˆ«å› ç´ åä»ç„¶å­˜åœ¨ï¼Œå¹¶ä¸”åœ¨å•ä¸€ç§æ—ç¾¤ä½“çš„æ¨¡å‹è®­ç»ƒä¸­ä¹Ÿå¯æ£€æµ‹å¾—åˆ°ã€‚è¿™äº›å‘ç°æ­ç¤ºäº†åŒ»ç–—å›¾åƒå¹¶éä¸­æ€§çš„ç”Ÿç‰©æ•°æ®ï¼Œè€Œæ˜¯è•´å«äº†ç¤¾ä¼šèƒŒæ™¯ä¿¡æ¯ã€‚æœ¬ç ”ç©¶é‡æ–°å®šä¹‰äº†åŒ»ç–—äººå·¥æ™ºèƒ½ä¸­çš„å…¬å¹³æ€§é—®é¢˜ï¼Œä¸ä»…è¦åœ¨æ•°æ®é›†ä¸Šå®ç°å¹³è¡¡æˆ–è°ƒæ•´é˜ˆå€¼ï¼Œè¿˜éœ€è¦æ·±å…¥æ¢ç´¢å¹¶æ¶ˆé™¤ä¸´åºŠæ•°æ®ä¸­åµŒå…¥çš„ç¤¾ä¼šæŒ‡çº¹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äººå·¥æ™ºèƒ½èƒ½å¤Ÿä»èƒ¸éƒ¨Xå…‰ç‰‡ä¸­æ£€æµ‹ç¤¾ä¼šä¸å¹³ç­‰ã€‚</li>
<li>æœ€å…ˆè¿›çš„æ¨¡å‹æ¶æ„èƒ½é¢„æµ‹æ‚£è€…çš„å¥åº·ä¿é™©ç±»å‹ï¼Œåæ˜ ç¤¾ä¼šç»æµåœ°ä½ã€‚</li>
<li>é¢„æµ‹å‡†ç¡®ç‡æ˜¾è‘—ï¼Œå³ä½¿æ§åˆ¶å¤šä¸ªå› ç´ åï¼Œè¿™ç§é¢„æµ‹ä»ç„¶æœ‰æ•ˆã€‚</li>
<li>éšè—çš„ç¤¾ä¼šç­¾åï¼ˆå¦‚ä¸´åºŠç¯å¢ƒã€è®¾å¤‡å·®å¼‚æˆ–æŠ¤ç†é€”å¾„ï¼‰å¯èƒ½åµŒå…¥åœ¨åŒ»ç–—å›¾åƒä¸­ã€‚</li>
<li>åŒ»ç–—å›¾åƒå¹¶éçº¯ç²¹ç”Ÿç‰©æ•°æ®ï¼Œè•´å«äº†ä¸°å¯Œçš„ç¤¾ä¼šèƒŒæ™¯ä¿¡æ¯ã€‚</li>
<li>å¯¹æ¨¡å‹å¦‚ä½•æ„ŸçŸ¥å’Œåˆ©ç”¨è¿™äº›éšè—çš„ç¤¾ä¼šç­¾åçš„ç ”ç©¶é‡æ–°å®šä¹‰äº†åŒ»ç–—äººå·¥æ™ºèƒ½çš„å…¬å¹³æ€§æŒ‘æˆ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.11030">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3efa5b74bf076d73b9d19d4f790aed4c" align="middle">
<img src="https://picx.zhimg.com/v2-745b0f92dcaf92bbf88f598197c3ee19" align="middle">
<img src="https://picx.zhimg.com/v2-b95e1e429731f5711d49c948ab2a617b" align="middle">
<img src="https://picx.zhimg.com/v2-743fda727ec2d9e013128f701a6f5aea" align="middle">
<img src="https://picx.zhimg.com/v2-4ced01ae56d78a6be7fb7bffccd3c788" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="ERMoE-Eigen-Reparameterized-Mixture-of-Experts-for-Stable-Routing-and-Interpretable-Specialization"><a href="#ERMoE-Eigen-Reparameterized-Mixture-of-Experts-for-Stable-Routing-and-Interpretable-Specialization" class="headerlink" title="ERMoE: Eigen-Reparameterized Mixture-of-Experts for Stable Routing and Interpretable Specialization"></a>ERMoE: Eigen-Reparameterized Mixture-of-Experts for Stable Routing and Interpretable Specialization</h2><p><strong>Authors:Anzhe Cheng, Shukai Duan, Shixuan Li, Chenzhong Yin, Mingxi Cheng, Heng Ping, Tamoghna Chattopadhyay, Sophia I Thomopoulos, Shahin Nazarian, Paul Thompson, Paul Bogdan</strong></p>
<p>Mixture-of-Experts (MoE) architectures expand model capacity by sparsely activating experts but face two core challenges: misalignment between router logits and each expertâ€™s internal structure leads to unstable routing and expert underutilization, and load imbalances create straggler bottlenecks. Standard solutions, such as auxiliary load-balancing losses, can reduce load disparities but often weaken expert specialization and hurt downstream performance. To address these issues, we propose ERMoE, a sparse MoE transformer that reparameterizes each expert in a learned orthonormal eigenbasis and replaces learned gating logits with an â€œEigenbasis Scoreâ€, defined as the cosine similarity between input features and an expertâ€™s basis. This content-aware routing ties token assignments directly to expertsâ€™ representation spaces, stabilizing utilization and promoting interpretable specialization without sacrificing sparsity. Crucially, ERMoE removes the need for explicit balancing losses and avoids the interfering gradients they introduce. We show that ERMoE achieves state-of-the-art accuracy on ImageNet classification and cross-modal image-text retrieval benchmarks (e.g., COCO, Flickr30K), while naturally producing flatter expert load distributions. Moreover, a 3D MRI variant (ERMoE-ba) improves brain age prediction accuracy by more than 7% and yields anatomically interpretable expert specializations. ERMoE thus introduces a new architectural principle for sparse expert models that directly addresses routing instabilities and enables improved performance with scalable, interpretable specialization.</p>
<blockquote>
<p>ä¸“å®¶çš„æ··åˆï¼ˆMoEï¼‰æ¶æ„é€šè¿‡ç¨€ç–åœ°æ¿€æ´»ä¸“å®¶æ¥æ‰©å±•æ¨¡å‹å®¹é‡ï¼Œä½†é¢ä¸´ä¸¤å¤§æ ¸å¿ƒæŒ‘æˆ˜ï¼šè·¯ç”±å™¨é€»è¾‘ä¸æ¯ä¸ªä¸“å®¶çš„å†…éƒ¨ç»“æ„ä¹‹é—´å­˜åœ¨ä¸åŒ¹é…ï¼Œå¯¼è‡´è·¯ç”±ä¸ç¨³å®šå’Œä¸“å®¶åˆ©ç”¨ä¸è¶³ï¼Œè€Œè´Ÿè½½ä¸å‡è¡¡åˆ™ä¼šäº§ç”Ÿæ»åç“¶é¢ˆã€‚æ ‡å‡†è§£å†³æ–¹æ¡ˆï¼Œå¦‚è¾…åŠ©è´Ÿè½½å‡è¡¡æŸå¤±ï¼Œå¯ä»¥å‡å°‘è´Ÿè½½å·®å¼‚ï¼Œä½†å¾€å¾€ä¼šå‰Šå¼±ä¸“å®¶çš„ä¸“ä¸šåŒ–å¹¶æŸå®³ä¸‹æ¸¸æ€§èƒ½ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ERMoEï¼Œè¿™æ˜¯ä¸€ç§ç¨€ç–çš„MoEè½¬æ¢å™¨ï¼Œå®ƒåœ¨å­¦ä¹ çš„æ­£äº¤ç‰¹å¾åŸºä¸­å¯¹æ¯ä¸ªä¸“å®¶è¿›è¡Œé‡æ–°å‚æ•°åŒ–ï¼Œå¹¶ç”¨â€œç‰¹å¾åŸºå¾—åˆ†â€æ›¿æ¢å­¦ä¹ åˆ°çš„é—¨æ§é€»è¾‘ï¼Œè¯¥å¾—åˆ†å®šä¹‰ä¸ºè¾“å…¥ç‰¹å¾ä¸ä¸“å®¶åŸºç¡€ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼æ€§ã€‚è¿™ç§å†…å®¹æ„ŸçŸ¥çš„è·¯ç”±ç›´æ¥å°†ä»¤ç‰Œåˆ†é…ä¸ä¸“å®¶çš„è¡¨ç¤ºç©ºé—´ç›¸å…³è”ï¼Œç¨³å®šäº†åˆ©ç”¨ï¼Œä¿ƒè¿›äº†å¯è§£é‡Šçš„ä¸“ä¸šåŒ–ï¼ŒåŒæ—¶ä¸ç‰ºç‰²ç¨€ç–æ€§ã€‚å…³é”®çš„æ˜¯ï¼ŒERMoEæ¶ˆé™¤äº†å¯¹æ˜¾å¼å¹³è¡¡æŸå¤±çš„éœ€æ±‚ï¼Œé¿å…äº†å®ƒä»¬å¼•å…¥çš„å¹²æ‰°æ¢¯åº¦ã€‚æˆ‘ä»¬è¡¨æ˜ï¼ŒERMoEåœ¨ImageNetåˆ†ç±»å’Œè·¨æ¨¡æ€å›¾åƒæ–‡æœ¬æ£€ç´¢åŸºå‡†æµ‹è¯•ï¼ˆä¾‹å¦‚COCOã€Flickr30Kï¼‰ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„å‡†ç¡®æ€§ï¼ŒåŒæ—¶è‡ªç„¶åœ°äº§ç”Ÿäº†å¹³å¦çš„ä¸“å®¶è´Ÿè½½åˆ†å¸ƒã€‚æ­¤å¤–ï¼Œ3D MRIå˜ä½“ï¼ˆERMoE-baï¼‰æé«˜äº†è„‘é¾„é¢„æµ‹ç²¾åº¦è¶…è¿‡7%ï¼Œå¹¶äº§ç”Ÿäº†å¯è§£å‰–è§£é‡Šçš„ä¸“å®¶ä¸“ä¸šåŒ–ã€‚å› æ­¤ï¼ŒERMoEä¸ºç¨€ç–ä¸“å®¶æ¨¡å‹å¼•å…¥äº†ä¸€ä¸ªæ–°çš„æ¶æ„åŸåˆ™ï¼Œç›´æ¥è§£å†³äº†è·¯ç”±ä¸ç¨³å®šé—®é¢˜ï¼Œå¹¶é€šè¿‡å¯ç¼©æ”¾ã€å¯è§£é‡Šçš„ä¸“ä¸šåŒ–å®ç°äº†æ”¹è¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.10971v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    é’ˆå¯¹MoEï¼ˆMixture-of-Expertsï¼‰æ¶æ„ä¸­è·¯ç”±ä¸ç¨³å®šå’Œä¸“å®¶åˆ©ç”¨ä¸è¶³çš„é—®é¢˜ï¼Œæå‡ºERMoEæ–¹æ¡ˆã€‚è¯¥æ–¹æ¡ˆé€šè¿‡é‡æ–°å‚æ•°åŒ–ä¸“å®¶æ¨¡å‹ï¼Œä½¿ç”¨æ­£äº¤åŸºæ›¿æ¢å­¦ä¹ åˆ°çš„é—¨æ§å¯¹æ•°å‡ ç‡ï¼ˆgating logitsï¼‰ï¼Œå¹¶é‡‡ç”¨åŸºäºè¾“å…¥ç‰¹å¾å’Œä¸“å®¶åŸºä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼æ€§çš„Eigenbasis Scoreè¿›è¡Œå†…å®¹æ„ŸçŸ¥è·¯ç”±ã€‚æ­¤æ–¹æ³•æé«˜äº†ä¸“å®¶åˆ©ç”¨ç‡ï¼Œä¿ƒè¿›äº†å¯è§£é‡Šçš„ä¸“é—¨åŒ–ï¼ŒåŒæ—¶é¿å…äº†ç‰ºç‰²ç¨€ç–æ€§ã€‚ERMoEæ— éœ€æ˜¾å¼å¹³è¡¡æŸå¤±ï¼Œé¿å…äº†å¹²æ‰°æ¢¯åº¦çš„é—®é¢˜ã€‚åœ¨ImageNetåˆ†ç±»å’Œè·¨æ¨¡æ€å›¾åƒæ–‡æœ¬æ£€ç´¢åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°æœ€ä½³ï¼ŒåŒæ—¶è‡ªç„¶äº§ç”Ÿæ›´å¹³å¦çš„ä¸“å®¶è´Ÿè½½åˆ†å¸ƒã€‚æ­¤å¤–ï¼Œåœ¨3D MRIçš„ERMoE-baå˜ä½“æé«˜äº†è„‘é¾„é¢„æµ‹ç²¾åº¦è¶…è¿‡7%ï¼Œå¹¶äº§ç”Ÿå¯è§£é‡Šçš„ä¸“å®¶ä¸“ä¸šåŒ–ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ul>
<li>ERMoEè§£å†³äº†MoEæ¶æ„ä¸­çš„è·¯ç”±ä¸ç¨³å®šå’Œä¸“å®¶åˆ©ç”¨ä¸è¶³çš„é—®é¢˜ã€‚</li>
<li>ERMoEé€šè¿‡é‡æ–°å‚æ•°åŒ–ä¸“å®¶æ¨¡å‹ï¼Œä½¿ç”¨æ­£äº¤åŸºæ›¿æ¢å­¦ä¹ åˆ°çš„é—¨æ§å¯¹æ•°å‡ ç‡ã€‚</li>
<li>åŸºäºè¾“å…¥ç‰¹å¾å’Œä¸“å®¶åŸºä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼æ€§çš„Eigenbasis Scoreç”¨äºå†…å®¹æ„ŸçŸ¥è·¯ç”±ã€‚</li>
<li>ERMoEæé«˜äº†ä¸“å®¶åˆ©ç”¨ç‡å¹¶ä¿ƒè¿›äº†å¯è§£é‡Šçš„ä¸“é—¨åŒ–ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹çš„ç¨€ç–æ€§ã€‚</li>
<li>ERMoEæ— éœ€æ˜¾å¼å¹³è¡¡æŸå¤±ï¼Œé¿å…äº†å¹²æ‰°æ¢¯åº¦çš„é—®é¢˜ã€‚</li>
<li>ERMoEåœ¨å›¾åƒåˆ†ç±»å’Œè·¨æ¨¡æ€æ£€ç´¢ç­‰å¤šä¸ªä»»åŠ¡ä¸Šè¡¨ç°æœ€ä½³ã€‚</li>
<li>ERMoEæé«˜äº†è„‘é¾„é¢„æµ‹ç²¾åº¦ï¼Œå¹¶å±•ç°å‡ºå¯è§£é‡Šçš„ä¸“å®¶ä¸“ä¸šåŒ–åœ¨åŒ»å­¦å›¾åƒåˆ†æä¸­çš„åº”ç”¨æ½œåŠ›ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.10971">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-aeee32681300b62ad9cd66a3fd589313" align="middle">
<img src="https://picx.zhimg.com/v2-11d8474d3f157c102ca064829fa94d47" align="middle">
<img src="https://picx.zhimg.com/v2-f8b6c32141f125f5c32e13a9d6895ca2" align="middle">
<img src="https://picx.zhimg.com/v2-07266e21e6c05f67ce79aa7ad2f83dcb" align="middle">
<img src="https://picx.zhimg.com/v2-9912af4df97fbeff6565c4baac86d86d" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Divide-Conquer-and-Unite-Hierarchical-Style-Recalibrated-Prototype-Alignment-for-Federated-Medical-Image-Segmentation"><a href="#Divide-Conquer-and-Unite-Hierarchical-Style-Recalibrated-Prototype-Alignment-for-Federated-Medical-Image-Segmentation" class="headerlink" title="Divide, Conquer and Unite: Hierarchical Style-Recalibrated Prototype Alignment for Federated Medical Image Segmentation"></a>Divide, Conquer and Unite: Hierarchical Style-Recalibrated Prototype Alignment for Federated Medical Image Segmentation</h2><p><strong>Authors:Xingyue Zhao, Wenke Huang, Xingguang Wang, Haoyu Zhao, Linghao Zhuang, Anwen Jiang, Guancheng Wan, Mang Ye</strong></p>
<p>Federated learning enables multiple medical institutions to train a global model without sharing data, yet feature heterogeneity from diverse scanners or protocols remains a major challenge. Many existing works attempt to address this issue by leveraging model representations (e.g., mean feature vectors) to correct local training; however, they often face two key limitations: 1) Incomplete Contextual Representation Learning: Current approaches primarily focus on final-layer features, overlooking critical multi-level cues and thus diluting essential context for accurate segmentation. 2) Layerwise Style Bias Accumulation: Although utilizing representations can partially align global features, these methods neglect domain-specific biases within intermediate layers, allowing style discrepancies to build up and reduce model robustness. To address these challenges, we propose FedBCS to bridge feature representation gaps via domain-invariant contextual prototypes alignment. Specifically, we introduce a frequency-domain adaptive style recalibration into prototype construction that not only decouples content-style representations but also learns optimal style parameters, enabling more robust domain-invariant prototypes. Furthermore, we design a context-aware dual-level prototype alignment method that extracts domain-invariant prototypes from different layers of both encoder and decoder and fuses them with contextual information for finer-grained representation alignment. Extensive experiments on two public datasets demonstrate that our method exhibits remarkable performance.</p>
<blockquote>
<p>è”åˆå­¦ä¹ ä½¿å¤šä¸ªåŒ»ç–—æœºæ„èƒ½å¤Ÿåœ¨ä¸å…±äº«æ•°æ®çš„æƒ…å†µä¸‹è®­ç»ƒå…¨çƒæ¨¡å‹ï¼Œä½†æ¥è‡ªä¸åŒæ‰«æä»ªæˆ–åè®®çš„ç‰¹å¾å¼‚è´¨æ€§ä»ç„¶æ˜¯ä¸€ä¸ªä¸»è¦æŒ‘æˆ˜ã€‚è®¸å¤šç°æœ‰å·¥ä½œè¯•å›¾é€šè¿‡åˆ©ç”¨æ¨¡å‹è¡¨ç¤ºï¼ˆä¾‹å¦‚ï¼Œå¹³å‡ç‰¹å¾å‘é‡ï¼‰æ¥çº æ­£å±€éƒ¨è®­ç»ƒæ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼›ç„¶è€Œï¼Œå®ƒä»¬é€šå¸¸é¢ä¸´ä¸¤ä¸ªå…³é”®å±€é™ï¼š1ï¼‰ä¸Šä¸‹æ–‡è¡¨ç¤ºå­¦ä¹ ä¸å®Œæ•´ï¼šå½“å‰æ–¹æ³•ä¸»è¦å…³æ³¨äºæœ€ç»ˆå±‚ç‰¹å¾ï¼Œå¿½ç•¥äº†å…³é”®çš„å¤šå±‚æ¬¡çº¿ç´¢ï¼Œä»è€Œç¨€é‡Šäº†å‡†ç¡®åˆ†å‰²çš„å¿…å¤‡ä¸Šä¸‹æ–‡ã€‚2ï¼‰é€å±‚é£æ ¼åå·®ç´¯ç§¯ï¼šè™½ç„¶åˆ©ç”¨è¡¨ç¤ºå¯ä»¥éƒ¨åˆ†å¯¹é½å…¨å±€ç‰¹å¾ï¼Œä½†è¿™äº›æ–¹æ³•å¿½è§†äº†ä¸­é—´å±‚ä¸­çš„ç‰¹å®šé¢†åŸŸåå·®ï¼Œä½¿å¾—é£æ ¼å·®å¼‚ç´¯ç§¯å¹¶é™ä½äº†æ¨¡å‹çš„ç¨³å¥æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºFedBCSæ¥é€šè¿‡é¢†åŸŸä¸å˜ä¸Šä¸‹æ–‡åŸå‹å¯¹é½æ¥å¼¥è¡¥ç‰¹å¾è¡¨ç¤ºå·®è·ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åœ¨åŸå‹æ„å»ºä¸­å¼•å…¥äº†ä¸€ç§é¢‘åŸŸè‡ªé€‚åº”é£æ ¼é‡æ–°æ ¡å‡†ï¼Œè¿™ä¸ä»…è§£è€¦äº†å†…å®¹-é£æ ¼è¡¨ç¤ºï¼Œè¿˜å­¦ä¹ äº†æœ€ä½³é£æ ¼å‚æ•°ï¼Œä»è€Œå®ç°æ›´ç¨³å¥çš„é¢†åŸŸä¸å˜åŸå‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„åŒçº§åŸå‹å¯¹é½æ–¹æ³•ï¼Œä»ä¸åŒå±‚çš„ç¼–ç å™¨å’Œè§£ç å™¨ä¸­æå–é¢†åŸŸä¸å˜åŸå‹ï¼Œå¹¶ä¸ä¸Šä¸‹æ–‡ä¿¡æ¯èåˆï¼Œä»¥å®ç°æ›´ç²¾ç»†çš„è¡¨ç¤ºå¯¹é½ã€‚åœ¨ä¸¤ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.10945v1">PDF</a> Accepted at AAAI-26</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºFedBCSæ–¹æ³•æ¥è§£å†³è”é‚¦å­¦ä¹ ä¸­åŒ»å­¦å›¾åƒç‰¹å¾å¼‚è´¨æ€§é—®é¢˜ã€‚é€šè¿‡å¼•å…¥åŸŸä¸å˜ä¸Šä¸‹æ–‡åŸå‹å¯¹é½å’Œé¢‘ç‡åŸŸè‡ªé€‚åº”é£æ ¼é‡æ–°æ ¡å‡†æŠ€æœ¯ï¼Œæé«˜äº†æ¨¡å‹çš„ç¨³å¥æ€§å’Œå‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è”é‚¦å­¦ä¹ åœ¨åŒ»å­¦å›¾åƒé¢†åŸŸçš„åº”ç”¨é¢ä¸´ç‰¹å¾å¼‚è´¨æ€§çš„æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•é€šè¿‡åˆ©ç”¨æ¨¡å‹è¡¨ç¤ºæ¥çº æ­£å±€éƒ¨è®­ç»ƒï¼Œä½†å­˜åœ¨ä¸Šä¸‹æ–‡è¡¨ç¤ºå­¦ä¹ ä¸å®Œæ•´å’Œå±‚æ¬¡é£æ ¼åå·®ç´¯ç§¯ä¸¤ä¸ªå…³é”®å±€é™ã€‚</li>
<li>FedBCSè¢«æå‡ºä»¥è§£å†³ç‰¹å¾è¡¨ç¤ºå·®è·ï¼Œé€šè¿‡åŸŸä¸å˜ä¸Šä¸‹æ–‡åŸå‹å¯¹é½æ¥å»ºç«‹æ¡¥æ¢ã€‚</li>
<li>å¼•å…¥é¢‘ç‡åŸŸè‡ªé€‚åº”é£æ ¼é‡æ–°æ ¡å‡†ï¼Œä»¥è§£è€¦å†…å®¹-é£æ ¼è¡¨ç¤ºå¹¶å­¦ä¹ æœ€ä½³é£æ ¼å‚æ•°ã€‚</li>
<li>è®¾è®¡äº†ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„åŒçº§åŸå‹å¯¹é½æ–¹æ³•ï¼Œä»ç¼–ç å™¨å’Œè§£ç å™¨çš„ä¸åŒå±‚æå–åŸŸä¸å˜åŸå‹ï¼Œå¹¶ä¸ä¸Šä¸‹æ–‡ä¿¡æ¯è¿›è¡Œç²¾ç»†å¯¹é½ã€‚</li>
<li>åœ¨ä¸¤ä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.10945">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-55404c4d27ba775d584628203267e4fd" align="middle">
<img src="https://picx.zhimg.com/v2-fd4ea391eb5ad4556c4356a1aee3c107" align="middle">
<img src="https://picx.zhimg.com/v2-55df402fd50a3b3fecdfe06d450ece46" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="FAST-CAD-A-Fairness-Aware-Framework-for-Non-Contact-Stroke-Diagnosis"><a href="#FAST-CAD-A-Fairness-Aware-Framework-for-Non-Contact-Stroke-Diagnosis" class="headerlink" title="FAST-CAD: A Fairness-Aware Framework for Non-Contact Stroke Diagnosis"></a>FAST-CAD: A Fairness-Aware Framework for Non-Contact Stroke Diagnosis</h2><p><strong>Authors:Tianming Sha, Zechuan Chen, Zhan Cheng, Haotian Zhai, Xuwei Ding, Keze Wang</strong></p>
<p>Stroke is an acute cerebrovascular disease, and timely diagnosis significantly improves patient survival. However, existing automated diagnosis methods suffer from fairness issues across demographic groups, potentially exacerbating healthcare disparities. In this work we propose FAST-CAD, a theoretically grounded framework that combines domain-adversarial training (DAT) with group distributionally robust optimization (Group-DRO) for fair and accurate non-contact stroke diagnosis. Our approach is built on domain adaptation and minimax fairness theory and provides convergence guarantees and fairness bounds. We curate a multimodal dataset covering 12 demographic subgroups defined by age, gender, and posture. FAST-CAD employs self-supervised encoders with adversarial domain discrimination to learn demographic-invariant representations, while Group-DRO optimizes worst-group risk to ensure robust performance across all subgroups. Extensive experiments show that our method achieves superior diagnostic performance while maintaining fairness across demographic groups, and our theoretical analysis supports the effectiveness of the unified DAT + Group-DRO framework. This work provides both practical advances and theoretical insights for fair medical AI systems.</p>
<blockquote>
<p>ä¸­é£æ˜¯ä¸€ç§æ€¥æ€§è„‘è¡€ç®¡ç–¾ç—…ï¼ŒåŠæ—¶è¯Šæ–­èƒ½æ˜¾è‘—æé«˜æ‚£è€…çš„å­˜æ´»ç‡ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è‡ªåŠ¨åŒ–è¯Šæ–­æ–¹æ³•åœ¨å„äººå£ç¾¤ä½“ä¸­å­˜åœ¨å…¬å¹³æ€§é—®é¢˜ï¼Œå¯èƒ½åŠ å‰§åŒ»ç–—ä¿å¥å·®å¼‚ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†FAST-CADï¼Œè¿™æ˜¯ä¸€ä¸ªæœ‰ç†è®ºåŸºç¡€çš„æ¡†æ¶ï¼Œå®ƒå°†åŸŸå¯¹æŠ—è®­ç»ƒï¼ˆDATï¼‰ä¸ç¾¤ä½“åˆ†å¸ƒé²æ£’ä¼˜åŒ–ï¼ˆGroup-DROï¼‰ç›¸ç»“åˆï¼Œç”¨äºå…¬å¹³ä¸”ç²¾ç¡®çš„éæ¥è§¦å¼ä¸­é£è¯Šæ–­ã€‚æˆ‘ä»¬çš„æ–¹æ³•å»ºç«‹åœ¨åŸŸé€‚åº”å’Œæå°æå¤§å…¬å¹³æ€§ç†è®ºä¹‹ä¸Šï¼Œæä¾›æ”¶æ•›ä¿è¯å’Œå…¬å¹³æ€§ç•Œé™ã€‚æˆ‘ä»¬æ•´ç†äº†ä¸€ä¸ªå¤šæ¨¡å¼æ•°æ®é›†ï¼Œè¦†ç›–ç”±å¹´é¾„ã€æ€§åˆ«å’Œå§¿åŠ¿å®šä¹‰çš„12ä¸ªäººå£äºšç»„ã€‚FAST-CADé‡‡ç”¨è‡ªç›‘ç£ç¼–ç å™¨ä¸å¯¹æŠ—åŸŸåˆ¤åˆ«ï¼Œå­¦ä¹ äººå£ä¸å˜çš„è¡¨ç¤ºï¼Œè€ŒGroup-DROåˆ™ä¼˜åŒ–æœ€åŠ£ç¾¤ä½“é£é™©ï¼Œä»¥ç¡®ä¿æ‰€æœ‰äºšç»„çš„ç¨³å¥æ€§èƒ½ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç»´æŒå„äººå£ç¾¤ä½“çš„å…¬å¹³æ€§çš„åŒæ—¶ï¼Œå®ç°äº†ä¼˜è¶Šçš„è¯Šæ–­æ€§èƒ½ã€‚æˆ‘ä»¬çš„ç†è®ºåˆ†ææ”¯æŒäº†DAT+Group-DROç»Ÿä¸€æ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚æœ¬ç ”ç©¶ä¸ºå…¬å¹³çš„åŒ»å­¦äººå·¥æ™ºèƒ½ç³»ç»Ÿæä¾›äº†å®è·µè¿›å±•å’Œç†è®ºè§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.08887v2">PDF</a> Accepted for oral presentation at the AAAI Conference on Artificial Intelligence 2026 (AAAI 2026)</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡ç« ä»‹ç»äº†æ€¥æ€§è„‘è¡€ç®¡ç–¾ç—…å³è„‘å’ä¸­çš„è¯Šæ–­æƒ…å†µã€‚ç°æœ‰çš„è‡ªåŠ¨åŒ–è¯Šæ–­æ–¹æ³•å­˜åœ¨è·¨ä¸åŒç¾¤ä½“ä¹‹é—´å…¬å¹³æ€§é—®é¢˜ï¼Œå¯èƒ½å¯¼è‡´å¥åº·æŠ¤ç†çš„å·®è·åŠ å‰§ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ç»“åˆé¢†åŸŸå¯¹æŠ—è®­ç»ƒï¼ˆDATï¼‰ä¸ç¾¤ä½“åˆ†å¸ƒç¨³å¥ä¼˜åŒ–ï¼ˆGroup-DROï¼‰çš„ç†è®ºæ¡†æ¶ï¼Œç§°ä¸ºFAST-CADï¼Œç”¨äºå®ç°å…¬å¹³è€Œå‡†ç¡®çš„éæ¥è§¦å’ä¸­è¯Šæ–­ã€‚å…¶æ„å»ºåœ¨é¢†åŸŸé€‚åº”å’Œæå°æå¤§å…¬å¹³æ€§ç†è®ºåŸºç¡€ä¸Šï¼Œå¹¶æä¾›æ”¶æ•›ä¿è¯å’Œå…¬å¹³æ€§ç•Œé™ã€‚ç ”ç©¶é€šè¿‡æ„å»ºä¸€ä¸ªè¦†ç›–ç”±å¹´é¾„ã€æ€§åˆ«å’Œå§¿åŠ¿å®šä¹‰çš„12ä¸ªäºšç¾¤ä½“æ„æˆçš„å¤šæ¨¡å¼æ•°æ®é›†è¿›è¡Œå®éªŒéªŒè¯ï¼Œç»“æœæ˜¾ç¤ºFAST-CADåœ¨ä¿æŒå…¬å¹³æ€§çš„åŒæ—¶ï¼Œåœ¨è¯Šæ–­æ€§èƒ½ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚æœ¬ç ”ç©¶ä¸ºå…¬å¹³åŒ»ç–—äººå·¥æ™ºèƒ½ç³»ç»Ÿæä¾›äº†å®è·µè¿›å±•å’Œç†è®ºè§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è„‘å’ä¸­æ˜¯ä¸€ç§æ€¥æ€§è„‘è¡€ç®¡ç–¾ç—…ï¼ŒåŠæ—¶è¯Šæ–­èƒ½æ˜¾è‘—æé«˜æ‚£è€…ç”Ÿå­˜ç‡ã€‚</li>
<li>ç°æœ‰è‡ªåŠ¨åŒ–è¯Šæ–­æ–¹æ³•åœ¨ä¸åŒç¾¤ä½“ä¹‹é—´å­˜åœ¨å…¬å¹³æ€§é—®é¢˜ã€‚</li>
<li>FAST-CADæ¡†æ¶ç»“åˆäº†é¢†åŸŸå¯¹æŠ—è®­ç»ƒï¼ˆDATï¼‰å’Œç¾¤ä½“åˆ†å¸ƒç¨³å¥ä¼˜åŒ–ï¼ˆGroup-DROï¼‰ï¼Œæ—¨åœ¨å®ç°å…¬å¹³è€Œå‡†ç¡®çš„éæ¥è§¦å’ä¸­è¯Šæ–­ã€‚</li>
<li>FAST-CADå»ºç«‹åœ¨é¢†åŸŸé€‚åº”å’Œæå°æå¤§å…¬å¹³æ€§ç†è®ºåŸºç¡€ä¸Šï¼Œç¡®ä¿æ”¶æ•›æ€§å’Œå…¬å¹³æ€§ç•Œé™ã€‚</li>
<li>ç ”ç©¶é€šè¿‡åŒ…å«å¤šä¸ªäºšç¾¤ä½“çš„å¤šæ¨¡å¼æ•°æ®é›†è¿›è¡ŒéªŒè¯ï¼Œè¿™äº›äºšç¾¤ä½“æ˜¯æ ¹æ®å¹´é¾„ã€æ€§åˆ«å’Œå§¿åŠ¿å®šä¹‰çš„ã€‚</li>
<li>FAST-CADä½¿ç”¨è‡ªç›‘ç£ç¼–ç å™¨å’Œå¯¹æŠ—é¢†åŸŸé‰´åˆ«æ¥å®ç°ç¾¤ä½“ä¸å˜è¡¨ç¤ºå­¦ä¹ ï¼ŒåŒæ—¶é€šè¿‡Group-DROä¼˜åŒ–æœ€ç³Ÿç³•ç¾¤ä½“é£é™©æ¥ç¡®ä¿æ‰€æœ‰äºšç¾¤ä½“çš„ç¨³å¥æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.08887">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b441a6a613cbb7d23084b079127dad30" align="middle">
<img src="https://picx.zhimg.com/v2-609588ee917cdbd12396465b8495ea1e" align="middle">
<img src="https://picx.zhimg.com/v2-5d825cd3507b3ba4b7459e23c0c0769a" align="middle">
<img src="https://picx.zhimg.com/v2-1d7606dfa305347887614c756f0bd25f" align="middle">
<img src="https://picx.zhimg.com/v2-07488ecf8079d43eb93e0c92bafc05d9" align="middle">
<img src="https://picx.zhimg.com/v2-2569e00fe4f1b476fd4d9dd843c58ff8" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="UCDSC-Open-Set-UnCertainty-aware-Deep-Simplex-Classifier-for-Medical-Image-Datasets"><a href="#UCDSC-Open-Set-UnCertainty-aware-Deep-Simplex-Classifier-for-Medical-Image-Datasets" class="headerlink" title="UCDSC: Open Set UnCertainty aware Deep Simplex Classifier for Medical Image Datasets"></a>UCDSC: Open Set UnCertainty aware Deep Simplex Classifier for Medical Image Datasets</h2><p><strong>Authors:Arnav Aditya, Nitin Kumar, Saurabh Shigwan</strong></p>
<p>Driven by advancements in deep learning, computer-aided diagnoses have made remarkable progress. However, outside controlled laboratory settings, algorithms may encounter several challenges. In the medical domain, these difficulties often stem from limited data availability due to ethical and legal restrictions, as well as the high cost and time required for expert annotations-especially in the face of emerging or rare diseases. In this context, open-set recognition plays a vital role by identifying whether a sample belongs to one of the known classes seen during training or should be rejected as an unknown. Recent studies have shown that features learned in the later stages of deep neural networks are observed to cluster around their class means, which themselves are arranged as individual vertices of a regular simplex [32]. The proposed method introduces a loss function designed to reject samples of unknown classes effectively by penalizing open space regions using auxiliary datasets. This approach achieves significant performance gain across four MedMNIST datasets-BloodMNIST, OCTMNIST, DermaMNIST, TissueMNIST and a publicly available skin dataset [29] outperforming state-of-the-art techniques.</p>
<blockquote>
<p>éšç€æ·±åº¦å­¦ä¹ çš„å‘å±•ï¼Œè®¡ç®—æœºè¾…åŠ©è¯Šæ–­å·²ç»å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ã€‚ç„¶è€Œï¼Œåœ¨å®éªŒå®¤æ§åˆ¶ç¯å¢ƒä¹‹å¤–ï¼Œç®—æ³•å¯èƒ½ä¼šé‡åˆ°è®¸å¤šæŒ‘æˆ˜ã€‚åœ¨åŒ»å­¦é¢†åŸŸï¼Œè¿™äº›å›°éš¾å¾€å¾€æºäºç”±äºä¼¦ç†å’Œæ³•å¾‹çš„é™åˆ¶å¯¼è‡´çš„æ•°æ®å¯ç”¨æ€§çš„é™åˆ¶ï¼Œä»¥åŠå°¤å…¶æ˜¯é¢å¯¹æ–°å…´æˆ–ç½•è§ç–¾ç—…æ—¶ï¼Œä¸“å®¶æ³¨é‡Šæ‰€éœ€çš„é«˜æˆæœ¬å’Œæ—¶é—´ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå¼€æ”¾é›†è¯†åˆ«å‘æŒ¥äº†é‡è¦ä½œç”¨ï¼Œå®ƒå¯ä»¥ç¡®å®šä¸€ä¸ªæ ·æœ¬æ˜¯å¦å±äºè®­ç»ƒæœŸé—´è§è¿‡çš„å·²çŸ¥ç±»åˆ«ï¼Œæˆ–è€…æ˜¯å¦åº”è¯¥è¢«æ‹’ç»ä¸ºæœªçŸ¥æ ·æœ¬ã€‚æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œåœ¨æ·±åº¦ç¥ç»ç½‘ç»œçš„åæœŸé˜¶æ®µå­¦ä¹ çš„ç‰¹å¾ä¼šè¢«è§‚å¯Ÿåˆ°èšé›†åœ¨å…¶ç±»åˆ«å‡å€¼å‘¨å›´ï¼Œè€Œè¿™äº›å‡å€¼æœ¬èº«åˆ™è¢«æ’åˆ—ä¸ºè§„åˆ™å•çº¯å½¢çš„å„ä¸ªé¡¶ç‚¹[32]ã€‚æ‰€æå‡ºçš„æ–¹æ³•å¼•å…¥äº†ä¸€ç§æŸå¤±å‡½æ•°ï¼Œæ—¨åœ¨é€šè¿‡åˆ©ç”¨è¾…åŠ©æ•°æ®é›†å¯¹å¼€æ”¾ç©ºé—´åŒºåŸŸè¿›è¡Œæƒ©ç½šï¼Œä»è€Œæœ‰æ•ˆåœ°æ‹’ç»æœªçŸ¥ç±»åˆ«çš„æ ·æœ¬ã€‚è¯¥æ–¹æ³•åœ¨å››ä¸ªMedMNISTæ•°æ®é›†ï¼ˆBloodMNISTã€OCTMNISTã€DermaMNISTã€TissueMNISTï¼‰å’Œå…¬å¼€å¯ç”¨çš„çš®è‚¤æ•°æ®é›†[29]ä¸Šå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¶…è¶Šäº†æœ€æ–°æŠ€æœ¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.08196v1">PDF</a> 10 pages, Accepted at IEEE&#x2F;CVF WACV 2026, Source code is available at this URL <a target="_blank" rel="noopener" href="https://github.com/Arnavadi19/UCDSC">https://github.com/Arnavadi19/UCDSC</a></p>
<p><strong>Summary</strong><br>     æ·±åº¦å­¦ä¹ é©±åŠ¨ä¸‹çš„è®¡ç®—æœºè¾…åŠ©è¯Šæ–­å·²å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œåœ¨å®é™…åº”ç”¨ç¯å¢ƒä¸­ï¼Œç®—æ³•é¢ä¸´è¯¸å¤šæŒ‘æˆ˜ï¼Œå¦‚åŒ»å­¦é¢†åŸŸçš„æ•°æ®è·å–å—é™ã€æ ‡æ³¨æˆæœ¬é«˜æ˜‚ä¸”è€—æ—¶ç­‰ã€‚å¼€æ”¾é›†è¯†åˆ«æŠ€æœ¯åœ¨æ­¤èµ·åˆ°å…³é”®ä½œç”¨ï¼Œèƒ½è¯†åˆ«æ ·æœ¬æ˜¯å¦å±äºè®­ç»ƒæ—¶çš„å·²çŸ¥ç±»åˆ«ï¼Œæˆ–æ˜¯æœªçŸ¥æ ·æœ¬ã€‚æœ€æ–°ç ”ç©¶è¡¨æ˜ï¼Œæ·±åº¦ç¥ç»ç½‘ç»œåæœŸå­¦ä¹ çš„ç‰¹å¾ä¼šå›´ç»•ç±»åˆ«å‡å€¼èšé›†ï¼Œè¿™äº›å‡å€¼ä½œä¸ºæ­£åˆ™å•çº¯å½¢çš„é¡¶ç‚¹æ’åˆ—ã€‚ç ”ç©¶æ–¹æ³•é€šè¿‡è®¾è®¡æŸå¤±å‡½æ•°æœ‰æ•ˆæ‹’ç»æœªçŸ¥ç±»åˆ«çš„æ ·æœ¬ï¼Œåˆ©ç”¨è¾…åŠ©æ•°æ®é›†å¯¹å¼€æ”¾ç©ºé—´åŒºåŸŸè¿›è¡Œæƒ©ç½šã€‚è¯¥æ–¹æ³•åœ¨å››ä¸ªMedMNISTæ•°æ®é›†åŠä¸€ä¸ªå…¬å¼€çš®è‚¤æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œè¶…è¶Šäº†ç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ åœ¨è®¡ç®—æœºè¾…åŠ©è¯Šæ–­ä¸­å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚</li>
<li>å®é™…åº”ç”¨ä¸­ï¼Œç®—æ³•é¢ä¸´æ•°æ®è·å–å—é™ã€æ ‡æ³¨æˆæœ¬é«˜ç­‰æŒ‘æˆ˜ã€‚</li>
<li>å¼€æ”¾é›†è¯†åˆ«æŠ€æœ¯å¯è¯†åˆ«æ ·æœ¬æ˜¯å¦å±äºå·²çŸ¥ç±»åˆ«æˆ–æœªçŸ¥æ ·æœ¬ã€‚</li>
<li>æ·±åº¦ç¥ç»ç½‘ç»œåæœŸå­¦ä¹ çš„ç‰¹å¾ä¼šå›´ç»•ç±»åˆ«å‡å€¼èšé›†ã€‚</li>
<li>ç ”ç©¶çš„æŸå¤±å‡½æ•°èƒ½æœ‰æ•ˆæ‹’ç»æœªçŸ¥ç±»åˆ«çš„æ ·æœ¬ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å¤šä¸ªMedMNISTæ•°æ®é›†åŠå…¬å¼€çš®è‚¤æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜è¶Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.08196">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-68d0d55d1b66bafb8ed8e08c3c34a161" align="middle">
<img src="https://picx.zhimg.com/v2-4d533d3fb6eab1502c26c1cbb106307e" align="middle">
<img src="https://picx.zhimg.com/v2-366d69eb3fd555d34e4f0978db983c3d" align="middle">
<img src="https://picx.zhimg.com/v2-05ead3e3397d350d9ec50983e2ae0f91" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-11-18/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-11-18/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-18/TTS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-6c0f2896747a03084555dbb3ec30e6a8" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-18  CLARITY Contextual Linguistic Adaptation and Accent Retrieval for Dual-Bias Mitigation in Text-to-Speech Generation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-11-18
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-18/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-1938658107a74287dec3bea13510e6c4" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-18  Intrinsic Dimension Estimation for Radio Galaxy Zoo using Diffusion Models
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-11-18
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">33297.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
