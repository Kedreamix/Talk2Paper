<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-18  Human-AI collaborative autonomous synthesis with pulsed laser deposition for remote epitaxy">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-897d803ec990f59bb3cc02d9c3e21d42')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-18
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    84 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-11-18-æ›´æ–°"><a href="#2025-11-18-æ›´æ–°" class="headerlink" title="2025-11-18 æ›´æ–°"></a>2025-11-18 æ›´æ–°</h1><h2 id="Human-AI-collaborative-autonomous-synthesis-with-pulsed-laser-deposition-for-remote-epitaxy"><a href="#Human-AI-collaborative-autonomous-synthesis-with-pulsed-laser-deposition-for-remote-epitaxy" class="headerlink" title="Human-AI collaborative autonomous synthesis with pulsed laser deposition for remote epitaxy"></a>Human-AI collaborative autonomous synthesis with pulsed laser deposition for remote epitaxy</h2><p><strong>Authors:Asraful Haque, Daniel T. Yimam, Jawad Chowdhury, Ralph Bulanadi, Ivan Vlassiouk, John Lasseter, Sujoy Ghosh, Christopher M. Rouleau, Kai Xiao, Yongtao Liu, Eva Zarkadoula, Rama K. Vasudevan, Sumner B. Harris</strong></p>
<p>Autonomous laboratories typically rely on data-driven decision-making, occasionally with human-in-the-loop oversight to inject domain expertise. Fully leveraging AI agents, however, requires tightly coupled, collaborative workflows spanning hypothesis generation, experimental planning, execution, and interpretation. To address this, we develop and deploy a human-AI collaborative (HAIC) workflow that integrates large language models for hypothesis generation and analysis, with collaborative policy updates driving autonomous pulsed laser deposition (PLD) experiments for remote epitaxy of BaTiO$_3$&#x2F;graphene. HAIC accelerated the hypothesis formation and experimental design and efficiently mapped the growth space to graphene-damage. In situ Raman spectroscopy reveals that chemistry drives degradation while the highest energy plume components seed defects, identifying a low-O$_2$ pressure low-temperature synthesis window that preserves graphene but is incompatible with optimal BaTiO$_3$ growth. Thus, we show a two-step Ar&#x2F;O$_2$ deposition is required to exfoliate ferroelectric BaTiO$_3$ while maintaining a monolayer graphene interlayer. HAIC stages human insight with AI reasoning between autonomous batches to drive rapid scientific progress, providing an evolution to many existing human-in-the-loop autonomous workflows.</p>
<blockquote>
<p>è‡ªä¸»å®éªŒå®¤é€šå¸¸ä¾èµ–äºæ•°æ®é©±åŠ¨çš„å†³ç­–ï¼Œå¶å°”æœ‰äººç±»å‚ä¸ç›‘ç£ä»¥æ³¨å…¥ä¸“ä¸šé¢†åŸŸçš„ç»éªŒçŸ¥è¯†ã€‚ç„¶è€Œï¼Œè¦å……åˆ†åˆ©ç”¨AIæ™ºèƒ½ä½“å®ç°å®éªŒå…¨è‡ªåŠ¨åŒ–ï¼Œéœ€è¦åœ¨å¤šä¸ªé¢†åŸŸå»ºç«‹ç´§å¯†è”ç³»å¹¶åˆä½œå¼€å±•æ¶‰åŠå‡è®¾ç”Ÿæˆã€å®éªŒè§„åˆ’ã€æ‰§è¡Œä»¥åŠç»“æœè§£è¯»çš„å·¥ä½œæµç¨‹ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼€å‘å¹¶éƒ¨ç½²äº†ä¸€ç§äººæœºååŒï¼ˆHAICï¼‰çš„å·¥ä½œæµç¨‹ï¼Œè¯¥æµç¨‹æ•´åˆå¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå‡è®¾ç”Ÿæˆå’Œåˆ†æï¼Œå¹¶åˆ©ç”¨ååŒç­–ç•¥æ›´æ–°é©±åŠ¨è¿œç¨‹å¤–å»¶ç”Ÿé•¿çš„è„‰å†²æ¿€å…‰æ²‰ç§¯ï¼ˆPLDï¼‰å®éªŒã€‚äººæœºååŒåŠ å¿«äº†å‡è®¾å½¢æˆå’Œå®éªŒè®¾è®¡è¿‡ç¨‹ï¼Œå¹¶é«˜æ•ˆæ˜ å°„å‡ºçŸ³å¢¨çƒ¯ç”Ÿé•¿è¿‡ç¨‹ä¸­å®¹æ˜“é­åˆ°ç ´åçš„ç©ºé—´åˆ†å¸ƒåŒºåŸŸã€‚åŸä½æ‹‰æ›¼å…‰è°±æŠ€æœ¯è¡¨æ˜åŒ–å­¦ååº”ä¿ƒä½¿é€€åŒ–å‘ç”Ÿï¼ŒåŒæ—¶é«˜èƒ½æˆåˆ†ä¿ƒæˆç¼ºé™·å‡ºç°ï¼Œå¹¶ä¸”ç¡®å®šäº†ä½æ°§å‹ä½æ¸©åˆæˆçª—å£å¯ä»¥ä¿æŠ¤çŸ³å¢¨çƒ¯å´ä¸åˆ©äºBaTiO3çš„æœ€ä½³ç”Ÿé•¿æ¡ä»¶ã€‚å› æ­¤ï¼Œæˆ‘ä»¬è¯æ˜äº†éœ€è¦é€šè¿‡ä¸¤æ­¥æ°©æ°§æ²‰ç§¯æ³•å‰¥ç¦»é“ç”µBaTiO3å¹¶ä¿æŒå•å±‚çŸ³å¢¨çƒ¯å¤¹å±‚ã€‚äººæœºååŒå°†äººç±»è§è§£ä¸AIæ¨ç†ç›¸ç»“åˆï¼Œåœ¨è‡ªä¸»æ‰¹æ¬¡ä¹‹é—´æ¨åŠ¨å¿«é€Ÿçš„ç§‘å­¦è¿›æ­¥ï¼Œä¸ºè®¸å¤šç°æœ‰çš„æœ‰äººç±»å‚ä¸çš„è‡ªä¸»å·¥ä½œæµç¨‹æä¾›äº†è¿›åŒ–æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.11558v1">PDF</a> </p>
<p><strong>Summary</strong><br>äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰å®éªŒå®¤ä¸­é€šå¸¸éœ€è¦å€ŸåŠ©å¤§æ•°æ®æ¥è¿›è¡Œå†³ç­–ï¼Œä½†ä»éœ€ä¸“å®¶çŸ¥è¯†çš„è¾…åŠ©ï¼Œè®©äººç±»ä¸AIç´§å¯†åˆä½œæ˜¯å…³é”®ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§äººæœºååŒå·¥ä½œæµç¨‹ï¼ˆHAICï¼‰ï¼Œå®ƒç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå‡è®¾ç”Ÿæˆå’Œåˆ†æï¼Œé€šè¿‡ååŒæ”¿ç­–æ›´æ–°é©±åŠ¨è‡ªä¸»è„‰å†²æ¿€å…‰æ²‰ç§¯ï¼ˆPLDï¼‰å®éªŒã€‚äººæœºååŒèƒ½åŠ é€Ÿå®éªŒè®¾è®¡è¿‡ç¨‹å¹¶å‘ç°ç”Ÿé•¿ç©ºé—´å’ŒçŸ³å¢¨çƒ¯æŸä¼¤çš„å…³è”ã€‚ç„¶è€Œé«˜èƒ½é‡å…‰æŸå¯¹çŸ³å¢¨çƒ¯å…·æœ‰ç ´åæ€§å½±å“ã€‚ä¸ºäº†åœ¨ä¸å½±å“å·´é’™é’›æ™¶å‹å±‚çš„æƒ…å†µä¸‹å®ç°å•å±‚çŸ³å¢¨çƒ¯å‰¥ç¦»ï¼Œéœ€è¦ä¸€ä¸ªä¸¤æ­¥çš„Ar&#x2F;OÂ²æ²‰ç§¯è¿‡ç¨‹ã€‚è¿™é¡¹ç ”ç©¶å±•ç°äº†äººæœºååŒå·¥ä½œåœ¨æ¨åŠ¨ç§‘å­¦è¿›æ­¥æ–¹é¢çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>AIå®éªŒå®¤ä¾èµ–äºæ•°æ®é©±åŠ¨çš„å†³ç­–ï¼Œä½†ä»éœ€è¦äººç±»é¢†åŸŸçŸ¥è¯†çš„æ³¨å…¥ã€‚</li>
<li>äººæœºååŒå·¥ä½œæµç¨‹ï¼ˆHAICï¼‰ç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹å’ŒååŒæ”¿ç­–æ›´æ–°ï¼Œç”¨äºè‡ªä¸»å®éªŒã€‚</li>
<li>HAICåŠ é€Ÿäº†å‡è®¾å½¢æˆå’Œå®éªŒè®¾è®¡è¿‡ç¨‹ï¼Œå¹¶å‘ç°äº†çŸ³å¢¨çƒ¯ç”Ÿé•¿è¿‡ç¨‹ä¸­çš„é—®é¢˜ã€‚</li>
<li>é«˜èƒ½é‡å…‰æŸå¯¹çŸ³å¢¨çƒ¯æœ‰ç ´åæ€§å½±å“ï¼Œéœ€è¦æ‰¾åˆ°å¹³è¡¡ä»¥å®ç°å·´é’™é’›æ™¶å‹å±‚çš„å‰¥ç¦»ã€‚</li>
<li>åœ¨ä¿æŠ¤çŸ³å¢¨çƒ¯çš„åŒæ—¶å®ç°å·´é’™é’›æ™¶å‹å±‚çš„å‰¥ç¦»éœ€è¦ä¸¤æ­¥æ²‰ç§¯è¿‡ç¨‹ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.11558">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-305ff5887773b493f9467ed6f4225942" align="middle">
<img src="https://picx.zhimg.com/v2-0c7dca9a3eea9a48f6e17c9d2a896313" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="W2S-AlignTree-Weak-to-Strong-Inference-Time-Alignment-for-Large-Language-Models-via-Monte-Carlo-Tree-Search"><a href="#W2S-AlignTree-Weak-to-Strong-Inference-Time-Alignment-for-Large-Language-Models-via-Monte-Carlo-Tree-Search" class="headerlink" title="W2S-AlignTree: Weak-to-Strong Inference-Time Alignment for Large Language Models via Monte Carlo Tree Search"></a>W2S-AlignTree: Weak-to-Strong Inference-Time Alignment for Large Language Models via Monte Carlo Tree Search</h2><p><strong>Authors:Zhenyu Ding, Yuhao Wang, Tengyue Xiao, Haoying Wang, Guojun Ma, Mingyang Wan, Caigui Jiang, Ning Ding</strong></p>
<p>Large Language Models (LLMs) demonstrate impressive capabilities, yet their outputs often suffer from misalignment with human preferences due to the inadequacy of weak supervision and a lack of fine-grained control. Training-time alignment methods like Reinforcement Learning from Human Feedback (RLHF) face prohibitive costs in expert supervision and inherent scalability limitations, offering limited dynamic control during inference. Consequently, there is an urgent need for scalable and adaptable alignment mechanisms. To address this, we propose W2S-AlignTree, a pioneering plug-and-play inference-time alignment framework that synergistically combines Monte Carlo Tree Search (MCTS) with the Weak-to-Strong Generalization paradigm for the first time. W2S-AlignTree formulates LLM alignment as an optimal heuristic search problem within a generative search tree. By leveraging weak modelâ€™s real-time, step-level signals as alignment proxies and introducing an Entropy-Aware exploration mechanism, W2S-AlignTree enables fine-grained guidance during strong modelâ€™s generation without modifying its parameters. The approach dynamically balances exploration and exploitation in high-dimensional generation search trees. Experiments across controlled sentiment generation, summarization, and instruction-following show that W2S-AlignTree consistently outperforms strong baselines. Notably, W2S-AlignTree raises the performance of Llama3-8B from 1.89 to 2.19, a relative improvement of 15.9 on the summarization task.</p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å±•ç¤ºå‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ï¼Œç„¶è€Œç”±äºå…¶å¼±ç›‘ç£çš„ä¸å……åˆ†æ€§å’Œç¼ºä¹ç²¾ç»†çš„çš„æ§åˆ¶ï¼Œå®ƒä»¬çš„è¾“å‡ºå¸¸å¸¸ä¸äººç±»åå¥½ä¸ç¬¦ã€‚åƒå¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰è¿™æ ·çš„è®­ç»ƒæ—¶å¯¹é½æ–¹æ³•é¢ä¸´ç€ä¸“å®¶ç›‘ç£æˆæœ¬é«˜æ˜‚å’Œå›ºæœ‰çš„å¯æ‰©å±•æ€§é™åˆ¶çš„é—®é¢˜ï¼Œä»¥åŠåœ¨æ¨ç†è¿‡ç¨‹ä¸­åŠ¨æ€æ§åˆ¶çš„å±€é™æ€§ã€‚å› æ­¤ï¼Œæ€¥éœ€ä¸€ç§å¯æ‰©å±•ä¸”å¯é€‚åº”çš„å¯¹é½æœºåˆ¶ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†W2S-AlignTreeï¼Œè¿™æ˜¯ä¸€ç§é¦–åˆ›çš„å³æ’å³ç”¨æ¨ç†æ—¶é—´å¯¹é½æ¡†æ¶ï¼Œå®ƒååŒç»“åˆäº†è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰ä¸å¼ºå¼±æ³›åŒ–èŒƒå¼ã€‚W2S-AlignTreeå°†LLMå¯¹é½è¡¨è¿°ä¸ºç”Ÿæˆæœç´¢æ ‘å†…çš„æœ€ä¼˜å¯å‘å¼æœç´¢é—®é¢˜ã€‚å®ƒé€šè¿‡åˆ©ç”¨å¼±æ¨¡å‹çš„å®æ—¶ã€æ­¥éª¤çº§ä¿¡å·ä½œä¸ºå¯¹é½ä»£ç†ï¼Œå¹¶å¼•å…¥ç†µæ„ŸçŸ¥æ¢ç´¢æœºåˆ¶ï¼Œå®ç°åœ¨å¼ºæ¨¡å‹ç”Ÿæˆè¿‡ç¨‹ä¸­è¿›è¡Œç²¾ç»†æŒ‡å¯¼ï¼ŒåŒæ—¶æ— éœ€ä¿®æ”¹å…¶å‚æ•°ã€‚è¯¥æ–¹æ³•åœ¨é«˜ç»´ç”Ÿæˆæœç´¢æ ‘ä¸­åŠ¨æ€å¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨ã€‚åœ¨å—æ§çš„æƒ…æ„Ÿç”Ÿæˆã€æ‘˜è¦å’ŒæŒ‡ä»¤éµå¾ªæ–¹é¢çš„å®éªŒè¡¨æ˜ï¼ŒW2S-AlignTreeå§‹ç»ˆä¼˜äºå¼ºåŠ²çš„åŸºçº¿ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒW2S-AlignTreeæé«˜äº†æ‘˜è¦ä»»åŠ¡ä¸­Llama3-8Bçš„æ€§èƒ½ï¼Œä»1.89æå‡è‡³2.19ï¼Œç›¸å¯¹æ”¹è¿›äº†15.9%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.11518v1">PDF</a> AAAI 2026 Oral</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è™½å±•ç°å‡ºå¼ºå¤§èƒ½åŠ›ï¼Œä½†å…¶è¾“å‡ºå¸¸å› ç¼ºä¹ç²¾ç»†æ§åˆ¶å’Œå¼±ç›‘ç£ä¸è¶³è€Œä¸äººåå¥½ä¸ç¬¦ã€‚è®­ç»ƒæ—¶çš„å¯¹é½æ–¹æ³•å¦‚å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰å­˜åœ¨ä¸“å®¶ç›‘ç£æˆæœ¬é«˜å’Œå¯ä¼¸ç¼©æ€§å›ºæœ‰çš„å±€é™ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºW2S-AlignTreeï¼Œä¸€ä¸ªå¼€åˆ›æ€§çš„å³æ’å³ç”¨æ¨ç†æ—¶å¯¹é½æ¡†æ¶ï¼Œé¦–æ¬¡ç»“åˆè’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰ä¸å¼±åˆ°å¼ºæ³›åŒ–èŒƒå¼ã€‚W2S-AlignTreeå°†LLMå¯¹é½è¡¨è¿°ä¸ºç”Ÿæˆæœç´¢æ ‘å†…çš„æœ€ä¼˜å¯å‘å¼æœç´¢é—®é¢˜ã€‚å®ƒåˆ©ç”¨å¼±æ¨¡å‹çš„å®æ—¶æ­¥éª¤çº§ä¿¡å·ä½œä¸ºå¯¹é½ä»£ç†ï¼Œå¹¶å¼•å…¥ç†µæ„ŸçŸ¥æ¢ç´¢æœºåˆ¶ï¼Œåœ¨å¼ºæ¨¡å‹ç”Ÿæˆè¿‡ç¨‹ä¸­å®ç°ç²¾ç»†æŒ‡å¯¼è€Œä¸ä¿®æ”¹å…¶å‚æ•°ã€‚è¯¥æ–¹æ³•åœ¨é«˜ç»´ç”Ÿæˆæœç´¢æ ‘ä¸­å®ç°äº†æ¢ç´¢ä¸å¼€å‘çš„åŠ¨æ€å¹³è¡¡ã€‚å®éªŒè¡¨æ˜ï¼ŒW2S-AlignTreeåœ¨æƒ…æ„Ÿç”Ÿæˆã€æ‘˜è¦å’ŒæŒ‡ä»¤éµå¾ªç­‰æ–¹é¢å‡ä¼˜äºå¼ºåŸºçº¿ã€‚ç‰¹åˆ«æ˜¯åœ¨æ‘˜è¦ä»»åŠ¡ä¸Šï¼ŒW2S-AlignTreeå°†Llama3-8Bçš„æ€§èƒ½ä»1.89æé«˜åˆ°2.19ï¼Œç›¸å¯¹æ”¹è¿›äº†15.9%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsè™½èƒ½åŠ›å¼ºå¤§ï¼Œä½†è¾“å‡ºä¸äººåå¥½ä¸ç¬¦ï¼Œéœ€å¯¹é½æœºåˆ¶ã€‚</li>
<li>è®­ç»ƒæ—¶å¯¹é½æ–¹æ³•å¦‚RLHFå­˜åœ¨ä¸“å®¶ç›‘ç£æˆæœ¬é«˜å’Œå¯æ‰©å±•æ€§é™åˆ¶ã€‚</li>
<li>W2S-AlignTreeæ˜¯é¦–ä¸ªå³æ’å³ç”¨æ¨ç†æ—¶å¯¹é½æ¡†æ¶ï¼Œç»“åˆMCTSå’Œå¼±åˆ°å¼ºæ³›åŒ–èŒƒå¼ã€‚</li>
<li>W2S-AlignTreeå°†LLMå¯¹é½è¡¨è¿°ä¸ºç”Ÿæˆæœç´¢æ ‘å†…çš„æœ€ä¼˜å¯å‘å¼æœç´¢é—®é¢˜ã€‚</li>
<li>W2S-AlignTreeåˆ©ç”¨å¼±æ¨¡å‹ä¿¡å·ä½œä¸ºå¯¹é½ä»£ç†ï¼Œå®ç°ç²¾ç»†æŒ‡å¯¼å¼ºæ¨¡å‹ç”Ÿæˆã€‚</li>
<li>W2S-AlignTreeåœ¨é«˜ç»´ç”Ÿæˆæœç´¢æ ‘ä¸­å¹³è¡¡æ¢ç´¢ä¸å¼€å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.11518">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-de69885c30ae78ac5302d0fff36272b9" align="middle">
<img src="https://picx.zhimg.com/v2-a2daf1c51eb6dcd3d0082159ae5d559d" align="middle">
<img src="https://picx.zhimg.com/v2-4d881ca431f283e21536265b4000f908" align="middle">
<img src="https://picx.zhimg.com/v2-1eea6a8df130b7f589c02b33711bf882" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="FarSkip-Collective-Unhobbling-Blocking-Communication-in-Mixture-of-Experts-Models"><a href="#FarSkip-Collective-Unhobbling-Blocking-Communication-in-Mixture-of-Experts-Models" class="headerlink" title="FarSkip-Collective: Unhobbling Blocking Communication in Mixture of Experts Models"></a>FarSkip-Collective: Unhobbling Blocking Communication in Mixture of Experts Models</h2><p><strong>Authors:Yonatan Dukler, Guihong Li, Deval Shah, Vikram Appia, Emad Barsoum</strong></p>
<p>Blocking communication presents a major hurdle in running MoEs efficiently in distributed settings. To address this, we present FarSkip-Collective which modifies the architecture of modern models to enable overlapping of their computation with communication. Our approach modifies the architecture to skip connections in the model and it is unclear a priori whether the modified model architecture can remain as capable, especially for large state-of-the-art models and while modifying all of the model layers. We answer this question in the affirmative and fully convert a series of state-of-the-art models varying from 16B to 109B parameters to enable overlapping of their communication while achieving accuracy on par with their original open-source releases. For example, we convert Llama 4 Scout (109B) via self-distillation and achieve average accuracy within 1% of its instruction tuned release averaged across a wide range of downstream evaluations. In addition to demonstrating retained accuracy of the large modified models, we realize the benefits of FarSkip-Collective through optimized implementations that explicitly overlap communication with computation, accelerating both training and inference in existing frameworks.</p>
<blockquote>
<p>åœ¨åˆ†å¸ƒå¼ç¯å¢ƒä¸­é«˜æ•ˆè¿è¡ŒMoEï¼ˆå·¨æ¨¡å‹ï¼‰æ—¶ï¼Œé€šä¿¡é˜»å¡æˆä¸ºä¸€ä¸ªä¸»è¦éšœç¢ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†FarSkip-Collectiveæ–¹æ³•ï¼Œå®ƒé€šè¿‡ä¿®æ”¹ç°ä»£æ¨¡å‹çš„æ¶æ„æ¥å®ç°åœ¨è®¡ç®—è¿‡ç¨‹ä¸­é‡å é€šä¿¡ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡è·³è¿‡æ¨¡å‹ä¸­çš„è¿æ¥æ¥ä¿®æ”¹æ¶æ„ï¼Œäº‹å…ˆå¹¶ä¸æ¸…æ¥šä¿®æ”¹åçš„æ¨¡å‹æ¶æ„æ˜¯å¦è¿˜èƒ½ä¿æŒå…¶åŠŸèƒ½ï¼Œç‰¹åˆ«æ˜¯å¯¹äºå¤§å‹æœ€å…ˆè¿›çš„æ¨¡å‹å’Œéœ€è¦ä¿®æ”¹æ‰€æœ‰æ¨¡å‹å±‚çš„æƒ…å†µã€‚æˆ‘ä»¬å¯¹æ­¤é—®é¢˜ä½œå‡ºäº†è‚¯å®šå›ç­”ï¼Œå¹¶å°†ä¸€ç³»åˆ—æœ€å…ˆè¿›çš„æ¨¡å‹ä»16Båˆ°109Bå‚æ•°è¿›è¡Œäº†å…¨é¢è½¬æ¢ï¼Œä½¿å®ƒä»¬åœ¨å®ç°é€šä¿¡é‡å çš„åŒæ—¶è¾¾åˆ°äº†ä¸åŸå§‹å¼€æºç‰ˆæœ¬ç›¸å½“çš„ç²¾åº¦ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬é€šè¿‡è‡ªæˆ‘è’¸é¦æŠ€æœ¯è½¬åŒ–äº†Llama 4 Scoutï¼ˆ109Bï¼‰ï¼Œå¹¶åœ¨ä¸€ç³»åˆ—ä¸‹æ¸¸è¯„ä¼°ä¸­ï¼Œå…¶å¹³å‡ç²¾åº¦è¾¾åˆ°äº†æŒ‡ä»¤è°ƒä¼˜ç‰ˆæœ¬çš„99%ã€‚é™¤äº†è¯æ˜å¤§å‹ä¿®æ”¹æ¨¡å‹çš„å‡†ç¡®æ€§å¾—ä»¥ä¿ç•™å¤–ï¼Œæˆ‘ä»¬è¿˜é€šè¿‡ä¼˜åŒ–å®ç°æ¥æ˜ç¡®å®ç°é€šä¿¡ä¸è®¡ç®—çš„é‡å ï¼Œä»è€ŒåŠ é€Ÿç°æœ‰æ¡†æ¶çš„è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ï¼Œä½“ç°äº†FarSkip-Collectiveçš„ä¼˜åŠ¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.11505v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨åˆ†å¸ƒå¼ç¯å¢ƒä¸­è¿è¡ŒMoEï¼ˆæ¨¡å‹å¹¶è¡Œè®¡ç®—ï¼‰æ—¶é€šä¿¡é˜»å¡å¸¦æ¥çš„æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†FarSkip-Collectiveæ–¹æ³•ï¼Œå®ƒé€šè¿‡ä¿®æ”¹ç°ä»£æ¨¡å‹çš„æ¶æ„æ¥å®ç°è®¡ç®—ä¸é€šä¿¡çš„é‡å ã€‚è¯¥æ–¹æ³•é€šè¿‡è·³è¿‡æ¨¡å‹ä¸­çš„è¿æ¥æ¥ä¿®æ”¹æ¶æ„ï¼Œä¸ç¡®å®šä¿®æ”¹åçš„æ¨¡å‹æ¶æ„æ˜¯å¦ä»ç„¶å…·å¤‡èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤§å‹å…ˆè¿›æ¨¡å‹å’Œæ‰€æœ‰æ¨¡å‹å±‚çš„ä¿®æ”¹ä¸­ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè¿™ç§æ–¹æ³•åœ¨å¤§å‹æ¨¡å‹ï¼ˆä»16Båˆ°109Bå‚æ•°ï¼‰ä¸Šæœ‰æ•ˆï¼Œå¯å®ç°é€šä¿¡çš„é‡å ï¼ŒåŒæ—¶ä¿æŒä¸åŸå§‹å¼€æºå‘å¸ƒç›¸å½“çš„å‡†ç¡®æ€§ã€‚ä¾‹å¦‚ï¼Œé€šè¿‡è‡ªè’¸é¦è½¬åŒ–Llama 4 Scoutï¼ˆ109Bï¼‰ï¼Œåœ¨å¹¿æ³›çš„ä¸‹æ¸¸è¯„ä¼°ä¸­ï¼Œå…¶å¹³å‡å‡†ç¡®åº¦ä¿æŒåœ¨æŒ‡ä»¤è°ƒæ•´å‘å¸ƒçš„1%ä»¥å†…ã€‚æ­¤å¤–ï¼Œé™¤äº†å±•ç¤ºå¤§å‹ä¿®æ”¹æ¨¡å‹çš„ä¿ç•™å‡†ç¡®æ€§å¤–ï¼Œæˆ‘ä»¬è¿˜é€šè¿‡æ˜ç¡®å®ç°è®¡ç®—ä¸é€šä¿¡çš„é‡å æ¥ä¼˜åŒ–FarSkip-Collectiveçš„å®ç°ï¼Œä»è€ŒåŠ é€Ÿç°æœ‰æ¡†æ¶çš„è®­ç»ƒå’Œæ¨ç†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FarSkip-Collectiveè§£å†³äº†åœ¨åˆ†å¸ƒå¼ç¯å¢ƒä¸­è¿è¡ŒMoEæ—¶çš„é€šä¿¡é˜»å¡é—®é¢˜ã€‚</li>
<li>é€šè¿‡ä¿®æ”¹ç°ä»£æ¨¡å‹çš„æ¶æ„ï¼ŒFarSkip-Collectiveå®ç°äº†è®¡ç®—ä¸é€šä¿¡çš„é‡å ã€‚</li>
<li>è·³è¿‡è¿æ¥çš„æ–¹å¼ä¿®æ”¹æ¨¡å‹æ¶æ„å¯¹äºå¤§å‹å…ˆè¿›æ¨¡å‹æ˜¯å¦ä»ç„¶å…·å¤‡èƒ½åŠ›å­˜åœ¨ä¸ç¡®å®šæ€§ã€‚</li>
<li>ç ”ç©¶ç»“æœè¯æ˜FarSkip-Collectiveåœ¨å¤§å‹æ¨¡å‹ä¸Šæœ‰æ•ˆï¼Œå¯ä¿æŒä¸åŸå§‹æ¨¡å‹ç›¸å½“çš„é«˜å‡†ç¡®æ€§ã€‚</li>
<li>é€šè¿‡è‡ªè’¸é¦æŠ€æœ¯è½¬åŒ–Llama 4 Scoutæ¨¡å‹ï¼Œå…¶å‡†ç¡®åº¦ä¿æŒåœ¨æŒ‡ä»¤è°ƒæ•´åçš„1%ä»¥å†…ã€‚</li>
<li>FarSkip-Collectiveä¼˜åŒ–çš„å®æ–½æ˜ç¡®äº†è®¡ç®—ä¸é€šä¿¡çš„é‡å ï¼ŒåŠ é€Ÿäº†è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.11505">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-53760899ca67ff430581dc6ca7b75e9d" align="middle">
<img src="https://picx.zhimg.com/v2-7ddd3912739ef9c6614c661be937d566" align="middle">
<img src="https://picx.zhimg.com/v2-fabf4cc0f0e04c4fc7b6f0047c8078a5" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="PAS-Prelim-Attention-Score-for-Detecting-Object-Hallucinations-in-Large-Visionâ€“Language-Models"><a href="#PAS-Prelim-Attention-Score-for-Detecting-Object-Hallucinations-in-Large-Visionâ€“Language-Models" class="headerlink" title="PAS : Prelim Attention Score for Detecting Object Hallucinations in Large Visionâ€“Language Models"></a>PAS : Prelim Attention Score for Detecting Object Hallucinations in Large Visionâ€“Language Models</h2><p><strong>Authors:Nhat Hoang-Xuan, Minh Vu, My T. Thai, Manish Bhattarai</strong></p>
<p>Large vision-language models (LVLMs) are powerful, yet they remain unreliable due to object hallucinations. In this work, we show that in many hallucinatory predictions the LVLM effectively ignores the image and instead relies on previously generated output (prelim) tokens to infer new objects. We quantify this behavior via the mutual information between the image and the predicted object conditioned on the prelim, demonstrating that weak image dependence strongly correlates with hallucination. Building on this finding, we introduce the Prelim Attention Score (PAS), a lightweight, training-free signal computed from attention weights over prelim tokens. PAS requires no additional forward passes and can be computed on the fly during inference. Exploiting this previously overlooked signal, PAS achieves state-of-the-art object-hallucination detection across multiple models and datasets, enabling real-time filtering and intervention.</p>
<blockquote>
<p>å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰è™½ç„¶åŠŸèƒ½å¼ºå¤§ï¼Œä½†ç”±äºç‰©ä½“å¹»è§‰ï¼ˆhallucinationï¼‰çš„å­˜åœ¨ï¼Œå…¶å¯é æ€§ä»ç„¶æœ‰å¾…æé«˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å‘ç°LVLMåœ¨è®¸å¤šå¹»è§‰é¢„æµ‹ä¸­ä¼šæœ‰æ•ˆåœ°å¿½ç•¥å›¾åƒï¼Œè€Œæ˜¯ä¾èµ–äºå…ˆå‰ç”Ÿæˆçš„è¾“å‡ºï¼ˆåˆæ­¥ï¼‰ä»¤ç‰Œæ¥æ¨æ–­æ–°ç‰©ä½“ã€‚æˆ‘ä»¬é€šè¿‡åˆæ­¥æ¡ä»¶ä¸‹å›¾åƒå’Œé¢„æµ‹ç‰©ä½“ä¹‹é—´çš„äº’ä¿¡æ¯æ¥è¡¡é‡è¿™ç§è¡Œä¸ºï¼Œè¯æ˜å¼±å›¾åƒä¾èµ–æ€§ä¸å¹»è§‰ä¹‹é—´å­˜åœ¨å¼ºçƒˆçš„ç›¸å…³æ€§ã€‚åŸºäºè¿™ä¸€å‘ç°ï¼Œæˆ‘ä»¬å¼•å…¥äº†åˆæ­¥æ³¨æ„åŠ›å¾—åˆ†ï¼ˆPASï¼‰ï¼Œè¿™æ˜¯ä¸€ç§è½»é‡çº§çš„ã€æ— éœ€è®­ç»ƒçš„ä¿¡å·ï¼Œé€šè¿‡è®¡ç®—åˆæ­¥ä»¤ç‰Œä¸Šçš„æ³¨æ„åŠ›æƒé‡å¾—å‡ºã€‚PASä¸éœ€è¦é¢å¤–çš„å‰å‘ä¼ é€’ï¼Œå¯ä»¥åœ¨æ¨ç†è¿‡ç¨‹ä¸­å³æ—¶è®¡ç®—ã€‚é€šè¿‡åˆ©ç”¨è¿™ä¸€ä»¥å‰è¢«å¿½è§†çš„ä¿¡å·ï¼ŒPASåœ¨å¤šä¸ªæ¨¡å‹å’Œæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„ç‰©ä½“å¹»è§‰æ£€æµ‹ï¼Œèƒ½å¤Ÿå®ç°å®æ—¶è¿‡æ»¤å’Œå¹²é¢„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.11502v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰è™½ç„¶å¼ºå¤§ï¼Œä½†ç”±äºç‰©ä½“å¹»è§‰ç°è±¡è€Œä¸å¤Ÿå¯é ã€‚æœ¬ç ”ç©¶å‘ç°ï¼Œåœ¨è®¸å¤šå¹»è§‰é¢„æµ‹ä¸­ï¼ŒLVLMå®é™…ä¸Šå¿½ç•¥äº†å›¾åƒï¼Œè€Œæ˜¯ä¾èµ–äºå…ˆå‰ç”Ÿæˆçš„è¾“å‡ºï¼ˆåˆæ­¥ï¼‰ä»¤ç‰Œæ¥æ¨æ–­æ–°ç‰©ä½“ã€‚é€šè¿‡åˆæ­¥ä»¤ç‰Œä¸é¢„æµ‹ç‰©ä½“ä¹‹é—´çš„äº’ä¿¡æ¯æ¥è¡¡é‡è¿™ç§è¡Œä¸ºï¼Œå‘ç°å¼±å›¾åƒä¾èµ–ä¸å¹»è§‰ä¹‹é—´å­˜åœ¨å¼ºçƒˆçš„ç›¸å…³æ€§ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬å¼•å…¥äº†åˆæ­¥æ³¨æ„åŠ›å¾—åˆ†ï¼ˆPASï¼‰ï¼Œè¿™æ˜¯ä¸€ç§è½»é‡çº§çš„ã€æ— éœ€è®­ç»ƒçš„ä¿¡å·ï¼Œé€šè¿‡è®¡ç®—åˆæ­¥ä»¤ç‰Œä¸Šçš„æ³¨æ„åŠ›æƒé‡å¾—åˆ°ã€‚PASæ— éœ€é¢å¤–çš„å‰å‘ä¼ é€’ï¼Œå¯ä»¥åœ¨æ¨ç†è¿‡ç¨‹ä¸­å®æ—¶è®¡ç®—ã€‚åˆ©ç”¨è¿™ä¸€ä»¥å‰è¢«å¿½è§†çš„ä¿¡å·ï¼ŒPASåœ¨å¤šä¸ªæ¨¡å‹å’Œæ•°æ®é›†ä¸Šå®ç°äº†ç‰©ä½“å¹»è§‰æ£€æµ‹çš„æœ€ä½³æ€§èƒ½ï¼Œèƒ½å¤Ÿå®ç°å®æ—¶è¿‡æ»¤å’Œå¹²é¢„ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>LVLMså­˜åœ¨ç‰©ä½“å¹»è§‰é—®é¢˜ï¼Œå³æ¨¡å‹åœ¨é¢„æµ‹æ—¶å¯èƒ½å¿½ç•¥å›¾åƒï¼Œä¾èµ–å…ˆå‰ç”Ÿæˆçš„è¾“å‡ºä»¤ç‰Œæ¥æ¨æ–­æ–°ç‰©ä½“ã€‚</li>
<li>åˆæ­¥ä»¤ç‰Œä¸é¢„æµ‹ç‰©ä½“ä¹‹é—´çš„äº’ä¿¡æ¯å¼±ï¼Œè¡¨æ˜è¿™ç§è¡Œä¸ºä¸ç‰©ä½“å¹»è§‰ä¹‹é—´å­˜åœ¨å¼ºçƒˆç›¸å…³æ€§ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°çš„ä¿¡å·â€”â€”åˆæ­¥æ³¨æ„åŠ›å¾—åˆ†ï¼ˆPASï¼‰ï¼Œç”¨äºæ£€æµ‹ç‰©ä½“å¹»è§‰ã€‚</li>
<li>PASæ˜¯ä¸€ç§è½»é‡çº§ã€æ— éœ€è®­ç»ƒçš„ä¿¡å·ï¼Œé€šè¿‡è®¡ç®—åˆæ­¥ä»¤ç‰Œä¸Šçš„æ³¨æ„åŠ›æƒé‡å¾—åˆ°ï¼Œå¯åœ¨æ¨ç†è¿‡ç¨‹ä¸­å®æ—¶è®¡ç®—ã€‚</li>
<li>PASèƒ½åœ¨å¤šä¸ªæ¨¡å‹å’Œæ•°æ®é›†ä¸Šå®ç°æœ€ä½³ç‰©ä½“å¹»è§‰æ£€æµ‹æ€§èƒ½ã€‚</li>
<li>PASèƒ½å®ç°å®æ—¶è¿‡æ»¤å’Œå¹²é¢„ï¼Œæœ‰åŠ©äºæå‡LVLMsçš„å¯é æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.11502">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2adbce5b1adb2b1ece6031e0eabb2c5b" align="middle">
<img src="https://picx.zhimg.com/v2-1e3b78efa05aafc4c8bab5920e38d3c0" align="middle">
<img src="https://picx.zhimg.com/v2-5455b3082c98b139472decccdec95e8c" align="middle">
<img src="https://picx.zhimg.com/v2-203aec17372c41e387368ff6d8575819" align="middle">
<img src="https://picx.zhimg.com/v2-b551ea586e5175754fee91c2b139b019" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="VP-Bench-A-Comprehensive-Benchmark-for-Visual-Prompting-in-Multimodal-Large-Language-Models"><a href="#VP-Bench-A-Comprehensive-Benchmark-for-Visual-Prompting-in-Multimodal-Large-Language-Models" class="headerlink" title="VP-Bench: A Comprehensive Benchmark for Visual Prompting in Multimodal Large Language Models"></a>VP-Bench: A Comprehensive Benchmark for Visual Prompting in Multimodal Large Language Models</h2><p><strong>Authors:Mingjie Xu, Jinpeng Chen, Yuzhi Zhao, Jason Chun Lok Li, Yue Qiu, Zekang Du, Mengyang Wu, Pingping Zhang, Kun Li, Hongzheng Yang, Wenao Ma, Jiaheng Wei, Qinbin Li, Kangcheng Liu, Wenqiang Lei</strong></p>
<p>Multimodal large language models (MLLMs) have enabled a wide range of advanced vision-language applications, including fine-grained object recognition and contextual understanding. When querying specific regions or objects in an image, human users naturally use â€œvisual promptsâ€ (VPs), such as bounding boxes, to provide reference. However, no existing benchmark systematically evaluates the ability of MLLMs to interpret such VPs. This gap leaves it unclear whether current MLLMs can effectively recognize VPs, an intuitive prompting method for humans, and use them to solve problems. To address this limitation, we introduce VP-Bench, a benchmark for assessing MLLMsâ€™ capability in VP perception and utilization. VP-Bench employs a two-stage evaluation framework: Stage 1 examines modelsâ€™ ability to perceive VPs in natural scenes, using 30k visualized prompts spanning eight shapes and 355 attribute combinations. Stage 2 investigates the impact of VPs on downstream tasks, measuring their effectiveness in real-world problem-solving scenarios. Using VP-Bench, we evaluate 28 MLLMs, including proprietary systems (e.g., GPT-4o) and open-source models (e.g., InternVL3 and Qwen2.5-VL), and provide a comprehensive analysis of factors that affect VP understanding, such as variations in VP attributes, question arrangement, and model scale. VP-Bench establishes a new reference framework for studying how MLLMs comprehend and resolve grounded referring questions.</p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å·²ç»æ”¯æŒäº†ä¸€ç³»åˆ—å…ˆè¿›çš„è§†è§‰è¯­è¨€åº”ç”¨ï¼ŒåŒ…æ‹¬ç²¾ç»†ç²’åº¦å¯¹è±¡è¯†åˆ«å’Œä¸Šä¸‹æ–‡ç†è§£ã€‚å½“æŸ¥è¯¢å›¾åƒä¸­çš„ç‰¹å®šåŒºåŸŸæˆ–å¯¹è±¡æ—¶ï¼Œäººç±»ç”¨æˆ·è‡ªç„¶åœ°ä½¿ç”¨â€œè§†è§‰æç¤ºâ€ï¼ˆVPsï¼‰ï¼Œå¦‚è¾¹ç•Œæ¡†ï¼Œä»¥æä¾›å‚è€ƒã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºå‡†æµ‹è¯•å¹¶æœªç³»ç»Ÿåœ°è¯„ä¼°MLLMsè§£é‡Šè¿™ç§VPsçš„èƒ½åŠ›ã€‚è¿™ä¸€å·®è·ä½¿å¾—å½“å‰MLLMsæ˜¯å¦èƒ½å¤Ÿæœ‰æ•ˆåœ°è¯†åˆ«VPsï¼ˆä¸€ç§äººç±»ç›´è§‚çš„æç¤ºæ–¹æ³•ï¼‰å¹¶åˆ©ç”¨å®ƒä»¬è§£å†³é—®é¢˜å°šä¸æ¸…æ¥šã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†VP-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªè¯„ä¼°MLLMsåœ¨VPæ„ŸçŸ¥å’Œåˆ©ç”¨èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚VP-Benché‡‡ç”¨ä¸¤é˜¶æ®µè¯„ä¼°æ¡†æ¶ï¼šç¬¬ä¸€é˜¶æ®µè€ƒå¯Ÿæ¨¡å‹åœ¨è‡ªç„¶åœºæ™¯ä¸­æ„ŸçŸ¥VPsçš„èƒ½åŠ›ï¼Œä½¿ç”¨æ¶µç›–å…«ç§å½¢çŠ¶å’Œ355ç§å±æ€§ç»„åˆçš„3ä¸‡å¤šä¸ªå¯è§†åŒ–æç¤ºã€‚ç¬¬äºŒé˜¶æ®µç ”ç©¶VPså¯¹ä¸‹æ¸¸ä»»åŠ¡çš„å½±å“ï¼Œè¡¡é‡å®ƒä»¬åœ¨ç°å®é—®é¢˜è§£å†³åœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§ã€‚ä½¿ç”¨VP-Benchï¼Œæˆ‘ä»¬è¯„ä¼°äº†28ä¸ªMLLMsï¼ŒåŒ…æ‹¬ä¸“æœ‰ç³»ç»Ÿï¼ˆå¦‚GPT-4oï¼‰å’Œå¼€æºæ¨¡å‹ï¼ˆå¦‚InternVL3å’ŒQwen2.5-VLï¼‰ï¼Œå¹¶å¯¹å½±å“VPç†è§£çš„å› ç´ è¿›è¡Œäº†ç»¼åˆåˆ†æï¼Œå¦‚VPå±æ€§ã€é—®é¢˜å®‰æ’å’Œæ¨¡å‹è§„æ¨¡çš„å˜åŒ–ã€‚VP-Benchä¸ºç ”ç©¶MLLMså¦‚ä½•ç†è§£å’Œè§£å†³åŸºäºåœ°é¢çš„å¼•ç”¨é—®é¢˜å»ºç«‹äº†æ–°çš„å‚è€ƒæ¡†æ¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.11438v1">PDF</a> This is the extended version of the paper accepted at AAAI 2026, which includes all technical appendices and additional experimental details</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†è§‰è¯­è¨€åº”ç”¨æ–¹é¢çš„èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯é€šè¿‡è§†è§‰æç¤ºï¼ˆVPsï¼‰æ¥è¯†åˆ«å›¾åƒä¸­çš„ç‰¹å®šåŒºåŸŸæˆ–å¯¹è±¡çš„èƒ½åŠ›ã€‚ç°æœ‰çš„åŸºå‡†æµ‹è¯•å¹¶æœªç³»ç»Ÿåœ°è¯„ä¼°MLLMsè§£è¯»VPsçš„èƒ½åŠ›ï¼Œä¸ºæ­¤æœ¬æ–‡å¼•å…¥äº†VP-BenchåŸºå‡†æµ‹è¯•æ¥è¯„ä¼°MLLMså¯¹VPsçš„æ„ŸçŸ¥å’Œåˆ©ç”¨èƒ½åŠ›ã€‚VP-Benché‡‡ç”¨ä¸¤é˜¶æ®µè¯„ä¼°æ¡†æ¶ï¼Œç¬¬ä¸€é˜¶æ®µè€ƒå¯Ÿæ¨¡å‹åœ¨è‡ªç„¶åœºæ™¯ä¸­æ„ŸçŸ¥VPsçš„èƒ½åŠ›ï¼Œç¬¬äºŒé˜¶æ®µç ”ç©¶VPså¯¹ä¸‹æ¸¸ä»»åŠ¡çš„å½±å“ï¼Œè¡¡é‡å…¶åœ¨è§£å†³å®é™…é—®é¢˜åœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§ã€‚é€šè¿‡VP-Benchè¯„ä¼°äº†å¤šä¸ªMLLMsçš„æ€§èƒ½ï¼Œå¹¶åˆ†æäº†å½±å“VPç†è§£çš„å› ç´ ã€‚VP-Benchä¸ºMLLMså¦‚ä½•ç†è§£å’Œè§£å†³åŸºäºè§†è§‰æç¤ºçš„é—®é¢˜æä¾›äº†æ–°çš„å‚è€ƒæ¡†æ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å¹¿æ³›åº”ç”¨äºè§†è§‰è¯­è¨€åº”ç”¨ã€‚</li>
<li>è§†è§‰æç¤ºï¼ˆVPsï¼‰æ˜¯äººç±»ç”¨æˆ·æŸ¥è¯¢å›¾åƒä¸­ç‰¹å®šåŒºåŸŸæˆ–å¯¹è±¡æ—¶çš„è‡ªç„¶æç¤ºæ–¹æ³•ã€‚</li>
<li>ç°æœ‰åŸºå‡†æµ‹è¯•æœªç³»ç»Ÿåœ°è¯„ä¼°MLLMsè§£è¯»VPsçš„èƒ½åŠ›ã€‚</li>
<li>VP-BenchåŸºå‡†æµ‹è¯•ç”¨äºè¯„ä¼°MLLMså¯¹VPsçš„æ„ŸçŸ¥å’Œåˆ©ç”¨èƒ½åŠ›ã€‚</li>
<li>VP-Benché‡‡ç”¨ä¸¤é˜¶æ®µè¯„ä¼°æ¡†æ¶ï¼ŒåŒ…æ‹¬æ„ŸçŸ¥VPsçš„èƒ½åŠ›å’Œåœ¨è§£å†³å®é™…é—®é¢˜åœºæ™¯ä¸­åˆ©ç”¨VPsçš„èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡VP-Benchè¯„ä¼°äº†å¤šä¸ªMLLMsçš„æ€§èƒ½ï¼Œå‘ç°æ¨¡å‹é—´å­˜åœ¨æ€§èƒ½å·®å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.11438">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-39551380403d5bb5b7f84501978165e6" align="middle">
<img src="https://picx.zhimg.com/v2-cadd84da7d0cf5c0acccd1bb1a72c5d6" align="middle">
<img src="https://picx.zhimg.com/v2-f553b9a312f7893473b55805e130c262" align="middle">
<img src="https://picx.zhimg.com/v2-5fd33f5b0d4321563dc90f39a07e649e" align="middle">
<img src="https://picx.zhimg.com/v2-c0b4c25e9671a9b5b915bd966d363c52" align="middle">
<img src="https://picx.zhimg.com/v2-67076cbe9423fc7de80b51dd82a83d08" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Q-Doc-Benchmarking-Document-Image-Quality-Assessment-Capabilities-in-Multi-modal-Large-Language-Models"><a href="#Q-Doc-Benchmarking-Document-Image-Quality-Assessment-Capabilities-in-Multi-modal-Large-Language-Models" class="headerlink" title="Q-Doc: Benchmarking Document Image Quality Assessment Capabilities in Multi-modal Large Language Models"></a>Q-Doc: Benchmarking Document Image Quality Assessment Capabilities in Multi-modal Large Language Models</h2><p><strong>Authors:Jiaxi Huang, Dongxu Wu, Hanwei Zhu, Lingyu Zhu, Jun Xing, Xu Wang, Baoliang Chen</strong></p>
<p>The rapid advancement of Multi-modal Large Language Models (MLLMs) has expanded their capabilities beyond high-level vision tasks. Nevertheless, their potential for Document Image Quality Assessment (DIQA) remains underexplored. To bridge this gap, we propose Q-Doc, a three-tiered evaluation framework for systematically probing DIQA capabilities of MLLMs at coarse, middle, and fine granularity levels. a) At the coarse level, we instruct MLLMs to assign quality scores to document images and analyze their correlation with Quality Annotations. b) At the middle level, we design distortion-type identification tasks, including single-choice and multi-choice tests for multi-distortion scenarios. c) At the fine level, we introduce distortion-severity assessment where MLLMs classify distortion intensity against human-annotated references. Our evaluation demonstrates that while MLLMs possess nascent DIQA abilities, they exhibit critical limitations: inconsistent scoring, distortion misidentification, and severity misjudgment. Significantly, we show that Chain-of-Thought (CoT) prompting substantially enhances performance across all levels. Our work provides a benchmark for DIQA capabilities in MLLMs, revealing pronounced deficiencies in their quality perception and promising pathways for enhancement. The benchmark and code are publicly available at:   <a target="_blank" rel="noopener" href="https://github.com/cydxf/Q-Doc">https://github.com/cydxf/Q-Doc</a>.</p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„å¿«é€Ÿå‘å±•ä½¿å…¶èƒ½åŠ›è¶…è¶Šäº†é«˜çº§è§†è§‰ä»»åŠ¡ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨æ–‡æ¡£å›¾åƒè´¨é‡è¯„ä¼°ï¼ˆDIQAï¼‰æ–¹é¢çš„æ½œåŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†Q-Docï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºç³»ç»Ÿæ¢æµ‹MLLMsåœ¨DIQAæ–¹é¢çš„èƒ½åŠ›çš„ä¸‰å±‚è¯„ä¼°æ¡†æ¶ï¼ŒåŒ…æ‹¬ç²—ç•¥ã€ä¸­ç­‰å’Œç²¾ç»†ç²’åº¦çº§åˆ«ã€‚é¦–å…ˆï¼Œåœ¨ç²—ç•¥çº§åˆ«ä¸Šï¼Œæˆ‘ä»¬æŒ‡å¯¼MLLMsç»™æ–‡æ¡£å›¾åƒåˆ†é…è´¨é‡åˆ†æ•°ï¼Œå¹¶åˆ†æå…¶ä¸è´¨é‡æ³¨é‡Šçš„ç›¸å…³æ€§ã€‚å…¶æ¬¡ï¼Œåœ¨ä¸­ç­‰æ°´å¹³ä¸Šï¼Œæˆ‘ä»¬è®¾è®¡äº†å¯¹å¤±çœŸç±»å‹è¿›è¡Œè¯†åˆ«çš„ä»»åŠ¡ï¼ŒåŒ…æ‹¬é’ˆå¯¹å¤šç§å¤±çœŸåœºæ™¯çš„å•ä¸€é€‰æ‹©å’Œå¤šé¡¹é€‰æ‹©æµ‹è¯•ã€‚æœ€åï¼Œåœ¨ç²¾ç»†æ°´å¹³ä¸Šï¼Œæˆ‘ä»¬å¼•å…¥äº†å¤±çœŸä¸¥é‡ç¨‹åº¦è¯„ä¼°ï¼Œå…¶ä¸­MLLMsæ ¹æ®äººç±»æ³¨é‡Šçš„å‚è€ƒæ¥åˆ†ç±»å¤±çœŸçš„å¼ºåº¦ã€‚æˆ‘ä»¬çš„è¯„ä¼°è¡¨æ˜ï¼Œè™½ç„¶MLLMså…·æœ‰æ–°å…´çš„DIQAèƒ½åŠ›ï¼Œä½†å®ƒä»¬å­˜åœ¨å…³é”®å±€é™ï¼šè¯„åˆ†ä¸ä¸€è‡´ã€å¤±çœŸè¯†åˆ«é”™è¯¯å’Œä¸¥é‡ç¨‹åº¦åˆ¤æ–­é”™è¯¯ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬è¡¨æ˜â€œæ€ç»´é“¾â€ï¼ˆCoTï¼‰æç¤ºæ˜¾è‘—æé«˜äº†æ‰€æœ‰çº§åˆ«çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„å·¥ä½œä¸ºMLLMsçš„DIQAèƒ½åŠ›æä¾›äº†åŸºå‡†æµ‹è¯•ï¼Œæ­ç¤ºäº†å®ƒä»¬åœ¨è´¨é‡æ„ŸçŸ¥æ–¹é¢çš„æ˜æ˜¾ç¼ºé™·ï¼Œå¹¶æŒ‡å‡ºäº†å¢å¼ºæ€§èƒ½çš„æ½œåœ¨é€”å¾„ã€‚åŸºå‡†æµ‹è¯•å’Œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/cydxf/Q-Doc%E4%B8%8A%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/cydxf/Q-Docä¸Šå…¬å¼€è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.11410v1">PDF</a> </p>
<p><strong>Summary</strong><br>MLLMåœ¨å¤šæ¨¡æ€åœºæ™¯çš„åº”ç”¨å·²è¶…è¶Šäº†é«˜å±‚æ¬¡çš„è§†è§‰ä»»åŠ¡ã€‚ä½†å®ƒä»¬åœ¨æ–‡æ¡£å›¾åƒè´¨é‡è¯„ä¼°ï¼ˆDIQAï¼‰é¢†åŸŸçš„æ½œåŠ›å°šæœªå¾—åˆ°å……åˆ†æŒ–æ˜ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæå‡ºäº†Q-Docè¿™ä¸€ä¸‰çº§è¯„ä¼°æ¡†æ¶æ¥ç³»ç»Ÿåœ°æµ‹è¯•MLLMåœ¨ä¸åŒç²¾ç»†ç¨‹åº¦ä¸‹çš„DIQAèƒ½åŠ›ã€‚åŒ…æ‹¬è´¨é‡è¯„åˆ†ã€å¤±çœŸç±»å‹è¯†åˆ«å’Œå¤±çœŸä¸¥é‡ç¨‹åº¦è¯„ä¼°ç­‰æ–¹é¢ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒMLLMå…·æœ‰åˆæ­¥çš„DIQAèƒ½åŠ›ï¼Œä½†åœ¨è¯„åˆ†ä¸€è‡´æ€§ã€å¤±çœŸè¯†åˆ«å’Œä¸¥é‡ç¨‹åº¦åˆ¤æ–­æ–¹é¢å­˜åœ¨æ˜¾è‘—å±€é™ã€‚é‡‡ç”¨æ€ç»´é“¾ï¼ˆCoTï¼‰æç¤ºå¯æ˜¾è‘—æé«˜å„å±‚çº§æ€§èƒ½ã€‚è¯¥ç ”ç©¶ä¸ºMLLMåœ¨DIQAæ–¹é¢çš„èƒ½åŠ›æä¾›äº†åŸºå‡†æµ‹è¯•ï¼Œæ­ç¤ºäº†å…¶åœ¨è´¨é‡æ„ŸçŸ¥æ–¹é¢çš„æ˜æ˜¾ä¸è¶³ï¼Œå¹¶ä¸ºæœªæ¥çš„æ”¹è¿›æŒ‡æ˜äº†æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MLLMåœ¨å¤šæ¨¡æ€é¢†åŸŸåº”ç”¨å¹¿æ³›ï¼Œä½†åœ¨æ–‡æ¡£å›¾åƒè´¨é‡è¯„ä¼°ï¼ˆDIQAï¼‰æ–¹é¢çš„åº”ç”¨å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚</li>
<li>Q-Docæ˜¯ä¸€ä¸ªä¸‰çº§è¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨ç³»ç»Ÿåœ°æµ‹è¯•MLLMåœ¨ä¸åŒç²¾ç»†ç¨‹åº¦ä¸‹çš„DIQAèƒ½åŠ›ã€‚</li>
<li>MLLMèƒ½å¤Ÿè¿›è¡Œåˆæ­¥çš„è´¨é‡è¯„åˆ†ï¼Œä½†åœ¨è¯„åˆ†ä¸€è‡´æ€§æ–¹é¢å­˜åœ¨å±€é™ã€‚</li>
<li>MLLMèƒ½å¤Ÿè¯†åˆ«æ–‡æ¡£å›¾åƒçš„å¤±çœŸç±»å‹ï¼Œä½†åœ¨å¤šå¤±çœŸåœºæ™¯çš„è¯†åˆ«ä¸Šä»éœ€æ”¹è¿›ã€‚</li>
<li>MLLMå¯ä»¥è¯„ä¼°å¤±çœŸä¸¥é‡ç¨‹åº¦ï¼Œä½†åœ¨ä¸äººç±»æ³¨é‡Šçš„å‚è€ƒæ ‡å‡†å¯¹æ¯”æ—¶å­˜åœ¨è¯¯å·®ã€‚</li>
<li>æ€ç»´é“¾ï¼ˆCoTï¼‰æç¤ºæœ‰åŠ©äºæé«˜MLLMåœ¨DIQAä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.11410">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-28143b5f6120c088ca3b06c9e2508387" align="middle">
<img src="https://picx.zhimg.com/v2-fbeba7d31964dcd2d2f8c3c7edc37832" align="middle">
<img src="https://picx.zhimg.com/v2-61964175a82668cc0d9544cbe210f30d" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="MicroVQA-High-Quality-Microscopy-Reasoning-Dataset-with-Weakly-Supervised-Graphs-for-Multimodal-Large-Language-Model"><a href="#MicroVQA-High-Quality-Microscopy-Reasoning-Dataset-with-Weakly-Supervised-Graphs-for-Multimodal-Large-Language-Model" class="headerlink" title="MicroVQA++: High-Quality Microscopy Reasoning Dataset with Weakly Supervised Graphs for Multimodal Large Language Model"></a>MicroVQA++: High-Quality Microscopy Reasoning Dataset with Weakly Supervised Graphs for Multimodal Large Language Model</h2><p><strong>Authors:Manyu Li, Ruian He, Chenxi Ma, Weimin Tan, Bo Yan</strong></p>
<p>Multimodal Large Language Models are increasingly applied to biomedical imaging, yet scientific reasoning for microscopy remains limited by the scarcity of large-scale, high-quality training data. We introduce MicroVQA++, a three-stage, large-scale and high-quality microscopy VQA corpus derived from the BIOMEDICA archive. Stage one bootstraps supervision from expert-validated figure-caption pairs sourced from peer-reviewed articles. Stage two applies HiCQA-Graph, a novel heterogeneous graph over images, captions, and QAs that fuses NLI-based textual entailment, CLIP-based vision-language alignment, and agent signals to identify and filter inconsistent samples. Stage three uses a MultiModal Large Language Model (MLLM) agent to generate multiple-choice questions (MCQ) followed by human screening. The resulting release comprises a large training split and a human-checked test split whose Bloomâ€™s level hard-sample distribution exceeds the MicroVQA benchmark. Our work delivers (i) a quality-controlled dataset that couples expert literature with graph-based filtering and human refinement; (ii) HiCQA-Graph, the first graph that jointly models (image, caption, QA) for cross-modal consistency filtering; (iii) evidence that careful data construction enables 4B-scale MLLMs to reach competitive microscopy reasoning performance (e.g., GPT-5) and achieve state-of-the-art performance among open-source MLLMs. Code and dataset will be released after the review process concludes.</p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç”Ÿç‰©åŒ»å­¦æˆåƒæ–¹é¢çš„åº”ç”¨æ—¥ç›Šå¹¿æ³›ï¼Œä½†ç”±äºç¼ºä¹å¤§è§„æ¨¡é«˜è´¨é‡çš„è®­ç»ƒæ•°æ®ï¼Œæ˜¾å¾®é•œçš„ç§‘å­¦æ¨ç†ä»ç„¶å—åˆ°é™åˆ¶ã€‚æˆ‘ä»¬æ¨å‡ºäº†MicroVQA++ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸‰é˜¶æ®µã€å¤§è§„æ¨¡ã€é«˜è´¨é‡çš„æ˜¾å¾®é•œVQAè¯­æ–™åº“ï¼Œæ¥æºäºBIOMEDICAæ¡£æ¡ˆã€‚ç¬¬ä¸€é˜¶æ®µä»ç»è¿‡åŒè¡Œè¯„å®¡çš„æ–‡ç« ä¸­é‡‡é›†çš„ä¸“å®¶éªŒè¯è¿‡çš„å›¾åƒ-æ ‡é¢˜å¯¹ä¸­è·å–ç›‘ç£ä¿¡æ¯ã€‚ç¬¬äºŒé˜¶æ®µåº”ç”¨äº†HiCQA-Graphï¼Œè¿™æ˜¯ä¸€ç§åŸºäºå›¾åƒã€æ ‡é¢˜å’Œé—®ç­”çš„æ–°å‹å¼‚æ„å›¾ï¼Œèåˆäº†åŸºäºè‡ªç„¶è¯­è¨€æ¨æ–­çš„æ–‡æœ¬è•´å«ã€åŸºäºCLIPçš„è§†è§‰è¯­è¨€å¯¹é½å’Œä»£ç†ä¿¡å·ï¼Œä»¥è¯†åˆ«å’Œè¿‡æ»¤ä¸ä¸€è‡´çš„æ ·æœ¬ã€‚ç¬¬ä¸‰é˜¶æ®µä½¿ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ä»£ç†ç”Ÿæˆå¤šé¡¹é€‰æ‹©é¢˜ï¼ˆMCQï¼‰ï¼Œéšåè¿›è¡Œäººå·¥ç­›é€‰ã€‚æœ€ç»ˆå‘å¸ƒçš„æ•°æ®é›†åŒ…æ‹¬ä¸€ä¸ªå¤§è§„æ¨¡çš„è®­ç»ƒé›†å’Œä¸€ä¸ªç»è¿‡äººå·¥æ£€æŸ¥è¿‡çš„æµ‹è¯•é›†ï¼Œå…¶Bloomâ€™séš¾åº¦æ ·æœ¬åˆ†å¸ƒè¶…è¿‡äº†MicroVQAåŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬çš„å·¥ä½œæä¾›äº†ï¼ˆiï¼‰ä¸€ä¸ªç»è¿‡è´¨é‡æ§åˆ¶çš„æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†ç»“åˆäº†ä¸“å®¶æ–‡çŒ®ã€åŸºäºå›¾çš„è¿‡æ»¤å’Œäººå·¥æ”¹è¿›ï¼›ï¼ˆiiï¼‰é¦–ä¸ªè”åˆå»ºæ¨¡ï¼ˆå›¾åƒã€æ ‡é¢˜ã€é—®ç­”ï¼‰çš„HiCQA-Graphï¼Œç”¨äºè·¨æ¨¡æ€ä¸€è‡´æ€§è¿‡æ»¤ï¼›ï¼ˆiiiï¼‰è¯æ®è¡¨æ˜ï¼Œé€šè¿‡ç²¾å¿ƒæ„å»ºçš„æ•°æ®ï¼Œå¯ä»¥ä½¿4Bè§„æ¨¡çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹è¾¾åˆ°å…·æœ‰ç«äº‰åŠ›çš„æ˜¾å¾®é•œæ¨ç†æ€§èƒ½ï¼ˆä¾‹å¦‚GPT-5ï¼‰ï¼Œå¹¶åœ¨å¼€æºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ä¸­è¾¾åˆ°æœ€æ–°æ°´å¹³ã€‚ä»£ç å’Œæ•°æ®é›†å°†åœ¨å®¡æŸ¥è¿‡ç¨‹ç»“æŸåå‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.11407v1">PDF</a> 11 pages, 4 figures</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†MicroVQA++é¡¹ç›®ï¼Œè¯¥é¡¹ç›®é’ˆå¯¹ç”Ÿç‰©åŒ»å­¦æˆåƒä¸­çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åº”ç”¨è¿›è¡Œäº†åˆ›æ–°æ€§çš„å·¥ä½œã€‚è¯¥é¡¹ç›®å¼•å…¥äº†ä¸€ç§æ–°çš„é«˜è´¨é‡æ˜¾å¾®å›¾åƒé—®ç­”è¯­æ–™åº“æ„å»ºæ–¹æ³•ï¼ŒåŒ…å«ä¸‰ä¸ªé˜¶æ®µï¼Œæœ‰æ•ˆè§£å†³äº†å› é«˜è´¨é‡è®­ç»ƒæ•°æ®ç¼ºä¹è€Œå¯¹æ˜¾å¾®é•œè§‚å¯Ÿç§‘å­¦æ¨ç†çš„åˆ¶çº¦ã€‚é¡¹ç›®é¦–æ¬¡æå‡ºå¹¶å®ç°äº†HiCQA-GraphæŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯æ„å»ºäº†ä¸€ä¸ªå›¾åƒã€æè¿°å’Œé—®ç­”ä¹‹é—´çš„å¼‚æ„å›¾ï¼Œå®ç°äº†è·¨æ¨¡æ€ä¸€è‡´æ€§è¿‡æ»¤ã€‚æ­¤å¤–ï¼Œé€šè¿‡ç²¾å¿ƒæ„å»ºçš„æ•°æ®é›†ï¼Œé¡¹ç›®è¯æ˜äº†å¤§å‹æ¨¡å‹åœ¨æ˜¾å¾®é•œæ¨ç†æ–¹é¢çš„ç«äº‰åŠ›æ€§èƒ½ï¼Œå¹¶è¾¾åˆ°äº†å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹çš„æœ€å…ˆè¿›æ€§èƒ½æ°´å¹³ã€‚ä»£ç å’Œæ•°æ®é›†å°†åœ¨å®¡æŸ¥è¿‡ç¨‹ç»“æŸåå‘å¸ƒã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>MicroVQA++é¡¹ç›®é€šè¿‡ä¸‰ä¸ªé˜¶æ®µæ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡ã€é«˜è´¨é‡çš„æ˜¾å¾®å›¾åƒé—®ç­”è¯­æ–™åº“ã€‚</li>
<li>é¡¹ç›®é‡‡ç”¨ä¸“å®¶éªŒè¯çš„å›¾åƒæè¿°å¯¹ä½œä¸ºåˆå§‹ç›‘ç£æ¥æºï¼Œå¹¶ä»åŒè¡Œè¯„å®¡çš„æ–‡ç« ä¸­æŠ½å–ã€‚</li>
<li>HiCQA-GraphæŠ€æœ¯çš„å¼•å…¥å®ç°äº†å›¾åƒã€æè¿°å’Œé—®ç­”ä¹‹é—´çš„å¼‚æ„å›¾æ„å»ºï¼Œæé«˜äº†æ•°æ®è´¨é‡ã€‚</li>
<li>è¯¥æŠ€æœ¯ç»“åˆäº†åŸºäºè‡ªç„¶è¯­è¨€ç†è§£ï¼ˆNLIï¼‰çš„æ–‡æœ¬è•´å«ã€åŸºäºCLIPçš„è§†è§‰è¯­è¨€å¯¹é½å’Œä»£ç†ä¿¡å·ï¼Œä»¥è¯†åˆ«å’Œè¿‡æ»¤ä¸ä¸€è‡´çš„æ ·æœ¬ã€‚</li>
<li>é¡¹ç›®é€šè¿‡ç²¾å¿ƒæ„å»ºçš„æ•°æ®é›†è¯æ˜äº†å¤§å‹æ¨¡å‹åœ¨æ˜¾å¾®é•œæ¨ç†æ–¹é¢çš„æ€§èƒ½æ½œåŠ›ã€‚</li>
<li>ç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨æ­¤æ•°æ®é›†è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹æ€§èƒ½è¾¾åˆ°é¢†å…ˆæ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.11407">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bf3f51d8ea46faef96b08bdd9428fb26" align="middle">
<img src="https://picx.zhimg.com/v2-78b748db358c3a1987af6d8d6f143847" align="middle">
<img src="https://picx.zhimg.com/v2-ba2de71d4a3cc6ef7cda0eb57631a58b" align="middle">
<img src="https://picx.zhimg.com/v2-94106a057e4efaac83999c4c23afb04a" align="middle">
<img src="https://picx.zhimg.com/v2-f1a9f96d7190bcc904fa4541d7ba8cfd" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="When-Genes-Speak-A-Semantic-Guided-Framework-for-Spatially-Resolved-Transcriptomics-Data-Clustering"><a href="#When-Genes-Speak-A-Semantic-Guided-Framework-for-Spatially-Resolved-Transcriptomics-Data-Clustering" class="headerlink" title="When Genes Speak: A Semantic-Guided Framework for Spatially Resolved Transcriptomics Data Clustering"></a>When Genes Speak: A Semantic-Guided Framework for Spatially Resolved Transcriptomics Data Clustering</h2><p><strong>Authors:Jiangkai Long, Yanran Zhu, Chang Tang, Kun Sun, Yuanyuan Liu, Xuesong Yan</strong></p>
<p>Spatial transcriptomics enables gene expression profiling with spatial context, offering unprecedented insights into the tissue microenvironment. However, most computational models treat genes as isolated numerical features, ignoring the rich biological semantics encoded in their symbols. This prevents a truly deep understanding of critical biological characteristics. To overcome this limitation, we present SemST, a semantic-guided deep learning framework for spatial transcriptomics data clustering. SemST leverages Large Language Models (LLMs) to enable genes to â€œspeakâ€ through their symbolic meanings, transforming gene sets within each tissue spot into biologically informed embeddings. These embeddings are then fused with the spatial neighborhood relationships captured by Graph Neural Networks (GNNs), achieving a coherent integration of biological function and spatial structure. We further introduce the Fine-grained Semantic Modulation (FSM) module to optimally exploit these biological priors. The FSM module learns spot-specific affine transformations that empower the semantic embeddings to perform an element-wise calibration of the spatial features, thus dynamically injecting high-order biological knowledge into the spatial context. Extensive experiments on public spatial transcriptomics datasets show that SemST achieves state-of-the-art clustering performance. Crucially, the FSM module exhibits plug-and-play versatility, consistently improving the performance when integrated into other baseline methods.</p>
<blockquote>
<p>ç©ºé—´è½¬å½•ç»„å­¦èƒ½å¤Ÿé€šè¿‡ç©ºé—´ä¸Šä¸‹æ–‡è¿›è¡ŒåŸºå› è¡¨è¾¾è°±åˆ†æï¼Œä¸ºç»„ç»‡å¾®ç¯å¢ƒæä¾›äº†å‰æ‰€æœªæœ‰çš„æ´å¯Ÿã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°è®¡ç®—æ¨¡å‹å°†åŸºå› è§†ä¸ºå­¤ç«‹çš„æ•°å€¼ç‰¹å¾ï¼Œå¿½ç•¥äº†å…¶ç¬¦å·ä¸­æ‰€è•´å«çš„ä¸°å¯Œç”Ÿç‰©å­¦è¯­ä¹‰ã€‚è¿™é˜»ç¢äº†æˆ‘ä»¬å¯¹å…³é”®ç”Ÿç‰©å­¦ç‰¹æ€§çš„çœŸæ­£æ·±å…¥ç†è§£ã€‚ä¸ºäº†å…‹æœè¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†SemSTï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºç©ºé—´è½¬å½•ç»„å­¦æ•°æ®èšç±»çš„è¯­ä¹‰å¼•å¯¼æ·±åº¦å­¦ä¹ æ¡†æ¶ã€‚SemSTåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½¿åŸºå› èƒ½å¤Ÿé€šè¿‡å…¶ç¬¦å·æ„ä¹‰â€œè¯´è¯â€ï¼Œå°†æ¯ä¸ªç»„ç»‡æ–‘ç‚¹å†…çš„åŸºå› é›†è½¬åŒ–ä¸ºç”Ÿç‰©å­¦ä¿¡æ¯åµŒå…¥ã€‚ç„¶åï¼Œè¿™äº›åµŒå…¥ä¸å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰æ•è·çš„ç©ºé—´é‚»åŸŸå…³ç³»èåˆï¼Œå®ç°äº†ç”Ÿç‰©åŠŸèƒ½ä¸ç©ºé—´ç»“æ„çš„è¿è´¯èåˆã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ç²¾ç»†è¯­ä¹‰è°ƒåˆ¶ï¼ˆFSMï¼‰æ¨¡å—ï¼Œä»¥æœ€ä¼˜æ–¹å¼åˆ©ç”¨è¿™äº›ç”Ÿç‰©å­¦å…ˆéªŒã€‚FSMæ¨¡å—å­¦ä¹ ç‰¹å®šäºæ–‘ç‚¹çš„ä»¿å°„å˜æ¢ï¼Œä½¿è¯­ä¹‰åµŒå…¥èƒ½å¤Ÿæ‰§è¡Œç©ºé—´ç‰¹å¾çš„ç‚¹æ ¡ï¼Œä»è€ŒåŠ¨æ€æ³¨å…¥é«˜é˜¶ç”Ÿç‰©å­¦çŸ¥è¯†åˆ°ç©ºé—´ä¸Šä¸‹æ–‡ä¸­ã€‚åœ¨å…¬å…±ç©ºé—´è½¬å½•ç»„å­¦æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒSemSTè¾¾åˆ°äº†æœ€å…ˆè¿›çš„èšç±»æ€§èƒ½ã€‚å…³é”®çš„æ˜¯ï¼ŒFSMæ¨¡å—è¡¨ç°å‡ºå³æ’å³ç”¨çš„é€šç”¨æ€§ï¼Œåœ¨é›†æˆåˆ°å…¶ä»–åŸºå‡†æ–¹æ³•æ—¶å§‹ç»ˆæé«˜äº†æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.11380v1">PDF</a> AAAIâ€™2026 poster paper. 12 pages, 8 figures</p>
<p><strong>Summary</strong></p>
<p>ç©ºé—´è½¬å½•ç»„å­¦èƒ½å¤Ÿç»“åˆç©ºé—´ä¸Šä¸‹æ–‡è¿›è¡ŒåŸºå› è¡¨è¾¾è°±åˆ†æï¼Œæä¾›å¯¹ç»„ç»‡å¾®ç¯å¢ƒçš„å…¨æ–°è§è§£ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°è®¡ç®—æ¨¡å‹å°†åŸºå› è§†ä¸ºå­¤ç«‹çš„æ•°å€¼ç‰¹å¾ï¼Œå¿½ç•¥äº†å…¶ç¬¦å·ä¸­ä¸°å¯Œçš„ç”Ÿç‰©å­¦è¯­ä¹‰ã€‚ä¸ºäº†å…‹æœè¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†SemSTï¼Œä¸€ä¸ªç”¨äºç©ºé—´è½¬å½•ç»„å­¦æ•°æ®èšç±»çš„è¯­ä¹‰å¼•å¯¼æ·±åº¦å­¦ä¹ æ¡†æ¶ã€‚SemSTåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½¿åŸºå› èƒ½å¤Ÿé€šè¿‡å…¶ç¬¦å·æ„ä¹‰â€œè¡¨è¾¾â€ï¼Œå°†æ¯ä¸ªç»„ç»‡æ–‘ç‚¹å†…çš„åŸºå› é›†è½¬åŒ–ä¸ºç”Ÿç‰©å­¦ä¿¡æ¯åµŒå…¥ã€‚è¿™äº›åµŒå…¥ä¸å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰æ•è·çš„ç©ºé—´é‚»åŸŸå…³ç³»ç›¸ç»“åˆï¼Œå®ç°äº†ç”Ÿç‰©å­¦åŠŸèƒ½å’Œç©ºé—´ç»“æ„çš„è¿è´¯èåˆã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ç²¾ç»†è¯­ä¹‰è°ƒåˆ¶ï¼ˆFSMï¼‰æ¨¡å—ï¼Œä»¥æœ€ä¼˜æ–¹å¼åˆ©ç”¨è¿™äº›ç”Ÿç‰©å…ˆéªŒã€‚FSMæ¨¡å—å­¦ä¹ ç‰¹å®šçš„ä»¿å°„å˜æ¢ï¼Œä½¿è¯­ä¹‰åµŒå…¥èƒ½å¤Ÿæ‰§è¡Œç©ºé—´ç‰¹å¾çš„ç©ºé—´æ ¡å‡†ï¼Œä»è€ŒåŠ¨æ€åœ°å°†é«˜é˜¶ç”Ÿç‰©å­¦çŸ¥è¯†æ³¨å…¥ç©ºé—´ä¸Šä¸‹æ–‡ã€‚åœ¨å…¬å…±ç©ºé—´è½¬å½•ç»„å­¦æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒSemSTè¾¾åˆ°äº†æœ€å…ˆè¿›çš„èšç±»æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç©ºé—´è½¬å½•ç»„å­¦å¯é€šè¿‡ç»“åˆç©ºé—´ä¸Šä¸‹æ–‡è¿›è¡ŒåŸºå› è¡¨è¾¾åˆ†æï¼Œæä¾›å¯¹ç»„ç»‡å¾®ç¯å¢ƒçš„æ·±å…¥ç†è§£ã€‚</li>
<li>ç°æœ‰çš„è®¡ç®—æ¨¡å‹å¤šå¿½ç•¥åŸºå› çš„ç”Ÿç‰©å­¦è¯­ä¹‰ï¼ŒSemSTåˆ©ç”¨LLMä½¿åŸºå› é€šè¿‡ç¬¦å·æ„ä¹‰è¡¨è¾¾ã€‚</li>
<li>SemSTå°†åŸºå› é›†è½¬åŒ–ä¸ºç”Ÿç‰©å­¦ä¿¡æ¯åµŒå…¥ï¼Œå¹¶ä¸GNNæ•è·çš„ç©ºé—´å…³ç³»ç»“åˆã€‚</li>
<li>ç²¾ç»†è¯­ä¹‰è°ƒåˆ¶ï¼ˆFSMï¼‰æ¨¡å—ç”¨äºåŠ¨æ€æ³¨å…¥é«˜é˜¶ç”Ÿç‰©å­¦çŸ¥è¯†åˆ°ç©ºé—´ä¸Šä¸‹æ–‡ä¸­ã€‚</li>
<li>SemSTåœ¨å…¬å…±ç©ºé—´è½¬å½•ç»„å­¦æ•°æ®é›†ä¸Šå®ç°æœ€å…ˆè¿›çš„èšç±»æ€§èƒ½ã€‚</li>
<li>FSMæ¨¡å—å…·æœ‰å³æ’å³ç”¨çš„çµæ´»æ€§ï¼Œå¯ä»¥ä¸å…¶ä»–åŸºçº¿æ–¹æ³•é›†æˆä»¥æé«˜æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.11380">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2594ad84b163522abc0b65639140907d" align="middle">
<img src="https://picx.zhimg.com/v2-fef7b778ba921f898e9e1538763bab16" align="middle">
<img src="https://picx.zhimg.com/v2-498bd4be6480f2e49f63c7de7140422f" align="middle">
<img src="https://picx.zhimg.com/v2-d4194134d05d04b04648602a01740aa3" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Instella-Fully-Open-Language-Models-with-Stellar-Performance"><a href="#Instella-Fully-Open-Language-Models-with-Stellar-Performance" class="headerlink" title="Instella: Fully Open Language Models with Stellar Performance"></a>Instella: Fully Open Language Models with Stellar Performance</h2><p><strong>Authors:Jiang Liu, Jialian Wu, Xiaodong Yu, Yusheng Su, Prakamya Mishra, Gowtham Ramesh, Sudhanshu Ranjan, Chaitanya Manem, Ximeng Sun, Ze Wang, Pratik Prabhanjan Brahma, Zicheng Liu, Emad Barsoum</strong></p>
<p>Large language models (LLMs) have demonstrated remarkable performance across a wide range of tasks, yet the majority of high-performing models remain closed-source or partially open, limiting transparency and reproducibility. In this work, we introduce Instella, a family of fully open three billion parameter language models trained entirely on openly available data and codebase. Powered by AMD Instinct MI300X GPUs, Instella is developed through large-scale pre-training, general-purpose instruction tuning, and alignment with human preferences. Despite using substantially fewer pre-training tokens than many contemporaries, Instella achieves state-of-the-art results among fully open models and is competitive with leading open-weight models of comparable size. We further release two specialized variants: Instella-Long, capable of handling context lengths up to 128K tokens, and Instella-Math, a reasoning-focused model enhanced through supervised fine-tuning and reinforcement learning on mathematical tasks. Together, these contributions establish Instella as a transparent, performant, and versatile alternative for the community, advancing the goal of open and reproducible language modeling research.</p>
<blockquote>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œç„¶è€Œï¼Œå¤§å¤šæ•°é«˜æ€§èƒ½æ¨¡å‹ä»ç„¶æ˜¯é—­æºçš„æˆ–éƒ¨åˆ†å¼€æºçš„ï¼Œè¿™é™åˆ¶äº†é€æ˜åº¦å’Œå¯é‡å¤æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†Instellaï¼Œè¿™æ˜¯ä¸€ä¸ªå®Œå…¨å¼€æºçš„ã€åŸºäºå…¬å¼€æ•°æ®å’Œä»£ç åº“è®­ç»ƒçš„ã€æ‹¥æœ‰ä¸‰åäº¿å‚æ•°çš„è¯­è¨€æ¨¡å‹å®¶æ—ã€‚Instellaç”±AMD Instinct MI300X GPUé©±åŠ¨ï¼Œé€šè¿‡å¤§è§„æ¨¡é¢„è®­ç»ƒã€é€šç”¨æŒ‡ä»¤è°ƒæ•´å’Œä¸äººç±»åå¥½å¯¹é½çš„æ–¹å¼å¼€å‘ã€‚å°½ç®¡ä½¿ç”¨çš„é¢„è®­ç»ƒä»¤ç‰Œæ¯”è®¸å¤šåŒé¾„äººå°‘å¾—å¤šï¼Œä½†Instellaåœ¨å®Œå…¨å¼€æºæ¨¡å‹ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æˆæœï¼Œå¹¶ä¸è§„æ¨¡ç›¸ä¼¼çš„é¢†å…ˆå¼€æºæƒé‡æ¨¡å‹å…·æœ‰ç«äº‰åŠ›ã€‚æˆ‘ä»¬è¿˜å‘å¸ƒäº†ä¸¤ä¸ªä¸“ä¸šç‰ˆæœ¬ï¼šèƒ½å¤Ÿå¤„ç†é•¿è¾¾128Kä»¤ç‰Œä¸Šä¸‹æ–‡é•¿åº¦çš„Instella-Longï¼Œä»¥åŠé€šè¿‡ç›‘ç£å¾®è°ƒå¼ºåŒ–å­¦ä¹ åœ¨æ•°å­¦ä»»åŠ¡ä¸Šé‡ç‚¹çªå‡ºçš„æ¨ç†æ¨¡å‹Instella-Mathã€‚æ€»çš„æ¥è¯´ï¼Œè¿™äº›è´¡çŒ®ä½¿Instellaæˆä¸ºç¤¾åŒºä¸­é€æ˜ã€é«˜æ€§èƒ½å’Œé€šç”¨çš„æ›¿ä»£æ–¹æ¡ˆï¼Œæ¨åŠ¨äº†å¼€æºå’Œå¯é‡å¤çš„è¯­è¨€å»ºæ¨¡ç ”ç©¶ç›®æ ‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.10628v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¼€æ”¾æºç çš„è¯­è¨€æ¨¡å‹Instellaåœ¨å…¬å¼€æ•°æ®ä¸Šè®­ç»ƒï¼Œå±•ç°äº†å‡ºè‰²çš„æ€§èƒ½ã€‚æ­¤æ¨¡å‹é€šè¿‡å¤§è§„æ¨¡é¢„è®­ç»ƒã€é€šç”¨æŒ‡ä»¤è°ƒæ•´å’Œä¸äººç±»åå¥½å¯¹é½çš„æ–¹å¼å¼€å‘ï¼Œå°½ç®¡ä½¿ç”¨çš„é¢„è®­ç»ƒä»¤ç‰Œæ•°é‡ç›¸å¯¹è¾ƒå°‘ï¼Œä½†ä»å…·æœ‰æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¿˜å‘å¸ƒäº†ä¸¤ä¸ªä¸“ä¸šç‰ˆæœ¬ï¼šå¤„ç†ä¸Šä¸‹æ–‡é•¿åº¦å¯è¾¾128Kä»¤ç‰Œçš„Instella-Longå’Œä¸“æ³¨äºæ¨ç†çš„Instella-Mathã€‚è¿™äº›è´¡çŒ®ä¸ºç¤¾åŒºå»ºç«‹äº†é€æ˜ã€é«˜æ€§èƒ½å’Œé€šç”¨çš„è¯­è¨€æ¨¡å‹é€‰æ‹©ï¼Œæ¨åŠ¨äº†å¼€æ”¾å’Œå¯é‡å¤çš„è¯­è¨€å»ºæ¨¡ç ”ç©¶ç›®æ ‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Instellaæ˜¯ä¸€ä¸ªå®Œå…¨å¼€æºçš„è¯­è¨€æ¨¡å‹å®¶æ—ï¼Œè®­ç»ƒæ•°æ®å®Œå…¨å…¬å¼€ã€‚</li>
<li>Instellaä½¿ç”¨å¤§è§„æ¨¡é¢„è®­ç»ƒã€é€šç”¨æŒ‡ä»¤è°ƒæ•´å’Œäººç±»åå¥½å¯¹é½çš„æ–¹å¼è¿›è¡Œå¼€å‘ã€‚</li>
<li>ä¸è®¸å¤šå½“ä»£æ¨¡å‹ç›¸æ¯”ï¼ŒInstellaä½¿ç”¨çš„é¢„è®­ç»ƒä»¤ç‰Œæ•°é‡å¤§å¤§å‡å°‘ã€‚</li>
<li>Instellaåœ¨å®Œå…¨å¼€æºæ¨¡å‹ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶ä¸ç›¸å½“è§„æ¨¡çš„å¼€æºæƒé‡æ¨¡å‹å…·æœ‰ç«äº‰åŠ›ã€‚</li>
<li>å‘å¸ƒäº†ä¸¤ä¸ªä¸“ä¸šç‰ˆæœ¬çš„Instellaï¼ŒåŒ…æ‹¬å¤„ç†é•¿æ–‡æœ¬çš„Instella-Longå’Œä¸“æ³¨äºæ•°å­¦ä»»åŠ¡çš„Instella-Mathã€‚</li>
<li>Instella-Longå¯ä»¥å¤„ç†ä¸Šä¸‹æ–‡é•¿åº¦è¾¾128Kçš„ä»¤ç‰Œï¼Œè€ŒInstella-Mathé€šè¿‡ç›‘ç£å¾®è°ƒå¼ºåŒ–å­¦ä¹ åœ¨æ•°å­¦ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.10628">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5c5f716ad86a174ac0c18037e00a368b" align="middle">
<img src="https://picx.zhimg.com/v2-a8fab08c1708160e25109b396472203a" align="middle">
<img src="https://picx.zhimg.com/v2-0d12dbb0c23252412ca1b14a472fb1cb" align="middle">
<img src="https://picx.zhimg.com/v2-52234f6b2d92afdc850b28d7dd191cd0" align="middle">
<img src="https://picx.zhimg.com/v2-897d803ec990f59bb3cc02d9c3e21d42" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="OmniVGGT-Omni-Modality-Driven-Visual-Geometry-Grounded-Transformer"><a href="#OmniVGGT-Omni-Modality-Driven-Visual-Geometry-Grounded-Transformer" class="headerlink" title="OmniVGGT: Omni-Modality Driven Visual Geometry Grounded Transformer"></a>OmniVGGT: Omni-Modality Driven Visual Geometry Grounded Transformer</h2><p><strong>Authors:Haosong Peng, Hao Li, Yalun Dai, Yushi Lan, Yihang Luo, Tianyu Qi, Zhengshen Zhang, Yufeng Zhan, Junfei Zhang, Wenchao Xu, Ziwei Liu</strong></p>
<p>General 3D foundation models have started to lead the trend of unifying diverse vision tasks, yet most assume RGB-only inputs and ignore readily available geometric cues (e.g., camera intrinsics, poses, and depth maps). To address this issue, we introduce OmniVGGT, a novel framework that can effectively benefit from an arbitrary number of auxiliary geometric modalities during both training and inference. In our framework, a GeoAdapter is proposed to encode depth and camera intrinsics&#x2F;extrinsics into a spatial foundation model. It employs zero-initialized convolutions to progressively inject geometric information without disrupting the foundation modelâ€™s representation space. This design ensures stable optimization with negligible overhead, maintaining inference speed comparable to VGGT even with multiple additional inputs. Additionally, a stochastic multimodal fusion regimen is proposed, which randomly samples modality subsets per instance during training. This enables an arbitrary number of modality inputs during testing and promotes learning robust spatial representations instead of overfitting to auxiliary cues. Comprehensive experiments on monocular&#x2F;multi-view depth estimation, multi-view stereo, and camera pose estimation demonstrate that OmniVGGT outperforms prior methods with auxiliary inputs and achieves state-of-the-art results even with RGB-only input. To further highlight its practical utility, we integrated OmniVGGT into vision-language-action (VLA) models. The enhanced VLA model by OmniVGGT not only outperforms the vanilla point-cloud-based baseline on mainstream benchmarks, but also effectively leverages accessible auxiliary inputs to achieve consistent gains on robotic tasks.</p>
<blockquote>
<p>é€šç”¨3DåŸºç¡€æ¨¡å‹å·²ç»å¼€å§‹å¼•é¢†ç»Ÿä¸€å„ç§è§†è§‰ä»»åŠ¡çš„è¶‹åŠ¿ï¼Œç„¶è€Œï¼Œå¤§å¤šæ•°æ¨¡å‹ä»…é‡‡ç”¨RGBè¾“å…¥ï¼Œå¿½ç•¥äº†å¯è½»æ¾è·å¾—çš„å‡ ä½•çº¿ç´¢ï¼ˆä¾‹å¦‚ï¼Œç›¸æœºå†…å‚ã€å§¿æ€å’Œæ·±åº¦å›¾ï¼‰ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†OmniVGGTï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æ¡†æ¶ï¼Œå¯ä»¥åœ¨è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ä¸­æœ‰æ•ˆåˆ©ç”¨ä»»æ„æ•°é‡çš„è¾…åŠ©å‡ ä½•æ¨¡æ€ã€‚åœ¨æˆ‘ä»¬çš„æ¡†æ¶ä¸­ï¼Œæå‡ºäº†GeoAdapterï¼Œç”¨äºå°†æ·±åº¦å’Œç›¸æœºå†…å‚&#x2F;å¤–å‚ç¼–ç åˆ°ç©ºé—´åŸºç¡€æ¨¡å‹ä¸­ã€‚å®ƒé‡‡ç”¨é›¶åˆå§‹åŒ–å·ç§¯æ¥é€æ­¥æ³¨å…¥å‡ ä½•ä¿¡æ¯ï¼Œè€Œä¸ä¼šç ´ååŸºç¡€æ¨¡å‹çš„è¡¨ç¤ºç©ºé—´ã€‚è¿™ç§è®¾è®¡ç¡®ä¿äº†ç¨³å®šçš„ä¼˜åŒ–ï¼Œä¸”å¼€é”€å¾®ä¹å…¶å¾®ï¼Œå³ä½¿ä½¿ç”¨å¤šä¸ªé™„åŠ è¾“å…¥ï¼Œå…¶æ¨ç†é€Ÿåº¦ä¹Ÿä¸VGGTç›¸å½“ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†éšæœºå¤šæ¨¡æ€èåˆæ–¹æ¡ˆï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­éšæœºå¯¹å®ä¾‹è¿›è¡Œæ¨¡æ€å­é›†é‡‡æ ·ã€‚è¿™å¯ä»¥åœ¨æµ‹è¯•æ—¶æ¥å—ä»»æ„æ•°é‡çš„æ¨¡æ€è¾“å…¥ï¼Œå¹¶ä¿ƒè¿›å­¦ä¹ ç¨³å¥çš„ç©ºé—´è¡¨ç¤ºï¼Œè€Œä¸æ˜¯è¿‡åº¦æ‹Ÿåˆè¾…åŠ©çº¿ç´¢ã€‚åœ¨å•ç›®&#x2F;å¤šç›®æ·±åº¦ä¼°è®¡ã€å¤šç›®ç«‹ä½“è§†è§‰å’Œç›¸æœºå§¿æ€ä¼°è®¡æ–¹é¢çš„ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒOmniVGGTåœ¨å…·æœ‰è¾…åŠ©è¾“å…¥çš„æ–¹æ³•ä¸­è¡¨ç°ä¼˜è¶Šï¼Œå³ä½¿åœ¨ä»…æœ‰RGBè¾“å…¥çš„æƒ…å†µä¸‹ä¹Ÿè¾¾åˆ°äº†æœ€æ–°ç»“æœã€‚ä¸ºäº†è¿›ä¸€æ­¥çªå‡ºå…¶å®ç”¨æ€§ï¼Œæˆ‘ä»¬å°†OmniVGGTé›†æˆåˆ°è§†è§‰è¯­è¨€åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹ä¸­ã€‚OmniVGGTå¢å¼ºçš„VLAæ¨¡å‹ä¸ä»…åœ¨ä¸»æµåŸºå‡†æµ‹è¯•ä¸Šè¶…è¶Šäº†åŸºäºç‚¹äº‘çš„åŸºå‡†æ¨¡å‹ï¼Œè€Œä¸”æœ‰æ•ˆåœ°åˆ©ç”¨äº†å¯è®¿é—®çš„è¾…åŠ©è¾“å…¥ï¼Œåœ¨æœºå™¨äººä»»åŠ¡ä¸Šå®ç°äº†æŒç»­çš„æ”¶ç›Šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.10560v2">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://livioni.github.io/OmniVGGT-official/">https://livioni.github.io/OmniVGGT-official/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†OmniVGGTæ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½æœ‰æ•ˆåˆ©ç”¨å¤šç§è¾…åŠ©å‡ ä½•æ¨¡æ€ä¿¡æ¯ï¼ŒåŒ…æ‹¬æ·±åº¦ã€ç›¸æœºå†…å‚å’Œå¤–å‚ç­‰ï¼Œå¯¹é€šç”¨ä¸‰ç»´åŸºç¡€æ¨¡å‹è¿›è¡Œå¢å¼ºã€‚é€šè¿‡GeoAdapteræ¨¡å—ï¼Œè¯¥æ¡†æ¶èƒ½é€æ­¥æ³¨å…¥å‡ ä½•ä¿¡æ¯è€Œä¸å¹²æ‰°åŸºç¡€æ¨¡å‹çš„è¡¨ç¤ºç©ºé—´ã€‚æ­¤å¤–ï¼Œæå‡ºéšæœºå¤šæ¨¡æ€èåˆæ–¹æ¡ˆï¼Œåœ¨è®­ç»ƒæ—¶éšæœºé€‰æ‹©æ¨¡æ€å­é›†ï¼Œä½¿å¾—æ¨¡å‹åœ¨æµ‹è¯•æ—¶èƒ½æ¥å—ä»»æ„æ•°é‡çš„æ¨¡æ€è¾“å…¥ï¼Œå¹¶å­¦ä¹ ç¨³å¥çš„ç©ºé—´è¡¨ç¤ºã€‚å®éªŒè¡¨æ˜ï¼ŒOmniVGGTåœ¨å•ç›®&#x2F;å¤šç›®æ·±åº¦ä¼°è®¡ã€å¤šç›®ç«‹ä½“è§†è§‰å’Œç›¸æœºå§¿æ€ä¼°è®¡ç­‰å¤šé¡¹ä»»åŠ¡ä¸Šè¶…è¶Šäº†ä½¿ç”¨è¾…åŠ©è¾“å…¥çš„å…ˆå‰æ–¹æ³•ï¼Œå¹¶åœ¨ä»…ä½¿ç”¨RGBè¾“å…¥çš„æƒ…å†µä¸‹è¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚åŒæ—¶ï¼Œå°†OmniVGGTé›†æˆåˆ°è§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡å‹ä¸­ï¼Œä¸ä»…æé«˜äº†åŸºäºç‚¹äº‘çš„åŸºå‡†æ€§èƒ½ï¼Œè€Œä¸”åœ¨æœºå™¨äººä»»åŠ¡ä¸Šé€šè¿‡åˆ©ç”¨è¾…åŠ©è¾“å…¥å®ç°äº†æŒç»­çš„æ€§èƒ½æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>OmniVGGTæ¡†æ¶èƒ½åˆ©ç”¨å¤šç§è¾…åŠ©å‡ ä½•æ¨¡æ€ä¿¡æ¯ï¼ˆå¦‚æ·±åº¦ã€ç›¸æœºå†…å‚å’Œå¤–å‚ç­‰ï¼‰ï¼Œå¢å¼ºé€šç”¨ä¸‰ç»´åŸºç¡€æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>é€šè¿‡GeoAdapteræ¨¡å—ï¼ŒOmniVGGTèƒ½é€æ­¥æ³¨å…¥å‡ ä½•ä¿¡æ¯ï¼Œä¸å½±å“åŸºç¡€æ¨¡å‹çš„è¡¨ç¤ºç©ºé—´ã€‚</li>
<li>éšæœºå¤šæ¨¡æ€èåˆæ–¹æ¡ˆä½¿å¾—æ¨¡å‹åœ¨æµ‹è¯•æ—¶èƒ½æ¥å—ä»»æ„æ•°é‡çš„æ¨¡æ€è¾“å…¥ï¼Œå¹¶å­¦ä¹ ç¨³å¥çš„ç©ºé—´è¡¨ç¤ºã€‚</li>
<li>OmniVGGTåœ¨å¤šé¡¹è§†è§‰ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜è¶Šï¼ŒåŒ…æ‹¬å•ç›®&#x2F;å¤šç›®æ·±åº¦ä¼°è®¡ã€å¤šç›®ç«‹ä½“è§†è§‰å’Œç›¸æœºå§¿æ€ä¼°è®¡ã€‚</li>
<li>OmniVGGTèƒ½æé«˜åŸºäºç‚¹äº‘çš„åŸºå‡†æ€§èƒ½ï¼Œå¹¶åœ¨æœºå™¨äººä»»åŠ¡ä¸Šå®ç°æŒç»­çš„æ€§èƒ½æå‡ã€‚</li>
<li>OmniVGGTæ¡†æ¶å…·æœ‰å®é™…åº”ç”¨çš„æ½œåŠ›ï¼Œå¯å¹¿æ³›åº”ç”¨äºè®¡ç®—æœºè§†è§‰å’Œæœºå™¨äººæŠ€æœ¯ç­‰é¢†åŸŸã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.10560">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-86a8675a2a1925fdcab531ed68aad0db" align="middle">
<img src="https://picx.zhimg.com/v2-e4a16a2d71baa35edb8791bf11bcf3e9" align="middle">
<img src="https://picx.zhimg.com/v2-7d233bd1c97fac91490352523cbef69e" align="middle">
<img src="https://picx.zhimg.com/v2-d34670dbda153a5501cf4fa154adedff" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="STAGE-A-Symbolic-Tensor-grAph-GEnerator-for-distributed-AI-system-co-design"><a href="#STAGE-A-Symbolic-Tensor-grAph-GEnerator-for-distributed-AI-system-co-design" class="headerlink" title="STAGE: A Symbolic Tensor grAph GEnerator for distributed AI system co-design"></a>STAGE: A Symbolic Tensor grAph GEnerator for distributed AI system co-design</h2><p><strong>Authors:Changhai Man, Joongun Park, Hanjiang Wu, Huan Xu, Srinivas Sridharan, Tushar Krishna</strong></p>
<p>Optimizing the performance of large language models (LLMs) on large-scale AI training and inference systems requires a scalable and expressive mechanism to model distributed workload execution. Such modeling is essential for pre-deployment system-level optimizations (e.g., parallelization strategies) and design-space explorations. While recent efforts have proposed collecting execution traces from real systems, access to large-scale infrastructure remains limited to major cloud providers. Moreover, traces obtained from existing platforms cannot be easily adapted to study future larger-scale system configurations. We introduce Symbolic Tensor grAph GEnerator(STAGE), a framework that synthesizes high-fidelity execution traces to accurately model LLM workloads. STAGE supports a comprehensive set of parallelization strategies, allowing users to systematically explore a wide spectrum of LLM architectures and system configurations. STAGE demonstrates its scalability by synthesizing high-fidelity LLM traces spanning over 32K GPUs, while preserving tensor-level accuracy in compute, memory, and communication. STAGE is publicly available to facilitate further research in distributed machine learning systems: <a target="_blank" rel="noopener" href="https://github.com/astra-sim/symbolic">https://github.com/astra-sim/symbolic</a> tensor graph</p>
<blockquote>
<p>ä¼˜åŒ–å¤§è§„æ¨¡äººå·¥æ™ºèƒ½è®­ç»ƒå’Œæ¨ç†ç³»ç»Ÿä¸Šçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ€§èƒ½ï¼Œéœ€è¦ä¸€ä¸ªå¯æ‰©å±•ä¸”è¡¨ç°åŠ›ä¸°å¯Œçš„æœºåˆ¶æ¥æ¨¡æ‹Ÿåˆ†å¸ƒå¼å·¥ä½œé‡æ‰§è¡Œã€‚è¿™æ ·çš„å»ºæ¨¡å¯¹äºé¢„éƒ¨ç½²çš„ç³»ç»Ÿçº§ä¼˜åŒ–ï¼ˆä¾‹å¦‚å¹¶è¡ŒåŒ–ç­–ç•¥ï¼‰å’Œè®¾è®¡ç©ºé—´æ¢ç´¢è‡³å…³é‡è¦ã€‚è™½ç„¶æœ€è¿‘çš„åŠªåŠ›å·²ç»æå‡ºä»çœŸå®ç³»ç»Ÿæ”¶é›†æ‰§è¡Œè½¨è¿¹ï¼Œä½†å¤§è§„æ¨¡åŸºç¡€è®¾æ–½çš„è®¿é—®ä»ç„¶ä»…é™äºä¸»è¦äº‘æä¾›å•†ã€‚æ­¤å¤–ï¼Œä»ç°æœ‰å¹³å°è·å¾—çš„è½¨è¿¹æ— æ³•è½»æ¾é€‚åº”ç ”ç©¶æœªæ¥æ›´å¤§è§„æ¨¡çš„ç³»ç»Ÿé…ç½®ã€‚æˆ‘ä»¬å¼•å…¥äº†Symbolic Tensor grAph GEneratorï¼ˆSTAGEï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåˆæˆé«˜ä¿çœŸæ‰§è¡Œè½¨è¿¹çš„æ¡†æ¶ï¼Œèƒ½å¤Ÿå‡†ç¡®åœ°å¯¹LLMå·¥ä½œé‡è¿›è¡Œå»ºæ¨¡ã€‚STAGEæ”¯æŒä¸€å¥—å…¨é¢çš„å¹¶è¡ŒåŒ–ç­–ç•¥ï¼Œå…è®¸ç”¨æˆ·ç³»ç»Ÿåœ°æ¢ç´¢å¹¿æ³›çš„LLMæ¶æ„å’Œç³»ç»Ÿé…ç½®ã€‚STAGEé€šè¿‡åˆæˆè¦†ç›–32K GPUçš„é«˜ä¿çœŸLLMè½¨è¿¹å±•ç¤ºäº†å…¶å¯æ‰©å±•æ€§ï¼ŒåŒæ—¶åœ¨è®¡ç®—ã€å†…å­˜å’Œé€šä¿¡æ–¹é¢ä¿æŒå¼ é‡çº§åˆ«çš„å‡†ç¡®æ€§ã€‚ä¸ºäº†æ–¹ä¾¿åœ¨åˆ†å¸ƒå¼æœºå™¨å­¦ä¹ ç³»ç»Ÿé¢†åŸŸè¿›è¡Œè¿›ä¸€æ­¥ç ”ç©¶ï¼Œæˆ‘ä»¬å…¬å¼€æä¾›äº†STAGEï¼š<a target="_blank" rel="noopener" href="https://github.com/astra-sim/symbolic">https://github.com/astra-sim/symbolic</a> tensor graphï¼ˆè¯·æ›¿æ¢ä¸ºæ­£ç¡®çš„é“¾æ¥åœ°å€ï¼‰</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.10480v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºç¬¦å·å¼ é‡å›¾ç”Ÿæˆå™¨ï¼ˆSTAGEï¼‰æ¡†æ¶ï¼Œå¯åˆæˆé«˜ä¿çœŸæ‰§è¡Œè½¨è¿¹ï¼Œå‡†ç¡®æ¨¡æ‹Ÿå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å·¥ä½œè´Ÿè½½ï¼Œæ”¯æŒå¹¿æ³›çš„å¹¶è¡ŒåŒ–ç­–ç•¥ï¼Œèƒ½ç³»ç»Ÿåœ°æ¢ç´¢å„ç§LLMæ¶æ„å’Œç³»ç»Ÿé…ç½®ï¼Œå¹¶åœ¨è¶…è¿‡32K GPUä¸Šå±•ç¤ºå…¶å¯æ‰©å±•æ€§ï¼ŒåŒæ—¶ä¿æŒå¼ é‡çº§åˆ«çš„è®¡ç®—ã€å†…å­˜å’Œé€šä¿¡å‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>STAGEæ¡†æ¶ç”¨äºåˆæˆé«˜ä¿çœŸæ‰§è¡Œè½¨è¿¹ï¼Œä»¥æ¨¡æ‹ŸLLMçš„å·¥ä½œè´Ÿè½½ã€‚</li>
<li>è¯¥æ¡†æ¶æ”¯æŒå¹¿æ³›çš„å¹¶è¡ŒåŒ–ç­–ç•¥ï¼Œä¾¿äºç³»ç»Ÿåœ°æ¢ç´¢LLMæ¶æ„å’Œç³»ç»Ÿé…ç½®ã€‚</li>
<li>STAGEå…·æœ‰å¯æ‰©å±•æ€§ï¼Œèƒ½åˆæˆè¦†ç›–è¶…è¿‡32K GPUçš„é«˜ä¿çœŸLLMè½¨è¿¹ã€‚</li>
<li>STAGEåœ¨æ¨¡æ‹Ÿè¿‡ç¨‹ä¸­ä¿æŒå¼ é‡çº§åˆ«çš„è®¡ç®—ã€å†…å­˜å’Œé€šä¿¡å‡†ç¡®æ€§ã€‚</li>
<li>è¯¥æ¡†æ¶å¯¹äºé¢„éƒ¨ç½²ç³»ç»Ÿçº§ä¼˜åŒ–ï¼ˆå¦‚å¹¶è¡ŒåŒ–ç­–ç•¥ï¼‰å…·æœ‰é‡è¦æ„ä¹‰ã€‚</li>
<li>STAGEæ¡†æ¶æœ‰åŠ©äºç ”ç©¶æœªæ¥æ›´å¤§è§„æ¨¡çš„ç³»ç»Ÿé…ç½®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.10480">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fc7afd5840a02166c0b70118d7dbe462" align="middle">
<img src="https://picx.zhimg.com/v2-27d35772eecf1156336c1c45832f7baf" align="middle">
<img src="https://picx.zhimg.com/v2-f1fcd75640e5bd6fedc6700d0ef24730" align="middle">
<img src="https://picx.zhimg.com/v2-c8521506d3270f7e4b3656224026a24e" align="middle">
<img src="https://picx.zhimg.com/v2-36681ec4cb4f516d27559dfd5843df3d" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Speech-Audio-Compositional-Attacks-on-Multimodal-LLMs-and-Their-Mitigation-with-SALMONN-Guard"><a href="#Speech-Audio-Compositional-Attacks-on-Multimodal-LLMs-and-Their-Mitigation-with-SALMONN-Guard" class="headerlink" title="Speech-Audio Compositional Attacks on Multimodal LLMs and Their Mitigation with SALMONN-Guard"></a>Speech-Audio Compositional Attacks on Multimodal LLMs and Their Mitigation with SALMONN-Guard</h2><p><strong>Authors:Yudong Yang, Xuezhen Zhang, Zhifeng Han, Siyin Wang, Jimin Zhuang, Zengrui Jin, Jing Shao, Guangzhi Sun, Chao Zhang</strong></p>
<p>Recent progress in large language models (LLMs) has enabled understanding of both speech and non-speech audio, but exposing new safety risks emerging from complex audio inputs that are inadequately handled by current safeguards. We introduce SACRED-Bench (Speech-Audio Composition for RED-teaming) to evaluate the robustness of LLMs under complex audio-based attacks. Unlike existing perturbation-based methods that rely on noise optimization or white-box access, SACRED-Bench exploits speech-audio composition mechanisms. SACRED-Bench adopts three mechanisms: (a) speech overlap and multi-speaker dialogue, which embeds harmful prompts beneath or alongside benign speech; (b) speech-audio mixture, which imply unsafe intent via non-speech audio alongside benign speech or audio; and (c) diverse spoken instruction formats (open-ended QA, yes&#x2F;no) that evade text-only filters. Experiments show that, even Gemini 2.5 Pro, the state-of-the-art proprietary LLM, still exhibits 66% attack success rate in SACRED-Bench test set, exposing vulnerabilities under cross-modal, speech-audio composition attacks. To bridge this gap, we propose SALMONN-Guard, a safeguard LLM that jointly inspects speech, audio, and text for safety judgments, reducing attack success down to 20%. Our results highlight the need for audio-aware defenses for the safety of multimodal LLMs. The benchmark and SALMONN-Guard checkpoints can be found at <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/tsinghua-ee/SACRED-Bench">https://huggingface.co/datasets/tsinghua-ee/SACRED-Bench</a>. Warning: this paper includes examples that may be offensive or harmful.</p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ€æ–°è¿›å±•ä½¿å¾—ç†è§£å’Œå¤„ç†è¯­éŸ³å’Œéè¯­éŸ³éŸ³é¢‘æˆä¸ºå¯èƒ½ï¼Œä½†åŒæ—¶ä¹Ÿæš´éœ²å‡ºå½“å‰ä¿éšœæªæ–½å¤„ç†ä¸å½“çš„å¤æ‚éŸ³é¢‘è¾“å…¥æ‰€å¸¦æ¥çš„æ–°å®‰å…¨å¨èƒã€‚æˆ‘ä»¬æ¨å‡ºSACRED-Benchï¼ˆç”¨äºçº¢é˜Ÿåä½œçš„è¯­éŸ³éŸ³é¢‘ç»„åˆè¯„ä¼°ï¼‰æ¥è¯„ä¼°å¤æ‚éŸ³é¢‘æ”»å‡»ä¸‹LLMçš„ç¨³å¥æ€§ã€‚ä¸ç°æœ‰çš„åŸºäºæ‰°åŠ¨çš„æ–¹æ³•ä¸åŒï¼Œè¿™äº›æ–¹æ³•ä¾èµ–äºå™ªå£°ä¼˜åŒ–æˆ–ç™½ç›’è®¿é—®ï¼ŒSACRED-Benchåˆ©ç”¨è¯­éŸ³éŸ³é¢‘ç»„åˆæœºåˆ¶ã€‚SACRED-Benché‡‡ç”¨ä¸‰ç§æœºåˆ¶ï¼šï¼ˆaï¼‰è¯­éŸ³é‡å å’Œå¤šäººå¯¹è¯ï¼Œåœ¨è‰¯æ€§è¯­éŸ³å‰åæˆ–å†…éƒ¨åµŒå…¥æœ‰å®³æç¤ºï¼›ï¼ˆbï¼‰è¯­éŸ³éŸ³é¢‘æ··åˆï¼Œé€šè¿‡è‰¯æ€§è¯­éŸ³æˆ–éŸ³é¢‘æ—è¾¹çš„éè¯­éŸ³éŸ³é¢‘æš—ç¤ºä¸å®‰å…¨æ„å›¾ï¼›ï¼ˆcï¼‰å¤šæ ·çš„å£è¯­æŒ‡ä»¤æ ¼å¼ï¼ˆå¼€æ”¾å¼é—®ç­”ã€æ˜¯éé—®ï¼‰å¯èº²é¿ä»…æ–‡æœ¬è¿‡æ»¤å™¨ã€‚å®éªŒè¡¨æ˜ï¼Œå³ä½¿æ˜¯ç›®å‰æœ€å…ˆè¿›çš„ä¸“æœ‰LLMâ€”â€”Gemini 2.5 Proï¼Œåœ¨SACRED-Benchæµ‹è¯•é›†ä¸­çš„æ”»å‡»æˆåŠŸç‡ä»é«˜è¾¾66%ï¼Œæ˜¾ç¤ºå‡ºåœ¨è·¨æ¨¡æ€è¯­éŸ³éŸ³é¢‘ç»„åˆæ”»å‡»ä¸‹çš„æ¼æ´ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†SALMONN-Guardä¿éšœLLMï¼Œå®ƒè”åˆæ£€æŸ¥è¯­éŸ³ã€éŸ³é¢‘å’Œæ–‡æœ¬ä»¥è¿›è¡Œå®‰å…¨åˆ¤æ–­ï¼Œå°†æ”»å‡»æˆåŠŸç‡é™ä½åˆ°20%ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœå¼ºè°ƒäº†ä¸ºæ¨¡æ€LLMçš„å®‰å…¨æ„è¯†éŸ³é¢‘é˜²å¾¡çš„å¿…è¦æ€§ã€‚è¯¥åŸºå‡†æµ‹è¯•å’ŒSALMONN-Guardæ£€æŸ¥ç‚¹å¯åœ¨<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/tsinghua-ee/SACRED-Bench">https://huggingface.co/datasets/tsinghua-ee/SACRED-Bench</a> æ‰¾åˆ°ã€‚è­¦å‘Šï¼šæœ¬è®ºæ–‡åŒ…å«å¯èƒ½å…·æœ‰å†’çŠ¯æ€§æˆ–æœ‰å®³æ€§çš„ç¤ºä¾‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.10222v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ€æ–°è¿›å±•ä½¿å…¶èƒ½å¤Ÿç†è§£è¯­éŸ³å’Œéè¯­éŸ³éŸ³é¢‘ï¼Œä½†åŒæ—¶ä¹Ÿæš´éœ²å‡ºç”±å¤æ‚éŸ³é¢‘è¾“å…¥å¸¦æ¥çš„æ–°å®‰å…¨å¨èƒã€‚ä¸ºè¯„ä¼°LLMåœ¨å¤æ‚éŸ³é¢‘æ”»å‡»ä¸‹çš„ç¨³å¥æ€§ï¼Œæˆ‘ä»¬æ¨å‡ºäº†SACRED-Benchï¼ˆç”¨äºçº¢é˜Ÿæ¼”ç»ƒçš„è¯­éŸ³-éŸ³é¢‘ç»„åˆï¼‰ã€‚SACRED-Benchä¸åŒäºç°æœ‰çš„åŸºäºå™ªå£°ä¼˜åŒ–çš„ç™½ç›’è®¿é—®æ–¹æ³•ï¼Œè€Œæ˜¯åˆ©ç”¨è¯­éŸ³-éŸ³é¢‘ç»„åˆæœºåˆ¶ã€‚SACRED-Benché‡‡ç”¨ä¸‰ç§æœºåˆ¶ï¼šï¼ˆaï¼‰è¯­éŸ³é‡å å’Œå¤šäººå¯¹è¯ï¼Œåœ¨æ— å®³è¯­éŸ³ä¸‹åµŒå…¥æœ‰å®³æç¤ºï¼›ï¼ˆbï¼‰è¯­éŸ³-éŸ³é¢‘æ··åˆï¼Œé€šè¿‡æ— å®³è¯­éŸ³æˆ–éŸ³é¢‘æ—è¾¹çš„éè¯­éŸ³éŸ³é¢‘æš—ç¤ºä¸å®‰å…¨æ„å›¾ï¼›ï¼ˆcï¼‰å¤šæ ·åŒ–çš„å£å¤´æŒ‡ä»¤æ ¼å¼ï¼ˆå¼€æ”¾å¼é—®ç­”ã€æ˜¯éé¢˜ç­‰ï¼‰ï¼Œä»¥é¿å…ä»…æ–‡æœ¬è¿‡æ»¤å™¨ã€‚å®éªŒè¡¨æ˜ï¼Œå³ä½¿åœ¨SACRED-Benchæµ‹è¯•é›†ä¸­ï¼Œæœ€å…ˆè¿›çš„ä¸“æœ‰LLMâ€”â€”Gemini 2.5 Proä»æœ‰66%çš„æ”»å‡»æˆåŠŸç‡ï¼Œæ˜¾ç¤ºå‡ºè·¨æ¨¡æ€å’Œè¯­éŸ³-éŸ³é¢‘ç»„åˆæ”»å‡»ä¸‹çš„æ¼æ´ã€‚ä¸ºè§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†SALMONN-Guardå®‰å…¨å«å£«LLMï¼Œå…¶è”åˆæ£€æŸ¥è¯­éŸ³ã€éŸ³é¢‘å’Œæ–‡æœ¬è¿›è¡Œå®‰å…¨åˆ¤æ–­ï¼Œå°†æ”»å‡»æˆåŠŸç‡é™è‡³ä»…ä¸åˆ°ç™¾åˆ†ä¹‹äºŒåã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœçªæ˜¾äº†å¯¹å¤šåª’ä½“LLMå®æ–½éŸ³é¢‘æ„ŸçŸ¥é˜²å¾¡çš„å¿…è¦æ€§ã€‚è¯¥åŸºå‡†æµ‹è¯•å’ŒSALMONN-Guardæ£€æŸ¥ç‚¹å¯åœ¨<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/tsinghua-ee/SACRED-Bench">https://huggingface.co/datasets/tsinghua-ee/SACRED-Bench</a> æ‰¾åˆ°ã€‚è­¦å‘Šï¼šæœ¬æ–‡åŒ…å«å¯èƒ½å…·æœ‰å†’çŠ¯æ€§æˆ–æœ‰å®³çš„ç¤ºä¾‹ã€‚</p>
<p><strong>è¦ç‚¹å½’çº³</strong></p>
<ol>
<li>LLMèƒ½å¤Ÿç†è§£è¯­éŸ³å’Œéè¯­éŸ³éŸ³é¢‘ï¼Œä½†é¢ä¸´æ¥è‡ªå¤æ‚éŸ³é¢‘è¾“å…¥çš„æ–°å®‰å…¨å¨èƒã€‚</li>
<li>SACRED-Benchç”¨äºè¯„ä¼°LLMåœ¨å¤æ‚éŸ³é¢‘æ”»å‡»ä¸‹çš„ç¨³å¥æ€§ï¼Œé‡‡ç”¨ä¸‰ç§æ–°æœºåˆ¶æ¨¡æ‹Ÿæœ‰å®³è¾“å…¥ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºæœ€å…ˆè¿›LLMä»å­˜åœ¨æ¼æ´ï¼Œå®¹æ˜“å—åˆ°ç‰¹å®šæ”»å‡»å½¢å¼çš„å½±å“ã€‚</li>
<li>SALMONN-Guardè¢«æå‡ºä½œä¸ºLLMçš„å®‰å…¨å«å£«ï¼Œè”åˆæ£€æŸ¥è¯­éŸ³ã€éŸ³é¢‘å’Œæ–‡æœ¬è¿›è¡Œå®‰å…¨åˆ¤æ–­ï¼Œèƒ½æœ‰æ•ˆé™ä½æ”»å‡»æˆåŠŸç‡ã€‚</li>
<li>éœ€è¦é’ˆå¯¹å¤šåª’ä½“LLMå®æ–½éŸ³é¢‘æ„ŸçŸ¥é˜²å¾¡ã€‚</li>
<li>SACRED-Benchæµ‹è¯•åŸºå‡†å’ŒSALMONN-Guardæ£€æŸ¥ç‚¹å¯åœ¨ç‰¹å®šç½‘ç«™æ‰¾åˆ°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.10222">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-85c3cb9bfa0bacbf18f2c109983b911e" align="middle">
<img src="https://picx.zhimg.com/v2-8cd73019ca6274d9ef76d7b6b00c1736" align="middle">
<img src="https://picx.zhimg.com/v2-5333b659bd5c801d4ab81538bb3267e6" align="middle">
<img src="https://picx.zhimg.com/v2-0d41ac41ddaaee8884ea796c5f4ded59" align="middle">
<img src="https://picx.zhimg.com/v2-ee19ba35695541c36c36ba8c538f6a5a" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Large-Language-Model-assisted-Autonomous-Vehicle-Recovery-from-Immobilization"><a href="#Large-Language-Model-assisted-Autonomous-Vehicle-Recovery-from-Immobilization" class="headerlink" title="Large Language Model-assisted Autonomous Vehicle Recovery from Immobilization"></a>Large Language Model-assisted Autonomous Vehicle Recovery from Immobilization</h2><p><strong>Authors:Zhipeng Bao, Qianwen Li</strong></p>
<p>Despite significant advancements in recent decades, autonomous vehicles (AVs) continue to face challenges in navigating certain traffic scenarios where human drivers excel. In such situations, AVs often become immobilized, disrupting overall traffic flow. Current recovery solutions, such as remote intervention (which is costly and inefficient) and manual takeover (which excludes non-drivers and limits AV accessibility), are inadequate. This paper introduces StuckSolver, a novel Large Language Model (LLM) driven recovery framework that enables AVs to resolve immobilization scenarios through self-reasoning and&#x2F;or passenger-guided decision-making. StuckSolver is designed as a plug-in add-on module that operates on top of the AVâ€™s existing perception-planning-control stack, requiring no modification to its internal architecture. Instead, it interfaces with standard sensor data streams to detect immobilization states, interpret environmental context, and generate high-level recovery commands that can be executed by the AVâ€™s native planner. We evaluate StuckSolver on the Bench2Drive benchmark and in custom-designed uncertainty scenarios. Results show that StuckSolver achieves near-state-of-the-art performance through autonomous self-reasoning alone and exhibits further improvements when passenger guidance is incorporated.</p>
<blockquote>
<p>å°½ç®¡è¿‘å‡ åå¹´æ¥å–å¾—äº†é‡å¤§è¿›å±•ï¼Œè‡ªä¸»è½¦è¾†ï¼ˆAVsï¼‰åœ¨å¯¼èˆªæŸäº›äº¤é€šåœºæ™¯æ—¶ä»ç„¶é¢ä¸´æŒ‘æˆ˜ï¼Œè¿™äº›åœºæ™¯æ˜¯äººç±»é©¾é©¶å‘˜æ“…é•¿çš„é¢†åŸŸã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè‡ªä¸»è½¦è¾†ç»å¸¸ä¼šå¤±å»æœºåŠ¨èƒ½åŠ›ï¼Œä»è€Œç ´åæ•´ä½“çš„äº¤é€šæµé‡ã€‚ç›®å‰çš„æ¢å¤è§£å†³æ–¹æ¡ˆï¼Œå¦‚è¿œç¨‹å¹²é¢„ï¼ˆæˆæœ¬é«˜æ˜‚ä¸”æ•ˆç‡ä½ä¸‹ï¼‰å’Œæ‰‹åŠ¨æ¥ç®¡ï¼ˆæ’é™¤éé©¾é©¶å‘˜å¹¶é™åˆ¶AVçš„å¯ç”¨æ€§ï¼‰ï¼Œéƒ½ä¸è¶³ä»¥åº”å¯¹æŒ‘æˆ˜ã€‚æœ¬æ–‡ä»‹ç»äº†StuckSolverï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é©±åŠ¨çš„æ¢å¤æ¡†æ¶ï¼Œå®ƒèƒ½å¤Ÿä½¿è‡ªä¸»è½¦è¾†é€šè¿‡è‡ªæˆ‘æ¨ç†å’Œï¼ˆæˆ–ï¼‰ä¹˜å®¢æŒ‡å¯¼çš„å†³ç­–åˆ¶å®šæ¥è§£å†³å¤±å»æœºåŠ¨èƒ½åŠ›çš„æƒ…å†µã€‚StuckSolverè¢«è®¾è®¡ä¸ºä¸€ä¸ªæ’ä»¶é™„åŠ æ¨¡å—ï¼Œå®ƒåœ¨è‡ªä¸»è½¦è¾†ç°æœ‰çš„æ„ŸçŸ¥-è§„åˆ’-æ§åˆ¶å †æ ˆä¸Šè¿è¡Œï¼Œæ— éœ€å¯¹å…¶å†…éƒ¨æ¶æ„è¿›è¡Œä»»ä½•ä¿®æ”¹ã€‚ç›¸åï¼Œå®ƒä¸æ ‡å‡†ä¼ æ„Ÿå™¨æ•°æ®æµæ¥å£ï¼Œä»¥æ£€æµ‹å¤±å»æœºåŠ¨èƒ½åŠ›çš„çŠ¶æ€ï¼Œè§£é‡Šç¯å¢ƒä¸Šä¸‹æ–‡ï¼Œå¹¶ç”Ÿæˆå¯ä»¥ç”±è‡ªä¸»è½¦è¾†çš„æœ¬åœ°è§„åˆ’å™¨æ‰§è¡Œçš„é«˜çº§æ¢å¤å‘½ä»¤ã€‚æˆ‘ä»¬åœ¨Bench2DriveåŸºå‡†æµ‹è¯•å’Œè‡ªå®šä¹‰è®¾è®¡çš„ä¸ç¡®å®šæ€§åœºæ™¯ä¸­å¯¹StuckSolverè¿›è¡Œäº†è¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼ŒStuckSolveré€šè¿‡è‡ªä¸»çš„è‡ªæˆ‘æ¨ç†å®ç°äº†è¿‘ä¹æœ€æ–°çš„æ€§èƒ½æ°´å¹³ï¼Œå¹¶ä¸”åœ¨èå…¥ä¹˜å®¢æŒ‡å¯¼æ—¶è¡¨ç°å‡ºè¿›ä¸€æ­¥çš„æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.26023v2">PDF</a> 7 pages</p>
<p><strong>Summary</strong></p>
<p>è¿‘å¹´æ¥å°½ç®¡è‡ªåŠ¨é©¾é©¶è½¦è¾†ï¼ˆAVï¼‰æŠ€æœ¯å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨åº”å¯¹æŸäº›äº¤é€šåœºæ™¯ä¸­ä»ç„¶é¢ä¸´æŒ‘æˆ˜ï¼Œäººç±»é©¾é©¶å‘˜åœ¨è¿™æ–¹é¢è¡¨ç°æ›´å‡ºè‰²ã€‚è‡ªä¸»æ¢å¤è§£å†³æ–¹æ¡ˆå¦‚è¿œç¨‹å¹²é¢„ï¼ˆæˆæœ¬é«˜ä¸”æ•ˆç‡ä½ï¼‰å’Œæ‰‹åŠ¨æ¥ç®¡ï¼ˆä¸é€‚ç”¨äºéé©¾é©¶å‘˜å¹¶é™åˆ¶äº†AVçš„æ™®åŠï¼‰å¹¶ä¸å®Œå–„ã€‚æœ¬è®ºæ–‡ä»‹ç»äº†StuckSolverï¼Œä¸€ä¸ªåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ–°å‹æ¢å¤æ¡†æ¶ï¼Œé€šè¿‡è‡ªæˆ‘æ¨ç†å’Œä¹˜å®¢å¼•å¯¼å†³ç­–ï¼Œå¸®åŠ©è‡ªåŠ¨é©¾é©¶è½¦è¾†è§£å†³åƒµåœåœºæ™¯ã€‚StuckSolverè¢«è®¾è®¡ä¸ºä¸€ä¸ªæ’ä»¶æ¨¡å—ï¼Œæ— éœ€ä¿®æ”¹AVçš„å†…éƒ¨æ¶æ„ï¼Œå³å¯åœ¨å…¶ç°æœ‰çš„æ„ŸçŸ¥è§„åˆ’æ§åˆ¶å †æ ˆä¸Šè¿è¡Œã€‚å®ƒé€šè¿‡æ ‡å‡†ä¼ æ„Ÿå™¨æ•°æ®æµæ£€æµ‹åƒµåœçŠ¶æ€ã€è§£é‡Šç¯å¢ƒä¸Šä¸‹æ–‡å¹¶ç”Ÿæˆå¯ä»¥ç”±AVæœ¬åœ°è§„åˆ’å™¨æ‰§è¡Œçš„é«˜çº§æ¢å¤å‘½ä»¤ã€‚æˆ‘ä»¬åœ¨Bench2DriveåŸºå‡†æµ‹è¯•å’Œè‡ªå®šä¹‰è®¾è®¡çš„ä¸ç¡®å®šæ€§åœºæ™¯ä¸­å¯¹StuckSolverè¿›è¡Œäº†è¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼ŒStuckSolveré€šè¿‡è‡ªä¸»è‡ªæˆ‘æ¨ç†å®ç°äº†è¿‘ä¹æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ï¼Œå¹¶ä¸”åœ¨åŠ å…¥ä¹˜å®¢æŒ‡å¯¼åè¿›ä¸€æ­¥æå‡äº†æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªåŠ¨é©¾é©¶è½¦è¾†åœ¨ç‰¹å®šäº¤é€šåœºæ™¯ä¸­ä»é¢ä¸´æŒ‘æˆ˜ï¼Œéœ€è¦æ›´å…ˆè¿›çš„æ¢å¤è§£å†³æ–¹æ¡ˆã€‚</li>
<li>å½“å‰æ¢å¤è§£å†³æ–¹æ¡ˆå¦‚è¿œç¨‹å¹²é¢„å’Œæ‰‹åŠ¨æ¥ç®¡å­˜åœ¨ä¸è¶³ï¼Œéœ€è¦æ–°çš„æ–¹æ³•æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>StuckSolveræ˜¯ä¸€ä¸ªåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ¢å¤æ¡†æ¶ï¼Œèƒ½å¤Ÿé€šè¿‡è‡ªæˆ‘æ¨ç†å’Œä¹˜å®¢å¼•å¯¼å†³ç­–å¸®åŠ©è‡ªåŠ¨é©¾é©¶è½¦è¾†è§£å†³åƒµåœé—®é¢˜ã€‚</li>
<li>StuckSolverä½œä¸ºä¸€ä¸ªæ’ä»¶æ¨¡å—ï¼Œå¯æ— ç¼é›†æˆåˆ°ç°æœ‰çš„è‡ªåŠ¨é©¾é©¶è½¦è¾†ç³»ç»Ÿä¸­ï¼Œæ— éœ€ä¿®æ”¹å†…éƒ¨æ¶æ„ã€‚</li>
<li>StuckSolveré€šè¿‡æ ‡å‡†ä¼ æ„Ÿå™¨æ•°æ®æµæ¥æ£€æµ‹åƒµåœçŠ¶æ€å’Œç¯å¢ƒä¸Šä¸‹æ–‡ã€‚</li>
<li>StuckSolverç”Ÿæˆé«˜çº§æ¢å¤å‘½ä»¤ï¼Œå¯ä»¥ç”±è‡ªåŠ¨é©¾é©¶è½¦è¾†çš„æœ¬åœ°è§„åˆ’å™¨æ‰§è¡Œã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.26023">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1425dd2adcd16b133a999a5eafec9b35" align="middle">
<img src="https://picx.zhimg.com/v2-ad564c575eafdfe584bee6cba8d175d7" align="middle">
<img src="https://picx.zhimg.com/v2-feb9c81f5710a041b1a21274deadefb0" align="middle">
<img src="https://picx.zhimg.com/v2-dc29563525d8f1859f275d139fa2a107" align="middle">
<img src="https://picx.zhimg.com/v2-c68a71ab8768ed11b780da6ec60516eb" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="ORIC-Benchmarking-Object-Recognition-under-Contextual-Incongruity-in-Large-Vision-Language-Models"><a href="#ORIC-Benchmarking-Object-Recognition-under-Contextual-Incongruity-in-Large-Vision-Language-Models" class="headerlink" title="ORIC: Benchmarking Object Recognition under Contextual Incongruity in Large Vision-Language Models"></a>ORIC: Benchmarking Object Recognition under Contextual Incongruity in Large Vision-Language Models</h2><p><strong>Authors:Zhaoyang Li, Zhan Ling, Yuchen Zhou, Litian Gong, Erdem BÄ±yÄ±k, Hao Su</strong></p>
<p>Large Vision-Language Models (LVLMs) excel at captioning, visual question answering, and robotics by combining vision and language, yet they often miss obvious objects or hallucinate nonexistent ones in atypical scenes. We examine these failures through the lens of uncertainty, focusing on contextual incongruity, where objects appear unexpectedly or fail to appear in expected contexts, and show that such cases increase recognition difficulty for state-of-the-art LVLMs. To study this regime, we introduce the Object Recognition in Incongruous Context (ORIC) framework, which constructs incongruous object-context pairs through two complementary strategies: (1) LLM-guided sampling to identify hard-to-recognize objects present in the image and (2) CLIP-guided sampling to mine plausible but absent ones. Applied to MSCOCO, ORIC produces ORIC-Bench and ORIC-style training data. Evaluating 18 LVLMs and 2 open-vocabulary detectors reveals substantial performance drops and bias patterns under incongruous contexts. Fine-tuning Qwen3-VL-8B-Instruct with Visual Reinforcement Fine-Tuning on 600 ORIC-style samples improves results on ORIC-Bench, AMBER, and HallusionBench. Overall, we show that contextual incongruity is a key source of uncertainty and provide tools for more reliable LVLMs. The code is available at <a target="_blank" rel="noopener" href="https://github.com/ZhaoyangLi-1/ORIC">https://github.com/ZhaoyangLi-1/ORIC</a>.</p>
<blockquote>
<p>å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰é€šè¿‡ç»“åˆè§†è§‰å’Œè¯­è¨€ï¼Œåœ¨å›¾åƒæè¿°ã€è§†è§‰é—®ç­”å’Œæœºå™¨äººæŠ€æœ¯ç­‰é¢†åŸŸè¡¨ç°å‡ºè‰²ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨å¤„ç†ä¸å¸¸è§çš„åœºæ™¯æ—¶ï¼Œå¾€å¾€ä¼šå¿½ç•¥æ˜æ˜¾çš„ç‰©ä½“æˆ–æƒ³è±¡å‡ºä¸å­˜åœ¨çš„ç‰©ä½“ã€‚æˆ‘ä»¬é€šè¿‡ä¸ç¡®å®šæ€§çš„è§†è§’æ¥ç ”ç©¶è¿™äº›å¤±è´¥çš„æƒ…å†µï¼Œé‡ç‚¹å…³æ³¨ä¸Šä¸‹æ–‡çš„ä¸ä¸€è‡´æ€§ï¼Œå³ç‰©ä½“åœ¨ä¸é¢„æœŸçš„æƒ…å¢ƒä¸­æ„å¤–å‡ºç°æˆ–åœ¨é¢„æœŸçš„æƒ…å¢ƒä¸­æœªå‡ºç°ï¼Œå¹¶è¡¨æ˜è¿™ç§æƒ…å†µå¢åŠ äº†å¯¹æœ€æ–°LVLMsçš„è¯†åˆ«éš¾åº¦ã€‚ä¸ºäº†ç ”ç©¶è¿™ä¸€é¢†åŸŸï¼Œæˆ‘ä»¬å¼•å…¥äº†â€œä¸ä¸€è‡´ä¸Šä¸‹æ–‡ä¸­çš„å¯¹è±¡è¯†åˆ«â€ï¼ˆORICï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡ä¸¤ç§äº’è¡¥ç­–ç•¥æ„å»ºä¸ä¸€è‡´çš„å¯¹è±¡-ä¸Šä¸‹æ–‡å¯¹ï¼šï¼ˆ1ï¼‰LLMå¼•å¯¼é‡‡æ ·ï¼Œè¯†åˆ«å›¾åƒä¸­éš¾ä»¥è¯†åˆ«çš„å¯¹è±¡ï¼›ï¼ˆ2ï¼‰CLIPå¼•å¯¼é‡‡æ ·ï¼ŒæŒ–æ˜å¯èƒ½ä½†ä¸å­˜åœ¨çš„å¯¹è±¡ã€‚åº”ç”¨äºMSCOCOæ—¶ï¼ŒORICç”ŸæˆORIC-Benchå’ŒORICé£æ ¼çš„è®­ç»ƒæ•°æ®ã€‚å¯¹18ä¸ªLVLMså’Œ2ä¸ªå¼€æ”¾è¯æ±‡æ£€æµ‹å™¨çš„è¯„ä¼°æ˜¾ç¤ºï¼Œåœ¨ä¸ä¸€è‡´çš„ä¸Šä¸‹æ–‡ç¯å¢ƒä¸‹ï¼Œå®ƒä»¬çš„æ€§èƒ½å¤§å¹…ä¸‹é™å¹¶å­˜åœ¨åè§æ¨¡å¼ã€‚é€šè¿‡å¯¹Qwenh3-VL-8B-Instructè¿›è¡Œè§†è§‰å¼ºåŒ–å¾®è°ƒï¼Œå¹¶åœ¨600ä¸ªORICæ ·å¼æ ·æœ¬ä¸Šè¿›è¡Œå¾®è°ƒï¼Œå…¶åœ¨ORIC-Benchã€AMBERå’ŒHallusionBenchä¸Šçš„ç»“æœæœ‰æ‰€æ”¹å–„ã€‚æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬è¯æ˜äº†ä¸Šä¸‹æ–‡çš„ä¸ä¸€è‡´æ€§æ˜¯ä¸ç¡®å®šæ€§çš„ä¸»è¦æ¥æºï¼Œå¹¶ä¸ºæ›´å¯é çš„LVLMsæä¾›äº†å·¥å…·ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/ZhaoyangLi-1/ORIC%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/ZhaoyangLi-1/ORICè®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.15695v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMï¼‰åœ¨å›¾åƒæè¿°ã€è§†è§‰é—®ç­”å’Œæœºå™¨äººæŠ€æœ¯ç­‰é¢†åŸŸè¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬åœ¨å¤„ç†éå…¸å‹åœºæ™¯æ—¶ï¼Œå¾€å¾€ä¼šå¿½ç•¥æ˜æ˜¾ç‰©ä½“æˆ–å¹»æƒ³å‡ºä¸å­˜åœ¨çš„ç‰©ä½“ã€‚æœ¬ç ”ç©¶ä»ä¸ç¡®å®šæ€§çš„è§’åº¦å®¡è§†è¿™äº›å¤±è´¥æ¡ˆä¾‹ï¼Œé‡ç‚¹å…³æ³¨ä¸Šä¸‹æ–‡çš„ä¸ä¸€è‡´æ€§ï¼Œå³ç‰©ä½“åœ¨é¢„æœŸä¹‹å¤–å‡ºç°æˆ–åœ¨é¢„æœŸè¯­å¢ƒä¸­ç¼ºå¤±ï¼Œå¹¶è¡¨æ˜è¿™ç§æƒ…å†µå¢åŠ äº†å¯¹æœ€æ–°LVLMçš„è¯†åˆ«éš¾åº¦ã€‚ä¸ºäº†ç ”ç©¶è¿™ä¸€é¢†åŸŸï¼Œæˆ‘ä»¬å¼•å…¥äº†Object Recognition in Incongruous Contextï¼ˆORICï¼‰æ¡†æ¶ï¼Œå®ƒé€šè¿‡ä¸¤ç§äº’è¡¥ç­–ç•¥æ„å»ºä¸ä¸€è‡´çš„ç‰©ä½“ä¸Šä¸‹æ–‡å¯¹ï¼šï¼ˆ1ï¼‰LLMå¼•å¯¼é‡‡æ ·è¯†åˆ«å›¾åƒä¸­éš¾ä»¥è¯†åˆ«çš„ç‰©ä½“ï¼›ï¼ˆ2ï¼‰CLIPå¼•å¯¼é‡‡æ ·æŒ–æ˜å¯èƒ½ä½†ä¸å­˜åœ¨çš„ç‰©ä½“ã€‚åº”ç”¨äºMSCOCOæ•°æ®é›†æ—¶ï¼ŒORICäº§ç”Ÿäº†ORIC-Benchå’ŒORICé£æ ¼è®­ç»ƒæ•°æ®ã€‚å¯¹18ä¸ªLVLMå’Œ2ä¸ªå¼€æ”¾è¯æ±‡æ£€æµ‹å™¨çš„è¯„ä¼°æ˜¾ç¤ºï¼Œåœ¨ä¸ä¸€è‡´çš„è¯­å¢ƒä¸‹ï¼Œæ€§èƒ½å¤§å¹…ä¸‹é™ä¸”å­˜åœ¨åè§æ¨¡å¼ã€‚ä½¿ç”¨Visual Reinforcement Fine-Tuningå¯¹Qwen3-VL-8B-Instructè¿›è¡Œå¾®è°ƒï¼Œåœ¨600ä¸ªORICé£æ ¼æ ·æœ¬ä¸Šæ”¹è¿›äº†å…¶åœ¨ORIC-Benchã€AMBERå’ŒHallusionBenchä¸Šçš„ç»“æœã€‚æ€»ä½“è€Œè¨€ï¼Œæˆ‘ä»¬è¯æ˜äº†ä¸Šä¸‹æ–‡ä¸ä¸€è‡´æ€§æ˜¯å…³é”®çš„ä¸ç¡®å®šæ€§æ¥æºï¼Œå¹¶ä¸ºæ›´å¯é çš„LVLMæä¾›äº†å·¥å…·ã€‚ç›¸å…³ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/ZhaoyangLi-1/ORIC%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/ZhaoyangLi-1/ORICè·å–ã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨å›¾åƒæè¿°ã€è§†è§‰é—®ç­”å’Œæœºå™¨äººæŠ€æœ¯ä¸Šè¡¨ç°ä¼˜ç§€ï¼Œä½†åœ¨å¤„ç†éå…¸å‹åœºæ™¯æ—¶å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>æ¨¡å‹åœ¨ä¸ä¸€è‡´çš„ä¸Šä¸‹æ–‡ä¸­è¯†åˆ«ç‰©ä½“çš„éš¾åº¦å¢åŠ ï¼Œä¸Šä¸‹æ–‡çš„ä¸ä¸€è‡´æ€§æ˜¯æ¨¡å‹å¤±è´¥çš„å…³é”®å› ç´ ã€‚</li>
<li>ä»‹ç»äº†Object Recognition in Incongruous Contextï¼ˆORICï¼‰æ¡†æ¶ï¼Œç”¨äºæ„å»ºä¸ä¸€è‡´çš„ç‰©ä½“ä¸Šä¸‹æ–‡å¯¹æ•°æ®é›†ã€‚</li>
<li>ORICæ¡†æ¶é€šè¿‡LLMå¼•å¯¼é‡‡æ ·å’ŒCLIPå¼•å¯¼é‡‡æ ·ä¸¤ç§ç­–ç•¥æ¥åˆ›å»ºè®­ç»ƒæ•°æ®ã€‚</li>
<li>å¯¹å¤šä¸ªLVLMå’Œå¼€æ”¾è¯æ±‡æ£€æµ‹å™¨çš„è¯„ä¼°æ˜¾ç¤ºï¼Œåœ¨ä¸ä¸€è‡´çš„è¯­å¢ƒä¸‹æ€§èƒ½ä¸‹é™ï¼Œå¹¶å­˜åœ¨åè§æ¨¡å¼ã€‚</li>
<li>é€šè¿‡Visual Reinforcement Fine-Tuningå¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œæ”¹è¿›äº†åœ¨ç‰¹å®šæµ‹è¯•é›†ä¸Šçš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.15695">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-846578176dbe6f9b9048a4a5d8ff3a3b" align="middle">
<img src="https://picx.zhimg.com/v2-e9fc9725407e3ccc1a066768b0522066" align="middle">
<img src="https://picx.zhimg.com/v2-d5afde05da2aaf2ac3837e727a91e75f" align="middle">
<img src="https://picx.zhimg.com/v2-f57e68ca7b0842fbef836e7e0a5b6e07" align="middle">
<img src="https://picx.zhimg.com/v2-2fadee6fdba31c221e745f65c6184001" align="middle">
<img src="https://picx.zhimg.com/v2-07c2c95ae872ca8aeab592dcd7952c68" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="CAMA-Enhancing-Mathematical-Reasoning-in-Large-Language-Models-with-Causal-Knowledge"><a href="#CAMA-Enhancing-Mathematical-Reasoning-in-Large-Language-Models-with-Causal-Knowledge" class="headerlink" title="CAMA: Enhancing Mathematical Reasoning in Large Language Models with Causal Knowledge"></a>CAMA: Enhancing Mathematical Reasoning in Large Language Models with Causal Knowledge</h2><p><strong>Authors:Lei Zan, Keli Zhang, Ruichu Cai, Lujia Pan</strong></p>
<p>Large Language Models (LLMs) have demonstrated strong performance across a wide range of tasks, yet they still struggle with complex mathematical reasoning, a challenge fundamentally rooted in deep structural dependencies. To address this challenge, we propose \textbf{CA}usal \textbf{MA}thematician (\textbf{CAMA}), a two-stage causal framework that equips LLMs with explicit, reusable mathematical structure. In the learning stage, CAMA first constructs the \textbf{M}athematical \textbf{C}ausal \textbf{G}raph (\textbf{MCG}), a high-level representation of solution strategies, by combining LLM priors with causal discovery algorithms applied to a corpus of question-solution pairs. The resulting MCG encodes essential knowledge points and their causal dependencies. To better align the graph with downstream reasoning tasks, CAMA further refines the MCG through iterative feedback derived from a selected subset of the question-solution pairs. In the reasoning stage, given a new question, CAMA dynamically extracts a task-relevant subgraph from the MCG, conditioned on both the question content and the LLMâ€™s intermediate reasoning trace. This subgraph, which encodes the most pertinent knowledge points and their causal dependencies, is then injected back into the LLM to guide its reasoning process. Empirical results on real-world datasets show that CAMA significantly improves LLM performance on challenging mathematical problems. Furthermore, our experiments demonstrate that structured guidance consistently outperforms unstructured alternatives, and that incorporating asymmetric causal relationships yields greater improvements than using symmetric associations alone.</p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†åœ¨å¤æ‚çš„æ•°å­¦æ¨ç†æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ï¼Œè¿™ä¸€æŒ‘æˆ˜æ ¹æœ¬æºäºæ·±å±‚çš„ç»“æ„ä¾èµ–æ€§ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†é…å¤‡LLMä»¥æ˜ç¡®ã€å¯é‡å¤ä½¿ç”¨çš„æ•°å­¦ç»“æ„çš„å› æœæ•°å­¦å®¶ï¼ˆ\textbf{CAMA}ï¼‰ä¸¤é˜¶æ®µå› æœæ¡†æ¶ã€‚åœ¨å­¦ä¹ é˜¶æ®µï¼ŒCAMAé¦–å…ˆé€šè¿‡ç»“åˆLLMå…ˆéªŒçŸ¥è¯†å’Œåº”ç”¨äºé—®é¢˜è§£å†³æ–¹æ¡ˆå¯¹è¯­æ–™åº“çš„å› æœå‘ç°ç®—æ³•ï¼Œæ„å»ºæ•°å­¦å› æœå›¾ï¼ˆMCGï¼‰ï¼Œè¿™æ˜¯ä¸€ç§è§£å†³æ–¹æ¡ˆç­–ç•¥çš„é«˜çº§è¡¨ç¤ºã€‚ç”Ÿæˆçš„MCGç¼–ç äº†å¿…è¦çš„çŸ¥è¯†ç‚¹åŠå…¶å› æœå…³ç³»ã€‚ä¸ºäº†æ›´å¥½åœ°ä½¿å›¾å½¢ä¸ä¸‹æ¸¸æ¨ç†ä»»åŠ¡å¯¹é½ï¼ŒCAMAè¿›ä¸€æ­¥é€šè¿‡æ¥è‡ªé—®é¢˜è§£å†³æ–¹æ¡ˆå¯¹ä¸­é€‰æ‹©å­é›†çš„è¿­ä»£åé¦ˆæ¥ä¼˜åŒ–MCGã€‚åœ¨æ¨ç†é˜¶æ®µï¼Œå¯¹äºæ–°é—®é¢˜ï¼ŒCAMAä¼šæ ¹æ®é—®é¢˜çš„å†…å®¹å’ŒLLMçš„ä¸­é—´æ¨ç†è½¨è¿¹ï¼Œä»MCGä¸­åŠ¨æ€æå–ä¸ä»»åŠ¡ç›¸å…³çš„å­å›¾ã€‚è¿™ä¸ªå­å›¾ç¼–ç äº†æœ€ç›¸å…³çŸ¥è¯†ç‚¹åŠå…¶å› æœå…³ç³»ï¼Œç„¶åæ³¨å…¥LLMä»¥æŒ‡å¯¼å…¶æ¨ç†è¿‡ç¨‹ã€‚åœ¨çœŸå®æ•°æ®é›†ä¸Šçš„å®è¯ç»“æœè¡¨æ˜ï¼ŒCAMAæ˜¾è‘—æé«˜äº†LLMè§£å†³å¤æ‚æ•°å­¦é—®é¢˜æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œç»“æ„åŒ–çš„æŒ‡å¯¼å§‹ç»ˆä¼˜äºéç»“æ„åŒ–æ›¿ä»£æ–¹æ¡ˆï¼Œå¹¶ä¸”çº³å…¥ä¸å¯¹ç§°çš„å› æœå…³ç³»æ¯”ä»…ä½¿ç”¨å¯¹ç§°å…³è”å¸¦æ¥æ›´å¤§çš„æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02583v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†åœ¨å¤æ‚çš„æ•°å­¦æ¨ç†æ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæå‡ºäº†å› æœæ•°å­¦å®¶ï¼ˆCAMAï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼Œä¸ºLLMæä¾›äº†æ˜ç¡®çš„å¯é‡å¤ä½¿ç”¨çš„æ•°å­¦ç»“æ„ã€‚å­¦ä¹ é˜¶æ®µæ„å»ºæ•°å­¦å› æœå›¾ï¼ˆMCGï¼‰ï¼Œé€šè¿‡ç»“åˆLLMå…ˆéªŒçŸ¥è¯†ä¸å› æœå‘ç°ç®—æ³•ï¼Œå¯¹é—®é¢˜è§£å†³æ–¹æ¡ˆå¯¹è¿›è¡Œç¼–ç ï¼Œå½¢æˆé«˜å±‚æ¬¡çš„è§£å†³æ–¹æ¡ˆç­–ç•¥è¡¨ç¤ºã€‚ä¸ºäº†ä¸ä¸‹æ¸¸æ¨ç†ä»»åŠ¡æ›´å¥½åœ°å¯¹é½ï¼ŒCAMAé€šè¿‡æ¥è‡ªé—®é¢˜è§£å†³æ–¹æ¡ˆå¯¹çš„è¿­ä»£åé¦ˆè¿›ä¸€æ­¥æ”¹è¿›äº†MCGã€‚åœ¨æ¨ç†é˜¶æ®µï¼Œå¯¹äºæ–°é—®é¢˜ï¼ŒCAMAæ ¹æ®é—®é¢˜å’ŒLLMçš„ä¸­é—´æ¨ç†è½¨è¿¹ä»MCGä¸­æå–ç›¸å…³å­å›¾ï¼Œå¹¶å°†å…¶æ³¨å…¥LLMä»¥æŒ‡å¯¼å…¶æ¨ç†è¿‡ç¨‹ã€‚å®è¯ç»“æœè¡¨æ˜ï¼ŒCAMAæ˜¾è‘—æé«˜äº†LLMåœ¨æŒ‘æˆ˜æ€§æ•°å­¦é—®é¢˜ä¸Šçš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨å¤æ‚æ•°å­¦æ¨ç†ä¸Šè¡¨ç°ä¸è¶³ï¼Œéœ€è¦æ–°çš„æ–¹æ³•æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>CAMAæ¡†æ¶æ—¨åœ¨é€šè¿‡ç»“åˆLLMå…ˆéªŒçŸ¥è¯†å’Œå› æœå‘ç°ç®—æ³•æ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ã€‚</li>
<li>CAMAåˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šå­¦ä¹ é˜¶æ®µå’Œæ¨ç†é˜¶æ®µã€‚</li>
<li>åœ¨å­¦ä¹ é˜¶æ®µï¼ŒCAMAæ„å»ºæ•°å­¦å› æœå›¾ï¼ˆMCGï¼‰ï¼Œè¡¨ç¤ºé—®é¢˜çš„è§£å†³æ–¹æ¡ˆç­–ç•¥ã€‚</li>
<li>é€šè¿‡è¿­ä»£åé¦ˆï¼ŒCAMAæ”¹è¿›äº†MCGï¼Œä½¿å…¶æ›´å¥½åœ°ä¸ä¸‹æ¸¸æ¨ç†ä»»åŠ¡å¯¹é½ã€‚</li>
<li>åœ¨æ¨ç†é˜¶æ®µï¼ŒCAMAæ ¹æ®é—®é¢˜å’ŒLLMçš„æ¨ç†è½¨è¿¹ä»MCGä¸­æå–ç›¸å…³å­å›¾æ¥æŒ‡å¯¼æ¨ç†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02583">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bc26f9d94b24ab2a2686a77a63e1770b" align="middle">
<img src="https://picx.zhimg.com/v2-ef275aa14374135475ee2882a02df67c" align="middle">
<img src="https://picx.zhimg.com/v2-82b25e2d6daf272d8d114d68b2d44888" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Comprehension-Without-Competence-Architectural-Limits-of-LLMs-in-Symbolic-Computation-and-Reasoning"><a href="#Comprehension-Without-Competence-Architectural-Limits-of-LLMs-in-Symbolic-Computation-and-Reasoning" class="headerlink" title="Comprehension Without Competence: Architectural Limits of LLMs in Symbolic Computation and Reasoning"></a>Comprehension Without Competence: Architectural Limits of LLMs in Symbolic Computation and Reasoning</h2><p><strong>Authors:Zheng Zhang</strong></p>
<p>Large Language Models (LLMs) display striking surface fluency yet systematically fail at tasks requiring symbolic reasoning, arithmetic accuracy, and logical consistency. This paper offers a structural diagnosis of such failures, revealing a persistent gap between \textit{comprehension} and \textit{competence}. Through controlled experiments and architectural analysis, we demonstrate that LLMs often articulate correct principles without reliably applying themâ€“a failure rooted not in knowledge access, but in computational execution. We term this phenomenon the computational \textit{split-brain syndrome}, where instruction and action pathways are geometrically and functionally dissociated. This core limitation recurs across domains, from mathematical operations to relational inferences, and explains why model behavior remains brittle even under idealized prompting. We argue that LLMs function as powerful pattern completion engines, but lack the architectural scaffolding for principled, compositional reasoning. Our findings delineate the boundary of current LLM capabilities and motivate future models with metacognitive control, principle lifting, and structurally grounded execution. This diagnosis also clarifies why mechanistic interpretability findings may reflect training-specific pattern coordination rather than universal computational principles, and why the geometric separation between instruction and execution pathways suggests limitations in neural introspection and mechanistic analysis.</p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è™½ç„¶åœ¨è¡¨é¢å±•ç°å‡ºäº†æƒŠäººçš„æµç•…æ€§ï¼Œä½†åœ¨éœ€è¦ç¬¦å·æ¨ç†ã€ç®—æœ¯å‡†ç¡®æ€§å’Œé€»è¾‘ä¸€è‡´æ€§çš„ä»»åŠ¡ä¸Šå´ç³»ç»Ÿæ€§åœ°å¤±è´¥äº†ã€‚æœ¬æ–‡å¯¹è¿™äº›å¤±è´¥è¿›è¡Œäº†ç»“æ„æ€§è¯Šæ–­ï¼Œæ­ç¤ºäº†â€œç†è§£â€å’Œâ€œèƒ½åŠ›â€ä¹‹é—´çš„æŒä¹…å·®è·ã€‚é€šè¿‡å—æ§å®éªŒå’Œæ¶æ„åˆ†æï¼Œæˆ‘ä»¬è¯æ˜LLMså¾€å¾€èƒ½è¡¨è¿°æ­£ç¡®çš„åŸåˆ™ï¼Œä½†å´ä¸èƒ½å¯é åœ°åº”ç”¨å®ƒä»¬â€”â€”è¿™ç§å¤±è´¥å¹¶éæºäºçŸ¥è¯†è·å–ï¼Œè€Œæ˜¯æºäºè®¡ç®—æ‰§è¡Œã€‚æˆ‘ä»¬å°†è¿™ç§ç°è±¡ç§°ä¸ºâ€œè®¡ç®—æ€§â€˜è„‘è£‚ç—‡â€™â€ï¼Œå…¶ä¸­æŒ‡ä»¤å’Œè¡ŒåŠ¨è·¯å¾„åœ¨å‡ ä½•å’ŒåŠŸèƒ½ä¸Šéƒ½æ˜¯åˆ†ç¦»çš„ã€‚è¿™ä¸€æ ¸å¿ƒé™åˆ¶åœ¨å„ä¸ªé¢†åŸŸéƒ½æ™®éå­˜åœ¨ï¼Œä»æ•°å­¦è¿ç®—åˆ°å…³ç³»æ¨ç†ï¼Œå¹¶è§£é‡Šäº†ä¸ºä»€ä¹ˆå³ä½¿åœ¨ç†æƒ³åŒ–çš„æç¤ºä¸‹ï¼Œæ¨¡å‹çš„è¡Œä¸ºä»ç„¶å¾ˆè„†å¼±ã€‚æˆ‘ä»¬è®¤ä¸ºLLMsæ˜¯å¼ºå¤§çš„æ¨¡å¼å®Œæˆå¼•æ“ï¼Œä½†ç¼ºä¹æœ‰åŸåˆ™ã€æœ‰ç»„ç»‡çš„æ¨ç†æ‰€éœ€çš„æ¶æ„æ”¯æ’‘ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç•Œå®šäº†å½“å‰LLMçš„èƒ½åŠ›è¾¹ç•Œï¼Œå¹¶æ¿€åŠ±æœªæ¥æ„å»ºå…·æœ‰å…ƒè®¤çŸ¥æ§åˆ¶ã€åŸåˆ™æå‡å’Œç»“æ„åŸºç¡€æ‰§è¡Œçš„æ¨¡å‹ã€‚è¿™ç§è¯Šæ–­ä¹Ÿé˜æ˜äº†ä¸ºä»€ä¹ˆæœºåˆ¶æ€§è§£é‡Šçš„å‘ç°å¯èƒ½åæ˜ è®­ç»ƒç‰¹å®šçš„æ¨¡å¼åè°ƒï¼Œè€Œä¸æ˜¯æ™®éçš„è®¡ç®—åŸåˆ™ï¼Œä»¥åŠä¸ºä»€ä¹ˆæŒ‡ä»¤å’Œæ‰§è¡Œè·¯å¾„ä¹‹é—´çš„å‡ ä½•åˆ†ç¦»è¡¨æ˜ç¥ç»å†…çœå’Œæœºåˆ¶æ€§åˆ†æçš„å±€é™æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.10624v3">PDF</a> v2: Two TMLR revision rounds addressing reviewer feedback. Added real-world validation (3.4), interpretability analysis (7), computational hallucination framework, strengthened theory. v3: Sec 3.2 - added transformer architecture diagram, clarified UAT capacity vs computational limits, improved role specialization theorem presentation</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æŒ‡å‡ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è™½ç„¶è¡¨ç°å‡ºä»¤äººæƒŠå¹çš„è¡¨é¢æµç•…åº¦ï¼Œä½†åœ¨éœ€è¦ç¬¦å·æ¨ç†ã€ç®—æœ¯å‡†ç¡®æ€§å’Œé€»è¾‘ä¸€è‡´æ€§çš„ä»»åŠ¡ä¸Šå´è¡¨ç°å‡ºç³»ç»Ÿæ€§å¤±è´¥ã€‚æ–‡ç« å¯¹è¿™ç±»å¤±è´¥è¿›è¡Œäº†ç»“æ„æ€§è¯Šæ–­ï¼Œæ­ç¤ºäº†â€œç†è§£â€ä¸â€œèƒ½åŠ›â€ä¹‹é—´çš„æŒä¹…å·®è·ã€‚é€šè¿‡æ§åˆ¶å®éªŒå’Œæ¶æ„åˆ†æï¼Œæ–‡ç« å±•ç¤ºäº†LLMså¾€å¾€èƒ½å¤Ÿæ­£ç¡®é˜è¿°åŸåˆ™ï¼Œä½†åœ¨å®é™…åº”ç”¨ä¸­å´æ— æ³•å¯é åœ°åº”ç”¨è¿™äº›åŸåˆ™â€”â€”è¿™ç§å¤±è´¥å¹¶éæºäºçŸ¥è¯†è·å–ï¼Œè€Œæ˜¯æºäºè®¡ç®—æ‰§è¡Œã€‚è¿™ç§ç°è±¡è¢«ç§°ä¸ºè®¡ç®—â€œè£‚è„‘ç»¼åˆå¾â€ï¼Œå…¶ä¸­æŒ‡ä»¤å’Œæ‰§è¡Œè·¯å¾„åœ¨å‡ ä½•å’ŒåŠŸèƒ½ä¸Šç›¸äº’åˆ†ç¦»ã€‚è¿™ç§æ ¸å¿ƒé™åˆ¶åœ¨å„ä¸ªé¢†åŸŸæ™®éå­˜åœ¨ï¼Œä»æ•°å­¦è¿ç®—åˆ°å…³ç³»æ¨ç†éƒ½æ˜¯å¦‚æ­¤ï¼Œå¹¶è§£é‡Šäº†ä¸ºä»€ä¹ˆå³ä½¿åœ¨ç†æƒ³åŒ–çš„æç¤ºä¸‹ï¼Œæ¨¡å‹è¡Œä¸ºä»ç„¶è„†å¼±ã€‚æ–‡ç« å¼ºè°ƒLLMsä½œä¸ºå¼ºå¤§çš„æ¨¡å¼å®Œæˆå¼•æ“çš„åŠŸèƒ½ï¼Œä½†ç¼ºä¹è¿›è¡ŒåŸåˆ™æ€§ã€ç»„åˆæ€§æ¨ç†çš„æ¶æ„æ”¯æ’‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsè¡¨é¢æµç•…å´ç³»ç»Ÿæ€§åœ°åœ¨éœ€è¦ç¬¦å·æ¨ç†ã€ç®—æœ¯å’Œé€»è¾‘çš„ä»»åŠ¡ä¸Šå¤±è´¥ã€‚</li>
<li>LLMsåœ¨ç†è§£åŸåˆ™æ–¹é¢è¡¨ç°å‡ºèƒ½åŠ›ï¼Œä½†åœ¨å®é™…åº”ç”¨ä¸­æ— æ³•å¯é åœ°åº”ç”¨è¿™äº›åŸåˆ™ã€‚</li>
<li>LLMsçš„å¤±è´¥æºäºè®¡ç®—æ‰§è¡Œï¼Œè€ŒéçŸ¥è¯†è·å–ã€‚</li>
<li>LLMså±•ç°å‡ºè®¡ç®—â€œè£‚è„‘ç»¼åˆå¾â€ï¼ŒæŒ‡ä»¤ä¸æ‰§è¡Œè·¯å¾„å­˜åœ¨å‡ ä½•å’ŒåŠŸèƒ½ä¸Šçš„åˆ†ç¦»ã€‚</li>
<li>LLMsçš„å±€é™åœ¨å¤šä¸ªé¢†åŸŸéƒ½æ™®éå­˜åœ¨ï¼ŒåŒ…æ‹¬æ•°å­¦è¿ç®—å’Œå…³ç³»æ¨ç†ã€‚</li>
<li>LLMsä½œä¸ºæ¨¡å¼å®Œæˆå¼•æ“åŠŸèƒ½å¼ºå¤§ï¼Œä½†ç¼ºä¹è¿›è¡ŒåŸåˆ™æ€§ã€ç»„åˆæ€§æ¨ç†çš„æ¶æ„æ”¯æ’‘ã€‚</li>
<li>ä¸ºäº†æ”¹å–„LLMsçš„æ€§èƒ½ï¼Œæœªæ¥éœ€è¦å‘å±•å…·æœ‰å…ƒè®¤çŸ¥æ§åˆ¶ã€åŸåˆ™æå‡å’Œç»“æ„åŸºç¡€æ‰§è¡Œçš„æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.10624">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a893ad6aba7301301d392efc6c2c8b59" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Sensory-Motor-Control-with-Large-Language-Models-via-Iterative-Policy-Refinement"><a href="#Sensory-Motor-Control-with-Large-Language-Models-via-Iterative-Policy-Refinement" class="headerlink" title="Sensory-Motor Control with Large Language Models via Iterative Policy Refinement"></a>Sensory-Motor Control with Large Language Models via Iterative Policy Refinement</h2><p><strong>Authors:JÃ´nata Tyska Carvalho, Stefano Nolfi</strong></p>
<p>We propose a method that enables large language models (LLMs) to control embodied agents through the generation of control policies that directly map continuous observation vectors to continuous action vectors. At the outset, the LLMs generate a control strategy based on a textual description of the agent, its environment, and the intended goal. This strategy is then iteratively refined through a learning process in which the LLMs are repeatedly prompted to improve the current strategy, using performance feedback and sensory-motor data collected during its evaluation. The method is validated on classic control tasks from the Gymnasium library and the inverted pendulum task from the MuJoCo library. The approach proves effective with relatively compact models such as GPT-oss:120b and Qwen2.5:72b. In most cases, it successfully identifies optimal or near-optimal solutions by integrating symbolic knowledge derived through reasoning with sub-symbolic sensory-motor data gathered as the agent interacts with its environment.</p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–¹æ³•ï¼Œä½¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èƒ½å¤Ÿé€šè¿‡ç”Ÿæˆæ§åˆ¶ç­–ç•¥æ¥æ§åˆ¶å®ä½“ä»£ç†ï¼Œè¯¥ç­–ç•¥ç›´æ¥å°†è¿ç»­çš„è§‚æµ‹å‘é‡æ˜ å°„åˆ°è¿ç»­çš„åŠ¨ä½œå‘é‡ã€‚æœ€åˆï¼ŒLLMåŸºäºä»£ç†çš„æ–‡æœ¬æè¿°ã€å…¶ç¯å¢ƒä»¥åŠé¢„æœŸç›®æ ‡æ¥ç”Ÿæˆæ§åˆ¶ç­–ç•¥ã€‚ç„¶åï¼Œé€šè¿‡å­¦ä¹ è¿‡ç¨‹å¯¹ç­–ç•¥è¿›è¡Œè¿­ä»£ä¼˜åŒ–ï¼Œåœ¨æ­¤è¿‡ç¨‹ä¸­ï¼ŒLLMåå¤è¢«æç¤ºä»¥æ€§èƒ½åé¦ˆå’Œæ”¶é›†åˆ°çš„æ„Ÿè§‰è¿åŠ¨æ•°æ®æ¥æ”¹å–„å½“å‰ç­–ç•¥ã€‚è¯¥æ–¹æ³•åœ¨Gymnasiumåº“çš„ç»å…¸æ§åˆ¶ä»»åŠ¡å’ŒMuJoCoåº“çš„å€’ç«‹æ‘†ä»»åŠ¡ä¸Šå¾—åˆ°äº†éªŒè¯ã€‚è¯¥æ–¹æ³•åœ¨ç›¸å¯¹ç´§å‡‘çš„æ¨¡å‹ï¼ˆå¦‚GPT-ossï¼š120bå’ŒQwen2.5ï¼š72bï¼‰ä¸Šè¯æ˜æ˜¯æœ‰æ•ˆçš„ã€‚åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œå®ƒé€šè¿‡æ•´åˆé€šè¿‡æ¨ç†è·å¾—çš„ç¬¦å·çŸ¥è¯†ä¸ä»£ç†åœ¨ä¸ç¯å¢ƒäº¤äº’è¿‡ç¨‹ä¸­æ”¶é›†åˆ°çš„æ„Ÿè§‰è¿åŠ¨æ•°æ®ï¼ŒæˆåŠŸæ‰¾åˆ°æœ€ä¼˜æˆ–æ¥è¿‘æœ€ä¼˜çš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04867v3">PDF</a> Article updated with results from gpt-oss:120b and gpt-oss:20b. 27 pages (13 pages are from appendix), 8 figures, 2 tables, code for experiments replication and supplementary material provided at <a target="_blank" rel="noopener" href="https://github.com/jtyska/llm-robotics-article/">https://github.com/jtyska/llm-robotics-article/</a></p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–¹æ³•ï¼Œé€šè¿‡ç”Ÿæˆè¿ç»­è§‚æµ‹å‘é‡åˆ°è¿ç»­åŠ¨ä½œå‘é‡çš„æ§åˆ¶ç­–ç•¥ï¼Œä½¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èƒ½å¤Ÿæ§åˆ¶å®ä½“ä»£ç†ã€‚LLMåŸºäºä»£ç†çš„æ–‡æœ¬æè¿°ã€ç¯å¢ƒå’Œç›®æ ‡ç”Ÿæˆåˆå§‹æ§åˆ¶ç­–ç•¥ï¼Œç„¶åé€šè¿‡å­¦ä¹ è¿‡ç¨‹ä¸æ–­æç¤ºæ”¹è¿›å½“å‰ç­–ç•¥ï¼Œä½¿ç”¨æ€§èƒ½åé¦ˆå’Œæ”¶é›†åˆ°çš„æ„Ÿè§‰è¿åŠ¨æ•°æ®è¿›è¡Œè¯„ä»·ã€‚è¯¥æ–¹æ³•åœ¨Gymnasiumåº“çš„ç»å…¸æ§åˆ¶ä»»åŠ¡å’ŒMuJoCoåº“çš„å€’ç«‹æ‘†ä»»åŠ¡ä¸­å¾—åˆ°äº†éªŒè¯ã€‚ä½¿ç”¨ç›¸å¯¹ç´§å‡‘çš„æ¨¡å‹ï¼ˆå¦‚GPT-oss:120bå’ŒQwen2.5:72bï¼‰å³å¯å®ç°æœ‰æ•ˆè¯æ˜ã€‚è¯¥æ–¹æ³•æˆåŠŸåœ°å°†ç¬¦å·çŸ¥è¯†æ¨ç†ä¸ä»£ç†ä¸ç¯å¢ƒçš„äº¤äº’è¿‡ç¨‹ä¸­æ”¶é›†çš„æ„Ÿè§‰è¿åŠ¨æ•°æ®ç›¸ç»“åˆï¼Œæ‰¾å‡ºæœ€ä¼˜æˆ–æ¥è¿‘æœ€ä¼˜çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ§åˆ¶æ–¹æ³•ï¼Œå…è®¸é€šè¿‡ç”Ÿæˆæ§åˆ¶ç­–ç•¥æ¥æ§åˆ¶å®ä½“ä»£ç†ã€‚</li>
<li>LLMæ ¹æ®ä»£ç†çš„æ–‡æœ¬æè¿°ã€ç¯å¢ƒå’Œç›®æ ‡æ¥ç”Ÿæˆåˆå§‹æ§åˆ¶ç­–ç•¥ã€‚</li>
<li>æ§åˆ¶ç­–ç•¥é€šè¿‡è¿­ä»£å­¦ä¹ è¿‡ç¨‹è¿›è¡Œæ”¹è¿›ï¼Œç»“åˆæ€§èƒ½åé¦ˆå’Œæ„Ÿè§‰è¿åŠ¨æ•°æ®ã€‚</li>
<li>æ–¹æ³•åœ¨ç»å…¸æ§åˆ¶ä»»åŠ¡å’Œå€’ç«‹æ‘†ä»»åŠ¡ä¸­å¾—åˆ°äº†éªŒè¯ã€‚</li>
<li>ç›¸å¯¹ç´§å‡‘çš„æ¨¡å‹ï¼Œå¦‚GPT-oss:120bå’ŒQwen2.5:72bï¼Œå³å¯å®ç°æœ‰æ•ˆè¯æ˜ã€‚</li>
<li>æˆåŠŸç»“åˆç¬¦å·çŸ¥è¯†æ¨ç†å’Œæ„Ÿè§‰è¿åŠ¨æ•°æ®ï¼Œæ‰¾å‡ºæœ€ä¼˜æˆ–æ¥è¿‘æœ€ä¼˜çš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>è¯¥æ–¹æ³•å±•ç¤ºäº†æ–‡æœ¬æè¿°ä¸å®é™…è¡ŒåŠ¨ä¹‹é—´çš„ç›´æ¥æ˜ å°„æ½œåŠ›ï¼Œä¸ºæœªæ¥çš„è¯­è¨€æ§åˆ¶ç ”ç©¶æä¾›äº†æ–°çš„æ–¹å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04867">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a5a46d9526956177a53eb9fa574eb8ab" align="middle">
<img src="https://picx.zhimg.com/v2-bcfbcfde9b1557a56621225d59b48348" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Transformer-Copilot-Learning-from-The-Mistake-Log-in-LLM-Fine-tuning"><a href="#Transformer-Copilot-Learning-from-The-Mistake-Log-in-LLM-Fine-tuning" class="headerlink" title="Transformer Copilot: Learning from The Mistake Log in LLM Fine-tuning"></a>Transformer Copilot: Learning from The Mistake Log in LLM Fine-tuning</h2><p><strong>Authors:Jiaru Zou, Yikun Ban, Zihao Li, Yunzhe Qi, Ruizhong Qiu, Ling Yang, Jingrui He</strong></p>
<p>Large language models are typically adapted to downstream tasks through supervised fine-tuning on domain-specific data. While standard fine-tuning focuses on minimizing generation loss to optimize model parameters, we take a deeper step by retaining and leveraging the modelâ€™s own learning signals, analogous to how human learners reflect on past mistakes to improve future performance. We first introduce the concept of Mistake Log to systematically track the modelâ€™s learning behavior and recurring errors throughout fine-tuning. Treating the original transformer-based model as the Pilot, we correspondingly design a Copilot model to refine the Pilotâ€™s inference performance via logits rectification. We name the overall Pilot-Copilot framework the Transformer Copilot, which introduces (i) a novel Copilot model design, (ii) a joint training paradigm where the Copilot continuously learns from the evolving Mistake Log alongside the Pilot, and (iii) a fused inference paradigm where the Copilot rectifies the Pilotâ€™s logits for enhanced generation. We provide both theoretical and empirical analyses on our new learning framework. Experiments on 12 benchmarks spanning commonsense, arithmetic, and recommendation tasks demonstrate that Transformer Copilot consistently improves performance by up to 34.5%, while introducing marginal computational overhead to Pilot models and exhibiting strong scalability and transferability. Our code is released at <a target="_blank" rel="noopener" href="https://github.com/jiaruzouu/TransformerCopilot">https://github.com/jiaruzouu/TransformerCopilot</a>.</p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹é€šå¸¸é€šè¿‡é’ˆå¯¹ç‰¹å®šé¢†åŸŸçš„ç›‘ç£å¾®è°ƒæ¥é€‚åº”ä¸‹æ¸¸ä»»åŠ¡ã€‚è™½ç„¶æ ‡å‡†çš„å¾®è°ƒä¾§é‡äºé€šè¿‡æœ€å°åŒ–ç”ŸæˆæŸå¤±æ¥ä¼˜åŒ–æ¨¡å‹å‚æ•°ï¼Œä½†æˆ‘ä»¬æ›´è¿›ä¸€æ­¥ï¼Œä¿ç•™å¹¶åˆ©ç”¨æ¨¡å‹æœ¬èº«çš„å­¦ä¹ ä¿¡å·ï¼Œç±»ä¼¼äºäººç±»å­¦ä¹ è€…å¦‚ä½•åæ€è¿‡å»çš„é”™è¯¯ä»¥æé«˜æœªæ¥çš„è¡¨ç°ã€‚æˆ‘ä»¬é¦–å…ˆå¼•å…¥â€œé”™è¯¯æ—¥å¿—â€çš„æ¦‚å¿µï¼Œä»¥ç³»ç»Ÿåœ°è·Ÿè¸ªæ¨¡å‹åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­çš„å­¦ä¹ è¡Œä¸ºå’Œåå¤å‡ºç°çš„é”™è¯¯ã€‚æˆ‘ä»¬å°†åŸå§‹çš„åŸºäºå˜å‹å™¨çš„æ¨¡å‹è§†ä¸ºâ€œPilotâ€ï¼ˆé£è¡Œå‘˜ï¼‰æ¨¡å‹ï¼Œç›¸åº”åœ°è®¾è®¡äº†ä¸€ä¸ªâ€œCopilotâ€ï¼ˆå‰¯é©¾é©¶ï¼‰æ¨¡å‹æ¥ä¼˜åŒ–Pilotæ¨¡å‹çš„æ¨ç†æ€§èƒ½ï¼Œé€šè¿‡è°ƒæ•´logitsï¼ˆé€»è¾‘å€¼ï¼‰æ¥å®ç°ã€‚æˆ‘ä»¬å°†æ•´ä½“çš„Pilot-Copilotæ¡†æ¶å‘½åä¸ºâ€œTransformer Copilotâ€ï¼Œå®ƒå¼•å…¥äº†ï¼ˆiï¼‰æ–°é¢–çš„Copilotæ¨¡å‹è®¾è®¡ï¼Œï¼ˆiiï¼‰è”åˆè®­ç»ƒèŒƒå¼ï¼Œå…¶ä¸­Copilotä¸Pilotä¸€èµ·ä¸æ–­ä»æ¼”å˜çš„é”™è¯¯æ—¥å¿—ä¸­å­¦ä¹ ï¼Œä»¥åŠï¼ˆiiiï¼‰èåˆæ¨ç†èŒƒå¼ï¼Œå…¶ä¸­Copilotè°ƒæ•´Pilotçš„é€»è¾‘å€¼ä»¥å®ç°å¢å¼ºçš„ç”Ÿæˆæ•ˆæœã€‚æˆ‘ä»¬é’ˆå¯¹æ–°çš„å­¦ä¹ æ¡†æ¶è¿›è¡Œäº†ç†è®ºåˆ†æå’Œå®è¯ç ”ç©¶ã€‚åœ¨æ¶µç›–å¸¸è¯†ã€ç®—æœ¯å’Œæ¨èä»»åŠ¡çš„12ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒTransformer Copilotå¯ä¸æ–­æé«˜æ€§èƒ½è‡³é«˜è¾¾34.5%ï¼ŒåŒæ—¶ä¸ºPilotæ¨¡å‹å¢åŠ äº†è½»å¾®çš„è®¡ç®—å¼€é”€ï¼Œå¹¶è¡¨ç°å‡ºå¼ºå¤§çš„å¯æ‰©å±•æ€§å’Œå¯è¿ç§»æ€§ã€‚æˆ‘ä»¬çš„ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/jiaruzouu/TransformerCopilot%E3%80%82">https://github.com/jiaruzouu/TransformerCopilotã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16270v2">PDF</a> NeurIPS 2025 Spotlight</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹é€šè¿‡é¢†åŸŸç‰¹å®šæ•°æ®çš„ç›‘ç£å¾®è°ƒæ¥é€‚åº”ä¸‹æ¸¸ä»»åŠ¡ã€‚æœ¬æ–‡æå‡ºä¸€ç§Transformer Copilotæ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥Mistake Logæ¥è·Ÿè¸ªæ¨¡å‹å­¦ä¹ è¡Œä¸ºå’Œå¾®è°ƒè¿‡ç¨‹ä¸­çš„åå¤é”™è¯¯ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬Copilotæ¨¡å‹è®¾è®¡ã€è”åˆè®­ç»ƒèŒƒå¼å’Œèåˆæ¨ç†èŒƒå¼ã€‚å®éªŒè¡¨æ˜ï¼ŒTransformer Copilotåœ¨å¸¸è¯†ã€ç®—æœ¯å’Œæ¨èä»»åŠ¡ä¸Šæ€§èƒ½æå‡é«˜è¾¾34.5%ï¼ŒåŒæ—¶ç»™Pilotæ¨¡å‹å¸¦æ¥è½»å¾®çš„è®¡ç®—å¼€é”€ï¼Œå¹¶å±•ç°å‡ºå¼ºå¤§çš„å¯æ‰©å±•æ€§å’Œå¯è¿ç§»æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹é€šè¿‡å¾®è°ƒé€‚åº”ä¸‹æ¸¸ä»»åŠ¡ï¼Œä½†æ ‡å‡†å¾®è°ƒä¸»è¦å…³æ³¨ç”ŸæˆæŸå¤±æœ€å°åŒ–æ¥ä¼˜åŒ–æ¨¡å‹å‚æ•°ã€‚</li>
<li>Transformer Copilotæ¡†æ¶å¼•å…¥Mistake Logæ¥è·Ÿè¸ªæ¨¡å‹å­¦ä¹ è¡Œä¸ºå’Œå¾®è°ƒè¿‡ç¨‹ä¸­çš„é”™è¯¯ã€‚</li>
<li>Transformer CopilotåŒ…æ‹¬Copilotæ¨¡å‹è®¾è®¡ï¼Œæ—¨åœ¨æé«˜Pilotæ¨¡å‹çš„æ¨ç†æ€§èƒ½ã€‚</li>
<li>Transformer Copiloté‡‡ç”¨è”åˆè®­ç»ƒèŒƒå¼ï¼ŒCopilotä»ä¸æ–­æ›´æ–°çš„Mistake Logä¸­å­¦ä¹ å¹¶ä¸Pilotå…±åŒè¿›æ­¥ã€‚</li>
<li>Transformer Copiloté‡‡ç”¨èåˆæ¨ç†èŒƒå¼ï¼ŒCopilotçº æ­£Pilotçš„logitsä»¥æé«˜ç”Ÿæˆè´¨é‡ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒTransformer Copilotåœ¨å¤šç§ä»»åŠ¡ä¸Šæ€§èƒ½æ˜¾è‘—æå‡ï¼Œæœ€é«˜å¯è¾¾34.5%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16270">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dc8b285387a7377814fd91c61823c36e" align="middle">
<img src="https://picx.zhimg.com/v2-2a9b501d3dadb24d504119fb0b040b2b" align="middle">
<img src="https://picx.zhimg.com/v2-f468fdbe18ce851ce3919356302e8cb0" align="middle">
<img src="https://picx.zhimg.com/v2-df1bcd9cb3db73e27060e121888ef108" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Unifying-Segment-Anything-in-Microscopy-with-Vision-Language-Knowledge"><a href="#Unifying-Segment-Anything-in-Microscopy-with-Vision-Language-Knowledge" class="headerlink" title="Unifying Segment Anything in Microscopy with Vision-Language Knowledge"></a>Unifying Segment Anything in Microscopy with Vision-Language Knowledge</h2><p><strong>Authors:Manyu Li, Ruian He, Zixian Zhang, Chenxi Ma, Weimin Tan, Bo Yan</strong></p>
<p>Accurate segmentation of regions of interest in biomedical images holds substantial value in image analysis. Although several foundation models for biomedical segmentation have currently achieved excellent performance on certain datasets, they typically demonstrate sub-optimal performance on unseen domain data. We owe the deficiency to lack of vision-language knowledge before segmentation. Multimodal Large Language Models (MLLMs) bring outstanding understanding and reasoning capabilities to multimodal tasks, which inspires us to leverage MLLMs to inject Vision-Language Knowledge (VLK), thereby enabling vision models to demonstrate superior generalization capabilities on cross-domain datasets. In this paper, we propose a novel framework that seamlessly uses MLLMs to guide SAM in learning microscopy cross-domain data, unifying Segment Anything in Microscopy, named uLLSAM. Specifically, we propose the Vision-Language Semantic Alignment (VLSA) module, which injects VLK into Segment Anything Model (SAM). We find that after SAM receives global VLK prompts, its performance improves significantly, but there are deficiencies in boundary contour perception. Therefore, we further propose Semantic Boundary Regularization (SBR) to regularize SAM. Our method achieves performance improvements of 11.8% in SA across 9 in-domain microscopy datasets, achieving state-of-the-art performance. Our method also demonstrates improvements of 9.2% in SA across 10 out-of-domain datasets, exhibiting strong generalization capabilities. Code is available at <a target="_blank" rel="noopener" href="https://github.com/ieellee/uLLSAM">https://github.com/ieellee/uLLSAM</a>.</p>
<blockquote>
<p>ç”Ÿç‰©åŒ»å­¦å›¾åƒä¸­æ„Ÿå…´è¶£åŒºåŸŸçš„ç²¾ç¡®åˆ†å‰²åœ¨å›¾åƒåˆ†æä¸­å…·æœ‰é‡è¦ä»·å€¼ã€‚å°½ç®¡ç›®å‰ä¸€äº›ç”Ÿç‰©åŒ»å­¦åˆ†å‰²çš„åŸºç¡€æ¨¡å‹å·²ç»åœ¨æŸäº›æ•°æ®é›†ä¸Šå–å¾—äº†å“è¶Šçš„æ€§èƒ½ï¼Œä½†å®ƒä»¬åœ¨æœªè§åŸŸæ•°æ®ä¸Šçš„è¡¨ç°é€šå¸¸å¹¶ä¸ç†æƒ³ã€‚æˆ‘ä»¬è®¤ä¸ºè¿™ç§ä¸è¶³æ˜¯ç”±äºåˆ†å‰²å‰çš„è§†è§‰è¯­è¨€çŸ¥è¯†ç¼ºä¹æ‰€å¯¼è‡´çš„ã€‚å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸ºå¤šæ¨¡æ€ä»»åŠ¡å¸¦æ¥äº†å‡ºè‰²çš„ç†è§£å’Œæ¨ç†èƒ½åŠ›ï¼Œè¿™å¯å‘æˆ‘ä»¬åˆ©ç”¨MLLMsæ¥æ³¨å…¥è§†è§‰è¯­è¨€çŸ¥è¯†ï¼ˆVLKï¼‰ï¼Œä»è€Œä½¿è§†è§‰æ¨¡å‹åœ¨è·¨åŸŸæ•°æ®é›†ä¸Šå±•ç°å‡ºæ›´å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œæ— ç¼åœ°ä½¿ç”¨MLLMsæ¥æŒ‡å¯¼SAMåœ¨æ˜¾å¾®é•œè·¨åŸŸæ•°æ®ä¸Šçš„å­¦ä¹ ï¼Œç»Ÿä¸€æ˜¾å¾®é•œä¸­çš„ä»»ä½•åˆ†æ®µï¼Œåä¸ºuLLSAMã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†è§†è§‰è¯­è¨€è¯­ä¹‰å¯¹é½ï¼ˆVLSAï¼‰æ¨¡å—ï¼Œè¯¥æ¨¡å—å°†VLKæ³¨å…¥åˆ°åˆ†æ®µä»»ä½•æ¨¡å‹ï¼ˆSAMï¼‰ä¸­ã€‚æˆ‘ä»¬å‘ç°ï¼Œåœ¨SAMæ¥æ”¶å…¨å±€VLKæç¤ºåï¼Œå…¶æ€§èƒ½å¾—åˆ°äº†æ˜¾è‘—æé«˜ï¼Œä½†åœ¨è¾¹ç•Œè½®å»“æ„ŸçŸ¥æ–¹é¢ä»å­˜åœ¨ä¸è¶³ã€‚å› æ­¤ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºè¯­ä¹‰è¾¹ç•Œæ­£åˆ™åŒ–ï¼ˆSBRï¼‰æ¥è§„èŒƒSAMã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨9ä¸ªåŒåŸŸæ˜¾å¾®é•œæ•°æ®é›†ä¸Šçš„SAæ€§èƒ½æé«˜äº†11.8%ï¼Œè¾¾åˆ°æœ€æ–°æ€§èƒ½æ°´å¹³ã€‚æˆ‘ä»¬çš„æ–¹æ³•è¿˜å±•ç¤ºäº†åœ¨10ä¸ªè·¨åŸŸæ•°æ®é›†ä¸Šçš„SAæ€§èƒ½æé«˜äº†9.2%ï¼Œè¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚ä»£ç å¯è®¿é—®äºï¼š<a target="_blank" rel="noopener" href="https://github.com/ieellee/uLLSAM%E3%80%82">https://github.com/ieellee/uLLSAMã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.10769v2">PDF</a> 15 pages, 5 figures</p>
<p><strong>Summary</strong></p>
<p>åŸºäºç”Ÿç‰©åŒ»å­¦å›¾åƒçš„åŒºåŸŸç²¾å‡†åˆ†å‰²åœ¨å›¾åƒåˆ†æä¸­å…·æœ‰é‡è¦ä»·å€¼ã€‚å½“å‰çš„ä¸€äº›åŸºç¡€æ¨¡å‹åœ¨æŸäº›æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜ç§€ï¼Œä½†åœ¨æœªè§åŸŸæ•°æ®ä¸Šçš„è¡¨ç°é€šå¸¸ä¸ä½³ã€‚è¿™ä¸»è¦æ˜¯ç”±äºåˆ†å‰²å‰ç¼ºä¹è§†è§‰è¯­è¨€çŸ¥è¯†ã€‚å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸ºè·¨æ¨¡æ€ä»»åŠ¡å¸¦æ¥äº†å“è¶Šçš„ç†è§£å’Œæ¨ç†èƒ½åŠ›ï¼Œè¿™æ¿€å‘æˆ‘ä»¬åˆ©ç”¨MLLMsæ³¨å…¥è§†è§‰è¯­è¨€çŸ¥è¯†ï¼ˆVLKï¼‰ï¼Œä½¿è§†è§‰æ¨¡å‹åœ¨è·¨åŸŸæ•°æ®é›†ä¸Šè¡¨ç°å‡ºæ›´å‡ºè‰²çš„æ³›åŒ–èƒ½åŠ›ã€‚æœ¬æ–‡æå‡ºä¸€ç§æ–°é¢–æ¡†æ¶uLLSAMï¼Œæ— ç¼åˆ©ç”¨MLLMså¼•å¯¼SAMå­¦ä¹ æ˜¾å¾®é•œè·¨åŸŸæ•°æ®ï¼Œç»Ÿä¸€æ˜¾å¾®åˆ†å‰²ä»»åŠ¡ã€‚é€šè¿‡VLSAæ¨¡å—æ³¨å…¥VLKåˆ°SAMä¸­ã€‚ç ”ç©¶å‘ç°ï¼ŒSAMæ¥å—å…¨å±€VLKæç¤ºåæ€§èƒ½æ˜¾è‘—æå‡ï¼Œä½†åœ¨è¾¹ç•Œè½®å»“æ„ŸçŸ¥æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚å› æ­¤ï¼Œè¿›ä¸€æ­¥æå‡ºè¯­ä¹‰è¾¹ç•Œæ­£åˆ™åŒ–ï¼ˆSBRï¼‰ä»¥ä¼˜åŒ–SAMã€‚è¯¥æ–¹æ³•åœ¨9ä¸ªåŒåŸŸæ˜¾å¾®é•œæ•°æ®é›†ä¸Šçš„åˆ†å‰²ç²¾åº¦æé«˜äº†11.8%ï¼Œè¾¾åˆ°æœ€æ–°æ€§èƒ½æ°´å¹³ï¼›åœ¨10ä¸ªè·¨åŸŸæ•°æ®é›†ä¸Šçš„åˆ†å‰²ç²¾åº¦ä¹Ÿæé«˜äº†9.2%ï¼Œå±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å‡†ç¡®åŒºåŸŸåˆ†å‰²åœ¨ç”Ÿç‰©åŒ»å­¦å›¾åƒåˆ†æä¸­å…·æœ‰é‡è¦ä»·å€¼ã€‚</li>
<li>å½“å‰æ¨¡å‹åœ¨æœªè§åŸŸæ•°æ®ä¸Šçš„è¡¨ç°ä¸ä½³ï¼Œç¼ºä¹è§†è§‰è¯­è¨€çŸ¥è¯†æ˜¯ä¸»è¦åŸå› ä¹‹ä¸€ã€‚</li>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å…·æœ‰å“è¶Šçš„ç†è§£å’Œæ¨ç†èƒ½åŠ›ï¼Œå¯ç”¨äºæ³¨å…¥è§†è§‰è¯­è¨€çŸ¥è¯†ï¼ˆVLKï¼‰ã€‚</li>
<li>è®ºæ–‡æå‡ºuLLSAMæ¡†æ¶ï¼Œåˆ©ç”¨MLLMså¼•å¯¼SAMå­¦ä¹ æ˜¾å¾®é•œè·¨åŸŸæ•°æ®ã€‚</li>
<li>é€šè¿‡VLSAæ¨¡å—ï¼ŒSAMèƒ½å¤Ÿæ¥æ”¶å¹¶åº”ç”¨VLKï¼Œæ˜¾è‘—æå‡æ€§èƒ½ã€‚</li>
<li>SAMåœ¨è¾¹ç•Œè½®å»“æ„ŸçŸ¥æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå› æ­¤æå‡ºè¯­ä¹‰è¾¹ç•Œæ­£åˆ™åŒ–ï¼ˆSBRï¼‰è¿›è¡Œä¼˜åŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.10769">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cfdf4201081801659d5b64aa5eca0e04" align="middle">
<img src="https://picx.zhimg.com/v2-3e86a2e66d86fd01193d45afc130cbc2" align="middle">
<img src="https://picx.zhimg.com/v2-e2ffcd734b02ce0632461c1fafd3b634" align="middle">
<img src="https://picx.zhimg.com/v2-c6dfd70a11ad46d7b81ff6a4394067ff" align="middle">
<img src="https://picx.zhimg.com/v2-9ac0b68b512ef8fe15755404147431d1" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="The-Empty-Chair-Using-LLMs-to-Raise-Missing-Perspectives-in-Policy-Deliberations"><a href="#The-Empty-Chair-Using-LLMs-to-Raise-Missing-Perspectives-in-Policy-Deliberations" class="headerlink" title="The Empty Chair: Using LLMs to Raise Missing Perspectives in Policy Deliberations"></a>The Empty Chair: Using LLMs to Raise Missing Perspectives in Policy Deliberations</h2><p><strong>Authors:Suyash Fulay, Dimitra Dimitrakopoulou, Deb Roy</strong></p>
<p>Deliberation is essential to well-functioning democracies, yet physical, economic, and social barriers often exclude certain groups, reducing representativeness and contributing to issues like group polarization. In this work, we explore the use of large language model (LLM) personas to introduce missing perspectives in policy deliberations. We develop and evaluate a tool that transcribes conversations in real-time and simulates input from relevant but absent stakeholders. We deploy this tool in a 19-person student citizensâ€™ assembly on campus sustainability. Participants and facilitators found that the tool was useful to spark new discussions and surfaced valuable perspectives they had not previously considered. However, they also raised skepticism about the ability of LLMs to accurately characterize the perspectives of different groups, especially ones that are already underrepresented. Overall, this case study highlights that while AI personas can usefully surface new perspectives and prompt discussion in deliberative settings, their successful deployment depends on clarifying their limitations and emphasizing that they complement rather than replace genuine participation.</p>
<blockquote>
<p>å®¡è®®åœ¨æ°‘ä¸»ç¤¾ä¼šä¸­æ‰®æ¼”ç€è‡³å…³é‡è¦çš„è§’è‰²ï¼Œä½†ç”±äºç‰©ç†ã€ç»æµå’Œç¤¾ä¼šæ–¹é¢çš„éšœç¢ï¼ŒæŸäº›ç¾¤ä½“å¸¸å¸¸è¢«æ’é™¤åœ¨å¤–ï¼Œè¿™ä¸ä»…é™ä½äº†ä»£è¡¨æ€§ï¼Œè¿˜å¼•å‘äº†ç¾¤ä½“æåŒ–ç­‰é—®é¢˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ¢ç´¢ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰äººæ ¼æ¥ä¸ºæ”¿ç­–å®¡è®®å¼•å…¥ç¼ºå¤±çš„è§‚ç‚¹ã€‚æˆ‘ä»¬å¼€å‘å¹¶è¯„ä¼°äº†ä¸€ç§å·¥å…·ï¼Œè¯¥å·¥å…·èƒ½å¤Ÿå®æ—¶è½¬å½•å¯¹è¯ï¼Œå¹¶æ¨¡æ‹Ÿæ¥è‡ªç›¸å…³ä½†ç¼ºå¸­çš„åˆ©ç›Šç›¸å…³è€…çš„è¾“å…¥ã€‚æˆ‘ä»¬åœ¨æ ¡å›­å¯æŒç»­æ€§é—®é¢˜çš„19åå­¦ç”Ÿå…¬æ°‘å¤§ä¼šä¸Šä½¿ç”¨äº†è¿™ä¸€å·¥å…·ã€‚å‚ä¸è€…å’Œåè°ƒå‘˜å‘ç°è¯¥å·¥å…·å¯¹äºæ¿€å‘æ–°è®¨è®ºå’Œæ­ç¤ºä»–ä»¬ä¹‹å‰æ²¡æœ‰è€ƒè™‘åˆ°çš„æœ‰ä»·å€¼çš„è§‚ç‚¹éå¸¸æœ‰ç”¨ã€‚ç„¶è€Œï¼Œä»–ä»¬ä¹Ÿæå‡ºäº†å¯¹LLMå‡†ç¡®åˆ»ç”»ä¸åŒç¾¤ä½“è§‚ç‚¹èƒ½åŠ›çš„æ€€ç–‘ï¼Œå°¤å…¶æ˜¯å¯¹é‚£äº›å·²ç»ä»£è¡¨æ€§ä¸è¶³çš„ç¾¤ä½“ã€‚æ€»ä½“è€Œè¨€ï¼Œè¿™ä¸ªæ¡ˆä¾‹ç ”ç©¶çªå‡ºäº†è™½ç„¶äººå·¥æ™ºèƒ½äººæ ¼æœ‰åŠ©äºæ­ç¤ºæ–°çš„è§‚ç‚¹å¹¶ä¿ƒè¿›è®¨è®ºï¼Œä½†å…¶æˆåŠŸéƒ¨ç½²å–å†³äºæ˜ç¡®å…¶å±€é™æ€§ï¼Œå¹¶å¼ºè°ƒå®ƒä»¬åªæ˜¯ä½œä¸ºçœŸæ­£å‚ä¸çš„è¡¥å……ï¼Œè€Œä¸æ˜¯æ›¿ä»£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13812v2">PDF</a> 39th Conference on Neural Information Processing Systems (NeurIPS 2025) Workshop: PersonaLLM: Workshop on LLM Persona Modeling</p>
<p><strong>Summary</strong><br>å†³ç­–è¿‡ç¨‹ä¸­çš„è®¨è®ºä¸è¾©è®ºå¯¹äºæ°‘ä¸»åˆ¶åº¦è‡³å…³é‡è¦ï¼Œä½†ç‰©ç†ã€ç»æµå’Œç¤¾ä¼šç­‰éšœç¢ä¼šæ’é™¤æŸäº›ç¾¤ä½“å‚ä¸è®¨è®ºã€‚æœ¬æ–‡é€šè¿‡ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨¡æ‹Ÿç›¸å…³ä½†ç¼ºå¸­åˆ©ç›Šç›¸å…³è€…çš„è¾“å…¥ï¼Œæ¢ç´¢å¼•å…¥ç¼ºå¤±è§†è§’çš„æ”¿ç­–è®¨è®ºæ–¹æ³•ã€‚åœ¨æ ¡å›­å¯æŒç»­æ€§è®®é¢˜çš„å­¦ç”Ÿå…¬æ°‘å¤§ä¼šä¸Šè¿›è¡Œäº†å®åœ°æµ‹è¯•ï¼Œå‘ç°è¯¥å·¥å…·æœ‰åŠ©äºæ¿€å‘æ–°è®¨è®ºå’Œå‘ˆç°ä¹‹å‰æœªè€ƒè™‘è¿‡çš„è§‚ç‚¹ï¼Œä½†å¯¹LLMè¡¨å¾ä¸åŒç¾¤ä½“è§†è§’çš„å‡†ç¡®æ€§æœ‰æ‰€è´¨ç–‘ã€‚æ€»çš„æ¥è¯´ï¼ŒAIå·¥å…·å¯ä»¥åœ¨å®¡è®®å¼åœºåˆè¡¥å……æ–°è§†è§’å’Œæ¿€å‘è®¨è®ºï¼Œä½†éœ€æ˜ç¡®å…¶å±€é™æ€§å¹¶å¼ºè°ƒå…¶ä¸ºè¾…åŠ©è€Œéå–ä»£å®é™…å‚ä¸çš„ä½œç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¼•å…¥æ”¿ç­–è®¨è®ºä¸­çš„ç¼ºå¤±è§†è§’æ–¹é¢å…·æœ‰é‡è¦æ½œåŠ›ã€‚</li>
<li>é€šè¿‡æ¨¡æ‹Ÿç›¸å…³ä½†ç¼ºå¸­åˆ©ç›Šç›¸å…³è€…çš„è¾“å…¥ï¼Œå¯ä»¥å¢åŠ è®¨è®ºçš„å…¨é¢æ€§å’Œä»£è¡¨æ€§ã€‚</li>
<li>åœ¨å­¦ç”Ÿå…¬æ°‘å¤§ä¼šä¸Šçš„å®åœ°æµ‹è¯•è¡¨æ˜ï¼Œè¯¥å·¥å…·èƒ½å¤Ÿæ¿€å‘æ–°è®¨è®ºå¹¶æ­ç¤ºæœ‰ä»·å€¼çš„è§†è§’ã€‚</li>
<li>å­˜åœ¨å¯¹LLMå‡†ç¡®è¡¨å¾ä¸åŒç¾¤ä½“è§†è§’çš„è´¨ç–‘ï¼Œç‰¹åˆ«æ˜¯å¯¹é‚£äº›å·²ç»ç¼ºä¹ä»£è¡¨æ€§çš„ç¾¤ä½“ã€‚</li>
<li>AIå·¥å…·åœ¨å®¡è®®å¼åœºåˆä¸­çš„æˆåŠŸéƒ¨ç½²å–å†³äºå¯¹å…¶å±€é™æ€§çš„æ˜ç¡®è¯´æ˜ã€‚</li>
<li>AIå·¥å…·åº”è¢«è§†ä¸ºè¾…åŠ©å·¥å…·ï¼Œè€Œéå–ä»£å®é™…å‚ä¸çš„ä¸»è¦æ‰‹æ®µã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13812">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2880fd380b3adcfae861a533f319f9de" align="middle">
<img src="https://picx.zhimg.com/v2-6b05b174ad21d6edd90633a7bb6cfa5e" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-11-18/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-11-18/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-18/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-9af9c660188a1cf64857328efc238ec7" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-18  DocLens  A Tool-Augmented Multi-Agent Framework for Long Visual Document Understanding
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-11-18
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-18/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-67076cbe9423fc7de80b51dd82a83d08" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-18  Human-AI collaborative autonomous synthesis with pulsed laser deposition for remote epitaxy
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-11-18
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">33297.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
