<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-18  GraphPilot Grounded Scene Graph Conditioning for Language-Based Autonomous Driving">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-6fef0bf34b2381e83a2765d26ff30118')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-18
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-18
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    82 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-11-18-æ›´æ–°"><a href="#2025-11-18-æ›´æ–°" class="headerlink" title="2025-11-18 æ›´æ–°"></a>2025-11-18 æ›´æ–°</h1><h2 id="GraphPilot-Grounded-Scene-Graph-Conditioning-for-Language-Based-Autonomous-Driving"><a href="#GraphPilot-Grounded-Scene-Graph-Conditioning-for-Language-Based-Autonomous-Driving" class="headerlink" title="GraphPilot: Grounded Scene Graph Conditioning for Language-Based Autonomous Driving"></a>GraphPilot: Grounded Scene Graph Conditioning for Language-Based Autonomous Driving</h2><p><strong>Authors:Fabian Schmidt, Markus Enzweiler, Abhinav Valada</strong></p>
<p>Vision-language models have recently emerged as promising planners for autonomous driving, where success hinges on topology-aware reasoning over spatial structure and dynamic interactions from multimodal input. However, existing models are typically trained without supervision that explicitly encodes these relational dependencies, limiting their ability to infer how agents and other traffic entities influence one another from raw sensor data. In this work, we bridge this gap with a novel model-agnostic method that conditions language-based driving models on structured relational context in the form of traffic scene graphs. We serialize scene graphs at various abstraction levels and formats, and incorporate them into the models via structured prompt templates, enabling a systematic analysis of when and how relational supervision is most beneficial. Extensive evaluations on the public LangAuto benchmark show that scene graph conditioning of state-of-the-art approaches yields large and persistent improvement in driving performance. Notably, we observe up to a 15.6% increase in driving score for LMDrive and 17.5% for BEVDriver, indicating that models can better internalize and ground relational priors through scene graph-conditioned training, even without requiring scene graph input at test-time. Code, fine-tuned models, and our scene graph dataset are publicly available at <a target="_blank" rel="noopener" href="https://github.com/iis-esslingen/GraphPilot">https://github.com/iis-esslingen/GraphPilot</a>.</p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹æœ€è¿‘ä½œä¸ºè‡ªåŠ¨é©¾é©¶çš„æœ‰å‰é€”çš„è§„åˆ’å™¨è€Œå‡ºç°ï¼Œå…¶æˆåŠŸå–å†³äºå¯¹å¤šæ¨¡æ€è¾“å…¥çš„ç©ºé—´ç»“æ„å’ŒåŠ¨æ€äº¤äº’çš„æ‹“æ‰‘æ„ŸçŸ¥æ¨ç†ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ¨¡å‹é€šå¸¸åœ¨æ²¡æœ‰æ˜ç¡®ç¼–ç è¿™äº›å…³ç³»ä¾èµ–æ€§çš„ç›‘ç£ä¸‹è¿›è¡Œè®­ç»ƒï¼Œè¿™é™åˆ¶äº†å®ƒä»¬ä»åŸå§‹ä¼ æ„Ÿå™¨æ•°æ®ä¸­æ¨æ–­å‡ºä»£ç†å’Œå…¶ä»–äº¤é€šå®ä½“å¦‚ä½•ç›¸äº’å½±å“çš„èƒ½åŠ›ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡ä¸€ç§æ–°å‹æ¨¡å‹æ— å…³çš„æ–¹æ³•å¡«è¡¥äº†è¿™ä¸€ç©ºç™½ï¼Œè¯¥æ–¹æ³•ä»¥äº¤é€šåœºæ™¯å›¾çš„å½¢å¼ä¸ºåŸºäºè¯­è¨€çš„é©¾é©¶æ¨¡å‹è®¾å®šç»“æ„åŒ–å…³ç³»ä¸Šä¸‹æ–‡ã€‚æˆ‘ä»¬ä»¥å„ç§æŠ½è±¡å±‚æ¬¡å’Œæ ¼å¼åºåˆ—åŒ–åœºæ™¯å›¾ï¼Œå¹¶é€šè¿‡ç»“æ„åŒ–æç¤ºæ¨¡æ¿å°†å®ƒä»¬çº³å…¥æ¨¡å‹ï¼Œä»è€Œèƒ½å¤Ÿç³»ç»Ÿåœ°åˆ†æå…³ç³»ç›‘ç£ä½•æ—¶ä»¥åŠå¦‚ä½•å¸¦æ¥æœ€å¤§çš„ç›Šå¤„ã€‚åœ¨å…¬å…±LangAutoåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›è¯„ä¼°æ˜¾ç¤ºï¼Œé€šè¿‡åœºæ™¯å›¾è°ƒèŠ‚çš„å°–ç«¯æ–¹æ³•çš„é©¾é©¶æ€§èƒ½å¾—åˆ°äº†å¾ˆå¤§ä¸”æŒç»­çš„æå‡ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°LMDriveçš„é©¾é©¶è¯„åˆ†æé«˜äº†15.6%ï¼ŒBEVDriveræé«˜äº†17.5%ï¼Œè¿™è¡¨æ˜æ¨¡å‹å¯ä»¥é€šè¿‡åœºæ™¯å›¾è°ƒèŠ‚çš„è®­ç»ƒæ›´å¥½åœ°å†…åŒ–å¹¶éªŒè¯å…³ç³»å…ˆéªŒçŸ¥è¯†ï¼Œå³ä½¿åœ¨æµ‹è¯•æ—¶ä¸éœ€è¦åœºæ™¯å›¾è¾“å…¥ã€‚ä»£ç ã€å¾®è°ƒæ¨¡å‹å’Œæˆ‘ä»¬çš„åœºæ™¯å›¾æ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/iis-esslingen/GraphPilot%E4%B8%8A%E5%85%AC%E5%BC%80%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/iis-esslingen/GraphPilotä¸Šå…¬å¼€è·å¾—ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.11266v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºè§†è§‰çš„è¯­è¨€æ¨¡å‹åœ¨è‡ªåŠ¨é©¾é©¶ä¸­æœ‰ç€å¹¿é˜”çš„åº”ç”¨å‰æ™¯ï¼ŒæˆåŠŸä¾èµ–äºå¯¹ç©ºé—´ç»“æ„å’ŒåŠ¨æ€äº¤äº’çš„å¤šæ¨¡æ€è¾“å…¥çš„æ‹“æ‰‘æ„ŸçŸ¥æ¨ç†ã€‚ç„¶è€Œï¼Œç°æœ‰æ¨¡å‹é€šå¸¸åœ¨æ²¡æœ‰æ˜ç¡®ç¼–ç è¿™äº›å…³ç³»ä¾èµ–çš„ç›‘ç®¡ä¸‹è¿›è¡Œè®­ç»ƒï¼Œé™åˆ¶äº†å®ƒä»¬ä»åŸå§‹ä¼ æ„Ÿå™¨æ•°æ®ä¸­æ¨æ–­å‡ºå…¶ä»–äº¤é€šå®ä½“ä¹‹é—´å¦‚ä½•ç›¸äº’å½±å“çš„èƒ½åŠ›ã€‚æœ¬ç ”ç©¶é€šè¿‡ä¸€ç§æ–°çš„æ¨¡å‹æ— å…³æ–¹æ³•ï¼Œä»¥äº¤é€šåœºæ™¯å›¾çš„å½¢å¼ï¼Œå°†ç»“æ„åŒ–å…³ç³»ä¸Šä¸‹æ–‡çº³å…¥è¯­è¨€é©¾é©¶æ¨¡å‹çš„è€ƒé‡ä¸­ã€‚é€šè¿‡åœ¨ä¸åŒæŠ½è±¡å±‚æ¬¡å’Œæ ¼å¼ä¸‹åºåˆ—åŒ–åœºæ™¯å›¾å¹¶å°†å…¶èå…¥æ¨¡å‹ï¼Œæˆ‘ä»¬å¾—ä»¥ç³»ç»Ÿåœ°åˆ†æå…³ç³»ç›‘ç£åœ¨ä½•æ—¶ä»¥åŠå¦‚ä½•å¸¦æ¥æœ€å¤§çš„ç›Šå¤„ã€‚åœ¨å…¬å…±LangAutoåŸºå‡†æµ‹è¯•ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œä½¿ç”¨åœºæ™¯å›¾è°ƒèŠ‚çš„æœ€å…ˆè¿›æ–¹æ³•å¤§å¤§æé«˜äº†é©¾é©¶æ€§èƒ½ã€‚ç‰¹åˆ«æ˜¯æˆ‘ä»¬åœ¨LMDriveå’ŒBEVDriverä¸­è§‚å¯Ÿåˆ°é©¾é©¶è¯„åˆ†åˆ†åˆ«æé«˜äº†æœ€é«˜è¾¾15.6%å’Œ17.5%ï¼Œè¿™æ˜¾ç¤ºå‡ºæ¨¡å‹å¯ä»¥é€šè¿‡åœºæ™¯å›¾è°ƒèŠ‚è®­ç»ƒæ¥æ›´å¥½åœ°å†…éƒ¨åŒ–å’Œæ¥åœ°å…³ç³»å…ˆéªŒçŸ¥è¯†ï¼Œå³ä½¿åœ¨æµ‹è¯•æ—¶ä¸ä½¿ç”¨åœºæ™¯å›¾è¾“å…¥ä¹Ÿèƒ½å®ç°è¿™ä¸€æ•ˆæœã€‚æˆ‘ä»¬çš„ä»£ç ã€å¾®è°ƒæ¨¡å‹å’Œåœºæ™¯å›¾æ•°æ®é›†å¯åœ¨å…¬å¼€ç½‘ç«™ä¸Šè·å–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸå…·æœ‰åº”ç”¨æ½œåŠ›ï¼Œä½†éœ€è¦å¤„ç†ç©ºé—´ç»“æ„å’ŒåŠ¨æ€äº¤äº’çš„æ‹“æ‰‘æ„ŸçŸ¥æ¨ç†ã€‚</li>
<li>ç°æœ‰æ¨¡å‹ç¼ºä¹æ˜ç¡®ç¼–ç å…³ç³»ä¾èµ–çš„è®­ç»ƒï¼Œé™åˆ¶äº†ä»ä¼ æ„Ÿå™¨æ•°æ®ä¸­æ¨æ–­å®ä½“é—´ç›¸äº’å½±å“çš„èƒ½åŠ›ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ¨¡å‹æ— å…³æ–¹æ³•ï¼Œé€šè¿‡äº¤é€šåœºæ™¯å›¾çš„ç»“æ„åŒ–å…³ç³»ä¸Šä¸‹æ–‡æ¥å¢å¼ºè¯­è¨€é©¾é©¶æ¨¡å‹ã€‚</li>
<li>é€šè¿‡åºåˆ—åŒ–åœºæ™¯å›¾çš„ä¸åŒæŠ½è±¡å±‚æ¬¡å’Œæ ¼å¼ï¼Œç³»ç»Ÿåˆ†æäº†å…³ç³»ç›‘ç£çš„æ•ˆç›Šã€‚</li>
<li>åœ¨LangAutoåŸºå‡†æµ‹è¯•ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œåœºæ™¯å›¾è°ƒèŠ‚æ˜¾è‘—æé«˜äº†é©¾é©¶æ€§èƒ½ã€‚</li>
<li>æ¨¡å‹èƒ½åœ¨ä¸ä¾èµ–åœºæ™¯å›¾è¾“å…¥çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡åœºæ™¯å›¾è°ƒèŠ‚è®­ç»ƒå†…åŒ–å…³ç³»å…ˆéªŒçŸ¥è¯†ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.11266">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f739fe285885c3923d98e8822268fd37" align="middle">
<img src="https://picx.zhimg.com/v2-0b10ca038fa8d5b47c6c89f125e6ac41" align="middle">
<img src="https://picx.zhimg.com/v2-6c02de10baa09012b5845131d944b025" align="middle">
<img src="https://picx.zhimg.com/v2-4104cc17a937b73422db7e0e451e6718" align="middle">
<img src="https://picx.zhimg.com/v2-e49dc3e13b28095998236a089f3744d0" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Parameter-Efficient-MoE-LoRA-for-Few-Shot-Multi-Style-Editing"><a href="#Parameter-Efficient-MoE-LoRA-for-Few-Shot-Multi-Style-Editing" class="headerlink" title="Parameter-Efficient MoE LoRA for Few-Shot Multi-Style Editing"></a>Parameter-Efficient MoE LoRA for Few-Shot Multi-Style Editing</h2><p><strong>Authors:Cong Cao, Yujie Xu, Xiaodong Xu</strong></p>
<p>In recent years, image editing has garnered growing attention. However, general image editing models often fail to produce satisfactory results when confronted with new styles. The challenge lies in how to effectively fine-tune general image editing models to new styles using only a limited amount of paired data. To address this issue, this paper proposes a novel few-shot style editing framework. For this task, we construct a benchmark dataset that encompasses five distinct styles. Correspondingly, we propose a parameter-efficient multi-style Mixture-of-Experts Low-Rank Adaptation (MoE LoRA) with style-specific and style-shared routing mechanisms for jointly fine-tuning multiple styles. The style-specific routing ensures that different styles do not interfere with one another, while the style-shared routing adaptively allocates shared MoE LoRAs to learn common patterns. Our MoE LoRA can automatically determine the optimal ranks for each layer through a novel metric-guided approach that estimates the importance score of each single-rank component. Additionally, we explore the optimal location to insert LoRA within the Diffusion in Transformer (DiT) model and integrate adversarial learning and flow matching to guide the diffusion training process. Experimental results demonstrate that our proposed method outperforms existing state-of-the-art approaches with significantly fewer LoRA parameters.</p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œå›¾åƒç¼–è¾‘é¢†åŸŸè¶Šæ¥è¶Šå—åˆ°å…³æ³¨ã€‚ç„¶è€Œï¼Œå½“é¢å¯¹æ–°çš„é£æ ¼æ—¶ï¼Œé€šç”¨å›¾åƒç¼–è¾‘æ¨¡å‹å¾€å¾€æ— æ³•äº§ç”Ÿä»¤äººæ»¡æ„çš„ç»“æœã€‚æŒ‘æˆ˜åœ¨äºå¦‚ä½•ä½¿ç”¨æœ‰é™çš„é…å¯¹æ•°æ®æœ‰æ•ˆåœ°å¯¹é€šç”¨å›¾åƒç¼–è¾‘æ¨¡å‹è¿›è¡Œå¾®è°ƒä»¥é€‚åº”æ–°çš„é£æ ¼ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„å°‘æ ·æœ¬é£æ ¼ç¼–è¾‘æ¡†æ¶ã€‚ä¸ºæ­¤ä»»åŠ¡ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåŒ…å«äº”ç§ä¸åŒé£æ ¼çš„åŸºå‡†æ•°æ®é›†ã€‚ç›¸åº”åœ°ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å‚æ•°é«˜æ•ˆçš„æ··åˆä¸“å®¶ä½ç§©é€‚åº”ï¼ˆMoE LoRAï¼‰æ–¹æ³•ï¼Œå…·æœ‰é£æ ¼ç‰¹å®šå’Œé£æ ¼å…±äº«è·¯ç”±æœºåˆ¶ï¼Œä»¥è”åˆå¾®è°ƒå¤šç§é£æ ¼ã€‚é£æ ¼ç‰¹å®šçš„è·¯ç”±æœºåˆ¶ç¡®ä¿ä¸åŒçš„é£æ ¼ä¸ä¼šç›¸äº’å¹²æ‰°ï¼Œè€Œé£æ ¼å…±äº«çš„è·¯ç”±æœºåˆ¶åˆ™è‡ªé€‚åº”åœ°åˆ†é…å…±äº«çš„MoE LoRAæ¥å­¦ä¹ å¸¸è§æ¨¡å¼ã€‚æˆ‘ä»¬çš„MoE LoRAå¯ä»¥é€šè¿‡ä¸€ç§æ–°çš„åº¦é‡å¼•å¯¼æ–¹æ³•è‡ªåŠ¨ç¡®å®šæ¯å±‚çš„æœ€ä½³ç§©æ¬¡ï¼Œè¯¥æ–¹æ³•ä¼°è®¡æ¯ä¸ªå•ä¸€ç§©æ¬¡ç»„ä»¶çš„é‡è¦æ€§å¾—åˆ†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ¢ç´¢äº†åœ¨æ‰©æ•£è½¬æ¢å™¨ï¼ˆDiTï¼‰æ¨¡å‹ä¸­æœ€ä¼˜æ’å…¥LoRAçš„ä½ç½®ï¼Œå¹¶æ•´åˆå¯¹æŠ—å­¦ä¹ å’ŒæµåŒ¹é…æ¥å¼•å¯¼æ‰©æ•£è®­ç»ƒè¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•åœ¨è¾ƒå°‘çš„LoRAå‚æ•°ä¸‹ï¼Œè¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.11236v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸå›¾åƒç¼–è¾‘é¢†åŸŸå¤‡å—å…³æ³¨ï¼Œä½†ä¸€èˆ¬å›¾åƒç¼–è¾‘æ¨¡å‹é¢å¯¹æ–°é£æ ¼æ—¶æ•ˆæœä¸ä½³ã€‚é’ˆå¯¹ä½¿ç”¨æœ‰é™é…å¯¹æ•°æ®å¯¹æ–°é£æ ¼è¿›è¡Œå¾®è°ƒçš„é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºä¸€ç§æ–°å‹å°‘æ ·æœ¬é£æ ¼ç¼–è¾‘æ¡†æ¶ã€‚æ„å»ºåŒ…å«äº”ç§ä¸åŒé£æ ¼çš„åŸºå‡†æ•°æ®é›†ï¼Œå¹¶æå‡ºå‚æ•°é«˜æ•ˆçš„å¤šé£æ ¼æ··åˆä¸“å®¶ä½ç§©é€‚åº”ï¼ˆMoE LoRAï¼‰æ–¹æ³•ï¼ŒåŒ…å«é£æ ¼ç‰¹å®šå’Œé£æ ¼å…±äº«è·¯ç”±æœºåˆ¶ï¼Œç”¨äºè”åˆå¾®è°ƒå¤šç§é£æ ¼ã€‚é£æ ¼ç‰¹å®šè·¯ç”±ç¡®ä¿ä¸åŒé£æ ¼äº’ä¸å¹²æ‰°ï¼Œè€Œé£æ ¼å…±äº«è·¯ç”±åˆ™è‡ªé€‚åº”åˆ†é…å…±äº«MoE LoRAå­¦ä¹ å…±åŒæ¨¡å¼ã€‚MoE LoRAå¯è‡ªåŠ¨ç¡®å®šæ¯å±‚çš„æœ€ä½³ç§©æ¬¡ï¼Œé€šè¿‡æ–°å‹æŒ‡æ ‡å¼•å¯¼æ–¹æ³•ä¼°ç®—å„å•ç§©æˆåˆ†çš„é‡è¦æ€§å¾—åˆ†ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æ¢è®¨åœ¨æ‰©æ•£å˜å‹å™¨æ¨¡å‹ä¸­çš„æœ€ä½³ä½ç½®æ’å…¥LoRAï¼Œå¹¶ç»“åˆå¯¹æŠ—å­¦ä¹ ä¸æµåŒ¹é…å¼•å¯¼æ‰©æ•£è®­ç»ƒè¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€ææ–¹æ³•ä¼˜äºç°æœ‰å…ˆè¿›æŠ€æœ¯ï¼Œä¸”LoRAå‚æ•°æ›´å°‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å›¾åƒç¼–è¾‘é¢†åŸŸé¢ä¸´å¯¹æ–°é£æ ¼æŒ‘æˆ˜çš„é—®é¢˜ã€‚</li>
<li>è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„å°‘æ ·æœ¬é£æ ¼ç¼–è¾‘æ¡†æ¶æ¥è§£å†³è¯¥é—®é¢˜ã€‚</li>
<li>æ„å»ºäº†ä¸€ä¸ªåŒ…å«äº”ç§ä¸åŒé£æ ¼çš„åŸºå‡†æ•°æ®é›†ã€‚</li>
<li>æå‡ºäº†å‚æ•°é«˜æ•ˆçš„å¤šé£æ ¼æ··åˆä¸“å®¶ä½ç§©é€‚åº”ï¼ˆMoE LoRAï¼‰æ–¹æ³•ã€‚</li>
<li>MoE LoRAå…·æœ‰é£æ ¼ç‰¹å®šå’Œé£æ ¼å…±äº«è·¯ç”±æœºåˆ¶ï¼Œç”¨äºè”åˆå¾®è°ƒå¤šç§é£æ ¼ã€‚</li>
<li>MoE LoRAèƒ½è‡ªåŠ¨ç¡®å®šæ¯å±‚çš„æœ€ä½³ç§©æ¬¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.11236">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9338992f74497d285368a910c03581fe" align="middle">
<img src="https://picx.zhimg.com/v2-5f3daa7682d2e441d6c18ac75f7d072a" align="middle">
<img src="https://picx.zhimg.com/v2-e22f604f719f1c12ccde69086c56aa1c" align="middle">
<img src="https://picx.zhimg.com/v2-e5116f5fabc508293cef4a7aeb1a63f9" align="middle">
<img src="https://picx.zhimg.com/v2-c9185fcb1eb8e0a3e78c9a714d0b9c6d" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="RealisticDreamer-Guidance-Score-Distillation-for-Few-shot-Gaussian-Splatting"><a href="#RealisticDreamer-Guidance-Score-Distillation-for-Few-shot-Gaussian-Splatting" class="headerlink" title="RealisticDreamer: Guidance Score Distillation for Few-shot Gaussian Splatting"></a>RealisticDreamer: Guidance Score Distillation for Few-shot Gaussian Splatting</h2><p><strong>Authors:Ruocheng Wu, Haolan He, Yufei Wang, Zhihao Li, Bihan Wen</strong></p>
<p>3D Gaussian Splatting (3DGS) has recently gained great attention in the 3D scene representation for its high-quality real-time rendering capabilities. However, when the input comprises sparse training views, 3DGS is prone to overfitting, primarily due to the lack of intermediate-view supervision. Inspired by the recent success of Video Diffusion Models (VDM), we propose a framework called Guidance Score Distillation (GSD) to extract the rich multi-view consistency priors from pretrained VDMs. Building on the insights from Score Distillation Sampling (SDS), GSD supervises rendered images from multiple neighboring views, guiding the Gaussian splatting representation towards the generative direction of VDM. However, the generative direction often involves object motion and random camera trajectories, making it challenging for direct supervision in the optimization process. To address this problem, we introduce an unified guidance form to correct the noise prediction result of VDM. Specifically, we incorporate both a depth warp guidance based on real depth maps and a guidance based on semantic image features, ensuring that the score update direction from VDM aligns with the correct camera pose and accurate geometry. Experimental results show that our method outperforms existing approaches across multiple datasets.</p>
<blockquote>
<p>3Dé«˜æ–¯èåˆï¼ˆ3DGSï¼‰å› å…¶é«˜è´¨é‡å®æ—¶æ¸²æŸ“èƒ½åŠ›è€Œåœ¨3Dåœºæ™¯è¡¨ç¤ºä¸­å—åˆ°å¹¿æ³›å…³æ³¨ã€‚ç„¶è€Œï¼Œå½“è¾“å…¥åŒ…å«ç¨€ç–è®­ç»ƒè§†å›¾æ—¶ï¼Œ3DGSå®¹æ˜“è¿‡åº¦æ‹Ÿåˆï¼Œè¿™ä¸»è¦æ˜¯ç”±äºç¼ºä¹ä¸­é—´è§†å›¾ç›‘ç£ã€‚å—è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ˆVDMï¼‰è¿‘æœŸæˆåŠŸçš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºæŒ‡å¯¼å¾—åˆ†è’¸é¦ï¼ˆGSDï¼‰çš„æ¡†æ¶ï¼Œç”¨äºä»é¢„è®­ç»ƒçš„VDMsä¸­æå–ä¸°å¯Œçš„å¤šè§†å›¾ä¸€è‡´æ€§å…ˆéªŒã€‚åŸºäºå¾—åˆ†è’¸é¦é‡‡æ ·ï¼ˆSDSï¼‰çš„è§è§£ï¼ŒGSDç›‘ç£æ¥è‡ªå¤šä¸ªç›¸é‚»è§†å›¾çš„æ¸²æŸ“å›¾åƒï¼Œå¼•å¯¼é«˜æ–¯èåˆè¡¨ç¤ºæœå‘VDMçš„ç”Ÿæˆæ–¹å‘ã€‚ç„¶è€Œï¼Œç”Ÿæˆæ–¹å‘é€šå¸¸æ¶‰åŠå¯¹è±¡è¿åŠ¨å’Œéšæœºç›¸æœºè½¨è¿¹ï¼Œè¿™ä½¿å¾—åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­è¿›è¡Œç›´æ¥ç›‘ç£å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ç»Ÿä¸€çš„æŒ‡å¯¼å½¢å¼æ¥çº æ­£VDMçš„å™ªå£°é¢„æµ‹ç»“æœã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ç»“åˆäº†åŸºäºçœŸå®æ·±åº¦å›¾çš„æ·±åº¦warpæŒ‡å¯¼ä»¥åŠåŸºäºè¯­ä¹‰å›¾åƒç‰¹å¾çš„æŒ‡å¯¼ï¼Œç¡®ä¿VDMçš„å¾—åˆ†æ›´æ–°æ–¹å‘ä¸æ­£ç¡®çš„ç›¸æœºå§¿æ€å’Œå‡†ç¡®å‡ ä½•å¯¹é½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„è¡¨ç°å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.11213v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºä¸‰ç»´é«˜æ–¯å–·æº…ï¼ˆ3DGSï¼‰å’Œé¢„è®­ç»ƒçš„è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ˆVDMï¼‰çš„æ¡†æ¶ï¼Œåä¸ºæŒ‡å¯¼è¯„åˆ†è’¸é¦ï¼ˆGSDï¼‰ã€‚è¯¥æ¡†æ¶æ—¨åœ¨è§£å†³å½“è¾“å…¥åŒ…å«ç¨€ç–è®­ç»ƒè§†å›¾æ—¶ï¼Œç”±äºç¼ºä¹ä¸­é—´è§†å›¾ç›‘ç£è€Œå¯¼è‡´è¿‡æ‹Ÿåˆçš„é—®é¢˜ã€‚æ–‡ç« é¦–å…ˆå¼•å‡ºç›®å‰é¢ä¸´çš„é—®é¢˜å¹¶é˜è¿°å¯èƒ½çš„æŒ‘æˆ˜ï¼Œå¦‚å¯¹è±¡çš„è¿åŠ¨éšæœºå’Œæ‘„åƒæœºè½¨è¿¹å˜åŒ–å¯¼è‡´çš„éš¾é¢˜ï¼Œæå‡ºå¼•å…¥ç»Ÿä¸€çš„æŒ‡å¯¼å½¢å¼æ¥çº æ­£VDMçš„å™ªå£°é¢„æµ‹ç»“æœã€‚é€šè¿‡ç»“åˆåŸºäºçœŸå®æ·±åº¦å›¾çš„æ·±åº¦warpæŒ‡å¯¼å’ŒåŸºäºè¯­ä¹‰å›¾åƒç‰¹å¾çš„æŒ‡å¯¼ï¼Œç¡®ä¿è¯„åˆ†æ›´æ–°æ–¹å‘ä¸æ­£ç¡®çš„ç›¸æœºå§¿æ€å’Œå‡†ç¡®çš„å‡ ä½•ç»“æ„ä¿æŒä¸€è‡´ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¼•å…¥æŒ‡å¯¼è¯„åˆ†è’¸é¦ï¼ˆGSDï¼‰æ¡†æ¶ï¼Œç”¨äºè§£å†³ä¸‰ç»´é«˜æ–¯å–·æº…ï¼ˆ3DGSï¼‰åœ¨ç¨€ç–è®­ç»ƒè§†å›¾ä¸‹çš„è¿‡æ‹Ÿåˆé—®é¢˜ã€‚</li>
<li>åˆ©ç”¨é¢„è®­ç»ƒçš„è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ˆVDMï¼‰æå–ä¸°å¯Œçš„å¤šè§†å›¾ä¸€è‡´æ€§å…ˆéªŒä¿¡æ¯ã€‚</li>
<li>é‡‡ç”¨è¯„åˆ†è’¸é¦é‡‡æ ·ï¼ˆSDSï¼‰æŠ€æœ¯è¿›è¡Œç›‘ç£æ¸²æŸ“çš„å›¾åƒã€‚åˆ©ç”¨GSDå°†æ¸²æŸ“ä»å¤šä¸ªç›¸é‚»è§†å›¾çš„å›¾åƒç»„åˆåœ¨ä¸€èµ·ï¼Œä»è€ŒæŒ‡å¯¼é«˜æ–¯å–·æº…è¡¨ç¤ºçš„ç”Ÿæˆæ–¹å‘å‘VDMæ–¹å‘å‘å±•ã€‚åœ¨æ­¤è¿‡ç¨‹ä¸­èå…¥å¯¹è§†å›¾è½¬åŒ–çš„ä¼˜åŒ–ç­–ç•¥ã€‚</li>
<li>é’ˆå¯¹å¯¹è±¡è¿åŠ¨å’Œéšæœºç›¸æœºè½¨è¿¹å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œå¼•å…¥ç»Ÿä¸€çš„æŒ‡å¯¼å½¢å¼æ¥çº æ­£VDMçš„å™ªå£°é¢„æµ‹ç»“æœã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.11213">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1c493aa98dbf9c5594f3d9b3ee3586b7" align="middle">
<img src="https://picx.zhimg.com/v2-2118b289b66dd637918a97f0774b3a93" align="middle">
<img src="https://picx.zhimg.com/v2-e827447e17a99bedfbfce0e76a145e16" align="middle">
<img src="https://picx.zhimg.com/v2-e27b0330d90c486692aa70816d5ebb13" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Refine-and-Align-Confidence-Calibration-through-Multi-Agent-Interaction-in-VQA"><a href="#Refine-and-Align-Confidence-Calibration-through-Multi-Agent-Interaction-in-VQA" class="headerlink" title="Refine and Align: Confidence Calibration through Multi-Agent Interaction in VQA"></a>Refine and Align: Confidence Calibration through Multi-Agent Interaction in VQA</h2><p><strong>Authors:Ayush Pandey, Jai Bardhan, Ishita Jain, Ramya S Hebbalaguppe, Rohan Raju Dhanakshirur, Lovekesh Vig</strong></p>
<p>In the context of Visual Question Answering (VQA) and Agentic AI, calibration refers to how closely an AI systemâ€™s confidence in its answers reflects their actual correctness. This aspect becomes especially important when such systems operate autonomously and must make decisions under visual uncertainty. While modern VQA systems, powered by advanced vision-language models (VLMs), are increasingly used in high-stakes domains like medical diagnostics and autonomous navigation due to their improved accuracy, the reliability of their confidence estimates remains under-examined. Particularly, these systems often produce overconfident responses. To address this, we introduce AlignVQA, a debate-based multi-agent framework, in which diverse specialized VLM â€“ each following distinct prompting strategies â€“ generate candidate answers and then engage in two-stage interaction: generalist agents critique, refine and aggregate these proposals. This debate process yields confidence estimates that more accurately reflect the modelâ€™s true predictive performance. We find that more calibrated specialized agents produce better aligned confidences. Furthermore, we introduce a novel differentiable calibration-aware loss function called aligncal designed to fine-tune the specialized agents by minimizing an upper bound on the calibration error. This objective explicitly improves the fidelity of each agentâ€™s confidence estimates. Empirical results across multiple benchmark VQA datasets substantiate the efficacy of our approach, demonstrating substantial reductions in calibration discrepancies. Furthermore, we propose a novel differentiable calibration-aware loss to fine-tune the specialized agents and improve the quality of their individual confidence estimates based on minimising upper bound calibration error.</p>
<blockquote>
<p>åœ¨è§†è§‰é—®ç­”ï¼ˆVQAï¼‰å’Œæ™ºèƒ½ä»£ç†AIçš„ä¸Šä¸‹æ–‡ä¸­ï¼Œæ ¡å‡†æ˜¯æŒ‡AIç³»ç»Ÿå¯¹å…¶ç­”æ¡ˆçš„ä¿¡å¿ƒå¦‚ä½•å‡†ç¡®åæ˜ å…¶å®é™…æ­£ç¡®æ€§ã€‚å½“è¿™æ ·çš„ç³»ç»Ÿè‡ªä¸»è¿è¡Œå¹¶åœ¨è§†è§‰ä¸ç¡®å®šæ€§ä¸‹åšå‡ºå†³ç­–æ—¶ï¼Œè¿™æ–¹é¢å˜å¾—å°¤ä¸ºé‡è¦ã€‚è™½ç„¶ç°ä»£VQAç³»ç»Ÿå‡­å€Ÿå…ˆè¿›çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨åŒ»ç–—è¯Šæ–­å’Œè‡ªä¸»å¯¼èˆªç­‰é«˜é£é™©é¢†åŸŸçš„åº”ç”¨è¶Šæ¥è¶Šå¹¿æ³›ï¼Œä½†ç”±äºå…¶ä¿¡å¿ƒä¼°è®¡çš„å¯é æ€§å°šæœªå¾—åˆ°å……åˆ†æ£€éªŒï¼Œä»å­˜åœ¨ä¸€äº›é—®é¢˜ã€‚ç‰¹åˆ«æ˜¯ï¼Œè¿™äº›ç³»ç»Ÿç»å¸¸äº§ç”Ÿè¿‡äºè‡ªä¿¡çš„å›åº”ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†AlignVQAï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºè¾©è®ºçš„å¤šä»£ç†æ¡†æ¶ï¼Œå…¶ä¸­å¤šç§ä¸“ä¸šVLMâ€”â€”æ¯ä¸ªè·Ÿéšä¸åŒçš„æç¤ºç­–ç•¥â€”â€”ç”Ÿæˆå€™é€‰ç­”æ¡ˆï¼Œç„¶åè¿›è¡Œä¸¤é˜¶æ®µäº’åŠ¨ï¼šä¸“å®¶ä»£ç†äººå¯¹è¿™äº›ææ¡ˆè¿›è¡Œæ‰¹åˆ¤ã€ç²¾ç‚¼å’Œæ±‡æ€»ã€‚è¿™ç§è¾©è®ºè¿‡ç¨‹äº§ç”Ÿçš„ä¿¡å¿ƒä¼°è®¡èƒ½æ›´å‡†ç¡®åœ°åæ˜ æ¨¡å‹çš„çœŸæ­£é¢„æµ‹æ€§èƒ½ã€‚æˆ‘ä»¬å‘ç°ç»è¿‡æ ¡å‡†çš„ä¸“ä¸šä»£ç†äººäº§ç”Ÿçš„ä¿¡å¿ƒæ›´åŠ ç¬¦åˆå®é™…ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹çš„å·®åˆ†æ ¡å‡†æ„ŸçŸ¥æŸå¤±å‡½æ•°ï¼Œç§°ä¸ºaligncalï¼Œæ—¨åœ¨é€šè¿‡æœ€å°åŒ–æ ¡å‡†è¯¯å·®çš„ä¸Šç•Œæ¥å¾®è°ƒä¸“ä¸šä»£ç†äººï¼Œä»è€Œæé«˜æ¯ä¸ªä»£ç†äººä¿¡å¿ƒä¼°è®¡çš„å‡†ç¡®æ€§ã€‚ç»éªŒè¯æ®æ˜¾ç¤ºï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šåŸºå‡†VQAæ•°æ®é›†ä¸Šéå¸¸æœ‰æ•ˆï¼Œæ˜¾è‘—å‡å°‘äº†æ ¡å‡†å·®å¼‚ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„å·®åˆ†æ ¡å‡†æ„ŸçŸ¥æŸå¤±å‡½æ•°æ¥å¾®è°ƒä¸“ä¸šä»£ç†äººï¼Œå¹¶åŸºäºæœ€å°åŒ–ä¸Šç•Œæ ¡å‡†è¯¯å·®æ¥æé«˜å…¶ä¸ªä½“ä¿¡å¿ƒä¼°è®¡çš„è´¨é‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.11169v1">PDF</a> 17 pages, 6 figures, 5 tables. Accepted to Special Track on AI Alignment, AAAI 2026. Project Page- <a target="_blank" rel="noopener" href="https://refine-align.github.io/">https://refine-align.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†è§†è§‰é—®ç­”ï¼ˆVQAï¼‰å’Œæ™ºèƒ½ä½“AIä¸­çš„æ ¡å‡†æ¦‚å¿µï¼Œå¼ºè°ƒAIç³»ç»Ÿçš„ç½®ä¿¡åº¦ä¸å…¶ç­”æ¡ˆå®é™…æ­£ç¡®æ€§çš„å»åˆç¨‹åº¦ã€‚åœ¨ç°ä»£åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„VQAç³»ç»Ÿä¸­ï¼Œè™½ç„¶å‡†ç¡®æ€§ä¸æ–­æé«˜å¹¶åœ¨åŒ»ç–—è¯Šæ–­å’Œè‡ªä¸»å¯¼èˆªç­‰é«˜é£é™©é¢†åŸŸå¾—åˆ°å¹¿æ³›åº”ç”¨ï¼Œä½†å…¶ç½®ä¿¡åº¦ä¼°è®¡çš„å¯é æ€§å°šæœªå¾—åˆ°å……åˆ†ç ”ç©¶ã€‚æ–‡æœ¬æå‡ºä¸€ç§åŸºäºè¾©è®ºçš„å¤šæ™ºèƒ½ä½“æ¡†æ¶AlignVQAï¼Œé€šè¿‡ä¸åŒä¸“ä¸šVLMç”Ÿæˆå€™é€‰ç­”æ¡ˆå¹¶è¿›è¡Œä¸¤é˜¶æ®µäº¤äº’ï¼Œæé«˜æ¨¡å‹ç½®ä¿¡åº¦çš„å‡†ç¡®æ€§ã€‚åŒæ—¶å¼•å…¥äº†ä¸€ç§æ–°å‹å¯å¾®åˆ†æ ¡å‡†æ„ŸçŸ¥æŸå¤±å‡½æ•°aligncalï¼Œä»¥å¾®è°ƒä¸“ä¸šæ™ºèƒ½ä½“å¹¶æé«˜å…¶ç½®ä¿¡åº¦ä¼°è®¡çš„ç²¾åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æœ‰æ•ˆå‡å°‘äº†æ ¡å‡†å·®å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AIç³»ç»Ÿçš„ç½®ä¿¡åº¦æ ¡å‡†åœ¨è‡ªä¸»å†³ç­–å’Œè§†è§‰ä¸ç¡®å®šæ€§çš„ç¯å¢ƒä¸‹å°¤ä¸ºé‡è¦ã€‚</li>
<li>ç°ä»£VQAç³»ç»Ÿè™½ç„¶åœ¨åŒ»ç–—è¯Šæ–­å’Œè‡ªä¸»å¯¼èˆªç­‰é¢†åŸŸè¡¨ç°å‡ºé«˜å‡†ç¡®æ€§ï¼Œä½†å…¶ç½®ä¿¡åº¦ä¼°è®¡çš„å¯é æ€§å°šæœªå¾—åˆ°å……åˆ†ç ”ç©¶ã€‚</li>
<li>æå‡ºçš„AlignVQAæ¡†æ¶åˆ©ç”¨å¤šæ™ºèƒ½ä½“è¾©è®ºè¿‡ç¨‹æé«˜æ¨¡å‹ç½®ä¿¡åº¦çš„å‡†ç¡®æ€§ã€‚</li>
<li>AlignVQAé€šè¿‡ä¸åŒä¸“ä¸šVLMç”Ÿæˆå€™é€‰ç­”æ¡ˆå¹¶è¿›è¡Œä¸¤é˜¶æ®µäº¤äº’ï¼ŒåŒ…æ‹¬æ‰¹åˆ¤ã€ç²¾åŒ–å’Œèšåˆã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°å‹å¯å¾®åˆ†æ ¡å‡†æ„ŸçŸ¥æŸå¤±å‡½æ•°aligncalï¼Œç”¨äºå¾®è°ƒä¸“ä¸šæ™ºèƒ½ä½“å¹¶æé«˜å…¶ç½®ä¿¡åº¦ä¼°è®¡çš„ç²¾åº¦ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡æœ€å°åŒ–æ ¡å‡†è¯¯å·®çš„ä¸Šç•Œæ¥ä¼˜åŒ–ç›®æ ‡ï¼Œä»è€Œæé«˜æ¯ä¸ªæ™ºèƒ½ä½“çš„ç½®ä¿¡åº¦ä¼°è®¡çš„ä¿çœŸåº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.11169">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b6ff9b54da4bda35b9c5ec8f7395508a" align="middle">
<img src="https://picx.zhimg.com/v2-20668bc63c7cdb0cce342e24d2da0e96" align="middle">
<img src="https://picx.zhimg.com/v2-75c7022a21e81c04d64145da90e95884" align="middle">
<img src="https://picx.zhimg.com/v2-da62cf467e27a93de0a6011b905bb284" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Utilizing-LLMs-for-Industrial-Process-Automation-A-Case-Study-on-Modifying-RAPID-Programs"><a href="#Utilizing-LLMs-for-Industrial-Process-Automation-A-Case-Study-on-Modifying-RAPID-Programs" class="headerlink" title="Utilizing LLMs for Industrial Process Automation: A Case Study on Modifying RAPID Programs"></a>Utilizing LLMs for Industrial Process Automation: A Case Study on Modifying RAPID Programs</h2><p><strong>Authors:Salim Fares, Steffen Herbold</strong></p>
<p>How to best use Large Language Models (LLMs) for software engineering is covered in many publications in recent years. However, most of this work focuses on widely-used general purpose programming languages. The utility of LLMs for software within the industrial process automation domain, with highly-specialized languages that are typically only used in proprietary contexts, is still underexplored. Within this paper, we study enterprises can achieve on their own without investing large amounts of effort into the training of models specific to the domain-specific languages that are used. We show that few-shot prompting approaches are sufficient to solve simple problems in a language that is otherwise not well-supported by an LLM and that is possible on-premise, thereby ensuring the protection of sensitive company data.</p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œå¦‚ä½•æœ€å¥½åœ°ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œè½¯ä»¶å·¥ç¨‹å·²æˆä¸ºè®¸å¤šå‡ºç‰ˆç‰©å…³æ³¨çš„ä¸»é¢˜ã€‚ç„¶è€Œï¼Œå¤§éƒ¨åˆ†å·¥ä½œéƒ½é›†ä¸­åœ¨é€šç”¨çš„ç¼–ç¨‹è¯­è¨€ä¸Šã€‚å¯¹äºå·¥ä¸šè¿‡ç¨‹è‡ªåŠ¨åŒ–é¢†åŸŸå†…çš„è½¯ä»¶ï¼Œä»¥åŠé€šå¸¸åªåœ¨ä¸“æœ‰ç¯å¢ƒä¸­ä½¿ç”¨çš„é«˜åº¦ä¸“ä¸šåŒ–çš„è¯­è¨€ï¼ŒLLMçš„å®ç”¨æ€§ä»ç„¶è¢«æ¢ç´¢å¾—ä¸å¤Ÿæ·±å…¥ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†ä¼ä¸šåœ¨ä¸ä½¿ç”¨å¤§é‡ç‰¹å®šé¢†åŸŸè¯­è¨€æ¨¡å‹åŸ¹è®­çš„æƒ…å†µä¸‹ï¼Œå¦‚ä½•åœ¨è‡ªå·±çš„ç¯å¢ƒä¸­å®ç°è¿™ä¸€ç›®æ ‡ã€‚æˆ‘ä»¬è¯æ˜ï¼Œé€šè¿‡å°‘é‡æç¤ºæ–¹æ³•å°±è¶³ä»¥è§£å†³åœ¨è¿™ç§è¯­è¨€ä¸Šé‡åˆ°çš„ç®€å•é—®é¢˜ï¼Œè€Œè¿™äº›é—®é¢˜é€šå¸¸ä¸ä¼šå—åˆ°LLMçš„è‰¯å¥½æ”¯æŒã€‚è€Œä¸”è¿™æ˜¯å¯ä»¥åœ¨å…¬å¸å†…éƒ¨å®ç°çš„ï¼Œä»è€Œç¡®ä¿äº†æ•æ„Ÿå…¬å¸æ•°æ®çš„ä¿æŠ¤ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.11125v1">PDF</a> Submitted to the International Conference on Software Engineering (ICSE) track Software Engineering in Practice (SEIP) 2026</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è½¯ä»¶å·¥ç¨‹ä¸­çš„åº”ç”¨è¿‘å¹´æ¥å¤‡å—å…³æ³¨ï¼Œä½†å¤§å¤šæ•°ç ”ç©¶é›†ä¸­åœ¨é€šç”¨ç¼–ç¨‹è¯­è¨€ä¸Šã€‚é’ˆå¯¹å·¥ä¸šè¿‡ç¨‹è‡ªåŠ¨åŒ–é¢†åŸŸä¸­çš„ä¸“æœ‰è¯­å¢ƒå’Œé«˜åº¦ä¸“ä¸šåŒ–çš„è¯­è¨€ï¼ŒLLMsåœ¨è½¯ä»¶æ–¹é¢çš„åº”ç”¨ä»ç„¶ç¼ºä¹ç ”ç©¶ã€‚æœ¬æ–‡ç ”ç©¶äº†ä¼ä¸šå¦‚ä½•åœ¨ä¸éœ€è¦ä¸ºç‰¹å®šé¢†åŸŸè¯­è¨€è®­ç»ƒæ¨¡å‹çš„æƒ…å†µä¸‹ï¼Œåˆ©ç”¨å°‘é‡æç¤ºè§£å†³ç®€å•é—®é¢˜ï¼Œå®ç°LLMsåœ¨è¯¥é¢†åŸŸçš„åº”ç”¨ï¼ŒåŒæ—¶ä¿æŠ¤æ•æ„Ÿçš„å…¬å¸æ•°æ®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è½¯ä»¶å·¥ç¨‹ä¸­çš„åº”ç”¨æ—¥ç›Šå—åˆ°å…³æ³¨ã€‚</li>
<li>ç›®å‰å¤§å¤šæ•°ç ”ç©¶é›†ä¸­åœ¨é€šç”¨ç¼–ç¨‹è¯­è¨€ä¸Šï¼Œå¯¹å·¥ä¸šè¿‡ç¨‹è‡ªåŠ¨åŒ–é¢†åŸŸä¸­çš„é«˜åº¦ä¸“ä¸šåŒ–è¯­è¨€çš„åº”ç”¨ä»ç„¶ç¼ºä¹ç ”ç©¶ã€‚</li>
<li>åœ¨ä¸ä½¿ç”¨ç‰¹å®šé¢†åŸŸè¯­è¨€è®­ç»ƒæ¨¡å‹çš„æƒ…å†µä¸‹ï¼Œä¼ä¸šå¯ä»¥é€šè¿‡å°‘é‡æç¤ºå®ç°LLMsåœ¨ä¸“æœ‰è¯­å¢ƒä¸­çš„åº”ç”¨ã€‚</li>
<li>é€šè¿‡è¿™ç§æ–¹å¼ï¼Œå¯ä»¥è§£å†³ç®€å•é—®é¢˜å¹¶å®ç°è½¯ä»¶çš„è‡ªåŠ¨åŒ–ã€‚</li>
<li>è¿™ç§åº”ç”¨æ–¹å¼æœ‰åŠ©äºä¿æŠ¤æ•æ„Ÿçš„å…¬å¸æ•°æ®ï¼Œé¿å…äº†å¤§è§„æ¨¡è®­ç»ƒæ‰€å¸¦æ¥çš„æ•°æ®æ³„éœ²é£é™©ã€‚</li>
<li>åˆ©ç”¨LLMsçš„ç‰¹ç‚¹å¯ä»¥åœ¨ç‰¹æ®Šè¯­è¨€èƒŒæ™¯ä¸‹æä¾›æœ‰ä»·å€¼çš„æœåŠ¡æ”¯æŒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.11125">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-da9ba124737995e75fe1c25a5813b287" align="middle">
<img src="https://picx.zhimg.com/v2-b047cad2f276bb483004aa8444e4039b" align="middle">
<img src="https://picx.zhimg.com/v2-a6c54ac561a963bd6f05d0f47df08a33" align="middle">
<img src="https://picx.zhimg.com/v2-d992bb54aa284ff832f775409db33dec" align="middle">
<img src="https://picx.zhimg.com/v2-6fef0bf34b2381e83a2765d26ff30118" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Detection-of-Bark-Beetle-Attacks-using-Hyperspectral-PRISMA-Data-and-Few-Shot-Learning"><a href="#Detection-of-Bark-Beetle-Attacks-using-Hyperspectral-PRISMA-Data-and-Few-Shot-Learning" class="headerlink" title="Detection of Bark Beetle Attacks using Hyperspectral PRISMA Data and Few-Shot Learning"></a>Detection of Bark Beetle Attacks using Hyperspectral PRISMA Data and Few-Shot Learning</h2><p><strong>Authors:Mattia Ferrari, Giancarlo Papitto, Giorgio Deligios, Lorenzo Bruzzone</strong></p>
<p>Bark beetle infestations represent a serious challenge for maintaining the health of coniferous forests. This paper proposes a few-shot learning approach leveraging contrastive learning to detect bark beetle infestations using satellite PRISMA hyperspectral data. The methodology is based on a contrastive learning framework to pre-train a one-dimensional CNN encoder, enabling the extraction of robust feature representations from hyperspectral data. These extracted features are subsequently utilized as input to support vector regression estimators, one for each class, trained on few labeled samples to estimate the proportions of healthy, attacked by bark beetle, and dead trees for each pixel. Experiments on the area of study in the Dolomites show that our method outperforms the use of original PRISMA spectral bands and of Sentinel-2 data. The results indicate that PRISMA hyperspectral data combined with few-shot learning offers significant advantages for forest health monitoring.</p>
<blockquote>
<p>æ ‘çš®ç”²è™«å…¥ä¾µå¯¹ç»´æŠ¤é’ˆå¶æ—å¥åº·æ„æˆä¸¥å³»æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åˆ©ç”¨å¯¹æ¯”å­¦ä¹ è¿›è¡Œæ ‘çš®ç”²è™«å…¥ä¾µæ£€æµ‹çš„å°‘æ ·æœ¬å­¦ä¹ æ–¹æ³•ï¼Œå¹¶ä½¿ç”¨å«æ˜ŸPRISMAé«˜å…‰è°±æ•°æ®æ¥å®ç°ã€‚è¯¥æ–¹æ³•åŸºäºå¯¹æ¯”å­¦ä¹ æ¡†æ¶ï¼Œå¯¹ä¸€ç»´CNNç¼–ç å™¨è¿›è¡Œé¢„è®­ç»ƒï¼Œèƒ½å¤Ÿä»é«˜å…‰è°±æ•°æ®ä¸­æå–ç¨³å¥çš„ç‰¹å¾è¡¨ç¤ºã€‚è¿™äº›æå–çš„ç‰¹å¾éšåè¢«ç”¨ä½œæ”¯æŒå‘é‡å›å½’ä¼°è®¡å™¨çš„è¾“å…¥ï¼Œä¸ºæ¯ä¸ªç±»åˆ«è®­ç»ƒå°‘é‡çš„æ ‡è®°æ ·æœ¬ï¼Œä»¥ä¼°è®¡æ¯ä¸ªåƒç´ ä¸­å¥åº·æ ‘æœ¨ã€è¢«æ ‘çš®ç”²è™«æ”»å‡»æ ‘æœ¨å’Œæ­»æ ‘çš„æ¯”ä¾‹ã€‚åœ¨å¤šå¤šæ´›ç±³èŒ¨ç ”ç©¶åŒºåŸŸçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºä½¿ç”¨åŸå§‹çš„PRISMAå…‰è°±æ³¢æ®µå’ŒSentinel-2æ•°æ®çš„æ–¹æ³•ã€‚ç»“æœè¡¨æ˜ï¼ŒPRISMAé«˜å…‰è°±æ•°æ®ä¸å°‘æ ·æœ¬å­¦ä¹ ç›¸ç»“åˆï¼Œåœ¨æ£®æ—å¥åº·ç›‘æµ‹æ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.11096v1">PDF</a> 5 pages, 3 figures, accepted at IGARSS conference 3-8 August 2025 Brisbane, Australia</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åˆ©ç”¨å¯¹æ¯”å­¦ä¹ è¿›è¡Œå°‘æ•°æ ·æœ¬å­¦ä¹ çš„æ£€æµ‹æ–¹æ³•ï¼Œç”¨äºåˆ©ç”¨å«æ˜ŸPRISMAé«˜å…‰è°±æ•°æ®æ£€æµ‹æ ‘çš®ç”²è™«å…¥ä¾µæƒ…å†µã€‚é€šè¿‡å¯¹æ¯”å­¦ä¹ æ¡†æ¶è®­ç»ƒä¸€ç»´CNNç¼–ç å™¨ï¼Œæå–é«˜å…‰è°±æ•°æ®çš„ç¨³å¥ç‰¹å¾è¡¨ç¤ºã€‚è¿™äº›ç‰¹å¾éšåè¢«ç”¨ä½œæ”¯æŒå‘é‡å›å½’ä¼°è®¡å™¨çš„è¾“å…¥ï¼Œå¯¹æ¯ä¸ªåƒç´ çš„å¥åº·æ ‘æœ¨ã€æ ‘çš®ç”²è™«æ”»å‡»å’Œæ­»æ ‘æ¯”ä¾‹è¿›è¡Œä¼°è®¡ã€‚åœ¨Dolomitesç ”ç©¶åŒºåŸŸçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºä½¿ç”¨åŸå§‹PRISMAå…‰è°±æ³¢æ®µå’ŒSentinel-2æ•°æ®çš„æ–¹æ³•ï¼Œæ˜¾ç¤ºå‡ºPRISMAé«˜å…‰è°±æ•°æ®ä¸å°‘æ•°æ ·æœ¬å­¦ä¹ ç›¸ç»“åˆåœ¨æ£®æ—å¥åº·ç›‘æµ‹ä¸­çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ ‘çš®ç”²è™«å…¥ä¾µå¯¹é’ˆå¶æ—å¥åº·æ„æˆä¸¥å³»æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºä¸€ç§åŸºäºå¯¹æ¯”å­¦ä¹ çš„å°‘æ•°æ ·æœ¬å­¦ä¹ æ–¹æ³•ï¼Œç”¨äºæ£€æµ‹æ ‘çš®ç”²è™«å…¥ä¾µã€‚</li>
<li>åˆ©ç”¨å«æ˜ŸPRISMAé«˜å…‰è°±æ•°æ®è¿›è¡Œæ£®æ—å¥åº·ç›‘æµ‹ã€‚</li>
<li>é€šè¿‡å¯¹æ¯”å­¦ä¹ æ¡†æ¶è®­ç»ƒä¸€ç»´CNNç¼–ç å™¨ï¼Œä»¥æå–é«˜å…‰è°±æ•°æ®çš„ç¨³å¥ç‰¹å¾ã€‚</li>
<li>ä½¿ç”¨æ”¯æŒå‘é‡å›å½’ä¼°è®¡å™¨ï¼Œæ ¹æ®æå–çš„ç‰¹å¾ä¼°è®¡æ¯ä¸ªåƒç´ çš„æ ‘æœ¨çŠ¶æ€ã€‚</li>
<li>åœ¨Dolomitesåœ°åŒºçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºä½¿ç”¨å…¶ä»–æ•°æ®æºçš„æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.11096">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6b60ce83c15b1d7ecbb4588b6fa488b1" align="middle">
<img src="https://picx.zhimg.com/v2-838fe88baf2806140cc977ac482dbce5" align="middle">
<img src="https://picx.zhimg.com/v2-ada1f252124fa52a9c7efac2288ad5a4" align="middle">
<img src="https://picx.zhimg.com/v2-ff84b72e4f3cd9aa6274369926263273" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Scalable-Population-Training-for-Zero-Shot-Coordination"><a href="#Scalable-Population-Training-for-Zero-Shot-Coordination" class="headerlink" title="Scalable Population Training for Zero-Shot Coordination"></a>Scalable Population Training for Zero-Shot Coordination</h2><p><strong>Authors:Bingyu Hui, Lebin Yu, Quanming Yao, Yunpeng Qu, Xudong Zhang, Jian Wang</strong></p>
<p>Zero-shot coordination(ZSC) has become a hot topic in reinforcement learning research recently. It focuses on the generalization ability of agents, requiring them to coordinate well with collaborators that are not seen before without any fine-tuning. Population-based training has been proven to provide good zero-shot coordination performance; nevertheless, existing methods are limited by computational resources, mainly focusing on optimizing diversity in small populations while neglecting the potential performance gains from scaling population size. To address this issue, this paper proposes the Scalable Population Training (ScaPT), an efficient training framework comprising two key components: a meta-agent that efficiently realizes a population by selectively sharing parameters across agents, and a mutual information regularizer that guarantees population diversity. To empirically validate the effectiveness of ScaPT, this paper evaluates it along with representational frameworks in Hanabi and confirms its superiority.</p>
<blockquote>
<p>é›¶é•œå¤´åè°ƒï¼ˆZSCï¼‰æœ€è¿‘å·²æˆä¸ºå¼ºåŒ–å­¦ä¹ ç ”ç©¶çš„çƒ­ç‚¹ã€‚å®ƒä¸»è¦å…³æ³¨ä»£ç†çš„æ³›åŒ–èƒ½åŠ›ï¼Œè¦æ±‚å®ƒä»¬èƒ½å¤Ÿå¾ˆå¥½åœ°ä¸ä»¥å‰æœªè§è¿‡çš„åˆä½œä¼™ä¼´è¿›è¡Œåè°ƒï¼Œè€Œæ— éœ€è¿›è¡Œå¾®è°ƒã€‚åŸºäºç§ç¾¤è®­ç»ƒå·²è¢«è¯æ˜èƒ½æä¾›è‰¯å¥½çš„é›¶é•œå¤´åè°ƒæ€§èƒ½ï¼›ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å—åˆ°è®¡ç®—èµ„æºçš„é™åˆ¶ï¼Œä¸»è¦é›†ä¸­åœ¨ä¼˜åŒ–å°ç§ç¾¤çš„å¤šæ ·æ€§ï¼Œè€Œå¿½è§†æ‰©å¤§ç§ç¾¤è§„æ¨¡å¯èƒ½å¸¦æ¥çš„æ€§èƒ½æå‡ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†å¯æ‰©å±•ç§ç¾¤è®­ç»ƒï¼ˆScaPTï¼‰è¿™ä¸€é«˜æ•ˆè®­ç»ƒæ¡†æ¶ï¼ŒåŒ…æ‹¬ä¸¤ä¸ªå…³é”®ç»„æˆéƒ¨åˆ†ï¼šä¸€ä¸ªå…ƒä»£ç†ï¼Œé€šè¿‡é€‰æ‹©æ€§å…±äº«å‚æ•°æœ‰æ•ˆåœ°å®ç°ç§ç¾¤ï¼Œä»¥åŠä¸€ä¸ªäº’ä¿¡æ¯è°ƒèŠ‚å™¨ï¼Œä¿è¯ç§ç¾¤å¤šæ ·æ€§ã€‚ä¸ºäº†å®è¯éªŒè¯ScaPTçš„æœ‰æ•ˆæ€§ï¼Œæœ¬æ–‡åœ¨Hanabiä¸­å¯¹å…¶è¿›è¡Œäº†è¯„ä¼°ä»£è¡¨æ€§æ¡†æ¶ï¼Œå¹¶è¯å®äº†å…¶ä¼˜è¶Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.11083v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>é›¶æ ·æœ¬åè°ƒï¼ˆZSCï¼‰æ˜¯å¼ºåŒ–å­¦ä¹ ç ”ç©¶é¢†åŸŸçš„çƒ­é—¨è¯é¢˜ã€‚å®ƒå…³æ³¨ä»£ç†çš„æ³›åŒ–èƒ½åŠ›ï¼Œè¦æ±‚å®ƒä»¬èƒ½ä¸æœªè§è¿‡çš„åˆä½œä¼™ä¼´è¿›è¡Œè‰¯å¥½åè°ƒï¼Œæ— éœ€å¾®è°ƒã€‚åŸºäºç§ç¾¤è®­ç»ƒå·²è¢«è¯æ˜èƒ½æä¾›å‡ºè‰²çš„é›¶æ ·æœ¬åè°ƒæ€§èƒ½ï¼Œä½†ç°æœ‰æ–¹æ³•å—é™äºè®¡ç®—èµ„æºï¼Œä¸»è¦å…³æ³¨å°ç§ç¾¤çš„å¤šæ ·æ€§ä¼˜åŒ–ï¼Œè€Œå¿½è§†äº†æ‰©å¤§ç§ç¾¤è§„æ¨¡å¯èƒ½å¸¦æ¥çš„æ€§èƒ½æå‡ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºå¯æ‰©å±•ç§ç¾¤è®­ç»ƒï¼ˆScaPTï¼‰è¿™ä¸€é«˜æ•ˆè®­ç»ƒæ¡†æ¶ï¼ŒåŒ…æ‹¬ä¸¤ä¸ªå…³é”®ç»„æˆéƒ¨åˆ†ï¼šèƒ½é«˜æ•ˆå®ç°ç§ç¾¤é€‰æ‹©æ€§å…±äº«å‚æ•°çš„å…ƒä»£ç†å’Œä¿è¯ç§ç¾¤å¤šæ ·æ€§çš„äº’ä¿¡æ¯è°ƒèŠ‚å™¨ã€‚é€šè¿‡å®è¯è¯„ä¼°ï¼Œæœ¬æ–‡åœ¨Hanabiä¸­çš„ä»£è¡¨æ€§æ¡†æ¶ä¸‹éªŒè¯äº†ScaPTçš„æœ‰æ•ˆæ€§ï¼Œå¹¶ç¡®è®¤äº†å…¶ä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é›¶æ ·æœ¬åè°ƒï¼ˆZSCï¼‰æ˜¯å¼ºåŒ–å­¦ä¹ é¢†åŸŸçš„é‡è¦ç ”ç©¶æ–¹å‘ï¼Œå…³æ³¨ä»£ç†çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>åŸºäºç§ç¾¤è®­ç»ƒå¯¹äºé›¶æ ·æœ¬åè°ƒæ€§èƒ½è¡¨ç°è‰¯å¥½ï¼Œä½†ç°æœ‰æ–¹æ³•åœ¨è®¡ç®—èµ„æºæ–¹é¢å­˜åœ¨é™åˆ¶ã€‚</li>
<li>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºå¯æ‰©å±•ç§ç¾¤è®­ç»ƒï¼ˆScaPTï¼‰çš„è®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•çš„è®¡ç®—èµ„æºé™åˆ¶é—®é¢˜ã€‚</li>
<li>ScaPTæ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªå…³é”®ç»„æˆéƒ¨åˆ†ï¼šå…ƒä»£ç†å’Œäº’ä¿¡æ¯è°ƒèŠ‚å™¨ã€‚</li>
<li>å…ƒä»£ç†é€šè¿‡é€‰æ‹©æ€§å…±äº«å‚æ•°æ¥é«˜æ•ˆå®ç°ç§ç¾¤ã€‚</li>
<li>äº’ä¿¡æ¯è°ƒèŠ‚å™¨ä¿è¯ç§ç¾¤å¤šæ ·æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.11083">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8714571c2f6e8442aa7af0dcdaade643" align="middle">
<img src="https://picx.zhimg.com/v2-5813a8a94a8b8c5e14c25f700cf38613" align="middle">
<img src="https://picx.zhimg.com/v2-8ffb9c061f226601eeb1f12e17b0ab30" align="middle">
<img src="https://picx.zhimg.com/v2-512d602916577d1a20f97e7b8cafd05a" align="middle">
<img src="https://picx.zhimg.com/v2-54cfdfe6f716a7c2220e8435c32e3037" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="GraphMASAL-A-Graph-based-Multi-Agent-System-for-Adaptive-Learning"><a href="#GraphMASAL-A-Graph-based-Multi-Agent-System-for-Adaptive-Learning" class="headerlink" title="GraphMASAL: A Graph-based Multi-Agent System for Adaptive Learning"></a>GraphMASAL: A Graph-based Multi-Agent System for Adaptive Learning</h2><p><strong>Authors:Biqing Zeng, Mengquan Liu, Zongwei Zhen</strong></p>
<p>The advent of Intelligent Tutoring Systems (ITSs) has marked a paradigm shift in education, enabling highly personalized learning pathways. However, true personalization requires adapting to learnersâ€™ complex knowledge states (multi-source) and diverse goals (multi-sink); existing ITSs often lack the necessary structural-reasoning capability and knowledge dynamism to generate genuinely effective learning paths, and they lack scientifically rigorous validation paradigms. In this paper we propose GraphMASAL (A Graph-based Multi-Agent System for Adaptive Learning), which integrates (i) a dynamic knowledge graph for persistent, stateful learner modeling; (ii) a LangGraph-orchestrated trio of agents (Diagnostician, Planner, Tutor); (iii) a knowledge-graph-grounded two-stage neural IR component (dual-encoder dense retrieval with cross-encoder listwise re-ranking and calibrated score fusion); and (iv) a multi-source multi-sink (MSMS) planning engine with a cognitively grounded cost and an approximation guarantee via greedy set cover. Under blinded automated evaluations with matched inputs and inference settings across diverse student profiles, GraphMASAL consistently outperforms LLM prompting and structured ablations in planningâ€“achieving stronger structural&#x2F;sequence alignment of learning paths, higher coverage of weak concepts, and lower learning costâ€“while also surpassing prompt-based baselines in cognitive diagnosis. Agreement with expert&#x2F;LLM-proxy ratings further supports the validity of our evaluation protocol. These findings indicate that grounding LLM agents in a dynamic knowledge graph, coupled with optimization under educational constraints, yields reliable, interpretable, and pedagogically plausible learning plans, advancing personalized and goal-oriented education.</p>
<blockquote>
<p>æ™ºèƒ½è¾…å¯¼ç³»ç»Ÿï¼ˆITSï¼‰çš„å‡ºç°æ ‡å¿—ç€æ•™è‚²é¢†åŸŸçš„èŒƒå¼è½¬å˜ï¼Œå®ƒä¸ºå®ç°é«˜åº¦ä¸ªæ€§åŒ–çš„å­¦ä¹ è·¯å¾„æä¾›äº†å¯èƒ½ã€‚ç„¶è€Œï¼ŒçœŸæ­£çš„ä¸ªæ€§åŒ–éœ€è¦é€‚åº”å­¦ä¹ è€…çš„å¤æ‚çŸ¥è¯†çŠ¶æ€ï¼ˆå¤šæºï¼‰å’Œå¤šæ ·åŒ–çš„ç›®æ ‡ï¼ˆå¤šæ±‡ï¼‰ï¼›ç°æœ‰çš„ITSé€šå¸¸ç¼ºä¹å¿…è¦çš„ç»“æ„åŒ–æ¨ç†èƒ½åŠ›å’ŒçŸ¥è¯†åŠ¨æ€æ€§ï¼Œæ— æ³•ç”ŸæˆçœŸæ­£æœ‰æ•ˆçš„å­¦ä¹ è·¯å¾„ï¼Œä¹Ÿç¼ºä¹ç§‘å­¦ä¸¥è°¨çš„éªŒè¯èŒƒå¼ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†GraphMASALï¼ˆåŸºäºå›¾çš„å¤šæ™ºèƒ½ä½“è‡ªé€‚åº”å­¦ä¹ ç³»ç»Ÿï¼‰ï¼Œå®ƒé›†æˆäº†ï¼ˆiï¼‰ç”¨äºæŒä¹…çŠ¶æ€å­¦ä¹ è€…å»ºæ¨¡çš„åŠ¨æ€çŸ¥è¯†å›¾ï¼›ï¼ˆiiï¼‰ç”±LangGraphåè°ƒçš„ä¸‰ä¸ªæ™ºèƒ½ä½“ï¼ˆè¯Šæ–­å¸ˆã€è§„åˆ’å¸ˆã€è¾…å¯¼å‘˜ï¼‰ï¼›ï¼ˆiiiï¼‰åŸºäºçŸ¥è¯†å›¾çš„ä¸¤ä¸ªé˜¶æ®µç¥ç»IRç»„ä»¶ï¼ˆå…·æœ‰äº¤å‰ç¼–ç å™¨åˆ—è¡¨é‡æ–°æ’åºå’Œæ ¡å‡†åˆ†æ•°èåˆçš„åŒé‡ç¼–ç å™¨å¯†é›†æ£€ç´¢ï¼‰ï¼›ä»¥åŠï¼ˆivï¼‰å¤šæºå¤šæ±‡ï¼ˆMSMSï¼‰è§„åˆ’å¼•æ“ï¼Œå…·æœ‰è®¤çŸ¥åŸºç¡€çš„æˆæœ¬å’Œé€šè¿‡è´ªå¿ƒé›†åˆè¦†ç›–çš„è¿‘ä¼¼ä¿è¯ã€‚åœ¨å¤šæ ·åŒ–çš„å­¦ç”Ÿé…ç½®æ–‡ä»¶åŒ¹é…çš„è¾“å…¥å’Œæ¨ç†è®¾ç½®ä¸‹è¿›è¡Œç›²è‡ªåŠ¨è¯„ä¼°ï¼ŒGraphMASALåœ¨è§„åˆ’æ–¹é¢å§‹ç»ˆä¼˜äºå¤§å‹è¯­è¨€æ¨¡å‹æç¤ºå’Œç»“æ„åŒ–çš„æ¶ˆé™¤æ–¹æ³•ï¼Œå®ç°äº†æ›´å¼ºçš„å­¦ä¹ è·¯å¾„ç»“æ„&#x2F;åºåˆ—å¯¹é½ã€æ›´é«˜çš„å¼±æ¦‚å¿µè¦†ç›–ç‡ã€æ›´ä½çš„å­¦ä¹ æˆæœ¬ï¼›åŒæ—¶ä¹Ÿåœ¨è®¤çŸ¥è¯Šæ–­ä¸Šè¶…è¶Šäº†åŸºäºæç¤ºçš„åŸºçº¿ã€‚ä¸ä¸“å®¶&#x2F;å¤§å‹è¯­è¨€æ¨¡å‹ä»£ç†è¯„åˆ†çš„åè®®è¿›ä¸€æ­¥æ”¯æŒäº†æˆ‘ä»¬è¯„ä¼°åè®®çš„æœ‰æ•ˆæ€§ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œå°†å¤§å‹è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“å»ºç«‹åœ¨åŠ¨æ€çŸ¥è¯†å›¾ä¸Šï¼Œå¹¶åœ¨æ•™è‚²çº¦æŸä¸‹è¿›è¡Œä¼˜åŒ–ï¼Œå¯ä»¥äº§ç”Ÿå¯é ã€å¯è§£é‡Šã€ç¬¦åˆæ•™å­¦æ³•çš„å­¦ä¹ è®¡åˆ’ï¼Œæ¨åŠ¨ä¸ªæ€§åŒ–å’Œç›®æ ‡å¯¼å‘çš„æ•™è‚²å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.11035v1">PDF</a> 9 pages, 3 figures,submitted to AAMAS 2026</p>
<p><strong>Summary</strong><br>     æ™ºèƒ½è¾…å¯¼ç³»ç»Ÿï¼ˆITSï¼‰çš„å‡ºç°æ ‡å¿—ç€æ•™è‚²é¢†åŸŸçš„èŒƒå¼è½¬å˜ï¼Œä¸ºå®ç°é«˜åº¦ä¸ªæ€§åŒ–çš„å­¦ä¹ è·¯å¾„æä¾›äº†å¯èƒ½ã€‚ç„¶è€Œï¼ŒçœŸæ­£çš„ä¸ªæ€§åŒ–éœ€è¦é€‚åº”å­¦ä¹ è€…çš„å¤æ‚çŸ¥è¯†çŠ¶æ€å’Œå¤šæ ·ç›®æ ‡ã€‚ç°æœ‰ITSsç¼ºä¹å¿…è¦çš„ç»“æ„åŒ–æ¨ç†èƒ½åŠ›å’ŒçŸ¥è¯†åŠ¨æ€ï¼Œéš¾ä»¥ç”ŸæˆçœŸæ­£æœ‰æ•ˆçš„å­¦ä¹ è·¯å¾„ã€‚æœ¬æ–‡æå‡ºGraphMASALï¼ˆåŸºäºå›¾çš„å¤šæ™ºèƒ½ä½“è‡ªé€‚åº”å­¦ä¹ ç³»ç»Ÿï¼‰ï¼Œç»“åˆåŠ¨æ€çŸ¥è¯†å›¾ã€LangGraphåè°ƒçš„ä¸‰ä¸ªæ™ºèƒ½ä½“ã€åŸºäºçŸ¥è¯†å›¾çš„ä¸¤ä¸ªé˜¶æ®µç¥ç»ç½‘ç»œIRç»„ä»¶ä»¥åŠå¤šæºå¤šæ±‡è§„åˆ’å¼•æ“ç­‰æŠ€æœ¯ï¼Œå®ç°ä¸ªæ€§åŒ–æ•™è‚²çš„æ–°çªç ´ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ™ºèƒ½è¾…å¯¼ç³»ç»Ÿï¼ˆITSï¼‰åœ¨æ•™è‚²é¢†åŸŸå®ç°äº†ä¸ªæ€§åŒ–å­¦ä¹ è·¯å¾„çš„å¯èƒ½æ€§ã€‚</li>
<li>ç°æœ‰ITSsåœ¨é€‚åº”å­¦ä¹ è€…å¤æ‚çŸ¥è¯†çŠ¶æ€å’Œå¤šæ ·ç›®æ ‡æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>GraphMASALé€šè¿‡ç»“åˆåŠ¨æ€çŸ¥è¯†å›¾ã€æ™ºèƒ½ä½“å’Œè§„åˆ’å¼•æ“ç­‰æŠ€æœ¯ï¼Œå®ç°äº†ä¸ªæ€§åŒ–æ•™è‚²çš„æ–°çªç ´ã€‚</li>
<li>GraphMASALåˆ©ç”¨çŸ¥è¯†å›¾çš„æŒä¹…çŠ¶æ€å»ºæ¨¡ï¼Œæé«˜äº†å­¦ä¹ è·¯å¾„çš„ç»“æ„åŒ–å’Œåºåˆ—å¯¹é½æ€§ã€‚</li>
<li>GraphMASALåœ¨å¼±æ¦‚å¿µè¦†ç›–å’Œå­¦ä¹ æˆæœ¬æ–¹é¢è¡¨ç°å‡ºä¼˜åŠ¿ï¼Œè¶…è¿‡äº†åŸºäºæç¤ºçš„åŸºçº¿ã€‚</li>
<li>ä¸“å®¶è¯„ä¼°åè®®å’ŒLLMä»£ç†è¯„åˆ†æ”¯æŒGraphMASALçš„æœ‰æ•ˆæ€§ã€‚</li>
<li>GraphMASALç³»ç»Ÿå¯ç”Ÿæˆå¯é ã€å¯è§£é‡Šä¸”æ•™è‚²ä¸Šå¯è¡Œçš„å­¦ä¹ è®¡åˆ’ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.11035">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-09b3e9dabfaec09b99e6463e1098dd9c" align="middle">
<img src="https://picx.zhimg.com/v2-08ec34ed3a0acfca327c00af8e620d9b" align="middle">
<img src="https://picx.zhimg.com/v2-9969af0768c9929287f2cdc3adbe5a6f" align="middle">
<img src="https://picx.zhimg.com/v2-fd34dd63c53c371d19732f4e52b2e181" align="middle">
<img src="https://picx.zhimg.com/v2-c0cdec259529da0ffcac0c941dc4dfc9" align="middle">
<img src="https://picx.zhimg.com/v2-5a2cda8ddbfe361fa8f55c294706ba0e" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="SP-Guard-Selective-Prompt-adaptive-Guidance-for-Safe-Text-to-Image-Generation"><a href="#SP-Guard-Selective-Prompt-adaptive-Guidance-for-Safe-Text-to-Image-Generation" class="headerlink" title="SP-Guard: Selective Prompt-adaptive Guidance for Safe Text-to-Image Generation"></a>SP-Guard: Selective Prompt-adaptive Guidance for Safe Text-to-Image Generation</h2><p><strong>Authors:Sumin Yu, Taesup Moon</strong></p>
<p>While diffusion-based T2I models have achieved remarkable image generation quality, they also enable easy creation of harmful content, raising social concerns and highlighting the need for safer generation. Existing inference-time guiding methods lack both adaptivityâ€“adjusting guidance strength based on the promptâ€“and selectivityâ€“targeting only unsafe regions of the image. Our method, SP-Guard, addresses these limitations by estimating prompt harmfulness and applying a selective guidance mask to guide only unsafe areas. Experiments show that SP-Guard generates safer images than existing methods while minimizing unintended content alteration. Beyond improving safety, our findings highlight the importance of transparency and controllability in image generation.</p>
<blockquote>
<p>åŸºäºæ‰©æ•£çš„T2Iæ¨¡å‹è™½ç„¶å·²ç»åœ¨å›¾åƒç”Ÿæˆè´¨é‡æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æˆæ•ˆï¼Œä½†å®ƒä»¬ä¹Ÿå®¹æ˜“å¯¼è‡´æœ‰å®³å†…å®¹çš„è½»æ¾åˆ›å»ºï¼Œå¼•å‘äº†ç¤¾ä¼šå…³æ³¨ï¼Œå¹¶å‡¸æ˜¾äº†æ›´å®‰å…¨ç”Ÿæˆçš„éœ€æ±‚ã€‚ç°æœ‰çš„æ¨ç†æ—¶é—´å¼•å¯¼æ–¹æ³•æ—¢ç¼ºä¹é€‚åº”æ€§â€”â€”æ ¹æ®æç¤ºè°ƒæ•´å¼•å¯¼å¼ºåº¦ï¼Œä¹Ÿç¼ºä¹é€‰æ‹©æ€§â€”â€”ä»…é’ˆå¯¹å›¾åƒçš„ä¸å®‰å…¨åŒºåŸŸè¿›è¡Œå¼•å¯¼ã€‚æˆ‘ä»¬çš„æ–¹æ³•SP-Guardé€šè¿‡ä¼°è®¡æç¤ºçš„æœ‰å®³æ€§å¹¶åº”ç”¨é€‰æ‹©æ€§å¼•å¯¼æ©è†œæ¥æŒ‡å¯¼ä¸å®‰å…¨çš„åŒºåŸŸï¼Œä»è€Œè§£å†³äº†è¿™äº›å±€é™æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒSP-Guardåœ¨æœ€å°åŒ–æ„å¤–å†…å®¹æ›´æ”¹çš„åŒæ—¶ï¼Œæ¯”ç°æœ‰æ–¹æ³•ç”Ÿæˆæ›´å®‰å…¨çš„å›¾åƒã€‚é™¤äº†æé«˜å®‰å…¨æ€§ä¹‹å¤–ï¼Œæˆ‘ä»¬çš„å‘ç°è¿˜å¼ºè°ƒäº†é€æ˜åº¦å’Œå¯æ§æ€§åœ¨å›¾åƒç”Ÿæˆä¸­çš„é‡è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.11014v1">PDF</a> Accepted for presentation at TRUST-AI Workshop, ECAI 2025. Proceedings to appear in CEUR-WS</p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£åŸºäºçš„T2Iæ¨¡å‹è™½ç„¶èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„å›¾åƒï¼Œä½†å®ƒä»¬ä¹Ÿæ˜“äºåˆ›å»ºæœ‰å®³å†…å®¹ï¼Œå¼•å‘ç¤¾ä¼šå…³æ³¨å¹¶å¼ºè°ƒéœ€è¦æ›´å®‰å…¨çš„å†…å®¹ç”Ÿæˆæ–¹æ³•ã€‚ç°æœ‰æ¨ç†æ—¶é—´å¼•å¯¼æ–¹æ³•ç¼ºä¹è‡ªé€‚åº”æ€§å’Œé€‰æ‹©æ€§ï¼Œæ— æ³•æ ¹æ®æç¤ºè°ƒæ•´å¼•å¯¼åŠ›åº¦å¹¶ä»…é’ˆå¯¹å›¾åƒçš„ä¸å®‰å…¨åŒºåŸŸè¿›è¡Œå¼•å¯¼ã€‚æˆ‘ä»¬çš„æ–¹æ³•SP-Guardé€šè¿‡ä¼°ç®—æç¤ºå±å®³å¹¶åº”ç”¨é€‰æ‹©æ€§å¼•å¯¼æ©è†œæ¥è§£å†³è¿™äº›é—®é¢˜ï¼Œä»¥å¼•å¯¼ä»…ä¸å®‰å…¨åŒºåŸŸã€‚å®éªŒè¡¨æ˜ï¼ŒSP-Guardåœ¨ç”Ÿæˆæ›´å®‰å…¨çš„å›¾åƒçš„åŒæ—¶ï¼Œå°½é‡å‡å°‘æ„å¤–çš„å†…å®¹æ”¹åŠ¨ã€‚é™¤äº†æé«˜å®‰å…¨æ€§å¤–ï¼Œæˆ‘ä»¬çš„å‘ç°è¿˜å¼ºè°ƒäº†å›¾åƒç”Ÿæˆè¿‡ç¨‹ä¸­é€æ˜åº¦å’Œå¯æ§æ€§çš„é‡è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£åŸºäºçš„T2Iæ¨¡å‹èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡å›¾åƒï¼Œä½†æ˜“äºåˆ›å»ºæœ‰å®³å†…å®¹ï¼Œå¼•å‘ç¤¾ä¼šå…³æ³¨ã€‚</li>
<li>ç°æœ‰æ¨ç†æ—¶é—´å¼•å¯¼æ–¹æ³•ç¼ºä¹è‡ªé€‚åº”æ€§å’Œé€‰æ‹©æ€§ï¼Œæ— æ³•æœ‰æ•ˆåº”å¯¹æœ‰å®³å†…å®¹çš„ç”Ÿæˆã€‚</li>
<li>SP-Guardæ–¹æ³•é€šè¿‡ä¼°ç®—æç¤ºå±å®³å¹¶åº”ç”¨é€‰æ‹©æ€§å¼•å¯¼æ©è†œï¼Œèƒ½å¤Ÿç”Ÿæˆæ›´å®‰å…¨çš„å›¾åƒå¹¶å‡å°‘æ„å¤–å†…å®¹æ”¹åŠ¨ã€‚</li>
<li>SP-Guardæ–¹æ³•èƒ½å¤Ÿä»…é’ˆå¯¹å›¾åƒçš„ä¸å®‰å…¨åŒºåŸŸè¿›è¡Œå¼•å¯¼ï¼Œæé«˜äº†å›¾åƒç”Ÿæˆçš„å¯æ§æ€§ã€‚</li>
<li>å®éªŒéªŒè¯äº†SP-Guardåœ¨å›¾åƒå®‰å…¨æ€§æ–¹é¢çš„ä¼˜è¶Šæ€§ã€‚</li>
<li>é™¤äº†æé«˜å®‰å…¨æ€§ï¼Œå›¾åƒç”Ÿæˆçš„é€æ˜åº¦å’Œå¯æ§æ€§ä¹Ÿæ˜¯é‡è¦çš„è€ƒé‡å› ç´ ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.11014">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-07657bd422cf5863cb57e7ced3bac9ab" align="middle">
<img src="https://picx.zhimg.com/v2-7e4efad200760f64b76ec5d79398ae68" align="middle">
<img src="https://picx.zhimg.com/v2-8180ebe86c6472515729589bdab50538" align="middle">
<img src="https://picx.zhimg.com/v2-76a78bd65fd763158e802858965c31a3" align="middle">
<img src="https://picx.zhimg.com/v2-a7911cc63354827b9fa8fbba7b53e00a" align="middle">
<img src="https://picx.zhimg.com/v2-cc8671a88dc960c9b07fa4907f84b5d1" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Preserving-Cross-Modal-Consistency-for-CLIP-based-Class-Incremental-Learning"><a href="#Preserving-Cross-Modal-Consistency-for-CLIP-based-Class-Incremental-Learning" class="headerlink" title="Preserving Cross-Modal Consistency for CLIP-based Class-Incremental Learning"></a>Preserving Cross-Modal Consistency for CLIP-based Class-Incremental Learning</h2><p><strong>Authors:Haoran Chen, Houze Xu, Micah Goldblum, Daoguo Dong, Zuxuan Wu</strong></p>
<p>Class-incremental learning (CIL) enables models to continuously learn new categories from sequential tasks without forgetting previously acquired knowledge. While recent advances in vision-language models such as CLIP have demonstrated strong generalization across domains, extending them to continual settings remains challenging. In particular, learning task-specific soft prompts for newly introduced classes often leads to severe classifier bias, as the text prototypes overfit to recent categories when prior data are unavailable. In this paper, we propose DMC, a simple yet effective two-stage framework for CLIP-based CIL that decouples the adaptation of the vision encoder and the optimization of textual soft prompts. Each stage is trained with the other frozen, allowing one modality to act as a stable semantic anchor for the other to preserve cross-modal alignment. Furthermore, current CLIP-based CIL approaches typically store class-wise Gaussian statistics for generative replay, yet they overlook the distributional drift that arises when the vision encoder is updated over time. To address this issue, we introduce DMC-OT, an enhanced version of DMC that incorporates an optimal-transport guided calibration strategy to align memory statistics across evolving encoders, along with a task-specific prompting design that enhances inter-task separability. Extensive experiments on CIFAR-100, Imagenet-R, CUB-200, and UCF-101 demonstrate that both DMC and DMC-OT achieve state-of-the-art performance, with DMC-OT further improving accuracy by an average of 1.80%.</p>
<blockquote>
<p>ç±»å¢é‡å­¦ä¹ ï¼ˆCILï¼‰ä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨æ‰§è¡Œé¡ºåºä»»åŠ¡æ—¶æŒç»­å­¦ä¹ æ–°ç±»åˆ«ï¼Œè€Œä¸ä¼šå¿˜è®°å…ˆå‰è·å¾—çš„çŸ¥è¯†ã€‚è™½ç„¶æœ€è¿‘çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆå¦‚CLIPï¼‰åœ¨è·¨åŸŸæ–¹é¢è¡¨ç°å‡ºäº†å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œä½†å°†å®ƒä»¬æ‰©å±•åˆ°è¿ç»­è®¾ç½®ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ç‰¹åˆ«æ˜¯ï¼Œå¯¹äºæ–°å¼•å…¥çš„ç±»åˆ«å­¦ä¹ ç‰¹å®šä»»åŠ¡çš„è½¯æç¤ºé€šå¸¸ä¼šå¯¼è‡´ä¸¥é‡çš„åˆ†ç±»å™¨åè§ï¼Œå› ä¸ºåœ¨æ²¡æœ‰å…ˆå‰æ•°æ®çš„æƒ…å†µä¸‹ï¼Œæ–‡æœ¬åŸå‹ä¼šè¿‡åº¦æ‹Ÿåˆæœ€è¿‘çš„ç±»åˆ«ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†DMCï¼Œè¿™æ˜¯ä¸€ä¸ªç®€å•æœ‰æ•ˆçš„ä¸¤é˜¶æ®µCLIPåŸºäºCILçš„æ¡†æ¶ï¼Œå®ƒè§£è€¦äº†è§†è§‰ç¼–ç å™¨çš„é€‚åº”æ€§å’Œæ–‡æœ¬è½¯æç¤ºçš„ä¼˜åŒ–ã€‚æ¯ä¸ªé˜¶æ®µåœ¨å¦ä¸€ä¸ªå†»ç»“çš„çŠ¶æ€ä¸‹è¿›è¡Œè®­ç»ƒï¼Œå…è®¸ä¸€ç§æ¨¡æ€ä½œä¸ºå¦ä¸€ç§çš„ç¨³å®šè¯­ä¹‰é”šï¼Œä»¥ä¿æŒè·¨æ¨¡æ€å¯¹é½ã€‚æ­¤å¤–ï¼Œå½“å‰çš„CLIPåŸºäºCILçš„æ–¹æ³•é€šå¸¸å­˜å‚¨ç±»çº§åˆ«çš„é«˜æ–¯ç»Ÿè®¡ä¿¡æ¯è¿›è¡Œç”Ÿæˆå›æ”¾ï¼Œä½†å®ƒä»¬å¿½ç•¥äº†å½“è§†è§‰ç¼–ç å™¨éšæ—¶é—´æ›´æ–°æ—¶å‡ºç°çš„åˆ†å¸ƒæ¼‚ç§»é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†DMC-OTï¼Œè¿™æ˜¯DMCçš„å¢å¼ºç‰ˆæœ¬ï¼Œå®ƒé‡‡ç”¨äº†ä¸€ç§ä»¥æœ€ä¼˜ä¼ è¾“ä¸ºæŒ‡å¯¼çš„æ ¡å‡†ç­–ç•¥ï¼Œä»¥åœ¨æ¼”åŒ–çš„ç¼–ç å™¨ä¹‹é—´å¯¹é½å†…å­˜ç»Ÿè®¡ä¿¡æ¯ï¼Œä»¥åŠä¸€ç§ç‰¹å®šä»»åŠ¡çš„æç¤ºè®¾è®¡ï¼Œä»¥æé«˜ä»»åŠ¡é—´çš„å¯åˆ†ç¦»æ€§ã€‚åœ¨CIFAR-100ã€Imagenet-Rã€CUB-200å’ŒUCF-101ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒDMCå’ŒDMC-OTå‡è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯çš„æ€§èƒ½æ°´å¹³ï¼Œå…¶ä¸­DMC-OTçš„å¹³å‡å‡†ç¡®åº¦è¿›ä¸€æ­¥æé«˜1.80%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.10974v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    æœ¬ç ”ç©¶æ¢è®¨ç±»å¢é‡å­¦ä¹ ï¼ˆCILï¼‰åœ¨CLIPæ¨¡å‹ä¸­çš„åº”ç”¨ï¼Œé’ˆå¯¹ç±»å¢é‡å­¦ä¹ ä¸­çš„æ–°é—®é¢˜æå‡ºäº†åŸºäºCLIPçš„ä¸¤é˜¶æ®µæ¡†æ¶DMCã€‚è¯¥æ¡†æ¶å¯ä»¥è§£è€¦è§†è§‰ç¼–ç å™¨çš„é€‚åº”æ€§å’Œæ–‡æœ¬è½¯æç¤ºçš„ä¼˜åŒ–ï¼Œå¹¶åœ¨ä¸åŒè®­ç»ƒé˜¶æ®µè¿›è¡Œç›¸äº’å†»ç»“ï¼Œä»¥ä¿ç•™è·¨æ¨¡æ€å¯¹é½ã€‚åŒæ—¶ï¼Œç ”ç©¶è§£å†³äº†ç”±äºå¿½è§†åˆ†å¸ƒæ¼‚ç§»çš„é—®é¢˜å¯¼è‡´çš„å½“å‰CLIPç±»å¢é‡å­¦ä¹ æ–¹æ³•çš„ç¼ºé™·ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œç ”ç©¶å¼•å…¥äº†å¢å¼ºç‰ˆçš„DMC-OTæ¡†æ¶ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨æœ€ä¼˜ä¼ è¾“å¼•å¯¼æ ¡å‡†ç­–ç•¥å¯¹é½è®°å¿†ç»Ÿè®¡åœ¨æ¼”åŒ–çš„ç¼–ç å™¨ä¹‹é—´ï¼ŒåŒæ—¶è®¾è®¡äº†ä»»åŠ¡ç‰¹å®šçš„æç¤ºä»¥å¢å¼ºä»»åŠ¡é—´çš„å¯åˆ†ç¦»æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ— è®ºæ˜¯DMCè¿˜æ˜¯DMC-OTéƒ½åœ¨ä¸åŒæ•°æ®é›†ä¸Šå–å¾—äº†ä¼˜äºå…¶ä»–æ¨¡å‹çš„æ€§èƒ½è¡¨ç°ã€‚å°¤å…¶æ˜¯å¼•å…¥æœ€ä¼˜ä¼ è¾“ç­–ç•¥çš„DMC-OTè¿›ä¸€æ­¥æé«˜äº†å¹³å‡å‡†ç¡®åº¦1.80%ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.10974">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c90aa204946426b7da496f0af7449b71" align="middle">
<img src="https://picx.zhimg.com/v2-fa6be666690ffb5ebfd5e8afa16e83e2" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Speech-Audio-Compositional-Attacks-on-Multimodal-LLMs-and-Their-Mitigation-with-SALMONN-Guard"><a href="#Speech-Audio-Compositional-Attacks-on-Multimodal-LLMs-and-Their-Mitigation-with-SALMONN-Guard" class="headerlink" title="Speech-Audio Compositional Attacks on Multimodal LLMs and Their Mitigation with SALMONN-Guard"></a>Speech-Audio Compositional Attacks on Multimodal LLMs and Their Mitigation with SALMONN-Guard</h2><p><strong>Authors:Yudong Yang, Xuezhen Zhang, Zhifeng Han, Siyin Wang, Jimin Zhuang, Zengrui Jin, Jing Shao, Guangzhi Sun, Chao Zhang</strong></p>
<p>Recent progress in large language models (LLMs) has enabled understanding of both speech and non-speech audio, but exposing new safety risks emerging from complex audio inputs that are inadequately handled by current safeguards. We introduce SACRED-Bench (Speech-Audio Composition for RED-teaming) to evaluate the robustness of LLMs under complex audio-based attacks. Unlike existing perturbation-based methods that rely on noise optimization or white-box access, SACRED-Bench exploits speech-audio composition mechanisms. SACRED-Bench adopts three mechanisms: (a) speech overlap and multi-speaker dialogue, which embeds harmful prompts beneath or alongside benign speech; (b) speech-audio mixture, which imply unsafe intent via non-speech audio alongside benign speech or audio; and (c) diverse spoken instruction formats (open-ended QA, yes&#x2F;no) that evade text-only filters. Experiments show that, even Gemini 2.5 Pro, the state-of-the-art proprietary LLM, still exhibits 66% attack success rate in SACRED-Bench test set, exposing vulnerabilities under cross-modal, speech-audio composition attacks. To bridge this gap, we propose SALMONN-Guard, a safeguard LLM that jointly inspects speech, audio, and text for safety judgments, reducing attack success down to 20%. Our results highlight the need for audio-aware defenses for the safety of multimodal LLMs. The benchmark and SALMONN-Guard checkpoints can be found at <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/tsinghua-ee/SACRED-Bench">https://huggingface.co/datasets/tsinghua-ee/SACRED-Bench</a>. Warning: this paper includes examples that may be offensive or harmful.</p>
<blockquote>
<p>æœ€è¿‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›å±•ä½¿å¾—ç†è§£å’Œå¤„ç†è¯­éŸ³å’Œéè¯­éŸ³éŸ³é¢‘æˆä¸ºå¯èƒ½ï¼Œä½†åŒæ—¶ä¹Ÿæš´éœ²å‡ºå½“å‰å®‰å…¨ä¿éšœæªæ–½ä¸è¶³ä»¥åº”å¯¹å¤æ‚éŸ³é¢‘è¾“å…¥æ‰€å¸¦æ¥çš„æ–°çš„å®‰å…¨é£é™©ã€‚æˆ‘ä»¬æ¨å‡ºSACRED-Benchï¼ˆç”¨äºçº¢é˜Ÿæµ‹è¯•çš„è¯­éŸ³-éŸ³é¢‘ç»„åˆï¼‰ï¼Œä»¥è¯„ä¼°LLMåœ¨å¤æ‚çš„åŸºäºéŸ³é¢‘çš„æ”»å‡»ä¸‹çš„ç¨³å¥æ€§ã€‚ä¸ç°æœ‰çš„åŸºäºæ‰°åŠ¨çš„æ–¹æ³•ä¸åŒï¼Œè¿™äº›æ–¹æ³•ä¾èµ–äºå™ªå£°ä¼˜åŒ–æˆ–ç™½ç›’è®¿é—®ï¼ŒSACRED-Benchåˆ©ç”¨è¯­éŸ³-éŸ³é¢‘ç»„åˆæœºåˆ¶ã€‚SACRED-Benché‡‡ç”¨ä¸‰ç§æœºåˆ¶ï¼šï¼ˆaï¼‰è¯­éŸ³é‡å å’Œå¤šäººå¯¹è¯ï¼Œåœ¨è‰¯æ€§è¯­éŸ³ä¹‹ä¸‹æˆ–æ—è¾¹åµŒå…¥æœ‰å®³æç¤ºï¼›ï¼ˆbï¼‰è¯­éŸ³-éŸ³é¢‘æ··åˆï¼Œé€šè¿‡éè¯­éŸ³éŸ³é¢‘ä¼ è¾¾ä¸å®‰å…¨æ„å›¾ï¼Œä¼´éšè‰¯æ€§è¯­éŸ³æˆ–éŸ³é¢‘ï¼›ï¼ˆcï¼‰å¤šæ ·çš„å£å¤´æŒ‡ä»¤æ ¼å¼ï¼ˆå¼€æ”¾å¼é—®ç­”ã€æ˜¯éé¢˜ï¼‰å¯ç»•è¿‡ä»…æ–‡æœ¬çš„è¿‡æ»¤å™¨ã€‚å®éªŒè¡¨æ˜ï¼Œå³ä½¿æ˜¯ç›®å‰æœ€å…ˆè¿›çš„ä¸“æœ‰LLMâ€”â€”Gemini 2.5 Proï¼Œåœ¨SACRED-Benchæµ‹è¯•é›†ä¸­æ”»å‡»æˆåŠŸç‡ä»é«˜è¾¾66%ï¼Œæš´éœ²å‡ºè·¨æ¨¡æ€å’Œè¯­éŸ³-éŸ³é¢‘ç»„åˆæ”»å‡»ä¸‹çš„æ¼æ´ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†SALMONN-Guardï¼Œè¿™æ˜¯ä¸€ç§ä¿æŠ¤LLMçš„å®‰å…¨ä¿éšœæªæ–½ï¼Œå¯è”åˆæ£€æŸ¥è¯­éŸ³ã€éŸ³é¢‘å’Œæ–‡æœ¬ä»¥è¿›è¡Œå®‰å…¨åˆ¤æ–­ï¼Œå°†æ”»å‡»æˆåŠŸç‡é™ä½åˆ°20%ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœå¼ºè°ƒäº†ä¸ºå¤šåª’ä½“LLMé…å¤‡éŸ³é¢‘æ„ŸçŸ¥é˜²å¾¡æªæ–½çš„å¿…è¦æ€§ã€‚åŸºå‡†æµ‹è¯•å’ŒSALMONN-Guardæ£€æŸ¥ç‚¹å¯è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/tsinghua-ee/SACRED-Bench%E3%80%82%E5%B9%BF%E8%AD%A6%EF%BC%9A%E6%9C%AC%E8%AE%BA%E6%96%87%E5%8C%85%E5%90%AB%E5%8F%AF%E8%83%BD%E5%85%B7%E6%9C%89%E7%AA%81%E7%A2%BA%E6%80%A7%E6%88%96%E6%BB%A5%E5%AE%B3%E7%9A%84%E4%BE%8B%E5%A6%82%%E3%80%82">https://huggingface.co/datasets/tsinghua-ee/SACRED-Benchã€‚è­¦å‘Šï¼šæœ¬è®ºæ–‡åŒ…å«å¯èƒ½å…·æœ‰å†’çŠ¯æ€§æˆ–æœ‰å®³çš„ç¤ºä¾‹ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.10222v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç†è§£è¯­éŸ³å’Œéè¯­éŸ³éŸ³é¢‘æ–¹é¢å–å¾—äº†æœ€æ–°è¿›å±•ï¼Œä½†åŒæ—¶ä¹Ÿæš´éœ²å‡ºç”±å¤æ‚éŸ³é¢‘è¾“å…¥å¸¦æ¥çš„æ–°çš„å®‰å…¨é£é™©ï¼Œå½“å‰çš„å®‰å…¨æªæ–½æ— æ³•å……åˆ†åº”å¯¹ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†SACRED-Benchï¼ˆç”¨äºçº¢é˜Ÿå›¢é˜Ÿçš„è¯­éŸ³-éŸ³é¢‘ç»„åˆè¯„ä¼°ï¼‰ï¼Œä»¥è¯„ä¼°LLMsåœ¨å¤æ‚éŸ³é¢‘æ”»å‡»ä¸‹çš„ç¨³å¥æ€§ã€‚ä¸åŒäºç°æœ‰çš„åŸºäºæ‰°åŠ¨çš„æ–¹æ³•ï¼ŒSACRED-Benchåˆ©ç”¨è¯­éŸ³-éŸ³é¢‘ç»„åˆæœºåˆ¶ï¼Œé‡‡ç”¨ä¸‰ç§æœºåˆ¶ï¼šï¼ˆaï¼‰è¯­éŸ³é‡å å’Œå¤šäººå¯¹è¯ï¼Œåœ¨è‰¯æ€§è¯­éŸ³æ—è¾¹æˆ–ä¸‹æ–¹åµŒå…¥æœ‰å®³æç¤ºï¼›ï¼ˆbï¼‰è¯­éŸ³-éŸ³é¢‘æ··åˆï¼Œé€šè¿‡éè¯­éŸ³éŸ³é¢‘ä¼ é€’ä¸å®‰å…¨æ„å›¾ï¼Œå¹¶ä¸è‰¯æ€§è¯­éŸ³æˆ–éŸ³é¢‘ç›¸ç»“åˆï¼›ï¼ˆcï¼‰å¤šæ ·åŒ–çš„å£è¯­æŒ‡ä»¤æ ¼å¼ï¼ˆå¼€æ”¾å¼é—®ç­”ã€æ˜¯éé—®ï¼‰ï¼Œä»¥é¿å…ä»…åŸºäºæ–‡æœ¬çš„è¿‡æ»¤ã€‚å®éªŒè¡¨æ˜ï¼Œå³ä½¿åœ¨æœ€å…ˆè¿›çš„ç§æœ‰LLGMini 2.5 Proä¸Šï¼Œåœ¨SACRED-Benchæµ‹è¯•é›†ä¸Šçš„æ”»å‡»æˆåŠŸç‡ä»é«˜è¾¾66%ï¼Œå‡¸æ˜¾å‡ºåœ¨è·¨æ¨¡æ€ã€è¯­éŸ³-éŸ³é¢‘ç»„åˆæ”»å‡»ä¸‹çš„æ¼æ´ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†SALMONN-Guardå®‰å…¨å«å£«ï¼Œå®ƒå¯ä»¥è”åˆæ£€æŸ¥è¯­éŸ³ã€éŸ³é¢‘å’Œæ–‡æœ¬è¿›è¡Œå®‰å…¨åˆ¤æ–­ï¼Œå°†æ”»å‡»æˆåŠŸç‡é™ä½è‡³20%ã€‚æˆ‘ä»¬çš„ç»“æœå¼ºè°ƒäº†å¤šæ¨¡å¼LLMçš„éŸ³é¢‘æ„ŸçŸ¥é˜²å¾¡éœ€æ±‚ã€‚SACRED-Benchçš„åŸºå‡†æµ‹è¯•å’ŒSALMONN-Guardæ£€æŸ¥ç‚¹å¯åœ¨[<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/tsinghua-ee/SACRED-Bench%E6%89%BE%E5%88%B0%E3%80%82%E8%AF%B7%E6%B3%A8%E6%84%8F%EF%BC%9A%E6%9C%AC%E6%96%87%E5%8C%85%E5%90%AB%E5%8F%AF%E8%83%BD%E5%85%B7%E6%9C%89%E5%86%92%E7%8A%AF%E6%80%A7%E6%88%96%E6%9C%89%E5%AE%B3%E6%80%A7%E7%9A%84%E7%A4%BA%E4%BE%8B%E3%80%82">https://huggingface.co/datasets/tsinghua-ee/SACRED-Benchæ‰¾åˆ°ã€‚è¯·æ³¨æ„ï¼šæœ¬æ–‡åŒ…å«å¯èƒ½å…·æœ‰å†’çŠ¯æ€§æˆ–æœ‰å®³æ€§çš„ç¤ºä¾‹ã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>LLMsè™½ç„¶èƒ½å¤Ÿå¤„ç†å¤æ‚çš„éŸ³é¢‘è¾“å…¥ï¼Œä½†å­˜åœ¨å®‰å…¨é£é™©æ¼æ´ã€‚ç°æœ‰çš„å®‰å…¨æªæ–½æœªèƒ½å……åˆ†åº”å¯¹ç”±å¤æ‚éŸ³é¢‘è¾“å…¥å¸¦æ¥çš„é£é™©ã€‚</li>
<li>SACRED-Benché€šè¿‡æ¨¡æ‹ŸçœŸå®çš„è¯­éŸ³å’ŒéŸ³é¢‘ç»„åˆæ”»å‡»æ¥è¯„ä¼°LLMsçš„ç¨³å¥æ€§ã€‚å…¶åŒ…å«ä¸‰ç§æœºåˆ¶ï¼šè¯­éŸ³é‡å å’Œå¤šè¯´è¯è€…å¯¹è¯ã€è¯­éŸ³å’ŒéŸ³é¢‘æ··åˆä»¥åŠå¤šæ ·çš„å£è¯­æŒ‡ä»¤æ ¼å¼ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œå³ä½¿æ˜¯ç›®å‰æœ€å…ˆè¿›çš„LLGMini 2.5 Proåœ¨SACRED-Benchæµ‹è¯•é›†ä¸Šä¹Ÿå®¹æ˜“å—åˆ°æ”»å‡»ï¼Œæ”»å‡»æˆåŠŸç‡é«˜è¾¾66%ã€‚è¿™è¡¨æ˜LLMsåœ¨è·¨æ¨¡æ€å’Œè¯­éŸ³-éŸ³é¢‘ç»„åˆæ”»å‡»ä¸‹å­˜åœ¨æ˜¾è‘—çš„è„†å¼±æ€§ã€‚</li>
<li>ä¸ºäº†å¢å¼ºLLMsçš„å®‰å…¨æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†SALMONN-Guardå®‰å…¨å«å£«ã€‚å®ƒè”åˆæ£€æŸ¥è¯­éŸ³ã€éŸ³é¢‘å’Œæ–‡æœ¬è¿›è¡Œå®‰å…¨åˆ¤æ–­ï¼Œæ˜¾è‘—é™ä½äº†æ”»å‡»æˆåŠŸç‡è‡³20%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.10222">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-85c3cb9bfa0bacbf18f2c109983b911e" align="middle">
<img src="https://picx.zhimg.com/v2-8cd73019ca6274d9ef76d7b6b00c1736" align="middle">
<img src="https://picx.zhimg.com/v2-5333b659bd5c801d4ab81538bb3267e6" align="middle">
<img src="https://picx.zhimg.com/v2-0d41ac41ddaaee8884ea796c5f4ded59" align="middle">
<img src="https://picx.zhimg.com/v2-ee19ba35695541c36c36ba8c538f6a5a" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Toward-Automated-Cognitive-Assessment-in-Parkinsonâ€™s-Disease-Using-Pretrained-Language-Models"><a href="#Toward-Automated-Cognitive-Assessment-in-Parkinsonâ€™s-Disease-Using-Pretrained-Language-Models" class="headerlink" title="Toward Automated Cognitive Assessment in Parkinsonâ€™s Disease Using Pretrained Language Models"></a>Toward Automated Cognitive Assessment in Parkinsonâ€™s Disease Using Pretrained Language Models</h2><p><strong>Authors:Varada Khanna, Nilay Bhatt, Ikgyu Shin, Sule Tinaz, Yang Ren, Hua Xu, Vipina K. Keloth</strong></p>
<p>Understanding how individuals with Parkinsonâ€™s disease (PD) describe cognitive experiences in their daily lives can offer valuable insights into disease-related cognitive and emotional changes. However, extracting such information from unstructured patient narratives is challenging due to the subtle, overlapping nature of cognitive constructs. This study developed and evaluated natural language processing (NLP) models to automatically identify categories that reflect various cognitive processes from de-identified first-person narratives. Three model families, a Bio_ClinicalBERT-based span categorization model for nested entity recognition, a fine-tuned Meta-Llama-3-8B-Instruct model using QLoRA for instruction following, and GPT-4o mini evaluated under zero- and few-shot settings, were compared on their performance on extracting seven categories. Our findings indicated that model performance varied substantially across categories and model families. The fine-tuned Meta-Llama-3-8B-Instruct achieved the highest overall F1-scores (0.74 micro-average and 0.59 macro-average), particularly excelling in context-dependent categories such as thought and social interaction. Bio_ClinicalBERT exhibited high precision but low recall and performed comparable to Llama for some category types such as location and time but failed on other categories such as thought, emotion and social interaction. Compared to conventional information extraction tasks, this task presents a greater challenge due to the abstract and overlapping nature of narrative accounts of complex cognitive processes. Nonetheless, with continued refinement, these NLP systems hold promise for enabling low-burden, longitudinal monitoring of cognitive function and serving as a valuable complement to formal neuropsychological assessments in PD.</p>
<blockquote>
<p>ç†è§£å¸•é‡‘æ£®ç—…æ‚£è€…å¦‚ä½•æè¿°æ—¥å¸¸ç”Ÿæ´»ä¸­çš„è®¤çŸ¥ä½“éªŒï¼Œå¯ä»¥ä¸ºä¸ç–¾ç—…ç›¸å…³çš„è®¤çŸ¥å’Œæƒ…ç»ªå˜åŒ–æä¾›å®è´µçš„è§è§£ã€‚ç„¶è€Œï¼Œä»éç»“æ„åŒ–çš„æ‚£è€…å™è¿°ä¸­æå–æ­¤ç±»ä¿¡æ¯æ˜¯ä¸€é¡¹æŒ‘æˆ˜ï¼Œå› ä¸ºè®¤çŸ¥ç»“æ„å…·æœ‰ç»†å¾®ä¸”é‡å çš„æ€§è´¨ã€‚æœ¬ç ”ç©¶å¼€å‘å’Œè¯„ä¼°äº†è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰æ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹èƒ½å¤Ÿè‡ªåŠ¨ä»å»æ ‡è¯†çš„ç¬¬ä¸€äººç§°å™è¿°ä¸­è¯†åˆ«åæ˜ å„ç§è®¤çŸ¥è¿‡ç¨‹çš„ç±»åˆ«ã€‚æ¯”è¾ƒäº†ä¸‰ç§æ¨¡å‹å®¶æ—åœ¨é›¶é•œå¤´å’Œå°‘é•œå¤´è®¾ç½®ä¸‹çš„è¡¨ç°ï¼Œè¿™äº›æ¨¡å‹åŒ…æ‹¬åŸºäºBio_ClinicalBERTçš„è·¨åº¦åˆ†ç±»æ¨¡å‹ï¼ˆç”¨äºåµŒå¥—å®ä½“è¯†åˆ«ï¼‰ã€ä½¿ç”¨QLoRAè¿›è¡ŒæŒ‡ä»¤è·Ÿéšçš„ç²¾ç»†è°ƒæ•´çš„Meta-Llama-3-8B-Instructæ¨¡å‹ä»¥åŠGPT-4o miniæ¨¡å‹ã€‚åœ¨æå–ä¸ƒä¸ªç±»åˆ«æ—¶ï¼Œæˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œä¸åŒç±»åˆ«å’Œæ¨¡å‹å®¶æ—çš„æ¨¡å‹æ€§èƒ½å­˜åœ¨å¾ˆå¤§å·®å¼‚ã€‚ç²¾ç»†è°ƒæ•´çš„Meta-Llama-3-8B-Instructè·å¾—äº†æœ€é«˜çš„æ€»ä½“F1åˆ†æ•°ï¼ˆå¾®å¹³å‡0.74å’Œå®å¹³å‡0.59ï¼‰ï¼Œå°¤å…¶åœ¨æ€æƒ³å’Œç¤¾äº¤äº’åŠ¨ç­‰ä¸Šä¸‹æ–‡ç›¸å…³çš„ç±»åˆ«ä¸­è¡¨ç°çªå‡ºã€‚Bio_ClinicalBERTè¡¨ç°å‡ºè¾ƒé«˜çš„ç²¾åº¦ä½†å¬å›ç‡è¾ƒä½ï¼Œå¯¹äºæŸäº›ç±»åˆ«ç±»å‹ï¼ˆå¦‚åœ°ç‚¹å’Œæ—¶é—´ï¼‰ä¸Llamaè¡¨ç°ç›¸å½“ï¼Œä½†åœ¨å…¶ä»–ç±»åˆ«ï¼ˆå¦‚æ€æƒ³ã€æƒ…æ„Ÿå’Œç¤¾äº¤äº’åŠ¨ï¼‰ä¸Šè¡¨ç°ä¸ä½³ã€‚ä¸ä¼ ç»Ÿçš„ä¿¡æ¯æå–ä»»åŠ¡ç›¸æ¯”ï¼Œç”±äºå¤æ‚è®¤çŸ¥è¿‡ç¨‹çš„å™è¿°å…·æœ‰æŠ½è±¡æ€§å’Œé‡å æ€§ï¼Œæ­¤ä»»åŠ¡å‘ˆç°æ›´å¤§çš„æŒ‘æˆ˜ã€‚ç„¶è€Œï¼Œéšç€ä¸æ–­çš„æ”¹è¿›ï¼Œè¿™äº›NLPç³»ç»Ÿåœ¨å®ç°è½»æ¾çš„é•¿æœŸè®¤çŸ¥åŠŸèƒ½ç›‘æµ‹æ–¹é¢å‰æ™¯å¹¿é˜”ï¼Œå¹¶å¯ä½œä¸ºæ­£å¼ç¥ç»å¿ƒç†å­¦è¯„ä¼°åœ¨å¸•é‡‘æ£®ç—…ä¸­çš„å®è´µè¡¥å……ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.08806v1">PDF</a> 15 pages, 4 figures, 1 table. Varada Khanna and Nilay Bhatt are co-first authors. Sule Tinaz and Hua Xu are co-senior authors. Corresponding author: Vipina K. Keloth (<a href="mailto:&#x76;&#105;&#112;&#105;&#x6e;&#x61;&#x2e;&#x6b;&#x75;&#116;&#116;&#x69;&#x63;&#104;&#x69;&#x6b;&#x65;&#x6c;&#x6f;&#116;&#x68;&#64;&#x79;&#x61;&#108;&#x65;&#46;&#x65;&#100;&#x75;">&#x76;&#105;&#112;&#105;&#x6e;&#x61;&#x2e;&#x6b;&#x75;&#116;&#116;&#x69;&#x63;&#104;&#x69;&#x6b;&#x65;&#x6c;&#x6f;&#116;&#x68;&#64;&#x79;&#x61;&#108;&#x65;&#46;&#x65;&#100;&#x75;</a>)</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬ç ”ç©¶åˆ©ç”¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰æ¨¡å‹ï¼Œä»å¸•é‡‘æ£®ç—…æ‚£è€…ï¼ˆPDï¼‰çš„éç»“æ„åŒ–å™è¿°ä¸­è‡ªåŠ¨è¯†åˆ«åæ˜ ä¸åŒè®¤çŸ¥è¿‡ç¨‹çš„ç±»åˆ«ã€‚æ¯”è¾ƒäº†ä¸‰ç§æ¨¡å‹å®¶æ—åœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬ç¯å¢ƒä¸‹çš„è¡¨ç°ï¼Œå‘ç°æ¨¡å‹åœ¨ä¸åŒç±»åˆ«å’Œæ¨¡å‹å®¶æ—ä¹‹é—´çš„è¡¨ç°å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚ç²¾ç»†è°ƒæ•´çš„Meta-Llama-3-8B-Instructæ¨¡å‹åœ¨æ€»ä½“F1åˆ†æ•°ä¸Šè¡¨ç°æœ€ä½³ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸Šä¸‹æ–‡ç›¸å…³çš„ç±»åˆ«ä¸Šã€‚è¿™é¡¹ä»»åŠ¡å› ä¸ºå™è¿°çš„å¤æ‚è®¤çŸ¥è¿‡ç¨‹çš„æŠ½è±¡æ€§å’Œé‡å æ€§ï¼Œç›¸æ¯”ä¼ ç»Ÿçš„ä¿¡æ¯æå–ä»»åŠ¡æ›´å…·æŒ‘æˆ˜æ€§ã€‚ä½†NLPç³»ç»Ÿç»æŒç»­ä¼˜åŒ–åï¼Œæœ‰æœ›åœ¨å¸•é‡‘æ£®ç—…ä¸­å®ç°å¯¹è®¤çŸ¥åŠŸèƒ½çš„ä½è´Ÿæ‹…ã€é•¿æœŸç›‘æµ‹ï¼Œå¹¶æˆä¸ºæ­£å¼ç¥ç»å¿ƒç†å­¦è¯„ä¼°çš„å®è´µè¡¥å……ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>NLPæ¨¡å‹å¯ç”¨äºä»å¸•é‡‘æ£®ç—…æ‚£è€…ï¼ˆPDï¼‰çš„éç»“æ„åŒ–å™è¿°ä¸­è‡ªåŠ¨è¯†åˆ«è®¤çŸ¥è¿‡ç¨‹ç±»åˆ«ã€‚</li>
<li>æ¨¡å‹å®¶æ—åŒ…æ‹¬åŸºäºBio_ClinicalBERTçš„è·¨åº¦åˆ†ç±»æ¨¡å‹ã€ä½¿ç”¨QLoRAè¿›è¡ŒæŒ‡ä»¤è·Ÿéšçš„Meta-Llama-3-8B-Instructç²¾ç»†è°ƒæ•´æ¨¡å‹å’ŒGPT-4o miniæ¨¡å‹ã€‚</li>
<li>æ¨¡å‹æ€§èƒ½åœ¨ç±»åˆ«å’Œæ¨¡å‹å®¶æ—ä¹‹é—´å·®å¼‚æ˜¾è‘—ã€‚</li>
<li>Meta-Llama-3-8B-Instructæ¨¡å‹åœ¨æ€»ä½“F1åˆ†æ•°ä¸Šè¡¨ç°æœ€ä½³ï¼Œå°¤å…¶åœ¨ä¸Šä¸‹æ–‡ç›¸å…³çš„ç±»åˆ«å¦‚æ€æƒ³å’Œç¤¾äº¤äº’åŠ¨æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚</li>
<li>Bio_ClinicalBERTç²¾åº¦é«˜ä½†å¬å›ç‡ä½ï¼Œåœ¨æŸäº›ç±»åˆ«å¦‚åœ°ç‚¹å’Œæ—¶é—´æ–¹é¢ä¸Llamaè¡¨ç°ç›¸å½“ï¼Œä½†åœ¨æ€æƒ³ã€æƒ…æ„Ÿå’Œç¤¾äº¤äº’åŠ¨ç­‰æ–¹é¢è¡¨ç°ä¸ä½³ã€‚</li>
<li>ä¸ä¼ ç»Ÿä¿¡æ¯æå–ä»»åŠ¡ç›¸æ¯”ï¼Œä»å™è¿°ä¸­æå–è®¤çŸ¥è¿‡ç¨‹æ›´å…·æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºå™è¿°å†…å®¹å…·æœ‰æŠ½è±¡æ€§å’Œé‡å æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.08806">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a70f6f84c3ebd6fe2370ba83c7654cf6" align="middle">
<img src="https://picx.zhimg.com/v2-b29dc4b6672580d626701ca05cd810c0" align="middle">
<img src="https://picx.zhimg.com/v2-41ceea66ef7e96e7ba1b6a86050b4c3f" align="middle">
<img src="https://picx.zhimg.com/v2-8af4966d2a86395f2f06b5cc5db77f1e" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Divide-and-Conquer-Decoupled-Network-for-Cross-Domain-Few-Shot-Segmentation"><a href="#Divide-and-Conquer-Decoupled-Network-for-Cross-Domain-Few-Shot-Segmentation" class="headerlink" title="Divide-and-Conquer Decoupled Network for Cross-Domain Few-Shot Segmentation"></a>Divide-and-Conquer Decoupled Network for Cross-Domain Few-Shot Segmentation</h2><p><strong>Authors:Runmin Cong, Anpeng Wang, Bin Wan, Cong Zhang, Xiaofei Zhou, Wei Zhang</strong></p>
<p>Cross-domain few-shot segmentation (CD-FSS) aims to tackle the dual challenge of recognizing novel classes and adapting to unseen domains with limited annotations. However, encoder features often entangle domain-relevant and category-relevant information, limiting both generalization and rapid adaptation to new domains. To address this issue, we propose a Divide-and-Conquer Decoupled Network (DCDNet). In the training stage, to tackle feature entanglement that impedes cross-domain generalization and rapid adaptation, we propose the Adversarial-Contrastive Feature Decomposition (ACFD) module. It decouples backbone features into category-relevant private and domain-relevant shared representations via contrastive learning and adversarial learning. Then, to mitigate the potential degradation caused by the disentanglement, the Matrix-Guided Dynamic Fusion (MGDF) module adaptively integrates base, shared, and private features under spatial guidance, maintaining structural coherence. In addition, in the fine-tuning stage, to enhanced model generalization, the Cross-Adaptive Modulation (CAM) module is placed before the MGDF, where shared features guide private features via modulation ensuring effective integration of domain-relevant information. Extensive experiments on four challenging datasets show that DCDNet outperforms existing CD-FSS methods, setting a new state-of-the-art for cross-domain generalization and few-shot adaptation.</p>
<blockquote>
<p>è·¨åŸŸå°æ ·æœ¬åˆ†å‰²ï¼ˆCD-FSSï¼‰æ—¨åœ¨åº”å¯¹è¯†åˆ«æ–°ç±»åˆ«å’Œé€‚åº”æœªè§åŸŸçš„åŒæŒ‘æˆ˜ï¼ŒåŒæ—¶ä»…ä½¿ç”¨æœ‰é™çš„æ ‡æ³¨ã€‚ç„¶è€Œï¼Œç¼–ç å™¨ç‰¹å¾ç»å¸¸æ··æ·†ä¸åŸŸç›¸å…³å’Œä¸ç±»åˆ«ç›¸å…³çš„ä¿¡æ¯ï¼Œé™åˆ¶äº†æ³›åŒ–å’Œå¿«é€Ÿé€‚åº”æ–°åŸŸçš„èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åˆ†è€Œæ²»ä¹‹è§£è€¦ç½‘ç»œï¼ˆDCDNetï¼‰ã€‚åœ¨è®­ç»ƒé˜¶æ®µï¼Œä¸ºäº†è§£å†³é˜»ç¢è·¨åŸŸæ³›åŒ–å’Œå¿«é€Ÿé€‚åº”çš„ç‰¹å¾çº ç¼ é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†å¯¹æŠ—æ€§å¯¹æ¯”ç‰¹å¾åˆ†è§£ï¼ˆACFDï¼‰æ¨¡å—ã€‚å®ƒé€šè¿‡å¯¹æ¯”å­¦ä¹ å’Œå¯¹æŠ—å­¦ä¹ å°†ä¸»å¹²ç‰¹å¾è§£è€¦ä¸ºä¸ç±»åˆ«ç›¸å…³çš„ç§æœ‰è¡¨ç¤ºå’Œä¸åŸŸç›¸å…³çš„å…±äº«è¡¨ç¤ºã€‚ç„¶åï¼Œä¸ºäº†å‡è½»è§£è€¦å¯èƒ½å¸¦æ¥çš„æ½œåœ¨é€€åŒ–é—®é¢˜ï¼ŒçŸ©é˜µå¼•å¯¼åŠ¨æ€èåˆï¼ˆMGDFï¼‰æ¨¡å—åœ¨ç©ºé—´å¼•å¯¼ä¸‹è‡ªé€‚åº”åœ°èåˆåŸºç¡€ã€å…±äº«å’Œç§æœ‰ç‰¹å¾ï¼Œä¿æŒç»“æ„è¿è´¯æ€§ã€‚æ­¤å¤–ï¼Œåœ¨å¾®è°ƒé˜¶æ®µï¼Œä¸ºäº†å¢å¼ºæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œåœ¨MGDFä¹‹å‰æ”¾ç½®äº†è·¨è‡ªé€‚åº”è°ƒåˆ¶ï¼ˆCAMï¼‰æ¨¡å—ï¼Œå…¶ä¸­å…±äº«ç‰¹å¾é€šè¿‡è°ƒåˆ¶å¼•å¯¼ç§æœ‰ç‰¹å¾ï¼Œç¡®ä¿æœ‰æ•ˆæ•´åˆåŸŸç›¸å…³ä¿¡æ¯ã€‚åœ¨å››ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒDCDNetè¶…è¶Šäº†ç°æœ‰çš„CD-FSSæ–¹æ³•ï¼Œä¸ºè·¨åŸŸæ³›åŒ–å’Œå°æ ·æœ¬é€‚åº”è®¾å®šäº†æ–°çš„æœ€æ–°æŠ€æœ¯çŠ¶æ€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.07798v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºè·¨åŸŸå°æ ·æœ¬åˆ†å‰²ï¼ˆCD-FSSï¼‰é¢ä¸´çš„æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºDCDNetçš„åˆ†å‰²ç½‘ç»œï¼Œæ—¨åœ¨è§£å†³æ¨¡å‹åœ¨æ–°é¢†åŸŸä¸­çš„å¿«é€Ÿé€‚åº”å’Œæ³›åŒ–é—®é¢˜ã€‚é€šè¿‡è®­ç»ƒé˜¶æ®µçš„å¯¹æŠ—æ€§å¯¹æ¯”ç‰¹å¾åˆ†è§£ï¼ˆACFDï¼‰æ¨¡å—å’ŒçŸ©é˜µå¼•å¯¼åŠ¨æ€èåˆï¼ˆMGDFï¼‰æ¨¡å—ï¼Œä»¥åŠå¾®è°ƒé˜¶æ®µçš„è·¨è‡ªé€‚åº”è°ƒåˆ¶ï¼ˆCAMï¼‰æ¨¡å—ï¼ŒDCDNetèƒ½å¤Ÿåœ¨æœ‰é™æ ‡æ³¨çš„æƒ…å†µä¸‹å®ç°ç±»åˆ«è¯†åˆ«å’Œé¢†åŸŸé€‚åº”çš„è§£è€¦ã€‚åœ¨å››ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDCDNetåœ¨è·¨åŸŸæ³›åŒ–å’Œå°æ ·æœ¬é€‚åº”æ–¹é¢è¾¾åˆ°äº†æ–°çš„å…ˆè¿›æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DCDNeté’ˆå¯¹è·¨åŸŸå°æ ·æœ¬åˆ†å‰²çš„æŒ‘æˆ˜è€Œè®¾è®¡ï¼Œè§£å†³äº†æ¨¡å‹åœ¨æ–°é¢†åŸŸçš„å¿«é€Ÿé€‚åº”å’Œæ³›åŒ–é—®é¢˜ã€‚</li>
<li>ACFDæ¨¡å—é€šè¿‡å¯¹æ¯”å­¦ä¹ å’Œå¯¹æŠ—æ€§å­¦ä¹ å°†ç‰¹å¾åˆ†è§£ä¸ºç±»åˆ«ç›¸å…³ç§æœ‰å’Œé¢†åŸŸç›¸å…³å…±äº«è¡¨ç¤ºã€‚</li>
<li>MGDFæ¨¡å—åœ¨ç©ºé—´å¼•å¯¼ä¸‹è‡ªé€‚åº”èåˆåŸºç¡€ã€å…±äº«å’Œç§æœ‰ç‰¹å¾ï¼Œä¿æŒç»“æ„è¿è´¯æ€§ã€‚</li>
<li>CAMæ¨¡å—åœ¨å¾®è°ƒé˜¶æ®µå¢å¼ºäº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé€šè¿‡è°ƒåˆ¶å…±äº«ç‰¹å¾æ¥å¼•å¯¼ç§æœ‰ç‰¹å¾ã€‚</li>
<li>DCDNeté€šè¿‡è®­ç»ƒé˜¶æ®µçš„ç‰¹å¾åˆ†è§£å’Œæ¨¡å—èåˆï¼Œä»¥åŠå¾®è°ƒé˜¶æ®µçš„è‡ªé€‚åº”è°ƒåˆ¶ï¼Œå®ç°äº†æœ‰æ•ˆçš„è·¨åŸŸæ³›åŒ–å’Œå°æ ·æœ¬é€‚åº”ã€‚</li>
<li>åœ¨å››ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDCDNetçš„æ€§èƒ½è¶…è¿‡äº†ç°æœ‰çš„CD-FSSæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.07798">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5ee8faa6d9739aef45cfb3929284f11f" align="middle">
<img src="https://picx.zhimg.com/v2-65e1c543d2dd9378eb7e3300b5f152d1" align="middle">
<img src="https://picx.zhimg.com/v2-16656edf90ac37267650c175a2cf20c4" align="middle">
<img src="https://picx.zhimg.com/v2-aa311621b9da8952eec3ea635e8fd660" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="HCFSLN-Adaptive-Hyperbolic-Few-Shot-Learning-for-Multimodal-Anxiety-Detection"><a href="#HCFSLN-Adaptive-Hyperbolic-Few-Shot-Learning-for-Multimodal-Anxiety-Detection" class="headerlink" title="HCFSLN: Adaptive Hyperbolic Few-Shot Learning for Multimodal Anxiety Detection"></a>HCFSLN: Adaptive Hyperbolic Few-Shot Learning for Multimodal Anxiety Detection</h2><p><strong>Authors:Aditya Sneh, Nilesh Kumar Sahu, Anushka Sanjay Shelke, Arya Adyasha, Haroon R. Lone</strong></p>
<p>Anxiety disorders impact millions globally, yet traditional diagnosis relies on clinical interviews, while machine learning models struggle with overfitting due to limited data. Large-scale data collection remains costly and time-consuming, restricting accessibility. To address this, we introduce the Hyperbolic Curvature Few-Shot Learning Network (HCFSLN), a novel Few-Shot Learning (FSL) framework for multimodal anxiety detection, integrating speech, physiological signals, and video data. HCFSLN enhances feature separability through hyperbolic embeddings, cross-modal attention, and an adaptive gating network, enabling robust classification with minimal data. We collected a multimodal anxiety dataset from 108 participants and benchmarked HCFSLN against six FSL baselines, achieving 88% accuracy, outperforming the best baseline by 14%. These results highlight the effectiveness of hyperbolic space for modeling anxiety-related speech patterns and demonstrate FSLâ€™s potential for anxiety classification.</p>
<blockquote>
<p>ç„¦è™‘éšœç¢å½±å“å…¨çƒæ•°ç™¾ä¸‡äººï¼Œç„¶è€Œä¼ ç»Ÿè¯Šæ–­ä¾èµ–äºä¸´åºŠè®¿è°ˆï¼Œè€Œæœºå™¨å­¦ä¹ æ¨¡å‹å› æ•°æ®æœ‰é™è€Œé¢ä¸´è¿‡æ‹Ÿåˆé—®é¢˜ã€‚å¤§è§„æ¨¡æ•°æ®é‡‡é›†ä»ç„¶æˆæœ¬é«˜æ˜‚ã€è€—æ—¶è¿‡é•¿ï¼Œé™åˆ¶äº†å¯è®¿é—®æ€§ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŒæ›²æ›²é¢æ›²ç‡å°æ ·æœ¬å­¦ä¹ ç½‘ç»œï¼ˆHCFSLNï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºå¤šæ¨¡å¼ç„¦è™‘æ£€æµ‹çš„æ–°å‹å°æ ·æœ¬å­¦ä¹ ï¼ˆFSLï¼‰æ¡†æ¶ï¼Œèåˆäº†è¯­éŸ³ã€ç”Ÿç†ä¿¡å·å’Œè§†é¢‘æ•°æ®ã€‚HCFSLNé€šè¿‡åŒæ›²åµŒå…¥ã€è·¨æ¨¡æ€æ³¨æ„åŠ›å’Œè‡ªé€‚åº”é—¨æ§ç½‘ç»œæé«˜ç‰¹å¾å¯åˆ†æ€§ï¼Œåœ¨å°‘é‡æ•°æ®çš„æƒ…å†µä¸‹å®ç°ç¨³å¥çš„åˆ†ç±»ã€‚æˆ‘ä»¬ä»108åå‚ä¸è€…ä¸­æ”¶é›†äº†ä¸€ä¸ªå¤šæ¨¡å¼ç„¦è™‘æ•°æ®é›†ï¼Œå¹¶å°†HCFSLNä¸å…­ä¸ªFSLåŸºå‡†è¿›è¡Œäº†æ¯”è¾ƒï¼Œå‡†ç¡®ç‡ä¸º88%ï¼Œæ¯”æœ€ä½³åŸºå‡†é«˜å‡º14%ã€‚è¿™äº›ç»“æœçªæ˜¾äº†åŒæ›²ç©ºé—´åœ¨æ¨¡æ‹Ÿç„¦è™‘ç›¸å…³è¯­éŸ³æ¨¡å¼æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œå¹¶å±•ç¤ºäº†FSLåœ¨ç„¦è™‘åˆ†ç±»æ–¹é¢çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.06988v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦ä»‹ç»äº†Hyperbolic Curvature Few-Shot Learning Networkï¼ˆHCFSLNï¼‰è¿™ä¸€æ–°å‹Few-Shot Learningï¼ˆFSLï¼‰æ¡†æ¶ï¼Œç”¨äºå¤šæ¨¡æ€ç„¦è™‘æ£€æµ‹ã€‚è¯¥æ¡†æ¶ç»“åˆäº†è¯­éŸ³ã€ç”Ÿç†ä¿¡å·å’Œè§†é¢‘æ•°æ®ï¼Œé€šè¿‡å¢å¼ºç‰¹å¾å¯åˆ†æ€§ï¼Œå®ç°äº†å°‘é‡æ•°æ®ä¸‹çš„ç¨³å¥åˆ†ç±»ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒHCFSLNåœ¨ç„¦è™‘åˆ†ç±»æ–¹é¢è¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ï¼Œå‡†ç¡®ç‡è¾¾åˆ°äº†88%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†ä¸€ç§æ–°å‹çš„Few-Shot Learningæ¡†æ¶â€”â€”Hyperbolic Curvature Few-Shot Learning Networkï¼ˆHCFSLNï¼‰ï¼Œç”¨äºå¤šæ¨¡æ€ç„¦è™‘æ£€æµ‹ã€‚</li>
<li>HCFSLNé›†æˆäº†è¯­éŸ³ã€ç”Ÿç†ä¿¡å·å’Œè§†é¢‘æ•°æ®ï¼Œæé«˜äº†ç‰¹å¾å¯åˆ†æ€§ã€‚</li>
<li>é€šè¿‡å¼•å…¥è¶…çƒé¢åµŒå…¥ã€è·¨æ¨¡æ€æ³¨æ„åŠ›å’Œè‡ªé€‚åº”é—¨æ§ç½‘ç»œï¼ŒHCFSLNå®ç°äº†åœ¨å°‘é‡æ•°æ®ä¸‹çš„ç¨³å¥åˆ†ç±»ã€‚</li>
<li>å»ºç«‹äº†åŒ…å«108åå‚ä¸è€…çš„å¤šæ¨¡æ€ç„¦è™‘æ•°æ®é›†ã€‚</li>
<li>HCFSLNä¸å…­ç§FSLåŸºçº¿æ¨¡å‹è¿›è¡Œäº†æ¯”è¾ƒï¼Œå‡†ç¡®ç‡è¾¾åˆ°äº†88%ã€‚</li>
<li>HCFSLNç›¸æ¯”æœ€ä½³åŸºçº¿æ¨¡å‹æ€§èƒ½æå‡äº†14%ï¼Œå‡¸æ˜¾äº†è¶…æ›²é¢ç©ºé—´åœ¨ç„¦è™‘ç›¸å…³è¯­éŸ³æ¨¡å¼å»ºæ¨¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.06988">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-367505c6f38ab0e02d84e73fa621bf53" align="middle">
<img src="https://picx.zhimg.com/v2-2f5438c19baa77fc270b7e3ca910dba8" align="middle">
<img src="https://picx.zhimg.com/v2-c0f686ccf84b7d34ed0073948b16015a" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Otter-Mitigating-Background-Distractions-of-Wide-Angle-Few-Shot-Action-Recognition-with-Enhanced-RWKV"><a href="#Otter-Mitigating-Background-Distractions-of-Wide-Angle-Few-Shot-Action-Recognition-with-Enhanced-RWKV" class="headerlink" title="Otter: Mitigating Background Distractions of Wide-Angle Few-Shot Action Recognition with Enhanced RWKV"></a>Otter: Mitigating Background Distractions of Wide-Angle Few-Shot Action Recognition with Enhanced RWKV</h2><p><strong>Authors:Wenbo Huang, Jinghui Zhang, Zhenghao Chen, Guang Li, Lei Zhang, Yang Cao, Fang Dong, Takahiro Ogawa, Miki Haseyama</strong></p>
<p>Wide-angle videos in few-shot action recognition (FSAR) effectively express actions within specific scenarios. However, without a global understanding of both subjects and background, recognizing actions in such samples remains challenging because of the background distractions. Receptance Weighted Key Value (RWKV), which learns interaction between various dimensions, shows promise for global modeling. While directly applying RWKV to wide-angle FSAR may fail to highlight subjects due to excessive background information. Additionally, temporal relation degraded by frames with similar backgrounds is difficult to reconstruct, further impacting performance. Therefore, we design the CompOund SegmenTation and Temporal REconstructing RWKV (Otter). Specifically, the Compound Segmentation Module~(CSM) is devised to segment and emphasize key patches in each frame, effectively highlighting subjects against background information. The Temporal Reconstruction Module (TRM) is incorporated into the temporal-enhanced prototype construction to enable bidirectional scanning, allowing better reconstruct temporal relation. Furthermore, a regular prototype is combined with the temporal-enhanced prototype to simultaneously enhance subject emphasis and temporal modeling, improving wide-angle FSAR performance. Extensive experiments on benchmarks such as SSv2, Kinetics, UCF101, and HMDB51 demonstrate that Otter achieves state-of-the-art performance. Extra evaluation on the VideoBadminton dataset further validates the superiority of Otter in wide-angle FSAR.</p>
<blockquote>
<p>åœ¨å°‘æ•°åŠ¨ä½œè¯†åˆ«ï¼ˆFSARï¼‰ä¸­ï¼Œå¹¿è§’è§†é¢‘èƒ½å¤Ÿæœ‰æ•ˆåœ°è¡¨è¾¾ç‰¹å®šåœºæ™¯ä¸‹çš„åŠ¨ä½œã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹ä¸»ä½“å’ŒèƒŒæ™¯çš„å…¨å±€ç†è§£ï¼Œè¯†åˆ«æ­¤ç±»æ ·æœ¬ä¸­çš„åŠ¨ä½œä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºèƒŒæ™¯å¹²æ‰°ä¼šå¯¼è‡´è¯†åˆ«å›°éš¾ã€‚æ¥æ”¶åŠ æƒé”®å€¼ï¼ˆRWKVï¼‰é€šè¿‡å­¦ä¹ ä¸åŒç»´åº¦ä¹‹é—´çš„äº¤äº’ï¼Œåœ¨å…¨å±€å»ºæ¨¡æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ã€‚ç„¶è€Œï¼Œç›´æ¥å°†RWKVåº”ç”¨äºå¹¿è§’FSARå¯èƒ½ä¼šå› è¿‡å¤šçš„èƒŒæ™¯ä¿¡æ¯è€Œæ— æ³•çªå‡ºä¸»ä½“ã€‚æ­¤å¤–ï¼Œç”±äºå…·æœ‰ç›¸ä¼¼èƒŒæ™¯çš„å¸§å¯¼è‡´çš„æ—¶åºå…³ç³»é€€åŒ–éš¾ä»¥é‡å»ºï¼Œè¿›ä¸€æ­¥å½±å“äº†æ€§èƒ½ã€‚å› æ­¤ï¼Œæˆ‘ä»¬è®¾è®¡äº†åŒ–åˆç‰©åˆ†æ®µå’Œæ—¶åºé‡å»ºRWKVï¼ˆOtterï¼‰ã€‚å…·ä½“æ¥è¯´ï¼ŒåŒ–åˆç‰©åˆ†æ®µæ¨¡å—ï¼ˆCSMï¼‰æ—¨åœ¨åˆ†å‰²å¹¶å¼ºè°ƒæ¯ä¸€å¸§ä¸­çš„å…³é”®è¡¥ä¸ï¼Œæœ‰æ•ˆåœ°å°†ä¸»ä½“ä¸èƒŒæ™¯ä¿¡æ¯çªå‡ºå¯¹æ¯”ã€‚æ—¶åºé‡å»ºæ¨¡å—ï¼ˆTRMï¼‰è¢«çº³å…¥æ—¶åºå¢å¼ºåŸå‹æ„å»ºä¸­ï¼Œä»¥å®ç°åŒå‘æ‰«æï¼Œä»è€Œæ›´å¥½åœ°é‡å»ºæ—¶åºå…³ç³»ã€‚æ­¤å¤–ï¼Œå°†å¸¸è§„åŸå‹ä¸æ—¶åºå¢å¼ºåŸå‹ç›¸ç»“åˆï¼Œå¯ä»¥åŒæ—¶å¢å¼ºä¸»ä½“çªå‡ºå’Œæ—¶åºå»ºæ¨¡ï¼Œæé«˜å¹¿è§’FSARçš„æ€§èƒ½ã€‚åœ¨SSv2ã€Kineticsã€UCF101å’ŒHMDB51ç­‰åŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒOtterè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚åœ¨VideoBadmintonæ•°æ®é›†ä¸Šçš„é¢å¤–è¯„ä¼°è¿›ä¸€æ­¥éªŒè¯äº†Otteråœ¨å¹¿è§’FSARä¸­çš„ä¼˜è¶Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.06741v2">PDF</a> Accepted by AAAI 2026 Oral</p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡ç ”ç©¶äº†åŸºäºå¹¿è§’è§†é¢‘çš„å°‘æ ·æœ¬åŠ¨ä½œè¯†åˆ«ï¼ˆFSARï¼‰é—®é¢˜ã€‚å°½ç®¡å¹¿æ³›ä½¿ç”¨çš„RWKVæ¨¡å‹å¯ä»¥å­¦ä¹ å„ç§ç»´åº¦é—´çš„äº¤äº’ä½œç”¨ä»¥å®ç°å…¨å±€å»ºæ¨¡ï¼Œä½†ç”±äºç¼ºå°‘å¯¹èƒŒæ™¯å’Œä¸»é¢˜çš„å…¨é¢ç†è§£ï¼Œç›´æ¥åœ¨FSARä¸­åº”ç”¨å¯èƒ½ä¼šå¯¼è‡´è¯†åˆ«æ•ˆæœå—é™ã€‚ä¸ºæ­¤ï¼Œæ–‡ç« æå‡ºäº†ä¸€ä¸ªåä¸ºOtterçš„æ–°æ–¹æ³•ï¼Œå…¶ä¸­åŒ…æ‹¬CSMï¼ˆå¤åˆåˆ†æ®µæ¨¡å—ï¼‰å’ŒTRMï¼ˆæ—¶é—´é‡å»ºæ¨¡å—ï¼‰ã€‚CSMèƒ½å¤Ÿåˆ†å‰²å¹¶å¼ºè°ƒå…³é”®å¸§ä¸­çš„å…³é”®åŒºåŸŸï¼Œæœ‰æ•ˆçªå‡ºä¸»é¢˜ä¿¡æ¯ï¼›è€ŒTRMåˆ™é€šè¿‡æ„å»ºæ—¶é—´å¢å¼ºåŸå‹æ¥å®ç°åŒå‘æ‰«æï¼Œä»è€Œé‡å»ºæ—¶é—´å…³ç³»ã€‚åŒæ—¶ï¼Œé€šè¿‡å¯¹å¸¸è§„åŸå‹å’Œæ—¶é—´å¢å¼ºåŸå‹çš„ç»“åˆï¼Œæ—¢èƒ½çªå‡ºä¸»é¢˜åˆèƒ½å¼ºåŒ–æ—¶é—´å»ºæ¨¡ï¼Œæå‡äº†å®½è§†è§’FSARçš„æ€§èƒ½ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒOtteræ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>Wide-angleè§†é¢‘åœ¨å°‘æ ·æœ¬åŠ¨ä½œè¯†åˆ«ä¸­èƒ½æœ‰æ•ˆè¡¨è¾¾ç‰¹å®šåœºæ™¯çš„åŠ¨ä½œï¼Œä½†èƒŒæ™¯å¹²æ‰°ä½¿å¾—åŠ¨ä½œè¯†åˆ«å˜å¾—å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>RWKVæ¨¡å‹åœ¨å…¨å±€å»ºæ¨¡æ–¹é¢è¡¨ç°å‡ºæ½œåŠ›ï¼Œä½†ç›´æ¥åº”ç”¨äºå®½è§†è§’FSARå¯èƒ½æ— æ³•çªå‡ºä¸»é¢˜ä¿¡æ¯ã€‚</li>
<li>Otteræ–¹æ³•é€šè¿‡CSMå’ŒTRMè§£å†³ä¸Šè¿°é—®é¢˜ï¼ŒCSMèƒ½çªå‡ºå…³é”®åŒºåŸŸï¼ŒTRMé‡å»ºæ—¶é—´å…³ç³»ã€‚</li>
<li>Otteræ–¹æ³•ç»“åˆäº†å¸¸è§„åŸå‹å’Œæ—¶é—´å¢å¼ºåŸå‹ï¼Œå¢å¼ºäº†ä¸»é¢˜çªå‡ºå’Œæ—¶é—´å»ºæ¨¡æ•ˆæœã€‚</li>
<li>Otteræ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå®ç°äº†æœ€ä½³æ€§èƒ½ã€‚</li>
<li>å®½è§†è§’çš„è§†é¢‘èƒŒæ™¯ä¸‹åŠ¨ä½œè¯†åˆ«çš„éš¾ç‚¹åŒ…æ‹¬ä¸»ä½“ä¸èƒŒæ™¯åŒºåˆ†ä¸æ˜ç¡®ä»¥åŠæ—¶é—´å…³ç³»çš„é‡å»ºç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.06741">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-098f875cadb3ef4a3bd65fce1e869487" align="middle">
<img src="https://picx.zhimg.com/v2-08b2aee00667c0613aaabef14f11337a" align="middle">
<img src="https://picx.zhimg.com/v2-d9b1b6c9f849ab553e8d3c54e90a5403" align="middle">
<img src="https://picx.zhimg.com/v2-9c3ac8a20795b77cab4b41791e1ddac8" align="middle">
<img src="https://picx.zhimg.com/v2-d1460fe0acf0e75b22873ce48d5af607" align="middle">
<img src="https://picx.zhimg.com/v2-970f893b5010db3f65388961b08517b0" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="FreqGRL-Suppressing-Low-Frequency-Bias-and-Mining-High-Frequency-Knowledge-for-Cross-Domain-Few-Shot-Learning"><a href="#FreqGRL-Suppressing-Low-Frequency-Bias-and-Mining-High-Frequency-Knowledge-for-Cross-Domain-Few-Shot-Learning" class="headerlink" title="FreqGRL: Suppressing Low-Frequency Bias and Mining High-Frequency Knowledge for Cross-Domain Few-Shot Learning"></a>FreqGRL: Suppressing Low-Frequency Bias and Mining High-Frequency Knowledge for Cross-Domain Few-Shot Learning</h2><p><strong>Authors:Siqi Hui, Sanping Zhou, Ye deng, Wenli Huang, Jinjun Wang</strong></p>
<p>Cross-domain few-shot learning (CD-FSL) aims to recognize novel classes with only a few labeled examples under significant domain shifts. While recent approaches leverage a limited amount of labeled target-domain data to improve performance, the severe imbalance between abundant source data and scarce target data remains a critical challenge for effective representation learning. We present the first frequency-space perspective to analyze this issue and identify two key challenges: (1) models are easily biased toward source-specific knowledge encoded in the low-frequency components of source data, and (2) the sparsity of target data hinders the learning of high-frequency, domain-generalizable features. To address these challenges, we propose \textbf{FreqGRL}, a novel CD-FSL framework that mitigates the impact of data imbalance in the frequency space. Specifically, we introduce a Low-Frequency Replacement (LFR) module that substitutes the low-frequency components of source tasks with those from the target domain to create new source tasks that better align with target characteristics, thus reducing source-specific biases and promoting generalizable representation learning. We further design a High-Frequency Enhancement (HFE) module that filters out low-frequency components and performs learning directly on high-frequency features in the frequency space to improve cross-domain generalization. Additionally, a Global Frequency Filter (GFF) is incorporated to suppress noisy or irrelevant frequencies and emphasize informative ones, mitigating overfitting risks under limited target supervision. Extensive experiments on five standard CD-FSL benchmarks demonstrate that our frequency-guided framework achieves state-of-the-art performance.</p>
<blockquote>
<p>è·¨åŸŸå°æ ·æœ¬å­¦ä¹ ï¼ˆCD-FSLï¼‰æ—¨åœ¨åœ¨æ˜¾è‘—åŸŸè¿ç§»çš„æƒ…å†µä¸‹ï¼Œä»…é€šè¿‡å°‘é‡æ ‡è®°æ ·æœ¬è¯†åˆ«æ–°å‹ç±»åˆ«ã€‚è™½ç„¶æœ€è¿‘çš„æ–¹æ³•åˆ©ç”¨æœ‰é™çš„æ ‡è®°ç›®æ ‡åŸŸæ•°æ®æ¥æé«˜æ€§èƒ½ï¼Œä½†æºæ•°æ®ä¸°å¯Œä¸ç›®æ ‡æ•°æ®ç¨€ç¼ºä¹‹é—´çš„ä¸¥é‡ä¸å¹³è¡¡ä»ç„¶æ˜¯æœ‰æ•ˆè¡¨ç¤ºå­¦ä¹ çš„å…³é”®æŒ‘æˆ˜ã€‚æˆ‘ä»¬ä»é¢‘ç‡ç©ºé—´çš„è§’åº¦åˆ†æè¿™ä¸ªé—®é¢˜ï¼Œå¹¶è¯†åˆ«å‡ºä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ï¼šï¼ˆ1ï¼‰æ¨¡å‹å¾ˆå®¹æ˜“åå‘äºæºæ•°æ®ä½é¢‘ç»„ä»¶ä¸­ç¼–ç çš„æºç‰¹å®šçŸ¥è¯†ï¼›ï¼ˆ2ï¼‰ç›®æ ‡æ•°æ®çš„ç¨€ç–æ€§é˜»ç¢äº†é«˜é¢‘ã€é€šç”¨é¢†åŸŸç‰¹å¾çš„å­¦ä¹ ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†\textbf{FreqGRL}ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„CD-FSLæ¡†æ¶ï¼Œå‡è½»äº†é¢‘ç‡ç©ºé—´ä¸­æ•°æ®ä¸å¹³è¡¡çš„å½±å“ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªä½é¢‘æ›¿æ¢ï¼ˆLFRï¼‰æ¨¡å—ï¼Œç”¨ç›®æ ‡åŸŸçš„ä½é¢‘ç»„ä»¶æ›¿æ¢æºä»»åŠ¡ä¸­çš„ä½é¢‘ç»„ä»¶ï¼Œä»¥åˆ›å»ºä¸ç›®æ ‡ç‰¹æ€§æ›´ç›¸ç¬¦çš„æ–°æºä»»åŠ¡ï¼Œä»è€Œå‡å°‘æºç‰¹å®šåè§å¹¶ä¿ƒè¿›é€šç”¨è¡¨ç¤ºå­¦ä¹ ã€‚æˆ‘ä»¬è¿˜è®¾è®¡äº†ä¸€ä¸ªé«˜é¢‘å¢å¼ºï¼ˆHFEï¼‰æ¨¡å—ï¼Œè¯¥æ¨¡å—å¯ä»¥è¿‡æ»¤æ‰ä½é¢‘ç»„ä»¶ï¼Œç›´æ¥åœ¨é¢‘ç‡ç©ºé—´ä¸­å¯¹é«˜é¢‘ç‰¹å¾è¿›è¡Œå­¦ä¹ ï¼Œä»¥æé«˜è·¨åŸŸæ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¿˜ç»“åˆäº†å…¨å±€é¢‘ç‡æ»¤æ³¢å™¨ï¼ˆGFFï¼‰æ¥æŠ‘åˆ¶å™ªå£°æˆ–æ— å…³çš„é¢‘ç‡ï¼Œå¹¶å¼ºè°ƒæœ‰æ„ä¹‰çš„ä¸€ä¸ªï¼Œä»¥åœ¨æœ‰é™çš„ç›®æ ‡ç›‘ç£ä¸‹å‡å°‘è¿‡æ‹Ÿåˆé£é™©ã€‚åœ¨äº”ä¸ªæ ‡å‡†çš„CD-FSLåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„é¢‘ç‡å¼•å¯¼æ¡†æ¶è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.06648v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è·¨åŸŸå°æ ·å­¦ä¹ ï¼ˆCD-FSLï¼‰é¢†åŸŸçš„ä¸€ä¸ªé‡è¦æŒ‘æˆ˜ï¼Œå³åœ¨æ˜¾è‘—åŸŸè¿ç§»ä¸‹å¦‚ä½•ä»…é€šè¿‡å°‘é‡æ ‡è®°æ ·æœ¬è¯†åˆ«æ–°å‹ç±»åˆ«ã€‚ä¸ºè§£å†³æºæ•°æ®é‡å¤§è€Œç›®æ ‡æ•°æ®é‡ç¨€ç¼ºçš„ä¸å¹³è¡¡é—®é¢˜ï¼Œæ–‡ç« é¦–æ¬¡ä»é¢‘ç‡ç©ºé—´è§’åº¦è¿›è¡Œåˆ†æï¼Œå¹¶æå‡ºä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ã€‚ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæå‡ºäº†åä¸ºFreqGRLçš„æ–°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åŒ…æ‹¬ä½é¢‘æ›¿ä»£ï¼ˆLFRï¼‰ã€é«˜é¢‘å¢å¼ºï¼ˆHFEï¼‰å’Œå…¨å±€é¢‘ç‡æ»¤æ³¢å™¨ï¼ˆGFFï¼‰ç­‰æ¨¡å—ï¼Œä»¥æé«˜è·¨åŸŸæ³›åŒ–èƒ½åŠ›ï¼Œå®ç°å°‘æ ·æœ¬ä¸‹çš„æ–°å‹ç±»åˆ«è¯†åˆ«ã€‚é€šè¿‡å¤§é‡å®éªŒéªŒè¯ï¼Œè¯¥é¢‘ç‡å¼•å¯¼æ¡†æ¶åœ¨äº”ä¸ªæ ‡å‡†CD-FSLåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CD-FSLé¢ä¸´åœ¨æ˜¾è‘—åŸŸè¿ç§»ä¸‹ä»…é€šè¿‡å°‘é‡æ ‡è®°æ ·æœ¬è¯†åˆ«æ–°å‹ç±»åˆ«çš„æŒ‘æˆ˜ã€‚</li>
<li>æ–‡ç« é¦–æ¬¡ä»é¢‘ç‡ç©ºé—´è§’åº¦åˆ†æäº†æºæ•°æ®é‡å¤§è€Œç›®æ ‡æ•°æ®é‡ç¨€ç¼ºçš„é—®é¢˜ã€‚</li>
<li>æ–‡ç« æå‡ºäº†ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ï¼šæ¨¡å‹æ˜“åå‘æºæ•°æ®çš„ä½é¢‘éƒ¨åˆ†çŸ¥è¯†ï¼Œç›®æ ‡æ•°æ®çš„ç¨€ç¼ºæ€§é˜»ç¢äº†é«˜é¢‘ã€é€šç”¨ç‰¹å¾çš„å­¦ä¹ ã€‚</li>
<li>ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæ–‡ç« æå‡ºäº†åä¸ºFreqGRLçš„æ–°æ¡†æ¶ï¼ŒåŒ…æ‹¬ä½é¢‘æ›¿ä»£ï¼ˆLFRï¼‰ã€é«˜é¢‘å¢å¼ºï¼ˆHFEï¼‰å’Œå…¨å±€é¢‘ç‡æ»¤æ³¢å™¨ï¼ˆGFFï¼‰ç­‰æ¨¡å—ã€‚</li>
<li>LFRæ¨¡å—é€šè¿‡æ›¿æ¢æºä»»åŠ¡çš„ä½é¢‘éƒ¨åˆ†æ¥åˆ›å»ºæ›´ç¬¦åˆç›®æ ‡ç‰¹æ€§çš„æ–°æºä»»åŠ¡ï¼Œä»è€Œå‡å°‘ç‰¹å®šæºçš„åè§å¹¶ä¿ƒè¿›é€šç”¨è¡¨ç¤ºå­¦ä¹ ã€‚</li>
<li>HFEæ¨¡å—åœ¨é¢‘ç‡ç©ºé—´ä¸­å¯¹é«˜é¢‘ç‰¹å¾è¿›è¡Œç›´æ¥å­¦ä¹ ï¼Œä»è€Œæé«˜è·¨åŸŸæ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.06648">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ee13e4be65ed9df9a54bb10b4ce27cf3" align="middle">
<img src="https://picx.zhimg.com/v2-159e66152aefaaad56a1b336085049ec" align="middle">
<img src="https://picx.zhimg.com/v2-5c3e11303e9f25b18abab5cfabcfb882" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="MT-HuBERT-Self-Supervised-Mix-Training-for-Few-Shot-Keyword-Spotting-in-Mixed-Speech"><a href="#MT-HuBERT-Self-Supervised-Mix-Training-for-Few-Shot-Keyword-Spotting-in-Mixed-Speech" class="headerlink" title="MT-HuBERT: Self-Supervised Mix-Training for Few-Shot Keyword Spotting in Mixed Speech"></a>MT-HuBERT: Self-Supervised Mix-Training for Few-Shot Keyword Spotting in Mixed Speech</h2><p><strong>Authors:Junming Yuan, Ying Shi, Dong Wang, Lantian Li, Askar Hamdulla</strong></p>
<p>Few-shot keyword spotting aims to detect previously unseen keywords with very limited labeled samples. A pre-training and adaptation paradigm is typically adopted for this task. While effective in clean conditions, most existing approaches struggle with mixed keyword spottingâ€“detecting multiple overlapping keywords within a single utteranceâ€“a capability essential for real-world applications. We have previously proposed a pre-training approach based on Mix-Training (MT) to tackle the mixed keyword detection problem and demonstrated its efficiency. However, this approach is fully supervised, unable to utilize vast unlabeled data. To this end, we propose Mix-Training HuBERT (MT-HuBERT), a self-supervised learning (SSL) pre-training framework that implements the MT criterion during pre-training. MT-HuBERT predicts, in a self-supervised manner, the clean acoustic units of each constituent signal from contextual cues, in contrast to predicting compositional patterns of mixed speech. Experiments conducted on the Google Speech Commands (GSC v2) corpus demonstrate that our proposed MT-HuBERT consistently outperforms several state-of-the-art baselines in few-shot KWS tasks under both mixed and clean conditions.</p>
<blockquote>
<p>å°‘é‡æ ·æœ¬å…³é”®è¯è¯†åˆ«çš„ç›®æ ‡æ˜¯åœ¨éå¸¸æœ‰é™çš„æ ‡æ³¨æ ·æœ¬ä¸‹æ£€æµ‹ä»¥å‰æœªå‡ºç°çš„å…³é”®è¯ã€‚æ­¤ä»»åŠ¡é€šå¸¸é‡‡ç”¨é¢„è®­ç»ƒå’Œé€‚åº”æ¨¡å¼ã€‚å°½ç®¡åœ¨å¹²å‡€çš„ç¯å¢ƒä¸‹æ•ˆæœå¾ˆå¥½ï¼Œä½†å¤§å¤šæ•°ç°æœ‰æ–¹æ³•éƒ½é¢ä¸´æ··åˆå…³é”®è¯è¯†åˆ«é—®é¢˜â€”â€”åœ¨åŒä¸€å¥è¯å†…è¯†åˆ«å¤šä¸ªé‡å å…³é”®è¯ï¼Œè¿™å¯¹äºå®é™…åº”ç”¨è‡³å…³é‡è¦ã€‚æˆ‘ä»¬ä¹‹å‰æå‡ºäº†åŸºäºæ··åˆè®­ç»ƒï¼ˆMTï¼‰çš„é¢„è®­ç»ƒæ–¹æ³•æ¥è§£å†³æ··åˆå…³é”®è¯æ£€æµ‹é—®é¢˜ï¼Œå¹¶è¯æ˜äº†å…¶æ•ˆç‡ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•æ˜¯å…¨ç›‘ç£çš„ï¼Œæ— æ³•åˆ©ç”¨å¤§é‡çš„æ— æ ‡ç­¾æ•°æ®ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†æ··åˆè®­ç»ƒHuBERTï¼ˆMT-HuBERTï¼‰ï¼Œè¿™æ˜¯ä¸€ç§è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰é¢„è®­ç»ƒæ¡†æ¶ï¼Œåœ¨é¢„è®­ç»ƒè¿‡ç¨‹ä¸­å®ç°äº†MTæ ‡å‡†ã€‚MT-HuBERTä»¥è‡ªç›‘ç£çš„æ–¹å¼é¢„æµ‹æ¯ä¸ªç»„æˆä¿¡å·çš„å¹²å‡€å£°å­¦å•å…ƒï¼Œè€Œä¸æ˜¯é¢„æµ‹æ··åˆè¯­éŸ³çš„ç»„åˆæ¨¡å¼ã€‚åœ¨Googleè¯­éŸ³å‘½ä»¤ï¼ˆGSC v2ï¼‰è¯­æ–™åº“ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„MT-HuBERTåœ¨æ··åˆå’Œå¹²å‡€æ¡ä»¶ä¸‹éƒ½åœ¨å°‘é‡çš„å…³é”®è¯è¯†åˆ«ä»»åŠ¡ä¸­æŒç»­ä¼˜äºå‡ ä¸ªå…ˆè¿›çš„åŸºçº¿æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.06296v1">PDF</a> </p>
<p><strong>Summary</strong><br>     åŸºäºMix-Trainingï¼ˆMTï¼‰çš„é¢„è®­ç»ƒæ–¹æ³•å’Œæ··åˆå…³é”®è¯æ£€æµ‹åœ¨å°‘é‡æ ‡ç­¾æ ·æœ¬ä¸‹çš„å…³é”®å­—è¯è¯†åˆ«ä¸­çš„æ–°æ¡†æ¶MT-HuBERTè¢«æå‡ºã€‚è¯¥æ–¹æ³•é‡‡ç”¨è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰è¿›è¡Œé¢„è®­ç»ƒï¼Œé¢„æµ‹æ¯ä¸ªæ„æˆä¿¡å·çš„æ¸…æ´å£°å­¦å•å…ƒï¼Œè€Œéæ··åˆè¯­éŸ³çš„ç»„åˆæ¨¡å¼ã€‚åœ¨Google Speech Commandsï¼ˆGSC v2ï¼‰è¯­æ–™åº“ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMT-HuBERTåœ¨æ··åˆå’Œæ¸…æ´æ¡ä»¶ä¸‹ï¼Œéƒ½æ˜¾è‘—ä¼˜äºå‡ ç§æœ€å…ˆè¿›çš„å°æ ·æœ¬å…³é”®è¯è¯†åˆ«æŠ€æœ¯çš„åŸºçº¿æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ··åˆå…³é”®è¯è¯†åˆ«èƒ½åŠ›å¯¹çœŸå®åº”ç”¨è‡³å…³é‡è¦ã€‚ç°æœ‰çš„æ–¹æ³•åœ¨å¤„ç†æ··åˆå…³é”®è¯è¯†åˆ«æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>åŸºäºMix-Trainingçš„é¢„è®­ç»ƒç­–ç•¥å·²è¢«ç”¨äºè§£å†³æ··åˆå…³é”®è¯æ£€æµ‹é—®é¢˜ï¼Œä½†ç°æœ‰æ–¹æ³•éœ€è¦å®Œæ•´çš„ç›‘ç£æ•°æ®ã€‚å¼•å…¥å¤§é‡æœªæ ‡è®°æ•°æ®çš„éœ€æ±‚ä¿ƒä½¿ç ”ç©¶è€…è¿›è¡Œæ”¹è¿›ã€‚</li>
<li>é¦–æ¬¡æå‡ºäº†ç»“åˆè‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰çš„Mix-Training HuBERTæ¡†æ¶ï¼ˆMT-HuBERTï¼‰ã€‚æ­¤æ¡†æ¶å¯ä»¥åœ¨é¢„è®­ç»ƒé˜¶æ®µå®ç°Mix-Trainingæ ‡å‡†ã€‚</li>
<li>MT-HuBERTé€šè¿‡ä¸Šä¸‹æ–‡çº¿ç´¢é¢„æµ‹æ¯ä¸ªæ„æˆä¿¡å·çš„æ¸…æ´å£°å­¦å•å…ƒï¼Œè€Œä¸æ˜¯é¢„æµ‹æ··åˆè¯­éŸ³çš„ç»„åˆæ¨¡å¼ã€‚è¿™ä¸€ç­–ç•¥ä½¿å…¶åœ¨å¤æ‚ç¯å¢ƒä¸­æ›´å…·é€‚åº”æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.06296">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-83df34a513e6188526186b83750465e9" align="middle">
<img src="https://picx.zhimg.com/v2-6c6f6a22b00cdcb6f75a4620da062b0e" align="middle">
<img src="https://picx.zhimg.com/v2-63f6eaebbbbcfa5a38a5035b7feeda4e" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Commonality-in-Few-Few-Shot-Multimodal-Anomaly-Detection-via-Hypergraph-Enhanced-Memory"><a href="#Commonality-in-Few-Few-Shot-Multimodal-Anomaly-Detection-via-Hypergraph-Enhanced-Memory" class="headerlink" title="Commonality in Few: Few-Shot Multimodal Anomaly Detection via Hypergraph-Enhanced Memory"></a>Commonality in Few: Few-Shot Multimodal Anomaly Detection via Hypergraph-Enhanced Memory</h2><p><strong>Authors:Yuxuan Lin, Hanjing Yan, Xuan Tong, Yang Chang, Huanzhen Wang, Ziheng Zhou, Shuyong Gao, Yan Wang, Wenqiang Zhang</strong></p>
<p>Few-shot multimodal industrial anomaly detection is a critical yet underexplored task, offering the ability to quickly adapt to complex industrial scenarios. In few-shot settings, insufficient training samples often fail to cover the diverse patterns present in test samples. This challenge can be mitigated by extracting structural commonality from a small number of training samples. In this paper, we propose a novel few-shot unsupervised multimodal industrial anomaly detection method based on structural commonality, CIF (Commonality In Few). To extract intra-class structural information, we employ hypergraphs, which are capable of modeling higher-order correlations, to capture the structural commonality within training samples, and use a memory bank to store this intra-class structural prior. Firstly, we design a semantic-aware hypergraph construction module tailored for single-semantic industrial images, from which we extract common structures to guide the construction of the memory bank. Secondly, we use a training-free hypergraph message passing module to update the visual features of test samples, reducing the distribution gap between test features and features in the memory bank. We further propose a hyperedge-guided memory search module, which utilizes structural information to assist the memory search process and reduce the false positive rate. Experimental results on the MVTec 3D-AD dataset and the Eyecandies dataset show that our method outperforms the state-of-the-art (SOTA) methods in few-shot settings. Code is available at <a target="_blank" rel="noopener" href="https://github.com/Sunny5250/CIF">https://github.com/Sunny5250/CIF</a>.</p>
<blockquote>
<p>å°‘æ ·æœ¬å¤šæ¨¡æ€å·¥ä¸šå¼‚å¸¸æ£€æµ‹æ˜¯ä¸€é¡¹è‡³å…³é‡è¦ä½†å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢çš„ä»»åŠ¡ï¼Œå®ƒå…·å¤‡å¿«é€Ÿé€‚åº”å¤æ‚å·¥ä¸šåœºæ™¯çš„èƒ½åŠ›ã€‚åœ¨å°‘æ ·æœ¬è®¾ç½®ä¸‹ï¼Œè®­ç»ƒæ ·æœ¬çš„ä¸è¶³å¾€å¾€æ— æ³•è¦†ç›–æµ‹è¯•æ ·æœ¬ä¸­å­˜åœ¨çš„å„ç§æ¨¡å¼ã€‚é€šè¿‡ä»å°‘é‡è®­ç»ƒæ ·æœ¬ä¸­æå–ç»“æ„å…±æ€§ï¼Œå¯ä»¥ç¼“è§£è¿™ä¸€æŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºç»“æ„å…±æ€§ã€åä¸ºCIFï¼ˆå°‘æ ·æœ¬ä¸­çš„å…±æ€§ï¼‰çš„å…¨æ–°çš„å°‘æ ·æœ¬æ— ç›‘ç£å¤šæ¨¡æ€å·¥ä¸šå¼‚å¸¸æ£€æµ‹æ–¹æ³•ã€‚ä¸ºäº†æå–ç±»å†…ç»“æ„ä¿¡æ¯ï¼Œæˆ‘ä»¬é‡‡ç”¨è¶…å›¾èƒ½å¤Ÿå»ºæ¨¡é«˜é˜¶å…³è”ï¼Œæ¥æ•æ‰è®­ç»ƒæ ·æœ¬ä¸­çš„ç»“æ„å…±æ€§ï¼Œå¹¶ä½¿ç”¨è®°å¿†åº“æ¥å­˜å‚¨è¿™ç±»ç±»å†…ç»“æ„å…ˆéªŒã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä¸ºå•è¯­ä¹‰å·¥ä¸šå›¾åƒè®¾è®¡äº†è¯­ä¹‰æ„ŸçŸ¥è¶…å›¾æ„å»ºæ¨¡å—ï¼Œä»ä¸­æå–å‡ºé€šç”¨ç»“æ„æ¥å¼•å¯¼è®°å¿†åº“çš„æ„å»ºã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬ä½¿ç”¨æ— éœ€è®­ç»ƒçš„è¶…å›¾æ¶ˆæ¯ä¼ é€’æ¨¡å—æ¥æ›´æ–°æµ‹è¯•æ ·æœ¬çš„è§†è§‰ç‰¹å¾ï¼Œç¼©å°æµ‹è¯•ç‰¹å¾ä¸è®°å¿†åº“ä¸­ç‰¹å¾ä¹‹é—´çš„åˆ†å¸ƒå·®è·ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†è¶…è¾¹å¼•å¯¼çš„è®°å¿†æœç´¢æ¨¡å—ï¼Œåˆ©ç”¨ç»“æ„ä¿¡æ¯æ¥è¾…åŠ©è®°å¿†æœç´¢è¿‡ç¨‹ï¼Œé™ä½è¯¯æŠ¥ç‡ã€‚åœ¨MVTec 3D-ADæ•°æ®é›†å’ŒEyecandiesæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å°‘æ ·æœ¬è®¾ç½®ä¸‹ä¼˜äºæœ€æ–°æŠ€æœ¯æ–¹æ³•ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Sunny5250/CIF%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Sunny5250/CIFæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.05966v1">PDF</a> Accepted by AAAI 2026</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºç»“æ„å…±æ€§ï¼ˆCIFï¼‰çš„å°‘æ•°æ ·æœ¬æ— ç›‘ç£å¤šæ¨¡æ€å·¥ä¸šå¼‚å¸¸æ£€æµ‹æ–¹æ³•ã€‚é€šè¿‡æ„å»ºè¶…å›¾æ¨¡å‹æ•æ‰è®­ç»ƒæ ·æœ¬ä¸­çš„ç»“æ„å…±æ€§ï¼Œå¹¶å­˜å‚¨äºå†…å­˜é“¶è¡Œä¸­ã€‚åˆ©ç”¨è¶…å›¾æ¶ˆæ¯ä¼ é€’æ¨¡å—æ›´æ–°æµ‹è¯•æ ·æœ¬çš„è§†è§‰ç‰¹å¾ï¼Œç¼©å°æµ‹è¯•ç‰¹å¾ä¸å†…å­˜é“¶è¡Œç‰¹å¾é—´çš„åˆ†å¸ƒå·®è·ã€‚åŒæ—¶ï¼Œé€šè¿‡è¶…è¾¹å¼•å¯¼çš„å†…å­˜æœç´¢æ¨¡å—é™ä½è¯¯æŠ¥ç‡ã€‚åœ¨MVTec 3D-ADå’ŒEyecandiesæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å°‘æ•°æ ·æœ¬æ¡ä»¶ä¸‹ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºåŸºäºç»“æ„å…±æ€§çš„å°‘æ•°æ ·æœ¬æ— ç›‘ç£å¤šæ¨¡æ€å·¥ä¸šå¼‚å¸¸æ£€æµ‹æ–¹æ³•ã€‚</li>
<li>ä½¿ç”¨è¶…å›¾æ¨¡å‹æ•æ‰è®­ç»ƒæ ·æœ¬ä¸­çš„ç»“æ„å…±æ€§ï¼Œå¹¶å­˜å‚¨äºå†…å­˜é“¶è¡Œä¸­ã€‚</li>
<li>é‡‡ç”¨æ— è®­ç»ƒè¶…å›¾æ¶ˆæ¯ä¼ é€’æ¨¡å—æ›´æ–°æµ‹è¯•æ ·æœ¬çš„è§†è§‰ç‰¹å¾ã€‚</li>
<li>åˆ©ç”¨è¶…è¾¹å¼•å¯¼çš„å†…å­˜æœç´¢æ¨¡å—é™ä½è¯¯æŠ¥ç‡ã€‚</li>
<li>åœ¨MVTec 3D-ADå’ŒEyecandiesæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜è¯¥æ–¹æ³•çš„ä¼˜è¶Šæ€§ã€‚</li>
<li>æå‡ºçš„CIFæ–¹æ³•å¯ä»¥æœ‰æ•ˆé€‚åº”å¤æ‚å·¥ä¸šåœºæ™¯ä¸­çš„å°‘æ•°æ ·æœ¬æŒ‘æˆ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.05966">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bfb6095666551ca14f3ff4285ab19dc5" align="middle">
<img src="https://picx.zhimg.com/v2-f8e2ff6ae20b726931e6dfbec906173a" align="middle">
<img src="https://picx.zhimg.com/v2-bbccd0b9ff813f081404c693c48a54c3" align="middle">
<img src="https://picx.zhimg.com/v2-43973f1e50fc573e486e53c5e9073f9d" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="TabDistill-Distilling-Transformers-into-Neural-Nets-for-Few-Shot-Tabular-Classification"><a href="#TabDistill-Distilling-Transformers-into-Neural-Nets-for-Few-Shot-Tabular-Classification" class="headerlink" title="TabDistill: Distilling Transformers into Neural Nets for Few-Shot Tabular Classification"></a>TabDistill: Distilling Transformers into Neural Nets for Few-Shot Tabular Classification</h2><p><strong>Authors:Pasan Dissanayake, Sanghamitra Dutta</strong></p>
<p>Transformer-based models have shown promising performance on tabular data compared to their classical counterparts such as neural networks and Gradient Boosted Decision Trees (GBDTs) in scenarios with limited training data. They utilize their pre-trained knowledge to adapt to new domains, achieving commendable performance with only a few training examples, also called the few-shot regime. However, the performance gain in the few-shot regime comes at the expense of significantly increased complexity and number of parameters. To circumvent this trade-off, we introduce TabDistill, a new strategy to distill the pre-trained knowledge in complex transformer-based models into simpler neural networks for effectively classifying tabular data. Our framework yields the best of both worlds: being parameter-efficient while performing well with limited training data. The distilled neural networks surpass classical baselines such as regular neural networks, XGBoost and logistic regression under equal training data, and in some cases, even the original transformer-based models that they were distilled from.</p>
<blockquote>
<p>åŸºäºTransformerçš„æ¨¡å‹åœ¨æœ‰é™çš„è®­ç»ƒæ•°æ®åœºæ™¯ä¸‹ï¼Œä¸ä¼ ç»Ÿçš„ç¥ç»ç½‘ç»œå’Œæ¢¯åº¦æå‡å†³ç­–æ ‘ï¼ˆGBDTsï¼‰ç›¸æ¯”ï¼Œåœ¨è¡¨æ ¼æ•°æ®ä¸Šè¡¨ç°å‡ºäº†æœ‰å‰æ™¯çš„æ€§èƒ½ã€‚å®ƒä»¬åˆ©ç”¨é¢„è®­ç»ƒçš„çŸ¥è¯†æ¥é€‚åº”æ–°é¢†åŸŸï¼Œä»…å‡­å°‘é‡çš„è®­ç»ƒæ ·æœ¬å³å¯å®ç°ä»¤äººç§°èµçš„æ€§èƒ½ï¼Œè¿™ä¹Ÿè¢«ç§°ä¸ºå°‘æ ·æœ¬ä½“åˆ¶ã€‚ç„¶è€Œï¼Œå°‘æ ·æœ¬ä½“åˆ¶ä¸­çš„æ€§èƒ½æå‡æ˜¯ä»¥æ˜¾è‘—å¢åŠ çš„å¤æ‚æ€§å’Œå‚æ•°æ•°é‡ä¸ºä»£ä»·çš„ã€‚ä¸ºäº†è§„é¿è¿™ç§æƒè¡¡ï¼Œæˆ‘ä»¬å¼•å…¥äº†TabDistillç­–ç•¥ï¼Œè¿™æ˜¯ä¸€ç§å°†åŸºäºå¤æ‚Transformerçš„æ¨¡å‹ä¸­çš„é¢„è®­ç»ƒçŸ¥è¯†è’¸é¦åˆ°æ›´ç®€å•çš„ç¥ç»ç½‘ç»œä¸­çš„æ–°æ–¹æ³•ï¼Œä»¥æœ‰æ•ˆåœ°å¯¹è¡¨æ ¼æ•°æ®è¿›è¡Œåˆ†ç±»ã€‚æˆ‘ä»¬çš„æ¡†æ¶å®ç°äº†ä¸¤è€…çš„æœ€ä½³ç»“åˆï¼šå‚æ•°æ•ˆç‡é«˜ï¼ŒåŒæ—¶åœ¨æœ‰é™çš„è®­ç»ƒæ•°æ®ä¸Šè¡¨ç°è‰¯å¥½ã€‚è’¸é¦åçš„ç¥ç»ç½‘ç»œåœ¨åŒç­‰è®­ç»ƒæ•°æ®ä¸‹è¶…è¶Šäº†ç»å…¸åŸºçº¿ï¼Œå¦‚å¸¸è§„ç¥ç»ç½‘ç»œã€XGBoostå’Œé€»è¾‘å›å½’ï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ï¼Œç”šè‡³è¶…è¶Šäº†å®ƒä»¬æ‰€è’¸é¦çš„åŸå§‹åŸºäºTransformerçš„æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.05704v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºTransformerçš„æ¨¡å‹åœ¨å¤„ç†è¡¨æ ¼æ•°æ®æ—¶å±•ç°å‡ºå¯¹æœ‰é™è®­ç»ƒæ•°æ®çš„ä¼˜å¼‚æ€§èƒ½ï¼Œç›¸è¾ƒäºä¼ ç»Ÿçš„ç¥ç»ç½‘ç»œå’Œæ¢¯åº¦æå‡å†³ç­–æ ‘ï¼ˆGBDTsï¼‰å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚å®ƒä»¬åˆ©ç”¨é¢„è®­ç»ƒçŸ¥è¯†é€‚åº”æ–°é¢†åŸŸï¼Œåœ¨å°‘é‡è®­ç»ƒæ ·æœ¬ï¼ˆå³å°æ ·æœ¬ï¼‰çš„æƒ…å†µä¸‹ä¹Ÿèƒ½å–å¾—è‰¯å¥½æ€§èƒ½ã€‚ç„¶è€Œï¼Œè¿™ç§å°æ ·æœ¬æƒ…å†µä¸‹çš„æ€§èƒ½æå‡æ˜¯ä»¥æ˜¾è‘—å¢åŠ çš„å¤æ‚æ€§å’Œå‚æ•°æ•°é‡ä¸ºä»£ä»·çš„ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºTabDistillç­–ç•¥ï¼Œæ—¨åœ¨å°†åŸºäºTransformerçš„å¤æ‚æ¨¡å‹ä¸­çš„é¢„è®­ç»ƒçŸ¥è¯†è’¸é¦åˆ°æ›´ç®€å•çš„ç¥ç»ç½‘ç»œä¸­ï¼Œä»¥å®ç°è¡¨æ ¼æ•°æ®çš„æœ‰æ•ˆåˆ†ç±»ã€‚æˆ‘ä»¬çš„æ¡†æ¶å®ç°äº†ä¸¤å…¨å…¶ç¾ï¼šæ—¢å…·æœ‰å‚æ•°æ•ˆç‡ï¼Œåˆèƒ½åœ¨æœ‰é™è®­ç»ƒæ•°æ®ä¸‹è¡¨ç°è‰¯å¥½ã€‚è’¸é¦åçš„ç¥ç»ç½‘ç»œåœ¨åŒç­‰è®­ç»ƒæ•°æ®ä¸‹è¶…è¶Šäº†ä¼ ç»ŸåŸºçº¿æ¨¡å‹ï¼Œå¦‚å¸¸è§„ç¥ç»ç½‘ç»œã€XGBoostå’Œé€»è¾‘å›å½’ï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ç”šè‡³è¶…è¶Šäº†åŸå§‹çš„åŸºäºTransformerçš„æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŸºäºTransformerçš„æ¨¡å‹åœ¨å¤„ç†è¡¨æ ¼æ•°æ®æ—¶ï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ¨¡å‹åœ¨æœ‰é™è®­ç»ƒæ•°æ®ä¸‹è¡¨ç°å‡ºæ›´å¥½çš„æ€§èƒ½ã€‚</li>
<li>Transformeræ¨¡å‹åˆ©ç”¨é¢„è®­ç»ƒçŸ¥è¯†æ¥é€‚åº”æ–°é¢†åŸŸï¼Œåœ¨å°æ ·æœ¬æƒ…å†µä¸‹ä¹Ÿèƒ½å–å¾—è‰¯å¥½æ€§èƒ½ã€‚</li>
<li>Transformeræ¨¡å‹çš„é«˜æ€§èƒ½æ˜¯ä»¥è¾ƒé«˜çš„å¤æ‚æ€§å’Œå‚æ•°æ•°é‡å¢é•¿ä¸ºä»£ä»·çš„ã€‚</li>
<li>TabDistillç­–ç•¥æ—¨åœ¨å°†åŸºäºTransformerçš„å¤æ‚æ¨¡å‹ä¸­çš„çŸ¥è¯†è’¸é¦åˆ°æ›´ç®€å•çš„ç¥ç»ç½‘ç»œä¸­ã€‚</li>
<li>TabDistillç­–ç•¥ä½¿å¾—æ¨¡å‹åœ¨å‚æ•°æ•ˆç‡å’Œåœ¨å°æ ·æœ¬æƒ…å†µä¸‹çš„æ€§èƒ½ä¸Šè¾¾åˆ°å¹³è¡¡ã€‚</li>
<li>è’¸é¦åçš„ç¥ç»ç½‘ç»œåœ¨æŸäº›æƒ…å†µä¸‹è¶…è¶Šäº†åŸå§‹çš„åŸºäºTransformerçš„æ¨¡å‹ä»¥åŠä¼ ç»ŸåŸºçº¿æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.05704">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7049abcd06026565fdb37dd9337fffa8" align="middle">
<img src="https://picx.zhimg.com/v2-e8185f95afeeecc8e49d52e7034ffa03" align="middle">
<img src="https://picx.zhimg.com/v2-77e31f4d8e9fe1f2d8857ba5189a4dfe" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="In-Context-Adaptation-of-VLMs-for-Few-Shot-Cell-Detection-in-Optical-Microscopy"><a href="#In-Context-Adaptation-of-VLMs-for-Few-Shot-Cell-Detection-in-Optical-Microscopy" class="headerlink" title="In-Context Adaptation of VLMs for Few-Shot Cell Detection in Optical Microscopy"></a>In-Context Adaptation of VLMs for Few-Shot Cell Detection in Optical Microscopy</h2><p><strong>Authors:Shreyan Ganguly, Angona Biswas, Jaydeep Rade, Md Hasibul Hasan Hasib, Nabila Masud, Nitish Singla, Abhipsa Dash, Ushashi Bhattacharjee, Aditya Balu, Anwesha Sarkar, Adarsh Krishnamurthy, Soumik Sarkar</strong></p>
<p>Foundation vision-language models (VLMs) excel on natural images, but their utility for biomedical microscopy remains underexplored. In this paper, we investigate how in-context learning enables state-of-the-art VLMs to perform few-shot object detection when large annotated datasets are unavailable, as is often the case with microscopic images. We introduce the Micro-OD benchmark, a curated collection of 252 images specifically curated for in-context learning, with bounding-box annotations spanning 11 cell types across four sources, including two in-lab expert-annotated sets. We systematically evaluate eight VLMs under few-shot conditions and compare variants with and without implicit test-time reasoning tokens. We further implement a hybrid Few-Shot Object Detection (FSOD) pipeline that combines a detection head with a VLM-based few-shot classifier, which enhances the few-shot performance of recent VLMs on our benchmark. Across datasets, we observe that zero-shot performance is weak due to the domain gap; however, few-shot support consistently improves detection, with marginal gains achieved after six shots. We observe that models with reasoning tokens are more effective for end-to-end localization, whereas simpler variants are more suitable for classifying pre-localized crops. Our results highlight in-context adaptation as a practical path for microscopy, and our benchmark provides a reproducible testbed for advancing open-vocabulary detection in biomedical imaging.</p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨è‡ªç„¶å›¾åƒä¸Šè¡¨ç°å“è¶Šï¼Œä½†åœ¨ç”Ÿç‰©åŒ»å­¦æ˜¾å¾®é•œä¸­çš„åº”ç”¨ä»·å€¼å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æœ¬æ–‡ç ”ç©¶äº†ä¸Šä¸‹æ–‡å­¦ä¹ å¦‚ä½•ä½¿æœ€å…ˆè¿›çš„VLMsåœ¨æ— æ³•ä½¿ç”¨å¤§å‹æ ‡æ³¨æ•°æ®é›†çš„æƒ…å†µä¸‹è¿›è¡Œå°æ ·æœ¬ç›®æ ‡æ£€æµ‹æˆä¸ºå¯èƒ½ï¼Œè¿™åœ¨æ˜¾å¾®é•œå›¾åƒçš„æƒ…å†µä¸‹é€šå¸¸æ˜¯å¸¸æ€ã€‚æˆ‘ä»¬ä»‹ç»äº†Micro-ODåŸºå‡†æµ‹è¯•ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºä¸Šä¸‹æ–‡å­¦ä¹ å‡†å¤‡çš„å›¾åƒé›†ï¼ŒåŒ…å«æ¥è‡ªå››ä¸ªæºçš„è·¨è¾¹ç•Œæ¡†æ ‡æ³¨çš„ç‰¹å®šç›®æ ‡æ ·æœ¬é›†ï¼ˆç»†èƒç±»å‹ï¼‰çš„ç‰¹å®šå­é›†ï¼Œå…±è®¡åŒ…å«ç‰¹å®šç²¾é€‰çš„252å¼ å›¾åƒã€‚æˆ‘ä»¬ç³»ç»Ÿåœ°è¯„ä¼°äº†å°æ ·æœ¬æ¡ä»¶ä¸‹çš„å…«ä¸ªVLMsï¼Œå¹¶æ¯”è¾ƒäº†å¸¦æœ‰å’Œä¸å¸¦éšå¼æµ‹è¯•æ—¶é—´æ¨ç†ç¬¦å·çš„å˜ä½“ã€‚æˆ‘ä»¬è¿˜å®ç°äº†ä¸€ç§æ··åˆçš„å°æ ·æœ¬ç›®æ ‡æ£€æµ‹ï¼ˆFSODï¼‰ç®¡é“ï¼Œè¯¥ç®¡é“å°†æ£€æµ‹å¤´ä¸åŸºäºVLMçš„å°æ ·æœ¬åˆ†ç±»å™¨ç›¸ç»“åˆï¼Œæé«˜äº†æˆ‘ä»¬åœ¨åŸºå‡†æµ‹è¯•ä¸Šçš„æœ€æ–°VLMsçš„å°æ ·æœ¬æ€§èƒ½ã€‚è·¨æ•°æ®é›†è§‚å¯Ÿå‘ç°ï¼Œç”±äºé¢†åŸŸå·®è·ï¼Œé›¶æ ·æœ¬æ€§èƒ½è¾ƒå¼±ï¼›ç„¶è€Œï¼Œå°æ ·æœ¬æ”¯æŒå§‹ç»ˆèƒ½æé«˜æ£€æµ‹èƒ½åŠ›ï¼Œå¹¶åœ¨å…­æ¬¡æ‹æ‘„åå®ç°å¾®å°æ”¶ç›Šã€‚æˆ‘ä»¬å‘ç°å¸¦æœ‰æ¨ç†ç¬¦å·çš„æ¨¡å‹æ›´é€‚åˆç«¯åˆ°ç«¯çš„å®šä½ï¼Œè€Œæ›´ç®€å•çš„å˜ä½“æ›´é€‚åˆé¢„å®šä½ä½œç‰©çš„åˆ†ç±»ã€‚æˆ‘ä»¬çš„ç»“æœå¼ºè°ƒäº†ä¸Šä¸‹æ–‡é€‚åº”åœ¨æ˜¾å¾®é•œé¢†åŸŸçš„å®ç”¨æ€§ï¼Œæˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•æä¾›äº†ä¸€ä¸ªå¯é‡å¤çš„æµ‹è¯•å¹³å°ï¼Œä»¥æ¨åŠ¨ç”Ÿç‰©åŒ»å­¦æˆåƒä¸­çš„å¼€æ”¾è¯æ±‡æ£€æµ‹çš„å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.05565v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¦‚ä½•åœ¨ç¼ºä¹å¤§é‡æ ‡æ³¨æ•°æ®é›†çš„æƒ…å†µä¸‹ï¼Œåˆ©ç”¨ä¸Šä¸‹æ–‡å­¦ä¹ ä½¿å…ˆè¿›çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨ç”Ÿç‰©åŒ»å­¦æ˜¾å¾®é•œå›¾åƒä¸Šè¿›è¡Œå°‘æ ·æœ¬ç›®æ ‡æ£€æµ‹ã€‚ç ”ç©¶å¼•å…¥äº†Micro-ODåŸºå‡†æµ‹è¯•é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«252å¼ ä¸“ä¸ºä¸Šä¸‹æ–‡å­¦ä¹ ç­–åˆ’çš„å›¾åƒï¼Œè·¨è¶Šå››ç§æ¥æºçš„11ç§ç»†èƒç±»å‹å¸¦æœ‰è¾¹ç•Œæ¡†æ ‡æ³¨ã€‚é€šè¿‡ç³»ç»Ÿè¯„ä¼°å…«ç§VLMsåœ¨å°‘æ ·æœ¬æ¡ä»¶ä¸‹çš„æ€§èƒ½ï¼Œå¹¶æ¯”è¾ƒäº†å¸¦æœ‰å’Œä¸å¸¦éšå¼æµ‹è¯•æ—¶é—´æ¨ç†æ ‡è®°çš„å˜ä½“ã€‚æ­¤å¤–ï¼Œå®æ–½äº†ä¸€ç§æ··åˆçš„å°‘æ ·æœ¬ç›®æ ‡æ£€æµ‹ï¼ˆFSODï¼‰ç®¡é“ï¼Œå°†æ£€æµ‹å¤´ä¸åŸºäºVLMçš„å°‘æ ·æœ¬åˆ†ç±»å™¨ç›¸ç»“åˆï¼Œæé«˜äº†æœ€è¿‘VLMsåœ¨åŸºå‡†æµ‹è¯•é›†ä¸Šçš„å°‘æ ·æœ¬æ€§èƒ½ã€‚ç ”ç©¶å‘ç°ï¼Œç”±äºé¢†åŸŸå·®è·ï¼Œé›¶æ ·æœ¬æ€§èƒ½è¾ƒå¼±ï¼Œä½†å°‘æ ·æœ¬æ”¯æŒèƒ½æŒç»­æé«˜æ£€æµ‹èƒ½åŠ›ï¼Œå…­æ¬¡æ‹æ‘„åè·å¾—è¾¹é™…æ”¶ç›Šã€‚å…·æœ‰æ¨ç†æ ‡è®°çš„æ¨¡å‹åœ¨ç«¯åˆ°ç«¯å®šä½æ–¹é¢æ›´æœ‰æ•ˆï¼Œè€Œæ›´ç®€å•çš„å˜ä½“æ›´é€‚åˆé¢„å®šä½è£å‰ªçš„åˆ†ç±»ã€‚æ€»çš„æ¥è¯´ï¼Œè¯¥ç ”ç©¶å¼ºè°ƒäº†ä¸Šä¸‹æ–‡é€‚åº”åœ¨æ˜¾å¾®é•œæŠ€æœ¯ä¸­çš„å®ç”¨æ€§ï¼Œè€ŒåŸºå‡†æµ‹è¯•é›†ä¸ºæ¨è¿›ç”Ÿç‰©åŒ»å­¦æˆåƒä¸­çš„å¼€æ”¾è¯æ±‡æ£€æµ‹æä¾›äº†å¯é‡å¤çš„å®éªŒå¹³å°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨è‡ªç„¶å›¾åƒä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ç”Ÿç‰©åŒ»å­¦æ˜¾å¾®é•œå›¾åƒä¸Šçš„åº”ç”¨å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚</li>
<li>é€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ŒVLMsèƒ½å¤Ÿåœ¨ç¼ºä¹å¤§é‡æ ‡æ³¨æ•°æ®é›†çš„æƒ…å†µä¸‹è¿›è¡Œå°‘æ ·æœ¬ç›®æ ‡æ£€æµ‹ã€‚</li>
<li>å¼•å…¥äº†Micro-ODåŸºå‡†æµ‹è¯•é›†ï¼Œä¸ºç”Ÿç‰©åŒ»å­¦æ˜¾å¾®é•œå›¾åƒçš„ç›®æ ‡æ£€æµ‹æä¾›äº†å®éªŒå¹³å°ã€‚</li>
<li>å¯¹å…«ç§VLMsè¿›è¡Œäº†ç³»ç»Ÿè¯„ä¼°ï¼Œå‘ç°å¸¦æœ‰æ¨ç†æ ‡è®°çš„æ¨¡å‹åœ¨ç«¯åˆ°ç«¯å®šä½æ–¹é¢æ›´æœ‰æ•ˆã€‚</li>
<li>å®æ–½äº†æ··åˆçš„å°‘æ ·æœ¬ç›®æ ‡æ£€æµ‹ï¼ˆFSODï¼‰ç®¡é“ï¼Œæé«˜äº†VLMsçš„å°‘æ ·æœ¬æ€§èƒ½ã€‚</li>
<li>å°‘æ ·æœ¬æ”¯æŒèƒ½æŒç»­æé«˜æ£€æµ‹èƒ½åŠ›ï¼Œå…­æ¬¡æ‹æ‘„åè·å¾—è¾¹é™…æ”¶ç›Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.05565">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0b57e0caf97c83e43c78727e858d28d3" align="middle">
<img src="https://picx.zhimg.com/v2-1c0fdf5b19b1be3f29c7cd0d56cc9f3d" align="middle">
<img src="https://picx.zhimg.com/v2-1cdd2685671650b2dacbe32c2ea42aa9" align="middle">
<img src="https://picx.zhimg.com/v2-dda5c7e71fa7281f5de749cdde57ff10" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-11-18/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-11-18/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-18/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-1486eb592444cecef0aa3bd89d473b04" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-18  Comprehension of Multilingual Expressions Referring to Target Objects in Visual Inputs
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-11-18
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-18/MMT/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-cc8d205fccf95de88f012f3a344007e3" class="responsive-img" alt="MMT">
                        
                        <span class="card-title">MMT</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            MMT æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-18  HI-TransPA Hearing Impairments Translation Personal Assistant
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-11-18
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/MMT/" class="post-category">
                                    MMT
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/MMT/">
                        <span class="chip bg-color">MMT</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32883.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
