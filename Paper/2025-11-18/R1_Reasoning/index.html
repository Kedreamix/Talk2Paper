<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-18  Human-AI collaborative autonomous synthesis with pulsed laser deposition for remote epitaxy">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-67076cbe9423fc7de80b51dd82a83d08')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-18
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-19
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    83 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-11-18-æ›´æ–°"><a href="#2025-11-18-æ›´æ–°" class="headerlink" title="2025-11-18 æ›´æ–°"></a>2025-11-18 æ›´æ–°</h1><h2 id="Human-AI-collaborative-autonomous-synthesis-with-pulsed-laser-deposition-for-remote-epitaxy"><a href="#Human-AI-collaborative-autonomous-synthesis-with-pulsed-laser-deposition-for-remote-epitaxy" class="headerlink" title="Human-AI collaborative autonomous synthesis with pulsed laser deposition for remote epitaxy"></a>Human-AI collaborative autonomous synthesis with pulsed laser deposition for remote epitaxy</h2><p><strong>Authors:Asraful Haque, Daniel T. Yimam, Jawad Chowdhury, Ralph Bulanadi, Ivan Vlassiouk, John Lasseter, Sujoy Ghosh, Christopher M. Rouleau, Kai Xiao, Yongtao Liu, Eva Zarkadoula, Rama K. Vasudevan, Sumner B. Harris</strong></p>
<p>Autonomous laboratories typically rely on data-driven decision-making, occasionally with human-in-the-loop oversight to inject domain expertise. Fully leveraging AI agents, however, requires tightly coupled, collaborative workflows spanning hypothesis generation, experimental planning, execution, and interpretation. To address this, we develop and deploy a human-AI collaborative (HAIC) workflow that integrates large language models for hypothesis generation and analysis, with collaborative policy updates driving autonomous pulsed laser deposition (PLD) experiments for remote epitaxy of BaTiO$_3$&#x2F;graphene. HAIC accelerated the hypothesis formation and experimental design and efficiently mapped the growth space to graphene-damage. In situ Raman spectroscopy reveals that chemistry drives degradation while the highest energy plume components seed defects, identifying a low-O$_2$ pressure low-temperature synthesis window that preserves graphene but is incompatible with optimal BaTiO$_3$ growth. Thus, we show a two-step Ar&#x2F;O$_2$ deposition is required to exfoliate ferroelectric BaTiO$_3$ while maintaining a monolayer graphene interlayer. HAIC stages human insight with AI reasoning between autonomous batches to drive rapid scientific progress, providing an evolution to many existing human-in-the-loop autonomous workflows.</p>
<blockquote>
<p>è‡ªä¸»å®éªŒå®¤é€šå¸¸ä¾èµ–äºæ•°æ®é©±åŠ¨çš„å†³ç­–åˆ¶å®šï¼Œå¶å°”ä¼šæœ‰äººå·¥å‚ä¸ç›‘ç£ä»¥æ³¨å…¥é¢†åŸŸä¸“ä¸šçŸ¥è¯†ã€‚ç„¶è€Œï¼Œè¦å……åˆ†åˆ©ç”¨AIä»£ç†ï¼Œéœ€è¦ç´§å¯†è€¦åˆã€åä½œçš„å·¥ä½œæµç¨‹ï¼Œæ¶µç›–å‡è®¾ç”Ÿæˆã€å®éªŒè§„åˆ’ã€æ‰§è¡Œå’Œè§£é‡Šã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼€å‘å¹¶éƒ¨ç½²äº†ä¸€ç§äººæœºååŒï¼ˆHAICï¼‰å·¥ä½œæµç¨‹ï¼Œè¯¥æµç¨‹æ•´åˆå¤§å‹è¯­è¨€æ¨¡å‹ç”¨äºå‡è®¾ç”Ÿæˆå’Œåˆ†æï¼Œé€šè¿‡ååŒæ”¿ç­–æ›´æ–°é©±åŠ¨è‡ªä¸»è„‰å†²æ¿€å…‰æ²‰ç§¯ï¼ˆPLDï¼‰å®éªŒï¼Œç”¨äºè¿œç¨‹å¤–å»¶ç”Ÿé•¿BaTiO3&#x2F;çŸ³å¢¨çƒ¯ã€‚HAICåŠ é€Ÿäº†å‡è®¾å½¢æˆå’Œå®éªŒè®¾è®¡ï¼Œæœ‰æ•ˆåœ°æ˜ å°„äº†çŸ³å¢¨çƒ¯æŸä¼¤çš„ç”Ÿé•¿ç©ºé—´ã€‚åŸä½æ‹‰æ›¼å…‰è°±è¡¨æ˜ï¼ŒåŒ–å­¦é©±åŠ¨é™è§£ï¼Œè€Œæœ€é«˜èƒ½é‡çš„ç¾½æµæˆåˆ†äº§ç”Ÿç¼ºé™·ï¼Œç¡®å®šäº†ä¸€ä¸ªä½æ°§å‹ä½æ¸©åˆæˆçª—å£ï¼Œè¯¥çª—å£å¯ä¿ç•™çŸ³å¢¨çƒ¯ä½†ä¸æœ€ä½³BaTiO3ç”Ÿé•¿ä¸å…¼å®¹ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æ˜¾ç¤ºéœ€è¦ä¸¤æ­¥Ar&#x2F;O2æ²‰ç§¯æ¥å‰¥ç¦»é“ç”µBaTiO3åŒæ—¶ä¿æŒå•å±‚çŸ³å¢¨çƒ¯æ’å±‚ã€‚HAICé˜¶æ®µå°†äººç±»è§è§£ä¸AIæ¨ç†ç›¸ç»“åˆï¼Œç”¨äºé©±åŠ¨è‡ªä¸»æ‰¹æ¬¡ä¹‹é—´çš„å¿«é€Ÿç§‘å­¦è¿›æ­¥ï¼Œä¸ºè®¸å¤šç°æœ‰çš„äººå·¥å‚ä¸ç›‘ç£çš„è‡ªä¸»å·¥ä½œæµç¨‹æä¾›äº†è¿›åŒ–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.11558v1">PDF</a> </p>
<p><strong>Summary</strong><br>è‡ªä¸»å®éªŒå®¤é€šå¸¸ä¾èµ–æ•°æ®é©±åŠ¨å†³ç­–ï¼Œæœ‰æ—¶éœ€è¦äººä¸ºä»‹å…¥ä»¥æ³¨å…¥é¢†åŸŸä¸“ä¸šçŸ¥è¯†ã€‚ä¸ºäº†å®ç°AIçš„æœ€å¤§åŒ–åˆ©ç”¨ï¼Œæˆ‘ä»¬éœ€è¦åœ¨å‡è®¾ç”Ÿæˆã€å®éªŒè§„åˆ’ã€æ‰§è¡Œå’Œè§£é‡Šä¹‹é—´æ„å»ºç´§å¯†åä½œçš„å·¥ä½œæµç¨‹ã€‚æˆ‘ä»¬å¼€å‘å¹¶éƒ¨ç½²äº†ä¸€ç§äººæœºååŒï¼ˆHAICï¼‰å·¥ä½œæµç¨‹ï¼Œæ•´åˆå¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå‡è®¾ç”Ÿæˆå’Œåˆ†æï¼ŒååŒæ”¿ç­–æ›´æ–°é©±åŠ¨è‡ªä¸»è„‰å†²æ¿€å…‰æ²‰ç§¯ï¼ˆPLDï¼‰å®éªŒï¼Œç”¨äºè¿œç¨‹å¤–å»¶ç”Ÿé•¿BaTiO3&#x2F;çŸ³å¢¨çƒ¯ã€‚äººæœºååŒåŠ å¿«äº†å‡è®¾å½¢æˆå’Œå®éªŒè®¾è®¡ï¼Œæœ‰æ•ˆæ˜ å°„äº†çŸ³å¢¨çƒ¯æŸä¼¤çš„ç”Ÿé•¿ç©ºé—´ã€‚åŸä½æ‹‰æ›¼å…‰è°±è¡¨æ˜åŒ–å­¦é©±åŠ¨é™è§£ï¼Œè€Œé«˜èƒ½ç­‰ç¦»å­ä½“æˆåˆ†ä¼šå¼•å‘ç¼ºé™·ï¼Œç¡®å®šäº†åœ¨ä½æ°§å‹å’Œä½æ¸©ä¸‹åˆæˆèƒ½å¤Ÿä¿æŒçŸ³å¢¨çƒ¯çš„åˆæˆçª—å£ï¼Œä½†ä¸é€‚ç”¨äºBaTiO3çš„æœ€ä½³ç”Ÿé•¿æ¡ä»¶ã€‚å› æ­¤ï¼Œå±•ç¤ºäº†ä¸€ç§éœ€è¦ä¸¤æ­¥çš„Ar&#x2F;O2æ²‰ç§¯å·¥è‰ºæ¥å‰¥ç¦»é“ç”µBaTiO3å¹¶ä¿æŒå•å±‚çŸ³å¢¨çƒ¯å¤¹å±‚ã€‚äººæœºååŒé˜¶æ®µå°†äººç±»æ´å¯ŸåŠ›å’ŒAIæ¨ç†èå…¥è‡ªä¸»æ‰¹æ¬¡å®éªŒä¸­ï¼Œæ¨åŠ¨äº†ç§‘å­¦ç ”ç©¶çš„å¿«é€Ÿå‘å±•ï¼Œä¸ºè®¸å¤šç°æœ‰çš„éœ€è¦äººä¸ºä»‹å…¥çš„è‡ªä¸»å·¥ä½œæµç¨‹æä¾›äº†å‡çº§æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªä¸»å®éªŒå®¤é€šå¸¸ä½¿ç”¨æ•°æ®é©±åŠ¨å†³ç­–è¿‡ç¨‹å¹¶å€ŸåŠ©é¢†åŸŸä¸“å®¶çš„äººå·¥å‚ä¸æ¥æå‡AIæ•ˆèƒ½ã€‚</li>
<li>äººæœºååŒï¼ˆHAICï¼‰å·¥ä½œæµç¨‹è¢«å¼€å‘å¹¶åº”ç”¨äºå®éªŒå†³ç­–è¿‡ç¨‹ï¼Œç‰¹åˆ«æ˜¯åœ¨è„‰å†²æ¿€å…‰æ²‰ç§¯å®éªŒä¸­ã€‚</li>
<li>HAICåŠ é€Ÿäº†å‡è®¾å½¢æˆå’Œå®éªŒè®¾è®¡ï¼Œé€šè¿‡æ˜ å°„ç”Ÿé•¿ç©ºé—´ä¼˜åŒ–å®éªŒè¿‡ç¨‹ã€‚</li>
<li>åŸä½æ‹‰æ›¼å…‰è°±æ­ç¤ºäº†çŸ³å¢¨çƒ¯æŸä¼¤çš„å…³é”®å› ç´ åŒ…æ‹¬åŒ–å­¦é€€åŒ–å’Œé«˜èƒ½ç­‰ç¦»å­ä½“å½±å“ã€‚</li>
<li>ä½æ°§å‹å’Œä½æ¸©ä¸‹çš„åˆæˆçª—å£æœ‰åˆ©äºä¿æŒçŸ³å¢¨çƒ¯ä½†ä¸åˆ©äºBaTiO3çš„æœ€ä½³ç”Ÿé•¿æ¡ä»¶ã€‚</li>
<li>éœ€è¦é‡‡ç”¨ä¸¤æ­¥Ar&#x2F;O2æ²‰ç§¯å·¥è‰ºæ¥å‰¥ç¦»é“ç”µBaTiO3å¹¶ä¿æŒå•å±‚çŸ³å¢¨çƒ¯å¤¹å±‚ç»“æ„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.11558">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-305ff5887773b493f9467ed6f4225942" align="middle">
<img src="https://picx.zhimg.com/v2-0c7dca9a3eea9a48f6e17c9d2a896313" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Bridging-Hidden-States-in-Vision-Language-Models"><a href="#Bridging-Hidden-States-in-Vision-Language-Models" class="headerlink" title="Bridging Hidden States in Vision-Language Models"></a>Bridging Hidden States in Vision-Language Models</h2><p><strong>Authors:Benjamin Fein-Ashley, Jacob Fein-Ashley</strong></p>
<p>Vision-Language Models (VLMs) are a new family of models that align image content with natural language. Existing approaches typically fuse either (a) early: by mixing tokens&#x2F;features inside the encoders, or (b) late: by comparing pooled embeddings. Many methods also tie fusion to an autoregressive decoder. However, the hidden states of both modalities already carry rich, modality-specific structure (spatial layout in vision; syntax and semantics in text), so directly aligning these states is a natural way to match what the two modalities â€œthinkâ€. We propose a lightweight fusion module: a few cross-only, bidirectional attention layers placed near the top of both encoders. Each layer projects the vision and text encoder hidden-state sequences into a shared space, attends across modalities, and sends gated residual updates back, with simple stabilizers to improve alignment. The encoders remain non-causal and strong for understanding, while generation stays cleanly decoupled via an optional decoder. Across standard retrieval, VQA, and visual reasoning benchmarks, BRIDGE outperforms comparable VLMs while preserving the bi-encoder efficiency of contrastive models. We make our code publicly available at <a target="_blank" rel="noopener" href="https://github.com/jfeinashley/BRIDGE">https://github.com/jfeinashley/BRIDGE</a>.</p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰æ˜¯ä¸€ç±»æ–°çš„æ¨¡å‹ï¼Œèƒ½å¤Ÿå°†å›¾åƒå†…å®¹ä¸è‡ªç„¶è¯­è¨€ç›¸ç»“åˆã€‚ç°æœ‰çš„æ–¹æ³•é€šå¸¸èåˆï¼ˆaï¼‰æ—©æœŸï¼šé€šè¿‡åœ¨ç¼–ç å™¨å†…éƒ¨æ··åˆä»¤ç‰Œ&#x2F;ç‰¹å¾ï¼Œæˆ–ï¼ˆbï¼‰åæœŸï¼šé€šè¿‡æ¯”è¾ƒæ± åŒ–åµŒå…¥ã€‚è®¸å¤šæ–¹æ³•è¿˜å°†èåˆä¸è‡ªå›å½’è§£ç å™¨ç›¸å…³è”ã€‚ç„¶è€Œï¼Œä¸¤ç§æ¨¡å¼çš„éšè—çŠ¶æ€å·²ç»æºå¸¦äº†ä¸°å¯Œä¸”ç‰¹å®šçš„æ¨¡å¼ç»“æ„ï¼ˆè§†è§‰ä¸­çš„ç©ºé—´å¸ƒå±€ï¼›æ–‡æœ¬ä¸­çš„è¯­æ³•å’Œè¯­ä¹‰ï¼‰ï¼Œå› æ­¤ç›´æ¥å¯¹é½è¿™äº›çŠ¶æ€æ˜¯åŒ¹é…ä¸¤ç§æ¨¡å¼â€œæ€è€ƒâ€å†…å®¹çš„ä¸€ç§è‡ªç„¶æ–¹å¼ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§è½»é‡çº§çš„èåˆæ¨¡å—ï¼šåœ¨ä¸¤ç§ç¼–ç å™¨çš„é¡¶éƒ¨é™„è¿‘æ”¾ç½®å‡ ä¸ªä»…äº¤å‰çš„åŒå‘æ³¨æ„å±‚ã€‚æ¯å±‚éƒ½å°†è§†è§‰å’Œæ–‡æœ¬ç¼–ç å™¨éšè—çŠ¶æ€åºåˆ—æŠ•å°„åˆ°å…±äº«ç©ºé—´ï¼Œè·¨æ¨¡æ€æ³¨æ„ï¼Œå¹¶é€šè¿‡ç®€å•çš„ç¨³å®šå™¨å‘é€é—¨æ§æ®‹å·®æ›´æ–°ä»¥æ”¹å–„å¯¹é½ã€‚ç¼–ç å™¨ä¿æŒéå› æœæ€§å¹¶å¢å¼ºç†è§£èƒ½åŠ›ï¼Œè€Œç”Ÿæˆåˆ™é€šè¿‡å¯é€‰çš„è§£ç å™¨ä¿æŒæ¸…æ™°è§£è€¦ã€‚åœ¨æ ‡å‡†æ£€ç´¢ã€è§†è§‰é—®ç­”å’Œè§†è§‰æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼ŒBRIDGEä¼˜äºç±»ä¼¼çš„VLMsï¼ŒåŒæ—¶ä¿ç•™äº†å¯¹æ¯”æ¨¡å‹çš„åŒå‘ç¼–ç å™¨æ•ˆç‡ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/jfeinashley/BRIDGE%E5%85%AC%E5%BC%80%E6%88%91%E4%BB%AC%E7%9A%84%E4%BB%A3%E7%A0%81%E3%80%82">https://github.com/jfeinashley/BRIDGEå…¬å¼€æˆ‘ä»¬çš„ä»£ç ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.11526v1">PDF</a> </p>
<p><strong>Summary</strong><br>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰é€šè¿‡èåˆå›¾åƒå’Œè‡ªç„¶è¯­è¨€å†…å®¹å½¢æˆæ–°çš„æ¨¡å‹å®¶æ—ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸åœ¨ç¼–ç å™¨å†…éƒ¨èåˆæ—©æœŸæˆ–æ™šæœŸä¿¡æ¯ï¼Œå¹¶ä¾èµ–äºè‡ªå›å½’è§£ç å™¨è¿›è¡Œèåˆã€‚ç„¶è€Œï¼Œç›´æ¥å¯¹é½ä¸¤ç§æ¨¡æ€çš„éšè—çŠ¶æ€æ˜¯ä¸€ç§è‡ªç„¶åŒ¹é…å®ƒä»¬â€œæ€è€ƒâ€çš„æ–¹å¼ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§è½»é‡çº§èåˆæ¨¡å—ï¼Œå³åœ¨ä¸¤ç§ç¼–ç å™¨çš„é¡¶éƒ¨æ”¾ç½®å°‘é‡ä»…è·¨åŒå‘æ³¨æ„åŠ›å±‚ã€‚è¿™äº›å±‚å°†è§†è§‰å’Œæ–‡æœ¬ç¼–ç å™¨çš„éšè—çŠ¶æ€åºåˆ—æŠ•å½±åˆ°å…±äº«ç©ºé—´ï¼Œè·¨æ¨¡æ€å…³æ³¨å¹¶å‘é€é—¨æ§æ®‹å·®æ›´æ–°ã€‚åŒæ—¶æ·»åŠ ç®€å•çš„ç¨³å®šå™¨ä»¥æ”¹å–„å¯¹é½æ•ˆæœã€‚è¯¥æ¨¡å‹åœ¨æ£€ç´¢ã€è§†è§‰é—®ç­”å’Œè§†è§‰æ¨ç†æ ‡å‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼ŒåŒæ—¶ä¿æŒäº†å¯¹æ¯”æ¨¡å‹çš„åŒå‘æ•ˆç‡ã€‚æˆ‘ä»¬çš„ä»£ç å…¬å¼€åœ¨ï¼š[å…¬å¼€é“¾æ¥]ï¼ˆè¯·æ›¿æ¢ä¸ºçœŸå®çš„é“¾æ¥åœ°å€ï¼‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>VLMsæ˜¯æ–°çš„æ¨¡å‹å®¶æ—ï¼Œé€šè¿‡èåˆå›¾åƒå’Œè‡ªç„¶è¯­è¨€å†…å®¹ï¼Œå®ç°äº†å›¾åƒä¸æ–‡æœ¬çš„ç›¸äº’å…³è”ã€‚</li>
<li>ç°æœ‰èåˆæ–¹æ³•ä¸»è¦åŒ…æ‹¬æ—©æœŸå’Œæ™šæœŸèåˆï¼Œå¹¶ä¸”ä¾èµ–äºè‡ªå›å½’è§£ç å™¨ã€‚</li>
<li>ç›´æ¥å¯¹é½è§†è§‰å’Œæ–‡æœ¬æ¨¡æ€çš„éšè—çŠ¶æ€æ˜¯ä¸€ç§è‡ªç„¶çš„æ–¹å¼æ¥åŒ¹é…ä¸¤è€…çš„ä¿¡æ¯ã€‚</li>
<li>æå‡ºäº†ä¸€ç§è½»é‡çº§èåˆæ¨¡å—ï¼Œé€šè¿‡è·¨æ¨¡æ€æ³¨æ„åŠ›å±‚å®ç°è§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯çš„èåˆã€‚</li>
<li>è¯¥æ¨¡å‹åœ¨å¤šä¸ªæ ‡å‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼ŒåŒ…æ‹¬æ£€ç´¢ã€è§†è§‰é—®ç­”å’Œè§†è§‰æ¨ç†ç­‰ä»»åŠ¡ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.11526">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9479371b638996ec350b55c5a668285a" align="middle">
<img src="https://picx.zhimg.com/v2-f51032840d761eb722f65985eff912eb" align="middle">
<img src="https://picx.zhimg.com/v2-6a991e67ca1406d29961fa61c944885c" align="middle">
<img src="https://picx.zhimg.com/v2-0035d28f3752e08caf39e51a0f52ff23" align="middle">
<img src="https://picx.zhimg.com/v2-bcc80a86778b90795fa363806cd1e42f" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="CVChess-A-Deep-Learning-Framework-for-Converting-Chessboard-Images-to-Forsyth-Edwards-Notation"><a href="#CVChess-A-Deep-Learning-Framework-for-Converting-Chessboard-Images-to-Forsyth-Edwards-Notation" class="headerlink" title="CVChess: A Deep Learning Framework for Converting Chessboard Images to Forsyth-Edwards Notation"></a>CVChess: A Deep Learning Framework for Converting Chessboard Images to Forsyth-Edwards Notation</h2><p><strong>Authors:Luthira Abeykoon, Ved Patel, Gawthaman Senthilvelan, Darshan Kasundra</strong></p>
<p>Chess has experienced a large increase in viewership since the pandemic, driven largely by the accessibility of online learning platforms. However, no equivalent assistance exists for physical chess games, creating a divide between analog and digital chess experiences. This paper presents CVChess, a deep learning framework for converting chessboard images to Forsyth-Edwards Notation (FEN), which is later input into online chess engines to provide you with the best next move. Our approach employs a convolutional neural network (CNN) with residual layers to perform piece recognition from smartphone camera images. The system processes RGB images of a physical chess board through a multistep process: image preprocessing using the Hough Line Transform for edge detection, projective transform to achieve a top-down board alignment, segmentation into 64 individual squares, and piece classification into 13 classes (6 unique white pieces, 6 unique black pieces and an empty square) using the residual CNN. Residual connections help retain low-level visual features while enabling deeper feature extraction, improving accuracy and stability during training. We train and evaluate our model using the Chess Recognition Dataset (ChessReD), containing 10,800 annotated smartphone images captured under diverse lighting conditions and angles. The resulting classifications are encoded as an FEN string, which can be fed into a chess engine to generate the most optimal move</p>
<blockquote>
<p>è±¡æ£‹ç”±äºåœ¨çº¿å­¦ä¹ å¹³å°çš„æ˜“è®¿é—®æ€§è€Œåœ¨ç–«æƒ…æœŸé—´è§‚ä¼—äººæ•°å¤§å¹…å¢åŠ ã€‚ç„¶è€Œï¼Œç›®å‰å°šæœªæœ‰é’ˆå¯¹å®ä½“è±¡æ£‹æ¸¸æˆçš„ç›¸åº”è¾…åŠ©å·¥å…·ï¼Œè¿™é€ æˆäº†æ¨¡æ‹Ÿä¸æ•°å­—è±¡æ£‹ä½“éªŒä¹‹é—´çš„é¸¿æ²Ÿã€‚æœ¬æ–‡ä»‹ç»äº†CVChessï¼Œä¸€ä¸ªæ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œå¯å°†è±¡æ£‹æ£‹ç›˜å›¾åƒè½¬æ¢ä¸ºForsyth-Edwardsè®°å·æ³•ï¼ˆFENï¼‰ï¼Œç„¶åè¾“å…¥åˆ°åœ¨çº¿è±¡æ£‹å¼•æ“ä¸­ï¼Œä¸ºä½ æä¾›æœ€ä½³çš„ä¸‹ä¸€æ­¥è¡ŒåŠ¨å»ºè®®ã€‚æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨å¸¦æœ‰æ®‹å·®å±‚çš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æ¥æ‰§è¡Œä»æ™ºèƒ½æ‰‹æœºæ‘„åƒå¤´å›¾åƒä¸­è¯†åˆ«æ£‹å­çš„ä»»åŠ¡ã€‚è¯¥ç³»ç»Ÿé€šè¿‡å¤šæ­¥éª¤å¤„ç†RGBæ ¼å¼çš„å®ä½“è±¡æ£‹æ£‹ç›˜å›¾åƒï¼šä½¿ç”¨éœå¤«çº¿å˜æ¢è¿›è¡Œè¾¹ç¼˜æ£€æµ‹çš„å›¾åƒé¢„å¤„ç†ï¼Œä»¥å®ç°ä»ä¸Šåˆ°ä¸‹çš„æ£‹ç›˜å¯¹é½çš„æŠ•å½±å˜æ¢ï¼Œå°†æ£‹ç›˜åˆ†å‰²æˆ64ä¸ªå•ç‹¬çš„æ–¹æ ¼ï¼Œå¹¶ä½¿ç”¨æ®‹å·®CNNå°†æ£‹å­åˆ†ç±»ä¸º13ç±»ï¼ˆ6ç§ç‹¬ç‰¹çš„ç™½æ£‹å­å’Œé»‘æ£‹å­ï¼Œä»¥åŠä¸€ä¸ªç©ºæ–¹æ ¼ï¼‰ã€‚æ®‹å·®è¿æ¥æœ‰åŠ©äºä¿ç•™ä½çº§åˆ«çš„è§†è§‰ç‰¹å¾ï¼ŒåŒæ—¶å®ç°æ›´æ·±çš„ç‰¹å¾æå–ï¼Œä»è€Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æé«˜å‡†ç¡®æ€§å’Œç¨³å®šæ€§ã€‚æˆ‘ä»¬ä½¿ç”¨åŒ…å«10800å¼ åœ¨å¤šç§ç…§æ˜æ¡ä»¶å’Œè§’åº¦ä¸‹æ‹æ‘„çš„æ ‡æ³¨æ™ºèƒ½æ‰‹æœºå›¾åƒçš„è±¡æ£‹è¯†åˆ«æ•°æ®é›†ï¼ˆChessReDï¼‰æ¥è®­ç»ƒå’Œè¯„ä¼°æˆ‘ä»¬çš„æ¨¡å‹ã€‚æ‰€å¾—çš„åˆ†ç±»ç»“æœç¼–ç ä¸ºFENå­—ç¬¦ä¸²ï¼Œå¯ä»¥è¾“å…¥åˆ°è±¡æ£‹å¼•æ“ä¸­ï¼Œç”Ÿæˆæœ€ä¼˜è´¨çš„è¡ŒåŠ¨å»ºè®®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.11522v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ç–«æƒ…æœŸé—´ï¼Œå›½é™…è±¡æ£‹è§‚çœ‹äººæ•°å¤§å¢ï¼Œä½†çº¿ä¸Šå­¦ä¹ å¹³å°çš„ä¾¿åˆ©ä»…é€‚ç”¨äºçº¿ä¸Šå›½é™…è±¡æ£‹ä½“éªŒï¼Œè€Œç°å®ä¸­çš„å›½é™…è±¡æ£‹ä»ç¼ºä¹ç›¸åº”çš„è¾…åŠ©å·¥å…·ã€‚æœ¬æ–‡ä»‹ç»äº†CVChessç³»ç»Ÿï¼Œé€šè¿‡æ·±åº¦å­¦ä¹ å°†æ£‹ç›˜å›¾åƒè½¬æ¢ä¸ºForsyth-Edwardsè®°å·æ³•ï¼ˆFENï¼‰ï¼Œè¿›è€Œè¾“å…¥åˆ°åœ¨çº¿å›½é™…è±¡æ£‹å¼•æ“è·å¾—æœ€ä½³ä¸‹ä¸€æ­¥æ£‹ç€ã€‚è¯¥ç³»ç»Ÿé‡‡ç”¨å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰è¿›è¡Œæ£‹å­è¯†åˆ«ï¼Œé€šè¿‡å¤šä¸ªæ­¥éª¤å¤„ç†æ‰‹æœºæ‹æ‘„çš„æ£‹ç›˜å›¾åƒï¼ŒåŒ…æ‹¬å›¾åƒé¢„å¤„ç†ã€æŠ•å½±å˜æ¢ã€åˆ†å‰²ä¸º64ä¸ªç‹¬ç«‹æ–¹æ ¼ä»¥åŠå°†æ£‹å­åˆ†ç±»ä¸ºé»‘ç™½æ£‹å­æˆ–ç©ºä½ç­‰13ç±»ã€‚åˆ©ç”¨æ®‹å·®è¿æ¥æé«˜ä½å±‚æ¬¡è§†è§‰ç‰¹å¾çš„ä¿ç•™èƒ½åŠ›ï¼ŒåŒæ—¶å®ç°æ·±å±‚æ¬¡ç‰¹å¾æå–ï¼Œæé«˜è®­ç»ƒå’Œè¯„ä¼°çš„å‡†ç¡®æ€§ã€‚å®éªŒé‡‡ç”¨Chess Recognition Datasetæ•°æ®é›†è¿›è¡Œè®­ç»ƒå’Œè¯„ä¼°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å›½é™…è±¡æ£‹å› ç–«æƒ…è§‚çœ‹äººæ•°å¤§å¢ï¼Œä½†ç°å®ä¸­ç¼ºä¹ç›¸åº”çš„è¾…åŠ©å·¥å…·ã€‚</li>
<li>CVChessç³»ç»Ÿé€šè¿‡æ·±åº¦å­¦ä¹ å°†æ£‹ç›˜å›¾åƒè½¬æ¢ä¸ºFENè®°å·æ³•ã€‚</li>
<li>ç³»ç»Ÿé‡‡ç”¨CNNè¿›è¡Œæ£‹å­è¯†åˆ«ï¼Œå¤„ç†æ‰‹æœºæ‹æ‘„çš„æ£‹ç›˜å›¾åƒã€‚</li>
<li>ç³»ç»ŸåŒ…æ‹¬å›¾åƒé¢„å¤„ç†ã€æŠ•å½±å˜æ¢ã€åˆ†å‰²å’Œæ£‹å­åˆ†ç±»ç­‰æ­¥éª¤ã€‚</li>
<li>æ®‹å·®è¿æ¥ç”¨äºæé«˜ä½å±‚æ¬¡è§†è§‰ç‰¹å¾çš„ä¿ç•™å’Œæ·±å±‚æ¬¡ç‰¹å¾æå–çš„å‡†ç¡®æ€§ã€‚</li>
<li>ç³»ç»Ÿé‡‡ç”¨Chess Recognition Datasetæ•°æ®é›†è¿›è¡Œè®­ç»ƒå’Œè¯„ä¼°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.11522">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-81f4495c707c1f9d9ebba10c319a418d" align="middle">
<img src="https://picx.zhimg.com/v2-01f67e97916ed97c74f6541b8f9b0e4b" align="middle">
<img src="https://picx.zhimg.com/v2-83d98356dddce15c272daac7fa3d6248" align="middle">
<img src="https://picx.zhimg.com/v2-5eed7604d85e629db88f2bfc37c8618e" align="middle">
<img src="https://picx.zhimg.com/v2-5c14674e0ec027bdf2be88497669b551" align="middle">
<img src="https://picx.zhimg.com/v2-b12897a8fb947cd87c778d2942aaa4a5" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="W2S-AlignTree-Weak-to-Strong-Inference-Time-Alignment-for-Large-Language-Models-via-Monte-Carlo-Tree-Search"><a href="#W2S-AlignTree-Weak-to-Strong-Inference-Time-Alignment-for-Large-Language-Models-via-Monte-Carlo-Tree-Search" class="headerlink" title="W2S-AlignTree: Weak-to-Strong Inference-Time Alignment for Large Language Models via Monte Carlo Tree Search"></a>W2S-AlignTree: Weak-to-Strong Inference-Time Alignment for Large Language Models via Monte Carlo Tree Search</h2><p><strong>Authors:Zhenyu Ding, Yuhao Wang, Tengyue Xiao, Haoying Wang, Guojun Ma, Mingyang Wan, Caigui Jiang, Ning Ding</strong></p>
<p>Large Language Models (LLMs) demonstrate impressive capabilities, yet their outputs often suffer from misalignment with human preferences due to the inadequacy of weak supervision and a lack of fine-grained control. Training-time alignment methods like Reinforcement Learning from Human Feedback (RLHF) face prohibitive costs in expert supervision and inherent scalability limitations, offering limited dynamic control during inference. Consequently, there is an urgent need for scalable and adaptable alignment mechanisms. To address this, we propose W2S-AlignTree, a pioneering plug-and-play inference-time alignment framework that synergistically combines Monte Carlo Tree Search (MCTS) with the Weak-to-Strong Generalization paradigm for the first time. W2S-AlignTree formulates LLM alignment as an optimal heuristic search problem within a generative search tree. By leveraging weak modelâ€™s real-time, step-level signals as alignment proxies and introducing an Entropy-Aware exploration mechanism, W2S-AlignTree enables fine-grained guidance during strong modelâ€™s generation without modifying its parameters. The approach dynamically balances exploration and exploitation in high-dimensional generation search trees. Experiments across controlled sentiment generation, summarization, and instruction-following show that W2S-AlignTree consistently outperforms strong baselines. Notably, W2S-AlignTree raises the performance of Llama3-8B from 1.89 to 2.19, a relative improvement of 15.9 on the summarization task.</p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å±•ç¤ºå‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ï¼Œç„¶è€Œï¼Œç”±äºå¼±ç›‘ç£çš„ä¸è¶³å’Œç¼ºä¹ç²¾ç»†æ§åˆ¶ï¼Œå®ƒä»¬çš„è¾“å‡ºé€šå¸¸ä¸äººç±»åå¥½å­˜åœ¨ä¸åŒ¹é…çš„æƒ…å†µã€‚ä¾‹å¦‚å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰ç­‰è®­ç»ƒæ—¶çš„å¯¹é½æ–¹æ³•ï¼Œé¢ä¸´ç€ä¸“å®¶ç›‘ç£æˆæœ¬é«˜æ˜‚å’Œå›ºæœ‰çš„å¯æ‰©å±•æ€§é™åˆ¶ï¼Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­æä¾›æœ‰é™çš„åŠ¨æ€æ§åˆ¶ã€‚å› æ­¤ï¼Œæ€¥éœ€ä¸€ç§å¯æ‰©å±•ä¸”å¯é€‚åº”çš„å¯¹é½æœºåˆ¶ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†W2S-AlignTreeï¼Œè¿™æ˜¯ä¸€ç§å¼€åˆ›æ€§çš„å³æ’å³ç”¨æ¨ç†æ—¶é—´å¯¹é½æ¡†æ¶ï¼Œé¦–æ¬¡ååŒç»“åˆäº†è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰å’Œå¼±åˆ°å¼ºæ³›åŒ–èŒƒå¼ã€‚W2S-AlignTreeå°†LLMå¯¹é½å…¬å¼åŒ–ä¸ºç”Ÿæˆæœç´¢æ ‘å†…çš„æœ€ä¼˜å¯å‘å¼æœç´¢é—®é¢˜ã€‚é€šè¿‡åˆ©ç”¨å¼±æ¨¡å‹çš„å®æ—¶ã€æ­¥éª¤çº§ä¿¡å·ä½œä¸ºå¯¹é½ä»£ç†ï¼Œå¹¶å¼•å…¥ç†µæ„ŸçŸ¥æ¢ç´¢æœºåˆ¶ï¼ŒW2S-AlignTreeèƒ½å¤Ÿåœ¨ä¸ä¿®æ”¹å¼ºæ¨¡å‹å‚æ•°çš„æƒ…å†µä¸‹ï¼Œå®ç°ç²¾ç»†æŒ‡å¯¼ã€‚è¯¥æ–¹æ³•åœ¨é«˜ç»´ç”Ÿæˆæœç´¢æ ‘ä¸­å®ç°äº†æ¢ç´¢ä¸å¼€å‘çš„åŠ¨æ€å¹³è¡¡ã€‚åœ¨å—æ§çš„æƒ…æ„Ÿç”Ÿæˆã€æ‘˜è¦ç”Ÿæˆå’Œæ‰§è¡ŒæŒ‡ä»¤ç­‰æ–¹é¢çš„å®éªŒè¡¨æ˜ï¼ŒW2S-AlignTreeå§‹ç»ˆä¼˜äºå¼ºå¤§çš„åŸºçº¿æ¨¡å‹ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒW2S-AlignTreeåœ¨æ‘˜è¦ä»»åŠ¡ä¸­å°†Llama3-8Bçš„æ€§èƒ½ä»1.89æé«˜åˆ°2.19ï¼Œç›¸å¯¹æå‡äº†15.9%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.11518v1">PDF</a> AAAI 2026 Oral</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è™½ç„¶å±•ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†å…¶è¾“å‡ºå¸¸å› ç¼ºä¹ç²¾ç»†æ§åˆ¶å’Œå¼±ç›‘ç£ä¸è¶³è€Œä¸äººç±»åå¥½ä¸ç¬¦ã€‚ç°æœ‰è®­ç»ƒæ—¶çš„å¯¹é½æ–¹æ³•å¦‚å¼ºåŒ–å­¦ä¹ äººç±»åé¦ˆï¼ˆRLHFï¼‰å­˜åœ¨é«˜æ˜‚çš„ä¸“å®¶ç›‘ç£æˆæœ¬å’Œå›ºæœ‰çš„å¯æ‰©å±•æ€§é™åˆ¶ï¼Œæ¨ç†æ—¶çš„åŠ¨æ€æ§åˆ¶æœ‰é™ã€‚å› æ­¤ï¼Œæ€¥éœ€å¯ä¼¸ç¼©ä¸”é€‚åº”æ€§å¼ºçš„å¯¹é½æœºåˆ¶ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†W2S-AlignTreeï¼Œè¿™æ˜¯ä¸€ä¸ªå¼€åˆ›æ€§çš„å³æ’å³ç”¨æ¨ç†æ—¶é—´å¯¹é½æ¡†æ¶ï¼Œé¦–æ¬¡ç»“åˆè’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰ä¸å¼±åˆ°å¼ºæ³›åŒ–èŒƒå¼ã€‚W2S-AlignTreeå°†LLMå¯¹é½åˆ¶å®šä¸ºç”Ÿæˆæœç´¢æ ‘å†…çš„æœ€ä¼˜å¯å‘å¼æœç´¢é—®é¢˜ã€‚å®ƒé€šè¿‡åˆ©ç”¨å¼±æ¨¡å‹çš„å®æ—¶æ­¥éª¤çº§ä¿¡å·ä½œä¸ºå¯¹é½ä»£ç†ï¼Œå¹¶å¼•å…¥ç†µæ„ŸçŸ¥æ¢ç´¢æœºåˆ¶ï¼Œåœ¨å¼ºæ¨¡å‹ç”Ÿæˆè¿‡ç¨‹ä¸­å®ç°äº†ç²¾ç»†æŒ‡å¯¼ï¼Œæ— éœ€ä¿®æ”¹å…¶å‚æ•°ã€‚è¯¥æ–¹æ³•åœ¨é«˜ç»´ç”Ÿæˆæœç´¢æ ‘ä¸­å®ç°äº†æ¢ç´¢ä¸å¼€å‘çš„åŠ¨æ€å¹³è¡¡ã€‚åœ¨æƒ…æ„Ÿç”Ÿæˆã€æ‘˜è¦å’ŒæŒ‡ä»¤éµå¾ªæ–¹é¢çš„å®éªŒè¡¨æ˜ï¼ŒW2S-AlignTreeæŒç»­ä¼˜äºå¼ºå¤§çš„åŸºçº¿ã€‚ç‰¹åˆ«åœ°ï¼ŒW2S-AlignTreeåœ¨æ‘˜è¦ä»»åŠ¡ä¸Šå°†Llama3-8Bçš„æ€§èƒ½ä»1.89æå‡è‡³2.19ï¼Œç›¸å¯¹æ”¹è¿›äº†15.9%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¾“å‡ºå¸¸å› ç¼ºä¹ç²¾ç»†æ§åˆ¶å’Œå¼±ç›‘ç£ä¸è¶³è€Œå¯¼è‡´ä¸äººç±»åå¥½ä¸ç¬¦çš„é—®é¢˜ã€‚</li>
<li>å½“å‰è®­ç»ƒæ—¶å¯¹é½æ–¹æ³•å¦‚å¼ºåŒ–å­¦ä¹ äººç±»åé¦ˆï¼ˆRLHFï¼‰é¢ä¸´é«˜æ˜‚çš„ä¸“å®¶ç›‘ç£æˆæœ¬å’Œå¯æ‰©å±•æ€§æŒ‘æˆ˜ã€‚</li>
<li>æ€¥éœ€åœ¨æ¨ç†æ—¶é—´å®ç°ç²¾ç»†æ§åˆ¶çš„å¯¹é½æœºåˆ¶ã€‚</li>
<li>W2S-AlignTreeæ˜¯ä¸€ä¸ªå³æ’å³ç”¨æ¨ç†æ—¶é—´å¯¹é½æ¡†æ¶ï¼Œç»“åˆè’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰ä¸å¼±åˆ°å¼ºæ³›åŒ–èŒƒå¼ã€‚</li>
<li>W2S-AlignTreeé€šè¿‡å°†LLMå¯¹é½åˆ¶å®šä¸ºç”Ÿæˆæœç´¢æ ‘å†…çš„æœ€ä¼˜å¯å‘å¼æœç´¢é—®é¢˜æ¥æ”¹è¿›æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>W2S-AlignTreeåˆ©ç”¨å¼±æ¨¡å‹çš„å®æ—¶æ­¥éª¤çº§ä¿¡å·ä½œä¸ºå¯¹é½ä»£ç†ï¼Œæ— éœ€ä¿®æ”¹å¼ºæ¨¡å‹çš„å‚æ•°å³å¯å®ç°ç²¾ç»†æŒ‡å¯¼ã€‚</li>
<li>W2S-AlignTreeåœ¨é«˜ç»´ç”Ÿæˆæœç´¢æ ‘ä¸­å®ç°äº†æ¢ç´¢ä¸å¼€å‘çš„åŠ¨æ€å¹³è¡¡ï¼Œä¸”åœ¨æƒ…æ„Ÿç”Ÿæˆã€æ‘˜è¦å’ŒæŒ‡ä»¤éµå¾ªæ–¹é¢çš„å®éªŒè¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.11518">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-de69885c30ae78ac5302d0fff36272b9" align="middle">
<img src="https://picx.zhimg.com/v2-a2daf1c51eb6dcd3d0082159ae5d559d" align="middle">
<img src="https://picx.zhimg.com/v2-4d881ca431f283e21536265b4000f908" align="middle">
<img src="https://picx.zhimg.com/v2-1eea6a8df130b7f589c02b33711bf882" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="FarSkip-Collective-Unhobbling-Blocking-Communication-in-Mixture-of-Experts-Models"><a href="#FarSkip-Collective-Unhobbling-Blocking-Communication-in-Mixture-of-Experts-Models" class="headerlink" title="FarSkip-Collective: Unhobbling Blocking Communication in Mixture of Experts Models"></a>FarSkip-Collective: Unhobbling Blocking Communication in Mixture of Experts Models</h2><p><strong>Authors:Yonatan Dukler, Guihong Li, Deval Shah, Vikram Appia, Emad Barsoum</strong></p>
<p>Blocking communication presents a major hurdle in running MoEs efficiently in distributed settings. To address this, we present FarSkip-Collective which modifies the architecture of modern models to enable overlapping of their computation with communication. Our approach modifies the architecture to skip connections in the model and it is unclear a priori whether the modified model architecture can remain as capable, especially for large state-of-the-art models and while modifying all of the model layers. We answer this question in the affirmative and fully convert a series of state-of-the-art models varying from 16B to 109B parameters to enable overlapping of their communication while achieving accuracy on par with their original open-source releases. For example, we convert Llama 4 Scout (109B) via self-distillation and achieve average accuracy within 1% of its instruction tuned release averaged across a wide range of downstream evaluations. In addition to demonstrating retained accuracy of the large modified models, we realize the benefits of FarSkip-Collective through optimized implementations that explicitly overlap communication with computation, accelerating both training and inference in existing frameworks.</p>
<blockquote>
<p>é˜»æ–­é€šä¿¡æ˜¯åœ¨åˆ†å¸ƒå¼ç¯å¢ƒä¸­é«˜æ•ˆè¿è¡ŒMoEsçš„ä¸»è¦éšœç¢ä¹‹ä¸€ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†FarSkip-Collectiveï¼Œå®ƒé€šè¿‡ä¿®æ”¹ç°ä»£æ¨¡å‹çš„æ¶æ„ï¼Œä½¿è®¡ç®—ä¸é€šä¿¡èƒ½å¤Ÿé‡å ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¿®æ”¹äº†æ¨¡å‹çš„æ¶æ„ï¼Œè·³è¿‡äº†æ¨¡å‹ä¸­çš„ä¸€äº›è¿æ¥ï¼Œäº‹å…ˆå¹¶ä¸æ¸…æ¥šä¿®æ”¹åçš„æ¨¡å‹æ¶æ„æ˜¯å¦è¿˜èƒ½ä¿æŒå…¶åŠŸèƒ½ï¼Œå°¤å…¶æ˜¯å¯¹äºå¤§å‹æœ€å…ˆè¿›çš„æ¨¡å‹å’Œæ‰€æœ‰æ¨¡å‹å±‚çš„ä¿®æ”¹ã€‚æˆ‘ä»¬è‚¯å®šåœ°å›ç­”äº†è¿™ä¸ªé—®é¢˜ï¼Œå¹¶å°†ä¸€ç³»åˆ—æœ€å…ˆè¿›çš„æ¨¡å‹ä»16Båˆ°109Bå‚æ•°å®Œå…¨è½¬æ¢ï¼Œä½¿å®ƒä»¬çš„é€šä¿¡èƒ½å¤Ÿé‡å ï¼ŒåŒæ—¶ä¿è¯å‡†ç¡®ç‡ä¸åŸå§‹å¼€æºå‘å¸ƒç‰ˆæœ¬ç›¸å½“ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬é€šè¿‡è‡ªæˆ‘è’¸é¦å°†Llama 4 Scoutï¼ˆ109Bï¼‰è¿›è¡Œè½¬æ¢ï¼Œåœ¨å¤šç§ä¸‹æ¸¸è¯„ä¼°ä¸­çš„å¹³å‡å‡†ç¡®ç‡åœ¨å…¶æŒ‡ä»¤è°ƒæ•´å‘å¸ƒçš„1%ä»¥å†…ã€‚é™¤äº†è¯æ˜å¤§å‹ä¿®æ”¹æ¨¡å‹çš„ä¿ç•™å‡†ç¡®ç‡å¤–ï¼Œæˆ‘ä»¬è¿˜é€šè¿‡ä¼˜åŒ–å®ç°è®¤è¯†åˆ°FarSkip-Collectiveçš„å¥½å¤„ï¼Œè¿™äº›å®ç°æ˜ç¡®åœ°ä½¿é€šä¿¡ä¸è®¡ç®—é‡å ï¼ŒåŠ å¿«ç°æœ‰æ¡†æ¶ä¸­çš„è®­ç»ƒå’Œæ¨ç†é€Ÿåº¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.11505v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æ¢è®¨äº†åœ¨ç°ä»£æ¨¡å‹è¿è¡Œä¸­ï¼Œæ²Ÿé€šé˜»ç¢å¯¹æ¨¡å‹æ•ˆç‡çš„å½±å“ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†FarSkip-Collectiveæ–¹æ³•ï¼Œé€šè¿‡ä¿®æ”¹æ¨¡å‹æ¶æ„å®ç°è®¡ç®—ä¸æ²Ÿé€šçš„é‡å ã€‚æ­¤æ–¹æ³•ä¿®æ”¹äº†æ¨¡å‹çš„è¿æ¥ç»“æ„ï¼Œå°šä¸æ˜ç¡®ä¿®æ”¹åçš„æ¨¡å‹æ¶æ„æ˜¯å¦ä»ç„¶å…·æœ‰åŠŸèƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤§å‹å…ˆè¿›æ¨¡å‹çš„å…¨éƒ¨å±‚éƒ½è¿›è¡Œä¿®æ”¹çš„æƒ…å†µä¸‹ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤§å‹å…ˆè¿›æ¨¡å‹ä¸Šå®ç°äº†æ²Ÿé€šçš„é‡å ï¼ŒåŒæ—¶ä¿æŒäº†ä¸åŸå§‹å¼€æºç‰ˆæœ¬ç›¸å½“çš„ç²¾åº¦ã€‚ä¾‹å¦‚ï¼Œé€šè¿‡è‡ªè’¸é¦æ–¹æ³•è½¬åŒ–äº†è§„æ¨¡ä¸º109Bå‚æ•°çš„Llama 4 Scoutæ¨¡å‹ï¼Œåœ¨å¤šç§ä¸‹æ¸¸è¯„ä¼°ä¸­å–å¾—äº†ä¸æŒ‡ä»¤å¾®è°ƒç‰ˆæœ¬ç›¸å·®ä¸åˆ°1%çš„å¹³å‡ç²¾åº¦ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜é€šè¿‡ä¼˜åŒ–å®ç°ï¼Œæ˜ç¡®äº†FarSkip-Collectiveçš„ä¼˜åŠ¿ï¼Œå®ç°äº†è®¡ç®—ä¸æ²Ÿé€šçš„æ˜ç¡®é‡å ï¼Œæé«˜äº†ç°æœ‰æ¡†æ¶çš„è®­ç»ƒå’Œæ¨ç†é€Ÿåº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FarSkip-Collectiveæ–¹æ³•æ—¨åœ¨è§£å†³åœ¨åˆ†å¸ƒå¼ç¯å¢ƒä¸­è¿è¡Œå¤§å‹æ¨¡å‹æ—¶çš„æ²Ÿé€šéšœç¢é—®é¢˜ã€‚</li>
<li>é€šè¿‡ä¿®æ”¹ç°ä»£æ¨¡å‹çš„æ¶æ„ï¼Œå®ç°è®¡ç®—ä¸æ²Ÿé€šçš„é‡å ã€‚</li>
<li>å¯¹äºå¤§å‹å…ˆè¿›æ¨¡å‹çš„å…¨å±‚ä¿®æ”¹åï¼Œä»èƒ½ä¿æŒæ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>æˆåŠŸè½¬åŒ–äº†å¤šç§è§„æ¨¡çš„å…ˆè¿›æ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨æ²Ÿé€šçš„åŒæ—¶å®ç°è®¡ç®—é‡å ã€‚</li>
<li>å¯¹è§„æ¨¡ä¸º109Bå‚æ•°çš„Llama 4 Scoutæ¨¡å‹è¿›è¡Œäº†è‡ªè’¸é¦è½¬åŒ–ï¼Œåœ¨å¤šç§ä¸‹æ¸¸è¯„ä¼°ä¸­çš„ç²¾åº¦æŸå¤±å°äº1%ã€‚</li>
<li>FarSkip-Collectiveçš„ä¼˜åŒ–å®ç°å¯ä»¥æ˜ç¡®é‡å æ²Ÿé€šè®¡ç®—ï¼Œæé«˜ç°æœ‰æ¡†æ¶çš„è®­ç»ƒå’Œæ¨ç†é€Ÿåº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.11505">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-53760899ca67ff430581dc6ca7b75e9d" align="middle">
<img src="https://picx.zhimg.com/v2-7ddd3912739ef9c6614c661be937d566" align="middle">
<img src="https://picx.zhimg.com/v2-fabf4cc0f0e04c4fc7b6f0047c8078a5" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="ImAgent-A-Unified-Multimodal-Agent-Framework-for-Test-Time-Scalable-Image-Generation"><a href="#ImAgent-A-Unified-Multimodal-Agent-Framework-for-Test-Time-Scalable-Image-Generation" class="headerlink" title="ImAgent: A Unified Multimodal Agent Framework for Test-Time Scalable Image Generation"></a>ImAgent: A Unified Multimodal Agent Framework for Test-Time Scalable Image Generation</h2><p><strong>Authors:Kaishen Wang, Ruibo Chen, Tong Zheng, Heng Huang</strong></p>
<p>Recent text-to-image (T2I) models have made remarkable progress in generating visually realistic and semantically coherent images. However, they still suffer from randomness and inconsistency with the given prompts, particularly when textual descriptions are vague or underspecified. Existing approaches, such as prompt rewriting, best-of-N sampling, and self-refinement, can mitigate these issues but usually require additional modules and operate independently, hindering test-time scaling efficiency and increasing computational overhead. In this paper, we introduce ImAgent, a training-free unified multimodal agent that integrates reasoning, generation, and self-evaluation within a single framework for efficient test-time scaling. Guided by a policy controller, multiple generation actions dynamically interact and self-organize to enhance image fidelity and semantic alignment without relying on external models. Extensive experiments on image generation and editing tasks demonstrate that ImAgent consistently improves over the backbone and even surpasses other strong baselines where the backbone model fails, highlighting the potential of unified multimodal agents for adaptive and efficient image generation under test-time scaling.</p>
<blockquote>
<p>æœ€è¿‘çš„æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ¨¡å‹åœ¨ç”Ÿæˆè§†è§‰çœŸå®æ„Ÿå’Œè¯­ä¹‰è¿è´¯æ€§å›¾åƒæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œå®ƒä»¬ä»ç„¶å—åˆ°éšæœºæ€§å’Œä¸ç»™å®šæç¤ºä¸ä¸€è‡´æ€§çš„å›°æ‰°ï¼Œç‰¹åˆ«æ˜¯åœ¨æ–‡æœ¬æè¿°æ¨¡ç³Šæˆ–æœªæŒ‡å®šæ—¶ã€‚ç°æœ‰æ–¹æ³•ï¼Œå¦‚æç¤ºé‡å†™ã€æœ€ä½³Né‡‡æ ·å’Œè‡ªæˆ‘å®Œå–„ï¼Œå¯ä»¥ç¼“è§£è¿™äº›é—®é¢˜ï¼Œä½†å®ƒä»¬é€šå¸¸éœ€è¦é¢å¤–çš„æ¨¡å—å¹¶ä¸”ç‹¬ç«‹è¿è¡Œï¼Œé˜»ç¢äº†æµ‹è¯•æ—¶çš„æ‰©å±•æ•ˆç‡å¹¶å¢åŠ äº†è®¡ç®—å¼€é”€ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ImAgentï¼Œè¿™æ˜¯ä¸€ä¸ªæ— éœ€è®­ç»ƒçš„ç»Ÿä¸€å¤šæ¨¡æ€ä»£ç†ï¼Œå®ƒåœ¨ä¸€ä¸ªæ¡†æ¶å†…é›†æˆäº†æ¨ç†ã€ç”Ÿæˆå’Œè‡ªæˆ‘è¯„ä¼°ï¼Œä»¥å®ç°é«˜æ•ˆçš„æµ‹è¯•æ—¶é—´æ‰©å±•ã€‚åœ¨ç­–ç•¥æ§åˆ¶å™¨çš„å¼•å¯¼ä¸‹ï¼Œå¤šä¸ªç”ŸæˆåŠ¨ä½œåŠ¨æ€äº¤äº’å¹¶è‡ªæˆ‘ç»„ç»‡ï¼Œä»¥æé«˜å›¾åƒçš„çœŸå®æ€§å’Œè¯­ä¹‰ä¸€è‡´æ€§ï¼Œè€Œæ— éœ€ä¾èµ–å¤–éƒ¨æ¨¡å‹ã€‚åœ¨å›¾åƒç”Ÿæˆå’Œç¼–è¾‘ä»»åŠ¡ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒImAgentæŒç»­æé«˜äº†åŸºç¡€æ¨¡å‹çš„æ€§èƒ½ï¼Œç”šè‡³åœ¨åŸºç¡€æ¨¡å‹å¤±è´¥çš„æƒ…å†µä¸‹è¶…è¿‡äº†å…¶ä»–å¼ºå¤§çš„åŸºçº¿æ¨¡å‹ï¼Œè¿™å‡¸æ˜¾äº†ç»Ÿä¸€å¤šæ¨¡æ€ä»£ç†åœ¨æµ‹è¯•æ—¶é—´æ‰©å±•ä¸‹çš„è‡ªé€‚åº”å’Œé«˜æ•ˆå›¾åƒç”Ÿæˆçš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.11483v1">PDF</a> 12 pages, 5 tables, 6 figures</p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬æè¿°äº†ä¸€ç§æ— éœ€è®­ç»ƒçš„ç»Ÿä¸€å¤šæ¨¡æ€ä»£ç†ImAgentï¼Œèƒ½å¤Ÿåœ¨å•ä¸€æ¡†æ¶å†…å®ç°æ¨ç†ã€ç”Ÿæˆå’Œè‡ªæˆ‘è¯„ä¼°ï¼Œä»è€Œæé«˜æµ‹è¯•æ—¶çš„æ•ˆç‡ã€‚ImAgenté€šè¿‡ç­–ç•¥æ§åˆ¶å™¨å¼•å¯¼ï¼Œå¤šä¸ªç”ŸæˆåŠ¨ä½œèƒ½å¤ŸåŠ¨æ€äº¤äº’å’Œè‡ªæˆ‘ç»„ç»‡ï¼Œå¢å¼ºå›¾åƒçš„çœŸå®æ€§å’Œè¯­ä¹‰ä¸€è‡´æ€§ï¼Œä¸”ä¸ä¾èµ–å¤–éƒ¨æ¨¡å‹ã€‚åœ¨å›¾åƒç”Ÿæˆå’Œç¼–è¾‘ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒImAgentèƒ½å¤ŸæŒç»­æé«˜èƒŒæ™¯æ¨¡å‹çš„æ€§èƒ½ï¼Œç”šè‡³åœ¨èƒŒæ™¯æ¨¡å‹å¤±è´¥çš„æƒ…å†µä¸‹è¶…è¶Šå…¶ä»–å¼ºå¤§çš„åŸºçº¿æ¨¡å‹ï¼Œå±•ç°å‡ºç»Ÿä¸€å¤šæ¨¡æ€ä»£ç†åœ¨æµ‹è¯•æ—¶è‡ªé€‚åº”å’Œé«˜æ•ˆå›¾åƒç”Ÿæˆçš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ImAgentæ˜¯ä¸€ä¸ªæ— éœ€è®­ç»ƒçš„å¤šæ¨¡æ€ä»£ç†ï¼Œèƒ½åœ¨å•ä¸€æ¡†æ¶å†…å®Œæˆæ¨ç†ã€ç”Ÿæˆå’Œè‡ªæˆ‘è¯„ä¼°ã€‚</li>
<li>ImAgenté€šè¿‡ç­–ç•¥æ§åˆ¶å™¨å¼•å¯¼å¤šä¸ªç”ŸæˆåŠ¨ä½œï¼Œå®ç°åŠ¨æ€äº¤äº’å’Œè‡ªæˆ‘ç»„ç»‡ã€‚</li>
<li>ImAgentèƒ½æé«˜å›¾åƒçš„çœŸå®æ€§å’Œè¯­ä¹‰ä¸€è‡´æ€§ï¼Œä¸”ä¸ä¾èµ–å¤–éƒ¨æ¨¡å‹ã€‚</li>
<li>ä¸èƒŒæ™¯æ¨¡å‹å’Œå…¶ä»–åŸºçº¿æ¨¡å‹ç›¸æ¯”ï¼ŒImAgentåœ¨å›¾åƒç”Ÿæˆå’Œç¼–è¾‘ä»»åŠ¡ä¸Šè¡¨ç°æ›´ä¼˜ç§€ã€‚</li>
<li>ImAgentèƒ½åœ¨æµ‹è¯•æ—¶å®ç°è‡ªé€‚åº”å’Œé«˜æ•ˆçš„å›¾åƒç”Ÿæˆã€‚</li>
<li>ç°æœ‰T2Iæ¨¡å‹å­˜åœ¨çš„é—®é¢˜åŒ…æ‹¬éšæœºæ€§å’Œä¸æç¤ºçš„ä¸ä¸€è‡´æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨æ–‡æœ¬æè¿°æ¨¡ç³Šæˆ–æœªæŒ‡å®šçš„æƒ…å†µä¸‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.11483">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c0f12726d17ef4a9258449a5e9cc5ce2" align="middle">
<img src="https://picx.zhimg.com/v2-226b0570ec9c347da4ee51fcb163817a" align="middle">
<img src="https://picx.zhimg.com/v2-6052c110c57cd9c4ae5cb6f1a87075a0" align="middle">
<img src="https://picx.zhimg.com/v2-61c643ffc15036bbe8b5983ea8238639" align="middle">
<img src="https://picx.zhimg.com/v2-993a9a38cadef608025b04185f94be13" align="middle">
<img src="https://picx.zhimg.com/v2-a19b7102f6d9297b4e8420998ccc1c6e" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Rethinking-Progression-of-Memory-State-in-Robotic-Manipulation-An-Object-Centric-Perspective"><a href="#Rethinking-Progression-of-Memory-State-in-Robotic-Manipulation-An-Object-Centric-Perspective" class="headerlink" title="Rethinking Progression of Memory State in Robotic Manipulation: An Object-Centric Perspective"></a>Rethinking Progression of Memory State in Robotic Manipulation: An Object-Centric Perspective</h2><p><strong>Authors:Nhat Chung, Taisei Hanyu, Toan Nguyen, Huy Le, Frederick Bumgarner, Duy Minh Ho Nguyen, Khoa Vo, Kashu Yamazaki, Chase Rainwater, Tung Kieu, Anh Nguyen, Ngan Le</strong></p>
<p>As embodied agents operate in increasingly complex environments, the ability to perceive, track, and reason about individual object instances over time becomes essential, especially in tasks requiring sequenced interactions with visually similar objects. In these non-Markovian settings, key decision cues are often hidden in object-specific histories rather than the current scene. Without persistent memory of prior interactions (what has been interacted with, where it has been, or how it has changed) visuomotor policies may fail, repeat past actions, or overlook completed ones. To surface this challenge, we introduce LIBERO-Mem, a non-Markovian task suite for stress-testing robotic manipulation under object-level partial observability. It combines short- and long-horizon object tracking with temporally sequenced subgoals, requiring reasoning beyond the current frame. However, vision-language-action (VLA) models often struggle in such settings, with token scaling quickly becoming intractable even for tasks spanning just a few hundred frames. We propose Embodied-SlotSSM, a slot-centric VLA framework built for temporal scalability. It maintains spatio-temporally consistent slot identities and leverages them through two mechanisms: (1) slot-state-space modeling for reconstructing short-term history, and (2) a relational encoder to align the input tokens with action decoding. Together, these components enable temporally grounded, context-aware action prediction. Experiments show Embodied-SlotSSMâ€™s baseline performance on LIBERO-Mem and general tasks, offering a scalable solution for non-Markovian reasoning in object-centric robotic policies.</p>
<blockquote>
<p>éšç€æ™ºèƒ½ä½“åœ¨æ—¥ç›Šå¤æ‚çš„ç¯å¢ƒä¸­è¿ä½œï¼Œéšæ—¶é—´æ„ŸçŸ¥ã€è¿½è¸ªå’Œæ¨ç†å•ä¸ªå¯¹è±¡å®ä¾‹çš„èƒ½åŠ›å˜å¾—è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦æ‰§è¡Œä¸è§†è§‰ä¸Šç›¸ä¼¼å¯¹è±¡çš„åºåˆ—äº¤äº’çš„ä»»åŠ¡ä¸­ã€‚åœ¨è¿™äº›éé©¬å°”å¯å¤«ç¯å¢ƒä¸­ï¼Œå…³é”®çš„å†³ç­–çº¿ç´¢é€šå¸¸éšè—åœ¨ç‰¹å®šå¯¹è±¡çš„è¿‡å»ç»å†ä¸­ï¼Œè€Œéå½“å‰åœºæ™¯ä¸­ã€‚æ²¡æœ‰å¯¹è¿‡å»äº¤äº’çš„æŒç»­è®°å¿†ï¼ˆå·²äº¤äº’çš„å¯¹è±¡ã€å¯¹è±¡æ‰€åœ¨ä½ç½®æˆ–å…¶å˜åŒ–æ–¹å¼ï¼‰ï¼Œè§†è§‰è¿åŠ¨ç­–ç•¥å¯èƒ½ä¼šå¤±æ•ˆã€é‡å¤è¿‡å»çš„è¡ŒåŠ¨æˆ–å¿½ç•¥å·²å®Œæˆçš„è¡ŒåŠ¨ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†LIBERO-Memï¼Œè¿™æ˜¯ä¸€ä¸ªéé©¬å°”å¯å¤«ä»»åŠ¡å¥—ä»¶ï¼Œç”¨äºåœ¨å¯¹è±¡çº§åˆ«çš„éƒ¨åˆ†å¯è§‚å¯Ÿæ€§ä¸‹å¯¹æœºå™¨äººæ“ä½œè¿›è¡Œå‹åŠ›æµ‹è¯•ã€‚å®ƒå°†çŸ­è§†å’Œé•¿è§†å¯¹è±¡è¿½è¸ªä¸æŒ‰æ—¶é—´é¡ºåºæ’åˆ—çš„å­ç›®æ ‡ç›¸ç»“åˆï¼Œéœ€è¦è¿›è¡Œè¶…å‡ºå½“å‰æ¡†æ¶çš„æ¨ç†ã€‚ç„¶è€Œï¼Œåœ¨è¿™ç§ç¯å¢ƒä¸­ï¼Œè§†è§‰è¯­è¨€åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹å¾€å¾€è¡¨ç°å›°éš¾ï¼Œå³ä½¿åœ¨ä»…æ¶‰åŠæ•°ç™¾å¸§çš„ä»»åŠ¡ä¸­ï¼Œç¬¦å·æ‰©å±•ä¹Ÿè¿…é€Ÿå˜å¾—éš¾ä»¥è§£å†³ã€‚æˆ‘ä»¬æå‡ºäº†é¢å‘æ—¶ç©ºå¯æ‰©å±•æ€§çš„æ’æ§½ä¸­å¿ƒVLAæ¡†æ¶Embodied-SlotSSMã€‚å®ƒä¿æŒæ—¶ç©ºä¸€è‡´çš„æ’æ§½èº«ä»½ï¼Œå¹¶é€šè¿‡ä¸¤ç§æœºåˆ¶åŠ ä»¥åˆ©ç”¨ï¼šï¼ˆ1ï¼‰æ’æ§½çŠ¶æ€ç©ºé—´å»ºæ¨¡ä»¥é‡å»ºçŸ­æœŸå†å²ï¼Œï¼ˆ2ï¼‰å…³ç³»ç¼–ç å™¨ä»¥å¯¹è¾“å…¥ç¬¦å·ä¸è¡ŒåŠ¨è§£ç è¿›è¡Œå¯¹é½ã€‚è¿™äº›ç»„ä»¶å…±åŒä½œç”¨ï¼Œå®ç°äº†åŸºäºæ—¶é—´ã€å…·æœ‰ä¸Šä¸‹æ–‡æ„è¯†çš„è¡ŒåŠ¨é¢„æµ‹ã€‚å®éªŒè¡¨æ˜ï¼ŒEmbodied-SlotSSMåœ¨LIBERO-Memå’Œä¸€èˆ¬ä»»åŠ¡ä¸Šçš„åŸºå‡†æ€§èƒ½è¡¨ç°ä¼˜å¼‚ï¼Œä¸ºéé©¬å°”å¯å¤«æ¨ç†åœ¨é¢å‘å¯¹è±¡çš„æœºå™¨äººç­–ç•¥ä¸­æä¾›äº†å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.11478v1">PDF</a> Accepted at AAAI 2026</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨æ—¥ç›Šå¤æ‚çš„ç¯å¢ƒä¸­ï¼Œå¯¹äºä¸ªä½“å¯¹è±¡å®ä¾‹çš„æ„ŸçŸ¥ã€è¿½è¸ªå’Œæ—¶é—´æ¨ç†èƒ½åŠ›çš„é‡è¦æ€§ã€‚ç‰¹åˆ«æ˜¯åœ¨éœ€è¦åºåˆ—åŒ–ä¸è§†è§‰ç›¸ä¼¼å¯¹è±¡äº¤äº’çš„ä»»åŠ¡ä¸­ï¼Œå…³é”®å†³ç­–çº¿ç´¢é€šå¸¸éšè—åœ¨ç‰¹å®šå¯¹è±¡çš„å†å²ä¸­è€Œéå½“å‰åœºæ™¯ä¸­ã€‚ç¼ºä¹å…ˆå‰äº¤äº’çš„æŒä¹…è®°å¿†å¯èƒ½å¯¼è‡´è§†è§‰è¿åŠ¨ç­–ç•¥å¤±è´¥ã€é‡å¤è¿‡å»çš„è¡ŒåŠ¨æˆ–å¿½è§†å·²å®Œæˆçš„è¡ŒåŠ¨ã€‚ä¸ºåº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæœ¬æ–‡å¼•å…¥äº†LIBERO-Memï¼Œä¸€ä¸ªéé©¬å°”å¯å¤«ä»»åŠ¡å¥—ä»¶ï¼Œç”¨äºæµ‹è¯•åœ¨å¯¹è±¡å±‚é¢éƒ¨åˆ†å¯è§‚å¯Ÿæ€§ä¸‹çš„æœºå™¨äººæ“ä½œã€‚å®ƒç»“åˆäº†çŸ­æœŸå’Œé•¿æœŸå¯¹è±¡è·Ÿè¸ªä»¥åŠæ—¶é—´åºåˆ—å­ç›®æ ‡ï¼Œéœ€è¦è¶…è¶Šå½“å‰æ¡†æ¶çš„æ¨ç†ã€‚ç„¶è€Œï¼Œåœ¨è¿™ç§è®¾ç½®ä¸­ï¼Œè§†è§‰è¯­è¨€åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹é€šå¸¸è¡¨ç°æŒ£æ‰ï¼Œå³ä½¿åœ¨åªæ¶‰åŠæ•°ç™¾å¸§çš„ä»»åŠ¡ä¸­ï¼Œç¬¦å·æ‰©å±•ä¹Ÿå˜å¾—è¿…é€Ÿä¸å¯è¡Œã€‚æœ¬æ–‡æå‡ºäº†Embodied-SlotSSMï¼Œä¸€ä¸ªé’ˆå¯¹æ—¶é—´å¯æ‰©å±•æ€§çš„æ§½ä½ä¸­å¿ƒVLAæ¡†æ¶ã€‚å®ƒä¿æŒæ—¶ç©ºä¸€è‡´çš„æ§½ä½èº«ä»½ï¼Œå¹¶é€šè¿‡ä¸¤ç§æœºåˆ¶åˆ©ç”¨å®ƒä»¬ï¼šï¼ˆ1ï¼‰æ§½ä½çŠ¶æ€ç©ºé—´å»ºæ¨¡ä»¥é‡å»ºçŸ­æœŸå†å²ï¼Œï¼ˆ2ï¼‰å…³ç³»ç¼–ç å™¨ä»¥ä¸è¡ŒåŠ¨è§£ç å¯¹é½è¾“å…¥ç¬¦å·ã€‚è¿™äº›ç»„ä»¶å…±åŒå®ç°äº†æ—¶é—´ä¸Šçš„ã€ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„è¡ŒåŠ¨é¢„æµ‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åœ¨å¤æ‚ç¯å¢ƒä¸­ï¼Œå¯¹ä¸ªä½“å¯¹è±¡å®ä¾‹çš„æ„ŸçŸ¥ã€è¿½è¸ªå’Œæ—¶é—´æ¨ç†èƒ½åŠ›å˜å¾—è‡³å…³é‡è¦ã€‚</li>
<li>åœ¨éœ€è¦åºåˆ—åŒ–çš„è§†è§‰ç›¸ä¼¼å¯¹è±¡äº¤äº’çš„ä»»åŠ¡ä¸­ï¼Œå…³é”®å†³ç­–çº¿ç´¢éšè—åœ¨å¯¹è±¡ç‰¹å®šçš„å†å²ä¸­ã€‚</li>
<li>ç¼ºä¹å¯¹å…ˆå‰äº¤äº’çš„æŒä¹…è®°å¿†å¯èƒ½å¯¼è‡´è§†è§‰è¿åŠ¨ç­–ç•¥å¤±è´¥ã€‚</li>
<li>LIBERO-Memæ˜¯ä¸€ä¸ªéé©¬å°”å¯å¤«ä»»åŠ¡å¥—ä»¶ï¼Œæ—¨åœ¨åº”å¯¹åœ¨å¯¹è±¡å±‚é¢éƒ¨åˆ†å¯è§‚å¯Ÿæ€§ä¸‹çš„æœºå™¨äººæ“ä½œæŒ‘æˆ˜ã€‚</li>
<li>è§†è§‰è¯­è¨€åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹åœ¨éé©¬å°”å¯å¤«ç¯å¢ƒä¸­é€šå¸¸è¡¨ç°ä¸ä½³ã€‚</li>
<li>Embodied-SlotSSMæ˜¯ä¸€ä¸ªé’ˆå¯¹æ—¶é—´å¯æ‰©å±•æ€§çš„æ§½ä½ä¸­å¿ƒVLAæ¡†æ¶ï¼Œé€šè¿‡ç»´æŒæ—¶ç©ºä¸€è‡´çš„æ§½ä½èº«ä»½å¹¶åˆ©ç”¨æ§½ä½çŠ¶æ€ç©ºé—´å»ºæ¨¡å’Œå…³ç³»ç¼–ç å™¨æ¥å®ç°ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„è¡ŒåŠ¨é¢„æµ‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.11478">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-688083474789dd6ba577e7f8e1272347" align="middle">
<img src="https://picx.zhimg.com/v2-d937f293e63a645c27487696348db4b6" align="middle">
<img src="https://picx.zhimg.com/v2-4fd6967dd743156a90717ec58d02a4c7" align="middle">
<img src="https://picx.zhimg.com/v2-73b0f5243ed7ba4fc789dd6b385047fd" align="middle">
<img src="https://picx.zhimg.com/v2-d1015a6f455f5eff9774fe3d23f552f2" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="From-Synthetic-Scenes-to-Real-Performance-Enhancing-Spatial-Reasoning-in-VLMs"><a href="#From-Synthetic-Scenes-to-Real-Performance-Enhancing-Spatial-Reasoning-in-VLMs" class="headerlink" title="From Synthetic Scenes to Real Performance: Enhancing Spatial Reasoning in VLMs"></a>From Synthetic Scenes to Real Performance: Enhancing Spatial Reasoning in VLMs</h2><p><strong>Authors:Massimo Rizzoli, Simone Alghisi, Seyed Mahed Mousavi, Giuseppe Riccardi</strong></p>
<p>Fine-tuning Vision-Language Models (VLMs) is a common strategy to improve performance following an ad-hoc data collection and annotation of real-world scenes. However, this process is often prone to biases, errors, and distribution imbalance, resulting in overfitting and imbalanced performance. Although a few studies have tried to address this problem by generating synthetic data, they lacked control over distribution bias and annotation quality. To address these challenges, we redesign the fine-tuning process in two ways. First, we control the generation of data and its annotations, ensuring it is free from bias, distribution imbalance, and annotation errors. We automatically construct the dataset by comprehensively sampling objectsâ€™ attributes, including color, shape, size, and position within the scene. Secondly, using this annotated dataset, we fine-tune state-of-the-art VLMs and assess performance transferability to real-world data on the absolute position task. We conduct exhaustive evaluations on both synthetic and real-world benchmarks. Our experiments reveal two key findings: 1) fine-tuning on balanced synthetic data yields uniform performance across the visual scene and mitigates common biases; and 2) fine-tuning on synthetic stimuli significantly improves performance on real-world data (COCO), outperforming models fine-tuned in the matched setting.</p>
<blockquote>
<p>å¾®è°ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰æ˜¯ä¸€ç§å¸¸è§çš„ç­–ç•¥ï¼Œç”¨äºåœ¨æ”¶é›†ç‰¹å®šæ•°æ®å¹¶æ ‡æ³¨çœŸå®åœºæ™¯ä¹‹åæé«˜æ€§èƒ½ã€‚ç„¶è€Œï¼Œè¿™ä¸ªè¿‡ç¨‹å¾€å¾€å®¹æ˜“å—åˆ°åè§ã€é”™è¯¯å’Œåˆ†å¸ƒä¸å¹³è¡¡çš„å½±å“ï¼Œå¯¼è‡´è¿‡åº¦æ‹Ÿåˆå’Œæ€§èƒ½ä¸å¹³è¡¡ã€‚å°½ç®¡æœ‰ä¸€äº›ç ”ç©¶è¯•å›¾é€šè¿‡ç”Ÿæˆåˆæˆæ•°æ®æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½†å®ƒä»¬å¯¹åˆ†å¸ƒåè§å’Œæ ‡æ³¨è´¨é‡ç¼ºä¹æ§åˆ¶ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬ä»¥ä¸¤ç§æ–¹å¼é‡æ–°è®¾è®¡å¾®è°ƒè¿‡ç¨‹ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æ§åˆ¶æ•°æ®çš„ç”ŸæˆåŠå…¶æ³¨é‡Šï¼Œç¡®ä¿å®ƒä¸å—åè§ã€åˆ†å¸ƒä¸å¹³è¡¡å’Œæ³¨é‡Šé”™è¯¯çš„å½±å“ã€‚æˆ‘ä»¬é€šè¿‡å…¨é¢é‡‡æ ·åœºæ™¯ä¸­ç‰©ä½“çš„å±æ€§ï¼ˆåŒ…æ‹¬é¢œè‰²ã€å½¢çŠ¶ã€å¤§å°å’Œä½ç½®ï¼‰æ¥è‡ªåŠ¨æ„å»ºæ•°æ®é›†ã€‚å…¶æ¬¡ï¼Œä½¿ç”¨è¿™ä¸ªæ ‡æ³¨æ•°æ®é›†ï¼Œæˆ‘ä»¬å¯¹æœ€å…ˆè¿›çš„VLMsè¿›è¡Œå¾®è°ƒï¼Œå¹¶è¯„ä¼°åœ¨ç»å¯¹ä½ç½®ä»»åŠ¡ä¸Šè½¬ç§»åˆ°çœŸå®ä¸–ç•Œæ•°æ®çš„æ€§èƒ½ã€‚æˆ‘ä»¬åœ¨åˆæˆå’ŒçœŸå®ä¸–ç•Œçš„åŸºå‡†æµ‹è¯•ä¸Šéƒ½è¿›è¡Œäº†è¯¦å°½çš„è¯„ä¼°ã€‚æˆ‘ä»¬çš„å®éªŒæ­ç¤ºäº†ä¸¤ä¸ªå…³é”®å‘ç°ï¼š1ï¼‰åœ¨å¹³è¡¡çš„åˆæˆæ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒå¯ä»¥åœ¨è§†è§‰åœºæ™¯ä¸Šäº§ç”Ÿç»Ÿä¸€çš„æ€§èƒ½ï¼Œå¹¶å‡è½»å¸¸è§çš„åè§ï¼›2ï¼‰ä¸åœ¨åŒ¹é…ç¯å¢ƒä¸­è¿›è¡Œå¾®è°ƒçš„æ¨¡å‹ç›¸æ¯”ï¼Œåœ¨åˆæˆåˆºæ¿€ä¸Šè¿›è¡Œå¾®è°ƒæ˜¾è‘—æé«˜äº†åœ¨çœŸå®ä¸–ç•Œæ•°æ®ï¼ˆCOCOï¼‰ä¸Šçš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.11440v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„å¾®è°ƒç­–ç•¥ã€‚ä¼ ç»Ÿçš„å¾®è°ƒæ–¹æ³•å®¹æ˜“å—åˆ°çœŸå®åœºæ™¯æ•°æ®æ”¶é›†å’Œæ ‡æ³¨ä¸­çš„åè§ã€é”™è¯¯å’Œåˆ†å¸ƒä¸å¹³è¡¡çš„å½±å“ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„å¾®è°ƒæ–¹æ³•ï¼šé¦–å…ˆï¼Œé€šè¿‡æ§åˆ¶æ•°æ®å’Œæ ‡æ³¨çš„ç”Ÿæˆï¼Œç¡®ä¿æ•°æ®æ— åè§ã€åˆ†å¸ƒå¹³è¡¡ä¸”æ ‡æ³¨æ— è¯¯ï¼›ç„¶åï¼Œä½¿ç”¨è‡ªåŠ¨æ„å»ºçš„æ•°æ®é›†å¯¹å…ˆè¿›çš„VLMsè¿›è¡Œå¾®è°ƒï¼Œå¹¶è¯„ä¼°å…¶åœ¨ç»å¯¹ä½ç½®ä»»åŠ¡ä¸Šå¯¹çœŸå®ä¸–ç•Œæ•°æ®çš„æ€§èƒ½è¿ç§»èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨å¹³è¡¡åˆæˆæ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒå¯ä»¥å¸¦æ¥å‡åŒ€çš„è§†è§‰åœºæ™¯æ€§èƒ½ï¼Œå¹¶ç¼“è§£å¸¸è§çš„åè§ï¼›æ­¤å¤–ï¼Œåœ¨åˆæˆåˆºæ¿€ä¸Šè¿›è¡Œå¾®è°ƒå¯æ˜¾è‘—æé«˜åœ¨çœŸå®ä¸–ç•Œæ•°æ®ï¼ˆCOCOï¼‰ä¸Šçš„æ€§èƒ½ï¼Œä¼˜äºåŒ¹é…è®¾ç½®ä¸‹çš„æ¨¡å‹å¾®è°ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¼ ç»Ÿå¾®è°ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„æ–¹æ³•å®¹æ˜“å—åˆ°åè§ã€é”™è¯¯å’Œåˆ†å¸ƒä¸å¹³è¡¡çš„å½±å“ã€‚</li>
<li>æ–°çš„å¾®è°ƒæ–¹æ³•é€šè¿‡æ§åˆ¶æ•°æ®å’Œæ ‡æ³¨çš„ç”Ÿæˆæ¥è§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>è‡ªåŠ¨æ„å»ºçš„æ•°æ®é›†ç”¨äºå¯¹å…ˆè¿›çš„VLMsè¿›è¡Œå¾®è°ƒï¼Œç¡®ä¿æ•°æ®æ— åè§ã€åˆ†å¸ƒå¹³è¡¡ä¸”æ ‡æ³¨æ— è¯¯ã€‚</li>
<li>åœ¨å¹³è¡¡åˆæˆæ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒå¯ä»¥å¸¦æ¥å‡åŒ€çš„è§†è§‰åœºæ™¯æ€§èƒ½ï¼Œå¹¶ç¼“è§£å¸¸è§çš„åè§ã€‚</li>
<li>åœ¨åˆæˆåˆºæ¿€ä¸Šè¿›è¡Œå¾®è°ƒå¯æ˜¾è‘—æé«˜åœ¨çœŸå®ä¸–ç•Œæ•°æ®ï¼ˆå¦‚COCOï¼‰ä¸Šçš„æ€§èƒ½ã€‚</li>
<li>ç›¸æ¯”åŒ¹é…è®¾ç½®ä¸‹çš„æ¨¡å‹å¾®è°ƒï¼Œä½¿ç”¨åˆæˆæ•°æ®è¿›è¡Œå¾®è°ƒè¡¨ç°æ›´ä½³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.11440">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-81d0250ef8147fdde696e39c71128f36" align="middle">
<img src="https://picx.zhimg.com/v2-4db2501ba0b27134d76f8c21d04f10a4" align="middle">
<img src="https://picx.zhimg.com/v2-b1742eb95be7f3de3e0a05965e7caa38" align="middle">
<img src="https://picx.zhimg.com/v2-62f60376058bdacfae3ac5b23d61fcdd" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="VP-Bench-A-Comprehensive-Benchmark-for-Visual-Prompting-in-Multimodal-Large-Language-Models"><a href="#VP-Bench-A-Comprehensive-Benchmark-for-Visual-Prompting-in-Multimodal-Large-Language-Models" class="headerlink" title="VP-Bench: A Comprehensive Benchmark for Visual Prompting in Multimodal Large Language Models"></a>VP-Bench: A Comprehensive Benchmark for Visual Prompting in Multimodal Large Language Models</h2><p><strong>Authors:Mingjie Xu, Jinpeng Chen, Yuzhi Zhao, Jason Chun Lok Li, Yue Qiu, Zekang Du, Mengyang Wu, Pingping Zhang, Kun Li, Hongzheng Yang, Wenao Ma, Jiaheng Wei, Qinbin Li, Kangcheng Liu, Wenqiang Lei</strong></p>
<p>Multimodal large language models (MLLMs) have enabled a wide range of advanced vision-language applications, including fine-grained object recognition and contextual understanding. When querying specific regions or objects in an image, human users naturally use â€œvisual promptsâ€ (VPs), such as bounding boxes, to provide reference. However, no existing benchmark systematically evaluates the ability of MLLMs to interpret such VPs. This gap leaves it unclear whether current MLLMs can effectively recognize VPs, an intuitive prompting method for humans, and use them to solve problems. To address this limitation, we introduce VP-Bench, a benchmark for assessing MLLMsâ€™ capability in VP perception and utilization. VP-Bench employs a two-stage evaluation framework: Stage 1 examines modelsâ€™ ability to perceive VPs in natural scenes, using 30k visualized prompts spanning eight shapes and 355 attribute combinations. Stage 2 investigates the impact of VPs on downstream tasks, measuring their effectiveness in real-world problem-solving scenarios. Using VP-Bench, we evaluate 28 MLLMs, including proprietary systems (e.g., GPT-4o) and open-source models (e.g., InternVL3 and Qwen2.5-VL), and provide a comprehensive analysis of factors that affect VP understanding, such as variations in VP attributes, question arrangement, and model scale. VP-Bench establishes a new reference framework for studying how MLLMs comprehend and resolve grounded referring questions.</p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å·²ç»å¯ç”¨äº†å¹¿æ³›çš„å…ˆè¿›è§†è§‰è¯­è¨€åº”ç”¨ç¨‹åºï¼ŒåŒ…æ‹¬ç²¾ç»†ç›®æ ‡è¯†åˆ«å’Œä¸Šä¸‹æ–‡ç†è§£ã€‚å½“åœ¨å›¾åƒä¸­æŸ¥è¯¢ç‰¹å®šåŒºåŸŸæˆ–å¯¹è±¡æ—¶ï¼Œäººç±»ç”¨æˆ·è‡ªç„¶åœ°ä½¿ç”¨â€œè§†è§‰æç¤ºâ€ï¼ˆVPsï¼‰ï¼Œå¦‚è¾¹ç•Œæ¡†ï¼Œä»¥æä¾›å‚è€ƒã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºå‡†æµ‹è¯•å¹¶æœªç³»ç»Ÿåœ°è¯„ä¼°MLLMsè§£é‡Šè¿™ç§VPsçš„èƒ½åŠ›ã€‚è¿™ä¸€å·®è·ä½¿å¾—å½“å‰MLLMsæ˜¯å¦èƒ½å¤Ÿæœ‰æ•ˆåœ°è¯†åˆ«VPsï¼ˆä¸€ç§äººç±»ç›´è§‚çš„æç¤ºæ–¹æ³•ï¼‰å¹¶ç”¨äºè§£å†³é—®é¢˜å°šä¸æ¸…æ¥šã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†VP-Benchï¼Œä¸€ä¸ªç”¨äºè¯„ä¼°MLLMsåœ¨VPæ„ŸçŸ¥å’Œåˆ©ç”¨èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚VP-Benché‡‡ç”¨ä¸¤é˜¶æ®µè¯„ä¼°æ¡†æ¶ï¼šç¬¬ä¸€é˜¶æ®µè€ƒå¯Ÿæ¨¡å‹åœ¨è‡ªç„¶åœºæ™¯ä¸­æ„ŸçŸ¥VPsçš„èƒ½åŠ›ï¼Œä½¿ç”¨æ¶µç›–å…«ç§å½¢çŠ¶å’Œ355ç§å±æ€§ç»„åˆçš„3ä¸‡å¯è§†åŒ–æç¤ºã€‚ç¬¬äºŒé˜¶æ®µç ”ç©¶VPså¯¹ä¸‹æ¸¸ä»»åŠ¡çš„å½±å“ï¼Œæµ‹é‡å®ƒä»¬åœ¨ç°å®é—®é¢˜è§£å†³åœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§ã€‚ä½¿ç”¨VP-Benchï¼Œæˆ‘ä»¬è¯„ä¼°äº†28ä¸ªMLLMsï¼ŒåŒ…æ‹¬ä¸“æœ‰ç³»ç»Ÿï¼ˆå¦‚GPT-4oï¼‰å’Œå¼€æºæ¨¡å‹ï¼ˆå¦‚InternVL3å’ŒQwen2.5-VLï¼‰ï¼Œå¹¶å…¨é¢åˆ†æäº†å½±å“VPç†è§£çš„å› ç´ ï¼Œå¦‚VPå±æ€§çš„å˜åŒ–ã€é—®é¢˜çš„å®‰æ’å’Œæ¨¡å‹è§„æ¨¡ã€‚VP-Benchä¸ºç ”ç©¶MLLMså¦‚ä½•ç†è§£å’Œè§£å†³åŸºäºåœ°é¢çš„å¼•ç”¨é—®é¢˜å»ºç«‹äº†æ–°çš„å‚è€ƒæ¡†æ¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.11438v1">PDF</a> This is the extended version of the paper accepted at AAAI 2026, which includes all technical appendices and additional experimental details</p>
<p><strong>Summary</strong></p>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨ç²¾ç»†ç²’åº¦å¯¹è±¡è¯†åˆ«å’Œä¸Šä¸‹æ–‡ç†è§£ç­‰å…ˆè¿›çš„è§†è§‰è¯­è¨€åº”ç”¨æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰åŸºå‡†æµ‹è¯•å¹¶æœªç³»ç»Ÿåœ°è¯„ä¼°MLLMså¯¹è§†è§‰æç¤ºï¼ˆVPsï¼‰çš„è§£è¯»èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†VP-BenchåŸºå‡†æµ‹è¯•ï¼Œä»¥è¯„ä¼°MLLMså¯¹VPsçš„æ„ŸçŸ¥å’Œåˆ©ç”¨èƒ½åŠ›ã€‚VP-Benché‡‡ç”¨ä¸¤é˜¶æ®µè¯„ä¼°æ¡†æ¶ï¼Œç¬¬ä¸€é˜¶æ®µè€ƒå¯Ÿæ¨¡å‹åœ¨è‡ªç„¶åœºæ™¯ä¸­æ„ŸçŸ¥VPsçš„èƒ½åŠ›ï¼Œç¬¬äºŒé˜¶æ®µç ”ç©¶VPså¯¹ä¸‹æ¸¸ä»»åŠ¡çš„å½±å“ï¼Œè¡¡é‡å…¶åœ¨è§£å†³å®é™…é—®é¢˜åœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§ã€‚é€šè¿‡VP-Benchï¼Œæˆ‘ä»¬å¯¹åŒ…æ‹¬ä¸“æœ‰ç³»ç»Ÿï¼ˆå¦‚GPT-4oï¼‰å’Œå¼€æºæ¨¡å‹ï¼ˆå¦‚InternVL3å’ŒQwen2.5-VLï¼‰åœ¨å†…çš„28ä¸ªMLLMsè¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶åˆ†æäº†å½±å“VPç†è§£çš„å› ç´ ï¼Œå¦‚VPå±æ€§ã€é—®é¢˜å®‰æ’å’Œæ¨¡å‹è§„æ¨¡ç­‰ã€‚VP-Benchä¸ºç ”ç©¶MLLMså¦‚ä½•ç†è§£å’Œè§£å†³åŸºäºåœ°é¢çš„å¼•ç”¨é—®é¢˜æä¾›äº†æ–°çš„å‚è€ƒæ¡†æ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†è§‰è¯­è¨€åº”ç”¨æ–¹é¢å…·æœ‰å¹¿æ³›åº”ç”¨ã€‚</li>
<li>ç›®å‰ç¼ºä¹ç³»ç»Ÿè¯„ä¼°MLLMså¯¹è§†è§‰æç¤ºï¼ˆVPsï¼‰è§£è¯»èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚</li>
<li>VP-BenchåŸºå‡†æµ‹è¯•ç”¨äºè¯„ä¼°MLLMsæ„ŸçŸ¥å’Œåˆ©ç”¨VPsçš„èƒ½åŠ›ã€‚</li>
<li>VP-Benché‡‡ç”¨ä¸¤é˜¶æ®µè¯„ä¼°æ¡†æ¶ï¼šæ„ŸçŸ¥VPsçš„èƒ½åŠ›å’Œè§£å†³å®é™…é—®é¢˜åœºæ™¯ä¸­VPsçš„æœ‰æ•ˆæ€§ã€‚</li>
<li>è¯„ä¼°äº†åŒ…æ‹¬ä¸“æœ‰ç³»ç»Ÿå’Œå¼€æºæ¨¡å‹åœ¨å†…çš„å¤šä¸ªMLLMsã€‚</li>
<li>åˆ†æäº†å½±å“VPç†è§£çš„å› ç´ ï¼ŒåŒ…æ‹¬VPå±æ€§ã€é—®é¢˜å®‰æ’å’Œæ¨¡å‹è§„æ¨¡ç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.11438">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-39551380403d5bb5b7f84501978165e6" align="middle">
<img src="https://picx.zhimg.com/v2-cadd84da7d0cf5c0acccd1bb1a72c5d6" align="middle">
<img src="https://picx.zhimg.com/v2-f553b9a312f7893473b55805e130c262" align="middle">
<img src="https://picx.zhimg.com/v2-5fd33f5b0d4321563dc90f39a07e649e" align="middle">
<img src="https://picx.zhimg.com/v2-c0b4c25e9671a9b5b915bd966d363c52" align="middle">
<img src="https://picx.zhimg.com/v2-67076cbe9423fc7de80b51dd82a83d08" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Unsupervised-Motion-Compensated-Decomposition-for-Cardiac-MRI-Reconstruction-via-Neural-Representation"><a href="#Unsupervised-Motion-Compensated-Decomposition-for-Cardiac-MRI-Reconstruction-via-Neural-Representation" class="headerlink" title="Unsupervised Motion-Compensated Decomposition for Cardiac MRI Reconstruction via Neural Representation"></a>Unsupervised Motion-Compensated Decomposition for Cardiac MRI Reconstruction via Neural Representation</h2><p><strong>Authors:Xuanyu Tian, Lixuan Chen, Qing Wu, Xiao Wang, Jie Feng, Yuyao Zhang, Hongjiang Wei</strong></p>
<p>Cardiac magnetic resonance (CMR) imaging is widely used to characterize cardiac morphology and function. To accelerate CMR imaging, various methods have been proposed to recover high-quality spatiotemporal CMR images from highly undersampled k-t space data. However, current CMR reconstruction techniques either fail to achieve satisfactory image quality or are restricted by the scarcity of ground truth data, leading to limited applicability in clinical scenarios. In this work, we proposed MoCo-INR, a new unsupervised method that integrates implicit neural representations (INR) with the conventional motion-compensated (MoCo) framework. Using explicit motion modeling and the continuous prior of INRs, MoCo-INR can produce accurate cardiac motion decomposition and high-quality CMR reconstruction. Furthermore, we introduce a new INR network architecture tailored to the CMR problem, which significantly stabilizes model optimization. Experiments on retrospective (simulated) datasets demonstrate the superiority of MoCo-INR over state-of-the-art methods, achieving fast convergence and fine-detailed reconstructions at ultra-high acceleration factors (e.g., 20x in VISTA sampling). Additionally, evaluations on prospective (real-acquired) free-breathing CMR scans highlight the clinical practicality of MoCo-INR for real-time imaging. Several ablation studies further confirm the effectiveness of the critical components of MoCo-INR.</p>
<blockquote>
<p>å¿ƒè„ç£å…±æŒ¯ï¼ˆCMRï¼‰æˆåƒå¹¿æ³›åº”ç”¨äºè¡¨å¾å¿ƒè„å½¢æ€å’ŒåŠŸèƒ½ã€‚ä¸ºäº†åŠ é€ŸCMRæˆåƒï¼Œå·²ç»æå‡ºäº†å„ç§æ–¹æ³•ä»é«˜åº¦æ¬ é‡‡æ ·çš„k-tç©ºé—´æ•°æ®ä¸­æ¢å¤é«˜è´¨é‡çš„æ—¶ç©ºCMRå›¾åƒã€‚ç„¶è€Œï¼Œå½“å‰çš„CMRé‡å»ºæŠ€æœ¯åœ¨å›¾åƒè´¨é‡æ–¹é¢æ— æ³•è¾¾åˆ°æ»¡æ„çš„æ•ˆæœï¼Œæˆ–è€…å—åˆ°çœŸå®æ•°æ®ç¨€ç¼ºçš„é™åˆ¶ï¼Œå¯¼è‡´åœ¨ä¸´åºŠåœºæ™¯ä¸­çš„åº”ç”¨æœ‰é™ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†MoCo-INRï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„æ— ç›‘ç£æ–¹æ³•ï¼Œå®ƒå°†éšå¼ç¥ç»è¡¨ç¤ºï¼ˆINRï¼‰ä¸ä¼ ç»Ÿè¿åŠ¨è¡¥å¿ï¼ˆMoCoï¼‰æ¡†æ¶ç›¸ç»“åˆã€‚é€šè¿‡æ˜¾å¼è¿åŠ¨å»ºæ¨¡å’ŒINRçš„è¿ç»­å…ˆéªŒï¼ŒMoCo-INRå¯ä»¥äº§ç”Ÿå‡†ç¡®çš„å¿ƒè„è¿åŠ¨åˆ†è§£å’Œé«˜è´¨é‡çš„CMRé‡å»ºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é’ˆå¯¹CMRé—®é¢˜å¼•å…¥äº†ä¸€ç§æ–°çš„INRç½‘ç»œæ¶æ„ï¼Œè¿™å¯ä»¥æ˜¾è‘—ç¨³å®šæ¨¡å‹ä¼˜åŒ–ã€‚åœ¨å›é¡¾æ€§ï¼ˆæ¨¡æ‹Ÿï¼‰æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMoCo-INRä¼˜äºæœ€æ–°æŠ€æœ¯ï¼Œå®ç°äº†å¿«é€Ÿæ”¶æ•›å’Œåœ¨è¶…é«˜åŠ é€Ÿå› å­ï¼ˆä¾‹å¦‚VISTAé‡‡æ ·ä¸­çš„20xï¼‰ä¸‹çš„ç²¾ç»†è¯¦ç»†é‡å»ºã€‚æ­¤å¤–ï¼Œå¯¹å‰ç»æ€§ï¼ˆå®æ—¶é‡‡é›†ï¼‰è‡ªç”±å‘¼å¸CMRæ‰«æçš„è¯„ä¼°çªå‡ºäº†MoCo-INRåœ¨å®æ—¶æˆåƒä¸­çš„ä¸´åºŠå®ç”¨æ€§ã€‚å‡ é¡¹æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥è¯å®äº†MoCo-INRå…³é”®ç»„ä»¶çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.11436v1">PDF</a> Accepted by AAAI-26</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¿ƒè„ç£å…±æŒ¯æˆåƒï¼ˆCMRï¼‰ä¸­ä¸€ç§æ–°çš„æ— ç›‘ç£æ–¹æ³•MoCo-INRï¼Œè¯¥æ–¹æ³•ç»“åˆäº†ä¼ ç»Ÿçš„è¿åŠ¨è¡¥å¿æ¡†æ¶å’Œéšå¼ç¥ç»è¡¨ç¤ºæ³•ï¼Œæ—¨åœ¨æé«˜CMRå›¾åƒçš„è´¨é‡ä¸åŠ é€Ÿæˆåƒã€‚é€šè¿‡æ˜¾å¼è¿åŠ¨å»ºæ¨¡å’Œè¿ç»­éšå¼è¡¨ç¤ºæ³•ï¼ŒMoCo-INRå¯äº§ç”Ÿç²¾ç¡®çš„å¿ƒè„è¿åŠ¨åˆ†è§£å’Œé«˜è´¨é‡çš„é‡å»ºå›¾åƒã€‚åœ¨å›é¡¾æ€§æ¨¡æ‹Ÿæ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜MoCo-INRä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œå¹¶ä¸”é€‚ç”¨äºçœŸå®è‡ªç”±å‘¼å¸ä¸‹æ‰«æçš„å‰æ™¯åˆ†æè¡¨æ˜å…¶å¯ç”¨äºå®æ—¶æˆåƒã€‚å‡ ä¸ªåˆ‡é™¤ç ”ç©¶è¿›ä¸€æ­¥éªŒè¯äº†MoCo-INRçš„å…³é”®ç»„æˆéƒ¨åˆ†çš„æœ‰æ•ˆæ€§ã€‚ç®€è€Œè¨€ä¹‹ï¼Œè¯¥æ–¹æ³•çš„ä¸´åºŠåº”ç”¨æ½œåŠ›å¾ˆå¤§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li><p>CMRæˆåƒå¹¿æ³›åº”ç”¨äºå¿ƒè„å½¢æ€å’ŒåŠŸèƒ½çš„è¡¨å¾ã€‚ä¸ºäº†åŠ é€Ÿæˆåƒè¿‡ç¨‹ï¼Œç ”ç©¶äººå‘˜æå‡ºäº†ä»é«˜åº¦æ¬ é‡‡æ ·çš„k-tç©ºé—´æ•°æ®ä¸­æ¢å¤é«˜è´¨é‡æ—¶ç©ºCMRå›¾åƒçš„æ–¹æ³•ã€‚ç„¶è€Œï¼Œå½“å‰çš„é‡æ„æŠ€æœ¯å­˜åœ¨ä¸è¶³ï¼Œè¦ä¹ˆå›¾åƒè´¨é‡ä¸ä½³ï¼Œè¦ä¹ˆå—é™äºçœŸå®æ•°æ®çš„ç¨€ç¼ºæ€§ï¼Œè¿™åœ¨ä¸´åºŠåº”ç”¨ä¸­å—åˆ°é™åˆ¶ã€‚</p>
</li>
<li><p>MoCo-INRæ˜¯ä¸€ç§æ–°æå‡ºçš„æ— ç›‘ç£æ–¹æ³•ï¼Œç»“åˆäº†éšå¼ç¥ç»è¡¨ç¤ºï¼ˆINRï¼‰ä¸ä¼ ç»Ÿè¿åŠ¨è¡¥å¿ï¼ˆMoCoï¼‰æ¡†æ¶ã€‚å®ƒé€šè¿‡æ˜ç¡®çš„è¿åŠ¨å»ºæ¨¡å’Œè¿ç»­çš„éšå¼è¡¨ç¤ºæ³•æ¥å‡†ç¡®åˆ†è§£å¿ƒè„è¿åŠ¨å¹¶äº§ç”Ÿé«˜è´¨é‡å›¾åƒã€‚</p>
</li>
<li><p>MoCo-INRå¼•å…¥äº†ä¸€ç§é’ˆå¯¹CMRé—®é¢˜çš„å®šåˆ¶INRç½‘ç»œæ¶æ„ï¼Œæ˜¾è‘—ç¨³å®šæ¨¡å‹ä¼˜åŒ–è¿‡ç¨‹ã€‚åœ¨å›é¡¾æ€§æ¨¡æ‹Ÿæ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜äº†MoCo-INRç›¸è¾ƒäºå…¶ä»–æ–¹æ³•çš„ä¼˜è¶Šæ€§ã€‚å®ƒèƒ½åœ¨æé«˜åŠ é€Ÿå› å­ï¼ˆå¦‚VISTAé‡‡æ ·ä¸­çš„20å€ï¼‰ä¸‹å®ç°å¿«é€Ÿæ”¶æ•›å’Œç²¾ç»†é‡å»ºã€‚</p>
</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.11436">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-eb5c10c65f0d0cfa02115de0e379d703" align="middle">
<img src="https://picx.zhimg.com/v2-c8afb714de35b4335344ce199e3bce18" align="middle">
<img src="https://picx.zhimg.com/v2-eba3010d89ad37c5b7c3dc98a27dcf02" align="middle">
<img src="https://picx.zhimg.com/v2-1f6b7a39d8a1d6d7bfdb75c4dbb60d05" align="middle">
<img src="https://picx.zhimg.com/v2-8ed2e4874384e76b8c52d989570c6874" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Q-Doc-Benchmarking-Document-Image-Quality-Assessment-Capabilities-in-Multi-modal-Large-Language-Models"><a href="#Q-Doc-Benchmarking-Document-Image-Quality-Assessment-Capabilities-in-Multi-modal-Large-Language-Models" class="headerlink" title="Q-Doc: Benchmarking Document Image Quality Assessment Capabilities in Multi-modal Large Language Models"></a>Q-Doc: Benchmarking Document Image Quality Assessment Capabilities in Multi-modal Large Language Models</h2><p><strong>Authors:Jiaxi Huang, Dongxu Wu, Hanwei Zhu, Lingyu Zhu, Jun Xing, Xu Wang, Baoliang Chen</strong></p>
<p>The rapid advancement of Multi-modal Large Language Models (MLLMs) has expanded their capabilities beyond high-level vision tasks. Nevertheless, their potential for Document Image Quality Assessment (DIQA) remains underexplored. To bridge this gap, we propose Q-Doc, a three-tiered evaluation framework for systematically probing DIQA capabilities of MLLMs at coarse, middle, and fine granularity levels. a) At the coarse level, we instruct MLLMs to assign quality scores to document images and analyze their correlation with Quality Annotations. b) At the middle level, we design distortion-type identification tasks, including single-choice and multi-choice tests for multi-distortion scenarios. c) At the fine level, we introduce distortion-severity assessment where MLLMs classify distortion intensity against human-annotated references. Our evaluation demonstrates that while MLLMs possess nascent DIQA abilities, they exhibit critical limitations: inconsistent scoring, distortion misidentification, and severity misjudgment. Significantly, we show that Chain-of-Thought (CoT) prompting substantially enhances performance across all levels. Our work provides a benchmark for DIQA capabilities in MLLMs, revealing pronounced deficiencies in their quality perception and promising pathways for enhancement. The benchmark and code are publicly available at:   <a target="_blank" rel="noopener" href="https://github.com/cydxf/Q-Doc">https://github.com/cydxf/Q-Doc</a>.</p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„å¿«é€Ÿå‘å±•ä½¿å…¶èƒ½åŠ›è¶…è¶Šäº†é«˜çº§è§†è§‰ä»»åŠ¡ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨æ–‡æ¡£å›¾åƒè´¨é‡è¯„ä¼°ï¼ˆDIQAï¼‰æ–¹é¢çš„æ½œåŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†Q-Docï¼Œè¿™æ˜¯ä¸€ä¸ªä¸‰çº§è¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨ç³»ç»Ÿåœ°æ¢ç´¢MLLMsåœ¨ç²—ã€ä¸­ã€ç»†ç²’åº¦çº§åˆ«ä¸Šçš„DIQAèƒ½åŠ›ã€‚aï¼‰åœ¨ç²—ç•¥çº§åˆ«ä¸Šï¼Œæˆ‘ä»¬æŒ‡å¯¼MLLMsä¸ºæ–‡æ¡£å›¾åƒåˆ†é…è´¨é‡åˆ†æ•°ï¼Œå¹¶åˆ†æå…¶ä¸è´¨é‡æ³¨é‡Šçš„ç›¸å…³æ€§ã€‚bï¼‰åœ¨ä¸­çº§æ°´å¹³ä¸Šï¼Œæˆ‘ä»¬è®¾è®¡äº†å¯¹å¤±çœŸç±»å‹è¿›è¡Œè¯†åˆ«çš„ä»»åŠ¡ï¼ŒåŒ…æ‹¬å•é€‰é¢˜å’Œå¤šé€‰é¢˜ï¼Œä»¥åº”å¯¹å¤šç§å¤±çœŸåœºæ™¯ã€‚cï¼‰åœ¨ç²¾ç»†çº§åˆ«ä¸Šï¼Œæˆ‘ä»¬å¼•å…¥äº†å¤±çœŸä¸¥é‡ç¨‹åº¦è¯„ä¼°ï¼Œå…¶ä¸­MLLMsæ ¹æ®äººç±»æ³¨é‡Šçš„å‚è€ƒåˆ†ç±»å¤±çœŸå¼ºåº¦ã€‚æˆ‘ä»¬çš„è¯„ä¼°è¡¨æ˜ï¼Œè™½ç„¶MLLMså…·æœ‰åˆæ­¥çš„DIQAèƒ½åŠ›ï¼Œä½†å®ƒä»¬è¡¨ç°å‡ºå…³é”®å±€é™æ€§ï¼šè¯„åˆ†ä¸ä¸€è‡´ã€å¤±çœŸè¯†åˆ«é”™è¯¯å’Œä¸¥é‡ç¨‹åº¦åˆ¤æ–­é”™è¯¯ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬è¡¨æ˜â€œæ€ç»´é“¾â€ï¼ˆCoTï¼‰æç¤ºæ˜¾è‘—æé«˜äº†æ‰€æœ‰çº§åˆ«çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„å·¥ä½œä¸ºMLLMsçš„DIQAèƒ½åŠ›æä¾›äº†åŸºå‡†æµ‹è¯•ï¼Œæ­ç¤ºäº†å®ƒä»¬åœ¨è´¨é‡æ„ŸçŸ¥æ–¹é¢çš„æ˜æ˜¾ç¼ºé™·å’Œæ½œåœ¨çš„æ”¹è¿›é€”å¾„ã€‚åŸºå‡†æµ‹è¯•å’Œä»£ç å¯åœ¨ä»¥ä¸‹ç½‘å€å…¬å¼€è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/cydxf/Q-Doc%E3%80%82">https://github.com/cydxf/Q-Docã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.11410v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨æ–‡æ¡£å›¾åƒè´¨é‡è¯„ä¼°ï¼ˆDIQAï¼‰æ–¹é¢çš„æ½œåŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æœ¬æ–‡æå‡ºQ-Docï¼Œä¸€ä¸ªä¸‰å±‚æ¬¡çš„è¯„ä¼°æ¡†æ¶ï¼Œä»¥ç³»ç»Ÿåœ°æ¢ç©¶MLLMsåœ¨ç²—ã€ä¸­ã€ç»†ç²’åº¦çº§åˆ«çš„DIQAèƒ½åŠ›ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼ŒMLLMsè™½å…·å¤‡åˆæ­¥çš„DIQAèƒ½åŠ›ï¼Œä½†åœ¨è¯„åˆ†ä¸€è‡´æ€§ã€å¤±çœŸè¯†åˆ«åŠä¸¥é‡ç¨‹åº¦è¯„ä¼°ç­‰æ–¹é¢å­˜åœ¨æ˜¾è‘—å±€é™ã€‚é€šè¿‡æ€ç»´é“¾ï¼ˆCoTï¼‰æç¤ºå¯æ˜¾è‘—æé«˜å„çº§æ€§èƒ½ã€‚æœ¬æ–‡æä¾›çš„åŸºå‡†æµ‹è¯•æœ‰åŠ©äºæ­ç¤ºMLLMsåœ¨è´¨é‡æ„ŸçŸ¥æ–¹é¢çš„ä¸è¶³ï¼Œå¹¶ä¸ºæ”¹è¿›æŒ‡æ˜äº†æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨æ–‡æ¡£å›¾åƒè´¨é‡è¯„ä¼°ï¼ˆDIQAï¼‰æ–¹é¢çš„åº”ç”¨æ½œåŠ›å°šæœªå……åˆ†ç ”ç©¶ã€‚</li>
<li>Q-Docæ˜¯ä¸€ä¸ªä¸‰å±‚æ¬¡çš„è¯„ä¼°æ¡†æ¶ï¼Œç”¨äºç³»ç»Ÿè¯„ä¼°MLLMsåœ¨ç²—ã€ä¸­ã€ç»†ç²’åº¦çº§åˆ«çš„DIQAèƒ½åŠ›ã€‚</li>
<li>åœ¨ç²—ç²’åº¦çº§åˆ«ï¼ŒMLLMsè¢«è®­ç»ƒä¸ºç»™æ–‡æ¡£å›¾åƒåˆ†é…è´¨é‡åˆ†æ•°ï¼Œä½†å­˜åœ¨è¯„åˆ†ä¸ä¸€è‡´çš„é—®é¢˜ã€‚</li>
<li>åœ¨ä¸­ç²’åº¦çº§åˆ«ï¼ŒMLLMsé¢ä¸´å¤±çœŸç±»å‹è¯†åˆ«æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬å•ä¸€å’Œå¤šç§å¤±çœŸçš„åœºæ™¯è¯†åˆ«ã€‚</li>
<li>åœ¨ç»†ç²’åº¦çº§åˆ«ï¼ŒMLLMsåœ¨è¯„ä¼°å¤±çœŸä¸¥é‡ç¨‹åº¦æ—¶å­˜åœ¨è¯¯åˆ¤ã€‚</li>
<li>æ€ç»´é“¾ï¼ˆCoTï¼‰æç¤ºèƒ½æ˜¾è‘—æé«˜MLLMsåœ¨å„çº§DIQAä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.11410">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-28143b5f6120c088ca3b06c9e2508387" align="middle">
<img src="https://picx.zhimg.com/v2-fbeba7d31964dcd2d2f8c3c7edc37832" align="middle">
<img src="https://picx.zhimg.com/v2-61964175a82668cc0d9544cbe210f30d" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="MicroVQA-High-Quality-Microscopy-Reasoning-Dataset-with-Weakly-Supervised-Graphs-for-Multimodal-Large-Language-Model"><a href="#MicroVQA-High-Quality-Microscopy-Reasoning-Dataset-with-Weakly-Supervised-Graphs-for-Multimodal-Large-Language-Model" class="headerlink" title="MicroVQA++: High-Quality Microscopy Reasoning Dataset with Weakly Supervised Graphs for Multimodal Large Language Model"></a>MicroVQA++: High-Quality Microscopy Reasoning Dataset with Weakly Supervised Graphs for Multimodal Large Language Model</h2><p><strong>Authors:Manyu Li, Ruian He, Chenxi Ma, Weimin Tan, Bo Yan</strong></p>
<p>Multimodal Large Language Models are increasingly applied to biomedical imaging, yet scientific reasoning for microscopy remains limited by the scarcity of large-scale, high-quality training data. We introduce MicroVQA++, a three-stage, large-scale and high-quality microscopy VQA corpus derived from the BIOMEDICA archive. Stage one bootstraps supervision from expert-validated figure-caption pairs sourced from peer-reviewed articles. Stage two applies HiCQA-Graph, a novel heterogeneous graph over images, captions, and QAs that fuses NLI-based textual entailment, CLIP-based vision-language alignment, and agent signals to identify and filter inconsistent samples. Stage three uses a MultiModal Large Language Model (MLLM) agent to generate multiple-choice questions (MCQ) followed by human screening. The resulting release comprises a large training split and a human-checked test split whose Bloomâ€™s level hard-sample distribution exceeds the MicroVQA benchmark. Our work delivers (i) a quality-controlled dataset that couples expert literature with graph-based filtering and human refinement; (ii) HiCQA-Graph, the first graph that jointly models (image, caption, QA) for cross-modal consistency filtering; (iii) evidence that careful data construction enables 4B-scale MLLMs to reach competitive microscopy reasoning performance (e.g., GPT-5) and achieve state-of-the-art performance among open-source MLLMs. Code and dataset will be released after the review process concludes.</p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç”Ÿç‰©åŒ»å­¦æˆåƒä¸­çš„åº”ç”¨æ—¥ç›Šå¹¿æ³›ï¼Œä½†ç”±äºç¼ºä¹å¤§è§„æ¨¡é«˜è´¨é‡çš„è®­ç»ƒæ•°æ®ï¼Œæ˜¾å¾®é•œç§‘å­¦æ¨ç†ä»ç„¶å—åˆ°é™åˆ¶ã€‚æˆ‘ä»¬æ¨å‡ºäº†MicroVQA++ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸‰é˜¶æ®µã€å¤§è§„æ¨¡ã€é«˜è´¨é‡çš„æ˜¾å¾®é•œé—®ç­”è¯­æ–™åº“ï¼Œæ¥æºäºBIOMEDICAæ¡£æ¡ˆã€‚ç¬¬ä¸€é˜¶æ®µä»ç»è¿‡åŒè¡Œè¯„å®¡çš„æ–‡ç« ä¸­è·å–çš„ä¸“å®¶éªŒè¯çš„å›¾åƒæ ‡é¢˜é…å¯¹ä¸­è·å–ç›‘ç£ä¿¡æ¯ã€‚ç¬¬äºŒé˜¶æ®µåº”ç”¨HiCQA-Graphï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹å¼‚è´¨å›¾ï¼Œæ¶µç›–å›¾åƒã€æ ‡é¢˜å’Œé—®ç­”ï¼Œèåˆäº†åŸºäºè‡ªç„¶è¯­è¨€æ¨ç†çš„æ–‡æœ¬è•´æ¶µã€åŸºäºCLIPçš„è§†è¯­è¨€å¯¹é½å’Œä»£ç†ä¿¡å·ï¼Œç”¨äºè¯†åˆ«å’Œè¿‡æ»¤ä¸ä¸€è‡´çš„æ ·æœ¬ã€‚ç¬¬ä¸‰é˜¶æ®µä½¿ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ä»£ç†ç”Ÿæˆå¤šé¡¹é€‰æ‹©é¢˜ï¼ˆMCQï¼‰ï¼Œéšåè¿›è¡Œäººå·¥ç­›é€‰ã€‚æ­¤æ¬¡å‘å¸ƒåŒ…æ‹¬ä¸€ä¸ªå¤§è§„æ¨¡çš„è®­ç»ƒé›†å’Œä¸€ä¸ªç»è¿‡äººå·¥æ£€æŸ¥è¿‡çš„æµ‹è¯•é›†ï¼Œå…¶Blooméš¾åº¦æ ·æœ¬åˆ†å¸ƒè¶…è¿‡äº†MicroVQAåŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬çš„å·¥ä½œæä¾›äº†ï¼ˆiï¼‰ä¸€ä¸ªè´¨é‡å—æ§çš„æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†ç»“åˆäº†ä¸“å®¶æ–‡çŒ®ã€åŸºäºå›¾çš„è¿‡æ»¤å’Œäººå·¥ç²¾ç‚¼ï¼›ï¼ˆiiï¼‰HiCQA-Graphï¼Œç¬¬ä¸€ä¸ªè”åˆå»ºæ¨¡ï¼ˆå›¾åƒã€æ ‡é¢˜ã€é—®ç­”ï¼‰çš„å›¾å½¢ï¼Œç”¨äºè·¨æ¨¡æ€ä¸€è‡´æ€§è¿‡æ»¤ï¼›ï¼ˆiiiï¼‰è¯æ˜ç²¾å¿ƒæ„å»ºçš„æ•°æ®å¯ä»¥ä½¿4Bè§„æ¨¡çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹è¾¾åˆ°ç«äº‰æ€§çš„æ˜¾å¾®é•œæ¨ç†æ€§èƒ½ï¼ˆä¾‹å¦‚GPT-5ï¼‰ï¼Œå¹¶åœ¨å¼€æºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ä¸­è¾¾åˆ°æœ€æ–°æ°´å¹³ã€‚ä»£ç å’Œæ•°æ®é›†å°†åœ¨å®¡æŸ¥è¿‡ç¨‹ç»“æŸåå‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.11407v1">PDF</a> 11 pages, 4 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†MicroVQA++é¡¹ç›®ï¼Œè¯¥é¡¹ç›®æ—¨åœ¨è§£å†³ç”Ÿç‰©åŒ»å­¦æˆåƒä¸­å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„ç¼ºä¹å¤§è§„æ¨¡é«˜è´¨é‡è®­ç»ƒæ•°æ®çš„é—®é¢˜ã€‚é¡¹ç›®åˆ†ä¸ºä¸‰ä¸ªé˜¶æ®µï¼Œé‡‡ç”¨ä¸“å®¶éªŒè¯çš„å›¾åƒå’Œæè¿°å¯¹ï¼Œä»¥åŠHiCQA-Graphå¼‚è´¨å›¾è¿›è¡Œæ ·æœ¬ç­›é€‰ï¼Œæœ€ç»ˆç”Ÿæˆå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„é—®é¢˜æ•°æ®é›†ã€‚å…¶ç‰¹è‰²åœ¨äºé€šè¿‡ä¸¥æ ¼çš„æ•°æ®é›†æ„å»ºæ–¹å¼ç¡®ä¿äº†æ•°æ®é›†çš„è´¨é‡ï¼ŒåŒæ—¶é€šè¿‡é«˜æ€§èƒ½çš„æ¨¡å‹å±•ç°äº†åœ¨æ˜¾å¾®å›¾åƒå¤„ç†ä¸Šçš„æ¨ç†èƒ½åŠ›ã€‚åœ¨å…¬å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹å½“ä¸­å®ç°äº†æœ€é¡¶å°–çš„æ˜¾å¾®å›¾åƒå¤„ç†æ•ˆæœã€‚ç›¸å…³æ•°æ®å°†åœ¨å®¡ç¨¿ç»“æŸåå…¬å¸ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>MicroVQA++é¡¹ç›®ä¸ºè§£å†³ç”Ÿç‰©åŒ»å­¦æˆåƒä¸­çš„å¤§å‹è¯­è¨€æ¨¡å‹ç¼ºä¹é«˜è´¨é‡è®­ç»ƒæ•°æ®çš„é—®é¢˜è€Œè®¾è®¡ã€‚</li>
<li>é¡¹ç›®åŒ…å«ä¸‰ä¸ªé˜¶æ®µï¼Œä»ä¸“å®¶éªŒè¯çš„å›¾åƒå’Œæè¿°å¯¹å¼€å§‹ï¼Œé€šè¿‡HiCQA-Graphå¼‚è´¨å›¾è¿›è¡Œæ ·æœ¬ç­›é€‰ï¼Œæœ€ç»ˆç”Ÿæˆå¤§å‹è¯­è¨€æ¨¡å‹çš„é—®é¢˜æ•°æ®é›†ã€‚</li>
<li>æ•°æ®é›†æ„å»ºè¿‡ç¨‹ä¸­é‡‡ç”¨ä¸¥æ ¼çš„è´¨æ§æ–¹æ³•ï¼Œç¡®ä¿æ•°æ®è´¨é‡ã€‚</li>
<li>é€šè¿‡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ˜¾å¾®å›¾åƒå¤„ç†ä¸Šå±•ç°äº†å‡ºè‰²çš„æ¨ç†æ€§èƒ½ã€‚</li>
<li>è¯¥é¡¹ç›®åœ¨å…¬å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹ä¸­å®ç°äº†é¡¶å°–æ°´å¹³çš„æ˜¾å¾®å›¾åƒå¤„ç†æ•ˆæœã€‚ </li>
<li>æ•°æ®é›†çš„å‘å¸ƒåŒ…å«äº†å¤§é‡è®­ç»ƒæ ·æœ¬å’Œä¸€ä¸ªç»è¿‡äººå·¥ç­›é€‰çš„æµ‹è¯•æ ·æœ¬é›†ï¼Œè¯¥æµ‹è¯•é›†çš„Bloomçº§åˆ«ç¡¬æ ·æœ¬åˆ†å¸ƒè¶…è¿‡äº†MicroVQAåŸºå‡†æµ‹è¯•ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.11407">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bf3f51d8ea46faef96b08bdd9428fb26" align="middle">
<img src="https://picx.zhimg.com/v2-78b748db358c3a1987af6d8d6f143847" align="middle">
<img src="https://picx.zhimg.com/v2-ba2de71d4a3cc6ef7cda0eb57631a58b" align="middle">
<img src="https://picx.zhimg.com/v2-94106a057e4efaac83999c4c23afb04a" align="middle">
<img src="https://picx.zhimg.com/v2-f1a9f96d7190bcc904fa4541d7ba8cfd" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Universal-Safety-Controllers-with-Learned-Prophecies"><a href="#Universal-Safety-Controllers-with-Learned-Prophecies" class="headerlink" title="Universal Safety Controllers with Learned Prophecies"></a>Universal Safety Controllers with Learned Prophecies</h2><p><strong>Authors:Bernd Finkbeiner, Niklas Metzger, Satya Prakash Nayak, Anne-Kathrin Schmuck</strong></p>
<p>\emph{Universal Safety Controllers (USCs)} are a promising logical control framework that guarantees the satisfaction of a given temporal safety specification when applied to any realizable plant model. Unlike traditional methods, which synthesize one logical controller over a given detailed plant model, USC synthesis constructs a \emph{generic controller} whose outputs are conditioned by plant behavior, called \emph{prophecies}. Thereby, USCs offer strong generalization and scalability benefits over classical logical controllers. However, the exact computation and verification of prophecies remain computationally challenging. In this paper, we introduce an approximation algorithm for USC synthesis that addresses these limitations via learning. Instead of computing exact prophecies, which reason about sets of trees via automata, we only compute under- and over-approximations from (small) example plants and infer computation tree logic (CTL) formulas as representations of prophecies. The resulting USC generalizes to unseen plants via a verification step and offers improved efficiency and explainability through small and concise CTL prophecies, which remain human-readable and interpretable. Experimental results demonstrate that our learned prophecies remain generalizable, yet are significantly more compact and interpretable than their exact tree automata representations.</p>
<blockquote>
<p>å¼ºè°ƒå®‰å…¨æ§åˆ¶å™¨ï¼ˆUSCsï¼‰æ˜¯ä¸€ç§æœ‰å‰æ™¯çš„é€»è¾‘æ§åˆ¶æ¡†æ¶ï¼Œå½“åº”ç”¨äºä»»ä½•å¯å®ç°çš„å·¥å‚æ¨¡å‹æ—¶ï¼Œå®ƒèƒ½ä¿è¯ç»™å®šæ—¶é—´å®‰å…¨è§„èŒƒçš„æ»¡è¶³ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•åœ¨ç»™å®šè¯¦ç»†çš„å·¥å‚æ¨¡å‹ä¸Šåˆæˆä¸€ä¸ªé€»è¾‘æ§åˆ¶å™¨ä¸åŒï¼ŒUSCåˆæˆæ„å»ºäº†ä¸€ä¸ªé€šç”¨æ§åˆ¶å™¨ï¼Œå…¶è¾“å‡ºå—å·¥å‚è¡Œä¸ºï¼ˆç§°ä¸ºé¢„è¨€ï¼‰çš„å½±å“ã€‚å› æ­¤ï¼Œä¸ç»å…¸é€»è¾‘æ§åˆ¶å™¨ç›¸æ¯”ï¼ŒUSCsæä¾›äº†å¼ºå¤§çš„é€šç”¨æ€§å’Œå¯æ‰©å±•æ€§ä¼˜åŠ¿ã€‚ç„¶è€Œï¼Œé¢„è¨€çš„ç²¾ç¡®è®¡ç®—å’ŒéªŒè¯åœ¨è®¡ç®—ä¸Šä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§ç”¨äºUSCåˆæˆçš„è¿‘ä¼¼ç®—æ³•ï¼Œè¯¥ç®—æ³•é€šè¿‡å­¦ä¹ è§£å†³è¿™äº›é™åˆ¶ã€‚æˆ‘ä»¬ä¸å†è®¡ç®—ç²¾ç¡®çš„é¢„è¨€ï¼ˆé€šè¿‡è‡ªåŠ¨æœºæ¨ç†æ ‘é›†ï¼‰ï¼Œè€Œåªä»ï¼ˆå°å‹ï¼‰ç¤ºä¾‹å·¥å‚è®¡ç®—é¢„è¨€çš„ä¸‹ç•Œå’Œä¸Šç•Œï¼Œå¹¶æ¨æ–­è®¡ç®—æ ‘é€»è¾‘ï¼ˆCTLï¼‰å…¬å¼ä½œä¸ºé¢„è¨€çš„è¡¨ç¤ºã€‚æ‰€å¾—USCé€šè¿‡éªŒè¯æ­¥éª¤æ¨å¹¿åˆ°æœªè§è¿‡çš„å·¥å‚ï¼Œå¹¶é€šè¿‡ç®€æ´ä¸”æ˜“äºç†è§£çš„CTLé¢„è¨€æé«˜äº†æ•ˆç‡å’Œå¯è§£é‡Šæ€§ï¼Œè¿™äº›é¢„è¨€ä¿æŒäººç±»å¯è¯»å’Œå¯è§£é‡Šæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬å­¦ä¹ çš„é¢„è¨€å…·æœ‰é€šç”¨æ€§ï¼Œå¹¶ä¸”ä¸å…¶ç²¾ç¡®æ ‘è‡ªåŠ¨æœºè¡¨ç¤ºç›¸æ¯”ï¼Œæ›´åŠ ç´§å‡‘å’Œå¯è§£é‡Šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.11390v1">PDF</a> AAAI 2026</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é€šç”¨å®‰å…¨æ§åˆ¶å™¨ï¼ˆUSCsï¼‰çš„é€»è¾‘æ§åˆ¶æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡ç”Ÿæˆä¸€ç§åŸºäºæ¤ç‰©è¡Œä¸ºé¢„æµ‹çš„é€šç”¨æ§åˆ¶å™¨ï¼Œä¿è¯äº†åœ¨ç»™å®šçš„å®é™…æ¤ç‰©æ¨¡å‹ä¸­æ»¡è¶³ç»™å®šçš„æ—¶é—´å®‰å…¨è§„èŒƒã€‚ä¸ä¼ ç»Ÿçš„é’ˆå¯¹ç‰¹å®šæ¤ç‰©æ¨¡å‹çš„é€»è¾‘æ§åˆ¶å™¨åˆæˆæ–¹æ³•ä¸åŒï¼ŒUSCsæä¾›äº†å¼ºå¤§çš„é€šç”¨æ€§å’Œå¯æ‰©å±•æ€§ä¼˜åŠ¿ã€‚ç„¶è€Œï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹USCåˆæˆçš„è¿‘ä¼¼ç®—æ³•ï¼Œé€šè¿‡å­¦ä¹ å’ŒéªŒè¯æ­¥éª¤æ¥è§£å†³è®¡ç®—é¢„è¨€çš„å¤æ‚æ€§ã€‚è¯¥ç®—æ³•ä»…è®¡ç®—æ¥è‡ªå°æ ·æœ¬æ¤ç‰©çš„ä¸Šä¸‹è¿‘ä¼¼å€¼ï¼Œå¹¶æ¨æ–­å‡ºä½œä¸ºé¢„è¨€è¡¨ç¤ºçš„è®¡ç®—æ ‘é€»è¾‘ï¼ˆCTLï¼‰å…¬å¼ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€å­¦ä¹ çš„é¢„è¨€å…·æœ‰è‰¯å¥½çš„é€šç”¨æ€§ã€ç´§å‡‘æ€§å’Œå¯è§£é‡Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é€šç”¨å®‰å…¨æ§åˆ¶å™¨ï¼ˆUSCsï¼‰æ˜¯ä¸€ç§é€»è¾‘æ§åˆ¶æ¡†æ¶ï¼Œå¯ä»¥åº”ç”¨äºä»»ä½•å¯å®ç°çš„æ¤ç‰©æ¨¡å‹ï¼Œå¹¶ä¿è¯æ»¡è¶³æ—¶é—´å®‰å…¨è§„èŒƒã€‚</li>
<li>USCsé€šè¿‡ç”ŸæˆåŸºäºæ¤ç‰©è¡Œä¸ºé¢„æµ‹çš„é€šç”¨æ§åˆ¶å™¨ï¼Œä¸ä¼ ç»Ÿçš„é’ˆå¯¹ç‰¹å®šæ¤ç‰©æ¨¡å‹çš„é€»è¾‘æ§åˆ¶å™¨åˆæˆæ–¹æ³•ä¸åŒã€‚</li>
<li>USCåˆæˆä¸­çš„é¢„è¨€è®¡ç®—å’ŒéªŒè¯æ˜¯è®¡ç®—ä¸Šå…·æœ‰æŒ‘æˆ˜æ€§çš„ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹USCåˆæˆçš„è¿‘ä¼¼ç®—æ³•ï¼Œè¯¥ç®—æ³•é€šè¿‡å­¦ä¹ å’ŒéªŒè¯æ­¥éª¤è§£å†³é¢„è¨€çš„ç²¾ç¡®è®¡ç®—é—®é¢˜ã€‚</li>
<li>è¿‘ä¼¼ç®—æ³•é€šè¿‡è®¡ç®—å°æ ·æœ¬æ¤ç‰©çš„ä¸Šä¸‹è¿‘ä¼¼å€¼æ¥å·¥ä½œï¼Œå¹¶æ¨æ–­å‡ºä½œä¸ºé¢„è¨€è¡¨ç¤ºçš„è®¡ç®—æ ‘é€»è¾‘ï¼ˆCTLï¼‰å…¬å¼ã€‚</li>
<li>æ‰€å¾—çš„USCé€šè¿‡å¯¹æœªè§è¿‡çš„æ¤ç‰©è¿›è¡ŒéªŒè¯æ­¥éª¤æ¥æ¨å¹¿ï¼Œå¹¶é€šè¿‡ç®€æ´çš„CTLé¢„è¨€æä¾›æ”¹è¿›çš„æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.11390">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cafa8045dde3209c4eb1f3c419b8946f" align="middle">
<img src="https://picx.zhimg.com/v2-41df5218a2a738bd07e38b279cb87bc7" align="middle">
<img src="https://picx.zhimg.com/v2-a5b7cd763e7c5a3effe086ac7640b85d" align="middle">
<img src="https://picx.zhimg.com/v2-1ee5333f72f9e02bc8f56a752abde0ba" align="middle">
<img src="https://picx.zhimg.com/v2-6b469546cc60a3ddf70278308b80566f" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="LaoBench-A-Large-Scale-Multidimensional-Lao-Benchmark-for-Large-Language-Models"><a href="#LaoBench-A-Large-Scale-Multidimensional-Lao-Benchmark-for-Large-Language-Models" class="headerlink" title="LaoBench: A Large-Scale Multidimensional Lao Benchmark for Large Language Models"></a>LaoBench: A Large-Scale Multidimensional Lao Benchmark for Large Language Models</h2><p><strong>Authors:Jian Gao, Richeng Xuan, Zhaolu Kang, Dingshi Liao, Wenxin Huang, Zongmou Huang, Yangdi Xu, Bowen Qin, Zheqi He, Xi Yang, Changjin Li</strong></p>
<p>The rapid advancement of large language models (LLMs) has not been matched by their evaluation in low-resource languages, especially Southeast Asian languages like Lao. To fill this gap, we introduce LaoBench, the first large-scale, high-quality, and multidimensional benchmark dataset dedicated to assessing LLMsâ€™ comprehensive language understanding and reasoning abilities in Lao. LaoBench comprises over 17,000 carefully curated samples spanning three core dimensions: knowledge application, K12 foundational education, and bilingual translation among Lao, Chinese, and English. The dataset is divided into open-source and closed-source subsets, with the closed-source portion enabling black-box evaluation on an official platform to ensure fairness and data security. Our data construction pipeline integrates expert human curation with automated agent-assisted verification, ensuring linguistic accuracy, cultural relevance, and educational value. Benchmarking multiple state-of-the-art LLMs on LaoBench reveals that current models still face significant challenges in mastering Lao across diverse tasks. We hope LaoBench will catalyze further research and development of AI technologies for underrepresented Southeast Asian languages.</p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•å¹¶æœªåœ¨èµ„æºåŒ®ä¹çš„è¯­è¨€ï¼ˆå°¤å…¶æ˜¯ä¸œå—äºšè¯­è¨€å¦‚è€æŒè¯­ï¼‰çš„è¯„ä¼°ä¸­å¾—åˆ°ç›¸åº”çš„åŒ¹é…ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æ¨å‡ºäº†è€æŒè¯­åŸºå‡†æµ‹è¯•é›†ï¼ˆLaoBenchï¼‰ã€‚å®ƒæ˜¯é¦–ä¸ªå¤§è§„æ¨¡ã€é«˜è´¨é‡ã€å¤šç»´åº¦çš„åŸºå‡†æµ‹è¯•é›†ï¼Œä¸“é—¨ç”¨äºè¯„ä¼°LLMåœ¨è€æŒè¯­çš„ç»¼åˆè¯­è¨€ç†è§£å’Œæ¨ç†èƒ½åŠ›ã€‚LaoBenchåŒ…å«è¶…è¿‡17,000ä¸ªç²¾å¿ƒæŒ‘é€‰çš„æ ·æœ¬ï¼Œæ¶µç›–ä¸‰ä¸ªæ ¸å¿ƒç»´åº¦ï¼šçŸ¥è¯†åº”ç”¨ã€K-12åŸºç¡€æ•™è‚²ä»¥åŠè€æŒè¯­ã€ä¸­æ–‡å’Œè‹±æ–‡ä¹‹é—´çš„åŒè¯­ç¿»è¯‘ã€‚è¯¥æ•°æ®é›†åˆ†ä¸ºå¼€æºå’Œé—­æºå­é›†ï¼Œé—­æºéƒ¨åˆ†å¯ä»¥åœ¨å®˜æ–¹å¹³å°ä¸Šè¿›è¡Œé»‘ç›’è¯„ä¼°ï¼Œä»¥ç¡®ä¿å…¬å¹³å’Œæ•°æ®å®‰å…¨ã€‚æˆ‘ä»¬çš„æ•°æ®æ„å»ºæµç¨‹èåˆäº†ä¸“å®¶äººå·¥å®¡æ ¸å’Œè‡ªåŠ¨åŒ–ä»£ç†è¾…åŠ©éªŒè¯ï¼Œç¡®ä¿è¯­è¨€å‡†ç¡®æ€§ã€æ–‡åŒ–ç›¸å…³æ€§å’Œæ•™è‚²ä»·å€¼ã€‚åœ¨LaoBenchä¸Šå¯¹å¤šä¸ªæœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡ŒåŸºå‡†æµ‹è¯•è¡¨æ˜ï¼Œå½“å‰æ¨¡å‹åœ¨æŒæ¡è€æŒè¯­æ–¹é¢ä»ç„¶é¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚æˆ‘ä»¬å¸Œæœ›LaoBenchèƒ½æ¨åŠ¨å¯¹ä»£è¡¨æ€§ä¸è¶³çš„ä¸œå—äºšè¯­è¨€çš„äººå·¥æ™ºèƒ½æŠ€æœ¯çš„è¿›ä¸€æ­¥ç ”ç©¶å’Œå¼€å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.11334v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨èµ„æºè´«ä¹è¯­è¨€å¦‚è€æŒè¯­çš„è¯„ä¼°ä¸Šå­˜åœ¨æ˜¾è‘—ä¸è¶³ã€‚ä¸ºå¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æ¨å‡ºè€æŒè¯­åŸºå‡†æµ‹è¯•æ•°æ®é›†LaoBenchï¼Œæ—¨åœ¨è¯„ä¼°LLMsåœ¨è€æŒè¯­çš„ç»¼åˆè¯­è¨€ç†è§£å’Œæ¨ç†èƒ½åŠ›ã€‚LaoBenchåŒ…å«è¶…è¿‡17,000ä¸ªç²¾å¿ƒæŒ‘é€‰çš„æ ·æœ¬ï¼Œæ¶µç›–çŸ¥è¯†åº”ç”¨ã€K12åŸºç¡€æ•™è‚²ä»¥åŠè€æŒè¯­ã€ä¸­æ–‡å’Œè‹±æ–‡ä¹‹é—´çš„åŒè¯­ç¿»è¯‘ç­‰ä¸‰ä¸ªæ ¸å¿ƒé¢†åŸŸã€‚æ•°æ®é›†åˆ†ä¸ºå¼€æºå’Œé—­æºå­é›†ï¼Œé—­æºéƒ¨åˆ†å¯åœ¨å®˜æ–¹å¹³å°ä¸Šè¿›è¡Œé»‘ç®±è¯„ä¼°ï¼Œä»¥ç¡®ä¿å…¬å¹³æ€§å’Œæ•°æ®å®‰å…¨ã€‚æˆ‘ä»¬çš„æ•°æ®æ„å»ºæµç¨‹ç»“åˆäº†ä¸“å®¶äººå·¥ç­›é€‰å’Œè‡ªåŠ¨åŒ–è¾…åŠ©éªŒè¯ï¼Œç¡®ä¿è¯­è¨€å‡†ç¡®æ€§ã€æ–‡åŒ–é€‚å®œæ€§å’Œæ•™è‚²ä»·å€¼ã€‚åœ¨LaoBenchä¸Šå¯¹ä¸åŒé¡¶å°–LLMsçš„åŸºå‡†æµ‹è¯•æ˜¾ç¤ºï¼Œå½“å‰æ¨¡å‹åœ¨æŒæ¡è€æŒè¯­æ–¹é¢ä»é¢ä¸´å·¨å¤§æŒ‘æˆ˜ã€‚æˆ‘ä»¬æœŸæœ›LaoBenchèƒ½æ¨åŠ¨é’ˆå¯¹ä»£è¡¨æ€§ä¸è¶³ä¸œå—äºšè¯­è¨€çš„äººå·¥æ™ºèƒ½æŠ€æœ¯çš„è¿›ä¸€æ­¥ç ”å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è€æŒè¯­çš„è¯„ä¼°ä¸Šå­˜åœ¨ä¸è¶³ã€‚</li>
<li>LaoBenchæ˜¯é¦–ä¸ªé’ˆå¯¹è€æŒè¯­çš„å¤§å‹ã€é«˜è´¨é‡ã€å¤šç»´åŸºå‡†æµ‹è¯•æ•°æ®é›†ã€‚</li>
<li>LaoBenchåŒ…å«çŸ¥è¯†åº”ç”¨ã€åŸºç¡€æ•™è‚²ä»¥åŠåŒè¯­ç¿»è¯‘ç­‰ä¸‰ä¸ªæ ¸å¿ƒé¢†åŸŸçš„æ ·æœ¬ã€‚</li>
<li>æ•°æ®é›†åˆ†ä¸ºå¼€æºå’Œé—­æºå­é›†ï¼Œä»¥ç¡®ä¿å…¬å¹³æ€§å’Œæ•°æ®å®‰å…¨ã€‚</li>
<li>æ•°æ®æ„å»ºæµç¨‹ç»“åˆä¸“å®¶äººå·¥ç­›é€‰å’Œè‡ªåŠ¨åŒ–éªŒè¯ï¼Œç¡®ä¿å‡†ç¡®æ€§ã€æ–‡åŒ–é€‚å®œæ€§å’Œæ•™è‚²ä»·å€¼ã€‚</li>
<li>åŸºå‡†æµ‹è¯•æ˜¾ç¤ºå½“å‰æ¨¡å‹åœ¨è€æŒè¯­æŒæ¡ä¸Šä»é¢ä¸´æŒ‘æˆ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.11334">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cb69aa421f39950bbcfb83eeea887ff5" align="middle">
<img src="https://picx.zhimg.com/v2-2a8768a7e306a274201d11f1c67a6b34" align="middle">
<img src="https://picx.zhimg.com/v2-c50765b80990bd56954bb13c33f87f9e" align="middle">
<img src="https://picx.zhimg.com/v2-846a618d82bda1ebbcd04b6a0503feb9" align="middle">
<img src="https://picx.zhimg.com/v2-d4026d4d67e7f2a62230ded60749fcbb" align="middle">
<img src="https://picx.zhimg.com/v2-3cd0b64d8e382924f6a41c97c2c6d096" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="NOVA-An-Agentic-Framework-for-Automated-Histopathology-Analysis-and-Discovery"><a href="#NOVA-An-Agentic-Framework-for-Automated-Histopathology-Analysis-and-Discovery" class="headerlink" title="NOVA: An Agentic Framework for Automated Histopathology Analysis and Discovery"></a>NOVA: An Agentic Framework for Automated Histopathology Analysis and Discovery</h2><p><strong>Authors:Anurag J. Vaidya, Felix Meissen, Daniel C. Castro, Shruthi Bannur, Tristan Lazard, Drew F. K. Williamson, Faisal Mahmood, Javier Alvarez-Valle, Stephanie L. Hyland, Kenza Bouzid</strong></p>
<p>Digitized histopathology analysis involves complex, time-intensive workflows and specialized expertise, limiting its accessibility. We introduce NOVA, an agentic framework that translates scientific queries into executable analysis pipelines by iteratively generating and running Python code. NOVA integrates 49 domain-specific tools (e.g., nuclei segmentation, whole-slide encoding) built on open-source software, and can also create new tools ad hoc. To evaluate such systems, we present SlideQuest, a 90-question benchmark â€“ verified by pathologists and biomedical scientists â€“ spanning data processing, quantitative analysis, and hypothesis testing. Unlike prior biomedical benchmarks focused on knowledge recall or diagnostic QA, SlideQuest demands multi-step reasoning, iterative coding, and computational problem solving. Quantitative evaluation shows NOVA outperforms coding-agent baselines, and a pathologist-verified case study links morphology to prognostically relevant PAM50 subtypes, demonstrating its scalable discovery potential.</p>
<blockquote>
<p>æ•°å­—åŒ–ç»„ç»‡ç—…ç†å­¦åˆ†ææ¶‰åŠå¤æ‚ä¸”è€—æ—¶çš„æµç¨‹ä»¥åŠä¸“ä¸šä¸“ä¸šçŸ¥è¯†ï¼Œè¿™é™åˆ¶äº†å…¶æ™®åŠæ€§ã€‚æˆ‘ä»¬ä»‹ç»äº†NOVAï¼Œè¿™æ˜¯ä¸€ä¸ªä»£ç†æ¡†æ¶ï¼Œé€šè¿‡å°†ç§‘å­¦æŸ¥è¯¢è½¬åŒ–ä¸ºå¯æ‰§è¡Œçš„åˆ†æç®¡é“ï¼Œé€šè¿‡è¿­ä»£ç”Ÿæˆå’Œè¿è¡ŒPythonä»£ç ã€‚NOVAæ•´åˆäº†49ä¸ªåŸºäºå¼€æºè½¯ä»¶çš„ç‰¹å®šé¢†åŸŸå·¥å…·ï¼ˆä¾‹å¦‚ç»†èƒæ ¸åˆ†å‰²ã€å…¨å¹»ç¯ç‰‡ç¼–ç ï¼‰ï¼Œè¿˜å¯ä»¥åˆ›å»ºæ–°çš„å³æ—¶å·¥å…·ã€‚ä¸ºäº†è¯„ä¼°è¿™æ ·çš„ç³»ç»Ÿï¼Œæˆ‘ä»¬æ¨å‡ºäº†SlideQuestï¼Œè¿™æ˜¯ä¸€ä¸ªç”±ç—…ç†å­¦å®¶å’Œç”Ÿç‰©åŒ»å­¦ç§‘å­¦å®¶éªŒè¯çš„åŒ…å«90ä¸ªé—®é¢˜çš„åŸºå‡†æµ‹è¯•ï¼Œæ¶‰åŠæ•°æ®å¤„ç†ã€å®šé‡åˆ†æå’Œå‡è®¾æ£€éªŒã€‚ä¸ä»¥å¾€çš„ä¾§é‡äºçŸ¥è¯†è®°å¿†æˆ–è¯Šæ–­è´¨é‡ä¿éšœçš„ç”Ÿç‰©åŒ»å­¦åŸºå‡†æµ‹è¯•ä¸åŒï¼ŒSlideQuestè¦æ±‚å¤šæ­¥éª¤æ¨ç†ã€è¿­ä»£ç¼–ç å’Œè®¡ç®—é—®é¢˜è§£å†³ã€‚å®šé‡è¯„ä¼°æ˜¾ç¤ºï¼ŒNOVAçš„è¡¨ç°ä¼˜äºç¼–ç ä»£ç†åŸºå‡†æµ‹è¯•ï¼Œå¹¶ä¸”ç»è¿‡ç—…ç†å­¦å®¶éªŒè¯çš„æ¡ˆä¾‹ç ”ç©¶å°†å½¢æ€ä¸é¢„åç›¸å…³çš„PAM50äºšå‹ç›¸è”ç³»ï¼Œå±•ç¤ºäº†å…¶å¯æ‰©å±•çš„å‘ç°æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.11324v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ•°å­—åŒ–ç»„ç»‡ç—…ç†å­¦åˆ†ææ¶‰åŠå¤æ‚ä¸”è€—æ—¶çš„æµç¨‹ï¼Œéœ€è¦ä¸“ä¸šçŸ¥è¯†ï¼Œé™åˆ¶äº†å…¶æ™®åŠæ€§ã€‚æœ¬æ–‡ä»‹ç»äº†NOVAï¼Œä¸€ä¸ªèƒ½å°†ç§‘å­¦æŸ¥è¯¢è½¬åŒ–ä¸ºå¯æ‰§è¡Œåˆ†æç®¡é“çš„æ™ºèƒ½æ¡†æ¶ï¼Œé€šè¿‡è¿­ä»£ç”Ÿæˆå’Œè¿è¡ŒPythonä»£ç æ¥å·¥ä½œã€‚NOVAæ•´åˆäº†49ç§åŸºäºå¼€æºè½¯ä»¶çš„é¢†åŸŸç‰¹å®šå·¥å…·ï¼ˆå¦‚ç»†èƒæ ¸åˆ†å‰²ã€å…¨å¹»ç¯ç‰‡ç¼–ç ï¼‰ï¼Œå¹¶èƒ½å³æ—¶åˆ›å»ºæ–°å·¥å…·ã€‚ä¸ºäº†è¯„ä¼°æ­¤ç±»ç³»ç»Ÿï¼Œæˆ‘ä»¬æ¨å‡ºäº†SlideQuestï¼Œä¸€ä¸ªåŒ…å«90ä¸ªé—®é¢˜çš„åŸºå‡†æµ‹è¯•ï¼Œç”±ç—…ç†å­¦å®¶å’Œç”Ÿç‰©åŒ»å­¦ç§‘å­¦å®¶éªŒè¯ï¼Œæ¶µç›–æ•°æ®å¤„ç†ã€å®šé‡åˆ†æå’Œå‡è®¾æ£€éªŒã€‚ä¸åŒäºä»¥å¾€ä¾§é‡äºçŸ¥è¯†å›å¿†æˆ–è¯Šæ–­è´¨é‡è¯„ä¼°çš„ç”Ÿç‰©åŒ»å­¦åŸºå‡†æµ‹è¯•ï¼ŒSlideQuestè¦æ±‚å¤šæ­¥éª¤æ¨ç†ã€è¿­ä»£ç¼–ç å’Œè®¡ç®—é—®é¢˜è§£å†³èƒ½åŠ›ã€‚å®šé‡è¯„ä¼°æ˜¾ç¤ºNOVAä¼˜äºç¼–ç ä»£ç†åŸºçº¿ï¼Œå¹¶ä¸”é€šè¿‡ç—…ç†å­¦å®¶éªŒè¯çš„ç—…ä¾‹ç ”ç©¶å°†å½¢æ€ä¸é¢„åç›¸å…³çš„PAM50äºšå‹ç›¸è”ç³»ï¼Œå±•ç¤ºäº†å…¶å¯æ‰©å±•çš„å‘ç°æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ•°å­—åŒ–ç»„ç»‡ç—…ç†å­¦åˆ†æå…·æœ‰å¤æ‚æ€§å’Œä¸“ä¸šæ€§ï¼Œé™åˆ¶äº†å…¶æ™®åŠæ€§ã€‚</li>
<li>NOVAæ˜¯ä¸€ä¸ªæ™ºèƒ½æ¡†æ¶ï¼Œèƒ½å°†ç§‘å­¦æŸ¥è¯¢è½¬åŒ–ä¸ºå¯æ‰§è¡Œåˆ†æç®¡é“ï¼Œé€šè¿‡è¿­ä»£ç”ŸæˆPythonä»£ç å·¥ä½œã€‚</li>
<li>NOVAæ•´åˆäº†å¤šç§é¢†åŸŸç‰¹å®šå·¥å…·ï¼Œå¹¶å…·å¤‡å³æ—¶åˆ›å»ºæ–°å·¥å…·çš„èƒ½åŠ›ã€‚</li>
<li>SlideQuestæ˜¯ä¸€ä¸ªåŒ…å«å¤šä¸ªé—®é¢˜çš„åŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–æ•°æ®å¤„ç†ã€å®šé‡åˆ†æå’Œå‡è®¾æ£€éªŒï¼Œæ—¨åœ¨è¯„ä¼°åˆ†æç³»ç»Ÿçš„æ€§èƒ½ã€‚</li>
<li>SlideQuestä¸åŒäºå…¶ä»–ç”Ÿç‰©åŒ»å­¦åŸºå‡†æµ‹è¯•ï¼Œå¼ºè°ƒå¤šæ­¥éª¤æ¨ç†ã€è¿­ä»£ç¼–ç å’Œè®¡ç®—é—®é¢˜è§£å†³èƒ½åŠ›ã€‚</li>
<li>å®šé‡è¯„ä¼°æ˜¾ç¤ºNOVAåœ¨æ€§èƒ½ä¸Šä¼˜äºç¼–ç ä»£ç†åŸºçº¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.11324">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f8577be915f074ed8a7bfa90f81b3e1b" align="middle">
<img src="https://picx.zhimg.com/v2-ebf32ebca6fd64c3bfdcabda07d44fdb" align="middle">
<img src="https://picx.zhimg.com/v2-5927dd5c27c394a714fb9e7c29d077a3" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Can-You-Tell-the-Difference-Contrastive-Explanations-for-ABox-Entailments"><a href="#Can-You-Tell-the-Difference-Contrastive-Explanations-for-ABox-Entailments" class="headerlink" title="Can You Tell the Difference? Contrastive Explanations for ABox Entailments"></a>Can You Tell the Difference? Contrastive Explanations for ABox Entailments</h2><p><strong>Authors:Patrick Koopmann, Yasir Mahmood, Axel-Cyrille Ngonga Ngomo, Balram Tiwari</strong></p>
<p>We introduce the notion of contrastive ABox explanations to answer questions of the type â€œWhy is a an instance of C, but b is not?â€. While there are various approaches for explaining positive entailments (why is C(a) entailed by the knowledge base) as well as missing entailments (why is C(b) not entailed) in isolation, contrastive explanations consider both at the same time, which allows them to focus on the relevant commonalities and differences between a and b. We develop an appropriate notion of contrastive explanations for the special case of ABox reasoning with description logic ontologies, and analyze the computational complexity for different variants under different optimality criteria, considering lightweight as well as more expressive description logics. We implemented a first method for computing one variant of contrastive explanations, and evaluated it on generated problems for realistic knowledge bases.</p>
<blockquote>
<p>æˆ‘ä»¬å¼•å…¥å¯¹æ¯”ABoxè§£é‡Šçš„æ¦‚å¿µï¼Œæ¥å›ç­”â€œä¸ºä»€ä¹ˆaæ˜¯Cçš„å®ä¾‹ï¼Œè€Œbä¸æ˜¯ï¼Ÿâ€ç±»å‹çš„é—®é¢˜ã€‚è™½ç„¶è§£é‡Šæ­£å‘è•´å«ï¼ˆä¸ºä»€ä¹ˆC(a)ç”±çŸ¥è¯†åº“å¼•å‘ï¼‰ä»¥åŠå•ç‹¬ç¼ºå¤±çš„è•´å«ï¼ˆä¸ºä»€ä¹ˆC(b)ä¸å¼•å‘ï¼‰çš„æ–¹æ³•æœ‰å¾ˆå¤šç§ï¼Œä½†å¯¹æ¯”è§£é‡Šå¯ä»¥åŒæ—¶è€ƒè™‘ä¸¤è€…ï¼Œè¿™ä½¿ä»–ä»¬èƒ½å¤Ÿå…³æ³¨aå’Œbä¹‹é—´çš„ç›¸å…³å…±æ€§å’Œå·®å¼‚ã€‚æˆ‘ä»¬é’ˆå¯¹æè¿°é€»è¾‘æœ¬ä½“ä¸­çš„ABoxæ¨ç†è¿™ä¸€ç‰¹æ®Šæƒ…å†µï¼Œå‘å±•äº†å¯¹æ¯”è§£é‡Šçš„åˆç†æ¦‚å¿µï¼Œå¹¶åœ¨ä¸åŒçš„ä¼˜åŒ–æ ‡å‡†ä¸‹åˆ†æäº†ä¸åŒå˜ç§çš„è®¡ç®—å¤æ‚æ€§ï¼ŒåŒæ—¶è€ƒè™‘äº†è½»é‡çº§å’Œæ›´é«˜çº§çš„æè¿°é€»è¾‘ã€‚æˆ‘ä»¬å®ç°äº†è®¡ç®—å¯¹æ¯”è§£é‡Šçš„ä¸€ç§å˜ä½“æ–¹æ³•ï¼Œå¹¶åœ¨é’ˆå¯¹ç°å®çŸ¥è¯†åº“ç”Ÿæˆçš„é—®é¢˜ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.11281v1">PDF</a> Technical report to the paper accepted at AAAI-2026</p>
<p><strong>Summary</strong></p>
<p>åœ¨ä»‹ç»å¯¹æ¯”ABoxè§£é‡Šæ¦‚å¿µæ—¶ï¼Œå›ç­”å…³äºä¸ºä»€ä¹ˆå¯¹è±¡aå±äºç±»åˆ«Cè€Œå¯¹è±¡bä¸å±äºçš„é—®é¢˜ã€‚å¯¹æ¯”è§£é‡ŠåŒæ—¶è€ƒè™‘æ­£å‘å’Œç¼ºå¤±çš„æ¨è®ºï¼Œå…³æ³¨å¯¹è±¡aå’Œbä¹‹é—´çš„å…±åŒç‚¹å’Œå·®å¼‚ã€‚å¯¹äºæè¿°é€»è¾‘æœ¬ä½“ä¸­çš„ABoxæ¨ç†ç‰¹æ®Šæƒ…å†µï¼Œæˆ‘ä»¬å‘å±•äº†å¯¹æ¯”è§£é‡Šçš„ç›¸å…³æ¦‚å¿µï¼Œå¹¶åˆ†æäº†ä¸åŒå˜ä½“åœ¨ä¸åŒæœ€ä¼˜æ ‡å‡†ä¸‹çš„è®¡ç®—å¤æ‚æ€§ï¼ŒåŒæ—¶è€ƒè™‘äº†ç®€æ´å’Œæ›´å¤æ‚çš„æè¿°é€»è¾‘ã€‚æˆ‘ä»¬å®ç°äº†è®¡ç®—å¯¹æ¯”è§£é‡Šçš„ä¸€ç§åˆæ­¥æ–¹æ³•ï¼Œå¹¶åœ¨é’ˆå¯¹ç°å®çŸ¥è¯†åº“ç”Ÿæˆçš„é—®é¢˜ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¯¹æ¯”ABoxè§£é‡Šæ—¨åœ¨å›ç­”ä¸ºä»€ä¹ˆå¯¹è±¡aå±äºç±»åˆ«Cè€Œå¯¹è±¡bä¸å±äºçš„é—®é¢˜ã€‚</li>
<li>å¯¹æ¯”è§£é‡ŠåŒæ—¶è€ƒè™‘æ­£å‘å’Œç¼ºå¤±çš„æ¨è®ºã€‚</li>
<li>å¯¹æ¯”è§£é‡Šå…³æ³¨å¯¹è±¡aå’Œbä¹‹é—´çš„å…±åŒç‚¹å’Œå·®å¼‚ã€‚</li>
<li>å¯¹äºæè¿°é€»è¾‘æœ¬ä½“ä¸­çš„ABoxæ¨ç†ï¼Œå‘å±•äº†å¯¹æ¯”è§£é‡Šçš„ç›¸å…³æ¦‚å¿µã€‚</li>
<li>åˆ†æäº†ä¸åŒå¯¹æ¯”è§£é‡Šå˜ä½“åœ¨ä¸åŒæœ€ä¼˜æ ‡å‡†ä¸‹çš„è®¡ç®—å¤æ‚æ€§ã€‚</li>
<li>å®ç°äº†ä¸€ç§è®¡ç®—å¯¹æ¯”è§£é‡Šçš„åˆæ­¥æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.11281">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0bb9eb1a9c8143280be98ca132cce5f9" align="middle">
<img src="https://picx.zhimg.com/v2-3b6d7b99bd99fb438f5509bad6c3814a" align="middle">
<img src="https://picx.zhimg.com/v2-964a67654437cb1f1c03e5ab09481eeb" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="GraphPilot-Grounded-Scene-Graph-Conditioning-for-Language-Based-Autonomous-Driving"><a href="#GraphPilot-Grounded-Scene-Graph-Conditioning-for-Language-Based-Autonomous-Driving" class="headerlink" title="GraphPilot: Grounded Scene Graph Conditioning for Language-Based Autonomous Driving"></a>GraphPilot: Grounded Scene Graph Conditioning for Language-Based Autonomous Driving</h2><p><strong>Authors:Fabian Schmidt, Markus Enzweiler, Abhinav Valada</strong></p>
<p>Vision-language models have recently emerged as promising planners for autonomous driving, where success hinges on topology-aware reasoning over spatial structure and dynamic interactions from multimodal input. However, existing models are typically trained without supervision that explicitly encodes these relational dependencies, limiting their ability to infer how agents and other traffic entities influence one another from raw sensor data. In this work, we bridge this gap with a novel model-agnostic method that conditions language-based driving models on structured relational context in the form of traffic scene graphs. We serialize scene graphs at various abstraction levels and formats, and incorporate them into the models via structured prompt templates, enabling a systematic analysis of when and how relational supervision is most beneficial. Extensive evaluations on the public LangAuto benchmark show that scene graph conditioning of state-of-the-art approaches yields large and persistent improvement in driving performance. Notably, we observe up to a 15.6% increase in driving score for LMDrive and 17.5% for BEVDriver, indicating that models can better internalize and ground relational priors through scene graph-conditioned training, even without requiring scene graph input at test-time. Code, fine-tuned models, and our scene graph dataset are publicly available at <a target="_blank" rel="noopener" href="https://github.com/iis-esslingen/GraphPilot">https://github.com/iis-esslingen/GraphPilot</a>.</p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹æœ€è¿‘ä½œä¸ºè‡ªåŠ¨é©¾é©¶çš„æ½œåœ¨è§„åˆ’å™¨è€Œå‡ºç°ï¼Œå…¶æˆåŠŸå–å†³äºå¯¹ç©ºé—´ç»“æ„å’Œå¤šæ¨¡æ€è¾“å…¥çš„åŠ¨æ€äº¤äº’è¿›è¡Œæ‹“æ‰‘æ„ŸçŸ¥æ¨ç†ã€‚ç„¶è€Œï¼Œç°æœ‰æ¨¡å‹é€šå¸¸åœ¨æ²¡æœ‰æ˜ç¡®ç¼–ç è¿™äº›å…³ç³»ä¾èµ–é¡¹çš„ç›‘ç£ä¸‹è¿›è¡Œè®­ç»ƒï¼Œè¿™é™åˆ¶äº†å®ƒä»¬ä»åŸå§‹ä¼ æ„Ÿå™¨æ•°æ®ä¸­æ¨æ–­å‡ºä»£ç†å’Œå…¶ä»–äº¤é€šå®ä½“å¦‚ä½•ç›¸äº’å½±å“çš„èƒ½åŠ›ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼¥è¡¥äº†è¿™ä¸€ç©ºç™½ï¼Œæå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¨¡å‹é€šç”¨æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä»¥äº¤é€šåœºæ™¯å›¾çš„å½¢å¼å¯¹åŸºäºè¯­è¨€çš„é©¾é©¶æ¨¡å‹è¿›è¡Œç»“æ„åŒ–å…³ç³»ä¸Šä¸‹æ–‡æ¡ä»¶è®¾ç½®ã€‚æˆ‘ä»¬ä»¥å„ç§æŠ½è±¡å±‚æ¬¡å’Œæ ¼å¼åºåˆ—åŒ–åœºæ™¯å›¾ï¼Œå¹¶é€šè¿‡ç»“æ„åŒ–æç¤ºæ¨¡æ¿å°†å®ƒä»¬çº³å…¥æ¨¡å‹ï¼Œä»è€Œèƒ½å¤Ÿç³»ç»Ÿåœ°åˆ†æå…³ç³»ç›‘ç£ä½•æ—¶ä»¥åŠå¦‚ä½•æœ€æœ‰ç›Šã€‚åœ¨å…¬å…±LangAutoåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›è¯„ä¼°æ˜¾ç¤ºï¼Œé€šè¿‡åœºæ™¯å›¾æ¡ä»¶è®¾ç½®çš„æœ€å…ˆè¿›æ–¹æ³•èƒ½å¤Ÿåœ¨é©¾é©¶æ€§èƒ½ä¸Šäº§ç”Ÿå·¨å¤§ä¸”æŒç»­çš„æ”¹è¿›ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°LMDriveçš„é©¾é©¶å¾—åˆ†æé«˜äº†15.6%ï¼ŒBEVDriveræé«˜äº†17.5%ï¼Œè¿™è¡¨æ˜æ¨¡å‹é€šè¿‡åœºæ™¯å›¾æ¡ä»¶è®­ç»ƒèƒ½å¤Ÿæ›´å¥½åœ°å†…éƒ¨åŒ–å’Œè§£é‡Šå…³ç³»å…ˆéªŒçŸ¥è¯†ï¼Œå³ä½¿ä¸éœ€è¦åœ¨æµ‹è¯•æ—¶è¾“å…¥åœºæ™¯å›¾ã€‚ç›¸å…³ä»£ç ã€å¾®è°ƒåçš„æ¨¡å‹å’Œåœºæ™¯å›¾æ•°æ®é›†å¯å…¬å¼€è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/iis-esslingen/GraphPilot%E3%80%82">https://github.com/iis-esslingen/GraphPilotã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.11266v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸå±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œéœ€è¦æ‹“æ‰‘æ„ŸçŸ¥æ¨ç†å’Œç©ºé—´ç»“æ„ä»¥åŠå¤šæ¨¡æ€è¾“å…¥çš„åŠ¨æ€äº¤äº’ã€‚ç„¶è€Œï¼Œç°æœ‰æ¨¡å‹é€šå¸¸åœ¨æ²¡æœ‰æ˜ç¡®ç¼–ç è¿™äº›å…³ç³»ä¾èµ–æ€§çš„ç›‘ç£ä¸‹è¿›è¡Œè®­ç»ƒï¼Œé™åˆ¶äº†å®ƒä»¬ä»åŸå§‹ä¼ æ„Ÿå™¨æ•°æ®ä¸­æ¨æ–­å‡ºå…¶ä»–äº¤é€šå®ä½“ä¹‹é—´ç›¸äº’ä½œç”¨çš„èƒ½åŠ›ã€‚æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹æ¨¡å‹æ— å…³çš„æ–¹æ³•ï¼Œä»¥äº¤é€šåœºæ™¯å›¾çš„å½¢å¼å¯¹åŸºäºè¯­è¨€çš„é©¾é©¶æ¨¡å‹è¿›è¡Œç»“æ„åŒ–å…³ç³»ä¸Šä¸‹æ–‡æ¡ä»¶å¤„ç†ã€‚æˆ‘ä»¬é€šè¿‡ä¸åŒæŠ½è±¡å±‚æ¬¡å’Œæ ¼å¼çš„åœºæ™¯å›¾åºåˆ—åŒ–ï¼Œå¹¶é€šè¿‡ç»“æ„åŒ–æç¤ºæ¨¡æ¿å°†å…¶çº³å…¥æ¨¡å‹ï¼Œä»è€Œç³»ç»Ÿåœ°åˆ†æå…³ç³»ç›‘ç£ä½•æ—¶ä»¥åŠå¦‚ä½•æœ€æœ‰ç›Šã€‚åœ¨å…¬å…±LangAutoåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›è¯„ä¼°æ˜¾ç¤ºï¼Œåœºæ™¯å›¾æ¡ä»¶çš„æœ€å…ˆè¿›æ–¹æ³•å¤§å¤§æé«˜äº†é©¾é©¶æ€§èƒ½ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°LMDriveçš„é©¾é©¶è¯„åˆ†æé«˜äº†15.6%ï¼ŒBEVDriveræé«˜äº†17.5%ï¼Œè¿™è¡¨æ˜æ¨¡å‹é€šè¿‡åœºæ™¯å›¾æ¡ä»¶è®­ç»ƒå¯ä»¥æ›´å¥½åœ°å†…åŒ–å¹¶éªŒè¯å…³ç³»å…ˆéªŒçŸ¥è¯†ï¼Œç”šè‡³åœ¨æµ‹è¯•æ—¶æ— éœ€åœºæ™¯å›¾è¾“å…¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸæœ‰å·¨å¤§æ½œåŠ›ï¼Œéœ€è¦å¤„ç†ç©ºé—´ç»“æ„å’Œå¤šæ¨¡æ€è¾“å…¥çš„åŠ¨æ€äº¤äº’ã€‚</li>
<li>ç°æœ‰æ¨¡å‹åœ¨è®­ç»ƒæ—¶ç¼ºä¹æ˜¾å¼ç¼–ç å…³ç³»ä¾èµ–æ€§çš„ç›‘ç£ï¼Œé™åˆ¶äº†å…¶å¯¹äº¤é€šå®ä½“é—´ç›¸äº’ä½œç”¨çš„ç†è§£ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹æ¨¡å‹æ— å…³çš„æ–¹æ³•ï¼Œé€šè¿‡äº¤é€šåœºæ™¯å›¾å¯¹è¯­è¨€é©¾é©¶æ¨¡å‹è¿›è¡Œç»“æ„åŒ–å…³ç³»ä¸Šä¸‹æ–‡æ¡ä»¶å¤„ç†ã€‚</li>
<li>é€šè¿‡ä¸åŒæŠ½è±¡å±‚æ¬¡å’Œæ ¼å¼çš„åœºæ™¯å›¾åºåˆ—åŒ–ï¼Œç»“åˆç»“æ„åŒ–æç¤ºæ¨¡æ¿ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿç³»ç»Ÿåœ°åˆ†æå…³ç³»ç›‘ç£çš„æ•ˆç›Šã€‚</li>
<li>åœ¨LangAutoåŸºå‡†æµ‹è¯•ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œåœºæ™¯å›¾æ¡ä»¶çš„æœ€å…ˆè¿›æ–¹æ³•èƒ½æ˜¾è‘—æé«˜é©¾é©¶æ€§èƒ½ã€‚</li>
<li>LMDriveå’ŒBEVDriverçš„é©¾é©¶è¯„åˆ†åˆ†åˆ«æé«˜äº†15.6%å’Œ17.5%ï¼Œè¡¨æ˜æ¨¡å‹é€šè¿‡åœºæ™¯å›¾æ¡ä»¶è®­ç»ƒå¯ä»¥æ›´å¥½åœ°å†…åŒ–å…³ç³»å…ˆéªŒçŸ¥è¯†ã€‚</li>
<li>æ¨¡å‹åœ¨æµ‹è¯•æ—¶æ— éœ€åœºæ™¯å›¾è¾“å…¥ï¼Œä¾ç„¶èƒ½å¤Ÿå—ç›Šäºä¹‹å‰çš„åœºæ™¯å›¾æ¡ä»¶è®­ç»ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.11266">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f739fe285885c3923d98e8822268fd37" align="middle">
<img src="https://picx.zhimg.com/v2-0b10ca038fa8d5b47c6c89f125e6ac41" align="middle">
<img src="https://picx.zhimg.com/v2-6c02de10baa09012b5845131d944b025" align="middle">
<img src="https://picx.zhimg.com/v2-4104cc17a937b73422db7e0e451e6718" align="middle">
<img src="https://picx.zhimg.com/v2-e49dc3e13b28095998236a089f3744d0" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Align-3-GR-Unified-Multi-Level-Alignment-for-LLM-based-Generative-Recommendation"><a href="#Align-3-GR-Unified-Multi-Level-Alignment-for-LLM-based-Generative-Recommendation" class="headerlink" title="Align$^3$GR: Unified Multi-Level Alignment for LLM-based Generative Recommendation"></a>Align$^3$GR: Unified Multi-Level Alignment for LLM-based Generative Recommendation</h2><p><strong>Authors:Wencai Ye, Mingjie Sun, Shuhang Chen, Wenjin Wu, Peng Jiang</strong></p>
<p>Large Language Models (LLMs) demonstrate significant advantages in leveraging structured world knowledge and multi-step reasoning capabilities. However, fundamental challenges arise when transforming LLMs into real-world recommender systems due to semantic and behavioral misalignment. To bridge this gap, we propose Align$^3$GR, a novel framework that unifies token-level, behavior modeling-level, and preference-level alignment. Our approach introduces: Dual tokenization fusing user-item semantic and collaborative signals. Enhanced behavior modeling with bidirectional semantic alignment. Progressive DPO strategy combining self-play (SP-DPO) and real-world feedback (RF-DPO) for dynamic preference adaptation. Experiments show Align$^3$GR outperforms the SOTA baseline by +17.8% in Recall@10 and +20.2% in NDCG@10 on the public dataset, with significant gains in online A&#x2F;B tests and full-scale deployment on an industrial large-scale recommendation platform.</p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨åˆ©ç”¨ç»“æ„åŒ–ä¸–ç•ŒçŸ¥è¯†å’Œå¤šæ­¥æ¨ç†èƒ½åŠ›æ–¹é¢æ˜¾ç¤ºå‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚ç„¶è€Œï¼Œå°†LLMsè½¬åŒ–ä¸ºç°å®ä¸–ç•Œçš„æ¨èç³»ç»Ÿæ—¶ï¼Œç”±äºè¯­ä¹‰å’Œè¡Œä¸ºçš„ä¸å¯¹é½ï¼Œä¼šå‡ºç°åŸºæœ¬æŒ‘æˆ˜ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†Align$^3$GRï¼Œè¿™æ˜¯ä¸€ç§ç»Ÿä¸€æ ‡è®°å±‚ã€è¡Œä¸ºå»ºæ¨¡å±‚å’Œåå¥½å±‚å¯¹é½çš„æ–°å‹æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†ï¼šèåˆç”¨æˆ·é¡¹ç›®è¯­ä¹‰å’ŒååŒä¿¡å·çš„åŒé‡æ ‡è®°ã€‚é€šè¿‡åŒå‘è¯­ä¹‰å¯¹é½å¢å¼ºè¡Œä¸ºå»ºæ¨¡ã€‚ç»“åˆè‡ªç©ï¼ˆSP-DPOï¼‰å’Œç°å®ä¸–ç•Œåé¦ˆï¼ˆRF-DPOï¼‰çš„æ¸è¿›å¼DPOç­–ç•¥ï¼Œç”¨äºåŠ¨æ€åå¥½é€‚åº”ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨å…¬å¼€æ•°æ®é›†ä¸Šï¼ŒAlign$^3$GRçš„Recall@10æŒ‡æ ‡ä¼˜äºæœ€æ–°æŠ€æœ¯åŸºçº¿+17.8%ï¼ŒNDCG@10æŒ‡æ ‡ä¼˜äº+20.2%ã€‚åœ¨çº¿A&#x2F;Bæµ‹è¯•å’Œå¤§å‹å·¥ä¸šæ¨èå¹³å°çš„å…¨é¢éƒ¨ç½²ä¹Ÿå–å¾—äº†æ˜¾è‘—æˆæœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.11255v1">PDF</a> Accepted by AAAI 2026 (Oral)</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨åˆ©ç”¨ç»“æ„åŒ–ä¸–ç•ŒçŸ¥è¯†å’Œå¤šæ­¥æ¨ç†èƒ½åŠ›æ–¹é¢å±•ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ï¼Œä½†åœ¨å°†å…¶è½¬åŒ–ä¸ºå®é™…æ¨èç³»ç»Ÿæ—¶é¢ä¸´è¯­ä¹‰å’Œè¡Œä¸ºå¤±é…çš„æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†Align$^3$GRæ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»Ÿä¸€äº†ä»¤ç‰Œçº§ã€è¡Œä¸ºå»ºæ¨¡çº§å’Œåå¥½çº§çš„å¯¹é½ï¼Œé€šè¿‡åŒä»£å¸åŒ–èåˆç”¨æˆ·é¡¹ç›®è¯­ä¹‰å’ŒååŒä¿¡å·ï¼Œå¢å¼ºäº†è¡Œä¸ºå»ºæ¨¡çš„åŒå‘è¯­ä¹‰å¯¹é½ï¼Œå¹¶é‡‡ç”¨äº†ç»“åˆè‡ªæˆ‘æ¸¸æˆï¼ˆSP-DPOï¼‰å’Œç°å®ä¸–ç•Œåé¦ˆï¼ˆRF-DPOï¼‰çš„æ¸è¿›å¼DPOç­–ç•¥ï¼Œå®ç°åŠ¨æ€åå¥½é€‚åº”ã€‚åœ¨å…¬å¼€æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒAlign$^3$GRç›¸æ¯”æœ€å…ˆè¿›åŸºçº¿åœ¨Recall@10å’ŒNDCG@10ä¸Šçš„æ€§èƒ½åˆ†åˆ«æé«˜äº†+17.8%å’Œ+20.2%ï¼Œåœ¨çº¿A&#x2F;Bæµ‹è¯•å’Œå¤§è§„æ¨¡æ¨èå¹³å°ä¸Šçš„å…¨é¢éƒ¨ç½²ä¹Ÿå–å¾—äº†æ˜¾è‘—æˆæ•ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç»“æ„åŒ–ä¸–ç•ŒçŸ¥è¯†å’Œå¤šæ­¥æ¨ç†æ–¹é¢å…·æœ‰ä¼˜åŠ¿ã€‚</li>
<li>å°†LLMsè½¬åŒ–ä¸ºå®é™…æ¨èç³»ç»Ÿå­˜åœ¨è¯­ä¹‰å’Œè¡Œä¸ºå¤±é…çš„æŒ‘æˆ˜ã€‚</li>
<li>Align$^3$GRæ¡†æ¶é€šè¿‡ç»Ÿä¸€ä»¤ç‰Œçº§ã€è¡Œä¸ºå»ºæ¨¡çº§å’Œåå¥½çº§çš„å¯¹é½æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚</li>
<li>Align$^3$GRé€šè¿‡åŒä»£å¸åŒ–èåˆç”¨æˆ·é¡¹ç›®è¯­ä¹‰å’ŒååŒä¿¡å·ã€‚</li>
<li>è¯¥æ¡†æ¶å¢å¼ºäº†è¡Œä¸ºå»ºæ¨¡çš„åŒå‘è¯­ä¹‰å¯¹é½ã€‚</li>
<li>Align$^3$GRé‡‡ç”¨ç»“åˆè‡ªæˆ‘æ¸¸æˆå’Œç°å®ä¸–ç•Œåé¦ˆçš„æ¸è¿›å¼DPOç­–ç•¥ï¼Œå®ç°åŠ¨æ€åå¥½é€‚åº”ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.11255">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-af6e8a50c67b8aeafeb6c3c30976d904" align="middle">
<img src="https://picx.zhimg.com/v2-0c4ddbb8bf7f8f1d26a93851443b7fdd" align="middle">
<img src="https://picx.zhimg.com/v2-3bdf6b5888b5e6a893bb6f1ab6d0d475" align="middle">
<img src="https://picx.zhimg.com/v2-71cd3a49b4fd206997f37bf03ac9148d" align="middle">
<img src="https://picx.zhimg.com/v2-b31ffd974caaf9290e5892987b391f6b" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Beyond-Flatlands-Unlocking-Spatial-Intelligence-by-Decoupling-3D-Reasoning-from-Numerical-Regression"><a href="#Beyond-Flatlands-Unlocking-Spatial-Intelligence-by-Decoupling-3D-Reasoning-from-Numerical-Regression" class="headerlink" title="Beyond Flatlands: Unlocking Spatial Intelligence by Decoupling 3D Reasoning from Numerical Regression"></a>Beyond Flatlands: Unlocking Spatial Intelligence by Decoupling 3D Reasoning from Numerical Regression</h2><p><strong>Authors:Zhongbin Guo, Jiahe Liu, Yushan Li, Wenyu Gao, Zhen Yang, Chenzhi Li, Xinyue Zhang, Ping Jian</strong></p>
<p>Existing Vision Language Models (VLMs) architecturally rooted in â€œflatlandâ€ perception, fundamentally struggle to comprehend real-world 3D spatial intelligence. This failure stems from a dual-bottleneck: input-stage conflict between computationally exorbitant geometric-aware encoders and superficial 2D-only features, and output-stage misalignment where discrete tokenizers are structurally incapable of producing precise, continuous numerical values. To break this impasse, we introduce GEODE (Geometric-Output and Decoupled-Input Engine), a novel architecture that resolves this dual-bottleneck by decoupling 3D reasoning from numerical generation. GEODE augments main VLM with two specialized, plug-and-play modules: Decoupled Rationale Module (DRM) that acts as spatial co-processor, aligning explicit 3D data with 2D visual features via cross-attention and distilling spatial Chain-of-Thought (CoT) logic into injectable Rationale Tokens; and Direct Regression Head (DRH), an â€œEmbedding-as-Valueâ€ paradigm which routes specialized control tokens to a lightweight MLP for precise, continuous regression of scalars and 3D bounding boxes. The synergy of these modules allows our 1.5B parameter model to function as a high-level semantic dispatcher, achieving state-of-the-art spatial reasoning performance that rivals 7B+ models.</p>
<blockquote>
<p>ç°æœ‰çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨æ¶æ„ä¸Šæ ¹æ¤äºâ€œå¹³é¢â€æ„ŸçŸ¥ï¼Œåœ¨ç†è§£çœŸå®ä¸–ç•Œçš„ä¸‰ç»´ç©ºé—´æ™ºèƒ½æ—¶å­˜åœ¨æ ¹æœ¬æ€§çš„å›°éš¾ã€‚è¿™ç§å¤±è´¥æºäºåŒé‡ç“¶é¢ˆï¼šè¾“å…¥é˜¶æ®µçš„å†²çªï¼Œå³è®¡ç®—æ˜‚è´µçš„å‡ ä½•æ„ŸçŸ¥ç¼–ç å™¨å’Œæµ…å±‚çš„ä»…äºŒç»´ç‰¹å¾ä¹‹é—´çš„å†²çªï¼›ä»¥åŠè¾“å‡ºé˜¶æ®µçš„ä¸å¯¹é½ï¼Œå³ç¦»æ•£çš„åˆ†è¯å™¨åœ¨ç»“æ„ä¸Šæ— æ³•ç”Ÿæˆç²¾ç¡®ã€è¿ç»­çš„æ•°å­—å€¼ã€‚ä¸ºäº†æ‰“ç ´è¿™ä¸€åƒµå±€ï¼Œæˆ‘ä»¬å¼•å…¥äº†GEODEï¼ˆå‡ ä½•è¾“å‡ºå’Œè§£è€¦è¾“å…¥å¼•æ“ï¼‰è¿™ä¸€æ–°å‹æ¶æ„ï¼Œå®ƒé€šè¿‡è§£è€¦ä¸‰ç»´æ¨ç†å’Œæ•°å€¼ç”Ÿæˆæ¥è§£å†³è¿™ä¸€åŒé‡ç“¶é¢ˆã€‚GEODEå¢å¼ºä¸»VLMä¸¤ä¸ªä¸“ä¸šã€å³æ’å³ç”¨çš„æ¨¡å—ï¼šè§£è€¦ç†ç”±æ¨¡å—ï¼ˆDRMï¼‰ä½œä¸ºç©ºé—´åå¤„ç†å™¨ï¼Œé€šè¿‡äº¤å‰æ³¨æ„åŠ›å°†æ˜¾å¼ä¸‰ç»´æ•°æ®ä¸äºŒç»´è§†è§‰ç‰¹å¾å¯¹é½ï¼Œå¹¶å°†ç©ºé—´æ€ç»´é“¾é€»è¾‘æç‚¼æˆå¯æ³¨å…¥çš„ç†ç”±ä»¤ç‰Œï¼›ä»¥åŠç›´æ¥å›å½’å¤´ï¼ˆDRHï¼‰ï¼Œè¿™æ˜¯ä¸€ç§â€œåµŒå…¥å³å€¼â€çš„æ¨¡å¼ï¼Œå®ƒå°†ä¸“ç”¨æ§åˆ¶ä»¤ç‰Œè·¯ç”±åˆ°è½»é‡çº§å¤šå±‚æ„ŸçŸ¥å™¨ï¼Œè¿›è¡Œç²¾ç¡®ã€è¿ç»­çš„æ ‡é‡å›å½’å’Œä¸‰ç»´è¾¹ç•Œæ¡†ã€‚è¿™äº›æ¨¡å—çš„ååŒä½œç”¨ä½¿å¾—æˆ‘ä»¬çš„1.5Bå‚æ•°æ¨¡å‹èƒ½å¤Ÿä½œä¸ºé«˜çº§è¯­ä¹‰è°ƒåº¦å™¨å‘æŒ¥ä½œç”¨ï¼Œå®ç°äº†å“è¶Šçš„ç©ºé—´æ¨ç†æ€§èƒ½ï¼Œä¸7B+æ¨¡å‹ç›¸åŒ¹æ•Œã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.11239v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä¸»è¦è®¨è®ºäº†ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨ç†è§£çœŸå®ä¸–ç•Œ3Dç©ºé—´æ™ºèƒ½æ–¹é¢çš„æ ¹æœ¬æ€§æŒ‘æˆ˜ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªæ–°çš„æ¶æ„GEODEæ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚GEODEé€šè¿‡è§£è€¦3Dæ¨ç†å’Œæ•°å€¼ç”Ÿæˆï¼Œä½¿ç”¨ä¸¤ä¸ªä¸“ç”¨æ¨¡å—â€”â€”è§£è€¦ç†æ€§æ¨¡å—ï¼ˆDRMï¼‰å’Œç›´æ¥å›å½’å¤´ï¼ˆDRHï¼‰æ¥æå‡VLMsçš„æ€§èƒ½ã€‚è¯¥æ¶æ„å®ç°äº†å¯¹3Dç©ºé—´çš„ç†è§£ï¼Œå¹¶è¾¾åˆ°äº†å…ˆè¿›çš„ç©ºé—´æ¨ç†æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨ç†è§£çœŸå®ä¸–ç•Œ3Dç©ºé—´æ™ºèƒ½æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>æŒ‘æˆ˜æºäºè¾“å…¥é˜¶æ®µçš„å†²çªå’Œè¾“å‡ºé˜¶æ®µçš„ä¸å¯¹é½ã€‚</li>
<li>GEODEæ¶æ„é€šè¿‡è§£è€¦3Dæ¨ç†å’Œæ•°å€¼ç”Ÿæˆæ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ã€‚</li>
<li>GEODEåŒ…å«ä¸¤ä¸ªä¸“ç”¨æ¨¡å—ï¼šDecoupled Rationale Module (DRM) å’Œ Direct Regression Head (DRH)ã€‚</li>
<li>DRMä½œä¸ºç©ºé—´åå¤„ç†å™¨ï¼Œé€šè¿‡å¯¹é½æ˜¾å¼3Dæ•°æ®å’Œ2Dè§†è§‰ç‰¹å¾ï¼Œå¹¶å°†ç©ºé—´é€»è¾‘è½¬åŒ–ä¸ºå¯æ³¨å…¥çš„ç†æ€§ä»¤ç‰Œã€‚</li>
<li>DRHé‡‡ç”¨â€œåµŒå…¥å³å€¼â€èŒƒå¼ï¼Œé€šè¿‡ä¸“ç”¨æ§åˆ¶ä»¤ç‰Œåˆ°è½»é‡çº§MLPï¼Œå®ç°ç²¾ç¡®ã€è¿ç»­çš„æ ‡é‡å›å½’å’Œ3Dè¾¹ç•Œæ¡†å›å½’ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.11239">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-45d76ab69908436e274a4d34b4849d96" align="middle">
<img src="https://picx.zhimg.com/v2-15bd8e6ccf851a31e0b95bee403fbc69" align="middle">
<img src="https://picx.zhimg.com/v2-60cf11d245c0e33efef3b5452400c01c" align="middle">
<img src="https://picx.zhimg.com/v2-9f25f1e0eec3444b04393f4de24bcc6a" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Multi-agent-Undercover-Gaming-Hallucination-Removal-via-Counterfactual-Test-for-Multimodal-Reasoning"><a href="#Multi-agent-Undercover-Gaming-Hallucination-Removal-via-Counterfactual-Test-for-Multimodal-Reasoning" class="headerlink" title="Multi-agent Undercover Gaming: Hallucination Removal via Counterfactual Test for Multimodal Reasoning"></a>Multi-agent Undercover Gaming: Hallucination Removal via Counterfactual Test for Multimodal Reasoning</h2><p><strong>Authors:Dayong Liang, Xiao-Yong Wei, Changmeng Zheng</strong></p>
<p>Hallucination continues to pose a major obstacle in the reasoning capabilities of large language models (LLMs). Although the Multi-Agent Debate (MAD) paradigm offers a promising solution by promoting consensus among multiple agents to enhance reliability, it relies on the unrealistic assumption that all debaters are rational and reflective, which is a condition that may not hold when agents themselves are prone to hallucinations. To address this gap, we introduce the Multi-agent Undercover Gaming (MUG) protocol, inspired by social deduction games like â€œWho is Undercover?â€. MUG reframes MAD as a process of detecting â€œundercoverâ€ agents (those suffering from hallucinations) by employing multimodal counterfactual tests. Specifically, we modify reference images to introduce counterfactual evidence and observe whether agents can accurately identify these changes, providing ground-truth for identifying hallucinating agents and enabling robust, crowd-powered multimodal reasoning. MUG advances MAD protocols along three key dimensions: (1) enabling factual verification beyond statistical consensus through counterfactual testing; (2) introducing cross-evidence reasoning via dynamically modified evidence sources instead of relying on static inputs; and (3) fostering active reasoning, where agents engage in probing discussions rather than passively answering questions. Collectively, these innovations offer a more reliable and effective framework for multimodal reasoning in LLMs. The source code can be accessed at <a target="_blank" rel="noopener" href="https://github.com/YongLD/MUG.git">https://github.com/YongLD/MUG.git</a>.</p>
<blockquote>
<p>å¹»è§‰ä»ç„¶æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†èƒ½åŠ›çš„ä¸»è¦éšœç¢ã€‚è™½ç„¶å¤šæ™ºèƒ½ä½“è¾©è®ºï¼ˆMADï¼‰èŒƒå¼é€šè¿‡ä¿ƒè¿›å¤šä¸ªæ™ºèƒ½ä½“ä¹‹é—´çš„å…±è¯†æ¥æé«˜å¯é æ€§ï¼Œä»è€Œæä¾›äº†æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆï¼Œä½†å®ƒä¾èµ–äºä¸€ä¸ªä¸åˆ‡å®é™…çš„å‡è®¾ï¼Œå³æ‰€æœ‰è¾©è®ºè€…éƒ½æ˜¯ç†æ€§å’Œåæ€çš„ï¼Œè¿™åœ¨æ™ºèƒ½ä½“è‡ªèº«å®¹æ˜“é™·å…¥å¹»è§‰çš„æƒ…å†µä¸‹å¯èƒ½ä¸æˆç«‹ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å—åˆ°â€œè°æ˜¯å§åº•ï¼Ÿâ€ç­‰ç¤¾ä¼šæ¨ç†æ¸¸æˆçš„å¯å‘ï¼Œå¼•å…¥äº†å¤šæ™ºèƒ½ä½“å§åº•æ¸¸æˆï¼ˆMUGï¼‰åè®®ã€‚MUGå°†MADé‡æ–°æ„å»ºä¸ºæ£€æµ‹â€œå§åº•â€æ™ºèƒ½ä½“ï¼ˆå³é‚£äº›å‡ºç°å¹»è§‰çš„æ™ºèƒ½ä½“ï¼‰çš„è¿‡ç¨‹ï¼Œé‡‡ç”¨å¤šæ¨¡å¼åäº‹å®æµ‹è¯•ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä¿®æ”¹å‚è€ƒå›¾åƒä»¥å¼•å…¥åäº‹å®è¯æ®ï¼Œå¹¶è§‚å¯Ÿæ™ºèƒ½ä½“æ˜¯å¦èƒ½å‡†ç¡®è¯†åˆ«è¿™äº›å˜åŒ–ï¼Œä¸ºè¯†åˆ«å¹»è§‰æ™ºèƒ½ä½“æä¾›çœŸå®ä¾æ®ï¼Œä»è€Œå®ç°å¥å£®çš„ã€ä¼—æºçš„å¤šæ¨¡å¼æ¨ç†ã€‚MUGåœ¨ä¸‰ä¸ªå…³é”®ç»´åº¦ä¸Šæ¨åŠ¨äº†MADåè®®çš„å‘å±•ï¼šï¼ˆ1ï¼‰é€šè¿‡åäº‹å®æµ‹è¯•å®ç°è¶…è¶Šç»Ÿè®¡å…±è¯†çš„äº‹å®éªŒè¯ï¼›ï¼ˆ2ï¼‰é€šè¿‡åŠ¨æ€ä¿®æ”¹çš„è¯æ®æºå¼•å…¥äº¤å‰è¯æ®æ¨ç†ï¼Œè€Œä¸æ˜¯ä¾èµ–é™æ€è¾“å…¥ï¼›ï¼ˆ3ï¼‰åŸ¹å…»ä¸»åŠ¨æ¨ç†ï¼Œè®©æ™ºèƒ½ä½“å‚ä¸æ¢ç©¶è®¨è®ºï¼Œè€Œä¸æ˜¯è¢«åŠ¨å›ç­”é—®é¢˜ã€‚æ€»çš„æ¥è¯´ï¼Œè¿™äº›åˆ›æ–°ä¸ºå¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„æ¨ç†æä¾›äº†æ›´å¯é ã€æ›´æœ‰æ•ˆçš„æ¡†æ¶ã€‚æºä»£ç å¯è®¿é—®<a target="_blank" rel="noopener" href="https://github.com/YongLD/MUG.git%E3%80%82">https://github.com/YongLD/MUG.gitã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.11182v1">PDF</a> Accepted by AAAI 2026</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ä»ç„¶é¢ä¸´å¹»è§‰è¿™ä¸€ä¸»è¦éšœç¢ã€‚è™½ç„¶å¤šæ™ºèƒ½ä½“è¾©è®ºï¼ˆMADï¼‰èŒƒå¼é€šè¿‡ä¿ƒè¿›å¤šä¸ªæ™ºèƒ½ä½“ä¹‹é—´çš„å…±è¯†æ¥å¢å¼ºå¯é æ€§ï¼Œä¸ºè§£å†³è¿™ä¸€é—®é¢˜æä¾›äº†å¸Œæœ›ï¼Œä½†å®ƒä¾èµ–äºæ‰€æœ‰è¾©è®ºè€…éƒ½æ˜¯ç†æ€§å’Œåæ€çš„è¿™ä¸€ä¸åˆ‡å®é™…çš„å‡è®¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å—åˆ°â€œè°æ˜¯å§åº•â€ç­‰ç¤¾ä¼šæ¨ç†æ¸¸æˆçš„å¯å‘ï¼Œå¼•å…¥äº†å¤šæ™ºèƒ½ä½“å§åº•æ¸¸æˆï¼ˆMUGï¼‰åè®®ã€‚MUGå°†MADé‡æ–°æ„å»ºä¸ºä¸€ä¸ªæ£€æµ‹â€œå§åº•â€æ™ºèƒ½ä½“çš„è¿‡ç¨‹ï¼Œé€šè¿‡é‡‡ç”¨å¤šæ¨¡æ€åäº‹å®æµ‹è¯•æ¥è¯†åˆ«é‚£äº›å—åˆ°å¹»è§‰å½±å“çš„æ™ºèƒ½ä½“ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä¿®æ”¹å‚è€ƒå›¾åƒä»¥å¼•å…¥åäº‹å®è¯æ®ï¼Œå¹¶è§‚å¯Ÿæ™ºèƒ½ä½“æ˜¯å¦èƒ½å‡†ç¡®è¯†åˆ«è¿™äº›å˜åŒ–ï¼Œä»è€Œä¸ºè¯†åˆ«å¹»è§‰æ™ºèƒ½ä½“æä¾›çœŸå®ä¾æ®ï¼Œå®ç°äº†å¯é ã€ä¼—åŒ…çš„å¤šæ¨¡æ€æ¨ç†ã€‚MUGåœ¨ä¸‰ä¸ªå…³é”®æ–¹é¢æ¨è¿›äº†MADåè®®ï¼šé€šè¿‡åäº‹å®æµ‹è¯•å®ç°äº‹å®éªŒè¯ï¼Œè€Œéä»…ä¾èµ–ç»Ÿè®¡å…±è¯†ï¼›é€šè¿‡åŠ¨æ€ä¿®æ”¹è¯æ®æ¥æºå¼•å…¥äº¤å‰è¯æ®æ¨ç†ï¼Œè€Œéä¾èµ–é™æ€è¾“å…¥ï¼›ä»¥åŠåŸ¹å…»æ™ºèƒ½ä½“çš„ä¸»åŠ¨æ¨ç†èƒ½åŠ›ï¼Œä½¿å…¶å‚ä¸æ¢è®¨è€Œéè¢«åŠ¨å›ç­”é—®é¢˜ã€‚è¿™äº›åˆ›æ–°ä¸ºLLMä¸­çš„å¤šæ¨¡æ€æ¨ç†æä¾›äº†æ›´å¯é ã€æœ‰æ•ˆçš„æ¡†æ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ¨ç†è¿‡ç¨‹ä¸­é¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜æ˜¯å¹»è§‰é—®é¢˜ã€‚</li>
<li>å¤šæ™ºèƒ½ä½“è¾©è®ºï¼ˆMADï¼‰èŒƒå¼è™½ç„¶æä¾›äº†è§£å†³ç­–ç•¥ï¼Œä½†åŸºäºä¸åˆ‡å®é™…çš„å‡è®¾ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„åè®®ï¼Œå³å¤šæ™ºèƒ½ä½“å§åº•æ¸¸æˆï¼ˆMUGï¼‰ï¼Œçµæ„Ÿæ¥è‡ªç¤¾äº¤æ¨ç†æ¸¸æˆã€‚</li>
<li>MUGé‡‡ç”¨å¤šæ¨¡æ€åäº‹å®æµ‹è¯•æ¥è¯†åˆ«å—å¹»è§‰å½±å“çš„æ™ºèƒ½ä½“ã€‚</li>
<li>é€šè¿‡ä¿®æ”¹å‚è€ƒå›¾åƒæ¥å¼•å…¥åäº‹å®è¯æ®å¹¶è¿›è¡Œè§‚å¯ŸéªŒè¯ã€‚</li>
<li>MUGé€šè¿‡äº‹å®éªŒè¯ã€äº¤å‰è¯æ®æ¨ç†å’Œä¸»åŠ¨æ¨ç†æ¨åŠ¨MADåè®®çš„å‘å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.11182">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-28fa06fbd6238a4f04d1b181c39f1610" align="middle">
<img src="https://picx.zhimg.com/v2-ab5c68aa02449a8de2883b0525fbb870" align="middle">
<img src="https://picx.zhimg.com/v2-06df1f9066e75b8ba18fea72a604a05a" align="middle">
<img src="https://picx.zhimg.com/v2-5c0b57196e6c6b02102daab64b423e8f" align="middle">
<img src="https://picx.zhimg.com/v2-f28ce0b00667d8a68fb4d998859d750a" align="middle">
<img src="https://picx.zhimg.com/v2-09e98810a621e201d2328d3e2b569a82" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-11-18/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-11-18/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-18/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-897d803ec990f59bb3cc02d9c3e21d42" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-18  Human-AI collaborative autonomous synthesis with pulsed laser deposition for remote epitaxy
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-11-18
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-17/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-3d57943303de825dfe353d07827e2920" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-17  Normality and the Turing Test
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-11-17
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">33125.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
