<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-18  Intrinsic Dimension Estimation for Radio Galaxy Zoo using Diffusion Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-1938658107a74287dec3bea13510e6c4')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-18
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    18.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    74 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-11-18-æ›´æ–°"><a href="#2025-11-18-æ›´æ–°" class="headerlink" title="2025-11-18 æ›´æ–°"></a>2025-11-18 æ›´æ–°</h1><h2 id="Intrinsic-Dimension-Estimation-for-Radio-Galaxy-Zoo-using-Diffusion-Models"><a href="#Intrinsic-Dimension-Estimation-for-Radio-Galaxy-Zoo-using-Diffusion-Models" class="headerlink" title="Intrinsic Dimension Estimation for Radio Galaxy Zoo using Diffusion Models"></a>Intrinsic Dimension Estimation for Radio Galaxy Zoo using Diffusion Models</h2><p><strong>Authors:Joan Font-Quer Roset, Devina Mohan, Anna Scaife</strong></p>
<p>In this work, we estimate the intrinsic dimension (iD) of the Radio Galaxy Zoo (RGZ) dataset using a score-based diffusion model. We examine how the iD estimates vary as a function of Bayesian neural network (BNN) energy scores, which measure how similar the radio sources are to the MiraBest subset of the RGZ dataset. We find that out-of-distribution sources exhibit higher iD values, and that the overall iD for RGZ exceeds those typically reported for natural image datasets. Furthermore, we analyse how iD varies across Fanaroff-Riley (FR) morphological classes and as a function of the signal-to-noise ratio (SNR). While no relationship is found between FR I and FR II classes, a weak trend toward higher SNR at lower iD. Future work using the RGZ dataset could make use of the relationship between iD and energy scores to quantitatively study and improve the representations learned by various self-supervised learning algorithms.</p>
<blockquote>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨åŸºäºå¾—åˆ†çš„æ‰©æ•£æ¨¡å‹ä¼°è®¡äº†Radio Galaxy Zooï¼ˆRGZï¼‰æ•°æ®é›†çš„å†…è•´ç»´åº¦ï¼ˆiDï¼‰ã€‚æˆ‘ä»¬ç ”ç©¶äº†iDä¼°è®¡å€¼å¦‚ä½•éšç€è´å¶æ–¯ç¥ç»ç½‘ç»œï¼ˆBNNï¼‰èƒ½é‡å¾—åˆ†çš„å˜åŒ–è€Œå˜åŒ–ï¼Œè¯¥èƒ½é‡å¾—åˆ†è¡¡é‡çš„æ˜¯å°„ç”µæºä¸RGZæ•°æ®é›†çš„MiraBestå­é›†ä¹‹é—´çš„ç›¸ä¼¼æ€§ã€‚æˆ‘ä»¬å‘ç°ï¼Œéåˆ†å¸ƒæºè¡¨ç°å‡ºæ›´é«˜çš„iDå€¼ï¼Œä¸”RGZçš„æ•´ä½“iDè¶…è¿‡äº†é€šå¸¸æŠ¥é“çš„è‡ªç„¶å›¾åƒæ•°æ®é›†çš„iDã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜åˆ†æäº†iDåœ¨Fanaroff-Rileyï¼ˆFRï¼‰å½¢æ€ç±»ä¹‹é—´ä»¥åŠéšç€ä¿¡å™ªæ¯”ï¼ˆSNRï¼‰çš„å˜åŒ–æƒ…å†µã€‚è™½ç„¶FR Iå’ŒFR IIç±»ä¹‹é—´æ²¡æœ‰å‘ç°å…³ç³»ï¼Œä½†å­˜åœ¨ä¸€ç§å¾®å¼±çš„è¶‹åŠ¿ï¼Œå³SNRè¾ƒé«˜æ—¶iDè¾ƒä½ã€‚æœªæ¥ä½¿ç”¨RGZæ•°æ®é›†çš„å·¥ä½œå¯ä»¥åˆ©ç”¨iDå’Œèƒ½é‡å¾—åˆ†ä¹‹é—´çš„å…³ç³»ï¼Œå¯¹å„ç§è‡ªç›‘ç£å­¦ä¹ ç®—æ³•å­¦åˆ°çš„è¡¨ç¤ºè¿›è¡Œå®šé‡ç ”ç©¶å¹¶æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.11490v1">PDF</a> 9 pages, 5 figures, 2 tables, submitted to NeurIPS 2025 ML4PS Workshop</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä½¿ç”¨åŸºäºåˆ†æ•°çš„æ‰©æ•£æ¨¡å‹è¯„ä¼°äº†Radio Galaxy Zooï¼ˆRGZï¼‰æ•°æ®é›†çš„å†…è•´ç»´åº¦ï¼ˆiDï¼‰ã€‚ç ”ç©¶å‘ç°ï¼Œé€šè¿‡åŸºäºè´å¶æ–¯ç¥ç»ç½‘ç»œï¼ˆBNNï¼‰çš„èƒ½é‡åˆ†æ•°ï¼Œå¯ä»¥è¡¡é‡æ— çº¿ç”µèµ„æºä¸RGZæ•°æ®é›†çš„MiraBestå­é›†ä¹‹é—´çš„ç›¸ä¼¼æ€§ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå¼‚å¸¸å€¼æ¥æºå…·æœ‰æ›´é«˜çš„iDå€¼ï¼Œä¸”RGZçš„æ€»ä½“iDå€¼é«˜äºé€šå¸¸æŠ¥é“çš„è‡ªç„¶å›¾åƒæ•°æ®é›†çš„å€¼ã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜åˆ†æäº†iDå¦‚ä½•éšFanaroff-Rileyï¼ˆFRï¼‰å½¢æ€åˆ†ç±»å’Œä¿¡å™ªæ¯”ï¼ˆSNRï¼‰è€Œå˜åŒ–ã€‚å°½ç®¡FR Iå’ŒFR IIç±»ä¹‹é—´æ²¡æœ‰å‘ç°å…³ç³»ï¼Œä½†å­˜åœ¨å¾®å¼±çš„è¶‹åŠ¿ï¼Œå³SNRè¾ƒé«˜æ—¶iDè¾ƒä½ã€‚æœªæ¥å¯ä»¥åˆ©ç”¨iDå’Œèƒ½é‡åˆ†æ•°ä¹‹é—´çš„å…³ç³»æ¥å®šé‡ç ”ç©¶å’Œæ”¹è¿›å„ç§è‡ªç›‘ç£å­¦ä¹ ç®—æ³•çš„è¡¨ç¤ºå­¦ä¹ ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä½¿ç”¨æ‰©æ•£æ¨¡å‹ä¼°è®¡äº†Radio Galaxy Zooï¼ˆRGZï¼‰æ•°æ®é›†çš„å†…è•´ç»´åº¦ï¼ˆiDï¼‰ã€‚</li>
<li>é€šè¿‡è´å¶æ–¯ç¥ç»ç½‘ç»œï¼ˆBNNï¼‰çš„èƒ½é‡åˆ†æ•°æ¥è¡¡é‡æ— çº¿ç”µèµ„æºä¸æ•°æ®é›†çš„ç›¸ä¼¼æ€§ã€‚</li>
<li>å‘ç°å¼‚å¸¸å€¼æ¥æºå…·æœ‰æ›´é«˜çš„iDå€¼ã€‚</li>
<li>RGZæ•°æ®é›†çš„æ€»ä½“iDå€¼é«˜äºè‡ªç„¶å›¾åƒæ•°æ®é›†ã€‚</li>
<li>iDéšFanaroff-Rileyï¼ˆFRï¼‰å½¢æ€åˆ†ç±»å˜åŒ–çš„åˆ†ææœªå‘ç°æ˜ç¡®å…³ç³»ã€‚</li>
<li>å­˜åœ¨ä¿¡å™ªæ¯”ï¼ˆSNRï¼‰ä¸iDä¹‹é—´çš„å¾®å¼±è¶‹åŠ¿ï¼Œå³SNRè¾ƒé«˜æ—¶iDè¾ƒä½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.11490">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-aa4f255c7d667f65fd8d9d0c30a3d64a" align="middle">
<img src="https://picx.zhimg.com/v2-0df274a7f80348f873a2e68e3b01f6fa" align="middle">
<img src="https://picx.zhimg.com/v2-50aba965f0af9899f08ed3e6834dff71" align="middle">
<img src="https://picx.zhimg.com/v2-85880237007438f9decf587a3db91699" align="middle">
<img src="https://picx.zhimg.com/v2-8d3a319f5e181a439de90c29dae23b2d" align="middle">
<img src="https://picx.zhimg.com/v2-eb825442b01b666f21f3b8ba7013c479" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Hi-DREAM-Brain-Inspired-Hierarchical-Diffusion-for-fMRI-Reconstruction-via-ROI-Encoder-and-visuAl-Mapping"><a href="#Hi-DREAM-Brain-Inspired-Hierarchical-Diffusion-for-fMRI-Reconstruction-via-ROI-Encoder-and-visuAl-Mapping" class="headerlink" title="Hi-DREAM: Brain Inspired Hierarchical Diffusion for fMRI Reconstruction via ROI Encoder and visuAl Mapping"></a>Hi-DREAM: Brain Inspired Hierarchical Diffusion for fMRI Reconstruction via ROI Encoder and visuAl Mapping</h2><p><strong>Authors:Guowei Zhang, Yun Zhao, Moein Khajehnejad, Adeel Razi, Levin Kuhlmann</strong></p>
<p>Mapping human brain activity to natural images offers a new window into vision and cognition, yet current diffusion-based decoders face a core difficulty: most condition directly on fMRI features without analyzing how visual information is organized across the cortex. This overlooks the brainâ€™s hierarchical processing and blurs the roles of early, middle, and late visual areas. We propose Hi-DREAM, a brain-inspired conditional diffusion framework that makes the cortical organization explicit. A region-of-interest (ROI) adapter groups fMRI into early&#x2F;mid&#x2F;late streams and converts them into a multi-scale cortical pyramid aligned with the U-Net depth (shallow scales preserve layout and edges; deeper scales emphasize objects and semantics). A lightweight, depth-matched ControlNet injects these scale-specific hints during denoising. The result is an efficient and interpretable decoder in which each signal plays a brain-like role, allowing the model not only to reconstruct images but also to illuminate functional contributions of different visual areas. Experiments on the Natural Scenes Dataset (NSD) show that Hi-DREAM attains state-of-the-art performance on high-level semantic metrics while maintaining competitive low-level fidelity. These findings suggest that structuring conditioning by cortical hierarchy is a powerful alternative to purely data-driven embeddings and provides a useful lens for studying the visual cortex.</p>
<blockquote>
<p>å°†äººç±»å¤§è„‘æ´»åŠ¨ä¸è‡ªç„¶å›¾åƒè¿›è¡Œæ˜ å°„ä¸ºç†è§£å’Œè§†è§‰è®¤çŸ¥æ‰“å¼€äº†ä¸€ä¸ªæ–°çª—å£ã€‚ç„¶è€Œï¼Œå½“å‰çš„åŸºäºæ‰©æ•£çš„è§£ç å™¨é¢ä¸´ä¸€ä¸ªæ ¸å¿ƒéš¾é¢˜ï¼šå¤§å¤šæ•°è§£ç å™¨ç›´æ¥å¯¹åŠŸèƒ½ç£å…±æŒ¯æˆåƒï¼ˆfMRIï¼‰ç‰¹å¾è¿›è¡Œæ¡ä»¶å¤„ç†ï¼Œè€Œæ²¡æœ‰åˆ†æè§†è§‰ä¿¡æ¯åœ¨å¤§è„‘çš®å±‚ä¸Šçš„ç»„ç»‡æ–¹å¼ã€‚è¿™å¿½ç•¥äº†å¤§è„‘çš„åˆ†å±‚å¤„ç†è¿‡ç¨‹ï¼Œå¹¶æ¨¡ç³Šäº†æ—©æœŸã€ä¸­æœŸå’Œæ™šæœŸè§†è§‰åŒºåŸŸçš„è§’è‰²ã€‚æˆ‘ä»¬æå‡ºäº†Hi-DREAMï¼Œè¿™æ˜¯ä¸€ä¸ªå—å¤§è„‘å¯å‘çš„æ¡ä»¶æ‰©æ•£æ¡†æ¶ï¼Œå®ƒæ˜ç¡®äº†çš®å±‚çš„ç»„ç»‡ã€‚æ„Ÿå…´è¶£åŒºåŸŸï¼ˆROIï¼‰é€‚é…å™¨å°†fMRIæ•°æ®åˆ†ä¸ºæ—©æœŸ&#x2F;ä¸­æœŸ&#x2F;æ™šæœŸæµï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºä¸U-Netæ·±åº¦ç›¸åŒ¹é…çš„å¤šå°ºåº¦çš®å±‚é‡‘å­—å¡”ï¼ˆæµ…å±‚ä¿ç•™å¸ƒå±€å’Œè¾¹ç¼˜ï¼›æ·±å±‚åˆ™å¼ºè°ƒå¯¹è±¡å’Œè¯­ä¹‰ï¼‰ã€‚ä¸€ä¸ªè½»ä¾¿çš„ã€æ·±åº¦åŒ¹é…çš„ControlNetåœ¨é™å™ªè¿‡ç¨‹ä¸­æ³¨å…¥äº†è¿™äº›å°ºåº¦ç‰¹å®šçš„æç¤ºã€‚ç»“æœæ˜¯ä¸€ä¸ªé«˜æ•ˆä¸”å¯è§£é‡Šçš„è§£ç å™¨ï¼Œå…¶ä¸­æ¯ä¸ªä¿¡å·éƒ½æ‰®æ¼”ç€ç±»ä¼¼å¤§è„‘çš„è§’è‰²ï¼Œä½¿æ¨¡å‹ä¸ä»…èƒ½å¤Ÿé‡å»ºå›¾åƒï¼Œè¿˜èƒ½å¤Ÿé˜æ˜ä¸åŒè§†è§‰åŒºåŸŸçš„èŒèƒ½è´¡çŒ®ã€‚åœ¨è‡ªç„¶åœºæ™¯æ•°æ®é›†ï¼ˆNSDï¼‰ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒHi-DREAMåœ¨é«˜å±‚æ¬¡è¯­ä¹‰æŒ‡æ ‡ä¸Šè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼ŒåŒæ—¶åœ¨ä½å±‚æ¬¡ä¿çœŸåº¦ä¸Šä¿æŒç«äº‰åŠ›ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œé€šè¿‡çš®å±‚ç»“æ„è¿›è¡Œæ¡ä»¶ç»“æ„åŒ–æ˜¯ä¸€ç§å¼ºå¤§çš„æ›¿ä»£æ–¹æ³•ï¼Œç›¸å¯¹äºçº¯ç²¹çš„æ•°æ®é©±åŠ¨åµŒå…¥ï¼Œå®ƒæä¾›äº†ä¸€ä¸ªç ”ç©¶è§†è§‰çš®å±‚çš„æœ‰ç”¨è§†è§’ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.11437v1">PDF</a> </p>
<p><strong>Summary</strong><br>     å¤§è„‘æ´»åŠ¨ä¸è‡ªç„¶å›¾åƒæ˜ å°„ä¸ºç ”ç©¶è§†è§‰ä¸è®¤çŸ¥æä¾›äº†æ–°çš„è§†è§’ï¼Œä½†ç°æœ‰åŸºäºæ‰©æ•£çš„è§£ç å™¨é¢ä¸´æ ¸å¿ƒéš¾é¢˜ï¼šå®ƒä»¬å¤§å¤šç›´æ¥æ ¹æ®fMRIç‰¹å¾è¿›è¡Œè§£ç ï¼Œå¿½ç•¥äº†è§†è§‰ä¿¡æ¯åœ¨å¤§è„‘çš®å±‚çš„ç»„ç»‡æ–¹å¼ã€‚ç ”ç©¶å›¢é˜Ÿæå‡ºäº†Hi-DREAMæ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ä¸ªå—å¤§è„‘å¯å‘çš„æ¡ä»¶æ‰©æ•£æ¡†æ¶ï¼Œæ˜ç¡®äº†çš®å±‚çš„ç»„ç»‡ã€‚è¯¥æ¨¡å‹é€šè¿‡æ„Ÿå…´è¶£åŒºåŸŸé€‚é…å™¨å°†fMRIåˆ†ä¸ºæ—©æœŸã€ä¸­æœŸå’Œæ™šæœŸä¿¡æ¯æµï¼Œå¹¶è½¬åŒ–ä¸ºä¸U-Netæ·±åº¦ç›¸åŒ¹é…çš„å¤šå°ºåº¦çš®å±‚é‡‘å­—å¡”ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒHi-DREAMåœ¨é«˜å±‚æ¬¡è¯­ä¹‰æŒ‡æ ‡ä¸Šè¾¾åˆ°æœ€ä½³æ€§èƒ½ï¼ŒåŒæ—¶åœ¨ä½å±‚æ¬¡ä¿çœŸåº¦ä¸Šä¿æŒç«äº‰åŠ›ã€‚è¿™æ˜¾ç¤ºå°†æ¡ä»¶ç»“æ„åŒ–æŒ‰çš®å±‚ç­‰çº§æ’åˆ—æ˜¯å¼ºå¤§çš„æ›¿ä»£æ–¹æ³•ï¼Œæœ‰åŠ©äºç ”ç©¶è§†è§‰çš®å±‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ˜ å°„å¤§è„‘æ´»åŠ¨ä¸è‡ªç„¶å›¾åƒä¸ºç†è§£è§†è§‰å’Œè®¤çŸ¥æä¾›äº†æ–°çš„è§†è§’ã€‚</li>
<li>å½“å‰æ‰©æ•£è§£ç å™¨å¤§å¤šç›´æ¥åŸºäºfMRIç‰¹å¾è¿›è¡Œè§£ç ï¼Œå¿½ç•¥äº†å¤§è„‘çš®å±‚çš„è§†è§‰ä¿¡æ¯ç»„ç»‡æ–¹å¼ã€‚</li>
<li>Hi-DREAMæ¨¡å‹æ˜¯ä¸€ä¸ªå—å¤§è„‘å¯å‘çš„æ¡ä»¶æ‰©æ•£æ¡†æ¶ï¼Œæ—¨åœ¨æ˜ç¡®çš®å±‚çš„ç»„ç»‡ã€‚</li>
<li>Hi-DREAMæ¨¡å‹é€šè¿‡ROIé€‚é…å™¨å°†fMRIåˆ†ä¸ºæ—©æœŸã€ä¸­æœŸå’Œæ™šæœŸä¿¡æ¯æµã€‚</li>
<li>Hi-DREAMæ¨¡å‹å°†fMRIä¿¡æ¯è½¬åŒ–ä¸ºå¤šå°ºåº¦çš®å±‚é‡‘å­—å¡”ï¼Œä¸U-Netæ·±åº¦ç›¸åŒ¹é…ã€‚</li>
<li>Hi-DREAMæ¨¡å‹å®éªŒç»“æœè¾¾åˆ°æœ€ä½³æ€§èƒ½æ ‡å‡†ï¼Œå°¤å…¶æ˜¯åœ¨é«˜å±‚æ¬¡è¯­ä¹‰æŒ‡æ ‡ä¸Šè¡¨ç°çªå‡ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.11437">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-44bdc1f7748c9f3e9df540c150743215" align="middle">
<img src="https://picx.zhimg.com/v2-baca6fab1b699735be5bbd3995ad4a3d" align="middle">
<img src="https://picx.zhimg.com/v2-a999b8c3eb8b34beb9aa4541460505bb" align="middle">
<img src="https://picx.zhimg.com/v2-035d7a4773e36a847b627588bcdcebaa" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="CountSteer-Steering-Attention-for-Object-Counting-in-Diffusion-Models"><a href="#CountSteer-Steering-Attention-for-Object-Counting-in-Diffusion-Models" class="headerlink" title="CountSteer: Steering Attention for Object Counting in Diffusion Models"></a>CountSteer: Steering Attention for Object Counting in Diffusion Models</h2><p><strong>Authors:Hyemin Boo, Hyoryung Kim, Myungjin Lee, Seunghyeon Lee, Jiyoung Lee, Jang-Hwan Choi, Hyunsoo Cho</strong></p>
<p>Text-to-image diffusion models generate realistic and coherent images but often fail to follow numerical instructions in text, revealing a gap between language and visual representation. Interestingly, we found that these models are not entirely blind to numbers-they are implicitly aware of their own counting accuracy, as their internal signals shift in consistent ways depending on whether the output meets the specified count. This observation suggests that the model already encodes a latent notion of numerical correctness, which can be harnessed to guide generation more precisely. Building on this intuition, we introduce CountSteer, a training-free method that improves generation of specified object counts by steering the modelâ€™s cross-attention hidden states during inference. In our experiments, CountSteer improved object-count accuracy by about 4% without compromising visual quality, demonstrating a simple yet effective step toward more controllable and semantically reliable text-to-image generation.</p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆé€¼çœŸä¸”è¿è´¯çš„å›¾åƒï¼Œä½†å®ƒä»¬å¾€å¾€æ— æ³•éµå¾ªæ–‡æœ¬ä¸­çš„æ•°å­—æŒ‡ä»¤ï¼Œè¿™æ­ç¤ºäº†è¯­è¨€ä¸è§†è§‰è¡¨ç¤ºä¹‹é—´çš„é¸¿æ²Ÿã€‚æœ‰è¶£çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç°è¿™äº›æ¨¡å‹å¹¶éå®Œå…¨å¿½è§†æ•°å­—â€”â€”å®ƒä»¬å¯¹è‡ªèº«çš„è®¡æ•°å‡†ç¡®æ€§æœ‰éšå«æ„è¯†ï¼Œå› ä¸ºå®ƒä»¬çš„å†…éƒ¨ä¿¡å·ä¼šæ ¹æ®è¾“å‡ºæ˜¯å¦æ»¡è¶³æŒ‡å®šè®¡æ•°è€Œå‘ˆç°å‡ºä¸€è‡´çš„å˜åŒ–ã€‚è¿™ä¸€è§‚å¯Ÿè¡¨æ˜ï¼Œæ¨¡å‹å·²ç»ç¼–ç äº†æ½œåœ¨çš„æ•°å€¼æ­£ç¡®æ€§æ¦‚å¿µï¼Œå¯ä»¥åŠ ä»¥åˆ©ç”¨ä»¥æ›´ç²¾ç¡®åœ°å¼•å¯¼ç”Ÿæˆã€‚åŸºäºè¿™ä¸€ç›´è§‰ï¼Œæˆ‘ä»¬å¼•å…¥äº†CountSteerï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„æ–¹æ³•ï¼Œé€šè¿‡å¼•å¯¼æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­çš„äº¤å‰æ³¨æ„éšè—çŠ¶æ€ï¼Œæé«˜æŒ‡å®šå¯¹è±¡è®¡æ•°çš„ç”Ÿæˆã€‚åœ¨æˆ‘ä»¬çš„å®éªŒä¸­ï¼ŒCountSteeråœ¨æé«˜å¯¹è±¡è®¡æ•°å‡†ç¡®æ€§çš„åŒæ—¶ï¼Œæ²¡æœ‰æŸå®³è§†è§‰è´¨é‡ï¼Œè¿™è¯æ˜äº†æ˜¯ä¸€ä¸ªç®€å•è€Œæœ‰æ•ˆçš„æ­¥éª¤ï¼Œæœç€æ›´å¯æ§å’Œè¯­ä¹‰ä¸Šæ›´å¯é çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ–¹å‘å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.11253v1">PDF</a> Accepted to AAAI 2026 Workshop on Shaping Responsible Synthetic Data in the Era of Foundation Models (RSD)</p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹å¯ä»¥ç”Ÿæˆé€¼çœŸä¸”è¿è´¯çš„å›¾åƒï¼Œä½†åœ¨éµå¾ªæ–‡æœ¬ä¸­çš„æ•°å­—æŒ‡ä»¤æ–¹é¢å¸¸å¸¸å‡ºç°é—®é¢˜ï¼Œè¿™æ˜¾ç¤ºå‡ºè¯­è¨€ä¸è§†è§‰è¡¨ç¤ºä¹‹é—´å­˜åœ¨å·®è·ã€‚ç ”ç©¶å‘ç°ï¼Œè¿™äº›æ¨¡å‹å¹¶éå®Œå…¨å¿½è§†æ•°å­—ï¼Œå®ƒä»¬å¯¹è®¡æ•°å‡†ç¡®æ€§æœ‰éšæ€§è®¤çŸ¥ã€‚æ¨¡å‹è¾“å‡ºç¬¦åˆè®¡æ•°æ—¶ï¼Œå…¶å†…éƒ¨ä¿¡å·ä¼šæœ‰è§„å¾‹åœ°å˜åŒ–ã€‚åŸºäºæ­¤è§‚å¯Ÿï¼Œå¼•å…¥äº†ä¸€ç§æ— éœ€è®­ç»ƒçš„æ–¹æ³•CountSteerï¼Œé€šè¿‡å¼•å¯¼æ¨¡å‹åœ¨æ¨ç†é˜¶æ®µçš„äº¤å‰æ³¨æ„åŠ›éšè—çŠ¶æ€ï¼Œæé«˜ç‰¹å®šå¯¹è±¡è®¡æ•°çš„ç”Ÿæˆå‡†ç¡®æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒCountSteeråœ¨ä¸æŸå®³è§†è§‰è´¨é‡çš„æƒ…å†µä¸‹æé«˜äº†çº¦4%çš„å¯¹è±¡è®¡æ•°å‡†ç¡®æ€§ï¼Œæ˜¯æœç€æ›´å¯æ§å’Œè¯­ä¹‰æ›´å¯é çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ–¹å‘è¿ˆå‡ºçš„ç®€å•è€Œæœ‰æ•ˆçš„ä¸€æ­¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆå›¾åƒæ—¶èƒ½å¤Ÿéµå¾ªæ–‡æœ¬æè¿°ï¼Œä½†å¤„ç†åŒ…å«æ•°å­—çš„æŒ‡ä»¤æ—¶å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>æ¨¡å‹å¹¶éå®Œå…¨å¿½è§†æ•°å­—ï¼Œè€Œæ˜¯å¯¹è®¡æ•°å‡†ç¡®æ€§æœ‰éšæ€§è®¤çŸ¥ã€‚</li>
<li>æ¨¡å‹å†…éƒ¨ä¿¡å·ä¼šæ ¹æ®è¾“å‡ºæ˜¯å¦ç¬¦åˆè®¡æ•°è¦æ±‚è€Œå‘ç”Ÿå˜åŒ–ã€‚</li>
<li>CountSteeræ–¹æ³•é€šè¿‡å¼•å¯¼æ¨¡å‹çš„äº¤å‰æ³¨æ„åŠ›éšè—çŠ¶æ€ï¼Œåœ¨ä¸æŸå®³è§†è§‰è´¨é‡çš„å‰æä¸‹æé«˜äº†å¯¹è±¡è®¡æ•°çš„å‡†ç¡®æ€§ã€‚</li>
<li>CountSteeræ˜¯ä¸€ç§æ— éœ€é¢å¤–è®­ç»ƒçš„æœ‰æ•ˆæ–¹æ³•ã€‚</li>
<li>CountSteeræ–¹æ³•æé«˜äº†æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„å¯æ§æ€§å’Œè¯­ä¹‰å¯é æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.11253">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d0bfb02e54ccda2d069f61b5e5c93fd5" align="middle">
<img src="https://picx.zhimg.com/v2-d94be2e50921dca7552aedcafd84ebb7" align="middle">
<img src="https://picx.zhimg.com/v2-051c786824f4967cf07b990861b19dfe" align="middle">
<img src="https://picx.zhimg.com/v2-118d57ff9e09fd2ec31d02436e176144" align="middle">
<img src="https://picx.zhimg.com/v2-8cdc58c99854aa2510859d1fb7fc155c" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Parameter-Efficient-MoE-LoRA-for-Few-Shot-Multi-Style-Editing"><a href="#Parameter-Efficient-MoE-LoRA-for-Few-Shot-Multi-Style-Editing" class="headerlink" title="Parameter-Efficient MoE LoRA for Few-Shot Multi-Style Editing"></a>Parameter-Efficient MoE LoRA for Few-Shot Multi-Style Editing</h2><p><strong>Authors:Cong Cao, Yujie Xu, Xiaodong Xu</strong></p>
<p>In recent years, image editing has garnered growing attention. However, general image editing models often fail to produce satisfactory results when confronted with new styles. The challenge lies in how to effectively fine-tune general image editing models to new styles using only a limited amount of paired data. To address this issue, this paper proposes a novel few-shot style editing framework. For this task, we construct a benchmark dataset that encompasses five distinct styles. Correspondingly, we propose a parameter-efficient multi-style Mixture-of-Experts Low-Rank Adaptation (MoE LoRA) with style-specific and style-shared routing mechanisms for jointly fine-tuning multiple styles. The style-specific routing ensures that different styles do not interfere with one another, while the style-shared routing adaptively allocates shared MoE LoRAs to learn common patterns. Our MoE LoRA can automatically determine the optimal ranks for each layer through a novel metric-guided approach that estimates the importance score of each single-rank component. Additionally, we explore the optimal location to insert LoRA within the Diffusion in Transformer (DiT) model and integrate adversarial learning and flow matching to guide the diffusion training process. Experimental results demonstrate that our proposed method outperforms existing state-of-the-art approaches with significantly fewer LoRA parameters.</p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œå›¾åƒç¼–è¾‘é¢†åŸŸå¼•èµ·äº†è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ã€‚ç„¶è€Œï¼Œå½“é¢å¯¹æ–°çš„é£æ ¼æ—¶ï¼Œé€šç”¨çš„å›¾åƒç¼–è¾‘æ¨¡å‹å¾€å¾€æ— æ³•äº§ç”Ÿä»¤äººæ»¡æ„çš„ç»“æœã€‚æŒ‘æˆ˜åœ¨äºå¦‚ä½•åˆ©ç”¨æœ‰é™çš„é…å¯¹æ•°æ®æœ‰æ•ˆåœ°å¾®è°ƒé€šç”¨å›¾åƒç¼–è¾‘æ¨¡å‹ä»¥é€‚åº”æ–°çš„é£æ ¼ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„å°‘æ ·æœ¬é£æ ¼ç¼–è¾‘æ¡†æ¶ã€‚ä¸ºäº†å®Œæˆæ­¤ä»»åŠ¡ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåŒ…å«äº”ç§ä¸åŒé£æ ¼çš„åŸºå‡†æ•°æ®é›†ã€‚ç›¸åº”åœ°ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å‚æ•°é«˜æ•ˆçš„å¤šé£æ ¼æ··åˆä¸“å®¶ä½ç§©é€‚åº”ï¼ˆMoE LoRAï¼‰æ–¹æ³•ï¼Œå…¶ä¸­åŒ…å«é£æ ¼ç‰¹å®šå’Œé£æ ¼å…±äº«è·¯ç”±æœºåˆ¶ï¼Œä»¥è”åˆå¾®è°ƒå¤šç§é£æ ¼ã€‚é£æ ¼ç‰¹å®šè·¯ç”±ç¡®ä¿ä¸åŒçš„é£æ ¼ä¸ä¼šç›¸äº’å¹²æ‰°ï¼Œè€Œé£æ ¼å…±äº«è·¯ç”±è‡ªé€‚åº”åœ°åˆ†é…å…±äº«MoE LoRAsæ¥å­¦ä¹ é€šç”¨æ¨¡å¼ã€‚æˆ‘ä»¬çš„MoE LoRAå¯ä»¥é€šè¿‡ä¸€ç§æ–°çš„åº¦é‡æŒ‡å¯¼æ–¹æ³•è‡ªåŠ¨ç¡®å®šæ¯å±‚çš„æœ€ä½³ç§©ï¼Œè¯¥æ–¹æ³•ä¼°è®¡æ¯ä¸ªå•ç§©ç»„ä»¶çš„é‡è¦æ€§å¾—åˆ†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ¢ç´¢äº†åœ¨æ‰©æ•£å˜å‹å™¨ï¼ˆDiTï¼‰æ¨¡å‹ä¸­æ’å…¥LoRAçš„æœ€ä½³ä½ç½®ï¼Œå¹¶é›†æˆäº†å¯¹æŠ—å­¦ä¹ å’ŒæµåŒ¹é…æ¥å¼•å¯¼æ‰©æ•£è®­ç»ƒè¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•åœ¨è¾ƒå°‘çš„LoRAå‚æ•°ä¸‹è¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.11236v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹å›¾åƒç¼–è¾‘çš„æ–°æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³é€šç”¨å›¾åƒç¼–è¾‘æ¨¡å‹åœ¨é¢å¯¹æ–°é£æ ¼æ—¶è¡¨ç°ä¸ä½³çš„é—®é¢˜ã€‚æ–‡ç« æ„å»ºäº†åŒ…å«äº”ç§ä¸åŒé£æ ¼çš„åŸºå‡†æ•°æ®é›†ï¼Œå¹¶æå‡ºäº†ä¸€ç§å‚æ•°é«˜æ•ˆçš„æ··åˆä¸“å®¶ä½ç§©é€‚åº”ï¼ˆMoE LoRAï¼‰æ–¹æ³•ï¼Œé€šè¿‡é£æ ¼ç‰¹å®šå’Œé£æ ¼å…±äº«çš„è·¯ç”±æœºåˆ¶è”åˆå¾®è°ƒå¤šç§é£æ ¼ã€‚MoE LoRAå¯è‡ªåŠ¨ç¡®å®šæ¯å±‚çš„æœ€ä½³ç§©ï¼Œé€šè¿‡ä¸€ç§æ–°çš„åº¦é‡å¼•å¯¼æ–¹æ³•ä¼°è®¡æ¯ä¸ªå•ç§©ç»„ä»¶çš„é‡è¦æ€§å¾—åˆ†ã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜æ¢è®¨äº†å°†LoRAæ’å…¥æ‰©æ•£æ¨¡å‹çš„æœ€ä¼˜ä½ç½®ï¼Œå¹¶é›†æˆäº†å¯¹æŠ—æ€§å­¦ä¹ å’ŒæµåŒ¹é…æ¥æŒ‡å¯¼æ‰©æ•£è®­ç»ƒè¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„å›¾åƒç¼–è¾‘æŠ€æœ¯ï¼Œä¸”LoRAå‚æ•°æ›´å°‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§æ–°çš„å›¾åƒç¼–è¾‘æ¡†æ¶ä»¥è§£å†³é€šç”¨æ¨¡å‹é¢å¯¹æ–°é£æ ¼æ—¶çš„æŒ‘æˆ˜ã€‚</li>
<li>æ„å»ºäº†ä¸€ä¸ªåŒ…å«äº”ç§ä¸åŒé£æ ¼çš„åŸºå‡†æ•°æ®é›†ï¼Œç”¨äºè¯„ä¼°å’Œæµ‹è¯•æ–°æ¡†æ¶çš„æ€§èƒ½ã€‚</li>
<li>æå‡ºäº†ä¸€ç§å‚æ•°é«˜æ•ˆçš„æ··åˆä¸“å®¶ä½ç§©é€‚åº”ï¼ˆMoE LoRAï¼‰æ–¹æ³•ï¼Œç”¨äºå¾®è°ƒå¤šç§é£æ ¼ã€‚</li>
<li>MoE LoRAå…·æœ‰é£æ ¼ç‰¹å®šå’Œé£æ ¼å…±äº«çš„è·¯ç”±æœºåˆ¶ï¼Œç¡®ä¿ä¸åŒé£æ ¼ä¸ä¼šç›¸äº’å¹²æ‰°ã€‚</li>
<li>MoE LoRAå¯è‡ªåŠ¨ç¡®å®šæ¯å±‚çš„æœ€ä½³ç§©ï¼Œé€šè¿‡åº¦é‡å¼•å¯¼æ–¹æ³•ä¼°è®¡æ¯ä¸ªå•ç§©ç»„ä»¶çš„é‡è¦æ€§ã€‚</li>
<li>æ¢è®¨äº†å°†LoRAæ’å…¥æ‰©æ•£æ¨¡å‹çš„æœ€ä¼˜ä½ç½®ï¼Œä»¥æé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.11236">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9338992f74497d285368a910c03581fe" align="middle">
<img src="https://picx.zhimg.com/v2-5f3daa7682d2e441d6c18ac75f7d072a" align="middle">
<img src="https://picx.zhimg.com/v2-e22f604f719f1c12ccde69086c56aa1c" align="middle">
<img src="https://picx.zhimg.com/v2-e5116f5fabc508293cef4a7aeb1a63f9" align="middle">
<img src="https://picx.zhimg.com/v2-c9185fcb1eb8e0a3e78c9a714d0b9c6d" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="3D-Gaussian-and-Diffusion-Based-Gaze-Redirection"><a href="#3D-Gaussian-and-Diffusion-Based-Gaze-Redirection" class="headerlink" title="3D Gaussian and Diffusion-Based Gaze Redirection"></a>3D Gaussian and Diffusion-Based Gaze Redirection</h2><p><strong>Authors:Abiram Panchalingam, Indu Bodala, Stuart Middleton</strong></p>
<p>High-fidelity gaze redirection is critical for generating augmented data to improve the generalization of gaze estimators. 3D Gaussian Splatting (3DGS) models like GazeGaussian represent the state-of-the-art but can struggle with rendering subtle, continuous gaze shifts. In this paper, we propose DiT-Gaze, a framework that enhances 3D gaze redirection models using a novel combination of Diffusion Transformer (DiT), weak supervision across gaze angles, and an orthogonality constraint loss. DiT allows higher-fidelity image synthesis, while our weak supervision strategy using synthetically generated intermediate gaze angles provides a smooth manifold of gaze directions during training. The orthogonality constraint loss mathematically enforces the disentanglement of internal representations for gaze, head pose, and expression. Comprehensive experiments show that DiT-Gaze sets a new state-of-the-art in both perceptual quality and redirection accuracy, reducing the state-of-the-art gaze error by 4.1% to 6.353 degrees, providing a superior method for creating synthetic training data. Our code and models will be made available for the research community to benchmark against.</p>
<blockquote>
<p>é«˜ä¿çœŸåº¦çš„è§†çº¿é‡å®šå‘å¯¹äºç”Ÿæˆå¢å¼ºæ•°æ®ä»¥æ”¹å–„è§†çº¿ä¼°è®¡å™¨çš„æ³›åŒ–èƒ½åŠ›è‡³å…³é‡è¦ã€‚è™½ç„¶åƒGazeGaussianè¿™æ ·çš„3Dé«˜æ–¯æ‹¼æ¥ï¼ˆ3DGSï¼‰æ¨¡å‹ä»£è¡¨äº†å½“å‰æŠ€æœ¯çš„å‰æ²¿ï¼Œä½†åœ¨å‘ˆç°å¾®å¦™ã€è¿ç»­çš„è§†çº¿è½¬ç§»æ—¶å¯èƒ½ä¼šé‡åˆ°å›°éš¾ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†DiT-Gazeæ¡†æ¶ï¼Œå®ƒé€šè¿‡ç»“åˆæ‰©æ•£å˜å‹å™¨ï¼ˆDiTï¼‰ã€å„è§’åº¦çš„å¼±ç›‘ç£ä»¥åŠæ­£äº¤çº¦æŸæŸå¤±ï¼Œå¢å¼ºäº†3Dè§†çº¿é‡å®šå‘æ¨¡å‹çš„æ€§èƒ½ã€‚DiTå…è®¸æ›´é«˜ä¿çœŸåº¦çš„å›¾åƒåˆæˆï¼Œè€Œæˆ‘ä»¬ä½¿ç”¨åˆæˆä¸­é—´è§†çº¿è§’åº¦çš„å¼±ç›‘ç£ç­–ç•¥ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æä¾›äº†å¹³æ»‘çš„è§†çº¿æ–¹å‘æµå½¢ã€‚æ­£äº¤çº¦æŸæŸå¤±ä»æ•°å­¦ä¸Šå¼ºåˆ¶å®ç°è§†çº¿ã€å¤´éƒ¨å§¿åŠ¿å’Œè¡¨æƒ…å†…éƒ¨è¡¨ç¤ºçš„å»è€¦åˆã€‚ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒDiT-Gazeåœ¨æ„ŸçŸ¥è´¨é‡å’Œé‡å®šå‘å‡†ç¡®æ€§æ–¹é¢å‡è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œå°†æœ€å…ˆè¿›çš„è§†çº¿è¯¯å·®é™ä½äº†4.1%ï¼Œè‡³6.353åº¦ï¼Œä¸ºåˆ›å»ºåˆæˆè®­ç»ƒæ•°æ®æä¾›äº†ä¸€ç§ä¼˜è¶Šçš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ¨¡å‹å°†å‘ç ”ç©¶ç¤¾åŒºå¼€æ”¾ï¼Œä»¥ä¾›åŸºå‡†æµ‹è¯•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.11231v1">PDF</a> </p>
<p><strong>Summary</strong><br>æ–°ä¸€ä»£çš„çœ¼ç¥æ•æ‰æŠ€æœ¯DiT-Gazeé€šè¿‡ç»“åˆDiffusion Transformerï¼ˆDiTï¼‰ã€å¼±ç›‘ç£ç­–ç•¥ä»¥åŠæ­£äº¤çº¦æŸæŸå¤±ï¼Œæé«˜äº†é«˜ä¿çœŸçœ¼ç¥é‡å®šå‘çš„èƒ½åŠ›ï¼Œä»è€Œç”Ÿæˆæ›´é€¼çœŸçš„åˆæˆæ•°æ®ç”¨äºè®­ç»ƒçœ¼ç¥æ•æ‰æ¨¡å‹ã€‚è¯¥æŠ€æœ¯ä¸ä»…æé«˜äº†æ„ŸçŸ¥è´¨é‡å’Œé‡å®šå‘ç²¾åº¦ï¼Œè¿˜å‡å°‘äº†çœ¼ç¥æ•æ‰è¯¯å·®ã€‚è¯¥æŠ€æœ¯çš„ä»£ç å’Œæ¨¡å‹å°†ä¸ºç ”ç©¶ç¤¾åŒºå…¬å¼€æä¾›ã€‚ </p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DiT-GazeæŠ€æœ¯ç»“åˆäº†Diffusion Transformerï¼ˆDiTï¼‰ä»¥å¢å¼º3Dçœ¼ç¥é‡å®šå‘æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>å¼±ç›‘ç£ç­–ç•¥ç”¨äºåˆæˆä¸­é—´çœ¼ç¥è§’åº¦çš„æ•°æ®ï¼Œä½¿è®­ç»ƒè¿‡ç¨‹ä¸­çš„çœ¼ç¥æ–¹å‘æ›´åŠ å¹³æ»‘ã€‚</li>
<li>æ­£äº¤çº¦æŸæŸå¤±è¢«ç”¨æ¥æ•°å­¦ä¸Šå¼ºåˆ¶åˆ†ç¦»çœ¼ç¥ã€å¤´éƒ¨å§¿åŠ¿å’Œè¡¨æƒ…çš„å†…éƒ¨è¡¨ç¤ºã€‚</li>
<li>DiT-Gazeåœ¨æ„ŸçŸ¥è´¨é‡å’Œé‡å®šå‘ç²¾åº¦ä¸Šè¾¾åˆ°äº†æ–°çš„ä¸šç•Œæ°´å‡†ã€‚</li>
<li>DiT-Gazeå‡å°‘äº†çœ¼ç¥æ•æ‰è¯¯å·®ï¼Œæå‡äº†è‡³å°‘4.1%ï¼Œè¾¾åˆ°6.353åº¦ã€‚</li>
<li>è¯¥æŠ€æœ¯å°†ä¸ºç ”ç©¶ç¤¾åŒºå…¬å¼€å¯ç”¨çš„ä»£ç å’Œæ¨¡å‹ï¼Œä¾¿äºå…¶ä»–ç ”ç©¶è€…è¿›è¡ŒåŸºå‡†æµ‹è¯•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.11231">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-197ceddcb9985ab2ca1f32608490bf58" align="middle">
<img src="https://picx.zhimg.com/v2-75ad3fbc9437b91bf4c15a5cdfec9b1d" align="middle">
<img src="https://picx.zhimg.com/v2-db8db8b9136852d36d4e0319f2bb94d5" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="RealisticDreamer-Guidance-Score-Distillation-for-Few-shot-Gaussian-Splatting"><a href="#RealisticDreamer-Guidance-Score-Distillation-for-Few-shot-Gaussian-Splatting" class="headerlink" title="RealisticDreamer: Guidance Score Distillation for Few-shot Gaussian Splatting"></a>RealisticDreamer: Guidance Score Distillation for Few-shot Gaussian Splatting</h2><p><strong>Authors:Ruocheng Wu, Haolan He, Yufei Wang, Zhihao Li, Bihan Wen</strong></p>
<p>3D Gaussian Splatting (3DGS) has recently gained great attention in the 3D scene representation for its high-quality real-time rendering capabilities. However, when the input comprises sparse training views, 3DGS is prone to overfitting, primarily due to the lack of intermediate-view supervision. Inspired by the recent success of Video Diffusion Models (VDM), we propose a framework called Guidance Score Distillation (GSD) to extract the rich multi-view consistency priors from pretrained VDMs. Building on the insights from Score Distillation Sampling (SDS), GSD supervises rendered images from multiple neighboring views, guiding the Gaussian splatting representation towards the generative direction of VDM. However, the generative direction often involves object motion and random camera trajectories, making it challenging for direct supervision in the optimization process. To address this problem, we introduce an unified guidance form to correct the noise prediction result of VDM. Specifically, we incorporate both a depth warp guidance based on real depth maps and a guidance based on semantic image features, ensuring that the score update direction from VDM aligns with the correct camera pose and accurate geometry. Experimental results show that our method outperforms existing approaches across multiple datasets.</p>
<blockquote>
<p>3Dé«˜æ–¯æç”»ï¼ˆ3DGSï¼‰å› å…¶é«˜è´¨é‡å®æ—¶æ¸²æŸ“èƒ½åŠ›è€Œåœ¨3Dåœºæ™¯è¡¨ç¤ºä¸­å—åˆ°å¹¿æ³›å…³æ³¨ã€‚ç„¶è€Œï¼Œå½“è¾“å…¥åŒ…å«ç¨€ç–è®­ç»ƒè§†å›¾æ—¶ï¼Œ3DGSå®¹æ˜“è¿‡æ‹Ÿåˆï¼Œè¿™ä¸»è¦æ˜¯ç”±äºç¼ºå°‘ä¸­é—´è§†å›¾ç›‘ç£ã€‚å—è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ˆVDMï¼‰è¿‘æœŸæˆåŠŸçš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºæŒ‡å¯¼è¯„åˆ†è’¸é¦ï¼ˆGSDï¼‰çš„æ¡†æ¶ï¼Œç”¨äºä»é¢„è®­ç»ƒçš„VDMsä¸­æå–ä¸°å¯Œçš„å¤šè§†å›¾ä¸€è‡´æ€§å…ˆéªŒã€‚åŸºäºè¯„åˆ†è’¸é¦é‡‡æ ·ï¼ˆSDSï¼‰çš„è§è§£ï¼ŒGSDç›‘ç£æ¥è‡ªå¤šä¸ªç›¸é‚»è§†å›¾çš„æ¸²æŸ“å›¾åƒï¼Œå°†é«˜æ–¯æç”»è¡¨ç¤ºå¼•å‘VDMçš„ç”Ÿæˆæ–¹å‘ã€‚ç„¶è€Œï¼Œç”Ÿæˆæ–¹å‘é€šå¸¸æ¶‰åŠå¯¹è±¡è¿åŠ¨å’Œéšæœºç›¸æœºè½¨è¿¹ï¼Œä½¿å¾—åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­ç›´æ¥ç›‘ç£å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ç»Ÿä¸€çš„æŒ‡å¯¼å½¢å¼æ¥æ ¡æ­£VDMçš„å™ªå£°é¢„æµ‹ç»“æœã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ç»“åˆåŸºäºçœŸå®æ·±åº¦å›¾çš„æ·±åº¦warpæŒ‡å¯¼ä»¥åŠåŸºäºè¯­ä¹‰å›¾åƒç‰¹å¾çš„æŒ‡å¯¼ï¼Œç¡®ä¿VDMçš„è¯„åˆ†æ›´æ–°æ–¹å‘ä¸æ­£ç¡®çš„ç›¸æœºå§¿æ€å’Œå‡†ç¡®çš„å‡ ä½•å¯¹é½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.11213v1">PDF</a> </p>
<p><strong>Summary</strong><br>     3Dé«˜æ–¯å–·ç»˜ï¼ˆ3DGSï¼‰åœ¨ä¸‰ç»´åœºæ™¯è¡¨ç¤ºä¸­å› å…¶é«˜è´¨é‡å®æ—¶æ¸²æŸ“èƒ½åŠ›è€Œå—åˆ°å¹¿æ³›å…³æ³¨ã€‚ç„¶è€Œï¼Œå½“è¾“å…¥åŒ…å«ç¨€ç–è®­ç»ƒè§†å›¾æ—¶ï¼Œ3DGSæ˜“å‡ºç°è¿‡æ‹Ÿåˆï¼Œä¸»è¦æ˜¯ç”±äºç¼ºä¹ä¸­é—´è§†å›¾ç›‘ç£ã€‚å—è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ˆVDMï¼‰æˆåŠŸçš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºGuidance Score Distillationï¼ˆGSDï¼‰çš„æ¡†æ¶ï¼Œä»é¢„è®­ç»ƒçš„VDMä¸­æå–ä¸°å¯Œçš„å¤šè§†å›¾ä¸€è‡´æ€§å…ˆéªŒã€‚GSDå»ºç«‹åœ¨Score Distillation Samplingï¼ˆSDSï¼‰çš„åŸºç¡€ä¸Šï¼Œå¯¹æ¥è‡ªå¤šä¸ªç›¸é‚»è§†å›¾çš„æ¸²æŸ“å›¾åƒè¿›è¡Œç›‘ç£ï¼Œå¼•å¯¼é«˜æ–¯å–·ç»˜è¡¨ç¤ºæœå‘VDMçš„ç”Ÿæˆæ–¹å‘ã€‚ä½†æ˜¯ï¼Œç”Ÿæˆæ–¹å‘é€šå¸¸æ¶‰åŠç‰©ä½“è¿åŠ¨å’Œéšæœºç›¸æœºè½¨è¿¹ï¼Œä½¿å¾—åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­ç›´æ¥ç›‘ç£å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ç»Ÿä¸€çš„æŒ‡å¯¼å½¢å¼æ¥æ ¡æ­£VDMçš„å™ªå£°é¢„æµ‹ç»“æœã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ç»“åˆåŸºäºçœŸå®æ·±åº¦å›¾çš„æ·±åº¦warpæŒ‡å¯¼å’ŒåŸºäºè¯­ä¹‰å›¾åƒç‰¹å¾çš„æŒ‡å¯¼ï¼Œç¡®ä¿ä»VDMå¾—åˆ°çš„åˆ†æ•°æ›´æ–°æ–¹å‘ä¸æ­£ç¡®çš„ç›¸æœºå§¿æ€å’Œç²¾ç¡®å‡ ä½•å¯¹é½ã€‚å®éªŒç»“æœæ˜¾æ˜¾ç¤ºï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„è¡¨ç°å‡è¶…è¶Šç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>3DGSåœ¨ä¸‰ç»´åœºæ™¯è¡¨ç¤ºä¸­å…·æœ‰é«˜è´¨é‡çš„å®æ—¶æ¸²æŸ“èƒ½åŠ›ã€‚</li>
<li>åœ¨è¾“å…¥åŒ…å«ç¨€ç–è®­ç»ƒè§†å›¾çš„æƒ…å†µä¸‹ï¼Œ3DGSæ˜“å‡ºç°è¿‡æ‹Ÿåˆé—®é¢˜ã€‚</li>
<li>VDMçš„æˆåŠŸå¯å‘äº†ä¸€ç§æ–°çš„æ¡†æ¶GSDï¼Œç”¨äºä»é¢„è®­ç»ƒçš„VDMä¸­æå–å¤šè§†å›¾ä¸€è‡´æ€§å…ˆéªŒã€‚</li>
<li>GSDåˆ©ç”¨SDSçš„æ–¹æ³•ï¼Œé€šè¿‡ç›‘ç£å¤šä¸ªç›¸é‚»è§†å›¾çš„æ¸²æŸ“å›¾åƒæ¥å¼•å¯¼é«˜æ–¯å–·ç»˜è¡¨ç¤ºæœå‘VDMçš„ç”Ÿæˆæ–¹å‘ã€‚</li>
<li>ç”Ÿæˆæ–¹å‘æ¶‰åŠç‰©ä½“è¿åŠ¨å’Œéšæœºç›¸æœºè½¨è¿¹ï¼Œä½¿å¾—ä¼˜åŒ–è¿‡ç¨‹ä¸­çš„ç›´æ¥ç›‘ç£å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§ç»Ÿä¸€çš„æŒ‡å¯¼å½¢å¼æ¥æ ¡æ­£VDMçš„å™ªå£°é¢„æµ‹ç»“æœï¼Œç¡®ä¿åˆ†æ•°æ›´æ–°æ–¹å‘ä¸æ­£ç¡®çš„ç›¸æœºå§¿æ€å’Œå‡ ä½•å¯¹é½ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGSDæ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.11213">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1c493aa98dbf9c5594f3d9b3ee3586b7" align="middle">
<img src="https://picx.zhimg.com/v2-2118b289b66dd637918a97f0774b3a93" align="middle">
<img src="https://picx.zhimg.com/v2-e827447e17a99bedfbfce0e76a145e16" align="middle">
<img src="https://picx.zhimg.com/v2-e27b0330d90c486692aa70816d5ebb13" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Evaluating-Latent-Generative-Paradigms-for-High-Fidelity-3D-Shape-Completion-from-a-Single-Depth-Image"><a href="#Evaluating-Latent-Generative-Paradigms-for-High-Fidelity-3D-Shape-Completion-from-a-Single-Depth-Image" class="headerlink" title="Evaluating Latent Generative Paradigms for High-Fidelity 3D Shape Completion from a Single Depth Image"></a>Evaluating Latent Generative Paradigms for High-Fidelity 3D Shape Completion from a Single Depth Image</h2><p><strong>Authors:Matthias Humt, Ulrich Hillenbrand, Rudolph Triebel</strong></p>
<p>While generative models have seen significant adoption across a wide range of data modalities, including 3D data, a consensus on which model is best suited for which task has yet to be reached. Further, conditional information such as text and images to steer the generation process are frequently employed, whereas others, like partial 3D data, have not been thoroughly evaluated. In this work, we compare two of the most promising generative modelsâ€“Denoising Diffusion Probabilistic Models and Autoregressive Causal Transformersâ€“which we adapt for the tasks of generative shape modeling and completion. We conduct a thorough quantitative evaluation and comparison of both tasks, including a baseline discriminative model and an extensive ablation study. Our results show that (1) the diffusion model with continuous latents outperforms both the discriminative model and the autoregressive approach and delivers state-of-the-art performance on multi-modal shape completion from a single, noisy depth image under realistic conditions and (2) when compared on the same discrete latent space, the autoregressive model can match or exceed diffusion performance on these tasks.</p>
<blockquote>
<p>å°½ç®¡ç”Ÿæˆæ¨¡å‹å·²è¢«å¹¿æ³›åº”ç”¨äºåŒ…æ‹¬3Dæ•°æ®åœ¨å†…çš„å¤šç§æ•°æ®æ¨¡æ€ï¼Œä½†å¯¹äºå“ªç§æ¨¡å‹æœ€é€‚åˆå“ªç§ä»»åŠ¡å°šæœªè¾¾æˆå…±è¯†ã€‚æ­¤å¤–ï¼Œç»å¸¸ä½¿ç”¨æ–‡æœ¬å’Œå›¾åƒç­‰æ¡ä»¶ä¿¡æ¯æ¥å¼•å¯¼ç”Ÿæˆè¿‡ç¨‹ï¼Œè€Œå…¶ä»–ä¿¡æ¯ï¼Œå¦‚éƒ¨åˆ†3Dæ•°æ®ï¼Œå°šæœªå¾—åˆ°å……åˆ†è¯„ä¼°ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ¯”è¾ƒäº†ä¸¤ç§æœ€æœ‰å‰é€”çš„ç”Ÿæˆæ¨¡å‹â€”â€”å»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹å’Œè‡ªå›å½’å› æœè½¬æ¢å™¨â€”â€”æˆ‘ä»¬å°†å…¶é€‚åº”äºç”Ÿæˆå½¢çŠ¶å»ºæ¨¡å’Œå®Œæˆçš„ä»»åŠ¡ã€‚æˆ‘ä»¬å¯¹è¿™ä¸¤ä¸ªä»»åŠ¡è¿›è¡Œäº†å…¨é¢çš„å®šé‡è¯„ä¼°å’Œæ¯”è¾ƒï¼ŒåŒ…æ‹¬åŸºçº¿åˆ¤åˆ«æ¨¡å‹å’Œå¹¿æ³›çš„æ¶ˆèç ”ç©¶ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼šï¼ˆ1ï¼‰å…·æœ‰è¿ç»­æ½œåœ¨ç©ºé—´çš„æ‰©æ•£æ¨¡å‹åœ¨å•å™ªå£°æ·±åº¦å›¾åƒçš„å¤šæ¨¡æ€å½¢çŠ¶å®Œæˆä»»åŠ¡ä¸Šè¡¨ç°æœ€ä½³ï¼›ï¼ˆ2ï¼‰åœ¨ç›¸åŒçš„ç¦»æ•£æ½œåœ¨ç©ºé—´ä¸Šè¿›è¡Œæ¯”è¾ƒæ—¶ï¼Œè‡ªå›å½’æ¨¡å‹åœ¨è¿™äº›ä»»åŠ¡ä¸Šçš„æ€§èƒ½å¯ä¸æ‰©æ•£æ¨¡å‹ç›¸åŒ¹é…æˆ–è¶…è¿‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.11074v1">PDF</a> 17 pages, 4 figures, 19 tables</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æ¯”è¾ƒäº†ä¸¤ç§æœ€å…·å‰æ™¯çš„ç”Ÿæˆæ¨¡å‹â€”â€”å»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹å’Œè‡ªå›å½’å› æœè½¬æ¢å™¨ï¼Œç”¨äºç”Ÿæˆå½¢çŠ¶å»ºæ¨¡å’Œè¡¥å…¨ä»»åŠ¡ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œæ‰©æ•£æ¨¡å‹åœ¨è¿ç»­æ½œåœ¨ç©ºé—´ä¸­è¡¨ç°æœ€ä½³ï¼Œè‡ªå›å½’æ¨¡å‹åœ¨ç¦»æ•£æ½œåœ¨ç©ºé—´ä¸­å¯åŒ¹é…æˆ–è¶…è¶Šæ‰©æ•£æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆæ¨¡å‹å¹¿æ³›åº”ç”¨äºå¤šç§æ•°æ®æ¨¡æ€ï¼Œä½†é’ˆå¯¹ä½•ç§ä»»åŠ¡é€‰æ‹©ä½•ç§æ¨¡å‹å°šæœªè¾¾æˆå…±è¯†ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹å’Œè‡ªå›å½’æ¨¡å‹æ˜¯ä¸¤ç§æœ€å…·å‰æ™¯çš„ç”Ÿæˆæ¨¡å‹ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹åœ¨è¿ç»­æ½œåœ¨ç©ºé—´ä¸­çš„æ€§èƒ½è¶…è¿‡åˆ¤åˆ«æ¨¡å‹å’Œè‡ªå›å½’æ–¹æ³•ï¼Œå°¤å…¶åœ¨å•å™ªå£°æ·±åº¦å›¾åƒçš„å¤šæ¨¡å¼å½¢çŠ¶è¡¥å…¨ä»»åŠ¡ä¸Šè¡¨ç°å“è¶Šã€‚</li>
<li>åœ¨ç›¸åŒçš„ç¦»æ•£æ½œåœ¨ç©ºé—´æ¯”è¾ƒï¼Œè‡ªå›å½’æ¨¡å‹çš„æ€§èƒ½å¯ä¸æ‰©æ•£æ¨¡å‹ç›¸å½“æˆ–æ›´å¥½ã€‚</li>
<li>æ–‡ä¸­è¿˜æåˆ°äº†æ¡ä»¶ä¿¡æ¯å¦‚æ–‡æœ¬å’Œå›¾åƒå¯¹ç”Ÿæˆè¿‡ç¨‹çš„æŒ‡å¯¼ä½œç”¨ï¼Œè€Œéƒ¨åˆ†3Dæ•°æ®å°šæœªå¾—åˆ°å……åˆ†è¯„ä¼°ã€‚</li>
<li>æ–‡ç« é€šè¿‡å®šé‡è¯„ä¼°å’Œæ¯”è¾ƒï¼Œä¸ºç”Ÿæˆå½¢çŠ¶å»ºæ¨¡å’Œè¡¥å…¨ä»»åŠ¡æä¾›äº†æ·±å…¥è§è§£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.11074">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-531750aaf20f1ce719e41be4cf9beaae" align="middle">
<img src="https://picx.zhimg.com/v2-c79bbbd808e1f9f785ff0fc296b891c5" align="middle">
<img src="https://picx.zhimg.com/v2-f5e6d6a4228a9591eb3b39e0e44e2c95" align="middle">
<img src="https://picx.zhimg.com/v2-5d36ca7426c31030cc04c653588a6aa2" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Algorithms-Trained-on-Normal-Chest-X-rays-Can-Predict-Health-Insurance-Types"><a href="#Algorithms-Trained-on-Normal-Chest-X-rays-Can-Predict-Health-Insurance-Types" class="headerlink" title="Algorithms Trained on Normal Chest X-rays Can Predict Health Insurance Types"></a>Algorithms Trained on Normal Chest X-rays Can Predict Health Insurance Types</h2><p><strong>Authors:Chi-Yu Chen, Rawan Abulibdeh, Arash Asgari, Leo Anthony Celi, Deirdre Goode, Hassan Hamidi, Laleh Seyyed-Kalantari, Po-Chih Kuo, Ned McCague, Thomas Sounack</strong></p>
<p>Artificial intelligence is revealing what medicine never intended to encode. Deep vision models, trained on chest X-rays, can now detect not only disease but also invisible traces of social inequality. In this study, we show that state-of-the-art architectures (DenseNet121, SwinV2-B, MedMamba) can predict a patientâ€™s health insurance type, a strong proxy for socioeconomic status, from normal chest X-rays with significant accuracy (AUC around 0.67 on MIMIC-CXR-JPG, 0.68 on CheXpert). The signal persists even when age, race, and sex are controlled for, and remains detectable when the model is trained exclusively on a single racial group. Patch-based occlusion reveals that the signal is diffuse rather than localized, embedded in the upper and mid-thoracic regions. This suggests that deep networks may be internalizing subtle traces of clinical environments, equipment differences, or care pathways; learning socioeconomic segregation itself. These findings challenge the assumption that medical images are neutral biological data. By uncovering how models perceive and exploit these hidden social signatures, this work reframes fairness in medical AI: the goal is no longer only to balance datasets or adjust thresholds, but to interrogate and disentangle the social fingerprints embedded in clinical data itself.</p>
<blockquote>
<p>äººå·¥æ™ºèƒ½æ­£åœ¨æ­ç¤ºåŒ»å­¦ä»æœªæ‰“ç®—ç¼–ç çš„ä¿¡æ¯ã€‚åŸºäºèƒ¸éƒ¨Xå…‰ç‰‡çš„æ·±åº¦è§†è§‰æ¨¡å‹ç°åœ¨ä¸ä»…å¯ä»¥æ£€æµ‹ç–¾ç—…ï¼Œè¿˜å¯ä»¥æ£€æµ‹ç¤¾ä¼šä¸å¹³ç­‰çš„éšå½¢ç—•è¿¹ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†æœ€å…ˆè¿›çš„æ¶æ„ï¼ˆDenseNet121ã€SwinV2-Bã€MedMambaï¼‰å¯ä»¥ä»æ­£å¸¸çš„èƒ¸éƒ¨Xå…‰ç‰‡ä¸­é¢„æµ‹æ‚£è€…çš„å¥åº·ä¿é™©ç±»å‹ï¼ˆä½œä¸ºç¤¾ä¼šç»æµåœ°ä½çš„æœ‰åŠ›ä»£ç†ï¼‰ï¼Œé¢„æµ‹çš„å‡†ç¡®æ€§ç›¸å½“é«˜ï¼ˆåœ¨MIMIC-CXR-JPGä¸Šä¸ºAUCçº¦0.67ï¼Œåœ¨CheXpertä¸Šä¸º0.68ï¼‰ã€‚å³ä½¿åœ¨æ§åˆ¶å¹´é¾„ã€ç§æ—å’Œæ€§åˆ«åï¼Œè¿™ä¸€ä¿¡å·ä»ç„¶å­˜åœ¨ï¼Œå¹¶ä¸”åœ¨æ¨¡å‹ä»…é’ˆå¯¹å•ä¸€ç§æ—ç¾¤ä½“è¿›è¡Œè®­ç»ƒæ—¶ä»ç„¶å¯æ£€æµ‹ã€‚åŸºäºè¡¥ä¸çš„é®æŒ¡è¡¨æ˜ï¼Œä¿¡å·æ˜¯å¼¥æ¼«çš„è€Œä¸æ˜¯å±€éƒ¨çš„ï¼ŒåµŒå…¥åœ¨èƒ¸éƒ¨ä¸ŠåŒºå’Œä¸­éƒ¨åŒºåŸŸã€‚è¿™è¡¨æ˜æ·±åº¦ç½‘ç»œå¯èƒ½æ­£åœ¨å†…åŒ–ä¸´åºŠç¯å¢ƒã€è®¾å¤‡å·®å¼‚æˆ–æŠ¤ç†è·¯å¾„çš„å¾®å¦™ç—•è¿¹ï¼›å­¦ä¹ ç¤¾ä¼šç»æµéš”ç¦»æœ¬èº«ã€‚è¿™äº›å‘ç°æŒ‘æˆ˜äº†åŒ»å­¦å›¾åƒæ˜¯ä¸­ç«‹ç”Ÿç‰©æ•°æ®çš„å‡è®¾ã€‚é€šè¿‡æ­ç¤ºæ¨¡å‹å¦‚ä½•æ„ŸçŸ¥å’Œåˆ©ç”¨è¿™äº›éšè—çš„ç¤¾ä¼šç­¾åï¼Œè¿™é¡¹å·¥ä½œé‡æ–°å®šä¹‰äº†åŒ»ç–—äººå·¥æ™ºèƒ½çš„å…¬å¹³æ€§ï¼šç›®æ ‡ä¸å†ä»…ä»…æ˜¯å¹³è¡¡æ•°æ®é›†æˆ–è°ƒæ•´é˜ˆå€¼ï¼Œè€Œæ˜¯è´¨ç–‘å’Œè§£é™¤åµŒå…¥åœ¨ä¸´åºŠæ•°æ®ä¸­çš„ç¤¾ä¼šæŒ‡çº¹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.11030v1">PDF</a> Submitting to MIDL 2026</p>
<p><strong>Summary</strong></p>
<p>äººå·¥æ™ºèƒ½ä¸ä»…åœ¨ç–¾ç—…æ£€æµ‹ä¸Šæœ‰æ‰€çªç ´ï¼Œè¿˜èƒ½ä»æ™®é€šçš„èƒ¸éƒ¨Xå…‰ç‰‡ä¸­è¯†åˆ«å‡ºç¤¾ä¼šä¸å¹³ç­‰çš„ç—•è¿¹ã€‚æœ€æ–°ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œé¡¶å°–æ¶æ„çš„æ¨¡å‹èƒ½å‡†ç¡®é¢„æµ‹ç—…äººçš„å¥åº·ä¿é™©ç±»å‹â€”â€”ä¸€ä¸ªç¤¾ä¼šç»æµåœ°ä½çš„æœ‰åŠ›æŒ‡æ ‡ï¼Œå‡†ç¡®ç‡è¾¾åˆ°äº†AUC 0.67ä»¥ä¸Šã€‚å³ä½¿æ§åˆ¶äº†å¹´é¾„ã€ç§æ—å’Œæ€§åˆ«å› ç´ ï¼Œè¿™ä¸€ä¿¡å·ä¾ç„¶å­˜åœ¨ï¼Œç”šè‡³åœ¨åªé’ˆå¯¹å•ä¸€ç§æ—ç¾¤ä½“è¿›è¡Œè®­ç»ƒæ—¶ä»ç„¶å¯æ£€æµ‹ã€‚è¿™è¡¨æ˜æ·±åº¦ç½‘ç»œå¯èƒ½æ­£åœ¨å†…åŒ–ä¸´åºŠç¯å¢ƒã€è®¾å¤‡å·®å¼‚æˆ–æŠ¤ç†è·¯å¾„çš„ç»†å¾®ç—•è¿¹ï¼Œç”šè‡³æ˜¯å­¦ä¹ ç¤¾ä¼šç»æµçš„åˆ†å‰²ã€‚è¿™ä¸€å‘ç°æŒ‘æˆ˜äº†åŒ»å­¦å›¾åƒæ˜¯ä¸­æ€§ç”Ÿç‰©æ•°æ®çš„å‡è®¾ï¼Œé‡æ–°å®šä¹‰äº†åŒ»ç–—äººå·¥æ™ºèƒ½çš„å…¬å¹³æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äººå·¥æ™ºèƒ½å¯ä»¥ä»èƒ¸éƒ¨Xå…‰ç‰‡ä¸­è¯†åˆ«å‡ºç¤¾ä¼šä¸å¹³ç­‰çš„ç—•è¿¹ã€‚</li>
<li>é¡¶å°–æ¨¡å‹èƒ½å‡†ç¡®é¢„æµ‹ç—…äººçš„å¥åº·ä¿é™©ç±»å‹ï¼Œåæ˜ ç¤¾ä¼šç»æµçŠ¶æ€ã€‚</li>
<li>æ¨¡å‹é¢„æµ‹çš„å‡†ç¡®æ€§åœ¨æ§åˆ¶å¹´é¾„ã€ç§æ—å’Œæ€§åˆ«å› ç´ åä¾ç„¶æ˜¾è‘—ã€‚</li>
<li>ä¿¡å·æ˜¯æ™®éçš„ï¼Œä¸ä»…å­˜åœ¨äºå¤šç§æ•°æ®é›†ä¸Šï¼Œè€Œä¸”å­˜åœ¨äºå•ä¸€ç§æ—ç¾¤ä½“çš„æ•°æ®ä¸­ã€‚</li>
<li>é€šè¿‡å±€éƒ¨é®æŒ¡å‘ç°ï¼Œè¿™äº›ç¤¾ä¼šä¿¡å·æ˜¯åˆ†æ•£çš„ï¼ŒåµŒå…¥åœ¨èƒ¸éƒ¨ä¸ŠåŠéƒ¨åˆ†å’Œä¸­éƒ¨åŒºåŸŸã€‚</li>
<li>ç ”ç©¶æŒ‘æˆ˜äº†åŒ»å­¦å›¾åƒä½œä¸ºä¸­æ€§æ•°æ®çš„å‡è®¾ï¼Œæ­ç¤ºäº†æ¨¡å‹å¦‚ä½•æ„ŸçŸ¥å’Œåˆ©ç”¨éšè—çš„ç¤¾ä¼šç­¾åã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.11030">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3efa5b74bf076d73b9d19d4f790aed4c" align="middle">
<img src="https://picx.zhimg.com/v2-745b0f92dcaf92bbf88f598197c3ee19" align="middle">
<img src="https://picx.zhimg.com/v2-b95e1e429731f5711d49c948ab2a617b" align="middle">
<img src="https://picx.zhimg.com/v2-743fda727ec2d9e013128f701a6f5aea" align="middle">
<img src="https://picx.zhimg.com/v2-4ced01ae56d78a6be7fb7bffccd3c788" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="SP-Guard-Selective-Prompt-adaptive-Guidance-for-Safe-Text-to-Image-Generation"><a href="#SP-Guard-Selective-Prompt-adaptive-Guidance-for-Safe-Text-to-Image-Generation" class="headerlink" title="SP-Guard: Selective Prompt-adaptive Guidance for Safe Text-to-Image Generation"></a>SP-Guard: Selective Prompt-adaptive Guidance for Safe Text-to-Image Generation</h2><p><strong>Authors:Sumin Yu, Taesup Moon</strong></p>
<p>While diffusion-based T2I models have achieved remarkable image generation quality, they also enable easy creation of harmful content, raising social concerns and highlighting the need for safer generation. Existing inference-time guiding methods lack both adaptivityâ€“adjusting guidance strength based on the promptâ€“and selectivityâ€“targeting only unsafe regions of the image. Our method, SP-Guard, addresses these limitations by estimating prompt harmfulness and applying a selective guidance mask to guide only unsafe areas. Experiments show that SP-Guard generates safer images than existing methods while minimizing unintended content alteration. Beyond improving safety, our findings highlight the importance of transparency and controllability in image generation.</p>
<blockquote>
<p>åŸºäºæ‰©æ•£çš„æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ¨¡å‹è™½ç„¶å·²ç»è¾¾åˆ°äº†æ˜¾è‘—çš„å›¾åƒç”Ÿæˆè´¨é‡ï¼Œä½†å®ƒä»¬ä¹Ÿæ˜“äºåˆ›å»ºæœ‰å®³å†…å®¹ï¼Œå¼•å‘äº†ç¤¾ä¼šå…³æ³¨ï¼Œå¹¶å¼ºè°ƒäº†æ›´å®‰å…¨ç”Ÿæˆçš„éœ€æ±‚ã€‚ç°æœ‰çš„æ¨ç†æ—¶é—´æŒ‡å¯¼æ–¹æ³•æ—¢ç¼ºä¹é€‚åº”æ€§â€”â€”æ ¹æ®æç¤ºè°ƒæ•´æŒ‡å¯¼åŠ›åº¦ï¼Œä¹Ÿç¼ºä¹é€‰æ‹©æ€§â€”â€”åªé’ˆå¯¹å›¾åƒçš„ä¸å®‰å…¨åŒºåŸŸè¿›è¡Œå®šä½ã€‚æˆ‘ä»¬çš„æ–¹æ³•SP-Guardé€šè¿‡ä¼°è®¡æç¤ºæœ‰å®³æ€§å¹¶åº”ç”¨é€‰æ‹©æ€§æŒ‡å¯¼æ©è†œæ¥æŒ‡å¯¼ä»…ä¸å®‰å…¨çš„åŒºåŸŸï¼Œè§£å†³äº†è¿™äº›é™åˆ¶ã€‚å®éªŒè¡¨æ˜ï¼ŒSP-Guardåœ¨æœ€å°åŒ–æ„å¤–å†…å®¹æ›´æ”¹çš„åŒæ—¶ï¼Œç”Ÿæˆäº†æ¯”ç°æœ‰æ–¹æ³•æ›´å®‰å…¨çš„å›¾åƒã€‚é™¤äº†æé«˜å®‰å…¨æ€§ä¹‹å¤–ï¼Œæˆ‘ä»¬çš„å‘ç°è¿˜çªå‡ºäº†å›¾åƒç”Ÿæˆä¸­é€æ˜åº¦å’Œå¯æ§æ€§çš„é‡è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.11014v1">PDF</a> Accepted for presentation at TRUST-AI Workshop, ECAI 2025. Proceedings to appear in CEUR-WS</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºæ‰©æ•£çš„T2Iæ¨¡å‹åœ¨å›¾åƒç”Ÿæˆæ–¹é¢çš„çªå‡ºè¡¨ç°ï¼Œä½†ä¹ŸæŒ‡å‡ºäº†å…¶æ˜“äºç”Ÿæˆæœ‰å®³å†…å®¹çš„é—®é¢˜ï¼Œå¼•å‘äº†ç¤¾ä¼šå…³æ³¨å¹¶å¼ºè°ƒäº†æ›´å®‰å…¨ç”Ÿæˆçš„éœ€æ±‚ã€‚ç°æœ‰æ¨ç†æ—¶é—´æŒ‡å¯¼æ–¹æ³•ç¼ºä¹è‡ªé€‚åº”æ€§å’Œé€‰æ‹©æ€§ï¼Œæ— æ³•æ ¹æ®æç¤ºè°ƒæ•´æŒ‡å¯¼åŠ›åº¦ï¼Œä¹Ÿæ— æ³•é’ˆå¯¹å›¾åƒçš„ä¸å®‰å…¨åŒºåŸŸè¿›è¡Œå®šä½ã€‚æœ¬æ–‡æå‡ºçš„SP-Guardæ–¹æ³•é€šè¿‡ä¼°ç®—æç¤ºå±å®³æ€§å¹¶åº”ç”¨é€‰æ‹©æ€§æŒ‡å¯¼æ©è†œï¼Œåªåœ¨ä¸å®‰å…¨åŒºåŸŸè¿›è¡ŒæŒ‡å¯¼æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚å®éªŒè¡¨æ˜ï¼ŒSP-Guardèƒ½å¤Ÿç”Ÿæˆæ¯”å…¶ä»–æ–¹æ³•æ›´å®‰å…¨çš„å›¾åƒï¼ŒåŒæ—¶æœ€å°åŒ–æ„å¤–å†…å®¹æ›´æ”¹ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å‘ç°é€æ˜åº¦å’Œå¯æ§æ€§æ˜¯å›¾åƒç”Ÿæˆä¸­çš„é‡è¦å› ç´ ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œä½†ä¹Ÿå­˜åœ¨ç”Ÿæˆæœ‰å®³å†…å®¹çš„é£é™©ã€‚</li>
<li>ç°æœ‰æ¨ç†æ—¶é—´æŒ‡å¯¼æ–¹æ³•ç¼ºä¹è‡ªé€‚åº”æ€§å’Œé€‰æ‹©æ€§ï¼Œæ— æ³•æ»¡è¶³å®‰å…¨ç”Ÿæˆçš„éœ€æ±‚ã€‚</li>
<li>SP-Guardæ–¹æ³•é€šè¿‡ä¼°ç®—æç¤ºå±å®³æ€§å¹¶åº”ç”¨é€‰æ‹©æ€§æŒ‡å¯¼æ©è†œè§£å†³ç°æœ‰æ–¹æ³•çš„å±€é™æ€§ã€‚</li>
<li>SP-Guardèƒ½åœ¨ç”Ÿæˆæ›´å®‰å…¨å›¾åƒçš„åŒæ—¶æœ€å°åŒ–æ„å¤–å†…å®¹æ›´æ”¹ã€‚</li>
<li>é™¤äº†æé«˜å®‰å…¨æ€§å¤–ï¼Œé€æ˜åº¦å’Œå¯æ§æ€§åœ¨å›¾åƒç”Ÿæˆä¸­ä¹Ÿå¾ˆé‡è¦ã€‚</li>
<li>SP-Guardçš„å®éªŒç»“æœè¯æ˜äº†å…¶æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.11014">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-07657bd422cf5863cb57e7ced3bac9ab" align="middle">
<img src="https://picx.zhimg.com/v2-7e4efad200760f64b76ec5d79398ae68" align="middle">
<img src="https://picx.zhimg.com/v2-8180ebe86c6472515729589bdab50538" align="middle">
<img src="https://picx.zhimg.com/v2-76a78bd65fd763158e802858965c31a3" align="middle">
<img src="https://picx.zhimg.com/v2-a7911cc63354827b9fa8fbba7b53e00a" align="middle">
<img src="https://picx.zhimg.com/v2-cc8671a88dc960c9b07fa4907f84b5d1" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Frequency-Aware-Vision-Language-Multimodality-Generalization-Network-for-Remote-Sensing-Image-Classification"><a href="#Frequency-Aware-Vision-Language-Multimodality-Generalization-Network-for-Remote-Sensing-Image-Classification" class="headerlink" title="Frequency-Aware Vision-Language Multimodality Generalization Network for Remote Sensing Image Classification"></a>Frequency-Aware Vision-Language Multimodality Generalization Network for Remote Sensing Image Classification</h2><p><strong>Authors:Junjie Zhang, Feng Zhao, Hanqiang Liu, Jun Yu</strong></p>
<p>The booming remote sensing (RS) technology is giving rise to a novel multimodality generalization task, which requires the model to overcome data heterogeneity while possessing powerful cross-scene generalization ability. Moreover, most vision-language models (VLMs) usually describe surface materials in RS images using universal texts, lacking proprietary linguistic prior knowledge specific to different RS vision modalities. In this work, we formalize RS multimodality generalization (RSMG) as a learning paradigm, and propose a frequency-aware vision-language multimodality generalization network (FVMGN) for RS image classification. Specifically, a diffusion-based training-test-time augmentation (DTAug) strategy is designed to reconstruct multimodal land-cover distributions, enriching input information for FVMGN. Following that, to overcome multimodal heterogeneity, a multimodal wavelet disentanglement (MWDis) module is developed to learn cross-domain invariant features by resampling low and high frequency components in the frequency domain. Considering the characteristics of RS vision modalities, shared and proprietary class texts is designed as linguistic inputs for the transformer-based text encoder to extract diverse text features. For multimodal vision inputs, a spatial-frequency-aware image encoder (SFIE) is constructed to realize local-global feature reconstruction and representation. Finally, a multiscale spatial-frequency feature alignment (MSFFA) module is suggested to construct a unified semantic space, ensuring refined multiscale alignment of different text and vision features in spatial and frequency domains. Extensive experiments show that FVMGN has the excellent multimodality generalization ability compared with state-of-the-art (SOTA) methods.</p>
<blockquote>
<p>é¥æ„ŸæŠ€æœ¯çš„è“¬å‹ƒå‘å±•å¼•å‘äº†ä¸€é¡¹æ–°å‹çš„å¤šæ¨¡æ€æ³›åŒ–ä»»åŠ¡ï¼Œè¦æ±‚æ¨¡å‹åœ¨æ‹¥æœ‰å¼ºå¤§çš„è·¨åœºæ™¯æ³›åŒ–èƒ½åŠ›çš„åŒæ—¶å…‹æœæ•°æ®å¼‚è´¨æ€§ã€‚æ­¤å¤–ï¼Œå¤§å¤šæ•°è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰é€šå¸¸ä½¿ç”¨é€šç”¨æ–‡æœ¬æè¿°é¥æ„Ÿå›¾åƒçš„è¡¨é¢ææ–™ï¼Œç¼ºä¹é’ˆå¯¹ä¸åŒé¥æ„Ÿè§†è§‰æ¨¡æ€çš„ä¸“æœ‰è¯­è¨€å…ˆéªŒçŸ¥è¯†ã€‚åœ¨æ­¤å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å°†é¥æ„Ÿå¤šæ¨¡æ€æ³›åŒ–ï¼ˆRSMGï¼‰å½¢å¼åŒ–ä¸ºä¸€ç§å­¦ä¹ èŒƒå¼ï¼Œå¹¶æå‡ºä¸€ç§é¢‘ç‡æ„ŸçŸ¥è§†è§‰è¯­è¨€å¤šæ¨¡æ€æ³›åŒ–ç½‘ç»œï¼ˆFVMGNï¼‰ç”¨äºé¥æ„Ÿå›¾åƒåˆ†ç±»ã€‚å…·ä½“æ¥è¯´ï¼Œè®¾è®¡äº†ä¸€ç§åŸºäºæ‰©æ•£çš„è®­ç»ƒ-æµ‹è¯•æ—¶é—´å¢å¼ºï¼ˆDTAugï¼‰ç­–ç•¥ï¼Œä»¥é‡å»ºå¤šæ¨¡æ€åœŸåœ°è¦†ç›–åˆ†å¸ƒï¼Œä¸°å¯ŒFVMGNçš„è¾“å…¥ä¿¡æ¯ã€‚ä¹‹åï¼Œä¸ºäº†å…‹æœå¤šæ¨¡æ€å¼‚è´¨æ€§ï¼Œå¼€å‘äº†ä¸€ç§å¤šæ¨¡æ€å°æ³¢è§£çº ç¼ ï¼ˆMWDisï¼‰æ¨¡å—ï¼Œé€šè¿‡é‡æ–°é‡‡æ ·é¢‘ç‡åŸŸä¸­çš„ä½é¢‘å’Œé«˜é¢‘æˆåˆ†æ¥å­¦ä¹ è·¨åŸŸä¸å˜ç‰¹å¾ã€‚è€ƒè™‘åˆ°é¥æ„Ÿè§†è§‰æ¨¡æ€çš„ç‰¹æ€§ï¼Œå°†å…±äº«å’Œä¸“æœ‰ç±»åˆ«æ–‡æœ¬è®¾è®¡ä¸ºè¯­è¨€è¾“å…¥ï¼Œç”¨äºåŸºäºå˜å‹å™¨çš„æ–‡æœ¬ç¼–ç å™¨ä»¥æå–å¤šæ ·çš„æ–‡æœ¬ç‰¹å¾ã€‚å¯¹äºå¤šæ¨¡æ€è§†è§‰è¾“å…¥ï¼Œæ„å»ºäº†ä¸€ä¸ªç©ºé—´é¢‘ç‡æ„ŸçŸ¥å›¾åƒç¼–ç å™¨ï¼ˆSFIEï¼‰ä»¥å®ç°å±€éƒ¨-å…¨å±€ç‰¹å¾é‡å»ºå’Œè¡¨ç¤ºã€‚æœ€åï¼Œå»ºè®®ä¸€ä¸ªå¤šå°ºåº¦ç©ºé—´é¢‘ç‡ç‰¹å¾å¯¹é½ï¼ˆMSFFAï¼‰æ¨¡å—ä»¥æ„å»ºç»Ÿä¸€è¯­ä¹‰ç©ºé—´ï¼Œç¡®ä¿åœ¨ç©ºé—´åŸŸå’Œé¢‘ç‡åŸŸä¸­å®ç°ä¸åŒæ–‡æœ¬å’Œè§†è§‰ç‰¹å¾çš„å¤šå°ºåº¦ç²¾ç»†å¯¹é½ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œä¸æœ€æ–°æ–¹æ³•ç›¸æ¯”ï¼ŒFVMGNå…·æœ‰å‡ºè‰²çš„å¤šæ¨¡æ€æ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.10774v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>é¥æ„ŸæŠ€æœ¯ä¸æ–°å‹å¤šæ¨¡æ€æ³›åŒ–ä»»åŠ¡çš„ç»“åˆï¼Œè¦æ±‚æ¨¡å‹å…‹æœæ•°æ®å¼‚è´¨æ€§ï¼Œå…·å¤‡å¼ºå¤§çš„è·¨åœºæ™¯æ³›åŒ–èƒ½åŠ›ã€‚é’ˆå¯¹æ­¤ï¼Œæœ¬æ–‡æå‡ºä¸€ç§é¢‘ç‡æ„ŸçŸ¥çš„è§†è¯­è¨€å¤šæ¨¡æ€æ³›åŒ–ç½‘ç»œï¼ˆFVMGNï¼‰ï¼Œç”¨äºé¥æ„Ÿå›¾åƒåˆ†ç±»ã€‚è®¾è®¡æ‰©æ•£è®­ç»ƒæµ‹è¯•æ—¶é—´å¢å¼ºç­–ç•¥ï¼Œé‡å»ºå¤šæ¨¡æ€åœŸåœ°è¦†ç›–åˆ†å¸ƒï¼Œä¸°å¯ŒFVMGNçš„è¾“å…¥ä¿¡æ¯ã€‚ä¸ºå…‹æœå¤šæ¨¡æ€å¼‚è´¨æ€§ï¼Œå¼€å‘å¤šæ¨¡æ€å°æ³¢åˆ†ç¦»æ¨¡å—ï¼Œé€šè¿‡é¢‘ç‡åŸŸé‡é‡‡æ ·é«˜ä½é¢‘æˆåˆ†å­¦ä¹ è·¨åŸŸä¸å˜ç‰¹å¾ã€‚é’ˆå¯¹é¥æ„Ÿè§†è§‰æ¨¡æ€ç‰¹æ€§ï¼Œè®¾è®¡å…±äº«å’Œä¸“æœ‰ç±»åˆ«æ–‡æœ¬ä½œä¸ºè¯­è¨€è¾“å…¥ï¼Œä¸ºåŸºäºå˜å‹å™¨çš„æ–‡æœ¬ç¼–ç å™¨æå–å¤šç§æ–‡æœ¬ç‰¹å¾ã€‚å¯¹äºå¤šæ¨¡æ€è§†è§‰è¾“å…¥ï¼Œæ„å»ºç©ºé—´é¢‘ç‡æ„ŸçŸ¥å›¾åƒç¼–ç å™¨å®ç°å±€éƒ¨å…¨å±€ç‰¹å¾é‡å»ºå’Œè¡¨ç¤ºã€‚æœ€åï¼Œæå‡ºå¤šå°ºåº¦ç©ºé—´é¢‘ç‡ç‰¹å¾å¯¹é½æ¨¡å—ï¼Œæ„å»ºç»Ÿä¸€è¯­ä¹‰ç©ºé—´ï¼Œç¡®ä¿ä¸åŒæ–‡æœ¬å’Œè§†è§‰ç‰¹å¾åœ¨ç©ºé—´åŸŸå’Œé¢‘ç‡åŸŸçš„å¤šå°ºåº¦ç²¾ç»†å¯¹é½ã€‚å®éªŒè¡¨æ˜ï¼ŒFVMGNç›¸è¾ƒäºæœ€æ–°æ–¹æ³•å…·æœ‰å‡ºè‰²çš„å¤šæ¨¡æ€æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>é¥æ„ŸæŠ€æœ¯çš„æ–°å¤šæ¨¡æ€æ³›åŒ–ä»»åŠ¡è¦æ±‚æ¨¡å‹å…‹æœæ•°æ®å¼‚è´¨æ€§ï¼Œå…·å¤‡å¼ºå¤§çš„è·¨åœºæ™¯æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>æå‡ºé¢‘ç‡æ„ŸçŸ¥çš„è§†è¯­è¨€å¤šæ¨¡æ€æ³›åŒ–ç½‘ç»œï¼ˆFVMGNï¼‰ç”¨äºé¥æ„Ÿå›¾åƒåˆ†ç±»ã€‚</li>
<li>æ‰©æ•£è®­ç»ƒæµ‹è¯•æ—¶é—´å¢å¼ºç­–ç•¥ç”¨äºé‡å»ºå¤šæ¨¡æ€åœŸåœ°è¦†ç›–åˆ†å¸ƒã€‚</li>
<li>å¤šæ¨¡æ€å°æ³¢åˆ†ç¦»æ¨¡å—å…‹æœå¤šæ¨¡æ€å¼‚è´¨æ€§ï¼Œå­¦ä¹ è·¨åŸŸä¸å˜ç‰¹å¾ã€‚</li>
<li>æ ¹æ®é¥æ„Ÿè§†è§‰æ¨¡æ€ç‰¹æ€§ï¼Œè®¾è®¡å…±äº«å’Œä¸“æœ‰ç±»åˆ«æ–‡æœ¬ä½œä¸ºè¯­è¨€è¾“å…¥ã€‚</li>
<li>ç©ºé—´é¢‘ç‡æ„ŸçŸ¥å›¾åƒç¼–ç å™¨å®ç°å±€éƒ¨å’Œå…¨å±€ç‰¹å¾é‡å»ºå’Œè¡¨ç¤ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.10774">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9f7433e78b3ef89cc167198cbd8fb791" align="middle">
<img src="https://picx.zhimg.com/v2-df6cc2d9780bbf2a3756b2d36835e14c" align="middle">
<img src="https://picx.zhimg.com/v2-5f65db3e8efb11753acda989bbc415a3" align="middle">
<img src="https://picx.zhimg.com/v2-548b88dce874dddde03a55fa4df930cd" align="middle">
<img src="https://picx.zhimg.com/v2-656056cafa851ee81ec73b23bbc67157" align="middle">
<img src="https://picx.zhimg.com/v2-4ad845a14d7eddcfb65308879d21c4e6" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Fast-Data-Attribution-for-Text-to-Image-Models"><a href="#Fast-Data-Attribution-for-Text-to-Image-Models" class="headerlink" title="Fast Data Attribution for Text-to-Image Models"></a>Fast Data Attribution for Text-to-Image Models</h2><p><strong>Authors:Sheng-Yu Wang, Aaron Hertzmann, Alexei A Efros, Richard Zhang, Jun-Yan Zhu</strong></p>
<p>Data attribution for text-to-image models aims to identify the training images that most significantly influenced a generated output. Existing attribution methods involve considerable computational resources for each query, making them impractical for real-world applications. We propose a novel approach for scalable and efficient data attribution. Our key idea is to distill a slow, unlearning-based attribution method to a feature embedding space for efficient retrieval of highly influential training images. During deployment, combined with efficient indexing and search methods, our method successfully finds highly influential images without running expensive attribution algorithms. We show extensive results on both medium-scale models trained on MSCOCO and large-scale Stable Diffusion models trained on LAION, demonstrating that our method can achieve better or competitive performance in a few seconds, faster than existing methods by 2,500x - 400,000x. Our work represents a meaningful step towards the large-scale application of data attribution methods on real-world models such as Stable Diffusion.</p>
<blockquote>
<p>æ–‡æœ¬å¯¹å›¾åƒæ¨¡å‹çš„å½’å› æ—¨åœ¨è¯†åˆ«å¯¹ç”Ÿæˆè¾“å‡ºå½±å“æœ€å¤§çš„è®­ç»ƒå›¾åƒã€‚ç°æœ‰çš„å½’å› æ–¹æ³•æ¯æ¬¡æŸ¥è¯¢éƒ½éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºï¼Œä½¿å¾—å®ƒä»¬åœ¨ç°å®ä¸–ç•Œçš„å®é™…åº”ç”¨ä¸­ä¸åˆ‡å®é™…ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºå¯æ‰©å±•å’Œé«˜æ•ˆæ•°æ®å½’å› çš„æ–°å‹æ–¹æ³•ã€‚æˆ‘ä»¬çš„ä¸»è¦æ€æƒ³æ˜¯å°†ç¼“æ…¢çš„æ— å­¦ä¹ å½’å› æ–¹æ³•æç‚¼åˆ°ç‰¹å¾åµŒå…¥ç©ºé—´ï¼Œä»¥ä¾¿é«˜æ•ˆåœ°æ£€ç´¢é«˜åº¦æœ‰å½±å“åŠ›çš„è®­ç»ƒå›¾åƒã€‚åœ¨éƒ¨ç½²è¿‡ç¨‹ä¸­ï¼Œç»“åˆé«˜æ•ˆçš„ç´¢å¼•å’Œæœç´¢æ–¹æ³•ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿåœ¨ä¸è¿è¡Œæ˜‚è´µçš„å½’å› ç®—æ³•çš„æƒ…å†µä¸‹æˆåŠŸæ‰¾åˆ°å…·æœ‰é«˜åº¦å½±å“åŠ›çš„å›¾åƒã€‚æˆ‘ä»¬åœ¨ä¸­ç­‰è§„æ¨¡çš„MSCOCOè®­ç»ƒæ¨¡å‹å’Œå¤§è§„æ¨¡LAIONè®­ç»ƒçš„Stable Diffusionæ¨¡å‹ä¸Šéƒ½å±•ç¤ºäº†å¹¿æ³›çš„ç»“æœï¼Œè¯æ˜æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥åœ¨å‡ ç§’é’Ÿå†…å®ç°æ›´å¥½çš„æ€§èƒ½æˆ–å…·æœ‰ç«äº‰åŠ›çš„è¡¨ç°ï¼Œæ¯”ç°æœ‰æ–¹æ³•å¿«2500å€è‡³40ä¸‡å€ã€‚æˆ‘ä»¬çš„å·¥ä½œä»£è¡¨äº†æ•°æ®å½’å› æ–¹æ³•åœ¨ç°å®ä¸–ç•Œæ¨¡å‹ï¼ˆå¦‚Stable Diffusionï¼‰çš„å¤§è§„æ¨¡åº”ç”¨æ–¹é¢è¿ˆå‡ºçš„æœ‰æ„ä¹‰çš„ä¸€æ­¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.10721v1">PDF</a> NeurIPS 2025 camera ready. Project page: <a target="_blank" rel="noopener" href="https://peterwang512.github.io/FastGDA">https://peterwang512.github.io/FastGDA</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§é«˜æ•ˆä¸”å¯æ‰©å±•çš„æ•°æ®å½’å› æ–¹æ³•ï¼Œç”¨äºæ–‡æœ¬ç”Ÿæˆå›¾åƒæ¨¡å‹ä¸­è¯†åˆ«å¯¹ç”Ÿæˆè¾“å‡ºå½±å“æœ€å¤§çš„è®­ç»ƒå›¾åƒã€‚è¯¥æ–¹æ³•é€šè¿‡å°†æ…¢çš„æ— å­¦ä¹ å½’å› æ–¹æ³•æç‚¼åˆ°ç‰¹å¾åµŒå…¥ç©ºé—´ï¼Œå®ç°äº†é«˜æ•ˆæ£€ç´¢å…³é”®è®­ç»ƒå›¾åƒçš„èƒ½åŠ›ã€‚åœ¨éƒ¨ç½²è¿‡ç¨‹ä¸­ï¼Œç»“åˆé«˜æ•ˆç´¢å¼•å’Œæœç´¢æ–¹æ³•ï¼Œèƒ½åœ¨ä¸è¿è¡Œæ˜‚è´µå½’å› ç®—æ³•çš„æƒ…å†µä¸‹æˆåŠŸæ‰¾åˆ°é«˜åº¦å½±å“ç»“æœçš„å›¾åƒã€‚æ­¤æ–¹æ³•åœ¨ä¸­å‹MSCOCOæ¨¡å‹å’Œå¤§å‹LAIONè®­ç»ƒç¨³å®šæ‰©æ•£æ¨¡å‹ä¸Šå±•ç°å‡ºæ›´å¥½çš„æˆ–ç›¸å½“çš„æ€§èƒ½ï¼Œä¸”åœ¨å‡ ç§’å†…å®Œæˆï¼Œè¿œè¶…ç°æœ‰æ–¹æ³•çš„é€Ÿåº¦ï¼Œæœ€é«˜è¾¾ä¸¤å€åˆ°æ•°åä¸‡å€çš„é€Ÿåº¦æå‡ã€‚è¿™é¡¹ç ”ç©¶æ˜¯å®ç°æ•°æ®å½’å› æ–¹æ³•åœ¨çœŸå®ä¸–ç•Œæ¨¡å‹å¦‚ç¨³å®šæ‰©æ•£çš„å¤§è§„æ¨¡åº”ç”¨çš„é‡è¦ä¸€æ­¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ•°æ®å½’å› æ–¹æ³•ï¼Œç”¨äºæ–‡æœ¬ç”Ÿæˆå›¾åƒæ¨¡å‹ã€‚</li>
<li>æ–¹æ³•å®ç°äº†åœ¨ç‰¹å¾åµŒå…¥ç©ºé—´çš„é«˜æ•ˆæ£€ç´¢ï¼Œæœ‰æ•ˆè¯†åˆ«å¯¹ç”Ÿæˆè¾“å‡ºå½±å“æœ€å¤§çš„è®­ç»ƒå›¾åƒã€‚</li>
<li>ç»“åˆé«˜æ•ˆç´¢å¼•å’Œæœç´¢æ–¹æ³•ï¼Œæ— éœ€è¿è¡Œæ˜‚è´µçš„å½’å› ç®—æ³•å³å¯æ‰¾åˆ°å…³é”®å›¾åƒã€‚</li>
<li>åœ¨ä¸­å‹å’Œå¤§å‹æ¨¡å‹ä¸Šå±•ç°å‡ºè‰¯å¥½æ€§èƒ½ï¼Œæ˜¾è‘—æé«˜äº†ç°æœ‰æ–¹æ³•çš„æ•ˆç‡ã€‚</li>
<li>æ–¹æ³•çš„æ€§èƒ½æå‡è¾¾åˆ°äº†ç°æœ‰æ–¹æ³•çš„æ•°åƒå€è‡³æ•°åä¸‡å€çš„é€Ÿåº¦æå‡ã€‚</li>
<li>ç ”ç©¶ä¸ºå®ç°æ•°æ®å½’å› æ–¹æ³•åœ¨çœŸå®ä¸–ç•Œæ¨¡å‹çš„å¤§è§„æ¨¡åº”ç”¨å¥ å®šäº†åŸºç¡€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.10721">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c0fd5c032db06cd6116049f085e67e39" align="middle">
<img src="https://picx.zhimg.com/v2-fcd69bba8f329912f01529208e4f4a07" align="middle">
<img src="https://picx.zhimg.com/v2-1bf5d387b82b5e3a5b854b3597edf70e" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="STELLAR-Scene-Text-Editor-for-Low-Resource-Languages-and-Real-World-Data"><a href="#STELLAR-Scene-Text-Editor-for-Low-Resource-Languages-and-Real-World-Data" class="headerlink" title="STELLAR: Scene Text Editor for Low-Resource Languages and Real-World Data"></a>STELLAR: Scene Text Editor for Low-Resource Languages and Real-World Data</h2><p><strong>Authors:Yongdeuk Seo, Hyun-seok Min, Sungchul Choi</strong></p>
<p>Scene Text Editing (STE) is the task of modifying text content in an image while preserving its visual style, such as font, color, and background. While recent diffusion-based approaches have shown improvements in visual quality, key limitations remain: lack of support for low-resource languages, domain gap between synthetic and real data, and the absence of appropriate metrics for evaluating text style preservation. To address these challenges, we propose STELLAR (Scene Text Editor for Low-resource LAnguages and Real-world data). STELLAR enables reliable multilingual editing through a language-adaptive glyph encoder and a multi-stage training strategy that first pre-trains on synthetic data and then fine-tunes on real images. We also construct a new dataset, STIPLAR(Scene Text Image Pairs of Low-resource lAnguages and Real-world data), for training and evaluation. Furthermore, we propose Text Appearance Similarity (TAS), a novel metric that assesses style preservation by independently measuring font, color, and background similarity, enabling robust evaluation even without ground truth. Experimental results demonstrate that STELLAR outperforms state-of-the-art models in visual consistency and recognition accuracy, achieving an average TAS improvement of 2.2% across languages over the baselines.</p>
<blockquote>
<p>åœºæ™¯æ–‡æœ¬ç¼–è¾‘ï¼ˆSTEï¼‰æ˜¯åœ¨ä¿ç•™å›¾åƒè§†è§‰é£æ ¼ï¼ˆå¦‚å­—ä½“ã€é¢œè‰²å’ŒèƒŒæ™¯ï¼‰çš„åŒæ—¶ï¼Œä¿®æ”¹å›¾åƒä¸­çš„æ–‡æœ¬å†…å®¹ã€‚è™½ç„¶æœ€è¿‘çš„æ‰©æ•£æ–¹æ³•åœ¨æé«˜è§†è§‰è´¨é‡æ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†ä»å­˜åœ¨å…³é”®é™åˆ¶ï¼šä¸æ”¯æŒä½èµ„æºè¯­è¨€ã€åˆæˆæ•°æ®å’ŒçœŸå®æ•°æ®ä¹‹é—´çš„é¢†åŸŸå·®è·ï¼Œä»¥åŠç¼ºä¹è¯„ä¼°æ–‡æœ¬é£æ ¼ä¿ç•™çš„åˆé€‚æŒ‡æ ‡ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†STELLARï¼ˆåœºæ™¯æ–‡æœ¬ç¼–è¾‘å™¨ï¼Œé€‚ç”¨äºä½èµ„æºè¯­è¨€å’ŒçœŸå®ä¸–ç•Œæ•°æ®ï¼‰ã€‚STELLARé€šè¿‡è¯­è¨€è‡ªé€‚åº”çš„ç¬¦å·ç¼–ç å™¨ä»¥åŠåˆ†é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œå®ç°äº†å¯é çš„å¤šè¯­è¨€ç¼–è¾‘ã€‚è¯¥ç­–ç•¥é¦–å…ˆè¿›è¡Œåˆæˆæ•°æ®é¢„è®­ç»ƒï¼Œç„¶ååœ¨çœŸå®å›¾åƒä¸Šè¿›è¡Œå¾®è°ƒã€‚æˆ‘ä»¬è¿˜æ„å»ºäº†ç”¨äºè®­ç»ƒå’Œè¯„ä¼°çš„æ–°æ•°æ®é›†STIPLARï¼ˆåœºæ™¯æ–‡æœ¬å›¾åƒå¯¹ï¼Œé€‚ç”¨äºä½èµ„æºè¯­è¨€å’ŒçœŸå®ä¸–ç•Œæ•°æ®ï¼‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†æ–‡æœ¬å¤–è§‚ç›¸ä¼¼æ€§ï¼ˆTASï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„æŒ‡æ ‡ï¼Œé€šè¿‡ç‹¬ç«‹æµ‹é‡å­—ä½“ã€é¢œè‰²å’ŒèƒŒæ™¯ç›¸ä¼¼æ€§æ¥è¯„ä¼°é£æ ¼ä¿ç•™æƒ…å†µï¼Œå³ä½¿åœ¨æ²¡æœ‰çœŸå®å€¼çš„æƒ…å†µä¸‹ä¹Ÿèƒ½å®ç°ç¨³å¥è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSTELLARåœ¨è§†è§‰ä¸€è‡´æ€§å’Œè¯†åˆ«å‡†ç¡®æ€§æ–¹é¢ä¼˜äºæœ€æ–°æ¨¡å‹ï¼Œä¸åŸºçº¿ç›¸æ¯”ï¼Œè·¨è¯­è¨€çš„å¹³å‡TASæ”¹è¿›äº†2.2%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.09977v2">PDF</a> Accepted to AAAI 2026 Workshop (Artificial Intelligence with Biased or Scarce Data)</p>
<p><strong>Summary</strong></p>
<p>åŸºäºåœºæ™¯æ–‡æœ¬ç¼–è¾‘ï¼ˆSTEï¼‰çš„ä»»åŠ¡æ˜¯ä¿®æ”¹å›¾åƒä¸­çš„æ–‡æœ¬å†…å®¹ï¼ŒåŒæ—¶ä¿ç•™å…¶è§†è§‰é£æ ¼ï¼Œå¦‚å­—ä½“ã€é¢œè‰²å’ŒèƒŒæ™¯ã€‚é’ˆå¯¹ä½èµ„æºè¯­è¨€ã€åˆæˆä¸ç°å®æ•°æ®ä¹‹é—´çš„åŸŸå·®è·ä»¥åŠç¼ºä¹é€‚å½“çš„æ–‡æœ¬é£æ ¼ä¿ç•™è¯„ä¼°æŒ‡æ ‡ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†STELLARï¼ˆåœºæ™¯æ–‡æœ¬ç¼–è¾‘å™¨ï¼Œç”¨äºä½èµ„æºè¯­è¨€å’Œç°å®æ•°æ®ï¼‰ã€‚STELLARé€šè¿‡è¯­è¨€è‡ªé€‚åº”å­—å½¢ç¼–ç å™¨å’Œå¤šé˜¶æ®µè®­ç»ƒç­–ç•¥å®ç°å¯é çš„å¤šè¯­è¨€ç¼–è¾‘ï¼Œå…ˆåˆæˆæ•°æ®è¿›è¡Œé¢„è®­ç»ƒï¼Œå†åœ¨çœŸå®å›¾åƒä¸Šè¿›è¡Œå¾®è°ƒã€‚æ­¤å¤–ï¼Œæ„å»ºäº†STIPLARï¼ˆåœºæ™¯æ–‡æœ¬å›¾åƒå¯¹ä½èµ„æºè¯­è¨€å’Œç°å®æ•°æ®ï¼‰æ–°æ•°æ®é›†ç”¨äºè®­ç»ƒå’Œè¯„ä¼°ã€‚è¿˜æå‡ºäº†Text Appearance Similarityï¼ˆTASï¼‰è¿™ä¸€æ–°æŒ‡æ ‡ï¼Œé€šè¿‡ç‹¬ç«‹æµ‹é‡å­—ä½“ã€é¢œè‰²å’ŒèƒŒæ™¯ç›¸ä¼¼æ€§æ¥è¯„ä¼°é£æ ¼ä¿ç•™æƒ…å†µï¼Œå³ä½¿åœ¨æ— çœŸå®æ•°æ®çš„æƒ…å†µä¸‹ä¹Ÿèƒ½è¿›è¡Œç¨³å¥è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSTELLARåœ¨è§†è§‰ä¸€è‡´æ€§å’Œè¯†åˆ«å‡†ç¡®ç‡æ–¹é¢ä¼˜äºæœ€æ–°æ¨¡å‹ï¼Œå¹³å‡è·¨è¯­è¨€ TAS è¾ƒåŸºçº¿æé«˜äº† 2.2%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>STELLARæ—¨åœ¨è§£å†³åœºæ™¯æ–‡æœ¬ç¼–è¾‘ä¸­çš„æŒ‘æˆ˜ï¼Œæ”¯æŒä½èµ„æºè¯­è¨€å¹¶é€‚ç”¨äºç°å®æ•°æ®ã€‚</li>
<li>é€šè¿‡è¯­è¨€è‡ªé€‚åº”å­—å½¢ç¼–ç å™¨å®ç°å¯é çš„å¤šè¯­è¨€ç¼–è¾‘ã€‚</li>
<li>é‡‡ç”¨å¤šé˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œå…ˆåœ¨åˆæˆæ•°æ®ä¸Šé¢„è®­ç»ƒï¼Œå†åœ¨çœŸå®å›¾åƒä¸Šå¾®è°ƒã€‚</li>
<li>æ„å»ºäº†STIPLARæ•°æ®é›†ç”¨äºè®­ç»ƒå’Œè¯„ä¼°åœºæ™¯æ–‡æœ¬ç¼–è¾‘æ¨¡å‹ã€‚</li>
<li>æå‡ºäº†Text Appearance Similarity (TAS)è¿™ä¸€æ–°æŒ‡æ ‡ï¼Œç”¨äºè¯„ä¼°æ–‡æœ¬é£æ ¼ä¿ç•™æƒ…å†µã€‚</li>
<li>å®éªŒæ˜¾ç¤ºï¼ŒSTELLARåœ¨è§†è§‰ä¸€è‡´æ€§å’Œè¯†åˆ«å‡†ç¡®ç‡ä¸Šè¾ƒç°æœ‰æ¨¡å‹æœ‰æ˜¾è‘—æå‡ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.09977">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c75df4ec0433132bb967612e121a4631" align="middle">
<img src="https://picx.zhimg.com/v2-f9bf46ca7d66fba301745ac493ae8101" align="middle">
<img src="https://picx.zhimg.com/v2-55d27f2bac797deafff96fa8c996dfe4" align="middle">
<img src="https://picx.zhimg.com/v2-c5db1faee7467269ad81cf16729fde90" align="middle">
<img src="https://picx.zhimg.com/v2-3b3efe257d648440fc2f52a020495990" align="middle">
<img src="https://picx.zhimg.com/v2-202ae2f6204a7cb25b147630c6ebb71f" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Self-Diffusion-Driven-Blind-Imaging"><a href="#Self-Diffusion-Driven-Blind-Imaging" class="headerlink" title="Self-Diffusion Driven Blind Imaging"></a>Self-Diffusion Driven Blind Imaging</h2><p><strong>Authors:Yanlong Yang, Guanxiong Luo</strong></p>
<p>Optical imaging systems are inherently imperfect due to diffraction limits, lens manufacturing tolerances, assembly misalignment, and other physical constraints. In addition, unavoidable camera shake and object motion further introduce non-ideal degradations during acquisition. These aberrations and motion-induced variations are typically unknown, difficult to measure, and costly to model or calibrate in practice. Blind inverse problems offer a promising direction by jointly estimating both the latent image and the unknown degradation kernel. However, existing approaches often suffer from convergence instability, limited prior expressiveness, and sensitivity to hyperparameters. Inspired by recent advances in self-diffusion, we propose DeblurSDI, a zero-shot, self-supervised blind imaging framework that requires no pre-training. DeblurSDI formulates blind image recovery as an iterative reverse self-diffusion process that begins from pure noise and progressively refines both the sharp image and the blur kernel. Extensive experiments on combined optical aberrations and motion blur demonstrate that DeblurSDI consistently outperforms other methods by a substantial margin.</p>
<blockquote>
<p>å…‰å­¦æˆåƒç³»ç»Ÿç”±äºè¡å°„æé™ã€é•œå¤´åˆ¶é€ å…¬å·®ã€è£…é…å¤±è°ƒç­‰ç‰©ç†çº¦æŸè€Œå¤©ç”Ÿä¸å®Œç¾ã€‚æ­¤å¤–ï¼Œä¸å¯é¿å…çš„ç›¸æœºæŠ–åŠ¨å’Œç‰©ä½“è¿åŠ¨ä¼šåœ¨é‡‡é›†è¿‡ç¨‹ä¸­è¿›ä¸€æ­¥å¼•å…¥éç†æƒ³é€€åŒ–ã€‚è¿™äº›ç•¸å˜å’Œè¿åŠ¨å¼•èµ·çš„å˜åŒ–é€šå¸¸æ˜¯æœªçŸ¥çš„ï¼Œéš¾ä»¥æµ‹é‡ï¼Œå¹¶ä¸”åœ¨å®è·µä¸­å¯¹å»ºæ¨¡æˆ–æ ¡å‡†çš„æˆæœ¬å¾ˆé«˜ã€‚ç›²åé—®é¢˜é€šè¿‡è”åˆä¼°è®¡æ½œåœ¨å›¾åƒå’ŒæœªçŸ¥çš„é€€åŒ–æ ¸æä¾›äº†ä¸€ä¸ªæœ‰å‰é€”çš„æ–¹å‘ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¸¸å¸¸å­˜åœ¨æ”¶æ•›ä¸ç¨³å®šã€å…ˆéªŒè¡¨è¾¾æœ‰é™ä»¥åŠå¯¹è¶…å‚æ•°æ•æ„Ÿç­‰é—®é¢˜ã€‚å—åˆ°è‡ªæ‰©æ•£æœ€æ–°è¿›å±•çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†DeblurSDIï¼Œè¿™æ˜¯ä¸€ä¸ªé›¶æ ·æœ¬ã€è‡ªç›‘ç£çš„ç›²æˆåƒæ¡†æ¶ï¼Œæ— éœ€é¢„å…ˆè®­ç»ƒã€‚DeblurSDIå°†ç›²å›¾åƒæ¢å¤åˆ¶å®šä¸ºä¸€ä¸ªä»çº¯å™ªå£°å¼€å§‹çš„è¿­ä»£åå‘è‡ªæ‰©æ•£è¿‡ç¨‹ï¼Œè¯¥è¿‡ç¨‹é€æ­¥ç»†åŒ–é”åˆ©çš„å›¾åƒå’Œæ¨¡ç³Šæ ¸ã€‚åœ¨å…‰å­¦ç•¸å˜å’Œè¿åŠ¨æ¨¡ç³Šç›¸ç»“åˆçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒDeblurSDIåœ¨å„æ–¹é¢å‡å¤§å¹…è¶…è¶Šå…¶ä»–æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.27439v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å…‰å­¦æˆåƒç³»ç»Ÿç”±äºè¡å°„æé™ã€é•œå¤´åˆ¶é€ å…¬å·®ã€è£…é…å¤±å‡†ä»¥åŠå…¶ä»–ç‰©ç†çº¦æŸè€Œå¤©ç”Ÿå­˜åœ¨ç¼ºé™·ã€‚æ­¤å¤–ï¼Œä¸å¯é¿å…çš„ç›¸æœºæŠ–åŠ¨å’Œç‰©ä½“è¿åŠ¨ä¼šåœ¨é‡‡é›†è¿‡ç¨‹ä¸­è¿›ä¸€æ­¥å¼•å…¥éç†æƒ³é€€åŒ–ã€‚è¿™äº›åƒå·®å’Œè¿åŠ¨å¼•èµ·çš„å˜åŒ–é€šå¸¸æ˜¯æœªçŸ¥çš„ï¼Œéš¾ä»¥æµ‹é‡ï¼Œå¹¶ä¸”åœ¨å®è·µä¸­å¯¹å»ºæ¨¡æˆ–æ ¡å‡†çš„æˆæœ¬å¾ˆé«˜ã€‚ç›²åé—®é¢˜æä¾›äº†ä¸€ç§æœ‰å‰æ™¯çš„æ–¹å‘ï¼Œå¯ä»¥è”åˆä¼°è®¡æ½œåœ¨å›¾åƒå’ŒæœªçŸ¥çš„é€€åŒ–å†…æ ¸ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¸¸å¸¸å­˜åœ¨æ”¶æ•›ä¸ç¨³å®šã€å…ˆéªŒè¡¨ç°åŠ›æœ‰é™ä»¥åŠå¯¹è¶…å‚æ•°æ•æ„Ÿçš„é—®é¢˜ã€‚å—è‡ªæ‰©æ•£æœ€æ–°è¿›å±•çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†DeblurSDIï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€é¢„è®­ç»ƒçš„é›¶æ ·æœ¬ã€è‡ªç›‘ç£ç›²æˆåƒæ¡†æ¶ã€‚DeblurSDIå°†ç›²å›¾åƒæ¢å¤åˆ¶å®šä¸ºä¸€ä¸ªä»çº¯å™ªå£°å¼€å§‹çš„è¿­ä»£åå‘è‡ªæ‰©æ•£è¿‡ç¨‹ï¼Œé€æ­¥ç²¾ç»†åŒ–æ¸…æ™°å›¾åƒå’Œæ¨¡ç³Šå†…æ ¸ã€‚åœ¨å…‰å­¦åƒå·®å’Œè¿åŠ¨æ¨¡ç³Šçš„æ··åˆå®éªŒä¸Šï¼ŒDeblurSDIå§‹ç»ˆå¤§å¹…ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å…‰å­¦æˆåƒç³»ç»Ÿå­˜åœ¨å¤šç§å›ºæœ‰ç¼ºé™·ï¼ŒåŒ…æ‹¬ç‰©ç†çº¦æŸå’Œåƒå·®ã€‚</li>
<li>ç›²åé—®é¢˜åœ¨ä¼°è®¡æ½œåœ¨å›¾åƒå’ŒæœªçŸ¥é€€åŒ–å†…æ ¸æ–¹é¢å…·æœ‰æ½œåŠ›ã€‚</li>
<li>ç°æœ‰æ–¹æ³•åœ¨å¤„ç†ç›²åé—®é¢˜æ—¶å­˜åœ¨æ”¶æ•›ä¸ç¨³å®šç­‰ç¼ºé™·ã€‚</li>
<li>DeblurSDIæ˜¯ä¸€ä¸ªè‡ªç›‘ç£çš„ç›²æˆåƒæ¡†æ¶ï¼Œé€šè¿‡è¿­ä»£åå‘è‡ªæ‰©æ•£è¿‡ç¨‹æ¢å¤å›¾åƒã€‚</li>
<li>DeblurSDIä¸éœ€è¦é¢„è®­ç»ƒï¼Œä¸”èƒ½æœ‰æ•ˆå¤„ç†å…‰å­¦åƒå·®å’Œè¿åŠ¨æ¨¡ç³Šã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒDeblurSDIåœ¨æ€§èƒ½ä¸Šå¤§å¹…è¶…è¶Šäº†å…¶ä»–æ–¹æ³•ã€‚</li>
<li>DeblurSDIçš„æ–¹æ³•æ˜¯åŸºäºè‡ªæ‰©æ•£çš„æœ€æ–°è¿›å±•ï¼Œé€šè¿‡é€æ­¥ç²¾ç»†åŒ–æ¸…æ™°å›¾åƒå’Œæ¨¡ç³Šå†…æ ¸æ¥æ¢å¤å›¾åƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.27439">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f3bc12e5c9f00c2346c1096b901ec845" align="middle">
<img src="https://picx.zhimg.com/v2-f06a060158738f3a6f5846670947f620" align="middle">
<img src="https://picx.zhimg.com/v2-e48b973d8f317c78c7847bd7a46b8832" align="middle">
<img src="https://picx.zhimg.com/v2-565574dd85f7bee88e4c9029f5cee54a" align="middle">
<img src="https://picx.zhimg.com/v2-17d5bc3a89531c39abe59e1fe177016a" align="middle">
<img src="https://picx.zhimg.com/v2-2c5742cab1c5ca37ded6367cd05be970" align="middle">
<img src="https://picx.zhimg.com/v2-064d088a220348916445f1be3a55099e" align="middle">
<img src="https://picx.zhimg.com/v2-1938658107a74287dec3bea13510e6c4" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Generative-AI-in-Map-Making-A-Technical-Exploration-and-Its-Implications-for-Cartographers"><a href="#Generative-AI-in-Map-Making-A-Technical-Exploration-and-Its-Implications-for-Cartographers" class="headerlink" title="Generative AI in Map-Making: A Technical Exploration and Its Implications for Cartographers"></a>Generative AI in Map-Making: A Technical Exploration and Its Implications for Cartographers</h2><p><strong>Authors:Claudio Affolter, Sidi Wu, Yizi Chen, Lorenz Hurni</strong></p>
<p>Traditional map-making relies heavily on Geographic Information Systems (GIS), requiring domain expertise and being time-consuming, especially for repetitive tasks. Recent advances in generative AI (GenAI), particularly image diffusion models, offer new opportunities for automating and democratizing the map-making process. However, these models struggle with accurate map creation due to limited control over spatial composition and semantic layout. To address this, we integrate vector data to guide map generation in different styles, specified by the textual prompts. Our model is the first to generate accurate maps in controlled styles, and we have integrated it into a web application to improve its usability and accessibility. We conducted a user study with professional cartographers to assess the fidelity of generated maps, the usability of the web application, and the implications of ever-emerging GenAI in map-making. The findings have suggested the potential of our developed application and, more generally, the GenAI models in helping both non-expert users and professionals in creating maps more efficiently. We have also outlined further technical improvements and emphasized the new role of cartographers to advance the paradigm of AI-assisted map-making. The code and pre-trained models are available at <a target="_blank" rel="noopener" href="https://github.com/claudaff/generative-ai-mapmaking/">https://github.com/claudaff/generative-ai-mapmaking/</a>.</p>
<blockquote>
<p>ä¼ ç»Ÿçš„åœ°å›¾åˆ¶ä½œä¸¥é‡ä¾èµ–äºåœ°ç†ä¿¡æ¯ç³»ç»Ÿï¼ˆGISï¼‰ï¼Œéœ€è¦é¢†åŸŸä¸“ä¸šçŸ¥è¯†ï¼Œå¹¶ä¸”è€—æ—¶ï¼Œå°¤å…¶æ˜¯å¯¹äºé‡å¤ä»»åŠ¡ã€‚æœ€è¿‘ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ï¼ˆGenAIï¼‰çš„è¿›å±•ï¼Œå°¤å…¶æ˜¯å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œä¸ºè‡ªåŠ¨åŒ–å’Œæ°‘ä¸»åŒ–åœ°å›¾åˆ¶ä½œæµç¨‹æä¾›äº†æ–°çš„æœºä¼šã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨åˆ›å»ºå‡†ç¡®åœ°å›¾æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œå› ä¸ºå¯¹ç©ºé—´ç»„æˆå’Œè¯­ä¹‰å¸ƒå±€çš„æ§åˆ¶æœ‰é™ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å°†çŸ¢é‡æ•°æ®é›†æˆåˆ°åœ°å›¾ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œé€šè¿‡æ–‡æœ¬æç¤ºå¼•å¯¼ä¸åŒé£æ ¼çš„åœ°å›¾ç”Ÿæˆã€‚æˆ‘ä»¬çš„æ¨¡å‹æ˜¯é¦–ä¸ªèƒ½å¤Ÿç”Ÿæˆæ§åˆ¶é£æ ¼å‡†ç¡®åœ°å›¾çš„æ¨¡å‹ï¼Œæˆ‘ä»¬å°†å®ƒé›†æˆåˆ°ä¸€ä¸ªwebåº”ç”¨ç¨‹åºä¸­ï¼Œä»¥æé«˜å…¶æ˜“ç”¨æ€§å’Œå¯è®¿é—®æ€§ã€‚æˆ‘ä»¬ä¸ä¸“ä¸šåˆ¶å›¾å¸ˆè¿›è¡Œäº†ä¸€é¡¹ç”¨æˆ·ç ”ç©¶ï¼Œä»¥è¯„ä¼°ç”Ÿæˆåœ°å›¾çš„ä¿çœŸåº¦ã€webåº”ç”¨ç¨‹åºçš„æ˜“ç”¨æ€§ä»¥åŠä¸æ–­æ¶Œç°çš„GenAIåœ¨åœ°å›¾åˆ¶ä½œä¸­çš„æ„ä¹‰ã€‚ç ”ç©¶ç»“æœè¡¨æ˜æˆ‘ä»¬å¼€å‘çš„åº”ç”¨ç¨‹åºå…·æœ‰æ½œåŠ›ï¼Œæ›´æ™®éåœ°è¯´ï¼ŒGenAIæ¨¡å‹æœ‰åŠ©äºéä¸“ä¸šç”¨æˆ·å’Œä¸“å®¶æ›´é«˜æ•ˆåœ°åˆ›å»ºåœ°å›¾ã€‚æˆ‘ä»¬è¿˜æ¦‚è¿°äº†è¿›ä¸€æ­¥çš„æŠ€æœ¯æ”¹è¿›å’Œå¼ºè°ƒäº†åˆ¶å›¾å¸ˆåœ¨æ¨è¿›AIè¾…åŠ©åœ°å›¾åˆ¶ä½œèŒƒå¼ä¸­çš„æ–°ä½œç”¨ã€‚ä»£ç å’Œé¢„å…ˆè®­ç»ƒçš„æ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/claudaff/generative-ai-mapmaking/%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/claudaff/generative-ai-mapmaking/è·å¾—ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.18959v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åˆ©ç”¨å›¾åƒæ‰©æ•£æ¨¡å‹ä¸çŸ¢é‡æ•°æ®ç»“åˆçš„æ–¹æ³•ï¼Œé€šè¿‡æ–‡æœ¬æç¤ºæ¥å¼•å¯¼åœ°å›¾çš„ç”Ÿæˆã€‚æ­¤æ–¹æ³•å¯åœ¨ä¸åŒé£æ ¼ä¸‹ç”Ÿæˆå‡†ç¡®çš„åœ°å›¾ï¼Œå¹¶é›†æˆåˆ°webåº”ç”¨ç¨‹åºä¸­ä»¥æé«˜å¯ç”¨æ€§å’Œå¯è®¿é—®æ€§ã€‚ç”¨æˆ·ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè¯¥åº”ç”¨ç¨‹åºå…·æœ‰æ½œåŠ›ï¼Œæ–°å…´çš„äººå·¥æ™ºèƒ½æ¨¡å‹æœ‰åŠ©äºéä¸“ä¸šäººå£«å’Œä¸“ä¸šäººå£«æ›´æœ‰æ•ˆåœ°åˆ›å»ºåœ°å›¾ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¼ ç»Ÿçš„åœ°å›¾åˆ¶ä½œä¾èµ–äºåœ°ç†ä¿¡æ¯ç³»ç»Ÿï¼ˆGISï¼‰ï¼Œéœ€è¦å¤§é‡ä¸“ä¸šçŸ¥è¯†ä¸”è€—æ—¶è€—åŠ›ã€‚</li>
<li>ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ï¼ˆGenAIï¼‰åœ¨è‡ªåŠ¨åŒ–å’Œæ°‘ä¸»åŒ–åœ°å›¾åˆ¶ä½œè¿‡ç¨‹ä¸­æä¾›äº†æ–°çš„æœºä¼šã€‚</li>
<li>å›¾åƒæ‰©æ•£æ¨¡å‹åœ¨å‡†ç¡®åˆ›å»ºåœ°å›¾æ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œç¼ºä¹ç©ºé—´æ„å›¾å’Œè¯­ä¹‰å¸ƒå±€çš„æ§åˆ¶ã€‚</li>
<li>é€šè¿‡ç»“åˆçŸ¢é‡æ•°æ®å’Œå¼•å¯¼åœ°å›¾ç”Ÿæˆçš„æ–‡æœ¬æç¤ºï¼Œå¯ä»¥ç”Ÿæˆå‡†ç¡®ä¸”é£æ ¼å¯æ§çš„åœ°å›¾ã€‚</li>
<li>é›†æˆæ¨¡å‹åˆ°webåº”ç”¨ç¨‹åºæé«˜äº†å…¶å¯ç”¨æ€§å’Œå¯è®¿é—®æ€§ã€‚</li>
<li>ç”¨æˆ·ç ”ç©¶è¯„ä¼°äº†ç”Ÿæˆåœ°å›¾çš„ä¿çœŸåº¦ã€webåº”ç”¨ç¨‹åºçš„å¯ç”¨æ€§ä»¥åŠæ–°å…´GenAIåœ¨åœ°å›¾åˆ¶ä½œä¸­çš„å½±å“ï¼Œæ˜¾ç¤ºå‡ºè¯¥åº”ç”¨ç¨‹åºçš„æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.18959">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-12c0fa8015ce70adc3ff5066fbb439c6" align="middle">
<img src="https://picx.zhimg.com/v2-c1106355df201c36dcdc6568323ca15b" align="middle">
<img src="https://picx.zhimg.com/v2-03b4333291d36addc7163347365605c1" align="middle">
<img src="https://picx.zhimg.com/v2-d4565a8f08584731cb9f304c6990a50f" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="MILD-Multi-Layer-Diffusion-Strategy-for-Complex-and-Precise-Multi-IP-Aware-Human-Erasing"><a href="#MILD-Multi-Layer-Diffusion-Strategy-for-Complex-and-Precise-Multi-IP-Aware-Human-Erasing" class="headerlink" title="MILD: Multi-Layer Diffusion Strategy for Complex and Precise Multi-IP Aware Human Erasing"></a>MILD: Multi-Layer Diffusion Strategy for Complex and Precise Multi-IP Aware Human Erasing</h2><p><strong>Authors:Jinghan Yu, Junhao Xiao, Zhiyuan Ma, Yue Ma, Kaiqi Liu, Yuhan Wang, Daizong Liu, Xianghao Meng, Jianjun Li</strong></p>
<p>Recent years have witnessed the success of diffusion models in image customization tasks. However, existing mask-guided human erasing methods still struggle in complex scenarios such as human-human occlusion, human-object entanglement, and human-background interference, mainly due to the lack of large-scale multi-instance datasets and effective spatial decoupling to separate foreground from background. To bridge these gaps, we curate the MILD dataset capturing diverse poses, occlusions, and complex multi-instance interactions. We then define the Cross-Domain Attention Gap (CAG), an attention-gap metric to quantify semantic leakage. On top of these, we propose Multi-Layer Diffusion (MILD), which decomposes the generation process into independent denoising pathways, enabling separate reconstruction of each foreground instance and the background. To enhance human-centric understanding, we introduce Human Morphology Guidance, a plug-and-play module that incorporates pose, parsing, and spatial relationships into the diffusion process to improve structural awareness and restoration quality. Additionally, we present Spatially-Modulated Attention, an adaptive mechanism that leverages spatial mask priors to modulate attention across semantic regions, further widening the CAG to effectively minimize boundary artifacts and mitigate semantic leakage. Experiments show that MILD significantly outperforms existing methods. Datasets and code are publicly available at: <a target="_blank" rel="noopener" href="https://mild-multi-layer-diffusion.github.io/">https://mild-multi-layer-diffusion.github.io/</a>.</p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œæ‰©æ•£æ¨¡å‹åœ¨å›¾åƒå®šåˆ¶ä»»åŠ¡ä¸­å–å¾—äº†æˆåŠŸã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºäºæ©è†œçš„æ“¦é™¤æ–¹æ³•åœ¨å¤æ‚åœºæ™¯ï¼ˆå¦‚äººä¸äººé®æŒ¡ã€äººä¸ç‰©ä½“çº ç¼ ä»¥åŠäººä¸èƒŒæ™¯å¹²æ‰°ï¼‰ä¸­ä»é¢ä¸´æŒ‘æˆ˜ï¼Œè¿™ä¸»è¦æ˜¯å› ä¸ºç¼ºä¹å¤§è§„æ¨¡å¤šå®ä¾‹æ•°æ®é›†ä»¥åŠæœ‰æ•ˆçš„ç©ºé—´è§£è€¦æŠ€æœ¯æ¥åˆ†ç¦»å‰æ™¯å’ŒèƒŒæ™¯ã€‚ä¸ºäº†å¡«è¡¥è¿™äº›ç©ºç™½ï¼Œæˆ‘ä»¬ç­–åˆ’äº†MILDæ•°æ®é›†ï¼Œæ•æ‰å¤šæ ·åŒ–çš„å§¿åŠ¿ã€é®æŒ¡ä»¥åŠå¤æ‚çš„å¤šå®ä¾‹äº¤äº’ã€‚éšåï¼Œæˆ‘ä»¬å®šä¹‰äº†è·¨åŸŸæ³¨æ„åŠ›å·®è·ï¼ˆCAGï¼‰ï¼Œè¿™æ˜¯ä¸€ç§è¡¡é‡è¯­ä¹‰æ³„æ¼çš„æ³¨æ„åŠ›å·®è·æŒ‡æ ‡ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æå‡ºäº†å¤šå±‚æ‰©æ•£ï¼ˆMILDï¼‰æ¨¡å‹ï¼Œå°†ç”Ÿæˆè¿‡ç¨‹åˆ†è§£ä¸ºç‹¬ç«‹çš„å»å™ªè·¯å¾„ï¼Œä»è€Œå®ç°æ¯ä¸ªå‰æ™¯å®ä¾‹å’ŒèƒŒæ™¯çš„å•ç‹¬é‡å»ºã€‚ä¸ºäº†å¢å¼ºä»¥äººä¸ºä¸­å¿ƒçš„ç†è§£ï¼Œæˆ‘ä»¬å¼•å…¥äº†äººç±»å½¢æ€å­¦æŒ‡å¯¼ï¼ˆHuman Morphology Guidanceï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªå³æ’å³ç”¨æ¨¡å—ï¼Œå°†å§¿åŠ¿ã€è§£æå’Œç©ºé—´å…³ç³»èå…¥æ‰©æ•£è¿‡ç¨‹ï¼Œä»¥æé«˜ç»“æ„æ„è¯†å’Œæ¢å¤è´¨é‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¨å‡ºäº†ç©ºé—´è°ƒåˆ¶æ³¨æ„åŠ›ï¼ˆSpatially-Modulated Attentionï¼‰ï¼Œè¿™æ˜¯ä¸€ç§è‡ªé€‚åº”æœºåˆ¶ï¼Œåˆ©ç”¨ç©ºé—´æ©è†œå…ˆéªŒæ¥è°ƒåˆ¶è¯­ä¹‰åŒºåŸŸçš„æ³¨æ„åŠ›ï¼Œè¿›ä¸€æ­¥æ‹“å®½CAGä»¥æœ‰æ•ˆå‡å°‘è¾¹ç•Œä¼ªå½±å¹¶ç¼“è§£è¯­ä¹‰æ³„æ¼ã€‚å®éªŒè¡¨æ˜ï¼ŒMILDæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚æ•°æ®é›†å’Œä»£ç å¯åœ¨ä»¥ä¸‹ç½‘å€å…¬å¼€è·å–ï¼š<a target="_blank" rel="noopener" href="https://mild-multi-layer-diffusion.github.io/">https://mild-multi-layer-diffusion.github.io/</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.06543v2">PDF</a> </p>
<p><strong>Summary</strong><br>     æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒå®šåˆ¶ä»»åŠ¡ä¸­å–å¾—äº†æˆåŠŸï¼Œä½†ä»é¢ä¸´å¤æ‚åœºæ™¯ä¸‹çš„æŒ‘æˆ˜ï¼Œå¦‚äººä¸äººé®æŒ¡ã€äººä¸ç‰©ä½“çº ç¼ ä»¥åŠäººä¸èƒŒæ™¯å¹²æ‰°ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬åˆ›å»ºäº†MILDæ•°æ®é›†å¹¶å®šä¹‰äº†è·¨åŸŸæ³¨æ„åŠ›é—´éš™ï¼ˆCAGï¼‰æŒ‡æ ‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†å¤šå±‚æ‰©æ•£æ¨¡å‹ï¼ˆMILDï¼‰ï¼Œå°†å…¶åˆ†è§£ä¸ºç‹¬ç«‹çš„å»å™ªè·¯å¾„ï¼Œèƒ½å¤Ÿåˆ†åˆ«é‡å»ºæ¯ä¸ªå‰æ™¯å®ä¾‹å’ŒèƒŒæ™¯ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†äººç±»å½¢æ€æŒ‡å¯¼æ¨¡å—å’Œç©ºé—´è°ƒåˆ¶æ³¨æ„åŠ›æœºåˆ¶ï¼Œä»¥æé«˜ç»“æ„æ„è¯†å’Œæ¢å¤è´¨é‡ï¼Œå¹¶æœ‰æ•ˆå‡å°‘è¾¹ç•Œä¼ªå½±å’Œè¯­ä¹‰æ³„æ¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒå®šåˆ¶ä»»åŠ¡ä¸­å–å¾—æ˜¾è‘—æˆåŠŸã€‚</li>
<li>ç°æœ‰çš„äººæ“¦é™¤æ–¹æ³•åœ¨å¤æ‚åœºæ™¯ä¸­ä»æœ‰æŒ‘æˆ˜ï¼Œå¦‚äººä¸äººé®æŒ¡ç­‰é—®é¢˜ã€‚</li>
<li>ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œåˆ›å»ºäº†MILDæ•°æ®é›†ï¼Œç”¨äºæ•æ‰å¤šæ ·çš„å§¿åŠ¿ã€é®æŒ¡å’Œå¤æ‚çš„å¤šå®ä¾‹äº¤äº’ã€‚</li>
<li>å®šä¹‰äº†è·¨åŸŸæ³¨æ„åŠ›é—´éš™ï¼ˆCAGï¼‰æŒ‡æ ‡ï¼Œç”¨äºé‡åŒ–è¯­ä¹‰æ³„æ¼ã€‚</li>
<li>æå‡ºäº†å¤šå±‚æ‰©æ•£æ¨¡å‹ï¼ˆMILDï¼‰ï¼Œèƒ½åˆ†åˆ«é‡å»ºå‰æ™¯å®ä¾‹å’ŒèƒŒæ™¯ã€‚</li>
<li>å¼•å…¥äº†äººç±»å½¢æ€æŒ‡å¯¼æ¨¡å—ï¼Œç»“åˆå§¿åŠ¿ã€è§£æå’Œç©ºé—´å…³ç³»ï¼Œæé«˜ç»“æ„æ„è¯†å’Œæ¢å¤è´¨é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.06543">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d3206ca0158bec41ddfba9edbbff37f1" align="middle">
<img src="https://picx.zhimg.com/v2-461ebdb1cd3d8b84a5c21d597e77b86b" align="middle">
<img src="https://picx.zhimg.com/v2-65799cc7f5f8a6a85f0fddd4e0ec330c" align="middle">
<img src="https://picx.zhimg.com/v2-c11dd8407aee4fcd68f63645c0b7289b" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Rethinking-Target-Label-Conditioning-in-Adversarial-Attacks-A-2D-Tensor-Guided-Generative-Approach"><a href="#Rethinking-Target-Label-Conditioning-in-Adversarial-Attacks-A-2D-Tensor-Guided-Generative-Approach" class="headerlink" title="Rethinking Target Label Conditioning in Adversarial Attacks: A 2D Tensor-Guided Generative Approach"></a>Rethinking Target Label Conditioning in Adversarial Attacks: A 2D Tensor-Guided Generative Approach</h2><p><strong>Authors:Hangyu Liu, Bo Peng, Pengxiang Ding, Donglin Wang</strong></p>
<p>Compared to single-target adversarial attacks, multi-target attacks have garnered significant attention due to their ability to generate adversarial images for multiple target classes simultaneously. However, existing generative approaches for multi-target attacks primarily encode target labels into one-dimensional tensors, leading to a loss of fine-grained visual information and overfitting to model-specific features during noise generation. To address this gap, we first identify and validate that the semantic feature quality and quantity are critical factors affecting the transferability of targeted attacks: 1) Feature quality refers to the structural and detailed completeness of the implanted target features, as deficiencies may result in the loss of key discriminative information; 2) Feature quantity refers to the spatial sufficiency of the implanted target features, as inadequacy limits the victim modelâ€™s attention to this feature. Based on these findings, we propose the 2D Tensor-Guided Adversarial Fusion (TGAF) framework, which leverages the powerful generative capabilities of diffusion models to encode target labels into two-dimensional semantic tensors for guiding adversarial noise generation. Additionally, we design a novel masking strategy tailored for the training process, ensuring that parts of the generated noise retain complete semantic information about the target class. Extensive experiments demonstrate that TGAF consistently surpasses state-of-the-art methods across various settings.</p>
<blockquote>
<p>ç›¸è¾ƒäºå•ç›®æ ‡å¯¹æŠ—æ”»å‡»ï¼Œå¤šç›®æ ‡æ”»å‡»å› å…¶èƒ½å¤ŸåŒæ—¶ä¸ºå¤šä¸ªç›®æ ‡ç±»åˆ«ç”Ÿæˆå¯¹æŠ—æ ·æœ¬çš„èƒ½åŠ›è€Œå¤‡å—å…³æ³¨ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¤šç›®æ ‡æ”»å‡»çš„ç”Ÿæˆæ–¹æ³•ä¸»è¦æ˜¯å°†ç›®æ ‡æ ‡ç­¾ç¼–ç ä¸ºä¸€ç»´å¼ é‡ï¼Œè¿™å¯¼è‡´åœ¨ç”Ÿæˆå™ªå£°æ—¶ä¸¢å¤±äº†ç²¾ç»†çš„è§†è§‰ä¿¡æ¯ï¼Œå¹¶å¯¹ç‰¹å®šæ¨¡å‹çš„ç‰¹æ€§äº§ç”Ÿè¿‡åº¦æ‹Ÿåˆã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬é¦–å…ˆç¡®å®šå¹¶éªŒè¯äº†è¯­ä¹‰ç‰¹å¾çš„è´¨é‡å’Œæ•°é‡æ˜¯å½±å“ç›®æ ‡æ”»å‡»è½¬ç§»æ€§çš„å…³é”®å› ç´ ï¼š1ï¼‰ç‰¹å¾è´¨é‡æŒ‡çš„æ˜¯æ¤å…¥ç›®æ ‡ç‰¹å¾çš„ç»“æ„å’Œç»†èŠ‚å®Œæ•´æ€§ï¼Œå› ä¸ºç¼ºé™·å¯èƒ½å¯¼è‡´å…³é”®é‰´åˆ«ä¿¡æ¯çš„ä¸¢å¤±ï¼›2ï¼‰ç‰¹å¾æ•°é‡æŒ‡çš„æ˜¯æ¤å…¥ç›®æ ‡ç‰¹å¾çš„ç©ºé—´å……è¶³æ€§ï¼Œå› ä¸ºä¸è¶³ä¼šé™åˆ¶å—å®³è€…æ¨¡å‹å¯¹æ­¤ç‰¹å¾çš„å…³æ³¨ã€‚åŸºäºè¿™äº›å‘ç°ï¼Œæˆ‘ä»¬æå‡ºäº†äºŒç»´å¼ é‡å¼•å¯¼å¯¹æŠ—èåˆï¼ˆTGAFï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„å¼ºå¤§ç”Ÿæˆèƒ½åŠ›ï¼Œå°†ç›®æ ‡æ ‡ç­¾ç¼–ç ä¸ºäºŒç»´è¯­ä¹‰å¼ é‡ï¼Œä»¥æŒ‡å¯¼å¯¹æŠ—å™ªå£°ç”Ÿæˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è®¾è®¡äº†ä¸€ç§é’ˆå¯¹è®­ç»ƒè¿‡ç¨‹çš„å®šåˆ¶æ©ç ç­–ç•¥ï¼Œç¡®ä¿ç”Ÿæˆçš„å™ªå£°éƒ¨åˆ†ä¿ç•™æœ‰å…³ç›®æ ‡ç±»åˆ«çš„å®Œæ•´è¯­ä¹‰ä¿¡æ¯ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒTGAFåœ¨å„ç§è®¾ç½®ä¸‹å‡è¶…è¶Šç°æœ‰å…ˆè¿›æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.14137v2">PDF</a> AAAI-26 (Oral)</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†å¤šç›®æ ‡æ”»å‡»åœ¨ç”Ÿæˆå¯¹æŠ—æ€§å›¾åƒæ–¹é¢çš„ä¼˜åŠ¿ï¼Œå¹¶æŒ‡å‡ºäº†ç°æœ‰ç”Ÿæˆæ–¹æ³•å­˜åœ¨çš„é—®é¢˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºäº†åŸºäºäºŒç»´å¼ é‡å¼•å¯¼çš„å¯¹æŠ—æ€§èåˆï¼ˆTGAFï¼‰æ¡†æ¶ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ï¼Œå°†ç›®æ ‡æ ‡ç­¾ç¼–ç ä¸ºäºŒç»´è¯­ä¹‰å¼ é‡ï¼Œä»¥æŒ‡å¯¼å¯¹æŠ—æ€§å™ªå£°ç”Ÿæˆã€‚åŒæ—¶ï¼Œè®¾è®¡äº†ä¸€ç§é’ˆå¯¹è®­ç»ƒè¿‡ç¨‹çš„å®šåˆ¶æ©ç ç­–ç•¥ï¼Œç¡®ä¿ç”Ÿæˆçš„å™ªå£°éƒ¨åˆ†ä¿ç•™ç›®æ ‡ç±»çš„å®Œæ•´è¯­ä¹‰ä¿¡æ¯ã€‚å®éªŒè¡¨æ˜ï¼ŒTGAFåœ¨ä¸åŒè®¾ç½®ä¸‹å§‹ç»ˆè¶…è¶Šç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šç›®æ ‡æ”»å‡»èƒ½åŒæ—¶ç”Ÿæˆé’ˆå¯¹å¤šä¸ªç›®æ ‡ç±»çš„å¯¹æŠ—æ€§å›¾åƒï¼Œå—åˆ°å¹¿æ³›å…³æ³¨ã€‚</li>
<li>ç°æœ‰ç”Ÿæˆæ–¹æ³•å°†ç›®æ ‡æ ‡ç­¾ç¼–ç ä¸ºä¸€ç»´å¼ é‡ï¼Œå¯¼è‡´ç²¾ç»†è§†è§‰ä¿¡æ¯ä¸¢å¤±å’Œæ¨¡å‹ç‰¹å®šç‰¹å¾çš„è¿‡æ‹Ÿåˆã€‚</li>
<li>è¯­ä¹‰ç‰¹å¾çš„è´¨é‡å’Œæ•°é‡æ˜¯å½±å“ç›®æ ‡æ”»å‡»å¯è½¬ç§»æ€§çš„å…³é”®å› ç´ ã€‚</li>
<li>ç‰¹å¾è´¨é‡æŒ‡æ¤å…¥ç›®æ ‡ç‰¹å¾çš„ç»“æ„å’Œç»†èŠ‚å®Œæ•´æ€§ï¼Œä¸è¶³ä¼šå¯¼è‡´å…³é”®é‰´åˆ«ä¿¡æ¯ä¸¢å¤±ã€‚</li>
<li>ç‰¹å¾æ•°é‡æŒ‡æ¤å…¥ç›®æ ‡ç‰¹å¾çš„ç©ºé—´å……è¶³æ€§ï¼Œä¸è¶³ä¼šé™åˆ¶å—å®³è€…æ¨¡å‹çš„å…³æ³¨ã€‚</li>
<li>æå‡ºçš„TGAFæ¡†æ¶åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ï¼Œå°†ç›®æ ‡æ ‡ç­¾ç¼–ç ä¸ºäºŒç»´è¯­ä¹‰å¼ é‡ï¼ŒæŒ‡å¯¼å¯¹æŠ—æ€§å™ªå£°ç”Ÿæˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.14137">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-de12219e179a4f51c4100660e611ff84" align="middle">
<img src="https://picx.zhimg.com/v2-fa7267f5f3e48336f2fb71c67ea77700" align="middle">
<img src="https://picx.zhimg.com/v2-09098b34b001ae8f2ad04af0bf46f0b3" align="middle">
<img src="https://picx.zhimg.com/v2-9888a7bc400bbc02a5d516e1539cb145" align="middle">
<img src="https://picx.zhimg.com/v2-93f4ad877a8479d82c26d82de123212f" align="middle">
<img src="https://picx.zhimg.com/v2-97f6db83cf1d5f4e91cb29a095bf3c59" align="middle">
<img src="https://picx.zhimg.com/v2-394b3176f4558c2148ef05b1fc9559a9" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Novel-Diffusion-Models-for-Multimodal-3D-Hand-Trajectory-Prediction"><a href="#Novel-Diffusion-Models-for-Multimodal-3D-Hand-Trajectory-Prediction" class="headerlink" title="Novel Diffusion Models for Multimodal 3D Hand Trajectory Prediction"></a>Novel Diffusion Models for Multimodal 3D Hand Trajectory Prediction</h2><p><strong>Authors:Junyi Ma, Wentao Bao, Jingyi Xu, Guanzhong Sun, Xieyuanli Chen, Hesheng Wang</strong></p>
<p>Predicting hand motion is critical for understanding human intentions and bridging the action space between human movements and robot manipulations. Existing hand trajectory prediction (HTP) methods forecast the future hand waypoints in 3D space conditioned on past egocentric observations. However, such models are only designed to accommodate 2D egocentric video inputs. There is a lack of awareness of multimodal environmental information from both 2D and 3D observations, hindering the further improvement of 3D HTP performance. In addition, these models overlook the synergy between hand movements and headset camera egomotion, either predicting hand trajectories in isolation or encoding egomotion only from past frames. To address these limitations, we propose novel diffusion models (MMTwin) for multimodal 3D hand trajectory prediction. MMTwin is designed to absorb multimodal information as input encompassing 2D RGB images, 3D point clouds, past hand waypoints, and text prompt. Besides, two latent diffusion models, the egomotion diffusion and the HTP diffusion as twins, are integrated into MMTwin to predict camera egomotion and future hand trajectories concurrently. We propose a novel hybrid Mamba-Transformer module as the denoising model of the HTP diffusion to better fuse multimodal features. The experimental results on three publicly available datasets and our self-recorded data demonstrate that our proposed MMTwin can predict plausible future 3D hand trajectories compared to the state-of-the-art baselines, and generalizes well to unseen environments. The code and pretrained models have been released at <a target="_blank" rel="noopener" href="https://github.com/IRMVLab/MMTwin">https://github.com/IRMVLab/MMTwin</a>.</p>
<blockquote>
<p>é¢„æµ‹æ‰‹éƒ¨åŠ¨ä½œå¯¹äºç†è§£äººç±»æ„å›¾ä»¥åŠå»ºç«‹äººç±»åŠ¨ä½œä¸æœºå™¨äººæ“ä½œä¹‹é—´çš„åŠ¨ä½œç©ºé—´è‡³å…³é‡è¦ã€‚ç°æœ‰çš„æ‰‹éƒ¨è½¨è¿¹é¢„æµ‹ï¼ˆHTPï¼‰æ–¹æ³•åŸºäºè¿‡å»çš„ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„è§‚å¯Ÿæ¥é¢„æµ‹æœªæ¥æ‰‹éƒ¨åœ¨ä¸‰ç»´ç©ºé—´ä¸­çš„è·¯å¾„ç‚¹ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹ä»…è®¾è®¡ç”¨äºå¤„ç†äºŒç»´ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„è§†é¢‘è¾“å…¥ã€‚å®ƒä»¬æ— æ³•æ„è¯†åˆ°æ¥è‡ªäºŒç»´å’Œä¸‰ç»´è§‚å¯Ÿçš„å¤šå…ƒç¯å¢ƒä¿¡æ¯ï¼Œé˜»ç¢äº†ä¸‰ç»´HTPæ€§èƒ½çš„è¿›ä¸€æ­¥æé«˜ã€‚æ­¤å¤–ï¼Œè¿™äº›æ¨¡å‹å¿½è§†äº†æ‰‹éƒ¨åŠ¨ä½œä¸å¤´æˆ´å¼ç›¸æœºè‡ªä¸»è¿åŠ¨ä¹‹é—´çš„ååŒä½œç”¨ï¼Œè¦ä¹ˆå•ç‹¬é¢„æµ‹æ‰‹éƒ¨è½¨è¿¹ï¼Œè¦ä¹ˆä»…ä»è¿‡å»çš„å¸§ä¸­ç¼–ç è‡ªä¸»è¿åŠ¨ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ç”¨äºå¤šæ¨¡æ€ä¸‰ç»´æ‰‹éƒ¨è½¨è¿¹é¢„æµ‹çš„æ–°å‹æ‰©æ•£æ¨¡å‹ï¼ˆMMTwinï¼‰ã€‚MMTwinæ—¨åœ¨å°†å¤šæ¨¡æ€ä¿¡æ¯ä½œä¸ºè¾“å…¥ï¼ŒåŒ…æ‹¬äºŒç»´RGBå›¾åƒã€ä¸‰ç»´ç‚¹äº‘ã€è¿‡å»çš„æ‰‹éƒ¨è·¯å¾„ç‚¹å’Œæ–‡æœ¬æç¤ºã€‚æ­¤å¤–ï¼Œè¿˜å°†ä¸¤ä¸ªæ½œåœ¨æ‰©æ•£æ¨¡å‹â€”â€”è‡ªä¸»è¿åŠ¨æ‰©æ•£å’ŒHTPæ‰©æ•£ç›¸ç»“åˆï¼Œæ„æˆåŒèƒèƒæ¨¡å‹ï¼Œä»¥åŒæ—¶é¢„æµ‹ç›¸æœºè‡ªä¸»è¿åŠ¨å’Œæœªæ¥æ‰‹éƒ¨è½¨è¿¹ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹æ··åˆçš„Mamba-Transformeræ¨¡å—ï¼Œä½œä¸ºHTPæ‰©æ•£çš„å»å™ªæ¨¡å‹ï¼Œä»¥æ›´å¥½åœ°èåˆå¤šæ¨¡æ€ç‰¹å¾ã€‚åœ¨ä¸‰ä¸ªå…¬å¼€æ•°æ®é›†å’Œæˆ‘ä»¬è‡ªè¡Œå½•åˆ¶çš„æ•°æ®ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œä¸æˆ‘ä»¬æå‡ºçš„MMTwinç›¸æ¯”ï¼Œæœ€å…ˆè¿›çš„åŸºçº¿å¯ä»¥é¢„æµ‹åˆç†çš„æœªæ¥ä¸‰ç»´æ‰‹éƒ¨è½¨è¿¹ï¼Œå¹¶åœ¨æœªè§è¿‡çš„ç¯å¢ƒä¸­æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚ä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/IRMVLab/MMTwin%E3%80%82">https://github.com/IRMVLab/MMTwinã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07375v2">PDF</a> Accepted to IROS 2025</p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ‰©æ•£æ¨¡å‹çš„å¤šæ¨¡æ€ä¸‰ç»´æ‰‹åŠ¿è½¨è¿¹é¢„æµ‹ã€‚é’ˆå¯¹ç°æœ‰æ‰‹åŠ¿è½¨è¿¹é¢„æµ‹æ¨¡å‹ä»…æ”¯æŒäºŒç»´è§†é¢‘è¾“å…¥çš„é—®é¢˜ï¼Œæå‡ºäº†å…¨æ–°çš„æ‰©æ•£æ¨¡å‹MMTwinã€‚å®ƒèƒ½å¤Ÿå¸æ”¶åŒ…æ‹¬äºŒç»´RGBå›¾åƒã€ä¸‰ç»´ç‚¹äº‘ã€è¿‡å»çš„æ‰‹åŠ¿è·¯å¾„ç‚¹å’Œæ–‡æœ¬æç¤ºåœ¨å†…çš„å¤šæ¨¡æ€ä¿¡æ¯ã€‚è¯¥æ¨¡å‹é€šè¿‡æ•´åˆä¸¤ä¸ªæ½œæ‰©æ•£æ¨¡å‹ï¼Œå³åŠ¨ä½œæ‰©æ•£å’Œæ‰‹åŠ¿è½¨è¿¹é¢„æµ‹æ‰©æ•£ï¼ŒåŒæ—¶é¢„æµ‹ç›¸æœºåŠ¨ä½œå’Œæœªæ¥çš„æ‰‹åŠ¿è½¨è¿¹ã€‚ä½¿ç”¨æ··åˆMamba-Transformeræ¨¡å—ä½œä¸ºæ‰‹åŠ¿è½¨è¿¹é¢„æµ‹æ‰©æ•£çš„å»å™ªæ¨¡å‹ï¼Œä»¥æ›´å¥½åœ°èåˆå¤šæ¨¡æ€ç‰¹å¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ç°æœ‰æœ€ä½³åŸºçº¿ç›¸æ¯”ï¼ŒMMTwinèƒ½å¤Ÿé¢„æµ‹æ›´å¯é çš„ä¸‰ç»´æ‰‹åŠ¿è½¨è¿¹ï¼Œå¹¶åœ¨æœªè§è¿‡çš„ç¯å¢ƒä¸­å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰‹åŠ¿è½¨è¿¹é¢„æµ‹åœ¨ç†è§£äººç±»æ„å›¾å’Œè¿æ¥äººç±»åŠ¨ä½œä¸æœºå™¨äººæ“ä½œä¹‹é—´èµ·ç€å…³é”®ä½œç”¨ã€‚</li>
<li>ç°æœ‰æ‰‹åŠ¿è½¨è¿¹é¢„æµ‹æ–¹æ³•ä»…åŸºäºè¿‡å»äºŒç»´è§‚å¯Ÿæ•°æ®é¢„æµ‹æœªæ¥æ‰‹åŠ¿è½¨è¿¹ï¼Œç¼ºä¹å¤šæ¨¡æ€ç¯å¢ƒä¿¡æ¯çš„åˆ©ç”¨ã€‚</li>
<li>æå‡ºæ–°å‹æ‰©æ•£æ¨¡å‹MMTwinï¼Œèåˆå¤šç§è¾“å…¥ä¿¡æ¯å¦‚äºŒç»´RGBå›¾åƒã€ä¸‰ç»´ç‚¹äº‘ç­‰ã€‚</li>
<li>MMTwiné€šè¿‡æ•´åˆåŠ¨ä½œæ‰©æ•£å’Œæ‰‹åŠ¿è½¨è¿¹é¢„æµ‹æ‰©æ•£ï¼ŒåŒæ—¶å¤„ç†ç›¸æœºåŠ¨ä½œå’Œæ‰‹åŠ¿è½¨è¿¹é¢„æµ‹ã€‚</li>
<li>é‡‡ç”¨æ··åˆMamba-Transformeræ¨¡å—ä½œä¸ºå»å™ªæ¨¡å‹ï¼Œä¼˜åŒ–å¤šæ¨¡æ€ç‰¹å¾çš„èåˆã€‚</li>
<li>å®éªŒè¯æ˜MMTwinç›¸è¾ƒäºç°æœ‰æ–¹æ³•èƒ½æ›´å‡†ç¡®åœ°é¢„æµ‹ä¸‰ç»´æ‰‹åŠ¿è½¨è¿¹ï¼Œä¸”å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07375">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c9d05cfc37f0b9f656aa374dcc72c0d9" align="middle">
<img src="https://picx.zhimg.com/v2-cf5cb9bbf7f5e25bd5a5c62fcbebb0e9" align="middle">
<img src="https://picx.zhimg.com/v2-e721dacdcac6e8e81ca101069188fc25" align="middle">
<img src="https://picx.zhimg.com/v2-9b5fa7039a5d739fd8b04a9e9abeea51" align="middle">
<img src="https://picx.zhimg.com/v2-7151998393e412e9185e6b2699c2e339" align="middle">
<img src="https://picx.zhimg.com/v2-e5fd4cf49f90016af622e02f13392ced" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Efficient-Image-Restoration-via-Latent-Consistency-Flow-Matching"><a href="#Efficient-Image-Restoration-via-Latent-Consistency-Flow-Matching" class="headerlink" title="Efficient Image Restoration via Latent Consistency Flow Matching"></a>Efficient Image Restoration via Latent Consistency Flow Matching</h2><p><strong>Authors:Elad Cohen, Idan Achituve, Idit Diamant, Arnon Netzer, Hai Victor Habi</strong></p>
<p>Recent advances in generative image restoration (IR) have demonstrated impressive results. However, these methods are hindered by their substantial size and computational demands, rendering them unsuitable for deployment on edge devices. This work introduces ELIR, an Efficient Latent Image Restoration method. ELIR addresses the distortion-perception trade-off within the latent space and produces high-quality images using a latent consistency flow-based model. In addition, ELIR introduces an efficient and lightweight architecture. Consequently, ELIR is 4$\times$ smaller and faster than state-of-the-art diffusion and flow-based approaches for blind face restoration, enabling a deployment on resource-constrained devices. Comprehensive evaluations of various image restoration tasks and datasets show that ELIR achieves competitive performance compared to state-of-the-art methods, effectively balancing distortion and perceptual quality metrics while significantly reducing model size and computational cost. The code is available at: <a target="_blank" rel="noopener" href="https://github.com/eladc-git/ELIR">https://github.com/eladc-git/ELIR</a></p>
<blockquote>
<p>æœ€æ–°çš„ç”Ÿæˆå¼å›¾åƒä¿®å¤ï¼ˆIRï¼‰æŠ€æœ¯å·²ç»å–å¾—äº†ä»¤äººç©ç›®çš„æˆæœã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å—åˆ°å…¶åºå¤§ä½“ç§¯å’Œè®¡ç®—éœ€æ±‚çš„é™åˆ¶ï¼Œä¸é€‚åˆåœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²ã€‚æœ¬ç ”ç©¶å¼•å…¥äº†ELIRï¼Œä¸€ç§é«˜æ•ˆçš„æ½œåœ¨å›¾åƒä¿®å¤æ–¹æ³•ã€‚ELIRè§£å†³äº†æ½œåœ¨ç©ºé—´å†…çš„å¤±çœŸä¸æ„ŸçŸ¥ä¹‹é—´çš„æƒè¡¡é—®é¢˜ï¼Œå¹¶ä½¿ç”¨åŸºäºæ½œåœ¨ä¸€è‡´æ€§æµçš„æ¨¡å‹ç”Ÿæˆé«˜è´¨é‡å›¾åƒã€‚æ­¤å¤–ï¼ŒELIRè¿˜å¼•å…¥äº†é«˜æ•ˆä¸”è½»é‡çº§çš„æ¶æ„ã€‚å› æ­¤ï¼Œä¸ä¼ ç»Ÿçš„æ‰©æ•£å’ŒåŸºäºæµçš„ç›²è„¸ä¿®å¤æ–¹æ³•ç›¸æ¯”ï¼ŒELIRä½“ç§¯æ›´å°ã€é€Ÿåº¦æ›´å¿«ï¼Œå¯è¾¾4å€ï¼Œèƒ½å¤Ÿåœ¨èµ„æºå—é™çš„è®¾å¤‡ä¸Šè¿›è¡Œéƒ¨ç½²ã€‚å¯¹ä¸åŒçš„å›¾åƒä¿®å¤ä»»åŠ¡å’Œæ•°æ®é›†çš„ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼ŒELIRä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ï¼Œåœ¨å¤±çœŸå’Œæ„ŸçŸ¥è´¨é‡æŒ‡æ ‡ä¹‹é—´å®ç°äº†æœ‰æ•ˆå¹³è¡¡ï¼ŒåŒæ—¶æ˜¾è‘—å‡å°‘äº†æ¨¡å‹å¤§å°å’Œè®¡ç®—æˆæœ¬ã€‚ä»£ç å¯ä»ä»¥ä¸‹ç½‘å€è·å–ï¼š<a target="_blank" rel="noopener" href="https://github.com/eladc-git/ELIR">https://github.com/eladc-git/ELIR</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.03500v2">PDF</a> 21 pages, 11 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§é«˜æ•ˆçš„æ½œåœ¨å›¾åƒæ¢å¤æ–¹æ³•â€”â€”ELIRã€‚è¯¥æ–¹æ³•è§£å†³äº†æ½œåœ¨ç©ºé—´å†…çš„å¤±çœŸä¸æ„ŸçŸ¥ä¹‹é—´çš„æƒè¡¡é—®é¢˜ï¼Œå¹¶åˆ©ç”¨åŸºäºæµçš„æ¨¡å‹äº§ç”Ÿé«˜è´¨é‡å›¾åƒã€‚ä¸ä¼ ç»Ÿçš„å›¾åƒæ¢å¤æ–¹æ³•ç›¸æ¯”ï¼ŒELIRæ‹¥æœ‰æ›´å°ã€æ›´å¿«é€Ÿçš„æ¶æ„ï¼Œå¯ä»¥åœ¨èµ„æºå—é™çš„è®¾å¤‡ä¸Šè¿›è¡Œéƒ¨ç½²ã€‚å®ƒèƒ½åœ¨å¹³è¡¡å¤±çœŸå’Œæ„ŸçŸ¥è´¨é‡æŒ‡æ ‡çš„åŒæ—¶ï¼Œå®ç°ä¸æœ€æ–°æŠ€æœ¯ç›¸å½“çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ELIRæ–¹æ³•è§£å†³äº†ç”Ÿæˆå¼å›¾åƒæ¢å¤ä¸­çš„å¤±çœŸä¸æ„ŸçŸ¥ä¹‹é—´çš„æƒè¡¡é—®é¢˜ã€‚</li>
<li>ELIRåˆ©ç”¨åŸºäºæµçš„æ¨¡å‹äº§ç”Ÿé«˜è´¨é‡å›¾åƒã€‚</li>
<li>ELIRå…·æœ‰é«˜æ•ˆä¸”è½»é‡çº§çš„æ¶æ„ï¼Œä½¿å¾—å…¶åœ¨èµ„æºå—é™çš„è®¾å¤‡ä¸Šèƒ½å¤Ÿè¿›è¡Œéƒ¨ç½²ã€‚</li>
<li>ELIRåœ¨å›¾åƒæ¢å¤ä»»åŠ¡ä¸Šçš„æ€§èƒ½ä¸æœ€æ–°æŠ€æœ¯ç›¸å½“ã€‚</li>
<li>ELIRç›¸æ¯”å½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œæ¨¡å‹æ›´å°ã€é€Ÿåº¦æ›´å¿«ã€‚</li>
<li>ä»£ç å·²å…¬å¼€ï¼Œå¯è®¿é—®äº <a target="_blank" rel="noopener" href="https://github.com/eladc-git/ELIR%E3%80%82">https://github.com/eladc-git/ELIRã€‚</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.03500">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-afd1b3d25c24299d31fa6095b36d0b52" align="middle">
<img src="https://picx.zhimg.com/v2-a80e238ccf27b2e2a3b60063a26721a4" align="middle">
<img src="https://picx.zhimg.com/v2-5cca1ebfa37064c14f499e9e3902675d" align="middle">
<img src="https://picx.zhimg.com/v2-8f1e7e1fbf8f16078b638fc65ffcc84c" align="middle">
<img src="https://picx.zhimg.com/v2-1bc5b1b288f35b75dfadd4bf9586341d" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Diff-IP2D-Diffusion-Based-Hand-Object-Interaction-Prediction-on-Egocentric-Videos"><a href="#Diff-IP2D-Diffusion-Based-Hand-Object-Interaction-Prediction-on-Egocentric-Videos" class="headerlink" title="Diff-IP2D: Diffusion-Based Hand-Object Interaction Prediction on Egocentric Videos"></a>Diff-IP2D: Diffusion-Based Hand-Object Interaction Prediction on Egocentric Videos</h2><p><strong>Authors:Junyi Ma, Jingyi Xu, Xieyuanli Chen, Hesheng Wang</strong></p>
<p>Understanding how humans would behave during hand-object interaction is vital for applications in service robot manipulation and extended reality. To achieve this, some recent works have been proposed to simultaneously forecast hand trajectories and object affordances on human egocentric videos. The joint prediction serves as a comprehensive representation of future hand-object interactions in 2D space, indicating potential human motion and motivation. However, the existing approaches mostly adopt the autoregressive paradigm for unidirectional prediction, which lacks mutual constraints within the holistic future sequence, and accumulates errors along the time axis. Meanwhile, these works basically overlook the effect of camera egomotion on first-person view predictions. To address these limitations, we propose a novel diffusion-based interaction prediction method, namely Diff-IP2D, to forecast future hand trajectories and object affordances concurrently in an iterative non-autoregressive manner. We transform the sequential 2D images into latent feature space and design a denoising diffusion model to predict future latent interaction features conditioned on past ones. Motion features are further integrated into the conditional denoising process to enable Diff-IP2D aware of the camera wearerâ€™s dynamics for more accurate interaction prediction. Extensive experiments demonstrate that our method significantly outperforms the state-of-the-art baselines on both the off-the-shelf metrics and our newly proposed evaluation protocol. This highlights the efficacy of leveraging a generative paradigm for 2D hand-object interaction prediction. The code of Diff-IP2D is released as open source at <a target="_blank" rel="noopener" href="https://github.com/IRMVLab/Diff-IP2D">https://github.com/IRMVLab/Diff-IP2D</a>.</p>
<blockquote>
<p>ç†è§£äººç±»åœ¨æ‰‹ä¸ç‰©ä½“äº¤äº’è¿‡ç¨‹ä¸­çš„è¡Œä¸ºå¯¹äºæœåŠ¡æœºå™¨äººæ“ä½œå’Œæ‰©å±•ç°å®åº”ç”¨è‡³å…³é‡è¦ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œè¿‘æœŸå·²ç»æå‡ºäº†ä¸€äº›æ–¹æ³•ï¼Œèƒ½å¤ŸåŒæ—¶é¢„æµ‹æ‰‹éƒ¨è½¨è¿¹å’Œç‰©ä½“åŠŸèƒ½åœ¨äººä½“è‡ªæˆ‘ä¸­å¿ƒè§†é¢‘ä¸­çš„è¡¨ç°ã€‚è¿™ç§è”åˆé¢„æµ‹ä¸ºæœªæ¥æ‰‹éƒ¨ä¸ç‰©ä½“åœ¨äºŒç»´ç©ºé—´ä¸­çš„äº¤äº’æä¾›äº†å…¨é¢çš„è¡¨å¾ï¼Œå±•ç¤ºäº†æ½œåœ¨çš„äººç±»è¿åŠ¨å’ŒåŠ¨æœºã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ–¹æ³•å¤§å¤šé‡‡ç”¨è‡ªå›å½’æ¨¡å¼è¿›è¡Œå•å‘é¢„æµ‹ï¼Œè¿™ç¼ºä¹å¯¹æ•´ä½“æœªæ¥åºåˆ—çš„å†…éƒ¨ç›¸äº’çº¦æŸï¼Œå¹¶ä¸”æ²¿ç€æ—¶é—´è½´ç§¯ç´¯è¯¯å·®ã€‚åŒæ—¶ï¼Œè¿™äº›ç ”ç©¶åŸºæœ¬ä¸Šå¿½è§†äº†ç›¸æœºè‡ªæˆ‘è¿åŠ¨å¯¹ç¬¬ä¸€äººç§°è§†è§’é¢„æµ‹çš„å½±å“ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£çš„äº¤äº’é¢„æµ‹æ–°æ–¹æ³•ï¼Œå³Diff-IP2Dï¼Œä»¥è¿­ä»£éè‡ªå›å½’çš„æ–¹å¼åŒæ—¶é¢„æµ‹æœªæ¥çš„æ‰‹éƒ¨è½¨è¿¹å’Œç‰©ä½“åŠŸèƒ½ã€‚æˆ‘ä»¬å°†äºŒç»´å›¾åƒåºåˆ—å˜æ¢ä¸ºæ½œåœ¨ç‰¹å¾ç©ºé—´ï¼Œå¹¶è®¾è®¡äº†ä¸€ç§å»å™ªæ‰©æ•£æ¨¡å‹ï¼Œæ ¹æ®è¿‡å»çš„ç‰¹å¾é¢„æµ‹æœªæ¥çš„æ½œåœ¨äº¤äº’ç‰¹å¾ã€‚è¿åŠ¨ç‰¹å¾è¿›ä¸€æ­¥é›†æˆåˆ°æ¡ä»¶å»å™ªè¿‡ç¨‹ä¸­ï¼Œä½¿Diff-IP2Dèƒ½å¤Ÿæ„ŸçŸ¥åˆ°ç›¸æœºä½©æˆ´è€…çš„åŠ¨æ€å˜åŒ–ï¼Œä»è€Œå®ç°æ›´ç²¾ç¡®äº¤äº’é¢„æµ‹ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç°æˆçš„æŒ‡æ ‡å’Œæˆ‘ä»¬æ–°æå‡ºçš„è¯„ä¼°åè®®ä¸Šéƒ½æ˜¾è‘—ä¼˜äºæœ€æ–°çš„åŸºçº¿æ–¹æ³•ã€‚è¿™å‡¸æ˜¾äº†åˆ©ç”¨ç”Ÿæˆæ¨¡å¼è¿›è¡ŒäºŒç»´æ‰‹éƒ¨ç‰©ä½“äº¤äº’é¢„æµ‹çš„æœ‰æ•ˆæ€§ã€‚Diff-IP2Dçš„ä»£ç å·²ä½œä¸ºå¼€æºå‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/IRMVLab/Diff-IP2D%E3%80%82">https://github.com/IRMVLab/Diff-IP2Dã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.04370v5">PDF</a> Accepted to IROS 2025</p>
<p><strong>Summary</strong><br>     ä¸ºåº”å¯¹æœåŠ¡æœºå™¨äººæ“ä½œå’Œæ‰©å±•ç°å®åº”ç”¨ä¸­çš„æ‰‹ä¸ç‰©ä½“äº¤äº’é¢„æµ‹æŒ‘æˆ˜ï¼Œæå‡ºä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„æ–¹æ³•Diff-IP2Dï¼Œä»¥è¿­ä»£éè‡ªå›å½’çš„æ–¹å¼åŒæ—¶é¢„æµ‹æœªæ¥æ‰‹è½¨è¿¹å’Œç‰©ä½“åŠŸèƒ½ã€‚é€šè¿‡å°†è¿ç»­äºŒç»´å›¾åƒè½¬æ¢ä¸ºæ½œåœ¨ç‰¹å¾ç©ºé—´ï¼Œè®¾è®¡å»å™ªæ‰©æ•£æ¨¡å‹ï¼Œæ ¹æ®è¿‡å»ç‰¹å¾é¢„æµ‹æœªæ¥äº¤äº’ç‰¹å¾ã€‚æ•´åˆè¿åŠ¨ç‰¹å¾åˆ°æ¡ä»¶å»å™ªè¿‡ç¨‹ä¸­ï¼Œæé«˜ç›¸æœºä½©æˆ´è€…åŠ¨æ€æ„ŸçŸ¥çš„å‡†ç¡®åº¦ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç°æœ‰æŒ‡æ ‡å’Œæ–°æå‡ºçš„è¯„ä¼°åè®®ä¸Šå‡æ˜¾è‘—ä¼˜äºæœ€å…ˆè¿›åŸºçº¿ã€‚çªæ˜¾äº†ç”Ÿæˆå¼èŒƒå¼åœ¨æ‰‹-ç‰©ä½“äºŒç»´äº¤äº’é¢„æµ‹ä¸­çš„é‡è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç†è§£æ‰‹ä¸ç‰©ä½“çš„äº¤äº’é¢„æµ‹å¯¹äºæœåŠ¡æœºå™¨äººæ“ä½œå’Œæ‰©å±•ç°å®åº”ç”¨è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¤§å¤šé‡‡ç”¨è‡ªå›å½’æ¨¡å¼è¿›è¡Œå•å‘é¢„æµ‹ï¼Œå­˜åœ¨ç¼ºä¹æ•´ä½“æœªæ¥åºåˆ—çš„ç›¸äº’çº¦æŸå’Œæ²¿æ—¶é—´è½´çš„è¯¯å·®ç´¯ç§¯é—®é¢˜ã€‚</li>
<li>æå‡ºçš„Diff-IP2Dæ–¹æ³•ä»¥è¿­ä»£éè‡ªå›å½’çš„æ–¹å¼åŒæ—¶é¢„æµ‹æœªæ¥æ‰‹è½¨è¿¹å’Œç‰©ä½“åŠŸèƒ½ã€‚</li>
<li>Diff-IP2Dé€šè¿‡å°†è¿ç»­äºŒç»´å›¾åƒè½¬æ¢ä¸ºæ½œåœ¨ç‰¹å¾ç©ºé—´ï¼Œå¹¶åˆ©ç”¨å»å™ªæ‰©æ•£æ¨¡å‹è¿›è¡Œé¢„æµ‹ã€‚</li>
<li>é›†æˆè¿åŠ¨ç‰¹å¾åˆ°æ¡ä»¶å»å™ªè¿‡ç¨‹ä¸­ï¼Œä»¥æé«˜ç›¸æœºä½©æˆ´è€…åŠ¨æ€æ„ŸçŸ¥çš„å‡†ç¡®åº¦ã€‚</li>
<li>å®éªŒè¯æ˜Diff-IP2Dåœ¨å¤šä¸ªè¯„ä¼°æŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>é‡Šæ”¾å¼€æºä»£ç ï¼Œç½‘å€ä¸º[é“¾æ¥åœ°å€]ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.04370">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-980bf8be06c18e0176345ef05a1d0e98" align="middle">
<img src="https://picx.zhimg.com/v2-3af9fed0b0b87c5edb7fce7b144fada8" align="middle">
<img src="https://picx.zhimg.com/v2-3b92f18d2fd1f682d0b15050881dd554" align="middle">
<img src="https://picx.zhimg.com/v2-993000b1f08fc3f5c4664864ae3d8e4d" align="middle">
<img src="https://picx.zhimg.com/v2-bbc24829abdc46fe8548f499591334bd" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-11-18/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-11-18/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-18/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-1d7606dfa305347887614c756f0bd25f" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-18  LARM A Large Articulated-Object Reconstruction Model
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-11-18
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-18/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-62ffe1e9df30904ba0f6cd23c70bae97" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-18  High Mobility Multiple-Channel AlScN/GaN Heterostructures
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-11-18
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">33297.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
