<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-10  A Survey on Latent Reasoning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-5c4fdda0376711e17333cca53334a489.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-10
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-19
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    89 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-07-10-æ›´æ–°"><a href="#2025-07-10-æ›´æ–°" class="headerlink" title="2025-07-10 æ›´æ–°"></a>2025-07-10 æ›´æ–°</h1><h2 id="A-Survey-on-Latent-Reasoning"><a href="#A-Survey-on-Latent-Reasoning" class="headerlink" title="A Survey on Latent Reasoning"></a>A Survey on Latent Reasoning</h2><p><strong>Authors:Rui-Jie Zhu, Tianhao Peng, Tianhao Cheng, Xingwei Qu, Jinfa Huang, Dawei Zhu, Hao Wang, Kaiwen Xue, Xuanliang Zhang, Yong Shan, Tianle Cai, Taylor Kergan, Assel Kembay, Andrew Smith, Chenghua Lin, Binh Nguyen, Yuqi Pan, Yuhong Chou, Zefan Cai, Zhenhe Wu, Yongchi Zhao, Tianyu Liu, Jian Yang, Wangchunshu Zhou, Chujie Zheng, Chongxuan Li, Yuyin Zhou, Zhoujun Li, Zhaoxiang Zhang, Jiaheng Liu, Ge Zhang, Wenhao Huang, Jason Eshraghian</strong></p>
<p>Large Language Models (LLMs) have demonstrated impressive reasoning capabilities, especially when guided by explicit chain-of-thought (CoT) reasoning that verbalizes intermediate steps. While CoT improves both interpretability and accuracy, its dependence on natural language reasoning limits the modelâ€™s expressive bandwidth. Latent reasoning tackles this bottleneck by performing multi-step inference entirely in the modelâ€™s continuous hidden state, eliminating token-level supervision. To advance latent reasoning research, this survey provides a comprehensive overview of the emerging field of latent reasoning. We begin by examining the foundational role of neural network layers as the computational substrate for reasoning, highlighting how hierarchical representations support complex transformations. Next, we explore diverse latent reasoning methodologies, including activation-based recurrence, hidden state propagation, and fine-tuning strategies that compress or internalize explicit reasoning traces. Finally, we discuss advanced paradigms such as infinite-depth latent reasoning via masked diffusion models, which enable globally consistent and reversible reasoning processes. By unifying these perspectives, we aim to clarify the conceptual landscape of latent reasoning and chart future directions for research at the frontier of LLM cognition. An associated GitHub repository collecting the latest papers and repos is available at: <a target="_blank" rel="noopener" href="https://github.com/multimodal-art-projection/LatentCoT-Horizon/">https://github.com/multimodal-art-projection/LatentCoT-Horizon/</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²ç»å±•ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„æ¨ç†èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨æ˜ç¡®çš„æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†å¼•å¯¼ä¸‹ï¼Œèƒ½å¤Ÿå£å¤´è¡¨è¾¾ä¸­é—´æ­¥éª¤ã€‚è™½ç„¶æ€ç»´é“¾æé«˜äº†å¯è§£é‡Šæ€§å’Œå‡†ç¡®æ€§ï¼Œä½†å®ƒå¯¹è‡ªç„¶è¯­è¨€æ¨ç†çš„ä¾èµ–é™åˆ¶äº†æ¨¡å‹çš„è¡¨è¾¾å¸¦å®½ã€‚æ½œåœ¨æ¨ç†é€šè¿‡å®Œå…¨åœ¨æ¨¡å‹çš„è¿ç»­éšè—çŠ¶æ€ä¸­è¿›è¡Œå¤šæ­¥æ¨ç†æ¥è§£å†³è¿™ä¸€ç“¶é¢ˆï¼Œä»è€Œæ¶ˆé™¤äº†åŸºäºç¬¦å·çš„ç›‘ç£ã€‚ä¸ºäº†æ¨åŠ¨æ½œåœ¨æ¨ç†ç ”ç©¶çš„å‘å±•ï¼Œè¿™ç¯‡ç»¼è¿°å¯¹æ–°å…´çš„é¢†åŸŸæä¾›äº†å…¨é¢çš„æ¦‚è¿°ã€‚æˆ‘ä»¬é¦–å…ˆè€ƒå¯Ÿç¥ç»ç½‘ç»œå±‚ä½œä¸ºè®¡ç®—åŸºç¡€çš„åŸºçŸ³ä½œç”¨ï¼Œå¼ºè°ƒå±‚æ¬¡åŒ–è¡¨ç¤ºå¦‚ä½•æ”¯æŒå¤æ‚è½¬æ¢ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æ¢ç´¢äº†å¤šç§æ½œåœ¨æ¨ç†æ–¹æ³•ï¼ŒåŒ…æ‹¬åŸºäºæ¿€æ´»çš„é€’å½’ã€éšè—çŠ¶æ€ä¼ æ’­ä»¥åŠå¾®è°ƒç­–ç•¥ç­‰ï¼Œè¿™äº›ç­–ç•¥å¯ä»¥å‹ç¼©æˆ–å†…åŒ–æ˜¾æ€§æ¨ç†ç—•è¿¹ã€‚æœ€åï¼Œæˆ‘ä»¬è®¨è®ºäº†å…ˆè¿›çš„èŒƒå¼ï¼Œå¦‚é€šè¿‡æ©ç æ‰©æ•£æ¨¡å‹å®ç°æ— é™æ·±åº¦çš„æ½œåœ¨æ¨ç†ï¼Œè¿™ä½¿å¾—å…¨å±€ä¸€è‡´æ€§å’Œå¯é€†çš„æ¨ç†è¿‡ç¨‹æˆä¸ºå¯èƒ½ã€‚é€šè¿‡ç»Ÿä¸€è¿™äº›è§‚ç‚¹ï¼Œæˆ‘ä»¬æ—¨åœ¨æ¾„æ¸…æ½œåœ¨æ¨ç†çš„æ¦‚å¿µæ™¯è§‚ï¼Œå¹¶ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹è®¤çŸ¥å‰æ²¿çš„ç ”ç©¶æŒ‡æ˜æœªæ¥æ–¹å‘ã€‚ç›¸å…³çš„GitHubä»“åº“æ”¶é›†äº†æœ€æ–°çš„è®ºæ–‡å’Œå­˜å‚¨åº“ï¼Œå¯ä¾›è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/multimodal-art-projection/LatentCoT-Horizon/%E3%80%82">https://github.com/multimodal-art-projection/LatentCoT-Horizon/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.06203v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡æ˜¾å¼æ€ç»´é“¾ï¼ˆCoTï¼‰æŒ‡å¯¼å±•ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„æ¨ç†èƒ½åŠ›ï¼ŒCoTèƒ½å¤Ÿè¡¨è¿°ä¸­é—´æ­¥éª¤ï¼Œæé«˜å¯è§£é‡Šæ€§å’Œå‡†ç¡®æ€§ã€‚ç„¶è€Œï¼ŒCoTå¯¹è‡ªç„¶è¯­è¨€æ¨ç†çš„ä¾èµ–é™åˆ¶äº†æ¨¡å‹çš„è¡¨è¾¾å¸¦å®½ã€‚æ½œåœ¨æ¨ç†é€šè¿‡å®Œå…¨åœ¨æ¨¡å‹çš„è¿ç»­éšè—çŠ¶æ€ä¸­è¿›è¡Œå¤šæ­¥æ¨ç†æ¥è§£å†³è¿™ä¸€ç“¶é¢ˆï¼Œæ— éœ€ä»¤ç‰Œçº§åˆ«çš„ç›‘ç£ã€‚è¿™ç¯‡ç»¼è¿°å¯¹æ½œåœ¨æ¨ç†è¿™ä¸€æ–°å…´é¢†åŸŸè¿›è¡Œäº†å…¨é¢çš„æ¦‚è¿°ï¼Œä»ç¥ç»ç½‘ç»œå±‚ä½œä¸ºæ¨ç†çš„è®¡ç®—åŸºå…ƒå‡ºå‘ï¼Œæ¢è®¨äº†å±‚æ¬¡è¡¨ç¤ºå¦‚ä½•æ”¯æŒå¤æ‚å˜æ¢ã€‚æ¥ç€æ¢ç´¢äº†å¤šç§æ½œåœ¨æ¨ç†æ–¹æ³•ï¼ŒåŒ…æ‹¬åŸºäºæ¿€æ´»çš„å¤å‘ã€éšè—çŠ¶æ€ä¼ æ’­ä»¥åŠç²¾ç»†è°ƒæ•´ç­–ç•¥ä»¥å‹ç¼©æˆ–å†…åŒ–æ˜¾å¼æ¨ç†ç—•è¿¹ã€‚æœ€åè®¨è®ºäº†é€šè¿‡æ©æ¨¡æ‰©æ•£æ¨¡å‹å®ç°æ— é™æ·±åº¦æ½œåœ¨æ¨ç†ç­‰å…ˆè¿›èŒƒå¼ï¼Œä½¿å…¨å±€ä¸€è‡´æ€§å’Œå¯é€†æ¨ç†è¿‡ç¨‹æˆä¸ºå¯èƒ½ã€‚æ—¨åœ¨ç»Ÿä¸€è¿™äº›è§‚ç‚¹ï¼Œæ¾„æ¸…æ½œåœ¨æ¨ç†çš„æ¦‚å¿µæ ¼å±€ï¼Œå¹¶ä¸ºLLMè®¤çŸ¥ç ”ç©¶çš„å‰æ²¿æŒ‡æ˜æœªæ¥ç ”ç©¶æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡æ˜¾å¼æ€ç»´é“¾ï¼ˆCoTï¼‰å±•ç°å‡ºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œä½†å­˜åœ¨è¡¨è¾¾å¸¦å®½é™åˆ¶ã€‚</li>
<li>æ½œåœ¨æ¨ç†æ–¹æ³•æ—¨åœ¨è§£å†³è¿™ä¸€ç“¶é¢ˆï¼Œå®Œå…¨åœ¨æ¨¡å‹çš„è¿ç»­éšè—çŠ¶æ€ä¸­è¿›è¡Œå¤šæ­¥æ¨ç†ï¼Œæ— éœ€ä»¤ç‰Œçº§åˆ«çš„ç›‘ç£ã€‚</li>
<li>ç¥ç»ç½‘ç»œå±‚ä½œä¸ºæ¨ç†çš„è®¡ç®—åŸºå…ƒï¼Œå±‚æ¬¡è¡¨ç¤ºæ”¯æŒå¤æ‚å˜æ¢ã€‚</li>
<li>å¤šç§æ½œåœ¨æ¨ç†æ–¹æ³•åŒ…æ‹¬åŸºäºæ¿€æ´»çš„å¤å‘ã€éšè—çŠ¶æ€ä¼ æ’­ä»¥åŠç²¾ç»†è°ƒæ•´ç­–ç•¥ã€‚</li>
<li>æ— é™æ·±åº¦æ½œåœ¨æ¨ç†é€šè¿‡æ©æ¨¡æ‰©æ•£æ¨¡å‹å®ç°ï¼Œä½¿å…¨å±€ä¸€è‡´æ€§å’Œå¯é€†æ¨ç†è¿‡ç¨‹æˆä¸ºå¯èƒ½ã€‚</li>
<li>ç»¼è¿°æ—¨åœ¨ç»Ÿä¸€æ½œåœ¨æ¨ç†çš„ä¸åŒè§‚ç‚¹ï¼Œæ¾„æ¸…æ¦‚å¿µæ ¼å±€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.06203">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a6f1945ef9271c964cc6fde42094f1e2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-204099482c33a699456b9aea8394dbbf.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Skywork-R1V3-Technical-Report"><a href="#Skywork-R1V3-Technical-Report" class="headerlink" title="Skywork-R1V3 Technical Report"></a>Skywork-R1V3 Technical Report</h2><p><strong>Authors:Wei Shen, Jiangbo Pei, Yi Peng, Xuchen Song, Yang Liu, Jian Peng, Haofeng Sun, Yunzhuo Hao, Peiyu Wang, Yahui Zhou</strong></p>
<p>We introduce Skywork-R1V3, an advanced, open-source vision-language model (VLM) that pioneers a new approach to visual reasoning. Its key innovation lies in effectively transferring reasoning skills from text-only Large Language Models (LLMs) to visual tasks. The strong performance of Skywork-R1V3 primarily stems from our elaborate post-training RL framework, which effectively activates and enhances the modelâ€™s reasoning ability, without the need for additional continue pre-training. Through this framework, we further uncover the fundamental role of the connector module in achieving robust cross-modal alignment for multimodal reasoning models. In addition, we introduce a unique indicator of reasoning capability, the entropy of critical reasoning tokens, which has proven highly effective for checkpoint selection during RL training. Skywork-R1V3 achieves state-of-the-art results on MMMU, significantly improving from 64.3% to 76.0%. This performance matches entry-level human capabilities. Remarkably, our RL-powered post-training approach enables even the 38B parameter model to rival top closed-source VLMs. The implementation successfully transfers mathematical reasoning to other subject-related reasoning tasks. We also include an analysis of curriculum learning and reinforcement finetuning strategies, along with a broader discussion on multimodal reasoning. Skywork-R1V3 represents a significant leap in multimodal reasoning, showcasing RL as a powerful engine for advancing open-source VLM capabilities. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†Skywork-R1V3ï¼Œè¿™æ˜¯ä¸€ä¸ªå…ˆè¿›çš„å¼€æºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ï¼Œå®ƒå¼€åˆ›äº†ä¸€ç§æ–°çš„è§†è§‰æ¨ç†æ–¹æ³•ã€‚å…¶ä¸»è¦åˆ›æ–°åœ¨äºæœ‰æ•ˆåœ°å°†ä»æ–‡æœ¬ä¸­è·å–çš„æ¨ç†èƒ½åŠ›è½¬ç§»åˆ°è§†è§‰ä»»åŠ¡ä¸Šã€‚Skywork-R1V3çš„å‡ºè‰²è¡¨ç°ä¸»è¦æºäºæˆ‘ä»¬ç²¾å¿ƒè®¾è®¡çš„åè®­ç»ƒå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æœ‰æ•ˆåœ°æ¿€æ´»å¹¶å¢å¼ºäº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œæ— éœ€é¢å¤–çš„ç»§ç»­é¢„è®­ç»ƒã€‚é€šè¿‡è¿™ä¸ªæ¡†æ¶ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å‘ç°äº†è¿æ¥å™¨æ¨¡å—åœ¨å®ç°ç¨³å¥çš„è·¨æ¨¡æ€å¯¹é½ä¸­çš„åŸºç¡€ä½œç”¨ï¼Œè¿™å¯¹äºå¤šæ¨¡æ€æ¨ç†æ¨¡å‹è‡³å…³é‡è¦ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç‹¬ç‰¹çš„æ¨ç†èƒ½åŠ›æŒ‡æ ‡â€”â€”å…³é”®æ¨ç†ä»£å¸çš„ç†µï¼Œè¿™åœ¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„æ£€æŸ¥ç‚¹é€‰æ‹©ä¸­è¢«è¯æ˜æ˜¯éå¸¸æœ‰æ•ˆçš„ã€‚Skywork-R1V3åœ¨MMMUä¸Šå–å¾—äº†æœ€æ–°ç»“æœï¼Œä»64.3%æ˜¾è‘—æé«˜åˆ°76.0%ã€‚è¿™ç§æ€§èƒ½ä¸å…¥é—¨çº§äººç±»èƒ½åŠ›ç›¸åŒ¹é…ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„åŸºäºå¼ºåŒ–å­¦ä¹ çš„åè®­ç»ƒæ³•ç”šè‡³ä½¿38Bå‚æ•°æ¨¡å‹èƒ½å¤Ÿä¸é¡¶çº§é—­æºVLMç›¸ç«äº‰ã€‚è¯¥å®ç°æˆåŠŸåœ°å°†æ•°å­¦æ¨ç†è½¬ç§»åˆ°å…¶ä»–ç›¸å…³æ¨ç†ä»»åŠ¡ä¸Šã€‚æˆ‘ä»¬è¿˜åŒ…æ‹¬å¯¹è¯¾ç¨‹å­¦ä¹ å’Œå¼ºåŒ–å¾®è°ƒç­–ç•¥çš„åˆ†æï¼Œä»¥åŠå¯¹å¤šæ¨¡æ€æ¨ç†çš„æ›´å¹¿æ³›è®¨è®ºã€‚Skywork-R1V3åœ¨å¤šæ¨¡æ€æ¨ç†ä¸­ä»£è¡¨äº†ä¸€ä¸ªé‡å¤§é£è·ƒï¼Œå±•ç¤ºäº†å¼ºåŒ–å­¦ä¹ ä½œä¸ºæ¨åŠ¨å¼€æºVLMèƒ½åŠ›å‘å±•çš„å¼ºå¤§å¼•æ“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.06167v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>Skywork-R1V3æ˜¯ä¸€æ¬¾å…ˆè¿›çš„å¼€æºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ï¼Œå®ƒå¼€åˆ›äº†ä¸€ç§æ–°çš„è§†è§‰æ¨ç†æ–¹æ³•ã€‚è¯¥æ¨¡å‹çš„å…³é”®åˆ›æ–°åœ¨äºæœ‰æ•ˆåœ°å°†çº¯æ–‡æœ¬å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›è½¬ç§»åˆ°è§†è§‰ä»»åŠ¡ä¸Šã€‚å…¶å¼ºå¤§çš„æ€§èƒ½ä¸»è¦æ¥è‡ªäºç²¾ç»†çš„åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„åæœŸè®­ç»ƒæ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿæ¿€æ´»å¹¶å¢å¼ºæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œæ— éœ€é¢å¤–çš„è¿ç»­é¢„è®­ç»ƒã€‚Skywork-R1V3åœ¨MMMUä¸Šå®ç°äº†æœ€æ–°ç»“æœï¼Œå…¶æ€§èƒ½ä¸äººç±»å…¥é—¨çº§èƒ½åŠ›ç›¸åŒ¹é…ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Skywork-R1V3æ˜¯ä¸€ä¸ªå…ˆè¿›çš„å¼€æºè§†è§‰è¯­è¨€æ¨¡å‹ï¼Œå…·æœ‰å¼ºå¤§çš„è§†è§‰æ¨ç†èƒ½åŠ›ã€‚</li>
<li>è¯¥æ¨¡å‹æˆåŠŸåœ°å°†çº¯æ–‡æœ¬å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›è½¬ç§»åˆ°è§†è§‰ä»»åŠ¡ä¸Šã€‚</li>
<li>Skywork-R1V3é‡‡ç”¨åŸºäºå¼ºåŒ–å­¦ä¹ çš„åæœŸè®­ç»ƒæ¡†æ¶ï¼Œæé«˜äº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>å¼•å…¥äº†æ¨ç†èƒ½åŠ›çš„æ–°æŒ‡æ ‡â€”â€”å…³é”®æ¨ç†ä»¤ç‰Œç†µï¼Œç”¨äºRLè®­ç»ƒè¿‡ç¨‹ä¸­çš„æ£€æŸ¥ç‚¹é€‰æ‹©ã€‚</li>
<li>Skywork-R1V3åœ¨MMMUä¸Šçš„æ€§èƒ½è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼Œä¸äººç±»å…¥é—¨çº§èƒ½åŠ›ç›¸åŒ¹é…ã€‚</li>
<li>è¯¥æ¨¡å‹å®ç°äº†æ•°å­¦æ¨ç†å‘å…¶ä»–ç›¸å…³æ¨ç†ä»»åŠ¡çš„æˆåŠŸè½¬ç§»ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.06167">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0265f2f378cc204078327bcf5c0735df.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6828544e25f077d8f088b10a12f9b2dc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4ec6cf3b2d1b4652d9d5c00941cb925c.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="FEVO-Financial-Knowledge-Expansion-and-Reasoning-Evolution-for-Large-Language-Models"><a href="#FEVO-Financial-Knowledge-Expansion-and-Reasoning-Evolution-for-Large-Language-Models" class="headerlink" title="FEVO: Financial Knowledge Expansion and Reasoning Evolution for Large   Language Models"></a>FEVO: Financial Knowledge Expansion and Reasoning Evolution for Large   Language Models</h2><p><strong>Authors:Bo Pang, Yalu Ouyang, Hangfei Xu, Ziqi Jia, Panpan Li, Shengzhao Wen, Lu Wang, Shiyong Li, Yanpeng Wang</strong></p>
<p>Advancements in reasoning for large language models (LLMs) have lead to significant performance improvements for LLMs in various fields such as mathematics and programming. However, research applying these advances to the financial domain, where considerable domain-specific knowledge is necessary to complete tasks, remains limited. To address this gap, we introduce FEVO (Financial Evolution), a multi-stage enhancement framework developed to enhance LLM performance in the financial domain. FEVO systemically enhances LLM performance by using continued pre-training (CPT) to expand financial domain knowledge, supervised fine-tuning (SFT) to instill structured, elaborate reasoning patterns, and reinforcement learning (RL) to further integrate the expanded financial domain knowledge with the learned structured reasoning. To ensure effective and efficient training, we leverage frontier reasoning models and rule-based filtering to curate FEVO-Train, high-quality datasets specifically designed for the different post-training phases. Using our framework, we train the FEVO series of models â€“ C32B, S32B, R32B â€“ from Qwen2.5-32B and evaluate them on seven benchmarks to assess financial and general capabilities, with results showing that FEVO-R32B achieves state-of-the-art performance on five financial benchmarks against much larger models as well as specialist models. More significantly, FEVO-R32B demonstrates markedly better performance than FEVO-R32B-0 (trained from Qwen2.5-32B-Instruct using only RL), thus validating the effectiveness of financial domain knowledge expansion and structured, logical reasoning distillation </p>
<blockquote>
<p>åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†æŠ€æœ¯æ–¹é¢å–å¾—çš„è¿›å±•ï¼Œå·²ç»å¤§å¤§æé«˜äº†LLMåœ¨æ•°å­¦ã€ç¼–ç¨‹ç­‰é¢†åŸŸä¸­çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå°†è¿™äº›è¿›å±•åº”ç”¨äºé‡‘èé¢†åŸŸçš„ç ”ç©¶ä»ç„¶æœ‰é™ï¼Œå®Œæˆé‡‘èé¢†åŸŸçš„ä»»åŠ¡éœ€è¦å¤§é‡çš„é¢†åŸŸç‰¹å®šçŸ¥è¯†ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¼•å…¥äº†FEVOï¼ˆé‡‘èè¿›åŒ–ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸ºå¢å¼ºLLMåœ¨é‡‘èé¢†åŸŸçš„æ€§èƒ½è€Œå¼€å‘çš„å¤šé˜¶æ®µå¢å¼ºæ¡†æ¶ã€‚FEVOç³»ç»Ÿé€šè¿‡æŒç»­é¢„è®­ç»ƒï¼ˆCPTï¼‰æ¥æ‰©å±•é‡‘èé¢†åŸŸçŸ¥è¯†ï¼Œé€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ¥çŒè¾“ç»“æ„åŒ–ã€ç²¾ç»†çš„æ¨ç†æ¨¡å¼ï¼Œä»¥åŠé€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¥è¿›ä¸€æ­¥å°†æ‰©å±•çš„é‡‘èé¢†åŸŸçŸ¥è¯†ä¸å­¦åˆ°çš„ç»“æ„åŒ–æ¨ç†ç›¸ç»“åˆï¼Œä»è€Œæé«˜LLMçš„æ€§èƒ½ã€‚ä¸ºäº†ç¡®ä¿æœ‰æ•ˆå’Œé«˜æ•ˆçš„è®­ç»ƒï¼Œæˆ‘ä»¬åˆ©ç”¨å‰æ²¿çš„æ¨ç†æ¨¡å‹å’ŒåŸºäºè§„åˆ™çš„è¿‡æ»¤æ¥åˆ›å»ºä¸“é—¨ä¸ºä¸åŒåè®­ç»ƒé˜¶æ®µè®¾è®¡çš„FEVO-Trainé«˜è´¨é‡æ•°æ®é›†ã€‚ä½¿ç”¨æˆ‘ä»¬çš„æ¡†æ¶ï¼Œæˆ‘ä»¬è®­ç»ƒäº†FEVOç³»åˆ—æ¨¡å‹â€”â€”C32Bã€S32Bã€R32Bï¼Œå®ƒä»¬ä»¥Qwen2.5-32Bä¸ºåŸºç¡€ï¼Œå¹¶åœ¨ä¸ƒä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¯„ä¼°å®ƒä»¬çš„é‡‘èå’Œä¸€èˆ¬èƒ½åŠ›ã€‚ç»“æœè¡¨æ˜ï¼ŒFEVO-R32Båœ¨äº”ä¸ªé‡‘èåŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä¸æ›´å¤§çš„æ¨¡å‹ä»¥åŠä¸“ä¸šæ¨¡å‹ç›¸æ¯”å…·æœ‰ä¼˜åŠ¿ã€‚æ›´é‡è¦çš„æ˜¯ï¼ŒFEVO-R32Bçš„æ€§èƒ½æ˜æ˜¾ä¼˜äºFEVO-R32B-0ï¼ˆä»¥Qwen2.5-32B-Instructä¸ºåŸºç¡€ä»…ä½¿ç”¨RLè¿›è¡Œè®­ç»ƒï¼‰ï¼Œä»è€ŒéªŒè¯äº†é‡‘èé¢†åŸŸçŸ¥è¯†æ‰©å±•å’Œç»“æ„åŒ–ã€é€»è¾‘æ¨ç†è’¸é¦çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.06057v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ¨ç†æ–¹é¢çš„è¿›å±•å·²ç»æå‡äº†å…¶åœ¨æ•°å­¦ã€ç¼–ç¨‹ç­‰é¢†åŸŸçš„åº”ç”¨æ€§èƒ½ã€‚ç„¶è€Œï¼Œå°†è¿™ä¸€è¿›å±•åº”ç”¨äºé‡‘èé¢†åŸŸçš„ç ”ç©¶ä»ç„¶æœ‰é™ï¼Œé‡‘èé¢†åŸŸéœ€è¦ç‰¹å®šçš„ä¸“ä¸šçŸ¥è¯†æ¥å®Œæˆä»»åŠ¡ã€‚ä¸ºè§£å†³è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æ¨å‡ºäº†FEVOï¼ˆé‡‘èè¿›åŒ–ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šé˜¶æ®µå¢å¼ºæ¡†æ¶ï¼Œæ—¨åœ¨æé«˜LLMåœ¨é‡‘èé¢†åŸŸçš„æ€§èƒ½ã€‚é€šè¿‡ç»§ç»­é¢„è®­ç»ƒï¼ˆCPTï¼‰æ¥æ‰©å±•é‡‘èé¢†åŸŸçŸ¥è¯†ï¼Œé€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ¥åŸ¹å…»ç»“æ„åŒ–ã€ç²¾ç»†çš„æ¨ç†æ¨¡å¼ï¼Œä»¥åŠé€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¥è¿›ä¸€æ­¥å°†æ‰©å±•çš„é‡‘èé¢†åŸŸçŸ¥è¯†ä¸å­¦ä¹ çš„ç»“æ„åŒ–æ¨ç†ç›¸ç»“åˆï¼Œç³»ç»Ÿæ€§åœ°æé«˜LLMçš„æ€§èƒ½ã€‚æˆ‘ä»¬è®­ç»ƒäº†FEVOç³»åˆ—æ¨¡å‹â€”â€”C32Bã€S32Bã€R32Bï¼Œå¹¶åœ¨ä¸ƒä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¯„ä¼°äº†å®ƒä»¬çš„é‡‘èå’Œä¸€èˆ¬èƒ½åŠ›ï¼Œç»“æœæ˜¾ç¤ºFEVO-R32Båœ¨äº”ä¸ªé‡‘èåŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ€§èƒ½ï¼Œå¹¶ä¸”åœ¨é’ˆå¯¹æ›´å¤§æ¨¡å‹å’Œä¸“å®¶æ¨¡å‹çš„æ¯”è¾ƒä¸­ä¹Ÿè¡¨ç°å‡ºè‰²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨æ¨ç†æ–¹é¢çš„è¿›æ­¥å·²åº”ç”¨äºå¤šä¸ªé¢†åŸŸï¼Œä½†åœ¨é‡‘èé¢†åŸŸçš„åº”ç”¨ä»ç„¶æœ‰é™ã€‚</li>
<li>FEVOæ˜¯ä¸€ä¸ªå¤šé˜¶æ®µå¢å¼ºæ¡†æ¶ï¼Œæ—¨åœ¨æé«˜LLMåœ¨é‡‘èé¢†åŸŸçš„æ€§èƒ½ã€‚</li>
<li>FEVOä½¿ç”¨ç»§ç»­é¢„è®­ç»ƒï¼ˆCPTï¼‰æ¥æ‰©å±•é‡‘èçŸ¥è¯†ï¼Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰åŸ¹å…»ç»“æ„åŒ–æ¨ç†ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ•´åˆçŸ¥è¯†ä¸æ¨ç†ã€‚</li>
<li>FEVOç³»åˆ—æ¨¡å‹â€”â€”C32Bã€S32Bã€R32Bå·²è®­ç»ƒï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>FEVO-R32Båœ¨äº”ä¸ªé‡‘èåŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°æœ€æ–°æŠ€æœ¯æ€§èƒ½ï¼Œå¹¶åœ¨ä¸æ›´å¤§æ¨¡å‹å’Œä¸“å®¶æ¨¡å‹çš„æ¯”è¾ƒä¸­è¡¨ç°çªå‡ºã€‚</li>
<li>FEVOæ¡†æ¶çš„æœ‰æ•ˆæ€§å¾—åˆ°äº†éªŒè¯ï¼Œé€šè¿‡æ¯”è¾ƒFEVO-R32Bä¸ä»…ä½¿ç”¨RLè®­ç»ƒçš„FEVO-R32B-0çš„æ€§èƒ½å·®å¼‚ã€‚</li>
<li>FEVOæ¡†æ¶ä¸ºé‡‘èé¢†åŸŸçš„å¤§è¯­è¨€æ¨¡å‹åº”ç”¨å¼€è¾Ÿäº†æ–°é€”å¾„ï¼Œæœ‰æœ›æ¨åŠ¨é‡‘èé¢†åŸŸçš„AIæŠ€æœ¯å‘å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.06057">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-17c5bed13f2cc82968a2cea726f739a3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-25af8dd35405c59ef930028bf48cb0b1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8455eab949d3c3384bb9b2aa0b3c8f04.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e8fc8bf273f3fddee2ac1530d2fd45b1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e98a8f549f38a7cd8cb4b456fcdc8ecb.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="CogniSQL-R1-Zero-Lightweight-Reinforced-Reasoning-for-Efficient-SQL-Generation"><a href="#CogniSQL-R1-Zero-Lightweight-Reinforced-Reasoning-for-Efficient-SQL-Generation" class="headerlink" title="CogniSQL-R1-Zero: Lightweight Reinforced Reasoning for Efficient SQL   Generation"></a>CogniSQL-R1-Zero: Lightweight Reinforced Reasoning for Efficient SQL   Generation</h2><p><strong>Authors:Kushal Gajjar, Harshit Sikchi, Arpit Singh Gautam, Marc Hammons, Saurabh Jha</strong></p>
<p>Translating natural language into SQL (Text-to-SQL) remains a core challenge at the intersection of language understanding and structured data access. Although large language models (LLMs) have improved fluency, generating correct and executable SQL, especially for complex queries, continues to be challenging. We introduce CogniSQL-R1-Zero, a reinforcement learning (RL) framework and model that produces accurate SQL using a lightweight reward signal based on execution correctness and format-tag compliance. By avoiding intermediate supervision, hybrid pipelines and complex reward shaping, our method encourages stable learning and stronger alignment with the ultimate task objective-producing executable programs. CogniSQL-R1-Zero achieves state-of-the-art execution accuracy on Text2SQL benchmark; BIRD bench, outperforming prior supervised and instruction-tuned baselines including SFT CodeS-7B, DeepSeek-Coder 236B, and Mistral 123B-despite being trained on a significantly smaller 7B backbone. This result underscores the scalability and efficiency of our RL-based approach when trained on just four NVIDIA A100 GPUs (40 GB VRAM each). To support further research in efficient and interpretable Text-to-SQL modeling, we release two curated datasets: (i) a collection of 5,024 reasoning traces with varying context lengths, and (ii) a positive-sampled corpus of 36,356 corpus of weakly supervised queries, each annotated with six semantically diverse reasoning paths. Together, these contributions advance scalable, execution-aligned Text-to-SQL generation. </p>
<blockquote>
<p>å°†è‡ªç„¶è¯­è¨€è½¬æ¢ä¸ºSQLï¼ˆæ–‡æœ¬åˆ°SQLï¼‰ä»ç„¶æ˜¯è¯­è¨€ç†è§£ä¸ç»“æ„åŒ–æ•°æ®è®¿é—®äº¤å‰ç‚¹ä¸Šçš„ä¸€ä¸ªæ ¸å¿ƒæŒ‘æˆ˜ã€‚å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æé«˜äº†æµç•…åº¦ï¼Œä½†ç”Ÿæˆæ­£ç¡®ä¸”å¯æ‰§è¡Œçš„SQLï¼Œå°¤å…¶æ˜¯å¯¹äºå¤æ‚æŸ¥è¯¢ï¼Œä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æˆ‘ä»¬æ¨å‡ºäº†CogniSQL-R1-Zeroï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ¡†æ¶å’Œæ¨¡å‹ï¼Œå®ƒä½¿ç”¨åŸºäºæ‰§è¡Œæ­£ç¡®æ€§å’Œæ ¼å¼æ ‡ç­¾ç¬¦åˆåº¦çš„è½»é‡çº§å¥–åŠ±ä¿¡å·æ¥ç”Ÿæˆå‡†ç¡®çš„SQLã€‚é€šè¿‡é¿å…ä¸­é—´ç›‘ç£ã€æ··åˆç®¡é“å’Œå¤æ‚çš„å¥–åŠ±å¡‘é€ ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é¼“åŠ±ç¨³å®šçš„å­¦ä¹ ï¼Œå¹¶ä¸æœ€ç»ˆä»»åŠ¡ç›®æ ‡â€”â€”äº§ç”Ÿå¯æ‰§è¡Œç¨‹åºâ€”â€”æ›´ç´§å¯†åœ°ç»“åˆã€‚CogniSQL-R1-Zeroåœ¨Text2SQLåŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ‰§è¡Œç²¾åº¦ï¼›BIRDåŸºå‡†æµ‹è¯•ï¼Œè¶…è¶Šäº†å…ˆå‰çš„ç›‘ç£æŒ‡ä»¤è°ƒä¼˜åŸºçº¿ï¼ŒåŒ…æ‹¬SFT CodeS-7Bã€DeepSeek-Coder 236Bå’ŒMistral 123Bï¼Œå°½ç®¡å®ƒæ˜¯åœ¨ä¸€ä¸ªæ˜¾è‘—è¾ƒå°çš„7Bä¸»å¹²ä¸Šè®­ç»ƒçš„ã€‚è¿™ä¸€ç»“æœçªå‡ºäº†æˆ‘ä»¬çš„åŸºäºRLçš„æ–¹æ³•åœ¨ä»…ä½¿ç”¨å››å°NVIDIA A100 GPUï¼ˆæ¯å°40GB VRAMï¼‰è¿›è¡Œè®­ç»ƒæ—¶çš„å¯æ‰©å±•æ€§å’Œæ•ˆç‡ã€‚ä¸ºäº†æ”¯æŒåœ¨é«˜æ•ˆå’Œå¯è§£é‡Šçš„æ–‡æœ¬åˆ°SQLå»ºæ¨¡æ–¹é¢çš„è¿›ä¸€æ­¥ç ”ç©¶ï¼Œæˆ‘ä»¬å‘å¸ƒäº†ä¸¤ä¸ªç²¾é€‰æ•°æ®é›†ï¼šï¼ˆiï¼‰åŒ…å«5024ä¸ªæ¨ç†è½¨è¿¹ï¼Œå…·æœ‰ä¸åŒçš„ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œï¼ˆiiï¼‰ä¸€ä¸ªç”±å¼±ç›‘ç£æŸ¥è¯¢ç»„æˆçš„ç§¯æé‡‡æ ·è¯­æ–™åº“ï¼Œæ¯ä¸ªæŸ¥è¯¢éƒ½æ ‡æœ‰å…­æ¡è¯­ä¹‰ä¸Šä¸åŒçš„æ¨ç†è·¯å¾„ã€‚è¿™äº›è´¡çŒ®å…±åŒæ¨åŠ¨äº†å¯æ‰©å±•çš„ã€ä¸æ‰§è¡Œå¯¹é½çš„æ–‡æœ¬åˆ°SQLç”Ÿæˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.06013v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨è‡ªç„¶è¯­è¨€åˆ°SQLè½¬æ¢ï¼ˆText-to-SQLï¼‰é¢†åŸŸçš„æŒ‘æˆ˜ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§æ–°çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶â€”â€”CogniSQL-R1-Zeroæ¨¡å‹ã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿé€šè¿‡æ‰§è¡Œæ­£ç¡®æ€§å’Œæ ¼å¼æ ‡ç­¾éµå®ˆçš„è½»é‡çº§å¥–åŠ±ä¿¡å·æ¥ç”Ÿæˆå‡†ç¡®çš„SQLã€‚åœ¨BIRD benchä¸Šçš„æ‰§è¡Œå‡†ç¡®ç‡è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼Œå°½ç®¡å®ƒçš„è®­ç»ƒæ˜¯åŸºäºè¾ƒå°çš„7Béª¨æ¶å®Œæˆçš„ã€‚æ­¤å¤–ï¼Œè¿˜å‘å¸ƒäº†ä¸¤ä¸ªæ•°æ®é›†ä»¥æ”¯æŒè¿›ä¸€æ­¥çš„Text-to-SQLå»ºæ¨¡ç ”ç©¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Text-to-SQLè½¬æ¢æ˜¯è‡ªç„¶è¯­è¨€ç†è§£å’Œç»“æ„åŒ–æ•°æ®è®¿é—®çš„äº¤å‰ç‚¹çš„ä¸€ä¸ªé‡è¦æŒ‘æˆ˜ã€‚</li>
<li>å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æé«˜äº†æµç•…åº¦ï¼Œä½†ç”Ÿæˆæ­£ç¡®ä¸”å¯æ‰§è¡Œçš„SQLå¯¹äºå¤æ‚æŸ¥è¯¢ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶â€”â€”CogniSQL-R1-Zeroæ¨¡å‹ï¼Œèƒ½å¤Ÿå‡†ç¡®ç”ŸæˆSQLã€‚</li>
<li>è¯¥æ¨¡å‹ä½¿ç”¨è½»é‡çº§å¥–åŠ±ä¿¡å·ï¼ŒåŸºäºæ‰§è¡Œæ­£ç¡®æ€§å’Œæ ¼å¼æ ‡ç­¾éµå®ˆï¼Œé¿å…ä¸­é—´ç›‘ç£ã€æ··åˆç®¡é“å’Œå¤æ‚çš„å¥–åŠ±å¡‘å½¢ã€‚</li>
<li>CogniSQL-R1-Zeroåœ¨Text2SQLåŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ‰§è¡Œå‡†ç¡®ç‡ï¼Œä¸”è®­ç»ƒåŸºäºè¾ƒå°çš„7Béª¨æ¶å®Œæˆã€‚</li>
<li>å‘å¸ƒäº†ä¸¤ä¸ªæ•°æ®é›†ä»¥æ”¯æŒè¿›ä¸€æ­¥çš„Text-to-SQLå»ºæ¨¡ç ”ç©¶ï¼ŒåŒ…æ‹¬åŒ…å«ä¸åŒä¸Šä¸‹æ–‡é•¿åº¦çš„æ¨ç†è½¨è¿¹é›†åˆå’Œç§¯ææ ·æœ¬è¯­æ–™åº“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.06013">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bd99e987a5dda13fd1c6b49a3b7b7641.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="BlueLM-2-5-3B-Technical-Report"><a href="#BlueLM-2-5-3B-Technical-Report" class="headerlink" title="BlueLM-2.5-3B Technical Report"></a>BlueLM-2.5-3B Technical Report</h2><p><strong>Authors:Baojiao Xiong, Boheng Chen, Chengzhi Wang, Daxiong Luo, Dongsheng Xu, Dongyang Liu, Fan Yang, Fangyuan Li, Fei Teng, Feng Wang, Fukang Qin, Fuquan Peng, Guanxin Tan, Guozhi Wang, Haibo Yu, Haohao Gao, Heng Liu, Hongbo Yang, Hongjian Zou, Houzheng Shen, Hu Meng, Huan Li, Hui Tan, Jiali Chen, Jianzhao Chen, Jinliang Zhu, Kai Wang, Lei Wu, Liangbing Liu, Liuyang Bian, Liyan He, Long Liu, Peiwen Li, Penggang Shi, Qi Ding, Rui Hu, Shuai Cao, Shuai Ren, Shuang Peng, Teng Xie, Weiji Chen, Weilin Xiang, Weixin Wu, Xi Yin, Xiaoxin Chen, Xu Chen, Yafei Wen, Yan Hu, Yanzhou Yang, Yina Xie, Yinghao Chen, Yixuan Liao, Yu Geng, Yuanjiang Ouyang, Yuanzhuo Yang, Yuehua He, Yushuai Peng, Zhaoxiong Wang, Zheng Wang, Zhibo Zhou, Ziyang Wu</strong></p>
<p>We present BlueLM-2.5-3B, a compact and unified dense Multimodal Large Language Model (MLLM) designed for efficient edge-device deployment, offering strong general-purpose and reasoning capabilities. To the best of our knowledge, this is the first 3B-scale MLLM to support both thinking and non-thinking modes, while also enabling explicit control over thinking token budget. BlueLM-2.5-3B is developed through diversified data curation, key data resampling, hybrid heterogeneous reinforcement learning, and a high-performance training infrastructure. Our model achieves superior multimodal capacity while preserving competitive pure-text performance with only 2.9 billion parameters. We conduct comprehensive evaluations across a broad range of multimodal and text-only benchmarks. In thinking mode, BlueLM-2.5-3B achieves comparable performance to Qwen3-4B on text-only benchmarks, and trails the larger Kimi-VL-A3B-16B by only about 5% on average across multimodal evaluations. In non-thinking mode, it outperforms Qwen2.5-VL-3B on the majority of multimodal benchmarks. Additionally, BlueLM-2.5-3B exhibits exceptional data efficiency. All of the aforementioned performance is achieved with substantially less total training data than Qwen2.5-VL-3B and Qwen3-4B. We hope our work contributes to the advancement of high-performance, on-device MLLMs and provides meaningful insights to the research community. </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºBlueLM-2.5-3Bï¼Œè¿™æ˜¯ä¸€æ¬¾ç´§å‡‘ä¸”ç»Ÿä¸€çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œä¸“ä¸ºè¾¹ç¼˜è®¾å¤‡çš„æœ‰æ•ˆéƒ¨ç½²è€Œè®¾è®¡ï¼Œå…·å¤‡å¼ºå¤§çš„é€šç”¨å’Œæ¨ç†èƒ½åŠ›ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯é¦–ä¸ªæ”¯æŒæ€è€ƒå’Œéæ€è€ƒæ¨¡å¼çš„3Bè§„æ¨¡MLLMï¼ŒåŒæ—¶èƒ½å¤Ÿæ˜ç¡®æ§åˆ¶æ€è€ƒä»¤ç‰Œé¢„ç®—ã€‚BlueLM-2.5-3Bé€šè¿‡å¤šæ ·åŒ–æ•°æ®æ”¶é›†ã€å…³é”®æ•°æ®é‡æ–°é‡‡æ ·ã€æ··åˆå¼‚æ„å¢å¼ºå­¦ä¹ å’Œé«˜æ€§èƒ½è®­ç»ƒåŸºç¡€è®¾æ–½è€Œå¼€å‘ã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨ä¿æŒä»…2.9äº¿å‚æ•°çš„åŒæ—¶ï¼Œå®ç°äº†å“è¶Šçš„å¤šæ¨¡æ€èƒ½åŠ›ï¼Œå¹¶åœ¨å¤šç§å¤šæ¨¡æ€å’Œçº¯æ–‡æœ¬åŸºå‡†æµ‹è¯•ä¸­è¿›è¡Œäº†å…¨é¢è¯„ä¼°ã€‚åœ¨æ€è€ƒæ¨¡å¼ä¸‹ï¼ŒBlueLM-2.5-3Båœ¨çº¯æ–‡æœ¬åŸºå‡†æµ‹è¯•ä¸­çš„æ€§èƒ½ä¸Qwen3-4Bç›¸å½“ï¼Œåœ¨å¤šæ¨¡æ€è¯„ä¼°ä¸Šçš„å¹³å‡è¡¨ç°ä»…æ¬¡äºKimi-VL-A3B-16Bï¼Œå·®è·çº¦ä¸º5%ã€‚åœ¨éæ€è€ƒæ¨¡å¼ä¸‹ï¼Œå®ƒåœ¨å¤§å¤šæ•°å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°ä¼˜äºQwen2.5-VL-3Bã€‚æ­¤å¤–ï¼ŒBlueLM-2.5-3Bè¡¨ç°å‡ºå“è¶Šçš„æ•°æ®æ•ˆç‡ã€‚ä¸Šè¿°æ‰€æœ‰æ€§èƒ½éƒ½æ˜¯åœ¨ä½¿ç”¨çš„æ€»è®­ç»ƒæ•°æ®é‡è¿œè¿œå°‘äºQwen2.5-VL-3Bå’ŒQwen3-4Bçš„æƒ…å†µä¸‹å®ç°çš„ã€‚æˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„ç ”ç©¶èƒ½ä¸ºé«˜æ€§èƒ½çš„ã€è®¾å¤‡ä¸Šçš„MLLMåšå‡ºè´¡çŒ®ï¼Œå¹¶ä¸ºç ”ç©¶ç¤¾åŒºæä¾›æœ‰æ„ä¹‰çš„è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.05934v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>æˆ‘ä»¬æ¨å‡ºäº†BlueLM-2.5-3Bï¼Œè¿™æ˜¯ä¸€æ¬¾é’ˆå¯¹è¾¹ç¼˜è®¾å¤‡éƒ¨ç½²è€Œè®¾è®¡çš„ç´§å‡‘ã€ç»Ÿä¸€çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ã€‚è¯¥æ¨¡å‹å…·å¤‡å¼ºå¤§çš„é€šç”¨å’Œæ¨ç†èƒ½åŠ›ï¼Œæ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œå®ƒæ˜¯é¦–ä¸ªæ”¯æŒæ€è€ƒå’Œéæ€è€ƒæ¨¡å¼çš„åŒæ—¶è¿˜èƒ½æ§åˆ¶æ€è€ƒä»¤ç‰Œé¢„ç®—çš„3Bè§„æ¨¡MLLMã€‚é€šè¿‡å¤šæ ·åŒ–æ•°æ®æ”¶é›†ã€å…³é”®æ•°æ®é‡é‡‡æ ·ã€æ··åˆå¼‚æ„å¢å¼ºå­¦ä¹ ä»¥åŠé«˜æ€§èƒ½è®­ç»ƒåŸºç¡€è®¾æ–½ï¼ŒBlueLM-2.5-3Bå®ç°äº†å“è¶Šçš„å¤šæ¨¡æ€èƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒä»…æœ‰2.9äº¿å‚æ•°æ—¶çš„æ–‡æœ¬æ€§èƒ½ç«äº‰åŠ›ã€‚åœ¨å¹¿æ³›çš„è·¨å¤šæ¨¡æ€å’Œæ–‡æœ¬åŸºå‡†æµ‹è¯•ä¸­è¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œå…¶æ€§èƒ½ä»¤äººç©ç›®ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>BlueLM-2.5-3Bæ˜¯ä¸€ä¸ªé€‚ç”¨äºè¾¹ç¼˜è®¾å¤‡éƒ¨ç½²çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ã€‚</li>
<li>è¯¥æ¨¡å‹å…·å¤‡å¼ºå¤§çš„é€šç”¨å’Œæ¨ç†èƒ½åŠ›ã€‚</li>
<li>BlueLM-2.5-3Bæ”¯æŒæ€è€ƒå’Œéæ€è€ƒä¸¤ç§æ¨¡å¼ï¼Œå¹¶å¯ä»¥æ§åˆ¶æ€è€ƒä»¤ç‰Œé¢„ç®—ã€‚</li>
<li>è¯¥æ¨¡å‹é€šè¿‡å¤šæ ·åŒ–æ•°æ®æ”¶é›†ã€å…³é”®æ•°æ®é‡é‡‡æ ·ç­‰æŠ€æœ¯å®ç°å“è¶Šçš„å¤šæ¨¡æ€èƒ½åŠ›ã€‚</li>
<li>BlueLM-2.5-3Båœ¨æ–‡æœ¬åŸºå‡†æµ‹è¯•ä¸­çš„æ€§èƒ½ä¸Qwen3-4Bç›¸å½“ï¼Œå¹¶åœ¨å¤šæ¨¡æ€è¯„ä¼°ä¸­ä»…è½åKimi-VL-A3B-16Bçº¦5%ã€‚</li>
<li>åœ¨éæ€è€ƒæ¨¡å¼ä¸‹ï¼ŒBlueLM-2.5-3Båœ¨å¤§å¤šæ•°å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­ä¼˜äºQwen2.5-VL-3Bã€‚</li>
<li>BlueLM-2.5-3Bå±•ç°å‡ºå“è¶Šçš„æ•°æ®æ•ˆç‡ï¼Œä½¿ç”¨çš„æ•°æ®é‡è¿œä½äºQwen2.5-VL-3Bå’ŒQwen3-4Bã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.05934">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d54e4290f0778cd47dd992a44a024fe0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1586a405acb3d7103ad4581528d02acf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bfbf6172b548c9b822f25faf5d57dc9c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-787188ef5bfbe6197ffbfd1b1cd78086.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="High-Resolution-Visual-Reasoning-via-Multi-Turn-Grounding-Based-Reinforcement-Learning"><a href="#High-Resolution-Visual-Reasoning-via-Multi-Turn-Grounding-Based-Reinforcement-Learning" class="headerlink" title="High-Resolution Visual Reasoning via Multi-Turn Grounding-Based   Reinforcement Learning"></a>High-Resolution Visual Reasoning via Multi-Turn Grounding-Based   Reinforcement Learning</h2><p><strong>Authors:Xinyu Huang, Yuhao Dong, Weiwei Tian, Bo Li, Rui Feng, Ziwei Liu</strong></p>
<p>State-of-the-art large multi-modal models (LMMs) face challenges when processing high-resolution images, as these inputs are converted into enormous visual tokens, many of which are irrelevant to the downstream task. In this paper, we propose Multi-turn Grounding-based Policy Optimization (MGPO), an end-to-end reinforcement learning (RL) framework that enables LMMs to iteratively focus on key visual regions by automatically cropping sub-images, based on model-predicted grounding coordinates within a multi-turn conversation framework. Compared to supervised fine-tuning (SFT), which requires costly additional grounding annotations, our approach highlights that LMMs can emerge robust grounding abilities during the RL training process, leveraging only a binary reward function derived from the correctness of the final answer. Additionally, we observe that LMMs struggle to autonomously trigger visual grounding during the rollout process. To address this cold start problem, we design a multi-turn conversational template and restrict policy loss computation to model outputs generated across multiple dialogue rounds, thereby promoting stable optimization. Extensive experiments demonstrate that, when trained on standard visual-question-short answering data without grounding annotations, MGPO effectively elicits stronger grounding capabilities compared to GRPO, leading to 5.4% improvement on in-distribution MME-Realworld and 5.2% improvement on the challenging out-of-distribution (OOD) V* Bench. Notably, MGPO post-training on Qwen2.5-VL-7B with 21K samples surpasses OpenAIâ€™s o1 and GPT-4o models on the OOD V* Bench. Codes are available at <a target="_blank" rel="noopener" href="https://github.com/EvolvingLMMs-Lab/MGPO">https://github.com/EvolvingLMMs-Lab/MGPO</a>. </p>
<blockquote>
<p>å½“å‰å…ˆè¿›çš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨å¤„ç†é«˜åˆ†è¾¨ç‡å›¾åƒæ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œå› ä¸ºè¿™äº›è¾“å…¥ä¼šè¢«è½¬åŒ–ä¸ºå·¨å¤§çš„è§†è§‰ä»¤ç‰Œï¼Œå…¶ä¸­è®¸å¤šä¸ä¸‹æ¸¸ä»»åŠ¡æ— å…³ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºå¤šè½®å®šä½çš„ç­–ç•¥ä¼˜åŒ–ï¼ˆMGPOï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç«¯åˆ°ç«¯çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¡†æ¶ï¼Œå®ƒä½¿LMMsèƒ½å¤Ÿé€šè¿‡è‡ªåŠ¨è£å‰ªå­å›¾åƒï¼Œåœ¨å¤šè½®å¯¹è¯æ¡†æ¶å†…åŸºäºæ¨¡å‹é¢„æµ‹çš„å®šä½åæ ‡æ¥è¿­ä»£å…³æ³¨å…³é”®è§†è§‰åŒºåŸŸã€‚ä¸éœ€è¦æ˜‚è´µé¢å¤–å®šä½æ³¨é‡Šçš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¼ºè°ƒï¼ŒLMMså¯ä»¥åœ¨RLè®­ç»ƒè¿‡ç¨‹ä¸­å½¢æˆç¨³å¥çš„å®šä½èƒ½åŠ›ï¼Œä»…åˆ©ç”¨åŸºäºæœ€ç»ˆç­”æ¡ˆæ­£ç¡®æ€§çš„äºŒå…ƒå¥–åŠ±å‡½æ•°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°LMMsåœ¨æ»šåŠ¨è¿‡ç¨‹ä¸­è‡ªä¸»è§¦å‘è§†è§‰å®šä½æ–¹é¢å­˜åœ¨å›°éš¾ã€‚ä¸ºäº†è§£å†³å†·å¯åŠ¨é—®é¢˜ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªå¤šè½®å¯¹è¯æ¨¡æ¿ï¼Œå¹¶å°†æ”¿ç­–æŸå¤±è®¡ç®—é™åˆ¶åœ¨å¤šä¸ªå¯¹è¯è½®æ¬¡ä¸­ç”Ÿæˆçš„æ¨¡å‹è¾“å‡ºä¸Šï¼Œä»è€Œä¿ƒè¿›ç¨³å®šä¼˜åŒ–ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œåœ¨æ ‡å‡†è§†è§‰é—®ç­”ç®€çŸ­å›ç­”æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒæ—¶ï¼Œæ— éœ€å®šä½æ³¨é‡Šçš„MGPOèƒ½æœ‰æ•ˆåœ°å¼•å‘æ›´å¼ºçš„å®šä½èƒ½åŠ›ï¼Œä¸GRPOç›¸æ¯”ï¼Œåœ¨å†…éƒ¨åˆ†å¸ƒMME-Realworldä¸Šæé«˜äº†5.4%ï¼Œåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„å¤–éƒ¨åˆ†å¸ƒï¼ˆOODï¼‰V* Benchä¸Šæé«˜äº†5.2%ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒMGPOåœ¨Qwen2.5-VL-7Bæ¨¡å‹ä¸Šä½¿ç”¨21Kæ ·æœ¬è¿›è¡Œåè®­ç»ƒï¼Œåœ¨OOD V* Benchä¸Šè¶…è¶Šäº†OpenAIçš„o1å’ŒGPT-4oæ¨¡å‹ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/EvolvingLMMs-Lab/MGPO">https://github.com/EvolvingLMMs-Lab/MGPO</a>ä¸­æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.05920v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰è§†è§‰å®šä½ç­–ç•¥ä¼˜åŒ–æ–¹æ³•â€”â€”å¤šè½®å®šä½åŸºç¡€ç­–ç•¥ä¼˜åŒ–ï¼ˆMGPOï¼‰ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨æ— éœ€é¢å¤–å®šä½æ ‡æ³¨çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡ç«¯åˆ°ç«¯çš„å¼ºåŒ–å­¦ä¹ è¿‡ç¨‹ï¼Œä½¿LMMsåœ¨å¯¹è¯æ¡†æ¶å†…è‡ªåŠ¨è£å‰ªå­å›¾åƒï¼Œå¹¶åŸºäºæ¨¡å‹é¢„æµ‹çš„å®šä½åæ ‡è¿›è¡Œè¿­ä»£ä¼˜åŒ–ã€‚ä¸ºè§£å†³å†·å¯åŠ¨é—®é¢˜ï¼Œè®¾è®¡å¤šè½®å¯¹è¯æ¨¡æ¿å¹¶é™åˆ¶ç­–ç•¥æŸå¤±è®¡ç®—åœ¨å¤šè½®å¯¹è¯è¾“å‡ºä¸­ï¼Œä¿ƒè¿›ç¨³å®šä¼˜åŒ–ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨æ— éœ€å®šä½æ ‡æ³¨çš„æ ‡å‡†è§†è§‰é—®ç­”æ•°æ®é›†ä¸Šè®­ç»ƒçš„MGPOï¼Œç›¸å¯¹äºä¼ ç»Ÿæ–¹æ³•ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡æ¨¡å‹çš„å®šä½èƒ½åŠ›ï¼Œå¹¶åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æœªè§æ•°æ®é›†ä¸Šå–å¾—æ˜¾è‘—æ•ˆæœã€‚ä»£ç å·²å…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MGPOæ˜¯ä¸€ä¸ªåŸºäºå¼ºåŒ–å­¦ä¹ çš„æ¡†æ¶ï¼Œæ—¨åœ¨ä½¿å¤§å‹å¤šæ¨¡æ€æ¨¡å‹åœ¨å¤„ç†é«˜åˆ†è¾¨ç‡å›¾åƒæ—¶èƒ½å¤Ÿè‡ªåŠ¨è£å‰ªå’Œå…³æ³¨å…³é”®è§†è§‰åŒºåŸŸã€‚</li>
<li>MGPOé€šè¿‡æ¨¡å‹é¢„æµ‹çš„å®šä½åæ ‡è¿›è¡Œè¿­ä»£ä¼˜åŒ–ï¼Œæ— éœ€é¢å¤–çš„å®šä½æ ‡æ³¨ã€‚</li>
<li>LMMsåœ¨è‡ªä¸»è§¦å‘è§†è§‰å®šä½æ–¹é¢å­˜åœ¨å†·å¯åŠ¨é—®é¢˜ï¼ŒMGPOé€šè¿‡å¤šè½®å¯¹è¯æ¨¡æ¿å’Œç­–ç•¥æŸå¤±è®¡ç®—çš„æ–¹å¼è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>å®éªŒè¯æ˜MGPOåœ¨æ ‡å‡†è§†è§‰é—®ç­”æ•°æ®é›†ä¸Šçš„å®šä½èƒ½åŠ›ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œä¸”åœ¨æœªè§æ•°æ®é›†ä¸Šæœ‰æ˜¾è‘—æ•ˆæœã€‚</li>
<li>MGPOåœ¨Qwen2.5-VL-7Bæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºOpenAIçš„o1å’ŒGPT-4oæ¨¡å‹ã€‚</li>
<li>MGPOæ–¹æ³•å·²å…¬å¼€æºä»£ç ï¼Œä¾¿äºç ”ç©¶è€…å’Œå¼€å‘è€…ä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.05920">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0113d14c8453554b3c39b8403dbd4794.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a4d2b039a1d8a09eab283a7bdb49400c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a6a11c0e1cc937f2cad43c60ad0a8b46.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e71441610faf3bc2b8f19cf5fe11f9dc.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Video-Event-Reasoning-and-Prediction-by-Fusing-World-Knowledge-from-LLMs-with-Vision-Foundation-Models"><a href="#Video-Event-Reasoning-and-Prediction-by-Fusing-World-Knowledge-from-LLMs-with-Vision-Foundation-Models" class="headerlink" title="Video Event Reasoning and Prediction by Fusing World Knowledge from LLMs   with Vision Foundation Models"></a>Video Event Reasoning and Prediction by Fusing World Knowledge from LLMs   with Vision Foundation Models</h2><p><strong>Authors:Lâ€™ea Dubois, Klaus Schmidt, Chengyu Wang, Ji-Hoon Park, Lin Wang, Santiago Munoz</strong></p>
<p>Current video understanding models excel at recognizing â€œwhatâ€ is happening but fall short in high-level cognitive tasks like causal reasoning and future prediction, a limitation rooted in their lack of commonsense world knowledge. To bridge this cognitive gap, we propose a novel framework that synergistically fuses a powerful Vision Foundation Model (VFM) for deep visual perception with a Large Language Model (LLM) serving as a knowledge-driven reasoning core. Our key technical innovation is a sophisticated fusion module, inspired by the Q-Former architecture, which distills complex spatiotemporal and object-centric visual features into a concise, language-aligned representation. This enables the LLM to effectively ground its inferential processes in direct visual evidence. The model is trained via a two-stage strategy, beginning with large-scale alignment pre-training on video-text data, followed by targeted instruction fine-tuning on a curated dataset designed to elicit advanced reasoning and prediction skills. Extensive experiments demonstrate that our model achieves state-of-the-art performance on multiple challenging benchmarks. Notably, it exhibits remarkable zero-shot generalization to unseen reasoning tasks, and our in-depth ablation studies validate the critical contribution of each architectural component. This work pushes the boundary of machine perception from simple recognition towards genuine cognitive understanding, paving the way for more intelligent and capable AI systems in robotics, human-computer interaction, and beyond. </p>
<blockquote>
<p>å½“å‰è§†é¢‘ç†è§£æ¨¡å‹åœ¨è¯†åˆ«â€œå‘ç”Ÿäº†ä»€ä¹ˆâ€æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨é«˜çº§è®¤çŸ¥ä»»åŠ¡ï¼ˆå¦‚å› æœæ¨ç†å’Œæœªæ¥é¢„æµ‹ï¼‰æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œè¿™ä¸€å±€é™æ€§æºäºå®ƒä»¬ç¼ºä¹å¸¸è¯†ä¸–ç•ŒçŸ¥è¯†ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€è®¤çŸ¥é¸¿æ²Ÿï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ååŒèåˆäº†ç”¨äºæ·±åº¦è§†è§‰æ„ŸçŸ¥çš„å¼ºå¤§è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆVFMï¼‰å’Œä½œä¸ºçŸ¥è¯†é©±åŠ¨æ¨ç†æ ¸å¿ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚æˆ‘ä»¬çš„å…³é”®æŠ€æœ¯åˆ›æ–°åœ¨äºä¸€ä¸ªå¤æ‚çš„èåˆæ¨¡å—ï¼Œè¯¥æ¨¡å—å—åˆ°Q-Formeræ¶æ„çš„å¯å‘ï¼Œå°†å¤æ‚çš„æ—¶ç©ºå’Œå¯¹è±¡ä¸­å¿ƒçš„è§†è§‰ç‰¹å¾è’¸é¦æˆç®€æ´ã€ä¸è¯­è¨€å¯¹é½çš„è¡¨ç¤ºå½¢å¼ã€‚è¿™ä½¿LLMèƒ½å¤Ÿæœ‰æ•ˆåœ°å°†å…¶æ¨ç†è¿‡ç¨‹ç›´æ¥å»ºç«‹åœ¨è§†è§‰è¯æ®ä¸Šã€‚è¯¥æ¨¡å‹é‡‡ç”¨ä¸¤é˜¶æ®µç­–ç•¥è¿›è¡Œè®­ç»ƒï¼Œé¦–å…ˆåœ¨å¤§è§„æ¨¡è§†é¢‘æ–‡æœ¬æ•°æ®ä¸Šè¿›è¡Œå¯¹é½é¢„è®­ç»ƒï¼Œç„¶ååœ¨ä¸“é—¨è®¾è®¡çš„æ•°æ®é›†ä¸Šè¿›è¡Œæœ‰é’ˆå¯¹æ€§çš„æŒ‡ä»¤å¾®è°ƒï¼Œä»¥æ¿€å‘é«˜çº§æ¨ç†å’Œé¢„æµ‹æŠ€èƒ½ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨å¤šä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå®ƒå¯¹æœªè§è¿‡çš„æ¨ç†ä»»åŠ¡è¡¨ç°å‡ºäº†æ˜¾è‘—çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œæˆ‘ä»¬çš„æ·±åº¦æ¶ˆèç ”ç©¶éªŒè¯äº†æ¯ä¸ªæ¶æ„ç»„ä»¶çš„å…³é”®è´¡çŒ®ã€‚è¿™é¡¹å·¥ä½œå°†æœºå™¨æ„ŸçŸ¥çš„è¾¹ç•Œä»ç®€å•çš„è¯†åˆ«æ¨å‘äº†çœŸæ­£çš„è®¤çŸ¥ç†è§£ï¼Œä¸ºæœºå™¨äººã€äººæœºäº¤äº’ç­‰é¢†åŸŸçš„æ›´æ™ºèƒ½ã€æ›´å¼ºå¤§çš„AIç³»ç»Ÿé“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.05822v1">PDF</a> 22 pages, 4 figures</p>
<p><strong>Summary</strong><br>å¤šåª’ä½“è§†é¢‘ç†è§£çš„ç°çŠ¶æ˜¯æ¨¡å‹ä¸»è¦è¯†åˆ«â€œå‘ç”Ÿäº†ä»€ä¹ˆâ€ï¼Œä½†åœ¨å› æœæ¨ç†å’Œæœªæ¥é¢„æµ‹ç­‰é«˜çº§è®¤çŸ¥ä»»åŠ¡ä¸Šè¡¨ç°ä¸è¶³ï¼Œç¼ºä¹å¸¸è¯†æ€§ä¸–ç•ŒçŸ¥è¯†ã€‚ä¸ºå¼¥è¡¥è¿™ä¸€è®¤çŸ¥å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªèåˆå¼ºå¤§çš„è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆVFMï¼‰å’ŒçŸ¥è¯†é©±åŠ¨æ¨ç†æ ¸å¿ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ–°æ¡†æ¶ã€‚å—Q-Formeræ¶æ„å¯å‘çš„å¤æ‚èåˆæ¨¡å—å¯å°†å¤æ‚çš„æ—¶ç©ºå’Œå¯¹è±¡ä¸­å¿ƒè§†è§‰ç‰¹å¾è’¸é¦æˆç®€æ´ã€ä¸è¯­è¨€å¯¹é½çš„è¡¨ç¤ºå½¢å¼ã€‚æ¨¡å‹é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œé¦–å…ˆåœ¨è§†é¢‘æ–‡æœ¬æ•°æ®ä¸Šè¿›è¡Œå¤§è§„æ¨¡å¯¹é½é¢„è®­ç»ƒï¼Œç„¶ååœ¨å®šåˆ¶æ•°æ®é›†ä¸Šè¿›è¡Œé’ˆå¯¹æ€§æŒ‡ä»¤å¾®è°ƒï¼Œä»¥æ¿€å‘é«˜çº§æ¨ç†å’Œé¢„æµ‹æŠ€èƒ½ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å¤šä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†æœ€æ–°æ€§èƒ½ã€‚å°¤å…¶æ˜¯ï¼Œå®ƒåœ¨æœªè§è¿‡çš„æ¨ç†ä»»åŠ¡ä¸Šå±•ç°å‡ºå“è¶Šçš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å·¥ä½œæ¨åŠ¨äº†å¤šåª’ä½“è§†é¢‘ç†è§£çš„è¿›æ­¥ï¼Œè¿ˆå‘çœŸæ­£çš„è®¤çŸ¥ç†è§£ï¼Œä¸ºæœºå™¨äººæŠ€æœ¯ã€äººæœºäº¤äº’ç­‰é¢†åŸŸæ›´æ™ºèƒ½ã€æ›´å¼ºå¤§çš„AIç³»ç»Ÿé“ºå¹³äº†é“è·¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰è§†é¢‘ç†è§£æ¨¡å‹çš„å±€é™æ€§åœ¨äºç¼ºä¹é«˜çº§è®¤çŸ¥ä»»åŠ¡å’Œå¸¸è¯†æ€§ä¸–ç•ŒçŸ¥è¯†ã€‚</li>
<li>æå‡ºèåˆè§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆVFMï¼‰ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ–°æ¡†æ¶æ¥å¼¥è¡¥è®¤çŸ¥å·®è·ã€‚</li>
<li>å¼•å…¥å¤æ‚èåˆæ¨¡å—ï¼Œå°†è§†è§‰ç‰¹å¾è½¬åŒ–ä¸ºè¯­è¨€å¯¹é½çš„è¡¨ç¤ºå½¢å¼ã€‚</li>
<li>æ¨¡å‹é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼ŒåŒ…æ‹¬å¤§è§„æ¨¡å¯¹é½é¢„è®­ç»ƒå’ŒæŒ‡ä»¤å¾®è°ƒã€‚</li>
<li>æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶æ˜¯é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›çªå‡ºã€‚</li>
<li>èåˆæ¡†æ¶æœ‰åŠ©äºå®ç°å¤šåª’ä½“è§†é¢‘ç†è§£çš„çœŸæ­£è®¤çŸ¥æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.05822">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9fb3cd773d319737b6b9f1cd4f27ffec.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-85dacf74f9ea724cabb8b48250a3af5e.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="SPADE-Spatial-Aware-Denoising-Network-for-Open-vocabulary-Panoptic-Scene-Graph-Generation-with-Long-and-Local-range-Context-Reasoning"><a href="#SPADE-Spatial-Aware-Denoising-Network-for-Open-vocabulary-Panoptic-Scene-Graph-Generation-with-Long-and-Local-range-Context-Reasoning" class="headerlink" title="SPADE: Spatial-Aware Denoising Network for Open-vocabulary Panoptic   Scene Graph Generation with Long- and Local-range Context Reasoning"></a>SPADE: Spatial-Aware Denoising Network for Open-vocabulary Panoptic   Scene Graph Generation with Long- and Local-range Context Reasoning</h2><p><strong>Authors:Xin Hu, Ke Qin, Guiduo Duan, Ming Li, Yuan-Fang Li, Tao He</strong></p>
<p>Panoptic Scene Graph Generation (PSG) integrates instance segmentation with relation understanding to capture pixel-level structural relationships in complex scenes. Although recent approaches leveraging pre-trained vision-language models (VLMs) have significantly improved performance in the open-vocabulary setting, they commonly ignore the inherent limitations of VLMs in spatial relation reasoning, such as difficulty in distinguishing object relative positions, which results in suboptimal relation prediction. Motivated by the denoising diffusion modelâ€™s inversion process in preserving the spatial structure of input images, we propose SPADE (SPatial-Aware Denoising-nEtwork) framework â€“ a novel approach for open-vocabulary PSG. SPADE consists of two key steps: (1) inversion-guided calibration for the UNet adaptation, and (2) spatial-aware context reasoning. In the first step, we calibrate a general pre-trained teacher diffusion model into a PSG-specific denoising network with cross-attention maps derived during inversion through a lightweight LoRA-based fine-tuning strategy. In the second step, we develop a spatial-aware relation graph transformer that captures both local and long-range contextual information, facilitating the generation of high-quality relation queries. Extensive experiments on benchmark PSG and Visual Genome datasets demonstrate that SPADE outperforms state-of-the-art methods in both closed- and open-set scenarios, particularly for spatial relationship prediction. </p>
<blockquote>
<p>å…¨æ™¯åœºæ™¯å›¾ç”Ÿæˆï¼ˆPSGï¼‰ç»“åˆäº†å®ä¾‹åˆ†å‰²å’Œå…³ç³»ç†è§£ï¼Œä»¥æ•æ‰å¤æ‚åœºæ™¯ä¸­çš„åƒç´ çº§ç»“æ„å…³ç³»ã€‚å°½ç®¡æœ€è¿‘åˆ©ç”¨é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„æ–¹æ³•åœ¨å¼€æ”¾è¯æ±‡è®¾ç½®ä¸­çš„æ€§èƒ½æ˜¾è‘—æé«˜ï¼Œä½†å®ƒä»¬é€šå¸¸å¿½ç•¥äº†VLMåœ¨ç©ºé—´å…³ç³»æ¨ç†æ–¹é¢çš„å›ºæœ‰å±€é™æ€§ï¼Œä¾‹å¦‚éš¾ä»¥åŒºåˆ†å¯¹è±¡çš„ç›¸å¯¹ä½ç½®ï¼Œå¯¼è‡´å…³ç³»é¢„æµ‹ä¸ç†æƒ³ã€‚å—å»å™ªæ‰©æ•£æ¨¡å‹åœ¨ä¿ç•™è¾“å…¥å›¾åƒç©ºé—´ç»“æ„æ—¶çš„åè½¬è¿‡ç¨‹çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†SPADEï¼ˆç©ºé—´æ„ŸçŸ¥å»å™ªç½‘ç»œï¼‰æ¡†æ¶â€”â€”ä¸€ç§ç”¨äºå¼€æ”¾è¯æ±‡PSGçš„æ–°æ–¹æ³•ã€‚SPADEç”±ä¸¤ä¸ªå…³é”®æ­¥éª¤ç»„æˆï¼šï¼ˆ1ï¼‰åè½¬å¼•å¯¼æ ¡å‡†ç”¨äºUNeté€‚åº”ï¼Œï¼ˆ2ï¼‰ç©ºé—´æ„ŸçŸ¥ä¸Šä¸‹æ–‡æ¨ç†ã€‚åœ¨ç¬¬ä¸€æ­¥ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡è½»é‡åŒ–LoRAå¾®è°ƒç­–ç•¥å°†é€šç”¨çš„é¢„è®­ç»ƒæ•™å¸ˆæ‰©æ•£æ¨¡å‹æ ¡å‡†ä¸ºPSGç‰¹å®šçš„å»å™ªç½‘ç»œï¼Œåœ¨æ­¤è¿‡ç¨‹ä¸­ä½¿ç”¨åè½¬æ—¶äº§ç”Ÿçš„äº¤å‰æ³¨æ„åŠ›å›¾ã€‚åœ¨ç¬¬äºŒæ­¥ä¸­ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§ç©ºé—´æ„ŸçŸ¥å…³ç³»å›¾è½¬æ¢å™¨ï¼Œèƒ½å¤Ÿæ•æ‰å±€éƒ¨å’Œé•¿è·ç¦»ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œæœ‰åŠ©äºç”Ÿæˆé«˜è´¨é‡çš„å…³ç³»æŸ¥è¯¢ã€‚åœ¨åŸºå‡†PSGå’ŒVisual Genomeæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSPADEåœ¨å°é—­å’Œå¼€æ”¾åœºæ™¯ä¸­å‡ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨ç©ºé—´å…³ç³»é¢„æµ‹æ–¹é¢ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.05798v1">PDF</a> Accepted to ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Panoptic Scene Graph Generationï¼ˆPSGï¼‰æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯ç»“åˆäº†å®ä¾‹åˆ†å‰²å’Œå…³ç³»ç†è§£ï¼Œä»¥æ•æ‰å¤æ‚åœºæ™¯ä¸­çš„åƒç´ çº§ç»“æ„å…³ç³»ã€‚è™½ç„¶åˆ©ç”¨é¢„è®­ç»ƒçš„è¯­è¨€è§†è§‰æ¨¡å‹ï¼ˆVLMï¼‰çš„æ–¹æ³•åœ¨å¼€æ”¾è¯æ±‡ç¯å¢ƒä¸­æ˜¾è‘—æé«˜äº†æ€§èƒ½ï¼Œä½†å®ƒä»¬é€šå¸¸å¿½ç•¥äº†VLMåœ¨ç©ºé—´å…³ç³»æ¨ç†æ–¹é¢çš„å›ºæœ‰å±€é™æ€§ï¼Œå¦‚éš¾ä»¥åŒºåˆ†å¯¹è±¡çš„ç›¸å¯¹ä½ç½®ï¼Œå¯¼è‡´å…³ç³»é¢„æµ‹ä¸ä½³ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†SPADEï¼ˆç©ºé—´æ„ŸçŸ¥å»å™ªç½‘ç»œï¼‰æ¡†æ¶ï¼ŒåŒ…æ‹¬ä¸¤ä¸ªå…³é”®æ­¥éª¤ï¼šåè½¬å¼•å¯¼æ ¡å‡†çš„UNeté€‚åº”æ€§å’Œç©ºé—´æ„ŸçŸ¥ä¸Šä¸‹æ–‡æ¨ç†ã€‚SPADEé€šè¿‡è½»é‡åŒ–LoRAæŠ€æœ¯å¾®è°ƒç­–ç•¥ï¼Œå°†ä¸€èˆ¬é¢„è®­ç»ƒçš„è€å¸ˆæ‰©æ•£æ¨¡å‹æ ¡å‡†ä¸ºPSGç‰¹å®šçš„å»å™ªç½‘ç»œï¼Œå¹¶å¼€å‘äº†ä¸€ä¸ªç©ºé—´æ„ŸçŸ¥å…³ç³»å›¾è½¬æ¢å™¨ï¼Œä»¥æ•æ‰æœ¬åœ°å’Œè¿œç¨‹ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä»è€Œç”Ÿæˆé«˜è´¨é‡çš„å…³ç³»æŸ¥è¯¢ã€‚åœ¨PSGå’ŒVisual Genomeæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSPADEåœ¨å°é—­å’Œå¼€æ”¾åœºæ™¯ä¸­å‡ä¼˜äºæœ€æ–°æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨ç©ºé—´å…³ç³»é¢„æµ‹æ–¹é¢ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PSGæŠ€æœ¯ç»“åˆäº†å®ä¾‹åˆ†å‰²å’Œå…³ç³»ç†è§£ï¼Œç”¨äºæ•æ‰å¤æ‚åœºæ™¯çš„åƒç´ çº§ç»“æ„å…³ç³»ã€‚</li>
<li>å°½ç®¡é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åœ¨å¼€æ”¾è¯æ±‡ç¯å¢ƒä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†å®ƒä»¬åœ¨ç©ºé—´å…³ç³»æ¨ç†æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>SPADEæ¡†æ¶ç”±ä¸¤ä¸ªå…³é”®æ­¥éª¤ç»„æˆï¼šåè½¬å¼•å¯¼æ ¡å‡†çš„UNeté€‚åº”æ€§å’Œç©ºé—´æ„ŸçŸ¥ä¸Šä¸‹æ–‡æ¨ç†ã€‚</li>
<li>SPADEé€šè¿‡è½»é‡åŒ–LoRAæŠ€æœ¯å°†é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹æ ¡å‡†ä¸ºPSGç‰¹å®šçš„å»å™ªç½‘ç»œã€‚</li>
<li>SPADEå¼€å‘äº†ä¸€ä¸ªç©ºé—´æ„ŸçŸ¥å…³ç³»å›¾è½¬æ¢å™¨ï¼Œèƒ½æ•æ‰æœ¬åœ°å’Œè¿œç¨‹ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œæé«˜å…³ç³»é¢„æµ‹çš„è´¨é‡ã€‚</li>
<li>åœ¨PSGå’ŒVisual Genomeæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSPADEåœ¨å°é—­å’Œå¼€æ”¾åœºæ™¯ä¸­å‡è¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.05798">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1e2054ab84f3e7273dbc061a405ef523.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-999fa7e809d9f96a652b8876b37d0985.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d1cbd528a1cd06d40f3dbde3f94fd0fd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4f93713181b9b1ded689959ea9965ac4.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Agentic-R1-Distilled-Dual-Strategy-Reasoning"><a href="#Agentic-R1-Distilled-Dual-Strategy-Reasoning" class="headerlink" title="Agentic-R1: Distilled Dual-Strategy Reasoning"></a>Agentic-R1: Distilled Dual-Strategy Reasoning</h2><p><strong>Authors:Weihua Du, Pranjal Aggarwal, Sean Welleck, Yiming Yang</strong></p>
<p>Current long chain-of-thought (long-CoT) models excel at mathematical reasoning but rely on slow and error-prone natural language traces. Tool-augmented agents address arithmetic via code execution, but often falter on complex logical tasks. We introduce a fine-tuning framework, DualDistill, that distills complementary reasoning strategies from multiple teachers into a unified student model. Using this approach, we train Agentic-R1, which dynamically selects the optimal strategy for each query, invoking tools for arithmetic and algorithmic problems, and using text-based reasoning for abstract ones. Our method improves accuracy across a range of tasks, including both computation-intensive and standard benchmarks, demonstrating the effectiveness of multi-strategy distillation in achieving robust and efficient reasoning. Our project is available at <a target="_blank" rel="noopener" href="https://github.com/StigLidu/DualDistill">https://github.com/StigLidu/DualDistill</a> </p>
<blockquote>
<p>å½“å‰çš„é•¿æ€ç»´é“¾ï¼ˆlong-CoTï¼‰æ¨¡å‹æ“…é•¿æ•°å­¦æ¨ç†ï¼Œä½†ä¾èµ–äºç¼“æ…¢ä¸”æ˜“å‡ºé”™çš„è‡ªç„¶è¯­è¨€è½¨è¿¹ã€‚å·¥å…·å¢å¼ºå‹ä»£ç†é€šè¿‡ä»£ç æ‰§è¡Œæ¥è§£å†³ç®—æœ¯é—®é¢˜ï¼Œä½†åœ¨å¤æ‚çš„é€»è¾‘ä»»åŠ¡ä¸­å¸¸å¸¸é‡åˆ°å›°éš¾ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å¾®è°ƒæ¡†æ¶ï¼Œåä¸ºDualDistillï¼Œè¯¥æ¡†æ¶ä»å¤šä¸ªæ•™å¸ˆæ¨¡å‹ä¸­æç‚¼å‡ºäº’è¡¥çš„æ¨ç†ç­–ç•¥ï¼Œå¹¶å°†å…¶èåˆåˆ°ä¸€ä¸ªç»Ÿä¸€çš„å­¦ç”Ÿæ¨¡å‹ä¸­ã€‚é€šè¿‡è¿™ç§æ–¹æ³•ï¼Œæˆ‘ä»¬è®­ç»ƒäº†Agentic-R1ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿé’ˆå¯¹æ¯ä¸ªæŸ¥è¯¢åŠ¨æ€é€‰æ‹©æœ€ä½³ç­–ç•¥ï¼Œå¯¹ç®—æœ¯å’Œç®—æ³•é—®é¢˜è°ƒç”¨å·¥å…·ï¼Œå¯¹æŠ½è±¡é—®é¢˜åˆ™é‡‡ç”¨åŸºäºæ–‡æœ¬çš„æ¨ç†ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šç§ä»»åŠ¡ä¸Šæé«˜äº†å‡†ç¡®æ€§ï¼ŒåŒ…æ‹¬è®¡ç®—å¯†é›†å‹ä»»åŠ¡å’Œæ ‡å‡†åŸºå‡†æµ‹è¯•ï¼Œè¯æ˜äº†å¤šç­–ç•¥è’¸é¦åœ¨å®ç°ç¨³å¥é«˜æ•ˆæ¨ç†æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„é¡¹ç›®å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/StigLidu/DualDistill%E4%B8%8A%E6%9F%A5%E7%9C%8B%E3%80%82">https://github.com/StigLidu/DualDistillä¸ŠæŸ¥çœ‹ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.05707v1">PDF</a> Preprint. 15 pages. Project available at   <a target="_blank" rel="noopener" href="https://github.com/StigLidu/DualDistill">https://github.com/StigLidu/DualDistill</a></p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†å½“å‰é•¿é“¾æ€ç»´æ¨¡å‹åœ¨å¤„ç†æ•°å­¦æ¨ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†é¢å¯¹è‡ªç„¶è¯­è¨€æ¨ç†å­˜åœ¨ååº”ç¼“æ…¢ã€å®¹æ˜“å‡ºé”™çš„é—®é¢˜ã€‚æ­¤å¤–ï¼Œé€šè¿‡å·¥å…·å¢å¼ºçš„æ™ºèƒ½ä»£ç†é€šå¸¸ä¸“æ³¨äºè§£å†³ç®—æœ¯é—®é¢˜ä½†å¯èƒ½é¢ä¸´å¤æ‚çš„é€»è¾‘æŒ‘æˆ˜ã€‚é’ˆå¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºä¸€ä¸ªç§°ä¸ºDualDistillçš„ç²¾è°ƒæ¡†æ¶ï¼Œå®ƒèƒ½å¤Ÿä»ä¸åŒçš„æ•™å¸ˆæ¨¡å‹ä¸­æç‚¼äº’è¡¥æ¨ç†ç­–ç•¥å¹¶å°†å…¶èå…¥å­¦ç”Ÿæ¨¡å‹ä¸­ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬è®­ç»ƒå‡ºAgentic-R1æ¨¡å‹ï¼Œå®ƒèƒ½é’ˆå¯¹æ¯ä¸ªæŸ¥è¯¢åŠ¨æ€é€‰æ‹©æœ€ä½³ç­–ç•¥ï¼Œä¸ºç®—æ³•é—®é¢˜å’Œç®—æœ¯é—®é¢˜è°ƒç”¨å·¥å…·å¹¶ä½¿ç”¨æ–‡æœ¬è¿›è¡ŒæŠ½è±¡æ¨ç†ã€‚æ­¤æ–¹æ³•æé«˜äº†å„ç§ä»»åŠ¡çš„å‡†ç¡®æ€§ï¼ŒåŒ…æ‹¬è®¡ç®—å¯†é›†å‹ä»»åŠ¡å’Œæ ‡å‡†åŸºå‡†æµ‹è¯•ï¼Œå±•ç¤ºäº†å¤šå…ƒç­–ç•¥èåˆåœ¨å®ç°ç¨³å¥é«˜æ•ˆæ¨ç†æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰é•¿é“¾æ€ç»´æ¨¡å‹æ“…é•¿æ•°å­¦æ¨ç†ï¼Œä½†åœ¨è‡ªç„¶è¯­è¨€æ¨ç†æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>å·¥å…·å¢å¼ºä»£ç†å¯ä»¥è§£å†³ç®—æœ¯é—®é¢˜ï¼Œä½†åœ¨å¤æ‚é€»è¾‘ä»»åŠ¡ä¸Šå¯èƒ½è¡¨ç°ä¸è¶³ã€‚</li>
<li>å¼•å…¥DualDistillç²¾è°ƒæ¡†æ¶ï¼Œèƒ½å¤Ÿä»å¤šä¸ªæ•™å¸ˆæ¨¡å‹ä¸­æç‚¼äº’è¡¥æ¨ç†ç­–ç•¥ã€‚</li>
<li>Agentic-R1æ¨¡å‹èƒ½å¤ŸåŠ¨æ€é€‰æ‹©æœ€ä½³ç­–ç•¥æ¥å¤„ç†ä¸åŒç±»å‹çš„æ¨ç†ä»»åŠ¡ã€‚</li>
<li>Agentic-R1æ¨¡å‹ç»“åˆäº†å·¥å…·ä½¿ç”¨å’Œæ–‡æœ¬æ¨ç†ï¼Œæé«˜äº†å¤„ç†å„ç§ä»»åŠ¡çš„å‡†ç¡®æ€§ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å¤„ç†è®¡ç®—å¯†é›†å‹ä»»åŠ¡å’Œæ ‡å‡†åŸºå‡†æµ‹è¯•æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.05707">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5102825a177dfda3c682f1ab595c941b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7f3744a75f7317e95be24cfdd1123f13.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-970fdd9f7b2f22e3e5ad93b7470e0519.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0d59099bc71a3cc9843e341e83381583.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="AutoTriton-Automatic-Triton-Programming-with-Reinforcement-Learning-in-LLMs"><a href="#AutoTriton-Automatic-Triton-Programming-with-Reinforcement-Learning-in-LLMs" class="headerlink" title="AutoTriton: Automatic Triton Programming with Reinforcement Learning in   LLMs"></a>AutoTriton: Automatic Triton Programming with Reinforcement Learning in   LLMs</h2><p><strong>Authors:Shangzhan Li, Zefan Wang, Ye He, Yuxuan Li, Qi Shi, Jianling Li, Yonggang Hu, Wanxiang Che, Xu Han, Zhiyuan Liu, Maosong Sun</strong></p>
<p>Kernel development in deep learning requires optimizing computational units across hardware while balancing memory management, parallelism, and hardware-specific optimizations through extensive empirical tuning. Although domain-specific languages like Triton simplify GPU programming by abstracting low-level details, developers must still manually tune critical parameters such as tile sizes and memory access patterns through iterative experimentation, creating substantial barriers to optimal performance and wider adoption. In this work, we introduce AutoTriton, the first model dedicated to Triton programming powered by reinforcement learning (RL). AutoTriton performs supervised fine-tuning (SFT) to be equipped with essential Triton programming expertise using a high-quality data gathering pipeline, and conducts RL with Group Relative Policy Optimization (GRPO) algorithm, combining a rule-based reward and an execution-based reward to further improve Triton programming ability, sequentially. Experiments across five evaluation channels of TritonBench and KernelBench illustrate that our 8B model AutoTriton achieves performance comparable to mainstream large models, including Claude-4-Sonnet and DeepSeek-R1-0528. Further experimental analysis demonstrates the crucial role of each module within AutoTriton, including the SFT stage, the RL stage, and the reward design strategy. These findings underscore the promise of RL for automatically generating high-performance kernels, and since high-performance kernels are core components of AI systems, this breakthrough establishes an important foundation for building more efficient AI systems. The model and code will be available at <a target="_blank" rel="noopener" href="https://github.com/AI9Stars/AutoTriton">https://github.com/AI9Stars/AutoTriton</a>. </p>
<blockquote>
<p>å†…æ ¸æ·±åº¦å­¦ä¹ çš„å‘å±•éœ€è¦åœ¨ç¡¬ä»¶ä¸Šä¼˜åŒ–è®¡ç®—å•å…ƒï¼ŒåŒæ—¶å¹³è¡¡å†…å­˜ç®¡ç†ã€å¹¶è¡Œæ€§å’Œç¡¬ä»¶ç‰¹å®šä¼˜åŒ–ï¼Œè¿™éœ€è¦é€šè¿‡å¤§é‡çš„ç»éªŒè°ƒæ•´æ¥å®ç°ã€‚è™½ç„¶åƒTritonè¿™æ ·çš„é¢†åŸŸç‰¹å®šè¯­è¨€é€šè¿‡æŠ½è±¡åº•å±‚ç»†èŠ‚æ¥ç®€åŒ–GPUç¼–ç¨‹ï¼Œä½†å¼€å‘è€…ä»ç„¶éœ€è¦æ‰‹åŠ¨è°ƒæ•´å…³é”®å‚æ•°ï¼Œå¦‚ç“¦ç‰‡å¤§å°å’Œå†…å­˜è®¿é—®æ¨¡å¼ï¼Œé€šè¿‡è¿­ä»£å®éªŒï¼Œè¿™ç»™è¾¾åˆ°æœ€ä½³æ€§èƒ½å’Œæ›´å¹¿æ³›çš„åº”ç”¨é€ æˆäº†é‡å¤§éšœç¢ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†AutoTritonï¼Œè¿™æ˜¯ç”±å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰é©±åŠ¨çš„é¦–ä¸ªä¸“é—¨ç”¨äºTritonç¼–ç¨‹çš„æ¨¡å‹ã€‚AutoTritoné€šè¿‡é«˜è´¨é‡çš„æ•°æ®æ”¶é›†ç®¡é“è¿›è¡Œæœ‰ç›‘ç£çš„å¾®è°ƒï¼ˆSFTï¼‰ï¼Œä»è€Œå…·å¤‡åŸºæœ¬çš„Tritonç¼–ç¨‹ä¸“ä¸šçŸ¥è¯†ï¼Œå¹¶ç»“åˆåŸºäºè§„åˆ™çš„å¥–åŠ±å’ŒåŸºäºæ‰§è¡Œçš„å¥–åŠ±ï¼Œé‡‡ç”¨ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ç®—æ³•è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼Œä»¥è¿›ä¸€æ­¥æé«˜Tritonç¼–ç¨‹èƒ½åŠ›ã€‚åœ¨TritonBenchå’ŒKernelBenchçš„äº”ä¸ªè¯„ä¼°é€šé“ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„8Bæ¨¡å‹AutoTritonçš„æ€§èƒ½ä¸ä¸»æµå¤§å‹æ¨¡å‹ï¼ˆåŒ…æ‹¬Claude-4-Sonnetå’ŒDeepSeek-R1-0528ï¼‰ç›¸å½“ã€‚è¿›ä¸€æ­¥çš„å®éªŒåˆ†æè¯æ˜äº†AutoTritonä¸­æ¯ä¸ªæ¨¡å—çš„é‡è¦ä½œç”¨ï¼ŒåŒ…æ‹¬SFTé˜¶æ®µã€RLé˜¶æ®µå’Œå¥–åŠ±è®¾è®¡ç­–ç•¥ã€‚è¿™äº›å‘ç°çªæ˜¾äº†å¼ºåŒ–å­¦ä¹ åœ¨è‡ªåŠ¨ç”Ÿæˆé«˜æ€§èƒ½å†…æ ¸æ–¹é¢çš„æ½œåŠ›ï¼Œç”±äºé«˜æ€§èƒ½å†…æ ¸æ˜¯AIç³»ç»Ÿçš„æ ¸å¿ƒç»„ä»¶ï¼Œè¿™ä¸€çªç ´ä¸ºæ„å»ºæ›´é«˜æ•ˆçš„AIç³»ç»Ÿå¥ å®šäº†é‡è¦åŸºç¡€ã€‚æ¨¡å‹å’Œä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/AI9Stars/AutoTriton%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/AI9Stars/AutoTritonä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.05687v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šè¯¥ç ”ç©¶å¼•å…¥äº†åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„AutoTritonæ¨¡å‹ï¼Œè¯¥æ¨¡å‹ç”¨äºç®€åŒ–å†…æ ¸å¼€å‘çš„GPUç¼–ç¨‹è¿‡ç¨‹ã€‚AutoTritoné€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰è·å¾—Tritonç¼–ç¨‹ä¸“ä¸šçŸ¥è¯†ï¼Œå¹¶é‡‡ç”¨åŸºäºè§„åˆ™çš„å¥–åŠ±å’Œæ‰§è¡Œå¥–åŠ±çš„Group Relative Policy Optimizationï¼ˆGRPOï¼‰ç®—æ³•ä¼˜åŒ–å†…æ ¸å¼€å‘è¿‡ç¨‹ã€‚å®éªŒç»“æœéªŒè¯äº†è¯¥æ¨¡å‹ç›¸è¾ƒäºä¸»æµæ¨¡å‹çš„ç«äº‰åŠ›ã€‚æ¨¡å‹çš„æ¨å‡ºå°†æå‡äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„æ€§èƒ½ä¸æ•ˆç‡ã€‚å…¶æ¨¡å‹å’Œæ•°æ®å°†ä¼šå…¬å¼€å‘å¸ƒäºgithubå¹³å°ä¾›ç”¨æˆ·è‡ªç”±è·å–ã€‚è¯¥å·¥ä½œç®€åŒ–GPUç¼–ç¨‹å¤æ‚æ€§çš„å…³é”®ç»„ä»¶è¿›ä¸€æ­¥æ”¹å–„äº†é«˜æ€§èƒ½AIç³»ç»Ÿçš„å»ºè®¾è¿›ç¨‹ã€‚æ¨¡å‹çš„å“è¶Šæ€§èƒ½å±•ç°äº†å…¶æ½œåŠ›ä¸å‰æ™¯ã€‚ </p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ul>
<li>AutoTritonæ¨¡å‹é‡‡ç”¨å¼ºåŒ–å­¦ä¹ æŠ€æœ¯ä¼˜åŒ–å†…æ ¸å¼€å‘è¿‡ç¨‹ï¼Œå®ç°GPUç¼–ç¨‹è‡ªåŠ¨åŒ–ã€‚</li>
<li>è¯¥æ¨¡å‹é€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰è·å¾—Tritonç¼–ç¨‹ä¸“ä¸šçŸ¥è¯†ï¼Œå¹¶é‡‡ç”¨Group Relative Policy Optimizationï¼ˆGRPOï¼‰ç®—æ³•æå‡æ€§èƒ½ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºAutoTritonæ¨¡å‹æ€§èƒ½ä¸ä¸»æµå¤§å‹æ¨¡å‹ç›¸å½“ï¼Œå¹¶åœ¨å¤šä¸ªè¯„ä¼°æ¸ é“è¡¨ç°å‡ºè‰²ã€‚</li>
<li>æ¨¡å‹åœ¨å¥–åŠ±è®¾è®¡ç­–ç•¥ä¸Šé‡‡ç”¨äº†ç»“åˆåŸºäºè§„åˆ™çš„å¥–åŠ±å’Œæ‰§è¡Œå¥–åŠ±çš„æ–¹æ³•ã€‚å®éªŒè¯æ˜è¿™ç§æ–¹æ³•æé«˜äº†æ¨¡å‹çš„æ€§èƒ½ä¼˜åŒ–èƒ½åŠ›ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.05687">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-38642ca47c7b487a0c532870de6db81e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-170e8043c4f640f2914cd6f26a8818df.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-65fc7304920d75f6d95247e7e7093140.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="MLlm-DR-Towards-Explainable-Depression-Recognition-with-MultiModal-Large-Language-Models"><a href="#MLlm-DR-Towards-Explainable-Depression-Recognition-with-MultiModal-Large-Language-Models" class="headerlink" title="MLlm-DR: Towards Explainable Depression Recognition with MultiModal   Large Language Models"></a>MLlm-DR: Towards Explainable Depression Recognition with MultiModal   Large Language Models</h2><p><strong>Authors:Wei Zhang, Juan Chen, En Zhu, Wenhong Cheng, YunPeng Li, Yanbo J. Wang</strong></p>
<p>Automated depression diagnosis aims to analyze multimodal information from interview videos to predict participantsâ€™ depression scores. Previous studies often lack clear explanations of how these scores were determined, limiting their adoption in clinical practice. While the advent of LLMs provides a possible pathway for explainable depression diagnosis, current LLMs capable of processing multimodal data lack training on interview data, resulting in poor diagnostic performance when used directly. In this paper, we propose a novel multimodal large language model (MLlm-DR) that can understand multimodal information inputs and supports explainable depression diagnosis. MLlm-DR integrates a smaller LLMs and a lightweight query module (LQ-former). Specifically, the smaller LLMs is designed to generate depression scores and corresponding evaluation rationales. To enhance its logical reasoning for domain-specific tasks while maintaining practicality, we constructed a robust training dataset to fine-tune it. Meanwhile, the LQ-former captures depression-related features from speech and visual data, aiding the modelâ€™s ability to process multimodal information, to achieve comprehensive depression diagnosis. Our approach achieves state-of-the-art results on two interview-based benchmark datasets, CMDC and E-DAIC-WOZ, demonstrating its effectiveness and superiority. </p>
<blockquote>
<p>è‡ªåŠ¨åŒ–æŠ‘éƒç—‡è¯Šæ–­æ—¨åœ¨åˆ†æè®¿è°ˆè§†é¢‘ä¸­çš„å¤šæ¨¡æ€ä¿¡æ¯ï¼Œä»¥é¢„æµ‹å‚ä¸è€…çš„æŠ‘éƒè¯„åˆ†ã€‚ä»¥å¾€çš„ç ”ç©¶å¾€å¾€ç¼ºä¹è¿™äº›è¯„åˆ†å¦‚ä½•ç¡®å®šçš„æ˜ç¡®è§£é‡Šï¼Œé™åˆ¶äº†å®ƒä»¬åœ¨ä¸´åºŠå®è·µä¸­çš„åº”ç”¨ã€‚è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å‡ºç°ä¸ºå¯è§£é‡Šçš„æŠ‘éƒç—‡è¯Šæ–­æä¾›äº†å¯èƒ½çš„é€”å¾„ï¼Œä½†ç›®å‰èƒ½å¤Ÿå¤„ç†å¤šæ¨¡æ€æ•°æ®çš„å¤§å‹è¯­è¨€æ¨¡å‹ç¼ºä¹è®¿è°ˆæ•°æ®çš„è®­ç»ƒï¼Œå¯¼è‡´åœ¨å®é™…åº”ç”¨ä¸­çš„è¯Šæ–­æ€§èƒ½è¾ƒå·®ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLlm-DRï¼‰ï¼Œèƒ½å¤Ÿç†è§£å¤šæ¨¡æ€ä¿¡æ¯è¾“å…¥å¹¶æ”¯æŒå¯è§£é‡Šçš„æŠ‘éƒç—‡è¯Šæ–­ã€‚MLlm-DRé›†æˆäº†è¾ƒå°çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œä¸€ä¸ªè½»é‡çº§çš„æŸ¥è¯¢æ¨¡å—ï¼ˆLQ-formerï¼‰ã€‚å…·ä½“æ¥è¯´ï¼Œè¾ƒå°çš„LLMsè¢«è®¾è®¡ç”¨äºç”ŸæˆæŠ‘éƒè¯„åˆ†å’Œç›¸åº”çš„è¯„ä¼°ä¾æ®ã€‚ä¸ºäº†æé«˜å…¶åœ¨ç‰¹å®šä»»åŠ¡ä¸­çš„é€»è¾‘æ¨ç†èƒ½åŠ›å¹¶ä¿æŒå®ç”¨æ€§ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªç¨³å¥çš„è®­ç»ƒæ•°æ®é›†å¯¹å…¶è¿›è¡Œå¾®è°ƒã€‚åŒæ—¶ï¼ŒLQ-formeré€šè¿‡æ•æ‰è¯­éŸ³å’Œè§†è§‰æ•°æ®ä¸­çš„æŠ‘éƒç—‡ç›¸å…³ç‰¹å¾ï¼Œè¾…åŠ©æ¨¡å‹å¤„ç†å¤šæ¨¡æ€ä¿¡æ¯çš„èƒ½åŠ›ï¼Œå®ç°å…¨é¢çš„æŠ‘éƒç—‡è¯Šæ–­ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨åŸºäºè®¿è°ˆçš„ä¸¤ä¸ªåŸºå‡†æ•°æ®é›†CMDCå’ŒE-DAIC-WOZä¸Šå–å¾—äº†æœ€æ–°æˆæœï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.05591v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºè§†é¢‘è®¿è°ˆçš„å¤šæ¨¡æ€ä¿¡æ¯ï¼Œè‡ªåŠ¨åŒ–æŠ‘éƒç—‡è¯Šæ–­æ—¨åœ¨é¢„æµ‹å‚ä¸è€…çš„æŠ‘éƒç¨‹åº¦åˆ†æ•°ã€‚ä»¥å¾€ç ”ç©¶ç¼ºä¹ç¡®å®šè¿™äº›åˆ†æ•°çš„æ˜ç¡®è§£é‡Šï¼Œé™åˆ¶äº†å…¶åœ¨ä¸´åºŠå®è·µä¸­çš„åº”ç”¨ã€‚æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLlm-DRï¼‰ï¼Œèƒ½äº†è§£å¤šæ¨¡æ€ä¿¡æ¯è¾“å…¥å¹¶æ”¯æŒå¯è§£é‡Šçš„æŠ‘éƒç—‡è¯Šæ–­ã€‚è¯¥æ¨¡å‹èåˆäº†è¾ƒå°çš„è¯­è¨€æ¨¡å‹å’Œè½»é‡çº§æŸ¥è¯¢æ¨¡å—ï¼ˆLQ-formerï¼‰ã€‚è¯­è¨€æ¨¡å‹ç”¨äºç”ŸæˆæŠ‘éƒç¨‹åº¦åˆ†æ•°å’Œç›¸åº”çš„è¯„ä¼°ä¾æ®ï¼Œè€ŒLQ-formeråˆ™èƒ½æ•æ‰è¯­éŸ³å’Œè§†è§‰æ•°æ®çš„æŠ‘éƒç›¸å…³ç‰¹å¾ï¼ŒååŠ©æ¨¡å‹å¤„ç†å¤šæ¨¡æ€ä¿¡æ¯ï¼Œå®ç°å…¨é¢çš„æŠ‘éƒç—‡è¯Šæ–­ã€‚åœ¨CMDCå’ŒE-DAIC-WOZä¸¤ä¸ªåŸºäºè®¿è°ˆçš„åŸºå‡†æ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•å–å¾—äº†æœ€ä½³ç»“æœï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªåŠ¨åŒ–æŠ‘éƒç—‡è¯Šæ–­é€šè¿‡åˆ†æè§†é¢‘è®¿è°ˆçš„å¤šæ¨¡æ€ä¿¡æ¯æ¥é¢„æµ‹å‚ä¸è€…çš„æŠ‘éƒç¨‹åº¦åˆ†æ•°ã€‚</li>
<li>ä»¥å¾€ç ”ç©¶ç¼ºä¹æ˜ç¡®çš„åˆ†æ•°åˆ¤å®šä¾æ®ï¼Œé™åˆ¶äº†å…¶åœ¨ä¸´åºŠå®è·µä¸­çš„åº”ç”¨ã€‚</li>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLlm-DRï¼‰èƒ½å¤„ç†å¤šæ¨¡æ€ä¿¡æ¯å¹¶æ”¯æŒå¯è§£é‡Šçš„æŠ‘éƒç—‡è¯Šæ–­ã€‚</li>
<li>MLlm-DRèåˆäº†è¾ƒå°çš„è¯­è¨€æ¨¡å‹å’Œè½»é‡çº§æŸ¥è¯¢æ¨¡å—ï¼ˆLQ-formerï¼‰ã€‚</li>
<li>è¯­è¨€æ¨¡å‹è´Ÿè´£ç”ŸæˆæŠ‘éƒç¨‹åº¦åˆ†æ•°å’Œç›¸åº”çš„è¯„ä¼°ä¾æ®ã€‚</li>
<li>LQ-formerèƒ½ä»è¯­éŸ³å’Œè§†è§‰æ•°æ®ä¸­æ•æ‰æŠ‘éƒç›¸å…³ç‰¹å¾ï¼Œå¢å¼ºäº†æ¨¡å‹çš„è¯Šæ–­èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.05591">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-77ff395e79e59157305a6ed4ec577976.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c79532f3630f42f218eb0581c9f7d263.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-26df7c4561da3aae2838cafbae17ac08.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-83fd8c93297339b1e313f84dd49ba2fe.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="ReLayout-Integrating-Relation-Reasoning-for-Content-aware-Layout-Generation-with-Multi-modal-Large-Language-Models"><a href="#ReLayout-Integrating-Relation-Reasoning-for-Content-aware-Layout-Generation-with-Multi-modal-Large-Language-Models" class="headerlink" title="ReLayout: Integrating Relation Reasoning for Content-aware Layout   Generation with Multi-modal Large Language Models"></a>ReLayout: Integrating Relation Reasoning for Content-aware Layout   Generation with Multi-modal Large Language Models</h2><p><strong>Authors:Jiaxu Tian, Xuehui Yu, Yaoxing Wang, Pan Wang, Guangqian Guo, Shan Gao</strong></p>
<p>Content-aware layout aims to arrange design elements appropriately on a given canvas to convey information effectively. Recently, the trend for this task has been to leverage large language models (LLMs) to generate layouts automatically, achieving remarkable performance. However, existing LLM-based methods fail to adequately interpret spatial relationships among visual themes and design elements, leading to structural and diverse problems in layout generation. To address this issue, we introduce ReLayout, a novel method that leverages relation-CoT to generate more reasonable and aesthetically coherent layouts by fundamentally originating from design concepts. Specifically, we enhance layout annotations by introducing explicit relation definitions, such as region, salient, and margin between elements, with the goal of decomposing the layout into smaller, structured, and recursive layouts, thereby enabling the generation of more structured layouts. Furthermore, based on these defined relationships, we introduce a layout prototype rebalance sampler, which defines layout prototype features across three dimensions and quantifies distinct layout styles. This sampler addresses uniformity issues in generation that arise from data bias in the prototype distribution balance process. Extensive experimental results verify that ReLayout outperforms baselines and can generate structural and diverse layouts that are more aligned with human aesthetics and more explainable. </p>
<blockquote>
<p>å†…å®¹æ„ŸçŸ¥å¸ƒå±€æ—¨åœ¨åˆç†å®‰æ’ç»™å®šç”»å¸ƒä¸Šçš„è®¾è®¡å…ƒç´ ï¼Œä»¥æœ‰æ•ˆåœ°ä¼ è¾¾ä¿¡æ¯ã€‚æœ€è¿‘ï¼Œé’ˆå¯¹æ­¤ä»»åŠ¡çš„è¶‹åŠ¿æ˜¯å€ŸåŠ©å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è‡ªåŠ¨ç”Ÿæˆå¸ƒå±€ï¼Œå¹¶å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½ã€‚ç„¶è€Œï¼ŒåŸºäºLLMçš„ç°æœ‰æ–¹æ³•æ— æ³•å……åˆ†è§£é‡Šè§†è§‰ä¸»é¢˜å’Œè®¾è®¡å…ƒç´ ä¹‹é—´çš„ç©ºé—´å…³ç³»ï¼Œå¯¼è‡´å¸ƒå±€ç”Ÿæˆä¸­å­˜åœ¨ç»“æ„å’Œå¤šæ ·åŒ–é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ReLayoutï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œå®ƒé€šè¿‡åˆ©ç”¨å…³ç³»-CoTä»æ ¹æœ¬ä¸Šæºäºè®¾è®¡æ¦‚å¿µæ¥ç”Ÿæˆæ›´åˆç†å’Œç¾å­¦ä¸Šè¿è´¯çš„å¸ƒå±€ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é€šè¿‡å¼•å…¥æ˜ç¡®çš„å…³è”å®šä¹‰æ¥å¢å¼ºå¸ƒå±€æ³¨é‡Šï¼Œå¦‚åŒºåŸŸã€æ˜¾è‘—æ€§å’Œå…ƒç´ ä¹‹é—´çš„è¾¹ç¼˜ï¼Œæ—¨åœ¨å°†å¸ƒå±€åˆ†è§£æˆæ›´å°ã€æ›´ç»“æ„åŒ–å’Œé€’å½’çš„å¸ƒå±€ï¼Œä»è€Œèƒ½å¤Ÿç”Ÿæˆæ›´ç»“æ„åŒ–çš„å¸ƒå±€ã€‚æ­¤å¤–ï¼ŒåŸºäºè¿™äº›å®šä¹‰çš„å…³ç³»ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¸ƒå±€åŸå‹å¹³è¡¡é‡‡æ ·å™¨ï¼Œå®ƒåœ¨ä¸‰ä¸ªç»´åº¦ä¸Šå®šä¹‰äº†å¸ƒå±€åŸå‹ç‰¹å¾å¹¶é‡åŒ–äº†ä¸åŒçš„å¸ƒå±€é£æ ¼ã€‚è¯¥é‡‡æ ·å™¨è§£å†³äº†åœ¨åŸå‹åˆ†å¸ƒå¹³è¡¡è¿‡ç¨‹ä¸­å› æ•°æ®åå·®è€Œäº§ç”Ÿçš„ç”Ÿæˆå‡åŒ€æ€§é—®é¢˜ã€‚å¤§é‡çš„å®éªŒç»“æœè¯å®ï¼ŒReLayoutä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œå¹¶èƒ½ç”Ÿæˆæ›´ç¬¦åˆäººç±»å®¡ç¾å’Œæ›´å…·è§£é‡Šæ€§çš„ç»“æ„å’Œå¤šæ ·åŒ–çš„å¸ƒå±€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.05568v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå†…å®¹æ„ŸçŸ¥çš„å¸ƒå±€æ—¨åœ¨åˆç†å®‰æ’è®¾è®¡å…ƒç´ ä»¥æœ‰æ•ˆä¼ è¾¾ä¿¡æ¯ã€‚å½“å‰è¶‹åŠ¿æ˜¯åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è‡ªåŠ¨ç”Ÿæˆå¸ƒå±€ï¼Œå¹¶å–å¾—æ˜¾è‘—æ€§èƒ½ã€‚ç„¶è€Œï¼Œç°æœ‰åŸºäºLLMçš„æ–¹æ³•åœ¨è§£è¯»è§†è§‰ä¸»é¢˜å’Œè®¾è®¡å…ƒç´ ä¹‹é—´çš„ç©ºé—´å…³ç³»æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå¯¼è‡´å¸ƒå±€ç”Ÿæˆä¸­å­˜åœ¨ç»“æ„å’Œå¤šæ ·åŒ–é—®é¢˜ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ReLayoutæ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨å…³ç³»è®¤çŸ¥å»ºæ¨¡ç”Ÿæˆæ›´åˆç†ã€ç¾å­¦ä¸Šæ›´è¿è´¯çš„å¸ƒå±€ã€‚é€šè¿‡å¼•å…¥æ˜ç¡®çš„å…³ç³»å®šä¹‰å¢å¼ºå¸ƒå±€æ³¨é‡Šï¼Œå¦‚åŒºåŸŸã€æ˜¾è‘—æ€§å’Œå…ƒç´ é—´çš„é—´è·ï¼Œå°†å¸ƒå±€åˆ†è§£ä¸ºæ›´å°ã€æ›´ç»“æ„åŒ–çš„é€’å½’å¸ƒå±€ï¼Œä»è€Œå®ç°æ›´ç»“æ„åŒ–å¸ƒå±€çš„ç”Ÿæˆã€‚æ­¤å¤–ï¼ŒåŸºäºè¿™äº›å®šä¹‰çš„å…³ç³»ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¸ƒå±€åŸå‹å¹³è¡¡é‡‡æ ·å™¨ï¼Œè¯¥é‡‡æ ·å™¨åœ¨ä¸‰ä¸ªç»´åº¦ä¸Šå®šä¹‰å¸ƒå±€åŸå‹ç‰¹å¾å¹¶é‡åŒ–ä¸åŒçš„å¸ƒå±€é£æ ¼ã€‚å®éªŒç»“æœéªŒè¯ï¼ŒReLayoutä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œå¹¶èƒ½ç”Ÿæˆæ›´ç¬¦åˆäººç±»å®¡ç¾å’Œæ›´å¯è§£é‡Šçš„ç»“æ„åŒ–å’Œå¤šæ ·åŒ–å¸ƒå±€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å†…å®¹æ„ŸçŸ¥å¸ƒå±€æ—¨åœ¨æœ‰æ•ˆä¼ è¾¾ä¿¡æ¯ï¼Œé€šè¿‡åˆç†å®‰æ’è®¾è®¡å…ƒç´ åœ¨ç»™å®šç”»å¸ƒä¸Šçš„ä½ç½®ã€‚</li>
<li>ç°æœ‰åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„å¸ƒå±€ç”Ÿæˆæ–¹æ³•å­˜åœ¨è§£è¯»ç©ºé—´å…³ç³»ä¸è¶³çš„é—®é¢˜ã€‚</li>
<li>ReLayoutæ–¹æ³•åˆ©ç”¨å…³ç³»è®¤çŸ¥å»ºæ¨¡ç”Ÿæˆæ›´åˆç†å’Œç¾å­¦ä¸Šè¿è´¯çš„å¸ƒå±€ã€‚</li>
<li>é€šè¿‡å¼•å…¥æ˜ç¡®çš„å…³ç³»å®šä¹‰ï¼ˆå¦‚åŒºåŸŸã€æ˜¾è‘—æ€§å’Œå…ƒç´ é—´è·ï¼‰å¢å¼ºå¸ƒå±€æ³¨é‡Šã€‚</li>
<li>ReLayoutå°†å¸ƒå±€åˆ†è§£ä¸ºæ›´å°ã€ç»“æ„åŒ–çš„é€’å½’å¸ƒå±€ï¼Œå®ç°æ›´ç»“æ„åŒ–å¸ƒå±€çš„ç”Ÿæˆã€‚</li>
<li>å¸ƒå±€åŸå‹å¹³è¡¡é‡‡æ ·å™¨çš„å¼•å…¥è§£å†³äº†å› æ•°æ®åè§è€Œäº§ç”Ÿçš„å¸ƒå±€ç”Ÿæˆå‡åŒ€æ€§é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.05568">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bfdc8bc0caa05a612c89498a3cd99627.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9c164fd4aaa27d416dd30e2b4ddd63bc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-171cdce766324e60d8150f4b2c5bbc3e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-140afd1d7cefecd0cd7bdd231d3ef0ae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-45ae8ab59845341983b665adb36a932b.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Learn-Globally-Speak-Locally-Bridging-the-Gaps-in-Multilingual-Reasoning"><a href="#Learn-Globally-Speak-Locally-Bridging-the-Gaps-in-Multilingual-Reasoning" class="headerlink" title="Learn Globally, Speak Locally: Bridging the Gaps in Multilingual   Reasoning"></a>Learn Globally, Speak Locally: Bridging the Gaps in Multilingual   Reasoning</h2><p><strong>Authors:Jaedong Hwang, Kumar Tanmay, Seok-Jin Lee, Ayush Agrawal, Hamid Palangi, Kumar Ayush, Ila Fiete, Paul Pu Liang</strong></p>
<p>Large Language Models (LLMs) have achieved strong performance in domains like mathematics, factual QA, and code generation, yet their multilingual reasoning capabilities in these tasks remain underdeveloped. Especially for low-resource languages such as Swahili or Thai, LLMs can often misinterpret prompts or default to reasoning in English. This implicit bias toward high-resource languages undermines factual accuracy, interpretability, and trust. Current multilingual benchmarks focus only on final answers, overlooking whether models actually reason in the target language. To address this gap, we introduce GeoFact-X, a geography-based multilingual factual reasoning benchmark with annotated reasoning traces in five languages: English, Hindi, Japanese, Swahili, and Thai. We further propose BRIDGE, a novel training method that guides supervised fine-tuning and test-time reinforcement learning with a language-consistency reward to align reasoning with the input language. Finally, we develop an automatic evaluation protocol using LLM-as-a-judge to assess answer correctness and the quality and language consistency of reasoning traces, enabling nuanced and scalable analysis beyond surface-level metrics. Our results show that BRIDGE significantly enhances multilingual reasoning fidelity, demonstrating that reasoning-aware multilingual reinforcement learning is crucial for robust cross-lingual generalization. <a target="_blank" rel="noopener" href="https://jd730.github.io/projects/GeoFact-X_BRIDGE">https://jd730.github.io/projects/GeoFact-X_BRIDGE</a> </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ•°å­¦ã€äº‹å®é—®ç­”å’Œä»£ç ç”Ÿæˆç­‰é¢†åŸŸè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†å®ƒä»¬åœ¨è¿™äº›ä»»åŠ¡ä¸­çš„å¤šè¯­ç§æ¨ç†èƒ½åŠ›ä»å¾…å‘å±•ã€‚ç‰¹åˆ«æ˜¯å¯¹äºæ–¯ç“¦å¸Œé‡Œè¯­æˆ–æ³°è¯­ç­‰ä½èµ„æºè¯­è¨€ï¼ŒLLMç»å¸¸è¯¯è§£æç¤ºæˆ–é»˜è®¤ä»¥è‹±è¯­è¿›è¡Œæ¨ç†ã€‚è¿™ç§å¯¹é«˜èµ„æºè¯­è¨€çš„éšæ€§åå¥½ä¼šæŸå®³äº‹å®å‡†ç¡®æ€§ã€å¯è§£é‡Šæ€§å’Œä¿¡ä»»åº¦ã€‚å½“å‰çš„å¤šè¯­ç§åŸºå‡†æµ‹è¯•ä»…å…³æ³¨æœ€ç»ˆç­”æ¡ˆï¼Œå¿½è§†æ¨¡å‹æ˜¯å¦ä»¥ç›®æ ‡è¯­è¨€è¿›è¡Œæ¨ç†ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¼•å…¥äº†GeoFact-Xï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºåœ°ç†çš„å¤šè¯­ç§äº‹å®æ¨ç†åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«äº”ç§è¯­è¨€çš„æ³¨é‡Šæ¨ç†è½¨è¿¹ï¼šè‹±è¯­ã€å°åœ°è¯­ã€æ—¥è¯­ã€æ–¯ç“¦å¸Œé‡Œè¯­å’Œæ³°è¯­ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„è®­ç»ƒæ–¹æ³•BRIDGEï¼Œè¯¥æ–¹æ³•é€šè¿‡ç›‘ç£å¾®è°ƒä¸æµ‹è¯•æ—¶çš„å¼ºåŒ–å­¦ä¹ ï¼Œä»¥è¯­è¨€ä¸€è‡´æ€§å¥–åŠ±æ¥å¼•å¯¼æ¨ç†ä¸è¾“å…¥è¯­è¨€å¯¹é½ã€‚æœ€åï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§è‡ªåŠ¨è¯„ä¼°åè®®ï¼Œåˆ©ç”¨LLM-as-a-judgeæ¥è¯„ä¼°ç­”æ¡ˆçš„æ­£ç¡®æ€§ã€æ¨ç†è½¨è¿¹çš„è´¨é‡å’Œè¯­è¨€ä¸€è‡´æ€§ï¼Œä»è€Œå®ç°è¶…è¶Šè¡¨é¢å±‚æ¬¡çš„æŒ‡æ ‡çš„å¾®å¦™å’Œå¯æ‰©å±•åˆ†æã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒBRIDGEæ˜¾è‘—æé«˜äº†å¤šè¯­ç§æ¨ç†çš„ä¿çœŸåº¦ï¼Œè¯æ˜æ¨ç†æ„ŸçŸ¥çš„å¤šè¯­ç§å¼ºåŒ–å­¦ä¹ å¯¹äºç¨³å¥çš„è·¨è¯­è¨€æ³›åŒ–è‡³å…³é‡è¦ã€‚è¯¦æƒ…è¯·è§<a target="_blank" rel="noopener" href="https://jd730.github.io/projects/GeoFact-X_BRIDGE%E3%80%82">https://jd730.github.io/projects/GeoFact-X_BRIDGEã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.05418v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ•°å­¦ã€äº‹å®é—®ç­”å’Œä»£ç ç”Ÿæˆç­‰é¢†åŸŸè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†åœ¨å¤šè¯­ç§æ¨ç†èƒ½åŠ›æ–¹é¢ä»æœ‰å¾…æå‡ã€‚ç‰¹åˆ«æ˜¯å¯¹äºæ–¯ç“¦å¸Œé‡Œè¯­æˆ–æ³°è¯­ç­‰ä½èµ„æºè¯­è¨€ï¼ŒLLMså®¹æ˜“è¯¯è§£æç¤ºæˆ–é»˜è®¤ä»¥è‹±è¯­è¿›è¡Œæ¨ç†ã€‚è¿™ç§å¯¹é«˜èµ„æºè¯­è¨€çš„éšæ€§åå¥½ä¼šå½±å“äº‹å®å‡†ç¡®æ€§ã€å¯è§£é‡Šæ€§å’Œä¿¡ä»»åº¦ã€‚å½“å‰çš„å¤šè¯­ç§åŸºå‡†æµ‹è¯•ä¸»è¦å…³æ³¨æœ€ç»ˆç­”æ¡ˆï¼Œå¿½è§†æ¨¡å‹æ˜¯å¦ç›®æ ‡è¯­è¨€è¿›è¡Œæ¨ç†ã€‚ä¸ºè§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºGeoFact-Xï¼Œä¸€ä¸ªåŸºäºåœ°ç†çš„å¤šè¯­ç§äº‹å®æ¨ç†åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«äº”ç§è¯­è¨€çš„æ¨ç†è½¨è¿¹æ ‡æ³¨ï¼šè‹±è¯­ã€å°åœ°è¯­ã€æ—¥è¯­ã€æ–¯ç“¦å¸Œé‡Œè¯­å’Œæ³°è¯­ã€‚æˆ‘ä»¬è¿˜æå‡ºBRIDGEï¼Œä¸€ç§æ–°å‹è®­ç»ƒæ–¹å¼ï¼Œé€šè¿‡æŒ‡å¯¼ç›‘ç£å¾®è°ƒä¸æµ‹è¯•æ—¶çš„å¼ºåŒ–å­¦ä¹ ï¼Œä»¥è¯­è¨€ä¸€è‡´æ€§å¥–åŠ±æ¥ä½¿æ¨ç†ä¸è¾“å…¥è¯­è¨€å¯¹é½ã€‚æœ€åï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§è‡ªåŠ¨è¯„ä¼°åè®®ï¼Œåˆ©ç”¨LLM-as-a-judgeè¯„ä¼°ç­”æ¡ˆçš„æ­£ç¡®æ€§ä»¥åŠæ¨ç†è½¨è¿¹çš„è´¨é‡å’Œè¯­è¨€ä¸€è‡´æ€§ï¼Œä»¥è¿›è¡Œè¶…è¶Šè¡¨é¢å±‚æ¬¡çš„ç»†è‡´å’Œè§„æ¨¡åŒ–åˆ†æã€‚ç ”ç©¶è¡¨æ˜ï¼ŒBRIDGEæ˜¾è‘—æé«˜äº†å¤šè¯­ç§æ¨ç†çš„å¿ å®åº¦ï¼Œè¯æ˜ä»¥æ¨ç†ä¸ºæ ¸å¿ƒçš„å¤šè¯­ç§å¼ºåŒ–å­¦ä¹ å¯¹äºè·¨è¯­è¨€æ³›åŒ–è‡³å…³é‡è¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šè¯­ç§æ¨ç†æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½èµ„æºè¯­è¨€é¢†åŸŸã€‚</li>
<li>å½“å‰å¤šè¯­ç§åŸºå‡†æµ‹è¯•ä¸»è¦å…³æ³¨æœ€ç»ˆç­”æ¡ˆï¼Œå¿½è§†æ¨¡å‹æ˜¯å¦çœŸæ­£åœ¨ç›®æ ‡è¯­è¨€è¿›è¡Œæ¨ç†ã€‚</li>
<li>æ¨å‡ºGeoFact-XåŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–å¤šç§è¯­è¨€çš„æ¨ç†è½¨è¿¹æ ‡æ³¨ã€‚</li>
<li>æå‡ºBRIDGEè®­ç»ƒæ–¹å¼ï¼Œé€šè¿‡ç›‘ç£å¾®è°ƒä¸å¼ºåŒ–å­¦ä¹ æé«˜å¤šè¯­ç§æ¨ç†èƒ½åŠ›ã€‚</li>
<li>å¼€å‘è‡ªåŠ¨è¯„ä¼°åè®®ï¼Œè¯„ä¼°ç­”æ¡ˆåŠæ¨ç†è½¨è¿¹çš„è´¨é‡å’Œè¯­è¨€ä¸€è‡´æ€§ã€‚</li>
<li>BRIDGEè®­ç»ƒæ˜¾è‘—æé«˜äº†å¤šè¯­ç§æ¨ç†çš„å¿ å®åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.05418">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-6411a34bb554eb3e39c2cb6d2de32e16.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-574f341039eab8428636e006fa20a071.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-465ae6fcb76084b7184ffe6d0355f202.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Reinforcement-Fine-Tuning-Naturally-Mitigates-Forgetting-in-Continual-Post-Training"><a href="#Reinforcement-Fine-Tuning-Naturally-Mitigates-Forgetting-in-Continual-Post-Training" class="headerlink" title="Reinforcement Fine-Tuning Naturally Mitigates Forgetting in Continual   Post-Training"></a>Reinforcement Fine-Tuning Naturally Mitigates Forgetting in Continual   Post-Training</h2><p><strong>Authors:Song Lai, Haohan Zhao, Rong Feng, Changyi Ma, Wenzhuo Liu, Hongbo Zhao, Xi Lin, Dong Yi, Min Xie, Qingfu Zhang, Hongbin Liu, Gaofeng Meng, Fei Zhu</strong></p>
<p>Continual post-training (CPT) is a popular and effective technique for adapting foundation models like multimodal large language models to specific and ever-evolving downstream tasks. While existing research has primarily concentrated on methods like data replay, model expansion, or parameter regularization, the fundamental role of the learning paradigm within CPT remains largely unexplored. This paper presents a comparative analysis of two core post-training paradigms: supervised fine-tuning (SFT) and reinforcement fine-tuning (RFT), investigating their respective impacts on knowledge retention during CPT. Our experiments are conducted on a benchmark comprising seven diverse multimodal tasks, utilizing Qwen2.5-VL-7B-Instruct as the base model for continual post-training. The investigation yields two significant findings: (1) When continuously learning on downstream tasks, SFT leads to catastrophic forgetting of previously learned tasks. In contrast, RFT inherently preserves prior knowledge and achieve performance comparable to multi-task training. (2) RFT successfully protects and even enhances the modelâ€™s general knowledge on standard benchmarks (e.g., MMMU and MMLU-Pro). Conversely, SFT degrades general model capabilities severely. Further analysis shows that explicit mechanisms, such as KL penalty and chain-of-thought reasoning, are not the primary factors. Instead, we find that the implicit regularization inherent to RFT is a key factor in mitigating forgetting. Finally, we propose a rollout-based instance filtering algorithm to improve the stability and efficiency of RFT. Our comprehensive study demonstrates the superiority of RFT as a robust paradigm for continual post-training. </p>
<blockquote>
<p>æŒç»­åè®­ç»ƒï¼ˆCPTï¼‰æ˜¯ä¸€ç§é€‚åº”åŸºç¡€æ¨¡å‹ï¼ˆå¦‚å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼‰åˆ°ç‰¹å®šä¸”ä¸æ–­å‘å±•çš„ä¸‹æ¸¸ä»»åŠ¡çš„æµè¡Œä¸”æœ‰æ•ˆçš„æŠ€æœ¯ã€‚å°½ç®¡ç°æœ‰ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨æ•°æ®å›æ”¾ã€æ¨¡å‹æ‰©å±•æˆ–å‚æ•°æ­£åˆ™åŒ–ç­‰æ–¹æ³•ä¸Šï¼Œä½†CPTä¸­å­¦ä¹ èŒƒå¼çš„åŸºæœ¬ä½œç”¨ä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æœ¬æ–‡å¯¹æ¯”åˆ†æä¸¤ç§æ ¸å¿ƒåè®­ç»ƒèŒƒå¼ï¼šæœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å¾®è°ƒï¼ˆRFTï¼‰ï¼Œç ”ç©¶å®ƒä»¬å¯¹CPTæœŸé—´çŸ¥è¯†ä¿ç•™çš„å„è‡ªå½±å“ã€‚æˆ‘ä»¬çš„å®éªŒæ˜¯åœ¨åŒ…å«ä¸ƒä¸ªä¸åŒå¤šæ¨¡æ€ä»»åŠ¡çš„åŸºå‡†æµ‹è¯•é›†ä¸Šè¿›è¡Œçš„ï¼Œå¹¶åˆ©ç”¨Qwen2.5-VL-7B-Instructä½œä¸ºæŒç»­åè®­ç»ƒçš„åŸºç¡€æ¨¡å‹ã€‚è°ƒæŸ¥äº§ç”Ÿäº†ä¸¤ä¸ªé‡è¦å‘ç°ï¼šï¼ˆ1ï¼‰åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šè¿›è¡ŒæŒç»­å­¦ä¹ æ—¶ï¼ŒSFTä¼šå¯¼è‡´å…ˆå‰å­¦ä¹ ä»»åŠ¡çš„ç¾éš¾æ€§é—å¿˜ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒRFTå›ºæœ‰åœ°ä¿ç•™å…ˆå‰çŸ¥è¯†ï¼Œå¹¶å®ç°ä¸å¤šä»»åŠ¡è®­ç»ƒç›¸å½“çš„æ€§èƒ½ã€‚ï¼ˆ2ï¼‰RFTæˆåŠŸä¿æŠ¤ç”šè‡³æé«˜äº†æ¨¡å‹åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•é›†ï¼ˆä¾‹å¦‚MMMUå’ŒMMLU-Proï¼‰ä¸Šçš„é€šç”¨çŸ¥è¯†ã€‚ç›¸åï¼ŒSFTä¸¥é‡é™ä½äº†æ¨¡å‹çš„é€šç”¨èƒ½åŠ›ã€‚è¿›ä¸€æ­¥åˆ†æè¡¨æ˜ï¼Œæ˜ç¡®çš„æœºåˆ¶ï¼ˆå¦‚KLæƒ©ç½šå’Œé“¾å¼æ€ç»´æ¨ç†ï¼‰å¹¶ä¸æ˜¯ä¸»è¦å› ç´ ã€‚ç›¸åï¼Œæˆ‘ä»¬å‘ç°RFTæ‰€å›ºæœ‰çš„éšå¼æ­£åˆ™åŒ–æ˜¯ç¼“è§£é—å¿˜çš„å…³é”®å› ç´ ã€‚æœ€åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºrolloutçš„å®ä¾‹è¿‡æ»¤ç®—æ³•ï¼Œä»¥æé«˜RFTçš„ç¨³å®šæ€§å’Œæ•ˆç‡ã€‚æˆ‘ä»¬çš„ç»¼åˆç ”ç©¶è¯æ˜äº†RFTä½œä¸ºæŒç»­åè®­ç»ƒçš„ç¨³å¥èŒƒå¼çš„ä¼˜è¶Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.05386v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬è®ºæ–‡å¯¹æŒç»­è®­ç»ƒåçš„ä¸¤ç§æ ¸å¿ƒèŒƒå¼â€”â€”ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å¾®è°ƒï¼ˆRFTï¼‰è¿›è¡Œäº†æ¯”è¾ƒåˆ†æï¼Œæ¢ç©¶äº†å®ƒä»¬åœ¨æŒç»­è®­ç»ƒè¿‡ç¨‹ä¸­å¯¹çŸ¥è¯†ä¿ç•™çš„å½±å“ã€‚å®éªŒä»¥Qwen2.5-VL-7B-Instructä¸ºåŸºç¡€æ¨¡å‹ï¼Œåœ¨ä¸ƒä¸ªä¸åŒçš„å¤šæ¨¡æ€ä»»åŠ¡ä¸Šè¿›è¡Œã€‚ç ”ç©¶å‘ç°ï¼Œåœ¨è¿›è¡Œä¸‹æ¸¸ä»»åŠ¡æŒç»­å­¦ä¹ æ—¶ï¼ŒSFTä¼šå¯¼è‡´å…ˆå‰å­¦ä¹ ä»»åŠ¡çš„ç¾éš¾æ€§é—å¿˜ï¼Œè€ŒRFTåˆ™èƒ½å¤Ÿä¿ç•™å…ˆéªŒçŸ¥è¯†å¹¶å®ç°ä¸å¤šä»»åŠ¡è®­ç»ƒç›¸å½“çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒRFTèƒ½å¤Ÿä¿æŠ¤ç”šè‡³æé«˜æ¨¡å‹åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ï¼ˆå¦‚MMMUå’ŒMMLU-Proï¼‰ä¸Šçš„é€šç”¨çŸ¥è¯†ï¼Œè€ŒSFTåˆ™ä¼šä¸¥é‡é™ä½æ¨¡å‹çš„é€šç”¨èƒ½åŠ›ã€‚è¿›ä¸€æ­¥åˆ†æè¡¨æ˜ï¼Œæ˜ç¡®çš„æœºåˆ¶ï¼ˆå¦‚KLæƒ©ç½šå’Œé“¾å¼æ€ç»´æ¨ç†ï¼‰å¹¶ä¸æ˜¯ä¸»è¦å› ç´ ï¼Œè€ŒRFTä¸­å›ºæœ‰çš„éšå¼æ­£åˆ™åŒ–æ˜¯ç¼“è§£é—å¿˜çš„å…³é”®ã€‚æœ€åï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºrolloutçš„å®ä¾‹è¿‡æ»¤ç®—æ³•ï¼Œä»¥æé«˜RFTçš„ç¨³å®šæ€§å’Œæ•ˆç‡ã€‚ç»¼åˆç ”ç©¶è¡¨æ˜ï¼ŒRFTä½œä¸ºä¸€ç§ç¨³å¥çš„æŒç»­è®­ç»ƒèŒƒå¼å…·æœ‰ä¼˜è¶Šæ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è®ºæ–‡å¯¹æ¯”åˆ†æäº†ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å¾®è°ƒï¼ˆRFTï¼‰ä¸¤ç§æ ¸å¿ƒæŒç»­è®­ç»ƒèŒƒå¼ã€‚</li>
<li>SFTåœ¨æŒç»­å­¦ä¹ ä¸‹æ¸¸ä»»åŠ¡æ—¶ä¼šå¯¼è‡´å…ˆå‰å­¦ä¹ ä»»åŠ¡çš„ç¾éš¾æ€§é—å¿˜ã€‚</li>
<li>RFTèƒ½å¤Ÿä¿ç•™å…ˆéªŒçŸ¥è¯†ï¼Œå®ç°ä¸å¤šä»»åŠ¡è®­ç»ƒç›¸å½“çš„æ€§èƒ½ã€‚</li>
<li>RFTèƒ½å¤Ÿä¿æŠ¤å¹¶æå‡æ¨¡å‹åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸Šçš„é€šç”¨çŸ¥è¯†ã€‚</li>
<li>æ˜ç¡®çš„æœºåˆ¶ï¼ˆå¦‚KLæƒ©ç½šå’Œé“¾å¼æ€ç»´æ¨ç†ï¼‰åœ¨ç¼“è§£é—å¿˜æ–¹é¢ä¸æ˜¯ä¸»è¦å› ç´ ã€‚</li>
<li>RFTä¸­çš„éšå¼æ­£åˆ™åŒ–æ˜¯ç¼“è§£é—å¿˜çš„å…³é”®ã€‚</li>
<li>è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºrolloutçš„å®ä¾‹è¿‡æ»¤ç®—æ³•ï¼Œä»¥æé«˜RFTçš„ç¨³å®šæ€§å’Œæ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.05386">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e04ae1e05565fe6f0b7102ce4b96d2ed.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ecba3ec850e4ccddd0b7e6179e0a1c7c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2a54ae64caa2c30331e5998651f46cde.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ae871baaf6c963f5dd4ee5cc2406173.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="A-Comparative-Study-of-Specialized-LLMs-as-Dense-Retrievers"><a href="#A-Comparative-Study-of-Specialized-LLMs-as-Dense-Retrievers" class="headerlink" title="A Comparative Study of Specialized LLMs as Dense Retrievers"></a>A Comparative Study of Specialized LLMs as Dense Retrievers</h2><p><strong>Authors:Hengran Zhang, Keping Bi, Jiafeng Guo</strong></p>
<p>While large language models (LLMs) are increasingly deployed as dense retrievers, the impact of their domain-specific specialization on retrieval effectiveness remains underexplored. This investigation systematically examines how task-specific adaptations in LLMs influence their retrieval capabilities, an essential step toward developing unified retrievers capable of handling text, code, images, and multimodal content. We conduct extensive experiments with eight Qwen2.5 7B LLMs, including base, instruction-tuned, code&#x2F;math-specialized, long reasoning, and vision-language models across zero-shot retrieval settings and the supervised setting. For the zero-shot retrieval settings, we consider text retrieval from the BEIR benchmark and code retrieval from the CoIR benchmark. Further, to evaluate supervised performance, all LLMs are fine-tuned on the MS MARCO dataset. We find that mathematical specialization and the long reasoning capability cause consistent degradation in three settings, indicating conflicts between mathematical reasoning and semantic matching. The vision-language model and code-specialized LLMs demonstrate superior zero-shot performance compared to other LLMs, even surpassing BM25 on the code retrieval task, and maintain comparable performance to base LLMs in supervised settings. These findings suggest promising directions for the unified retrieval task leveraging cross-domain and cross-modal fusion. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¶Šæ¥è¶Šå¤šåœ°è¢«éƒ¨ç½²ä¸ºå¯†é›†æ£€ç´¢å™¨ï¼Œå…¶é¢†åŸŸç‰¹å®šä¸“ä¸šåŒ–å¯¹æ£€ç´¢æ•ˆæœçš„å½±å“å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚è¿™é¡¹ç ”ç©¶ç³»ç»Ÿåœ°æ¢è®¨äº†LLMä¸­çš„ä»»åŠ¡ç‰¹å®šé€‚åº”å¦‚ä½•å½±å“å…¶æ£€ç´¢èƒ½åŠ›ï¼Œè¿™æ˜¯æœç€å¼€å‘èƒ½å¤Ÿå¤„ç†æ–‡æœ¬ã€ä»£ç ã€å›¾åƒå’Œå¤šæ¨¡å¼å†…å®¹çš„ç»Ÿä¸€æ£€ç´¢å™¨è¿ˆå‡ºçš„é‡è¦ä¸€æ­¥ã€‚æˆ‘ä»¬åœ¨å…«ä¸ªQwen2.5 7B LLMä¸Šè¿›è¡Œäº†å¹¿æ³›å®éªŒï¼ŒåŒ…æ‹¬åŸºç¡€ã€æŒ‡ä»¤è°ƒä¼˜ã€ä»£ç &#x2F;æ•°å­¦ä¸“ä¸šåŒ–ã€é•¿æœŸæ¨ç†å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼Œæ¶µç›–äº†é›¶å°„å‡»æ£€ç´¢è®¾ç½®å’Œæœ‰ç›‘ç£è®¾ç½®ã€‚å¯¹äºé›¶å°„å‡»æ£€ç´¢è®¾ç½®ï¼Œæˆ‘ä»¬è€ƒè™‘ä»BEIRåŸºå‡†æµ‹è¯•ä¸­è¿›è¡Œæ–‡æœ¬æ£€ç´¢ï¼Œå¹¶ä»CoIRåŸºå‡†æµ‹è¯•ä¸­è¿›è¡Œä»£ç æ£€ç´¢ã€‚æ­¤å¤–ï¼Œä¸ºäº†è¯„ä¼°ç›‘ç£æ€§èƒ½ï¼Œæ‰€æœ‰LLMéƒ½åœ¨MS MARCOæ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒã€‚æˆ‘ä»¬å‘ç°æ•°å­¦ä¸“ä¸šåŒ–å’Œé•¿æœŸæ¨ç†èƒ½åŠ›ä¼šå¯¼è‡´ä¸‰ç§ç¯å¢ƒä¸‹çš„æŒç»­é€€åŒ–ï¼Œè¡¨æ˜æ•°å­¦æ¨ç†å’Œè¯­ä¹‰åŒ¹é…ä¹‹é—´å­˜åœ¨å†²çªã€‚è§†è§‰è¯­è¨€æ¨¡å‹å’Œä»£ç ä¸“ä¸šåŒ–çš„LLMåœ¨é›¶å°„å‡»æ€§èƒ½ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä¸å…¶ä»–LLMç›¸æ¯”å…·æœ‰ä¼˜åŠ¿ï¼Œç”šè‡³åœ¨ä»£ç æ£€ç´¢ä»»åŠ¡ä¸Šè¶…è¶Šäº†BM25ï¼Œåœ¨æœ‰ç›‘ç£ç¯å¢ƒä¸‹ä¿æŒä¸åŸºç¡€LLMç›¸å½“çš„æ€§èƒ½ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œåˆ©ç”¨è·¨åŸŸå’Œè·¨æ¨¡æ€èåˆçš„ç»Ÿä¸€æ£€ç´¢ä»»åŠ¡å¤§æœ‰å¸Œæœ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.03958v1">PDF</a> Accepted by CCIR25 and published by Springer LNCS or LNAI</p>
<p><strong>Summary</strong>ï¼šå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¯†é›†æ£€ç´¢å™¨ä¸­çš„åº”ç”¨è¶Šæ¥è¶Šå¹¿æ³›ï¼Œä½†å…¶é’ˆå¯¹ç‰¹å®šé¢†åŸŸçš„ä¸“ä¸šåŒ–å¯¹æ£€ç´¢æ•ˆæœçš„å½±å“å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æœ¬ç ”ç©¶ç³»ç»Ÿåœ°æ¢è®¨äº†LLMçš„ä»»åŠ¡ç‰¹å®šé€‚åº”æ€§å¯¹å…¶æ£€ç´¢èƒ½åŠ›çš„å½±å“ï¼Œè¿™æ˜¯å¼€å‘èƒ½å¤Ÿå¤„ç†æ–‡æœ¬ã€ä»£ç ã€å›¾åƒå’Œå¤šæ¨¡æ€å†…å®¹çš„ç»Ÿä¸€æ£€ç´¢å™¨çš„å…³é”®æ­¥éª¤ã€‚é€šè¿‡å¯¹å…«ç§ä¸åŒä¸“ä¸šåŒ–çš„LLMè¿›è¡Œå¹¿æ³›çš„å®éªŒï¼ŒåŒ…æ‹¬åŸºç¡€æ¨¡å‹ã€æŒ‡ä»¤è°ƒä¼˜ã€ä»£ç &#x2F;æ•°å­¦ä¸“ä¸šã€é€»è¾‘æ¨ç†å’Œè§†è§‰è¯­è¨€æ¨¡å‹ç­‰ï¼Œç ”ç©¶å‘ç°åœ¨ä¸‰ç§è®¾ç½®ä¸­æ•°å­¦ä¸“ä¸šåŒ–å’Œé€»è¾‘æ¨ç†èƒ½åŠ›çš„ä¸€è‡´æ€§ä¸‹é™è¡¨æ˜æ•°å­¦æ¨ç†å’Œè¯­ä¹‰åŒ¹é…ä¹‹é—´çš„å†²çªã€‚è§†è§‰è¯­è¨€æ¨¡å‹å’Œé’ˆå¯¹ä»£ç ä¸“ä¸šçš„LLMåœ¨é›¶å°„å‡»è®¾ç½®ä¸­çš„è¡¨ç°ä¼˜äºå…¶ä»–LLMï¼Œç”šè‡³åœ¨ä»£ç æ£€ç´¢ä»»åŠ¡ä¸Šè¶…è¶Šäº†BM25ï¼Œåœ¨ç›‘ç£è®¾ç½®ä¸­çš„æ€§èƒ½ä¹Ÿä¸å…¶ä»–åŸºç¡€LLMç›¸å½“ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œåˆ©ç”¨è·¨åŸŸå’Œè·¨æ¨¡æ€èåˆçš„ç»Ÿä¸€æ£€ç´¢ä»»åŠ¡å…·æœ‰å¹¿é˜”çš„å‘å±•å‰æ™¯ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¯†é›†æ£€ç´¢ä¸­çš„åº”ç”¨é€æ¸æ™®åŠï¼Œä½†å…¶é’ˆå¯¹ç‰¹å®šé¢†åŸŸçš„ä¸“ä¸šåŒ–å¯¹æ£€ç´¢æ•ˆæœå½±å“å°šå¾…ç ”ç©¶ã€‚</li>
<li>LLMçš„ä»»åŠ¡ç‰¹å®šé€‚åº”æ€§å¯¹å…¶æ£€ç´¢èƒ½åŠ›æœ‰ç³»ç»Ÿæ€§å½±å“ï¼Œè¿™æ˜¯å¼€å‘ç»Ÿä¸€æ£€ç´¢å™¨çš„é‡è¦æ­¥éª¤ã€‚</li>
<li>åœ¨ä¸åŒè®¾ç½®ä¸­ï¼Œæ•°å­¦ä¸“ä¸šåŒ–å’Œé€»è¾‘æ¨ç†èƒ½åŠ›çš„ä¸€è‡´æ€§ä¸‹é™è¡¨æ˜æ•°å­¦æ¨ç†å’Œè¯­ä¹‰åŒ¹é…ä¹‹é—´å­˜åœ¨å†²çªã€‚</li>
<li>è§†è§‰è¯­è¨€æ¨¡å‹å’Œé’ˆå¯¹ä»£ç ä¸“ä¸šçš„LLMåœ¨é›¶å°„å‡»è®¾ç½®ä¸­çš„è¡¨ç°ä¼˜äºå…¶ä»–LLMã€‚</li>
<li>é’ˆå¯¹ä»£ç ä¸“ä¸šçš„LLMåœ¨ä»£ç æ£€ç´¢ä»»åŠ¡ä¸Šçš„è¡¨ç°ç”šè‡³è¶…è¶Šäº†BM25ã€‚</li>
<li>åœ¨ç›‘ç£è®¾ç½®ä¸­ï¼Œè§†è§‰è¯­è¨€æ¨¡å‹å’Œé’ˆå¯¹ä»£ç ä¸“ä¸šçš„LLMçš„æ€§èƒ½ä¸å…¶ä»–åŸºç¡€LLMç›¸å½“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.03958">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ffb853bcbb83504fc0261e28b3c54ebd.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Enhancing-Adaptive-Behavioral-Interventions-with-LLM-Inference-from-Participant-Described-States"><a href="#Enhancing-Adaptive-Behavioral-Interventions-with-LLM-Inference-from-Participant-Described-States" class="headerlink" title="Enhancing Adaptive Behavioral Interventions with LLM Inference from   Participant-Described States"></a>Enhancing Adaptive Behavioral Interventions with LLM Inference from   Participant-Described States</h2><p><strong>Authors:Karine Karine, Benjamin M. Marlin</strong></p>
<p>The use of reinforcement learning (RL) methods to support health behavior change via personalized and just-in-time adaptive interventions is of significant interest to health and behavioral science researchers focused on problems such as smoking cessation support and physical activity promotion. However, RL methods are often applied to these domains using a small collection of context variables to mitigate the significant data scarcity issues that arise from practical limitations on the design of adaptive intervention trials. In this paper, we explore an approach to significantly expanding the state space of an adaptive intervention without impacting data efficiency. The proposed approach enables intervention participants to provide natural language descriptions of aspects of their current state. It then leverages inference with pre-trained large language models (LLMs) to better align the policy of a base RL method with these state descriptions. To evaluate our method, we develop a novel physical activity intervention simulation environment that generates text-based state descriptions conditioned on latent state variables using an auxiliary LLM. We show that this approach has the potential to significantly improve the performance of online policy learning methods. </p>
<blockquote>
<p>ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ–¹æ³•ï¼Œé€šè¿‡ä¸ªæ€§åŒ–çš„å³æ—¶é€‚åº”æ€§å¹²é¢„æ¥æ”¯æŒå¥åº·è¡Œä¸ºæ”¹å˜ï¼Œå¯¹äºä¸“æ³¨äºæˆ’çƒŸæ”¯æŒå’Œä½“è‚²æ´»åŠ¨æ¨å¹¿ç­‰é—®é¢˜çš„å¥åº·å’Œè¡Œä¸ºç§‘å­¦ç ”ç©¶è€…æ¥è¯´ï¼Œå…·æœ‰æå¤§çš„å…´è¶£ã€‚ç„¶è€Œï¼Œç”±äºé€‚åº”å¹²é¢„è¯•éªŒè®¾è®¡çš„å®é™…é™åˆ¶æ‰€å¯¼è‡´çš„ä¸¥é‡æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼ŒRLæ–¹æ³•é€šå¸¸åº”ç”¨äºè¿™äº›é¢†åŸŸï¼Œä½¿ç”¨å°‘é‡ä¸Šä¸‹æ–‡å˜é‡æ¥ç¼“è§£ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ¢ç´¢äº†ä¸€ç§åœ¨ä¸å½±å“æ•°æ®æ•ˆç‡çš„æƒ…å†µä¸‹å¤§å¹…æ‰©å±•é€‚åº”æ€§å¹²é¢„çŠ¶æ€ç©ºé—´çš„æ–¹æ³•ã€‚æ‰€æå‡ºçš„æ–¹æ³•ä½¿å¹²é¢„å‚ä¸è€…èƒ½å¤Ÿæä¾›å¯¹å½“å‰çŠ¶æ€æ–¹é¢çš„è‡ªç„¶è¯­è¨€æè¿°ã€‚ç„¶åï¼Œå®ƒåˆ©ç”¨ä¸é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œæ¨æ–­ï¼Œä»¥æ›´å¥½åœ°å°†åŸºç¡€RLæ–¹æ³•çš„ç­–ç•¥ä¸è¿™äº›çŠ¶æ€æè¿°å¯¹é½ã€‚ä¸ºäº†è¯„ä¼°æˆ‘ä»¬çš„æ–¹æ³•ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªæ–°å‹ä½“è‚²æ´»åŠ¨å¹²é¢„æ¨¡æ‹Ÿç¯å¢ƒï¼Œè¯¥ç¯å¢ƒä½¿ç”¨è¾…åŠ©LLMæ ¹æ®æ½œåœ¨çŠ¶æ€å˜é‡ç”Ÿæˆæ–‡æœ¬çŠ¶æ€æè¿°ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œè¿™ç§æ–¹æ³•æœ‰å¯èƒ½æ˜¾è‘—æ”¹å–„åœ¨çº¿ç­–ç•¥å­¦ä¹ æ–¹æ³•çš„è¡¨ç°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.03871v1">PDF</a> Accepted at Machine Learning for Healthcare (MLHC) 2025</p>
<p><strong>Summary</strong><br>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ–¹æ³•è¢«ç”¨äºæ”¯æŒå¥åº·è¡Œä¸ºæ”¹å˜ï¼Œé€šè¿‡ä¸ªæ€§åŒ–åŠæ—¶é€‚åº”æ€§å¹²é¢„æ¥ä¿ƒè¿›æˆ’çƒŸæ”¯æŒå’Œèº«ä½“æ´»åŠ¨æ¨å¹¿ç­‰é—®é¢˜ï¼Œå¤‡å—å¥åº·å’Œè¡Œä¸ºç§‘å­¦ç ”ç©¶è€…å…³æ³¨ã€‚ç„¶è€Œï¼Œå®é™…åº”ç”¨ä¸­å¸¸å¸¸é¢ä¸´æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œé™åˆ¶äº†é€‚åº”æ€§å¹²é¢„è¯•éªŒçš„è®¾è®¡ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ˜¾è‘—æ‰©å±•é€‚åº”æ€§å¹²é¢„çŠ¶æ€ç©ºé—´çš„æ–¹æ³•ï¼Œé€šè¿‡è®©å‚ä¸è€…æä¾›å½“å‰çŠ¶æ€çš„è‡ªç„¶è¯­è¨€æè¿°ï¼Œå¹¶åˆ©ç”¨é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œæ¨ç†ï¼Œæ›´å¥½åœ°å°†åŸºç¡€å¼ºåŒ–å­¦ä¹ æ–¹æ³•çš„ç­–ç•¥ä¸è¿™äº›çŠ¶æ€æè¿°å¯¹é½ã€‚é€šè¿‡å¼€å‘æ–°å‹èº«ä½“æ´»åŠ¨å¹²é¢„æ¨¡æ‹Ÿç¯å¢ƒï¼Œæˆ‘ä»¬è¯æ˜äº†è¯¥æ–¹æ³•åœ¨æ”¹å–„åœ¨çº¿ç­–ç•¥å­¦ä¹ æ–¹æ³•çš„æ€§èƒ½ä¸Šå…·æœ‰å·¨å¤§æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ åœ¨å¥åº·è¡Œä¸ºæ”¹å˜ä¸­çš„åº”ç”¨é‡è¦ä¸”å—åˆ°å¹¿æ³›å…³æ³¨ã€‚</li>
<li>æ•°æ®ç¨€ç¼ºæ˜¯åº”ç”¨å¼ºåŒ–å­¦ä¹ æ—¶é¢ä¸´çš„é—®é¢˜ä¹‹ä¸€ã€‚</li>
<li>å…è®¸å‚ä¸è€…æä¾›å½“å‰çŠ¶æ€çš„è‡ªç„¶è¯­è¨€æè¿°æ˜¯è§£å†³æ•°æ®ç¨€ç¼ºé—®é¢˜çš„ä¸€ç§æœ‰æ•ˆæ–¹æ³•ã€‚</li>
<li>é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ç”¨äºæ¨ç†å¯ä»¥æ›´å¥½åœ°åŒ¹é…å¼ºåŒ–å­¦ä¹ ç­–ç•¥çš„å½“å‰çŠ¶æ€æè¿°ã€‚</li>
<li>æå‡ºçš„æ–°å‹ç‰©ç†æ´»åŠ¨å¹²é¢„æ¨¡æ‹Ÿç¯å¢ƒç”¨äºè¯„ä¼°æ­¤æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>è¯¥æ–¹æ³•æœ‰æœ›æ˜¾è‘—æé«˜åœ¨çº¿ç­–ç•¥å­¦ä¹ æ–¹æ³•çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.03871">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-19d71ddbd7e08a610fad246aca510c39.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="BMMR-A-Large-Scale-Bilingual-Multimodal-Multi-Discipline-Reasoning-Dataset"><a href="#BMMR-A-Large-Scale-Bilingual-Multimodal-Multi-Discipline-Reasoning-Dataset" class="headerlink" title="BMMR: A Large-Scale Bilingual Multimodal Multi-Discipline Reasoning   Dataset"></a>BMMR: A Large-Scale Bilingual Multimodal Multi-Discipline Reasoning   Dataset</h2><p><strong>Authors:Zhiheng Xi, Guanyu Li, Yutao Fan, Honglin Guo, Yufang Liu, Xiaoran Fan, Jiaqi Liu, Jingchao Ding, Wangmeng Zuo, Zhenfei Yin, Lei Bai, Tao Ji, Tao Gui, Qi Zhang, Philip Torr, Xuanjing Huang</strong></p>
<p>In this paper, we introduce BMMR, a large-scale bilingual, multimodal, multi-disciplinary reasoning dataset for the community to develop and evaluate large multimodal models (LMMs). BMMR comprises 110k college-level questions spanning 300 UNESCO-defined subjects, spanning diverse formats-multiple-choice, fill-in-the-blank, and open-ended QA-and sourced from both print and digital media such as books, exams, and quizzes. All data are curated and filtered via a human-in-the-loop and scalable framework, and each instance is paired with a high-quality reasoning path. The dataset is organized into two parts: BMMR-Eval that comprises 20,458 high-quality instances to comprehensively assess LMMsâ€™ knowledge and reasoning across multiple disciplines in both Chinese and English; and BMMR-Train that contains 88,991 instances to support further research and development, extending the current focus on mathematical reasoning to diverse disciplines and domains. In addition, we propose the process-based multi-discipline verifier (i.e., BMMR-Verifier) for accurate and fine-grained evaluation of reasoning paths. Extensive experiments on 24 models reveal that (i) even SOTA models (e.g., o3 and Gemini-2.5-Pro) leave substantial headroom on BMMR-Eval; (ii) reasoning models exhibit discipline bias and outperform LMMs only on specific subjects; (iii) open-source models still trail their proprietary counterparts; and (iv) fine-tuning on BMMR-Train narrows this gap. Additionally, we conduct reasoning-chain analyses using BMMR-Verifier and other in-depth studies, uncovering the challenges LMMs currently face in multidisciplinary reasoning. We will release the data, and we hope our work can offer insights and contributions to the community. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†BMMRï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„åŒè¯­ã€å¤šæ¨¡æ€ã€å¤šå­¦ç§‘æ¨ç†æ•°æ®é›†ï¼Œä¾›ç¤¾åŒºç”¨äºå¼€å‘å’Œè¯„ä¼°å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰ã€‚BMMRåŒ…å«æ¶‰åŠè”åˆå›½æ•™ç§‘æ–‡ç»„ç»‡å®šä¹‰çš„300ä¸ªå­¦ç§‘çš„11ä¸‡é“å¤§å­¦çº§åˆ«é—®é¢˜ï¼Œæ¶µç›–å¤šç§é¢˜å‹ï¼Œå¦‚é€‰æ‹©é¢˜ã€å¡«ç©ºé¢˜å’Œå¼€æ”¾å¼çš„é—®ç­”é¢˜ï¼Œæ¥æºäºä¹¦ç±ã€è€ƒè¯•å’Œæµ‹éªŒç­‰çº¸è´¨å’Œæ•°å­—åª’ä½“ã€‚æ‰€æœ‰æ•°æ®éƒ½æ˜¯é€šè¿‡äººæœºå¾ªç¯å’Œå¯æ‰©å±•çš„æ¡†æ¶è¿›è¡Œç­›é€‰å’Œè¿‡æ»¤çš„ï¼Œæ¯ä¸ªå®ä¾‹éƒ½é…æœ‰ä¸€æ¡é«˜è´¨é‡æ¨ç†è·¯å¾„ã€‚è¯¥æ•°æ®é›†åˆ†ä¸ºä¸¤éƒ¨åˆ†ï¼šBMMR-EvalåŒ…å«2ä¸‡é›¶458ä¸ªé«˜è´¨é‡å®ä¾‹ï¼Œæ—¨åœ¨å…¨é¢è¯„ä¼°LMMsåœ¨ä¸­è‹±æ–‡å¤šä¸ªå­¦ç§‘çš„çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ï¼›BMMR-TrainåŒ…å«8ä¸‡é›¶991ä¸ªå®ä¾‹ï¼Œç”¨äºæ”¯æŒè¿›ä¸€æ­¥çš„ç ”ç©¶å’Œå¼€å‘ï¼Œä»å½“å‰å¯¹æ•°å­¦æ¨ç†çš„å…³æ³¨æ‰©å±•åˆ°å¤šä¸ªå­¦ç§‘é¢†åŸŸã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºè¿‡ç¨‹çš„è·¨å­¦ç§‘éªŒè¯å™¨ï¼ˆå³BMMR-Verifierï¼‰ï¼Œå¯¹æ¨ç†è·¯å¾„è¿›è¡Œå‡†ç¡®ç»†è‡´çš„è¯„ä¼°ã€‚å¯¹24ä¸ªæ¨¡å‹çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼šï¼ˆiï¼‰å³ä½¿åœ¨BMMR-Evalä¸Šï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„æŠ€æœ¯æ¨¡å‹ï¼ˆå¦‚o3å’ŒGemini-2.5-Proï¼‰ä»æœ‰å¾ˆå¤§çš„æå‡ç©ºé—´ï¼›ï¼ˆiiï¼‰æ¨ç†æ¨¡å‹å­˜åœ¨å­¦ç§‘åè§ï¼Œä»…åœ¨ç‰¹å®šå­¦ç§‘ä¸Šä¼˜äºLMMsï¼›ï¼ˆiiiï¼‰å¼€æºæ¨¡å‹ä»ç„¶è½åäºä¸“æœ‰æ¨¡å‹ï¼›ï¼ˆivï¼‰åœ¨BMMR-Trainä¸Šè¿›è¡Œå¾®è°ƒå¯ä»¥ç¼©å°è¿™ä¸€å·®è·ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä½¿ç”¨BMMR-Verifierè¿›è¡Œæ¨ç†é“¾åˆ†æä»¥åŠå…¶ä»–æ·±å…¥ç ”ç©¶ï¼Œæ­ç¤ºäº†LMMså½“å‰åœ¨å¤šå­¦ç§‘æ¨ç†æ–¹é¢æ‰€é¢ä¸´çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬å°†å‘å¸ƒè¿™äº›æ•°æ®ï¼Œå¸Œæœ›æˆ‘ä»¬çš„å·¥ä½œèƒ½ä¸ºç¤¾åŒºæä¾›è§è§£å’Œè´¡çŒ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.03483v2">PDF</a> Preprint</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªå¤§è§„æ¨¡çš„åŒè¯­ã€å¤šæ¨¡æ€ã€å¤šå­¦ç§‘æ¨ç†æ•°æ®é›†BMMRï¼Œä¾›ç¤¾åŒºç”¨äºå¼€å‘å’Œè¯„ä¼°å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰ã€‚BMMRåŒ…å«110ké“æ¶‰åŠ300ä¸ªè”åˆå›½æ•™ç§‘æ–‡ç»„ç»‡å®šä¹‰å­¦ç§‘çš„å¤§å­¦çº§åˆ«é—®é¢˜ï¼Œæ¶µç›–å¤šç§é¢˜å‹ï¼Œå¦‚é€‰æ‹©é¢˜ã€å¡«ç©ºé¢˜å’Œå¼€æ”¾é—®ç­”ï¼Œæ¥æºäºä¹¦ç±ã€è€ƒè¯•å’Œæµ‹éªŒç­‰å°åˆ·å’Œæ•°å­—åª’ä½“ã€‚æ•°æ®é›†é€šè¿‡äººæœºåä½œå’Œå¯æ‰©å±•æ¡†æ¶è¿›è¡Œç­›é€‰å’Œè¿‡æ»¤ï¼Œæ¯ä¸ªå®ä¾‹éƒ½é…å¤‡é«˜è´¨é‡æ¨ç†è·¯å¾„ã€‚æ•°æ®é›†åˆ†ä¸ºä¸¤éƒ¨åˆ†ï¼šBMMR-EvalåŒ…å«20,458é“é«˜è´¨é‡å®ä¾‹ï¼Œæ—¨åœ¨å…¨é¢è¯„ä¼°LMMsåœ¨å¤šå­¦ç§‘ä¸­çš„çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ï¼ˆåŒ…æ‹¬ä¸­æ–‡å’Œè‹±æ–‡ï¼‰ï¼›BMMR-TrainåŒ…å«88,991ä¸ªå®ä¾‹ï¼Œç”¨äºæ”¯æŒè¿›ä¸€æ­¥çš„ç ”ç©¶å’Œå¼€å‘ï¼Œæ‰©å±•å½“å‰å¯¹æ•°å­¦æ¨ç†çš„å…³æ³¨ï¼Œæ¶µç›–å¤šä¸ªå­¦ç§‘å’Œé¢†åŸŸã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æå‡ºäº†åŸºäºè¿‡ç¨‹çš„è·¨å­¦ç§‘éªŒè¯å™¨ï¼ˆå³BMMR-Verifierï¼‰è¿›è¡Œæ¨ç†è·¯å¾„çš„ç²¾ç¡®å’Œç»†è‡´è¯„ä¼°ã€‚å¯¹24ä¸ªæ¨¡å‹çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼šï¼ˆiï¼‰å³ä½¿åœ¨BMMR-Evalä¸Šï¼Œç°æœ‰é¡¶å°–æ¨¡å‹ä»æœ‰æå‡ç©ºé—´ï¼›ï¼ˆiiï¼‰æ¨ç†æ¨¡å‹åœ¨æŸäº›ç‰¹å®šå­¦ç§‘ä¸Šè¡¨ç°ä¼˜äºLMMsï¼Œä½†å­˜åœ¨å­¦ç§‘åè§ï¼›ï¼ˆiiiï¼‰å¼€æºæ¨¡å‹ä»è½åäºä¸“æœ‰æ¨¡å‹ï¼›ï¼ˆivï¼‰åœ¨BMMR-Trainä¸Šè¿›è¡Œå¾®è°ƒç¼©å°äº†è¿™ä¸€å·®è·ã€‚æ­¤å¤–ï¼Œé€šè¿‡BMMR-Verifierå’Œå…¶ä»–æ·±å…¥çš„åˆ†æç ”ç©¶ï¼Œæ­ç¤ºäº†LMMsåœ¨å¤šå­¦ç§‘æ¨ç†æ–¹é¢æ‰€é¢ä¸´çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬å°†å‘å¸ƒè¿™äº›æ•°æ®é›†ï¼Œå¸Œæœ›æˆ‘ä»¬çš„å·¥ä½œèƒ½ä¸ºç¤¾åŒºæä¾›è§è§£å’Œè´¡çŒ®ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¼•å…¥å¤§è§„æ¨¡åŒè¯­ã€å¤šæ¨¡æ€ã€å¤šå­¦ç§‘æ¨ç†æ•°æ®é›†BMMRï¼Œç”¨äºå¼€å‘å’Œè¯„ä¼°å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰ã€‚</li>
<li>æ•°æ®é›†åŒ…å«å¤šç§é¢˜å‹å’Œæ¥æºï¼Œæ¶µç›–å¹¿æ³›çš„å­¦ç§‘é¢†åŸŸã€‚</li>
<li>æå‡ºåŸºäºè¿‡ç¨‹çš„è·¨å­¦ç§‘éªŒè¯å™¨ï¼ˆBMMR-Verifierï¼‰è¿›è¡Œç²¾ç»†çš„æ¨ç†è·¯å¾„è¯„ä¼°ã€‚</li>
<li>å®éªŒè¡¨æ˜ç°æœ‰é¡¶å°–æ¨¡å‹åœ¨BMMR-Evalä¸Šä»æœ‰æå‡ç©ºé—´ã€‚</li>
<li>æ¨ç†æ¨¡å‹è¡¨ç°å­˜åœ¨å­¦ç§‘åè§ï¼ŒæŸäº›ç‰¹å®šå­¦ç§‘ä¸Šè¡¨ç°è¾ƒå¥½ã€‚</li>
<li>å¼€æºæ¨¡å‹åœ¨æ€§èƒ½ä¸Šä»è½åäºä¸“æœ‰æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.03483">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3aa64b0a4540af2233171e95ba5dec67.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6f8294fb6431538a60a73b589391a8b8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7449f6f9d9d666caa00b7a1cce33cca2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2f4c72028fd56f3c0034104240ad86d2.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="ARF-RLHF-Adaptive-Reward-Following-for-RLHF-through-Emotion-Driven-Self-Supervision-and-Trace-Biased-Dynamic-Optimization"><a href="#ARF-RLHF-Adaptive-Reward-Following-for-RLHF-through-Emotion-Driven-Self-Supervision-and-Trace-Biased-Dynamic-Optimization" class="headerlink" title="ARF-RLHF: Adaptive Reward-Following for RLHF through Emotion-Driven   Self-Supervision and Trace-Biased Dynamic Optimization"></a>ARF-RLHF: Adaptive Reward-Following for RLHF through Emotion-Driven   Self-Supervision and Trace-Biased Dynamic Optimization</h2><p><strong>Authors:YuXuan Zhang</strong></p>
<p>With the rapid advancement of Reinforcement Learning from Human Feedback (RLHF) and autoregressive transformers, state-of-the-art models such as GPT-4.0, DeepSeek R1, and Llama 3.3 increasingly emphasize answer depth and personalization. However, most existing RLHF approaches (e.g., PPO, DPO) still rely on a binary-preference (BT) paradigm, which, while reducing annotation costs, still requires substantial human effort and captures only group-level tendencies rather than individual preferences. To overcome these limitations, we propose Adaptive Reward-Following (ARF), a self-assessment framework that leverages a high-precision emotion analyzer achieving over 70% accuracy on GoEmotions, Sentiment140, and DailyDialog to convert free-form user feedback into continuous preference scores. We further enrich and debias these signals through lightweight data augmentations, including synonym replacement, random trace truncation, and score bias annotation algorithm. A Dynamic Adapter Preference Tracker continuously models evolving user tastes in real time, enabling our novel Trace Bias (TB) fine-tuning algorithm to optimize directly on these tracked rewards instead of coarse binary labels. Experiments on Qwen-2&#x2F;2.5, Gemma-2, and Llama-3.2 across four preference domains demonstrate that ARF achieves an improvement of 3.3% over PPO and 7.6% over DPO. Moreover, TB preserves theoretical alignment with PPO and DPO objectives. Overall, ARF presents a scalable, personalized, and cost-effective approach to RLHF LLMs through autonomous reward modeling. </p>
<blockquote>
<p>éšç€å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰å’Œè‡ªå›å½’å˜å‹å™¨çš„å¿«é€Ÿå‘å±•ï¼Œæœ€å…ˆè¿›çš„æ¨¡å‹å¦‚GPT-4.0ã€DeepSeek R1å’ŒLlama 3.3è¶Šæ¥è¶Šå¼ºè°ƒç­”æ¡ˆçš„æ·±åº¦å’Œä¸ªæ€§åŒ–ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¤§å¤šæ•°RLHFæ–¹æ³•ï¼ˆå¦‚PPOã€DPOï¼‰ä»ç„¶ä¾èµ–äºäºŒå…ƒåå¥½ï¼ˆBTï¼‰èŒƒå¼ï¼Œè™½ç„¶é™ä½äº†æ ‡æ³¨æˆæœ¬ï¼Œä½†ä»éœ€è¦å¤§é‡çš„äººåŠ›æŠ•å…¥ï¼Œå¹¶ä¸”åªèƒ½æ•æ‰ç¾¤ä½“å±‚é¢çš„è¶‹åŠ¿ï¼Œè€Œéä¸ªäººåå¥½ã€‚</p>
</blockquote>
<p>ä¸ºäº†å…‹æœè¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†è‡ªé€‚åº”å¥–åŠ±è·Ÿè¸ªï¼ˆARFï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªè‡ªæˆ‘è¯„ä¼°æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨é«˜ç²¾åº¦æƒ…ç»ªåˆ†æå™¨ï¼ˆåœ¨GoEmotionsã€Sentiment140å’ŒDailyDialogä¸Šçš„å‡†ç¡®ç‡è¶…è¿‡7å®â€ï¼‰ï¼Œå°†è‡ªç”±å½¢å¼çš„ç”¨æˆ·åé¦ˆè½¬åŒ–ä¸ºè¿ç»­åå¥½åˆ†æ•°ã€‚æˆ‘ä»¬è¿˜é€šè¿‡è½»é‡çº§çš„æ•°æ®å¢å¼ºæ¥ä¸°å¯Œå’Œå»åç½®è¿™äº›ä¿¡å·ï¼ŒåŒ…æ‹¬åŒä¹‰è¯æ›¿æ¢ã€éšæœºè½¨è¿¹æˆªæ–­å’Œè¯„åˆ†åå·®æ³¨é‡Šç®—æ³•ã€‚åŠ¨æ€é€‚é…å™¨åå¥½è·Ÿè¸ªå™¨å®æ—¶å»ºæ¨¡ä¸æ–­å˜åŒ–çš„ç”¨æˆ·å“å‘³ï¼Œä½¿æˆ‘ä»¬çš„æ–°å‹è½¨è¿¹åå·®ï¼ˆTBï¼‰å¾®è°ƒç®—æ³•å¯ä»¥ç›´æ¥åœ¨è¿™äº›è·Ÿè¸ªçš„å¥–åŠ±ä¸Šè¿›è¡Œä¼˜åŒ–ï¼Œè€Œä¸æ˜¯ç²—ç•¥çš„äºŒå…ƒæ ‡ç­¾ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.03069v1">PDF</a> Preprint under review</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¼ºåŒ–å­¦ä¹ äººç±»åé¦ˆï¼ˆRLHFï¼‰ã€è‡ªå›å½’å˜å‹å™¨ä»¥åŠå…ˆè¿›æ¨¡å‹å¦‚GPT-4.0ã€DeepSeek R1å’ŒLlama 3.3çš„å¿«é€Ÿå‘å±•ï¼Œå›ç­”çš„æ·±åº¦å’Œä¸ªæ€§åŒ–è¶Šæ¥è¶Šå—åˆ°é‡è§†ã€‚ç„¶è€Œï¼Œç°æœ‰çš„RLHFæ–¹æ³•ä»ç„¶ä¾èµ–äºäºŒå…ƒåå¥½ï¼ˆBTï¼‰æ¨¡å¼ï¼Œéœ€è¦å¤§é‡äººåŠ›å¹¶ä»…æ•æ‰ç¾¤ä½“åå¥½è€Œéä¸ªä½“åå¥½ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†è‡ªé€‚åº”å¥–åŠ±è·Ÿè¸ªï¼ˆARFï¼‰è¿™ä¸€è‡ªæˆ‘è¯„ä¼°æ¡†æ¶ï¼Œåˆ©ç”¨é«˜ç²¾åº¦æƒ…ç»ªåˆ†æå™¨å°†è‡ªç”±å½¢å¼çš„ç”¨æˆ·åé¦ˆè½¬åŒ–ä¸ºè¿ç»­åå¥½åˆ†æ•°ï¼Œå¹¶é€šè¿‡è½»é‡çº§æ•°æ®å¢å¼ºå’ŒåŠ¨æ€é€‚é…å™¨åå¥½è·Ÿè¸ªæŠ€æœ¯æ¥ä¸°å¯Œå’Œå»åè¿™äº›ä¿¡å·ã€‚å®éªŒè¡¨æ˜ï¼ŒARFåœ¨å››ä¸ªåå¥½é¢†åŸŸå‡ä¼˜äºPPOå’ŒDPOï¼Œå®ç°äº†å¯è§„æ¨¡åŒ–ã€ä¸ªæ€§åŒ–å’Œæˆæœ¬æ•ˆç›Šé«˜çš„LLMå¼ºåŒ–å­¦ä¹ äººç±»åé¦ˆæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å…ˆè¿›æ¨¡å‹å¦‚GPT-4.0ç­‰å¼ºè°ƒç­”æ¡ˆçš„æ·±åº¦å’Œä¸ªæ€§åŒ–ã€‚</li>
<li>ç°æœ‰RLHFæ–¹æ³•ä¾èµ–äºŒå…ƒåå¥½æ¨¡å¼ï¼Œéœ€è¦å¤§é‡äººåŠ›å¹¶ä»…æ•æ‰ç¾¤ä½“åå¥½ã€‚</li>
<li>ARFæ¡†æ¶åˆ©ç”¨é«˜ç²¾åº¦æƒ…ç»ªåˆ†æå™¨è½¬åŒ–ç”¨æˆ·åé¦ˆä¸ºè¿ç»­åå¥½åˆ†æ•°ã€‚</li>
<li>é€šè¿‡è½»é‡çº§æ•°æ®å¢å¼ºæŠ€æœ¯ä¸°å¯Œå’Œå»åç”¨æˆ·åé¦ˆä¿¡å·ã€‚</li>
<li>åŠ¨æ€é€‚é…å™¨åå¥½è·Ÿè¸ªæŠ€æœ¯å®æ—¶å»ºæ¨¡ç”¨æˆ·å£å‘³å˜åŒ–ã€‚</li>
<li>ARFé€šè¿‡è‡ªä¸»å¥–åŠ±å»ºæ¨¡å®ç°äº†å¯è§„æ¨¡åŒ–ã€ä¸ªæ€§åŒ–å’Œæˆæœ¬æ•ˆç›Šé«˜çš„RLHFæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.03069">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-fd40a7e7e3c6e602415d5b42793f4437.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-39f5da1e5b645033c29ec969142d5e61.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-98089710525b8dbd83fc7af67e2ff920.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="AIGI-Holmes-Towards-Explainable-and-Generalizable-AI-Generated-Image-Detection-via-Multimodal-Large-Language-Models"><a href="#AIGI-Holmes-Towards-Explainable-and-Generalizable-AI-Generated-Image-Detection-via-Multimodal-Large-Language-Models" class="headerlink" title="AIGI-Holmes: Towards Explainable and Generalizable AI-Generated Image   Detection via Multimodal Large Language Models"></a>AIGI-Holmes: Towards Explainable and Generalizable AI-Generated Image   Detection via Multimodal Large Language Models</h2><p><strong>Authors:Ziyin Zhou, Yunpeng Luo, Yuanchen Wu, Ke Sun, Jiayi Ji, Ke Yan, Shouhong Ding, Xiaoshuai Sun, Yunsheng Wu, Rongrong Ji</strong></p>
<p>The rapid development of AI-generated content (AIGC) technology has led to the misuse of highly realistic AI-generated images (AIGI) in spreading misinformation, posing a threat to public information security. Although existing AIGI detection techniques are generally effective, they face two issues: 1) a lack of human-verifiable explanations, and 2) a lack of generalization in the latest generation technology. To address these issues, we introduce a large-scale and comprehensive dataset, Holmes-Set, which includes the Holmes-SFTSet, an instruction-tuning dataset with explanations on whether images are AI-generated, and the Holmes-DPOSet, a human-aligned preference dataset. Our work introduces an efficient data annotation method called the Multi-Expert Jury, enhancing data generation through structured MLLM explanations and quality control via cross-model evaluation, expert defect filtering, and human preference modification. In addition, we propose Holmes Pipeline, a meticulously designed three-stage training framework comprising visual expert pre-training, supervised fine-tuning, and direct preference optimization. Holmes Pipeline adapts multimodal large language models (MLLMs) for AIGI detection while generating human-verifiable and human-aligned explanations, ultimately yielding our model AIGI-Holmes. During the inference stage, we introduce a collaborative decoding strategy that integrates the model perception of the visual expert with the semantic reasoning of MLLMs, further enhancing the generalization capabilities. Extensive experiments on three benchmarks validate the effectiveness of our AIGI-Holmes. </p>
<blockquote>
<p>äººå·¥æ™ºèƒ½ç”Ÿæˆå†…å®¹ï¼ˆAIGCï¼‰æŠ€æœ¯çš„å¿«é€Ÿå‘å±•å¯¼è‡´äº†é«˜åº¦é€¼çœŸçš„AIç”Ÿæˆå›¾åƒï¼ˆAIGIï¼‰è¢«æ»¥ç”¨ï¼Œç”¨äºä¼ æ’­é”™è¯¯ä¿¡æ¯ï¼Œå¯¹å…¬ä¼—ä¿¡æ¯å®‰å…¨æ„æˆå¨èƒã€‚å°½ç®¡ç°æœ‰çš„AIGIæ£€æµ‹æŠ€æœ¯é€šå¸¸æœ‰æ•ˆï¼Œä½†å®ƒä»¬é¢ä¸´ä¸¤ä¸ªé—®é¢˜ï¼šä¸€æ˜¯ç¼ºä¹å¯éªŒè¯çš„äººä¸ºè§£é‡Šï¼ŒäºŒæ˜¯åœ¨æœ€æ–°æŠ€æœ¯ä¸­ç¼ºä¹æ³›åŒ–èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸¤ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå¤§è§„æ¨¡ä¸”å…¨é¢çš„æ•°æ®é›†Holmes-Setï¼Œå…¶ä¸­åŒ…æ‹¬Holmes-SFTSetï¼ˆä¸€ä¸ªå¸¦æœ‰å…³äºå›¾åƒæ˜¯å¦ç”±AIç”Ÿæˆè§£é‡Šçš„æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†ï¼‰å’ŒHolmes-DPOSetï¼ˆä¸€ä¸ªäººä¸ºå¯¹é½çš„åå¥½æ•°æ®é›†ï¼‰ã€‚æˆ‘ä»¬çš„å·¥ä½œå¼•å…¥äº†ä¸€ç§é«˜æ•ˆçš„æ•°æ®æ ‡æ³¨æ–¹æ³•ï¼Œç§°ä¸ºâ€œå¤šå…ƒä¸“å®¶é™ªå®¡å›¢â€ï¼Œé€šè¿‡ç»“æ„åŒ–çš„MLLMè§£é‡Šå’Œè´¨é‡æ§åˆ¶æ¥å¢å¼ºæ•°æ®ç”Ÿæˆï¼Œè´¨é‡æ§åˆ¶åŒ…æ‹¬è·¨æ¨¡å‹è¯„ä¼°ã€ä¸“å®¶ç¼ºé™·è¿‡æ»¤å’Œäººä¸ºåå¥½ä¿®æ­£ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ç²¾å¿ƒè®¾è®¡çš„ä¸‰é˜¶æ®µè®­ç»ƒæ¡†æ¶Holmes Pipelineï¼ŒåŒ…æ‹¬è§†è§‰ä¸“å®¶é¢„è®­ç»ƒã€ç›‘ç£å¾®è°ƒä»¥åŠç›´æ¥åå¥½ä¼˜åŒ–ã€‚Holmes Pipelineé€‚åº”äºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ç”¨äºæ£€æµ‹AIGIï¼ŒåŒæ—¶ç”Ÿæˆå¯éªŒè¯å’Œäººä¸ºå¯¹é½çš„è§£é‡Šï¼Œæœ€ç»ˆå¾—åˆ°æˆ‘ä»¬çš„æ¨¡å‹AIGI-Holmesã€‚åœ¨æ¨ç†é˜¶æ®µï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ååŒè§£ç ç­–ç•¥ï¼Œå°†è§†è§‰ä¸“å®¶çš„æ¨¡å‹æ„ŸçŸ¥ä¸MLLMsçš„è¯­ä¹‰æ¨ç†ç›¸ç»“åˆï¼Œè¿›ä¸€æ­¥æé«˜äº†æ³›åŒ–èƒ½åŠ›ã€‚åœ¨ä¸‰ä¸ªåŸºå‡†ä¸Šçš„å¤§é‡å®éªŒéªŒè¯äº†æˆ‘ä»¬çš„AIGI-Holmesçš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.02664v2">PDF</a> Accepted to ICCV 2025</p>
<p><strong>Summary</strong><br>     äººå·¥æ™ºèƒ½ç”Ÿæˆå†…å®¹ï¼ˆAIGCï¼‰æŠ€æœ¯çš„å¿«é€Ÿå‘å±•å¯¼è‡´AIç”Ÿæˆå›¾åƒï¼ˆAIGIï¼‰è¢«æ»¥ç”¨ï¼Œä¼ æ’­é”™è¯¯ä¿¡æ¯ï¼Œå¨èƒå…¬ä¼—ä¿¡æ¯å®‰å…¨ã€‚ç°æœ‰AIGIæ£€æµ‹æŠ€æœ¯è™½æ™®éæœ‰æ•ˆï¼Œä½†å­˜åœ¨ç¼ºä¹å¯éªŒè¯çš„äººç±»è§£é‡Šå’Œæ–°æŠ€æœ¯æ™®åŠçš„æ³›åŒ–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œç ”ç©¶å›¢é˜Ÿå¼•å…¥å¤§è§„æ¨¡ç»¼åˆæ•°æ®é›†Holmes-Setï¼ŒåŒ…æ‹¬å¸¦æœ‰å›¾åƒæ˜¯å¦AIç”Ÿæˆè§£é‡Šçš„æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†Holmes-SFTSetå’Œäººç±»å¯¹é½åå¥½æ•°æ®é›†Holmes-DPOSetã€‚è¯¥ç ”ç©¶é‡‡ç”¨å¤šä¸“å®¶è¯„å®¡çš„é«˜æ•ˆæ•°æ®æ ‡æ³¨æ–¹æ³•ï¼Œé€šè¿‡ç»“æ„åŒ–MLLMè§£é‡Šå’Œè´¨é‡æ§åˆ¶æªæ–½æé«˜æ•°æ®ç”Ÿæˆè´¨é‡ã€‚åŒæ—¶æå‡ºHolmes Pipelineä¸‰é˜¶æ®µè®­ç»ƒæ¡†æ¶ï¼ŒåŒ…æ‹¬è§†è§‰ä¸“å®¶é¢„è®­ç»ƒã€ç›‘ç£å¾®è°ƒåŠç›´æ¥åå¥½ä¼˜åŒ–ç­‰ã€‚å®éªŒè¯æ˜ï¼Œæ–°æ¨¡å‹AIGI-Holmesèƒ½æœ‰æ•ˆæ£€æµ‹AIGIå¹¶ç”Ÿæˆå¯éªŒè¯çš„äººç±»å¯¹é½è§£é‡Šï¼ŒåŒæ—¶å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AIç”Ÿæˆå†…å®¹ï¼ˆAIGCï¼‰æŠ€æœ¯å¿«é€Ÿå‘å±•ï¼Œå¯¼è‡´AIç”Ÿæˆå›¾åƒï¼ˆAIGIï¼‰è¢«æ»¥ç”¨ä¼ æ’­é”™è¯¯ä¿¡æ¯ï¼Œå¨èƒå…¬ä¼—ä¿¡æ¯å®‰å…¨ã€‚</li>
<li>ç°æœ‰AIGIæ£€æµ‹æŠ€æœ¯é¢ä¸´ç¼ºä¹å¯éªŒè¯çš„äººç±»è§£é‡Šå’Œæ–°æŠ€æœ¯æ™®åŠçš„æ³›åŒ–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚</li>
<li>ç ”ç©¶å›¢é˜Ÿå¼•å…¥å¤§è§„æ¨¡ç»¼åˆæ•°æ®é›†Holmes-Setï¼ŒåŒ…æ‹¬å¸¦æœ‰è§£é‡Šçš„æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†å’Œäººç±»å¯¹é½åå¥½æ•°æ®é›†ã€‚</li>
<li>é‡‡ç”¨å¤šä¸“å®¶è¯„å®¡çš„æ•°æ®æ ‡æ³¨æ–¹æ³•æé«˜æ•°æ®è´¨é‡ï¼ŒåŒ…æ‹¬ç»“æ„åŒ–MLLMè§£é‡Šå’Œè´¨é‡æ§åˆ¶æªæ–½ã€‚</li>
<li>æå‡ºHolmes Pipelineä¸‰é˜¶æ®µè®­ç»ƒæ¡†æ¶ï¼ŒåŒ…æ‹¬è§†è§‰ä¸“å®¶é¢„è®­ç»ƒã€ç›‘ç£å¾®è°ƒåŠç›´æ¥åå¥½ä¼˜åŒ–ç­‰ã€‚</li>
<li>AIGIæ£€æµ‹æ¨¡å‹ç”Ÿæˆçš„äººç±»å¯¹é½è§£é‡Šå…·æœ‰å¯éªŒè¯æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.02664">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4e80e266fadd191fd678cb20a98c7f6f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-36a2d26239472c644959493de853681e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-555ab8b2cdca9007b241b61c16d388c2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-42a0f3ca8b6587a61ff2f5bad2640090.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cd7ea3ea1f82521b7ce150fefa86af9e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-59e647b3c4d7e56df48c7b62475a98fc.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Eka-Eval-A-Comprehensive-Evaluation-Framework-for-Large-Language-Models-in-Indian-Languages"><a href="#Eka-Eval-A-Comprehensive-Evaluation-Framework-for-Large-Language-Models-in-Indian-Languages" class="headerlink" title="Eka-Eval : A Comprehensive Evaluation Framework for Large Language   Models in Indian Languages"></a>Eka-Eval : A Comprehensive Evaluation Framework for Large Language   Models in Indian Languages</h2><p><strong>Authors:Samridhi Raj Sinha, Rajvee Sheth, Abhishek Upperwal, Mayank Singh</strong></p>
<p>The rapid advancement of Large Language Models (LLMs) has intensified the need for evaluation frameworks that address the requirements of linguistically diverse regions, such as India, and go beyond English-centric benchmarks. We introduce EKA-EVAL, a unified evaluation framework that integrates over 35+ benchmarks (including 10 Indic benchmarks) across nine major evaluation categories. The framework provides broader coverage than existing Indian language evaluation tools, offering 11 core capabilities through a modular architecture, seamless integration with Hugging Face and proprietary models, and plug-and-play usability. As the first end-to-end suite for scalable, multilingual LLM benchmarking, the framework combines extensive benchmarks, modular workflows, and dedicated support for low-resource Indian languages to enable inclusive assessment of LLM capabilities across diverse domains. We conducted extensive comparisons against five existing baselines, demonstrating that EKA-EVAL achieves the highest participant ratings in four out of five categories. The framework is open-source and publicly available at: <a target="_blank" rel="noopener" href="https://github.com/lingo-iitgn/eka-eval">https://github.com/lingo-iitgn/eka-eval</a>. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œéœ€è¦å¼ºåŒ–è¯„ä¼°æ¡†æ¶çš„éœ€æ±‚ï¼Œä»¥æ»¡è¶³å°åº¦ç­‰è¯­è¨€å¤šæ ·åŒ–çš„åœ°åŒºéœ€æ±‚ï¼Œå¹¶è¶…è¶Šä»¥è‹±è¯­ä¸ºä¸­å¿ƒçš„åŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬ä»‹ç»äº†EKA-EVALè¿™ä¸€ç»Ÿä¸€è¯„ä¼°æ¡†æ¶ï¼Œå®ƒé›†æˆäº†è¶…è¿‡35é¡¹åŸºå‡†æµ‹è¯•ï¼ˆåŒ…æ‹¬10é¡¹å°åº¦è¯­è¨€åŸºå‡†æµ‹è¯•ï¼‰ï¼Œæ¶µç›–ä¹å¤§ä¸»è¦è¯„ä¼°ç±»åˆ«ã€‚è¯¥æ¡†æ¶æä¾›äº†æ¯”ç°æœ‰å°åº¦è¯­è¨€è¯„ä¼°å·¥å…·æ›´å¹¿æ³›çš„è¦†ç›–èŒƒå›´ï¼Œé€šè¿‡æ¨¡å—åŒ–æ¶æ„æä¾›11é¡¹æ ¸å¿ƒèƒ½åŠ›ï¼Œä¸Hugging Faceå’Œä¸“æœ‰æ¨¡å‹æ— ç¼é›†æˆï¼Œä»¥åŠå³æ’å³ç”¨çš„æ˜“ç”¨æ€§ã€‚ä½œä¸ºé¦–ä¸ªç«¯åˆ°ç«¯ã€å¯ä¼¸ç¼©çš„å¤šè¯­ç§LLMåŸºå‡†æµ‹è¯•å¥—ä»¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†å¹¿æ³›çš„åŸºå‡†æµ‹è¯•ã€æ¨¡å—åŒ–å·¥ä½œæµç¨‹ä»¥åŠå¯¹ä½èµ„æºå°åº¦è¯­è¨€çš„ä¸“é¡¹æ”¯æŒï¼Œä»¥å®ç°å¯¹ä¸åŒé¢†åŸŸLLMèƒ½åŠ›çš„å…¨é¢è¯„ä¼°ã€‚æˆ‘ä»¬ä¸äº”ä¸ªç°æœ‰åŸºçº¿è¿›è¡Œäº†å¹¿æ³›æ¯”è¾ƒï¼Œç»“æœè¡¨æ˜ï¼Œåœ¨äº”ä¸ªç±»åˆ«ä¸­ï¼ŒEKA-EVALåœ¨å››ä¸ªç±»åˆ«ä¸­è·å¾—äº†æœ€é«˜å‚ä¸è€…è¯„åˆ†ã€‚è¯¥æ¡†æ¶æ˜¯å¼€æºçš„ï¼Œå¯åœ¨ä»¥ä¸‹ç½‘å€å…¬å¼€è·å–ï¼š<a target="_blank" rel="noopener" href="https://github.com/lingo-iitgn/eka-eval">https://github.com/lingo-iitgn/eka-eval</a> ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01853v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•åŠ å‰§äº†å¯¹è¯„ä¼°æ¡†æ¶çš„éœ€æ±‚ï¼Œè¿™äº›è¯„ä¼°æ¡†æ¶éœ€è¦æ»¡è¶³è¯­è¨€å¤šæ ·åŒ–åœ°åŒºï¼ˆå¦‚å°åº¦ï¼‰çš„è¦æ±‚ï¼Œå¹¶è¶…è¶Šè‹±è¯­ä¸­å¿ƒçš„åŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬ä»‹ç»äº†EKA-EVALï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€è¯„ä¼°æ¡†æ¶ï¼Œæ•´åˆäº†35ä¸ªä»¥ä¸Šçš„åŸºå‡†æµ‹è¯•ï¼ˆåŒ…æ‹¬10ä¸ªå°åº¦è¯­è¨€åŸºå‡†æµ‹è¯•ï¼‰ï¼Œæ¶µç›–ä¹å¤§ä¸»è¦è¯„ä¼°ç±»åˆ«ã€‚è¯¥æ¡†æ¶æä¾›æ¯”ç°æœ‰å°åº¦è¯­è¨€è¯„ä¼°å·¥å…·æ›´å¹¿æ³›çš„è¦†ç›–ï¼Œé€šè¿‡æ¨¡å—åŒ–æ¶æ„æä¾›11é¡¹æ ¸å¿ƒèƒ½åŠ›ï¼Œä¸Hugging Faceå’Œä¸“æœ‰æ¨¡å‹æ— ç¼é›†æˆï¼Œä»¥åŠå³æ’å³ç”¨çš„æ˜“ç”¨æ€§ã€‚ä½œä¸ºé¦–ä¸ªç«¯åˆ°ç«¯ã€å¯ä¼¸ç¼©ã€å¤šè¯­è¨€LLMåŸºå‡†æµ‹è¯•å¥—ä»¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†å¹¿æ³›çš„åŸºå‡†æµ‹è¯•ã€æ¨¡å—åŒ–å·¥ä½œæµç¨‹ä»¥åŠå¯¹ä½èµ„æºå°åº¦è¯­è¨€çš„ä¸“é—¨æ”¯æŒï¼Œä»¥å®ç°å¯¹LLMåœ¨å„ç§é¢†åŸŸèƒ½åŠ›çš„åŒ…å®¹æ€§è¯„ä¼°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>EKA-EVALæ˜¯ä¸€ä¸ªç»Ÿä¸€è¯„ä¼°æ¡†æ¶ï¼Œæ»¡è¶³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¯„ä¼°éœ€æ±‚ã€‚</li>
<li>è¯¥æ¡†æ¶è€ƒè™‘åˆ°äº†è¯­è¨€å¤šæ ·æ€§ï¼Œç‰¹åˆ«åŒ…æ‹¬å°åº¦è¯­è¨€ã€‚</li>
<li>EKA-EVALæ•´åˆäº†è¶…è¿‡35ä¸ªåŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–ä¹å¤§è¯„ä¼°ç±»åˆ«ã€‚</li>
<li>ç›¸æ¯”å…¶ä»–å°åº¦è¯­è¨€è¯„ä¼°å·¥å…·ï¼ŒEKA-EVALæä¾›æ›´å¹¿æ³›çš„è¦†ç›–ã€‚</li>
<li>æ¡†æ¶å…·æœ‰æ¨¡å—åŒ–æ¶æ„ã€æ˜“äºä¸Hugging Faceå’Œä¸“æœ‰æ¨¡å‹é›†æˆç­‰ç‰¹ç‚¹ã€‚</li>
<li>EKA-EVALåœ¨äº”ä¸ªç±»åˆ«ä¸­çš„å››ä¸ªè·å¾—äº†æœ€é«˜å‚ä¸è€…è¯„åˆ†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01853">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3db2ef75fb9a5287429658512a44f9dd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-51681274fc6459c1fc6ef0a8d3d8ee9a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5c4fdda0376711e17333cca53334a489.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-07-10/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-07-10/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-10/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-2c0960bebc3e38df91e408a734247a84.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-10  A Survey on Latent Reasoning
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-07-10
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-09/Text-to-Motion/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-1b3485d13f6b6b2da8f6300b80003be8.jpg" class="responsive-img" alt="Text-to-Motion">
                        
                        <span class="card-title">Text-to-Motion</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Text-to-Motion æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-09  Spatio-Temporal Control for Masked Motion Synthesis
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-07-09
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Text-to-Motion/" class="post-category">
                                    Text-to-Motion
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Text-to-Motion/">
                        <span class="chip bg-color">Text-to-Motion</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">29774.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
