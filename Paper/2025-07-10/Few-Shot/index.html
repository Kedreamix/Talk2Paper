<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot 方向最新论文已更新，请持续关注 Update in 2025-07-10  DS@GT at CheckThat! 2025 Ensemble Methods for Detection of Scientific   Discourse on Social Media">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-1586eec3e5508d29f64965bd4e6087ff.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-07-10
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-08-19
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    28 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-07-10-更新"><a href="#2025-07-10-更新" class="headerlink" title="2025-07-10 更新"></a>2025-07-10 更新</h1><h2 id="DS-GT-at-CheckThat-2025-Ensemble-Methods-for-Detection-of-Scientific-Discourse-on-Social-Media"><a href="#DS-GT-at-CheckThat-2025-Ensemble-Methods-for-Detection-of-Scientific-Discourse-on-Social-Media" class="headerlink" title="DS@GT at CheckThat! 2025: Ensemble Methods for Detection of Scientific   Discourse on Social Media"></a>DS@GT at CheckThat! 2025: Ensemble Methods for Detection of Scientific   Discourse on Social Media</h2><p><strong>Authors:Ayush Parikh, Hoang Thanh Thanh Truong, Jeanette Schofield, Maximilian Heil</strong></p>
<p>In this paper, we, as the DS@GT team for CLEF 2025 CheckThat! Task 4a Scientific Web Discourse Detection, present the methods we explored for this task. For this multiclass classification task, we determined if a tweet contained a scientific claim, a reference to a scientific study or publication, and&#x2F;or mentions of scientific entities, such as a university or a scientist. We present 3 modeling approaches for this task: transformer finetuning, few-shot prompting of LLMs, and a combined ensemble model whose design was informed by earlier experiments. Our team placed 7th in the competition, achieving a macro-averaged F1 score of 0.8611, an improvement over the DeBERTaV3 baseline of 0.8375. Our code is available on Github at <a target="_blank" rel="noopener" href="https://github.com/dsgt-arc/checkthat-2025-swd/tree/main/subtask-4a">https://github.com/dsgt-arc/checkthat-2025-swd/tree/main/subtask-4a</a>. </p>
<blockquote>
<p>在这篇论文中，我们作为CLEF 2025 CheckThat!任务4a科学网络话语检测的DS@GT团队，介绍了我们为这项任务所探索的方法。针对这项多类分类任务，我们确定了推特中是否包含科学主张、对科学研究或出版的引用，以及是否提及科学实体，如大学或科学家。我们为这项任务提出了三种建模方法：变压器微调、大型语言模型的少量提示，以及根据早期实验设计的组合集成模型。我们的团队在比赛中排名第7，取得了宏观平均F1分数为0.8611的成绩，相较于DeBERTaV3基准模型的0.8375有所提升。我们的代码可在Github上获取：<a target="_blank" rel="noopener" href="https://github.com/dsgt-arc/checkthat-2025-swd/tree/main/subtask-4a">链接</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.06205v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了DS@GT团队在CLEF 2025 CheckThat!任务4a科学网络话语检测中所探索的方法。针对多类分类任务，我们确定推特中是否包含科学声明、对科学研究的引用、以及提及科学实体（如大学或科学家）。我们为此任务提出了三种建模方法：微调转换器、LLM的少样本提示和结合早期实验的集成模型设计。我们的团队在比赛中获得第七名，宏观平均F1分数为0.8611，较DeBERTaV3基准的0.8375有所提高。代码可在Github上找到。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>介绍了团队在检测科学网络话语的多类分类任务中所采用的方法。</li>
<li>确定了推特中是否包含科学声明、对科学研究的引用以及提及的科学实体。</li>
<li>提出了三种建模方法：微调转换器、LLM的少样本提示和集成模型设计。</li>
<li>团队在比赛中获得第七名，宏观平均F1分数为0.8611。</li>
<li>与DeBERTaV3基准相比，取得了F1分数的改进。</li>
<li>代码已在Github上公开。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.06205">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-1586eec3e5508d29f64965bd4e6087ff.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d682ea7762a7c221920e3972930efc09.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c1eb29f40e781ab1ca2e08653a2e88e2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b2b58ae161d4b07441de0c01403267a.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="DocIE-XLLM25-In-Context-Learning-for-Information-Extraction-using-Fully-Synthetic-Demonstrations"><a href="#DocIE-XLLM25-In-Context-Learning-for-Information-Extraction-using-Fully-Synthetic-Demonstrations" class="headerlink" title="DocIE@XLLM25: In-Context Learning for Information Extraction using Fully   Synthetic Demonstrations"></a>DocIE@XLLM25: In-Context Learning for Information Extraction using Fully   Synthetic Demonstrations</h2><p><strong>Authors:Nicholas Popovič, Ashish Kangen, Tim Schopf, Michael Färber</strong></p>
<p>Large, high-quality annotated corpora remain scarce in document-level entity and relation extraction in zero-shot or few-shot settings. In this paper, we present a fully automatic, LLM-based pipeline for synthetic data generation and in-context learning for document-level entity and relation extraction. In contrast to existing approaches that rely on manually annotated demonstrations or direct zero-shot inference, our method combines synthetic data generation with retrieval-based in-context learning, using a reasoning-optimized language model. This allows us to build a high-quality demonstration database without manual annotation and to dynamically retrieve relevant examples at inference time. Based on our approach we produce a synthetic dataset of over $5k$ Wikipedia abstracts with approximately $59k$ entities and $30k$ relation triples. Finally, we evaluate in-context learning performance on the DocIE shared task, extracting entities and relations from long documents in a zero-shot setting. We find that in-context joint entity and relation extraction at document-level remains a challenging task, even for state-of-the-art large language models. </p>
<blockquote>
<p>在零样本或少样本环境下，文档级别的实体和关系抽取仍然缺乏大规模的高质量标注语料库。在本文中，我们提出了一种基于大型语言模型的全自动合成数据生成和上下文学习管道，用于文档级别的实体和关系抽取。与依赖手动标注演示或直接零样本推理的现有方法不同，我们的方法结合了合成数据生成和基于检索的上下文学习，并使用优化推理的语言模型。这使我们能够建立无需手动注释的高质量演示数据库，并在推理时动态检索相关示例。基于我们的方法，我们生成了一个包含超过5k个维基百科摘要的合成数据集，其中包含大约5.9万个实体和3万个关系三元组。最后，我们在DocIE共享任务上评估了上下文学习效果，该任务是在零样本设置下从长文档中抽取实体和关系。我们发现即使在最先进的大型语言模型面前，文档级别的上下文联合实体和关系抽取仍然是一项具有挑战性的任务。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.05997v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种全自动的、基于大型语言模型（LLM）的合成数据生成和上下文学习管道，用于文档级别的实体和关系提取。该方法结合了合成数据生成和基于检索的上下文学习，无需手动注释即可构建高质量的演示数据库，并在推理时动态检索相关示例。基于该方法，我们制作了一个包含超过5k篇Wikipedia摘要的合成数据集，其中包含约59k个实体和30k个关系三元组。最后，我们在DocIE共享任务上评估了上下文学习的性能，该任务是在零样本设置下从长文档中提取实体和关系。我们发现，即使在最先进的大型语言模型面前，文档级别的联合实体和关系提取仍然是一项具有挑战性的任务。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>介绍了全自动、基于LLM的合成数据生成和上下文学习管道，用于文档级别的实体和关系提取。</li>
<li>结合了合成数据生成和基于检索的上下文学习，以构建高质量的演示数据库，并在推理时动态检索相关示例。</li>
<li>制作了一个包含超过5k篇Wikipedia摘要的合成数据集，包含约59k个实体和30k个关系三元组。</li>
<li>在DocIE共享任务上评估了上下文学习的性能，发现文档级别的联合实体和关系提取具有挑战性。</li>
<li>该方法允许在没有手动注释的情况下构建演示数据库。</li>
<li>所提出的方法在零样本设置下具有良好的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.05997">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-efbf0f193946b58ec4e3159481bebdf1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3ca48d616f9785fe529929040b963142.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b603d9876eabf4bf772a93bf0bb5abd6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-14020fd9d430513937b6eb7e2372c40c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-66d5af970dae7e51c4d551c7ca0f0947.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="I-2-R-Inter-and-Intra-image-Refinement-in-Few-Shot-Segmentation"><a href="#I-2-R-Inter-and-Intra-image-Refinement-in-Few-Shot-Segmentation" class="headerlink" title="I$^2$R: Inter and Intra-image Refinement in Few Shot Segmentation"></a>I$^2$R: Inter and Intra-image Refinement in Few Shot Segmentation</h2><p><strong>Authors:Ourui Fu, Hangzhou He, Xinliang Zhang, Lei Zhu, Shuang Zeng, ZhaoHeng Xie, Yanye Lu</strong></p>
<p>The annotation bottleneck in semantic segmentation has driven significant interest in few-shot segmentation, which aims to develop segmentation models capable of generalizing rapidly to novel classes using minimal exemplars. Conventional training paradigms typically generate query prior maps by extracting masked-area features from support images, followed by making predictions guided by these prior maps. However, current approaches remain constrained by two critical limitations stemming from inter- and intra-image discrepancies, both of which significantly degrade segmentation performance: 1) The semantic gap between support and query images results in mismatched features and inaccurate prior maps; 2) Visually similar yet semantically distinct regions within support or query images lead to false negative or false positive predictions. We propose a novel FSS method called \textbf{I$^2$R}: 1) Using category-specific high level representations which aggregate global semantic cues from support and query images, enabling more precise inter-image region localization and address the first limitation. 2) Directional masking strategy that suppresses inconsistent support-query pixel pairs, which exhibit high feature similarity but conflicting mask, to mitigate the second issue. Experiments demonstrate that our method outperforms state-of-the-art approaches, achieving improvements of 1.9% and 2.1% in mIoU under the 1-shot setting on PASCAL-5$^i$ and COCO-20$^i$ benchmarks, respectively. </p>
<blockquote>
<p>语义分割中的标注瓶颈引发了少量拍摄分割（few-shot segmentation）的极大兴趣。少量拍摄分割的目标是开发能够利用少量样本快速推广到新的类别的分割模型。传统的训练范式通常通过从支持图像中提取掩盖区域特征来生成查询先验图，然后通过这些先验图进行预测。然而，当前的方法仍然受到两个关键限制的影响，这些限制源于图像内部和图像之间的差异性，两者都会显著影响分割性能：（1）支持图像和查询图像之间的语义差距导致特征不匹配和不准确的先验图；（2）在支持图像或查询图像中视觉上相似但语义上不同的区域导致假阴性或假阳性预测。我们提出了一种新的FSS方法，称为I$^2$R：通过利用特定类别的高级表示形式来聚合来自支持图像和查询图像的全局语义线索，从而实现更精确的区域间定位并解决第一个限制。通过采用定向掩蔽策略来抑制不一致的支持-查询像素对（这些像素对表现出较高的特征相似性但存在冲突的掩蔽），从而缓解第二个问题。实验表明，我们的方法在PASCAL-5$^i$和COCO-20$^i$基准测试下的单镜头设置中，相较于最新方法提高了mIoU指标，分别提高了1.9％和2.1％。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.05838v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了少样本语义分割（Few-Shot Segmentation）领域的问题和挑战。针对现有方法的两个关键局限性——跨图像语义差距和图像内视觉相似但语义不同的区域，提出了一种新的方法I$^2$R。该方法利用类别特定的高级表示和方向性掩模策略来分别解决这两个问题，实现了更精确的跨图像区域定位和减少误判。实验结果表明，该方法在PASCAL-5$^i$和COCO-20$^i$基准测试中，相对于现有方法提高了1.9%和2.1%的mIoU。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>现有方法在语义分割的标注瓶颈上促使了对少样本分割（Few-Shot Segmentation）的关注，目标是使用少量样本快速推广到新的类别。</li>
<li>传统训练模式通过从支持图像中提取掩码区域特征生成查询先验图，但在实际应用中存在两个关键局限性。</li>
<li>第一个局限性是跨图像语义差距导致的特征不匹配和不准确的先验图。本文提出了使用类别特定的高级表示来解决这个问题。</li>
<li>第二个局限性是图像内视觉上相似但语义不同的区域导致误判。为此，本文引入了方向性掩模策略来减少不一致的支持-查询像素对的干扰。</li>
<li>该方法名为I$^2$R，在PASCAL-5$^i$和COCO-20$^i$基准测试中表现优异，相对于现有方法提升了mIoU得分。</li>
<li>I$^2$R方法通过更精确的跨图像区域定位和减少误判，解决了传统方法的局限性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.05838">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2bf906e21815b3920111b59df1ab147b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-56c9c5882e7eb275cecc9d54affb2a4a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d5abb28bd2780cd916e67feac98934d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-34c91c764ddad2c1c64aaffa57d91ddd.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="SenseCF-LLM-Prompted-Counterfactuals-for-Intervention-and-Sensor-Data-Augmentation"><a href="#SenseCF-LLM-Prompted-Counterfactuals-for-Intervention-and-Sensor-Data-Augmentation" class="headerlink" title="SenseCF: LLM-Prompted Counterfactuals for Intervention and Sensor Data   Augmentation"></a>SenseCF: LLM-Prompted Counterfactuals for Intervention and Sensor Data   Augmentation</h2><p><strong>Authors:Shovito Barua Soumma, Asiful Arefeen, Stephanie M. Carpenter, Melanie Hingle, Hassan Ghasemzadeh</strong></p>
<p>Counterfactual explanations (CFs) offer human-centric insights into machine learning predictions by highlighting minimal changes required to alter an outcome. Therefore, CFs can be used as (i) interventions for abnormality prevention and (ii) augmented data for training robust models. In this work, we explore large language models (LLMs), specifically GPT-4o-mini, for generating CFs in a zero-shot and three-shot setting. We evaluate our approach on two datasets: the AI-Readi flagship dataset for stress prediction and a public dataset for heart disease detection. Compared to traditional methods such as DiCE, CFNOW, and NICE, our few-shot LLM-based approach achieves high plausibility (up to 99%), strong validity (up to 0.99), and competitive sparsity. Moreover, using LLM-generated CFs as augmented samples improves downstream classifier performance (an average accuracy gain of 5%), especially in low-data regimes. This demonstrates the potential of prompt-based generative techniques to enhance explainability and robustness in clinical and physiological prediction tasks. Code base: github.com&#x2F;anonymous&#x2F;SenseCF. </p>
<blockquote>
<p>因果解释（CFs）通过强调改变结果所需的最小变化，为机器学习预测提供了以人类为中心的见解。因此，因果解释可用于（i）异常预防干预和（ii）增强数据以训练稳健模型。在这项工作中，我们探索了大规模语言模型（LLM），特别是GPT-4o-mini，在零击和三次射击环境中生成因果解释。我们在两个数据集上评估了我们的方法：用于压力预测的AI-Readi旗舰数据集和用于心脏病检测公开数据集。与传统的DiCE、CFNOW和NICE等方法相比，我们基于少量数据的LLM方法实现了高达99%的可信度和高达0.99的强烈有效性，并且稀疏性具有竞争力。此外，使用LLM生成的因果解释作为增强样本提高了下游分类器的性能（平均准确率提高5%），特别是在数据较少的情况下。这证明了基于提示的生成技术增强临床和生理预测任务的可解释性和稳健性的潜力。代码库：github.com&#x2F;anonymous&#x2F;SenseCF。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.05541v1">PDF</a> In review</p>
<p><strong>Summary</strong></p>
<p>基于文本描述，该研究探讨了使用大型语言模型（LLMs）生成反事实解释（CFs）的方法，用于提升机器学习的预测解释性和模型的稳健性。研究通过GPT-4o-mini模型在零样本和三样本环境下生成CFs，并在两个数据集上评估了方法的性能。与传统方法相比，该研究的方法实现了高可信度、有效性和竞争力强的稀疏性。此外，使用LLM生成的CFs作为增强样本能够提高下游分类器的性能，特别是在数据较少的情况下。这显示了提示生成技术在临床和生理预测任务中增强解释性和稳健性的潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>反事实解释（CFs）能够通过强调改变结果所需的最小变化来提供关于机器学习预测的人性化见解。</li>
<li>CFs可以用作（i）异常预防的干预措施和（ii）训练稳健模型时的增强数据。</li>
<li>研究采用大型语言模型（LLMs）生成CFs，并在零样本和三样本环境下进行尝试。</li>
<li>在两个数据集上评估了该方法，实现了高可信度、有效性和竞争力强的稀疏性。</li>
<li>与传统方法相比，LLM生成CFs的方法表现出优势。</li>
<li>使用LLM生成的CFs作为增强样本能够提高下游分类器的性能，尤其在数据较少时效果更显著。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.05541">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-a74ea596c1b036968569d0b073c331ec.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e961d7609386aa324c37fc1fa390e366.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d938b0979367a13284a99ff703768452.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d70b478fe7285f6db6e41fa5cc8bc7ae.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a9cfb6c897717b250e230da29ceea426.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="pFedMMA-Personalized-Federated-Fine-Tuning-with-Multi-Modal-Adapter-for-Vision-Language-Models"><a href="#pFedMMA-Personalized-Federated-Fine-Tuning-with-Multi-Modal-Adapter-for-Vision-Language-Models" class="headerlink" title="pFedMMA: Personalized Federated Fine-Tuning with Multi-Modal Adapter for   Vision-Language Models"></a>pFedMMA: Personalized Federated Fine-Tuning with Multi-Modal Adapter for   Vision-Language Models</h2><p><strong>Authors:Sajjad Ghiasvand, Mahnoosh Alizadeh, Ramtin Pedarsani</strong></p>
<p>Vision-Language Models (VLMs) like CLIP have demonstrated remarkable generalization in zero- and few-shot settings, but adapting them efficiently to decentralized, heterogeneous data remains a challenge. While prompt tuning has emerged as a popular parameter-efficient approach in personalized federated learning, existing methods often sacrifice generalization in favor of personalization, struggling particularly on unseen classes or domains. In this work, we propose pFedMMA, the first personalized federated learning framework that leverages multi-modal adapters for vision-language tasks. Each adapter contains modality-specific up- and down-projection layers alongside a globally shared projection that aligns cross-modal features. Our asymmetric optimization strategy allows clients to locally adapt to personalized data distributions while collaboratively training the shared projection to improve global generalization. This design is also communication-efficient, as only the shared component is exchanged during rounds. Through extensive experiments across eleven datasets, including domain- and label-shift scenarios, we show that pFedMMA achieves state-of-the-art trade-offs between personalization and generalization, outperforming recent federated prompt tuning methods. The code is available at <a target="_blank" rel="noopener" href="https://github.com/sajjad-ucsb/pFedMMA">https://github.com/sajjad-ucsb/pFedMMA</a>. </p>
<blockquote>
<p>视觉语言模型（如CLIP）在零样本和少样本设置下已经展现出显著的泛化能力，但在分布式、异构数据上有效地适应它们仍然是一个挑战。虽然提示调整已成为个性化联邦学习中的参数高效方法的热门选择，但现有方法往往牺牲泛化性以换取个性化，尤其是在未见类别或领域上表现挣扎。在这项工作中，我们提出了pFedMMA，这是第一个利用多模态适配器的个性化联邦学习框架，用于视觉语言任务。每个适配器包含模态特定的上下投影层以及全局共享投影，以对齐跨模态特征。我们的不对称优化策略允许客户端本地适应个性化数据分布，同时协作训练共享投影以提高全局泛化能力。这种设计还具有通信效率，因为仅在几轮中交换共享组件。通过包括领域和标签偏移场景在内的十一个数据集的广泛实验，我们证明了pFedMMA在个性化和泛化之间达到了最先进的权衡，并超越了最近的联邦提示调整方法。代码可在<a target="_blank" rel="noopener" href="https://github.com/sajjad-ucsb/pFedMMA%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/sajjad-ucsb/pFedMMA找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.05394v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>VLMs如CLIP在零样本和少样本设置下展现出卓越的泛化能力，但在去中心化、异质数据的适应效率上仍面临挑战。现有方法常牺牲泛化能力以换取个性化，特别是在未见类别或领域上表现欠佳。本文提出pFedMMA，首个针对视觉语言任务利用多模态适配器的个性化联邦学习框架。pFedMMA设计有模态特定上下投影层及全局共享投影以对齐跨模态特征。不对称优化策略使客户端可适应个性化数据分布，同时训练共享投影以提高全局泛化。框架在11个数据集上的实验证明，pFedMMA在个性化与泛化间达到最优平衡，超越现有联邦提示调整方法。代码已公开。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VLMs（如CLIP）在零样本和少样本环境下具有良好的泛化性能。</li>
<li>适应去中心化和异质数据对VLMs仍是挑战。</li>
<li>现有方法牺牲泛化能力以实现个性化，特别是在未见类别或领域上。</li>
<li>pFedMMA是首个结合多模态适配器的个性化联邦学习框架。</li>
<li>pFedMMA包含模态特定上下投影层及全局共享投影设计。</li>
<li>pFedMMA采用不对称优化策略，实现个性化数据适应和全局泛化的平衡。</li>
<li>pFedMMA在多个数据集上实现个性化与泛化的最佳平衡，代码已公开。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.05394">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-4288aca987773dbd15eb0a5b8712d8b1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bd3571103cd1f279f2b0a6842f0bdf5a.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Causal-Foundation-Models-Disentangling-Physics-from-Instrument-Properties"><a href="#Causal-Foundation-Models-Disentangling-Physics-from-Instrument-Properties" class="headerlink" title="Causal Foundation Models: Disentangling Physics from Instrument   Properties"></a>Causal Foundation Models: Disentangling Physics from Instrument   Properties</h2><p><strong>Authors:Jeroen Audenaert, Daniel Muthukrishna, Paul F. Gregory, David W. Hogg, V. Ashley Villar</strong></p>
<p>Foundation models for structured time series data must contend with a fundamental challenge: observations often conflate the true underlying physical phenomena with systematic distortions introduced by measurement instruments. This entanglement limits model generalization, especially in heterogeneous or multi-instrument settings. We present a causally-motivated foundation model that explicitly disentangles physical and instrumental factors using a dual-encoder architecture trained with structured contrastive learning. Leveraging naturally occurring observational triplets (i.e., where the same target is measured under varying conditions, and distinct targets are measured under shared conditions) our model learns separate latent representations for the underlying physical signal and instrument effects. Evaluated on simulated astronomical time series designed to resemble the complexity of variable stars observed by missions like NASA’s Transiting Exoplanet Survey Satellite (TESS), our method significantly outperforms traditional single-latent space foundation models on downstream prediction tasks, particularly in low-data regimes. These results demonstrate that our model supports key capabilities of foundation models, including few-shot generalization and efficient adaptation, and highlight the importance of encoding causal structure into representation learning for structured data. </p>
<blockquote>
<p>结构时间序列数据的基础模型面临一个基本挑战：观测结果往往将真正的底层物理现象与测量仪器引入的系统扭曲混淆在一起。这种纠缠限制了模型的泛化能力，特别是在异构或多仪器环境中。我们提出了一种因果驱动的基础模型，该模型使用双编码器架构进行明确的结构对比学习，以分离物理和仪器因素。通过利用自然发生的观测三元组（即在变化条件下对同一目标进行测量，以及在共享条件下对不同目标进行测量），我们的模型学会了对底层物理信号和仪器效果的单独潜在表示。在模拟的天文时间序列上进行了评估，该模拟设计旨在模仿由美国宇航局的凌星外行星勘测卫星（TESS）等任务观察到的变星的复杂性，我们的方法在下游预测任务上显著优于传统的单一潜在空间基础模型，特别是在数据稀缺的情况下。这些结果证明了我们的模型支持基础模型的关键功能，包括少量样本的泛化和高效适应，并强调了将因果结构编码到结构化数据表示学习中的重要性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.05333v1">PDF</a> 8 pages, 5 figures. Accepted to the ICML 2025 Foundation Models for   Structured Data Workshop and accepted to the Machine Learning for   Astrophysics Workshop 2025</p>
<p><strong>Summary</strong></p>
<p>本文提出一种因果驱动的基础模型，用于显式地解开结构化时间序列数据中物理和仪器因素之间的纠缠。该模型采用双编码器架构，并利用结构化对比学习进行训练。通过利用自然观测的三元组，模型能够学习底层物理信号和仪器效应的单独潜在表示。在模拟的天文时间序列数据上的评估表明，该方法在下游预测任务上显著优于传统单潜在空间基础模型，特别是在数据稀缺的情况下。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>现有基础模型在处理结构化时间序列数据时面临挑战，因为观测结果通常将底层物理现象与仪器引入的系统性扭曲混淆在一起。</li>
<li>提出的因果驱动基础模型使用双编码器架构，能够显式地解开物理和仪器因素，从而提高模型的泛化能力。</li>
<li>该模型利用结构化对比学习进行训练，通过自然观测的三元组学习底层物理信号和仪器效应的单独潜在表示。</li>
<li>模型在模拟的天文时间序列数据上的表现优于传统模型，特别是在数据稀缺的情况下。</li>
<li>该研究强调了因果结构在结构化数据表示学习中的重要性。</li>
<li>模型的应用潜力不仅限于天文学领域，还可应用于其他需要处理复杂时间序列数据的领域。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.05333">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-4af8a1622c9d86882f73a2f2c2b217e7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7669f2d499929a1085a7b0373f798c17.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f3fb8fa9100f07c5a28b10a2c9132518.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Efficient-Detection-of-Intermittent-Job-Failures-Using-Few-Shot-Learning"><a href="#Efficient-Detection-of-Intermittent-Job-Failures-Using-Few-Shot-Learning" class="headerlink" title="Efficient Detection of Intermittent Job Failures Using Few-Shot Learning"></a>Efficient Detection of Intermittent Job Failures Using Few-Shot Learning</h2><p><strong>Authors:Henri Aïdasso, Francis Bordeleau, Ali Tizghadam</strong></p>
<p>One of the main challenges developers face in the use of continuous integration (CI) and deployment pipelines is the occurrence of intermittent job failures, which result from unexpected non-deterministic issues (e.g., flaky tests or infrastructure problems) rather than regular code-related errors such as bugs. Prior studies developed machine learning (ML) models trained on large datasets of job logs to classify job failures as either intermittent or regular. As an alternative to costly manual labeling of large datasets, the state-of-the-art (SOTA) approach leveraged a heuristic based on non-deterministic job reruns. However, this method mislabels intermittent job failures as regular in contexts where rerunning suspicious job failures is not an explicit policy, and therefore limits the SOTA’s performance in practice. In fact, our manual analysis of 2,125 job failures from 5 industrial and 1 open-source projects reveals that, on average, 32% of intermittent job failures are mislabeled as regular. To address these limitations, this paper introduces a novel approach to intermittent job failure detection using few-shot learning (FSL). Specifically, we fine-tune a small language model using a few number of manually labeled log examples to generate rich embeddings, which are then used to train an ML classifier. Our FSL-based approach achieves 70-88% F1-score with only 12 shots in all projects, outperforming the SOTA, which proved ineffective (34-52% F1-score) in 4 projects. Overall, this study underlines the importance of data quality over quantity and provides a more efficient and practical framework for the detection of intermittent job failures in organizations. </p>
<blockquote>
<p>开发者在使用持续集成（CI）和部署管道时面临的主要挑战之一是间歇性作业失败的发生。这些失败是由于意外的非确定性问题（例如，测试不稳定或基础设施问题）导致的，而不是常规的代码相关错误，如错误。早期的研究已经开发出了基于大型作业日志数据集训练的机器学习（ML）模型，用于将作业失败分类为间歇性失败或常规失败。作为对大型数据集昂贵手动标签的替代方案，最新方法利用基于非确定性作业重跑的启发式方法。然而，在重试可疑作业失败不是明确策略的情况下，这种方法会将间歇性作业失败误判为常规情况，因此在实际应用中限制了最新方法的性能。实际上，我们对来自五个工业项目和一开源项目的 2，125 次作业失败进行的手动分析显示，平均而言，有 32% 的间歇性作业失败被误判为常规情况。为了解决这些局限性，本文引入了一种新的间歇性作业故障检测法——基于少量学习的学习方法（FSL）。具体来说，我们使用少量手动标记的日志示例微调小型语言模型来生成丰富的嵌入表示，然后使用这些嵌入表示训练 ML 分类器。基于我们的少量学习的FSL方法在所有项目中仅使用 12 个样本就达到了 70-88% 的 F1 分数，优于先前技术状态的最优方法（在四个项目中 F1 分数仅为 34-52%），证明了其有效性。总体而言，该研究强调了数据质量而非数量在间歇性作业故障检测中的重要性，并提供了一个更有效率且实用的框架用于组织中的间歇性作业故障检测。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.04173v2">PDF</a> Accepted at the 41st International Conference on Software Maintenance   and Evolution - ICSME 2025 (Industry Track); 12 pages; typos corrected</p>
<p><strong>Summary</strong></p>
<p>该文本介绍了在连续集成和部署管道的使用中，开发者面临的一个主要挑战是偶发性作业失败的问题。针对这一问题，研究者引入了基于少量标注日志样本的少数学习（FSL）新方法来进行偶发性作业失败的检测。该方法通过微调小型语言模型生成丰富的嵌入，然后用于训练机器学习分类器。实验结果显示，该方法在所有项目中的F1分数达到70-88%，仅使用12个样本，优于现有方法。总体而言，该研究强调了数据质量比数量更重要，并为组织中的偶发性作业失败检测提供了更高效、更实用的框架。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>偶发性作业失败是连续集成和部署管道中的一大挑战，源于非确定性问题，如测试不稳定或基础设施问题。</li>
<li>现有方法使用基于大型作业日志数据集训练的机器学习模型来分类作业失败类型。</li>
<li>最新方法引入基于少数学习的偶发性作业失败检测，仅使用少量手动标注的日志样本进行微调。</li>
<li>该方法生成丰富的嵌入，用于训练机器学习分类器，实现高F1分数（70-88%）。</li>
<li>与现有方法相比，该方法在多个项目中表现更优，突显数据质量的重要性。</li>
<li>研究结果提供了一种更实用、高效的框架来检测组织中的偶发性作业失败。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.04173">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-b66f0a459952a2f431323f8f7b811348.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b6eca8ddb4e4248beb561748cad07a95.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3c2f2e5e2a41bb812e8e0151c2521329.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0e15d541c83e97a180a769e1cff37892.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef2531920378b6d944cc487092f16698.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-07-10/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-07-10/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-10/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-d97cbcc437560fc51e9faef30c668919.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation 方向最新论文已更新，请持续关注 Update in 2025-07-10  EC-Flow Enabling Versatile Robotic Manipulation from Action-Unlabeled   Videos via Embodiment-Centric Flow
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-07-10
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-10/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-096ba8a0aff224167a07cb075ccf4792.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2025-07-10  Conditional Multi-Stage Failure Recovery for Embodied Agents
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-07-10
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">27685.3k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
