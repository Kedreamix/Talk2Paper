<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-10  Differentiable Reward Optimization for LLM based TTS system">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-aa7ffa0896d7eb265d5b8c30cd89da1b.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-10
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-19
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    8.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    34 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-07-10-æ›´æ–°"><a href="#2025-07-10-æ›´æ–°" class="headerlink" title="2025-07-10 æ›´æ–°"></a>2025-07-10 æ›´æ–°</h1><h2 id="Differentiable-Reward-Optimization-for-LLM-based-TTS-system"><a href="#Differentiable-Reward-Optimization-for-LLM-based-TTS-system" class="headerlink" title="Differentiable Reward Optimization for LLM based TTS system"></a>Differentiable Reward Optimization for LLM based TTS system</h2><p><strong>Authors:Changfeng Gao, Zhihao Du, Shiliang Zhang</strong></p>
<p>This paper proposes a novel Differentiable Reward Optimization (DiffRO) method aimed at enhancing the performance of neural codec language models based text-to-speech (TTS) systems. In contrast to conventional reinforcement learning from human feedback (RLHF) approaches applied to TTS, DiffRO directly compute the rewards based on neural codec tokens, rather than relying on synthesized audio. Furthermore, we employ the Gumbel-Softmax technique to render the reward function differentiable, thereby streamlining the RLHF training process. Additionally, we introduce a multi-task reward (MTR) model which can provide feedback from different perspectives and find that it can augment the systemâ€™s capability to follow instructions effectively.Experimental results indicate that DiffRO significantly improves the pronunciation accuracy of the TTS system, achieving state-of-the-art (SOTA) WER results on the seed-tts-eval benchmark. Moreover, with the integration of the MTR model, we demonstrate the ability to control emotional and quality attributes in a zero-shot manner. </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„å¯å¾®åˆ†å¥–åŠ±ä¼˜åŒ–ï¼ˆDiffROï¼‰æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜åŸºäºç¥ç»ç¼–ç è¯­è¨€æ¨¡å‹çš„æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿçš„æ€§èƒ½ã€‚ä¸ä¼ ç»Ÿçš„åº”ç”¨äºTTSçš„äººåé¦ˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰æ–¹æ³•ç›¸æ¯”ï¼ŒDiffROç›´æ¥åŸºäºç¥ç»ç¼–ç ä»¤ç‰Œè®¡ç®—å¥–åŠ±ï¼Œè€Œä¸æ˜¯ä¾èµ–äºåˆæˆéŸ³é¢‘ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é‡‡ç”¨Gumbel-SoftmaxæŠ€æœ¯ä½¿å¥–åŠ±å‡½æ•°å¯å¾®åˆ†ï¼Œä»è€Œç®€åŒ–äº†RLHFè®­ç»ƒè¿‡ç¨‹ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªå¤šä»»åŠ¡å¥–åŠ±ï¼ˆMTRï¼‰æ¨¡å‹ï¼Œå¯ä»¥ä»ä¸åŒçš„è§’åº¦æä¾›åé¦ˆï¼Œå¹¶å‘ç°å®ƒå¯ä»¥å¢å¼ºç³»ç»Ÿæœ‰æ•ˆéµå¾ªæŒ‡ä»¤çš„èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDiffROæ˜¾è‘—æé«˜äº†TTSç³»ç»Ÿçš„å‘éŸ³å‡†ç¡®æ€§ï¼Œåœ¨seed-tts-evalåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ç»“æœã€‚è€Œä¸”ï¼Œé€šè¿‡æ•´åˆMTRæ¨¡å‹ï¼Œæˆ‘ä»¬å±•ç¤ºäº†é›¶æ ·æœ¬æ–¹å¼æ§åˆ¶æƒ…æ„Ÿå’Œå“è´¨å±æ€§çš„èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.05911v1">PDF</a> </p>
<p><strong>Summary</strong><br>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºå¯å¾®å¥–åŠ±ä¼˜åŒ–ï¼ˆDiffROï¼‰çš„æ–°æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜åŸºäºç¥ç»ç¼–ç è¯­è¨€æ¨¡å‹çš„æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿçš„æ€§èƒ½ã€‚ä¸å¸¸è§„çš„åº”ç”¨äºTTSçš„åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰æ–¹æ³•ä¸åŒï¼ŒDiffROç›´æ¥åŸºäºç¥ç»ç¼–ç ä»¤ç‰Œè®¡ç®—å¥–åŠ±ï¼Œè€Œä¸æ˜¯ä¾èµ–äºåˆæˆçš„éŸ³é¢‘ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡é‡‡ç”¨Gumbel-SoftmaxæŠ€æœ¯ä½¿å¥–åŠ±å‡½æ•°å¯å¾®ï¼Œä»è€Œç®€åŒ–äº†RLHFè®­ç»ƒè¿‡ç¨‹ã€‚åŒæ—¶ï¼Œå¼•å…¥å¤šä»»åŠ¡å¥–åŠ±ï¼ˆMTRï¼‰æ¨¡å‹ï¼Œå¯ä»ä¸åŒè§’åº¦æä¾›åé¦ˆï¼Œæé«˜äº†ç³»ç»Ÿéµå¾ªæŒ‡ä»¤çš„æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDiffROæ˜¾è‘—æé«˜äº†TTSç³»ç»Ÿçš„å‘éŸ³å‡†ç¡®æ€§ï¼Œåœ¨seed-tts-evalåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ç»“æœã€‚è€Œä¸”ï¼Œé€šè¿‡æ•´åˆMTRæ¨¡å‹ï¼Œæˆ‘ä»¬å±•ç¤ºäº†é›¶æ ·æœ¬æ–¹å¼æ§åˆ¶æƒ…æ„Ÿå’Œå“è´¨å±æ€§çš„èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DiffROæ–¹æ³•è¢«æå‡ºä»¥æé«˜åŸºäºç¥ç»ç¼–ç è¯­è¨€æ¨¡å‹çš„TTSç³»ç»Ÿæ€§èƒ½ã€‚</li>
<li>ä¸ä¼ ç»ŸRLHFæ–¹æ³•ä¸åŒï¼ŒDiffROç›´æ¥åŸºäºç¥ç»ç¼–ç ä»¤ç‰Œè®¡ç®—å¥–åŠ±ã€‚</li>
<li>Gumbel-SoftmaxæŠ€æœ¯è¢«ç”¨äºä½¿å¥–åŠ±å‡½æ•°å¯å¾®ï¼Œç®€åŒ–äº†RLHFè®­ç»ƒè¿‡ç¨‹ã€‚</li>
<li>å¼•å…¥å¤šä»»åŠ¡å¥–åŠ±ï¼ˆMTRï¼‰æ¨¡å‹ï¼Œä»ä¸åŒè§’åº¦æä¾›åé¦ˆï¼Œæé«˜ç³»ç»Ÿéµå¾ªæŒ‡ä»¤çš„èƒ½åŠ›ã€‚</li>
<li>DiffROæ˜¾è‘—æé«˜TTSç³»ç»Ÿçš„å‘éŸ³å‡†ç¡®æ€§ï¼Œè¾¾åˆ°æœ€å…ˆè¿›çš„WERç»“æœã€‚</li>
<li>MTRæ¨¡å‹çš„æ•´åˆä½¿ç³»ç»Ÿèƒ½å¤Ÿé›¶æ ·æœ¬æ–¹å¼æ§åˆ¶æƒ…æ„Ÿå’Œå“è´¨å±æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.05911">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9858c76d77fbf47c8b02ecadc6cdb853.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-159d914ae0345d310414194c07f45363.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-83cb962fd27f21fcf841f8ae401703c9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7b295084b3dfb233b46d22757a022926.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="ContextASR-Bench-A-Massive-Contextual-Speech-Recognition-Benchmark"><a href="#ContextASR-Bench-A-Massive-Contextual-Speech-Recognition-Benchmark" class="headerlink" title="ContextASR-Bench: A Massive Contextual Speech Recognition Benchmark"></a>ContextASR-Bench: A Massive Contextual Speech Recognition Benchmark</h2><p><strong>Authors:He Wang, Linhan Ma, Dake Guo, Xiong Wang, Lei Xie, Jin Xu, Junyang Lin</strong></p>
<p>Automatic Speech Recognition (ASR) has been extensively investigated, yet prior evaluative efforts have largely been restricted to contextless paradigms. This constraint stems from the limited proficiency of conventional ASR models in context modeling and their deficiency in memory and reasoning based on world knowledge. Recent breakthroughs in the development of Large Language Models (LLMs) and corresponding Large Audio Language Models (LALMs) have markedly enhanced the visibility of general artificial intelligence capabilities. Consequently, there exists a compelling need for a benchmark that can evaluate both the generality and intelligence of ASR systems. To address this gap, we propose ContextASR-Bench: a comprehensive, large-scale benchmark designed to assess contextual speech recognition. This benchmark encompasses up to 40,000 data entries across over 10 domains, enabling a thorough evaluation of model performance in scenarios that omit or incorporate coarse-grained or fine-grained contextual information. Moreover, diverging from conventional ASR evaluations, our benchmark includes an analysis of model efficacy in recognizing named entities mentioned within the auditory input. Our extensive evaluation highlights that LALMs, with strong world knowledge and context learning capabilities, outperform conventional ASR models by a large margin. The dataset and evaluation code have been released at <a target="_blank" rel="noopener" href="https://github.com/MrSupW/ContextASR-Bench">https://github.com/MrSupW/ContextASR-Bench</a>. </p>
<blockquote>
<p>è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰å·²ç»å¾—åˆ°äº†å¹¿æ³›çš„ç ”ç©¶ï¼Œä½†ä¹‹å‰çš„è¯„ä¼°å·¥ä½œå¤§å¤šå±€é™äºæ— ä¸Šä¸‹æ–‡çš„æ¨¡å¼ã€‚è¿™ä¸€é™åˆ¶æºäºä¼ ç»ŸASRæ¨¡å‹åœ¨ä¸Šä¸‹æ–‡å»ºæ¨¡æ–¹é¢çš„æœ‰é™èƒ½åŠ›ï¼Œä»¥åŠå®ƒä»¬åœ¨åŸºäºä¸–ç•ŒçŸ¥è¯†çš„è®°å¿†å’Œæ¨ç†æ–¹é¢çš„ä¸è¶³ã€‚æœ€è¿‘ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œç›¸åº”çš„å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆLALMï¼‰çš„å‘å±•å–å¾—äº†çªç ´ï¼Œæ˜¾è‘—æé«˜äº†é€šç”¨äººå·¥æ™ºèƒ½èƒ½åŠ›å¯è§åº¦ã€‚å› æ­¤ï¼Œå­˜åœ¨ä¸€ç§è¿«åˆ‡éœ€è¦å¯¹èƒ½å¤Ÿè¯„ä¼°ASRç³»ç»Ÿé€šç”¨æ€§å’Œæ™ºèƒ½æ€§çš„åŸºå‡†æµ‹è¯•ã€‚ä¸ºè§£å†³è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†ContextASR-Benchï¼šä¸€ä¸ªå…¨é¢ã€å¤§è§„æ¨¡çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°ä¸Šä¸‹æ–‡è¯­éŸ³è¯†åˆ«ã€‚è¯¥åŸºå‡†æµ‹è¯•åŒ…å«è¶…è¿‡10ä¸ªé¢†åŸŸçš„å¤šè¾¾4ä¸‡ä¸ªæ•°æ®æ¡ç›®ï¼Œèƒ½å¤Ÿåœ¨çœç•¥æˆ–åŒ…å«ç²—ç²’åº¦æˆ–ç»†ç²’åº¦ä¸Šä¸‹æ–‡ä¿¡æ¯çš„åœºæ™¯ä¸­å½»åº•è¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚æ­¤å¤–ï¼Œä¸ä¼ ç»Ÿçš„ASRè¯„ä¼°ä¸åŒï¼Œæˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•è¿˜åŒ…æ‹¬å¯¹æ¨¡å‹è¯†åˆ«å¬è§‰è¾“å…¥ä¸­æåˆ°çš„å‘½åå®ä½“çš„åŠŸæ•ˆçš„åˆ†æã€‚æˆ‘ä»¬çš„å¹¿æ³›è¯„ä¼°å¼ºè°ƒï¼Œå…·æœ‰å¼ºå¤§ä¸–ç•ŒçŸ¥è¯†å’Œä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›çš„LALMåœ¨æ€§èƒ½ä¸Šå¤§å¤§è¶…è¿‡äº†ä¼ ç»ŸASRæ¨¡å‹ã€‚æ•°æ®é›†å’Œè¯„ä¼°ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/MrSupW/ContextASR-Bench%E3%80%82">https://github.com/MrSupW/ContextASR-Benchã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.05727v1">PDF</a> 18 pages, 4 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰çš„æ–°æŒ‘æˆ˜ï¼Œä¼ ç»ŸASRæ¨¡å‹åœ¨è¯­å¢ƒå»ºæ¨¡æ–¹é¢çš„å±€é™æ€§ä½¿å…¶ç¼ºä¹è®°å¿†å’ŒåŸºäºä¸–ç•ŒçŸ¥è¯†çš„æ¨ç†èƒ½åŠ›ã€‚éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œå¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆLALMï¼‰çš„å‘å±•ï¼Œæœ‰å¿…è¦å»ºç«‹ä¸€ä¸ªèƒ½å¤Ÿè¯„ä¼°ASRç³»ç»Ÿçš„é€šç”¨æ€§å’Œæ™ºèƒ½æ€§çš„åŸºå‡†æµ‹è¯•ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†ContextASR-BenchåŸºå‡†æµ‹è¯•ï¼Œè¯¥æµ‹è¯•åŒ…å«å¤šè¾¾4ä¸‡ä¸ªæ•°æ®æ¡ç›®ï¼Œè·¨è¶Š10ä¸ªé¢†åŸŸï¼Œå¹¶åŒ…æ‹¬åœ¨åŒ…å«æˆ–æ’é™¤ç²—ç²’åº¦æˆ–ç»†ç²’åº¦ä¸Šä¸‹æ–‡ä¿¡æ¯çš„åœºæ™¯ä¸­å¯¹æ¨¡å‹æ€§èƒ½çš„ç»¼åˆè¯„ä¼°ã€‚åˆ†æå‘ç°ï¼Œå…·æœ‰å¼ºå¤§ä¸–ç•ŒçŸ¥è¯†å’Œä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›çš„LALMæ˜æ˜¾ä¼˜äºä¼ ç»ŸASRæ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¼ ç»ŸASRæ¨¡å‹åœ¨è¯­å¢ƒå»ºæ¨¡æ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œç¼ºä¹è®°å¿†å’ŒåŸºäºä¸–ç•ŒçŸ¥è¯†çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œå¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆLALMï¼‰çš„å‘å±•ä¸ºASRç³»ç»Ÿå¸¦æ¥äº†æ–°çš„å¯èƒ½æ€§ã€‚</li>
<li>ContextASR-BenchåŸºå‡†æµ‹è¯•æ˜¯ä¸ºäº†è¯„ä¼°ASRç³»ç»Ÿçš„é€šç”¨æ€§å’Œæ™ºèƒ½æ€§è€Œæå‡ºçš„ã€‚</li>
<li>ContextASR-BenchåŒ…å«å¤šè¾¾4ä¸‡ä¸ªæ•°æ®æ¡ç›®ï¼Œè·¨è¶Š10ä¸ªé¢†åŸŸï¼Œå¯å…¨é¢è¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>ContextASR-BenchåŒ…æ‹¬åœ¨åŒ…å«æˆ–æ’é™¤ä¸Šä¸‹æ–‡ä¿¡æ¯çš„åœºæ™¯ä¸­å¯¹æ¨¡å‹æ€§èƒ½çš„åˆ†æã€‚</li>
<li>LALMåœ¨æ¨¡å‹æ€§èƒ½ä¸Šæ˜æ˜¾ä¼˜äºä¼ ç»ŸASRæ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.05727">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-68c2bf82ab30fcbe5c7fcd2fd14e9a85.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-19d2138e196f212094316f5f99a61000.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-493a36da15ad19e22ece8791a4556980.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-29d58ece8ac512128debff40dc7de13a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8c1739b4776eab714091d4a342a6653d.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Robust-One-step-Speech-Enhancement-via-Consistency-Distillation"><a href="#Robust-One-step-Speech-Enhancement-via-Consistency-Distillation" class="headerlink" title="Robust One-step Speech Enhancement via Consistency Distillation"></a>Robust One-step Speech Enhancement via Consistency Distillation</h2><p><strong>Authors:Liang Xu, Longfei Felix Yan, W. Bastiaan Kleijn</strong></p>
<p>Diffusion models have shown strong performance in speech enhancement, but their real-time applicability has been limited by multi-step iterative sampling. Consistency distillation has recently emerged as a promising alternative by distilling a one-step consistency model from a multi-step diffusion-based teacher model. However, distilled consistency models are inherently biased towards the sampling trajectory of the teacher model, making them less robust to noise and prone to inheriting inaccuracies from the teacher model. To address this limitation, we propose ROSE-CD: Robust One-step Speech Enhancement via Consistency Distillation, a novel approach for distilling a one-step consistency model. Specifically, we introduce a randomized learning trajectory to improve the modelâ€™s robustness to noise. Furthermore, we jointly optimize the one-step model with two time-domain auxiliary losses, enabling it to recover from teacher-induced errors and surpass the teacher model in overall performance. This is the first pure one-step consistency distillation model for diffusion-based speech enhancement, achieving 54 times faster inference speed and superior performance compared to its 30-step teacher model. Experiments on the VoiceBank-DEMAND dataset demonstrate that the proposed model achieves state-of-the-art performance in terms of speech quality. Moreover, its generalization ability is validated on both an out-of-domain dataset and real-world noisy recordings. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹åœ¨è¯­éŸ³å¢å¼ºæ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†å…¶å®æ—¶åº”ç”¨æ€§å—åˆ°å¤šæ­¥è¿­ä»£é‡‡æ ·çš„é™åˆ¶ã€‚è¿‘æœŸï¼Œä¸€è‡´æ€§è’¸é¦ä½œä¸ºä¸€ç§æœ‰å‰æ™¯çš„æ›¿ä»£æ–¹æ¡ˆå´­éœ²å¤´è§’ï¼Œå®ƒé€šè¿‡ä»åŸºäºå¤šæ­¥æ‰©æ•£çš„æ•™å¸ˆæ¨¡å‹ä¸­è’¸é¦å‡ºä¸€æ­¥ä¸€è‡´æ€§æ¨¡å‹ã€‚ç„¶è€Œï¼Œè’¸é¦å‡ºçš„ä¸€è‡´æ€§æ¨¡å‹æœ¬è´¨ä¸Šåå‘äºæ•™å¸ˆæ¨¡å‹çš„é‡‡æ ·è½¨è¿¹ï¼Œå¯¼è‡´å…¶å¯¹å™ªå£°çš„é²æ£’æ€§è¾ƒä½ï¼Œå¹¶å®¹æ˜“ç»§æ‰¿æ•™å¸ˆæ¨¡å‹çš„è¯¯å·®ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ROSE-CDï¼šé€šè¿‡ä¸€è‡´æ€§è’¸é¦å®ç°ç¨³å¥çš„ä¸€æ­¥è¯­éŸ³å¢å¼ºã€‚è¿™æ˜¯ä¸€ç§è’¸é¦ä¸€æ­¥ä¸€è‡´æ€§æ¨¡å‹çš„æ–°æ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥éšæœºå­¦ä¹ è½¨è¿¹æ¥æé«˜æ¨¡å‹å¯¹å™ªå£°çš„é²æ£’æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜é€šè¿‡ä¸¤ä¸ªæ—¶åŸŸè¾…åŠ©æŸå¤±è”åˆä¼˜åŒ–ä¸€æ­¥æ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿä»æ•™å¸ˆå¼•å‘çš„é”™è¯¯ä¸­æ¢å¤ï¼Œå¹¶åœ¨æ€»ä½“æ€§èƒ½ä¸Šè¶…è¶Šæ•™å¸ˆæ¨¡å‹ã€‚è¿™æ˜¯é¦–ä¸ªç”¨äºåŸºäºæ‰©æ•£çš„è¯­éŸ³å¢å¼ºçº¯ä¸€æ­¥ä¸€è‡´æ€§è’¸é¦æ¨¡å‹ï¼Œå…¶æ¨ç†é€Ÿåº¦è¾¾åˆ°æ•™å¸ˆæ¨¡å‹çš„54å€ï¼Œä¸”æ€§èƒ½æ›´ä¸ºä¼˜è¶Šã€‚åœ¨VoiceBank-DEMANDæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨è¯­éŸ³è´¨é‡æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œå…¶åœ¨åŸŸå¤–æ•°æ®é›†å’ŒçœŸå®ä¸–ç•Œå™ªå£°è®°å½•ä¸Šçš„æ³›åŒ–èƒ½åŠ›ä¹Ÿå¾—åˆ°äº†éªŒè¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.05688v1">PDF</a> Accepted to IEEE WASPAA 2025. 6 pages, 1 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºä¸€è‡´æ€§è’¸é¦çš„é²æ£’ä¸€æ­¥è¯­éŸ³å¢å¼ºæ–¹æ³•ï¼ˆROSE-CDï¼‰ï¼Œç”¨äºè§£å†³æ‰©æ•£æ¨¡å‹åœ¨è¯­éŸ³å¢å¼ºä¸­çš„å®æ—¶åº”ç”¨é—®é¢˜ã€‚é€šè¿‡å¼•å…¥éšæœºå­¦ä¹ è½¨è¿¹å’Œè”åˆä¼˜åŒ–ä¸€æ­¥æ¨¡å‹ä¸ä¸¤ä¸ªæ—¶åŸŸè¾…åŠ©æŸå¤±ï¼Œæé«˜äº†æ¨¡å‹çš„æŠ—å™ªå£°èƒ½åŠ›å’Œä»æ•™å¸ˆæ¨¡å‹è¯±å¯¼çš„é”™è¯¯ä¸­æ¢å¤çš„èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨è¯­éŸ³è´¨é‡æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå…·æœ‰å‡ºè‰²çš„æ³›åŒ–èƒ½åŠ›å’Œå¿«é€Ÿæ¨ç†é€Ÿåº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨è¯­éŸ³å¢å¼ºä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†å®æ—¶åº”ç”¨å—åˆ°å¤šæ­¥è¿­ä»£é‡‡æ ·çš„é™åˆ¶ã€‚</li>
<li>ä¸€è‡´æ€§è’¸é¦æ˜¯ä¸€ç§æ–°å…´çš„æœ‰å‰é€”çš„æ›¿ä»£æ–¹æ³•ï¼Œé€šè¿‡è’¸é¦ä¸€æ­¥ä¸€è‡´æ€§æ¨¡å‹ä»å¤šæ­¥æ‰©æ•£æ•™å¸ˆæ¨¡å‹ä¸­ã€‚</li>
<li>è’¸é¦çš„ä¸€è‡´æ€§æ¨¡å‹åå‘äºæ•™å¸ˆæ¨¡å‹çš„é‡‡æ ·è½¨è¿¹ï¼Œå¯¹å™ªå£°çš„é²æ£’æ€§è¾ƒä½ï¼Œå¹¶å¯èƒ½ç»§æ‰¿æ•™å¸ˆæ¨¡å‹çš„ä¸å‡†ç¡®ä¹‹å¤„ã€‚</li>
<li>ROSE-CDæ–¹æ³•é€šè¿‡å¼•å…¥éšæœºå­¦ä¹ è½¨è¿¹æ¥æé«˜æ¨¡å‹çš„æŠ—å™ªå£°èƒ½åŠ›ã€‚</li>
<li>ROSE-CDé€šè¿‡è”åˆä¼˜åŒ–ä¸€æ­¥æ¨¡å‹ä¸ä¸¤ä¸ªæ—¶åŸŸè¾…åŠ©æŸå¤±ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿä»æ•™å¸ˆæ¨¡å‹è¯±å¯¼çš„é”™è¯¯ä¸­æ¢å¤å¹¶è¶…è¶Šæ•™å¸ˆæ¨¡å‹çš„æ•´ä½“æ€§èƒ½ã€‚</li>
<li>ROSE-CDæ˜¯åœ¨æ‰©æ•£è¯­éŸ³å¢å¼ºä¸­é¦–æ¬¡æå‡ºçš„çº¯ä¸€æ­¥ä¸€è‡´æ€§è’¸é¦æ¨¡å‹ï¼Œå…·æœ‰å¿«é€Ÿæ¨ç†é€Ÿåº¦å’Œå“è¶Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.05688">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d1a217f1daf61665fc84e8bca285726f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-989558c152394972a25f9b4913f4fdf5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-db113b607707b16166be86cd9d517e3f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ebeae83fe2826ae5030b4561b48e6de1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-11cf2f0782ef6670b1e90a8ece263a77.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="ADMC-Attention-based-Diffusion-Model-for-Missing-Modalities-Feature-Completion"><a href="#ADMC-Attention-based-Diffusion-Model-for-Missing-Modalities-Feature-Completion" class="headerlink" title="ADMC: Attention-based Diffusion Model for Missing Modalities Feature   Completion"></a>ADMC: Attention-based Diffusion Model for Missing Modalities Feature   Completion</h2><p><strong>Authors:Wei Zhang, Juan Chen, Yanbo J. Wang, En Zhu, Xuan Yang, Yiduo Wang</strong></p>
<p>Multimodal emotion and intent recognition is essential for automated human-computer interaction, It aims to analyze usersâ€™ speech, text, and visual information to predict their emotions or intent. One of the significant challenges is that missing modalities due to sensor malfunctions or incomplete data. Traditional methods that attempt to reconstruct missing information often suffer from over-coupling and imprecise generation processes, leading to suboptimal outcomes. To address these issues, we introduce an Attention-based Diffusion model for Missing Modalities feature Completion (ADMC). Our framework independently trains feature extraction networks for each modality, preserving their unique characteristics and avoiding over-coupling. The Attention-based Diffusion Network (ADN) generates missing modality features that closely align with authentic multimodal distribution, enhancing performance across all missing-modality scenarios. Moreover, ADNâ€™s cross-modal generation offers improved recognition even in full-modality contexts. Our approach achieves state-of-the-art results on the IEMOCAP and MIntRec benchmarks, demonstrating its effectiveness in both missing and complete modality scenarios. </p>
<blockquote>
<p>å¤šæ¨¡æ€æƒ…æ„Ÿä¸æ„å›¾è¯†åˆ«æ˜¯è‡ªåŠ¨åŒ–äººæœºäº¤äº’çš„æ ¸å¿ƒï¼Œå…¶ç›®æ ‡æ˜¯é€šè¿‡åˆ†æç”¨æˆ·çš„è¯­éŸ³ã€æ–‡æœ¬å’Œè§†è§‰ä¿¡æ¯æ¥é¢„æµ‹å…¶æƒ…æ„Ÿæˆ–æ„å›¾ã€‚é¢ä¸´çš„æŒ‘æˆ˜ä¹‹ä¸€æ˜¯ä¼ æ„Ÿå™¨æ•…éšœæˆ–æ•°æ®ä¸å®Œæ•´å¯¼è‡´çš„æ¨¡æ€ç¼ºå¤±ã€‚ä¼ ç»Ÿçš„æ–¹æ³•è¯•å›¾é‡å»ºç¼ºå¤±çš„ä¿¡æ¯ï¼Œä½†å¸¸å¸¸å—åˆ°è¿‡åº¦è€¦åˆå’Œä¸ç²¾ç¡®ç”Ÿæˆè¿‡ç¨‹çš„å½±å“ï¼Œå¯¼è‡´ç»“æœä¸ç†æƒ³ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºäºæ³¨æ„åŠ›çš„æ‰©æ•£æ¨¡å‹ï¼ˆADMCï¼‰è¿›è¡Œç¼ºå¤±æ¨¡æ€ç‰¹å¾è¡¥å…¨ã€‚æˆ‘ä»¬çš„æ¡†æ¶ç‹¬ç«‹è®­ç»ƒæ¯ç§æ¨¡æ€çš„ç‰¹å¾æå–ç½‘ç»œï¼Œä¿ç•™å…¶ç‹¬ç‰¹ç‰¹æ€§ï¼Œé¿å…è¿‡åº¦è€¦åˆã€‚åŸºäºæ³¨æ„åŠ›çš„æ‰©æ•£ç½‘ç»œï¼ˆADNï¼‰ç”Ÿæˆç¼ºå¤±æ¨¡æ€çš„ç‰¹å¾ï¼Œè¿™äº›ç‰¹å¾ç´§å¯†ç¬¦åˆçœŸå®çš„å¤šæ¨¡æ€åˆ†å¸ƒï¼Œæé«˜äº†æ‰€æœ‰ç¼ºå¤±æ¨¡æ€åœºæ™¯çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒADNçš„è·¨æ¨¡æ€ç”Ÿæˆç”šè‡³åœ¨å®Œæ•´æ¨¡æ€ä¸Šä¸‹æ–‡ä¸­ä¹Ÿæä¾›äº†æ”¹è¿›çš„è¯†åˆ«èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨IEMOCAPå’ŒMIntRecåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€æ–°ç»“æœï¼Œè¯æ˜äº†å®ƒåœ¨ç¼ºå¤±å’Œå®Œæ•´æ¨¡æ€åœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.05624v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤šæ¨¡æ€æƒ…æ„Ÿä¸æ„å›¾è¯†åˆ«åœ¨è‡ªåŠ¨åŒ–äººæœºäº¤äº’ä¸­è‡³å…³é‡è¦ã€‚å®ƒé€šè¿‡ç”¨æˆ·çš„è¯­éŸ³ã€æ–‡æœ¬å’Œè§†è§‰ä¿¡æ¯åˆ†ææ¥é¢„æµ‹ç”¨æˆ·çš„æƒ…æ„Ÿæˆ–æ„å›¾ã€‚é¢å¯¹ä¼ æ„Ÿå™¨æ•…éšœæˆ–æ•°æ®ä¸å®Œæ•´å¯¼è‡´çš„ç¼ºå¤±æ¨¡æ€é—®é¢˜ï¼Œä¼ ç»Ÿæ–¹æ³•å¾€å¾€å­˜åœ¨è¿‡åº¦è€¦åˆå’Œä¸ç²¾ç¡®ç”Ÿæˆè¿‡ç¨‹çš„é—®é¢˜ï¼Œå¯¼è‡´ç»“æœä¸ä½³ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºäºæ³¨æ„åŠ›çš„æ‰©æ•£æ¨¡å‹ç”¨äºç¼ºå¤±æ¨¡æ€ç‰¹å¾è¡¥å…¨ï¼ˆADMCï¼‰ã€‚æˆ‘ä»¬çš„æ¡†æ¶ç‹¬ç«‹è®­ç»ƒæ¯ç§æ¨¡æ€çš„ç‰¹å¾æå–ç½‘ç»œï¼Œä¿ç•™å…¶ç‹¬ç‰¹æ€§å¹¶é¿å…è¿‡åº¦è€¦åˆã€‚åŸºäºæ³¨æ„åŠ›çš„æ‰©æ•£ç½‘ç»œï¼ˆADNï¼‰ç”Ÿæˆçš„ç¼ºå¤±æ¨¡æ€ç‰¹å¾èƒ½ä¸çœŸå®çš„å¤šæ¨¡æ€åˆ†å¸ƒç´§å¯†å¯¹é½ï¼Œæå‡å„ç§ç¼ºå¤±æ¨¡æ€åœºæ™¯çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒADNçš„è·¨æ¨¡æ€ç”Ÿæˆèƒ½åŠ›åœ¨å…¨æ¨¡æ€ä¸Šä¸‹æ–‡ä¸­ä¹Ÿèƒ½æä¾›æ”¹è¿›åçš„è¯†åˆ«æ•ˆæœã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨IEMOCAPå’ŒMIntRecåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€æ–°æˆæœï¼Œè¯æ˜äº†å®ƒåœ¨ç¼ºå¤±å’Œå®Œæ•´æ¨¡æ€åœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€æƒ…æ„Ÿä¸æ„å›¾è¯†åˆ«åœ¨è‡ªåŠ¨åŒ–äººæœºäº¤äº’ä¸­çš„é‡è¦æ€§ã€‚</li>
<li>ç¼ºå¤±æ¨¡æ€æ˜¯è‡ªåŠ¨åŒ–è¯†åˆ«çš„ä¸€ä¸ªæŒ‘æˆ˜ï¼Œä¼ ç»Ÿæ–¹æ³•å­˜åœ¨è¿‡åº¦è€¦åˆå’Œä¸ç²¾ç¡®ç”Ÿæˆçš„é—®é¢˜ã€‚</li>
<li>å¼•å…¥åŸºäºæ³¨æ„åŠ›çš„æ‰©æ•£æ¨¡å‹ç”¨äºç¼ºå¤±æ¨¡æ€ç‰¹å¾è¡¥å…¨ï¼ˆADMCï¼‰ã€‚</li>
<li>ADMCæ¡†æ¶ç‹¬ç«‹è®­ç»ƒæ¯ç§æ¨¡æ€çš„ç‰¹å¾æå–ç½‘ç»œï¼Œé¿å…è¿‡åº¦è€¦åˆã€‚</li>
<li>åŸºäºæ³¨æ„åŠ›çš„æ‰©æ•£ç½‘ç»œï¼ˆADNï¼‰ç”Ÿæˆçš„ç¼ºå¤±æ¨¡æ€ç‰¹å¾ä¸çœŸå®å¤šæ¨¡æ€åˆ†å¸ƒå¯¹é½ï¼Œæå‡æ€§èƒ½ã€‚</li>
<li>ADNçš„è·¨æ¨¡æ€ç”Ÿæˆèƒ½åŠ›åœ¨å…¨æ¨¡æ€ä¸Šä¸‹æ–‡ä¸­ä¹Ÿèƒ½æä¾›æ”¹è¿›åçš„è¯†åˆ«æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.05624">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-dce98e467c368495760e6da62242ca4b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fae18c8a487a716d67d9a6e430d4f3ea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e62ca0370984128149a32d3e239b7357.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="MLlm-DR-Towards-Explainable-Depression-Recognition-with-MultiModal-Large-Language-Models"><a href="#MLlm-DR-Towards-Explainable-Depression-Recognition-with-MultiModal-Large-Language-Models" class="headerlink" title="MLlm-DR: Towards Explainable Depression Recognition with MultiModal   Large Language Models"></a>MLlm-DR: Towards Explainable Depression Recognition with MultiModal   Large Language Models</h2><p><strong>Authors:Wei Zhang, Juan Chen, En Zhu, Wenhong Cheng, YunPeng Li, Yanbo J. Wang</strong></p>
<p>Automated depression diagnosis aims to analyze multimodal information from interview videos to predict participantsâ€™ depression scores. Previous studies often lack clear explanations of how these scores were determined, limiting their adoption in clinical practice. While the advent of LLMs provides a possible pathway for explainable depression diagnosis, current LLMs capable of processing multimodal data lack training on interview data, resulting in poor diagnostic performance when used directly. In this paper, we propose a novel multimodal large language model (MLlm-DR) that can understand multimodal information inputs and supports explainable depression diagnosis. MLlm-DR integrates a smaller LLMs and a lightweight query module (LQ-former). Specifically, the smaller LLMs is designed to generate depression scores and corresponding evaluation rationales. To enhance its logical reasoning for domain-specific tasks while maintaining practicality, we constructed a robust training dataset to fine-tune it. Meanwhile, the LQ-former captures depression-related features from speech and visual data, aiding the modelâ€™s ability to process multimodal information, to achieve comprehensive depression diagnosis. Our approach achieves state-of-the-art results on two interview-based benchmark datasets, CMDC and E-DAIC-WOZ, demonstrating its effectiveness and superiority. </p>
<blockquote>
<p>è‡ªåŠ¨åŒ–æŠ‘éƒç—‡è¯Šæ–­æ—¨åœ¨åˆ†æè®¿è°ˆè§†é¢‘ä¸­çš„å¤šæ¨¡å¼ä¿¡æ¯ï¼Œä»¥é¢„æµ‹å‚ä¸è€…çš„æŠ‘éƒè¯„åˆ†ã€‚ä¹‹å‰çš„ç ”ç©¶å¾€å¾€ç¼ºä¹å…³äºè¿™äº›è¯„åˆ†å¦‚ä½•ç¡®å®šçš„æ˜ç¡®è§£é‡Šï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨ä¸´åºŠå®è·µä¸­çš„åº”ç”¨ã€‚è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å‡ºç°ä¸ºå¯è§£é‡Šçš„æŠ‘éƒç—‡è¯Šæ–­æä¾›äº†ä¸€æ¡å¯èƒ½çš„é€”å¾„ï¼Œä½†ç›®å‰èƒ½å¤Ÿå¤„ç†å¤šæ¨¡å¼æ•°æ®çš„å¤§å‹è¯­è¨€æ¨¡å‹ç¼ºä¹è®¿è°ˆæ•°æ®çš„è®­ç»ƒï¼Œå¯¼è‡´å½“ç›´æ¥ä½¿ç”¨æ—¶è¯Šæ–­æ€§èƒ½è¾ƒå·®ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„å¤šæ¨¡å¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLlm-DRï¼‰ï¼Œèƒ½å¤Ÿç†è§£å¤šæ¨¡å¼ä¿¡æ¯è¾“å…¥å¹¶æ”¯æŒå¯è§£é‡Šçš„æŠ‘éƒç—‡è¯Šæ–­ã€‚MLlm-DRæ•´åˆäº†è¾ƒå°çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œä¸€ä¸ªè½»é‡çº§çš„æŸ¥è¯¢æ¨¡å—ï¼ˆLQ-formerï¼‰ã€‚å…·ä½“æ¥è¯´ï¼Œè¾ƒå°çš„LLMsè¢«è®¾è®¡ç”¨æ¥ç”ŸæˆæŠ‘éƒè¯„åˆ†å’Œç›¸åº”çš„è¯„ä¼°ä¾æ®ã€‚ä¸ºäº†å¢å¼ºå…¶åœ¨ç‰¹å®šé¢†åŸŸçš„é€»è¾‘æ¨ç†èƒ½åŠ›åŒæ—¶ä¿æŒå®ç”¨æ€§ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªç¨³å¥çš„è®­ç»ƒæ•°æ®é›†å¯¹å…¶è¿›è¡Œå¾®è°ƒã€‚åŒæ—¶ï¼ŒLQ-formerä»è¯­éŸ³å’Œè§†è§‰æ•°æ®ä¸­æ•è·ä¸æŠ‘éƒç—‡ç›¸å…³çš„ç‰¹å¾ï¼Œå¸®åŠ©æ¨¡å‹å¤„ç†å¤šæ¨¡å¼ä¿¡æ¯ï¼Œä»¥å®ç°å…¨é¢çš„æŠ‘éƒç—‡è¯Šæ–­ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨åŸºäºè®¿è°ˆçš„ä¸¤ä¸ªåŸºå‡†æ•°æ®é›†CMDCå’ŒE-DAIC-WOZä¸Šå–å¾—äº†æœ€æ–°æˆæœï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.05591v1">PDF</a> </p>
<p><strong>Summary</strong><br>è‡ªåŠ¨åŒ–æŠ‘éƒç—‡è¯Šæ–­æ—¨åœ¨é€šè¿‡åˆ†æè®¿è°ˆè§†é¢‘ä¸­çš„å¤šæ¨¡æ€ä¿¡æ¯æ¥é¢„æµ‹å‚ä¸è€…çš„æŠ‘éƒç¨‹åº¦ã€‚ç„¶è€Œï¼Œå…ˆå‰çš„ç ”ç©¶å¾€å¾€ç¼ºä¹æ˜ç¡®è§£é‡Šè¿™äº›åˆ†æ•°çš„ç¡®å®šæ–¹å¼ï¼Œé™åˆ¶äº†å…¶åœ¨ä¸´åºŠå®è·µä¸­çš„åº”ç”¨ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLlm-DRï¼‰ï¼Œèƒ½å¤Ÿå¤„ç†å¤šæ¨¡æ€ä¿¡æ¯è¾“å…¥å¹¶æ”¯æŒå¯è§£é‡Šçš„æŠ‘éƒç—‡è¯Šæ–­ã€‚è¯¥æ¨¡å‹ç»“åˆäº†è¾ƒå°çš„è¯­è¨€æ¨¡å‹å’Œè½»é‡çº§æŸ¥è¯¢æ¨¡å—ï¼ˆLQ-formerï¼‰ï¼Œä»¥ç”ŸæˆæŠ‘éƒåˆ†æ•°å’Œç›¸åº”çš„è¯„ä¼°ä¾æ®ï¼Œå¹¶é€šè¿‡æ„å»ºç¨³å¥çš„è®­ç»ƒæ•°æ®é›†è¿›è¡Œå¾®è°ƒï¼Œæé«˜å…¶é¢†åŸŸç‰¹å®šä»»åŠ¡çš„é€»è¾‘æ¨ç†èƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒå®ç”¨æ€§ã€‚LQ-formeræœ‰åŠ©äºæ¨¡å‹å¤„ç†å¤šæ¨¡æ€ä¿¡æ¯ï¼Œåœ¨ä¸¤ç§åŸºäºè®¿è°ˆçš„åŸºå‡†æ•°æ®é›†CMDCå’ŒE-DAIC-WOZä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æˆæœï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªåŠ¨åŒ–æŠ‘éƒç—‡è¯Šæ–­é€šè¿‡åˆ†æè®¿è°ˆè§†é¢‘ä¸­çš„å¤šæ¨¡æ€ä¿¡æ¯é¢„æµ‹å‚ä¸è€…çš„æŠ‘éƒç¨‹åº¦ã€‚</li>
<li>å…ˆå‰ç ”ç©¶ç¼ºä¹æ˜ç¡®çš„è¯Šæ–­åˆ†æ•°è§£é‡Šï¼Œé™åˆ¶äº†å…¶åœ¨ä¸´åºŠå®è·µä¸­çš„åº”ç”¨ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLlm-DRï¼‰ç”¨äºæ”¯æŒå¯è§£é‡Šçš„æŠ‘éƒç—‡è¯Šæ–­ã€‚</li>
<li>MLlm-DRç»“åˆäº†è¾ƒå°çš„è¯­è¨€æ¨¡å‹å’Œè½»é‡çº§æŸ¥è¯¢æ¨¡å—ï¼ˆLQ-formerï¼‰ã€‚</li>
<li>è¾ƒå°çš„è¯­è¨€æ¨¡å‹ç”¨äºç”ŸæˆæŠ‘éƒåˆ†æ•°å’Œè¯„ä¼°ä¾æ®ã€‚</li>
<li>é€šè¿‡æ„å»ºç¨³å¥çš„è®­ç»ƒæ•°æ®é›†å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œæé«˜é¢†åŸŸä»»åŠ¡çš„é€»è¾‘æ¨ç†èƒ½åŠ›å¹¶ç»´æŒå®ç”¨æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.05591">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-77ff395e79e59157305a6ed4ec577976.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c79532f3630f42f218eb0581c9f7d263.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-26df7c4561da3aae2838cafbae17ac08.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-83fd8c93297339b1e313f84dd49ba2fe.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="SHNU-Multilingual-Conversational-Speech-Recognition-System-for-INTERSPEECH-2025-MLC-SLM-Challenge"><a href="#SHNU-Multilingual-Conversational-Speech-Recognition-System-for-INTERSPEECH-2025-MLC-SLM-Challenge" class="headerlink" title="SHNU Multilingual Conversational Speech Recognition System for   INTERSPEECH 2025 MLC-SLM Challenge"></a>SHNU Multilingual Conversational Speech Recognition System for   INTERSPEECH 2025 MLC-SLM Challenge</h2><p><strong>Authors:Yuxiang Mei, Yuang Zheng, Dongxing Xu, Yanhua Long</strong></p>
<p>This paper describes SHNU multilingual conversational speech recognition system (SHNU-mASR, team name-â€œmaybeâ€), submitted to Track 1 of the INTERSPEECH 2025 MLC-SLM Challenge. Our system integrates a parallel-speech-encoder architecture with a large language model (LLM) to form a unified multilingual ASR framework. The parallel-speech-encoder consists of two pre-trained encoders, the Whisper-large-v3 encoder and mHuBERT-147 encoder. Their output embeddings are concatenated and fed into the LLM, enabling the model to leverage complementary acoustic and linguistic knowledge and achieve competitive performance. Moreover, we adopt a tri-stage training strategy to jointly update the low-rank adaptation modules and projector parameters of both the speech encoders and the LLM. In addition, we incorporate an additional language-aware prompt at the LLM input to enhance language-specific text generation. The SHNU-mASR system achieves an overall character&#x2F;word error rate (CER&#x2F;WER) of 11.76% on the blind evaluation set of the challenge, outperforming the official MLC-SLM baseline by 8.41 absolute CER&#x2F;WER, without increasing the baseline training data. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†SHNUå¤šè¯­ç§å¯¹è¯è¯­éŸ³è¯†åˆ«ç³»ç»Ÿï¼ˆSHNU-mASRï¼Œå›¢é˜Ÿåä¸ºâ€œmaybeâ€ï¼‰ï¼Œè¯¥ç³»ç»Ÿæäº¤è‡³INTERSPEECH 2025 MLC-SLM Challengeçš„Track 1ã€‚æˆ‘ä»¬çš„ç³»ç»Ÿå°†å¹¶è¡Œè¯­éŸ³ç¼–ç å™¨æ¶æ„ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é›†æˆåœ¨ä¸€èµ·ï¼Œå½¢æˆä¸€ä¸ªç»Ÿä¸€çš„å¤šè¯­ç§ASRæ¡†æ¶ã€‚å¹¶è¡Œè¯­éŸ³ç¼–ç å™¨ç”±ä¸¤ä¸ªé¢„è®­ç»ƒç¼–ç å™¨ç»„æˆï¼Œå³Whisper-large-v3ç¼–ç å™¨å’ŒmHuBERT-147ç¼–ç å™¨ã€‚ä»–ä»¬çš„è¾“å‡ºåµŒå…¥è¿›è¡Œæ‹¼æ¥ï¼Œå¹¶è¾“å…¥åˆ°LLMä¸­ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåˆ©ç”¨äº’è¡¥çš„å£°å­¦çŸ¥è¯†å’Œè¯­è¨€çŸ¥è¯†ï¼Œå®ç°æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é‡‡ç”¨ä¸‰é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œè”åˆæ›´æ–°ä½ç§©é€‚åº”æ¨¡å—å’Œè¯­éŸ³ç¼–ç å™¨å’ŒLLMçš„æŠ•å½±ä»ªå‚æ•°ã€‚å¦å¤–ï¼Œæˆ‘ä»¬åœ¨LLMçš„è¾“å…¥ç«¯å¢åŠ äº†ä¸€ä¸ªé¢å¤–çš„è¯­è¨€æ„ŸçŸ¥æç¤ºï¼Œä»¥å¢å¼ºç‰¹å®šè¯­è¨€çš„æ–‡æœ¬ç”Ÿæˆã€‚SHNU-mASRç³»ç»Ÿåœ¨æŒ‘æˆ˜ç›²è¯„æ•°æ®é›†ä¸Šå®ç°äº†11.76%çš„æ•´ä½“å­—ç¬¦&#x2F;å•è¯é”™è¯¯ç‡ï¼ˆCER&#x2F;WERï¼‰ï¼Œç›¸è¾ƒäºå®˜æ–¹MLC-SLMåŸºçº¿æé«˜äº†8.41ä¸ªç»å¯¹CER&#x2F;WERï¼Œä¸”æ²¡æœ‰å¢åŠ åŸºçº¿è®­ç»ƒæ•°æ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.03343v2">PDF</a> Accepted by Interspeech 2025 MLC-SLM workshop</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†SHNUçš„å¤šè¯­ç§å¯¹è¯è¯­éŸ³è¯†åˆ«ç³»ç»Ÿï¼ˆSHNU-mASRï¼‰ï¼Œè¯¥ç³»ç»Ÿå‚ä¸äº†INTERSPEECH 2025 MLC-SLM Challengeçš„Track 1æŒ‘æˆ˜ã€‚è¯¥ç³»ç»Ÿç»“åˆäº†å¹¶è¡Œè¯­éŸ³ç¼–ç å™¨æ¶æ„å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œå½¢æˆä¸€ä¸ªç»Ÿä¸€çš„å¤šè¯­ç§ASRæ¡†æ¶ã€‚é‡‡ç”¨é¢„è®­ç»ƒçš„Whisper-large-v3å’ŒmHuBERT-147ç¼–ç å™¨ï¼Œå…¶è¾“å‡ºåµŒå…¥é€šè¿‡è¿æ¥å¹¶è¾“å…¥åˆ°LLMä¸­ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåˆ©ç”¨äº’è¡¥çš„å£°å­¦çŸ¥è¯†å’Œè¯­è¨€çŸ¥è¯†ï¼Œå®ç°äº†è‰¯å¥½çš„æ€§èƒ½ã€‚é€šè¿‡é‡‡ç”¨ä¸‰é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œå¯¹ä½é˜¶é€‚åº”æ¨¡å—å’ŒæŠ•å½±å‚æ•°è¿›è¡Œè”åˆæ›´æ–°ï¼Œè¿›ä¸€æ­¥å¢å¼ºäº†ç³»ç»Ÿçš„æ€§èƒ½ã€‚åœ¨æŒ‘æˆ˜çš„æ— æ ‡æ³¨è¯„ä¼°é›†ä¸Šï¼ŒSHNU-mASRç³»ç»Ÿçš„å­—ç¬¦&#x2F;è¯é”™è¯¯ç‡ï¼ˆCER&#x2F;WERï¼‰è¾¾åˆ°äº†11.76%ï¼Œç›¸è¾ƒäºå®˜æ–¹MLC-SLMåŸºçº¿æé«˜äº†8.41ä¸ªç»å¯¹CER&#x2F;WERï¼Œä¸”æœªå¢åŠ åŸºçº¿è®­ç»ƒæ•°æ®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SHNUå›¢é˜Ÿæå‡ºäº†ä¸€ç§å¤šè¯­ç§å¯¹è¯è¯­éŸ³è¯†åˆ«ç³»ç»ŸSHNU-mASRï¼Œé’ˆå¯¹INTERSPEECH 2025 MLC-SLM Challengeçš„Track 1è¿›è¡Œäº†æŒ‘æˆ˜ã€‚</li>
<li>ç³»ç»Ÿé›†æˆäº†å¹¶è¡Œè¯­éŸ³ç¼–ç å™¨æ¶æ„å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œå½¢æˆç»Ÿä¸€çš„å¤šè¯­ç§ASRæ¡†æ¶ã€‚</li>
<li>é‡‡ç”¨é¢„è®­ç»ƒçš„Whisper-large-v3å’ŒmHuBERT-147ç¼–ç å™¨ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåˆ©ç”¨äº’è¡¥çš„å£°å­¦çŸ¥è¯†å’Œè¯­è¨€çŸ¥è¯†ã€‚</li>
<li>é€šè¿‡ä¸‰é˜¶æ®µè®­ç»ƒç­–ç•¥è”åˆæ›´æ–°ä½é˜¶é€‚åº”æ¨¡å—å’ŒæŠ•å½±å‚æ•°ï¼Œå¢å¼ºäº†ç³»ç»Ÿæ€§èƒ½ã€‚</li>
<li>SHNU-mASRç³»ç»Ÿå®ç°äº†è¾ƒä½çš„å­—ç¬¦&#x2F;è¯é”™è¯¯ç‡ï¼ˆCER&#x2F;WERï¼‰ï¼Œç›¸è¾ƒäºå®˜æ–¹åŸºçº¿æœ‰æ˜¾è‘—çš„æå‡ã€‚</li>
<li>è¯¥æå‡æ˜¯åœ¨ä¸å¢åŠ åŸºçº¿è®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹å®ç°çš„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.03343">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ab69e91ecf1b3eb3fc743fb37adbcc7f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-78728db70d2a6f96c98bfc01bd386bbd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-868c350da442b3a2455601df5b764564.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c6393ee73c68038700dfe5e4af37aab1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8ff5b76954df33b73e355b423be7e28e.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Evaluating-Logit-Based-GOP-Scores-for-Mispronunciation-Detection"><a href="#Evaluating-Logit-Based-GOP-Scores-for-Mispronunciation-Detection" class="headerlink" title="Evaluating Logit-Based GOP Scores for Mispronunciation Detection"></a>Evaluating Logit-Based GOP Scores for Mispronunciation Detection</h2><p><strong>Authors:Aditya Kamlesh Parikh, Cristian Tejedor-Garcia, Catia Cucchiarini, Helmer Strik</strong></p>
<p>Pronunciation assessment relies on goodness of pronunciation (GOP) scores, traditionally derived from softmax-based posterior probabilities. However, posterior probabilities may suffer from overconfidence and poor phoneme separation, limiting their effectiveness. This study compares logit-based GOP scores with probability-based GOP scores for mispronunciation detection. We conducted our experiment on two L2 English speech datasets spoken by Dutch and Mandarin speakers, assessing classification performance and correlation with human ratings. Logit-based methods outperform probability-based GOP in classification, but their effectiveness depends on dataset characteristics. The maximum logit GOP shows the strongest alignment with human perception, while a combination of different GOP scores balances probability and logit features. The findings suggest that hybrid GOP methods incorporating uncertainty modeling and phoneme-specific weighting improve pronunciation assessment. </p>
<blockquote>
<p>å‘éŸ³è¯„ä¼°ä¾èµ–äºå‘éŸ³è´¨é‡ï¼ˆGOPï¼‰åˆ†æ•°ï¼Œè¿™äº›åˆ†æ•°ä¼ ç»Ÿä¸Šæ˜¯ä»åŸºäºsoftmaxçš„åéªŒæ¦‚ç‡ä¸­å¾—å‡ºçš„ã€‚ç„¶è€Œï¼ŒåéªŒæ¦‚ç‡å¯èƒ½ä¼šå—åˆ°è¿‡åº¦è‡ªä¿¡å’ŒéŸ³ç´ åˆ†ç¦»ä¸è‰¯çš„å½±å“ï¼Œä»è€Œé™åˆ¶äº†å…¶æœ‰æ•ˆæ€§ã€‚æœ¬ç ”ç©¶å¯¹æ¯”äº†åŸºäºå¯¹æ•°å‡ ç‡ï¼ˆlogitï¼‰çš„GOPåˆ†æ•°ä¸åŸºäºæ¦‚ç‡çš„GOPåˆ†æ•°åœ¨å‘éŸ³é”™è¯¯æ£€æµ‹æ–¹é¢çš„è¡¨ç°ã€‚æˆ‘ä»¬åœ¨ç”±è·å…°è¯­å’Œæ™®é€šè¯å‘éŸ³çš„ä¸¤ç»„è‹±è¯­äºŒè¯­è¯­éŸ³æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Œè¯„ä¼°äº†åˆ†ç±»æ€§èƒ½å’Œä¸äººç±»è¯„åˆ†çš„ç›¸å…³æ€§ã€‚åŸºäºå¯¹æ•°å‡ ç‡çš„æ–¹æ³•åœ¨åˆ†ç±»ä¸Šä¼˜äºåŸºäºæ¦‚ç‡çš„GOPï¼Œä½†å…¶æœ‰æ•ˆæ€§å–å†³äºæ•°æ®é›†çš„ç‰¹æ€§ã€‚æœ€å¤§å¯¹æ•°å‡ ç‡GOPä¸äººçš„æ„ŸçŸ¥ä¸€è‡´æ€§æœ€å¼ºï¼Œè€Œç»“åˆä¸åŒçš„GOPåˆ†æ•°åˆ™å¹³è¡¡äº†æ¦‚ç‡å’Œå¯¹æ•°å‡ ç‡ç‰¹å¾ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œé‡‡ç”¨ä¸ç¡®å®šæ€§å»ºæ¨¡å’ŒéŸ³ç´ ç‰¹å®šæƒé‡çš„æ··åˆGOPæ–¹æ³•èƒ½æ”¹å–„å‘éŸ³è¯„ä¼°æ•ˆæœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.12067v2">PDF</a> Accepted to Interspeech 2025. This publication is part of the project   Responsible AI for Voice Diagnostics (RAIVD) with file number NGF.1607.22.013   of the research programme NGF AiNed Fellowship Grants which is financed by   the Dutch Research Council (NWO)</p>
<p><strong>æ€»ç»“</strong></p>
<p>æœ¬æ–‡ä¸»è¦æ¢è®¨äº†å‘éŸ³è¯„ä¼°ä¸­çš„å¥½å‘éŸ³ï¼ˆGOPï¼‰å¾—åˆ†è®¡ç®—æ–¹æ³•ã€‚ç ”ç©¶å‘ç°åŸºäºlogitçš„GOPå¾—åˆ†è®¡ç®—æ–¹æ³•ç›¸è¾ƒäºä¼ ç»Ÿçš„åŸºäºæ¦‚ç‡çš„å¾—åˆ†è®¡ç®—æ–¹æ³•èƒ½æ›´å¥½åœ°ç”¨äºå‘éŸ³é”™è¯¯çš„æ£€æµ‹ã€‚æ­¤å¤–ï¼Œä¸åŒæ•°æ®é›†ä¸‹åŸºäºå¯¹æ•°æ¦‚ç‡å¾—åˆ†æ–¹æ³•çš„æœ€ä½³åº”ç”¨æ–¹å¼ä¹Ÿè¿›è¡Œäº†ç ”ç©¶ï¼Œå‘ç°æœ€å¤§å¯¹æ•°æ¦‚ç‡å¾—åˆ†ä¸äººçš„æ„ŸçŸ¥æœ€ä¸ºä¸€è‡´ï¼Œè€Œç»“åˆä¸åŒGOPå¾—åˆ†çš„æ–¹æ³•å¯ä»¥å¹³è¡¡æ¦‚ç‡å’Œå¯¹æ•°æ¦‚ç‡ç‰¹å¾ã€‚ç ”ç©¶è¿˜è¡¨æ˜ï¼Œç»“åˆä¸ç¡®å®šæ€§å»ºæ¨¡å’ŒéŸ³ç´ ç‰¹å®šæƒé‡çš„æ··åˆGOPæ–¹æ³•èƒ½æé«˜å‘éŸ³è¯„ä¼°çš„å‡†ç¡®åº¦ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>åŸºäºå¯¹æ•°æ¦‚ç‡ï¼ˆlogitï¼‰çš„GOPå¾—åˆ†è®¡ç®—æ–¹æ³•ç›¸è¾ƒäºåŸºäºæ¦‚ç‡çš„æ–¹æ³•åœ¨å‘éŸ³è¯„ä¼°ä¸­è¡¨ç°æ›´ä¼˜ã€‚</li>
<li>æœ€å¤§å¯¹æ•°æ¦‚ç‡GOPå¾—åˆ†ä¸äººçš„æ„ŸçŸ¥æœ€ä¸ºä¸€è‡´ã€‚</li>
<li>ç»“åˆä¸åŒGOPå¾—åˆ†çš„æ··åˆæ–¹æ³•èƒ½å¤Ÿå¹³è¡¡æ¦‚ç‡å’Œå¯¹æ•°æ¦‚ç‡ç‰¹å¾ï¼Œæé«˜è¯„ä¼°å‡†ç¡®æ€§ã€‚</li>
<li>æ•°æ®é›†ç‰¹æ€§ä¼šå½±å“åŸºäºå¯¹æ•°æ¦‚ç‡æ–¹æ³•çš„æ€§èƒ½ã€‚</li>
<li>åŸºäºä¸ç¡®å®šæ€§å»ºæ¨¡çš„æ··åˆGOPæ–¹æ³•èƒ½è¿›ä¸€æ­¥æé«˜å‘éŸ³è¯„ä¼°çš„ç²¾ç¡®åº¦ã€‚</li>
<li>éŸ³ç´ ç‰¹å®šæƒé‡åœ¨å‘éŸ³è¯„ä¼°ä¸­èµ·åˆ°é‡è¦ä½œç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.12067">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-aa7ffa0896d7eb265d5b8c30cd89da1b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4d2ffefcadb7986d12abc1b06a4ee9d9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-19b13486a955b3416d1668dda634dda8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6301d21768c3ca88aa615318e14eaba1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cb336a761db775dc178b43f37690d370.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Enhancing-GOP-in-CTC-Based-Mispronunciation-Detection-with-Phonological-Knowledge"><a href="#Enhancing-GOP-in-CTC-Based-Mispronunciation-Detection-with-Phonological-Knowledge" class="headerlink" title="Enhancing GOP in CTC-Based Mispronunciation Detection with Phonological   Knowledge"></a>Enhancing GOP in CTC-Based Mispronunciation Detection with Phonological   Knowledge</h2><p><strong>Authors:Aditya Kamlesh Parikh, Cristian Tejedor-Garcia, Catia Cucchiarini, Helmer Strik</strong></p>
<p>Computer-Assisted Pronunciation Training (CAPT) systems employ automatic measures of pronunciation quality, such as the goodness of pronunciation (GOP) metric. GOP relies on forced alignments, which are prone to labeling and segmentation errors due to acoustic variability. While alignment-free methods address these challenges, they are computationally expensive and scale poorly with phoneme sequence length and inventory size. To enhance efficiency, we introduce a substitution-aware alignment-free GOP that restricts phoneme substitutions based on phoneme clusters and common learner errors. We evaluated our GOP on two L2 English speech datasets, one with child speech, My Pronunciation Coach (MPC), and SpeechOcean762, which includes child and adult speech. We compared RPS (restricted phoneme substitutions) and UPS (unrestricted phoneme substitutions) setups within alignment-free methods, which outperformed the baseline. We discuss our results and outline avenues for future research. </p>
<blockquote>
<p>è®¡ç®—æœºè¾…åŠ©å‘éŸ³è®­ç»ƒï¼ˆCAPTï¼‰ç³»ç»Ÿé‡‡ç”¨å‘éŸ³è´¨é‡è‡ªåŠ¨æµ‹é‡ï¼Œå¦‚å‘éŸ³è´¨é‡ï¼ˆGOPï¼‰æŒ‡æ ‡ã€‚GOPä¾èµ–äºå¼ºåˆ¶å¯¹é½ï¼Œç”±äºå£°å­¦å˜åŒ–ï¼Œå®ƒå®¹æ˜“å—åˆ°æ ‡è®°å’Œåˆ†æ®µé”™è¯¯çš„å½±å“ã€‚è™½ç„¶æ— å¯¹é½æ–¹æ³•è§£å†³äº†è¿™äº›æŒ‘æˆ˜ï¼Œä½†å®ƒä»¬åœ¨è®¡ç®—ä¸Šå¾ˆæ˜‚è´µï¼Œéšç€éŸ³ç´ åºåˆ—é•¿åº¦å’Œåº“å­˜å¤§å°çš„å¢åŠ ï¼Œæ€§èƒ½ä¸‹é™ã€‚ä¸ºäº†æé«˜æ•ˆç‡ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºéŸ³ç´ èšç±»å’Œå­¦ä¹ è€…å¸¸è§é”™è¯¯çš„æ›¿ä»£æ„ŸçŸ¥æ— å¯¹é½GOPã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªè‹±è¯­äºŒçº§è¯­éŸ³æ•°æ®é›†ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„GOPï¼Œå…¶ä¸­ä¸€ä¸ªæ•°æ®é›†åŒ…å«å„¿ç«¥è¯­éŸ³My Pronunciation Coachï¼ˆMPCï¼‰ï¼Œå¦ä¸€ä¸ªæ•°æ®é›†åŒ…å«å„¿ç«¥å’Œæˆäººè¯­éŸ³çš„SpeechOcean762ã€‚æˆ‘ä»¬åœ¨æ— å¯¹é½æ–¹æ³•ä¸­æ¯”è¾ƒäº†RPSï¼ˆå—é™éŸ³ç´ æ›¿ä»£ï¼‰å’ŒUPSï¼ˆéå—é™éŸ³ç´ æ›¿ä»£ï¼‰è®¾ç½®ï¼Œå®ƒä»¬ä¼˜äºåŸºçº¿æ°´å¹³ã€‚æˆ‘ä»¬è®¨è®ºäº†æˆ‘ä»¬çš„ç»“æœå¹¶æ¦‚è¿°äº†æœªæ¥ç ”ç©¶çš„æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02080v2">PDF</a> Accepted to Interspeech 2025. This publication is part of the project   Responsible AI for Voice Diagnostics (RAIVD) with file number NGF.1607.22.013   of the research programme NGF AiNed Fellowship Grants which is financed by   the Dutch Research Council (NWO)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è®¡ç®—æœºè¾…åŠ©å‘éŸ³è®­ç»ƒï¼ˆCAPTï¼‰ç³»ç»Ÿé‡‡ç”¨åŸºäºå¼ºåˆ¶å¯¹é½çš„å‘éŸ³è´¨é‡è¯„ä¼°æ–¹æ³•ï¼Œå¦‚å‘éŸ³è´¨é‡è¯„ä¼°ï¼ˆGOPï¼‰æŒ‡æ ‡ã€‚ç„¶è€Œï¼Œç”±äºå£°å­¦å˜åŒ–å¯¼è‡´çš„æ ‡ç­¾å’Œåˆ†æ®µè¯¯å·®å½±å“äº†å¼ºåˆ¶å¯¹é½æ–¹æ³•çš„å‡†ç¡®æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œç ”ç©¶è€…æå‡ºäº†ä¸€ç§åŸºäºéŸ³ç´ èšç±»å’Œå¸¸è§å­¦ä¹ è€…é”™è¯¯çš„æ›¿ä»£æ„ŸçŸ¥å¯¹é½æ— å…³çš„GOPæ–¹æ³•ï¼Œä»¥æé«˜æ•ˆç‡ã€‚è¯¥æ–¹æ³•åœ¨ä¸¤ç§è‹±è¯­äºŒè¯­è¯­éŸ³æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºåŸºçº¿æ–¹æ³•ï¼ŒåŒ…æ‹¬é’ˆå¯¹å„¿ç«¥è¯­éŸ³çš„My Pronunciation Coachæ•°æ®é›†å’ŒåŒ…å«å„¿ç«¥å’Œæˆäººè¯­éŸ³çš„SpeechOcean762æ•°æ®é›†ã€‚æœ¬æ–‡æ¢è®¨äº†ç»“æœå¹¶å±•æœ›äº†æœªæ¥ç ”ç©¶æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®¡ç®—æœºè¾…åŠ©å‘éŸ³è®­ç»ƒï¼ˆCAPTï¼‰ç³»ç»Ÿä½¿ç”¨è‡ªåŠ¨è¯„ä¼°å‘éŸ³è´¨é‡çš„æ–¹æ³•ï¼Œå¦‚å‘éŸ³è´¨é‡è¯„ä¼°ï¼ˆGOPï¼‰æŒ‡æ ‡ã€‚</li>
<li>ä¼ ç»ŸGOPä¾èµ–å¼ºåˆ¶å¯¹é½ï¼Œæ˜“å—åˆ°å£°å­¦å˜åŒ–çš„æ ‡ç­¾å’Œåˆ†æ®µè¯¯å·®å½±å“ã€‚</li>
<li>æå‡ºäº†åŸºäºéŸ³ç´ èšç±»å’Œå¸¸è§å­¦ä¹ è€…é”™è¯¯çš„æ›¿ä»£æ„ŸçŸ¥å¯¹é½æ— å…³çš„GOPæ–¹æ³•ï¼Œä»¥æé«˜æ•ˆç‡ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨ä¸¤ç§è‹±è¯­äºŒè¯­è¯­éŸ³æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚</li>
<li>ç ”ç©¶å¯¹æ¯”äº†å—é™éŸ³ç´ æ›¿ä»£ï¼ˆRPSï¼‰å’Œæ— é™åˆ¶éŸ³ç´ æ›¿ä»£ï¼ˆUPSï¼‰çš„è®¾å®šã€‚</li>
<li>ç»“æœæ˜¾ç¤ºï¼Œå¯¹é½æ— å…³çš„æ–¹æ³•åœ¨è¯­éŸ³æ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02080">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fbef8c9f6b963368f052349f4b6c0752.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c330a88a70fd19735e63e9eeea39c3ba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3c60998febc97474bc1e7013e61089d1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9af5182dff1bbab3a1904d421e438e4e.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Low-Rank-and-Sparse-Model-Merging-for-Multi-Lingual-Speech-Recognition-and-Translation"><a href="#Low-Rank-and-Sparse-Model-Merging-for-Multi-Lingual-Speech-Recognition-and-Translation" class="headerlink" title="Low-Rank and Sparse Model Merging for Multi-Lingual Speech Recognition   and Translation"></a>Low-Rank and Sparse Model Merging for Multi-Lingual Speech Recognition   and Translation</h2><p><strong>Authors:Qiuming Zhao, Guangzhi Sun, Chao Zhang</strong></p>
<p>Language diversity presents a significant challenge in speech-to-text (S2T) tasks, such as automatic speech recognition and translation. Traditional multi-lingual multi-task training approaches aim to address this by jointly optimising multiple speech recognition and translation tasks across various languages. While models like Whisper, built on these strategies, demonstrate strong performance, they still face issues of high computational cost, language interference, suboptimal training configurations, and limited extensibility. To overcome these challenges, we introduce LoRS-Merging (low-rank and sparse model merging), a novel technique designed to efficiently integrate models trained on different languages or tasks while preserving performance and reducing computational overhead. LoRS-Merging combines low-rank and sparse pruning to retain essential structures while eliminating redundant parameters, mitigating language interference, and enhancing extensibility. Experimental results across 10 languages demonstrate that LoRS-Merging significantly outperforms multi-lingual multi-task training, sequential training, and other merging methods, achieving over 20% improvement in normalised performance. Our findings suggest that model merging, particularly LoRS-Merging, is a scalable and effective complement to traditional multi-lingual training strategies for S2T applications. </p>
<blockquote>
<p>è¯­è¨€å¤šæ ·æ€§åœ¨è¯­éŸ³åˆ°æ–‡æœ¬ï¼ˆS2Tï¼‰çš„ä»»åŠ¡ä¸­ï¼Œå¦‚è‡ªåŠ¨è¯­éŸ³è¯†åˆ«å’Œç¿»è¯‘ï¼Œæ„æˆäº†ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ã€‚ä¼ ç»Ÿå¤šè¯­è¨€å¤šä»»åŠ¡è®­ç»ƒçš„æ–¹æ³•æ—¨åœ¨é€šè¿‡è”åˆä¼˜åŒ–å¤šç§è¯­è¨€çš„è¯­éŸ³è¯†åˆ«å’Œç¿»è¯‘ä»»åŠ¡æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚è™½ç„¶åŸºäºè¿™äº›ç­–ç•¥æ„å»ºçš„æ¨¡å‹ï¼ˆå¦‚whisperï¼‰è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†å®ƒä»¬ä»ç„¶é¢ä¸´è®¡ç®—æˆæœ¬é«˜ã€è¯­è¨€å¹²æ‰°ã€è®­ç»ƒé…ç½®ä¸ä½³å’Œæ‰©å±•æ€§æœ‰é™ç­‰é—®é¢˜ã€‚ä¸ºäº†å…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†LoRS-Mergingï¼ˆä½ç§©å’Œç¨€ç–æ¨¡å‹åˆå¹¶ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨é«˜æ•ˆé›†æˆåœ¨ä¸åŒè¯­è¨€æˆ–ä»»åŠ¡ä¸Šè®­ç»ƒçš„æ¨¡å‹çš„æ–°æŠ€æœ¯ï¼ŒåŒæ—¶ä¿ç•™æ€§èƒ½å¹¶é™ä½è®¡ç®—å¼€é”€ã€‚LoRS-Mergingç»“åˆäº†ä½ç§©å’Œç¨€ç–å‰ªææŠ€æœ¯ï¼Œä»¥ä¿ç•™é‡è¦ç»“æ„çš„åŒæ—¶æ¶ˆé™¤å†—ä½™å‚æ•°ï¼Œå‡è½»è¯­è¨€å¹²æ‰°å¹¶å¢å¼ºæ‰©å±•æ€§ã€‚åœ¨10ç§è¯­è¨€ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒLoRS-Mergingæ˜¾è‘—ä¼˜äºå¤šè¯­è¨€å¤šä»»åŠ¡è®­ç»ƒã€é¡ºåºè®­ç»ƒå’Œå…¶ä»–åˆå¹¶æ–¹æ³•ï¼Œåœ¨æ ‡å‡†åŒ–æ€§èƒ½ä¸Šæé«˜äº†è¶…è¿‡20%ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæ¨¡å‹åˆå¹¶ï¼Œç‰¹åˆ«æ˜¯LoRS-Mergingï¼Œæ˜¯S2Tåº”ç”¨ä¸­ä¼ ç»Ÿå¤šè¯­è¨€è®­ç»ƒç­–ç•¥çš„å¯æ‰©å±•å’Œæœ‰æ•ˆçš„è¡¥å……ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.17380v3">PDF</a> 13 pages</p>
<p><strong>Summary</strong></p>
<p>è¯­è¨€å¤šæ ·æ€§åœ¨è¯­éŸ³è½¬æ–‡æœ¬ï¼ˆS2Tï¼‰ä»»åŠ¡ä¸­ï¼Œå¦‚è‡ªåŠ¨è¯­éŸ³è¯†åˆ«å’Œç¿»è¯‘ä¸­ï¼Œå¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ã€‚ä¼ ç»Ÿçš„å¤šè¯­è¨€å¤šä»»åŠ¡è®­ç»ƒæ—¨åœ¨é€šè¿‡è”åˆä¼˜åŒ–å¤šç§è¯­è¨€å’Œä»»åŠ¡çš„è¯­éŸ³è¯†åˆ«å’Œç¿»è¯‘ä»»åŠ¡æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚å°½ç®¡åŸºäºè¿™äº›ç­–ç•¥æ„å»ºçš„æ¨¡å‹å¦‚Whisperè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†å®ƒä»¬ä»ç„¶é¢ä¸´è®¡ç®—æˆæœ¬é«˜ã€è¯­è¨€å¹²æ‰°ã€è®­ç»ƒé…ç½®ä¸ä½³å’Œæ‰©å±•æ€§æœ‰é™ç­‰é—®é¢˜ã€‚ä¸ºäº†å…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†LoRS-Mergingï¼ˆä½é˜¶å’Œç¨€ç–æ¨¡å‹åˆå¹¶ï¼‰æŠ€æœ¯ï¼Œå®ƒæ˜¯ä¸“é—¨è®¾è®¡ç”¨æ¥æœ‰æ•ˆæ•´åˆä¸åŒè¯­è¨€æˆ–ä»»åŠ¡è®­ç»ƒè¿‡çš„æ¨¡å‹ï¼ŒåŒæ—¶ä¿æŒæ€§èƒ½å¹¶å‡å°‘è®¡ç®—å¼€é”€ã€‚é€šè¿‡ä½é˜¶å’Œç¨€ç–ä¿®å‰ªç›¸ç»“åˆï¼ŒLoRS-Mergingä¿ç•™äº†å…³é”®ç»“æ„å¹¶æ¶ˆé™¤äº†å†—ä½™å‚æ•°ï¼Œå‡è½»äº†è¯­è¨€å¹²æ‰°å¹¶æé«˜äº†æ‰©å±•æ€§ã€‚åœ¨10ç§è¯­è¨€ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒLoRS-Mergingæ˜¾è‘—ä¼˜äºå¤šè¯­è¨€å¤šä»»åŠ¡è®­ç»ƒã€é¡ºåºè®­ç»ƒå’Œå…¶ä»–åˆå¹¶æ–¹æ³•ï¼Œåœ¨æ ‡å‡†åŒ–æ€§èƒ½ä¸Šæé«˜äº†è¶…è¿‡20%ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæ¨¡å‹åˆå¹¶ï¼Œå°¤å…¶æ˜¯LoRS-Mergingï¼Œæ˜¯S2Tåº”ç”¨ä¸­ä¼ ç»Ÿå¤šè¯­è¨€è®­ç»ƒç­–ç•¥çš„å¯æ‰©å±•å’Œæœ‰æ•ˆçš„è¡¥å……ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­è¨€å¤šæ ·æ€§å¯¹è¯­éŸ³è½¬æ–‡æœ¬ä»»åŠ¡æå‡ºäº†æŒ‘æˆ˜ã€‚</li>
<li>ä¼ ç»Ÿå¤šè¯­è¨€å¤šä»»åŠ¡è®­ç»ƒæ—¨åœ¨åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œä½†ä»å­˜åœ¨è®¡ç®—æˆæœ¬é«˜ã€è¯­è¨€å¹²æ‰°ç­‰é—®é¢˜ã€‚</li>
<li>LoRS-MergingæŠ€æœ¯é€šè¿‡æ•´åˆä¸åŒè¯­è¨€æˆ–ä»»åŠ¡çš„æ¨¡å‹æ¥æé«˜æ•ˆç‡ã€‚</li>
<li>LoRS-Mergingç»“åˆäº†ä½é˜¶å’Œç¨€ç–ä¿®å‰ªï¼Œä»¥æ¶ˆé™¤å†—ä½™å‚æ•°å¹¶ä¿ç•™å…³é”®ç»“æ„ã€‚</li>
<li>LoRS-Mergingåœ¨å¤šç§è¯­è¨€ä¸Šçš„å®éªŒè¡¨ç°ä¼˜äºå…¶ä»–è®­ç»ƒæ–¹æ³•ã€‚</li>
<li>LoRS-Mergingæé«˜äº†æ¨¡å‹çš„æ‰©å±•æ€§ï¼Œå¹¶å¯ä½œä¸ºä¼ ç»Ÿå¤šè¯­è¨€è®­ç»ƒç­–ç•¥çš„æœ‰æ•ˆè¡¥å……ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.17380">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-29be6502827a607505489f0c3802f9df.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-95bbe15e79f1c53c08e34f15362baba0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-17b4ffc6fd8d4b280679dd50fe97ac7a.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-07-10/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-07-10/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-10/GAN/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-cb0fa0a1c11e440719d655aa9a1a20f9.jpg" class="responsive-img" alt="GAN">
                        
                        <span class="card-title">GAN</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            GAN æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-10  ScoreAdv Score-based Targeted Generation of Natural Adversarial   Examples via Diffusion Models
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-07-10
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/GAN/" class="post-category">
                                    GAN
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/GAN/">
                        <span class="chip bg-color">GAN</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-10/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-1269d4a30d4a85ab9d63fa4fbf1c40a3.jpg" class="responsive-img" alt="äººè„¸ç›¸å…³">
                        
                        <span class="card-title">äººè„¸ç›¸å…³</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            äººè„¸ç›¸å…³ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-10  CorrDetail Visual Detail Enhanced Self-Correction for Face Forgery   Detection
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-07-10
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/" class="post-category">
                                    äººè„¸ç›¸å…³
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/">
                        <span class="chip bg-color">äººè„¸ç›¸å…³</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">30191.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
