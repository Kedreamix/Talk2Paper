<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech 方向最新论文已更新，请持续关注 Update in 2025-07-10  Differentiable Reward Optimization for LLM based TTS system">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-aa7ffa0896d7eb265d5b8c30cd89da1b.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-07-10
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-08-19
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    8.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    34 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-07-10-更新"><a href="#2025-07-10-更新" class="headerlink" title="2025-07-10 更新"></a>2025-07-10 更新</h1><h2 id="Differentiable-Reward-Optimization-for-LLM-based-TTS-system"><a href="#Differentiable-Reward-Optimization-for-LLM-based-TTS-system" class="headerlink" title="Differentiable Reward Optimization for LLM based TTS system"></a>Differentiable Reward Optimization for LLM based TTS system</h2><p><strong>Authors:Changfeng Gao, Zhihao Du, Shiliang Zhang</strong></p>
<p>This paper proposes a novel Differentiable Reward Optimization (DiffRO) method aimed at enhancing the performance of neural codec language models based text-to-speech (TTS) systems. In contrast to conventional reinforcement learning from human feedback (RLHF) approaches applied to TTS, DiffRO directly compute the rewards based on neural codec tokens, rather than relying on synthesized audio. Furthermore, we employ the Gumbel-Softmax technique to render the reward function differentiable, thereby streamlining the RLHF training process. Additionally, we introduce a multi-task reward (MTR) model which can provide feedback from different perspectives and find that it can augment the system’s capability to follow instructions effectively.Experimental results indicate that DiffRO significantly improves the pronunciation accuracy of the TTS system, achieving state-of-the-art (SOTA) WER results on the seed-tts-eval benchmark. Moreover, with the integration of the MTR model, we demonstrate the ability to control emotional and quality attributes in a zero-shot manner. </p>
<blockquote>
<p>本文提出了一种新型的可微分奖励优化（DiffRO）方法，旨在提高基于神经编码语言模型的文本到语音（TTS）系统的性能。与传统的应用于TTS的人反馈强化学习（RLHF）方法相比，DiffRO直接基于神经编码令牌计算奖励，而不是依赖于合成音频。此外，我们采用Gumbel-Softmax技术使奖励函数可微分，从而简化了RLHF训练过程。我们还引入了一个多任务奖励（MTR）模型，可以从不同的角度提供反馈，并发现它可以增强系统有效遵循指令的能力。实验结果表明，DiffRO显著提高了TTS系统的发音准确性，在seed-tts-eval基准测试中实现了最先进的词错误率（WER）结果。而且，通过整合MTR模型，我们展示了零样本方式控制情感和品质属性的能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.05911v1">PDF</a> </p>
<p><strong>Summary</strong><br>本文提出了一种名为可微奖励优化（DiffRO）的新方法，旨在提高基于神经编码语言模型的文本到语音（TTS）系统的性能。与常规的应用于TTS的基于人类反馈的强化学习（RLHF）方法不同，DiffRO直接基于神经编码令牌计算奖励，而不是依赖于合成的音频。此外，本文采用Gumbel-Softmax技术使奖励函数可微，从而简化了RLHF训练过程。同时，引入多任务奖励（MTR）模型，可从不同角度提供反馈，提高了系统遵循指令的有效性。实验结果表明，DiffRO显著提高了TTS系统的发音准确性，在seed-tts-eval基准测试中实现了最先进的词错误率（WER）结果。而且，通过整合MTR模型，我们展示了零样本方式控制情感和品质属性的能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DiffRO方法被提出以提高基于神经编码语言模型的TTS系统性能。</li>
<li>与传统RLHF方法不同，DiffRO直接基于神经编码令牌计算奖励。</li>
<li>Gumbel-Softmax技术被用于使奖励函数可微，简化了RLHF训练过程。</li>
<li>引入多任务奖励（MTR）模型，从不同角度提供反馈，提高系统遵循指令的能力。</li>
<li>DiffRO显著提高TTS系统的发音准确性，达到最先进的WER结果。</li>
<li>MTR模型的整合使系统能够零样本方式控制情感和品质属性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.05911">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9858c76d77fbf47c8b02ecadc6cdb853.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-159d914ae0345d310414194c07f45363.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-83cb962fd27f21fcf841f8ae401703c9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7b295084b3dfb233b46d22757a022926.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="ContextASR-Bench-A-Massive-Contextual-Speech-Recognition-Benchmark"><a href="#ContextASR-Bench-A-Massive-Contextual-Speech-Recognition-Benchmark" class="headerlink" title="ContextASR-Bench: A Massive Contextual Speech Recognition Benchmark"></a>ContextASR-Bench: A Massive Contextual Speech Recognition Benchmark</h2><p><strong>Authors:He Wang, Linhan Ma, Dake Guo, Xiong Wang, Lei Xie, Jin Xu, Junyang Lin</strong></p>
<p>Automatic Speech Recognition (ASR) has been extensively investigated, yet prior evaluative efforts have largely been restricted to contextless paradigms. This constraint stems from the limited proficiency of conventional ASR models in context modeling and their deficiency in memory and reasoning based on world knowledge. Recent breakthroughs in the development of Large Language Models (LLMs) and corresponding Large Audio Language Models (LALMs) have markedly enhanced the visibility of general artificial intelligence capabilities. Consequently, there exists a compelling need for a benchmark that can evaluate both the generality and intelligence of ASR systems. To address this gap, we propose ContextASR-Bench: a comprehensive, large-scale benchmark designed to assess contextual speech recognition. This benchmark encompasses up to 40,000 data entries across over 10 domains, enabling a thorough evaluation of model performance in scenarios that omit or incorporate coarse-grained or fine-grained contextual information. Moreover, diverging from conventional ASR evaluations, our benchmark includes an analysis of model efficacy in recognizing named entities mentioned within the auditory input. Our extensive evaluation highlights that LALMs, with strong world knowledge and context learning capabilities, outperform conventional ASR models by a large margin. The dataset and evaluation code have been released at <a target="_blank" rel="noopener" href="https://github.com/MrSupW/ContextASR-Bench">https://github.com/MrSupW/ContextASR-Bench</a>. </p>
<blockquote>
<p>自动语音识别（ASR）已经得到了广泛的研究，但之前的评估工作大多局限于无上下文的模式。这一限制源于传统ASR模型在上下文建模方面的有限能力，以及它们在基于世界知识的记忆和推理方面的不足。最近，大型语言模型（LLM）和相应的大型音频语言模型（LALM）的发展取得了突破，显著提高了通用人工智能能力可见度。因此，存在一种迫切需要对能够评估ASR系统通用性和智能性的基准测试。为解决这一空白，我们提出了ContextASR-Bench：一个全面、大规模的基准测试，旨在评估上下文语音识别。该基准测试包含超过10个领域的多达4万个数据条目，能够在省略或包含粗粒度或细粒度上下文信息的场景中彻底评估模型性能。此外，与传统的ASR评估不同，我们的基准测试还包括对模型识别听觉输入中提到的命名实体的功效的分析。我们的广泛评估强调，具有强大世界知识和上下文学习能力的LALM在性能上大大超过了传统ASR模型。数据集和评估代码已发布在<a target="_blank" rel="noopener" href="https://github.com/MrSupW/ContextASR-Bench%E3%80%82">https://github.com/MrSupW/ContextASR-Bench。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.05727v1">PDF</a> 18 pages, 4 figures</p>
<p><strong>Summary</strong></p>
<p>本文介绍了自动语音识别（ASR）的新挑战，传统ASR模型在语境建模方面的局限性使其缺乏记忆和基于世界知识的推理能力。随着大型语言模型（LLM）和大型音频语言模型（LALM）的发展，有必要建立一个能够评估ASR系统的通用性和智能性的基准测试。为此，提出了ContextASR-Bench基准测试，该测试包含多达4万个数据条目，跨越10个领域，并包括在包含或排除粗粒度或细粒度上下文信息的场景中对模型性能的综合评估。分析发现，具有强大世界知识和上下文学习能力的LALM明显优于传统ASR模型。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>传统ASR模型在语境建模方面存在局限性，缺乏记忆和基于世界知识的推理能力。</li>
<li>大型语言模型（LLM）和大型音频语言模型（LALM）的发展为ASR系统带来了新的可能性。</li>
<li>ContextASR-Bench基准测试是为了评估ASR系统的通用性和智能性而提出的。</li>
<li>ContextASR-Bench包含多达4万个数据条目，跨越10个领域，可全面评估模型性能。</li>
<li>ContextASR-Bench包括在包含或排除上下文信息的场景中对模型性能的分析。</li>
<li>LALM在模型性能上明显优于传统ASR模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.05727">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-68c2bf82ab30fcbe5c7fcd2fd14e9a85.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-19d2138e196f212094316f5f99a61000.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-493a36da15ad19e22ece8791a4556980.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-29d58ece8ac512128debff40dc7de13a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8c1739b4776eab714091d4a342a6653d.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Robust-One-step-Speech-Enhancement-via-Consistency-Distillation"><a href="#Robust-One-step-Speech-Enhancement-via-Consistency-Distillation" class="headerlink" title="Robust One-step Speech Enhancement via Consistency Distillation"></a>Robust One-step Speech Enhancement via Consistency Distillation</h2><p><strong>Authors:Liang Xu, Longfei Felix Yan, W. Bastiaan Kleijn</strong></p>
<p>Diffusion models have shown strong performance in speech enhancement, but their real-time applicability has been limited by multi-step iterative sampling. Consistency distillation has recently emerged as a promising alternative by distilling a one-step consistency model from a multi-step diffusion-based teacher model. However, distilled consistency models are inherently biased towards the sampling trajectory of the teacher model, making them less robust to noise and prone to inheriting inaccuracies from the teacher model. To address this limitation, we propose ROSE-CD: Robust One-step Speech Enhancement via Consistency Distillation, a novel approach for distilling a one-step consistency model. Specifically, we introduce a randomized learning trajectory to improve the model’s robustness to noise. Furthermore, we jointly optimize the one-step model with two time-domain auxiliary losses, enabling it to recover from teacher-induced errors and surpass the teacher model in overall performance. This is the first pure one-step consistency distillation model for diffusion-based speech enhancement, achieving 54 times faster inference speed and superior performance compared to its 30-step teacher model. Experiments on the VoiceBank-DEMAND dataset demonstrate that the proposed model achieves state-of-the-art performance in terms of speech quality. Moreover, its generalization ability is validated on both an out-of-domain dataset and real-world noisy recordings. </p>
<blockquote>
<p>扩散模型在语音增强方面表现出强大的性能，但其实时应用性受到多步迭代采样的限制。近期，一致性蒸馏作为一种有前景的替代方案崭露头角，它通过从基于多步扩散的教师模型中蒸馏出一步一致性模型。然而，蒸馏出的一致性模型本质上偏向于教师模型的采样轨迹，导致其对噪声的鲁棒性较低，并容易继承教师模型的误差。为了解决这个问题，我们提出了ROSE-CD：通过一致性蒸馏实现稳健的一步语音增强。这是一种蒸馏一步一致性模型的新方法。具体来说，我们引入随机学习轨迹来提高模型对噪声的鲁棒性。此外，我们还通过两个时域辅助损失联合优化一步模型，使其能够从教师引发的错误中恢复，并在总体性能上超越教师模型。这是首个用于基于扩散的语音增强纯一步一致性蒸馏模型，其推理速度达到教师模型的54倍，且性能更为优越。在VoiceBank-DEMAND数据集上的实验表明，该模型在语音质量方面达到了最先进的性能。此外，其在域外数据集和真实世界噪声记录上的泛化能力也得到了验证。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.05688v1">PDF</a> Accepted to IEEE WASPAA 2025. 6 pages, 1 figures</p>
<p><strong>Summary</strong></p>
<p>本文提出了一种基于一致性蒸馏的鲁棒一步语音增强方法（ROSE-CD），用于解决扩散模型在语音增强中的实时应用问题。通过引入随机学习轨迹和联合优化一步模型与两个时域辅助损失，提高了模型的抗噪声能力和从教师模型诱导的错误中恢复的能力。实验结果表明，该模型在语音质量方面达到了最先进的性能，具有出色的泛化能力和快速推理速度。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型在语音增强中表现出强大的性能，但实时应用受到多步迭代采样的限制。</li>
<li>一致性蒸馏是一种新兴的有前途的替代方法，通过蒸馏一步一致性模型从多步扩散教师模型中。</li>
<li>蒸馏的一致性模型偏向于教师模型的采样轨迹，对噪声的鲁棒性较低，并可能继承教师模型的不准确之处。</li>
<li>ROSE-CD方法通过引入随机学习轨迹来提高模型的抗噪声能力。</li>
<li>ROSE-CD通过联合优化一步模型与两个时域辅助损失，使模型能够从教师模型诱导的错误中恢复并超越教师模型的整体性能。</li>
<li>ROSE-CD是在扩散语音增强中首次提出的纯一步一致性蒸馏模型，具有快速推理速度和卓越性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.05688">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d1a217f1daf61665fc84e8bca285726f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-989558c152394972a25f9b4913f4fdf5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-db113b607707b16166be86cd9d517e3f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ebeae83fe2826ae5030b4561b48e6de1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-11cf2f0782ef6670b1e90a8ece263a77.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="ADMC-Attention-based-Diffusion-Model-for-Missing-Modalities-Feature-Completion"><a href="#ADMC-Attention-based-Diffusion-Model-for-Missing-Modalities-Feature-Completion" class="headerlink" title="ADMC: Attention-based Diffusion Model for Missing Modalities Feature   Completion"></a>ADMC: Attention-based Diffusion Model for Missing Modalities Feature   Completion</h2><p><strong>Authors:Wei Zhang, Juan Chen, Yanbo J. Wang, En Zhu, Xuan Yang, Yiduo Wang</strong></p>
<p>Multimodal emotion and intent recognition is essential for automated human-computer interaction, It aims to analyze users’ speech, text, and visual information to predict their emotions or intent. One of the significant challenges is that missing modalities due to sensor malfunctions or incomplete data. Traditional methods that attempt to reconstruct missing information often suffer from over-coupling and imprecise generation processes, leading to suboptimal outcomes. To address these issues, we introduce an Attention-based Diffusion model for Missing Modalities feature Completion (ADMC). Our framework independently trains feature extraction networks for each modality, preserving their unique characteristics and avoiding over-coupling. The Attention-based Diffusion Network (ADN) generates missing modality features that closely align with authentic multimodal distribution, enhancing performance across all missing-modality scenarios. Moreover, ADN’s cross-modal generation offers improved recognition even in full-modality contexts. Our approach achieves state-of-the-art results on the IEMOCAP and MIntRec benchmarks, demonstrating its effectiveness in both missing and complete modality scenarios. </p>
<blockquote>
<p>多模态情感与意图识别是自动化人机交互的核心，其目标是通过分析用户的语音、文本和视觉信息来预测其情感或意图。面临的挑战之一是传感器故障或数据不完整导致的模态缺失。传统的方法试图重建缺失的信息，但常常受到过度耦合和不精确生成过程的影响，导致结果不理想。为了解决这些问题，我们引入了基于注意力的扩散模型（ADMC）进行缺失模态特征补全。我们的框架独立训练每种模态的特征提取网络，保留其独特特性，避免过度耦合。基于注意力的扩散网络（ADN）生成缺失模态的特征，这些特征紧密符合真实的多模态分布，提高了所有缺失模态场景的性能。此外，ADN的跨模态生成甚至在完整模态上下文中也提供了改进的识别能力。我们的方法在IEMOCAP和MIntRec基准测试中取得了最新结果，证明了它在缺失和完整模态场景中的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.05624v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>多模态情感与意图识别在自动化人机交互中至关重要。它通过用户的语音、文本和视觉信息分析来预测用户的情感或意图。面对传感器故障或数据不完整导致的缺失模态问题，传统方法往往存在过度耦合和不精确生成过程的问题，导致结果不佳。为此，我们引入了基于注意力的扩散模型用于缺失模态特征补全（ADMC）。我们的框架独立训练每种模态的特征提取网络，保留其独特性并避免过度耦合。基于注意力的扩散网络（ADN）生成的缺失模态特征能与真实的多模态分布紧密对齐，提升各种缺失模态场景的性能。此外，ADN的跨模态生成能力在全模态上下文中也能提供改进后的识别效果。我们的方法在IEMOCAP和MIntRec基准测试中取得了最新成果，证明了它在缺失和完整模态场景中的有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态情感与意图识别在自动化人机交互中的重要性。</li>
<li>缺失模态是自动化识别的一个挑战，传统方法存在过度耦合和不精确生成的问题。</li>
<li>引入基于注意力的扩散模型用于缺失模态特征补全（ADMC）。</li>
<li>ADMC框架独立训练每种模态的特征提取网络，避免过度耦合。</li>
<li>基于注意力的扩散网络（ADN）生成的缺失模态特征与真实多模态分布对齐，提升性能。</li>
<li>ADN的跨模态生成能力在全模态上下文中也能提供改进后的识别效果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.05624">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-dce98e467c368495760e6da62242ca4b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fae18c8a487a716d67d9a6e430d4f3ea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e62ca0370984128149a32d3e239b7357.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="MLlm-DR-Towards-Explainable-Depression-Recognition-with-MultiModal-Large-Language-Models"><a href="#MLlm-DR-Towards-Explainable-Depression-Recognition-with-MultiModal-Large-Language-Models" class="headerlink" title="MLlm-DR: Towards Explainable Depression Recognition with MultiModal   Large Language Models"></a>MLlm-DR: Towards Explainable Depression Recognition with MultiModal   Large Language Models</h2><p><strong>Authors:Wei Zhang, Juan Chen, En Zhu, Wenhong Cheng, YunPeng Li, Yanbo J. Wang</strong></p>
<p>Automated depression diagnosis aims to analyze multimodal information from interview videos to predict participants’ depression scores. Previous studies often lack clear explanations of how these scores were determined, limiting their adoption in clinical practice. While the advent of LLMs provides a possible pathway for explainable depression diagnosis, current LLMs capable of processing multimodal data lack training on interview data, resulting in poor diagnostic performance when used directly. In this paper, we propose a novel multimodal large language model (MLlm-DR) that can understand multimodal information inputs and supports explainable depression diagnosis. MLlm-DR integrates a smaller LLMs and a lightweight query module (LQ-former). Specifically, the smaller LLMs is designed to generate depression scores and corresponding evaluation rationales. To enhance its logical reasoning for domain-specific tasks while maintaining practicality, we constructed a robust training dataset to fine-tune it. Meanwhile, the LQ-former captures depression-related features from speech and visual data, aiding the model’s ability to process multimodal information, to achieve comprehensive depression diagnosis. Our approach achieves state-of-the-art results on two interview-based benchmark datasets, CMDC and E-DAIC-WOZ, demonstrating its effectiveness and superiority. </p>
<blockquote>
<p>自动化抑郁症诊断旨在分析访谈视频中的多模式信息，以预测参与者的抑郁评分。之前的研究往往缺乏关于这些评分如何确定的明确解释，这限制了它们在临床实践中的应用。虽然大型语言模型（LLMs）的出现为可解释的抑郁症诊断提供了一条可能的途径，但目前能够处理多模式数据的大型语言模型缺乏访谈数据的训练，导致当直接使用时诊断性能较差。在本文中，我们提出了一种新型的多模式大型语言模型（MLlm-DR），能够理解多模式信息输入并支持可解释的抑郁症诊断。MLlm-DR整合了较小的大型语言模型（LLMs）和一个轻量级的查询模块（LQ-former）。具体来说，较小的LLMs被设计用来生成抑郁评分和相应的评估依据。为了增强其在特定领域的逻辑推理能力同时保持实用性，我们构建了一个稳健的训练数据集对其进行微调。同时，LQ-former从语音和视觉数据中捕获与抑郁症相关的特征，帮助模型处理多模式信息，以实现全面的抑郁症诊断。我们的方法在基于访谈的两个基准数据集CMDC和E-DAIC-WOZ上取得了最新成果，证明了其有效性和优越性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.05591v1">PDF</a> </p>
<p><strong>Summary</strong><br>自动化抑郁症诊断旨在通过分析访谈视频中的多模态信息来预测参与者的抑郁程度。然而，先前的研究往往缺乏明确解释这些分数的确定方式，限制了其在临床实践中的应用。本文提出了一种新型的多模态大型语言模型（MLlm-DR），能够处理多模态信息输入并支持可解释的抑郁症诊断。该模型结合了较小的语言模型和轻量级查询模块（LQ-former），以生成抑郁分数和相应的评估依据，并通过构建稳健的训练数据集进行微调，提高其领域特定任务的逻辑推理能力，同时保持实用性。LQ-former有助于模型处理多模态信息，在两种基于访谈的基准数据集CMDC和E-DAIC-WOZ上取得了最先进的成果，证明了其有效性和优越性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>自动化抑郁症诊断通过分析访谈视频中的多模态信息预测参与者的抑郁程度。</li>
<li>先前研究缺乏明确的诊断分数解释，限制了其在临床实践中的应用。</li>
<li>提出了一种新型的多模态大型语言模型（MLlm-DR）用于支持可解释的抑郁症诊断。</li>
<li>MLlm-DR结合了较小的语言模型和轻量级查询模块（LQ-former）。</li>
<li>较小的语言模型用于生成抑郁分数和评估依据。</li>
<li>通过构建稳健的训练数据集对模型进行微调，提高领域任务的逻辑推理能力并维持实用性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.05591">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-77ff395e79e59157305a6ed4ec577976.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c79532f3630f42f218eb0581c9f7d263.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-26df7c4561da3aae2838cafbae17ac08.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-83fd8c93297339b1e313f84dd49ba2fe.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="SHNU-Multilingual-Conversational-Speech-Recognition-System-for-INTERSPEECH-2025-MLC-SLM-Challenge"><a href="#SHNU-Multilingual-Conversational-Speech-Recognition-System-for-INTERSPEECH-2025-MLC-SLM-Challenge" class="headerlink" title="SHNU Multilingual Conversational Speech Recognition System for   INTERSPEECH 2025 MLC-SLM Challenge"></a>SHNU Multilingual Conversational Speech Recognition System for   INTERSPEECH 2025 MLC-SLM Challenge</h2><p><strong>Authors:Yuxiang Mei, Yuang Zheng, Dongxing Xu, Yanhua Long</strong></p>
<p>This paper describes SHNU multilingual conversational speech recognition system (SHNU-mASR, team name-“maybe”), submitted to Track 1 of the INTERSPEECH 2025 MLC-SLM Challenge. Our system integrates a parallel-speech-encoder architecture with a large language model (LLM) to form a unified multilingual ASR framework. The parallel-speech-encoder consists of two pre-trained encoders, the Whisper-large-v3 encoder and mHuBERT-147 encoder. Their output embeddings are concatenated and fed into the LLM, enabling the model to leverage complementary acoustic and linguistic knowledge and achieve competitive performance. Moreover, we adopt a tri-stage training strategy to jointly update the low-rank adaptation modules and projector parameters of both the speech encoders and the LLM. In addition, we incorporate an additional language-aware prompt at the LLM input to enhance language-specific text generation. The SHNU-mASR system achieves an overall character&#x2F;word error rate (CER&#x2F;WER) of 11.76% on the blind evaluation set of the challenge, outperforming the official MLC-SLM baseline by 8.41 absolute CER&#x2F;WER, without increasing the baseline training data. </p>
<blockquote>
<p>本文介绍了SHNU多语种对话语音识别系统（SHNU-mASR，团队名为“maybe”），该系统提交至INTERSPEECH 2025 MLC-SLM Challenge的Track 1。我们的系统将并行语音编码器架构与大型语言模型（LLM）集成在一起，形成一个统一的多语种ASR框架。并行语音编码器由两个预训练编码器组成，即Whisper-large-v3编码器和mHuBERT-147编码器。他们的输出嵌入进行拼接，并输入到LLM中，使模型能够利用互补的声学知识和语言知识，实现有竞争力的性能。此外，我们采用三阶段训练策略，联合更新低秩适应模块和语音编码器和LLM的投影仪参数。另外，我们在LLM的输入端增加了一个额外的语言感知提示，以增强特定语言的文本生成。SHNU-mASR系统在挑战盲评数据集上实现了11.76%的整体字符&#x2F;单词错误率（CER&#x2F;WER），相较于官方MLC-SLM基线提高了8.41个绝对CER&#x2F;WER，且没有增加基线训练数据。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.03343v2">PDF</a> Accepted by Interspeech 2025 MLC-SLM workshop</p>
<p><strong>Summary</strong></p>
<p>本文介绍了SHNU的多语种对话语音识别系统（SHNU-mASR），该系统参与了INTERSPEECH 2025 MLC-SLM Challenge的Track 1挑战。该系统结合了并行语音编码器架构和大型语言模型（LLM），形成一个统一的多语种ASR框架。采用预训练的Whisper-large-v3和mHuBERT-147编码器，其输出嵌入通过连接并输入到LLM中，使得模型能够利用互补的声学知识和语言知识，实现了良好的性能。通过采用三阶段训练策略，对低阶适应模块和投影参数进行联合更新，进一步增强了系统的性能。在挑战的无标注评估集上，SHNU-mASR系统的字符&#x2F;词错误率（CER&#x2F;WER）达到了11.76%，相较于官方MLC-SLM基线提高了8.41个绝对CER&#x2F;WER，且未增加基线训练数据。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SHNU团队提出了一种多语种对话语音识别系统SHNU-mASR，针对INTERSPEECH 2025 MLC-SLM Challenge的Track 1进行了挑战。</li>
<li>系统集成了并行语音编码器架构和大型语言模型（LLM），形成统一的多语种ASR框架。</li>
<li>采用预训练的Whisper-large-v3和mHuBERT-147编码器，使模型能够利用互补的声学知识和语言知识。</li>
<li>通过三阶段训练策略联合更新低阶适应模块和投影参数，增强了系统性能。</li>
<li>SHNU-mASR系统实现了较低的字符&#x2F;词错误率（CER&#x2F;WER），相较于官方基线有显著的提升。</li>
<li>该提升是在不增加基线训练数据的情况下实现的。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.03343">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ab69e91ecf1b3eb3fc743fb37adbcc7f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-78728db70d2a6f96c98bfc01bd386bbd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-868c350da442b3a2455601df5b764564.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c6393ee73c68038700dfe5e4af37aab1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8ff5b76954df33b73e355b423be7e28e.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Evaluating-Logit-Based-GOP-Scores-for-Mispronunciation-Detection"><a href="#Evaluating-Logit-Based-GOP-Scores-for-Mispronunciation-Detection" class="headerlink" title="Evaluating Logit-Based GOP Scores for Mispronunciation Detection"></a>Evaluating Logit-Based GOP Scores for Mispronunciation Detection</h2><p><strong>Authors:Aditya Kamlesh Parikh, Cristian Tejedor-Garcia, Catia Cucchiarini, Helmer Strik</strong></p>
<p>Pronunciation assessment relies on goodness of pronunciation (GOP) scores, traditionally derived from softmax-based posterior probabilities. However, posterior probabilities may suffer from overconfidence and poor phoneme separation, limiting their effectiveness. This study compares logit-based GOP scores with probability-based GOP scores for mispronunciation detection. We conducted our experiment on two L2 English speech datasets spoken by Dutch and Mandarin speakers, assessing classification performance and correlation with human ratings. Logit-based methods outperform probability-based GOP in classification, but their effectiveness depends on dataset characteristics. The maximum logit GOP shows the strongest alignment with human perception, while a combination of different GOP scores balances probability and logit features. The findings suggest that hybrid GOP methods incorporating uncertainty modeling and phoneme-specific weighting improve pronunciation assessment. </p>
<blockquote>
<p>发音评估依赖于发音质量（GOP）分数，这些分数传统上是从基于softmax的后验概率中得出的。然而，后验概率可能会受到过度自信和音素分离不良的影响，从而限制了其有效性。本研究对比了基于对数几率（logit）的GOP分数与基于概率的GOP分数在发音错误检测方面的表现。我们在由荷兰语和普通话发音的两组英语二语语音数据集上进行了实验，评估了分类性能和与人类评分的相关性。基于对数几率的方法在分类上优于基于概率的GOP，但其有效性取决于数据集的特性。最大对数几率GOP与人的感知一致性最强，而结合不同的GOP分数则平衡了概率和对数几率特征。研究结果表明，采用不确定性建模和音素特定权重的混合GOP方法能改善发音评估效果。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.12067v2">PDF</a> Accepted to Interspeech 2025. This publication is part of the project   Responsible AI for Voice Diagnostics (RAIVD) with file number NGF.1607.22.013   of the research programme NGF AiNed Fellowship Grants which is financed by   the Dutch Research Council (NWO)</p>
<p><strong>总结</strong></p>
<p>本文主要探讨了发音评估中的好发音（GOP）得分计算方法。研究发现基于logit的GOP得分计算方法相较于传统的基于概率的得分计算方法能更好地用于发音错误的检测。此外，不同数据集下基于对数概率得分方法的最佳应用方式也进行了研究，发现最大对数概率得分与人的感知最为一致，而结合不同GOP得分的方法可以平衡概率和对数概率特征。研究还表明，结合不确定性建模和音素特定权重的混合GOP方法能提高发音评估的准确度。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>基于对数概率（logit）的GOP得分计算方法相较于基于概率的方法在发音评估中表现更优。</li>
<li>最大对数概率GOP得分与人的感知最为一致。</li>
<li>结合不同GOP得分的混合方法能够平衡概率和对数概率特征，提高评估准确性。</li>
<li>数据集特性会影响基于对数概率方法的性能。</li>
<li>基于不确定性建模的混合GOP方法能进一步提高发音评估的精确度。</li>
<li>音素特定权重在发音评估中起到重要作用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.12067">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-aa7ffa0896d7eb265d5b8c30cd89da1b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4d2ffefcadb7986d12abc1b06a4ee9d9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-19b13486a955b3416d1668dda634dda8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6301d21768c3ca88aa615318e14eaba1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cb336a761db775dc178b43f37690d370.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Enhancing-GOP-in-CTC-Based-Mispronunciation-Detection-with-Phonological-Knowledge"><a href="#Enhancing-GOP-in-CTC-Based-Mispronunciation-Detection-with-Phonological-Knowledge" class="headerlink" title="Enhancing GOP in CTC-Based Mispronunciation Detection with Phonological   Knowledge"></a>Enhancing GOP in CTC-Based Mispronunciation Detection with Phonological   Knowledge</h2><p><strong>Authors:Aditya Kamlesh Parikh, Cristian Tejedor-Garcia, Catia Cucchiarini, Helmer Strik</strong></p>
<p>Computer-Assisted Pronunciation Training (CAPT) systems employ automatic measures of pronunciation quality, such as the goodness of pronunciation (GOP) metric. GOP relies on forced alignments, which are prone to labeling and segmentation errors due to acoustic variability. While alignment-free methods address these challenges, they are computationally expensive and scale poorly with phoneme sequence length and inventory size. To enhance efficiency, we introduce a substitution-aware alignment-free GOP that restricts phoneme substitutions based on phoneme clusters and common learner errors. We evaluated our GOP on two L2 English speech datasets, one with child speech, My Pronunciation Coach (MPC), and SpeechOcean762, which includes child and adult speech. We compared RPS (restricted phoneme substitutions) and UPS (unrestricted phoneme substitutions) setups within alignment-free methods, which outperformed the baseline. We discuss our results and outline avenues for future research. </p>
<blockquote>
<p>计算机辅助发音训练（CAPT）系统采用发音质量自动测量，如发音质量（GOP）指标。GOP依赖于强制对齐，由于声学变化，它容易受到标记和分段错误的影响。虽然无对齐方法解决了这些挑战，但它们在计算上很昂贵，随着音素序列长度和库存大小的增加，性能下降。为了提高效率，我们引入了一种基于音素聚类和学习者常见错误的替代感知无对齐GOP。我们在两个英语二级语音数据集上评估了我们的GOP，其中一个数据集包含儿童语音My Pronunciation Coach（MPC），另一个数据集包含儿童和成人语音的SpeechOcean762。我们在无对齐方法中比较了RPS（受限音素替代）和UPS（非受限音素替代）设置，它们优于基线水平。我们讨论了我们的结果并概述了未来研究的方向。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02080v2">PDF</a> Accepted to Interspeech 2025. This publication is part of the project   Responsible AI for Voice Diagnostics (RAIVD) with file number NGF.1607.22.013   of the research programme NGF AiNed Fellowship Grants which is financed by   the Dutch Research Council (NWO)</p>
<p><strong>Summary</strong></p>
<p>本文介绍了计算机辅助发音训练（CAPT）系统采用基于强制对齐的发音质量评估方法，如发音质量评估（GOP）指标。然而，由于声学变化导致的标签和分段误差影响了强制对齐方法的准确性。为了解决这个问题，研究者提出了一种基于音素聚类和常见学习者错误的替代感知对齐无关的GOP方法，以提高效率。该方法在两种英语二语语音数据集上的表现优于基线方法，包括针对儿童语音的My Pronunciation Coach数据集和包含儿童和成人语音的SpeechOcean762数据集。本文探讨了结果并展望了未来研究方向。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>计算机辅助发音训练（CAPT）系统使用自动评估发音质量的方法，如发音质量评估（GOP）指标。</li>
<li>传统GOP依赖强制对齐，易受到声学变化的标签和分段误差影响。</li>
<li>提出了基于音素聚类和常见学习者错误的替代感知对齐无关的GOP方法，以提高效率。</li>
<li>该方法在两种英语二语语音数据集上的表现优于基线方法。</li>
<li>研究对比了受限音素替代（RPS）和无限制音素替代（UPS）的设定。</li>
<li>结果显示，对齐无关的方法在语音数据集上的性能优于基线方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02080">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-fbef8c9f6b963368f052349f4b6c0752.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c330a88a70fd19735e63e9eeea39c3ba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3c60998febc97474bc1e7013e61089d1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9af5182dff1bbab3a1904d421e438e4e.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Low-Rank-and-Sparse-Model-Merging-for-Multi-Lingual-Speech-Recognition-and-Translation"><a href="#Low-Rank-and-Sparse-Model-Merging-for-Multi-Lingual-Speech-Recognition-and-Translation" class="headerlink" title="Low-Rank and Sparse Model Merging for Multi-Lingual Speech Recognition   and Translation"></a>Low-Rank and Sparse Model Merging for Multi-Lingual Speech Recognition   and Translation</h2><p><strong>Authors:Qiuming Zhao, Guangzhi Sun, Chao Zhang</strong></p>
<p>Language diversity presents a significant challenge in speech-to-text (S2T) tasks, such as automatic speech recognition and translation. Traditional multi-lingual multi-task training approaches aim to address this by jointly optimising multiple speech recognition and translation tasks across various languages. While models like Whisper, built on these strategies, demonstrate strong performance, they still face issues of high computational cost, language interference, suboptimal training configurations, and limited extensibility. To overcome these challenges, we introduce LoRS-Merging (low-rank and sparse model merging), a novel technique designed to efficiently integrate models trained on different languages or tasks while preserving performance and reducing computational overhead. LoRS-Merging combines low-rank and sparse pruning to retain essential structures while eliminating redundant parameters, mitigating language interference, and enhancing extensibility. Experimental results across 10 languages demonstrate that LoRS-Merging significantly outperforms multi-lingual multi-task training, sequential training, and other merging methods, achieving over 20% improvement in normalised performance. Our findings suggest that model merging, particularly LoRS-Merging, is a scalable and effective complement to traditional multi-lingual training strategies for S2T applications. </p>
<blockquote>
<p>语言多样性在语音到文本（S2T）的任务中，如自动语音识别和翻译，构成了一个重大挑战。传统多语言多任务训练的方法旨在通过联合优化多种语言的语音识别和翻译任务来解决这一问题。虽然基于这些策略构建的模型（如whisper）表现出强大的性能，但它们仍然面临计算成本高、语言干扰、训练配置不佳和扩展性有限等问题。为了克服这些挑战，我们引入了LoRS-Merging（低秩和稀疏模型合并），这是一种旨在高效集成在不同语言或任务上训练的模型的新技术，同时保留性能并降低计算开销。LoRS-Merging结合了低秩和稀疏剪枝技术，以保留重要结构的同时消除冗余参数，减轻语言干扰并增强扩展性。在10种语言上的实验结果表明，LoRS-Merging显著优于多语言多任务训练、顺序训练和其他合并方法，在标准化性能上提高了超过20%。我们的研究结果表明，模型合并，特别是LoRS-Merging，是S2T应用中传统多语言训练策略的可扩展和有效的补充。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.17380v3">PDF</a> 13 pages</p>
<p><strong>Summary</strong></p>
<p>语言多样性在语音转文本（S2T）任务中，如自动语音识别和翻译中，带来了重大挑战。传统的多语言多任务训练旨在通过联合优化多种语言和任务的语音识别和翻译任务来解决这一问题。尽管基于这些策略构建的模型如Whisper表现出强大的性能，但它们仍然面临计算成本高、语言干扰、训练配置不佳和扩展性有限等问题。为了克服这些挑战，我们引入了LoRS-Merging（低阶和稀疏模型合并）技术，它是专门设计用来有效整合不同语言或任务训练过的模型，同时保持性能并减少计算开销。通过低阶和稀疏修剪相结合，LoRS-Merging保留了关键结构并消除了冗余参数，减轻了语言干扰并提高了扩展性。在10种语言上的实验结果表明，LoRS-Merging显著优于多语言多任务训练、顺序训练和其他合并方法，在标准化性能上提高了超过20%。我们的研究结果表明，模型合并，尤其是LoRS-Merging，是S2T应用中传统多语言训练策略的可扩展和有效的补充。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>语言多样性对语音转文本任务提出了挑战。</li>
<li>传统多语言多任务训练旨在应对这一挑战，但仍存在计算成本高、语言干扰等问题。</li>
<li>LoRS-Merging技术通过整合不同语言或任务的模型来提高效率。</li>
<li>LoRS-Merging结合了低阶和稀疏修剪，以消除冗余参数并保留关键结构。</li>
<li>LoRS-Merging在多种语言上的实验表现优于其他训练方法。</li>
<li>LoRS-Merging提高了模型的扩展性，并可作为传统多语言训练策略的有效补充。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.17380">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-29be6502827a607505489f0c3802f9df.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-95bbe15e79f1c53c08e34f15362baba0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-17b4ffc6fd8d4b280679dd50fe97ac7a.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-07-10/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-07-10/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-10/GAN/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-cb0fa0a1c11e440719d655aa9a1a20f9.jpg" class="responsive-img" alt="GAN">
                        
                        <span class="card-title">GAN</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            GAN 方向最新论文已更新，请持续关注 Update in 2025-07-10  ScoreAdv Score-based Targeted Generation of Natural Adversarial   Examples via Diffusion Models
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-07-10
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/GAN/" class="post-category">
                                    GAN
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/GAN/">
                        <span class="chip bg-color">GAN</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-10/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-1269d4a30d4a85ab9d63fa4fbf1c40a3.jpg" class="responsive-img" alt="人脸相关">
                        
                        <span class="card-title">人脸相关</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            人脸相关 方向最新论文已更新，请持续关注 Update in 2025-07-10  CorrDetail Visual Detail Enhanced Self-Correction for Face Forgery   Detection
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-07-10
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/" class="post-category">
                                    人脸相关
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/">
                        <span class="chip bg-color">人脸相关</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">30191.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
