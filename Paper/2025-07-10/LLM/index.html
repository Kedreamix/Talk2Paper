<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-10  A Survey on Latent Reasoning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-2c0960bebc3e38df91e408a734247a84.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-10
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-19
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    87 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-07-10-æ›´æ–°"><a href="#2025-07-10-æ›´æ–°" class="headerlink" title="2025-07-10 æ›´æ–°"></a>2025-07-10 æ›´æ–°</h1><h2 id="A-Survey-on-Latent-Reasoning"><a href="#A-Survey-on-Latent-Reasoning" class="headerlink" title="A Survey on Latent Reasoning"></a>A Survey on Latent Reasoning</h2><p><strong>Authors:Rui-Jie Zhu, Tianhao Peng, Tianhao Cheng, Xingwei Qu, Jinfa Huang, Dawei Zhu, Hao Wang, Kaiwen Xue, Xuanliang Zhang, Yong Shan, Tianle Cai, Taylor Kergan, Assel Kembay, Andrew Smith, Chenghua Lin, Binh Nguyen, Yuqi Pan, Yuhong Chou, Zefan Cai, Zhenhe Wu, Yongchi Zhao, Tianyu Liu, Jian Yang, Wangchunshu Zhou, Chujie Zheng, Chongxuan Li, Yuyin Zhou, Zhoujun Li, Zhaoxiang Zhang, Jiaheng Liu, Ge Zhang, Wenhao Huang, Jason Eshraghian</strong></p>
<p>Large Language Models (LLMs) have demonstrated impressive reasoning capabilities, especially when guided by explicit chain-of-thought (CoT) reasoning that verbalizes intermediate steps. While CoT improves both interpretability and accuracy, its dependence on natural language reasoning limits the modelâ€™s expressive bandwidth. Latent reasoning tackles this bottleneck by performing multi-step inference entirely in the modelâ€™s continuous hidden state, eliminating token-level supervision. To advance latent reasoning research, this survey provides a comprehensive overview of the emerging field of latent reasoning. We begin by examining the foundational role of neural network layers as the computational substrate for reasoning, highlighting how hierarchical representations support complex transformations. Next, we explore diverse latent reasoning methodologies, including activation-based recurrence, hidden state propagation, and fine-tuning strategies that compress or internalize explicit reasoning traces. Finally, we discuss advanced paradigms such as infinite-depth latent reasoning via masked diffusion models, which enable globally consistent and reversible reasoning processes. By unifying these perspectives, we aim to clarify the conceptual landscape of latent reasoning and chart future directions for research at the frontier of LLM cognition. An associated GitHub repository collecting the latest papers and repos is available at: <a target="_blank" rel="noopener" href="https://github.com/multimodal-art-projection/LatentCoT-Horizon/">https://github.com/multimodal-art-projection/LatentCoT-Horizon/</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¡¨ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„æ¨ç†èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨æ˜ç¡®çš„æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†å¼•å¯¼ä¸‹ï¼Œé€šè¿‡è¯­è¨€åŒ–ä¸­é—´æ­¥éª¤è€Œå±•ç°å‡ºçš„èƒ½åŠ›ã€‚è™½ç„¶æ€ç»´é“¾æ¨ç†æé«˜äº†å¯è§£é‡Šæ€§å’Œå‡†ç¡®æ€§ï¼Œä½†å®ƒå¯¹è‡ªç„¶è¯­è¨€æ¨ç†çš„ä¾èµ–é™åˆ¶äº†æ¨¡å‹çš„è¡¨è¾¾å¸¦å®½ã€‚æ½œåœ¨æ¨ç†é€šè¿‡å®Œå…¨åˆ©ç”¨æ¨¡å‹çš„è¿ç»­éšè—çŠ¶æ€è¿›è¡Œå¤šæ­¥æ¨ç†æ¥è§£å†³è¿™ä¸€ç“¶é¢ˆï¼Œæ¶ˆé™¤äº†åŸºäºæ ‡è®°çš„ç›‘ç£ã€‚ä¸ºäº†æ¨åŠ¨æ½œåœ¨æ¨ç†ç ”ç©¶çš„å‘å±•ï¼Œè¿™ç¯‡ç»¼è¿°æä¾›äº†å¯¹æ–°å…´æ½œåœ¨æ¨ç†é¢†åŸŸçš„å…¨é¢æ¦‚è¿°ã€‚æˆ‘ä»¬é¦–å…ˆæ¢è®¨äº†ç¥ç»ç½‘ç»œå±‚ä½œä¸ºæ¨ç†è®¡ç®—åŸºç¡€çš„ä½œç”¨ï¼Œå¼ºè°ƒäº†å±‚æ¬¡è¡¨ç¤ºå¦‚ä½•æ”¯æŒå¤æ‚è½¬æ¢ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æ¢ç´¢äº†å¤šç§æ½œåœ¨æ¨ç†æ–¹æ³•ï¼ŒåŒ…æ‹¬åŸºäºæ¿€æ´»çš„é€’å½’ã€éšè—çŠ¶æ€ä¼ æ’­ä»¥åŠå¾®è°ƒç­–ç•¥ï¼Œè¿™äº›ç­–ç•¥å¯ä»¥å‹ç¼©æˆ–å†…åŒ–æ˜¾æ€§æ¨ç†ç—•è¿¹ã€‚æœ€åï¼Œæˆ‘ä»¬è®¨è®ºäº†é«˜çº§èŒƒå¼ï¼Œå¦‚é€šè¿‡æ©ç æ‰©æ•£æ¨¡å‹å®ç°çš„æ— é™æ·±åº¦æ½œåœ¨æ¨ç†ï¼Œè¿™å¯ç”¨äº†å…¨å±€ä¸€è‡´æ€§å’Œå¯é€†çš„æ¨ç†è¿‡ç¨‹ã€‚é€šè¿‡ç»Ÿä¸€è¿™äº›è§‚ç‚¹ï¼Œæˆ‘ä»¬æ—¨åœ¨æ¾„æ¸…æ½œåœ¨æ¨ç†çš„æ¦‚å¿µæ ¼å±€ï¼Œå¹¶ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹è®¤çŸ¥å‰æ²¿çš„ç ”ç©¶æŒ‡æ˜æœªæ¥æ–¹å‘ã€‚ç›¸å…³çš„GitHubä»“åº“æ”¶é›†äº†æœ€æ–°çš„è®ºæ–‡å’Œä»“åº“èµ„æºï¼Œå¯åœ¨ä»¥ä¸‹é“¾æ¥æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/multimodal-art-projection/LatentCoT-Horizon/">https://github.com/multimodal-art-projection/LatentCoT-Horizon/</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.06203v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡æ˜ç¡®çš„æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†å±•ç¤ºå‡ºè‰²çš„æ¨ç†èƒ½åŠ›ï¼Œè¿™ç§æ¨ç†æ–¹å¼èƒ½å¤Ÿå£å¤´è¡¨è¾¾ä¸­é—´æ­¥éª¤ã€‚è™½ç„¶CoTæé«˜äº†å¯è§£é‡Šæ€§å’Œå‡†ç¡®æ€§ï¼Œä½†å®ƒå¯¹è‡ªç„¶è¯­è¨€æ¨ç†çš„ä¾èµ–é™åˆ¶äº†æ¨¡å‹çš„è¡¨è¾¾å¸¦å®½ã€‚æ½œåœ¨æ¨ç†é€šè¿‡å®Œå…¨åœ¨æ¨¡å‹çš„è¿ç»­éšè—çŠ¶æ€ä¸­è¿›è¡Œå¤šæ­¥æ¨ç†æ¥è§£å†³è¿™ä¸€ç“¶é¢ˆï¼Œä»è€Œæ¶ˆé™¤äº†ä»¤ç‰Œçº§åˆ«çš„ç›‘ç£ã€‚æœ¬æ–‡å…¨é¢æ¦‚è¿°äº†æ½œåœ¨æ¨ç†è¿™ä¸€æ–°å…´é¢†åŸŸçš„å‘å±•ã€‚æ–‡ç« é¦–å…ˆç ”ç©¶ç¥ç»ç½‘ç»œå±‚ä½œä¸ºæ¨ç†è®¡ç®—åŸºåº•çš„åŸºçŸ³ä½œç”¨ï¼Œå¹¶å¼ºè°ƒåˆ†å±‚è¡¨ç¤ºå¦‚ä½•æ”¯æŒå¤æ‚è½¬æ¢ã€‚ç„¶åï¼Œæ¢è®¨äº†å„ç§æ½œåœ¨æ¨ç†æ–¹æ³•ï¼ŒåŒ…æ‹¬åŸºäºæ¿€æ´»çš„é€’å½’ã€éšè—çŠ¶æ€ä¼ æ’­ä»¥åŠå¾®è°ƒç­–ç•¥ç­‰ï¼Œè¿™äº›ç­–ç•¥èƒ½å¤Ÿå‹ç¼©æˆ–å†…åŒ–æ˜¾æ€§æ¨ç†ç—•è¿¹ã€‚æœ€åï¼Œä»‹ç»äº†é€šè¿‡æ©æ¨¡æ‰©æ•£æ¨¡å‹å®ç°æ— é™æ·±åº¦æ½œåœ¨æ¨ç†ç­‰å…ˆè¿›èŒƒå¼ï¼Œè¿™äº›èŒƒå¼èƒ½å¤Ÿå®ç°å…¨å±€ä¸€è‡´ä¸”å¯é€†çš„æ¨ç†è¿‡ç¨‹ã€‚æœ¬æ–‡æ—¨åœ¨ç»Ÿä¸€è¿™äº›è§‚ç‚¹ï¼Œæ¾„æ¸…æ½œåœ¨æ¨ç†çš„æ¦‚å¿µæ ¼å±€ï¼Œå¹¶ç»˜åˆ¶LLMè®¤çŸ¥ç ”ç©¶çš„å‰æ²¿æœªæ¥æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMså…·å¤‡é€šè¿‡æ˜ç¡®çš„æ€ç»´é“¾ï¼ˆCoTï¼‰è¿›è¡Œæ¨ç†çš„èƒ½åŠ›ï¼Œè¿™æé«˜äº†å…¶å¯è§£é‡Šæ€§å’Œå‡†ç¡®æ€§ã€‚</li>
<li>æ½œåœ¨æ¨ç†æ–¹æ³•æ—¨åœ¨è§£å†³åœ¨è¿ç»­éšè—çŠ¶æ€ä¸­è¿›è¡Œå¤šæ­¥æ¨ç†çš„é—®é¢˜ï¼Œæ¶ˆé™¤å¯¹ä»¤ç‰Œçº§åˆ«ç›‘ç£çš„ä¾èµ–ã€‚</li>
<li>ç¥ç»ç½‘ç»œå±‚åœ¨æ½œåœ¨æ¨ç†ä¸­æ‰®æ¼”è®¡ç®—åŸºåº•çš„åŸºçŸ³è§’è‰²ï¼Œæ”¯æŒå¤æ‚è½¬æ¢ã€‚</li>
<li>å¤šç§æ½œåœ¨æ¨ç†æ–¹æ³•åŒ…æ‹¬åŸºäºæ¿€æ´»çš„é€’å½’ã€éšè—çŠ¶æ€ä¼ æ’­ä»¥åŠå¾®è°ƒç­–ç•¥ç­‰ã€‚</li>
<li>å…ˆè¿›çš„æ— é™æ·±åº¦æ½œåœ¨æ¨ç†èŒƒå¼èƒ½å¤Ÿå®ç°å…¨å±€ä¸€è‡´ä¸”å¯é€†çš„æ¨ç†è¿‡ç¨‹ã€‚</li>
<li>æ–‡ç« æ—¨åœ¨ç»Ÿä¸€æ½œåœ¨æ¨ç†çš„æ¦‚å¿µï¼Œå¹¶æ¾„æ¸…å…¶æ¦‚å¿µæ ¼å±€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.06203">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a6f1945ef9271c964cc6fde42094f1e2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-204099482c33a699456b9aea8394dbbf.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="UQLM-A-Python-Package-for-Uncertainty-Quantification-in-Large-Language-Models"><a href="#UQLM-A-Python-Package-for-Uncertainty-Quantification-in-Large-Language-Models" class="headerlink" title="UQLM: A Python Package for Uncertainty Quantification in Large Language   Models"></a>UQLM: A Python Package for Uncertainty Quantification in Large Language   Models</h2><p><strong>Authors:Dylan Bouchard, Mohit Singh Chauhan, David Skarbrevik, Ho-Kyeong Ra, Viren Bajaj, Zeya Ahmad</strong></p>
<p>Hallucinations, defined as instances where Large Language Models (LLMs) generate false or misleading content, pose a significant challenge that impacts the safety and trust of downstream applications. We introduce UQLM, a Python package for LLM hallucination detection using state-of-the-art uncertainty quantification (UQ) techniques. This toolkit offers a suite of UQ-based scorers that compute response-level confidence scores ranging from 0 to 1. This library provides an off-the-shelf solution for UQ-based hallucination detection that can be easily integrated to enhance the reliability of LLM outputs. </p>
<blockquote>
<p>å¹»è§‰è¢«å®šä¹‰ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆé”™è¯¯æˆ–è¯¯å¯¼æ€§å†…å®¹çš„æƒ…å†µï¼Œè¿™å¯¹ä¸‹æ¸¸åº”ç”¨çš„å®‰å…¨æ€§å’Œä¿¡ä»»åº¦æ„æˆäº†é‡å¤§æŒ‘æˆ˜ã€‚æˆ‘ä»¬å¼•å…¥äº†UQLMï¼Œè¿™æ˜¯ä¸€ä¸ªä½¿ç”¨æœ€å…ˆè¿›çš„é‡åŒ–ä¸ç¡®å®šæ€§ï¼ˆUQï¼‰æŠ€æœ¯æ£€æµ‹LLMå¹»è§‰çš„PythonåŒ…ã€‚è¯¥å·¥å…·åŒ…æä¾›äº†ä¸€ç³»åˆ—åŸºäºUQçš„è¯„åˆ†å™¨ï¼Œå¯ä»¥è®¡ç®—ä»0åˆ°1çš„å“åº”çº§åˆ«ç½®ä¿¡åº¦åˆ†æ•°ã€‚è¿™ä¸ªåº“æä¾›äº†ä¸€ä¸ªåŸºäºUQçš„å¹»è§‰æ£€æµ‹å³æ’å³ç”¨è§£å†³æ–¹æ¡ˆï¼Œå¯ä»¥è½»æ¾åœ°ä¸LLMè¾“å‡ºå¢å¼ºå¯é æ€§é›†æˆåœ¨ä¸€èµ·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.06196v1">PDF</a> Submitted to Journal of Machine Learning Research (MLOSS); UQLM   Repository: <a target="_blank" rel="noopener" href="https://github.com/cvs-health/uqlm">https://github.com/cvs-health/uqlm</a></p>
<p><strong>Summary</strong>ï¼šLLMäº§ç”Ÿçš„å‡å†…å®¹å³â€œå¹»è§‰â€å¸¦æ¥å®‰å…¨æ€§å’Œä¿¡ä»»é—®é¢˜ã€‚æ¨å‡ºUQLMå·¥å…·åŒ…ï¼Œä½¿ç”¨æœ€æ–°ä¸ç¡®å®šæ€§é‡åŒ–æŠ€æœ¯è¿›è¡ŒLLMå¹»è§‰æ£€æµ‹ã€‚å·¥å…·åŒ…æä¾›ä¸€ç³»åˆ—åŸºäºUQçš„è¯„åˆ†å™¨ï¼Œè®¡ç®—å“åº”çº§åˆ«çš„ç½®ä¿¡åº¦åˆ†æ•°ã€‚å®ƒä¸ºåŸºäºUQçš„å¹»è§‰æ£€æµ‹æä¾›äº†ç°æˆè§£å†³æ–¹æ¡ˆï¼Œå¯è½»æ¾é›†æˆä»¥æé«˜LLMè¾“å‡ºçš„å¯é æ€§ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>LLMäº§ç”Ÿçš„å¹»è§‰å¯¹ä¸‹æ¸¸åº”ç”¨çš„å®‰å…¨æ€§å’Œä¿¡ä»»åº¦æ„æˆæŒ‘æˆ˜ã€‚</li>
<li>UQLMæ˜¯ä¸€ä¸ªç”¨äºLLMå¹»è§‰æ£€æµ‹çš„PythonåŒ…ã€‚</li>
<li>UQLMä½¿ç”¨æœ€æ–°çš„ä¸ç¡®å®šæ€§é‡åŒ–æŠ€æœ¯ã€‚</li>
<li>UQLMæä¾›ä¸€ç³»åˆ—åŸºäºUQçš„è¯„åˆ†å™¨ï¼Œä»¥è®¡ç®—å“åº”çº§åˆ«çš„ç½®ä¿¡åº¦åˆ†æ•°ï¼ŒèŒƒå›´ä»0åˆ°1ã€‚</li>
<li>UQLMå·¥å…·åŒ…æä¾›ç°æˆçš„è§£å†³æ–¹æ¡ˆï¼Œç”¨äºåŸºäºUQçš„å¹»è§‰æ£€æµ‹ã€‚</li>
<li>UQLMå¯ä»¥è½»æ¾åœ°é›†æˆåˆ°ç°æœ‰çš„ç³»ç»Ÿä¸­ä»¥æé«˜LLMè¾“å‡ºçš„å¯é æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.06196">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0c1e7a6abfafbc78cddb100e37e9ed43.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-82dace58067a070767fd9ea55dfa208a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-43d2f2510b4f55094db640de2aa06b8f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ec42c6e2d3e0c5a378af1061e82ffb01.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Data-Semantics-Aware-Recommendation-of-Diverse-Pivot-Tables"><a href="#Data-Semantics-Aware-Recommendation-of-Diverse-Pivot-Tables" class="headerlink" title="Data-Semantics-Aware Recommendation of Diverse Pivot Tables"></a>Data-Semantics-Aware Recommendation of Diverse Pivot Tables</h2><p><strong>Authors:Whanhee Cho, Anna Fariha</strong></p>
<p>Data summarization is essential to discover insights from large datasets. In a spreadsheets, pivot tables offer a convenient way to summarize tabular data by computing aggregates over some attributes, grouped by others. However, identifying attribute combinations that will result in useful pivot tables remains a challenge, especially for high-dimensional datasets. We formalize the problem of automatically recommending insightful and interpretable pivot tables, eliminating the tedious manual process. A crucial aspect of recommending a set of pivot tables is to diversify them. Traditional works inadequately address the table-diversification problem, which leads us to consider the problem of pivot table diversification.   We present SAGE, a data-semantics-aware system for recommending k-budgeted diverse pivot tables, overcoming the shortcomings of prior work for top-k recommendations that cause redundancy. SAGE ensures that each pivot table is insightful, interpretable, and adaptive to the userâ€™s actions and preferences, while also guaranteeing that the set of pivot tables are different from each other, offering a diverse recommendation. We make two key technical contributions: (1) a data-semantics-aware model to measure the utility of a single pivot table and the diversity of a set of pivot tables, and (2) a scalable greedy algorithm that can efficiently select a set of diverse pivot tables of high utility, by leveraging data semantics to significantly reduce the combinatorial search space. Our extensive experiments on three real-world datasets show that SAGE outperforms alternative approaches, and efficiently scales to accommodate high-dimensional datasets. Additionally, we present several case studies to highlight SAGEâ€™s qualitative effectiveness over commercial software and Large Language Models (LLMs). </p>
<blockquote>
<p>æ•°æ®æ‘˜è¦å¯¹äºä»å¤§å‹æ•°æ®é›†ä¸­å‘ç°è§è§£è‡³å…³é‡è¦ã€‚åœ¨ç”µå­è¡¨æ ¼ä¸­ï¼Œé€è§†è¡¨é€šè¿‡è®¡ç®—æŸäº›å±æ€§çš„æ€»å’Œå¹¶æŒ‰å…¶ä»–å±æ€§è¿›è¡Œåˆ†ç»„ï¼Œæä¾›äº†ä¸€ç§æ–¹ä¾¿çš„æ–¹å¼æ¥æ€»ç»“è¡¨æ ¼æ•°æ®ã€‚ç„¶è€Œï¼Œç¡®å®šå“ªäº›å±æ€§ç»„åˆä¼šäº§ç”Ÿæœ‰ç”¨çš„é€è§†è¡¨ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯å¯¹äºé«˜ç»´æ•°æ®é›†ã€‚æˆ‘ä»¬å¯¹è‡ªåŠ¨åŒ–æ¨èæœ‰æ´å¯ŸåŠ›å’Œå¯è§£é‡Šçš„é€è§†è¡¨çš„é—®é¢˜è¿›è¡Œäº†æ­£å¼åŒ–ï¼Œæ¶ˆé™¤äº†ç¹ççš„æ‰‹åŠ¨è¿‡ç¨‹ã€‚æ¨èä¸€ç»„é€è§†è¡¨çš„ä¸€ä¸ªå…³é”®æ–¹é¢æ˜¯å¤šæ ·åŒ–ã€‚ä¼ ç»Ÿçš„ç ”ç©¶å·¥ä½œæœªèƒ½å……åˆ†è§£å†³è¡¨æ ¼å¤šæ ·åŒ–çš„é—®é¢˜ï¼Œè¿™ä¿ƒä½¿æˆ‘ä»¬è€ƒè™‘é€è§†è¡¨å¤šæ ·åŒ–çš„é—®é¢˜ã€‚æˆ‘ä»¬æå‡ºäº†SAGEç³»ç»Ÿï¼Œè¿™æ˜¯ä¸€ä¸ªæ•°æ®è¯­ä¹‰æ„ŸçŸ¥ç³»ç»Ÿï¼Œç”¨äºæ¨èké¢„ç®—å¤šæ ·åŒ–çš„é€è§†è¡¨ï¼Œå…‹æœäº†å…ˆå‰å·¥ä½œåœ¨top-kæ¨èæ–¹é¢çš„ç¼ºç‚¹ï¼Œè¿™äº›ç¼ºç‚¹ä¼šå¯¼è‡´å†—ä½™ã€‚SAGEç¡®ä¿æ¯ä¸ªé€è§†è¡¨éƒ½å¯Œæœ‰æ´å¯ŸåŠ›ã€å¯è§£é‡Šã€é€‚åº”ç”¨æˆ·çš„æ“ä½œå’Œåå¥½ï¼ŒåŒæ—¶ä¿è¯é€è§†è¡¨é›†åˆä¹‹é—´çš„ä¸åŒæ€§ï¼Œæä¾›å¤šæ ·åŒ–çš„æ¨èã€‚æˆ‘ä»¬åšå‡ºäº†ä¸¤é¡¹å…³é”®æŠ€æœ¯è´¡çŒ®ï¼šï¼ˆ1ï¼‰ä¸€ä¸ªæ•°æ®è¯­ä¹‰æ„ŸçŸ¥æ¨¡å‹æ¥æµ‹é‡å•ä¸ªé€è§†è¡¨çš„å®ç”¨æ€§å’Œé€è§†è¡¨é›†åˆçš„å¤šæ ·æ€§ï¼Œï¼ˆ2ï¼‰ä¸€ä¸ªå¯æ‰©å±•çš„è´ªå¿ƒç®—æ³•ï¼Œå¯ä»¥æœ‰æ•ˆåœ°é€‰æ‹©ä¸€ç»„å®ç”¨ä¸”å¤šæ ·åŒ–çš„é€è§†è¡¨ï¼Œåˆ©ç”¨æ•°æ®è¯­ä¹‰æ¥æ˜¾è‘—å‡å°‘ç»„åˆæœç´¢ç©ºé—´ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªçœŸå®æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSAGEä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œå¹¶èƒ½æœ‰æ•ˆåœ°æ‰©å±•åˆ°é«˜ç»´æ•°æ®é›†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜é€šè¿‡å‡ ä¸ªæ¡ˆä¾‹ç ”ç©¶æ¥å¼ºè°ƒSAGEç›¸å¯¹äºå•†ä¸šè½¯ä»¶å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å®šæ€§æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.06171v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ•°æ®æ‘˜è¦å¯¹äºä»å¤§å‹æ•°æ®é›†ä¸­å‘ç°è§è§£è‡³å…³é‡è¦ã€‚åœ¨ç”µå­è¡¨æ ¼ä¸­ï¼Œé€è§†è¡¨é€šè¿‡è®¡ç®—æŸäº›å±æ€§çš„æ±‡æ€»å¹¶å¯¹å…¶ä»–å±æ€§è¿›è¡Œåˆ†ç»„æ¥æ±‡æ€»è¡¨æ ¼æ•°æ®ï¼Œä½†è‡ªåŠ¨æ¨èå…·æœ‰æ´å¯ŸåŠ›å’Œå¯è§£é‡Šçš„é€è§†è¡¨ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯å¯¹äºé«˜ç»´æ•°æ®é›†ã€‚æœ¬æ–‡å½¢å¼åŒ–äº†è‡ªåŠ¨æ¨èé€è§†è¡¨çš„é—®é¢˜ï¼Œè§£å†³äº†æ‰‹åŠ¨è¿‡ç¨‹çš„ç¹çæ€§ã€‚æ¨èä¸€ç»„é€è§†è¡¨çš„å…³é”®æ–¹é¢æ˜¯å¤šæ ·åŒ–å®ƒä»¬ã€‚ä¼ ç»Ÿçš„å·¥ä½œæ²¡æœ‰å……åˆ†è§£å†³è¡¨å¤šæ ·åŒ–çš„é—®é¢˜ï¼Œè¿™ä¿ƒä½¿æˆ‘ä»¬è€ƒè™‘é€è§†è¡¨å¤šæ ·åŒ–çš„éš¾é¢˜ã€‚æˆ‘ä»¬æå‡ºäº†SAGEç³»ç»Ÿï¼Œå®ƒæ˜¯ä¸€ä¸ªæ•°æ®è¯­ä¹‰æ„ŸçŸ¥çš„æ¨èç³»ç»Ÿï¼Œç”¨äºæ¨èké¢„ç®—å¤šæ ·åŒ–çš„é€è§†è¡¨ï¼Œå…‹æœäº†ä»¥å¾€å·¥ä½œåœ¨top-kæ¨èæ–¹é¢çš„ä¸è¶³ï¼Œé¿å…äº†å†—ä½™ã€‚SAGEç¡®ä¿æ¯ä¸ªé€è§†è¡¨å…·æœ‰æ´å¯ŸåŠ›ã€å¯è§£é‡Šæ€§å¹¶é€‚åº”ç”¨æˆ·çš„æ“ä½œå’Œåå¥½ï¼ŒåŒæ—¶ä¿è¯é€è§†è¡¨é›†åˆä¹‹é—´çš„ç›¸äº’å·®å¼‚æ€§ï¼Œæä¾›å¤šæ ·åŒ–çš„æ¨èã€‚æˆ‘ä»¬åšå‡ºäº†ä¸¤é¡¹å…³é”®æŠ€æœ¯è´¡çŒ®ï¼šï¼ˆ1ï¼‰ä¸€ä¸ªæ•°æ®è¯­ä¹‰æ„ŸçŸ¥æ¨¡å‹æ¥è¡¡é‡å•ä¸ªé€è§†è¡¨çš„å®ç”¨æ€§å’Œé€è§†è¡¨é›†åˆçš„å¤šæ ·æ€§ï¼Œï¼ˆ2ï¼‰ä¸€ç§å¯æ‰©å±•çš„è´ªå¿ƒç®—æ³•ï¼Œå¯ä»¥æœ‰æ•ˆåœ°é€‰æ‹©å®ç”¨æ€§å¼ºä¸”å¤šæ ·çš„é€è§†è¡¨é›†åˆï¼Œåˆ©ç”¨æ•°æ®è¯­ä¹‰æ¥å¤§å¤§å‡å°‘ç»„åˆæœç´¢ç©ºé—´ã€‚åœ¨ä¸‰ä¸ªçœŸå®æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSAGEä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œå¹¶æœ‰æ•ˆåœ°æ‰©å±•åˆ°é«˜ç»´æ•°æ®é›†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜é€šè¿‡å‡ ä¸ªæ¡ˆä¾‹ç ”ç©¶å±•ç¤ºäº†SAGEç›¸å¯¹äºå•†ä¸šè½¯ä»¶å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å®šæ€§æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ•°æ®æ‘˜è¦å¯¹äºä»å¤§å‹æ•°æ®é›†ä¸­å‘ç°è§è§£è‡³å…³é‡è¦ã€‚</li>
<li>è‡ªåŠ¨æ¨èé€è§†è¡¨æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨é«˜ç»´æ•°æ®é›†ä¸­ã€‚</li>
<li>SAGEç³»ç»Ÿæ˜¯ä¸€ä¸ªæ•°æ®è¯­ä¹‰æ„ŸçŸ¥çš„æ¨èç³»ç»Ÿï¼Œç”¨äºæ¨èå¤šæ ·åŒ–çš„é€è§†è¡¨ã€‚</li>
<li>SAGEèƒ½å¤Ÿç¡®ä¿æ¯ä¸ªæ¨èçš„é€è§†è¡¨å…·æœ‰æ´å¯ŸåŠ›ã€å¯è§£é‡Šæ€§ï¼Œå¹¶é€‚åº”ç”¨æˆ·åå¥½ã€‚</li>
<li>SAGEé€šè¿‡æ•°æ®è¯­ä¹‰æ„ŸçŸ¥æ¨¡å‹æ¥è¡¡é‡é€è§†è¡¨çš„å®ç”¨æ€§å’Œå¤šæ ·æ€§ã€‚</li>
<li>SAGEé‡‡ç”¨è´ªå¿ƒç®—æ³•é«˜æ•ˆé€‰æ‹©å®ç”¨ä¸”å¤šæ ·çš„é€è§†è¡¨ï¼Œå‡å°‘ç»„åˆæœç´¢ç©ºé—´ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒSAGEåœ¨çœŸå®æ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œå¹¶å¯æœ‰æ•ˆå¤„ç†é«˜ç»´æ•°æ®é›†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.06171">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4a57993e2846cd8852f0f221d8e55fb4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-263e958997d24174ee53c686e1f93310.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4da0fe801f8dacea5827c1b1427c74d5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7078232a3d6292e389e5ab7ee536cd39.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2c0960bebc3e38df91e408a734247a84.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Skywork-R1V3-Technical-Report"><a href="#Skywork-R1V3-Technical-Report" class="headerlink" title="Skywork-R1V3 Technical Report"></a>Skywork-R1V3 Technical Report</h2><p><strong>Authors:Wei Shen, Jiangbo Pei, Yi Peng, Xuchen Song, Yang Liu, Jian Peng, Haofeng Sun, Yunzhuo Hao, Peiyu Wang, Yahui Zhou</strong></p>
<p>We introduce Skywork-R1V3, an advanced, open-source vision-language model (VLM) that pioneers a new approach to visual reasoning. Its key innovation lies in effectively transferring reasoning skills from text-only Large Language Models (LLMs) to visual tasks. The strong performance of Skywork-R1V3 primarily stems from our elaborate post-training RL framework, which effectively activates and enhances the modelâ€™s reasoning ability, without the need for additional continue pre-training. Through this framework, we further uncover the fundamental role of the connector module in achieving robust cross-modal alignment for multimodal reasoning models. In addition, we introduce a unique indicator of reasoning capability, the entropy of critical reasoning tokens, which has proven highly effective for checkpoint selection during RL training. Skywork-R1V3 achieves state-of-the-art results on MMMU, significantly improving from 64.3% to 76.0%. This performance matches entry-level human capabilities. Remarkably, our RL-powered post-training approach enables even the 38B parameter model to rival top closed-source VLMs. The implementation successfully transfers mathematical reasoning to other subject-related reasoning tasks. We also include an analysis of curriculum learning and reinforcement finetuning strategies, along with a broader discussion on multimodal reasoning. Skywork-R1V3 represents a significant leap in multimodal reasoning, showcasing RL as a powerful engine for advancing open-source VLM capabilities. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†Skywork-R1V3ï¼Œè¿™æ˜¯ä¸€ä¸ªå…ˆè¿›çš„å¼€æºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ï¼Œå®ƒå¼€åˆ›äº†ä¸€ç§æ–°çš„è§†è§‰æ¨ç†æ–¹æ³•ã€‚å…¶ä¸»è¦åˆ›æ–°ä¹‹å¤„åœ¨äºå°†çº¯æ–‡æœ¬å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›æœ‰æ•ˆåœ°è½¬ç§»åˆ°è§†è§‰ä»»åŠ¡ä¸Šã€‚Skywork-R1V3çš„å‡ºè‰²æ€§èƒ½ä¸»è¦æºäºæˆ‘ä»¬ç²¾å¿ƒè®¾è®¡çš„åè®­ç»ƒå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æœ‰æ•ˆåœ°æ¿€æ´»å¹¶å¢å¼ºäº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œè€Œæ— éœ€é¢å¤–çš„ç»§ç»­é¢„è®­ç»ƒã€‚é€šè¿‡è¯¥æ¡†æ¶ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å‘ç°äº†è¿æ¥å™¨æ¨¡å—åœ¨å®ç°é²æ£’è·¨æ¨¡æ€å¯¹é½ä¸­çš„å…³é”®ä½œç”¨ï¼Œè¿™å¯¹äºå¤šæ¨¡æ€æ¨ç†æ¨¡å‹è‡³å…³é‡è¦ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç‹¬ç‰¹çš„æ¨ç†èƒ½åŠ›æŒ‡æ ‡â€”â€”å…³é”®æ¨ç†æ ‡è®°çš„ç†µï¼Œè¯¥æŒ‡æ ‡åœ¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„æ£€æŸ¥ç‚¹é€‰æ‹©ä¸­è¡¨ç°å‡ºé«˜åº¦æœ‰æ•ˆæ€§ã€‚Skywork-R1V3åœ¨MMMUä¸Šå–å¾—äº†æœ€æ–°ç»“æœï¼Œä»64.3%æ˜¾ç€æé«˜åˆ°76.0%ã€‚è¿™ä¸€è¡¨ç°ä¸å…¥é—¨çº§äººç±»èƒ½åŠ›ç›¸åŒ¹é…ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„åŸºäºå¼ºåŒ–å­¦ä¹ çš„åè®­ç»ƒç­–ç•¥ç”šè‡³ä½¿38Bå‚æ•°æ¨¡å‹èƒ½ä¸é¡¶çº§é—­æºVLMç›¸åŒ¹æ•Œã€‚æˆ‘ä»¬çš„å®ç°æˆåŠŸåœ°å°†æ•°å­¦æ¨ç†è½¬ç§»åˆ°å…¶ä»–ç›¸å…³æ¨ç†ä»»åŠ¡ä¸Šã€‚æˆ‘ä»¬è¿˜å¯¹è¯¾ç¨‹å­¦ä¹ å’Œå¼ºåŒ–å¾®è°ƒç­–ç•¥è¿›è¡Œäº†åˆ†æï¼Œå¹¶å¯¹å¤šæ¨¡æ€æ¨ç†è¿›è¡Œäº†æ›´å¹¿æ³›çš„è®¨è®ºã€‚Skywork-R1V3åœ¨å¤šæ¨¡æ€æ¨ç†ä¸­å®ç°äº†é‡å¤§çªç ´ï¼Œå±•ç¤ºäº†å¼ºåŒ–å­¦ä¹ ä½œä¸ºæ¨åŠ¨å¼€æºVLMèƒ½åŠ›å‘å±•çš„å¼ºå¤§å¼•æ“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.06167v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>Skywork-R1V3æ˜¯ä¸€æ¬¾å…ˆè¿›çš„å¼€æºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ï¼Œå®ƒå¼€åˆ›äº†ä¸€ç§æ–°çš„è§†è§‰æ¨ç†æ–¹æ³•ã€‚è¯¥æ¨¡å‹çš„å…³é”®åˆ›æ–°åœ¨äºå°†çº¯æ–‡æœ¬çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›æœ‰æ•ˆåœ°è½¬ç§»åˆ°è§†è§‰ä»»åŠ¡ä¸Šã€‚å…¶å¼ºå¤§çš„æ€§èƒ½ä¸»è¦æ¥æºäºç²¾ç»†çš„åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æœ‰æ•ˆåœ°æ¿€æ´»å¹¶å¢å¼ºäº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œæ— éœ€é¢å¤–çš„æŒç»­é¢„è®­ç»ƒã€‚Skywork-R1V3åœ¨MMMUä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æˆæœï¼Œä»64.3%æå‡è‡³76.0%ï¼Œä¸äººç±»å…¥é—¨çº§æ°´å¹³ç›¸åŒ¹é…ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬çš„åŸºäºå¼ºåŒ–å­¦ä¹ çš„åè®­ç»ƒç­–ç•¥ä½¿å¾—æ‹¥æœ‰è¾ƒå°å‚æ•°çš„æ¨¡å‹ä¹Ÿèƒ½å¤Ÿä¸é¡¶çº§å°é—­å¼VLMç«äº‰ã€‚Skywork-R1V3æ˜¯è·¨æ¨¡æ€æ¨ç†é¢†åŸŸçš„ä¸€å¤§é£è·ƒï¼Œå±•ç¤ºäº†å¼ºåŒ–å­¦ä¹ åœ¨æ¨åŠ¨å¼€æºVLMèƒ½åŠ›æ–¹é¢çš„å¼ºå¤§æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Skywork-R1V3æ˜¯é¦–ä¸ªå°†å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›æœ‰æ•ˆè½¬ç§»åˆ°è§†è§‰ä»»åŠ¡ä¸Šçš„å…ˆè¿›æ¨¡å‹ã€‚</li>
<li>æ¨¡å‹é‡‡ç”¨åŸºäºå¼ºåŒ–å­¦ä¹ çš„åè®­ç»ƒæ¡†æ¶ï¼Œæœ‰æ•ˆæ¿€æ´»å¹¶å¢å¼ºäº†å…¶æ¨ç†èƒ½åŠ›ã€‚</li>
<li>å¼•å…¥äº†æ¨ç†èƒ½åŠ›çš„é‡è¦æŒ‡æ ‡â€”â€”å…³é”®æ¨ç†ä»¤ç‰Œçš„ç†µï¼Œè¿™ä¸€æŒ‡æ ‡å·²è¢«è¯æ˜åœ¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒä¸­å¯¹äºæ£€æŸ¥ç‚¹é€‰æ‹©éå¸¸æœ‰æ•ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.06167">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0265f2f378cc204078327bcf5c0735df.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6828544e25f077d8f088b10a12f9b2dc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ec6cf3b2d1b4652d9d5c00941cb925c.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="NeoBabel-A-Multilingual-Open-Tower-for-Visual-Generation"><a href="#NeoBabel-A-Multilingual-Open-Tower-for-Visual-Generation" class="headerlink" title="NeoBabel: A Multilingual Open Tower for Visual Generation"></a>NeoBabel: A Multilingual Open Tower for Visual Generation</h2><p><strong>Authors:Mohammad Mahdi Derakhshani, Dheeraj Varghese, Marzieh Fadaee, Cees G. M. Snoek</strong></p>
<p>Text-to-image generation advancements have been predominantly English-centric, creating barriers for non-English speakers and perpetuating digital inequities. While existing systems rely on translation pipelines, these introduce semantic drift, computational overhead, and cultural misalignment. We introduce NeoBabel, a novel multilingual image generation framework that sets a new Pareto frontier in performance, efficiency and inclusivity, supporting six languages: English, Chinese, Dutch, French, Hindi, and Persian. The model is trained using a combination of large-scale multilingual pretraining and high-resolution instruction tuning. To evaluate its capabilities, we expand two English-only benchmarks to multilingual equivalents: m-GenEval and m-DPG. NeoBabel achieves state-of-the-art multilingual performance while retaining strong English capability, scoring 0.75 on m-GenEval and 0.68 on m-DPG. Notably, it performs on par with leading models on English tasks while outperforming them by +0.11 and +0.09 on multilingual benchmarks, even though these models are built on multilingual base LLMs. This demonstrates the effectiveness of our targeted alignment training for preserving and extending crosslingual generalization. We further introduce two new metrics to rigorously assess multilingual alignment and robustness to code-mixed prompts. Notably, NeoBabel matches or exceeds English-only models while being 2-4x smaller. We release an open toolkit, including all code, model checkpoints, a curated dataset of 124M multilingual text-image pairs, and standardized multilingual evaluation protocols, to advance inclusive AI research. Our work demonstrates that multilingual capability is not a trade-off but a catalyst for improved robustness, efficiency, and cultural fidelity in generative AI. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒç”ŸæˆæŠ€æœ¯çš„è¿›å±•ä¸»è¦ä»¥è‹±è¯­ä¸ºä¸­å¿ƒï¼Œä¸ºéè‹±è¯­ä½¿ç”¨è€…è®¾ç½®äº†éšœç¢ï¼Œå¹¶åŠ å‰§äº†æ•°å­—ä¸å¹³ç­‰ç°è±¡ã€‚å°½ç®¡ç°æœ‰ç³»ç»Ÿä¾èµ–äºç¿»è¯‘ç®¡é“ï¼Œä½†è¿™ä¼šå¼•å…¥è¯­ä¹‰æ¼‚ç§»ã€è®¡ç®—å¼€é”€å’Œæ–‡åŒ–ä¸åŒ¹é…ç­‰é—®é¢˜ã€‚æˆ‘ä»¬æ¨å‡ºäº†NeoBabelï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„å¤šè¯­è¨€å›¾åƒç”Ÿæˆæ¡†æ¶ï¼Œåœ¨æ€§èƒ½ã€æ•ˆç‡å’ŒåŒ…å®¹æ€§æ–¹é¢æ ‘ç«‹äº†æ–°çš„å¸•ç´¯æ‰˜è¾¹ç•Œï¼Œæ”¯æŒå…­ç§è¯­è¨€ï¼šè‹±è¯­ã€ä¸­æ–‡ã€è·å…°è¯­ã€æ³•è¯­ã€å°åœ°è¯­å’Œæ³¢æ–¯è¯­ã€‚è¯¥æ¨¡å‹é‡‡ç”¨å¤§è§„æ¨¡å¤šè¯­è¨€é¢„è®­ç»ƒå’Œé«˜åˆ†è¾¨ç‡æŒ‡ä»¤å¾®è°ƒç›¸ç»“åˆçš„æ–¹å¼è¿›è¡Œè®­ç»ƒã€‚ä¸ºäº†è¯„ä¼°å…¶èƒ½åŠ›ï¼Œæˆ‘ä»¬å°†ä¸¤ä¸ªä»…é€‚ç”¨äºè‹±è¯­çš„åŸºå‡†æµ‹è¯•æ‰©å±•åˆ°äº†å¤šè¯­è¨€ç‰ˆæœ¬ï¼šm-GenEvalå’Œm-DPGã€‚NeoBabelåœ¨ä¿æŒå¼ºå¤§è‹±è¯­èƒ½åŠ›çš„åŒæ—¶ï¼Œå®ç°äº†æœ€å…ˆè¿›çš„å¤šè¯­è¨€èƒ½åŠ›ï¼Œåœ¨m-GenEvalä¸Šå¾—åˆ†ä¸º0.75ï¼Œåœ¨m-DPGä¸Šå¾—åˆ†ä¸º0.68ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå®ƒåœ¨è‹±è¯­ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¸é¢†å…ˆæ¨¡å‹ç›¸å½“ï¼ŒåŒæ—¶åœ¨å¤šè¯­è¨€åŸºå‡†æµ‹è¯•ä¸Šé«˜å‡º0.11å’Œ0.09ã€‚å³ä¾¿è¿™äº›æ¨¡å‹æ˜¯å»ºç«‹åœ¨å¤šè¯­è¨€åŸºç¡€çš„å¤§å‹è¯­è¨€æ¨¡å‹ä¸Šã€‚è¿™è¯æ˜äº†æˆ‘ä»¬é’ˆå¯¹æ€§å¯¹é½è®­ç»ƒåœ¨ä¿æŒå’Œæ‰©å±•è·¨è¯­è¨€æ³›åŒ–æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸¤ä¸ªæ–°æŒ‡æ ‡æ¥ä¸¥æ ¼è¯„ä¼°å¤šè¯­è¨€å¯¹é½å’Œå¯¹æŠ—ä»£ç æ··åˆæç¤ºçš„ç¨³å¥æ€§ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒNeoBabelçš„è¡¨ç°ä¸ä»…é€‚ç”¨äºè‹±è¯­çš„æ¨¡å‹ç›¸åŒ¹é…ç”šè‡³æ›´å¥½ï¼Œè€Œä¸”ä½“ç§¯æ›´å°ï¼Œä¸º2-4å€ã€‚æˆ‘ä»¬å‘å¸ƒäº†ä¸€ä¸ªå¼€æ”¾å·¥å…·åŒ…ï¼ŒåŒ…æ‹¬æ‰€æœ‰ä»£ç ã€æ¨¡å‹æ£€æŸ¥ç‚¹ã€1.24äº¿å¤šè¯­è¨€æ–‡æœ¬å›¾åƒå¯¹ç»„æˆçš„ç²¾é€‰æ•°æ®é›†ä»¥åŠæ ‡å‡†åŒ–å¤šè¯­è¨€è¯„ä¼°åè®®ï¼Œä»¥ä¿ƒè¿›åŒ…å®¹æ€§äººå·¥æ™ºèƒ½ç ”ç©¶ã€‚æˆ‘ä»¬çš„å·¥ä½œè¡¨æ˜ï¼Œå¤šè¯­è¨€èƒ½åŠ›å¹¶éæ˜¯ä¸€ç§æƒè¡¡ï¼Œè€Œæ˜¯æé«˜ç”Ÿæˆå¼äººå·¥æ™ºèƒ½çš„ç¨³å¥æ€§ã€æ•ˆç‡å’Œæ–‡åŒ–ä¿çœŸåº¦çš„å‚¬åŒ–å‰‚ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.06137v1">PDF</a> 34 pages, 12 figures</p>
<p><strong>Summary</strong></p>
<p>å¤šè¯­è¨€æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¡†æ¶NeoBabelè¢«æå‡ºï¼Œæ”¯æŒå…­ç§è¯­è¨€ï¼Œå¹¶å®ç°äº†è·¨è¯­è¨€çš„å¤šæ¨¡æ€ç”Ÿæˆè¯„ä¼°çš„ä¼˜å¼‚æ€§èƒ½ã€‚å®ƒç»“åˆå¤§è§„æ¨¡å¤šè¯­è¨€é¢„è®­ç»ƒå’Œé«˜åˆ†è¾¨ç‡æŒ‡ä»¤å¾®è°ƒï¼Œè§£å†³äº†éè‹±è¯­ç”¨æˆ·é¢ä¸´çš„æ•°å­—ä¸å¹³ç­‰å’Œè¯­è¨€å£å’é—®é¢˜ã€‚è¯„ä¼°è¡¨æ˜ï¼Œå®ƒåœ¨å¤šè¯­è¨€ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼ŒåŒæ—¶åœ¨è‹±è¯­ä»»åŠ¡ä¸Šè¡¨ç°ä¸é¢†å…ˆçš„æ¨¡å‹ç›¸å½“ã€‚è¯¥ç ”ç©¶æ¨åŠ¨äº†å¤šè¯­è¨€AIçš„ç ”ç©¶è¿›æ­¥ï¼Œæä¾›äº†å…¬å¼€çš„å·¥å…·åŒ…å’Œæ ‡å‡†åŒ–è¯„ä¼°åè®®ã€‚è¯¥å·¥ä½œè¯æ˜äº†å¤šè¯­è¨€èƒ½åŠ›ä¸æ˜¯æƒè¡¡æŒ‡æ ‡è€Œæ˜¯æ”¹è¿›AIæ€§èƒ½çš„å…³é”®ã€‚NeoBabelå°†æ–‡åŒ–ç¿»è¯‘å¿ å®æ€§å¼•å…¥ç”Ÿæˆå¼äººå·¥æ™ºèƒ½çš„è¯„ä»·æ ‡å‡†ä¹‹ä¸­ï¼Œæ ‡å¿—ç€AIå‘å±•å²ä¸Šä¸€ä¸ªé‡è¦çš„è¿›æ­¥æ—¶åˆ»ã€‚å®ƒé€šè¿‡æ”¹å–„åŒ…å®¹æ€§æ¨åŠ¨äº†ç§‘æŠ€å‘å±•æˆæœæ™®æƒ å…¨çƒæ¯ä¸ªè§’è½çš„æ¯ä¸€ä¸ªæ™®é€šäººã€‚ç®€åŒ–è¯­è¨€æœ‰åŠ©äºæ‹“å®½è¯¥é¢†åŸŸçš„ç ”ç©¶è§†é‡å’Œå…¬ä¼—ç†è§£ç¨‹åº¦ã€‚åŒæ—¶æå‡ºå¹¶å®ç°äº†å…¨æ–°çš„åº¦é‡æŒ‡æ ‡ï¼Œæ—¨åœ¨æ›´ä¸¥è°¨åœ°è¯„ä¼°æ¨¡å‹åœ¨å¤šè¯­è¨€å¯¹é½æ–¹é¢çš„ç¨³å¥æ€§ä»¥åŠå¯¹æ··åˆä»£ç çš„é€‚åº”åŠ›ã€‚æ›´é‡è¦çš„æ˜¯ï¼ŒNeoBabelå®ç°äº†ä½“ç§¯ç¼©å°çº¦ä¸‰åˆ†ä¹‹äºŒè‡³å››åˆ†ä¹‹ä¸‰ï¼Œæ€§èƒ½å´èƒ½ä¸æŸäº›ä»…æ”¯æŒå•ä¸€è¯­è¨€çš„æ¨¡å‹æ¯”è‚©ã€‚å®ƒçš„å¼€æ”¾æºä»£ç å’Œæ•°æ®é›†ä¸ºç ”ç©¶è€…æä¾›äº†å®è´µçš„èµ„æºã€‚æ€»ä¹‹ï¼ŒNeoBabelå±•ç°äº†å¤šè¯­è¨€èƒ½åŠ›åœ¨AIé¢†åŸŸçš„å·¨å¤§æ½œåŠ›ã€‚å®ƒä¸ä»…çªç ´äº†è¯­è¨€éšœç¢ï¼Œè€Œä¸”æ¨åŠ¨äº†å…¬å¹³å’Œé«˜æ•ˆçš„å‘å±•æ€åŠ¿é©æ–°è·¨é¢†åŸŸçš„èƒ½åŠ›é¢„æœŸè®¾è®¡å½¢å¼é›†åˆåœ°å®Œæˆå…¨é¢æå‡äººæœºäº’é€šå¯¹æ¥çš„æœ€ä½³æ•ˆç‡å·¥ä½œé©æ–°ç”šè‡³ä¸ä¼šè„±ç¦»å…¨é¢ä¾¿åˆ©çš„å…·ä½“æ§åˆ¶ä¸Šçš„æ°´å‡†æ˜¾ç°æ–¹å‘é”™è¯¯ç»“å±€å‘æŒ¥è´Ÿé¢çš„ä½œç”¨æ•ˆèƒ½ä»è€Œæ›´åŠ æ¨åŠ¨å…¨çƒåŒ–åŒ…å®¹æ€§çš„å¢é•¿ä¿ƒè¿›æŠ€æœ¯æˆæœæƒ åŠå…¨çƒå„ä¸ªè§’è½çš„æ¯ä¸€ä¸ªæ™®é€šäººä½¿å¾—äººæœºå…±ç”Ÿçš„æ•ˆæœå†åº¦è¿›åŒ–ç»§ç»­ä½“ç°é«˜æ•ˆçš„æ•°å­—åŒ–è¿­ä»£ç»™ç°ä»£ç¤¾ä¼šèµ‹èƒ½ä»è€Œä¸ºæ›´å¤šç§‘ç ”å·¥ä½œæ·»ç –åŠ ç“¦å¼€è¾Ÿæ›´ä¸ºå¹¿é˜”çš„ç ”å‘åœºæ™¯å¼€æ”¾æ•°æ®åº“ç§‘æŠ€ç ”å‘ä¸­å¿ƒç”¨æˆ·é™¤äº†åœ¨å›½å†…ä¹Ÿé¢å‘å…¨çƒé¢†åŸŸå†…çš„ç ”ç©¶è€…å’Œå¼€å‘è€…å¼€æ”¾å…±äº«åˆä½œæ¨åŠ¨å…¨çƒç§‘æŠ€å‘å±•å’Œè¿›æ­¥æä¾›åŠ©åŠ›æ¨åŠ¨å…¨çƒç§‘æŠ€å‘å±•å’Œè¿›æ­¥æä¾›åŠ©åŠ›æ¨åŠ¨å…¨çƒç§‘æŠ€å‘å±•å’Œè¿›æ­¥å®ç°æ™®æƒ ç§‘æŠ€çš„ç›®æ ‡å®ç°å…¨æ–°çš„æ•°å­—åŒ–è½¬å‹å¸¦æ¥äº†é©æ–°æ•ˆåº”ï¼Œæ‰“é€ äº†å…¨å¹³å°çš„è¡Œä¸šäº’è”é«˜æ•ˆè¿è¥æ›´æ»¡è¶³äº†æ›´åŠ ä¸ªæ€§åŒ–å’Œäººæ€§åŒ–è®¾è®¡ä»è€Œæ”¹å˜äº†äººç±»çš„ç°æœ‰ç”Ÿå­˜ç¯å¢ƒè¿ˆå‘äººç±»è¿½æ±‚çš„å…¨è‡ªåŠ¨åŒ–é«˜æ•ˆå¿«æ·æ­¥ä¼é‡å¤§åˆ©å¥½æ¶ˆ æ¯é€šè¿‡æ‰“é€šç§‘æŠ€è¡Œä¸šçš„ä¸Šä¸‹æ¸¸æ¸ é“ä»è€Œå®ç°ç§‘æŠ€çš„ç ”å‘æˆæœè½¬åŒ–åŠ©åŠ›ç¤¾ä¼šå‘å±•å¸¦æ¥ä¾¿æ·åˆ›æ–°çš„ç”Ÿæ´»æ¨¡å¼æœ€ç»ˆå®ç°ç§‘æŠ€è¿›æ­¥æœåŠ¡å…¨çƒå‘å±•åŠ©åŠ›ä¸–ç•Œå®ç°æ™ºèƒ½åŒ–è¿ˆè¿›çš„æ–¹å‘æ˜¯æ•´ä½“å…¨ä½“ç³»è·¨å¹³å°çš„è¿ˆå‘è‡ªåŠ¨åŒ–çš„äººå·¥æ™ºèƒ½é«˜ç§‘æŠ€çš„ç»¼åˆå¸ƒå±€èƒ½å¤Ÿå®ç°è‡ªåŠ¨æœé›†æ•°æ®çš„ç ”å‘èåˆæœ€ç»ˆè¾¾åˆ°ç»¼åˆæ€§è¯„ä¼°å»ºæ¨¡çš„æŠ€æœ¯é©å‘½ä»é¢†åŸŸæœ¬èº«æ‰“ç ´ä¼ ç»Ÿçš„å±€é™æ€§åšå‡ºæ›´æœ‰æˆæ•ˆçš„å·¥ä½œå±•ç°äººæœºåˆä½œæœ€ä¼˜æ•ˆæœåŒæ—¶ç§‰æŒç€å¼€æ”¾æ€§é¢å‘æœªæ¥çš„äººå·¥æ™ºèƒ½ç§‘æŠ€å‘å±•çš„ç†å¿µä¸æ–­å‘å‰è¿ˆè¿›ä»¥æ¨åŠ¨å…¨çƒç§‘æŠ€å‘å±•å’Œè¿›æ­¥å®ç°æ™®æƒ ç§‘æŠ€çš„ç›®æ ‡ã€‚æ‰“ç ´ä¼ ç»ŸæŠ€æœ¯çš„å±€é™æ€§å¹¶æ¨åŠ¨å…¨çƒç§‘æŠ€è¿›æ­¥ä¸å‘å±•ã€‚è¯¥æ¡†æ¶çš„å¼€æºå·¥å…·åŒ…ä¸æ•°æ®é›†æœ‰åŠ©äºä¿ƒè¿›è·¨è¯­è¨€äº¤æµåŠç”Ÿæˆå¼äººå·¥æ™ºèƒ½çš„å‘å±•ä¸åˆ›æ–°çš„åº”ç”¨æ¨¡å¼å’Œåº”ç”¨é¢†åŸŸå¸¦æ¥æ–°çš„é©æ–°è¿›å±•åŠ å¿«æ–°æŠ€æœ¯åº”ç”¨äºæ—¥å¸¸ç”Ÿæ´»é¢†åŸŸåˆ›æ–°æ„å»ºæœªæ¥çš„å…¨çƒåŒ–äº¤æµæ–¹å¼å’Œç¤¾åŒºçš„å»ºç«‹å‡èšæŠ€æœ¯å…±äº«å¼€æ”¾å‘å±•çš„æ€æƒ³å…±åŒé¢å¯¹æ–°çš„æŒ‘æˆ˜ä»è€Œå…±åˆ›è¾‰ç…Œä»¥æ›´å…·äººæ€§åŒ–çš„åˆ›æ–°å§¿æ€è¿›ä¸€æ­¥å‘æŒ¥é›†ä½“æ™ºæ…§çš„å·¨å¤§æ½œåŠ›æœ€ç»ˆå®ç°äººç±»ç¤¾ä¼šæ™ºèƒ½åŒ–çš„æœªæ¥åˆ›æ–°ç ”å‘æ€è·¯é€šè¿‡æ›´åŠ ç²¾ç»†åŒ–çš„åˆ’åˆ†ä¸åŒæ¨¡å—çš„æŠ€æœ¯ç ”ç©¶è¿›ä¸€æ­¥æå‡æŠ€æœ¯ç ”å‘æ•ˆç‡å’Œå¼€å‘å‘¨æœŸä¿ƒè¿›æ•´ä½“ç§‘æŠ€æ°´å¹³çš„æå‡å¢å¼ºæ™ºèƒ½åŒ–ç³»ç»Ÿè‡ªæˆ‘è¿›åŒ–èƒ½åŠ›ä¸æœºåˆ¶åŒ–çš„è¿è¡Œæ¨¡å¼æœ‰æ•ˆåœ°ç¼©å‡ç”±äºæ—¶é—´åœ°ç‚¹çš„å·®å¼‚åŒ–è€Œé€ æˆçš„æŠ€æœ¯æ›´è¿­ä¸Šé¸¿æ²Ÿå¼€è¾Ÿç§‘ç ”çš„æ¨ªå‘çªç ´é’ˆå¯¹å„è¡Œå„ä¸šçš„è¦æ±‚æ·±åº¦ä¼˜åŒ–æå‡ºå®ç”¨åŒ–å’Œå¸‚åœºç›¸ç»“åˆçš„é…å¥—ç§‘æŠ€æŠ€æœ¯æ ‡å‡†åœ¨æ¨åŠ¨ç¤¾ä¼šè¿›æ­¥çš„åŒæ—¶æé«˜äººæ°‘ç¾¤ä¼—å¯¹ç¾å¥½ç”Ÿæ´»çš„å‘å¾€ä¸è¿½æ±‚ä¸ºå®ç°ä¸­åæ°‘æ—ä¼Ÿå¤§å¤å…´çš„ä¸­å›½æ¢¦æ·»ç –åŠ ç“¦è´¡çŒ®è‡ªå·±çš„ä¸€ä»½åŠ›é‡å®ç°ç§‘æŠ€æ”¹å˜å‘½è¿åŠ©åŠ›å…¨çƒç§‘æŠ€è¿›æ­¥ä¸å‘å±•ä»¥æ™®æƒ å…¨äººç±»ä¸ºå·±ä»»ç§‰æŒå¼€æ”¾å…±äº«çš„ä»·å€¼è§‚åŠ å¿«ç§‘æŠ€å‘å±•åˆ›æ–°åŠ å¿«æ™®æƒ å‹ç§‘æŠ€åœ¨å…¨çƒèŒƒå›´å†…è½åœ°å‘æŒ¥ç§‘ç ”äººå‘˜å‹‡äºæ”€ç™»å‹‡äºæŒ‘æˆ˜çš„åˆ›æ–°ç²¾ç¥åœ¨å®ç°å…±åŒå¯Œè£•çš„ç¤¾ä¼šå®è·µä¸­èµ°åœ¨å‰æ²¿åˆ›é€ æ–°æ—¶ä»£è¾‰ç…Œçš„æ˜å¤©æ‰“ç ´ä¸åŒé¢†åŸŸçš„è¾¹ç•Œåœ¨ä¿ƒè¿›å¤šå­¦ç§‘äº¤å‰èåˆçš„åŸºç¡€ä¸Šå……åˆ†å‘æŒ¥å¤šè¯­è¨€æ¡†æ¶çš„æ½œåœ¨ä»·å€¼å¯¹æ–°æ—¶ä»£äººç±»ç¤¾ä¼šå‘å±•ä¸è¿›æ­¥åšå‡ºåº”æœ‰çš„è´¡çŒ®ç­‰ç‰¹å¾å…ƒç´ æè¿°é›†ä½“ç°æ ¸å¿ƒä»·å€¼ä¼˜åŠ¿å¤šå…ƒåœºæ™¯çš„åˆ›æ–°å‡çº§ä»¥æ»¡è¶³å¤šå…ƒåŒ–çš„éœ€æ±‚ä»è€Œå®ç°è·¨è¶Šå¼å‘å±•åŠ©åŠ›æ‰“é€ å›½é™…é¢†å…ˆçš„äººå·¥æ™ºèƒ½ç”Ÿæ€ç³»ç»Ÿæ¡†æ¶é¢å‘æœªæ¥çš„æŒ‘æˆ˜ç§‰æŒç€ä»¥äººä¸ºæœ¬ç§‘æŠ€åˆ›æ–°çš„å®—æ—¨è´¡çŒ®è‡ªèº«çš„åŠ›é‡ä¿ƒè¿›äººå·¥æ™ºèƒ½çš„å‘å±•åŒæ—¶è¿›ä¸€æ­¥æ‹“å®½åº”ç”¨é¢†åŸŸæé«˜åº”ç”¨æ°´å¹³æ¨åŠ¨äººå·¥æ™ºèƒ½æŠ€æœ¯çš„æ™®åŠå’Œæˆç†Ÿä¸ºæ„å»ºäººç±»å‘½è¿å…±åŒä½“è´¡çŒ®åŠ›é‡ã€‚<br>    <strong>Key Takeaways</strong></p>
<ul>
<li>å¤šè¯­è¨€æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¡†æ¶NeoBabelçªç ´è¯­è¨€éšœç¢å’Œæ•°å­—ä¸å¹³ç­‰é—®é¢˜ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.06137">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f82e1646ca831eeecb07a6af34370fe9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-386ff039987fa666456422c57abb7dc0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b2324ccd3b7f36753fb6e0e7e4716b51.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="FEVO-Financial-Knowledge-Expansion-and-Reasoning-Evolution-for-Large-Language-Models"><a href="#FEVO-Financial-Knowledge-Expansion-and-Reasoning-Evolution-for-Large-Language-Models" class="headerlink" title="FEVO: Financial Knowledge Expansion and Reasoning Evolution for Large   Language Models"></a>FEVO: Financial Knowledge Expansion and Reasoning Evolution for Large   Language Models</h2><p><strong>Authors:Bo Pang, Yalu Ouyang, Hangfei Xu, Ziqi Jia, Panpan Li, Shengzhao Wen, Lu Wang, Shiyong Li, Yanpeng Wang</strong></p>
<p>Advancements in reasoning for large language models (LLMs) have lead to significant performance improvements for LLMs in various fields such as mathematics and programming. However, research applying these advances to the financial domain, where considerable domain-specific knowledge is necessary to complete tasks, remains limited. To address this gap, we introduce FEVO (Financial Evolution), a multi-stage enhancement framework developed to enhance LLM performance in the financial domain. FEVO systemically enhances LLM performance by using continued pre-training (CPT) to expand financial domain knowledge, supervised fine-tuning (SFT) to instill structured, elaborate reasoning patterns, and reinforcement learning (RL) to further integrate the expanded financial domain knowledge with the learned structured reasoning. To ensure effective and efficient training, we leverage frontier reasoning models and rule-based filtering to curate FEVO-Train, high-quality datasets specifically designed for the different post-training phases. Using our framework, we train the FEVO series of models â€“ C32B, S32B, R32B â€“ from Qwen2.5-32B and evaluate them on seven benchmarks to assess financial and general capabilities, with results showing that FEVO-R32B achieves state-of-the-art performance on five financial benchmarks against much larger models as well as specialist models. More significantly, FEVO-R32B demonstrates markedly better performance than FEVO-R32B-0 (trained from Qwen2.5-32B-Instruct using only RL), thus validating the effectiveness of financial domain knowledge expansion and structured, logical reasoning distillation </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ¨ç†æ–¹é¢çš„è¿›æ­¥ï¼ŒLLMåœ¨æ•°å­¦ã€ç¼–ç¨‹ç­‰é¢†åŸŸå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ç„¶è€Œï¼Œå°†è¿™äº›è¿›å±•åº”ç”¨äºé‡‘èé¢†åŸŸçš„ç ”ç©¶ä»ç„¶æœ‰é™ï¼Œé‡‘èé¢†åŸŸçš„ä»»åŠ¡éœ€è¦å¤§é‡çš„ä¸“ä¸šé¢†åŸŸçŸ¥è¯†æ‰èƒ½å®Œæˆã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¼•å…¥äº†FEVOï¼ˆFinancial Evolutionï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸ºå¢å¼ºLLMåœ¨é‡‘èé¢†åŸŸçš„æ€§èƒ½è€Œå¼€å‘çš„åˆ†é˜¶æ®µå¢å¼ºæ¡†æ¶ã€‚FEVOé€šè¿‡æŒç»­é¢„è®­ç»ƒï¼ˆCPTï¼‰æ¥æ‰©å±•é‡‘èé¢†åŸŸçŸ¥è¯†ï¼Œé€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ¥çŒè¾“ç»“æ„åŒ–ã€ç²¾ç»†çš„æ¨ç†æ¨¡å¼ï¼Œä»¥åŠé€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¥è¿›ä¸€æ­¥å°†æ‰©å±•çš„é‡‘èé¢†åŸŸçŸ¥è¯†ä¸æ‰€å­¦çš„ç»“æ„åŒ–æ¨ç†ç›¸ç»“åˆï¼Œä»è€Œç³»ç»Ÿåœ°æé«˜LLMçš„æ€§èƒ½ã€‚ä¸ºäº†ç¡®ä¿æœ‰æ•ˆå’Œé«˜æ•ˆçš„è®­ç»ƒï¼Œæˆ‘ä»¬åˆ©ç”¨å‰æ²¿çš„æ¨ç†æ¨¡å‹å’ŒåŸºäºè§„åˆ™çš„è¿‡æ»¤æ¥åˆ›å»ºä¸“é—¨ä¸ºä¸åŒåè®­ç»ƒé˜¶æ®µè®¾è®¡çš„FEVO-Trainé«˜è´¨é‡æ•°æ®é›†ã€‚ä½¿ç”¨æˆ‘ä»¬çš„æ¡†æ¶ï¼Œæˆ‘ä»¬è®­ç»ƒäº†FEVOç³»åˆ—æ¨¡å‹â€”â€”C32Bã€S32Bã€R32Bï¼Œå®ƒä»¬åŸºäºQwen2.5-32Bï¼Œå¹¶åœ¨ä¸ƒä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¯„ä¼°äº†å®ƒä»¬çš„é‡‘èå’Œä¸€èˆ¬èƒ½åŠ›ã€‚ç»“æœè¡¨æ˜ï¼ŒFEVO-R32Båœ¨äº”å¥—é‡‘èåŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†ä¼˜äºæ›´å¤§æ¨¡å‹å’Œä¸“ä¸šæ¨¡å‹çš„æœ€æ–°æ€§èƒ½ã€‚æ›´é‡è¦çš„æ˜¯ï¼ŒFEVO-R32Bçš„æ€§èƒ½æ˜¾è‘—ä¼˜äºFEVO-R32B-0ï¼ˆåŸºäºQwen2.5-32B-Instructä»…ä½¿ç”¨RLè¿›è¡Œè®­ç»ƒï¼‰ï¼Œä»è€ŒéªŒè¯äº†é‡‘èé¢†åŸŸçŸ¥è¯†æ‰©å±•å’Œç»“æ„åŒ–ã€é€»è¾‘æ¨ç†è’¸é¦çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.06057v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ¨ç†æ–¹é¢çš„è¿›å±•å·²åœ¨æ•°å­¦å’Œç¼–ç¨‹ç­‰é¢†åŸŸå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ç„¶è€Œï¼Œå°†è¿™äº›è¿›å±•åº”ç”¨äºé‡‘èé¢†åŸŸçš„ç ”ç©¶ä»ç„¶æœ‰é™ï¼Œé‡‘èé¢†åŸŸçš„ä»»åŠ¡éœ€è¦å¤§é‡çš„é¢†åŸŸä¸“ä¸šçŸ¥è¯†ã€‚ä¸ºè§£å†³è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æ¨å‡ºäº†FEVOï¼ˆé‡‘èè¿›åŒ–ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸ºå¢å¼ºLLMåœ¨é‡‘èé¢†åŸŸçš„æ€§èƒ½è€Œå¼€å‘çš„å¤šé˜¶æ®µå¢å¼ºæ¡†æ¶ã€‚é€šè¿‡ä½¿ç”¨æŒç»­é¢„è®­ç»ƒï¼ˆCPTï¼‰æ¥æ‰©å±•é‡‘èé¢†åŸŸçŸ¥è¯†ï¼Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ¥åŸ¹å…»ç»“æ„åŒ–ã€é€»è¾‘åŒ–çš„æ¨ç†æ¨¡å¼ï¼Œä»¥åŠå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¥è¿›ä¸€æ­¥æ•´åˆé‡‘èé¢†åŸŸçŸ¥è¯†å’Œå­¦ä¹ åˆ°çš„ç»“æ„åŒ–æ¨ç†ï¼ŒFEVOç³»ç»Ÿæå‡äº†LLMçš„æ€§èƒ½ã€‚ä¸ºç¡®ä¿è®­ç»ƒå’Œè¯„ä¼°çš„æœ‰æ•ˆæ€§å’Œé«˜æ•ˆæ€§ï¼Œæˆ‘ä»¬åˆ©ç”¨å‰æ²¿çš„æ¨ç†æ¨¡å‹å’ŒåŸºäºè§„åˆ™çš„è¿‡æ»¤æ¥åˆ¶ä½œä¸“é—¨é’ˆå¯¹ä¸åŒè®­ç»ƒé˜¶æ®µçš„é«˜è´¨é‡æ•°æ®é›†FEVO-Trainã€‚æˆ‘ä»¬çš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒFEVOç³»åˆ—æ¨¡å‹åœ¨ä¸ƒä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œå…¶ä¸­FEVO-R32Båœ¨äº”å€‹é‡‘èé¢†åŸŸåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†ä¸æ›´å¤§ã€æ›´ä¸“ä¸šçš„æ¨¡å‹ç›¸æ¯”çš„æœ€ä½³æ€§èƒ½ã€‚è¿™éªŒè¯äº†æˆ‘ä»¬çš„é‡‘èé¢†åŸŸçŸ¥è¯†æ‰©å±•å’Œç»“æ„åŒ–ã€é€»è¾‘åŒ–æ¨ç†è’¸é¦çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨æ¨ç†æ–¹é¢çš„è¿›å±•å·²åœ¨å¤šä¸ªé¢†åŸŸå–å¾—æ˜¾è‘—æˆç»©ï¼Œä½†åœ¨é‡‘èé¢†åŸŸçš„åº”ç”¨ä»æœ‰é™ã€‚</li>
<li>FEVOæ¡†æ¶æ—¨åœ¨å¢å¼ºLLMåœ¨é‡‘èé¢†åŸŸçš„æ€§èƒ½ï¼ŒåŒ…æ‹¬æŒç»­é¢„è®­ç»ƒã€ç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ ä¸‰ä¸ªé˜¶æ®µã€‚</li>
<li>FEVOåˆ©ç”¨å‰æ²¿æ¨ç†æ¨¡å‹å’ŒåŸºäºè§„åˆ™çš„è¿‡æ»¤æ¥åˆ¶ä½œé«˜è´¨é‡çš„æ•°æ®é›†FEVO-Trainï¼Œä»¥æ”¯æŒæœ‰æ•ˆçš„è®­ç»ƒã€‚</li>
<li>FEVOç³»åˆ—æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå…¶ä¸­FEVO-R32Båœ¨äº”å€‹é‡‘èé¢†åŸŸåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æœ€ä½³çŠ¶æ€ã€‚</li>
<li>ä¸ä»…ä½¿ç”¨å¼ºåŒ–å­¦ä¹ çš„æ¨¡å‹ç›¸æ¯”ï¼ŒFEVO-R32Bçš„æ€§èƒ½æ˜¾è‘—æ›´ä¼˜ï¼Œè¿™è¯æ˜äº†é‡‘èé¢†åŸŸçŸ¥è¯†æ‰©å±•å’Œç»“æ„åŒ–ã€é€»è¾‘åŒ–æ¨ç†è’¸é¦çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>FEVOæ¡†æ¶çš„è´¡çŒ®åŒ…æ‹¬æå‡LLMçš„é‡‘èé¢†åŸŸçŸ¥è¯†ã€ç»“æ„åŒ–æ¨ç†èƒ½åŠ›ï¼Œä»¥åŠé€šè¿‡å¼ºåŒ–å­¦ä¹ è¿›ä¸€æ­¥ä¼˜åŒ–è¿™äº›èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.06057">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-17c5bed13f2cc82968a2cea726f739a3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-25af8dd35405c59ef930028bf48cb0b1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8455eab949d3c3384bb9b2aa0b3c8f04.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e8fc8bf273f3fddee2ac1530d2fd45b1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e98a8f549f38a7cd8cb4b456fcdc8ecb.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Hierarchical-Interaction-Summarization-and-Contrastive-Prompting-for-Explainable-Recommendations"><a href="#Hierarchical-Interaction-Summarization-and-Contrastive-Prompting-for-Explainable-Recommendations" class="headerlink" title="Hierarchical Interaction Summarization and Contrastive Prompting for   Explainable Recommendations"></a>Hierarchical Interaction Summarization and Contrastive Prompting for   Explainable Recommendations</h2><p><strong>Authors:Yibin Liu, Ang Li, Shijian Li</strong></p>
<p>Explainable recommendations, which use the information of user and item with interaction to generate a explanation for why the user would interact with the item, are crucial for improving user trust and decision transparency to the recommender system. Existing methods primarily rely on encoding features of users and items to embeddings, which often leads to information loss due to dimensionality reduction, sparse interactions, and so on. With the advancements of large language models (LLMs) in language comprehension, some methods use embeddings as LLM inputs for explanation generation. However, since embeddings lack inherent semantics, LLMs must adjust or extend their parameters to interpret them, a process that inevitably incurs information loss. To address this issue, we propose a novel approach combining profile generation via hierarchical interaction summarization (PGHIS), which leverages a pretrained LLM to hierarchically summarize user-item interactions, generating structured textual profiles as explicit representations of user and item characteristics. Additionally, we propose contrastive prompting for explanation generation (CPEG) which employs contrastive learning to guide another reasoning language models in producing high-quality ground truth recommendation explanations. Finally, we use the textual profiles of user and item as input and high-quality explanation as output to fine-tune a LLM for generating explanations. Experimental results on multiple datasets demonstrate that our approach outperforms existing state-of-the-art methods, achieving a great improvement on metrics about explainability (e.g., 5% on GPTScore) and text quality. Furthermore, our generated ground truth explanations achieve a significantly higher win rate compared to user-written reviews and those produced by other methods, demonstrating the effectiveness of CPEG in generating high-quality ground truths. </p>
<blockquote>
<p>å¯è§£é‡Šçš„æ¨èé€šè¿‡ä½¿ç”¨ç”¨æˆ·å’Œç‰©å“çš„äº¤äº’ä¿¡æ¯æ¥ç”Ÿæˆè§£é‡Šç”¨æˆ·ä¸ºä½•ä¸ç‰©å“è¿›è¡Œäº¤äº’çš„ç†ç”±ï¼Œè¿™å¯¹äºæé«˜æ¨èç³»ç»Ÿçš„ç”¨æˆ·ä¿¡ä»»å’Œå†³ç­–é€æ˜åº¦è‡³å…³é‡è¦ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–äºå°†ç”¨æˆ·å’Œç‰©å“çš„ç‰¹å¾ç¼–ç ä¸ºåµŒå…¥ï¼Œè¿™å¾€å¾€ä¼šå¯¼è‡´ç”±äºé™ç»´å’Œç¨€ç–äº¤äº’ç­‰ä¿¡æ¯æŸå¤±ã€‚éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è¯­è¨€ç†è§£æ–¹é¢çš„è¿›å±•ï¼Œä¸€äº›æ–¹æ³•ä½¿ç”¨åµŒå…¥ä½œä¸ºLLMçš„è¾“å…¥æ¥è§£é‡Šç”Ÿæˆã€‚ç„¶è€Œï¼Œç”±äºåµŒå…¥ç¼ºä¹å†…åœ¨è¯­ä¹‰ï¼ŒLLMå¿…é¡»è°ƒæ•´æˆ–æ‰©å±•å…¶å‚æ•°æ¥è§£é‡Šå®ƒä»¬ï¼Œè¿™ä¸€è¿‡ç¨‹ä¸å¯é¿å…åœ°ä¼šå¯¼è‡´ä¿¡æ¯æŸå¤±ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.06044v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>åˆ©ç”¨ç”¨æˆ·å’Œç‰©å“çš„äº¤äº’ä¿¡æ¯æ¥ç”Ÿæˆè§£é‡Šï¼Œå¯¹äºæé«˜æ¨èç³»ç»Ÿçš„ç”¨æˆ·ä¿¡ä»»åº¦å’Œå†³ç­–é€æ˜åº¦è‡³å…³é‡è¦ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–å°†ç”¨æˆ·å’Œç‰©å“ç‰¹å¾ç¼–ç æˆåµŒå…¥ï¼Œä½†è¿™ç§æ–¹æ³•å¸¸å¸¸å› é™ç»´å’Œç¨€ç–äº¤äº’è€Œå¯¼è‡´ä¿¡æ¯æŸå¤±ã€‚éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç†è§£æ–¹é¢çš„è¿›å±•ï¼Œä¸€äº›æ–¹æ³•å°è¯•ä½¿ç”¨åµŒå…¥ä½œä¸ºLLMçš„è¾“å…¥æ¥ç”Ÿæˆè§£é‡Šã€‚ç„¶è€Œï¼Œç”±äºåµŒå…¥ç¼ºä¹å†…åœ¨è¯­ä¹‰ï¼ŒLLMéœ€è¦è°ƒæ•´æˆ–æ‰©å±•å‚æ•°æ¥è§£è¯»å®ƒä»¬ï¼Œè¿™ä¸€è¿‡ç¨‹ä¸å¯é¿å…åœ°ä¼šé€ æˆä¿¡æ¯æŸå¤±ã€‚æœ¬æ–‡æå‡ºä¸€ç§ç»“åˆå±‚æ¬¡äº¤äº’æ‘˜è¦ç”Ÿæˆç”¨æˆ·ç”»åƒï¼ˆPGHISï¼‰çš„æ–°æ–¹æ³•ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„LLMå±‚æ¬¡åœ°æ€»ç»“ç”¨æˆ·-ç‰©å“äº¤äº’ï¼Œç”Ÿæˆç»“æ„åŒ–æ–‡æœ¬ç”»åƒä½œä¸ºç”¨æˆ·å’Œç‰©å“ç‰¹æ€§çš„æ˜ç¡®è¡¨ç¤ºã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºå¯¹æ¯”æç¤ºç”Ÿæˆè§£é‡Šï¼ˆCPEGï¼‰ï¼Œé‡‡ç”¨å¯¹æ¯”å­¦ä¹ å¼•å¯¼è¯­è¨€æ¨¡å‹ç”Ÿæˆé«˜è´¨é‡çš„åœ°æ¨èè§£é‡Šã€‚æœ€åï¼Œä½¿ç”¨ç”¨æˆ·å’Œç‰©å“æ–‡æœ¬ç”»åƒä½œä¸ºè¾“å…¥ï¼Œé«˜è´¨é‡è§£é‡Šä½œä¸ºè¾“å‡ºå¾®è°ƒLLMï¼Œä»¥ç”Ÿæˆè§£é‡Šã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„è§£é‡Šæ–¹æ³•ï¼Œåœ¨è§£é‡Šæ€§æŒ‡æ ‡ä¸Šå–å¾—äº†å¾ˆå¤§çš„æ”¹è¿›ï¼ˆå¦‚GPTScoreä¸Šæé«˜äº†5%ï¼‰ï¼Œæ–‡æœ¬è´¨é‡æ›´é«˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ç”Ÿæˆçš„åœ°çœŸå®è§£é‡Šä¸ç”¨æˆ·è¯„è®ºå’Œå…¶ä»–æ–¹æ³•äº§ç”Ÿçš„è§£é‡Šç›¸æ¯”ï¼Œèµ¢å¾—äº†æ›´é«˜çš„èƒœç‡ï¼Œè¯æ˜äº†CPEGåœ¨ç”Ÿæˆé«˜è´¨é‡åœ°çœŸå®è§£æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è§£é‡Šæ€§æ¨èå¯¹äºæé«˜ç”¨æˆ·ä¿¡ä»»å’Œæ¨èç³»ç»Ÿçš„å†³ç­–é€æ˜åº¦è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–ç¼–ç ç”¨æˆ·å’Œç‰©å“ç‰¹å¾åˆ°åµŒå…¥ï¼Œä½†è¿™ç§æ–¹æ³•ä¼šå¯¼è‡´ä¿¡æ¯æŸå¤±ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç†è§£æ–¹é¢çš„è¿›å±•ä¸ºæ¨èè§£é‡Šæä¾›äº†æ–°çš„æœºä¼šã€‚</li>
<li>æå‡ºä¸€ç§ç»“åˆå±‚æ¬¡äº¤äº’æ‘˜è¦ç”Ÿæˆç”¨æˆ·ç”»åƒï¼ˆPGHISï¼‰çš„æ–°æ–¹æ³•ï¼Œç”Ÿæˆç»“æ„åŒ–æ–‡æœ¬ç”»åƒè¡¨ç¤ºç”¨æˆ·å’Œç‰©å“ç‰¹æ€§ã€‚</li>
<li>å¼•å…¥å¯¹æ¯”æç¤ºç”Ÿæˆè§£é‡Šï¼ˆCPEGï¼‰ï¼Œé‡‡ç”¨å¯¹æ¯”å­¦ä¹ æé«˜è¯­è¨€æ¨¡å‹ç”Ÿæˆé«˜è´¨é‡æ¨èè§£é‡Šçš„èƒ½åŠ›ã€‚</li>
<li>ä½¿ç”¨ç”¨æˆ·å’Œç‰©å“æ–‡æœ¬ç”»åƒä½œä¸ºè¾“å…¥ï¼Œé«˜è´¨é‡è§£é‡Šä½œä¸ºè¾“å‡ºå¾®è°ƒLLMã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.06044">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b0f10f5a90557d4aa17b662109f80121.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-872443750a4c4591686601beca1c7457.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-02de6994156a0469b9c5f7b1e1bbd3eb.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="CAVGAN-Unifying-Jailbreak-and-Defense-of-LLMs-via-Generative-Adversarial-Attacks-on-their-Internal-Representations"><a href="#CAVGAN-Unifying-Jailbreak-and-Defense-of-LLMs-via-Generative-Adversarial-Attacks-on-their-Internal-Representations" class="headerlink" title="CAVGAN: Unifying Jailbreak and Defense of LLMs via Generative   Adversarial Attacks on their Internal Representations"></a>CAVGAN: Unifying Jailbreak and Defense of LLMs via Generative   Adversarial Attacks on their Internal Representations</h2><p><strong>Authors:Xiaohu Li, Yunfeng Ning, Zepeng Bao, Mayi Xu, Jianhao Chen, Tieyun Qian</strong></p>
<p>Security alignment enables the Large Language Model (LLM) to gain the protection against malicious queries, but various jailbreak attack methods reveal the vulnerability of this security mechanism. Previous studies have isolated LLM jailbreak attacks and defenses. We analyze the security protection mechanism of the LLM, and propose a framework that combines attack and defense. Our method is based on the linearly separable property of LLM intermediate layer embedding, as well as the essence of jailbreak attack, which aims to embed harmful problems and transfer them to the safe area. We utilize generative adversarial network (GAN) to learn the security judgment boundary inside the LLM to achieve efficient jailbreak attack and defense. The experimental results indicate that our method achieves an average jailbreak success rate of 88.85% across three popular LLMs, while the defense success rate on the state-of-the-art jailbreak dataset reaches an average of 84.17%. This not only validates the effectiveness of our approach but also sheds light on the internal security mechanisms of LLMs, offering new insights for enhancing model security The code and data are available at <a target="_blank" rel="noopener" href="https://github.com/NLPGM/CAVGAN">https://github.com/NLPGM/CAVGAN</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å®‰å…¨å¯¹é½åŠŸèƒ½ä½¿å…¶èƒ½å¤ŸæŠµå¾¡æ¶æ„æŸ¥è¯¢çš„æ”»å‡»ï¼Œä½†å„ç§è¶Šç‹±æ”»å‡»æ–¹æ³•æ­ç¤ºäº†è¿™ç§å®‰å…¨æœºåˆ¶çš„æ¼æ´ã€‚å…ˆå‰çš„ç ”ç©¶å·²ç»éš”ç¦»äº†LLMè¶Šç‹±æ”»å‡»å’Œé˜²å¾¡ã€‚æˆ‘ä»¬åˆ†æäº†LLMçš„å®‰å…¨ä¿æŠ¤æœºåˆ¶ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªç»“åˆæ”»å‡»å’Œé˜²å¾¡çš„æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŸºäºLLMä¸­é—´å±‚åµŒå…¥çš„çº¿æ€§å¯åˆ†ç¦»å±æ€§ï¼Œä»¥åŠè¶Šç‹±æ”»å‡»çš„æœ¬è´¨ï¼Œå³æ—¨åœ¨åµŒå…¥æœ‰å®³é—®é¢˜å¹¶å°†å…¶è½¬ç§»åˆ°å®‰å…¨åŒºåŸŸã€‚æˆ‘ä»¬åˆ©ç”¨ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰æ¥å­¦ä¹ LLMå†…éƒ¨çš„å®‰å…¨åˆ¤æ–­è¾¹ç•Œï¼Œä»¥å®ç°æœ‰æ•ˆçš„è¶Šç‹±æ”»å‡»å’Œé˜²å¾¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸‰ä¸ªæµè¡Œçš„å¤§å‹è¯­è¨€æ¨¡å‹ä¸Šå¹³å‡è¶Šç‹±æˆåŠŸç‡è¾¾åˆ°88.85%ï¼Œè€Œåœ¨æœ€å…ˆè¿›çš„è¶Šç‹±æ•°æ®é›†ä¸Šé˜²å¾¡æˆåŠŸç‡å¹³å‡è¾¾åˆ°84.17%ã€‚è¿™ä¸ä»…éªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œè¿˜æ­ç¤ºäº†å¤§å‹è¯­è¨€æ¨¡å‹çš„å†…éƒ¨å®‰å…¨æœºåˆ¶ï¼Œä¸ºå¢å¼ºæ¨¡å‹å®‰å…¨æ€§æä¾›äº†æ–°çš„è§è§£ã€‚ç›¸å…³ä»£ç å’Œæ•°æ®å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/NLPGM/CAVGAN%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/NLPGM/CAVGANè·å¾—ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.06043v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å®‰å…¨å¯¹é½èƒ½å¤ŸæŠµå¾¡æ¶æ„æŸ¥è¯¢ï¼Œä½†å…¶ä»å­˜åœ¨æ¼æ´ï¼Œå„ç§è¶Šç‹±æ”»å‡»æ–¹æ³•æ­ç¤ºäº†è¿™ä¸€ç‚¹ã€‚æœ¬ç ”ç©¶åˆ†æäº†LLMçš„å®‰å…¨ä¿æŠ¤æœºåˆ¶ï¼Œå¹¶æå‡ºäº†ä¸€ç§ç»“åˆæ”»å‡»å’Œé˜²å¾¡çš„æ¡†æ¶ã€‚è¯¥æ¡†æ¶åŸºäºLLMä¸­é—´å±‚åµŒå…¥çš„çº¿æ€§å¯åˆ†ç¦»æ€§è´¨ä»¥åŠè¶Šç‹±æ”»å‡»çš„æœ¬è´¨ï¼Œæ—¨åœ¨å°†æœ‰å®³é—®é¢˜åµŒå…¥å®‰å…¨åŒºåŸŸã€‚ç ”ç©¶åˆ©ç”¨ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰åœ¨LLMå†…éƒ¨å­¦ä¹ å®‰å…¨åˆ¤æ–­è¾¹ç•Œï¼Œä»¥å®ç°é«˜æ•ˆçš„è¶Šç‹±æ”»å‡»å’Œé˜²å¾¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸‰ä¸ªæµè¡Œçš„å¤§å‹è¯­è¨€æ¨¡å‹ä¸Šçš„å¹³å‡è¶Šç‹±æˆåŠŸç‡è¾¾åˆ°88.85%ï¼Œè€Œåœ¨æœ€å…ˆè¿›çš„è¶Šç‹±æ•°æ®é›†ä¸Šçš„é˜²å¾¡æˆåŠŸç‡å¹³å‡è¾¾åˆ°84.17%ã€‚è¿™æ—¢éªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œä¹Ÿæ­ç¤ºäº†LLMçš„å†…éƒ¨å®‰å…¨æœºåˆ¶ï¼Œä¸ºå¢å¼ºæ¨¡å‹å®‰å…¨æ€§æä¾›äº†æ–°çš„è§è§£ã€‚ç›¸å…³ä»£ç å’Œæ•°æ®é›†å¯åœ¨NLPGM&#x2F;CAVGANç½‘ç«™è·å–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMçš„å®‰å…¨å¯¹é½èƒ½å¤ŸæŠµå¾¡æ¶æ„æŸ¥è¯¢ï¼Œä½†ä»å­˜åœ¨è¢«è¶Šç‹±æ”»å‡»æ–¹æ³•æ”»ç ´çš„é£é™©ã€‚</li>
<li>ç ”ç©¶è€…æå‡ºäº†ç»“åˆæ”»å‡»å’Œé˜²å¾¡çš„æ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼ºLLMçš„å®‰å…¨æ€§ã€‚</li>
<li>è¯¥æ¡†æ¶åŸºäºLLMä¸­é—´å±‚åµŒå…¥çš„çº¿æ€§å¯åˆ†ç¦»æ€§è´¨ä»¥åŠè¶Šç‹±æ”»å‡»çš„æœ¬è´¨å·¥ä½œã€‚</li>
<li>åˆ©ç”¨ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰åœ¨LLMå†…éƒ¨å­¦ä¹ å®‰å…¨åˆ¤æ–­è¾¹ç•Œæ˜¯ä¸€ç§æœ‰æ•ˆçš„æ‰‹æ®µã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¡†æ¶åœ¨å¤šä¸ªLLMä¸Šçš„å¹³å‡è¶Šç‹±æˆåŠŸç‡è¾ƒé«˜ï¼Œé˜²å¾¡æˆåŠŸç‡ä¹Ÿç›¸å¯¹è¾ƒé«˜ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ä»…éªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œä¹Ÿä¸ºç†è§£LLMçš„å†…éƒ¨å®‰å…¨æœºåˆ¶æä¾›äº†æ–°çš„è§†è§’ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.06043">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bd305989d1191bee33181f06ec37978b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-628eb2833c1e5451033007afde8cb37d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-108f870e149560a5615b9d1b67c53aac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-41b88f28942cfa514abf5eb8ca07f2d5.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="CogniSQL-R1-Zero-Lightweight-Reinforced-Reasoning-for-Efficient-SQL-Generation"><a href="#CogniSQL-R1-Zero-Lightweight-Reinforced-Reasoning-for-Efficient-SQL-Generation" class="headerlink" title="CogniSQL-R1-Zero: Lightweight Reinforced Reasoning for Efficient SQL   Generation"></a>CogniSQL-R1-Zero: Lightweight Reinforced Reasoning for Efficient SQL   Generation</h2><p><strong>Authors:Kushal Gajjar, Harshit Sikchi, Arpit Singh Gautam, Marc Hammons, Saurabh Jha</strong></p>
<p>Translating natural language into SQL (Text-to-SQL) remains a core challenge at the intersection of language understanding and structured data access. Although large language models (LLMs) have improved fluency, generating correct and executable SQL, especially for complex queries, continues to be challenging. We introduce CogniSQL-R1-Zero, a reinforcement learning (RL) framework and model that produces accurate SQL using a lightweight reward signal based on execution correctness and format-tag compliance. By avoiding intermediate supervision, hybrid pipelines and complex reward shaping, our method encourages stable learning and stronger alignment with the ultimate task objective-producing executable programs. CogniSQL-R1-Zero achieves state-of-the-art execution accuracy on Text2SQL benchmark; BIRD bench, outperforming prior supervised and instruction-tuned baselines including SFT CodeS-7B, DeepSeek-Coder 236B, and Mistral 123B-despite being trained on a significantly smaller 7B backbone. This result underscores the scalability and efficiency of our RL-based approach when trained on just four NVIDIA A100 GPUs (40 GB VRAM each). To support further research in efficient and interpretable Text-to-SQL modeling, we release two curated datasets: (i) a collection of 5,024 reasoning traces with varying context lengths, and (ii) a positive-sampled corpus of 36,356 corpus of weakly supervised queries, each annotated with six semantically diverse reasoning paths. Together, these contributions advance scalable, execution-aligned Text-to-SQL generation. </p>
<blockquote>
<p>å°†è‡ªç„¶è¯­è¨€ç¿»è¯‘æˆSQLï¼ˆæ–‡æœ¬åˆ°SQLï¼‰ä»ç„¶æ˜¯è¯­è¨€ç†è§£ä¸ç»“æ„åŒ–æ•°æ®è®¿é—®äº¤å‰ç‚¹çš„æ ¸å¿ƒæŒ‘æˆ˜ã€‚å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æé«˜äº†æµç•…æ€§ï¼Œä½†ç”Ÿæˆæ­£ç¡®ä¸”å¯æ‰§è¡Œçš„SQLï¼Œå°¤å…¶æ˜¯å¯¹äºå¤æ‚æŸ¥è¯¢ï¼Œä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æˆ‘ä»¬æ¨å‡ºäº†CogniSQL-R1-Zeroï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ¡†æ¶å’Œæ¨¡å‹ï¼Œå®ƒä½¿ç”¨åŸºäºæ‰§è¡Œæ­£ç¡®æ€§å’Œæ ¼å¼æ ‡ç­¾ç¬¦åˆçš„è½»é‡çº§å¥–åŠ±ä¿¡å·æ¥ç”Ÿæˆå‡†ç¡®çš„SQLã€‚é€šè¿‡é¿å…ä¸­é—´ç›‘ç£ã€æ··åˆç®¡é“å’Œå¤æ‚çš„å¥–åŠ±å¡‘é€ ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é¼“åŠ±ç¨³å®šçš„å­¦ä¹ ï¼Œå¹¶ä¸æœ€ç»ˆä»»åŠ¡ç›®æ ‡â€”â€”ç”Ÿæˆå¯æ‰§è¡Œç¨‹åºâ€”â€”æ›´ç´§å¯†åœ°ç»“åˆã€‚CogniSQL-R1-Zeroåœ¨Text2SQLåŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ‰§è¡Œå‡†ç¡®æ€§ï¼›å°½ç®¡æ˜¯åœ¨ä¸€ä¸ªæ˜¾è‘—è¾ƒå°çš„70äº¿å‚æ•°çš„åç«¯ä¸Šè¿›è¡Œè®­ç»ƒçš„ï¼Œä½†åœ¨BIRDåŸºå‡†æµ‹è¯•ä¸­ä»ä¼˜äºå…ˆå‰çš„ç›‘ç£æŒ‡ä»¤è°ƒæ•´åŸºçº¿ï¼ŒåŒ…æ‹¬SFT CodeS-7Bã€DeepSeek-Coder 236Bå’ŒMistral 123Bã€‚è¿™ä¸€ç»“æœçªå‡ºäº†æˆ‘ä»¬çš„åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•åœ¨ä»…ä½¿ç”¨å››ä¸ªNVIDIA A100 GPUï¼ˆæ¯ä¸ªæ‹¥æœ‰40GB VRAMï¼‰è¿›è¡Œè®­ç»ƒæ—¶çš„å¯æ‰©å±•æ€§å’Œæ•ˆç‡ã€‚ä¸ºäº†æ”¯æŒåœ¨é«˜æ•ˆä¸”å¯è§£é‡Šçš„æ–‡æœ¬åˆ°SQLå»ºæ¨¡æ–¹é¢çš„è¿›ä¸€æ­¥ç ”ç©¶ï¼Œæˆ‘ä»¬å‘å¸ƒäº†ä¸¤ä¸ªç²¾é€‰æ•°æ®é›†ï¼šï¼ˆiï¼‰åŒ…å«5024ä¸ªæ¨ç†è½¨è¿¹çš„é›†åˆï¼Œå…·æœ‰ä¸åŒçš„ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œï¼ˆiiï¼‰ä¸€ä¸ªç§¯ææ ·æœ¬è¯­æ–™åº“ï¼ŒåŒ…å«36356ä¸ªå¼±ç›‘ç£æŸ¥è¯¢çš„è¯­æ–™åº“ï¼Œæ¯ä¸ªéƒ½æ ‡æœ‰å…­æ¡è¯­ä¹‰å¤šæ ·çš„æ¨ç†è·¯å¾„ã€‚è¿™äº›è´¡çŒ®å…±åŒæ¨åŠ¨äº†å¯æ‰©å±•ã€ä¸æ‰§è¡Œç›¸åŒ¹é…çš„æ–‡æœ¬åˆ°SQLç”Ÿæˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.06013v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡ä»‹ç»äº†åœ¨è‡ªç„¶è¯­è¨€åˆ°SQLï¼ˆText-to-SQLï¼‰è½¬æ¢æ–¹é¢çš„æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ—¶ã€‚ä½œè€…æå‡ºäº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ¡†æ¶å’Œæ¨¡å‹â€”â€”CogniSQL-R1-Zeroï¼Œå®ƒèƒ½å¤Ÿæ ¹æ®æ‰§è¡Œæ­£ç¡®æ€§å’Œæ ¼å¼æ ‡ç­¾éµå®ˆæƒ…å†µï¼Œç”Ÿæˆå‡†ç¡®çš„SQLã€‚è¯¥æ–¹æ³•é¿å…äº†ä¸­é—´ç›‘ç£ã€æ··åˆç®¡é“å’Œå¤æ‚çš„å¥–åŠ±å¡‘é€ ï¼Œé¼“åŠ±ç¨³å®šå­¦ä¹ ï¼Œå¹¶ä¸æœ€ç»ˆä»»åŠ¡ç›®æ ‡â€”â€”ç”Ÿæˆå¯æ‰§è¡Œç¨‹åºæ›´ç´§å¯†å¯¹é½ã€‚CogniSQL-R1-Zeroåœ¨Text2SQLåŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ‰§è¡Œç²¾åº¦ï¼Œä¸”ä»…åœ¨å››ä¸ªNVIDIA A100 GPUä¸Šè®­ç»ƒï¼Œæ˜¾ç¤ºå‡ºå…¶å¯æ‰©å±•æ€§å’Œæ•ˆç‡ã€‚ä½œè€…è¿˜å‘å¸ƒäº†ä¸¤ä¸ªæ•°æ®é›†ä»¥æ”¯æŒè¿›ä¸€æ­¥çš„ç ”ç©¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªç„¶è¯­è¨€åˆ°SQLçš„è½¬æ¢æ˜¯ä¸€ä¸ªæ ¸å¿ƒæŒ‘æˆ˜ï¼Œæ¶‰åŠè¯­è¨€ç†è§£å’Œç»“æ„åŒ–æ•°æ®è®¿é—®çš„äº¤å‰ã€‚</li>
<li>å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æé«˜äº†æµç•…æ€§ï¼Œä½†ç”Ÿæˆæ­£ç¡®ä¸”å¯æ‰§è¡Œçš„SQLï¼Œå°¤å…¶æ˜¯å¤æ‚æŸ¥è¯¢ï¼Œä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>CogniSQL-R1-Zeroæ˜¯ä¸€ä¸ªåŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ¡†æ¶å’Œæ¨¡å‹ï¼Œå¯ç”Ÿæˆå‡†ç¡®çš„SQLã€‚</li>
<li>è¯¥æ–¹æ³•é¼“åŠ±ç¨³å®šå­¦ä¹ ï¼Œä¸æœ€ç»ˆä»»åŠ¡ç›®æ ‡â€”â€”ç”Ÿæˆå¯æ‰§è¡Œç¨‹åºæ›´ç´§å¯†å¯¹é½ï¼Œé¿å…ä¸­é—´ç›‘ç£ã€æ··åˆç®¡é“å’Œå¤æ‚çš„å¥–åŠ±å¡‘é€ ã€‚</li>
<li>CogniSQL-R1-Zeroåœ¨Text2SQLåŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ‰§è¡Œç²¾åº¦ï¼Œè®­ç»ƒæ‰€ä½¿ç”¨çš„GPUæ•°é‡è¾ƒå°‘ï¼Œæ˜¾ç¤ºå‡ºå…¶æ•ˆç‡ã€‚</li>
<li>ä½œè€…å‘å¸ƒäº†ä¸¤ä¸ªæ•°æ®é›†ä»¥æ”¯æŒText-to-SQLå»ºæ¨¡çš„è¿›ä¸€æ­¥ç ”ç©¶ï¼ŒåŒ…æ‹¬åŒ…å«ä¸åŒä¸Šä¸‹æ–‡é•¿åº¦çš„æ¨ç†è½¨è¿¹å’Œç§¯ææ ·æœ¬è¯­æ–™åº“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.06013">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-bd99e987a5dda13fd1c6b49a3b7b7641.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="When-Transformers-Meet-Recommenders-Integrating-Self-Attentive-Sequential-Recommendation-with-Fine-Tuned-LLMs"><a href="#When-Transformers-Meet-Recommenders-Integrating-Self-Attentive-Sequential-Recommendation-with-Fine-Tuned-LLMs" class="headerlink" title="When Transformers Meet Recommenders: Integrating Self-Attentive   Sequential Recommendation with Fine-Tuned LLMs"></a>When Transformers Meet Recommenders: Integrating Self-Attentive   Sequential Recommendation with Fine-Tuned LLMs</h2><p><strong>Authors:Kechen Liu</strong></p>
<p>Self-Attentive Sequential Recommendation (SASRec) effectively captures long-term user preferences by applying attention mechanisms to historical interactions. Concurrently, the rise of Large Language Models (LLMs) has motivated research into LLM-based recommendation, which leverages their powerful generalization and language understanding capabilities. However, LLMs often lack the domain-specific knowledge and collaborative signals essential for high-quality recommendations when relying solely on textual prompts. To address this limitation, this study proposes SASRecLLM, a novel framework that integrates SASRec as a collaborative encoder with an LLM fine-tuned using Low-Rank Adaptation (LoRA). The components are connected via a mapping layer to align their dimensional spaces, and three targeted training strategies are designed to optimize the hybrid architecture. Extensive experiments on multiple datasets demonstrate that SASRecLLM achieves robust and consistent improvements over strong baselines in both cold-start and warm-start scenarios. This work advances the field of LLM-based recommendation by presenting a modular and effective paradigm for fusing structured collaborative filtering with the semantic power of fine-tuned LLMs. The implementation is available on GitHub: <a target="_blank" rel="noopener" href="https://github.com/kechenkristin/RecLLM">https://github.com/kechenkristin/RecLLM</a> </p>
<blockquote>
<p>è‡ªæˆ‘å…³æ³¨åºåˆ—æ¨èï¼ˆSASRecï¼‰é€šè¿‡åº”ç”¨æ³¨æ„åŠ›æœºåˆ¶åˆ°å†å²äº¤äº’æ¥æœ‰æ•ˆåœ°æ•æ‰ç”¨æˆ·çš„é•¿æœŸåå¥½ã€‚åŒæ—¶ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å…´èµ·æ¨åŠ¨äº†åŸºäºLLMçš„æ¨èç ”ç©¶ï¼Œè¯¥æ¨èåˆ©ç”¨LLMå¼ºå¤§çš„æ³›åŒ–å’Œè¯­è¨€ç†è§£èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå½“ä»…ä¾èµ–æ–‡æœ¬æç¤ºæ—¶ï¼ŒLLMå¾€å¾€ç¼ºä¹ç‰¹å®šé¢†åŸŸçš„çŸ¥è¯†å’Œé‡è¦çš„ååŒä¿¡å·ï¼Œè¿™å¯¹äºé«˜è´¨é‡æ¨èè‡³å…³é‡è¦ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæœ¬ç ”ç©¶æå‡ºäº†SASRecLLMï¼Œè¿™æ˜¯ä¸€ä¸ªç»“åˆäº†SASRecååŒç¼–ç å™¨å’Œé€šè¿‡ä½ç§©é€‚é…ï¼ˆLoRAï¼‰å¾®è°ƒLLMçš„æ–°å‹æ¡†æ¶ã€‚è¿™äº›ç»„ä»¶é€šè¿‡æ˜ å°„å±‚è¿æ¥ä»¥å¯¹å…¶ç»´åº¦ç©ºé—´è¿›è¡Œå¯¹é½ï¼Œå¹¶è®¾è®¡äº†ä¸‰ç§æœ‰é’ˆå¯¹æ€§çš„è®­ç»ƒç­–ç•¥æ¥ä¼˜åŒ–æ··åˆæ¶æ„ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒSASRecLLMåœ¨å†·å¯åŠ¨å’Œæš–å¯åŠ¨åœºæ™¯ä¸­å‡å®ç°äº†å¯¹å¼ºåŸºå‡†çº¿çš„ç¨³å¥å’Œä¸€è‡´æ”¹è¿›ã€‚æœ¬ç ”ç©¶é€šè¿‡å±•ç¤ºèåˆç»“æ„åŒ–ååŒè¿‡æ»¤ä¸å¾®è°ƒLLMè¯­ä¹‰èƒ½åŠ›çš„æ¨¡å—åŒ–æœ‰æ•ˆèŒƒå¼ï¼Œæ¨åŠ¨äº†åŸºäºLLMçš„æ¨èé¢†åŸŸçš„å‘å±•ã€‚å®ç°ä»£ç å·²åœ¨GitHubä¸Šæä¾›ï¼š<a target="_blank" rel="noopener" href="https://github.com/kechenkristin/RecLLM%E3%80%82">https://github.com/kechenkristin/RecLLMã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.05733v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†SASRecLLMæ¡†æ¶ï¼Œå®ƒç»“åˆäº†SASRecååŒç¼–ç å™¨ä¸ç»è¿‡LoRAå¾®è°ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚é€šè¿‡æ˜ å°„å±‚è¿æ¥å„ç»„ä»¶ä»¥å¯¹é½ç»´åº¦ç©ºé—´ï¼Œå¹¶è®¾è®¡ä¸‰ç§é’ˆå¯¹æ€§è®­ç»ƒç­–ç•¥ä¼˜åŒ–æ··åˆæ¶æ„ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSASRecLLMåœ¨å†·å¯åŠ¨å’Œæš–å¯åŠ¨åœºæ™¯ä¸­å‡å®ç°äº†å¯¹å¼ºåŸºå‡†çº¿çš„ç¨³å¥å’Œä¸€è‡´æ”¹è¿›ã€‚è¯¥ç ”ç©¶ä¸ºèåˆç»“æ„åŒ–ååŒè¿‡æ»¤ä¸å¾®è°ƒLLMè¯­ä¹‰åŠ›é‡çš„æ¨¡å—åŒ–æœ‰æ•ˆèŒƒå¼åœ¨æ¨èç³»ç»Ÿé¢†åŸŸæä¾›äº†è¿›å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SASRecLLMç»“åˆäº†SASRecå’ŒLLMï¼Œæ—¨åœ¨åˆ©ç”¨ä¸¤è€…çš„ä¼˜åŠ¿ï¼Œæœ‰æ•ˆæ•æ‰ç”¨æˆ·é•¿æœŸåå¥½å¹¶èåˆé¢†åŸŸçŸ¥è¯†ã€‚</li>
<li>SASRecé€šè¿‡æ³¨æ„åŠ›æœºåˆ¶å¯¹å†å²äº¤äº’è¿›è¡Œç¼–ç ï¼Œè€ŒLLMåˆ™æä¾›å¼ºå¤§çš„æ³›åŒ–å’Œè¯­è¨€ç†è§£èƒ½åŠ›ã€‚</li>
<li>LoRAæŠ€æœ¯ç”¨äºå¾®è°ƒLLMï¼Œä»¥é€‚åº”æ¨èç³»ç»Ÿçš„ç‰¹å®šéœ€æ±‚ã€‚</li>
<li>SASRecLLMé€šè¿‡æ˜ å°„å±‚æ•´åˆäº†SASRecå’ŒLLMçš„ç»´åº¦ç©ºé—´ã€‚</li>
<li>è¯¥æ¡†æ¶é‡‡ç”¨ä¸‰ç§é’ˆå¯¹æ€§è®­ç»ƒç­–ç•¥æ¥ä¼˜åŒ–æ··åˆæ¶æ„çš„æ€§èƒ½ã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSASRecLLMåœ¨å†·å¯åŠ¨å’Œæš–å¯åŠ¨æƒ…å†µä¸‹å‡è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.05733">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-f4448e9859cf9aa27fbdb4c59481d3c8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-886b3f84b8e3a4bf7b43ca0e85e6e942.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0bb617bf17ded7a6f36bd67fcdc71693.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-92ee2cf42398440f62ef0c740b027040.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cb7c2318d873d75cc509f18332b98536.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="GAF-Guard-An-Agentic-Framework-for-Risk-Management-and-Governance-in-Large-Language-Models"><a href="#GAF-Guard-An-Agentic-Framework-for-Risk-Management-and-Governance-in-Large-Language-Models" class="headerlink" title="GAF-Guard: An Agentic Framework for Risk Management and Governance in   Large Language Models"></a>GAF-Guard: An Agentic Framework for Risk Management and Governance in   Large Language Models</h2><p><strong>Authors:Seshu Tirupathi, Dhaval Salwala, Elizabeth Daly, Inge Vejsbjerg</strong></p>
<p>As Large Language Models (LLMs) continue to be increasingly applied across various domains, their widespread adoption necessitates rigorous monitoring to prevent unintended negative consequences and ensure robustness. Furthermore, LLMs must be designed to align with human values, like preventing harmful content and ensuring responsible usage. The current automated systems and solutions for monitoring LLMs in production are primarily centered on LLM-specific concerns like hallucination etc, with little consideration given to the requirements of specific use-cases and user preferences. This paper introduces GAF-Guard, a novel agentic framework for LLM governance that places the user, the use-case, and the model itself at the center. The framework is designed to detect and monitor risks associated with the deployment of LLM based applications. The approach models autonomous agents that identify risks, activate risk detection tools, within specific use-cases and facilitate continuous monitoring and reporting to enhance AI safety, and user expectations. The code is available at <a target="_blank" rel="noopener" href="https://github.com/IBM/risk-atlas-nexus-demos/tree/main/gaf-guard">https://github.com/IBM/risk-atlas-nexus-demos/tree/main/gaf-guard</a>. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„ä¸ªé¢†åŸŸçš„åº”ç”¨æ—¥ç›Šå¹¿æ³›ï¼Œä¸ºäº†é˜²æ­¢å‡ºç°æ„å¤–çš„è´Ÿé¢åæœå¹¶ç¡®ä¿å…¶ç¨³å¥æ€§ï¼Œå¿…é¡»å¯¹å…¶è¿›è¡Œä¸¥æ ¼ç›‘æ§ã€‚æ­¤å¤–ï¼ŒLLMçš„è®¾è®¡å¿…é¡»ä¸äººç±»ä»·å€¼è§‚ç›¸ä¸€è‡´ï¼Œå¦‚é˜²æ­¢æœ‰å®³å†…å®¹å¹¶ç¡®ä¿è´Ÿè´£ä»»çš„ä½¿ç”¨ã€‚å½“å‰ç”¨äºç›‘æ§ç”Ÿäº§ç¯å¢ƒä¸­LLMçš„è‡ªåŠ¨åŒ–ç³»ç»Ÿå’Œè§£å†³æ–¹æ¡ˆä¸»è¦é›†ä¸­äºLLMç‰¹å®šçš„å…³æ³¨ç‚¹ï¼Œå¦‚å¹»è§‰ç­‰ï¼Œè€Œå¾ˆå°‘è€ƒè™‘ç‰¹å®šç”¨ä¾‹å’Œç”¨æˆ·åå¥½ã€‚æœ¬æ–‡ä»‹ç»äº†GAF-Guardï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºLLMæ²»ç†çš„æ–°å‹ä»£ç†æ¡†æ¶ï¼Œä»¥ç”¨æˆ·ã€ç”¨ä¾‹å’Œæ¨¡å‹æœ¬èº«ä¸ºä¸­å¿ƒã€‚è¯¥æ¡†æ¶æ—¨åœ¨æ£€æµ‹å’Œç›‘æ§ä¸éƒ¨ç½²LLMåº”ç”¨ç¨‹åºç›¸å…³çš„é£é™©ã€‚è¯¥æ–¹æ³•å»ºç«‹è‡ªä¸»ä»£ç†ï¼Œèƒ½å¤Ÿåœ¨ç‰¹å®šç”¨ä¾‹ä¸­è¯†åˆ«é£é™©ã€æ¿€æ´»é£é™©æ£€æµ‹å·¥å…·ï¼Œå¹¶ä¿ƒè¿›æŒç»­ç›‘æ§å’ŒæŠ¥å‘Šï¼Œä»¥æé«˜äººå·¥æ™ºèƒ½çš„å®‰å…¨æ€§å’Œç”¨æˆ·æœŸæœ›ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/IBM/risk-atlas-nexus-demos/tree/main/gaf-guard%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/IBM/risk-atlas-nexus-demos/tree/main/gaf-guardæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.02986v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¹¿æ³›åº”ç”¨éœ€è¦ä¸¥æ ¼çš„ç›‘æ§ä»¥ç¡®ä¿å…¶ç¨³å¥æ€§å’Œé¿å…äº§ç”Ÿè´Ÿé¢åæœã€‚æœ¬æ–‡ä»‹ç»äº†GAF-Guardæ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ—¨åœ¨é’ˆå¯¹ç‰¹å®šç”¨ä¾‹å’Œç”¨æˆ·åå¥½è¿›è¡ŒLLMçš„é£é™©æ£€æµ‹ä¸ç›‘æ§ã€‚é€šè¿‡è‡ªä¸»ä»£ç†è¯†åˆ«é£é™©ã€æ¿€æ´»é£é™©æ£€æµ‹å·¥å…·ï¼Œå®ç°æŒç»­ç›‘æ§å’ŒæŠ¥å‘Šï¼Œæé«˜äººå·¥æ™ºèƒ½çš„å®‰å…¨æ€§å’Œç”¨æˆ·æ»¡æ„åº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMçš„å¹¿æ³›åº”ç”¨éœ€è¦å¯¹å…¶è¿›è¡Œä¸¥æ ¼çš„ç›‘æ§ä»¥ç¡®ä¿å…¶ç¨³å¥æ€§å¹¶é¿å…äº§ç”Ÿè´Ÿé¢åæœã€‚</li>
<li>å½“å‰LLMçš„ç›‘æµ‹ç³»ç»Ÿä¸»è¦å…³æ³¨æ¨¡å‹æœ¬èº«çš„é—®é¢˜ï¼Œä½†å¿½ç•¥äº†ç‰¹å®šç”¨ä¾‹å’Œç”¨æˆ·åå¥½ã€‚</li>
<li>GAF-Guardæ˜¯ä¸€ä¸ªæ–°å‹çš„LLMæ²»ç†æ¡†æ¶ï¼Œå°†ç”¨æˆ·ã€ç”¨ä¾‹å’Œæ¨¡å‹æœ¬èº«ç½®äºä¸­å¿ƒä½ç½®ã€‚</li>
<li>GAF-Guardé€šè¿‡è‡ªä¸»ä»£ç†è¯†åˆ«é£é™©å¹¶æ¿€æ´»é£é™©æ£€æµ‹å·¥å…·ï¼Œä»¥å®ç°å¯¹LLMåº”ç”¨çš„æŒç»­ç›‘æ§ã€‚</li>
<li>è¯¥æ¡†æ¶æ—¨åœ¨æé«˜äººå·¥æ™ºèƒ½çš„å®‰å…¨æ€§å’Œç”¨æˆ·æ»¡æ„åº¦ã€‚</li>
<li>GAF-Guardæ¡†æ¶å…·æœ‰å¼ºå¤§çš„é€‚ç”¨æ€§ï¼Œå¯ä»¥é€‚åº”ä¸åŒçš„ä½¿ç”¨åœºæ™¯å’Œéœ€æ±‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.02986">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0a100499a8a8b3f8795612f313294c32.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-48f38f7f1157c5a49d5de88e036abd7d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a92f85235f14b24f9828551649db3c6d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5a3a953ffa1d4a1493e34939418eb844.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-208b7a15dd8bd3067e6b9a1b9c0b63a5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0e10ba0dc07770edc3b49e0d3faff54b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ae3d557d5fb5c5488a4cdb29b4b17196.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Tailored-Conversations-beyond-LLMs-A-RL-Based-Dialogue-Manager"><a href="#Tailored-Conversations-beyond-LLMs-A-RL-Based-Dialogue-Manager" class="headerlink" title="Tailored Conversations beyond LLMs: A RL-Based Dialogue Manager"></a>Tailored Conversations beyond LLMs: A RL-Based Dialogue Manager</h2><p><strong>Authors:Lucie Galland, Catherine Pelachaud, Florian Pecune</strong></p>
<p>In this work, we propose a novel framework that integrates large language models (LLMs) with an RL-based dialogue manager for open-ended dialogue with a specific goal. By leveraging hierarchical reinforcement learning to model the structured phases of dialogue and employ meta-learning to enhance adaptability across diverse user profiles, our approach enhances adaptability and efficiency, enabling the system to learn from limited data, transition fluidly between dialogue phases, and personalize responses to heterogeneous patient needs. We apply our framework to Motivational Interviews, aiming to foster behavior change, and demonstrate that the proposed dialogue manager outperforms a state-of-the-art LLM baseline in terms of reward, showing a potential benefit of conditioning LLMs to create open-ended dialogue systems with specific goals. </p>
<blockquote>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸åŸºäºRLçš„å¯¹è¯ç®¡ç†å™¨é›†æˆåœ¨ä¸€èµ·ï¼Œç”¨äºå®ç°å…·æœ‰ç‰¹å®šç›®æ ‡çš„å¼€æ”¾å¼å¯¹è¯ã€‚æˆ‘ä»¬é€šè¿‡åˆ©ç”¨åˆ†å±‚å¼ºåŒ–å­¦ä¹ æ¥æ¨¡æ‹Ÿå¯¹è¯çš„ç»“æ„åŒ–é˜¶æ®µï¼Œå¹¶è¿ç”¨å…ƒå­¦ä¹ æ¥æé«˜åœ¨ä¸åŒç”¨æˆ·é…ç½®æ–‡ä»¶ä¹‹é—´çš„é€‚åº”æ€§ï¼Œä»è€Œæé«˜é€‚åº”æ€§å’Œæ•ˆç‡ï¼Œä½¿ç³»ç»Ÿèƒ½å¤Ÿåœ¨æœ‰é™çš„æ•°æ®ä¸­å­¦ä¹ ï¼Œåœ¨å¯¹è¯é˜¶æ®µä¹‹é—´å¹³ç¨³è¿‡æ¸¡ï¼Œå¹¶å¯¹ä¸åŒæ‚£è€…çš„éœ€æ±‚åšå‡ºä¸ªæ€§åŒ–å“åº”ã€‚æˆ‘ä»¬å°†è¯¥æ¡†æ¶åº”ç”¨äºåŠ¨æœºé¢è¯•ï¼Œæ—¨åœ¨ä¿ƒè¿›è¡Œä¸ºæ”¹å˜ï¼Œå¹¶è¯æ˜æ‰€æå‡ºçš„å¯¹è¯ç®¡ç†å™¨åœ¨å¥–åŠ±æ–¹é¢ä¼˜äºæœ€æ–°LLMåŸºå‡†æµ‹è¯•ï¼Œæ˜¾ç¤ºå‡ºå°†LLMè®¾ç½®ä¸ºåˆ›å»ºå…·æœ‰ç‰¹å®šç›®æ ‡çš„å¼€æ”¾å¼å¯¹è¯ç³»ç»Ÿçš„æ½œåœ¨ä¼˜åŠ¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.19652v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹æ¡†æ¶ï¼Œå®ƒå°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸åŸºäºå¼ºåŒ–å­¦ä¹ çš„å¯¹è¯ç®¡ç†å™¨ç›¸ç»“åˆï¼Œç”¨äºå®ç°å…·æœ‰ç‰¹å®šç›®æ ‡çš„å¼€æ”¾å¼å¯¹è¯ã€‚é€šè¿‡åˆ©ç”¨åˆ†å±‚å¼ºåŒ–å­¦ä¹ æ¥æ¨¡æ‹Ÿå¯¹è¯çš„ç»“æ„åŒ–é˜¶æ®µï¼Œå¹¶å€ŸåŠ©å…ƒå­¦ä¹ æ¥æé«˜å¯¹ä¸åŒç”¨æˆ·éœ€æ±‚çš„é€‚åº”æ€§ï¼Œæ­¤æ–¹æ³•å¢å¼ºäº†ç³»ç»Ÿçš„é€‚åº”æ€§å’Œæ•ˆç‡ï¼Œä½¿ç³»ç»Ÿèƒ½å¤Ÿåœ¨æœ‰é™çš„æ•°æ®ä¸­å­¦ä¹ ï¼Œæµç•…åœ°è¿‡æ¸¡å¯¹è¯é˜¶æ®µï¼Œå¹¶å¯¹ä¸åŒæ‚£è€…çš„éœ€æ±‚è¿›è¡Œä¸ªæ€§åŒ–å›åº”ã€‚æœ¬æ–‡å°†æ¡†æ¶åº”ç”¨äºåŠ¨æœºé¢è¯•ï¼Œæ—¨åœ¨ä¿ƒè¿›è¡Œä¸ºæ”¹å˜ï¼Œå¹¶è¯æ˜æ‰€æå‡ºçš„å¯¹è¯ç®¡ç†å™¨åœ¨å¥–åŠ±æ–¹é¢ä¼˜äºæœ€å…ˆè¿›LLMåŸºçº¿ï¼Œæ˜¾ç¤ºå‡ºå°†LLMsæ¡ä»¶åŒ–ä»¥åˆ›å»ºå…·æœ‰ç‰¹å®šç›®æ ‡çš„å¼€æ”¾å¼å¯¹è¯ç³»ç»Ÿçš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§ç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œå¼ºåŒ–å­¦ä¹ å¯¹è¯ç®¡ç†å™¨çš„æ–°å‹æ¡†æ¶ã€‚</li>
<li>åˆ©ç”¨åˆ†å±‚å¼ºåŒ–å­¦ä¹ æ¨¡æ‹Ÿå¯¹è¯çš„ç»“æ„åŒ–é˜¶æ®µï¼Œæé«˜ç³»ç»Ÿçš„å¯¹è¯ç®¡ç†æ•ˆç‡ã€‚</li>
<li>é€šè¿‡å…ƒå­¦ä¹ å¢å¼ºå¯¹ä¸åŒç”¨æˆ·éœ€æ±‚çš„é€‚åº”æ€§ï¼Œä½¿ç³»ç»Ÿèƒ½å¤Ÿé€‚åº”å¤šæ ·åŒ–çš„ç”¨æˆ·é…ç½®æ–‡ä»¶ã€‚</li>
<li>æ¡†æ¶åº”ç”¨äºåŠ¨æœºé¢è¯•ï¼Œæ—¨åœ¨ä¿ƒè¿›è¡Œä¸ºæ”¹å˜ã€‚</li>
<li>æå‡ºçš„å¯¹è¯ç®¡ç†å™¨åœ¨å¥–åŠ±æ–¹é¢ä¼˜äºç°æœ‰çš„LLMåŸºçº¿ã€‚</li>
<li>è¡¨æ˜ç»“åˆLLMså’Œå¼ºåŒ–å­¦ä¹ å¯ä»¥åˆ›å»ºå…·æœ‰ç‰¹å®šç›®æ ‡çš„å¼€æ”¾å¼å¯¹è¯ç³»ç»Ÿã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.19652">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c5e7bb6c56ed42dc20dd45d683a2e5e4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f5e1078ccfbbc9914d62d08e60dee349.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b14465b24ac8ac3cfe6e2093cc18adeb.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Hume-Introducing-System-2-Thinking-in-Visual-Language-Action-Model"><a href="#Hume-Introducing-System-2-Thinking-in-Visual-Language-Action-Model" class="headerlink" title="Hume: Introducing System-2 Thinking in Visual-Language-Action Model"></a>Hume: Introducing System-2 Thinking in Visual-Language-Action Model</h2><p><strong>Authors:Haoming Song, Delin Qu, Yuanqi Yao, Qizhi Chen, Qi Lv, Yiwen Tang, Modi Shi, Guanghui Ren, Maoqing Yao, Bin Zhao, Dong Wang, Xuelong Li</strong></p>
<p>Humans practice slow thinking before performing actual actions when handling complex tasks in the physical world. This thinking paradigm, recently, has achieved remarkable advancement in boosting Large Language Models (LLMs) to solve complex tasks in digital domains. However, the potential of slow thinking remains largely unexplored for robotic foundation models interacting with the physical world. In this work, we propose Hume: a dual-system Vision-Language-Action (VLA) model with value-guided System-2 thinking and cascaded action denoising, exploring human-like thinking capabilities of Vision-Language-Action models for dexterous robot control. System 2 of Hume implements value-Guided thinking by extending a Vision-Language-Action Model backbone with a novel value-query head to estimate the state-action value of predicted actions. The value-guided thinking is conducted by repeat sampling multiple action candidates and selecting one according to state-action value. System 1 of Hume is a lightweight reactive visuomotor policy that takes System 2 selected action and performs cascaded action denoising for dexterous robot control. At deployment time, System 2 performs value-guided thinking at a low frequency while System 1 asynchronously receives the System 2 selected action candidate and predicts fluid actions in real time. We show that Hume outperforms the existing state-of-the-art Vision-Language-Action models across multiple simulation benchmark and real-robot deployments. </p>
<blockquote>
<p>åœ¨å¤„ç†ç‰©ç†ä¸–ç•Œçš„å¤æ‚ä»»åŠ¡æ—¶ï¼Œäººç±»ä¼šåœ¨å®é™…è¡ŒåŠ¨ä¹‹å‰è¿›è¡Œæ…¢æ€è€ƒã€‚æœ€è¿‘ï¼Œè¿™ç§æ€è€ƒèŒƒå¼åœ¨æ¨åŠ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ•°å­—é¢†åŸŸè§£å†³å¤æ‚ä»»åŠ¡æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œæ…¢æ€è€ƒåœ¨æœºå™¨äººåŸºç¡€æ¨¡å‹ä¸ç‰©ç†ä¸–ç•Œäº¤äº’ä¸­çš„æ½œåŠ›å°šæœªå¾—åˆ°å¹¿æ³›æ¢ç´¢ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Humeï¼šä¸€ç§å…·æœ‰ä»·å€¼å¯¼å‘çš„System-2æ€è€ƒå’Œçº§è”åŠ¨ä½œå»å™ªçš„åŒç³»ç»Ÿè§†è§‰è¯­è¨€åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹ï¼Œæ¢ç´¢è§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡å‹åœ¨çµå·§æœºå™¨äººæ§åˆ¶æ–¹é¢çš„äººç±»æ€ç»´èƒ½åŠ›ã€‚Humeçš„System 2é€šè¿‡æ‰©å±•è§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡å‹çš„éª¨å¹²ç½‘å¹¶åŠ å…¥æ–°å‹ä»·å€¼æŸ¥è¯¢å¤´æ¥ä¼°è®¡é¢„æµ‹åŠ¨ä½œçš„çŠ¶æ€-åŠ¨ä½œä»·å€¼ï¼Œä»è€Œå®ç°ä»·å€¼å¯¼å‘çš„æ€è€ƒã€‚ä»·å€¼å¯¼å‘çš„æ€è€ƒæ˜¯é€šè¿‡å¤šæ¬¡é‡‡æ ·å¤šä¸ªåŠ¨ä½œå€™é€‰è€…å¹¶æ ¹æ®çŠ¶æ€-åŠ¨ä½œä»·å€¼è¿›è¡Œé€‰æ‹©æ¥å®Œæˆçš„ã€‚Humeçš„System 1æ˜¯ä¸€ä¸ªè½»é‡çº§çš„ååº”å‹è§†è§‰è¿åŠ¨ç­–ç•¥ï¼Œå®ƒæ¥å—System 2é€‰æ‹©çš„åŠ¨ä½œï¼Œå¹¶è¿›è¡Œçº§è”åŠ¨ä½œå»å™ªï¼Œä»¥å®ç°çµå·§çš„æœºå™¨äººæ§åˆ¶ã€‚åœ¨éƒ¨ç½²æ—¶ï¼ŒSystem 2ä»¥è¾ƒä½é¢‘ç‡è¿›è¡Œä»·å€¼å¯¼å‘çš„æ€è€ƒï¼Œè€ŒSystem 1åˆ™å¼‚æ­¥æ¥æ”¶System 2é€‰æ‹©çš„åŠ¨ä½œå€™é€‰è€…å¹¶å®æ—¶é¢„æµ‹æµç•…çš„åŠ¨ä½œã€‚æˆ‘ä»¬åœ¨å¤šä¸ªä»¿çœŸåŸºå‡†æµ‹è¯•å’ŒçœŸå®æœºå™¨äººéƒ¨ç½²ä¸­å±•ç¤ºäº†Humeè¶…è¶Šç°æœ‰æœ€å…ˆè¿›çš„è§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡å‹çš„è¡¨ç°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21432v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†åœ¨å¤„ç†ç‰©ç†ä¸–ç•Œçš„å¤æ‚ä»»åŠ¡æ—¶ï¼Œäººç±»åœ¨è¿›è¡Œå®é™…æ“ä½œå‰ä¼šè¿›è¡Œæ…¢æ€è€ƒçš„æ¨¡å¼ã€‚æœ€è¿‘ï¼Œè¿™ç§æ€è€ƒæ¨¡å¼åœ¨æ¨åŠ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è§£å†³æ•°å­—é¢†åŸŸçš„å¤æ‚ä»»åŠ¡æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œå¯¹äºæœºå™¨äººåŸºç¡€æ¨¡å‹ä¸ç‰©ç†ä¸–ç•Œçš„äº¤äº’è€Œè¨€ï¼Œæ…¢æ€è€ƒçš„å·¨å¤§æ½œåŠ›å°šæœªå¾—åˆ°å¹¿æ³›æ¢ç´¢ã€‚æœ¬æ–‡æå‡ºäº†ä¸€é¡¹æ¢ç´¢äººä¸€æ ·çš„æ€è€ƒèƒ½åŠ›çš„æŠ€æœ¯â€”â€”Humeï¼Œè¿™æ˜¯ä¸€ç§å…·æœ‰ä»·å€¼å¯¼å‘çš„System-2æ€è€ƒå’Œçº§è”åŠ¨ä½œå»å™ªçš„åŒç³»ç»Ÿè§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹ã€‚System 2é€šè¿‡å¢åŠ ä¸€ä¸ªä»·å€¼æŸ¥è¯¢å¤´æ¥ä¼°è®¡é¢„æµ‹åŠ¨ä½œçš„çŠ¶æ€-åŠ¨ä½œä»·å€¼ï¼Œä»è€Œå®ç°ä»·å€¼å¯¼å‘çš„æ€è€ƒã€‚ä»·å€¼å¯¼å‘çš„æ€è€ƒæ˜¯é€šè¿‡é‡å¤é‡‡æ ·å¤šä¸ªåŠ¨ä½œå€™é€‰è€…å¹¶é€‰æ‹©å…·æœ‰æœ€é«˜çŠ¶æ€-åŠ¨ä½œä»·å€¼çš„åŠ¨ä½œæ¥å®ç°çš„ã€‚System 1æ˜¯Humeçš„ä¸€ä¸ªè½»é‡çº§ååº”è§†è§‰è¿åŠ¨ç­–ç•¥ï¼Œå®ƒæ¥æ”¶System 2é€‰æ‹©çš„åŠ¨ä½œå¹¶è¿›è¡Œçº§è”åŠ¨ä½œå»å™ªï¼Œä»¥å®ç°ç²¾ç»†çš„æœºå™¨äººæ§åˆ¶ã€‚åœ¨éƒ¨ç½²æ—¶ï¼ŒSystem 2ä»¥è¾ƒä½é¢‘ç‡è¿›è¡Œä»·å€¼å¯¼å‘çš„æ€è€ƒï¼Œè€ŒSystem 1åˆ™å¼‚æ­¥æ¥æ”¶System 2é€‰æ‹©çš„åŠ¨ä½œå€™é€‰è€…å¹¶å®æ—¶é¢„æµ‹æµç•…çš„åŠ¨ä½œã€‚ç ”ç©¶è¡¨æ˜ï¼ŒHumeåœ¨å¤šä¸ªæ¨¡æ‹ŸåŸºå‡†æµ‹è¯•å’ŒçœŸå®æœºå™¨äººéƒ¨ç½²æ–¹é¢çš„è¡¨ç°å‡ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äººç±»åœ¨å¤„ç†ç‰©ç†ä¸–ç•Œçš„å¤æ‚ä»»åŠ¡æ—¶ä¼šé‡‡ç”¨æ…¢æ€è€ƒæ¨¡å¼ï¼Œè¿™ä¸€æ¨¡å¼åœ¨æ•°å­—é¢†åŸŸçš„LLMä¸­å·²æœ‰æ˜¾è‘—è¿›å±•ã€‚</li>
<li>æœºå™¨äººåŸºç¡€æ¨¡å‹åœ¨ä¸ç‰©ç†ä¸–ç•Œäº¤äº’æ—¶ï¼Œæ…¢æ€è€ƒæ¨¡å¼çš„æ½œåŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚</li>
<li>Humeæ˜¯ä¸€ä¸ªåŒç³»ç»Ÿçš„è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹ï¼Œç»“åˆäº†ä»·å€¼å¯¼å‘çš„System-2æ€è€ƒå’Œçº§è”åŠ¨ä½œå»å™ªæŠ€æœ¯ã€‚</li>
<li>System 2é€šè¿‡ä»·å€¼æŸ¥è¯¢å¤´å®ç°ä»·å€¼å¯¼å‘çš„æ€è€ƒï¼Œé€šè¿‡é‡å¤é‡‡æ ·å’Œé€‰æ‹©æœ€é«˜çŠ¶æ€-åŠ¨ä½œä»·å€¼çš„åŠ¨ä½œæ¥è¿›è¡Œæ€è€ƒã€‚</li>
<li>System 1æ˜¯ä¸€ä¸ªè½»é‡çº§çš„ååº”è§†è§‰è¿åŠ¨ç­–ç•¥ï¼Œç”¨äºæ¥æ”¶System 2é€‰æ‹©çš„åŠ¨ä½œå¹¶è¿›è¡Œå»å™ªå¤„ç†ï¼Œä»¥å®ç°ç²¾ç»†çš„æœºå™¨äººæ§åˆ¶ã€‚</li>
<li>åœ¨éƒ¨ç½²æ—¶ï¼ŒHumeçš„System 2ä»¥ä½é¢‘ç‡è¿›è¡Œä»·å€¼å¯¼å‘çš„æ€è€ƒï¼Œè€ŒSystem 1åˆ™å®æ—¶é¢„æµ‹æµç•…çš„åŠ¨ä½œã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21432">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1971d3e1e731d8b7f2f68cac1965cf6b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-79c3ef85b9c92fcd6b72c814d99c0d51.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-abc900831161b510cd85e0811bc81c96.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="MAIN-Mutual-Alignment-Is-Necessary-for-instruction-tuning"><a href="#MAIN-Mutual-Alignment-Is-Necessary-for-instruction-tuning" class="headerlink" title="MAIN: Mutual Alignment Is Necessary for instruction tuning"></a>MAIN: Mutual Alignment Is Necessary for instruction tuning</h2><p><strong>Authors:Fanyi Yang, Jianfeng Liu, Xin Zhang, Haoyu Liu, Xixin Cao, Yuefeng Zhan, Hao Sun, Weiwei Deng, Feng Sun, Qi Zhang</strong></p>
<p>Instruction tuning has empowered large language models (LLMs) to achieve remarkable performance, yet its success heavily depends on the availability of large-scale, high-quality instruction-response pairs. To meet this demand, various methods have been developed to synthesize data at scale. However, current methods for scaling up data generation often overlook a crucial aspect: the alignment between instructions and responses. We hypothesize that the quality of instruction-response pairs is determined not by the individual quality of each component, but by the degree of mutual alignment. To address this, we propose a Mutual Alignment Framework (MAIN) which enforces coherence between instructions and responses through mutual constraints. We demonstrate that MAIN generalizes well across model architectures and sizes, achieving state-of-the-art performance on LLaMA, Mistral, and Qwen models across diverse benchmarks. This work underscores the critical role of instruction-response alignment in enabling generalizable and high-quality instruction tuning for LLMs. </p>
<blockquote>
<p>æŒ‡ä»¤å¾®è°ƒå·²ç»ä½¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œä½†å…¶æˆåŠŸå¾ˆå¤§ç¨‹åº¦ä¸Šå–å†³äºå¤§è§„æ¨¡é«˜è´¨é‡æŒ‡ä»¤-å“åº”å¯¹çš„å¯ç”¨æ€§ã€‚ä¸ºäº†æ»¡è¶³è¿™ä¸€éœ€æ±‚ï¼Œå·²ç»å¼€å‘äº†å„ç§æ–¹æ³•æ¥å¤§è§„æ¨¡åˆæˆæ•°æ®ã€‚ç„¶è€Œï¼Œå½“å‰æ‰©å¤§æ•°æ®ç”Ÿæˆè§„æ¨¡çš„æ–¹æ³•å¾€å¾€å¿½è§†äº†ä¸€ä¸ªå…³é”®æ–¹é¢ï¼šæŒ‡ä»¤å’Œå“åº”ä¹‹é—´çš„å¯¹é½ã€‚æˆ‘ä»¬å‡è®¾æŒ‡ä»¤-å“åº”å¯¹çš„è´¨å¹¶ä¸å–å†³äºæ¯ä¸ªç»„ä»¶çš„å•ç‹¬è´¨é‡ï¼Œè€Œæ˜¯å–å†³äºç›¸äº’å¯¹é½çš„ç¨‹åº¦ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ç›¸äº’å¯¹é½æ¡†æ¶ï¼ˆMAINï¼‰ï¼Œé€šè¿‡ç›¸äº’çº¦æŸæ¥ç¡®ä¿æŒ‡ä»¤å’Œå“åº”ä¹‹é—´çš„è¿è´¯æ€§ã€‚æˆ‘ä»¬è¯æ˜MAINåœ¨æ¨¡å‹æ¶æ„å’Œè§„æ¨¡ä¸Šå…·æœ‰å¾ˆå¥½çš„é€šç”¨æ€§ï¼Œåœ¨LLaMAã€Mistralå’ŒQwenæ¨¡å‹ä¸Šå®ç°å¤šæ ·åŒ–åŸºå‡†æµ‹è¯•çš„æœ€ä½³æ€§èƒ½ã€‚è¿™é¡¹å·¥ä½œå¼ºè°ƒäº†æŒ‡ä»¤å“åº”å¯¹é½åœ¨ä½¿å¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿè¿›è¡Œé€šç”¨åŒ–å’Œé«˜è´¨é‡æŒ‡ä»¤è°ƒæ•´æ–¹é¢çš„é‡è¦ä½œç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.12913v2">PDF</a> </p>
<p><strong>Summary</strong><br>æŒ‡ä»¤å¾®è°ƒä½¿å¾—å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œä½†æˆåŠŸå¾ˆå¤§ç¨‹åº¦ä¸Šå–å†³äºå¤§è§„æ¨¡é«˜è´¨é‡æŒ‡ä»¤å“åº”å¯¹çš„å¯ç”¨æ€§ã€‚ä¸ºæ»¡è¶³è¿™ä¸€éœ€æ±‚ï¼Œå·²ç»å¼€å‘äº†å„ç§æ–¹æ³•æ¥å¤§è§„æ¨¡åˆæˆæ•°æ®ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ•°æ®ç”Ÿæˆæ–¹æ³•å¾€å¾€å¿½ç•¥äº†å…³é”®æ–¹é¢ï¼šæŒ‡ä»¤ä¸å“åº”ä¹‹é—´çš„å¯¹é½ã€‚æœ¬å·¥ä½œå‡è®¾æŒ‡ä»¤å“åº”å¯¹çš„è´¨å¹¶ä¸å–å†³äºå…¶å•ä¸€ç»„ä»¶çš„è´¨é‡ï¼Œè€Œæ˜¯å–å†³äºåŒæ–¹çš„ç›¸äº’å¯¹é½ç¨‹åº¦ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŒå‘å¯¹é½æ¡†æ¶ï¼ˆMAINï¼‰ï¼Œé€šè¿‡åŒå‘çº¦æŸç¡®ä¿æŒ‡ä»¤ä¸å“åº”ä¹‹é—´çš„è¿è´¯æ€§ã€‚æˆ‘ä»¬åœ¨LLaMAã€Mistralå’ŒQwenæ¨¡å‹ä¸Šå±•ç¤ºäº†MAINçš„è‰¯å¥½æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶åœ¨å¤šç§åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚æœ¬ç ”ç©¶å¼ºè°ƒäº†æŒ‡ä»¤å“åº”å¯¹é½åœ¨ä½¿LLMå®ç°é€šç”¨å’Œé«˜è´¨é‡çš„æŒ‡ä»¤å¾®è°ƒä¸­çš„å…³é”®ä½œç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æŒ‡ä»¤å¾®è°ƒå¯¹äºLLMæ€§èƒ½çš„æå‡è‡³å…³é‡è¦ï¼Œä¾èµ–äºå¤§è§„æ¨¡é«˜è´¨é‡æŒ‡ä»¤å“åº”å¯¹çš„å¯ç”¨æ€§ã€‚</li>
<li>å½“å‰æ•°æ®ç”Ÿæˆæ–¹æ³•å¾€å¾€å¿½è§†æŒ‡ä»¤ä¸å“åº”ä¹‹é—´çš„å¯¹é½ã€‚</li>
<li>æŒ‡ä»¤å“åº”å¯¹çš„è´¨å–å†³äºåŒæ–¹çš„ç›¸äº’å¯¹é½ç¨‹åº¦ï¼Œè€Œéå•ä¸€ç»„ä»¶çš„è´¨é‡ã€‚</li>
<li>æå‡ºäº†åŒå‘å¯¹é½æ¡†æ¶ï¼ˆMAINï¼‰æ¥ç¡®ä¿æŒ‡ä»¤ä¸å“åº”ä¹‹é—´çš„è¿è´¯æ€§ã€‚</li>
<li>MAINæ¡†æ¶åœ¨ä¸åŒæ¨¡å‹æ¶æ„å’Œè§„æ¨¡ä¸Šå…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>MAINåœ¨å¤šç§åŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³çš„è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.12913">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-61f14a858baf4b6d2058eded2897a0d8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-27331bc7cb0863edb1a6a0c10dcc9e41.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-31f25872c1dee6264918f67bbcbaca7c.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="A-Cascading-Cooperative-Multi-agent-Framework-for-On-ramp-Merging-Control-Integrating-Large-Language-Models"><a href="#A-Cascading-Cooperative-Multi-agent-Framework-for-On-ramp-Merging-Control-Integrating-Large-Language-Models" class="headerlink" title="A Cascading Cooperative Multi-agent Framework for On-ramp Merging   Control Integrating Large Language Models"></a>A Cascading Cooperative Multi-agent Framework for On-ramp Merging   Control Integrating Large Language Models</h2><p><strong>Authors:Miao Zhang, Zhenlong Fang, Tianyi Wang, Qian Zhang, Shuai Lu, Junfeng Jiao, Tianyu Shi</strong></p>
<p>Traditional Reinforcement Learning (RL) suffers from replicating human-like behaviors, generalizing effectively in multi-agent scenarios, and overcoming inherent interpretability issues.These tasks are compounded when deep environment understanding, agent coordination and dynamic optimization are required. While Large Language Model (LLM) enhanced methods have shown promise in generalization and interoperability, they often neglect necessary multi-agent coordination. Therefore, we introduce the Cascading Cooperative Multi-agent (CCMA) framework, integrating RL for individual interactions, a fine-tuned LLM for regional cooperation, a reward function for global optimization, and the Retrieval-augmented Generation mechanism to dynamically optimize decision-making across complex driving scenarios. Our experiments demonstrate that the CCMA outperforms existing RL methods, demonstrating significant improvements in both micro and macro-level performance in complex driving environments. </p>
<blockquote>
<p>ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å¤åˆ¶äººç±»è¡Œä¸ºã€åœ¨å¤šæ™ºèƒ½ä½“åœºæ™¯ä¸­çš„æœ‰æ•ˆæ¨å¹¿ä»¥åŠå…‹æœå›ºæœ‰çš„è§£é‡Šæ€§é—®é¢˜æ–¹é¢å­˜åœ¨å›°éš¾ã€‚å½“éœ€è¦æ·±å…¥äº†è§£ç¯å¢ƒã€æ™ºèƒ½ä½“åè°ƒå’ŒåŠ¨æ€ä¼˜åŒ–æ—¶ï¼Œè¿™äº›ä»»åŠ¡ä¼šå˜å¾—æ›´åŠ å¤æ‚ã€‚è™½ç„¶é‡‡ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¢å¼ºæ–¹æ³•å·²åœ¨æ¨å¹¿å’Œäº’æ“ä½œæ€§æ–¹é¢æ˜¾ç¤ºå‡ºå¸Œæœ›ï¼Œä½†å®ƒä»¬å¾€å¾€å¿½è§†äº†å¿…è¦çš„å¤šæ™ºèƒ½ä½“åè°ƒã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†çº§è”åˆä½œå¤šæ™ºèƒ½ä½“ï¼ˆCCMAï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†RLç”¨äºä¸ªä½“äº¤äº’ï¼Œå¾®è°ƒLLMç”¨äºåŒºåŸŸåˆä½œï¼Œå¥–åŠ±å‡½æ•°ç”¨äºå…¨å±€ä¼˜åŒ–ï¼Œæ£€ç´¢å¢å¼ºç”Ÿæˆæœºåˆ¶ä»¥åœ¨å¤æ‚çš„é©¾é©¶åœºæ™¯ä¸­åŠ¨æ€ä¼˜åŒ–å†³ç­–ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒCCMAåœ¨å¤æ‚çš„é©¾é©¶ç¯å¢ƒä¸­ä¼˜äºç°æœ‰çš„RLæ–¹æ³•ï¼Œåœ¨å¾®è§‚å’Œå®è§‚æ€§èƒ½ä¸Šå‡æ˜¾ç¤ºå‡ºæ˜¾è‘—æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.08199v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åœ¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸­ï¼Œå­˜åœ¨éš¾ä»¥å¤åˆ¶äººç±»è¡Œä¸ºã€åœ¨å¤šæ™ºèƒ½ä½“åœºæ™¯ä¸­æœ‰æ•ˆæ³›åŒ–ä»¥åŠè§£å†³å†…åœ¨è§£é‡Šæ€§é—®é¢˜ç­‰æŒ‘æˆ˜ã€‚å½“éœ€è¦æ·±åº¦ç¯å¢ƒç†è§£ã€æ™ºèƒ½ä½“åè°ƒå’ŒåŠ¨æ€ä¼˜åŒ–æ—¶ï¼Œè¿™äº›ä»»åŠ¡æ›´åŠ å¤æ‚ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¢å¼ºæ–¹æ³•åœ¨æ³›åŒ–å’Œäº’æ“ä½œæ€§æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å¾€å¾€å¿½ç•¥äº†å¿…è¦çš„å¤šæ™ºèƒ½ä½“åè°ƒã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†çº§è”åˆä½œå¤šæ™ºèƒ½ä½“ï¼ˆCCMAï¼‰æ¡†æ¶ï¼Œæ•´åˆä¸ªä½“äº¤äº’çš„RLã€åŒºåŸŸåˆä½œçš„å¾®è°ƒLLMã€å…¨å±€ä¼˜åŒ–çš„å¥–åŠ±å‡½æ•°ä»¥åŠç”¨äºåŠ¨æ€ä¼˜åŒ–å¤æ‚é©¾é©¶åœºæ™¯ä¸­å†³ç­–ç”Ÿæˆçš„æ£€ç´¢å¢å¼ºç”Ÿæˆæœºåˆ¶ã€‚å®éªŒè¡¨æ˜ï¼ŒCCMAåœ¨å¤æ‚é©¾é©¶ç¯å¢ƒä¸­ç›¸è¾ƒäºç°æœ‰RLæ–¹æ³•è¡¨ç°å‡ºæ›´é«˜çš„å¾®è§‚å’Œå®è§‚æ€§èƒ½æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ é¢ä¸´å¤åˆ¶äººç±»è¡Œä¸ºã€å¤šæ™ºèƒ½ä½“åœºæ™¯æ³›åŒ–å’Œè§£é‡Šæ€§é—®é¢˜ã€‚</li>
<li>åœ¨éœ€è¦æ·±åº¦ç¯å¢ƒç†è§£ã€æ™ºèƒ½ä½“åè°ƒå’ŒåŠ¨æ€ä¼˜åŒ–çš„åœºæ™¯ä¸‹ï¼Œè¿™äº›é—®é¢˜æ›´åŠ çªå‡ºã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹å¢å¼ºæ–¹æ³•åœ¨æ³›åŒ–å’Œäº’æ“ä½œæ€§æ–¹é¢å…·æ½œåŠ›ï¼Œä½†ç¼ºä¹å¤šæ™ºèƒ½ä½“åè°ƒã€‚</li>
<li>å¼•å…¥CCMAæ¡†æ¶ï¼Œæ•´åˆRLã€LLMã€å¥–åŠ±å‡½æ•°å’Œæ£€ç´¢å¢å¼ºç”Ÿæˆæœºåˆ¶ã€‚</li>
<li>CCMAæ¡†æ¶æ—¨åœ¨è§£å†³ä¸ªä½“äº¤äº’ã€åŒºåŸŸåˆä½œå’Œå…¨å±€ä¼˜åŒ–é—®é¢˜ã€‚</li>
<li>å®éªŒè¡¨æ˜CCMAåœ¨å¤æ‚é©¾é©¶ç¯å¢ƒä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæœ‰æ˜¾è‘—æ”¹å–„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.08199">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-58cdd4296799be8b3ce735e245ead637.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2689612830870d1b5ecb418577a6652e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e3a4883ae1afebe5ebf33df659b19424.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0ffaf724c9979232caf675d776f43744.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a34cf34db9f2422c14d1a0c1101122af.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cb24005b1a7d25e042f0dd75deccd5f4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1d30c195de0dcb75ca6b6a6322718409.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-30fe3a2f8302550624fc3d1c1f5f542c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a87db7852eeeedda3011620f604a93ec.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Tractable-Transformers-for-Flexible-Conditional-Generation"><a href="#Tractable-Transformers-for-Flexible-Conditional-Generation" class="headerlink" title="Tractable Transformers for Flexible Conditional Generation"></a>Tractable Transformers for Flexible Conditional Generation</h2><p><strong>Authors:Anji Liu, Xuejie Liu, Dayuan Zhao, Mathias Niepert, Yitao Liang, Guy Van den Broeck</strong></p>
<p>Non-autoregressive (NAR) generative models are valuable because they can handle diverse conditional generation tasks in a more principled way than their autoregressive (AR) counterparts, which are constrained by sequential dependency requirements. Recent advancements in NAR models, such as diffusion language models, have demonstrated superior performance in unconditional generation compared to AR models (e.g., GPTs) of similar sizes. However, such improvements do not always lead to improved conditional generation performance. We show that a key reason for this gap is the difficulty in generalizing to conditional probability queries (i.e., the set of unknown variables) unseen during training. As a result, strong unconditional generation performance does not guarantee high-quality conditional generation. This paper proposes Tractable Transformers (Tracformer), a Transformer-based generative model that is more robust to different conditional generation tasks. Unlike existing models that rely solely on global contextual features derived from full inputs, Tracformers incorporate a sparse Transformer encoder to capture both local and global contextual information. This information is routed through a decoder for conditional generation. Empirical results demonstrate that Tracformers achieve state-of-the-art conditional generation performance on text modeling compared to recent diffusion and AR model baselines. </p>
<blockquote>
<p>éè‡ªå›å½’ï¼ˆNARï¼‰ç”Ÿæˆæ¨¡å‹å…·æœ‰ä»·å€¼ï¼Œå› ä¸ºå®ƒä»¬èƒ½å¤Ÿä»¥æ¯”è‡ªå›å½’ï¼ˆARï¼‰æ¨¡å‹æ›´åŸåˆ™åŒ–çš„æ–¹å¼å¤„ç†å„ç§æ¡ä»¶ç”Ÿæˆä»»åŠ¡ã€‚è‡ªå›å½’æ¨¡å‹å—åˆ°åºåˆ—ä¾èµ–æ€§çš„çº¦æŸã€‚æœ€è¿‘çš„NARæ¨¡å‹è¿›å±•ï¼Œå¦‚æ‰©æ•£è¯­è¨€æ¨¡å‹ï¼Œåœ¨æ— æ¡ä»¶ç”Ÿæˆæ–¹é¢å·²ç»è¡¨ç°å‡ºä¼˜äºç±»ä¼¼è§„æ¨¡çš„ARæ¨¡å‹ï¼ˆä¾‹å¦‚GPTï¼‰çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œè¿™ç§æ”¹è¿›å¹¶ä¸ä¸€å®šå¯¼è‡´æ¡ä»¶ç”Ÿæˆæ€§èƒ½çš„æ”¹å–„ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œé€ æˆè¿™ä¸€å·®è·çš„å…³é”®åŸå› æ˜¯éš¾ä»¥æ¨å¹¿åˆ°è®­ç»ƒæœŸé—´æœªè§è¿‡çš„æ¡ä»¶æ¦‚ç‡æŸ¥è¯¢ï¼ˆå³æœªçŸ¥å˜é‡é›†ï¼‰ã€‚å› æ­¤ï¼Œå¼ºå¤§çš„æ— æ¡ä»¶ç”Ÿæˆæ€§èƒ½å¹¶ä¸èƒ½ä¿è¯é«˜è´¨é‡çš„æ¡ä»¶ç”Ÿæˆã€‚æœ¬æ–‡æå‡ºäº†â€œTractable Transformersï¼ˆTracformerï¼‰â€ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºTransformerçš„ç”Ÿæˆæ¨¡å‹ï¼Œèƒ½å¤Ÿæ›´ç¨³å¥åœ°åº”å¯¹ä¸åŒçš„æ¡ä»¶ç”Ÿæˆä»»åŠ¡ã€‚ä¸ç°æœ‰ä»…ä¾èµ–å…¨å±€ä¸Šä¸‹æ–‡ç‰¹å¾çš„æ¨¡å‹ä¸åŒï¼ŒTracformersç»“åˆç¨€ç–Transformerç¼–ç å™¨æ¥æ•è·å±€éƒ¨å’Œå…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚è¿™äº›ä¿¡æ¯é€šè¿‡è§£ç å™¨è¿›è¡Œæœ‰æ¡ä»¶ç”Ÿæˆã€‚ç»éªŒç»“æœè¡¨æ˜ï¼Œåœ¨æ–‡æœ¬å»ºæ¨¡æ–¹é¢ï¼ŒTracformerså®ç°äº†æœ€æ–°çš„æœ‰æ¡ä»¶ç”Ÿæˆæ€§èƒ½ï¼Œè¶…è¿‡äº†æœ€è¿‘çš„æ‰©æ•£å’ŒARæ¨¡å‹åŸºçº¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.07616v2">PDF</a> </p>
<p><strong>Summary</strong><br>éè‡ªå›å½’ï¼ˆNARï¼‰ç”Ÿæˆæ¨¡å‹èƒ½å¤Ÿæ›´çµæ´»åœ°å¤„ç†å„ç§æ¡ä»¶ç”Ÿæˆä»»åŠ¡ï¼Œä¼˜äºå—åºåˆ—ä¾èµ–æ€§é™åˆ¶çš„è‡ªå›å½’ï¼ˆARï¼‰æ¨¡å‹ã€‚æœ€æ–°NARæ¨¡å‹å¦‚æ‰©æ•£è¯­è¨€æ¨¡å‹åœ¨æ— æ¡ä»¶ç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½†åœ¨æ¡ä»¶ç”Ÿæˆæ–¹é¢ä¸ä¸€å®šä¼˜äºç±»ä¼¼è§„æ¨¡çš„ARæ¨¡å‹ï¼ˆå¦‚GPTsï¼‰ã€‚æœ¬æ–‡æŒ‡å‡ºï¼Œè¿™ä¸€å·®è·çš„å…³é”®åŸå› åœ¨äºå¯¹è®­ç»ƒæœŸé—´æœªè§è¿‡çš„æ¡ä»¶æ¦‚ç‡æŸ¥è¯¢çš„æ³›åŒ–å›°éš¾ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºTractable Transformersï¼ˆTracformerï¼‰ï¼Œä¸€ç§åŸºäºTransformerçš„ç”Ÿæˆæ¨¡å‹ï¼Œèƒ½æ›´ç¨³å¥åœ°åº”å¯¹ä¸åŒçš„æ¡ä»¶ç”Ÿæˆä»»åŠ¡ã€‚ä¸ä»…ä¾èµ–å…¨å±€ä¸Šä¸‹æ–‡ç‰¹å¾çš„ç°æœ‰æ¨¡å‹ä¸åŒï¼ŒTracformersç»“åˆç¨€ç–Transformerç¼–ç å™¨æ¥æ•è·å±€éƒ¨å’Œå…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå¹¶é€šè¿‡è§£ç å™¨è¿›è¡Œæ¡ä»¶ç”Ÿæˆã€‚å®è¯ç»“æœè¡¨æ˜ï¼ŒTracformersåœ¨æ–‡æœ¬å»ºæ¨¡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ¡ä»¶ç”Ÿæˆæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>NARç”Ÿæˆæ¨¡å‹ç›¸æ¯”ARæ¨¡å‹åœ¨æ¡ä»¶ç”Ÿæˆä»»åŠ¡ä¸Šæ›´å…·ä¼˜åŠ¿ï¼Œèƒ½å¤Ÿæ›´çµæ´»åœ°å¤„ç†å¤šæ ·åŒ–çš„æ¡ä»¶ã€‚</li>
<li>æ‰©æ•£è¯­è¨€æ¨¡å‹åœ¨æ— æ¡ä»¶ç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½†åœ¨æ¡ä»¶ç”Ÿæˆæ–¹é¢ä¸ä¸€å®šä¼˜äºARæ¨¡å‹ã€‚</li>
<li>æ¡ä»¶ç”Ÿæˆæ€§èƒ½å·®è·çš„å…³é”®åœ¨äºæ¨¡å‹å¯¹æœªè§è¿‡çš„æ¡ä»¶æ¦‚ç‡æŸ¥è¯¢çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>Tractable Transformersï¼ˆTracformerï¼‰æ˜¯ä¸€ç§æ–°çš„åŸºäºTransformerçš„ç”Ÿæˆæ¨¡å‹ï¼Œæ—¨åœ¨æé«˜å¯¹ä¸åŒæ¡ä»¶ç”Ÿæˆä»»åŠ¡çš„ç¨³å¥æ€§ã€‚</li>
<li>Tracformersç»“åˆç¨€ç–Transformerç¼–ç å™¨æ¥æ•è·å±€éƒ¨å’Œå…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚</li>
<li>Tracformersé€šè¿‡è§£ç å™¨è¿›è¡Œæ¡ä»¶ç”Ÿæˆï¼Œå®ç°äº†æ–‡æœ¬å»ºæ¨¡ä¸Šçš„å…ˆè¿›æ¡ä»¶ç”Ÿæˆæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.07616">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-37fda23e4762306de7bc8d32749d11fb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e75d763f00d0ffa5c918fe71bf7f8773.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3d7af32e05678125a1b850bfe0599ff2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4d2b2e153cccbd0c11892af7c3d49a72.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="UniForm-A-Unified-Multi-Task-Diffusion-Transformer-for-Audio-Video-Generation"><a href="#UniForm-A-Unified-Multi-Task-Diffusion-Transformer-for-Audio-Video-Generation" class="headerlink" title="UniForm: A Unified Multi-Task Diffusion Transformer for Audio-Video   Generation"></a>UniForm: A Unified Multi-Task Diffusion Transformer for Audio-Video   Generation</h2><p><strong>Authors:Lei Zhao, Linfeng Feng, Dongxu Ge, Rujin Chen, Fangqiu Yi, Chi Zhang, Xiao-Lei Zhang, Xuelong Li</strong></p>
<p>With the rise of diffusion models, audio-video generation has been revolutionized. However, most existing methods rely on separate modules for each modality, with limited exploration of unified generative architectures. In addition, many are confined to a single task and small-scale datasets. To overcome these limitations, we introduce UniForm, a unified multi-task diffusion transformer that generates both audio and visual modalities in a shared latent space. By using a unified denoising network, UniForm captures the inherent correlations between sound and vision. Additionally, we propose task-specific noise schemes and task tokens, enabling the model to support multiple tasks with a single set of parameters, including video-to-audio, audio-to-video and text-to-audio-video generation. Furthermore, by leveraging large language models and a large-scale text-audio-video combined dataset, UniForm achieves greater generative diversity than prior approaches. Experiments show that UniForm achieves performance close to the state-of-the-art single-task models across three generation tasks, with generated content that is not only highly aligned with real-world data distributions but also enables more diverse and fine-grained generation. </p>
<blockquote>
<p>éšç€æ‰©æ•£æ¨¡å‹çš„å…´èµ·ï¼ŒéŸ³è§†é¢‘ç”Ÿæˆé¢†åŸŸå‘ç”Ÿäº†é©å‘½æ€§çš„å˜åŒ–ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰æ–¹æ³•ä¾èµ–äºæ¯ç§æ¨¡æ€çš„ç‹¬ç«‹æ¨¡å—ï¼Œå¯¹ç»Ÿä¸€ç”Ÿæˆæ¶æ„çš„æ¢ç´¢æœ‰é™ã€‚æ­¤å¤–ï¼Œè®¸å¤šæ–¹æ³•ä»…é™äºå•ä¸€ä»»åŠ¡å’Œå°è§„æ¨¡æ•°æ®é›†ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†UniFormï¼Œè¿™æ˜¯ä¸€ç§ç»Ÿä¸€çš„å¤šä»»åŠ¡æ‰©æ•£å˜å‹å™¨ï¼Œå®ƒåœ¨å…±äº«æ½œåœ¨ç©ºé—´ä¸­ç”ŸæˆéŸ³é¢‘å’Œè§†é¢‘ä¸¤ç§æ¨¡æ€ã€‚é€šè¿‡ä½¿ç”¨ç»Ÿä¸€çš„å»å™ªç½‘ç»œï¼ŒUniFormæ•æ‰äº†å£°éŸ³å’Œè§†è§‰ä¹‹é—´çš„å†…åœ¨å…³è”ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ç‰¹å®šçš„ä»»åŠ¡å™ªå£°æ–¹æ¡ˆå’Œä»»åŠ¡ä»¤ç‰Œï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿä½¿ç”¨ä¸€ç»„å‚æ•°æ”¯æŒå¤šé¡¹ä»»åŠ¡ï¼ŒåŒ…æ‹¬è§†é¢‘åˆ°éŸ³é¢‘ã€éŸ³é¢‘åˆ°è§†é¢‘å’Œæ–‡æœ¬åˆ°éŸ³é¢‘è§†é¢‘çš„ç”Ÿæˆã€‚æ­¤å¤–ï¼Œé€šè¿‡åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å’Œå¤§è§„æ¨¡æ–‡æœ¬-éŸ³é¢‘-è§†é¢‘ç»„åˆæ•°æ®é›†ï¼ŒUniFormå®ç°äº†æ¯”ä»¥å‰çš„æ–¹æ³•æ›´å¤§çš„ç”Ÿæˆå¤šæ ·æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒUniFormåœ¨ä¸‰é¡¹ç”Ÿæˆä»»åŠ¡ä¸Šçš„æ€§èƒ½æ¥è¿‘æœ€å…ˆè¿›çš„å•ä»»åŠ¡æ¨¡å‹ï¼Œç”Ÿæˆçš„å†…å®¹ä¸ä»…é«˜åº¦ç¬¦åˆç°å®ä¸–ç•Œæ•°æ®åˆ†å¸ƒï¼Œè€Œä¸”èƒ½å¤Ÿå®ç°æ›´å¤šæ ·åŒ–å’Œç²¾ç»†çš„ç”Ÿæˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.03897v5">PDF</a> Our demos are available at <a target="_blank" rel="noopener" href="https://uniform-t2av.github.io/">https://uniform-t2av.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>éšç€æ‰©æ•£æ¨¡å‹çš„å‘å±•ï¼ŒéŸ³è§†é¢‘ç”Ÿæˆé¢†åŸŸç»å†äº†é©æ–°ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¤§å¤šä¾èµ–ç‹¬ç«‹æ¨¡å—å¤„ç†ä¸åŒæ¨¡æ€ï¼Œå¯¹ç»Ÿä¸€ç”Ÿæˆæ¶æ„çš„æ¢ç´¢æœ‰é™ã€‚ä¸ºå…‹æœè¿™äº›å±€é™ï¼Œæˆ‘ä»¬æ¨å‡ºUniFormï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„å¤šä»»åŠ¡æ‰©æ•£å˜å‹å™¨ï¼Œåœ¨å…±äº«æ½œåœ¨ç©ºé—´ä¸­ç”ŸæˆéŸ³é¢‘å’Œè§†é¢‘ä¸¤ç§æ¨¡æ€ã€‚é€šè¿‡é‡‡ç”¨ç»Ÿä¸€å»å™ªç½‘ç»œï¼ŒUniFormæ•æ‰å£°éŸ³å’Œè§†è§‰ä¹‹é—´çš„å†…åœ¨å…³è”ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºé’ˆå¯¹ä»»åŠ¡çš„å™ªå£°æ–¹æ¡ˆå’Œä»»åŠ¡ä»¤ç‰Œï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿä½¿ç”¨å•ä¸€å‚æ•°é›†æ”¯æŒå¤šé¡¹ä»»åŠ¡ï¼ŒåŒ…æ‹¬è§†é¢‘è½¬éŸ³é¢‘ã€éŸ³é¢‘è½¬è§†é¢‘å’Œæ–‡æœ¬è½¬éŸ³é¢‘è§†é¢‘ç”Ÿæˆã€‚å€ŸåŠ©å¤§å‹è¯­è¨€æ¨¡å‹å’Œå¤§è§„æ¨¡æ–‡æœ¬éŸ³è§†é¢‘ç»„åˆæ•°æ®é›†ï¼ŒUniFormå®ç°äº†æ¯”å…ˆå‰æ–¹æ³•æ›´é«˜çš„ç”Ÿæˆå¤šæ ·æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒUniFormåœ¨ä¸‰é¡¹ç”Ÿæˆä»»åŠ¡ä¸Šçš„æ€§èƒ½æ¥è¿‘æœ€æ–°å•ä»»åŠ¡æ¨¡å‹ï¼Œç”Ÿæˆçš„å†…å®¹ä¸ä»…é«˜åº¦ç¬¦åˆçœŸå®ä¸–ç•Œæ•°æ®åˆ†å¸ƒï¼Œè¿˜èƒ½å®ç°æ›´å¤šæ ·åŒ–å’Œç²¾ç»†åŒ–çš„ç”Ÿæˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹æ¨åŠ¨äº†éŸ³è§†é¢‘ç”Ÿæˆé¢†åŸŸçš„é©æ–°ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¤§å¤šé‡‡ç”¨ç‹¬ç«‹æ¨¡å—å¤„ç†ä¸åŒæ¨¡æ€ï¼Œç¼ºä¹ç»Ÿä¸€ç”Ÿæˆæ¶æ„ã€‚</li>
<li>UniFormæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„å¤šä»»åŠ¡æ‰©æ•£å˜å‹å™¨ï¼Œå¯åœ¨å…±äº«æ½œåœ¨ç©ºé—´ä¸­ç”ŸæˆéŸ³é¢‘å’Œè§†é¢‘ã€‚</li>
<li>UniFormé‡‡ç”¨ç»Ÿä¸€å»å™ªç½‘ç»œï¼Œæ•æ‰å£°éŸ³å’Œè§†è§‰ä¹‹é—´çš„å†…åœ¨å…³è”ã€‚</li>
<li>é€šè¿‡ä»»åŠ¡ç‰¹å®šå™ªå£°æ–¹æ¡ˆå’Œä»»åŠ¡ä»¤ç‰Œï¼ŒUniFormæ”¯æŒå¤šé¡¹ä»»åŠ¡ä½¿ç”¨å•ä¸€å‚æ•°é›†ã€‚</li>
<li>UniFormå€ŸåŠ©å¤§å‹è¯­è¨€æ¨¡å‹å’Œå¤§è§„æ¨¡æ•°æ®é›†å®ç°äº†æ›´é«˜çš„ç”Ÿæˆå¤šæ ·æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.03897">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6c7198e596174b9e46e098a876a88fbc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e9ebbc7829455a56be8d061d9accf9c2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9c4510180b82c0f3c166e85df8d00810.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5a704447a599431e9dab91ee29adcdaa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ccbe9be0c294d9b17eb8a06a5de5fb80.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3a37c5eb55f429ddff09f4c23870fe81.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Agents-Are-All-You-Need-for-LLM-Unlearning"><a href="#Agents-Are-All-You-Need-for-LLM-Unlearning" class="headerlink" title="Agents Are All You Need for LLM Unlearning"></a>Agents Are All You Need for LLM Unlearning</h2><p><strong>Authors:Debdeep Sanyal, Murari Mandal</strong></p>
<p>Information removal or suppression in large language models (LLMs) is a desired functionality, useful in AI regulation, legal compliance, safety, and privacy. LLM unlearning methods aim to remove information on demand from LLMs. Current LLM unlearning methods struggle to balance the unlearning efficacy and utility due to the competing nature of these objectives. Keeping the unlearning process computationally feasible without assuming access to the model weights is an overlooked area. In this work we show that \textit{agents might be all we need for effective and practical inference-time LLM unlearning}. We present the first agentic LLM unlearning (\texttt{ALU}) method, a multi-agent, retrain-free, model-agnostic approach to LLM unlearning that achieves effective unlearning while preserving the utility. Our \texttt{ALU} framework unlearns by involving multiple LLM agents, each designed for a specific step in the unlearning process, without the need to update model weights for any of the agents in the framework. Users can easily request any set of unlearning instances in any sequence, and \texttt{ALU} seamlessly adapts in real time. This is facilitated without requiring any changes in the underlying LLM model. Through extensive experiments on established benchmarks (TOFU, WMDP, WPU) and jailbreaking techniques (many shot, target masking, other languages), we demonstrate that \texttt{ALU} consistently stands out as the most robust inference-time LLM unlearning framework among current state-of-the-art methods while incurring time cost that remains effectively constant regardless of the number of unlearning targets. We further highlight \texttt{ALU}â€™s superior performance compared to existing methods when evaluated at scale. Specifically, \texttt{ALU} is assessed on up to 1000 unlearning targets, exceeding the evaluation scope of all previously proposed LLM unlearning methods. </p>
<blockquote>
<p>ä¿¡æ¯åˆ é™¤æˆ–æŠ‘åˆ¶åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­æ˜¯ä¸€é¡¹ç†æƒ³çš„åŠŸèƒ½ï¼Œåœ¨äººå·¥æ™ºèƒ½ç›‘ç®¡ã€æ³•å¾‹åˆè§„ã€å®‰å…¨å’Œéšç§æ–¹é¢éå¸¸æœ‰ç”¨ã€‚LLMçš„é—å¿˜æ–¹æ³•æ—¨åœ¨æŒ‰éœ€ä»LLMä¸­åˆ é™¤ä¿¡æ¯ã€‚å½“å‰çš„LLMé—å¿˜æ–¹æ³•ç”±äºè¿™äº›ç›®æ ‡çš„ç«äº‰æ€§è´¨è€Œéš¾ä»¥å¹³è¡¡é—å¿˜æ•ˆæœå’Œå®ç”¨æ€§ã€‚ä¿æŒé—å¿˜è¿‡ç¨‹çš„è®¡ç®—å¯è¡Œæ€§ï¼ŒåŒæ—¶æ— éœ€è®¿é—®æ¨¡å‹æƒé‡æ˜¯ä¸€ä¸ªè¢«å¿½è§†çš„é¢†åŸŸã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è¡¨æ˜â€œä»£ç†å¯èƒ½æ˜¯å®ç°æœ‰æ•ˆå’Œå®é™…æ¨ç†æ—¶é—´LLMé—å¿˜æ‰€éœ€çš„ä¸€åˆ‡â€ã€‚æˆ‘ä»¬æå‡ºäº†ç¬¬ä¸€ä¸ªä»£ç†LLMé—å¿˜ï¼ˆALUï¼‰æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§å¤šä»£ç†ã€æ— éœ€é‡æ–°è®­ç»ƒã€æ¨¡å‹æ— å…³çš„æ–¹æ³•æ¥æ‰§è¡ŒLLMé—å¿˜ï¼Œåœ¨æœ‰æ•ˆé—å¿˜çš„åŒæ—¶ä¿æŒå®ç”¨æ€§ã€‚æˆ‘ä»¬çš„ALUæ¡†æ¶é€šè¿‡æ¶‰åŠå¤šä¸ªLLMä»£ç†æ¥æ‰§è¡Œé—å¿˜æ“ä½œï¼Œæ¯ä¸ªä»£ç†éƒ½é’ˆå¯¹é—å¿˜è¿‡ç¨‹ä¸­çš„ç‰¹å®šæ­¥éª¤è€Œè®¾è®¡ï¼Œæ— éœ€æ›´æ–°æ¡†æ¶ä¸­ä»»ä½•ä»£ç†çš„æ¨¡å‹æƒé‡ã€‚ç”¨æˆ·å¯ä»¥è½»æ¾æŒ‰ä»»æ„é¡ºåºè¯·æ±‚ä»»ä½•é—å¿˜å®ä¾‹ï¼ŒALUå¯ä»¥å®æ—¶æ— ç¼é€‚åº”ã€‚è¿™å¯ä»¥åœ¨æ— éœ€æ›´æ”¹åŸºç¡€LLMæ¨¡å‹çš„æƒ…å†µä¸‹å®ç°ã€‚é€šè¿‡åœ¨å¤§è§„æ¨¡åŸºå‡†æµ‹è¯•ï¼ˆTOFUã€WMDPã€WPUï¼‰å’Œè¶Šç‹±æŠ€æœ¯ï¼ˆå¤šé•œå¤´ã€ç›®æ ‡æ©ç ã€å…¶ä»–è¯­è¨€ï¼‰ä¸Šè¿›è¡Œå¹¿æ³›å®éªŒï¼Œæˆ‘ä»¬è¯æ˜ALUä½œä¸ºå½“å‰æœ€å…ˆè¿›çš„æ¨ç†æ—¶é—´LLMé—å¿˜æ¡†æ¶ä¸­æœ€ä¸ºç¨³å¥çš„ä¸€ä¸ªï¼Œå…¶æ—¶é—´æˆæœ¬ä¿æŒæ’å®šï¼Œä¸å—é—å¿˜ç›®æ ‡æ•°é‡çš„å½±å“ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥å¼ºè°ƒäº†ALUåœ¨è§„æ¨¡è¯„ä¼°æ—¶ç›¸å¯¹äºç°æœ‰æ–¹æ³•çš„å“è¶Šæ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼ŒALUåœ¨å¤šè¾¾1000ä¸ªé—å¿˜ç›®æ ‡ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œè¶…å‡ºäº†æ‰€æœ‰å…ˆå‰æå‡ºçš„LLMé—å¿˜æ–¹æ³•çš„è¯„ä¼°èŒƒå›´ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.00406v2">PDF</a> Accepted to COLM 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„ä¿¡æ¯ç§»é™¤æˆ–æŠ‘åˆ¶åœ¨äººå·¥æ™ºèƒ½ç›‘ç®¡ã€æ³•å¾‹åˆè§„ã€å®‰å…¨å’Œéšç§æ–¹é¢éå¸¸æœ‰ç”¨ã€‚LLMçš„é—å¿˜æ–¹æ³•æ—¨åœ¨æŒ‰éœ€ä»LLMä¸­ç§»é™¤ä¿¡æ¯ã€‚å½“å‰LLMçš„é—å¿˜æ–¹æ³•éš¾ä»¥å¹³è¡¡é—å¿˜æ•ˆæœå’Œå®ç”¨æ€§ã€‚æœ¬ç ”ç©¶å±•ç¤ºäº†â€œå¯èƒ½æ‰€æœ‰éœ€è¦çš„éƒ½æ˜¯ä»£ç†ï¼Œä»¥å®ç°æœ‰æ•ˆä¸”å®ç”¨çš„æ¨ç†æ—¶é—´LLMé—å¿˜â€ã€‚æˆ‘ä»¬æå‡ºäº†ç¬¬ä¸€ä¸ªä»£ç†LLMé—å¿˜ï¼ˆALUï¼‰æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§å¤šä»£ç†ã€æ— éœ€é‡æ–°è®­ç»ƒã€æ¨¡å‹æ— å…³çš„æ–¹æ³•ï¼Œå¯å®ç°æœ‰æ•ˆçš„é—å¿˜åŒæ—¶ä¿ç•™å®ç”¨æ€§ã€‚ALUæ¡†æ¶é€šè¿‡æ¶‰åŠå¤šä¸ªLLMä»£ç†è¿›è¡Œé—å¿˜ï¼Œæ¯ä¸ªä»£ç†éƒ½é’ˆå¯¹é—å¿˜è¿‡ç¨‹ä¸­çš„ç‰¹å®šæ­¥éª¤è€Œè®¾è®¡ï¼Œæ— éœ€æ›´æ–°æ¨¡å‹ä¸­ä»»ä½•ä»£ç†çš„æƒé‡ã€‚ç”¨æˆ·å¯è½»æ¾è¯·æ±‚ä»»ä½•åºåˆ—çš„é—å¿˜å®ä¾‹ï¼ŒALUå®æ—¶æ— ç¼é€‚åº”ã€‚è¿™æ— éœ€å¯¹åº•å±‚LLMæ¨¡å‹è¿›è¡Œä»»ä½•æ›´æ”¹ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†ALUåœ¨ç°æœ‰åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°æœ€ä¼˜ç§€ï¼Œå¹¶ä¸”å…·æœ‰å‡ºè‰²çš„æ€§èƒ½ã€‚ç‰¹åˆ«æ˜¯åœ¨å¤§è§„æ¨¡è¯„ä¼°ä¸­ï¼ŒALUåœ¨å¤„ç†å¤šè¾¾1000ä¸ªé—å¿˜ç›®æ ‡æ—¶ä»è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œè¶…è¶Šäº†æ‰€æœ‰å…ˆå‰æå‡ºçš„LLMé—å¿˜æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMä¸­çš„ä¿¡æ¯ç§»é™¤æˆ–æŠ‘åˆ¶åœ¨å¤šä¸ªé¢†åŸŸå…·æœ‰å®ç”¨æ€§ã€‚</li>
<li>å½“å‰LLMé—å¿˜æ–¹æ³•é¢ä¸´é—å¿˜æ•ˆæœå’Œå®ç”¨æ€§ä¹‹é—´çš„å¹³è¡¡æŒ‘æˆ˜ã€‚</li>
<li>å¼•å…¥å¤šä»£ç†æ–¹æ³•ä»¥å®ç°æ¨ç†æ—¶é—´LLMé—å¿˜ï¼Œæ— éœ€é‡æ–°è®­ç»ƒæ¨¡å‹ã€‚</li>
<li>ALUæ¡†æ¶é€šè¿‡æ¶‰åŠå¤šä¸ªLLMä»£ç†è¿›è¡Œé—å¿˜ï¼Œæ¯ä¸ªä»£ç†è´Ÿè´£ç‰¹å®šæ­¥éª¤ã€‚</li>
<li>ALUæ–¹æ³•å…è®¸ç”¨æˆ·çµæ´»è¯·æ±‚ä»»ä½•åºåˆ—çš„é—å¿˜å®ä¾‹ï¼Œå¹¶å…·æœ‰å®æ—¶é€‚åº”æ€§ã€‚</li>
<li>ALUåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜ç§€ï¼Œå°¤å…¶æ˜¯å¤„ç†å¤§é‡é—å¿˜ç›®æ ‡æ—¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.00406">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-70e0c897e7e381e9e30da4ceb96596b1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f8250ed0cab805f2d53b7b463f658b31.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d89d192f7b145920714dc8aeac97d194.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dbef061ed41281eaf4540e50de112df6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-096ba8a0aff224167a07cb075ccf4792.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="On-Fusing-ChatGPT-and-Ensemble-Learning-in-Discon-tinuous-Named-Entity-Recognition-in-Health-Corpora"><a href="#On-Fusing-ChatGPT-and-Ensemble-Learning-in-Discon-tinuous-Named-Entity-Recognition-in-Health-Corpora" class="headerlink" title="On Fusing ChatGPT and Ensemble Learning in Discon-tinuous Named Entity   Recognition in Health Corpora"></a>On Fusing ChatGPT and Ensemble Learning in Discon-tinuous Named Entity   Recognition in Health Corpora</h2><p><strong>Authors:Tzu-Chieh Chen, Wen-Yang Lin</strong></p>
<p>Named Entity Recognition has traditionally been a key task in natural language processing, aiming to identify and extract important terms from unstructured text data. However, a notable challenge for contemporary deep-learning NER models has been identifying discontinuous entities, which are often fragmented within the text. To date, methods to address Discontinuous Named Entity Recognition have not been explored using ensemble learning to the best of our knowledge. Furthermore, the rise of large language models, such as ChatGPT in recent years, has shown significant effectiveness across many NLP tasks. Most existing approaches, however, have primarily utilized ChatGPT as a problem-solving tool rather than exploring its potential as an integrative element within ensemble learning algorithms. In this study, we investigated the integration of ChatGPT as an arbitrator within an ensemble method, aiming to enhance performance on DNER tasks. Our method combines five state-of-the-art NER models with ChatGPT using custom prompt engineering to assess the robustness and generalization capabilities of the ensemble algorithm. We conducted experiments on three benchmark medical datasets, comparing our method against the five SOTA models, individual applications of GPT-3.5 and GPT-4, and a voting ensemble method. The results indicate that our proposed fusion of ChatGPT with the ensemble learning algorithm outperforms the SOTA results in the CADEC, ShARe13, and ShARe14 datasets, showcasing its potential to enhance NLP applications in the healthcare domain. </p>
<blockquote>
<p>å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰ä¸€ç›´æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„å…³é”®ä»»åŠ¡ï¼Œæ—¨åœ¨ä»éç»“æ„åŒ–æ–‡æœ¬æ•°æ®ä¸­è¯†åˆ«å’Œæå–é‡è¦æœ¯è¯­ã€‚ç„¶è€Œï¼Œå¯¹äºå½“ä»£æ·±åº¦å­¦ä¹ çš„NERæ¨¡å‹æ¥è¯´ï¼Œè¯†åˆ«ä¸è¿ç»­çš„å®ä½“æ˜¯ä¸€ä¸ªæ˜¾è‘—çš„æŒ‘æˆ˜ï¼Œè¿™äº›å®ä½“é€šå¸¸åœ¨æ–‡æœ¬ä¸­æ˜¯ç¢ç‰‡åŒ–çš„ã€‚åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œå°šæœªæœ‰æ–¹æ³•å°è¯•ä½¿ç”¨é›†æˆå­¦ä¹ æ¥è§£å†³ä¸è¿ç»­å‘½åå®ä½“è¯†åˆ«ï¼ˆDNERï¼‰é—®é¢˜ã€‚æ­¤å¤–ï¼Œè¿‘å¹´æ¥å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå¦‚ChatGPTï¼‰åœ¨è®¸å¤šè‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æœ‰æ•ˆæ€§ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰æ–¹æ³•ä¸»è¦å°†ChatGPTç”¨ä½œé—®é¢˜è§£å†³å·¥å…·ï¼Œè€Œæ²¡æœ‰æ¢ç´¢å…¶åœ¨é›†æˆå­¦ä¹ ç®—æ³•ä¸­ä½œä¸ºæ•´åˆå…ƒç´ çš„åº”ç”¨æ½œåŠ›ã€‚æœ¬ç ”ç©¶è°ƒæŸ¥äº†ChatGPTä½œä¸ºä»²è£è€…åœ¨é›†æˆæ–¹æ³•ä¸­çš„åº”ç”¨ï¼Œæ—¨åœ¨æé«˜å…¶åœ¨DNERä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†äº”ä¸ªæœ€å…ˆè¿›çš„NERæ¨¡å‹ä¸ChatGPTï¼Œä½¿ç”¨è‡ªå®šä¹‰çš„æç¤ºå·¥ç¨‹æ¥è¯„ä¼°é›†æˆç®—æ³•çš„ç¨³å¥æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªåŸºå‡†åŒ»ç–—æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Œå°†æˆ‘ä»¬çš„æ–¹æ³•ä¸äº”ä¸ªæœ€å…ˆè¿›æ¨¡å‹ã€GPT-3.5å’ŒGPT-4çš„å•ç‹¬åº”ç”¨ä»¥åŠæŠ•ç¥¨é›†æˆæ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„å°†ChatGPTä¸é›†æˆå­¦ä¹ ç®—æ³•ç›¸ç»“åˆçš„æ–¹æ³•åœ¨CADECã€ShARe1 3å’ŒShARe 1 4æ•°æ®é›†ä¸Šçš„è¡¨ç°è¶…è¿‡äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œå±•ç¤ºäº†å…¶åœ¨å¢å¼ºåŒ»ç–—ä¿å¥é¢†åŸŸNLPåº”ç”¨çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.16976v2">PDF</a> 13 pages; a short version has been accpeted for presentation at   MedInfo2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†é›†æˆChatGPTä½œä¸ºè£å†³è€…çš„æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜åœ¨æ–­è¯å‘½åå®ä½“è¯†åˆ«ï¼ˆDNERï¼‰ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚è¯¥ç ”ç©¶ç»“åˆäº†äº”ç§æœ€å…ˆè¿›çš„NERæ¨¡å‹ä¸ChatGPTï¼Œé€šè¿‡è‡ªå®šä¹‰æç¤ºå·¥ç¨‹æ¥è¯„ä¼°é›†æˆç®—æ³•çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥èåˆæ–¹æ³•åœ¨ä¸‰ä¸ªåŒ»ç–—åŸºå‡†æ•°æ®é›†ä¸Šè¶…è¶Šäº†å…ˆè¿›æ¨¡å‹ï¼Œå±•ç¤ºäº†åœ¨åŒ»ç–—ä¿å¥é¢†åŸŸè‡ªç„¶è¯­è¨€å¤„ç†åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–­è¯å‘½åå®ä½“è¯†åˆ«ï¼ˆDNERï¼‰æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„ä¸€ä¸ªæŒ‘æˆ˜ï¼Œä¼ ç»Ÿæ–¹æ³•éš¾ä»¥è¯†åˆ«ç¢ç‰‡åŒ–æ–‡æœ¬ä¸­çš„éè¿ç»­å®ä½“ã€‚</li>
<li>ç°æœ‰æ·±åº¦å­¦ä¹ æ–¹æ³•åœ¨è§£å†³DNERé—®é¢˜æ—¶æœªå……åˆ†åˆ©ç”¨é›†æˆå­¦ä¹ ã€‚</li>
<li>ChatGPTç­‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šä¸ªNLPä»»åŠ¡ä¸Šæ˜¾ç¤ºå‡ºæ˜¾è‘—æ•ˆæœï¼Œä½†å…¶åœ¨é›†æˆå­¦ä¹ ç®—æ³•ä¸­çš„æ½œåŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚</li>
<li>ç ”ç©¶ä¸­ï¼ŒChatGPTè¢«æ•´åˆåˆ°é›†æˆå­¦ä¹ ä¸­ä½œä¸ºè£å†³è€…ï¼Œæ—¨åœ¨æé«˜DNERä»»åŠ¡çš„æ€§èƒ½ã€‚</li>
<li>ç»“åˆäº”ç§æœ€å…ˆè¿›çš„NERæ¨¡å‹å’ŒChatGPTè¿›è¡Œå®éªŒï¼Œé€šè¿‡è‡ªå®šä¹‰æç¤ºå·¥ç¨‹è¯„ä¼°é›†æˆç®—æ³•çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>å®éªŒåœ¨ä¸‰ä¸ªåŒ»ç–—åŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œï¼Œç»“æœè¡¨æ˜èåˆChatGPTçš„é›†æˆå­¦ä¹ æ–¹æ³•è¶…è¶Šäº†å…ˆè¿›æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.16976">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a9c483fce20d8568b39aba3e9050c4cd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-71308df7374544a9117566e1c0833b7c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9d0e2323f49a34e3870f4f7d25b20470.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-665f5a538a35afb60cf9302401f281bb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eafa8dfb14d16b1880836e281eb2b891.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-390f6d1f2e1541fe19a9231d13e96dbd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-92c0c38251a5a1e1816ab075cfcc1188.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d8ca372ae722100fa7ca30c00d8d2ff3.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="A-Causal-World-Model-Underlying-Next-Token-Prediction-Exploring-GPT-in-a-Controlled-Environment"><a href="#A-Causal-World-Model-Underlying-Next-Token-Prediction-Exploring-GPT-in-a-Controlled-Environment" class="headerlink" title="A Causal World Model Underlying Next Token Prediction: Exploring GPT in   a Controlled Environment"></a>A Causal World Model Underlying Next Token Prediction: Exploring GPT in   a Controlled Environment</h2><p><strong>Authors:Raanan Y. Rohekar, Yaniv Gurwicz, Sungduk Yu, Estelle Aflalo, Vasudev Lal</strong></p>
<p>Are generative pre-trained transformer (GPT) models, trained only to predict the next token, implicitly learning a world model from which sequences are generated one token at a time? We address this question by deriving a causal interpretation of the attention mechanism in GPT and presenting a causal world model that arises from this interpretation. Furthermore, we propose that GPT models, at inference time, can be utilized for zero-shot causal structure learning for input sequences, and introduce a corresponding confidence score. Empirical tests were conducted in controlled environments using the setups of the Othello and Chess strategy games. A GPT, pre-trained on real-world games played with the intention of winning, was tested on out-of-distribution synthetic data consisting of sequences of random legal moves. We find that the GPT model is likely to generate legal next moves for out-of-distribution sequences for which a causal structure is encoded in the attention mechanism with high confidence. In cases where it generates illegal moves, it also fails to capture a causal structure. </p>
<blockquote>
<p>é¢„è®­ç»ƒç”Ÿæˆå¼è½¬æ¢å™¨ï¼ˆGPTï¼‰æ¨¡å‹æ˜¯å¦ä»…é€šè¿‡é¢„æµ‹ä¸‹ä¸€ä¸ªä»¤ç‰Œæ¥éšå¼åœ°å­¦ä¹ ä¸€ä¸ªä¸–ç•Œæ¨¡å‹ï¼Œä»è¿™ä¸ªæ¨¡å‹ä¸­ï¼Œåºåˆ—æ˜¯ä¸€ä¸ªæ¥ä¸€ä¸ªç”Ÿæˆçš„ï¼Ÿæˆ‘ä»¬é€šè¿‡æ¨å¯¼GPTä¸­æ³¨æ„åŠ›æœºåˆ¶çš„å› æœè§£é‡Šï¼Œæå‡ºä¸€ä¸ªç”±æ­¤äº§ç”Ÿçš„å› æœä¸–ç•Œæ¨¡å‹æ¥å›ç­”è¿™ä¸ªé—®é¢˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºï¼Œåœ¨æ¨ç†é˜¶æ®µï¼ŒGPTæ¨¡å‹å¯ç”¨äºè¾“å…¥åºåˆ—çš„é›¶æ‹å› æœç»“æ„å­¦ä¹ ï¼Œå¹¶å¼•å…¥ç›¸åº”çš„ç½®ä¿¡åº¦è¯„åˆ†ã€‚åœ¨Othelloå’Œè±¡æ£‹ç­–ç•¥æ¸¸æˆçš„è®¾ç½®ä¸‹ï¼Œæˆ‘ä»¬åœ¨å—æ§ç¯å¢ƒä¸­è¿›è¡Œäº†å®è¯æµ‹è¯•ã€‚ä¸€ä¸ªç»è¿‡ç°å®ä¸–ç•Œè·èƒœæ¸¸æˆè®­ç»ƒçš„GPTè¢«æµ‹è¯•åœ¨ç”±éšæœºåˆæ³•åŠ¨ä½œåºåˆ—ç»„æˆçš„ä¸åœ¨åˆ†å¸ƒå†…çš„åˆæˆæ•°æ®ä¸Šã€‚æˆ‘ä»¬å‘ç°GPTæ¨¡å‹å¾ˆå¯èƒ½ä¼šä¸ºä¸åœ¨åˆ†å¸ƒå†…çš„åºåˆ—ç”Ÿæˆåˆæ³•çš„ä¸‹ä¸€ä¸ªåŠ¨ä½œï¼Œè¿™äº›åŠ¨ä½œçš„å› æœç»“æ„åœ¨æ³¨æ„åŠ›æœºåˆ¶ä¸­é«˜ç½®ä¿¡åº¦ç¼–ç ã€‚åœ¨å®ƒäº§ç”Ÿéæ³•åŠ¨ä½œçš„æƒ…å†µä¸‹ï¼Œå®ƒä¹Ÿæœªèƒ½æ•æ‰åˆ°å› æœå…³ç³»ç»“æ„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07446v4">PDF</a> International Conference on Machine Learning (ICML), 2025</p>
<p><strong>Summary</strong></p>
<p>GPTæ¨¡å‹æ˜¯å¦é€šè¿‡ä»…é¢„æµ‹ä¸‹ä¸€ä¸ªä»¤ç‰Œçš„æ–¹å¼ï¼Œéšå¼åœ°å­¦ä¹ äº†ä¸€ä¸ªç”Ÿæˆåºåˆ—çš„ä¸–ç•Œæ¨¡å‹ï¼Ÿæœ¬æ–‡é€šè¿‡æ¨å¯¼GPTä¸­æ³¨æ„åŠ›æœºåˆ¶çš„å› æœè§£é‡Šï¼Œæå‡ºäº†ä¸€ä¸ªç”±æ­¤äº§ç”Ÿçš„å› æœä¸–ç•Œæ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¤ä¸ºGPTæ¨¡å‹åœ¨æ¨ç†æ—¶å¯ç”¨äºé›¶æ ·æœ¬å› æœç»“æ„å­¦ä¹ è¾“å…¥åºåˆ—ï¼Œå¹¶å¼•å…¥äº†ç›¸åº”çš„ç½®ä¿¡åº¦è¯„åˆ†ã€‚åœ¨Othelloå’ŒChessç­–ç•¥æ¸¸æˆçš„è®¾å®šä¸‹ï¼Œæˆ‘ä»¬è¿›è¡Œäº†å®è¯æµ‹è¯•ã€‚é¢„è®­ç»ƒåœ¨çœŸå®ä¸–ç•Œæ¸¸æˆä¸Šçš„GPTæ¨¡å‹åœ¨éšæœºåˆæ³•åŠ¨ä½œåºåˆ—çš„ç¦»ç¾¤åˆæˆæ•°æ®ä¸Šè¿›è¡Œæµ‹è¯•ã€‚æˆ‘ä»¬å‘ç°GPTæ¨¡å‹å¾ˆå¯èƒ½ä¸ºå…·æœ‰ç¼–ç åœ¨æ³¨æ„åŠ›æœºåˆ¶ä¸­çš„å› æœç»“æ„çš„ç¦»ç¾¤åºåˆ—ç”Ÿæˆåˆæ³•çš„ä¸‹ä¸€ä¸ªåŠ¨ä½œï¼Œå¹¶ä¸ºé«˜ç½®ä¿¡åº¦çš„ç”ŸæˆåŠ¨ä½œç¼–ç ç»“æ„ã€‚å½“ç”Ÿæˆéæ³•åŠ¨ä½œæ—¶ï¼Œå®ƒæœªèƒ½æ•æ‰åˆ°å› æœå…³ç³»ç»“æ„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GPTæ¨¡å‹é€šè¿‡é¢„æµ‹ä¸‹ä¸€ä¸ªä»¤ç‰Œéšå¼åœ°å­¦ä¹ äº†ç”Ÿæˆåºåˆ—çš„ä¸–ç•Œæ¨¡å‹ã€‚</li>
<li>GPTä¸­çš„æ³¨æ„åŠ›æœºåˆ¶æä¾›äº†ä¸€ä¸ªå› æœè§£é‡Šï¼Œå¹¶ç”±æ­¤äº§ç”Ÿäº†å› æœä¸–ç•Œæ¨¡å‹ã€‚</li>
<li>GPTæ¨¡å‹åœ¨æ¨ç†é˜¶æ®µå¯ç”¨äºé›¶æ ·æœ¬å› æœç»“æ„å­¦ä¹ è¾“å…¥åºåˆ—ã€‚</li>
<li>GPTæ¨¡å‹å¯ä»¥ç”Ÿæˆå¸¦æœ‰ç¼–ç åœ¨æ³¨æ„åŠ›æœºåˆ¶ä¸­çš„å› æœç»“æ„çš„ç¦»ç¾¤åºåˆ—çš„åˆæ³•ä¸‹ä¸€ä¸ªåŠ¨ä½œã€‚</li>
<li>å½“GPTæ¨¡å‹ç”Ÿæˆéæ³•åŠ¨ä½œæ—¶ï¼Œæ„å‘³ç€å®ƒæœªèƒ½æ•æ‰åˆ°å› æœå…³ç³»ç»“æ„ã€‚</li>
<li>GPTæ¨¡å‹çš„ç½®ä¿¡åº¦è¯„åˆ†åœ¨ç”ŸæˆåŠ¨ä½œæ—¶å…·æœ‰å®é™…æ„ä¹‰ï¼Œå¯ç”¨äºè¯„ä¼°æ¨¡å‹çš„ç¡®å®šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.07446">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-d60ddd4c9a67f79af2c6b67d49c7979a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fb04651769f854a9d8b1b3355d545a7e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-49aafc3454939387198dc696461df3a9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-33d4d353fbb691120b47a1bd052eae6e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-da22c2ad0a999fb91cb1d8320f24717c.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-07-10/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-07-10/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-10/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-096ba8a0aff224167a07cb075ccf4792.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-10  Conditional Multi-Stage Failure Recovery for Embodied Agents
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-07-10
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-10/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-5c4fdda0376711e17333cca53334a489.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-10  A Survey on Latent Reasoning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-07-10
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">30806.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
