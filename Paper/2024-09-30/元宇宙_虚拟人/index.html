<!DOCTYPE HTML><html lang="zh-CN"><head><meta charset="utf-8"><meta name="keywords" content="元宇宙/虚拟人"><meta name="description" content="元宇宙/虚拟人 方向最新论文已更新，请持续关注 Update in 2024-09-30  LightAvatar Efficient Head Avatar as Dynamic Neural Light Field"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="renderer" content="webkit|ie-stand|ie-comp"><meta name="mobile-web-app-capable" content="yes"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"><meta name="referrer" content="no-referrer-when-downgrade"><title>元宇宙/虚拟人 | Talk2Paper</title><link rel="icon" type="image/png" href="/Talk2Paper/favicon.png"><style>body{background-image:url(/Talk2Paper/background.jpg);background-repeat:no-repeat;background-size:100% 100%;background-attachment:fixed}</style><link rel="stylesheet" href="/Talk2Paper/libs/awesome/css/all.min.css"><link rel="stylesheet" href="/Talk2Paper/libs/materialize/materialize.min.css"><link rel="stylesheet" href="/Talk2Paper/libs/aos/aos.css"><link rel="stylesheet" href="/Talk2Paper/libs/animate/animate.min.css"><link rel="stylesheet" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css"><link rel="stylesheet" href="/Talk2Paper/css/matery.css"><link rel="stylesheet" href="/Talk2Paper/css/my.css"><link rel="stylesheet" href="/Talk2Paper/css/dark.css" media="none" onload='"all"!=media&&(media="all")'><link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css"><link rel="stylesheet" href="/Talk2Paper/css/post.css"><script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script><meta name="generator" content="Hexo 7.3.0"></head><body><header class="navbar-fixed"><nav id="headNav" class="bg-color nav-transparent"><div id="navContainer" class="nav-wrapper container"><div class="brand-logo"><a href="/Talk2Paper/" class="waves-effect waves-light"><img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO"> <span class="logo-span">Talk2Paper</span></a></div><a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a><ul class="right nav-menu"><li class="hide-on-med-and-down nav-item"><a href="/Talk2Paper/" class="waves-effect waves-light"><i class="fas fa-home" style="zoom:0.6"></i> <span>首页</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/Talk2Paper/tags" class="waves-effect waves-light"><i class="fas fa-tags" style="zoom:0.6"></i> <span>标签</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/Talk2Paper/categories" class="waves-effect waves-light"><i class="fas fa-bookmark" style="zoom:0.6"></i> <span>分类</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/Talk2Paper/archives" class="waves-effect waves-light"><i class="fas fa-archive" style="zoom:0.6"></i> <span>归档</span></a></li><li><a href="#searchModal" class="modal-trigger waves-effect waves-light"><i id="searchIcon" class="fas fa-search" title="搜索" style="zoom:0.85"></i></a></li><li><a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式"><i id="sum-moon-icon" class="fas fa-sun" style="zoom:0.85"></i></a></li></ul><div id="mobile-nav" class="side-nav sidenav"><div class="mobile-head bg-color"><img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img"><div class="logo-name">Talk2Paper</div><div class="logo-desc">Never really desperate, only the lost of the soul.</div></div><ul class="menu-list mobile-menu-list"><li class="m-nav-item"><a href="/Talk2Paper/" class="waves-effect waves-light"><i class="fa-fw fas fa-home"></i> 首页</a></li><li class="m-nav-item"><a href="/Talk2Paper/tags" class="waves-effect waves-light"><i class="fa-fw fas fa-tags"></i> 标签</a></li><li class="m-nav-item"><a href="/Talk2Paper/categories" class="waves-effect waves-light"><i class="fa-fw fas fa-bookmark"></i> 分类</a></li><li class="m-nav-item"><a href="/Talk2Paper/archives" class="waves-effect waves-light"><i class="fa-fw fas fa-archive"></i> 归档</a></li><li><div class="divider"></div></li><li><a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank"><i class="fab fa-github-square fa-fw"></i> Fork Me</a></li></ul></div></div><style>.nav-transparent .github-corner{display:none!important}.github-corner{position:absolute;z-index:10;top:0;right:0;border:0;transform:scale(1.1)}.github-corner svg{color:#0f9d58;fill:#fff;height:64px;width:64px}.github-corner:hover .octo-arm{animation:a .56s ease-in-out}.github-corner .octo-arm{animation:none}@keyframes a{0%,to{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}</style><a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank" data-tooltip="Fork Me" data-position="left" data-delay="50"><svg viewBox="0 0 250 250" aria-hidden="true"><path d="M0 0 115 115 130 115 142 142 250 250 250 0Z"></path><path d="M128.3 109C113.8 99.7 119 89.6 119 89.6 122 82.7 120.5 78.6 120.5 78.6 119.2 72 123.4 76.3 123.4 76.3 127.3 80.9 125.5 87.3 125.5 87.3 122.9 97.6 130.6 101.9 134.4 103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115 115C114.9 115.1 118.7 116.5 119.8 115.4L133.7 101.6C136.9 99.2 139.9 98.4 142.2 98.6 133.8 88 127.5 74.4 143.8 58 148.5 53.4 154 51.2 159.7 51 160.3 49.4 163.2 43.6 171.4 40.1 171.4 40.1 176.1 42.5 178.8 56.2 183.1 58.6 187.2 61.8 190.9 65.4 194.5 69 197.7 73.2 200.1 77.6 213.8 80.2 216.3 84.9 216.3 84.9 212.7 93.1 206.9 96 205.4 96.6 205.1 102.4 203 107.8 198.3 112.5 181.9 128.9 168.3 122.5 157.7 114.1 157.9 116.9 156.7 120.9 152.7 124.9L141 136.5C139.8 137.7 141.6 141.9 141.8 141.8Z" fill="currentColor" class="octo-body"></path></svg></a></nav></header><div class="bg-cover pd-header post-cover" style="background-image:url('https://picx.zhimg.com/v2-35ca8870fea42c6b9c3feb32de431d47.jpg')"><div class="container" style="right:0;left:0"><div class="row"><div class="col s12 m12 l12"><div class="brand"><h1 class="description center-align post-title">元宇宙/虚拟人</h1></div></div></div></div></div><main class="post-container content"><div class="row"><div id="main-content" class="col s12 m12 l9"><div id="artDetail"><div class="card"><div class="card-content article-info"><div class="row tag-cate"><div class="col s7"><div class="article-tag"><a href="/Talk2Paper/tags/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/"><span class="chip bg-color">元宇宙/虚拟人</span></a></div></div><div class="col s5 right-align"><div class="post-cate"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/Talk2Paper/categories/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/" class="post-category">元宇宙/虚拟人</a></div></div></div><div class="post-info"><div class="post-date info-break-policy"><i class="far fa-calendar-minus fa-fw"></i> 发布日期:&nbsp;&nbsp; 2024-09-30</div><div class="post-date info-break-policy"><i class="far fa-calendar-check fa-fw"></i> 更新日期:&nbsp;&nbsp; 2024-12-11</div><div class="info-break-policy"><i class="far fa-file-word fa-fw"></i> 文章字数:&nbsp;&nbsp; 3.8k</div><div class="info-break-policy"><i class="far fa-clock fa-fw"></i> 阅读时长:&nbsp;&nbsp; 14 分</div><div id="busuanzi_container_page_pv" class="info-break-policy"><i class="far fa-eye fa-fw"></i> 阅读次数:&nbsp;&nbsp;<span id="busuanzi_value_page_pv"></span></div></div></div><hr class="clearfix"><div class="card-content article-card-content"><div id="articleContent"><blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a target="_blank" rel="noopener" href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-09-30-更新"><a href="#2024-09-30-更新" class="headerlink" title="2024-09-30 更新"></a>2024-09-30 更新</h1><h2 id="LightAvatar-Efficient-Head-Avatar-as-Dynamic-Neural-Light-Field"><a href="#LightAvatar-Efficient-Head-Avatar-as-Dynamic-Neural-Light-Field" class="headerlink" title="LightAvatar: Efficient Head Avatar as Dynamic Neural Light Field"></a>LightAvatar: Efficient Head Avatar as Dynamic Neural Light Field</h2><p><strong>Authors:Huan Wang, Feitong Tan, Ziqian Bai, Yinda Zhang, Shichen Liu, Qiangeng Xu, Menglei Chai, Anish Prabhu, Rohit Pandey, Sean Fanello, Zeng Huang, Yun Fu</strong></p><p>Recent works have shown that neural radiance fields (NeRFs) on top of parametric models have reached SOTA quality to build photorealistic head avatars from a monocular video. However, one major limitation of the NeRF-based avatars is the slow rendering speed due to the dense point sampling of NeRF, preventing them from broader utility on resource-constrained devices. We introduce LightAvatar, the first head avatar model based on neural light fields (NeLFs). LightAvatar renders an image from 3DMM parameters and a camera pose via a single network forward pass, without using mesh or volume rendering. The proposed approach, while being conceptually appealing, poses a significant challenge towards real-time efficiency and training stability. To resolve them, we introduce dedicated network designs to obtain proper representations for the NeLF model and maintain a low FLOPs budget. Meanwhile, we tap into a distillation-based training strategy that uses a pretrained avatar model as teacher to synthesize abundant pseudo data for training. A warping field network is introduced to correct the fitting error in the real data so that the model can learn better. Extensive experiments suggest that our method can achieve new SOTA image quality quantitatively or qualitatively, while being significantly faster than the counterparts, reporting 174.1 FPS (512x512 resolution) on a consumer-grade GPU (RTX3090) with no customized optimization.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.18057v1">PDF</a> Appear in ECCV’24 CADL Workshop. Code: <a target="_blank" rel="noopener" href="https://github.com/MingSun-Tse/LightAvatar-TensorFlow">https://github.com/MingSun-Tse/LightAvatar-TensorFlow</a></p><p><strong>Summary</strong><br>利用NeRFs构建真实头像，通过NeLFs实现快速渲染。</p><p><strong>Key Takeaways</strong></p><ol><li>NeRFs在构建真实头像方面达到SOTA质量。</li><li>NeRFs渲染速度慢，限制其在资源受限设备上的应用。</li><li>LightAvatar基于NeLFs，实现从3DMM参数和相机姿态快速渲染头像。</li><li>LightAvatar不使用网格或体积渲染，提高效率。</li><li>优化网络设计以实现NeLF模型的实时效率和训练稳定性。</li><li>使用预训练模型作为教师，通过蒸馏策略生成伪数据训练。</li><li>引入扭曲场网络校正真实数据拟合误差，提升模型学习效果。</li></ol><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>标题：LightAvatar：基于神经光照场的高效头部化身技术</p></li><li><p>作者：Huan Wang及其他合著者（具体名单见原文）</p></li><li><p>隶属机构：第一作者Huan Wang曾在美国东北大学和Google实习。</p></li><li><p>关键词：神经光照场（NeLF）、头部化身、实时渲染、神经网络、参数模型</p></li><li><p>链接：论文链接（待补充，具体链接以实际发布为准），GitHub代码链接（待补充，具体链接以实际发布为准）</p></li><li><p>摘要：</p><ul><li>(1) 研究背景：本文的研究背景是关于基于神经光照场的高效头部化身技术。近年来，神经辐射场（NeRF）在构建逼真的头部化身方面取得了显著进展，但它们的主要局限性是渲染速度慢，无法广泛应用于资源受限的设备。因此，本文提出了基于神经光照场（NeLF）的LightAvatar模型，旨在解决这一问题。</li><li>(2) 过去的方法及其问题：过去的方法主要基于NeRF技术构建头部化身，虽然质量高，但渲染速度慢。这个问题限制了它们在资源受限设备上的广泛应用。因此，需要一种更高效的头部化身技术来满足实时应用的需求。</li><li>(3) 研究方法：本文提出了基于神经光照场（NeLF）的LightAvatar模型。该模型通过单个网络前向传递，从3DMM参数和相机姿态渲染图像，无需使用网格或体积渲染。为解决实时效率和训练稳定性方面的挑战，本文引入了专门的网络设计，以获得适当的NeLF模型表示，并维持低浮点运算（FLOPs）预算。同时，采用基于蒸馏的训练策略，使用预训练的化身模型作为教师进行合成数据的训练。</li><li>(4) 任务与性能：本文的方法在头部化身任务上取得了显著成果，实现了较快的渲染速度和较高的图像质量。与现有方法相比，LightAvatar在渲染速度和图像质量方面均有所超越。实验结果表明，该方法达到了预期的目标，为实时应用提供了高效的头部化身技术。</li></ul></li></ol><p>希望这个总结符合您的要求！<br>7. 方法论概述：</p><p>本研究采用了一种基于神经光照场（NeLF）的高效头部化身技术，即LightAvatar模型。方法论主要包含以下几个步骤：</p><ul><li>(1) 研究背景分析：文章首先分析了当前头部化身技术的局限性，如渲染速度慢，无法广泛应用于资源受限的设备等。</li><li>(2) 问题提出：针对上述问题，提出了基于神经光照场（NeLF）的LightAvatar模型，旨在实现高效头部化身技术，满足实时应用的需求。</li><li>(3) 模型构建：LightAvatar模型通过单个网络前向传递，从3DMM参数和相机姿态渲染图像，无需使用网格或体积渲染。为了应对实时效率和训练稳定性方面的挑战，引入了专门的网络设计，并维持低浮点运算（FLOPs）预算。同时，采用基于蒸馏的训练策略，使用预训练的化身模型作为教师进行合成数据的训练。</li><li>(4) 实验验证：文章通过实验验证了LightAvatar模型的有效性，在头部化身任务上取得了显著成果，实现了较快的渲染速度和较高的图像质量。实验结果表明，该方法达到了预期的目标，为实时应用提供了高效的头部化身技术。此外，还对模型的性能进行了对比分析，验证了其在渲染速度和图像质量方面的优势。这一结果验证了基于神经光照场的LightAvatar模型在实际应用中的可行性和优越性。</li></ul><p>以上内容仅供参考，具体细节和方法论的实施方式可能需要根据原文进行详细解读和梳理。<br>8. Conclusion:</p><pre><code>- (1) 本研究的工作意义重大。在头部化身技术领域，该文章提出了一种基于神经光照场（NeLF）的LightAvatar模型，解决了现有技术渲染速度慢、无法广泛应用于资源受限设备的问题。该研究为实时应用提供了高效的头部化身技术，具有重要的实际应用价值。

- (2) 创新点：本文的创新之处在于提出了基于神经光照场（NeLF）的LightAvatar模型，通过单个网络前向传递，从3DMM参数和相机姿态渲染图像，实现了高效的头部化身技术。
性能：实验结果表明，LightAvatar模型在头部化身任务上取得了显著成果，实现了较快的渲染速度和较高的图像质量，优于现有方法。
工作量：文章对模型的构建和实验验证进行了详细的阐述，但关于具体实现的细节和技术难度未做深入探讨，如网络设计的具体结构、蒸馏训练策略的具体实施方式等。
</code></pre><p>以上结论仅供参考，具体评价可能需要根据原文的详细内容进行深入分析。</p><details><summary>点此查看论文截图</summary><img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-6ba7d0913a191f3ae9bcf297663a3c09.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-8f0739cce843124abdd4f19bc6f3bff0.jpg" align="middle"></details><h2 id="Gaussian-Deja-vu-Creating-Controllable-3D-Gaussian-Head-Avatars-with-Enhanced-Generalization-and-Personalization-Abilities"><a href="#Gaussian-Deja-vu-Creating-Controllable-3D-Gaussian-Head-Avatars-with-Enhanced-Generalization-and-Personalization-Abilities" class="headerlink" title="Gaussian Deja-vu: Creating Controllable 3D Gaussian Head-Avatars with   Enhanced Generalization and Personalization Abilities"></a>Gaussian Deja-vu: Creating Controllable 3D Gaussian Head-Avatars with Enhanced Generalization and Personalization Abilities</h2><p><strong>Authors:Peizhi Yan, Rabab Ward, Qiang Tang, Shan Du</strong></p><p>Recent advancements in 3D Gaussian Splatting (3DGS) have unlocked significant potential for modeling 3D head avatars, providing greater flexibility than mesh-based methods and more efficient rendering compared to NeRF-based approaches. Despite these advancements, the creation of controllable 3DGS-based head avatars remains time-intensive, often requiring tens of minutes to hours. To expedite this process, we here introduce the &#96;&#96;Gaussian D&#39;ej`a-vu” framework, which first obtains a generalized model of the head avatar and then personalizes the result. The generalized model is trained on large 2D (synthetic and real) image datasets. This model provides a well-initialized 3D Gaussian head that is further refined using a monocular video to achieve the personalized head avatar. For personalizing, we propose learnable expression-aware rectification blendmaps to correct the initial 3D Gaussians, ensuring rapid convergence without the reliance on neural networks. Experiments demonstrate that the proposed method meets its objectives. It outperforms state-of-the-art 3D Gaussian head avatars in terms of photorealistic quality as well as reduces training time consumption to at least a quarter of the existing methods, producing the avatar in minutes.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.16147v2">PDF</a> 11 pages, Accepted by WACV 2025 in Round 1</p><p><strong>Summary</strong><br>3DGS技术优化，构建高效可控的3D头像生成模型。</p><p><strong>Key Takeaways</strong></p><ol><li>3DGS在3D头像建模中提供比网格方法更大的灵活性和比NeRF更高效的渲染。</li><li>现有3DGS头像创建耗时，需数分钟至数小时。</li><li>提出“Gaussian D&#39;ej`a-vu”框架，先获取头像通用模型，再个性化定制。</li><li>通用模型基于大规模2D图像数据集训练。</li><li>利用单目视频进一步精炼3D头像。</li><li>提出可学习的表达式感知校正混合图，实现快速收敛。</li><li>新方法在真实感和训练时间上优于现有方法，缩短至四分之一。</li></ol><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>标题：基于高斯混合模型的快速可控三维头像创建研究</p></li><li><p>作者：严培植、沃德·拉巴巴、唐强、杜山</p></li><li><p>隶属机构：严培植、沃德·拉巴巴隶属加拿大不列颠哥伦比亚大学，唐强隶属华为加拿大分公司，杜山隶属加拿大不列颠哥伦比亚大学奥肯根校区。</p></li><li><p>关键词：高斯混合模型、三维头像创建、可控性、渲染效率、个性化模型</p></li><li><p>链接：论文链接（待补充），GitHub代码链接（待补充，若无则填写“None”）</p></li><li><p>总结：</p><ul><li>(1)研究背景：<br>随着虚拟现实、增强现实、游戏制作等领域的发展，对快速创建高质量三维头像的需求日益增加。文章探讨如何高效地创建具有可控性的三维高斯头像模型，以解决现有方法的效率和质量控制问题。</li></ul><p>-(2)过去的方法及存在的问题：<br>现有方法主要包括基于网格的方法和基于NeRF的方法。基于网格的方法虽然渲染效率高，但缺乏灵活性；而基于NeRF的方法虽然灵活，但渲染效率较低。文章旨在克服这些方法的不足，提出一种更高效、高质量且可控的三维高斯头像创建方法。</p><p>-(3)研究方法：<br>文章提出了“Gaussian D´ej&#96;a-vu”框架，首先通过大型二维图像数据集训练通用模型，然后个性化结果。通用模型采用三维高斯混合模型，通过单目视频进一步精细化，实现个性化头像。为个性化处理，文章提出了可学习的表情感知校正混合图（blendmaps），以纠正初始三维高斯模型，确保快速收敛，无需依赖神经网络。</p><p>-(4)任务与性能：<br>文章的方法旨在创建高质量的三维高斯头像模型，具有可控的面部表情和视角。实验表明，该方法在照片逼真质量方面优于现有方法，并将训练时间消耗减少至少四分之一，能够在几分钟内生成头像。这些性能表明该方法在支持其目标方面取得了显著进展。</p></li></ol><p>请注意，由于缺少详细的论文内容，某些信息可能无法完全准确概括。以上内容仅供参考，请在实际阅读论文后做出更为准确的总结和评价。<br>7. 方法论：</p><p>这篇文章提出了一个基于高斯混合模型的快速可控三维头像创建方法，具体步骤如下：</p><ul><li><p>(1) 研究背景与动机：针对现有三维头像创建方法（如基于网格的方法和基于NeRF的方法）存在的效率和质量控制问题，文章旨在开发一种更高效、高质量且可控的三维头像创建方法。</p></li><li><p>(2) 数据准备：首先，通过大型二维图像数据集训练通用模型。这些数据集可能包含各种面部表情和角度的头像图像。</p></li><li><p>(3) 通用模型构建：利用三维高斯混合模型创建通用模型。这个模型具有良好的通用性和灵活性，能够适应多种不同的头像形状和表情。</p></li><li><p>(4) 个性化处理：为了创建个性化的三维头像，文章提出了可学习的表情感知校正混合图（blendmaps）。这种技术用于纠正初始的三维高斯模型，以确保快速收敛并达到个性化效果。</p></li><li><p>(5) 实验流程：在实际实验中，通过单目视频进一步精细化通用模型，实现个性化头像的创建。实验过程包括数据采集、模型训练、模型评估等步骤。</p></li><li><p>(6) 性能评估：通过实验对比，证明该方法在照片逼真质量方面优于现有方法，并将训练时间消耗减少至少四分之一。此外，该方法能够在几分钟内生成高质量的三维头像。</p></li></ul><p>总的来说，该文章通过结合高斯混合模型和个性化处理技术，提出了一种高效、高质量且可控的三维头像创建方法。这种方法克服了现有方法的不足，为虚拟现实、增强现实、游戏制作等领域提供了一种新的解决方案。<br>8. 结论：</p><pre><code>- (1) 该研究工作在虚拟现实、增强现实、游戏制作等领域具有重要意义，它提供了一种快速创建高质量三维头像的新方法，满足了这些领域对高质量三维头像的日益增长的需求。
 
- (2) 创新点：该文章提出了一种基于高斯混合模型的快速可控三维头像创建方法，该方法结合了大型二维图像数据集和个性化处理技术，实现了高质量、高效率的三维头像创建。同时，文章还提出了可学习的表情感知校正混合图（blendmaps）技术，用于纠正初始三维高斯模型，确保快速收敛并达到个性化效果。
 性能：实验结果表明，该方法在照片逼真质量方面优于现有方法，训练时间消耗减少至少四分之一，能够在几分钟内生成高质量的三维头像。这表明该文章提出的方法在性能和效率方面都取得了显著的进展。
 工作量：文章对方法的实现进行了详细的描述和解释，提供了清晰的实验过程和结果，工作量较为充足。但是，由于缺少详细的论文内容，无法全面评估其工作量的大小。
</code></pre><details><summary>点此查看论文截图</summary><img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-03d2392bdddc196453b9c3bf3140c8a5.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-41ce0c960b001c3433e8f53f14598019.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-6fcd3ef7a1064ac1787a3a9488d68df8.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-35ca8870fea42c6b9c3feb32de431d47.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-14cc411449649510fb55a247aa080e88.jpg" align="middle"></details></div><hr><div class="reprint" id="reprint-statement"><div class="reprint__author"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-user">文章作者:</i></span> <span class="reprint-info"><a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a></span></div><div class="reprint__type"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-link">文章链接:</i></span> <span class="reprint-info"><a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-09-30/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/">https://kedreamix.github.io/Talk2Paper/Paper/2024-09-30/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/</a></span></div><div class="reprint__notice"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-copyright">版权声明:</i></span> <span class="reprint-info">本博客所有文章除特別声明外，均采用 <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a> 许可协议。转载请注明来源 <a href="/Talk2Paper/about" target="_blank">Kedreamix</a> !</span></div></div><script async defer>function navToReprintStatement(){$("html, body").animate({scrollTop:$("#reprint-statement").offset().top-80},800)}document.addEventListener("copy",(function(t){M.toast({html:'<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>'})}))</script><div class="tag_share" style="display:block"><div class="post-meta__tag-list" style="display:inline-block"><div class="article-tag"><a href="/Talk2Paper/tags/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/"><span class="chip bg-color">元宇宙/虚拟人</span></a></div></div><div class="post_share" style="zoom:80%;width:fit-content;display:inline-block;float:right;margin:-.15rem 0"><link rel="stylesheet" href="/Talk2Paper/libs/share/css/share.min.css"><div id="article-share"><div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div><script src="/Talk2Paper/libs/share/js/social-share.min.js"></script></div></div></div></div></div><article id="prenext-posts" class="prev-next articles"><div class="row article-row"><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge left-badge text-color"><i class="fas fa-chevron-left"></i> &nbsp;上一篇</div><div class="card"><a href="/Talk2Paper/Paper/2024-09-30/Talking%20Head%20Generation/"><div class="card-image"><img src="https://pic1.zhimg.com/v2-ffd8853d59269eaf03b2e197f7818a6b.jpg" class="responsive-img" alt="Talking Head Generation"> <span class="card-title">Talking Head Generation</span></div></a><div class="card-content article-content"><div class="summary block-with-text">Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2024-09-30 Stable Video Portraits</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i> 2024-09-30</span><span class="publish-author"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">Talking Head Generation</a></span></div></div><div class="card-action article-tags"><a href="/Talk2Paper/tags/Talking-Head-Generation/"><span class="chip bg-color">Talking Head Generation</span></a></div></div></div><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge right-badge text-color">下一篇&nbsp;<i class="fas fa-chevron-right"></i></div><div class="card"><a href="/Talk2Paper/Paper/2024-09-27/Diffusion%20Models/"><div class="card-image"><img src="https://picx.zhimg.com/v2-159f110782a0e9cd0ad544d1039ee7f0.jpg" class="responsive-img" alt="Diffusion Models"> <span class="card-title">Diffusion Models</span></div></a><div class="card-content article-content"><div class="summary block-with-text">Diffusion Models 方向最新论文已更新，请持续关注 Update in 2024-09-27 Degradation-Guided One-Step Image Super-Resolution with Diffusion Priors</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i> 2024-09-27</span><span class="publish-author"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">Diffusion Models</a></span></div></div><div class="card-action article-tags"><a href="/Talk2Paper/tags/Diffusion-Models/"><span class="chip bg-color">Diffusion Models</span></a></div></div></div></div></article></div><script src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script><script src="/Talk2Paper/libs/codeBlock/codeLang.js"></script><script src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script><script src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script></div><div id="toc-aside" class="expanded col l3 hide-on-med-and-down"><div class="toc-widget card" style="background-color:#fff"><div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div><div id="toc-content"></div></div></div></div><div id="floating-toc-btn" class="hide-on-med-and-down"><a class="btn-floating btn-large bg-color"><i class="fas fa-list-ul"></i></a></div><script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script><script>$((function(){tocbot.init({tocSelector:"#toc-content",contentSelector:"#articleContent",headingsOffset:-(.4*$(window).height()-45),collapseDepth:Number("0"),headingSelector:"h2, h3, h4"});let t=parseInt(.4*$(window).height()-64),e=$(".toc-widget");$(window).scroll((function(){$(window).scrollTop()>t?e.addClass("toc-fixed"):e.removeClass("toc-fixed")}));const o="expanded";let n=$("#toc-aside"),i=$("#main-content");$("#floating-toc-btn .btn-floating").click((function(){n.hasClass(o)?(n.removeClass(o).hide(),i.removeClass("l9")):(n.addClass(o).show(),i.addClass("l9")),function(t,e){let o=$("#"+t);if(0===o.length)return;let n=o.width();n+=n>=450?21:n>=350&&n<450?18:n>=300&&n<350?16:14,$("#"+e).width(n)}("artDetail","prenext-posts")}))}))</script></main><footer class="page-footer bg-color"><link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css"><style>.aplayer .aplayer-lrc p{display:none;font-size:12px;font-weight:700;line-height:16px!important}.aplayer .aplayer-lrc p.aplayer-lrc-current{display:none;font-size:15px;color:#42b983}.aplayer.aplayer-fixed.aplayer-narrow .aplayer-body{left:-66px!important}.aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover{left:0!important}</style><div><div class="row"><meting-js class="col l8 offset-l2 m10 offset-m1 s12" server="netease" type="playlist" id="503838841" fixed="true" autoplay theme="#42b983" loop order="random" preload="auto" volume="0.7" list-folded="true"></meting-js></div></div><script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script><script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script><div class="container row center-align" style="margin-bottom:15px!important"><div class="col s12 m8 l8 copy-right">Copyright&nbsp;&copy; <span id="year">2024</span> <a href="/Talk2Paper/about" target="_blank">Kedreamix</a> |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a> |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a><br>&nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span class="white-color">5676k</span> <span id="busuanzi_container_site_pv">&nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;<span id="busuanzi_value_site_pv" class="white-color"></span></span> <span id="busuanzi_container_site_uv">&nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;<span id="busuanzi_value_site_uv" class="white-color"></span></span><br><span id="sitetime">Loading ...</span><script>var calcSiteTime=function(){var e=864e5,t=new Date,n="2024",i=t.getFullYear(),a=t.getMonth()+1,r=t.getDate(),s=t.getHours(),o=t.getMinutes(),g=t.getSeconds(),d=Date.UTC(n,"1","1","0","0","0"),m=Date.UTC(i,a,r,s,o,g)-d,l=Math.floor(m/31536e6),c=Math.floor(m/e-365*l);if(n===String(i)){document.getElementById("year").innerHTML=i;var u="This site has been running for "+c+" days";u="本站已运行 "+c+" 天",document.getElementById("sitetime").innerHTML=u}else{document.getElementById("year").innerHTML=n+" - "+i;var T="This site has been running for "+l+" years and "+c+" days";T="本站已运行 "+l+" 年 "+c+" 天",document.getElementById("sitetime").innerHTML=T}};calcSiteTime()</script><br></div><div class="col s12 m4 l4 social-link social-statis"><a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50"><i class="fab fa-github"></i></a><a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50"><i class="fas fa-envelope-open"></i></a> <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50"><i class="fab fa-zhihu1">知</i></a></div></div></footer><div class="progress-bar"></div><div id="searchModal" class="modal"><div class="modal-content"><div class="search-header"><span class="title"><i class="fas fa-search"></i> &nbsp;&nbsp;搜索</span> <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字" class="search-input"></div><div id="searchResult"></div></div></div><script>$((function(){!function(t,e,r){"use strict";$.ajax({url:t,dataType:"xml",success:function(t){var n=$("entry",t).map((function(){return{title:$("title",this).text(),content:$("content",this).text(),url:$("url",this).text()}})).get(),a=document.getElementById(e),s=document.getElementById(r);a.addEventListener("input",(function(){var t='<ul class="search-result-list">',e=this.value.trim().toLowerCase().split(/[\s\-]+/);s.innerHTML="",this.value.trim().length<=0||(n.forEach((function(r){var n=!0,a=r.title.trim().toLowerCase(),s=r.content.trim().replace(/<[^>]+>/g,"").toLowerCase(),i=r.url;i=0===i.indexOf("/")?r.url:"/"+i;var l=-1,c=-1,u=-1;if(""!==a&&""!==s&&e.forEach((function(t,e){l=a.indexOf(t),c=s.indexOf(t),l<0&&c<0?n=!1:(c<0&&(c=0),0===e&&(u=c))})),n){t+="<li><a href='"+i+"' class='search-result-title'>"+a+"</a>";var o=r.content.trim().replace(/<[^>]+>/g,"");if(u>=0){var h=u-20,f=u+80;h<0&&(h=0),0===h&&(f=100),f>o.length&&(f=o.length);var m=o.substr(h,f);e.forEach((function(t){var e=new RegExp(t,"gi");m=m.replace(e,'<em class="search-keyword">'+t+"</em>")})),t+='<p class="search-result">'+m+"...</p>"}t+="</li>"}})),t+="</ul>",s.innerHTML=t)}))}})}("/Talk2Paper/search.xml","searchInput","searchResult")}))</script><div class="stars-con"><div id="stars"></div><div id="stars2"></div><div id="stars3"></div></div><script>function switchNightMode(){$('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($("body")),setTimeout((function(){$("body").hasClass("DarkMode")?($("body").removeClass("DarkMode"),localStorage.setItem("isDark","0"),$("#sum-moon-icon").removeClass("fa-sun").addClass("fa-moon")):($("body").addClass("DarkMode"),localStorage.setItem("isDark","1"),$("#sum-moon-icon").addClass("fa-sun").removeClass("fa-moon")),setTimeout((function(){$(".Cuteen_DarkSky").fadeOut(1e3,(function(){$(this).remove()}))}),2e3)}))}</script><div id="backTop" class="top-scroll"><a class="btn-floating btn-large waves-effect waves-light" href="#!"><i class="fas fa-arrow-up"></i></a></div><script src="/Talk2Paper/libs/materialize/materialize.min.js"></script><script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script><script src="/Talk2Paper/libs/aos/aos.js"></script><script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script><script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script><script src="/Talk2Paper/js/matery.js"></script><script>var windowWidth=$(window).width();windowWidth>768&&document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>')</script><script src="https://ssl.captcha.qq.com/TCaptcha.js"></script><script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script><button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}()</script><script src="/Talk2Paper/libs/others/clicklove.js" async></script><script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script><script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script><script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({log:!1,pluginJsPath:"lib/",pluginModelPath:"assets/",pluginRootPath:"live2dw/",tagMode:!1})</script><style>[bg-lazy]{background-image:none!important;background-color:#eee!important}</style><script>window.imageLazyLoadSetting={isSPA:!1,preloadRatio:3,processImages:null}</script><script>window.addEventListener("load",(function(){var a=/\.(gif|jpg|jpeg|tiff|png)$/i,e=/^data:image\/[a-z\d\-\.\+]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach((function(t){var r=t.parentNode;"A"===r.tagName&&(a.test(r.href)||e.test(r.href))&&(r.href=t.dataset.original)}))}))</script><script>(t=>{t.imageLazyLoadSetting.processImages=n;var e=t.imageLazyLoadSetting.isSPA,a=t.imageLazyLoadSetting.preloadRatio||1,o=i();function i(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function n(n){(e||n)&&(o=i());for(var r,d=0;d<o.length;d++)0<=(r=(r=o[d]).getBoundingClientRect()).bottom&&0<=r.left&&r.top<=(t.innerHeight*a||document.documentElement.clientHeight*a)&&(()=>{var e,a,i,n,r=o[d];a=function(){o=o.filter((function(t){return r!==t})),t.imageLazyLoadSetting.onImageLoaded&&t.imageLazyLoadSetting.onImageLoaded(r)},(e=r).dataset.loaded||(e.hasAttribute("bg-lazy")?(e.removeAttribute("bg-lazy"),a&&a()):(i=new Image,n=e.getAttribute("data-original"),i.onload=function(){e.src=n,e.removeAttribute("data-original"),e.setAttribute("data-loaded",!0),a&&a()},i.onerror=function(){e.removeAttribute("data-original"),e.setAttribute("data-loaded",!1),e.src=n},e.src!==n&&(i.src=n)))})()}function r(){clearTimeout(n.tId),n.tId=setTimeout(n,500)}n(),document.addEventListener("scroll",r),t.addEventListener("resize",r),t.addEventListener("orientationchange",r)})(this)</script></body></html><script>var st,OriginTitile=document.title;document.addEventListener("visibilitychange",(function(){document.hidden?(document.title="Σ(っ °Д °;)っ诶，页面崩溃了嘛？",clearTimeout(st)):(document.title="φ(゜▽゜*)♪咦，又好了！",st=setTimeout((function(){document.title=OriginTitile}),3e3))}))</script>