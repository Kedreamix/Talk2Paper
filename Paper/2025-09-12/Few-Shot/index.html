<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot 方向最新论文已更新，请持续关注 Update in 2025-09-12  Implicit Shape-Prior for Few-Shot Assisted 3D Segmentation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Few-Shot/2509.08401v1/page_1_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-09-12
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-10-02
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    6.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    25 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-09-12-更新"><a href="#2025-09-12-更新" class="headerlink" title="2025-09-12 更新"></a>2025-09-12 更新</h1><h2 id="Implicit-Shape-Prior-for-Few-Shot-Assisted-3D-Segmentation"><a href="#Implicit-Shape-Prior-for-Few-Shot-Assisted-3D-Segmentation" class="headerlink" title="Implicit Shape-Prior for Few-Shot Assisted 3D Segmentation"></a>Implicit Shape-Prior for Few-Shot Assisted 3D Segmentation</h2><p><strong>Authors:Mathilde Monvoisin, Louise Piecuch, Blanche Texier, Cédric Hémon, Anaïs Barateau, Jérémie Huet, Antoine Nordez, Anne-Sophie Boureau, Jean-Claude Nunes, Diana Mateus</strong></p>
<p>The objective of this paper is to significantly reduce the manual workload required from medical professionals in complex 3D segmentation tasks that cannot be yet fully automated. For instance, in radiotherapy planning, organs at risk must be accurately identified in computed tomography (CT) or magnetic resonance imaging (MRI) scans to ensure they are spared from harmful radiation. Similarly, diagnosing age-related degenerative diseases such as sarcopenia, which involve progressive muscle volume loss and strength, is commonly based on muscular mass measurements often obtained from manual segmentation of medical volumes. To alleviate the manual-segmentation burden, this paper introduces an implicit shape prior to segment volumes from sparse slice manual annotations generalized to the multi-organ case, along with a simple framework for automatically selecting the most informative slices to guide and minimize the next interactions. The experimental validation shows the method’s effectiveness on two medical use cases: assisted segmentation in the context of at risks organs for brain cancer patients, and acceleration of the creation of a new database with unseen muscle shapes for patients with sarcopenia. </p>
<blockquote>
<p>本文的目标是为了显著减少在复杂的3D分割任务中，医疗专业人员所需的手动工作量，这些任务目前尚无法完全自动化。例如，在放射治疗计划中，必须在计算机断层扫描（CT）或磁共振成像（MRI）扫描中准确识别出风险器官，以确保这些器官免受有害辐射的影响。同样，诊断与年龄相关的退行性疾病，如肌肉体积逐渐丧失和力量减弱的肌少症，通常基于肌肉质量测量，而这些测量往往是从医疗体积的手动分割中获得的。为了减轻手动分割的负担，本文引入了一种隐式形状先验，从稀疏切片的手动注释中分割体积，并将其推广到多器官的情况，以及一个简单框架，用于自动选择最具信息量的切片来指导和最小化下一次交互。实验验证显示该方法在两种医学用例中的有效性：辅助分割脑癌患者风险器官的情境，以及加速为肌少症患者创建包含未见肌肉形状的新数据库。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.08580v1">PDF</a> Both first Authors contributed equally to this work, lastnames in   alphabetical order. This preprint has not undergone peer review or any   post-submission improvements or corrections. The Version of Record of this   contribution will be published in a Springer Nature Computer Science book   series (CCIS, LNAI, LNBI, LNBIP, LNCS) and the doi will soon be released</p>
<p><strong>Summary</strong></p>
<p>本文旨在减少医学专家在复杂的三维分割任务中的手动工作量，这些任务目前尚无法完全自动化。例如，在放射治疗计划中，必须在计算机断层扫描（CT）或磁共振成像（MRI）中准确识别风险器官，以确保它们免受有害辐射。同样，诊断年龄相关的退行性疾病（如肌肉减少症），需要基于手动分割医学体积所获得的肌肉质量测量。本文引入了一种隐式形状先验方法，可以从稀疏切片手动注释中分割体积并将其推广到多器官情况，以及一个简单框架，可以自动选择信息量最大的切片来指导和最小化下一次交互。实验验证显示该方法在两种医学用例中的有效性：辅助分割风险器官的背景下的脑癌患者，以及加速创建具有未见肌肉形状的新数据库的患者肌肉减少症。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>该论文旨在减轻医学专家在复杂三维分割任务中的手动工作量，这些任务目前尚无法完全自动化。</li>
<li>在放射治疗计划和诊断退行性疾病（如肌肉减少症）中，医学体积的准确分割是关键步骤。</li>
<li>论文引入了一种隐式形状先验方法，可以从稀疏切片手动注释中分割体积，并将其推广到多器官情况。</li>
<li>该方法采用了一个简单框架自动选择信息量最大的切片，以指导和最小化下一次交互。</li>
<li>实验验证显示该方法在风险器官的分割和肌肉减少症诊断中的有效性。</li>
<li>该方法有助于减轻医学专家在诊断过程中的负担，提高医疗质量和效率。</li>
<li>此技术有可能在将来推动医疗行业的自动化和智能化进程。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.08580">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Few-Shot/2509.08580v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Few-Shot/2509.08580v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Few-Shot/2509.08580v1/page_5_0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Two-Sides-of-the-Same-Optimization-Coin-Model-Degradation-and-Representation-Collapse-in-Graph-Foundation-Models"><a href="#Two-Sides-of-the-Same-Optimization-Coin-Model-Degradation-and-Representation-Collapse-in-Graph-Foundation-Models" class="headerlink" title="Two Sides of the Same Optimization Coin: Model Degradation and   Representation Collapse in Graph Foundation Models"></a>Two Sides of the Same Optimization Coin: Model Degradation and   Representation Collapse in Graph Foundation Models</h2><p><strong>Authors:Xunkai Li, Daohan Su, Sicheng Liu, Ru Zhang, Rong-Hua Li, Guoren Wang</strong></p>
<p>Graph foundation models, inspired by the success of LLMs, are designed to learn the optimal embedding from multi-domain TAGs for the downstream cross-task generalization capability. During our investigation, graph VQ-MAE stands out among the increasingly diverse landscape of GFM architectures. This is attributed to its ability to jointly encode topology and textual attributes from multiple domains into discrete embedding spaces with clear semantic boundaries. Despite its potential, domain generalization conflicts cause imperceptible pitfalls. In this paper, we instantiate two of them, and they are just like two sides of the same GFM optimization coin - Side 1 Model Degradation: The encoder and codebook fail to capture the diversity of inputs; Side 2 Representation Collapse: The hidden embedding and codebook vector fail to preserve semantic separability due to constraints from narrow representation subspaces. These two pitfalls (sides) collectively impair the decoder and generate the low-quality reconstructed supervision, causing the GFM optimization dilemma during pre-training (coin). Through empirical investigation, we attribute the above challenges to Information Bottleneck and Regularization Deficit. To address them, we propose MoT (Mixture-of-Tinkers) - (1) Information Tinker for Two Pitfalls, which utilizes an edge-wise semantic fusion strategy and a mixture-of-codebooks with domain-aware routing to improve information capacity. (2) Regularization Tinker for Optimization Coin, which utilizes two additional regularizations to further improve gradient supervision in our proposed Information Tinker. Notably, as a flexible architecture, MoT adheres to the scaling laws of GFM, offering a controllable model scale. Compared to SOTA baselines, experiments on 22 datasets across 6 domains demonstrate that MoT achieves significant improvements in supervised, few-shot, and zero-shot scenarios. </p>
<blockquote>
<p>图模型基础，受到大型语言模型（LLMs）成功的启发，旨在从多域标签（TAGs）中学习最佳嵌入，以获得下游跨任务泛化能力。在我们的研究中，图VQ-MAE在日益多样化的GFM架构中脱颖而出。这归功于它将多个领域的拓扑和文本属性联合编码到具有清晰语义边界的离散嵌入空间中的能力。尽管它潜力巨大，但域泛化冲突会导致难以察觉的陷阱。在本文中，我们实例化了两个陷阱，它们就像是GFM优化硬币的两面——第一面模型退化：编码器和代码本无法捕捉输入的多样性；第二面表示崩溃：隐藏嵌入和代码本向量由于狭窄表示子空间的约束而无法保持语义可分性。这两个陷阱（两面）共同损害了解码器并产生了低质量重建的监督，导致了预训练过程中的GFM优化困境（硬币）。通过实证研究，我们将上述挑战归因于信息瓶颈和正则化缺陷。为了解决这个问题，我们提出了MoT（混合微调器）-（1）针对两个陷阱的信息微调器，它利用边缘语义融合策略和带有域感知路由的混合代码本，以提高信息容量。（2）针对优化硬币的正则化微调器，它利用两种额外的正则化方法，以进一步提高我们提出的信息微调器中的梯度监督。值得注意的是，作为一个灵活的架构，MoT遵循GFM的扩展定律，提供了一个可控的模型规模。在6个领域的22个数据集上的实验表明，与最新技术基准相比，MoT在监督、少样本和零样本场景中实现了显著改进。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.08401v1">PDF</a> </p>
<p><strong>Summary</strong><br>     图模型借鉴大型语言模型的优点，旨在从多域标签学习最优嵌入，以实现下游跨任务的泛化能力。在研究中，图VQ-MAE在众多图模型架构中脱颖而出，它能够从多个领域联合编码拓扑和文本属性到离散嵌入空间，具有清晰的语义边界。然而，域泛化冲突会引发一些隐患。针对这些问题，本文提出了MoT（混合微调）方案，包括针对两个隐患的信息微调器和优化硬币的正则化微调器。MoT遵循图模型的规模定律，提供了一个可控的模型规模。在22个数据集、6个领域的实验中，MoT在监督学习、小样本学习和零样本学习场景下均取得了显著的提升。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>图模型借鉴LLMs成功，旨在从多域标签学习最优嵌入，提高下游任务的泛化能力。</li>
<li>图VQ-MAE能够联合编码拓扑和文本属性到离散嵌入空间。</li>
<li>域的泛化冲突会引发模型隐患，例如模型退化和表示崩溃。</li>
<li>模型退化的原因是编码器和代码本无法捕捉输入的多样性。</li>
<li>表示崩溃是因为隐藏嵌入和代码本向量在狭窄的表示子空间内失去语义可分性。</li>
<li>MoT方案通过信息微调器和优化硬币的正则化微调器来解决上述问题。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.08401">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Few-Shot/2509.08401v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Few-Shot/2509.08401v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Few-Shot/2509.08401v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Few-Shot/2509.08401v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Few-shot-Personalization-via-In-Context-Learning-for-Speech-Emotion-Recognition-based-on-Speech-Language-Model"><a href="#Few-shot-Personalization-via-In-Context-Learning-for-Speech-Emotion-Recognition-based-on-Speech-Language-Model" class="headerlink" title="Few-shot Personalization via In-Context Learning for Speech Emotion   Recognition based on Speech-Language Model"></a>Few-shot Personalization via In-Context Learning for Speech Emotion   Recognition based on Speech-Language Model</h2><p><strong>Authors:Mana Ihori, Taiga Yamane, Naotaka Kawata, Naoki Makishima, Tomohiro Tanaka, Satoshi Suzuki, Shota Orihashi, Ryo Masumura</strong></p>
<p>This paper proposes a personalization method for speech emotion recognition (SER) through in-context learning (ICL). Since the expression of emotions varies from person to person, speaker-specific adaptation is crucial for improving the SER performance. Conventional SER methods have been personalized using emotional utterances of a target speaker, but it is often difficult to prepare utterances corresponding to all emotion labels in advance. Our idea to overcome this difficulty is to obtain speaker characteristics by conditioning a few emotional utterances of the target speaker in ICL-based inference. ICL is a method to perform unseen tasks by conditioning a few input-output examples through inference in large language models (LLMs). We meta-train a speech-language model extended from the LLM to learn how to perform personalized SER via ICL. Experimental results using our newly collected SER dataset demonstrate that the proposed method outperforms conventional methods. </p>
<blockquote>
<p>本文提出了一种通过上下文学习（ICL）进行语音情感识别（SER）个性化方法。由于情感的表达因人而异，因此针对特定说话人的适应对于提高SER性能至关重要。传统SER方法已经通过目标说话人的情感表达进行了个性化处理，但很难提前准备所有情感标签对应的表达。我们的想法是通过在基于ICL的推理中条件化目标说话人的少数情感表达来获得说话人特征，以克服这一难题。ICL是一种通过在大语言模型（LLM）中进行推理来条件化少数输入输出示例以执行未见任务的方法。我们对从LLM扩展的语音语言模型进行元训练，以学习如何通过ICL执行个性化的SER。使用我们新收集的SER数据集进行的实验结果表明，所提出的方法优于传统方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.08344v1">PDF</a> Accepted by ASRU 2025</p>
<p><strong>Summary</strong></p>
<p>本论文提出了一种基于上下文学习（ICL）的个性化语音情感识别（SER）方法。由于情感表达存在个体差异，针对特定说话人的适应对于提高SER性能至关重要。论文通过利用目标说话人的少量情感话语来实现个性化，但预先准备所有情感标签的话语往往很困难。论文通过ICL进行推断，以通过少数输入-输出示例解决未见任务，在大语言模型（LLM）中进行条件训练获得说话人的特性。使用新收集的SER数据集进行的实验结果表明，该方法优于传统方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>论文提出了一种基于上下文学习的个性化语音情感识别方法。</li>
<li>情感表达存在个体差异，因此针对特定说话人的适应很重要。</li>
<li>传统SER方法使用目标说话人的情感话语进行个性化，但预先准备所有情感标签的话语很困难。</li>
<li>通过ICL进行推断，用少数输入-输出示例解决未见任务。</li>
<li>通过在大语言模型上进行条件训练，获得说话人的特性。</li>
<li>新收集的SER数据集上的实验结果表明，该方法的有效性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.08344">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Few-Shot/2509.08344v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Few-Shot/2509.08344v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Few-Shot/2509.08344v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Few-Shot/2509.08344v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Few-Shot/2509.08344v1/page_5_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Few-Shot/2509.08344v1/page_5_2.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Expert-Guided-Explainable-Few-Shot-Learning-for-Medical-Image-Diagnosis"><a href="#Expert-Guided-Explainable-Few-Shot-Learning-for-Medical-Image-Diagnosis" class="headerlink" title="Expert-Guided Explainable Few-Shot Learning for Medical Image Diagnosis"></a>Expert-Guided Explainable Few-Shot Learning for Medical Image Diagnosis</h2><p><strong>Authors:Ifrat Ikhtear Uddin, Longwei Wang, KC Santosh</strong></p>
<p>Medical image analysis often faces significant challenges due to limited expert-annotated data, hindering both model generalization and clinical adoption. We propose an expert-guided explainable few-shot learning framework that integrates radiologist-provided regions-of-interests (ROIs) into model training to simultaneously enhance classification performance and interpretability. Leveraging Grad-CAM for spatial attention supervision, we introduce an explanation loss based on Dice similarity to align model attention with diagnostically relevant regions during training. This explanation loss is jointly optimized with a standard prototypical network objective, encouraging the model to focus on clinically meaningful features even under limited data conditions. We evaluate our framework on two distinct datasets: BraTS (MRI) and VinDr-CXR (Chest X-ray), achieving significant accuracy improvements from 77.09% to 83.61% on BraTS and from 54.33% to 73.29% on VinDr-CXR compared to non-guided models. Grad-CAM visualizations further confirm that expert-guided training consistently aligns attention with diagnostic regions, improving both predictive reliability and clinical trustworthiness. Our findings demonstrate the effectiveness of incorporating expert-guided attention supervision to bridge the gap between performance and interpretability in few-shot medical image diagnosis. </p>
<blockquote>
<p>医疗图像分析经常由于专家标注数据有限而面临重大挑战，这阻碍了模型的通用化和临床采用。我们提出了一种专家引导的可解释的少量学习框架，该框架将放射科医生提供的感兴趣区域（ROI）集成到模型训练中，以同时提高分类性能和解释性。我们借助Grad-CAM进行空间注意力监督，并引入基于Dice相似度的解释损失，以在训练过程中使模型注意力与诊断相关区域对齐。这种解释损失与标准原型网络目标联合优化，鼓励模型即使在数据有限的情况下也关注临床上意义重大的特征。我们在两个不同的数据集BraTS（MRI）和VinDr-CXR（胸部X射线）上评估了我们的框架，与无引导模型相比，BraTS的准确率从77.09%提高到83.61%，VinDr-CXR的准确率从54.33%提高到73.29%。Grad-CAM可视化进一步证实，专家引导的训练始终能使注意力与诊断区域对齐，提高了预测可靠性和临床可信度。我们的研究结果表明，在少量医疗图像诊断中融入专家引导的监督能有效地缩小性能和解释性之间的差距。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.08007v1">PDF</a> Accepted for publication in the proceedings of MICCAI Workshop on   Data Engineering in Medical Imaging 2025</p>
<p><strong>Summary</strong></p>
<p>本文提出一种专家引导的可解释的少量学习框架，该框架将放射科医生提供的感兴趣区域（ROIs）集成到模型训练中，以提高分类性能和解释性。通过利用Grad-CAM进行空间注意力监督，并引入基于Dice相似度的解释损失，使模型注意力与诊断相关区域对齐。该解释损失与标准原型网络目标联合优化，鼓励模型在有限数据条件下关注临床有意义的特征。在BraTS和VinDr-CXR两个数据集上的评估显示，与无引导模型相比，准确率分别提高了从77.09%到83.61%和从54.33%到73.29%。Grad-CAM可视化进一步证实，专家引导的训练能使模型注意力始终与诊断区域对齐，提高了预测可靠性和临床可信度。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>医学图像分析面临专家标注数据有限的挑战，影响模型通用化和临床采纳。</li>
<li>提出一种专家引导的可解释的少量学习框架，结合放射科医生提供的感兴趣区域（ROIs）进行模型训练。</li>
<li>利用Grad-CAM进行空间注意力监督，引入基于Dice相似度的解释损失，使模型关注诊断相关区域。</li>
<li>解释损失与原型网络目标联合优化，提高模型在有限数据下的临床有意义特征关注。</li>
<li>在BraTS和VinDr-CXR数据集上评估，准确率显著提升。</li>
<li>Grad-CAM可视化显示专家引导的训练使模型注意力与诊断区域对齐。</li>
<li>专家引导有助于提高预测可靠性和临床可信度。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.08007">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Few-Shot/2509.08007v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Few-Shot/2509.08007v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="TerraMind-Large-Scale-Generative-Multimodality-for-Earth-Observation"><a href="#TerraMind-Large-Scale-Generative-Multimodality-for-Earth-Observation" class="headerlink" title="TerraMind: Large-Scale Generative Multimodality for Earth Observation"></a>TerraMind: Large-Scale Generative Multimodality for Earth Observation</h2><p><strong>Authors:Johannes Jakubik, Felix Yang, Benedikt Blumenstiel, Erik Scheurer, Rocco Sedona, Stefano Maurogiovanni, Jente Bosmans, Nikolaos Dionelis, Valerio Marsocci, Niklas Kopp, Rahul Ramachandran, Paolo Fraccaro, Thomas Brunschwiler, Gabriele Cavallaro, Juan Bernabe-Moreno, Nicolas Longépé</strong></p>
<p>We present TerraMind, the first any-to-any generative, multimodal foundation model for Earth observation (EO). Unlike other multimodal models, TerraMind is pretrained on dual-scale representations combining both token-level and pixel-level data across modalities. On a token level, TerraMind encodes high-level contextual information to learn cross-modal relationships, while on a pixel level, TerraMind leverages fine-grained representations to capture critical spatial nuances. We pretrained TerraMind on nine geospatial modalities of a global, large-scale dataset. In this paper, we demonstrate that (i) TerraMind’s dual-scale early fusion approach unlocks a range of zero-shot and few-shot applications for Earth observation, (ii) TerraMind introduces “Thinking-in-Modalities” (TiM) – the capability of generating additional artificial data during finetuning and inference to improve the model output – and (iii) TerraMind achieves beyond state-of-the-art performance in community-standard benchmarks for EO like PANGAEA. The pretraining dataset, the model weights, and our code are open-sourced under a permissive license. </p>
<blockquote>
<p>我们推出了TerraMind，这是首个面向地球观测（EO）的任意到任意生成、多模态基础模型。与其他多模态模型不同，TerraMind在双尺度表示上进行预训练，结合了跨模态的令牌级别和像素级别的数据。在令牌级别上，TerraMind编码高级上下文信息以学习跨模态关系，而在像素级别上，TerraMind利用精细表示来捕捉关键的空间细微差别。我们在全球大规模数据集的九个地理空间模态上预训练了TerraMind。在本文中，我们证明了（i）TerraMind的双尺度早期融合方法开启了地球观测的零样本和少样本应用的范围，（ii）TerraMind引入了“模态思考”（TiM）——在微调过程中生成额外的人工数据并在推理期间改善模型输出的能力——以及（iii）TerraMind在像泛非洲地球观测计划这样的社区标准基准测试中实现了超越最新技术的性能。预训练数据集、模型权重和我们的代码都是在许可下开源的。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.11171v4">PDF</a> Accepted at ICCV’25</p>
<p><strong>Summary</strong></p>
<p>TerraMind是首个面向地球观测的任意输入任意输出的生成式、多模态基础模型。它采用双尺度表示法，结合标记级别和像素级别的数据跨模态进行预训练。TerraMind的预训练数据集规模庞大，涵盖全球九大地理空间模态。该模型可实现零样本和少样本应用，引入“模态思考”（TiM）能力，能在微调及推理阶段生成额外的模拟数据，提升模型输出效果，且性能超越地球观测领域的社区标准基准测试。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TerraMind是首个地球观测领域的任意输入任意输出生成式多模态基础模型。</li>
<li>TerraMind采用独特的双尺度预训练方法，结合标记级别和像素级别的数据跨模态进行训练。</li>
<li>模型可以在零样本和少样本场景下应用。</li>
<li>引入“模态思考”（TiM）能力，能在微调及推理阶段生成额外的模拟数据。</li>
<li>TerraMind的预训练数据集规模庞大，涵盖九大地理空间模态。</li>
<li>TerraMind性能超越地球观测领域的社区标准基准测试。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.11171">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Few-Shot/2504.11171v4/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Few-Shot/2504.11171v4/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Few-Shot/2504.11171v4/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Few-Shot/2504.11171v4/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Few-Shot/2504.11171v4/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Few-Shot/2504.11171v4/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Few-Shot/2504.11171v4/page_5_0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Investigating-Compositional-Reasoning-in-Time-Series-Foundation-Models"><a href="#Investigating-Compositional-Reasoning-in-Time-Series-Foundation-Models" class="headerlink" title="Investigating Compositional Reasoning in Time Series Foundation Models"></a>Investigating Compositional Reasoning in Time Series Foundation Models</h2><p><strong>Authors:Willa Potosnak, Cristian Challu, Mononito Goswami, Kin G. Olivares, Michał Wiliński, Nina Żukowska, Artur Dubrawski</strong></p>
<p>Large pre-trained time series foundation models (TSFMs) have demonstrated promising zero-shot performance across a wide range of domains. However, a question remains: Do TSFMs succeed by memorizing patterns in training data, or do they possess the ability to reason about such patterns? While reasoning is a topic of great interest in the study of Large Language Models (LLMs), it is undefined and largely unexplored in the context of TSFMs. In this work, inspired by language modeling literature, we formally define compositional reasoning in forecasting and distinguish it from in-distribution generalization. We evaluate the reasoning and generalization capabilities of 16 popular deep learning forecasting models on multiple synthetic and real-world datasets. Additionally, through controlled studies, we systematically examine which design choices in 7 popular open-source TSFMs contribute to improved reasoning capabilities. Our study yields key insights into the impact of TSFM architecture design on compositional reasoning and generalization. We find that patch-based Transformers have the best reasoning performance, closely followed by residualized MLP-based architectures, which are 97% less computationally complex in terms of FLOPs and 86% smaller in terms of the number of trainable parameters. Interestingly, in some zero-shot out-of-distribution scenarios, these models can outperform moving average and exponential smoothing statistical baselines trained on in-distribution data. Only a few design choices, such as the tokenization method, had a significant (negative) impact on Transformer model performance. </p>
<blockquote>
<p>大型预训练时间序列基础模型（TSFMs）在多个领域都表现出了很有前景的零样本性能。但有一个问题仍然待解：TSFM是通过训练数据中的模式记忆取得成功，还是它们具备对这些模式进行推理的能力？虽然推理是大型语言模型（LLM）研究中的热门话题，但在TSFM的上下文中，它尚未被明确定义和广泛探索。在这项工作中，我们受到语言建模文献的启发，正式定义了预测中的组合推理，并将其与内部分布泛化区分开来。我们评估了16个流行的深度学习预测模型在多个合成和真实数据集上的推理和泛化能力。此外，通过对照研究，我们系统地检查了7个流行开源TSFM中的哪些设计选择有助于改进推理能力。我们的研究对于TSFM架构设计对组合推理和泛化的影响提供了关键见解。我们发现基于补丁的Transformer具有最佳的推理性能，紧随其后的是基于残差MLP的架构，其在浮点运算方面减少了97%，在可训练参数数量方面减少了86%。有趣的是，在某些零样本超出分布场景中，这些模型可以超越在内部分布数据上训练的移动平均和指数平滑统计基线。只有少数设计选择，如令牌化方法，对Transformer模型性能产生了重大（负面）影响。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.06037v2">PDF</a> </p>
<p><strong>Summary</strong><br>     时序序列基础大模型（TSFMs）展现出跨域零样本性能。但问题在于它们是否通过记忆训练数据成功，还是它们具有理解这些模式的能力？本研究受到语言建模文献的启发，正式定义了预测中的组合推理并将其与分布内泛化区分开。我们评估了多个合成和真实数据集上流行的深度学习预测模型的推理和泛化能力。此外，通过受控研究，我们系统地研究了设计选择对TSFMs推理能力的贡献。研究发现，基于补丁的Transformer具有最佳的推理性能，其次是基于残差化的MLP架构，其计算复杂度FLOPs减少了97%，可训练参数减少了86%。在某些零样本分布外场景中，这些模型甚至可以超越在分布内数据上训练的移动平均和指数平滑统计基线。仅少数设计选择如令牌化方法对Transformer模型性能有显著负面影响。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TSFMs展现出跨域零样本性能。</li>
<li>TSFMs的推理能力与记忆训练数据的能力有所区别。</li>
<li>通过研究明确了预测模型中的组合推理和分布内泛化的概念差异。</li>
<li>在多种数据集上的研究显示基于补丁的Transformer具有最佳推理性能。</li>
<li>基于残差化的MLP架构表现优秀，具有较低的计算复杂度和较小的参数规模。</li>
<li>在某些场景下，先进的模型可超越传统统计基线方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.06037">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Few-Shot/2502.06037v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Few-Shot/2502.06037v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Few-Shot/2502.06037v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Few-Shot/2502.06037v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-12/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-12/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-12/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_I2I Translation/2503.09151v3/page_2_0.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation 方向最新论文已更新，请持续关注 Update in 2025-09-12  Reangle-A-Video 4D Video Generation as Video-to-Video Translation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-12
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-12/Agent/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Agent/2506.07392v2/page_4_0.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2025-09-12  Automatic Failure Attribution and Critical Step Prediction Method for   Multi-Agent Systems Based on Causal Inference
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-12
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">29474.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
