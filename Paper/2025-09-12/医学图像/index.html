<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-12  Delving into the depths of NGC 3783 with XRISM II. Cross-calibration of   X-ray instruments used in the large, multi-mission observational campaign">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_åŒ»å­¦å›¾åƒ/2509.08234v1/page_5_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-12
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-07
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    18.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    78 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-09-12-æ›´æ–°"><a href="#2025-09-12-æ›´æ–°" class="headerlink" title="2025-09-12 æ›´æ–°"></a>2025-09-12 æ›´æ–°</h1><h2 id="Delving-into-the-depths-of-NGC-3783-with-XRISM-II-Cross-calibration-of-X-ray-instruments-used-in-the-large-multi-mission-observational-campaign"><a href="#Delving-into-the-depths-of-NGC-3783-with-XRISM-II-Cross-calibration-of-X-ray-instruments-used-in-the-large-multi-mission-observational-campaign" class="headerlink" title="Delving into the depths of NGC 3783 with XRISM II. Cross-calibration of   X-ray instruments used in the large, multi-mission observational campaign"></a>Delving into the depths of NGC 3783 with XRISM II. Cross-calibration of   X-ray instruments used in the large, multi-mission observational campaign</h2><p><strong>Authors: XRISM collaboration</strong></p>
<p>Accurate X-ray spectroscopic measurements are fundamental for deriving basic physical parameters of the most abundant baryon components in the Universe. The plethora of X-ray observatories currently operational enables a panchromatic view of the high-energy emission of celestial sources. However, uncertainties in the energy-dependent calibration of the instrument transfer functions (e.g. the effective area, energy redistribution, or gain) can limit - and historically, did limit - the accuracy of X-ray spectroscopic measurements.   We revised the status of the cross-calibration among the scientific payload on board four operation missions: Chandra, NuSTAR, XMM-Newton, and the recently launched XRISM. XRISM carries the micro-calorimeter Resolve, which yields the best energy resolution at energies above 2 keV. For this purpose, we used the data from a 10-day-long observational campaign targeting the nearby active galactic nucleus NGC 3783, carried out in July 2024.   We present a novel model-independent method for assessing the cross-calibration status that is based on a multi-node spline of the spectra with the highest-resolving power (XRISM&#x2F;Resolve in our campaign). We also estimated the impact of the intrinsic variability of NGC 3783 on the cross-calibration status due to the different time coverages of participating observatories and performed an empirical reassessment of the Resolve throughput at low energies.   Based on this analysis, we derived a set of energy-dependent correction factors of the observed responses, enabling a statistically robust analysis of the whole spectral dataset. They will be employed in subsequent papers describing the astrophysical results of the campaign. </p>
<blockquote>
<p>å‡†ç¡®çš„Xå°„çº¿å…‰è°±æµ‹é‡å¯¹äºæ¨å¯¼å®‡å®™ä¸­ä¸°åº¦æœ€é«˜çš„é‡å­æˆåˆ†çš„åŸºæœ¬ç‰©ç†å‚æ•°è‡³å…³é‡è¦ã€‚å½“å‰è¿è¥çš„ä¼—å¤šXå°„çº¿å¤©æ–‡å°èƒ½å¤Ÿå¯¹å¤©ä½“æºçš„é«˜èƒ½å‘å°„è¿›è¡Œå…¨å…‰è°±è§‚å¯Ÿã€‚ç„¶è€Œï¼Œä»ªå™¨ä¼ é€’å‡½æ•°ï¼ˆä¾‹å¦‚æœ‰æ•ˆé¢ç§¯ã€èƒ½é‡é‡æ–°åˆ†é…æˆ–å¢ç›Šï¼‰çš„èƒ½æºä¾èµ–æ€§æ ¡å‡†ä¸­çš„ä¸ç¡®å®šæ€§å¯èƒ½ä¼šé™åˆ¶ï¼ˆå†å²ä¸Šä¹Ÿç¡®å®é™åˆ¶è¿‡ï¼‰Xå°„çº¿å…‰è°±æµ‹é‡çš„å‡†ç¡®æ€§ã€‚æˆ‘ä»¬å›é¡¾äº†å››ä¸ªè¿è¥ä»»åŠ¡ä¸­ç§‘å­¦æœ‰æ•ˆè½½è·ä¹‹é—´çš„äº¤å‰æ ¡å‡†æƒ…å†µï¼šé’±å¾·æ‹‰ã€NuSTARã€XMM-ç‰›é¡¿å’Œæœ€è¿‘å‘å°„çš„XRISMã€‚XRISMæºå¸¦å¾®å‹çƒ­é‡è®¡Resolveï¼Œåœ¨é«˜äº2åƒç”µå­ä¼çš„èƒ½é‡ä¸‹å…·æœ‰æœ€ä½³èƒ½é‡åˆ†è¾¨ç‡ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†äº2024å¹´7æœˆå¼€å±•çš„é’ˆå¯¹é™„è¿‘æ´»åŠ¨æ˜Ÿç³»æ ¸NGC 3783çš„ä¸ºæœŸ10å¤©çš„è§‚æµ‹æ´»åŠ¨çš„æ•°æ®ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„ç‹¬ç«‹äºæ¨¡å‹ä¹‹å¤–çš„è¯„ä¼°äº¤å‰æ ¡å‡†çŠ¶å†µçš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åŸºäºå…·æœ‰æœ€é«˜åˆ†è¾¨ç‡ï¼ˆæˆ‘ä»¬çš„æ´»åŠ¨ä¸­çš„XRISM&#x2F;Resolveï¼‰çš„å¤šèŠ‚ç‚¹å…‰è°±æ’å€¼ã€‚æˆ‘ä»¬è¿˜ä¼°è®¡äº†ç”±äºå‚ä¸è§‚æµ‹çš„å¤©æ–‡å°çš„ä¸åŒçš„æ—¶é—´è¦†ç›–èŒƒå›´ï¼ŒNGC 3783çš„å†…åœ¨å˜åŒ–å¯¹äº¤å‰æ ¡å‡†çŠ¶æ€çš„å½±å“ï¼Œå¹¶å¯¹Resolveåœ¨ä½èƒ½ä¸‹çš„ååé‡è¿›è¡Œäº†ç»éªŒé‡æ–°è¯„ä¼°ã€‚åŸºäºè¿™ä¸€åˆ†æï¼Œæˆ‘ä»¬å¾—å‡ºäº†ä¸€ç»„è§‚å¯Ÿåˆ°çš„å“åº”çš„èƒ½é‡ç›¸å…³æ ¡æ­£å› å­ï¼Œèƒ½å¤Ÿå¯¹æ•´ä¸ªå…‰è°±æ•°æ®é›†è¿›è¡Œç»Ÿè®¡ç¨³å¥çš„åˆ†æã€‚ä»–ä»¬å°†åœ¨åç»­è®ºæ–‡ä¸­ç”¨äºæè¿°æ´»åŠ¨çš„å¤©æ–‡ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.08649v1">PDF</a> 12 pages, 12 figures, Astronomy &amp; Astrophysics, accepted for   publication</p>
<p><strong>æ‘˜è¦</strong></p>
<p>Xå°„çº¿å…‰è°±æµ‹é‡çš„å‡†ç¡®æ€§å¯¹äºæ¨å¯¼å®‡å®™ä¸­æœ€å¤šå·´åŸå­æˆåˆ†çš„åŸºæœ¬ç‰©ç†å‚æ•°è‡³å…³é‡è¦ã€‚å½“å‰è¿è¥çš„ä¼—å¤šXå°„çº¿è§‚æµ‹ç«™èƒ½å¤Ÿå®ç°å…¨å¤©å€™é«˜èƒ½é‡å‘å°„çš„è§‚æµ‹ã€‚ç„¶è€Œï¼Œä»ªå™¨ä¼ è¾“åŠŸèƒ½çš„èƒ½é‡ç›¸å…³æ ¡å‡†ï¼ˆå¦‚æœ‰æ•ˆé¢ç§¯ã€èƒ½é‡é‡æ–°åˆ†é…æˆ–å¢ç›Šï¼‰çš„ä¸ç¡®å®šæ€§å¯èƒ½ä¼šé™åˆ¶Xå°„çº¿å…‰è°±æµ‹é‡çš„å‡†ç¡®æ€§ã€‚æœ¬æ–‡å›é¡¾äº†å››ä¸ªè¿è¡Œä»»åŠ¡ä¸­ç§‘å­¦è½½è·ä¹‹é—´çš„äº¤å‰æ ¡å‡†çŠ¶æ€ï¼šChandraã€NuSTARã€XMM-Newtonä»¥åŠæœ€è¿‘å‘å°„çš„XRISMã€‚XRISMæºå¸¦æœ‰é«˜åˆ†è¾¨ç‡å¾®é‡çƒ­é‡è®¡Resolveï¼Œå¯åœ¨é«˜äº2 keVçš„èƒ½é‡ä¸‹å®ç°æœ€ä½³èƒ½é‡åˆ†è¾¨ç‡ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†é’ˆå¯¹é™„è¿‘æ´»åŠ¨æ˜Ÿç³»æ ¸NGC 3783çš„ä¸ºæœŸ10å¤©çš„è§‚æµ‹æ´»åŠ¨çš„æ•°æ®ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹ç‹¬ç«‹äºæ¨¡å‹çš„æ–¹æ³•ï¼Œç”¨ä»¥è¯„ä¼°äº¤å‰æ ¡å‡†çŠ¶æ€ï¼Œè¯¥æ–¹æ³•åŸºäºå…·æœ‰æœ€é«˜åˆ†è¾¨ç‡çš„å¤šèŠ‚ç‚¹è°±çº¿æ’å€¼ï¼ˆæˆ‘ä»¬çš„æ´»åŠ¨ä¸­çš„XRISM&#x2F;Resolveï¼‰ã€‚æˆ‘ä»¬è¿˜ä¼°è®¡äº†ç”±äºå‚ä¸è§‚æµ‹ç«™çš„ä¸åŒæ—¶é—´è¦†ç›–ï¼ŒNGC 3783çš„å†…åœ¨å˜åŒ–å¯¹äº¤å‰æ ¡å‡†çŠ¶æ€çš„å½±å“ï¼Œå¹¶å¯¹Resolveåœ¨ä½èƒ½é‡çš„é€šè¿‡ç‡è¿›è¡Œäº†ç»éªŒå†è¯„ä¼°ã€‚åŸºäºè¿™é¡¹åˆ†æï¼Œæˆ‘ä»¬å¾—åˆ°äº†ä¸€ç³»åˆ—èƒ½é‡ç›¸å…³çš„æ ¡æ­£å› å­ï¼Œå¯¹è§‚æµ‹ååº”è¿›è¡Œæ ¡æ­£ï¼Œå®ç°å¯¹æ•´ä¸ªå…‰è°±æ•°æ®é›†çš„ç»Ÿè®¡ç¨³å¥åˆ†æã€‚è¿™äº›æ ¡æ­£å› å­å°†åœ¨åç»­æè¿°æ´»åŠ¨æˆæœçš„è®ºæ–‡ä¸­ä½¿ç”¨ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>Xå°„çº¿å…‰è°±æµ‹é‡çš„å‡†ç¡®æ€§å¯¹äºæ¨å¯¼å®‡å®™åŸºæœ¬ç‰©ç†å‚æ•°è‡³å…³é‡è¦ã€‚</li>
<li>å½“å‰è¿è¥çš„Xå°„çº¿è§‚æµ‹ç«™å®ç°äº†å¯¹å¤©ä½“é«˜èƒ½é‡å‘å°„çš„å…¨æ–¹ä½è§‚æµ‹ã€‚</li>
<li>ä»ªå™¨ä¼ è¾“åŠŸèƒ½çš„èƒ½é‡ç›¸å…³æ ¡å‡†çš„ä¸ç¡®å®šæ€§å¯èƒ½å½±å“Xå°„çº¿å…‰è°±æµ‹é‡çš„å‡†ç¡®æ€§ã€‚</li>
<li>XRISMå’Œå…¶é«˜åˆ†è¾¨ç‡å¾®é‡çƒ­é‡è®¡Resolveåœ¨Xå°„çº¿å…‰è°±æµ‹é‡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>ä½¿ç”¨æ–°å‹ç‹¬ç«‹äºæ¨¡å‹çš„æ–¹æ³•è¯„ä¼°äº¤å‰æ ¡å‡†çŠ¶æ€ï¼Œä»¥æ›´å‡†ç¡®åœ°æµ‹é‡Xå°„çº¿å…‰è°±ã€‚</li>
<li>NGC 3783çš„å†…åœ¨å˜åŒ–å¯¹äº¤å‰æ ¡å‡†çŠ¶æ€æœ‰å½±å“ï¼Œå°¤å…¶æ˜¯åœ¨ä¸åŒæ—¶é—´è¦†ç›–çš„è§‚æµ‹æ´»åŠ¨ä¸­ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.08649">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_åŒ»å­¦å›¾åƒ/2509.08649v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_åŒ»å­¦å›¾åƒ/2509.08649v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_åŒ»å­¦å›¾åƒ/2509.08649v1/page_1_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_åŒ»å­¦å›¾åƒ/2509.08649v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_åŒ»å­¦å›¾åƒ/2509.08649v1/page_5_0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="RoentMod-A-Synthetic-Chest-X-Ray-Modification-Model-to-Identify-and-Correct-Image-Interpretation-Model-Shortcuts"><a href="#RoentMod-A-Synthetic-Chest-X-Ray-Modification-Model-to-Identify-and-Correct-Image-Interpretation-Model-Shortcuts" class="headerlink" title="RoentMod: A Synthetic Chest X-Ray Modification Model to Identify and   Correct Image Interpretation Model Shortcuts"></a>RoentMod: A Synthetic Chest X-Ray Modification Model to Identify and   Correct Image Interpretation Model Shortcuts</h2><p><strong>Authors:Lauren H. Cooke, Matthias Jung, Jan M. Brendel, Nora M. Kerkovits, Borek Foldyna, Michael T. Lu, Vineet K. Raghu</strong></p>
<p>Chest radiographs (CXRs) are among the most common tests in medicine. Automated image interpretation may reduce radiologists&#39; workload and expand access to diagnostic expertise. Deep learning multi-task and foundation models have shown strong performance for CXR interpretation but are vulnerable to shortcut learning, where models rely on spurious and off-target correlations rather than clinically relevant features to make decisions. We introduce RoentMod, a counterfactual image editing framework that generates anatomically realistic CXRs with user-specified, synthetic pathology while preserving unrelated anatomical features of the original scan. RoentMod combines an open-source medical image generator (RoentGen) with an image-to-image modification model without requiring retraining. In reader studies with board-certified radiologists and radiology residents, RoentMod-produced images appeared realistic in 93% of cases, correctly incorporated the specified finding in 89-99% of cases, and preserved native anatomy comparable to real follow-up CXRs. Using RoentMod, we demonstrate that state-of-the-art multi-task and foundation models frequently exploit off-target pathology as shortcuts, limiting their specificity. Incorporating RoentMod-generated counterfactual images during training mitigated this vulnerability, improving model discrimination across multiple pathologies by 3-19% AUC in internal validation and by 1-11% for 5 out of 6 tested pathologies in external testing. These findings establish RoentMod as a broadly applicable tool for probing and correcting shortcut learning in medical AI. By enabling controlled counterfactual interventions, RoentMod enhances the robustness and interpretability of CXR interpretation models and provides a generalizable strategy for improving foundation models in medical imaging. </p>
<blockquote>
<p>èƒ¸éƒ¨Xå°„çº¿ï¼ˆCXRï¼‰æ˜¯åŒ»å­¦ä¸­æœ€å¸¸è§çš„æ£€æŸ¥ä¹‹ä¸€ã€‚è‡ªåŠ¨å›¾åƒè§£è¯»å¯ä»¥å‡å°‘æ”¾å°„ç§‘åŒ»å¸ˆçš„å·¥ä½œé‡ï¼Œå¹¶æ‰©å¤§è¯Šæ–­ä¸“ä¸šçš„å¯åŠæ€§ã€‚æ·±åº¦å­¦ä¹ å¤šä»»åŠ¡æ¨¡å‹å’ŒåŸºç¡€æ¨¡å‹åœ¨CXRè§£è¯»æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†å®¹æ˜“é™·å…¥æ·å¾„å­¦ä¹ ï¼Œå³æ¨¡å‹ä¾èµ–äºå¶ç„¶å’Œåç¦»ç›®æ ‡çš„å…³è”ï¼Œè€Œä¸æ˜¯ä¸´åºŠç›¸å…³çš„ç‰¹å¾æ¥åšå‡ºå†³ç­–ã€‚æˆ‘ä»¬å¼•å…¥äº†RoentModï¼Œè¿™æ˜¯ä¸€ç§åŸºäºåäº‹å®çš„å›¾åƒç¼–è¾‘æ¡†æ¶ï¼Œèƒ½å¤Ÿç”Ÿæˆå…·æœ‰ç”¨æˆ·æŒ‡å®šåˆæˆç—…ç†çš„è§£å‰–ç»“æ„çœŸå®çš„CXRå›¾åƒï¼ŒåŒæ—¶ä¿ç•™åŸå§‹æ‰«æä¸­æ— å…³è§£å‰–ç‰¹å¾ã€‚RoentModç»“åˆäº†å¼€æºåŒ»å­¦å›¾åƒç”Ÿæˆå™¨ï¼ˆRoentGenï¼‰å’Œå›¾åƒåˆ°å›¾åƒçš„ä¿®æ”¹æ¨¡å‹ï¼Œæ— éœ€é‡æ–°è®­ç»ƒã€‚åœ¨ä¸è®¤è¯æ”¾å°„ç§‘åŒ»å¸ˆå’Œæ”¾å°„ç§‘ä½é™¢åŒ»å¸ˆçš„è¯»è€…ç ”ç©¶ä¸­ï¼ŒRoentModç”Ÿæˆçš„å›¾åƒåœ¨93%çš„æƒ…å†µä¸‹çœ‹èµ·æ¥å¾ˆçœŸå®ï¼Œåœ¨89-99%çš„æƒ…å†µä¸‹æ­£ç¡®çº³å…¥äº†æŒ‡å®šçš„å‘ç°ç‰©ï¼Œå¹¶ä¸”ä¿ç•™çš„åŸå§‹è§£å‰–ç»“æ„ä¸çœŸå®çš„åç»­CXRç›¸å½“ã€‚æˆ‘ä»¬ä½¿ç”¨RoentModè¯æ˜ï¼Œæœ€å…ˆè¿›çš„å¤šä»»åŠ¡æ¨¡å‹å’ŒåŸºç¡€æ¨¡å‹ç»å¸¸åˆ©ç”¨åç¦»ç›®æ ‡çš„ç—…ç†ä½œä¸ºæ·å¾„ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬çš„ç‰¹å¼‚æ€§ã€‚åœ¨åŸ¹è®­è¿‡ç¨‹ä¸­èå…¥RoentModç”Ÿæˆçš„åäº‹å®å›¾åƒå‡è½»äº†è¿™ä¸€æ¼æ´ï¼Œé€šè¿‡å†…éƒ¨éªŒè¯å’Œå¤–éƒ¨æµ‹è¯•ï¼Œæ¨¡å‹åœ¨å¤šç—…ç†æ–¹é¢çš„è¾¨åˆ«èƒ½åŠ›æé«˜äº†3-19%çš„AUCï¼ˆæ›²çº¿ä¸‹é¢ç§¯ï¼‰ä»¥åŠå¤–éƒ¨æµ‹è¯•ä¸­æµ‹è¯•çš„å…­ç§ç—…ç†ä¸­çš„äº”ç§æé«˜äº†1-11%çš„AUCã€‚è¿™äº›å‘ç°ç¡®ç«‹äº†RoentModä½œä¸ºä¸€ç§å¹¿æ³›åº”ç”¨äºæ¢ç´¢å’Œæ”¹è¿›åŒ»ç–—äººå·¥æ™ºèƒ½æ·å¾„å­¦ä¹ çš„å·¥å…·ã€‚é€šè¿‡å®ç°å—æ§çš„åäº‹å®å¹²é¢„ï¼ŒRoentModå¢å¼ºäº†CXRè§£è¯»æ¨¡å‹çš„ç¨³å¥æ€§å’Œå¯è§£é‡Šæ€§ï¼Œå¹¶æä¾›äº†ä¸€ç§é€šç”¨çš„ç­–ç•¥æ¥æ”¹å–„åŒ»å­¦æˆåƒä¸­çš„åŸºç¡€æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.08640v1">PDF</a> 25 + 8 pages, 4 + 7 figures</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºRoentModçš„åŒ»å­¦å›¾åƒç¼–è¾‘æ¡†æ¶ï¼Œç”¨äºç”Ÿæˆå…·æœ‰ç”¨æˆ·æŒ‡å®šåˆæˆç—…ç†çš„è§£å‰–å­¦çœŸå®èƒ¸ç‰‡ã€‚è¯¥æ¡†æ¶ç»“åˆäº†å¼€æºåŒ»å­¦å›¾åƒç”Ÿæˆå™¨RoentGenå’Œä¸€ä¸ªå›¾åƒåˆ°å›¾åƒçš„ä¿®æ”¹æ¨¡å‹ï¼Œæ— éœ€é‡æ–°è®­ç»ƒã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼ŒRoentModç”Ÿæˆçš„å›¾åƒåœ¨å¤§å¤šæ•°ç—…ä¾‹ä¸­çœ‹èµ·æ¥éå¸¸çœŸå®ï¼Œèƒ½æ­£ç¡®èå…¥æŒ‡å®šçš„å‘ç°ï¼Œå¹¶ä¿ç•™ä¸åŸæ‰«ææ— å…³çš„åŸç”Ÿè§£å‰–ç»“æ„ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶è¿˜æ­ç¤ºäº†æœ€å…ˆè¿›çš„å¤šä»»åŠ¡å’ŒåŸºç¡€æ¨¡å‹ç»å¸¸åˆ©ç”¨éç›®æ ‡ç—…ç†ä½œä¸ºæ·å¾„ï¼Œé™åˆ¶äº†å…¶ç‰¹å¼‚æ€§ã€‚åœ¨è®­ç»ƒä¸­èå…¥RoentModç”Ÿæˆçš„åäº‹å®å›¾åƒç¼“è§£äº†è¿™ä¸€æ¼æ´ï¼Œæé«˜äº†æ¨¡å‹å¯¹å¤šç§ç—…ç†çš„è¾¨åˆ«èƒ½åŠ›ã€‚æ€»çš„æ¥è¯´ï¼ŒRoentModä½œä¸ºä¸€ç§å¹¿æ³›åº”ç”¨äºåŒ»å­¦äººå·¥æ™ºèƒ½æ¢æµ‹å’Œçº æ­£æ·å¾„å­¦ä¹ çš„å·¥å…·ï¼Œé€šè¿‡å®ç°å—æ§çš„åäº‹å®å¹²é¢„ï¼Œæé«˜äº†èƒ¸ç‰‡è§£è¯»æ¨¡å‹çš„ç¨³å¥æ€§å’Œå¯è§£é‡Šæ€§ï¼Œå¹¶ä¸ºæ”¹å–„åŒ»å­¦æˆåƒåŸºç¡€æ¨¡å‹æä¾›äº†å¯æ¨å¹¿çš„ç­–ç•¥ã€‚</p>
<p><strong>å…³é”®è¦ç‚¹</strong></p>
<ol>
<li>RoentModæ˜¯ä¸€ä¸ªç”¨äºç”Ÿæˆå…·æœ‰ç”¨æˆ·æŒ‡å®šåˆæˆç—…ç†çš„è§£å‰–å­¦çœŸå®èƒ¸ç‰‡çš„æ¡†æ¶ã€‚</li>
<li>RoentModç»“åˆäº†RoentGenåŒ»å­¦å›¾åƒç”Ÿæˆå™¨å’Œæ— éœ€é‡æ–°è®­ç»ƒçš„å›¾åƒä¿®æ”¹æ¨¡å‹ã€‚</li>
<li>RoentModç”Ÿæˆçš„å›¾åƒåœ¨è¯»è€…ç ”ç©¶ä¸­è¢«è®¤è¯ä¸ºçœŸå®ï¼Œå¹¶æ­£ç¡®èå…¥äº†æŒ‡å®šçš„å‘ç°ã€‚</li>
<li>æœ€å…ˆè¿›çš„å¤šä»»åŠ¡å’ŒåŸºç¡€æ¨¡å‹åœ¨åŒ»å­¦å›¾åƒè§£è¯»ä¸­å­˜åœ¨åˆ©ç”¨éç›®æ ‡ç—…ç†ä½œä¸ºæ·å¾„çš„é—®é¢˜ã€‚</li>
<li>èå…¥RoentModç”Ÿæˆçš„åäº‹å®å›¾åƒè®­ç»ƒç¼“è§£äº†æ¨¡å‹å¯¹æ·å¾„çš„ä¾èµ–ï¼Œæé«˜äº†æ¨¡å‹çš„è¾¨åˆ«èƒ½åŠ›ã€‚</li>
<li>RoentModå¢å¼ºäº†æ¨¡å‹çš„ç¨³å¥æ€§å’Œå¯è§£é‡Šæ€§ï¼Œä¸ºæ”¹å–„åŒ»å­¦æˆåƒåŸºç¡€æ¨¡å‹æä¾›äº†ç­–ç•¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.08640">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_åŒ»å­¦å›¾åƒ/2509.08640v1/page_0_0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Implicit-Shape-Prior-for-Few-Shot-Assisted-3D-Segmentation"><a href="#Implicit-Shape-Prior-for-Few-Shot-Assisted-3D-Segmentation" class="headerlink" title="Implicit Shape-Prior for Few-Shot Assisted 3D Segmentation"></a>Implicit Shape-Prior for Few-Shot Assisted 3D Segmentation</h2><p><strong>Authors:Mathilde Monvoisin, Louise Piecuch, Blanche Texier, CÃ©dric HÃ©mon, AnaÃ¯s Barateau, JÃ©rÃ©mie Huet, Antoine Nordez, Anne-Sophie Boureau, Jean-Claude Nunes, Diana Mateus</strong></p>
<p>The objective of this paper is to significantly reduce the manual workload required from medical professionals in complex 3D segmentation tasks that cannot be yet fully automated. For instance, in radiotherapy planning, organs at risk must be accurately identified in computed tomography (CT) or magnetic resonance imaging (MRI) scans to ensure they are spared from harmful radiation. Similarly, diagnosing age-related degenerative diseases such as sarcopenia, which involve progressive muscle volume loss and strength, is commonly based on muscular mass measurements often obtained from manual segmentation of medical volumes. To alleviate the manual-segmentation burden, this paper introduces an implicit shape prior to segment volumes from sparse slice manual annotations generalized to the multi-organ case, along with a simple framework for automatically selecting the most informative slices to guide and minimize the next interactions. The experimental validation shows the methodâ€™s effectiveness on two medical use cases: assisted segmentation in the context of at risks organs for brain cancer patients, and acceleration of the creation of a new database with unseen muscle shapes for patients with sarcopenia. </p>
<blockquote>
<p>æœ¬æ–‡çš„ç›®æ ‡æ˜¯ä¸ºäº†æ˜¾è‘—å‡å°‘åœ¨å¤æ‚çš„3Dåˆ†å‰²ä»»åŠ¡ä¸­ï¼ŒåŒ»å­¦ä¸“ä¸šäººå£«æ‰€éœ€çš„æ‰‹åŠ¨å·¥ä½œé‡ï¼Œè¿™äº›ä»»åŠ¡ç›®å‰è¿˜ä¸èƒ½å®Œå…¨è‡ªåŠ¨åŒ–ã€‚ä¾‹å¦‚ï¼Œåœ¨æ”¾å°„æ²»ç–—è®¡åˆ’ä¸­ï¼Œå¿…é¡»åœ¨è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰æˆ–ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰æ‰«æä¸­å‡†ç¡®è¯†åˆ«å‡ºæœ‰é£é™©çš„å™¨å®˜ï¼Œä»¥ç¡®ä¿è¿™äº›å™¨å®˜å…å—æœ‰å®³è¾å°„çš„å½±å“ã€‚åŒæ ·ï¼Œè¯Šæ–­ä¸è‚Œè‚‰ä½“ç§¯å‡å°‘å’ŒåŠ›é‡å‡å¼±æœ‰å…³çš„å¹´é¾„ç›¸å…³æ€§é€€åŒ–ç–¾ç—…ï¼Œå¦‚è‚Œå°‘ç—‡ï¼Œé€šå¸¸åŸºäºä»æ‰‹åŠ¨åˆ†å‰²åŒ»å­¦ä½“ç§¯æ‰€è·å¾—çš„è‚Œè‚‰è´¨é‡æµ‹é‡ã€‚ä¸ºäº†å‡è½»æ‰‹åŠ¨åˆ†å‰²çš„è´Ÿæ‹…ï¼Œæœ¬æ–‡å¼•å…¥äº†ä¸€ç§éšå¼å½¢çŠ¶å…ˆéªŒï¼Œä»ç¨€ç–åˆ‡ç‰‡æ‰‹åŠ¨æ³¨é‡Šä¸­åˆ†å‰²ä½“ç§¯ï¼Œå¹¶å°†å…¶æ¨å¹¿åˆ°å¤šå™¨å®˜æƒ…å†µï¼Œä»¥åŠä¸€ä¸ªç®€å•æ¡†æ¶ï¼Œç”¨äºè‡ªåŠ¨é€‰æ‹©ä¿¡æ¯é‡æœ€å¤§çš„åˆ‡ç‰‡æ¥æŒ‡å¯¼å’Œæœ€å°åŒ–ä¸‹ä¸€æ¬¡äº¤äº’ã€‚å®éªŒéªŒè¯æ˜¾ç¤ºè¯¥æ–¹æ³•åœ¨ä¸¤ç§åŒ»å­¦ç”¨ä¾‹ä¸­çš„æœ‰æ•ˆæ€§ï¼šè¾…åŠ©åˆ†å‰²è„‘ç™Œæ‚£è€…é£é™©å™¨å®˜çš„æƒ…å¢ƒï¼Œä»¥åŠåŠ é€Ÿä¸ºè‚Œå°‘ç—‡æ‚£è€…åˆ›å»ºåŒ…å«æœªè§è‚Œè‚‰å½¢çŠ¶çš„æ–°æ•°æ®åº“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.08580v1">PDF</a> Both first Authors contributed equally to this work, lastnames in   alphabetical order. This preprint has not undergone peer review or any   post-submission improvements or corrections. The Version of Record of this   contribution will be published in a Springer Nature Computer Science book   series (CCIS, LNAI, LNBI, LNBIP, LNCS) and the doi will soon be released</p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒåˆ†å‰²ç ”ç©¶æ—¨åœ¨é™ä½ä¸“ä¸šäººå£«åœ¨å¤æ‚ä¸‰ç»´åˆ†å‰²ä»»åŠ¡ä¸­çš„å·¥ä½œé‡ï¼Œæ— æ³•å…¨è‡ªåŠ¨æ‰§è¡Œçš„ä»»åŠ¡é€šè¿‡å¼•å…¥éšå¼å½¢çŠ¶å…ˆéªŒå®ç°è‡ªåŠ¨åŒ–ã€‚è®ºæ–‡ä»‹ç»äº†åŸºäºç¨€ç–åˆ‡ç‰‡æ‰‹åŠ¨æ³¨é‡Šçš„åˆ†å‰²æ–¹æ³•ï¼Œå¹¶å°†å…¶æ¨å¹¿åˆ°å¤šå™¨å®˜é¢†åŸŸï¼Œå¹¶æ„å»ºäº†ä¸€ä¸ªç®€å•çš„æ¡†æ¶è‡ªåŠ¨é€‰æ‹©æœ€å…·ä¿¡æ¯é‡çš„åˆ‡ç‰‡ä½œä¸ºæŒ‡å¯¼å¹¶æœ€å°åŒ–åç»­äº¤äº’ã€‚å®éªŒéªŒè¯è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é£é™©å™¨å®˜åˆ†å‰²å’Œè‚Œè‚‰å½¢çŠ¶æ•°æ®åº“åˆ›å»ºç­‰åŒ»å­¦åº”ç”¨åœºæ™¯ä¸­æœ‰æ•ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>è¯¥è®ºæ–‡æ—¨åœ¨å‡å°‘åŒ»å­¦ä¸“ä¸šäººå£«åœ¨å¤æ‚ä¸‰ç»´åˆ†å‰²ä»»åŠ¡ä¸­çš„å·¥ä½œé‡ã€‚</li>
<li>å¼•å…¥éšå¼å½¢çŠ¶å…ˆéªŒï¼Œé€šè¿‡è‡ªåŠ¨åŒ–æ–¹å¼å®ç°éš¾ä»¥å®Œå…¨è‡ªåŠ¨åŒ–çš„ä»»åŠ¡ã€‚</li>
<li>æå‡ºä¸€ç§åŸºäºç¨€ç–åˆ‡ç‰‡æ‰‹åŠ¨æ³¨é‡Šçš„åˆ†å‰²æ–¹æ³•ï¼Œé€‚ç”¨äºå¤šå™¨å®˜åˆ†å‰²ã€‚</li>
<li>æ„å»ºäº†ä¸€ä¸ªç®€å•çš„æ¡†æ¶è‡ªåŠ¨é€‰æ‹©æœ€å…·ä¿¡æ¯é‡çš„åˆ‡ç‰‡æ¥æŒ‡å¯¼å¹¶æœ€å°åŒ–åç»­äº¤äº’ã€‚</li>
<li>å®éªŒéªŒè¯æ˜¾ç¤ºè¯¥æ–¹æ³•åœ¨é£é™©å™¨å®˜åˆ†å‰²å’Œè‚Œè‚‰å½¢çŠ¶æ•°æ®åº“åˆ›å»ºç­‰åŒ»å­¦åº”ç”¨åœºæ™¯ä¸­æœ‰æ•ˆã€‚</li>
<li>è¯¥æ–¹æ³•å¯ç”¨äºè¾…åŠ©æ”¾ç–—è®¡åˆ’ä¸­çš„é£é™©å™¨å®˜åˆ†å‰²ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.08580">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_åŒ»å­¦å›¾åƒ/2509.08580v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_åŒ»å­¦å›¾åƒ/2509.08580v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_åŒ»å­¦å›¾åƒ/2509.08580v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Vision-Language-Semantic-Aggregation-Leveraging-Foundation-Model-for-Generalizable-Medical-Image-Segmentation"><a href="#Vision-Language-Semantic-Aggregation-Leveraging-Foundation-Model-for-Generalizable-Medical-Image-Segmentation" class="headerlink" title="Vision-Language Semantic Aggregation Leveraging Foundation Model for   Generalizable Medical Image Segmentation"></a>Vision-Language Semantic Aggregation Leveraging Foundation Model for   Generalizable Medical Image Segmentation</h2><p><strong>Authors:Wenjun Yu, Yinchen Zhou, Jia-Xuan Jiang, Shubin Zeng, Yuee Li, Zhong Wang</strong></p>
<p>Multimodal models have achieved remarkable success in natural image segmentation, yet they often underperform when applied to the medical domain. Through extensive study, we attribute this performance gap to the challenges of multimodal fusion, primarily the significant semantic gap between abstract textual prompts and fine-grained medical visual features, as well as the resulting feature dispersion. To address these issues, we revisit the problem from the perspective of semantic aggregation. Specifically, we propose an Expectation-Maximization (EM) Aggregation mechanism and a Text-Guided Pixel Decoder. The former mitigates feature dispersion by dynamically clustering features into compact semantic centers to enhance cross-modal correspondence. The latter is designed to bridge the semantic gap by leveraging domain-invariant textual knowledge to effectively guide deep visual representations. The synergy between these two mechanisms significantly improves the modelâ€™s generalization ability. Extensive experiments on public cardiac and fundus datasets demonstrate that our method consistently outperforms existing SOTA approaches across multiple domain generalization benchmarks. </p>
<blockquote>
<p>å¤šæ¨¡æ€æ¨¡å‹åœ¨è‡ªç„¶å›¾åƒåˆ†å‰²æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œä½†å½“åº”ç”¨äºåŒ»å­¦é¢†åŸŸæ—¶ï¼Œå…¶æ€§èƒ½å¾€å¾€ä¸ä½³ã€‚é€šè¿‡æ·±å…¥ç ”ç©¶ï¼Œæˆ‘ä»¬å°†è¿™ç§æ€§èƒ½å·®è·å½’å› äºå¤šæ¨¡æ€èåˆçš„æŒ‘æˆ˜ï¼Œä¸»è¦æ˜¯æŠ½è±¡çš„æ–‡æœ¬æç¤ºå’Œç²¾ç»†çš„åŒ»å­¦è§†è§‰ç‰¹å¾ä¹‹é—´çš„è¯­ä¹‰å·®è·ï¼Œä»¥åŠç”±æ­¤å¯¼è‡´çš„ç‰¹å¾åˆ†æ•£ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬ä»è¯­ä¹‰èšåˆçš„è§’åº¦é‡æ–°å®¡è§†é—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æœŸæœ›æœ€å¤§åŒ–ï¼ˆEMï¼‰èšåˆæœºåˆ¶å’Œæ–‡æœ¬å¼•å¯¼åƒç´ è§£ç å™¨ã€‚å‰è€…é€šè¿‡åŠ¨æ€åœ°å°†ç‰¹å¾èšç±»åˆ°ç´§å‡‘çš„è¯­ä¹‰ä¸­å¿ƒï¼Œå¢å¼ºè·¨æ¨¡æ€å¯¹åº”æ€§ï¼Œä»è€Œç¼“è§£ç‰¹å¾åˆ†æ•£é—®é¢˜ã€‚åè€…åˆ™åˆ©ç”¨é¢†åŸŸä¸å˜çš„æ–‡æœ¬çŸ¥è¯†æ¥æœ‰æ•ˆåœ°å¼•å¯¼æ·±åº¦è§†è§‰è¡¨å¾ï¼Œä»è€Œç¼©å°è¯­ä¹‰å·®è·ã€‚è¿™ä¸¤ç§æœºåˆ¶çš„ååŒä½œç”¨æ˜¾è‘—æé«˜äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚åœ¨å…¬å…±å¿ƒè„å’Œçœ¼åº•æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªåŸŸæ³›åŒ–åŸºå‡†æµ‹è¯•ä¸Šå§‹ç»ˆä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.08570v1">PDF</a> 29 pages and 8 figures</p>
<p><strong>Summary</strong><br>å¤šæ¨¡æ€æ¨¡å‹åœ¨è‡ªç„¶å›¾åƒåˆ†å‰²é¢†åŸŸå–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œä½†å½“åº”ç”¨äºåŒ»å­¦é¢†åŸŸæ—¶å¸¸å¸¸è¡¨ç°ä¸ä½³ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œè¿™ä¸»è¦æ˜¯å› ä¸ºé¢ä¸´å¤šæ¨¡æ€èåˆçš„æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯æŠ½è±¡æ–‡æœ¬æç¤ºå’Œç²¾ç»†åŒ»å­¦è§†è§‰ç‰¹å¾ä¹‹é—´çš„è¯­ä¹‰é¸¿æ²Ÿä»¥åŠç”±æ­¤äº§ç”Ÿçš„ç‰¹å¾åˆ†æ•£é—®é¢˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºæœŸæœ›æœ€å¤§åŒ–ï¼ˆEMï¼‰çš„èšåˆæœºåˆ¶å’Œæ–‡æœ¬å¼•å¯¼åƒç´ è§£ç å™¨ã€‚å‰è€…é€šè¿‡åŠ¨æ€å°†ç‰¹å¾èšç±»åˆ°ç´§å‡‘çš„è¯­ä¹‰ä¸­å¿ƒæ¥ç¼“è§£ç‰¹å¾åˆ†æ•£é—®é¢˜ï¼Œä»è€Œæé«˜è·¨æ¨¡æ€çš„å¯¹åº”å…³ç³»ã€‚åè€…æ—¨åœ¨åˆ©ç”¨é¢†åŸŸä¸å˜çš„æ–‡æœ¬çŸ¥è¯†æ¥æœ‰æ•ˆåœ°å¼•å¯¼æ·±åº¦è§†è§‰è¡¨å¾ï¼Œä»è€Œç¼©å°è¯­ä¹‰é¸¿æ²Ÿã€‚è¿™ä¸¤ç§æœºåˆ¶çš„ååŒä½œç”¨æ˜¾è‘—æé«˜äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚åœ¨å…¬å…±å¿ƒè„å’Œçœ¼åº•æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šåŸŸæ³›åŒ–åŸºå‡†æµ‹è¯•ä¸­å§‹ç»ˆä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€æ¨¡å‹åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­è¡¨ç°æ¬ ä½³ã€‚</li>
<li>ä¸»è¦æŒ‘æˆ˜åœ¨äºå¤šæ¨¡æ€èåˆé—®é¢˜ï¼Œç‰¹åˆ«æ˜¯è¯­ä¹‰é¸¿æ²Ÿå’Œç‰¹å¾åˆ†æ•£ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†æœŸæœ›æœ€å¤§åŒ–ï¼ˆEMï¼‰èšåˆæœºåˆ¶ä»¥ç¼“è§£ç‰¹å¾åˆ†æ•£é—®é¢˜ã€‚</li>
<li>ç ”ç©¶å¼•å…¥äº†æ–‡æœ¬å¼•å¯¼åƒç´ è§£ç å™¨ä»¥ç¼©å°è¯­ä¹‰é¸¿æ²Ÿå¹¶å¢å¼ºæ¨¡å‹æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡åŠ¨æ€èšç±»ç‰¹å¾å’Œåˆ©ç”¨æ–‡æœ¬çŸ¥è¯†æ¥æé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>åœ¨å…¬å…±å¿ƒè„å’Œçœ¼åº•æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šåŸŸæ³›åŒ–æµ‹è¯•ä¸­è¡¨ç°ä¼˜è¶Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.08570">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_åŒ»å­¦å›¾åƒ/2509.08570v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_åŒ»å­¦å›¾åƒ/2509.08570v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Color-Blind-Image-Sensors-Towards-Digital-Twin-of-Human-Retina"><a href="#Color-Blind-Image-Sensors-Towards-Digital-Twin-of-Human-Retina" class="headerlink" title="Color-Blind Image Sensors: Towards Digital Twin of Human Retina"></a>Color-Blind Image Sensors: Towards Digital Twin of Human Retina</h2><p><strong>Authors:Yushan Meng, Bryce Widdicombe, Dechuan Sun, Paul Beckett, Peter van Wijngaarden, Efstratios Skafidas, Ampalavanapillai Nirmalathas, Ranjith Unnithan</strong></p>
<p>The human retina contains a complex arrangement of photoreceptors that convert light into visual information. Conventional image sensors mimic the trichromacy of the retina using periodic filter mosaics responsive to three primary colors. However, this is, at best, an approximation, as an actual retina exhibits a quasi-random spatial distribution of light-sensitive rod and cone photoreceptors, where the ratio of rods to cones and their concentrations vary across the retina. Hence, the periodic mosaics are limited to accurately simulate the properties of the eye. Here, we present an image sensor with similar distribution, spacing, ratios and spectral characteristics of an actual foveal mosaic for emulating eye-like sampling and mimicking color blindness. To perform image reconstruction, we use a fully convolutional U-Net neural network adopting the concept of receptive fields in the retinal circuitry. Our research will enable the development of digital twin of a retina to further understand color vision deficiencies. </p>
<blockquote>
<p>äººç±»è§†ç½‘è†œåŒ…å«å¤æ‚çš„æ„Ÿå…‰ç»†èƒæ’åˆ—ï¼Œè¿™äº›ç»†èƒå°†å…‰è½¬åŒ–ä¸ºè§†è§‰ä¿¡æ¯ã€‚ä¼ ç»Ÿçš„å›¾åƒä¼ æ„Ÿå™¨é€šè¿‡å‘¨æœŸæ€§æ»¤æ³¢å™¨é©¬èµ›å…‹å“åº”ä¸‰ç§ä¸»è¦é¢œè‰²æ¥æ¨¡ä»¿è§†ç½‘è†œçš„ä¸‰è‰²è§†è§‰ã€‚ç„¶è€Œï¼Œè¿™æœ€å¤šåªæ˜¯ä¸€ç§è¿‘ä¼¼ï¼Œå› ä¸ºå®é™…çš„è§†ç½‘è†œè¡¨ç°å‡ºå…‰æ•æ†çŠ¶ç»†èƒå’Œé”¥çŠ¶æ„Ÿå…‰ç»†èƒçš„å‡†éšæœºç©ºé—´åˆ†å¸ƒï¼Œå…¶ä¸­æ†çŠ¶ç»†èƒå’Œé”¥çŠ¶ç»†èƒçš„æ¯”ä¾‹åŠå…¶æµ“åº¦åœ¨è§†ç½‘è†œä¸Šæœ‰æ‰€ä¸åŒã€‚å› æ­¤ï¼Œå‘¨æœŸæ€§é©¬èµ›å…‹åœ¨æ¨¡æ‹Ÿçœ¼ç›å±æ€§æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å…·æœ‰å®é™…çœ¼åº•é©¬èµ›å…‹ç›¸ä¼¼åˆ†å¸ƒã€é—´è·ã€æ¯”ä¾‹å’Œå…‰è°±ç‰¹æ€§çš„å›¾åƒä¼ æ„Ÿå™¨ï¼Œä»¥æ¨¡æ‹Ÿçœ¼é‡‡æ ·å¹¶æ¨¡ä»¿è‰²ç›²ã€‚ä¸ºäº†è¿›è¡Œå›¾åƒé‡å»ºï¼Œæˆ‘ä»¬é‡‡ç”¨äº†å…¨å·ç§¯U-Netç¥ç»ç½‘ç»œï¼Œé‡‡ç”¨è§†ç½‘è†œç”µè·¯ä¸­æ„Ÿå—é‡çš„æ¦‚å¿µã€‚æˆ‘ä»¬çš„ç ”ç©¶å°†èƒ½å¤Ÿå®ç°è§†ç½‘è†œçš„æ•°å­—å­ªç”Ÿï¼Œä»¥è¿›ä¸€æ­¥äº†è§£è‰²è§‰ç¼ºé™·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.08518v1">PDF</a> Journal</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ¨¡æ‹Ÿè§†ç½‘è†œå›¾åƒä¼ æ„Ÿå™¨ï¼Œå…¶æ¨¡æ‹Ÿäººçœ¼è§†ç½‘è†œä¸­çš„æ„Ÿå…‰å™¨åˆ†å¸ƒã€é—´è·ã€æ¯”ä¾‹å’Œå…‰è°±ç‰¹æ€§ï¼Œä»¥æ¨¡æ‹Ÿäººçœ¼çš„é‡‡æ ·å’Œè‰²ç›²ç°è±¡ã€‚è¯¥ç ”ç©¶ä½¿ç”¨å…¨å·ç§¯U-Netç¥ç»ç½‘ç»œå¹¶é‡‡ç”¨è§†ç½‘è†œç”µè·¯ä¸­çš„æ„Ÿå—é‡æ¦‚å¿µè¿›è¡Œå›¾åƒé‡å»ºï¼Œæœ‰æœ›ä¸ºæ·±å…¥äº†è§£è‰²è§‰ç¼ºé™·æä¾›æ•°å­—è§†ç½‘è†œåŒèƒèƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†ç½‘è†œåŒ…å«å¤æ‚çš„å…‰æ„Ÿå—å™¨æ’åˆ—ï¼Œå°†å…‰è½¬åŒ–ä¸ºè§†è§‰ä¿¡æ¯ã€‚</li>
<li>ä¼ ç»Ÿå›¾åƒä¼ æ„Ÿå™¨æ¨¡ä»¿è§†ç½‘è†œçš„ä¸‰è‰²è§†è§‰ï¼Œä½†ä¸å®é™…è§†ç½‘è†œçš„å…‰æ„Ÿå—å™¨åˆ†å¸ƒå­˜åœ¨å·®å¼‚ã€‚</li>
<li>äººçœ¼è§†ç½‘è†œä¸­çš„å…‰æ„Ÿå—å™¨æ¯”ä¾‹å’Œæµ“åº¦åœ¨è§†ç½‘è†œä¸Šæœ‰æ‰€ä¸åŒã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§æ¨¡æ‹Ÿå®é™…è§†ç½‘è†œç‰¹æ€§çš„å›¾åƒä¼ æ„Ÿå™¨ã€‚</li>
<li>è¯¥ç ”ç©¶ä½¿ç”¨å…¨å·ç§¯U-Netç¥ç»ç½‘ç»œå’Œè§†ç½‘è†œç”µè·¯ä¸­çš„æ„Ÿå—é‡æ¦‚å¿µè¿›è¡Œå›¾åƒé‡å»ºã€‚</li>
<li>æ­¤é¡¹ç ”ç©¶å¯åŠ©åŠ›å¼€å‘æ•°å­—è§†ç½‘è†œåŒèƒèƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.08518">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_åŒ»å­¦å›¾åƒ/2509.08518v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_åŒ»å­¦å›¾åƒ/2509.08518v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_åŒ»å­¦å›¾åƒ/2509.08518v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_åŒ»å­¦å›¾åƒ/2509.08518v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="LD-ViCE-Latent-Diffusion-Model-for-Video-Counterfactual-Explanations"><a href="#LD-ViCE-Latent-Diffusion-Model-for-Video-Counterfactual-Explanations" class="headerlink" title="LD-ViCE: Latent Diffusion Model for Video Counterfactual Explanations"></a>LD-ViCE: Latent Diffusion Model for Video Counterfactual Explanations</h2><p><strong>Authors:Payal Varshney, Adriano Lucieri, Christoph Balada, Sheraz Ahmed, Andreas Dengel</strong></p>
<p>Video-based AI systems are increasingly adopted in safety-critical domains such as autonomous driving and healthcare. However, interpreting their decisions remains challenging due to the inherent spatiotemporal complexity of video data and the opacity of deep learning models. Existing explanation techniques often suffer from limited temporal coherence, insufficient robustness, and a lack of actionable causal insights. Current counterfactual explanation methods typically do not incorporate guidance from the target model, reducing semantic fidelity and practical utility. We introduce Latent Diffusion for Video Counterfactual Explanations (LD-ViCE), a novel framework designed to explain the behavior of video-based AI models. Compared to previous approaches, LD-ViCE reduces the computational costs of generating explanations by operating in latent space using a state-of-the-art diffusion model, while producing realistic and interpretable counterfactuals through an additional refinement step. Our experiments demonstrate the effectiveness of LD-ViCE across three diverse video datasets, including EchoNet-Dynamic (cardiac ultrasound), FERV39k (facial expression), and Something-Something V2 (action recognition). LD-ViCE outperforms a recent state-of-the-art method, achieving an increase in R2 score of up to 68% while reducing inference time by half. Qualitative analysis confirms that LD-ViCE generates semantically meaningful and temporally coherent explanations, offering valuable insights into the target model behavior. LD-ViCE represents a valuable step toward the trustworthy deployment of AI in safety-critical domains. </p>
<blockquote>
<p>åŸºäºè§†é¢‘çš„AIç³»ç»Ÿè¶Šæ¥è¶Šå¤šåœ°è¢«åº”ç”¨åœ¨è‡ªåŠ¨é©¾é©¶å’ŒåŒ»ç–—ç­‰å®‰å…¨å…³é”®é¢†åŸŸã€‚ç„¶è€Œï¼Œç”±äºè§†é¢‘æ•°æ®çš„å›ºæœ‰æ—¶ç©ºå¤æ‚æ€§å’Œæ·±åº¦å­¦ä¹ æ¨¡å‹çš„é€æ˜åº¦ï¼Œè§£é‡Šå®ƒä»¬çš„å†³ç­–ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ç°æœ‰çš„è§£é‡ŠæŠ€æœ¯é€šå¸¸å­˜åœ¨æ—¶é—´è¿è´¯æ€§æœ‰é™ã€é²æ£’æ€§ä¸è¶³å’Œç¼ºä¹å¯æ“ä½œæ€§çš„å› æœæ´å¯Ÿç­‰é—®é¢˜ã€‚å½“å‰çš„åäº‹å®è§£é‡Šæ–¹æ³•é€šå¸¸ä¸ç»“åˆç›®æ ‡æ¨¡å‹çš„æŒ‡å¯¼ï¼Œé™ä½äº†è¯­ä¹‰ä¿çœŸåº¦å’Œå®ç”¨æ€§ã€‚æˆ‘ä»¬å¼•å…¥äº†ç”¨äºè§†é¢‘åäº‹å®è§£é‡Šçš„æ½œåœ¨æ‰©æ•£ï¼ˆLatent Diffusion for Video Counterfactual Explanationsï¼ŒLD-ViCEï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è§£é‡ŠåŸºäºè§†é¢‘çš„AIæ¨¡å‹è¡Œä¸ºçš„æ–°å‹æ¡†æ¶ã€‚ä¸ä»¥å‰çš„æ–¹æ³•ç›¸æ¯”ï¼ŒLD-ViCEé€šè¿‡åœ¨æ½œåœ¨ç©ºé—´ä½¿ç”¨æœ€å…ˆè¿›çš„æ‰©æ•£æ¨¡å‹æ¥ç”Ÿæˆè§£é‡Šï¼Œä»è€Œé™ä½äº†ç”Ÿæˆè§£é‡Šçš„è®¡ç®—æˆæœ¬ï¼ŒåŒæ—¶é€šè¿‡é¢å¤–çš„ç»†åŒ–æ­¥éª¤äº§ç”Ÿç°å®å’Œå¯è§£é‡Šçš„åäº‹å®ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒLD-ViCEåœ¨ä¸‰ä¸ªä¸åŒçš„è§†é¢‘æ•°æ®é›†ä¸Šéå¸¸æœ‰æ•ˆï¼ŒåŒ…æ‹¬EchoNet-Dynamicï¼ˆå¿ƒè„è¶…å£°ï¼‰ã€FERV39kï¼ˆé¢éƒ¨è¡¨æƒ…ï¼‰å’ŒSomething-Something V2ï¼ˆåŠ¨ä½œè¯†åˆ«ï¼‰ã€‚LD-ViCEä¼˜äºæœ€è¿‘çš„æœ€å…ˆè¿›æ–¹æ³•ï¼ŒR2åˆ†æ•°æé«˜äº†é«˜è¾¾68%ï¼ŒåŒæ—¶å°†æ¨ç†æ—¶é—´å‡å°‘äº†ä¸€åŠã€‚å®šæ€§åˆ†æè¯å®ï¼ŒLD-ViCEç”Ÿæˆçš„è§£é‡Šåœ¨è¯­ä¹‰ä¸Šæ˜¯æœ‰æ„ä¹‰çš„ï¼Œæ—¶é—´ä¸Šè¿è´¯çš„ï¼Œä¸ºç›®æ ‡æ¨¡å‹çš„è¡Œä¸ºæä¾›äº†æœ‰ä»·å€¼çš„è§è§£ã€‚LD-ViCEæœç€åœ¨å…³é”®å®‰å…¨é¢†åŸŸå¯ä¿¡éƒ¨ç½²AIçš„æ–¹å‘è¿ˆå‡ºäº†å®è´µçš„ä¸€æ­¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.08422v1">PDF</a> 30 pages</p>
<p><strong>Summary</strong><br>     é’ˆå¯¹è§†é¢‘åŸºAIç³»ç»Ÿå†³ç­–è§£é‡Šçš„éš¾é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºLatent Diffusion for Video Counterfactual Explanationsï¼ˆLD-ViCEï¼‰çš„æ–°å‹æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡æ“ä½œæ½œåœ¨ç©ºé—´é™ä½ç”Ÿæˆè§£é‡Šçš„è®¡ç®—æˆæœ¬ï¼ŒåŒæ—¶äº§ç”ŸçœŸå®ä¸”å¯è§£è¯»çš„åäº‹å®è§£é‡Šã€‚å®éªŒè¯æ˜ï¼ŒLD-ViCEåœ¨ä¸‰ä¸ªä¸åŒè§†é¢‘æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œèƒ½ç”Ÿæˆè¯­ä¹‰ä¸Šè¿è´¯ä¸”æ—¶é—´ä¸Šè¿è´¯çš„è§£é‡Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>è§†é¢‘åŸºAIç³»ç»Ÿåœ¨å®‰å…¨å…³é”®é¢†åŸŸçš„åº”ç”¨è¶Šæ¥è¶Šå¹¿æ³›ï¼Œä½†è§£é‡Šå…¶å†³ç­–ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>ç°æœ‰è§£é‡ŠæŠ€æœ¯å­˜åœ¨æ—¶é—´è¿è´¯æ€§æœ‰é™ã€ç¨³å¥æ€§ä¸è¶³å’Œç¼ºä¹å¯æ“ä½œæ€§çš„å› æœè§è§£çš„é—®é¢˜ã€‚</li>
<li>LD-ViCEæ¡†æ¶æ—¨åœ¨è§£é‡Šè§†é¢‘åŸºAIæ¨¡å‹çš„è¡Œä¸ºï¼Œé€šè¿‡æ“ä½œæ½œåœ¨ç©ºé—´é™ä½ç”Ÿæˆè§£é‡Šçš„è®¡ç®—æˆæœ¬ã€‚</li>
<li>LD-ViCEé‡‡ç”¨å…ˆè¿›çš„æ‰©æ•£æ¨¡å‹å’Œç»†åŒ–æ­¥éª¤ï¼Œäº§ç”ŸçœŸå®ä¸”å¯è§£è¯»çš„åäº‹å®è§£é‡Šã€‚</li>
<li>LD-ViCEåœ¨ä¸‰ä¸ªä¸åŒè§†é¢‘æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒåŒ…æ‹¬EchoNet-Dynamicï¼ˆå¿ƒè„è¶…å£°ï¼‰ã€FERV39kï¼ˆé¢éƒ¨è¡¨æƒ…ï¼‰å’ŒSomething-Something V2ï¼ˆåŠ¨ä½œè¯†åˆ«ï¼‰ã€‚</li>
<li>LD-ViCEçš„RÂ²å¾—åˆ†æé«˜å¹…åº¦é«˜è¾¾68%ï¼ŒåŒæ—¶æ¨ç†æ—¶é—´å‡å°‘ä¸€åŠã€‚</li>
<li>LD-ViCEç”Ÿæˆçš„è§£é‡Šè¯­ä¹‰ä¸Šè¿è´¯ä¸”æ—¶é—´ä¸Šè¿è´¯ï¼Œä¸ºç†è§£ç›®æ ‡æ¨¡å‹è¡Œä¸ºæä¾›äº†æœ‰ä»·å€¼çš„è§è§£ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.08422">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_åŒ»å­¦å›¾åƒ/2509.08422v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_åŒ»å­¦å›¾åƒ/2509.08422v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_åŒ»å­¦å›¾åƒ/2509.08422v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_åŒ»å­¦å›¾åƒ/2509.08422v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_åŒ»å­¦å›¾åƒ/2509.08422v1/page_5_1.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="SimCroP-Radiograph-Representation-Learning-with-Similarity-driven-Cross-granularity-Pre-training"><a href="#SimCroP-Radiograph-Representation-Learning-with-Similarity-driven-Cross-granularity-Pre-training" class="headerlink" title="SimCroP: Radiograph Representation Learning with Similarity-driven   Cross-granularity Pre-training"></a>SimCroP: Radiograph Representation Learning with Similarity-driven   Cross-granularity Pre-training</h2><p><strong>Authors:Rongsheng Wang, Fenghe Tang, Qingsong Yao, Rui Yan, Xu Zhang, Zhen Huang, Haoran Lai, Zhiyang He, Xiaodong Tao, Zihang Jiang, Shaohua Kevin Zhou</strong></p>
<p>Medical vision-language pre-training shows great potential in learning representative features from massive paired radiographs and reports. However, in computed tomography (CT) scans, the distribution of lesions which contain intricate structures is characterized by spatial sparsity. Besides, the complex and implicit relationships between different pathological descriptions in each sentence of the report and their corresponding sub-regions in radiographs pose additional challenges. In this paper, we propose a Similarity-Driven Cross-Granularity Pre-training (SimCroP) framework on chest CTs, which combines similarity-driven alignment and cross-granularity fusion to improve radiograph interpretation. We first leverage multi-modal masked modeling to optimize the encoder for understanding precise low-level semantics from radiographs. Then, similarity-driven alignment is designed to pre-train the encoder to adaptively select and align the correct patches corresponding to each sentence in reports. The cross-granularity fusion module integrates multimodal information across instance level and word-patch level, which helps the model better capture key pathology structures in sparse radiographs, resulting in improved performance for multi-scale downstream tasks. SimCroP is pre-trained on a large-scale paired CT-reports dataset and validated on image classification and segmentation tasks across five public datasets. Experimental results demonstrate that SimCroP outperforms both cutting-edge medical self-supervised learning methods and medical vision-language pre-training methods. Codes and models are available at <a target="_blank" rel="noopener" href="https://github.com/ToniChopp/SimCroP">https://github.com/ToniChopp/SimCroP</a>. </p>
<blockquote>
<p>åŒ»å­¦è§†è§‰è¯­è¨€é¢„è®­ç»ƒæ˜¾ç¤ºå‡ºä»å¤§é‡é…å¯¹çš„æ”¾å°„å›¾åƒå’ŒæŠ¥å‘Šä¸­å­¦ä¹ ä»£è¡¨æ€§ç‰¹å¾çš„å·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œåœ¨è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰ä¸­ï¼ŒåŒ…å«å¤æ‚ç»“æ„çš„ç—…ç¶çš„ç©ºé—´ç¨€ç–æ€§åˆ†å¸ƒæ˜¯ä¸€ä¸ªç‰¹ç‚¹ã€‚æ­¤å¤–ï¼ŒæŠ¥å‘Šä¸­çš„æ¯ä¸ªå¥å­ä¸­ä¸åŒçš„ç—…ç†æè¿°ä¸å…¶åœ¨æ”¾å°„å›¾åƒä¸­ç›¸åº”å­åŒºåŸŸä¹‹é—´å¤æ‚è€Œéšæ™¦çš„å…³ç³»å¸¦æ¥äº†é¢å¤–çš„æŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºèƒ¸éƒ¨CTçš„Similarity-Driven Cross-Granularity Pre-trainingï¼ˆSimCroPï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†ç›¸ä¼¼æ€§é©±åŠ¨å¯¹é½å’Œè·¨ç²’åº¦èåˆï¼Œä»¥æ”¹è¿›æ”¾å°„å›¾åƒçš„è§£é‡Šã€‚æˆ‘ä»¬é¦–å…ˆåˆ©ç”¨å¤šæ¨¡å¼é®æŒ¡å»ºæ¨¡æ¥ä¼˜åŒ–ç¼–ç å™¨ï¼Œä»¥ç†è§£æ¥è‡ªæ”¾å°„å›¾åƒçš„ç²¾ç¡®ä½çº§è¯­ä¹‰ã€‚ç„¶åï¼Œè®¾è®¡ç›¸ä¼¼æ€§é©±åŠ¨å¯¹é½æ¥é¢„è®­ç»ƒç¼–ç å™¨ï¼Œä½¿å…¶èƒ½å¤Ÿè‡ªé€‚åº”åœ°é€‰æ‹©å’Œå¯¹é½ä¸æŠ¥å‘Šä¸­æ¯ä¸ªå¥å­ç›¸å¯¹åº”çš„æ­£ç¡®æ–‘å—ã€‚è·¨ç²’åº¦èåˆæ¨¡å—èåˆäº†å®ä¾‹çº§åˆ«å’Œå•è¯æ–‘å—çº§åˆ«çš„å¤šæ¨¡å¼ä¿¡æ¯ï¼Œæœ‰åŠ©äºæ¨¡å‹åœ¨ç¨€ç–çš„æ”¾å°„å›¾åƒä¸­æ›´å¥½åœ°æ•æ‰å…³é”®ç—…ç†ç»“æ„ï¼Œä»è€Œæé«˜å¤šå°ºåº¦ä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½ã€‚SimCroPæ˜¯åœ¨å¤§è§„æ¨¡é…å¯¹CTæŠ¥å‘Šæ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒçš„ï¼Œå¹¶åœ¨äº”ä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„å›¾åƒåˆ†ç±»å’Œåˆ†å‰²ä»»åŠ¡ä¸­è¿›è¡Œäº†éªŒè¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSimCroPåœ¨å…ˆè¿›åŒ»å­¦è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•å’ŒåŒ»å­¦è§†è§‰è¯­è¨€é¢„è®­ç»ƒæ–¹æ³•çš„æ€§èƒ½ä¸Šè¡¨ç°æ›´ä¼˜è¶Šã€‚ä»£ç å’Œæ¨¡å‹å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/ToniChopp/SimCroP%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/ToniChopp/SimCroPè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.08311v1">PDF</a> Accepted by MICCAI 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>åŒ»å­¦è§†è§‰ä¸è¯­è¨€é¢„è®­ç»ƒæŠ€æœ¯åœ¨ä»å¤§é‡é…å¯¹æ”¾å°„å½±åƒä¸æŠ¥å‘Šä¸­å­¦ä¹ ä»£è¡¨æ€§ç‰¹å¾æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ã€‚åœ¨CTæ‰«æä¸­ï¼Œç”±äºç—…ç¶å†…å«å¤æ‚ç»“æ„ä¸”å…·æœ‰ç©ºé—´ç¨€ç–æ€§åˆ†å¸ƒç‰¹å¾ï¼Œå…¶è¯†åˆ«é¢ä¸´æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼ŒæŠ¥å‘Šä¸­æ¯ä¸€å¥è¯å¯¹ä¸åŒç—…ç†æè¿°ä¸å…¶åœ¨æ”¾å°„å½±åƒä¸­å¯¹åº”å­åŒºåŸŸä¹‹é—´å¤æ‚è€Œéšå«çš„å…³ç³»ä¹Ÿå¢åŠ äº†éš¾åº¦ã€‚æœ¬æ–‡æå‡ºä¸€ç§é’ˆå¯¹èƒ¸éƒ¨CTçš„Similarity-Driven Cross-Granularity Pre-trainingï¼ˆSimCroPï¼‰æ¡†æ¶ï¼Œç»“åˆç›¸ä¼¼æ€§é©±åŠ¨å¯¹é½å’Œè·¨ç²’åº¦èåˆï¼Œä»¥æ”¹å–„æ”¾å°„å½±åƒè§£è¯»ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬åˆ©ç”¨å¤šæ¨¡æ€æ©æ¨¡å»ºæ¨¡ä¼˜åŒ–ç¼–ç å™¨ï¼Œä»¥ç†è§£æ”¾å°„å½±åƒä¸­çš„ç²¾ç¡®ä½çº§è¯­ä¹‰ã€‚æ¥ç€ï¼Œè®¾è®¡ç›¸ä¼¼æ€§é©±åŠ¨å¯¹é½æ¥é¢„è®­ç»ƒç¼–ç å™¨ï¼Œä½¿å…¶èƒ½å¤Ÿè‡ªé€‚åº”é€‰æ‹©å’Œå¯¹é½æŠ¥å‘Šä¸­æ¯å¥è¯çš„æ­£ç¡®æ–‘å—ã€‚è·¨ç²’åº¦èåˆæ¨¡å—æ•´åˆå®ä¾‹çº§åˆ«å’Œè¯-æ–‘å—çº§åˆ«çš„å¤šæ¨¡æ€ä¿¡æ¯ï¼Œå¸®åŠ©æ¨¡å‹åœ¨ç¨€ç–æ”¾å°„å½±åƒä¸­æ›´å¥½åœ°æ•æ‰å…³é”®ç—…ç†ç»“æ„ï¼Œå¯¹å¤šå°ºåº¦ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½æœ‰æ‰€æå‡ã€‚SimCroPæ¡†æ¶åœ¨å¤§å‹é…å¯¹CTæŠ¥å‘Šæ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå¹¶åœ¨äº”ä¸ªå…¬å…±æ•°æ®é›†ä¸Šè¿›è¡Œå›¾åƒåˆ†ç±»å’Œåˆ†å‰²ä»»åŠ¡éªŒè¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSimCroPåœ¨å…ˆè¿›çš„åŒ»å­¦è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•å’ŒåŒ»å­¦è§†è§‰è¯­è¨€é¢„è®­ç»ƒæ–¹æ³•çš„æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚ç›¸å…³ä»£ç å’Œæ¨¡å‹å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/ToniChopp/SimCroP%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/ToniChopp/SimCroPè·å–ã€‚</a></p>
<p><strong>è¦ç‚¹æ¦‚æ‹¬</strong></p>
<ol>
<li>åŒ»å­¦è§†è§‰-è¯­è¨€é¢„è®­ç»ƒåœ¨ç†è§£å¤§é‡æ”¾å°„å½±åƒä¸æŠ¥å‘Šä¸­çš„ä»£è¡¨æ€§ç‰¹å¾æ–¹é¢å…·æœ‰æ½œåŠ›ã€‚</li>
<li>CTæ‰«æä¸­çš„ç—…ç¶å…·æœ‰ç©ºé—´ç¨€ç–æ€§å’Œå¤æ‚ç»“æ„ç‰¹å¾ï¼Œå¢åŠ äº†è¯†åˆ«éš¾åº¦ã€‚</li>
<li>æŠ¥å‘Šä¸­ç—…ç†æè¿°ä¸å…¶åœ¨æ”¾å°„å½±åƒä¸­å¯¹åº”å­åŒºåŸŸçš„å…³ç³»å¤æ‚ä¸”éšå«ã€‚</li>
<li>SimCroPæ¡†æ¶é€šè¿‡ç›¸ä¼¼æ€§é©±åŠ¨å¯¹é½å’Œè·¨ç²’åº¦èåˆæ¥æå‡æ”¾å°„å½±åƒè§£è¯»ã€‚</li>
<li>åˆ©ç”¨å¤šæ¨¡æ€æ©æ¨¡å»ºæ¨¡ä¼˜åŒ–ç¼–ç å™¨å¯¹ä½çº§è¯­ä¹‰çš„ç†è§£ã€‚</li>
<li>SimCroPæ¡†æ¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†é¢„è®­ç»ƒå¹¶åœ¨å›¾åƒåˆ†ç±»å’Œåˆ†å‰²ä»»åŠ¡ä¸­è¡¨ç°ä¼˜è¶Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.08311">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_åŒ»å­¦å›¾åƒ/2509.08311v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_åŒ»å­¦å›¾åƒ/2509.08311v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_åŒ»å­¦å›¾åƒ/2509.08311v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_åŒ»å­¦å›¾åƒ/2509.08311v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="RepViT-CXR-A-Channel-Replication-Strategy-for-Vision-Transformers-in-Chest-X-ray-Tuberculosis-and-Pneumonia-Classification"><a href="#RepViT-CXR-A-Channel-Replication-Strategy-for-Vision-Transformers-in-Chest-X-ray-Tuberculosis-and-Pneumonia-Classification" class="headerlink" title="RepViT-CXR: A Channel Replication Strategy for Vision Transformers in   Chest X-ray Tuberculosis and Pneumonia Classification"></a>RepViT-CXR: A Channel Replication Strategy for Vision Transformers in   Chest X-ray Tuberculosis and Pneumonia Classification</h2><p><strong>Authors:Faisal Ahmed</strong></p>
<p>Chest X-ray (CXR) imaging remains one of the most widely used diagnostic tools for detecting pulmonary diseases such as tuberculosis (TB) and pneumonia. Recent advances in deep learning, particularly Vision Transformers (ViTs), have shown strong potential for automated medical image analysis. However, most ViT architectures are pretrained on natural images and require three-channel inputs, while CXR scans are inherently grayscale. To address this gap, we propose RepViT-CXR, a channel replication strategy that adapts single-channel CXR images into a ViT-compatible format without introducing additional information loss. We evaluate RepViT-CXR on three benchmark datasets. On the TB-CXR dataset,our method achieved an accuracy of 99.9% and an AUC of 99.9%, surpassing prior state-of-the-art methods such as Topo-CXR (99.3% accuracy, 99.8% AUC). For the Pediatric Pneumonia dataset, RepViT-CXR obtained 99.0% accuracy, with 99.2% recall, 99.3% precision, and an AUC of 99.0%, outperforming strong baselines including DCNN and VGG16. On the Shenzhen TB dataset, our approach achieved 91.1% accuracy and an AUC of 91.2%, marking a performance improvement over previously reported CNN-based methods. These results demonstrate that a simple yet effective channel replication strategy allows ViTs to fully leverage their representational power on grayscale medical imaging tasks. RepViT-CXR establishes a new state of the art for TB and pneumonia detection from chest X-rays, showing strong potential for deployment in real-world clinical screening systems. </p>
<blockquote>
<p>èƒ¸éƒ¨Xå°„çº¿ï¼ˆCXRï¼‰æˆåƒä»ç„¶æ˜¯æ£€æµ‹è‚ºç»“æ ¸ï¼ˆTBï¼‰å’Œè‚ºç‚ç­‰è‚ºéƒ¨ç–¾ç—…çš„æœ€å¹¿æ³›ä½¿ç”¨çš„è¯Šæ–­å·¥å…·ä¹‹ä¸€ã€‚æœ€è¿‘æ·±åº¦å­¦ä¹ ï¼Œå°¤å…¶æ˜¯è§†è§‰è½¬æ¢å™¨ï¼ˆViTsï¼‰çš„è¿›å±•ï¼Œå·²æ˜¾ç¤ºå‡ºåœ¨è‡ªåŠ¨åŒ–åŒ»å­¦å›¾åƒåˆ†ææ–¹é¢çš„å¼ºå¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ViTæ¶æ„éƒ½æ˜¯åœ¨è‡ªç„¶å›¾åƒä¸Šè¿›è¡Œé¢„è®­ç»ƒçš„ï¼Œéœ€è¦ä¸‰é€šé“è¾“å…¥ï¼Œè€ŒCXRæ‰«ææœ¬è´¨ä¸Šæ˜¯ç°åº¦çš„ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†RepViT-CXRï¼Œè¿™æ˜¯ä¸€ç§é€šé“å¤åˆ¶ç­–ç•¥ï¼Œå®ƒå¯ä»¥å°†å•é€šé“CXRå›¾åƒé€‚åº”ä¸ºViTå…¼å®¹æ ¼å¼ï¼Œè€Œä¸ä¼šå¼•å…¥é¢å¤–çš„ä¿¡æ¯æŸå¤±ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¯„ä¼°äº†RepViT-CXRã€‚åœ¨TB-CXRæ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†99.9%çš„å‡†ç¡®ç‡å’Œ99.9%çš„AUCï¼Œè¶…è¶Šäº†å…ˆå‰çš„æœ€æ–°æ–¹æ³•ï¼Œå¦‚Topo-CXRï¼ˆå‡†ç¡®ç‡ä¸º99.3%ï¼ŒAUCä¸º99.8%ï¼‰ã€‚å¯¹äºå°å„¿è‚ºç‚æ•°æ®é›†ï¼ŒRepViT-CXRè¾¾åˆ°äº†99.0%çš„å‡†ç¡®ç‡ï¼Œå¬å›ç‡ä¸º99.2%ï¼Œç²¾ç¡®åº¦ä¸º99.3%ï¼ŒAUCä¸º99.0%ï¼Œä¼˜äºDCNNå’ŒVGG16ç­‰å¼ºå¤§çš„åŸºçº¿ã€‚åœ¨æ·±åœ³çš„TBæ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†91.1%çš„å‡†ç¡®ç‡å’Œ91.2%çš„AUCï¼Œç›¸è¾ƒäºä¹‹å‰æŠ¥é“çš„åŸºäºCNNçš„æ–¹æ³•ï¼Œæ€§èƒ½æœ‰æ‰€æé«˜ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œç®€å•è€Œæœ‰æ•ˆçš„é€šé“å¤åˆ¶ç­–ç•¥å…è®¸ViTsåœ¨ç°åº¦åŒ»å­¦æˆåƒä»»åŠ¡ä¸Šå……åˆ†åˆ©ç”¨å…¶è¡¨å¾èƒ½åŠ›ã€‚RepViT-CXRä¸ºè‚ºç»“æ ¸å’Œè‚ºç‚çš„èƒ¸éƒ¨Xå°„çº¿æ£€æµ‹å»ºç«‹äº†æ–°çš„æŠ€æœ¯æ ‡æ†ï¼Œæ˜¾ç¤ºå‡ºåœ¨çœŸå®ä¸–ç•Œä¸´åºŠç­›æŸ¥ç³»ç»Ÿä¸­éƒ¨ç½²çš„å¼ºå¤§æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.08234v1">PDF</a> 10 pages, 5 figures</p>
<p><strong>æ‘˜è¦</strong><br>    æœ¬æ–‡æå‡ºä¸€ç§åä¸ºRepViT-CXRçš„æ¸ é“å¤åˆ¶ç­–ç•¥ï¼Œèƒ½å°†å•é€šé“çš„Chest X-rayï¼ˆCXRï¼‰å›¾åƒè½¬åŒ–ä¸ºé€‚åˆViTæ¨¡å‹åˆ†æçš„æ ¼å¼ï¼ŒåŒæ—¶ä¸å¢åŠ ä¿¡æ¯æŸå¤±ã€‚è¯¥ç­–ç•¥åœ¨è‚ºç»“æ ¸ï¼ˆTBï¼‰å’Œè‚ºç‚çš„CXRå›¾åƒè¯Šæ–­ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå–å¾—äº†æ–°çš„é¢†å…ˆæ°´å¹³ï¼Œå±•ç°å‡ºåœ¨å®é™…ä¸´åºŠç­›æŸ¥ç³»ç»Ÿä¸­éƒ¨ç½²çš„æ½œåŠ›ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>Chest X-ray (CXR) ä»ç„¶æ˜¯æ£€æµ‹è‚ºéƒ¨ç–¾ç—…å¦‚è‚ºç»“æ ¸å’Œè‚ºç‚çš„æœ€å¸¸ç”¨çš„è¯Šæ–­å·¥å…·ä¹‹ä¸€ã€‚</li>
<li>è¿‘æœŸæ·±åº¦å­¦ä¹ å’ŒVision Transformersï¼ˆViTsï¼‰åœ¨åŒ»å­¦å›¾åƒåˆ†ææ–¹é¢å±•ç°å‡ºå¼ºå¤§çš„æ½œåŠ›ã€‚</li>
<li>å¤§å¤šæ•°ViTæ¶æ„éƒ½æ˜¯åœ¨è‡ªç„¶å›¾åƒä¸Šè¿›è¡Œé¢„è®­ç»ƒçš„ï¼Œéœ€è¦ä¸‰é€šé“è¾“å…¥ï¼Œè€ŒCXRæ‰«ææ˜¯ç°åº¦çš„ã€‚</li>
<li>æå‡ºRepViT-CXRçš„æ¸ é“å¤åˆ¶ç­–ç•¥ï¼Œå°†å•é€šé“CXRå›¾åƒè½¬åŒ–ä¸ºViTå…¼å®¹æ ¼å¼ã€‚</li>
<li>RepViT-CXRåœ¨ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨ç°ä¼˜ç§€ï¼šåœ¨TB-CXRæ•°æ®é›†ä¸Šå‡†ç¡®ç‡å’ŒAUCå‡è¾¾åˆ°99.9%ï¼Œè¶…è¿‡å…ˆå‰çš„æ–¹æ³•ã€‚</li>
<li>åœ¨å°å„¿è‚ºç‚æ•°æ®é›†ä¸Šï¼ŒRepViT-CXRè·å¾—99.0%çš„å‡†ç¡®ç‡ã€99.2%çš„å¬å›ç‡ã€99.3%çš„ç²¾ç¡®åº¦å’Œ99.0%çš„AUCï¼Œä¼˜äºDCNNå’ŒVGG16ç­‰å¼ºåŸºçº¿ã€‚</li>
<li>RepViT-CXRæ–¹æ³•åœ¨æ·±åœ³å¸‚è‚ºç»“æ ¸æ•°æ®é›†ä¸Šè¾¾åˆ°91.1%çš„å‡†ç¡®ç‡å’Œ91.2%çš„AUCï¼Œç›¸æ¯”ä¹‹å‰æŠ¥é“çš„CNNæ–¹æ³•æœ‰æ‰€æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.08234">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_åŒ»å­¦å›¾åƒ/2509.08234v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_åŒ»å­¦å›¾åƒ/2509.08234v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_åŒ»å­¦å›¾åƒ/2509.08234v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_åŒ»å­¦å›¾åƒ/2509.08234v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="The-WISSH-quasar-project-XII-X-ray-view-of-the-most-luminous-quasi-stellar-objects-at-Cosmic-Noon"><a href="#The-WISSH-quasar-project-XII-X-ray-view-of-the-most-luminous-quasi-stellar-objects-at-Cosmic-Noon" class="headerlink" title="The WISSH quasar project. XII. X-ray view of the most luminous   quasi-stellar objects at Cosmic Noon"></a>The WISSH quasar project. XII. X-ray view of the most luminous   quasi-stellar objects at Cosmic Noon</h2><p><strong>Authors:C. Degli Agosti, C. Vignali, E. Piconcelli, L. Zappacosta, E. Bertola, R. Middei, I. Saccheo, G. Vietri, F. Vito, A. Bongiorno, M. Bischetti, G. Bruni, S. Carniani, G. Cresci, C. Feruglio, F. Salvestrini, A. Travascio, M. Gaspari, E. Glikman, E. Kammoun, G. Lanzuisi, M. Laurenti, G. Miniutti, C. Pinto, V. Testa, F. Tombesi, A. Tortosa, F. Fiore</strong></p>
<p>To improve our knowledge of nuclear emission in luminous QSOs at Cosmic Noon, we studied the X-ray emission of the WISE&#x2F;SDSS-selected hyper-luminous (WISSH) QSO sample: 85 broad-line AGN with $L_{bol}&gt;few\times 10^{47},erg,s^{-1}$ at $z\sim 2-4$. Our aim is to characterise their X-ray spectra and explore relations between X-ray luminosity and other bands, comparing powerful QSOs with the general AGN population. We performed spectral analysis for about half of the sample; 16 sources were analysed via their hardness ratio; for the others we estimated their intrinsic luminosity $L_{2-10,keV}$. Only 8 sources are undetected. We report a large dispersion in $L_{2-10,keV}$ despite the narrow distribution of $L_{bol}$, $L_{2500,\r{A}}$ and $\lambda L_{6,\mu m}$ (about one-third of the sources classified as X-ray weak). This suggests differences in X-ray corona and accretion flow physics between hyper-luminous and less powerful AGN. X-ray photon index distribution is consistent with that of lower-$z$, lower-$L_{bol}$ AGN, and does not depend on the Eddington ratio ($\lambda_{Edd}$) or X-ray weakness. Most WISSH QSOs with intrinsic absorption estimates show little to no obscuration ($N_H \le 5\times 10^{22},cm^{-2}$). Among the obscured sources we find blue QSOs without broad absorption lines within the â€œforbidden regionâ€ of the $Log(N_H)-Log(\lambda_{Edd})$ plane, typically occupied by dust-reddened QSOs and associated with intense feedback. We confirm a correlation between $L_{2-10,keV}$ and CIV line blueshift, a tracer of nuclear ionized outflows. Multi-wavelength data and complete X-ray coverage enabled the investigation of the disk-corona interplay at the highest luminosity regimes. The broad distribution of bolometric correction and X-ray - to - optical index suggest caution when using $L_{bol}$, $L_{2500,\r{A}}$ or $L_{6,\mu m}$ as direct X-ray proxy for individual luminous QSOs. </p>
<blockquote>
<p>ä¸ºäº†äº†è§£å®‡å®™ä¸­åˆæœŸå‘å…‰é‡å­æ ‡åº¦è¶…æ–°æ˜Ÿï¼ˆQSOsï¼‰çš„æ ¸å‘å°„ç‰¹æ€§ï¼Œæˆ‘ä»¬å¯¹é€šè¿‡WAISE&#x2F;SDSSé€‰å®šçš„è¶…äº®ï¼ˆWISSHï¼‰QSOæ ·æœ¬çš„Xå°„çº¿å‘å°„è¿›è¡Œäº†ç ”ç©¶ï¼Œè¿™äº›æ ·æœ¬åŒ…æ‹¬85ä¸ªå®½çº¿æ´»è·ƒæ˜Ÿç³»æ ¸ï¼ˆAGNï¼‰ï¼Œå…¶æ€»å…‰åº¦ï¼ˆ$L_{bol}$ï¼‰å¤§äºè‹¥å¹²å€$10^{47} , erg , s^{-1}$ï¼Œä½äºçº¢ç§»$z\sim 2-4$çš„åŒºé—´ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯åˆ»ç”»å®ƒä»¬çš„Xå°„çº¿å…‰è°±ï¼Œæ¢ç´¢Xå°„çº¿å…‰åº¦ä¸å…¶ä»–æ³¢æ®µçš„è”ç³»ï¼Œå¹¶å°†è¿™äº›å¼ºå¤§çš„QSOsä¸ä¸€èˆ¬çš„AGNç¾¤ä½“è¿›è¡Œæ¯”è¾ƒã€‚æˆ‘ä»¬å¯¹å¤§çº¦ä¸€åŠçš„æ ·æœ¬è¿›è¡Œäº†å…‰è°±åˆ†æï¼›é€šè¿‡ç¡¬åº¦æ¯”åˆ†æäº†å…¶ä¸­16ä¸ªæºï¼›å¯¹äºå…¶ä»–æºï¼Œæˆ‘ä»¬ä¼°è®¡äº†å…¶å›ºæœ‰å…‰åº¦ï¼ˆ$L_{2-10 , keV}$ï¼‰ã€‚ä»…æœ‰8ä¸ªæºæœªè¢«æ¢æµ‹åˆ°ã€‚å°½ç®¡æ€»å…‰åº¦ï¼ˆ$L_{bol}$ï¼‰ã€æ³¢é•¿ $L_{2500 \r{A}}$ ä»¥åŠ $\lambda L_{6 , \mu m}$ çš„åˆ†å¸ƒè¾ƒçª„ï¼ˆçº¦ä¸‰åˆ†ä¹‹ä¸€çš„æºè¢«å½’ç±»ä¸ºXå°„çº¿å¼±ï¼‰ï¼Œä½†æˆ‘ä»¬æŠ¥å‘Šäº† $L_{2-10 , keV}$ çš„è¾ƒå¤§åˆ†æ•£ã€‚è¿™è¡¨æ˜è¶…äº®AGNä¸åŠŸç‡è¾ƒä½çš„AGNä¹‹é—´çš„Xå°„çº¿å†•å’Œå¢æµç‰©ç†ç‰¹æ€§çš„å·®å¼‚ã€‚Xå°„çº¿å…‰å­æŒ‡æ•°åˆ†å¸ƒä¸ä½çº¢ç§»ã€ä½æ€»å…‰åº¦çš„AGNä¸€è‡´ï¼Œä¸ä¾èµ–äºçˆ±ä¸é¡¿æ¯”ç‡ï¼ˆ$\lambda_{Edd}$ï¼‰æˆ–Xå°„çº¿å¼±åº¦ã€‚å¤§å¤šæ•°å…·æœ‰å›ºæœ‰å¸æ”¶ä¼°è®¡å€¼çš„WISSH QSOsæ˜¾ç¤ºå‡ ä¹æ²¡æœ‰é®è”½ä½œç”¨ï¼ˆ$N_H \le 5\times 10^{22} , cm^{-2}$ï¼‰ã€‚åœ¨é®è”½æºä¸­ï¼Œæˆ‘ä»¬å‘ç°äº†ä¸€äº›æ²¡æœ‰å®½å¸æ”¶çº¿çš„è“è‰²QSOsä½äºâ€œç¦æ­¢åŒºåŸŸâ€å†…ï¼Œè¯¥åŒºåŸŸé€šå¸¸è¢«å°˜åŸƒçº¢åŒ–çš„QSOså æ®ï¼Œå¹¶ä¸å¼ºçƒˆçš„åé¦ˆæœ‰å…³ã€‚æˆ‘ä»¬è¯å®äº† $L_{2-10 , keV}$ ä¸CIVçº¿è“ç§»ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œè¿™æ˜¯æ ¸ç¦»å­æµå‡ºçš„è¿¹è±¡ã€‚å¤šæ³¢é•¿æ•°æ®å’Œå®Œæ•´çš„Xå°„çº¿è¦†ç›–ä½¿å¾—åœ¨æœ€é«˜å…‰åº¦èŒƒå›´å†…ç ”ç©¶æ˜Ÿç›˜ç›¸äº’ä½œç”¨æˆä¸ºå¯èƒ½ã€‚æ€»å…‰åº¦çš„å¹¿æ³›æ ¡æ­£å’ŒXå°„çº¿ä¸å…‰å­¦æŒ‡æ•°è¡¨æ˜ï¼Œåœ¨ä½¿ç”¨ $L_{bol}$ã€ $L_{2500 \r{A}}$ æˆ– $L_{6 , \mu m}$ ä½œä¸ºå•ä¸ªå‘å…‰QSOsçš„ç›´æ¥Xå°„çº¿ä»£ç†æ—¶éœ€è¦è°¨æ…ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.08055v1">PDF</a> Accepted for publication in Astronomy &amp; Astrophysics</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡å¯¹å®‡å®™ä¸­åˆæœŸæ ¸å‘å°„çš„ç ”ç©¶è¿›è¡Œäº†æ·±å…¥æ¢è®¨ï¼Œç‰¹åˆ«æ˜¯å¯¹é«˜å…‰åº¦QSOçš„Xå°„çº¿å‘å°„è¿›è¡Œäº†ç ”ç©¶ã€‚é€šè¿‡å¯¹WISE&#x2F;SDSSé€‰å®šçš„è¶…é«˜å…‰åº¦ï¼ˆWISSHï¼‰QSOæ ·æœ¬çš„Xå°„çº¿å…‰è°±åˆ†æï¼Œæˆ‘ä»¬å¯¹å…¶Xå°„çº¿å…‰è°±è¿›è¡Œäº†è¡¨å¾ï¼Œå¹¶æ¢è®¨äº†Xå°„çº¿å…‰åº¦ä¸å…¶ä»–æ³¢æ®µä¹‹é—´çš„å…³ç³»ã€‚æœ¬æ–‡æŠ¥é“äº†æ ·æœ¬ä¸­çº¦ä¸€åŠæºçš„å…‰è°±åˆ†æç»“æœï¼Œå¯¹å¦å¤–çš„æºé€šè¿‡ç¡¬åº¦æ¯”è¿›è¡Œäº†åˆ†æï¼Œå¹¶å¯¹è¿™äº›æºçš„å›ºæœ‰å…‰åº¦Lè¿›è¡Œäº†ä¼°ç®—ã€‚ç»“æœå‘ç°è¶…é«˜å…‰åº¦QSOä¸­å­˜åœ¨è¾ƒå¤§çš„Xå°„çº¿å…‰åº¦åˆ†æ•£ç°è±¡ï¼Œè¿™æš—ç¤ºäº†è¶…é«˜å…‰åº¦QSOä¸æ™®é€šä½å…‰åº¦AGNXå°„çº¿å†•å’Œå¸ç§¯æµç‰©ç†æ€§è´¨ä¹‹é—´çš„å·®å¼‚ã€‚æ­¤å¤–ï¼Œå¤§å¤šæ•°WISSH QSOsçš„å›ºæœ‰å¸æ”¶ä¼°è®¡æ˜¾ç¤ºæ²¡æœ‰æ˜æ˜¾çš„é®è”½ç°è±¡ã€‚æœ€åï¼Œæˆ‘ä»¬å‘ç°äº†ä¸€ç§ç°è±¡ï¼Œå³CIVçº¿è“ç§»ä¸Xå°„çº¿å…‰åº¦ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œè¿™å¯èƒ½ä¸æ ¸ç”µç¦»æµå‡ºæœ‰å…³ã€‚æœ¬æ–‡å¼ºè°ƒäº†åœ¨ä½¿ç”¨Lbolã€L 2500 Aæˆ–L 6Î¼mä½œä¸ºä¸ªä½“é«˜å…‰åº¦QSOçš„ç›´æ¥Xå°„çº¿ä»£ç†æ—¶éœ€è¦è°¨æ…ã€‚æ€»çš„æ¥è¯´ï¼Œæœ¬æ–‡ä¸ºæˆ‘ä»¬å¯¹å®‡å®™ä¸­åˆæœŸé«˜å…‰åº¦QSOçš„æ ¸å‘å°„æœ‰äº†æ›´æ·±å…¥çš„äº†è§£ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¯¹å®‡å®™ä¸­åˆæœŸæ ¸å‘å°„è¿›è¡Œäº†æ·±å…¥ç ”ç©¶ï¼Œç‰¹åˆ«æ˜¯å¯¹é«˜å…‰åº¦QSOçš„Xå°„çº¿å‘å°„è¿›è¡Œäº†è€ƒå¯Ÿã€‚</li>
<li>ç ”ç©¶äº†è¶…é«˜å…‰åº¦QSOæ ·æœ¬çš„Xå°„çº¿å…‰è°±ç‰¹æ€§ã€‚</li>
<li>å‘ç°è¶…é«˜å…‰åº¦QSOçš„Xå°„çº¿å…‰åº¦å­˜åœ¨è¾ƒå¤§çš„åˆ†æ•£ç°è±¡ï¼Œæš—ç¤ºå…¶ä¸æ™®é€šä½å…‰åº¦AGNXå°„çº¿å†•å’Œå¸ç§¯æµç‰©ç†æ€§è´¨å­˜åœ¨å·®å¼‚ã€‚</li>
<li>å¤§éƒ¨åˆ†WISSH QSOsçš„å›ºæœ‰å¸æ”¶ä¼°è®¡æ˜¾ç¤ºæ²¡æœ‰æ˜æ˜¾çš„é®è”½ç°è±¡ã€‚</li>
<li>å‘ç°CIVçº¿è“ç§»ä¸Xå°„çº¿å…‰åº¦ä¹‹é—´å­˜åœ¨ç›¸å…³æ€§ï¼Œå¯èƒ½ä¸æ ¸ç”µç¦»æµå‡ºæœ‰å…³ã€‚</li>
<li>ä½¿ç”¨Lbolã€L 2500 Aæˆ–L 6Î¼mä½œä¸ºé«˜å…‰åº¦QSOçš„ç›´æ¥Xå°„çº¿ä»£ç†æ—¶å­˜åœ¨ä¸ç¡®å®šæ€§ï¼Œéœ€è°¨æ…ä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.08055">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_åŒ»å­¦å›¾åƒ/2509.08055v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_åŒ»å­¦å›¾åƒ/2509.08055v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_åŒ»å­¦å›¾åƒ/2509.08055v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_åŒ»å­¦å›¾åƒ/2509.08055v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_åŒ»å­¦å›¾åƒ/2509.08055v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Expert-Guided-Explainable-Few-Shot-Learning-for-Medical-Image-Diagnosis"><a href="#Expert-Guided-Explainable-Few-Shot-Learning-for-Medical-Image-Diagnosis" class="headerlink" title="Expert-Guided Explainable Few-Shot Learning for Medical Image Diagnosis"></a>Expert-Guided Explainable Few-Shot Learning for Medical Image Diagnosis</h2><p><strong>Authors:Ifrat Ikhtear Uddin, Longwei Wang, KC Santosh</strong></p>
<p>Medical image analysis often faces significant challenges due to limited expert-annotated data, hindering both model generalization and clinical adoption. We propose an expert-guided explainable few-shot learning framework that integrates radiologist-provided regions-of-interests (ROIs) into model training to simultaneously enhance classification performance and interpretability. Leveraging Grad-CAM for spatial attention supervision, we introduce an explanation loss based on Dice similarity to align model attention with diagnostically relevant regions during training. This explanation loss is jointly optimized with a standard prototypical network objective, encouraging the model to focus on clinically meaningful features even under limited data conditions. We evaluate our framework on two distinct datasets: BraTS (MRI) and VinDr-CXR (Chest X-ray), achieving significant accuracy improvements from 77.09% to 83.61% on BraTS and from 54.33% to 73.29% on VinDr-CXR compared to non-guided models. Grad-CAM visualizations further confirm that expert-guided training consistently aligns attention with diagnostic regions, improving both predictive reliability and clinical trustworthiness. Our findings demonstrate the effectiveness of incorporating expert-guided attention supervision to bridge the gap between performance and interpretability in few-shot medical image diagnosis. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒåˆ†æå¸¸å¸¸å› ä¸ºç¼ºä¹ä¸“å®¶æ ‡æ³¨çš„æ•°æ®è€Œé¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼Œè¿™ä¸ä»…å½±å“äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œä¹Ÿé˜»ç¢äº†å…¶åœ¨ä¸´åºŠä¸Šçš„å®é™…åº”ç”¨ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸“å®¶å¼•å¯¼çš„å¯è§£é‡Šçš„å°‘é‡å­¦ä¹ æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†æ”¾å°„ç§‘åŒ»ç”Ÿæä¾›çš„æ„Ÿå…´è¶£åŒºåŸŸï¼ˆROIï¼‰æ•´åˆåˆ°æ¨¡å‹è®­ç»ƒä¸­ï¼Œä»¥åŒæ—¶æé«˜åˆ†ç±»æ€§èƒ½å’Œè§£é‡Šæ€§ã€‚æˆ‘ä»¬å€ŸåŠ©Grad-CAMè¿›è¡Œç©ºé—´æ³¨æ„åŠ›ç›‘ç£ï¼Œå¹¶å¼•å…¥åŸºäºDiceç›¸ä¼¼åº¦çš„è§£é‡ŠæŸå¤±ï¼Œä»¥åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä½¿æ¨¡å‹æ³¨æ„åŠ›ä¸è¯Šæ–­ç›¸å…³åŒºåŸŸå¯¹é½ã€‚è¿™ç§è§£é‡ŠæŸå¤±ä¸æ ‡å‡†åŸå‹ç½‘ç»œç›®æ ‡è”åˆä¼˜åŒ–ï¼Œé¼“åŠ±æ¨¡å‹å³ä½¿åœ¨æ•°æ®æœ‰é™çš„æƒ…å†µä¸‹ä¹Ÿå…³æ³¨ä¸´åºŠä¸Šå…·æœ‰é‡è¦æ„ä¹‰çš„ç‰¹ç‚¹ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªä¸åŒçš„æ•°æ®é›†BraTSï¼ˆMRIï¼‰å’ŒVinDr-CXRï¼ˆèƒ¸éƒ¨Xå…‰ç‰‡ï¼‰ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ¡†æ¶ï¼Œä¸æ— å¯¼å‘æ¨¡å‹ç›¸æ¯”ï¼ŒBraTSçš„å‡†ç¡®æ€§ä»77.09%æé«˜åˆ°83.61%ï¼ŒVinDr-CXRä»54.33%æé«˜åˆ°73.29%ã€‚Grad-CAMå¯è§†åŒ–è¿›ä¸€æ­¥è¯å®ï¼Œä¸“å®¶å¼•å¯¼çš„è®­ç»ƒèƒ½ä½¿æ³¨æ„åŠ›å§‹ç»ˆä¸è¯Šæ–­åŒºåŸŸå¯¹é½ï¼Œæé«˜äº†é¢„æµ‹å¯é æ€§å’Œä¸´åºŠå¯ä¿¡åº¦ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œåœ¨å°‘é‡åŒ»å­¦å›¾åƒè¯Šæ–­ä¸­èå…¥ä¸“å®¶å¼•å¯¼çš„æ³¨æ„åŠ›ç›‘ç£ï¼Œèƒ½æœ‰æ•ˆç¼©å°æ€§èƒ½ä¸è§£é‡Šæ€§ä¹‹é—´çš„å·®è·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.08007v1">PDF</a> Accepted for publication in the proceedings of MICCAI Workshop on   Data Engineering in Medical Imaging 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§ä¸“å®¶å¼•å¯¼çš„å¯è§£é‡Šçš„å°‘æ ·æœ¬å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡æ•´åˆæ”¾å°„ç§‘åŒ»ç”Ÿæä¾›çš„æ„Ÿå…´è¶£åŒºåŸŸï¼ˆROIsï¼‰åˆ°æ¨¡å‹è®­ç»ƒä¸­ï¼ŒåŒæ—¶æé«˜åˆ†ç±»æ€§èƒ½å’Œè§£é‡Šæ€§ã€‚åˆ©ç”¨Grad-CAMè¿›è¡Œç©ºé—´æ³¨æ„åŠ›ç›‘ç£ï¼Œå¹¶å¼•å…¥åŸºäºDiceç›¸ä¼¼åº¦çš„è§£é‡ŠæŸå¤±ï¼Œä½¿æ¨¡å‹æ³¨æ„åŠ›ä¸è¯Šæ–­ç›¸å…³åŒºåŸŸå¯¹é½ã€‚è¯¥è§£é‡ŠæŸå¤±ä¸æ ‡å‡†åŸå‹ç½‘ç»œç›®æ ‡è”åˆä¼˜åŒ–ï¼Œé¼“åŠ±æ¨¡å‹åœ¨æœ‰é™æ•°æ®æ¡ä»¶ä¸‹å…³æ³¨ä¸´åºŠæœ‰æ„ä¹‰çš„ç‰¹å¾ã€‚åœ¨BraTSå’ŒVinDr-CXRä¸¤ä¸ªæ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œä¸æ— å¼•å¯¼æ¨¡å‹ç›¸æ¯”ï¼Œå‡†ç¡®ç‡åˆ†åˆ«ä»77.09%æé«˜åˆ°83.61%å’Œä»54.33%æé«˜åˆ°73.29%ã€‚Grad-CAMå¯è§†åŒ–è¿›ä¸€æ­¥è¯å®ï¼Œä¸“å®¶å¼•å¯¼çš„è®­ç»ƒèƒ½ä½¿æ³¨æ„åŠ›å§‹ç»ˆä¸è¯Šæ–­åŒºåŸŸå¯¹é½ï¼Œæé«˜é¢„æµ‹å¯é æ€§å’Œä¸´åºŠå¯ä¿¡åº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†æé¢ä¸´ä¸“å®¶æ ‡æ³¨æ•°æ®æœ‰é™çš„æŒ‘æˆ˜ï¼Œå½±å“æ¨¡å‹é€šç”¨åŒ–å’Œä¸´åºŠåº”ç”¨ã€‚</li>
<li>æå‡ºä¸€ç§ä¸“å®¶å¼•å¯¼çš„å¯è§£é‡Šçš„å°‘æ ·æœ¬å­¦ä¹ æ¡†æ¶ï¼Œå°†æ”¾å°„ç§‘åŒ»ç”Ÿæä¾›çš„æ„Ÿå…´è¶£åŒºåŸŸï¼ˆROIsï¼‰èå…¥æ¨¡å‹è®­ç»ƒã€‚</li>
<li>åˆ©ç”¨Grad-CAMè¿›è¡Œç©ºé—´æ³¨æ„åŠ›ç›‘ç£ï¼Œå¹¶å¼•å…¥åŸºäºDiceç›¸ä¼¼åº¦çš„è§£é‡ŠæŸå¤±ã€‚</li>
<li>è§£é‡ŠæŸå¤±ä¸åŸå‹ç½‘ç»œç›®æ ‡è”åˆä¼˜åŒ–ï¼Œæé«˜æ¨¡å‹åœ¨æœ‰é™æ•°æ®ä¸‹å¯¹ä¸´åºŠæœ‰æ„ä¹‰ç‰¹å¾çš„å…³æ³¨ã€‚</li>
<li>åœ¨BraTSå’ŒVinDr-CXRæ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æé«˜åˆ†ç±»å‡†ç¡®ç‡ã€‚</li>
<li>Grad-CAMå¯è§†åŒ–è¯å®ä¸“å®¶å¼•å¯¼è®­ç»ƒèƒ½ä½¿æ¨¡å‹æ³¨æ„åŠ›ä¸è¯Šæ–­åŒºåŸŸå¯¹é½ã€‚</li>
<li>ä¸“å®¶å¼•å¯¼çš„æ–¹æ³•æé«˜äº†é¢„æµ‹å¯é æ€§å’Œä¸´åºŠå¯ä¿¡åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.08007">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_åŒ»å­¦å›¾åƒ/2509.08007v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_åŒ»å­¦å›¾åƒ/2509.08007v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="EDFFDNet-Towards-Accurate-and-Efficient-Unsupervised-Multi-Grid-Image-Registration"><a href="#EDFFDNet-Towards-Accurate-and-Efficient-Unsupervised-Multi-Grid-Image-Registration" class="headerlink" title="EDFFDNet: Towards Accurate and Efficient Unsupervised Multi-Grid Image   Registration"></a>EDFFDNet: Towards Accurate and Efficient Unsupervised Multi-Grid Image   Registration</h2><p><strong>Authors:Haokai Zhu, Bo Qu, Si-Yuan Cao, Runmin Zhang, Shujie Chen, Bailin Yang, Hui-Liang Shen</strong></p>
<p>Previous deep image registration methods that employ single homography, multi-grid homography, or thin-plate spline often struggle with real scenes containing depth disparities due to their inherent limitations. To address this, we propose an Exponential-Decay Free-Form Deformation Network (EDFFDNet), which employs free-form deformation with an exponential-decay basis function. This design achieves higher efficiency and performs well in scenes with depth disparities, benefiting from its inherent locality. We also introduce an Adaptive Sparse Motion Aggregator (ASMA), which replaces the MLP motion aggregator used in previous methods. By transforming dense interactions into sparse ones, ASMA reduces parameters and improves accuracy. Additionally, we propose a progressive correlation refinement strategy that leverages global-local correlation patterns for coarse-to-fine motion estimation, further enhancing efficiency and accuracy. Experiments demonstrate that EDFFDNet reduces parameters, memory, and total runtime by 70.5%, 32.6%, and 33.7%, respectively, while achieving a 0.5 dB PSNR gain over the state-of-the-art method. With an additional local refinement stage,EDFFDNet-2 further improves PSNR by 1.06 dB while maintaining lower computational costs. Our method also demonstrates strong generalization ability across datasets, outperforming previous deep learning methods. </p>
<blockquote>
<p>å…ˆå‰é‡‡ç”¨å•åº”æ€§ã€å¤šç½‘æ ¼å•åº”æ€§æˆ–è–„æ¿æ’å€¼çš„æ·±åº¦å›¾åƒé…å‡†æ–¹æ³•ï¼Œç”±äºå›ºæœ‰å±€é™æ€§ï¼Œåœ¨å¤„ç†åŒ…å«æ·±åº¦å·®å¼‚çš„çœŸå®åœºæ™¯æ—¶å¸¸å¸¸é‡åˆ°å›°éš¾ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†æŒ‡æ•°è¡°å‡è‡ªç”±å½¢æ€å˜å½¢ç½‘ç»œï¼ˆEDFFDNetï¼‰ï¼Œè¯¥ç½‘ç»œé‡‡ç”¨è‡ªç”±å½¢æ€å˜å½¢å’ŒæŒ‡æ•°è¡°å‡åŸºå‡½æ•°ã€‚è¿™ç§è®¾è®¡å®ç°äº†é«˜æ•ˆç‡ï¼Œå¹¶åœ¨æ·±åº¦å·®å¼‚åœºæ™¯ä¸­è¡¨ç°è‰¯å¥½ï¼Œå¾—ç›Šäºå…¶å›ºæœ‰çš„å±€éƒ¨æ€§ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†è‡ªé€‚åº”ç¨€ç–è¿åŠ¨èšåˆå™¨ï¼ˆASMAï¼‰ï¼Œå–ä»£äº†å…ˆå‰æ–¹æ³•ä¸­çš„å¤šå±‚è¿åŠ¨èšåˆå™¨ã€‚ASMAé€šè¿‡å°†å¯†é›†äº¤äº’è½¬æ¢ä¸ºç¨€ç–äº¤äº’ï¼Œå‡å°‘äº†å‚æ•°ï¼Œæé«˜äº†ç²¾åº¦ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ¸è¿›çš„ç›¸å…³æ€§ç»†åŒ–ç­–ç•¥ï¼Œåˆ©ç”¨å…¨å±€-å±€éƒ¨ç›¸å…³æ€§æ¨¡å¼è¿›è¡Œä»ç²—åˆ°ç»†çš„è¿åŠ¨ä¼°è®¡ï¼Œè¿›ä¸€æ­¥æé«˜æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒEDFFDNetåœ¨å‚æ•°ã€å†…å­˜å’Œæ€»è¿è¡Œæ—¶é—´æ–¹é¢åˆ†åˆ«å‡å°‘äº†70.5%ã€32.6%å’Œ33.7%ï¼ŒåŒæ—¶åœ¨æœ€å…ˆè¿›çš„æ–¹æ³•åŸºç¡€ä¸Šå®ç°äº†0.5 dBçš„å³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰å¢ç›Šã€‚é€šè¿‡é¢å¤–çš„å±€éƒ¨ç»†åŒ–é˜¶æ®µï¼ŒEDFFDNet-2è¿›ä¸€æ­¥æé«˜äº†PSNRå€¼1.06 dBï¼ŒåŒæ—¶ä¿æŒäº†è¾ƒä½çš„è®¡ç®—æˆæœ¬ã€‚æˆ‘ä»¬çš„æ–¹æ³•è¿˜è¡¨ç°å‡ºäº†å¼ºå¤§çš„è·¨æ•°æ®é›†æ³›åŒ–èƒ½åŠ›ï¼Œä¼˜äºå…ˆå‰çš„æ·±åº¦å­¦ä¹ æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.07662v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºè‡ªç”±å½¢æ€å˜å½¢å’ŒæŒ‡æ•°è¡°å‡åŸºå‡½æ•°çš„Exponential-Decay Free-Form Deformation Networkï¼ˆEDFFDNetï¼‰æ¨¡å‹ï¼Œç”¨äºè§£å†³ä¼ ç»Ÿæ·±åº¦å›¾åƒé…å‡†æ–¹æ³•åœ¨é¢å¯¹å…·æœ‰æ·±åº¦å·®å¼‚çš„çœŸå®åœºæ™¯æ—¶çš„å±€é™æ€§é—®é¢˜ã€‚åŒæ—¶å¼•å…¥è‡ªé€‚åº”ç¨€ç–è¿åŠ¨èšåˆå™¨ï¼ˆASMAï¼‰å’Œæ¸è¿›å¼ç›¸å…³æ€§ä¼˜åŒ–ç­–ç•¥ï¼Œæé«˜æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒEDFFDNetåœ¨å‡å°‘å‚æ•°ã€å†…å­˜å’Œæ€»è¿è¡Œæ—¶é—´çš„åŒæ—¶ï¼Œè¾ƒç°æœ‰æ–¹æ³•æé«˜äº†å›¾åƒè´¨é‡ã€‚EDFFDNet-2åœ¨å±€éƒ¨ç»†åŒ–é˜¶æ®µè¿›ä¸€æ­¥æå‡äº†æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>EDFFDNetæ¨¡å‹é‡‡ç”¨è‡ªç”±å½¢æ€å˜å½¢å’ŒæŒ‡æ•°è¡°å‡åŸºå‡½æ•°ï¼Œæé«˜äº†å¤„ç†å…·æœ‰æ·±åº¦å·®å¼‚åœºæ™¯çš„æ•ˆç‡ã€‚</li>
<li>è‡ªé€‚åº”ç¨€ç–è¿åŠ¨èšåˆå™¨ï¼ˆASMAï¼‰æ›¿æ¢ä¼ ç»Ÿæ–¹æ³•ä¸­çš„MLPè¿åŠ¨èšåˆå™¨ï¼Œé™ä½å‚æ•°å¹¶æé«˜å‡†ç¡®æ€§ã€‚</li>
<li>æå‡ºæ¸è¿›ç›¸å…³æ€§ä¼˜åŒ–ç­–ç•¥ï¼Œåˆ©ç”¨å…¨å±€å’Œå±€éƒ¨ç›¸å…³æ€§æ¨¡å¼è¿›è¡Œä»ç²—åˆ°ç»†çš„è¿åŠ¨ä¼°è®¡ï¼Œè¿›ä¸€æ­¥æé«˜æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒEDFFDNetè¾ƒç°æœ‰æ–¹æ³•å‡å°‘äº†å‚æ•°ã€å†…å­˜å’Œè¿è¡Œæ—¶é—´ï¼Œå¹¶æé«˜äº†PSNRå€¼ã€‚</li>
<li>EDFFDNet-2åœ¨å±€éƒ¨ç»†åŒ–é˜¶æ®µè¿›ä¸€æ­¥æå‡äº†å›¾åƒè´¨é‡ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨ä¸åŒæ•°æ®é›†ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨æ·±åº¦å›¾åƒé…å‡†é¢†åŸŸå…·æœ‰æ½œåœ¨çš„åº”ç”¨ä»·å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.07662">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_åŒ»å­¦å›¾åƒ/2509.07662v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_åŒ»å­¦å›¾åƒ/2509.07662v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_åŒ»å­¦å›¾åƒ/2509.07662v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_åŒ»å­¦å›¾åƒ/2509.07662v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="HU-based-Foreground-Masking-for-3D-Medical-Masked-Image-Modeling"><a href="#HU-based-Foreground-Masking-for-3D-Medical-Masked-Image-Modeling" class="headerlink" title="HU-based Foreground Masking for 3D Medical Masked Image Modeling"></a>HU-based Foreground Masking for 3D Medical Masked Image Modeling</h2><p><strong>Authors:Jin Lee, Vu Dang, Gwang-Hyun Yu, Anh Le, Zahid Rahman, Jin-Ho Jang, Heonzoo Lee, Kun-Yung Kim, Jin-Sul Kim, Jin-Young Kim</strong></p>
<p>While Masked Image Modeling (MIM) has revolutionized fields of computer vision, its adoption in 3D medical image computing has been limited by the use of random masking, which overlooks the density of anatomical objects. To address this limitation, we enhance the pretext task with a simple yet effective masking strategy. Leveraging Hounsfield Unit (HU) measurements, we implement an HU-based Foreground Masking, which focuses on the intensity distribution of visceral organs and excludes non-tissue regions, such as air and fluid, that lack diagnostically meaningful features. Extensive experiments on five public 3D medical imaging datasets demonstrate that our masking consistently improves performance, both in quality of segmentation and Dice score (BTCV:<del>84.64%, Flare22:</del>92.43%, MM-WHS:<del>90.67%, Amos22:</del>88.64%, BraTS:~78.55%). These results underscore the importance of domain-centric MIM and suggest a promising direction for representation learning in medical image segmentation. Implementation is available at github.com&#x2F;AISeedHub&#x2F;SubFore&#x2F;. </p>
<blockquote>
<p>è™½ç„¶Masked Image Modelingï¼ˆMIMï¼‰åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸå·²ç»å®ç°äº†é©å‘½æ€§çš„è¿›å±•ï¼Œä½†å…¶åœ¨ä¸‰ç»´åŒ»å­¦å›¾åƒè®¡ç®—ä¸­çš„åº”ç”¨å´å› éšæœºé®è”½çš„ä½¿ç”¨è€Œå—åˆ°é™åˆ¶ï¼Œå¿½è§†äº†è§£å‰–ç‰©ä½“å¯†åº¦çš„ä¿¡æ¯ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬é€šè¿‡ä¸€é¡¹ç®€å•è€Œæœ‰æ•ˆçš„é®è”½ç­–ç•¥æ¥å¢å¼ºé¢„è®¾ä»»åŠ¡ã€‚åˆ©ç”¨Hounsfield Unitï¼ˆHUï¼‰æµ‹é‡å€¼ï¼Œæˆ‘ä»¬å®ç°äº†åŸºäºHUçš„å‰æ™¯é®è”½ç­–ç•¥ï¼Œè¯¥ç­–ç•¥ä¾§é‡äºå†…è„å™¨å®˜çš„å¼ºåº¦åˆ†å¸ƒï¼Œå¹¶æ’é™¤ç¼ºä¹è¯Šæ–­æ„ä¹‰çš„ç‰¹å¾çš„éç»„ç»‡åŒºåŸŸï¼Œå¦‚ç©ºæ°”å’Œæµä½“ã€‚åœ¨äº”ä¸ªå…¬å¼€çš„ä¸‰ç»´åŒ»å­¦å›¾åƒæ•°æ®é›†ä¸Šè¿›è¡Œçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„é®è”½ç­–ç•¥åœ¨åˆ†å‰²è´¨é‡å’ŒDiceåˆ†æ•°æ–¹é¢éƒ½è¡¨ç°å‡ºäº†ä¸€è‡´æ€§çš„æ”¹è¿›ï¼ˆBTCV:<del>84.64%ï¼ŒFlare22:</del>92.43%ï¼ŒMM-WHS:<del>90.67%ï¼ŒAmos22:</del>88.64%ï¼ŒBraTS:~78.55%ï¼‰ã€‚è¿™äº›ç»“æœå¼ºè°ƒäº†ä»¥é¢†åŸŸä¸ºä¸­å¿ƒçš„MIMçš„é‡è¦æ€§ï¼Œå¹¶ä¸ºåŒ»å­¦å›¾åƒåˆ†å‰²çš„è¡¨ç¤ºå­¦ä¹ æŒ‡æ˜äº†æœ‰å‰æ™¯çš„æ–¹å‘ã€‚å…·ä½“å®ç°å¯è®¿é—®github.com&#x2F;AISeedHub&#x2F;SubFore&#x2F;ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.07534v1">PDF</a> Accepted by MICCAI AMAI Workshop 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºHounsfieldå•ä½ï¼ˆHUï¼‰æµ‹é‡çš„å‰æ™¯æ©æ¨¡ç­–ç•¥ï¼Œç”¨äºæ”¹è¿›åœ¨ä¸‰ç»´åŒ»å­¦å›¾åƒè®¡ç®—ä¸­çš„Masked Image Modelingï¼ˆMIMï¼‰ã€‚æ–°ç­–ç•¥å…³æ³¨å†…è„å™¨å®˜çš„å¼ºåº¦åˆ†å¸ƒï¼Œå¹¶æ’é™¤ç¼ºä¹è¯Šæ–­ç‰¹å¾çš„éç»„ç»‡åŒºåŸŸï¼Œå¦‚ç©ºæ°”å’Œæµä½“ã€‚åœ¨äº”ä¸ªå…¬å…±ä¸‰ç»´åŒ»å­¦æˆåƒæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œè¿™ç§æ©æ¨¡ç­–ç•¥åœ¨åˆ†å‰²è´¨é‡å’ŒDiceåˆ†æ•°æ–¹é¢æŒç»­æé«˜äº†æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Masked Image Modelingï¼ˆMIMï¼‰åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„åº”ç”¨å—åˆ°é™åˆ¶ï¼Œä¸»è¦ç”±äºéšæœºæ©æ¨¡å¿½ç•¥äº†è§£å‰–å¯¹è±¡çš„å¯†åº¦ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºHounsfieldå•ä½ï¼ˆHUï¼‰çš„å‰æ™¯æ©æ¨¡ç­–ç•¥ï¼Œä¸“æ³¨äºå†…è„å™¨å®˜çš„å¼ºåº¦åˆ†å¸ƒã€‚</li>
<li>æ–°ç­–ç•¥æ’é™¤éç»„ç»‡åŒºåŸŸï¼Œå¦‚ç©ºæ°”å’Œæµä½“ï¼Œè¿™äº›åŒºåŸŸç¼ºä¹è¯Šæ–­ç‰¹å¾ã€‚</li>
<li>åœ¨äº”ä¸ªå…¬å…±ä¸‰ç»´åŒ»å­¦æˆåƒæ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæ–°çš„æ©æ¨¡ç­–ç•¥æé«˜äº†åˆ†å‰²è´¨é‡å’ŒDiceåˆ†æ•°ã€‚</li>
<li>å®éªŒç»“æœå¼ºè°ƒé¢†åŸŸç‰¹å®šçš„MIMçš„é‡è¦æ€§ï¼Œå¹¶ä¸ºåŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„è¡¨ç¤ºå­¦ä¹ æä¾›äº†æœ‰å‰æ™¯çš„æ–¹å‘ã€‚</li>
<li>å…¬å¼€äº†å®æ–½ç»†èŠ‚ï¼Œå¯åœ¨github.com&#x2F;AISeedHub&#x2F;SubFore&#x2F;æ‰¾åˆ°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.07534">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_åŒ»å­¦å›¾åƒ/2509.07534v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_åŒ»å­¦å›¾åƒ/2509.07534v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_åŒ»å­¦å›¾åƒ/2509.07534v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_åŒ»å­¦å›¾åƒ/2509.07534v1/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_åŒ»å­¦å›¾åƒ/2509.07534v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Advanced-Brain-Tumor-Segmentation-Using-EMCAD-Efficient-Multi-scale-Convolutional-Attention-Decoding"><a href="#Advanced-Brain-Tumor-Segmentation-Using-EMCAD-Efficient-Multi-scale-Convolutional-Attention-Decoding" class="headerlink" title="Advanced Brain Tumor Segmentation Using EMCAD: Efficient Multi-scale   Convolutional Attention Decoding"></a>Advanced Brain Tumor Segmentation Using EMCAD: Efficient Multi-scale   Convolutional Attention Decoding</h2><p><strong>Authors:GodsGift Uzor, Tania-Amanda Nkoyo Fredrick Eneye, Chukwuebuka Ijezue</strong></p>
<p>Brain tumor segmentation is a critical pre-processing step in the medical image analysis pipeline that involves precise delineation of tumor regions from healthy brain tissue in medical imaging data, particularly MRI scans. An efficient and effective decoding mechanism is crucial in brain tumor segmentation especially in scenarios with limited computational resources. However these decoding mechanisms usually come with high computational costs. To address this concern EMCAD a new efficient multi-scale convolutional attention decoder designed was utilized to optimize both performance and computational efficiency for brain tumor segmentation on the BraTs2020 dataset consisting of MRI scans from 369 brain tumor patients. The preliminary result obtained by the model achieved a best Dice score of 0.31 and maintained a stable mean Dice score of 0.285 plus&#x2F;minus 0.015 throughout the training process which is moderate. The initial model maintained consistent performance across the validation set without showing signs of over-fitting. </p>
<blockquote>
<p>è„‘è‚¿ç˜¤åˆ†å‰²æ˜¯åŒ»å­¦å›¾åƒåˆ†ææµç¨‹ä¸­çš„å…³é”®é¢„å¤„ç†æ­¥éª¤ï¼Œè¯¥æ­¥éª¤æ¶‰åŠä»åŒ»å­¦æˆåƒæ•°æ®ä¸­ç²¾ç¡®åˆ’åˆ†è‚¿ç˜¤åŒºåŸŸå’Œå¥åº·è„‘ç»„ç»‡ï¼Œç‰¹åˆ«æ˜¯MRIæ‰«æã€‚åœ¨è„‘è‚¿ç˜¤åˆ†å‰²ä¸­ï¼Œç‰¹åˆ«æ˜¯åœ¨è®¡ç®—èµ„æºæœ‰é™çš„æƒ…å†µä¸‹ï¼Œé«˜æ•ˆä¸”æœ‰æ•ˆçš„è§£ç æœºåˆ¶è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œè¿™äº›è§£ç æœºåˆ¶é€šå¸¸ä¼´éšç€è¾ƒé«˜çš„è®¡ç®—æˆæœ¬ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œé‡‡ç”¨äº†ä¸€ç§æ–°å‹é«˜æ•ˆçš„å¤šå°ºåº¦å·ç§¯æ³¨æ„åŠ›è§£ç å™¨ï¼ˆEMCADï¼‰ï¼Œä»¥ä¼˜åŒ–BrainTs2020æ•°æ®é›†ä¸Šè„‘è‚¿ç˜¤åˆ†å‰²çš„æ€§èƒ½å’Œè®¡ç®—æ•ˆç‡ã€‚è¯¥æ•°æ®é›†åŒ…å«æ¥è‡ª369åè„‘è‚¿ç˜¤æ‚£è€…çš„MRIæ‰«æã€‚è¯¥æ¨¡å‹å–å¾—çš„åˆæ­¥æœ€ä½³Diceç³»æ•°ä¸º0.31ï¼Œå¹¶åœ¨æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸­ç»´æŒäº†ç¨³å®šçš„å¹³å‡Diceç³»æ•°ï¼Œå³æ­£è´Ÿ0.015ï¼Œå±äºä¸­ç­‰æ°´å¹³ã€‚åˆå§‹æ¨¡å‹åœ¨éªŒè¯é›†ä¸Šçš„è¡¨ç°ä¿æŒä¸€è‡´ï¼Œæ²¡æœ‰å‡ºç°è¿‡æ‹Ÿåˆçš„è¿¹è±¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.05431v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>é’ˆå¯¹åŒ»å­¦å›¾åƒåˆ†æä¸­çš„è„‘è‚¿ç˜¤åˆ†å‰²é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨è®¡ç®—èµ„æºæœ‰é™çš„åœºæ™¯ä¸‹ï¼Œé«˜æ•ˆè§£ç æœºåˆ¶å°¤ä¸ºé‡è¦ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œç ”ç©¶é‡‡ç”¨äº†æ–°å‹çš„å¤šå°ºåº¦å·ç§¯æ³¨æ„åŠ›è§£ç å™¨ï¼ˆEMCADï¼‰ï¼Œåœ¨BraTs2020æ•°æ®é›†ä¸Šè¿›è¡Œè„‘è‚¿ç˜¤åˆ†å‰²ï¼Œå®ç°äº†æ€§èƒ½å’Œè®¡ç®—æ•ˆç‡çš„ä¼˜åŒ–ã€‚åˆæ­¥å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ¨¡å‹çš„æœ€ä½³Diceç³»æ•°ä¸º0.31ï¼Œå¹¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¿æŒç¨³å®šï¼Œå¹³å‡Diceç³»æ•°ä¸º0.285Â±0.015ï¼Œè¡¨ç°ä¸­ç­‰ã€‚åˆæ­¥æ¨¡å‹åœ¨éªŒè¯é›†ä¸Šè¡¨ç°ä¸€è‡´ï¼Œæ²¡æœ‰è¿‡æ‹Ÿåˆçš„è¿¹è±¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è„‘è‚¿ç˜¤åˆ†å‰²æ˜¯åŒ»å­¦å›¾åƒåˆ†æä¸­çš„é‡è¦é¢„å¤„ç†æ­¥éª¤ï¼Œéœ€è¦ä»åŒ»å­¦æˆåƒæ•°æ®ä¸­ç²¾ç¡®åˆ’åˆ†è‚¿ç˜¤åŒºåŸŸå’Œå¥åº·è„‘ç»„ç»‡ã€‚</li>
<li>åœ¨è®¡ç®—èµ„æºæœ‰é™çš„æƒ…å†µä¸‹ï¼Œé«˜æ•ˆçš„è§£ç æœºåˆ¶åœ¨è„‘è‚¿ç˜¤åˆ†å‰²ä¸­è‡³å…³é‡è¦ã€‚</li>
<li>EMCADæ˜¯ä¸€ç§æ–°å‹çš„å¤šå°ºåº¦å·ç§¯æ³¨æ„åŠ›è§£ç å™¨ï¼Œæ—¨åœ¨ä¼˜åŒ–è„‘è‚¿ç˜¤åˆ†å‰²çš„æ€§èƒ½å’Œè®¡ç®—æ•ˆç‡ã€‚</li>
<li>åœ¨BraTs2020æ•°æ®é›†ä¸Šè¿›è¡Œçš„åˆæ­¥å®éªŒæ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹çš„æœ€ä½³Diceç³»æ•°ä¸º0.31ï¼Œè¡¨ç°ä¸­ç­‰ã€‚</li>
<li>æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¿æŒç¨³å®šçš„å¹³å‡Diceç³»æ•°ï¼Œä¸º0.285Â±0.015ã€‚</li>
<li>åˆæ­¥æ¨¡å‹åœ¨éªŒè¯é›†ä¸Šçš„è¡¨ç°ä¸€è‡´ï¼Œæ²¡æœ‰è¿‡æ‹Ÿåˆçš„è¿¹è±¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.05431">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_åŒ»å­¦å›¾åƒ/2509.05431v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_åŒ»å­¦å›¾åƒ/2509.05431v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_åŒ»å­¦å›¾åƒ/2509.05431v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_åŒ»å­¦å›¾åƒ/2509.05431v1/page_5_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_åŒ»å­¦å›¾åƒ/2509.05431v1/page_5_2.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_åŒ»å­¦å›¾åƒ/2509.05431v1/page_5_3.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_åŒ»å­¦å›¾åƒ/2509.05431v1/page_5_4.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Systematic-Integration-of-Attention-Modules-into-CNNs-for-Accurate-and-Generalizable-Medical-Image-Diagnosis"><a href="#Systematic-Integration-of-Attention-Modules-into-CNNs-for-Accurate-and-Generalizable-Medical-Image-Diagnosis" class="headerlink" title="Systematic Integration of Attention Modules into CNNs for Accurate and   Generalizable Medical Image Diagnosis"></a>Systematic Integration of Attention Modules into CNNs for Accurate and   Generalizable Medical Image Diagnosis</h2><p><strong>Authors:Zahid Ullah, Minki Hong, Tahir Mahmood, Jihie Kim</strong></p>
<p>Deep learning has become a powerful tool for medical image analysis; however, conventional Convolutional Neural Networks (CNNs) often fail to capture the fine-grained and complex features critical for accurate diagnosis. To address this limitation, we systematically integrate attention mechanisms into five widely adopted CNN architectures, namely, VGG16, ResNet18, InceptionV3, DenseNet121, and EfficientNetB5, to enhance their ability to focus on salient regions and improve discriminative performance. Specifically, each baseline model is augmented with either a Squeeze and Excitation block or a hybrid Convolutional Block Attention Module, allowing adaptive recalibration of channel and spatial feature representations. The proposed models are evaluated on two distinct medical imaging datasets, a brain tumor MRI dataset comprising multiple tumor subtypes, and a Products of Conception histopathological dataset containing four tissue categories. Experimental results demonstrate that attention augmented CNNs consistently outperform baseline architectures across all metrics. In particular, EfficientNetB5 with hybrid attention achieves the highest overall performance, delivering substantial gains on both datasets. Beyond improved classification accuracy, attention mechanisms enhance feature localization, leading to better generalization across heterogeneous imaging modalities. This work contributes a systematic comparative framework for embedding attention modules in diverse CNN architectures and rigorously assesses their impact across multiple medical imaging tasks. The findings provide practical insights for the development of robust, interpretable, and clinically applicable deep learning based decision support systems. </p>
<blockquote>
<p>æ·±åº¦å­¦ä¹ å·²æˆä¸ºåŒ»å­¦å›¾åƒåˆ†æçš„æœ‰åŠ›å·¥å…·ï¼›ç„¶è€Œï¼Œä¼ ç»Ÿçš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å¾€å¾€æ— æ³•æ•è·å¯¹å‡†ç¡®è¯Šæ–­è‡³å…³é‡è¦çš„ç²¾ç»†ä¸”å¤æ‚çš„ç‰¹å¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°å°†åœ¨äº”ç§å¹¿æ³›é‡‡ç”¨çš„CNNæ¶æ„ï¼ˆå³VGG16ã€ResNet18ã€InceptionV3ã€DenseNet121å’ŒEfficientNetB5ï¼‰ä¸­é›†æˆæ³¨æ„åŠ›æœºåˆ¶ï¼Œä»¥å¢å¼ºå…¶å…³æ³¨æ˜¾è‘—åŒºåŸŸçš„èƒ½åŠ›å¹¶æé«˜é‰´åˆ«æ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼Œæ¯ä¸ªåŸºå‡†æ¨¡å‹éƒ½å¢åŠ äº†Squeezeå’ŒExcitationå—æˆ–æ··åˆå·ç§¯å—æ³¨æ„åŠ›æ¨¡å—ï¼Œå®ç°å¯¹é€šé“å’Œç©ºé—´ç‰¹å¾è¡¨ç¤ºçš„è‡ªé€‚åº”é‡æ–°æ ¡å‡†ã€‚æ‰€æå‡ºçš„æ¨¡å‹åœ¨ä¸¤ä¸ªä¸åŒçš„åŒ»å­¦æˆåƒæ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œå…¶ä¸­åŒ…æ‹¬å¤šç§è‚¿ç˜¤äºšå‹çš„è„‘è‚¿ç˜¤MRIæ•°æ®é›†å’ŒåŒ…å«å››ç§ç»„ç»‡ç±»åˆ«çš„å¦Šå¨ äº§ç‰©ç»„ç»‡ç—…ç†å­¦æ•°æ®é›†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå¢å¼ºæ³¨æ„åŠ›çš„CNNåœ¨æ‰€æœ‰æŒ‡æ ‡ä¸Šå§‹ç»ˆä¼˜äºåŸºå‡†æ¶æ„ã€‚ç‰¹åˆ«æ˜¯ï¼Œå…·æœ‰æ··åˆæ³¨æ„åŠ›çš„EfficientNetB5è·å¾—äº†æœ€é«˜çš„æ€»ä½“æ€§èƒ½ï¼Œå¹¶åœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸Šéƒ½å®ç°äº†æ˜¾è‘—çš„æ”¶ç›Šã€‚é™¤äº†æé«˜åˆ†ç±»ç²¾åº¦å¤–ï¼Œæ³¨æ„åŠ›æœºåˆ¶è¿˜å¢å¼ºäº†ç‰¹å¾å®šä½ï¼Œä»è€Œåœ¨å„ç§å¼‚æ„æˆåƒæ¨¡æ€ä¹‹é—´å®ç°äº†æ›´å¥½çš„æ³›åŒ–ã€‚è¿™é¡¹å·¥ä½œä¸ºåœ¨å¤šç§CNNæ¶æ„ä¸­åµŒå…¥æ³¨æ„åŠ›æ¨¡å—æä¾›äº†ä¸€ä¸ªç³»ç»Ÿçš„æ¯”è¾ƒæ¡†æ¶ï¼Œå¹¶ä¸¥æ ¼è¯„ä¼°äº†å®ƒä»¬åœ¨å¤šä¸ªåŒ»å­¦æˆåƒä»»åŠ¡ä¸­çš„å½±å“ã€‚ç ”ç©¶ç»“æœä¸ºå¼€å‘å®ç”¨ã€å¯è§£é‡Šå’Œä¸´åºŠä¸Šé€‚ç”¨çš„åŸºäºæ·±åº¦å­¦ä¹ çš„å†³ç­–æ”¯æŒç³»ç»Ÿæä¾›äº†å®é™…è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.05343v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ·±åº¦å­¦ä¹ åœ¨åŒ»å­¦å›¾åƒåˆ†æä¸­å…·æœ‰å¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†ä¼ ç»Ÿå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰åœ¨æ•æ‰ç²¾ç»†ç²’åº¦å’Œå¤æ‚ç‰¹å¾æ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œè¿™å¯¹äºå‡†ç¡®è¯Šæ–­è‡³å…³é‡è¦ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡ç³»ç»Ÿåœ°å°†åœ¨äº”ç§å¹¿æ³›é‡‡ç”¨çš„CNNæ¶æ„ä¸­æ•´åˆæ³¨æ„åŠ›æœºåˆ¶ï¼ŒåŒ…æ‹¬VGG16ã€ResNet18ã€InceptionV3ã€DenseNet121å’ŒEfficientNetB5ï¼Œä»¥å¢å¼ºå…¶èšç„¦æ˜¾è‘—åŒºåŸŸçš„èƒ½åŠ›å¹¶æé«˜è¾¨åˆ«æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå¢å¼ºæ³¨æ„åŠ›çš„CNNåœ¨å„ç§æŒ‡æ ‡ä¸Šå§‹ç»ˆä¼˜äºåŸºç¡€æ¶æ„ã€‚ç‰¹åˆ«æ˜¯ï¼Œå¸¦æœ‰æ··åˆæ³¨æ„åŠ›çš„EfficientNetB5å®ç°æœ€é«˜æ•´ä½“æ€§èƒ½ï¼Œåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°æ˜¾è‘—æ”¹è¿›ã€‚é™¤æé«˜åˆ†ç±»ç²¾åº¦å¤–ï¼Œæ³¨æ„åŠ›æœºåˆ¶è¿˜æé«˜äº†ç‰¹å¾å®šä½èƒ½åŠ›ï¼Œåœ¨å¤šç§å¼‚è´¨æˆåƒæ¨¡æ€ä¹‹é—´å®ç°äº†æ›´å¥½çš„æ³›åŒ–ã€‚æœ¬ç ”ç©¶ä¸ºåµŒå…¥æ³¨æ„åŠ›æ¨¡å—çš„ç³»ç»Ÿæ¯”è¾ƒæ¡†æ¶åŠå…¶åœ¨å¤šç§åŒ»å­¦æˆåƒä»»åŠ¡ä¸­çš„å½±å“è¯„ä¼°åšå‡ºäº†è´¡çŒ®ï¼Œä¸ºå¼€å‘ç¨³å¥ã€å¯è§£é‡Šå’Œä¸´åºŠé€‚ç”¨çš„æ·±åº¦å­¦ä¹ å†³ç­–æ”¯æŒç³»ç»Ÿæä¾›äº†å®é™…è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ åœ¨åŒ»å­¦å›¾åƒåˆ†æä¸­å…·æœ‰å¼ºå¤§èƒ½åŠ›ï¼Œä½†ä¼ ç»ŸCNNå­˜åœ¨æ•æ‰ç²¾ç»†ç²’åº¦å’Œå¤æ‚ç‰¹å¾çš„å±€é™æ€§ã€‚</li>
<li>ä¸ºæé«˜è¯Šæ–­å‡†ç¡®æ€§ï¼Œç ”ç©¶è€…åœ¨äº”ç§CNNæ¶æ„ä¸­æ•´åˆäº†æ³¨æ„åŠ›æœºåˆ¶ã€‚</li>
<li>æ³¨æ„åŠ›å¢å¼ºå‹CNNåœ¨å„ç§æŒ‡æ ‡ä¸Šä¼˜äºåŸºç¡€æ¶æ„ã€‚</li>
<li>EfficientNetB5ç»“åˆæ··åˆæ³¨æ„åŠ›æœºåˆ¶è¡¨ç°æœ€ä½³ï¼Œåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°æ˜¾è‘—æ€§èƒ½æå‡ã€‚</li>
<li>æ³¨æ„åŠ›æœºåˆ¶ä¸ä»…æé«˜äº†åˆ†ç±»ç²¾åº¦ï¼Œè¿˜å¢å¼ºäº†ç‰¹å¾å®šä½èƒ½åŠ›ï¼Œæé«˜äº†æ¨¡å‹åœ¨ä¸åŒæˆåƒæ¨¡æ€ä¹‹é—´çš„æ³›åŒ–æ€§èƒ½ã€‚</li>
<li>æœ¬ç ”ç©¶ä¸ºåµŒå…¥æ³¨æ„åŠ›æ¨¡å—çš„ç³»ç»Ÿæ¯”è¾ƒæ¡†æ¶æä¾›äº†è´¡çŒ®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.05343">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_åŒ»å­¦å›¾åƒ/2509.05343v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_åŒ»å­¦å›¾åƒ/2509.05343v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_åŒ»å­¦å›¾åƒ/2509.05343v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Hessian-Based-Lightweight-Neural-Network-HessNet-for-State-of-the-Art-Brain-Vessel-Segmentation-on-a-Minimal-Training-Dataset"><a href="#Hessian-Based-Lightweight-Neural-Network-HessNet-for-State-of-the-Art-Brain-Vessel-Segmentation-on-a-Minimal-Training-Dataset" class="headerlink" title="Hessian-Based Lightweight Neural Network HessNet for State-of-the-Art   Brain Vessel Segmentation on a Minimal Training Dataset"></a>Hessian-Based Lightweight Neural Network HessNet for State-of-the-Art   Brain Vessel Segmentation on a Minimal Training Dataset</h2><p><strong>Authors:Alexandra Bernadotte, Elfimov Nikita, Mikhail Shutov, Ivan Menshikov</strong></p>
<p>Accurate segmentation of blood vessels in brain magnetic resonance angiography (MRA) is essential for successful surgical procedures, such as aneurysm repair or bypass surgery. Currently, annotation is primarily performed through manual segmentation or classical methods, such as the Frangi filter, which often lack sufficient accuracy. Neural networks have emerged as powerful tools for medical image segmentation, but their development depends on well-annotated training datasets. However, there is a notable lack of publicly available MRA datasets with detailed brain vessel annotations. To address this gap, we propose a novel semi-supervised learning lightweight neural network with Hessian matrices on board for 3D segmentation of complex structures such as tubular structures, which we named HessNet. The solution is a Hessian-based neural network with only 6000 parameters. HessNet can run on the CPU and significantly reduces the resource requirements for training neural networks. The accuracy of vessel segmentation on a minimal training dataset reaches state-of-the-art results. It helps us create a large, semi-manually annotated brain vessel dataset of brain MRA images based on the IXI dataset (annotated 200 images). Annotation was performed by three experts under the supervision of three neurovascular surgeons after applying HessNet. It provides high accuracy of vessel segmentation and allows experts to focus only on the most complex important cases. The dataset is available at <a target="_blank" rel="noopener" href="https://git.scinalytics.com/terilat/VesselDatasetPartly">https://git.scinalytics.com/terilat/VesselDatasetPartly</a>. </p>
<blockquote>
<p>åœ¨è„‘éƒ¨ç£å…±æŒ¯è¡€ç®¡é€ å½±ï¼ˆMRAï¼‰ä¸­ï¼Œå¯¹è¡€ç®¡è¿›è¡Œç²¾ç¡®åˆ†å‰²å¯¹äºæ‰‹æœ¯æˆåŠŸè‡³å…³é‡è¦ï¼Œå¦‚åŠ¨è„‰ç˜¤ä¿®å¤æˆ–æ­æ¡¥æ‰‹æœ¯ã€‚ç›®å‰ï¼Œä¸»è¦é€šè¿‡æ‰‹åŠ¨åˆ†å‰²æˆ–ç»å…¸æ–¹æ³•ï¼ˆå¦‚Frangiæ»¤æ³¢å™¨ï¼‰è¿›è¡Œæ ‡æ³¨ï¼Œä½†è¿™ç§æ–¹æ³•å¾€å¾€ç¼ºä¹è¶³å¤Ÿçš„å‡†ç¡®æ€§ã€‚ç¥ç»ç½‘ç»œå·²ä½œä¸ºåŒ»å­¦å›¾åƒåˆ†å‰²çš„å¼ºå¤§å·¥å…·å‡ºç°ï¼Œä½†å…¶å‘å±•å–å†³äºç»è¿‡è‰¯å¥½æ³¨é‡Šçš„è®­ç»ƒæ•°æ®é›†ã€‚ç„¶è€Œï¼Œç¼ºä¹å¸¦æœ‰è¯¦ç»†è„‘è¡€ç®¡æ³¨é‡Šçš„å…¬å¼€MRAæ•°æ®é›†ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹åŠç›‘ç£å­¦ä¹ è½»é‡åŒ–ç¥ç»ç½‘ç»œï¼Œè¯¥ç½‘ç»œå†…ç½®HessiançŸ©é˜µï¼Œç”¨äºå¯¹ç®¡çŠ¶ç»“æ„ç­‰å¤æ‚ç»“æ„è¿›è¡Œ3Dåˆ†å‰²ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºHessNetã€‚è¯¥è§£å†³æ–¹æ¡ˆæ˜¯ä¸€ä¸ªåŸºäºHessiançš„ç¥ç»ç½‘ç»œï¼Œä»…æœ‰6000ä¸ªå‚æ•°ã€‚HessNetå¯åœ¨CPUä¸Šè¿è¡Œï¼Œå¯æ˜¾è‘—é™ä½è®­ç»ƒç¥ç»ç½‘ç»œæ‰€éœ€çš„èµ„æºè¦æ±‚ã€‚åœ¨æœ€å°è®­ç»ƒæ•°æ®é›†ä¸Šçš„è¡€ç®¡åˆ†å‰²ç²¾åº¦è¾¾åˆ°äº†æœ€æ–°ç»“æœã€‚å®ƒå¸®åŠ©æˆ‘ä»¬åŸºäºIXIæ•°æ®é›†åˆ›å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡çš„åŠæ‰‹åŠ¨æ³¨é‡Šçš„è„‘éƒ¨MRAå›¾åƒè„‘è¡€ç®¡æ•°æ®é›†ï¼ˆæ³¨é‡Šäº†200å¼ å›¾åƒï¼‰ã€‚åœ¨ç¥ç»è¡€ç®¡å¤–ç§‘åŒ»ç”Ÿçš„ç›‘ç£ä¸‹ï¼Œä¸‰åä¸“å®¶åº”ç”¨HessNetè¿›è¡Œäº†æ ‡æ³¨ã€‚å®ƒæä¾›äº†é«˜ç²¾åº¦çš„è¡€ç®¡åˆ†å‰²ï¼Œä½¿ä¸“å®¶èƒ½å¤Ÿä¸“æ³¨äºæœ€å¤æ‚çš„å…³é”®ç—…ä¾‹ã€‚æ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://git.scinalytics.com/terilat/VesselDatasetPartly%E5%A4%9A%E5%B9%BF%E3%80%82">https://git.scinalytics.com/terilat/VesselDatasetPartlyæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.15660v3">PDF</a> 11 pages, 2 figures</p>
<p><strong>æ‘˜è¦</strong></p>
<p>ç¥ç»ç½‘ç»œæŠ€æœ¯ä¸ºè§£å†³åŒ»å­¦å›¾åƒåˆ†å‰²é—®é¢˜æä¾›äº†å¼ºæœ‰åŠ›çš„å·¥å…·ï¼Œä½†åœ¨è„‘éƒ¨ç£å…±æŒ¯è¡€ç®¡é€ å½±ï¼ˆMRAï¼‰ä¸­å‡†ç¡®åˆ†å‰²è¡€ç®¡å¯¹äºæ‰‹æœ¯æˆåŠŸè‡³å…³é‡è¦ï¼Œå¦‚åŠ¨è„‰ç˜¤ä¿®å¤æˆ–æ­æ¡¥æ‰‹æœ¯ã€‚ç›®å‰ä¸»è¦é€šè¿‡æ‰‹åŠ¨åˆ†å‰²æˆ–ç»å…¸æ–¹æ³•ï¼ˆå¦‚Frangiæ»¤æ³¢å™¨ï¼‰è¿›è¡Œæ ‡æ³¨ï¼Œä½†å‡†ç¡®æ€§ä¸è¶³ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„åŠç›‘ç£å­¦ä¹ è½»é‡åŒ–ç¥ç»ç½‘ç»œï¼Œé‡‡ç”¨HessiançŸ©é˜µè¿›è¡Œ3Då¤æ‚ç»“æ„ï¼ˆå¦‚ç®¡çŠ¶ç»“æ„ï¼‰åˆ†å‰²ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºHessNetã€‚è¯¥è§£å†³æ–¹æ¡ˆæ˜¯ä¸€ä¸ªåŸºäºHessiançš„ç¥ç»ç½‘ç»œï¼Œä»…æœ‰6000ä¸ªå‚æ•°ï¼Œå¯åœ¨CPUä¸Šè¿è¡Œï¼Œæ˜¾è‘—é™ä½äº†è®­ç»ƒç¥ç»ç½‘ç»œæ‰€éœ€çš„èµ„æºè¦æ±‚ã€‚åœ¨æœ€å°è®­ç»ƒæ•°æ®é›†ä¸Šçš„è¡€ç®¡åˆ†å‰²ç²¾åº¦è¾¾åˆ°äº†ä¸šç•Œé¢†å…ˆæ°´å¹³ã€‚å®ƒå¸®åŠ©æˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡çš„åŠæ‰‹åŠ¨æ ‡æ³¨çš„è„‘éƒ¨è¡€ç®¡æ•°æ®é›†ï¼ŒåŸºäºIXIæ•°æ®é›†ï¼ˆæ ‡æ³¨äº†200å¼ å›¾åƒï¼‰ã€‚æ ‡æ³¨å·¥ä½œç”±ä¸‰åä¸“å®¶åœ¨ä¸‰ä½ç¥ç»è¡€ç®¡å¤–ç§‘åŒ»ç”Ÿçš„ç›‘ç£ä¸‹è¿›è¡Œï¼Œå¹¶åº”ç”¨äº†HessNetã€‚è¿™æä¾›äº†é«˜ç²¾åº¦çš„è¡€ç®¡åˆ†å‰²ï¼Œå¹¶ä½¿ä¸“å®¶èƒ½å¤Ÿä¸“æ³¨äºæœ€å¤æ‚ã€æœ€é‡è¦çš„ç—…ä¾‹ã€‚æ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://git.scinalytics.com/terilat/VesselDatasetPartly%E8%8E%B7%E5%8F%96%E3%80%82">https://git.scinalytics.com/terilat/VesselDatasetPartlyè·å–ã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç¥ç»ç½‘ç»œæŠ€æœ¯åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­æœ‰å¹¿æ³›åº”ç”¨å‰æ™¯ï¼Œç‰¹åˆ«æ˜¯åœ¨è„‘éƒ¨ç£å…±æŒ¯è¡€ç®¡é€ å½±ï¼ˆMRAï¼‰ä¸­ã€‚</li>
<li>ç›®å‰æ‰‹åŠ¨åˆ†å‰²å’Œç»å…¸æ–¹æ³•ï¼ˆå¦‚Frangiæ»¤æ³¢å™¨ï¼‰åœ¨MRAè¡€ç®¡æ ‡æ³¨æ–¹é¢å­˜åœ¨å‡†ç¡®æ€§é—®é¢˜ã€‚</li>
<li>HessNetæ˜¯ä¸€ç§æ–°å‹çš„åŠç›‘ç£å­¦ä¹ è½»é‡åŒ–ç¥ç»ç½‘ç»œï¼Œåˆ©ç”¨HessiançŸ©é˜µè¿›è¡Œå¤æ‚ç»“æ„ï¼ˆå¦‚ç®¡çŠ¶ç»“æ„ï¼‰çš„3Dåˆ†å‰²ã€‚</li>
<li>HessNetå…·æœ‰è¶…é«˜çš„å‚æ•°æ•ˆç‡ï¼Œä»…æœ‰6000ä¸ªå‚æ•°ï¼Œä¸”å¯åœ¨CPUä¸Šè¿è¡Œï¼Œé™ä½èµ„æºéœ€æ±‚ã€‚</li>
<li>åœ¨æœ‰é™è®­ç»ƒæ•°æ®é›†ä¸Šï¼ŒHessNetå®ç°äº†é«˜ç²¾åº¦çš„è¡€ç®¡åˆ†å‰²ï¼Œè¾¾åˆ°ä¸šç•Œé¢†å…ˆæ°´å¹³ã€‚</li>
<li>åŸºäºHessNetï¼Œåˆ›å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡çš„åŠæ‰‹åŠ¨æ ‡æ³¨çš„è„‘éƒ¨è¡€ç®¡æ•°æ®é›†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.15660">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_åŒ»å­¦å›¾åƒ/2508.15660v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_åŒ»å­¦å›¾åƒ/2508.15660v3/page_5_0.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Identifying-actionable-driver-mutations-in-lung-cancer-using-an-efficient-Asymmetric-Transformer-Decoder"><a href="#Identifying-actionable-driver-mutations-in-lung-cancer-using-an-efficient-Asymmetric-Transformer-Decoder" class="headerlink" title="Identifying actionable driver mutations in lung cancer using an   efficient Asymmetric Transformer Decoder"></a>Identifying actionable driver mutations in lung cancer using an   efficient Asymmetric Transformer Decoder</h2><p><strong>Authors:Biagio Brattoli, Jack Shi, Jongchan Park, Taebum Lee, Donggeun Yoo, Sergio Pereira</strong></p>
<p>Identifying actionable driver mutations in non-small cell lung cancer (NSCLC) can impact treatment decisions and significantly improve patient outcomes. Despite guideline recommendations, broader adoption of genetic testing remains challenging due to limited availability and lengthy turnaround times. Machine Learning (ML) methods for Computational Pathology (CPath) offer a potential solution; however, research often focuses on only one or two common mutations, limiting the clinical value of these tools and the pool of patients who can benefit from them. This study evaluates various Multiple Instance Learning (MIL) techniques to detect six key actionable NSCLC driver mutations: ALK, BRAF, EGFR, ERBB2, KRAS, and MET ex14. Additionally, we introduce an Asymmetric Transformer Decoder model that employs queries and key-values of varying dimensions to maintain a low query dimensionality. This approach efficiently extracts information from patch embeddings and minimizes overfitting risks, proving highly adaptable to the MIL setting. Moreover, we present a method to directly utilize tissue type in the model, addressing a typical MIL limitation where either all regions or only some specific regions are analyzed, neglecting biological relevance. Our method outperforms top MIL models by an average of 3%, and over 4% when predicting rare mutations such as ERBB2 and BRAF, moving ML-based tests closer to being practical alternatives to standard genetic testing. </p>
<blockquote>
<p>è¯†åˆ«éå°ç»†èƒè‚ºç™Œï¼ˆNSCLCï¼‰ä¸­çš„å¯æ“ä½œé©±åŠ¨åŸºå› çªå˜å¯ä»¥å¯¹æ²»ç–—å†³ç­–äº§ç”Ÿå½±å“ï¼Œå¹¶æ˜¾è‘—æ”¹å–„æ‚£è€…é¢„åã€‚å°½ç®¡æœ‰æŒ‡å—æ¨èï¼Œä½†ç”±äºæœ‰é™çš„å¯ç”¨æ€§å’Œæ¼«é•¿çš„å‘¨è½¬æ—¶é—´ï¼Œæ›´å¹¿æ³›åœ°é‡‡ç”¨åŸºå› æ£€æµ‹ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚è®¡ç®—ç—…ç†å­¦ï¼ˆCPathï¼‰ä¸­çš„æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰æ–¹æ³•æä¾›äº†æ½œåœ¨çš„è§£å†³æ–¹æ¡ˆï¼›ç„¶è€Œï¼Œç ”ç©¶é€šå¸¸åªå…³æ³¨ä¸€ç§æˆ–ä¸¤ç§å¸¸è§çš„çªå˜ï¼Œè¿™é™åˆ¶äº†è¿™äº›å·¥å…·çš„ä¸´åºŠä»·å€¼ä»¥åŠå¯ä»¥ä»è¿™äº›å·¥å…·ä¸­å—ç›Šçš„æ‚£è€…ç¾¤ä½“ã€‚æœ¬ç ”ç©¶è¯„ä¼°äº†å„ç§å¤šå®ä¾‹å­¦ä¹ ï¼ˆMILï¼‰æŠ€æœ¯æ¥æ£€æµ‹å…­ç§å…³é”®å¯æ“ä½œNSCLCé©±åŠ¨åŸºå› çªå˜ï¼šALKã€BRAFã€EGFRã€ERBB2ã€KRASå’ŒMET ex14ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ä¸å¯¹ç§°å˜å‹å™¨è§£ç å™¨æ¨¡å‹ï¼Œè¯¥æ¨¡å‹é‡‡ç”¨ä¸åŒç»´åº¦çš„æŸ¥è¯¢å’Œé”®å€¼å¯¹æ¥ç»´æŒä½æŸ¥è¯¢ç»´åº¦ã€‚è¿™ç§æ–¹æ³•æœ‰æ•ˆåœ°ä»è¡¥ä¸åµŒå…¥ä¸­æå–ä¿¡æ¯ï¼Œå¹¶æœ€å°åŒ–è¿‡åº¦æ‹Ÿåˆçš„é£é™©ï¼Œè¯æ˜å…¶é«˜åº¦é€‚åº”MILç¯å¢ƒã€‚è€Œä¸”ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–¹æ³•ï¼Œç›´æ¥åœ¨æ¨¡å‹ä¸­ä½¿ç”¨ç»„ç»‡ç±»å‹ï¼Œè§£å†³äº†ä¸€ä¸ªå…¸å‹çš„MILé™åˆ¶ï¼Œå³åªåˆ†ææ‰€æœ‰åŒºåŸŸæˆ–æŸäº›ç‰¹å®šåŒºåŸŸï¼Œè€Œå¿½ç•¥äº†ç”Ÿç‰©å­¦ç›¸å…³æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¹³å‡ä¼˜äºé¡¶çº§MILæ¨¡å‹çº¦3%ï¼Œåœ¨é¢„æµ‹å¦‚ERBB2å’ŒBRAFç­‰ç½•è§çªå˜æ—¶è¶…è¿‡4%ï¼Œä½¿åŸºäºMLçš„æµ‹è¯•æ›´æ¥è¿‘å®ç”¨çš„æ›¿ä»£æ ‡å‡†åŸºå› æ£€æµ‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02431v3">PDF</a> Accepted at MICCAI 2025 Workshop COMPAYL</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶åˆ©ç”¨æœºå™¨å­¦ä¹ æŠ€æœ¯æ£€æµ‹éå°ç»†èƒè‚ºç™Œï¼ˆNSCLCï¼‰çš„å…­ç§å…³é”®å¯æ“ä½œçš„é©±åŠ¨åŸºå› çªå˜ï¼ŒåŒ…æ‹¬ALKã€BRAFã€EGFRã€ERBB2ã€KRASå’ŒMET ex14ã€‚ç ”ç©¶é‡‡ç”¨å¤šç§å®ä¾‹å­¦ä¹ ï¼ˆMILï¼‰æŠ€æœ¯ï¼Œå¹¶å¼•å…¥ä¸å¯¹ç§°è½¬æ¢å™¨è§£ç å™¨æ¨¡å‹ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿé«˜æ•ˆæå–ä¿¡æ¯ï¼Œé™ä½è¿‡æ‹Ÿåˆé£é™©ï¼Œå¹¶ç›´æ¥åˆ©ç”¨ç»„ç»‡ç±»å‹ï¼Œè§£å†³äº†å…¸å‹çš„MILé™åˆ¶ã€‚è¯¥æ–¹æ³•ä¼˜äºé¡¶çº§MILæ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯åœ¨é¢„æµ‹ç½•è§çªå˜å¦‚ERBB2å’ŒBRAFæ—¶ï¼Œä½¿å¾—åŸºäºMLçš„æµ‹è¯•æˆä¸ºå®ç”¨æ›¿ä»£æ ‡å‡†é—ä¼ æµ‹è¯•çš„ä¸€ç§å¯èƒ½æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœºå™¨å­¦ä¹ æŠ€æœ¯åœ¨éå°ç»†èƒè‚ºç™ŒåŸºå› çªå˜æ£€æµ‹ä¸­çš„åº”ç”¨å¯ä»¥æé«˜æ²»ç–—å†³ç­–å’Œæ‚£è€…é¢„åã€‚</li>
<li>é—ä¼ æµ‹è¯•çš„æ›´å¹¿æ³›é‡‡ç”¨å—åˆ°é™åˆ¶ï¼Œå› ä¸ºå¯ç”¨æ€§å’Œå‘¨è½¬æ—¶é—´æœ‰é™ã€‚</li>
<li>è®¡ç®—ç—…ç†å­¦ï¼ˆCPathï¼‰çš„æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰æ–¹æ³•æä¾›æ½œåœ¨è§£å†³æ–¹æ¡ˆã€‚</li>
<li>ç ”ç©¶é‡ç‚¹é›†ä¸­åœ¨å…­ç§å…³é”®å¯æ“ä½œçš„éå°ç»†èƒè‚ºç™Œï¼ˆNSCLCï¼‰é©±åŠ¨åŸºå› çªå˜ä¸Šã€‚</li>
<li>å¼•å…¥ä¸å¯¹ç§°è½¬æ¢å™¨è§£ç å™¨æ¨¡å‹ï¼Œå¯é«˜æ•ˆæå–ä¿¡æ¯å¹¶é™ä½è¿‡æ‹Ÿåˆé£é™©ã€‚</li>
<li>è¯¥æ–¹æ³•ç›´æ¥åˆ©ç”¨ç»„ç»‡ç±»å‹ï¼Œè§£å†³äº†å…¸å‹çš„MILé™åˆ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02431">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_åŒ»å­¦å›¾åƒ/2508.02431v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_åŒ»å­¦å›¾åƒ/2508.02431v3/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_åŒ»å­¦å›¾åƒ/2508.02431v3/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_åŒ»å­¦å›¾åƒ/2508.02431v3/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_åŒ»å­¦å›¾åƒ/2508.02431v3/page_5_0.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="DiffOSeg-Omni-Medical-Image-Segmentation-via-Multi-Expert-Collaboration-Diffusion-Model"><a href="#DiffOSeg-Omni-Medical-Image-Segmentation-via-Multi-Expert-Collaboration-Diffusion-Model" class="headerlink" title="DiffOSeg: Omni Medical Image Segmentation via Multi-Expert Collaboration   Diffusion Model"></a>DiffOSeg: Omni Medical Image Segmentation via Multi-Expert Collaboration   Diffusion Model</h2><p><strong>Authors:Han Zhang, Xiangde Luo, Yong Chen, Kang Li</strong></p>
<p>Annotation variability remains a substantial challenge in medical image segmentation, stemming from ambiguous imaging boundaries and diverse clinical expertise. Traditional deep learning methods producing single deterministic segmentation predictions often fail to capture these annotator biases. Although recent studies have explored multi-rater segmentation, existing methods typically focus on a single perspective â€“ either generating a probabilistic &#96;&#96;gold standardâ€™â€™ consensus or preserving expert-specific preferences â€“ thus struggling to provide a more omni view. In this study, we propose DiffOSeg, a two-stage diffusion-based framework, which aims to simultaneously achieve both consensus-driven (combining all expertsâ€™ opinions) and preference-driven (reflecting expertsâ€™ individual assessments) segmentation. Stage I establishes population consensus through a probabilistic consensus strategy, while Stage II captures expert-specific preference via adaptive prompts. Demonstrated on two public datasets (LIDC-IDRI and NPC-170), our model outperforms existing state-of-the-art methods across all evaluated metrics. Source code is available at <a target="_blank" rel="noopener" href="https://github.com/string-ellipses/DiffOSeg">https://github.com/string-ellipses/DiffOSeg</a> . </p>
<blockquote>
<p>æ ‡æ³¨å·®å¼‚æ€§åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­ä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§æŒ‘æˆ˜ï¼Œå…¶æ ¹æºåœ¨äºæˆåƒè¾¹ç•Œæ¨¡ç³Šå’Œä¸´åºŠä¸“ä¸šçŸ¥è¯†å¤šæ ·ã€‚ä¼ ç»Ÿäº§ç”Ÿå•ä¸€ç¡®å®šæ€§åˆ†å‰²é¢„æµ‹çš„æ·±åº¦å­¦ä¹ æ–¹æ³•é€šå¸¸æ— æ³•æ•æ‰è¿™äº›æ ‡æ³¨è€…åè§ã€‚å°½ç®¡æœ€è¿‘æœ‰ç ”ç©¶è¡¨æ˜å¤šè¯„ä»·è€…åˆ†å‰²æ–¹æ³•ï¼Œä½†ç°æœ‰æ–¹æ³•é€šå¸¸åªå…³æ³¨å•ä¸€è§†è§’ï¼Œè¦ä¹ˆç”Ÿæˆæ¦‚ç‡â€œé‡‘æ ‡å‡†â€å…±è¯†ï¼Œè¦ä¹ˆä¿ç•™ä¸“å®¶ç‰¹å®šåå¥½ï¼Œå› æ­¤å¾ˆéš¾æä¾›æ›´å…¨é¢çš„è§†å›¾ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†DiffOSegï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºä¸¤é˜¶æ®µæ‰©æ•£çš„æ¡†æ¶ï¼Œæ—¨åœ¨åŒæ—¶å®ç°å…±è¯†é©±åŠ¨ï¼ˆç»“åˆæ‰€æœ‰ä¸“å®¶çš„æ„è§ï¼‰å’Œåå¥½é©±åŠ¨ï¼ˆåæ˜ ä¸“å®¶çš„ä¸ªäººè¯„ä¼°ï¼‰çš„åˆ†å‰²ã€‚ç¬¬ä¸€é˜¶æ®µé€šè¿‡æ¦‚ç‡å…±è¯†ç­–ç•¥å»ºç«‹äººç¾¤å…±è¯†ï¼Œè€Œç¬¬äºŒé˜¶æ®µé€šè¿‡è‡ªé€‚åº”æç¤ºæ•æ‰ä¸“å®¶ç‰¹å®šåå¥½ã€‚åœ¨LIDC-IDRIå’ŒNPC-170ä¸¤ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šçš„æ¼”ç¤ºè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡ä¸Šå‡ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ¨¡å‹ã€‚æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/string-ellipses/DiffOSeg%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/string-ellipses/DiffOSegä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.13087v2">PDF</a> </p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­ä»å­˜åœ¨æ ‡æ³¨å·®å¼‚æ€§æŒ‘æˆ˜ï¼Œæºäºæˆåƒè¾¹ç•Œæ¨¡ç³Šå’Œä¸´åºŠä¸“å®¶æ„è§å¤šæ ·æ€§ã€‚ä¼ ç»Ÿæ·±åº¦å­¦ä¹ æ–¹æ³•äº§ç”Ÿçš„å•ä¸€ç¡®å®šæ€§åˆ†å‰²é¢„æµ‹å¾€å¾€æ— æ³•æ•æ‰æ ‡æ³¨è€…åå·®ã€‚æœ¬ç ”ç©¶æå‡ºDiffOSegæ¡†æ¶ï¼Œæ—¨åœ¨åŒæ—¶å®ç°å…±è¯†é©±åŠ¨å’Œåå¥½é©±åŠ¨çš„åˆ†å‰²ã€‚è¯¥æ¡†æ¶åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼Œç¬¬ä¸€é˜¶æ®µé€šè¿‡æ¦‚ç‡å…±è¯†ç­–ç•¥å»ºç«‹ç¾¤ä½“å…±è¯†ï¼Œç¬¬äºŒé˜¶æ®µé€šè¿‡è‡ªé€‚åº”æç¤ºæ•æ‰ä¸“å®¶ç‰¹å®šåå¥½ã€‚åœ¨å…¬å…±æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†å‰²é¢ä¸´æ ‡æ³¨å·®å¼‚æ€§æŒ‘æˆ˜ï¼Œæˆå› äºæˆåƒè¾¹ç•Œæ¨¡ç³Šå’Œä¸´åºŠä¸“å®¶æ„è§å·®å¼‚ã€‚</li>
<li>ä¼ ç»Ÿæ·±åº¦å­¦ä¹ æ–¹æ³•æ— æ³•æœ‰æ•ˆæ•æ‰æ ‡æ³¨è€…åå·®ã€‚</li>
<li>DiffOSegæ¡†æ¶åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼Œæ—¨åœ¨åŒæ—¶å®ç°å…±è¯†é©±åŠ¨å’Œåå¥½é©±åŠ¨çš„åˆ†å‰²ã€‚</li>
<li>ç¬¬ä¸€é˜¶æ®µé€šè¿‡æ¦‚ç‡å…±è¯†ç­–ç•¥å»ºç«‹ç¾¤ä½“å…±è¯†ã€‚</li>
<li>ç¬¬äºŒé˜¶æ®µé€šè¿‡è‡ªé€‚åº”æç¤ºæ•æ‰ä¸“å®¶ç‰¹å®šåå¥½ã€‚</li>
<li>DiffOSegåœ¨å…¬å…±æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.13087">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_åŒ»å­¦å›¾åƒ/2507.13087v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_åŒ»å­¦å›¾åƒ/2507.13087v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_åŒ»å­¦å›¾åƒ/2507.13087v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="A-versatile-foundation-model-for-cine-cardiac-magnetic-resonance-image-analysis-tasks"><a href="#A-versatile-foundation-model-for-cine-cardiac-magnetic-resonance-image-analysis-tasks" class="headerlink" title="A versatile foundation model for cine cardiac magnetic resonance image   analysis tasks"></a>A versatile foundation model for cine cardiac magnetic resonance image   analysis tasks</h2><p><strong>Authors:Yunguan Fu, Wenjia Bai, Weixi Yi, Charlotte Manisty, Anish N Bhuva, Thomas A Treibel, James C Moon, Matthew J Clarkson, Rhodri Huw Davies, Yipeng Hu</strong></p>
<p>Here we present a versatile foundation model that can perform a range of clinically-relevant image analysis tasks, including segmentation, landmark localisation, diagnosis, and prognostication. A multi-view convolution-transformer masked autoencoder, named as CineMA, was trained on 15 million cine images from 74,916 subjects. The model was validated on multiple image analysis tasks and compared to existing models on &gt;4,500 images from eight independent datasets with diverse population characteristics, representing the largest benchmark study for cine CMR so far. CineMA consistently outperformed conventional convolutional neural networks (CNNs) in delineating ventricular boundaries and estimating ejection fraction, a key measure of cardiac function. The improved performance was preserved, even when the model only used half of fine-tuning data. CineMA also surpassed CNNs in disease detection and matched their performance in long-axis function measurement. Interestingly, we found that CineMA can also detect cardiac changes in systemic diseases, such as diabetes, hypertension and cancer, and can also predict mortality. Finally, we assessed model fairness and demonstrated consistent model performance across demographic subgroups. These findings highlight CineMAâ€™s accuracy, learning efficiency, adaptability, and fairness, underscoring its potential as a foundation model for automated cardiac image analysis to support clinical workflow and cardiovascular research. All training and inference code and models are made publicly available at <a target="_blank" rel="noopener" href="https://github.com/mathpluscode/CineMA">https://github.com/mathpluscode/CineMA</a>. </p>
<blockquote>
<p>åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å±•ç¤ºäº†ä¸€ä¸ªé€šç”¨åŸºç¡€æ¨¡å‹ï¼Œå¯ä»¥æ‰§è¡Œä¸€ç³»åˆ—ä¸ä¸´åºŠç›¸å…³çš„å›¾åƒåˆ†æä»»åŠ¡ï¼ŒåŒ…æ‹¬åˆ†å‰²ã€åœ°æ ‡å®šä½ã€è¯Šæ–­å’Œé¢„åã€‚è¯¥æ¨¡å‹é‡‡ç”¨å¤šè§†è§’å·ç§¯è½¬æ¢å™¨æ©ç è‡ªç¼–ç å™¨ï¼Œå‘½åä¸ºCineMAï¼Œåœ¨æ¥è‡ª74916ä¸ªä¸»ä½“çš„1500ä¸‡ç”µå½±å›¾åƒä¸Šè¿›è¡Œè®­ç»ƒã€‚è¯¥æ¨¡å‹åœ¨å¤šä¸ªå›¾åƒåˆ†æä»»åŠ¡ä¸Šè¿›è¡Œäº†éªŒè¯ï¼Œå¹¶åœ¨æ¥è‡ªå…«ä¸ªç‹¬ç«‹æ•°æ®é›†çš„è¶…è¿‡4500å¼ å›¾åƒä¸Šä¸ç°æœ‰æ¨¡å‹è¿›è¡Œäº†æ¯”è¾ƒï¼Œä»£è¡¨äº†è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„ç”µå½±CMRåŸºå‡†æµ‹è¯•ã€‚CineMAåœ¨æç»˜å¿ƒå®¤è¾¹ç•Œå’Œä¼°è®¡å°„è¡€åˆ†æ•°æ–¹é¢å§‹ç»ˆä¼˜äºä¼ ç»Ÿçš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ï¼Œå°„è¡€åˆ†æ•°æ˜¯å¿ƒè„åŠŸèƒ½çš„å…³é”®æŒ‡æ ‡ã€‚å³ä½¿æ¨¡å‹åªä½¿ç”¨ä¸€åŠçš„ç²¾è°ƒæ•°æ®ï¼Œå…¶æ€§èƒ½ä¹Ÿå¾—ä»¥ä¿æŒã€‚CineMAåœ¨ç–¾ç—…æ£€æµ‹æ–¹é¢ä¹Ÿè¶…è¶Šäº†CNNï¼Œå¹¶åœ¨é•¿è½´åŠŸèƒ½æµ‹é‡æ–¹é¢ä¸CNNæ€§èƒ½ç›¸åŒ¹é…ã€‚æœ‰è¶£çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç°CineMAè¿˜å¯ä»¥æ£€æµ‹ç³–å°¿ç—…ã€é«˜è¡€å‹å’Œç™Œç—‡ç­‰ç³»ç»Ÿæ€§ç–¾ç—…çš„å¿ƒè„å˜åŒ–ï¼Œå¹¶å¯ä»¥è¿›è¡Œæ­»äº¡é¢„æµ‹ã€‚æœ€åï¼Œæˆ‘ä»¬å¯¹æ¨¡å‹å…¬å¹³æ€§è¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶è¯æ˜äº†å…¶åœ¨å„äººå£äºšç»„ä¸­çš„æ¨¡å‹æ€§èƒ½è¡¨ç°ä¸€è‡´ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†CineMAåœ¨å‡†ç¡®æ€§ã€å­¦ä¹ æ•ˆç‡ã€é€‚åº”æ€§å’Œå…¬å¹³æ€§æ–¹é¢çš„ä¼˜åŠ¿ï¼Œçªæ˜¾äº†å…¶ä½œä¸ºè‡ªåŠ¨åŒ–å¿ƒè„å›¾åƒåˆ†æåŸºç¡€æ¨¡å‹çš„æ½œåŠ›ï¼Œå¯æ”¯æŒä¸´åºŠå·¥ä½œæµç¨‹å’Œå¿ƒè¡€ç®¡ç ”ç©¶ã€‚æ‰€æœ‰è®­ç»ƒå’Œæ¨ç†ä»£ç å’Œæ¨¡å‹éƒ½å…¬å¼€å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/mathpluscode/CineMA">https://github.com/mathpluscode/CineMA</a>è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.00679v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§å¤šåŠŸèƒ½åŸºç¡€æ¨¡å‹CineMAï¼Œè¯¥æ¨¡å‹å¯åœ¨ä¸´åºŠç›¸å…³çš„å›¾åƒåˆ†æä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼ŒåŒ…æ‹¬åˆ†å‰²ã€åœ°æ ‡å®šä½ã€è¯Šæ–­å’Œé¢„åé¢„æµ‹ã€‚CineMAåœ¨å¤§é‡å¿ƒè„ç”µå½±å›¾åƒæ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¹¶åœ¨å¤šä¸ªç‹¬ç«‹æ•°æ®é›†ä¸Šè¿›è¡Œäº†éªŒè¯ï¼Œè¡¨ç°å‡ºè¾ƒé«˜çš„å‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒCineMAè¿˜èƒ½æ£€æµ‹å…¨èº«æ€§ç–¾ç—…ä¸­çš„å¿ƒè„å˜åŒ–å¹¶è¿›è¡Œæ­»äº¡é¢„æµ‹ã€‚ç ”ç©¶ç»“æœå¼ºè°ƒäº†CineMAçš„å‡†ç¡®æ€§ã€å­¦ä¹ æ•ˆç‡ã€é€‚åº”æ€§å’Œå…¬å¹³æ€§ï¼Œå¯ä½œä¸ºè‡ªåŠ¨åŒ–å¿ƒè„å›¾åƒåˆ†æçš„åŸºç¡€æ¨¡å‹ï¼Œæ”¯æŒä¸´åºŠå·¥ä½œå’Œå¿ƒè¡€ç®¡ç ”ç©¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CineMAæ˜¯ä¸€ç§å¤šåŠŸèƒ½çš„åŒ»å­¦å›¾åƒåˆ†ææ¨¡å‹ï¼Œèƒ½å¤Ÿæ‰§è¡ŒåŒ…æ‹¬åˆ†å‰²ã€åœ°æ ‡å®šä½ã€è¯Šæ–­å’Œé¢„åé¢„æµ‹ç­‰ä»»åŠ¡ã€‚</li>
<li>CineMAåœ¨å¤§é‡å¿ƒè„ç”µå½±å›¾åƒæ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¹¶è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚</li>
<li>ä¸ç°æœ‰æ¨¡å‹ç›¸æ¯”ï¼ŒCineMAåœ¨å¿ƒå®¤è¾¹ç•Œæç»˜å’Œå°„è¡€åˆ†æ•°ä¼°è®¡æ–¹é¢è¡¨ç°æ›´ä¼˜ç§€ã€‚</li>
<li>CineMAåœ¨ç–¾ç—…æ£€æµ‹å’Œé•¿æœŸåŠŸèƒ½æµ‹é‡æ–¹é¢ä¹Ÿæœ‰å‡ºè‰²çš„è¡¨ç°ã€‚</li>
<li>CineMAèƒ½å¤Ÿæ£€æµ‹å…¨èº«æ€§ç–¾ç—…ä¸­çš„å¿ƒè„å˜åŒ–ï¼Œå¹¶è¿›è¡Œæ­»äº¡é¢„æµ‹ã€‚</li>
<li>CineMAå…·æœ‰å‡†ç¡®æ€§ã€å­¦ä¹ æ•ˆç‡å’Œé€‚åº”æ€§å¼ºçš„ç‰¹ç‚¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.00679">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_åŒ»å­¦å›¾åƒ/2506.00679v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_åŒ»å­¦å›¾åƒ/2506.00679v2/page_4_0.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-12/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-12/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-12/TTS/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_TTS/2508.16665v3/page_0_0.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-12  Streaming Sequence-to-Sequence Learning with Delayed Streams Modeling
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-12
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-12/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_ç‰™é½¿ä¿®å¤/2509.07923v1/page_2_0.jpg" class="responsive-img" alt="ç‰™é½¿ä¿®å¤">
                        
                        <span class="card-title">ç‰™é½¿ä¿®å¤</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            ç‰™é½¿ä¿®å¤ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-12  Multimodal Contrastive Pretraining of CBCT and IOS for Enhanced Tooth   Segmentation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-12
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/" class="post-category">
                                    ç‰™é½¿ä¿®å¤
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                        <span class="chip bg-color">ç‰™é½¿ä¿®å¤</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">30055.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
