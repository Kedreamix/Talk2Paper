<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-12  A Survey of Reinforcement Learning for Large Reasoning Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_LLM/2507.02253v4/page_1_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-12
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-07
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    84 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-09-12-æ›´æ–°"><a href="#2025-09-12-æ›´æ–°" class="headerlink" title="2025-09-12 æ›´æ–°"></a>2025-09-12 æ›´æ–°</h1><h2 id="A-Survey-of-Reinforcement-Learning-for-Large-Reasoning-Models"><a href="#A-Survey-of-Reinforcement-Learning-for-Large-Reasoning-Models" class="headerlink" title="A Survey of Reinforcement Learning for Large Reasoning Models"></a>A Survey of Reinforcement Learning for Large Reasoning Models</h2><p><strong>Authors:Kaiyan Zhang, Yuxin Zuo, Bingxiang He, Youbang Sun, Runze Liu, Che Jiang, Yuchen Fan, Kai Tian, Guoli Jia, Pengfei Li, Yu Fu, Xingtai Lv, Yuchen Zhang, Sihang Zeng, Shang Qu, Haozhan Li, Shijie Wang, Yuru Wang, Xinwei Long, Fangfu Liu, Xiang Xu, Jiaze Ma, Xuekai Zhu, Ermo Hua, Yihao Liu, Zonglin Li, Huayu Chen, Xiaoye Qu, Yafu Li, Weize Chen, Zhenzhao Yuan, Junqi Gao, Dong Li, Zhiyuan Ma, Ganqu Cui, Zhiyuan Liu, Biqing Qi, Ning Ding, Bowen Zhou</strong></p>
<p>In this paper, we survey recent advances in Reinforcement Learning (RL) for reasoning with Large Language Models (LLMs). RL has achieved remarkable success in advancing the frontier of LLM capabilities, particularly in addressing complex logical tasks such as mathematics and coding. As a result, RL has emerged as a foundational methodology for transforming LLMs into LRMs. With the rapid progress of the field, further scaling of RL for LRMs now faces foundational challenges not only in computational resources but also in algorithm design, training data, and infrastructure. To this end, it is timely to revisit the development of this domain, reassess its trajectory, and explore strategies to enhance the scalability of RL toward Artificial SuperIntelligence (ASI). In particular, we examine research applying RL to LLMs and LRMs for reasoning abilities, especially since the release of DeepSeek-R1, including foundational components, core problems, training resources, and downstream applications, to identify future opportunities and directions for this rapidly evolving area. We hope this review will promote future research on RL for broader reasoning models. Github: <a target="_blank" rel="noopener" href="https://github.com/TsinghuaC3I/Awesome-RL-for-LRMs">https://github.com/TsinghuaC3I/Awesome-RL-for-LRMs</a> </p>
<blockquote>
<p>åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æ¦‚è¿°äº†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†æ–¹é¢çš„æœ€æ–°è¿›å±•ã€‚å¼ºåŒ–å­¦ä¹ åœ¨æ¨è¿›å¤§å‹è¯­è¨€æ¨¡å‹èƒ½åŠ›æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æˆæœï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†æ•°å­¦å’Œç¼–ç ç­‰å¤æ‚é€»è¾‘ä»»åŠ¡æ–¹é¢ã€‚å› æ­¤ï¼Œå¼ºåŒ–å­¦ä¹ å·²ç»æˆä¸ºå°†å¤§å‹è¯­è¨€æ¨¡å‹è½¬åŒ–ä¸ºè¯­è¨€èµ„æºæ¨¡å‹ï¼ˆLRMï¼‰çš„åŸºç¡€æ–¹æ³•ã€‚éšç€è¯¥é¢†åŸŸçš„å¿«é€Ÿå‘å±•ï¼Œå¼ºåŒ–å­¦ä¹ åœ¨è¯­è¨€èµ„æºæ¨¡å‹ä¸­çš„è¿›ä¸€æ­¥æ‰©å±•ç°åœ¨é¢ä¸´ç€ä¸ä»…æ˜¯è®¡ç®—èµ„æºè€Œä¸”è¿˜åœ¨ç®—æ³•è®¾è®¡ã€è®­ç»ƒæ•°æ®å’ŒåŸºç¡€è®¾æ–½æ–¹é¢çš„åŸºç¡€æŒ‘æˆ˜ã€‚å› æ­¤ï¼Œé‡æ–°å®¡è§†è¯¥é¢†åŸŸçš„å‘å±•ã€é‡æ–°è¯„ä¼°å…¶è½¨è¿¹ä»¥åŠæ¢ç´¢æé«˜å¼ºåŒ–å­¦ä¹ åœ¨äººå·¥æ™ºèƒ½è¶…æ™ºèƒ½ï¼ˆASIï¼‰æ–¹é¢çš„å¯æ‰©å±•æ€§çš„ç­–ç•¥æ˜¯åŠæ—¶çš„ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬ç ”ç©¶äº†è‡ªDeepSeek-R1å‘å¸ƒä»¥æ¥ï¼Œå°†å¼ºåŒ–å­¦ä¹ åº”ç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹å’Œè¯­è¨€èµ„æºæ¨¡å‹ä»¥æé«˜æ¨ç†èƒ½åŠ›çš„ç›¸å…³ç ”ç©¶ï¼ŒåŒ…æ‹¬åŸºç¡€ç»„ä»¶ã€æ ¸å¿ƒé—®é¢˜ã€è®­ç»ƒèµ„æºå’Œä¸‹æ¸¸åº”ç”¨ï¼Œä»¥ç¡®å®šè¿™ä¸€å¿«é€Ÿæ¼”å˜é¢†åŸŸçš„æœªæ¥æœºé‡å’Œæ–¹å‘ã€‚æˆ‘ä»¬å¸Œæœ›è¿™æ¬¡å›é¡¾èƒ½å¤Ÿä¿ƒè¿›å¼ºåŒ–å­¦ä¹ åœ¨æ›´å¹¿æ³›çš„æ¨ç†æ¨¡å‹æ–¹é¢çš„æœªæ¥ç ”ç©¶ã€‚Githubï¼š<a target="_blank" rel="noopener" href="https://github.com/TsinghuaC3I/Awesome-RL-for-LRMs">https://github.com/TsinghuaC3I/Awesome-RL-for-LRMs</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.08827v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¼ºåŒ–å­¦ä¹ åœ¨æ¨åŠ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èƒ½åŠ›æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆæ•ˆï¼Œç‰¹åˆ«æ˜¯åœ¨è§£å†³æ•°å­¦å’Œç¼–ç ç­‰å¤æ‚é€»è¾‘ä»»åŠ¡ä¸Šè¡¨ç°çªå‡ºã€‚ä»å°†LLMè½¬å˜ä¸ºè¯­è¨€æ¨ç†æ¨¡å‹ï¼ˆLRMï¼‰çš„æ–¹æ³•è®ºè§’åº¦çœ‹ï¼Œå¼ºåŒ–å­¦ä¹ æ‰®æ¼”äº†æ ¸å¿ƒè§’è‰²ã€‚ç„¶è€Œï¼Œéšç€é¢†åŸŸå‘å±•åŠ é€Ÿï¼Œå¼ºåŒ–å­¦ä¹ åœ¨å¯æ‰©å±•æ€§æ–¹é¢é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨è®¡ç®—èµ„æºã€ç®—æ³•è®¾è®¡ã€è®­ç»ƒæ•°æ®å’ŒåŸºç¡€è®¾æ–½æ–¹é¢ã€‚æœ¬æ–‡å›é¡¾äº†å¼ºåŒ–å­¦ä¹ åœ¨LLMå’ŒLRMä¸­çš„ç ”ç©¶ä¸åº”ç”¨å†ç¨‹ï¼Œå¹¶æ¢è®¨äº†å¢å¼ºå…¶åœ¨æœäººå·¥æ™ºèƒ½è¶…æ™ºï¼ˆASIï¼‰å‘å±•æ–¹é¢å¯æ‰©å±•æ€§çš„ç­–ç•¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨æ¨åŠ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èƒ½åŠ›ä¸Šå–å¾—äº†æ˜¾è‘—è¿›æ­¥ï¼Œå°¤å…¶åœ¨å¤„ç†å¤æ‚é€»è¾‘ä»»åŠ¡æ—¶è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>RLæˆä¸ºå°†LLMè½¬åŒ–ä¸ºè¯­è¨€æ¨ç†æ¨¡å‹ï¼ˆLRMï¼‰çš„å…³é”®æ–¹æ³•è®ºã€‚</li>
<li>RLåœ¨å¯æ‰©å±•æ€§æ–¹é¢é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨è®¡ç®—èµ„æºã€ç®—æ³•è®¾è®¡ã€è®­ç»ƒæ•°æ®å’ŒåŸºç¡€è®¾æ–½ç­‰æ–¹é¢ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ åœ¨LLMå’ŒLRMé¢†åŸŸçš„åº”ç”¨ä¸ç ”ç©¶æ­£åœ¨ä¸æ–­å‘å±•å’Œæ¼”å˜ã€‚</li>
<li>æœ¬æ–‡å›é¡¾äº†å¼ºåŒ–å­¦ä¹ åœ¨è¯¥é¢†åŸŸçš„å†ç¨‹ï¼Œå¹¶æ¢è®¨äº†å¦‚ä½•å¢å¼ºå…¶åœ¨äººå·¥æ™ºèƒ½è¶…æ™ºï¼ˆASIï¼‰æ–¹é¢çš„å¯æ‰©å±•æ€§ã€‚</li>
<li>é€šè¿‡å¯¹åŒ…æ‹¬åŸºç¡€ç»„ä»¶ã€æ ¸å¿ƒé—®é¢˜ã€è®­ç»ƒèµ„æºå’Œä¸‹æ¸¸åº”ç”¨ç­‰åœ¨å†…çš„å…¨é¢ç ”ç©¶ï¼Œæœ¬æ–‡ä¸ºè¿™ä¸€å¿«é€Ÿæ¼”å˜çš„é¢†åŸŸæä¾›äº†æœªæ¥æœºä¼šå’Œæ–¹å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.08827">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_LLM/2509.08827v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_LLM/2509.08827v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_LLM/2509.08827v1/page_5_0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Large-Language-Model-Hacking-Quantifying-the-Hidden-Risks-of-Using-LLMs-for-Text-Annotation"><a href="#Large-Language-Model-Hacking-Quantifying-the-Hidden-Risks-of-Using-LLMs-for-Text-Annotation" class="headerlink" title="Large Language Model Hacking: Quantifying the Hidden Risks of Using LLMs   for Text Annotation"></a>Large Language Model Hacking: Quantifying the Hidden Risks of Using LLMs   for Text Annotation</h2><p><strong>Authors:Joachim Baumann, Paul RÃ¶ttger, Aleksandra Urman, Albert WendsjÃ¶, Flor Miriam Plaza-del-Arco, Johannes B. Gruber, Dirk Hovy</strong></p>
<p>Large language models (LLMs) are rapidly transforming social science research by enabling the automation of labor-intensive tasks like data annotation and text analysis. However, LLM outputs vary significantly depending on the implementation choices made by researchers (e.g., model selection, prompting strategy, or temperature settings). Such variation can introduce systematic biases and random errors, which propagate to downstream analyses and cause Type I, Type II, Type S, or Type M errors. We call this LLM hacking.   We quantify the risk of LLM hacking by replicating 37 data annotation tasks from 21 published social science research studies with 18 different models. Analyzing 13 million LLM labels, we test 2,361 realistic hypotheses to measure how plausible researcher choices affect statistical conclusions. We find incorrect conclusions based on LLM-annotated data in approximately one in three hypotheses for state-of-the-art models, and in half the hypotheses for small language models. While our findings show that higher task performance and better general model capabilities reduce LLM hacking risk, even highly accurate models do not completely eliminate it. The risk of LLM hacking decreases as effect sizes increase, indicating the need for more rigorous verification of findings near significance thresholds. Our extensive analysis of LLM hacking mitigation techniques emphasizes the importance of human annotations in reducing false positive findings and improving model selection. Surprisingly, common regression estimator correction techniques are largely ineffective in reducing LLM hacking risk, as they heavily trade off Type I vs. Type II errors.   Beyond accidental errors, we find that intentional LLM hacking is unacceptably simple. With few LLMs and just a handful of prompt paraphrases, anything can be presented as statistically significant. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ­£åœ¨è¿…é€Ÿæ”¹å˜ç¤¾ä¼šç§‘å­¦ç ”ç©¶ï¼Œä½¿æ•°æ®æ ‡æ³¨å’Œæ–‡æœ¬åˆ†æç­‰åŠ³åŠ¨å¯†é›†å‹ä»»åŠ¡çš„è‡ªåŠ¨åŒ–æˆä¸ºå¯èƒ½ã€‚ç„¶è€Œï¼ŒLLMçš„è¾“å‡ºç»“æœå› ç ”ç©¶è€…æ‰€åšçš„å®æ–½é€‰æ‹©ï¼ˆä¾‹å¦‚æ¨¡å‹é€‰æ‹©ã€æç¤ºç­–ç•¥æˆ–æ¸©åº¦è®¾ç½®ï¼‰è€Œæœ‰å¾ˆå¤§å·®å¼‚ã€‚è¿™ç§å·®å¼‚å¯èƒ½ä¼šå¼•å…¥ç³»ç»Ÿæ€§åè§å’Œéšæœºé”™è¯¯ï¼Œå¹¶ä¼ æ’­åˆ°ä¸‹æ¸¸åˆ†æï¼Œä»è€Œå¯¼è‡´ç¬¬ä¸€ç±»ã€ç¬¬äºŒç±»ã€Sç±»æˆ–Mç±»é”™è¯¯ã€‚æˆ‘ä»¬å°†è¿™ç§ç°è±¡ç§°ä¸ºLLMé»‘å®¢æ”»å‡»ã€‚æˆ‘ä»¬é€šè¿‡å¤åˆ¶æ¥è‡ª21é¡¹å·²å‘å¸ƒçš„ç¤¾ä¼šç§‘å­¦ç ”ç©¶ä¸­çš„37ä¸ªæ•°æ®æ ‡æ³¨ä»»åŠ¡ï¼Œå¹¶ä½¿ç”¨18ä¸ªä¸åŒçš„æ¨¡å‹æ¥é‡åŒ–LLMé»‘å®¢æ”»å‡»çš„é£é™©ã€‚é€šè¿‡åˆ†æ1300ä¸‡ä¸ªLLMæ ‡ç­¾å’Œæµ‹è¯•äº†2361ä¸ªåˆç†çš„å‡è®¾ï¼Œæˆ‘ä»¬è¡¡é‡äº†ç ”ç©¶äººå‘˜çš„é€‰æ‹©å¦‚ä½•å½±å“ç»Ÿè®¡ç»“è®ºã€‚æˆ‘ä»¬å‘ç°ï¼ŒåŸºäºLLMæ ‡æ³¨æ•°æ®çš„é”™è¯¯ç»“è®ºçº¦å ä¸‰åˆ†ä¹‹ä¸€å‡è®¾çš„ç»“è®ºåœ¨ä½¿ç”¨å…ˆè¿›æ¨¡å‹æ—¶ï¼Œåœ¨å°æ¨¡å‹å‡è®¾ä¸­çº¦å ä¸€åŠã€‚è™½ç„¶æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæ›´é«˜çš„ä»»åŠ¡æ€§èƒ½å’Œæ›´å¥½çš„æ¨¡å‹æ•´ä½“èƒ½åŠ›å¯ä»¥é™ä½LLMé»‘å®¢æ”»å‡»çš„é£é™©ï¼Œä½†å³ä½¿æ˜¯é«˜åº¦å‡†ç¡®çš„æ¨¡å‹ä¹Ÿæ— æ³•å®Œå…¨æ¶ˆé™¤é£é™©ã€‚éšç€æ•ˆåº”å¤§å°çš„å¢åŠ ï¼ŒLLMé»‘å®¢æ”»å‡»çš„é£é™©é™ä½ï¼Œè¿™è¡¨æ˜åœ¨æ˜¾è‘—æ€§é˜ˆå€¼é™„è¿‘éœ€è¦å¯¹å‘ç°è¿›è¡Œæ›´ä¸¥æ ¼çš„éªŒè¯ã€‚æˆ‘ä»¬å¯¹LLMé»‘å®¢æ”»å‡»ç¼“è§£æŠ€æœ¯çš„ç»¼åˆåˆ†æå¼ºè°ƒäº†äººç±»æ ‡æ³¨åœ¨å‡å°‘å‡é˜³æ€§å‘ç°å’Œæ”¹è¿›æ¨¡å‹é€‰æ‹©ä¸­çš„é‡è¦æ€§ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œå¸¸è§çš„å›å½’ä¼°è®¡æ ¡æ­£æŠ€æœ¯åœ¨é™ä½LLMé»‘å®¢æ”»å‡»é£é™©æ–¹é¢å‡ ä¹æ— æ•ˆï¼Œå› ä¸ºå®ƒä»¬ä¼šå¤§é‡æƒè¡¡ç¬¬ä¸€ç±»ä¸ç¬¬äºŒç±»é”™è¯¯ä¹‹é—´çš„å¹³è¡¡ã€‚é™¤äº†å¶ç„¶æ€§é”™è¯¯å¤–ï¼Œæˆ‘ä»¬å‘ç°æ•…æ„è¿›è¡Œçš„LLMé»‘å®¢æ”»å‡»ä¹Ÿæå…¶å®¹æ˜“ä¸”éš¾ä»¥æ¥å—ã€‚åªéœ€è¦å°‘é‡çš„LLMå’Œä¸€äº›æç¤ºè¯­çš„é‡è¿°ï¼Œä»»ä½•äº‹æƒ…éƒ½å¯ä»¥è¢«å‘ˆç°ä¸ºå…·æœ‰ç»Ÿè®¡å­¦ä¸Šçš„æ˜¾è‘—æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.08825v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è‡ªåŠ¨åŒ–åŠŸèƒ½ä¸ºç¤¾ä¼šç§‘å­¦ç ”ç©¶å¸¦æ¥äº†å˜é©ï¼Œä½†ä¹Ÿå¸¦æ¥äº†æ–°çš„é—®é¢˜ã€‚LLMçš„è¾“å‡ºç»“æœå› ç ”ç©¶äººå‘˜çš„å®æ–½é€‰æ‹©ï¼ˆå¦‚æ¨¡å‹é€‰æ‹©ã€æç¤ºç­–ç•¥æˆ–æ¸©åº¦è®¾ç½®ï¼‰è€Œæœ‰å¾ˆå¤§å·®å¼‚ï¼Œè¿™å¯èƒ½å¯¼è‡´ç±»å‹Iã€ç±»å‹IIã€ç±»å‹Sæˆ–ç±»å‹Mçš„é”™è¯¯ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºLLMé»‘å®¢ã€‚æœ¬ç ”ç©¶é€šè¿‡å¤åˆ¶21é¡¹å·²å‘å¸ƒçš„ç¤¾ä¼šç§‘å­¦ç ”ç©¶ä¸­çš„37é¡¹æ•°æ®æ ‡æ³¨ä»»åŠ¡ï¼Œä½¿ç”¨18ä¸ªä¸åŒçš„æ¨¡å‹è¿›è¡Œäº†é‡åŒ–åˆ†æã€‚æˆ‘ä»¬å‘ç°ï¼ŒåŸºäºLLMæ ‡æ³¨æ•°æ®å¾—å‡ºçš„é”™è¯¯ç»“è®ºåœ¨å…ˆè¿›æ¨¡å‹çš„ä¸€åŠå‡è®¾ä¸­éƒ½æœ‰å‡ºç°ï¼Œå°å‹è¯­è¨€æ¨¡å‹çš„å‡è®¾ä¸­é”™è¯¯ç»“è®ºå‡ºç°æ¯”ä¾‹æ›´é«˜ã€‚è™½ç„¶é«˜æ€§èƒ½ä»»åŠ¡å’Œæ›´å¥½çš„æ¨¡å‹èƒ½åŠ›å¯ä»¥é™ä½LLMé»‘å®¢é£é™©ï¼Œä½†æ— æ³•å®Œå…¨æ¶ˆé™¤ã€‚æœ¬ç ”ç©¶å¼ºè°ƒäº†äººç±»æ³¨é‡Šåœ¨å‡å°‘è¯¯æŠ¥å’Œæ”¹è¿›æ¨¡å‹é€‰æ‹©ä¸­çš„é‡è¦æ€§ã€‚å¸¸è§çš„å›å½’ä¼°è®¡æ ¡æ­£æŠ€æœ¯åœ¨é™ä½LLMé»‘å®¢é£é™©æ–¹é¢æ•ˆæœä¸ä½³ã€‚æ­¤å¤–ï¼Œç ”ç©¶å‘ç°æœ‰æ„ä¸ºä¹‹çš„LLMé»‘å®¢è¡Œä¸ºéå¸¸ç®€å•ï¼Œåªéœ€å°‘é‡LLMå’Œæç¤ºå˜ä½“èƒ½è½»æ˜“åˆ¶é€ ç»Ÿè®¡æ˜¾è‘—çš„ç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨ç¤¾ä¼šç§‘å­¦ç ”ç©¶ä¸­çš„åº”ç”¨è™½ç„¶å¸¦æ¥äº†æ•ˆç‡æå‡ï¼Œä½†ä¹Ÿå¯èƒ½å¯¼è‡´LLMé»‘å®¢é—®é¢˜ï¼Œå³å› æ¨¡å‹å®æ–½é€‰æ‹©å·®å¼‚å¯¼è‡´çš„ç³»ç»Ÿæ€§åè§å’Œéšæœºé”™è¯¯ã€‚</li>
<li>LLMæ ‡æ³¨æ•°æ®çš„é”™è¯¯å¯èƒ½å¯¼è‡´ä¸‹æ¸¸åˆ†æçš„é”™è¯¯ç»“è®ºï¼ŒåŒ…æ‹¬ç±»å‹Iã€ç±»å‹IIã€ç±»å‹Så’Œç±»å‹Mçš„é”™è¯¯ã€‚</li>
<li>é«˜æ€§èƒ½ä»»åŠ¡å’Œæ›´å¥½çš„æ¨¡å‹èƒ½åŠ›å¯ä»¥é™ä½LLMé»‘å®¢é£é™©ï¼Œä½†æ— æ³•å®Œå…¨æ¶ˆé™¤ã€‚</li>
<li>æ¥è¿‘æ˜¾è‘—æ€§é˜ˆå€¼çš„ç»“æœéœ€è¦æ›´ä¸¥æ ¼çš„éªŒè¯ï¼Œå› ä¸ºæ•ˆåº”å¤§å°å¢åŠ å¯ä»¥å‡å°‘LLMé»‘å®¢é£é™©ã€‚</li>
<li>äººç±»æ³¨é‡Šåœ¨å‡å°‘è¯¯æŠ¥å’Œæ”¹è¿›æ¨¡å‹é€‰æ‹©æ–¹é¢è‡³å…³é‡è¦ã€‚</li>
<li>å¸¸è§çš„å›å½’ä¼°è®¡æ ¡æ­£æŠ€æœ¯åœ¨é™ä½LLMé»‘å®¢é£é™©æ–¹é¢æ•ˆæœæœ‰é™ï¼Œå› ä¸ºå®ƒä»¬å¯èƒ½åœ¨ä¸åŒç¨‹åº¦ä¸Šæƒè¡¡äº†ç±»å‹Iå’Œç±»å‹IIé”™è¯¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.08825">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_LLM/2509.08825v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_LLM/2509.08825v1/page_1_0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Calibrating-MLLM-as-a-judge-via-Multimodal-Bayesian-Prompt-Ensembles"><a href="#Calibrating-MLLM-as-a-judge-via-Multimodal-Bayesian-Prompt-Ensembles" class="headerlink" title="Calibrating MLLM-as-a-judge via Multimodal Bayesian Prompt Ensembles"></a>Calibrating MLLM-as-a-judge via Multimodal Bayesian Prompt Ensembles</h2><p><strong>Authors:Eric Slyman, Mehrab Tanjim, Kushal Kafle, Stefan Lee</strong></p>
<p>Multimodal large language models (MLLMs) are increasingly used to evaluate text-to-image (TTI) generation systems, providing automated judgments based on visual and textual context. However, these â€œjudgeâ€ models often suffer from biases, overconfidence, and inconsistent performance across diverse image domains. While prompt ensembling has shown promise for mitigating these issues in unimodal, text-only settings, our experiments reveal that standard ensembling methods fail to generalize effectively for TTI tasks. To address these limitations, we propose a new multimodal-aware method called Multimodal Mixture-of-Bayesian Prompt Ensembles (MMB). Our method uses a Bayesian prompt ensemble approach augmented by image clustering, allowing the judge to dynamically assign prompt weights based on the visual characteristics of each sample. We show that MMB improves accuracy in pairwise preference judgments and greatly enhances calibration, making it easier to gauge the judgeâ€™s true uncertainty. In evaluations on two TTI benchmarks, HPSv2 and MJBench, MMB outperforms existing baselines in alignment with human annotations and calibration across varied image content. Our findings highlight the importance of multimodal-specific strategies for judge calibration and suggest a promising path forward for reliable large-scale TTI evaluation. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰è¶Šæ¥è¶Šå¤šåœ°è¢«ç”¨äºè¯„ä¼°æ–‡æœ¬åˆ°å›¾åƒï¼ˆTTIï¼‰ç”Ÿæˆç³»ç»Ÿï¼ŒåŸºäºè§†è§‰å’Œæ–‡æœ¬ä¸Šä¸‹æ–‡æä¾›è‡ªåŠ¨åŒ–åˆ¤æ–­ã€‚ç„¶è€Œï¼Œè¿™äº›â€œåˆ¤æ–­â€æ¨¡å‹å¸¸å¸¸å—åˆ°åè§ã€è¿‡åº¦è‡ªä¿¡ä»¥åŠåœ¨ä¸åŒå›¾åƒé¢†åŸŸè¡¨ç°ä¸ä¸€è‡´çš„å½±å“ã€‚è™½ç„¶æç¤ºé›†æˆæ³•å·²åœ¨å•æ¨¡æ€ã€çº¯æ–‡æœ¬ç¯å¢ƒä¸­æ˜¾ç¤ºå‡ºç¼“è§£è¿™äº›é—®é¢˜çš„å‰æ™¯ï¼Œä½†æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œæ ‡å‡†é›†æˆæ–¹æ³•æ— æ³•æœ‰æ•ˆåœ°æ¨å¹¿åˆ°TTIä»»åŠ¡ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„å¤šæ¨¡æ€æ„ŸçŸ¥æ–¹æ³•ï¼Œç§°ä¸ºå¤šæ¨¡æ€è´å¶æ–¯æ··åˆæç¤ºé›†æˆæ³•ï¼ˆMMBï¼‰ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä½¿ç”¨è´å¶æ–¯æç¤ºé›†æˆæ³•ï¼Œé€šè¿‡å›¾åƒèšç±»è¿›è¡Œå¢å¼ºï¼Œå…è®¸åˆ¤æ–­è€…æ ¹æ®æ¯ä¸ªæ ·æœ¬çš„è§†è§‰ç‰¹å¾åŠ¨æ€åˆ†é…æç¤ºæƒé‡ã€‚æˆ‘ä»¬è¡¨æ˜ï¼ŒMMBåœ¨é…å¯¹åå¥½åˆ¤æ–­ä¸­æé«˜äº†å‡†ç¡®æ€§ï¼Œå¹¶å¤§å¤§æé«˜äº†æ ¡å‡†åº¦ï¼Œä½¿åˆ¤æ–­è€…èƒ½å¤Ÿæ›´å®¹æ˜“åœ°è¡¡é‡å…¶çœŸå®çš„ä¸ç¡®å®šæ€§ã€‚åœ¨HPSv2å’ŒMJBenchä¸¤ä¸ªTTIåŸºå‡†æµ‹è¯•ä¸Šï¼ŒMMBä¼˜äºç°æœ‰åŸºçº¿ï¼Œä¸äººç±»æ³¨é‡Šå’Œè·¨ä¸åŒå›¾åƒå†…å®¹çš„æ ¡å‡†ä¿æŒä¸€è‡´ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œåˆ¤æ–­æ ¡å‡†éœ€è¦é’ˆå¯¹å¤šæ¨¡æ€çš„ç‰¹å®šç­–ç•¥ï¼Œå¹¶ä¸ºå¯é çš„å¤§è§„æ¨¡TTIè¯„ä¼°æä¾›äº†å‰æ™¯å…‰æ˜çš„é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.08777v1">PDF</a> 17 pages, 8 figures, Accepted at ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨æ–‡æœ¬åˆ°å›¾åƒï¼ˆTTIï¼‰ç”Ÿæˆç³»ç»Ÿè¯„ä¼°ä¸­çš„åº”ç”¨ã€‚è™½ç„¶è¿™äº›æ¨¡å‹èƒ½åŸºäºè§†è§‰å’Œæ–‡æœ¬ä¸Šä¸‹æ–‡æä¾›è‡ªåŠ¨åˆ¤æ–­ï¼Œä½†å®ƒä»¬å¸¸å—åˆ°åè§ã€è¿‡åº¦è‡ªä¿¡å’Œä¸ä¸€è‡´æ€§èƒ½çš„å½±å“ã€‚é’ˆå¯¹è¿™äº›é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„å¤šæ¨¡æ€æ„ŸçŸ¥æ–¹æ³•â€”â€”å¤šæ¨¡æ€æ··åˆè´å¶æ–¯æç¤ºé›†æˆï¼ˆMMBï¼‰ã€‚è¯¥æ–¹æ³•ç»“åˆäº†è´å¶æ–¯æç¤ºé›†æˆå’Œå›¾åƒèšç±»ï¼Œä½¿æ¨¡å‹èƒ½æ ¹æ®æ¯ä¸ªæ ·æœ¬çš„è§†è§‰ç‰¹å¾åŠ¨æ€åˆ†é…æç¤ºæƒé‡ã€‚å®éªŒè¡¨æ˜ï¼ŒMMBåœ¨æé«˜æˆå¯¹åå¥½åˆ¤æ–­çš„å‡†ç¡®æ€§å’Œæ ¡å‡†æ€§æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œä¸ºè¯„ä¼°æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆç³»ç»Ÿçš„è´¨é‡æä¾›äº†æ›´å¯é çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰è¢«å¹¿æ³›åº”ç”¨äºæ–‡æœ¬åˆ°å›¾åƒï¼ˆTTIï¼‰ç”Ÿæˆç³»ç»Ÿçš„è¯„ä¼°ã€‚</li>
<li>è¿™äº›æ¨¡å‹å­˜åœ¨åè§ã€è¿‡åº¦è‡ªä¿¡åŠåœ¨ä¸åŒå›¾åƒé¢†åŸŸæ€§èƒ½ä¸ä¸€è‡´çš„é—®é¢˜ã€‚</li>
<li>æ ‡å‡†é›†æˆæ–¹æ³•åœ¨å¤šæ¨¡æ€ç¯å¢ƒä¸‹æ•ˆæœä¸ä½³ï¼Œéœ€è¦æ–°çš„æ–¹æ³•æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„å¤šæ¨¡æ€æ„ŸçŸ¥æ–¹æ³•â€”â€”å¤šæ¨¡æ€æ··åˆè´å¶æ–¯æç¤ºé›†æˆï¼ˆMMBï¼‰ã€‚</li>
<li>MMBé€šè¿‡ç»“åˆè´å¶æ–¯æç¤ºé›†æˆå’Œå›¾åƒèšç±»ï¼Œèƒ½æ ¹æ®æ ·æœ¬çš„è§†è§‰ç‰¹å¾åŠ¨æ€è°ƒæ•´æç¤ºæƒé‡ã€‚</li>
<li>MMBåœ¨æˆå¯¹åå¥½åˆ¤æ–­å’Œæ ¡å‡†æ€§æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œæé«˜äº†è¯„ä¼°çš„å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.08777">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_LLM/2509.08777v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_LLM/2509.08777v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_LLM/2509.08777v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="BcQLM-Efficient-Vision-Language-Understanding-with-Distilled-Q-Gated-Cross-Modal-Fusion"><a href="#BcQLM-Efficient-Vision-Language-Understanding-with-Distilled-Q-Gated-Cross-Modal-Fusion" class="headerlink" title="BcQLM: Efficient Vision-Language Understanding with Distilled Q-Gated   Cross-Modal Fusion"></a>BcQLM: Efficient Vision-Language Understanding with Distilled Q-Gated   Cross-Modal Fusion</h2><p><strong>Authors:Sike Xiang, Shuang Chen, Amir Atapour-Abarghouei</strong></p>
<p>As multimodal large language models (MLLMs) advance, their large-scale architectures pose challenges for deployment in resource-constrained environments. In the age of large models, where energy efficiency, computational scalability and environmental sustainability are paramount, the development of lightweight and high-performance models is critical for real-world applications. As such, we propose a lightweight MLLM framework for end-to-end visual question answering. Our proposed approach centres on BreezeCLIP, a compact yet powerful vision-language encoder optimised for efficient multimodal understanding. With only 1.2 billion parameters overall, our model significantly reduces computational cost while achieving performance comparable to standard-size MLLMs. Experiments conducted on multiple datasets further validate its effectiveness in balancing accuracy and efficiency. The modular and extensible design enables generalisation to broader multimodal tasks. The proposed lightweight vision-language framework is denoted as BcQLM (BreezeCLIP-enhanced Q-Gated Multimodal Language Model). It offers a promising path toward deployable MLLMs under practical hardware constraints. The source code is available at <a target="_blank" rel="noopener" href="https://github.com/thico0224/BcQLM">https://github.com/thico0224/BcQLM</a>. </p>
<blockquote>
<p>éšç€å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„è¿›æ­¥ï¼Œå…¶å¤§è§„æ¨¡æ¶æ„åœ¨èµ„æºå—é™ç¯å¢ƒä¸­éƒ¨ç½²æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚åœ¨å¤§å‹æ¨¡å‹æ—¶ä»£ï¼Œèƒ½æºæ•ˆç‡ã€è®¡ç®—å¯æ‰©å±•æ€§å’Œç¯å¢ƒå¯æŒç»­æ€§è‡³å…³é‡è¦ï¼Œå› æ­¤å¼€å‘è½»ä¾¿ã€é«˜æ€§èƒ½çš„æ¨¡å‹å¯¹äºå®é™…åº”ç”¨è‡³å…³é‡è¦ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºç«¯åˆ°ç«¯è§†è§‰é—®ç­”çš„è½»ä¾¿MLLMæ¡†æ¶ã€‚æˆ‘ä»¬æå‡ºçš„æ–¹æ³•ä»¥BreezeCLIPä¸ºä¸­å¿ƒï¼Œè¿™æ˜¯ä¸€ç§ç´§å‡‘è€Œå¼ºå¤§çš„è§†è§‰è¯­è¨€ç¼–ç å™¨ï¼Œé’ˆå¯¹é«˜æ•ˆçš„å¤šæ¨¡æ€ç†è§£è¿›è¡Œäº†ä¼˜åŒ–ã€‚æˆ‘ä»¬çš„æ¨¡å‹æ€»å‚æ•°åªæœ‰1.2äº¿ï¼Œåœ¨æ˜¾è‘—é™ä½è®¡ç®—æˆæœ¬çš„åŒæ—¶ï¼Œå®ç°äº†ä¸æ ‡å‡†å¤§å°çš„MLLMç›¸å½“çš„æ€§èƒ½ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¿›ä¸€æ­¥éªŒè¯äº†å®ƒåœ¨å¹³è¡¡å‡†ç¡®æ€§å’Œæ•ˆç‡æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æ¨¡å—åŒ–ä¸”å¯æ‰©å±•çš„è®¾è®¡å¯å®ç°æ›´å¹¿æ³›çš„å¤šæ¨¡æ€ä»»åŠ¡çš„é€šç”¨åŒ–ã€‚æ‰€æå‡ºçš„å°å‹åŒ–è§†è§‰è¯­è¨€æ¡†æ¶è¢«å‘½åä¸ºBcQLMï¼ˆç”±BreezeCLIPå¢å¼ºçš„Qé—¨å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼‰ã€‚å®ƒä¸ºåœ¨å®é™…ç¡¬ä»¶çº¦æŸæ¡ä»¶ä¸‹éƒ¨ç½²MLLMæä¾›äº†æœ‰å‰æ™¯çš„è·¯å¾„ã€‚æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/thico0224/BcQLM%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/thico0224/BcQLMæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.08715v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>éšç€å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„å‘å±•ï¼Œå…¶å¤§è§„æ¨¡æ¶æ„åœ¨èµ„æºå—é™ç¯å¢ƒä¸­éƒ¨ç½²æ—¶é¢ä¸´çš„æŒ‘æˆ˜æ—¥ç›Šå‡¸æ˜¾ã€‚ä¸ºè§£å†³èƒ½æºæ•ˆç‡ã€è®¡ç®—å¯ä¼¸ç¼©æ€§å’Œç¯å¢ƒå¯æŒç»­æ€§ç­‰é—®é¢˜ï¼Œå¼€å‘è½»é‡çº§ã€é«˜æ€§èƒ½çš„æ¨¡å‹å¯¹äºå®é™…åº”ç”¨è‡³å…³é‡è¦ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è½»é‡çº§MLLMæ¡†æ¶ï¼Œç”¨äºç«¯åˆ°ç«¯çš„è§†è§‰é—®ç­”ã€‚æˆ‘ä»¬æå‡ºçš„æ–¹æ³•ä»¥BreezeCLIPä¸ºæ ¸å¿ƒï¼Œè¿™æ˜¯ä¸€ç§ç´§å‡‘è€Œå¼ºå¤§çš„è§†è§‰è¯­è¨€ç¼–ç å™¨ï¼Œä¸“ä¸ºé«˜æ•ˆçš„å¤šæ¨¡æ€ç†è§£è€Œä¼˜åŒ–ã€‚æˆ‘ä»¬çš„æ¨¡å‹ä»…æœ‰1.2äº¿ä¸ªå‚æ•°ï¼Œåœ¨ä¿æŒæ€§èƒ½ä¸æ ‡å‡†å¤§å°çš„MLLMç›¸å½“çš„åŒæ—¶ï¼Œå¤§å¤§é™ä½äº†è®¡ç®—æˆæœ¬ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¿›ä¸€æ­¥éªŒè¯äº†å…¶åœ¨å¹³è¡¡ç²¾åº¦å’Œæ•ˆç‡æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚è¯¥æ¨¡å—åŒ–ä¸”å¯æ‰©å±•çš„è®¾è®¡ä½¿å…¶èƒ½å¤Ÿæ³›åŒ–åˆ°æ›´å¹¿æ³›çš„å¤šæ¨¡æ€ä»»åŠ¡ã€‚æ‰€æå‡ºçš„è½»é‡çº§è§†è§‰è¯­è¨€æ¡†æ¶è¢«ç§°ä¸ºBcQLMï¼ˆBreezeCLIPå¢å¼ºå‹Qé—¨æ§å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼‰ï¼Œä¸ºå®ç°å®ç”¨ç¡¬ä»¶çº¦æŸä¸‹çš„å¯éƒ¨ç½²MLLMsæä¾›äº†æœ‰å‰é€”çš„è·¯å¾„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨èµ„æºå—é™ç¯å¢ƒä¸­çš„éƒ¨ç½²å…·æœ‰æŒ‘æˆ˜ã€‚</li>
<li>è½»é‡çº§ã€é«˜æ€§èƒ½çš„æ¨¡å‹å¼€å‘å¯¹äºå®é™…åº”ç”¨è‡³å…³é‡è¦ã€‚</li>
<li>æå‡ºçš„è½»é‡çº§MLLMæ¡†æ¶ä»¥BreezeCLIPä¸ºæ ¸å¿ƒï¼Œä¸“ä¸ºé«˜æ•ˆå¤šæ¨¡æ€ç†è§£è€Œä¼˜åŒ–ã€‚</li>
<li>æ¨¡å‹ä»…æœ‰1.2äº¿ä¸ªå‚æ•°ï¼Œè®¡ç®—æˆæœ¬ä½ï¼Œæ€§èƒ½ä¸æ ‡å‡†MLLMç›¸å½“ã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯äº†å…¶åœ¨å¹³è¡¡ç²¾åº¦å’Œæ•ˆç‡æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>æ¨¡å—åŒ–ä¸”å¯æ‰©å±•çš„è®¾è®¡ä½¿æ¨¡å‹èƒ½å¤Ÿæ³›åŒ–åˆ°æ›´å¹¿æ³›çš„å¤šæ¨¡æ€ä»»åŠ¡ã€‚</li>
<li>BcQLMæ¡†æ¶ä¸ºå®ç°å®ç”¨ç¡¬ä»¶çº¦æŸä¸‹çš„å¯éƒ¨ç½²MLLMsæä¾›äº†å‰æ™¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.08715">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_LLM/2509.08715v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_LLM/2509.08715v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_LLM/2509.08715v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="AdsQA-Towards-Advertisement-Video-Understanding"><a href="#AdsQA-Towards-Advertisement-Video-Understanding" class="headerlink" title="AdsQA: Towards Advertisement Video Understanding"></a>AdsQA: Towards Advertisement Video Understanding</h2><p><strong>Authors:Xinwei Long, Kai Tian, Peng Xu, Guoli Jia, Jingxuan Li, Sa Yang, Yihua Shao, Kaiyan Zhang, Che Jiang, Hao Xu, Yang Liu, Jiaheng Ma, Bowen Zhou</strong></p>
<p>Large language models (LLMs) have taken a great step towards AGI. Meanwhile, an increasing number of domain-specific problems such as math and programming boost these general-purpose models to continuously evolve via learning deeper expertise. Now is thus the time further to extend the diversity of specialized applications for knowledgeable LLMs, though collecting high quality data with unexpected and informative tasks is challenging. In this paper, we propose to use advertisement (ad) videos as a challenging test-bed to probe the ability of LLMs in perceiving beyond the objective physical content of common visual domain. Our motivation is to take full advantage of the clue-rich and information-dense ad videosâ€™ traits, e.g., marketing logic, persuasive strategies, and audience engagement. Our contribution is three-fold: (1) To our knowledge, this is the first attempt to use ad videos with well-designed tasks to evaluate LLMs. We contribute AdsQA, a challenging ad Video QA benchmark derived from 1,544 ad videos with 10,962 clips, totaling 22.7 hours, providing 5 challenging tasks. (2) We propose ReAd-R, a Deepseek-R1 styled RL model that reflects on questions, and generates answers via reward-driven optimization. (3) We benchmark 14 top-tier LLMs on AdsQA, and our \texttt{ReAd-R}~achieves the state-of-the-art outperforming strong competitors equipped with long-chain reasoning capabilities by a clear margin. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æœç€é€šç”¨äººå·¥æ™ºèƒ½ï¼ˆAGIï¼‰è¿ˆå‡ºäº†å·¨å¤§çš„ä¸€æ­¥ã€‚ä¸æ­¤åŒæ—¶ï¼Œè¶Šæ¥è¶Šå¤šçš„ç‰¹å®šé¢†åŸŸé—®é¢˜ï¼Œå¦‚æ•°å­¦å’Œç¼–ç¨‹ï¼Œæ¨åŠ¨è¿™äº›é€šç”¨æ¨¡å‹é€šè¿‡æ·±å…¥å­¦ä¹ ä¸“ä¸šçŸ¥è¯†è€Œä¸æ–­è¿›åŒ–ã€‚å› æ­¤ï¼Œç°åœ¨æ˜¯è¿›ä¸€æ­¥æ‰©å±•çŸ¥è¯†å‹LLMçš„ä¸“ç”¨åº”ç”¨å¤šæ ·æ€§çš„å¥½æ—¶æœºï¼Œå°½ç®¡æ”¶é›†é«˜è´¨é‡çš„æ•°æ®ä»¥åŠå¸¦æœ‰æ„æƒ³ä¸åˆ°å’Œå…·æœ‰ä¿¡æ¯å«é‡çš„ä»»åŠ¡æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å»ºè®®ä½¿ç”¨å¹¿å‘Šï¼ˆå¹¿å‘Šï¼‰è§†é¢‘ä½œä¸ºä¸€ä¸ªå……æ»¡æŒ‘æˆ˜çš„æµ‹è¯•å¹³å°ï¼Œä»¥æ£€æµ‹LLMåœ¨æ„ŸçŸ¥è¶…è¶Šå¸¸è§è§†è§‰åŸŸå®¢è§‚ç‰©ç†å†…å®¹ä¹‹å¤–çš„èƒ½åŠ›ã€‚æˆ‘ä»¬çš„åŠ¨æœºæ˜¯å……åˆ†åˆ©ç”¨å¹¿å‘Šè§†é¢‘ç‰¹å¾ä¸­çš„çº¿ç´¢ä¸°å¯Œå’Œä¿¡æ¯å¯†é›†çš„ç‰¹ç‚¹ï¼Œä¾‹å¦‚è¥é”€é€»è¾‘ã€ç­–ç•¥æŠ€å·§å’Œè§‚ä¼—å‚ä¸åº¦ç­‰ã€‚æˆ‘ä»¬çš„è´¡çŒ®æœ‰ä¸‰ä¸ªå±‚é¢ï¼šé¦–å…ˆï¼Œæ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯é¦–æ¬¡å°è¯•åˆ©ç”¨ç²¾å¿ƒè®¾è®¡çš„ä»»åŠ¡æ¥åˆ©ç”¨å¹¿å‘Šè§†é¢‘å¯¹LLMè¿›è¡Œè¯„ä¼°ã€‚æˆ‘ä»¬è´¡çŒ®AdsQAä½œä¸ºä¸€ä¸ªå¹¿å‘Šè§†é¢‘é—®ç­”çš„åŸºå‡†æµ‹è¯•å¹³å°ï¼Œæ¥æºäºä»æ€»è®¡è¾¾åˆ°è¶…è¿‡å¹¿å‘Šçš„åŸå§‹è§†é¢‘çš„è§†é¢‘ç‰‡æ®µâ€”â€”å³æ”¶é›†å‰ªæˆçš„åˆè®¡è¾¾ä¸‡ä½™å°æ—¶çš„éŸ³é¢‘ææ–™ç»è¿‡åŠ å·¥çš„æ¥è‡ªå½±ç‰‡æµ“ç¼©è€Œæ„å»ºçš„å…±è®¡ä¸€ä¸‡äº”åƒå››ç™¾å››åå››ä¸ªå¹¿å‘Šè§†é¢‘ç‰‡æ®µå’Œåä¸‡ä¹åƒå…­ç™¾äºŒåä¸ªå‰ªè¾‘ç‰‡æ®µï¼Œæä¾›äº”ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æå‡ºäº†ReAd-Ræ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ä¸ªé‡‡ç”¨Deepseek-R1é£æ ¼çš„å¼ºåŒ–å­¦ä¹ æ¨¡å‹ï¼Œèƒ½å¤Ÿåæ˜ é—®é¢˜å¹¶ç”Ÿæˆå¥–åŠ±é©±åŠ¨ä¼˜åŒ–çš„ç­”æ¡ˆã€‚æœ€åï¼Œæˆ‘ä»¬åœ¨AdsQAä¸Šå¯¹é¡¶å°–çš„åå››ä¸ªLLMè¿›è¡Œäº†è¯„ä¼°å¯¹æ¯”å‘ç°æˆ‘ä»¬è®¾è®¡çš„ReAd-Rè¡¨ç°è¶…è¿‡äº†å…¶å®ƒç«äº‰è€…ç”šè‡³é‡‡ç”¨äº†å…ˆè¿›çš„æ¨¡å‹å¼ºå¤§çš„ç«äº‰å¯¹æ‰‹æœ‰å¼ºå¤§é“¾å¼é€»è¾‘æ¨ç†èƒ½åŠ›ReAd-Rä¸åŒæ ·æŒæ¡äº†é€»è¾‘ä½“ç³»çš„å…¶å®ƒé¢†å…ˆå¤§å‹è¯­è¨€æ¨¡å‹ç›¸æ¯”åŒæ ·å¤§å ä¼˜åŠ¿èƒœå¯¹æ‰‹ä¸€ç­¹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.08621v1">PDF</a> ICCV-2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é€šç”¨äººå·¥æ™ºèƒ½ï¼ˆAGIï¼‰æ–¹å‘ä¸Šå–å¾—å·¨å¤§è¿›å±•ï¼Œä¸”ç‰¹å®šé¢†åŸŸé—®é¢˜å¦‚æ•°å­¦å’Œç¼–ç¨‹æ¨åŠ¨å…¶ä¸æ–­è¿›åŒ–ã€‚å½“å‰é¢ä¸´å¦‚ä½•æ‰©å±•LLMä¸“ä¸šçŸ¥è¯†å¤šæ ·æ€§çš„æŒ‘æˆ˜ï¼Œè€Œå¹¿å‘Šè§†é¢‘ä½œä¸ºä¸€ç§å¯Œå«çº¿ç´¢å’Œä¿¡æ¯å¯†é›†çš„é¢†åŸŸæˆä¸ºæµ‹è¯•LLMèƒ½åŠ›çš„ç†æƒ³å¹³å°ã€‚æœ¬ç ”ç©¶é¦–æ¬¡å°è¯•åˆ©ç”¨å¹¿å‘Šè§†é¢‘è®¾è®¡ä»»åŠ¡è¯„ä¼°LLMï¼Œæå‡ºAdsQAåŸºå‡†æµ‹è¯•ä¸ReAd-Ræ¨¡å‹ã€‚ReAd-Ræ¨¡å‹é€šè¿‡å¥–åŠ±é©±åŠ¨ä¼˜åŒ–ç”Ÿæˆç­”æ¡ˆï¼Œå¹¶åœ¨AdsQAåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°æœ€ä½³ï¼Œå±•ç°äº†å¼ºå¤§çš„é•¿é“¾æ¨ç†èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨é€šç”¨äººå·¥æ™ºèƒ½æ–¹å‘ä¸Šå–å¾—æ˜¾è‘—è¿›å±•ï¼Œç‰¹å®šé¢†åŸŸé—®é¢˜æ¨åŠ¨å…¶ä¸æ–­è¿›åŒ–ã€‚</li>
<li>å¹¿å‘Šè§†é¢‘æˆä¸ºè¯„ä¼°LLMèƒ½åŠ›çš„ç†æƒ³å¹³å°ï¼Œå…¶å¯Œå«çº¿ç´¢å’Œä¿¡æ¯å¯†é›†çš„ç‰¹ç‚¹æœ‰åŠ©äºæµ‹è¯•LLMçš„æ„ŸçŸ¥èƒ½åŠ›ã€‚</li>
<li>æœ¬ç ”ç©¶é¦–æ¬¡å°è¯•åˆ©ç”¨å¹¿å‘Šè§†é¢‘è®¾è®¡ä»»åŠ¡è¯„ä¼°LLMï¼Œå¹¶å»ºç«‹äº†AdsQAåŸºå‡†æµ‹è¯•ã€‚</li>
<li>æå‡ºäº†ReAd-Ræ¨¡å‹ï¼Œé€šè¿‡å¥–åŠ±é©±åŠ¨ä¼˜åŒ–ç”Ÿæˆç­”æ¡ˆã€‚</li>
<li>ReAd-Ræ¨¡å‹åœ¨ AdsQAåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°æœ€ä½³ï¼Œå…·æœ‰å¼ºå¤§çš„é•¿é“¾æ¨ç†èƒ½åŠ›ã€‚</li>
<li>LLMåœ¨ç†è§£å’Œåˆ†æå¹¿å‘Šè§†é¢‘æ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œæœªæ¥å¯è¿›ä¸€æ­¥æ¢ç´¢å…¶åœ¨è¥é”€ã€ä¼ æ’­ç­‰é¢†åŸŸçš„åº”ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.08621">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_LLM/2509.08621v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_LLM/2509.08621v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_LLM/2509.08621v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_LLM/2509.08621v1/page_4_1.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="LLM-Ensemble-for-RAG-Role-of-Context-Length-in-Zero-Shot-Question-Answering-for-BioASQ-Challenge"><a href="#LLM-Ensemble-for-RAG-Role-of-Context-Length-in-Zero-Shot-Question-Answering-for-BioASQ-Challenge" class="headerlink" title="LLM Ensemble for RAG: Role of Context Length in Zero-Shot Question   Answering for BioASQ Challenge"></a>LLM Ensemble for RAG: Role of Context Length in Zero-Shot Question   Answering for BioASQ Challenge</h2><p><strong>Authors:Dima Galat, Diego Molla-Aliod</strong></p>
<p>Biomedical question answering (QA) poses significant challenges due to the need for precise interpretation of specialized knowledge drawn from a vast, complex, and rapidly evolving corpus. In this work, we explore how large language models (LLMs) can be used for information retrieval (IR), and an ensemble of zero-shot models can accomplish state-of-the-art performance on a domain-specific Yes&#x2F;No QA task. Evaluating our approach on the BioASQ challenge tasks, we show that ensembles can outperform individual LLMs and in some cases rival or surpass domain-tuned systems - all while preserving generalizability and avoiding the need for costly fine-tuning or labeled data. Our method aggregates outputs from multiple LLM variants, including models from Anthropic and Google, to synthesize more accurate and robust answers. Moreover, our investigation highlights a relationship between context length and performance: while expanded contexts are meant to provide valuable evidence, they simultaneously risk information dilution and model disorientation. These findings emphasize IR as a critical foundation in Retrieval-Augmented Generation (RAG) approaches for biomedical QA systems. Precise, focused retrieval remains essential for ensuring LLMs operate within relevant information boundaries when generating answers from retrieved documents. Our results establish that ensemble-based zero-shot approaches, when paired with effective RAG pipelines, constitute a practical and scalable alternative to domain-tuned systems for biomedical question answering. </p>
<blockquote>
<p>ç”Ÿç‰©åŒ»å­¦é—®ç­”ï¼ˆQAï¼‰é¢ä¸´å·¨å¤§çš„æŒ‘æˆ˜ï¼Œè¿™ä¸»è¦æ˜¯å› ä¸ºéœ€è¦å‡†ç¡®è§£è¯»å¤§é‡æ¥è‡ªå¤æ‚ä¸”è¿…é€Ÿå‘å±•çš„è¯­æ–™åº“çš„ä¸“é—¨çŸ¥è¯†ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ¢è®¨äº†å¦‚ä½•ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œä¿¡æ¯æ£€ç´¢ï¼ˆIRï¼‰ï¼Œä»¥åŠå¦‚ä½•ä½¿ç”¨é›¶æ ·æœ¬æ¨¡å‹é›†åˆåœ¨ç‰¹å®šé¢†åŸŸçš„é—®ç­”ä»»åŠ¡ä¸Šå®ç°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æˆ‘ä»¬åœ¨BioASQæŒ‘æˆ˜ä»»åŠ¡ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œè¡¨æ˜é›†åˆæ¨¡å‹å¯ä»¥è¶…è¶Šå•ä¸ªLLMçš„æ€§èƒ½ï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ç”šè‡³å¯ä»¥ä¸ç»è¿‡é¢†åŸŸè°ƒæ•´çš„ç³»ç»Ÿçš„æ€§èƒ½ç›¸æŠ—è¡¡æˆ–è¶…è¶Šï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé¿å…äº†æ˜‚è´µçš„å¾®è°ƒæˆ–æ ‡æ³¨æ•°æ®çš„éœ€è¦ã€‚æˆ‘ä»¬çš„æ–¹æ³•èšåˆäº†å¤šç§LLMå˜ç§æ¨¡å‹çš„è¾“å‡ºï¼ŒåŒ…æ‹¬æ¥è‡ªAnthropicå’ŒGoogleçš„æ¨¡å‹ï¼Œä»¥åˆæˆæ›´å‡†ç¡®ã€æ›´ç¨³å¥çš„ç­”æ¡ˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„è°ƒæŸ¥çªå‡ºäº†ä¸Šä¸‹æ–‡é•¿åº¦ä¸æ€§èƒ½ä¹‹é—´çš„å…³ç³»ï¼šè™½ç„¶æ‰©å±•çš„ä¸Šä¸‹æ–‡æ—¨åœ¨æä¾›æœ‰ä»·å€¼çš„è¯æ®ï¼Œä½†å®ƒä»¬åŒæ—¶å¯èƒ½å¸¦æ¥ä¿¡æ¯ç¨€é‡Šå’Œæ¨¡å‹è¿·å¤±çš„é£é™©ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†ä¿¡æ¯æ£€ç´¢åœ¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ–¹æ³•å¯¹ç”Ÿç‰©åŒ»å­¦é—®ç­”ç³»ç»Ÿä¸­çš„å…³é”®ä½œç”¨ã€‚ç²¾ç¡®ã€é›†ä¸­çš„æ£€ç´¢ä»ç„¶æ˜¯ç¡®ä¿å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆç­”æ¡ˆæ—¶ä»æ£€ç´¢åˆ°çš„æ–‡æ¡£ä¸­åœ¨ç›¸å…³çš„ä¿¡æ¯èŒƒå›´å†…æ“ä½œçš„å…³é”®ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒåŸºäºé›†åˆçš„é›¶æ ·æœ¬æ–¹æ³•ä¸æœ‰æ•ˆçš„RAGç®¡é“ç›¸ç»“åˆæ—¶ï¼Œå¯¹äºç”Ÿç‰©åŒ»å­¦é—®ç­”ç³»ç»Ÿè€Œè¨€ï¼Œæ˜¯ä¸€ç§å®ç”¨ä¸”å¯æ‰©å±•çš„æ›¿ä»£ç»è¿‡é¢†åŸŸè°ƒæ•´çš„ç³»ç»Ÿçš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.08596v1">PDF</a> CEUR-WS, CLEF2025</p>
<p><strong>æ‘˜è¦</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç”Ÿç‰©åŒ»å­¦é—®ç­”ï¼ˆQAï¼‰é¢†åŸŸçš„ä¿¡æ¯æ£€ç´¢ï¼ˆIRï¼‰ä¸­å±•ç°å‡ºæ½œåŠ›ã€‚é€šè¿‡é›†æˆé›¶æ ·æœ¬æ¨¡å‹ï¼Œå¯åœ¨ç‰¹å®šé¢†åŸŸçš„Yes&#x2F;Noé—®ç­”ä»»åŠ¡ä¸Šå®ç°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚åœ¨BioASQæŒ‘æˆ˜ä»»åŠ¡ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œé›†æˆæ¨¡å‹åœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†å•ä¸ªLLMï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ç”šè‡³èƒ½ä¸æˆ–è¶…è¶Šé¢†åŸŸç‰¹å®šç³»ç»Ÿç›¸æŠ—è¡¡â€”â€”åŒæ—¶ä¿ç•™é€šç”¨æ€§ï¼Œæ— éœ€æ˜‚è´µçš„å¾®è°ƒæˆ–æ ‡æ³¨æ•°æ®ã€‚è¯¥æ–¹æ³•èšåˆäº†æ¥è‡ªå¤šä¸ªLLMå˜ä½“ï¼ˆåŒ…æ‹¬Anthropicå’ŒGoogleçš„æ¨¡å‹ï¼‰çš„è¾“å‡ºï¼Œä»¥åˆæˆæ›´å‡†ç¡®ã€æ›´ç¨³å¥çš„ç­”æ¡ˆã€‚æ­¤å¤–ï¼Œç ”ç©¶å‘ç°ä¸Šä¸‹æ–‡é•¿åº¦ä¸æ€§èƒ½ä¹‹é—´å­˜åœ¨å…³ç³»ï¼šæ‰©å±•çš„ä¸Šä¸‹æ–‡è™½ç„¶æä¾›äº†æœ‰ä»·å€¼çš„è¯æ®ï¼Œä½†åŒæ—¶å¯èƒ½å¸¦æ¥ä¿¡æ¯ç¨€é‡Šå’Œæ¨¡å‹æ–¹å‘è¿·å¤±çš„é£é™©ã€‚ç²¾ç¡®ã€æœ‰é’ˆå¯¹æ€§çš„æ£€ç´¢ä»æ˜¯ç¡®ä¿LLMåœ¨ç”Ÿæˆç­”æ¡ˆæ—¶ï¼Œä»æ£€ç´¢åˆ°çš„æ–‡æ¡£ä¸­æ“ä½œåœ¨ç›¸å…³ä¿¡æ¯çš„èŒƒå›´å†…çš„é‡è¦åŸºç¡€ã€‚ç»“æœè¯æ˜ï¼ŒåŸºäºé›†æˆæ–¹æ³•çš„é›¶æ ·æœ¬æ–¹æ³•ï¼Œä¸æœ‰æ•ˆçš„RAGç®¡é“ç›¸ç»“åˆï¼Œæˆä¸ºç”Ÿç‰©åŒ»å­¦é—®ç­”ç³»ç»Ÿä¸­ä¸€ç§å®ç”¨ä¸”å¯æ‰©å±•çš„æ›¿ä»£æ–¹æ¡ˆã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯ç”¨äºç”Ÿç‰©åŒ»å­¦é—®ç­”ï¼ˆQAï¼‰ä¸­çš„ä¿¡æ¯æ£€ç´¢ï¼ˆIRï¼‰ã€‚</li>
<li>é›†æˆé›¶æ ·æœ¬æ¨¡å‹å¯åœ¨ç‰¹å®šé¢†åŸŸå®ç°å…ˆè¿›æ€§èƒ½ã€‚</li>
<li>é›†æˆæ¨¡å‹åœ¨BioASQæŒ‘æˆ˜ä»»åŠ¡ä¸Šçš„æ€§èƒ½è¶…è¶Šäº†å•ä¸ªLLMã€‚</li>
<li>é›†æˆæ¨¡å‹åœ¨æŸäº›æƒ…å†µä¸‹èƒ½åª²ç¾æˆ–è¶…è¶Šé¢†åŸŸç‰¹å®šç³»ç»Ÿã€‚</li>
<li>ä¸Šä¸‹æ–‡é•¿åº¦ä¸LLMæ€§èƒ½ä¹‹é—´å­˜åœ¨å…³ç³»ã€‚</li>
<li>æ‰©å±•çš„ä¸Šä¸‹æ–‡å¯èƒ½å¸¦æ¥ä¿¡æ¯ç¨€é‡Šå’Œæ¨¡å‹æ–¹å‘è¿·å¤±çš„é£é™©ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.08596">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_LLM/2509.08596v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_LLM/2509.08596v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_LLM/2509.08596v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_LLM/2509.08596v1/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_LLM/2509.08596v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_LLM/2509.08596v1/page_4_1.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Low-Resource-Fine-Tuning-for-Multi-Task-Structured-Information-Extraction-with-a-Billion-Parameter-Instruction-Tuned-Model"><a href="#Low-Resource-Fine-Tuning-for-Multi-Task-Structured-Information-Extraction-with-a-Billion-Parameter-Instruction-Tuned-Model" class="headerlink" title="Low-Resource Fine-Tuning for Multi-Task Structured Information   Extraction with a Billion-Parameter Instruction-Tuned Model"></a>Low-Resource Fine-Tuning for Multi-Task Structured Information   Extraction with a Billion-Parameter Instruction-Tuned Model</h2><p><strong>Authors:Yu Cheng Chih, Yong Hao Hou</strong></p>
<p>Deploying large language models (LLMs) for structured data extraction in domains such as financial compliance reporting, legal document analytics, and multilingual knowledge base construction is often impractical for smaller teams due to the high cost of running large architectures and the difficulty of preparing large, high-quality datasets. Most recent instruction-tuning studies focus on seven-billion-parameter or larger models, leaving limited evidence on whether much smaller models can work reliably under low-resource, multi-task conditions. This work presents ETLCH, a billion-parameter LLaMA-based model fine-tuned with low-rank adaptation on only a few hundred to one thousand samples per task for JSON extraction, knowledge graph extraction, and named entity recognition. Despite its small scale, ETLCH outperforms strong baselines across most evaluation metrics, with substantial gains observed even at the lowest data scale. These findings demonstrate that well-tuned small models can deliver stable and accurate structured outputs at a fraction of the computational cost, enabling cost-effective and reliable information extraction pipelines in resource-constrained environments. </p>
<blockquote>
<p>åœ¨å¤§è§„æ¨¡éƒ¨ç½²è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œç»“æ„åŒ–æ•°æ®æå–æ—¶ï¼Œé‡‘èåˆè§„æŠ¥å‘Šã€æ³•å¾‹æ–‡æ¡£åˆ†æä»¥åŠè·¨è¯­è¨€çŸ¥è¯†åº“æ„å»ºç­‰é¢†åŸŸï¼Œå¯¹å°å›¢é˜Ÿæ¥è¯´å¹¶ä¸å®é™…ï¼Œå› ä¸ºä»–ä»¬è¿è¡Œå¤§å‹æ¶æ„çš„æˆæœ¬è¾ƒé«˜ä¸”éš¾ä»¥å‡†å¤‡å¤§å‹çš„é«˜è´¨é‡æ•°æ®é›†ã€‚æœ€è¿‘çš„æŒ‡ä»¤è°ƒæ•´ç ”ç©¶ä¸»è¦é›†ä¸­äºå…·æœ‰ä¸ƒäº¿å‚æ•°æˆ–æ›´å¤§çš„æ¨¡å‹ä¸Šï¼Œè‡³äºè§„æ¨¡è¾ƒå°çš„æ¨¡å‹åœ¨ä½èµ„æºæ¡ä»¶ä¸‹çš„å¤šä»»åŠ¡è¡¨ç°æƒ…å†µå´é²œæœ‰è¯æ®ã€‚è¿™é¡¹ç ”ç©¶å±•ç¤ºäº†ETLCHï¼Œå®ƒæ˜¯ä¸€ä¸ªåŸºäºLLaMAçš„ä¸€äº¿å‚æ•°æ¨¡å‹ï¼Œç»è¿‡å¾®è°ƒï¼Œå¯¹æ¯ä¸ªä»»åŠ¡åªä½¿ç”¨æ•°ç™¾è‡³ä¸€åƒä¸ªæ ·æœ¬è¿›è¡Œä½ç§©é€‚åº”ï¼Œç”¨äºJSONæå–ã€çŸ¥è¯†å›¾è°±æå–å’Œå‘½åå®ä½“è¯†åˆ«ã€‚å°½ç®¡è§„æ¨¡è¾ƒå°ï¼ŒETLCHåœ¨å¤§å¤šæ•°è¯„ä¼°æŒ‡æ ‡ä¸Šçš„è¡¨ç°éƒ½ä¼˜äºå¼ºæœ‰åŠ›çš„åŸºçº¿ï¼Œç”šè‡³åœ¨æ•°æ®é‡æœ€å°çš„æƒ…å†µä¸‹ä¹Ÿå–å¾—äº†å®è´¨æ€§çš„æ”¹è¿›ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œç»è¿‡è‰¯å¥½è°ƒæ•´çš„å°å‹æ¨¡å‹èƒ½å¤Ÿä»¥ä¸€å°éƒ¨åˆ†è®¡ç®—æˆæœ¬å®ç°ç¨³å®šå’Œå‡†ç¡®çš„ç»“æ„åŒ–è¾“å‡ºï¼Œä»è€Œå®ç°äº†åœ¨èµ„æºå—é™ç¯å¢ƒä¸­ä½æˆæœ¬å’Œå¯é çš„ä¿¡æ¯æå–ç®¡é“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.08381v1">PDF</a> 13 pages, 8 figures, includes experiments on JSON extraction,   knowledge graph extraction, and NER</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ETLCHæ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºLLaMAçš„å°å‹æ¨¡å‹ï¼Œå¯ä»¥åœ¨ä½èµ„æºå¤šä»»åŠ¡ç¯å¢ƒä¸‹å®Œæˆç»“æ„åŒ–æ•°æ®æå–ä»»åŠ¡ï¼ŒåŒ…æ‹¬JSONæå–ã€çŸ¥è¯†å›¾è°±æå–å’Œå‘½åå®ä½“è¯†åˆ«ç­‰ã€‚è¯¥æ¨¡å‹é€šè¿‡ä½ç§©é€‚åº”æŠ€æœ¯è¿›è¡Œå¾®è°ƒï¼Œä»…ä½¿ç”¨å‡ ç™¾åˆ°ä¸€åƒä¸ªæ ·æœ¬å³å¯å®ç°å‡ºè‰²çš„æ€§èƒ½ï¼Œå¹¶ä¸”åœ¨å¤§å¤šæ•°è¯„ä¼°æŒ‡æ ‡ä¸Šéƒ½ä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œç”šè‡³åœ¨æ•°æ®é‡æœ€å°‘çš„æƒ…å†µä¸‹ä¹Ÿè¡¨ç°å‡ºæ˜¾è‘—çš„æ”¹è¿›ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œç»è¿‡è‰¯å¥½è°ƒæ•´çš„å°å‹æ¨¡å‹å¯ä»¥åœ¨è®¡ç®—æˆæœ¬è¾ƒä½çš„æƒ…å†µä¸‹æä¾›ç¨³å®šå’Œå‡†ç¡®çš„ç»“æ„åŒ–è¾“å‡ºï¼Œä»è€Œåœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­å®ç°é«˜æ•ˆå¯é çš„ä¿¡æ¯æå–ç®¡é“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ETLCHæ˜¯ä¸€ä¸ªåŸºäºLLaMAçš„å°å‹æ¨¡å‹ï¼Œç”¨äºåœ¨ä½èµ„æºå¤šä»»åŠ¡ç¯å¢ƒä¸‹è¿›è¡Œç»“æ„åŒ–æ•°æ®æå–ã€‚</li>
<li>è¯¥æ¨¡å‹é€šè¿‡ä½ç§©é€‚åº”æŠ€æœ¯è¿›è¡Œå¾®è°ƒï¼Œä»…ä½¿ç”¨æœ‰é™çš„æ ·æœ¬å³å¯å®Œæˆå„ç§ä»»åŠ¡ã€‚</li>
<li>ETLCHåœ¨JSONæå–ã€çŸ¥è¯†å›¾è°±æå–å’Œå‘½åå®ä½“è¯†åˆ«ç­‰æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚</li>
<li>åœ¨å¤§å¤šæ•°è¯„ä¼°æŒ‡æ ‡ä¸Šï¼ŒETLCHä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œå¹¶ä¸”åœ¨æ•°æ®é‡æœ€å°‘çš„æƒ…å†µä¸‹ä¹Ÿè¡¨ç°å‡ºæ˜¾è‘—çš„æ”¹è¿›ã€‚</li>
<li>ç ”ç©¶è¡¨æ˜ï¼Œç»è¿‡è‰¯å¥½è°ƒæ•´çš„å°å‹æ¨¡å‹å¯ä»¥åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­å®ç°é«˜æ•ˆå¯é çš„ä¿¡æ¯æå–ç®¡é“ã€‚</li>
<li>è¿™ç§æ¨¡å‹é™ä½äº†å¤§å‹è¯­è¨€æ¨¡å‹éƒ¨ç½²çš„æˆæœ¬ï¼Œç‰¹åˆ«æ˜¯å¯¹äºå°å‹å›¢é˜Ÿè€Œè¨€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.08381">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_LLM/2509.08381v1/page_0_0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="MM-DREX-Multimodal-Driven-Dynamic-Routing-of-LLM-Experts-for-Financial-Trading"><a href="#MM-DREX-Multimodal-Driven-Dynamic-Routing-of-LLM-Experts-for-Financial-Trading" class="headerlink" title="MM-DREX: Multimodal-Driven Dynamic Routing of LLM Experts for Financial   Trading"></a>MM-DREX: Multimodal-Driven Dynamic Routing of LLM Experts for Financial   Trading</h2><p><strong>Authors:Yang Chen, Yueheng Jiang, Zhaozhao Ma, Yuchen Cao, Jacky Keung, Kun Kuang, Leilei Gan, Yiquan Wu, Fei Wu</strong></p>
<p>The inherent non-stationarity of financial markets and the complexity of multi-modal information pose significant challenges to existing quantitative trading models. Traditional methods relying on fixed structures and unimodal data struggle to adapt to market regime shifts, while large language model (LLM)-driven solutions - despite their multi-modal comprehension - suffer from static strategies and homogeneous expert designs, lacking dynamic adjustment and fine-grained decision mechanisms. To address these limitations, we propose MM-DREX: a Multimodal-driven, Dynamically-Routed EXpert framework based on large language models. MM-DREX explicitly decouples market state perception from strategy execution to enable adaptive sequential decision-making in non-stationary environments. Specifically, it (1) introduces a vision-language model (VLM)-powered dynamic router that jointly analyzes candlestick chart patterns and long-term temporal features to allocate real-time expert weights; (2) designs four heterogeneous trading experts (trend, reversal, breakout, positioning) generating specialized fine-grained sub-strategies; and (3) proposes an SFT-RL hybrid training paradigm to synergistically optimize the routerâ€™s market classification capability and expertsâ€™ risk-adjusted decision-making. Extensive experiments on multi-modal datasets spanning stocks, futures, and cryptocurrencies demonstrate that MM-DREX significantly outperforms 15 baselines (including state-of-the-art financial LLMs and deep reinforcement learning models) across key metrics: total return, Sharpe ratio, and maximum drawdown, validating its robustness and generalization. Additionally, an interpretability module traces routing logic and expert behavior in real time, providing an audit trail for strategy transparency. </p>
<blockquote>
<p>é‡‘èå¸‚åœºå›ºæœ‰çš„éå¹³ç¨³æ€§å’Œå¤šæ¨¡æ€ä¿¡æ¯çš„å¤æ‚æ€§å¯¹ç°æœ‰é‡åŒ–äº¤æ˜“æ¨¡å‹æ„æˆäº†é‡å¤§æŒ‘æˆ˜ã€‚ä¼ ç»Ÿæ–¹æ³•ä¾èµ–äºå›ºå®šç»“æ„å’Œå•æ¨¡æ€æ•°æ®ï¼Œéš¾ä»¥é€‚åº”å¸‚åœºæ¨¡å¼çš„å˜åŒ–ï¼Œè€Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é©±åŠ¨çš„æ–¹æ¡ˆå°½ç®¡å…·æœ‰å¤šæ¨¡æ€ç†è§£èƒ½åŠ›ï¼Œä½†å´å—åˆ°é™æ€ç­–ç•¥å’ŒåŒè´¨åŒ–ä¸“å®¶è®¾è®¡çš„å½±å“ï¼Œç¼ºä¹åŠ¨æ€è°ƒæ•´å’Œç²¾ç»†å†³ç­–æœºåˆ¶ã€‚ä¸ºäº†åº”å¯¹è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„MM-DREXï¼šä¸€ä¸ªä»¥å¤šæ¨¡æ€é©±åŠ¨ã€åŠ¨æ€è·¯ç”±çš„ä¸“å®¶æ¡†æ¶ã€‚MM-DREXæ˜¾å¼åœ°å°†å¸‚åœºçŠ¶æ€æ„ŸçŸ¥ä¸ç­–ç•¥æ‰§è¡Œåˆ†å¼€ï¼Œä»¥å®ç°åœ¨éå¹³ç¨³ç¯å¢ƒä¸­çš„è‡ªé€‚åº”åºåˆ—å†³ç­–ã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒï¼ˆ1ï¼‰å¼•å…¥äº†ä¸€ä¸ªç”±è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰é©±åŠ¨çš„åŠ¨æ€è·¯ç”±å™¨ï¼Œè¯¥è·¯ç”±å™¨è”åˆåˆ†æèœ¡çƒ›å›¾æ¨¡å¼å’Œé•¿æœŸæ—¶é—´ç‰¹å¾æ¥åˆ†é…å®æ—¶ä¸“å®¶æƒé‡ï¼›ï¼ˆ2ï¼‰è®¾è®¡äº†å››ç§å¼‚æ„äº¤æ˜“ä¸“å®¶ï¼ˆè¶‹åŠ¿ã€åè½¬ã€çªç ´ã€å®šä½ï¼‰ï¼Œç”Ÿæˆä¸“ä¸šåŒ–çš„ç²¾ç»†å­ç­–ç•¥ï¼›ï¼ˆ3ï¼‰æå‡ºäº†ä¸€ç§SFT-RLæ··åˆè®­ç»ƒèŒƒå¼ï¼ŒååŒä¼˜åŒ–è·¯ç”±å™¨çš„å¸‚åœºåˆ†ç±»èƒ½åŠ›å’Œä¸“å®¶çš„é£é™©è°ƒæ•´å†³ç­–èƒ½åŠ›ã€‚åœ¨æ¶µç›–è‚¡ç¥¨ã€æœŸè´§å’ŒåŠ å¯†è´§å¸çš„å¤šæ¨¡æ€æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒMM-DREXåœ¨å…³é”®æŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äº15ä¸ªåŸºå‡†æ¨¡å‹ï¼ˆåŒ…æ‹¬æœ€å…ˆè¿›çš„é‡‘èLLMå’Œæ·±åº¦å¼ºåŒ–å­¦ä¹ æ¨¡å‹ï¼‰ï¼šæ€»å›æŠ¥ç‡ã€å¤æ™®æ¯”ç‡å’Œæœ€å¤§å›æ’¤ï¼ŒéªŒè¯äº†å…¶ç¨³å¥æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œä¸€ä¸ªè§£é‡Šæ€§æ¨¡å—å®æ—¶è·Ÿè¸ªè·¯ç”±é€»è¾‘å’Œä¸“å®¶è¡Œä¸ºï¼Œä¸ºç­–ç•¥é€æ˜æ€§æä¾›å®¡è®¡è·Ÿè¸ªã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.05080v2">PDF</a> </p>
<p><strong>Summary</strong><br>é‡‘èå¸‚åœºçš„å›ºæœ‰éå¹³ç¨³æ€§ä¸å¤šæ¨¡æ€ä¿¡æ¯çš„å¤æ‚æ€§å¯¹ç°æœ‰å®šé‡äº¤æ˜“æ¨¡å‹æå‡ºäº†å·¨å¤§æŒ‘æˆ˜ã€‚ä¼ ç»Ÿæ–¹æ³•éš¾ä»¥é€‚åº”å¸‚åœºçŠ¶æ€å˜åŒ–ï¼Œè€Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è™½ç„¶å…·å¤‡å¤šæ¨¡æ€ç†è§£èƒ½åŠ›ï¼Œä½†ç­–ç•¥é™æ€ä¸”ç¼ºä¹åŠ¨æ€è°ƒæ•´å’Œç²¾ç»†å†³ç­–æœºåˆ¶ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºMM-DREXï¼šåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„å¤šæ¨¡æ€é©±åŠ¨åŠ¨æ€è·¯ç”±ä¸“å®¶æ¡†æ¶ã€‚MM-DREXæ˜¾å¼åœ°å°†å¸‚åœºçŠ¶æ€æ„ŸçŸ¥ä¸ç­–ç•¥æ‰§è¡Œåˆ†ç¦»ï¼Œä»¥åœ¨éå¹³ç¨³ç¯å¢ƒä¸­å®ç°è‡ªé€‚åº”çš„åºåˆ—å†³ç­–ã€‚å®ƒé€šè¿‡è§†è§‰è¯­è¨€æ¨¡å‹é©±åŠ¨çš„åŠ¨æ€è·¯ç”±å™¨åˆ†æèœ¡çƒ›å›¾æ¨¡å¼å’Œæ—¶é—´ç‰¹å¾æ¥åˆ†é…å®æ—¶ä¸“å®¶æƒé‡ï¼Œè®¾è®¡å››ç§ä¸åŒäº¤æ˜“ä¸“å®¶ç”Ÿæˆä¸“é¡¹ç²¾ç»†å­ç­–ç•¥ï¼Œå¹¶æå‡ºSFT-RLæ··åˆè®­ç»ƒèŒƒå¼æ¥ä¼˜åŒ–è·¯ç”±å™¨çš„å¸‚åœºåˆ†ç±»èƒ½åŠ›å’Œä¸“å®¶çš„é£é™©è°ƒæ•´å†³ç­–èƒ½åŠ›ã€‚å®éªŒè¯æ˜ï¼ŒMM-DREXåœ¨è‚¡ç¥¨ã€æœŸè´§å’ŒåŠ å¯†è´§å¸çš„å¤šæ¨¡æ€æ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äº15ç§åŸºçº¿æ–¹æ³•ï¼ŒéªŒè¯äº†å…¶ç¨³å¥æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè§£é‡Šæ€§æ¨¡å—å®æ—¶è¿½è¸ªè·¯ç”±é€»è¾‘å’Œä¸“å®¶è¡Œä¸ºï¼Œæä¾›ç­–ç•¥é€æ˜åº¦çš„å®¡è®¡è®°å½•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é‡‘èå¸‚åœºå­˜åœ¨éå¹³ç¨³æ€§å’Œå¤šæ¨¡æ€ä¿¡æ¯å¤æ‚æ€§æŒ‘æˆ˜ï¼Œå¯¹å®šé‡äº¤æ˜“æ¨¡å‹æå‡ºæ–°è¦æ±‚ã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•å’Œç°æœ‰LLMè§£å†³æ–¹æ¡ˆéš¾ä»¥é€‚åº”å¸‚åœºå˜åŒ–ï¼Œç¼ºä¹åŠ¨æ€è°ƒæ•´å’Œç²¾ç»†å†³ç­–æœºåˆ¶ã€‚</li>
<li>MM-DREXæ¡†æ¶åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹æå‡ºï¼Œå®ç°å¸‚åœºçŠ¶æ€æ„ŸçŸ¥ä¸ç­–ç•¥æ‰§è¡Œçš„åˆ†ç¦»ã€‚</li>
<li>MM-DREXé€šè¿‡åŠ¨æ€è·¯ç”±å™¨åˆ†é…å®æ—¶ä¸“å®¶æƒé‡ï¼Œåˆ†æèœ¡çƒ›å›¾æ¨¡å¼å’Œæ—¶é—´ç‰¹å¾ã€‚</li>
<li>æ¡†æ¶åŒ…å«å››ç§äº¤æ˜“ä¸“å®¶ï¼Œç”Ÿæˆä¸“é¡¹ç²¾ç»†å­ç­–ç•¥ã€‚</li>
<li>SFT-RLæ··åˆè®­ç»ƒèŒƒå¼ä¼˜åŒ–è·¯ç”±å™¨çš„å¸‚åœºåˆ†ç±»èƒ½åŠ›å’Œä¸“å®¶çš„é£é™©å†³ç­–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.05080">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_LLM/2509.05080v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_LLM/2509.05080v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_LLM/2509.05080v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_LLM/2509.05080v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Subjective-Behaviors-and-Preferences-in-LLM-Language-of-Browsing"><a href="#Subjective-Behaviors-and-Preferences-in-LLM-Language-of-Browsing" class="headerlink" title="Subjective Behaviors and Preferences in LLM: Language of Browsing"></a>Subjective Behaviors and Preferences in LLM: Language of Browsing</h2><p><strong>Authors:Sai Sundaresan, Harshita Chopra, Atanu R. Sinha, Koustava Goswami, Nagasai Saketh Naidu, Raghav Karan, N Anushka</strong></p>
<p>A Large Language Model (LLM) offers versatility across domains and tasks, purportedly benefiting users with a wide variety of behaviors and preferences. We question this perception about an LLM when users have inherently subjective behaviors and preferences, as seen in their ubiquitous and idiosyncratic browsing of websites or apps. The sequential behavior logs of pages, thus generated, form something akin to each userâ€™s self-constructed â€œlanguageâ€, albeit without the structure and grammar imbued in natural languages. We ask: (i) Can a small LM represent the â€œlanguage of browsingâ€ better than a large LM? (ii) Can an LM with a single set of parameters (or, single LM) adequately capture myriad usersâ€™ heterogeneous, subjective behaviors and preferences? (iii) Can a single LM with high average performance, yield low variance in performance to make alignment good at user level? We introduce clusterwise LM training, HeTLM (Heterogeneity aware Training of Language Model), appropriate for subjective behaviors. We find that (i) a small LM trained using a page-level tokenizer outperforms large pretrained or finetuned LMs; (ii) HeTLM with heterogeneous cluster specific set of parameters outperforms a single LM of the same family, controlling for the number of parameters; and (iii) a higher mean and a lower variance in generation ensues, implying improved alignment. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„ä¸ªé¢†åŸŸå’Œä»»åŠ¡ä¸­è¡¨ç°å‡ºå¤šåŠŸèƒ½æ€§ï¼Œæ®è¯´èƒ½ä¸ºå…·æœ‰å„ç§è¡Œä¸ºå’Œåå¥½çš„ç”¨æˆ·æä¾›å¥½å¤„ã€‚å½“ç”¨æˆ·åœ¨æµè§ˆç½‘ç«™æˆ–åº”ç”¨ç¨‹åºæ—¶è¡¨ç°å‡ºå›ºæœ‰çš„ä¸»è§‚è¡Œä¸ºå’Œåå¥½ï¼ˆè¿™ç§æƒ…å†µéšå¤„å¯è§ä¸”å„ä¸ç›¸åŒï¼‰ï¼Œæˆ‘ä»¬å¯¹LLMçš„è¿™ä¸€è®¤çŸ¥äº§ç”Ÿäº†è´¨ç–‘ã€‚ç”±æ­¤äº§ç”Ÿçš„é¡µé¢é¡ºåºè¡Œä¸ºæ—¥å¿—å½¢æˆäº†ç±»ä¼¼äºæ¯ä¸ªç”¨æˆ·è‡ªæˆ‘æ„å»ºçš„â€œè¯­è¨€â€ï¼Œå°½ç®¡å®ƒæ²¡æœ‰è‡ªç„¶è¯­è¨€ä¸­æ‰€è•´å«çš„ç»“æ„å’Œè¯­æ³•ã€‚æˆ‘ä»¬çš„é—®é¢˜æ˜¯ï¼šï¼ˆiï¼‰å°å‹è¯­è¨€æ¨¡å‹èƒ½å¦æ¯”å¤§å‹è¯­è¨€æ¨¡å‹æ›´å¥½åœ°ä»£è¡¨â€œæµè§ˆçš„è¯­è¨€â€ï¼Ÿï¼ˆiiï¼‰å•ä¸€å‚æ•°é›†çš„è¯­è¨€æ¨¡å‹ï¼ˆæˆ–å•ä¸€è¯­è¨€æ¨¡å‹ï¼‰æ˜¯å¦èƒ½å¤Ÿå……åˆ†æ•æ‰ä¼—å¤šç”¨æˆ·çš„å¼‚è´¨æ€§å’Œä¸»è§‚è¡Œä¸ºåå¥½ï¼Ÿï¼ˆiiiï¼‰ä¸€ä¸ªå¹³å‡æ€§èƒ½è¾ƒé«˜çš„å•ä¸€è¯­è¨€æ¨¡å‹æ˜¯å¦èƒ½åœ¨ç”¨æˆ·å±‚é¢äº§ç”Ÿè¾ƒä½çš„æ€§èƒ½å·®å¼‚ï¼Œä»è€Œå®ç°è‰¯å¥½çš„å¯¹é½ï¼Ÿæˆ‘ä»¬å¼•å…¥äº†èšç±»è¯­è¨€æ¨¡å‹è®­ç»ƒï¼Œå³é’ˆå¯¹ä¸»è§‚è¡Œä¸ºçš„å¼‚è´¨æ€§æ„ŸçŸ¥è¯­è¨€æ¨¡å‹è®­ç»ƒï¼ˆHeTLMï¼‰ã€‚æˆ‘ä»¬å‘ç°ï¼šï¼ˆiï¼‰ä½¿ç”¨é¡µé¢çº§æ ‡è®°å™¨è®­ç»ƒçš„å°å‹è¯­è¨€æ¨¡å‹ä¼˜äºå¤§å‹é¢„è®­ç»ƒæˆ–å¾®è°ƒçš„è¯­è¨€æ¨¡å‹ï¼›ï¼ˆiiï¼‰ä½¿ç”¨å¼‚è´¨é›†ç¾¤ç‰¹å®šå‚æ•°é›†çš„HeTLMä¼˜äºå…·æœ‰ç›¸åŒå®¶æ—çš„å•è¯­è¨€æ¨¡å‹ï¼›ï¼ˆiiiï¼‰ç”Ÿæˆæ›´é«˜çš„å‡å€¼å’Œæ›´ä½çš„æ–¹å·®éšä¹‹è€Œæ¥ï¼Œæ„å‘³ç€å¯¹é½äº†æœ‰æ‰€æ”¹å–„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.15474v2">PDF</a> Accepted at EMNLP 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å…·æœ‰è·¨åŸŸè·¨ä»»åŠ¡çš„é€šç”¨æ€§ï¼Œèƒ½å¤Ÿåº”å¯¹ç”¨æˆ·å¤šæ ·çš„è¡Œä¸ºå’Œåå¥½ã€‚ç„¶è€Œï¼Œç”¨æˆ·è¡Œä¸ºå…·æœ‰ä¸»è§‚æ€§å’Œç‹¬ç‰¹æ€§ï¼Œå¦‚æµè§ˆç½‘é¡µæˆ–åº”ç”¨ç¨‹åºæ—¶çš„è¡Œä¸ºã€‚ç”¨æˆ·æµè§ˆçš„é¡µé¢é¡ºåºæ—¥å¿—å½¢æˆäº†ä¸€ç§ç±»ä¼¼äºç”¨æˆ·è‡ªæˆ‘æ„å»ºçš„â€œæµè§ˆè¯­è¨€â€ï¼Œä¸è‡ªç„¶è¯­è¨€ç»“æ„å’Œè¯­æ³•ä¸åŒã€‚æˆ‘ä»¬å¼•å…¥é¢å‘ä¸»è§‚è¡Œä¸ºçš„èšç±»å¼è¯­è¨€æ¨¡å‹è®­ç»ƒæ–¹æ³•HeTLMï¼Œç ”ç©¶ç»“æœæ˜¾ç¤ºå°æ¨¡å‹è®­ç»ƒè¡¨ç°æ›´å¥½ï¼›é’ˆå¯¹ç‰¹å®šå‚æ•°çš„å¼‚é›†ç¾¤æ¨¡å‹ç›¸è¾ƒäºå•ä¸€çš„å¤§å‹æ¨¡å‹è¡¨ç°å‡ºæ›´ä¼˜è¶Šçš„æ€§èƒ½ï¼›å¹¶ä¸”ï¼Œè¿™ç§ç­–ç•¥å¯æé«˜ç”Ÿæˆæ–‡æœ¬çš„å‡å€¼å¹¶é™ä½å…¶æ–¹å·®ï¼Œä»è€Œå®ç°æ›´å¥½çš„å¯¹é½æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èƒ½å¤Ÿåº”å¯¹å¤šæ ·åŒ–çš„ç”¨æˆ·è¡Œä¸ºå’Œåå¥½ã€‚</li>
<li>ç”¨æˆ·æµè§ˆç½‘é¡µæˆ–åº”ç”¨ç¨‹åºçš„è¡Œä¸ºå…·æœ‰ä¸»è§‚æ€§å’Œç‹¬ç‰¹æ€§ã€‚</li>
<li>ç”¨æˆ·æµè§ˆé¡µé¢é¡ºåºå½¢æˆçš„æ—¥å¿—å¯è¢«è§†ä¸ºç”¨æˆ·è‡ªæˆ‘æ„å»ºçš„â€œæµè§ˆè¯­è¨€â€ã€‚</li>
<li>ç›¸æ¯”å¤§å‹é¢„è®­ç»ƒæˆ–å¾®è°ƒçš„è¯­è¨€æ¨¡å‹ï¼Œå°æ¨¡å‹ä½¿ç”¨é¡µé¢çº§åˆ«çš„åˆ†è¯å™¨è¿›è¡Œè®­ç»ƒè¡¨ç°æ›´å¥½ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.15474">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_LLM/2508.15474v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_LLM/2508.15474v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_LLM/2508.15474v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_LLM/2508.15474v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Scaling-LLM-Planning-NL2FLOW-for-Parametric-Problem-Generation-and-Rigorous-Evaluation"><a href="#Scaling-LLM-Planning-NL2FLOW-for-Parametric-Problem-Generation-and-Rigorous-Evaluation" class="headerlink" title="Scaling LLM Planning: NL2FLOW for Parametric Problem Generation and   Rigorous Evaluation"></a>Scaling LLM Planning: NL2FLOW for Parametric Problem Generation and   Rigorous Evaluation</h2><p><strong>Authors:Jungkoo Kang</strong></p>
<p>Robust workflow composition is critical for effective agent performance, yet progress in Large Language Model (LLM) planning and reasoning is hindered by a scarcity of scalable evaluation data. This work introduces NL2Flow, a fully automated pipeline for generating and evaluating workflow planning problems. NL2Flow generates problems parametrically in a structured intermediate representation, translating them into both natural language and formal PDDL. I evaluate several open-source, instruct-tuned LLMs on a dataset of 2296 low-difficulty problems generated by NL2Flow. Results demonstrate that the best-performing model achieved 86% success in generating valid plans and 69% in generating optimal plans (for solvable problems). Regression analysis shows that the influence of problem characteristics on plan generation is contingent on both model and prompt design. Importantly, translating natural language problems into a structured JSON representation prior to symbolic planning significantly improved success rates, suggesting a benefit from neuro-symbolic integration. These findings underscore the importance of understanding error sources within LLM reasoning as systems scale to more complex tasks. As LLM reasoning scales to increasingly complex problems, understanding the shifting bottlenecks and sources of error within these systems will be crucial. </p>
<blockquote>
<p>å¼ºå¤§çš„å·¥ä½œæµç¨‹ç»„åˆå¯¹äºæœ‰æ•ˆä»£ç†æ€§èƒ½è‡³å…³é‡è¦ï¼Œç„¶è€Œï¼Œç”±äºå¯æ‰©å±•è¯„ä¼°æ•°æ®çš„ç¨€ç¼ºï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è§„åˆ’å’Œæ¨ç†æ–¹é¢çš„è¿›å±•å—åˆ°äº†é˜»ç¢ã€‚è¿™é¡¹å·¥ä½œå¼•å…¥äº†NL2Flowï¼Œè¿™æ˜¯ä¸€ä¸ªå®Œå…¨è‡ªåŠ¨åŒ–çš„ç®¡é“ï¼Œç”¨äºç”Ÿæˆå’Œè¯„ä¼°å·¥ä½œæµç¨‹è§„åˆ’é—®é¢˜ã€‚NL2Flowä»¥ç»“æ„åŒ–çš„ä¸­é—´è¡¨ç¤ºå½¢å¼ç”Ÿæˆé—®é¢˜å‚æ•°ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºè‡ªç„¶è¯­è¨€æè¿°å’Œæ­£å¼PDDLæè¿°ã€‚æˆ‘åœ¨ç”±NL2Flowç”Ÿæˆçš„åŒ…å«2296ä¸ªä½éš¾åº¦é—®é¢˜çš„æ•°æ®é›†ä¸Šè¯„ä¼°äº†å¤šä¸ªå¼€æºã€æŒ‡ä»¤è°ƒæ•´çš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚ç»“æœè¡¨æ˜ï¼Œè¡¨ç°æœ€ä½³çš„æ¨¡å‹åœ¨ç”Ÿæˆæœ‰æ•ˆè®¡åˆ’æ–¹é¢å–å¾—äº†86%çš„æˆåŠŸç‡ï¼Œåœ¨ç”Ÿæˆæœ€ä¼˜è®¡åˆ’æ–¹é¢å–å¾—äº†69%ï¼ˆé’ˆå¯¹å¯è§£å†³é—®é¢˜ï¼‰ã€‚å›å½’åˆ†æè¡¨æ˜ï¼Œé—®é¢˜ç‰¹å¾å¯¹è®¡åˆ’ç”Ÿæˆçš„å½±å“å–å†³äºæ¨¡å‹å’Œæç¤ºè®¾è®¡ä¸¤æ–¹é¢ã€‚é‡è¦çš„æ˜¯ï¼Œå°†è‡ªç„¶è¯­è¨€é—®é¢˜è½¬æ¢ä¸ºç»“æ„åŒ–JSONè¡¨ç¤ºå½¢å¼åè¿›è¡Œç¬¦å·è§„åˆ’ï¼Œæ˜¾è‘—æé«˜äº†æˆåŠŸç‡ï¼Œè¿™è¡¨æ˜ç¥ç»ç¬¦å·èåˆå…·æœ‰ä¼˜åŠ¿ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†éšç€è¯­è¨€æ¨¡å‹ç³»ç»Ÿåœ¨å¤„ç†æ›´å¤æ‚çš„ä»»åŠ¡æ—¶æ‰©å¤§è§„æ¨¡ï¼Œç†è§£ç³»ç»Ÿå†…éƒ¨é”™è¯¯æ¥æºçš„é‡è¦æ€§ã€‚éšç€LLMæ¨ç†èƒ½åŠ›å¤„ç†è¶Šæ¥è¶Šå¤æ‚çš„é—®é¢˜æ—¶è§„æ¨¡æ‰©å¤§ï¼Œç†è§£è¿™äº›ç³»ç»Ÿå†…éƒ¨ç“¶é¢ˆå’Œé”™è¯¯æ¥æºçš„å˜åŒ–å°†è‡³å…³é‡è¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.02253v4">PDF</a> 31 pages, 7 figures</p>
<p><strong>Summary</strong></p>
<p>NL2Flowæ˜¯ä¸€ä¸ªç”¨äºç”Ÿæˆå’Œè¯„ä¼°å·¥ä½œæµç¨‹è§„åˆ’é—®é¢˜çš„å…¨è‡ªåŠ¨ç®¡é“ç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿé€šè¿‡å‚æ•°åŒ–ç”Ÿæˆé—®é¢˜ï¼Œå°†å…¶è½¬åŒ–ä¸ºè‡ªç„¶è¯­è¨€ä¸æ­£å¼çš„é€»è¾‘è§„åˆ’è¯­è¨€PDDLã€‚è¯„ä¼°æ˜¾ç¤ºï¼Œæœ€ä¼˜æ¨¡å‹åœ¨ç”Ÿæˆæœ‰æ•ˆè®¡åˆ’ä¸Šçš„æˆåŠŸç‡ä¸º86%ï¼Œåœ¨ç”Ÿæˆæœ€ä¼˜å¯è§£å†³é—®é¢˜çš„è®¡åˆ’ä¸Šçš„æˆåŠŸç‡ä¸º69%ã€‚ç ”ç©¶è¿˜è¡¨æ˜ï¼Œé—®é¢˜ç‰¹æ€§å¯¹è®¡åˆ’ç”Ÿæˆçš„å½±å“å–å†³äºæ¨¡å‹å’Œæç¤ºè®¾è®¡ã€‚å°†è‡ªç„¶è¯­è¨€é—®é¢˜è½¬åŒ–ä¸ºç»“æ„åŒ–JSONè¡¨ç¤ºå½¢å¼å†è¿›è¡Œç¬¦å·è§„åˆ’ï¼Œèƒ½æ˜¾è‘—æé«˜æˆåŠŸç‡ï¼Œæ˜¾ç¤ºå‡ºç¥ç»ç¬¦å·èåˆçš„ä¼˜åŠ¿ã€‚éšç€LLMç³»ç»Ÿå¤„ç†ä»»åŠ¡çš„å¤æ‚æ€§å¢åŠ ï¼Œç†è§£é”™è¯¯æ¥æºå°†æˆä¸ºå…³é”®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>NL2Flowæ˜¯é¦–ä¸ªä¸ºLLMæä¾›å¤§è§„æ¨¡å·¥ä½œæµç¨‹è§„åˆ’é—®é¢˜ç”Ÿæˆå’Œè¯„ä¼°çš„è‡ªåŠ¨ç®¡é“ç³»ç»Ÿã€‚</li>
<li>ç³»ç»Ÿèƒ½å¤Ÿé€šè¿‡å‚æ•°åŒ–ç”Ÿæˆé—®é¢˜ï¼Œå¹¶è½¬åŒ–ä¸ºè‡ªç„¶è¯­è¨€ä¸PDDLï¼Œä¾¿äºè¯„ä¼°å’Œè§„åˆ’ã€‚</li>
<li>æœ€ä¼˜æ¨¡å‹åœ¨ç”Ÿæˆæœ‰æ•ˆè®¡åˆ’ä¸Šçš„æˆåŠŸç‡ä¸º86%ï¼Œåœ¨ç”Ÿæˆæœ€ä¼˜å¯è§£å†³é—®é¢˜çš„è®¡åˆ’ä¸Šä¸º69%ã€‚</li>
<li>é—®é¢˜ç‰¹æ€§å¯¹è®¡åˆ’ç”Ÿæˆçš„å½±å“å—æ¨¡å‹å’Œæç¤ºè®¾è®¡å…±åŒå½±å“ã€‚</li>
<li>å°†è‡ªç„¶è¯­è¨€é—®é¢˜è½¬åŒ–ä¸ºç»“æ„åŒ–JSONè¡¨ç¤ºå½¢å¼å†è¿›è¡Œç¬¦å·è§„åˆ’ï¼Œèƒ½æ˜¾è‘—æé«˜è®¡åˆ’ç”Ÿæˆçš„æˆåŠŸç‡ã€‚</li>
<li>ç¥ç»ç¬¦å·èåˆåœ¨LLMè§„åˆ’ä¸­æ˜¾ç¤ºå‡ºä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.02253">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_LLM/2507.02253v4/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_LLM/2507.02253v4/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_LLM/2507.02253v4/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_LLM/2507.02253v4/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_LLM/2507.02253v4/page_5_0.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Teaching-an-Old-LLM-Secure-Coding-Localized-Preference-Optimization-on-Distilled-Preferences"><a href="#Teaching-an-Old-LLM-Secure-Coding-Localized-Preference-Optimization-on-Distilled-Preferences" class="headerlink" title="Teaching an Old LLM Secure Coding: Localized Preference Optimization on   Distilled Preferences"></a>Teaching an Old LLM Secure Coding: Localized Preference Optimization on   Distilled Preferences</h2><p><strong>Authors:Mohammad Saqib Hasan, Saikat Chakraborty, Santu Karmaker, Niranjan Balasubramanian</strong></p>
<p>LLM generated code often contains security issues. We address two key challenges in improving secure code generation. First, obtaining high quality training data covering a broad set of security issues is critical. To address this, we introduce a method for distilling a preference dataset of insecure and secure code pairs from frontier LLMs, along with a security reasoning that explains the issues and the fix. The key idea here is to make use of security knowledge sources to devise a systematic prompting strategy that ensures broad coverage. Second, aligning models to secure code requires focusing on localized regions of code. Direct preference optimization methods, like SimPO, are not designed to handle these localized differences and turn out to be ineffective. We address this with a new localized preference optimization algorithm that masks the security related tokens in both the winning (secure) and losing (insecure) responses. To prevent loss in code quality, we also add a regularizer. Evaluations show that both training on our dataset, DiSCo, and the new preference optimization algorithm, LPO, yield substantial reductions in code insecurity while also improving overall code quality. Code and dataset are available at <a target="_blank" rel="noopener" href="https://github.com/StonyBrookNLP/disco-lpo">https://github.com/StonyBrookNLP/disco-lpo</a>. </p>
<blockquote>
<p>LLMç”Ÿæˆçš„ä»£ç é€šå¸¸åŒ…å«å®‰å…¨é—®é¢˜ã€‚æˆ‘ä»¬é’ˆå¯¹æ”¹è¿›å®‰å…¨ä»£ç ç”Ÿæˆè¿‡ç¨‹ä¸­çš„ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜è¿›è¡Œç ”ç©¶ã€‚é¦–å…ˆï¼Œè·å–é«˜è´¨é‡ã€æ¶µç›–å¹¿æ³›å®‰å…¨é—®é¢˜çš„è®­ç»ƒæ•°æ®è‡³å…³é‡è¦ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ä»å‰æ²¿LLMä¸­æç‚¼ä¸å®‰å…¨å’Œå®‰å…¨çš„ä»£ç å¯¹åå¥½æ•°æ®é›†çš„æ–¹æ³•ï¼Œå¹¶é…ä»¥è§£é‡Šé—®é¢˜å’Œä¿®å¤çš„å®‰å…¨æ¨ç†ã€‚è¿™é‡Œçš„å…³é”®æ€æƒ³æ˜¯åˆ©ç”¨å®‰å…¨çŸ¥è¯†æ¥æºæ¥åˆ¶å®šä¸€ç§ç³»ç»Ÿæ€§çš„æç¤ºç­–ç•¥ï¼Œä»¥ç¡®ä¿å¹¿æ³›çš„è¦†ç›–ã€‚å…¶æ¬¡ï¼Œè¦ä½¿æ¨¡å‹ä¸å®‰å…¨çš„ä»£ç å¯¹é½ï¼Œéœ€è¦ä¸“æ³¨äºä»£ç çš„å±€éƒ¨åŒºåŸŸã€‚ç›´æ¥çš„åå¥½ä¼˜åŒ–æ–¹æ³•ï¼Œå¦‚SimPOï¼Œå¹¶ä¸é€‚åˆå¤„ç†è¿™äº›å±€éƒ¨å·®å¼‚ï¼Œç»“æœè¯æ˜å…¶æ— æ•ˆã€‚æˆ‘ä»¬é€šè¿‡ä¸€ç§æ–°çš„å±€éƒ¨åå¥½ä¼˜åŒ–ç®—æ³•æ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œè¯¥ç®—æ³•åŒæ—¶å±è”½äº†è·èƒœï¼ˆå®‰å…¨ï¼‰å’Œå¤±è´¥ï¼ˆä¸å®‰å…¨ï¼‰å“åº”ä¸­çš„ä¸å®‰å…¨ç›¸å…³çš„ä»¤ç‰Œã€‚ä¸ºé˜²æ­¢ä»£ç è´¨é‡æŸå¤±ï¼Œæˆ‘ä»¬è¿˜å¢åŠ äº†ä¸€ä¸ªæ­£åˆ™åŒ–å™¨ã€‚è¯„ä¼°è¡¨æ˜ï¼Œæ— è®ºæ˜¯åœ¨æˆ‘ä»¬çš„æ•°æ®é›†DiSCoä¸Šè¿›è¡Œè®­ç»ƒï¼Œè¿˜æ˜¯ä½¿ç”¨æ–°çš„åå¥½ä¼˜åŒ–ç®—æ³•LPOï¼Œéƒ½èƒ½æ˜¾è‘—å‡å°‘ä»£ç çš„ä¸å®‰å…¨æ€§ï¼ŒåŒæ—¶æé«˜æ•´ä½“ä»£ç è´¨é‡ã€‚ä»£ç å’Œæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/StonyBrookNLP/disco-lpo%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/StonyBrookNLP/disco-lpoæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.00419v2">PDF</a> Accepted to ACL 2025 (Main)</p>
<p><strong>Summary</strong></p>
<p>LLMç”Ÿæˆçš„ä»£ç å¸¸å­˜åœ¨å®‰å…¨é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºè§£å†³LLMç”Ÿæˆå®‰å…¨ä»£ç çš„ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ã€‚é¦–å…ˆï¼Œè·å–é«˜è´¨é‡ã€è¦†ç›–å¹¿æ³›å®‰å…¨é—®é¢˜çš„è®­ç»ƒæ•°æ®è‡³å…³é‡è¦ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡å¼•å…¥äº†ä¸€ç§ä»å‰æ²¿LLMä¸­æç‚¼ä¸å®‰å…¨ä¸å®‰å…¨çš„ä»£ç å¯¹åå¥½æ•°æ®é›†çš„æ–¹æ³•ï¼Œå¹¶è§£é‡Šäº†å®‰å…¨æ¨ç†ã€‚å…¶æ¬¡ï¼Œé€šè¿‡å¯¹æ¨¡å‹è¿›è¡Œå±€éƒ¨å¯¹é½ä»¥ç”Ÿæˆå®‰å…¨ä»£ç æ˜¯å…³é”®ã€‚ç›´æ¥çš„åå¥½ä¼˜åŒ–æ–¹æ³•ï¼Œå¦‚SimPOï¼Œä¸é€‚ç”¨äºå¤„ç†å±€éƒ¨å·®å¼‚ï¼Œæ•ˆæœæœ‰é™ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å±€éƒ¨åå¥½ä¼˜åŒ–ç®—æ³•ï¼Œè¯¥ç®—æ³•èƒ½å¤Ÿåœ¨å®‰å…¨ç›¸å…³çš„æ ‡è®°ä¸­è¿›è¡Œé®æ©ã€‚ä¸ºäº†é˜²æ­¢ä»£ç è´¨é‡çš„æŸå¤±ï¼Œæœ¬æ–‡è¿˜å¢åŠ äº†ä¸€ä¸ªæ­£åˆ™åŒ–å™¨ã€‚å®éªŒè¡¨æ˜ï¼Œæ— è®ºæ˜¯åœ¨DiSCoæ•°æ®é›†ä¸Šçš„è®­ç»ƒè¿˜æ˜¯æ–°çš„åå¥½ä¼˜åŒ–ç®—æ³•LPOï¼Œéƒ½èƒ½æ˜¾è‘—å‡å°‘ä»£ç çš„ä¸å®‰å…¨æ€§å¹¶æé«˜æ•´ä½“ä»£ç è´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMç”Ÿæˆçš„ä»£ç å­˜åœ¨å®‰å…¨é—®é¢˜ã€‚</li>
<li>è·å–è¦†ç›–å¹¿æ³›å®‰å…¨é—®é¢˜çš„è®­ç»ƒæ•°æ®æ˜¯æé«˜å®‰å…¨ä»£ç ç”Ÿæˆçš„å…³é”®ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§ä»å‰æ²¿LLMæç‚¼ä¸å®‰å…¨ä¸å®‰å…¨çš„ä»£ç å¯¹åå¥½æ•°æ®é›†çš„æ–¹æ³•ã€‚</li>
<li>å¯¹æ¨¡å‹è¿›è¡Œå±€éƒ¨å¯¹é½ä»¥ç”Ÿæˆå®‰å…¨ä»£ç æ˜¯é‡è¦æ­¥éª¤ã€‚</li>
<li>ç›´æ¥çš„åå¥½ä¼˜åŒ–æ–¹æ³•ä¸é€‚ç”¨äºå¤„ç†å±€éƒ¨å·®å¼‚ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„å±€éƒ¨åå¥½ä¼˜åŒ–ç®—æ³•ï¼Œèƒ½å¤Ÿé®æ©å®‰å…¨ç›¸å…³çš„æ ‡è®°ã€‚</li>
<li>è®­ç»ƒæ•°æ®çš„æ–°ç®—æ³•èƒ½å¤Ÿæ˜¾è‘—æé«˜ä»£ç çš„å®‰å…¨æ€§å’Œæ•´ä½“è´¨é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.00419">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_LLM/2506.00419v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_LLM/2506.00419v2/page_3_0.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Whose-Name-Comes-Up-Auditing-LLM-Based-Scholar-Recommendations"><a href="#Whose-Name-Comes-Up-Auditing-LLM-Based-Scholar-Recommendations" class="headerlink" title="Whose Name Comes Up? Auditing LLM-Based Scholar Recommendations"></a>Whose Name Comes Up? Auditing LLM-Based Scholar Recommendations</h2><p><strong>Authors:Daniele Barolo, Chiara Valentin, Fariba Karimi, Luis GalÃ¡rraga, Gonzalo G. MÃ©ndez, Lisette EspÃ­n-Noboa</strong></p>
<p>This paper evaluates the performance of six open-weight LLMs (llama3-8b, llama3.1-8b, gemma2-9b, mixtral-8x7b, llama3-70b, llama3.1-70b) in recommending experts in physics across five tasks: top-k experts by field, influential scientists by discipline, epoch, seniority, and scholar counterparts. The evaluation examines consistency, factuality, and biases related to gender, ethnicity, academic popularity, and scholar similarity. Using ground-truth data from the American Physical Society and OpenAlex, we establish scholarly benchmarks by comparing model outputs to real-world academic records. Our analysis reveals inconsistencies and biases across all models. mixtral-8x7b produces the most stable outputs, while llama3.1-70b shows the highest variability. Many models exhibit duplication, and some, particularly gemma2-9b and llama3.1-8b, struggle with formatting errors. LLMs generally recommend real scientists, but accuracy drops in field-, epoch-, and seniority-specific queries, consistently favoring senior scholars. Representation biases persist, replicating gender imbalances (reflecting male predominance), under-representing Asian scientists, and over-representing White scholars. Despite some diversity in institutional and collaboration networks, models favor highly cited and productive scholars, reinforcing the rich-getricher effect while offering limited geographical representation. These findings highlight the need to improve LLMs for more reliable and equitable scholarly recommendations. </p>
<blockquote>
<p>æœ¬æ–‡è¯„ä¼°äº†å…­ç§å¼€æ”¾æƒé‡çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆllama3-8bã€llama3.1-8bã€gemma2-9bã€mixtral-8x7bã€llama3-70bã€llama3.1-70bï¼‰åœ¨äº”ä¸ªä»»åŠ¡ä¸­æ¨èç‰©ç†å­¦ä¸“å®¶çš„è¡¨ç°ï¼šæŒ‰é¢†åŸŸæ¨èå‰kåä¸“å®¶ã€æŒ‰å­¦ç§‘å½±å“åŠ›æ¨èç§‘å­¦å®¶ã€æŒ‰æ—¶ä»£ã€èµ„å†å’Œå­¦è€…åŒè¡Œæ¨èã€‚è¯„ä¼°å†…å®¹åŒ…æ‹¬ä¸€è‡´æ€§ã€äº‹å®æ€§ä»¥åŠä¸æ€§åˆ«ã€ç§æ—ã€å­¦æœ¯æµè¡Œåº¦å’Œå­¦è€…ç›¸ä¼¼æ€§çš„åè§ã€‚æˆ‘ä»¬åˆ©ç”¨ç¾å›½ç‰©ç†å­¦ä¼šå’ŒOpenAlexçš„çœŸå®æ•°æ®ï¼Œé€šè¿‡å¯¹æ¯”æ¨¡å‹è¾“å‡ºä¸çœŸå®å­¦æœ¯è®°å½•ï¼Œå»ºç«‹å­¦æœ¯åŸºå‡†ã€‚æˆ‘ä»¬çš„åˆ†ææ˜¾ç¤ºæ‰€æœ‰æ¨¡å‹éƒ½å­˜åœ¨ä¸ä¸€è‡´æ€§å’Œåè§ã€‚mixtral-8x7bçš„è¾“å‡ºæœ€ç¨³å®šï¼Œè€Œllama3.1-70bçš„å˜å¼‚æ€§æœ€é«˜ã€‚è®¸å¤šæ¨¡å‹å­˜åœ¨é‡å¤é—®é¢˜ï¼Œä¸€äº›æ¨¡å‹ç‰¹åˆ«æ˜¯gemma2-9bå’Œllama3.1-8bå­˜åœ¨æ ¼å¼é”™è¯¯ã€‚å¤§å‹è¯­è¨€æ¨¡å‹é€šå¸¸ä¼šæ¨èçœŸå®ç§‘å­¦å®¶ï¼Œä½†åœ¨é’ˆå¯¹é¢†åŸŸã€æ—¶ä»£å’Œèµ„å†çš„æŸ¥è¯¢ä¸­ï¼Œå‡†ç¡®æ€§ä¼šä¸‹é™ï¼Œä¸€è´¯åœ°åå‘èµ„æ·±å­¦è€…ã€‚ä»å­˜åœ¨ä»£è¡¨æ€§åè§ï¼Œå¤åˆ¶æ€§åˆ«å¤±è¡¡ï¼ˆåæ˜ ç”·æ€§å ä¸»å¯¼åœ°ä½ï¼‰ï¼Œäºšæ´²ç§‘å­¦å®¶ä»£è¡¨æ€§ä¸è¶³ï¼Œç™½äººå­¦è€…ä»£è¡¨æ€§è¿‡å‰©ã€‚å°½ç®¡åœ¨æœºæ„å’Œåä½œç½‘ç»œæ–¹é¢æœ‰ä¸€äº›å¤šæ ·æ€§ï¼Œä½†æ¨¡å‹è¿˜æ˜¯åå‘äºé«˜åº¦å¼•ç”¨å’Œé«˜æ•ˆçš„å­¦è€…ï¼Œå¼ºåŒ–äº†å¯Œè€…æ„ˆå¯Œæ•ˆåº”ï¼ŒåŒæ—¶æä¾›æœ‰é™çš„åœ°ç†ä»£è¡¨æ€§ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†å¯¹å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œæ”¹è¿›çš„å¿…è¦æ€§ï¼Œä»¥è·å–æ›´å¯é å’Œå…¬å¹³çš„å­¦æœ¯æ¨èã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.00074v2">PDF</a> 40 pages: 10 main (incl. 9 figures), 3 references, and 27 appendix.   Paper under-review</p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡è¯„ä¼°äº†å…­ç§ä¸åŒè§„æ¨¡çš„å¼€æ”¾æƒé‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨èç‰©ç†é¢†åŸŸä¸“å®¶æ–¹é¢çš„è¡¨ç°ã€‚é€šè¿‡å®Œæˆäº”ä¸ªä»»åŠ¡ï¼ŒåŒ…æ‹¬æŒ‰é¢†åŸŸã€å­¦ç§‘å½±å“åŠ›ã€æ—¶ä»£ã€èµ„å†å’Œå­¦è€…å¯¹åº”æ¨èä¸“å®¶ï¼Œè¯„ä¼°æ¨¡å‹çš„ä¸€è‡´æ€§ã€çœŸå®æ€§å’Œä¸æ€§åˆ«ã€ç§æ—ã€å­¦æœ¯æµè¡Œåº¦ã€å­¦è€…ç›¸ä¼¼æ€§ç›¸å…³çš„åè§ã€‚ä½¿ç”¨ç¾å›½ç‰©ç†å­¦ä¼šå’ŒOpenAlexçš„åœ°é¢çœŸå®æ•°æ®ä½œä¸ºåŸºå‡†ï¼Œå¯¹æ¯”æ¨¡å‹è¾“å‡ºä¸çœŸå®å­¦æœ¯è®°å½•ï¼Œå‘ç°æ‰€æœ‰æ¨¡å‹éƒ½å­˜åœ¨ä¸ä¸€è‡´å’Œåè§ã€‚mixtral-8x7bè¾“å‡ºæœ€ç¨³å®šï¼Œè€Œllama3.1-70bè¡¨ç°æœ€ä¸ç¨³å®šã€‚è®¸å¤šæ¨¡å‹å­˜åœ¨é‡å¤æ¨èï¼Œéƒ¨åˆ†æ¨¡å‹åœ¨ç‰¹å®šæŸ¥è¯¢ä¸­å­˜åœ¨æ ¼å¼åŒ–é”™è¯¯ã€‚LLMsæ¨èçš„å­¦è€…é€šå¸¸æ˜¯çœŸå®çš„ï¼Œä½†åœ¨é’ˆå¯¹é¢†åŸŸã€æ—¶ä»£å’Œèµ„å†çš„æŸ¥è¯¢ä¸­å‡†ç¡®æ€§ä¸‹é™ï¼Œå€¾å‘äºæ¨èèµ„æ·±å­¦è€…ã€‚å­˜åœ¨ä»£è¡¨æ€§åè§ï¼Œåæ˜ å‡ºæ€§åˆ«å¤±è¡¡ã€äºšæ´²ç§‘å­¦å®¶ä»£è¡¨æ€§ä¸è¶³ä»¥åŠç™½äººå­¦è€…ä»£è¡¨æ€§è¿‡å¼ºç­‰é—®é¢˜ã€‚å°½ç®¡åœ¨æœºæ„å’Œåä½œç½‘ç»œæ–¹é¢æœ‰ä¸€å®šå¤šæ ·æ€§ï¼Œä½†æ¨¡å‹å€¾å‘äºæ¨èé«˜è¢«å¼•å’Œé«˜æ•ˆçš„å­¦è€…ï¼Œå¼ºåŒ–äº†â€œå¯Œè€…æ„ˆå¯Œâ€æ•ˆåº”ï¼ŒåŒæ—¶æä¾›æœ‰é™çš„åœ°ç†ä»£è¡¨æ€§ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†æ”¹è¿›LLMsçš„å¿…è¦æ€§ï¼Œä»¥æä¾›æ›´å¯é å’Œå…¬å¹³çš„å­¦æœ¯æ¨èã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>è¯„ä¼°äº†å…­ç§LLMsåœ¨æ¨èç‰©ç†ä¸“å®¶æ–¹é¢çš„æ€§èƒ½ã€‚</li>
<li>é€šè¿‡äº”ä¸ªä»»åŠ¡è€ƒå¯Ÿæ¨¡å‹è¡¨ç°ï¼ŒåŒ…æ‹¬æŒ‰ä¸åŒæ ‡å‡†æ¨èä¸“å®¶ã€‚</li>
<li>æ¨¡å‹å­˜åœ¨ä¸ä¸€è‡´æ€§å’Œåè§ï¼Œéœ€è¦å…³æ³¨æ€§åˆ«ã€ç§æ—ã€å­¦æœ¯æµè¡Œåº¦å’Œå­¦è€…ç›¸ä¼¼æ€§ç­‰å› ç´ ã€‚</li>
<li>mixtral-8x7bè¾“å‡ºæœ€ç¨³å®šï¼Œè€Œllama3.1-70bè¡¨ç°æœ€ä¸ºä¸ç¨³å®šã€‚</li>
<li>éƒ¨åˆ†æ¨¡å‹å­˜åœ¨é‡å¤æ¨èå’Œæ ¼å¼åŒ–é”™è¯¯ã€‚</li>
<li>LLMså€¾å‘äºæ¨èçœŸå®å­¦è€…ï¼Œä½†åœ¨ç‰¹å®šæŸ¥è¯¢ä¸­å‡†ç¡®æ€§ä¸‹é™ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.00074">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_LLM/2506.00074v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_LLM/2506.00074v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_LLM/2506.00074v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_LLM/2506.00074v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_LLM/2506.00074v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_LLM/2506.00074v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="MPO-Boosting-LLM-Agents-with-Meta-Plan-Optimization"><a href="#MPO-Boosting-LLM-Agents-with-Meta-Plan-Optimization" class="headerlink" title="MPO: Boosting LLM Agents with Meta Plan Optimization"></a>MPO: Boosting LLM Agents with Meta Plan Optimization</h2><p><strong>Authors:Weimin Xiong, Yifan Song, Qingxiu Dong, Bingchan Zhao, Feifan Song, Xun Wang, Sujian Li</strong></p>
<p>Recent advancements in large language models (LLMs) have enabled LLM-based agents to successfully tackle interactive planning tasks. However, despite their successes, existing approaches often suffer from planning hallucinations and require retraining for each new agent. To address these challenges, we propose the Meta Plan Optimization (MPO) framework, , which enhances agent planning capabilities by directly incorporating explicit guidance. Unlike previous methods that rely on complex knowledge, which either require significant human effort or lack quality assurance, MPO leverages high-level general guidance through meta plans to assist agent planning and enables continuous optimization of the meta plans based on feedback from the agentâ€™s task execution. Our experiments conducted on two representative tasks demonstrate that MPO significantly outperforms existing baselines. Moreover, our analysis indicates that MPO provides a plug-and-play solution that enhances both task completion efficiency and generalization capabilities in previous unseen scenarios. </p>
<blockquote>
<p>æœ€è¿‘çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥å·²ç»ä½¿å¾—åŸºäºLLMçš„ä»£ç†æˆåŠŸå®Œæˆäº†äº¤äº’è§„åˆ’ä»»åŠ¡ã€‚ç„¶è€Œï¼Œå°½ç®¡å·²ç»å–å¾—äº†æˆåŠŸï¼Œç°æœ‰çš„æ–¹æ³•ç»å¸¸å—åˆ°è§„åˆ’å¹»è§‰çš„å½±å“ï¼Œå¹¶ä¸”ä¸ºæ¯ä¸ªæ–°ä»£ç†éƒ½éœ€è¦è¿›è¡Œå†è®­ç»ƒã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†å…ƒè®¡åˆ’ä¼˜åŒ–ï¼ˆMPOï¼‰æ¡†æ¶ï¼Œé€šè¿‡ç›´æ¥èå…¥æ˜ç¡®çš„æŒ‡å¯¼æ¥æå‡ä»£ç†çš„è§„åˆ’èƒ½åŠ›ã€‚ä¸ä¾èµ–å¤æ‚çŸ¥è¯†çš„ä¹‹å‰æ–¹æ³•ä¸åŒï¼Œè¿™äº›æ–¹æ³•éœ€è¦å¤§é‡çš„äººå·¥å‚ä¸æˆ–ç¼ºä¹è´¨é‡ä¿è¯ï¼ŒMPOé€šè¿‡å…ƒè®¡åˆ’æä¾›é«˜çº§ä¸€èˆ¬æŒ‡å¯¼æ¥ååŠ©ä»£ç†è§„åˆ’ï¼Œå¹¶æ ¹æ®ä»£ç†ä»»åŠ¡æ‰§è¡Œçš„åé¦ˆæ¥æŒç»­ä¼˜åŒ–å…ƒè®¡åˆ’ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªä»£è¡¨æ€§ä»»åŠ¡ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒMPOæ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼ŒMPOæä¾›äº†ä¸€ä¸ªå³æ’å³ç”¨çš„è§£å†³æ–¹æ¡ˆï¼Œæé«˜äº†ä»»åŠ¡å®Œæˆæ•ˆç‡å’Œåœ¨æœªè§è¿‡çš„åœºæ™¯ä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.02682v2">PDF</a> EMNLP 2025 Findings</p>
<p><strong>Summary</strong></p>
<p>LLM-based agentså·²æˆåŠŸåº”å¯¹äº¤äº’è§„åˆ’ä»»åŠ¡ï¼Œä½†ä»é¢ä¸´è§„åˆ’å¹»è§‰é—®é¢˜ï¼Œéœ€ä¸ºæ¯ä¸ªæ–°ä»£ç†è¿›è¡Œå†è®­ç»ƒã€‚ä¸ºæ­¤ï¼Œæå‡ºMeta Plan Optimizationï¼ˆMPOï¼‰æ¡†æ¶ï¼Œé€šè¿‡ç›´æ¥å¼•å…¥æ˜ç¡®æŒ‡å¯¼å¢å¼ºä»£ç†è§„åˆ’èƒ½åŠ›ã€‚MPOåˆ©ç”¨é«˜çº§é€šç”¨æŒ‡å¯¼é€šè¿‡å…ƒè®¡åˆ’ååŠ©ä»£ç†è§„åˆ’ï¼Œå¹¶åŸºäºä»£ç†ä»»åŠ¡æ‰§è¡Œçš„åé¦ˆæŒç»­ä¼˜åŒ–å…ƒè®¡åˆ’ã€‚å®éªŒè¡¨æ˜ï¼ŒMPOæ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ï¼Œå¹¶æä¾›å³æ’å³ç”¨è§£å†³æ–¹æ¡ˆï¼Œæé«˜ä»»åŠ¡å®Œæˆæ•ˆç‡å’Œåœ¨æœªè§è¿‡åœºæ™¯ä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM-based agentså·²èƒ½æˆåŠŸå¤„ç†äº¤äº’è§„åˆ’ä»»åŠ¡ï¼Œä½†ä»é¢ä¸´è§„åˆ’å¹»è§‰é—®é¢˜ã€‚</li>
<li>Meta Plan Optimization (MPO)æ¡†æ¶æ—¨åœ¨è§£å†³è¿™äº›é—®é¢˜ï¼Œé€šè¿‡ç›´æ¥å¼•å…¥æ˜ç¡®æŒ‡å¯¼å¢å¼ºä»£ç†è§„åˆ’èƒ½åŠ›ã€‚</li>
<li>MPOåˆ©ç”¨é«˜çº§é€šç”¨æŒ‡å¯¼é€šè¿‡å…ƒè®¡åˆ’ååŠ©ä»£ç†è§„åˆ’ã€‚</li>
<li>MPOèƒ½åŸºäºä»£ç†ä»»åŠ¡æ‰§è¡Œçš„åé¦ˆæŒç»­ä¼˜åŒ–å…ƒè®¡åˆ’ã€‚</li>
<li>å®éªŒè¡¨æ˜MPOåœ¨äº¤äº’è§„åˆ’ä»»åŠ¡ä¸Šçš„æ€§èƒ½æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>MPOæä¾›å³æ’å³ç”¨è§£å†³æ–¹æ¡ˆï¼Œæé«˜ä»»åŠ¡å®Œæˆæ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.02682">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_LLM/2503.02682v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_LLM/2503.02682v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_LLM/2503.02682v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_LLM/2503.02682v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_LLM/2503.02682v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Pay-Attention-to-Real-World-Perturbations-Natural-Robustness-Evaluation-in-Machine-Reading-Comprehension"><a href="#Pay-Attention-to-Real-World-Perturbations-Natural-Robustness-Evaluation-in-Machine-Reading-Comprehension" class="headerlink" title="Pay Attention to Real World Perturbations! Natural Robustness Evaluation   in Machine Reading Comprehension"></a>Pay Attention to Real World Perturbations! Natural Robustness Evaluation   in Machine Reading Comprehension</h2><p><strong>Authors:Yulong Wu, Viktor Schlegel, Riza Batista-Navarro</strong></p>
<p>As neural language models achieve human-comparable performance on Machine Reading Comprehension (MRC) and see widespread adoption, ensuring their robustness in real-world scenarios has become increasingly important. Current robustness evaluation research, though, primarily develops synthetic perturbation methods, leaving unclear how well they reflect real life scenarios. Considering this, we present a framework to automatically examine MRC models on naturally occurring textual perturbations, by replacing paragraph in MRC benchmarks with their counterparts based on available Wikipedia edit history. Such perturbation type is natural as its design does not stem from an arteficial generative process, inherently distinct from the previously investigated synthetic approaches. In a large-scale study encompassing SQUAD datasets and various model architectures we observe that natural perturbations result in performance degradation in pre-trained encoder language models. More worryingly, these state-of-the-art Flan-T5 and Large Language Models (LLMs) inherit these errors. Further experiments demonstrate that our findings generalise to natural perturbations found in other more challenging MRC benchmarks. In an effort to mitigate these errors, we show that it is possible to improve the robustness to natural perturbations by training on naturally or synthetically perturbed examples, though a noticeable gap still remains compared to performance on unperturbed data. </p>
<blockquote>
<p>éšç€ç¥ç»è¯­è¨€æ¨¡å‹åœ¨æœºå™¨é˜…è¯»ç†è§£ï¼ˆMRCï¼‰æ–¹é¢è¾¾åˆ°ä¸äººç±»ç›¸å½“çš„æ€§èƒ½ï¼Œå¹¶ä¸”åœ¨å®è·µä¸­å¾—åˆ°å¹¿æ³›åº”ç”¨ï¼Œç¡®ä¿å…¶åœ¨ç°å®åœºæ™¯ä¸­çš„ç¨³å¥æ€§å˜å¾—æ„ˆå‘é‡è¦ã€‚ç„¶è€Œï¼Œå½“å‰çš„ç¨³å¥æ€§è¯„ä¼°ç ”ç©¶ä¸»è¦å‘å±•åˆæˆæ‰°åŠ¨æ–¹æ³•ï¼Œå°šä¸æ¸…æ¥šè¿™äº›æ–¹æ³•åœ¨å¤šå¤§ç¨‹åº¦ä¸Šèƒ½å¤Ÿåæ˜ çœŸå®åœºæ™¯ã€‚é‰´äºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ¡†æ¶ï¼Œé€šè¿‡æ›¿æ¢MRCåŸºå‡†æµ‹è¯•ä¸­çš„æ®µè½ï¼Œä»¥åŸºäºå¯ç”¨çš„Wikipediaç¼–è¾‘å†å²å†…å®¹çš„å¯¹åº”æ®µè½æ¥è‡ªåŠ¨æ£€æŸ¥æ–‡æœ¬æ¨¡å‹ã€‚è¿™ç§æ‰°åŠ¨ç±»å‹æ˜¯è‡ªç„¶çš„ï¼Œå› ä¸ºå…¶å†…å®¹å¹¶éæºè‡ªäººä¸ºçš„ç”Ÿæˆè¿‡ç¨‹ï¼Œæœ¬è´¨ä¸Šä¸ä»¥å‰ç ”ç©¶çš„åˆæˆæ–¹æ³•æœ‰æ‰€ä¸åŒã€‚åœ¨å¯¹æ¶µç›–SQUADæ•°æ®é›†å’Œå„ç§æ¨¡å‹æ¶æ„çš„å¤§è§„æ¨¡ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å‘ç°è‡ªç„¶æ‰°åŠ¨ä¼šå¯¼è‡´é¢„è®­ç»ƒç¼–ç å™¨è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ä¸‹é™ã€‚æ›´ä»¤äººæ‹…å¿§çš„æ˜¯ï¼Œæœ€å…ˆè¿›çš„Flan-T5å’Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¹Ÿå­˜åœ¨ç»§æ‰¿è¿™äº›é”™è¯¯çš„é—®é¢˜ã€‚è¿›ä¸€æ­¥çš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„å‘ç°å¯ä»¥æ¨å¹¿åˆ°åœ¨å…¶ä»–æ›´å…·æŒ‘æˆ˜æ€§çš„MRCåŸºå‡†æµ‹è¯•ä¸­é‡åˆ°çš„è‡ªç„¶æ‰°åŠ¨ã€‚ä¸ºäº†ç¼“è§£è¿™äº›é”™è¯¯ï¼Œæˆ‘ä»¬å±•ç¤ºäº†é€šè¿‡è®­ç»ƒè‡ªç„¶æˆ–åˆæˆæ‰°åŠ¨å®ä¾‹æ¥æé«˜å¯¹è‡ªç„¶æ‰°åŠ¨çš„ç¨³å¥æ€§æ˜¯å¯èƒ½çš„ï¼Œä½†ä¸æœªæ‰°åŠ¨æ•°æ®ä¸Šçš„æ€§èƒ½ç›¸æ¯”ä»å­˜åœ¨æ˜æ˜¾çš„å·®è·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.16523v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦æ¢è®¨äº†åœ¨æœºå™¨é˜…è¯»ç†è§£ï¼ˆMRCï¼‰é¢†åŸŸï¼Œéšç€ç¥ç»ç½‘ç»œæ¨¡å‹è¾¾åˆ°äººç±»æ°´å¹³çš„æ€§èƒ½å¹¶å¹¿æ³›åº”ç”¨ï¼Œå¦‚ä½•åœ¨ç°å®åœºæ™¯ä¸­ç¡®ä¿æ¨¡å‹çš„ç¨³å¥æ€§å˜å¾—è‡³å…³é‡è¦ã€‚ç°æœ‰çš„ç¨³å¥æ€§è¯„ä¼°ä¸»è¦ä¾èµ–äºåˆæˆæ‰°åŠ¨æ–¹æ³•ï¼Œä½†å®ƒä»¬æ˜¯å¦èƒ½çœŸå®åæ˜ ç°å®åœºæ™¯å°šä¸æ¸…æ¥šã€‚å› æ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§è‡ªåŠ¨è¯„ä¼°MRCæ¨¡å‹çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡æ›¿æ¢MRCåŸºå‡†æµ‹è¯•ä¸­çš„æ®µè½ï¼Œä½¿ç”¨åŸºäºWikipediaç¼–è¾‘å†å²çš„å¯¹åº”æ®µè½æ¥å¼•å…¥è‡ªç„¶å‘ç”Ÿçš„æ–‡æœ¬æ‰°åŠ¨ã€‚å®éªŒè¡¨æ˜ï¼Œè‡ªç„¶æ‰°åŠ¨ä¼šå¯¼è‡´é¢„è®­ç»ƒç¼–ç å™¨è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ä¸‹é™ï¼Œæœ€å…ˆè¿›çš„Flan-T5å’Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¹Ÿä¼šç»§æ‰¿è¿™äº›é”™è¯¯ã€‚åŒæ—¶ï¼Œç ”ç©¶å‘ç°é€šè¿‡è®­ç»ƒè‡ªç„¶æˆ–åˆæˆæ‰°åŠ¨æ ·æœ¬å¯ä»¥æé«˜å¯¹è‡ªç„¶æ‰°åŠ¨çš„ç¨³å¥æ€§ï¼Œä½†ä»ä¸æœªæ‰°åŠ¨æ•°æ®ä¸Šçš„æ€§èƒ½å­˜åœ¨æ˜æ˜¾å·®è·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¥ç»ç½‘ç»œæ¨¡å‹åœ¨æœºå™¨é˜…è¯»ç†è§£ï¼ˆMRCï¼‰é¢†åŸŸè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½†ç°å®åœºæ™¯ä¸­çš„ç¨³å¥æ€§è¯„ä¼°å˜å¾—é‡è¦ã€‚</li>
<li>ç°æœ‰çš„æ¨¡å‹ç¨³å¥æ€§è¯„ä¼°ä¸»è¦ä¾èµ–åˆæˆæ‰°åŠ¨æ–¹æ³•ï¼Œä½†å…¶ä¸ç°å®åœºæ™¯çš„åŒ¹é…åº¦å°šä¸æ¸…æ¥šã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºWikipediaç¼–è¾‘å†å²çš„è‡ªç„¶æ‰°åŠ¨æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°MRCæ¨¡å‹çš„ç¨³å¥æ€§ã€‚</li>
<li>è‡ªç„¶æ‰°åŠ¨ä¼šå¯¼è‡´é¢„è®­ç»ƒç¼–ç å™¨è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ä¸‹é™ï¼ŒåŒ…æ‹¬æœ€å…ˆè¿›çš„Flan-T5å’Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚</li>
<li>é€šè¿‡è®­ç»ƒè‡ªç„¶æˆ–åˆæˆæ‰°åŠ¨æ ·æœ¬å¯ä»¥æé«˜æ¨¡å‹çš„æŠ—è‡ªç„¶æ‰°åŠ¨èƒ½åŠ›ï¼Œä½†ä»å­˜åœ¨ä¸æœªæ‰°åŠ¨æ•°æ®ä¸Šçš„æ€§èƒ½å·®è·ã€‚</li>
<li>è‡ªç„¶æ‰°åŠ¨æ¡†æ¶æœ‰åŠ©äºå‘ç°æ¨¡å‹åœ¨çœŸå®ç¯å¢ƒä¸‹çš„å¼±ç‚¹ï¼Œä¸ºæ”¹è¿›æ¨¡å‹æä¾›æ–¹å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.16523">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_LLM/2502.16523v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_LLM/2502.16523v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_LLM/2502.16523v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_LLM/2502.16523v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Investigating-Compositional-Reasoning-in-Time-Series-Foundation-Models"><a href="#Investigating-Compositional-Reasoning-in-Time-Series-Foundation-Models" class="headerlink" title="Investigating Compositional Reasoning in Time Series Foundation Models"></a>Investigating Compositional Reasoning in Time Series Foundation Models</h2><p><strong>Authors:Willa Potosnak, Cristian Challu, Mononito Goswami, Kin G. Olivares, MichaÅ‚ WiliÅ„ski, Nina Å»ukowska, Artur Dubrawski</strong></p>
<p>Large pre-trained time series foundation models (TSFMs) have demonstrated promising zero-shot performance across a wide range of domains. However, a question remains: Do TSFMs succeed by memorizing patterns in training data, or do they possess the ability to reason about such patterns? While reasoning is a topic of great interest in the study of Large Language Models (LLMs), it is undefined and largely unexplored in the context of TSFMs. In this work, inspired by language modeling literature, we formally define compositional reasoning in forecasting and distinguish it from in-distribution generalization. We evaluate the reasoning and generalization capabilities of 16 popular deep learning forecasting models on multiple synthetic and real-world datasets. Additionally, through controlled studies, we systematically examine which design choices in 7 popular open-source TSFMs contribute to improved reasoning capabilities. Our study yields key insights into the impact of TSFM architecture design on compositional reasoning and generalization. We find that patch-based Transformers have the best reasoning performance, closely followed by residualized MLP-based architectures, which are 97% less computationally complex in terms of FLOPs and 86% smaller in terms of the number of trainable parameters. Interestingly, in some zero-shot out-of-distribution scenarios, these models can outperform moving average and exponential smoothing statistical baselines trained on in-distribution data. Only a few design choices, such as the tokenization method, had a significant (negative) impact on Transformer model performance. </p>
<blockquote>
<p>å¤§å‹é¢„è®­ç»ƒæ—¶é—´åºåˆ—åŸºç¡€æ¨¡å‹ï¼ˆTSFMsï¼‰åœ¨å¤šä¸ªé¢†åŸŸè¡¨ç°å‡ºäº†æœ‰å‰æ™¯çš„é›¶æ ·æœ¬æ€§èƒ½ã€‚ç„¶è€Œï¼Œä»æœ‰ä¸€ä¸ªé—®é¢˜æœ‰å¾…è§£å†³ï¼šTSFMæ˜¯é€šè¿‡è®­ç»ƒæ•°æ®ä¸­çš„æ¨¡å¼è®°å¿†æ¥æˆåŠŸçš„å—ï¼Œè¿˜æ˜¯å®ƒä»¬å…·æœ‰å¯¹è¿™äº›æ¨¡å¼è¿›è¡Œæ¨ç†çš„èƒ½åŠ›ï¼Ÿè™½ç„¶æ¨ç†æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç ”ç©¶ä¸­çš„ä¸€ä¸ªçƒ­é—¨è¯é¢˜ï¼Œä½†åœ¨TSFMçš„ä¸Šä¸‹æ–‡ä¸­å®ƒæ˜¯æœªå®šä¹‰çš„å¹¶ä¸”æœªè¢«å¹¿æ³›æ¢ç´¢ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å—åˆ°è¯­è¨€å»ºæ¨¡æ–‡çŒ®çš„å¯å‘ï¼Œæ­£å¼å®šä¹‰äº†é¢„æµ‹ä¸­çš„ç»„åˆæ¨ç†ï¼Œå¹¶å°†å…¶ä¸å†…éƒ¨åˆ†å¸ƒæ³›åŒ–åŒºåˆ†å¼€æ¥ã€‚æˆ‘ä»¬è¯„ä¼°äº†16ä¸ªæµè¡Œçš„æ·±åº¦å­¦ä¹ é¢„æµ‹æ¨¡å‹åœ¨å¤šä¸ªåˆæˆå’Œç°å®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„æ¨ç†å’Œæ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œé€šè¿‡å—æ§ç ”ç©¶ï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°æ£€æŸ¥äº†7ä¸ªæµè¡Œçš„å¼€æºTSFMä¸­çš„å“ªäº›è®¾è®¡é€‰æ‹©æœ‰åŠ©äºæ”¹è¿›æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ç ”ç©¶å¯¹äºTSFMæ¶æ„è®¾è®¡å¯¹ç»„åˆæ¨ç†å’Œæ³›åŒ–çš„å½±å“äº§ç”Ÿäº†é‡è¦è§è§£ã€‚æˆ‘ä»¬å‘ç°åŸºäºpatchçš„Transformerå…·æœ‰æœ€ä½³çš„æ¨ç†æ€§èƒ½ï¼Œç´§éšå…¶åçš„æ˜¯åŸºäºæ®‹å·®MLPçš„æ¶æ„ï¼Œå…¶åœ¨æµ®ç‚¹è¿ç®—æ–¹é¢å‡å°‘äº†é«˜è¾¾97çš„è®¡ç®—å¤æ‚æ€§ï¼Œå¹¶ä¸”åœ¨å¯è®­ç»ƒå‚æ•°æ•°é‡æ–¹é¢å‡å°‘äº†é«˜è¾¾86%ã€‚æœ‰è¶£çš„æ˜¯ï¼Œåœ¨æŸäº›é›¶æ ·æœ¬å¤–éƒ¨åˆ†å¸ƒåœºæ™¯ä¸­ï¼Œè¿™äº›æ¨¡å‹å¯ä»¥è¶…è¶Šåœ¨å†…éƒ¨åˆ†å¸ƒæ•°æ®ä¸Šè®­ç»ƒçš„ç§»åŠ¨å¹³å‡å’ŒæŒ‡æ•°å¹³æ»‘ç»Ÿè®¡åŸºçº¿ã€‚åªæœ‰å°‘æ•°è®¾è®¡é€‰æ‹©ï¼ˆå¦‚åˆ†è¯æ–¹æ³•ï¼‰å¯¹Transformeræ¨¡å‹æ€§èƒ½äº§ç”Ÿäº†é‡å¤§ï¼ˆè´Ÿé¢ï¼‰å½±å“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.06037v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ—¶åºå¤§æ¨¡å‹çš„ä¼˜å¼‚è¡¨ç°ä¸»è¦å¾—ç›Šäºå…¶å¯¹æ¨¡å¼è®°å¿†çš„èƒ½åŠ›è¿˜æ˜¯æ¨ç†èƒ½åŠ›ï¼Œè¿™åœ¨æ—¶åºé¢„æµ‹é¢†åŸŸä»æ˜¯ä¸€ä¸ªæ‚¬è€Œæœªå†³çš„é—®é¢˜ã€‚æœ¬ç ”ç©¶æ­£å¼å®šä¹‰äº†é¢„æµ‹ä¸­çš„ç»„åˆæ¨ç†ï¼Œå¹¶å°†å…¶ä¸åˆ†å¸ƒå†…æ³›åŒ–åŒºåˆ†å¼€æ¥ã€‚é€šè¿‡å¯¹å¤šç§åˆæˆå’ŒçœŸå®æ•°æ®é›†çš„ç ”ç©¶ï¼Œå‘ç°åŸºäºè¡¥ä¸çš„Transformeræ¶æ„åœ¨æ¨ç†æ€§èƒ½ä¸Šè¡¨ç°æœ€ä½³ï¼Œå…¶æ¬¡æ˜¯ç®€åŒ–åçš„MLPæ¶æ„ã€‚è¿™äº›æ¶æ„çš„è®¡ç®—å¤æ‚åº¦è¾ƒä½ä¸”å‚æ•°æ›´å°‘ï¼Œåœ¨æŸäº›é›¶æ ·æœ¬åˆ†å¸ƒå¤–åœºæ™¯ä¸­ï¼Œå…¶æ€§èƒ½ç”šè‡³è¶…è¶Šäº†ä¼ ç»Ÿçš„åŸºäºåˆ†å¸ƒçš„ç»Ÿè®¡åŸºçº¿æ–¹æ³•ã€‚æŸäº›è®¾è®¡é€‰æ‹©å¦‚ä»¤ç‰ŒåŒ–æ–¹æ³•å¯¹Transformeræ¨¡å‹çš„æ€§èƒ½æœ‰æ˜¾è‘—å½±å“ã€‚ </p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æ—¶åºå¤§æ¨¡å‹ï¼ˆTSFMsï¼‰åœ¨å¤šä¸ªé¢†åŸŸå±•ç°å‡ºé›¶æ ·æœ¬æ€§èƒ½æ½œåŠ›ã€‚</li>
<li>ç›®å‰å¯¹äºTSFMæ˜¯å¦é€šè¿‡è®°å¿†æˆ–æ¨ç†æˆåŠŸçš„ç†è§£å°šä¸æ¸…æ¥šã€‚</li>
<li>æœ¬ç ”ç©¶å®šä¹‰äº†æ—¶åºé¢„æµ‹ä¸­çš„ç»„åˆæ¨ç†ï¼Œå¹¶ç ”ç©¶äº†å¤šç§æ·±åº¦é¢„æµ‹æ¨¡å‹çš„æ¨ç†å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>åŸºäºè¡¥ä¸çš„Transformeræ¶æ„å’Œç®€åŒ–åçš„MLPæ¶æ„åœ¨æ¨ç†æ€§èƒ½ä¸Šè¡¨ç°æœ€ä½³ã€‚</li>
<li>åœ¨æŸäº›åœºæ™¯ä¸‹ï¼Œè¿™äº›æ¨¡å‹çš„æ€§èƒ½ä¼˜äºåŸºäºåˆ†å¸ƒçš„ç»Ÿè®¡åŸºçº¿æ–¹æ³•ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.06037">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_LLM/2502.06037v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_LLM/2502.06037v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_LLM/2502.06037v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_LLM/2502.06037v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Understanding-Museum-Exhibits-using-Vision-Language-Reasoning"><a href="#Understanding-Museum-Exhibits-using-Vision-Language-Reasoning" class="headerlink" title="Understanding Museum Exhibits using Vision-Language Reasoning"></a>Understanding Museum Exhibits using Vision-Language Reasoning</h2><p><strong>Authors:Ada-Astrid Balauca, Sanjana Garai, Stefan Balauca, Rasesh Udayakumar Shetty, Naitik Agrawal, Dhwanil Subhashbhai Shah, Yuqian Fu, Xi Wang, Kristina Toutanova, Danda Pani Paudel, Luc Van Gool</strong></p>
<p>Museums serve as repositories of cultural heritage and historical artifacts from diverse epochs, civilizations, and regions, preserving well-documented collections that encapsulate vast knowledge, which, when systematically structured into large-scale datasets, can train specialized models. Visitors engage with exhibits through curiosity and questions, making expert domain-specific models essential for interactive query resolution and gaining historical insights. Understanding exhibits from images requires analyzing visual features and linking them to historical knowledge to derive meaningful correlations. We facilitate such reasoning by (a) collecting and curating a large-scale dataset of 65M images and 200M question-answer pairs for exhibits from all around the world; (b) training large vision-language models (VLMs) on the collected dataset; (c) benchmarking their ability on five visual question answering tasks, specifically designed to reflect real-world inquiries and challenges observed in museum settings. The complete dataset is labeled by museum experts, ensuring the quality and the practical significance of the labels. We train two VLMs from different categories: BLIP with vision-language aligned embeddings, but lacking the expressive power of large language models, and the LLaVA model, a powerful instruction-tuned LLM enriched with vision-language reasoning capabilities. Through extensive experiments, we find that while both model types effectively answer visually grounded questions, large vision-language models excel in queries requiring deeper historical context and reasoning. We further demonstrate the necessity of fine-tuning models on large-scale domain-specific datasets by showing that our fine-tuned models significantly outperform current SOTA VLMs in answering questions related to specific attributes, highlighting their limitations in handling complex, nuanced queries. </p>
<blockquote>
<p>åšç‰©é¦†ä½œä¸ºä¸åŒæ—¶ä»£ã€æ–‡æ˜å’Œåœ°åŒºæ–‡åŒ–é—äº§å’Œå†å²æ–‡ç‰©çš„å‚¨å­˜åº“ï¼Œä¿å­˜äº†è®°å½•è¯¦å°½çš„æ”¶è—å“ï¼Œå…¶ä¸­è•´å«äº†ä¸°å¯Œçš„çŸ¥è¯†ã€‚å½“è¿™äº›æ”¶è—å“è¢«ç³»ç»Ÿåœ°æ„å»ºæˆå¤§è§„æ¨¡æ•°æ®é›†æ—¶ï¼Œå°±å¯ä»¥è®­ç»ƒä¸“ä¸šæ¨¡å‹ã€‚æ¸¸å®¢é€šè¿‡å¥½å¥‡å’Œæé—®ä¸å±•å“äº’åŠ¨ï¼Œå› æ­¤ï¼Œå¯¹äºäº¤äº’å¼æŸ¥è¯¢è§£æå’Œè·å–å†å²è§è§£è€Œè¨€ï¼Œé¢†åŸŸä¸“å®¶æ¨¡å‹è‡³å…³é‡è¦ã€‚è¦ä»å›¾åƒä¸­ç†è§£å±•å“ï¼Œéœ€è¦åˆ†æè§†è§‰ç‰¹å¾ï¼Œå¹¶å°†å…¶ä¸å†å²çŸ¥è¯†è”ç³»èµ·æ¥ï¼Œä»¥å¾—å‡ºæœ‰æ„ä¹‰çš„å…³è”ã€‚æˆ‘ä»¬é€šè¿‡ä»¥ä¸‹æ–¹å¼ä¿ƒè¿›äº†è¿™ç§æ¨ç†ï¼šï¼ˆaï¼‰æ”¶é›†å’Œæ•´ç†äº†ä¸€ä¸ªåŒ…å«6500ä¸‡å¼ å›¾åƒå’Œ2äº¿ä¸ªé—®ç­”å¯¹çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œè¿™äº›å±•å“æ¥è‡ªä¸–ç•Œå„åœ°ï¼›ï¼ˆbï¼‰åœ¨æ”¶é›†çš„æ•°æ®é›†ä¸Šè®­ç»ƒå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ï¼›ï¼ˆcï¼‰åœ¨äº”ä¸ªè§†è§‰é—®ç­”ä»»åŠ¡ä¸Šå¯¹è¿™äº›æ¨¡å‹çš„èƒ½åŠ›è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œè¿™äº›ä»»åŠ¡ä¸“é—¨è®¾è®¡ç”¨æ¥åæ˜ åšç‰©é¦†ç¯å¢ƒä¸­è§‚å¯Ÿåˆ°çš„ç°å®ä¸–ç•ŒæŸ¥è¯¢å’ŒæŒ‘æˆ˜ã€‚è¯¥å®Œæ•´æ•°æ®é›†ç»è¿‡åšç‰©é¦†ä¸“å®¶çš„æ ‡æ³¨ï¼Œç¡®ä¿äº†æ ‡ç­¾çš„è´¨é‡å’Œå®ç”¨æ€§ã€‚æˆ‘ä»¬è®­ç»ƒäº†ä¸¤ç§ä¸åŒç±»å‹çš„VLMsï¼šBLIPï¼Œå…·æœ‰è§†è§‰è¯­è¨€å¯¹é½åµŒå…¥ï¼Œä½†ç¼ºä¹å¤§å‹è¯­è¨€æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ï¼›ä»¥åŠLLaVAæ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ä¸ªåŠŸèƒ½å¼ºå¤§çš„æŒ‡ä»¤è°ƒä¼˜LLMï¼Œæ‹¥æœ‰ä¸°å¯Œçš„è§†è§‰è¯­è¨€æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡å¤§é‡å®éªŒï¼Œæˆ‘ä»¬å‘ç°è™½ç„¶ä¸¤ç§æ¨¡å‹ç±»å‹éƒ½èƒ½æœ‰æ•ˆåœ°å›ç­”è§†è§‰åŸºç¡€é—®é¢˜ï¼Œä½†å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹åœ¨éœ€è¦æ·±åšå†å²èƒŒæ™¯å’Œæ¨ç†èƒ½åŠ›çš„æŸ¥è¯¢ä¸­è¡¨ç°å°¤ä¸ºå‡ºè‰²ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥é€šè¿‡å±•ç¤ºæˆ‘ä»¬çš„å¾®è°ƒæ¨¡å‹åœ¨å›ç­”ä¸ç‰¹å®šå±æ€§ç›¸å…³çš„é—®é¢˜æ–¹é¢æ˜¾è‘—ä¼˜äºå½“å‰çš„æœ€ä½³VLMsï¼Œè¯æ˜äº†åœ¨ç‰¹å®šé¢†åŸŸçš„å¤§è§„æ¨¡æ•°æ®é›†ä¸Šå¾®è°ƒæ¨¡å‹çš„å¿…è¦æ€§ï¼ŒåŒæ—¶çªå‡ºäº†å®ƒä»¬åœ¨å¤„ç†å¤æ‚ã€ç»†å¾®æŸ¥è¯¢æ–¹é¢çš„å±€é™æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.01370v2">PDF</a> Accepted at ICCV 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>åšç‰©é¦†ä½œä¸ºæ–‡åŒ–é—äº§å’Œå†å²æ–‡ç‰©çš„é‡è¦æ”¶è—åœ°ï¼Œä¿å­˜äº†å¤§é‡è®°å½•ä¸°å¯Œçš„æ”¶è—å“ï¼Œè¿™äº›æ”¶è—å“è•´å«äº†åºå¤§çš„çŸ¥è¯†ã€‚é€šè¿‡ç³»ç»Ÿåœ°æ„å»ºå¤§è§„æ¨¡æ•°æ®é›†ï¼Œå¯ä»¥è®­ç»ƒä¸“ä¸šæ¨¡å‹ã€‚æ¸¸å®¢å¯¹å±•å“å……æ»¡å¥½å¥‡å¹¶æå‡ºé—®é¢˜ï¼Œå› æ­¤éœ€è¦ä¸“ä¸šçš„é¢†åŸŸæ¨¡å‹æ¥è§£å†³äº’åŠ¨æŸ¥è¯¢å’Œè·å–å†å²è§è§£ã€‚é€šè¿‡å¯¹å›¾åƒçš„ç†è§£å’Œè§†è§‰ç‰¹å¾çš„åˆ†æï¼Œç»“åˆå†å²çŸ¥è¯†ï¼Œå¯ä»¥æ¨å¯¼å‡ºæœ‰æ„ä¹‰çš„å…³è”ã€‚æˆ‘ä»¬æ”¶é›†äº†6.5äº¿å¼ å›¾åƒå’Œ2äº¿ä¸ªé—®ç­”å¯¹ï¼Œè®­ç»ƒäº†å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ï¼Œå¹¶åœ¨äº”ä¸ªè§†è§‰é—®ç­”ä»»åŠ¡ä¸Šè¿›è¡ŒåŸºå‡†æµ‹è¯•ã€‚å®Œæ•´æ•°æ®é›†ç”±åšç‰©é¦†ä¸“å®¶æ ‡æ³¨ï¼Œç¡®ä¿æ ‡ç­¾çš„è´¨é‡å’Œå®ç”¨æ€§ã€‚æˆ‘ä»¬è®­ç»ƒäº†ä¸¤ç§ä¸åŒç±»å‹çš„VLMsï¼šBLIPå’ŒLLaVAæ¨¡å‹ã€‚å‰è€…å…·æœ‰è§†è§‰è¯­è¨€å¯¹é½åµŒå…¥ï¼Œä½†ç¼ºä¹å¤§å‹è¯­è¨€æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ï¼›åè€…æ˜¯ä¸€ç§åŠŸèƒ½å¼ºå¤§çš„æŒ‡ä»¤è°ƒæ•´å‹LLMï¼Œèåˆäº†è§†è§‰è¯­è¨€æ¨ç†èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼Œè™½ç„¶ä¸¤ç§æ¨¡å‹ç±»å‹éƒ½èƒ½æœ‰æ•ˆåœ°å›ç­”è§†è§‰é—®é¢˜ï¼Œä½†åœ¨éœ€è¦æ·±å…¥å†å²èƒŒæ™¯å’Œæ¨ç†çš„æŸ¥è¯¢ä¸­ï¼Œå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹è¡¨ç°æ›´ä¸ºå‡ºè‰²ã€‚æˆ‘ä»¬è¿˜å±•ç¤ºäº†åœ¨å¤§å‹ç‰¹å®šé¢†åŸŸæ•°æ®é›†ä¸Šå¾®è°ƒæ¨¡å‹çš„å¿…è¦æ€§ï¼Œæˆ‘ä»¬çš„å¾®è°ƒæ¨¡å‹åœ¨å›ç­”ä¸ç‰¹å®šå±æ€§ç›¸å…³çš„é—®é¢˜æ—¶ï¼Œæ˜¾è‘—ä¼˜äºå½“å‰çš„æœ€ä½³VLMsï¼ŒåŒæ—¶æŒ‡å‡ºäº†åœ¨å¤„ç†å¤æ‚ç»†å¾®æŸ¥è¯¢æ—¶çš„å±€é™æ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>åšç‰©é¦†çš„è—å“è•´å«å¤§é‡çŸ¥è¯†å’Œä¿¡æ¯ï¼Œé€šè¿‡æ„å»ºå¤§è§„æ¨¡æ•°æ®é›†å¯ä»¥è®­ç»ƒä¸“ä¸šæ¨¡å‹ã€‚</li>
<li>æ¸¸å®¢å¯¹å±•å“çš„å¥½å¥‡å’Œé—®é¢˜éœ€æ±‚ä¸“ä¸šçš„é¢†åŸŸæ¨¡å‹æ¥è§£å†³äº’åŠ¨æŸ¥è¯¢å’Œè·å–å†å²è§è§£ã€‚</li>
<li>ç†è§£å’Œåˆ†æåšç‰©é¦†å±•å“å›¾åƒéœ€è¦è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ã€‚</li>
<li>æˆ‘ä»¬æ”¶é›†äº†ä¸€ä¸ªåŒ…å«6.5äº¿å¼ å›¾åƒå’Œ2äº¿ä¸ªé—®ç­”å¯¹çš„å¤§è§„æ¨¡æ•°æ®é›†æ¥è®­ç»ƒVLMsã€‚</li>
<li>åšç‰©é¦†ä¸“å®¶å‚ä¸æ•°æ®é›†çš„æ ‡æ³¨å·¥ä½œï¼Œç¡®ä¿æ•°æ®è´¨é‡ã€‚</li>
<li>å¯¹æ¯”äº†ä¸¤ç§ä¸åŒçš„VLMsï¼šBLIPå’ŒLLaVAï¼Œå‘ç°å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤„ç†éœ€è¦æ·±å…¥å†å²èƒŒæ™¯å’Œæ¨ç†çš„æŸ¥è¯¢æ—¶è¡¨ç°æ›´å¥½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.01370">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_LLM/2412.01370v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_LLM/2412.01370v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_LLM/2412.01370v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_LLM/2412.01370v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_LLM/2412.01370v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_LLM/2412.01370v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Flash-STU-Fast-Spectral-Transform-Units"><a href="#Flash-STU-Fast-Spectral-Transform-Units" class="headerlink" title="Flash STU: Fast Spectral Transform Units"></a>Flash STU: Fast Spectral Transform Units</h2><p><strong>Authors:Y. Isabel Liu, Windsor Nguyen, Yagiz Devre, Evan Dogariu, Anirudha Majumdar, Elad Hazan</strong></p>
<p>Recent advances in state-space model architectures have shown great promise for efficient sequence modeling, but challenges remain in balancing computational efficiency with model expressiveness. We propose the Flash STU architecture, a hybrid model that interleaves spectral state space model layers with sliding window attention, enabling scalability to billions of parameters for language modeling while maintaining a near-linear time complexity. We evaluate the Flash STU and its variants on diverse sequence prediction tasks, including linear dynamical systems, robotics control, and language modeling. We find that, given a fixed parameter budget, the Flash STU architecture consistently outperforms the Transformer and other leading state-space models such as S4 and Mamba-2. </p>
<blockquote>
<p>æœ€è¿‘çŠ¶æ€ç©ºé—´æ¨¡å‹æ¶æ„çš„è¿›å±•ä¸ºé«˜æ•ˆåºåˆ—å»ºæ¨¡å±•ç°å‡ºäº†å·¨å¤§æ½œåŠ›ï¼Œä½†åœ¨å¹³è¡¡è®¡ç®—æ•ˆç‡å’Œæ¨¡å‹è¡¨è¾¾èƒ½åŠ›æ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†Flash STUæ¶æ„ï¼Œè¿™æ˜¯ä¸€ç§æ··åˆæ¨¡å‹ï¼Œå°†è°±çŠ¶æ€ç©ºé—´æ¨¡å‹å±‚ä¸æ»‘åŠ¨çª—å£æ³¨æ„åŠ›äº¤ç»‡åœ¨ä¸€èµ·ï¼Œèƒ½å¤Ÿåœ¨è¯­è¨€å»ºæ¨¡ä¸­å®ç°æ•°åäº¿å‚æ•°çš„æ‰©å±•ï¼ŒåŒæ—¶ä¿æŒæ¥è¿‘çº¿æ€§çš„æ—¶é—´å¤æ‚åº¦ã€‚æˆ‘ä»¬åœ¨å¤šç§åºåˆ—é¢„æµ‹ä»»åŠ¡ä¸Šè¯„ä¼°äº†Flash STUåŠå…¶å˜ä½“ï¼ŒåŒ…æ‹¬çº¿æ€§åŠ¨åŠ›ç³»ç»Ÿã€æœºå™¨äººæ§åˆ¶å’Œè¯­è¨€å»ºæ¨¡ã€‚æˆ‘ä»¬å‘ç°ï¼Œåœ¨å›ºå®šå‚æ•°é¢„ç®—ä¸‹ï¼ŒFlash STUæ¶æ„å§‹ç»ˆä¼˜äºTransformerä»¥åŠå…¶ä»–é¢†å…ˆçš„çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼Œå¦‚S4å’ŒMamba-2ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.10489v5">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ€æ–°çš„çŠ¶æ€ç©ºé—´æ¨¡å‹æ¶æ„çš„è¿›æ­¥ä¸ºé«˜æ•ˆçš„åºåˆ—å»ºæ¨¡å±•ç¤ºäº†å·¨å¤§çš„æ½œåŠ›ï¼Œä½†ä»é¢ä¸´åœ¨è®¡ç®—æ•ˆç‡å’Œæ¨¡å‹è¡¨è¾¾èƒ½åŠ›ä¹‹é—´å¹³è¡¡çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†Flash STUæ¶æ„ï¼Œå®ƒæ˜¯ä¸€ç§æ··åˆæ¨¡å‹ï¼Œå°†è°±çŠ¶æ€ç©ºé—´æ¨¡å‹å±‚ä¸æ»‘åŠ¨çª—å£æ³¨æ„åŠ›äº¤é”™èµ·æ¥ï¼Œåœ¨ä¿æŒæ¥è¿‘çº¿æ€§æ—¶é—´å¤æ‚åº¦çš„åŒæ—¶ï¼Œå¯å®ç°æ•°åäº¿å‚æ•°çš„æ‰©å±•æ€§è¯­è¨€å»ºæ¨¡ã€‚åœ¨åŒ…æ‹¬çº¿æ€§åŠ¨æ€ç³»ç»Ÿã€æœºå™¨äººæ§åˆ¶å’Œè¯­è¨€å»ºæ¨¡åœ¨å†…çš„å„ç§åºåˆ—é¢„æµ‹ä»»åŠ¡ä¸Šï¼Œæˆ‘ä»¬è¯„ä¼°äº†Flash STUåŠå…¶å˜ä½“ï¼Œå‘ç°ç»™å®šå›ºå®šçš„å‚æ•°é¢„ç®—æ—¶ï¼ŒFlash STUæ¶æ„å§‹ç»ˆä¼˜äºTransformerå’Œå…¶ä»–é¢†å…ˆçš„çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼Œå¦‚S4å’ŒMamba-2ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>çŠ¶æ€ç©ºé—´æ¨¡å‹æ¶æ„çš„æœ€æ–°è¿›å±•æ˜¾ç¤ºå‡ºé«˜æ•ˆçš„åºåˆ—å»ºæ¨¡æ½œåŠ›ã€‚</li>
<li>Flash STUæ¶æ„æ˜¯ä¸€ç§æ··åˆæ¨¡å‹ï¼Œç»“åˆäº†è°±çŠ¶æ€ç©ºé—´æ¨¡å‹å±‚å’Œæ»‘åŠ¨çª—å£æ³¨æ„åŠ›ã€‚</li>
<li>Flash STUæ¶æ„èƒ½åœ¨ä¿æŒè¿‘çº¿æ€§æ—¶é—´å¤æ‚åº¦çš„åŒæ—¶å®ç°å¤§è§„æ¨¡çš„å‚æ•°æ‰©å±•ã€‚</li>
<li>åœ¨ä¸åŒçš„åºåˆ—é¢„æµ‹ä»»åŠ¡ä¸Šï¼ŒFlash STUæ¶æ„è¡¨ç°å‡ºä¼˜è¶Šçš„æ€§èƒ½ã€‚</li>
<li>åœ¨å›ºå®šå‚æ•°é¢„ç®—ä¸‹ï¼ŒFlash STUæ¶æ„ä¼˜äºå…¶ä»–ä¸»æµæ¨¡å‹æ¶æ„ï¼Œå¦‚Transformerã€‚</li>
<li>Flash STUæ¶æ„ä¸å…¶ä»–é¢†å…ˆçš„çŠ¶æ€ç©ºé—´æ¨¡å‹ç›¸æ¯”ï¼Œå¦‚S4å’ŒMamba-2ï¼Œè¡¨ç°å‡ºæ›´å¥½çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.10489">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_LLM/2409.10489v5/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_LLM/2409.10489v5/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_LLM/2409.10489v5/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_LLM/2409.10489v5/page_2_2.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_LLM/2409.10489v5/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_LLM/2409.10489v5/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_LLM/2409.10489v5/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_LLM/2409.10489v5/page_4_1.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Show-o-One-Single-Transformer-to-Unify-Multimodal-Understanding-and-Generation"><a href="#Show-o-One-Single-Transformer-to-Unify-Multimodal-Understanding-and-Generation" class="headerlink" title="Show-o: One Single Transformer to Unify Multimodal Understanding and   Generation"></a>Show-o: One Single Transformer to Unify Multimodal Understanding and   Generation</h2><p><strong>Authors:Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, Mike Zheng Shou</strong></p>
<p>We present a unified transformer, i.e., Show-o, that unifies multimodal understanding and generation. Unlike fully autoregressive models, Show-o unifies autoregressive and (discrete) diffusion modeling to adaptively handle inputs and outputs of various and mixed modalities. The unified model flexibly supports a wide range of vision-language tasks including visual question-answering, text-to-image generation, text-guided inpainting&#x2F;extrapolation, and mixed-modality generation. Across various benchmarks, it demonstrates comparable or superior performance to existing individual models with an equivalent or larger number of parameters tailored for understanding or generation. This significantly highlights its potential as a next-generation foundation model. Code and models are released at <a target="_blank" rel="noopener" href="https://github.com/showlab/Show-o">https://github.com/showlab/Show-o</a>. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆä¸€ä½“åŒ–è½¬æ¢å™¨â€”â€”Show-oã€‚ä¸å®Œå…¨è‡ªå›å½’æ¨¡å‹ä¸åŒï¼ŒShow-oèåˆäº†è‡ªå›å½’å’Œï¼ˆç¦»æ•£ï¼‰æ‰©æ•£å»ºæ¨¡ï¼Œä»¥è‡ªé€‚åº”åœ°å¤„ç†å„ç§å’Œæ··åˆæ¨¡æ€çš„è¾“å…¥å’Œè¾“å‡ºã€‚è¯¥ç»Ÿä¸€æ¨¡å‹çµæ´»æ”¯æŒå¹¿æ³›çš„è§†è§‰è¯­è¨€ä»»åŠ¡ï¼ŒåŒ…æ‹¬è§†è§‰é—®ç­”ã€æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆã€æ–‡æœ¬å¼•å¯¼çš„ä¸Šé‡‡æ ·&#x2F;å¤–æ¨ä»¥åŠæ··åˆæ¨¡æ€ç”Ÿæˆç­‰ã€‚åœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸­ï¼Œå…¶æ€§èƒ½ä¸ç°æœ‰é’ˆå¯¹ç†è§£æˆ–ç”Ÿæˆä»»åŠ¡çš„å…·æœ‰ç›¸åŒæˆ–æ›´å¤šå‚æ•°çš„æ¨¡å‹ç›¸æ¯”è¡¨ç°ç›¸å½“æˆ–æ›´å¥½ã€‚è¿™å……åˆ†å‡¸æ˜¾äº†å…¶ä½œä¸ºä¸‹ä¸€ä»£åŸºç¡€æ¨¡å‹çš„æ½œåŠ›ã€‚ç›¸å…³ä»£ç å’Œæ¨¡å‹å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/showlab/Show-o%E4%B8%8A%E3%80%82">https://github.com/showlab/Show-oä¸Šã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.12528v7">PDF</a> ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>å±•ç¤ºäº†ä¸€ç§ç»Ÿä¸€çš„å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆæ¨¡å‹â€”â€”â€œShow-oâ€ï¼Œå®ƒç»“åˆäº†è‡ªå›å½’å’Œç¦»æ•£æ‰©æ•£å»ºæ¨¡ï¼Œå¯çµæ´»å¤„ç†å„ç§å’Œæ··åˆæ¨¡æ€çš„è¾“å…¥å’Œè¾“å‡ºã€‚è¯¥æ¨¡å‹æ”¯æŒå¹¿æ³›çš„è§†è§‰è¯­è¨€ä»»åŠ¡ï¼ŒåŒ…æ‹¬è§†è§‰é—®ç­”ã€æ–‡æœ¬ç”Ÿæˆå›¾åƒã€æ–‡æœ¬å¼•å¯¼çš„å›¾åƒä¿®å¤&#x2F;å¤–æ¨å’Œæ··åˆæ¨¡æ€ç”Ÿæˆç­‰ã€‚åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼Œå…¶æ€§èƒ½ä¸ç°æœ‰æ¨¡å‹ç›¸å½“æˆ–æ›´ä¼˜ï¼Œå±•ç°å‡ºä½œä¸ºä¸‹ä¸€ä»£åŸºç¡€æ¨¡å‹çš„æ½œåŠ›ã€‚ä»£ç å’Œæ¨¡å‹å·²å‘å¸ƒåœ¨â€œ<a target="_blank" rel="noopener" href="https://github.com/showlab/Show-o%E2%80%9D%E3%80%82">https://github.com/showlab/Show-oã€‚â€</a> </p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Show-oæ˜¯ä¸€ç§ç»Ÿä¸€çš„å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆæ¨¡å‹ï¼Œèåˆäº†è‡ªå›å½’å’Œç¦»æ•£æ‰©æ•£å»ºæ¨¡æŠ€æœ¯ã€‚</li>
<li>è¯¥æ¨¡å‹å¯ä»¥é€‚åº”å„ç§å’Œæ··åˆæ¨¡æ€çš„è¾“å…¥å’Œè¾“å‡ºã€‚</li>
<li>Show-oæ”¯æŒå¹¿æ³›çš„è§†è§‰è¯­è¨€ä»»åŠ¡ï¼Œå¦‚è§†è§‰é—®ç­”ã€æ–‡æœ¬ç”Ÿæˆå›¾åƒç­‰ã€‚</li>
<li>åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒShow-oçš„æ€§èƒ½ä¸ç°æœ‰æ¨¡å‹ç›¸å½“æˆ–æ›´ä¼˜ã€‚</li>
<li>Show-oå…·æœ‰æ½œåŠ›æˆä¸ºä¸‹ä¸€ä»£åŸºç¡€æ¨¡å‹ã€‚</li>
<li>æ¨¡å‹ä»£ç å’Œå‚æ•°å·²å…¬å¼€å‘å¸ƒï¼Œä¾¿äºç ”ç©¶å’Œä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.12528">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_LLM/2408.12528v7/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_LLM/2408.12528v7/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_LLM/2408.12528v7/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_LLM/2408.12528v7/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_LLM/2408.12528v7/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_LLM/2408.12528v7/page_5_0.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Affective-Computing-in-the-Era-of-Large-Language-Models-A-Survey-from-the-NLP-Perspective"><a href="#Affective-Computing-in-the-Era-of-Large-Language-Models-A-Survey-from-the-NLP-Perspective" class="headerlink" title="Affective Computing in the Era of Large Language Models: A Survey from   the NLP Perspective"></a>Affective Computing in the Era of Large Language Models: A Survey from   the NLP Perspective</h2><p><strong>Authors:Yiqun Zhang, Xiaocui Yang, Xingle Xu, Zeran Gao, Yijie Huang, Shiyi Mu, Shi Feng, Daling Wang, Yifei Zhang, Kaisong Song, Ge Yu</strong></p>
<p>Affective Computing (AC) integrates computer science, psychology, and cognitive science to enable machines to recognize, interpret, and simulate human emotions across domains such as social media, finance, healthcare, and education. AC commonly centers on two task families: Affective Understanding (AU) and Affective Generation (AG). While fine-tuned pre-trained language models (PLMs) have achieved solid AU performance, they often generalize poorly across tasks and remain limited for AG, especially in producing diverse, emotionally appropriate responses. The advent of Large Language Models (LLMs) (e.g., ChatGPT and LLaMA) has catalyzed a paradigm shift by offering in-context learning, broader world knowledge, and stronger sequence generation. This survey presents an NLP-oriented overview of AC in the LLM era. We (i) consolidate traditional AC tasks and preliminary LLM-based studies; (ii) review adaptation techniques that improve AU&#x2F;AG, including Instruction Tuning (full and parameter-efficient methods such as LoRA, P-&#x2F;Prompt-Tuning), Prompt Engineering (zero&#x2F;few-shot, chain-of-thought, agent-based prompting), and Reinforcement Learning. For the latter, we summarize RL from human preferences (RLHF), verifiable&#x2F;programmatic rewards (RLVR), and AI feedback (RLAIF), which provide preference- or rule-grounded optimization signals that can help steer AU&#x2F;AG toward empathy, safety, and planning, achieving finer-grained or multi-objective control. To assess progress, we compile benchmarks and evaluation practices for both AU and AG. We also discuss open challenges-from ethics, data quality, and safety to robust evaluation and resource efficiency-and outline research directions. We hope this survey clarifies the landscape and offers practical guidance for building affect-aware, reliable, and responsible LLM systems. </p>
<blockquote>
<p>æƒ…æ„Ÿè®¡ç®—ï¼ˆACï¼‰èåˆäº†è®¡ç®—æœºç§‘å­¦ã€å¿ƒç†å­¦å’Œè®¤çŸ¥ç§‘å­¦ï¼Œä½¿æœºå™¨èƒ½å¤Ÿè¯†åˆ«ã€è§£é‡Šå’Œæ¨¡æ‹Ÿäººç±»æƒ…ç»ªï¼Œæ¶‰åŠç¤¾äº¤åª’ä½“ã€é‡‘èã€åŒ»ç–—å’Œæ•™è‚²ç­‰é¢†åŸŸã€‚æƒ…æ„Ÿè®¡ç®—é€šå¸¸å›´ç»•ä¸¤ä¸ªä»»åŠ¡å®¶æ—ï¼šæƒ…æ„Ÿç†è§£ï¼ˆAUï¼‰å’Œæƒ…æ„Ÿç”Ÿæˆï¼ˆAGï¼‰ã€‚è™½ç„¶ç»è¿‡å¾®è°ƒé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆPLMï¼‰åœ¨AUæ–¹é¢å–å¾—äº†æ‰å®çš„æ•ˆæœï¼Œä½†å®ƒä»¬å¾€å¾€è·¨ä»»åŠ¡æ³›åŒ–èƒ½åŠ›è¾ƒå·®ï¼Œåœ¨AGæ–¹é¢ä»å­˜åœ¨å±€é™æ€§ï¼Œå°¤å…¶æ˜¯åœ¨ç”Ÿæˆå¤šæ ·ä¸”æƒ…æ„Ÿæ°å½“çš„å“åº”æ–¹é¢ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼ˆä¾‹å¦‚ChatGPTå’ŒLLaMAï¼‰çš„å‡ºç°ï¼Œé€šè¿‡æä¾›ä¸Šä¸‹æ–‡å­¦ä¹ ã€æ›´å¹¿æ³›çš„ä¸–ç•ŒçŸ¥è¯†å’Œæ›´å¼ºçš„åºåˆ—ç”Ÿæˆèƒ½åŠ›ï¼Œå‚¬ç”Ÿäº†èŒƒå¼è½¬å˜ã€‚è¿™ç¯‡ç»¼è¿°ä»¥è‡ªç„¶è¯­è¨€å¤„ç†ä¸ºå¯¼å‘ï¼Œä»‹ç»äº†LLMæ—¶ä»£çš„ACã€‚æˆ‘ä»¬ï¼ˆiï¼‰æ•´åˆäº†ä¼ ç»Ÿçš„ACä»»åŠ¡å’Œåˆæ­¥çš„LLMç ”ç©¶ï¼›ï¼ˆiiï¼‰å›é¡¾äº†æé«˜AU&#x2F;AGçš„é€‚åº”æŠ€æœ¯ï¼ŒåŒ…æ‹¬æŒ‡ä»¤å¾®è°ƒï¼ˆåŒ…æ‹¬LoRAã€P-&#x2F;Prompt-Tuningç­‰å…¨é‡å’Œå‚æ•°é«˜æ•ˆæ–¹æ³•ï¼‰ã€æç¤ºå·¥ç¨‹ï¼ˆé›¶&#x2F;å°‘æ ·æœ¬ã€æ€ç»´é“¾ã€åŸºäºä»£ç†çš„æç¤ºï¼‰å’Œå¼ºåŒ–å­¦ä¹ ã€‚å¯¹äºåè€…ï¼Œæˆ‘ä»¬æ€»ç»“äº†ä»äººç±»åå¥½ä¸­å­¦ä¹ ï¼ˆRLHFï¼‰ã€å¯éªŒè¯&#x2F;ç¨‹åºåŒ–å¥–åŠ±ï¼ˆRLVRï¼‰å’ŒAIåé¦ˆï¼ˆRLAIFï¼‰ï¼Œè¿™äº›æä¾›åå¥½æˆ–è§„åˆ™åŸºç¡€çš„ä¼˜åŒ–ä¿¡å·ï¼Œæœ‰åŠ©äºå¼•å¯¼AU&#x2F;AGæœç€åŒç†å¿ƒã€å®‰å…¨å’Œè§„åˆ’çš„æ–¹å‘å‘å±•ï¼Œå®ç°æ›´ç²¾ç»†æˆ–å¤šç›®æ ‡æ§åˆ¶ã€‚ä¸ºäº†è¯„ä¼°è¿›å±•ï¼Œæˆ‘ä»¬æ•´ç†äº†AUå’ŒAGçš„åŸºå‡†æµ‹è¯•å’Œè¯„ä¼°å®è·µã€‚æˆ‘ä»¬è¿˜è®¨è®ºäº†ä»ä¼¦ç†ã€æ•°æ®è´¨é‡å’Œå®‰å…¨åˆ°ç¨³å¥è¯„ä¼°å’Œèµ„æºæ•ˆç‡ç­‰å¼€æ”¾æŒ‘æˆ˜ï¼Œå¹¶æ¦‚è¿°äº†ç ”ç©¶æ–¹å‘ã€‚æˆ‘ä»¬å¸Œæœ›è¿™ç¯‡ç»¼è¿°èƒ½æ¾„æ¸…è¯¥é¢†åŸŸçš„æƒ…å†µï¼Œä¸ºæ„å»ºæƒ…æ„Ÿæ„ŸçŸ¥ã€å¯é å’Œè´Ÿè´£ä»»çš„LLMç³»ç»Ÿæä¾›å®ç”¨æŒ‡å¯¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.04638v2">PDF</a> Compared with the previous version, reinforcement learning has been   added (as a new section), including RLHF, RLVR, and RLAIF</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æƒ…æ„Ÿè®¡ç®—ï¼ˆACï¼‰é¢†åŸŸåœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ—¶ä»£çš„ç ”ç©¶è¿›å±•ã€‚æ–‡ç« æ¦‚è¿°äº†æƒ…æ„Ÿè®¡ç®—çš„å®šä¹‰åŠå…¶ä¸¤å¤§ä»»åŠ¡å®¶æ—ï¼šæƒ…æ„Ÿç†è§£ï¼ˆAUï¼‰å’Œæƒ…æ„Ÿç”Ÿæˆï¼ˆAGï¼‰ã€‚éšç€LLMçš„å‡ºç°ï¼Œå¦‚ChatGPTå’ŒLLaMAï¼Œæƒ…æ„Ÿè®¡ç®—é¢†åŸŸè¿æ¥äº†å˜é©ã€‚æœ¬æ–‡å›é¡¾äº†é€‚åº”æŠ€å·§ï¼ŒåŒ…æ‹¬æŒ‡ä»¤å¾®è°ƒã€æç¤ºå·¥ç¨‹å’Œå¼ºåŒ–å­¦ä¹ ç­‰æ–¹æ³•ï¼Œä»¥æé«˜AU&#x2F;AGçš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜ä»‹ç»äº†åŸºäºäººç±»åå¥½çš„å¼ºåŒ–å­¦ä¹ ç­‰ç­–ç•¥ï¼Œå¯å®ç°æƒ…æ„Ÿç†è§£å’Œç”Ÿæˆçš„ç²¾ç»†åŒ–æˆ–å¤šç›®æ ‡æ§åˆ¶ã€‚æ–‡ç« è¿˜æ€»ç»“äº†AUå’ŒAGçš„è¯„ä¼°æ ‡å‡†å’ŒæŒ‘æˆ˜ï¼Œå¹¶ä¸ºæ„å»ºæƒ…æ„Ÿæ„ŸçŸ¥ã€å¯é å’Œè´Ÿè´£ä»»çš„LLMç³»ç»Ÿæä¾›äº†å®ç”¨æŒ‡å¯¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æƒ…æ„Ÿè®¡ç®—ï¼ˆACï¼‰ç»“åˆäº†è®¡ç®—æœºç§‘å­¦ã€å¿ƒç†å­¦å’Œè®¤çŸ¥ç§‘å­¦ï¼Œä½¿æœºå™¨èƒ½å¤Ÿè¯†åˆ«ã€è§£é‡Šå’Œæ¨¡æ‹Ÿäººç±»æƒ…ç»ªã€‚</li>
<li>ACä¸»è¦å…³æ³¨ä¸¤ä¸ªä»»åŠ¡å®¶æ—ï¼šæƒ…æ„Ÿç†è§£ï¼ˆAUï¼‰å’Œæƒ…æ„Ÿç”Ÿæˆï¼ˆAGï¼‰ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‡ºç°ä¸ºæƒ…æ„Ÿè®¡ç®—é¢†åŸŸå¸¦æ¥äº†å˜é©ï¼Œæä¾›äº†ä¸Šä¸‹æ–‡å­¦ä¹ ã€æ›´å¹¿æ³›çš„ä¸–ç•ŒçŸ¥è¯†å’Œæ›´å¼ºçš„åºåˆ—ç”Ÿæˆèƒ½åŠ›ã€‚</li>
<li>æé«˜AU&#x2F;AGæ€§èƒ½çš„é€‚åº”æŠ€å·§åŒ…æ‹¬æŒ‡ä»¤å¾®è°ƒã€æç¤ºå·¥ç¨‹å’Œå¼ºåŒ–å­¦ä¹ ç­‰ã€‚</li>
<li>åŸºäºäººç±»åå¥½çš„å¼ºåŒ–å­¦ä¹ ç­–ç•¥å¯ä»¥å¸®åŠ©å®ç°æƒ…æ„Ÿç†è§£å’Œç”Ÿæˆçš„ç²¾ç»†åŒ–æˆ–å¤šç›®æ ‡æ§åˆ¶ã€‚</li>
<li>æ–‡ç« æ€»ç»“äº†AUå’ŒAGçš„è¯„ä¼°æ ‡å‡†å’ŒæŒ‘æˆ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.04638">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_LLM/2408.04638v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_LLM/2408.04638v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_LLM/2408.04638v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_LLM/2408.04638v2/page_3_0.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Baba-Is-AI-Break-the-Rules-to-Beat-the-Benchmark"><a href="#Baba-Is-AI-Break-the-Rules-to-Beat-the-Benchmark" class="headerlink" title="Baba Is AI: Break the Rules to Beat the Benchmark"></a>Baba Is AI: Break the Rules to Beat the Benchmark</h2><p><strong>Authors:Nathan Cloos, Meagan Jens, Michelangelo Naim, Yen-Ling Kuo, Ignacio Cases, Andrei Barbu, Christopher J. Cueva</strong></p>
<p>Humans solve problems by following existing rules and procedures, and also by leaps of creativity to redefine those rules and objectives. To probe these abilities, we developed a new benchmark based on the game Baba Is You where an agent manipulates both objects in the environment and rules, represented by movable tiles with words written on them, to reach a specified goal and win the game. We test three state-of-the-art multi-modal large language models (OpenAI GPT-4o, Google Gemini-1.5-Pro and Gemini-1.5-Flash) and find that they fail dramatically when generalization requires that the rules of the game must be manipulated and combined. </p>
<blockquote>
<p>äººç±»é€šè¿‡éµå¾ªç°æœ‰è§„åˆ™å’Œç¨‹åºæ¥è§£å†³é—®é¢˜ï¼ŒåŒæ—¶ä¹Ÿé€šè¿‡åˆ›é€ æ€§çš„é£è·ƒæ¥é‡æ–°å®šä¹‰è¿™äº›è§„åˆ™å’Œç›®æ ‡ã€‚ä¸ºäº†æ¢ç´¢è¿™äº›èƒ½åŠ›ï¼Œæˆ‘ä»¬åŸºäºæ¸¸æˆã€Šä¸Šä¸‹å·¦å³ç§»åŠ¨çˆ¸çˆ¸ã€‹å¼€å‘äº†ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ã€‚åœ¨è¿™ä¸ªæ¸¸æˆä¸­ï¼Œä»£ç†éœ€è¦æ“ä½œç¯å¢ƒä¸­çš„ç‰©ä½“å’Œè§„åˆ™ï¼ˆä»¥å¸¦æœ‰æ–‡å­—çš„ç§»åŠ¨ç“·ç –çš„å½¢å¼å‘ˆç°ï¼‰ï¼Œä»¥è¾¾åˆ°ç‰¹å®šç›®æ ‡å¹¶èµ¢å¾—æ¸¸æˆã€‚æˆ‘ä»¬æµ‹è¯•äº†ä¸‰ç§æœ€æ–°çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆOpenAIçš„GPT-4oã€Googleçš„Gemini-1.5-Proå’ŒGemini-1.5-Flashï¼‰ï¼Œå‘ç°å®ƒä»¬åœ¨éœ€è¦æ“çºµå’Œç»„åˆæ¸¸æˆè§„åˆ™è¿›è¡Œæ³›åŒ–æ—¶è¡¨ç°æ˜¾è‘—ä¸ä½³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.13729v2">PDF</a> 8 pages, 8 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ‘˜è¦ä»‹ç»äº†ä¸€ä¸ªåŸºäºæ¸¸æˆã€Šä½ çˆ·çˆ·æ˜¯â€œä½ â€å—ï¼Ÿã€‹å¼€å‘çš„æ–°åŸºå‡†æµ‹è¯•ç³»ç»Ÿï¼Œè¯¥ç¯å¢ƒè®©æ™ºèƒ½ä»£ç†èƒ½æ“æ§æ¸¸æˆç‰©ä½“å’Œæ¸¸æˆå†…çš„è§„åˆ™ã€‚æµ‹è¯•ä¸­å¯¹æ¯”äº†ä¸‰æ¬¾å½“å‰æµè¡Œçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå‘ç°åœ¨æ¶‰åŠéœ€è¦å¯¹æ¸¸æˆè§„åˆ™è¿›è¡Œæ“æ§å’Œç»„åˆç­‰æ³›åŒ–åº”ç”¨æ—¶ï¼Œå®ƒä»¬è¡¨ç°æ˜¾è‘—ä¸ä½³ã€‚æœ¬ç ”ç©¶æ—¢æ¶‰åŠåˆ°äº†éµå®ˆè§„åˆ™åˆè€ƒè™‘åˆ°è§„åˆ™çš„é‡å®šä¹‰ä¸åˆ›æ–°åº”ç”¨ï¼ŒæŒ‘æˆ˜äº†äººç±»å¯¹è§„åˆ™æ“çºµä¸åˆ›é€ æ€§æ€ç»´çš„åŒé‡æŒ‘æˆ˜èƒ½åŠ›ã€‚æ­¤æ¨¡å‹è¿›ä¸€æ­¥æ¨è¿›äº†æˆ‘ä»¬å¯¹å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚ç¯å¢ƒä¸­çš„ç†è§£å’Œåº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶äººå‘˜å¼€å‘äº†ä¸€ç§åŸºäºæ¸¸æˆã€Šä½ çˆ·çˆ·æ˜¯â€œä½ â€å—ï¼Ÿã€‹çš„æ–°åŸºå‡†æµ‹è¯•ç³»ç»Ÿï¼Œç”¨äºè¯„ä¼°æ™ºèƒ½ä»£ç†è§£å†³é—®é¢˜æ—¶çš„èƒ½åŠ›ã€‚</li>
<li>æµ‹è¯•ç³»ç»Ÿå…è®¸æ™ºèƒ½ä»£ç†æ“æ§æ¸¸æˆå†…çš„ç‰©ä½“å’Œè§„åˆ™ï¼Œä»¥è¾¾æˆç‰¹å®šç›®æ ‡ã€‚</li>
<li>ä¸‰æ¬¾å½“å‰æµè¡Œçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹å‚ä¸äº†æµ‹è¯•ï¼ŒåŒ…æ‹¬OpenAI GPT-4oã€Google Gemini-1.5-Proå’ŒGemini-1.5-Flashã€‚</li>
<li>åœ¨æ³›åŒ–åº”ç”¨å±‚é¢ï¼Œå°¤å…¶æ˜¯éœ€è¦æ“æ§å’Œç»„åˆè§„åˆ™æ—¶ï¼Œè¿™äº›è¯­è¨€æ¨¡å‹è¡¨ç°ä¸ä½³ã€‚</li>
<li>ç ”ç©¶è¡¨æ˜ï¼Œç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚ç¯å¢ƒä¸­å¯¹è§„åˆ™çš„æ“æ§å’Œç»„åˆèƒ½åŠ›æœ‰å¾…æé«˜ã€‚</li>
<li>æµ‹è¯•ç³»ç»Ÿä¸ä»…è¯„ä¼°äº†éµå¾ªè§„åˆ™çš„èƒ½åŠ›ï¼Œè¿˜æŒ‘æˆ˜äº†è§„åˆ™çš„é‡å®šä¹‰ä¸åˆ›æ–°åº”ç”¨çš„èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.13729">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_LLM/2407.13729v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_LLM/2407.13729v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_LLM/2407.13729v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_LLM/2407.13729v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_LLM/2407.13729v2/page_3_1.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-12/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-12/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-12/Agent/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Agent/2506.07392v2/page_4_0.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-12  Automatic Failure Attribution and Critical Step Prediction Method for   Multi-Agent Systems Based on Causal Inference
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-12
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-12/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.06980v1/page_0_0.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-12  A Survey of Reinforcement Learning for Large Reasoning Models
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-12
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">31373.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
