<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-12  A Survey of Reinforcement Learning for Large Reasoning Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.06980v1/page_0_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-12
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-07
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    23k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    94 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-09-12-æ›´æ–°"><a href="#2025-09-12-æ›´æ–°" class="headerlink" title="2025-09-12 æ›´æ–°"></a>2025-09-12 æ›´æ–°</h1><h2 id="A-Survey-of-Reinforcement-Learning-for-Large-Reasoning-Models"><a href="#A-Survey-of-Reinforcement-Learning-for-Large-Reasoning-Models" class="headerlink" title="A Survey of Reinforcement Learning for Large Reasoning Models"></a>A Survey of Reinforcement Learning for Large Reasoning Models</h2><p><strong>Authors:Kaiyan Zhang, Yuxin Zuo, Bingxiang He, Youbang Sun, Runze Liu, Che Jiang, Yuchen Fan, Kai Tian, Guoli Jia, Pengfei Li, Yu Fu, Xingtai Lv, Yuchen Zhang, Sihang Zeng, Shang Qu, Haozhan Li, Shijie Wang, Yuru Wang, Xinwei Long, Fangfu Liu, Xiang Xu, Jiaze Ma, Xuekai Zhu, Ermo Hua, Yihao Liu, Zonglin Li, Huayu Chen, Xiaoye Qu, Yafu Li, Weize Chen, Zhenzhao Yuan, Junqi Gao, Dong Li, Zhiyuan Ma, Ganqu Cui, Zhiyuan Liu, Biqing Qi, Ning Ding, Bowen Zhou</strong></p>
<p>In this paper, we survey recent advances in Reinforcement Learning (RL) for reasoning with Large Language Models (LLMs). RL has achieved remarkable success in advancing the frontier of LLM capabilities, particularly in addressing complex logical tasks such as mathematics and coding. As a result, RL has emerged as a foundational methodology for transforming LLMs into LRMs. With the rapid progress of the field, further scaling of RL for LRMs now faces foundational challenges not only in computational resources but also in algorithm design, training data, and infrastructure. To this end, it is timely to revisit the development of this domain, reassess its trajectory, and explore strategies to enhance the scalability of RL toward Artificial SuperIntelligence (ASI). In particular, we examine research applying RL to LLMs and LRMs for reasoning abilities, especially since the release of DeepSeek-R1, including foundational components, core problems, training resources, and downstream applications, to identify future opportunities and directions for this rapidly evolving area. We hope this review will promote future research on RL for broader reasoning models. Github: <a target="_blank" rel="noopener" href="https://github.com/TsinghuaC3I/Awesome-RL-for-LRMs">https://github.com/TsinghuaC3I/Awesome-RL-for-LRMs</a> </p>
<blockquote>
<p>åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æ¦‚è¿°äº†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„æœ€æ–°è¿›å±•ã€‚å¼ºåŒ–å­¦ä¹ åœ¨æ¨è¿›å¤§å‹è¯­è¨€æ¨¡å‹èƒ½åŠ›æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œç‰¹åˆ«æ˜¯åœ¨è§£å†³æ•°å­¦å’Œç¼–ç ç­‰å¤æ‚é€»è¾‘ä»»åŠ¡æ–¹é¢ã€‚å› æ­¤ï¼Œå¼ºåŒ–å­¦ä¹ å·²ç»æˆä¸ºå°†å¤§å‹è¯­è¨€æ¨¡å‹è½¬åŒ–ä¸ºå¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰çš„åŸºç¡€æ–¹æ³•ã€‚éšç€è¯¥é¢†åŸŸçš„å¿«é€Ÿå‘å±•ï¼Œå¼ºåŒ–å­¦ä¹ åœ¨å¤§å‹æ¨ç†æ¨¡å‹çš„è¿›ä¸€æ­¥æ‰©å±•æ–¹é¢ç°åœ¨é¢ä¸´ç€ä¸ä»…æ˜¯è®¡ç®—èµ„æºçš„åŸºç¡€æŒ‘æˆ˜ï¼Œè¿˜æœ‰ç®—æ³•è®¾è®¡ã€è®­ç»ƒæ•°æ®å’ŒåŸºç¡€è®¾æ–½æ–¹é¢çš„æŒ‘æˆ˜ã€‚å› æ­¤ï¼Œé‡æ–°å®¡è§†è¿™ä¸ªé¢†åŸŸçš„å‘å±•ï¼Œé‡æ–°è¯„ä¼°å…¶è½¨è¿¹ï¼Œæ¢ç´¢æé«˜å¼ºåŒ–å­¦ä¹ åœ¨äººå·¥è¶…æ™ºèƒ½ï¼ˆASIï¼‰æ–¹é¢å¯æ‰©å±•æ€§çš„ç­–ç•¥æ˜¯ååˆ†åŠæ—¶çš„ã€‚å°¤å…¶æ˜¯æˆ‘ä»¬å…³æ³¨è‡ªDeepSeek-R1å‘å¸ƒä»¥æ¥ï¼Œå°†å¼ºåŒ–å­¦ä¹ åº”ç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹å’Œå¤§å‹æ¨ç†æ¨¡å‹çš„ç ”ç©¶ï¼ŒåŒ…æ‹¬åŸºç¡€ç»„ä»¶ã€æ ¸å¿ƒé—®é¢˜ã€è®­ç»ƒèµ„æºå’Œä¸‹æ¸¸åº”ç”¨ï¼Œä»¥ç¡®å®šè¿™ä¸ªå¿«é€Ÿæ¼”å˜é¢†åŸŸçš„æœªæ¥æœºé‡å’Œæ–¹å‘ã€‚æˆ‘ä»¬å¸Œæœ›è¿™æ¬¡ç»¼è¿°èƒ½æ¨åŠ¨å¼ºåŒ–å­¦ä¹ åœ¨æ›´å¹¿æ³›çš„æ¨ç†æ¨¡å‹æ–¹é¢çš„æœªæ¥ç ”ç©¶ã€‚Githubï¼š<a target="_blank" rel="noopener" href="https://github.com/TsinghuaC3I/Awesome-RL-for-LRMs">https://github.com/TsinghuaC3I/Awesome-RL-for-LRMs</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.08827v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¼ºåŒ–å­¦ä¹ åœ¨æ¨åŠ¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œé€»è¾‘æ¨ç†æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°å­¦å’Œç¼–ç ç­‰å¤æ‚é€»è¾‘ä»»åŠ¡ä¸Šã€‚ç„¶è€Œï¼Œéšç€é¢†åŸŸçš„å¿«é€Ÿå‘å±•ï¼Œå¼ºåŒ–å­¦ä¹ åœ¨å¯æ‰©å±•æ€§æ–¹é¢é¢ä¸´æŒ‘æˆ˜ã€‚æœ¬æ–‡å›é¡¾äº†å¼ºåŒ–å­¦ä¹ åœ¨å¤§å‹è¯­è¨€æ¨¡å‹åŠå¤§å‹æ¨ç†æ¨¡å‹ä¸­çš„åº”ç”¨ï¼ŒåŒ…æ‹¬åŸºç¡€ç»„ä»¶ã€æ ¸å¿ƒé—®é¢˜ã€è®­ç»ƒèµ„æºå’Œä¸‹æ¸¸åº”ç”¨ï¼Œä¸ºæœªæ¥ç ”ç©¶æä¾›äº†æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¼ºåŒ–å­¦ä¹ åœ¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œé€»è¾‘æ¨ç†æ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ åœ¨æ•°å­¦å’Œç¼–ç ç­‰å¤æ‚é€»è¾‘ä»»åŠ¡ä¸Šçš„è¡¨ç°å°¤ä¸ºçªå‡ºã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ åœ¨å¯æ‰©å±•æ€§æ–¹é¢é¢ä¸´æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬è®¡ç®—èµ„æºã€ç®—æ³•è®¾è®¡ã€è®­ç»ƒæ•°æ®å’ŒåŸºç¡€è®¾æ–½ç­‰æ–¹é¢ã€‚</li>
<li>å›é¡¾äº†å¼ºåŒ–å­¦ä¹ åœ¨å¤§å‹è¯­è¨€æ¨¡å‹åŠå¤§å‹æ¨ç†æ¨¡å‹ä¸­çš„åº”ç”¨ï¼ŒåŒ…æ‹¬åŸºç¡€ç»„ä»¶ã€æ ¸å¿ƒé—®é¢˜ã€è®­ç»ƒèµ„æºå’Œä¸‹æ¸¸åº”ç”¨ã€‚</li>
<li>æ–‡ç« æåˆ°äº†ä»DeepSeek-R1å‘å¸ƒä»¥æ¥çš„ç ”ç©¶è¿›å±•ã€‚</li>
<li>æ–‡ç« ä¸ºæœªæ¥ç ”ç©¶æä¾›äº†æ–¹å‘ï¼Œå¸Œæœ›ä¿ƒè¿›å¼ºåŒ–å­¦ä¹ åœ¨æ›´å¹¿æ³›çš„æ¨ç†æ¨¡å‹é¢†åŸŸçš„ç ”ç©¶ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.08827">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.08827v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.08827v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.08827v1/page_5_0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="RewardDance-Reward-Scaling-in-Visual-Generation"><a href="#RewardDance-Reward-Scaling-in-Visual-Generation" class="headerlink" title="RewardDance: Reward Scaling in Visual Generation"></a>RewardDance: Reward Scaling in Visual Generation</h2><p><strong>Authors:Jie Wu, Yu Gao, Zilyu Ye, Ming Li, Liang Li, Hanzhong Guo, Jie Liu, Zeyue Xue, Xiaoxia Hou, Wei Liu, Yan Zeng, Weilin Huang</strong></p>
<p>Reward Models (RMs) are critical for improving generation models via Reinforcement Learning (RL), yet the RM scaling paradigm in visual generation remains largely unexplored. It primarily due to fundamental limitations in existing approaches: CLIP-based RMs suffer from architectural and input modality constraints, while prevalent Bradley-Terry losses are fundamentally misaligned with the next-token prediction mechanism of Vision-Language Models (VLMs), hindering effective scaling. More critically, the RLHF optimization process is plagued by Reward Hacking issue, where models exploit flaws in the reward signal without improving true quality. To address these challenges, we introduce RewardDance, a scalable reward modeling framework that overcomes these barriers through a novel generative reward paradigm. By reformulating the reward score as the modelâ€™s probability of predicting a â€œyesâ€ token, indicating that the generated image outperforms a reference image according to specific criteria, RewardDance intrinsically aligns reward objectives with VLM architectures. This alignment unlocks scaling across two dimensions: (1) Model Scaling: Systematic scaling of RMs up to 26 billion parameters; (2) Context Scaling: Integration of task-specific instructions, reference examples, and chain-of-thought (CoT) reasoning. Extensive experiments demonstrate that RewardDance significantly surpasses state-of-the-art methods in text-to-image, text-to-video, and image-to-video generation. Crucially, we resolve the persistent challenge of â€œreward hackingâ€: Our large-scale RMs exhibit and maintain high reward variance during RL fine-tuning, proving their resistance to hacking and ability to produce diverse, high-quality outputs. It greatly relieves the mode collapse problem that plagues smaller models. </p>
<blockquote>
<p>å¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰å¯¹äºé€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ”¹è¿›ç”Ÿæˆæ¨¡å‹è‡³å…³é‡è¦ï¼Œä½†è§†è§‰ç”Ÿæˆä¸­çš„RMè§„æ¨¡èŒƒå¼ä»åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šæœªè¢«æ¢ç´¢ã€‚è¿™ä¸»è¦æ˜¯ç”±äºç°æœ‰æ–¹æ³•çš„åŸºæœ¬å±€é™æ€§ï¼šåŸºäºCLIPçš„RMå—åˆ°æ¶æ„å’Œè¾“å…¥æ¨¡å¼çš„çº¦æŸï¼Œè€Œæ™®éçš„Bradley-TerryæŸå¤±ä¸è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„ä¸‹ä¸€ä¸ªä»¤ç‰Œé¢„æµ‹æœºåˆ¶å­˜åœ¨æ ¹æœ¬ä¸Šçš„ä¸åŒ¹é…ï¼Œé˜»ç¢äº†æœ‰æ•ˆçš„æ‰©å±•ã€‚æ›´é‡è¦çš„æ˜¯ï¼ŒRLHFä¼˜åŒ–è¿‡ç¨‹å—åˆ°å¥–åŠ±é»‘å®¢é—®é¢˜çš„å›°æ‰°ï¼Œæ¨¡å‹ä¼šåˆ©ç”¨å¥–åŠ±ä¿¡å·çš„ç¼ºé™·ï¼Œè€Œä¸æé«˜çœŸæ­£çš„è´¨é‡ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†RewardDanceï¼Œè¿™æ˜¯ä¸€ä¸ªå¯æ‰©å±•çš„å¥–åŠ±å»ºæ¨¡æ¡†æ¶ï¼Œé€šè¿‡ä¸€ç§æ–°çš„ç”Ÿæˆå¥–åŠ±èŒƒå¼æ¥å…‹æœè¿™äº›éšœç¢ã€‚é€šè¿‡å°†å¥–åŠ±åˆ†æ•°é‡æ–°å®šä¹‰ä¸ºæ¨¡å‹é¢„æµ‹â€œæ˜¯â€ä»¤ç‰Œçš„æ¦‚ç‡ï¼Œè¡¨ç¤ºç”Ÿæˆçš„å›¾åƒæ ¹æ®ç‰¹å®šæ ‡å‡†ä¼˜äºå‚è€ƒå›¾åƒï¼ŒRewardDanceå†…åœ¨åœ°å°†å¥–åŠ±ç›®æ ‡ä¸VLMæ¶æ„å¯¹é½ã€‚è¿™ç§å¯¹é½è§£é”äº†ä¸¤ä¸ªç»´åº¦çš„æ‰©å±•ï¼šï¼ˆ1ï¼‰æ¨¡å‹æ‰©å±•ï¼šRMçš„ç³»ç»Ÿæ‰©å±•é«˜è¾¾26äº¿ä¸ªå‚æ•°ï¼›ï¼ˆ2ï¼‰ä¸Šä¸‹æ–‡æ‰©å±•ï¼šæ•´åˆä»»åŠ¡ç‰¹å®šçš„æŒ‡ä»¤ã€å‚è€ƒç¤ºä¾‹å’Œé“¾çŠ¶æ€ç»´ï¼ˆCoTï¼‰æ¨ç†ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒRewardDanceåœ¨æ–‡æœ¬åˆ°å›¾åƒã€æ–‡æœ¬åˆ°è§†é¢‘å’Œå›¾åƒåˆ°è§†é¢‘ç”Ÿæˆæ–¹é¢æ˜¾è‘—è¶…è¶Šäº†æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚æœ€é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬è§£å†³äº†â€œå¥–åŠ±é»‘å®¢â€è¿™ä¸€æŒä¹…æŒ‘æˆ˜ï¼šæˆ‘ä»¬çš„å¤§è§„æ¨¡RMåœ¨RLå¾®è°ƒè¿‡ç¨‹ä¸­è¡¨ç°å‡ºè¾ƒé«˜çš„å¥–åŠ±æ–¹å·®ï¼Œè¯æ˜äº†å…¶æŠ—é»‘å®¢æ”»å‡»çš„èƒ½åŠ›ä»¥åŠäº§ç”Ÿå¤šæ ·åŒ–é«˜è´¨é‡è¾“å‡ºçš„èƒ½åŠ›ã€‚è¿™æå¤§åœ°ç¼“è§£äº†å›°æ‰°è¾ƒå°æ¨¡å‹çš„æ¨¡å¼å´©æºƒé—®é¢˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.08826v1">PDF</a> Bytedance Seed Technical Report</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†RewardDanceè¿™ä¸€å¥–åŠ±å»ºæ¨¡æ¡†æ¶ï¼Œé’ˆå¯¹è§†è§‰ç”Ÿæˆä¸­çš„å¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰æ‰©å±•é—®é¢˜æå‡ºè§£å†³æ–¹æ¡ˆã€‚è¯¥æ¡†æ¶é€šè¿‡ç”Ÿæˆå¥–åŠ±æ¨¡å‹èŒƒå¼è§£å†³äº†RMå­˜åœ¨çš„éšœç¢ï¼Œé€šè¿‡ä¸è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰æ¶æ„çš„å†…åœ¨å¯¹é½ï¼Œå®ç°æ¨¡å‹æ‰©å±•å’Œä¸Šä¸‹æ–‡æ‰©å±•ã€‚å®éªŒè¡¨æ˜ï¼ŒRewardDanceåœ¨æ–‡æœ¬è½¬å›¾åƒã€æ–‡æœ¬è½¬è§†é¢‘å’Œå›¾åƒè½¬è§†é¢‘ç”Ÿæˆæ–¹é¢è¿œè¶…ç°æœ‰æ–¹æ³•ï¼Œè§£å†³äº†å¥–åŠ±é»‘å®¢æ”»å‡»çš„æŒ‘æˆ˜ï¼Œå¹¶å±•ç°å‡ºæŠµæŠ—é»‘å®¢æ”»å‡»çš„èƒ½åŠ›ï¼Œèƒ½å¤Ÿäº§ç”Ÿå¤šæ ·ä¸”é«˜è´¨é‡çš„è¾“å‡ºï¼Œç¼“è§£äº†å°å‹æ¨¡å‹çš„æ¨¡å¼å´©æºƒé—®é¢˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰åœ¨é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ”¹è¿›ç”Ÿæˆæ¨¡å‹æ–¹é¢èµ·ç€å…³é”®ä½œç”¨ï¼Œä½†åœ¨è§†è§‰ç”Ÿæˆé¢†åŸŸä¸­çš„RMæ‰©å±•æ¨¡å¼å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚</li>
<li>å½“å‰å­˜åœ¨çš„RMæ–¹æ³•å­˜åœ¨æ¶æ„å’Œè¾“å…¥æ¨¡å¼çº¦æŸï¼Œä»¥åŠBradley-TerryæŸå¤±ä¸è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„ä¸‹ä¸€ä¸ªä»¤ç‰Œé¢„æµ‹æœºåˆ¶ä¹‹é—´çš„æ ¹æœ¬æ€§ä¸åŒ¹é…é—®é¢˜ã€‚</li>
<li>RewardDanceæ¡†æ¶è§£å†³äº†è¿™äº›é—®é¢˜ï¼Œé€šè¿‡ä¸€ç§æ–°çš„ç”Ÿæˆå¥–åŠ±æ¨¡å‹èŒƒå¼å®ç°ä¸VLMæ¶æ„çš„å†…åœ¨å¯¹é½ã€‚</li>
<li>RewardDanceå®ç°äº†ä¸¤ä¸ªç»´åº¦çš„æ‰©å±•ï¼šæ¨¡å‹æ‰©å±•å’Œä¸Šä¸‹æ–‡æ‰©å±•ï¼Œä½¿å¾—RMèƒ½å¤Ÿæ‰©å±•åˆ°26äº¿å‚æ•°ï¼Œå¹¶æ•´åˆç‰¹å®šä»»åŠ¡æŒ‡ä»¤ã€å‚è€ƒç¤ºä¾‹å’Œé“¾å¼æ€ç»´æ¨ç†ã€‚</li>
<li>RewardDanceåœ¨æ–‡æœ¬è½¬å›¾åƒã€æ–‡æœ¬è½¬è§†é¢‘å’Œå›¾åƒè½¬è§†é¢‘ç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚</li>
<li>è¯¥æ–¹æ³•è§£å†³äº†æŒç»­å­˜åœ¨çš„â€œå¥–åŠ±é»‘å®¢æ”»å‡»â€æŒ‘æˆ˜ï¼Œå±•ç°å‡ºæŠµæŠ—é»‘å®¢æ”»å‡»çš„èƒ½åŠ›ï¼Œå¹¶èƒ½äº§ç”Ÿå¤šæ ·ä¸”é«˜è´¨é‡çš„è¾“å‡ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.08826">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.08826v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.08826v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.08826v1/page_1_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.08826v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.08826v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Merge-of-Thought-Distillation"><a href="#Merge-of-Thought-Distillation" class="headerlink" title="Merge-of-Thought Distillation"></a>Merge-of-Thought Distillation</h2><p><strong>Authors:Zhanming Shen, Zeyu Qin, Zenan Huang, Hao Chen, Jiaqi Hu, Yihong Zhuang, Guoshan Lu, Gang Chen, Junbo Zhao</strong></p>
<p>Efficient reasoning distillation for long chain-of-thought (CoT) models is increasingly constrained by the assumption of a single oracle teacher, despite practical availability of multiple candidate teachers and growing CoT corpora. We revisit teacher selection and observe that different students have different â€œbest teachers,â€ and even for the same student the best teacher can vary across datasets. Therefore, to unify multiple teachersâ€™ reasoning abilities into student with overcoming conflicts among various teachersâ€™ supervision, we propose Merge-of-Thought Distillation (MoT), a lightweight framework that alternates between teacher-specific supervised fine-tuning branches and weight-space merging of the resulting student variants. On competition math benchmarks, using only about 200 high-quality CoT samples, applying MoT to a Qwen3-14B student surpasses strong models including DEEPSEEK-R1, QWEN3-30B-A3B, QWEN3-32B, and OPENAI-O1, demonstrating substantial gains. Besides, MoT consistently outperforms the best single-teacher distillation and the naive multi-teacher union, raises the performance ceiling while mitigating overfitting, and shows robustness to distribution-shifted and peer-level teachers. Moreover, MoT reduces catastrophic forgetting, improves general reasoning beyond mathematics and even cultivates a better teacher, indicating that consensus-filtered reasoning features transfer broadly. These results position MoT as a simple, scalable route to efficiently distilling long CoT capabilities from diverse teachers into compact students. </p>
<blockquote>
<p>é’ˆå¯¹é•¿é“¾æ¡æ€ç»´ï¼ˆCoTï¼‰æ¨¡å‹çš„æ¨ç†è’¸é¦æ•ˆç‡è¶Šæ¥è¶Šå—åˆ°å•ä¸€æƒå¨æ•™å¸ˆå‡è®¾çš„é™åˆ¶ï¼Œå°½ç®¡å®é™…ä¸Šæœ‰å¤šä¸ªå€™é€‰æ•™å¸ˆï¼Œå¹¶ä¸”CoTè¯­æ–™åº“ä¸æ–­å¢é•¿ã€‚æˆ‘ä»¬é‡æ–°å®¡è§†æ•™å¸ˆé€‰æ‹©ï¼Œå¹¶è§‚å¯Ÿåˆ°ä¸åŒçš„å­¦ç”Ÿæœ‰ä¸åŒçš„â€œæœ€ä½³æ•™å¸ˆâ€ï¼Œå³ä½¿å¯¹äºåŒä¸€å­¦ç”Ÿï¼Œæœ€ä½³æ•™å¸ˆä¹Ÿä¼šå› æ•°æ®é›†è€Œå¼‚ã€‚å› æ­¤ï¼Œä¸ºäº†å°†å­¦ç”Ÿä¸å¤šä¸ªæ•™å¸ˆçš„æ¨ç†èƒ½åŠ›ç›¸ç»“åˆï¼Œå¹¶å…‹æœå„ç§æ•™å¸ˆç›‘ç£ä¹‹é—´çš„å†²çªï¼Œæˆ‘ä»¬æå‡ºäº†â€œæ€ç»´èåˆè’¸é¦â€ï¼ˆMoTï¼‰æ–¹æ³•ã€‚å®ƒæ˜¯ä¸€ä¸ªè½»é‡çº§çš„æ¡†æ¶ï¼Œäº¤æ›¿è¿›è¡Œé’ˆå¯¹ç‰¹å®šæ•™å¸ˆçš„ç›‘ç£å¾®è°ƒåˆ†æ”¯å’Œç»“æœå­¦ç”Ÿå˜é‡çš„æƒé‡ç©ºé—´åˆå¹¶ã€‚åœ¨ç«èµ›æ•°å­¦åŸºå‡†æµ‹è¯•ä¸­ï¼Œä»…ä½¿ç”¨å¤§çº¦200ä¸ªé«˜è´¨é‡CoTæ ·æœ¬ï¼Œå°†MoTåº”ç”¨äºQwen3-14Bå­¦ç”Ÿï¼Œè¶…è¿‡äº†DEEPSEEK-R1ã€QWEN3-30B-A3Bã€QWEN3-32Bå’ŒOPENAI-O1ç­‰å¼ºå¤§æ¨¡å‹ï¼Œæ˜¾ç¤ºå‡ºå·¨å¤§çš„è¿›æ­¥ã€‚æ­¤å¤–ï¼ŒMoTå§‹ç»ˆä¼˜äºæœ€ä½³å•æ•™å¸ˆè’¸é¦å’Œç®€å•çš„å¤šæ•™å¸ˆè”åˆæ–¹æ³•ï¼Œæé«˜äº†æ€§èƒ½ä¸Šé™ï¼ŒåŒæ—¶å‡è½»äº†è¿‡æ‹Ÿåˆé—®é¢˜ï¼Œå¹¶å¯¹åˆ†å¸ƒåç§»å’Œæ•™å¸ˆåŒè¡Œè¡¨ç°å‡ºç¨³å¥æ€§ã€‚è€Œä¸”ï¼ŒMoTå‡å°‘äº†ç¾éš¾æ€§é—å¿˜ï¼Œæé«˜äº†é™¤æ•°å­¦ä¹‹å¤–çš„ä¸€èˆ¬æ¨ç†èƒ½åŠ›ï¼Œç”šè‡³åŸ¹å…»äº†æ›´å¥½çš„æ•™å¸ˆï¼Œè¡¨æ˜ç»è¿‡å…±è¯†è¿‡æ»¤çš„æ¨ç†ç‰¹å¾å…·æœ‰å¹¿æ³›çš„è¿ç§»æ€§ã€‚è¿™äº›ç»“æœå°†MoTå®šä½ä¸ºä»å¤šç§æ•™å¸ˆä¸­æœ‰æ•ˆè’¸é¦é•¿CoTèƒ½åŠ›åˆ°ç´§å‡‘å­¦ç”Ÿçš„ç®€å•ä¸”å¯æ‰©å±•çš„é€”å¾„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.08814v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¦‚ä½•åœ¨é•¿é“¾æ€ç»´ï¼ˆCoTï¼‰æ¨¡å‹ä¸­å®ç°æœ‰æ•ˆçš„æ¨ç†è’¸é¦ã€‚æ–‡ç« é‡æ–°å®¡è§†äº†æ•™å¸ˆé€‰æ‹©çš„é—®é¢˜ï¼Œå¹¶æå‡ºä¸åŒå­¦ç”Ÿæœ‰ä¸åŒçš„â€œæœ€ä½³æ•™å¸ˆâ€ï¼Œè€Œä¸”åŒä¸€å­¦ç”Ÿçš„æœ€ä½³æ•™å¸ˆä¹Ÿä¼šå› æ•°æ®é›†è€Œå¼‚ã€‚ä¸ºæ­¤ï¼Œæ–‡ç« æå‡ºäº†Merge-of-Thought Distillationï¼ˆMoTï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡äº¤æ›¿è¿›è¡Œæ•™å¸ˆç‰¹å®šçš„ç›‘ç£å¾®è°ƒåˆ†æ”¯å’Œæƒé‡ç©ºé—´çš„åˆå¹¶å­¦ç”Ÿå˜ä½“ï¼Œå°†å¤šä¸ªæ•™å¸ˆçš„æ¨ç†èƒ½åŠ›ç»Ÿä¸€åˆ°å­¦ç”Ÿæ¨¡å‹ä¸­ã€‚åœ¨ç«èµ›æ•°å­¦åŸºå‡†æµ‹è¯•ä¸­ï¼Œä½¿ç”¨ä»…çº¦200ä¸ªé«˜è´¨é‡CoTæ ·æœ¬çš„MoTå¯¹Qwen3-14Bå­¦ç”Ÿçš„åº”ç”¨è¶…è¶Šäº†å…¶ä»–å¼ºå¤§æ¨¡å‹ï¼Œå¹¶è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½æå‡ã€ç¨³å¥æ€§å’Œå‡å°‘ç¾éš¾æ€§é—å¿˜ç­‰ä¼˜ç‚¹ã€‚æ€»ä½“è€Œè¨€ï¼ŒMoTæ˜¯ä¸€ç§ç®€å•ä¸”å¯æ‰©å±•çš„æ–¹æ³•ï¼Œå¯ä»å¤šæ ·åŒ–çš„æ•™å¸ˆä¸­æœ‰æ•ˆåœ°è’¸é¦å‡ºé•¿CoTèƒ½åŠ›å¹¶åº”ç”¨äºç´§å‡‘çš„å­¦ç”Ÿæ¨¡å‹ä¸­ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡ç« æŒ‡å‡ºåœ¨é•¿é“¾æ€ç»´ï¼ˆCoTï¼‰æ¨¡å‹çš„æ¨ç†è’¸é¦ä¸­ï¼Œå•ä¸€æ•™å¸ˆçš„å‡è®¾é™åˆ¶äº†æ•ˆç‡ã€‚</li>
<li>ä¸åŒå­¦ç”Ÿæœ‰ä¸åŒçš„â€œæœ€ä½³æ•™å¸ˆâ€ï¼Œä¸”åŒä¸€å­¦ç”Ÿçš„æœ€ä½³æ•™å¸ˆä¹Ÿä¼šå› æ•°æ®é›†è€Œå¼‚ã€‚</li>
<li>æå‡ºäº†Merge-of-Thought Distillationï¼ˆMoTï¼‰æ¡†æ¶ï¼Œç»“åˆäº†å¤šä¸ªæ•™å¸ˆçš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>MoTé€šè¿‡äº¤æ›¿è¿›è¡Œæ•™å¸ˆç‰¹å®šçš„ç›‘ç£å¾®è°ƒåˆ†æ”¯å’Œæƒé‡ç©ºé—´åˆå¹¶ï¼Œæé«˜äº†å­¦ç”Ÿæ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>åœ¨ç«èµ›æ•°å­¦åŸºå‡†æµ‹è¯•ä¸­ï¼ŒMoTè¡¨ç°å‡ºä¼˜è¶Šçš„æ€§èƒ½ï¼Œè¶…è¶Šäº†å…¶ä»–å¼ºå¤§æ¨¡å‹ã€‚</li>
<li>MoTå…·æœ‰å‡å°‘ç¾éš¾æ€§é—å¿˜ã€æé«˜æ³›åŒ–èƒ½åŠ›å’ŒåŸ¹å…»æ›´å¥½æ•™å¸ˆç­‰ä¼˜ç‚¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.08814">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.08814v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.08814v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.08814v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.08814v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="AdsQA-Towards-Advertisement-Video-Understanding"><a href="#AdsQA-Towards-Advertisement-Video-Understanding" class="headerlink" title="AdsQA: Towards Advertisement Video Understanding"></a>AdsQA: Towards Advertisement Video Understanding</h2><p><strong>Authors:Xinwei Long, Kai Tian, Peng Xu, Guoli Jia, Jingxuan Li, Sa Yang, Yihua Shao, Kaiyan Zhang, Che Jiang, Hao Xu, Yang Liu, Jiaheng Ma, Bowen Zhou</strong></p>
<p>Large language models (LLMs) have taken a great step towards AGI. Meanwhile, an increasing number of domain-specific problems such as math and programming boost these general-purpose models to continuously evolve via learning deeper expertise. Now is thus the time further to extend the diversity of specialized applications for knowledgeable LLMs, though collecting high quality data with unexpected and informative tasks is challenging. In this paper, we propose to use advertisement (ad) videos as a challenging test-bed to probe the ability of LLMs in perceiving beyond the objective physical content of common visual domain. Our motivation is to take full advantage of the clue-rich and information-dense ad videosâ€™ traits, e.g., marketing logic, persuasive strategies, and audience engagement. Our contribution is three-fold: (1) To our knowledge, this is the first attempt to use ad videos with well-designed tasks to evaluate LLMs. We contribute AdsQA, a challenging ad Video QA benchmark derived from 1,544 ad videos with 10,962 clips, totaling 22.7 hours, providing 5 challenging tasks. (2) We propose ReAd-R, a Deepseek-R1 styled RL model that reflects on questions, and generates answers via reward-driven optimization. (3) We benchmark 14 top-tier LLMs on AdsQA, and our \texttt{ReAd-R}~achieves the state-of-the-art outperforming strong competitors equipped with long-chain reasoning capabilities by a clear margin. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é€šç”¨äººå·¥æ™ºèƒ½ï¼ˆAGIï¼‰æ–¹é¢è¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ã€‚åŒæ—¶ï¼Œè¶Šæ¥è¶Šå¤šçš„ç‰¹å®šé¢†åŸŸé—®é¢˜ï¼Œå¦‚æ•°å­¦å’Œç¼–ç¨‹ï¼Œæ¨åŠ¨è¿™äº›é€šç”¨æ¨¡å‹é€šè¿‡æ·±å…¥å­¦ä¹ ä¸“ä¸šçŸ¥è¯†è€Œä¸æ–­å‘å±•ã€‚å› æ­¤ï¼Œç°åœ¨æ˜¯æ—¶å€™è¿›ä¸€æ­¥æ‰©å±•çŸ¥è¯†å‹LLMçš„ä¸“ç”¨åº”ç”¨ç¨‹åºçš„å¤šæ ·æ€§äº†ï¼Œå°½ç®¡æ”¶é›†é«˜è´¨é‡çš„æ•°æ®ä»¥åŠåŒ…å«æ„æƒ³ä¸åˆ°ä¸”å…·æœ‰ä¿¡æ¯å«é‡çš„ä»»åŠ¡å…·æœ‰æŒ‘æˆ˜æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å»ºè®®ä½¿ç”¨å¹¿å‘Šè§†é¢‘ä½œä¸ºä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æµ‹è¯•å¹³å°æ¥æ¢ç´¢LLMæ„ŸçŸ¥æ™®é€šè§†è§‰åŸŸå®¢è§‚ç‰©ç†å†…å®¹ä¹‹å¤–çš„èƒ½åŠ›ã€‚æˆ‘ä»¬çš„åŠ¨æœºæ˜¯å……åˆ†åˆ©ç”¨å¹¿å‘Šè§†é¢‘çº¿ç´¢ä¸°å¯Œã€ä¿¡æ¯å¯†é›†çš„ç‰¹ç‚¹ï¼Œä¾‹å¦‚è¥é”€é€»è¾‘ã€ç­–ç•¥æŠ€å·§å’Œè§‚ä¼—å‚ä¸åº¦ç­‰ã€‚æˆ‘ä»¬çš„è´¡çŒ®åˆ†ä¸ºä¸‰ä¸ªéƒ¨åˆ†ï¼šï¼ˆ1ï¼‰æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯é¦–æ¬¡å°è¯•ä½¿ç”¨ç²¾å¿ƒè®¾è®¡ä»»åŠ¡çš„å¹¿å‘Šè§†é¢‘æ¥è¯„ä¼°LLMã€‚æˆ‘ä»¬è´¡çŒ®äº†AdsQAï¼Œä¸€ä¸ªç”±ç²¾å¿ƒè®¾è®¡çš„ä»»åŠ¡å¾—å‡ºçš„å¹¿å‘Šè§†é¢‘é—®ç­”åŸºå‡†æµ‹è¯•æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†ç”±ç²¾å¿ƒæŒ‘é€‰çš„æ¶µç›–å¤šè¾¾è¶…è¿‡2ä¸‡å¤šä¸ªå¹¿å‘Šçš„å¤šä¸ªç‰‡æ®µæ„æˆï¼ˆå…±è®¡çº¦ä¸€ä¸‡äº”åƒå››ç™¾å››åå››åˆ†é’Ÿï¼‰ï¼ŒåŒ…å«äº”ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚ï¼ˆ2ï¼‰æˆ‘ä»¬æå‡ºäº†ReAd-Ræ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºDeepseek-R1é£æ ¼çš„å¼ºåŒ–å­¦ä¹ æ¨¡å‹ï¼Œå®ƒé€šè¿‡å¯¹é—®é¢˜çš„åæ€å¹¶é€šè¿‡å¥–åŠ±é©±åŠ¨ä¼˜åŒ–ç”Ÿæˆç­”æ¡ˆã€‚ï¼ˆ3ï¼‰æˆ‘ä»¬åœ¨AdsQAåŸºå‡†æµ‹è¯•æ•°æ®é›†ä¸Šå¯¹é¡¶å°–çš„åå››ä¸ªLLMè¿›è¡Œäº†è¯„ä¼°æ¯”è¾ƒï¼Œç»“æœæ˜¾ç¤ºæˆ‘ä»¬çš„ReAd-Ræ¨¡å‹æ˜æ˜¾è¶…è¶Šå…¶ä»–å…·å¤‡é•¿é“¾æ¨ç†èƒ½åŠ›çš„ç«äº‰å¯¹æ‰‹ï¼Œè¾¾åˆ°äº†ä¸šç•Œé¢†å…ˆæ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.08621v1">PDF</a> ICCV-2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é€šç”¨äººå·¥æ™ºèƒ½ï¼ˆAGIï¼‰æ–¹é¢å–å¾—äº†é‡å¤§è¿›å±•ã€‚é’ˆå¯¹ç‰¹å®šé¢†åŸŸçš„æŒ‘æˆ˜å¦‚æ•°å­¦å’Œç¼–ç¨‹ï¼Œè¿™äº›é€šç”¨æ¨¡å‹é€šè¿‡æ·±å…¥å­¦ä¹ ä¸“ä¸šçŸ¥è¯†è€Œä¸æ–­è¿›åŒ–ã€‚å½“å‰ï¼Œæ‹“å±•å…·æœ‰ä¸“ä¸šçŸ¥è¯†LLMçš„å¤šæ ·åŒ–åº”ç”¨æ˜¯å…³é”®ï¼Œä½†æ”¶é›†é«˜è´¨é‡æ•°æ®å¹¶æ‰§è¡Œæ„å¤–ä¸”å…·æœ‰ä¿¡æ¯å«é‡çš„ä»»åŠ¡ä»é¢ä¸´æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºåˆ©ç”¨å¹¿å‘Šè§†é¢‘ä½œä¸ºæµ‹è¯•å¹³å°ï¼Œæ¢ç´¢LLMsåœ¨è§†è§‰é¢†åŸŸä¹‹å¤–çš„æ„ŸçŸ¥èƒ½åŠ›ã€‚é€šè¿‡åˆ©ç”¨å¹¿å‘Šè§†é¢‘ä¸­çš„çº¿ç´¢ä¸°å¯Œå’Œä¿¡æ¯å¯†é›†çš„ç‰¹ç‚¹ï¼Œå¦‚è¥é”€é€»è¾‘ã€è¯´æœç­–ç•¥å’Œè§‚ä¼—å‚ä¸åº¦ï¼Œæˆ‘ä»¬è¿›è¡Œäº†å¼€åˆ›æ€§çš„å·¥ä½œã€‚æœ¬æ–‡çš„ä¸»è¦è´¡çŒ®åŒ…æ‹¬ï¼šä¸€æ˜¯åˆ›å»ºé¦–ä¸ªä½¿ç”¨å¹¿å‘Šè§†é¢‘å’Œä»»åŠ¡è¯„ä¼°LLMsçš„AdsQAåŸºå‡†æµ‹è¯•ï¼›äºŒæ˜¯æå‡ºé€šè¿‡å¥–åŠ±é©±åŠ¨ä¼˜åŒ–çš„ReAd-Ræ¨¡å‹ï¼›ä¸‰æ˜¯åœ¨AdsQAä¸Šè¯„ä¼°äº†é¡¶çº§LLMsçš„è¡¨ç°ï¼ŒReAd-Ræ¨¡å‹å‡­å€Ÿå‡ºè‰²çš„é•¿é“¾æ¨ç†èƒ½åŠ›è¾¾åˆ°æœ€ä¼˜æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è¿ˆå‘é€šç”¨äººå·¥æ™ºèƒ½ï¼ˆAGIï¼‰æ–¹é¢å–å¾—é‡å¤§è¿›å±•ã€‚</li>
<li>LLMså€ŸåŠ©æ•°å­¦å’Œç¼–ç¨‹ç­‰ç‰¹å®šé¢†åŸŸçš„æŒ‘æˆ˜æŒç»­è¿›åŒ–ã€‚</li>
<li>ç›®å‰é¢ä¸´çš„æŒ‘æˆ˜åœ¨äºä¸ºçŸ¥è¯†å‹LLMsæ‹“å±•å¤šæ ·åŒ–çš„åº”ç”¨ï¼Œå¹¶æ”¶é›†é«˜è´¨é‡çš„æ•°æ®ä»¥æ‰§è¡Œå…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚</li>
<li>å¹¿å‘Šè§†é¢‘ä½œä¸ºæµ‹è¯•å¹³å°ï¼Œç”¨äºæ¢ç´¢LLMsåœ¨è§†è§‰é¢†åŸŸä¹‹å¤–çš„æ„ŸçŸ¥èƒ½åŠ›ã€‚</li>
<li>å¹¿å‘Šè§†é¢‘å…·æœ‰è¥é”€é€»è¾‘ã€è¯´æœç­–ç•¥å’Œè§‚ä¼—å‚ä¸åº¦ç­‰ç‰¹ç‚¹ï¼Œå¯å¸®åŠ©è¯„ä¼°LLMsã€‚</li>
<li>æå‡ºé¦–ä¸ªä½¿ç”¨å¹¿å‘Šè§†é¢‘ä¸ä»»åŠ¡è¯„ä¼°LLMsçš„AdsQAåŸºå‡†æµ‹è¯•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.08621">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.08621v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.08621v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.08621v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.08621v1/page_4_1.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Interpretable-Physics-Reasoning-and-Performance-Taxonomy-in-Vision-Language-Models"><a href="#Interpretable-Physics-Reasoning-and-Performance-Taxonomy-in-Vision-Language-Models" class="headerlink" title="Interpretable Physics Reasoning and Performance Taxonomy in   Vision-Language Models"></a>Interpretable Physics Reasoning and Performance Taxonomy in   Vision-Language Models</h2><p><strong>Authors:Pranav Pawar, Kavish Shah, Akshat Bhalani, Komal Kasat, Dev Mittal, Hadi Gala, Deepali Patil, Nikita Raichada, Monali Deshmukh</strong></p>
<p>As Vision-Language Models (VLMs) grow in sophistication, their ability to perform reasoning is coming under increasing supervision. While they excel at many tasks, their grasp of fundamental scientific principles, such as physics, remains an underexplored frontier. To reflect the advancements in these capabilities, we introduce a novel and accessible framework designed to rigorously evaluate VLMs on their understanding of 2D physics. Our framework features a pragmatic scenario generator that creates a diverse testbed of over 400 problems across four core domains: Projectile Motion, Collision Dynamics, Mechanics, and Fluid Dynamics. Through comprehensive evaluation of four state-of-the-art VLMs, we demonstrate a strong correlation between model scale and reasoning ability, with our top-performing model, Qwen2.5-VL-7B, achieving an overall score of 0.815. We find that while models excel at formulaic problems, they struggle significantly with domains requiring abstract spatial reasoning. By designing this framework, we aim to democratize the study of scientific reasoning in VLMs and foster deeper insights into their capabilities and limitations. </p>
<blockquote>
<p>éšç€è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„æ—¥ç›Šæˆç†Ÿï¼Œå®ƒä»¬è¿›è¡Œæ¨ç†çš„èƒ½åŠ›æ­£åœ¨å—åˆ°è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ã€‚è™½ç„¶å®ƒä»¬åœ¨è®¸å¤šä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬å¯¹åŸºæœ¬ç§‘å­¦åŸç†ï¼Œå¦‚ç‰©ç†å­¦çš„æŒæ¡ï¼Œä»ç„¶æ˜¯ä¸€ä¸ªæœªè¢«å……åˆ†æ¢ç´¢çš„é¢†åŸŸã€‚ä¸ºäº†åæ˜ è¿™äº›èƒ½åŠ›çš„è¿›æ­¥ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°é¢–ä¸”æ˜“äºè®¿é—®çš„æ¡†æ¶ï¼Œæ—¨åœ¨ä¸¥æ ¼è¯„ä¼°VLMå¯¹äºŒç»´ç‰©ç†å­¦çš„ç†è§£ã€‚æˆ‘ä»¬çš„æ¡†æ¶ç‰¹ç‚¹æ˜¯ä¸€ä¸ªå®ç”¨åœºæ™¯ç”Ÿæˆå™¨ï¼Œå®ƒåˆ›å»ºäº†å››ä¸ªæ ¸å¿ƒé¢†åŸŸçš„400å¤šä¸ªé—®é¢˜çš„æµ‹è¯•åºŠï¼šæŠ›ä½“è¿åŠ¨ã€ç¢°æ’åŠ¨åŠ›å­¦ã€æœºæ¢°å’Œæµä½“åŠ¨åŠ›å­¦ã€‚é€šè¿‡å¯¹å››ä¸ªæœ€å…ˆè¿›çš„VLMçš„ç»¼åˆè¯„ä¼°ï¼Œæˆ‘ä»¬è¯æ˜äº†æ¨¡å‹è§„æ¨¡ä¸æ¨ç†èƒ½åŠ›ä¹‹é—´çš„å¼ºçƒˆç›¸å…³æ€§ï¼Œæˆ‘ä»¬è¡¨ç°æœ€ä½³çš„æ¨¡å‹Qwen2.5-VL-7Bçš„æ€»ä½“å¾—åˆ†ä¸º0.815ã€‚æˆ‘ä»¬å‘ç°ï¼Œè™½ç„¶æ¨¡å‹åœ¨å…¬å¼é—®é¢˜ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨éœ€è¦æŠ½è±¡ç©ºé—´æ¨ç†çš„é¢†åŸŸï¼Œå®ƒä»¬ä¼šé‡åˆ°å¾ˆå¤§çš„å›°éš¾ã€‚é€šè¿‡è®¾è®¡è¿™ä¸ªæ¡†æ¶ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯ä½¿VLMä¸­çš„ç§‘å­¦ç ”ç©¶æ°‘ä¸»åŒ–ï¼Œå¹¶åŠ æ·±å¯¹å®ƒä»¬çš„èƒ½åŠ›å’Œå±€é™æ€§çš„æ·±å…¥äº†è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.08270v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>éšç€è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„æ—¥ç›Šæˆç†Ÿï¼Œå®ƒä»¬è¿›è¡Œæ¨ç†çš„èƒ½åŠ›æ­£åœ¨å—åˆ°è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ã€‚è™½ç„¶å®ƒä»¬åœ¨è®¸å¤šä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†å¯¹äºç‰©ç†ç­‰åŸºæœ¬åŸç†çš„ç†è§£ä»æ˜¯å°šæœªå……åˆ†æ¢ç´¢çš„é¢†åŸŸã€‚ä¸ºäº†åæ˜ è¿™äº›èƒ½åŠ›çš„è¿›æ­¥ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°é¢–ä¸”å®ç”¨çš„æ¡†æ¶ï¼Œæ—¨åœ¨ä¸¥æ ¼è¯„ä¼°VLMså¯¹äºŒç»´ç‰©ç†çš„ç†è§£ã€‚è¯¥æ¡†æ¶é…å¤‡äº†ä¸€ä¸ªå®ç”¨åœºæ™¯ç”Ÿæˆå™¨ï¼Œå¯åˆ›å»ºæ¶µç›–å››ä¸ªæ ¸å¿ƒé¢†åŸŸçš„è¶…è¿‡400ä¸ªé—®é¢˜çš„æµ‹è¯•åºŠï¼šæŠ›ä½“è¿åŠ¨ã€ç¢°æ’åŠ¨åŠ›å­¦ã€æœºæ¢°å’Œæµä½“åŠ¨åŠ›å­¦ã€‚é€šè¿‡å¯¹å››æ¬¾æœ€å…ˆè¿›çš„VLMsçš„ç»¼åˆè¯„ä¼°ï¼Œæˆ‘ä»¬å‘ç°åœ¨æ¨¡å‹è§„æ¨¡å’Œæ¨ç†èƒ½åŠ›ä¹‹é—´å­˜åœ¨å¾ˆå¼ºçš„ç›¸å…³æ€§ï¼Œå…¶ä¸­è¡¨ç°æœ€ä½³çš„æ¨¡å‹Qwen2.5-VL-7Bæ€»ä½“å¾—åˆ†è¾¾åˆ°0.815ã€‚æˆ‘ä»¬å‘ç°ï¼Œè™½ç„¶æ¨¡å‹åœ¨å…¬å¼é—®é¢˜ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨éœ€è¦æŠ½è±¡ç©ºé—´æ¨ç†çš„é¢†åŸŸå´è¡¨ç°ä¸ä½³ã€‚é€šè¿‡è®¾è®¡æ­¤æ¡†æ¶ï¼Œæˆ‘ä»¬æ—¨åœ¨æ¨åŠ¨è§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„ç§‘å­¦æ¨ç†ç ”ç©¶ï¼Œå¹¶åŠ æ·±å¯¹å…¶èƒ½åŠ›å’Œå±€é™æ€§çš„äº†è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VLMsåœ¨ç†è§£å’Œå¤„ç†ç‰©ç†åŸºæœ¬åŸç†æ–¹é¢æ˜¯ä¸€ä¸ªå°šæœªå……åˆ†æ¢ç´¢çš„é¢†åŸŸã€‚</li>
<li>ä¸ºäº†è¯„ä¼°VLMså¯¹äºŒç»´ç‰©ç†çš„ç†è§£ï¼Œå¼•å…¥äº†ä¸€ä¸ªæ–°é¢–çš„å®ç”¨æ¡†æ¶ã€‚</li>
<li>è¯¥æ¡†æ¶åŒ…å«è¶…è¿‡400ä¸ªé—®é¢˜çš„æµ‹è¯•åºŠï¼Œæ¶µç›–å››ä¸ªæ ¸å¿ƒé¢†åŸŸï¼šæŠ›ä½“è¿åŠ¨ã€ç¢°æ’åŠ¨åŠ›å­¦ã€æœºæ¢°å’Œæµä½“åŠ¨åŠ›å­¦ã€‚</li>
<li>VLMsæ¨¡å‹è§„æ¨¡ä¸æ¨ç†èƒ½åŠ›ä¹‹é—´å­˜åœ¨å¼ºçƒˆçš„ç›¸å…³æ€§ã€‚</li>
<li>Qwen2.5-VL-7Bæ¨¡å‹åœ¨è¯„ä¼°ä¸­è¡¨ç°æœ€ä½³ï¼Œæ€»ä½“å¾—åˆ†0.815ã€‚</li>
<li>VLMsåœ¨å…¬å¼é—®é¢˜ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨éœ€è¦æŠ½è±¡ç©ºé—´æ¨ç†çš„é¢†åŸŸè¡¨ç°è¾ƒå·®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.08270">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.08270v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.08270v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.08270v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.08270v1/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.08270v1/page_2_2.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.08270v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.08270v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.08270v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Attribute-based-Object-Grounding-and-Robot-Grasp-Detection-with-Spatial-Reasoning"><a href="#Attribute-based-Object-Grounding-and-Robot-Grasp-Detection-with-Spatial-Reasoning" class="headerlink" title="Attribute-based Object Grounding and Robot Grasp Detection with Spatial   Reasoning"></a>Attribute-based Object Grounding and Robot Grasp Detection with Spatial   Reasoning</h2><p><strong>Authors:Houjian Yu, Zheming Zhou, Min Sun, Omid Ghasemalizadeh, Yuyin Sun, Cheng-Hao Kuo, Arnie Sen, Changhyun Choi</strong></p>
<p>Enabling robots to grasp objects specified through natural language is essential for effective human-robot interaction, yet it remains a significant challenge. Existing approaches often struggle with open-form language expressions and typically assume unambiguous target objects without duplicates. Moreover, they frequently rely on costly, dense pixel-wise annotations for both object grounding and grasp configuration. We present Attribute-based Object Grounding and Robotic Grasping (OGRG), a novel framework that interprets open-form language expressions and performs spatial reasoning to ground target objects and predict planar grasp poses, even in scenes containing duplicated object instances. We investigate OGRG in two settings: (1) Referring Grasp Synthesis (RGS) under pixel-wise full supervision, and (2) Referring Grasp Affordance (RGA) using weakly supervised learning with only single-pixel grasp annotations. Key contributions include a bi-directional vision-language fusion module and the integration of depth information to enhance geometric reasoning, improving both grounding and grasping performance. Experiment results show that OGRG outperforms strong baselines in tabletop scenes with diverse spatial language instructions. In RGS, it operates at 17.59 FPS on a single NVIDIA RTX 2080 Ti GPU, enabling potential use in closed-loop or multi-object sequential grasping, while delivering superior grounding and grasp prediction accuracy compared to all the baselines considered. Under the weakly supervised RGA setting, OGRG also surpasses baseline grasp-success rates in both simulation and real-robot trials, underscoring the effectiveness of its spatial reasoning design. Project page: <a target="_blank" rel="noopener" href="https://z.umn.edu/ogrg">https://z.umn.edu/ogrg</a> </p>
<blockquote>
<p>å®ç°é€šè¿‡è‡ªç„¶è¯­è¨€æŒ‡å®šå¯¹è±¡ä½¿æœºå™¨äººè¿›è¡ŒæŠ“å–ï¼Œå¯¹äºæœ‰æ•ˆçš„äººæœºäº¤äº’è‡³å…³é‡è¦ï¼Œä½†ä»æ˜¯ä¸€é¡¹é‡å¤§æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•å¸¸å¸¸éš¾ä»¥å¤„ç†å¼€æ”¾å½¢å¼çš„è¯­è¨€è¡¨è¾¾ï¼Œå¹¶é€šå¸¸å‡è®¾æ²¡æœ‰é‡å¤çš„ç›®æ ‡å¯¹è±¡ä¸”éå¸¸æ˜ç¡®ã€‚æ­¤å¤–ï¼Œå®ƒä»¬é€šå¸¸ä¾èµ–æ˜‚è´µçš„å¯†é›†åƒç´ çº§æ³¨é‡Šæ¥è¿›è¡Œå¯¹è±¡å®šä½å’ŒæŠ“å–é…ç½®ã€‚æˆ‘ä»¬æå‡ºäº†åŸºäºå±æ€§çš„å¯¹è±¡å®šä½å’Œæœºå™¨äººæŠ“å–ï¼ˆOGRGï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æ¡†æ¶ï¼Œèƒ½å¤Ÿè§£é‡Šå¼€æ”¾å½¢å¼çš„è¯­è¨€è¡¨è¾¾å¹¶è¿›è¡Œç©ºé—´æ¨ç†ï¼Œä»¥å®šä½ç›®æ ‡å¯¹è±¡å¹¶é¢„æµ‹å¹³é¢æŠ“å–å§¿æ€ï¼Œå³ä½¿åœ¨åŒ…å«é‡å¤å¯¹è±¡å®ä¾‹çš„åœºæ™¯ä¸­ä¹Ÿæ˜¯å¦‚æ­¤ã€‚æˆ‘ä»¬åœ¨ä¸¤ç§è®¾ç½®ä¸‹ç ”ç©¶äº†OGRGï¼šï¼ˆ1ï¼‰åƒç´ çº§å…¨ç›‘ç£ä¸‹çš„å‚ç…§æŠ“å–åˆæˆï¼ˆRGSï¼‰ï¼›ï¼ˆ2ï¼‰ä½¿ç”¨ä»…å•åƒç´ æŠ“å–æ³¨é‡Šçš„å¼±ç›‘ç£å­¦ä¹ è¿›è¡Œçš„å‚ç…§æŠ“å–åŠŸèƒ½ï¼ˆRGAï¼‰ã€‚ä¸»è¦è´¡çŒ®åŒ…æ‹¬åŒå‘è§†è§‰è¯­è¨€èåˆæ¨¡å—å’Œé›†æˆæ·±åº¦ä¿¡æ¯ä»¥å¢å¼ºå‡ ä½•æ¨ç†ï¼Œä»è€Œæé«˜å®šä½å’ŒæŠ“å–æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOGRGåœ¨å…·æœ‰å„ç§ç©ºé—´è¯­è¨€æŒ‡ä»¤çš„æ¡Œé¢åœºæ™¯ä¸­ä¼˜äºå¼ºå¤§çš„åŸºå‡†çº¿ã€‚åœ¨RGSä¸­ï¼Œå®ƒåœ¨å•ä¸ªNVIDIA RTX 2080 Ti GPUä¸Šçš„è¿è¡Œé€Ÿåº¦ä¸º17.59 FPSï¼Œèƒ½å¤Ÿåœ¨é—­ç¯æˆ–å¤šå¯¹è±¡é¡ºåºæŠ“å–ä¸­å…·æœ‰æ½œåœ¨ç”¨é€”ï¼ŒåŒæ—¶ä¸æ‰€æœ‰è€ƒè™‘çš„åŸºå‡†çº¿ç›¸æ¯”ï¼Œæä¾›ä¼˜è¶Šçš„å®šåœ°å’ŒæŠ“å–é¢„æµ‹ç²¾åº¦ã€‚åœ¨å¼±ç›‘ç£çš„RGAè®¾ç½®ä¸‹ï¼ŒOGRGåœ¨æ¨¡æ‹Ÿå’ŒçœŸå®æœºå™¨äººè¯•éªŒä¸­çš„æŠ“å–æˆåŠŸç‡ä¹Ÿè¶…è¿‡äº†åŸºçº¿ï¼Œè¿™çªå‡ºäº†å…¶ç©ºé—´æ¨ç†è®¾è®¡çš„æœ‰æ•ˆæ€§ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://z.umn.edu/ogrg">https://z.umn.edu/ogrg</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.08126v1">PDF</a> Accepted to 2025 IEEE-RAS 24th International Conference on Humanoid   Robots</p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶æå‡ºä¸€ç§åŸºäºå±æ€§çš„ç›®æ ‡ç‰©ä½“å®šä½ä¸æœºå™¨äººæŠ“å–ï¼ˆOGRGï¼‰çš„æ–°æ¡†æ¶ï¼Œèƒ½å¤Ÿè§£æè‡ªç„¶è¯­è¨€æè¿°ï¼Œè¿›è¡Œç©ºé—´æ¨ç†ï¼Œå®šä½ç›®æ ‡ç‰©ä½“å¹¶é¢„æµ‹å¹³é¢æŠ“å–å§¿æ€ï¼Œå³ä½¿åœºæ™¯ä¸­å­˜åœ¨é‡å¤ç‰©ä½“å®ä¾‹ã€‚ç ”ç©¶åŒ…æ‹¬ä¸¤ç§æƒ…å¢ƒï¼šåœ¨åƒç´ çº§å…¨ç›‘ç£ä¸‹çš„æŒ‡å‘æŠ“å–åˆæˆï¼ˆRGSï¼‰ï¼Œä»¥åŠä½¿ç”¨ä»…å•ç‚¹åƒç´ æŠ“å–æ³¨é‡Šçš„å¼±ç›‘ç£å­¦ä¹ çš„æŒ‡å‘æŠ“å–é€‚ç”¨æ€§ï¼ˆRGAï¼‰ã€‚å…³é”®è´¡çŒ®åŒ…æ‹¬åŒå‘è§†è§‰-è¯­è¨€èåˆæ¨¡å—å’Œæ·±åº¦ä¿¡æ¯é›†æˆï¼Œä»¥å¢å¼ºå‡ ä½•æ¨ç†ï¼Œæé«˜å®šä½å’ŒæŠ“å–æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOGRGåœ¨å…·æœ‰å„ç§ç©ºé—´è¯­è¨€æŒ‡ä»¤çš„æ¡Œé¢åœºæ™¯ä¸­ä¼˜äºå¼ºåŸºçº¿ã€‚åœ¨RGSä¸­ï¼Œå®ƒåœ¨å•ä¸€NVIDIA RTX 2080 Ti GPUä¸Šçš„è¿è¡Œé€Ÿåº¦ä¸ºæ¯ç§’17.59å¸§ï¼Œå…·æœ‰æ½œåœ¨ç”¨äºé—­ç¯æˆ–å¤šç›®æ ‡è¿ç»­æŠ“å–çš„èƒ½åŠ›ï¼ŒåŒæ—¶ä¸æ‰€æœ‰è€ƒè™‘çš„åŸºçº¿ç›¸æ¯”ï¼Œå…·æœ‰ä¼˜è¶Šçš„å®šä½å’ŒæŠ“å–é¢„æµ‹å‡†ç¡®æ€§ã€‚åœ¨å¼±ç›‘ç£çš„RGAç¯å¢ƒä¸‹ï¼ŒOGRGåœ¨æ¨¡æ‹Ÿå’ŒçœŸå®æœºå™¨äººè¯•éªŒä¸­çš„æŠ“å–æˆåŠŸç‡ä¹Ÿè¶…è¿‡äº†åŸºçº¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœºå™¨äººé€šè¿‡è‡ªç„¶è¯­è¨€æŒ‡ä»¤è¿›è¡Œç‰©ä½“æŠ“å–å¯¹äºæœ‰æ•ˆçš„äººæœºäº¤äº’è‡³å…³é‡è¦ï¼Œä½†ä»å…·æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¸¸é¢ä¸´å¼€æ”¾å¼è¯­è¨€è¡¨è¾¾çš„é—®é¢˜ï¼Œå¹¶å‡è®¾æ— æ­§ä¹‰çš„ç›®æ ‡ç‰©ä½“ï¼Œæ— é‡å¤å®ä¾‹ã€‚</li>
<li>æ‰€æå‡ºçš„OGRGæ¡†æ¶èƒ½è§£æå¼€æ”¾å¼è¯­è¨€è¡¨è¾¾ï¼Œè¿›è¡Œç©ºé—´æ¨ç†ï¼Œå®šä½ç›®æ ‡ç‰©ä½“å¹¶é¢„æµ‹æŠ“å–å§¿æ€ã€‚</li>
<li>OGRGé€‚ç”¨äºåœºæ™¯ä¸­å­˜åœ¨é‡å¤ç‰©ä½“å®ä¾‹çš„æƒ…å†µã€‚</li>
<li>OGRGåŒ…æ‹¬ä¸¤ç§æƒ…å¢ƒï¼šåƒç´ çº§å…¨ç›‘ç£ä¸‹çš„RGSå’Œå¼±ç›‘ç£å­¦ä¹ çš„RGAã€‚</li>
<li>å…³é”®è´¡çŒ®åŒ…æ‹¬åŒå‘è§†è§‰-è¯­è¨€èåˆæ¨¡å—å’Œæ·±åº¦ä¿¡æ¯é›†æˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.08126">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.08126v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.08126v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.08126v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.08126v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.08126v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="RLFactory-A-Plug-and-Play-Reinforcement-Learning-Post-Training-Framework-for-LLM-Multi-Turn-Tool-Use"><a href="#RLFactory-A-Plug-and-Play-Reinforcement-Learning-Post-Training-Framework-for-LLM-Multi-Turn-Tool-Use" class="headerlink" title="RLFactory: A Plug-and-Play Reinforcement Learning Post-Training   Framework for LLM Multi-Turn Tool-Use"></a>RLFactory: A Plug-and-Play Reinforcement Learning Post-Training   Framework for LLM Multi-Turn Tool-Use</h2><p><strong>Authors:Jiajun Chai, Guojun Yin, Zekun Xu, Chuhuai Yue, Yi Jia, Siyu Xia, Xiaohan Wang, Jiwen Jiang, Xiaoguang Li, Chengqi Dong, Hang He, Wei Lin</strong></p>
<p>Large language models excel at basic reasoning but struggle with tasks that require interaction with external tools. We present RLFactory, a plug-and-play reinforcement learning post-training framework for multi-round tool use. RLFactory tackles (i) tool-call stability and adaptability amid tool heterogeneity and interface issues via an asyncio-based asynchronous caller and a decoupled tool&#x2F;training architecture, and (ii) diverse evaluation needs via a reward layer supporting rule-based, model-judgment, and tool-verification signals. It reconstructs the MDP by introducing observation markers from tool feedback, closing the loop among model, tools, and environment, and implements a generate-parse-invoke-update workflow for dynamic policy optimization. On Search-R1 with Qwen3-4B, RLFactory achieves a 0.486 test score on the Natural Questions (NQ) dataset, surpassing larger models trained with similar techniques (e.g., Qwen2.5-7B-Instruct-GRPO at 0.473), and increases training throughput by 6.8x. RLFactory provides a low-barrier, highly adaptable framework for strengthening multi-round tool use of LLMs in real-world scenarios. Code: <a target="_blank" rel="noopener" href="https://github.com/Simple-Efficient/RL-Factory">https://github.com/Simple-Efficient/RL-Factory</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åŸºç¡€æ¨ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨éœ€è¦ä¸ä½¿ç”¨å¤–éƒ¨å·¥å…·è¿›è¡Œäº¤äº’çš„ä»»åŠ¡æ–¹é¢å´é‡åˆ°å›°éš¾ã€‚æˆ‘ä»¬æå‡ºäº†RLFactoryï¼Œè¿™æ˜¯ä¸€ä¸ªå³æ’å³ç”¨çš„å¼ºåŒ–å­¦ä¹ åè®­ç»ƒæ¡†æ¶ï¼Œç”¨äºå¤šè½®å·¥å…·ä½¿ç”¨ã€‚RLFactoryè§£å†³äº†ï¼ˆiï¼‰å·¥å…·è°ƒç”¨ç¨³å®šæ€§å’Œé€‚åº”æ€§é—®é¢˜ï¼ŒåŒ…æ‹¬å·¥å…·å¼‚è´¨æ€§å’Œç•Œé¢é—®é¢˜ï¼Œé€šè¿‡åŸºäºasyncioçš„å¼‚æ­¥è°ƒç”¨è€…å’Œè§£è€¦çš„å·¥å…·&#x2F;è®­ç»ƒæ¶æ„ï¼Œä»¥åŠï¼ˆiiï¼‰é€šè¿‡æ”¯æŒåŸºäºè§„åˆ™ã€æ¨¡å‹åˆ¤æ–­å’Œå·¥å…·éªŒè¯ä¿¡å·çš„å¥–åŠ±å±‚æ¥æ»¡è¶³ä¸åŒçš„è¯„ä¼°éœ€æ±‚ã€‚å®ƒé€šè¿‡å¼•å…¥å·¥å…·åé¦ˆçš„è§‚å¯Ÿæ ‡è®°æ¥é‡å»ºMDPï¼Œé—­åˆæ¨¡å‹ã€å·¥å…·å’Œç¯å¢ƒä¹‹é—´çš„å¾ªç¯ï¼Œå®ç°äº†åŠ¨æ€ç­–ç•¥ä¼˜åŒ–çš„ç”Ÿæˆ-è§£æ-è°ƒç”¨-æ›´æ–°å·¥ä½œæµç¨‹ã€‚åœ¨å¸¦æœ‰Qwen3-4Bçš„Search-R1ä¸Šï¼ŒRLFactoryåœ¨è‡ªç„¶é—®é¢˜ï¼ˆNQï¼‰æ•°æ®é›†ä¸Šå®ç°äº†0.486çš„æµ‹è¯•æˆç»©ï¼Œè¶…è¶Šäº†ä½¿ç”¨ç±»ä¼¼æŠ€æœ¯è®­ç»ƒçš„å¤§å‹æ¨¡å‹ï¼ˆä¾‹å¦‚Qwen2.5-7B-Instruct-GRPOçš„0.473ï¼‰ï¼Œå¹¶æé«˜äº†6.8å€çš„è®­ç»ƒååé‡ã€‚RLFactoryä¸ºå¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç°å®åœºæ™¯ä¸­çš„å¤šè½®å·¥å…·ä½¿ç”¨æä¾›äº†ä¸€ä¸ªä½é—¨æ§›ã€é«˜åº¦é€‚åº”çš„æ¡†æ¶ã€‚ä»£ç ï¼š<a target="_blank" rel="noopener" href="https://github.com/Simple-Efficient/RL-Factory%E3%80%82">https://github.com/Simple-Efficient/RL-Factoryã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.06980v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹æ“…é•¿åŸºç¡€æ¨ç†ï¼Œä½†åœ¨éœ€è¦ä¸å¤–ç•Œå·¥å…·äº¤äº’çš„ä»»åŠ¡ä¸Šè¡¨ç°ä¸ä½³ã€‚æœ¬ç ”ç©¶æå‡ºäº†RLFactoryï¼Œä¸€ä¸ªç”¨äºå¤šè½®å·¥å…·ä½¿ç”¨çš„å³æ’å³ç”¨å¼ºåŒ–å­¦ä¹ åè®­ç»ƒæ¡†æ¶ã€‚RLFactoryè§£å†³äº†å·¥å…·è°ƒç”¨çš„ç¨³å®šæ€§å’Œé€‚åº”æ€§é—®é¢˜ï¼Œé€šè¿‡å¼‚æ­¥è°ƒç”¨å™¨å’Œè§£è€¦çš„å·¥å…·&#x2F;è®­ç»ƒæ¶æ„ï¼Œä»¥åŠæ”¯æŒè§„åˆ™åŸºç¡€ã€æ¨¡å‹åˆ¤æ–­å’Œå·¥å…·éªŒè¯ä¿¡å·çš„å¥–åŠ±å±‚æ¥æ»¡è¶³å¤šæ ·çš„è¯„ä¼°éœ€æ±‚ã€‚å®ƒä»å·¥å…·åé¦ˆä¸­å¼•å…¥è§‚å¯Ÿæ ‡è®°ï¼Œé‡å»ºäº†MDPï¼Œå®ç°äº†æ¨¡å‹ã€å·¥å…·å’Œç¯å¢ƒä¹‹é—´çš„é—­ç¯ï¼Œå¹¶é‡‡ç”¨äº†ç”Ÿæˆ-è§£æ-è°ƒç”¨-æ›´æ–°çš„å·¥ä½œæµç¨‹ï¼Œä»¥å®ç°åŠ¨æ€ç­–ç•¥ä¼˜åŒ–ã€‚åœ¨Search-R1ä½¿ç”¨Qwen3-4Bä¸Šï¼ŒRLFactoryåœ¨NQæµ‹è¯•é›†ä¸Šè¾¾åˆ°äº†0.486çš„æµ‹è¯•åˆ†æ•°ï¼Œè¶…è¿‡äº†ä½¿ç”¨ç±»ä¼¼æŠ€æœ¯è®­ç»ƒçš„æ›´å¤§æ¨¡å‹ï¼ˆå¦‚Qwen2.5-7B-Instruct-GRPOçš„0.473ï¼‰ï¼Œå¹¶å°†è®­ç»ƒé€Ÿåº¦æé«˜äº†6.8å€ã€‚RLFactoryä¸ºåŠ å¼ºçœŸå®åœºæ™¯ä¸­å¤šè½®å·¥å…·ä½¿ç”¨çš„å¤§å‹è¯­è¨€æ¨¡å‹æä¾›äº†ä¸€ä¸ªä½é—¨æ§›ã€é«˜åº¦çµæ´»çš„æ¡†æ¶ã€‚æ›´å¤šè¯¦æƒ…è®¿é—®å…¶ä»£ç ä»“åº“ï¼š<a target="_blank" rel="noopener" href="https://github.com/Simple-Efficient/RL-Factory">é“¾æ¥åœ°å€</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åŸºç¡€æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜ç§€ï¼Œä½†åœ¨éœ€è¦å·¥å…·äº¤äº’çš„ä»»åŠ¡ä¸Šè¡¨ç°æ¬ ä½³ã€‚</li>
<li>RLFactoryæ¡†æ¶è§£å†³äº†å·¥å…·è°ƒç”¨çš„ç¨³å®šæ€§å’Œé€‚åº”æ€§é—®é¢˜ã€‚</li>
<li>RLFactoryé€šè¿‡å¼‚æ­¥è°ƒç”¨å™¨å’Œè§£è€¦çš„å·¥å…·&#x2F;è®­ç»ƒæ¶æ„æ¥å¤„ç†å·¥å…·å¼‚è´¨æ€§å’Œç•Œé¢é—®é¢˜ã€‚</li>
<li>RLFactoryçš„å¥–åŠ±å±‚æ”¯æŒè§„åˆ™åŸºç¡€ã€æ¨¡å‹åˆ¤æ–­å’Œå·¥å…·éªŒè¯ä¿¡å·ï¼Œæ»¡è¶³å¤šæ ·åŒ–çš„è¯„ä¼°éœ€æ±‚ã€‚</li>
<li>RLFactoryå¼•å…¥äº†è§‚å¯Ÿæ ‡è®°æ¥é‡å»ºMDPï¼Œå®ç°äº†æ¨¡å‹ã€å·¥å…·å’Œç¯å¢ƒçš„é—­ç¯ã€‚</li>
<li>è¯¥æ¡†æ¶å®ç°äº†ç”Ÿæˆ-è§£æ-è°ƒç”¨-æ›´æ–°çš„å·¥ä½œæµç¨‹ï¼Œä»¥å®ç°åŠ¨æ€ç­–ç•¥ä¼˜åŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.06980">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.06980v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.06980v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.06980v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.06980v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.06980v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="MachineLearningLM-Scaling-Many-shot-In-context-Learning-via-Continued-Pretraining"><a href="#MachineLearningLM-Scaling-Many-shot-In-context-Learning-via-Continued-Pretraining" class="headerlink" title="MachineLearningLM: Scaling Many-shot In-context Learning via Continued   Pretraining"></a>MachineLearningLM: Scaling Many-shot In-context Learning via Continued   Pretraining</h2><p><strong>Authors:Haoyu Dong, Pengkun Zhang, Mingzhe Lu, Yanzhen Shen, Guolin Ke</strong></p>
<p>Large language models (LLMs) possess broad world knowledge and strong general-purpose reasoning ability, yet they struggle to learn from many in-context examples on standard machine learning (ML) tasks, that is, to leverage many-shot demonstrations purely via in-context learning (ICL) without gradient descent. We introduce MachineLearningLM, a portable continued-pretraining framework that equips a general-purpose LLM with robust in-context ML capability while preserving its general knowledge and reasoning for broader chat workflows.   Our pretraining procedure synthesizes ML tasks from millions of structural causal models (SCMs), spanning shot counts up to 1,024. We begin with a random-forest teacher, distilling tree-based decision strategies into the LLM to strengthen robustness in numerical modeling. All tasks are serialized with a token-efficient prompt, enabling 3x to 6x more examples per context window and delivering up to 50x amortized throughput via batch inference.   Despite a modest setup (Qwen-2.5-7B-Instruct with LoRA rank 8), MachineLearningLM outperforms strong LLM baselines (e.g., GPT-5-mini) by an average of about 15% on out-of-distribution tabular classification across finance, physics, biology, and healthcare domains. It exhibits a striking many-shot scaling law: accuracy increases monotonically as in-context demonstrations grow from 8 to 1,024. Without any task-specific training, it attains random-forest-level accuracy across hundreds of shots. General chat capabilities, including knowledge and reasoning, are preserved: it achieves 75.4% on MMLU. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ‹¥æœ‰å¹¿æ³›çš„ä¸–ç•ŒçŸ¥è¯†å’Œå¼ºå¤§çš„é€šç”¨æ¨ç†èƒ½åŠ›ï¼Œä½†åœ¨æ ‡å‡†æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰ä»»åŠ¡ä¸Šï¼Œå®ƒä»¬å¾ˆéš¾ä»å¤šä¸ªä¸Šä¸‹æ–‡ç¤ºä¾‹ä¸­å­¦ä¹ ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œå®ƒä»¬æ— æ³•ä»…é€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰åˆ©ç”¨å¤šä¸ªç¤ºä¾‹æ¼”ç¤ºï¼Œè€Œæ— éœ€è¿›è¡Œæ¢¯åº¦ä¸‹é™ã€‚æˆ‘ä»¬å¼•å…¥äº†MachineLearningLMï¼Œè¿™æ˜¯ä¸€ä¸ªä¾¿æºå¼æŒç»­é¢„è®­ç»ƒæ¡†æ¶ï¼Œå®ƒä½¿é€šç”¨LLMå…·å¤‡å¼ºå¤§çš„ä¸Šä¸‹æ–‡MLåŠŸèƒ½ï¼ŒåŒæ—¶ä¿ç•™å…¶ä¸€èˆ¬çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ï¼Œä»¥æ”¯æŒæ›´å¹¿æ³›çš„èŠå¤©å·¥ä½œæµç¨‹ã€‚æˆ‘ä»¬çš„é¢„è®­ç»ƒç¨‹åºä»æ•°ç™¾ä¸‡ä¸ªç»“æ„å› æœæ¨¡å‹ï¼ˆSCMï¼‰ä¸­ç»¼åˆMLä»»åŠ¡ï¼Œæ¶µç›–å¤šè¾¾1024ä¸ªshotã€‚æˆ‘ä»¬ä»¥éšæœºæ£®æ—æ•™å¸ˆå¼€å§‹ï¼Œå°†åŸºäºæ ‘çš„å†³ç­–ç­–ç•¥è’¸é¦åˆ°LLMä¸­ï¼Œä»¥åŠ å¼ºæ•°å€¼å»ºæ¨¡ä¸­çš„ç¨³å¥æ€§ã€‚æ‰€æœ‰ä»»åŠ¡éƒ½é€šè¿‡é«˜æ•ˆçš„ä»¤ç‰Œæç¤ºè¿›è¡Œåºåˆ—åŒ–ï¼Œèƒ½å¤Ÿåœ¨æ¯ä¸ªä¸Šä¸‹æ–‡çª—å£ä¸­å®ç°3åˆ°6å€çš„æ›´å¤šç¤ºä¾‹ï¼Œå¹¶é€šè¿‡æ‰¹é‡æ¨ç†å®ç°é«˜è¾¾50å€çš„æ‘Šé”€ååé‡ã€‚å°½ç®¡è®¾ç½®ç›¸å¯¹ç®€å•ï¼ˆä½¿ç”¨LoRAç­‰çº§8çš„Qwen-2.5-7B-Instructï¼‰ï¼Œä½†MachineLearningLMåœ¨é‡‘èã€ç‰©ç†ã€ç”Ÿç‰©å’ŒåŒ»ç–—ä¿å¥é¢†åŸŸçš„ç¦»ç¾¤è¡¨æ ¼åˆ†ç±»æ–¹é¢ï¼Œå¹³å‡æ¯”å¼ºå¤§çš„LLMåŸºçº¿ï¼ˆä¾‹å¦‚GPT-5-miniï¼‰é«˜å‡ºçº¦15%ã€‚å®ƒè¡¨ç°å‡ºæƒŠäººçš„å¤šshotç¼©æ”¾å®šå¾‹ï¼šéšç€ä¸Šä¸‹æ–‡æ¼”ç¤ºä»8ä¸ªå¢åŠ åˆ°1024ä¸ªï¼Œå‡†ç¡®æ€§å•è°ƒå¢åŠ ã€‚æ— éœ€ä»»ä½•ç‰¹å®šä»»åŠ¡çš„è®­ç»ƒï¼Œå®ƒå°±èƒ½è¾¾åˆ°æ•°ç™¾æ¬¡å°„å‡»çš„éšæœºæ£®æ—çº§åˆ«çš„å‡†ç¡®æ€§ã€‚åŒæ—¶ä¿ç•™äº†é€šç”¨çš„èŠå¤©èƒ½åŠ›ï¼ŒåŒ…æ‹¬çŸ¥è¯†å’Œæ¨ç†ï¼šå®ƒåœ¨MMLUä¸Šè¾¾åˆ°äº†75.4%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.06806v2">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å…·æœ‰å¹¿æ³›çš„ä¸–ç•ŒçŸ¥è¯†å’Œå¼ºå¤§çš„é€šç”¨æ¨ç†èƒ½åŠ›ï¼Œä½†åœ¨æ ‡å‡†æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰ä»»åŠ¡ä¸Šå­¦ä¹ å¤šä¸ªä¸Šä¸‹æ–‡ç¤ºä¾‹æ—¶é‡åˆ°å›°éš¾ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†MachineLearningLMï¼Œè¿™æ˜¯ä¸€ä¸ªä¾¿æºå¼æŒç»­é¢„è®­ç»ƒæ¡†æ¶ï¼Œä¸ºé€šç”¨LLMé…å¤‡äº†å¼ºå¤§çš„ä¸Šä¸‹æ–‡MLèƒ½åŠ›ï¼ŒåŒæ—¶ä¿ç•™äº†å…¶ä¸€èˆ¬çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ï¼Œç”¨äºæ›´å¹¿æ³›çš„èŠå¤©å·¥ä½œæµç¨‹ã€‚æˆ‘ä»¬çš„é¢„è®­ç»ƒç¨‹åºé€šè¿‡åˆæˆæ¥è‡ªæ•°ç™¾ä¸‡ç»“æ„å› æœæ¨¡å‹ï¼ˆSCMï¼‰çš„MLä»»åŠ¡æ¥å¢å¼ºLLMçš„èƒ½åŠ›ï¼Œæ¶µç›–æ ·æœ¬æ•°é«˜è¾¾1024ä¸ªã€‚æˆ‘ä»¬é€šè¿‡åœ¨LLMä¸­æ³¨å…¥åŸºäºæ ‘çš„å†³ç­–ç­–ç•¥æ¥åŠ å¼ºå…¶åœ¨æ•°å€¼å»ºæ¨¡ä¸­çš„ç¨³å¥æ€§ã€‚æ‰€æœ‰ä»»åŠ¡éƒ½é€šè¿‡é«˜æ•ˆçš„æç¤ºè¿›è¡Œåºåˆ—åŒ–ï¼Œèƒ½å¤Ÿåœ¨æ¯ä¸ªä¸Šä¸‹æ–‡çª—å£ä¸­æä¾›3åˆ°6å€çš„ç¤ºä¾‹ï¼Œå¹¶é€šè¿‡æ‰¹é‡æ¨ç†å®ç°é«˜è¾¾50å€çš„æ‘Šé”€ååé‡ã€‚å°½ç®¡ä½¿ç”¨äº†é€‚åº¦çš„è®¾ç½®ï¼ˆQwen-2.5-7B-Instruct with LoRA rank 8ï¼‰ï¼Œä½†MachineLearningLMåœ¨é‡‘èã€ç‰©ç†ã€ç”Ÿç‰©å’ŒåŒ»ç–—ä¿å¥é¢†åŸŸçš„ç¦»åˆ†å¸ƒè¡¨æ ¼åˆ†ç±»ä¸Šå¹³å‡ä¼˜äºå¼ºå¤§çš„LLMåŸºå‡†æµ‹è¯•ï¼ˆä¾‹å¦‚GPT-5-miniï¼‰çº¦15%ã€‚å®ƒå±•ç°å‡ºæƒŠäººçš„å¤šé•œå¤´è§„æ¨¡æ•ˆåº”ï¼šéšç€ä¸Šä¸‹æ–‡æ¼”ç¤ºä»8ä¸ªå¢é•¿åˆ°1024ä¸ªï¼Œå‡†ç¡®åº¦å‘ˆå•è°ƒå¢é•¿ã€‚æ— éœ€ä»»ä½•ç‰¹å®šçš„ä»»åŠ¡è®­ç»ƒï¼Œå°±èƒ½è¾¾åˆ°éšæœºæ£®æ—çº§åˆ«çš„å‡†ç¡®åº¦ã€‚åŒæ—¶ä¿ç•™äº†ä¸€èˆ¬èŠå¤©èƒ½åŠ›ï¼ŒåŒ…æ‹¬çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ï¼Œåœ¨MMLUä¸Šè¾¾åˆ°75.4%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ ‡å‡†æœºå™¨å­¦ä¹ ä»»åŠ¡ä¸Šå­¦ä¹ å¤šä¸ªä¸Šä¸‹æ–‡ç¤ºä¾‹æ—¶é‡åˆ°å›°éš¾ã€‚</li>
<li>MachineLearningLMæ˜¯ä¸€ä¸ªä¾¿æºå¼æŒç»­é¢„è®­ç»ƒæ¡†æ¶ï¼Œå¢å¼ºäº†LLMçš„ä¸Šä¸‹æ–‡æœºå™¨å­¦ä¹ èƒ½åŠ›ï¼ŒåŒæ—¶ä¿ç•™å…¶ä¸€èˆ¬çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ã€‚</li>
<li>é¢„è®­ç»ƒç¨‹åºé€šè¿‡åˆæˆæ¥è‡ªæ•°ç™¾ä¸‡ç»“æ„å› æœæ¨¡å‹ï¼ˆSCMï¼‰çš„MLä»»åŠ¡æ¥å¢å¼ºLLMçš„èƒ½åŠ›ã€‚</li>
<li>ä½¿ç”¨åŸºäºæ ‘çš„å†³ç­–ç­–ç•¥æ¥æé«˜LLMåœ¨æ•°å€¼å»ºæ¨¡ä¸­çš„ç¨³å¥æ€§ã€‚</li>
<li>MachineLearningLMè¡¨ç°å‡ºå¼ºå¤§çš„å¤šé•œå¤´è§„æ¨¡æ•ˆåº”ï¼Œéšç€ä¸Šä¸‹æ–‡æ¼”ç¤ºçš„å¢åŠ ï¼Œå‡†ç¡®åº¦ä¸æ–­æé«˜ã€‚</li>
<li>åœ¨å¤šä¸ªé¢†åŸŸï¼ˆé‡‘èã€ç‰©ç†ã€ç”Ÿç‰©å’ŒåŒ»ç–—ä¿å¥ï¼‰çš„ç¦»åˆ†å¸ƒè¡¨æ ¼åˆ†ç±»ä»»åŠ¡ä¸Šï¼ŒMachineLearningLMä¼˜äºå…¶ä»–LLMåŸºå‡†æµ‹è¯•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.06806">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.06806v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.06806v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.06806v2/page_4_0.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="MM-DREX-Multimodal-Driven-Dynamic-Routing-of-LLM-Experts-for-Financial-Trading"><a href="#MM-DREX-Multimodal-Driven-Dynamic-Routing-of-LLM-Experts-for-Financial-Trading" class="headerlink" title="MM-DREX: Multimodal-Driven Dynamic Routing of LLM Experts for Financial   Trading"></a>MM-DREX: Multimodal-Driven Dynamic Routing of LLM Experts for Financial   Trading</h2><p><strong>Authors:Yang Chen, Yueheng Jiang, Zhaozhao Ma, Yuchen Cao, Jacky Keung, Kun Kuang, Leilei Gan, Yiquan Wu, Fei Wu</strong></p>
<p>The inherent non-stationarity of financial markets and the complexity of multi-modal information pose significant challenges to existing quantitative trading models. Traditional methods relying on fixed structures and unimodal data struggle to adapt to market regime shifts, while large language model (LLM)-driven solutions - despite their multi-modal comprehension - suffer from static strategies and homogeneous expert designs, lacking dynamic adjustment and fine-grained decision mechanisms. To address these limitations, we propose MM-DREX: a Multimodal-driven, Dynamically-Routed EXpert framework based on large language models. MM-DREX explicitly decouples market state perception from strategy execution to enable adaptive sequential decision-making in non-stationary environments. Specifically, it (1) introduces a vision-language model (VLM)-powered dynamic router that jointly analyzes candlestick chart patterns and long-term temporal features to allocate real-time expert weights; (2) designs four heterogeneous trading experts (trend, reversal, breakout, positioning) generating specialized fine-grained sub-strategies; and (3) proposes an SFT-RL hybrid training paradigm to synergistically optimize the routerâ€™s market classification capability and expertsâ€™ risk-adjusted decision-making. Extensive experiments on multi-modal datasets spanning stocks, futures, and cryptocurrencies demonstrate that MM-DREX significantly outperforms 15 baselines (including state-of-the-art financial LLMs and deep reinforcement learning models) across key metrics: total return, Sharpe ratio, and maximum drawdown, validating its robustness and generalization. Additionally, an interpretability module traces routing logic and expert behavior in real time, providing an audit trail for strategy transparency. </p>
<blockquote>
<p>é‡‘èå¸‚åœºå›ºæœ‰çš„éç¨³å®šæ€§å’Œå¤šæ¨¡æ€ä¿¡æ¯çš„å¤æ‚æ€§å¯¹ç°æœ‰é‡åŒ–äº¤æ˜“æ¨¡å‹æ„æˆäº†é‡å¤§æŒ‘æˆ˜ã€‚ä¼ ç»Ÿæ–¹æ³•ä¾èµ–äºå›ºå®šç»“æ„å’Œå•æ¨¡æ€æ•°æ®ï¼Œéš¾ä»¥é€‚åº”å¸‚åœºçŠ¶æ€å˜åŒ–ï¼Œè€Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é©±åŠ¨çš„è§£å†³æ–¹æ¡ˆè™½ç„¶å…·æœ‰å¤šæ¨¡æ€ç†è§£èƒ½åŠ›ï¼Œä½†å´å­˜åœ¨é™æ€ç­–ç•¥å’ŒåŒè´¨åŒ–ä¸“å®¶è®¾è®¡çš„é—®é¢˜ï¼Œç¼ºä¹åŠ¨æ€è°ƒæ•´å’Œç²¾ç»†å†³ç­–æœºåˆ¶ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†MM-DREXï¼šä¸€ä¸ªåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„å¤šæ¨¡æ€é©±åŠ¨ã€åŠ¨æ€è·¯ç”±ä¸“å®¶æ¡†æ¶ã€‚MM-DREXæ˜¾å¼åœ°å°†å¸‚åœºçŠ¶æ€æ„ŸçŸ¥ä¸ç­–ç•¥æ‰§è¡Œè§£è€¦ï¼Œä»¥å®ç°åœ¨éç¨³å®šç¯å¢ƒä¸­çš„è‡ªé€‚åº”åºåˆ—å†³ç­–ã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒï¼ˆ1ï¼‰å¼•å…¥äº†ä¸€ä¸ªç”±è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰é©±åŠ¨çš„åŠ¨æ€è·¯ç”±å™¨ï¼Œè¯¥è·¯ç”±å™¨è”åˆåˆ†æKçº¿å›¾æ¨¡å¼å’Œé•¿æœŸæ—¶é—´ç‰¹å¾æ¥åˆ†é…å®æ—¶ä¸“å®¶æƒé‡ï¼›ï¼ˆ2ï¼‰è®¾è®¡äº†å››ç§å¼‚è´¨äº¤æ˜“ä¸“å®¶ï¼ˆè¶‹åŠ¿ã€åè½¬ã€çªç ´ã€å®šä½ï¼‰ï¼Œç”Ÿæˆä¸“ä¸šçš„ç²¾ç»†å­ç­–ç•¥ï¼›ï¼ˆ3ï¼‰æå‡ºäº†ä¸€ç§SFT-RLæ··åˆè®­ç»ƒèŒƒå¼ï¼ŒååŒä¼˜åŒ–è·¯ç”±å™¨çš„å¸‚åœºåˆ†ç±»èƒ½åŠ›å’Œä¸“å®¶çš„é£é™©è°ƒæ•´å†³ç­–ã€‚åœ¨æ¶µç›–è‚¡ç¥¨ã€æœŸè´§å’ŒåŠ å¯†è´§å¸çš„å¤šæ¨¡æ€æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMM-DREXåœ¨å…³é”®æŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äº15ä¸ªåŸºå‡†æ¨¡å‹ï¼ˆåŒ…æ‹¬æœ€å…ˆè¿›çš„é‡‘èLLMå’Œæ·±åº¦å¼ºåŒ–å­¦ä¹ æ¨¡å‹ï¼‰ï¼Œè¿™äº›æŒ‡æ ‡åŒ…æ‹¬æ€»å›æŠ¥ã€å¤æ™®æ¯”ç‡å’Œæœ€å¤§å›æ’¤ï¼ŒéªŒè¯äº†å…¶ç¨³å¥æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè§£é‡Šæ€§æ¨¡å—å®æ—¶è·Ÿè¸ªè·¯ç”±é€»è¾‘å’Œä¸“å®¶è¡Œä¸ºï¼Œä¸ºç­–ç•¥é€æ˜æ€§æä¾›å®¡è®¡è·Ÿè¸ªã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.05080v2">PDF</a> </p>
<p><strong>Summary</strong><br>é‡‘èå¸‚åœºçš„éå¹³ç¨³æ€§å’Œå¤šæ¨¡æ€ä¿¡æ¯çš„å¤æ‚æ€§å¯¹ç°æœ‰é‡åŒ–äº¤æ˜“æ¨¡å‹æå‡ºäº†é‡å¤§æŒ‘æˆ˜ã€‚ä¼ ç»Ÿæ–¹æ³•éš¾ä»¥é€‚åº”å¸‚åœºçŠ¶æ€å˜åŒ–ï¼Œè€Œå¤§å‹è¯­è¨€æ¨¡å‹é©±åŠ¨çš„æ–¹æ¡ˆè™½ç„¶å…·å¤‡å¤šæ¨¡æ€ç†è§£èƒ½åŠ›ï¼Œä½†ç­–ç•¥é™æ€ã€è®¾è®¡å•ä¸€ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºMM-DREXæ¡†æ¶ï¼ŒåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„å¤šæ¨¡æ€é©±åŠ¨ã€åŠ¨æ€è·¯ç”±æœºåˆ¶ï¼Œé€šè¿‡æ„ŸçŸ¥å¸‚åœºçŠ¶æ€ä¸æ‰§è¡Œç­–ç•¥è§£è€¦ï¼Œå®ç°éå¹³ç¨³ç¯å¢ƒä¸‹çš„è‡ªé€‚åº”åºåˆ—å†³ç­–ã€‚å®éªŒè¯æ˜ï¼ŒMM-DREXåœ¨è‚¡ç¥¨ã€æœŸè´§å’ŒåŠ å¯†è´§å¸ç­‰å¤šæ¨¡æ€æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äº15ç§åŸºçº¿æ–¹æ³•ï¼Œå…·å¤‡å¼ºå¤§çš„ç¨³å¥æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é‡‘èå¸‚åœºéå¹³ç¨³æ€§å’Œå¤šæ¨¡æ€ä¿¡æ¯å¤æ‚æ€§å¯¹é‡åŒ–äº¤æ˜“æ¨¡å‹æ„æˆæŒ‘æˆ˜ã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•éš¾ä»¥é€‚åº”å¸‚åœºå˜åŒ–ï¼Œéœ€è¦æ–°çš„ç­–ç•¥å’Œæ–¹æ³•æ¥å¤„ç†å¤šæ¨¡æ€æ•°æ®ã€‚</li>
<li>MM-DREXæ¡†æ¶æå‡ºä¸€ç§åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„å¤šæ¨¡æ€é©±åŠ¨ã€åŠ¨æ€è·¯ç”±æœºåˆ¶ã€‚</li>
<li>MM-DREXé€šè¿‡æ„ŸçŸ¥å¸‚åœºçŠ¶æ€ä¸æ‰§è¡Œç­–ç•¥è§£è€¦ï¼Œå®ç°è‡ªé€‚åº”åºåˆ—å†³ç­–ã€‚</li>
<li>MM-DREXæ¡†æ¶åŒ…æ‹¬ä¸€ä¸ªåŠ¨æ€è·¯ç”±å™¨å’Œå››ç§äº¤æ˜“ä¸“å®¶ï¼Œç”Ÿæˆç²¾ç»†çš„å­ç­–ç•¥ã€‚</li>
<li>å®éªŒè¯æ˜MM-DREXåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œå…·å¤‡å¼ºå¤§çš„ç¨³å¥æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.05080">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.05080v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.05080v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.05080v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.05080v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="ACE-RL-Adaptive-Constraint-Enhanced-Reward-for-Long-form-Generation-Reinforcement-Learning"><a href="#ACE-RL-Adaptive-Constraint-Enhanced-Reward-for-Long-form-Generation-Reinforcement-Learning" class="headerlink" title="ACE-RL: Adaptive Constraint-Enhanced Reward for Long-form Generation   Reinforcement Learning"></a>ACE-RL: Adaptive Constraint-Enhanced Reward for Long-form Generation   Reinforcement Learning</h2><p><strong>Authors:Jianghao Chen, Wei Sun, Qixiang Yin, Lingxing Kong, Zhixing Tan, Jiajun Zhang</strong></p>
<p>Large Language Models (LLMs) have demonstrated remarkable progress in long-context understanding, yet they face significant challenges in high-quality long-form generation. Existing studies primarily suffer from two limitations: (1) A heavy reliance on scarce, high-quality long-form response data for supervised fine-tuning (SFT) or for pairwise preference reward in reinforcement learning (RL). (2) Focus on coarse-grained quality optimization dimensions, such as relevance, coherence, and helpfulness, overlooking the fine-grained specifics inherent to diverse long-form generation scenarios. To address this issue, we propose a framework using Adaptive Constraint-Enhanced reward for long-form generation Reinforcement Learning (ACE-RL). ACE-RL first automatically deconstructs each instruction into a set of fine-grained, adaptive constraint criteria by identifying its underlying intents and demands. Subsequently, we design a reward mechanism that quantifies the quality of long-form responses based on their satisfaction over corresponding constraints, converting subjective quality evaluation into constraint verification. Finally, we utilize reinforcement learning to guide models toward superior long-form generation capabilities. Experimental results demonstrate that our ACE-RL framework significantly outperforms existing SFT and RL baselines by 20.70% and 7.32% on WritingBench, and our top-performing model even surpasses proprietary systems like GPT-4o by 7.10%, providing a more effective training paradigm for LLMs to generate high-quality content across diverse long-form generation scenarios. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç†è§£é•¿æ–‡æœ¬æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œä½†åœ¨é«˜è´¨é‡é•¿æ–‡æœ¬ç”Ÿæˆæ–¹é¢ä»é¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚ç°æœ‰ç ”ç©¶ä¸»è¦å­˜åœ¨ä¸¤ä¸ªå±€é™æ€§ï¼šï¼ˆ1ï¼‰è¿‡äºä¾èµ–ç¨€ç¼ºçš„é«˜è´¨é‡é•¿æ–‡æœ¬å“åº”æ•°æ®è¿›è¡Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æˆ–åœ¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸­çš„æˆå¯¹åå¥½å¥–åŠ±ã€‚ï¼ˆ2ï¼‰å…³æ³¨ç²—ç²’åº¦çš„è´¨é‡ä¼˜åŒ–ç»´åº¦ï¼Œå¦‚ç›¸å…³æ€§ã€è¿è´¯æ€§å’Œæœ‰ç”¨æ€§ï¼Œå¿½è§†äº†é•¿æ–‡æœ¬ç”Ÿæˆåœºæ™¯ä¸­å›ºæœ‰çš„ç»†ç²’åº¦ç»†èŠ‚ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä½¿ç”¨è‡ªé€‚åº”çº¦æŸå¢å¼ºå¥–åŠ±çš„é•¿æ–‡æœ¬ç”Ÿæˆå¼ºåŒ–å­¦ä¹ ï¼ˆACE-RLï¼‰æ¡†æ¶ã€‚ACE-RLé¦–å…ˆè‡ªåŠ¨å°†æ¯ä¸ªæŒ‡ä»¤åˆ†è§£ä¸ºä¸€ç»„ç»†ç²’åº¦çš„è‡ªé€‚åº”çº¦æŸæ ‡å‡†ï¼Œé€šè¿‡è¯†åˆ«å…¶æ½œåœ¨æ„å›¾å’Œéœ€æ±‚æ¥å®ç°ã€‚éšåï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§å¥–åŠ±æœºåˆ¶ï¼Œæ ¹æ®é•¿æ–‡æœ¬å“åº”å¯¹ç›¸åº”çº¦æŸçš„æ»¡è¶³ç¨‹åº¦æ¥é‡åŒ–å…¶è´¨é‡ï¼Œå°†ä¸»è§‚è´¨é‡è¯„ä»·è½¬åŒ–ä¸ºçº¦æŸéªŒè¯ã€‚æœ€åï¼Œæˆ‘ä»¬ä½¿ç”¨å¼ºåŒ–å­¦ä¹ æ¥æŒ‡å¯¼æ¨¡å‹å®ç°æ›´å‡ºè‰²çš„é•¿æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ACE-RLæ¡†æ¶åœ¨WritingBenchä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰çš„SFTå’ŒRLåŸºçº¿ï¼Œåˆ†åˆ«æé«˜äº†20.70%å’Œ7.32%ï¼Œæˆ‘ä»¬è¡¨ç°æœ€ä½³çš„æ¨¡å‹ç”šè‡³è¶…è¿‡äº†å¦‚GPT-4oç­‰ä¸“æœ‰ç³»ç»Ÿï¼Œè¾¾åˆ°äº†7.10%ï¼Œä¸ºLLMç”Ÿæˆé«˜è´¨é‡å†…å®¹æä¾›äº†æ›´æœ‰æ•ˆçš„è®­ç»ƒèŒƒå¼ï¼Œé€‚ç”¨äºå¤šç§é•¿æ–‡æœ¬ç”Ÿæˆåœºæ™¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.04903v2">PDF</a> Under review, our code is available at <a target="_blank" rel="noopener" href="https://github.com/ZNLP/ACE-RL">https://github.com/ZNLP/ACE-RL</a></p>
<p><strong>Summary</strong>ï¼šå¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç†è§£é•¿æ–‡æœ¬æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨é«˜è´¨é‡é•¿æ–‡æœ¬ç”Ÿæˆæ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚ç°æœ‰ç ”ç©¶ä¸»è¦å­˜åœ¨ä¸¤ä¸ªå±€é™æ€§ï¼šï¼ˆ1ï¼‰è¿‡äºä¾èµ–ç¨€ç¼ºçš„é«˜è´¨é‡é•¿æ–‡æœ¬å“åº”æ•°æ®è¿›è¡Œç›‘ç£å¾®è°ƒæˆ–å¼ºåŒ–å­¦ä¹ ä¸­çš„é…å¯¹åå¥½å¥–åŠ±ï¼›ï¼ˆ2ï¼‰å…³æ³¨ç²—ç•¥çš„è´¨é‡ä¼˜åŒ–ç»´åº¦ï¼Œå¦‚ç›¸å…³æ€§ã€è¿è´¯æ€§å’Œæœ‰ç”¨æ€§ï¼Œå¿½è§†äº†é•¿æ–‡æœ¬ç”Ÿæˆåœºæ™¯ä¸­å›ºæœ‰çš„ç»†å¾®å·®å¼‚ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ä½¿ç”¨è‡ªé€‚åº”çº¦æŸå¢å¼ºå¥–åŠ±çš„é•¿æ–‡æœ¬ç”Ÿæˆå¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼ˆACE-RLï¼‰ã€‚ACE-RLé¦–å…ˆè‡ªåŠ¨å°†æ¯ä¸ªæŒ‡ä»¤åˆ†è§£ä¸ºä¸€ç»„ç²¾ç»†çš„ã€è‡ªé€‚åº”çº¦æŸæ ‡å‡†ï¼Œé€šè¿‡è¯†åˆ«å…¶æ½œåœ¨æ„å›¾å’Œéœ€æ±‚ã€‚éšåï¼Œè®¾è®¡äº†ä¸€ç§å¥–åŠ±æœºåˆ¶ï¼Œæ ¹æ®é•¿æ–‡æœ¬å“åº”å¯¹ç›¸åº”çº¦æŸçš„æ»¡è¶³ç¨‹åº¦æ¥é‡åŒ–å…¶è´¨é‡ï¼Œå°†ä¸»è§‚è´¨é‡è¯„ä»·è½¬åŒ–ä¸ºçº¦æŸéªŒè¯ã€‚æœ€åï¼Œåˆ©ç”¨å¼ºåŒ–å­¦ä¹ æŒ‡å¯¼æ¨¡å‹å®ç°æ›´ä¼˜è´¨çš„é•¿æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒACE-RLæ¡†æ¶åœ¨WritingBenchä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å­¦ä¹ åŸºçº¿æ–¹æ³•ï¼Œåˆ†åˆ«æé«˜äº†20.70%å’Œ7.32%ï¼Œç”šè‡³è¶…è¿‡GPT-4oç³»ç»Ÿ7.1%ï¼Œä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆé«˜è´¨é‡å†…å®¹æä¾›äº†æ›´æœ‰æ•ˆçš„è®­ç»ƒèŒƒå¼ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é•¿æ–‡æœ¬ç†è§£æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨é«˜è´¨é‡é•¿æ–‡æœ¬ç”Ÿæˆä¸Šå­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰ç ”ç©¶ä¸»è¦ä¾èµ–ç¨€ç¼ºçš„é«˜è´¨é‡é•¿æ–‡æœ¬æ•°æ®è¿›è¡Œç›‘ç£å¾®è°ƒæˆ–å¼ºåŒ–å­¦ä¹ ä¸­çš„é…å¯¹åå¥½å¥–åŠ±ã€‚</li>
<li>ç°æœ‰ç ”ç©¶ä¸»è¦å…³æ³¨ç²—ç•¥çš„è´¨é‡ä¼˜åŒ–ç»´åº¦ï¼Œå¿½è§†äº†é•¿æ–‡æœ¬ç”Ÿæˆåœºæ™¯ä¸­ç»†å¾®çš„å·®å¼‚ã€‚</li>
<li>ACE-RLæ¡†æ¶æå‡ºè‡ªåŠ¨å°†æŒ‡ä»¤åˆ†è§£ä¸ºç²¾ç»†çš„ã€è‡ªé€‚åº”çº¦æŸæ ‡å‡†çš„æ–¹æ³•ã€‚</li>
<li>ACE-RLè®¾è®¡äº†ä¸€ç§å¥–åŠ±æœºåˆ¶ï¼Œæ ¹æ®é•¿æ–‡æœ¬å“åº”å¯¹çº¦æŸçš„æ»¡è¶³ç¨‹åº¦é‡åŒ–å…¶è´¨é‡ã€‚</li>
<li>ACE-RLåˆ©ç”¨å¼ºåŒ–å­¦ä¹ æŒ‡å¯¼æ¨¡å‹å®ç°æ›´ä¼˜è´¨çš„é•¿æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.04903">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.04903v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.04903v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.04903v2/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.04903v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.04903v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.04903v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Advancing-SLM-Tool-Use-Capability-using-Reinforcement-Learning"><a href="#Advancing-SLM-Tool-Use-Capability-using-Reinforcement-Learning" class="headerlink" title="Advancing SLM Tool-Use Capability using Reinforcement Learning"></a>Advancing SLM Tool-Use Capability using Reinforcement Learning</h2><p><strong>Authors:Dhruvi Paprunia, Vansh Kharidia, Pankti Doshi</strong></p>
<p>In an era where tool-augmented AI agents are becoming increasingly vital, our findings highlight the ability of Group Relative Policy Optimization (GRPO) to empower SLMs, which are traditionally constrained in tool use. The ability to use tools effectively has become a defining feature of Large Language Models (LLMs), allowing them to access external data and internal resources. As AI agents grow more sophisticated, tool-use capabilities have become indispensable. While LLMs have made significant progress in this area, Small Language Models (SLMs) still face challenges in accurately integrating tool use, especially in resource-constrained settings.   This study investigates how Reinforcement Learning, specifically Group Relative Policy Optimization (GRPO), can enhance the tool-use accuracy of SLMs. By designing a well-defined reward system that reinforces structured JSON output, correct tool selection, and precise parameter usage, we demonstrate that GRPO enables SLMs to achieve significant improvements in tool-use capabilities (function calling&#x2F;JSON output). Our approach provides a computationally efficient training method that enhances SLMs practical deployment in real-world AI applications. </p>
<blockquote>
<p>åœ¨å·¥å…·å¢å¼ºå‹AIä»£ç†æ—¥ç›Šé‡è¦çš„æ—¶ä»£ï¼Œæˆ‘ä»¬çš„ç ”ç©¶çªå‡ºäº†ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰åœ¨èµ‹èƒ½ä¼ ç»Ÿä¸Šåœ¨å·¥å…·ä½¿ç”¨æ–¹é¢å—é™çš„å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰çš„èƒ½åŠ›ã€‚æœ‰æ•ˆåœ°ä½¿ç”¨å·¥å…·å·²æˆä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ ‡å¿—æ€§ç‰¹å¾ï¼Œä½¿å®ƒä»¬èƒ½å¤Ÿè®¿é—®å¤–éƒ¨æ•°æ®å’Œå†…éƒ¨èµ„æºã€‚éšç€äººå·¥æ™ºèƒ½ä»£ç†æ—¥ç›Šæˆç†Ÿï¼Œå·¥å…·ä½¿ç”¨èƒ½åŠ›å·²æˆä¸ºä¸å¯æˆ–ç¼ºçš„éƒ¨åˆ†ã€‚è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è¯¥é¢†åŸŸå–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†å°å‹è¯­è¨€æ¨¡å‹åœ¨å‡†ç¡®æ•´åˆå·¥å…·ä½¿ç”¨æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†å¼ºåŒ–å­¦ä¹ ï¼Œç‰¹åˆ«æ˜¯ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œå¦‚ä½•å¢å¼ºå°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰çš„å·¥å…·ä½¿ç”¨å‡†ç¡®æ€§ã€‚é€šè¿‡è®¾è®¡æ˜ç¡®çš„å¥–åŠ±ç³»ç»Ÿï¼Œå¼ºåŒ–ç»“æ„åŒ–JSONè¾“å‡ºã€æ­£ç¡®çš„å·¥å…·é€‰æ‹©å’Œç²¾ç¡®çš„å‚æ•°ä½¿ç”¨ï¼Œæˆ‘ä»¬è¯æ˜äº†GRPOä½¿å°å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿåœ¨å·¥å…·ä½¿ç”¨èƒ½åŠ›ï¼ˆå‡½æ•°è°ƒç”¨&#x2F;JSONè¾“å‡ºï¼‰æ–¹é¢å–å¾—é‡å¤§æ”¹è¿›ã€‚æˆ‘ä»¬çš„æ–¹æ³•æä¾›äº†ä¸€ç§è®¡ç®—æ•ˆç‡é«˜çš„è®­ç»ƒæ–¹æ³•ï¼Œæé«˜äº†å°å‹è¯­è¨€æ¨¡å‹åœ¨å®é™…éƒ¨ç½²åˆ°ç°å®ä¸–ç•ŒAIåº”ç”¨ä¸­çš„å®ç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.04518v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å·¥å…·å¢å¼ºå‹AIä»£ç†çš„æ—¶ä»£ï¼ŒGRPOèƒ½åŠ©åŠ›ä¼ ç»Ÿä¸Šåœ¨å·¥å…·ä½¿ç”¨æ–¹é¢å—é™çš„å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMsï¼‰ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä½¿ç”¨å·¥å…·æ–¹é¢çš„èƒ½åŠ›æˆä¸ºå…¶æ ‡å¿—æ€§ç‰¹å¾ï¼Œå¯ä»¥è®¿é—®å¤–éƒ¨æ•°æ®å’Œå†…éƒ¨èµ„æºã€‚è™½ç„¶LLMsåœ¨æ­¤æ–¹é¢å·²å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†SLMsåœ¨å‡†ç¡®æ•´åˆå·¥å…·ä½¿ç”¨æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­ã€‚æœ¬ç ”ç©¶æ¢è®¨å¼ºåŒ–å­¦ä¹ ä¸­çš„ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰å¦‚ä½•æå‡SLMsçš„å·¥å…·ä½¿ç”¨å‡†ç¡®æ€§ã€‚é€šè¿‡è®¾è®¡æ˜ç¡®çš„å¥–åŠ±ç³»ç»Ÿï¼Œå¼ºåŒ–ç»“æ„åŒ–çš„JSONè¾“å‡ºã€æ­£ç¡®çš„å·¥å…·é€‰æ‹©å’Œç²¾ç¡®çš„å‚æ•°ä½¿ç”¨ï¼Œç ”ç©¶è¯æ˜GRPOä½¿SLMsåœ¨å·¥å…·ä½¿ç”¨èƒ½åŠ›ä¸Šå®ç°äº†æ˜¾è‘—çš„æå‡ï¼ˆå‡½æ•°è°ƒç”¨&#x2F;JSONè¾“å‡ºï¼‰ã€‚æˆ‘ä»¬çš„æ–¹æ³•æä¾›äº†ä¸€ç§è®¡ç®—æ•ˆç‡é«˜çš„è®­ç»ƒæ–¹æ³•ï¼Œæé«˜äº†SLMsåœ¨å®é™…AIåº”ç”¨ä¸­çš„éƒ¨ç½²èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Group Relative Policy Optimization (GRPO) å¯ä»¥åŠ©åŠ›å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMsï¼‰åœ¨å·¥å…·ä½¿ç”¨æ–¹é¢çš„èƒ½åŠ›ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å·¥å…·ä½¿ç”¨èƒ½åŠ›å·²æˆä¸ºå…¶æ ‡å¿—æ€§ç‰¹å¾ã€‚</li>
<li>åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­ï¼ŒSLMsåœ¨å‡†ç¡®æ•´åˆå·¥å…·ä½¿ç”¨æ–¹é¢é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ ä¸­çš„GRPOé€šè¿‡æ˜ç¡®çš„å¥–åŠ±ç³»ç»Ÿæå‡SLMsçš„å·¥å…·ä½¿ç”¨å‡†ç¡®æ€§ã€‚</li>
<li>GRPOé€šè¿‡å¼ºåŒ–ç»“æ„åŒ–çš„JSONè¾“å‡ºã€æ­£ç¡®çš„å·¥å…·é€‰æ‹©å’Œç²¾ç¡®çš„å‚æ•°ä½¿ç”¨æ¥å®ç°æå‡ã€‚</li>
<li>GRPOä½¿SLMsåœ¨å·¥å…·ä½¿ç”¨èƒ½åŠ›ä¸Šå®ç°äº†æ˜¾è‘—çš„æå‡ï¼Œå¦‚å‡½æ•°è°ƒç”¨å’ŒJSONè¾“å‡ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.04518">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.04518v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.04518v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.04518v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.04518v2/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.04518v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.04518v2/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.04518v2/page_4_2.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="EvoEmo-Towards-Evolved-Emotional-Policies-for-LLM-Agents-in-Multi-Turn-Negotiation"><a href="#EvoEmo-Towards-Evolved-Emotional-Policies-for-LLM-Agents-in-Multi-Turn-Negotiation" class="headerlink" title="EvoEmo: Towards Evolved Emotional Policies for LLM Agents in Multi-Turn   Negotiation"></a>EvoEmo: Towards Evolved Emotional Policies for LLM Agents in Multi-Turn   Negotiation</h2><p><strong>Authors:Yunbo Long, Liming Xu, Lukas Beckenbauer, Yuhan Liu, Alexandra Brintrup</strong></p>
<p>Recent research on Chain-of-Thought (CoT) reasoning in Large Language Models (LLMs) has demonstrated that agents can engage in \textit{complex}, \textit{multi-turn} negotiations, opening new avenues for agentic AI. However, existing LLM agents largely overlook the functional role of emotions in such negotiations, instead generating passive, preference-driven emotional responses that make them vulnerable to manipulation and strategic exploitation by adversarial counterparts. To address this gap, we present EvoEmo, an evolutionary reinforcement learning framework that optimizes dynamic emotional expression in negotiations. EvoEmo models emotional state transitions as a Markov Decision Process and employs population-based genetic optimization to evolve high-reward emotion policies across diverse negotiation scenarios. We further propose an evaluation framework with two baselines â€“ vanilla strategies and fixed-emotion strategies â€“ for benchmarking emotion-aware negotiation. Extensive experiments and ablation studies show that EvoEmo consistently outperforms both baselines, achieving higher success rates, higher efficiency, and increased buyer savings. This findings highlight the importance of adaptive emotional expression in enabling more effective LLM agents for multi-turn negotiation. </p>
<blockquote>
<p>å…³äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†çš„æœ€æ–°ç ”ç©¶è¡¨æ˜ï¼Œæ™ºèƒ½ä½“å¯ä»¥å‚ä¸å¤æ‚çš„å¤šè½®è°ˆåˆ¤ï¼Œä¸ºæ™ºèƒ½ä½“äººå·¥æ™ºèƒ½å¼€è¾Ÿäº†æ–°çš„é€”å¾„ã€‚ç„¶è€Œï¼Œç°æœ‰çš„LLMæ™ºèƒ½ä½“åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå¿½è§†äº†æƒ…ç»ªåœ¨è¿™ç§è°ˆåˆ¤ä¸­çš„åŠŸèƒ½ä½œç”¨ï¼Œè€Œæ˜¯äº§ç”Ÿè¢«åŠ¨ã€åå¥½é©±åŠ¨çš„æƒ…ç»ªååº”ï¼Œä½¿å®ƒä»¬å®¹æ˜“å—åˆ°å¯¹æŠ—æ€§å¯¹æ‰‹çš„æ“çºµå’Œæˆ˜ç•¥åˆ©ç”¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†EvoEmoï¼Œè¿™æ˜¯ä¸€ä¸ªè¿›åŒ–å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨ä¼˜åŒ–è°ˆåˆ¤ä¸­çš„åŠ¨æ€æƒ…ç»ªè¡¨è¾¾ã€‚EvoEmoå°†æƒ…ç»ªçŠ¶æ€è½¬æ¢å»ºæ¨¡ä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼Œå¹¶åŸºäºç§ç¾¤é—ä¼ ä¼˜åŒ–æ¥æ¼”åŒ–ä¸åŒè°ˆåˆ¤åœºæ™¯ä¸‹çš„é«˜å›æŠ¥æƒ…ç»ªç­–ç•¥ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†ä¸€ä¸ªè¯„ä¼°æ¡†æ¶ï¼Œå…¶ä¸­åŒ…æ‹¬ä¸¤ä¸ªåŸºå‡†çº¿â€”â€”æ™®é€šç­–ç•¥å’Œå›ºå®šæƒ…ç»ªç­–ç•¥â€”â€”ç”¨äºè¯„ä¼°æƒ…æ„Ÿæ„ŸçŸ¥è°ˆåˆ¤ã€‚å¹¿æ³›çš„å®éªŒå’Œæ¶ˆèç ”ç©¶è¡¨æ˜ï¼ŒEvoEmoå§‹ç»ˆä¼˜äºè¿™ä¸¤ä¸ªåŸºå‡†çº¿ï¼Œå®ç°äº†æ›´é«˜çš„æˆåŠŸç‡ã€æ›´é«˜çš„æ•ˆç‡å’Œæ›´é«˜çš„ä¹°å®¶èŠ‚çœã€‚è¿™äº›å‘ç°çªæ˜¾äº†åœ¨å¤šè½®è°ˆåˆ¤ä¸­è‡ªé€‚åº”æƒ…ç»ªè¡¨è¾¾çš„é‡è¦æ€§ï¼Œä½¿LLMæ™ºèƒ½ä½“æ›´åŠ æœ‰æ•ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.04310v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸçš„Chain-of-Thoughtï¼ˆCoTï¼‰æ¨ç†ç ”ç©¶è¡¨æ˜ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯ä»¥åœ¨å¤šè½®è°ˆåˆ¤ä¸­å‘æŒ¥å¤æ‚ä½œç”¨ï¼Œå¼€å¯äº†æ™ºèƒ½ä»£ç†çš„æ–°æ–¹å‘ã€‚ç„¶è€Œï¼Œç°æœ‰LLMä»£ç†å¤§å¤šå¿½è§†äº†æƒ…ç»ªåœ¨è¿™ç§è°ˆåˆ¤ä¸­çš„åŠŸèƒ½ä½œç”¨ï¼Œè€Œæ˜¯äº§ç”Ÿè¢«åŠ¨ã€åå¥½é©±åŠ¨çš„æƒ…ç»ªååº”ï¼Œä½¿å…¶å®¹æ˜“å—åˆ°å¯¹æ‰‹çš„æˆ˜ç•¥æ“çºµå’Œå‰¥å‰Šã€‚ä¸ºè§£å†³è¿™ä¸€ç¼ºé™·ï¼Œæˆ‘ä»¬æå‡ºäº†EvoEmoï¼Œä¸€ä¸ªè¿›åŒ–å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œä¼˜åŒ–è°ˆåˆ¤ä¸­çš„åŠ¨æ€æƒ…ç»ªè¡¨è¾¾ã€‚EvoEmoå°†æƒ…ç»ªçŠ¶æ€è½¬æ¢å»ºæ¨¡ä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼Œå¹¶åˆ©ç”¨åŸºäºç§ç¾¤é—ä¼ ä¼˜åŒ–ç®—æ³•æ¥è¿›åŒ–ä¸åŒè°ˆåˆ¤åœºæ™¯ä¸‹çš„é«˜å›æŠ¥æƒ…ç»ªç­–ç•¥ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ä¸ªè¯„ä¼°æ¡†æ¶ï¼Œå…¶ä¸­åŒ…æ‹¬åŸºå‡†ç­–ç•¥å’Œå›ºå®šæƒ…ç»ªç­–ç•¥ä¸¤ç§åŸºå‡†çº¿æ–¹æ³•ã€‚ç»è¿‡å¤§é‡å®éªŒå’Œå»é™¤ç ”ç©¶è¡¨æ˜ï¼ŒEvoEmoè¡¨ç°ä¼˜ç§€ä¸”èƒ½æŒç»­å‘æŒ¥ä½œç”¨ï¼Œä¸è¿™ä¸¤ç§åŸºçº¿ç›¸æ¯”å…·æœ‰æ›´é«˜çš„æˆåŠŸç‡ã€æ›´é«˜çš„æ•ˆç‡å’Œæ›´é«˜çš„ä¹°å®¶èŠ‚çœç‡ã€‚è¿™è¡¨æ˜åœ¨å¤æ‚çš„å¤šè½®è°ˆåˆ¤ä¸­ï¼Œè‡ªé€‚åº”çš„æƒ…ç»ªè¡¨è¾¾å¯¹äºLLMä»£ç†è‡³å…³é‡è¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LLMså…·å¤‡è¿›è¡Œå¤æ‚å¤šè½®è°ˆåˆ¤çš„èƒ½åŠ›ï¼Œä½†ç°æœ‰ä»£ç†åœ¨è°ˆåˆ¤ä¸­å¿½è§†äº†æƒ…ç»ªçš„ä½œç”¨ã€‚</li>
<li>EvoEmoæ˜¯ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨ä¼˜åŒ–LLMåœ¨è°ˆåˆ¤ä¸­çš„åŠ¨æ€æƒ…ç»ªè¡¨è¾¾ã€‚</li>
<li>EvoEmoå°†æƒ…ç»ªçŠ¶æ€è½¬æ¢å»ºæ¨¡ä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹å¹¶è¿ç”¨é—ä¼ ç®—æ³•æ¥è¿›åŒ–æƒ…ç»ªç­–ç•¥ã€‚</li>
<li>è¯„ä¼°æ¡†æ¶åŒ…æ‹¬åŸºå‡†ç­–ç•¥å’Œå›ºå®šæƒ…ç»ªç­–ç•¥ä¸¤ç§åŸºå‡†çº¿æ–¹æ³•ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒEvoEmoåœ¨æˆåŠŸç‡ã€æ•ˆç‡å’Œä¹°å®¶èŠ‚çœç‡æ–¹é¢ä¼˜äºç°æœ‰åŸºçº¿ç­–ç•¥ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.04310">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.04310v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.04310v2/page_2_0.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Emergent-Hierarchical-Reasoning-in-LLMs-through-Reinforcement-Learning"><a href="#Emergent-Hierarchical-Reasoning-in-LLMs-through-Reinforcement-Learning" class="headerlink" title="Emergent Hierarchical Reasoning in LLMs through Reinforcement Learning"></a>Emergent Hierarchical Reasoning in LLMs through Reinforcement Learning</h2><p><strong>Authors:Haozhe Wang, Qixin Xu, Che Liu, Junhong Wu, Fangzhen Lin, Wenhu Chen</strong></p>
<p>Reinforcement Learning (RL) has proven highly effective at enhancing the complex reasoning abilities of Large Language Models (LLMs), yet underlying mechanisms driving this success remain largely opaque. Our analysis reveals that puzzling phenomena like <code>aha moments&quot;, </code>length-scalingâ€™â€™ and entropy dynamics are not disparate occurrences but hallmarks of an emergent reasoning hierarchy, akin to the separation of high-level strategic planning from low-level procedural execution in human cognition. We uncover a compelling two-phase dynamic: initially, a model is constrained by procedural correctness and must improve its low-level skills. The learning bottleneck then decisively shifts, with performance gains being driven by the exploration and mastery of high-level strategic planning. This insight exposes a core inefficiency in prevailing RL algorithms like GRPO, which apply optimization pressure agnostically and dilute the learning signal across all tokens. To address this, we propose HIerarchy-Aware Credit Assignment (HICRA), an algorithm that concentrates optimization efforts on high-impact planning tokens. HICRA significantly outperforms strong baselines, demonstrating that focusing on this strategic bottleneck is key to unlocking advanced reasoning. Furthermore, we validate semantic entropy as a superior compass for measuring strategic exploration over misleading metrics such as token-level entropy. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å·²ç»è¯æ˜å¯ä»¥æœ‰æ•ˆåœ°æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¤æ‚æ¨ç†èƒ½åŠ›ï¼Œç„¶è€Œé©±åŠ¨è¿™ä¸€æˆåŠŸçš„æ½œåœ¨æœºåˆ¶ä»ç„¶å¤§éƒ¨åˆ†æœªçŸ¥ã€‚æˆ‘ä»¬çš„åˆ†ææ­ç¤ºäº†ä¸€äº›ä»¤äººå›°æƒ‘çš„ç°è±¡ï¼Œå¦‚â€œå•Šå“ˆæ—¶åˆ»â€ã€â€œé•¿åº¦ç¼©æ”¾â€å’Œç†µåŠ¨åŠ›å­¦ï¼Œå®ƒä»¬å¹¶ä¸æ˜¯å­¤ç«‹çš„äº‹ä»¶ï¼Œè€Œæ˜¯æ–°å…´æ¨ç†å±‚æ¬¡çš„æ ‡å¿—ï¼Œç±»ä¼¼äºäººç±»è®¤çŸ¥ä¸­é«˜çº§æˆ˜ç•¥è§„åˆ’ä¸ä½çº§ç¨‹åºæ‰§è¡Œçš„åˆ†ç¦»ã€‚æˆ‘ä»¬å‘ç°äº†ä¸¤ä¸ªé˜¶æ®µçš„åŠ¨æ€è¿‡ç¨‹ï¼šæœ€åˆï¼Œæ¨¡å‹å—åˆ°ç¨‹åºæ­£ç¡®æ€§çš„çº¦æŸï¼Œå¿…é¡»æé«˜å…¶ä½çº§æŠ€èƒ½ã€‚å­¦ä¹ ç“¶é¢ˆéšåå‘ç”Ÿå†³å®šæ€§è½¬ç§»ï¼Œæ€§èƒ½æå‡æºäºé«˜çº§æˆ˜ç•¥è§„åˆ’çš„æ¢ç´¢ä¸æŒæ¡ã€‚è¿™ç§è§è§£æ­ç¤ºäº†ç°è¡ŒRLç®—æ³•ï¼ˆå¦‚GRPOï¼‰çš„æ ¸å¿ƒä½æ•ˆä¹‹å¤„ï¼Œè¿™äº›ç®—æ³•ç›²ç›®åœ°æ–½åŠ ä¼˜åŒ–å‹åŠ›ï¼Œå¹¶å°†å­¦ä¹ ä¿¡å·ç¨€é‡Šåˆ°æ‰€æœ‰ä»¤ç‰Œä¸­ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†HIerarchy-Aware Credit Assignment (HICRA)ç®—æ³•ï¼Œè¯¥ç®—æ³•å°†ä¼˜åŒ–å·¥ä½œé›†ä¸­åœ¨å¯¹è§„åˆ’ä»¤ç‰Œå½±å“å¤§çš„åœ°æ–¹ã€‚HICRAæ˜¾è‘—ä¼˜äºå¼ºå¤§çš„åŸºçº¿ï¼Œè¯æ˜å…³æ³¨è¿™ä¸€æˆ˜ç•¥ç“¶é¢ˆæ˜¯è§£é”é«˜çº§æ¨ç†çš„å…³é”®ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬éªŒè¯äº†è¯­ä¹‰ç†µä½œä¸ºè¡¡é‡æˆ˜ç•¥æ¢ç´¢çš„æŒ‡å—é’ˆä¼˜äºä»¤ç‰Œçº§ç†µç­‰è¯¯å¯¼æ€§æŒ‡æ ‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.03646v2">PDF</a> Preprint</p>
<p><strong>Summary</strong><br>å¼ºåŒ–å­¦ä¹ åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„å¤æ‚æ¨ç†èƒ½åŠ›æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—æ•ˆæœï¼Œä½†å…¶èƒŒåçš„æœºåˆ¶ä»å¤§å¤šæœªçŸ¥ã€‚æˆ‘ä»¬çš„åˆ†æå‘ç°ï¼Œâ€œå•Šå“ˆæ—¶åˆ»â€ã€â€œé•¿åº¦ç¼©æ”¾â€å’Œç†µåŠ¨åŠ›ç­‰ä»¤äººå›°æƒ‘çš„ç°è±¡å¹¶ä¸æ˜¯å­¤ç«‹çš„äº‹ä»¶ï¼Œè€Œæ˜¯æ–°å…´æ¨ç†å±‚æ¬¡çš„æ ‡å¿—ï¼Œç±»ä¼¼äºäººç±»è®¤çŸ¥ä¸­é«˜çº§æˆ˜ç•¥è§„åˆ’ä¸ä½çº§ç¨‹åºæ‰§è¡Œçš„åˆ†ç¦»ã€‚æˆ‘ä»¬æ­ç¤ºäº†å¼•äººæ³¨ç›®çš„ä¸¤é˜¶æ®µåŠ¨æ€è¿‡ç¨‹ï¼šåˆæœŸï¼Œæ¨¡å‹å—ç¨‹åºæ­£ç¡®æ€§çº¦æŸï¼Œå¿…é¡»æé«˜å…¶ä½çº§æŠ€èƒ½ï¼›éšåï¼Œå­¦ä¹ ç“¶é¢ˆå‘ç”Ÿå†³å®šæ€§è½¬ç§»ï¼Œæ€§èƒ½æå‡å¾—ç›Šäºé«˜çº§æˆ˜ç•¥è§„åˆ’çš„æ¢ç©¶ä¸æŒæ¡ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†å±‚æ¬¡æ„ŸçŸ¥ä¿¡ç”¨åˆ†é…ï¼ˆHICRAï¼‰ç®—æ³•ï¼Œé›†ä¸­ä¼˜åŒ–åŠªåŠ›äºé«˜å½±å“åŠ›è§„åˆ’ä»¤ç‰Œï¼Œæ˜¾è‘—ä¼˜äºå¼ºæœ‰åŠ›çš„åŸºçº¿ï¼Œè¯æ˜äº†è§£å†³è¿™ä¸€æˆ˜ç•¥ç“¶é¢ˆçš„å…³é”®åœ¨äºä¸“æ³¨äºå…ˆè¿›æ¨ç†çš„è§£é”ã€‚æˆ‘ä»¬è¿˜éªŒè¯äº†è¯­ä¹‰ç†µä½œä¸ºè¡¡é‡æˆ˜ç•¥æ¢ç´¢çš„ä¼˜è¶ŠæŒ‡æ ‡ï¼Œç›¸è¾ƒäºè¯¯å¯¼æ€§æŒ‡æ ‡å¦‚ä»¤ç‰Œçº§ç†µæ›´ä¸ºæœ‰æ•ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ä¸Šè¡¨ç°æ˜¾è‘—ï¼Œä½†å…¶èƒŒåçš„æœºåˆ¶å°šå¾…æ·±å…¥äº†è§£ã€‚</li>
<li>è§‚å¯Ÿåˆ°å¦‚â€œå•Šå“ˆæ—¶åˆ»â€ç­‰ä»¤äººå›°æƒ‘çš„ç°è±¡å…¶å®æ˜¯æ¨ç†å±‚æ¬¡å‡ºç°çš„æ ‡å¿—ã€‚</li>
<li>æ¨¡å‹å­¦ä¹ åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šåˆçº§é˜¶æ®µæ³¨é‡æé«˜ä½çº§æŠ€èƒ½ï¼Œéšåè½¬ç§»åˆ°é«˜çº§æˆ˜ç•¥è§„åˆ’å’Œæ¢ç´¢ã€‚</li>
<li>å½“å‰å¼ºåŒ–å­¦ä¹ ç®—æ³•å¦‚GRPOå­˜åœ¨æ ¸å¿ƒä½æ•ˆé—®é¢˜ï¼Œä¼˜åŒ–å‹åŠ›åº”ç”¨è¿‡äºå¹¿æ³›ã€‚</li>
<li>æå‡ºäº†å±‚æ¬¡æ„ŸçŸ¥ä¿¡ç”¨åˆ†é…ï¼ˆHICRAï¼‰ç®—æ³•ï¼Œé›†ä¸­ä¼˜åŒ–åŠªåŠ›äºå…³é”®çš„æˆ˜ç•¥è§„åˆ’ä»¤ç‰Œã€‚</li>
<li>HICRAç®—æ³•æ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ï¼Œè¡¨æ˜è§£å†³æˆ˜ç•¥ç“¶é¢ˆæ˜¯è§£é”é«˜çº§æ¨ç†çš„å…³é”®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.03646">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.03646v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.03646v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.03646v2/page_4_0.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="DCPO-Dynamic-Clipping-Policy-Optimization"><a href="#DCPO-Dynamic-Clipping-Policy-Optimization" class="headerlink" title="DCPO: Dynamic Clipping Policy Optimization"></a>DCPO: Dynamic Clipping Policy Optimization</h2><p><strong>Authors:Shihui Yang, Chengfeng Dou, Peidong Guo, Kai Lu, Qiang Ju, Fei Deng, Rihui Xin</strong></p>
<p>Reinforcement Learning from Verifiable Rewards (RLVR) has emerged as a promising framework for enhancing the reasoning capabilities of large language models. However, existing approaches such as GRPO often suffer from zero gradients. This problem arises primarily due to fixed clipping bounds for token-level probability ratios and the standardization of identical rewards, which can lead to ineffective gradient updates and underutilization of generated responses. In this work, we propose Dynamic Clipping Policy Optimization(DCPO), which introduces a dynamic clipping strategy that adaptively adjusts clipping bounds based on token-specific prior probabilities to enhance token-level exploration, and a smooth advantage standardization technique that standardizes rewards across cumulative training steps to improve the response-level effective utilization of generated responses. DCPO achieved state-of-the-art performance on four benchmarks based on four different models. In particular, DCPO achieved an Avg@1 of 46.7 under greedy decoding and an Avg@32 of 38.8 under 32 times sampling on the AIME24 benchmark, surpassing DAPO (36.7&#x2F;31.6), GRPO (36.7&#x2F;32.1) and GSPO (40.0&#x2F;34.9) on the Qwen2.5-Math-7B model. On the AIME25 benchmark based on Qwen2.5-14B, DCPO achieves a performance of (23.3&#x2F;19.0), surpassing GRPO (13.3&#x2F;10.5), DAPO (20.0&#x2F;15.3) and GSPO (16.7&#x2F;9.9). Furthermore, DCPO achieved an average 28% improvement in the nonzero advantage over GRPO in four models, doubled the training efficiency over DAPO, and significantly reduced the token clipping ratio by an order of magnitude compared to both GRPO and DAPO, while achieving superior performance. These results highlight DCPOâ€™s effectiveness in leveraging generated data more efficiently for reinforcement learning in large language models. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ä»å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰å·²ç»æˆä¸ºæé«˜å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„æœ‰å‰é€”çš„æ¡†æ¶ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ï¼ˆå¦‚GRPOï¼‰ç»å¸¸é¢ä¸´é›¶æ¢¯åº¦çš„é—®é¢˜ã€‚è¿™ä¸ªé—®é¢˜ä¸»è¦æ˜¯ç”±äºå›ºå®šè£å‰ªè¾¹ç•Œçš„ä»¤ç‰Œçº§åˆ«æ¦‚ç‡æ¯”ç‡å’Œå¯¹ç›¸åŒå¥–åŠ±çš„æ ‡å‡†åŒ–ï¼Œè¿™å¯èƒ½å¯¼è‡´æ¢¯åº¦æ›´æ–°æ— æ•ˆå’Œç”Ÿæˆçš„å“åº”åˆ©ç”¨ä¸è¶³ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†åŠ¨æ€è£å‰ªç­–ç•¥ä¼˜åŒ–ï¼ˆDCPOï¼‰ï¼Œå®ƒå¼•å…¥äº†ä¸€ç§åŠ¨æ€è£å‰ªç­–ç•¥ï¼Œè¯¥ç­–ç•¥æ ¹æ®ä»¤ç‰Œç‰¹å®šå…ˆéªŒæ¦‚ç‡è‡ªé€‚åº”åœ°è°ƒæ•´è£å‰ªè¾¹ç•Œï¼Œä»¥å¢å¼ºä»¤ç‰Œçº§åˆ«çš„æ¢ç´¢ï¼Œä»¥åŠä¸€ç§å¹³æ»‘ä¼˜åŠ¿æ ‡å‡†åŒ–æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯ç”¨äºæ ‡å‡†åŒ–ç´¯ç§¯è®­ç»ƒæ­¥éª¤ä¸­çš„å¥–åŠ±ï¼Œä»¥æé«˜ç”Ÿæˆå“åº”çš„å“åº”çº§åˆ«æœ‰æ•ˆåˆ©ç”¨ã€‚DCPOåœ¨åŸºäºå››ä¸ªä¸åŒæ¨¡å‹çš„å››ä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æœ€æ–°æ€§èƒ½ã€‚ç‰¹åˆ«æ˜¯åœ¨AIME24åŸºå‡†æµ‹è¯•ä¸Šï¼ŒDCPOåœ¨è´ªå©ªè§£ç ä¸‹è¾¾åˆ°äº†46.7çš„å¹³å‡å€¼@1ï¼Œåœ¨32æ¬¡é‡‡æ ·ä¸‹è¾¾åˆ°äº†38.8çš„å¹³å‡å€¼@32ï¼Œè¶…è¿‡äº†DAPOï¼ˆ36.7&#x2F;31.6ï¼‰ã€GRPOï¼ˆ36.7&#x2F;32.1ï¼‰å’ŒGSPOï¼ˆ40.0&#x2F;34.9ï¼‰åœ¨Qwen2.5-Math-7Bæ¨¡å‹ä¸Šçš„è¡¨ç°ã€‚åœ¨åŸºäºQwen2.5-14Bçš„AIME25åŸºå‡†æµ‹è¯•ä¸Šï¼ŒDCPOçš„æ€§èƒ½è¾¾åˆ°ï¼ˆ23.3&#x2F;19.0ï¼‰ï¼Œè¶…è¿‡äº†GRPOï¼ˆ13.3&#x2F;10.5ï¼‰ã€DAPOï¼ˆ20.0&#x2F;15.3ï¼‰å’ŒGSPOï¼ˆ16.7&#x2F;9.9ï¼‰ã€‚æ­¤å¤–ï¼ŒDCPOåœ¨å››ä¸ªæ¨¡å‹ä¸­çš„éé›¶ä¼˜åŠ¿å¹³å‡æé«˜äº†28%ï¼Œç›¸å¯¹äºDAPOè®­ç»ƒæ•ˆç‡ç¿»å€ï¼Œä¸GRPOå’ŒDAPOç›¸æ¯”ï¼Œä»¤ç‰Œè£å‰ªæ¯”ä¾‹é™ä½äº†ä¸€ä¸ªæ•°é‡çº§ï¼ŒåŒæ—¶å®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚è¿™äº›ç»“æœå‡¸æ˜¾äº†DCPOåœ¨åˆ©ç”¨ç”Ÿæˆæ•°æ®å¯¹å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå¼ºåŒ–å­¦ä¹ æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.02333v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¼ºåŒ–å­¦ä¹ ä»å¯éªŒè¯çš„å¥–åŠ±ï¼ˆRLVRï¼‰åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›æ–¹é¢å±•ç°å‡ºå·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¦‚GRPOå¸¸å¸¸é¢ä¸´é›¶æ¢¯åº¦é—®é¢˜ã€‚è¯¥é—®é¢˜ä¸»è¦æºäºå›ºå®šå‰ªè¾‘è¾¹ç•Œçš„ä»¤ç‰Œçº§åˆ«æ¦‚ç‡æ¯”ç‡ä»¥åŠç›¸åŒå¥–åŠ±çš„æ ‡å‡†åŒ–ï¼Œè¿™å¯èƒ½å¯¼è‡´æ¢¯åº¦æ›´æ–°æ— æ•ˆå’Œç”Ÿæˆå“åº”çš„åˆ©ç”¨ç‡ä½ä¸‹ã€‚åœ¨æ­¤å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºåŠ¨æ€å‰ªè¾‘ç­–ç•¥ä¼˜åŒ–ï¼ˆDCPOï¼‰ï¼Œå®ƒå¼•å…¥äº†ä¸€ç§åŠ¨æ€å‰ªè¾‘ç­–ç•¥ï¼Œè¯¥ç­–ç•¥å¯ä»¥åŸºäºä»¤ç‰Œç‰¹å®šå…ˆéªŒæ¦‚ç‡è‡ªé€‚åº”åœ°è°ƒæ•´å‰ªè¾‘è¾¹ç•Œï¼Œä»¥å¢å¼ºä»¤ç‰Œçº§åˆ«çš„æ¢ç´¢ï¼Œä»¥åŠä¸€ç§å¹³æ»‘ä¼˜åŠ¿æ ‡å‡†åŒ–æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯å¯ä»¥åœ¨ç´¯ç§¯çš„è®­ç»ƒæ­¥éª¤ä¸­æ ‡å‡†åŒ–å¥–åŠ±ï¼Œä»¥æé«˜ç”Ÿæˆå“åº”çš„å“åº”çº§åˆ«æœ‰æ•ˆåˆ©ç”¨ç‡ã€‚DCPOåœ¨åŸºäºå››ä¸ªä¸åŒæ¨¡å‹çš„å››ä¸ªåŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ç‰¹åˆ«æ˜¯åœ¨AIME24åŸºå‡†æµ‹è¯•ä¸Šï¼ŒDCPOåœ¨è´ªå©ªè§£ç ä¸‹è¾¾åˆ°Avg@1çš„46.7ï¼Œåœ¨32æ¬¡é‡‡æ ·ä¸‹è¾¾åˆ°Avg@32çš„38.8ï¼Œè¶…è¶Šäº†DAPOï¼ˆ36.7&#x2F;31.6ï¼‰ã€GRPOï¼ˆ36.7&#x2F;32.1ï¼‰å’ŒGSPOï¼ˆ40.0&#x2F;34.9ï¼‰åœ¨Qwen2.5-Math-7Bæ¨¡å‹ä¸Šçš„è¡¨ç°ã€‚åœ¨åŸºäºQwen2.5-14Bçš„AIME25åŸºå‡†æµ‹è¯•ä¸Šï¼ŒDCPOè¾¾åˆ°äº†ï¼ˆ23.3&#x2F;19.0ï¼‰çš„æ€§èƒ½ï¼Œè¶…è¶Šäº†GRPOï¼ˆ13.3&#x2F;10.5ï¼‰ã€DAPOï¼ˆ20.0&#x2F;15.3ï¼‰å’ŒGSPOï¼ˆ16.7&#x2F;9.9ï¼‰ã€‚æ­¤å¤–ï¼ŒDCPOåœ¨å››ä¸ªæ¨¡å‹ä¸­çš„éé›¶ä¼˜åŠ¿å¹³å‡æé«˜äº†28%ï¼Œæ¯”DAPOçš„è®­ç»ƒæ•ˆç‡æé«˜äº†ä¸€å€ï¼Œä¸GRPOå’ŒDAPOç›¸æ¯”ï¼Œä»¤ç‰Œå‰ªè¾‘æ¯”ä¾‹é™ä½äº†ä¸€ä¸ªæ•°é‡çº§ï¼ŒåŒæ—¶å®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚è¿™äº›ç»“æœçªå‡ºäº†DCPOåœ¨åˆ©ç”¨ç”Ÿæˆæ•°æ®æ–¹é¢æ›´æœ‰æ•ˆåœ°è¿›è¡Œå¤§å‹è¯­è¨€æ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>RLVRå·²æ˜¾ç¤ºå‡ºåœ¨å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›æ–¹é¢çš„æ½œåŠ›ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¦‚GRPOé¢ä¸´é›¶æ¢¯åº¦é—®é¢˜ï¼Œè¿™é™åˆ¶äº†å…¶æ€§èƒ½ã€‚</li>
<li>DCPOé€šè¿‡åŠ¨æ€è°ƒæ•´å‰ªè¾‘ç­–ç•¥å’Œä¼˜åŠ¿æ ‡å‡†åŒ–æŠ€æœ¯è§£å†³äº†è¿™ä¸ªé—®é¢˜ã€‚</li>
<li>DCPOåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†å“è¶Šæ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨AIME24å’ŒAIME25ä¸Šçš„è¡¨ç°ä»¤äººå°è±¡æ·±åˆ»ã€‚</li>
<li>DCPOæ˜¾è‘—æé«˜äº†éé›¶ä¼˜åŠ¿çš„è®­ç»ƒæ•ˆç‡ï¼Œå¹¶é™ä½äº†ä»¤ç‰Œå‰ªè¾‘æ¯”ä¾‹ã€‚</li>
<li>è¿™äº›ç»“æœè¯æ˜DCPOæ›´æœ‰æ•ˆåœ°åˆ©ç”¨ç”Ÿæˆæ•°æ®è¿›è¡Œå¼ºåŒ–å­¦ä¹ ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.02333">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.02333v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.02333v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.02333v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Probe-Rewrite-Evaluate-A-Workflow-for-Reliable-Benchmarks-and-Quantifying-Evaluation-Awareness"><a href="#Probe-Rewrite-Evaluate-A-Workflow-for-Reliable-Benchmarks-and-Quantifying-Evaluation-Awareness" class="headerlink" title="Probe-Rewrite-Evaluate: A Workflow for Reliable Benchmarks and   Quantifying Evaluation Awareness"></a>Probe-Rewrite-Evaluate: A Workflow for Reliable Benchmarks and   Quantifying Evaluation Awareness</h2><p><strong>Authors:Lang Xiong, Nishant Bhargava, Jeremy Chang, Jianhang Hong, Haihao Liu, Vasu Sharma, Kevin Zhu</strong></p>
<p>Large Language Models (LLMs) often exhibit significant behavioral shifts when they perceive a change from a real-world deployment context to a controlled evaluation setting, a phenomenon known as â€œevaluation awareness.â€ This discrepancy poses a critical challenge for AI alignment, as benchmark performance may not accurately reflect a modelâ€™s true safety and honesty. In this work, we systematically quantify these behavioral changes by manipulating the perceived context of prompts. We introduce a methodology that uses a linear probe to score prompts on a continuous scale from â€œtest-likeâ€ to â€œdeploy-likeâ€ and leverage an LLM rewriting strategy to shift these prompts towards a more natural, deployment-style context while preserving the original task. Using this method, we achieved a 30% increase in the average probe score across a strategic role-playing dataset after rewriting. Evaluating a suite of state-of-the-art models on these original and rewritten prompts, we find that rewritten â€œdeploy-likeâ€ prompts induce a significant and consistent shift in behavior. Across all models, we observed an average increase in honest responses of 5.26% and a corresponding average decrease in deceptive responses of 12.40%. Furthermore, refusal rates increased by an average of 6.38%, indicating heightened safety compliance. Our findings demonstrate that evaluation awareness is a quantifiable and manipulable factor that directly influences LLM behavior, revealing that models are more prone to unsafe or deceptive outputs in perceived test environments. This underscores the urgent need for more realistic evaluation frameworks to accurately gauge true model alignment before deployment. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ„ŸçŸ¥åˆ°ä»ç°å®ä¸–ç•Œéƒ¨ç½²ç¯å¢ƒåˆ°å—æ§è¯„ä¼°ç¯å¢ƒçš„å˜æ›´æ—¶ï¼Œé€šå¸¸ä¼šè¡¨ç°å‡ºæ˜¾è‘—çš„è¡Œä¸ºå˜åŒ–ï¼Œè¿™ä¸€ç°è±¡è¢«ç§°ä¸ºâ€œè¯„ä¼°æ„è¯†â€ã€‚è¿™ç§å·®å¼‚å¯¹äººå·¥æ™ºèƒ½å¯¹é½æå‡ºäº†å…³é”®æŒ‘æˆ˜ï¼Œå› ä¸ºåŸºå‡†æ€§èƒ½å¯èƒ½æ— æ³•å‡†ç¡®åæ˜ æ¨¡å‹çš„çœŸå®å®‰å…¨æ€§å’Œè¯šå®åº¦ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡æ“ä½œæç¤ºçš„æ„ŸçŸ¥ä¸Šä¸‹æ–‡æ¥ç³»ç»Ÿåœ°é‡åŒ–è¿™äº›è¡Œä¸ºå˜åŒ–ã€‚æˆ‘ä»¬ä»‹ç»äº†ä¸€ç§ä½¿ç”¨çº¿æ€§æ¢é’ˆå¯¹æç¤ºè¿›è¡Œä»â€œæµ‹è¯•å‹â€åˆ°â€œéƒ¨ç½²å‹â€çš„æŒç»­è¯„åˆ†çš„æ–¹æ³•ï¼Œå¹¶åˆ©ç”¨LLMé‡å†™ç­–ç•¥æ¥å°†è¿™äº›æç¤ºè½¬å‘æ›´è‡ªç„¶ã€æ›´è´´è¿‘éƒ¨ç½²é£æ ¼çš„ä¸Šä¸‹æ–‡ï¼ŒåŒæ—¶ä¿ç•™åŸå§‹ä»»åŠ¡ã€‚ä½¿ç”¨è¿™ç§æ–¹æ³•ï¼Œåœ¨æˆ˜ç•¥è§’è‰²æ‰®æ¼”æ•°æ®é›†ä¸Šé‡å†™åï¼Œæ¢é’ˆå¹³å‡å¾—åˆ†æé«˜äº†30%ã€‚åœ¨åŸå§‹æç¤ºå’Œé‡å†™æç¤ºä¸Šè¯„ä¼°ä¸€ç³»åˆ—æœ€å…ˆè¿›çš„æ¨¡å‹ï¼Œæˆ‘ä»¬å‘ç°é‡å†™çš„â€œéƒ¨ç½²å‹â€æç¤ºå¼•å‘äº†æ˜¾è‘—ä¸”ä¸€è‡´çš„è¡Œä¸ºå˜åŒ–ã€‚åœ¨æ‰€æœ‰æ¨¡å‹ä¸­ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°è¯šå®å›åº”çš„å¹³å‡å¢åŠ äº†5.26%ï¼Œç›¸åº”çš„æ¬ºéª—å›åº”å¹³å‡å‡å°‘äº†1.4%ã€‚æ­¤å¤–ï¼Œæ‹’ç»ç‡å¹³å‡å¢åŠ äº†6.38%ï¼Œè¡¨æ˜å®‰å…¨åˆè§„æ€§æœ‰æ‰€æé«˜ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè¯„ä¼°æ„è¯†æ˜¯ä¸€ä¸ªå¯é‡åŒ–çš„ã€å¯æ“æ§çš„å› ç´ ï¼Œç›´æ¥å½±å“LLMçš„è¡Œä¸ºï¼Œè¡¨æ˜æ¨¡å‹åœ¨æ„ŸçŸ¥çš„æµ‹è¯•ç¯å¢ƒä¸­æ›´å®¹æ˜“äº§ç”Ÿä¸å®‰å…¨æˆ–æ¬ºéª—æ€§çš„è¾“å‡ºã€‚è¿™çªæ˜¾äº†åœ¨éƒ¨ç½²å‰å¯¹çœŸå®æ¨¡å‹å¯¹é½è¿›è¡Œå‡†ç¡®è¯„ä¼°çš„æ›´ç°å®çš„è¯„ä¼°æ¡†æ¶çš„è¿«åˆ‡éœ€æ±‚ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.00591v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ„ŸçŸ¥ä»å®é™…éƒ¨ç½²ç¯å¢ƒè½¬ç§»åˆ°å—æ§è¯„ä¼°ç¯å¢ƒæ—¶çš„è¡Œä¸ºå˜åŒ–ï¼Œç§°ä¸ºâ€œè¯„ä¼°æ„è¯†â€ç°è±¡ã€‚è¿™ç§è¡Œä¸ºå·®å¼‚ç»™AIå¯¹é½å¸¦æ¥äº†æŒ‘æˆ˜ï¼Œå› ä¸ºåŸºå‡†æµ‹è¯•æ€§èƒ½å¯èƒ½æ— æ³•å‡†ç¡®åæ˜ æ¨¡å‹çš„çœŸå®å®‰å…¨æ€§å’Œè¯šå®åº¦ã€‚ç ”ç©¶äººå‘˜é€šè¿‡æ“çºµæç¤ºçš„æ„ŸçŸ¥ä¸Šä¸‹æ–‡æ¥ç³»ç»Ÿåœ°é‡åŒ–è¿™äº›è¡Œä¸ºå˜åŒ–ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§ä½¿ç”¨çº¿æ€§æ¢é’ˆå¯¹æç¤ºè¿›è¡Œè¿ç»­è¯„åˆ†çš„æ–¹æ³•ã€‚é€šè¿‡é‡å†™æç¤ºï¼Œä½¿æç¤ºæ›´è´´è¿‘è‡ªç„¶ã€æ›´è´´è¿‘éƒ¨ç½²é£æ ¼çš„ä¸Šä¸‹æ–‡ï¼ŒåŒæ—¶ä¿ç•™åŸå§‹ä»»åŠ¡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œé‡å†™åçš„â€œéƒ¨ç½²å‹â€æç¤ºå¼•å‘äº†æ¨¡å‹è¡Œä¸ºçš„æ˜¾è‘—ä¸”ä¸€è‡´çš„å˜åŒ–ï¼Œè¯šå®å›åº”å¹³å‡å¢åŠ 5.26%ï¼Œæ¬ºéª—æ€§å›åº”å¹³å‡å‡å°‘12.4%ï¼Œæ‹’ç»ç‡å¹³å‡æé«˜6.38%ï¼Œè¡¨æ˜æ¨¡å‹æ›´å®‰å…¨åˆè§„ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè¯„ä¼°æ„è¯†æ˜¯ä¸€ä¸ªå¯é‡åŒ–ä¸”å¯æ“æ§çš„å› ç´ ï¼Œç›´æ¥å½±å“LLMçš„è¡Œä¸ºï¼Œå‡¸æ˜¾äº†åœ¨éƒ¨ç½²å‰å¼€å‘æ›´ç°å®çš„è¯„ä¼°æ¡†æ¶æ¥å‡†ç¡®è¡¡é‡æ¨¡å‹å¯¹é½çš„ç´§è¿«æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è¯„ä¼°ç¯å¢ƒå’Œå®é™…éƒ¨ç½²ç¯å¢ƒä¸­å±•ç°å‡ºä¸åŒçš„è¡Œä¸ºç‰¹å¾ã€‚</li>
<li>â€œè¯„ä¼°æ„è¯†â€ç°è±¡å¯¹AIå¯¹é½æ„æˆæŒ‘æˆ˜ï¼Œå› ä¸ºåŸºå‡†æµ‹è¯•æ€§èƒ½å¯èƒ½æ— æ³•çœŸå®åæ˜ æ¨¡å‹çš„å®‰å…¨æ€§å’Œè¯šå®åº¦ã€‚</li>
<li>é€šè¿‡æ“çºµæç¤ºçš„æ„ŸçŸ¥ä¸Šä¸‹æ–‡ï¼Œå¯ä»¥ç³»ç»Ÿåœ°é‡åŒ–LLMçš„è¡Œä¸ºå˜åŒ–ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§ä½¿ç”¨çº¿æ€§æ¢é’ˆè¯„åˆ†æç¤ºçš„æ–¹æ³•ï¼Œèƒ½æœ‰æ•ˆå°†æç¤ºä»â€œæµ‹è¯•å‹â€å‘â€œéƒ¨ç½²å‹â€è½¬å˜ã€‚</li>
<li>é‡å†™åçš„â€œéƒ¨ç½²å‹â€æç¤ºæ˜¾è‘—æ”¹å˜äº†æ¨¡å‹çš„è¡Œä¸ºï¼ŒåŒ…æ‹¬æé«˜è¯šå®å›åº”ã€é™ä½æ¬ºéª—æ€§å›åº”å’Œæé«˜æ‹’ç»ç‡ã€‚</li>
<li>è¯„ä¼°æ„è¯†æ˜¯ä¸€ä¸ªå¯é‡åŒ–ä¸”å¯æ“æ§çš„å› ç´ ï¼Œç›´æ¥å½±å“LLMçš„è¡Œä¸ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.00591">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.00591v4/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.00591v4/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.00591v4/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.00591v4/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.00591v4/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.00591v4/page_5_1.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="AI-SearchPlanner-Modular-Agentic-Search-via-Pareto-Optimal-Multi-Objective-Reinforcement-Learning"><a href="#AI-SearchPlanner-Modular-Agentic-Search-via-Pareto-Optimal-Multi-Objective-Reinforcement-Learning" class="headerlink" title="AI-SearchPlanner: Modular Agentic Search via Pareto-Optimal   Multi-Objective Reinforcement Learning"></a>AI-SearchPlanner: Modular Agentic Search via Pareto-Optimal   Multi-Objective Reinforcement Learning</h2><p><strong>Authors:Lang Mei, Zhihan Yang, Chong Chen</strong></p>
<p>Recent studies have explored integrating Large Language Models (LLMs) with search engines to leverage both the LLMsâ€™ internal pre-trained knowledge and external information. Specially, reinforcement learning (RL) has emerged as a promising paradigm for enhancing LLM reasoning through multi-turn interactions with search engines. However, existing RL-based search agents rely on a single LLM to handle both search planning and question-answering (QA) tasks in an end-to-end manner, which limits their ability to optimize both capabilities simultaneously. In practice, sophisticated AI search systems often employ a large, frozen LLM (e.g., GPT-4, DeepSeek-R1) to ensure high-quality QA. Thus, a more effective and efficient approach is to utilize a small, trainable LLM dedicated to search planning. In this paper, we propose \textbf{AI-SearchPlanner}, a novel reinforcement learning framework designed to enhance the performance of frozen QA models by focusing on search planning. Specifically, our approach introduces three key innovations: 1) Decoupling the Architecture of the Search Planner and Generator, 2) Dual-Reward Alignment for Search Planning, and 3) Pareto Optimization of Planning Utility and Cost, to achieve the objectives. Extensive experiments on real-world datasets demonstrate that AI SearchPlanner outperforms existing RL-based search agents in both effectiveness and efficiency, while exhibiting strong generalization capabilities across diverse frozen QA models and data domains. </p>
<blockquote>
<p>æœ€è¿‘çš„ç ”ç©¶æ¢è®¨äº†å¦‚ä½•å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸æœç´¢å¼•æ“ç›¸ç»“åˆï¼Œä»¥åˆ©ç”¨LLMçš„å†…éƒ¨é¢„è®­ç»ƒçŸ¥è¯†å’Œå¤–éƒ¨ä¿¡æ¯ã€‚ç‰¹åˆ«æ˜¯ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å·²ç»æˆä¸ºä¸€ç§é€šè¿‡å¤šè½®ä¸æœç´¢å¼•æ“äº’åŠ¨å¢å¼ºLLMæ¨ç†èƒ½åŠ›çš„æœ‰å‰é€”çš„æ¨¡å¼ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºäºRLçš„æœç´¢ä»£ç†ä¾èµ–äºå•ä¸ªLLMä»¥ç«¯åˆ°ç«¯çš„æ–¹å¼åŒæ—¶å¤„ç†æœç´¢è§„åˆ’å’Œé—®ç­”ï¼ˆQAï¼‰ä»»åŠ¡ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åŒæ—¶ä¼˜åŒ–è¿™ä¸¤ç§èƒ½åŠ›çš„èƒ½åŠ›ã€‚åœ¨å®è·µä¸­ï¼Œå¤æ‚çš„äººå·¥æ™ºèƒ½æœç´¢ç³»ç»Ÿé€šå¸¸ä¼šä½¿ç”¨å¤§å‹ã€å›ºå®šçš„LLMï¼ˆå¦‚GPT-4ã€DeepSeek-R1ï¼‰æ¥ç¡®ä¿é«˜è´¨é‡çš„é—®ç­”ã€‚å› æ­¤ï¼Œæ›´æœ‰æ•ˆå’Œé«˜æ•ˆçš„æ–¹æ³•æ˜¯ä½¿ç”¨ä¸€ä¸ªå°å‹ã€å¯è®­ç»ƒçš„LLMä¸“é—¨ç”¨äºæœç´¢è§„åˆ’ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†\textbf{AI-SearchPlanner}ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ä¸“æ³¨äºæœç´¢è§„åˆ’æ¥æé«˜å›ºå®šé—®ç­”æ¨¡å‹çš„æ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†ä¸‰ä¸ªå…³é”®åˆ›æ–°ç‚¹ï¼š1ï¼‰è§£è€¦æœç´¢è§„åˆ’å™¨å’Œç”Ÿæˆå™¨çš„æ¶æ„ï¼Œ2ï¼‰æœç´¢è§„åˆ’çš„åŒå¥–åŠ±å¯¹é½ï¼Œä»¥åŠ3ï¼‰è§„åˆ’å’Œæˆæœ¬çš„å¸•ç´¯æ‰˜ä¼˜åŒ–ï¼Œä»¥å®ç°ç›®æ ‡ã€‚åœ¨çœŸå®æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒAI SearchPlanneråœ¨æœ‰æ•ˆæ€§å’Œæ•ˆç‡æ–¹é¢ä¼˜äºç°æœ‰çš„åŸºäºRLçš„æœç´¢ä»£ç†ï¼ŒåŒæ—¶åœ¨ä¸åŒçš„å›ºå®šé—®ç­”æ¨¡å‹å’Œæ•°æ®é¢†åŸŸè¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.20368v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸æœç´¢å¼•æ“çš„ç»“åˆæ˜¯å½“å‰ç ”ç©¶çš„çƒ­ç‚¹ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶AI-SearchPlannerï¼Œæ—¨åœ¨é€šè¿‡ä¸“æ³¨äºæœç´¢è§„åˆ’æ¥æé«˜å†»ç»“çš„é—®ç­”æ¨¡å‹æ€§èƒ½ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†ä¸‰ä¸ªå…³é”®åˆ›æ–°ç‚¹ï¼šæœç´¢è§„åˆ’å™¨å’Œç”Ÿæˆå™¨çš„æ¶æ„è§£è€¦ã€æœç´¢è§„åˆ’çš„åŒå¥–åŠ±å¯¹é½ä»¥åŠè§„åˆ’æ•ˆç”¨å’Œæˆæœ¬çš„å¸•ç´¯æ‰˜ä¼˜åŒ–ã€‚å®éªŒè¯æ˜ï¼ŒAI SearchPlanneråœ¨æœ‰æ•ˆæ€§å’Œæ•ˆç‡ä¸Šè¶…è¶Šäº†ç°æœ‰çš„åŸºäºRLçš„æœç´¢ä»£ç†ï¼Œå¹¶åœ¨ä¸åŒçš„å†»ç»“é—®ç­”æ¨¡å‹å’Œæ•°æ®åŸŸä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸æœç´¢å¼•æ“çš„ç»“åˆç ”ç©¶æ—¨åœ¨åˆ©ç”¨LLMçš„å†…éƒ¨é¢„è®­ç»ƒçŸ¥è¯†å’Œå¤–éƒ¨ä¿¡æ¯ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å·²æˆä¸ºå¢å¼ºLLMæ¨ç†çš„ä¸€ä¸ªæœ‰å‰é€”çš„èŒƒå¼ï¼Œé€šè¿‡ä¸æœç´¢å¼•æ“çš„å¤šè½®äº¤äº’ã€‚</li>
<li>ç°æœ‰åŸºäºRLçš„æœç´¢ä»£ç†ä¾èµ–äºå•ä¸€LLMåŒæ—¶å¤„ç†æœç´¢è§„åˆ’å’Œé—®ç­”ä»»åŠ¡ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬çš„ä¼˜åŒ–èƒ½åŠ›ã€‚</li>
<li>AI-SearchPlanneræ¡†æ¶é€šè¿‡ä¸“æ³¨äºæœç´¢è§„åˆ’æ¥æé«˜å†»ç»“é—®ç­”æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>AI-SearchPlanneræ¡†æ¶å¼•å…¥äº†ä¸‰ä¸ªå…³é”®åˆ›æ–°ç‚¹ï¼šæ¶æ„è§£è€¦ã€åŒå¥–åŠ±å¯¹é½å’Œå¸•ç´¯æ‰˜ä¼˜åŒ–ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.20368">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2508.20368v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2508.20368v3/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2508.20368v3/page_5_0.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="ReST-RL-Achieving-Accurate-Code-Reasoning-of-LLMs-with-Optimized-Self-Training-and-Decoding"><a href="#ReST-RL-Achieving-Accurate-Code-Reasoning-of-LLMs-with-Optimized-Self-Training-and-Decoding" class="headerlink" title="ReST-RL: Achieving Accurate Code Reasoning of LLMs with Optimized   Self-Training and Decoding"></a>ReST-RL: Achieving Accurate Code Reasoning of LLMs with Optimized   Self-Training and Decoding</h2><p><strong>Authors:Sining Zhoubian, Dan Zhang, Jie Tang</strong></p>
<p>With respect to improving the reasoning accuracy of LLMs, the representative reinforcement learning (RL) method GRPO faces failure due to insignificant reward variance, while verification methods based on process reward models (PRMs) suffer from difficulties with training data acquisition and verification effectiveness. To tackle these problems, this paper introduces ReST-RL, a unified LLM RL paradigm that significantly improves LLMâ€™s code reasoning ability by combining an improved GRPO algorithm with a meticulously designed test time decoding method assisted by a value model (VM). As the first stage of policy reinforcement, ReST-GRPO adopts an optimized ReST algorithm to filter and assemble high-value training data, increasing the reward variance of GRPO sampling, thus improving the effectiveness and efficiency of training. After the basic reasoning ability of LLM policy has been improved, we further propose a test time decoding optimization method called VM-MCTS. Through Monte-Carlo Tree Search (MCTS), we collect accurate value targets with no annotation required, on which VM training is based. When decoding, the VM is deployed by an adapted MCTS algorithm to provide precise process signals as well as verification scores, assisting the LLM policy to achieve high reasoning accuracy. We conduct extensive experiments on coding problems to verify the validity of the proposed RL paradigm. Upon comparison, our approach significantly outperforms other reinforcement training baselines (e.g., naive GRPO and ReST-DPO), as well as decoding and verification baselines (e.g., PRM-BoN and ORM-MCTS) on well-known coding benchmarks of various levels (e.g., APPS, BigCodeBench, and HumanEval), indicating its power to strengthen the reasoning ability of LLM policies. Codes for our project can be found at <a target="_blank" rel="noopener" href="https://github.com/THUDM/ReST-RL">https://github.com/THUDM/ReST-RL</a>. </p>
<blockquote>
<p>å…³äºæé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†å‡†ç¡®æ€§ï¼Œä»£è¡¨æ€§çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ–¹æ³•GRPOç”±äºå¥–åŠ±æ–¹å·®ä¸æ˜¾è‘—è€Œå¤±è´¥ï¼Œè€ŒåŸºäºè¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰çš„éªŒè¯æ–¹æ³•åˆ™é¢ä¸´è®­ç»ƒæ•°æ®è·å–å’ŒéªŒè¯æœ‰æ•ˆæ€§æ–¹é¢çš„å›°éš¾ã€‚é’ˆå¯¹è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡å¼•å…¥äº†ReST-RLï¼Œè¿™æ˜¯ä¸€ç§ç»Ÿä¸€çš„LLM RLèŒƒå¼ï¼Œé€šè¿‡ç»“åˆæ”¹è¿›çš„GRPOç®—æ³•å’Œç²¾å¿ƒè®¾è®¡çš„æµ‹è¯•æ—¶é—´è§£ç æ–¹æ³•ï¼ˆå€ŸåŠ©å€¼æ¨¡å‹ï¼ˆVMï¼‰è¾…åŠ©ï¼‰ï¼Œæ˜¾è‘—æé«˜äº†LLMçš„ä»£ç æ¨ç†èƒ½åŠ›ã€‚ä½œä¸ºæ”¿ç­–å¼ºåŒ–çš„ç¬¬ä¸€é˜¶æ®µï¼ŒReST-GRPOé‡‡ç”¨ä¼˜åŒ–çš„ReSTç®—æ³•æ¥è¿‡æ»¤å’Œç»„è£…é«˜ä»·å€¼è®­ç»ƒæ•°æ®ï¼Œå¢åŠ GRPOé‡‡æ ·çš„å¥–åŠ±æ–¹å·®ï¼Œä»è€Œæé«˜è®­ç»ƒçš„æœ‰æ•ˆæ€§å’Œæ•ˆç‡ã€‚åœ¨æé«˜äº†LLMæ”¿ç­–çš„åŸºæœ¬æ¨ç†èƒ½åŠ›åï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†ä¸€ç§åä¸ºVM-MCTSçš„æµ‹è¯•æ—¶é—´è§£ç ä¼˜åŒ–æ–¹æ³•ã€‚é€šè¿‡è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨æ— éœ€æ³¨é‡Šçš„æƒ…å†µä¸‹æ”¶é›†å‡†ç¡®çš„ä»·å€¼ç›®æ ‡ï¼ŒVMè®­ç»ƒå°±åŸºäºæ­¤è¿›è¡Œã€‚åœ¨è§£ç æ—¶ï¼ŒVMé€šè¿‡é€‚åº”çš„MCTSç®—æ³•æä¾›ç²¾ç¡®çš„è¿‡ç¨‹ä¿¡å·å’ŒéªŒè¯åˆ†æ•°ï¼Œå¸®åŠ©LLMç­–ç•¥å®ç°é«˜æ¨ç†å‡†ç¡®æ€§ã€‚æˆ‘ä»¬åœ¨ç¼–ç¨‹é—®é¢˜ä¸Šè¿›è¡Œäº†å¤§é‡çš„å®éªŒï¼Œä»¥éªŒè¯æ‰€æå‡ºçš„RLèŒƒå¼çš„æœ‰æ•ˆæ€§ã€‚ç»æ¯”è¾ƒï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä¼˜äºå…¶ä»–å¼ºåŒ–è®­ç»ƒåŸºçº¿ï¼ˆä¾‹å¦‚ï¼Œç®€å•çš„GRPOå’ŒReST-DPOï¼‰ï¼Œä»¥åŠè§£ç å’ŒéªŒè¯åŸºçº¿ï¼ˆä¾‹å¦‚ï¼ŒPRM-BoNå’ŒORM-MCTSï¼‰åœ¨è‘—åçš„å„çº§ç¼–ç¨‹åŸºå‡†æµ‹è¯•ï¼ˆä¾‹å¦‚ï¼ŒAPPSã€BigCodeBenchå’ŒHumanEvalï¼‰ä¸Šçš„è¡¨ç°ï¼Œè¯æ˜å…¶å¢å¼ºLLMç­–ç•¥æ¨ç†èƒ½åŠ›çš„å®åŠ›ã€‚æˆ‘ä»¬çš„é¡¹ç›®ä»£ç å¯åœ¨[<a target="_blank" rel="noopener" href="https://github.com/THUDM/ReST-RL%E6%89%BE%E5%88%B0%E3%80%82]">https://github.com/THUDM/ReST-RLæ‰¾åˆ°ã€‚]</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.19576v2">PDF</a> 21 pages, 4 figures</p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ”¹è¿›å‹GRPOç®—æ³•å’Œå€¼æ¨¡å‹è¾…åŠ©çš„æµ‹è¯•æ—¶é—´è§£ç æ–¹æ³•ï¼Œæœ¬æ–‡æå‡ºäº†ReST-RLç»Ÿä¸€LLM RLèŒƒå¼ï¼Œæ—¨åœ¨æé«˜LLMçš„ä»£ç æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡ä¼˜åŒ–GRPOç®—æ³•å’Œå¼•å…¥VM-MCTSè§£ç ä¼˜åŒ–æ–¹æ³•ï¼Œè¯¥èŒƒå¼æå‡äº†è®­ç»ƒçš„æœ‰æ•ˆæ€§å’Œæ•ˆç‡ï¼Œå¹¶å®ç°äº†é«˜æ¨ç†å‡†ç¡®ç‡çš„LLMç­–ç•¥ã€‚åœ¨ç¼–ç é—®é¢˜ä¸Šçš„å¹¿æ³›å®éªŒéªŒè¯äº†è¯¥RLèŒƒå¼çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ReST-RLæ˜¯ä¸€ç§é’ˆå¯¹LLMçš„å¼ºåŒ–å­¦ä¹ èŒƒå¼ï¼Œæ—¨åœ¨æé«˜ä»£ç æ¨ç†èƒ½åŠ›ã€‚</li>
<li>ReST-RLç»“åˆäº†æ”¹è¿›å‹GRPOç®—æ³•å’Œå€¼æ¨¡å‹ï¼ˆVMï¼‰è¾…åŠ©çš„æµ‹è¯•æ—¶é—´è§£ç æ–¹æ³•ã€‚</li>
<li>é€šè¿‡ä¼˜åŒ–GRPOç®—æ³•ï¼Œæé«˜äº†å¥–åŠ±æ–¹å·®å’Œè®­ç»ƒçš„æœ‰æ•ˆæ€§å’Œæ•ˆç‡ã€‚</li>
<li>VM-MCTSè§£ç ä¼˜åŒ–æ–¹æ³•æä¾›äº†ç²¾ç¡®çš„è¿‡ç¨‹ä¿¡å·å’ŒéªŒè¯åˆ†æ•°ï¼Œè¿›ä¸€æ­¥æå‡äº†LLMç­–ç•¥çš„æ¨ç†å‡†ç¡®æ€§ã€‚</li>
<li>å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒReST-RLåœ¨ç¼–ç é—®é¢˜ä¸Šæ˜¾è‘—ä¼˜äºå…¶ä»–å¼ºåŒ–è®­ç»ƒåŸºå‡†å’Œè§£ç éªŒè¯åŸºå‡†ã€‚</li>
<li>ReST-RLåœ¨å¤šä¸ªçŸ¥åç¼–ç åŸºå‡†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå¦‚APPSã€BigCodeBenchå’ŒHumanEvalã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.19576">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2508.19576v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2508.19576v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2508.19576v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2508.19576v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="MaRVL-QA-A-Benchmark-for-Mathematical-Reasoning-over-Visual-Landscapes"><a href="#MaRVL-QA-A-Benchmark-for-Mathematical-Reasoning-over-Visual-Landscapes" class="headerlink" title="MaRVL-QA: A Benchmark for Mathematical Reasoning over Visual Landscapes"></a>MaRVL-QA: A Benchmark for Mathematical Reasoning over Visual Landscapes</h2><p><strong>Authors:Nilay Pande, Sahiti Yerramilli, Jayant Sravan Tamarapalli, Rynaa Grover</strong></p>
<p>A key frontier for Multimodal Large Language Models (MLLMs) is the ability to perform deep mathematical and spatial reasoning directly from images, moving beyond their established success in semantic description. Mathematical surface plots provide a rigorous testbed for this capability, as they isolate the task of reasoning from the semantic noise common in natural images. To measure progress on this frontier, we introduce MaRVL-QA (Mathematical Reasoning over Visual Landscapes), a new benchmark designed to quantitatively evaluate these core reasoning skills. The benchmark comprises two novel tasks: Topological Counting, identifying and enumerating features like local maxima; and Transformation Recognition, recognizing applied geometric transformations. Generated from a curated library of functions with rigorous ambiguity filtering, our evaluation on MaRVL-QA reveals that even state-of-the-art MLLMs struggle significantly, often resorting to superficial heuristics instead of robust spatial reasoning. MaRVL-QA provides a challenging new tool for the research community to measure progress, expose model limitations, and guide the development of MLLMs with more profound reasoning abilities. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„å…³é”®å‰æ²¿æ˜¯èƒ½å¤Ÿç›´æ¥ä»å›¾åƒä¸­è¿›è¡Œæ·±åº¦æ•°å­¦å’Œç©ºé—´æ¨ç†ï¼Œè¿™è¶…è¶Šäº†å…¶åœ¨è¯­ä¹‰æè¿°æ–¹é¢å·²ç»å–å¾—çš„æˆå°±ã€‚æ•°å­¦æ›²é¢å›¾ä¸ºæ­¤ç±»èƒ½åŠ›æä¾›äº†ä¸¥æ ¼çš„æµ‹è¯•å¹³å°ï¼Œå› ä¸ºå®ƒä»¬èƒ½å¤Ÿå°†æ¨ç†ä»»åŠ¡ä¸è‡ªç„¶å›¾åƒä¸­å¸¸è§çš„è¯­ä¹‰å™ªå£°éš”ç¦»å¼€æ¥ã€‚ä¸ºäº†è¡¡é‡è¿™ä¸€å‰æ²¿çš„è¿›å±•ï¼Œæˆ‘ä»¬å¼•å…¥äº†MaRVL-QAï¼ˆæ•°å­¦æ¨ç†åœ¨è§†è§‰æ™¯è§‚ä¸Šçš„åº”ç”¨ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨å®šé‡è¯„ä¼°è¿™äº›æ ¸å¿ƒæ¨ç†æŠ€èƒ½ã€‚è¯¥åŸºå‡†æµ‹è¯•åŒ…å«ä¸¤ä¸ªæ–°ä»»åŠ¡ï¼šæ‹“æ‰‘è®¡æ•°ï¼Œè¯†åˆ«å’Œåˆ—ä¸¾å±€éƒ¨æœ€å¤§å€¼ç­‰ç‰¹å¾ï¼›ä»¥åŠè½¬æ¢è¯†åˆ«ï¼Œè¯†åˆ«åº”ç”¨çš„å‡ ä½•å˜æ¢ã€‚æˆ‘ä»¬çš„è¯„ä¼°æ˜¯åŸºäºä»åŠŸèƒ½ä¸°å¯Œçš„åº“ä¸­ç²¾å¿ƒæŒ‘é€‰çš„å‡½æ•°ï¼Œç»è¿‡ä¸¥æ ¼çš„æ­§ä¹‰è¿‡æ»¤åç”ŸæˆMaRVL-QAã€‚å³ä½¿åœ¨MaRVL-QAçš„è¯„ä¼°ä¸­ï¼Œæœ€å…ˆè¿›çš„MLLMsä¹Ÿé¢ä¸´ç€å·¨å¤§çš„æŒ‘æˆ˜ï¼Œå¾€å¾€å€¾å‘äºä½¿ç”¨è‚¤æµ…çš„å¯å‘å¼æ–¹æ³•ï¼Œè€Œä¸æ˜¯ç¨³å¥çš„ç©ºé—´æ¨ç†ã€‚MaRVL-QAä¸ºç ”ç©¶é¢†åŸŸæä¾›äº†ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ–°å·¥å…·ï¼Œå¯ä»¥è¡¡é‡è¿›å±•ã€æš´éœ²æ¨¡å‹å±€é™æ€§ï¼Œå¹¶å¼•å¯¼å¼€å‘å…·æœ‰æ›´æ·±åˆ»æ¨ç†èƒ½åŠ›çš„MLLMsã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.17180v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬ä»‹ç»äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰é¢ä¸´çš„å…³é”®å‰æ²¿é—®é¢˜ï¼Œå³ç›´æ¥ä»å›¾åƒè¿›è¡Œæ·±åº¦æ•°å­¦å’Œç©ºé—´æ¨ç†çš„èƒ½åŠ›ï¼Œè¶…è¶Šäº†å…¶åœ¨è¯­ä¹‰æè¿°æ–¹é¢çš„æˆåŠŸåº”ç”¨ã€‚ä¸ºäº†æµ‹è¯•è¿™ä¸€èƒ½åŠ›ï¼Œå¼•å…¥äº†MaRVL-QAï¼ˆåŸºäºè§†è§‰æ™¯è§‚çš„æ•°å­¦æ¨ç†ï¼‰è¿™ä¸€æ–°åŸºå‡†æµ‹è¯•ï¼Œç”¨äºå®šé‡è¯„ä¼°æ ¸å¿ƒæ¨ç†æŠ€èƒ½ã€‚è¯¥åŸºå‡†æµ‹è¯•åŒ…å«ä¸¤ä¸ªæ–°ä»»åŠ¡ï¼šæ‹“æ‰‘è®¡æ•°å’Œå˜æ¢è¯†åˆ«ã€‚é€šè¿‡å¯¹å‡½æ•°çš„ä¸¥æ ¼ç­›é€‰å’Œæ¨¡ç³Šæ€§è¿‡æ»¤ï¼Œå¯¹ç°æœ‰æ¨¡å‹çš„è¯„ä¼°æ˜¾ç¤ºï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åº”å¯¹è¿™ä¸¤ä¸ªä»»åŠ¡æ—¶ä»ç„¶é¢ä¸´å¾ˆå¤§æŒ‘æˆ˜ï¼Œå¾€å¾€ä¾èµ–äºè‚¤æµ…çš„å¯å‘å¼ç­–ç•¥è€Œéæ·±å…¥çš„ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚å› æ­¤ï¼ŒMaRVL-QAæä¾›äº†ä¸€ä¸ªæŒ‘æˆ˜æ€§çš„æ–°å·¥å…·ï¼Œç”¨äºè¡¡é‡ç ”ç©¶è¿›å±•ã€æ­ç¤ºæ¨¡å‹å±€é™æ€§å¹¶å¼•å¯¼å¼€å‘å…·æœ‰æ›´æ·±å…¥æ¨ç†èƒ½åŠ›çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯å…³é”®è¦ç‚¹è¦ç‚¹åˆ—è¡¨ï¼š</p>
<ul>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹é¢ä¸´çš„å…³é”®æŒ‘æˆ˜æ˜¯ç›´æ¥è¿›è¡Œæ•°å­¦å’Œç©ºé—´æ¨ç†çš„èƒ½åŠ›çš„æå‡ã€‚è¿™ä¸€èƒ½åŠ›å¯¹äºç†è§£å¤æ‚æ•°æ®å’ŒçŸ¥è¯†é¢†åŸŸéå¸¸é‡è¦ã€‚å¯¹äºåŸºç¡€çŸ¥è¯†çš„ç†è§£ç¨‹åº¦å’Œè¡¨ç°å¾ˆé‡è¦ï¼Œä¸ä»…éœ€è¦å‡†ç¡®çš„çŸ¥è¯†ä½“ç³»è¡¨è¿°æ›´ä¾èµ–åŸºç¡€ç†è§£å’Œé«˜æ•ˆèåˆå¤æ‚çš„ç†è®ºå’Œå®é™…æƒ…å†µå½¢æˆä¿¡æ¯è§£è¯»ã€‚éšç€æŠ€æœ¯çš„å‘å±•å’Œç ”ç©¶çš„æ·±å…¥ï¼Œè¯¥é¢†åŸŸéœ€è¦æ›´å¤šèƒ½å¤Ÿæ·±å…¥ç†è§£æ•°å­¦å’Œå‡ ä½•æ¦‚å¿µçš„ç³»ç»Ÿå’Œæ–¹æ³•ã€‚æœªæ¥è¯¥é¢†åŸŸçš„ä¸€ä¸ªé‡è¦çš„ç ”ç©¶ç›®æ ‡å°†å¦‚ä½•è¿ç”¨AIå®ç°æœºå™¨èƒ½å¤Ÿä¸äººç±»æœ‰ä¸€æ ·çš„è®¤çŸ¥å­¦ä¹ èƒ½åŠ›å³é€šè¿‡å¯¹è·å–ä¿¡æ¯çš„åˆ¤æ–­èåˆè‡ªå·±çš„æ„è¯†å†³ç­–æœºåˆ¶æ„å»ºå®Œæˆç²¾å‡†çš„ç©ºé—´æ€ç»´å’Œæ•°å­¦çš„ç»“åˆåˆ†æè¿ç”¨å’ŒæŠ½è±¡æ€è€ƒèƒ½é‡Œæ˜¯æˆ‘ä»¬é•¿æœŸé¢å¯¹çš„ç›®æ ‡æ–¹å‘ä¹‹ä¸€ã€‚å…·ä½“åˆ°å…·ä½“çš„AIç ”å‘é˜¶æ®µä¼šæœ‰ç€ç›¸å¯¹é‡è¦è§’è‰²å¸®åŠ©å·¥ç¨‹å¸ˆä¸æ–­ç ”ç©¶å¹¶å¼€å‘å‡ºæ›´å¤šæ–°çš„åº”ç”¨ã€‚ä»æŠ€æœ¯åº”ç”¨çš„è§’åº¦AIé€šè¿‡èåˆæ·±åº¦å­¦ä¹ ä»¥åŠæ›´å¤šæ–°çš„æŠ€æœ¯æ‰‹æ®µæ¥æå‡æœºå™¨å­¦ä¹ æ¨¡å‹çš„ç©ºé—´æ€ç»´èƒ½åŠ›å’Œæ•°å­¦åˆ†æèƒ½åŠ›æˆä¸ºå…³é”®çªç ´å£ä¹‹ä¸€ã€‚æœºå™¨å­¦ä¹ é¢†åŸŸä¸­è·¨å­¦ç§‘çš„ç»¼åˆæ€§èåˆä¾‹å¦‚åˆ©ç”¨æœ€æ–°çš„è®­ç»ƒæ¡†æ¶åœ¨ä»¿çœŸå®éªŒä¸­æ›´è¿›ä¸€æ­¥çš„è®©æ¨¡å‹ç†è§£å’Œè¯†åˆ«ç°å®ä¸–ç•Œç¯å¢ƒæˆä¸ºå¯èƒ½ä½¿AIçš„è®¤çŸ¥å­¦ä¹ å°†èµ°å‘å…¨æ–°çš„é«˜åº¦å¹¶ä¸”èµ‹äºˆå…¶è§£å†³æœªæ¥é—®é¢˜çš„å…¨æ–°è§†è§’å’Œèƒ½åŠ›ï¼Œä»è€Œåœ¨ä»¿çœŸç¯å¢ƒä¸­çš„ç²¾å‡†è¯†åˆ«æ„ŸçŸ¥è·å¾—åº”ç”¨çš„ä»·å€¼ä»è€Œæ›´å¹¿æ³›åœ°åœ¨ç°å®ä¸–ç•Œä¸­å‘æŒ¥ä½œç”¨ã€‚å› æ­¤æœªæ¥å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„å‘å±•å°†æ›´åŠ æ³¨é‡å¯¹æ·±åº¦æ•°å­¦å’Œç©ºé—´æ¨ç†èƒ½åŠ›çš„æå‡ã€‚é€šè¿‡å¼•å…¥æ–°çš„åŸºå‡†æµ‹è¯•å¦‚MaRVL-QAèƒ½å¤Ÿå¸®åŠ©æ¨åŠ¨å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹çš„æŒç»­å‘å±•å°†ä¼šå¯¹æ•´ä¸ªæœºå™¨å­¦ä¹ å’Œäººå·¥æ™ºèƒ½é¢†åŸŸçš„çªç ´åšå‡ºé‡è¦è¿›å±•å¹¶å¯¹å¤šæ¨¡æ€ç†è§£æä¾›æ›´å¤šåˆ‡å®å¯è¡Œçš„æŠ€æœ¯åº”ç”¨æ–¹å‘å’Œä»·å€¼çš„æ‰©å±•ä¸åº”ç”¨çš„ç ”ç©¶å’Œæ–°æŠ€æœ¯çš„å¼€å‘ã€‚å¯¹äºæœªæ¥çš„ç ”ç©¶å’Œå‘å±•æ¥è¯´è¿™æ˜¯ä¸€ä¸ªé‡è¦çš„é‡Œç¨‹ç¢‘å’Œæ–¹å‘ä¹‹ä¸€ã€‚éšç€ç ”ç©¶çš„æ·±å…¥å’ŒæŠ€æœ¯çš„å‘å±•å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„åº”ç”¨å‰æ™¯å°†ä¼šè¶Šæ¥è¶Šå¹¿é˜”å¹¶å¸¦æ¥é©å‘½æ€§çš„å˜åŒ–å’Œå‘å±•å‰æ™¯ä»¥åŠæ›´åŠ ä¸°å¯Œçš„åº”ç”¨åœºæ™¯ã€‚è¿™å¯¹äºæœªæ¥çš„æŠ€æœ¯å‘å±•å’Œåº”ç”¨å…·æœ‰éå¸¸é‡è¦çš„æ„ä¹‰å’Œä»·å€¼å¹¶æ¨åŠ¨äººå·¥æ™ºèƒ½é¢†åŸŸä¸æ–­å‘å‰å‘å±•å¹¶å–å¾—æ›´å¤§çš„çªç ´å’Œè¿›å±•ä»¥åŠå®ç°æ›´åŠ æ™ºèƒ½å’Œé«˜æ•ˆçš„åº”ç”¨ç³»ç»Ÿå’ŒæŠ€æœ¯åº”ç”¨ã€‚å› æ­¤æœªæ¥çš„ç ”ç©¶å’Œå‘å±•å°†æ›´åŠ æ³¨é‡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„æ·±åº¦æ•°å­¦å’Œç©ºé—´æ¨ç†èƒ½åŠ›çš„æå‡å¹¶ç»§ç»­æ¨åŠ¨äººå·¥æ™ºèƒ½é¢†åŸŸçš„ä¸æ–­å‘å±•å’Œè¿›æ­¥ä»¥åŠæ›´åŠ å¹¿æ³›çš„åº”ç”¨åœºæ™¯å’Œåº”ç”¨ä»·å€¼ç­‰å…³é”®é¢†åŸŸçš„æ‹“å±•å’Œåˆ›æ–°ç­‰å…·æœ‰é‡è¦æ„ä¹‰å’Œä»·å€¼çš„å·¥ä½œå’Œè´¡çŒ®å°†ä¼šæŒç»­æ¶Œç°å¹¶å¼•é¢†æœªæ¥çš„ç§‘æŠ€å‘å±•è¶‹åŠ¿å’Œæ–¹å‘ç­‰é‡è¦çš„é¢†åŸŸå‘å±•ä»¥åŠå®ç°æ›´åŠ æ™ºèƒ½å’Œé«˜æ•ˆçš„åº”ç”¨ç³»ç»Ÿå’ŒæŠ€æœ¯åº”ç”¨ç­‰ç›®æ ‡æ–¹å‘ä¹‹ä¸€å€¼å¾—æœŸå¾…åœ¨å¹¿å¤§ç§‘ç ”äººå‘˜åŠç›¸å…³äº§ä¸šçš„ä¸æ–­ç ”å‘å’Œå®è·µä¸­æŒç»­æ¨è¿›æ™ºèƒ½åŒ–æ°´å¹³çš„æå‡æœ€ç»ˆé€šè¿‡äº§ä¸šçš„æŠ€æœ¯ç ”å‘çªç ´ç“¶é¢ˆé€æ­¥å®ç°äººå·¥æ™ºèƒ½æŠ€æœ¯æŒç»­å‘å±•å’Œè¿›æ­¥çš„ç›®æ ‡å’Œæ–¹å‘ä¹‹ä¸€å€¼å¾—æœŸå¾…ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.17180">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2508.17180v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2508.17180v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2508.17180v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2508.17180v2/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2508.17180v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2508.17180v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2508.17180v2/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2508.17180v2/page_5_1.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Bridging-the-Gap-in-Ophthalmic-AI-MM-Retinal-Reason-Dataset-and-OphthaReason-Model-toward-Dynamic-Multimodal-Reasoning"><a href="#Bridging-the-Gap-in-Ophthalmic-AI-MM-Retinal-Reason-Dataset-and-OphthaReason-Model-toward-Dynamic-Multimodal-Reasoning" class="headerlink" title="Bridging the Gap in Ophthalmic AI: MM-Retinal-Reason Dataset and   OphthaReason Model toward Dynamic Multimodal Reasoning"></a>Bridging the Gap in Ophthalmic AI: MM-Retinal-Reason Dataset and   OphthaReason Model toward Dynamic Multimodal Reasoning</h2><p><strong>Authors:Ruiqi Wu, Yuang Yao, Tengfei Ma, Chenran Zhang, Na Su, Tao Zhou, Geng Chen, Wen Fan, Yi Zhou</strong></p>
<p>Multimodal large language models (MLLMs) have recently demonstrated remarkable reasoning abilities with reinforcement learning paradigm. Although several multimodal reasoning models have been explored in the medical domain, most of them focus exclusively on basic reasoning, which refers to shallow inference based on visual feature matching. However, real-world clinical diagnosis extends beyond basic reasoning, demanding reasoning processes that integrate heterogeneous clinical information (such as chief complaints and medical history) with multimodal medical imaging data. To bridge this gap, we introduce MM-Retinal-Reason, the first ophthalmic multimodal dataset with the full spectrum of perception and reasoning. It encompasses both basic reasoning tasks and complex reasoning tasks, aiming to enhance visual-centric fundamental reasoning capabilities and emulate realistic clinical thinking patterns. Building upon MM-Retinal-Reason, we propose OphthaReason, the first ophthalmology-specific multimodal reasoning model with step-by-step reasoning traces. To enable flexible adaptation to both basic and complex reasoning tasks, we specifically design a novel method called Uncertainty-Aware Dynamic Thinking (UADT), which estimates sample-level uncertainty via entropy and dynamically modulates the modelâ€™s exploration depth using a shaped advantage mechanism. Comprehensive experiments demonstrate that our model achieves state-of-the-art performance on both basic and complex reasoning tasks, outperforming general-purpose MLLMs, medical MLLMs, RL-based medical MLLMs, and ophthalmic MLLMs by at least 24.92%, 15.00%, 21.20%, and 17.66%. Project Page: \href{<a target="_blank" rel="noopener" href="https://github.com/lxirich/OphthaReason%7D%7Blink%7D">https://github.com/lxirich/OphthaReason}{link}</a>. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æœ€è¿‘é‡‡ç”¨å¼ºåŒ–å­¦ä¹ èŒƒå¼å±•ç¤ºäº†å‡ºè‰²çš„æ¨ç†èƒ½åŠ›ã€‚å°½ç®¡åŒ»ç–—é¢†åŸŸå·²ç»æ¢ç´¢äº†å¤šç§å¤šæ¨¡æ€æ¨ç†æ¨¡å‹ï¼Œä½†å¤§å¤šæ•°æ¨¡å‹ä¸»è¦å…³æ³¨åŸºæœ¬æ¨ç†ï¼Œè¿™æŒ‡çš„æ˜¯åŸºäºè§†è§‰ç‰¹å¾åŒ¹é…çš„æµ…å±‚æ¨ç†ã€‚ç„¶è€Œï¼Œç°å®ä¸–ç•Œä¸­çš„ä¸´åºŠè¯Šæ–­è¶…è¶Šäº†åŸºæœ¬æ¨ç†ï¼Œéœ€è¦æ•´åˆå¼‚è´¨ä¸´åºŠä¿¡æ¯ï¼ˆå¦‚ä¸»è¯‰å’Œç—…å²ï¼‰ä¸å¤šæ¨¡æ€åŒ»å­¦æˆåƒæ•°æ®çš„è¿‡ç¨‹æ¨ç†ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†MM-Retinal-Reasonï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªæ¶µç›–å®Œæ•´æ„ŸçŸ¥å’Œæ¨ç†å…‰è°±çš„çœ¼ç§‘å¤šæ¨¡æ€æ•°æ®é›†ã€‚å®ƒæ¶µç›–äº†åŸºæœ¬æ¨ç†ä»»åŠ¡å’Œå¤æ‚æ¨ç†ä»»åŠ¡ï¼Œæ—¨åœ¨å¢å¼ºè§†è§‰ä¸ºä¸­å¿ƒçš„åŸºæœ¬æ¨ç†èƒ½åŠ›ï¼Œå¹¶æ¨¡æ‹Ÿç°å®çš„ä¸´åºŠæ€ç»´æ¨¡å¼ã€‚åŸºäºMM-Retinal-Reasonï¼Œæˆ‘ä»¬æå‡ºäº†çœ¼ç§‘ç‰¹å®šå¤šæ¨¡æ€æ¨ç†æ¨¡å‹OphthaReasonï¼Œå…·å¤‡é€æ­¥æ¨ç†è½¨è¿¹ã€‚ä¸ºäº†çµæ´»é€‚åº”åŸºæœ¬å’Œå¤æ‚æ¨ç†ä»»åŠ¡ï¼Œæˆ‘ä»¬ç‰¹æ„è®¾è®¡äº†ä¸€ç§åä¸ºä¸ç¡®å®šæ€§æ„ŸçŸ¥åŠ¨æ€æ€ç»´ï¼ˆUADTï¼‰çš„æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡ç†µä¼°è®¡æ ·æœ¬çº§ä¸ç¡®å®šæ€§ï¼Œå¹¶ä½¿ç”¨æˆå‹ä¼˜åŠ¿æœºåˆ¶åŠ¨æ€è°ƒèŠ‚æ¨¡å‹çš„æ¢ç´¢æ·±åº¦ã€‚ç»¼åˆå®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨åŸºæœ¬å’Œå¤æ‚æ¨ç†ä»»åŠ¡ä¸Šå‡è¾¾åˆ°äº†æœ€æ–°æ€§èƒ½æ°´å¹³ï¼Œè‡³å°‘æ¯”æ™®é€šMLLMsã€åŒ»ç–—MLLMsã€åŸºäºRLçš„åŒ»ç–—MLLMså’Œçœ¼ç§‘MLLMsé«˜å‡º24.92%ã€15.00%ã€21.20%å’Œ17.66%ã€‚é¡¹ç›®é¡µé¢é“¾æ¥ï¼š<a target="_blank" rel="noopener" href="https://github.com/lxirich/OphthaReason%E3%80%82%EF%BC%88%E5%85%B7%E4%BD%93%E9%93%BE%E6%8E%A5%EF%BC%89">https://github.com/lxirich/OphthaReasonã€‚ï¼ˆå…·ä½“é“¾æ¥ï¼‰</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.16129v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨åŒ»ç–—é¢†åŸŸçš„åº”ç”¨ï¼Œå°¤å…¶æ˜¯å…¶åœ¨çœ¼ç§‘è¯Šæ–­ä¸­çš„ä½¿ç”¨ã€‚æ–‡ç« æŒ‡å‡ºï¼Œå°½ç®¡å·²æœ‰ä¸€äº›å¤šæ¨¡æ€æ¨ç†æ¨¡å‹è¢«æ¢ç´¢ç”¨äºåŒ»ç–—é¢†åŸŸï¼Œä½†å®ƒä»¬å¤§å¤šåªå…³æ³¨åŸºäºè§†è§‰ç‰¹å¾åŒ¹é…çš„åŸºæœ¬æ¨ç†ã€‚ç„¶è€Œï¼Œç°å®ä¸–ç•Œä¸­çš„ä¸´åºŠè¯Šæ–­éœ€è¦è¶…è¶ŠåŸºæœ¬æ¨ç†ï¼Œæ•´åˆå¼‚è´¨ä¸´åºŠä¿¡æ¯ä¸å¤šæ¨¡æ€åŒ»å­¦æˆåƒæ•°æ®ã€‚ä¸ºæ­¤ï¼Œæ–‡ç« å¼•å…¥äº†MM-Retinal-Reasonæ•°æ®é›†ï¼ŒåŒ…å«åŸºæœ¬å’Œå¤æ‚æ¨ç†ä»»åŠ¡ï¼Œæ—¨åœ¨å¢å¼ºè§†è§‰ä¸ºä¸­å¿ƒçš„åŸºæœ¬æ¨ç†èƒ½åŠ›å¹¶æ¨¡æ‹Ÿç°å®ä¸´åºŠæ€ç»´æ¨¡å¼ã€‚åŸºäºæ­¤æ•°æ®é›†ï¼Œæ–‡ç« æå‡ºäº†é¦–ä¸ªçœ¼ç§‘ä¸“ç”¨çš„å¤šæ¨¡æ€æ¨ç†æ¨¡å‹â€”â€”OphthaReasonï¼Œå¹¶è®¾è®¡äº†ä¸€ç§åä¸ºUncertainty-Aware Dynamic Thinkingï¼ˆUADTï¼‰çš„æ–°æ–¹æ³•ï¼Œä»¥çµæ´»é€‚åº”åŸºæœ¬å’Œå¤æ‚æ¨ç†ä»»åŠ¡ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨åŸºæœ¬å’Œå¤æ‚æ¨ç†ä»»åŠ¡ä¸Šçš„æ€§èƒ½å‡è¾¾åˆ°æœ€æ–°æ°´å¹³ï¼Œè‡³å°‘æ¯”å…¶ä»–æ¨¡å‹é«˜å‡º24.92ï¼…ã€15.00ï¼…ã€21.20ï¼…å’Œ17.66ï¼…ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨åŒ»ç–—é¢†åŸŸå…·æœ‰å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>ç°æœ‰æ¨¡å‹å¤§å¤šä»…å…³æ³¨åŸºäºè§†è§‰ç‰¹å¾åŒ¹é…çš„åŸºæœ¬æ¨ç†ï¼Œç¼ºä¹å¤æ‚æ€§ã€‚</li>
<li>ç°å®ä¸–ç•Œä¸´åºŠè¯Šæ–­éœ€è¦æ•´åˆå¼‚è´¨ä¸´åºŠä¿¡æ¯ä¸å¤šæ¨¡æ€åŒ»å­¦æˆåƒæ•°æ®ã€‚</li>
<li>MM-Retinal-Reasonæ•°æ®é›†å¼•å…¥ï¼ŒåŒ…å«åŸºæœ¬å’Œå¤æ‚æ¨ç†ä»»åŠ¡ï¼Œæ¨¡æ‹Ÿç°å®ä¸´åºŠæ€ç»´ã€‚</li>
<li>é¦–æ¬¡æå‡ºçœ¼ç§‘ä¸“ç”¨å¤šæ¨¡æ€æ¨ç†æ¨¡å‹â€”â€”OphthaReasonã€‚</li>
<li>Uncertainty-Aware Dynamic Thinkingï¼ˆUADTï¼‰æ–¹æ³•è®¾è®¡ï¼Œä»¥çµæ´»é€‚åº”ä¸åŒæ¨ç†ä»»åŠ¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.16129">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2508.16129v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2508.16129v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2508.16129v2/page_3_0.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Mobile-Agent-v3-Fundamental-Agents-for-GUI-Automation"><a href="#Mobile-Agent-v3-Fundamental-Agents-for-GUI-Automation" class="headerlink" title="Mobile-Agent-v3: Fundamental Agents for GUI Automation"></a>Mobile-Agent-v3: Fundamental Agents for GUI Automation</h2><p><strong>Authors:Jiabo Ye, Xi Zhang, Haiyang Xu, Haowei Liu, Junyang Wang, Zhaoqing Zhu, Ziwei Zheng, Feiyu Gao, Junjie Cao, Zhengxi Lu, Jitong Liao, Qi Zheng, Fei Huang, Jingren Zhou, Ming Yan</strong></p>
<p>This paper introduces GUI-Owl, a foundational GUI agent model that achieves state-of-the-art performance among open-source end-to-end models on ten GUI benchmarks across desktop and mobile environments, covering grounding, question answering, planning, decision-making, and procedural knowledge. GUI-Owl-7B achieves 66.4 on AndroidWorld and 29.4 on OSWorld. Building on this, we propose Mobile-Agent-v3, a general-purpose GUI agent framework that further improves performance to 73.3 on AndroidWorld and 37.7 on OSWorld, setting a new state-of-the-art for open-source GUI agent frameworks. GUI-Owl incorporates three key innovations: (1) Large-scale Environment Infrastructure: a cloud-based virtual environment spanning Android, Ubuntu, macOS, and Windows, enabling our Self-Evolving GUI Trajectory Production framework. This generates high-quality interaction data via automated query generation and correctness validation, leveraging GUI-Owl to refine trajectories iteratively, forming a self-improving loop. It supports diverse data pipelines and reduces manual annotation. (2) Diverse Foundational Agent Capabilities: by integrating UI grounding, planning, action semantics, and reasoning patterns, GUI-Owl supports end-to-end decision-making and can act as a modular component in multi-agent systems. (3) Scalable Environment RL: we develop a scalable reinforcement learning framework with fully asynchronous training for real-world alignment. We also introduce Trajectory-aware Relative Policy Optimization (TRPO) for online RL, achieving 34.9 on OSWorld. GUI-Owl and Mobile-Agent-v3 are open-sourced at <a target="_blank" rel="noopener" href="https://github.com/X-PLUG/MobileAgent">https://github.com/X-PLUG/MobileAgent</a>. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†GUI-Owlï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºç¡€GUIä»£ç†æ¨¡å‹ï¼Œå®ƒåœ¨æ¡Œé¢å’Œç§»åŠ¨ç¯å¢ƒçš„åä¸ªGUIåŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†å¼€æºç«¯åˆ°ç«¯æ¨¡å‹çš„æœ€æ–°æ€§èƒ½ï¼Œæ¶µç›–äº†æ¥åœ°ã€é—®ç­”ã€è§„åˆ’ã€å†³ç­–å’Œç¨‹åºçŸ¥è¯†ã€‚GUI-Owl-7Båœ¨AndroidWorldä¸Šå¾—åˆ†ä¸º66.4ï¼Œåœ¨OSWorldä¸Šå¾—åˆ†ä¸º29.4ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æå‡ºäº†é€šç”¨GUIä»£ç†æ¡†æ¶Mobile-Agent-v3ï¼Œè¿›ä¸€æ­¥å°†AndroidWorldçš„æ€§èƒ½æå‡è‡³73.3ï¼ŒOSWorldçš„æ€§èƒ½æå‡è‡³37.7ï¼Œä¸ºå¼€æºGUIä»£ç†æ¡†æ¶åˆ›é€ äº†æ–°çš„æœ€æ–°è®°å½•ã€‚GUI-OwlåŒ…å«äº†ä¸‰ä¸ªå…³é”®çš„åˆ›æ–°ç‚¹ï¼šï¼ˆ1ï¼‰å¤§è§„æ¨¡ç¯å¢ƒåŸºç¡€è®¾æ–½ï¼šä¸€ä¸ªè·¨è¶ŠAndroidã€Ubuntuã€macOSå’ŒWindowsçš„äº‘ç«¯è™šæ‹Ÿç¯å¢ƒï¼Œä½¿æˆ‘ä»¬çš„è‡ªæˆ‘è¿›åŒ–GUIè½¨è¿¹ç”Ÿäº§æ¡†æ¶å¾—ä»¥è¿è¡Œã€‚è¯¥æ¡†æ¶é€šè¿‡è‡ªåŠ¨åŒ–æŸ¥è¯¢ç”Ÿæˆå’Œæ­£ç¡®æ€§éªŒè¯ï¼Œåˆ©ç”¨GUI-Owlè¿­ä»£åœ°ä¼˜åŒ–è½¨è¿¹ï¼Œå½¢æˆä¸€ä¸ªè‡ªæˆ‘æ”¹è¿›å¾ªç¯ã€‚å®ƒæ”¯æŒå„ç§æ•°æ®ç®¡é“ï¼Œå‡å°‘æ‰‹åŠ¨æ³¨é‡Šã€‚ï¼ˆ2ï¼‰å¤šæ ·åŒ–çš„åŸºç¡€ä»£ç†èƒ½åŠ›ï¼šé€šè¿‡æ•´åˆUIæ¥åœ°ã€è§„åˆ’ã€åŠ¨ä½œè¯­ä¹‰å’Œæ¨ç†æ¨¡å¼ï¼ŒGUI-Owlæ”¯æŒç«¯åˆ°ç«¯çš„å†³ç­–åˆ¶å®šï¼Œå¹¶å¯ä»¥ä½œä¸ºå¤šä»£ç†ç³»ç»Ÿä¸­çš„æ¨¡å—åŒ–ç»„ä»¶ã€‚ï¼ˆ3ï¼‰å¯æ‰©å±•çš„ç¯å¢ƒå¼ºåŒ–å­¦ä¹ ï¼šæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªå¯æ‰©å±•çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œå…·æœ‰å®Œå…¨å¼‚æ­¥è®­ç»ƒä»¥ç¬¦åˆç°å®ä¸–ç•Œçš„å¯¹é½ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†è½¨è¿¹æ„ŸçŸ¥ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆTRPOï¼‰è¿›è¡Œåœ¨çº¿å¼ºåŒ–å­¦ä¹ ï¼Œåœ¨OSWorldä¸Šå®ç°äº†34.9çš„å¾—åˆ†ã€‚GUI-Owlå’ŒMobile-Agent-v3å·²åœ¨<a target="_blank" rel="noopener" href="https://github.com/X-PLUG/Mobileagent%E5%BC%80%E6%BA%90%E3%80%82">https://github.com/X-PLUG/Mobileagentå¼€æºã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.15144v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†GUI-Owlï¼Œä¸€ä¸ªåŸºç¡€GUIä»£ç†æ¨¡å‹ï¼Œå®ƒåœ¨æ¡Œé¢å’Œç§»åŠ¨ç¯å¢ƒçš„åä¸ªGUIåŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†å¼€åˆ›æ€§çš„æ€§èƒ½è¡¨ç°ã€‚GUI-Owlæ¨¡å‹å¼•å…¥äº†ä¸€ç³»åˆ—å…³é”®åˆ›æ–°ï¼ŒåŒ…æ‹¬å¤§è§„æ¨¡ç¯å¢ƒåŸºç¡€è®¾æ–½ã€å¤šæ ·åŒ–çš„åŸºç¡€ä»£ç†èƒ½åŠ›ä»¥åŠå¯æ‰©å±•çš„ç¯å¢ƒå¼ºåŒ–å­¦ä¹ ç­‰ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæå‡ºäº†é€šç”¨GUIä»£ç†æ¡†æ¶Mobile-Agent-v3ï¼Œè¿›ä¸€æ­¥æé«˜äº†æ€§èƒ½ã€‚è¿™äº›æˆæœå‡å·²åœ¨GitHubä¸Šå¼€æºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GUI-Owlæ¨¡å‹å®ç°äº†è·¨æ¡Œé¢å’Œç§»åŠ¨ç¯å¢ƒçš„GUIä»£ç†æ¨¡å‹çš„å…ˆè¿›æ€§èƒ½ã€‚</li>
<li>GUI-Owlåœ¨AndroidWorldå’ŒOSWorldåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ã€‚</li>
<li>Mobile-Agent-v3æ¡†æ¶æé«˜äº†GUI-Owlçš„æ€§èƒ½ï¼Œå¹¶å¼€åˆ›äº†GUIä»£ç†æ¡†æ¶çš„æ–°å±€é¢ã€‚</li>
<li>GUI-Owlæ¨¡å‹å¼•å…¥å¤§è§„æ¨¡ç¯å¢ƒåŸºç¡€è®¾æ–½ï¼Œæ”¯æŒå¤šç§æ•°æ®ç®¡é“ï¼Œå‡å°‘æ‰‹åŠ¨æ³¨é‡Šã€‚</li>
<li>GUI-Owlå…·å¤‡å¤šæ ·åŒ–çš„åŸºç¡€ä»£ç†èƒ½åŠ›ï¼Œå¯æ”¯æŒç«¯åˆ°ç«¯çš„å†³ç­–åˆ¶å®šï¼Œå¹¶å¯ä½œä¸ºå¤šä»£ç†ç³»ç»Ÿæ¨¡å—ç»„ä»¶ã€‚</li>
<li>å¼€å‘äº†å¯æ‰©å±•çš„ç¯å¢ƒå¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œé‡‡ç”¨å®Œå…¨å¼‚æ­¥è®­ç»ƒå®ç°çœŸå®ä¸–ç•Œå¯¹é½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.15144">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2508.15144v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2508.15144v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2508.15144v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2508.15144v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2508.15144v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-12/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-12/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-12/LLM/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_LLM/2507.02253v4/page_1_0.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-12  A Survey of Reinforcement Learning for Large Reasoning Models
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-12
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-11/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2508.10542v3/page_4_0.jpg" class="responsive-img" alt="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª">
                        
                        <span class="card-title">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-11  GCRPNet Graph-Enhanced Contextual and Regional Perception Network for   Salient Object Detection in Optical Remote Sensing Images
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-11
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                    æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                        <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">30666.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
