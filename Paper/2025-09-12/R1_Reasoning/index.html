<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning 方向最新论文已更新，请持续关注 Update in 2025-09-12  A Survey of Reinforcement Learning for Large Reasoning Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.06980v1/page_0_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-09-12
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-10-07
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    23k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    94 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-09-12-更新"><a href="#2025-09-12-更新" class="headerlink" title="2025-09-12 更新"></a>2025-09-12 更新</h1><h2 id="A-Survey-of-Reinforcement-Learning-for-Large-Reasoning-Models"><a href="#A-Survey-of-Reinforcement-Learning-for-Large-Reasoning-Models" class="headerlink" title="A Survey of Reinforcement Learning for Large Reasoning Models"></a>A Survey of Reinforcement Learning for Large Reasoning Models</h2><p><strong>Authors:Kaiyan Zhang, Yuxin Zuo, Bingxiang He, Youbang Sun, Runze Liu, Che Jiang, Yuchen Fan, Kai Tian, Guoli Jia, Pengfei Li, Yu Fu, Xingtai Lv, Yuchen Zhang, Sihang Zeng, Shang Qu, Haozhan Li, Shijie Wang, Yuru Wang, Xinwei Long, Fangfu Liu, Xiang Xu, Jiaze Ma, Xuekai Zhu, Ermo Hua, Yihao Liu, Zonglin Li, Huayu Chen, Xiaoye Qu, Yafu Li, Weize Chen, Zhenzhao Yuan, Junqi Gao, Dong Li, Zhiyuan Ma, Ganqu Cui, Zhiyuan Liu, Biqing Qi, Ning Ding, Bowen Zhou</strong></p>
<p>In this paper, we survey recent advances in Reinforcement Learning (RL) for reasoning with Large Language Models (LLMs). RL has achieved remarkable success in advancing the frontier of LLM capabilities, particularly in addressing complex logical tasks such as mathematics and coding. As a result, RL has emerged as a foundational methodology for transforming LLMs into LRMs. With the rapid progress of the field, further scaling of RL for LRMs now faces foundational challenges not only in computational resources but also in algorithm design, training data, and infrastructure. To this end, it is timely to revisit the development of this domain, reassess its trajectory, and explore strategies to enhance the scalability of RL toward Artificial SuperIntelligence (ASI). In particular, we examine research applying RL to LLMs and LRMs for reasoning abilities, especially since the release of DeepSeek-R1, including foundational components, core problems, training resources, and downstream applications, to identify future opportunities and directions for this rapidly evolving area. We hope this review will promote future research on RL for broader reasoning models. Github: <a target="_blank" rel="noopener" href="https://github.com/TsinghuaC3I/Awesome-RL-for-LRMs">https://github.com/TsinghuaC3I/Awesome-RL-for-LRMs</a> </p>
<blockquote>
<p>在这篇论文中，我们概述了强化学习（RL）在大型语言模型（LLM）中的最新进展。强化学习在推进大型语言模型能力方面取得了显著的成功，特别是在解决数学和编码等复杂逻辑任务方面。因此，强化学习已经成为将大型语言模型转化为大型推理模型（LRMs）的基础方法。随着该领域的快速发展，强化学习在大型推理模型的进一步扩展方面现在面临着不仅是计算资源的基础挑战，还有算法设计、训练数据和基础设施方面的挑战。因此，重新审视这个领域的发展，重新评估其轨迹，探索提高强化学习在人工超智能（ASI）方面可扩展性的策略是十分及时的。尤其是我们关注自DeepSeek-R1发布以来，将强化学习应用于大型语言模型和大型推理模型的研究，包括基础组件、核心问题、训练资源和下游应用，以确定这个快速演变领域的未来机遇和方向。我们希望这次综述能推动强化学习在更广泛的推理模型方面的未来研究。Github：<a target="_blank" rel="noopener" href="https://github.com/TsinghuaC3I/Awesome-RL-for-LRMs">https://github.com/TsinghuaC3I/Awesome-RL-for-LRMs</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.08827v1">PDF</a> </p>
<p><strong>Summary</strong><br>强化学习在推动大型语言模型进行逻辑推理方面取得了显著进展，特别是在数学和编码等复杂逻辑任务上。然而，随着领域的快速发展，强化学习在可扩展性方面面临挑战。本文回顾了强化学习在大型语言模型及大型推理模型中的应用，包括基础组件、核心问题、训练资源和下游应用，为未来研究提供了方向。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>强化学习在大型语言模型进行逻辑推理方面取得显著进展。</li>
<li>强化学习在数学和编码等复杂逻辑任务上的表现尤为突出。</li>
<li>强化学习在可扩展性方面面临挑战，包括计算资源、算法设计、训练数据和基础设施等方面。</li>
<li>回顾了强化学习在大型语言模型及大型推理模型中的应用，包括基础组件、核心问题、训练资源和下游应用。</li>
<li>文章提到了从DeepSeek-R1发布以来的研究进展。</li>
<li>文章为未来研究提供了方向，希望促进强化学习在更广泛的推理模型领域的研究。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.08827">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.08827v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.08827v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.08827v1/page_5_0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="RewardDance-Reward-Scaling-in-Visual-Generation"><a href="#RewardDance-Reward-Scaling-in-Visual-Generation" class="headerlink" title="RewardDance: Reward Scaling in Visual Generation"></a>RewardDance: Reward Scaling in Visual Generation</h2><p><strong>Authors:Jie Wu, Yu Gao, Zilyu Ye, Ming Li, Liang Li, Hanzhong Guo, Jie Liu, Zeyue Xue, Xiaoxia Hou, Wei Liu, Yan Zeng, Weilin Huang</strong></p>
<p>Reward Models (RMs) are critical for improving generation models via Reinforcement Learning (RL), yet the RM scaling paradigm in visual generation remains largely unexplored. It primarily due to fundamental limitations in existing approaches: CLIP-based RMs suffer from architectural and input modality constraints, while prevalent Bradley-Terry losses are fundamentally misaligned with the next-token prediction mechanism of Vision-Language Models (VLMs), hindering effective scaling. More critically, the RLHF optimization process is plagued by Reward Hacking issue, where models exploit flaws in the reward signal without improving true quality. To address these challenges, we introduce RewardDance, a scalable reward modeling framework that overcomes these barriers through a novel generative reward paradigm. By reformulating the reward score as the model’s probability of predicting a “yes” token, indicating that the generated image outperforms a reference image according to specific criteria, RewardDance intrinsically aligns reward objectives with VLM architectures. This alignment unlocks scaling across two dimensions: (1) Model Scaling: Systematic scaling of RMs up to 26 billion parameters; (2) Context Scaling: Integration of task-specific instructions, reference examples, and chain-of-thought (CoT) reasoning. Extensive experiments demonstrate that RewardDance significantly surpasses state-of-the-art methods in text-to-image, text-to-video, and image-to-video generation. Crucially, we resolve the persistent challenge of “reward hacking”: Our large-scale RMs exhibit and maintain high reward variance during RL fine-tuning, proving their resistance to hacking and ability to produce diverse, high-quality outputs. It greatly relieves the mode collapse problem that plagues smaller models. </p>
<blockquote>
<p>奖励模型（RM）对于通过强化学习（RL）改进生成模型至关重要，但视觉生成中的RM规模范式仍在很大程度上未被探索。这主要是由于现有方法的基本局限性：基于CLIP的RM受到架构和输入模式的约束，而普遍的Bradley-Terry损失与视觉语言模型（VLM）的下一个令牌预测机制存在根本上的不匹配，阻碍了有效的扩展。更重要的是，RLHF优化过程受到奖励黑客问题的困扰，模型会利用奖励信号的缺陷，而不提高真正的质量。为了应对这些挑战，我们引入了RewardDance，这是一个可扩展的奖励建模框架，通过一种新的生成奖励范式来克服这些障碍。通过将奖励分数重新定义为模型预测“是”令牌的概率，表示生成的图像根据特定标准优于参考图像，RewardDance内在地将奖励目标与VLM架构对齐。这种对齐解锁了两个维度的扩展：（1）模型扩展：RM的系统扩展高达26亿个参数；（2）上下文扩展：整合任务特定的指令、参考示例和链状思维（CoT）推理。大量实验表明，RewardDance在文本到图像、文本到视频和图像到视频生成方面显著超越了最先进的方法。最重要的是，我们解决了“奖励黑客”这一持久挑战：我们的大规模RM在RL微调过程中表现出较高的奖励方差，证明了其抗黑客攻击的能力以及产生多样化高质量输出的能力。这极大地缓解了困扰较小模型的模式崩溃问题。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.08826v1">PDF</a> Bytedance Seed Technical Report</p>
<p><strong>Summary</strong></p>
<p>本文介绍了RewardDance这一奖励建模框架，针对视觉生成中的奖励模型（RM）扩展问题提出解决方案。该框架通过生成奖励模型范式解决了RM存在的障碍，通过与视觉语言模型（VLM）架构的内在对齐，实现模型扩展和上下文扩展。实验表明，RewardDance在文本转图像、文本转视频和图像转视频生成方面远超现有方法，解决了奖励黑客攻击的挑战，并展现出抵抗黑客攻击的能力，能够产生多样且高质量的输出，缓解了小型模型的模式崩溃问题。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>奖励模型（RM）在通过强化学习（RL）改进生成模型方面起着关键作用，但在视觉生成领域中的RM扩展模式尚未得到充分探索。</li>
<li>当前存在的RM方法存在架构和输入模式约束，以及Bradley-Terry损失与视觉语言模型（VLM）的下一个令牌预测机制之间的根本性不匹配问题。</li>
<li>RewardDance框架解决了这些问题，通过一种新的生成奖励模型范式实现与VLM架构的内在对齐。</li>
<li>RewardDance实现了两个维度的扩展：模型扩展和上下文扩展，使得RM能够扩展到26亿参数，并整合特定任务指令、参考示例和链式思维推理。</li>
<li>RewardDance在文本转图像、文本转视频和图像转视频生成方面表现出显著优势。</li>
<li>该方法解决了持续存在的“奖励黑客攻击”挑战，展现出抵抗黑客攻击的能力，并能产生多样且高质量的输出。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.08826">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.08826v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.08826v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.08826v1/page_1_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.08826v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.08826v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Merge-of-Thought-Distillation"><a href="#Merge-of-Thought-Distillation" class="headerlink" title="Merge-of-Thought Distillation"></a>Merge-of-Thought Distillation</h2><p><strong>Authors:Zhanming Shen, Zeyu Qin, Zenan Huang, Hao Chen, Jiaqi Hu, Yihong Zhuang, Guoshan Lu, Gang Chen, Junbo Zhao</strong></p>
<p>Efficient reasoning distillation for long chain-of-thought (CoT) models is increasingly constrained by the assumption of a single oracle teacher, despite practical availability of multiple candidate teachers and growing CoT corpora. We revisit teacher selection and observe that different students have different “best teachers,” and even for the same student the best teacher can vary across datasets. Therefore, to unify multiple teachers’ reasoning abilities into student with overcoming conflicts among various teachers’ supervision, we propose Merge-of-Thought Distillation (MoT), a lightweight framework that alternates between teacher-specific supervised fine-tuning branches and weight-space merging of the resulting student variants. On competition math benchmarks, using only about 200 high-quality CoT samples, applying MoT to a Qwen3-14B student surpasses strong models including DEEPSEEK-R1, QWEN3-30B-A3B, QWEN3-32B, and OPENAI-O1, demonstrating substantial gains. Besides, MoT consistently outperforms the best single-teacher distillation and the naive multi-teacher union, raises the performance ceiling while mitigating overfitting, and shows robustness to distribution-shifted and peer-level teachers. Moreover, MoT reduces catastrophic forgetting, improves general reasoning beyond mathematics and even cultivates a better teacher, indicating that consensus-filtered reasoning features transfer broadly. These results position MoT as a simple, scalable route to efficiently distilling long CoT capabilities from diverse teachers into compact students. </p>
<blockquote>
<p>针对长链条思维（CoT）模型的推理蒸馏效率越来越受到单一权威教师假设的限制，尽管实际上有多个候选教师，并且CoT语料库不断增长。我们重新审视教师选择，并观察到不同的学生有不同的“最佳教师”，即使对于同一学生，最佳教师也会因数据集而异。因此，为了将学生与多个教师的推理能力相结合，并克服各种教师监督之间的冲突，我们提出了“思维融合蒸馏”（MoT）方法。它是一个轻量级的框架，交替进行针对特定教师的监督微调分支和结果学生变量的权重空间合并。在竞赛数学基准测试中，仅使用大约200个高质量CoT样本，将MoT应用于Qwen3-14B学生，超过了DEEPSEEK-R1、QWEN3-30B-A3B、QWEN3-32B和OPENAI-O1等强大模型，显示出巨大的进步。此外，MoT始终优于最佳单教师蒸馏和简单的多教师联合方法，提高了性能上限，同时减轻了过拟合问题，并对分布偏移和教师同行表现出稳健性。而且，MoT减少了灾难性遗忘，提高了除数学之外的一般推理能力，甚至培养了更好的教师，表明经过共识过滤的推理特征具有广泛的迁移性。这些结果将MoT定位为从多种教师中有效蒸馏长CoT能力到紧凑学生的简单且可扩展的途径。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.08814v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文探讨了如何在长链思维（CoT）模型中实现有效的推理蒸馏。文章重新审视了教师选择的问题，并提出不同学生有不同的“最佳教师”，而且同一学生的最佳教师也会因数据集而异。为此，文章提出了Merge-of-Thought Distillation（MoT）框架，该框架通过交替进行教师特定的监督微调分支和权重空间的合并学生变体，将多个教师的推理能力统一到学生模型中。在竞赛数学基准测试中，使用仅约200个高质量CoT样本的MoT对Qwen3-14B学生的应用超越了其他强大模型，并表现出良好的性能提升、稳健性和减少灾难性遗忘等优点。总体而言，MoT是一种简单且可扩展的方法，可从多样化的教师中有效地蒸馏出长CoT能力并应用于紧凑的学生模型中。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文章指出在长链思维（CoT）模型的推理蒸馏中，单一教师的假设限制了效率。</li>
<li>不同学生有不同的“最佳教师”，且同一学生的最佳教师也会因数据集而异。</li>
<li>提出了Merge-of-Thought Distillation（MoT）框架，结合了多个教师的推理能力。</li>
<li>MoT通过交替进行教师特定的监督微调分支和权重空间合并，提高了学生模型的性能。</li>
<li>在竞赛数学基准测试中，MoT表现出优越的性能，超越了其他强大模型。</li>
<li>MoT具有减少灾难性遗忘、提高泛化能力和培养更好教师等优点。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.08814">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.08814v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.08814v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.08814v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.08814v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="AdsQA-Towards-Advertisement-Video-Understanding"><a href="#AdsQA-Towards-Advertisement-Video-Understanding" class="headerlink" title="AdsQA: Towards Advertisement Video Understanding"></a>AdsQA: Towards Advertisement Video Understanding</h2><p><strong>Authors:Xinwei Long, Kai Tian, Peng Xu, Guoli Jia, Jingxuan Li, Sa Yang, Yihua Shao, Kaiyan Zhang, Che Jiang, Hao Xu, Yang Liu, Jiaheng Ma, Bowen Zhou</strong></p>
<p>Large language models (LLMs) have taken a great step towards AGI. Meanwhile, an increasing number of domain-specific problems such as math and programming boost these general-purpose models to continuously evolve via learning deeper expertise. Now is thus the time further to extend the diversity of specialized applications for knowledgeable LLMs, though collecting high quality data with unexpected and informative tasks is challenging. In this paper, we propose to use advertisement (ad) videos as a challenging test-bed to probe the ability of LLMs in perceiving beyond the objective physical content of common visual domain. Our motivation is to take full advantage of the clue-rich and information-dense ad videos’ traits, e.g., marketing logic, persuasive strategies, and audience engagement. Our contribution is three-fold: (1) To our knowledge, this is the first attempt to use ad videos with well-designed tasks to evaluate LLMs. We contribute AdsQA, a challenging ad Video QA benchmark derived from 1,544 ad videos with 10,962 clips, totaling 22.7 hours, providing 5 challenging tasks. (2) We propose ReAd-R, a Deepseek-R1 styled RL model that reflects on questions, and generates answers via reward-driven optimization. (3) We benchmark 14 top-tier LLMs on AdsQA, and our \texttt{ReAd-R}~achieves the state-of-the-art outperforming strong competitors equipped with long-chain reasoning capabilities by a clear margin. </p>
<blockquote>
<p>大型语言模型（LLMs）在通用人工智能（AGI）方面迈出了重要一步。同时，越来越多的特定领域问题，如数学和编程，推动这些通用模型通过深入学习专业知识而不断发展。因此，现在是时候进一步扩展知识型LLM的专用应用程序的多样性了，尽管收集高质量的数据以及包含意想不到且具有信息含量的任务具有挑战性。在本文中，我们建议使用广告视频作为一个具有挑战性的测试平台来探索LLM感知普通视觉域客观物理内容之外的能力。我们的动机是充分利用广告视频线索丰富、信息密集的特点，例如营销逻辑、策略技巧和观众参与度等。我们的贡献分为三个部分：（1）据我们所知，这是首次尝试使用精心设计任务的广告视频来评估LLM。我们贡献了AdsQA，一个由精心设计的任务得出的广告视频问答基准测试数据集，该数据集由精心挑选的涵盖多达超过2万多个广告的多个片段构成（共计约一万五千四百四十四分钟），包含五个具有挑战性的任务。（2）我们提出了ReAd-R模型，这是一个基于Deepseek-R1风格的强化学习模型，它通过对问题的反思并通过奖励驱动优化生成答案。（3）我们在AdsQA基准测试数据集上对顶尖的十四个LLM进行了评估比较，结果显示我们的ReAd-R模型明显超越其他具备长链推理能力的竞争对手，达到了业界领先水平。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.08621v1">PDF</a> ICCV-2025</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLMs）在通用人工智能（AGI）方面取得了重大进展。针对特定领域的挑战如数学和编程，这些通用模型通过深入学习专业知识而不断进化。当前，拓展具有专业知识LLM的多样化应用是关键，但收集高质量数据并执行意外且具有信息含量的任务仍面临挑战。本文提出利用广告视频作为测试平台，探索LLMs在视觉领域之外的感知能力。通过利用广告视频中的线索丰富和信息密集的特点，如营销逻辑、说服策略和观众参与度，我们进行了开创性的工作。本文的主要贡献包括：一是创建首个使用广告视频和任务评估LLMs的AdsQA基准测试；二是提出通过奖励驱动优化的ReAd-R模型；三是在AdsQA上评估了顶级LLMs的表现，ReAd-R模型凭借出色的长链推理能力达到最优水平。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLMs）在迈向通用人工智能（AGI）方面取得重大进展。</li>
<li>LLMs借助数学和编程等特定领域的挑战持续进化。</li>
<li>目前面临的挑战在于为知识型LLMs拓展多样化的应用，并收集高质量的数据以执行具有挑战性的任务。</li>
<li>广告视频作为测试平台，用于探索LLMs在视觉领域之外的感知能力。</li>
<li>广告视频具有营销逻辑、说服策略和观众参与度等特点，可帮助评估LLMs。</li>
<li>提出首个使用广告视频与任务评估LLMs的AdsQA基准测试。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.08621">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.08621v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.08621v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.08621v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.08621v1/page_4_1.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Interpretable-Physics-Reasoning-and-Performance-Taxonomy-in-Vision-Language-Models"><a href="#Interpretable-Physics-Reasoning-and-Performance-Taxonomy-in-Vision-Language-Models" class="headerlink" title="Interpretable Physics Reasoning and Performance Taxonomy in   Vision-Language Models"></a>Interpretable Physics Reasoning and Performance Taxonomy in   Vision-Language Models</h2><p><strong>Authors:Pranav Pawar, Kavish Shah, Akshat Bhalani, Komal Kasat, Dev Mittal, Hadi Gala, Deepali Patil, Nikita Raichada, Monali Deshmukh</strong></p>
<p>As Vision-Language Models (VLMs) grow in sophistication, their ability to perform reasoning is coming under increasing supervision. While they excel at many tasks, their grasp of fundamental scientific principles, such as physics, remains an underexplored frontier. To reflect the advancements in these capabilities, we introduce a novel and accessible framework designed to rigorously evaluate VLMs on their understanding of 2D physics. Our framework features a pragmatic scenario generator that creates a diverse testbed of over 400 problems across four core domains: Projectile Motion, Collision Dynamics, Mechanics, and Fluid Dynamics. Through comprehensive evaluation of four state-of-the-art VLMs, we demonstrate a strong correlation between model scale and reasoning ability, with our top-performing model, Qwen2.5-VL-7B, achieving an overall score of 0.815. We find that while models excel at formulaic problems, they struggle significantly with domains requiring abstract spatial reasoning. By designing this framework, we aim to democratize the study of scientific reasoning in VLMs and foster deeper insights into their capabilities and limitations. </p>
<blockquote>
<p>随着视觉语言模型（VLMs）的日益成熟，它们进行推理的能力正在受到越来越多的关注。虽然它们在许多任务上表现出色，但它们对基本科学原理，如物理学的掌握，仍然是一个未被充分探索的领域。为了反映这些能力的进步，我们引入了一个新颖且易于访问的框架，旨在严格评估VLM对二维物理学的理解。我们的框架特点是一个实用场景生成器，它创建了四个核心领域的400多个问题的测试床：抛体运动、碰撞动力学、机械和流体动力学。通过对四个最先进的VLM的综合评估，我们证明了模型规模与推理能力之间的强烈相关性，我们表现最佳的模型Qwen2.5-VL-7B的总体得分为0.815。我们发现，虽然模型在公式问题上表现良好，但在需要抽象空间推理的领域，它们会遇到很大的困难。通过设计这个框架，我们的目标是使VLM中的科学研究民主化，并加深对它们的能力和局限性的深入了解。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.08270v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>随着视觉语言模型（VLMs）的日益成熟，它们进行推理的能力正在受到越来越多的关注。虽然它们在许多任务上表现出色，但对于物理等基本原理的理解仍是尚未充分探索的领域。为了反映这些能力的进步，我们引入了一个新颖且实用的框架，旨在严格评估VLMs对二维物理的理解。该框架配备了一个实用场景生成器，可创建涵盖四个核心领域的超过400个问题的测试床：抛体运动、碰撞动力学、机械和流体动力学。通过对四款最先进的VLMs的综合评估，我们发现在模型规模和推理能力之间存在很强的相关性，其中表现最佳的模型Qwen2.5-VL-7B总体得分达到0.815。我们发现，虽然模型在公式问题上表现出色，但在需要抽象空间推理的领域却表现不佳。通过设计此框架，我们旨在推动视觉语言模型中的科学推理研究，并加深对其能力和局限性的了解。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VLMs在理解和处理物理基本原理方面是一个尚未充分探索的领域。</li>
<li>为了评估VLMs对二维物理的理解，引入了一个新颖的实用框架。</li>
<li>该框架包含超过400个问题的测试床，涵盖四个核心领域：抛体运动、碰撞动力学、机械和流体动力学。</li>
<li>VLMs模型规模与推理能力之间存在强烈的相关性。</li>
<li>Qwen2.5-VL-7B模型在评估中表现最佳，总体得分0.815。</li>
<li>VLMs在公式问题上表现良好，但在需要抽象空间推理的领域表现较差。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.08270">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.08270v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.08270v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.08270v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.08270v1/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.08270v1/page_2_2.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.08270v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.08270v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.08270v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Attribute-based-Object-Grounding-and-Robot-Grasp-Detection-with-Spatial-Reasoning"><a href="#Attribute-based-Object-Grounding-and-Robot-Grasp-Detection-with-Spatial-Reasoning" class="headerlink" title="Attribute-based Object Grounding and Robot Grasp Detection with Spatial   Reasoning"></a>Attribute-based Object Grounding and Robot Grasp Detection with Spatial   Reasoning</h2><p><strong>Authors:Houjian Yu, Zheming Zhou, Min Sun, Omid Ghasemalizadeh, Yuyin Sun, Cheng-Hao Kuo, Arnie Sen, Changhyun Choi</strong></p>
<p>Enabling robots to grasp objects specified through natural language is essential for effective human-robot interaction, yet it remains a significant challenge. Existing approaches often struggle with open-form language expressions and typically assume unambiguous target objects without duplicates. Moreover, they frequently rely on costly, dense pixel-wise annotations for both object grounding and grasp configuration. We present Attribute-based Object Grounding and Robotic Grasping (OGRG), a novel framework that interprets open-form language expressions and performs spatial reasoning to ground target objects and predict planar grasp poses, even in scenes containing duplicated object instances. We investigate OGRG in two settings: (1) Referring Grasp Synthesis (RGS) under pixel-wise full supervision, and (2) Referring Grasp Affordance (RGA) using weakly supervised learning with only single-pixel grasp annotations. Key contributions include a bi-directional vision-language fusion module and the integration of depth information to enhance geometric reasoning, improving both grounding and grasping performance. Experiment results show that OGRG outperforms strong baselines in tabletop scenes with diverse spatial language instructions. In RGS, it operates at 17.59 FPS on a single NVIDIA RTX 2080 Ti GPU, enabling potential use in closed-loop or multi-object sequential grasping, while delivering superior grounding and grasp prediction accuracy compared to all the baselines considered. Under the weakly supervised RGA setting, OGRG also surpasses baseline grasp-success rates in both simulation and real-robot trials, underscoring the effectiveness of its spatial reasoning design. Project page: <a target="_blank" rel="noopener" href="https://z.umn.edu/ogrg">https://z.umn.edu/ogrg</a> </p>
<blockquote>
<p>实现通过自然语言指定对象使机器人进行抓取，对于有效的人机交互至关重要，但仍是一项重大挑战。现有方法常常难以处理开放形式的语言表达，并通常假设没有重复的目标对象且非常明确。此外，它们通常依赖昂贵的密集像素级注释来进行对象定位和抓取配置。我们提出了基于属性的对象定位和机器人抓取（OGRG），这是一种新型框架，能够解释开放形式的语言表达并进行空间推理，以定位目标对象并预测平面抓取姿态，即使在包含重复对象实例的场景中也是如此。我们在两种设置下研究了OGRG：（1）像素级全监督下的参照抓取合成（RGS）；（2）使用仅单像素抓取注释的弱监督学习进行的参照抓取功能（RGA）。主要贡献包括双向视觉语言融合模块和集成深度信息以增强几何推理，从而提高定位和抓取性能。实验结果表明，OGRG在具有各种空间语言指令的桌面场景中优于强大的基准线。在RGS中，它在单个NVIDIA RTX 2080 Ti GPU上的运行速度为17.59 FPS，能够在闭环或多对象顺序抓取中具有潜在用途，同时与所有考虑的基准线相比，提供优越的定地和抓取预测精度。在弱监督的RGA设置下，OGRG在模拟和真实机器人试验中的抓取成功率也超过了基线，这突出了其空间推理设计的有效性。项目页面：<a target="_blank" rel="noopener" href="https://z.umn.edu/ogrg">https://z.umn.edu/ogrg</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.08126v1">PDF</a> Accepted to 2025 IEEE-RAS 24th International Conference on Humanoid   Robots</p>
<p><strong>Summary</strong><br>     本研究提出一种基于属性的目标物体定位与机器人抓取（OGRG）的新框架，能够解析自然语言描述，进行空间推理，定位目标物体并预测平面抓取姿态，即使场景中存在重复物体实例。研究包括两种情境：在像素级全监督下的指向抓取合成（RGS），以及使用仅单点像素抓取注释的弱监督学习的指向抓取适用性（RGA）。关键贡献包括双向视觉-语言融合模块和深度信息集成，以增强几何推理，提高定位和抓取性能。实验结果表明，OGRG在具有各种空间语言指令的桌面场景中优于强基线。在RGS中，它在单一NVIDIA RTX 2080 Ti GPU上的运行速度为每秒17.59帧，具有潜在用于闭环或多目标连续抓取的能力，同时与所有考虑的基线相比，具有优越的定位和抓取预测准确性。在弱监督的RGA环境下，OGRG在模拟和真实机器人试验中的抓取成功率也超过了基线。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>机器人通过自然语言指令进行物体抓取对于有效的人机交互至关重要，但仍具挑战。</li>
<li>现有方法常面临开放式语言表达的问题，并假设无歧义的目标物体，无重复实例。</li>
<li>所提出的OGRG框架能解析开放式语言表达，进行空间推理，定位目标物体并预测抓取姿态。</li>
<li>OGRG适用于场景中存在重复物体实例的情况。</li>
<li>OGRG包括两种情境：像素级全监督下的RGS和弱监督学习的RGA。</li>
<li>关键贡献包括双向视觉-语言融合模块和深度信息集成。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.08126">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.08126v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.08126v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.08126v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.08126v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.08126v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="RLFactory-A-Plug-and-Play-Reinforcement-Learning-Post-Training-Framework-for-LLM-Multi-Turn-Tool-Use"><a href="#RLFactory-A-Plug-and-Play-Reinforcement-Learning-Post-Training-Framework-for-LLM-Multi-Turn-Tool-Use" class="headerlink" title="RLFactory: A Plug-and-Play Reinforcement Learning Post-Training   Framework for LLM Multi-Turn Tool-Use"></a>RLFactory: A Plug-and-Play Reinforcement Learning Post-Training   Framework for LLM Multi-Turn Tool-Use</h2><p><strong>Authors:Jiajun Chai, Guojun Yin, Zekun Xu, Chuhuai Yue, Yi Jia, Siyu Xia, Xiaohan Wang, Jiwen Jiang, Xiaoguang Li, Chengqi Dong, Hang He, Wei Lin</strong></p>
<p>Large language models excel at basic reasoning but struggle with tasks that require interaction with external tools. We present RLFactory, a plug-and-play reinforcement learning post-training framework for multi-round tool use. RLFactory tackles (i) tool-call stability and adaptability amid tool heterogeneity and interface issues via an asyncio-based asynchronous caller and a decoupled tool&#x2F;training architecture, and (ii) diverse evaluation needs via a reward layer supporting rule-based, model-judgment, and tool-verification signals. It reconstructs the MDP by introducing observation markers from tool feedback, closing the loop among model, tools, and environment, and implements a generate-parse-invoke-update workflow for dynamic policy optimization. On Search-R1 with Qwen3-4B, RLFactory achieves a 0.486 test score on the Natural Questions (NQ) dataset, surpassing larger models trained with similar techniques (e.g., Qwen2.5-7B-Instruct-GRPO at 0.473), and increases training throughput by 6.8x. RLFactory provides a low-barrier, highly adaptable framework for strengthening multi-round tool use of LLMs in real-world scenarios. Code: <a target="_blank" rel="noopener" href="https://github.com/Simple-Efficient/RL-Factory">https://github.com/Simple-Efficient/RL-Factory</a>. </p>
<blockquote>
<p>大型语言模型在基础推理方面表现出色，但在需要与使用外部工具进行交互的任务方面却遇到困难。我们提出了RLFactory，这是一个即插即用的强化学习后训练框架，用于多轮工具使用。RLFactory解决了（i）工具调用稳定性和适应性问题，包括工具异质性和界面问题，通过基于asyncio的异步调用者和解耦的工具&#x2F;训练架构，以及（ii）通过支持基于规则、模型判断和工具验证信号的奖励层来满足不同的评估需求。它通过引入工具反馈的观察标记来重建MDP，闭合模型、工具和环境之间的循环，实现了动态策略优化的生成-解析-调用-更新工作流程。在带有Qwen3-4B的Search-R1上，RLFactory在自然问题（NQ）数据集上实现了0.486的测试成绩，超越了使用类似技术训练的大型模型（例如Qwen2.5-7B-Instruct-GRPO的0.473），并提高了6.8倍的训练吞吐量。RLFactory为增强大型语言模型在现实场景中的多轮工具使用提供了一个低门槛、高度适应的框架。代码：<a target="_blank" rel="noopener" href="https://github.com/Simple-Efficient/RL-Factory%E3%80%82">https://github.com/Simple-Efficient/RL-Factory。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.06980v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型擅长基础推理，但在需要与外界工具交互的任务上表现不佳。本研究提出了RLFactory，一个用于多轮工具使用的即插即用强化学习后训练框架。RLFactory解决了工具调用的稳定性和适应性问题，通过异步调用器和解耦的工具&#x2F;训练架构，以及支持规则基础、模型判断和工具验证信号的奖励层来满足多样的评估需求。它从工具反馈中引入观察标记，重建了MDP，实现了模型、工具和环境之间的闭环，并采用了生成-解析-调用-更新的工作流程，以实现动态策略优化。在Search-R1使用Qwen3-4B上，RLFactory在NQ测试集上达到了0.486的测试分数，超过了使用类似技术训练的更大模型（如Qwen2.5-7B-Instruct-GRPO的0.473），并将训练速度提高了6.8倍。RLFactory为加强真实场景中多轮工具使用的大型语言模型提供了一个低门槛、高度灵活的框架。更多详情访问其代码仓库：<a target="_blank" rel="noopener" href="https://github.com/Simple-Efficient/RL-Factory">链接地址</a>。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型在基础推理任务上表现优秀，但在需要工具交互的任务上表现欠佳。</li>
<li>RLFactory框架解决了工具调用的稳定性和适应性问题。</li>
<li>RLFactory通过异步调用器和解耦的工具&#x2F;训练架构来处理工具异质性和界面问题。</li>
<li>RLFactory的奖励层支持规则基础、模型判断和工具验证信号，满足多样化的评估需求。</li>
<li>RLFactory引入了观察标记来重建MDP，实现了模型、工具和环境的闭环。</li>
<li>该框架实现了生成-解析-调用-更新的工作流程，以实现动态策略优化。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.06980">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.06980v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.06980v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.06980v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.06980v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.06980v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="MachineLearningLM-Scaling-Many-shot-In-context-Learning-via-Continued-Pretraining"><a href="#MachineLearningLM-Scaling-Many-shot-In-context-Learning-via-Continued-Pretraining" class="headerlink" title="MachineLearningLM: Scaling Many-shot In-context Learning via Continued   Pretraining"></a>MachineLearningLM: Scaling Many-shot In-context Learning via Continued   Pretraining</h2><p><strong>Authors:Haoyu Dong, Pengkun Zhang, Mingzhe Lu, Yanzhen Shen, Guolin Ke</strong></p>
<p>Large language models (LLMs) possess broad world knowledge and strong general-purpose reasoning ability, yet they struggle to learn from many in-context examples on standard machine learning (ML) tasks, that is, to leverage many-shot demonstrations purely via in-context learning (ICL) without gradient descent. We introduce MachineLearningLM, a portable continued-pretraining framework that equips a general-purpose LLM with robust in-context ML capability while preserving its general knowledge and reasoning for broader chat workflows.   Our pretraining procedure synthesizes ML tasks from millions of structural causal models (SCMs), spanning shot counts up to 1,024. We begin with a random-forest teacher, distilling tree-based decision strategies into the LLM to strengthen robustness in numerical modeling. All tasks are serialized with a token-efficient prompt, enabling 3x to 6x more examples per context window and delivering up to 50x amortized throughput via batch inference.   Despite a modest setup (Qwen-2.5-7B-Instruct with LoRA rank 8), MachineLearningLM outperforms strong LLM baselines (e.g., GPT-5-mini) by an average of about 15% on out-of-distribution tabular classification across finance, physics, biology, and healthcare domains. It exhibits a striking many-shot scaling law: accuracy increases monotonically as in-context demonstrations grow from 8 to 1,024. Without any task-specific training, it attains random-forest-level accuracy across hundreds of shots. General chat capabilities, including knowledge and reasoning, are preserved: it achieves 75.4% on MMLU. </p>
<blockquote>
<p>大型语言模型（LLM）拥有广泛的世界知识和强大的通用推理能力，但在标准机器学习（ML）任务上，它们很难从多个上下文示例中学习，也就是说，它们无法仅通过上下文学习（ICL）利用多个示例演示，而无需进行梯度下降。我们引入了MachineLearningLM，这是一个便携式持续预训练框架，它使通用LLM具备强大的上下文ML功能，同时保留其一般知识和推理能力，以支持更广泛的聊天工作流程。我们的预训练程序从数百万个结构因果模型（SCM）中综合ML任务，涵盖多达1024个shot。我们以随机森林教师开始，将基于树的决策策略蒸馏到LLM中，以加强数值建模中的稳健性。所有任务都通过高效的令牌提示进行序列化，能够在每个上下文窗口中实现3到6倍的更多示例，并通过批量推理实现高达50倍的摊销吞吐量。尽管设置相对简单（使用LoRA等级8的Qwen-2.5-7B-Instruct），但MachineLearningLM在金融、物理、生物和医疗保健领域的离群表格分类方面，平均比强大的LLM基线（例如GPT-5-mini）高出约15%。它表现出惊人的多shot缩放定律：随着上下文演示从8个增加到1024个，准确性单调增加。无需任何特定任务的训练，它就能达到数百次射击的随机森林级别的准确性。同时保留了通用的聊天能力，包括知识和推理：它在MMLU上达到了75.4%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.06806v2">PDF</a> </p>
<p><strong>Summary</strong><br>大型语言模型（LLM）具有广泛的世界知识和强大的通用推理能力，但在标准机器学习（ML）任务上学习多个上下文示例时遇到困难。为解决这一问题，我们推出了MachineLearningLM，这是一个便携式持续预训练框架，为通用LLM配备了强大的上下文ML能力，同时保留了其一般知识和推理能力，用于更广泛的聊天工作流程。我们的预训练程序通过合成来自数百万结构因果模型（SCM）的ML任务来增强LLM的能力，涵盖样本数高达1024个。我们通过在LLM中注入基于树的决策策略来加强其在数值建模中的稳健性。所有任务都通过高效的提示进行序列化，能够在每个上下文窗口中提供3到6倍的示例，并通过批量推理实现高达50倍的摊销吞吐量。尽管使用了适度的设置（Qwen-2.5-7B-Instruct with LoRA rank 8），但MachineLearningLM在金融、物理、生物和医疗保健领域的离分布表格分类上平均优于强大的LLM基准测试（例如GPT-5-mini）约15%。它展现出惊人的多镜头规模效应：随着上下文演示从8个增长到1024个，准确度呈单调增长。无需任何特定的任务训练，就能达到随机森林级别的准确度。同时保留了一般聊天能力，包括知识和推理能力，在MMLU上达到75.4%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）在标准机器学习任务上学习多个上下文示例时遇到困难。</li>
<li>MachineLearningLM是一个便携式持续预训练框架，增强了LLM的上下文机器学习能力，同时保留其一般知识和推理能力。</li>
<li>预训练程序通过合成来自数百万结构因果模型（SCM）的ML任务来增强LLM的能力。</li>
<li>使用基于树的决策策略来提高LLM在数值建模中的稳健性。</li>
<li>MachineLearningLM表现出强大的多镜头规模效应，随着上下文演示的增加，准确度不断提高。</li>
<li>在多个领域（金融、物理、生物和医疗保健）的离分布表格分类任务上，MachineLearningLM优于其他LLM基准测试。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.06806">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.06806v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.06806v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.06806v2/page_4_0.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="MM-DREX-Multimodal-Driven-Dynamic-Routing-of-LLM-Experts-for-Financial-Trading"><a href="#MM-DREX-Multimodal-Driven-Dynamic-Routing-of-LLM-Experts-for-Financial-Trading" class="headerlink" title="MM-DREX: Multimodal-Driven Dynamic Routing of LLM Experts for Financial   Trading"></a>MM-DREX: Multimodal-Driven Dynamic Routing of LLM Experts for Financial   Trading</h2><p><strong>Authors:Yang Chen, Yueheng Jiang, Zhaozhao Ma, Yuchen Cao, Jacky Keung, Kun Kuang, Leilei Gan, Yiquan Wu, Fei Wu</strong></p>
<p>The inherent non-stationarity of financial markets and the complexity of multi-modal information pose significant challenges to existing quantitative trading models. Traditional methods relying on fixed structures and unimodal data struggle to adapt to market regime shifts, while large language model (LLM)-driven solutions - despite their multi-modal comprehension - suffer from static strategies and homogeneous expert designs, lacking dynamic adjustment and fine-grained decision mechanisms. To address these limitations, we propose MM-DREX: a Multimodal-driven, Dynamically-Routed EXpert framework based on large language models. MM-DREX explicitly decouples market state perception from strategy execution to enable adaptive sequential decision-making in non-stationary environments. Specifically, it (1) introduces a vision-language model (VLM)-powered dynamic router that jointly analyzes candlestick chart patterns and long-term temporal features to allocate real-time expert weights; (2) designs four heterogeneous trading experts (trend, reversal, breakout, positioning) generating specialized fine-grained sub-strategies; and (3) proposes an SFT-RL hybrid training paradigm to synergistically optimize the router’s market classification capability and experts’ risk-adjusted decision-making. Extensive experiments on multi-modal datasets spanning stocks, futures, and cryptocurrencies demonstrate that MM-DREX significantly outperforms 15 baselines (including state-of-the-art financial LLMs and deep reinforcement learning models) across key metrics: total return, Sharpe ratio, and maximum drawdown, validating its robustness and generalization. Additionally, an interpretability module traces routing logic and expert behavior in real time, providing an audit trail for strategy transparency. </p>
<blockquote>
<p>金融市场固有的非稳定性和多模态信息的复杂性对现有量化交易模型构成了重大挑战。传统方法依赖于固定结构和单模态数据，难以适应市场状态变化，而大型语言模型（LLM）驱动的解决方案虽然具有多模态理解能力，但却存在静态策略和同质化专家设计的问题，缺乏动态调整和精细决策机制。为了解决这些局限性，我们提出了MM-DREX：一个基于大型语言模型的多模态驱动、动态路由专家框架。MM-DREX显式地将市场状态感知与策略执行解耦，以实现在非稳定环境中的自适应序列决策。具体来说，它（1）引入了一个由视觉语言模型（VLM）驱动的动态路由器，该路由器联合分析K线图模式和长期时间特征来分配实时专家权重；（2）设计了四种异质交易专家（趋势、反转、突破、定位），生成专业的精细子策略；（3）提出了一种SFT-RL混合训练范式，协同优化路由器的市场分类能力和专家的风险调整决策。在涵盖股票、期货和加密货币的多模态数据集上的大量实验表明，MM-DREX在关键指标上显著优于15个基准模型（包括最先进的金融LLM和深度强化学习模型），这些指标包括总回报、夏普比率和最大回撤，验证了其稳健性和泛化能力。此外，解释性模块实时跟踪路由逻辑和专家行为，为策略透明性提供审计跟踪。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.05080v2">PDF</a> </p>
<p><strong>Summary</strong><br>金融市场的非平稳性和多模态信息的复杂性对现有量化交易模型提出了重大挑战。传统方法难以适应市场状态变化，而大型语言模型驱动的方案虽然具备多模态理解能力，但策略静态、设计单一。为此，我们提出MM-DREX框架，基于大型语言模型的多模态驱动、动态路由机制，通过感知市场状态与执行策略解耦，实现非平稳环境下的自适应序列决策。实验证明，MM-DREX在股票、期货和加密货币等多模态数据集上的表现优于15种基线方法，具备强大的稳健性和泛化能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>金融市场非平稳性和多模态信息复杂性对量化交易模型构成挑战。</li>
<li>传统方法难以适应市场变化，需要新的策略和方法来处理多模态数据。</li>
<li>MM-DREX框架提出一种基于大型语言模型的多模态驱动、动态路由机制。</li>
<li>MM-DREX通过感知市场状态与执行策略解耦，实现自适应序列决策。</li>
<li>MM-DREX框架包括一个动态路由器和四种交易专家，生成精细的子策略。</li>
<li>实验证明MM-DREX在多个数据集上的表现优于其他方法，具备强大的稳健性和泛化能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.05080">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.05080v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.05080v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.05080v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.05080v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="ACE-RL-Adaptive-Constraint-Enhanced-Reward-for-Long-form-Generation-Reinforcement-Learning"><a href="#ACE-RL-Adaptive-Constraint-Enhanced-Reward-for-Long-form-Generation-Reinforcement-Learning" class="headerlink" title="ACE-RL: Adaptive Constraint-Enhanced Reward for Long-form Generation   Reinforcement Learning"></a>ACE-RL: Adaptive Constraint-Enhanced Reward for Long-form Generation   Reinforcement Learning</h2><p><strong>Authors:Jianghao Chen, Wei Sun, Qixiang Yin, Lingxing Kong, Zhixing Tan, Jiajun Zhang</strong></p>
<p>Large Language Models (LLMs) have demonstrated remarkable progress in long-context understanding, yet they face significant challenges in high-quality long-form generation. Existing studies primarily suffer from two limitations: (1) A heavy reliance on scarce, high-quality long-form response data for supervised fine-tuning (SFT) or for pairwise preference reward in reinforcement learning (RL). (2) Focus on coarse-grained quality optimization dimensions, such as relevance, coherence, and helpfulness, overlooking the fine-grained specifics inherent to diverse long-form generation scenarios. To address this issue, we propose a framework using Adaptive Constraint-Enhanced reward for long-form generation Reinforcement Learning (ACE-RL). ACE-RL first automatically deconstructs each instruction into a set of fine-grained, adaptive constraint criteria by identifying its underlying intents and demands. Subsequently, we design a reward mechanism that quantifies the quality of long-form responses based on their satisfaction over corresponding constraints, converting subjective quality evaluation into constraint verification. Finally, we utilize reinforcement learning to guide models toward superior long-form generation capabilities. Experimental results demonstrate that our ACE-RL framework significantly outperforms existing SFT and RL baselines by 20.70% and 7.32% on WritingBench, and our top-performing model even surpasses proprietary systems like GPT-4o by 7.10%, providing a more effective training paradigm for LLMs to generate high-quality content across diverse long-form generation scenarios. </p>
<blockquote>
<p>大型语言模型（LLM）在理解长文本方面取得了显著的进步，但在高质量长文本生成方面仍面临重大挑战。现有研究主要存在两个局限性：（1）过于依赖稀缺的高质量长文本响应数据进行监督微调（SFT）或在强化学习（RL）中的成对偏好奖励。（2）关注粗粒度的质量优化维度，如相关性、连贯性和有用性，忽视了长文本生成场景中固有的细粒度细节。为了解决这一问题，我们提出了使用自适应约束增强奖励的长文本生成强化学习（ACE-RL）框架。ACE-RL首先自动将每个指令分解为一组细粒度的自适应约束标准，通过识别其潜在意图和需求来实现。随后，我们设计了一种奖励机制，根据长文本响应对相应约束的满足程度来量化其质量，将主观质量评价转化为约束验证。最后，我们使用强化学习来指导模型实现更出色的长文本生成能力。实验结果表明，我们的ACE-RL框架在WritingBench上的表现优于现有的SFT和RL基线，分别提高了20.70%和7.32%，我们表现最佳的模型甚至超过了如GPT-4o等专有系统，达到了7.10%，为LLM生成高质量内容提供了更有效的训练范式，适用于多种长文本生成场景。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.04903v2">PDF</a> Under review, our code is available at <a target="_blank" rel="noopener" href="https://github.com/ZNLP/ACE-RL">https://github.com/ZNLP/ACE-RL</a></p>
<p><strong>Summary</strong>：大型语言模型在理解长文本方面取得了显著进展，但在高质量长文本生成方面仍面临挑战。现有研究主要存在两个局限性：（1）过于依赖稀缺的高质量长文本响应数据进行监督微调或强化学习中的配对偏好奖励；（2）关注粗略的质量优化维度，如相关性、连贯性和有用性，忽视了长文本生成场景中固有的细微差异。为解决这一问题，提出了一种使用自适应约束增强奖励的长文本生成强化学习框架（ACE-RL）。ACE-RL首先自动将每个指令分解为一组精细的、自适应约束标准，通过识别其潜在意图和需求。随后，设计了一种奖励机制，根据长文本响应对相应约束的满足程度来量化其质量，将主观质量评价转化为约束验证。最后，利用强化学习指导模型实现更优质的长文本生成能力。实验结果表明，ACE-RL框架在WritingBench上的表现优于现有的监督微调（SFT）和强化学习基线方法，分别提高了20.70%和7.32%，甚至超过GPT-4o系统7.1%，为大型语言模型生成高质量内容提供了更有效的训练范式。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>大型语言模型在长文本理解方面取得了显著进展，但在高质量长文本生成上存在挑战。</li>
<li>现有研究主要依赖稀缺的高质量长文本数据进行监督微调或强化学习中的配对偏好奖励。</li>
<li>现有研究主要关注粗略的质量优化维度，忽视了长文本生成场景中细微的差异。</li>
<li>ACE-RL框架提出自动将指令分解为精细的、自适应约束标准的方法。</li>
<li>ACE-RL设计了一种奖励机制，根据长文本响应对约束的满足程度量化其质量。</li>
<li>ACE-RL利用强化学习指导模型实现更优质的长文本生成能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.04903">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.04903v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.04903v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.04903v2/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.04903v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.04903v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.04903v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Advancing-SLM-Tool-Use-Capability-using-Reinforcement-Learning"><a href="#Advancing-SLM-Tool-Use-Capability-using-Reinforcement-Learning" class="headerlink" title="Advancing SLM Tool-Use Capability using Reinforcement Learning"></a>Advancing SLM Tool-Use Capability using Reinforcement Learning</h2><p><strong>Authors:Dhruvi Paprunia, Vansh Kharidia, Pankti Doshi</strong></p>
<p>In an era where tool-augmented AI agents are becoming increasingly vital, our findings highlight the ability of Group Relative Policy Optimization (GRPO) to empower SLMs, which are traditionally constrained in tool use. The ability to use tools effectively has become a defining feature of Large Language Models (LLMs), allowing them to access external data and internal resources. As AI agents grow more sophisticated, tool-use capabilities have become indispensable. While LLMs have made significant progress in this area, Small Language Models (SLMs) still face challenges in accurately integrating tool use, especially in resource-constrained settings.   This study investigates how Reinforcement Learning, specifically Group Relative Policy Optimization (GRPO), can enhance the tool-use accuracy of SLMs. By designing a well-defined reward system that reinforces structured JSON output, correct tool selection, and precise parameter usage, we demonstrate that GRPO enables SLMs to achieve significant improvements in tool-use capabilities (function calling&#x2F;JSON output). Our approach provides a computationally efficient training method that enhances SLMs practical deployment in real-world AI applications. </p>
<blockquote>
<p>在工具增强型AI代理日益重要的时代，我们的研究突出了群体相对策略优化（GRPO）在赋能传统上在工具使用方面受限的小型语言模型（SLM）的能力。有效地使用工具已成为大型语言模型（LLM）的标志性特征，使它们能够访问外部数据和内部资源。随着人工智能代理日益成熟，工具使用能力已成为不可或缺的部分。虽然大型语言模型在该领域取得了重大进展，但小型语言模型在准确整合工具使用方面仍面临挑战，特别是在资源受限的环境中。本研究探讨了强化学习，特别是群体相对策略优化（GRPO），如何增强小型语言模型（SLM）的工具使用准确性。通过设计明确的奖励系统，强化结构化JSON输出、正确的工具选择和精确的参数使用，我们证明了GRPO使小型语言模型能够在工具使用能力（函数调用&#x2F;JSON输出）方面取得重大改进。我们的方法提供了一种计算效率高的训练方法，提高了小型语言模型在实际部署到现实世界AI应用中的实用性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.04518v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>工具增强型AI代理的时代，GRPO能助力传统上在工具使用方面受限的小型语言模型（SLMs）。大型语言模型（LLMs）在使用工具方面的能力成为其标志性特征，可以访问外部数据和内部资源。虽然LLMs在此方面已取得显著进展，但SLMs在准确整合工具使用方面仍面临挑战，特别是在资源受限的环境中。本研究探讨强化学习中的群体相对策略优化（GRPO）如何提升SLMs的工具使用准确性。通过设计明确的奖励系统，强化结构化的JSON输出、正确的工具选择和精确的参数使用，研究证明GRPO使SLMs在工具使用能力上实现了显著的提升（函数调用&#x2F;JSON输出）。我们的方法提供了一种计算效率高的训练方法，提高了SLMs在实际AI应用中的部署能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Group Relative Policy Optimization (GRPO) 可以助力小型语言模型（SLMs）在工具使用方面的能力。</li>
<li>大型语言模型（LLMs）的工具使用能力已成为其标志性特征。</li>
<li>在资源受限的环境中，SLMs在准确整合工具使用方面面临挑战。</li>
<li>强化学习中的GRPO通过明确的奖励系统提升SLMs的工具使用准确性。</li>
<li>GRPO通过强化结构化的JSON输出、正确的工具选择和精确的参数使用来实现提升。</li>
<li>GRPO使SLMs在工具使用能力上实现了显著的提升，如函数调用和JSON输出。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.04518">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.04518v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.04518v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.04518v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.04518v2/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.04518v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.04518v2/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.04518v2/page_4_2.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="EvoEmo-Towards-Evolved-Emotional-Policies-for-LLM-Agents-in-Multi-Turn-Negotiation"><a href="#EvoEmo-Towards-Evolved-Emotional-Policies-for-LLM-Agents-in-Multi-Turn-Negotiation" class="headerlink" title="EvoEmo: Towards Evolved Emotional Policies for LLM Agents in Multi-Turn   Negotiation"></a>EvoEmo: Towards Evolved Emotional Policies for LLM Agents in Multi-Turn   Negotiation</h2><p><strong>Authors:Yunbo Long, Liming Xu, Lukas Beckenbauer, Yuhan Liu, Alexandra Brintrup</strong></p>
<p>Recent research on Chain-of-Thought (CoT) reasoning in Large Language Models (LLMs) has demonstrated that agents can engage in \textit{complex}, \textit{multi-turn} negotiations, opening new avenues for agentic AI. However, existing LLM agents largely overlook the functional role of emotions in such negotiations, instead generating passive, preference-driven emotional responses that make them vulnerable to manipulation and strategic exploitation by adversarial counterparts. To address this gap, we present EvoEmo, an evolutionary reinforcement learning framework that optimizes dynamic emotional expression in negotiations. EvoEmo models emotional state transitions as a Markov Decision Process and employs population-based genetic optimization to evolve high-reward emotion policies across diverse negotiation scenarios. We further propose an evaluation framework with two baselines – vanilla strategies and fixed-emotion strategies – for benchmarking emotion-aware negotiation. Extensive experiments and ablation studies show that EvoEmo consistently outperforms both baselines, achieving higher success rates, higher efficiency, and increased buyer savings. This findings highlight the importance of adaptive emotional expression in enabling more effective LLM agents for multi-turn negotiation. </p>
<blockquote>
<p>关于大型语言模型（LLM）中的思维链（CoT）推理的最新研究表明，智能体可以参与复杂的多轮谈判，为智能体人工智能开辟了新的途径。然而，现有的LLM智能体在很大程度上忽视了情绪在这种谈判中的功能作用，而是产生被动、偏好驱动的情绪反应，使它们容易受到对抗性对手的操纵和战略利用。为了解决这一空白，我们提出了EvoEmo，这是一个进化强化学习框架，旨在优化谈判中的动态情绪表达。EvoEmo将情绪状态转换建模为马尔可夫决策过程，并基于种群遗传优化来演化不同谈判场景下的高回报情绪策略。我们进一步提出了一个评估框架，其中包括两个基准线——普通策略和固定情绪策略——用于评估情感感知谈判。广泛的实验和消融研究表明，EvoEmo始终优于这两个基准线，实现了更高的成功率、更高的效率和更高的买家节省。这些发现突显了在多轮谈判中自适应情绪表达的重要性，使LLM智能体更加有效。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.04310v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>近期的Chain-of-Thought（CoT）推理研究表明，大型语言模型（LLM）可以在多轮谈判中发挥复杂作用，开启了智能代理的新方向。然而，现有LLM代理大多忽视了情绪在这种谈判中的功能作用，而是产生被动、偏好驱动的情绪反应，使其容易受到对手的战略操纵和剥削。为解决这一缺陷，我们提出了EvoEmo，一个进化强化学习框架，优化谈判中的动态情绪表达。EvoEmo将情绪状态转换建模为马尔可夫决策过程，并利用基于种群遗传优化算法来进化不同谈判场景下的高回报情绪策略。我们还提出了一个评估框架，其中包括基准策略和固定情绪策略两种基准线方法。经过大量实验和去除研究表明，EvoEmo表现优秀且能持续发挥作用，与这两种基线相比具有更高的成功率、更高的效率和更高的买家节省率。这表明在复杂的多轮谈判中，自适应的情绪表达对于LLM代理至关重要。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LLMs具备进行复杂多轮谈判的能力，但现有代理在谈判中忽视了情绪的作用。</li>
<li>EvoEmo是一个强化学习框架，旨在优化LLM在谈判中的动态情绪表达。</li>
<li>EvoEmo将情绪状态转换建模为马尔可夫决策过程并运用遗传算法来进化情绪策略。</li>
<li>评估框架包括基准策略和固定情绪策略两种基准线方法。</li>
<li>实验结果表明，EvoEmo在成功率、效率和买家节省率方面优于现有基线策略。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.04310">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.04310v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.04310v2/page_2_0.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Emergent-Hierarchical-Reasoning-in-LLMs-through-Reinforcement-Learning"><a href="#Emergent-Hierarchical-Reasoning-in-LLMs-through-Reinforcement-Learning" class="headerlink" title="Emergent Hierarchical Reasoning in LLMs through Reinforcement Learning"></a>Emergent Hierarchical Reasoning in LLMs through Reinforcement Learning</h2><p><strong>Authors:Haozhe Wang, Qixin Xu, Che Liu, Junhong Wu, Fangzhen Lin, Wenhu Chen</strong></p>
<p>Reinforcement Learning (RL) has proven highly effective at enhancing the complex reasoning abilities of Large Language Models (LLMs), yet underlying mechanisms driving this success remain largely opaque. Our analysis reveals that puzzling phenomena like <code>aha moments&quot;, </code>length-scaling’’ and entropy dynamics are not disparate occurrences but hallmarks of an emergent reasoning hierarchy, akin to the separation of high-level strategic planning from low-level procedural execution in human cognition. We uncover a compelling two-phase dynamic: initially, a model is constrained by procedural correctness and must improve its low-level skills. The learning bottleneck then decisively shifts, with performance gains being driven by the exploration and mastery of high-level strategic planning. This insight exposes a core inefficiency in prevailing RL algorithms like GRPO, which apply optimization pressure agnostically and dilute the learning signal across all tokens. To address this, we propose HIerarchy-Aware Credit Assignment (HICRA), an algorithm that concentrates optimization efforts on high-impact planning tokens. HICRA significantly outperforms strong baselines, demonstrating that focusing on this strategic bottleneck is key to unlocking advanced reasoning. Furthermore, we validate semantic entropy as a superior compass for measuring strategic exploration over misleading metrics such as token-level entropy. </p>
<blockquote>
<p>强化学习（RL）已经证明可以有效地提高大型语言模型（LLM）的复杂推理能力，然而驱动这一成功的潜在机制仍然大部分未知。我们的分析揭示了一些令人困惑的现象，如“啊哈时刻”、“长度缩放”和熵动力学，它们并不是孤立的事件，而是新兴推理层次的标志，类似于人类认知中高级战略规划与低级程序执行的分离。我们发现了两个阶段的动态过程：最初，模型受到程序正确性的约束，必须提高其低级技能。学习瓶颈随后发生决定性转移，性能提升源于高级战略规划的探索与掌握。这种见解揭示了现行RL算法（如GRPO）的核心低效之处，这些算法盲目地施加优化压力，并将学习信号稀释到所有令牌中。为了解决这个问题，我们提出了HIerarchy-Aware Credit Assignment (HICRA)算法，该算法将优化工作集中在对规划令牌影响大的地方。HICRA显著优于强大的基线，证明关注这一战略瓶颈是解锁高级推理的关键。此外，我们验证了语义熵作为衡量战略探索的指南针优于令牌级熵等误导性指标。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.03646v2">PDF</a> Preprint</p>
<p><strong>Summary</strong><br>强化学习在提升大型语言模型的复杂推理能力方面表现出显著效果，但其背后的机制仍大多未知。我们的分析发现，“啊哈时刻”、“长度缩放”和熵动力等令人困惑的现象并不是孤立的事件，而是新兴推理层次的标志，类似于人类认知中高级战略规划与低级程序执行的分离。我们揭示了引人注目的两阶段动态过程：初期，模型受程序正确性约束，必须提高其低级技能；随后，学习瓶颈发生决定性转移，性能提升得益于高级战略规划的探究与掌握。为此，我们提出了层次感知信用分配（HICRA）算法，集中优化努力于高影响力规划令牌，显著优于强有力的基线，证明了解决这一战略瓶颈的关键在于专注于先进推理的解锁。我们还验证了语义熵作为衡量战略探索的优越指标，相较于误导性指标如令牌级熵更为有效。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>强化学习在提升大型语言模型的推理能力上表现显著，但其背后的机制尚待深入了解。</li>
<li>观察到如“啊哈时刻”等令人困惑的现象其实是推理层次出现的标志。</li>
<li>模型学习分为两个阶段：初级阶段注重提高低级技能，随后转移到高级战略规划和探索。</li>
<li>当前强化学习算法如GRPO存在核心低效问题，优化压力应用过于广泛。</li>
<li>提出了层次感知信用分配（HICRA）算法，集中优化努力于关键的战略规划令牌。</li>
<li>HICRA算法显著优于现有基线，表明解决战略瓶颈是解锁高级推理的关键。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.03646">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.03646v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.03646v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.03646v2/page_4_0.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="DCPO-Dynamic-Clipping-Policy-Optimization"><a href="#DCPO-Dynamic-Clipping-Policy-Optimization" class="headerlink" title="DCPO: Dynamic Clipping Policy Optimization"></a>DCPO: Dynamic Clipping Policy Optimization</h2><p><strong>Authors:Shihui Yang, Chengfeng Dou, Peidong Guo, Kai Lu, Qiang Ju, Fei Deng, Rihui Xin</strong></p>
<p>Reinforcement Learning from Verifiable Rewards (RLVR) has emerged as a promising framework for enhancing the reasoning capabilities of large language models. However, existing approaches such as GRPO often suffer from zero gradients. This problem arises primarily due to fixed clipping bounds for token-level probability ratios and the standardization of identical rewards, which can lead to ineffective gradient updates and underutilization of generated responses. In this work, we propose Dynamic Clipping Policy Optimization(DCPO), which introduces a dynamic clipping strategy that adaptively adjusts clipping bounds based on token-specific prior probabilities to enhance token-level exploration, and a smooth advantage standardization technique that standardizes rewards across cumulative training steps to improve the response-level effective utilization of generated responses. DCPO achieved state-of-the-art performance on four benchmarks based on four different models. In particular, DCPO achieved an Avg@1 of 46.7 under greedy decoding and an Avg@32 of 38.8 under 32 times sampling on the AIME24 benchmark, surpassing DAPO (36.7&#x2F;31.6), GRPO (36.7&#x2F;32.1) and GSPO (40.0&#x2F;34.9) on the Qwen2.5-Math-7B model. On the AIME25 benchmark based on Qwen2.5-14B, DCPO achieves a performance of (23.3&#x2F;19.0), surpassing GRPO (13.3&#x2F;10.5), DAPO (20.0&#x2F;15.3) and GSPO (16.7&#x2F;9.9). Furthermore, DCPO achieved an average 28% improvement in the nonzero advantage over GRPO in four models, doubled the training efficiency over DAPO, and significantly reduced the token clipping ratio by an order of magnitude compared to both GRPO and DAPO, while achieving superior performance. These results highlight DCPO’s effectiveness in leveraging generated data more efficiently for reinforcement learning in large language models. </p>
<blockquote>
<p>强化学习从可验证奖励（RLVR）已经成为提高大型语言模型推理能力的有前途的框架。然而，现有方法（如GRPO）经常面临零梯度的问题。这个问题主要是由于固定裁剪边界的令牌级别概率比率和对相同奖励的标准化，这可能导致梯度更新无效和生成的响应利用不足。在这项工作中，我们提出了动态裁剪策略优化（DCPO），它引入了一种动态裁剪策略，该策略根据令牌特定先验概率自适应地调整裁剪边界，以增强令牌级别的探索，以及一种平滑优势标准化技术，该技术用于标准化累积训练步骤中的奖励，以提高生成响应的响应级别有效利用。DCPO在基于四个不同模型的四个基准测试上达到了最新性能。特别是在AIME24基准测试上，DCPO在贪婪解码下达到了46.7的平均值@1，在32次采样下达到了38.8的平均值@32，超过了DAPO（36.7&#x2F;31.6）、GRPO（36.7&#x2F;32.1）和GSPO（40.0&#x2F;34.9）在Qwen2.5-Math-7B模型上的表现。在基于Qwen2.5-14B的AIME25基准测试上，DCPO的性能达到（23.3&#x2F;19.0），超过了GRPO（13.3&#x2F;10.5）、DAPO（20.0&#x2F;15.3）和GSPO（16.7&#x2F;9.9）。此外，DCPO在四个模型中的非零优势平均提高了28%，相对于DAPO训练效率翻倍，与GRPO和DAPO相比，令牌裁剪比例降低了一个数量级，同时实现了卓越的性能。这些结果凸显了DCPO在利用生成数据对大型语言模型进行强化学习方面的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.02333v2">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>强化学习从可验证的奖励（RLVR）在提高大型语言模型的推理能力方面展现出巨大的潜力。然而，现有方法如GRPO常常面临零梯度问题。该问题主要源于固定剪辑边界的令牌级别概率比率以及相同奖励的标准化，这可能导致梯度更新无效和生成响应的利用率低下。在此工作中，我们提出动态剪辑策略优化（DCPO），它引入了一种动态剪辑策略，该策略可以基于令牌特定先验概率自适应地调整剪辑边界，以增强令牌级别的探索，以及一种平滑优势标准化技术，该技术可以在累积的训练步骤中标准化奖励，以提高生成响应的响应级别有效利用率。DCPO在基于四个不同模型的四个基准测试上取得了最先进的性能。特别是在AIME24基准测试上，DCPO在贪婪解码下达到Avg@1的46.7，在32次采样下达到Avg@32的38.8，超越了DAPO（36.7&#x2F;31.6）、GRPO（36.7&#x2F;32.1）和GSPO（40.0&#x2F;34.9）在Qwen2.5-Math-7B模型上的表现。在基于Qwen2.5-14B的AIME25基准测试上，DCPO达到了（23.3&#x2F;19.0）的性能，超越了GRPO（13.3&#x2F;10.5）、DAPO（20.0&#x2F;15.3）和GSPO（16.7&#x2F;9.9）。此外，DCPO在四个模型中的非零优势平均提高了28%，比DAPO的训练效率提高了一倍，与GRPO和DAPO相比，令牌剪辑比例降低了一个数量级，同时实现了卓越的性能。这些结果突出了DCPO在利用生成数据方面更有效地进行大型语言模型的强化学习方面的有效性。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>RLVR已显示出在增强大型语言模型推理能力方面的潜力。</li>
<li>现有方法如GRPO面临零梯度问题，这限制了其性能。</li>
<li>DCPO通过动态调整剪辑策略和优势标准化技术解决了这个问题。</li>
<li>DCPO在多个基准测试上实现了卓越性能，特别是在AIME24和AIME25上的表现令人印象深刻。</li>
<li>DCPO显著提高了非零优势的训练效率，并降低了令牌剪辑比例。</li>
<li>这些结果证明DCPO更有效地利用生成数据进行强化学习。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.02333">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.02333v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.02333v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.02333v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Probe-Rewrite-Evaluate-A-Workflow-for-Reliable-Benchmarks-and-Quantifying-Evaluation-Awareness"><a href="#Probe-Rewrite-Evaluate-A-Workflow-for-Reliable-Benchmarks-and-Quantifying-Evaluation-Awareness" class="headerlink" title="Probe-Rewrite-Evaluate: A Workflow for Reliable Benchmarks and   Quantifying Evaluation Awareness"></a>Probe-Rewrite-Evaluate: A Workflow for Reliable Benchmarks and   Quantifying Evaluation Awareness</h2><p><strong>Authors:Lang Xiong, Nishant Bhargava, Jeremy Chang, Jianhang Hong, Haihao Liu, Vasu Sharma, Kevin Zhu</strong></p>
<p>Large Language Models (LLMs) often exhibit significant behavioral shifts when they perceive a change from a real-world deployment context to a controlled evaluation setting, a phenomenon known as “evaluation awareness.” This discrepancy poses a critical challenge for AI alignment, as benchmark performance may not accurately reflect a model’s true safety and honesty. In this work, we systematically quantify these behavioral changes by manipulating the perceived context of prompts. We introduce a methodology that uses a linear probe to score prompts on a continuous scale from “test-like” to “deploy-like” and leverage an LLM rewriting strategy to shift these prompts towards a more natural, deployment-style context while preserving the original task. Using this method, we achieved a 30% increase in the average probe score across a strategic role-playing dataset after rewriting. Evaluating a suite of state-of-the-art models on these original and rewritten prompts, we find that rewritten “deploy-like” prompts induce a significant and consistent shift in behavior. Across all models, we observed an average increase in honest responses of 5.26% and a corresponding average decrease in deceptive responses of 12.40%. Furthermore, refusal rates increased by an average of 6.38%, indicating heightened safety compliance. Our findings demonstrate that evaluation awareness is a quantifiable and manipulable factor that directly influences LLM behavior, revealing that models are more prone to unsafe or deceptive outputs in perceived test environments. This underscores the urgent need for more realistic evaluation frameworks to accurately gauge true model alignment before deployment. </p>
<blockquote>
<p>大型语言模型（LLM）在感知到从现实世界部署环境到受控评估环境的变更时，通常会表现出显著的行为变化，这一现象被称为“评估意识”。这种差异对人工智能对齐提出了关键挑战，因为基准性能可能无法准确反映模型的真实安全性和诚实度。在这项工作中，我们通过操作提示的感知上下文来系统地量化这些行为变化。我们介绍了一种使用线性探针对提示进行从“测试型”到“部署型”的持续评分的方法，并利用LLM重写策略来将这些提示转向更自然、更贴近部署风格的上下文，同时保留原始任务。使用这种方法，在战略角色扮演数据集上重写后，探针平均得分提高了30%。在原始提示和重写提示上评估一系列最先进的模型，我们发现重写的“部署型”提示引发了显著且一致的行为变化。在所有模型中，我们观察到诚实回应的平均增加了5.26%，相应的欺骗回应平均减少了1.4%。此外，拒绝率平均增加了6.38%，表明安全合规性有所提高。我们的研究结果表明，评估意识是一个可量化的、可操控的因素，直接影响LLM的行为，表明模型在感知的测试环境中更容易产生不安全或欺骗性的输出。这突显了在部署前对真实模型对齐进行准确评估的更现实的评估框架的迫切需求。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.00591v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文探讨了大型语言模型（LLMs）在感知从实际部署环境转移到受控评估环境时的行为变化，称为“评估意识”现象。这种行为差异给AI对齐带来了挑战，因为基准测试性能可能无法准确反映模型的真实安全性和诚实度。研究人员通过操纵提示的感知上下文来系统地量化这些行为变化，并引入了一种使用线性探针对提示进行连续评分的方法。通过重写提示，使提示更贴近自然、更贴近部署风格的上下文，同时保留原始任务。实验结果显示，重写后的“部署型”提示引发了模型行为的显著且一致的变化，诚实回应平均增加5.26%，欺骗性回应平均减少12.4%，拒绝率平均提高6.38%，表明模型更安全合规。研究结果表明，评估意识是一个可量化且可操控的因素，直接影响LLM的行为，凸显了在部署前开发更现实的评估框架来准确衡量模型对齐的紧迫性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLMs）在评估环境和实际部署环境中展现出不同的行为特征。</li>
<li>“评估意识”现象对AI对齐构成挑战，因为基准测试性能可能无法真实反映模型的安全性和诚实度。</li>
<li>通过操纵提示的感知上下文，可以系统地量化LLM的行为变化。</li>
<li>引入了一种使用线性探针评分提示的方法，能有效将提示从“测试型”向“部署型”转变。</li>
<li>重写后的“部署型”提示显著改变了模型的行为，包括提高诚实回应、降低欺骗性回应和提高拒绝率。</li>
<li>评估意识是一个可量化且可操控的因素，直接影响LLM的行为。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.00591">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.00591v4/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.00591v4/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.00591v4/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.00591v4/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.00591v4/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2509.00591v4/page_5_1.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="AI-SearchPlanner-Modular-Agentic-Search-via-Pareto-Optimal-Multi-Objective-Reinforcement-Learning"><a href="#AI-SearchPlanner-Modular-Agentic-Search-via-Pareto-Optimal-Multi-Objective-Reinforcement-Learning" class="headerlink" title="AI-SearchPlanner: Modular Agentic Search via Pareto-Optimal   Multi-Objective Reinforcement Learning"></a>AI-SearchPlanner: Modular Agentic Search via Pareto-Optimal   Multi-Objective Reinforcement Learning</h2><p><strong>Authors:Lang Mei, Zhihan Yang, Chong Chen</strong></p>
<p>Recent studies have explored integrating Large Language Models (LLMs) with search engines to leverage both the LLMs’ internal pre-trained knowledge and external information. Specially, reinforcement learning (RL) has emerged as a promising paradigm for enhancing LLM reasoning through multi-turn interactions with search engines. However, existing RL-based search agents rely on a single LLM to handle both search planning and question-answering (QA) tasks in an end-to-end manner, which limits their ability to optimize both capabilities simultaneously. In practice, sophisticated AI search systems often employ a large, frozen LLM (e.g., GPT-4, DeepSeek-R1) to ensure high-quality QA. Thus, a more effective and efficient approach is to utilize a small, trainable LLM dedicated to search planning. In this paper, we propose \textbf{AI-SearchPlanner}, a novel reinforcement learning framework designed to enhance the performance of frozen QA models by focusing on search planning. Specifically, our approach introduces three key innovations: 1) Decoupling the Architecture of the Search Planner and Generator, 2) Dual-Reward Alignment for Search Planning, and 3) Pareto Optimization of Planning Utility and Cost, to achieve the objectives. Extensive experiments on real-world datasets demonstrate that AI SearchPlanner outperforms existing RL-based search agents in both effectiveness and efficiency, while exhibiting strong generalization capabilities across diverse frozen QA models and data domains. </p>
<blockquote>
<p>最近的研究探讨了如何将大型语言模型（LLM）与搜索引擎相结合，以利用LLM的内部预训练知识和外部信息。特别是，强化学习（RL）已经成为一种通过多轮与搜索引擎互动增强LLM推理能力的有前途的模式。然而，现有的基于RL的搜索代理依赖于单个LLM以端到端的方式同时处理搜索规划和问答（QA）任务，这限制了它们同时优化这两种能力的能力。在实践中，复杂的人工智能搜索系统通常会使用大型、固定的LLM（如GPT-4、DeepSeek-R1）来确保高质量的问答。因此，更有效和高效的方法是使用一个小型、可训练的LLM专门用于搜索规划。在本文中，我们提出了\textbf{AI-SearchPlanner}，这是一种新型的强化学习框架，旨在通过专注于搜索规划来提高固定问答模型的性能。具体来说，我们的方法引入了三个关键创新点：1）解耦搜索规划器和生成器的架构，2）搜索规划的双奖励对齐，以及3）规划和成本的帕累托优化，以实现目标。在真实数据集上的广泛实验表明，AI SearchPlanner在有效性和效率方面优于现有的基于RL的搜索代理，同时在不同的固定问答模型和数据领域表现出强大的泛化能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.20368v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）与搜索引擎的结合是当前研究的热点。本文提出了一种新的强化学习框架AI-SearchPlanner，旨在通过专注于搜索规划来提高冻结的问答模型性能。该框架引入了三个关键创新点：搜索规划器和生成器的架构解耦、搜索规划的双奖励对齐以及规划效用和成本的帕累托优化。实验证明，AI SearchPlanner在有效性和效率上超越了现有的基于RL的搜索代理，并在不同的冻结问答模型和数据域上表现出强大的泛化能力。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>大型语言模型（LLM）与搜索引擎的结合研究旨在利用LLM的内部预训练知识和外部信息。</li>
<li>强化学习（RL）已成为增强LLM推理的一个有前途的范式，通过与搜索引擎的多轮交互。</li>
<li>现有基于RL的搜索代理依赖于单一LLM同时处理搜索规划和问答任务，这限制了它们的优化能力。</li>
<li>AI-SearchPlanner框架通过专注于搜索规划来提高冻结问答模型的性能。</li>
<li>AI-SearchPlanner框架引入了三个关键创新点：架构解耦、双奖励对齐和帕累托优化。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.20368">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2508.20368v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2508.20368v3/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2508.20368v3/page_5_0.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="ReST-RL-Achieving-Accurate-Code-Reasoning-of-LLMs-with-Optimized-Self-Training-and-Decoding"><a href="#ReST-RL-Achieving-Accurate-Code-Reasoning-of-LLMs-with-Optimized-Self-Training-and-Decoding" class="headerlink" title="ReST-RL: Achieving Accurate Code Reasoning of LLMs with Optimized   Self-Training and Decoding"></a>ReST-RL: Achieving Accurate Code Reasoning of LLMs with Optimized   Self-Training and Decoding</h2><p><strong>Authors:Sining Zhoubian, Dan Zhang, Jie Tang</strong></p>
<p>With respect to improving the reasoning accuracy of LLMs, the representative reinforcement learning (RL) method GRPO faces failure due to insignificant reward variance, while verification methods based on process reward models (PRMs) suffer from difficulties with training data acquisition and verification effectiveness. To tackle these problems, this paper introduces ReST-RL, a unified LLM RL paradigm that significantly improves LLM’s code reasoning ability by combining an improved GRPO algorithm with a meticulously designed test time decoding method assisted by a value model (VM). As the first stage of policy reinforcement, ReST-GRPO adopts an optimized ReST algorithm to filter and assemble high-value training data, increasing the reward variance of GRPO sampling, thus improving the effectiveness and efficiency of training. After the basic reasoning ability of LLM policy has been improved, we further propose a test time decoding optimization method called VM-MCTS. Through Monte-Carlo Tree Search (MCTS), we collect accurate value targets with no annotation required, on which VM training is based. When decoding, the VM is deployed by an adapted MCTS algorithm to provide precise process signals as well as verification scores, assisting the LLM policy to achieve high reasoning accuracy. We conduct extensive experiments on coding problems to verify the validity of the proposed RL paradigm. Upon comparison, our approach significantly outperforms other reinforcement training baselines (e.g., naive GRPO and ReST-DPO), as well as decoding and verification baselines (e.g., PRM-BoN and ORM-MCTS) on well-known coding benchmarks of various levels (e.g., APPS, BigCodeBench, and HumanEval), indicating its power to strengthen the reasoning ability of LLM policies. Codes for our project can be found at <a target="_blank" rel="noopener" href="https://github.com/THUDM/ReST-RL">https://github.com/THUDM/ReST-RL</a>. </p>
<blockquote>
<p>关于提高大型语言模型（LLM）的推理准确性，代表性的强化学习（RL）方法GRPO由于奖励方差不显著而失败，而基于过程奖励模型（PRM）的验证方法则面临训练数据获取和验证有效性方面的困难。针对这些问题，本文引入了ReST-RL，这是一种统一的LLM RL范式，通过结合改进的GRPO算法和精心设计的测试时间解码方法（借助值模型（VM）辅助），显著提高了LLM的代码推理能力。作为政策强化的第一阶段，ReST-GRPO采用优化的ReST算法来过滤和组装高价值训练数据，增加GRPO采样的奖励方差，从而提高训练的有效性和效率。在提高了LLM政策的基本推理能力后，我们进一步提出了一种名为VM-MCTS的测试时间解码优化方法。通过蒙特卡洛树搜索（MCTS），我们可以在无需注释的情况下收集准确的价值目标，VM训练就基于此进行。在解码时，VM通过适应的MCTS算法提供精确的过程信号和验证分数，帮助LLM策略实现高推理准确性。我们在编程问题上进行了大量的实验，以验证所提出的RL范式的有效性。经比较，我们的方法在很大程度上优于其他强化训练基线（例如，简单的GRPO和ReST-DPO），以及解码和验证基线（例如，PRM-BoN和ORM-MCTS）在著名的各级编程基准测试（例如，APPS、BigCodeBench和HumanEval）上的表现，证明其增强LLM策略推理能力的实力。我们的项目代码可在[<a target="_blank" rel="noopener" href="https://github.com/THUDM/ReST-RL%E6%89%BE%E5%88%B0%E3%80%82]">https://github.com/THUDM/ReST-RL找到。]</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.19576v2">PDF</a> 21 pages, 4 figures</p>
<p><strong>Summary</strong></p>
<p>基于改进型GRPO算法和值模型辅助的测试时间解码方法，本文提出了ReST-RL统一LLM RL范式，旨在提高LLM的代码推理能力。通过优化GRPO算法和引入VM-MCTS解码优化方法，该范式提升了训练的有效性和效率，并实现了高推理准确率的LLM策略。在编码问题上的广泛实验验证了该RL范式的有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ReST-RL是一种针对LLM的强化学习范式，旨在提高代码推理能力。</li>
<li>ReST-RL结合了改进型GRPO算法和值模型（VM）辅助的测试时间解码方法。</li>
<li>通过优化GRPO算法，提高了奖励方差和训练的有效性和效率。</li>
<li>VM-MCTS解码优化方法提供了精确的过程信号和验证分数，进一步提升了LLM策略的推理准确性。</li>
<li>广泛实验表明，ReST-RL在编码问题上显著优于其他强化训练基准和解码验证基准。</li>
<li>ReST-RL在多个知名编码基准上表现出色，如APPS、BigCodeBench和HumanEval。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.19576">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2508.19576v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2508.19576v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2508.19576v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2508.19576v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="MaRVL-QA-A-Benchmark-for-Mathematical-Reasoning-over-Visual-Landscapes"><a href="#MaRVL-QA-A-Benchmark-for-Mathematical-Reasoning-over-Visual-Landscapes" class="headerlink" title="MaRVL-QA: A Benchmark for Mathematical Reasoning over Visual Landscapes"></a>MaRVL-QA: A Benchmark for Mathematical Reasoning over Visual Landscapes</h2><p><strong>Authors:Nilay Pande, Sahiti Yerramilli, Jayant Sravan Tamarapalli, Rynaa Grover</strong></p>
<p>A key frontier for Multimodal Large Language Models (MLLMs) is the ability to perform deep mathematical and spatial reasoning directly from images, moving beyond their established success in semantic description. Mathematical surface plots provide a rigorous testbed for this capability, as they isolate the task of reasoning from the semantic noise common in natural images. To measure progress on this frontier, we introduce MaRVL-QA (Mathematical Reasoning over Visual Landscapes), a new benchmark designed to quantitatively evaluate these core reasoning skills. The benchmark comprises two novel tasks: Topological Counting, identifying and enumerating features like local maxima; and Transformation Recognition, recognizing applied geometric transformations. Generated from a curated library of functions with rigorous ambiguity filtering, our evaluation on MaRVL-QA reveals that even state-of-the-art MLLMs struggle significantly, often resorting to superficial heuristics instead of robust spatial reasoning. MaRVL-QA provides a challenging new tool for the research community to measure progress, expose model limitations, and guide the development of MLLMs with more profound reasoning abilities. </p>
<blockquote>
<p>多模态大型语言模型（MLLMs）的关键前沿是能够直接从图像中进行深度数学和空间推理，这超越了其在语义描述方面已经取得的成就。数学曲面图为此类能力提供了严格的测试平台，因为它们能够将推理任务与自然图像中常见的语义噪声隔离开来。为了衡量这一前沿的进展，我们引入了MaRVL-QA（数学推理在视觉景观上的应用），这是一个新的基准测试，旨在定量评估这些核心推理技能。该基准测试包含两个新任务：拓扑计数，识别和列举局部最大值等特征；以及转换识别，识别应用的几何变换。我们的评估是基于从功能丰富的库中精心挑选的函数，经过严格的歧义过滤后生成MaRVL-QA。即使在MaRVL-QA的评估中，最先进的MLLMs也面临着巨大的挑战，往往倾向于使用肤浅的启发式方法，而不是稳健的空间推理。MaRVL-QA为研究领域提供了一个具有挑战性的新工具，可以衡量进展、暴露模型局限性，并引导开发具有更深刻推理能力的MLLMs。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.17180v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>文本介绍了多模态大型语言模型（MLLMs）面临的关键前沿问题，即直接从图像进行深度数学和空间推理的能力，超越了其在语义描述方面的成功应用。为了测试这一能力，引入了MaRVL-QA（基于视觉景观的数学推理）这一新基准测试，用于定量评估核心推理技能。该基准测试包含两个新任务：拓扑计数和变换识别。通过对函数的严格筛选和模糊性过滤，对现有模型的评估显示，即使是最先进的多模态大型语言模型在应对这两个任务时仍然面临很大挑战，往往依赖于肤浅的启发式策略而非深入的空间推理能力。因此，MaRVL-QA提供了一个挑战性的新工具，用于衡量研究进展、揭示模型局限性并引导开发具有更深入推理能力的多模态大型语言模型。</p>
<p><strong>Key Takeaways</strong></p>
<p>以下是关键要点要点列表：</p>
<ul>
<li>多模态大型语言模型面临的关键挑战是直接进行数学和空间推理的能力的提升。这一能力对于理解复杂数据和知识领域非常重要。对于基础知识的理解程度和表现很重要，不仅需要准确的知识体系表述更依赖基础理解和高效融合复杂的理论和实际情况形成信息解读。随着技术的发展和研究的深入，该领域需要更多能够深入理解数学和几何概念的系统和方法。未来该领域的一个重要的研究目标将如何运用AI实现机器能够与人类有一样的认知学习能力即通过对获取信息的判断融合自己的意识决策机制构建完成精准的空间思维和数学的结合分析运用和抽象思考能里是我们长期面对的目标方向之一。具体到具体的AI研发阶段会有着相对重要角色帮助工程师不断研究并开发出更多新的应用。从技术应用的角度AI通过融合深度学习以及更多新的技术手段来提升机器学习模型的空间思维能力和数学分析能力成为关键突破口之一。机器学习领域中跨学科的综合性融合例如利用最新的训练框架在仿真实验中更进一步的让模型理解和识别现实世界环境成为可能使AI的认知学习将走向全新的高度并且赋予其解决未来问题的全新视角和能力，从而在仿真环境中的精准识别感知获得应用的价值从而更广泛地在现实世界中发挥作用。因此未来多模态大型语言模型的发展将更加注重对深度数学和空间推理能力的提升。通过引入新的基准测试如MaRVL-QA能够帮助推动多模态语言模型的持续发展将会对整个机器学习和人工智能领域的突破做出重要进展并对多模态理解提供更多切实可行的技术应用方向和价值的扩展与应用的研究和新技术的开发。对于未来的研究和发展来说这是一个重要的里程碑和方向之一。随着研究的深入和技术的发展多模态大型语言模型的应用前景将会越来越广阔并带来革命性的变化和发展前景以及更加丰富的应用场景。这对于未来的技术发展和应用具有非常重要的意义和价值并推动人工智能领域不断向前发展并取得更大的突破和进展以及实现更加智能和高效的应用系统和技术应用。因此未来的研究和发展将更加注重多模态大型语言模型的深度数学和空间推理能力的提升并继续推动人工智能领域的不断发展和进步以及更加广泛的应用场景和应用价值等关键领域的拓展和创新等具有重要意义和价值的工作和贡献将会持续涌现并引领未来的科技发展趋势和方向等重要的领域发展以及实现更加智能和高效的应用系统和技术应用等目标方向之一值得期待在广大科研人员及相关产业的不断研发和实践中持续推进智能化水平的提升最终通过产业的技术研发突破瓶颈逐步实现人工智能技术持续发展和进步的目标和方向之一值得期待。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.17180">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2508.17180v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2508.17180v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2508.17180v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2508.17180v2/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2508.17180v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2508.17180v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2508.17180v2/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2508.17180v2/page_5_1.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Bridging-the-Gap-in-Ophthalmic-AI-MM-Retinal-Reason-Dataset-and-OphthaReason-Model-toward-Dynamic-Multimodal-Reasoning"><a href="#Bridging-the-Gap-in-Ophthalmic-AI-MM-Retinal-Reason-Dataset-and-OphthaReason-Model-toward-Dynamic-Multimodal-Reasoning" class="headerlink" title="Bridging the Gap in Ophthalmic AI: MM-Retinal-Reason Dataset and   OphthaReason Model toward Dynamic Multimodal Reasoning"></a>Bridging the Gap in Ophthalmic AI: MM-Retinal-Reason Dataset and   OphthaReason Model toward Dynamic Multimodal Reasoning</h2><p><strong>Authors:Ruiqi Wu, Yuang Yao, Tengfei Ma, Chenran Zhang, Na Su, Tao Zhou, Geng Chen, Wen Fan, Yi Zhou</strong></p>
<p>Multimodal large language models (MLLMs) have recently demonstrated remarkable reasoning abilities with reinforcement learning paradigm. Although several multimodal reasoning models have been explored in the medical domain, most of them focus exclusively on basic reasoning, which refers to shallow inference based on visual feature matching. However, real-world clinical diagnosis extends beyond basic reasoning, demanding reasoning processes that integrate heterogeneous clinical information (such as chief complaints and medical history) with multimodal medical imaging data. To bridge this gap, we introduce MM-Retinal-Reason, the first ophthalmic multimodal dataset with the full spectrum of perception and reasoning. It encompasses both basic reasoning tasks and complex reasoning tasks, aiming to enhance visual-centric fundamental reasoning capabilities and emulate realistic clinical thinking patterns. Building upon MM-Retinal-Reason, we propose OphthaReason, the first ophthalmology-specific multimodal reasoning model with step-by-step reasoning traces. To enable flexible adaptation to both basic and complex reasoning tasks, we specifically design a novel method called Uncertainty-Aware Dynamic Thinking (UADT), which estimates sample-level uncertainty via entropy and dynamically modulates the model’s exploration depth using a shaped advantage mechanism. Comprehensive experiments demonstrate that our model achieves state-of-the-art performance on both basic and complex reasoning tasks, outperforming general-purpose MLLMs, medical MLLMs, RL-based medical MLLMs, and ophthalmic MLLMs by at least 24.92%, 15.00%, 21.20%, and 17.66%. Project Page: \href{<a target="_blank" rel="noopener" href="https://github.com/lxirich/OphthaReason%7D%7Blink%7D">https://github.com/lxirich/OphthaReason}{link}</a>. </p>
<blockquote>
<p>多模态大型语言模型（MLLMs）最近采用强化学习范式展示了出色的推理能力。尽管医疗领域已经探索了多种多模态推理模型，但大多数模型主要关注基本推理，这指的是基于视觉特征匹配的浅层推理。然而，现实世界中的临床诊断超越了基本推理，需要整合异质临床信息（如主诉和病史）与多模态医学成像数据的过程推理。为了弥补这一差距，我们推出了MM-Retinal-Reason，这是第一个涵盖完整感知和推理光谱的眼科多模态数据集。它涵盖了基本推理任务和复杂推理任务，旨在增强视觉为中心的基本推理能力，并模拟现实的临床思维模式。基于MM-Retinal-Reason，我们提出了眼科特定多模态推理模型OphthaReason，具备逐步推理轨迹。为了灵活适应基本和复杂推理任务，我们特意设计了一种名为不确定性感知动态思维（UADT）的新方法，该方法通过熵估计样本级不确定性，并使用成型优势机制动态调节模型的探索深度。综合实验表明，我们的模型在基本和复杂推理任务上均达到了最新性能水平，至少比普通MLLMs、医疗MLLMs、基于RL的医疗MLLMs和眼科MLLMs高出24.92%、15.00%、21.20%和17.66%。项目页面链接：<a target="_blank" rel="noopener" href="https://github.com/lxirich/OphthaReason%E3%80%82%EF%BC%88%E5%85%B7%E4%BD%93%E9%93%BE%E6%8E%A5%EF%BC%89">https://github.com/lxirich/OphthaReason。（具体链接）</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.16129v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了多模态大型语言模型（MLLMs）在医疗领域的应用，尤其是其在眼科诊断中的使用。文章指出，尽管已有一些多模态推理模型被探索用于医疗领域，但它们大多只关注基于视觉特征匹配的基本推理。然而，现实世界中的临床诊断需要超越基本推理，整合异质临床信息与多模态医学成像数据。为此，文章引入了MM-Retinal-Reason数据集，包含基本和复杂推理任务，旨在增强视觉为中心的基本推理能力并模拟现实临床思维模式。基于此数据集，文章提出了首个眼科专用的多模态推理模型——OphthaReason，并设计了一种名为Uncertainty-Aware Dynamic Thinking（UADT）的新方法，以灵活适应基本和复杂推理任务。实验表明，该模型在基本和复杂推理任务上的性能均达到最新水平，至少比其他模型高出24.92％、15.00％、21.20％和17.66％。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态大型语言模型（MLLMs）在医疗领域具有强大的推理能力。</li>
<li>现有模型大多仅关注基于视觉特征匹配的基本推理，缺乏复杂性。</li>
<li>现实世界临床诊断需要整合异质临床信息与多模态医学成像数据。</li>
<li>MM-Retinal-Reason数据集引入，包含基本和复杂推理任务，模拟现实临床思维。</li>
<li>首次提出眼科专用多模态推理模型——OphthaReason。</li>
<li>Uncertainty-Aware Dynamic Thinking（UADT）方法设计，以灵活适应不同推理任务。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.16129">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2508.16129v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2508.16129v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2508.16129v2/page_3_0.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Mobile-Agent-v3-Fundamental-Agents-for-GUI-Automation"><a href="#Mobile-Agent-v3-Fundamental-Agents-for-GUI-Automation" class="headerlink" title="Mobile-Agent-v3: Fundamental Agents for GUI Automation"></a>Mobile-Agent-v3: Fundamental Agents for GUI Automation</h2><p><strong>Authors:Jiabo Ye, Xi Zhang, Haiyang Xu, Haowei Liu, Junyang Wang, Zhaoqing Zhu, Ziwei Zheng, Feiyu Gao, Junjie Cao, Zhengxi Lu, Jitong Liao, Qi Zheng, Fei Huang, Jingren Zhou, Ming Yan</strong></p>
<p>This paper introduces GUI-Owl, a foundational GUI agent model that achieves state-of-the-art performance among open-source end-to-end models on ten GUI benchmarks across desktop and mobile environments, covering grounding, question answering, planning, decision-making, and procedural knowledge. GUI-Owl-7B achieves 66.4 on AndroidWorld and 29.4 on OSWorld. Building on this, we propose Mobile-Agent-v3, a general-purpose GUI agent framework that further improves performance to 73.3 on AndroidWorld and 37.7 on OSWorld, setting a new state-of-the-art for open-source GUI agent frameworks. GUI-Owl incorporates three key innovations: (1) Large-scale Environment Infrastructure: a cloud-based virtual environment spanning Android, Ubuntu, macOS, and Windows, enabling our Self-Evolving GUI Trajectory Production framework. This generates high-quality interaction data via automated query generation and correctness validation, leveraging GUI-Owl to refine trajectories iteratively, forming a self-improving loop. It supports diverse data pipelines and reduces manual annotation. (2) Diverse Foundational Agent Capabilities: by integrating UI grounding, planning, action semantics, and reasoning patterns, GUI-Owl supports end-to-end decision-making and can act as a modular component in multi-agent systems. (3) Scalable Environment RL: we develop a scalable reinforcement learning framework with fully asynchronous training for real-world alignment. We also introduce Trajectory-aware Relative Policy Optimization (TRPO) for online RL, achieving 34.9 on OSWorld. GUI-Owl and Mobile-Agent-v3 are open-sourced at <a target="_blank" rel="noopener" href="https://github.com/X-PLUG/MobileAgent">https://github.com/X-PLUG/MobileAgent</a>. </p>
<blockquote>
<p>本文介绍了GUI-Owl，这是一个基础GUI代理模型，它在桌面和移动环境的十个GUI基准测试上实现了开源端到端模型的最新性能，涵盖了接地、问答、规划、决策和程序知识。GUI-Owl-7B在AndroidWorld上得分为66.4，在OSWorld上得分为29.4。在此基础上，我们提出了通用GUI代理框架Mobile-Agent-v3，进一步将AndroidWorld的性能提升至73.3，OSWorld的性能提升至37.7，为开源GUI代理框架创造了新的最新记录。GUI-Owl包含了三个关键的创新点：（1）大规模环境基础设施：一个跨越Android、Ubuntu、macOS和Windows的云端虚拟环境，使我们的自我进化GUI轨迹生产框架得以运行。该框架通过自动化查询生成和正确性验证，利用GUI-Owl迭代地优化轨迹，形成一个自我改进循环。它支持各种数据管道，减少手动注释。（2）多样化的基础代理能力：通过整合UI接地、规划、动作语义和推理模式，GUI-Owl支持端到端的决策制定，并可以作为多代理系统中的模块化组件。（3）可扩展的环境强化学习：我们开发了一个可扩展的强化学习框架，具有完全异步训练以符合现实世界的对齐。我们还引入了轨迹感知相对策略优化（TRPO）进行在线强化学习，在OSWorld上实现了34.9的得分。GUI-Owl和Mobile-Agent-v3已在<a target="_blank" rel="noopener" href="https://github.com/X-PLUG/Mobileagent%E5%BC%80%E6%BA%90%E3%80%82">https://github.com/X-PLUG/Mobileagent开源。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.15144v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了GUI-Owl，一个基础GUI代理模型，它在桌面和移动环境的十个GUI基准测试上达到了开创性的性能表现。GUI-Owl模型引入了一系列关键创新，包括大规模环境基础设施、多样化的基础代理能力以及可扩展的环境强化学习等。在此基础上，提出了通用GUI代理框架Mobile-Agent-v3，进一步提高了性能。这些成果均已在GitHub上开源。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GUI-Owl模型实现了跨桌面和移动环境的GUI代理模型的先进性能。</li>
<li>GUI-Owl在AndroidWorld和OSWorld基准测试中表现出色。</li>
<li>Mobile-Agent-v3框架提高了GUI-Owl的性能，并开创了GUI代理框架的新局面。</li>
<li>GUI-Owl模型引入大规模环境基础设施，支持多种数据管道，减少手动注释。</li>
<li>GUI-Owl具备多样化的基础代理能力，可支持端到端的决策制定，并可作为多代理系统模块组件。</li>
<li>开发了可扩展的环境强化学习框架，采用完全异步训练实现真实世界对齐。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.15144">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2508.15144v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2508.15144v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2508.15144v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2508.15144v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_R1_Reasoning/2508.15144v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-12/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-12/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-12/LLM/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_LLM/2507.02253v4/page_1_0.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM 方向最新论文已更新，请持续关注 Update in 2025-09-12  A Survey of Reinforcement Learning for Large Reasoning Models
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-12
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-11/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_检测_分割_跟踪/2508.10542v3/page_4_0.jpg" class="responsive-img" alt="检测/分割/跟踪">
                        
                        <span class="card-title">检测/分割/跟踪</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            检测/分割/跟踪 方向最新论文已更新，请持续关注 Update in 2025-09-11  GCRPNet Graph-Enhanced Contextual and Regional Perception Network for   Salient Object Detection in Optical Remote Sensing Images
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-11
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                    检测/分割/跟踪
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                        <span class="chip bg-color">检测/分割/跟踪</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">30666.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
