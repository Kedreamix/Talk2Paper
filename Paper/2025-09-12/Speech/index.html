<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-12  Streaming Sequence-to-Sequence Learning with Delayed Streams Modeling">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Speech/2509.00405v2/page_0_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-12
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-07
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    9.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    36 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-09-12-æ›´æ–°"><a href="#2025-09-12-æ›´æ–°" class="headerlink" title="2025-09-12 æ›´æ–°"></a>2025-09-12 æ›´æ–°</h1><h2 id="Streaming-Sequence-to-Sequence-Learning-with-Delayed-Streams-Modeling"><a href="#Streaming-Sequence-to-Sequence-Learning-with-Delayed-Streams-Modeling" class="headerlink" title="Streaming Sequence-to-Sequence Learning with Delayed Streams Modeling"></a>Streaming Sequence-to-Sequence Learning with Delayed Streams Modeling</h2><p><strong>Authors:Neil Zeghidour, Eugene Kharitonov, Manu Orsini, VÃ¡clav Volhejn, Gabriel de Marmiesse, Edouard Grave, Patrick PÃ©rez, Laurent MazarÃ©, Alexandre DÃ©fossez</strong></p>
<p>We introduce Delayed Streams Modeling (DSM), a flexible formulation for streaming, multimodal sequence-to-sequence learning. Sequence-to-sequence generation is often cast in an offline manner, where the model consumes the complete input sequence before generating the first output timestep. Alternatively, streaming sequence-to-sequence rely on learning a policy for choosing when to advance on the input stream, or write to the output stream. DSM instead models already time-aligned streams with a decoder-only language model. By moving the alignment to a pre-processing step,and introducing appropriate delays between streams, DSM provides streaming inference of arbitrary output sequences, from any input combination, making it applicable to many sequence-to-sequence problems. In particular, given text and audio streams, automatic speech recognition (ASR) corresponds to the text stream being delayed, while the opposite gives a text-to-speech (TTS) model. We perform extensive experiments for these two major sequence-to-sequence tasks, showing that DSM provides state-of-the-art performance and latency while supporting arbitrary long sequences, being even competitive with offline baselines. Code, samples and demos are available at <a target="_blank" rel="noopener" href="https://github.com/kyutai-labs/delayed-streams-modeling">https://github.com/kyutai-labs/delayed-streams-modeling</a> </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†å»¶è¿Ÿæµå»ºæ¨¡ï¼ˆDSMï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºæµå¼ã€å¤šæ¨¡æ€åºåˆ—åˆ°åºåˆ—å­¦ä¹ çš„çµæ´»å…¬å¼ã€‚åºåˆ—åˆ°åºåˆ—ç”Ÿæˆé€šå¸¸ä»¥ä¸€ç§ç¦»çº¿çš„æ–¹å¼è¿›è¡Œï¼Œæ¨¡å‹åœ¨ç”Ÿæˆç¬¬ä¸€ä¸ªè¾“å‡ºæ—¶é—´æ­¥ä¹‹å‰ä¼šæ¶ˆè€—å®Œæ•´çš„è¾“å…¥åºåˆ—ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæµå¼åºåˆ—åˆ°åºåˆ—åˆ™ä¾èµ–äºå­¦ä¹ ä¸€ä¸ªç­–ç•¥ï¼Œä»¥ç¡®å®šä½•æ—¶æ¨è¿›è¾“å…¥æµï¼Œæˆ–å†™å…¥è¾“å‡ºæµã€‚ç„¶è€Œï¼ŒDSMä½¿ç”¨ä»…è§£ç çš„è¯­è¨€æ¨¡å‹å¯¹å·²ç»æ—¶é—´å¯¹é½çš„æµè¿›è¡Œå»ºæ¨¡ã€‚é€šè¿‡å°†å¯¹é½ç§»åŠ¨åˆ°é¢„å¤„ç†æ­¥éª¤ï¼Œå¹¶åœ¨æµä¹‹é—´å¼•å…¥é€‚å½“çš„å»¶è¿Ÿï¼ŒDSMå¯ä»¥å¯¹ä»»æ„è¾“å‡ºåºåˆ—è¿›è¡Œæµå¼æ¨ç†ï¼Œé€‚ç”¨äºè®¸å¤šåºåˆ—åˆ°åºåˆ—é—®é¢˜ï¼Œé€‚ç”¨äºä»»ä½•è¾“å…¥ç»„åˆã€‚ç‰¹åˆ«æ˜¯ç»™å®šæ–‡æœ¬å’ŒéŸ³é¢‘æµæ—¶ï¼Œè¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰å¯¹åº”äºæ–‡æœ¬æµå»¶è¿Ÿï¼Œè€Œç›¸ååˆ™ç»™å‡ºäº†æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å‹ã€‚æˆ‘ä»¬ä¸ºè¿™ä¸¤ä¸ªä¸»è¦çš„åºåˆ—åˆ°åºåˆ—ä»»åŠ¡è¿›è¡Œäº†å¤§é‡å®éªŒï¼Œç»“æœè¡¨æ˜ï¼ŒDSMåœ¨æä¾›æœ€å…ˆè¿›çš„æ€§èƒ½å’Œå»¶è¿Ÿçš„åŒæ—¶ï¼Œè¿˜æ”¯æŒä»»æ„é•¿åºåˆ—ï¼Œç”šè‡³ä¸ç¦»çº¿åŸºå‡†æµ‹è¯•ç›¸ç«äº‰ã€‚ä»£ç ã€æ ·æœ¬å’Œæ¼”ç¤ºå¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/kyutai-labs/delayed-streams-modeling%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/kyutai-labs/delayed-streams-modelingæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.08753v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å»¶è¿Ÿæµå»ºæ¨¡ï¼ˆDSMï¼‰ï¼Œè¿™æ˜¯ä¸€ç§çµæ´»çš„å¤šæ¨¡æ€åºåˆ—åˆ°åºåˆ—å­¦ä¹ çš„æµå¼å¤„ç†æ–¹æ³•ã€‚ä¼ ç»Ÿçš„åºåˆ—åˆ°åºåˆ—ç”Ÿæˆå¤šä¸ºç¦»çº¿æ¨¡å¼ï¼Œè€ŒDSMå°†è¾“å…¥åºåˆ—ä¸è¾“å‡ºæµçš„æ—¶åºå¯¹é½è½¬ç§»è‡³é¢„å¤„ç†æ­¥éª¤ï¼Œé€šè¿‡å¼•å…¥é€‚å½“çš„å»¶è¿Ÿæ¥å®ç°ä»»æ„è¾“å‡ºåºåˆ—çš„æµå¼æ¨æ–­ï¼Œé€‚ç”¨äºå¤šç§åºåˆ—åˆ°åºåˆ—é—®é¢˜ã€‚åœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰å’Œæ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰ç­‰å…³é”®ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜ç§€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å»¶è¿Ÿæµå»ºæ¨¡ï¼ˆDSMï¼‰æ˜¯ä¸€ç§çµæ´»çš„å¤šæ¨¡æ€åºåˆ—åˆ°åºåˆ—å­¦ä¹ çš„æµå¼å¤„ç†æ–¹æ³•ã€‚</li>
<li>ä¼ ç»Ÿåºåˆ—åˆ°åºåˆ—ç”Ÿæˆå¤šä¸ºç¦»çº¿æ¨¡å¼ï¼Œè€ŒDSMèƒ½å®ç°ä»»æ„è¾“å‡ºåºåˆ—çš„æµå¼æ¨æ–­ã€‚</li>
<li>DSMé€šè¿‡é¢„å¤„ç†çš„æ—¶åºå¯¹é½å’Œå¼•å…¥é€‚å½“çš„å»¶è¿Ÿæ¥å®ç°æµå¼æ¨æ–­ã€‚</li>
<li>DSMé€‚ç”¨äºå¤šç§åºåˆ—åˆ°åºåˆ—é—®é¢˜ã€‚</li>
<li>åœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ä»»åŠ¡ä¸Šï¼ŒDSMè¡¨ç°ä¼˜ç§€ï¼Œå¯å»¶è¿Ÿæ–‡æœ¬æµä»¥å®ç°ASRã€‚</li>
<li>åœ¨æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰ä»»åŠ¡ä¸Šï¼ŒDSMé€šè¿‡å»¶è¿ŸéŸ³é¢‘æµå®ç°TTSæ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.08753">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Speech/2509.08753v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Speech/2509.08753v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Speech/2509.08753v1/page_4_0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Joint-Learning-using-Mixture-of-Expert-Based-Representation-for-Enhanced-Speech-Generation-and-Robust-Emotion-Recognition"><a href="#Joint-Learning-using-Mixture-of-Expert-Based-Representation-for-Enhanced-Speech-Generation-and-Robust-Emotion-Recognition" class="headerlink" title="Joint Learning using Mixture-of-Expert-Based Representation for Enhanced   Speech Generation and Robust Emotion Recognition"></a>Joint Learning using Mixture-of-Expert-Based Representation for Enhanced   Speech Generation and Robust Emotion Recognition</h2><p><strong>Authors:Jing-Tong Tzeng, Carlos Busso, Chi-Chun Lee</strong></p>
<p>Speech emotion recognition (SER) plays a critical role in building emotion-aware speech systems, but its performance degrades significantly under noisy conditions. Although speech enhancement (SE) can improve robustness, it often introduces artifacts that obscure emotional cues and adds computational overhead to the pipeline. Multi-task learning (MTL) offers an alternative by jointly optimizing SE and SER tasks. However, conventional shared-backbone models frequently suffer from gradient interference and representational conflicts between tasks. To address these challenges, we propose the Sparse Mixture-of-Experts Representation Integration Technique (Sparse MERIT), a flexible MTL framework that applies frame-wise expert routing over self-supervised speech representations. Sparse MERIT incorporates task-specific gating networks that dynamically select from a shared pool of experts for each frame, enabling parameter-efficient and task-adaptive representation learning. Experiments on the MSP-Podcast corpus show that Sparse MERIT consistently outperforms baseline models on both SER and SE tasks. Under the most challenging condition of -5 dB signal-to-noise ratio (SNR), Sparse MERIT improves SER F1-macro by an average of 12.0% over a baseline relying on a SE pre-processing strategy, and by 3.4% over a naive MTL baseline, with statistical significance on unseen noise conditions. For SE, Sparse MERIT improves segmental SNR (SSNR) by 28.2% over the SE pre-processing baseline and by 20.0% over the naive MTL baseline. These results demonstrate that Sparse MERIT provides robust and generalizable performance for both emotion recognition and enhancement tasks in noisy environments. </p>
<blockquote>
<p>è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰åœ¨æ„å»ºæƒ…æ„Ÿæ„ŸçŸ¥è¯­éŸ³ç³»ç»Ÿä¸­èµ·ç€å…³é”®ä½œç”¨ï¼Œä½†åœ¨å™ªå£°ç¯å¢ƒä¸‹å…¶æ€§èƒ½ä¼šæ˜¾è‘—ä¸‹é™ã€‚è™½ç„¶è¯­éŸ³å¢å¼ºï¼ˆSEï¼‰å¯ä»¥æé«˜ç¨³å¥æ€§ï¼Œä½†å®ƒå¾€å¾€ä¼šå¼•å…¥æ©ç›–æƒ…æ„Ÿçº¿ç´¢çš„ä¼ªå½±ï¼Œå¹¶ä¸ºç®¡é“å¢åŠ è®¡ç®—å¼€é”€ã€‚å¤šä»»åŠ¡å­¦ä¹ ï¼ˆMTLï¼‰é€šè¿‡è”åˆä¼˜åŒ–SEå’ŒSERä»»åŠ¡æä¾›äº†ä¸€ç§æ›¿ä»£æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„å…±äº«éª¨å¹²æ¨¡å‹ç»å¸¸å—åˆ°æ¢¯åº¦å¹²æ‰°å’Œä»»åŠ¡è¡¨ç¤ºå†²çªçš„å½±å“ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†çµæ´»çš„MTLæ¡†æ¶â€”â€”ç¨€ç–æ··åˆä¸“å®¶è¡¨ç¤ºé›†æˆæŠ€æœ¯ï¼ˆSparse MERITï¼‰ï¼Œå®ƒåº”ç”¨å¸§çº§ä¸“å®¶è·¯ç”±å¯¹è‡ªç›‘ç£è¯­éŸ³è¡¨ç¤ºè¿›è¡Œå»ºæ¨¡ã€‚Sparse MERITç»“åˆäº†ç‰¹å®šä»»åŠ¡çš„é—¨æ§ç½‘ç»œï¼Œå¯ä»¥åŠ¨æ€åœ°ä»å…±äº«çš„ä¸“å®¶æ± ä¸­ä¸ºæ¯ä¸€å¸§é€‰æ‹©ä¸“å®¶ï¼Œä»è€Œå®ç°å‚æ•°é«˜æ•ˆå’Œä»»åŠ¡è‡ªé€‚åº”çš„è¡¨ç¤ºå­¦ä¹ ã€‚åœ¨MSP-Podcastè¯­æ–™åº“ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSparse MERITåœ¨SERå’ŒSEä»»åŠ¡ä¸Šå‡ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚åœ¨ä¿¡å™ªæ¯”ï¼ˆSNRï¼‰ä¸º-5 dBçš„æœ€å…·æŒ‘æˆ˜æ€§çš„æ¡ä»¶ä¸‹ï¼Œä¸ä¾èµ–SEé¢„å¤„ç†ç­–ç•¥çš„åŸºçº¿ç›¸æ¯”ï¼ŒSparse MERITçš„SER F1-macroå¹³å‡æé«˜äº†12.0%ï¼Œä¸ç®€å•çš„MTLåŸºçº¿ç›¸æ¯”æé«˜äº†3.4%ï¼Œåœ¨æœªè§è¿‡çš„å™ªå£°æ¡ä»¶ä¸‹å…·æœ‰ç»Ÿè®¡å­¦ä¸Šçš„æ˜¾è‘—æ„ä¹‰ã€‚å¯¹äºSEï¼ŒSparse MERITæ¯”SEé¢„å¤„ç†åŸºçº¿æé«˜äº†28.2%çš„æ®µSNRï¼ˆSSNRï¼‰ï¼Œæ¯”ç®€å•çš„MTLåŸºçº¿æé«˜äº†20.0%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒSparse MERITåœ¨å™ªå£°ç¯å¢ƒä¸‹çš„æƒ…æ„Ÿè¯†åˆ«å’Œå¢å¼ºä»»åŠ¡ä¸­éƒ½æä¾›äº†ç¨³å¥å’Œé€šç”¨çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.08470v1">PDF</a> </p>
<p><strong>Summary</strong><br>     è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«åœ¨æ„å»ºæƒ…æ„Ÿæ„ŸçŸ¥è¯­éŸ³ç³»ç»Ÿä¸­æ‰®æ¼”ç€å…³é”®è§’è‰²ï¼Œä½†åœ¨å™ªå£°ç¯å¢ƒä¸‹æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚è™½ç„¶è¯­éŸ³å¢å¼ºå¯ä»¥æé«˜ç¨³å¥æ€§ï¼Œä½†å®ƒå¸¸å¸¸å¼•å…¥æ©ç›–æƒ…æ„Ÿçº¿ç´¢çš„ä¼ªå½±å¹¶å¢åŠ è®¡ç®—å¼€é”€ã€‚å¤šä»»åŠ¡å­¦ä¹ æä¾›äº†ä¸€ä¸ªæ›¿ä»£æ–¹æ¡ˆï¼Œé€šè¿‡è”åˆä¼˜åŒ–è¯­éŸ³å¢å¼ºå’Œè¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ä»»åŠ¡ã€‚ç„¶è€Œï¼Œå¸¸è§„å…±äº«éª¨å¹²æ¨¡å‹å¸¸å—æ¢¯åº¦å¹²æ‰°å’Œä»»åŠ¡è¡¨ç¤ºå†²çªçš„å½±å“ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæå‡ºäº†çµæ´»çš„å¤šä»»åŠ¡å­¦ä¹ æ¡†æ¶â€”â€”Sparse Mixture-of-Experts Representation Integration Technique (Sparse MERIT)ï¼Œå®ƒåº”ç”¨å¸§çº§ä¸“å®¶è·¯ç”±äºè‡ªæˆ‘ç›‘ç£çš„è¯­éŸ³è¡¨ç¤ºä¸Šã€‚Sparse MERITç»“åˆäº†ä»»åŠ¡ç‰¹å®šé—¨æ§ç½‘ç»œï¼ŒåŠ¨æ€ä¸ºæ¯å¸§é€‰æ‹©å…±äº«ä¸“å®¶æ± ä¸­çš„ä¸“å®¶ï¼Œå®ç°å‚æ•°é«˜æ•ˆå’Œä»»åŠ¡è‡ªé€‚åº”çš„è¡¨ç¤ºå­¦ä¹ ã€‚å®éªŒè¡¨æ˜ï¼ŒSparse MERITåœ¨è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«å’Œè¯­éŸ³å¢å¼ºä»»åŠ¡ä¸Šå‡ä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œåœ¨ä¿¡å™ªæ¯”-5 dBçš„æœ€å…·æŒ‘æˆ˜æ¡ä»¶ä¸‹ï¼Œç›¸è¾ƒäºä¾èµ–è¯­éŸ³å¢å¼ºé¢„å¤„ç†çš„åŸºçº¿æ¨¡å‹ï¼ŒSparse MERITåœ¨æƒ…æ„Ÿè¯†åˆ«çš„F1-macroå¾—åˆ†ä¸Šå¹³å‡æé«˜äº†12.0%ï¼Œç›¸è¾ƒäºç®€å•çš„å¤šä»»åŠ¡å­¦ä¹ åŸºçº¿æé«˜äº†3.4%ï¼Œåœ¨æœªè§è¿‡çš„å™ªå£°æ¡ä»¶ä¸‹å…·æœ‰ç»Ÿè®¡æ˜¾è‘—æ€§ã€‚å¯¹äºè¯­éŸ³å¢å¼ºï¼ŒSparse MERITåœ¨åˆ†æ®µä¿¡å™ªæ¯”ï¼ˆSSNRï¼‰ä¸Šæ¯”è¯­éŸ³å¢å¼ºé¢„å¤„ç†åŸºçº¿æé«˜äº†28.2%ï¼Œæ¯”ç®€å•çš„å¤šä»»åŠ¡å­¦ä¹ åŸºçº¿æé«˜äº†20.0%ã€‚ç»“æœè¡¨æ˜ï¼ŒSparse MERITåœ¨å™ªå£°ç¯å¢ƒä¸‹ä¸ºè¯­éŸ³æƒ…æ„Ÿè¯†åˆ«å’Œå¢å¼ºä»»åŠ¡æä¾›äº†ç¨³å¥å’Œå¯æ³›åŒ–çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰åœ¨å™ªå£°ç¯å¢ƒä¸‹æ€§èƒ½ä¸‹é™ã€‚</li>
<li>è¯­éŸ³å¢å¼ºï¼ˆSEï¼‰èƒ½æé«˜ç¨³å¥æ€§ï¼Œä½†å¯èƒ½å¼•å…¥æ©ç›–æƒ…æ„Ÿçº¿ç´¢çš„ä¼ªå½±ã€‚</li>
<li>å¤šä»»åŠ¡å­¦ä¹ ï¼ˆMTLï¼‰é€šè¿‡è”åˆä¼˜åŒ–SEå’ŒSERä»»åŠ¡ä½œä¸ºæ›¿ä»£æ–¹æ¡ˆã€‚</li>
<li>å¸¸è§„å…±äº«éª¨å¹²æ¨¡å‹é¢ä¸´æ¢¯åº¦å¹²æ‰°å’Œè¡¨ç¤ºå†²çªçš„æŒ‘æˆ˜ã€‚</li>
<li>Sparse MERITé€šè¿‡å¸§çº§ä¸“å®¶è·¯ç”±å’Œè‡ªæˆ‘ç›‘ç£çš„è¯­éŸ³è¡¨ç¤ºæä¾›çµæ´»å¤šä»»åŠ¡å­¦ä¹ ã€‚</li>
<li>Sparse MERITåœ¨å™ªå£°ç¯å¢ƒä¸‹æ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œå°¤å…¶åœ¨-5 dBçš„SNRæ¡ä»¶ä¸‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.08470">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Speech/2509.08470v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Speech/2509.08470v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Few-shot-Personalization-via-In-Context-Learning-for-Speech-Emotion-Recognition-based-on-Speech-Language-Model"><a href="#Few-shot-Personalization-via-In-Context-Learning-for-Speech-Emotion-Recognition-based-on-Speech-Language-Model" class="headerlink" title="Few-shot Personalization via In-Context Learning for Speech Emotion   Recognition based on Speech-Language Model"></a>Few-shot Personalization via In-Context Learning for Speech Emotion   Recognition based on Speech-Language Model</h2><p><strong>Authors:Mana Ihori, Taiga Yamane, Naotaka Kawata, Naoki Makishima, Tomohiro Tanaka, Satoshi Suzuki, Shota Orihashi, Ryo Masumura</strong></p>
<p>This paper proposes a personalization method for speech emotion recognition (SER) through in-context learning (ICL). Since the expression of emotions varies from person to person, speaker-specific adaptation is crucial for improving the SER performance. Conventional SER methods have been personalized using emotional utterances of a target speaker, but it is often difficult to prepare utterances corresponding to all emotion labels in advance. Our idea to overcome this difficulty is to obtain speaker characteristics by conditioning a few emotional utterances of the target speaker in ICL-based inference. ICL is a method to perform unseen tasks by conditioning a few input-output examples through inference in large language models (LLMs). We meta-train a speech-language model extended from the LLM to learn how to perform personalized SER via ICL. Experimental results using our newly collected SER dataset demonstrate that the proposed method outperforms conventional methods. </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰çš„è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰ä¸ªæ€§åŒ–æ–¹æ³•ã€‚ç”±äºæƒ…æ„Ÿçš„è¡¨è¾¾å› äººè€Œå¼‚ï¼Œå› æ­¤é’ˆå¯¹ç‰¹å®šè¯´è¯äººçš„é€‚åº”å¯¹äºæé«˜SERæ€§èƒ½è‡³å…³é‡è¦ã€‚ä¼ ç»Ÿçš„SERæ–¹æ³•å·²ç»é€šè¿‡ç›®æ ‡è¯´è¯äººçš„æƒ…æ„Ÿè¯è¯­è¿›è¡Œäº†ä¸ªæ€§åŒ–å¤„ç†ï¼Œä½†å¾€å¾€éš¾ä»¥æå‰å‡†å¤‡ä¸æ‰€æœ‰æƒ…æ„Ÿæ ‡ç­¾ç›¸å¯¹åº”çš„è¯è¯­ã€‚æˆ‘ä»¬å…‹æœè¿™ä¸€å›°éš¾çš„æƒ³æ³•æ˜¯é€šè¿‡åœ¨åŸºäºICLçš„æ¨ç†ä¸­æ¡ä»¶åŒ–ç›®æ ‡è¯´è¯äººçš„å°‘æ•°æƒ…æ„Ÿè¯è¯­æ¥è·å¾—è¯´è¯äººç‰¹å¾ã€‚ICLæ˜¯ä¸€ç§é€šè¿‡åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­è¿›è¡Œæ¨ç†æ¥æ‰§è¡Œæœªè§ä»»åŠ¡çš„æ–¹æ³•ï¼Œé€šè¿‡å°‘æ•°è¾“å…¥-è¾“å‡ºç¤ºä¾‹è¿›è¡Œæ¡ä»¶å¤„ç†ã€‚æˆ‘ä»¬å¯¹ä»LLMæ‰©å±•çš„è¯­éŸ³è¯­è¨€æ¨¡å‹è¿›è¡Œå…ƒè®­ç»ƒï¼Œä»¥å­¦ä¹ å¦‚ä½•é€šè¿‡ICLæ‰§è¡Œä¸ªæ€§åŒ–çš„SERã€‚ä½¿ç”¨æˆ‘ä»¬æ–°æ”¶é›†çš„SERæ•°æ®é›†è¿›è¡Œçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.08344v1">PDF</a> Accepted by ASRU 2025</p>
<p><strong>æ€»ç»“</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰çš„ä¸ªæ€§åŒ–è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰æ–¹æ³•ã€‚ç”±äºæƒ…æ„Ÿè¡¨è¾¾å…·æœ‰ä¸ªä½“å·®å¼‚ï¼Œé’ˆå¯¹ç‰¹å®šè¯´è¯äººçš„é€‚åº”æ€§å¯¹äºæé«˜SERæ€§èƒ½è‡³å…³é‡è¦ã€‚è™½ç„¶ä¼ ç»Ÿæ–¹æ³•å·²å°è¯•é€šè¿‡ç›®æ ‡è¯´è¯äººçš„æƒ…æ„Ÿè¯è¯­å®ç°ä¸ªæ€§åŒ–ï¼Œä½†é¢„å…ˆå‡†å¤‡æ‰€æœ‰æƒ…æ„Ÿæ ‡ç­¾çš„è¯­å¥å¾€å¾€éå¸¸å›°éš¾ã€‚æœ¬ç ”ç©¶é€šè¿‡ICLæ¨ç†ä¸­çš„æ¡ä»¶è¯­å¥è·å¾—è¯´è¯äººçš„ç‰¹æ€§æ¥å…‹æœè¿™ä¸€éš¾é¢˜ã€‚ICLæ˜¯ä¸€ç§åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­é€šè¿‡æ¨ç†å¤„ç†æœªè§ä»»åŠ¡çš„æ–¹æ³•ã€‚æœ¬ç ”ç©¶é€šè¿‡å…ƒè®­ç»ƒæ‰©å±•äº†è¯­éŸ³è¯­è¨€æ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿå­¦ä¹ å¦‚ä½•é€šè¿‡ICLæ‰§è¡Œä¸ªæ€§åŒ–SERã€‚ä½¿ç”¨æ–°æ”¶é›†çš„SERæ•°æ®é›†çš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚</p>
<p><strong>è¦ç‚¹åˆ†æ</strong></p>
<ol>
<li>è®ºæ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰çš„ä¸ªæ€§åŒ–æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åŸºäºä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰ã€‚</li>
<li>æƒ…æ„Ÿè¡¨è¾¾å…·æœ‰ä¸ªä½“å·®å¼‚ï¼Œå› æ­¤é’ˆå¯¹ç‰¹å®šè¯´è¯äººçš„é€‚åº”æ€§å¯¹äºæé«˜SERæ€§èƒ½è‡³å…³é‡è¦ã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•ä½¿ç”¨ç›®æ ‡è¯´è¯äººçš„æƒ…æ„Ÿè¯è¯­è¿›è¡Œä¸ªæ€§åŒ–ï¼Œä½†é¢„å…ˆå‡†å¤‡æ‰€æœ‰æƒ…æ„Ÿæ ‡ç­¾çš„è¯­å¥æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚</li>
<li>ICLæ˜¯ä¸€ç§åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­è¿›è¡Œæ¨ç†çš„æ–¹æ³•ï¼Œå¯ä»¥é€šè¿‡å°‘é‡çš„è¾“å…¥-è¾“å‡ºç¤ºä¾‹æ¥å¤„ç†æœªè§ä»»åŠ¡ã€‚</li>
<li>ç ”ç©¶ä¸­çš„è¯­éŸ³è¯­è¨€æ¨¡å‹é€šè¿‡å…ƒè®­ç»ƒå­¦ä¹ å¦‚ä½•é€šè¿‡ICLæ‰§è¡Œä¸ªæ€§åŒ–SERã€‚</li>
<li>ä½¿ç”¨æ–°æ”¶é›†çš„SERæ•°æ®é›†è¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†ä¼ ç»Ÿæ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•æä¾›äº†ä¸€ç§æœ‰æ•ˆåˆ©ç”¨å°‘é‡æƒ…æ„Ÿè¯­å¥æ¥æ•æ‰ç›®æ ‡è¯´è¯äººçš„ç‰¹æ€§ï¼Œä»è€Œæé«˜äº†SERçš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.08344">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Speech/2509.08344v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Speech/2509.08344v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Speech/2509.08344v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Speech/2509.08344v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Speech/2509.08344v1/page_5_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Speech/2509.08344v1/page_5_2.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="A-Bottom-up-Framework-with-Language-universal-Speech-Attribute-Modeling-for-Syllable-based-ASR"><a href="#A-Bottom-up-Framework-with-Language-universal-Speech-Attribute-Modeling-for-Syllable-based-ASR" class="headerlink" title="A Bottom-up Framework with Language-universal Speech Attribute Modeling   for Syllable-based ASR"></a>A Bottom-up Framework with Language-universal Speech Attribute Modeling   for Syllable-based ASR</h2><p><strong>Authors:Hao Yen, Pin-Jui Ku, Sabato Marco Siniscalchi, Chin-Hui Lee</strong></p>
<p>We propose a bottom-up framework for automatic speech recognition (ASR) in syllable-based languages by unifying language-universal articulatory attribute modeling with syllable-level prediction. The system first recognizes sequences or lattices of articulatory attributes that serve as a language-universal, interpretable representation of pronunciation, and then transforms them into syllables through a structured knowledge integration process. We introduce two evaluation metrics, namely Pronunciation Error Rate (PrER) and Syllable Homonym Error Rate (SHER), to evaluate the modelâ€™s ability to capture pronunciation and handle syllable ambiguities. Experimental results on the AISHELL-1 Mandarin corpus demonstrate that the proposed bottom-up framework achieves competitive performance and exhibits better robustness under low-resource conditions compared to the direct syllable prediction model. Furthermore, we investigate the zero-shot cross-lingual transferability on Japanese and demonstrate significant improvements over character- and phoneme-based baselines by 40% error rate reduction. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºåº•å±‚çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¡†æ¶ï¼Œç”¨äºéŸ³èŠ‚ç±»è¯­è¨€ï¼Œé€šè¿‡ç»Ÿä¸€è¯­è¨€é€šç”¨çš„å‘éŸ³å±æ€§å»ºæ¨¡ä¸éŸ³èŠ‚çº§é¢„æµ‹æ¥å®ç°ã€‚è¯¥ç³»ç»Ÿé¦–å…ˆè¯†åˆ«å‘éŸ³å±æ€§çš„åºåˆ—æˆ–ç½‘æ ¼ï¼Œä½œä¸ºè¯­è¨€é€šç”¨çš„å¯è§£é‡Šå‘éŸ³è¡¨ç¤ºï¼Œç„¶åé€šè¿‡ç»“æ„åŒ–çŸ¥è¯†æ•´åˆè¿‡ç¨‹å°†å®ƒä»¬è½¬æ¢ä¸ºéŸ³èŠ‚ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸¤ç§è¯„ä¼°æŒ‡æ ‡ï¼Œå³å‘éŸ³é”™è¯¯ç‡ï¼ˆPrERï¼‰å’ŒéŸ³èŠ‚åŒéŸ³å­—é”™è¯¯ç‡ï¼ˆSHERï¼‰ï¼Œä»¥è¯„ä¼°æ¨¡å‹æ•æ‰å‘éŸ³å’Œå¤„ç†éŸ³èŠ‚æ­§ä¹‰çš„èƒ½åŠ›ã€‚åœ¨AISHELL-1æ™®é€šè¯è¯­æ–™åº“ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æåº•å±‚æ¡†æ¶çš„æ€§èƒ½å…·æœ‰ç«äº‰åŠ›ï¼Œä¸ç›´æ¥éŸ³èŠ‚é¢„æµ‹æ¨¡å‹ç›¸æ¯”ï¼Œåœ¨ä½èµ„æºæ¡ä»¶ä¸‹è¡¨ç°å‡ºæ›´å¥½çš„ç¨³å¥æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨æ—¥è¯­ä¸Šè¿›è¡Œäº†é›¶æ ·æœ¬è·¨è¯­è¨€å¯è¿ç§»æ€§çš„ç ”ç©¶ï¼Œç›¸è¾ƒäºå­—ç¬¦å’ŒéŸ³ç´ åŸºå‡†çº¿ï¼Œå®ç°äº†40%çš„é”™è¯¯ç‡é™ä½ï¼Œå–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.08173v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§é¢å‘éŸ³èŠ‚è¯­è¨€çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰çš„è‡ªä¸‹è€Œä¸Šæ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†è¯­è¨€é€šç”¨çš„å‘éŸ³å±æ€§å»ºæ¨¡ä¸éŸ³èŠ‚çº§åˆ«çš„é¢„æµ‹ã€‚ç³»ç»Ÿé¦–å…ˆè¯†åˆ«å‘éŸ³å±æ€§çš„åºåˆ—æˆ–æ ¼ç½‘ï¼Œä½œä¸ºè¯­è¨€é€šç”¨çš„å¯è§£é‡Šå‘éŸ³è¡¨ç¤ºï¼Œç„¶åé€šè¿‡ç»“æ„åŒ–çŸ¥è¯†æ•´åˆè¿‡ç¨‹å°†å®ƒä»¬è½¬åŒ–ä¸ºéŸ³èŠ‚ã€‚å¼•å…¥ä¸¤ç§è¯„ä¼°æŒ‡æ ‡â€”â€”å‘éŸ³é”™è¯¯ç‡å’ŒéŸ³èŠ‚åŒéŸ³è¯é”™è¯¯ç‡ï¼Œä»¥è¯„ä¼°æ¨¡å‹æ•æ‰å‘éŸ³å’Œå¤„ç†éŸ³èŠ‚æ­§ä¹‰çš„èƒ½åŠ›ã€‚åœ¨AISHELL-1æ™®é€šè¯è¯­æ–™åº“ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ç›´æ¥éŸ³èŠ‚é¢„æµ‹æ¨¡å‹ç›¸æ¯”ï¼Œæ‰€æè‡ªä¸‹è€Œä¸Šæ¡†æ¶å…·æœ‰ç«äº‰åŠ›ï¼Œå¹¶åœ¨ä½èµ„æºæ¡ä»¶ä¸‹è¡¨ç°å‡ºæ›´å¥½çš„ç¨³å¥æ€§ã€‚æ­¤å¤–ï¼Œåœ¨æ—¥è¯­ä¸Šçš„é›¶æ ·æœ¬è·¨è¯­è¨€è¿ç§»èƒ½åŠ›æµ‹è¯•ä¹Ÿè¯æ˜äº†è¯¥æ¡†æ¶çš„æ˜¾è‘—æ”¹è¿›ï¼Œç›¸æ¯”å­—ç¬¦å’ŒéŸ³ç´ åŸºçº¿è¯¯å·®ç‡é™ä½äº†40%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æå‡ºä¸€ç§é’ˆå¯¹éŸ³èŠ‚è¯­è¨€çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰è‡ªä¸‹è€Œä¸Šæ¡†æ¶ã€‚</li>
<li>ç»“åˆè¯­è¨€é€šç”¨çš„å‘éŸ³å±æ€§å»ºæ¨¡ä¸éŸ³èŠ‚çº§åˆ«é¢„æµ‹ã€‚</li>
<li>é€šè¿‡è¯†åˆ«å‘éŸ³å±æ€§åºåˆ—æˆ–æ ¼ç½‘ï¼Œè½¬åŒ–ä¸ºè¯­è¨€é€šç”¨çš„å¯è§£é‡Šå‘éŸ³è¡¨ç¤ºã€‚</li>
<li>å¼•å…¥å‘éŸ³é”™è¯¯ç‡å’ŒéŸ³èŠ‚åŒéŸ³è¯é”™è¯¯ç‡ä¸¤ç§è¯„ä¼°æŒ‡æ ‡ã€‚</li>
<li>åœ¨AISHELL-1æ™®é€šè¯è¯­æ–™åº“ä¸Šè¡¨ç°å‡ºè‰¯å¥½æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½èµ„æºæ¡ä»¶ä¸‹ã€‚</li>
<li>æ˜¾ç¤ºå‡ºæ˜¾è‘—çš„è·¨è¯­è¨€è¿ç§»èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨æ—¥è¯­ä¸Šçš„è¡¨ç°ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.08173">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Speech/2509.08173v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Speech/2509.08173v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Speech/2509.08173v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Speech/2509.08173v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Speech/2509.08173v1/page_3_1.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Affine-Modulation-based-Audiogram-Fusion-Network-for-Joint-Noise-Reduction-and-Hearing-Loss-Compensation"><a href="#Affine-Modulation-based-Audiogram-Fusion-Network-for-Joint-Noise-Reduction-and-Hearing-Loss-Compensation" class="headerlink" title="Affine Modulation-based Audiogram Fusion Network for Joint Noise   Reduction and Hearing Loss Compensation"></a>Affine Modulation-based Audiogram Fusion Network for Joint Noise   Reduction and Hearing Loss Compensation</h2><p><strong>Authors:Ye Ni, Ruiyu Liang, Xiaoshuai Hao, Jiaming Cheng, Qingyun Wang, Chengwei Huang, Cairong Zou, Wei Zhou, Weiping Ding, BjÃ¶rn W. Schuller</strong></p>
<p>Hearing aids (HAs) are widely used to provide personalized speech enhancement (PSE) services, improving the quality of life for individuals with hearing loss. However, HA performance significantly declines in noisy environments as it treats noise reduction (NR) and hearing loss compensation (HLC) as separate tasks. This separation leads to a lack of systematic optimization, overlooking the interactions between these two critical tasks, and increases the system complexity. To address these challenges, we propose a novel audiogram fusion network, named AFN-HearNet, which simultaneously tackles the NR and HLC tasks by fusing cross-domain audiogram and spectrum features. We propose an audiogram-specific encoder that transforms the sparse audiogram profile into a deep representation, addressing the alignment problem of cross-domain features prior to fusion. To incorporate the interactions between NR and HLC tasks, we propose the affine modulation-based audiogram fusion frequency-temporal Conformer that adaptively fuses these two features into a unified deep representation for speech reconstruction. Furthermore, we introduce a voice activity detection auxiliary training task to embed speech and non-speech patterns into the unified deep representation implicitly. We conduct comprehensive experiments across multiple datasets to validate the effectiveness of each proposed module. The results indicate that the AFN-HearNet significantly outperforms state-of-the-art in-context fusion joint models regarding key metrics such as HASQI and PESQ, achieving a considerable trade-off between performance and efficiency. The source code and data will be released at <a target="_blank" rel="noopener" href="https://github.com/deepnetni/AFN-HearNet">https://github.com/deepnetni/AFN-HearNet</a>. </p>
<blockquote>
<p>åŠ©å¬å™¨ï¼ˆHAsï¼‰å¹¿æ³›åº”ç”¨äºä¸ªæ€§åŒ–è¯­éŸ³å¢å¼ºï¼ˆPSEï¼‰æœåŠ¡ï¼Œä»¥æé«˜å¬éšœäººå£«çš„ç”Ÿæ´»è´¨é‡ã€‚ç„¶è€Œï¼Œåœ¨å˜ˆæ‚çš„ç¯å¢ƒä¸­ï¼ŒåŠ©å¬å™¨çš„æ€§èƒ½ä¼šå¤§å¹…ä¸‹é™ï¼Œå› ä¸ºå®ƒå°†é™å™ªï¼ˆNRï¼‰å’Œå¬åŠ›æŸå¤±è¡¥å¿ï¼ˆHLCï¼‰è§†ä¸ºå•ç‹¬çš„ä»»åŠ¡ã€‚è¿™ç§åˆ†ç¦»å¯¼è‡´ç¼ºä¹ç³»ç»Ÿä¼˜åŒ–ï¼Œå¿½è§†äº†è¿™ä¸¤ä¸ªå…³é”®ä»»åŠ¡ä¹‹é—´çš„ç›¸äº’ä½œç”¨ï¼Œå¹¶å¢åŠ äº†ç³»ç»Ÿå¤æ‚æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹éŸ³é¢‘å›¾èåˆç½‘ç»œï¼Œåä¸ºAFN-HearNetï¼Œå®ƒé€šè¿‡èåˆè·¨åŸŸéŸ³é¢‘å›¾å’Œå…‰è°±ç‰¹å¾ï¼ŒåŒæ—¶è§£å†³NRå’ŒHLCä»»åŠ¡ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§é’ˆå¯¹éŸ³é¢‘å›¾çš„ç‰¹å®šç¼–ç å™¨ï¼Œå°†ç¨€ç–éŸ³é¢‘å›¾è½®å»“è½¬æ¢ä¸ºæ·±åº¦è¡¨ç¤ºï¼Œè§£å†³è·¨åŸŸç‰¹å¾èåˆä¹‹å‰çš„å¯¹é½é—®é¢˜ã€‚ä¸ºäº†ç»“åˆNRå’ŒHLCä»»åŠ¡ä¹‹é—´çš„ç›¸äº’ä½œç”¨ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºä»¿å°„è°ƒåˆ¶çš„éŸ³é¢‘å›¾èåˆæ—¶ç©ºå·ç§¯ç½‘ç»œï¼Œè¯¥ç½‘ç»œè‡ªé€‚åº”åœ°å°†è¿™ä¸¤ä¸ªç‰¹å¾èåˆä¸ºç»Ÿä¸€çš„æ·±åº¦è¡¨ç¤ºï¼Œç”¨äºè¯­éŸ³é‡å»ºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªè¯­éŸ³æ´»åŠ¨æ£€æµ‹è¾…åŠ©è®­ç»ƒä»»åŠ¡ï¼Œå°†è¯­éŸ³å’Œéè¯­éŸ³æ¨¡å¼éšå¼åœ°åµŒå…¥åˆ°ç»Ÿä¸€çš„æ·±åº¦è¡¨ç¤ºä¸­ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†å…¨é¢çš„å®éªŒï¼Œä»¥éªŒè¯æ‰€æå‡ºæ¨¡å—çš„æœ‰æ•ˆæ€§ã€‚ç»“æœè¡¨æ˜ï¼ŒAFN-HearNetåœ¨å…³é”®æŒ‡æ ‡HASQIå’ŒPESQæ–¹é¢æ˜¾è‘—ä¼˜äºæœ€æ–°çš„ä¸Šä¸‹æ–‡èåˆè”åˆæ¨¡å‹ï¼Œåœ¨æ€§èƒ½å’Œæ•ˆç‡ä¹‹é—´å®ç°äº†å¯è§‚çš„æƒè¡¡ã€‚æºä»£ç å’Œæ•°æ®å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/deepnetni/AFN-HearNet%E4%B8%8A%E5%8F%91%E5%B8%A%E6%8A%95%E6%94%BE">https://github.com/deepnetni/AFN-HearNetä¸Šå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.07341v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>åŠ©å¬å™¨ï¼ˆHAï¼‰æä¾›ä¸ªæ€§åŒ–è¯­éŸ³å¢å¼ºï¼ˆPSEï¼‰æœåŠ¡ï¼Œæé«˜å¬éšœäººå£«çš„ç”Ÿæ´»è´¨é‡ã€‚ç„¶è€Œï¼Œåœ¨å˜ˆæ‚ç¯å¢ƒä¸­ï¼ŒåŠ©å¬å™¨æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œå®ƒå°†å™ªå£°é™ä½ï¼ˆNRï¼‰å’Œå¬åŠ›æŸå¤±è¡¥å¿ï¼ˆHLCï¼‰è§†ä¸ºå•ç‹¬çš„ä»»åŠ¡å¤„ç†ã€‚è¿™ç§åˆ†ç¦»å¯¼è‡´ç¼ºä¹ç³»ç»Ÿä¼˜åŒ–ï¼Œå¿½ç•¥äº†è¿™ä¸¤ä¸ªå…³é”®ä»»åŠ¡ä¹‹é—´çš„ç›¸äº’ä½œç”¨ï¼Œå¹¶å¢åŠ äº†ç³»ç»Ÿå¤æ‚æ€§ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºAFN-HearNetçš„æ–°å‹å¬åŠ›å›¾èåˆç½‘ç»œï¼Œå¯åŒæ—¶è§£å†³NRå’ŒHLCä»»åŠ¡ï¼Œé€šè¿‡èåˆè·¨åŸŸå¬åŠ›å›¾å’Œé¢‘è°±ç‰¹å¾ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå¬åŠ›å›¾ç‰¹å®šç¼–ç å™¨ï¼Œå°†ç¨€ç–å¬åŠ›å›¾è½¬æ¢ä¸ºæ·±åº¦è¡¨ç¤ºï¼Œè§£å†³äº†è·¨åŸŸç‰¹å¾å¯¹é½é—®é¢˜ã€‚ä¸ºäº†èå…¥NRå’ŒHLCä»»åŠ¡é—´çš„äº¤äº’ä½œç”¨ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºä»¿å°„è°ƒåˆ¶çš„å¬åŠ›å›¾èåˆé¢‘æ—¶å·ç§¯ç¥ç»ç½‘ç»œï¼ˆConformerï¼‰ï¼Œè‡ªé€‚åº”åœ°å°†è¿™ä¸¤ä¸ªç‰¹å¾èåˆä¸ºç»Ÿä¸€çš„æ·±åº¦è¡¨ç¤ºç”¨äºè¯­éŸ³é‡å»ºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†è¯­éŸ³æ´»åŠ¨æ£€æµ‹è¾…åŠ©è®­ç»ƒä»»åŠ¡ï¼Œå°†è¯­éŸ³å’Œéè¯­éŸ³æ¨¡å¼åµŒå…¥åˆ°ç»Ÿä¸€çš„æ·±åº¦è¡¨ç¤ºä¸­ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†å…¨é¢çš„å®éªŒï¼ŒéªŒè¯äº†æ¯ä¸ªæ¨¡å—çš„æœ‰æ•ˆæ€§ã€‚ç»“æœè¡¨æ˜ï¼ŒAFN-HearNetåœ¨å…³é”®æŒ‡æ ‡HASQIå’ŒPESQä¸Šæ˜¾è‘—ä¼˜äºæœ€æ–°çš„ä¸Šä¸‹æ–‡èåˆè”åˆæ¨¡å‹ï¼Œå®ç°äº†æ€§èƒ½å’Œæ•ˆç‡ä¹‹é—´çš„è‰¯å¥½æƒè¡¡ã€‚æºä»£ç å’Œæ•°æ®å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/deepnetni/AFN-HearNet%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/deepnetni/AFN-HearNetå‘å¸ƒã€‚</a></p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>åŠ©å¬å™¨åœ¨å˜ˆæ‚ç¯å¢ƒä¸­æ€§èƒ½ä¸‹é™ï¼Œå› ä¸ºå°†å™ªå£°é™ä½å’Œå¬åŠ›æŸå¤±è¡¥å¿è§†ä¸ºç‹¬ç«‹ä»»åŠ¡å¤„ç†ã€‚</li>
<li>æå‡ºæ–°å‹AFN-HearNetç½‘ç»œï¼Œèåˆè·¨åŸŸå¬åŠ›å›¾å’Œé¢‘è°±ç‰¹å¾ä»¥æ”¹å–„åŠ©å¬å™¨æ€§èƒ½ã€‚</li>
<li>AFN-HearNetå¼•å…¥å¬åŠ›å›¾ç‰¹å®šç¼–ç å™¨ä»¥è½¬æ¢ç¨€ç–å¬åŠ›å›¾åˆ°æ·±åº¦è¡¨ç¤ºå½¢å¼ã€‚</li>
<li>å¼•å…¥ä»¿å°„è°ƒåˆ¶Conformeræ¨¡å—æ¥é€‚åº”åœ°èåˆNRå’ŒHLCç‰¹å¾ã€‚</li>
<li>é€šè¿‡è¯­éŸ³æ´»åŠ¨æ£€æµ‹è¾…åŠ©è®­ç»ƒä»»åŠ¡åµŒå…¥è¯­éŸ³å’Œéè¯­éŸ³æ¨¡å¼åˆ°ç»Ÿä¸€æ·±åº¦è¡¨ç¤ºä¸­ã€‚</li>
<li>å®éªŒè¯æ˜AFN-HearNetåœ¨å…³é”®æŒ‡æ ‡ä¸Šä¼˜äºå…¶ä»–æ¨¡å‹ï¼Œå®ç°äº†æ€§èƒ½å’Œæ•ˆç‡çš„å¹³è¡¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.07341">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Speech/2509.07341v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Speech/2509.07341v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Speech/2509.07341v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Speech/2509.07341v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="SaD-A-Scenario-Aware-Discriminator-for-Speech-Enhancement"><a href="#SaD-A-Scenario-Aware-Discriminator-for-Speech-Enhancement" class="headerlink" title="SaD: A Scenario-Aware Discriminator for Speech Enhancement"></a>SaD: A Scenario-Aware Discriminator for Speech Enhancement</h2><p><strong>Authors:Xihao Yuan, Siqi Liu, Yan Chen, Hang Zhou, Chang Liu, Hanting Chen, Jie Hu</strong></p>
<p>Generative adversarial network-based models have shown remarkable performance in the field of speech enhancement. However, the current optimization strategies for these models predominantly focus on refining the architecture of the generator or enhancing the quality evaluation metrics of the discriminator. This approach often overlooks the rich contextual information inherent in diverse scenarios. In this paper, we propose a scenario-aware discriminator that captures scene-specific features and performs frequency-domain division, thereby enabling a more accurate quality assessment of the enhanced speech generated by the generator. We conducted comprehensive experiments on three representative models using two publicly available datasets. The results demonstrate that our method can effectively adapt to various generator architectures without altering their structure, thereby unlocking further performance gains in speech enhancement across different scenarios. </p>
<blockquote>
<p>åŸºäºç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰çš„æ¨¡å‹åœ¨è¯­éŸ³å¢å¼ºé¢†åŸŸè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå½“å‰é’ˆå¯¹è¿™äº›æ¨¡å‹çš„ä¼˜åŒ–ç­–ç•¥ä¸»è¦é›†ä¸­åœ¨æ”¹è¿›ç”Ÿæˆå™¨çš„æ¶æ„æˆ–æé«˜åˆ¤åˆ«å™¨çš„è´¨é‡è¯„ä¼°æŒ‡æ ‡ä¸Šã€‚è¿™ç§æ–¹æ³•å¾€å¾€ä¼šå¿½ç•¥ä¸åŒåœºæ™¯ä¸­ä¸°å¯Œçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åœºæ™¯æ„ŸçŸ¥åˆ¤åˆ«å™¨ï¼Œè¯¥åˆ¤åˆ«å™¨èƒ½å¤Ÿæ•è·åœºæ™¯ç‰¹å®šç‰¹å¾å¹¶æ‰§è¡Œé¢‘åŸŸåˆ†å‰²ï¼Œä»è€Œå®ç°å¯¹ç”Ÿæˆå™¨ç”Ÿæˆçš„å¢å¼ºè¯­éŸ³çš„æ›´å‡†ç¡®è´¨é‡è¯„ä¼°ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªä»£è¡¨æ€§æ¨¡å‹ä¸Šä½¿ç”¨äº†ä¸¤ä¸ªå…¬å¼€æ•°æ®é›†è¿›è¡Œäº†å…¨é¢çš„å®éªŒã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥æœ‰æ•ˆåœ°é€‚åº”å„ç§ç”Ÿæˆå™¨æ¶æ„ï¼Œè€Œæ— éœ€æ”¹å˜å…¶ç»“æ„ï¼Œä»è€Œåœ¨ä¸åŒåœºæ™¯çš„è¯­éŸ³å¢å¼ºä¸­å®ç°äº†è¿›ä¸€æ­¥çš„æ€§èƒ½æå‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.00405v2">PDF</a> 5 pages, 2 figures. Accepted by InterSpeech2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºåœºæ™¯æ„ŸçŸ¥çš„åˆ¤åˆ«å™¨ï¼Œç”¨äºæ•æ‰åœºæ™¯ç‰¹å®šç‰¹å¾å¹¶æ‰§è¡Œé¢‘åŸŸåˆ†å‰²ï¼Œä»è€Œæ›´å‡†ç¡®åœ°å¯¹ç”Ÿæˆå™¨ç”Ÿæˆçš„å¢å¼ºè¯­éŸ³è¿›è¡Œè´¨é‡è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å¯æœ‰æ•ˆé€‚åº”å„ç§ç”Ÿæˆå™¨æ¶æ„ï¼Œæ— éœ€æ”¹å˜å…¶ç»“æ„ï¼Œå¯åœ¨ä¸åŒåœºæ™¯ä¸‹å®ç°è¯­éŸ³å¢å¼ºçš„æ€§èƒ½æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆå¯¹æŠ—ç½‘ç»œæ¨¡å‹åœ¨è¯­éŸ³å¢å¼ºé¢†åŸŸè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>å½“å‰ä¼˜åŒ–ç­–ç•¥ä¸»è¦å…³æ³¨ç”Ÿæˆå™¨çš„æ¶æ„æˆ–åˆ¤åˆ«å™¨è´¨é‡è¯„ä¼°æŒ‡æ ‡çš„æ”¹è¿›ã€‚</li>
<li>æå‡ºçš„åœºæ™¯æ„ŸçŸ¥åˆ¤åˆ«å™¨èƒ½æ•æ‰åœºæ™¯ç‰¹å®šç‰¹å¾ï¼Œå¹¶è¿›è¡Œé¢‘åŸŸåˆ†å‰²ã€‚</li>
<li>åˆ¤åˆ«å™¨èƒ½æé«˜å¯¹ç”Ÿæˆå™¨äº§ç”Ÿçš„å¢å¼ºè¯­éŸ³çš„è´¨é‡è¯„ä¼°å‡†ç¡®æ€§ã€‚</li>
<li>æ–¹æ³•é€‚åº”å¤šç§ç”Ÿæˆå™¨æ¶æ„ï¼Œæ— éœ€æ”¹å˜å…¶ç»“æ„ã€‚</li>
<li>åœ¨ä¸åŒåœºæ™¯ä¸‹ï¼Œè¯¥æ–¹æ³•èƒ½å®ç°è¯­éŸ³å¢å¼ºçš„æ€§èƒ½æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.00405">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Speech/2509.00405v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Speech/2509.00405v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Speech/2509.00405v2/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Speech/2509.00405v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Speech/2509.00405v2/page_3_1.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="The-Model-Hears-You-Audio-Language-Model-Deployments-Should-Consider-the-Principle-of-Least-Privilege"><a href="#The-Model-Hears-You-Audio-Language-Model-Deployments-Should-Consider-the-Principle-of-Least-Privilege" class="headerlink" title="The Model Hears You: Audio Language Model Deployments Should Consider   the Principle of Least Privilege"></a>The Model Hears You: Audio Language Model Deployments Should Consider   the Principle of Least Privilege</h2><p><strong>Authors:Luxi He, Xiangyu Qi, Michel Liao, Inyoung Cheong, Prateek Mittal, Danqi Chen, Peter Henderson</strong></p>
<p>The latest Audio Language Models (Audio LMs) process speech directly instead of relying on a separate transcription step. This shift preserves detailed information, such as intonation or the presence of multiple speakers, that would otherwise be lost in transcription. However, it also introduces new safety risks, including the potential misuse of speaker identity cues and other sensitive vocal attributes, which could have legal implications. In this paper, we urge a closer examination of how these models are built and deployed. Our experiments show that end-to-end modeling, compared with cascaded pipelines, creates socio-technical safety risks such as identity inference, biased decision-making, and emotion detection. This raises concerns about whether Audio LMs store voiceprints and function in ways that create uncertainty under existing legal regimes. We then argue that the Principle of Least Privilege should be considered to guide the development and deployment of these models. Specifically, evaluations should assess (1) the privacy and safety risks associated with end-to-end modeling; and (2) the appropriate scope of information access. Finally, we highlight related gaps in current audio LM benchmarks and identify key open research questions, both technical and policy-related, that must be addressed to enable the responsible deployment of end-to-end Audio LMs. </p>
<blockquote>
<p>æœ€æ–°çš„éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆAudio LMsï¼‰èƒ½å¤Ÿç›´æ¥å¤„ç†è¯­éŸ³ï¼Œè€Œæ— éœ€ä¾èµ–å•ç‹¬çš„è½¬å½•æ­¥éª¤ã€‚è¿™ç§è½¬å˜ä¿ç•™äº†è¯¦ç»†ä¿¡æ¯ï¼Œå¦‚è¯­è°ƒæˆ–å¤šåæ¼”è®²è€…çš„å­˜åœ¨ï¼Œè¿™äº›ä¿¡æ¯åœ¨è½¬å½•è¿‡ç¨‹ä¸­å¯èƒ½ä¼šä¸¢å¤±ã€‚ç„¶è€Œï¼Œè¿™ä¹Ÿå¸¦æ¥äº†æ–°çš„å®‰å…¨é£é™©ï¼ŒåŒ…æ‹¬å¯èƒ½æ»¥ç”¨è¯´è¯äººèº«ä»½çº¿ç´¢å’Œå…¶ä»–æ•æ„Ÿè¯­éŸ³ç‰¹å¾ï¼Œå¹¶å¯èƒ½æ¶‰åŠæ³•å¾‹é£é™©ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ•¦ä¿ƒå¯¹å¦‚ä½•æ„å»ºå’Œéƒ¨ç½²è¿™äº›æ¨¡å‹è¿›è¡Œæ›´æ·±å…¥çš„å®¡æŸ¥ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œä¸çº§è”ç®¡é“ç›¸æ¯”ï¼Œç«¯åˆ°ç«¯çš„å»ºæ¨¡ä¼šäº§ç”Ÿç¤¾ä¼šæŠ€æœ¯å®‰å…¨é£é™©ï¼Œå¦‚èº«ä»½æ¨æ–­ã€å†³ç­–åè§å’Œæƒ…ç»ªæ£€æµ‹ã€‚è¿™å¼•å‘äº†äººä»¬å¯¹Audio LMsæ˜¯å¦å­˜å‚¨è¯­éŸ³ç‰¹å¾å¹¶ä»¥åœ¨å½“å‰æ³•å¾‹ä½“ç³»ä¸‹äº§ç”Ÿä¸ç¡®å®šæ€§æ–¹å¼è¿è¡Œçš„æ‹…å¿§ã€‚ç„¶åï¼Œæˆ‘ä»¬ä¸»å¼ è€ƒè™‘â€œæœ€å°ç‰¹æƒåŸåˆ™â€æ¥æŒ‡å¯¼è¿™äº›æ¨¡å‹çš„å¼€å‘å’Œéƒ¨ç½²ã€‚å…·ä½“è€Œè¨€ï¼Œè¯„ä¼°åº”è¯¥åŒ…æ‹¬ï¼ˆ1ï¼‰ä¸ç«¯åˆ°ç«¯å»ºæ¨¡ç›¸å…³çš„éšç§å’Œå®‰å…¨é£é™©ï¼›ï¼ˆ2ï¼‰ä¿¡æ¯è®¿é—®çš„é€‚å½“èŒƒå›´ã€‚æœ€åï¼Œæˆ‘ä»¬å¼ºè°ƒäº†å½“å‰éŸ³é¢‘LMåŸºå‡†æµ‹è¯•ä¸­çš„ç›¸å…³ç©ºç™½ï¼Œå¹¶ç¡®å®šäº†å¿…é¡»è§£å†³çš„å…³é”®å¼€æ”¾ç ”ç©¶é—®é¢˜ï¼ŒåŒ…æ‹¬æŠ€æœ¯å’Œæ”¿ç­–ç›¸å…³é—®é¢˜ï¼Œä»¥å®ç°ç«¯åˆ°ç«¯Audio LMsçš„è´£ä»»éƒ¨ç½²ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.16833v2">PDF</a> Published at AIES 2025</p>
<p><strong>æ‘˜è¦</strong><br>è¯­éŸ³è¯†åˆ«é¢†åŸŸçš„ç ”ç©¶ä¸­ï¼Œæœ€æ–°æ¨å‡ºçš„éŸ³é¢‘è¯­è¨€æ¨¡å‹å¯ç›´æ¥å¤„ç†è¯­éŸ³ä¿¡æ¯ï¼Œæ— éœ€ç»è¿‡å•ç‹¬çš„è½¬å½•æ­¥éª¤ã€‚è¿™ä¸€å˜é©èƒ½å¤Ÿä¿ç•™å¦‚è¯­è°ƒã€å¤šäººå¯¹è¯ç­‰ç»†èŠ‚ä¿¡æ¯ï¼Œé¿å…äº†è½¬å½•è¿‡ç¨‹ä¸­å¯èƒ½äº§ç”Ÿçš„ä¿¡æ¯æŸå¤±ã€‚ç„¶è€Œï¼Œè¿™ä¹Ÿå¸¦æ¥äº†æ–°çš„å®‰å…¨é£é™©ï¼Œå¦‚æ»¥ç”¨è¯´è¯äººèº«ä»½çº¿ç´¢å’Œå…¶ä»–æ•æ„Ÿè¯­éŸ³ç‰¹å¾ï¼Œå¹¶å¯èƒ½å¼•å‘æ³•å¾‹äº‰è®®ã€‚æœ¬æ–‡å‘¼åæ›´æ·±å…¥åœ°ç ”ç©¶è¿™äº›æ¨¡å‹çš„æ„å»ºä¸éƒ¨ç½²è¿‡ç¨‹ã€‚å®éªŒæ˜¾ç¤ºï¼Œç«¯åˆ°ç«¯çš„å»ºæ¨¡æ–¹å¼ä¸çº§è”ç®¡é“ç›¸æ¯”ï¼Œä¼šå¼•å‘ç¤¾ä¼šæŠ€æœ¯å®‰å…¨é£é™©ï¼Œå¦‚èº«ä»½æ¨æ–­ã€å†³ç­–åè§å’Œæƒ…æ„Ÿæ£€æµ‹ç­‰ã€‚è¿™å¼•å‘äº†å…³äºéŸ³é¢‘è¯­è¨€æ¨¡å‹æ˜¯å¦å­˜å‚¨è¯­éŸ³ç‰¹å¾ä»¥åŠå¦‚ä½•ä»¥ç°æœ‰æ³•å¾‹ä½“ç³»ä¸‹ä¸ç¡®å®šçš„æ–¹å¼è¿è¡Œçš„æ‹…å¿§ã€‚æœ¬æ–‡ä¸»å¼ é‡‡ç”¨æœ€å°ç‰¹æƒåŸåˆ™æ¥æŒ‡å¯¼è¿™äº›æ¨¡å‹çš„å‘å±•å’Œåº”ç”¨ï¼Œå¹¶å¯¹å…¶è¿›è¡Œè¯„ä¼°ï¼šä¸€æ˜¯å¯¹ç«¯åˆ°ç«¯å»ºæ¨¡çš„éšç§å’Œå®‰å…¨é£é™©è¿›è¡Œè¯„ä¼°ï¼›äºŒæ˜¯è¯„ä¼°ä¿¡æ¯è®¿é—®çš„é€‚å½“èŒƒå›´ã€‚æœ€åï¼Œæœ¬æ–‡å¼ºè°ƒäº†å½“å‰éŸ³é¢‘è¯­è¨€æ¨¡å‹åŸºå‡†æµ‹è¯•ä¸­çš„ç©ºç™½ï¼Œå¹¶æŒ‡å‡ºäº†å¿…é¡»è§£å†³çš„å…³é”®å¼€æ”¾ç ”ç©¶é—®é¢˜ï¼ŒåŒ…æ‹¬æŠ€æœ¯å’Œæ”¿ç­–æ–¹é¢çš„é—®é¢˜ï¼Œä»¥å®ç°ç«¯åˆ°ç«¯çš„éŸ³é¢‘è¯­è¨€æ¨¡å‹çš„è´Ÿè´£ä»»éƒ¨ç½²ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æœ€æ–°éŸ³é¢‘è¯­è¨€æ¨¡å‹å¯ç›´æ¥å¤„ç†è¯­éŸ³ä¿¡æ¯ï¼Œé¿å…äº†è½¬å½•è¿‡ç¨‹ä¸­çš„ä¿¡æ¯æŸå¤±ã€‚</li>
<li>ç›´æ¥å¤„ç†è¯­éŸ³ä¿¡æ¯ä¹Ÿå¸¦æ¥äº†æ–°çš„å®‰å…¨é£é™©ï¼Œå¦‚æ»¥ç”¨è¯´è¯äººèº«ä»½çº¿ç´¢å’Œå…¶ä»–æ•æ„Ÿè¯­éŸ³ç‰¹å¾ã€‚</li>
<li>ç«¯åˆ°ç«¯çš„å»ºæ¨¡æ–¹å¼å¯èƒ½å¼•å‘ç¤¾ä¼šæŠ€æœ¯å®‰å…¨é£é™©ï¼Œå¦‚èº«ä»½æ¨æ–­ã€å†³ç­–åè§å’Œæƒ…æ„Ÿæ£€æµ‹ç­‰ã€‚</li>
<li>å¼•å‘äº†å…³äºéŸ³é¢‘è¯­è¨€æ¨¡å‹æ˜¯å¦å­˜å‚¨è¯­éŸ³ç‰¹å¾ä»¥åŠå¦‚ä½•ä»¥ç°æœ‰æ³•å¾‹ä½“ç³»ä¸‹ä¸ç¡®å®šçš„æ–¹å¼è¿è¡Œçš„æ‹…å¿§ã€‚</li>
<li>åº”é‡‡ç”¨æœ€å°ç‰¹æƒåŸåˆ™æ¥æŒ‡å¯¼éŸ³é¢‘è¯­è¨€æ¨¡å‹çš„å‘å±•å’Œåº”ç”¨ï¼Œå¹¶è¿›è¡Œéšç§å’Œå®‰å…¨é£é™©è¯„ä¼°ã€‚</li>
<li>å½“å‰éŸ³é¢‘è¯­è¨€æ¨¡å‹åŸºå‡†æµ‹è¯•å­˜åœ¨ç©ºç™½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.16833">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Speech/2503.16833v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Speech/2503.16833v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Speech/2503.16833v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Speech/2503.16833v2/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Speech/2503.16833v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="VINP-Variational-Bayesian-Inference-with-Neural-Speech-Prior-for-Joint-ASR-Effective-Speech-Dereverberation-and-Blind-RIR-Identification"><a href="#VINP-Variational-Bayesian-Inference-with-Neural-Speech-Prior-for-Joint-ASR-Effective-Speech-Dereverberation-and-Blind-RIR-Identification" class="headerlink" title="VINP: Variational Bayesian Inference with Neural Speech Prior for Joint   ASR-Effective Speech Dereverberation and Blind RIR Identification"></a>VINP: Variational Bayesian Inference with Neural Speech Prior for Joint   ASR-Effective Speech Dereverberation and Blind RIR Identification</h2><p><strong>Authors:Pengyu Wang, Ying Fang, Xiaofei Li</strong></p>
<p>Reverberant speech, denoting the speech signal degraded by reverberation, contains crucial knowledge of both anechoic source speech and room impulse response (RIR). This work proposes a variational Bayesian inference (VBI) framework with neural speech prior (VINP) for joint speech dereverberation and blind RIR identification. In VINP, a probabilistic signal model is constructed in the time-frequency (T-F) domain based on convolution transfer function (CTF) approximation. For the first time, we propose using an arbitrary discriminative dereverberation deep neural network (DNN) to estimate the prior distribution of anechoic speech within a probabilistic model. By integrating both reverberant speech and the anechoic speech prior, VINP yields the maximum a posteriori (MAP) and maximum likelihood (ML) estimations of the anechoic speech spectrum and CTF filter, respectively. After simple transformations, the waveforms of anechoic speech and RIR are estimated. VINP is effective for automatic speech recognition (ASR) systems, which sets it apart from most deep learning (DL)-based single-channel dereverberation approaches. Experiments on single-channel speech dereverberation demonstrate that VINP attains state-of-the-art (SOTA) performance in mean opinion score (MOS) and word error rate (WER). For blind RIR identification, experiments demonstrate that VINP achieves SOTA performance in estimating reverberation time at 60 dB (RT60) and advanced performance in direct-to-reverberation ratio (DRR) estimation. Codes and audio samples are available online. </p>
<blockquote>
<p>å¸¦æœ‰æ··å“çš„è¯­éŸ³ä¿¡å·ä½“ç°äº†åŸå§‹æ— æ··å“è¯­éŸ³ä¿¡å·å’Œæˆ¿é—´è„‰å†²å“åº”ï¼ˆRIRï¼‰çš„å…³é”®çŸ¥è¯†ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ä¸ªç»“åˆäº†ç¥ç»è¯­éŸ³å…ˆéªŒçš„å˜è´å¶æ–¯æ¨ç†ï¼ˆVBIï¼‰æ¡†æ¶ï¼Œç”¨äºè”åˆè¯­éŸ³å»æ··å“å’Œç›²RIRè¯†åˆ«ã€‚åœ¨VINPä¸­ï¼ŒåŸºäºå·ç§¯ä¼ é€’å‡½æ•°ï¼ˆCTFï¼‰è¿‘ä¼¼å€¼åœ¨æ—¶é¢‘ï¼ˆT-Fï¼‰åŸŸæ„å»ºäº†æ¦‚ç‡ä¿¡å·æ¨¡å‹ã€‚æˆ‘ä»¬é¦–æ¬¡æå‡ºä½¿ç”¨ä»»æ„çš„åˆ¤åˆ«å»æ··å“æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰æ¥ä¼°è®¡æ¦‚ç‡æ¨¡å‹ä¸­çš„æ— æ··å“è¯­éŸ³çš„å…ˆéªŒåˆ†å¸ƒã€‚é€šè¿‡æ•´åˆå¸¦æ··å“çš„è¯­éŸ³å’Œæ— æ··å“è¯­éŸ³å…ˆéªŒï¼ŒVINPå¯ä»¥å¾—åˆ°æ— æ··å“è¯­éŸ³è°±å’ŒCTFæ»¤æ³¢å™¨çš„æœ€å¤§åéªŒï¼ˆMAPï¼‰å’Œæœ€å¤§ä¼¼ç„¶ï¼ˆMLï¼‰ä¼°è®¡ã€‚ç»è¿‡ç®€å•çš„è½¬æ¢ï¼Œå¯ä»¥ä¼°ç®—å‡ºæ— æ··å“è¯­éŸ³å’ŒRIRçš„æ³¢å½¢ã€‚VINPå¯¹äºè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿéå¸¸æœ‰æ•ˆï¼Œè¿™ä¸å¤§å¤šæ•°åŸºäºæ·±åº¦å­¦ä¹ çš„å•é€šé“å»æ··å“æ–¹æ³•æœ‰æ‰€ä¸åŒã€‚åœ¨å•é€šé“è¯­éŸ³å»æ··å“çš„å®éªŒä¸­ï¼ŒVINPåœ¨å¹³å‡æ„è§å¾—åˆ†ï¼ˆMOSï¼‰å’Œè¯é”™è¯¯ç‡ï¼ˆWERï¼‰æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼ˆSOTAï¼‰ã€‚å¯¹äºç›²RIRè¯†åˆ«ï¼Œå®éªŒè¡¨æ˜VINPåœ¨ä¼°è®¡60åˆ†è´çš„æ··å“æ—¶é—´ï¼ˆRT60ï¼‰æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼Œå¹¶ä¸”åœ¨ç›´æ¥-æ··å“æ¯”ç‡ï¼ˆDRRï¼‰ä¼°è®¡æ–¹é¢å–å¾—äº†å…ˆè¿›çš„æ€§èƒ½ã€‚ç›¸å…³ä»£ç å’ŒéŸ³é¢‘æ ·æœ¬å¯ä»¥åœ¨ç½‘ä¸Šæ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.07205v3">PDF</a> Submitted to IEEE&#x2F;ACM Trans. on TASLP</p>
<p><strong>Summary</strong><br>     è¯¥ç ”ç©¶åˆ©ç”¨å˜åˆ†è´å¶æ–¯æ¨æ–­ï¼ˆVBIï¼‰æ¡†æ¶ä¸ç¥ç»ç½‘ç»œè¯­éŸ³å…ˆéªŒï¼ˆVINPï¼‰è”åˆè¿›è¡Œè¯­éŸ³å»æ··å“å’Œç›²æˆ¿é—´å†²å‡»å“åº”ï¼ˆRIRï¼‰è¯†åˆ«ã€‚åŸºäºå·ç§¯ä¼ é€’å‡½æ•°ï¼ˆCTFï¼‰è¿‘ä¼¼å€¼åœ¨æ—¶é—´é¢‘ç‡ï¼ˆT-Fï¼‰åŸŸæ„å»ºæ¦‚ç‡ä¿¡å·æ¨¡å‹ï¼Œé¦–æ¬¡æå‡ºä½¿ç”¨ä»»æ„åˆ¤åˆ«å»æ··å“æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰æ¥ä¼°è®¡æ— æ··å“è¯­éŸ³çš„å…ˆéªŒåˆ†å¸ƒã€‚æ•´åˆæ··å“è¯­éŸ³å’Œæ— æ··å“è¯­éŸ³å…ˆéªŒä¿¡æ¯ï¼ŒVINPå¯è·å¾—æ— æ··å“è¯­éŸ³è°±å’ŒCTFæ»¤æ³¢å™¨çš„æœ€å¤§åéªŒï¼ˆMAPï¼‰å’Œæœ€å¤§ä¼¼ç„¶ï¼ˆMLï¼‰ä¼°è®¡å€¼ã€‚ç»è¿‡ç®€å•å˜æ¢ï¼Œå¯ä¼°è®¡æ— æ··å“è¯­éŸ³å’ŒRIRçš„æ³¢å½¢ã€‚è¯¥ç ”ç©¶æ–¹æ³•å¯¹è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿæœ‰æ•ˆï¼Œåœ¨å•é€šé“è¯­éŸ³å»æ··å“æ–¹é¢å–å¾—äº†æœ€ä½³æ€§èƒ½ã€‚åŒæ—¶ç›²RIRè¯†åˆ«å®éªŒç»“æœäº¦æ˜¾ç¤ºå…¶æ€§èƒ½ä¼˜å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å˜åˆ†è´å¶æ–¯æ¨æ–­ï¼ˆVBIï¼‰æ¡†æ¶ç»“åˆç¥ç»ç½‘ç»œè¯­éŸ³å…ˆéªŒï¼ˆVINPï¼‰ç”¨äºè”åˆè¯­éŸ³å»æ··å“å’Œç›²æˆ¿é—´å†²å‡»å“åº”ï¼ˆRIRï¼‰è¯†åˆ«ã€‚</li>
<li>åŸºäºå·ç§¯ä¼ é€’å‡½æ•°ï¼ˆCTFï¼‰åœ¨T-FåŸŸæ„å»ºæ¦‚ç‡ä¿¡å·æ¨¡å‹ã€‚</li>
<li>ä½¿ç”¨ä»»æ„åˆ¤åˆ«å»æ··å“æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰ä¼°è®¡æ— æ··å“è¯­éŸ³çš„å…ˆéªŒåˆ†å¸ƒã€‚</li>
<li>VINPå¯ä»¥è·å¾—æ— æ··å“è¯­éŸ³è°±å’ŒCTFæ»¤æ³¢å™¨çš„MAPå’ŒMLä¼°è®¡ã€‚</li>
<li>VINPå¯¹è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿæœ‰æ•ˆã€‚</li>
<li>åœ¨å•é€šé“è¯­éŸ³å»æ··å“æ–¹é¢ï¼ŒVINPè¾¾åˆ°äº†æœ€ä½³æ€§èƒ½ï¼Œä½“ç°åœ¨å¹³å‡æ„è§å¾—åˆ†ï¼ˆMOSï¼‰å’Œè¯é”™è¯¯ç‡ï¼ˆWERï¼‰ä¸Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.07205">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Speech/2502.07205v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Speech/2502.07205v3/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Speech/2502.07205v3/page_3_0.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Neural-Enhanced-Dynamic-Range-Compression-Inversion-A-Hybrid-Approach-for-Restoring-Audio-Dynamics"><a href="#Neural-Enhanced-Dynamic-Range-Compression-Inversion-A-Hybrid-Approach-for-Restoring-Audio-Dynamics" class="headerlink" title="Neural-Enhanced Dynamic Range Compression Inversion: A Hybrid Approach   for Restoring Audio Dynamics"></a>Neural-Enhanced Dynamic Range Compression Inversion: A Hybrid Approach   for Restoring Audio Dynamics</h2><p><strong>Authors:Haoran Sun, Dominique Fourer, Hichem Maaref</strong></p>
<p>Dynamic Range Compression (DRC) is a widely used audio effect that adjusts signal dynamics for applications in music production, broadcasting, and speech processing. Inverting DRC is of broad importance for restoring the original dynamics, enabling remixing, and enhancing the overall audio quality. Existing DRC inversion methods either overlook key parameters or rely on precise parameter values, which can be challenging to estimate accurately. To address this limitation, we introduce a hybrid approach that combines model-based DRC inversion with neural networks to achieve robust DRC parameter estimation and audio restoration simultaneously. Our method uses tailored neural network architectures (classification and regression), which are then integrated into a model-based inversion framework to reconstruct the original signal. Experimental evaluations on various music and speech datasets confirm the effectiveness and robustness of our approach, outperforming several state-of-the-art techniques. </p>
<blockquote>
<p>åŠ¨æ€èŒƒå›´å‹ç¼©ï¼ˆDRCï¼‰æ˜¯ä¸€ç§å¹¿æ³›åº”ç”¨äºéŸ³ä¹åˆ¶ä½œã€å¹¿æ’­å’Œè¯­éŸ³å¤„ç†ç­‰åº”ç”¨çš„éŸ³é¢‘æ•ˆæœï¼Œå®ƒç”¨äºè°ƒæ•´ä¿¡å·åŠ¨æ€ã€‚å¯¹DRCè¿›è¡Œåè½¬å¯¹äºæ¢å¤åŸå§‹åŠ¨æ€ã€å®ç°æ··éŸ³å’Œå¢å¼ºæ•´ä½“éŸ³é¢‘è´¨é‡å…·æœ‰é‡å¤§æ„ä¹‰ã€‚ç°æœ‰çš„DRCåè½¬æ–¹æ³•è¦ä¹ˆå¿½ç•¥äº†å…³é”®å‚æ•°ï¼Œè¦ä¹ˆä¾èµ–äºç²¾ç¡®çš„å‚æ•°å€¼ï¼Œè€Œå‡†ç¡®ä¼°è®¡è¿™äº›å‚æ•°å€¼å¯èƒ½å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ··åˆæ–¹æ³•ï¼Œå®ƒå°†åŸºäºæ¨¡å‹çš„DRCåè½¬ä¸ç¥ç»ç½‘ç»œç›¸ç»“åˆï¼ŒåŒæ—¶å®ç°ç¨³å¥çš„DRCå‚æ•°ä¼°è®¡å’ŒéŸ³é¢‘æ¢å¤ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä½¿ç”¨å®šåˆ¶åŒ–çš„ç¥ç»ç½‘ç»œæ¶æ„ï¼ˆåˆ†ç±»å’Œå›å½’ï¼‰ï¼Œç„¶åå°†å…¶é›†æˆåˆ°åŸºäºæ¨¡å‹çš„åè½¬æ¡†æ¶ä¸­æ¥é‡å»ºåŸå§‹ä¿¡å·ã€‚å¯¹å„ç§éŸ³ä¹å’Œè¯­éŸ³æ•°æ®é›†çš„å®éªŒè¯„ä¼°è¯å®äº†æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œç¨³å¥æ€§ï¼Œä¼˜äºå‡ ç§æœ€æ–°æŠ€æœ¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.04337v2">PDF</a> This work has been submitted to the IEEE for possible publication</p>
<p><strong>æ€»ç»“</strong></p>
<p>åŠ¨æ€èŒƒå›´å‹ç¼©ï¼ˆDRCï¼‰æ˜¯ä¸€ç§å¹¿æ³›åº”ç”¨äºéŸ³é¢‘å¤„ç†çš„æ•ˆåº”ï¼Œç”¨äºè°ƒæ•´ä¿¡å·åŠ¨æ€ï¼Œåº”ç”¨äºéŸ³ä¹åˆ¶ä½œã€å¹¿æ’­å’Œè¯­éŸ³å¤„ç†ç­‰ã€‚å¯¹DRCè¿›è¡Œåè½¬å¯¹äºæ¢å¤åŸå§‹åŠ¨æ€ã€æ··éŸ³å’Œå¢å¼ºæ•´ä½“éŸ³é¢‘è´¨é‡å…·æœ‰é‡è¦æ„ä¹‰ã€‚ç°æœ‰çš„DRCåè½¬æ–¹æ³•è¦ä¹ˆå¿½ç•¥å…³é”®å‚æ•°ï¼Œè¦ä¹ˆä¾èµ–äºéš¾ä»¥å‡†ç¡®ä¼°è®¡çš„ç²¾ç¡®å‚æ•°å€¼ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ··åˆæ–¹æ³•ï¼Œå°†åŸºäºæ¨¡å‹çš„DRCåè½¬ä¸ç¥ç»ç½‘ç»œç›¸ç»“åˆï¼Œå®ç°ç¨³å¥çš„DRCå‚æ•°ä¼°è®¡å’ŒéŸ³é¢‘æ¢å¤ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä½¿ç”¨å®šåˆ¶åŒ–çš„ç¥ç»ç½‘ç»œæ¶æ„ï¼ˆåˆ†ç±»å’Œå›å½’ï¼‰ï¼Œç„¶åå°†å…¶é›†æˆåˆ°åŸºäºæ¨¡å‹çš„åè½¬æ¡†æ¶ä¸­ä»¥é‡å»ºåŸå§‹ä¿¡å·ã€‚åœ¨å„ç§éŸ³ä¹å’Œè¯­éŸ³æ•°æ®é›†ä¸Šçš„å®éªŒè¯„ä¼°è¯å®äº†æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œç¨³å¥æ€§ï¼Œä¼˜äºå‡ ç§å…ˆè¿›æŠ€æœ¯ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>åŠ¨æ€èŒƒå›´å‹ç¼©ï¼ˆDRCï¼‰æ˜¯ä¸€ç§å¹¿æ³›åº”ç”¨äºéŸ³é¢‘å¤„ç†çš„æ•ˆåº”ï¼Œç”¨äºè°ƒæ•´ä¿¡å·åŠ¨æ€ã€‚</li>
<li>DRCåè½¬å¯¹äºæ¢å¤åŸå§‹åŠ¨æ€ã€æ··éŸ³å’Œå¢å¼ºæ•´ä½“éŸ³é¢‘è´¨é‡å…·æœ‰é‡è¦æ„ä¹‰ã€‚</li>
<li>ç°æœ‰DRCåè½¬æ–¹æ³•å­˜åœ¨æŒ‘æˆ˜ï¼Œå¦‚å¿½ç•¥å…³é”®å‚æ•°æˆ–ä¾èµ–éš¾ä»¥å‡†ç¡®ä¼°è®¡çš„å‚æ•°å€¼ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ··åˆæ–¹æ³•ï¼Œç»“åˆæ¨¡å‹é©±åŠ¨çš„DRCåè½¬å’Œç¥ç»ç½‘ç»œï¼Œå®ç°ç¨³å¥çš„DRCå‚æ•°ä¼°è®¡å’ŒéŸ³é¢‘æ¢å¤ã€‚</li>
<li>ä½¿ç”¨å®šåˆ¶åŒ–çš„ç¥ç»ç½‘ç»œæ¶æ„ï¼ˆåˆ†ç±»å’Œå›å½’ï¼‰ï¼Œé›†æˆåˆ°åŸºäºæ¨¡å‹çš„åè½¬æ¡†æ¶ä¸­é‡å»ºåŸå§‹ä¿¡å·ã€‚</li>
<li>å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§æ•°æ®é›†ä¸Šè¡¨ç°æœ‰æ•ˆä¸”ç¨³å¥ï¼Œä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
<li>è¯¥æ–¹æ³•å¯¹äºéŸ³é¢‘å¤„ç†å’Œè¯­éŸ³å¤„ç†ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.04337">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Speech/2411.04337v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Speech/2411.04337v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Speech/2411.04337v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Speech/2411.04337v2/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Speech/2411.04337v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-12/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-12/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-12/GAN/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_GAN/2509.08392v1/page_2_0.jpg" class="responsive-img" alt="GAN">
                        
                        <span class="card-title">GAN</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            GAN æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-12  VRAE Vertical Residual Autoencoder for License Plate Denoising and   Deblurring
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-12
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/GAN/" class="post-category">
                                    GAN
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/GAN/">
                        <span class="chip bg-color">GAN</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-12/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F_Breast%20Ultrasound/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_åŒ»å­¦å½±åƒ_Breast Ultrasound/2506.23903v3/page_0_0.jpg" class="responsive-img" alt="åŒ»å­¦å½±åƒ/Breast Ultrasound">
                        
                        <span class="card-title">åŒ»å­¦å½±åƒ/Breast Ultrasound</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å½±åƒ/Breast Ultrasound æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-12  XBusNet Text-Guided Breast Ultrasound Segmentation via Multimodal   Vision-Language Learning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-12
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/" class="post-category">
                                    åŒ»å­¦å½±åƒ/Breast Ultrasound
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/">
                        <span class="chip bg-color">åŒ»å­¦å½±åƒ/Breast Ultrasound</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32562k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
