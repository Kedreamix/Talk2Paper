<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech 方向最新论文已更新，请持续关注 Update in 2025-09-12  Streaming Sequence-to-Sequence Learning with Delayed Streams Modeling">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Speech/2509.00405v2/page_0_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-09-12
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-09-13
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    9.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    36 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-09-12-更新"><a href="#2025-09-12-更新" class="headerlink" title="2025-09-12 更新"></a>2025-09-12 更新</h1><h2 id="Streaming-Sequence-to-Sequence-Learning-with-Delayed-Streams-Modeling"><a href="#Streaming-Sequence-to-Sequence-Learning-with-Delayed-Streams-Modeling" class="headerlink" title="Streaming Sequence-to-Sequence Learning with Delayed Streams Modeling"></a>Streaming Sequence-to-Sequence Learning with Delayed Streams Modeling</h2><p><strong>Authors:Neil Zeghidour, Eugene Kharitonov, Manu Orsini, Václav Volhejn, Gabriel de Marmiesse, Edouard Grave, Patrick Pérez, Laurent Mazaré, Alexandre Défossez</strong></p>
<p>We introduce Delayed Streams Modeling (DSM), a flexible formulation for streaming, multimodal sequence-to-sequence learning. Sequence-to-sequence generation is often cast in an offline manner, where the model consumes the complete input sequence before generating the first output timestep. Alternatively, streaming sequence-to-sequence rely on learning a policy for choosing when to advance on the input stream, or write to the output stream. DSM instead models already time-aligned streams with a decoder-only language model. By moving the alignment to a pre-processing step,and introducing appropriate delays between streams, DSM provides streaming inference of arbitrary output sequences, from any input combination, making it applicable to many sequence-to-sequence problems. In particular, given text and audio streams, automatic speech recognition (ASR) corresponds to the text stream being delayed, while the opposite gives a text-to-speech (TTS) model. We perform extensive experiments for these two major sequence-to-sequence tasks, showing that DSM provides state-of-the-art performance and latency while supporting arbitrary long sequences, being even competitive with offline baselines. Code, samples and demos are available at <a target="_blank" rel="noopener" href="https://github.com/kyutai-labs/delayed-streams-modeling">https://github.com/kyutai-labs/delayed-streams-modeling</a> </p>
<blockquote>
<p>我们介绍了延迟流建模（DSM），这是一种用于流式、多模态序列到序列学习的灵活公式。序列到序列生成通常以一种离线的方式进行，模型在生成第一个输出时间步之前会消耗完整的输入序列。相比之下，流式序列到序列则依赖于学习一个策略，以确定何时推进输入流，或写入输出流。然而，DSM使用仅解码的语言模型对已经时间对齐的流进行建模。通过将对齐移动到预处理步骤，并在流之间引入适当的延迟，DSM可以对任意输出序列进行流式推理，适用于许多序列到序列问题，适用于任何输入组合。特别是给定文本和音频流时，语音识别（ASR）对应于文本流延迟，而相反则给出了文本到语音（TTS）模型。我们为这两个主要的序列到序列任务进行了大量实验，结果表明，DSM在提供最先进的性能和延迟的同时，还支持任意长序列，甚至与离线基准测试相竞争。代码、样本和演示可在<a target="_blank" rel="noopener" href="https://github.com/kyutai-labs/delayed-streams-modeling%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/kyutai-labs/delayed-streams-modeling找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.08753v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了延迟流建模（DSM），这是一种灵活的多模态序列到序列学习的流式处理方法。传统的序列到序列生成多为离线模式，而DSM将输入序列与输出流的时序对齐转移至预处理步骤，通过引入适当的延迟来实现任意输出序列的流式推断，适用于多种序列到序列问题。在自动语音识别（ASR）和文本到语音（TTS）等关键任务上表现优秀。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>延迟流建模（DSM）是一种灵活的多模态序列到序列学习的流式处理方法。</li>
<li>传统序列到序列生成多为离线模式，而DSM能实现任意输出序列的流式推断。</li>
<li>DSM通过预处理的时序对齐和引入适当的延迟来实现流式推断。</li>
<li>DSM适用于多种序列到序列问题。</li>
<li>在自动语音识别（ASR）任务上，DSM表现优秀，可延迟文本流以实现ASR。</li>
<li>在文本到语音（TTS）任务上，DSM通过延迟音频流实现TTS模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.08753">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Speech/2509.08753v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Speech/2509.08753v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Speech/2509.08753v1/page_4_0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Joint-Learning-using-Mixture-of-Expert-Based-Representation-for-Enhanced-Speech-Generation-and-Robust-Emotion-Recognition"><a href="#Joint-Learning-using-Mixture-of-Expert-Based-Representation-for-Enhanced-Speech-Generation-and-Robust-Emotion-Recognition" class="headerlink" title="Joint Learning using Mixture-of-Expert-Based Representation for Enhanced   Speech Generation and Robust Emotion Recognition"></a>Joint Learning using Mixture-of-Expert-Based Representation for Enhanced   Speech Generation and Robust Emotion Recognition</h2><p><strong>Authors:Jing-Tong Tzeng, Carlos Busso, Chi-Chun Lee</strong></p>
<p>Speech emotion recognition (SER) plays a critical role in building emotion-aware speech systems, but its performance degrades significantly under noisy conditions. Although speech enhancement (SE) can improve robustness, it often introduces artifacts that obscure emotional cues and adds computational overhead to the pipeline. Multi-task learning (MTL) offers an alternative by jointly optimizing SE and SER tasks. However, conventional shared-backbone models frequently suffer from gradient interference and representational conflicts between tasks. To address these challenges, we propose the Sparse Mixture-of-Experts Representation Integration Technique (Sparse MERIT), a flexible MTL framework that applies frame-wise expert routing over self-supervised speech representations. Sparse MERIT incorporates task-specific gating networks that dynamically select from a shared pool of experts for each frame, enabling parameter-efficient and task-adaptive representation learning. Experiments on the MSP-Podcast corpus show that Sparse MERIT consistently outperforms baseline models on both SER and SE tasks. Under the most challenging condition of -5 dB signal-to-noise ratio (SNR), Sparse MERIT improves SER F1-macro by an average of 12.0% over a baseline relying on a SE pre-processing strategy, and by 3.4% over a naive MTL baseline, with statistical significance on unseen noise conditions. For SE, Sparse MERIT improves segmental SNR (SSNR) by 28.2% over the SE pre-processing baseline and by 20.0% over the naive MTL baseline. These results demonstrate that Sparse MERIT provides robust and generalizable performance for both emotion recognition and enhancement tasks in noisy environments. </p>
<blockquote>
<p>语音情感识别（SER）在构建情感感知语音系统中起着关键作用，但在噪声环境下其性能会显著下降。虽然语音增强（SE）可以提高稳健性，但它往往会引入掩盖情感线索的伪影，并为管道增加计算开销。多任务学习（MTL）通过联合优化SE和SER任务提供了一种替代方案。然而，传统的共享骨干模型经常受到梯度干扰和任务表示冲突的影响。为了解决这些挑战，我们提出了灵活的MTL框架——稀疏混合专家表示集成技术（Sparse MERIT），它应用帧级专家路由对自监督语音表示进行建模。Sparse MERIT结合了特定任务的门控网络，可以动态地从共享的专家池中为每一帧选择专家，从而实现参数高效和任务自适应的表示学习。在MSP-Podcast语料库上的实验表明，Sparse MERIT在SER和SE任务上均优于基线模型。在信噪比（SNR）为-5 dB的最具挑战性的条件下，与依赖SE预处理策略的基线相比，Sparse MERIT的SER F1-macro平均提高了12.0%，与简单的MTL基线相比提高了3.4%，在未见过的噪声条件下具有统计学上的显著意义。对于SE，Sparse MERIT比SE预处理基线提高了28.2%的段SNR（SSNR），比简单的MTL基线提高了20.0%。这些结果表明，Sparse MERIT在噪声环境下的情感识别和增强任务中都提供了稳健和通用的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.08470v1">PDF</a> </p>
<p><strong>Summary</strong><br>     语音情感识别在构建情感感知语音系统中扮演着关键角色，但在噪声环境下性能显著下降。虽然语音增强可以提高稳健性，但它常常引入掩盖情感线索的伪影并增加计算开销。多任务学习提供了一个替代方案，通过联合优化语音增强和语音情感识别任务。然而，常规共享骨干模型常受梯度干扰和任务表示冲突的影响。为解决这些挑战，提出了灵活的多任务学习框架——Sparse Mixture-of-Experts Representation Integration Technique (Sparse MERIT)，它应用帧级专家路由于自我监督的语音表示上。Sparse MERIT结合了任务特定门控网络，动态为每帧选择共享专家池中的专家，实现参数高效和任务自适应的表示学习。实验表明，Sparse MERIT在语音情感识别和语音增强任务上均优于基线模型，在信噪比-5 dB的最具挑战条件下，相较于依赖语音增强预处理的基线模型，Sparse MERIT在情感识别的F1-macro得分上平均提高了12.0%，相较于简单的多任务学习基线提高了3.4%，在未见过的噪声条件下具有统计显著性。对于语音增强，Sparse MERIT在分段信噪比（SSNR）上比语音增强预处理基线提高了28.2%，比简单的多任务学习基线提高了20.0%。结果表明，Sparse MERIT在噪声环境下为语音情感识别和增强任务提供了稳健和可泛化的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>语音情感识别（SER）在噪声环境下性能下降。</li>
<li>语音增强（SE）能提高稳健性，但可能引入掩盖情感线索的伪影。</li>
<li>多任务学习（MTL）通过联合优化SE和SER任务作为替代方案。</li>
<li>常规共享骨干模型面临梯度干扰和表示冲突的挑战。</li>
<li>Sparse MERIT通过帧级专家路由和自我监督的语音表示提供灵活多任务学习。</li>
<li>Sparse MERIT在噪声环境下显著优于基线模型，尤其在-5 dB的SNR条件下。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.08470">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Speech/2509.08470v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Speech/2509.08470v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Few-shot-Personalization-via-In-Context-Learning-for-Speech-Emotion-Recognition-based-on-Speech-Language-Model"><a href="#Few-shot-Personalization-via-In-Context-Learning-for-Speech-Emotion-Recognition-based-on-Speech-Language-Model" class="headerlink" title="Few-shot Personalization via In-Context Learning for Speech Emotion   Recognition based on Speech-Language Model"></a>Few-shot Personalization via In-Context Learning for Speech Emotion   Recognition based on Speech-Language Model</h2><p><strong>Authors:Mana Ihori, Taiga Yamane, Naotaka Kawata, Naoki Makishima, Tomohiro Tanaka, Satoshi Suzuki, Shota Orihashi, Ryo Masumura</strong></p>
<p>This paper proposes a personalization method for speech emotion recognition (SER) through in-context learning (ICL). Since the expression of emotions varies from person to person, speaker-specific adaptation is crucial for improving the SER performance. Conventional SER methods have been personalized using emotional utterances of a target speaker, but it is often difficult to prepare utterances corresponding to all emotion labels in advance. Our idea to overcome this difficulty is to obtain speaker characteristics by conditioning a few emotional utterances of the target speaker in ICL-based inference. ICL is a method to perform unseen tasks by conditioning a few input-output examples through inference in large language models (LLMs). We meta-train a speech-language model extended from the LLM to learn how to perform personalized SER via ICL. Experimental results using our newly collected SER dataset demonstrate that the proposed method outperforms conventional methods. </p>
<blockquote>
<p>本文提出了一种基于上下文学习（ICL）的语音情感识别（SER）个性化方法。由于情感的表达因人而异，因此针对特定说话人的适应对于提高SER性能至关重要。传统的SER方法已经通过目标说话人的情感话语进行了个性化处理，但往往难以提前准备与所有情感标签相对应的话语。我们克服这一困难的想法是通过在基于ICL的推理中条件化目标说话人的少数情感话语来获得说话人特征。ICL是一种通过在大语言模型（LLM）中进行推理来执行未见任务的方法，通过少数输入-输出示例进行条件处理。我们对从LLM扩展的语音语言模型进行元训练，以学习如何通过ICL执行个性化的SER。使用我们新收集的SER数据集进行的实验结果表明，该方法优于传统方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.08344v1">PDF</a> Accepted by ASRU 2025</p>
<p><strong>总结</strong></p>
<p>本文提出了一种基于上下文学习（ICL）的个性化语音情感识别（SER）方法。由于情感表达具有个体差异，针对特定说话人的适应性对于提高SER性能至关重要。虽然传统方法已尝试通过目标说话人的情感话语实现个性化，但预先准备所有情感标签的语句往往非常困难。本研究通过ICL推理中的条件语句获得说话人的特性来克服这一难题。ICL是一种在大语言模型（LLM）中通过推理处理未见任务的方法。本研究通过元训练扩展了语音语言模型，使其能够学习如何通过ICL执行个性化SER。使用新收集的SER数据集的实验结果表明，该方法优于传统方法。</p>
<p><strong>要点分析</strong></p>
<ol>
<li>论文提出了一种针对语音情感识别（SER）的个性化方法，该方法基于上下文学习（ICL）。</li>
<li>情感表达具有个体差异，因此针对特定说话人的适应性对于提高SER性能至关重要。</li>
<li>传统方法使用目标说话人的情感话语进行个性化，但预先准备所有情感标签的语句是一个挑战。</li>
<li>ICL是一种在大语言模型（LLM）中进行推理的方法，可以通过少量的输入-输出示例来处理未见任务。</li>
<li>研究中的语音语言模型通过元训练学习如何通过ICL执行个性化SER。</li>
<li>使用新收集的SER数据集进行的实验表明，该方法在性能上超越了传统方法。</li>
<li>该方法提供了一种有效利用少量情感语句来捕捉目标说话人的特性，从而提高了SER的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.08344">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Speech/2509.08344v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Speech/2509.08344v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Speech/2509.08344v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Speech/2509.08344v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Speech/2509.08344v1/page_5_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Speech/2509.08344v1/page_5_2.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="A-Bottom-up-Framework-with-Language-universal-Speech-Attribute-Modeling-for-Syllable-based-ASR"><a href="#A-Bottom-up-Framework-with-Language-universal-Speech-Attribute-Modeling-for-Syllable-based-ASR" class="headerlink" title="A Bottom-up Framework with Language-universal Speech Attribute Modeling   for Syllable-based ASR"></a>A Bottom-up Framework with Language-universal Speech Attribute Modeling   for Syllable-based ASR</h2><p><strong>Authors:Hao Yen, Pin-Jui Ku, Sabato Marco Siniscalchi, Chin-Hui Lee</strong></p>
<p>We propose a bottom-up framework for automatic speech recognition (ASR) in syllable-based languages by unifying language-universal articulatory attribute modeling with syllable-level prediction. The system first recognizes sequences or lattices of articulatory attributes that serve as a language-universal, interpretable representation of pronunciation, and then transforms them into syllables through a structured knowledge integration process. We introduce two evaluation metrics, namely Pronunciation Error Rate (PrER) and Syllable Homonym Error Rate (SHER), to evaluate the model’s ability to capture pronunciation and handle syllable ambiguities. Experimental results on the AISHELL-1 Mandarin corpus demonstrate that the proposed bottom-up framework achieves competitive performance and exhibits better robustness under low-resource conditions compared to the direct syllable prediction model. Furthermore, we investigate the zero-shot cross-lingual transferability on Japanese and demonstrate significant improvements over character- and phoneme-based baselines by 40% error rate reduction. </p>
<blockquote>
<p>我们提出了一种基于底层的自动语音识别（ASR）框架，用于音节类语言，通过统一语言通用的发音属性建模与音节级预测来实现。该系统首先识别发音属性的序列或网格，作为语言通用的可解释发音表示，然后通过结构化知识整合过程将它们转换为音节。我们引入了两种评估指标，即发音错误率（PrER）和音节同音字错误率（SHER），以评估模型捕捉发音和处理音节歧义的能力。在AISHELL-1普通话语料库上的实验结果表明，所提底层框架的性能具有竞争力，与直接音节预测模型相比，在低资源条件下表现出更好的稳健性。此外，我们在日语上进行了零样本跨语言可迁移性的研究，相较于字符和音素基准线，实现了40%的错误率降低，取得了显著改进。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.08173v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种面向音节语言的自动语音识别（ASR）的自下而上框架，该框架结合了语言通用的发音属性建模与音节级别的预测。系统首先识别发音属性的序列或格网，作为语言通用的可解释发音表示，然后通过结构化知识整合过程将它们转化为音节。引入两种评估指标——发音错误率和音节同音词错误率，以评估模型捕捉发音和处理音节歧义的能力。在AISHELL-1普通话语料库上的实验结果表明，与直接音节预测模型相比，所提自下而上框架具有竞争力，并在低资源条件下表现出更好的稳健性。此外，在日语上的零样本跨语言迁移能力测试也证明了该框架的显著改进，相比字符和音素基线误差率降低了40%。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>提出一种针对音节语言的自动语音识别（ASR）自下而上框架。</li>
<li>结合语言通用的发音属性建模与音节级别预测。</li>
<li>通过识别发音属性序列或格网，转化为语言通用的可解释发音表示。</li>
<li>引入发音错误率和音节同音词错误率两种评估指标。</li>
<li>在AISHELL-1普通话语料库上表现出良好性能，特别是在低资源条件下。</li>
<li>显示出显著的跨语言迁移能力，特别是在日语上的表现。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.08173">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Speech/2509.08173v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Speech/2509.08173v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Speech/2509.08173v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Speech/2509.08173v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Speech/2509.08173v1/page_3_1.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Affine-Modulation-based-Audiogram-Fusion-Network-for-Joint-Noise-Reduction-and-Hearing-Loss-Compensation"><a href="#Affine-Modulation-based-Audiogram-Fusion-Network-for-Joint-Noise-Reduction-and-Hearing-Loss-Compensation" class="headerlink" title="Affine Modulation-based Audiogram Fusion Network for Joint Noise   Reduction and Hearing Loss Compensation"></a>Affine Modulation-based Audiogram Fusion Network for Joint Noise   Reduction and Hearing Loss Compensation</h2><p><strong>Authors:Ye Ni, Ruiyu Liang, Xiaoshuai Hao, Jiaming Cheng, Qingyun Wang, Chengwei Huang, Cairong Zou, Wei Zhou, Weiping Ding, Björn W. Schuller</strong></p>
<p>Hearing aids (HAs) are widely used to provide personalized speech enhancement (PSE) services, improving the quality of life for individuals with hearing loss. However, HA performance significantly declines in noisy environments as it treats noise reduction (NR) and hearing loss compensation (HLC) as separate tasks. This separation leads to a lack of systematic optimization, overlooking the interactions between these two critical tasks, and increases the system complexity. To address these challenges, we propose a novel audiogram fusion network, named AFN-HearNet, which simultaneously tackles the NR and HLC tasks by fusing cross-domain audiogram and spectrum features. We propose an audiogram-specific encoder that transforms the sparse audiogram profile into a deep representation, addressing the alignment problem of cross-domain features prior to fusion. To incorporate the interactions between NR and HLC tasks, we propose the affine modulation-based audiogram fusion frequency-temporal Conformer that adaptively fuses these two features into a unified deep representation for speech reconstruction. Furthermore, we introduce a voice activity detection auxiliary training task to embed speech and non-speech patterns into the unified deep representation implicitly. We conduct comprehensive experiments across multiple datasets to validate the effectiveness of each proposed module. The results indicate that the AFN-HearNet significantly outperforms state-of-the-art in-context fusion joint models regarding key metrics such as HASQI and PESQ, achieving a considerable trade-off between performance and efficiency. The source code and data will be released at <a target="_blank" rel="noopener" href="https://github.com/deepnetni/AFN-HearNet">https://github.com/deepnetni/AFN-HearNet</a>. </p>
<blockquote>
<p>助听器（HAs）广泛应用于个性化语音增强（PSE）服务，以提高听障人士的生活质量。然而，在嘈杂的环境中，助听器的性能会大幅下降，因为它将降噪（NR）和听力损失补偿（HLC）视为单独的任务。这种分离导致缺乏系统优化，忽视了这两个关键任务之间的相互作用，并增加了系统复杂性。为了解决这些挑战，我们提出了一种新型音频图融合网络，名为AFN-HearNet，它通过融合跨域音频图和光谱特征，同时解决NR和HLC任务。我们提出了一种针对音频图的特定编码器，将稀疏音频图轮廓转换为深度表示，解决跨域特征融合之前的对齐问题。为了结合NR和HLC任务之间的相互作用，我们提出了基于仿射调制的音频图融合时空卷积网络，该网络自适应地将这两个特征融合为统一的深度表示，用于语音重建。此外，我们引入了一个语音活动检测辅助训练任务，将语音和非语音模式隐式地嵌入到统一的深度表示中。我们在多个数据集上进行了全面的实验，以验证所提出模块的有效性。结果表明，AFN-HearNet在关键指标HASQI和PESQ方面显著优于最新的上下文融合联合模型，在性能和效率之间实现了可观的权衡。源代码和数据将在<a target="_blank" rel="noopener" href="https://github.com/deepnetni/AFN-HearNet%E4%B8%8A%E5%8F%91%E5%B8%A%E6%8A%95%E6%94%BE">https://github.com/deepnetni/AFN-HearNet上发布。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.07341v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>助听器（HA）提供个性化语音增强（PSE）服务，提高听障人士的生活质量。然而，在嘈杂环境中，助听器性能显著下降，它将噪声降低（NR）和听力损失补偿（HLC）视为单独的任务处理。这种分离导致缺乏系统优化，忽略了这两个关键任务之间的相互作用，并增加了系统复杂性。为解决这些挑战，我们提出了一种名为AFN-HearNet的新型听力图融合网络，可同时解决NR和HLC任务，通过融合跨域听力图和频谱特征。我们提出了一个听力图特定编码器，将稀疏听力图转换为深度表示，解决了跨域特征对齐问题。为了融入NR和HLC任务间的交互作用，我们提出了基于仿射调制的听力图融合频时卷积神经网络（Conformer），自适应地将这两个特征融合为统一的深度表示用于语音重建。此外，我们还引入了语音活动检测辅助训练任务，将语音和非语音模式嵌入到统一的深度表示中。我们在多个数据集上进行了全面的实验，验证了每个模块的有效性。结果表明，AFN-HearNet在关键指标HASQI和PESQ上显著优于最新的上下文融合联合模型，实现了性能和效率之间的良好权衡。源代码和数据将在<a target="_blank" rel="noopener" href="https://github.com/deepnetni/AFN-HearNet%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/deepnetni/AFN-HearNet发布。</a></p>
<p><strong>要点</strong></p>
<ol>
<li>助听器在嘈杂环境中性能下降，因为将噪声降低和听力损失补偿视为独立任务处理。</li>
<li>提出新型AFN-HearNet网络，融合跨域听力图和频谱特征以改善助听器性能。</li>
<li>AFN-HearNet引入听力图特定编码器以转换稀疏听力图到深度表示形式。</li>
<li>引入仿射调制Conformer模块来适应地融合NR和HLC特征。</li>
<li>通过语音活动检测辅助训练任务嵌入语音和非语音模式到统一深度表示中。</li>
<li>实验证明AFN-HearNet在关键指标上优于其他模型，实现了性能和效率的平衡。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.07341">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Speech/2509.07341v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Speech/2509.07341v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Speech/2509.07341v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Speech/2509.07341v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="SaD-A-Scenario-Aware-Discriminator-for-Speech-Enhancement"><a href="#SaD-A-Scenario-Aware-Discriminator-for-Speech-Enhancement" class="headerlink" title="SaD: A Scenario-Aware Discriminator for Speech Enhancement"></a>SaD: A Scenario-Aware Discriminator for Speech Enhancement</h2><p><strong>Authors:Xihao Yuan, Siqi Liu, Yan Chen, Hang Zhou, Chang Liu, Hanting Chen, Jie Hu</strong></p>
<p>Generative adversarial network-based models have shown remarkable performance in the field of speech enhancement. However, the current optimization strategies for these models predominantly focus on refining the architecture of the generator or enhancing the quality evaluation metrics of the discriminator. This approach often overlooks the rich contextual information inherent in diverse scenarios. In this paper, we propose a scenario-aware discriminator that captures scene-specific features and performs frequency-domain division, thereby enabling a more accurate quality assessment of the enhanced speech generated by the generator. We conducted comprehensive experiments on three representative models using two publicly available datasets. The results demonstrate that our method can effectively adapt to various generator architectures without altering their structure, thereby unlocking further performance gains in speech enhancement across different scenarios. </p>
<blockquote>
<p>基于生成对抗网络（GAN）的模型在语音增强领域表现出卓越的性能。然而，当前针对这些模型的优化策略主要集中在改进生成器的架构或提高判别器的质量评估指标上。这种方法往往会忽略不同场景中丰富的上下文信息。在本文中，我们提出了一种场景感知判别器，该判别器能够捕获场景特定特征并执行频域分割，从而实现对生成器生成的增强语音的更准确质量评估。我们在三个代表性模型上使用了两个公开数据集进行了全面的实验。结果表明，我们的方法可以有效地适应各种生成器架构，而无需改变其结构，从而在不同场景的语音增强中实现了进一步的性能提升。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.00405v2">PDF</a> 5 pages, 2 figures. Accepted by InterSpeech2025</p>
<p><strong>Summary</strong></p>
<p>本文提出一种基于场景感知的判别器，用于捕捉场景特定特征并执行频域分割，从而更准确地对生成器生成的增强语音进行质量评估。实验结果表明，该方法可有效适应各种生成器架构，无需改变其结构，可在不同场景下实现语音增强的性能提升。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>生成对抗网络模型在语音增强领域表现出卓越性能。</li>
<li>当前优化策略主要关注生成器的架构或判别器质量评估指标的改进。</li>
<li>提出的场景感知判别器能捕捉场景特定特征，并进行频域分割。</li>
<li>判别器能提高对生成器产生的增强语音的质量评估准确性。</li>
<li>方法适应多种生成器架构，无需改变其结构。</li>
<li>在不同场景下，该方法能实现语音增强的性能提升。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.00405">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Speech/2509.00405v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Speech/2509.00405v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Speech/2509.00405v2/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Speech/2509.00405v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Speech/2509.00405v2/page_3_1.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="The-Model-Hears-You-Audio-Language-Model-Deployments-Should-Consider-the-Principle-of-Least-Privilege"><a href="#The-Model-Hears-You-Audio-Language-Model-Deployments-Should-Consider-the-Principle-of-Least-Privilege" class="headerlink" title="The Model Hears You: Audio Language Model Deployments Should Consider   the Principle of Least Privilege"></a>The Model Hears You: Audio Language Model Deployments Should Consider   the Principle of Least Privilege</h2><p><strong>Authors:Luxi He, Xiangyu Qi, Michel Liao, Inyoung Cheong, Prateek Mittal, Danqi Chen, Peter Henderson</strong></p>
<p>The latest Audio Language Models (Audio LMs) process speech directly instead of relying on a separate transcription step. This shift preserves detailed information, such as intonation or the presence of multiple speakers, that would otherwise be lost in transcription. However, it also introduces new safety risks, including the potential misuse of speaker identity cues and other sensitive vocal attributes, which could have legal implications. In this paper, we urge a closer examination of how these models are built and deployed. Our experiments show that end-to-end modeling, compared with cascaded pipelines, creates socio-technical safety risks such as identity inference, biased decision-making, and emotion detection. This raises concerns about whether Audio LMs store voiceprints and function in ways that create uncertainty under existing legal regimes. We then argue that the Principle of Least Privilege should be considered to guide the development and deployment of these models. Specifically, evaluations should assess (1) the privacy and safety risks associated with end-to-end modeling; and (2) the appropriate scope of information access. Finally, we highlight related gaps in current audio LM benchmarks and identify key open research questions, both technical and policy-related, that must be addressed to enable the responsible deployment of end-to-end Audio LMs. </p>
<blockquote>
<p>最新的音频语言模型（Audio LMs）能够直接处理语音，而无需依赖单独的转录步骤。这种转变保留了详细信息，如语调或多名演讲者的存在，这些信息在转录过程中可能会丢失。然而，这也带来了新的安全风险，包括可能滥用说话人身份线索和其他敏感语音特征，并可能涉及法律风险。在本文中，我们敦促对如何构建和部署这些模型进行更深入的审查。我们的实验表明，与级联管道相比，端到端的建模会产生社会技术安全风险，如身份推断、决策偏见和情绪检测。这引发了人们对Audio LMs是否存储语音特征并以在当前法律体系下产生不确定性方式运行的担忧。然后，我们主张考虑“最小特权原则”来指导这些模型的开发和部署。具体而言，评估应该包括（1）与端到端建模相关的隐私和安全风险；（2）信息访问的适当范围。最后，我们强调了当前音频LM基准测试中的相关空白，并确定了必须解决的关键开放研究问题，包括技术和政策相关问题，以实现端到端Audio LMs的责任部署。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.16833v2">PDF</a> Published at AIES 2025</p>
<p><strong>摘要</strong><br>语音识别领域的研究中，最新推出的音频语言模型可直接处理语音信息，无需经过单独的转录步骤。这一变革能够保留如语调、多人对话等细节信息，避免了转录过程中可能产生的信息损失。然而，这也带来了新的安全风险，如滥用说话人身份线索和其他敏感语音特征，并可能引发法律争议。本文呼吁更深入地研究这些模型的构建与部署过程。实验显示，端到端的建模方式与级联管道相比，会引发社会技术安全风险，如身份推断、决策偏见和情感检测等。这引发了关于音频语言模型是否存储语音特征以及如何以现有法律体系下不确定的方式运行的担忧。本文主张采用最小特权原则来指导这些模型的发展和应用，并对其进行评估：一是对端到端建模的隐私和安全风险进行评估；二是评估信息访问的适当范围。最后，本文强调了当前音频语言模型基准测试中的空白，并指出了必须解决的关键开放研究问题，包括技术和政策方面的问题，以实现端到端的音频语言模型的负责任部署。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>最新音频语言模型可直接处理语音信息，避免了转录过程中的信息损失。</li>
<li>直接处理语音信息也带来了新的安全风险，如滥用说话人身份线索和其他敏感语音特征。</li>
<li>端到端的建模方式可能引发社会技术安全风险，如身份推断、决策偏见和情感检测等。</li>
<li>引发了关于音频语言模型是否存储语音特征以及如何以现有法律体系下不确定的方式运行的担忧。</li>
<li>应采用最小特权原则来指导音频语言模型的发展和应用，并进行隐私和安全风险评估。</li>
<li>当前音频语言模型基准测试存在空白。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.16833">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Speech/2503.16833v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Speech/2503.16833v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Speech/2503.16833v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Speech/2503.16833v2/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Speech/2503.16833v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="VINP-Variational-Bayesian-Inference-with-Neural-Speech-Prior-for-Joint-ASR-Effective-Speech-Dereverberation-and-Blind-RIR-Identification"><a href="#VINP-Variational-Bayesian-Inference-with-Neural-Speech-Prior-for-Joint-ASR-Effective-Speech-Dereverberation-and-Blind-RIR-Identification" class="headerlink" title="VINP: Variational Bayesian Inference with Neural Speech Prior for Joint   ASR-Effective Speech Dereverberation and Blind RIR Identification"></a>VINP: Variational Bayesian Inference with Neural Speech Prior for Joint   ASR-Effective Speech Dereverberation and Blind RIR Identification</h2><p><strong>Authors:Pengyu Wang, Ying Fang, Xiaofei Li</strong></p>
<p>Reverberant speech, denoting the speech signal degraded by reverberation, contains crucial knowledge of both anechoic source speech and room impulse response (RIR). This work proposes a variational Bayesian inference (VBI) framework with neural speech prior (VINP) for joint speech dereverberation and blind RIR identification. In VINP, a probabilistic signal model is constructed in the time-frequency (T-F) domain based on convolution transfer function (CTF) approximation. For the first time, we propose using an arbitrary discriminative dereverberation deep neural network (DNN) to estimate the prior distribution of anechoic speech within a probabilistic model. By integrating both reverberant speech and the anechoic speech prior, VINP yields the maximum a posteriori (MAP) and maximum likelihood (ML) estimations of the anechoic speech spectrum and CTF filter, respectively. After simple transformations, the waveforms of anechoic speech and RIR are estimated. VINP is effective for automatic speech recognition (ASR) systems, which sets it apart from most deep learning (DL)-based single-channel dereverberation approaches. Experiments on single-channel speech dereverberation demonstrate that VINP attains state-of-the-art (SOTA) performance in mean opinion score (MOS) and word error rate (WER). For blind RIR identification, experiments demonstrate that VINP achieves SOTA performance in estimating reverberation time at 60 dB (RT60) and advanced performance in direct-to-reverberation ratio (DRR) estimation. Codes and audio samples are available online. </p>
<blockquote>
<p>带有混响的语音信号体现了原始无混响语音信号和房间脉冲响应（RIR）的关键知识。本研究提出了一个结合了神经语音先验的变贝叶斯推理（VBI）框架，用于联合语音去混响和盲RIR识别。在VINP中，基于卷积传递函数（CTF）近似值在时频（T-F）域构建了概率信号模型。我们首次提出使用任意的判别去混响深度神经网络（DNN）来估计概率模型中的无混响语音的先验分布。通过整合带混响的语音和无混响语音先验，VINP可以得到无混响语音谱和CTF滤波器的最大后验（MAP）和最大似然（ML）估计。经过简单的转换，可以估算出无混响语音和RIR的波形。VINP对于自动语音识别（ASR）系统非常有效，这与大多数基于深度学习的单通道去混响方法有所不同。在单通道语音去混响的实验中，VINP在平均意见得分（MOS）和词错误率（WER）方面达到了最新水平（SOTA）。对于盲RIR识别，实验表明VINP在估计60分贝的混响时间（RT60）方面达到了最新水平，并且在直接-混响比率（DRR）估计方面取得了先进的性能。相关代码和音频样本可以在网上找到。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.07205v3">PDF</a> Submitted to IEEE&#x2F;ACM Trans. on TASLP</p>
<p><strong>Summary</strong><br>     该研究利用变分贝叶斯推断（VBI）框架与神经网络语音先验（VINP）联合进行语音去混响和盲房间冲击响应（RIR）识别。基于卷积传递函数（CTF）近似值在时间频率（T-F）域构建概率信号模型，首次提出使用任意判别去混响深度神经网络（DNN）来估计无混响语音的先验分布。整合混响语音和无混响语音先验信息，VINP可获得无混响语音谱和CTF滤波器的最大后验（MAP）和最大似然（ML）估计值。经过简单变换，可估计无混响语音和RIR的波形。该研究方法对自动语音识别（ASR）系统有效，在单通道语音去混响方面取得了最佳性能。同时盲RIR识别实验结果亦显示其性能优异。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>变分贝叶斯推断（VBI）框架结合神经网络语音先验（VINP）用于联合语音去混响和盲房间冲击响应（RIR）识别。</li>
<li>基于卷积传递函数（CTF）在T-F域构建概率信号模型。</li>
<li>使用任意判别去混响深度神经网络（DNN）估计无混响语音的先验分布。</li>
<li>VINP可以获得无混响语音谱和CTF滤波器的MAP和ML估计。</li>
<li>VINP对自动语音识别（ASR）系统有效。</li>
<li>在单通道语音去混响方面，VINP达到了最佳性能，体现在平均意见得分（MOS）和词错误率（WER）上。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.07205">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Speech/2502.07205v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Speech/2502.07205v3/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Speech/2502.07205v3/page_3_0.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Neural-Enhanced-Dynamic-Range-Compression-Inversion-A-Hybrid-Approach-for-Restoring-Audio-Dynamics"><a href="#Neural-Enhanced-Dynamic-Range-Compression-Inversion-A-Hybrid-Approach-for-Restoring-Audio-Dynamics" class="headerlink" title="Neural-Enhanced Dynamic Range Compression Inversion: A Hybrid Approach   for Restoring Audio Dynamics"></a>Neural-Enhanced Dynamic Range Compression Inversion: A Hybrid Approach   for Restoring Audio Dynamics</h2><p><strong>Authors:Haoran Sun, Dominique Fourer, Hichem Maaref</strong></p>
<p>Dynamic Range Compression (DRC) is a widely used audio effect that adjusts signal dynamics for applications in music production, broadcasting, and speech processing. Inverting DRC is of broad importance for restoring the original dynamics, enabling remixing, and enhancing the overall audio quality. Existing DRC inversion methods either overlook key parameters or rely on precise parameter values, which can be challenging to estimate accurately. To address this limitation, we introduce a hybrid approach that combines model-based DRC inversion with neural networks to achieve robust DRC parameter estimation and audio restoration simultaneously. Our method uses tailored neural network architectures (classification and regression), which are then integrated into a model-based inversion framework to reconstruct the original signal. Experimental evaluations on various music and speech datasets confirm the effectiveness and robustness of our approach, outperforming several state-of-the-art techniques. </p>
<blockquote>
<p>动态范围压缩（DRC）是一种广泛应用于音乐制作、广播和语音处理等应用的音频效果，它用于调整信号动态。对DRC进行反转对于恢复原始动态、实现混音和增强整体音频质量具有重大意义。现有的DRC反转方法要么忽略了关键参数，要么依赖于精确的参数值，而准确估计这些参数值可能具有挑战性。为了解决这一局限性，我们引入了一种混合方法，它将基于模型的DRC反转与神经网络相结合，同时实现稳健的DRC参数估计和音频恢复。我们的方法使用定制化的神经网络架构（分类和回归），然后将其集成到基于模型的反转框架中来重建原始信号。对各种音乐和语音数据集的实验评估证实了我们方法的有效性和稳健性，优于几种最新技术。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.04337v2">PDF</a> This work has been submitted to the IEEE for possible publication</p>
<p><strong>总结</strong></p>
<p>动态范围压缩（DRC）是一种广泛应用于音频处理的效应，用于调整信号动态，应用于音乐制作、广播和语音处理等。对DRC进行反转对于恢复原始动态、混音和增强整体音频质量具有重要意义。现有的DRC反转方法要么忽略关键参数，要么依赖于难以准确估计的精确参数值。为了解决这一局限性，我们提出了一种混合方法，将基于模型的DRC反转与神经网络相结合，实现稳健的DRC参数估计和音频恢复。我们的方法使用定制化的神经网络架构（分类和回归），然后将其集成到基于模型的反转框架中以重建原始信号。在各种音乐和语音数据集上的实验评估证实了我们方法的有效性和稳健性，优于几种先进技术。</p>
<p><strong>要点</strong></p>
<ol>
<li>动态范围压缩（DRC）是一种广泛应用于音频处理的效应，用于调整信号动态。</li>
<li>DRC反转对于恢复原始动态、混音和增强整体音频质量具有重要意义。</li>
<li>现有DRC反转方法存在挑战，如忽略关键参数或依赖难以准确估计的参数值。</li>
<li>提出了一种混合方法，结合模型驱动的DRC反转和神经网络，实现稳健的DRC参数估计和音频恢复。</li>
<li>使用定制化的神经网络架构（分类和回归），集成到基于模型的反转框架中重建原始信号。</li>
<li>实验评估表明，该方法在多种数据集上表现有效且稳健，优于现有技术。</li>
<li>该方法对于音频处理和语音处理等领域具有广泛的应用前景。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.04337">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Speech/2411.04337v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Speech/2411.04337v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Speech/2411.04337v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Speech/2411.04337v2/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_Speech/2411.04337v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-12/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-12/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-12/GAN/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_GAN/2509.08392v1/page_2_0.jpg" class="responsive-img" alt="GAN">
                        
                        <span class="card-title">GAN</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            GAN 方向最新论文已更新，请持续关注 Update in 2025-09-12  VRAE Vertical Residual Autoencoder for License Plate Denoising and   Deblurring
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-12
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/GAN/" class="post-category">
                                    GAN
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/GAN/">
                        <span class="chip bg-color">GAN</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-12/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F_Breast%20Ultrasound/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-12\./crop_医学影像_Breast Ultrasound/2506.23903v3/page_0_0.jpg" class="responsive-img" alt="医学影像/Breast Ultrasound">
                        
                        <span class="card-title">医学影像/Breast Ultrasound</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学影像/Breast Ultrasound 方向最新论文已更新，请持续关注 Update in 2025-09-12  XBusNet Text-Guided Breast Ultrasound Segmentation via Multimodal   Vision-Language Learning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-12
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/" class="post-category">
                                    医学影像/Breast Ultrasound
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/">
                        <span class="chip bg-color">医学影像/Breast Ultrasound</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">27663.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
