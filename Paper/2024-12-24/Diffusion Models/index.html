<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-24  Personalized Representation from Personalized Generation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-1884053092a544fa03e65cf36dff6ad4.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-24
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-24
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    13.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    53 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2024-12-24-æ›´æ–°"><a href="#2024-12-24-æ›´æ–°" class="headerlink" title="2024-12-24 æ›´æ–°"></a>2024-12-24 æ›´æ–°</h1><h2 id="Personalized-Representation-from-Personalized-Generation"><a href="#Personalized-Representation-from-Personalized-Generation" class="headerlink" title="Personalized Representation from Personalized Generation"></a>Personalized Representation from Personalized Generation</h2><p><strong>Authors:Shobhita Sundaram, Julia Chae, Yonglong Tian, Sara Beery, Phillip Isola</strong></p>
<p>Modern vision models excel at general purpose downstream tasks. It is unclear, however, how they may be used for personalized vision tasks, which are both fine-grained and data-scarce. Recent works have successfully applied synthetic data to general-purpose representation learning, while advances in T2I diffusion models have enabled the generation of personalized images from just a few real examples. Here, we explore a potential connection between these ideas, and formalize the challenge of using personalized synthetic data to learn personalized representations, which encode knowledge about an object of interest and may be flexibly applied to any downstream task relating to the target object. We introduce an evaluation suite for this challenge, including reformulations of two existing datasets and a novel dataset explicitly constructed for this purpose, and propose a contrastive learning approach that makes creative use of image generators. We show that our method improves personalized representation learning for diverse downstream tasks, from recognition to segmentation, and analyze characteristics of image generation approaches that are key to this gain. </p>
<blockquote>
<p>ç°ä»£è§†è§‰æ¨¡å‹åœ¨é€šç”¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ã€‚ç„¶è€Œï¼Œå¯¹äºç²¾ç»†ä¸”æ•°æ®ç¨€ç¼ºçš„ä¸ªæ€§åŒ–è§†è§‰ä»»åŠ¡ï¼Œå¦‚ä½•åº”ç”¨è¿™äº›æ¨¡å‹å°šä¸æ¸…æ¥šã€‚æœ€è¿‘çš„ç ”ç©¶å·¥ä½œå·²æˆåŠŸå°†åˆæˆæ•°æ®åº”ç”¨äºé€šç”¨è¡¨ç¤ºå­¦ä¹ ï¼Œè€Œæ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹çš„è¿›å±•ä½¿å¾—ä»…ä»å‡ ä¸ªçœŸå®ç¤ºä¾‹ä¸­ç”Ÿæˆä¸ªæ€§åŒ–å›¾åƒæˆä¸ºå¯èƒ½ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æ¢ç´¢äº†è¿™ä¸¤è€…ä¹‹é—´çš„æ½œåœ¨è”ç³»ï¼Œå¹¶æ­£å¼æå‡ºäº†ä½¿ç”¨ä¸ªæ€§åŒ–åˆæˆæ•°æ®æ¥å­¦ä¹ ä¸ªæ€§åŒ–è¡¨ç¤ºçš„æŒ‘æˆ˜ï¼Œè¿™ç§è¡¨ç¤ºå½¢å¼èƒ½å¤Ÿç¼–ç ç›®æ ‡å¯¹è±¡çš„æœ‰å…³çŸ¥è¯†ï¼Œå¹¶å¯çµæ´»åº”ç”¨äºä¸è¯¥ç›®æ ‡å¯¹è±¡ç›¸å…³çš„ä»»ä½•ä¸‹æ¸¸ä»»åŠ¡ã€‚æˆ‘ä»¬ä¸ºè¿™ä¸ªæŒ‘æˆ˜è®¾è®¡äº†ä¸€å¥—è¯„ä¼°æ–¹æ¡ˆï¼ŒåŒ…æ‹¬å¯¹ä¸¤ä¸ªç°æœ‰æ•°æ®é›†çš„é‡æ–°æ„å»ºå’Œä¸€ä¸ªä¸“é—¨ä¸ºæ­¤ç›®çš„æ„å»ºçš„æ–°æ•°æ®é›†ï¼Œå¹¶æå‡ºäº†ä¸€ç§å¯¹æ¯”å­¦ä¹ æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯ä»¥åˆ›é€ æ€§åœ°åˆ©ç”¨å›¾åƒç”Ÿæˆå™¨ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥æ”¹å–„å¤šæ ·åŒ–ä¸‹æ¸¸ä»»åŠ¡çš„ä¸ªæ€§åŒ–è¡¨ç¤ºå­¦ä¹ ï¼ŒåŒ…æ‹¬è¯†åˆ«åˆ°åˆ†å‰²ï¼Œå¹¶å¯¹å›¾åƒç”Ÿæˆæ–¹æ³•çš„å…³é”®ç‰¹æ€§è¿›è¡Œäº†åˆ†æï¼Œè¿™äº›ç‰¹æ€§å¯¹æ­¤å¢ç›Šè‡³å…³é‡è¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.16156v1">PDF</a> S.S. and J.C contributed equally; S.B. and P.I. co-supervised.   Project page: <a target="_blank" rel="noopener" href="https://personalized-rep.github.io/">https://personalized-rep.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>ç°ä»£è§†è§‰æ¨¡å‹åœ¨é€šç”¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ä¸ªæ€§åŒ–è§†è§‰ä»»åŠ¡æ–¹é¢ä»å­˜åœ¨ä¸æ˜ç¡®æ€§ï¼Œå°¤å…¶æ˜¯é’ˆå¯¹ç²¾ç»†åŒ–ä¸”æ•°æ®ç¨€ç¼ºçš„ä»»åŠ¡ã€‚è¿‘æœŸå·¥ä½œæˆåŠŸå°†åˆæˆæ•°æ®åº”ç”¨äºé€šç”¨è¡¨ç¤ºå­¦ä¹ ï¼Œè€Œæ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹çš„å‘å±•ä½¿å¾—ä»…ä»å°‘é‡çœŸå®ç¤ºä¾‹ç”Ÿæˆä¸ªæ€§åŒ–å›¾åƒæˆä¸ºå¯èƒ½ã€‚æœ¬æ–‡æ¢ç´¢äº†è¿™ä¸¤è€…ä¹‹é—´çš„æ½œåœ¨è”ç³»ï¼Œå¹¶æ­£å¼æå‡ºäº†åˆ©ç”¨ä¸ªæ€§åŒ–åˆæˆæ•°æ®å­¦ä¹ ä¸ªæ€§åŒ–è¡¨ç¤ºçš„æŒ‘æˆ˜ã€‚è¿™äº›è¡¨ç¤ºå½¢å¼èƒ½å¤Ÿç¼–ç ç›®æ ‡å¯¹è±¡çš„æœ‰å…³çŸ¥è¯†å¹¶å¯çµæ´»åº”ç”¨äºä¸ä¹‹ç›¸å…³çš„ä»»ä½•ä¸‹æ¸¸ä»»åŠ¡ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€å¥—é’ˆå¯¹è¿™ä¸€æŒ‘æˆ˜çš„è¯„ä»·æ–¹æ¡ˆï¼ŒåŒ…æ‹¬ä¸¤ä¸ªç°æœ‰æ•°æ®é›†çš„é‡æ–°æ„å»ºä»¥åŠä¸€ä¸ªä¸“é—¨ä¸ºè¯¥ç›®çš„æ„å»ºçš„æ–°é¢–æ•°æ®é›†ï¼Œå¹¶æå‡ºäº†ä¸€ç§åˆ›é€ æ€§åˆ©ç”¨å›¾åƒç”Ÿæˆå™¨çš„å¯¹æ¯”å­¦ä¹ æ–¹æ³•ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å¯æ”¹å–„å¤šæ ·åŒ–çš„ä¸‹æ¸¸ä»»åŠ¡çš„ä¸ªæ€§åŒ–è¡¨ç¤ºå­¦ä¹ ï¼ŒåŒ…æ‹¬è¯†åˆ«ä¸åˆ†å‰²ï¼Œå¹¶åˆ†æäº†å›¾åƒç”Ÿæˆæ–¹æ³•çš„å…³é”®ç‰¹æ€§å¯¹æ­¤å¢ç›Šçš„å½±å“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°ä»£è§†è§‰æ¨¡å‹åœ¨é€šç”¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨ä¸ªæ€§åŒ–è§†è§‰ä»»åŠ¡ä¸Šå­˜åœ¨ä¸ç¡®å®šæ€§ã€‚</li>
<li>åˆæˆæ•°æ®å·²æˆåŠŸåº”ç”¨äºé€šç”¨è¡¨ç¤ºå­¦ä¹ ã€‚</li>
<li>T2Iæ‰©æ•£æ¨¡å‹å¯ä»å°‘é‡çœŸå®ç¤ºä¾‹ç”Ÿæˆä¸ªæ€§åŒ–å›¾åƒã€‚</li>
<li>æœ¬æ–‡æ¢ç´¢äº†å°†åˆæˆæ•°æ®ä¸ä¸ªæ€§åŒ–è§†è§‰ä»»åŠ¡ç»“åˆçš„å¯èƒ½æ€§ï¼Œå¹¶æ­£å¼æå‡ºäº†å­¦ä¹ ä¸ªæ€§åŒ–è¡¨ç¤ºçš„æŒ‘æˆ˜ã€‚</li>
<li>ä»‹ç»äº†ä¸€å¥—è¯„ä»·æ–¹æ¡ˆï¼ŒåŒ…æ‹¬é’ˆå¯¹ä¸ªæ€§åŒ–è§†è§‰ä»»åŠ¡çš„ç°æœ‰æ•°æ®é›†çš„é‡æ–°æ„å»ºå’Œæ–°æ•°æ®é›†çš„å¼€å‘ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åˆ©ç”¨å›¾åƒç”Ÿæˆå™¨çš„å¯¹æ¯”å­¦ä¹ æ–¹æ³•ï¼Œå¯æ”¹å–„ä¸ªæ€§åŒ–è¡¨ç¤ºå­¦ä¹ ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.16156">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-1884053092a544fa03e65cf36dff6ad4.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-f073e9ddedbe9dd64df432c9b8a3bb01.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-3baf545297c39bf04a08240f6898050d.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Label-Efficient-Data-Augmentation-with-Video-Diffusion-Models-for-Guidewire-Segmentation-in-Cardiac-Fluoroscopy"><a href="#Label-Efficient-Data-Augmentation-with-Video-Diffusion-Models-for-Guidewire-Segmentation-in-Cardiac-Fluoroscopy" class="headerlink" title="Label-Efficient Data Augmentation with Video Diffusion Models for   Guidewire Segmentation in Cardiac Fluoroscopy"></a>Label-Efficient Data Augmentation with Video Diffusion Models for   Guidewire Segmentation in Cardiac Fluoroscopy</h2><p><strong>Authors:Shaoyan Pan, Yikang Liu, Lin Zhao, Eric Z. Chen, Xiao Chen, Terrence Chen, Shanhui Sun</strong></p>
<p>The accurate segmentation of guidewires in interventional cardiac fluoroscopy videos is crucial for computer-aided navigation tasks. Although deep learning methods have demonstrated high accuracy and robustness in wire segmentation, they require substantial annotated datasets for generalizability, underscoring the need for extensive labeled data to enhance model performance. To address this challenge, we propose the Segmentation-guided Frame-consistency Video Diffusion Model (SF-VD) to generate large collections of labeled fluoroscopy videos, augmenting the training data for wire segmentation networks. SF-VD leverages videos with limited annotations by independently modeling scene distribution and motion distribution. It first samples the scene distribution by generating 2D fluoroscopy images with wires positioned according to a specified input mask, and then samples the motion distribution by progressively generating subsequent frames, ensuring frame-to-frame coherence through a frame-consistency strategy. A segmentation-guided mechanism further refines the process by adjusting wire contrast, ensuring a diverse range of visibility in the synthesized image. Evaluation on a fluoroscopy dataset confirms the superior quality of the generated videos and shows significant improvements in guidewire segmentation. </p>
<blockquote>
<p>åœ¨å¿ƒè„ä»‹å…¥æ‰‹æœ¯çš„è§å…‰é€è§†è§†é¢‘ä¸­ï¼Œå¯¹å¯¼çº¿è¿›è¡Œç²¾ç¡®åˆ†å‰²å¯¹äºè®¡ç®—æœºè¾…åŠ©å¯¼èˆªä»»åŠ¡è‡³å…³é‡è¦ã€‚è™½ç„¶æ·±åº¦å­¦ä¹ çš„æ–¹æ³•åœ¨å¯¼çº¿åˆ†å‰²æ–¹é¢è¡¨ç°å‡ºäº†é«˜å‡†ç¡®æ€§å’Œç¨³å¥æ€§ï¼Œä½†å®ƒä»¬éœ€è¦å¤§é‡æ ‡æ³¨æ•°æ®é›†ä»¥å®ç°æ™®éé€‚ç”¨æ€§ï¼Œè¿™å¼ºè°ƒäº†éœ€è¦æ›´å¤šæ ‡æ³¨æ•°æ®æ¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºåˆ†å‰²å¼•å¯¼çš„å¸§ä¸€è‡´æ€§è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ˆSF-VDï¼‰ï¼Œç”¨äºç”Ÿæˆå¤§é‡æ ‡æ³¨çš„è§å…‰é€è§†è§†é¢‘ï¼Œå¢å¼ºå¯¼çº¿åˆ†å‰²ç½‘ç»œçš„è®­ç»ƒæ•°æ®ã€‚SF-VDé€šè¿‡ç‹¬ç«‹å»ºæ¨¡åœºæ™¯åˆ†å¸ƒå’Œè¿åŠ¨åˆ†å¸ƒï¼Œå……åˆ†åˆ©ç”¨äº†æ ‡æ³¨æœ‰é™çš„æ•°æ®é›†ã€‚å®ƒé¦–å…ˆé€šè¿‡æ ¹æ®æŒ‡å®šçš„è¾“å…¥æ©è†œç”Ÿæˆå¸¦æœ‰å¯¼çº¿çš„äºŒç»´è§å…‰é€è§†å›¾åƒæ¥é‡‡æ ·åœºæ™¯åˆ†å¸ƒï¼Œç„¶åé€šè¿‡é€æ­¥ç”Ÿæˆåç»­å¸§æ¥é‡‡æ ·è¿åŠ¨åˆ†å¸ƒï¼Œå¹¶é€šè¿‡å¸§ä¸€è‡´æ€§ç­–ç•¥ç¡®ä¿å¸§åˆ°å¸§çš„è¿è´¯æ€§ã€‚åˆ†å‰²å¼•å¯¼æœºåˆ¶è¿›ä¸€æ­¥å¾®è°ƒè¿‡ç¨‹ï¼Œé€šè¿‡è°ƒæ•´å¯¼çº¿å¯¹æ¯”åº¦ï¼Œç¡®ä¿åˆæˆå›¾åƒä¸­å¯è§æ€§èŒƒå›´å¤šæ ·ã€‚åœ¨è§å…‰é€è§†æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¯æ˜äº†æ‰€ç”Ÿæˆè§†é¢‘çš„é«˜è´¨é‡ä»¥åŠæ˜¾è‘—æé«˜äº†å¯¼çº¿åˆ†å‰²çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.16050v1">PDF</a> AAAI 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>åœ¨ä»‹å…¥å¿ƒè„è§å…‰è§†é¢‘ä¸­å¯¹å¯¼çº¿è¿›è¡Œç²¾ç¡®åˆ†å‰²å¯¹äºè®¡ç®—æœºè¾…åŠ©å¯¼èˆªä»»åŠ¡è‡³å…³é‡è¦ã€‚æ·±åº¦å­¦ä¹ è™½å·²å±•ç°å‡ºåœ¨å¯¼çº¿åˆ†å‰²æ–¹é¢çš„é«˜ç²¾åº¦å’Œç¨³å¥æ€§ï¼Œä½†å…¶éœ€è¦å¤§é‡æ ‡æ³¨æ•°æ®é›†æ‰èƒ½è¾¾åˆ°é€šç”¨æ€§ï¼Œå› æ­¤äºŸéœ€æ‰©å……æ ‡è®°æ•°æ®ä»¥å¢å¼ºæ¨¡å‹æ€§èƒ½ã€‚ä¸ºè§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºåˆ†å‰²å¼•å¯¼çš„å¸§ä¸€è‡´æ€§è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ˆSF-VDï¼‰ï¼Œç”¨äºç”Ÿæˆå¤§é‡æ ‡è®°è§å…‰è§†é¢‘ï¼Œæ‰©å……å¯¼çº¿åˆ†å‰²ç½‘ç»œçš„è®­ç»ƒæ•°æ®ã€‚SF-VDåˆ©ç”¨æœ‰é™æ ‡æ³¨çš„è§†é¢‘ï¼Œé€šè¿‡ç‹¬ç«‹å»ºæ¨¡åœºæ™¯åˆ†å¸ƒå’Œè¿åŠ¨åˆ†å¸ƒæ¥å®ç°ã€‚å®ƒé¦–å…ˆæ ¹æ®æŒ‡å®šçš„è¾“å…¥æ©è†œç”Ÿæˆå¸¦æœ‰å¯¼çº¿çš„äºŒç»´è§å…‰å›¾åƒæ¥é‡‡æ ·åœºæ™¯åˆ†å¸ƒï¼Œç„¶åé€šè¿‡é€æ­¥ç”Ÿæˆåç»­å¸§æ¥é‡‡æ ·è¿åŠ¨åˆ†å¸ƒï¼Œç¡®ä¿å¸§é—´ä¸€è‡´æ€§ã€‚åˆ†å‰²å¼•å¯¼æœºåˆ¶è¿›ä¸€æ­¥è°ƒæ•´å¯¼çº¿å¯¹æ¯”åº¦ï¼Œç¡®ä¿åˆæˆå›¾åƒä¸­å¯è§æ€§å¤šæ ·ã€‚åœ¨è§å…‰æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¯æ˜äº†ç”Ÿæˆè§†é¢‘çš„é«˜è´¨é‡ï¼Œå¹¶æ˜¾ç¤ºå‡ºåœ¨å¯¼çº¿åˆ†å‰²æ–¹é¢çš„æ˜¾è‘—æ”¹å–„ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å‡†ç¡®åˆ†å‰²ä»‹å…¥å¿ƒè„è§å…‰è§†é¢‘ä¸­çš„å¯¼çº¿å¯¹è®¡ç®—æœºå¯¼èˆªè‡³å…³é‡è¦ã€‚</li>
<li>æ·±åº¦å­¦ä¹ åœ¨å¯¼çº¿åˆ†å‰²ä¸Šè¡¨ç°å‡ºé«˜å‡†ç¡®åº¦å’Œç¨³å¥æ€§ï¼Œä½†éœ€è¦å¤§é‡æ ‡æ³¨æ•°æ®ã€‚</li>
<li>æå‡ºSF-VDæ¨¡å‹ä»¥ç”Ÿæˆæ ‡è®°è§å…‰è§†é¢‘ï¼Œå¢å¼ºå¯¼çº¿åˆ†å‰²ç½‘ç»œçš„è®­ç»ƒæ•°æ®ã€‚</li>
<li>SF-VDé€šè¿‡ç‹¬ç«‹å»ºæ¨¡åœºæ™¯åˆ†å¸ƒå’Œè¿åŠ¨åˆ†å¸ƒæ¥å®ç°é«˜æ•ˆç”Ÿæˆã€‚</li>
<li>æ¨¡å‹é€šè¿‡ç”ŸæˆäºŒç»´è§å…‰å›¾åƒå’Œé€æ­¥ç”Ÿæˆåç»­å¸§æ¥é‡‡æ ·åœºæ™¯å’Œè¿åŠ¨åˆ†å¸ƒã€‚</li>
<li>åˆ†å‰²å¼•å¯¼æœºåˆ¶ç¡®ä¿åˆæˆå›¾åƒä¸­å¯¼çº¿çš„å¯è§æ€§å¤šæ ·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.16050">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-2823eb08bd9481ba20a23d5b12fb1314.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-c9a2f0b88c0190b7511a9244cdd8f011.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-d6dfead2b062aa6a8f15edfc59eb3429.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-0d47112dbe22818737ff4b9362916e1d.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Semi-Supervised-Adaptation-of-Diffusion-Models-for-Handwritten-Text-Generation"><a href="#Semi-Supervised-Adaptation-of-Diffusion-Models-for-Handwritten-Text-Generation" class="headerlink" title="Semi-Supervised Adaptation of Diffusion Models for Handwritten Text   Generation"></a>Semi-Supervised Adaptation of Diffusion Models for Handwritten Text   Generation</h2><p><strong>Authors:Kai Brandenbusch</strong></p>
<p>The generation of images of realistic looking, readable handwritten text is a challenging task which is referred to as handwritten text generation (HTG). Given a string and examples from a writer, the goal is to synthesize an image depicting the correctly spelled word in handwriting with the calligraphic style of the desired writer. An important application of HTG is the generation of training images in order to adapt downstream models for new data sets. With their success in natural image generation, diffusion models (DMs) have become the state-of-the-art approach in HTG. In this work, we present an extension of a latent DM for HTG to enable generation of writing styles not seen during training by learning style conditioning with a masked auto encoder. Our proposed content encoder allows for different ways of conditioning the DM on textual and calligraphic features. Additionally, we employ classifier-free guidance and explore the influence on the quality of the generated training images. For adapting the model to a new unlabeled data set, we propose a semi-supervised training scheme. We evaluate our approach on the IAM-database and use the RIMES-database to examine the generation of data not seen during training achieving improvements in this particularly promising application of DMs for HTG. </p>
<blockquote>
<p>æ‰‹å†™æ–‡æœ¬ç”Ÿæˆï¼ˆHTGï¼‰æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå³ç”Ÿæˆçœ‹èµ·æ¥éå¸¸é€¼çœŸã€å¯è¯†åˆ«çš„æ‰‹å†™æ–‡æœ¬å›¾åƒã€‚ç»™å®šä¸€ä¸ªå­—ç¬¦ä¸²å’Œä½œè€…çš„ä¾‹å­ï¼Œç›®æ ‡æ˜¯åˆæˆä¸€å¼ æ­£ç¡®æ‹¼å†™çš„æ‰‹å†™å•è¯å›¾åƒï¼Œå¹¶å…·æœ‰æ‰€éœ€ä½œè€…çš„ä¹¦æ³•é£æ ¼ã€‚HTGçš„ä¸€ä¸ªé‡è¦åº”ç”¨æ˜¯ç”Ÿæˆè®­ç»ƒå›¾åƒï¼Œä»¥é€‚åº”æ–°çš„æ•°æ®é›†ã€‚ç”±äºå…¶åœ¨è‡ªç„¶å›¾åƒç”Ÿæˆæ–¹é¢çš„æˆåŠŸï¼Œæ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰å·²æˆä¸ºHTGä¸­æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é’ˆå¯¹HTGçš„æ½œåœ¨DMçš„æ‰©å±•ï¼Œé€šè¿‡å¸¦æœ‰æ©ç çš„è‡ªç¼–ç å™¨å­¦ä¹ é£æ ¼è°ƒèŠ‚ï¼Œä»¥ç”Ÿæˆåœ¨è®­ç»ƒä¸­æœªè§è¿‡çš„ä¹¦å†™é£æ ¼ã€‚æˆ‘ä»¬æå‡ºçš„å†…å®¹ç¼–ç å™¨å…è®¸ä»¥ä¸åŒçš„æ–¹å¼å°†DMè°ƒèŠ‚åˆ°æ–‡æœ¬å’Œä¹¦æ³•ç‰¹å¾ä¸Šã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†æ— åˆ†ç±»å™¨å¼•å¯¼ï¼Œå¹¶æ¢ç´¢äº†å¯¹ç”Ÿæˆè®­ç»ƒå›¾åƒè´¨é‡çš„å½±å“ã€‚ä¸ºäº†å°†æ¨¡å‹é€‚åº”æ–°çš„æœªæ ‡è®°æ•°æ®é›†ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŠç›‘ç£è®­ç»ƒæ–¹æ¡ˆã€‚æˆ‘ä»¬åœ¨IAMæ•°æ®åº“ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œå¹¶ä½¿ç”¨RIMESæ•°æ®åº“æ¥æ£€æŸ¥åœ¨è®­ç»ƒä¸­æœªè§åˆ°çš„æ•°æ®çš„ç”Ÿæˆæƒ…å†µï¼Œåœ¨å®ç°è¿™ä¸€ç‰¹åˆ«æœ‰å‰é€”çš„HTG DMåº”ç”¨ä¸­å–å¾—äº†æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15853v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æ‰‹å†™æ–‡æœ¬ç”Ÿæˆï¼ˆHTGï¼‰çš„æŒ‘æˆ˜ï¼Œå…¶ç›®æ ‡æ˜¯ç”Ÿæˆå…·æœ‰ç‰¹å®šä¹¦å†™é£æ ¼çš„æ‰‹å†™æ–‡æœ¬å›¾åƒã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œç ”ç©¶è€…ä»¬å°†æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰åº”ç”¨äºHTGï¼Œå¹¶å–å¾—äº†æ˜¾è‘—æˆæœã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ½œåœ¨DMçš„HTGæ‰©å±•æ–¹æ³•ï¼Œé€šè¿‡æ©ç è‡ªåŠ¨ç¼–ç å™¨å­¦ä¹ é£æ ¼è°ƒèŠ‚ï¼Œä»¥ç”Ÿæˆåœ¨è®­ç»ƒä¸­æœªè§è¿‡çš„ä¹¦å†™é£æ ¼ã€‚åŒæ—¶ï¼Œç ”ç©¶è¿˜æ¶‰åŠæ¨¡å‹å¯¹æ–°æ•°æ®é›†çš„é€‚åº”æ€§ï¼Œæå‡ºäº†åŠç›‘ç£è®­ç»ƒæ–¹æ¡ˆã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨IAMæ•°æ®åº“å’ŒRIMESæ•°æ®åº“ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰‹å†™æ–‡æœ¬ç”Ÿæˆï¼ˆHTGï¼‰æ˜¯ä¸€é¡¹æŒ‘æˆ˜ä»»åŠ¡ï¼Œç›®æ ‡æ˜¯ç”Ÿæˆå…·æœ‰ç‰¹å®šä¹¦å†™é£æ ¼çš„æ‰‹å†™æ–‡æœ¬å›¾åƒã€‚</li>
<li>æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰åœ¨æ‰‹å†™æ–‡æœ¬ç”Ÿæˆé¢†åŸŸè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œæˆä¸ºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºæ½œåœ¨DMçš„HTGæ‰©å±•æ–¹æ³•ï¼Œèƒ½å¤Ÿç”Ÿæˆåœ¨è®­ç»ƒä¸­æœªè§è¿‡çš„ä¹¦å†™é£æ ¼ã€‚</li>
<li>é€šè¿‡æ©ç è‡ªåŠ¨ç¼–ç å™¨å­¦ä¹ é£æ ¼è°ƒèŠ‚ï¼Œå®ç°äº†å¯¹DMçš„å†…å®¹ç¼–ç å™¨çš„ä¸åŒè°ƒèŠ‚æ–¹å¼ã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨äº†æ— åˆ†ç±»å™¨å¼•å¯¼çš„æ–¹æ³•ï¼Œå¹¶æ¢è®¨äº†å…¶å¯¹ç”Ÿæˆè®­ç»ƒå›¾åƒè´¨é‡çš„å½±å“ã€‚</li>
<li>ä¸ºäº†é€‚åº”æ–°æ•°æ®é›†ï¼Œç ”ç©¶æå‡ºäº†åŠç›‘ç£è®­ç»ƒæ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15853">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-463d7c8ca6c6c07d4e8f820868886648.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Diffusion-Based-Conditional-Image-Editing-through-Optimized-Inference-with-Guidance"><a href="#Diffusion-Based-Conditional-Image-Editing-through-Optimized-Inference-with-Guidance" class="headerlink" title="Diffusion-Based Conditional Image Editing through Optimized Inference   with Guidance"></a>Diffusion-Based Conditional Image Editing through Optimized Inference   with Guidance</h2><p><strong>Authors:Hyunsoo Lee, Minsoo Kang, Bohyung Han</strong></p>
<p>We present a simple but effective training-free approach for text-driven image-to-image translation based on a pretrained text-to-image diffusion model. Our goal is to generate an image that aligns with the target task while preserving the structure and background of a source image. To this end, we derive the representation guidance with a combination of two objectives: maximizing the similarity to the target prompt based on the CLIP score and minimizing the structural distance to the source latent variable. This guidance improves the fidelity of the generated target image to the given target prompt while maintaining the structure integrity of the source image. To incorporate the representation guidance component, we optimize the target latent variable of diffusion modelâ€™s reverse process with the guidance. Experimental results demonstrate that our method achieves outstanding image-to-image translation performance on various tasks when combined with the pretrained Stable Diffusion model. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„åŸºäºé¢„è®­ç»ƒæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„è®­ç»ƒå¤–æ–‡æœ¬é©±åŠ¨å›¾åƒåˆ°å›¾åƒè½¬æ¢æ–¹æ³•ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯ç”Ÿæˆç¬¦åˆç›®æ ‡ä»»åŠ¡çš„å›¾åƒï¼ŒåŒæ—¶ä¿ç•™æºå›¾åƒçš„ç»“æ„å’ŒèƒŒæ™¯ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬é€šè¿‡ç»“åˆä¸¤ä¸ªç›®æ ‡æ¥æ¨å¯¼è¡¨ç¤ºå¼•å¯¼ï¼šåŸºäºCLIPåˆ†æ•°çš„æœ€å¤§åŒ–ä¸ç›®æ ‡æç¤ºçš„ç›¸ä¼¼æ€§ï¼Œä»¥åŠæœ€å°åŒ–ä¸æºæ½œåœ¨å˜é‡çš„ç»“æ„è·ç¦»ã€‚è¿™ç§å¼•å¯¼æé«˜äº†ç”Ÿæˆçš„ç›®æ ‡å›¾åƒå¯¹ç»™å®šç›®æ ‡æç¤ºçš„ä¿çœŸåº¦ï¼ŒåŒæ—¶ä¿æŒäº†æºå›¾åƒçš„ç»“æ„å®Œæ•´æ€§ã€‚ä¸ºäº†èå…¥è¡¨ç¤ºå¼•å¯¼æˆåˆ†ï¼Œæˆ‘ä»¬ä¼˜åŒ–äº†æ‰©æ•£æ¨¡å‹åå‘è¿‡ç¨‹çš„ç›®æ ‡æ½œåœ¨å˜é‡ä»¥è¿›è¡Œå¼•å¯¼ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸é¢„è®­ç»ƒçš„Stable Diffusionæ¨¡å‹ç›¸ç»“åˆæ—¶ï¼Œåœ¨å„ç§ä»»åŠ¡ä¸Šå®ç°äº†å‡ºè‰²çš„å›¾åƒåˆ°å›¾åƒçš„ç¿»è¯‘æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15798v1">PDF</a> WACV 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºé¢„è®­ç»ƒæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„ç®€å•æœ‰æ•ˆçš„è®­ç»ƒå…è´¹æ–‡æœ¬é©±åŠ¨å›¾åƒåˆ°å›¾åƒè½¬æ¢æ–¹æ³•ã€‚è¯¥æ–¹æ³•æ—¨åœ¨ç”Ÿæˆä¸ç›®æ ‡ä»»åŠ¡å¯¹é½çš„å›¾åƒï¼ŒåŒæ—¶ä¿ç•™æºå›¾åƒçš„ç»“æ„å’ŒèƒŒæ™¯ã€‚é€šè¿‡ç»“åˆä¸¤ä¸ªç›®æ ‡ï¼šæœ€å¤§åŒ–åŸºäºCLIPåˆ†æ•°çš„ç›®æ ‡æç¤ºçš„ç›¸ä¼¼æ€§ï¼Œä»¥åŠæœ€å°åŒ–æºæ½œåœ¨å˜é‡ä¸ç»“æ„è·ç¦»ï¼Œä»¥æ”¹è¿›ç”Ÿæˆçš„å›¾åƒå¯¹ç»™å®šç›®æ ‡æç¤ºçš„ä¿çœŸåº¦å¹¶ä¿æŒæºå›¾åƒçš„ç»“æ„å®Œæ•´æ€§ã€‚é€šè¿‡ä¼˜åŒ–æ‰©æ•£æ¨¡å‹åå‘è¿‡ç¨‹ä¸­çš„ç›®æ ‡æ½œåœ¨å˜é‡æ¥å®ç°è¡¨ç¤ºæŒ‡å¯¼ç»„ä»¶çš„èå…¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå½“ä¸é¢„è®­ç»ƒçš„Stable Diffusionæ¨¡å‹ç»“åˆæ—¶ï¼Œè¯¥æ–¹æ³•åœ¨å„ç§ä»»åŠ¡ä¸Šå®ç°äº†å‡ºè‰²çš„å›¾åƒåˆ°å›¾åƒè½¬æ¢æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§åŸºäºé¢„è®­ç»ƒæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„æ–‡æœ¬é©±åŠ¨å›¾åƒè½¬æ¢æ–¹æ³•ã€‚</li>
<li>æ—¨åœ¨ç”Ÿæˆä¸ç›®æ ‡ä»»åŠ¡å¯¹é½çš„å›¾åƒï¼ŒåŒæ—¶ä¿ç•™æºå›¾åƒçš„ç»“æ„å’ŒèƒŒæ™¯ã€‚</li>
<li>é€šè¿‡ç»“åˆä¸¤ä¸ªç›®æ ‡æ¥å®ç°è¡¨ç¤ºæŒ‡å¯¼ï¼šæœ€å¤§åŒ–ç›®æ ‡æç¤ºçš„ç›¸ä¼¼æ€§å¹¶æœ€å°åŒ–æºå›¾åƒçš„ç»“æ„è·ç¦»ã€‚</li>
<li>ä¼˜åŒ–æ‰©æ•£æ¨¡å‹åå‘è¿‡ç¨‹ä¸­çš„ç›®æ ‡æ½œåœ¨å˜é‡ä»¥èå…¥è¡¨ç¤ºæŒ‡å¯¼ç»„ä»¶ã€‚</li>
<li>æ–¹æ³•åœ¨å¤šç§ä»»åŠ¡ä¸Šå®ç°äº†å‡ºè‰²çš„å›¾åƒè½¬æ¢æ€§èƒ½ã€‚</li>
<li>ä¸é¢„è®­ç»ƒçš„Stable Diffusionæ¨¡å‹ç»“åˆä½¿ç”¨æ—¶ï¼Œè¯¥æ–¹æ³•è¡¨ç°å°¤ä¸ºå‡ºè‰²ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15798">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-87b4b6c5d7d89e96e80ad4406449847d.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-05105c3999a0c6cd1deccd2dd5951ec0.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-c9394744f2e118c4a3bc969da30f1a33.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-672f09092b7703aa65f632753bc82c98.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-d0ff41a2be6c8399777cde9c0a25e60a.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="CustomTTT-Motion-and-Appearance-Customized-Video-Generation-via-Test-Time-Training"><a href="#CustomTTT-Motion-and-Appearance-Customized-Video-Generation-via-Test-Time-Training" class="headerlink" title="CustomTTT: Motion and Appearance Customized Video Generation via   Test-Time Training"></a>CustomTTT: Motion and Appearance Customized Video Generation via   Test-Time Training</h2><p><strong>Authors:Xiuli Bi, Jian Lu, Bo Liu, Xiaodong Cun, Yong Zhang, Weisheng Li, Bin Xiao</strong></p>
<p>Benefiting from large-scale pre-training of text-video pairs, current text-to-video (T2V) diffusion models can generate high-quality videos from the text description. Besides, given some reference images or videos, the parameter-efficient fine-tuning method, i.e. LoRA, can generate high-quality customized concepts, e.g., the specific subject or the motions from a reference video. However, combining the trained multiple concepts from different references into a single network shows obvious artifacts. To this end, we propose CustomTTT, where we can joint custom the appearance and the motion of the given video easily. In detail, we first analyze the prompt influence in the current video diffusion model and find the LoRAs are only needed for the specific layers for appearance and motion customization. Besides, since each LoRA is trained individually, we propose a novel test-time training technique to update parameters after combination utilizing the trained customized models. We conduct detailed experiments to verify the effectiveness of the proposed methods. Our method outperforms several state-of-the-art works in both qualitative and quantitative evaluations. </p>
<blockquote>
<p>å—ç›Šäºå¤§è§„æ¨¡æ–‡æœ¬-è§†é¢‘å¯¹é¢„è®­ç»ƒï¼Œå½“å‰çš„æ–‡æœ¬åˆ°è§†é¢‘ï¼ˆT2Vï¼‰æ‰©æ•£æ¨¡å‹å¯ä»¥ä»æ–‡æœ¬æè¿°ç”Ÿæˆé«˜è´¨é‡è§†é¢‘ã€‚æ­¤å¤–ï¼Œç»™å®šä¸€äº›å‚è€ƒå›¾åƒæˆ–è§†é¢‘ï¼Œå‚æ•°é«˜æ•ˆçš„å¾®è°ƒæ–¹æ³•ï¼Œå³LoRAï¼Œå¯ä»¥ç”Ÿæˆé«˜è´¨é‡çš„è‡ªå®šæ¦‚å¿µï¼Œä¾‹å¦‚å‚è€ƒè§†é¢‘ä¸­çš„ç‰¹å®šä¸»é¢˜æˆ–åŠ¨ä½œã€‚ç„¶è€Œï¼Œä»ä¸åŒå‚è€ƒä¸­è®­ç»ƒå‡ºçš„å¤šä¸ªæ¦‚å¿µåˆå¹¶åˆ°å•ä¸ªç½‘ç»œä¸­ä¼šäº§ç”Ÿæ˜æ˜¾çš„ä¼ªå½±ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†CustomTTTï¼Œå¯ä»¥è½»æ¾åœ°è”åˆå®šåˆ¶ç»™å®šè§†é¢‘çš„å¤–è§‚å’Œè¿åŠ¨ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆåˆ†æå½“å‰è§†é¢‘æ‰©æ•£æ¨¡å‹ä¸­çš„æç¤ºå½±å“ï¼Œå¹¶å‘ç°å¯¹äºå¤–è§‚å’Œè¿åŠ¨å®šåˆ¶è€Œè¨€ï¼ŒLoRAä»…éœ€è¦é’ˆå¯¹ç‰¹å®šå±‚è¿›è¡Œè®­ç»ƒã€‚æ­¤å¤–ï¼Œç”±äºæ¯ä¸ªLoRAéƒ½æ˜¯å•ç‹¬è®­ç»ƒçš„ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹æµ‹è¯•æ—¶é—´è®­ç»ƒæŠ€æœ¯ï¼Œåˆ©ç”¨è®­ç»ƒå¥½çš„è‡ªå®šä¹‰æ¨¡å‹åœ¨ç»„åˆåæ›´æ–°å‚æ•°ã€‚æˆ‘ä»¬è¿›è¡Œäº†è¯¦ç»†çš„å®éªŒæ¥éªŒè¯æ‰€æå‡ºæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å®šæ€§å’Œå®šé‡è¯„ä¼°ä¸­éƒ½ä¼˜äºå‡ ç§æœ€æ–°æŠ€æœ¯ä½œå“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15646v1">PDF</a> Accepted in AAAI 2025</p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬æè¿°äº†å½“å‰æ–‡æœ¬åˆ°è§†é¢‘ï¼ˆT2Vï¼‰æ‰©æ•£æ¨¡å‹åˆ©ç”¨å¤§è§„æ¨¡é¢„è®­ç»ƒçš„æ–‡æœ¬-è§†é¢‘å¯¹ç”Ÿæˆé«˜è´¨é‡è§†é¢‘çš„æƒ…å†µã€‚æ–‡ç« è¿˜ä»‹ç»äº†ä½¿ç”¨å‚è€ƒå›¾åƒæˆ–è§†é¢‘çš„å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•ï¼ˆå¦‚LoRAï¼‰æ¥ç”Ÿæˆç‰¹å®šæ¦‚å¿µçš„è§†é¢‘ã€‚ç„¶è€Œï¼Œä»å¤šä¸ªå‚è€ƒä¸­è®­ç»ƒçš„æ¦‚å¿µåˆå¹¶åˆ°å•ä¸ªç½‘ç»œä¸­ä¼šäº§ç”Ÿæ˜æ˜¾çš„äººå·¥åˆ¶å“ã€‚ä¸ºæ­¤ï¼Œæ–‡ç« æå‡ºäº†CustomTTTæ–¹æ³•ï¼Œèƒ½å¤Ÿè½»æ¾åœ°å°†ç»™å®šè§†é¢‘çš„å¤–è§‚å’Œè¿åŠ¨ç»“åˆèµ·æ¥ã€‚ç ”ç©¶å‘ç°LoRAåªå¯¹ç‰¹å®šå±‚è¿›è¡Œå¤–è§‚å’Œè¿åŠ¨å®šåˆ¶ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°å‹æµ‹è¯•æ—¶é—´è®­ç»ƒæŠ€æœ¯ï¼Œåœ¨ç»“åˆè®­ç»ƒå¥½çš„å®šåˆ¶æ¨¡å‹åæ›´æ–°å‚æ•°ã€‚å®éªŒè¯æ˜è¯¥æ–¹æ³•åœ¨å®šæ€§å’Œå®šé‡è¯„ä¼°ä¸Šå‡ä¼˜äºå‡ ç§æœ€æ–°æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰æ–‡æœ¬åˆ°è§†é¢‘ï¼ˆT2Vï¼‰æ‰©æ•£æ¨¡å‹èƒ½å¤Ÿåˆ©ç”¨å¤§è§„æ¨¡é¢„è®­ç»ƒç”Ÿæˆé«˜è´¨é‡è§†é¢‘ã€‚</li>
<li>å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•ï¼ˆå¦‚LoRAï¼‰å¯ä»¥ä»å‚è€ƒå›¾åƒæˆ–è§†é¢‘ç”Ÿæˆç‰¹å®šæ¦‚å¿µçš„è§†é¢‘ã€‚</li>
<li>åˆå¹¶æ¥è‡ªä¸åŒå‚è€ƒçš„è®­ç»ƒæ¦‚å¿µåˆ°å•ä¸€ç½‘ç»œä¸­ä¼šäº§ç”Ÿæ˜æ˜¾çš„äººå·¥åˆ¶å“ã€‚</li>
<li>CustomTTTæ–¹æ³•èƒ½å¤Ÿè½»æ¾ç»“åˆç»™å®šè§†é¢‘çš„å¤–è§‚å’Œè¿åŠ¨ã€‚</li>
<li>LoRAä¸»è¦ç”¨äºç‰¹å®šå±‚çš„å¤–è§‚å’Œè¿åŠ¨å®šåˆ¶ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹æµ‹è¯•æ—¶é—´è®­ç»ƒæŠ€æœ¯ï¼Œåœ¨ç»“åˆè®­ç»ƒå¥½çš„å®šåˆ¶æ¨¡å‹åæ›´æ–°å‚æ•°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15646">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-abe541889a8174127540ee110cb615c2.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-126d05fe6292ac9faa0c7d0948b68b7e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-05d39e1b0b64ddbb836a2c2dbef06f12.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-bf8d56f0056223d1d4f5b911258ec96b.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-46407ef1002b5a9761c561f4242ebe0f.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="ChangeDiff-A-Multi-Temporal-Change-Detection-Data-Generator-with-Flexible-Text-Prompts-via-Diffusion-Model"><a href="#ChangeDiff-A-Multi-Temporal-Change-Detection-Data-Generator-with-Flexible-Text-Prompts-via-Diffusion-Model" class="headerlink" title="ChangeDiff: A Multi-Temporal Change Detection Data Generator with   Flexible Text Prompts via Diffusion Model"></a>ChangeDiff: A Multi-Temporal Change Detection Data Generator with   Flexible Text Prompts via Diffusion Model</h2><p><strong>Authors:Qi Zang, Jiayi Yang, Shuang Wang, Dong Zhao, Wenjun Yi, Zhun Zhong</strong></p>
<p>Data-driven deep learning models have enabled tremendous progress in change detection (CD) with the support of pixel-level annotations. However, collecting diverse data and manually annotating them is costly, laborious, and knowledge-intensive. Existing generative methods for CD data synthesis show competitive potential in addressing this issue but still face the following limitations: 1) difficulty in flexibly controlling change events, 2) dependence on additional data to train the data generators, 3) focus on specific change detection tasks. To this end, this paper focuses on the semantic CD (SCD) task and develops a multi-temporal SCD data generator ChangeDiff by exploring powerful diffusion models. ChangeDiff innovatively generates change data in two steps: first, it uses text prompts and a text-to-layout (T2L) model to create continuous layouts, and then it employs layout-to-image (L2I) to convert these layouts into images. Specifically, we propose multi-class distribution-guided text prompts (MCDG-TP), allowing for layouts to be generated flexibly through controllable classes and their corresponding ratios. Subsequently, to generalize the T2L model to the proposed MCDG-TP, a class distribution refinement loss is further designed as training supervision. %For the former, a multi-classdistribution-guided text prompt (MCDG-TP) is proposed to complement via controllable classes and ratios. To generalize the text-to-image diffusion model to the proposed MCDG-TP, a class distribution refinement loss is designed as training supervision. For the latter, MCDG-TP in three modes is proposed to synthesize new layout masks from various texts. Our generated data shows significant progress in temporal continuity, spatial diversity, and quality realism, empowering change detectors with accuracy and transferability. The code is available at <a target="_blank" rel="noopener" href="https://github.com/DZhaoXd/ChangeDiff">https://github.com/DZhaoXd/ChangeDiff</a> </p>
<blockquote>
<p>æ•°æ®é©±åŠ¨çš„æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨å˜åŒ–æ£€æµ‹ï¼ˆCDï¼‰æ–¹é¢å–å¾—äº†å·¨å¤§çš„è¿›æ­¥ï¼Œè¿™å¾—ç›Šäºåƒç´ çº§æ³¨é‡Šçš„æ”¯æŒã€‚ç„¶è€Œï¼Œæ”¶é›†å¤šæ ·åŒ–çš„æ•°æ®å¹¶å¯¹å…¶è¿›è¡Œæ‰‹åŠ¨æ ‡æ³¨æ˜¯æˆæœ¬é«˜æ˜‚ã€è€—æ—¶ä¸”çŸ¥è¯†å¯†é›†å‹çš„ã€‚ç°æœ‰çš„CDæ•°æ®åˆæˆç”Ÿæˆæ–¹æ³•å…·æœ‰ç«äº‰åŠ›ï¼Œå¹¶åœ¨è§£å†³æ­¤é—®é¢˜æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å®ƒä»¬ä»é¢ä¸´ä»¥ä¸‹å±€é™æ€§ï¼š1ï¼‰éš¾ä»¥çµæ´»æ§åˆ¶å˜åŒ–äº‹ä»¶ï¼›2ï¼‰ä¾èµ–äºå…¶ä»–æ•°æ®æ¥è®­ç»ƒæ•°æ®ç”Ÿæˆå™¨ï¼›3ï¼‰ä¸“æ³¨äºç‰¹å®šçš„å˜åŒ–æ£€æµ‹ä»»åŠ¡ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡ä¸“æ³¨äºè¯­ä¹‰CDï¼ˆSCDï¼‰ä»»åŠ¡ï¼Œå¹¶é€šè¿‡æ¢ç´¢å¼ºå¤§çš„æ‰©æ•£æ¨¡å‹ï¼Œå¼€å‘äº†ä¸€ç§å¤šæ—¶é—´å°ºåº¦SCDæ•°æ®ç”Ÿæˆå™¨ChangeDiffã€‚ChangeDiffåˆ›æ–°æ€§åœ°é€šè¿‡ä¸¤ä¸ªæ­¥éª¤ç”Ÿæˆå˜åŒ–æ•°æ®ï¼šé¦–å…ˆï¼Œå®ƒä½¿ç”¨æ–‡æœ¬æç¤ºå’Œæ–‡æœ¬åˆ°å¸ƒå±€ï¼ˆT2Lï¼‰æ¨¡å‹æ¥åˆ›å»ºè¿ç»­å¸ƒå±€ï¼Œç„¶åé‡‡ç”¨å¸ƒå±€åˆ°å›¾åƒï¼ˆL2Iï¼‰å°†è¿™äº›å¸ƒå±€è½¬æ¢ä¸ºå›¾åƒã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†å¤šç±»åˆ†å¸ƒå¼•å¯¼æ–‡æœ¬æç¤ºï¼ˆMCDG-TPï¼‰ï¼Œé€šè¿‡å¯æ§ç±»åˆ«åŠå…¶ç›¸åº”æ¯”ä¾‹ï¼Œä½¿å¸ƒå±€ç”Ÿæˆæ›´åŠ çµæ´»ã€‚éšåï¼Œä¸ºäº†å°†T2Læ¨¡å‹æ¨å¹¿åˆ°æå‡ºçš„MCDG-TPï¼Œè¿›ä¸€æ­¥è®¾è®¡äº†ç±»åˆ†å¸ƒç»†åŒ–æŸå¤±ä½œä¸ºè®­ç»ƒç›‘ç£ã€‚é’ˆå¯¹å‰è€…ï¼Œæå‡ºäº†å¤šç±»åˆ†å¸ƒå¼•å¯¼æ–‡æœ¬æç¤ºï¼ˆMCDG-TPï¼‰è¿›è¡Œè¡¥å……ï¼Œé€šè¿‡å¯æ§ç±»åˆ«å’Œæ¯”ä¾‹å®ç°ã€‚ä¸ºäº†å°†æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹æ¨å¹¿åˆ°æå‡ºçš„MCDG-TPï¼Œè®¾è®¡äº†ç±»åˆ†å¸ƒç»†åŒ–æŸå¤±ä½œä¸ºè®­ç»ƒç›‘ç£ã€‚é’ˆå¯¹åè€…ï¼Œæå‡ºäº†ä¸‰ç§æ¨¡å¼ä¸‹çš„MCDG-TPï¼Œç”¨äºæ ¹æ®å„ç§æ–‡æœ¬åˆæˆæ–°çš„å¸ƒå±€æ©è†œã€‚æˆ‘ä»¬ç”Ÿæˆçš„æ•°æ®åœ¨æ—¶åºè¿ç»­æ€§ã€ç©ºé—´å¤šæ ·æ€§å’Œè´¨é‡ç°å®æ€§æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä¸ºæé«˜å˜åŒ–æ£€æµ‹å™¨çš„å‡†ç¡®æ€§å’Œå¯è¿ç§»æ€§æä¾›äº†åŠ¨åŠ›ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/DZhaoXd/ChangeDiff">https://github.com/DZhaoXd/ChangeDiff</a>æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15541v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å…³æ³¨è¯­ä¹‰å˜åŒ–æ£€æµ‹ï¼ˆSCDï¼‰ä»»åŠ¡ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹å¼€å‘äº†ä¸€ç§å¤šæ—¶æ€SCDæ•°æ®ç”Ÿæˆå™¨ChangeDiffã€‚ChangeDiffé€šè¿‡ä¸¤æ­¥ç”Ÿæˆå˜åŒ–æ•°æ®ï¼šé¦–å…ˆä½¿ç”¨æ–‡æœ¬æç¤ºå’Œæ–‡æœ¬åˆ°å¸ƒå±€ï¼ˆT2Lï¼‰æ¨¡å‹åˆ›å»ºè¿ç»­å¸ƒå±€ï¼Œç„¶ååˆ©ç”¨å¸ƒå±€åˆ°å›¾åƒï¼ˆL2Iï¼‰å°†è¿™äº›å¸ƒå±€è½¬æ¢ä¸ºå›¾åƒã€‚ä¸ºè§£å†³ç°æœ‰ç”Ÿæˆæ–¹æ³•çš„å±€é™æ€§ï¼Œæå‡ºäº†å¤šç±»åˆ†å¸ƒå¼•å¯¼æ–‡æœ¬æç¤ºï¼ˆMCDG-TPï¼‰ï¼Œé€šè¿‡å¯æ§ç±»åˆ«å’Œç›¸åº”æ¯”ä¾‹å®ç°çµæ´»å¸ƒå±€ç”Ÿæˆã€‚æ­¤å¤–ï¼Œä¸ºå°†T2Læ¨¡å‹æ¨å¹¿åˆ°MCDG-TPï¼Œè®¾è®¡äº†ä¸€ç§ç±»åˆ†å¸ƒç»†åŒ–æŸå¤±ä½œä¸ºè®­ç»ƒç›‘ç£ã€‚è¯¥ç”Ÿæˆå™¨åœ¨æ—¶åºè¿ç»­æ€§ã€ç©ºé—´å¤šæ ·æ€§å’Œè´¨é‡ç°å®æ€§æ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ï¼Œå¢å¼ºäº†å˜åŒ–æ£€æµ‹å™¨çš„å‡†ç¡®æ€§å’Œå¯è¿ç§»æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ•°æ®é©±åŠ¨çš„æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨å˜åŒ–æ£€æµ‹ä¸­å–å¾—å·¨å¤§è¿›å±•ï¼Œä½†æ•°æ®æ”¶é›†å’Œæ‰‹åŠ¨æ ‡æ³¨æˆæœ¬é«˜æ˜‚ã€‚</li>
<li>ç°æœ‰ç”Ÿæˆæ–¹æ³•åœ¨å˜åŒ–æ£€æµ‹æ•°æ®åˆæˆä¸­å…·æœ‰ç«äº‰æ½œåŠ›ï¼Œä½†ä»å­˜åœ¨æ§åˆ¶å˜åŒ–äº‹ä»¶éš¾ã€ä¾èµ–é¢å¤–æ•°æ®è®­ç»ƒæ•°æ®ç”Ÿæˆå™¨ç­‰å±€é™ã€‚</li>
<li>æœ¬æ–‡å…³æ³¨è¯­ä¹‰å˜åŒ–æ£€æµ‹ï¼ˆSCDï¼‰ä»»åŠ¡ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹å¼€å‘å¤šæ—¶æ€SCDæ•°æ®ç”Ÿæˆå™¨ChangeDiffã€‚</li>
<li>ChangeDiffé€šè¿‡ä¸¤æ­¥ç”Ÿæˆå˜åŒ–æ•°æ®ï¼šåˆ›å»ºè¿ç»­å¸ƒå±€å¹¶è½¬æ¢ä¸ºå›¾åƒã€‚</li>
<li>å¼•å…¥å¤šç±»åˆ†å¸ƒå¼•å¯¼æ–‡æœ¬æç¤ºï¼ˆMCDG-TPï¼‰ï¼Œå®ç°çµæ´»å¸ƒå±€ç”Ÿæˆã€‚</li>
<li>è®¾è®¡ç±»åˆ†å¸ƒç»†åŒ–æŸå¤±ä½œä¸ºè®­ç»ƒç›‘ç£ï¼Œä»¥å°†T2Læ¨¡å‹æ¨å¹¿åˆ°MCDG-TPã€‚</li>
<li>ç”Ÿæˆçš„æ•°æ®åœ¨æ—¶åºè¿ç»­æ€§ã€ç©ºé—´å¤šæ ·æ€§å’Œè´¨é‡ç°å®æ€§æ–¹é¢è¡¨ç°æ˜¾è‘—ï¼Œå¢å¼ºäº†å˜åŒ–æ£€æµ‹å™¨çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15541">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-724baf628bfff423d866535a8669e330.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-90df6c8d62c1e752b9eb9f56a7a9628a.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-2d13d8ec85436aaf35a938e897c5a1ab.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-6818ec1fc1f0c314c9db8727a1d9e950.jpg" align="middle">
<img src="/medias/loading.gif" data-original="D:\MyBlog\AutoFX\arxiv\2024-12-24\./crop_Diffusion Models/2412.15541v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Stylish-and-Functional-Guided-Interpolation-Subject-to-Physical-Constraints"><a href="#Stylish-and-Functional-Guided-Interpolation-Subject-to-Physical-Constraints" class="headerlink" title="Stylish and Functional: Guided Interpolation Subject to Physical   Constraints"></a>Stylish and Functional: Guided Interpolation Subject to Physical   Constraints</h2><p><strong>Authors:Yan-Ying Chen, Nikos Arechiga, Chenyang Yuan, Matthew Hong, Matt Klenk, Charlene Wu</strong></p>
<p>Generative AI is revolutionizing engineering design practices by enabling rapid prototyping and manipulation of designs. One example of design manipulation involves taking two reference design images and using them as prompts to generate a design image that combines aspects of both. Real engineering designs have physical constraints and functional requirements in addition to aesthetic design considerations. Internet-scale foundation models commonly used for image generation, however, are unable to take these physical constraints and functional requirements into consideration as part of the generation process. We consider the problem of generating a design inspired by two input designs, and propose a zero-shot framework toward enforcing physical, functional requirements over the generation process by leveraging a pretrained diffusion model as the backbone. As a case study, we consider the example of rotational symmetry in generation of wheel designs. Automotive wheels are required to be rotationally symmetric for physical stability. We formulate the requirement of rotational symmetry by the use of a symmetrizer, and we use this symmetrizer to guide the diffusion process towards symmetric wheel generations. Our experimental results find that the proposed approach makes generated interpolations with higher realism than methods in related work, as evaluated by Fr&#39;echet inception distance (FID). We also find that our approach generates designs that more closely satisfy physical and functional requirements than generating without the symmetry guidance. </p>
<blockquote>
<p>ç”Ÿæˆå¼äººå·¥æ™ºèƒ½æ­£åœ¨é€šè¿‡å®ç°è®¾è®¡çš„å¿«é€ŸåŸå‹åˆ¶ä½œå’Œæ“ä½œï¼Œé©æ–°å·¥ç¨‹è®¾è®¡å®è·µã€‚è®¾è®¡æ“ä½œçš„ä¸€ä¸ªä¾‹å­æ˜¯ï¼Œä½¿ç”¨ä¸¤å¼ å‚è€ƒè®¾è®¡å›¾åƒä½œä¸ºæç¤ºï¼Œç”Ÿæˆä¸€å¼ ç»“åˆäº†ä¸¤è€…ç‰¹ç‚¹çš„å›¾åƒã€‚åœ¨å®é™…çš„å·¥ç¨‹è®¾è®¡è¿‡ç¨‹ä¸­ï¼Œé™¤äº†å®¡ç¾è®¾è®¡è€ƒè™‘å› ç´ ä¹‹å¤–ï¼Œè¿˜å­˜åœ¨ç‰©ç†çº¦æŸå’ŒåŠŸèƒ½è¦æ±‚ã€‚ç„¶è€Œï¼Œå¸¸ç”¨äºå›¾åƒç”Ÿæˆçš„äº’è”ç½‘è§„æ¨¡åŸºç¡€æ¨¡å‹å´æ— æ³•å°†è¿™äº›ç‰©ç†çº¦æŸå’ŒåŠŸèƒ½è¦æ±‚çº³å…¥ç”Ÿæˆè¿‡ç¨‹çš„è€ƒé‡ä¹‹ä¸­ã€‚æˆ‘ä»¬è€ƒè™‘äº†ç”±ä¸¤ä¸ªè¾“å…¥è®¾è®¡ç”Ÿæˆè®¾è®¡çš„é—®é¢˜ï¼Œå¹¶æå‡ºäº†ä¸€ç§é›¶æ ·æœ¬æ¡†æ¶ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ä½œä¸ºéª¨å¹²æ¥å¼ºåˆ¶åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­å®æ–½ç‰©ç†å’ŒåŠŸèƒ½è¦æ±‚ã€‚ä½œä¸ºæ¡ˆä¾‹ç ”ç©¶ï¼Œæˆ‘ä»¬è€ƒè™‘äº†ç”Ÿæˆè½®æ¯‚è®¾è®¡ä¸­çš„æ—‹è½¬å¯¹ç§°æ€§é—®é¢˜ã€‚æ±½è½¦è½®æ¯‚éœ€è¦æ—‹è½¬å¯¹ç§°ä»¥ä¿è¯ç‰©ç†ç¨³å®šæ€§ã€‚æˆ‘ä»¬é€šè¿‡ä½¿ç”¨å¯¹ç§°åŒ–å·¥å…·æ¥åˆ¶å®šæ—‹è½¬å¯¹ç§°æ€§çš„è¦æ±‚ï¼Œå¹¶åˆ©ç”¨å®ƒæ¥å¼•å¯¼æ‰©æ•£è¿‡ç¨‹äº§ç”Ÿå¯¹ç§°çš„è½®æ¯‚ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼Œç›¸è¾ƒäºç›¸å…³å·¥ä½œä¸­æå‡ºçš„æ–¹æ³•ï¼Œé‡‡ç”¨æœ¬æ–¹æ³•ç”Ÿæˆçš„æ’å€¼ç»“æœå…·æœ‰æ›´é«˜çš„é€¼çœŸæ€§ï¼Œå¹¶é€šè¿‡FrÃ©chet inceptionè·ç¦»ï¼ˆFIDï¼‰è¿›è¡Œäº†è¯„ä¼°ã€‚æˆ‘ä»¬è¿˜å‘ç°ï¼Œä¸æ²¡æœ‰å¯¹ç§°æŒ‡å¯¼çš„ç”Ÿæˆç›¸æ¯”ï¼Œæœ¬æ–¹æ³•ç”Ÿæˆçš„è®¾è®¡æ›´èƒ½æ»¡è¶³ç‰©ç†å’ŒåŠŸèƒ½è¦æ±‚ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15507v1">PDF</a> Accepted by Foundation Models for Science Workshop, 38th Conference   on Neural Information Processing Systems (NeurIPS 2024)</p>
<p><strong>Summary</strong></p>
<p>åŸºäºç”Ÿæˆå¼äººå·¥æ™ºèƒ½çš„èƒ½åŠ›ï¼Œè®¾è®¡å®è·µæ­£åœ¨ç»å†ä¸€åœºé©å‘½æ€§çš„å˜é©ï¼ŒåŒ…æ‹¬å¿«é€ŸåŸå‹è®¾è®¡å’Œè®¾è®¡æ“æ§ã€‚ä½†ç°æœ‰çš„äº’è”ç½‘è§„æ¨¡åŸºç¡€æ¨¡å‹æ— æ³•è€ƒè™‘ç‰©ç†çº¦æŸå’ŒåŠŸèƒ½æ€§éœ€æ±‚ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºé¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„é›¶æ ·æœ¬æ¡†æ¶ï¼Œé€šè¿‡åˆ©ç”¨å¯¹ç§°å™¨æ¥æŒ‡å¯¼æ‰©æ•£è¿‡ç¨‹ï¼Œæ»¡è¶³ç‰©ç†ç¨³å®šæ€§å’ŒåŠŸèƒ½æ€§éœ€æ±‚ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ç”Ÿæˆçš„æ’å€¼å›¾åƒæ›´å…·çœŸå®æ„Ÿï¼Œä¸”ç”Ÿæˆçš„è®¾è®¡æ›´æ»¡è¶³ç‰©ç†å’ŒåŠŸèƒ½æ€§éœ€æ±‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆå¼äººå·¥æ™ºèƒ½æ­£åœ¨æ”¹å˜å·¥ç¨‹è®¾è®¡çš„å®è·µæ–¹å¼ï¼Œå¯ä»¥å®ç°å¿«é€ŸåŸå‹è®¾è®¡å’Œè®¾è®¡æ“æ§ã€‚</li>
<li>å½“å‰äº’è”ç½‘è§„æ¨¡åŸºç¡€æ¨¡å‹åœ¨è®¾è®¡è¿‡ç¨‹ä¸­æ— æ³•è€ƒè™‘ç‰©ç†çº¦æŸå’ŒåŠŸèƒ½æ€§éœ€æ±‚ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºé¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„é›¶æ ·æœ¬æ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥å¯¹ç§°å™¨æ¥æ»¡è¶³ç‰©ç†ç¨³å®šæ€§å’ŒåŠŸèƒ½æ€§éœ€æ±‚ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ç”Ÿæˆçš„å›¾åƒæ›´å…·çœŸå®æ„Ÿã€‚</li>
<li>è¯¥æ–¹æ³•ç”Ÿæˆçš„è®¾è®¡æ›´èƒ½æ»¡è¶³ç‰©ç†å’ŒåŠŸèƒ½æ€§éœ€æ±‚ã€‚</li>
<li>ä»¥è½¦è½®è®¾è®¡ä¸ºä¾‹ï¼Œé€šè¿‡æ—‹è½¬å¯¹ç§°æ€§è¦æ±‚æ¥æŒ‡å¯¼æ‰©æ•£è¿‡ç¨‹ï¼Œç”Ÿæˆæ—‹è½¬å¯¹ç§°çš„è½¦è½®è®¾è®¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15507">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-7d4e57f6a08434d5dba8f57be9eee784.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-939659d07d0b8cd6026986172e984579.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-b01487bbdc2937051812537efc887df9.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-8acb1e84d2e935c4a53a41403cad9490.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-cb48128138c87ee00c6d349c9316c475.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-2740312f32dbf418902bb26ae139d017.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-5d27902402d9c28c82ef36162cf14deb.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="GCA-3D-Towards-Generalized-and-Consistent-Domain-Adaptation-of-3D-Generators"><a href="#GCA-3D-Towards-Generalized-and-Consistent-Domain-Adaptation-of-3D-Generators" class="headerlink" title="GCA-3D: Towards Generalized and Consistent Domain Adaptation of 3D   Generators"></a>GCA-3D: Towards Generalized and Consistent Domain Adaptation of 3D   Generators</h2><p><strong>Authors:Hengjia Li, Yang Liu, Yibo Zhao, Haoran Cheng, Yang Yang, Linxuan Xia, Zekai Luo, Qibo Qiu, Boxi Wu, Tu Zheng, Zheng Yang, Deng Cai</strong></p>
<p>Recently, 3D generative domain adaptation has emerged to adapt the pre-trained generator to other domains without collecting massive datasets and camera pose distributions. Typically, they leverage large-scale pre-trained text-to-image diffusion models to synthesize images for the target domain and then fine-tune the 3D model. However, they suffer from the tedious pipeline of data generation, which inevitably introduces pose bias between the source domain and synthetic dataset. Furthermore, they are not generalized to support one-shot image-guided domain adaptation, which is more challenging due to the more severe pose bias and additional identity bias introduced by the single image reference. To address these issues, we propose GCA-3D, a generalized and consistent 3D domain adaptation method without the intricate pipeline of data generation. Different from previous pipeline methods, we introduce multi-modal depth-aware score distillation sampling loss to efficiently adapt 3D generative models in a non-adversarial manner. This multi-modal loss enables GCA-3D in both text prompt and one-shot image prompt adaptation. Besides, it leverages per-instance depth maps from the volume rendering module to mitigate the overfitting problem and retain the diversity of results. To enhance the pose and identity consistency, we further propose a hierarchical spatial consistency loss to align the spatial structure between the generated images in the source and target domain. Experiments demonstrate that GCA-3D outperforms previous methods in terms of efficiency, generalization, pose accuracy, and identity consistency. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œ3Dç”Ÿæˆé¢†åŸŸé€‚åº”æŠ€æœ¯å‡ºç°ï¼Œç”¨äºå°†é¢„è®­ç»ƒçš„ç”Ÿæˆå™¨é€‚åº”åˆ°å…¶ä»–é¢†åŸŸï¼Œè€Œæ— éœ€æ”¶é›†å¤§è§„æ¨¡æ•°æ®é›†å’Œç›¸æœºå§¿æ€åˆ†å¸ƒã€‚é€šå¸¸ï¼Œå®ƒä»¬åˆ©ç”¨å¤§è§„æ¨¡é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹æ¥åˆæˆç›®æ ‡é¢†åŸŸçš„å›¾åƒï¼Œç„¶åå¯¹3Dæ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚ç„¶è€Œï¼Œå®ƒä»¬å­˜åœ¨æ•°æ®ç”Ÿæˆæµç¨‹ç¹ççš„é—®é¢˜ï¼Œè¿™ä¸å¯é¿å…åœ°ä¼šåœ¨æºåŸŸå’Œåˆæˆæ•°æ®é›†ä¹‹é—´å¼•å…¥å§¿æ€åå·®ã€‚æ­¤å¤–ï¼Œå®ƒä»¬å¹¶ä¸é€šç”¨ï¼Œæ— æ³•æ”¯æŒå•å¼ å›¾åƒå¼•å¯¼çš„é¢†åŸŸé€‚åº”ï¼Œç”±äºæ›´ä¸¥é‡çš„å§¿æ€åå·®å’Œç”±å•å¼ å›¾åƒå¼•å…¥çš„èº«ä»½åå·®ï¼Œè¿™ä½¿å¾—æ›´å…·æŒ‘æˆ˜æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†GCA-3Dï¼Œè¿™æ˜¯ä¸€ç§é€šç”¨ä¸”ä¸€è‡´çš„3Dé¢†åŸŸé€‚åº”æ–¹æ³•ï¼Œæ— éœ€å¤æ‚çš„æ•°æ®ç”Ÿæˆæµç¨‹ã€‚ä¸åŒäºä¹‹å‰çš„æ–¹æ³•ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¤šæ¨¡æ€æ·±åº¦æ„ŸçŸ¥åˆ†æ•°è’¸é¦é‡‡æ ·æŸå¤±ï¼Œä»¥éå¯¹æŠ—æ€§çš„æ–¹å¼æœ‰æ•ˆåœ°é€‚åº”3Dç”Ÿæˆæ¨¡å‹ã€‚è¿™ç§å¤šæ¨¡æ€æŸå¤±ä½¿GCA-3Dåœ¨æ–‡æœ¬æç¤ºå’Œå•å¼ å›¾åƒæç¤ºé€‚åº”ä¸­éƒ½èƒ½å‘æŒ¥ä½œç”¨ã€‚æ­¤å¤–ï¼Œå®ƒä»ä½“ç§¯æ¸²æŸ“æ¨¡å—åˆ©ç”¨æ¯ä¸ªå®ä¾‹çš„æ·±åº¦å›¾æ¥ç¼“è§£è¿‡æ‹Ÿåˆé—®é¢˜å¹¶ä¿æŒç»“æœçš„å¤šæ ·æ€§ã€‚ä¸ºäº†æé«˜å§¿æ€å’Œèº«ä»½çš„ä¸€è‡´æ€§ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†åˆ†å±‚ç©ºé—´ä¸€è‡´æ€§æŸå¤±ï¼Œä»¥å¯¹é½æºåŸŸå’Œç›®æ ‡åŸŸç”Ÿæˆå›¾åƒçš„ç©ºé—´ç»“æ„ã€‚å®éªŒè¡¨æ˜ï¼ŒGCA-3Dåœ¨æ•ˆç‡ã€é€šç”¨æ€§ã€å§¿æ€å‡†ç¡®æ€§å’Œèº«ä»½ä¸€è‡´æ€§æ–¹é¢ä¼˜äºä»¥å‰çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15491v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹3Dç”ŸæˆåŸŸé€‚åº”çš„æ–°æ–¹æ³•GCA-3Dã€‚è¯¥æ–¹æ³•é€šè¿‡å¼•å…¥å¤šæ¨¡æ€æ·±åº¦æ„ŸçŸ¥åˆ†æ•°è’¸é¦é‡‡æ ·æŸå¤±ï¼Œèƒ½å¤Ÿé«˜æ•ˆã€éå¯¹æŠ—å¼åœ°é€‚åº”3Dç”Ÿæˆæ¨¡å‹ã€‚ä¸ä¼ ç»Ÿçš„é€šè¿‡æ•°æ®ç”Ÿæˆçš„æ–¹æ³•ç›¸æ¯”ï¼ŒGCA-3Dé¿å…äº†å¤æ‚çš„æ•°æ®ç”Ÿæˆæµç¨‹ï¼Œå¹¶è§£å†³äº†å§¿åŠ¿åå·®å’Œèº«ä»½åå·®çš„é—®é¢˜ã€‚æ­¤å¤–ï¼Œå®ƒåˆ©ç”¨ä½“ç§¯æ¸²æŸ“æ¨¡å—çš„æ·±åº¦å›¾å‡è½»è¿‡æ‹Ÿåˆé—®é¢˜å¹¶ä¿ç•™ç»“æœçš„å¤šæ ·æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒGCA-3Dåœ¨æ•ˆç‡ã€æ³›åŒ–èƒ½åŠ›ã€å§¿åŠ¿å‡†ç¡®æ€§å’Œèº«ä»½ä¸€è‡´æ€§æ–¹é¢ä¼˜äºä»¥å‰çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>3Dç”ŸæˆåŸŸé€‚åº”æ–¹æ³•æ—¨åœ¨å°†é¢„è®­ç»ƒçš„ç”Ÿæˆå™¨é€‚åº”åˆ°å…¶ä»–é¢†åŸŸï¼Œæ— éœ€æ”¶é›†å¤§è§„æ¨¡æ•°æ®é›†å’Œç›¸æœºå§¿æ€åˆ†å¸ƒã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•åˆ©ç”¨å¤§è§„æ¨¡é¢„è®­ç»ƒæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹è¿›è¡Œå›¾åƒåˆæˆï¼Œç„¶åå¯¹3Dæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä½†å­˜åœ¨æ•°æ®ç”Ÿæˆæµç¨‹ç¹ççš„é—®é¢˜ã€‚</li>
<li>GCA-3Dæ˜¯ä¸€ç§æ–°å‹çš„3DåŸŸé€‚åº”æ–¹æ³•ï¼Œé¿å…äº†å¤æ‚çš„æ•°æ®ç”Ÿæˆæµç¨‹ã€‚</li>
<li>GCA-3Dé€šè¿‡å¼•å…¥å¤šæ¨¡æ€æ·±åº¦æ„ŸçŸ¥åˆ†æ•°è’¸é¦é‡‡æ ·æŸå¤±ï¼Œå®ç°äº†é«˜æ•ˆã€éå¯¹æŠ—å¼çš„3Dæ¨¡å‹é€‚åº”ã€‚</li>
<li>GCA-3Dæ”¯æŒæ–‡æœ¬æç¤ºå’Œå•å¼ å›¾åƒæç¤ºé€‚åº”ï¼Œè§£å†³äº†å§¿åŠ¿åå·®å’Œèº«ä»½åå·®çš„é—®é¢˜ã€‚</li>
<li>GCA-3Dåˆ©ç”¨ä½“ç§¯æ¸²æŸ“æ¨¡å—çš„æ·±åº¦å›¾æ¥å‡è½»è¿‡æ‹Ÿåˆé—®é¢˜ï¼Œå¹¶ä¿ç•™ç»“æœçš„å¤šæ ·æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15491">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-d4bf1ebc33c5695a87b469a44c3ec13c.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-bb725e035c1dbaa553de8cd8d7d610f9.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-f7e3db7780a192bb4e12e0e1c57f72cf.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-b6719a6af5cd8d041c1de743dcb13cd0.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-5e9e097f5e504a741c794b8751142624.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-e444cd4e72af35ab1a8afd734bdf9574.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-db91e0e230e8f90feff4a49b7cb8c6a0.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Dataset-Augmentation-by-Mixing-Visual-Concepts"><a href="#Dataset-Augmentation-by-Mixing-Visual-Concepts" class="headerlink" title="Dataset Augmentation by Mixing Visual Concepts"></a>Dataset Augmentation by Mixing Visual Concepts</h2><p><strong>Authors:Abdullah Al Rahat, Hemanth Venkateswara</strong></p>
<p>This paper proposes a dataset augmentation method by fine-tuning pre-trained diffusion models. Generating images using a pre-trained diffusion model with textual conditioning often results in domain discrepancy between real data and generated images. We propose a fine-tuning approach where we adapt the diffusion model by conditioning it with real images and novel text embeddings. We introduce a unique procedure called Mixing Visual Concepts (MVC) where we create novel text embeddings from image captions. The MVC enables us to generate multiple images which are diverse and yet similar to the real data enabling us to perform effective dataset augmentation. We perform comprehensive qualitative and quantitative evaluations with the proposed dataset augmentation approach showcasing both coarse-grained and finegrained changes in generated images. Our approach outperforms state-of-the-art augmentation techniques on benchmark classification tasks. </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§é€šè¿‡å¾®è°ƒé¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹æ¥è¿›è¡Œæ•°æ®é›†å¢å¼ºæ–¹æ³•ã€‚ä½¿ç”¨å¸¦æœ‰æ–‡æœ¬æ¡ä»¶çš„é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ç”Ÿæˆå›¾åƒå¾€å¾€ä¼šå¯¼è‡´çœŸå®æ•°æ®ä¸ç”Ÿæˆå›¾åƒä¹‹é—´çš„é¢†åŸŸå·®å¼‚ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§å¾®è°ƒæ–¹æ³•ï¼Œé€šè¿‡ä»¥çœŸå®å›¾åƒå’Œæ–°å‹æ–‡æœ¬åµŒå…¥ä¸ºæ¡ä»¶æ¥é€‚åº”æ‰©æ•£æ¨¡å‹ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ç‹¬ç‰¹çš„ç¨‹åºï¼Œç§°ä¸ºæ··åˆè§†è§‰æ¦‚å¿µï¼ˆMVCï¼‰ï¼Œæˆ‘ä»¬ä»å›¾åƒå­—å¹•ä¸­åˆ›å»ºæ–°å‹çš„æ–‡æœ¬åµŒå…¥ã€‚MVCä½¿æˆ‘ä»¬èƒ½å¤Ÿç”Ÿæˆå¤šç§ä¸çœŸå®æ•°æ®å¤šæ ·ä¸”ç›¸ä¼¼çš„å›¾åƒï¼Œä»è€Œèƒ½å¤Ÿæ‰§è¡Œæœ‰æ•ˆçš„æ•°æ®é›†å¢å¼ºã€‚æˆ‘ä»¬è¿›è¡Œäº†å…¨é¢çš„å®šæ€§å’Œå®šé‡è¯„ä¼°ï¼Œå¯¹æ‰€æå‡ºçš„æ•°æ®é›†å¢å¼ºæ–¹æ³•å±•ç¤ºäº†ç”Ÿæˆå›¾åƒçš„ç²—ç»†ç²’åº¦å˜åŒ–ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨åŸºå‡†åˆ†ç±»ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºæœ€å…ˆè¿›çš„å¢å¼ºæŠ€æœ¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15358v1">PDF</a> Accepted at WACV 2025 main conference</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§é€šè¿‡å¾®è°ƒé¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹æ¥è¿›è¡Œæ•°æ®é›†å¢å¼ºæ–¹æ³•ã€‚ä½¿ç”¨é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹æ ¹æ®æ–‡æœ¬æ¡ä»¶ç”Ÿæˆå›¾åƒå¾€å¾€ä¼šå¯¼è‡´çœŸå®æ•°æ®ä¸ç”Ÿæˆå›¾åƒä¹‹é—´çš„é¢†åŸŸå·®å¼‚ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§å¾®è°ƒæ–¹æ³•ï¼Œé€šè¿‡ä»¥çœŸå®å›¾åƒå’Œæ–°å‹æ–‡æœ¬åµŒå…¥ä¸ºæ¡ä»¶æ¥é€‚åº”æ‰©æ•£æ¨¡å‹ã€‚å¼•å…¥äº†ä¸€ç§åä¸ºæ··åˆè§†è§‰æ¦‚å¿µï¼ˆMVCï¼‰çš„ç‹¬ç‰¹ç¨‹åºï¼Œé€šè¿‡å›¾åƒæ ‡é¢˜åˆ›å»ºæ–°å‹æ–‡æœ¬åµŒå…¥ã€‚MVCèƒ½å¤Ÿç”Ÿæˆå¤šä¸ªæ—¢å¤šæ ·åˆç±»ä¼¼äºçœŸå®æ•°æ®çš„å›¾åƒï¼Œä»è€Œå®ç°äº†æœ‰æ•ˆçš„æ•°æ®é›†å¢å¼ºã€‚é€šè¿‡ç»¼åˆçš„å®šæ€§å’Œå®šé‡è¯„ä¼°ï¼Œå±•ç¤ºäº†æ‰€æå‡ºçš„æ•°æ®é›†å¢å¼ºæ–¹æ³•åœ¨ç”Ÿæˆå›¾åƒä¸­çš„ç²—ç²’åº¦å’Œç»†ç²’åº¦å˜åŒ–ï¼Œå¹¶åœ¨åŸºå‡†åˆ†ç±»ä»»åŠ¡ä¸Šä¼˜äºæœ€æ–°çš„å¢å¼ºæŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºå¾®è°ƒé¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„æ•°æ®é›†å¢å¼ºæ–¹æ³•ã€‚</li>
<li>ç”Ÿæˆå›¾åƒä¸çœŸå®æ•°æ®é—´å­˜åœ¨é¢†åŸŸå·®å¼‚ã€‚</li>
<li>è®ºæ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºâ€œæ··åˆè§†è§‰æ¦‚å¿µï¼ˆMVCï¼‰â€çš„ç‹¬ç‰¹ç¨‹åºï¼Œç”¨äºåˆ›å»ºæ–°å‹æ–‡æœ¬åµŒå…¥ã€‚</li>
<li>MVCèƒ½å¤Ÿç”Ÿæˆå¤šæ ·ä¸”ç±»ä¼¼äºçœŸå®æ•°æ®çš„å›¾åƒï¼Œå®ç°æœ‰æ•ˆçš„æ•°æ®é›†å¢å¼ºã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡ç»¼åˆçš„å®šæ€§å’Œå®šé‡è¯„ä¼°ï¼Œå±•ç¤ºäº†åœ¨ç”Ÿæˆå›¾åƒä¸­çš„ç²—ç²’åº¦å’Œç»†ç²’åº¦å˜åŒ–ã€‚</li>
<li>æ‰€æå‡ºçš„æ–¹æ³•åœ¨åŸºå‡†åˆ†ç±»ä»»åŠ¡ä¸Šä¼˜äºå…¶ä»–æœ€æ–°çš„å¢å¼ºæŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15358">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-da24878a80a1820c2774f5087da36a97.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-72b89a42c3279c5807cfb86bdf7978d0.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-1eac951449dd3f1cc80397abb470cb27.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-c8e0161e9bba2b63880629f0f7cb485d.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-c3f981321328dbbe65cff504ca6dcc1a.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-33a05446fb2bf4699e1f52edd76dff0f.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="The-Unreasonable-Effectiveness-of-Guidance-for-Diffusion-Models"><a href="#The-Unreasonable-Effectiveness-of-Guidance-for-Diffusion-Models" class="headerlink" title="The Unreasonable Effectiveness of Guidance for Diffusion Models"></a>The Unreasonable Effectiveness of Guidance for Diffusion Models</h2><p><strong>Authors:Tim Kaiser, Nikolas Adaloglou, Markus Kollmann</strong></p>
<p>Guidance is an error-correcting technique used to improve the perceptual quality of images generated by diffusion models. Typically, the correction is achieved by linear extrapolation, using an auxiliary diffusion model that has lower performance than the primary model. Using a 2D toy example, we show that it is highly beneficial when the auxiliary model exhibits similar errors as the primary one but stronger. We verify this finding in higher dimensions, where we show that competitive generative performance to state-of-the-art guidance methods can be achieved when the auxiliary model differs from the primary one only by having stronger weight regularization. As an independent contribution, we investigate whether upweighting long-range spatial dependencies improves visual fidelity. The result is a novel guidance method, which we call sliding window guidance (SWG), that guides the primary model with itself by constraining its receptive field. Intriguingly, SWG aligns better with human preferences than state-of-the-art guidance methods while requiring neither training, architectural modifications, nor class conditioning. The code will be released. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹ä¸­çš„æŒ‡å¯¼æŠ€æœ¯æ˜¯ä¸€ç§ç”¨äºæé«˜ç”Ÿæˆå›¾åƒæ„ŸçŸ¥è´¨é‡çš„çº é”™æŠ€æœ¯ã€‚é€šå¸¸ï¼Œçº é”™æ˜¯é€šè¿‡çº¿æ€§å¤–æ¨å®Œæˆçš„ï¼Œä½¿ç”¨ä¸€ä¸ªæ€§èƒ½ä½äºä¸»æ¨¡å‹çš„è¾…åŠ©æ‰©æ•£æ¨¡å‹ã€‚é€šè¿‡äºŒç»´ç©å…·ç¤ºä¾‹ï¼Œæˆ‘ä»¬å±•ç¤ºäº†å½“è¾…åŠ©æ¨¡å‹è¡¨ç°å‡ºä¸ä¸»æ¨¡å‹ç›¸ä¼¼çš„é”™è¯¯ä½†å¼ºåº¦æ›´å¤§æ—¶ï¼Œå®ƒçš„ç›Šå¤„éå¸¸å¤§ã€‚æˆ‘ä»¬åœ¨æ›´é«˜ç»´åº¦éªŒè¯äº†è¿™ä¸€å‘ç°ï¼Œåœ¨é‚£é‡Œæˆ‘ä»¬å±•ç¤ºï¼Œä»…é€šè¿‡åŠ å¼ºæƒé‡æ­£åˆ™åŒ–ï¼Œå½“è¾…åŠ©æ¨¡å‹ä¸ä¸»æ¨¡å‹ä¸åŒæ—¶ï¼Œå¯ä»¥å®ç°ä¸æœ€æ–°æŒ‡å¯¼æ–¹æ³•ç›¸ç«äº‰çš„ç”Ÿäº§æ€§èƒ½ã€‚ä½œä¸ºä¸€ä¸ªç‹¬ç«‹çš„è´¡çŒ®ï¼Œæˆ‘ä»¬è°ƒæŸ¥äº†åŠ å¼ºè¿œç¨‹ç©ºé—´ä¾èµ–æ˜¯å¦æé«˜è§†è§‰ä¿çœŸåº¦ã€‚ç»“æœæ˜¯æ–°å‹æŒ‡å¯¼æ–¹æ³•â€”â€”æˆ‘ä»¬ç§°ä¹‹ä¸ºæ»‘åŠ¨çª—å£æŒ‡å¯¼ï¼ˆSWGï¼‰ï¼Œå®ƒé€šè¿‡çº¦æŸä¸»æ¨¡å‹çš„æ„Ÿå—é‡æ¥æŒ‡å¯¼ä¸»æ¨¡å‹ã€‚æœ‰è¶£çš„æ˜¯ï¼ŒSWGæ›´ç¬¦åˆäººç±»åå¥½ï¼Œè€Œæ— éœ€è¿›è¡Œè®­ç»ƒã€æ¶æ„ä¿®æ”¹æˆ–ç±»åˆ«æ¡ä»¶è®¾ç½®ï¼ŒåŒæ—¶ä¼˜äºæœ€æ–°çš„æŒ‡å¯¼æ–¹æ³•ã€‚ä»£ç å°†å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.10257v2">PDF</a> Preprint. 30 pages, 19 figures in total, including appendix</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ‰©æ•£æ¨¡å‹ä¸­çš„Guidanceæ˜¯ä¸€ç§ç”¨äºæé«˜ç”Ÿæˆå›¾åƒæ„ŸçŸ¥è´¨é‡çš„é”™è¯¯æ ¡æ­£æŠ€æœ¯ã€‚é€šè¿‡çº¿æ€§å¤–æ¨ä½¿ç”¨è¾…åŠ©æ‰©æ•£æ¨¡å‹å®ç°æ ¡æ­£ï¼Œå…¶æ€§èƒ½é€šå¸¸ä½äºä¸»æ¨¡å‹ã€‚åœ¨äºŒç»´ç©å…·ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å‘ç°åœ¨è¾…åŠ©æ¨¡å‹å±•ç°å‡ºä¸ä¸»æ¨¡å‹ç›¸ä¼¼çš„é”™è¯¯ä½†å¼ºåº¦æ›´å¤§æ—¶ï¼Œå…¶æ•ˆç›Šæé«˜ã€‚æˆ‘ä»¬åœ¨æ›´é«˜ç»´åº¦éªŒè¯äº†è¿™ä¸€å‘ç°ï¼Œè¡¨æ˜ä»…é€šè¿‡æ›´å¼ºçš„æƒé‡æ­£åˆ™åŒ–ä½¿è¾…åŠ©æ¨¡å‹ä¸ä¸»æ¨¡å‹ä¸åŒï¼Œå°±èƒ½å®ç°ä¸æœ€æ–°æŒ‡å¯¼æ–¹æ³•ç›¸æŠ—è¡¡çš„ç”Ÿæˆæ€§èƒ½ã€‚ä½œä¸ºç‹¬ç«‹è´¡çŒ®ï¼Œæˆ‘ä»¬è°ƒæŸ¥äº†å¢å¼ºé•¿è·ç¦»ç©ºé—´ä¾èµ–æ€§æ˜¯å¦å¯ä»¥æé«˜è§†è§‰é€¼çœŸåº¦ã€‚ç»“æœæ˜¯ä¸€ç§æ–°å‹æŒ‡å¯¼æ–¹æ³•â€”â€”æˆ‘ä»¬ç§°ä¹‹ä¸ºæ»‘åŠ¨çª—å£æŒ‡å¯¼ï¼ˆSWGï¼‰ï¼Œå®ƒé€šè¿‡çº¦æŸä¸»æ¨¡å‹çš„æ„Ÿå—é‡æ¥æŒ‡å¯¼ä¸»æ¨¡å‹ã€‚æœ‰è¶£çš„æ˜¯ï¼ŒSWGæ›´ç¬¦åˆäººç±»åå¥½ï¼Œæ—¢ä¸éœ€è¦è®­ç»ƒã€ä¹Ÿä¸éœ€è¦æ¶æ„ä¿®æ”¹æˆ–ç±»åˆ«æ¡ä»¶ï¼Œå³å¯è¶…è¶Šæœ€æ–°æŒ‡å¯¼æ–¹æ³•ã€‚ä»£ç å°†å…¬å¼€å‘å¸ƒã€‚</p>
<p><strong>è¦ç‚¹æŒæ¡</strong></p>
<ol>
<li>Guidanceæ˜¯æ‰©æ•£æ¨¡å‹ä¸­ç”¨äºæé«˜ç”Ÿæˆå›¾åƒæ„ŸçŸ¥è´¨é‡çš„é”™è¯¯æ ¡æ­£æŠ€æœ¯ã€‚</li>
<li>é€šè¿‡çº¿æ€§å¤–æ¨å’Œè¾…åŠ©æ‰©æ•£æ¨¡å‹å®ç°Guidanceæ ¡æ­£ï¼Œå…¶æ€§èƒ½é€šå¸¸ä½äºä¸»æ¨¡å‹ã€‚</li>
<li>åœ¨äºŒç»´å’Œæ›´é«˜ç»´åº¦çš„ç¤ºä¾‹ä¸­ï¼Œè¾…åŠ©æ¨¡å‹ä¸ä¸»æ¨¡å‹çš„ç›¸ä¼¼é”™è¯¯ä½†å¼ºåº¦æ›´å¤§æ—¶æ•ˆç›Šæ˜¾è‘—ã€‚</li>
<li>ä»…é€šè¿‡æ›´å¼ºçš„æƒé‡æ­£åˆ™åŒ–ï¼Œå°±èƒ½ä½¿è¾…åŠ©æ¨¡å‹ä¸ä¸»æ¨¡å‹åŒºåˆ†å¼€æ¥ï¼Œä»è€Œå®ç°å¼ºå¤§çš„ç”Ÿæˆæ€§èƒ½ã€‚</li>
<li>è°ƒæŸ¥äº†å¢å¼ºé•¿è·ç¦»ç©ºé—´ä¾èµ–æ€§å¯¹æé«˜è§†è§‰é€¼çœŸåº¦çš„å½±å“ã€‚</li>
<li>æå‡ºäº†æ–°å‹æŒ‡å¯¼æ–¹æ³•â€”â€”æ»‘åŠ¨çª—å£æŒ‡å¯¼ï¼ˆSWGï¼‰ï¼Œé€šè¿‡çº¦æŸä¸»æ¨¡å‹çš„æ„Ÿå—é‡æ¥å·¥ä½œã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.10257">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-c90386c1519075d4542d9e0897334189.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-178022ce5957b8994282437f3aed7136.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-64f0275af3090a8d7c73fed8ff37e0e9.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-08779c40b00cf66981d7d6bb50f3070b.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-ef3769cd2654427587b38c6cc8f43f84.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="What-to-Preserve-and-What-to-Transfer-Faithful-Identity-Preserving-Diffusion-based-Hairstyle-Transfer"><a href="#What-to-Preserve-and-What-to-Transfer-Faithful-Identity-Preserving-Diffusion-based-Hairstyle-Transfer" class="headerlink" title="What to Preserve and What to Transfer: Faithful, Identity-Preserving   Diffusion-based Hairstyle Transfer"></a>What to Preserve and What to Transfer: Faithful, Identity-Preserving   Diffusion-based Hairstyle Transfer</h2><p><strong>Authors:Chaeyeon Chung, Sunghyun Park, Jeongho Kim, Jaegul Choo</strong></p>
<p>Hairstyle transfer is a challenging task in the image editing field that modifies the hairstyle of a given face image while preserving its other appearance and background features. The existing hairstyle transfer approaches heavily rely on StyleGAN, which is pre-trained on cropped and aligned face images. Hence, they struggle to generalize under challenging conditions such as extreme variations of head poses or focal lengths. To address this issue, we propose a one-stage hairstyle transfer diffusion model, HairFusion, that applies to real-world scenarios. Specifically, we carefully design a hair-agnostic representation as the input of the model, where the original hair information is thoroughly eliminated. Next, we introduce a hair align cross-attention (Align-CA) to accurately align the reference hairstyle with the face image while considering the difference in their head poses. To enhance the preservation of the face imageâ€™s original features, we leverage adaptive hair blending during the inference, where the outputâ€™s hair regions are estimated by the cross-attention map in Align-CA and blended with non-hair areas of the face image. Our experimental results show that our method achieves state-of-the-art performance compared to the existing methods in preserving the integrity of both the transferred hairstyle and the surrounding features. The codes are available at <a target="_blank" rel="noopener" href="https://github.com/cychungg/HairFusion">https://github.com/cychungg/HairFusion</a> </p>
<blockquote>
<p>å‘å‹è½¬ç§»æ˜¯å›¾åƒç¼–è¾‘é¢†åŸŸä¸­çš„ä¸€ä¸ªæŒ‘æˆ˜æ€§ä»»åŠ¡ï¼Œå®ƒä¿®æ”¹ç»™å®šäººè„¸å›¾åƒçš„å‘å‹ï¼ŒåŒæ—¶ä¿ç•™å…¶å…¶ä»–å¤–è§‚å’ŒèƒŒæ™¯ç‰¹å¾ã€‚ç°æœ‰çš„å‘å‹è½¬ç§»æ–¹æ³•ä¸¥é‡ä¾èµ–äºStyleGANï¼Œè¯¥æ¨¡å‹æ˜¯åœ¨è£å‰ªå’Œå¯¹é½çš„äººè„¸å›¾åƒä¸Šè¿›è¡Œé¢„è®­ç»ƒçš„ã€‚å› æ­¤ï¼Œå®ƒä»¬åœ¨æç«¯å¤´éƒ¨å§¿åŠ¿æˆ–ç„¦è·ç­‰æŒ‘æˆ˜æ¡ä»¶ä¸‹éš¾ä»¥æ¨å¹¿ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸€ç«™å¼çš„å‘å‹è½¬ç§»æ‰©æ•£æ¨¡å‹HairFusionï¼Œé€‚ç”¨äºçœŸå®åœºæ™¯ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ç²¾å¿ƒè®¾è®¡äº†ä¸€ä¸ªä¸å¤´å‘æ— å…³çš„è¡¨ç¤ºä½œä¸ºæ¨¡å‹çš„è¾“å…¥ï¼Œå…¶ä¸­åŸå§‹çš„å¤´å‘ä¿¡æ¯è¢«å½»åº•æ¶ˆé™¤ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å¤´å‘å¯¹é½äº¤å‰æ³¨æ„åŠ›ï¼ˆAlign-CAï¼‰æœºåˆ¶ï¼Œä»¥å‡†ç¡®åœ°å°†å‚è€ƒå‘å‹ä¸è„¸éƒ¨å›¾åƒå¯¹é½ï¼ŒåŒæ—¶è€ƒè™‘å®ƒä»¬å¤´éƒ¨å§¿åŠ¿çš„å·®å¼‚ã€‚ä¸ºäº†æé«˜è„¸éƒ¨å›¾åƒåŸå§‹ç‰¹å¾çš„ä¿ç•™æ•ˆæœï¼Œæˆ‘ä»¬åœ¨æ¨ç†è¿‡ç¨‹ä¸­åˆ©ç”¨äº†è‡ªé€‚åº”å¤´å‘æ··åˆæŠ€æœ¯ï¼Œå…¶ä¸­è¾“å‡ºå¤´å‘çš„åŒºåŸŸé€šè¿‡Align-CAä¸­çš„äº¤å‰æ³¨æ„åŠ›å›¾è¿›è¡Œä¼°è®¡ï¼Œå¹¶ä¸è„¸éƒ¨å›¾åƒçš„éå¤´å‘åŒºåŸŸè¿›è¡Œæ··åˆã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¿ç•™è½¬ç§»å‘å‹å’Œå‘¨å›´ç‰¹å¾å®Œæ•´æ€§æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/cychungg/HairFusion%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/cychungg/HairFusionè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.16450v2">PDF</a> Accepted to AAAI 2025</p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ‰©æ•£æ¨¡å‹çš„äººè„¸å›¾åƒå‘å‹è½¬æ¢æŠ€æœ¯ï¼Œæ—¨åœ¨è§£å†³åœ¨çœŸå®åœºæ™¯ä¸­å‘å‹è½¬æ¢çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•é€šè¿‡è®¾è®¡ä¸€ç§å¤´å‘æ— å…³çš„è¡¨ç¤ºæ–¹å¼ä½œä¸ºæ¨¡å‹çš„è¾“å…¥ï¼Œæ¶ˆé™¤åŸå§‹å¤´å‘ä¿¡æ¯ã€‚åŒæ—¶ï¼Œå¼•å…¥å¤´å‘å¯¹é½äº¤å‰æ³¨æ„åŠ›æœºåˆ¶æ¥å‡†ç¡®å¯¹é½å‚è€ƒå‘å‹ä¸è„¸éƒ¨å›¾åƒï¼Œå¹¶åœ¨æ¨ç†è¿‡ç¨‹ä¸­é‡‡ç”¨è‡ªé€‚åº”å¤´å‘æ··åˆæŠ€æœ¯ï¼Œæé«˜é¢éƒ¨å›¾åƒåŸå§‹ç‰¹å¾çš„ä¿ç•™æ•ˆæœã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿ç•™è½¬ç§»å‘å‹å’Œå‘¨å›´ç‰¹å¾å®Œæ•´æ€§æ–¹é¢è¾¾åˆ°äº†å…ˆè¿›æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥æŠ€æœ¯æ—¨åœ¨è§£å†³çœŸå®åœºæ™¯ä¸­å‘å‹è½¬æ¢çš„æŒ‘æˆ˜æ€§é—®é¢˜ã€‚</li>
<li>é€šè¿‡è®¾è®¡å¤´å‘æ— å…³çš„è¡¨ç¤ºæ–¹å¼ä½œä¸ºæ¨¡å‹è¾“å…¥ï¼Œæ¶ˆé™¤åŸå§‹å¤´å‘ä¿¡æ¯ã€‚</li>
<li>å¼•å…¥å¤´å‘å¯¹é½äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œå‡†ç¡®å¯¹é½å‚è€ƒå‘å‹ä¸è„¸éƒ¨å›¾åƒã€‚</li>
<li>é‡‡ç”¨è‡ªé€‚åº”å¤´å‘æ··åˆæŠ€æœ¯ï¼Œæé«˜é¢éƒ¨å›¾åƒåŸå§‹ç‰¹å¾çš„ä¿ç•™æ•ˆæœã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡å®éªŒéªŒè¯äº†å…¶åœ¨ä¿ç•™è½¬ç§»å‘å‹å’Œå‘¨å›´ç‰¹å¾å®Œæ•´æ€§æ–¹é¢çš„ä¼˜åŠ¿ã€‚</li>
<li>è¯¥æ–¹æ³•çš„ä»£ç å·²å…¬å¼€å¯ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.16450">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-eafb89e7ed4c01d3565d0ce08aea2625.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-0e5e4305f0608cd074ad6870aa231254.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-8c21182fa121ed482e6afc4b34ff745e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-c0d2f8f89bc97995e280c2af143e7136.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-89e2b79626cc66ba70986c14c43cb2c8.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-b57471a2a7272c96ef7b635e1313948e.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Unlearning-Concepts-in-Diffusion-Model-via-Concept-Domain-Correction-and-Concept-Preserving-Gradient"><a href="#Unlearning-Concepts-in-Diffusion-Model-via-Concept-Domain-Correction-and-Concept-Preserving-Gradient" class="headerlink" title="Unlearning Concepts in Diffusion Model via Concept Domain Correction and   Concept Preserving Gradient"></a>Unlearning Concepts in Diffusion Model via Concept Domain Correction and   Concept Preserving Gradient</h2><p><strong>Authors:Yongliang Wu, Shiji Zhou, Mingzhuo Yang, Lianzhe Wang, Heng Chang, Wenbo Zhu, Xinting Hu, Xiao Zhou, Xu Yang</strong></p>
<p>Text-to-image diffusion models have achieved remarkable success in generating photorealistic images. However, the inclusion of sensitive information during pre-training poses significant risks. Machine Unlearning (MU) offers a promising solution to eliminate sensitive concepts from these models. Despite its potential, existing MU methods face two main challenges: 1) limited generalization, where concept erasure is effective only within the unlearned set, failing to prevent sensitive concept generation from out-of-set prompts; and 2) utility degradation, where removing target concepts significantly impacts the modelâ€™s overall performance. To address these issues, we propose a novel concept domain correction framework named \textbf{DoCo} (\textbf{Do}main \textbf{Co}rrection). By aligning the output domains of sensitive and anchor concepts through adversarial training, our approach ensures comprehensive unlearning of target concepts. Additionally, we introduce a concept-preserving gradient surgery technique that mitigates conflicting gradient components, thereby preserving the modelâ€™s utility while unlearning specific concepts. Extensive experiments across various instances, styles, and offensive concepts demonstrate the effectiveness of our method in unlearning targeted concepts with minimal impact on related concepts, outperforming previous approaches even for out-of-distribution prompts. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆé€¼çœŸçš„å›¾åƒæ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æˆå°±ã€‚ç„¶è€Œï¼Œåœ¨é¢„è®­ç»ƒè¿‡ç¨‹ä¸­å¼•å…¥æ•æ„Ÿä¿¡æ¯å¸¦æ¥äº†å·¨å¤§çš„é£é™©ã€‚æœºå™¨é—å¿˜ï¼ˆMUï¼‰ä¸ºä»è¿™äº›æ¨¡å‹ä¸­æ¶ˆé™¤æ•æ„Ÿæ¦‚å¿µæä¾›äº†æœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆã€‚å°½ç®¡æ½œåŠ›å·¨å¤§ï¼Œä½†ç°æœ‰çš„MUæ–¹æ³•é¢ä¸´ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼š1ï¼‰æœ‰é™çš„æ³›åŒ–èƒ½åŠ›ï¼Œæ¦‚å¿µæ¶ˆé™¤ä»…åœ¨æœªå­¦ä¹ çš„é›†åˆå†…æœ‰æ•ˆï¼Œæ— æ³•é˜²æ­¢æ¥è‡ªé›†åˆå¤–æç¤ºçš„æ•æ„Ÿæ¦‚å¿µç”Ÿæˆï¼›2ï¼‰æ•ˆç”¨é™ä½ï¼Œç§»é™¤ç›®æ ‡æ¦‚å¿µä¼šæ˜¾è‘—å½±å“æ¨¡å‹çš„æ€»ä½“æ€§èƒ½ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºDoCoï¼ˆé¢†åŸŸä¿®æ­£ï¼‰çš„æ–°å‹æ¦‚å¿µåŸŸä¿®æ­£æ¡†æ¶ã€‚é€šè¿‡å¯¹æ•æ„Ÿæ¦‚å¿µå’Œé”šå®šæ¦‚å¿µè¾“å‡ºåŸŸçš„å¯¹é½ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç¡®ä¿äº†ç›®æ ‡æ¦‚å¿µçš„å…¨é¢é—å¿˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ¦‚å¿µä¿ç•™æ¢¯åº¦æ‰‹æœ¯æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯å¯ä»¥ç¼“è§£å†²çªçš„æ¢¯åº¦æˆåˆ†ï¼Œä»è€Œåœ¨é—å¿˜ç‰¹å®šæ¦‚å¿µçš„åŒæ—¶ä¿æŒæ¨¡å‹çš„å®ç”¨æ€§ã€‚åœ¨å„ç§å®ä¾‹ã€é£æ ¼å’Œå†’çŠ¯æ¦‚å¿µçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨é—å¿˜ç›®æ ‡æ¦‚å¿µæ–¹é¢éå¸¸æœ‰æ•ˆï¼Œå¯¹ç›¸å…³æ¦‚å¿µçš„å½±å“æœ€å°ï¼Œç”šè‡³åœ¨è¶…å‡ºåˆ†å¸ƒèŒƒå›´çš„æç¤ºä¸­ä¹Ÿä¼˜äºä»¥å‰çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.15304v2">PDF</a> AAAI 2025</p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆé€¼çœŸçš„å›¾åƒæ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œä½†åœ¨é¢„è®­ç»ƒè¿‡ç¨‹ä¸­å¼•å…¥æ•æ„Ÿä¿¡æ¯å¸¦æ¥äº†å¾ˆå¤§çš„é£é™©ã€‚æœºå™¨é—å¿˜ï¼ˆMUï¼‰ä¸ºè§£å†³è¿™ä¸€é—®é¢˜æä¾›äº†æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œç°æœ‰çš„MUæ–¹æ³•é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šæ¦‚å¿µæ“¦é™¤ä»…é™äºæœªå­¦ä¹ çš„æ•°æ®é›†å†…æœ‰æ•ˆï¼Œæ— æ³•é˜²æ­¢æ•æ„Ÿæ¦‚å¿µä»æ•°æ®é›†å¤–çš„æç¤ºäº§ç”Ÿï¼›ç§»é™¤ç›®æ ‡æ¦‚å¿µä¼šå½±å“æ¨¡å‹çš„æ€»ä½“æ€§èƒ½ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åä¸ºDoCoçš„æ–°æ¦‚å¿µåŸŸæ ¡æ­£æ¡†æ¶ï¼Œé€šè¿‡å¯¹æ•æ„Ÿæ¦‚å¿µå’Œé”šæ¦‚å¿µè¾“å‡ºåŸŸçš„å¯¹é½ï¼Œç¡®ä¿ç›®æ ‡æ¦‚å¿µçš„å…¨é¢é—å¿˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ¦‚å¿µä¿ç•™æ¢¯åº¦æ‰‹æœ¯æŠ€æœ¯ï¼Œä»¥ç¼“è§£å†²çªæ¢¯åº¦æˆåˆ†ï¼Œä»è€Œåœ¨é—å¿˜ç‰¹å®šæ¦‚å¿µçš„åŒæ—¶ä¿ç•™æ¨¡å‹çš„å®ç”¨æ€§ã€‚å®éªŒè¯æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨é—å¿˜ç›®æ ‡æ¦‚å¿µæ—¶å¯¹ç›¸å…³å†…å®¹å½±å“æœ€å°ï¼Œå³ä½¿å¯¹äºåˆ†å¸ƒå¤–çš„æç¤ºä¹Ÿä¼˜äºä»¥å‰çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹èƒ½ç”Ÿæˆé€¼çœŸçš„å›¾åƒï¼Œä½†é¢„è®­ç»ƒä¸­çš„æ•æ„Ÿä¿¡æ¯å¼•å…¥å­˜åœ¨é£é™©ã€‚</li>
<li>ç°æœ‰æœºå™¨é—å¿˜ï¼ˆMUï¼‰æ–¹æ³•é¢ä¸´å±€é™æ€§å’Œå½±å“æ¨¡å‹æ•´ä½“æ€§èƒ½çš„æŒ‘æˆ˜ã€‚</li>
<li>DoCoæ¡†æ¶é€šè¿‡æ¦‚å¿µåŸŸæ ¡æ­£å’Œæ¢¯åº¦æ‰‹æœ¯æŠ€æœ¯è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>DoCoæ¡†æ¶ç¡®ä¿äº†ç›®æ ‡æ¦‚å¿µçš„å…¨é¢é—å¿˜ï¼ŒåŒæ—¶æœ€å°åŒ–äº†å¯¹ç›¸å…³å†…å®¹çš„å½±å“ã€‚</li>
<li>å®éªŒè¯æ˜DoCoæ¡†æ¶å¯¹äºåˆ†å¸ƒå¤–çš„æç¤ºä¹Ÿä¼˜äºä¹‹å‰çš„æ–¹æ³•ã€‚</li>
<li>DoCoæ¡†æ¶å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå¯ä»¥åº”ç”¨äºå„ç§å®ä¾‹ã€é£æ ¼å’Œå†’çŠ¯æ€§æ¦‚å¿µçš„é—å¿˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.15304">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-7af18ba29c0628ccc0c1851696470091.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-896e8f9b74aa7dce6904a192581f87d3.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-3361b0493deb037dc6586a33f2185a5a.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-1e2940107bb9ca11f90e44d5c85f4f9a.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-b36adc874e7a34ffad769b52be19c9f1.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-c123b6c11d1e1776600e409abb0e0fb7.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Synthesizing-Moving-People-with-3D-Control"><a href="#Synthesizing-Moving-People-with-3D-Control" class="headerlink" title="Synthesizing Moving People with 3D Control"></a>Synthesizing Moving People with 3D Control</h2><p><strong>Authors:Boyi Li, Junming Chen, Jathushan Rajasegaran, Yossi Gandelsman, Alexei A. Efros, Jitendra Malik</strong></p>
<p>In this paper, we present a diffusion model-based framework for animating people from a single image for a given target 3D motion sequence. Our approach has two core components: a) learning priors about invisible parts of the human body and clothing, and b) rendering novel body poses with proper clothing and texture. For the first part, we learn an in-filling diffusion model to hallucinate unseen parts of a person given a single image. We train this model on texture map space, which makes it more sample-efficient since it is invariant to pose and viewpoint. Second, we develop a diffusion-based rendering pipeline, which is controlled by 3D human poses. This produces realistic renderings of novel poses of the person, including clothing, hair, and plausible in-filling of unseen regions. This disentangled approach allows our method to generate a sequence of images that are faithful to the target motion in the 3D pose and, to the input image in terms of visual similarity. In addition to that, the 3D control allows various synthetic camera trajectories to render a person. Our experiments show that our method is resilient in generating prolonged motions and varied challenging and complex poses compared to prior methods. Please check our website for more details: <a target="_blank" rel="noopener" href="https://boyiliee.github.io/3DHM.github.io/">https://boyiliee.github.io/3DHM.github.io/</a>. </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„æ¡†æ¶ï¼Œç”¨äºä»ç»™å®šçš„ç›®æ ‡3Dè¿åŠ¨åºåˆ—å¯¹å•ä¸ªäººå›¾åƒè¿›è¡ŒåŠ¨ç”»å¤„ç†ã€‚æˆ‘ä»¬çš„æ–¹æ³•æœ‰ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼ša)å­¦ä¹ å…³äºäººä½“å’Œæœè£…çš„éšè—éƒ¨åˆ†å…ˆéªŒçŸ¥è¯†ï¼Œb)ä½¿ç”¨é€‚å½“çš„æœè£…å’Œçº¹ç†å‘ˆç°æ–°çš„èº«ä½“å§¿æ€ã€‚å¯¹äºç¬¬ä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬å­¦ä¹ äº†ä¸€ä¸ªå¡«å……æ‰©æ•£æ¨¡å‹ï¼Œæ ¹æ®å•å¼ å›¾åƒæ¥æ„æƒ³äººçš„ä¸å¯è§éƒ¨åˆ†ã€‚æˆ‘ä»¬åœ¨çº¹ç†å›¾ç©ºé—´ä¸Šè®­ç»ƒè¿™ä¸ªæ¨¡å‹ï¼Œè¿™ä½¿å¾—å®ƒæ›´åŠ æ ·æœ¬é«˜æ•ˆï¼Œå› ä¸ºå®ƒå¯¹å§¿åŠ¿å’Œè§†è§’å…·æœ‰ä¸å˜æ€§ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªåŸºäºæ‰©æ•£çš„æ¸²æŸ“ç®¡é“ï¼Œå®ƒç”±3Däººç±»å§¿åŠ¿æ§åˆ¶ã€‚è¿™å¯ä»¥äº§ç”Ÿäººç‰©çš„æ–°å§¿æ€çš„é€¼çœŸæ¸²æŸ“ï¼ŒåŒ…æ‹¬æœè£…ã€å¤´å‘å’Œå¯¹æœªè§é¢éƒ¨ä½çš„åˆç†å¡«å……ã€‚è¿™ç§ç‹¬ç«‹çš„æ–¹æ³•å…è®¸æˆ‘ä»¬çš„æ–¹æ³•åœ¨ç”Ÿæˆçš„å›¾åƒåºåˆ—ä¸­ä¸ç›®æ ‡çš„3Då§¿åŠ¿ä¿æŒä¸€è‡´æ€§ï¼Œå¹¶ä¸”åœ¨è§†è§‰ç›¸ä¼¼æ€§æ–¹é¢å¿ äºè¾“å…¥å›¾åƒã€‚é™¤æ­¤ä¹‹å¤–ï¼Œæˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç”ŸæˆæŒç»­è¿åŠ¨ä»¥åŠé¢å¯¹å„ç§å¤æ‚å’ŒæŒ‘æˆ˜æ€§çš„å§¿åŠ¿æ–¹é¢ï¼Œç›¸è¾ƒäºå…ˆå‰çš„æ–¹æ³•å…·æœ‰æ›´å¼ºçš„éŸ§æ€§ã€‚æ›´å¤šè¯¦ç»†ä¿¡æ¯è¯·è®¿é—®æˆ‘ä»¬çš„ç½‘ç«™ï¼š[<a target="_blank" rel="noopener" href="https://boyiliee.github.io/3DHM.github.io/]">https://boyiliee.github.io/3DHM.github.io/]</a> ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2401.10889v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„æ¡†æ¶ï¼Œèƒ½å¤Ÿä»å•ä¸€å›¾åƒä¸­ç”Ÿæˆç»™å®šç›®æ ‡3DåŠ¨ä½œåºåˆ—çš„äººç‰©åŠ¨ç”»ã€‚è¯¥æ–¹æ³•åŒ…å«ä¸¤ä¸ªæ ¸å¿ƒéƒ¨åˆ†ï¼šä¸€æ˜¯å­¦ä¹ äººä½“å’Œæœè£…çš„éšè—éƒ¨åˆ†å…ˆéªŒçŸ¥è¯†ï¼ŒäºŒæ˜¯ä½¿ç”¨æ‰©æ•£æ¨¡å‹æ¸²æŸ“æ–°çš„èº«ä½“å§¿æ€å¹¶é™„å¸¦é€‚å½“çš„æœè£…å’Œçº¹ç†ã€‚é€šè¿‡è®­ç»ƒä¸€ä¸ªå¡«å……æ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ¨¡æ‹Ÿç»™å®šå•ä¸€å›¾åƒä¸­çš„äººç‰©éšè—éƒ¨åˆ†ã€‚åœ¨çº¹ç†æ˜ å°„ç©ºé—´ä¸Šçš„è®­ç»ƒä½¿æ¨¡å‹å…·æœ‰å§¿åŠ¿å’Œè§†è§’çš„ä¸å˜æ€§ï¼Œä»è€Œæ›´åŠ æ ·æœ¬é«˜æ•ˆã€‚æ­¤å¤–ï¼Œå¼€å‘äº†ä¸€ä¸ªåŸºäºæ‰©æ•£çš„æ¸²æŸ“ç®¡é“ï¼Œç”±3Däººç±»å§¿åŠ¿æ§åˆ¶ï¼Œèƒ½å¤Ÿç”Ÿæˆé€¼çœŸçš„äººç‰©æ–°å§¿æ€å›¾åƒï¼ŒåŒ…æ‹¬æœè£…ã€å¤´å‘å’Œåˆç†çš„éšè—åŒºåŸŸå¡«å……ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿç”Ÿæˆä¸€ç³»åˆ—å¿ äºç›®æ ‡è¿åŠ¨çš„å›¾åƒï¼Œå¹¶ä¸”åœ¨è§†è§‰ç›¸ä¼¼æ€§æ–¹é¢å¿ å®äºè¾“å…¥å›¾åƒã€‚æ­¤å¤–ï¼Œ3Dæ§åˆ¶å…è®¸å„ç§åˆæˆç›¸æœºè½¨è¿¹æ¥æ¸²æŸ“äººç‰©ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç”Ÿæˆé•¿æ—¶é—´çš„å¤æ‚åŠ¨ä½œå’Œå…·æœ‰æŒ‘æˆ˜æ€§çš„å§¿æ€æ–¹é¢å…·æœ‰å¾ˆå¼ºçš„é²æ£’æ€§ã€‚è¯¦æƒ…è¯·å‚è§æˆ‘ä»¬çš„ç½‘ç«™ï¼š[ç½‘ç«™åœ°å€]ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†åŸºäºæ‰©æ•£æ¨¡å‹çš„æ¡†æ¶ï¼Œç”¨äºä»å•ä¸€å›¾åƒç”Ÿæˆç›®æ ‡3DåŠ¨ä½œåºåˆ—çš„äººç‰©åŠ¨ç”»ã€‚</li>
<li>åŒ…å«ä¸¤ä¸ªæ ¸å¿ƒéƒ¨åˆ†ï¼šå­¦ä¹ äººä½“å’Œæœè£…çš„éšè—éƒ¨åˆ†å…ˆéªŒçŸ¥è¯†ï¼Œä»¥åŠä½¿ç”¨æ‰©æ•£æ¨¡å‹è¿›è¡Œæ–°å§¿æ€çš„æ¸²æŸ“ã€‚</li>
<li>é€šè¿‡è®­ç»ƒå¡«å……æ‰©æ•£æ¨¡å‹æ¥æ¨¡æ‹Ÿäººç‰©çš„éšè—éƒ¨åˆ†ï¼Œå¹¶åœ¨çº¹ç†æ˜ å°„ç©ºé—´ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä»¥å®ç°å§¿åŠ¿å’Œè§†è§’çš„ä¸å˜æ€§ã€‚</li>
<li>å¼€å‘äº†ä¸€ä¸ªåŸºäºæ‰©æ•£çš„æ¸²æŸ“ç®¡é“ï¼Œç”±3Däººç±»å§¿åŠ¿æ§åˆ¶ï¼Œèƒ½å¤Ÿç”Ÿæˆé€¼çœŸçš„äººç‰©æ–°å§¿æ€å›¾åƒã€‚</li>
<li>æ–¹æ³•èƒ½å¤Ÿç”Ÿæˆä¸€ç³»åˆ—å¿ äºç›®æ ‡è¿åŠ¨çš„å›¾åƒï¼Œå¹¶ä¸”åœ¨è§†è§‰ç›¸ä¼¼æ€§æ–¹é¢å¿ å®äºè¾“å…¥å›¾åƒã€‚</li>
<li>3Dæ§åˆ¶å…è®¸é€šè¿‡å„ç§åˆæˆç›¸æœºè½¨è¿¹æ¥æ¸²æŸ“äººç‰©ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2401.10889">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-896929cfd0d886a2da346e39ab4b637e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-019984fe3c00699e3af1656d5544b6e0.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-d5b8bc924be338ae688d06122ac010e8.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-62dbf8bc3a9fbbf87ca21fb2edb3a20c.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-33b9573a063086ba8544f2450889e74f.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-96ff658e1ba5705082b14e1cef914898.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-24/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-24/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-24/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-e297f6a640d6952c8be78e822bae2889.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-24  Convolutional Deep Operator Networks for Learning Nonlinear Focused   Ultrasound Wave Propagation in Heterogeneous Spinal Cord Anatomy
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-12-24
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-24/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-b95efdaf21aff49b82e4a624031b1c6f.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-24  NeuroPump Simultaneous Geometric and Color Rectification for Underwater   Images
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-24
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">7652.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    


        <style>
            [bg-lazy] {
                background-image: none !important;
                background-color: #eee !important;
            }
        </style>
        <script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                preloadRatio: 3,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z\d\-\.\+]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(t.test(e.href)||r.test(e.href))&&(e.href=a.dataset.original)})});</script><script>(r=>{r.imageLazyLoadSetting.processImages=t;var a=r.imageLazyLoadSetting.isSPA,o=r.imageLazyLoadSetting.preloadRatio||1,d=i();function i(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function t(t){(a||t)&&(d=i());for(var e,n=0;n<d.length;n++)0<=(e=(e=d[n]).getBoundingClientRect()).bottom&&0<=e.left&&e.top<=(r.innerHeight*o||document.documentElement.clientHeight*o)&&(()=>{var t,e,a,o,i=d[n];e=function(){d=d.filter(function(t){return i!==t}),r.imageLazyLoadSetting.onImageLoaded&&r.imageLazyLoadSetting.onImageLoaded(i)},(t=i).dataset.loaded||(t.hasAttribute("bg-lazy")?(t.removeAttribute("bg-lazy"),e&&e()):(a=new Image,o=t.getAttribute("data-original"),a.onload=function(){t.src=o,t.removeAttribute("data-original"),t.setAttribute("data-loaded",!0),e&&e()},a.onerror=function(){t.removeAttribute("data-original"),t.setAttribute("data-loaded",!1),t.src=o},t.src!==o&&(a.src=o)))})()}function e(){clearTimeout(t.tId),t.tId=setTimeout(t,500)}t(),document.addEventListener("scroll",e),r.addEventListener("resize",e),r.addEventListener("orientationchange",e)})(this);</script><script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
