<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot 方向最新论文已更新，请持续关注 Update in 2024-12-24  MR-GDINO Efficient Open-World Continual Object Detection">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-f6f173a6496bfc3db3faf3a0df7918b5.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2024-12-24
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2024-12-24
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    7.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    30 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2024-12-24-更新"><a href="#2024-12-24-更新" class="headerlink" title="2024-12-24 更新"></a>2024-12-24 更新</h1><h2 id="MR-GDINO-Efficient-Open-World-Continual-Object-Detection"><a href="#MR-GDINO-Efficient-Open-World-Continual-Object-Detection" class="headerlink" title="MR-GDINO: Efficient Open-World Continual Object Detection"></a>MR-GDINO: Efficient Open-World Continual Object Detection</h2><p><strong>Authors:Bowen Dong, Zitong Huang, Guanglei Yang, Lei Zhang, Wangmeng Zuo</strong></p>
<p>Open-world (OW) recognition and detection models show strong zero- and few-shot adaptation abilities, inspiring their use as initializations in continual learning methods to improve performance. Despite promising results on seen classes, such OW abilities on unseen classes are largely degenerated due to catastrophic forgetting. To tackle this challenge, we propose an open-world continual object detection task, requiring detectors to generalize to old, new, and unseen categories in continual learning scenarios. Based on this task, we present a challenging yet practical OW-COD benchmark to assess detection abilities. The goal is to motivate OW detectors to simultaneously preserve learned classes, adapt to new classes, and maintain open-world capabilities under few-shot adaptations. To mitigate forgetting in unseen categories, we propose MR-GDINO, a strong, efficient and scalable baseline via memory and retrieval mechanisms within a highly scalable memory pool. Experimental results show that existing continual detectors suffer from severe forgetting for both seen and unseen categories. In contrast, MR-GDINO largely mitigates forgetting with only 0.1% activated extra parameters, achieving state-of-the-art performance for old, new, and unseen categories. </p>
<blockquote>
<p>开放世界（OW）识别和检测模型表现出强大的零样本和少样本适应力，这激发了它们作为初始化在持续学习方法中的使用，以提高性能。尽管在已知类别上取得了有前景的结果，但由于灾难性遗忘，这类OW模型在未知类别上的能力大大退化。为了应对这一挑战，我们提出了一个开放世界持续目标检测任务，要求检测器在持续学习场景中推广到旧、新和未知类别。基于这项任务，我们提出了一个具有挑战性但实用的OW-COD基准测试来评估检测能力。目标是激励OW检测器在保留已学类别、适应新类别和维持开放世界能力的同时，进行少样本适应。为了缓解对未知类别的遗忘，我们提出了MR-GDINO，这是一个强大的、高效的、可扩展的基线，通过内存和检索机制在一个高度可扩展的内存池内实现。实验结果表明，现有的持续检测器在已知和未知类别上都存在严重的遗忘问题。相比之下，MR-GDINO在很大程度上缓解了遗忘问题，只需激活0.1%的额外参数，即可实现对旧、新和未知类别的最佳性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15979v1">PDF</a> Website: <a target="_blank" rel="noopener" href="https://m1saka.moe/owcod/">https://m1saka.moe/owcod/</a> . Code is available at:   <a target="_blank" rel="noopener" href="https://github.com/DongSky/MR-GDINO">https://github.com/DongSky/MR-GDINO</a></p>
<p><strong>Summary</strong></p>
<p>开放世界（OW）识别与检测模型在零次和少次适应方面表现出强大的能力，在持续学习方法中作为初始化使用有助于提高性能。然而，对于未见类别，这种能力会大大退化，面临灾难性遗忘的问题。为解决这一挑战，本文提出了开放世界持续对象检测任务，要求检测器在持续学习场景中泛化到旧、新和未见类别。基于此任务，本文构建了一个具有挑战性且实用的OW-COD基准测试集，以评估检测能力。目标是激励OW检测器在少次适应的同时保留已学类别、适应新类别并保持开放世界的能力。为缓解未见类别的遗忘问题，本文提出了MR-GDINO，通过内存和检索机制构建了一个高度可扩展的内存池中的强效、高效且可扩展的基线。实验结果表明，现有的持续检测器在旧类别和新类别上都存在严重的遗忘问题。相比之下，MR-GDINO仅激活额外的0.1%参数即可大大缓解遗忘问题，并在旧、新和未见类别上实现最佳性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>开放世界识别与检测模型具备强大的零次和少次适应能力，适用于持续学习场景。</li>
<li>在持续学习环境中，模型对未见类别的适应能力退化，面临灾难性遗忘问题。</li>
<li>提出开放世界持续对象检测任务，要求模型在持续学习场景中泛化到旧、新和未见类别。</li>
<li>构建OW-COD基准测试集以评估检测模型的性能。</li>
<li>MR-GDINO通过内存和检索机制提出一种强效、高效且可扩展的基线解决方案。</li>
<li>现有持续检测器在旧类别和新类别上表现不佳，存在严重的遗忘问题。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15979">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c7655ea5bfe2c7edf5cda28a810297e6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c67f270d92a0b582e0f8224b537b87e4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5f259829d5f680f4587202efbf032300.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-045237e83a4e9cb7eb47acc5651e9fa0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Enhancing-Generalized-Few-Shot-Semantic-Segmentation-via-Effective-Knowledge-Transfer"><a href="#Enhancing-Generalized-Few-Shot-Semantic-Segmentation-via-Effective-Knowledge-Transfer" class="headerlink" title="Enhancing Generalized Few-Shot Semantic Segmentation via Effective   Knowledge Transfer"></a>Enhancing Generalized Few-Shot Semantic Segmentation via Effective   Knowledge Transfer</h2><p><strong>Authors:Xinyue Chen, Miaojing Shi, Zijian Zhou, Lianghua He, Sophia Tsoka</strong></p>
<p>Generalized few-shot semantic segmentation (GFSS) aims to segment objects of both base and novel classes, using sufficient samples of base classes and few samples of novel classes. Representative GFSS approaches typically employ a two-phase training scheme, involving base class pre-training followed by novel class fine-tuning, to learn the classifiers for base and novel classes respectively. Nevertheless, distribution gap exists between base and novel classes in this process. To narrow this gap, we exploit effective knowledge transfer from base to novel classes. First, a novel prototype modulation module is designed to modulate novel class prototypes by exploiting the correlations between base and novel classes. Second, a novel classifier calibration module is proposed to calibrate the weight distribution of the novel classifier according to that of the base classifier. Furthermore, existing GFSS approaches suffer from a lack of contextual information for novel classes due to their limited samples, we thereby introduce a context consistency learning scheme to transfer the contextual knowledge from base to novel classes. Extensive experiments on PASCAL-5$^i$ and COCO-20$^i$ demonstrate that our approach significantly enhances the state of the art in the GFSS setting. The code is available at: <a target="_blank" rel="noopener" href="https://github.com/HHHHedy/GFSS-EKT">https://github.com/HHHHedy/GFSS-EKT</a>. </p>
<blockquote>
<p>广义少样本语义分割（GFSS）旨在利用基础类别的充足样本和少量新类别的样本，对基础类别和新类别的对象进行分割。代表性的GFSS方法通常采用两阶段训练方案，包括基础类别预训练和新类别微调，以分别学习基础类别和新类别的分类器。然而，在此过程中，基础类别和新类别之间存在分布差距。为了缩小这一差距，我们从基础类别到新类别实现了有效的知识转移。首先，设计了一个新型原型调制模块，通过利用基础类别和新类别之间的相关性来调节新类别原型。其次，提出了一种新的分类器校准模块，根据基础分类器的权重分布校准新分类器的权重分布。此外，由于现有GFSS方法新类别样本有限，缺乏上下文信息，因此我们引入了一种上下文一致性学习方案，将上下文知识从基础类别转移到新类别。在PASCAL-5i和COCO-20i上的大量实验表明，我们的方法在GFSS设置中显著提高了现有技术水平。代码可在：<a target="_blank" rel="noopener" href="https://github.com/HHHHedy/GFSS-EKT%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/HHHHedy/GFSS-EKT获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15835v1">PDF</a> Accepted to AAAI 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了广义小样本语义分割（GFSS）的目标和方法。GFSS旨在利用基础类的充足样本和新颖类的少量样本对基础类和新颖类的对象进行分割。文章提出了一种有效的知识转移方法，缩小基础类和新颖类之间的分布差距。设计了一个新型原型调制模块，利用基础类和新颖类之间的相关性来调制新颖类原型。同时，提出了一个新型分类器校准模块，根据基础分类器的权重分布校准新颖分类器的权重分布。为了解决新颖类样本有限导致的缺乏上下文信息的问题，引入了上下文一致性学习方案，将上下文知识从基础类转移到新颖类。实验证明，该方法在PASCAL-5i和COCO-20i数据集上显著提高了GFSS的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>广义小样本语义分割（GFSS）旨在利用基础类的充足样本和新颖类的少量样本进行对象分割。</li>
<li>现有的GFSS方法存在基础类和新颖类之间的分布差距问题。</li>
<li>通过设计新型原型调制模块和分类器校准模块，实现有效知识从基础类到新颖类的转移，以缩小分布差距。</li>
<li>引入上下文一致性学习方案，解决新颖类样本有限导致的缺乏上下文信息的问题。</li>
<li>方法在PASCAL-5i和COCO-20i数据集上显著提高了GFSS的性能。</li>
<li>代码已公开可用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15835">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-ecaff4a12309e7ee95857aad4934ed45.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9a8a1555cd16ff3560124fd8e93ab9c4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d41ecf29a590825409d849458efff70e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ba55cfb02798ab66f5ca304097ee84c1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8048e08d4cf4612af6e59c75cef6be05.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Cross-Modal-Few-Shot-Learning-with-Second-Order-Neural-Ordinary-Differential-Equations"><a href="#Cross-Modal-Few-Shot-Learning-with-Second-Order-Neural-Ordinary-Differential-Equations" class="headerlink" title="Cross-Modal Few-Shot Learning with Second-Order Neural Ordinary   Differential Equations"></a>Cross-Modal Few-Shot Learning with Second-Order Neural Ordinary   Differential Equations</h2><p><strong>Authors:Yi Zhang, Chun-Wun Cheng, Junyi He, Zhihai He, Carola-Bibiane Schönlieb, Yuyan Chen, Angelica I Aviles-Rivero</strong></p>
<p>We introduce SONO, a novel method leveraging Second-Order Neural Ordinary Differential Equations (Second-Order NODEs) to enhance cross-modal few-shot learning. By employing a simple yet effective architecture consisting of a Second-Order NODEs model paired with a cross-modal classifier, SONO addresses the significant challenge of overfitting, which is common in few-shot scenarios due to limited training examples. Our second-order approach can approximate a broader class of functions, enhancing the model’s expressive power and feature generalization capabilities. We initialize our cross-modal classifier with text embeddings derived from class-relevant prompts, streamlining training efficiency by avoiding the need for frequent text encoder processing. Additionally, we utilize text-based image augmentation, exploiting CLIP’s robust image-text correlation to enrich training data significantly. Extensive experiments across multiple datasets demonstrate that SONO outperforms existing state-of-the-art methods in few-shot learning performance. </p>
<blockquote>
<p>我们介绍了一种新的方法SONO，它利用二阶神经常微分方程（Second-Order NODEs）增强跨模态小样本学习。SONO采用简单有效的架构，包括二阶NODEs模型和跨模态分类器，解决了小样本场景中常见的过拟合问题，这是由于训练样本有限所致。我们的二阶方法可以近似更广泛的函数，提高模型的表达能力和特征泛化能力。我们用与类别相关的提示生成的文本嵌入来初始化我们的跨模态分类器，避免了频繁使用文本编码器处理的需求，从而简化了训练效率。此外，我们利用基于文本的图像增强，利用CLIP强大的图像-文本相关性，显著丰富训练数据。在多个数据集上的广泛实验表明，SONO在少样本学习性能上超越了现有的最先进方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15813v1">PDF</a> </p>
<p><strong>Summary</strong><br>SONO是一种利用二阶神经常微分方程（Second-Order NODEs）增强跨模态小样学习的新方法。它通过采用简单有效的架构，结合二阶神经常微分方程模型和跨模态分类器，解决了小样场景下因训练样本有限而导致的过拟合问题。二阶方法能更广泛地逼近各类函数，增强模型的表达能力和特征泛化能力。此外，SONO使用基于文本的图像增强技术，利用CLIP的图像文本相关性丰富训练数据。实验证明，SONO在少样本学习方面的性能优于现有先进技术。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SONO利用二阶神经常微分方程（Second-Order NODEs）增强跨模态小样学习。</li>
<li>该方法通过结合二阶神经常微分方程模型和跨模态分类器，解决了小样场景中的过拟合问题。</li>
<li>二阶方法能够更广泛地逼近各类函数，提高模型的表达能力和特征泛化能力。</li>
<li>SONO使用基于文本的图像嵌入初始化跨模态分类器，提高训练效率。</li>
<li>利用CLIP的图像文本相关性进行基于文本的图像增强。</li>
<li>通过丰富的实验验证，SONO在少样本学习方面性能优越。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15813">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-1349311fe633a25222599df7052a2e23.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-62fd490c93a17edf24fd02e7ab31dcbe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-26ce0cef777a6bdf39a1b5a3b3b7a934.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="The-Role-of-Recurrency-in-Image-Segmentation-for-Noisy-and-Limited-Sample-Settings"><a href="#The-Role-of-Recurrency-in-Image-Segmentation-for-Noisy-and-Limited-Sample-Settings" class="headerlink" title="The Role of Recurrency in Image Segmentation for Noisy and Limited   Sample Settings"></a>The Role of Recurrency in Image Segmentation for Noisy and Limited   Sample Settings</h2><p><strong>Authors:David Calhas, João Marques, Arlindo L. Oliveira</strong></p>
<p>The biological brain has inspired multiple advances in machine learning. However, most state-of-the-art models in computer vision do not operate like the human brain, simply because they are not capable of changing or improving their decisions&#x2F;outputs based on a deeper analysis. The brain is recurrent, while these models are not. It is therefore relevant to explore what would be the impact of adding recurrent mechanisms to existing state-of-the-art architectures and to answer the question of whether recurrency can improve existing architectures. To this end, we build on a feed-forward segmentation model and explore multiple types of recurrency for image segmentation. We explore self-organizing, relational, and memory retrieval types of recurrency that minimize a specific energy function. In our experiments, we tested these models on artificial and medical imaging data, while analyzing the impact of high levels of noise and few-shot learning settings. Our results do not validate our initial hypothesis that recurrent models should perform better in these settings, suggesting that these recurrent architectures, by themselves, are not sufficient to surpass state-of-the-art feed-forward versions and that additional work needs to be done on the topic. </p>
<blockquote>
<p>生物大脑为机器学习带来了诸多进步。然而，计算机视觉领域的大多数最新模型并不如人脑那样运作，原因仅仅在于它们无法基于更深入的分析来改变或改进其决策&#x2F;输出。大脑是递归的，而这些模型并不是。因此，探索在现有最新架构中添加递归机制的影响，以及递归能否改进现有架构的问题，是十分重要的。为此，我们在前馈分割模型的基础上，探索了多种用于图像分割的递归类型。我们探索了最小化特定能量函数的自组织、关系和内存检索三种类型的递归。在实验中，我们对人工和医学成像数据进行了测试，同时分析了高噪声和少镜头学习设置的影响。我们的结果并未证实我们的初步假设，即递归模型在这些设置中的表现会更好，这表明这些递归架构本身不足以超越最新的前馈版本，并且还需要在此主题上投入额外的工作。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15734v1">PDF</a> 24 pages</p>
<p><strong>Summary</strong><br>     本文探索了将递归机制添加到现有最先进的架构中对图像分割的影响，实验了自我组织、关系和内存检索等多种递归类型。然而，实验结果并不支持初始假设，即递归模型在高噪声和少镜头学习环境中表现更好，表明仅依靠这些递归架构不足以超越现有最先进的前馈版本，需要在此主题上进一步开展工作。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>递归机制在计算机视觉领域具有探索潜力。</li>
<li>研究人员尝试将递归机制添加到现有最先进的架构中进行图像分割。</li>
<li>实验包括自我组织、关系和内存检索等类型的递归。</li>
<li>实验在高噪声和少镜头学习环境下进行。</li>
<li>实验结果不支持假设递归模型会表现更好。</li>
<li>递归架构本身不足以超越现有最先进的馈送前版本。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15734">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-6658043ebace6a32214666964b876d41.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c0ebaf64ad2fb81c637a5bb7aaff19f5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2675f79d3ef59d1e6ec442b98dc53b26.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-03531701b060add6cffd8a479e5787ed.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Task-Specific-Preconditioner-for-Cross-Domain-Few-Shot-Learning"><a href="#Task-Specific-Preconditioner-for-Cross-Domain-Few-Shot-Learning" class="headerlink" title="Task-Specific Preconditioner for Cross-Domain Few-Shot Learning"></a>Task-Specific Preconditioner for Cross-Domain Few-Shot Learning</h2><p><strong>Authors:Suhyun Kang, Jungwon Park, Wonseok Lee, Wonjong Rhee</strong></p>
<p>Cross-Domain Few-Shot Learning<del>(CDFSL) methods typically parameterize models with task-agnostic and task-specific parameters. To adapt task-specific parameters, recent approaches have utilized fixed optimization strategies, despite their potential sub-optimality across varying domains or target tasks. To address this issue, we propose a novel adaptation mechanism called Task-Specific Preconditioned gradient descent</del>(TSP). Our method first meta-learns Domain-Specific Preconditioners~(DSPs) that capture the characteristics of each meta-training domain, which are then linearly combined using task-coefficients to form the Task-Specific Preconditioner. The preconditioner is applied to gradient descent, making the optimization adaptive to the target task. We constrain our preconditioners to be positive definite, guiding the preconditioned gradient toward the direction of steepest descent. Empirical evaluations on the Meta-Dataset show that TSP achieves state-of-the-art performance across diverse experimental scenarios. </p>
<blockquote>
<p>跨域小样本学习（CDFSL）方法通常使用任务通用参数和任务特定参数对模型进行参数化。为了调整任务特定参数，尽管不同的领域或目标任务可能存在潜在的最优性差异，但最近的方法仍采用了固定的优化策略。为了解决这个问题，我们提出了一种新的自适应机制，称为任务特定预调节梯度下降（TSP）。我们的方法首先通过元学习领域特定预调节器（DSP），捕捉每个元训练领域的特征，然后使用任务系数进行线性组合，形成任务特定预调节器。预调节器应用于梯度下降，使优化适应目标任务。我们将预调节器限制为正定矩阵，引导预调节梯度朝向最陡下降方向。在Meta-Dataset上的经验评估表明，TSP在多种实验场景中实现了最先进的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15483v1">PDF</a> Accepted by AAAI 2025</p>
<p><strong>Summary</strong></p>
<p>文章提出了基于跨域的小样本学习（CDFSL）方法的一种改进自适应机制，称为任务特定预处理梯度下降（TSP）。TSP通过元学习领域特定预处理器（DSP）来捕获每个元训练领域的特征，然后使用任务系数进行线性组合形成任务特定预处理器。预处理器应用于梯度下降，使优化适应目标任务。在Meta-Dataset上的实证评估表明，TSP在多种实验场景下均达到最佳性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>跨域小样本学习方法通常使用任务无关和任务特定的参数对模型进行参数化。</li>
<li>最近的方法使用固定的优化策略来适应任务特定参数，但这在不同领域或目标任务中可能表现不佳。</li>
<li>提出的TSP方法通过元学习领域特定预处理器（DSP）捕获每个元训练领域的特征。</li>
<li>TSP使用线性组合和任务系数形成任务特定预处理器，用于梯度下降。</li>
<li>预处理器使优化适应目标任务的特性。</li>
<li>通过约束预处理器为正定矩阵，可以引导预处理梯度向最陡方向下降。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15483">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f6f173a6496bfc3db3faf3a0df7918b5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ce0a70b331162dcc3d8b47a11f594887.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-41a9081d11a92d1c909fc75db19dc734.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f22146968be77ff2e87a2ec07d84e542.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7c31ffc597389ad533a216ae5176a215.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="PERC-Plan-As-Query-Example-Retrieval-for-Underrepresented-Code-Generation"><a href="#PERC-Plan-As-Query-Example-Retrieval-for-Underrepresented-Code-Generation" class="headerlink" title="PERC: Plan-As-Query Example Retrieval for Underrepresented Code   Generation"></a>PERC: Plan-As-Query Example Retrieval for Underrepresented Code   Generation</h2><p><strong>Authors:Jaeseok Yoo, Hojae Han, Youngwon Lee, Jaejin Kim, Seung-won Hwang</strong></p>
<p>Code generation with large language models has shown significant promise, especially when employing retrieval-augmented generation (RAG) with few-shot examples. However, selecting effective examples that enhance generation quality remains a challenging task, particularly when the target programming language (PL) is underrepresented. In this study, we present two key findings: (1) retrieving examples whose presented algorithmic plans can be referenced for generating the desired behavior significantly improves generation accuracy, and (2) converting code into pseudocode effectively captures such algorithmic plans, enhancing retrieval quality even when the source and the target PLs are different. Based on these findings, we propose Plan-as-query Example Retrieval for few-shot prompting in Code generation (PERC), a novel framework that utilizes algorithmic plans to identify and retrieve effective examples. We validate the effectiveness of PERC through extensive experiments on the CodeContests, HumanEval and MultiPL-E benchmarks: PERC consistently outperforms the state-of-the-art RAG methods in code generation, both when the source and target programming languages match or differ, highlighting its adaptability and robustness in diverse coding environments. </p>
<blockquote>
<p>代码生成与大型语言模型相结合已经展现出巨大的潜力，特别是在使用小样本检索增强生成（RAG）的情况下。然而，在选择能够提高生成质量的有效样本方面，仍然是一项具有挑战性的任务，尤其是在目标编程语言（PL）代表性不足的情况下。在这项研究中，我们发现了两个关键结果：（1）检索那些提供的算法计划可以作为生成所需行为的参考，可以显著提高生成准确性；（2）将代码转换为伪代码可以有效地捕获此类算法计划，即使在源语言和目标编程语言不同的情况下，也能提高检索质量。基于这些发现，我们提出了面向代码生成的小样本提示的“计划查询示例检索”（PERC）新框架，该框架利用算法计划来识别和检索有效示例。我们在CodeContests、HumanEval和MultiPL-E基准测试上对PERC的有效性进行了广泛实验验证：无论是在源语言和目标编程语言相匹配还是存在差异的情况下，PERC在代码生成方面始终优于最新RAG方法，突显其在多样化编码环境中的适应性和稳健性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.12447v2">PDF</a> Accepted by COLING 2025 main conference</p>
<p><strong>Summary</strong></p>
<p>大型语言模型在代码生成方面展现出巨大潜力，特别是采用少量示例的检索增强生成（RAG）技术。本研究发现：一是检索能展示算法计划的例子能显著提高生成准确性；二是将代码转化为伪代码能有效捕捉算法计划，甚至在源语言和目标编程语言不同的情况下也能提高检索质量。基于这些发现，提出了基于算法计划检索有效示例的新框架——Plan-as-query Example Retrieval for few-shot prompting in Code generation（PERC）。在CodeContests、HumanEval和MultiPL-E基准测试上的实验表明，PERC在代码生成方面始终优于最新的RAG方法，无论是在源语言和目标编程语言相匹配还是不相匹配的情况下，都展现了其在多种编程环境中的适应性和稳健性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>检索展示算法计划的例子能显著提高代码生成的准确性。</li>
<li>将代码转化为伪代码有助于提高检索质量，尤其在源语言和目标编程语言不同时。</li>
<li>提出了一种新的基于算法计划的检索框架PERC，用于在少量提示下进行代码生成。</li>
<li>PERC在多个基准测试上表现优于最新的RAG方法。</li>
<li>PERC在源语言和目标编程语言相匹配或不相匹配的情况下都能有效工作。</li>
<li>PERC在多种编程环境中展现出适应性和稳健性。</li>
<li>代码生成是大型语言模型的一个重要应用领域，特别是在使用少量示例的情况下。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.12447">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-871464023cce5fff10c461da3cfc9945.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0109fad767ab94d484400ac5069615ac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-edf566b65c4287d516b082a83d064dd1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c7d98e336f3140319ca6c4004e2f4dc7.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Manta-Enhancing-Mamba-for-Few-Shot-Action-Recognition-of-Long-Sub-Sequence"><a href="#Manta-Enhancing-Mamba-for-Few-Shot-Action-Recognition-of-Long-Sub-Sequence" class="headerlink" title="Manta: Enhancing Mamba for Few-Shot Action Recognition of Long   Sub-Sequence"></a>Manta: Enhancing Mamba for Few-Shot Action Recognition of Long   Sub-Sequence</h2><p><strong>Authors:Wenbo Huang, Jinghui Zhang, Guang Li, Lei Zhang, Shuoyuan Wang, Fang Dong, Jiahui Jin, Takahiro Ogawa, Miki Haseyama</strong></p>
<p>In few-shot action recognition (FSAR), long sub-sequences of video naturally express entire actions more effectively. However, the high computational complexity of mainstream Transformer-based methods limits their application. Recent Mamba demonstrates efficiency in modeling long sequences, but directly applying Mamba to FSAR overlooks the importance of local feature modeling and alignment. Moreover, long sub-sequences within the same class accumulate intra-class variance, which adversely impacts FSAR performance. To solve these challenges, we propose a Matryoshka MAmba and CoNtrasTive LeArning framework (Manta). Firstly, the Matryoshka Mamba introduces multiple Inner Modules to enhance local feature representation, rather than directly modeling global features. An Outer Module captures dependencies of timeline between these local features for implicit temporal alignment. Secondly, a hybrid contrastive learning paradigm, combining both supervised and unsupervised methods, is designed to mitigate the negative effects of intra-class variance accumulation. The Matryoshka Mamba and the hybrid contrastive learning paradigm operate in two parallel branches within Manta, enhancing Mamba for FSAR of long sub-sequence. Manta achieves new state-of-the-art performance on prominent benchmarks, including SSv2, Kinetics, UCF101, and HMDB51. Extensive empirical studies prove that Manta significantly improves FSAR of long sub-sequence from multiple perspectives. </p>
<blockquote>
<p>在少量动作识别（FSAR）中，视频的长子序列更自然地表达了整个动作。然而，主流基于Transformer的方法的高计算复杂度限制了其应用。最近的Mamba在建模长序列方面展示了效率，但直接将Mamba应用于FSAR忽视了局部特征建模和对齐的重要性。此外，同一类别内的长子序列会累积类内差异，这对FSAR性能产生不利影响。为了解决这些挑战，我们提出了一个Matryoshka Mamba和对比学习框架（Manta）。首先，Matryoshka Mamba引入了多个内部模块来增强局部特征表示，而不是直接建模全局特征。外部模块捕获这些局部特征之间时间线的依赖性，进行隐式的时间对齐。其次，结合有监督和无监督方法的混合对比学习范式，旨在减轻类内差异积累带来的负面影响。Matryoshka Mamba和混合对比学习范式在Manta的两个并行分支中运行，增强了Mamba对长子序列的FSAR能力。Manta在包括SSv2、Kinetics、UCF101和HMDB51在内的主流基准测试上达到了最新状态的性能。大量的实证研究证明，Manta从多个角度显著提高了长子序列的FSAR性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07481v2">PDF</a> Accepted by AAAI 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了针对少样本动作识别（FSAR）的挑战，提出了一种名为Manta的新框架。Manta通过Matryoshka Mamba和混合对比学习的方法解决了长序列建模和局部特征对齐的问题，以及同类内方差积累带来的问题。Matryoshka Mamba通过引入多个内部模块增强局部特征表示，外部模块则捕捉这些局部特征的时间线依赖性以实现隐式时间对齐。混合对比学习范式结合监督和无监督方法，减轻了同类内方差积累的负面影响。Manta在SSv2、Kinetics、UCF101和HMDB51等主流基准测试中取得了最新状态的艺术性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Manta框架解决了少样本动作识别（FSAR）中长序列建模的挑战。</li>
<li>Matryoshka Mamba通过引入多个内部模块和外部模块，增强了局部特征表示和时间线依赖性捕捉。</li>
<li>混合对比学习范式结合监督和无监督方法，以减轻同类内方差积累的负面影响。</li>
<li>Manta实现了对长序列的隐式时间对齐。</li>
<li>Manta在多个主流基准测试中实现了最新状态的艺术性能。</li>
<li>Matryoshka Mamba和混合对比学习范式在Manta框架中并行运作，增强了Mamba对FSAR长子序列的识别能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.07481">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-791b23804e885cd93246cbb74bf90011.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d16c2681d6549d6322cdf09a77cba898.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b7850902a9c3912646aa3a165d6fc9b2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d1d20f12b2ec1bc52f161cf0f0f664e5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d3efc2920012bc9e302b18d829345f0e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-27e490f47ca16f70e9a33df1c71fe152.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-34c6e325a9bcc51cecd7512552bf34d5.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Multimodal-Task-Vectors-Enable-Many-Shot-Multimodal-In-Context-Learning"><a href="#Multimodal-Task-Vectors-Enable-Many-Shot-Multimodal-In-Context-Learning" class="headerlink" title="Multimodal Task Vectors Enable Many-Shot Multimodal In-Context Learning"></a>Multimodal Task Vectors Enable Many-Shot Multimodal In-Context Learning</h2><p><strong>Authors:Brandon Huang, Chancharik Mitra, Assaf Arbelle, Leonid Karlinsky, Trevor Darrell, Roei Herzig</strong></p>
<p>The recent success of interleaved Large Multimodal Models (LMMs) in few-shot learning suggests that in-context learning (ICL) with many examples can be promising for learning new tasks. However, this many-shot multimodal ICL setting has one crucial problem: it is fundamentally limited by the model’s context length set at pretraining. The problem is especially prominent in the multimodal domain, which processes both text and images, requiring additional tokens. This motivates the need for a multimodal method to compress many shots into fewer tokens without finetuning. In this work, we enable LMMs to perform multimodal, many-shot in-context learning by leveraging Multimodal Task Vectors (MTV) – compact implicit representations of in-context examples compressed in the model’s attention heads. Specifically, we first demonstrate the existence of such MTV in LMMs and then leverage these extracted MTV to enable many-shot in-context learning for various vision-and-language tasks. Our experiments suggest that MTV can scale in performance with the number of compressed shots and generalize to similar out-of-domain tasks without additional context length for inference. Code: <a target="_blank" rel="noopener" href="https://github.com/Brandon3964/MultiModal-Task-Vector">https://github.com/Brandon3964/MultiModal-Task-Vector</a> </p>
<blockquote>
<p>近期的穿插式大型多模态模型（LMMs）在少样本学习中的成功表明，使用多个例子的上下文学习（ICL）对于学习新任务可能是很有前景的。然而，这种多模态的许多例子上下文学习环境存在一个关键问题：它从根本上受到模型在预训练时设定的上下文长度的限制。这个问题在多模态领域尤为突出，该领域需要处理文本和图像，需要额外的标记符号。这促使需要一种多模态方法，将多个样本压缩成更少的标记符号，无需微调。在这项工作中，我们通过利用多模态任务向量（MTV）——压缩在模型注意力头中的上下文例子的紧凑隐式表示，使LMMs能够执行多模态、多例子的上下文学习。具体来说，我们首先证明这种MTV在LMMs中的存在性，然后利用这些提取的MTV来为各种视觉和语言任务提供多例子的上下文学习能力。我们的实验表明，随着压缩样本数量的增加，MTV的性能也可以提高，并能够推广到类似的域外任务，无需额外的推理上下文长度。代码：<a target="_blank" rel="noopener" href="https://github.com/Brandon3964/MultiModal-Task-Vector">https://github.com/Brandon3964/MultiModal-Task-Vector</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.15334v3">PDF</a> Published in NeurIPS 2024</p>
<p><strong>Summary</strong></p>
<p>大模态模型（LMMs）在少样本学习中的成功表明，利用大量实例进行上下文学习（ICL）是有前景的。然而，多模态的上下文学习面临一个关键问题：模型预训练时的上下文长度限制。特别是在处理文本和图像的多模态领域，该问题尤为突出，需要额外的标记。因此，需要一种多模态方法将多个样本压缩成更少的标记，无需微调。本研究通过利用多任务向量（MTV），使LMMs能够进行多模态、多上下文学习，多任务向量是预训练模型中注意力头内压缩的上下文实例的紧凑隐式表示。实验表明，MTV在压缩样本数量增加时性能有所提升，并能推广到类似的外域任务，无需额外的推理上下文长度。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大模态模型（LMMs）在少样本学习中的成功表明上下文学习（ICL）潜力巨大。</li>
<li>多模态上下文学习面临预训练时的上下文长度限制问题。</li>
<li>多任务向量（MTV）是模型注意力头内的隐式表示，能解决多模态、多上下文学习问题。</li>
<li>MTV通过压缩上下文实例，使LMMs能够处理更多的样本。</li>
<li>MTV在压缩样本数量增加时性能提升，并适用于各种视觉和语言任务。</li>
<li>MTV方法能够推广到类似的外域任务，无需额外的推理上下文长度。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.15334">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-625fcaf6b8482c2b91ee1f0f5716f75d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cd4a4aad7d696d8b9ef065d854ca17d6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b4e5c40ab6ec63cc5e01283186526739.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-24/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-24/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-24/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-05105c3999a0c6cd1deccd2dd5951ec0.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation 方向最新论文已更新，请持续关注 Update in 2024-12-24  Self-supervised Spatial-Temporal Learner for Precipitation Nowcasting
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-12-24
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-24/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-c30275e38a084ae99a2feb9c3b5590ab.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2024-12-24  Tacit Learning with Adaptive Information Selection for Cooperative   Multi-Agent Reinforcement Learning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-24
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">16355.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
