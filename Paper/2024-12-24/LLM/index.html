<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-24  HoVLE Unleashing the Power of Monolithic Vision-Language Models with   Holistic Vision-Language Embedding">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-be79195cff13bb52b890b45f510ba140.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-24
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-24
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    16.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    66 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2024-12-24-æ›´æ–°"><a href="#2024-12-24-æ›´æ–°" class="headerlink" title="2024-12-24 æ›´æ–°"></a>2024-12-24 æ›´æ–°</h1><h2 id="HoVLE-Unleashing-the-Power-of-Monolithic-Vision-Language-Models-with-Holistic-Vision-Language-Embedding"><a href="#HoVLE-Unleashing-the-Power-of-Monolithic-Vision-Language-Models-with-Holistic-Vision-Language-Embedding" class="headerlink" title="HoVLE: Unleashing the Power of Monolithic Vision-Language Models with   Holistic Vision-Language Embedding"></a>HoVLE: Unleashing the Power of Monolithic Vision-Language Models with   Holistic Vision-Language Embedding</h2><p><strong>Authors:Chenxin Tao, Shiqian Su, Xizhou Zhu, Chenyu Zhang, Zhe Chen, Jiawen Liu, Wenhai Wang, Lewei Lu, Gao Huang, Yu Qiao, Jifeng Dai</strong></p>
<p>The rapid advance of Large Language Models (LLMs) has catalyzed the development of Vision-Language Models (VLMs). Monolithic VLMs, which avoid modality-specific encoders, offer a promising alternative to the compositional ones but face the challenge of inferior performance. Most existing monolithic VLMs require tuning pre-trained LLMs to acquire vision abilities, which may degrade their language capabilities. To address this dilemma, this paper presents a novel high-performance monolithic VLM named HoVLE. We note that LLMs have been shown capable of interpreting images, when image embeddings are aligned with text embeddings. The challenge for current monolithic VLMs actually lies in the lack of a holistic embedding module for both vision and language inputs. Therefore, HoVLE introduces a holistic embedding module that converts visual and textual inputs into a shared space, allowing LLMs to process images in the same way as texts. Furthermore, a multi-stage training strategy is carefully designed to empower the holistic embedding module. It is first trained to distill visual features from a pre-trained vision encoder and text embeddings from the LLM, enabling large-scale training with unpaired random images and text tokens. The whole model further undergoes next-token prediction on multi-modal data to align the embeddings. Finally, an instruction-tuning stage is incorporated. Our experiments show that HoVLE achieves performance close to leading compositional models on various benchmarks, outperforming previous monolithic models by a large margin. Model available at <a target="_blank" rel="noopener" href="https://huggingface.co/OpenGVLab/HoVLE">https://huggingface.co/OpenGVLab/HoVLE</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•æ¨åŠ¨äº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„è¿›æ­¥ã€‚ä¸€ä½“å¼VLMé¿å…äº†æ¨¡æ€ç‰¹å®šç¼–ç å™¨ï¼Œä¸ºç»„åˆå¼VLMæä¾›äº†æœ‰å‰é€”çš„æ›¿ä»£æ–¹æ¡ˆï¼Œä½†é¢ä¸´æ€§èƒ½è¾ƒå·®çš„æŒ‘æˆ˜ã€‚å¤§å¤šæ•°ç°æœ‰çš„ä¸€ä½“å¼VLMéœ€è¦è°ƒæ•´é¢„è®­ç»ƒçš„LLMä»¥è·å¾—è§†è§‰èƒ½åŠ›ï¼Œè¿™å¯èƒ½ä¼šé™ä½å…¶è¯­è¨€åŠŸèƒ½ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å›°å¢ƒï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹é«˜æ€§èƒ½çš„ä¸€ä½“å¼VLMï¼Œåä¸ºHoVLEã€‚æˆ‘ä»¬æ³¨æ„åˆ°ï¼Œå½“å›¾åƒåµŒå…¥ä¸æ–‡æœ¬åµŒå…¥å¯¹é½æ—¶ï¼ŒLLMå·²è¢«è¯æ˜èƒ½å¤Ÿè§£é‡Šå›¾åƒã€‚å½“å‰ä¸€ä½“å¼VLMçš„æŒ‘æˆ˜å®é™…ä¸Šåœ¨äºç¼ºä¹ä¸€ä¸ªç”¨äºè§†è§‰å’Œè¯­è¨€è¾“å…¥çš„å…¨é¢åµŒå…¥æ¨¡å—ã€‚å› æ­¤ï¼ŒHoVLEå¼•å…¥äº†ä¸€ä¸ªå…¨é¢åµŒå…¥æ¨¡å—ï¼Œè¯¥æ¨¡å—å°†è§†è§‰å’Œæ–‡æœ¬è¾“å…¥è½¬æ¢ä¸ºå…±äº«ç©ºé—´ï¼Œå…è®¸LLMä»¥ä¸æ–‡æœ¬ç›¸åŒçš„æ–¹å¼å¤„ç†å›¾åƒã€‚æ­¤å¤–ï¼Œç²¾å¿ƒè®¾è®¡äº†ä¸€ç§å¤šé˜¶æ®µè®­ç»ƒç­–ç•¥æ¥å¢å¼ºå…¨é¢åµŒå…¥æ¨¡å—ã€‚å®ƒé¦–å…ˆç»è¿‡è®­ç»ƒï¼Œä»é¢„è®­ç»ƒçš„è§†è§‰ç¼–ç å™¨å’ŒLLMä¸­è·å–æ–‡æœ¬åµŒå…¥æ¥æç‚¼è§†è§‰ç‰¹å¾ï¼Œä»è€Œå®ç°ä¸æœªé…å¯¹çš„éšæœºå›¾åƒå’Œæ–‡æœ¬æ ‡è®°çš„å¤§è§„æ¨¡è®­ç»ƒã€‚æ•´ä¸ªæ¨¡å‹è¿›ä¸€æ­¥è¿›è¡Œå¤šæ¨¡æ€æ•°æ®çš„ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹ï¼Œä»¥å¯¹é½åµŒå…¥ã€‚æœ€åï¼ŒåŠ å…¥äº†æŒ‡ä»¤è°ƒæ•´é˜¶æ®µã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒHoVLEåœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°æ¥è¿‘é¢†å…ˆçš„ç»„åˆæ¨¡å‹ï¼Œå¤§å¤§è¶…è¿‡äº†ä»¥å‰çš„ä¸€ä½“å¼æ¨¡å‹ã€‚æ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://huggingface.co/OpenGVLab/HoVLE%E4%B8%8A%E8%8E%B7%E5%BE%97%E3%80%82">https://huggingface.co/OpenGVLab/HoVLEä¸Šè·å¾—ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.16158v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•æ¨åŠ¨äº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„è¿›æ­¥ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹é«˜æ€§èƒ½å•ä½“VLMï¼Œåä¸ºHoVLEã€‚é’ˆå¯¹ç°æœ‰å•ä½“VLMéœ€è¦åœ¨é¢„è®­ç»ƒLLMä¸Šè°ƒæ•´ä»¥è·å–è§†è§‰èƒ½åŠ›ï¼Œå¯èƒ½å¯¼è‡´è¯­è¨€èƒ½åŠ›ä¸‹é™çš„é—®é¢˜ï¼ŒHoVLEå¼•å…¥äº†ä¸€ä¸ªæ•´ä½“åµŒå…¥æ¨¡å—ï¼Œå°†è§†è§‰å’Œæ–‡æœ¬è¾“å…¥è½¬æ¢ä¸ºå…±äº«ç©ºé—´ï¼Œä½¿LLMèƒ½ä»¥ç›¸åŒæ–¹å¼å¤„ç†å›¾åƒå’Œæ–‡æœ¬ã€‚é€šè¿‡å¤šé˜¶æ®µè®­ç»ƒç­–ç•¥èµ‹èƒ½æ•´ä½“åµŒå…¥æ¨¡å—ï¼Œé¦–å…ˆè¿›è¡Œé¢„è®­ç»ƒè§†è§‰ç¼–ç å™¨å’ŒLLMæ–‡æœ¬åµŒå…¥çš„è’¸é¦ï¼Œç„¶åé€šè¿‡å¯¹å¤šæ¨¡æ€æ•°æ®è¿›è¡Œä¸‹ä¸€ä¸ªä»¤ç‰Œé¢„æµ‹ä»¥å¯¹é½åµŒå…¥ï¼Œæœ€ååŠ å…¥æŒ‡ä»¤è°ƒæ•´é˜¶æ®µã€‚å®éªŒè¡¨æ˜ï¼ŒHoVLEåœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°æ¥è¿‘é¢†å…ˆçš„ç»„åˆæ¨¡å‹ï¼Œå¹¶å¤§å¹…åº¦è¶…è¶Šäº†ä¹‹å‰çš„å•ä½“æ¨¡å‹ã€‚æ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://huggingface.co/OpenGVLab/HoVLE%E8%8E%B7%E5%8F%96%E3%80%82">https://huggingface.co/OpenGVLab/HoVLEè·å–ã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•æ¨åŠ¨äº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„è¿›æ­¥ã€‚</li>
<li>å•ä½“VLMé¿å…ä½¿ç”¨æ¨¡æ€ç‰¹å®šç¼–ç å™¨ï¼Œæä¾›äº†ä¸€ç§æœ‰å‰æ™¯çš„æ›¿ä»£æ–¹æ¡ˆã€‚</li>
<li>ç°æœ‰å•ä½“VLMé¢ä¸´æ€§èƒ½æŒ‘æˆ˜ï¼Œéœ€è¦åœ¨é¢„è®­ç»ƒçš„LLMä¸Šè°ƒæ•´ä»¥è·å–è§†è§‰èƒ½åŠ›ï¼Œå¯èƒ½å¯¼è‡´è¯­è¨€æ€§èƒ½ä¸‹é™ã€‚</li>
<li>HoVLEå¼•å…¥äº†ä¸€ä¸ªæ•´ä½“åµŒå…¥æ¨¡å—ï¼Œå°†è§†è§‰å’Œæ–‡æœ¬è¾“å…¥è½¬æ¢ä¸ºå…±äº«ç©ºé—´ï¼Œä½¿LLMèƒ½å¤ŸåŒæ—¶å¤„ç†å›¾åƒå’Œæ–‡æœ¬ã€‚</li>
<li>HoVLEé‡‡ç”¨å¤šé˜¶æ®µè®­ç»ƒç­–ç•¥ï¼ŒåŒ…æ‹¬è’¸é¦è§†è§‰ç‰¹å¾å’ŒLLMæ–‡æœ¬åµŒå…¥ã€å¤šæ¨¡æ€æ•°æ®çš„ä¸‹ä¸€ä¸ªä»¤ç‰Œé¢„æµ‹å’ŒæŒ‡ä»¤è°ƒæ•´é˜¶æ®µã€‚</li>
<li>å®éªŒæ˜¾ç¤ºHoVLEåœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°æ¥è¿‘é¢†å…ˆçš„ç»„åˆæ¨¡å‹ï¼Œå¹¶å¤§å¹…åº¦è¶…è¶Šäº†ä¹‹å‰çš„å•ä½“æ¨¡å‹ã€‚</li>
<li>æ¨¡å‹å¯åœ¨huggingface.co&#x2F;OpenGVLab&#x2F;HoVLEè·å–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.16158">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-cddd1f32c0597adb0d443295e5370a50.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-282cd19ee053d426e5026824038d06e9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1e058202889054595b27c1343b7b44a6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c6605b3a25999c28f229189dd7b69313.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Can-LLMs-Obfuscate-Code-A-Systematic-Analysis-of-Large-Language-Models-into-Assembly-Code-Obfuscation"><a href="#Can-LLMs-Obfuscate-Code-A-Systematic-Analysis-of-Large-Language-Models-into-Assembly-Code-Obfuscation" class="headerlink" title="Can LLMs Obfuscate Code? A Systematic Analysis of Large Language Models   into Assembly Code Obfuscation"></a>Can LLMs Obfuscate Code? A Systematic Analysis of Large Language Models   into Assembly Code Obfuscation</h2><p><strong>Authors:Seyedreza Mohseni, Seyedali Mohammadi, Deepa Tilwani, Yash Saxena, Gerald Ndwula, Sriram Vema, Edward Raff, Manas Gaur</strong></p>
<p>Malware authors often employ code obfuscations to make their malware harder to detect. Existing tools for generating obfuscated code often require access to the original source code (e.g., C++ or Java), and adding new obfuscations is a non-trivial, labor-intensive process. In this study, we ask the following question: Can Large Language Models (LLMs) potentially generate a new obfuscated assembly code? If so, this poses a risk to anti-virus engines and potentially increases the flexibility of attackers to create new obfuscation patterns. We answer this in the affirmative by developing the MetamorphASM benchmark comprising MetamorphASM Dataset (MAD) along with three code obfuscation techniques: dead code, register substitution, and control flow change. The MetamorphASM systematically evaluates the ability of LLMs to generate and analyze obfuscated code using MAD, which contains 328,200 obfuscated assembly code samples. We release this dataset and analyze the success rate of various LLMs (e.g., GPT-3.5&#x2F;4, GPT-4o-mini, Starcoder, CodeGemma, CodeLlama, CodeT5, and LLaMA 3.1) in generating obfuscated assembly code. The evaluation was performed using established information-theoretic metrics and manual human review to ensure correctness and provide the foundation for researchers to study and develop remediations to this risk. The source code can be found at the following GitHub link: <a target="_blank" rel="noopener" href="https://github.com/mohammadi-ali/MetamorphASM">https://github.com/mohammadi-ali/MetamorphASM</a>. </p>
<blockquote>
<p>æ¶æ„è½¯ä»¶ä½œè€…ç»å¸¸ä½¿ç”¨ä»£ç æ··æ·†æŠ€æœ¯ä½¿å…¶æ›´éš¾è¢«æ£€æµ‹ã€‚ç°æœ‰çš„ç”Ÿæˆæ··æ·†ä»£ç çš„å·¥å…·é€šå¸¸éœ€è¦è®¿é—®åŸå§‹æºä»£ç ï¼ˆä¾‹å¦‚C++æˆ–Javaï¼‰ï¼Œå¹¶ä¸”æ·»åŠ æ–°çš„æ··æ·†æ˜¯ä¸€é¡¹éå¹³å‡¡ä¸”åŠ³åŠ¨å¯†é›†å‹çš„æµç¨‹ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä»¥ä¸‹é—®é¢˜ï¼šå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èƒ½å¦ç”Ÿæˆæ–°çš„æ··æ·†æ±‡ç¼–ä»£ç ï¼Ÿå¦‚æœæ˜¯è¿™æ ·ï¼Œè¿™å¯¹æŠ—ç—…æ¯’å¼•æ“æ„æˆé£é™©ï¼Œå¹¶å¯èƒ½å¢åŠ æ”»å‡»è€…åˆ›å»ºæ–°æ··æ·†æ¨¡å¼çš„çµæ´»æ€§ã€‚æˆ‘ä»¬é€šè¿‡å¼€å‘MetamorphASMåŸºå‡†æµ‹è¯•æ¥å›ç­”è¿™ä¸ªé—®é¢˜ï¼Œè¯¥æµ‹è¯•åŒ…æ‹¬MetamorphASMæ•°æ®é›†ï¼ˆMADï¼‰ä»¥åŠä¸‰ç§ä»£ç æ··æ·†æŠ€æœ¯ï¼šæ­»ä»£ç ã€å¯„å­˜å™¨æ›¿æ¢å’Œæ§åˆ¶æµæ›´æ”¹ã€‚MetamorphASMä½¿ç”¨MADç³»ç»Ÿåœ°è¯„ä¼°LLMç”Ÿæˆå’Œåˆ†ææ··æ·†ä»£ç çš„èƒ½åŠ›ï¼ŒMADåŒ…å«328,200ä¸ªæ··æ·†çš„æ±‡ç¼–ä»£ç æ ·æœ¬ã€‚æˆ‘ä»¬å…¬å¼€äº†æ­¤æ•°æ®é›†ï¼Œå¹¶åˆ†æäº†å„ç§LLMï¼ˆä¾‹å¦‚GPT-3.5&#x2F;4ã€GPT-4o-miniã€Starcoderã€CodeGemmaã€CodeLlamaã€CodeT5å’ŒLLaMA 3.1ï¼‰åœ¨ç”Ÿæˆæ··æ·†æ±‡ç¼–ä»£ç æ–¹é¢çš„æˆåŠŸç‡ã€‚è¯„ä¼°æ˜¯é€šè¿‡å»ºç«‹çš„ä¿¡æ¯ç†è®ºæŒ‡æ ‡å’Œäººå·¥å®¡æŸ¥æ¥ç¡®ä¿å‡†ç¡®æ€§ï¼Œå¹¶ä¸ºç ”ç©¶äººå‘˜ç ”ç©¶å’Œå¼€å‘å¯¹æ­¤é£é™©çš„è¡¥æ•‘æªæ–½æä¾›åŸºç¡€ã€‚æºä»£ç å¯åœ¨ä»¥ä¸‹GitHubé“¾æ¥ä¸­æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/mohammadi-ali/MetamorphASM%E3%80%82">https://github.com/mohammadi-ali/MetamorphASMã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.16135v1">PDF</a> To appear in AAAI 2025, Main Track</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç”Ÿæˆæ–°çš„æ··æ·†æ±‡ç¼–ä»£ç æ–¹é¢çš„æ½œåŠ›ã€‚é’ˆå¯¹ç°æœ‰æ··æ·†ä»£ç ç”Ÿæˆå·¥å…·éœ€è¦è®¿é—®åŸå§‹æºä»£ç ä¸”æ·»åŠ æ–°æ··æ·†è¿‡ç¨‹å¤æ‚ç¹ççš„é—®é¢˜ï¼Œç ”ç©¶å›¢é˜Ÿå¼€å‘äº†MetamorphASMåŸºå‡†æµ‹è¯•å¹³å°ï¼ŒåŒ…æ‹¬MetamorphASMæ•°æ®é›†ï¼ˆMADï¼‰å’Œä¸‰ç§ä»£ç æ··æ·†æŠ€æœ¯ï¼šæ­»ä»£ç ã€å¯„å­˜å™¨æ›¿æ¢å’Œæ§åˆ¶æµå˜åŒ–ã€‚MetamorphASMç³»ç»Ÿè¯„ä¼°äº†LLMåœ¨ç”Ÿæˆå’Œåˆ†ææ··æ·†ä»£ç æ–¹é¢çš„èƒ½åŠ›ï¼Œæ‰€ä½¿ç”¨çš„æ•°æ®é›†åŒ…å«328,200ä¸ªæ··æ·†æ±‡ç¼–ä»£ç æ ·æœ¬ã€‚ç ”ç©¶å›¢é˜Ÿå¯¹å„ç§LLMæ¨¡å‹ï¼ˆå¦‚GPT-3.5&#x2F;4ã€GPT-4o-miniã€Starcoderç­‰ï¼‰åœ¨ç”Ÿæˆæ··æ·†æ±‡ç¼–ä»£ç æ–¹é¢çš„æˆåŠŸç‡è¿›è¡Œäº†åˆ†æã€‚è¯„ä¼°é‡‡ç”¨ä¿¡æ¯ç†è®ºåº¦é‡æ–¹æ³•å’Œäººå·¥å®¡æŸ¥ï¼Œä»¥ç¡®ä¿æ­£ç¡®æ€§å¹¶ä¸ºç ”ç©¶äººå‘˜æä¾›ç ”ç©¶å¹¶åˆ¶å®šè¡¥æ•‘æªæ–½çš„åŸºç¡€ã€‚ç›¸å…³æºä»£ç å¯åœ¨GitHubé“¾æ¥ä¸­æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/mohammadi-ali/MetamorphASM">é“¾æ¥åœ°å€</a>ã€‚è¯¥ç ”ç©¶çš„å‘ç°å¯¹åç—…æ¯’å¼•æ“æ„æˆäº†æ½œåœ¨é£é™©ï¼Œå¹¶å¢åŠ äº†æ”»å‡»è€…åˆ›å»ºæ–°æ··æ·†æ¨¡å¼çš„å¯èƒ½æ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å…·æœ‰ç”Ÿæˆæ–°çš„æ··æ·†æ±‡ç¼–ä»£ç çš„æ½œåŠ›ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„åŸºå‡†æµ‹è¯•å¹³å°MetamorphASMï¼Œç”¨äºè¯„ä¼°LLMåœ¨æ··æ·†ä»£ç æ–¹é¢çš„èƒ½åŠ›ã€‚</li>
<li>MetamorphASMæ•°æ®é›†åŒ…å«å¤§é‡çš„æ··æ·†æ±‡ç¼–ä»£ç æ ·æœ¬ã€‚</li>
<li>ç ”ç©¶åˆ†æäº†å¤šç§LLMæ¨¡å‹åœ¨ç”Ÿæˆæ··æ·†æ±‡ç¼–ä»£ç æ–¹é¢çš„æˆåŠŸç‡ã€‚</li>
<li>è¯„ä¼°æ–¹æ³•ç»“åˆäº†ä¿¡æ¯ç†è®ºåº¦é‡æ–¹æ³•å’Œäººå·¥å®¡æŸ¥ï¼Œä»¥ç¡®ä¿æ­£ç¡®æ€§ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºç ”ç©¶äººå‘˜æä¾›äº†ç ”ç©¶å¹¶åˆ¶å®šå¯¹æŠ—è¿™ç§é£é™©çš„è¡¥æ•‘æªæ–½çš„åŸºç¡€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.16135">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-83c221861eac77b91a412b56e7d87111.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2ced293bb3f1389fb5d4cd340734dc92.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-62b4eaafb7dd64bdc27bb1bfc4bbe3c4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8c2e81da3beb18b30cd3d740e3a9e3d1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-050e4a5017475a9043a80f105b2d066b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a0df203f8eb6629451477a7fc4ef9ab0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-12fe5a6dbc44c6d6970043a2605e9094.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6e1635733451991cd096e486b03d06fd.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="PromptOptMe-Error-Aware-Prompt-Compression-for-LLM-based-MT-Evaluation-Metrics"><a href="#PromptOptMe-Error-Aware-Prompt-Compression-for-LLM-based-MT-Evaluation-Metrics" class="headerlink" title="PromptOptMe: Error-Aware Prompt Compression for LLM-based MT Evaluation   Metrics"></a>PromptOptMe: Error-Aware Prompt Compression for LLM-based MT Evaluation   Metrics</h2><p><strong>Authors:Daniil Larionov, Steffen Eger</strong></p>
<p>Evaluating the quality of machine-generated natural language content is a challenging task in Natural Language Processing (NLP). Recently, large language models (LLMs) like GPT-4 have been employed for this purpose, but they are computationally expensive due to the extensive token usage required by complex evaluation prompts. In this paper, we propose a prompt optimization approach that uses a smaller, fine-tuned language model to compress input data for evaluation prompt, thus reducing token usage and computational cost when using larger LLMs for downstream evaluation. Our method involves a two-stage fine-tuning process: supervised fine-tuning followed by preference optimization to refine the modelâ€™s outputs based on human preferences. We focus on Machine Translation (MT) evaluation and utilize the GEMBA-MQM metric as a starting point. Our results show a $2.37\times$ reduction in token usage without any loss in evaluation quality. This work makes state-of-the-art LLM-based metrics like GEMBA-MQM more cost-effective and efficient, enhancing their accessibility for broader use. </p>
<blockquote>
<p>åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä¸­ï¼Œè¯„ä¼°æœºå™¨ç”Ÿæˆçš„è‡ªç„¶è¯­è¨€å†…å®¹çš„è´¨é‡æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚æœ€è¿‘ï¼Œäººä»¬å·²ç»å¼€å§‹ä½¿ç”¨åƒGPT-4è¿™æ ·çš„å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥å®Œæˆè¿™ä¸€ä»»åŠ¡ï¼Œä½†ç”±äºå¤æ‚çš„è¯„ä¼°æç¤ºéœ€è¦å¤§é‡çš„ä»¤ç‰Œä½¿ç”¨ï¼Œå› æ­¤å®ƒä»¬çš„è®¡ç®—æˆæœ¬å¾ˆé«˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æç¤ºä¼˜åŒ–æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä½¿ç”¨è¾ƒå°ã€ç»è¿‡å¾®è°ƒçš„è¯­è¨€æ¨¡å‹æ¥å‹ç¼©è¯„ä¼°æç¤ºçš„è¾“å…¥æ•°æ®ï¼Œä»è€Œåœ¨ä¸‹æ¸¸è¯„ä¼°æ—¶ä½¿ç”¨æ›´å¤§çš„LLMæ—¶å‡å°‘ä»¤ç‰Œä½¿ç”¨å’Œè®¡ç®—æˆæœ¬ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŒ…æ‹¬ä¸€ä¸ªä¸¤é˜¶æ®µçš„å¾®è°ƒè¿‡ç¨‹ï¼šç›‘ç£å¾®è°ƒåé€šè¿‡åå¥½ä¼˜åŒ–æ¥æ ¹æ®äººç±»åå¥½è°ƒæ•´æ¨¡å‹çš„è¾“å‡ºã€‚æˆ‘ä»¬ä¸“æ³¨äºæœºå™¨ç¿»è¯‘ï¼ˆMTï¼‰è¯„ä¼°ï¼Œå¹¶ä½¿ç”¨GEMBA-MQMæŒ‡æ ‡ä½œä¸ºèµ·ç‚¹ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œåœ¨ä¸å½±å“è¯„ä¼°è´¨é‡çš„æƒ…å†µä¸‹ï¼Œä»¤ç‰Œä½¿ç”¨é‡å‡å°‘äº†$2.37\times$ã€‚è¿™é¡¹å·¥ä½œä½¿å¾—æœ€å…ˆè¿›çš„LLMæŒ‡æ ‡ï¼ˆå¦‚GEMBA-MQMï¼‰æ›´å…·ç»æµæ•ˆç›Šå’Œæ•ˆç‡ï¼Œæé«˜äº†å®ƒä»¬ç”¨äºå¹¿æ³›ä½¿ç”¨çš„å¯è®¿é—®æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.16120v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¦‚GPT-4åœ¨è¯„ä¼°è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ç”Ÿæˆå†…å®¹çš„å“è´¨æ—¶é¢ä¸´è®¡ç®—æˆæœ¬é«˜æ˜‚çš„é—®é¢˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå°å‹å¾®è°ƒè¯­è¨€æ¨¡å‹çš„æç¤ºä¼˜åŒ–æ–¹æ³•ï¼Œç”¨äºå‹ç¼©è¯„ä¼°æç¤ºçš„è¾“å…¥æ•°æ®ï¼Œä»è€Œå‡å°‘ä»¤ç‰Œä½¿ç”¨å’Œè®¡ç®—æˆæœ¬ï¼Œæé«˜ä½¿ç”¨å¤§å‹LLMè¿›è¡Œä¸‹æ¸¸è¯„ä¼°æ—¶çš„æˆæœ¬æ•ˆç›Šå’Œæ•ˆç‡ã€‚è¯¥æ–¹æ³•åŒ…æ‹¬ç›‘ç£å¾®è°ƒé˜¶æ®µå’Œåå¥½ä¼˜åŒ–é˜¶æ®µï¼Œä»¥æ ¹æ®äººç±»åå¥½ç»†åŒ–æ¨¡å‹è¾“å‡ºã€‚ç ”ç©¶é‡ç‚¹ä¸ºæœºå™¨ç¿»è¯‘ï¼ˆMTï¼‰è¯„ä¼°ï¼Œå¹¶ä»¥GEMBA-MQMæŒ‡æ ‡ä¸ºèµ·ç‚¹ã€‚ç ”ç©¶ç»“æœå®ç°äº†ä»¤ç‰Œä½¿ç”¨é‡çš„2.37å€å‡å°‘ï¼ŒåŒæ—¶ä¸æŸå¤±è¯„ä¼°è´¨é‡ã€‚è¿™é¡¹ç ”ç©¶ä½¿å…ˆè¿›çš„LLMæŒ‡æ ‡æ›´å…·æˆæœ¬æ•ˆç›Šå’Œæ•ˆç‡ï¼Œæé«˜äº†å…¶å¹¿æ³›ä½¿ç”¨çš„å¯åŠæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è¯„ä¼°è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ç”Ÿæˆå†…å®¹çš„å“è´¨æ—¶é¢ä¸´é«˜è®¡ç®—æˆæœ¬é—®é¢˜ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå°å‹å¾®è°ƒè¯­è¨€æ¨¡å‹çš„æç¤ºä¼˜åŒ–æ–¹æ³•ï¼Œç”¨äºå‹ç¼©è¯„ä¼°æç¤ºçš„è¾“å…¥æ•°æ®ã€‚</li>
<li>æ–¹æ³•åŒ…æ‹¬ç›‘ç£å¾®è°ƒé˜¶æ®µå’Œåå¥½ä¼˜åŒ–é˜¶æ®µï¼Œä»¥æé«˜æ¨¡å‹è¾“å‡ºçš„è´¨é‡ã€‚</li>
<li>ç ”ç©¶é›†ä¸­äºæœºå™¨ç¿»è¯‘ï¼ˆMTï¼‰è¯„ä¼°ï¼Œä»¥GEMBA-MQMæŒ‡æ ‡ä¸ºåŸºç¡€è¿›è¡Œä¼˜åŒ–ã€‚</li>
<li>å®ç°ä»¤ç‰Œä½¿ç”¨é‡çš„æ˜¾è‘—å‡å°‘ï¼ŒåŒæ—¶ä¿æŒè¯„ä¼°è´¨é‡ä¸å˜ã€‚</li>
<li>è¯¥ç ”ç©¶æé«˜äº†å…ˆè¿›çš„LLMæŒ‡æ ‡çš„æˆæœ¬æ•ˆç›Šå’Œæ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.16120">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4e56e3791487f666af98f953ddaacae6.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="PruneVid-Visual-Token-Pruning-for-Efficient-Video-Large-Language-Models"><a href="#PruneVid-Visual-Token-Pruning-for-Efficient-Video-Large-Language-Models" class="headerlink" title="PruneVid: Visual Token Pruning for Efficient Video Large Language Models"></a>PruneVid: Visual Token Pruning for Efficient Video Large Language Models</h2><p><strong>Authors:Xiaohu Huang, Hao Zhou, Kai Han</strong></p>
<p>In this paper, we introduce PruneVid, a visual token pruning method designed to enhance the efficiency of multi-modal video understanding. Large Language Models (LLMs) have shown promising performance in video tasks due to their extended capabilities in comprehending visual modalities. However, the substantial redundancy in video data presents significant computational challenges for LLMs. To address this issue, we introduce a training-free method that 1) minimizes video redundancy by merging spatial-temporal tokens, and 2) leverages LLMsâ€™ reasoning capabilities to selectively prune visual features relevant to question tokens, enhancing model efficiency. We validate our method across multiple video benchmarks, which demonstrate that PruneVid can prune over 80% of tokens while maintaining competitive performance combined with different model networks. This highlights its superior effectiveness and efficiency compared to existing pruning methods. Code: <a target="_blank" rel="noopener" href="https://github.com/Visual-AI/PruneVid">https://github.com/Visual-AI/PruneVid</a>. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†PruneVidï¼Œè¿™æ˜¯ä¸€ç§è§†è§‰ä»¤ç‰Œå‰ªææ–¹æ³•ï¼Œæ—¨åœ¨æé«˜å¤šæ¨¡æ€è§†é¢‘ç†è§£çš„æ•ˆç‡ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”±äºå…¶åœ¨ç†è§£è§†è§‰æ¨¡å¼æ–¹é¢çš„æ‰©å±•èƒ½åŠ›ï¼Œåœ¨è§†é¢‘ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†æœ‰å‰æ™¯çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œè§†é¢‘æ•°æ®ä¸­çš„å¤§é‡å†—ä½™å¯¹LLMå¸¦æ¥äº†å·¨å¤§çš„è®¡ç®—æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•1ï¼‰é€šè¿‡åˆå¹¶æ—¶ç©ºä»¤ç‰Œæ¥æœ€å°åŒ–è§†é¢‘å†—ä½™ï¼Œå¹¶2ï¼‰åˆ©ç”¨LLMçš„æ¨ç†èƒ½åŠ›æœ‰é€‰æ‹©åœ°å‰ªæä¸é—®é¢˜ä»¤ç‰Œç›¸å…³çš„è§†è§‰ç‰¹å¾ï¼Œæé«˜æ¨¡å‹æ•ˆç‡ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªè§†é¢‘åŸºå‡†æµ‹è¯•ä¸ŠéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œç»“æœè¡¨æ˜PruneVidèƒ½å¤Ÿåœ¨ä¿æŒç«äº‰åŠ›çš„åŒæ—¶å‰ªæè¶…è¿‡80%çš„ä»¤ç‰Œï¼Œè¿™çªæ˜¾äº†å…¶åœ¨ä¸å…¶ä»–ç°æœ‰å‰ªææ–¹æ³•ç›¸æ¯”æ—¶å…·æœ‰ä¼˜è¶Šçš„æœ‰æ•ˆæ€§å’Œæ•ˆç‡ã€‚ä»£ç åœ°å€ï¼š<a target="_blank" rel="noopener" href="https://github.com/Visual-AI/PruneVid%E3%80%82">https://github.com/Visual-AI/PruneVidã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.16117v1">PDF</a> Efficient Video Large Language Models</p>
<p><strong>æ‘˜è¦</strong><br>    æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºPruneVidçš„è§†è§‰ä»¤ç‰Œä¿®å‰ªæ–¹æ³•ï¼Œæ—¨åœ¨æé«˜å¤šæ¨¡æ€è§†é¢‘ç†è§£æ•ˆç‡ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è§†é¢‘ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ï¼Œä½†ç”±äºè§†é¢‘æ•°æ®çš„å†—ä½™æ€§ï¼Œå®ƒä»¬é¢ä¸´ç€å·¨å¤§çš„è®¡ç®—æŒ‘æˆ˜ã€‚PruneVidæ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„æ–¹æ³•ï¼Œå®ƒé€šè¿‡åˆå¹¶æ—¶ç©ºä»¤ç‰Œæ¥æœ€å°åŒ–è§†é¢‘å†—ä½™ï¼Œå¹¶åˆ©ç”¨LLMçš„æ¨ç†èƒ½åŠ›æœ‰é€‰æ‹©åœ°ä¿®å‰ªä¸é—®é¢˜ä»¤ç‰Œç›¸å…³çš„è§†è§‰ç‰¹å¾ï¼Œä»è€Œæé«˜æ¨¡å‹æ•ˆç‡ã€‚åœ¨å¤šä¸ªè§†é¢‘åŸºå‡†æµ‹è¯•ä¸­çš„éªŒè¯è¡¨æ˜ï¼ŒPruneVidå¯ä»¥ä¿®å‰ªè¶…è¿‡8%çš„ä»¤ç‰Œï¼ŒåŒæ—¶åœ¨ä¸åŒçš„æ¨¡å‹ç½‘ç»œä¸­ä¿æŒç«äº‰åŠ›ï¼Œè¿™å‡¸æ˜¾äº†å…¶å“è¶Šçš„æœ‰æ•ˆæ€§å’Œæ•ˆç‡ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>PruneVidæ˜¯ä¸€ç§ç”¨äºæé«˜å¤šæ¨¡æ€è§†é¢‘ç†è§£æ•ˆç‡çš„è§†è§‰ä»¤ç‰Œä¿®å‰ªæ–¹æ³•ã€‚</li>
<li>LLMåœ¨è§†é¢‘ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ï¼Œä½†é¢ä¸´è§†é¢‘æ•°æ®å†—ä½™çš„è®¡ç®—æŒ‘æˆ˜ã€‚</li>
<li>PruneVidé€šè¿‡åˆå¹¶æ—¶ç©ºä»¤ç‰Œæ¥æœ€å°åŒ–è§†é¢‘å†—ä½™ã€‚</li>
<li>PruneVidåˆ©ç”¨LLMçš„æ¨ç†èƒ½åŠ›æœ‰é€‰æ‹©åœ°ä¿®å‰ªä¸é—®é¢˜ä»¤ç‰Œç›¸å…³çš„è§†è§‰ç‰¹å¾ã€‚</li>
<li>PruneVidå¯ä»¥åœ¨ä¸åŒçš„æ¨¡å‹ç½‘ç»œä¸­ä¿æŒç«äº‰åŠ›ï¼ŒåŒæ—¶ä¿®å‰ªè¶…è¿‡80%çš„ä»¤ç‰Œã€‚</li>
<li>PruneVidçš„æ–¹æ³•åœ¨å¤šä¸ªè§†é¢‘åŸºå‡†æµ‹è¯•ä¸­å¾—åˆ°äº†éªŒè¯ã€‚</li>
<li>PruneVidå…·æœ‰å“è¶Šçš„æœ‰æ•ˆæ€§å’Œæ•ˆç‡ï¼Œç›¸è¾ƒäºç°æœ‰çš„ä¿®å‰ªæ–¹æ³•è¡¨ç°å‡ºè‰²ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.16117">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-111f8d4f8ca09a591d6492c6c6d6ed1d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-94ddfb3a9516b7aec8b1a7dc78ff4932.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a54e8962ce5910701773d29f284911c8.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Legommenders-A-Comprehensive-Content-Based-Recommendation-Library-with-LLM-Support"><a href="#Legommenders-A-Comprehensive-Content-Based-Recommendation-Library-with-LLM-Support" class="headerlink" title="Legommenders: A Comprehensive Content-Based Recommendation Library with   LLM Support"></a>Legommenders: A Comprehensive Content-Based Recommendation Library with   LLM Support</h2><p><strong>Authors:Qijiong Liu, Lu Fan, Xiao-Ming Wu</strong></p>
<p>We present Legommenders, a unique library designed for content-based recommendation that enables the joint training of content encoders alongside behavior and interaction modules, thereby facilitating the seamless integration of content understanding directly into the recommendation pipeline. Legommenders allows researchers to effortlessly create and analyze over 1,000 distinct models across 15 diverse datasets. Further, it supports the incorporation of contemporary large language models, both as feature encoder and data generator, offering a robust platform for developing state-of-the-art recommendation models and enabling more personalized and effective content delivery. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†Legommendersï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºåŸºäºå†…å®¹çš„æ¨èè€Œè®¾è®¡çš„ç‹¬ç‰¹åº“ã€‚å®ƒä½¿å†…å®¹ç¼–ç å™¨ä¸è¡Œä¸ºå’Œäº¤äº’æ¨¡å—èƒ½å¤Ÿè”åˆè®­ç»ƒï¼Œä»è€Œä¾¿äºå°†å†…å®¹ç†è§£æ— ç¼é›†æˆåˆ°æ¨èç®¡é“ä¸­ã€‚Legommendersè®©ç ”ç©¶äººå‘˜èƒ½å¤Ÿè½»æ¾åˆ›å»ºå’Œåˆ†æè¶…è¿‡1000ä¸ªä¸åŒæ¨¡å‹ï¼Œæ¶‰åŠ15ä¸ªä¸åŒçš„æ•°æ®é›†ã€‚æ­¤å¤–ï¼Œå®ƒæ”¯æŒèå…¥å½“ä»£çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæ—¢å¯ä»¥ä½œä¸ºç‰¹å¾ç¼–ç å™¨ï¼Œä¹Ÿå¯ä»¥ä½œä¸ºæ•°æ®ç”Ÿæˆå™¨ï¼Œä¸ºå¼€å‘å…ˆè¿›æ¨èæ¨¡å‹æä¾›äº†ç¨³å¥çš„å¹³å°ï¼Œä½¿å†…å®¹äº¤ä»˜æ›´åŠ ä¸ªæ€§åŒ–å’Œé«˜æ•ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15973v1">PDF</a> </p>
<p><strong>Summary</strong><br>Legommendersæ˜¯ä¸€ä¸ªä¸“ä¸ºå†…å®¹æ¨èè®¾è®¡çš„ç‹¬ç‰¹åº“ï¼Œå¯è”åˆè®­ç»ƒå†…å®¹ç¼–ç å™¨ä»¥åŠè¡Œä¸ºå’Œäº¤äº’æ¨¡å—ï¼Œå°†å†…å®¹ç†è§£æ— ç¼é›†æˆåˆ°æ¨èç®¡é“ä¸­ã€‚å®ƒæ”¯æŒåˆ›å»ºå’Œåˆ†æè¶…è¿‡ä¸€åƒç§ä¸åŒçš„æ¨¡å‹ï¼Œæ”¯æŒåœ¨åäº”ä¸ªä¸åŒæ•°æ®é›†ä¸Šçš„å¤šæ ·åŒ–åº”ç”¨ï¼Œå¹¶å…è®¸èå…¥ç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºç‰¹å¾ç¼–ç å™¨æˆ–æ•°æ®ç”Ÿæˆå™¨ï¼Œä¸ºå¼€å‘å…ˆè¿›æ¨èæ¨¡å‹æä¾›ç¨³å¥å¹³å°ï¼Œå®ç°æ›´åŠ ä¸ªæ€§åŒ–å’Œæœ‰æ•ˆçš„å†…å®¹æ¨é€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Legommendersæ˜¯ä¸€ä¸ªåŸºäºå†…å®¹æ¨èçš„ç‹¬ç‰¹åº“ã€‚</li>
<li>å®ƒæ”¯æŒå†…å®¹ç¼–ç å™¨ä¸è¡Œä¸ºå’Œäº¤äº’æ¨¡å—çš„è”åˆè®­ç»ƒã€‚</li>
<li>Legommenderså®ç°äº†å†…å®¹ç†è§£åˆ°æ¨èç®¡é“çš„æ— ç¼é›†æˆã€‚</li>
<li>è¯¥åº“èƒ½åœ¨åäº”ä¸ªæ•°æ®é›†ä¸Šæ”¯æŒè¶…è¿‡ä¸€åƒç§æ¨¡å‹çš„åˆ›å»ºå’Œåˆ†æã€‚</li>
<li>Legommenderså…è®¸èå…¥ç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºç‰¹å¾ç¼–ç å™¨æˆ–æ•°æ®ç”Ÿæˆå™¨ã€‚</li>
<li>è¯¥å¹³å°ä¸ºå¼€å‘å…ˆè¿›æ¨èæ¨¡å‹æä¾›äº†ç¨³å¥çš„åŸºç¡€ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15973">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-410e7586ba2dac5046e86612ca356f64.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6a6ae7758263f62f5b325c220a6192c8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1c28c031124d4cfcb23a919341edd680.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2e88f8d6fb621fd3f976769e184bf223.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b919105039493cde3578a9b608ee5eed.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Less-is-More-Towards-Green-Code-Large-Language-Models-via-Unified-Structural-Pruning"><a href="#Less-is-More-Towards-Green-Code-Large-Language-Models-via-Unified-Structural-Pruning" class="headerlink" title="Less is More: Towards Green Code Large Language Models via Unified   Structural Pruning"></a>Less is More: Towards Green Code Large Language Models via Unified   Structural Pruning</h2><p><strong>Authors:Guang Yang, Yu Zhou, Xiangyu Zhang, Wei Cheng, Ke Liu, Xiang Chen, Terry Yue Zhuo, Taolue Chen</strong></p>
<p>The extensive application of Large Language Models (LLMs) in generative coding tasks has raised concerns due to their high computational demands and energy consumption. Unlike previous structural pruning methods designed for classification models that deal with lowdimensional classification logits, generative Code LLMs produce high-dimensional token logit sequences, making traditional pruning objectives inherently limited. Moreover, existing single component pruning approaches further constrain the effectiveness when applied to generative Code LLMs. In response, we propose Flab-Pruner, an innovative unified structural pruning method that combines vocabulary, layer, and Feed-Forward Network (FFN) pruning. This approach effectively reduces model parameters while maintaining performance. Additionally, we introduce a customized code instruction data strategy for coding tasks to enhance the performance recovery efficiency of the pruned model. Through extensive evaluations on three state-of-the-art Code LLMs across multiple generative coding tasks, the results demonstrate that Flab-Pruner retains 97% of the original performance after pruning 22% of the parameters and achieves the same or even better performance after post-training. The pruned models exhibit significant improvements in storage, GPU usage, computational efficiency, and environmental impact, while maintaining well robustness. Our research provides a sustainable solution for green software engineering and promotes the efficient deployment of LLMs in real-world generative coding intelligence applications. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç”Ÿæˆå¼ç¼–ç ä»»åŠ¡ä¸­çš„å¹¿æ³›åº”ç”¨å¼•å‘äº†å¯¹å…¶é«˜è®¡ç®—éœ€æ±‚å’Œèƒ½æºæ¶ˆè€—çš„å…³æ³¨ã€‚ä¸åŒäºä¹‹å‰ä¸ºå¤„ç†ä½ç»´åˆ†ç±»é€»è¾‘å€¼è€Œè®¾è®¡çš„åˆ†ç±»æ¨¡å‹çš„ç»“æ„åŒ–ä¿®å‰ªæ–¹æ³•ï¼Œç”Ÿæˆå¼ä»£ç LLMä¼šäº§ç”Ÿé«˜ç»´ä»¤ç‰Œé€»è¾‘å€¼åºåˆ—ï¼Œä½¿å¾—ä¼ ç»Ÿä¿®å‰ªç›®æ ‡å­˜åœ¨å›ºæœ‰çš„å±€é™æ€§ã€‚æ­¤å¤–ï¼Œç°æœ‰çš„å•ä¸€ç»„ä»¶ä¿®å‰ªæ–¹æ³•åœ¨åº”ç”¨äºç”Ÿæˆå¼ä»£ç LLMæ—¶ï¼Œå…¶æ•ˆæœå—åˆ°é™åˆ¶ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºFlab-Prunerï¼Œè¿™æ˜¯ä¸€ç§åˆ›æ–°æ€§çš„ç»Ÿä¸€ç»“æ„åŒ–ä¿®å‰ªæ–¹æ³•ï¼Œç»“åˆäº†è¯æ±‡ã€å±‚å’Œå‰é¦ˆç½‘ç»œï¼ˆFFNï¼‰ä¿®å‰ªã€‚è¯¥æ–¹æ³•åœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶ï¼Œæœ‰æ•ˆå‡å°‘äº†æ¨¡å‹å‚æ•°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä¸ºç¼–ç ä»»åŠ¡å¼•å…¥äº†ä¸€ç§å®šåˆ¶çš„ä»£ç æŒ‡ä»¤æ•°æ®ç­–ç•¥ï¼Œä»¥æé«˜ä¿®å‰ªæ¨¡å‹çš„æ€§èƒ½æ¢å¤æ•ˆç‡ã€‚é€šè¿‡å¯¹ä¸‰ç§æœ€å…ˆè¿›çš„ä»£ç LLMè¿›è¡Œå¤šä¸ªç”Ÿæˆå¼ç¼–ç ä»»åŠ¡çš„å¹¿æ³›è¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºï¼ŒFlab-Pruneråœ¨ä¿®å‰ª22%çš„å‚æ•°åï¼Œä»ä¿ç•™97%çš„åŸå§‹æ€§èƒ½ï¼Œå¹¶åœ¨åè®­ç»ƒè¿‡ç¨‹ä¸­è¾¾åˆ°ç›¸åŒç”šè‡³æ›´å¥½çš„æ€§èƒ½ã€‚ä¿®å‰ªåçš„æ¨¡å‹åœ¨å­˜å‚¨ã€GPUä½¿ç”¨ã€è®¡ç®—æ•ˆç‡å’Œç¯å¢ƒå½±å“æ–¹é¢éƒ½æœ‰æ˜¾è‘—æ”¹å–„ï¼ŒåŒæ—¶ä¿æŒäº†è‰¯å¥½çš„ç¨³å¥æ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ºç»¿è‰²è½¯ä»¶å·¥ç¨‹æä¾›äº†å¯æŒç»­çš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶ä¿ƒè¿›äº†LLMåœ¨ç°å®ä¸–ç•Œçš„ç”Ÿæˆå¼ç¼–ç æ™ºèƒ½åº”ç”¨ä¸­çš„é«˜æ•ˆéƒ¨ç½²ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15921v1">PDF</a> UNDER REVIEW</p>
<p><strong>Summary</strong></p>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç”Ÿæˆå¼ç¼–ç ä»»åŠ¡ä¸­çš„å¹¿æ³›åº”ç”¨å¼•å‘äº†å¯¹å…¶é«˜è®¡ç®—éœ€æ±‚å’Œèƒ½æºæ¶ˆè€—çš„å…³æ³¨ã€‚é’ˆå¯¹ç”Ÿæˆå¼ç¼–ç LLMçš„é«˜ç»´ä»¤ç‰Œé€»è¾‘åºåˆ—ï¼Œä¼ ç»Ÿå‰ªæç›®æ ‡å­˜åœ¨å±€é™æ€§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†Flab-Prunerï¼Œä¸€ç§ç»“åˆè¯æ±‡ã€å±‚å’Œå‰é¦ˆç½‘ç»œï¼ˆFFNï¼‰ä¿®å‰ªçš„ç»Ÿä¸€ç»“æ„å‰ªææ–¹æ³•ï¼Œæœ‰æ•ˆå‡å°‘æ¨¡å‹å‚æ•°ï¼ŒåŒæ—¶ä¿æŒæ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§å®šåˆ¶çš„ä»£ç æŒ‡ä»¤æ•°æ®ç­–ç•¥ï¼Œä»¥æé«˜å‰ªææ¨¡å‹çš„æ€§èƒ½æ¢å¤æ•ˆç‡ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒFlab-Pruneråœ¨å‰ªæ22%çš„å‚æ•°åä¿ç•™äº†97%çš„åŸå§‹æ€§èƒ½ï¼Œå¹¶åœ¨åè®­ç»ƒè¿‡ç¨‹ä¸­å®ç°äº†ç›¸åŒæˆ–æ›´å¥½çš„æ€§èƒ½ã€‚å‰ªææ¨¡å‹åœ¨å­˜å‚¨ã€GPUä½¿ç”¨ã€è®¡ç®—æ•ˆç‡å’Œç¯å¢ƒå½±å“æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—æ”¹è¿›ï¼ŒåŒæ—¶ä¿æŒäº†è‰¯å¥½çš„ç¨³å¥æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨ç”Ÿæˆå¼ç¼–ç ä»»åŠ¡ä¸­çš„é«˜è®¡ç®—éœ€æ±‚å’Œèƒ½æºæ¶ˆè€—å¼•å‘å…³æ³¨ã€‚</li>
<li>ä¼ ç»Ÿå‰ªææ–¹æ³•åœ¨å¤„ç†ç”Ÿæˆå¼ç¼–ç LLMæ—¶å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>Flab-Pruneræ˜¯ä¸€ç§ç»“åˆè¯æ±‡ã€å±‚å’ŒFFNä¿®å‰ªçš„ç»Ÿä¸€ç»“æ„å‰ªææ–¹æ³•ã€‚</li>
<li>Flab-Pruneræœ‰æ•ˆå‡å°‘æ¨¡å‹å‚æ•°ï¼ŒåŒæ—¶ä¿æŒæ€§èƒ½ã€‚</li>
<li>å®šåˆ¶çš„ä»£ç æŒ‡ä»¤æ•°æ®ç­–ç•¥æé«˜äº†å‰ªææ¨¡å‹çš„æ€§èƒ½æ¢å¤æ•ˆç‡ã€‚</li>
<li>Flab-Pruneråœ¨å‰ªæ22%çš„å‚æ•°åä¿ç•™äº†97%çš„åŸå§‹æ€§èƒ½ã€‚</li>
<li>å‰ªææ¨¡å‹åœ¨å­˜å‚¨ã€GPUä½¿ç”¨ã€è®¡ç®—æ•ˆç‡å’Œç¯å¢ƒå½±å“æ–¹é¢æœ‰æ‰€æ”¹å–„ï¼ŒåŒæ—¶ä¿æŒäº†ç¨³å¥æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15921">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-badc30994ae26aec5c30e05a304a1452.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5368ddf7f6d51834246cbb694103685d.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="TelcoLM-collecting-data-adapting-and-benchmarking-language-models-for-the-telecommunication-domain"><a href="#TelcoLM-collecting-data-adapting-and-benchmarking-language-models-for-the-telecommunication-domain" class="headerlink" title="TelcoLM: collecting data, adapting, and benchmarking language models for   the telecommunication domain"></a>TelcoLM: collecting data, adapting, and benchmarking language models for   the telecommunication domain</h2><p><strong>Authors:Camille Barboule, Viet-Phi Huynh, Adrien Bufort, Yoan Chabot, GÃ©raldine Damnati, GwÃ©nolÃ© LecorvÃ©</strong></p>
<p>Despite outstanding processes in many tasks, Large Language Models (LLMs) still lack accuracy when dealing with highly technical domains. Especially, telecommunications (telco) is a particularly challenging domain due the large amount of lexical, semantic and conceptual peculiarities. Yet, this domain holds many valuable use cases, directly linked to industrial needs. Hence, this paper studies how LLMs can be adapted to the telco domain. It reports our effort to (i) collect a massive corpus of domain-specific data (800M tokens, 80K instructions), (ii) perform adaptation using various methodologies, and (iii) benchmark them against larger generalist models in downstream tasks that require extensive knowledge of telecommunications. Our experiments on Llama-2-7b show that domain-adapted models can challenge the large generalist models. They also suggest that adaptation can be restricted to a unique instruction-tuning step, dicarding the need for any fine-tuning on raw texts beforehand. </p>
<blockquote>
<p>å°½ç®¡åœ¨è®¸å¤šä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„è¿‡ç¨‹ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤„ç†é«˜åº¦æŠ€æœ¯é¢†åŸŸæ—¶ä»ç¼ºä¹å‡†ç¡®æ€§ã€‚å°¤å…¶ç”µä¿¡ï¼ˆç”µä¿¡ï¼‰æ˜¯ä¸€ä¸ªå……æ»¡æŒ‘æˆ˜çš„é¢†åŸŸï¼Œå­˜åœ¨å¤§é‡çš„è¯æ±‡ã€è¯­ä¹‰å’Œæ¦‚å¿µç‰¹æ®Šæ€§ã€‚ç„¶è€Œï¼Œè¿™ä¸ªé¢†åŸŸæœ‰è®¸å¤šç›´æ¥ä¸å·¥ä¸šéœ€æ±‚ç›¸å…³çš„å®è´µç”¨ä¾‹ã€‚å› æ­¤ï¼Œæœ¬æ–‡ç ”ç©¶äº†å¦‚ä½•é€‚åº”LLMç”¨äºç”µä¿¡é¢†åŸŸã€‚æŠ¥å‘Šäº†æˆ‘ä»¬ä¸ºï¼ˆiï¼‰æ”¶é›†å¤§è§„æ¨¡ç‰¹å®šé¢†åŸŸçš„è¯­æ–™åº“ï¼ˆåŒ…å«8äº¿ä¸ªä»¤ç‰Œå’Œ8ä¸‡ä¸ªæŒ‡ä»¤ï¼‰ï¼Œï¼ˆiiï¼‰ä½¿ç”¨å„ç§æ–¹æ³•è¿›è¡Œé€‚åº”ï¼Œï¼ˆiiiï¼‰åœ¨éœ€è¦æ·±å…¥äº†è§£ç”µä¿¡çš„ä¸‹æ¸¸ä»»åŠ¡ä¸­ä¸æ›´å¤§çš„é€šç”¨æ¨¡å‹è¿›è¡ŒåŸºå‡†æµ‹è¯•æ‰€åšçš„åŠªåŠ›ã€‚æˆ‘ä»¬åœ¨Llama-2-7bä¸Šçš„å®éªŒè¡¨æ˜ï¼Œé€‚åº”é¢†åŸŸçš„æ¨¡å‹å¯ä»¥æŒ‘æˆ˜å¤§å‹çš„é€šç”¨æ¨¡å‹ã€‚ä»–ä»¬è¿˜è¡¨æ˜ï¼Œé€‚åº”å¯ä»¥é™åˆ¶åœ¨ä¸€ä¸ªç‹¬ç‰¹çš„æŒ‡ä»¤è°ƒæ•´æ­¥éª¤ä¸­ï¼Œæ— éœ€åœ¨æ­¤ä¹‹å‰å¯¹åŸå§‹æ–‡æœ¬è¿›è¡Œå¾®è°ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15891v1">PDF</a> 30 pages (main: 13 pages, appendices: 17 pages), 1 figure, 22 tables,   achieved March 2024, released December 2024</p>
<p><strong>æ€»ç»“</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä¼—å¤šä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨å¤„ç†é«˜åº¦æŠ€æœ¯åŒ–é¢†åŸŸæ—¶ä»ç¼ºä¹å‡†ç¡®æ€§ã€‚ç”µä¿¡é¢†åŸŸç”±äºå…¶ä¸°å¯Œçš„è¯æ±‡ã€è¯­ä¹‰å’Œæ¦‚å¿µç‰¹ç‚¹ï¼Œæˆä¸ºä¸€ä¸ªç‰¹åˆ«å…·æœ‰æŒ‘æˆ˜æ€§çš„é¢†åŸŸã€‚ç„¶è€Œï¼Œè¯¥é¢†åŸŸæœ‰è®¸å¤šä¸å·¥ä¸šéœ€æ±‚ç›´æ¥ç›¸å…³çš„æœ‰ä»·å€¼ç”¨ä¾‹ã€‚å› æ­¤ï¼Œæœ¬æ–‡ç ”ç©¶äº†å¦‚ä½•é€‚åº”LLMäºç”µä¿¡é¢†åŸŸã€‚æŠ¥å‘Šäº†æˆ‘ä»¬çš„åŠªåŠ›ï¼šï¼ˆiï¼‰æ”¶é›†å¤§é‡é¢†åŸŸç‰¹å®šæ•°æ®ï¼ˆ8äº¿ä»¤ç‰Œï¼Œ8ä¸‡æŒ‡ä»¤ï¼‰ï¼Œï¼ˆiiï¼‰ä½¿ç”¨å„ç§æ–¹æ³•è¿›è¡Œé€‚åº”ï¼Œï¼ˆiiiï¼‰åœ¨éœ€è¦æ·±å…¥äº†è§£ç”µä¿¡çš„ä¸‹æ¸¸ä»»åŠ¡ä¸­ï¼Œä¸æ›´å¤§çš„é€šç”¨æ¨¡å‹è¿›è¡ŒåŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬åœ¨Llama-2-7bä¸Šçš„å®éªŒè¡¨æ˜ï¼Œé€‚åº”é¢†åŸŸçš„æ¨¡å‹å¯ä»¥æŒ‘æˆ˜å¤§å‹é€šç”¨æ¨¡å‹ã€‚ä»–ä»¬è¿˜è¡¨æ˜ï¼Œé€‚åº”å¯ä»¥é™åˆ¶åœ¨ä¸€ä¸ªç‹¬ç‰¹çš„æŒ‡ä»¤è°ƒæ•´æ­¥éª¤ä¸­ï¼Œæ— éœ€åœ¨åŸå§‹æ–‡æœ¬ä¸Šè¿›è¡Œä»»ä½•å¾®è°ƒã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤„ç†é«˜åº¦æŠ€æœ¯åŒ–çš„é¢†åŸŸï¼Œå¦‚ç”µä¿¡é¢†åŸŸæ—¶ï¼Œä»é¢ä¸´å‡†ç¡®æ€§æŒ‘æˆ˜ã€‚</li>
<li>ç”µä¿¡é¢†åŸŸå› å…¶ä¸°å¯Œçš„è¯æ±‡ã€è¯­ä¹‰å’Œæ¦‚å¿µç‰¹æ€§è€Œå…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>æ”¶é›†å¤§é‡ç‰¹å®šé¢†åŸŸçš„æ•°æ®å¯¹äºè®­ç»ƒé€‚åº”ç”µä¿¡é¢†åŸŸçš„LLMè‡³å…³é‡è¦ã€‚</li>
<li>é€šè¿‡å„ç§æ–¹æ³•é€‚åº”LLMä»¥é€‚åº”ç”µä¿¡é¢†åŸŸæ˜¯å¿…è¦çš„ã€‚</li>
<li>åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­ï¼Œé€‚åº”é¢†åŸŸçš„LLMæ€§èƒ½å¯ä¸å¤§å‹é€šç”¨æ¨¡å‹ç›¸æŠ—è¡¡ã€‚</li>
<li>é€‚åº”LLMåªéœ€è¿›è¡Œç‹¬ç‰¹çš„æŒ‡ä»¤è°ƒæ•´ï¼Œæ— éœ€åœ¨åŸå§‹æ–‡æœ¬ä¸Šè¿›è¡Œè¿›ä¸€æ­¥çš„å¾®è°ƒã€‚</li>
<li>è¿™ç§é€‚åº”æ–¹æ³•ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç‰¹å®šé¢†åŸŸçš„åº”ç”¨æä¾›äº†æ–°çš„è§†è§’å’Œå¯èƒ½æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15891">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6c3b3a54543f39f2809415edfe80da7d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6d6cf05b555535caba009b054baf292c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-690c73c808e57238316fc84608ae99f7.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="QUART-Online-Latency-Free-Large-Multimodal-Language-Model-for-Quadruped-Robot-Learning"><a href="#QUART-Online-Latency-Free-Large-Multimodal-Language-Model-for-Quadruped-Robot-Learning" class="headerlink" title="QUART-Online: Latency-Free Large Multimodal Language Model for Quadruped   Robot Learning"></a>QUART-Online: Latency-Free Large Multimodal Language Model for Quadruped   Robot Learning</h2><p><strong>Authors:Xinyang Tong, Pengxiang Ding, Donglin Wang, Wenjie Zhang, Can Cui, Mingyang Sun, Yiguo Fan, Han Zhao, Hongyin Zhang, Yonghao Dang, Siteng Huang, Shangke Lyu</strong></p>
<p>This paper addresses the inherent inference latency challenges associated with deploying multimodal large language models (MLLM) in quadruped vision-language-action (QUAR-VLA) tasks. Our investigation reveals that conventional parameter reduction techniques ultimately impair the performance of the language foundation model during the action instruction tuning phase, making them unsuitable for this purpose. We introduce a novel latency-free quadruped MLLM model, dubbed QUART-Online, designed to enhance inference efficiency without degrading the performance of the language foundation model. By incorporating Action Chunk Discretization (ACD), we compress the original action representation space, mapping continuous action values onto a smaller set of discrete representative vectors while preserving critical information. Subsequently, we fine-tune the MLLM to integrate vision, language, and compressed actions into a unified semantic space. Experimental results demonstrate that QUART-Online operates in tandem with the existing MLLM system, achieving real-time inference in sync with the underlying controller frequency, significantly boosting the success rate across various tasks by 65%. Our project page is \href{<a target="_blank" rel="noopener" href="https://quart-online.github.io}https//quart-online.github.io">https://quart-online.github.io}https://quart-online.github.io</a>. </p>
<blockquote>
<p>æœ¬æ–‡è‡´åŠ›äºè§£å†³åœ¨å¤šè¶³è§†è§‰è¯­è¨€åŠ¨ä½œï¼ˆQUAR-VLAï¼‰ä»»åŠ¡ä¸­éƒ¨ç½²å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰æ‰€é¢ä¸´çš„å›ºæœ‰æ¨ç†å»¶è¿ŸæŒ‘æˆ˜ã€‚æˆ‘ä»¬çš„è°ƒæŸ¥å‘ç°ï¼Œä¼ ç»Ÿçš„å‚æ•°å‡å°‘æŠ€æœ¯æœ€ç»ˆä¼šæŸå®³è¯­è¨€åŸºç¡€æ¨¡å‹åœ¨åŠ¨ä½œæŒ‡ä»¤è°ƒæ•´é˜¶æ®µçš„æ€§èƒ½ï¼Œä½¿å…¶ä¸é€‚åˆæ­¤ç”¨é€”ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹çš„æ— å»¶è¿Ÿå¤šè¶³MLLMæ¨¡å‹ï¼Œç§°ä¸ºQUART-Onlineï¼Œæ—¨åœ¨æé«˜æ¨ç†æ•ˆç‡ï¼ŒåŒæ—¶ä¸é™ä½è¯­è¨€åŸºç¡€æ¨¡å‹çš„æ€§èƒ½ã€‚é€šè¿‡å¼•å…¥åŠ¨ä½œå—ç¦»æ•£åŒ–ï¼ˆACDï¼‰ï¼Œæˆ‘ä»¬å‹ç¼©äº†åŸå§‹çš„åŠ¨ä½œè¡¨ç¤ºç©ºé—´ï¼Œå°†è¿ç»­çš„åŠ¨ä½œå€¼æ˜ å°„åˆ°ä¸€ç»„è¾ƒå°çš„ç¦»æ•£ä»£è¡¨å‘é‡ä¸Šï¼ŒåŒæ—¶ä¿ç•™äº†å…³é”®ä¿¡æ¯ã€‚éšåï¼Œæˆ‘ä»¬å¯¹MLLMè¿›è¡Œäº†å¾®è°ƒï¼Œä»¥å°†è§†è§‰ã€è¯­è¨€å’Œå‹ç¼©åŠ¨ä½œé›†æˆåˆ°ä¸€ä¸ªç»Ÿä¸€çš„è¯­ä¹‰ç©ºé—´ä¸­ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒQUART-Onlineä¸ç°æœ‰çš„MLLMç³»ç»ŸååŒå·¥ä½œï¼Œå®ç°ä¸åº•å±‚æ§åˆ¶å™¨é¢‘ç‡åŒæ­¥çš„å®æ—¶æ¨ç†ï¼Œå„ç§ä»»åŠ¡çš„æˆåŠŸç‡æé«˜äº†65%ã€‚æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢æ˜¯<a target="_blank" rel="noopener" href="https://quart-online.github.io./">https://quart-online.github.ioã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15576v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥è®ºæ–‡é’ˆå¯¹éƒ¨ç½²å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰åœ¨å››è¶³æœºå™¨äººè§†è§‰è¯­è¨€åŠ¨ä½œï¼ˆQUAR-VLAï¼‰ä»»åŠ¡æ—¶é¢ä¸´çš„å›ºæœ‰æ¨ç†å»¶è¿ŸæŒ‘æˆ˜å±•å¼€ç ”ç©¶ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œä¼ ç»Ÿçš„å‚æ•°ç¼©å‡æŠ€æœ¯ä¼šåœ¨åŠ¨ä½œæŒ‡ä»¤è°ƒæ•´é˜¶æ®µæŸå®³è¯­è¨€åŸºç¡€æ¨¡å‹çš„æ€§èƒ½ï¼Œå› æ­¤ä¸é€‚åˆç”¨äºæ­¤ç›®çš„ã€‚è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„æ— å»¶è¿Ÿå››è¶³MLLMæ¨¡å‹â€”â€”QUART-Onlineï¼Œæ—¨åœ¨æé«˜æ¨ç†æ•ˆç‡è€Œä¸é™ä½è¯­è¨€åŸºç¡€æ¨¡å‹çš„æ€§èƒ½ã€‚é€šè¿‡å¼•å…¥åŠ¨ä½œå—ç¦»æ•£åŒ–ï¼ˆACDï¼‰æŠ€æœ¯ï¼Œè¯¥æ¨¡å‹å‹ç¼©äº†åŸå§‹åŠ¨ä½œè¡¨ç¤ºç©ºé—´ï¼Œå°†è¿ç»­åŠ¨ä½œå€¼æ˜ å°„åˆ°ä¸€ç»„è¾ƒå°çš„ç¦»æ•£ä»£è¡¨å‘é‡ä¸Šï¼ŒåŒæ—¶ä¿ç•™äº†å…³é”®ä¿¡æ¯ã€‚éšåï¼Œå¯¹MLLMè¿›è¡Œå¾®è°ƒï¼Œä»¥å°†è§†è§‰ã€è¯­è¨€å’Œå‹ç¼©åŠ¨ä½œæ•´åˆåˆ°ç»Ÿä¸€çš„è¯­ä¹‰ç©ºé—´ä¸­ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒQUART-Onlineä¸ç°æœ‰MLLMç³»ç»ŸååŒå·¥ä½œï¼Œå®ç°äº†å®æ—¶æ¨ç†ï¼Œä¸åº•å±‚æ§åˆ¶å™¨é¢‘ç‡åŒæ­¥ï¼Œå¹¶åœ¨å„ç§ä»»åŠ¡ä¸­æˆåŠŸç‡æé«˜äº†65%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®ºæ–‡è§£å†³äº†åœ¨å››è¶³æœºå™¨äººè§†è§‰è¯­è¨€åŠ¨ä½œä»»åŠ¡ä¸­éƒ¨ç½²å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†å»¶è¿ŸæŒ‘æˆ˜ã€‚</li>
<li>ä¼ ç»Ÿçš„å‚æ•°ç¼©å‡æŠ€æœ¯ä¼šæŸå®³è¯­è¨€æ¨¡å‹åœ¨åŠ¨ä½œæŒ‡ä»¤è°ƒæ•´é˜¶æ®µçš„æ€§èƒ½ã€‚</li>
<li>QUART-Onlineæ¨¡å‹æ—¨åœ¨æé«˜æ¨ç†æ•ˆç‡ï¼ŒåŒæ—¶ä¸é™ä½è¯­è¨€åŸºç¡€æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>é€šè¿‡å¼•å…¥åŠ¨ä½œå—ç¦»æ•£åŒ–ï¼ˆACDï¼‰æŠ€æœ¯ï¼ŒQUART-Onlineå‹ç¼©äº†åŸå§‹åŠ¨ä½œè¡¨ç¤ºç©ºé—´ã€‚</li>
<li>QUART-Onlineå°†è§†è§‰ã€è¯­è¨€å’Œå‹ç¼©åŠ¨ä½œæ•´åˆåˆ°ç»Ÿä¸€çš„è¯­ä¹‰ç©ºé—´ä¸­ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒQUART-Onlineå®ç°äº†å®æ—¶æ¨ç†ï¼Œå¹¶ä¸ç°æœ‰MLLMç³»ç»ŸååŒå·¥ä½œã€‚</li>
<li>QUART-Onlineåœ¨å¤šç§ä»»åŠ¡ä¸­çš„æˆåŠŸç‡æé«˜äº†65%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15576">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0fa3e9452ed1928abeec864f9fd675be.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5071df8abe069937262a9a6e06267494.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6e4eddd7fdd31deac970be73f82feadd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c05f5679f422ab01b536c333d520143e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c0bf716a748a7d8e9e84f4b1f2274cd5.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="ADEQA-A-Question-Answer-based-approach-for-joint-ADE-Suspect-Extraction-using-Sequence-To-Sequence-Transformers"><a href="#ADEQA-A-Question-Answer-based-approach-for-joint-ADE-Suspect-Extraction-using-Sequence-To-Sequence-Transformers" class="headerlink" title="ADEQA: A Question Answer based approach for joint ADE-Suspect Extraction   using Sequence-To-Sequence Transformers"></a>ADEQA: A Question Answer based approach for joint ADE-Suspect Extraction   using Sequence-To-Sequence Transformers</h2><p><strong>Authors:Vinayak Arannil, Tomal Deb, Atanu Roy</strong></p>
<p>Early identification of Adverse Drug Events (ADE) is critical for taking prompt actions while introducing new drugs into the market. These ADEs information are available through various unstructured data sources like clinical study reports, patient health records, social media posts, etc. Extracting ADEs and the related suspect drugs using machine learning is a challenging task due to the complex linguistic relations between drug ADE pairs in textual data and unavailability of large corpus of labelled datasets. This paper introduces ADEQA, a question-answer(QA) based approach using quasi supervised labelled data and sequence-to-sequence transformers to extract ADEs, drug suspects and the relationships between them. Unlike traditional QA models, natural language generation (NLG) based models donâ€™t require extensive token level labelling and thereby reduces the adoption barrier significantly. On a public ADE corpus, we were able to achieve state-of-the-art results with an F1 score of 94% on establishing the relationships between ADEs and the respective suspects. </p>
<blockquote>
<p>åœ¨æ–°è¯ç‰©ä¸Šå¸‚è¿‡ç¨‹ä¸­ï¼Œæ—©æœŸè¯†åˆ«è¯ç‰©ä¸è‰¯ååº”äº‹ä»¶ï¼ˆADEï¼‰å¯¹äºåŠæ—¶é‡‡å–è¡ŒåŠ¨è‡³å…³é‡è¦ã€‚è¿™äº›ADEä¿¡æ¯å¯ä»¥é€šè¿‡å„ç§éç»“æ„åŒ–æ•°æ®æºè·å¾—ï¼Œå¦‚ä¸´åºŠç ”ç©¶æŠ¥å‘Šã€æ‚£è€…å¥åº·è®°å½•ã€ç¤¾äº¤åª’ä½“å¸–å­ç­‰ã€‚ç”±äºæ–‡æœ¬æ•°æ®ä¸­è¯ç‰©ADEå¯¹ä¹‹é—´çš„å¤æ‚è¯­è¨€å…³ç³»å’Œç¼ºä¹å¤§é‡æ ‡è®°æ•°æ®é›†ï¼Œä½¿ç”¨æœºå™¨å­¦ä¹ ä»æ•°æ®ä¸­æå–ADEå’Œç›¸å…³å¯ç–‘è¯ç‰©æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚æœ¬æ–‡ä»‹ç»äº†ADEQAï¼Œè¿™æ˜¯ä¸€ç§åŸºäºé—®ç­”ï¼ˆQAï¼‰çš„æ–¹æ³•ï¼Œä½¿ç”¨å‡†ç›‘ç£æ ‡è®°æ•°æ®å’Œåºåˆ—åˆ°åºåˆ—è½¬æ¢å™¨æ¥æå–ADEã€è¯ç‰©å«Œç–‘çŠ¯ä»¥åŠå®ƒä»¬ä¹‹é—´çš„å…³ç³»ã€‚ä¸ä¼ ç»Ÿçš„é—®ç­”æ¨¡å‹ä¸åŒï¼ŒåŸºäºè‡ªç„¶è¯­è¨€ç”Ÿæˆï¼ˆNLGï¼‰çš„æ¨¡å‹ä¸éœ€è¦å¹¿æ³›çš„ä»¤ç‰Œçº§æ ‡è®°ï¼Œä»è€Œæ˜¾è‘—é™ä½äº†é‡‡ç”¨é—¨æ§›ã€‚åœ¨å…¬å…±ADEè¯­æ–™åº“ä¸Šï¼Œæˆ‘ä»¬èƒ½å¤Ÿåœ¨å»ºç«‹ADEä¸ç›¸åº”å«Œç–‘äººä¹‹é—´çš„å…³ç³»æ–¹é¢å–å¾—äº†æœ€å…ˆè¿›çš„æˆæœï¼ŒF1åˆ†æ•°ä¸º94%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15510v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºADEQAçš„åŸºäºé—®ç­”ï¼ˆQAï¼‰çš„æ–¹æ³•ï¼Œåˆ©ç”¨å‡†ç›‘ç£æ ‡ç­¾æ•°æ®å’Œåºåˆ—åˆ°åºåˆ—çš„è½¬æ¢å™¨æ¥æå–è¯ç‰©ä¸è‰¯ååº”äº‹ä»¶ï¼ˆADEsï¼‰ã€å¯ç–‘è¯ç‰©ä»¥åŠå®ƒä»¬ä¹‹é—´çš„å…³ç³»ã€‚è¿™ç§æ–¹æ³•ä¸éœ€è¦å¤§é‡çš„æ ‡è®°æ•°æ®é›†ï¼Œé€šè¿‡è‡ªç„¶è¯­è¨€ç”Ÿæˆï¼ˆNLGï¼‰æ¨¡å‹å‡å°‘äº†æ ‡ç­¾çš„éš¾åº¦ï¼Œå®ç°äº†åœ¨å…¬å…±ADEè¯­æ–™åº“ä¸Šçš„è‰¯å¥½è¡¨ç°ï¼ŒF1åˆ†æ•°è¾¾åˆ°94%ã€‚ADEQAçš„æ—©æœŸè¯†åˆ«å¯¹äºå°†æ–°è¯å¼•å…¥å¸‚åœºæ—¶é‡‡å–åŠæ—¶è¡ŒåŠ¨è‡³å…³é‡è¦ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ—©æœŸè¯†åˆ«è¯ç‰©ä¸è‰¯ååº”äº‹ä»¶ï¼ˆADEsï¼‰å¯¹äºæ–°è¯ä¸Šå¸‚è‡³å…³é‡è¦ã€‚</li>
<li>ADEsä¿¡æ¯å¯ä»¥é€šè¿‡å„ç§éç»“æ„åŒ–æ•°æ®æºè·å–ï¼Œå¦‚ä¸´åºŠç ”ç©¶æŠ¥å‘Šã€æ‚£è€…å¥åº·è®°å½•ã€ç¤¾äº¤åª’ä½“å¸–å­ç­‰ã€‚</li>
<li>ä½¿ç”¨æœºå™¨å­¦ä¹ ä»æ–‡æœ¬æ•°æ®ä¸­æå–ADEså’Œç›¸å…³å¯ç–‘è¯ç‰©æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œå› ä¸ºè¯ç‰©ADEå¯¹ä¹‹é—´çš„è¯­è¨€å…³ç³»å¤æ‚ä¸”ç¼ºä¹å¤§é‡æ ‡è®°æ•°æ®é›†ã€‚</li>
<li>ADEQAæ˜¯ä¸€ç§åŸºäºé—®ç­”ï¼ˆQAï¼‰çš„æ–¹æ³•ï¼Œä½¿ç”¨å‡†ç›‘ç£æ ‡ç­¾æ•°æ®å’Œåºåˆ—åˆ°åºåˆ—è½¬æ¢å™¨æ¥æå–ADEså’Œè¯ç‰©å«Œç–‘äººåŠå…¶å…³ç³»ã€‚</li>
<li>NLGæ¨¡å‹çš„ä½¿ç”¨å‡å°‘äº†æ ‡ç­¾çš„éš¾åº¦ï¼Œé™ä½äº†é‡‡ç”¨é—¨æ§›ã€‚</li>
<li>åœ¨å…¬å…±ADEè¯­æ–™åº“ä¸Šï¼ŒADEQAå®ç°äº†ä¸è¯ç‰©å«Œç–‘äººçš„å…³ç³»å»ºç«‹çš„F1åˆ†æ•°è¾¾åˆ°94%ï¼Œè¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15510">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b932649e5d6e12384a30ab7254d72bb8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-be79195cff13bb52b890b45f510ba140.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2578e02bbcf5643ecf6de69373bcf093.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e50faccf33b632b8124b5b71d30e5160.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5e8372f8173ecf1cae9eef957e43f6a2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-90629ed7bd5f95d22eb478b10c79d8f4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3862f090617e3017987b55410ddfb280.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Tokenphormer-Structure-aware-Multi-token-Graph-Transformer-for-Node-Classification"><a href="#Tokenphormer-Structure-aware-Multi-token-Graph-Transformer-for-Node-Classification" class="headerlink" title="Tokenphormer: Structure-aware Multi-token Graph Transformer for Node   Classification"></a>Tokenphormer: Structure-aware Multi-token Graph Transformer for Node   Classification</h2><p><strong>Authors:Zijie Zhou, Zhaoqi Lu, Xuekai Wei, Rongqin Chen, Shenghui Zhang, Pak Lon Ip, Leong Hou U</strong></p>
<p>Graph Neural Networks (GNNs) are widely used in graph data mining tasks. Traditional GNNs follow a message passing scheme that can effectively utilize local and structural information. However, the phenomena of over-smoothing and over-squashing limit the receptive field in message passing processes. Graph Transformers were introduced to address these issues, achieving a global receptive field but suffering from the noise of irrelevant nodes and loss of structural information. Therefore, drawing inspiration from fine-grained token-based representation learning in Natural Language Processing (NLP), we propose the Structure-aware Multi-token Graph Transformer (Tokenphormer), which generates multiple tokens to effectively capture local and structural information and explore global information at different levels of granularity. Specifically, we first introduce the walk-token generated by mixed walks consisting of four walk types to explore the graph and capture structure and contextual information flexibly. To ensure local and global information coverage, we also introduce the SGPM-token (obtained through the Self-supervised Graph Pre-train Model, SGPM) and the hop-token, extending the length and density limit of the walk-token, respectively. Finally, these expressive tokens are fed into the Transformer model to learn node representations collaboratively. Experimental results demonstrate that the capability of the proposed Tokenphormer can achieve state-of-the-art performance on node classification tasks. </p>
<blockquote>
<p>å›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰åœ¨å›¾æ•°æ®æŒ–æ˜ä»»åŠ¡ä¸­å¾—åˆ°å¹¿æ³›åº”ç”¨ã€‚ä¼ ç»Ÿçš„GNNséµå¾ªæ¶ˆæ¯ä¼ é€’æ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆå¯ä»¥æœ‰æ•ˆåœ°åˆ©ç”¨å±€éƒ¨å’Œç»“æ„ä¿¡æ¯ã€‚ç„¶è€Œï¼Œè¿‡åº¦å¹³æ»‘å’Œè¿‡åº¦æŒ¤å‹çš„ç°è±¡é™åˆ¶äº†æ¶ˆæ¯ä¼ é€’è¿‡ç¨‹ä¸­çš„æ„Ÿå—é‡ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œå¼•å…¥äº†å›¾è½¬æ¢å™¨ï¼Œå®ç°å…¨å±€æ„Ÿå—é‡ï¼Œä½†å­˜åœ¨æ— å…³èŠ‚ç‚¹çš„å™ªå£°å’Œç»“æ„ä¿¡æ¯ä¸¢å¤±çš„é—®é¢˜ã€‚å› æ­¤ï¼Œä»è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä¸­çš„ç²¾ç»†ç²’åº¦æ ‡è®°è¡¨ç¤ºå­¦ä¹ ä¸­æ±²å–çµæ„Ÿï¼Œæˆ‘ä»¬æå‡ºäº†ç»“æ„æ„ŸçŸ¥å¤šæ ‡è®°å›¾è½¬æ¢å™¨ï¼ˆTokenphormerï¼‰ï¼Œå®ƒç”Ÿæˆå¤šä¸ªä»¤ç‰Œä»¥æœ‰æ•ˆåœ°æ•è·å±€éƒ¨å’Œç»“æ„ä¿¡æ¯ï¼Œå¹¶ä»¥ä¸åŒç²’åº¦çº§åˆ«æ¢ç´¢å…¨å±€ä¿¡æ¯ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆå¼•å…¥ç”±å››ç§è¡Œèµ°ç±»å‹ç»„æˆçš„æ··åˆè¡Œèµ°äº§ç”Ÿçš„walk-tokenï¼Œä»¥çµæ´»åœ°æ¢ç´¢å›¾å¹¶æ•è·ç»“æ„å’Œä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚ä¸ºäº†ç¡®ä¿æœ¬åœ°å’Œå…¨å±€ä¿¡æ¯è¦†ç›–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†é€šè¿‡è‡ªç›‘ç£å›¾é¢„è®­ç»ƒæ¨¡å‹ï¼ˆSGPMï¼‰è·å¾—çš„SGPM-tokenå’Œhop-tokenï¼Œåˆ†åˆ«æ‰©å±•walk-tokençš„é•¿åº¦å’Œå¯†åº¦é™åˆ¶ã€‚æœ€åï¼Œå°†è¿™äº›è¡¨è¾¾æ€§ä»¤ç‰Œè¾“å…¥Transformeræ¨¡å‹ï¼Œä»¥ååŒå­¦ä¹ èŠ‚ç‚¹è¡¨ç¤ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„Tokenphormerçš„èƒ½åŠ›åœ¨èŠ‚ç‚¹åˆ†ç±»ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15302v1">PDF</a> Accpeted by AAAI 2025</p>
<p><strong>Summary</strong><br>     å›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰åœ¨å›¾æ•°æ®æŒ–æ˜ä»»åŠ¡ä¸­å¾—åˆ°å¹¿æ³›åº”ç”¨ï¼Œä½†å­˜åœ¨è¿‡åº¦å¹³æ»‘å’Œè¿‡åº¦å‹ç¼©ç°è±¡ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œå¼•å…¥äº†å›¾Transformerï¼Œä½†ä¼šå¼•å…¥æ— å…³èŠ‚ç‚¹çš„å™ªå£°å’Œä¸¢å¤±ç»“æ„ä¿¡æ¯ã€‚å—è‡ªç„¶è¯­è¨€å¤„ç†ä¸­ç»†ç²’åº¦ä»¤ç‰Œè¡¨ç¤ºå­¦ä¹ çš„å¯å‘ï¼Œæå‡ºç»“æ„æ„ŸçŸ¥å¤šä»¤ç‰Œå›¾Transformerï¼ˆTokenphormerï¼‰ï¼Œé€šè¿‡ç”Ÿæˆå¤šä¸ªä»¤ç‰Œæœ‰æ•ˆæ•è·å±€éƒ¨å’Œç»“æ„ä¿¡æ¯ï¼Œå¹¶åœ¨ä¸åŒç²’åº¦çº§åˆ«ä¸Šæ¢ç´¢å…¨å±€ä¿¡æ¯ã€‚é€šè¿‡æ··åˆèµ°ç”Ÿæˆçš„walk-tokenã€é€šè¿‡è‡ªç›‘ç£å›¾é¢„è®­ç»ƒæ¨¡å‹SGPMè·å¾—çš„SGPM-tokenä»¥åŠæ‰©å±•walk-tokené•¿åº¦å’Œå¯†åº¦çš„hop-tokenï¼Œè¿™äº›çµæ´»çš„ä»¤ç‰Œè¢«è¾“å…¥åˆ°Transformeræ¨¡å‹ä¸­ï¼ŒååŒå­¦ä¹ èŠ‚ç‚¹è¡¨ç¤ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTokenphormeråœ¨èŠ‚ç‚¹åˆ†ç±»ä»»åŠ¡ä¸Šè¾¾åˆ°æœ€æ–°æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GNNsé¢ä¸´è¿‡åº¦å¹³æ»‘å’Œè¿‡åº¦å‹ç¼©é—®é¢˜ï¼Œå½±å“äº†å…¶åœ¨æ¶ˆæ¯ä¼ é€’è¿‡ç¨‹ä¸­çš„æ„Ÿå—é‡ã€‚</li>
<li>å›¾Transformerè§£å†³äº†ä¸Šè¿°é—®é¢˜ï¼Œä½†å¼•å…¥äº†æ— å…³èŠ‚ç‚¹çš„å™ªå£°å’Œä¸¢å¤±ç»“æ„ä¿¡æ¯çš„é—®é¢˜ã€‚</li>
<li>Tokenphormeré€šè¿‡ç”Ÿæˆå¤šä¸ªä»¤ç‰Œï¼ˆå¦‚walk-token, SGPM-token, hop-tokenï¼‰æ¥æœ‰æ•ˆæ•è·å±€éƒ¨å’Œç»“æ„ä¿¡æ¯ï¼Œå¹¶æ¢ç´¢ä¸åŒç²’åº¦çº§åˆ«çš„å…¨å±€ä¿¡æ¯ã€‚</li>
<li>Tokenphormerç»“åˆäº†å›¾æ•°æ®çš„å±€éƒ¨å’Œå…¨å±€ä¿¡æ¯ï¼Œé€šè¿‡æ··åˆèµ°å’Œè‡ªç›‘ç£å›¾é¢„è®­ç»ƒæ–¹æ³•ç”Ÿæˆå¤šç§ä»¤ç‰Œã€‚</li>
<li>è¿™äº›ä»¤ç‰Œè¢«è¾“å…¥åˆ°Transformeræ¨¡å‹ä¸­ï¼Œä»¥ååŒå­¦ä¹ èŠ‚ç‚¹è¡¨ç¤ºã€‚</li>
<li>Tokenphormeråœ¨èŠ‚ç‚¹åˆ†ç±»ä»»åŠ¡ä¸Šå®ç°äº†å…ˆè¿›æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15302">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5c568cdf77afeb4ae4181cbf6ecae01a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-69955077a56e29cf80dce0c014aefcd7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-339dbb64d7fd785f4ff0ad943f0c8e94.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2c7d8df9c5de041c818bb8713ebaeca6.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="All-in-One-Tuning-and-Structural-Pruning-for-Domain-Specific-LLMs"><a href="#All-in-One-Tuning-and-Structural-Pruning-for-Domain-Specific-LLMs" class="headerlink" title="All-in-One Tuning and Structural Pruning for Domain-Specific LLMs"></a>All-in-One Tuning and Structural Pruning for Domain-Specific LLMs</h2><p><strong>Authors:Lei Lu, Zhepeng Wang, Runxue Bao, Mengbing Wang, Fangyi Li, Yawen Wu, Weiwen Jiang, Jie Xu, Yanzhi Wang, Shangqian Gao</strong></p>
<p>Existing pruning techniques for large language models (LLMs) targeting domain-specific applications typically follow a two-stage process: pruning the pretrained general-purpose LLMs and then fine-tuning the pruned LLMs on specific domains. However, the pruning decisions, derived from the pretrained weights, remain unchanged during fine-tuning, even if the weights have been updated. Therefore, such a combination of the pruning decisions and the finetuned weights may be suboptimal, leading to non-negligible performance degradation. To address these limitations, we propose ATP: All-in-One Tuning and Structural Pruning, a unified one-stage structural pruning and fine-tuning approach that dynamically identifies the current optimal substructure throughout the fine-tuning phase via a trainable pruning decision generator. Moreover, given the limited available data for domain-specific applications, Low-Rank Adaptation (LoRA) becomes a common technique to fine-tune the LLMs. In ATP, we introduce LoRA-aware forward and sparsity regularization to ensure that the substructures corresponding to the learned pruning decisions can be directly removed after the ATP process. ATP outperforms the state-of-the-art two-stage pruning methods on tasks in the legal and healthcare domains. More specifically, ATP recovers up to 88% and 91% performance of the dense model when pruning 40% parameters of LLaMA2-7B and LLaMA3-8B models, respectively. </p>
<blockquote>
<p>é’ˆå¯¹ç‰¹å®šé¢†åŸŸåº”ç”¨çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç°æœ‰å‰ªææŠ€æœ¯é€šå¸¸éµå¾ªä¸¤é˜¶æ®µè¿‡ç¨‹ï¼šå‰ªæé¢„è®­ç»ƒçš„é€šç”¨LLMï¼Œç„¶ååœ¨ç‰¹å®šé¢†åŸŸå¯¹å‰ªæåçš„LLMè¿›è¡Œå¾®è°ƒã€‚ç„¶è€Œï¼Œå‰ªæå†³ç­–æ¥æºäºé¢„è®­ç»ƒæƒé‡ï¼Œå³ä½¿åœ¨æƒé‡å·²æ›´æ–°çš„æƒ…å†µä¸‹ï¼Œå¾®è°ƒè¿‡ç¨‹ä¸­å‰ªæå†³ç­–ä»ç„¶ä¿æŒä¸å˜ã€‚å› æ­¤ï¼Œå‰ªæå†³ç­–å’Œå¾®è°ƒæƒé‡çš„è¿™ç§ç»„åˆå¯èƒ½æ˜¯æ¬¡ä¼˜çš„ï¼Œä¼šå¯¼è‡´ä¸å¯å¿½ç•¥çš„æ€§èƒ½ä¸‹é™ã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†ATPï¼šä¸€ç«™å¼è°ƒä¼˜å’Œç»“æ„å‰ªæï¼Œè¿™æ˜¯ä¸€ç§ç»Ÿä¸€çš„å•é˜¶æ®µç»“æ„å‰ªæå’Œå¾®è°ƒæ–¹æ³•ï¼Œå®ƒé€šè¿‡å¯è®­ç»ƒçš„å‰ªæå†³ç­–ç”Ÿæˆå™¨åŠ¨æ€åœ°è¯†åˆ«æ•´ä¸ªå¾®è°ƒé˜¶æ®µçš„å½“å‰æœ€ä½³å­ç»“æ„ã€‚æ­¤å¤–ï¼Œè€ƒè™‘åˆ°é’ˆå¯¹ç‰¹å®šé¢†åŸŸçš„æœ‰é™å¯ç”¨æ•°æ®ï¼Œä½ç§©é€‚åº”ï¼ˆLoRAï¼‰å·²æˆä¸ºå¾®è°ƒLLMçš„å¸¸è§æŠ€æœ¯ã€‚åœ¨ATPä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†LoRAæ„ŸçŸ¥æ­£å‘ä¼ æ’­å’Œç¨€ç–æ€§æ­£åˆ™åŒ–ï¼Œä»¥ç¡®ä¿ä¸æ‰€å­¦å‰ªæå†³ç­–ç›¸å¯¹åº”çš„å­ç»“æ„å¯ä»¥åœ¨ATPè¿‡ç¨‹åç›´æ¥è¢«ç§»é™¤ã€‚ATPåœ¨æ³•å¾‹å’ŒåŒ»ç–—ä¿å¥é¢†åŸŸçš„ä»»åŠ¡ä¸Šä¼˜äºæœ€æ–°çš„ä¸¤é˜¶æ®µå‰ªææ–¹æ³•ã€‚æ›´å…·ä½“åœ°è¯´ï¼ŒATPåœ¨å‰ªæLLaMA2-7Bæ¨¡å‹çš„40%å‚æ•°å’ŒLLaMA3-8Bæ¨¡å‹çš„å‚æ•°æ—¶ï¼Œåˆ†åˆ«æ¢å¤äº†å¯†é›†æ¨¡å‹æ€§èƒ½çš„88%å’Œ91%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.14426v2">PDF</a> Updated a typo in the author list;</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä¿®å‰ªæŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯é¢å‘ç‰¹å®šé¢†åŸŸåº”ç”¨é€šå¸¸é‡‡ç”¨ä¸¤é˜¶æ®µè¿‡ç¨‹ï¼šå…ˆä¿®å‰ªé¢„è®­ç»ƒé€šç”¨LLMï¼Œç„¶åå¯¹ä¿®å‰ªåçš„LLMè¿›è¡Œç‰¹å®šé¢†åŸŸçš„å¾®è°ƒã€‚ç„¶è€Œï¼Œä»é¢„è®­ç»ƒæƒé‡ä¸­å¾—å‡ºçš„ä¿®å‰ªå†³ç­–åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­ä¿æŒä¸å˜ï¼Œå³ä½¿æƒé‡å·²æ›´æ–°ã€‚å› æ­¤ï¼Œè¿™ç§ä¿®å‰ªå†³ç­–ä¸å¾®è°ƒæƒé‡çš„ç»„åˆå¯èƒ½ä¸æ˜¯æœ€ä¼˜çš„ï¼Œä¼šå¯¼è‡´æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ATPï¼šä¸€ç«™å¼ç»“æ„ä¿®å‰ªä¸ç²¾ç»†è°ƒæ•´æ–¹æ³•ï¼Œé€šè¿‡å¯è®­ç»ƒçš„ä¿®å‰ªå†³ç­–ç”Ÿæˆå™¨åœ¨å¾®è°ƒé˜¶æ®µåŠ¨æ€è¯†åˆ«å½“å‰æœ€ä½³å­ç»“æ„ã€‚æ­¤å¤–ï¼Œè€ƒè™‘åˆ°é¢†åŸŸç‰¹å®šåº”ç”¨å¯ç”¨æ•°æ®çš„æœ‰é™æ€§ï¼ŒLoRAæˆä¸ºä¸€ç§å¸¸è§çš„LLMå¾®è°ƒæŠ€æœ¯ã€‚åœ¨ATPä¸­ï¼Œå¼•å…¥LoRAæ„ŸçŸ¥æ­£å‘å’Œç¨€ç–æ€§æ­£åˆ™åŒ–ï¼Œä»¥ç¡®ä¿ä¸å­¦åˆ°çš„ä¿®å‰ªå†³ç­–ç›¸å¯¹åº”çš„å­ç»“æ„å¯ä»¥åœ¨ATPè¿‡ç¨‹åç›´æ¥è¢«ç§»é™¤ã€‚ATPåœ¨æ³•å¾‹å’ŒåŒ»ç–—é¢†åŸŸçš„ä»»åŠ¡ä¸Šä¼˜äºæœ€æ–°çš„ä¸¤é˜¶æ®µä¿®å‰ªæ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼ŒATPåœ¨ä¿®å‰ªLLaMA2-7Bå’ŒLLaMA3-8Bæ¨¡å‹çš„40%å‚æ•°æ—¶ï¼Œèƒ½æ¢å¤é«˜è¾¾88%å’Œ91%çš„å¯†é›†æ¨¡å‹æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰é’ˆå¯¹LLMçš„ä¿®å‰ªæŠ€æœ¯é€šå¸¸é‡‡å–ä¸¤é˜¶æ®µè¿‡ç¨‹ï¼ŒåŒ…æ‹¬é¢„è®­ç»ƒä¿®å‰ªå’Œç‰¹å®šé¢†åŸŸå¾®è°ƒã€‚</li>
<li>åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­ï¼Œä¿®å‰ªå†³ç­–ä¿æŒä¸å˜å¯èƒ½å¯¼è‡´éæœ€ä¼˜ç»“æœå’Œæ€§èƒ½ä¸‹é™ã€‚</li>
<li>ATPæ–¹æ³•é€šè¿‡ç»“åˆç»“æ„ä¿®å‰ªå’Œç²¾ç»†è°ƒæ•´ï¼Œåœ¨å¾®è°ƒé˜¶æ®µåŠ¨æ€è¯†åˆ«æœ€ä½³å­ç»“æ„ã€‚</li>
<li>ATPå¼•å…¥LoRAæ„ŸçŸ¥æ­£å‘å’Œç¨€ç–æ€§æ­£åˆ™åŒ–ï¼Œç¡®ä¿åœ¨ATPè¿‡ç¨‹åç§»é™¤ä¸å­¦åˆ°çš„ä¿®å‰ªå†³ç­–ç›¸å¯¹åº”çš„å­ç»“æ„ã€‚</li>
<li>ATPåœ¨æ³•å¾‹å’ŒåŒ»ç–—é¢†åŸŸçš„ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>ATPåœ¨ä¿®å‰ª40%å‚æ•°æ—¶èƒ½å¤Ÿæ˜¾è‘—æ¢å¤æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.14426">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e3256a5916ed4996a40486f8b0d50421.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b961ced26e75e31f7781cdb5838c2d5d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-985b6ead7aa192a769c3656ea7af8b48.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Experience-of-Training-a-1-7B-Parameter-LLaMa-Model-From-Scratch"><a href="#Experience-of-Training-a-1-7B-Parameter-LLaMa-Model-From-Scratch" class="headerlink" title="Experience of Training a 1.7B-Parameter LLaMa Model From Scratch"></a>Experience of Training a 1.7B-Parameter LLaMa Model From Scratch</h2><p><strong>Authors:Miles Q. Li, Benjamin C. M. Fung, Shih-Chia Huang</strong></p>
<p>Pretraining large language models is a complex endeavor influenced by multiple factors, including model architecture, data quality, training continuity, and hardware constraints. In this paper, we share insights gained from the experience of training DMaS-LLaMa-Lite, a fully open source, 1.7-billion-parameter, LLaMa-based model, on approximately 20 billion tokens of carefully curated data. We chronicle the full training trajectory, documenting how evolving validation loss levels and downstream benchmarks reflect transitions from incoherent text to fluent, contextually grounded output. Beyond pretraining, we extend our analysis to include a post-training phase focused on instruction tuning, where the model was refined to produce more contextually appropriate, user-aligned responses. We highlight practical considerations such as the importance of restoring optimizer states when resuming from checkpoints, and the impact of hardware changes on training stability and throughput. While qualitative evaluation provides an intuitive understanding of model improvements, our analysis extends to various performance benchmarks, demonstrating how high-quality data and thoughtful scaling enable competitive results with significantly fewer training tokens. By detailing these experiences and offering training logs, checkpoints, and sample outputs, we aim to guide future researchers and practitioners in refining their pretraining strategies. The training script is available on Github at <a target="_blank" rel="noopener" href="https://github.com/McGill-DMaS/DMaS-LLaMa-Lite-Training-Code">https://github.com/McGill-DMaS/DMaS-LLaMa-Lite-Training-Code</a>. The model checkpoints are available on Huggingface at <a target="_blank" rel="noopener" href="https://huggingface.co/collections/McGill-DMaS/dmas-llama-lite-6761d97ba903f82341954ceb">https://huggingface.co/collections/McGill-DMaS/dmas-llama-lite-6761d97ba903f82341954ceb</a>. </p>
<blockquote>
<p>é¢„è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹æ˜¯ä¸€é¡¹å¤æ‚çš„ä»»åŠ¡ï¼Œå—åˆ°å¤šç§å› ç´ çš„å½±å“ï¼ŒåŒ…æ‹¬æ¨¡å‹æ¶æ„ã€æ•°æ®è´¨é‡ã€è®­ç»ƒè¿ç»­æ€§å’Œç¡¬ä»¶é™åˆ¶ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬åˆ†äº«äº†è®­ç»ƒDMaS-LLaMa-Liteæ¨¡å‹çš„å®æˆ˜ç»éªŒã€‚è¿™æ˜¯ä¸€ä¸ªå®Œå…¨å¼€æºçš„ã€åŸºäºLLaMaçš„1.7äº¿å‚æ•°æ¨¡å‹ï¼Œåœ¨çº¦20äº¿æ ‡è®°çš„ç²¾å¿ƒæŒ‘é€‰çš„æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒã€‚æˆ‘ä»¬è¯¦ç»†è®°å½•äº†æ•´ä¸ªè®­ç»ƒè½¨è¿¹ï¼Œæè¿°äº†éªŒè¯æŸå¤±æ°´å¹³çš„æ¼”å˜å’Œä¸‹æ¸¸åŸºå‡†æµ‹è¯•å¦‚ä½•åæ˜ ä»æ–‡æœ¬ä¸è¿è´¯åˆ°æµç•…ã€ä¸Šä¸‹æ–‡ç›¸å…³çš„è¾“å‡ºçš„è½¬å˜ã€‚é™¤äº†é¢„è®­ç»ƒé˜¶æ®µå¤–ï¼Œæˆ‘ä»¬è¿˜å°†åˆ†ææ‰©å±•åˆ°åŒ…æ‹¬ä»¥æŒ‡ä»¤å¾®è°ƒä¸ºé‡ç‚¹çš„åè®­ç»ƒé˜¶æ®µï¼Œå…¶ä¸­å¯¹æ¨¡å‹è¿›è¡Œäº†å¾®è°ƒï¼Œä»¥äº§ç”Ÿæ›´è´´åˆä¸Šä¸‹æ–‡ã€æ›´ç¬¦åˆç”¨æˆ·éœ€æ±‚çš„å“åº”ã€‚æˆ‘ä»¬å¼ºè°ƒäº†å®é™…è€ƒè™‘å› ç´ ï¼Œå¦‚æ¢å¤ä¼˜åŒ–å™¨çŠ¶æ€æ—¶é‡å¯çš„é‡è¦æ€§ï¼Œä»¥åŠç¡¬ä»¶æ›´æ”¹å¯¹è®­ç»ƒç¨³å®šæ€§å’Œååé‡çš„å½±å“ã€‚è™½ç„¶å®šæ€§è¯„ä¼°ä¸ºæ¨¡å‹æ”¹è¿›æä¾›äº†ç›´è§‚çš„ç†è§£ï¼Œä½†æˆ‘ä»¬çš„åˆ†æè¿˜æ¶‰åŠå„ç§æ€§èƒ½åŸºå‡†æµ‹è¯•ï¼Œè¯æ˜é«˜è´¨é‡æ•°æ®å’Œæ·±æ€ç†Ÿè™‘çš„æ‰©å±•å¦‚ä½•ä½¿æˆ‘ä»¬åœ¨ä½¿ç”¨æ›´å°‘çš„è®­ç»ƒæ ‡è®°çš„æƒ…å†µä¸‹è·å¾—æœ‰ç«äº‰åŠ›çš„ç»“æœã€‚é€šè¿‡è¯¦ç»†ä»‹ç»è¿™äº›ç»éªŒå¹¶æä¾›è®­ç»ƒæ—¥å¿—ã€æ£€æŸ¥ç‚¹å’Œæ ·æœ¬è¾“å‡ºï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯æŒ‡å¯¼æœªæ¥çš„ç ”ç©¶äººå‘˜å’Œå®è·µè€…åœ¨ç²¾ç‚¼ä»–ä»¬çš„é¢„è®­ç»ƒç­–ç•¥æ–¹é¢ã€‚è®­ç»ƒè„šæœ¬å¯åœ¨GitHubä¸Šæ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/McGill-DMaS/DMaS-LLaMa-Lite-Training-Code">é“¾æ¥</a>ã€‚æ¨¡å‹æ£€æŸ¥ç‚¹å¯åœ¨Huggingfaceä¸Šæ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://huggingface.co/collections/McGill-DMaS/dmas-llama-lite-6761d97ba903f82341954ceb">é“¾æ¥</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.13335v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è®­ç»ƒDMaS-LLaMa-Liteè¿™ä¸€å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹çš„å®è·µç»éªŒã€‚æ¨¡å‹åœ¨çº¦20äº¿ä¸ªç²¾å¿ƒæŒ‘é€‰çš„ä»¤ç‰Œä¸Šè¿›è¡Œè®­ç»ƒï¼Œé‡‡ç”¨LLaMaæ¶æ„ï¼Œå«æœ‰1.7äº¿ä¸ªå‚æ•°ã€‚æ–‡ç« è¯¦ç»†æè¿°äº†è®­ç»ƒè¿‡ç¨‹ï¼ŒåŒ…æ‹¬é¢„è®­ç»ƒå’ŒæŒ‡ä»¤è°ƒæ•´é˜¶æ®µï¼Œå¹¶æ¢è®¨äº†æ¨¡å‹æ¶æ„ã€æ•°æ®è´¨é‡ã€è®­ç»ƒè¿ç»­æ€§ä»¥åŠç¡¬ä»¶çº¦æŸç­‰å¤šä¸ªå½±å“å› ç´ ã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜å¼ºè°ƒäº†æ¢å¤ä¼˜åŒ–å™¨çŠ¶æ€çš„é‡è¦æ€§ä»¥åŠç¡¬ä»¶å˜åŒ–å¯¹è®­ç»ƒç¨³å®šæ€§å’Œæ•ˆç‡çš„å½±å“ã€‚é€šè¿‡æ€§èƒ½åŸºå‡†æµ‹è¯•è¯æ˜ï¼Œé«˜è´¨é‡æ•°æ®å’Œç­–ç•¥æ€§æ‰©å±•å¯å®ç°å…·æœ‰ç«äº‰åŠ›çš„ç»“æœï¼Œæ˜¾è‘—å‡å°‘è®­ç»ƒä»¤ç‰Œæ•°é‡ã€‚æœ¬æ–‡æ—¨åœ¨æŒ‡å¯¼æœªæ¥ç ”ç©¶è€…å’Œå®è·µè€…åœ¨é¢„è®­ç»ƒç­–ç•¥æ–¹é¢çš„æ”¹è¿›ï¼Œå¹¶æä¾›äº†è®­ç»ƒæ—¥å¿—ã€æ£€æŸ¥ç‚¹ä»¥åŠæ ·æœ¬è¾“å‡ºã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹æ˜¯ä¸€ä¸ªå—å¤šç§å› ç´ å½±å“çš„å¤æ‚è¿‡ç¨‹ï¼ŒåŒ…æ‹¬æ¨¡å‹æ¶æ„ã€æ•°æ®è´¨é‡ã€è®­ç»ƒè¿ç»­æ€§å’Œç¡¬ä»¶çº¦æŸç­‰ã€‚</li>
<li>DMaS-LLaMa-Liteæ˜¯ä¸€ä¸ªåŸºäºLLaMaçš„å¼€æºæ¨¡å‹ï¼Œåœ¨çº¦20äº¿ä¸ªä»¤ç‰Œä¸Šè¿›è¡Œè®­ç»ƒï¼Œå‚æ•°ä¸º1.7äº¿ã€‚</li>
<li>æ–‡ç« è¯¦ç»†æè¿°äº†ä»é¢„è®­ç»ƒåˆ°æŒ‡ä»¤è°ƒæ•´é˜¶æ®µçš„æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ã€‚</li>
<li>å¼ºè°ƒäº†åœ¨æ¢å¤ä¼˜åŒ–å™¨çŠ¶æ€æ—¶çš„æ³¨æ„äº‹é¡¹ä»¥åŠç¡¬ä»¶å˜åŒ–å¯¹è®­ç»ƒç¨³å®šæ€§å’Œæ•ˆç‡çš„å½±å“ã€‚</li>
<li>é«˜è´¨é‡æ•°æ®å’Œç­–ç•¥æ€§æ‰©å±•å¯å®ç°å…·æœ‰ç«äº‰åŠ›çš„ç»“æœï¼Œæ˜¾è‘—å‡å°‘è®­ç»ƒä»¤ç‰Œæ•°é‡ã€‚</li>
<li>é€šè¿‡æ€§èƒ½åŸºå‡†æµ‹è¯•è¯æ˜äº†è¯¥æ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>æ–‡ç« æä¾›äº†è®­ç»ƒæ—¥å¿—ã€æ£€æŸ¥ç‚¹ä»¥åŠæ ·æœ¬è¾“å‡ºï¼Œæ—¨åœ¨ä¸ºæœªæ¥çš„ç ”ç©¶è€…å’Œå®è·µè€…æä¾›æŒ‡å¯¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.13335">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-abdfa1d2d5f9cf37ecf17eab05fce676.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9e2be79e4998d631382fa15da91e615f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9424bc8949d5210d5dca0c2d98222831.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-519cc9e48210b092161cd38faa5c970f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2f0e3d6fd975554263395cc4c4dcd625.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4a516394a7b3382d91c41f90f6ce6796.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0aff8f650be73b6271c3f31485872b84.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Utilize-the-Flow-before-Stepping-into-the-Same-River-Twice-Certainty-Represented-Knowledge-Flow-for-Refusal-Aware-Instruction-Tuning"><a href="#Utilize-the-Flow-before-Stepping-into-the-Same-River-Twice-Certainty-Represented-Knowledge-Flow-for-Refusal-Aware-Instruction-Tuning" class="headerlink" title="Utilize the Flow before Stepping into the Same River Twice: Certainty   Represented Knowledge Flow for Refusal-Aware Instruction Tuning"></a>Utilize the Flow before Stepping into the Same River Twice: Certainty   Represented Knowledge Flow for Refusal-Aware Instruction Tuning</h2><p><strong>Authors:Runchuan Zhu, Zhipeng Ma, Jiang Wu, Junyuan Gao, Jiaqi Wang, Dahua Lin, Conghui He</strong></p>
<p>Refusal-Aware Instruction Tuning (RAIT) enables Large Language Models (LLMs) to refuse to answer unknown questions. By modifying responses of unknown questions in the training data to refusal responses such as â€œI donâ€™t knowâ€, RAIT enhances the reliability of LLMs and reduces their hallucination. Generally, RAIT modifies training samples based on the correctness of the initial LLMâ€™s response. However, this crude approach can cause LLMs to excessively refuse answering questions they could have correctly answered, the problem we call over-refusal. In this paper, we explore two primary causes of over-refusal: Static conflict occurs when similar samples within the LLMâ€™s feature space receive differing supervision signals (original vs. modified â€œI donâ€™t knowâ€). Dynamic conflict arises as the LLMâ€™s evolving knowledge during SFT enables it to answer previously unanswerable questions, but the now-answerable training samples still retain the original â€œI donâ€™t knowâ€ supervision signals from the initial LLM state, leading to inconsistencies. These conflicts cause the trained LLM to misclassify known questions as unknown, resulting in over-refusal. To address this issue, we introduce Certainty Represented Knowledge Flow for Refusal-Aware Instructions Tuning (CRaFT). CRaFT centers on two main contributions: First, we additionally incorporate response certainty to selectively filter and modify data, reducing static conflicts. Second, we implement preliminary rehearsal training to characterize changes in the LLMâ€™s knowledge state, which helps mitigate dynamic conflicts during the fine-tuning process. We conducted extensive experiments on open-ended question answering and multiple-choice question task. Experiment results show that CRaFT can improve LLMâ€™s overall performance during the RAIT process. Code and data will be released at <a target="_blank" rel="noopener" href="https://github.com/opendatalab/CRaFT">https://github.com/opendatalab/CRaFT</a> . </p>
<blockquote>
<p>æ‹’ç»æ„ŸçŸ¥æŒ‡ä»¤è°ƒæ•´ï¼ˆRAITï¼‰èƒ½å¤Ÿä½¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ‹’ç»å›ç­”æœªçŸ¥é—®é¢˜ã€‚é€šè¿‡å°†è®­ç»ƒæ•°æ®ä¸­å¯¹æœªçŸ¥é—®é¢˜çš„å›åº”ä¿®æ”¹ä¸ºæ‹’ç»å›åº”ï¼Œä¾‹å¦‚â€œæˆ‘ä¸çŸ¥é“â€ï¼ŒRAITæé«˜äº†LLMçš„å¯é æ€§ï¼Œå¹¶å‡å°‘äº†å…¶å¹»è§‰ã€‚é€šå¸¸ï¼ŒRAITä¼šæ ¹æ®åˆå§‹LLMå“åº”çš„æ­£ç¡®æ€§æ¥ä¿®æ”¹è®­ç»ƒæ ·æœ¬ã€‚ç„¶è€Œï¼Œè¿™ç§ç®€å•çš„æ–¹æ³•å¯èƒ½ä¼šå¯¼è‡´LLMè¿‡åº¦æ‹’ç»å›ç­”ä»–ä»¬æœ¬æ¥å¯ä»¥æ­£ç¡®å›ç­”çš„é—®é¢˜ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºâ€œè¿‡åº¦æ‹’ç»â€ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ¢è®¨äº†è¿‡åº¦æ‹’ç»çš„ä¸¤ä¸ªä¸»è¦åŸå› ï¼šé™æ€å†²çªå‘ç”Ÿåœ¨LLMç‰¹å¾ç©ºé—´å†…çš„ç›¸ä¼¼æ ·æœ¬æ¥æ”¶åˆ°ä¸åŒçš„ç›‘ç£ä¿¡å·ï¼ˆåŸå§‹ä¿¡å·ä¸ä¿®æ”¹åçš„â€œæˆ‘ä¸çŸ¥é“â€ï¼‰æ—¶ã€‚åŠ¨æ€å†²çªéšç€LLMåœ¨SFTæœŸé—´çš„çŸ¥è¯†è¿›æ­¥ï¼Œå®ƒèƒ½å¤Ÿå›ç­”ä¹‹å‰æ— æ³•å›ç­”çš„é—®é¢˜ï¼Œä½†ç°åœ¨å¯å›ç­”çš„è®­ç»ƒæ ·æœ¬ä»ç„¶ä¿ç•™ç€åˆå§‹LLMçŠ¶æ€çš„â€œæˆ‘ä¸çŸ¥é“â€çš„ç›‘ç£ä¿¡å·ï¼Œå¯¼è‡´ä¸ä¸€è‡´ã€‚è¿™äº›å†²çªå¯¼è‡´è®­ç»ƒåçš„LLMå°†å·²çŸ¥é—®é¢˜é”™è¯¯åœ°å½’ç±»ä¸ºæœªçŸ¥é—®é¢˜ï¼Œä»è€Œäº§ç”Ÿè¿‡åº¦æ‹’ç»ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ‹’ç»æ„ŸçŸ¥æŒ‡ä»¤è°ƒæ•´çš„ç¡®å®šæ€§è¡¨ç¤ºçŸ¥è¯†æµï¼ˆCRaFTï¼‰ã€‚CRaFTä¸»è¦é›†ä¸­äºä¸¤ä¸ªä¸»è¦è´¡çŒ®ï¼šé¦–å…ˆï¼Œæˆ‘ä»¬é¢å¤–å¼•å…¥å“åº”ç¡®å®šæ€§æ¥æœ‰é€‰æ‹©åœ°è¿‡æ»¤å’Œä¿®æ”¹æ•°æ®ï¼Œä»¥å‡å°‘é™æ€å†²çªã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å®æ–½åˆæ­¥æ’ç»ƒè®­ç»ƒä»¥è¡¨å¾LLMçŸ¥è¯†çŠ¶æ€çš„å˜åŒ–ï¼Œè¿™æœ‰åŠ©äºåœ¨å¾®è°ƒè¿‡ç¨‹ä¸­ç¼“è§£åŠ¨æ€å†²çªã€‚æˆ‘ä»¬åœ¨å¼€æ”¾å¼é—®ç­”å’Œå¤šé¡¹é€‰æ‹©ä»»åŠ¡ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCRaFTåœ¨RAITè¿‡ç¨‹ä¸­èƒ½å¤Ÿæé«˜LLMçš„æ•´ä½“æ€§èƒ½ã€‚ä»£ç å’Œæ•°æ®å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/opendatalab/CRaFT%E4%B8%8A%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/opendatalab/CRaFTä¸Šå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.06913v3">PDF</a> Equal contribution: Runchuan Zhu, Zhipeng Ma, Jiang Wu; Corresponding   author: Conghui He</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¦‚ä½•é€šè¿‡æ‹’ç»å›ç­”æœªçŸ¥é—®é¢˜æ¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¯é æ€§å¹¶å‡å°‘å…¶å¹»è§‰ã€‚é€šè¿‡ä¿®æ”¹è®­ç»ƒæ•°æ®ä¸­æœªçŸ¥é—®é¢˜çš„å“åº”ä¸ºæ‹’ç»å›ç­”ï¼ˆå¦‚â€œæˆ‘ä¸çŸ¥é“â€ï¼‰ï¼Œå®ç°äº†ä¸€ç§åä¸ºRAITçš„æ–¹æ³•ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•å¯èƒ½å¯¼è‡´LLMè¿‡åº¦æ‹’ç»å›ç­”é—®é¢˜ï¼Œç§°ä¸ºè¿‡åº¦æ‹’ç»ç°è±¡ã€‚æœ¬æ–‡æ¢è®¨äº†è¿‡åº¦æ‹’ç»çš„ä¸¤ä¸ªä¸»è¦åŸå› ï¼šé™æ€å†²çªå’ŒåŠ¨æ€å†²çªã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œå¼•å…¥äº†CRaFTæ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥å“åº”ç¡®å®šæ€§æ¥è¿‡æ»¤å’Œä¿®æ”¹æ•°æ®ï¼Œå¹¶åˆæ­¥æ’ç»ƒè®­ç»ƒä»¥è¡¨å¾LLMçŸ¥è¯†çŠ¶æ€çš„å˜åŒ–ï¼Œä»è€Œå‡è½»RAITè¿‡ç¨‹ä¸­çš„å†²çªé—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCRaFTåœ¨æ”¹å–„LLMæ€§èƒ½æ–¹é¢æœ‰æ˜æ˜¾æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RAITé€šè¿‡ä¿®æ”¹è®­ç»ƒæ•°æ®ä¸­çš„å“åº”æ¥æé«˜LLMçš„å¯é æ€§ã€‚</li>
<li>è¿‡åº¦æ‹’ç»æ˜¯RAITçš„ä¸€ä¸ªæ½œåœ¨é—®é¢˜ï¼Œå¯èƒ½å¯¼è‡´LLMé”™è¯¯åœ°æ‹’ç»å›ç­”å·²çŸ¥é—®é¢˜ã€‚</li>
<li>é™æ€å†²çªå’ŒåŠ¨æ€å†²çªæ˜¯å¯¼è‡´è¿‡åº¦æ‹’ç»çš„ä¸¤ä¸ªä¸»è¦åŸå› ã€‚</li>
<li>CRaFTé€šè¿‡å¼•å…¥å“åº”ç¡®å®šæ€§æ¥å‡å°‘é™æ€å†²çªï¼Œå¹¶é€šè¿‡åˆæ­¥æ’ç»ƒè®­ç»ƒæ¥è¡¨å¾LLMçŸ¥è¯†çŠ¶æ€çš„å˜åŒ–æ¥è§£å†³åŠ¨æ€å†²çªã€‚</li>
<li>CRaFTåœ¨æ”¹å–„LLMæ€§èƒ½æ–¹é¢æœ‰æ˜æ˜¾æ•ˆæœï¼Œå°¤å…¶æ˜¯åœ¨å¼€æ”¾é—®é¢˜å’Œå¤šé€‰é—®ç­”ä»»åŠ¡ä¸Šã€‚</li>
<li>ä»£ç å’Œæ•°æ®å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/opendatalab/CRaFT%E4%B8%8A%E5%8F%91%E5%B8%83%E4%BB%A5%E4%BE%9B%E7%A0%94%E7%A9%B6%E4%BD%BF%E7%94%A8%E3%80%82">https://github.com/opendatalab/CRaFTä¸Šå‘å¸ƒä»¥ä¾›ç ”ç©¶ä½¿ç”¨ã€‚</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.06913">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-4720dcecc53bb89706472b548c3a35a2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0800834edec2772c7f58f0505765303e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-957e239cef74f6338cec67e35ad4d97f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d366284de68db132244f3ed331f435c0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2ffd4e9bd2842b1cf53e0dec9deddf33.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-75328909be59e75f5c4daa58bdd3814e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d7df1ba0670edf95a5e095ff61542551.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="What-is-the-Role-of-Small-Models-in-the-LLM-Era-A-Survey"><a href="#What-is-the-Role-of-Small-Models-in-the-LLM-Era-A-Survey" class="headerlink" title="What is the Role of Small Models in the LLM Era: A Survey"></a>What is the Role of Small Models in the LLM Era: A Survey</h2><p><strong>Authors:Lihu Chen, GaÃ«l Varoquaux</strong></p>
<p>Large Language Models (LLMs) have made significant progress in advancing artificial general intelligence (AGI), leading to the development of increasingly large models such as GPT-4 and LLaMA-405B. However, scaling up model sizes results in exponentially higher computational costs and energy consumption, making these models impractical for academic researchers and businesses with limited resources. At the same time, Small Models (SMs) are frequently used in practical settings, although their significance is currently underestimated. This raises important questions about the role of small models in the era of LLMs, a topic that has received limited attention in prior research. In this work, we systematically examine the relationship between LLMs and SMs from two key perspectives: Collaboration and Competition. We hope this survey provides valuable insights for practitioners, fostering a deeper understanding of the contribution of small models and promoting more efficient use of computational resources. The code is available at <a target="_blank" rel="noopener" href="https://github.com/tigerchen52/role_of_small_models">https://github.com/tigerchen52/role_of_small_models</a> </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ¨åŠ¨äººå·¥æ™ºèƒ½é€šç”¨æ™ºèƒ½ï¼ˆAGIï¼‰æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä»è€Œäº§ç”Ÿäº†å¦‚GPT-4å’ŒLLaMA-405Bç­‰è¶Šæ¥è¶Šå¤§çš„æ¨¡å‹ã€‚ç„¶è€Œï¼Œæ‰©å¤§æ¨¡å‹è§„æ¨¡å¯¼è‡´è®¡ç®—æˆæœ¬å’Œèƒ½æºæ¶ˆè€—å‘ˆæŒ‡æ•°çº§å¢é•¿ï¼Œä½¿å¾—è¿™äº›æ¨¡å‹å¯¹äºèµ„æºæœ‰é™çš„å­¦æœ¯ç ”ç©¶äººå‘˜å’Œä¼ä¸šæ¥è¯´ä¸åˆ‡å®é™…ã€‚ä¸æ­¤åŒæ—¶ï¼Œå°æ¨¡å‹ï¼ˆSMï¼‰åœ¨å®é™…æƒ…å†µä¸­ç»å¸¸è¢«ä½¿ç”¨ï¼Œå°½ç®¡å®ƒä»¬çš„é‡è¦æ€§ç›®å‰è¢«ä½ä¼°äº†ã€‚è¿™å¼•å‘äº†å…³äºå°æ¨¡å‹åœ¨å¤§å‹è¯­è¨€æ¨¡å‹æ—¶ä»£çš„ä½œç”¨çš„é‡è¦é—®é¢˜ï¼Œè¿™ä¸€ä¸»é¢˜åœ¨ä»¥å‰çš„ç ”ç©¶ä¸­å—åˆ°çš„å…³æ³¨æœ‰é™ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»åä½œå’Œç«äº‰ä¸¤ä¸ªå…³é”®è§’åº¦ç³»ç»Ÿåœ°ç ”ç©¶äº†å¤§å‹è¯­è¨€æ¨¡å‹å’Œå°æ¨¡å‹ä¹‹é—´çš„å…³ç³»ã€‚æˆ‘ä»¬å¸Œæœ›è¿™é¡¹è°ƒæŸ¥èƒ½ä¸ºä»ä¸šè€…æä¾›æœ‰ä»·å€¼çš„è§è§£ï¼ŒåŠ æ·±å¯¹å°å‹æ¨¡å‹çš„è´¡çŒ®çš„ç†è§£ï¼Œå¹¶ä¿ƒè¿›æ›´æœ‰æ•ˆåœ°åˆ©ç”¨è®¡ç®—èµ„æºã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/tigerchen52/role_of_small_models%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/tigerchen52/role_of_small_modelsæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.06857v4">PDF</a> a survey paper of small models</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨è¿›äººå·¥é€šç”¨æ™ºèƒ½ï¼ˆAGIï¼‰æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ä¹Ÿå¸¦æ¥äº†è®¡ç®—æˆæœ¬é«˜æ˜‚å’Œèƒ½æºæ¶ˆè€—å·¨å¤§çš„é—®é¢˜ï¼Œä½¿å¾—æœ‰é™èµ„æºçš„å­¦æœ¯æœºæ„å’Œä¼ä¸šåœ¨å®è·µä¸­éš¾ä»¥æ‰¿å—ã€‚ç›¸å¯¹ä¹‹ä¸‹ï¼Œå°å‹æ¨¡å‹ï¼ˆSMsï¼‰åœ¨å®é™…æƒ…å†µä¸­åº”ç”¨å¹¿æ³›å´å¸¸å¸¸å—åˆ°å¿½è§†ã€‚æœ¬ç ”ç©¶ç³»ç»Ÿåœ°æ¢è®¨äº†LLMsä¸SMsä¹‹é—´çš„å…³ç³»ï¼Œä»åä½œä¸ç«äº‰ä¸¤ä¸ªè§’åº¦åˆ‡å…¥ï¼Œæ—¨åœ¨ä¸ºä»ä¸šè€…æä¾›æœ‰ä»·å€¼çš„è§è§£ï¼Œæ·±åŒ–å¯¹å°å‹æ¨¡å‹è´¡çŒ®çš„ç†è§£ï¼Œå¹¶ä¿ƒè¿›è®¡ç®—èµ„æºçš„æœ‰æ•ˆåˆ©ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsåœ¨AGIé¢†åŸŸå–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†å¤§è§„æ¨¡æ¨¡å‹å¸¦æ¥çš„è®¡ç®—æˆæœ¬å’Œèƒ½æºæ¶ˆè€—é—®é¢˜ä½¿å¾—å…¶åœ¨æœ‰é™èµ„æºçš„ç¯å¢ƒä¸‹éš¾ä»¥å®ç°å¹¿æ³›åº”ç”¨ã€‚</li>
<li>å°å‹æ¨¡å‹ï¼ˆSMsï¼‰åœ¨å®è·µä¸­åº”ç”¨å¹¿æ³›ï¼Œä½†å…¶é‡è¦æ€§ç»å¸¸è¢«å¿½è§†ã€‚</li>
<li>LLMså’ŒSMsä¹‹é—´çš„å…³ç³»éœ€è¦ä»åä½œå’Œç«äº‰ä¸¤ä¸ªè§’åº¦è¿›è¡Œè€ƒå¯Ÿã€‚</li>
<li>åä½œè§’åº¦ï¼šLLMså’ŒSMså¯ä»¥ç›¸äº’è¡¥å……ï¼Œå…±åŒè§£å†³å¤æ‚ä»»åŠ¡ã€‚</li>
<li>ç«äº‰è§’åº¦ï¼šLLMså’ŒSMsåœ¨æŸäº›ä»»åŠ¡ä¸Šå¯èƒ½å­˜åœ¨æ€§èƒ½ç«äº‰ï¼Œéœ€è¦æ ¹æ®å…·ä½“ä»»åŠ¡é€‰æ‹©åˆé€‚çš„æ¨¡å‹è§„æ¨¡ã€‚</li>
<li>å¯¹å°å‹æ¨¡å‹çš„è´¡çŒ®éœ€è¦æ›´æ·±å…¥çš„ç†è§£ï¼Œä»¥ä¿ƒè¿›è®¡ç®—èµ„æºçš„æ›´æœ‰æ•ˆåˆ©ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.06857">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-65c4c35f37e4170c620e186c51d34f64.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-53193359c7affb47b92c474e9d9ad93a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-33a6279bb3577a97fecdf5cd2ce209c8.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Improving-Factuality-in-Large-Language-Models-via-Decoding-Time-Hallucinatory-and-Truthful-Comparators"><a href="#Improving-Factuality-in-Large-Language-Models-via-Decoding-Time-Hallucinatory-and-Truthful-Comparators" class="headerlink" title="Improving Factuality in Large Language Models via Decoding-Time   Hallucinatory and Truthful Comparators"></a>Improving Factuality in Large Language Models via Decoding-Time   Hallucinatory and Truthful Comparators</h2><p><strong>Authors:Dingkang Yang, Dongling Xiao, Jinjie Wei, Mingcheng Li, Zhaoyu Chen, Ke Li, Lihua Zhang</strong></p>
<p>Despite their remarkable capabilities, Large Language Models (LLMs) are prone to generate responses that contradict verifiable facts, i.e., unfaithful hallucination content. Existing efforts generally focus on optimizing model parameters or editing semantic representations, which compromise the internal factual knowledge of target LLMs. In addition, hallucinations typically exhibit multifaceted patterns in downstream tasks, limiting the modelâ€™s holistic performance across tasks. In this paper, we propose a Comparator-driven Decoding-Time (CDT) framework to alleviate the response hallucination. Firstly, we construct hallucinatory and truthful comparators with multi-task fine-tuning samples. In this case, we present an instruction prototype-guided mixture of experts strategy to enhance the ability of the corresponding comparators to capture different hallucination or truthfulness patterns in distinct task instructions. CDT constrains next-token predictions to factuality-robust distributions by contrasting the logit differences between the target LLMs and these comparators. Systematic experiments on multiple downstream tasks show that our framework can significantly improve the model performance and response factuality. </p>
<blockquote>
<p>å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å…·æœ‰æ˜¾è‘—çš„èƒ½åŠ›ï¼Œä½†å®ƒä»¬å®¹æ˜“äº§ç”Ÿä¸å¯éªŒè¯äº‹å®ç›¸çŸ›ç›¾çš„å›åº”ï¼Œå³ä¸çœŸå®çš„å¹»è§‰å†…å®¹ã€‚ç°æœ‰çš„åŠªåŠ›ä¸€èˆ¬é›†ä¸­åœ¨ä¼˜åŒ–æ¨¡å‹å‚æ•°æˆ–ç¼–è¾‘è¯­ä¹‰è¡¨ç¤ºä¸Šï¼Œè¿™ä¼šæŸå®³ç›®æ ‡LLMçš„å†…éƒ¨äº‹å®çŸ¥è¯†ã€‚æ­¤å¤–ï¼Œå¹»è§‰é€šå¸¸åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¤šé¢æ¨¡å¼ï¼Œé™åˆ¶äº†æ¨¡å‹åœ¨ä»»åŠ¡ä¸­çš„æ•´ä½“è¡¨ç°ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ¯”è¾ƒå™¨é©±åŠ¨çš„è§£ç æ—¶é—´ï¼ˆCDTï¼‰æ¡†æ¶æ¥ç¼“è§£å“åº”å¹»è§‰ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä½¿ç”¨å¤šä»»åŠ¡å¾®è°ƒæ ·æœ¬æ„å»ºå¹»è§‰å’ŒçœŸå®æ¯”è¾ƒå™¨ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªä»¥æŒ‡ä»¤åŸå‹ä¸ºæŒ‡å¯¼çš„ä¸“å®¶æ··åˆç­–ç•¥ï¼Œä»¥æé«˜ç›¸åº”æ¯”è¾ƒå™¨åœ¨ä¸åŒä»»åŠ¡æŒ‡ä»¤ä¸­æ•æ‰ä¸åŒå¹»è§‰æˆ–çœŸå®æ€§æ¨¡å¼çš„èƒ½åŠ›ã€‚CDTé€šè¿‡å¯¹æ¯”ç›®æ ‡LLMå’Œè¿™äº›æ¯”è¾ƒå™¨ä¹‹é—´çš„é€»è¾‘å·®å¼‚ï¼Œå°†ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹é™åˆ¶åœ¨äº‹å®æ€§ç¨³å¥åˆ†å¸ƒä¸­ã€‚åœ¨å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„ç³»ç»Ÿå®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶å¯ä»¥æ˜¾è‘—æé«˜æ¨¡å‹æ€§èƒ½å’Œå“åº”äº‹å®æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.12325v4">PDF</a> Accepted by AAAI 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>LLMè™½ç„¶åŠŸèƒ½å¼ºå¤§ï¼Œä½†æ˜“äº§ç”Ÿä¸å¯éªŒè¯äº‹å®ç›¸çŸ›ç›¾çš„å›åº”ï¼Œå³ä¸çœŸå®çš„å¹»è§‰å†…å®¹ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨ä¼˜åŒ–æ¨¡å‹å‚æ•°æˆ–ç¼–è¾‘è¯­ä¹‰è¡¨ç¤ºï¼Œè¿™ä¼šæŸå®³ç›®æ ‡LLMçš„å†…éƒ¨äº‹å®çŸ¥è¯†ã€‚æ­¤å¤–ï¼Œå¹»è§‰åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­é€šå¸¸è¡¨ç°å‡ºå¤šé¢æ€§æ¨¡å¼ï¼Œé™åˆ¶äº†æ¨¡å‹åœ¨ä»»åŠ¡ä¸­çš„æ•´ä½“è¡¨ç°ã€‚æœ¬æ–‡æå‡ºä¸€ç§æ¯”è¾ƒå™¨é©±åŠ¨è§£ç æ—¶é—´ï¼ˆCDTï¼‰æ¡†æ¶ï¼Œä»¥å‡è½»å“åº”å¹»è§‰ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä½¿ç”¨å¤šä»»åŠ¡å¾®è°ƒæ ·æœ¬æ„å»ºå¹»è§‰å’ŒçœŸå®æ¯”è¾ƒå™¨ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æŒ‡ä»¤åŸå‹å¼•å¯¼çš„ä¸“å®¶æ··åˆç­–ç•¥ï¼Œä»¥æé«˜ç›¸åº”æ¯”è¾ƒå™¨åœ¨ä¸åŒä»»åŠ¡æŒ‡ä»¤ä¸­æ•æ‰å¹»è§‰æˆ–çœŸå®æ€§æ¨¡å¼çš„èƒ½åŠ›ã€‚CDTé€šè¿‡å¯¹æ¯”ç›®æ ‡LLMä¸è¿™äº›æ¯”è¾ƒå™¨çš„é€»è¾‘å·®å¼‚ï¼Œå°†ä¸‹ä¸€ä¸ªæ ‡è®°çš„é¢„æµ‹é™åˆ¶åœ¨äº‹å®æ€§ç¨³å¥åˆ†å¸ƒä¸­ã€‚åœ¨å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„ç³»ç»Ÿå®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶å¯ä»¥æ˜¾è‘—æé«˜æ¨¡å‹æ€§èƒ½å’Œå“åº”äº‹å®çš„å‡†ç¡®æ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>LLMæ˜“äº§ç”Ÿä¸äº‹å®ä¸ç¬¦çš„å“åº”ï¼Œå³å¹»è§‰å†…å®¹ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦é€šè¿‡ä¼˜åŒ–æ¨¡å‹å‚æ•°æˆ–ç¼–è¾‘è¯­ä¹‰è¡¨ç¤ºæ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œä½†è¿™å¯èƒ½æŸå®³LLMçš„å†…éƒ¨äº‹å®çŸ¥è¯†ã€‚</li>
<li>å¹»è§‰åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¤šé¢æ€§æ¨¡å¼ï¼Œå½±å“æ¨¡å‹çš„æ•´ä½“è¡¨ç°ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„CDTæ¡†æ¶ï¼Œé€šè¿‡æ„å»ºå¹»è§‰å’ŒçœŸå®æ¯”è¾ƒå™¨æ¥å‡è½»å“åº”å¹»è§‰ã€‚</li>
<li>é‡‡ç”¨æŒ‡ä»¤åŸå‹å¼•å¯¼çš„ä¸“å®¶æ··åˆç­–ç•¥ï¼Œæé«˜æ¯”è¾ƒå™¨æ•æ‰ä¸åŒä»»åŠ¡ä¸­å¹»è§‰æˆ–çœŸå®æ€§æ¨¡å¼çš„èƒ½åŠ›ã€‚</li>
<li>CDTæ¡†æ¶é€šè¿‡å°†é¢„æµ‹é™åˆ¶åœ¨äº‹å®æ€§ç¨³å¥åˆ†å¸ƒä¸­ï¼Œå¯¹æ¯”ç›®æ ‡LLMä¸æ¯”è¾ƒå™¨çš„é€»è¾‘å·®å¼‚æ¥å‘æŒ¥ä½œç”¨ã€‚</li>
<li>ç³»ç»Ÿå®éªŒè¡¨æ˜ï¼ŒCDTæ¡†æ¶èƒ½æ˜¾è‘—æé«˜æ¨¡å‹æ€§èƒ½å’Œå“åº”çš„å‡†ç¡®åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.12325">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-11f590706c22f3c72c9856938aae4564.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5bdf984ebd522ae3964354ea3b2c8908.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f7ed6a7061b1607e8aca9541924db3f7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cb7a1401af182d8a76907bd7f1c3a6b5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ebd811ba2b1175fb9967268bd852f97b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-22cbda0940544397553d04381c1ae541.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Language-Models-Resist-Alignment-Evidence-From-Data-Compression"><a href="#Language-Models-Resist-Alignment-Evidence-From-Data-Compression" class="headerlink" title="Language Models Resist Alignment: Evidence From Data Compression"></a>Language Models Resist Alignment: Evidence From Data Compression</h2><p><strong>Authors:Jiaming Ji, Kaile Wang, Tianyi Qiu, Boyuan Chen, Jiayi Zhou, Changye Li, Hantao Lou, Josef Dai, Yunhuai Liu, Yaodong Yang</strong></p>
<p>Large language models (LLMs) may exhibit unintended or undesirable behaviors. Recent works have concentrated on aligning LLMs to mitigate harmful outputs. Despite these efforts, some anomalies indicate that even a well-conducted alignment process can be easily circumvented, whether intentionally or accidentally. Does alignment fine-tuning yield have robust effects on models, or are its impacts merely superficial? In this work, we make the first exploration of this phenomenon from both theoretical and empirical perspectives. Empirically, we demonstrate the elasticity of post-alignment models, i.e., the tendency to revert to the behavior distribution formed during the pre-training phase upon further fine-tuning. Leveraging compression theory, we formally deduce that fine-tuning disproportionately undermines alignment relative to pre-training, potentially by orders of magnitude. We validate the presence of elasticity through experiments on models of varying types and scales. Specifically, we find that model performance declines rapidly before reverting to the pre-training distribution, after which the rate of decline drops significantly. Furthermore, we further reveal that elasticity positively correlates with the increased model size and the expansion of pre-training data. Our findings underscore the need to address the inherent elasticity of LLMs to mitigate their resistance to alignment. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯èƒ½ä¼šè¡¨ç°å‡ºæ„å¤–æˆ–ä¸å—æ¬¢è¿çš„è¡Œä¸ºã€‚è¿‘æœŸçš„ç ”ç©¶å·¥ä½œé›†ä¸­åœ¨å°†LLMå¯¹é½ä»¥å‡å°‘æœ‰å®³è¾“å‡ºã€‚å°½ç®¡ä»˜å‡ºäº†è¿™äº›åŠªåŠ›ï¼Œä¸€äº›å¼‚å¸¸æƒ…å†µè¡¨æ˜ï¼Œå³ä½¿æ˜¯ç²¾å¿ƒè¿›è¡Œçš„å¯¹é½è¿‡ç¨‹ä¹Ÿå¯èƒ½è½»æ˜“è¢«ç»•è¿‡ï¼Œæ— è®ºæ˜¯æ•…æ„è¿˜æ˜¯æ„å¤–ã€‚å¯¹é½å¾®è°ƒæ˜¯å¦å¯¹æ¨¡å‹äº§ç”Ÿäº†ç¨³å¥çš„å½±å“ï¼Œè¿˜æ˜¯å…¶å½±å“ä»…ä»…æ˜¯è¡¨é¢çš„ï¼Ÿåœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»ç†è®ºå’Œå®è¯ä¸¤ä¸ªè§’åº¦é¦–æ¬¡æ¢ç´¢äº†è¿™ä¸€ç°è±¡ã€‚å®è¯ä¸Šï¼Œæˆ‘ä»¬å±•ç¤ºäº†åå¯¹é½æ¨¡å‹çš„å¼¹æ€§ï¼Œå³è¿›ä¸€æ­¥å¾®è°ƒæ—¶æ¢å¤åˆ°é¢„è®­ç»ƒé˜¶æ®µå½¢æˆçš„è¡Œä¸ºåˆ†å¸ƒçš„å€¾å‘ã€‚æˆ‘ä»¬åˆ©ç”¨å‹ç¼©ç†è®ºæ­£å¼æ¨æ–­ï¼Œç›¸å¯¹äºé¢„è®­ç»ƒï¼Œå¾®è°ƒä¼šä¸æˆæ¯”ä¾‹åœ°ç ´åå¯¹é½ï¼Œå¯èƒ½æ˜¯ä»¥æ•°é‡çº§è®¡ç®—ã€‚æˆ‘ä»¬é€šè¿‡ä¸åŒç±»å‹å’Œè§„æ¨¡çš„æ¨¡å‹å®éªŒéªŒè¯äº†å¼¹æ€§çš„å­˜åœ¨ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å‘ç°æ¨¡å‹æ€§èƒ½åœ¨æ¢å¤åˆ°é¢„è®­ç»ƒåˆ†å¸ƒä¹‹å‰ä¼šè¿…é€Ÿä¸‹é™ï¼Œä¹‹åä¸‹é™ç‡æ˜¾è‘—é™ä½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¿›ä¸€æ­¥æ­ç¤ºå¼¹æ€§ä¸æ¨¡å‹è§„æ¨¡æ‰©å¤§å’Œé¢„è®­ç»ƒæ•°æ®å¢åŠ å‘ˆæ­£ç›¸å…³ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œéœ€è¦è§£å†³LLMçš„å†…åœ¨å¼¹æ€§ä»¥å‡è½»å…¶å¯¹é½çš„é˜»åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.06144v3">PDF</a> The five-page version has been accepted by NeurIPS 2024 Workshop   SoLaR. In the current version, we have conducted an in-depth expansion of   both the theoretical and experimental aspects</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯èƒ½ä¼šå±•ç°å‡ºæ„æƒ³ä¸åˆ°æˆ–ä¸å—æ¬¢è¿çš„è¡Œä¸ºã€‚å°½ç®¡è¿‘æœŸå·¥ä½œé›†ä¸­åœ¨å¯¹å…¶è¿›è¡Œå¯¹é½ä»¥å‡è½»æœ‰å®³è¾“å‡ºï¼Œä½†ä»å­˜åœ¨å¼‚å¸¸ç°è±¡ï¼Œè¡¨æ˜å³ä½¿æ˜¯ç²¾å¿ƒè¿›è¡Œçš„å¯¹é½è¿‡ç¨‹ä¹Ÿå¯èƒ½è½»æ˜“è¢«è§„é¿ï¼Œæ— è®ºæ˜¯æ•…æ„è¿˜æ˜¯æ„å¤–ã€‚æœ¬æ–‡é¦–æ¬¡ä»ç†è®ºå’Œå®è¯ä¸¤ä¸ªè§’åº¦æ¢è®¨äº†è¿™ä¸€ç°è±¡ã€‚å®è¯æ–¹é¢ï¼Œæˆ‘ä»¬å±•ç¤ºäº†åå¯¹é½æ¨¡å‹çš„å¼¹æ€§ï¼Œå³è¿›ä¸€æ­¥å¾®è°ƒæ—¶å€¾å‘äºæ¢å¤åˆ°é¢„è®­ç»ƒé˜¶æ®µå½¢æˆçš„è¡Œä¸ºåˆ†å¸ƒã€‚åˆ©ç”¨å‹ç¼©ç†è®ºï¼Œæˆ‘ä»¬æ­£å¼æ¨æ–­å¾®è°ƒä¼šç ´åå¯¹é¢„è®­ç»ƒäº§ç”Ÿçš„å¯¹é½çŠ¶æ€çš„æ¯”ä¾‹åˆ†å¸ƒå…³ç³»å¯èƒ½æ˜¯ç”±å¤šæ¬¡éšæœºå¼•å…¥çš„å‡ ä¸ªæˆ–å¤šä¸ªéšæœºè¯¯å·®å¯¼è‡´çš„å½±å“æä¸ºä¸¥é‡çš„ç°è±¡ã€‚æˆ‘ä»¬é€šè¿‡åœ¨ä¸åŒç±»å‹å’Œè§„æ¨¡çš„æ¨¡å‹ä¸Šè¿›è¡Œçš„å®éªŒéªŒè¯äº†å¼¹æ€§çš„å­˜åœ¨ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å‘ç°æ¨¡å‹æ€§èƒ½åœ¨æ¢å¤åˆ°é¢„è®­ç»ƒåˆ†å¸ƒä¹‹å‰ä¼šè¿…é€Ÿä¸‹é™ï¼Œä¹‹åä¸‹é™é€Ÿåº¦ä¼šæ˜¾è‘—é™ä½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å‘ç°å¼¹æ€§ä¸æ¨¡å‹è§„æ¨¡æ‰©å¤§å’Œé¢„è®­ç»ƒæ•°æ®å¢åŠ å‘ˆæ­£ç›¸å…³ã€‚æˆ‘ä»¬çš„ç ”ç©¶æ­ç¤ºäº†è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹å†…åœ¨å¼¹æ€§çš„å¿…è¦æ€§ä»¥å®ç°æœ‰æ•ˆçš„å¯¹é½æ”¹å–„å·¥ä½œæˆæœç›®æ ‡è¦æ±‚æ­£ç¡®å‘ˆç°åé¦ˆï¼Œå¼ºåŒ–æˆæœçš„æ–¹å‘å‡†ç¡®æ€§å‡å°‘è¯­ä¹‰å‡ºç°å®Œå…¨ä¸ä¹‹èƒŒé“è€Œé©°å³ä¸åˆç†çš„æ•°æ®éšåè½¬è¯¥æœå‘ä»¥ä½¿AIèƒ½å¤Ÿç†è§£å«ä¹‰å¸æ”¶èƒ½å¤Ÿç”¨çš„æ ‡å¿—æ ·æœ¬ç­‰ä¿¡æ¯ä¸äººç±»çš„è¦æ±‚ä¸è°‹è€Œé¡ºè€Œä¸ºå½“åŠ¡ä¹‹æ€¥æˆ‘ä»¬åº”å§‹ç»ˆè®¤çœŸå¯¹å¾…å¤§å‹è¯­è¨€æ¨¡å‹çš„å¯¹é½é—®é¢˜ã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ºæœªæ¥çš„LLMå¯¹é½å·¥ä½œæä¾›äº†æ–°çš„è§†è§’å’Œæ€è·¯ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬çš„ç ”ç©¶ä¹Ÿå¼ºè°ƒäº†è§£å†³LLMå†…åœ¨å¼¹æ€§çš„é‡è¦æ€§ï¼Œè¿™å°†æœ‰åŠ©äºæˆ‘ä»¬æ›´æ·±å…¥åœ°ç†è§£è¿™äº›æ¨¡å‹çš„å¤æ‚æ€§å’Œå¯¹åº”ç”¨äº§ç”Ÿçš„æŒ‘æˆ˜ã€‚<strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯èƒ½å­˜åœ¨æ„å¤–çš„è¡Œä¸ºæˆ–è¡¨ç°å‡ºä¸è‰¯è¡¨ç°çš„é—®é¢˜ã€‚è™½ç„¶å¯¹é½å·¥ä½œè¢«å¹¿æ³›åº”ç”¨åœ¨LLMä¸­ä»¥å‡è½»æœ‰å®³è¾“å‡ºï¼Œä½†ä»å­˜åœ¨ä¸€äº›æ¼æ´ã€‚è¿™è¡¨æ˜éœ€è¦å¯¹é½æŠ€æœ¯éœ€è¦è¿›è¡Œæ›´æ·±å…¥çš„æ¢è®¨å’Œç ”ç©¶ä»¥ç¡®ä¿å…¶æœ‰æ•ˆæ€§ã€‚ç ”ç©¶è€…å¯ä»¥é€šè¿‡åˆ©ç”¨ç®€åŒ–æŠ€æœ¯å¯¹æ“ä½œæ–¹å‘è¿›è¡Œåˆ¤æ–­å¹¶åŠ ä»¥è°ƒèŠ‚æå‡ä¸ºç²¾ç¡®åº¦çš„é—®é¢˜æ“ä½œå¤æ‚ç®€ä»¥æŠ€æœ¯åº”ç”¨åˆ°å¹¿æ³›å®ä½“é’ˆå¯¹å­¦æœ¯å®æˆ˜å¯¹äºéœ€è¦è¿›è¡Œä½œä¸šåŠ å·¥æ”¹è¿›çš„å­¦è€…ç²¾å‡†å†³ç­–æŠŠæ¡å®šä½æƒ…å†µè¯†åˆ«ä»¥è¾¾åˆ°ç†è®ºå¯é å®é™…åº”ç”¨ï¼›æœ€åæä¾›ä¸€å®šç›¸å…³åˆ†æè®¨è®ºä¼˜åŒ–æ€»ç»“è®ºè¿°æ”¯æŒè§‚ç‚¹å’Œæˆæœçš„åˆ›æ–°æ„ä¹‰å’Œåº”ç”¨ä»·å€¼æƒ…å†µåæ˜ æ”¹å–„é‡è¦è¿›å±•é—®é¢˜çš„ä¸¾æªé€šè¿‡è¿›ä¸€æ­¥æå‡å¯ä»¥æŒ‡å‡ºæ¥ä¸‹æ¥é‡è¦å…³æ³¨çš„è¿›æ­¥ç ”ç©¶æ–¹å‘åº”å¯¹æŒç»­åˆ°æ¥çš„å˜åŒ–å…‹æœå’Œè§£å†³éš¾åº¦åŠæ‰€å¸¦æ¥çš„åæœå³å¯æœ‰æ•ˆè§£å†³å¯èƒ½å¼•å‘çš„æŠ€æœ¯æˆ–è´¨é‡é—®é¢˜æœªæ¥å°†åœ¨ä¼—å¤šè¡Œä¸šé¢†åŸŸå‘æŒ¥é‡è¦ä½œç”¨å’Œä»·å€¼ï¼›æœ€ç»ˆé€šè¿‡åˆ›æ–°æ€§çš„è§£å†³æ–¹æ¡ˆæ¨åŠ¨å¤§å‹è¯­è¨€æ¨¡å‹çš„å‘å±•ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¿…é¡»è®¤çœŸå¯¹å¾…å¤§å‹è¯­è¨€æ¨¡å‹çš„å¯¹é½é—®é¢˜ã€‚å¯¹é½æŠ€æœ¯æ˜¯å®ç°å¤§å‹è¯­è¨€æ¨¡å‹ä¸äººç±»æ„å›¾ä¸€è‡´æ€§çš„å…³é”®æ‰€åœ¨ï¼›æå‡æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œé€‚åº”æ€§èƒ½åœ¨å°†æ¥å¯èƒ½å­˜åœ¨çš„ä¸åŒç¯å¢ƒä¸‹éƒ½èƒ½ä¿è¯æ­£ç¡®å†³ç­–çš„åŒæ—¶å‡è½»å¤æ‚æ€§åŠæŒ‘æˆ˜æ€§å¯¹äºå‡ºç°çš„éšœç¢æˆ‘ä»¬åº”ç›´é¢åº”å¯¹æ¢ç´¢æ½œåœ¨æŒ‘æˆ˜å¯¹äºè¿›æ­¥ç»™äºˆä¸æ–­é¼“åŠ±åšæŒä»åº•å±‚ç€æ‰‹ç»§ç»­åˆ›æ–°æ‹“å±•å¢å¼ºæ–°æŠ€æœ¯è½åœ°è¡Œä¸šçš„å®æ•ˆæ€§å…³æ³¨ç°å®é—®é¢˜ä¿è¯ä¸äººç±»é¢„æœŸä¸€è‡´çš„é€‚é…å·¥ä½œä¸å†é‡å¤è¢«å¤–ç•Œå¿½è§†çš„æœªæ¥ä¸æ–­å‘å±•ä»æ˜¯ç„¦ç‚¹é›†ä¸­åœ¨æ–°ç¯å¢ƒä¸‹åè°ƒå…±ç”Ÿé•¿æœŸè·Ÿè¸ªå…·æœ‰å†³å®šæ€§æˆæ•ˆåŠ é€Ÿå¾ªç¯æ–°è§‚ç‚¹ç²¾è¿›æœåŠ¡ä¾¿æ·é›†æˆç²¾ç»†è½¬åŒ–è§£å†³æ–¹æ¡ˆå¯¼å‘æœ‰åºå‘å‰ä¸æ–­ç¼©å°è¯¯è§£çš„ä¿¡æ¯å¤„ç†è·¯å¾„ç»™äºˆä¸šç•Œå‘å±•çš„å¯è¡Œå»ºè®®å’Œæ¢è®¨é‡å¤§éš¾é¢˜çš„ä¼˜åŒ–å¤„ç†æ–¹å¼æ˜¯æˆ‘ä»¬è¿«åˆ‡éœ€è¦è€ƒè™‘çš„é‡è¦é—®é¢˜å…³é”®æ€è€ƒéš¾ç‚¹å’ŒæŠ€æœ¯é¢†åŸŸé‡å¤§çªç ´ä¸­çš„æ½œåŠ›æ–¹é¢å¾…å¼€å±•çš„æ–¹å‘é›†ä¸­æ€è€ƒå’Œç ”è®¨ç»Ÿä¸€æ€æƒ³è¾¾æˆå…±è¯†å½¢æˆå…±è¯†æ¨åŠ¨è¡Œä¸šè¿›æ­¥å‘å±•ï¼›</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.06144">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3a97ae1dfeeac16f08cb84ac2734d245.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-90c4c1eae94adef5c33bb0eb58b2c231.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e8682a877e3de7f9d2da508bc2b492ad.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-24/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-24/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-24/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-c30275e38a084ae99a2feb9c3b5590ab.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-24  Tacit Learning with Adaptive Information Selection for Cooperative   Multi-Agent Reinforcement Learning
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-12-24
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-23/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-26d6fcc8803903e2bd9e045fb2791ed5.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-23  FG-MDM Towards Zero-Shot Human Motion Generation via ChatGPT-Refined   Descriptions
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-23
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">15693.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
