<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Agent">
    <meta name="description" content="Agent 方向最新论文已更新，请持续关注 Update in 2024-12-24  Tacit Learning with Adaptive Information Selection for Cooperative   Multi-Agent Reinforcement Learning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Agent | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-c30275e38a084ae99a2feb9c3b5590ab.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Agent</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Agent/">
                                <span class="chip bg-color">Agent</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                Agent
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2024-12-24
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2024-12-24
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    10.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    42 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2024-12-24-更新"><a href="#2024-12-24-更新" class="headerlink" title="2024-12-24 更新"></a>2024-12-24 更新</h1><h2 id="Tacit-Learning-with-Adaptive-Information-Selection-for-Cooperative-Multi-Agent-Reinforcement-Learning"><a href="#Tacit-Learning-with-Adaptive-Information-Selection-for-Cooperative-Multi-Agent-Reinforcement-Learning" class="headerlink" title="Tacit Learning with Adaptive Information Selection for Cooperative   Multi-Agent Reinforcement Learning"></a>Tacit Learning with Adaptive Information Selection for Cooperative   Multi-Agent Reinforcement Learning</h2><p><strong>Authors:Lunjun Liu, Weilai Jiang, Yaonan Wang</strong></p>
<p>In multi-agent reinforcement learning (MARL), the centralized training with decentralized execution (CTDE) framework has gained widespread adoption due to its strong performance. However, the further development of CTDE faces two key challenges. First, agents struggle to autonomously assess the relevance of input information for cooperative tasks, impairing their decision-making abilities. Second, in communication-limited scenarios with partial observability, agents are unable to access global information, restricting their ability to collaborate effectively from a global perspective. To address these challenges, we introduce a novel cooperative MARL framework based on information selection and tacit learning. In this framework, agents gradually develop implicit coordination during training, enabling them to infer the cooperative behavior of others in a discrete space without communication, relying solely on local information. Moreover, we integrate gating and selection mechanisms, allowing agents to adaptively filter information based on environmental changes, thereby enhancing their decision-making capabilities. Experiments on popular MARL benchmarks show that our framework can be seamlessly integrated with state-of-the-art algorithms, leading to significant performance improvements. </p>
<blockquote>
<p>在多智能体强化学习（MARL）中，由于强大的性能表现，采用集中训练与分散执行（CTDE）框架的方法得到了广泛应用。然而，CTDE的进一步发展面临两大挑战。首先，智能体难以自主评估输入信息对于协同任务的相关性，从而损害了其决策能力。其次，在通信受限且部分可观测的场景中，智能体无法访问全局信息，限制了它们从全局角度进行有效协作的能力。为了解决这些挑战，我们引入了一种基于信息选择和默识学习的新型合作MARL框架。在此框架中，智能体在训练过程中逐渐发展出隐性协调，使它们能够在离散空间中不依赖通信就能推断出他人的合作行为。此外，我们结合了门控和选择机制，使智能体能根据环境变化自适应地过滤信息，从而提高其决策能力。在流行的MARL基准测试上的实验表明，我们的框架可以无缝地融入最先进的算法，带来显著的性能提升。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15639v1">PDF</a> Accepted by AAMAS 2025 (Extended Abstract)</p>
<p><strong>Summary</strong></p>
<p>在基于信息选择和默式学习的多智能体强化学习框架中，智能体通过训练过程中逐渐发展隐式协调，能在局部信息环境下推断其他智能体的合作行为。该框架解决了集中训练与分散执行框架面临的信息筛选与全局沟通问题，提高了智能体在特定环境中的决策能力。通过流行的多智能体强化学习基准测试，该框架与现有先进算法的结合表现出显著的性能提升。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多智能体强化学习（MARL）中普遍采用集中训练与分散执行（CTDE）框架。此框架存在自主评估输入信息的重要性和全局信息难以获取的问题。</li>
<li>提出了一种基于信息选择和默式学习的新型合作MARL框架，解决了上述问题。智能体通过训练逐渐发展隐式协调，可以在离散空间中仅依赖局部信息推断其他智能体的合作行为。</li>
<li>新框架融合了门控和选择机制，允许智能体根据环境变化自适应地筛选信息，增强了决策能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15639">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-7f7c304129f77c8fd8957971bf73d836.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4c13821a1701fb99bdb4f4643efd044e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-19b1203af9e35282371010546c6ff5ab.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-772b51958c86fa3aa46eed555885fe58.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-03a786febe9de38824d3a03fb960e146.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ae63531229a9b34ed997778a5334e1c9.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Multi-modal-Agent-Tuning-Building-a-VLM-Driven-Agent-for-Efficient-Tool-Usage"><a href="#Multi-modal-Agent-Tuning-Building-a-VLM-Driven-Agent-for-Efficient-Tool-Usage" class="headerlink" title="Multi-modal Agent Tuning: Building a VLM-Driven Agent for Efficient Tool   Usage"></a>Multi-modal Agent Tuning: Building a VLM-Driven Agent for Efficient Tool   Usage</h2><p><strong>Authors:Zhi Gao, Bofei Zhang, Pengxiang Li, Xiaojian Ma, Tao Yuan, Yue Fan, Yuwei Wu, Yunde Jia, Song-Chun Zhu, Qing Li</strong></p>
<p>The advancement of large language models (LLMs) prompts the development of multi-modal agents, which are used as a controller to call external tools, providing a feasible way to solve practical tasks. In this paper, we propose a multi-modal agent tuning method that automatically generates multi-modal tool-usage data and tunes a vision-language model (VLM) as the controller for powerful tool-usage reasoning. To preserve the data quality, we prompt the GPT-4o mini model to generate queries, files, and trajectories, followed by query-file and trajectory verifiers. Based on the data synthesis pipeline, we collect the MM-Traj dataset that contains 20K tasks with trajectories of tool usage. Then, we develop the T3-Agent via \underline{T}rajectory \underline{T}uning on VLMs for \underline{T}ool usage using MM-Traj. Evaluations on the GTA and GAIA benchmarks show that the T3-Agent consistently achieves improvements on two popular VLMs: MiniCPM-V-8.5B and {Qwen2-VL-7B}, which outperforms untrained VLMs by $20%$, showing the effectiveness of the proposed data synthesis pipeline, leading to high-quality data for tool-usage capabilities. </p>
<blockquote>
<p>随着大型语言模型（LLM）的进展，多模态代理的发展也随之而来。这些代理被用作控制器来调用外部工具，为解决实际任务提供了可行的方法。在本文中，我们提出了一种多模态代理调整方法，该方法可自动生成多模态工具使用数据，并调整视觉语言模型（VLM）作为强大的工具使用控制器。为了保持数据质量，我们引导GPT-4o小型模型生成查询、文件和轨迹，随后进行查询文件验证器和轨迹验证器。基于数据合成管道，我们收集了包含2万个任务的MM-Traj数据集，其中包含工具使用轨迹。然后，我们通过使用MM-Traj的轨迹调整VLM来开发T3-Agent。在GTA和GAIA基准测试上的评估表明，T3-Agent在两款流行的VLM上持续实现改进：MiniCPM-V-8.5B和Qwen2-VL-7B。相较于未经训练的VLM，其性能提高了20%，显示了所提出的数据合成管道的有效性，该管道有助于为工具使用能力提供高质量数据。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15606v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型的进步推动了多模态代理的发展，作为调用外部工具的控制器，为解决实际任务提供了可行的方法。本文提出了一种多模态代理调优方法，该方法可自动生成多模态工具使用数据，并调整视觉语言模型作为强大的工具使用控制器。通过GPT-4o小型模型生成查询、文件和轨迹，并通过查询文件和轨迹验证器保证数据质量。基于数据合成管道，我们收集了MM-Traj数据集，包含具有工具使用轨迹的2万个任务。通过MM-Traj上的轨迹调优VLMs开发T3-Agent，在GTA和GAIA基准测试上的评估表明，T3-Agent在两种流行的VLMs上实现了持续的改进，即MiniCPM-V-8.5B和Qwen2-VL-7B，其性能优于未训练的VLMs达20%，证明了数据合成管道的有效性，为工具使用能力生成高质量数据。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型的进步推动了多模态代理的发展，这些代理可用作外部工具的控制器以解决实际任务。</li>
<li>提出了一种多模态代理调优方法，能自动生成多模态工具使用数据。</li>
<li>利用GPT-4o小型模型生成查询、文件和轨迹，确保数据质量。</li>
<li>基于数据合成管道，收集了包含2万个任务的MM-Traj数据集。</li>
<li>开发了一个名为T3-Agent的工具，通过轨迹调优视觉语言模型（VLM）。</li>
<li>在GTA和GAIA基准测试上，T3-Agent在两种流行的VLMs上实现了显著的性能改进。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15606">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-268afcff9f222c16d93b74f989d18b88.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a8dda2d00eb1b900c3e1645b50ec9af4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a88943776924dfe5ec1b0304cc1f95b5.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Multi-Agent-Reinforcement-Learning-for-Sequential-Satellite-Assignment-Problems"><a href="#Multi-Agent-Reinforcement-Learning-for-Sequential-Satellite-Assignment-Problems" class="headerlink" title="Multi Agent Reinforcement Learning for Sequential Satellite Assignment   Problems"></a>Multi Agent Reinforcement Learning for Sequential Satellite Assignment   Problems</h2><p><strong>Authors:Joshua Holder, Natasha Jaques, Mehran Mesbahi</strong></p>
<p>Assignment problems are a classic combinatorial optimization problem in which a group of agents must be assigned to a group of tasks such that maximum utility is achieved while satisfying assignment constraints. Given the utility of each agent completing each task, polynomial-time algorithms exist to solve a single assignment problem in its simplest form. However, in many modern-day applications such as satellite constellations, power grids, and mobile robot scheduling, assignment problems unfold over time, with the utility for a given assignment depending heavily on the state of the system. We apply multi-agent reinforcement learning to this problem, learning the value of assignments by bootstrapping from a known polynomial-time greedy solver and then learning from further experience. We then choose assignments using a distributed optimal assignment mechanism rather than by selecting them directly. We demonstrate that this algorithm is theoretically justified and avoids pitfalls experienced by other RL algorithms in this setting. Finally, we show that our algorithm significantly outperforms other methods in the literature, even while scaling to realistic scenarios with hundreds of agents and tasks. </p>
<blockquote>
<p>分配问题是经典组合优化问题之一，其中必须将一组代理分配给一组任务，以在满足分配约束的同时实现最大效用。给定每个代理完成每个任务的效用，存在多项式时间算法来解决最简单的单一分配问题。然而，在许多现代应用（如卫星星座、电网和移动机器人调度）中，分配问题会随着时间的推移而展开，给定分配的效用很大程度上取决于系统的状态。我们应用多代理强化学习来解决这个问题，通过从已知的多项式时间贪婪求解器中进行引导并学习进一步的经验来估算分配的价值。然后，我们使用分布式最优分配机制来选择分配，而不是直接选择它们。我们证明了该算法的理论依据，避免了其他强化学习算法在此设置中遇到的陷阱。最后，我们证明我们的算法在文献中的其他方法中具有显著优势，即使在扩展到具有数百个代理和任务的实际场景中也是如此。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15573v1">PDF</a> </p>
<p><strong>Summary</strong><br>     组合优化问题中的分配问题是一类经典问题，即如何让一组代理完成一组任务，以实现最大效用并满足分配约束。对于简单的分配问题，存在多项式时间算法。但在现代应用如卫星星座、电网和移动机器人调度中，分配问题随时间展开，效用取决于系统的状态。我们应用多代理强化学习来解决这个问题，通过从已知的多项式时间贪心求解器中进行引导并从经验中学习来评估分配的价值。我们使用分布式最优分配机制进行选择而非直接选择分配方式。我们的算法理论合理且能避免其他强化学习算法在此设置中的陷阱。最终，我们的算法在文献中的其他方法上表现优越，甚至在扩展到数百个代理和任务的实际场景中也是如此。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>分配问题是组合优化中的经典问题，涉及将代理分配给任务以实现最大效用并满足约束。</li>
<li>在现代应用中，分配问题随着时间展开，且效用取决于系统状态。</li>
<li>多代理强化学习用于解决复杂的分配问题。</li>
<li>通过结合多项式时间贪心求解器和经验学习来评估分配价值。</li>
<li>使用分布式最优分配机制进行选择，避免直接选择分配方式。</li>
<li>所提出的算法在理论上是合理的，并能有效避免其他强化学习算法的陷阱。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15573">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-ff7df73d07e3bc468554881e3eb8b316.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bf791d6a83508a1e169659d0f09d7a32.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8f68a888c64bfe102e6969d3d49ae35b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b015d7371f27cf3d6427a10658324c12.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-77569896ead2a1872b9473be29aba9fb.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Novelty-Guided-Data-Reuse-for-Efficient-and-Diversified-Multi-Agent-Reinforcement-Learning"><a href="#Novelty-Guided-Data-Reuse-for-Efficient-and-Diversified-Multi-Agent-Reinforcement-Learning" class="headerlink" title="Novelty-Guided Data Reuse for Efficient and Diversified Multi-Agent   Reinforcement Learning"></a>Novelty-Guided Data Reuse for Efficient and Diversified Multi-Agent   Reinforcement Learning</h2><p><strong>Authors:Yangkun Chen, Kai Yang, Jian Tao, Jiafei Lyu</strong></p>
<p>Recently, deep Multi-Agent Reinforcement Learning (MARL) has demonstrated its potential to tackle complex cooperative tasks, pushing the boundaries of AI in collaborative environments. However, the efficiency of these systems is often compromised by inadequate sample utilization and a lack of diversity in learning strategies. To enhance MARL performance, we introduce a novel sample reuse approach that dynamically adjusts policy updates based on observation novelty. Specifically, we employ a Random Network Distillation (RND) network to gauge the novelty of each agent’s current state, assigning additional sample update opportunities based on the uniqueness of the data. We name our method Multi-Agent Novelty-GuidEd sample Reuse (MANGER). This method increases sample efficiency and promotes exploration and diverse agent behaviors. Our evaluations confirm substantial improvements in MARL effectiveness in complex cooperative scenarios such as Google Research Football and super-hard StarCraft II micromanagement tasks. </p>
<blockquote>
<p>最近，深度多智能体强化学习（MARL）已显示出其在解决复杂合作任务方面的潜力，推动了人工智能在协作环境中的边界。然而，这些系统的效率常常因样本利用不足和学习策略缺乏多样性而受到影响。为了提高MARL的性能，我们引入了一种新型样本再利用方法，该方法根据观测的新颖性动态调整策略更新。具体来说，我们采用随机网络蒸馏（RND）网络来衡量每个智能体的当前状态的新颖性，根据数据的唯一性分配额外的样本更新机会。我们将我们的方法命名为多智能体新颖性引导样本再利用（MANGER）。此方法提高了样本效率，并促进了探索和智能体的多样化行为。我们的评估在复杂的合作场景中确认了MARL的大幅改进，例如在Google Research Football和超级难的星际争霸II微观管理任务中。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15517v1">PDF</a> AAAI 2025</p>
<p><strong>Summary</strong></p>
<p>深度多智能体强化学习（MARL）在解决复杂的协同任务方面具有巨大的潜力，推动了人工智能在协作环境中的边界扩展。然而，样本利用率不足和学习策略缺乏多样性常常影响这类系统的效率。为提升MARL性能，我们提出了一种新型样本复用方法——Multi-Agent Novelty-GuidEd sample Reuse（MANGER），该方法根据观测的新颖性动态调整策略更新。具体地，我们采用Random Network Distillation（RND）网络评估各智能体的当前状态的新颖性，并根据数据的独特性分配额外的样本更新机会。此方法提高了样本效率，促进了探索和智能体的多样化行为。评估结果显示，在复杂的协同场景如Google Research Football和超级困难的StarCraft II微观管理任务中，MARL的有效性得到了显著提高。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>深度多智能体强化学习（MARL）在解决复杂的协同任务方面具有潜力。</li>
<li>当前MARL系统常面临样本利用率不足和学习策略缺乏多样性的问题。</li>
<li>引入了一种新型样本复用方法——MANGER，根据观测的新颖性动态调整策略更新。</li>
<li>MANGER方法采用RND网络评估智能体的当前状态的新颖性。</li>
<li>MANGER提高了样本效率，促进了探索和智能体的多样化行为。</li>
<li>在复杂的协同场景中，如Google Research Football和StarCraft II微观管理任务，MANGER方法提高了MARL的有效性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15517">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e1d9095e9e92617c8868b34de23e0fef.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1aed80cc60f638c88ce3e5ab1a39094d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2af9b0034f2ab65168c98cddd234bdd8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ac49829b3e3b02da7b53bc4491139ff6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-843f1e75da850e5468b9ee2e1002b22f.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Mitigating-Social-Bias-in-Large-Language-Models-A-Multi-Objective-Approach-within-a-Multi-Agent-Framework"><a href="#Mitigating-Social-Bias-in-Large-Language-Models-A-Multi-Objective-Approach-within-a-Multi-Agent-Framework" class="headerlink" title="Mitigating Social Bias in Large Language Models: A Multi-Objective   Approach within a Multi-Agent Framework"></a>Mitigating Social Bias in Large Language Models: A Multi-Objective   Approach within a Multi-Agent Framework</h2><p><strong>Authors:Zhenjie Xu, Wenqing Chen, Yi Tang, Xuanying Li, Cheng Hu, Zhixuan Chu, Kui Ren, Zibin Zheng, Zhichao Lu</strong></p>
<p>Natural language processing (NLP) has seen remarkable advancements with the development of large language models (LLMs). Despite these advancements, LLMs often produce socially biased outputs. Recent studies have mainly addressed this problem by prompting LLMs to behave ethically, but this approach results in unacceptable performance degradation. In this paper, we propose a multi-objective approach within a multi-agent framework (MOMA) to mitigate social bias in LLMs without significantly compromising their performance. The key idea of MOMA involves deploying multiple agents to perform causal interventions on bias-related contents of the input questions, breaking the shortcut connection between these contents and the corresponding answers. Unlike traditional debiasing techniques leading to performance degradation, MOMA substantially reduces bias while maintaining accuracy in downstream tasks. Our experiments conducted on two datasets and two models demonstrate that MOMA reduces bias scores by up to 87.7%, with only a marginal performance degradation of up to 6.8% in the BBQ dataset. Additionally, it significantly enhances the multi-objective metric icat in the StereoSet dataset by up to 58.1%. Code will be made available at <a target="_blank" rel="noopener" href="https://github.com/Cortantse/MOMA">https://github.com/Cortantse/MOMA</a>. </p>
<blockquote>
<p>自然语言处理（NLP）随着大型语言模型（LLM）的发展而取得了显著进步。然而，尽管有这些进展，LLM通常会产生带有社会偏见的结果。最近的研究主要通过提示LLM以符合道德的行为来解决这个问题，但这种方法会导致性能不可接受的下降。在本文中，我们提出了一个多目标多智能体框架（MOMA）的方法，以减轻LLM中的社会偏见，而不会对其性能造成重大损害。MOMA的关键思想是利用多个智能体对输入问题中的偏见相关内容进行因果干预，从而切断这些内容与相应答案之间的直接联系。与传统的可能导致性能下降的消除偏见技术不同，MOMA在降低偏见的同时，保持下游任务的准确性。我们在两个数据集和两个模型上进行的实验表明，MOMA将偏见分数降低了高达87.7%，仅在BBQ数据集上的性能下降幅度为最高6.8%。此外，它在StereoSet数据集上显著提高了多目标指标icat，最高提升了58.1%。代码将在<a target="_blank" rel="noopener" href="https://github.com/Cortantse/MOMA">https://github.com/Cortantse/MOMA</a>上提供。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15504v1">PDF</a> This work has been accepted at The 39th Annual AAAI Conference on   Artificial Intelligence (AAAI-2025)</p>
<p><strong>Summary</strong><br>自然语言处理（NLP）领域随着大型语言模型（LLM）的发展取得了显著进步。然而，LLM的输出常带有社会偏见。尽管已有研究通过提示LLM以符合伦理的方式行为来解决这一问题，但这可能导致性能下降。本文提出一种多目标多智能体框架（MOMA）的方法，旨在缓解LLM中的社会偏见问题，同时不会显著损害性能。MOMA的关键思想是利用多个智能体对输入问题中的偏见相关内容进行因果干预，切断这些内容与相应答案之间的直接联系。与传统导致性能下降的消除偏见技术不同，MOMA在降低偏见的同时维持下游任务的准确性。在两项数据集和两个模型上的实验显示，MOMA将偏见得分降低了高达87.7%，同时在BBQ数据集上的性能仅轻微下降6.8%。此外，它在StereoSet数据集上的多目标指标icat也有显著提升，提升了高达58.1%。代码将在<a target="_blank" rel="noopener" href="https://github.com/Cortantse/MOMA%E5%85%AC%E5%BC%80%E5%88%86%E4%BA%AB%E3%80%82">https://github.com/Cortantse/MOMA公开分享。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）的输出常带有社会偏见。</li>
<li>传统解决偏见问题的方法可能导致模型性能下降。</li>
<li>MOMA方法通过多智能体框架进行因果干预，旨在缓解LLM中的社会偏见问题。</li>
<li>MOMA在降低偏见的同时维持下游任务的准确性。</li>
<li>在实验数据集上，MOMA显著降低了偏见得分，最高达87.7%。</li>
<li>MOMA对模型性能的影响轻微，性能下降最高达6.8%。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15504">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-3d0c6c31cf211219480bf8b94d123917.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ff2617591e1a6fe5cdc1034eea6331b7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-323b419cffe6c17dd7e83506a3518e94.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3f8fcbcc706e98cc1954bfa1a98ce4c9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6bbac2731363dd7748025f96c9b87078.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4b268901f2b7e9815f059b10914344a6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-98379efcba6a17d3901466613f46273d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-490fa04f1e227757e9a21b49cc68b362.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="AdaSociety-An-Adaptive-Environment-with-Social-Structures-for-Multi-Agent-Decision-Making"><a href="#AdaSociety-An-Adaptive-Environment-with-Social-Structures-for-Multi-Agent-Decision-Making" class="headerlink" title="AdaSociety: An Adaptive Environment with Social Structures for   Multi-Agent Decision-Making"></a>AdaSociety: An Adaptive Environment with Social Structures for   Multi-Agent Decision-Making</h2><p><strong>Authors:Yizhe Huang, Xingbo Wang, Hao Liu, Fanqi Kong, Aoyang Qin, Min Tang, Xiaoxi Wang, Song-Chun Zhu, Mingjie Bi, Siyuan Qi, Xue Feng</strong></p>
<p>Traditional interactive environments limit agents’ intelligence growth with fixed tasks. Recently, single-agent environments address this by generating new tasks based on agent actions, enhancing task diversity. We consider the decision-making problem in multi-agent settings, where tasks are further influenced by social connections, affecting rewards and information access. However, existing multi-agent environments lack a combination of adaptive physical surroundings and social connections, hindering the learning of intelligent behaviors. To address this, we introduce AdaSociety, a customizable multi-agent environment featuring expanding state and action spaces, alongside explicit and alterable social structures. As agents progress, the environment adaptively generates new tasks with social structures for agents to undertake. In AdaSociety, we develop three mini-games showcasing distinct social structures and tasks. Initial results demonstrate that specific social structures can promote both individual and collective benefits, though current reinforcement learning and LLM-based algorithms show limited effectiveness in leveraging social structures to enhance performance. Overall, AdaSociety serves as a valuable research platform for exploring intelligence in diverse physical and social settings. The code is available at <a target="_blank" rel="noopener" href="https://github.com/bigai-ai/AdaSociety">https://github.com/bigai-ai/AdaSociety</a>. </p>
<blockquote>
<p>传统的交互环境通过固定任务限制了代理的智能增长。最近，单代理环境通过基于代理行动生成新任务来解决这个问题，提高了任务的多样性。我们考虑多代理环境中的决策问题，其中的任务受到社会连接的进一步影响，从而影响奖励和信息的获取。然而，现有的多代理环境缺乏自适应的物理环境和社会连接的组合，阻碍了智能行为的学习。为了解决这一问题，我们推出了AdaSociety，这是一个可定制的多代理环境，具有可扩展的状态和行动空间，以及明确且可更改的社会结构。随着代理的进步，环境会自适应地生成具有社会结构的新任务供代理执行。在AdaSociety中，我们开发了三个小型游戏，展示了不同的社会结构和任务。初步结果表明，特定的社会结构可以促进个人和集体的利益，尽管目前的强化学习和基于大型语言模型（LLM）的算法在利用社会结构提高性能方面显示出有限的有效性。总的来说，AdaSociety是一个有价值的研究平台，可用于探索在多样化的物理和社会环境中的智能。代码可在<a target="_blank" rel="noopener" href="https://github.com/bigai-ai/AdaSociety">https://github.com/bigai-ai/AdaSociety</a>获得。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.03865v3">PDF</a> Accepted at NeurIPS D&amp;B 2024</p>
<p><strong>Summary</strong></p>
<p>传统交互环境限制智能代理的发展，任务固定。近期，单代理环境通过生成基于代理行为的新任务，增强任务多样性，来解决这一问题。本文研究多代理环境中的决策问题，任务受社会连接影响，改变奖励和资讯获取。然而，现有多代理环境缺乏自适应的物理环境和社会连接，阻碍智能行为的学习。为解决此问题，我们推出AdaSociety，一个可定制的多代理环境，拥有扩展的状态和行动空间，以及明确且可改变的社会结构。随着代理的进步，环境会自适应生成具有社会结构的新任务供代理执行。在AdaSociety中，我们开发了三个展示不同社会结构和任务的小游戏。初步结果显示，特定的社会结构可以促进个人和集体的利益，但目前的强化学习和大型语言模型算法在利用社会结构提高性能方面效果有限。总体而言，AdaSociety是一个有价值的研究平台，用于探索不同物理和社会环境下的智能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>传统交互环境限制智能代理的发展，任务固定，单代理环境通过生成新任务增强任务多样性。</li>
<li>多代理环境中的决策受社会连接影响，改变奖励和资讯获取。</li>
<li>现有多代理环境缺乏自适应的物理环境和社会连接。</li>
<li>AdaSociety是一个可定制的多代理环境，拥有扩展的状态和行动空间，以及社会结构。</li>
<li>AdaSociety环境中，代理可以在执行任务的进程中自适应地生成具有社会结构的新任务。</li>
<li>初步研究显示特定社会结构对个人和集体有益，但现有算法在利用社会结构提高智能行为性能方面有限。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.03865">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a7a922fcb4dea37a229524deaae01767.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-40fd698afd250054ea2b435a592027fa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8a99a111b26b5137b58d4ed1caedaf33.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8d0a7c8c54cc553e2da5073afb34c2bf.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="MLE-bench-Evaluating-Machine-Learning-Agents-on-Machine-Learning-Engineering"><a href="#MLE-bench-Evaluating-Machine-Learning-Agents-on-Machine-Learning-Engineering" class="headerlink" title="MLE-bench: Evaluating Machine Learning Agents on Machine Learning   Engineering"></a>MLE-bench: Evaluating Machine Learning Agents on Machine Learning   Engineering</h2><p><strong>Authors:Jun Shern Chan, Neil Chowdhury, Oliver Jaffe, James Aung, Dane Sherburn, Evan Mays, Giulio Starace, Kevin Liu, Leon Maksin, Tejal Patwardhan, Lilian Weng, Aleksander Mądry</strong></p>
<p>We introduce MLE-bench, a benchmark for measuring how well AI agents perform at machine learning engineering. To this end, we curate 75 ML engineering-related competitions from Kaggle, creating a diverse set of challenging tasks that test real-world ML engineering skills such as training models, preparing datasets, and running experiments. We establish human baselines for each competition using Kaggle’s publicly available leaderboards. We use open-source agent scaffolds to evaluate several frontier language models on our benchmark, finding that the best-performing setup–OpenAI’s o1-preview with AIDE scaffolding–achieves at least the level of a Kaggle bronze medal in 16.9% of competitions. In addition to our main results, we investigate various forms of resource scaling for AI agents and the impact of contamination from pre-training. We open-source our benchmark code (github.com&#x2F;openai&#x2F;mle-bench&#x2F;) to facilitate future research in understanding the ML engineering capabilities of AI agents. </p>
<blockquote>
<p>我们介绍了MLE-bench，这是一个用于衡量AI代理在机器学习工程方面表现如何的基准测试。为此，我们从Kaggle中精心挑选了75个与机器学习工程相关的竞赛，创建了一系列具有挑战性的任务集，这些任务测试了现实世界中机器学习工程的技能，如训练模型、准备数据集和进行实验。我们利用Kaggle的公开排行榜为每个竞赛制定人类基准。我们使用开源代理脚手架来评估前沿语言模型在我们的基准测试上的表现，发现表现最佳的配置是OpenAI的o1-preview与AIDE脚手架，在16.9%的竞赛中至少达到Kaggle铜牌水平。除了我们的主要结果外，我们还研究了AI代理的各种资源扩展形式和预训练污染的影响。我们开源我们的基准代码（github.com&#x2F;openai&#x2F;mle-bench&#x2F;），以促进未来对AI代理机器学习工程能力的研究。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.07095v5">PDF</a> 10 pages, 17 pages appendix. Equal contribution by first seven   authors, authors randomized. Added Section A.9</p>
<p><strong>Summary</strong></p>
<p>MLE-bench是专门用于衡量AI代理在机器学习工程方面的性能的基准测试平台。通过对Kaggle上的75个与机器学习工程相关的竞赛进行整理，形成了一个多样化且具有挑战性的任务集，涉及模型训练、数据集准备和实验运行等真实世界的技能。通过开源代理脚手架对前沿语言模型进行评估，发现最佳表现的组合是OpenAI的o1-preview与AIDE脚手架组合，在某些竞赛中达到了Kaggle铜牌水平。此外，该研究还探讨了AI代理的各种资源扩展形式和预训练污染的影响，并公开了基准测试代码，以促进未来对AI代理的机器学习工程能力的研究。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MLE-bench是一个衡量AI代理在机器学习工程方面性能的基准测试平台。</li>
<li>平台通过整理Kaggle上的75个相关竞赛，形成多样化且具有挑战性的任务集。</li>
<li>开源代理脚手架被用于评估前沿语言模型。</li>
<li>最佳表现的AI代理组合是OpenAI的o1-preview与AIDE脚手架组合。</li>
<li>该组合在某些竞赛中达到了Kaggle铜牌水平。</li>
<li>研究还探讨了AI代理的资源扩展和预训练污染问题。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.07095">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-590d0dd834c04d8fcda18a842c706e7a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c30275e38a084ae99a2feb9c3b5590ab.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6029a310edb0f83fd5ac9b802eb7a464.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7a8ffefe1a18446049dbb08c8e36ff37.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-33cf9a6a8dcd92969723a32b91587a61.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="LTLf-Synthesis-on-First-Order-Agent-Programs-in-Nondeterministic-Environments"><a href="#LTLf-Synthesis-on-First-Order-Agent-Programs-in-Nondeterministic-Environments" class="headerlink" title="LTLf Synthesis on First-Order Agent Programs in Nondeterministic   Environments"></a>LTLf Synthesis on First-Order Agent Programs in Nondeterministic   Environments</h2><p><strong>Authors:Till Hofmann, Jens Claßen</strong></p>
<p>We investigate the synthesis of policies for high-level agent programs expressed in Golog, a language based on situation calculus that incorporates nondeterministic programming constructs. Unlike traditional approaches for program realization that assume full agent control or rely on incremental search, we address scenarios where environmental nondeterminism significantly influences program outcomes. Our synthesis problem involves deriving a policy that successfully realizes a given Golog program while ensuring the satisfaction of a temporal specification, expressed in Linear Temporal Logic on finite traces (LTLf), across all possible environmental behaviors. By leveraging an expressive class of first-order action theories, we construct a finite game arena that encapsulates program executions and tracks the satisfaction of the temporal goal. A game-theoretic approach is employed to derive such a policy. Experimental results demonstrate this approach’s feasibility in domains with unbounded objects and non-local effects. This work bridges agent programming and temporal logic synthesis, providing a framework for robust agent behavior in nondeterministic environments. </p>
<blockquote>
<p>我们研究了在Golog中表达的高级代理程序的策略合成。Golog是一种基于情况计算的语言，它结合了非确定性编程结构。与假设完全代理控制或依赖于增量搜索的传统程序实现方法不同，我们解决了环境非确定性对程序结果产生重大影响的场景。我们的合成问题涉及推导出成功实现给定Golog程序的策略，同时确保在所有可能的环境行为中，以有限轨迹上的线性时序逻辑（LTLf）表达的时序规范得到满足。通过利用一阶动作理论的表现力丰富的类别，我们构建了一个有限的博弈舞台，该舞台涵盖了程序执行并跟踪时序目标的满足情况。采用博弈论的方法推导出这样的策略。实验结果表明，该方法在具有无限对象和非局部影响的领域中是可行的。这项工作将代理编程和时序逻辑合成联系在一起，为在非确定性环境中实现稳健的代理行为提供了框架。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.00726v2">PDF</a> Accepted at AAAI’25</p>
<p><strong>Summary</strong></p>
<p>本文研究了在基于情境计算的语言Golog中表达的高级代理程序的策略合成。针对环境非确定性对程序结果产生显著影响的情况，提出了一种合成策略。该策略能够成功实现给定的Golog程序，并确保在所有可能的环境行为中满足线性时序逻辑对有限轨迹的时空规范。通过利用一阶动作理论，构建了一个有限的博弈场所，该场所能够封装程序执行并跟踪时间目标的满足情况。采用博弈论的方法推导出这种策略。实验结果表明，该方法在具有无限对象和非局部效应的领域中是可行的。这项工作将代理编程和时序逻辑合成联系在一起，为在不确定环境中实现稳健的代理行为提供了框架。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究了基于情境计算的语言Golog中的高级代理程序策略合成。</li>
<li>解决了环境非确定性显著影响程序结果的问题。</li>
<li>成功实现了给定Golog程序，确保在所有可能的环境行为中满足线性时序逻辑对有限轨迹的时空规范。</li>
<li>利用一阶动作理论构建了一个有限的博弈场所，用于封装程序执行并跟踪时间目标的满足情况。</li>
<li>采用博弈论方法推导出策略。</li>
<li>实验验证了该策略在具有无限对象和非局部效应的领域的可行性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.00726">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-dbd247dbdbde07585226164d8d8f9f81.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="DialSim-A-Real-Time-Simulator-for-Evaluating-Long-Term-Multi-Party-Dialogue-Understanding-of-Conversational-Agents"><a href="#DialSim-A-Real-Time-Simulator-for-Evaluating-Long-Term-Multi-Party-Dialogue-Understanding-of-Conversational-Agents" class="headerlink" title="DialSim: A Real-Time Simulator for Evaluating Long-Term Multi-Party   Dialogue Understanding of Conversational Agents"></a>DialSim: A Real-Time Simulator for Evaluating Long-Term Multi-Party   Dialogue Understanding of Conversational Agents</h2><p><strong>Authors:Jiho Kim, Woosog Chay, Hyeonji Hwang, Daeun Kyung, Hyunseung Chung, Eunbyeol Cho, Yohan Jo, Edward Choi</strong></p>
<p>Recent advancements in Large Language Models (LLMs) have significantly enhanced the capabilities of conversational agents, making them applicable to various fields (e.g., education). Despite their progress, the evaluation of the agents often overlooks the complexities of real-world conversations, such as real-time interactions, multi-party dialogues, and extended contextual dependencies. To bridge this gap, we introduce DialSim, a real-time dialogue simulator. In this simulator, an agent is assigned the role of a character from popular TV shows, requiring it to respond to spontaneous questions using past dialogue information and to distinguish between known and unknown information. Key features of DialSim include assessing the agent’s ability to respond within a reasonable time limit, handling long-term multi-party dialogues, and evaluating performance under randomized questioning with LongDialQA, a novel, high-quality question-answering dataset. Our experiments using DialSim reveal the strengths and weaknesses of the latest conversational agents, offering valuable insights for future advancements in conversational AI. DialSim is available at <a target="_blank" rel="noopener" href="https://dialsim.github.io/">https://dialsim.github.io/</a>. </p>
<blockquote>
<p>最近大型语言模型（LLM）的进步显著增强了对话代理的能力，使其适用于各个领域（例如教育）。尽管取得了进展，但对代理的评估往往忽略了现实世界中对话的复杂性，如实时互动、多方对话和扩展的上下文依赖关系。为了弥补这一差距，我们引入了DialSim，一个实时对话模拟器。在这个模拟器中，代理被分配扮演流行电视剧中的角色，需要利用过去的对话信息回答突发问题，并区分已知和未知信息。DialSim的关键功能包括评估代理在合理时间限制内作出反应的能力，处理长期多方对话，以及使用新的高质量问答数据集LongDialQA在随机提问下评估性能。我们使用DialSim进行的实验揭示了最新对话代理的优缺点，为对话AI的未来进步提供了宝贵见解。DialSim可在<a target="_blank" rel="noopener" href="https://dialsim.github.io/%E8%AE%BF%E9%97%AE%E3%80%82">https://dialsim.github.io/访问。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.13144v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）的最新进展显著增强了对话代理的能力，使其适用于多个领域（如教育）。然而，对代理的评估往往忽略了现实对话的复杂性，如实时互动、多方对话和扩展的上下文依赖关系。为了弥补这一差距，我们推出了DialSim实时对话模拟器。该模拟器要求代理扮演流行电视节目中的角色，根据过去的对话信息对突发问题作出回应，并区分已知和未知信息。DialSim的关键功能包括评估代理在合理时间限制内作出回应的能力、处理长期多方对话的能力以及使用LongDialQA这一新型高质量问答数据集在随机提问下评估表现的能力。我们的实验揭示了最新对话代理的优势和劣势，为对话AI的未来发展提供了宝贵见解。DialSim可访问网站为：<a target="_blank" rel="noopener" href="https://dialsim.github.io/%E3%80%82">https://dialsim.github.io/。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）的进步增强了对话代理的能力，推动其在多领域应用。</li>
<li>现有的代理评估方法往往忽略现实对话的复杂性，如实时互动、多方对话和上下文依赖。</li>
<li>DialSim是一个实时对话模拟器，旨在弥补上述差距。</li>
<li>DialSim要求代理扮演流行电视节目中的角色，并基于过去对话信息回应突发问题。</li>
<li>DialSim具备评估代理在特定条件下的能力，如响应时间、处理多方对话和随机提问下的表现。</li>
<li>通过使用DialSim进行的实验揭示了对话代理的优势和劣势。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.13144">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-8c584e38d7012780efb32b44fea140b0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-63d65a3a4162150e3e7d6f1345f8dc41.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-379e21ea0c54cf9485a322e0008379cd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7a61eea18be2c4241dbae6e672909886.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-45d7f7754df33406b50d25bff52bf58d.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="LLM-Based-Multi-Agent-Systems-for-Software-Engineering-Literature-Review-Vision-and-the-Road-Ahead"><a href="#LLM-Based-Multi-Agent-Systems-for-Software-Engineering-Literature-Review-Vision-and-the-Road-Ahead" class="headerlink" title="LLM-Based Multi-Agent Systems for Software Engineering: Literature   Review, Vision and the Road Ahead"></a>LLM-Based Multi-Agent Systems for Software Engineering: Literature   Review, Vision and the Road Ahead</h2><p><strong>Authors:Junda He, Christoph Treude, David Lo</strong></p>
<p>Integrating Large Language Models (LLMs) into autonomous agents marks a significant shift in the research landscape by offering cognitive abilities that are competitive with human planning and reasoning. This paper explores the transformative potential of integrating Large Language Models into Multi-Agent (LMA) systems for addressing complex challenges in software engineering (SE). By leveraging the collaborative and specialized abilities of multiple agents, LMA systems enable autonomous problem-solving, improve robustness, and provide scalable solutions for managing the complexity of real-world software projects. In this paper, we conduct a systematic review of recent primary studies to map the current landscape of LMA applications across various stages of the software development lifecycle (SDLC). To illustrate current capabilities and limitations, we perform two case studies to demonstrate the effectiveness of state-of-the-art LMA frameworks. Additionally, we identify critical research gaps and propose a comprehensive research agenda focused on enhancing individual agent capabilities and optimizing agent synergy. Our work outlines a forward-looking vision for developing fully autonomous, scalable, and trustworthy LMA systems, laying the foundation for the evolution of Software Engineering 2.0. </p>
<blockquote>
<p>将大型语言模型（LLMs）集成到自主代理中，为研究领域带来了显著的变化，提供了与人类规划和推理相竞争的认知能力。本文探讨了将大型语言模型集成到多代理（LMA）系统中，以解决软件工程（SE）中的复杂挑战的变革潜力。通过利用多个代理的协作和专门能力，LMA系统能够实现自主解决问题，提高稳健性，并提供管理真实世界软件项目复杂性的可扩展解决方案。本文将对最近的主要研究进行系统性的回顾，以映射软件开发生命周期（SDLC）各个阶段中LMA应用的当前格局。为了说明当前的能力和局限性，我们进行了两个案例研究，以展示最新LMA框架的有效性。此外，我们还确定了关键的研究空白，并提出了以增强单个代理能力和优化代理协同为重点的综合研究议程。我们的工作为开发完全自主、可扩展和可信赖的LMA系统描绘了一个前瞻性的愿景，为软件工程2.0的演变奠定了基础。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.04834v3">PDF</a> TOSEM 2030 Special Issue</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）融入自主代理标志着研究领域的重大转变，为软件工程中解决复杂挑战提供了与人类规划、推理相竞争的认知能力。本文探讨了将大型语言模型融入多代理（LMA）系统的变革潜力，通过多个代理的协作和特殊能力，LMA系统可实现自主解决问题、提高稳健性，并为真实世界软件项目的复杂性管理提供可扩展解决方案。本文进行近期主要研究的系统综述，以了解LMA应用在软件开发生命周期（SDLC）各阶段中的现状。通过两个案例研究展示最先进LMA框架的有效性并发现重要研究差距，提出优化个体代理能力和代理协同的全面研究议程。本文工作为未来软件工程的自主化、可扩展性和可信度发展奠定了基石。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>大型语言模型（LLM）融入自主代理为软件工程中解决复杂挑战提供了与人类规划、推理能力相当的认知工具。</li>
<li>LMA系统通过多代理的协作和特殊能力实现自主解决问题，提高稳健性并应对真实世界软件项目的复杂性管理需求。</li>
<li>系统综述展示了LMA在软件开发生命周期不同阶段的现状，通过案例研究证明了最先进LMA框架的有效性。</li>
<li>发现当前研究的重大差距并呼吁增强个体代理能力与优化代理协同的研究议程。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.04834">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e7fab5bfcd969e38795c0e11089b5ff0.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="ExpeL-LLM-Agents-Are-Experiential-Learners"><a href="#ExpeL-LLM-Agents-Are-Experiential-Learners" class="headerlink" title="ExpeL: LLM Agents Are Experiential Learners"></a>ExpeL: LLM Agents Are Experiential Learners</h2><p><strong>Authors:Andrew Zhao, Daniel Huang, Quentin Xu, Matthieu Lin, Yong-Jin Liu, Gao Huang</strong></p>
<p>The recent surge in research interest in applying large language models (LLMs) to decision-making tasks has flourished by leveraging the extensive world knowledge embedded in LLMs. While there is a growing demand to tailor LLMs for custom decision-making tasks, finetuning them for specific tasks is resource-intensive and may diminish the model’s generalization capabilities. Moreover, state-of-the-art language models like GPT-4 and Claude are primarily accessible through API calls, with their parametric weights remaining proprietary and unavailable to the public. This scenario emphasizes the growing need for new methodologies that allow learning from agent experiences without requiring parametric updates. To address these problems, we introduce the Experiential Learning (ExpeL) agent. Our agent autonomously gathers experiences and extracts knowledge using natural language from a collection of training tasks. At inference, the agent recalls its extracted insights and past experiences to make informed decisions. Our empirical results highlight the robust learning efficacy of the ExpeL agent, indicating a consistent enhancement in its performance as it accumulates experiences. We further explore the emerging capabilities and transfer learning potential of the ExpeL agent through qualitative observations and additional experiments. </p>
<blockquote>
<p>最近，对于将大型语言模型（LLM）应用于决策制定任务的研究兴趣激增，这得益于LLM中嵌入的丰富的世界知识。虽然对针对特定决策制定任务定制LLM的需求不断增长，但对它们进行微调是资源密集型的，并且可能会降低模型的泛化能力。此外，像GPT-4和Claude等最先进的语言模型主要通过API调用访问，其参数权重保持专有且不对公众开放。这一情景强调了对新方法的需求不断增长，这些方法允许从代理经验中学习而无需进行参数更新。为了解决这些问题，我们引入了体验式学习（ExpeL）代理。我们的代理能够自主收集经验并使用自然语言从一系列训练任务中提取知识。在推理过程中，代理会回想起其提取的见解和过去的经验来做出决策。我们的实证结果突出了ExpeL代理的稳健学习效益，显示随着经验的积累，其性能持续提高。我们进一步通过定性观察和额外实验探索了ExpeL代理的新兴能力和迁移学习的潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10144v3">PDF</a> Accepted by the 38th Annual AAAI Conference on Artificial   Intelligence (AAAI-24)</p>
<p><strong>Summary</strong></p>
<p>大语言模型（LLMs）在决策任务中的应用近期受到广泛关注，借助LLMs中嵌入的丰富世界知识，相关研究蓬勃发展。然而，针对特定任务定制LLMs的需求日益增长，但对其进行微调需要大量资源，并可能降低模型的泛化能力。主流语言模型如GPT-4和Claude主要通过API调用访问，其参数权重保持专有，无法公开。在此背景下，强调了对新方法论的需求，该方法论需要能够在不要求进行参数更新的情况下从代理经验中学习。为解决这些问题，我们推出了Experiential Learning（ExpeL）代理。该代理能够自主收集经验，并利用自然语言从大量训练任务中提取知识。在推理过程中，代理会回顾其提取的见解和过去的经验以做出明智的决策。我们的实证结果突出了ExpeL代理的强大学习效能，显示随着经验的积累，其性能持续提高。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大语言模型（LLMs）在决策任务中的应用受到关注，但定制LLMs的微调需要大量资源和可能影响模型的泛化能力。</li>
<li>当前主流语言模型如GPT-4和Claude通过API调用访问，参数权重保持专有。</li>
<li>需要新的方法论，能够在不更新参数的情况下从代理经验中学习。</li>
<li>Experiential Learning（ExpeL）代理能够自主收集经验并从训练任务中提取知识。</li>
<li>ExpeL代理利用自然语言和过去的经验来做出决策。</li>
<li>实证结果显示ExpeL代理具有强大的学习效能，随着经验的积累，性能持续提高。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2308.10144">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e3a6392c12db5e50ec1317060499316d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-397cfa93ed1d77ad62be41ef8e4f341a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f2f7200cdda4bad7117b6627e563efcc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-707db1600814640234730b3437f2a19a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7e083a5b67d99bab4426f444490ee70f.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-24/Agent/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-24/Agent/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Agent/">
                                    <span class="chip bg-color">Agent</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-24/Few-Shot/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-f6f173a6496bfc3db3faf3a0df7918b5.jpg" class="responsive-img" alt="Few-Shot">
                        
                        <span class="card-title">Few-Shot</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Few-Shot 方向最新论文已更新，请持续关注 Update in 2024-12-24  MR-GDINO Efficient Open-World Continual Object Detection
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-12-24
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                    Few-Shot
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Few-Shot/">
                        <span class="chip bg-color">Few-Shot</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-24/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-be79195cff13bb52b890b45f510ba140.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM 方向最新论文已更新，请持续关注 Update in 2024-12-24  HoVLE Unleashing the Power of Monolithic Vision-Language Models with   Holistic Vision-Language Embedding
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-24
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">11880.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
