<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-09  QiMeng-SALV Signal-Aware Learning for Verilog Code Generation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-169d63785192baad672571e5927a1cfe')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-09
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-18
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    17.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    72 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-11-09-æ›´æ–°"><a href="#2025-11-09-æ›´æ–°" class="headerlink" title="2025-11-09 æ›´æ–°"></a>2025-11-09 æ›´æ–°</h1><h2 id="QiMeng-SALV-Signal-Aware-Learning-for-Verilog-Code-Generation"><a href="#QiMeng-SALV-Signal-Aware-Learning-for-Verilog-Code-Generation" class="headerlink" title="QiMeng-SALV: Signal-Aware Learning for Verilog Code Generation"></a>QiMeng-SALV: Signal-Aware Learning for Verilog Code Generation</h2><p><strong>Authors:Yang Zhang, Rui Zhang, Jiaming Guo, Lei Huang, Di Huang, Yunpu Zhao, Shuyao Cheng, Pengwei Jin, Chongxiao Li, Zidong Du, Xing Hu, Qi Guo, Yunji Chen</strong></p>
<p>The remarkable progress of Large Language Models (LLMs) presents promising opportunities for Verilog code generation which is significantly important for automated circuit design. The lacking of meaningful functional rewards hinders the preference optimization based on Reinforcement Learning (RL) for producing functionally correct Verilog code. In this paper, we propose Signal-Aware Learning for Verilog code generation (QiMeng-SALV) by leveraging code segments of functionally correct output signal to optimize RL training. Considering Verilog code specifies the structural interconnection of hardware gates and wires so that different output signals are independent, the key insight of QiMeng-SALV is to extract verified signal-aware implementations in partially incorrect modules, so as to enhance the extraction of meaningful functional rewards. Roughly, we verify the functional correctness of signals in generated module by comparing with that of reference module in the training data. Then abstract syntax tree (AST) is employed to identify signal-aware code segments which can provide meaningful functional rewards from erroneous modules. Finally, we introduce signal-aware DPO which is optimized on the correct signal-level code segments, thereby preventing noise and interference from incorrect signals. The proposed QiMeng-SALV underscores the paradigm shift from conventional module-level to fine-grained signal-level optimization in Verilog code generation, addressing the issue of insufficient functional rewards. Experiments demonstrate that our method achieves state-of-the-art performance on VerilogEval and RTLLM, with a 7B parameter model matching the performance of the DeepSeek v3 671B model and significantly outperforming the leading open-source model CodeV trained on the same dataset. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/zy1xxx/SALV">https://github.com/zy1xxx/SALV</a>. </p>
<blockquote>
<p>è‡ªç„¶è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ˜¾è‘—è¿›æ­¥ä¸ºVerilogä»£ç ç”Ÿæˆæä¾›äº†å……æ»¡å¸Œæœ›çš„æœºé‡ï¼Œè¿™å¯¹è‡ªåŠ¨åŒ–ç”µè·¯è®¾è®¡å…·æœ‰é‡è¦æ„ä¹‰ã€‚ç¼ºä¹æœ‰æ„ä¹‰çš„å¥–åŠ±é˜»ç¢äº†åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„ä¼˜åŒ–åå¥½ï¼Œä»è€Œæ— æ³•ç”ŸæˆåŠŸèƒ½æ­£ç¡®çš„Verilogä»£ç ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†åˆ©ç”¨åŠŸèƒ½æ­£ç¡®çš„è¾“å‡ºä¿¡å·ä»£ç ç‰‡æ®µä¼˜åŒ–å¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„Verilogä»£ç ç”Ÿæˆçš„ä¿¡å·æ„ŸçŸ¥å­¦ä¹ ï¼ˆQiMeng-SALVï¼‰ã€‚è€ƒè™‘åˆ°Verilogä»£ç æŒ‡å®šç¡¬ä»¶é—¨å’Œå¯¼çº¿çš„ç»“æ„äº’è¿ï¼Œä¸åŒè¾“å‡ºä¿¡å·æ˜¯ç‹¬ç«‹çš„ï¼ŒQiMeng-SALVçš„å…³é”®åœ¨äºæå–éƒ¨åˆ†é”™è¯¯æ¨¡å—ä¸­çš„éªŒè¯ä¿¡å·æ„ŸçŸ¥å®ç°ï¼Œä»¥å¢å¼ºæœ‰æ„ä¹‰çš„åŠŸèƒ½å¥–åŠ±çš„æå–ã€‚å¤§è‡´ä¸Šï¼Œæˆ‘ä»¬é€šè¿‡å°†ç”Ÿæˆçš„æ¨¡å—ä¸­çš„ä¿¡å·åŠŸèƒ½ä¸è®­ç»ƒæ•°æ®ä¸­çš„å‚è€ƒæ¨¡å—è¿›è¡Œæ¯”è¾ƒï¼ŒéªŒè¯ä¿¡å·çš„å‡½æ•°æ­£ç¡®æ€§ã€‚ç„¶åï¼Œä½¿ç”¨æŠ½è±¡è¯­æ³•æ ‘ï¼ˆASTï¼‰æ¥è¯†åˆ«æ¥è‡ªé”™è¯¯æ¨¡å—çš„èƒ½æä¾›æœ‰æ„ä¹‰åŠŸèƒ½å¥–åŠ±çš„ä¿¡å·æ„ŸçŸ¥ä»£ç ç‰‡æ®µã€‚æœ€åï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¿¡å·æ„ŸçŸ¥DPOï¼Œå®ƒåœ¨æ­£ç¡®çš„ä¿¡å·çº§ä»£ç ç‰‡æ®µä¸Šè¿›è¡Œä¼˜åŒ–ï¼Œä»è€Œé˜²æ­¢äº†é”™è¯¯ä¿¡å·çš„å™ªå£°å’Œå¹²æ‰°ã€‚æå‡ºçš„QiMeng-SALVå¼ºè°ƒäº†Verilogä»£ç ç”Ÿæˆä¸­ä»ä¼ ç»Ÿçš„æ¨¡å—çº§åˆ°ç»†ç²’åº¦çš„ä¿¡å·çº§ä¼˜åŒ–çš„èŒƒå¼è½¬å˜ï¼Œè§£å†³äº†åŠŸèƒ½å¥–åŠ±ä¸è¶³çš„é—®é¢˜ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨VerilogEvalå’ŒRTLLVMä¸Šè¾¾åˆ°äº†æœ€æ–°æ€§èƒ½æ°´å¹³ï¼Œä½¿ç”¨ä¸€ä¸ªè§„æ¨¡ä¸º7Bçš„å‚æ•°æ¨¡å‹å³å¯ä¸DeepSeek v3 671Bæ¨¡å‹æ€§èƒ½ç›¸åŒ¹é…ï¼Œå¹¶ä¸”åœ¨åŒä¸€æ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºé¢†å…ˆçš„å¼€æºæ¨¡å‹CodeVã€‚æˆ‘ä»¬çš„ä»£ç ä½äº<a target="_blank" rel="noopener" href="https://github.com/zy1xxx/SALV%E3%80%82">https://github.com/zy1xxx/SALVã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.19296v2">PDF</a> Accepted to NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ˜¾è‘—è¿›å±•ä¸ºVerilogä»£ç ç”Ÿæˆå¸¦æ¥äº†å……æ»¡å¸Œæœ›çš„æœºé‡ã€‚åœ¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸­ï¼Œç”±äºç¼ºä¹æœ‰æ„ä¹‰çš„åŠŸèƒ½å¥–åŠ±é˜»ç¢äº†åŸºäºåŠŸèƒ½æ­£ç¡®çš„Verilogä»£ç çš„ä¼˜åŒ–ã€‚æœ¬æ–‡æå‡ºåˆ©ç”¨åŠŸèƒ½æ­£ç¡®çš„è¾“å‡ºä¿¡å·çš„ä»£ç ç‰‡æ®µä¼˜åŒ–RLè®­ç»ƒçš„ä¿¡å·æ„ŸçŸ¥å­¦ä¹ ï¼ˆQiMeng-SALVï¼‰ã€‚é€šè¿‡éªŒè¯ç”Ÿæˆæ¨¡å—ä¸­ä¿¡å·çš„åŠŸèƒ½æ­£ç¡®æ€§ï¼Œå¹¶ä»é”™è¯¯çš„æ¨¡å—ä¸­æå–éªŒè¯è¿‡çš„ä¿¡å·æ„ŸçŸ¥å®ç°ï¼Œä»¥å¢å¼ºæœ‰æ„ä¹‰çš„åŠŸèƒ½å¥–åŠ±çš„æå–ã€‚æœ€åï¼Œæˆ‘ä»¬ä»‹ç»äº†åœ¨æ­£ç¡®çš„ä¿¡å·çº§åˆ«ä»£ç ç‰‡æ®µä¸Šè¿›è¡Œä¼˜åŒ–çš„ä¿¡å·æ„ŸçŸ¥DPOï¼Œä»è€Œé˜²æ­¢äº†é”™è¯¯ä¿¡å·çš„å™ªå£°å’Œå¹²æ‰°ã€‚QiMeng-SALVä»ä¼ ç»Ÿçš„æ¨¡å—çº§åˆ«è½¬å‘ç²¾ç»†ç²’åº¦çš„ä¿¡å·çº§åˆ«ä¼˜åŒ–ï¼Œè§£å†³äº†Verilogä»£ç ç”Ÿæˆä¸­åŠŸèƒ½å¥–åŠ±ä¸è¶³çš„é—®é¢˜ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨VerilogEvalå’ŒRTLLVMä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsçš„è¿›æ­¥ä¸ºVerilogä»£ç ç”Ÿæˆå¸¦æ¥äº†æ–°æœºé‡ã€‚</li>
<li>ç¼ºä¹æœ‰æ„ä¹‰çš„åŠŸèƒ½å¥–åŠ±æ˜¯RLåœ¨Verilogä»£ç ç”Ÿæˆä¸­çš„æŒ‘æˆ˜ã€‚</li>
<li>QiMeng-SALVé€šè¿‡åˆ©ç”¨åŠŸèƒ½æ­£ç¡®çš„è¾“å‡ºä¿¡å·ä»£ç ç‰‡æ®µæ¥ä¼˜åŒ–RLè®­ç»ƒã€‚</li>
<li>QiMeng-SALVéªŒè¯ç”Ÿæˆæ¨¡å—ä¸­ä¿¡å·çš„åŠŸèƒ½æ­£ç¡®æ€§ã€‚</li>
<li>ä¿¡å·æ„ŸçŸ¥DPOèƒ½é˜²æ­¢é”™è¯¯ä¿¡å·çš„å™ªå£°å’Œå¹²æ‰°ã€‚</li>
<li>QiMeng-SALVå®ç°äº†ä»æ¨¡å—çº§åˆ«åˆ°ä¿¡å·çº§åˆ«çš„ä¼˜åŒ–è½¬å˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.19296">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-393aa708b654187141d6cb475ebc00b1" align="middle">
<img src="https://picx.zhimg.com/v2-3b5269ea0fee04e7297692ace3b0a189" align="middle">
<img src="https://picx.zhimg.com/v2-0794df6be86f3de4d383815ec4d8015a" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="The-Zero-Step-Thinking-An-Empirical-Study-of-Mode-Selection-as-Harder-Early-Exit-in-Reasoning-Models"><a href="#The-Zero-Step-Thinking-An-Empirical-Study-of-Mode-Selection-as-Harder-Early-Exit-in-Reasoning-Models" class="headerlink" title="The Zero-Step Thinking: An Empirical Study of Mode Selection as Harder   Early Exit in Reasoning Models"></a>The Zero-Step Thinking: An Empirical Study of Mode Selection as Harder   Early Exit in Reasoning Models</h2><p><strong>Authors:Yuqiao Tan, Shizhu He, Kang Liu, Jun Zhao</strong></p>
<p>Reasoning models have demonstrated exceptional performance in tasks such as mathematics and logical reasoning, primarily due to their ability to engage in step-by-step thinking during the reasoning process. However, this often leads to overthinking, resulting in unnecessary computational overhead. To address this issue, Mode Selection aims to automatically decide between Long-CoT (Chain-of-Thought) or Short-CoT by utilizing either a Thinking or NoThinking mode. Simultaneously, Early Exit determines the optimal stopping point during the iterative reasoning process. Both methods seek to reduce the computational burden. In this paper, we first identify Mode Selection as a more challenging variant of the Early Exit problem, as they share similar objectives but differ in decision timing. While Early Exit focuses on determining the best stopping point for concise reasoning at inference time, Mode Selection must make this decision at the beginning of the reasoning process, relying on pre-defined fake thoughts without engaging in an explicit reasoning process, referred to as zero-step thinking. Through empirical studies on nine baselines, we observe that prompt-based approaches often fail due to their limited classification capabilities when provided with minimal hand-crafted information. In contrast, approaches that leverage internal information generally perform better across most scenarios but still exhibit issues with stability. Our findings indicate that existing methods relying solely on the information provided by models are insufficient for effectively addressing Mode Selection in scenarios with limited information, highlighting the ongoing challenges of this task. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/Trae1ounG/Zero_Step_Thinking">https://github.com/Trae1ounG/Zero_Step_Thinking</a>. </p>
<blockquote>
<p>æ¨ç†æ¨¡å‹åœ¨æ•°å­¦å’Œé€»è¾‘æ¨ç†ç­‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†å“è¶Šçš„æ€§èƒ½ï¼Œè¿™ä¸»è¦å½’åŠŸäºå…¶åœ¨æ¨ç†è¿‡ç¨‹ä¸­è¿›è¡Œé€æ­¥æ€è€ƒçš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™å¸¸å¸¸å¯¼è‡´è¿‡åº¦æ€è€ƒï¼Œä»è€Œäº§ç”Ÿä¸å¿…è¦çš„è®¡ç®—è´Ÿæ‹…ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæ¨¡å¼é€‰æ‹©æ—¨åœ¨é€šè¿‡åˆ©ç”¨æ€è€ƒæˆ–æ— æ€è€ƒæ¨¡å¼æ¥è‡ªåŠ¨é€‰æ‹©åœ¨é•¿æ¨ç†é“¾ï¼ˆLong-CoTï¼‰æˆ–çŸ­æ¨ç†é“¾ï¼ˆShort-CoTï¼‰ä¹‹é—´è¿›è¡Œé€‰æ‹©ã€‚åŒæ—¶ï¼Œæ—©æœŸé€€å‡ºï¼ˆEarly Exitï¼‰åˆ™ç¡®å®šè¿­ä»£æ¨ç†è¿‡ç¨‹ä¸­çš„æœ€ä½³åœæ­¢ç‚¹ã€‚è¿™ä¸¤ç§æ–¹æ³•éƒ½æ—¨åœ¨å‡å°‘è®¡ç®—è´Ÿæ‹…ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆå°†æ¨¡å¼é€‰æ‹©è¯†åˆ«ä¸ºæ—©æœŸé€€å‡ºé—®é¢˜çš„æ›´å…·æŒ‘æˆ˜æ€§çš„å˜ä½“ï¼Œå› ä¸ºå®ƒä»¬åœ¨ç›®æ ‡ä¸Šå…·æœ‰ç›¸ä¼¼æ€§ï¼Œä½†åœ¨å†³ç­–æ—¶é—´ä¸Šå­˜åœ¨å·®å¼‚ã€‚æ—©æœŸé€€å‡ºä¾§é‡äºåœ¨æ¨ç†æ—¶ç¡®å®šæœ€ä½³åœæ­¢ç‚¹ä»¥è¿›è¡Œç®€æ´æ¨ç†ï¼Œè€Œæ¨¡å¼é€‰æ‹©å¿…é¡»åœ¨æ¨ç†è¿‡ç¨‹çš„å¼€å§‹é˜¶æ®µåšå‡ºå†³å®šï¼Œä¾èµ–äºé¢„å…ˆè®¾å®šçš„è™šæ‹Ÿæ€è€ƒï¼Œè€Œä¸æ¶‰åŠæ˜ç¡®çš„æ¨ç†è¿‡ç¨‹ï¼Œè¿™è¢«ç§°ä¸ºé›¶æ­¥æ€è€ƒã€‚é€šè¿‡å¯¹ä¹ä¸ªåŸºå‡†ç‚¹è¿›è¡Œå®è¯ç ”ç©¶ï¼Œæˆ‘ä»¬å‘ç°åŸºäºæç¤ºçš„æ–¹æ³•å¾€å¾€å› åˆ†ç±»èƒ½åŠ›æœ‰é™è€Œå¤±è´¥ï¼Œå½“æä¾›çš„ä¿¡æ¯å¾ˆå°‘æ—¶æ›´æ˜¯å¦‚æ­¤ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œåˆ©ç”¨å†…éƒ¨ä¿¡æ¯çš„åšæ³•åœ¨å¤§å¤šæ•°åœºæ™¯ä¸­è¡¨ç°æ›´å¥½ï¼Œä½†ä»ç„¶å­˜åœ¨ç¨³å®šæ€§é—®é¢˜ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œä»…ä¾èµ–æ¨¡å‹æä¾›çš„ä¿¡æ¯çš„ç°æœ‰æ–¹æ³•ä¸è¶³ä»¥åœ¨æœ‰é™ä¿¡æ¯çš„åœºæ™¯ä¸­æœ‰æ•ˆåœ°è§£å†³æ¨¡å¼é€‰æ‹©é—®é¢˜ï¼Œè¿™çªæ˜¾äº†è¿™é¡¹ä»»åŠ¡æ‰€é¢ä¸´çš„æŒç»­æŒ‘æˆ˜ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Trae1ounG/Zero_Step_Thinking">https://github.com/Trae1ounG/Zero_Step_Thinking</a>æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.19176v1">PDF</a> Accepted by NeurIPSâ€™25 Efficient Reasoning Workshop</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†æ¨ç†æ¨¡å‹åœ¨ä»»åŠ¡ä¸­çš„å‡ºè‰²è¡¨ç°åŠå…¶å¯¼è‡´çš„è¿‡åº¦è®¡ç®—é—®é¢˜ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæå‡ºäº†Mode Selectionå’ŒEarly Exitä¸¤ç§æ–¹æ³•ã€‚Mode Selectionèƒ½åœ¨æ¨ç†è¿‡ç¨‹å¼€å§‹æ—¶è‡ªåŠ¨é€‰æ‹©Long-CoTæˆ–Short-CoTæ¨¡å¼ï¼Œè€ŒEarly Exitåˆ™ç¡®å®šæ¨ç†è¿‡ç¨‹ä¸­çš„æœ€ä½³åœæ­¢ç‚¹ã€‚æ–‡ç« æŒ‡å‡ºMode Selectionæ˜¯Early Exitçš„ä¸€ä¸ªæ›´å…·æŒ‘æˆ˜æ€§çš„å˜ä½“ï¼Œå› ä¸ºå®ƒå¿…é¡»åœ¨æ²¡æœ‰æ˜ç¡®çš„æ¨ç†è¿‡ç¨‹çš„æƒ…å†µä¸‹ï¼Œä¾é é¢„è®¾çš„è™šæ‹Ÿæƒ³æ³•åšå‡ºå†³ç­–ã€‚é€šè¿‡å¯¹ä¹ç§åŸºå‡†æ–¹æ³•çš„å®è¯ç ”ç©¶ï¼Œå‘ç°åŸºäºæç¤ºçš„æ–¹æ³•åœ¨æä¾›å°‘é‡æ‰‹å·¥ä¿¡æ¯æ—¶å¾€å¾€è¡¨ç°ä¸ä½³ï¼Œè€Œåˆ©ç”¨å†…éƒ¨ä¿¡æ¯çš„æ–¹æ³•åœ¨å¤§å¤šæ•°åœºæ™¯ä¸­è¡¨ç°æ›´å¥½ä½†ä»å­˜åœ¨ç¨³å®šæ€§é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨æœ‰é™ä¿¡æ¯çš„åœºæ™¯ä¸‹ï¼Œä»…ä¾èµ–æ¨¡å‹æä¾›çš„ä¿¡æ¯ä¸è¶³ä»¥æœ‰æ•ˆè§£å†³Mode Selectioné—®é¢˜ï¼Œè¡¨æ˜è¯¥ä»»åŠ¡ä»å­˜åœ¨æŒ‘æˆ˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¨ç†æ¨¡å‹åœ¨æ•°å­¦å’Œé€»è¾‘æ¨ç†ç­‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å¸¸å¸¸å¯¼è‡´è¿‡åº¦è®¡ç®—ã€‚</li>
<li>Mode Selectionå’ŒEarly Exitä¸¤ç§æ–¹æ³•è¢«æå‡ºæ¥å‡å°‘è®¡ç®—è´Ÿæ‹…ã€‚</li>
<li>Mode Selectionè‡ªåŠ¨é€‰æ‹©æ¨ç†æ¨¡å¼ï¼Œè€ŒEarly Exitç¡®å®šæ¨ç†è¿‡ç¨‹çš„æœ€ä½³åœæ­¢ç‚¹ã€‚</li>
<li>Mode Selectionæ˜¯Early Exitçš„ä¸€ä¸ªæ›´å…·æŒ‘æˆ˜æ€§çš„å˜ä½“ï¼Œå› ä¸ºå®ƒå¿…é¡»åœ¨æ²¡æœ‰æ˜ç¡®çš„æ¨ç†è¿‡ç¨‹çš„æƒ…å†µä¸‹åšå‡ºå†³ç­–ã€‚</li>
<li>åŸºäºæç¤ºçš„æ–¹æ³•åœ¨æä¾›å°‘é‡æ‰‹å·¥ä¿¡æ¯æ—¶è¡¨ç°ä¸ä½³ã€‚</li>
<li>åˆ©ç”¨å†…éƒ¨ä¿¡æ¯çš„æ–¹æ³•åœ¨å¤§å¤šæ•°åœºæ™¯ä¸­è¡¨ç°è¾ƒå¥½ï¼Œä½†ä»å­˜åœ¨ç¨³å®šæ€§é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.19176">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-556b18f50a4f5fbac86da746afb9da27" align="middle">
<img src="https://picx.zhimg.com/v2-14620620b192c7fa57f099f126d505e5" align="middle">
<img src="https://picx.zhimg.com/v2-b9bdfcdb7365fe826fcbc1663119d915" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Noise-corrected-GRPO-From-Noisy-Rewards-to-Unbiased-Gradients"><a href="#Noise-corrected-GRPO-From-Noisy-Rewards-to-Unbiased-Gradients" class="headerlink" title="Noise-corrected GRPO: From Noisy Rewards to Unbiased Gradients"></a>Noise-corrected GRPO: From Noisy Rewards to Unbiased Gradients</h2><p><strong>Authors:Omar El Mansouri, Mohamed El Amine Seddik, Salem Lahlou</strong></p>
<p>Reinforcement learning from human feedback (RLHF) or verifiable rewards (RLVR), the standard paradigm for aligning LLMs or building recent SOTA reasoning models, is highly sensitive to noise from inconsistent or erroneous rewards. Yet, the interaction between such noise and widely used group-based policy optimization methods remains underexplored. We introduce a noise-robust Group Relative Policy Optimization (GRPO) and Done Right GRPO (Dr.GRPO) framework that explicitly models reward corruption as Bernoulli noise. Our method applies noise correction after estimating reward flip probabilities to debias the learning signal, yielding provably unbiased gradient estimates. Theoretical analysis shows that group-based methods inherently mitigate individual-level noise, and our correction strategy amplifies this robustness. Empirically, we observe consistent improvements across math and code tasks when applying our noise correction to standard reward model usage, with particular gains of up to 6.7 percentage points in accuracy on math tasks and 1.5 on code tasks under realistic reward model conditions. This work bridges label-noise correction from supervised learning with modern RLHF, offering both theoretical insights and a practical algorithm for noisy real-world deployment. </p>
<blockquote>
<p>åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰æˆ–å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰æ˜¯è°ƒæ•´å¤§å‹è¯­è¨€æ¨¡å‹æˆ–æ„å»ºæœ€æ–°å…ˆè¿›æ¨ç†æ¨¡å‹çš„æ ‡å‡†èŒƒå¼ï¼Œå¯¹äºæ¥è‡ªä¸ä¸€è‡´æˆ–é”™è¯¯å¥–åŠ±çš„å™ªå£°å…·æœ‰é«˜åº¦æ•æ„Ÿæ€§ã€‚ç„¶è€Œï¼Œæ­¤ç±»å™ªå£°ä¸å¹¿æ³›ä½¿ç”¨çš„åŸºäºç¾¤ä½“çš„ç­–ç•¥ä¼˜åŒ–æ–¹æ³•ä¹‹é—´çš„ç›¸äº’ä½œç”¨ä»æœªè¢«å……åˆ†æ¢ç´¢ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ç¨³å¥çš„ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰å’Œæ­£ç¡®å®ŒæˆGRPOï¼ˆDr.GRPOï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ˜¾å¼åœ°å°†å¥–åŠ±è…è´¥å»ºæ¨¡ä¸ºä¼¯åŠªåˆ©å™ªå£°ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¼°è®¡å¥–åŠ±ç¿»è½¬æ¦‚ç‡åè¿›è¡Œå™ªå£°æ ¡æ­£ï¼Œä»¥æ¶ˆé™¤å­¦ä¹ ä¿¡å·çš„åè§ï¼Œä»è€Œäº§ç”Ÿå¯è¯æ˜çš„æ— åè§æ¢¯åº¦ä¼°è®¡ã€‚ç†è®ºåˆ†æè¡¨æ˜ï¼ŒåŸºäºç¾¤ä½“çš„æ–¹æ³•æœ¬è´¨ä¸Šå‡è½»äº†ä¸ªäººå±‚é¢çš„å™ªå£°ï¼Œæˆ‘ä»¬çš„æ ¡æ­£ç­–ç•¥å¢å¼ºäº†è¿™ç§ç¨³å¥æ€§ã€‚ä»å®è¯è§’åº¦çœ‹ï¼Œåœ¨æˆ‘ä»¬çš„å™ªå£°æ ¡æ­£åº”ç”¨äºæ ‡å‡†å¥–åŠ±æ¨¡å‹ä½¿ç”¨æ—¶ï¼Œæ•°å­¦å’Œä»£ç ä»»åŠ¡ä¸Šå‡è§‚å¯Ÿåˆ°äº†ä¸€è‡´çš„æ”¹è¿›ï¼Œå…¶ä¸­æ•°å­¦ä»»åŠ¡åœ¨å‡†ç¡®ç‡ä¸Šæé«˜äº†é«˜è¾¾6.7ä¸ªç™¾åˆ†ç‚¹ï¼Œä»£ç ä»»åŠ¡æé«˜äº†1.5ä¸ªç™¾åˆ†ç‚¹ï¼Œå¤„äºç°å®çš„å¥–åŠ±æ¨¡å‹æ¡ä»¶ä¸‹ã€‚è¿™é¡¹å·¥ä½œå°†ç›‘ç£å­¦ä¹ ä¸­çš„æ ‡ç­¾å™ªå£°æ ¡æ­£ä¸ç°ä»£RLHFç›¸ç»“åˆï¼Œä¸ºå™ªå£°ç°å®ä¸–ç•Œéƒ¨ç½²æä¾›äº†ç†è®ºè§è§£å’Œå®ç”¨ç®—æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.18924v2">PDF</a> </p>
<p><strong>Summary</strong><br>     é’ˆå¯¹å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰æˆ–å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰çš„æ ‡å‡†èŒƒå¼ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ç¨³å¥çš„ç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰å’Œæ­£ç¡®å®ŒæˆGRPOï¼ˆDr.GRPOï¼‰æ¡†æ¶ã€‚è¯¥æ¡†æ¶æ˜¾å¼åœ°å»ºæ¨¡å¥–åŠ±è…è´¥ä¸ºä¼¯åŠªåˆ©å™ªå£°ï¼Œå¹¶åœ¨ä¼°è®¡å¥–åŠ±ç¿»è½¬æ¦‚ç‡ååº”ç”¨å™ªå£°æ ¡æ­£æ¥çº æ­£å­¦ä¹ ä¿¡å·ï¼Œä»è€Œäº§ç”Ÿå¯éªŒè¯çš„æ— åæ¢¯åº¦ä¼°è®¡ã€‚ç†è®ºåˆ†æå’Œå®è¯ç»“æœè¡¨æ˜ï¼Œç¾¤ç»„æ–¹æ³•æœ¬èº«å°±èƒ½ç¼“è§£ä¸ªä½“å±‚é¢çš„å™ªå£°ï¼Œè€Œæˆ‘ä»¬çš„æ ¡æ­£ç­–ç•¥å¢å¼ºäº†è¿™ç§ç¨³å¥æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆæˆ–å¯éªŒè¯å¥–åŠ±çš„æ ‡å‡†èŒƒå¼å¯¹å™ªå£°æ•æ„Ÿï¼Œå™ªå£°æ¥æºäºä¸ä¸€è‡´æˆ–é”™è¯¯çš„å¥–åŠ±ã€‚</li>
<li>æå‡ºçš„Group Relative Policy Optimization (GRPO) å’Œ Done Right GRPO (Dr.GRPO) æ¡†æ¶æ˜¾å¼åœ°å»ºæ¨¡å¥–åŠ±è…è´¥ä¸ºä¼¯åŠªåˆ©å™ªå£°ã€‚</li>
<li>é€šè¿‡å™ªå£°æ ¡æ­£ä¼°è®¡å¥–åŠ±ç¿»è½¬æ¦‚ç‡ï¼Œä»¥çº æ­£å­¦ä¹ ä¿¡å·ï¼Œäº§ç”Ÿå¯éªŒè¯çš„æ— åæ¢¯åº¦ä¼°è®¡ã€‚</li>
<li>ç¾¤ä½“æ–¹æ³•æœ¬èº«å°±å…·æœ‰ç¼“è§£ä¸ªä½“å±‚é¢å™ªå£°çš„ç‰¹æ€§ã€‚</li>
<li>å™ªå£°æ ¡æ­£ç­–ç•¥æé«˜äº†ç¾¤ä½“æ–¹æ³•çš„ç¨³å¥æ€§ã€‚</li>
<li>åœ¨æ•°å­¦å’Œä»£ç ä»»åŠ¡ä¸Šåº”ç”¨å™ªå£°æ ¡æ­£ï¼Œå¯ä»¥è§‚å¯Ÿåˆ°æŒç»­çš„æ€§èƒ½æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.18924">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5feae24b58b913f196ef6a42229d212a" align="middle">
<img src="https://picx.zhimg.com/v2-26daa5a9320027c1e4972f6aaa572dbf" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="KAT-Coder-Technical-Report"><a href="#KAT-Coder-Technical-Report" class="headerlink" title="KAT-Coder Technical Report"></a>KAT-Coder Technical Report</h2><p><strong>Authors:Zizheng Zhan, Ken Deng, Jinghui Wang, Xiaojiang Zhang, Huaixi Tang, Minglei Zhang, Zhiyi Lai, Haoyang Huang, Wen Xiang, Kun Wu, Wenhao Zhuang, Shaojie Wang, Shangpeng Yan, Kepeng Lei, Zongxian Feng, Huiming Wang, Zheng Lin, Mengtong Li, Mengfei Xie, Yinghan Cui, Xuxing Chen, Chao Wang, Weihao Li, Wenqiang Zhu, Jiarong Zhang, Jingxuan Xu, Songwei Yu, Yifan Yao, Xinping Lei, C. Zhang, Han Li, Junqi Xiong, Zuchen Gao, Dailin Li, Haimo Li, Jiaheng Liu, Yuqun Zhang, Junyi Peng, Haotian Zhang, Bin Chen</strong></p>
<p>Recent advances in large language models (LLMs) have enabled progress in agentic coding, where models autonomously reason, plan, and act within interactive software development workflows. However, bridging the gap between static text-based training and dynamic real-world agentic execution remains a core challenge. In this technical report, we present KAT-Coder, a large-scale agentic code model trained through a multi-stage curriculum encompassing Mid-Term Training, Supervised Fine-Tuning (SFT), Reinforcement Fine-Tuning (RFT), and Reinforcement-to-Deployment Adaptation. The Mid-Term stage enhances reasoning, planning, and reflection capabilities through a corpus of real software engineering data and synthetic agentic interactions. The SFT stage constructs a million-sample dataset balancing twenty programming languages, ten development contexts, and ten task archetypes. The RFT stage introduces a novel multi-ground-truth reward formulation for stable and sample-efficient policy optimization. Finally, the Reinforcement-to-Deployment phase adapts the model to production-grade IDE environments using Error-Masked SFT and Tree-Structured Trajectory Training. In summary, these stages enable KAT-Coder to achieve robust tool-use reliability, instruction alignment, and long-context reasoning, forming a deployable foundation for real-world intelligent coding agents. Our KAT series 32B model, KAT-Dev, has been open-sourced on <a target="_blank" rel="noopener" href="https://huggingface.co/Kwaipilot/KAT-Dev">https://huggingface.co/Kwaipilot/KAT-Dev</a>. </p>
<blockquote>
<p>æœ€è¿‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥æ¨åŠ¨äº†æ™ºèƒ½ç¼–ç çš„å‘å±•ï¼Œæ¨¡å‹èƒ½å¤Ÿåœ¨äº¤äº’å¼è½¯ä»¶å¼€å‘å·¥ä½œæµä¸­è‡ªä¸»æ¨ç†ã€è§„åˆ’å’Œè¡ŒåŠ¨ã€‚ç„¶è€Œï¼Œå¼¥åˆåŸºäºé™æ€æ–‡æœ¬çš„è®­ç»ƒå’ŒåŠ¨æ€ç°å®ä¸–ç•Œæ™ºèƒ½æ‰§è¡Œä¹‹é—´çš„å·®è·ä»ç„¶æ˜¯æ ¸å¿ƒæŒ‘æˆ˜ã€‚åœ¨æœ¬æŠ€æœ¯æŠ¥å‘Šä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†KAT-Coderï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡å¤šé˜¶æ®µè¯¾ç¨‹è®­ç»ƒçš„å¤§è§„æ¨¡æ™ºèƒ½ä»£ç æ¨¡å‹ï¼ŒåŒ…æ‹¬ä¸­æœŸè®­ç»ƒã€ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€å¼ºåŒ–å¾®è°ƒï¼ˆRFTï¼‰å’Œä»å¼ºåŒ–åˆ°éƒ¨ç½²çš„é€‚åº”ã€‚ä¸­æœŸè®­ç»ƒé˜¶æ®µé€šè¿‡çœŸå®è½¯ä»¶å·¥ç¨‹æ•°æ®å’Œåˆæˆæ™ºèƒ½äº¤äº’çš„è¯­æ–™åº“å¢å¼ºæ¨ç†ã€è§„åˆ’å’Œåæ€èƒ½åŠ›ã€‚SFTé˜¶æ®µæ„å»ºäº†ä¸€ä¸ªç™¾ä¸‡æ ·æœ¬æ•°æ®é›†ï¼Œå¹³è¡¡äº†äºŒåç§ç¼–ç¨‹è¯­è¨€ã€åç§å¼€å‘ä¸Šä¸‹æ–‡å’Œåç§ä»»åŠ¡åŸå‹ã€‚RFTé˜¶æ®µå¼•å…¥äº†ä¸€ç§æ–°å‹å¤šåœ°é¢çœŸå®å¥–åŠ±å…¬å¼ï¼Œç”¨äºç¨³å®šå’Œé«˜æ•ˆæ ·æœ¬çš„ç­–ç•¥ä¼˜åŒ–ã€‚æœ€åï¼Œä»å¼ºåŒ–åˆ°éƒ¨ç½²çš„é˜¶æ®µä½¿æ¨¡å‹é€‚åº”ç”Ÿäº§çº§çš„IDEç¯å¢ƒï¼Œé‡‡ç”¨é”™è¯¯æ©ç›–SFTå’Œæ ‘ç»“æ„è½¨è¿¹è®­ç»ƒã€‚æ€»ä¹‹ï¼Œè¿™äº›é˜¶æ®µä½¿KAT-Coderå®ç°äº†ç¨³å¥çš„å·¥å…·ä½¿ç”¨å¯é æ€§ã€æŒ‡ä»¤å¯¹é½å’Œé•¿ä¸Šä¸‹æ–‡æ¨ç†ï¼Œå½¢æˆäº†ç°å®ä¸–ç•Œæ™ºèƒ½ç¼–ç ä»£ç†çš„å¯éƒ¨ç½²åŸºç¡€ã€‚æˆ‘ä»¬çš„KATç³»åˆ—32Bæ¨¡å‹KAT-Devå·²åœ¨<a target="_blank" rel="noopener" href="https://huggingface.co/Kwaipilot/KAT-Dev%E4%B8%8A%E5%BC%80%E6%BA%90%E3%80%82">https://huggingface.co/Kwaipilot/KAT-Devä¸Šå¼€æºã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.18779v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä»£ç†ç¼–ç æ–¹é¢å–å¾—äº†è¿›å±•ï¼Œè¿™äº›æ¨¡å‹å¯ä»¥åœ¨äº¤äº’å¼è½¯ä»¶å¼€å‘æµç¨‹ä¸­è‡ªä¸»æ¨ç†ã€è§„åˆ’å’Œè¡ŒåŠ¨ã€‚ç„¶è€Œï¼Œæ¶ˆé™¤é™æ€æ–‡æœ¬è®­ç»ƒä¸åŠ¨æ€ç°å®ä¸–ç•Œä»£ç†æ‰§è¡Œä¹‹é—´çš„é¸¿æ²Ÿä»æ˜¯æ ¸å¿ƒæŒ‘æˆ˜ã€‚æœ¬æŠ€æœ¯æŠ¥å‘Šä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†KAT-Coderï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡ä»£ç†ç¼–ç æ¨¡å‹ï¼Œç»è¿‡ä¸­æœŸè®­ç»ƒã€ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€å¼ºåŒ–å¾®è°ƒï¼ˆRFTï¼‰å’Œå¼ºåŒ–åˆ°éƒ¨ç½²é€‚åº”ç­‰åˆ†é˜¶æ®µè®­ç»ƒã€‚è¿™äº›é˜¶æ®µä½¿å¾—KAT-Coderåœ¨å·¥å…·ä½¿ç”¨å¯é æ€§ã€æŒ‡ä»¤å¯¹é½å’Œé•¿è¯­å¢ƒæ¨ç†æ–¹é¢è¡¨ç°ä¼˜ç§€ï¼Œä¸ºç°å®ä¸–ç•Œçš„æ™ºèƒ½ç¼–ç ä»£ç†æä¾›äº†å¯éƒ¨ç½²çš„åŸºç¡€ã€‚æˆ‘ä»¬çš„KATç³»åˆ—æ¨¡å‹KAT-Devå·²åœ¨Huggingfaceä¸Šä»¥å¼€æºå½¢å¼æä¾›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä»£ç†ç¼–ç æ–¹é¢å–å¾—è¿›å±•ï¼Œèƒ½è‡ªä¸»æ¨ç†ã€è§„åˆ’å’Œè¡ŒåŠ¨äºäº¤äº’å¼è½¯ä»¶å¼€å‘æµç¨‹ã€‚</li>
<li>KAT-Coderæ˜¯ä¸€ä¸ªå¤§è§„æ¨¡ä»£ç†ç¼–ç æ¨¡å‹ï¼Œé€šè¿‡å¤šé˜¶æ®µè®­ç»ƒå®ç°é«˜æ•ˆæ€§èƒ½ã€‚</li>
<li>ä¸­æœŸè®­ç»ƒé˜¶æ®µå¢å¼ºäº†KAT-Coderçš„æ¨ç†ã€è§„åˆ’å’Œåæ€èƒ½åŠ›ã€‚</li>
<li>ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰é˜¶æ®µæ„å»ºäº†å¹³è¡¡å¤šç§ç¼–ç¨‹è¯­è¨€ã€å¼€å‘ç¯å¢ƒå’Œä»»åŠ¡åŸå‹çš„ç™¾ä¸‡æ ·æœ¬æ•°æ®é›†ã€‚</li>
<li>å¼ºåŒ–å¾®è°ƒï¼ˆRFTï¼‰é˜¶æ®µå¼•å…¥äº†å¤šåœ°é¢çœŸå®å¥–åŠ±å…¬å¼ï¼Œç”¨äºç¨³å®šå’Œé«˜æ•ˆçš„ç­–ç•¥ä¼˜åŒ–ã€‚</li>
<li>å¼ºåŒ–åˆ°éƒ¨ç½²é˜¶æ®µä½¿æ¨¡å‹é€‚åº”ç”Ÿäº§çº§IDEç¯å¢ƒï¼Œé€šè¿‡é”™è¯¯æ©è”½SFTå’Œæ ‘ç»“æ„è½¨è¿¹è®­ç»ƒå®ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.18779">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4b4db8e7cea20454e6b2f852797fc35f" align="middle">
<img src="https://picx.zhimg.com/v2-8920429815945ee823947feef75e8a95" align="middle">
<img src="https://picx.zhimg.com/v2-7fa653a9a1e6e6c040d3fc1973220c20" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="SimBench-Benchmarking-the-Ability-of-Large-Language-Models-to-Simulate-Human-Behaviors"><a href="#SimBench-Benchmarking-the-Ability-of-Large-Language-Models-to-Simulate-Human-Behaviors" class="headerlink" title="SimBench: Benchmarking the Ability of Large Language Models to Simulate   Human Behaviors"></a>SimBench: Benchmarking the Ability of Large Language Models to Simulate   Human Behaviors</h2><p><strong>Authors:Tiancheng Hu, Joachim Baumann, Lorenzo Lupo, Nigel Collier, Dirk Hovy, Paul RÃ¶ttger</strong></p>
<p>Large language model (LLM) simulations of human behavior have the potential to revolutionize the social and behavioral sciences, if and only if they faithfully reflect real human behaviors. Current evaluations are fragmented, based on bespoke tasks and metrics, creating a patchwork of incomparable results. To address this, we introduce SimBench, the first large-scale, standardized benchmark for a robust, reproducible science of LLM simulation. By unifying 20 diverse datasets covering tasks from moral decision-making to economic choice across a large global participant pool, SimBench provides the necessary foundation to ask fundamental questions about when, how, and why LLM simulations succeed or fail. We show that, while even the best LLMs today have limited simulation ability (score: 40.80&#x2F;100), performance scales log-linearly with model size. Simulation performance is not improved by increased inference-time compute. We demonstrate an alignment-simulation trade-off: instruction-tuning improves performance on low-entropy (consensus) questions but degrades it on high-entropy (diverse) ones. Models particularly struggle when simulating specific demographic groups. Finally, we demonstrate that simulation ability correlates most strongly with deep, knowledge-intensive reasoning (MMLU-Pro, r&#x3D;0.939). By making progress measurable, we aim to accelerate the development of more faithful LLM simulators. </p>
<blockquote>
<p>å…³äºäººç±»è¡Œä¸ºçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨¡æ‹Ÿï¼Œå¦‚æœèƒ½å¤ŸçœŸå®åœ°åæ˜ äººç±»è¡Œä¸ºï¼Œå°±æœ‰æ½œåŠ›å½»åº•æ”¹å˜ç¤¾ä¼šå’Œè¡Œä¸ºç§‘å­¦ã€‚ç›®å‰çš„è¯„ä¼°æ˜¯åˆ†æ•£çš„ï¼ŒåŸºäºç‰¹å®šä»»åŠ¡å’ŒæŒ‡æ ‡ï¼Œå¯¼è‡´ç»“æœæ— æ³•æ¯”è¾ƒã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†SimBenchï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå¤§è§„æ¨¡æ ‡å‡†åŒ–åŸºå‡†æµ‹è¯•ï¼Œç”¨äºæ„å»ºç¨³å¥ã€å¯å¤åˆ¶çš„å¤§å‹è¯­è¨€æ¨¡å‹æ¨¡æ‹Ÿç§‘å­¦ã€‚SimBenchç»Ÿä¸€äº†æ¶µç›–ä»é“å¾·å†³ç­–åˆ°ç»æµé€‰æ‹©çš„å…¨çƒå¤§å‹å‚ä¸è€…æ± çš„å¤šç§ä»»åŠ¡çš„äºŒåç§ä¸åŒæ•°æ®é›†ï¼Œè¿™ä¸ºæå‡ºå…³äºå¤§å‹è¯­è¨€æ¨¡å‹æ¨¡æ‹Ÿä½•æ—¶æˆåŠŸæˆ–å¤±è´¥ã€å¦‚ä½•æˆåŠŸæˆ–å¤±è´¥ä»¥åŠä¸ºä½•æˆåŠŸæˆ–å¤±è´¥ç­‰åŸºæœ¬é—®é¢˜æä¾›äº†å¿…è¦çš„åŸºç¡€ã€‚æˆ‘ä»¬çš„ç ”ç©¶æ˜¾ç¤ºï¼Œå³ä½¿æ˜¯ç›®å‰æœ€å¥½çš„å¤§å‹è¯­è¨€æ¨¡å‹æ¨¡æ‹Ÿèƒ½åŠ›ä¹Ÿæœ‰é™ï¼ˆå¾—åˆ†ï¼š40.80&#x2F;100ï¼‰ï¼Œæ€§èƒ½ä¼šéšç€æ¨¡å‹è§„æ¨¡çš„å¢é•¿è€Œå‘ˆç°å¯¹æ•°çº¿æ€§å¢é•¿è¶‹åŠ¿ã€‚å¢åŠ æ¨ç†æ—¶é—´çš„è®¡ç®—å¹¶ä¸ä¼šæé«˜æ¨¡æ‹Ÿæ€§èƒ½ã€‚æˆ‘ä»¬è¯æ˜äº†æŒ‡ä»¤è°ƒæ•´ä¸æ¨¡æ‹Ÿä¹‹é—´çš„æƒè¡¡ï¼šæŒ‡ä»¤è°ƒæ•´åœ¨ä½ç†µï¼ˆå…±è¯†ï¼‰é—®é¢˜ä¸Šçš„è¡¨ç°æœ‰æ‰€æé«˜ï¼Œä½†åœ¨é«˜ç†µï¼ˆå¤šæ ·æ€§ï¼‰é—®é¢˜ä¸Šçš„è¡¨ç°å´æœ‰æ‰€ä¸‹é™ã€‚å½“æ¨¡æ‹Ÿç‰¹å®šäººå£ç¾¤ä½“æ—¶ï¼Œæ¨¡å‹ç‰¹åˆ«å›°éš¾ã€‚æœ€åï¼Œæˆ‘ä»¬è¯æ˜æ¨¡æ‹Ÿèƒ½åŠ›ä¸æ·±åº¦çŸ¥è¯†å¯†é›†å‹æ¨ç†ä¹‹é—´å­˜åœ¨å¼ºçƒˆçš„ç›¸å…³æ€§ï¼ˆMMLU-Proç›¸å…³æ€§ç³»æ•°ä¸ºr&#x3D;0.939ï¼‰ã€‚é€šè¿‡ä½¿è¿›æ­¥å¯è¡¡é‡ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯åŠ é€Ÿæ›´çœŸå®çš„å¤§å‹è¯­è¨€æ¨¡å‹æ¨¡æ‹Ÿå™¨çš„å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.17516v3">PDF</a> Project Website: <a target="_blank" rel="noopener" href="http://simbench.tiancheng.hu/">http://simbench.tiancheng.hu/</a> Data:   <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/pitehu/SimBench">https://huggingface.co/datasets/pitehu/SimBench</a></p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯¹äººç±»è¡Œä¸ºçš„æ¨¡æ‹Ÿæœ‰æ½œåŠ›é©æ–°ç¤¾ä¼šå’Œè¡Œä¸ºç§‘å­¦ï¼Œå‰ææ˜¯å…¶å¿…é¡»çœŸå®åœ°åæ˜ äººç±»è¡Œä¸ºã€‚å½“å‰çš„è¯„ä»·æ–¹æ³•åˆ†æ•£ã€ç¼ºä¹ç»Ÿä¸€æ ‡å‡†ï¼Œå¯¼è‡´ç»“æœéš¾ä»¥æ¯”è¾ƒã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡å¼•å…¥SimBenchï¼Œé¦–ä¸ªå¤§è§„æ¨¡ã€æ ‡å‡†åŒ–çš„LLMæ¨¡æ‹Ÿè¯„ä¼°åŸºå‡†ï¼Œä»¥å»ºç«‹ç¨³å¥ã€å¯é‡å¤çš„ç§‘å­¦è¯„ä¼°ä½“ç³»ã€‚SimBenchç»Ÿä¸€äº†20ä¸ªæ¶µç›–é“å¾·å†³ç­–ã€ç»æµé€‰æ‹©ç­‰ä»»åŠ¡çš„å¤šæ ·åŒ–æ•°æ®é›†ï¼Œå¹¶æ¶‰åŠå…¨çƒå¤§é‡å‚ä¸è€…ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œå³ä½¿æ˜¯ç›®å‰æœ€å¥½çš„LLMï¼Œå…¶æ¨¡æ‹Ÿèƒ½åŠ›ä¾ç„¶æœ‰é™ï¼Œæ€§èƒ½éšæ¨¡å‹è§„æ¨¡å¢åŠ è€Œæå‡ï¼Œä½†ä¸æ¨ç†èƒ½åŠ›ç›¸å…³åº¦æœ€é«˜ã€‚é€šè¿‡SimBenchï¼Œæˆ‘ä»¬æ—¨åœ¨åŠ é€Ÿæ›´çœŸå®çš„äººç±»è¡Œä¸ºæ¨¡æ‹Ÿå™¨çš„å¼€å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯¹äººç±»è¡Œä¸ºçš„æ¨¡æ‹Ÿæ½œåŠ›å·¨å¤§ï¼Œå‰ææ˜¯åæ˜ çœŸå®äººç±»è¡Œä¸ºã€‚</li>
<li>å½“å‰LLMæ¨¡æ‹Ÿè¯„ä»·å­˜åœ¨ç¢ç‰‡åŒ–é—®é¢˜ï¼Œç¼ºä¹ç»Ÿä¸€æ ‡å‡†ã€‚</li>
<li>SimBenchä¸ºLLMæ¨¡æ‹Ÿè¯„ä¼°æä¾›äº†å¤§è§„æ¨¡ã€æ ‡å‡†åŒ–çš„åŸºå‡†ã€‚</li>
<li>SimBenchç»Ÿä¸€äº†å¤šæ ·åŒ–æ•°æ®é›†ï¼Œæ¶µç›–å…¨çƒå¤§é‡å‚ä¸è€…ã€‚</li>
<li>ç›®å‰LLMæ¨¡æ‹Ÿèƒ½åŠ›æœ‰é™ï¼Œæ€§èƒ½éšæ¨¡å‹è§„æ¨¡å¢åŠ è€Œæå‡ã€‚</li>
<li>LLMæ¨¡æ‹Ÿæ€§èƒ½ä¸æ¨ç†èƒ½åŠ›ç›¸å…³åº¦æœ€é«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.17516">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-478c45935531f031c636db6579fdf09e" align="middle">
<img src="https://picx.zhimg.com/v2-5d4c1ed777d490906f144590f6ceafc1" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Uniworld-V2-Reinforce-Image-Editing-with-Diffusion-Negative-aware-Finetuning-and-MLLM-Implicit-Feedback"><a href="#Uniworld-V2-Reinforce-Image-Editing-with-Diffusion-Negative-aware-Finetuning-and-MLLM-Implicit-Feedback" class="headerlink" title="Uniworld-V2: Reinforce Image Editing with Diffusion Negative-aware   Finetuning and MLLM Implicit Feedback"></a>Uniworld-V2: Reinforce Image Editing with Diffusion Negative-aware   Finetuning and MLLM Implicit Feedback</h2><p><strong>Authors:Zongjian Li, Zheyuan Liu, Qihui Zhang, Bin Lin, Feize Wu, Shenghai Yuan, Zhiyuan Yan, Yang Ye, Wangbo Yu, Yuwei Niu, Shaodong Wang, Xinhua Cheng, Li Yuan</strong></p>
<p>Instruction-based image editing has achieved remarkable progress; however, models solely trained via supervised fine-tuning often overfit to annotated patterns, hindering their ability to explore and generalize beyond training distributions. To this end, we introduce Edit-R1, a novel post-training framework for instruction-based image editing based on policy optimization. Specifically, we utilize Diffusion Negative-aware Finetuning (DiffusionNFT), a likelihood-free policy optimization method consistent with the flow matching forward process, thereby enabling the use of higher-order samplers and more efficient training. Another key challenge here is the absence of a universal reward model, resulting from the diverse nature of editing instructions and tasks. To bridge this gap, we employ a Multimodal Large Language Model (MLLM) as a unified, training-free reward model, leveraging its output logits to provide fine-grained feedback. Furthermore, we carefully design a low-variance group filtering mechanism to reduce MLLM scoring noise and stabilize optimization. \texttt{UniWorld-V2}, trained with this framework, achieves \textbf{state-of-the-art} results on the ImgEdit and GEdit-Bench benchmarks, scoring 4.49 and 7.83, respectively. Crucially, our framework is model-agnostic, delivering substantial performance gains when applied to diverse base models like Qwen-Image-Edit and FLUX-Kontext, demonstrating its wide applicability. Code and models are publicly available to support further research. </p>
<blockquote>
<p>åŸºäºæŒ‡ä»¤çš„å›¾åƒç¼–è¾‘å·²ç»å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ã€‚ç„¶è€Œï¼Œä»…é€šè¿‡ç›‘ç£å¾®è°ƒè®­ç»ƒçš„æ¨¡å‹å¾€å¾€ä¼šè¿‡åº¦æ‹Ÿåˆæ ‡æ³¨æ¨¡å¼ï¼Œé˜»ç¢äº†å…¶åœ¨è®­ç»ƒåˆ†å¸ƒä¹‹å¤–çš„æ¢ç´¢å’Œæ³›åŒ–èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†Edit-R1ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºç­–ç•¥ä¼˜åŒ–çš„æ–°å‹æŒ‡ä»¤å¼å›¾åƒç¼–è¾‘åè®­ç»ƒæ¡†æ¶ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†æ‰©æ•£è´Ÿæ„ŸçŸ¥å¾®è°ƒï¼ˆDiffusionNFTï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ä¸æ­£å‘è¿‡ç¨‹åŒ¹é…æµçš„æ— ä¼¼ç„¶ç­–ç•¥ä¼˜åŒ–æ–¹æ³•ï¼Œä»è€Œèƒ½å¤Ÿä½¿ç”¨é«˜é˜¶é‡‡æ ·å™¨å¹¶æ›´æœ‰æ•ˆåœ°è¿›è¡Œè®­ç»ƒã€‚å¦ä¸€ä¸ªå…³é”®æŒ‘æˆ˜åœ¨äºç¼ºä¹é€šç”¨çš„å¥–åŠ±æ¨¡å‹ï¼Œè¿™æ˜¯ç”±äºç¼–è¾‘æŒ‡ä»¤å’Œä»»åŠ¡çš„å¤šæ ·æ€§æ‰€å¯¼è‡´çš„ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ä½œä¸ºç»Ÿä¸€çš„ã€æ— éœ€è®­ç»ƒå¥–åŠ±æ¨¡å‹ï¼Œåˆ©ç”¨å…¶è¾“å‡ºå¯¹æ•°å‡ ç‡æä¾›ç²¾ç»†çš„åé¦ˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ç²¾å¿ƒè®¾è®¡äº†ä¸€ç§ä½æ–¹å·®ç»„è¿‡æ»¤æœºåˆ¶ï¼Œä»¥é™ä½MLLMè¯„åˆ†å™ªå£°å¹¶ç¨³å®šä¼˜åŒ–è¿‡ç¨‹ã€‚ä½¿ç”¨æ­¤æ¡†æ¶è®­ç»ƒçš„UniWorld-V2åœ¨ImgEditå’ŒGEdit-BenchåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€æ–°ç»“æœï¼Œå¾—åˆ†åˆ†åˆ«ä¸º4.49å’Œ7.83ã€‚æœ€é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ¡†æ¶å¯¹æ¨¡å‹æ— ç‰¹å®šè¦æ±‚ï¼Œåœ¨åº”ç”¨äºä¸åŒçš„åŸºç¡€æ¨¡å‹ï¼ˆå¦‚Qwen-Image-Editå’ŒFLUX-Kontextï¼‰æ—¶ï¼Œå¯ä»¥å®ç°æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¯æ˜äº†å…¶å¹¿æ³›çš„åº”ç”¨æ€§ã€‚ç›¸å…³ä»£ç å’Œæ¨¡å‹å·²å…¬å¼€å‘å¸ƒï¼Œä»¥æ”¯æŒè¿›ä¸€æ­¥çš„ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.16888v3">PDF</a> </p>
<p><strong>Summary</strong><br>    åŸºäºæŒ‡ä»¤çš„å›¾åƒç¼–è¾‘é¢†åŸŸå·²å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†ä»…é€šè¿‡ç›‘ç£å¾®è°ƒè®­ç»ƒçš„æ¨¡å‹å®¹æ˜“è¿‡åº¦æ‹Ÿåˆæ³¨é‡Šæ¨¡å¼ï¼Œé™åˆ¶äº†å…¶åœ¨è®­ç»ƒåˆ†å¸ƒä¹‹å¤–çš„æ¢ç´¢ä¸æ³›åŒ–èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºEdit-R1ï¼Œä¸€ç§åŸºäºç­–ç•¥ä¼˜åŒ–çš„æŒ‡ä»¤å¼å›¾åƒç¼–è¾‘æ–°å‹åè®­ç»ƒæ¡†æ¶ã€‚æˆ‘ä»¬é‡‡ç”¨æ— éœ€ä¼¼çœŸåº¦çš„æ‰©æ•£è´Ÿæ„ŸçŸ¥å¾®è°ƒæ³•ï¼ˆDiffusionNFTï¼‰ï¼Œè¯¥æ–¹æ³•ä¸æ­£å‘è¿‡ç¨‹ç›¸åŒ¹é…ï¼Œå¯ä½¿ç”¨æ›´é«˜é˜¶çš„é‡‡æ ·å™¨å¹¶æå‡è®­ç»ƒæ•ˆç‡ã€‚ä¸ºè§£å†³ä¸åŒç¼–è¾‘æŒ‡ä»¤ä¸ä»»åŠ¡çš„é€šç”¨å¥–åŠ±æ¨¡å‹ç¼ºå¤±é—®é¢˜ï¼Œæˆ‘ä»¬é‡‡ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ä½œä¸ºç»Ÿä¸€ã€æ— è®­ç»ƒå¥–åŠ±æ¨¡å‹ï¼Œåˆ©ç”¨å…¶è¾“å‡ºå¯¹æ•°å‡ ç‡æä¾›ç²¾ç»†åé¦ˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ç²¾å¿ƒè®¾è®¡äº†ä½æ–¹å·®ç»„è¿‡æ»¤æœºåˆ¶ï¼Œä»¥å‡å°‘MLLMè¯„åˆ†å™ªå£°å¹¶ç¨³å®šä¼˜åŒ–ã€‚ä½¿ç”¨æ­¤æ¡†æ¶è®­ç»ƒçš„UniWorld-V2åœ¨ImgEditå’ŒGEdit-BenchåŸºå‡†æµ‹è¯•ä¸­å–å¾—æœ€æ–°æœ€ä½³æˆç»©ï¼Œåˆ†åˆ«å¾—åˆ†4.49å’Œ7.83ã€‚æ­¤æ¡†æ¶å…·æœ‰æ¨¡å‹æ— å…³æ€§ï¼Œåœ¨å¤šç§åŸºç¡€æ¨¡å‹ä¸Šåº”ç”¨æ—¶ï¼Œå¯å¸¦æ¥æ˜¾è‘—æ€§èƒ½æå‡ï¼Œè¯æ˜å…¶å¹¿æ³›åº”ç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Edit-R1æ˜¯ä¸€ä¸ªåŸºäºç­–ç•¥ä¼˜åŒ–çš„æ–°å‹åè®­ç»ƒæ¡†æ¶ï¼Œç”¨äºæŒ‡ä»¤å¼å›¾åƒç¼–è¾‘ã€‚</li>
<li>å¼•å…¥DiffusionNFTæ–¹æ³•ï¼Œæå‡æ¨¡å‹è®­ç»ƒæ•ˆç‡å’Œæ€§èƒ½ã€‚</li>
<li>åˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ä½œä¸ºé€šç”¨å¥–åŠ±æ¨¡å‹ï¼Œæä¾›ç²¾ç»†åé¦ˆã€‚</li>
<li>è®¾è®¡ä½æ–¹å·®ç»„è¿‡æ»¤æœºåˆ¶ï¼Œå‡å°‘å¥–åŠ±æ¨¡å‹è¯„åˆ†å™ªå£°ã€‚</li>
<li>UniWorld-V2åœ¨ImgEditå’ŒGEdit-BenchåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å“è¶Šã€‚</li>
<li>æ¡†æ¶å…·æœ‰æ¨¡å‹æ— å…³æ€§ï¼Œå¯å¹¿æ³›åº”ç”¨äºå¤šç§åŸºç¡€æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.16888">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dddff4ed233b6ad138de66bb2bbef2c0" align="middle">
<img src="https://picx.zhimg.com/v2-2edf15511314f9b02168a85c40f99475" align="middle">
<img src="https://picx.zhimg.com/v2-f8afaa72b1a3a96446d8be68e5a804b5" align="middle">
<img src="https://picx.zhimg.com/v2-d4064fa37c706aa27f9c418930827398" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Can-generative-AI-figure-out-figurative-language-The-influence-of-idioms-on-essay-scoring-by-ChatGPT-Gemini-and-Deepseek"><a href="#Can-generative-AI-figure-out-figurative-language-The-influence-of-idioms-on-essay-scoring-by-ChatGPT-Gemini-and-Deepseek" class="headerlink" title="Can generative AI figure out figurative language? The influence of   idioms on essay scoring by ChatGPT, Gemini, and Deepseek"></a>Can generative AI figure out figurative language? The influence of   idioms on essay scoring by ChatGPT, Gemini, and Deepseek</h2><p><strong>Authors:Enis OÄŸuz</strong></p>
<p>The developments in Generative AI technologies have paved the way for numerous innovations in different fields. Recently, Generative AI has been proposed as a competitor to AES systems in evaluating student essays automatically. Considering the potential limitations of AI in processing idioms, this study assessed the scoring performances of Generative AI models for essays with and without idioms by incorporating insights from Corpus Linguistics and Computational Linguistics. Two equal essay lists were created from 348 student essays taken from a corpus: one with multiple idioms present in each essay and another with no idioms in essays. Three Generative AI models (ChatGPT, Gemini, and Deepseek) were asked to score all essays in both lists three times, using the same rubric used by human raters in assigning essay scores. The results revealed excellent consistency for all models, but Gemini outperformed its competitors in interrater reliability with human raters. There was also no detectable bias for any demographic group in AI assessment. For essays with multiple idioms, Gemini followed a the most similar pattern to human raters. While the models in the study demonstrated potential for a hybrid approach, Gemini was the best candidate for the task due to its ability to handle figurative language and showed promise for handling essay-scoring tasks alone in the future. </p>
<blockquote>
<p>ç”Ÿæˆå¼äººå·¥æ™ºèƒ½æŠ€æœ¯çš„å‘å±•ä¸ºä¸åŒé¢†åŸŸçš„åˆ›æ–°é“ºå¹³äº†é“è·¯ã€‚æœ€è¿‘ï¼Œç”Ÿæˆå¼äººå·¥æ™ºèƒ½è¢«æè®®ä½œä¸ºè¯„ä¼°å­¦ç”Ÿä½œæ–‡çš„è‡ªåŠ¨è¯„ä¼°ç³»ç»Ÿï¼ˆAESï¼‰çš„ç«äº‰å¯¹æ‰‹ã€‚è€ƒè™‘åˆ°äººå·¥æ™ºèƒ½åœ¨å¤„ç†æˆè¯­æ—¶å¯èƒ½å­˜åœ¨çš„æ½œåœ¨å±€é™æ€§ï¼Œæœ¬ç ”ç©¶ç»“åˆè¯­æ–™åº“è¯­è¨€å­¦å’Œè®¡ç®—è¯­è¨€å­¦çš„è§è§£ï¼Œè¯„ä¼°äº†ç”Ÿæˆå¼äººå·¥æ™ºèƒ½æ¨¡å‹åœ¨æœ‰ã€æ— æˆè¯­ä½œæ–‡ä¸Šçš„è¯„åˆ†è¡¨ç°ã€‚ä»è¯­æ–™åº“ä¸­é€‰å–348ç¯‡å­¦ç”Ÿä½œæ–‡ï¼Œåˆ›å»ºäº†ä¸¤ä¸ªç›¸åŒçš„ä½œæ–‡åˆ—è¡¨ï¼šä¸€ä¸ªåˆ—è¡¨ä¸­æ¯ç¯‡ä½œæ–‡éƒ½æœ‰å¤šä¸ªæˆè¯­ï¼Œå¦ä¸€ä¸ªåˆ—è¡¨åˆ™æ²¡æœ‰æˆè¯­ã€‚è¯·ä¸‰ä¸ªç”Ÿæˆå¼äººå·¥æ™ºèƒ½æ¨¡å‹ï¼ˆChatGPTã€Geminiå’ŒDeepseekï¼‰å¯¹æ‰€æœ‰ä½œæ–‡è¿›è¡Œä¸‰æ¬¡è¯„åˆ†ï¼Œè¯„åˆ†ä½¿ç”¨ä¸äººç±»è¯„åˆ†è€…ç›¸åŒçš„ä½œæ–‡è¯„åˆ†æ ‡å‡†ã€‚ç»“æœè¡¨æ˜ï¼Œæ‰€æœ‰æ¨¡å‹çš„è¯„åˆ†ä¸€è‡´æ€§å¾ˆå¥½ï¼Œä½†Geminiåœ¨ä¸å…¶ä»–æ¨¡å‹ç›¸æ¯”æ—¶è¡¨ç°å‡ºæ›´é«˜çš„è·¨è¯„ä»·è€…å¯é æ€§ã€‚æ­¤å¤–ï¼Œäººå·¥æ™ºèƒ½è¯„ä¼°ä¸­æ²¡æœ‰ä»»ä½•é’ˆå¯¹ä»»ä½•äººå£ç»Ÿè®¡ç¾¤ä½“çš„åè§ã€‚å¯¹äºåŒ…å«å¤šä¸ªæˆè¯­çš„ä½œæ–‡ï¼ŒGeminiçš„è¯„åˆ†æ¨¡å¼ä¸äººç±»è¯„åˆ†è€…æœ€ä¸ºç›¸ä¼¼ã€‚è™½ç„¶ç ”ç©¶ä¸­çš„æ¨¡å‹éƒ½è¡¨ç°å‡ºæ··åˆæ–¹æ³•çš„æ½œåŠ›ï¼Œä½†Geminiç”±äºå…¶å¤„ç†ä¿®è¾è¯­è¨€çš„èƒ½åŠ›è€Œæˆä¸ºè¯¥ä»»åŠ¡çš„æœ€ä½³å€™é€‰æ¨¡å‹ï¼Œå¹¶æœ‰æœ›åœ¨å°†æ¥å•ç‹¬å¤„ç†ä½œæ–‡è¯„åˆ†ä»»åŠ¡æ—¶è¡¨ç°å‡ºæ›´å¤§çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.15009v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºç”Ÿæˆå¼AIæŠ€æœ¯çš„å‘å±•ï¼Œå¯¹äºåŒ…å«ä¹ è¯­å’Œä¸åŒ…å«ä¹ è¯­çš„ä½œæ–‡ï¼Œç ”ç©¶äººå‘˜è¯„ä¼°äº†ç”Ÿæˆå¼AIæ¨¡å‹åœ¨è¯„åˆ†æ–¹é¢çš„è¡¨ç°ã€‚é€šè¿‡å¯¹é€‰å®šçš„ä¸‰ä¸ªç”Ÿæˆå¼AIæ¨¡å‹ï¼ˆChatGPTã€åŒå­åº§å’ŒDeepseekï¼‰è¿›è¡Œä¸‰æ¬¡è¯„åˆ†æµ‹è¯•ï¼Œå‘ç°å®ƒä»¬åœ¨è¯„åˆ†ä¸Šè¡¨ç°å‡ºé«˜åº¦ä¸€è‡´æ€§ï¼Œå…¶ä¸­åŒå­åº§æ¨¡å‹ä¸äººå·¥è¯„åˆ†è€…çš„è¯„åˆ†æ¨¡å¼æœ€ä¸ºæ¥è¿‘ï¼Œä¸”åœ¨å¤„ç†ä¹ è¯­ä½œæ–‡æ–¹é¢è¡¨ç°æœ€ä½³ã€‚è¿™è¡¨æ˜åŒå­åº§æ¨¡å‹åœ¨å¤„ç†æ¯”å–»æ€§è¯­è¨€æ–¹é¢å…·æœ‰è¾ƒå¼ºçš„èƒ½åŠ›ï¼Œå¹¶æœ‰æœ›åœ¨æœªæ¥ç‹¬ç«‹å¤„ç†ä½œæ–‡è¯„åˆ†ä»»åŠ¡ã€‚æ€»ä½“è€Œè¨€ï¼Œè¿™äº›æ¨¡å‹å±•ç°äº†æ··åˆè¯„ä¼°çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆå¼AIæŠ€æœ¯ä¸ºå¤šä¸ªé¢†åŸŸå¸¦æ¥äº†åˆ›æ–°ã€‚</li>
<li>ç”Ÿæˆå¼AIè¢«æè®®ä½œä¸ºAESç³»ç»Ÿåœ¨è‡ªåŠ¨è¯„ä¼°å­¦ç”Ÿä½œæ–‡æ–¹é¢çš„ç«äº‰å¯¹æ‰‹ã€‚</li>
<li>AIåœ¨å¤„ç†ä¹ è¯­æ–¹é¢å¯èƒ½å­˜åœ¨æ½œåœ¨å±€é™æ€§ã€‚</li>
<li>é€šè¿‡è¯­æ–™åº“è¯­è¨€å­¦å’Œè®¡ç®—è¯­è¨€å­¦ï¼Œè¯„ä¼°äº†åŒ…å«ä¹ è¯­å’Œä¸åŒ…å«ä¹ è¯­çš„ä½œæ–‡çš„ç”Ÿæˆå¼AIæ¨¡å‹è¯„åˆ†è¡¨ç°ã€‚</li>
<li>ä¸‰ä¸ªç”Ÿæˆå¼AIæ¨¡å‹åœ¨è¯„åˆ†ä¸Šè¡¨ç°å‡ºé«˜åº¦ä¸€è‡´æ€§ã€‚</li>
<li>åŒå­åº§æ¨¡å‹åœ¨å¤„ç†æ¯”å–»æ€§è¯­è¨€å’Œä¸äººå·¥è¯„åˆ†è€…æ¨¡å¼åŒ¹é…æ–¹é¢è¡¨ç°æœ€ä½³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.15009">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5430fa31543fde1aa368241c98d397d9" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="FlexAC-Towards-Flexible-Control-of-Associative-Reasoning-in-Multimodal-Large-Language-Models"><a href="#FlexAC-Towards-Flexible-Control-of-Associative-Reasoning-in-Multimodal-Large-Language-Models" class="headerlink" title="FlexAC: Towards Flexible Control of Associative Reasoning in Multimodal   Large Language Models"></a>FlexAC: Towards Flexible Control of Associative Reasoning in Multimodal   Large Language Models</h2><p><strong>Authors:Shengming Yuan, Xinyu Lyu, Shuailong Wang, Beitao Chen, Jingkuan Song, Lianli Gao</strong></p>
<p>Multimodal large language models (MLLMs) face an inherent trade-off between faithfulness and creativity, as different tasks require varying degrees of associative reasoning. However, existing methods lack the flexibility to modulate this reasoning strength, limiting MLLMsâ€™ adaptability across factual and creative scenarios. To bridge this gap, we propose equipping MLLMs with mechanisms that enable flexible control over associative reasoning. We begin by investigating the internal mechanisms underlying associative behavior in MLLMs and find that: (1) middle layers play a pivotal role in shaping modelâ€™s associative tendencies, (2) modifying representations in these layers effectively regulates associative reasoning strength, and (3) hallucinations can be exploited to derive steering vectors that guide this modulation. Building on these findings, we introduce Flexible Association Control (FlexAC), a lightweight and training-free framework for modulating associative behavior in MLLMs. FlexAC first induces hallucination-guided intermediate representations to encode associative directions. Then, it selects high-association instances to construct effective associative steering vectors, whose strengths are adaptively calibrated to balance creative guidance with output stability. Finally, recognizing the multi-dimensional nature of associative reasoning, FlexAC incorporates task-specific associative vectors derived from a forward pass on a few target-domain samples, enabling models to follow diverse associative directions and better adapt to creative tasks. Notably, our method achieves up to a 5.8x improvement in creativity on Creation-MMBench and a 29% reduction in hallucination rate on CHAIR, surpassing existing baselines and demonstrating its effectiveness in enabling flexible control over associative reasoning in MLLMs. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/ylhz/FlexAC">https://github.com/ylhz/FlexAC</a>. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰é¢ä¸´ç€å¿ è¯šåº¦å’Œåˆ›é€ åŠ›ä¹‹é—´çš„å›ºæœ‰æƒè¡¡ï¼Œå› ä¸ºä¸åŒçš„ä»»åŠ¡éœ€è¦ä¸åŒç¨‹åº¦çš„è”æƒ³æ¨ç†ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ç¼ºä¹è°ƒæ•´è¿™ç§æ¨ç†å¼ºåº¦çš„çµæ´»æ€§ï¼Œé™åˆ¶äº†MLLMsåœ¨äº‹å®å’Œåˆ›æ„åœºæ™¯ä¸­çš„é€‚åº”æ€§ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºä¸ºMLLMsé…å¤‡æœºåˆ¶ï¼Œä»¥å®ç°å¯¹è”æƒ³æ¨ç†çš„çµæ´»æ§åˆ¶ã€‚æˆ‘ä»¬é€šè¿‡è°ƒæŸ¥MLLMsä¸­è”æƒ³è¡Œä¸ºçš„å†…åœ¨æœºåˆ¶æ¥å¼€å§‹ç ”ç©¶ï¼Œå¹¶å‘ç°ï¼šï¼ˆ1ï¼‰ä¸­å±‚åœ¨å¡‘é€ æ¨¡å‹çš„è”æƒ³å€¾å‘æ–¹é¢èµ·ç€å…³é”®ä½œç”¨ï¼›ï¼ˆ2ï¼‰ä¿®æ”¹è¿™äº›å±‚çš„è¡¨ç¤ºå¯ä»¥æœ‰æ•ˆåœ°è°ƒèŠ‚è”æƒ³æ¨ç†çš„å¼ºåº¦ï¼›ï¼ˆ3ï¼‰å¯ä»¥åˆ©ç”¨å¹»è§‰æ¥æ´¾ç”Ÿå‡ºå¼•å¯¼å‘é‡ï¼Œå¼•å¯¼è¿™ç§è°ƒåˆ¶ã€‚åŸºäºè¿™äº›å‘ç°ï¼Œæˆ‘ä»¬å¼•å…¥äº†Flexible Association Controlï¼ˆFlexACï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè°ƒèŠ‚MLLMsä¸­è”æƒ³è¡Œä¸ºçš„è½»é‡çº§ã€æ— éœ€è®­ç»ƒæ¡†æ¶ã€‚FlexACé¦–å…ˆé€šè¿‡å¹»è§‰å¼•å¯¼çš„ä¸­é—´è¡¨ç¤ºæ¥ç¼–ç è”æƒ³æ–¹å‘ã€‚ç„¶åï¼Œå®ƒé€‰æ‹©é«˜åº¦å…³è”çš„å®ä¾‹æ¥æ„å»ºæœ‰æ•ˆçš„è”æƒ³å¼•å¯¼å‘é‡ï¼Œå…¶å¼ºåº¦è‡ªé€‚åº”åœ°æ ¡å‡†ä»¥å¹³è¡¡åˆ›é€ æ€§æŒ‡å¯¼å’Œè¾“å‡ºç¨³å®šæ€§ã€‚æœ€åï¼Œè®¤è¯†åˆ°è”æƒ³æ¨ç†çš„å¤šç»´æ€§è´¨ï¼ŒFlexACç»“åˆäº†æ¥è‡ªç›®æ ‡åŸŸæ ·æœ¬å‰å‘ä¼ é€’çš„ä»»åŠ¡ç‰¹å®šè”æƒ³å‘é‡ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿéµå¾ªä¸åŒçš„è”æƒ³æ–¹å‘å¹¶æ›´å¥½åœ°é€‚åº”åˆ›é€ æ€§ä»»åŠ¡ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨Creation-MMBenchçš„åˆ›é€ åŠ›æ–¹é¢å®ç°äº†é«˜è¾¾5.8å€çš„æ”¹è¿›ï¼Œåœ¨CHAIRä¸Šçš„å¹»è§‰ç‡é™ä½äº†29%ï¼Œè¶…è¶Šäº†ç°æœ‰åŸºå‡†ï¼Œè¯æ˜äº†å…¶åœ¨MLLMsä¸­å®ç°å¯¹è”æƒ³æ¨ç†çš„çµæ´»æ§åˆ¶çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/ylhz/FlexAC%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/ylhz/FlexACä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.11190v3">PDF</a> 19 pages, 11 figures. Accepted by the 39th Conference on Neural   Information Processing Systems (NeurIPS 2025)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¿ å®æ€§å’Œåˆ›é€ åŠ›ä¹‹é—´çš„æƒè¡¡é—®é¢˜ã€‚é’ˆå¯¹ä»»åŠ¡éœ€æ±‚çš„å…³è”æ€§æ¨ç†ç¨‹åº¦ä¸åŒï¼ŒMLLMsé¢ä¸´è¿™ä¸€å›ºæœ‰çŸ›ç›¾ã€‚ç°æœ‰æ–¹æ³•ç¼ºä¹è°ƒæ•´æ¨ç†å¼ºåº¦çš„çµæ´»æ€§ï¼Œé™åˆ¶äº†MLLMsåœ¨äº‹å®å’Œåˆ›æ„åœºæ™¯ä¸­çš„é€‚åº”æ€§ã€‚ä¸ºå¼¥è¡¥è¿™ä¸€ä¸è¶³ï¼Œæœ¬æ–‡æå‡ºäº†ä¸ºMLLMsé…å¤‡æœºåˆ¶ï¼Œä»¥å®ç°å¯¹å…³è”æ€§æ¨ç†çš„çµæ´»æ§åˆ¶ã€‚é€šè¿‡æ¢ç©¶MLLMsä¸­å…³è”æ€§è¡Œä¸ºçš„å†…åœ¨æœºåˆ¶ï¼Œå‘ç°ä¸­é—´å±‚åœ¨å¡‘é€ æ¨¡å‹å…³è”æ€§å€¾å‘æ–¹é¢èµ·å…³é”®ä½œç”¨ï¼Œä¿®æ”¹è¿™äº›å±‚çš„è¡¨ç¤ºå¯ä»¥æœ‰æ•ˆåœ°è°ƒèŠ‚å…³è”æ€§æ¨ç†å¼ºåº¦ï¼Œè€Œå¹»è§‰å¯ä»¥è¢«ç”¨æ¥æ¨å¯¼å¼•å¯¼è¿™ç§è°ƒèŠ‚çš„å¼•å¯¼å‘é‡ã€‚åŸºäºè¿™äº›å‘ç°ï¼Œæœ¬æ–‡å¼•å…¥äº†Flexible Association Controlï¼ˆFlexACï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªè½»é‡çº§ã€æ— éœ€è®­ç»ƒå³å¯è°ƒæ•´MLLMsä¸­å…³è”è¡Œä¸ºçš„æ¡†æ¶ã€‚FlexACé¦–å…ˆé€šè¿‡å¹»è§‰å¼•å¯¼çš„ä¸­é—´è¡¨ç¤ºæ¥ç¼–ç å…³è”æ–¹å‘ï¼Œç„¶åé€‰æ‹©é«˜å…³è”å®ä¾‹æ¥æ„å»ºæœ‰æ•ˆçš„å…³è”å¼•å¯¼å‘é‡ï¼Œå…¶å¼ºåº¦å¯è‡ªé€‚åº”åœ°å¹³è¡¡åˆ›æ„æŒ‡å¯¼å’Œè¾“å‡ºç¨³å®šæ€§ã€‚æœ€åï¼ŒFlexACè®¤è¯†åˆ°å…³è”æ¨ç†çš„å¤šç»´æ€§è´¨ï¼Œå¹¶ç»“åˆç›®æ ‡åŸŸæ ·æœ¬çš„å‰å‘ä¼ é€’æ¥ç”Ÿæˆä»»åŠ¡ç‰¹å®šå…³è”å‘é‡ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿé€‚åº”ä¸åŒçš„å…³è”æ–¹å‘å¹¶æ›´å¥½åœ°é€‚åº”åˆ›é€ æ€§ä»»åŠ¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åˆ›é€ åŠ›æå‡æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆæ•ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¿ å®æ€§å’Œåˆ›é€ åŠ›ä¹‹é—´å­˜åœ¨æƒè¡¡ã€‚</li>
<li>MLLMsåœ¨å…³è”æ€§æ¨ç†æ–¹é¢ç¼ºä¹çµæ´»æ€§ï¼Œé™åˆ¶äº†å…¶åœ¨ä¸åŒåœºæ™¯ä¸­çš„é€‚åº”æ€§ã€‚</li>
<li>ä¸­é—´å±‚åœ¨MLLMsçš„å…³è”æ€§è¡Œä¸ºä¸­èµ·å…³é”®ä½œç”¨ã€‚</li>
<li>ä¿®æ”¹ä¸­é—´å±‚çš„è¡¨ç¤ºå¯ä»¥æœ‰æ•ˆåœ°è°ƒèŠ‚å…³è”æ€§æ¨ç†å¼ºåº¦ã€‚</li>
<li>å¹»è§‰å¯ä»¥è¢«ç”¨æ¥æ¨å¯¼å¼•å¯¼å‘é‡ï¼Œä»¥æ§åˆ¶MLLMsçš„å…³è”æ€§è¡Œä¸ºã€‚</li>
<li>FlexACæ¡†æ¶æ˜¯ä¸€ç§è½»é‡çº§ã€æ— éœ€è®­ç»ƒçš„æ–¹æ³•ï¼Œå¯å®ç°MLLMsä¸­å…³è”è¡Œä¸ºçš„çµæ´»æ§åˆ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.11190">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-169d63785192baad672571e5927a1cfe" align="middle">
<img src="https://picx.zhimg.com/v2-7e1d5d2d946bfc5917879c9e49268976" align="middle">
<img src="https://picx.zhimg.com/v2-0b7b6828339b5fe51682de8d7435b936" align="middle">
<img src="https://picx.zhimg.com/v2-def40d6383d2d0109991595a971521f6" align="middle">
<img src="https://picx.zhimg.com/v2-6a381aec7c7db8c50dbbdedefc7f6533" align="middle">
<img src="https://picx.zhimg.com/v2-5d4f3d42b48a0cba13dbb36b9282a023" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="FlyLoRA-Boosting-Task-Decoupling-and-Parameter-Efficiency-via-Implicit-Rank-Wise-Mixture-of-Experts"><a href="#FlyLoRA-Boosting-Task-Decoupling-and-Parameter-Efficiency-via-Implicit-Rank-Wise-Mixture-of-Experts" class="headerlink" title="FlyLoRA: Boosting Task Decoupling and Parameter Efficiency via Implicit   Rank-Wise Mixture-of-Experts"></a>FlyLoRA: Boosting Task Decoupling and Parameter Efficiency via Implicit   Rank-Wise Mixture-of-Experts</h2><p><strong>Authors:Heming Zou, Yunliang Zang, Wutong Xu, Yao Zhu, Xiangyang Ji</strong></p>
<p>Low-Rank Adaptation (LoRA) is a widely used parameter-efficient fine-tuning method for foundation models, but it suffers from parameter interference, resulting in suboptimal performance. Although Mixture-of-Experts (MoE)-based LoRA variants show promise in mitigating intra-task correlations in single-task instruction tuning, they introduce additional router parameters and remain ineffective in multi-task model merging where inter-task interference arises. Inspired by the fly olfactory circuit, we propose FlyLoRA, an implicit MoE-based LoRA variant that introduces: (1) rank-wise expert activation in the up-projection matrix, and (2) an implicit router that unifies expert routing and down-projection, where a frozen sparse random projection matrix replaces the traditional dense trainable version. This design resolves the trade-off between intra-task decorrelation and computational efficiency by eliminating the need for an explicit router, while inherently mitigating inter-task interference due to the orthogonality property of random matrices. Extensive experiments across four domains â€“ general knowledge understanding, scientific question answering, mathematical reasoning, and code generation â€“ demonstrate consistent performance improvements over existing methods. Beyond empirical gains, FlyLoRA highlights how biological structures can inspire innovations in AI technologies. Code is available at <a target="_blank" rel="noopener" href="https://github.com/gfyddha/FlyLoRA">https://github.com/gfyddha/FlyLoRA</a>. </p>
<blockquote>
<p>Low-Rank Adaptationï¼ˆLoRAï¼‰æ˜¯ä¸€ç§å¹¿æ³›åº”ç”¨äºåŸºç¡€æ¨¡å‹çš„å‚æ•°æ•ˆç‡é«˜çš„å¾®è°ƒæ–¹æ³•ï¼Œä½†å®ƒå­˜åœ¨å‚æ•°å¹²æ‰°çš„é—®é¢˜ï¼Œå¯¼è‡´æ€§èƒ½ä¸ä½³ã€‚è™½ç„¶åŸºäºä¸“å®¶æ··åˆï¼ˆMoEï¼‰çš„LoRAå˜ä½“åœ¨å•ä»»åŠ¡æŒ‡ä»¤è°ƒæ•´ä¸­ç¼“è§£ä»»åŠ¡å†…ç›¸å…³æ€§æ–¹é¢æ˜¾ç¤ºå‡ºå¸Œæœ›ï¼Œä½†å®ƒä»¬å¼•å…¥äº†é¢å¤–çš„è·¯ç”±å™¨å‚æ•°ï¼Œå¹¶ä¸”åœ¨å¤šä»»åŠ¡æ¨¡å‹åˆå¹¶ä¸­ä»ç„¶æ— æ•ˆï¼Œè¿™é‡Œä¼šå‡ºç°ä»»åŠ¡é—´å¹²æ‰°ã€‚å—è‹è‡å—…è§‰ç”µè·¯çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†FlyLoRAï¼Œè¿™æ˜¯ä¸€ç§åŸºäºéšå¼MoEçš„LoRAå˜ä½“ï¼Œå®ƒå¼•å…¥äº†ï¼šï¼ˆ1ï¼‰åœ¨æŠ•å½±çŸ©é˜µä¸­è¿›è¡Œç­‰çº§ä¸“å®¶æ¿€æ´»ï¼›ï¼ˆ2ï¼‰ä¸€ç§éšå¼è·¯ç”±å™¨ï¼Œç»Ÿä¸€ä¸“å®¶è·¯ç”±å’Œé™ç»´æŠ•å½±ï¼Œå…¶ä¸­å†»ç»“çš„ç¨€ç–éšæœºæŠ•å½±çŸ©é˜µå–ä»£äº†ä¼ ç»Ÿçš„å¯†é›†å¯è®­ç»ƒç‰ˆæœ¬ã€‚è¿™ç§è®¾è®¡é€šè¿‡ä¸éœ€è¦æ˜¾å¼è·¯ç”±å™¨è§£å†³äº†ä»»åŠ¡å†…è§£ç›¸å…³å’Œè®¡ç®—æ•ˆç‡ä¹‹é—´çš„æƒè¡¡ï¼ŒåŒæ—¶ç”±äºå…¶éšæœºçŸ©é˜µçš„æ­£äº¤æ€§å±æ€§ï¼Œå›ºæœ‰åœ°å‡è½»äº†ä»»åŠ¡é—´çš„å¹²æ‰°ã€‚åœ¨å››ä¸ªé¢†åŸŸâ€”â€”é€šç”¨çŸ¥è¯†ç†è§£ã€ç§‘å­¦é—®ç­”ã€æ•°å­¦æ¨ç†å’Œä»£ç ç”Ÿæˆâ€”â€”çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œå®ƒå®ç°äº†æ€§èƒ½ä¸Šçš„æŒç»­æ”¹è¿›ã€‚é™¤äº†ç»éªŒæ”¶ç›Šä¹‹å¤–ï¼ŒFlyLoRAè¿˜å¼ºè°ƒäº†ç”Ÿç‰©ç»“æ„å¦‚ä½•æ¿€å‘äººå·¥æ™ºèƒ½æŠ€æœ¯çš„åˆ›æ–°ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/gfyddha/FlyLoRA%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/gfyddha/FlyLoRAæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.08396v2">PDF</a> NeurIPS 2025 accepted paper</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºä»¿ç”Ÿè®¾è®¡çš„æ–°å‹å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•FlyLoRAï¼Œè§£å†³äº†ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰ä¸­çš„å‚æ•°å¹²æ‰°é—®é¢˜ã€‚é€šè¿‡å¼•å…¥rank-wiseä¸“å®¶æ¿€æ´»å’Œéšæ€§è·¯ç”±å™¨ï¼ŒFlyLoRAèƒ½å¤Ÿåœ¨å¤šä»»åŠ¡æ¨¡å‹åˆå¹¶ä¸­æœ‰æ•ˆç¼“è§£ä»»åŠ¡é—´å¹²æ‰°ï¼Œæé«˜äº†æ¨¡å‹æ€§èƒ½ã€‚å®éªŒè¯æ˜ï¼ŒFlyLoRAåœ¨å¤šä¸ªé¢†åŸŸå‡å®ç°äº†å¯¹ç°æœ‰äººå·¥æ™ºèƒ½æŠ€æœ¯çš„æ€§èƒ½æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>FlyLoRAæ˜¯ä¸€ç§åŸºäºéšå¼MoEï¼ˆMixture-of-Expertsï¼‰çš„LoRAå˜ä½“ï¼Œè§£å†³äº†å‚æ•°å¹²æ‰°é—®é¢˜ã€‚</li>
<li>å¼•å…¥rank-wiseä¸“å®¶æ¿€æ´»å’Œéšæ€§è·¯ç”±å™¨ï¼Œæé«˜äº†æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>é€šè¿‡æ¶ˆé™¤æ˜¾å¼è·¯ç”±å™¨éœ€æ±‚ï¼ŒFlyLoRAå®ç°äº†ä»»åŠ¡å†…å»ç›¸å…³æ€§å’Œè®¡ç®—æ•ˆç‡ä¹‹é—´çš„å¹³è¡¡ã€‚</li>
<li>éšæœºçŸ©é˜µçš„æ­£äº¤å±æ€§å›ºæœ‰åœ°å‡è½»äº†ä»»åŠ¡é—´å¹²æ‰°ã€‚</li>
<li>FlyLoRAåœ¨å¤šä¸ªé¢†åŸŸå®ç°äº†å¯¹ç°æœ‰äººå·¥æ™ºèƒ½æŠ€æœ¯çš„æ€§èƒ½æå‡ã€‚</li>
<li>è¯¥æ–¹æ³•ä»ç”Ÿç‰©å­¦ç»“æ„ä¸­è·å¾—çµæ„Ÿï¼Œå±•ç°äº†ç”Ÿç‰©å­¦å¯¹äººå·¥æ™ºèƒ½æŠ€æœ¯çš„å¯å‘æ½œåŠ›ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.08396">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e5b0dcec8915d196c4df323e036ee6e5" align="middle">
<img src="https://picx.zhimg.com/v2-2275cc2dd6f71f3d9bd7b386e5f869ff" align="middle">
<img src="https://picx.zhimg.com/v2-35b0df053417bdac845dd0485704f2ed" align="middle">
<img src="https://picx.zhimg.com/v2-67d3d13e77349b9b9aea02d983321c3b" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Training-Large-Language-Models-To-Reason-In-Parallel-With-Global-Forking-Tokens"><a href="#Training-Large-Language-Models-To-Reason-In-Parallel-With-Global-Forking-Tokens" class="headerlink" title="Training Large Language Models To Reason In Parallel With Global Forking   Tokens"></a>Training Large Language Models To Reason In Parallel With Global Forking   Tokens</h2><p><strong>Authors:Sheng Jia, Xiao Wang, Shiva Prasad Kasiviswanathan</strong></p>
<p>Although LLMs have demonstrated improved performance by scaling parallel test-time compute, doing so relies on generating reasoning paths that are both diverse and accurate. For challenging problems, the forking tokens that trigger diverse yet correct reasoning modes are typically deep in the sampling tree. Consequently, common strategies to encourage diversity, such as temperature scaling, encounter a worsened trade-off between diversity and accuracy. Motivated by this challenge, we treat parallel reasoning as a set-of-next-token-prediction problem, and incorporate a set-based global loss into Supervised Fine-Tuning (SFT) using self-supervised bipartite matching between our global forking tokens and unique reasoning traces. We observe that, while naive fine-tuning with multiple reasoning traces collapses these unique reasoning modes, our proposed method, Set Supervised Fine-Tuning (SSFT), preserves these modes and produces emergent global forking tokens. Experiments on multiple reasoning benchmarks show that our SSFT consistently outperforms SFT under both Pass@1 and Cons@k metrics. </p>
<blockquote>
<p>å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€šè¿‡æ‰©å±•å¹¶è¡Œæµ‹è¯•æ—¶é—´è®¡ç®—å±•ç¤ºäº†æ”¹è¿›çš„æ€§èƒ½ï¼Œä½†è¿™ç§åšæ³•ä¾èµ–äºç”Ÿæˆå¤šæ ·ä¸”å‡†ç¡®çš„æ¨ç†è·¯å¾„ã€‚å¯¹äºå…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ï¼Œè§¦å‘å¤šæ ·ä½†æ­£ç¡®çš„æ¨ç†æ¨¡å¼çš„åˆ†å‰ä»¤ç‰Œé€šå¸¸ä½äºé‡‡æ ·æ ‘çš„æ·±å¤„ã€‚å› æ­¤ï¼Œé¼“åŠ±å¤šæ ·æ€§çš„å¸¸è§ç­–ç•¥ï¼ˆå¦‚æ¸©åº¦ç¼©æ”¾ï¼‰åœ¨å¤šæ ·æ€§å’Œå‡†ç¡®æ€§ä¹‹é—´é¢ä¸´æƒè¡¡å›°éš¾çš„å›°å¢ƒã€‚å—æ­¤æŒ‘æˆ˜æ¿€åŠ±ï¼Œæˆ‘ä»¬å°†å¹¶è¡Œæ¨ç†è§†ä¸ºä¸‹ä¸€ä¸ªä»¤ç‰Œé¢„æµ‹çš„é—®é¢˜é›†åˆï¼Œå¹¶é€šè¿‡æˆ‘ä»¬å…¨å±€åˆ†å‰ä»¤ç‰Œå’Œè‡ªæˆ‘ç›‘ç£äºŒåˆ†åŒ¹é…ä¹‹é—´çš„åŸºäºé›†åˆçš„å…¨å±€æŸå¤±ï¼Œå°†å…¶çº³å…¥ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œè™½ç„¶ç”¨å¤šä¸ªæ¨ç†è½¨è¿¹è¿›è¡Œç®€å•å¾®è°ƒä¼šç ´åè¿™äº›ç‹¬ç‰¹çš„æ¨ç†æ¨¡å¼ï¼Œä½†æˆ‘ä»¬æå‡ºçš„æ–¹æ³•â€”â€”é›†åˆç›‘ç£å¾®è°ƒï¼ˆSSFTï¼‰å¯ä»¥ä¿ç•™è¿™äº›æ¨¡å¼å¹¶äº§ç”Ÿæ–°å…´çš„å…¨å±€åˆ†å‰ä»¤ç‰Œã€‚åœ¨å¤šä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„SSFTåœ¨Pass@1å’ŒCons@kæŒ‡æ ‡ä¸‹å§‹ç»ˆä¼˜äºSFTã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.05132v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€šè¿‡å¢åŠ å¹¶è¡Œæµ‹è¯•æ—¶çš„è®¡ç®—è§„æ¨¡æ¥æé«˜æ€§èƒ½ï¼Œè¿™éœ€è¦ç”Ÿæˆå¤šæ ·ä¸”å‡†ç¡®çš„æ¨ç†è·¯å¾„ã€‚å¯¹äºéš¾é¢˜ï¼Œè§¦å‘å¤šæ ·ä½†æ­£ç¡®çš„æ¨ç†æ¨¡å¼çš„æ ‡è®°é€šå¸¸ä½äºé‡‡æ ·æ ‘çš„æ·±å¤„ã€‚å› æ­¤ï¼Œé¼“åŠ±å¤šæ ·æ€§çš„å¸¸è§„ç­–ç•¥ï¼ˆå¦‚æ¸©åº¦ç¼©æ”¾ï¼‰åœ¨å¤šæ ·æ€§å’Œå‡†ç¡®æ€§ä¹‹é—´é­é‡äº†æƒè¡¡é—®é¢˜ã€‚æœ¬ç ”ç©¶å°†å¹¶è¡Œæ¨ç†è§†ä¸ºä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹é—®é¢˜ï¼Œå¹¶å°†åŸºäºé›†åˆçš„å…¨å±€æŸå¤±çº³å…¥ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ä¸­ï¼Œé€šè¿‡å…¨å±€æ ‡è®°å’Œç‹¬ç‰¹æ¨ç†è½¨è¿¹ä¹‹é—´çš„è‡ªæˆ‘ç›‘ç£äºŒåˆ†åŒ¹é…æ¥å®ç°ã€‚å®éªŒè¡¨æ˜ï¼Œä¸ä»…ä½¿ç”¨å•ä¸€æ¨ç†è½¨è¿¹çš„å¾®è°ƒæ–¹æ³•ç›¸æ¯”ï¼Œæœ¬ç ”ç©¶æ‰€æå‡ºçš„é›†åˆç›‘ç£å¾®è°ƒï¼ˆSSFTï¼‰èƒ½å¤Ÿä¿ç•™è¿™äº›ç‹¬ç‰¹çš„æ¨ç†æ¨¡å¼å¹¶äº§ç”Ÿæ–°å…´çš„å…¨å±€æ ‡è®°ã€‚åœ¨å¤šä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸Šï¼ŒSSFTåœ¨Pass@1å’ŒCons@kæŒ‡æ ‡ä¸Šçš„è¡¨ç°å‡ä¼˜äºSFTã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æå‡æ€§èƒ½ä¾èµ–äºç”Ÿæˆå¤šæ ·ä¸”å‡†ç¡®çš„æ¨ç†è·¯å¾„ã€‚</li>
<li>è§¦å‘å¤šæ ·æ­£ç¡®æ¨ç†æ¨¡å¼çš„æ ‡è®°ä½äºé‡‡æ ·æ ‘çš„æ·±å¤„ã€‚</li>
<li>å¸¸è§„é¼“åŠ±å¤šæ ·æ€§çš„ç­–ç•¥åœ¨å¹³è¡¡å¤šæ ·æ€§å’Œå‡†ç¡®æ€§æ—¶å­˜åœ¨é—®é¢˜ã€‚</li>
<li>æœ¬ç ”ç©¶å°†å¹¶è¡Œæ¨ç†è§†ä¸ºä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹é—®é¢˜ï¼Œå¹¶çº³å…¥ç›‘ç£å¾®è°ƒä¸­ã€‚</li>
<li>é€šè¿‡å…¨å±€æ ‡è®°å’Œç‹¬ç‰¹æ¨ç†è½¨è¿¹çš„è‡ªæˆ‘ç›‘ç£äºŒåˆ†åŒ¹é…æ¥å®ç°æ–¹æ³•ã€‚</li>
<li>é›†åˆç›‘ç£å¾®è°ƒï¼ˆSSFTï¼‰èƒ½å¤Ÿä¿ç•™ç‹¬ç‰¹çš„æ¨ç†æ¨¡å¼å¹¶äº§ç”Ÿæ–°å…´çš„å…¨å±€æ ‡è®°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.05132">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-389e097b8d1ad4a2580eae5de932f0af" align="middle">
<img src="https://picx.zhimg.com/v2-b5a1ddaff105ff90bf56b4cf8c425fc7" align="middle">
<img src="https://picx.zhimg.com/v2-68c5b59105df848c23203fdace88d41c" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="ChessArena-A-Chess-Testbed-for-Evaluating-Strategic-Reasoning-Capabilities-of-Large-Language-Models"><a href="#ChessArena-A-Chess-Testbed-for-Evaluating-Strategic-Reasoning-Capabilities-of-Large-Language-Models" class="headerlink" title="ChessArena: A Chess Testbed for Evaluating Strategic Reasoning   Capabilities of Large Language Models"></a>ChessArena: A Chess Testbed for Evaluating Strategic Reasoning   Capabilities of Large Language Models</h2><p><strong>Authors:Jincheng Liu, Sijun He, Jingjing Wu, Xiangsen Wang, Yang Chen, Zhaoqi Kuang, Siqi Bao, Yuan Yao</strong></p>
<p>Recent large language models (LLMs) have shown strong reasoning capabilities. However, a critical question remains: do these models possess genuine reasoning skills particularly complex strategic reasoning or are they primarily excelling at sophisticated pattern recognition within their training data? To address this question, this paper presents a chess testbed, ChessArena, to evaluate the strategic reasoning capabilities of LLMs. Chess requires complex strategic reasoning capabilities including long-term planning, strict rule comprehension, and multi-turn conversation memorization. Specifically, ChessArena is a competitive framework where LLMs play against each other, under four different play modes. The testbed is equipped with a ranking algorithm and a leaderboard. The testbed can also evaluate fine-grained capabilities including basic understanding, move selection, and puzzle solving. Over 13 LLMs with different modes are evaluated in ChessArena, playing over 800 games. The results reveal significant shortcomings in current LLMs: no model can beat Maia-1100 (a chess engine at human amateur level), while some even failed to defeat a random player that selects moves arbitrarily. We also present a strong baseline to the testbed: our fine-tuned Qwen3-8B substantially improved performance, approaching much larger state-of-the-art reasoning models. </p>
<blockquote>
<p>è¿‘æœŸçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å±•ç°å‡ºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œä¸€ä¸ªé‡è¦çš„é—®é¢˜ä»ç„¶å­˜åœ¨ï¼šè¿™äº›æ¨¡å‹æ˜¯å¦å…·å¤‡çœŸæ­£çš„æ¨ç†æŠ€èƒ½ï¼Œå°¤å…¶æ˜¯å¤æ‚çš„æˆ˜ç•¥æ¨ç†ï¼Œè¿˜æ˜¯å®ƒä»¬ä¸»è¦æ“…é•¿åœ¨è®­ç»ƒæ•°æ®ä¸­çš„å¤æ‚æ¨¡å¼è¯†åˆ«ï¼Ÿä¸ºäº†å›ç­”è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªè±¡æ£‹æµ‹è¯•å¹³å°ChessArenaï¼Œä»¥è¯„ä¼°LLMçš„æˆ˜ç•¥æ¨ç†èƒ½åŠ›ã€‚è±¡æ£‹éœ€è¦å¤æ‚çš„æˆ˜ç•¥æ¨ç†èƒ½åŠ›ï¼ŒåŒ…æ‹¬é•¿æœŸè§„åˆ’ã€ä¸¥æ ¼ç†è§£è§„åˆ™å’Œå¤šè½®å¯¹è¯è®°å¿†ã€‚å…·ä½“æ¥è¯´ï¼ŒChessArenaæ˜¯ä¸€ä¸ªç«äº‰æ¡†æ¶ï¼ŒLLMä¹‹é—´åœ¨æ­¤å¹³å°ä¸Šç›¸äº’å¯¹æŠ—ï¼Œå…±æœ‰å››ç§ä¸åŒçš„æ¸¸æˆæ¨¡å¼ã€‚è¯¥å¹³å°é…å¤‡æœ‰æ’åç®—æ³•å’Œæ’è¡Œæ¦œã€‚è¯¥å¹³å°è¿˜å¯ä»¥è¯„ä¼°ç²¾ç»†çš„èƒ½åŠ›ï¼ŒåŒ…æ‹¬åŸºæœ¬ç†è§£ã€åŠ¨ä½œé€‰æ‹©å’Œé—®é¢˜è§£å†³ã€‚åœ¨ChessArenaä¸­è¯„ä¼°äº†è¶…è¿‡13ç§ä¸åŒæ¨¡å¼çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œè¿›è¡Œäº†è¶…è¿‡800åœºæ¯”èµ›ã€‚ç»“æœæ­ç¤ºäº†å½“å‰LLMçš„æ˜¾è‘—ä¸è¶³ï¼šæ²¡æœ‰æ¨¡å‹èƒ½å¤Ÿæˆ˜èƒœäººç±»ä¸šä½™æ°´å¹³çš„Maia-1100å¼•æ“ï¼Œç”šè‡³æœ‰äº›æ¨¡å‹æ— æ³•å‡»è´¥éšæœºç©å®¶é€‰æ‹©çš„éšæœºåŠ¨ä½œã€‚æˆ‘ä»¬è¿˜ä¸ºæµ‹è¯•å¹³å°æä¾›äº†å¼ºæœ‰åŠ›çš„åŸºå‡†ï¼šæˆ‘ä»¬å¾®è°ƒåçš„Qwen3-8Bå¤§å¹…æé«˜äº†æ€§èƒ½ï¼Œæ¥è¿‘å½“å‰æœ€å…ˆè¿›çš„æ¨ç†æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.24239v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ¨ç†èƒ½åŠ›ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†å­˜åœ¨ç–‘é—®ï¼šå®ƒä»¬æ˜¯å¦çœŸæ­£å…·å¤‡å¤æ‚çš„æˆ˜ç•¥æ¨ç†èƒ½åŠ›ï¼ŒæŠ‘æˆ–åªæ˜¯æ“…é•¿è®­ç»ƒæ•°æ®ä¸­çš„æ¨¡å¼è¯†åˆ«ï¼Ÿä¸ºè§£ç­”è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºChessArenaçš„è±¡æ£‹æµ‹è¯•å¹³å°ï¼Œæ—¨åœ¨è¯„ä¼°LLMçš„æˆ˜ç•¥æ¨ç†èƒ½åŠ›ã€‚è¯¥å¹³å°è®©LLMç›¸äº’ç«æŠ€ï¼ŒåŒ…å«å››ç§æ¸¸æˆæ¨¡å¼ï¼Œå¹¶é…å¤‡æ’åç®—æ³•å’Œæ’è¡Œæ¦œã€‚é€šè¿‡è¯„ä¼°åŸºç¡€ç†è§£ã€æ£‹æ­¥é€‰æ‹©å’Œè§£é¢˜èƒ½åŠ›ç­‰å¤šæ–¹é¢çš„ç²¾ç»†èƒ½åŠ›ï¼Œè¶…è¿‡13æ¬¾LLMå‚ä¸æµ‹è¯•ï¼Œå…±å¯¹æˆ˜800ä½™å±€ã€‚ç»“æœå‘ç°å½“å‰LLMå­˜åœ¨æ˜¾è‘—ä¸è¶³ï¼Œæ— æ³•å‡»è´¥äººç±»ä¸šä½™æ°´å¹³çš„è±¡æ£‹å¼•æ“Maia-1100ï¼Œéƒ¨åˆ†æ¨¡å‹ç”šè‡³ä¸æ•Œéšæœºé€‰æ‹©çš„å¯¹æ‰‹ã€‚ç»è¿‡è°ƒä¼˜çš„Qwen3-8Bæ¨¡å‹è¡¨ç°çªå‡ºï¼Œæ¥è¿‘é¡¶å°–æ¨ç†æ¨¡å‹æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›å—åˆ°å…³æ³¨ï¼Œå­˜åœ¨å…³äºå…¶æ˜¯å¦å…·å¤‡å¤æ‚æˆ˜ç•¥æ¨ç†èƒ½åŠ›çš„ç–‘é—®ã€‚</li>
<li>ChessArenaæµ‹è¯•å¹³å°è¢«æå‡ºï¼Œç”¨äºè¯„ä¼°LLMçš„æˆ˜ç•¥æ¨ç†èƒ½åŠ›ï¼ŒåŒ…æ‹¬é•¿æœŸè§„åˆ’ã€è§„åˆ™ç†è§£å’Œå¤šè½®å¯¹è¯è®°å¿†ç­‰æ–¹é¢ã€‚</li>
<li>å¹³å°é‡‡ç”¨ç«æŠ€æ¨¡å¼ï¼ŒåŒ…å«å››ç§æ¸¸æˆæ¨¡å¼ï¼Œå¹¶é…å¤‡æ’åç®—æ³•å’Œæ’è¡Œæ¦œã€‚</li>
<li>æµ‹è¯•å‘ç°å½“å‰LLMå­˜åœ¨æ˜¾è‘—ä¸è¶³ï¼Œæ— æ³•å‡»è´¥äººç±»ä¸šä½™æ°´å¹³çš„è±¡æ£‹å¼•æ“ã€‚</li>
<li>éƒ¨åˆ†LLMæ¨¡å‹åœ¨æµ‹è¯•ä¸­è¡¨ç°ä¸ä½³ï¼Œç”šè‡³ä¸æ•Œéšæœºé€‰æ‹©çš„å¯¹æ‰‹ã€‚</li>
<li>è°ƒä¼˜åçš„Qwen3-8Bæ¨¡å‹è¡¨ç°çªå‡ºï¼Œæˆä¸ºæµ‹è¯•ä¸­çš„å¼ºåŸºçº¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.24239">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fec6e44714124618ccd8761fe0be6ba6" align="middle">
<img src="https://picx.zhimg.com/v2-9f3bc45b36aea036db3cf6f93b9267bb" align="middle">
<img src="https://picx.zhimg.com/v2-bf8c84bb23d0ea242d884e07c1a1ef41" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="p-less-Sampling-A-Robust-Hyperparameter-Free-Approach-for-LLM-Decoding"><a href="#p-less-Sampling-A-Robust-Hyperparameter-Free-Approach-for-LLM-Decoding" class="headerlink" title="p-less Sampling: A Robust Hyperparameter-Free Approach for LLM Decoding"></a>p-less Sampling: A Robust Hyperparameter-Free Approach for LLM Decoding</h2><p><strong>Authors:Runyan Tan, Shuang Wu, Phillip Howard</strong></p>
<p>Obtaining high-quality outputs from Large Language Models (LLMs) often depends upon the choice of a sampling-based decoding strategy to probabilistically choose the next token at each generation step. While a variety of such sampling methods have been proposed, their performance can be sensitive to the selection of hyperparameters which may require different settings depending upon the generation task and temperature configuration. In this work, we introduce $p$-less sampling: an information-theoretic approach to sampling which dynamically sets a truncation threshold at each decoding step based on the entire token probability distribution. Unlike existing methods, $p$-less sampling has no hyperparameters and consistently produces high-quality outputs as temperature increases. We provide theoretical perspectives on $p$-less sampling to ground our proposed method and conduct experiments to empirically validate its effectiveness across a range of math, logical reasoning, and creative writing tasks. Our results demonstrate how $p$-less sampling consistently outperforms existing sampling approaches while exhibiting much less degradation in text quality at higher temperature values. We further show how $p$-less achieves greater inference-time efficiency than alternative methods through lower average token sampling times and shorter generation lengths, without sacrificing accuracy. Finally, we provide analyses to highlight the benefits of $p$-less through qualitative examples, case studies, and diversity assessments. The code is available at <a target="_blank" rel="noopener" href="https://github.com/ryttry/p-less">https://github.com/ryttry/p-less</a> . </p>
<blockquote>
<p>è·å–å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é«˜è´¨é‡è¾“å‡ºé€šå¸¸å–å†³äºåŸºäºé‡‡æ ·ç­–ç•¥çš„è§£ç ç­–ç•¥çš„é€‰æ‹©ï¼Œè¯¥ç­–ç•¥ä»¥æ¦‚ç‡æ–¹å¼é€‰æ‹©æ¯ä¸ªç”Ÿæˆæ­¥éª¤ä¸­çš„ä¸‹ä¸€ä¸ªä»¤ç‰Œã€‚è™½ç„¶å·²ç»æå‡ºäº†å¤šç§è¿™æ ·çš„é‡‡æ ·æ–¹æ³•ï¼Œä½†å®ƒä»¬çš„æ€§èƒ½å¯¹è¶…å‚æ•°çš„é€‰æ‹©å¾ˆæ•æ„Ÿï¼Œè¿™å¯èƒ½éœ€è¦æ ¹æ®ä¸åŒçš„ç”Ÿæˆä»»åŠ¡å’Œæ¸©åº¦é…ç½®è¿›è¡Œä¸åŒçš„è®¾ç½®ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†$p$-lessé‡‡æ ·ï¼šä¸€ç§åŸºäºä¿¡æ¯ç†è®ºçš„é‡‡æ ·æ–¹æ³•ï¼Œå®ƒæ ¹æ®æ•´ä¸ªä»¤ç‰Œæ¦‚ç‡åˆ†å¸ƒåŠ¨æ€åœ°åœ¨æ¯ä¸ªè§£ç æ­¥éª¤ä¸­è®¾ç½®æˆªæ–­é˜ˆå€¼ã€‚ä¸ç°æœ‰æ–¹æ³•ä¸åŒï¼Œ$p$-lessé‡‡æ ·æ²¡æœ‰è¶…å‚æ•°ï¼Œå¹¶ä¸”éšç€æ¸©åº¦çš„å‡é«˜ï¼Œå§‹ç»ˆäº§ç”Ÿé«˜è´¨é‡çš„è¾“å‡ºã€‚æˆ‘ä»¬ä»ç†è®ºä¸Šé˜è¿°äº†$p$-lessé‡‡æ ·çš„è§†è§’æ¥æ”¯æ’‘æˆ‘ä»¬æå‡ºçš„æ–¹æ³•ï¼Œå¹¶é€šè¿‡å®éªŒåœ¨å¤šç§æ•°å­¦ã€é€»è¾‘æ¨ç†å’Œåˆ›é€ æ€§å†™ä½œä»»åŠ¡ä¸Šå¯¹å…¶æœ‰æ•ˆæ€§è¿›è¡Œäº†å®è¯éªŒè¯ã€‚ç»“æœè¡¨æ˜ï¼Œ$p$-lessé‡‡æ ·åœ¨æ–‡æœ¬è´¨é‡è¾ƒé«˜æ—¶å§‹ç»ˆä¼˜äºç°æœ‰é‡‡æ ·æ–¹æ³•ï¼Œå¹¶ä¸”åœ¨è¾ƒé«˜æ¸©åº¦ä¸‹è¡¨ç°å‡ºè¾ƒå°‘çš„æ–‡æœ¬è´¨é‡ä¸‹é™ã€‚æˆ‘ä»¬è¿˜å±•ç¤ºäº†å¦‚ä½•é€šè¿‡è¾ƒä½çš„å¹³å‡ä»¤ç‰Œé‡‡æ ·æ—¶é—´å’Œè¾ƒçŸ­çš„ç”Ÿæˆé•¿åº¦æ¥å®ç°æ›´é«˜çš„æ¨ç†æ—¶é—´æ•ˆç‡ï¼Œè€Œä¸ä¼šç‰ºç‰²å‡†ç¡®æ€§æ¥è¯æ˜æ— $p$çš„ä¼˜è¶Šæ€§ã€‚æœ€åï¼Œæˆ‘ä»¬é€šè¿‡åˆ†æå®šæ€§ç¤ºä¾‹ã€æ¡ˆä¾‹ç ”ç©¶å’Œå¤šæ ·æ€§è¯„ä¼°æ¥å¼ºè°ƒæ— $p$çš„ç›Šå¤„ã€‚ç›¸å…³ä»£ç å·²å‘å¸ƒåœ¨ <a target="_blank" rel="noopener" href="https://github.com/ryttry/p-less">https://github.com/ryttry/p-less</a> ä¸Šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.23234v4">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡ä»‹ç»äº†æ— å‚æ•°é‡‡æ ·æ–¹æ³•ï¼ˆp-less samplingï¼‰ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºä¿¡æ¯ç†è®ºçš„æ–¹æ³•ï¼Œç”¨äºåœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­åŠ¨æ€è®¾å®šè§£ç æ­¥é•¿çš„æˆªæ–­é˜ˆå€¼ã€‚è¯¥æ–¹æ³•æ— éœ€è°ƒæ•´è¶…å‚æ•°ï¼Œéšç€æ¸©åº¦çš„å‡é«˜ï¼Œè¾“å‡ºè´¨é‡å§‹ç»ˆä¿æŒåœ¨è¾ƒé«˜æ°´å¹³ã€‚é€šè¿‡ç†è®ºå’Œå®éªŒéªŒè¯ï¼Œp-lessé‡‡æ ·åœ¨å„ç§ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼ŒåŒ…æ‹¬æ•°å­¦ã€é€»è¾‘æ¨ç†å’Œåˆ›é€ æ€§å†™ä½œä»»åŠ¡ã€‚æ­¤å¤–ï¼Œp-lessé‡‡æ ·è¿˜å…·æœ‰æ›´é«˜çš„æ¨ç†æ•ˆç‡ï¼Œç¼©çŸ­äº†å¹³å‡ä»¤ç‰Œé‡‡æ ·æ—¶é—´å’Œç”Ÿæˆé•¿åº¦ï¼ŒåŒæ—¶ä¸æŸå¤±å‡†ç¡®æ€§ã€‚æœ€åï¼Œé€šè¿‡å®šæ€§ç¤ºä¾‹ã€æ¡ˆä¾‹ç ”ç©¶å’Œå¤šæ ·æ€§è¯„ä¼°ï¼Œå±•ç¤ºäº†p-lessçš„ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>p-lessé‡‡æ ·æ˜¯ä¸€ç§åŸºäºä¿¡æ¯ç†è®ºçš„é‡‡æ ·æ–¹æ³•ï¼Œé€‚ç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹ã€‚</li>
<li>è¯¥æ–¹æ³•æ— éœ€è°ƒæ•´è¶…å‚æ•°ï¼Œé€‚ç”¨äºä¸åŒçš„ç”Ÿæˆä»»åŠ¡å’Œæ¸©åº¦é…ç½®ã€‚</li>
<li>éšç€æ¸©åº¦çš„å‡é«˜ï¼Œp-lessé‡‡æ ·èƒ½æŒç»­äº§ç”Ÿé«˜è´¨é‡çš„è¾“å‡ºã€‚</li>
<li>p-lessé‡‡æ ·åœ¨å„ç§ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼ŒåŒ…æ‹¬æ•°å­¦ã€é€»è¾‘æ¨ç†å’Œåˆ›é€ æ€§å†™ä½œä»»åŠ¡ã€‚</li>
<li>p-lessé‡‡æ ·æé«˜äº†æ¨ç†æ•ˆç‡ï¼Œç¼©çŸ­äº†å¹³å‡ä»¤ç‰Œé‡‡æ ·æ—¶é—´å’Œç”Ÿæˆé•¿åº¦ã€‚</li>
<li>p-lessé‡‡æ ·åœ¨ä¸æŸå¤±å‡†ç¡®æ€§çš„æƒ…å†µä¸‹å®ç°äº†é«˜æ•ˆæ¨ç†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.23234">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c12b782eccc86b55ac0b81422f1dbf04" align="middle">
<img src="https://picx.zhimg.com/v2-fe5646becb48d5bb0818572473ce25e6" align="middle">
<img src="https://picx.zhimg.com/v2-699b205b889a0f52908d740a2a78a36e" align="middle">
<img src="https://picx.zhimg.com/v2-8b5aab67e9914294d066e179b7c18ca1" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="SciReasoner-Laying-the-Scientific-Reasoning-Ground-Across-Disciplines"><a href="#SciReasoner-Laying-the-Scientific-Reasoning-Ground-Across-Disciplines" class="headerlink" title="SciReasoner: Laying the Scientific Reasoning Ground Across Disciplines"></a>SciReasoner: Laying the Scientific Reasoning Ground Across Disciplines</h2><p><strong>Authors:Yizhou Wang, Chen Tang, Han Deng, Jiabei Xiao, Jiaqi Liu, Jianyu Wu, Jun Yao, Pengze Li, Encheng Su, Lintao Wang, Guohang Zhuang, Yuchen Ren, Ben Fei, Ming Hu, Xin Chen, Dongzhan Zhou, Junjun He, Xiangyu Yue, Zhenfei Yin, Jiamin Wu, Qihao Zheng, Yuhao Zhou, Huihui Xu, Chenglong Ma, Yan Lu, Wenlong Zhang, Chunfeng Song, Philip Torr, Shixiang Tang, Xinzhu Ma, Wanli Ouyang, Lei Bai</strong></p>
<p>We present a scientific reasoning foundation model that aligns natural language with heterogeneous scientific representations. The model is pretrained on a 206B-token corpus spanning scientific text, pure sequences, and sequence-text pairs, then aligned via SFT on 40M instructions, annealed cold-start bootstrapping to elicit long-form chain-of-thought, and reinforcement learning with task-specific reward shaping, which instills deliberate scientific reasoning. It supports four capability families, covering up to 103 tasks across workflows: (i) faithful translation between text and scientific formats, (ii) text&#x2F;knowledge extraction, (iii) property prediction, (iv) property classification, (v) unconditional and conditional sequence generation and design. Compared with specialist systems, our approach broadens instruction coverage, improves cross-domain generalization, and enhances fidelity. We detail data curation and training and show that cross-discipline learning strengthens transfer and downstream reliability. The model, instruct tuning datasets and the evaluation code are open-sourced at <a target="_blank" rel="noopener" href="https://huggingface.co/SciReason">https://huggingface.co/SciReason</a> and <a target="_blank" rel="noopener" href="https://github.com/open-sciencelab/SciReason">https://github.com/open-sciencelab/SciReason</a>. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç§‘å­¦æ¨ç†åŸºç¡€æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å°†è‡ªç„¶è¯­è¨€ä¸å¼‚è´¨ç§‘å­¦è¡¨ç¤ºè¿›è¡Œå¯¹é½ã€‚è¯¥æ¨¡å‹åœ¨åŒ…å«ç§‘å­¦æ–‡æœ¬ã€çº¯åºåˆ—å’Œåºåˆ—æ–‡æœ¬å¯¹çš„206Bæ ‡è®°è¯­æ–™åº“ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œç„¶åé€šè¿‡40MæŒ‡ä»¤è¿›è¡ŒSFTå¯¹é½ï¼Œé‡‡ç”¨é€€ç«å†·å¯åŠ¨å¼•å¯¼æ³•æ¿€å‘é•¿å½¢å¼æ€ç»´é“¾ï¼Œå¹¶ä½¿ç”¨ä»»åŠ¡ç‰¹å®šå¥–åŠ±å½¢çŠ¶è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼Œä»è€ŒçŒè¾“æœ‰æ„è¯†çš„ç§‘å­¦æ¨ç†ã€‚å®ƒæ”¯æŒå››ä¸ªèƒ½åŠ›å®¶æ—ï¼Œæ¶µç›–103ä¸ªä»»åŠ¡çš„å·¥ä½œæµç¨‹ï¼šï¼ˆiï¼‰æ–‡æœ¬å’Œç§‘å­¦æ ¼å¼ä¹‹é—´çš„å¿ å®ç¿»è¯‘ï¼Œï¼ˆiiï¼‰æ–‡æœ¬&#x2F;çŸ¥è¯†æå–ï¼Œï¼ˆiiiï¼‰å±æ€§é¢„æµ‹ï¼Œï¼ˆivï¼‰å±æ€§åˆ†ç±»ï¼Œï¼ˆvï¼‰æ— æ¡ä»¶å’Œæ¡ä»¶åºåˆ—ç”Ÿæˆå’Œè®¾è®¡ã€‚ä¸ä¸“ç”¨ç³»ç»Ÿç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ‰©å¤§äº†æŒ‡ä»¤è¦†ç›–èŒƒå›´ï¼Œæé«˜äº†è·¨åŸŸæ³›åŒ–èƒ½åŠ›ï¼Œå¹¶å¢å¼ºäº†ä¿çœŸåº¦ã€‚æˆ‘ä»¬è¯¦ç»†æè¿°äº†æ•°æ®æ•´ç†å’Œè®­ç»ƒè¿‡ç¨‹ï¼Œå¹¶è¡¨æ˜è·¨å­¦ç§‘å­¦ä¹ åŠ å¼ºäº†è½¬ç§»å’Œä¸‹æ¸¸å¯é æ€§ã€‚è¯¥æ¨¡å‹ã€æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†å’Œè¯„ä¼°ä»£ç å·²åœ¨<a target="_blank" rel="noopener" href="https://huggingface.co/SciReason%E5%92%8Chttps://github.com/open-sciencelab/SciReason%E4%B8%8A%E5%BC%80%E6%BA%90%E3%80%82">https://huggingface.co/SciReasonå’Œhttps://github.com/open-sciencelab/SciReasonä¸Šå¼€æºã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.21320v2">PDF</a> technical report</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡ç« ä»‹ç»äº†ä¸€ä¸ªç§‘å­¦æ¨ç†åŸºç¡€æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å°†è‡ªç„¶è¯­è¨€ä¸å¼‚è´¨ç§‘å­¦è¡¨ç¤ºå¯¹é½ã€‚æ¨¡å‹åœ¨åŒ…å«ç§‘å­¦æ–‡æœ¬ã€çº¯åºåˆ—å’Œåºåˆ—æ–‡æœ¬å¯¹çš„206Bæ ‡è®°è¯­æ–™åº“ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œç„¶åé€šè¿‡SFTå¯¹é½çš„40MæŒ‡ä»¤è¿›è¡Œå¾®è°ƒï¼Œé‡‡ç”¨å†·å¯åŠ¨å¼•å¯¼æ³•æ¿€å‘é•¿å½¢å¼çš„æ€ç»´é“¾ï¼Œå¹¶é€šè¿‡ä»»åŠ¡ç‰¹å®šçš„å¥–åŠ±å¡‘å½¢å¼ºåŒ–å­¦ä¹ ï¼Œä»¥çŒè¾“æœ‰æ„çš„ç§‘å­¦æ¨ç†ã€‚è¯¥æ¨¡å‹æ”¯æŒå››ä¸ªèƒ½åŠ›å®¶æ—ï¼Œæ¶µç›–103ä¸ªä»»åŠ¡å’Œå·¥ä½œæµç¨‹ï¼ŒåŒ…æ‹¬æ–‡æœ¬ä¸ç§‘å­¦æ ¼å¼ä¹‹é—´çš„å¿ å®ç¿»è¯‘ã€æ–‡æœ¬&#x2F;çŸ¥è¯†æå–ã€å±æ€§é¢„æµ‹ã€å±æ€§åˆ†ç±»ã€æ— æ¡ä»¶å’Œæœ‰æ¡ä»¶çš„åºåˆ—ç”Ÿæˆå’Œè®¾è®¡ç­‰ã€‚ä¸ä¸“é¡¹ç³»ç»Ÿç›¸æ¯”ï¼Œè¯¥æ–¹æ³•æ‰©å¤§äº†æŒ‡ä»¤è¦†ç›–èŒƒå›´ï¼Œæé«˜äº†è·¨åŸŸæ³›åŒ–èƒ½åŠ›ï¼Œå¹¶æé«˜äº†ä¿çœŸåº¦ã€‚æ–‡ç« è¯¦ç»†ä»‹ç»äº†æ•°æ®æ”¶é›†å’Œæ¨¡å‹è®­ç»ƒçš„æƒ…å†µï¼Œå¹¶è¡¨æ˜è·¨å­¦ç§‘å­¦ä¹ åŠ å¼ºäº†è½¬ç§»å’Œä¸‹æ¸¸çš„å¯é æ€§ã€‚æ¨¡å‹å’Œè¯„ä¼°ä»£ç å·²åœ¨<a target="_blank" rel="noopener" href="https://huggingface.co/SciReason%E5%92%8Chttps://github.com/open-sciencelab/SciReason%E5%BC%80%E6%BA%90%E3%80%82">https://huggingface.co/SciReasonå’Œhttps://github.com/open-sciencelab/SciReasonå¼€æºã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¨¡å‹å°†è‡ªç„¶è¯­è¨€ä¸å¼‚è´¨ç§‘å­¦è¡¨ç¤ºå¯¹é½ï¼Œæ¶µç›–å¤šç§ç§‘å­¦æ–‡æœ¬æ ¼å¼ã€‚</li>
<li>æ¨¡å‹åœ¨å¤§é‡ç§‘å­¦æ–‡æœ¬æ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå¹¶ç»è¿‡æŒ‡ä»¤å¾®è°ƒã€‚</li>
<li>é‡‡ç”¨å†·å¯åŠ¨å¼•å¯¼æ³•æ¿€å‘é•¿å½¢å¼çš„æ€ç»´é“¾ï¼Œä»¥åŠå¼ºåŒ–å­¦ä¹ æ¥åŸ¹å…»ç§‘å­¦æ¨ç†èƒ½åŠ›ã€‚</li>
<li>æ¨¡å‹æ”¯æŒå¤šç§ä»»åŠ¡å’Œå·¥ä½œæµç¨‹ï¼ŒåŒ…æ‹¬ç¿»è¯‘ã€æå–ã€é¢„æµ‹ã€åˆ†ç±»ä»¥åŠåºåˆ—ç”Ÿæˆå’Œè®¾è®¡ç­‰ã€‚</li>
<li>ä¸å…¶ä»–ä¸“é¡¹ç³»ç»Ÿç›¸æ¯”ï¼Œè¯¥æ¨¡å‹åœ¨æŒ‡ä»¤è¦†ç›–ã€è·¨åŸŸæ³›åŒ–å’Œä¿çœŸåº¦æ–¹é¢æœ‰æ‰€ä¼˜åŠ¿ã€‚</li>
<li>æ–‡ç« å¼ºè°ƒäº†æ•°æ®æ”¶é›†å’Œæ¨¡å‹è®­ç»ƒçš„é‡è¦æ€§ï¼Œå¹¶å±•ç¤ºäº†è·¨å­¦ç§‘å­¦ä¹ çš„æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.21320">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ae95443c48455b2dabc3bc74795671ff" align="middle">
<img src="https://picx.zhimg.com/v2-b6305c19980605360763578c81ffe369" align="middle">
<img src="https://picx.zhimg.com/v2-afe996cc209842e261724c91f84852e7" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="LLMs-as-Layout-Designers-Enhanced-Spatial-Reasoning-for-Content-Aware-Layout-Generation"><a href="#LLMs-as-Layout-Designers-Enhanced-Spatial-Reasoning-for-Content-Aware-Layout-Generation" class="headerlink" title="LLMs as Layout Designers: Enhanced Spatial Reasoning for Content-Aware   Layout Generation"></a>LLMs as Layout Designers: Enhanced Spatial Reasoning for Content-Aware   Layout Generation</h2><p><strong>Authors:Sha Li, Stefano Petrangeli, Yu Shen, Xiang Chen, Naren Ramakrishnan</strong></p>
<p>While Large Language Models (LLMs) have demonstrated impressive reasoning and planning abilities in textual domains and can effectively follow instructions for complex tasks, their ability to understand and manipulate spatial relationships remains limited. Such capabilities are crucial for content-aware graphic layout design, where the goal is to arrange heterogeneous elements onto a canvas so that final design remains visually balanced and structurally feasible. This problem requires precise coordination of placement, alignment, and structural organization of multiple elements within a constrained visual space. To address this limitation, we introduce LaySPA, a reinforcement learning-based framework that augments LLM-based agents with explicit spatial reasoning capabilities for layout design. LaySPA employs hybrid reward signals that jointly capture geometric constraints, structural fidelity, and visual quality, enabling agents to navigate the canvas, model inter-element relationships, and optimize spatial arrangements. Through group-relative policy optimization, the agent generates content-aware layouts that reflect salient regions, respect spatial constraints, and produces an interpretable reasoning trace explaining placement decisions and a structured layout specification. Experimental results show that LaySPA substantially improves the generation of structurally valid and visually appealing layouts, outperforming larger general-purpose LLMs and achieving performance comparable to state-of-the-art specialized layout models. </p>
<blockquote>
<p>è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ–‡æœ¬é¢†åŸŸå±•ç¤ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„æ¨ç†å’Œè§„åˆ’èƒ½åŠ›ï¼Œå¹¶å¯ä»¥æœ‰æ•ˆæ‰§è¡Œå¤æ‚ä»»åŠ¡çš„æŒ‡ä»¤ï¼Œä½†å®ƒä»¬åœ¨ç†è§£å’Œæ“ä½œç©ºé—´å…³ç³»æ–¹é¢çš„èƒ½åŠ›ä»ç„¶æœ‰é™ã€‚è¿™ç§èƒ½åŠ›å¯¹äºå†…å®¹æ„ŸçŸ¥çš„å›¾å½¢å¸ƒå±€è®¾è®¡è‡³å…³é‡è¦ï¼Œå…¶ç›®çš„æ˜¯å°†ä¸åŒçš„å…ƒç´ æ’åˆ—åœ¨ç”»å¸ƒä¸Šï¼Œä½¿æœ€ç»ˆè®¾è®¡åœ¨è§†è§‰ä¸Šä¿æŒå¹³è¡¡å’Œç»“æ„ä¸Šçš„å¯è¡Œæ€§ã€‚è¿™ä¸ªé—®é¢˜éœ€è¦åœ¨ä¸€ä¸ªå—é™çš„è§†è§‰ç©ºé—´å†…ï¼Œç²¾ç¡®åè°ƒå¤šä¸ªå…ƒç´ çš„æ”¾ç½®ã€å¯¹é½å’Œç»“æ„ç»„ç»‡ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†LaySPAï¼Œä¸€ä¸ªåŸºäºå¼ºåŒ–å­¦ä¹ çš„æ¡†æ¶ï¼Œå®ƒé€šè¿‡å¢å¼ºLLMä»£ç†çš„ç©ºé—´æ¨ç†èƒ½åŠ›ï¼Œç”¨äºå¸ƒå±€è®¾è®¡ã€‚LaySPAé‡‡ç”¨æ··åˆå¥–åŠ±ä¿¡å·ï¼Œè”åˆæ•è·å‡ ä½•çº¦æŸã€ç»“æ„ä¿çœŸåº¦å’Œè§†è§‰è´¨é‡ï¼Œä½¿ä»£ç†èƒ½å¤Ÿæµè§ˆç”»å¸ƒï¼Œæ¨¡æ‹Ÿå…ƒç´ ä¹‹é—´çš„å…³ç³»ï¼Œå¹¶ä¼˜åŒ–ç©ºé—´å¸ƒå±€ã€‚é€šè¿‡ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼Œä»£ç†ç”Ÿæˆçš„å†…å®¹æ„ŸçŸ¥å¸ƒå±€åæ˜ äº†æ˜¾è‘—åŒºåŸŸï¼Œéµå®ˆç©ºé—´çº¦æŸï¼Œå¹¶äº§ç”Ÿå¯è§£é‡Šæ¨ç†è½¨è¿¹æ¥è§£é‡Šæ”¾ç½®å†³ç­–å’Œç»“æ„å¸ƒå±€è§„èŒƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLaySPAåœ¨ç”Ÿæˆç»“æ„ä¸Šæœ‰æ•ˆå’Œè§†è§‰ä¸Šå¸å¼•äººçš„å¸ƒå±€æ–¹é¢å¤§æœ‰è£¨ç›Šï¼Œå…¶è¡¨ç°ä¼˜äºå¤§å‹é€šç”¨LLMï¼Œä¸”æ€§èƒ½ä¸æœ€æ–°ä¸“ä¸šåŒ–çš„å¸ƒå±€æ¨¡å‹ç›¸å½“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.16891v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ–‡æœ¬é¢†åŸŸå±•ç°å‡ºå¼ºå¤§çš„æ¨ç†å’Œè§„åˆ’èƒ½åŠ›ï¼Œå¹¶èƒ½æœ‰æ•ˆæ‰§è¡Œå¤æ‚ä»»åŠ¡çš„æŒ‡ä»¤ï¼Œä½†åœ¨ç†è§£å’Œæ“ä½œç©ºé—´å…³ç³»æ–¹é¢çš„èƒ½åŠ›ä»ç„¶æœ‰é™ã€‚è¿™å¯¹äºå†…å®¹æ„ŸçŸ¥çš„å›¾å½¢å¸ƒå±€è®¾è®¡è‡³å…³é‡è¦ï¼Œè¯¥é¢†åŸŸçš„ç›®æ ‡æ˜¯å°†ä¸åŒçš„å…ƒç´ æ’åˆ—åœ¨ç”»å¸ƒä¸Šï¼Œä½¿æœ€ç»ˆè®¾è®¡åœ¨è§†è§‰ä¸Šä¿æŒå¹³è¡¡å¹¶åœ¨ç»“æ„ä¸Šå¯è¡Œã€‚ä¸ºè§£å†³è¿™ä¸€å±€é™ï¼Œæˆ‘ä»¬æ¨å‡ºäº†LaySPAï¼Œè¿™æ˜¯ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ¡†æ¶ï¼Œå¢å¼ºäº†LLMä»£ç†çš„ç©ºé—´æ¨ç†èƒ½åŠ›ï¼Œç”¨äºå¸ƒå±€è®¾è®¡ã€‚LaySPAé‡‡ç”¨æ··åˆå¥–åŠ±ä¿¡å·ï¼Œè”åˆæ•æ‰å‡ ä½•çº¦æŸã€ç»“æ„ä¿çœŸåº¦å’Œè§†è§‰è´¨é‡ï¼Œä½¿ä»£ç†èƒ½å¤Ÿæµè§ˆç”»å¸ƒã€æ¨¡æ‹Ÿå…ƒç´ é—´çš„å…³ç³»ï¼Œå¹¶ä¼˜åŒ–ç©ºé—´å¸ƒå±€ã€‚é€šè¿‡ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼Œè¯¥ä»£ç†èƒ½å¤Ÿç”Ÿæˆæ„ŸçŸ¥å†…å®¹çš„å¸ƒå±€ï¼Œåæ˜ æ˜¾è‘—åŒºåŸŸã€å°Šé‡ç©ºé—´çº¦æŸï¼Œå¹¶äº§ç”Ÿè§£é‡Šæ”¾ç½®å†³ç­–å’Œç»“æ„åŒ–å¸ƒå±€è§„èŒƒçš„æ¨ç†è½¨è¿¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLaySPAåœ¨ç”Ÿæˆç»“æ„ä¸Šæœ‰æ•ˆå’Œè§†è§‰ä¸Šå¸å¼•äººçš„å¸ƒå±€æ–¹é¢æœ‰ç€æ˜¾è‘—çš„æå‡ï¼Œå…¶æ€§èƒ½ä¼˜äºæ›´å¤§çš„é€šç”¨LLMsï¼Œå¹¶ä¸æœ€å…ˆè¿›çš„ä¸“ç”¨å¸ƒå±€æ¨¡å‹ç›¸å½“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç†è§£å’Œæ“ä½œç©ºé—´å…³ç³»æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>å†…å®¹æ„ŸçŸ¥çš„å›¾å½¢å¸ƒå±€è®¾è®¡éœ€è¦å®‰æ’å¼‚è´¨å…ƒç´ åœ¨ç”»å¸ƒä¸Šï¼Œä»¥ä¿æŒè§†è§‰å¹³è¡¡å’Œç»“æ„æ€§ã€‚</li>
<li>LaySPAæ˜¯ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œå¢å¼ºäº†LLMsçš„ç©ºé—´æ¨ç†èƒ½åŠ›ï¼Œç”¨äºå¸ƒå±€è®¾è®¡ã€‚</li>
<li>LaySPAé‡‡ç”¨æ··åˆå¥–åŠ±ä¿¡å·æ¥æ•æ‰å‡ ä½•çº¦æŸã€ç»“æ„ä¿çœŸåº¦å’Œè§†è§‰è´¨é‡ã€‚</li>
<li>LaySPAé€šè¿‡ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ç”Ÿæˆå†…å®¹æ„ŸçŸ¥çš„å¸ƒå±€ã€‚</li>
<li>LaySPAæé«˜äº†ç”Ÿæˆæœ‰æ•ˆå’Œå¸å¼•äººå¸ƒå±€çš„å®è´¨æ€§èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.16891">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6896d806ebf3b8abcb3da74825bfd3f2" align="middle">
<img src="https://picx.zhimg.com/v2-aa7ba4226a7d6dcef3d8092cd579487f" align="middle">
<img src="https://picx.zhimg.com/v2-e7549270297a53e3e4c2dfc23c034d0b" align="middle">
<img src="https://picx.zhimg.com/v2-4be2f1fdf7356ab467aeacdc145456c8" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="RPRO-Ranked-Preference-Reinforcement-Optimization-for-Enhancing-Medical-QA-and-Diagnostic-Reasoning"><a href="#RPRO-Ranked-Preference-Reinforcement-Optimization-for-Enhancing-Medical-QA-and-Diagnostic-Reasoning" class="headerlink" title="RPRO: Ranked Preference Reinforcement Optimization for Enhancing Medical   QA and Diagnostic Reasoning"></a>RPRO: Ranked Preference Reinforcement Optimization for Enhancing Medical   QA and Diagnostic Reasoning</h2><p><strong>Authors:Chia-Hsuan Hsu, Jun-En Ding, Hsin-Ling Hsu, Chun-Chieh Liao, Fang-Ming Hung, Feng Liu</strong></p>
<p>Medical question answering requires advanced reasoning that integrates domain knowledge with logical inference. However, existing large language models (LLMs) often generate reasoning chains that lack factual accuracy and clinical reliability. We propose Ranked Preference Reinforcement Optimization (RPRO), a novel framework that uniquely combines reinforcement learning with preference-driven reasoning refinement to enhance clinical chain-of-thought (CoT) performance. RPRO differentiates itself from prior approaches by employing task-adaptive reasoning templates and a probabilistic evaluation mechanism that aligns outputs with established clinical workflows, while automatically identifying and correcting low-quality reasoning chains. Unlike traditional pairwise preference methods, RPRO introduces a groupwise ranking optimization based on the Bradley-Terry model and incorporates KL-divergence regularization for stable training. Experiments on PubMedQA and MedQA-USMLE show consistent improvements over strong baselines. Remarkably, our 1.1B parameter model outperforms much larger 7B-13B models, including medical-specialized variants. These findings demonstrate that combining preference optimization with quality-driven refinement offers a scalable and effective approach to building more reliable, clinically grounded medical LLMs. </p>
<blockquote>
<p>åŒ»ç–—é—®ç­”éœ€è¦èåˆé¢†åŸŸçŸ¥è¯†å’Œé€»è¾‘æ¨ç†çš„å…ˆè¿›æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šå¸¸ç”Ÿæˆçš„æ¨ç†é“¾ç¼ºä¹äº‹å®å‡†ç¡®æ€§å’Œä¸´åºŠå¯é æ€§ã€‚æˆ‘ä»¬æå‡ºäº†æ’ååå¥½å¼ºåŒ–ä¼˜åŒ–ï¼ˆRPROï¼‰è¿™ä¸€æ–°é¢–æ¡†æ¶ï¼Œå®ƒé€šè¿‡å¼ºåŒ–å­¦ä¹ ä¸åå¥½é©±åŠ¨æ¨ç†ç²¾ç‚¼çš„ç»“åˆï¼Œä»¥æé«˜ä¸´åºŠæ€ç»´é“¾ï¼ˆCoTï¼‰çš„æ€§èƒ½ã€‚RPROé€šè¿‡é‡‡ç”¨ä»»åŠ¡é€‚åº”æ€§æ¨ç†æ¨¡æ¿å’Œæ¦‚ç‡è¯„ä¼°æœºåˆ¶ï¼Œå°†è¾“å‡ºä¸æ—¢å®šä¸´åºŠå·¥ä½œæµç¨‹å¯¹é½ï¼ŒåŒæ—¶è‡ªåŠ¨è¯†åˆ«å’Œçº æ­£ä½è´¨é‡æ¨ç†é“¾ï¼Œä»è€Œä¸å…ˆå‰çš„æ–¹æ³•ç›¸åŒºåˆ«ã€‚ä¸ä¼ ç»Ÿçš„æˆå¯¹åå¥½æ–¹æ³•ä¸åŒï¼ŒRPROå¼•å…¥åŸºäºBradley-Terryæ¨¡å‹çš„ç»„æ’åä¼˜åŒ–ï¼Œå¹¶èå…¥KLæ•£åº¦æ­£åˆ™åŒ–ä»¥å®ç°ç¨³å®šè®­ç»ƒã€‚åœ¨PubMedQAå’ŒMedQA-USMLEä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä¸å¼ºå¤§çš„åŸºçº¿ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ¨¡å‹æœ‰æŒç»­çš„æ”¹è¿›ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬1.1Bå‚æ•°æ¨¡å‹çš„è¡¨ç°ä¼˜äº7B-13Bçš„å¤§å‹æ¨¡å‹ï¼ŒåŒ…æ‹¬åŒ»ç–—ä¸“ä¸šå˜ä½“ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œåå¥½ä¼˜åŒ–ä¸è´¨é‡é©±åŠ¨ç²¾ç‚¼çš„ç»“åˆæä¾›äº†ä¸€ç§å¯æ‰©å±•å’Œæœ‰æ•ˆçš„é€”å¾„ï¼Œç”¨äºæ„å»ºæ›´å¯é ã€ä»¥ä¸´åºŠä¸ºåŸºç¡€çš„åŒ»ç–—LLMã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.00974v2">PDF</a> </p>
<p><strong>æ€»ç»“</strong></p>
<p>åŒ»å­¦é—®ç­”éœ€è¦èåˆé¢†åŸŸçŸ¥è¯†å’Œé€»è¾‘æ¨ç†çš„å…ˆè¿›æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰äº§ç”Ÿçš„æ¨ç†é“¾å¾€å¾€ç¼ºä¹äº‹å®å‡†ç¡®æ€§å’Œä¸´åºŠå¯é æ€§ã€‚æœ¬æ–‡æå‡ºä¸€ç§åä¸ºRanked Preference Reinforcement Optimizationï¼ˆRPROï¼‰çš„æ–°å‹æ¡†æ¶ï¼Œç»“åˆå¼ºåŒ–å­¦ä¹ ä¸åå¥½é©±åŠ¨æ¨ç†ä¼˜åŒ–ï¼Œæå‡ä¸´åºŠæ¨ç†é“¾ï¼ˆCoTï¼‰çš„æ€§èƒ½ã€‚RPROçš„ç‹¬ç‰¹ä¹‹å¤„åœ¨äºé‡‡ç”¨ä»»åŠ¡é€‚åº”æ€§æ¨ç†æ¨¡æ¿å’Œæ¦‚ç‡è¯„ä¼°æœºåˆ¶ï¼Œä½¿è¾“å‡ºä¸æ—¢å®šä¸´åºŠå·¥ä½œæµç¨‹ç›¸ç¬¦ï¼ŒåŒæ—¶è‡ªåŠ¨è¯†åˆ«å’Œçº æ­£ä½è´¨é‡æ¨ç†é“¾ã€‚ä¸åŒäºä¼ ç»Ÿçš„æˆå¯¹åå¥½æ–¹æ³•ï¼ŒRPROå¼•å…¥åŸºäºBradley-Terryæ¨¡å‹çš„ç»„æ’åä¼˜åŒ–ï¼Œå¹¶ç»“åˆKLæ•£åº¦æ­£åˆ™åŒ–å®ç°ç¨³å®šè®­ç»ƒã€‚åœ¨PubMedQAå’ŒMedQA-USMLEä¸Šçš„å®éªŒè¡¨æ˜ï¼Œç›¸è¾ƒäºå¼ºå¤§çš„åŸºçº¿æ¨¡å‹ï¼ŒRPROè¡¨ç°å‡ºä¸€è‡´çš„ä¼˜åŠ¿ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå‚æ•°è§„æ¨¡ä¸º1.1Bçš„æ¨¡å‹ç”šè‡³è¶…è¶Šäº†å‚æ•°è§„æ¨¡æ›´å¤§çš„7Bè‡³13Bæ¨¡å‹ï¼ŒåŒ…æ‹¬åŒ»å­¦ä¸“ç”¨å˜ä½“ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œåå¥½ä¼˜åŒ–ä¸è´¨é‡é©±åŠ¨ç»†åŒ–ç›¸ç»“åˆçš„æ–¹æ³•ä¸ºæ„å»ºæ›´å¯é ã€æ›´è´´è¿‘ä¸´åºŠçš„å¤§å‹è¯­è¨€æ¨¡å‹æä¾›äº†å¯ä¼¸ç¼©å’Œæœ‰æ•ˆçš„é€”å¾„ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>åŒ»å­¦é—®ç­”éœ€è¦èåˆé¢†åŸŸçŸ¥è¯†å’Œé€»è¾‘æ¨ç†çš„å…ˆè¿›æ¨ç†èƒ½åŠ›ã€‚</li>
<li>ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åŒ»å­¦é—®ç­”ä¸­çš„æ¨ç†é“¾å¸¸ç¼ºä¹äº‹å®å‡†ç¡®æ€§å’Œä¸´åºŠå¯é æ€§ã€‚</li>
<li>RPROæ¡†æ¶ç»“åˆå¼ºåŒ–å­¦ä¹ ä¸åå¥½é©±åŠ¨æ¨ç†ä¼˜åŒ–ï¼Œæ—¨åœ¨æé«˜ä¸´åºŠæ¨ç†é“¾æ€§èƒ½ã€‚</li>
<li>RPROé‡‡ç”¨ä»»åŠ¡é€‚åº”æ€§æ¨ç†æ¨¡æ¿å’Œæ¦‚ç‡è¯„ä¼°æœºåˆ¶ï¼Œä½¿è¾“å‡ºæ›´ç¬¦åˆä¸´åºŠå·¥ä½œæµç¨‹ã€‚</li>
<li>RPROèƒ½è‡ªåŠ¨è¯†åˆ«å¹¶çº æ­£ä½è´¨é‡æ¨ç†é“¾ã€‚</li>
<li>ä¸ä¼ ç»Ÿåå¥½æ–¹æ³•ä¸åŒï¼ŒRPROå¼•å…¥ç»„æ’åä¼˜åŒ–å’ŒKLæ•£åº¦æ­£åˆ™åŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.00974">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c73d112beac33eb451a83016f38e2988" align="middle">
<img src="https://picx.zhimg.com/v2-ca61a33dd3667801ca401ce30261a6c1" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Interpretable-Reward-Model-via-Sparse-Autoencoder"><a href="#Interpretable-Reward-Model-via-Sparse-Autoencoder" class="headerlink" title="Interpretable Reward Model via Sparse Autoencoder"></a>Interpretable Reward Model via Sparse Autoencoder</h2><p><strong>Authors:Shuyi Zhang, Wei Shi, Sihang Li, Jiayi Liao, Tao Liang, Hengxing Cai, Xiang Wang</strong></p>
<p>Large language models (LLMs) have been widely deployed across numerous fields. Reinforcement Learning from Human Feedback (RLHF) leverages reward models (RMs) as proxies for human preferences to align LLM behaviors with human values, making the accuracy, reliability, and interpretability of RMs critical for effective alignment. However, traditional RMs lack interpretability, offer limited insight into the reasoning behind reward assignments, and are inflexible toward user preference shifts. While recent multidimensional RMs aim for improved interpretability, they often fail to provide feature-level attribution and require costly annotations. To overcome these limitations, we introduce the Sparse Autoencoder-enhanced Reward Model (SARM), a novel architecture that integrates a pretrained Sparse Autoencoder (SAE) into a reward model. SARM maps the hidden activations of LLM-based RM into an interpretable, sparse, and monosemantic feature space, from which a scalar head aggregates feature activations to produce transparent and conceptually meaningful reward scores. Empirical evaluations demonstrate that SARM facilitates direct feature-level attribution of reward assignments, allows dynamic adjustment to preference shifts, and achieves superior alignment performance compared to conventional reward models. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/schrieffer-z/sarm">https://github.com/schrieffer-z/sarm</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²å¹¿æ³›åº”ç”¨äºå¤šä¸ªé¢†åŸŸã€‚å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰åˆ©ç”¨å¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰ä½œä¸ºäººç±»åå¥½çš„ä»£ç†ï¼Œä½¿LLMè¡Œä¸ºä¸äººç±»ä»·å€¼è§‚ä¿æŒä¸€è‡´ï¼Œå› æ­¤RMçš„å‡†ç¡®æ€§ã€å¯é æ€§å’Œå¯è§£é‡Šæ€§å¯¹äºæœ‰æ•ˆå¯¹é½è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œä¼ ç»ŸRMç¼ºä¹å¯è§£é‡Šæ€§ï¼Œå¯¹å¥–åŠ±åˆ†é…èƒŒåçš„æ¨ç†æä¾›æœ‰é™çš„è§è§£ï¼Œå¹¶ä¸”å¯¹ç”¨æˆ·åå¥½å˜åŒ–ä¸å¤Ÿçµæ´»ã€‚è™½ç„¶æœ€è¿‘çš„å¤šç»´RMæ—¨åœ¨æé«˜å¯è§£é‡Šæ€§ï¼Œä½†å®ƒä»¬å¾€å¾€æ— æ³•æä¾›ç‰¹å¾çº§åˆ«çš„å½’å±ï¼Œå¹¶ä¸”éœ€è¦æ˜‚è´µçš„æ³¨é‡Šã€‚ä¸ºäº†å…‹æœè¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç¨€ç–è‡ªç¼–ç å™¨å¢å¼ºå¥–åŠ±æ¨¡å‹ï¼ˆSARMï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æ¶æ„ï¼Œå®ƒå°†é¢„è®­ç»ƒçš„ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSAEï¼‰é›†æˆåˆ°å¥–åŠ±æ¨¡å‹ä¸­ã€‚SARMå°†åŸºäºLLMçš„RMçš„éšè—æ¿€æ´»æ˜ å°„åˆ°å¯è§£é‡Šã€ç¨€ç–å’Œå•è¯­ä¹‰ç‰¹å¾ç©ºé—´ï¼Œå…¶ä¸­æ ‡é‡å¤´èšåˆç‰¹å¾æ¿€æ´»ä»¥äº§ç”Ÿé€æ˜ä¸”æ¦‚å¿µä¸Šæœ‰æ„ä¹‰çš„å¥–åŠ±åˆ†æ•°ã€‚ç»éªŒè¯„ä¼°è¡¨æ˜ï¼ŒSARMä¿ƒè¿›äº†å¥–åŠ±åˆ†é…çš„ç‰¹å¾çº§åˆ«å½’å±ï¼Œå…è®¸åŠ¨æ€è°ƒæ•´åå¥½å˜åŒ–ï¼Œå¹¶å®ç°äº†ä¸å¸¸è§„å¥–åŠ±æ¨¡å‹ç›¸æ¯”æ›´ä¼˜è¶Šçš„å¯¹é½æ€§èƒ½ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/schrieffer-z/sarm%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/schrieffer-z/sarmè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.08746v3">PDF</a> Commercial firm need to review this paper before publishing it</p>
<p><strong>Summary</strong></p>
<p>å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä¼—å¤šé¢†åŸŸå¹¿æ³›åº”ç”¨ã€‚å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰åˆ©ç”¨å¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰ä½œä¸ºäººç±»åå¥½çš„ä»£ç†ï¼Œä½¿LLMè¡Œä¸ºä¸äººçš„ä»·å€¼è§‚ä¸€è‡´ï¼Œä½¿å¾—RMçš„å‡†ç¡®æ€§ã€å¯é æ€§å’Œå¯è§£é‡Šæ€§å¯¹äºæœ‰æ•ˆå¯¹é½è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œä¼ ç»ŸRMç¼ºä¹å¯è§£é‡Šæ€§ï¼Œå¯¹äººç±»å¥–åŠ±åˆ†é…èƒŒåçš„æ¨ç†æä¾›æœ‰é™è§è§£ï¼Œä¸”å¯¹ç”¨æˆ·åå¥½å˜åŒ–ä¸å¤Ÿçµæ´»ã€‚ä¸ºå…‹æœè¿™äº›å±€é™ï¼Œæˆ‘ä»¬å¼•å…¥ç¨€ç–è‡ªç¼–ç å™¨å¢å¼ºå¥–åŠ±æ¨¡å‹ï¼ˆSARMï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æ¶æ„ï¼Œå°†é¢„è®­ç»ƒçš„ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSAEï¼‰èå…¥å¥–åŠ±æ¨¡å‹ä¸­ã€‚SARMå°†LLMåŸºäºRMçš„éšè—æ¿€æ´»æ˜ å°„åˆ°å¯è§£é‡Šã€ç¨€ç–ä¸”å•è¯­ä¹‰çš„ç‰¹å¾ç©ºé—´ï¼Œæ ‡é‡å¤´ä»è¯¥ç©ºé—´èšåˆç‰¹å¾æ¿€æ´»ä»¥äº§ç”Ÿé€æ˜ä¸”æ¦‚å¿µä¸Šæ„ä¹‰æ˜ç¡®çš„å¥–åŠ±åˆ†æ•°ã€‚å®è¯è¯„ä¼°è¡¨æ˜ï¼ŒSARMä¿ƒè¿›å¥–åŠ±åˆ†é…çš„ç›´æ¥ç‰¹å¾çº§å½’å› ï¼Œå…è®¸åŠ¨æ€é€‚åº”åå¥½å˜åŒ–ï¼Œå¹¶å®ç°äº†ç›¸è¾ƒäºä¼ ç»Ÿå¥–åŠ±æ¨¡å‹çš„ä¼˜è¶Šå¯¹é½æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¹¿æ³›åº”ç”¨äºå¤šä¸ªé¢†åŸŸã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰ä½¿ç”¨å¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰æ¥ä»£è¡¨äººç±»åå¥½ï¼Œä»¥å¯¹é½LLMè¡Œä¸ºä¸äººç±»ä»·å€¼è§‚ã€‚</li>
<li>ä¼ ç»ŸRMç¼ºä¹å¯è§£é‡Šæ€§ï¼Œéš¾ä»¥äº†è§£å¥–åŠ±åˆ†é…èƒŒåçš„æ¨ç†ï¼Œå¹¶ä¸”ä¸èƒ½çµæ´»åœ°é€‚åº”ç”¨æˆ·åå¥½å˜åŒ–ã€‚</li>
<li>æ–°æå‡ºçš„ç¨€ç–è‡ªç¼–ç å™¨å¢å¼ºå¥–åŠ±æ¨¡å‹ï¼ˆSARMï¼‰æ•´åˆäº†é¢„è®­ç»ƒçš„ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSAEï¼‰åˆ°RMä¸­ï¼Œæå‡äº†RMçš„å¯è§£é‡Šæ€§ã€‚</li>
<li>SARMå°†RMçš„éšè—æ¿€æ´»æ˜ å°„åˆ°å¯è§£é‡Šã€ç¨€ç–çš„ç‰¹å¾ç©ºé—´ï¼Œæä¾›å¥–åŠ±åˆ†é…çš„ç›´æ¥ç‰¹å¾çº§å½’å› ã€‚</li>
<li>SARMå…è®¸åŠ¨æ€è°ƒæ•´ä»¥é€‚åº”åå¥½å˜åŒ–ï¼Œå¹¶é€šè¿‡å®è¯è¯„ä¼°å®ç°äº†ä¼˜è¶Šçš„å¯¹é½æ€§èƒ½ã€‚</li>
<li>ç›¸å…³ä»£ç å·²å…¬å¼€å¯ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.08746">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e2a0401118c47f42664c149a95254637" align="middle">
<img src="https://picx.zhimg.com/v2-9e38c8e6b12330c78ec551a169893282" align="middle">
<img src="https://picx.zhimg.com/v2-926930e1ed1f7c34ee897abec757b01f" align="middle">
<img src="https://picx.zhimg.com/v2-491c62cf4888b77b5071074321f6bb81" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Audio-Thinker-Guiding-Audio-Language-Model-When-and-How-to-Think-via-Reinforcement-Learning"><a href="#Audio-Thinker-Guiding-Audio-Language-Model-When-and-How-to-Think-via-Reinforcement-Learning" class="headerlink" title="Audio-Thinker: Guiding Audio Language Model When and How to Think via   Reinforcement Learning"></a>Audio-Thinker: Guiding Audio Language Model When and How to Think via   Reinforcement Learning</h2><p><strong>Authors:Shu Wu, Chenxing Li, Wenfu Wang, Hao Zhang, Hualei Wang, Meng Yu, Dong Yu</strong></p>
<p>Recent advancements in large language models, multimodal large language models, and large audio language models (LALMs) have significantly improved their reasoning capabilities through reinforcement learning with rule-based rewards. However, the explicit reasoning process has yet to show significant benefits for audio question answering, and effectively leveraging deep reasoning remains an open challenge, with LALMs still falling short of human-level auditory-language reasoning. To address these limitations, we propose Audio-Thinker, a reinforcement learning framework designed to enhance the reasoning capabilities of LALMs, with a focus on improving adaptability, consistency, and effectiveness. Our approach introduces an adaptive think accuracy reward, enabling the model to adjust its reasoning strategies based on task complexity dynamically. Furthermore, we incorporate an external reward model to evaluate the overall consistency and quality of the reasoning process, complemented by think-based rewards that help the model distinguish between valid and flawed reasoning paths during training. Experimental results demonstrate that our Audio-Thinker model outperforms existing reasoning-oriented LALMs across various benchmark tasks, exhibiting superior reasoning and generalization capabilities. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ã€å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹å’Œå¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆLALMï¼‰çš„è¿›æ­¥é€šè¿‡åŸºäºè§„åˆ™çš„å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ æ˜¾è‘—æé«˜äº†å…¶æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œæ˜ç¡®çš„æ¨ç†è¿‡ç¨‹åœ¨éŸ³é¢‘é—®ç­”æ–¹é¢å°šæœªæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ•ˆç›Šï¼Œæœ‰æ•ˆåˆ©ç”¨æ·±åº¦æ¨ç†ä»ç„¶æ˜¯ä¸€ä¸ªå¼€æ”¾æ€§çš„æŒ‘æˆ˜ï¼ŒLALMåœ¨å¬è§‰è¯­è¨€æ¨ç†æ–¹é¢ä»æœªèƒ½è¾¾åˆ°äººç±»æ°´å¹³ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†Audio-Thinkerï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨æé«˜LALMæ¨ç†èƒ½åŠ›çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œé‡ç‚¹å…³æ³¨é€‚åº”æ€§ã€ä¸€è‡´æ€§å’Œæœ‰æ•ˆæ€§çš„æå‡ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†ä¸€ç§è‡ªé€‚åº”çš„æ€è€ƒå‡†ç¡®æ€§å¥–åŠ±ï¼Œä½¿æ¨¡å‹èƒ½å¤ŸåŸºäºä»»åŠ¡çš„å¤æ‚æ€§åŠ¨æ€åœ°è°ƒæ•´å…¶æ¨ç†ç­–ç•¥ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬èå…¥äº†ä¸€ä¸ªå¤–éƒ¨å¥–åŠ±æ¨¡å‹æ¥è¯„ä¼°æ¨ç†è¿‡ç¨‹çš„æ•´ä½“ä¸€è‡´æ€§å’Œè´¨é‡ï¼Œè¾…ä»¥åŸºäºæ€è€ƒå¥–åŠ±ï¼Œå¸®åŠ©æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŒºåˆ†æœ‰æ•ˆçš„å’Œé”™è¯¯çš„æ¨ç†è·¯å¾„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„Audio-Thinkeræ¨¡å‹åœ¨å„ç§åŸºå‡†ä»»åŠ¡ä¸Šä¼˜äºç°æœ‰çš„ä»¥æ¨ç†ä¸ºå¯¼å‘çš„LALMï¼Œå±•ç°å‡ºå“è¶Šçš„æ¨ç†å’Œæ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.08039v3">PDF</a> preprint</p>
<p><strong>Summary</strong>ï¼š<br>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ã€å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹å’Œå¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆLALMï¼‰çš„è¿‘æœŸè¿›å±•ï¼Œé€šè¿‡åŸºäºè§„åˆ™çš„å¥–åŠ±è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼Œå…¶æ¨ç†èƒ½åŠ›å·²æ˜¾è‘—æé«˜ã€‚ç„¶è€Œï¼Œåœ¨éŸ³é¢‘é—®ç­”æ–¹é¢ï¼Œæ˜ç¡®æ¨ç†è¿‡ç¨‹å°šæœªæ˜¾ç¤ºå‡ºæ˜¾è‘—æ•ˆç›Šï¼Œæœ‰æ•ˆåˆ©ç”¨æ·±åº¦æ¨ç†ä»æ˜¯å¼€æ”¾æŒ‘æˆ˜ï¼ŒLALMåœ¨éŸ³é¢‘è¯­è¨€æ¨ç†æ–¹é¢ä»è¾¾ä¸åˆ°äººç±»æ°´å¹³ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Audio-Thinkerï¼Œä¸€ä¸ªæ—¨åœ¨æé«˜LALMæ¨ç†èƒ½åŠ›çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œé‡ç‚¹æé«˜å…¶é€‚åº”æ€§ã€ä¸€è‡´æ€§å’Œæœ‰æ•ˆæ€§ã€‚é€šè¿‡å¼•å…¥è‡ªé€‚åº”æ€è€ƒç²¾åº¦å¥–åŠ±ï¼Œä½¿æ¨¡å‹èƒ½å¤ŸåŸºäºä»»åŠ¡å¤æ‚æ€§åŠ¨æ€è°ƒæ•´å…¶æ¨ç†ç­–ç•¥ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ç»“åˆå¤–éƒ¨å¥–åŠ±æ¨¡å‹æ¥è¯„ä¼°æ¨ç†è¿‡ç¨‹çš„æ•´ä½“ä¸€è‡´æ€§å’Œè´¨é‡ï¼Œè¾…ä»¥åŸºäºæ€è€ƒçš„å¥–åŠ±ï¼Œå¸®åŠ©æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŒºåˆ†æœ‰æ•ˆçš„å’Œé”™è¯¯çš„æ¨ç†è·¯å¾„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„Audio-Thinkeræ¨¡å‹åœ¨å„é¡¹åŸºå‡†ä»»åŠ¡ä¸Šä¼˜äºç°æœ‰çš„ä»¥æ¨ç†ä¸ºå¯¼å‘çš„LALMï¼Œå±•ç°å‡ºå“è¶Šçš„æ¨ç†å’Œæ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>è¿‘æœŸå¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¼ºåŒ–å­¦ä¹ ä¸‹æé«˜äº†æ¨ç†èƒ½åŠ›ã€‚</li>
<li>éŸ³é¢‘é—®ç­”ä¸­æ˜ç¡®æ¨ç†è¿‡ç¨‹çš„æ•ˆç›Šå°šæœªæ˜¾ç°ã€‚</li>
<li>LALMåœ¨éŸ³é¢‘è¯­è¨€æ¨ç†ä¸Šä»è¾¾ä¸åˆ°äººç±»æ°´å¹³ã€‚</li>
<li>Audio-Thinkeræ˜¯ä¸€ä¸ªæ—¨åœ¨æé«˜LALMæ¨ç†èƒ½åŠ›çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ã€‚</li>
<li>Audio-Thinkerå¼•å…¥è‡ªé€‚åº”æ€è€ƒç²¾åº¦å¥–åŠ±ä»¥æé«˜æ¨¡å‹é€‚åº”æ€§ã€‚</li>
<li>ç»“åˆå¤–éƒ¨å¥–åŠ±æ¨¡å‹å’ŒåŸºäºæ€è€ƒçš„å¥–åŠ±æ¥è¯„ä¼°æ¨ç†è¿‡ç¨‹çš„è´¨é‡å’ŒåŒºåˆ†æœ‰æ•ˆä¸é”™è¯¯è·¯å¾„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.08039">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-73a9eaee454185646801b9e845ebe50e" align="middle">
<img src="https://picx.zhimg.com/v2-ac49b30637545901965c2041fff1d0ed" align="middle">
<img src="https://picx.zhimg.com/v2-c755e54d7549cdf46af05b938e2641b3" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-11-09/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-11-09/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-09/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-509bb1a6994152d04278cf654ee56ca5" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-09  RLAIF-V Open-Source AI Feedback Leads to Super GPT-4V Trustworthiness
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-11-09
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-08/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-7852cef2e0dbacd0b30030bf7a2efb90" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-08  THEval. Evaluation Framework for Talking Head Video Generation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-11-08
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32883.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
