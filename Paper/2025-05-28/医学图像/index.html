<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-28  Deep Spectral Prior">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-008555b808d4437acca293ee2013d487.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-28
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-24
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    80 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-28-æ›´æ–°"><a href="#2025-05-28-æ›´æ–°" class="headerlink" title="2025-05-28 æ›´æ–°"></a>2025-05-28 æ›´æ–°</h1><h2 id="Deep-Spectral-Prior"><a href="#Deep-Spectral-Prior" class="headerlink" title="Deep Spectral Prior"></a>Deep Spectral Prior</h2><p><strong>Authors:Yanqi Cheng, Tieyong Zeng, Pietro Lio, Carola-Bibiane SchÃ¶nlieb, Angelica I Aviles-Rivero</strong></p>
<p>We introduce Deep Spectral Prior (DSP), a new formulation of Deep Image Prior (DIP) that redefines image reconstruction as a frequency-domain alignment problem. Unlike traditional DIP, which relies on pixel-wise loss and early stopping to mitigate overfitting, DSP directly matches Fourier coefficients between the network output and observed measurements. This shift introduces an explicit inductive bias towards spectral coherence, aligning with the known frequency structure of images and the spectral bias of convolutional neural networks. We provide a rigorous theoretical framework demonstrating that DSP acts as an implicit spectral regulariser, suppressing high-frequency noise by design and eliminating the need for early stopping. Our analysis spans four core dimensions establishing smooth convergence dynamics, local stability, and favourable bias-variance tradeoffs. We further show that DSP naturally projects reconstructions onto a frequency-consistent manifold, enhancing interpretability and robustness. These theoretical guarantees are supported by empirical results across denoising, inpainting, and super-resolution tasks, where DSP consistently outperforms classical DIP and other unsupervised baselines. </p>
<blockquote>
<p>æˆ‘ä»¬å¼•å…¥äº†æ·±åº¦è°±å…ˆéªŒï¼ˆDSPï¼‰ï¼Œè¿™æ˜¯æ·±åº¦å›¾åƒå…ˆéªŒï¼ˆDIPï¼‰çš„ä¸€ç§æ–°å½¢å¼ï¼Œå®ƒå°†å›¾åƒé‡å»ºé‡æ–°å®šä¹‰ä¸ºä¸€ä¸ªé¢‘åŸŸå¯¹é½é—®é¢˜ã€‚ä¸åŒäºä¼ ç»Ÿçš„DIPï¼Œå®ƒä¾èµ–äºåƒç´ çº§çš„æŸå¤±å’Œæ—©æœŸåœæ­¢æ¥ç¼“è§£è¿‡æ‹Ÿåˆï¼ŒDSPç›´æ¥åŒ¹é…ç½‘ç»œè¾“å‡ºå’Œè§‚æµ‹æµ‹é‡ä¹‹é—´çš„å‚…é‡Œå¶ç³»æ•°ã€‚è¿™ç§è½¬å˜å¼•å…¥äº†å¯¹é¢‘è°±ä¸€è‡´æ€§çš„æ˜ç¡®å½’çº³åç½®ï¼Œä¸å·²çŸ¥çš„å›¾åƒé¢‘ç‡ç»“æ„å’Œå·ç§¯ç¥ç»ç½‘ç»œçš„é¢‘è°±åç½®ç›¸å¯¹åº”ã€‚æˆ‘ä»¬æä¾›äº†ä¸€ä¸ªä¸¥æ ¼çš„ç†è®ºæ¡†æ¶ï¼Œè¯æ˜DSPå……å½“éšå¼è°±æ­£åˆ™åŒ–å™¨ï¼Œé€šè¿‡è®¾è®¡æŠ‘åˆ¶é«˜é¢‘å™ªå£°ï¼Œä¸éœ€è¦æ—©æœŸåœæ­¢ã€‚æˆ‘ä»¬çš„åˆ†ææ¶µç›–äº†å››ä¸ªæ ¸å¿ƒç»´åº¦ï¼Œå»ºç«‹äº†å¹³ç¨³çš„æ”¶æ•›åŠ¨åŠ›å­¦ã€å±€éƒ¨ç¨³å®šæ€§å’Œæœ‰åˆ©çš„åè§-æ–¹å·®æƒè¡¡ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è¡¨æ˜ï¼ŒDSPè‡ªç„¶åœ°å°†é‡å»ºæŠ•å½±åˆ°é¢‘ç‡ä¸€è‡´çš„æµå½¢ä¸Šï¼Œå¢å¼ºäº†å¯è§£é‡Šæ€§å’Œç¨³å¥æ€§ã€‚è¿™äº›ç†è®ºä¿è¯åœ¨é™å™ªã€ä¿®å¤å’Œè¶…åˆ†è¾¨ç‡ä»»åŠ¡ä¸Šçš„ç»éªŒç»“æœä¸­å¾—åˆ°äº†æ”¯æŒï¼Œåœ¨è¿™äº›ä»»åŠ¡ä¸­ï¼ŒDSPå§‹ç»ˆä¼˜äºç»å…¸çš„DIPå’Œå…¶ä»–æ— ç›‘ç£åŸºå‡†æµ‹è¯•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19873v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>Deep Spectral Priorï¼ˆDSPï¼‰æ˜¯ä¸€ç§æ–°çš„Deep Image Priorï¼ˆDIPï¼‰è¡¨è¿°æ–¹å¼ï¼Œå®ƒå°†å›¾åƒé‡å»ºå®šä¹‰ä¸ºé¢‘åŸŸå¯¹é½é—®é¢˜ã€‚ä¸ä¼ ç»Ÿçš„DIPä¸åŒï¼ŒDSPç›´æ¥åŒ¹é…ç½‘ç»œè¾“å‡ºå’Œè§‚æµ‹å€¼ä¹‹é—´çš„å‚…é‡Œå¶ç³»æ•°ï¼Œå¼•å…¥äº†å¯¹é¢‘è°±ä¸€è‡´æ€§çš„æ˜¾æ€§è¯±å¯¼åè§ã€‚DSPçš„ç†è®ºæ¡†æ¶è¯æ˜å…¶èƒ½å¤Ÿä½œä¸ºä¸€ç§éšå¼é¢‘è°±æ­£åˆ™åŒ–å™¨ï¼Œé€šè¿‡è®¾è®¡æŠ‘åˆ¶é«˜é¢‘å™ªå£°ï¼Œå¹¶æ¶ˆé™¤å¯¹æ—©æœŸåœæ­¢çš„éœ€è¦ã€‚æ­¤å¤–ï¼ŒDSPåœ¨ç†è®ºä¸Šä¿è¯äº†å¹³æ»‘çš„æ”¶æ•›åŠ¨åŠ›å­¦ã€å±€éƒ¨ç¨³å®šæ€§ä»¥åŠæœ‰åˆ©çš„åå·®-æ–¹å·®æƒè¡¡ã€‚å®è¯åˆ†ææ˜¾ç¤ºï¼ŒDSPåœ¨é™å™ªã€ä¿®å¤å’Œè¶…åˆ†è¾¨ç‡ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä¸”ç›¸è¾ƒäºä¼ ç»Ÿçš„DIPå’Œå…¶ä»–æ— ç›‘ç£åŸºå‡†æµ‹è¯•å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DSPæ˜¯DIPçš„æ–°è¡¨è¿°æ–¹å¼ï¼Œå°†å›¾åƒé‡å»ºå®šä¹‰ä¸ºé¢‘åŸŸå¯¹é½é—®é¢˜ã€‚</li>
<li>DSPç›´æ¥åŒ¹é…ç½‘ç»œè¾“å‡ºå’Œè§‚æµ‹å€¼ä¹‹é—´çš„å‚…é‡Œå¶ç³»æ•°ï¼Œå¼•å…¥é¢‘è°±ä¸€è‡´æ€§åè§ã€‚</li>
<li>DSPä½œä¸ºä¸€ç§éšå¼é¢‘è°±æ­£åˆ™åŒ–å™¨ï¼Œèƒ½å¤ŸæŠ‘åˆ¶é«˜é¢‘å™ªå£°ï¼Œæ— éœ€æ—©æœŸåœæ­¢ã€‚</li>
<li>DSPå…·æœ‰ç†è®ºä¿è¯ï¼ŒåŒ…æ‹¬å¹³æ»‘æ”¶æ•›åŠ¨åŠ›å­¦ã€å±€éƒ¨ç¨³å®šæ€§å’Œåå·®-æ–¹å·®æƒè¡¡ã€‚</li>
<li>DSPèƒ½æé«˜å›¾åƒé‡å»ºçš„å¯è§£é‡Šæ€§å’Œç¨³å¥æ€§ï¼Œé€šè¿‡é¢‘ç‡ä¸€è‡´æ€§å°†é‡å»ºæŠ•å½±åˆ°æµå½¢ä¸Šã€‚</li>
<li>DSPåœ¨é™å™ªã€ä¿®å¤å’Œè¶…åˆ†è¾¨ç‡ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºä¼ ç»ŸDIPå’Œå…¶ä»–æ— ç›‘ç£æ–¹æ³•ã€‚</li>
<li>DSPçš„å¼•å…¥ä¸ºå›¾åƒé‡å»ºé—®é¢˜æä¾›äº†æ–°çš„è§†è§’å’Œè§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19873">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-6f32d5b45714403f4748cdcd9951b3b9.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="The-Missing-Point-in-Vision-Transformers-for-Universal-Image-Segmentation"><a href="#The-Missing-Point-in-Vision-Transformers-for-Universal-Image-Segmentation" class="headerlink" title="The Missing Point in Vision Transformers for Universal Image   Segmentation"></a>The Missing Point in Vision Transformers for Universal Image   Segmentation</h2><p><strong>Authors:Sajjad Shahabodini, Mobina Mansoori, Farnoush Bayatmakou, Jamshid Abouei, Konstantinos N. Plataniotis, Arash Mohammadi</strong></p>
<p>Image segmentation remains a challenging task in computer vision, demanding robust mask generation and precise classification. Recent mask-based approaches yield high-quality masks by capturing global context. However, accurately classifying these masks, especially in the presence of ambiguous boundaries and imbalanced class distributions, remains an open challenge. In this work, we introduce ViT-P, a novel two-stage segmentation framework that decouples mask generation from classification. The first stage employs a proposal generator to produce class-agnostic mask proposals, while the second stage utilizes a point-based classification model built on the Vision Transformer (ViT) to refine predictions by focusing on mask central points. ViT-P serves as a pre-training-free adapter, allowing the integration of various pre-trained vision transformers without modifying their architecture, ensuring adaptability to dense prediction tasks. Furthermore, we demonstrate that coarse and bounding box annotations can effectively enhance classification without requiring additional training on fine annotation datasets, reducing annotation costs while maintaining strong performance. Extensive experiments across COCO, ADE20K, and Cityscapes datasets validate the effectiveness of ViT-P, achieving state-of-the-art results with 54.0 PQ on ADE20K panoptic segmentation, 87.4 mIoU on Cityscapes semantic segmentation, and 63.6 mIoU on ADE20K semantic segmentation. The code and pretrained models are available at: <a target="_blank" rel="noopener" href="https://github.com/sajjad-sh33/ViT-P%7D%7Bhttps://github.com/sajjad-sh33/ViT-P">https://github.com/sajjad-sh33/ViT-P}{https://github.com/sajjad-sh33/ViT-P</a>. </p>
<blockquote>
<p>å›¾åƒåˆ†å‰²ä»æ˜¯è®¡ç®—æœºè§†è§‰ä¸­çš„ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œéœ€è¦ç”Ÿæˆç¨³å¥çš„æ©è†œå’Œç²¾ç¡®çš„åˆ†ç±»ã€‚æœ€è¿‘çš„åŸºäºæ©è†œçš„æ–¹æ³•é€šè¿‡æ•æ‰å…¨å±€ä¸Šä¸‹æ–‡æ¥ç”Ÿæˆé«˜è´¨é‡æ©è†œã€‚ç„¶è€Œï¼Œåœ¨è¿™äº›æ©è†œä¸Šè¿›è¡Œå‡†ç¡®åˆ†ç±»ä»ç„¶æ˜¯ä¸€ä¸ªå¼€æ”¾æ€§çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å­˜åœ¨è¾¹ç•Œæ¨¡ç³Šå’Œç±»åˆ«åˆ†å¸ƒä¸å¹³è¡¡çš„æƒ…å†µä¸‹ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ViT-Pï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„ä¸¤é˜¶æ®µåˆ†å‰²æ¡†æ¶ï¼Œå®ƒå°†æ©è†œç”Ÿæˆä¸åˆ†ç±»è§£è€¦ã€‚ç¬¬ä¸€é˜¶æ®µé‡‡ç”¨ææ¡ˆç”Ÿæˆå™¨æ¥äº§ç”Ÿç±»æ— å…³çš„æ©è†œææ¡ˆï¼Œè€Œç¬¬äºŒé˜¶æ®µåˆ™åŸºäºè§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰é‡‡ç”¨ç‚¹åˆ†ç±»æ¨¡å‹ï¼Œé€šè¿‡å…³æ³¨æ©è†œä¸­å¿ƒç‚¹æ¥ä¼˜åŒ–é¢„æµ‹ã€‚ViT-Pä½œä¸ºä¸€ç§æ— éœ€é¢„è®­ç»ƒçš„é€‚é…å™¨ï¼Œå¯ä»¥é›†æˆå„ç§é¢„è®­ç»ƒçš„è§†è§‰è½¬æ¢å™¨è€Œæ— éœ€ä¿®æ”¹å…¶æ¶æ„ï¼Œç¡®ä¿é€‚åº”å¯†é›†é¢„æµ‹ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¯æ˜ç²—æ ‡æ³¨å’Œè¾¹ç•Œæ¡†æ ‡æ³¨å¯ä»¥æœ‰æ•ˆåœ°å¢å¼ºåˆ†ç±»ï¼Œè€Œæ— éœ€åœ¨ç²¾ç»†æ ‡æ³¨æ•°æ®é›†ä¸Šè¿›è¡Œé¢å¤–çš„è®­ç»ƒï¼Œä»è€Œåœ¨é™ä½æ ‡æ³¨æˆæœ¬çš„åŒæ—¶ä¿æŒå¼ºå¤§çš„æ€§èƒ½ã€‚åœ¨COCOã€ADE20Kå’ŒCityscapesæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒéªŒè¯äº†ViT-Pçš„æœ‰æ•ˆæ€§ï¼Œå…¶åœ¨ADE20Kå…¨æ™¯åˆ†å‰²ä¸Šå®ç°äº†54.0 PQçš„æœ€æ–°ç»“æœï¼ŒCityscapesè¯­ä¹‰åˆ†å‰²ä¸Šå®ç°äº†87.4 mIoUï¼ŒADE20Kè¯­ä¹‰åˆ†å‰²ä¸Šå®ç°äº†63.6 mIoUã€‚ä»£ç å’Œé¢„å…ˆè®­ç»ƒçš„æ¨¡å‹å¯åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/sajjad-sh33/ViT-P%E3%80%82">https://github.com/sajjad-sh33/ViT-Pã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19795v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºViT-Pçš„æ–°å‹ä¸¤é˜¶æ®µå›¾åƒåˆ†å‰²æ¡†æ¶ï¼Œå®ƒè§£å†³äº†å›¾åƒåˆ†å‰²ä¸­çš„æ©è†œç”Ÿæˆå’Œåˆ†ç±»æŒ‘æˆ˜ã€‚ç¬¬ä¸€é˜¶æ®µç”Ÿæˆç±»åˆ«æ— å…³çš„æ©è†œææ¡ˆï¼Œç¬¬äºŒé˜¶æ®µä½¿ç”¨åŸºäºVision Transformerï¼ˆViTï¼‰çš„ç‚¹åˆ†ç±»æ¨¡å‹è¿›è¡Œé¢„æµ‹ç²¾ç»†åŒ–ã€‚ViT-Pé€‚ç”¨äºå¤šç§é¢„è®­ç»ƒè§†è§‰å˜å‹å™¨ï¼Œæ— éœ€é¢„è®­ç»ƒï¼Œä¸”èƒ½åˆ©ç”¨ç²—æ ‡æ³¨å’Œè¾¹ç•Œæ¡†æ ‡æ³¨æé«˜åˆ†ç±»æ€§èƒ½ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒViT-Pè¾¾åˆ°äº†å…ˆè¿›æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ViT-Pæ˜¯ä¸€ç§ä¸¤é˜¶æ®µå›¾åƒåˆ†å‰²æ¡†æ¶ï¼Œç”¨äºè§£å†³å›¾åƒåˆ†å‰²ä¸­çš„æ©è†œç”Ÿæˆå’Œåˆ†ç±»æŒ‘æˆ˜ã€‚</li>
<li>ç¬¬ä¸€é˜¶æ®µç”Ÿæˆç±»åˆ«æ— å…³çš„æ©è†œææ¡ˆã€‚</li>
<li>ç¬¬äºŒé˜¶æ®µä½¿ç”¨åŸºäºVision Transformerçš„ç‚¹åˆ†ç±»æ¨¡å‹è¿›è¡Œé¢„æµ‹ç²¾ç»†åŒ–ã€‚</li>
<li>ViT-Pæ— éœ€é¢„è®­ç»ƒï¼Œå¹¶èƒ½é€‚åº”å¤šç§é¢„è®­ç»ƒè§†è§‰å˜å‹å™¨ã€‚</li>
<li>åˆ©ç”¨ç²—æ ‡æ³¨å’Œè¾¹ç•Œæ¡†æ ‡æ³¨èƒ½æé«˜åˆ†ç±»æ€§èƒ½ï¼Œé™ä½æ ‡æ³¨æˆæœ¬ã€‚</li>
<li>ViT-Påœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†å…ˆè¿›æ°´å¹³çš„ç»“æœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19795">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-52d7272cd63f408b69c9f82ecb5bdadc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b398f954148eaef3d0c3aa00e369ed0d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-912a3191a01709501f6829aa2dcc1c17.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Advancements-in-Medical-Image-Classification-through-Fine-Tuning-Natural-Domain-Foundation-Models"><a href="#Advancements-in-Medical-Image-Classification-through-Fine-Tuning-Natural-Domain-Foundation-Models" class="headerlink" title="Advancements in Medical Image Classification through Fine-Tuning Natural   Domain Foundation Models"></a>Advancements in Medical Image Classification through Fine-Tuning Natural   Domain Foundation Models</h2><p><strong>Authors:Mobina Mansoori, Sajjad Shahabodini, Farnoush Bayatmakou, Jamshid Abouei, Konstantinos N. Plataniotis, Arash Mohammadi</strong></p>
<p>Using massive datasets, foundation models are large-scale, pre-trained models that perform a wide range of tasks. These models have shown consistently improved results with the introduction of new methods. It is crucial to analyze how these trends impact the medical field and determine whether these advancements can drive meaningful change. This study investigates the application of recent state-of-the-art foundation models, DINOv2, MAE, VMamba, CoCa, SAM2, and AIMv2, for medical image classification. We explore their effectiveness on datasets including CBIS-DDSM for mammography, ISIC2019 for skin lesions, APTOS2019 for diabetic retinopathy, and CHEXPERT for chest radiographs. By fine-tuning these models and evaluating their configurations, we aim to understand the potential of these advancements in medical image classification. The results indicate that these advanced models significantly enhance classification outcomes, demonstrating robust performance despite limited labeled data. Based on our results, AIMv2, DINOv2, and SAM2 models outperformed others, demonstrating that progress in natural domain training has positively impacted the medical domain and improved classification outcomes. Our code is publicly available at: <a target="_blank" rel="noopener" href="https://github.com/sajjad-sh33/Medical-Transfer-Learning">https://github.com/sajjad-sh33/Medical-Transfer-Learning</a>. </p>
<blockquote>
<p>åˆ©ç”¨å¤§è§„æ¨¡æ•°æ®é›†ï¼ŒåŸºç¡€æ¨¡å‹æ˜¯å¤§å‹é¢„è®­ç»ƒæ¨¡å‹ï¼Œå¯ä»¥æ‰§è¡Œå„ç§ä»»åŠ¡ã€‚éšç€æ–°æ–¹æ³•çš„å¼•å…¥ï¼Œè¿™äº›æ¨¡å‹çš„ç»“æœæŒç»­å¾—åˆ°æ”¹è¿›ã€‚åˆ†æè¿™äº›è¶‹åŠ¿å¦‚ä½•å½±å“åŒ»ç–—é¢†åŸŸï¼Œä»¥åŠè¿™äº›è¿›æ­¥æ˜¯å¦èƒ½å¸¦æ¥æœ‰æ„ä¹‰çš„å˜åŒ–è‡³å…³é‡è¦ã€‚æœ¬ç ”ç©¶è°ƒæŸ¥äº†æœ€æ–°æœ€å…ˆè¿›çš„åŸºç¡€æ¨¡å‹ï¼ˆDINOv2ã€MAEã€VMambaã€CoCaã€SAM2å’ŒAIMv2ï¼‰åœ¨åŒ»å­¦å›¾åƒåˆ†ç±»ä¸­çš„åº”ç”¨ã€‚æˆ‘ä»¬åœ¨CBIS-DDSMä¹³è…ºç™Œç­›æŸ¥æ•°æ®é›†ã€ISIC2019çš®è‚¤ç—…å˜æ•°æ®é›†ã€APTOS2019ç³–å°¿ç—…è§†ç½‘è†œç—…å˜æ•°æ®é›†å’ŒCHEXPERTèƒ¸éƒ¨Xå°„çº¿æ•°æ®é›†ä¸Šæ¢ç´¢äº†å®ƒä»¬çš„æœ‰æ•ˆæ€§ã€‚é€šè¿‡å¾®è°ƒè¿™äº›æ¨¡å‹å¹¶è¯„ä¼°å…¶é…ç½®ï¼Œæˆ‘ä»¬æ—¨åœ¨äº†è§£è¿™äº›è¿›æ­¥åœ¨åŒ»å­¦å›¾åƒåˆ†ç±»ä¸­çš„æ½œåŠ›ã€‚ç»“æœè¡¨æ˜ï¼Œè¿™äº›å…ˆè¿›æ¨¡å‹æ˜¾è‘—æé«˜äº†åˆ†ç±»æ•ˆæœï¼Œåœ¨æœ‰é™æ ‡è®°æ•°æ®çš„æƒ…å†µä¸‹è¡¨ç°å‡ºç¨³å¥çš„æ€§èƒ½ã€‚æ ¹æ®æˆ‘ä»¬çš„ç»“æœï¼ŒAIMv2ã€DINOv2å’ŒSAM2æ¨¡å‹è¡¨ç°æœ€ä½³ï¼Œè¿™è¡¨æ˜è‡ªç„¶åŸŸè®­ç»ƒçš„è¿›å±•å¯¹åŒ»å­¦é¢†åŸŸäº§ç”Ÿäº†ç§¯æå½±å“ï¼Œæé«˜äº†åˆ†ç±»æ•ˆæœã€‚æˆ‘ä»¬çš„ä»£ç å…¬å¼€åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/sajjad-sh33/Medical-Transfer-Learning%E3%80%82">https://github.com/sajjad-sh33/Medical-Transfer-Learningã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19779v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥ç ”ç©¶è°ƒæŸ¥äº†æœ€æ–°å…ˆè¿›çš„åŸºé‡‘ä¼šæ¨¡å‹åœ¨åŒ»å­¦å›¾åƒåˆ†ç±»ä¸­çš„åº”ç”¨ï¼ŒåŒ…æ‹¬DINOv2ã€MAEã€VMambaã€CoCaã€SAM2å’ŒAIMv2ç­‰ã€‚åœ¨åŒ»å­¦å›¾åƒåˆ†ç±»æ•°æ®é›†ä¸Šè¿›è¡Œç²¾ç»†è°ƒæ•´å¹¶è¯„ä¼°é…ç½®åï¼Œè¿™äº›æ¨¡å‹å±•ç°å‡ºæ˜¾è‘—çš„åˆ†ç±»æ•ˆæœï¼Œç‰¹åˆ«æ˜¯åœ¨æœ‰é™æ ‡è®°æ•°æ®çš„æƒ…å†µä¸‹ã€‚AIMv2ã€DINOv2å’ŒSAM2è¡¨ç°æœ€ä½³ï¼Œè¡¨æ˜è‡ªç„¶åŸŸè®­ç»ƒçš„è¿›å±•å¯¹åŒ»å­¦é¢†åŸŸäº§ç”Ÿäº†ç§¯æå½±å“ï¼Œå¹¶æé«˜äº†åˆ†ç±»ç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŸºé‡‘ä¼šæ¨¡å‹æ˜¯ä½¿ç”¨å¤§è§„æ¨¡æ•°æ®é›†è¿›è¡Œé¢„è®­ç»ƒçš„å¤§å‹æ¨¡å‹ï¼Œèƒ½å¤Ÿæ‰§è¡Œå„ç§ä»»åŠ¡ï¼Œå¹¶éšç€æ–°æ–¹æ³•çš„å¼•å…¥è€ŒæŒç»­æé«˜ç»“æœã€‚</li>
<li>è¯¥ç ”ç©¶è°ƒæŸ¥äº†æœ€æ–°å…ˆè¿›çš„åŸºé‡‘ä¼šæ¨¡å‹åœ¨åŒ»å­¦å›¾åƒåˆ†ç±»ä¸­çš„åº”ç”¨ï¼Œæ¶‰åŠå¤šä¸ªæ•°æ®é›†ï¼ŒåŒ…æ‹¬CBIS-DDSMã€ISIC2019ã€APTOS2019å’ŒCHEXPERTã€‚</li>
<li>è¿™äº›æ¨¡å‹åœ¨åŒ»å­¦å›¾åƒåˆ†ç±»æ–¹é¢å±•ç°å‡ºæ˜¾è‘—çš„æ•ˆæœï¼Œç‰¹åˆ«æ˜¯åœ¨æœ‰é™æ ‡è®°æ•°æ®çš„æƒ…å†µä¸‹ã€‚</li>
<li>AIMv2ã€DINOv2å’ŒSAM2ç­‰æ¨¡å‹è¡¨ç°æœ€ä½³ï¼Œè¡¨æ˜å®ƒä»¬åœ¨åŒ»å­¦å›¾åƒåˆ†ç±»æ–¹é¢å…·æœ‰æ›´é«˜çš„æ½œåŠ›ã€‚</li>
<li>ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè‡ªç„¶åŸŸè®­ç»ƒçš„è¿›å±•å¯¹åŒ»å­¦é¢†åŸŸäº§ç”Ÿäº†ç§¯æå½±å“ï¼Œå¹¶æé«˜äº†åˆ†ç±»ç»“æœã€‚</li>
<li>å…¬å¼€å¯ç”¨çš„ä»£ç ä¸ºåŒ»å­¦è¿ç§»å­¦ä¹ æä¾›äº†æœ‰ç”¨çš„èµ„æºå’Œå‚è€ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19779">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-c4849f959470ec1b3b7decb23f4f5450.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9fa542dfa7294bb755eab96249863694.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-563e20569e5e487f315ad744f2396627.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-04c93c9de2139324ab0b85c2f249358c.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Cross-Sequence-Semi-Supervised-Learning-for-Multi-Parametric-MRI-Based-Visual-Pathway-Delineation"><a href="#Cross-Sequence-Semi-Supervised-Learning-for-Multi-Parametric-MRI-Based-Visual-Pathway-Delineation" class="headerlink" title="Cross-Sequence Semi-Supervised Learning for Multi-Parametric MRI-Based   Visual Pathway Delineation"></a>Cross-Sequence Semi-Supervised Learning for Multi-Parametric MRI-Based   Visual Pathway Delineation</h2><p><strong>Authors:Alou Diakite, Cheng Li, Lei Xie, Yuanjing Feng, Ruoyou Wu, Jianzhong He, Hairong Zheng, Shanshan Wang</strong></p>
<p>Accurately delineating the visual pathway (VP) is crucial for understanding the human visual system and diagnosing related disorders. Exploring multi-parametric MR imaging data has been identified as an important way to delineate VP. However, due to the complex cross-sequence relationships, existing methods cannot effectively model the complementary information from different MRI sequences. In addition, these existing methods heavily rely on large training data with labels, which is labor-intensive and time-consuming to obtain. In this work, we propose a novel semi-supervised multi-parametric feature decomposition framework for VP delineation. Specifically, a correlation-constrained feature decomposition (CFD) is designed to handle the complex cross-sequence relationships by capturing the unique characteristics of each MRI sequence and easing the multi-parametric information fusion process. Furthermore, a consistency-based sample enhancement (CSE) module is developed to address the limited labeled data issue, by generating and promoting meaningful edge information from unlabeled data. We validate our framework using two public datasets, and one in-house Multi-Shell Diffusion MRI (MDM) dataset. Experimental results demonstrate the superiority of our approach in terms of delineation performance when compared to seven state-of-the-art approaches. </p>
<blockquote>
<p>å‡†ç¡®åœ°æç»˜è§†è§‰é€šè·¯ï¼ˆVPï¼‰å¯¹äºç†è§£äººç±»è§†è§‰ç³»ç»Ÿå’Œè¯Šæ–­ç›¸å…³ç–¾ç—…è‡³å…³é‡è¦ã€‚æ¢ç´¢å¤šå‚æ•°MRæˆåƒæ•°æ®å·²è¢«ç¡®å®šä¸ºæç»˜VPçš„é‡è¦æ–¹å¼ã€‚ç„¶è€Œï¼Œç”±äºå¤æ‚çš„è·¨åºåˆ—å…³ç³»ï¼Œç°æœ‰æ–¹æ³•æ— æ³•æœ‰æ•ˆåœ°å¯¹æ¥è‡ªä¸åŒMRIåºåˆ—çš„äº’è¡¥ä¿¡æ¯è¿›è¡Œå»ºæ¨¡ã€‚æ­¤å¤–ï¼Œè¿™äº›æ–¹æ³•ä¸¥é‡ä¾èµ–äºå¸¦æœ‰æ ‡ç­¾çš„å¤§é‡è®­ç»ƒæ•°æ®ï¼Œè€Œè¿™äº›æ•°æ®çš„è·å–æ—¢è€—è´¹äººåŠ›åˆè€—æ—¶ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„åŠç›‘ç£å¤šå‚æ•°ç‰¹å¾åˆ†è§£æ¡†æ¶ï¼Œç”¨äºVPæç»˜ã€‚å…·ä½“æ¥è¯´ï¼Œè®¾è®¡äº†ä¸€ç§ç›¸å…³æ€§çº¦æŸç‰¹å¾åˆ†è§£ï¼ˆCFDï¼‰ï¼Œé€šè¿‡æ•æ‰æ¯ä¸ªMRIåºåˆ—çš„ç‹¬ç‰¹ç‰¹å¾æ¥å¤„ç†å¤æ‚çš„è·¨åºåˆ—å…³ç³»ï¼Œå¹¶ç®€åŒ–å¤šå‚æ•°ä¿¡æ¯èåˆè¿‡ç¨‹ã€‚æ­¤å¤–ï¼Œå¼€å‘äº†ä¸€ä¸ªåŸºäºä¸€è‡´æ€§çš„æ ·æœ¬å¢å¼ºï¼ˆCSEï¼‰æ¨¡å—ï¼Œä»¥è§£å†³æ ‡ç­¾æ•°æ®æœ‰é™çš„é—®é¢˜ï¼Œé€šè¿‡ä»æœªæ ‡è®°çš„æ•°æ®ç”Ÿæˆå¹¶ä¿ƒè¿›æœ‰æ„ä¹‰çš„è¾¹ç¼˜ä¿¡æ¯ã€‚æˆ‘ä»¬ä½¿ç”¨ä¸¤ä¸ªå…¬å…±æ•°æ®é›†å’Œä¸€ä¸ªå†…éƒ¨çš„å¤šå£³æ‰©æ•£MRIï¼ˆMDMï¼‰æ•°æ®é›†éªŒè¯äº†æˆ‘ä»¬çš„æ¡†æ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ä¸ƒç§æœ€æ–°æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æç»˜æ€§èƒ½æ–¹é¢å…·æœ‰ä¼˜åŠ¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19733v1">PDF</a> </p>
<p><strong>Summary</strong><br>åŒ»å­¦è§†è§‰è·¯å¾„ï¼ˆVPï¼‰çš„ç²¾ç¡®æç»˜å¯¹äºç†è§£äººç±»è§†è§‰ç³»ç»Ÿå’Œè¯Šæ–­ç›¸å…³ç–¾ç—…è‡³å…³é‡è¦ã€‚å¤šå‚æ•°MRæˆåƒæ•°æ®æ˜¯æç»˜VPçš„é‡è¦æ–¹æ³•ï¼Œä½†ç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆå»ºæ¨¡ä¸åŒMRIåºåˆ—çš„äº’è¡¥ä¿¡æ¯ã€‚æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹åŠç›‘ç£å¤šå‚æ•°ç‰¹å¾åˆ†è§£æ¡†æ¶ï¼ŒåŒ…æ‹¬ç”¨äºå¤„ç†å¤æ‚è·¨åºåˆ—å…³ç³»çš„å…³è”çº¦æŸç‰¹å¾åˆ†è§£ï¼ˆCFDï¼‰ï¼Œå¹¶å¼€å‘ä¸€è‡´æ€§æ ·æœ¬å¢å¼ºï¼ˆCSEï¼‰æ¨¡å—è§£å†³æ ‡æ³¨æ•°æ®æœ‰é™é—®é¢˜ã€‚å®éªŒç»“æœè¯æ˜è¯¥æ–¹æ³•åœ¨å¯¹æ¯”ä¸ƒä¸ªæœ€æ–°æ–¹æ³•æ—¶å…·æœ‰ä¼˜è¶Šçš„æè¿°æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å‡†ç¡®æç»˜è§†è§‰è·¯å¾„ï¼ˆVPï¼‰å¯¹ç†è§£äººç±»è§†è§‰ç³»ç»Ÿå’Œè¯Šæ–­ç›¸å…³ç–¾ç—…è‡³å…³é‡è¦ã€‚</li>
<li>å¤šå‚æ•°MRæˆåƒæ•°æ®æ˜¯æç»˜VPçš„é‡è¦æ–¹æ³•ã€‚</li>
<li>ç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆå»ºæ¨¡ä¸åŒMRIåºåˆ—çš„äº’è¡¥ä¿¡æ¯ï¼Œä¸”éœ€è¦å¤§é‡æ ‡æ³¨æ•°æ®è¿›è¡Œè®­ç»ƒã€‚</li>
<li>æå‡ºä¸€ç§æ–°å‹åŠç›‘ç£å¤šå‚æ•°ç‰¹å¾åˆ†è§£æ¡†æ¶ï¼ŒåŒ…æ‹¬å…³è”çº¦æŸç‰¹å¾åˆ†è§£ï¼ˆCFDï¼‰å’Œä¸€è‡´æ€§æ ·æœ¬å¢å¼ºï¼ˆCSEï¼‰æ¨¡å—ã€‚</li>
<li>CFDèƒ½å¤Ÿæ•æ‰æ¯ä¸ªMRIåºåˆ—çš„ç‹¬ç‰¹ç‰¹å¾ï¼Œå¹¶ç®€åŒ–å¤šå‚æ•°ä¿¡æ¯èåˆè¿‡ç¨‹ã€‚</li>
<li>CSEæ¨¡å—é€šè¿‡ä»æ— æ ‡ç­¾æ•°æ®ä¸­ç”Ÿæˆå¹¶æ¨å¹¿è¾¹ç¼˜ä¿¡æ¯ï¼Œè§£å†³äº†æ ‡æ³¨æ•°æ®æœ‰é™çš„é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19733">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8b9e7174c0947225d60d01482c2939b2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1c0a5b1c3343a911e388b2aa9e5814dd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-90fdff78a4ef9edbfd154101a1f916d1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c2dbd05bc99a0bee1ad7777811a07ad4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6b3bd683fcbdf65c05a505108c1fdd23.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="CAD-Coder-Text-to-CAD-Generation-with-Chain-of-Thought-and-Geometric-Reward"><a href="#CAD-Coder-Text-to-CAD-Generation-with-Chain-of-Thought-and-Geometric-Reward" class="headerlink" title="CAD-Coder: Text-to-CAD Generation with Chain-of-Thought and Geometric   Reward"></a>CAD-Coder: Text-to-CAD Generation with Chain-of-Thought and Geometric   Reward</h2><p><strong>Authors:Yandong Guan, Xilin Wang, Xingxi Ming, Jing Zhang, Dong Xu, Qian Yu</strong></p>
<p>In this work, we introduce CAD-Coder, a novel framework that reformulates text-to-CAD as the generation of CadQuery scripts - a Python-based, parametric CAD language. This representation enables direct geometric validation, a richer modeling vocabulary, and seamless integration with existing LLMs. To further enhance code validity and geometric fidelity, we propose a two-stage learning pipeline: (1) supervised fine-tuning on paired text-CadQuery data, and (2) reinforcement learning with Group Reward Policy Optimization (GRPO), guided by a CAD-specific reward comprising both a geometric reward (Chamfer Distance) and a format reward. We also introduce a chain-of-thought (CoT) planning process to improve model reasoning, and construct a large-scale, high-quality dataset of 110K text-CadQuery-3D model triplets and 1.5K CoT samples via an automated pipeline. Extensive experiments demonstrate that CAD-Coder enables LLMs to generate diverse, valid, and complex CAD models directly from natural language, advancing the state of the art of text-to-CAD generation and geometric reasoning. </p>
<blockquote>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†CAD-Coderè¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œå®ƒå°†æ–‡æœ¬åˆ°CADï¼ˆè®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼‰çš„é—®é¢˜é‡æ–°å®šä¹‰ä¸ºCadQueryè„šæœ¬çš„ç”Ÿæˆé—®é¢˜ã€‚CadQueryæ˜¯ä¸€ç§åŸºäºPythonçš„å‚æ•°åŒ–CADè¯­è¨€ã€‚è¿™ç§è¡¨ç¤ºæ–¹æ³•èƒ½å¤Ÿå®ç°ç›´æ¥çš„å‡ ä½•éªŒè¯ã€æ›´ä¸°å¯Œçš„å»ºæ¨¡è¯æ±‡å’Œæ— ç¼é›†æˆç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜ä»£ç çš„æœ‰æ•ˆæ€§å’Œå‡ ä½•ä¿çœŸåº¦ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªä¸¤é˜¶æ®µçš„å­¦ä¹ æµç¨‹ï¼ŒåŒ…æ‹¬ï¼šï¼ˆ1ï¼‰åœ¨é…å¯¹æ–‡æœ¬-CadQueryæ•°æ®ä¸Šè¿›è¡Œç›‘ç£å¾®è°ƒï¼›ï¼ˆ2ï¼‰é‡‡ç”¨é›†å›¢å¥–åŠ±æ”¿ç­–ä¼˜åŒ–ï¼ˆGRPOï¼‰è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼Œä»¥CADç‰¹å®šå¥–åŠ±ä¸ºæŒ‡å¯¼ï¼Œè¯¥å¥–åŠ±åŒ…æ‹¬å‡ ä½•å¥–åŠ±ï¼ˆChamferè·ç¦»ï¼‰å’Œæ ¼å¼å¥–åŠ±ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†æ€ç»´é“¾ï¼ˆCoTï¼‰è§„åˆ’è¿‡ç¨‹æ¥æ”¹å–„æ¨¡å‹æ¨ç†èƒ½åŠ›ï¼Œå¹¶é€šè¿‡è‡ªåŠ¨åŒ–æµç¨‹æ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡ã€é«˜è´¨é‡çš„åŒ…å«11ä¸‡ä»½æ–‡æœ¬-CadQuery-3Dæ¨¡å‹ä¸‰å…ƒç»„å’Œ1500ä»½CoTæ ·æœ¬çš„æ•°æ®é›†ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒCAD-Coderä½¿å¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿç›´æ¥ä»è‡ªç„¶è¯­è¨€ç”Ÿæˆå¤šæ ·ã€æœ‰æ•ˆå’Œå¤æ‚çš„CADæ¨¡å‹ï¼Œæ¨åŠ¨äº†æ–‡æœ¬åˆ°CADç”Ÿæˆå’Œå‡ ä½•æ¨ç†çš„æœ€æ–°æŠ€æœ¯è¿›å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19713v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>CAD-Coderæ¡†æ¶èƒ½å°†æ–‡æœ¬è½¬åŒ–ä¸ºCADè®¾è®¡ï¼Œé€šè¿‡ç”ŸæˆCadQueryè„šæœ¬å®ç°ã€‚è¯¥æ¡†æ¶æ”¯æŒç›´æ¥å‡ ä½•éªŒè¯ã€ä¸°å¯Œçš„å»ºæ¨¡è¯æ±‡ï¼Œå¹¶èƒ½æ— ç¼é›†æˆç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚ä¸ºæé«˜ä»£ç çš„æœ‰æ•ˆæ€§å’Œå‡ ä½•ç²¾åº¦ï¼Œç ”ç©¶æå‡ºäº†åŒ…å«ç›‘ç£å¾®è°ƒä¸å¼ºåŒ–å­¦ä¹ çš„ä¸¤é˜¶æ®µå­¦ä¹ ç®¡é“ï¼Œå¹¶å¼•å…¥äº†CADç‰¹å®šå¥–åŠ±ç­–ç•¥ã€‚åŒæ—¶ï¼Œé€šè¿‡æ€è€ƒé“¾è§„åˆ’è¿‡ç¨‹æé«˜æ¨¡å‹æ¨ç†èƒ½åŠ›ï¼Œå¹¶æ„å»ºå¤§è§„æ¨¡é«˜è´¨é‡æ•°æ®é›†ã€‚å®éªŒè¡¨æ˜ï¼ŒCAD-Coderèƒ½ä¿ƒä½¿è¯­è¨€æ¨¡å‹ç›´æ¥ä»è‡ªç„¶è¯­è¨€ç”Ÿæˆå¤šæ ·ã€æœ‰æ•ˆä¸”å¤æ‚çš„CADæ¨¡å‹ï¼Œæ¨åŠ¨æ–‡æœ¬åˆ°CADç”Ÿæˆå’Œå‡ ä½•æ¨ç†é¢†åŸŸçš„å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CAD-Coderæ˜¯ä¸€ä¸ªå°†æ–‡æœ¬è½¬åŒ–ä¸ºCADè®¾è®¡çš„å…¨æ–°æ¡†æ¶ï¼Œé€šè¿‡ç”ŸæˆCadQueryè„šæœ¬å®ç°ã€‚</li>
<li>è¯¥æ¡†æ¶æ”¯æŒç›´æ¥å‡ ä½•éªŒè¯ã€ä¸°å¯Œçš„å»ºæ¨¡è¯æ±‡ï¼Œå¹¶èƒ½æ— ç¼é›†æˆç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚</li>
<li>ç ”ç©¶æå‡ºä¸¤é˜¶æ®µå­¦ä¹ ç®¡é“ï¼ŒåŒ…æ‹¬ç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ ï¼Œä»¥æé«˜ä»£ç çš„æœ‰æ•ˆæ€§å’Œå‡ ä½•ç²¾åº¦ã€‚</li>
<li>å¼•å…¥äº†CADç‰¹å®šå¥–åŠ±ç­–ç•¥ï¼Œç»“åˆå‡ ä½•å¥–åŠ±å’Œæ ¼å¼å¥–åŠ±ã€‚</li>
<li>é€šè¿‡æ€è€ƒé“¾è§„åˆ’è¿‡ç¨‹æé«˜æ¨¡å‹æ¨ç†èƒ½åŠ›ã€‚</li>
<li>æ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡ã€é«˜è´¨é‡çš„æ•°æ®é›†ï¼ŒåŒ…å«110Kæ–‡æœ¬-CadQuery-3Dæ¨¡å‹ä¸‰å…ƒç»„å’Œ1.5Kæ€è€ƒé“¾æ ·æœ¬ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19713">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ee2e8429815181010302285a44073724.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3f227d4738b014e8188273432dfbe8a8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f8b6fc049ecf570cbd11a087a8e656d4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1cb506603d9fdb4d883c01ece9acd88f.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="LangDAug-Langevin-Data-Augmentation-for-Multi-Source-Domain-Generalization-in-Medical-Image-Segmentation"><a href="#LangDAug-Langevin-Data-Augmentation-for-Multi-Source-Domain-Generalization-in-Medical-Image-Segmentation" class="headerlink" title="LangDAug: Langevin Data Augmentation for Multi-Source Domain   Generalization in Medical Image Segmentation"></a>LangDAug: Langevin Data Augmentation for Multi-Source Domain   Generalization in Medical Image Segmentation</h2><p><strong>Authors:Piyush Tiwary, Kinjawl Bhattacharyya, Prathosh A. P</strong></p>
<p>Medical image segmentation models often struggle to generalize across different domains due to various reasons. Domain Generalization (DG) methods overcome this either through representation learning or data augmentation (DAug). While representation learning methods seek domain-invariant features, they often rely on ad-hoc techniques and lack formal guarantees. DAug methods, which enrich model representations through synthetic samples, have shown comparable or superior performance to representation learning approaches. We propose LangDAug, a novel $\textbf{Lang}$evin $\textbf{D}$ata $\textbf{Aug}$mentation for multi-source domain generalization in 2D medical image segmentation. LangDAug leverages Energy-Based Models (EBMs) trained via contrastive divergence to traverse between source domains, generating intermediate samples through Langevin dynamics. Theoretical analysis shows that LangDAug induces a regularization effect, and for GLMs, it upper-bounds the Rademacher complexity by the intrinsic dimensionality of the data manifold. Through extensive experiments on Fundus segmentation and 2D MRI prostate segmentation benchmarks, we show that LangDAug outperforms state-of-the-art domain generalization methods and effectively complements existing domain-randomization approaches. The codebase for our method is available at <a target="_blank" rel="noopener" href="https://github.com/backpropagator/LangDAug">https://github.com/backpropagator/LangDAug</a>. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒåˆ†å‰²æ¨¡å‹ç”±äºå„ç§åŸå› ï¼Œå¾€å¾€éš¾ä»¥åœ¨ä¸åŒé¢†åŸŸè¿›è¡Œæ¨å¹¿ã€‚é¢†åŸŸæ³›åŒ–ï¼ˆDGï¼‰æ–¹æ³•é€šè¿‡è¡¨ç¤ºå­¦ä¹ æˆ–æ•°æ®å¢å¼ºï¼ˆDAugï¼‰æ¥å…‹æœè¿™ä¸€é—®é¢˜ã€‚è¡¨ç¤ºå­¦ä¹ æ–¹æ³•è™½ç„¶å¯»æ‰¾é¢†åŸŸä¸å˜çš„ç‰¹å¾ï¼Œä½†å®ƒä»¬å¾€å¾€ä¾èµ–äºç‰¹æ®ŠæŠ€å·§ï¼Œç¼ºä¹æ­£å¼ä¿è¯ã€‚æ•°æ®å¢å¼ºæ–¹æ³•é€šè¿‡åˆæˆæ ·æœ¬ä¸°å¯Œæ¨¡å‹è¡¨ç¤ºï¼Œå…¶æ€§èƒ½ä¸è¡¨ç¤ºå­¦ä¹ æ–¹æ³•ç›¸å½“æˆ–æ›´ä¼˜ã€‚æˆ‘ä»¬æå‡ºäº†LangDAugï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„é’ˆå¯¹äºŒç»´åŒ»å­¦å›¾åƒåˆ†å‰²çš„å¤šæºåŸŸæ³›åŒ–æœ—ç»´æ•°æ®å¢å¼ºæ–¹æ³•ã€‚LangDAugåˆ©ç”¨é€šè¿‡å¯¹æ¯”æ•£åº¦è®­ç»ƒçš„èƒ½é‡æ¨¡å‹ï¼ˆEBMsï¼‰ï¼Œåœ¨æºåŸŸä¹‹é—´è¿›è¡Œéå†ï¼Œé€šè¿‡æœ—ç»´åŠ¨åŠ›å­¦ç”Ÿæˆä¸­é—´æ ·æœ¬ã€‚ç†è®ºåˆ†æè¡¨æ˜ï¼ŒLangDAugäº§ç”Ÿæ­£åˆ™åŒ–æ•ˆåº”ï¼Œå¯¹äºå¹¿ä¹‰çº¿æ€§æ¨¡å‹ï¼ˆGLMsï¼‰ï¼Œå®ƒé€šè¿‡æ•°æ®æµå½¢çš„å›ºæœ‰ç»´åº¦ä¸Šé™Rademacherå¤æ‚æ€§ã€‚åœ¨çœ¼åº•åˆ†å‰²å’ŒäºŒç»´MRIå‰åˆ—è…ºåˆ†å‰²åŸºå‡†æµ‹è¯•çš„å¤§é‡å®éªŒä¸­ï¼Œæˆ‘ä»¬è¯æ˜äº†LangDAugä¼˜äºæœ€æ–°çš„é¢†åŸŸæ³›åŒ–æ–¹æ³•ï¼Œå¹¶èƒ½æœ‰æ•ˆåœ°è¡¥å……ç°æœ‰çš„é¢†åŸŸéšæœºåŒ–æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„ä»£ç åº“å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/backpropagator/LangDAug%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/backpropagator/LangDAugæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19659v1">PDF</a> Accepted at ICML 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨åŒ»å­¦å›¾åƒåˆ†å‰²æ¨¡å‹åœ¨ä¸åŒé¢†åŸŸä¸­çš„æ³›åŒ–é—®é¢˜ï¼Œæå‡ºä¸€ç§åä¸ºLangDAugçš„æ–°å‹æ•°æ®å¢å¼ºæ–¹æ³•ï¼Œç”¨äºå¤šæºé¢†åŸŸæ³›åŒ–ã€‚è¯¥æ–¹æ³•åŸºäºèƒ½é‡åŸºæ¨¡å‹ï¼ˆEBMsï¼‰å’ŒLangevinåŠ¨æ€ï¼Œé€šè¿‡å¯¹æ¯”åˆ†æ­§åœ¨æºé¢†åŸŸä¹‹é—´éå†ï¼Œç”Ÿæˆä¸­é—´æ ·æœ¬ã€‚ç†è®ºåˆ†ææ˜¾ç¤ºï¼ŒLangDAugå…·æœ‰æ­£åˆ™åŒ–æ•ˆæœï¼Œä¸”å¯¹äºå¹¿ä¹‰çº¿æ€§æ¨¡å‹ï¼ˆGLMsï¼‰ï¼Œå®ƒèƒ½é€šè¿‡æ•°æ®æµçš„å†…åœ¨ç»´åº¦ä¸Šé™Rademacherå¤æ‚æ€§ã€‚åœ¨åŸºé‡‘åˆ†å‰²å’ŒäºŒç»´ç£å…±æŒ¯æˆåƒå‰åˆ—è…ºåˆ†å‰²åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒLangDAugä¼˜äºæœ€æ–°çš„é¢†åŸŸæ³›åŒ–æ–¹æ³•ï¼Œå¹¶èƒ½æœ‰æ•ˆåœ°è¡¥å……ç°æœ‰çš„é¢†åŸŸéšæœºåŒ–æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†å‰²æ¨¡å‹åœ¨ä¸åŒé¢†åŸŸçš„æ³›åŒ–æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚</li>
<li>Domain Generalization (DG)æ–¹æ³•é€šè¿‡è¡¨ç¤ºå­¦ä¹ æˆ–æ•°æ®å¢å¼ºï¼ˆDAugï¼‰æ¥å…‹æœè¿™ä¸€é—®é¢˜ã€‚</li>
<li>LangDAugæ˜¯ä¸€ç§æ–°å‹çš„æ•°æ®å¢å¼ºæ–¹æ³•ï¼Œç”¨äºå¤šæºé¢†åŸŸæ³›åŒ–åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­ã€‚</li>
<li>LangDAugåˆ©ç”¨èƒ½é‡åŸºæ¨¡å‹ï¼ˆEBMsï¼‰å’ŒLangevinåŠ¨æ€ç”Ÿæˆä¸­é—´æ ·æœ¬ã€‚</li>
<li>ç†è®ºåˆ†ææ˜¾ç¤ºLangDAugå…·æœ‰æ­£åˆ™åŒ–æ•ˆæœï¼Œå¹¶èƒ½é™åˆ¶Rademacherå¤æ‚æ€§ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒLangDAugåœ¨åŸºé‡‘åˆ†å‰²å’ŒäºŒç»´ç£å…±æŒ¯æˆåƒå‰åˆ—è…ºåˆ†å‰²åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19659">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-413aae7333a1e8c92f715058925bdd33.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-db985de7c5ce54e24ea90ca23c75efd2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-290d077619bde34b121dce7de959472b.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Rep3D-Re-parameterize-Large-3D-Kernels-with-Low-Rank-Receptive-Modeling-for-Medical-Imaging"><a href="#Rep3D-Re-parameterize-Large-3D-Kernels-with-Low-Rank-Receptive-Modeling-for-Medical-Imaging" class="headerlink" title="Rep3D: Re-parameterize Large 3D Kernels with Low-Rank Receptive Modeling   for Medical Imaging"></a>Rep3D: Re-parameterize Large 3D Kernels with Low-Rank Receptive Modeling   for Medical Imaging</h2><p><strong>Authors:Ho Hin Lee, Quan Liu, Shunxing Bao, Yuankai Huo, Bennett A. Landman</strong></p>
<p>In contrast to vision transformers, which model long-range dependencies through global self-attention, large kernel convolutions provide a more efficient and scalable alternative, particularly in high-resolution 3D volumetric settings. However, naively increasing kernel size often leads to optimization instability and degradation in performance. Motivated by the spatial bias observed in effective receptive fields (ERFs), we hypothesize that different kernel elements converge at variable rates during training. To support this, we derive a theoretical connection between element-wise gradients and first-order optimization, showing that structurally re-parameterized convolution blocks inherently induce spatially varying learning rates. Building on this insight, we introduce Rep3D, a 3D convolutional framework that incorporates a learnable spatial prior into large kernel training. A lightweight two-stage modulation network generates a receptive-biased scaling mask, adaptively re-weighting kernel updates and enabling local-to-global convergence behavior. Rep3D adopts a plain encoder design with large depthwise convolutions, avoiding the architectural complexity of multi-branch compositions. We evaluate Rep3D on five challenging 3D segmentation benchmarks and demonstrate consistent improvements over state-of-the-art baselines, including transformer-based and fixed-prior re-parameterization methods. By unifying spatial inductive bias with optimization-aware learning, Rep3D offers an interpretable, and scalable solution for 3D medical image analysis. The source code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/leeh43/Rep3D">https://github.com/leeh43/Rep3D</a>. </p>
<blockquote>
<p>ä¸é€šè¿‡å…¨å±€è‡ªæ³¨æ„åŠ›æœºåˆ¶å»ºæ¨¡é•¿ç¨‹ä¾èµ–å…³ç³»çš„è§†è§‰å˜å‹å™¨ç›¸æ¯”ï¼Œå¤§å†…æ ¸å·ç§¯æä¾›äº†ä¸€ç§æ›´é«˜æ•ˆä¸”å¯æ‰©å±•çš„æ›¿ä»£æ–¹æ¡ˆï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜åˆ†è¾¨ç‡çš„3Dä½“ç§¯ç¯å¢ƒä¸­ã€‚ç„¶è€Œï¼Œç›²ç›®åœ°å¢åŠ å†…æ ¸å¤§å°å¾€å¾€ä¼šå¯¼è‡´ä¼˜åŒ–ä¸ç¨³å®šå’Œæ€§èƒ½ä¸‹é™ã€‚å—æœ‰æ•ˆæ„Ÿå—é‡ï¼ˆERFsï¼‰ä¸­è§‚å¯Ÿåˆ°çš„ç©ºé—´åç½®çš„å¯å‘ï¼Œæˆ‘ä»¬å‡è®¾åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä¸åŒçš„å†…æ ¸å…ƒç´ ä¼šä»¥ä¸åŒçš„é€Ÿç‡æ”¶æ•›ã€‚ä¸ºäº†æ”¯æŒè¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬æ¨å¯¼å‡ºå…ƒç´ çº§æ¢¯åº¦ä¸ä¸€é˜¶ä¼˜åŒ–ä¹‹é—´çš„ç†è®ºè”ç³»ï¼Œè¡¨æ˜ç»“æ„ä¸Šé‡æ–°å‚æ•°åŒ–çš„å·ç§¯å—å›ºæœ‰åœ°ä¼šè¯±å¯¼ç©ºé—´ä¸Šå˜åŒ–çš„å­¦ä¹ é€Ÿç‡ã€‚åŸºäºè¿™ä¸€è§è§£ï¼Œæˆ‘ä»¬å¼•å…¥äº†Rep3Dï¼Œè¿™æ˜¯ä¸€ä¸ªèå…¥å¯å­¦ä¹ ç©ºé—´å…ˆéªŒçŸ¥è¯†çš„å¤§å‹å†…æ ¸è®­ç»ƒä¸­çš„3Då·ç§¯æ¡†æ¶ã€‚ä¸€ä¸ªè½»é‡çº§çš„ä¸¤é˜¶æ®µè°ƒåˆ¶ç½‘ç»œç”Ÿæˆä¸€ä¸ªæ„Ÿå—é‡åå‘çš„ç¼©æ”¾æ©ç ï¼Œè‡ªé€‚åº”åœ°é‡æ–°åŠ æƒå†…æ ¸æ›´æ–°ï¼Œå¹¶å®ç°äº†ä»å±€éƒ¨åˆ°å…¨å±€çš„æ”¶æ•›è¡Œä¸ºã€‚Rep3Dé‡‡ç”¨ç®€å•çš„ç¼–ç å™¨è®¾è®¡ï¼Œä½¿ç”¨å¤§å‹æ·±åº¦å·ç§¯ï¼Œé¿å…äº†å¤šåˆ†æ”¯ç»„åˆçš„æ¶æ„å¤æ‚æ€§ã€‚æˆ‘ä»¬åœ¨äº”ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„3Dåˆ†å‰²åŸºå‡†ä¸Šå¯¹Rep3Dè¿›è¡Œäº†è¯„ä¼°ï¼Œè¯æ˜äº†å…¶ç›¸è¾ƒäºåŒ…æ‹¬åŸºäºè½¬æ¢å™¨å’Œå›ºå®šå…ˆéªŒé‡å‚æ•°åŒ–æ–¹æ³•åœ¨å†…çš„æœ€æ–°åŸºçº¿å…·æœ‰æŒç»­çš„å¯æ”¹è¿›æ€§ã€‚é€šè¿‡å°†ç©ºé—´å½’çº³åè§ä¸ä¼˜åŒ–æ„ŸçŸ¥å­¦ä¹ ç›¸ç»“åˆï¼ŒRep3Dä¸º3DåŒ»å­¦å›¾åƒåˆ†ææä¾›äº†å¯è§£é‡Šä¸”å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/leeh43/Rep3D">https://github.com/leeh43/Rep3D</a>è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19603v1">PDF</a> 14 pages</p>
<p><strong>Summary</strong><br>    æœ¬æ–‡æ¢è®¨äº†ä¸åŒäºæ¨¡å‹é•¿æœŸä¾èµ–å…³ç³»çš„è§†è§‰å˜å‹å™¨ï¼Œå¤§å†…æ ¸å·ç§¯åœ¨é«˜åˆ†è¾¨ç‡ä¸‰ç»´ä½“ç§¯è®¾ç½®ä¸­çš„é«˜æ•ˆæ€§å’Œå¯æ‰©å±•æ€§æ›¿ä»£æ–¹æ¡ˆã€‚ä½†å•çº¯å¢åŠ å†…æ ¸å¤§å°ä¼šå¯¼è‡´ä¼˜åŒ–ä¸ç¨³å®šå’Œæ€§èƒ½ä¸‹é™ã€‚ç ”ç©¶æå‡ºä¸€ä¸ªç†è®ºè”ç³»å…ƒç´ æ¢¯åº¦ä¸ä¸€é˜¶ä¼˜åŒ–ä¹‹é—´çš„è§‚ç‚¹ï¼Œæ˜¾ç¤ºå‡ºé‡æ–°å‚æ•°åŒ–çš„å·ç§¯å—å›ºæœ‰åœ°å¼•å‘ç©ºé—´å˜åŒ–å­¦ä¹ ç‡ã€‚åŸºäºæ­¤ï¼Œå¼•å…¥Rep3Dæ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†å¯å­¦ä¹ çš„ç©ºé—´å…ˆéªŒçŸ¥è¯†èå…¥å¤§å†…æ ¸è®­ç»ƒä¸­ã€‚é€šè¿‡ç”Ÿæˆå…·æœ‰æ„Ÿå—é‡åå¥½çš„ç¼©æ”¾æ©è†œï¼Œè‡ªé€‚åº”åœ°é‡æ–°åŠ æƒå†…æ ¸æ›´æ–°ï¼Œå®ç°å±€éƒ¨åˆ°å…¨å±€çš„æ”¶æ•›è¡Œä¸ºã€‚åœ¨äº”ä¸ªæŒ‘æˆ˜æ€§çš„ä¸‰ç»´åˆ†å‰²åŸºå‡†æµ‹è¯•ä¸­è¯„ä¼°äº†Rep3Dï¼Œè¯æ˜äº†å…¶åœ¨æœ€æ–°åŸºå‡†ä¸Šçš„æŒç»­æ”¹è¿›ï¼ŒåŒ…æ‹¬åŸºäºè½¬æ¢å™¨å’Œå›ºå®šå…ˆéªŒé‡æ–°å‚æ•°åŒ–æ–¹æ³•ã€‚Rep3Dé€šè¿‡å°†ç©ºé—´å½’çº³åç½®ä¸å…·æœ‰æ„è¯†çš„å­¦ä¹ ç»Ÿä¸€èµ·æ¥ï¼Œä¸ºä¸‰ç»´åŒ»å­¦å›¾åƒåˆ†ææä¾›äº†ä¸€ç§å¯è§£é‡Šå’Œå¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å†…æ ¸å·ç§¯åœ¨é«˜åˆ†è¾¨ç‡ä¸‰ç»´åŒ»å­¦å›¾åƒåˆ†æä¸­æä¾›äº†é«˜æ•ˆå’Œå¯æ‰©å±•çš„æ›¿ä»£æ–¹æ¡ˆã€‚</li>
<li>å•çº¯å¢åŠ å†…æ ¸å¤§å°å¯èƒ½å¯¼è‡´ä¼˜åŒ–ä¸ç¨³å®šå’Œæ€§èƒ½ä¸‹é™ã€‚</li>
<li>æå‡ºäº†å…ƒç´ æ¢¯åº¦ä¸ä¸€é˜¶ä¼˜åŒ–ä¹‹é—´çš„ç†è®ºè”ç³»ï¼Œæ­ç¤ºäº†ç©ºé—´å˜åŒ–å­¦ä¹ ç‡çš„å†…åœ¨æœºåˆ¶ã€‚</li>
<li>å¼•å…¥äº†Rep3Dæ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†å¤§å†…æ ¸è®­ç»ƒä¸­çš„å¯å­¦ä¹ ç©ºé—´å…ˆéªŒçŸ¥è¯†ã€‚</li>
<li>é€šè¿‡ç”Ÿæˆå…·æœ‰æ„Ÿå—é‡åå¥½çš„ç¼©æ”¾æ©è†œï¼Œè‡ªé€‚åº”åœ°é‡æ–°åŠ æƒå†…æ ¸æ›´æ–°ã€‚</li>
<li>Rep3Dåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå¯¹æœ€æ–°æŠ€æœ¯çš„æŒç»­æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19603">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9890c3ea4a77b6868bb259a4293a2202.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aaf2d5abf6ce85900c2e1c96853c2076.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-acbcfec6771ef2b49ade7184a9e92396.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="What-You-Perceive-Is-What-You-Conceive-A-Cognition-Inspired-Framework-for-Open-Vocabulary-Image-Segmentation"><a href="#What-You-Perceive-Is-What-You-Conceive-A-Cognition-Inspired-Framework-for-Open-Vocabulary-Image-Segmentation" class="headerlink" title="What You Perceive Is What You Conceive: A Cognition-Inspired Framework   for Open Vocabulary Image Segmentation"></a>What You Perceive Is What You Conceive: A Cognition-Inspired Framework   for Open Vocabulary Image Segmentation</h2><p><strong>Authors:Jianghang Lin, Yue Hu, Jiangtao Shen, Yunhang Shen, Liujuan Cao, Shengchuan Zhang, Rongrong Ji</strong></p>
<p>Open vocabulary image segmentation tackles the challenge of recognizing dynamically adjustable, predefined novel categories at inference time by leveraging vision-language alignment. However, existing paradigms typically perform class-agnostic region segmentation followed by category matching, which deviates from the human visual systemâ€™s process of recognizing objects based on semantic concepts, leading to poor alignment between region segmentation and target concepts. To bridge this gap, we propose a novel Cognition-Inspired Framework for open vocabulary image segmentation that emulates the human visual recognition process: first forming a conceptual understanding of an object, then perceiving its spatial extent. The framework consists of three core components: (1) A Generative Vision-Language Model (G-VLM) that mimics human cognition by generating object concepts to provide semantic guidance for region segmentation. (2) A Concept-Aware Visual Enhancer Module that fuses textual concept features with global visual representations, enabling adaptive visual perception based on target concepts. (3) A Cognition-Inspired Decoder that integrates local instance features with G-VLM-provided semantic cues, allowing selective classification over a subset of relevant categories. Extensive experiments demonstrate that our framework achieves significant improvements, reaching $27.2$ PQ, $17.0$ mAP, and $35.3$ mIoU on A-150. It further attains $56.2$, $28.2$, $15.4$, $59.2$, $18.7$, and $95.8$ mIoU on Cityscapes, Mapillary Vistas, A-847, PC-59, PC-459, and PAS-20, respectively. In addition, our framework supports vocabulary-free segmentation, offering enhanced flexibility in recognizing unseen categories. Code will be public. </p>
<blockquote>
<p>å¼€æ”¾è¯æ±‡å›¾åƒåˆ†å‰²é€šè¿‡åˆ©ç”¨è§†è§‰è¯­è¨€å¯¹é½æ¥è§£å†³åœ¨æ¨ç†æ—¶è¯†åˆ«åŠ¨æ€å¯è°ƒæ•´ã€é¢„å…ˆå®šä¹‰çš„æ–°ç±»åˆ«çš„æŒ‘æˆ˜ã€‚ç„¶è€Œï¼Œç°æœ‰çš„èŒƒå¼é€šå¸¸æ‰§è¡Œç±»åˆ«æ— å…³çš„åŒºåŸŸåˆ†å‰²ï¼Œç„¶åè¿›è¡Œç±»åˆ«åŒ¹é…ï¼Œè¿™åç¦»äº†äººç±»è§†è§‰ç³»ç»ŸåŸºäºè¯­ä¹‰æ¦‚å¿µè¯†åˆ«ç‰©ä½“çš„è¿‡ç¨‹ï¼Œå¯¼è‡´åŒºåŸŸåˆ†å‰²ä¸ç›®æ ‡æ¦‚å¿µä¹‹é—´çš„å¯¹é½ä¸ä½³ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ¨¡æ‹Ÿäººç±»è§†è§‰è¯†åˆ«è¿‡ç¨‹çš„å¼€æ”¾è¯æ±‡å›¾åƒåˆ†å‰²çš„è®¤çŸ¥å¯å‘æ¡†æ¶ï¼šé¦–å…ˆå½¢æˆå¯¹ç‰©ä½“çš„æ¦‚å¿µç†è§£ï¼Œç„¶åæ„ŸçŸ¥å…¶ç©ºé—´èŒƒå›´ã€‚è¯¥æ¡†æ¶ç”±ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶æ„æˆï¼šï¼ˆ1ï¼‰ç”Ÿæˆå¼è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆG-VLMï¼‰ï¼Œé€šè¿‡ç”Ÿæˆå¯¹è±¡æ¦‚å¿µæ¥æ¨¡æ‹Ÿäººç±»è®¤çŸ¥ï¼Œä¸ºåŒºåŸŸåˆ†å‰²æä¾›è¯­ä¹‰æŒ‡å¯¼ã€‚ï¼ˆ2ï¼‰æ¦‚å¿µæ„ŸçŸ¥è§†è§‰å¢å¼ºæ¨¡å—ï¼Œå°†æ–‡æœ¬æ¦‚å¿µç‰¹å¾ä¸å…¨å±€è§†è§‰è¡¨ç¤ºç›¸èåˆï¼Œå®ç°åŸºäºç›®æ ‡æ¦‚å¿µçš„è‡ªé€‚åº”è§†è§‰æ„ŸçŸ¥ã€‚ï¼ˆ3ï¼‰è®¤çŸ¥å¯å‘è§£ç å™¨ï¼Œå®ƒèåˆäº†å±€éƒ¨å®ä¾‹ç‰¹å¾ä¸G-VLMæä¾›çš„è¯­ä¹‰çº¿ç´¢ï¼Œå…è®¸åœ¨ç›¸å…³ç±»åˆ«å­é›†ä¸Šè¿›è¡Œé€‰æ‹©æ€§åˆ†ç±»ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶å®ç°äº†æ˜¾è‘—æ”¹è¿›ï¼Œåœ¨A-150ä¸Šè¾¾åˆ°äº†27.2 PQã€17.0 mAPå’Œ35.3 mIoUã€‚æ­¤å¤–ï¼Œå®ƒåœ¨Cityscapesã€Mapillary Vistasã€A-847ã€PC-59ã€PC-459å’ŒPAS-20ä¸Šåˆ†åˆ«è¾¾åˆ°äº†56.2ã€28.2ã€15.4ã€59.2ã€18.7å’Œ95.8 mIoUã€‚æˆ‘ä»¬çš„æ¡†æ¶è¿˜æ”¯æŒæ— è¯æ±‡åˆ†å‰²ï¼Œä¸ºè¯†åˆ«æœªè§ç±»åˆ«æä¾›äº†æ›´é«˜çš„çµæ´»æ€§ã€‚ä»£ç å°†å…¬å¼€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19569v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§è®¤çŸ¥å¯å‘æ¡†æ¶ï¼Œç”¨äºå¼€æ”¾è¯æ±‡å›¾åƒåˆ†å‰²ï¼Œè¯¥æ¡†æ¶æ¨¡æ‹Ÿäººç±»è§†è§‰è¯†åˆ«è¿‡ç¨‹ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬ç”Ÿæˆå¼è§†è§‰è¯­è¨€æ¨¡å‹ã€æ¦‚å¿µæ„ŸçŸ¥è§†è§‰å¢å¼ºå™¨æ¨¡å—å’Œè®¤çŸ¥å¯å‘è§£ç å™¨ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ã€‚å®éªŒç»“æœè¯æ˜äº†è¯¥æ¡†æ¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„æ˜¾è‘—æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼€æ”¾è¯æ±‡å›¾åƒåˆ†å‰²é€šè¿‡åˆ©ç”¨è§†è§‰è¯­è¨€å¯¹é½æ¥è¯†åˆ«åŠ¨æ€å¯è°ƒæ•´ã€é¢„å®šä¹‰çš„æœªçŸ¥ç±»åˆ«ã€‚</li>
<li>ç°æœ‰æ–¹æ³•é€šå¸¸è¿›è¡Œç±»åˆ«æ— å…³çš„åŒºåŸŸåˆ†å‰²ï¼Œç„¶åè¿›è¡Œç±»åˆ«åŒ¹é…ï¼Œè¿™ä¸äººè¯†åˆ«ç‰©ä½“åŸºäºè¯­ä¹‰æ¦‚å¿µçš„è§†è§‰ç³»ç»Ÿè¿‡ç¨‹ä¸ç¬¦ï¼Œå¯¼è‡´åŒºåŸŸåˆ†å‰²ä¸ç›®æ ‡æ¦‚å¿µä¹‹é—´çš„å¯¹é½ä¸ä½³ã€‚</li>
<li>æå‡ºçš„è®¤çŸ¥å¯å‘æ¡†æ¶æ¨¡æ‹Ÿäººç±»è§†è§‰è¯†åˆ«è¿‡ç¨‹ï¼ŒåŒ…æ‹¬ç”Ÿæˆç‰©ä½“æ¦‚å¿µã€æ„ŸçŸ¥ç‰©ä½“ç©ºé—´èŒƒå›´çš„æ­¥éª¤ã€‚</li>
<li>æ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šç”Ÿæˆå¼è§†è§‰è¯­è¨€æ¨¡å‹ã€æ¦‚å¿µæ„ŸçŸ¥è§†è§‰å¢å¼ºå™¨æ¨¡å—å’Œè®¤çŸ¥å¯å‘è§£ç å™¨ã€‚è¿™äº›ç»„ä»¶é€šè¿‡èåˆæ–‡æœ¬æ¦‚å¿µç‰¹å¾å’Œå…¨å±€è§†è§‰è¡¨ç¤ºï¼Œå®ç°åŸºäºç›®æ ‡æ¦‚å¿µçš„è‡ªé€‚åº”è§†è§‰æ„ŸçŸ¥ï¼Œå¹¶æ•´åˆå±€éƒ¨å®ä¾‹ç‰¹å¾ä¸è¯­ä¹‰çº¿ç´¢è¿›è¡Œé€‰æ‹©æ€§åˆ†ç±»ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼ŒåŒ…æ‹¬A-150ã€Cityscapesã€Mapillary Vistasç­‰ã€‚</li>
<li>è¯¥æ¡†æ¶æ”¯æŒè¯æ±‡æ— å…³çš„åˆ†å‰²ï¼Œæé«˜äº†è¯†åˆ«æœªè§ç±»åˆ«çš„çµæ´»æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19569">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2427bded105b1d51260c5f4bfffff56c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-59ba6f98abbca878db32ad04d1345e7f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-39aab738ed71f5af5de6ca668c2a89f8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fba39b993fcb4d83e489ccb845b64066.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-35a420e67cee697e620bcf696b0aa969.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Automated-CAD-Modeling-Sequence-Generation-from-Text-Descriptions-via-Transformer-Based-Large-Language-Models"><a href="#Automated-CAD-Modeling-Sequence-Generation-from-Text-Descriptions-via-Transformer-Based-Large-Language-Models" class="headerlink" title="Automated CAD Modeling Sequence Generation from Text Descriptions via   Transformer-Based Large Language Models"></a>Automated CAD Modeling Sequence Generation from Text Descriptions via   Transformer-Based Large Language Models</h2><p><strong>Authors:Jianxing Liao, Junyan Xu, Yatao Sun, Maowen Tang, Sicheng He, Jingxian Liao, Shui Yu, Yun Li, Hongguan Xiao</strong></p>
<p>Designing complex computer-aided design (CAD) models is often time-consuming due to challenges such as computational inefficiency and the difficulty of generating precise models. We propose a novel language-guided framework for industrial design automation to address these issues, integrating large language models (LLMs) with computer-automated design (CAutoD).Through this framework, CAD models are automatically generated from parameters and appearance descriptions, supporting the automation of design tasks during the detailed CAD design phase. Our approach introduces three key innovations: (1) a semi-automated data annotation pipeline that leverages LLMs and vision-language large models (VLLMs) to generate high-quality parameters and appearance descriptions; (2) a Transformer-based CAD generator (TCADGen) that predicts modeling sequences via dual-channel feature aggregation; (3) an enhanced CAD modeling generation model, called CADLLM, that is designed to refine the generated sequences by incorporating the confidence scores from TCADGen. Experimental results demonstrate that the proposed approach outperforms traditional methods in both accuracy and efficiency, providing a powerful tool for automating industrial workflows and generating complex CAD models from textual prompts. The code is available at <a target="_blank" rel="noopener" href="https://jianxliao.github.io/cadllm-page/">https://jianxliao.github.io/cadllm-page/</a> </p>
<blockquote>
<p>è®¾è®¡å¤æ‚çš„è®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰æ¨¡å‹é€šå¸¸å¾ˆè€—æ—¶ï¼Œå› ä¸ºå­˜åœ¨è®¡ç®—æ•ˆç‡ä½ä¸‹å’Œéš¾ä»¥ç”Ÿæˆç²¾ç¡®æ¨¡å‹ç­‰æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºå·¥ä¸šè‡ªåŠ¨åŒ–è®¾è®¡çš„æ–°å‹è¯­è¨€å¼•å¯¼æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸è®¡ç®—æœºè‡ªåŠ¨åŒ–è®¾è®¡ï¼ˆCAutoDï¼‰ç›¸ç»“åˆã€‚é€šè¿‡æ­¤æ¡†æ¶ï¼ŒCADæ¨¡å‹å¯ä»¥æ ¹æ®å‚æ•°å’Œå¤–è§‚æè¿°è‡ªåŠ¨ç”Ÿæˆï¼Œæ”¯æŒè¯¦ç»†CADè®¾è®¡é˜¶æ®µçš„è‡ªåŠ¨åŒ–è®¾è®¡ä»»åŠ¡ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†ä¸‰ä¸ªå…³é”®åˆ›æ–°ç‚¹ï¼šï¼ˆ1ï¼‰ä¸€ç§åŠè‡ªåŠ¨æ•°æ®æ³¨é‡Šç®¡é“ï¼Œåˆ©ç”¨LLMå’Œè§†è§‰è¯­è¨€å¤§å‹æ¨¡å‹ï¼ˆVLLMï¼‰ç”Ÿæˆé«˜è´¨é‡å‚æ•°å’Œå¤–è§‚æè¿°ï¼›ï¼ˆ2ï¼‰åŸºäºTransformerçš„CADç”Ÿæˆå™¨ï¼ˆTCADGenï¼‰ï¼Œé€šè¿‡åŒé€šé“ç‰¹å¾èšåˆé¢„æµ‹å»ºæ¨¡åºåˆ—ï¼›ï¼ˆ3ï¼‰ä¸€ç§å¢å¼ºçš„CADå»ºæ¨¡ç”Ÿæˆæ¨¡å‹ï¼Œç§°ä¸ºCADLLMï¼Œæ—¨åœ¨é€šè¿‡ç»“åˆTCADGençš„ä¿¡å¿ƒåˆ†æ•°æ¥ä¼˜åŒ–ç”Ÿæˆçš„åºåˆ—ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å‡†ç¡®æ€§å’Œæ•ˆç‡æ–¹é¢éƒ½ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œä¸ºè‡ªåŠ¨åŒ–å·¥ä¸šå·¥ä½œæµç¨‹å’Œæ ¹æ®æ–‡æœ¬æç¤ºç”Ÿæˆå¤æ‚çš„CADæ¨¡å‹æä¾›äº†å¼ºå¤§çš„å·¥å…·ã€‚ç›¸å…³ä»£ç å¯åœ¨[<a target="_blank" rel="noopener" href="https://jianxliao.github.io/cadllm-page/]%E6%89%BE%E5%88%B0%E3%80%82">https://jianxliao.github.io/cadllm-page/]æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19490v1">PDF</a> Accepted by ACL 2025 Main Conference</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†ä¸€ç§æ–°å‹çš„è¯­è¨€å¼•å¯¼å·¥ä¸šè®¾è®¡è‡ªåŠ¨åŒ–æ¡†æ¶ï¼Œå®ƒç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸è®¡ç®—æœºè‡ªåŠ¨åŒ–è®¾è®¡ï¼ˆCAutoDï¼‰ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿè‡ªåŠ¨ä»å‚æ•°å’Œå¤–è§‚æè¿°ç”ŸæˆCADæ¨¡å‹ï¼Œæ”¯æŒè¯¦ç»†è®¾è®¡é˜¶æ®µçš„è®¾è®¡ä»»åŠ¡è‡ªåŠ¨åŒ–ã€‚å…¶ä¸‰å¤§åˆ›æ–°ç‚¹åŒ…æ‹¬ï¼šåŠè‡ªåŠ¨æ•°æ®æ ‡æ³¨ç®¡é“ã€åŸºäºTransformerçš„CADç”Ÿæˆå™¨ï¼ˆTCADGenï¼‰ä»¥åŠå¢å¼ºçš„CADå»ºæ¨¡ç”Ÿæˆæ¨¡å‹CADLLMã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å‡†ç¡®æ€§å’Œæ•ˆç‡ä¸Šå‡ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œä¸ºè‡ªåŠ¨åŒ–å·¥ä¸šæµç¨‹å’Œä»æ–‡æœ¬æç¤ºç”Ÿæˆå¤æ‚CADæ¨¡å‹æä¾›äº†å¼ºå¤§å·¥å…·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥æ¡†æ¶æ•´åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸è®¡ç®—æœºè‡ªåŠ¨åŒ–è®¾è®¡ï¼ˆCAutoDï¼‰ï¼Œæé«˜äº†è®¾è®¡æ•ˆç‡å’Œç²¾åº¦ã€‚</li>
<li>é‡‡ç”¨åŠè‡ªåŠ¨æ•°æ®æ ‡æ³¨ç®¡é“ï¼Œåˆ©ç”¨LLMså’Œè§†è§‰è¯­è¨€å¤§å‹æ¨¡å‹ï¼ˆVLLMsï¼‰ç”Ÿæˆé«˜è´¨é‡å‚æ•°å’Œå¤–è§‚æè¿°ã€‚</li>
<li>TCADGenåŸºäºTransformerçš„CADç”Ÿæˆå™¨èƒ½é€šè¿‡åŒé€šé“ç‰¹å¾èšåˆé¢„æµ‹å»ºæ¨¡åºåˆ—ã€‚</li>
<li>CADLLMæ¨¡å‹ç”¨äºä¼˜åŒ–ç”Ÿæˆçš„åºåˆ—ï¼Œé€šè¿‡ç»“åˆTCADGençš„ä¿¡å¿ƒåˆ†æ•°æ¥å®Œå–„CADå»ºæ¨¡ç”Ÿæˆã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å‡†ç¡®åº¦å’Œæ•ˆç‡æ–¹é¢å‡ä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚</li>
<li>è¯¥æ¡†æ¶ä¸ºè‡ªåŠ¨åŒ–å·¥ä¸šæµç¨‹æä¾›äº†æœ‰åŠ›æ”¯æŒï¼Œèƒ½å¤Ÿä»æ–‡æœ¬æç¤ºç”Ÿæˆå¤æ‚çš„CADæ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19490">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8bf87cea09b4d7d92a23af6cd91eea52.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-476cd34b77b90f40bdc6d477d3c51572.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-570e66c82a74441f84ce8df5509fc916.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Advancing-Limited-Angle-CT-Reconstruction-Through-Diffusion-Based-Sinogram-Completion"><a href="#Advancing-Limited-Angle-CT-Reconstruction-Through-Diffusion-Based-Sinogram-Completion" class="headerlink" title="Advancing Limited-Angle CT Reconstruction Through Diffusion-Based   Sinogram Completion"></a>Advancing Limited-Angle CT Reconstruction Through Diffusion-Based   Sinogram Completion</h2><p><strong>Authors:Jiaqi Guo, Santiago Lopez-Tapia, Aggelos K. Katsaggelos</strong></p>
<p>Limited Angle Computed Tomography (LACT) often faces significant challenges due to missing angular information. Unlike previous methods that operate in the image domain, we propose a new method that focuses on sinogram inpainting. We leverage MR-SDEs, a variant of diffusion models that characterize the diffusion process with mean-reverting stochastic differential equations, to fill in missing angular data at the projection level. Furthermore, by combining distillation with constraining the output of the model using the pseudo-inverse of the inpainting matrix, the diffusion process is accelerated and done in a step, enabling efficient and accurate sinogram completion. A subsequent post-processing module back-projects the inpainted sinogram into the image domain and further refines the reconstruction, effectively suppressing artifacts while preserving critical structural details. Quantitative experimental results demonstrate that the proposed method achieves state-of-the-art performance in both perceptual and fidelity quality, offering a promising solution for LACT reconstruction in scientific and clinical applications. </p>
<blockquote>
<p>æœ‰é™è§’åº¦è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆLACTï¼‰ç”±äºç¼ºå°‘è§’åº¦ä¿¡æ¯è€Œé¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚ä¸åŒäºä¹‹å‰åœ¨å›¾åƒåŸŸæ“ä½œçš„æ–¹æ³•ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä¸»è¦å…³æ³¨sinogramçš„å¡«å……ã€‚æˆ‘ä»¬åˆ©ç”¨MR-SDEsï¼ˆä¸€ç§ç”¨å¹³å‡å›å½’éšæœºå¾®åˆ†æ–¹ç¨‹æè¿°æ‰©æ•£è¿‡ç¨‹çš„æ‰©æ•£æ¨¡å‹çš„å˜ä½“ï¼‰åœ¨æŠ•å½±å±‚é¢å¡«å……ç¼ºå¤±çš„è§’åº¦æ•°æ®ã€‚æ­¤å¤–ï¼Œé€šè¿‡ç»“åˆè’¸é¦æŠ€æœ¯å¹¶ä½¿ç”¨å¡«å……çŸ©é˜µçš„ä¼ªé€†çº¦æŸæ¨¡å‹çš„è¾“å‡ºï¼Œå¯ä»¥åŠ é€Ÿæ‰©æ•£è¿‡ç¨‹å¹¶åœ¨ä¸€æ­¥å†…å®Œæˆï¼Œä»è€Œå®ç°é«˜æ•ˆä¸”å‡†ç¡®çš„sinogramå®Œæˆã€‚éšåçš„åå¤„ç†æ¨¡å—å°†å¡«å……çš„sinogramåå‘æŠ•å½±åˆ°å›¾åƒåŸŸï¼Œå¹¶è¿›ä¸€æ­¥ä¼˜åŒ–é‡å»ºç»“æœï¼Œæœ‰æ•ˆåœ°æŠ‘åˆ¶ä¼ªå½±çš„åŒæ—¶ä¿ç•™å…³é”®çš„ç»“æ„ç»†èŠ‚ã€‚å®šé‡å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨æ„ŸçŸ¥å’Œä¿çœŸè´¨é‡æ–¹é¢éƒ½è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œä¸ºç§‘å­¦å’Œä¸´åºŠåº”ç”¨ä¸­LACTé‡å»ºæä¾›äº†æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19385v1">PDF</a> Accepted at the 2025 IEEE International Conference on Image   Processing (Oral)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºMR-SDEsæ‰©æ•£æ¨¡å‹çš„æ–°æ–¹æ³•ï¼Œä¸“æ³¨äºè¾›æ ¼æ‹‰å§†æ’å€¼ï¼Œä»¥å¡«å……æœ‰é™è§’åº¦è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆLACTï¼‰ä¸­çš„ç¼ºå¤±è§’åº¦æ•°æ®ã€‚ç»“åˆè’¸é¦æŠ€æœ¯å’Œä½¿ç”¨è¡¥å…¨çŸ©é˜µçš„ä¼ªé€†çº¦æŸæ¨¡å‹è¾“å‡ºï¼ŒåŠ é€Ÿäº†æ‰©æ•£è¿‡ç¨‹ï¼Œå®ç°äº†é«˜æ•ˆä¸”å‡†ç¡®çš„è¾›æ ¼æ‹‰å§†è¡¥å…¨ã€‚åå¤„ç†æ¨¡å—å°†è¡¥å…¨çš„è¾›æ ¼æ‹‰å§†æŠ•å½±å›å›¾åƒåŸŸï¼Œè¿›ä¸€æ­¥æ”¹è¿›é‡å»ºæ•ˆæœï¼Œæœ‰æ•ˆæŠ‘åˆ¶ä¼ªå½±åŒæ—¶ä¿ç•™å…³é”®ç»“æ„ç»†èŠ‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æå‡ºäº†ä¸€ç§æ–°çš„åŸºäºMR-SDEsæ‰©æ•£æ¨¡å‹çš„æ–¹æ³•ï¼Œä¸“æ³¨äºè¾›æ ¼æ‹‰å§†æ’å€¼ä»¥è§£å†³LACTä¸­çš„ç¼ºå¤±è§’åº¦ä¿¡æ¯é—®é¢˜ã€‚</li>
<li>åœ¨æŠ•å½±çº§åˆ«å¡«å……ç¼ºå¤±çš„è§’åº¦æ•°æ®ï¼Œç»“åˆè’¸é¦æŠ€æœ¯åŠ é€Ÿæ‰©æ•£è¿‡ç¨‹ã€‚</li>
<li>ä½¿ç”¨çº¦æŸæ¨¡å‹è¾“å‡ºæ¥æé«˜è¾›æ ¼æ‹‰å§†è¡¥å…¨çš„å‡†ç¡®æ€§ã€‚</li>
<li>åå¤„ç†æ¨¡å—èƒ½å¤Ÿå°†è¡¥å…¨çš„è¾›æ ¼æ‹‰å§†æŠ•å½±å›å›¾åƒåŸŸï¼Œæœ‰æ•ˆæŠ‘åˆ¶ä¼ªå½±å¹¶ä¿ç•™å…³é”®ç»“æ„ç»†èŠ‚ã€‚</li>
<li>å®šé‡å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ„ŸçŸ¥å’Œä¿çœŸè´¨é‡æ–¹é¢è¾¾åˆ°æœ€æ–°æ°´å¹³ï¼Œä¸ºç§‘å­¦å’Œä¸´åºŠåº”ç”¨ä¸­çš„LACTé‡å»ºæä¾›äº†æœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19385">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9b0de5c928fc1bca3633f57120ba5a3d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ab27e2612a03c1f249b254237f34cae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5206070dc91963da2bc84d1a570f2ada.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3fb9935aee4c21cf52cc87a7e219daca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1b18c2a3f9ad8156d4fc67b302dca6e1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-05d8e195cb1d43252e54c3a9df80d6c7.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="PolyPose-Localizing-Deformable-Anatomy-in-3D-from-Sparse-2D-X-ray-Images-using-Polyrigid-Transforms"><a href="#PolyPose-Localizing-Deformable-Anatomy-in-3D-from-Sparse-2D-X-ray-Images-using-Polyrigid-Transforms" class="headerlink" title="PolyPose: Localizing Deformable Anatomy in 3D from Sparse 2D X-ray   Images using Polyrigid Transforms"></a>PolyPose: Localizing Deformable Anatomy in 3D from Sparse 2D X-ray   Images using Polyrigid Transforms</h2><p><strong>Authors:Vivek Gopalakrishnan, Neel Dey, Polina Golland</strong></p>
<p>Determining the 3D pose of a patient from a limited set of 2D X-ray images is a critical task in interventional settings. While preoperative volumetric imaging (e.g., CT and MRI) provides precise 3D localization and visualization of anatomical targets, these modalities cannot be acquired during procedures, where fast 2D imaging (X-ray) is used instead. To integrate volumetric guidance into intraoperative procedures, we present PolyPose, a simple and robust method for deformable 2D&#x2F;3D registration. PolyPose parameterizes complex 3D deformation fields as a composition of rigid transforms, leveraging the biological constraint that individual bones do not bend in typical motion. Unlike existing methods that either assume no inter-joint movement or fail outright in this under-determined setting, our polyrigid formulation enforces anatomically plausible priors that respect the piecewise rigid nature of human movement. This approach eliminates the need for expensive deformation regularizers that require patient- and procedure-specific hyperparameter optimization. Across extensive experiments on diverse datasets from orthopedic surgery and radiotherapy, we show that this strong inductive bias enables PolyPose to successfully align the patientâ€™s preoperative volume to as few as two X-ray images, thereby providing crucial 3D guidance in challenging sparse-view and limited-angle settings where current registration methods fail. </p>
<blockquote>
<p>åœ¨ä»‹å…¥ç¯å¢ƒä¸­ï¼Œä»æœ‰é™çš„2D Xå°„çº¿å›¾åƒé›†ä¸­ç¡®å®šæ‚£è€…çš„3Då§¿åŠ¿æ˜¯ä¸€é¡¹å…³é”®ä»»åŠ¡ã€‚è™½ç„¶æœ¯å‰ä½“ç§¯æˆåƒï¼ˆä¾‹å¦‚CTå’ŒMRIï¼‰å¯ä»¥æä¾›ç²¾ç¡®çš„3Då®šä½å’Œè§£å‰–ç›®æ ‡çš„å¯è§†åŒ–ï¼Œä½†è¿™äº›æ¨¡å¼ä¸èƒ½åœ¨æ‰‹æœ¯è¿‡ç¨‹ä¸­è·å–ï¼Œæ­¤æ—¶ä½¿ç”¨çš„æ˜¯å¿«é€Ÿçš„2Dæˆåƒï¼ˆXå°„çº¿ï¼‰ã€‚ä¸ºäº†å°†ä½“ç§¯å¼•å¯¼é›†æˆåˆ°æœ¯ä¸­ç¨‹åºä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†PolyPoseï¼Œè¿™æ˜¯ä¸€ç§ç®€å•è€Œç¨³å¥çš„å¯å˜å½¢2D&#x2F;3Dæ³¨å†Œæ–¹æ³•ã€‚PolyPoseå°†å¤æ‚çš„3Då˜å½¢åœºå‚æ•°åŒ–ä¸ºåˆšä½“å˜æ¢çš„ç»„åˆï¼Œåˆ©ç”¨ä¸ªä½“éª¨éª¼åœ¨å…¸å‹è¿åŠ¨ä¸­çš„å¼¯æ›²çº¦æŸã€‚ä¸ç°æœ‰çš„æ–¹æ³•ä¸åŒï¼Œè¿™äº›æ–¹æ³•è¦ä¹ˆå‡è®¾å…³èŠ‚ä¹‹é—´æ²¡æœ‰è¿åŠ¨ï¼Œè¦ä¹ˆåœ¨è¿™ä¸ªæœªçŸ¥è®¾å®šçš„ç¯å¢ƒä¸‹å½»åº•å¤±è´¥ï¼Œæˆ‘ä»¬çš„å¤šåˆšä½“å…¬å¼å¼ºåˆ¶é‡‡ç”¨ç¬¦åˆäººä½“è§£å‰–ç»“æ„çš„å…ˆéªŒçŸ¥è¯†ï¼Œå°Šé‡äººä½“çš„åˆ†æ®µåˆšæ€§è¿åŠ¨ç‰¹æ€§ã€‚è¿™ç§æ–¹æ³•æ¶ˆé™¤äº†æ˜‚è´µçš„å˜å½¢æ­£åˆ™åŒ–å™¨ï¼Œè¿™äº›æ­£åˆ™åŒ–å™¨éœ€è¦è¿›è¡Œæ‚£è€…å’Œæ‰‹æœ¯ç‰¹å®šçš„è¶…å‚æ•°ä¼˜åŒ–ã€‚åœ¨éª¨ç§‘æ‰‹æœ¯å’Œæ”¾å°„æ²»ç–—ç­‰ä¸åŒæ•°æ®é›†çš„å¤§é‡å®éªŒä¸­ï¼Œæˆ‘ä»¬è¯æ˜äº†è¿™ç§å¼ºå¤§çš„å½’çº³åè§ä½¿PolyPoseèƒ½å¤Ÿå°†æ‚£è€…çš„æœ¯å‰ä½“ç§¯æˆåŠŸå¯¹é½åˆ°ä»…ä¸¤å¼ Xå°„çº¿å›¾åƒä¸Šï¼Œä»è€Œåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„ç¨€ç–è§†å›¾å’Œæœ‰é™è§’åº¦è®¾ç½®ä¸‹ä¸ºå½“å‰çš„æ³¨å†Œæ–¹æ³•æä¾›å…³é”®çš„3DæŒ‡å¯¼ï¼Œè€Œè¿™äº›æƒ…å†µä¸‹å½“å‰æ³¨å†Œæ–¹æ³•å¾€å¾€ä¼šå¤±è´¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19256v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>PolyPoseæ˜¯ä¸€ç§ç®€å•è€Œç¨³å¥çš„å˜å½¢2D&#x2F;3Dæ³¨å†Œæ–¹æ³•ï¼Œé€‚ç”¨äºä»‹å…¥æ‰‹æœ¯ä¸­çš„ä½“ç§¯å¼•å¯¼ã€‚å®ƒé€šè¿‡å‚æ•°åŒ–å¤æ‚çš„3Då˜å½¢åœºä¸ºåˆšæ€§å˜æ¢çš„ç»„åˆï¼Œåˆ©ç”¨ä¸ªä½“éª¨éª¼åœ¨å…¸å‹è¿åŠ¨ä¸­çš„åˆšæ€§çº¦æŸï¼Œè§£å†³äº†åœ¨æœ‰é™2D Xå°„çº¿å›¾åƒä¸‹æ‚£è€…3Då§¿æ€ç¡®å®šçš„å…³é”®é—®é¢˜ã€‚è¯¥æ–¹æ³•åœ¨å¤šæ ·åŒ–çš„æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒéªŒè¯ï¼Œåœ¨å½“å‰çš„æ³¨å†Œæ–¹æ³•éš¾ä»¥åº”å¯¹çš„æŒ‘æˆ˜æ€§ç¨€ç–è§†å›¾å’Œæœ‰é™è§’åº¦è®¾ç½®ä¸‹ï¼Œè¯¥æ–¹æ³•èƒ½å¤ŸæˆåŠŸåœ°å°†æ‚£è€…çš„æœ¯å‰ä½“ç§¯ä¸ä»…ä¸¤å¼ Xå°„çº¿å›¾åƒå¯¹é½ï¼Œä»è€Œä¸ºæ‰‹æœ¯æä¾›å…³é”®çš„3DæŒ‡å¯¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PolyPoseæ–¹æ³•æ—¨åœ¨è§£å†³åœ¨ä»‹å…¥æ‰‹æœ¯ä¸­é€šè¿‡æœ‰é™çš„2D Xå°„çº¿å›¾åƒç¡®å®šæ‚£è€…3Då§¿æ€çš„éš¾é¢˜ã€‚</li>
<li>å®ƒåˆ©ç”¨å‚æ•°åŒ–çš„å¤æ‚3Då˜å½¢åœºæ¥å®ç°å˜å½¢è¿‡ç¨‹çš„æ¨¡æ‹Ÿã€‚</li>
<li>è¯¥æ–¹æ³•é‡‡ç”¨åˆšæ€§å˜æ¢çš„ç»„åˆæ¥è¡¨ç¤ºå˜å½¢åœºï¼Œç¬¦åˆäººä½“éª¨éª¼åœ¨å…¸å‹è¿åŠ¨ä¸­çš„åˆšæ€§çº¦æŸã€‚</li>
<li>PolyPoseé€šè¿‡å¼•å…¥è§£å‰–ç»“æ„å…ˆéªŒçŸ¥è¯†æ¥çº¦æŸäººä½“åˆ†æ®µåˆšæ€§çš„è¿åŠ¨ç‰¹æ€§ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•çš„å±€é™æ€§ã€‚</li>
<li>è¯¥æ–¹æ³•æ— éœ€æ˜‚è´µçš„å˜å½¢æ­£åˆ™åŒ–å™¨ï¼Œé¿å…äº†é’ˆå¯¹ç‰¹å®šæ‚£è€…å’Œç¨‹åºçš„è¶…å‚æ•°ä¼˜åŒ–éœ€æ±‚ã€‚</li>
<li>åœ¨éª¨ç§‘æ‰‹æœ¯å’Œæ”¾å°„æ²»ç–—ç­‰ä¸åŒæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒéªŒè¯äº†PolyPoseçš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19256">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-fb8c36697b4736529f7671df4839fbcc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3eb00f213d94e43bb89137527ce19c7e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cdfd91d540c21c40ece51d4665d16884.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="MedITok-A-Unified-Tokenizer-for-Medical-Image-Synthesis-and-Interpretation"><a href="#MedITok-A-Unified-Tokenizer-for-Medical-Image-Synthesis-and-Interpretation" class="headerlink" title="MedITok: A Unified Tokenizer for Medical Image Synthesis and   Interpretation"></a>MedITok: A Unified Tokenizer for Medical Image Synthesis and   Interpretation</h2><p><strong>Authors:Chenglong Ma, Yuanfeng Ji, Jin Ye, Zilong Li, Chenhui Wang, Junzhi Ning, Wei Li, Lihao Liu, Qiushan Guo, Tianbin Li, Junjun He, Hongming Shan</strong></p>
<p>Advanced autoregressive models have reshaped multimodal AI. However, their transformative potential in medical imaging remains largely untapped due to the absence of a unified visual tokenizer â€“ one capable of capturing fine-grained visual structures for faithful image reconstruction and realistic image synthesis, as well as rich semantics for accurate diagnosis and image interpretation. To this end, we present MedITok, the first unified tokenizer tailored for medical images, encoding both low-level structural details and high-level clinical semantics within a unified latent space. To balance these competing objectives, we introduce a novel two-stage training framework: a visual representation alignment stage that cold-starts the tokenizer reconstruction learning with a visual semantic constraint, followed by a textual semantic representation alignment stage that infuses detailed clinical semantics into the latent space. Trained on the meticulously collected large-scale dataset with over 30 million medical images and 2 million image-caption pairs, MedITok achieves state-of-the-art performance on more than 30 datasets across 9 imaging modalities and 4 different tasks. By providing a unified token space for autoregressive modeling, MedITok supports a wide range of tasks in clinical diagnostics and generative healthcare applications. Model and code will be made publicly available at: <a target="_blank" rel="noopener" href="https://github.com/Masaaki-75/meditok">https://github.com/Masaaki-75/meditok</a>. </p>
<blockquote>
<p>å…ˆè¿›çš„è‡ªå›å½’æ¨¡å‹å·²ç»é‡å¡‘äº†å¤šæ¨¡æ€äººå·¥æ™ºèƒ½ã€‚ç„¶è€Œï¼Œç”±äºç¼ºå°‘ç»Ÿä¸€çš„è§†è§‰åˆ†è¯å™¨â€”â€”ä¸€ä¸ªèƒ½å¤Ÿæ•æ‰ç²¾ç»†ç²’åº¦çš„è§†è§‰ç»“æ„ä»¥å®ç°å¿ å®çš„å›¾åƒé‡å»ºå’Œé€¼çœŸçš„å›¾åƒåˆæˆï¼Œä»¥åŠä¸°å¯Œçš„è¯­ä¹‰ä»¥è¿›è¡Œå‡†ç¡®çš„è¯Šæ–­å’Œå›¾åƒè§£é‡Šçš„è§†è§‰åˆ†è¯å™¨ï¼Œå…¶åœ¨åŒ»å­¦æˆåƒä¸­çš„å˜é©æ½œåŠ›åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå°šæœªè¢«å¼€å‘å‡ºæ¥ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºäº†MedITokï¼Œå®ƒæ˜¯é’ˆå¯¹åŒ»å­¦å›¾åƒçš„é¦–ä¸ªç»Ÿä¸€åˆ†è¯å™¨ï¼Œåœ¨ä¸€ä¸ªç»Ÿä¸€çš„æ½œåœ¨ç©ºé—´å†…ç¼–ç ä½çº§åˆ«çš„ç»“æ„ç»†èŠ‚å’Œé«˜çº§åˆ«çš„ä¸´åºŠè¯­ä¹‰ã€‚ä¸ºäº†å¹³è¡¡è¿™äº›ç›¸äº’ç«äº‰çš„ç›®æ ‡ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹çš„ä¸¤é˜¶æ®µè®­ç»ƒæ¡†æ¶ï¼šè§†è§‰è¡¨ç¤ºå¯¹é½é˜¶æ®µï¼Œç”¨è§†è§‰è¯­ä¹‰çº¦æŸå†·å¯åŠ¨åˆ†è¯å™¨é‡å»ºå­¦ä¹ ï¼Œç„¶åæ˜¯æ–‡æœ¬è¯­ä¹‰è¡¨ç¤ºå¯¹é½é˜¶æ®µï¼Œå°†è¯¦ç»†çš„ä¸´åºŠè¯­ä¹‰æ³¨å…¥æ½œåœ¨ç©ºé—´ã€‚MedITokæ˜¯åœ¨ç²¾å¿ƒæ”¶é›†çš„å¤§è§„æ¨¡æ•°æ®é›†ä¸Šè®­ç»ƒçš„ï¼ŒåŒ…å«è¶…è¿‡3000ä¸‡å¼ åŒ»å­¦å›¾åƒå’Œ200ä¸‡å¼ å›¾åƒ-å­—å¹•å¯¹ã€‚å®ƒåœ¨è¶…è¿‡30ä¸ªæ•°æ®é›†ã€9ç§æˆåƒæ¨¡æ€å’Œ4ä¸ªä¸åŒä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚é€šè¿‡ä¸ºè‡ªå›å½’æ¨¡å‹æä¾›ç»Ÿä¸€çš„æ ‡è®°ç©ºé—´ï¼ŒMedITokæ”¯æŒä¸´åºŠè¯Šæ–­å’Œç”ŸæˆåŒ»ç–—åº”ç”¨çš„å¹¿æ³›ä»»åŠ¡ã€‚æ¨¡å‹å’Œä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/Masaaki-75/meditok%E4%B8%8A%E5%85%AC%E5%BC%80%E5%8F%91%E5%B8%AE%E3%80%82">https://github.com/Masaaki-75/meditokä¸Šå…¬å¼€å‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19225v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å…ˆè¿›çš„è‡ªå›å½’æ¨¡å‹å·²ç»é‡å¡‘äº†å¤šæ¨¡æ€äººå·¥æ™ºèƒ½ï¼Œä½†åœ¨åŒ»å­¦æˆåƒé¢†åŸŸçš„åº”ç”¨æ½œåŠ›å°šæœªå¾—åˆ°å……åˆ†å‘æ˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†MedITokï¼Œè¿™æ˜¯ä¸€æ¬¾é’ˆå¯¹åŒ»å­¦å›¾åƒè®¾è®¡çš„ç»Ÿä¸€åˆ†è¯å™¨ï¼Œå¯åœ¨ç»Ÿä¸€çš„æ½œåœ¨ç©ºé—´ä¸­ç¼–ç ä½çº§åˆ«çš„ç»“æ„ç»†èŠ‚å’Œé«˜çº§åˆ«çš„ä¸´åºŠè¯­ä¹‰ã€‚é€šè¿‡å¼•å…¥ä¸¤é˜¶æ®µè®­ç»ƒæ¡†æ¶ï¼Œå®ç°è§†è§‰è¡¨ç¤ºå¯¹é½å’Œæ–‡æœ¬è¯­ä¹‰è¡¨ç¤ºå¯¹é½ï¼Œä½¿MedITokåœ¨è¶…è¿‡30ä¸ªæ•°æ®é›†ã€9ç§æˆåƒæ¨¡æ€å’Œ4ä¸ªä¸åŒä»»åŠ¡ä¸Šå–å¾—äº†æœ€æ–°æŠ€æœ¯æ°´å¹³çš„æ€§èƒ½ã€‚MedITokä¸ºè‡ªå›å½’æ¨¡å‹æä¾›äº†ä¸€ä¸ªç»Ÿä¸€çš„ä»¤ç‰Œç©ºé—´ï¼Œæ”¯æŒä¸´åºŠè¯Šæ–­å’Œç”ŸæˆåŒ»ç–—åº”ç”¨ä¸­çš„å¤šç§ä»»åŠ¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MedITokæ˜¯é¦–ä¸ªé’ˆå¯¹åŒ»å­¦å›¾åƒè®¾è®¡çš„ç»Ÿä¸€åˆ†è¯å™¨ã€‚</li>
<li>å®ƒèƒ½åœ¨ç»Ÿä¸€çš„æ½œåœ¨ç©ºé—´ä¸­ç¼–ç ä½çº§åˆ«çš„ç»“æ„ç»†èŠ‚å’Œé«˜çº§åˆ«çš„ä¸´åºŠè¯­ä¹‰ã€‚</li>
<li>é€šè¿‡ä¸¤é˜¶æ®µè®­ç»ƒæ¡†æ¶å®ç°è§†è§‰å’Œæ–‡æœ¬è¯­ä¹‰è¡¨ç¤ºçš„å¯¹é½ã€‚</li>
<li>MedITokåœ¨å¤šä¸ªæ•°æ®é›†ã€ä¸åŒæˆåƒæ¨¡æ€å’Œä»»åŠ¡ä¸Šå–å¾—äº†æœ€æ–°æŠ€æœ¯æ°´å¹³çš„æ€§èƒ½ã€‚</li>
<li>MedITokæ”¯æŒè‡ªå›å½’æ¨¡å‹åœ¨ä¸´åºŠåŒ»å­¦é¢†åŸŸçš„åº”ç”¨ï¼ŒåŒ…æ‹¬ä¸´åºŠè¯Šæ–­å’Œç”ŸæˆåŒ»ç–—åº”ç”¨ã€‚</li>
<li>è¯¥æ¨¡å‹åœ¨å¤§å‹æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼ŒåŒ…å«è¶…è¿‡3000ä¸‡å¼ åŒ»å­¦å›¾åƒå’Œ200ä¸‡å¼ å›¾åƒ-å­—å¹•å¯¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19225">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cf51181e4a260cdccf0660e11efe6efb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-22d2df5e472a7fdabfc971de0d9bedfe.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-32b6c9dad8651e452162b4d798be181c.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Domain-and-Task-Focused-Example-Selection-for-Data-Efficient-Contrastive-Medical-Image-Segmentation"><a href="#Domain-and-Task-Focused-Example-Selection-for-Data-Efficient-Contrastive-Medical-Image-Segmentation" class="headerlink" title="Domain and Task-Focused Example Selection for Data-Efficient Contrastive   Medical Image Segmentation"></a>Domain and Task-Focused Example Selection for Data-Efficient Contrastive   Medical Image Segmentation</h2><p><strong>Authors:Tyler Ward, Aaron Moseley, Abdullah-Al-Zubaer Imran</strong></p>
<p>Segmentation is one of the most important tasks in the medical imaging pipeline as it influences a number of image-based decisions. To be effective, fully supervised segmentation approaches require large amounts of manually annotated training data. However, the pixel-level annotation process is expensive, time-consuming, and error-prone, hindering progress and making it challenging to perform effective segmentations. Therefore, models must learn efficiently from limited labeled data. Self-supervised learning (SSL), particularly contrastive learning via pre-training on unlabeled data and fine-tuning on limited annotations, can facilitate such limited labeled image segmentation. To this end, we propose a novel self-supervised contrastive learning framework for medical image segmentation, leveraging inherent relationships of different images, dubbed PolyCL. Without requiring any pixel-level annotations or unreasonable data augmentations, our PolyCL learns and transfers context-aware discriminant features useful for segmentation from an innovative surrogate, in a task-related manner. Additionally, we integrate the Segment Anything Model (SAM) into our framework in two novel ways: as a post-processing refinement module that improves the accuracy of predicted masks using bounding box prompts derived from coarse outputs, and as a propagation mechanism via SAM 2 that generates volumetric segmentations from a single annotated 2D slice. Experimental evaluations on three public computed tomography (CT) datasets demonstrate that PolyCL outperforms fully-supervised and self-supervised baselines in both low-data and cross-domain scenarios. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/tbwa233/PolyCL">https://github.com/tbwa233/PolyCL</a>. </p>
<blockquote>
<p>åˆ†å‰²æ˜¯åŒ»å­¦å½±åƒå¤„ç†æµç¨‹ä¸­æœ€é‡è¦çš„ä»»åŠ¡ä¹‹ä¸€ï¼Œå› ä¸ºå®ƒä¼šå½±å“è®¸å¤šåŸºäºå½±åƒçš„å†³ç­–ã€‚ä¸ºäº†å–å¾—è‰¯å¥½æ•ˆæœï¼Œå…¨ç›‘ç£åˆ†å‰²æ–¹æ³•éœ€è¦å¤§é‡æ‰‹åŠ¨æ ‡æ³¨çš„è®­ç»ƒæ•°æ®ã€‚ç„¶è€Œï¼Œåƒç´ çº§æ ‡æ³¨è¿‡ç¨‹æˆæœ¬é«˜æ˜‚ã€è€—æ—¶ä¸”æ˜“å‡ºé”™ï¼Œé˜»ç¢äº†è¿›åº¦ï¼Œä½¿å¾—è¿›è¡Œæœ‰æ•ˆåˆ†å‰²å˜å¾—å…·æœ‰æŒ‘æˆ˜æ€§ã€‚å› æ­¤ï¼Œæ¨¡å‹å¿…é¡»é«˜æ•ˆåœ°ä»å°è§„æ¨¡æ ‡è®°æ•°æ®ä¸­å­¦ä¹ ã€‚è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰å°¤å…¶æ˜¯é€šè¿‡é¢„è®­ç»ƒæ— æ ‡ç­¾æ•°æ®å’Œå¾®è°ƒæœ‰é™æ ‡æ³¨æ•°æ®çš„å¯¹æ¯”å­¦ä¹ ï¼Œå¯ä»¥ä¿ƒè¿›è¿™ç§æœ‰é™æ ‡æ³¨å›¾åƒåˆ†å‰²ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºåŒ»å­¦å›¾åƒåˆ†å‰²çš„æ–°å‹è‡ªç›‘ç£å¯¹æ¯”å­¦ä¹ æ¡†æ¶ï¼Œåˆ©ç”¨ä¸åŒå›¾åƒä¹‹é—´çš„å†…åœ¨å…³ç³»ï¼Œç§°ä¸ºPolyCLã€‚æˆ‘ä»¬çš„PolyCLä¸éœ€è¦ä»»ä½•åƒç´ çº§æ ‡æ³¨æˆ–ä¸åˆç†çš„æ•°æ®å¢å¼ºï¼Œè€Œæ˜¯ä»¥ä¸€ç§ä¸ä»»åŠ¡ç›¸å…³çš„æ–¹å¼ï¼Œä»ä¸€ä¸ªåˆ›æ–°æ›¿ä»£ç‰©ä¸­å­¦ä¹ å¹¶è½¬ç§»æœ‰åŠ©äºåˆ†å‰²çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥åˆ¤åˆ«ç‰¹å¾ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä»¥ä¸¤ç§æ–°é¢–çš„æ–¹å¼å°†â€œåˆ†å‰²ä»»ä½•äº‹ç‰©æ¨¡å‹â€ï¼ˆSAMï¼‰é›†æˆåˆ°æˆ‘ä»¬çš„æ¡†æ¶ä¸­ï¼šä½œä¸ºåå¤„ç†ä¼˜åŒ–æ¨¡å—ï¼Œåˆ©ç”¨æ¥è‡ªç²—ç•¥è¾“å‡ºçš„è¾¹ç•Œæ¡†æç¤ºæé«˜é¢„æµ‹æ©ç çš„å‡†ç¡®æ€§ï¼›ä»¥åŠä½œä¸ºé€šè¿‡SAM 2çš„ä¼ æ’­æœºåˆ¶ï¼Œä»å•ä¸ªæ³¨é‡Šçš„2Dåˆ‡ç‰‡ç”Ÿæˆä½“ç§¯åˆ†å‰²ã€‚åœ¨ä¸‰ä¸ªå…¬å…±è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰æ•°æ®é›†ä¸Šçš„å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒPolyCLåœ¨ä½æ•°æ®é‡å’Œè·¨åŸŸåœºæ™¯ä¸­å‡ä¼˜äºå…¨ç›‘ç£å’Œè‡ªç›‘ç£åŸºçº¿ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/tbwa233/PolyCL%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/tbwa233/PolyCLä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19208v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>åŒ»å­¦å›¾åƒåˆ†å‰²æ˜¯åŒ»å­¦æˆåƒæµç¨‹ä¸­æœ€é‡è¦çš„ä»»åŠ¡ä¹‹ä¸€ï¼Œå› ä¸ºå®ƒå½±å“è®¸å¤šåŸºäºå›¾åƒçš„å†³å®šã€‚ç„¶è€Œï¼Œå®Œå…¨ç›‘ç£çš„åˆ†å‰²æ–¹æ³•éœ€è¦å¤§é‡æ‰‹åŠ¨æ³¨é‡Šçš„è®­ç»ƒæ•°æ®æ‰èƒ½æœ‰æ•ˆã€‚ä½†åƒç´ çº§æ³¨é‡Šè¿‡ç¨‹æ—¢æ˜‚è´µåˆè€—æ—¶ï¼Œè¿˜å®¹æ˜“å‡ºé”™ï¼Œè¿™ä½¿å¾—ä»æœ‰é™æ ‡è®°æ•°æ®ä¸­æœ‰æ•ˆå­¦ä¹ å˜å¾—å…·æœ‰æŒ‘æˆ˜æ€§ã€‚è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰ï¼Œç‰¹åˆ«æ˜¯é€šè¿‡é¢„è®­ç»ƒæ— æ ‡ç­¾æ•°æ®å’Œå¾®è°ƒæœ‰é™æ³¨é‡Šè¿›è¡Œå¯¹æ¯”å­¦ä¹ ï¼Œå¯ä»¥ä¿ƒè¿›æœ‰é™çš„æ ‡è®°å›¾åƒåˆ†å‰²ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„è‡ªæˆ‘ç›‘ç£å¯¹æ¯”å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºåŒ»å­¦å›¾åƒåˆ†å‰²ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨ä¸åŒå›¾åƒä¹‹é—´çš„å†…åœ¨å…³ç³»ï¼Œè¢«ç§°ä¸ºPolyCLã€‚PolyCLä¸éœ€è¦ä»»ä½•åƒç´ çº§æ³¨é‡Šæˆ–ä¸åˆç†çš„æ•°æ®å¢å¼ºï¼Œä»¥ä»»åŠ¡ç›¸å…³çš„æ–¹å¼ä»åˆ›æ–°çš„æ›¿ä»£ç‰©ä¸­å­¦ä¹ å¹¶è½¬ç§»ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„åˆ¤åˆ«ç‰¹å¾ï¼Œè¿™äº›ç‰¹å¾å¯¹äºåˆ†å‰²å¾ˆæœ‰ç”¨ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä»¥ä¸¤ç§æ–°é¢–çš„æ–¹å¼å°†Segment Anything Modelï¼ˆSAMï¼‰é›†æˆåˆ°æˆ‘ä»¬çš„æ¡†æ¶ä¸­ï¼šä½œä¸ºåå¤„ç†ç»†åŒ–æ¨¡å—ï¼Œåˆ©ç”¨æ¥è‡ªç²—ç•¥è¾“å‡ºçš„è¾¹ç•Œæ¡†æç¤ºæé«˜é¢„æµ‹æ©ç çš„å‡†ç¡®æ€§ï¼›ä½œä¸ºSAM 2çš„ä¼ æ’­æœºåˆ¶ï¼Œä»å•ä¸ªæ³¨é‡Šçš„2Dåˆ‡ç‰‡ç”Ÿæˆä½“ç§¯åˆ†å‰²ã€‚åœ¨ä¸‰ä¸ªå…¬å…±è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰æ•°æ®é›†ä¸Šçš„å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒPolyCLåœ¨ä½æ•°æ®å’Œè·¨åŸŸåœºæ™¯ä¸­å‡ä¼˜äºå®Œå…¨ç›‘ç£å’Œè‡ªç›‘ç£çš„åŸºçº¿ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/tbwa233/PolyCL%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/tbwa233/PolyCLä¸Šæ‰¾åˆ°ã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†å‰²æ˜¯åŒ»å­¦æˆåƒä¸­çš„é‡è¦ç¯èŠ‚ï¼Œå¯¹å¤šç§åŸºäºå›¾åƒçš„å†³å®šäº§ç”Ÿå½±å“ã€‚</li>
<li>å®Œå…¨ç›‘ç£çš„åˆ†å‰²æ–¹æ³•éœ€è¦å¤§é‡æ‰‹åŠ¨æ ‡æ³¨çš„è®­ç»ƒæ•°æ®ï¼Œä½†è¿™ä¸€è¿‡ç¨‹æ—¢è€—æ—¶åˆæ˜‚è´µã€‚</li>
<li>è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰å’Œå¯¹æ¯”å­¦ä¹ åœ¨æœ‰é™çš„æ ‡è®°æ•°æ®ä¸Šä¿ƒè¿›åŒ»å­¦å›¾åƒåˆ†å‰²ã€‚</li>
<li>PolyCLæ¡†æ¶åˆ©ç”¨æ— æ ‡ç­¾æ•°æ®çš„é¢„è®­ç»ƒå’Œæœ‰é™æ³¨é‡Šçš„å¾®è°ƒï¼Œæ— éœ€åƒç´ çº§æ³¨é‡Šæˆ–ä¸åˆç†çš„æ•°æ®å¢å¼ºã€‚</li>
<li>PolyCLæ¡†æ¶é€šè¿‡å­¦ä¹ ä»»åŠ¡ç›¸å…³çš„æ–¹å¼ä»æ›¿ä»£ç‰©ä¸­å­¦ä¹ ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„åˆ¤åˆ«ç‰¹å¾ã€‚</li>
<li>Segment Anything Modelï¼ˆSAMï¼‰ä»¥ä¸¤ç§æ–¹å¼é›†æˆåˆ°PolyCLæ¡†æ¶ä¸­ï¼šä½œä¸ºåå¤„ç†ç»†åŒ–æ¨¡å—å’Œæé«˜é¢„æµ‹æ©ç å‡†ç¡®æ€§çš„ä¼ æ’­æœºåˆ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19208">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2b065fd8b0e2b90b058e92dc23436670.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e81e7cacea08a971c4f9536139bc306f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b7e47825f6d5bfeff0b22da4a1be7459.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-54042b707075d18d4db98ee14c047c9b.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="CDPDNet-Integrating-Text-Guidance-with-Hybrid-Vision-Encoders-for-Medical-Image-Segmentation"><a href="#CDPDNet-Integrating-Text-Guidance-with-Hybrid-Vision-Encoders-for-Medical-Image-Segmentation" class="headerlink" title="CDPDNet: Integrating Text Guidance with Hybrid Vision Encoders for   Medical Image Segmentation"></a>CDPDNet: Integrating Text Guidance with Hybrid Vision Encoders for   Medical Image Segmentation</h2><p><strong>Authors:Jiong Wu, Yang Xing, Boxiao Yu, Wei Shao, Kuang Gong</strong></p>
<p>Most publicly available medical segmentation datasets are only partially labeled, with annotations provided for a subset of anatomical structures. When multiple datasets are combined for training, this incomplete annotation poses challenges, as it limits the modelâ€™s ability to learn shared anatomical representations among datasets. Furthermore, vision-only frameworks often fail to capture complex anatomical relationships and task-specific distinctions, leading to reduced segmentation accuracy and poor generalizability to unseen datasets. In this study, we proposed a novel CLIP-DINO Prompt-Driven Segmentation Network (CDPDNet), which combined a self-supervised vision transformer with CLIP-based text embedding and introduced task-specific text prompts to tackle these challenges. Specifically, the framework was constructed upon a convolutional neural network (CNN) and incorporated DINOv2 to extract both fine-grained and global visual features, which were then fused using a multi-head cross-attention module to overcome the limited long-range modeling capability of CNNs. In addition, CLIP-derived text embeddings were projected into the visual space to help model complex relationships among organs and tumors. To further address the partial label challenge and enhance inter-task discriminative capability, a Text-based Task Prompt Generation (TTPG) module that generated task-specific prompts was designed to guide the segmentation. Extensive experiments on multiple medical imaging datasets demonstrated that CDPDNet consistently outperformed existing state-of-the-art segmentation methods. Code and pretrained model are available at: <a target="_blank" rel="noopener" href="https://github.com/wujiong-hub/CDPDNet.git">https://github.com/wujiong-hub/CDPDNet.git</a>. </p>
<blockquote>
<p>å¤§éƒ¨åˆ†å…¬å¼€å¯ç”¨çš„åŒ»å­¦åˆ†å‰²æ•°æ®é›†ä»…éƒ¨åˆ†æ ‡æ³¨ï¼Œåªä¸ºéƒ¨åˆ†è§£å‰–ç»“æ„æä¾›æ³¨é‡Šã€‚å½“å¤šä¸ªæ•°æ®é›†ç»„åˆè¿›è¡Œè®­ç»ƒæ—¶ï¼Œè¿™ç§ä¸å®Œå…¨çš„æ³¨é‡Šå¸¦æ¥äº†æŒ‘æˆ˜ï¼Œå› ä¸ºå®ƒé™åˆ¶äº†æ¨¡å‹åœ¨æ•°æ®é›†ä¹‹é—´å­¦ä¹ å…±äº«è§£å‰–è¡¨å¾çš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œä»…ä¾èµ–è§†è§‰çš„æ¡†æ¶é€šå¸¸æ— æ³•æ•æ‰å¤æ‚çš„è§£å‰–å…³ç³»å’Œä»»åŠ¡ç‰¹å®šçš„åŒºåˆ«ï¼Œä»è€Œå¯¼è‡´åˆ†å‰²ç²¾åº¦é™ä½ï¼Œå¹¶ä¸”éš¾ä»¥æ¨å¹¿åˆ°æœªè§è¿‡çš„æ•°æ®é›†ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„CLIP-DINO Prompt-Driven Segmentation Networkï¼ˆCDPDNetï¼‰ï¼Œå®ƒå°†è‡ªæˆ‘ç›‘ç£çš„è§†è§‰å˜å‹å™¨ä¸CLIPåŸºäºæ–‡æœ¬çš„åµŒå…¥ç›¸ç»“åˆï¼Œå¹¶å¼•å…¥ä»»åŠ¡ç‰¹å®šçš„æ–‡æœ¬æç¤ºæ¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚å…·ä½“æ¥è¯´ï¼Œè¯¥æ¡†æ¶å»ºç«‹åœ¨å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ä¹‹ä¸Šï¼Œå¹¶èåˆäº†DINOv2ä»¥æå–ç²¾ç»†ç²’åº¦å’Œå…¨å±€è§†è§‰ç‰¹å¾ï¼Œç„¶åä½¿ç”¨å¤šå¤´äº¤å‰æ³¨æ„æ¨¡å—èåˆè¿™äº›ç‰¹å¾ï¼Œä»¥å…‹æœCNNæœ‰é™çš„é•¿æœŸå»ºæ¨¡èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒCLIPè¡ç”Ÿçš„æ–‡æœ¬åµŒå…¥è¢«æŠ•å°„åˆ°è§†è§‰ç©ºé—´ä¸­ï¼Œä»¥å¸®åŠ©å»ºæ¨¡å™¨å®˜å’Œè‚¿ç˜¤ä¹‹é—´çš„å¤æ‚å…³ç³»ã€‚ä¸ºäº†è¿›ä¸€æ­¥è§£å†³éƒ¨åˆ†æ ‡ç­¾æŒ‘æˆ˜å¹¶å¢å¼ºä»»åŠ¡é—´çš„è¾¨åˆ«èƒ½åŠ›ï¼Œè®¾è®¡äº†ä¸€ä¸ªåŸºäºæ–‡æœ¬çš„ä»»åŠ¡æç¤ºç”Ÿæˆï¼ˆTTPGï¼‰æ¨¡å—ï¼Œä»¥ç”Ÿæˆç‰¹å®šä»»åŠ¡çš„æç¤ºæ¥æŒ‡å¯¼åˆ†å‰²ã€‚åœ¨å¤šä¸ªåŒ»å­¦å½±åƒæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒCDPDNetå§‹ç»ˆä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›çš„åˆ†å‰²æ–¹æ³•ã€‚ä»£ç å’Œé¢„å…ˆè®­ç»ƒçš„æ¨¡å‹å¯åœ¨[<a target="_blank" rel="noopener" href="https://github.com/wujiong-hub/CDPDNet.git%E6%89%BE%E5%88%B0%E3%80%82]">https://github.com/wujiong-hub/CDPDNet.gitæ‰¾åˆ°ã€‚]</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.18958v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹çš„CLIP-DINO Prompté©±åŠ¨åˆ†å‰²ç½‘ç»œï¼ˆCDPDNetï¼‰ï¼Œé€šè¿‡ç»“åˆè‡ªç›‘ç£è§†è§‰å˜æ¢å™¨ã€CLIPåŸºç¡€æ–‡æœ¬åµŒå…¥å’Œç‰¹å®šä»»åŠ¡æ–‡æœ¬æç¤ºï¼Œè§£å†³åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­é¢ä¸´çš„éƒ¨åˆ†æ ‡æ³¨å’Œå¤æ‚è§£å‰–å…³ç³»çš„é—®é¢˜ã€‚å®éªŒè¯æ˜ï¼ŒCDPDNetåœ¨å¤šä¸ªåŒ»å­¦æˆåƒæ•°æ®é›†ä¸Šçš„è¡¨ç°å‡ä¼˜äºç°æœ‰å…ˆè¿›åˆ†å‰²æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CDPDNetç»“åˆè‡ªç›‘ç£è§†è§‰å˜æ¢å™¨å’ŒCLIPåŸºç¡€æ–‡æœ¬åµŒå…¥ï¼Œè§£å†³åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„éƒ¨åˆ†æ ‡æ³¨é—®é¢˜ã€‚</li>
<li>é€šè¿‡å¼•å…¥ç‰¹å®šä»»åŠ¡æ–‡æœ¬æç¤ºï¼ŒCDPDNetèƒ½å¤Ÿæ•æ‰å¤æ‚çš„è§£å‰–å…³ç³»ï¼Œæé«˜åˆ†å‰²ç²¾åº¦ã€‚</li>
<li>CDPDNeté‡‡ç”¨å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å’ŒDINOv2æå–ç²¾ç»†é¢—ç²’å’Œå…¨å±€è§†è§‰ç‰¹å¾ï¼Œç„¶åé€šè¿‡å¤šå¤´äº¤å‰æ³¨æ„åŠ›æ¨¡å—èåˆè¿™äº›ç‰¹å¾ï¼Œä»¥å…‹æœCNNçš„é•¿æœŸå»ºæ¨¡èƒ½åŠ›é™åˆ¶ã€‚</li>
<li>CLIPè¡ç”Ÿçš„æ–‡æœ¬åµŒå…¥è¢«æŠ•å°„åˆ°è§†è§‰ç©ºé—´ï¼Œä»¥å¸®åŠ©æ¨¡å‹æ¨¡æ‹Ÿå™¨å®˜å’Œè‚¿ç˜¤ä¹‹é—´çš„å¤æ‚å…³ç³»ã€‚</li>
<li>ä¸ºåº”å¯¹éƒ¨åˆ†æ ‡ç­¾æŒ‘æˆ˜å’Œæé«˜ä»»åŠ¡é—´çš„è¾¨åˆ«èƒ½åŠ›ï¼Œè®¾è®¡äº†åŸºäºæ–‡æœ¬çš„ä»»åŠ¡æç¤ºç”Ÿæˆï¼ˆTTPGï¼‰æ¨¡å—ã€‚</li>
<li>åœ¨å¤šä¸ªåŒ»å­¦æˆåƒæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¯æ˜ï¼ŒCDPDNetçš„æ€§èƒ½ä¼˜äºå…¶ä»–å…ˆè¿›çš„åˆ†å‰²æ–¹æ³•ã€‚</li>
<li>ä»£ç å’Œé¢„å…ˆè®­ç»ƒçš„æ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/wujiong-hub/CDPDNet.git%E4%B8%8A%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/wujiong-hub/CDPDNet.gitä¸Šè·å–ã€‚</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.18958">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-008555b808d4437acca293ee2013d487.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1caa4ca25d9da3c713d834debb18b05b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d5e4448fd40791b96a39fe2b6a143e81.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a597aaeb4933c4e5f5eed4b2f45bd828.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c3656a1fb73ee40b14268f03ed2ad006.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Are-Vision-Language-Models-Ready-for-Clinical-Diagnosis-A-3D-Medical-Benchmark-for-Tumor-centric-Visual-Question-Answering"><a href="#Are-Vision-Language-Models-Ready-for-Clinical-Diagnosis-A-3D-Medical-Benchmark-for-Tumor-centric-Visual-Question-Answering" class="headerlink" title="Are Vision Language Models Ready for Clinical Diagnosis? A 3D Medical   Benchmark for Tumor-centric Visual Question Answering"></a>Are Vision Language Models Ready for Clinical Diagnosis? A 3D Medical   Benchmark for Tumor-centric Visual Question Answering</h2><p><strong>Authors:Yixiong Chen, Wenjie Xiao, Pedro R. A. S. Bassi, Xinze Zhou, Sezgin Er, Ibrahim Ethem Hamamci, Zongwei Zhou, Alan Yuille</strong></p>
<p>Vision-Language Models (VLMs) have shown promise in various 2D visual tasks, yet their readiness for 3D clinical diagnosis remains unclear due to stringent demands for recognition precision, reasoning ability, and domain knowledge. To systematically evaluate these dimensions, we present DeepTumorVQA, a diagnostic visual question answering (VQA) benchmark targeting abdominal tumors in CT scans. It comprises 9,262 CT volumes (3.7M slices) from 17 public datasets, with 395K expert-level questions spanning four categories: Recognition, Measurement, Visual Reasoning, and Medical Reasoning. DeepTumorVQA introduces unique challenges, including small tumor detection and clinical reasoning across 3D anatomy. Benchmarking four advanced VLMs (RadFM, M3D, Merlin, CT-CHAT), we find current models perform adequately on measurement tasks but struggle with lesion recognition and reasoning, and are still not meeting clinical needs. Two key insights emerge: (1) large-scale multimodal pretraining plays a crucial role in DeepTumorVQA testing performance, making RadFM stand out among all VLMs. (2) Our dataset exposes critical differences in VLM components, where proper image preprocessing and design of vision modules significantly affect 3D perception. To facilitate medical multimodal research, we have released DeepTumorVQA as a rigorous benchmark: <a target="_blank" rel="noopener" href="https://github.com/Schuture/DeepTumorVQA">https://github.com/Schuture/DeepTumorVQA</a>. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å„ç§2Dè§†è§‰ä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ï¼Œä½†ç”±äºå¯¹è¯†åˆ«ç²¾åº¦ã€æ¨ç†èƒ½åŠ›å’Œé¢†åŸŸçŸ¥è¯†çš„ä¸¥æ ¼éœ€æ±‚ï¼Œå®ƒä»¬æ˜¯å¦å‡†å¤‡å¥½ç”¨äº3Dä¸´åºŠè¯Šæ–­ä»ç„¶ä¸æ˜ç¡®ã€‚ä¸ºäº†ç³»ç»Ÿåœ°è¯„ä¼°è¿™äº›æ–¹é¢ï¼Œæˆ‘ä»¬æ¨å‡ºäº†DeepTumorVQAï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹è…¹éƒ¨è‚¿ç˜¤CTæ‰«æçš„è¯Šæ–­è§†è§‰é—®ç­”ï¼ˆVQAï¼‰åŸºå‡†æµ‹è¯•ã€‚å®ƒåŒ…å«æ¥è‡ª17ä¸ªå…¬å…±æ•°æ®é›†çš„9ï¼Œ262ä¸ªCTä½“ç§¯ï¼ˆ370ä¸‡åˆ‡ç‰‡ï¼‰ï¼Œæ¶µç›–å››ä¸ªç±»åˆ«çš„ä¸“å®¶çº§é—®é¢˜ï¼Œå…±39.5ä¸‡ä¸ªï¼šè¯†åˆ«ã€æµ‹é‡ã€è§†è§‰æ¨ç†å’ŒåŒ»å­¦æ¨ç†ã€‚DeepTumorVQAå¸¦æ¥äº†ç‹¬ç‰¹çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬åœ¨ä¸‰ç»´è§£å‰–ç»“æ„ä¸­çš„å°è‚¿ç˜¤æ£€æµ‹å’Œä¸´åºŠæ¨ç†ã€‚å¯¹å››ä¸ªå…ˆè¿›çš„VLMï¼ˆRadFMã€M3Dã€Merlinã€CT-CHATï¼‰è¿›è¡ŒåŸºå‡†æµ‹è¯•åï¼Œæˆ‘ä»¬å‘ç°å½“å‰æ¨¡å‹åœ¨æµ‹é‡ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨ç—…ç¶è¯†åˆ«å’Œæ¨ç†æ–¹é¢é‡åˆ°å›°éš¾ï¼Œä»ä¸èƒ½æ»¡è¶³ä¸´åºŠéœ€æ±‚ã€‚å‡ºç°äº†ä¸¤ä¸ªå…³é”®è§è§£ï¼šï¼ˆ1ï¼‰å¤§è§„æ¨¡å¤šæ¨¡å¼é¢„è®­ç»ƒåœ¨DeepTumorVQAæµ‹è¯•æ€§èƒ½ä¸­èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ï¼Œä½¿RadFMåœ¨æ‰€æœ‰VLMä¸­è„±é¢–è€Œå‡ºã€‚ï¼ˆ2ï¼‰æˆ‘ä»¬çš„æ•°æ®é›†æš´éœ²äº†VLMç»„ä»¶ä¹‹é—´çš„å…³é”®å·®å¼‚ï¼Œå…¶ä¸­é€‚å½“çš„å›¾åƒé¢„å¤„ç†å’Œè§†è§‰æ¨¡å—çš„è®¾è®¡å¯¹3Dæ„ŸçŸ¥æœ‰é‡å¤§å½±å“ã€‚ä¸ºäº†ä¿ƒè¿›åŒ»å­¦å¤šæ¨¡å¼ç ”ç©¶ï¼Œæˆ‘ä»¬å·²ç»å°†DeepTumorVQAä½œä¸ºä¸¥æ ¼çš„åŸºå‡†æµ‹è¯•å‘å¸ƒï¼š<a target="_blank" rel="noopener" href="https://github.com/Schuture/DeepTumorVQA%E3%80%82">https://github.com/Schuture/DeepTumorVQAã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.18915v1">PDF</a> NeurIPS 2025 datasets&amp;benchmarks track submission</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹è…¹éƒ¨è‚¿ç˜¤åœ¨CTæ‰«æä¸­çš„è¯Šæ–­è§†è§‰é—®ç­”ï¼ˆVQAï¼‰åŸºå‡†æµ‹è¯•DeepTumorVQAã€‚è¯¥åŸºå‡†æµ‹è¯•åŒ…å«æ¥è‡ª17ä¸ªå…¬å…±æ•°æ®é›†çš„9262ä¸ªCTä½“ç§¯ï¼ˆ370ä¸‡åˆ‡ç‰‡ï¼‰ï¼Œä»¥åŠæ¶µç›–è¯†åˆ«ã€æµ‹é‡ã€è§†è§‰æ¨ç†å’ŒåŒ»å­¦æ¨ç†å››ä¸ªç±»åˆ«çš„39.5ä¸‡ä¸“å®¶çº§é—®é¢˜ã€‚è¯„ä¼°äº†å››ç§å…ˆè¿›çš„VLMæ¨¡å‹ï¼Œå‘ç°å½“å‰æ¨¡å‹åœ¨æµ‹é‡ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨ç—…ç¶è¯†åˆ«å’Œæ¨ç†æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œå°šä¸èƒ½æ»¡è¶³ä¸´åºŠéœ€æ±‚ã€‚å¤§è§„æ¨¡å¤šæ¨¡å¼é¢„è®­ç»ƒåœ¨DeepTumorVQAæµ‹è¯•ä¸­èµ·ç€å…³é”®ä½œç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DeepTumorVQAæ˜¯ä¸€ä¸ªé’ˆå¯¹è…¹éƒ¨è‚¿ç˜¤CTæ‰«æçš„è¯Šæ–­è§†è§‰é—®ç­”ï¼ˆVQAï¼‰åŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–äº†å¤šç§ä»»åŠ¡ç±»å‹ï¼ŒåŒ…æ‹¬è¯†åˆ«ã€æµ‹é‡ã€è§†è§‰æ¨ç†å’ŒåŒ»å­¦æ¨ç†ã€‚</li>
<li>å½“å‰VLMæ¨¡å‹åœ¨æµ‹é‡ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨ç—…ç¶è¯†åˆ«å’Œæ¨ç†æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œéš¾ä»¥æ»¡è¶³ä¸´åºŠéœ€æ±‚ã€‚</li>
<li>å¤§è§„æ¨¡å¤šæ¨¡å¼é¢„è®­ç»ƒå¯¹DeepTumorVQAæµ‹è¯•æ€§èƒ½è‡³å…³é‡è¦ã€‚</li>
<li>DeepTumorVQAæ•°æ®é›†æ­ç¤ºäº†VLMç»„ä»¶çš„å…³é”®å·®å¼‚ï¼Œé€‚å½“çš„å›¾åƒé¢„å¤„ç†å’Œè§†è§‰æ¨¡å—çš„è®¾è®¡å¯¹3Dæ„ŸçŸ¥æœ‰é‡å¤§å½±å“ã€‚</li>
<li>DeepTumorVQAå·²ä½œä¸ºä¸¥æ ¼çš„åŸºå‡†æµ‹è¯•å‘å¸ƒï¼Œä»¥æ¨åŠ¨åŒ»ç–—å¤šæ¨¡å¼ç ”ç©¶ã€‚</li>
<li>é€šè¿‡æ¯”è¾ƒä¸åŒçš„VLMæ¨¡å‹ï¼Œå‘ç°RadFMåœ¨DeepTumorVQAæµ‹è¯•ä¸­è¡¨ç°çªå‡ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.18915">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f637ce6a2e9aaed1b4f749b4128d6707.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e1ea7f5db06bc1e208e6cb9e349a4c23.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e583c5a957b88d67c777f878067f4160.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7768599038fb5040beec5e5347483499.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a7ed1a5fd9aff5e788a2f26b07df4beb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b4c83df0fdb440071c0caca1bf89939b.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="MSLAU-Net-A-Hybird-CNN-Transformer-Network-for-Medical-Image-Segmentation"><a href="#MSLAU-Net-A-Hybird-CNN-Transformer-Network-for-Medical-Image-Segmentation" class="headerlink" title="MSLAU-Net: A Hybird CNN-Transformer Network for Medical Image   Segmentation"></a>MSLAU-Net: A Hybird CNN-Transformer Network for Medical Image   Segmentation</h2><p><strong>Authors:Libin Lan, Yanxin Li, Xiaojuan Liu, Juan Zhou, Jianxun Zhang, Nannan Huang, Yudong Zhang</strong></p>
<p>Both CNN-based and Transformer-based methods have achieved remarkable success in medical image segmentation tasks. However, CNN-based methods struggle to effectively capture global contextual information due to the inherent limitations of convolution operations. Meanwhile, Transformer-based methods suffer from insufficient local feature modeling and face challenges related to the high computational complexity caused by the self-attention mechanism. To address these limitations, we propose a novel hybrid CNN-Transformer architecture, named MSLAU-Net, which integrates the strengths of both paradigms. The proposed MSLAU-Net incorporates two key ideas. First, it introduces Multi-Scale Linear Attention, designed to efficiently extract multi-scale features from medical images while modeling long-range dependencies with low computational complexity. Second, it adopts a top-down feature aggregation mechanism, which performs multi-level feature aggregation and restores spatial resolution using a lightweight structure. Extensive experiments conducted on benchmark datasets covering three imaging modalities demonstrate that the proposed MSLAU-Net outperforms other state-of-the-art methods on nearly all evaluation metrics, validating the superiority, effectiveness, and robustness of our approach. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/Monsoon49/MSLAU-Net">https://github.com/Monsoon49/MSLAU-Net</a>. </p>
<blockquote>
<p>åŸºäºCNNçš„æ–¹æ³•å’ŒåŸºäºTransformerçš„æ–¹æ³•åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­éƒ½å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸã€‚ç„¶è€Œï¼Œç”±äºå·ç§¯æ“ä½œå›ºæœ‰çš„å±€é™æ€§ï¼ŒåŸºäºCNNçš„æ–¹æ³•åœ¨æœ‰æ•ˆåœ°æ•è·å…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯æ–¹é¢å­˜åœ¨å›°éš¾ã€‚åŒæ—¶ï¼ŒåŸºäºTransformerçš„æ–¹æ³•åœ¨å±€éƒ¨ç‰¹å¾å»ºæ¨¡æ–¹é¢ä¸è¶³ï¼Œå¹¶ä¸”é¢ä¸´ç”±è‡ªæ³¨æ„åŠ›æœºåˆ¶å¼•èµ·çš„é«˜è®¡ç®—å¤æ‚åº¦çš„æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„æ··åˆCNN-Transformeræ¶æ„ï¼Œåä¸ºMSLAU-Netï¼Œå®ƒç»“åˆäº†ä¸¤ç§èŒƒå¼çš„å¥½å¤„ã€‚æ‰€æå‡ºçš„MSLAU-NetåŒ…å«ä¸¤ä¸ªå…³é”®æ€æƒ³ã€‚é¦–å…ˆï¼Œå®ƒå¼•å…¥äº†å¤šå°ºåº¦çº¿æ€§æ³¨æ„åŠ›ï¼Œæ—¨åœ¨ä»åŒ»å­¦å›¾åƒä¸­æœ‰æ•ˆåœ°æå–å¤šå°ºåº¦ç‰¹å¾ï¼ŒåŒæ—¶ä»¥ä½è®¡ç®—å¤æ‚åº¦å¯¹é•¿è·ç¦»ä¾èµ–å…³ç³»è¿›è¡Œå»ºæ¨¡ã€‚å…¶æ¬¡ï¼Œå®ƒé‡‡ç”¨è‡ªä¸Šè€Œä¸‹çš„ç‰¹å¾èšåˆæœºåˆ¶ï¼Œè¿›è¡Œå¤šçº§ç‰¹å¾èšåˆï¼Œå¹¶ä½¿ç”¨è½»é‡çº§ç»“æ„æ¢å¤ç©ºé—´åˆ†è¾¨ç‡ã€‚åœ¨æ¶µç›–ä¸‰ç§æˆåƒæ¨¡å¼çš„åŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„MSLAU-Netå‡ ä¹åœ¨æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡ä¸Šéƒ½ä¼˜äºå…¶ä»–æœ€å…ˆè¿›çš„æ–¹æ³•ï¼ŒéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•çš„ä¼˜è¶Šæ€§ã€æœ‰æ•ˆæ€§å’Œç¨³å¥æ€§ã€‚æˆ‘ä»¬çš„ä»£ç ä½äº<a target="_blank" rel="noopener" href="https://github.com/Monsoon49/MSLAU-Net%E3%80%82">https://github.com/Monsoon49/MSLAU-Netã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.18823v1">PDF</a> 13 pages, 7 figures, 7 tables</p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­ï¼ŒCNNå’ŒTransformeræ–¹æ³•å‡æœ‰æ˜¾è‘—æˆæ•ˆï¼Œä½†å„æœ‰ä¸è¶³ã€‚ä¸ºç»“åˆä¸¤è€…ä¼˜åŠ¿ï¼Œæå‡ºåä¸ºMSLAU-Netçš„æ··åˆCNN-Transformeræ¶æ„ã€‚å®ƒå¼•å…¥å¤šå°ºåº¦çº¿æ€§æ³¨æ„åŠ›æœºåˆ¶ï¼Œå®ç°é«˜æ•ˆå¤šå°ºåº¦ç‰¹å¾æå–å’Œé•¿è·ç¦»ä¾èµ–å»ºæ¨¡ï¼ŒåŒæ—¶é‡‡ç”¨è‡ªä¸Šè€Œä¸‹çš„ç‰¹å¾èšåˆæœºåˆ¶ï¼Œå®ç°å¤šå±‚çº§ç‰¹å¾èšåˆå’Œç©ºé—´åˆ†è¾¨ç‡æ¢å¤ã€‚åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMSLAU-Netåœ¨å„é¡¹è¯„ä¼°æŒ‡æ ‡ä¸Šå‡è¶…è¶Šå…¶ä»–å…ˆè¿›æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CNNå’ŒTransformeræ–¹æ³•åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­å‡å–å¾—æ˜¾è‘—æˆæ•ˆï¼Œä½†å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>CNNæ–¹æ³•éš¾ä»¥æœ‰æ•ˆæ•æ‰å…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œè€ŒTransformeræ–¹æ³•åˆ™é¢ä¸´å±€éƒ¨ç‰¹å¾å»ºæ¨¡ä¸è¶³å’Œè®¡ç®—å¤æ‚åº¦é«˜çš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºåä¸ºMSLAU-Netçš„æ··åˆCNN-Transformeræ¶æ„ï¼Œç»“åˆä¸¤è€…ä¼˜åŠ¿ã€‚</li>
<li>MSLAU-Netå¼•å…¥å¤šå°ºåº¦çº¿æ€§æ³¨æ„åŠ›æœºåˆ¶ï¼Œå®ç°å¤šå°ºåº¦ç‰¹å¾æå–å’Œé•¿è·ç¦»ä¾èµ–å»ºæ¨¡ã€‚</li>
<li>MSLAU-Neté‡‡ç”¨è‡ªä¸Šè€Œä¸‹çš„ç‰¹å¾èšåˆæœºåˆ¶ï¼Œå®ç°å¤šå±‚çº§ç‰¹å¾èšåˆå’Œç©ºé—´åˆ†è¾¨ç‡æ¢å¤ã€‚</li>
<li>åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMSLAU-Netåœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.18823">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d47ae92c31b299ce5650b611abf722eb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-088fe6a81f2b992b46afee745d383601.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c976f13a94c7e682fa3664076ec24fea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-be978102231d60c0bd0f1d0511a99bba.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="HyperFake-Hyperspectral-Reconstruction-and-Attention-Guided-Analysis-for-Advanced-Deepfake-Detection"><a href="#HyperFake-Hyperspectral-Reconstruction-and-Attention-Guided-Analysis-for-Advanced-Deepfake-Detection" class="headerlink" title="HyperFake: Hyperspectral Reconstruction and Attention-Guided Analysis   for Advanced Deepfake Detection"></a>HyperFake: Hyperspectral Reconstruction and Attention-Guided Analysis   for Advanced Deepfake Detection</h2><p><strong>Authors:Pavan C Shekar, Pawan Soni, Vivek Kanhangad</strong></p>
<p>Deepfakes pose a significant threat to digital media security, with current detection methods struggling to generalize across different manipulation techniques and datasets. While recent approaches combine CNN-based architectures with Vision Transformers or leverage multi-modal learning, they remain limited by the inherent constraints of RGB data. We introduce HyperFake, a novel deepfake detection pipeline that reconstructs 31-channel hyperspectral data from standard RGB videos, revealing hidden manipulation traces invisible to conventional methods. Using an improved MST++ architecture, HyperFake enhances hyperspectral reconstruction, while a spectral attention mechanism selects the most critical spectral features for deepfake detection. The refined spectral data is then processed by an EfficientNet-based classifier optimized for spectral analysis, enabling more accurate and generalizable detection across different deepfake styles and datasets, all without the need for expensive hyperspectral cameras. To the best of our knowledge, this is the first approach to leverage hyperspectral imaging reconstruction for deepfake detection, opening new possibilities for detecting increasingly sophisticated manipulations. </p>
<blockquote>
<p>æ·±åº¦ä¼ªé€ æŠ€æœ¯å¯¹æ•°å­—åª’ä½“å®‰å…¨æ„æˆäº†é‡å¤§å¨èƒï¼Œç°æœ‰çš„æ£€æµ‹æ–¹æ³•åœ¨åº”å¯¹ä¸åŒçš„æ“ä½œæŠ€æœ¯å’Œæ•°æ®é›†æ—¶ï¼Œæ³›åŒ–èƒ½åŠ›æœ‰é™ã€‚è™½ç„¶æœ€è¿‘çš„æ–¹æ³•ç»“åˆäº†åŸºäºCNNçš„æ¶æ„ä¸è§†è§‰å˜å‹å™¨ï¼Œæˆ–è€…åˆ©ç”¨å¤šæ¨¡æ€å­¦ä¹ ï¼Œä½†å®ƒä»¬ä»ç„¶å—åˆ°RGBæ•°æ®å›ºæœ‰çº¦æŸçš„é™åˆ¶ã€‚æˆ‘ä»¬å¼•å…¥äº†HyperFakeï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æ·±åº¦ä¼ªé€ æ£€æµ‹æµç¨‹ï¼Œå®ƒèƒ½å¤Ÿä»æ ‡å‡†RGBè§†é¢‘ä¸­é‡å»º31é€šé“çš„è¶…é«˜å…‰è°±æ•°æ®ï¼Œæ­ç¤ºä¼ ç»Ÿæ–¹æ³•æ— æ³•å¯Ÿè§‰çš„éšè—æ“ä½œç—•è¿¹ã€‚ä½¿ç”¨æ”¹è¿›çš„MST++æ¶æ„ï¼ŒHyperFakeå¢å¼ºäº†è¶…é«˜å…‰è°±é‡å»ºï¼ŒåŒæ—¶å…‰è°±æ³¨æ„æœºåˆ¶é€‰æ‹©äº†å¯¹æ·±åº¦ä¼ªé€ æ£€æµ‹æœ€å…³é”®çš„å…‰è°±ç‰¹å¾ã€‚ç„¶åï¼Œç»è¿‡ç»†åŒ–çš„å…‰è°±æ•°æ®ç”±é’ˆå¯¹å…‰è°±åˆ†æä¼˜åŒ–çš„EfficientNetåˆ†ç±»å™¨è¿›è¡Œå¤„ç†ï¼Œèƒ½å¤Ÿåœ¨ä¸åŒçš„æ·±åº¦ä¼ªé€ é£æ ¼å’Œæ•°æ®é›†ä¸Šå®ç°æ›´å‡†ç¡®ã€æ›´é€šç”¨çš„æ£€æµ‹ï¼Œè€Œä¸”æ— éœ€æ˜‚è´µçš„è¶…é«˜å…‰è°±ç›¸æœºã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯é¦–æ¬¡åˆ©ç”¨è¶…é«˜å…‰è°±æˆåƒé‡å»ºè¿›è¡Œæ·±åº¦ä¼ªé€ æ£€æµ‹çš„æ–¹æ³•ï¼Œä¸ºæ£€æµ‹æ—¥ç›Šå¤æ‚çš„æ“ä½œå¼€è¾Ÿäº†æ–°é€”å¾„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.18587v1">PDF</a> 6 pages, 3 figures, 1 table. Preliminary results on FaceForensics++   dataset. First approach to use hyperspectral reconstruction for deepfake   detection</p>
<p><strong>Summary</strong><br>æ·±åº¦ä¼ªé€ æŠ€æœ¯å¯¹æ•°å­—åª’ä½“å®‰å…¨æ„æˆé‡å¤§å¨èƒï¼Œå½“å‰æ£€æµ‹æ‰‹æ®µåœ¨åº”å¯¹ä¸åŒä¼ªé€ æŠ€æœ¯å’Œæ•°æ®é›†æ—¶å­˜åœ¨æ³›åŒ–å›°éš¾çš„é—®é¢˜ã€‚æœ¬æ–‡æå‡ºHyperFakeï¼Œä¸€ç§æ–°å‹æ·±åº¦ä¼ªé€ æ£€æµ‹æµç¨‹ï¼Œé€šè¿‡é‡å»º31é€šé“çš„è¶…é«˜å…‰è°±æ•°æ®ï¼Œæ­ç¤ºä¼ ç»Ÿæ–¹æ³•æ— æ³•å¯Ÿè§‰çš„æ“çºµç—•è¿¹ã€‚åˆ©ç”¨æ”¹è¿›åçš„MST++æ¶æ„å’Œå…‰è°±æ³¨æ„åŠ›æœºåˆ¶ï¼ŒHyperFakeæé«˜äº†è¶…é«˜å…‰è°±æ•°æ®çš„é‡å»ºèƒ½åŠ›ï¼Œå¹¶é€‰æ‹©æœ€å…³é”®çš„å…‰è°±ç‰¹å¾è¿›è¡Œæ·±åº¦ä¼ªé€ æ£€æµ‹ã€‚ç»è¿‡ä¼˜åŒ–çš„EfficientNetåˆ†ç±»å™¨å¯¹å…‰è°±æ•°æ®è¿›è¡Œåˆ†æï¼Œæé«˜äº†åœ¨ä¸åŒæ·±åº¦ä¼ªé€ æ–¹å¼å’Œæ•°æ®é›†ä¸Šçš„æ£€æµ‹å‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼Œä¸”æ— éœ€æ˜‚è´µçš„è¶…é«˜å…‰è°±ç›¸æœºã€‚è¿™æ˜¯é¦–æ¬¡å°è¯•åˆ©ç”¨è¶…é«˜å…‰è°±æˆåƒé‡å»ºæŠ€æœ¯è¿›è¡Œæ·±åº¦ä¼ªé€ æ£€æµ‹ï¼Œä¸ºæ£€æµ‹æ—¥ç›Šå¤æ‚çš„æ“çºµæä¾›äº†æ–°çš„å¯èƒ½æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦ä¼ªé€ æŠ€æœ¯å¨èƒæ•°å­—åª’ä½“å®‰å…¨ï¼Œç°æœ‰æ£€æµ‹æ‰‹æ®µæ³›åŒ–èƒ½åŠ›ä¸è¶³ã€‚</li>
<li>HyperFakeåˆ©ç”¨è¶…é«˜å…‰è°±æ•°æ®é‡å»ºæŠ€æœ¯æ­ç¤ºéšè—æ“çºµç—•è¿¹ã€‚</li>
<li>HyperFakeé‡‡ç”¨æ”¹è¿›MST++æ¶æ„å’Œå…‰è°±æ³¨æ„åŠ›æœºåˆ¶æé«˜æ£€æµ‹æ€§èƒ½ã€‚</li>
<li>EfficientNetåˆ†ç±»å™¨ä¼˜åŒ–ç”¨äºå…‰è°±åˆ†æï¼Œæé«˜æ£€æµ‹å‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>HyperFakeæ— éœ€æ˜‚è´µçš„è¶…é«˜å…‰è°±ç›¸æœºå³å¯å®ç°æ£€æµ‹ã€‚</li>
<li>è¿™æ˜¯é¦–æ¬¡å°è¯•å°†è¶…é«˜å…‰è°±æˆåƒé‡å»ºæŠ€æœ¯åº”ç”¨äºæ·±åº¦ä¼ªé€ æ£€æµ‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.18587">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1d56f3fec0314f1cfa98687909f56ea6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8a8432741d4ea6501c6f4e5c76f5e143.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7c6075a61db777b48b2ae2e046aa3968.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-467f434e15caa0b6999a9341125c463e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-89e893efa8e94f84d522a842c6fa74c7.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="ThinkVideo-High-Quality-Reasoning-Video-Segmentation-with-Chain-of-Thoughts"><a href="#ThinkVideo-High-Quality-Reasoning-Video-Segmentation-with-Chain-of-Thoughts" class="headerlink" title="ThinkVideo: High-Quality Reasoning Video Segmentation with Chain of   Thoughts"></a>ThinkVideo: High-Quality Reasoning Video Segmentation with Chain of   Thoughts</h2><p><strong>Authors:Shiu-hong Kao, Yu-Wing Tai, Chi-Keung Tang</strong></p>
<p>Reasoning Video Object Segmentation is a challenging task, which generates a mask sequence from an input video and an implicit, complex text query. Existing works probe into the problem by finetuning Multimodal Large Language Models (MLLM) for segmentation-based output, while still falling short in difficult cases on videos given temporally-sensitive queries, primarily due to the failure to integrate temporal and spatial information. In this paper, we propose ThinkVideo, a novel framework which leverages the zero-shot Chain-of-Thought (CoT) capability of MLLM to address these challenges. Specifically, ThinkVideo utilizes the CoT prompts to extract object selectivities associated with particular keyframes, then bridging the reasoning image segmentation model and SAM2 video processor to output mask sequences. The ThinkVideo framework is training-free and compatible with closed-source MLLMs, which can be applied to Reasoning Video Instance Segmentation. We further extend the framework for online video streams, where the CoT is used to update the object of interest when a better target starts to emerge and becomes visible. We conduct extensive experiments on video object segmentation with explicit and implicit queries. The results show that ThinkVideo significantly outperforms previous works in both cases, qualitatively and quantitatively. </p>
<blockquote>
<p>è§†é¢‘å¯¹è±¡æ¨ç†åˆ†å‰²æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå®ƒæ ¹æ®è¾“å…¥çš„è§†é¢‘å’Œéšå«çš„å¤æ‚æ–‡æœ¬æŸ¥è¯¢ç”Ÿæˆä¸€ä¸ªæ©è†œåºåˆ—ã€‚ç°æœ‰å·¥ä½œé€šè¿‡å¾®è°ƒå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰æ¥è¿›è¡ŒåŸºäºåˆ†å‰²çš„è¾“å‡ºï¼Œä½†åœ¨å¤„ç†è§†é¢‘æ—¶ä»å­˜åœ¨æ—¶é—´æ•æ„ŸæŸ¥è¯¢å›°éš¾çš„é—®é¢˜ï¼Œä¸»è¦æ˜¯å› ä¸ºæ—¶ç©ºä¿¡æ¯çš„èåˆå¤±è´¥ã€‚é’ˆå¯¹è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ThinkVideoæ¡†æ¶ï¼Œå®ƒåˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„é›¶æ ·æœ¬æ€ç»´é“¾ï¼ˆCoTï¼‰èƒ½åŠ›æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚å…·ä½“æ¥è¯´ï¼ŒThinkVideoåˆ©ç”¨CoTæç¤ºæ¥æå–ä¸ç‰¹å®šå…³é”®å¸§å…³è”çš„å¯¹è±¡é€‰æ‹©æ€§ï¼Œç„¶åé€šè¿‡è¿æ¥æ¨ç†å›¾åƒåˆ†å‰²æ¨¡å‹å’ŒSAM2è§†é¢‘å¤„ç†å™¨æ¥è¾“å‡ºæ©è†œåºåˆ—ã€‚ThinkVideoæ¡†æ¶æ— éœ€è®­ç»ƒä¸”ä¸å¼€æºMLLMå…¼å®¹ï¼Œå¯åº”ç”¨äºè§†é¢‘å®ä¾‹æ¨ç†åˆ†å‰²ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥å°†æ¡†æ¶æ‰©å±•åˆ°åœ¨çº¿è§†é¢‘æµä¸­ï¼Œå…¶ä¸­ä½¿ç”¨CoTåœ¨å‡ºç°æ›´å¥½çš„ç›®æ ‡å¹¶å¼€å§‹å¯è§æ—¶è¿›è¡Œæ›´æ–°æ„Ÿå…´è¶£çš„å¯¹è±¡ã€‚æˆ‘ä»¬å¯¹å¸¦æœ‰æ˜ç¡®å’Œéšå«æŸ¥è¯¢çš„è§†é¢‘å¯¹è±¡åˆ†å‰²è¿›è¡Œäº†å¤§é‡å®éªŒã€‚ç»“æœè¡¨æ˜ï¼Œæ— è®ºæ˜¯åœ¨å®šæ€§è¿˜æ˜¯å®šé‡ä¸Šï¼ŒThinkVideoéƒ½æ˜¾è‘—ä¼˜äºä»¥å‰çš„å·¥ä½œã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.18561v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://cse.hkust.edu.hk/~skao/thinkvideo.html">https://cse.hkust.edu.hk/~skao/thinkvideo.html</a></p>
<p><strong>Summary</strong><br>åŒ»å­¦è§†é¢‘å¯¹è±¡åˆ†å‰²æ˜¯ä¸€é¡¹æŒ‘æˆ˜ä»»åŠ¡ï¼Œéœ€ä»è¾“å…¥è§†é¢‘å’Œå¤æ‚æ–‡æœ¬æŸ¥è¯¢ä¸­ç”Ÿæˆæ©è†œåºåˆ—ã€‚ç°æœ‰å·¥ä½œé€šè¿‡å¾®è°ƒå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰è¿›è¡Œåˆ†å‰²è¾“å‡ºï¼Œä½†åœ¨å¯¹å…·æœ‰æ—¶é—´æ•æ„ŸæŸ¥è¯¢çš„è§†é¢‘ä¸Šè¡¨ç°ä¸è¶³ã€‚æœ¬æ–‡æå‡ºThinkVideoæ¡†æ¶ï¼Œåˆ©ç”¨MLLMçš„é›¶æ ·æœ¬æ€ç»´é“¾ï¼ˆCoTï¼‰èƒ½åŠ›è§£å†³æŒ‘æˆ˜ï¼Œæå–ä¸ç‰¹å®šå…³é”®å¸§å…³è”çš„å¯¹è±¡é€‰æ‹©æ€§ï¼Œå†è¿æ¥æ¨ç†å›¾åƒåˆ†å‰²æ¨¡å‹å’ŒSAM2è§†é¢‘å¤„ç†å™¨ä»¥è¾“å‡ºæ©è†œåºåˆ—ã€‚ThinkVideoæ¡†æ¶æ— éœ€è®­ç»ƒä¸”å…¼å®¹é—­æºMLLMï¼Œå¯åº”ç”¨äºæ¨ç†è§†é¢‘å®ä¾‹åˆ†å‰²ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ‰©å±•äº†æ¡†æ¶ä»¥å¤„ç†åœ¨çº¿è§†é¢‘æµï¼Œä½¿ç”¨CoTåœ¨æ›´å¥½çš„ç›®æ ‡å‡ºç°æ—¶è¿›è¡Œæ›´æ–°ã€‚å®éªŒè¡¨æ˜ï¼ŒThinkVideoåœ¨æ˜¾æ€§å’Œéšæ€§æŸ¥è¯¢çš„è§†é¢‘å¯¹è±¡åˆ†å‰²ä¸Šå‡æ˜¾è‘—ä¼˜äºä»¥å‰çš„å·¥ä½œã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦è§†é¢‘å¯¹è±¡åˆ†å‰²æ˜¯ä¸€é¡¹æŒ‘æˆ˜ä»»åŠ¡ï¼Œéœ€è¦ä»è¾“å…¥è§†é¢‘å’Œå¤æ‚æ–‡æœ¬æŸ¥è¯¢ç”Ÿæˆæ©è†œåºåˆ—ã€‚</li>
<li>ç°æœ‰å·¥ä½œè™½ç„¶å°è¯•é€šè¿‡å¾®è°ƒå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰è§£å†³è¯¥é—®é¢˜ï¼Œä½†åœ¨å¤„ç†å…·æœ‰æ—¶é—´æ•æ„ŸæŸ¥è¯¢çš„è§†é¢‘æ—¶ä»æœ‰å›°éš¾ã€‚</li>
<li>ThinkVideoæ¡†æ¶åˆ©ç”¨MLLMçš„é›¶æ ·æœ¬æ€ç»´é“¾ï¼ˆCoTï¼‰èƒ½åŠ›æ¥è§£å†³æŒ‘æˆ˜ã€‚</li>
<li>ThinkVideoæ¡†æ¶å¯ä»¥æå–ä¸ç‰¹å®šå…³é”®å¸§å…³è”çš„å¯¹è±¡é€‰æ‹©æ€§ï¼Œè¿æ¥æ¨ç†å›¾åƒåˆ†å‰²æ¨¡å‹å’ŒSAM2è§†é¢‘å¤„ç†å™¨æ¥è¾“å‡ºæ©è†œåºåˆ—ã€‚</li>
<li>ThinkVideoæ¡†æ¶æ— éœ€è®­ç»ƒï¼Œä¸”å…¼å®¹å¤šç§é—­æºMLLMã€‚</li>
<li>è¯¥æ¡†æ¶å¯åº”ç”¨äºæ¨ç†è§†é¢‘å®ä¾‹åˆ†å‰²ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.18561">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a9b84bfd525efce4dceaecf90036f94f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ca9b511aef0f2c6912a09384bde59c02.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bc30f94ca70e72264b51e5ac739e5032.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-10643ccb9e28b18ac512c89361598ba9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-104ae05d8b98be434cec2565b26282ee.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="TK-Mamba-Marrying-KAN-with-Mamba-for-Text-Driven-3D-Medical-Image-Segmentation"><a href="#TK-Mamba-Marrying-KAN-with-Mamba-for-Text-Driven-3D-Medical-Image-Segmentation" class="headerlink" title="TK-Mamba: Marrying KAN with Mamba for Text-Driven 3D Medical Image   Segmentation"></a>TK-Mamba: Marrying KAN with Mamba for Text-Driven 3D Medical Image   Segmentation</h2><p><strong>Authors:Haoyu Yang, Yuxiang Cai, Jintao Chen, Xuhong Zhang, Wenhui Lei, Xiaoming Shi, Jianwei Yin, Yankai Jiang</strong></p>
<p>3D medical image segmentation is vital for clinical diagnosis and treatment but is challenged by high-dimensional data and complex spatial dependencies. Traditional single-modality networks, such as CNNs and Transformers, are often limited by computational inefficiency and constrained contextual modeling in 3D settings. We introduce a novel multimodal framework that leverages Mamba and Kolmogorov-Arnold Networks (KAN) as an efficient backbone for long-sequence modeling. Our approach features three key innovations: First, an EGSC (Enhanced Gated Spatial Convolution) module captures spatial information when unfolding 3D images into 1D sequences. Second, we extend Group-Rational KAN (GR-KAN), a Kolmogorov-Arnold Networks variant with rational basis functions, into 3D-Group-Rational KAN (3D-GR-KAN) for 3D medical imaging - its first application in this domain - enabling superior feature representation tailored to volumetric data. Third, a dual-branch text-driven strategy leverages CLIPâ€™s text embeddings: one branch swaps one-hot labels for semantic vectors to preserve inter-organ semantic relationships, while the other aligns images with detailed organ descriptions to enhance semantic alignment. Experiments on the Medical Segmentation Decathlon (MSD) and KiTS23 datasets show our method achieving state-of-the-art performance, surpassing existing approaches in accuracy and efficiency. This work highlights the power of combining advanced sequence modeling, extended network architectures, and vision-language synergy to push forward 3D medical image segmentation, delivering a scalable solution for clinical use. The source code is openly available at <a target="_blank" rel="noopener" href="https://github.com/yhy-whu/TK-Mamba">https://github.com/yhy-whu/TK-Mamba</a>. </p>
<blockquote>
<p>ä¸‰ç»´åŒ»å­¦å›¾åƒåˆ†å‰²å¯¹ä¸´åºŠè¯Šæ–­å’Œæ²»ç–—è‡³å…³é‡è¦ï¼Œä½†é¢ä¸´ç€é«˜ç»´æ•°æ®å’Œå¤æ‚ç©ºé—´ä¾èµ–æ€§çš„æŒ‘æˆ˜ã€‚ä¼ ç»Ÿçš„å•ä¸€æ¨¡å¼ç½‘ç»œï¼Œå¦‚å·ç§¯ç¥ç»ç½‘ç»œå’ŒTransformerï¼Œåœ¨ä¸‰ç»´ç¯å¢ƒä¸­å¾€å¾€å—åˆ°è®¡ç®—æ•ˆç‡ä½ä¸‹å’Œä¸Šä¸‹æ–‡å»ºæ¨¡å—é™çš„é™åˆ¶ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹çš„å¤šæ¨¡å¼æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨Mambaå’ŒKolmogorov-Arnoldç½‘ç»œï¼ˆKANï¼‰ä½œä¸ºé•¿åºåˆ—å»ºæ¨¡çš„æœ‰æ•ˆéª¨å¹²ã€‚æˆ‘ä»¬çš„æ–¹æ³•æœ‰ä¸‰ä¸ªå…³é”®åˆ›æ–°ç‚¹ï¼šé¦–å…ˆï¼ŒEGSCï¼ˆå¢å¼ºé—¨æ§ç©ºé—´å·ç§¯ï¼‰æ¨¡å—åœ¨å°†ä¸‰ç»´å›¾åƒå±•å¼€æˆä¸€ç»´åºåˆ—æ—¶æ•è·ç©ºé—´ä¿¡æ¯ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å°†åŸºäºæœ‰ç†åŸºå‡½æ•°çš„Kolmogorov-Arnoldç½‘ç»œå˜ä½“Group-Rational KANï¼ˆGR-KANï¼‰æ‰©å±•åˆ°ç”¨äºä¸‰ç»´åŒ»å­¦å½±åƒçš„3D-Group-Rational KANï¼ˆ3D-GR-KANï¼‰ï¼Œè¿™æ˜¯è¯¥é¢†åŸŸä¸­çš„é¦–æ¬¡åº”ç”¨ï¼Œèƒ½å¤Ÿå®ç°é’ˆå¯¹ä½“ç§¯æ•°æ®çš„ä¼˜è¶Šç‰¹å¾è¡¨ç¤ºã€‚ç¬¬ä¸‰ï¼ŒåŒåˆ†æ”¯æ–‡æœ¬é©±åŠ¨ç­–ç•¥åˆ©ç”¨CLIPçš„æ–‡æœ¬åµŒå…¥ï¼šä¸€ä¸ªåˆ†æ”¯ç”¨è¯­ä¹‰å‘é‡æ›¿æ¢å•çƒ­æ ‡ç­¾ä»¥ä¿ç•™å™¨å®˜é—´çš„è¯­ä¹‰å…³ç³»ï¼Œè€Œå¦ä¸€ä¸ªåˆ†æ”¯åˆ™å°†å›¾åƒä¸è¯¦ç»†çš„å™¨å®˜æè¿°å¯¹é½ï¼Œä»¥å¢å¼ºè¯­ä¹‰å¯¹é½ã€‚åœ¨Medical Segmentation Decathlonï¼ˆMSDï¼‰å’ŒKiTS23æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨å‡†ç¡®æ€§å’Œæ•ˆç‡ä¸Šè¶…è¶Šäº†ç°æœ‰æ–¹æ³•ã€‚è¿™é¡¹å·¥ä½œå¼ºè°ƒäº†ç»“åˆå…ˆè¿›åºåˆ—å»ºæ¨¡ã€æ‰©å±•ç½‘ç»œæ¶æ„å’Œè§†è§‰è¯­è¨€ååŒä½œç”¨æ¨åŠ¨ä¸‰ç»´åŒ»å­¦å›¾åƒåˆ†å‰²çš„åŠ›é‡ï¼Œä¸ºä¸´åºŠä½¿ç”¨æä¾›äº†å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚æºä»£ç å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/yhy-whu/TK-Mamba%E3%80%82">https://github.com/yhy-whu/TK-Mambaã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.18525v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ç”¨äºä¸‰ç»´åŒ»å­¦å›¾åƒåˆ†å‰²çš„å¤šæ¨¡æ€æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†Mambaå’ŒKolmogorov-Arnoldç½‘ç»œï¼ˆKANï¼‰ï¼Œå¹¶å¼•å…¥äº†ä¸‰ä¸ªå…³é”®åˆ›æ–°ç‚¹ï¼šEGSCæ¨¡å—ã€3D-GR-KANç½‘ç»œå’ŒåŒåˆ†æ”¯æ–‡æœ¬é©±åŠ¨ç­–ç•¥ã€‚è¯¥æ¡†æ¶åœ¨åŒ»å­¦åˆ†å‰²æŒ‘æˆ˜èµ›ï¼ˆMSDï¼‰å’ŒKiTS23æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œå…¶æ€§èƒ½è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼Œè¶…è¶Šäº†ç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¸‰ç»´åŒ»å­¦å›¾åƒåˆ†å‰²åœ¨ä¸´åºŠè¯Šæ–­å’Œæ²»ç–—ä¸­è‡³å…³é‡è¦ï¼Œé¢ä¸´é«˜ç»´æ•°æ®å’Œå¤æ‚ç©ºé—´ä¾èµ–æ€§çš„æŒ‘æˆ˜ã€‚</li>
<li>ä¼ ç»Ÿå•æ¨¡æ€ç½‘ç»œï¼ˆå¦‚CNNå’ŒTransformerï¼‰åœ¨å¤„ç†ä¸‰ç»´æ•°æ®æ—¶å­˜åœ¨è®¡ç®—æ•ˆç‡ä½ä¸‹å’Œä¸Šä¸‹æ–‡å»ºæ¨¡å—é™çš„é—®é¢˜ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§å¤šæ¨¡æ€æ¡†æ¶ï¼Œç»“åˆMambaå’ŒKolmogorov-Arnoldç½‘ç»œï¼ˆKANï¼‰ï¼Œä¸ºé•¿åºåˆ—å»ºæ¨¡æä¾›äº†é«˜æ•ˆçš„åç«¯ã€‚</li>
<li>æ¡†æ¶çš„ä¸‰ä¸ªå…³é”®åˆ›æ–°ç‚¹åŒ…æ‹¬ï¼šEGSCæ¨¡å—ã€3D-GR-KANç½‘ç»œå’ŒåŒåˆ†æ”¯æ–‡æœ¬é©±åŠ¨ç­–ç•¥ã€‚</li>
<li>EGSCæ¨¡å—åœ¨å°†ä¸‰ç»´å›¾åƒå±•å¼€æˆä¸€ç»´åºåˆ—æ—¶èƒ½å¤Ÿæ•æ‰ç©ºé—´ä¿¡æ¯ã€‚</li>
<li>é¦–æ¬¡å°†Group-Rational KANï¼ˆGR-KANï¼‰æ‰©å±•åˆ°ä¸‰ç»´é¢†åŸŸçš„3D-GR-KANï¼Œå®ç°äº†å¯¹ä½“ç§¯æ•°æ®çš„ä¼˜åŒ–ç‰¹å¾è¡¨ç¤ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.18525">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cda2711f6ea4032c776a7924e759b0b4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2c3362458602e0b6d8a3542f3876aa0b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-709060d0445d1c86397bfe3ba2b6d69f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a4772fc9fcdf77768ceda5f528bd5ce9.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="CENet-Context-Enhancement-Network-for-Medical-Image-Segmentation"><a href="#CENet-Context-Enhancement-Network-for-Medical-Image-Segmentation" class="headerlink" title="CENet: Context Enhancement Network for Medical Image Segmentation"></a>CENet: Context Enhancement Network for Medical Image Segmentation</h2><p><strong>Authors:Afshin Bozorgpour, Sina Ghorbani Kolahi, Reza Azad, Ilker Hacihaliloglu, Dorit Merhof</strong></p>
<p>Medical image segmentation, particularly in multi-domain scenarios, requires precise preservation of anatomical structures across diverse representations. While deep learning has advanced this field, existing models often struggle with accurate boundary representation, variability in organ morphology, and information loss during downsampling, limiting their accuracy and robustness. To address these challenges, we propose the Context Enhancement Network (CENet), a novel segmentation framework featuring two key innovations. First, the Dual Selective Enhancement Block (DSEB) integrated into skip connections enhances boundary details and improves the detection of smaller organs in a context-aware manner. Second, the Context Feature Attention Module (CFAM) in the decoder employs a multi-scale design to maintain spatial integrity, reduce feature redundancy, and mitigate overly enhanced representations. Extensive evaluations on both radiology and dermoscopic datasets demonstrate that CENet outperforms state-of-the-art (SOTA) methods in multi-organ segmentation and boundary detail preservation, offering a robust and accurate solution for complex medical image analysis tasks. The code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/xmindflow/cenet">https://github.com/xmindflow/cenet</a>. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒåˆ†å‰²ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤šé¢†åŸŸåœºæ™¯ä¸­ï¼Œéœ€è¦åœ¨å„ç§è¡¨ç¤ºæ³•ä¸­ç²¾ç¡®ä¿ç•™è§£å‰–ç»“æ„ã€‚è™½ç„¶æ·±åº¦å­¦ä¹ å·²ç»æ¨åŠ¨äº†è¯¥é¢†åŸŸçš„å‘å±•ï¼Œä½†ç°æœ‰æ¨¡å‹åœ¨å‡†ç¡®è¡¨ç¤ºè¾¹ç•Œã€å™¨å®˜å½¢æ€çš„å˜å¼‚ä»¥åŠä¸‹é‡‡æ ·è¿‡ç¨‹ä¸­çš„ä¿¡æ¯ä¸¢å¤±æ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ï¼Œè¿™é™åˆ¶äº†å…¶å‡†ç¡®æ€§å’Œç¨³å¥æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†Context Enhancement Network (CENet)ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„åˆ†å‰²æ¡†æ¶ï¼Œå…·æœ‰ä¸¤ä¸ªå…³é”®çš„åˆ›æ–°ç‚¹ã€‚é¦–å…ˆï¼Œé›†æˆåˆ°è·³è¿‡è¿æ¥ä¸­çš„Dual Selective Enhancement Block (DSEB)å¯ä»¥å¢å¼ºè¾¹ç•Œç»†èŠ‚ï¼Œå¹¶ä»¥ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„æ–¹å¼æé«˜å¯¹å°å™¨å®˜çš„æ£€æµ‹ã€‚å…¶æ¬¡ï¼Œè§£ç å™¨ä¸­çš„Context Feature Attention Module (CFAM)é‡‡ç”¨å¤šå°ºåº¦è®¾è®¡ï¼Œä»¥ç»´æŒç©ºé—´å®Œæ•´æ€§ã€å‡å°‘ç‰¹å¾å†—ä½™ï¼Œå¹¶å‡è½»è¿‡åº¦å¢å¼ºçš„è¡¨ç¤ºã€‚åœ¨æ”¾å°„å­¦å’Œçš®è‚¤ç§‘æ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒCENetåœ¨å¤šå™¨å®˜åˆ†å‰²å’Œè¾¹ç•Œç»†èŠ‚ä¿ç•™æ–¹é¢ä¼˜äºæœ€æ–°ï¼ˆSOTAï¼‰æ–¹æ³•ï¼Œä¸ºå¤æ‚çš„åŒ»å­¦å›¾åƒåˆ†æä»»åŠ¡æä¾›äº†ç¨³å¥ä¸”å‡†ç¡®çš„è§£å†³æ–¹æ¡ˆã€‚ä»£ç å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/xmindflow/cenet%E3%80%82">https://github.com/xmindflow/cenetã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.18423v1">PDF</a> Provisionally accepted at MICCAI-2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>é’ˆå¯¹åŒ»å­¦å›¾åƒåˆ†å‰²åœ¨å¤šé¢†åŸŸåœºæ™¯ä¸­å¯¹è§£å‰–ç»“æ„ç²¾ç¡®ä¿ç•™çš„éœ€æ±‚ï¼Œæ–‡ç« æå‡ºäº†ä¸€ç§æ–°çš„åˆ†å‰²æ¡†æ¶Context Enhancement Networkï¼ˆCENetï¼‰ã€‚è¯¥æ¡†æ¶å…·æœ‰ä¸¤å¤§åˆ›æ–°ç‚¹ï¼šä¸€æ˜¯é€šè¿‡é›†æˆDual Selective Enhancement Blockï¼ˆDSEBï¼‰åˆ°skipè¿æ¥ä»¥å¢å¼ºè¾¹ç•Œç»†èŠ‚å’Œæ£€æµ‹è¾ƒå°å™¨å®˜çš„èƒ½åŠ›ï¼›äºŒæ˜¯Context Feature Attention Moduleï¼ˆCFAMï¼‰é‡‡ç”¨å¤šå°ºåº¦è®¾è®¡ä»¥ç»´æŒç©ºé—´å®Œæ•´æ€§ã€å‡å°‘ç‰¹å¾å†—ä½™å¹¶å‡è½»è¿‡åº¦å¢å¼ºè¡¨ç¤ºã€‚åœ¨æ”¾å°„å’Œçš®è‚¤é•œæ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒCENetåœ¨å¤šå™¨å®˜åˆ†å‰²å’Œè¾¹ç•Œç»†èŠ‚ä¿ç•™æ–¹é¢ä¼˜äºç°æœ‰å…ˆè¿›æŠ€æœ¯ï¼Œä¸ºå¤æ‚åŒ»å­¦å›¾åƒåˆ†æä»»åŠ¡æä¾›äº†ç¨³å¥ä¸”å‡†ç¡®çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†å‰²åœ¨å¤šé¢†åŸŸåœºæ™¯ä¸­éœ€è¦ç²¾ç¡®ä¿ç•™è§£å‰–ç»“æ„ã€‚</li>
<li>ç°æœ‰æ¨¡å‹åœ¨è¾¹ç•Œè¡¨ç¤ºã€å™¨å®˜å½¢æ€å˜å¼‚åŠä¸‹é‡‡æ ·ä¸­çš„ä¿¡æ¯æŸå¤±æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>CENeté€šè¿‡DSEBå¢å¼ºè¾¹ç•Œç»†èŠ‚ï¼Œæé«˜å°å™¨å®˜æ£€æµ‹èƒ½åŠ›ã€‚</li>
<li>CFAMé‡‡ç”¨å¤šå°ºåº¦è®¾è®¡æ¥ç»´æŒç©ºé—´å®Œæ•´æ€§ï¼Œå‡å°‘ç‰¹å¾å†—ä½™ã€‚</li>
<li>CENetåœ¨æ”¾å°„å’Œçš®è‚¤é•œæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰å…ˆè¿›æŠ€æœ¯ã€‚</li>
<li>CENetå¯¹äºå¤æ‚åŒ»å­¦å›¾åƒåˆ†æä»»åŠ¡æä¾›äº†ç¨³å¥ä¸”å‡†ç¡®çš„è§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.18423">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-178fcc712c6fd61505dfbdc9a3042df1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-50a00d0d4079befa789accf7595baeaf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d9e6d8993d58e0c533e78cd285ba57f0.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-28/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-28/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-28/TTS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-f9fa5d40cacd81f3f87412b0f4b1980b.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-28  Zero-Shot Streaming Text to Speech Synthesis with Transducer and   Auto-Regressive Modeling
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-28
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-28/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-5206070dc91963da2bc84d1a570f2ada.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-28  Multimodal LLM-Guided Semantic Correction in Text-to-Image Diffusion
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-28
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">28051.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
