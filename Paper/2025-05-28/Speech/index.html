<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-28  Mixture of LoRA Experts for Low-Resourced Multi-Accent Automatic Speech   Recognition">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-66ee140547c030fe34bf3065db693d9e.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-28
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-24
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    17.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    71 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-28-æ›´æ–°"><a href="#2025-05-28-æ›´æ–°" class="headerlink" title="2025-05-28 æ›´æ–°"></a>2025-05-28 æ›´æ–°</h1><h2 id="Mixture-of-LoRA-Experts-for-Low-Resourced-Multi-Accent-Automatic-Speech-Recognition"><a href="#Mixture-of-LoRA-Experts-for-Low-Resourced-Multi-Accent-Automatic-Speech-Recognition" class="headerlink" title="Mixture of LoRA Experts for Low-Resourced Multi-Accent Automatic Speech   Recognition"></a>Mixture of LoRA Experts for Low-Resourced Multi-Accent Automatic Speech   Recognition</h2><p><strong>Authors:RaphaÃ«l Bagat, Irina Illina, Emmanuel Vincent</strong></p>
<p>We aim to improve the robustness of Automatic Speech Recognition (ASR) systems against non-native speech, particularly in low-resourced multi-accent settings. We introduce Mixture of Accent-Specific LoRAs (MAS-LoRA), a fine-tuning method that leverages a mixture of Low-Rank Adaptation (LoRA) experts, each specialized in a specific accent. This method can be used when the accent is known or unknown at inference time, without the need to fine-tune the model again. Our experiments, conducted using Whisper on the L2-ARCTIC corpus, demonstrate significant improvements in Word Error Rate compared to regular LoRA and full fine-tuning when the accent is unknown. When the accent is known, the results further improve. Furthermore, MAS-LoRA shows less catastrophic forgetting than the other fine-tuning methods. To the best of our knowledge, this is the first use of a mixture of LoRA experts for non-native multi-accent ASR. </p>
<blockquote>
<p>æˆ‘ä»¬çš„ç›®æ ‡æ˜¯æé«˜è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿå¯¹éæ¯è¯­è¯­éŸ³çš„ç¨³å¥æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨èµ„æºç¨€ç¼ºçš„å¤šå£éŸ³ç¯å¢ƒä¸­ã€‚æˆ‘ä»¬å¼•å…¥äº†å£éŸ³ç‰¹å®šLoRAçš„æ··åˆæ–¹æ³•ï¼ˆMAS-LoRAï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å¾®è°ƒæ–¹æ³•ï¼Œåˆ©ç”¨ä¸€ç³»åˆ—ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰ä¸“å®¶ï¼Œæ¯ä¸ªä¸“å®¶éƒ½ä¸“é—¨é’ˆå¯¹ä¸€ç§å£éŸ³ã€‚è¿™ç§æ–¹æ³•å¯ä»¥åœ¨æ¨ç†é˜¶æ®µä½¿ç”¨ï¼Œæ— éœ€çŸ¥é“å£éŸ³ä¿¡æ¯å³å¯é‡æ–°å¾®è°ƒæ¨¡å‹ã€‚æˆ‘ä»¬åœ¨L2-ARCTICè¯­æ–™åº“ä¸Šä½¿ç”¨whisperè¿›è¡Œäº†å®éªŒï¼Œè¯æ˜å½“å£éŸ³æœªçŸ¥æ—¶ï¼Œä¸ä¼ ç»Ÿçš„LoRAå’Œå…¨é‡å¾®è°ƒç›¸æ¯”ï¼ŒMAS-LoRAåœ¨è¯é”™è¯¯ç‡æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚å½“çŸ¥é“å£éŸ³æ—¶ï¼Œç»“æœä¼šè¿›ä¸€æ­¥æ”¹å–„ã€‚æ­¤å¤–ï¼ŒMAS-LoRAä¸å…¶ä»–å¾®è°ƒæ–¹æ³•ç›¸æ¯”ï¼Œæ˜¾ç¤ºå‡ºè¾ƒå°‘çš„ç¾éš¾æ€§é—å¿˜ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯é¦–æ¬¡ä½¿ç”¨LoRAä¸“å®¶çš„æ··åˆæ–¹æ³•æ¥è§£å†³éæ¯è¯­çš„å¤šå£éŸ³ASRé—®é¢˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20006v1">PDF</a> Submitted to Interspeech 2025</p>
<p><strong>Summary</strong>ï¼š<br>é’ˆå¯¹éæ¯è¯­è¯­éŸ³çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿï¼Œåœ¨èµ„æºç¨€ç¼ºçš„å¤šå£éŸ³ç¯å¢ƒä¸‹ï¼Œæˆ‘ä»¬å¼•å…¥äº†å£éŸ³ç‰¹å®šLoRAæ··åˆï¼ˆMAS-LoRAï¼‰çš„å¾®è°ƒæ–¹æ³•ã€‚è¯¥æ–¹æ³•åˆ©ç”¨ä¸€ç³»åˆ—ä¸“é—¨é’ˆå¯¹ç‰¹å®šå£éŸ³çš„Low-Rank Adaptationï¼ˆLoRAï¼‰ä¸“å®¶æ··åˆè€Œæˆï¼Œå¯åœ¨æ¨ç†é˜¶æ®µåº”ç”¨äºå·²çŸ¥æˆ–æœªçŸ¥å£éŸ³çš„æƒ…å†µï¼Œæ— éœ€å†æ¬¡å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚å®éªŒè¡¨æ˜ï¼Œä¸å¸¸è§„LoRAå’Œå…¨é‡å¾®è°ƒç›¸æ¯”ï¼ŒMAS-LoRAåœ¨æœªçŸ¥å£éŸ³æƒ…å†µä¸‹æ˜¾è‘—æé«˜äº†å•è¯é”™è¯¯ç‡ã€‚å½“å£éŸ³å·²çŸ¥æ—¶ï¼Œç»“æœè¿›ä¸€æ­¥æ”¹å–„ã€‚æ­¤å¤–ï¼ŒMAS-LoRAè¿˜æ˜¾ç¤ºå‡ºæ¯”å…¶ä»–å¾®è°ƒæ–¹æ³•æ›´å°‘çš„ç¾éš¾æ€§é—å¿˜ã€‚è¿™æ˜¯é¦–æ¬¡å°†LoRAä¸“å®¶æ··åˆç”¨äºéæ¯è¯­å¤šå£éŸ³ASRã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ç ”ç©¶ç›®æ ‡æ˜¯æé«˜è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿå¯¹éæ¯è¯­è¯­éŸ³çš„ç¨³å¥æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨èµ„æºæœ‰é™çš„å¤šå£éŸ³ç¯å¢ƒä¸­ã€‚</li>
<li>å¼•å…¥äº†MAS-LoRAæ–¹æ³•ï¼Œå®ƒç»“åˆäº†å¤šä¸ªé’ˆå¯¹ç‰¹å®šå£éŸ³çš„LoRAä¸“å®¶ï¼Œä»¥æé«˜ASRç³»ç»Ÿçš„æ€§èƒ½ã€‚</li>
<li>MAS-LoRAå¯åœ¨æ¨ç†é˜¶æ®µåº”ç”¨äºå·²çŸ¥æˆ–æœªçŸ¥å£éŸ³çš„æƒ…å†µï¼Œæ— éœ€å¯¹æ¨¡å‹è¿›è¡ŒäºŒæ¬¡å¾®è°ƒã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒMAS-LoRAåœ¨æœªçŸ¥å£éŸ³æƒ…å†µä¸‹æ˜¾è‘—æé«˜äº†å•è¯é”™è¯¯ç‡ï¼Œå¹¶ä¸”å½“å£éŸ³å·²çŸ¥æ—¶ï¼Œæ•ˆæœæ›´ä½³ã€‚</li>
<li>ä¸å…¶ä»–å¾®è°ƒæ–¹æ³•ç›¸æ¯”ï¼ŒMAS-LoRAæ˜¾ç¤ºå‡ºè¾ƒå°‘çš„ç¾éš¾æ€§é—å¿˜ã€‚</li>
<li>MAS-LoRAæ˜¯é¦–æ¬¡å°†LoRAä¸“å®¶æ··åˆåº”ç”¨äºéæ¯è¯­å¤šå£éŸ³ASRçš„ç ”ç©¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20006">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4a17620977606ed2a665fcee7e3d2648.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4939afa92874a09b83773f6fa9c541a3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-76d279dc2d40d806f9f1733c900b0b90.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-867acaf5ed51baf5654e8828f8b500a2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9833f4680951bc4b15efd243bcff5740.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-240ee36427784d181308e9b073815cff.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Mel-McNet-A-Mel-Scale-Framework-for-Online-Multichannel-Speech-Enhancement"><a href="#Mel-McNet-A-Mel-Scale-Framework-for-Online-Multichannel-Speech-Enhancement" class="headerlink" title="Mel-McNet: A Mel-Scale Framework for Online Multichannel Speech   Enhancement"></a>Mel-McNet: A Mel-Scale Framework for Online Multichannel Speech   Enhancement</h2><p><strong>Authors:Yujie Yang, Bing Yang, Xiaofei Li</strong></p>
<p>Online multichannel speech enhancement has been intensively studied recently. Though Mel-scale frequency is more matched with human auditory perception and computationally efficient than linear frequency, few works are implemented in a Mel-frequency domain. To this end, this work proposes a Mel-scale framework (namely Mel-McNet). It processes spectral and spatial information with two key components: an effective STFT-to-Mel module compressing multi-channel STFT features into Mel-frequency representations, and a modified McNet backbone directly operating in the Mel domain to generate enhanced LogMel spectra. The spectra can be directly fed to vocoders for waveform reconstruction or ASR systems for transcription. Experiments on CHiME-3 show that Mel-McNet can reduce computational complexity by 60% while maintaining comparable enhancement and ASR performance to the original McNet. Mel-McNet also outperforms other SOTA methods, verifying the potential of Mel-scale speech enhancement. </p>
<blockquote>
<p>è¿‘æœŸï¼Œåœ¨çº¿å¤šé€šé“è¯­éŸ³å¢å¼ºå¾—åˆ°äº†å¹¿æ³›çš„ç ”ç©¶ã€‚è™½ç„¶æ¢…å°”å°ºåº¦é¢‘ç‡ä¸äººç±»å¬è§‰æ„ŸçŸ¥æ›´åŠ åŒ¹é…ä¸”è®¡ç®—æ•ˆç‡æ›´é«˜ï¼Œä½†å¾ˆå°‘æœ‰äººåœ¨æ¢…å°”é¢‘ç‡åŸŸè¿›è¡Œå®ç°ã€‚ä¸ºæ­¤ï¼Œè¿™é¡¹å·¥ä½œæå‡ºäº†ä¸€ä¸ªæ¢…å°”å°ºåº¦æ¡†æ¶ï¼ˆå³Mel-McNetï¼‰ã€‚å®ƒé€šè¿‡ä¸¤ä¸ªå…³é”®ç»„ä»¶å¤„ç†è°±å’Œç©ºé—´ä¿¡æ¯ï¼šä¸€ä¸ªæœ‰æ•ˆçš„STFT-to-Melæ¨¡å—ï¼Œå°†å¤šé€šé“STFTç‰¹å¾å‹ç¼©ä¸ºæ¢…å°”é¢‘ç‡è¡¨ç¤ºï¼Œå’Œä¸€ä¸ªä¿®æ”¹çš„McNetéª¨å¹²ç½‘ç›´æ¥åœ¨æ¢…å°”åŸŸæ“ä½œä»¥ç”Ÿæˆå¢å¼ºçš„LogMelè°±ã€‚è¿™äº›è°±å¯ä»¥ç›´æ¥é¦ˆé€ç»™vocodersè¿›è¡Œæ³¢å½¢é‡å»ºæˆ–ASRç³»ç»Ÿè¿›è¡Œè½¬å½•ã€‚åœ¨CHiME-3ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMel-McNetå¯ä»¥å°†è®¡ç®—å¤æ‚åº¦é™ä½60%ï¼ŒåŒæ—¶ä¿æŒä¸åŸå§‹McNetç›¸å½“çš„å¢å¼ºå’ŒASRæ€§èƒ½ã€‚Mel-McNetè¿˜ä¼˜äºå…¶ä»–SOTAæ–¹æ³•ï¼ŒéªŒè¯äº†æ¢…å°”å°ºåº¦è¯­éŸ³å¢å¼ºçš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19576v1">PDF</a> Accepted by Interspeech 2025</p>
<p><strong>æ€»ç»“</strong></p>
<p>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºMelé¢‘åŸŸçš„å¤šé€šé“è¯­éŸ³å¢å¼ºæ¡†æ¶ï¼ˆMel-McNetï¼‰ã€‚å®ƒé€šè¿‡ä¸¤ä¸ªå…³é”®ç»„ä»¶å¤„ç†é¢‘è°±å’Œç©ºé—´ä¿¡æ¯ï¼šæœ‰æ•ˆçš„STFT-to-Melæ¨¡å—å°†å¤šé€šé“STFTç‰¹å¾å‹ç¼©æˆMelé¢‘åŸŸè¡¨ç¤ºï¼Œä»¥åŠä¿®æ”¹çš„McNetéª¨å¹²ç½‘ç›´æ¥åœ¨MelåŸŸç”Ÿæˆå¢å¼ºçš„LogMelé¢‘è°±ã€‚è¯¥æ¡†æ¶å¯ä»¥åº”ç”¨äºè¯­éŸ³åˆæˆæˆ–è¯­éŸ³è¯†åˆ«ç³»ç»Ÿï¼Œå®éªŒè¡¨æ˜ï¼ŒMel-McNetåœ¨é™ä½è®¡ç®—å¤æ‚åº¦60%çš„åŒæ—¶ï¼Œä¿æŒäº†ä¸åŸå§‹McNetç›¸å½“çš„å¢å¼ºå’Œè¯­éŸ³è¯†åˆ«æ€§èƒ½ã€‚ä¸å…¶ä»–å…ˆè¿›æ–¹æ³•ç›¸æ¯”ï¼ŒMel-McNetçš„è¡¨ç°æ›´åŠ å‡ºè‰²ï¼ŒéªŒè¯äº†Melé¢‘åŸŸè¯­éŸ³å¢å¼ºçš„æ½œåŠ›ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>Melé¢‘åŸŸåœ¨å¤šé€šé“è¯­éŸ³å¢å¼ºä¸­å…·æœ‰é‡è¦çš„åº”ç”¨ä»·å€¼ã€‚</li>
<li>Mel-McNetæ¡†æ¶åˆ©ç”¨STFT-to-Melæ¨¡å—å°†å¤šé€šé“STFTç‰¹å¾è½¬æ¢ä¸ºMelé¢‘åŸŸè¡¨ç¤ºã€‚</li>
<li>Mel-McNeté€šè¿‡ç›´æ¥ç”Ÿæˆå¢å¼ºçš„LogMelé¢‘è°±ï¼Œæé«˜äº†è¯­éŸ³è´¨é‡å’Œè¯­éŸ³è¯†åˆ«æ€§èƒ½ã€‚</li>
<li>Mel-McNetåœ¨è®¡ç®—å¤æ‚åº¦æ–¹é¢ç›¸æ¯”åŸå§‹McNeté™ä½äº†60%ã€‚</li>
<li>Mel-McNetåœ¨CHiME-3æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–å…ˆè¿›æ–¹æ³•ã€‚</li>
<li>å®éªŒç»“æœè¯æ˜äº†Melé¢‘åŸŸè¯­éŸ³å¢å¼ºçš„æ½œåŠ›å’Œä¼˜åŠ¿ã€‚</li>
<li>Mel-McNetæ¡†æ¶å¯ä¸ºè¯­éŸ³åˆæˆå’Œè¯­éŸ³è¯†åˆ«ç³»ç»Ÿæä¾›æœ‰æ•ˆçš„è¯­éŸ³å¢å¼ºè§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19576">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d643398e1b3e66d1d47bb17b4bef4076.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-81217b798e8587d12f0f7225b85292a9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6ca2b76c186ba843f7c6c6c3581d2371.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Training-Free-Multi-Step-Audio-Source-Separation"><a href="#Training-Free-Multi-Step-Audio-Source-Separation" class="headerlink" title="Training-Free Multi-Step Audio Source Separation"></a>Training-Free Multi-Step Audio Source Separation</h2><p><strong>Authors:Yongyi Zang, Jingyi Li, Qiuqiang Kong</strong></p>
<p>Audio source separation aims to separate a mixture into target sources. Previous audio source separation systems usually conduct one-step inference, which does not fully explore the separation ability of models. In this work, we reveal that pretrained one-step audio source separation models can be leveraged for multi-step separation without additional training. We propose a simple yet effective inference method that iteratively applies separation by optimally blending the input mixture with the previous stepâ€™s separation result. At each step, we determine the optimal blending ratio by maximizing a metric. We prove that our method always yield improvement over one-step inference, provide error bounds based on model smoothness and metric robustness, and provide theoretical analysis connecting our method to denoising along linear interpolation paths between noise and clean distributions, a property we link to denoising diffusion bridge models. Our approach effectively delivers improved separation performance as a â€œfree lunchâ€ from existing models. Our empirical results demonstrate that our multi-step separation approach consistently outperforms one-step inference across both speech enhancement and music source separation tasks, and can achieve scaling performance similar to training a larger model, using more data, or in some cases employing a multi-step training objective. These improvements appear not only on the optimization metric during multi-step inference, but also extend to nearly all non-optimized metrics (with one exception). We also discuss limitations of our approach and directions for future research. </p>
<blockquote>
<p>éŸ³é¢‘æºåˆ†ç¦»æ—¨åœ¨å°†æ··åˆéŸ³é¢‘åˆ†ç¦»ä¸ºç›®æ ‡æºã€‚ä¹‹å‰çš„éŸ³é¢‘æºåˆ†ç¦»ç³»ç»Ÿé€šå¸¸è¿›è¡Œä¸€æ¬¡æ¨ç†ï¼Œæ²¡æœ‰å……åˆ†æ¢ç´¢æ¨¡å‹çš„åˆ†ç¦»èƒ½åŠ›ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å‘ç°é¢„è®­ç»ƒçš„ä¸€æ­¥éŸ³é¢‘æºåˆ†ç¦»æ¨¡å‹å¯ç”¨äºå¤šæ­¥åˆ†ç¦»è€Œæ— éœ€é¢å¤–è®­ç»ƒã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„æ¨ç†æ–¹æ³•ï¼Œé€šè¿‡è¿­ä»£åº”ç”¨åˆ†ç¦»ï¼Œå°†è¾“å…¥æ··åˆç‰©ä¸ä¸Šä¸€æ­¥çš„åˆ†ç¦»ç»“æœæœ€ä¼˜åœ°æ··åˆã€‚æ¯ä¸€æ­¥ï¼Œæˆ‘ä»¬é€šè¿‡æœ€å¤§åŒ–ä¸€ä¸ªæŒ‡æ ‡æ¥ç¡®å®šæœ€ä½³æ··åˆæ¯”ä¾‹ã€‚æˆ‘ä»¬è¯æ˜æˆ‘ä»¬çš„æ–¹æ³•æ€»æ˜¯ä¼˜äºä¸€æ­¥æ¨ç†ï¼Œæä¾›åŸºäºæ¨¡å‹å¹³æ»‘å’ŒæŒ‡æ ‡ç¨³å¥æ€§çš„è¯¯å·®ç•Œé™ï¼Œå¹¶æä¾›ç†è®ºåˆ†æï¼Œå°†æˆ‘ä»¬çš„æ–¹æ³•ä¸å»å™ªè”ç³»åˆ°å™ªå£°å’Œæ¸…æ´åˆ†å¸ƒä¹‹é—´çš„çº¿æ€§æ’å€¼è·¯å¾„ä¸Šçš„å±æ€§ï¼Œè¿™ä¸å»å™ªæ‰©æ•£æ¡¥æ¨¡å‹ç›¸å…³ã€‚æˆ‘ä»¬çš„æ–¹æ³•æœ‰æ•ˆåœ°æé«˜äº†ç°æœ‰æ¨¡å‹çš„åˆ†ç¦»æ€§èƒ½ï¼Œå°±åƒâ€œå…è´¹åˆé¤â€ä¸€æ ·ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„å¤šæ­¥åˆ†ç¦»æ–¹æ³•åœ¨ä¸€ç³»åˆ—è¯­éŸ³å¢å¼ºå’ŒéŸ³ä¹æºåˆ†ç¦»ä»»åŠ¡ä¸Šå§‹ç»ˆä¼˜äºä¸€æ­¥æ¨ç†ï¼Œå¹¶ä¸”å¯ä»¥å®ç°ä¸è®­ç»ƒæ›´å¤§çš„æ¨¡å‹ã€ä½¿ç”¨æ›´å¤šçš„æ•°æ®æˆ–åœ¨æŸäº›æƒ…å†µä¸‹é‡‡ç”¨å¤šæ­¥è®­ç»ƒç›®æ ‡ç›¸ä¼¼çš„å¯æ‰©å±•æ€§èƒ½ã€‚è¿™äº›æ”¹è¿›ä¸ä»…å‡ºç°åœ¨å¤šæ­¥æ¨ç†è¿‡ç¨‹ä¸­çš„ä¼˜åŒ–æŒ‡æ ‡ä¸Šï¼Œè€Œä¸”å‡ ä¹æ‰©å±•åˆ°æ‰€æœ‰æœªä¼˜åŒ–çš„æŒ‡æ ‡ï¼ˆæœ‰ä¸€ä¸ªä¾‹å¤–ï¼‰ã€‚æˆ‘ä»¬è¿˜è®¨è®ºäº†æˆ‘ä»¬çš„æ–¹æ³•çš„å±€é™æ€§ä»¥åŠæœªæ¥ç ”ç©¶çš„æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19534v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ­ç¤ºäº†ä¸€ç«™å¼éŸ³é¢‘æºåˆ†ç¦»æ¨¡å‹å¯ä»¥åº”ç”¨äºå¤šæ­¥åˆ†ç¦»ï¼Œè€Œæ— éœ€é¢å¤–è®­ç»ƒã€‚é€šè¿‡è¿­ä»£åº”ç”¨åˆ†ç¦»æ–¹æ³•å¹¶ä¼˜åŒ–æ··åˆè¾“å…¥ä¸ä¸Šä¸€æ­¥åˆ†ç¦»ç»“æœçš„æ··åˆæ¯”ä¾‹ï¼Œæå‡ºäº†ç®€å•æœ‰æ•ˆçš„æ¨ç†æ–¹æ³•ã€‚è¯¥æ–¹æ³•åœ¨ç†è®ºä¸Šä¸å»å™ªæ‰©æ•£æ¡¥æ¨¡å‹ç›¸è”ç³»ï¼Œå¯æ”¹å–„ç°æœ‰æ¨¡å‹çš„åˆ†ç¦»æ€§èƒ½ã€‚ç»éªŒç»“æœè¡¨æ˜ï¼Œå¤šæ­¥åˆ†ç¦»æ–¹æ³•åœ¨ä¸€ç«™å¼æ¨ç†ä¸­è¡¨ç°æ›´ä¼˜ç§€ï¼Œå¹¶èƒ½åœ¨è¯­éŸ³å¢å¼ºå’ŒéŸ³ä¹æºåˆ†ç¦»ä»»åŠ¡ä¸Šå®ç°ç±»ä¼¼æ›´å¤§æ¨¡å‹çš„æ€§èƒ½æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>éŸ³é¢‘æºåˆ†ç¦»æ—¨åœ¨å°†æ··åˆéŸ³é¢‘åˆ†ç¦»ä¸ºç›®æ ‡æºã€‚</li>
<li>ä¸€ç«™å¼éŸ³é¢‘æºåˆ†ç¦»æ¨¡å‹å¯ä»¥åº”ç”¨äºå¤šæ­¥åˆ†ç¦»ã€‚</li>
<li>é€šè¿‡è¿­ä»£åº”ç”¨åˆ†ç¦»æ–¹æ³•å¹¶ä¼˜åŒ–æ··åˆæ¯”ä¾‹ï¼Œæå‡ºç®€å•æœ‰æ•ˆçš„æ¨ç†æ–¹æ³•ã€‚</li>
<li>æ–¹æ³•åœ¨ç†è®ºä¸Šä¸å»å™ªæ‰©æ•£æ¡¥æ¨¡å‹ç›¸è”ç³»ã€‚</li>
<li>å¤šæ­¥åˆ†ç¦»æ–¹æ³•åœ¨ä¸€ç«™å¼æ¨ç†ä¸­è¡¨ç°æ›´ä¼˜ç§€ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨è¯­éŸ³å¢å¼ºå’ŒéŸ³ä¹æºåˆ†ç¦»ä»»åŠ¡ä¸Šå®ç°æ€§èƒ½æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19534">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1203e2ea45c27b04270d21d259af49df.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="AmpleHate-Amplifying-the-Attention-for-Versatile-Implicit-Hate-Detection"><a href="#AmpleHate-Amplifying-the-Attention-for-Versatile-Implicit-Hate-Detection" class="headerlink" title="AmpleHate: Amplifying the Attention for Versatile Implicit Hate   Detection"></a>AmpleHate: Amplifying the Attention for Versatile Implicit Hate   Detection</h2><p><strong>Authors:Yejin Lee, Joonghyuk Hahn, Hyeseon Ahn, Yo-Sub Han</strong></p>
<p>Implicit hate speech detection is challenging due to its subtlety and reliance on contextual interpretation rather than explicit offensive words. Current approaches rely on contrastive learning, which are shown to be effective on distinguishing hate and non-hate sentences. Humans, however, detect implicit hate speech by first identifying specific targets within the text and subsequently interpreting how these target relate to their surrounding context. Motivated by this reasoning process, we propose AmpleHate, a novel approach designed to mirror human inference for implicit hate detection. AmpleHate identifies explicit target using a pretrained Named Entity Recognition model and capture implicit target information via [CLS] tokens. It computes attention-based relationships between explicit, implicit targets and sentence context and then, directly injects these relational vectors into the final sentence representation. This amplifies the critical signals of target-context relations for determining implicit hate. Experiments demonstrate that AmpleHate achieves state-of-the-art performance, outperforming contrastive learning baselines by an average of 82.14% and achieve faster convergence. Qualitative analyses further reveal that attention patterns produced by AmpleHate closely align with human judgement, underscoring its interpretability and robustness. </p>
<blockquote>
<p>éšå¼ä»‡æ¨è¨€è®ºæ£€æµ‹å› å…¶ç»†å¾®æ€§å’Œä¾èµ–äºä¸Šä¸‹æ–‡è§£è¯»è€Œéæ˜æ˜¾çš„å†’çŠ¯æ€§è¯æ±‡è€Œå…·æœ‰æŒ‘æˆ˜æ€§ã€‚å½“å‰çš„æ–¹æ³•ä¾èµ–äºå¯¹æ¯”å­¦ä¹ ï¼Œåœ¨åŒºåˆ†ä»‡æ¨å’Œéä»‡æ¨å¥å­æ–¹é¢æ˜¾ç¤ºå‡ºæœ‰æ•ˆæ€§ã€‚ç„¶è€Œï¼Œäººç±»æ£€æµ‹éšå¼ä»‡æ¨è¨€è®ºæ˜¯å…ˆè¯†åˆ«æ–‡æœ¬ä¸­çš„ç‰¹å®šç›®æ ‡ï¼Œç„¶åè§£é‡Šè¿™äº›ç›®æ ‡å¦‚ä½•ä¸ä»–ä»¬çš„ä¸Šä¸‹æ–‡ç›¸å…³è”ã€‚å—è¿™ç§æ¨ç†è¿‡ç¨‹çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†AmpleHateï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨æ¨¡æ‹Ÿäººç±»æ¨æ–­éšå¼ä»‡æ¨æ£€æµ‹çš„æ–°æ–¹æ³•ã€‚AmpleHateä½¿ç”¨é¢„è®­ç»ƒçš„å‘½åå®ä½“è¯†åˆ«æ¨¡å‹è¯†åˆ«æ˜¾å¼ç›®æ ‡ï¼Œå¹¶é€šè¿‡[CLS]ä»¤ç‰Œæ•è·éšå¼ç›®æ ‡ä¿¡æ¯ã€‚å®ƒè®¡ç®—æ˜¾å¼ã€éšå¼ç›®æ ‡ä¸å¥å­ä¸Šä¸‹æ–‡ä¹‹é—´çš„åŸºäºæ³¨æ„åŠ›çš„å…³ç³»ï¼Œç„¶åå°†è¿™äº›å…³ç³»å‘é‡ç›´æ¥æ³¨å…¥æœ€ç»ˆçš„å¥å­è¡¨ç¤ºä¸­ã€‚è¿™æ”¾å¤§äº†ç¡®å®šéšå¼ä»‡æ¨çš„ç›®æ ‡-ä¸Šä¸‹æ–‡å…³ç³»çš„å…³é”®ä¿¡å·ã€‚å®éªŒè¡¨æ˜ï¼ŒAmpleHateè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹³å‡æ¯”å¯¹æ¯”å­¦ä¹ åŸºçº¿é«˜å‡º82.14%ï¼Œå¹¶ä¸”å®ç°äº†æ›´å¿«çš„æ”¶æ•›é€Ÿåº¦ã€‚å®šæ€§åˆ†æè¿›ä¸€æ­¥è¡¨æ˜ï¼ŒAmpleHateäº§ç”Ÿçš„æ³¨æ„åŠ›æ¨¡å¼ä¸äººç±»åˆ¤æ–­ç´§å¯†å»åˆï¼Œå¼ºè°ƒäº†å…¶å¯è§£é‡Šæ€§å’Œç¨³å¥æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19528v1">PDF</a> 13 pages, 4 figures, Under Review</p>
<p><strong>æ‘˜è¦</strong></p>
<p>éšæ¶è¨€æ£€æµ‹é¢ä¸´è¯¸å¤šæŒ‘æˆ˜ï¼Œå› å…¶è¡¨è¾¾å¾®å¦™ä¸”ä¾èµ–äºè¯­å¢ƒè§£è¯»è€Œéç›´æ¥å†’çŠ¯æ€§è¯æ±‡ã€‚å½“å‰æ–¹æ³•ä¸»è¦ä¾èµ–å¯¹æ¯”å­¦ä¹ ï¼Œåœ¨åŒºåˆ†ä»‡æ¨ä¸éä»‡æ¨è¯­å¥æ–¹é¢æ•ˆæœæ˜¾è‘—ã€‚ç„¶è€Œï¼Œäººç±»æ£€æµ‹éšæ¶è¨€æ—¶é¦–å…ˆè¯†åˆ«æ–‡æœ¬ä¸­çš„ç‰¹å®šç›®æ ‡ï¼Œéšåè§£è¯»è¿™äº›ç›®æ ‡å¦‚ä½•ä¸å‘¨é­è¯­å¢ƒå…³è”ã€‚å—è¿™ä¸€è¿‡ç¨‹å¯å‘ï¼Œæˆ‘ä»¬æå‡ºAmpleHateè¿™ä¸€æ–°æ–¹æ³•ï¼Œæ—¨åœ¨æ¨¡æ‹Ÿäººç±»æ¨ç†è¿‡ç¨‹è¿›è¡Œéšæ¶è¨€æ£€æµ‹ã€‚AmpleHateè¿ç”¨é¢„è®­ç»ƒå‘½åå®ä½“è¯†åˆ«æ¨¡å‹è¯†åˆ«æ˜¾æ€§ç›®æ ‡ï¼Œå¹¶é€šè¿‡[CLS]ä»¤ç‰Œæ•æ‰éšæ€§ç›®æ ‡ä¿¡æ¯ã€‚å®ƒè®¡ç®—æ˜¾æ€§ã€éšæ€§ç›®æ ‡ä¸å¥å­è¯­å¢ƒé—´çš„å…³æ³¨å…³ç³»ï¼Œéšåå°†è¿™äº›å…³ç³»å‘é‡ç›´æ¥æ³¨å…¥æœ€ç»ˆå¥å­è¡¨è¿°ä¸­ã€‚è¿™æ”¾å¤§äº†ç›®æ ‡è¯­å¢ƒå…³ç³»çš„å…³é”®ä¿¡å·ï¼Œä»¥ç¡®å®šéšæ¶è¨€çš„å­˜åœ¨ã€‚å®éªŒè¯æ˜ï¼ŒAmpleHateå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ï¼Œè¾ƒå¯¹æ¯”å­¦ä¹ åŸºçº¿å¹³å‡é«˜å‡º82.14%ï¼Œä¸”æ”¶æ•›é€Ÿåº¦æ›´å¿«ã€‚å®šæ€§åˆ†æè¿›ä¸€æ­¥è¡¨æ˜ï¼ŒAmpleHateäº§ç”Ÿçš„å…³æ³¨æ¨¡å¼ä¸äººç±»åˆ¤æ–­é«˜åº¦ä¸€è‡´ï¼Œå‡¸æ˜¾å…¶å¯è§£é‡Šæ€§å’Œç¨³å¥æ€§ã€‚</p>
<p><strong>è¦ç‚¹æ¦‚æ‹¬</strong></p>
<ol>
<li>éšæ¶è¨€æ£€æµ‹çš„æŒ‘æˆ˜æ€§åœ¨äºå…¶è¡¨è¾¾æ–¹å¼çš„å¾®å¦™å’Œä¾èµ–è¯­å¢ƒè§£è¯»çš„ç‰¹ç‚¹ã€‚</li>
<li>å½“å‰çš„ä¸»è¦æ–¹æ³•æ˜¯é€šè¿‡å¯¹æ¯”å­¦ä¹ æ¥åŒºåˆ†ä»‡æ¨å’Œéä»‡æ¨è¯­å¥ã€‚</li>
<li>äººç±»æ£€æµ‹éšæ¶è¨€æ—¶ï¼Œé¦–å…ˆè¯†åˆ«æ–‡æœ¬ä¸­çš„ç‰¹å®šç›®æ ‡ï¼Œå¹¶è§£è¯»è¿™äº›ç›®æ ‡ä¸è¯­å¢ƒçš„å…³ç³»ã€‚</li>
<li>AmpleHateæ–¹æ³•æ¨¡æ‹Ÿäººç±»æ¨ç†è¿‡ç¨‹è¿›è¡Œéšæ¶è¨€æ£€æµ‹ï¼Œé€šè¿‡è¯†åˆ«æ˜¾æ€§ç›®æ ‡å’Œéšæ€§ç›®æ ‡ä¿¡æ¯ï¼Œè®¡ç®—å®ƒä»¬ä¸å¥å­è¯­å¢ƒçš„å…³æ³¨å…³ç³»ã€‚</li>
<li>AmpleHateå°†å…³ç³»å‘é‡ç›´æ¥æ³¨å…¥å¥å­æœ€ç»ˆè¡¨è¿°ä¸­ï¼Œæ”¾å¤§ç›®æ ‡è¯­å¢ƒå…³ç³»çš„å…³é”®ä¿¡å·ã€‚</li>
<li>å®éªŒè¯æ˜AmpleHateæ€§èƒ½å“è¶Šï¼Œè¾ƒå¯¹æ¯”å­¦ä¹ åŸºçº¿æœ‰æ˜¾è‘—æé«˜ï¼Œä¸”æ”¶æ•›é€Ÿåº¦æ›´å¿«ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19528">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1a09bb6f8dc3ba7b853278a97f68e4b4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-49e983d6bc3ca2daa2d582f60538f1d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ea743330f209fb7503c044ab5cb61344.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d86e5eb9e0cd9c490823477cb0659ab0.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Multi-Channel-Acoustic-Echo-Cancellation-Based-on-Direction-of-Arrival-Estimation"><a href="#Multi-Channel-Acoustic-Echo-Cancellation-Based-on-Direction-of-Arrival-Estimation" class="headerlink" title="Multi-Channel Acoustic Echo Cancellation Based on Direction-of-Arrival   Estimation"></a>Multi-Channel Acoustic Echo Cancellation Based on Direction-of-Arrival   Estimation</h2><p><strong>Authors:Fei Zhao, Xueliang Zhang, Zhong-Qiu Wang</strong></p>
<p>Acoustic echo cancellation (AEC) is an important speech signal processing technology that can remove echoes from microphone signals to enable natural-sounding full-duplex speech communication. While single-channel AEC is widely adopted, multi-channel AEC can leverage spatial cues afforded by multiple microphones to achieve better performance. Existing multi-channel AEC approaches typically combine beamforming with deep neural networks (DNN). This work proposes a two-stage algorithm that enhances multi-channel AEC by incorporating sound source directional cues. Specifically, a lightweight DNN is first trained to predict the sound source directions, and then the predicted directional information, multi-channel microphone signals, and single-channel far-end signal are jointly fed into an AEC network to estimate the near-end signal. Evaluation results show that the proposed algorithm outperforms baseline approaches and exhibits robust generalization across diverse acoustic environments. </p>
<blockquote>
<p>å£°å­¦å›å£°æ¶ˆé™¤ï¼ˆAECï¼‰æ˜¯ä¸€é¡¹é‡è¦çš„è¯­éŸ³ä¿¡å·å¤„ç† technologiesï¼Œèƒ½å¤Ÿæ¶ˆé™¤éº¦å…‹é£ä¿¡å·ä¸­çš„å›å£°ï¼Œä»è€Œå®ç°è‡ªç„¶çš„å…¨åŒå·¥è¯­éŸ³é€šä¿¡ã€‚è™½ç„¶å•é€šé“AECå·²å¾—åˆ°å¹¿æ³›åº”ç”¨ï¼Œä½†å¤šé€šé“AECå¯ä»¥åˆ©ç”¨å¤šä¸ªéº¦å…‹é£æä¾›çš„ç©ºé—´çº¿ç´¢æ¥å®ç°æ›´å¥½çš„æ€§èƒ½ã€‚ç°æœ‰çš„å¤šé€šé“AECæ–¹æ³•é€šå¸¸å°†æ³¢æŸå½¢æˆä¸æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰ç›¸ç»“åˆã€‚è¿™é¡¹å·¥ä½œæå‡ºäº†ä¸€ä¸ªä¸¤é˜¶æ®µçš„ç®—æ³•ï¼Œé€šè¿‡èå…¥å£°æºæ–¹å‘çº¿ç´¢æ¥å¢å¼ºå¤šé€šé“AECã€‚å…·ä½“æ¥è¯´ï¼Œé¦–å…ˆè®­ç»ƒä¸€ä¸ªè½»é‡çº§çš„DNNæ¥é¢„æµ‹å£°æºæ–¹å‘ï¼Œç„¶åå°†é¢„æµ‹çš„æ–¹ä½ä¿¡æ¯ã€å¤šé€šé“éº¦å…‹é£ä¿¡å·å’Œå•é€šé“è¿œç«¯ä¿¡å·ä¸€èµ·è¾“å…¥åˆ°AECç½‘ç»œä¸­æ¥ä¼°è®¡è¿‘ç«¯ä¿¡å·ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œè¯¥ç®—æ³•ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œå¹¶åœ¨å„ç§å£°å­¦ç¯å¢ƒä¸­è¡¨ç°å‡ºç¨³å¥çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19493v1">PDF</a> Accepted by Interspeech 2025</p>
<p><strong>Summary</strong>ï¼š<br>å¤šé€šé“å£°å­¦å›å£°æ¶ˆé™¤ï¼ˆAECï¼‰æŠ€æœ¯åˆ©ç”¨å¤šä¸ªéº¦å…‹é£çš„ç©ºåŸŸçº¿ç´¢ï¼Œé€šè¿‡ç»“åˆæ³¢æŸæˆå½¢å’Œæ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰å®ç°æ›´å¥½çš„æ€§èƒ½ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µç®—æ³•ï¼Œé€šè¿‡å¼•å…¥å£°æºæ–¹å‘çº¿ç´¢æ¥å¢å¼ºå¤šé€šé“AECã€‚é¦–å…ˆè®­ç»ƒä¸€ä¸ªè½»é‡çº§çš„DNNæ¥é¢„æµ‹å£°æºæ–¹å‘ï¼Œç„¶åå°†é¢„æµ‹çš„æ–¹ä½ä¿¡æ¯ã€å¤šé€šé“éº¦å…‹é£ä¿¡å·å’Œå•é€šé“è¿œç«¯ä¿¡å·ä¸€èµ·è¾“å…¥åˆ°AECç½‘ç»œä¸­ä»¥ä¼°è®¡è¿‘ç«¯ä¿¡å·ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œè¯¥ç®—æ³•ä¼˜äºåŸºå‡†æ–¹æ³•ï¼Œå¹¶åœ¨å„ç§å£°å­¦ç¯å¢ƒä¸­è¡¨ç°å‡ºç¨³å¥çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤šé€šé“AECæŠ€æœ¯åˆ©ç”¨å¤šä¸ªéº¦å…‹é£çš„ç©ºåŸŸçº¿ç´¢ï¼Œèƒ½å¤Ÿå®ç°æ›´å¥½çš„å›å£°æ¶ˆé™¤æ€§èƒ½ã€‚</li>
<li>ç°æœ‰å¤šé€šé“AECæ–¹æ³•é€šå¸¸ç»“åˆæ³¢æŸæˆå½¢å’Œæ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰ã€‚</li>
<li>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µç®—æ³•ï¼Œé€šè¿‡å¼•å…¥å£°æºæ–¹å‘çº¿ç´¢æ¥å¢å¼ºå¤šé€šé“AECã€‚</li>
<li>è¯¥ç®—æ³•é¦–å…ˆä½¿ç”¨è½»é‡çº§DNNé¢„æµ‹å£°æºæ–¹å‘ã€‚</li>
<li>è¯¥ç®—æ³•å°†é¢„æµ‹çš„å£°æºæ–¹å‘ã€å¤šé€šé“éº¦å…‹é£ä¿¡å·å’Œå•é€šé“è¿œç«¯ä¿¡å·ä¸€èµ·è¾“å…¥åˆ°AECç½‘ç»œã€‚</li>
<li>è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œè¯¥ç®—æ³•æ€§èƒ½ä¼˜äºåŸºå‡†æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19493">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-602bdefab088e7c53769a62576bf7cea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-531a8c9a7d55e706a095f5f303739a98.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e1bfe980e4a32e910e8896c6a613da70.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8ad2c346fb681c6c583d90a572362090.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4a71773f81adfc0ec97d57389c44fcdb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-23ee6205bde9cf43b0d1e304d93fc308.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="FlowSE-Efficient-and-High-Quality-Speech-Enhancement-via-Flow-Matching"><a href="#FlowSE-Efficient-and-High-Quality-Speech-Enhancement-via-Flow-Matching" class="headerlink" title="FlowSE: Efficient and High-Quality Speech Enhancement via Flow Matching"></a>FlowSE: Efficient and High-Quality Speech Enhancement via Flow Matching</h2><p><strong>Authors:Ziqian Wang, Zikai Liu, Xinfa Zhu, Yike Zhu, Mingshuai Liu, Jun Chen, Longshuai Xiao, Chao Weng, Lei Xie</strong></p>
<p>Generative models have excelled in audio tasks using approaches such as language models, diffusion, and flow matching. However, existing generative approaches for speech enhancement (SE) face notable challenges: language model-based methods suffer from quantization loss, leading to compromised speaker similarity and intelligibility, while diffusion models require complex training and high inference latency. To address these challenges, we propose FlowSE, a flow-matching-based model for SE. Flow matching learns a continuous transformation between noisy and clean speech distributions in a single pass, significantly reducing inference latency while maintaining high-quality reconstruction. Specifically, FlowSE trains on noisy mel spectrograms and optional character sequences, optimizing a conditional flow matching loss with ground-truth mel spectrograms as supervision. It implicitly learns speechâ€™s temporal-spectral structure and text-speech alignment. During inference, FlowSE can operate with or without textual information, achieving impressive results in both scenarios, with further improvements when transcripts are available. Extensive experiments demonstrate that FlowSE significantly outperforms state-of-the-art generative methods, establishing a new paradigm for generative-based SE and demonstrating the potential of flow matching to advance the field. Our code, pre-trained checkpoints, and audio samples are available. </p>
<blockquote>
<p>ç”Ÿæˆæ¨¡å‹åœ¨éŸ³é¢‘ä»»åŠ¡æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œé‡‡ç”¨äº†è¯­è¨€æ¨¡å‹ã€æ‰©æ•£å’ŒæµåŒ¹é…ç­‰æ–¹æ³•ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è¯­éŸ³å¢å¼ºï¼ˆSEï¼‰ç”Ÿæˆæ–¹æ³•é¢ä¸´æ˜¾è‘—æŒ‘æˆ˜ï¼šåŸºäºè¯­è¨€æ¨¡å‹çš„æ–¹æ³•é­å—é‡åŒ–æŸå¤±ï¼Œå¯¼è‡´è¯´è¯äººç›¸ä¼¼æ€§å’Œå¯æ‡‚åº¦å—æŸï¼Œè€Œæ‰©æ•£æ¨¡å‹åˆ™éœ€è¦å¤æ‚çš„è®­ç»ƒå’Œé«˜çš„æ¨ç†å»¶è¿Ÿã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºæµåŒ¹é…çš„SEæ¨¡å‹FlowSEã€‚æµåŒ¹é…ä¸€æ¬¡æ€§å­¦ä¹ å¸¦å™ªå’Œå¹²å‡€è¯­éŸ³åˆ†å¸ƒä¹‹é—´çš„è¿ç»­è½¬æ¢ï¼Œåœ¨ä¿æŒé«˜è´¨é‡é‡å»ºçš„åŒæ—¶ï¼Œæ˜¾è‘—å‡å°‘äº†æ¨ç†å»¶è¿Ÿã€‚å…·ä½“æ¥è¯´ï¼ŒFlowSEåœ¨å¸¦å™ªæ¢…å°”é¢‘è°±å’Œå¯é€‰å­—ç¬¦åºåˆ—ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä»¥åœ°é¢çœŸå®æ¢…å°”é¢‘è°±å›¾ä¸ºç›‘ç£ï¼Œä¼˜åŒ–æ¡ä»¶æµåŒ¹é…æŸå¤±ã€‚å®ƒéšå¼åœ°å­¦ä¹ è¯­éŸ³çš„æ—¶ç©ºé¢‘è°±ç»“æ„å’Œæ–‡æœ¬è¯­éŸ³å¯¹é½ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼ŒFlowSEå¯ä»¥å¸¦æœ‰æˆ–ä¸å¸¦æ–‡æœ¬ä¿¡æ¯è¿›è¡Œæ“ä½œï¼Œåœ¨è¿™ä¸¤ç§æƒ…å†µä¸‹éƒ½å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„ç»“æœï¼Œåœ¨æœ‰å¯ç”¨è½¬å½•æ—¶è¿›ä¸€æ­¥æ”¹è¿›ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒFlowSEæ˜¾è‘—ä¼˜äºæœ€å…ˆè¿›çš„ç”Ÿæˆæ–¹æ³•ï¼Œä¸ºåŸºäºç”Ÿæˆçš„SEå»ºç«‹äº†æ–°èŒƒå¼ï¼Œå¹¶å±•ç¤ºäº†æµåŒ¹é…æ¨åŠ¨è¯¥é¢†åŸŸå‘å±•çš„æ½œåŠ›ã€‚æˆ‘ä»¬çš„ä»£ç ã€é¢„è®­ç»ƒæ£€æŸ¥ç‚¹å’ŒéŸ³é¢‘æ ·æœ¬å¯ä¾›ä½¿ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19476v1">PDF</a> Accepted to InterSpeech 2025</p>
<p><strong>Summary</strong></p>
<p>åŸºäºæµåŒ¹é…æŠ€æœ¯çš„æ–°å‹è¯­éŸ³å¢å¼ºæ¨¡å‹FlowSEè¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ã€‚å®ƒé€šè¿‡ä¸€æ¬¡è¿‡ç¨‹å­¦ä¹ å™ªå£°è¯­éŸ³å’Œçº¯å‡€è¯­éŸ³çš„åˆ†å¸ƒè½¬æ¢ï¼Œå…·å¤‡é«˜æ•ˆæ¨ç†æ—¶é—´å’Œé«˜é‡å»ºè´¨é‡ã€‚FlowSEå¯é€šè¿‡ä¼˜åŒ–æœ‰æ¡ä»¶æµåŒ¹é…æŸå¤±è¿›è¡Œè®­ç»ƒï¼Œåœ¨æœ‰å£°è°±å›¾ç›‘ç£ä¸‹ï¼Œå¯éšå¼å­¦ä¹ è¯­éŸ³çš„æ—¶é¢‘ç»“æ„å’Œæ–‡æœ¬è¯­éŸ³å¯¹é½ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼ŒFlowSEå¯åœ¨æœ‰æˆ–æ— æ–‡æœ¬ä¿¡æ¯çš„æƒ…å†µä¸‹è¿è¡Œï¼Œå¹¶åœ¨ä¸¤ç§æƒ…å†µä¸‹å‡å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„ç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆæ¨¡å‹åœ¨éŸ³é¢‘ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œé‡‡ç”¨è¯­è¨€æ¨¡å‹ã€æ‰©æ•£å’ŒæµåŒ¹é…ç­‰æ–¹æ³•ã€‚</li>
<li>ç°æœ‰è¯­éŸ³å¢å¼ºç”Ÿæˆæ–¹æ³•é¢ä¸´æŒ‘æˆ˜ï¼Œå¦‚è¯­è¨€æ¨¡å‹çš„é‡åŒ–æŸå¤±å’Œæ‰©æ•£æ¨¡å‹çš„é«˜å¤æ‚åº¦å’Œæ¨ç†å»¶è¿Ÿã€‚</li>
<li>FlowSEæ˜¯ä¸€ä¸ªåŸºäºæµåŒ¹é…çš„æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚</li>
<li>FlowSEé€šè¿‡ä¼˜åŒ–æœ‰æ¡ä»¶æµåŒ¹é…æŸå¤±è¿›è¡Œè®­ç»ƒï¼Œå¯åœ¨å£°è°±å›¾ç›‘ç£ä¸‹éšå¼å­¦ä¹ è¯­éŸ³ç»“æ„ã€‚</li>
<li>FlowSEåœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œå¯çµæ´»å¤„ç†æœ‰æˆ–æ— æ–‡æœ¬ä¿¡æ¯çš„æƒ…å†µï¼Œå¹¶å‡å–å¾—è‰¯å¥½æ•ˆæœã€‚</li>
<li>FlowSEæ˜¾è‘—ä¼˜äºç°æœ‰ç”Ÿæˆæ–¹æ³•ï¼Œä¸ºç”Ÿæˆå¼è¯­éŸ³å¢å¼ºå»ºç«‹äº†æ–°èŒƒä¾‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19476">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-7b2d8035e64daf9aa6646ad7ca9eb554.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5fea74b2eb2ceff0e0a40bee92bd325b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-62fcfc69000fcb5926ea36cb567ea599.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="SoloSpeech-Enhancing-Intelligibility-and-Quality-in-Target-Speech-Extraction-through-a-Cascaded-Generative-Pipeline"><a href="#SoloSpeech-Enhancing-Intelligibility-and-Quality-in-Target-Speech-Extraction-through-a-Cascaded-Generative-Pipeline" class="headerlink" title="SoloSpeech: Enhancing Intelligibility and Quality in Target Speech   Extraction through a Cascaded Generative Pipeline"></a>SoloSpeech: Enhancing Intelligibility and Quality in Target Speech   Extraction through a Cascaded Generative Pipeline</h2><p><strong>Authors:Helin Wang, Jiarui Hai, Dongchao Yang, Chen Chen, Kai Li, Junyi Peng, Thomas Thebaud, Laureano Moro Velazquez, Jesus Villalba, Najim Dehak</strong></p>
<p>Target Speech Extraction (TSE) aims to isolate a target speakerâ€™s voice from a mixture of multiple speakers by leveraging speaker-specific cues, typically provided as auxiliary audio (a.k.a. cue audio). Although recent advancements in TSE have primarily employed discriminative models that offer high perceptual quality, these models often introduce unwanted artifacts, reduce naturalness, and are sensitive to discrepancies between training and testing environments. On the other hand, generative models for TSE lag in perceptual quality and intelligibility. To address these challenges, we present SoloSpeech, a novel cascaded generative pipeline that integrates compression, extraction, reconstruction, and correction processes. SoloSpeech features a speaker-embedding-free target extractor that utilizes conditional information from the cue audioâ€™s latent space, aligning it with the mixture audioâ€™s latent space to prevent mismatches. Evaluated on the widely-used Libri2Mix dataset, SoloSpeech achieves the new state-of-the-art intelligibility and quality in target speech extraction and speech separation tasks while demonstrating exceptional generalization on out-of-domain data and real-world scenarios. </p>
<blockquote>
<p>ç›®æ ‡è¯­éŸ³æå–ï¼ˆTSEï¼‰æ—¨åœ¨åˆ©ç”¨ç‰¹å®šäºè¯´è¯äººçš„çº¿ç´¢ï¼ˆé€šå¸¸ä½œä¸ºè¾…åŠ©éŸ³é¢‘ï¼ˆåˆç§°çº¿ç´¢éŸ³é¢‘ï¼‰æä¾›ï¼‰ä»å¤šä¸ªè¯´è¯äººçš„æ··åˆå£°éŸ³ä¸­åˆ†ç¦»å‡ºç›®æ ‡è¯´è¯äººçš„å£°éŸ³ã€‚å°½ç®¡æœ€è¿‘çš„TSEè¿›å±•ä¸»è¦é‡‡ç”¨äº†æä¾›é«˜æ„ŸçŸ¥è´¨é‡çš„åˆ¤åˆ«æ¨¡å‹ï¼Œä½†è¿™äº›æ¨¡å‹å¾€å¾€ä¼šå¼•å…¥ä¸éœ€è¦çš„ä¼ªå½±ï¼Œé™ä½è‡ªç„¶åº¦ï¼Œå¹¶å¯¹è®­ç»ƒå’Œæµ‹è¯•ç¯å¢ƒä¹‹é—´çš„å·®å¼‚æ•æ„Ÿã€‚å¦ä¸€æ–¹é¢ï¼ŒTSEçš„ç”Ÿæˆæ¨¡å‹åœ¨æ„ŸçŸ¥è´¨é‡å’Œæ¸…æ™°åº¦æ–¹é¢æœ‰æ‰€ä¸è¶³ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†SoloSpeechï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çº§è”ç”Ÿæˆç®¡é“ï¼Œå®ƒé›†æˆäº†å‹ç¼©ã€æå–ã€é‡å»ºå’Œæ ¡æ­£è¿‡ç¨‹ã€‚SoloSpeechçš„ç‰¹ç‚¹æ˜¯æ— éœ€è¯´è¯äººåµŒå…¥çš„ç›®æ ‡æå–å™¨ï¼Œå®ƒåˆ©ç”¨çº¿ç´¢éŸ³é¢‘çš„æ½œåœ¨ç©ºé—´çš„æ¡ä»¶ä¿¡æ¯ï¼Œå°†å…¶ä¸æ··åˆéŸ³é¢‘çš„æ½œåœ¨ç©ºé—´å¯¹é½ï¼Œä»¥é˜²æ­¢ä¸åŒ¹é…ã€‚åœ¨å¹¿æ³›ä½¿ç”¨çš„Libri2Mixæ•°æ®é›†ä¸Šï¼ŒSoloSpeechåœ¨ç›®æ ‡è¯­éŸ³æå–å’Œè¯­éŸ³åˆ†ç¦»ä»»åŠ¡ä¸­å®ç°äº†æœ€æ–°çš„æ¸…æ™°åº¦å’Œè´¨é‡æ°´å¹³ï¼ŒåŒæ—¶åœ¨è·¨é¢†åŸŸæ•°æ®å’ŒçœŸå®ä¸–ç•Œåœºæ™¯ä¸Šå±•ç¤ºäº†å‡ºè‰²çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19314v1">PDF</a> </p>
<p><strong>Summary</strong><br>ç›®æ ‡è¯­éŸ³æå–ï¼ˆTSEï¼‰æ—¨åœ¨åˆ©ç”¨ç‰¹å®šäºè¯´è¯äººçš„çº¿ç´¢ï¼ˆé€šå¸¸ä½œä¸ºè¾…åŠ©éŸ³é¢‘æä¾›ï¼‰ï¼Œä»å¤šä¸ªè¯´è¯äººçš„æ··åˆå£°éŸ³ä¸­åˆ†ç¦»å‡ºç›®æ ‡è¯´è¯äººçš„å£°éŸ³ã€‚å°½ç®¡æœ€è¿‘çš„ç ”ç©¶ä¸»è¦é‡‡ç”¨äº†æä¾›é«˜æ„ŸçŸ¥è´¨é‡çš„åˆ¤åˆ«æ¨¡å‹ï¼Œä½†è¿™äº›æ¨¡å‹å¸¸å¸¸å¼•å…¥ä¸å¿…è¦çš„ä¼ªå½±ï¼Œé™ä½äº†è‡ªç„¶åº¦ï¼Œå¹¶å¯¹è®­ç»ƒå’Œæµ‹è¯•ç¯å¢ƒä¹‹é—´çš„å·®å¼‚æ•æ„Ÿã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åä¸ºSoloSpeechçš„æ–°å‹çº§è”ç”Ÿæˆç®¡é“ï¼Œå®ƒé›†æˆäº†å‹ç¼©ã€æå–ã€é‡å»ºå’Œæ ¡æ­£è¿‡ç¨‹ã€‚SoloSpeechçš„ç‰¹ç‚¹æ˜¯æ— éœ€ä½¿ç”¨è¯´è¯äººåµŒå…¥çš„ç›®æ ‡æå–å™¨ï¼Œåˆ©ç”¨æ¥è‡ªæç¤ºéŸ³é¢‘çš„æ½œåœ¨ç©ºé—´çš„æ¡ä»¶ä¿¡æ¯ï¼Œä¸æ··åˆéŸ³é¢‘çš„æ½œåœ¨ç©ºé—´å¯¹é½ï¼Œä»¥é¿å…ä¸åŒ¹é…ã€‚åœ¨å¹¿æ³›ä½¿ç”¨çš„Libri2Mixæ•°æ®é›†ä¸Šï¼ŒSoloSpeechåœ¨ç›®æ ‡è¯­éŸ³æå–å’Œè¯­éŸ³åˆ†ç¦»ä»»åŠ¡ä¸­å®ç°äº†æ–°çš„æœ€å…ˆè¿›çš„å¯ç†è§£æ€§å’Œè´¨é‡ï¼Œå¹¶åœ¨è¶…å‡ºé¢†åŸŸçš„æ•°æ®å’ŒçœŸå®ä¸–ç•Œåœºæ™¯ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç›®æ ‡è¯­éŸ³æå–ï¼ˆTSEï¼‰æ—¨åœ¨ä»æ··åˆå£°éŸ³ä¸­åˆ†ç¦»å‡ºç›®æ ‡è¯´è¯äººçš„å£°éŸ³ï¼Œåˆ©ç”¨ç‰¹å®šäºè¯´è¯äººçš„çº¿ç´¢ã€‚</li>
<li>ç°æœ‰æ¨¡å‹ï¼ˆåˆ¤åˆ«æ¨¡å‹ï¼‰å­˜åœ¨å¼•å…¥ä¼ªå½±ã€é™ä½è‡ªç„¶åº¦å’Œå¯¹ç¯å¢ƒå·®å¼‚æ•æ„Ÿçš„é—®é¢˜ã€‚</li>
<li>SoloSpeechæ˜¯ä¸€ç§æ–°å‹çš„çº§è”ç”Ÿæˆç®¡é“ï¼Œé›†æˆäº†å‹ç¼©ã€æå–ã€é‡å»ºå’Œæ ¡æ­£è¿‡ç¨‹ã€‚</li>
<li>SoloSpeeché‡‡ç”¨æ— è¯´è¯äººåµŒå…¥çš„ç›®æ ‡æå–å™¨è®¾è®¡ï¼Œåˆ©ç”¨æç¤ºéŸ³é¢‘å’Œæ··åˆéŸ³é¢‘çš„æ½œåœ¨ç©ºé—´æ¡ä»¶ä¿¡æ¯ã€‚</li>
<li>SoloSpeechåœ¨Libri2Mixæ•°æ®é›†ä¸Šå®ç°äº†æ–°çš„æœ€å…ˆè¿›çš„å¯ç†è§£æ€§å’Œè´¨é‡ã€‚</li>
<li>SoloSpeechåœ¨è¶…å‡ºé¢†åŸŸçš„æ•°æ®å’ŒçœŸå®ä¸–ç•Œåœºæ™¯ä¸­è¡¨ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19314">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-639373a27c06fdf66efd5372039c03c8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-427f1d98f44b606a591a7df43fc34727.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4d4b3411393f5cd9d4a494f56120efb4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f7a660eb64f8d76b8fc7c66d5640d567.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6b1a4ab8a735330ac8c22f03b1727d5d.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="EnvSDD-Benchmarking-Environmental-Sound-Deepfake-Detection"><a href="#EnvSDD-Benchmarking-Environmental-Sound-Deepfake-Detection" class="headerlink" title="EnvSDD: Benchmarking Environmental Sound Deepfake Detection"></a>EnvSDD: Benchmarking Environmental Sound Deepfake Detection</h2><p><strong>Authors:Han Yin, Yang Xiao, Rohan Kumar Das, Jisheng Bai, Haohe Liu, Wenwu Wang, Mark D Plumbley</strong></p>
<p>Audio generation systems now create very realistic soundscapes that can enhance media production, but also pose potential risks. Several studies have examined deepfakes in speech or singing voice. However, environmental sounds have different characteristics, which may make methods for detecting speech and singing deepfakes less effective for real-world sounds. In addition, existing datasets for environmental sound deepfake detection are limited in scale and audio types. To address this gap, we introduce EnvSDD, the first large-scale curated dataset designed for this task, consisting of 45.25 hours of real and 316.74 hours of fake audio. The test set includes diverse conditions to evaluate the generalizability, such as unseen generation models and unseen datasets. We also propose an audio deepfake detection system, based on a pre-trained audio foundation model. Results on EnvSDD show that our proposed system outperforms the state-of-the-art systems from speech and singing domains. </p>
<blockquote>
<p>éŸ³é¢‘ç”Ÿæˆç³»ç»Ÿå¦‚ä»Šèƒ½å¤Ÿåˆ›å»ºéå¸¸é€¼çœŸçš„å£°éŸ³åœºæ™¯ï¼Œè¿™å¯ä»¥å¢å¼ºåª’ä½“åˆ¶ä½œï¼Œä½†ä¹Ÿå­˜åœ¨æ½œåœ¨é£é™©ã€‚ä¸€äº›ç ”ç©¶å·²ç»ç ”ç©¶äº†è¯­éŸ³æˆ–æ­Œå”±å£°éŸ³çš„æ·±åº¦ä¼ªé€ ã€‚ç„¶è€Œï¼Œç¯å¢ƒå£°éŸ³å…·æœ‰ä¸åŒçš„ç‰¹æ€§ï¼Œè¿™å¯èƒ½ä½¿æ£€æµ‹è¯­éŸ³å’Œæ­Œå”±æ·±åº¦ä¼ªé€ çš„æ–¹æ³•å¯¹ç°å®ä¸–ç•Œçš„å£°éŸ³æ•ˆæœè¾ƒå·®ã€‚æ­¤å¤–ï¼Œç”¨äºç¯å¢ƒå£°éŸ³æ·±åº¦ä¼ªé€ æ£€æµ‹çš„ç°æœ‰æ•°æ®é›†åœ¨è§„æ¨¡å’ŒéŸ³é¢‘ç±»å‹æ–¹é¢å—åˆ°é™åˆ¶ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¼•å…¥äº†EnvSDDï¼Œè¿™æ˜¯ä¸ºæ­¤ä»»åŠ¡è®¾è®¡çš„å¤§è§„æ¨¡å®šåˆ¶æ•°æ®é›†ï¼ŒåŒ…å«45.25å°æ—¶çš„çœŸå®éŸ³é¢‘å’Œ316.74å°æ—¶çš„è™šå‡éŸ³é¢‘ã€‚æµ‹è¯•é›†åŒ…æ‹¬å„ç§æ¡ä»¶ï¼Œä»¥è¯„ä¼°å…¶æ³›åŒ–èƒ½åŠ›ï¼Œä¾‹å¦‚æœªè§è¿‡çš„ç”Ÿæˆæ¨¡å‹å’Œæœªè§è¿‡çš„æ•°æ®é›†ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ä¸ªåŸºäºé¢„è®­ç»ƒéŸ³é¢‘åŸºç¡€æ¨¡å‹çš„éŸ³é¢‘æ·±åº¦ä¼ªé€ æ£€æµ‹ç³»ç»Ÿã€‚åœ¨EnvSDDä¸Šçš„ç»“æœè¡¨æ˜¾ç¤ºï¼Œæˆ‘ä»¬æå‡ºçš„ç³»ç»Ÿä¼˜äºè¯­éŸ³å’Œæ­Œå”±é¢†åŸŸçš„æœ€å…ˆè¿›çš„ç³»ç»Ÿã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19203v1">PDF</a> Accepted by Interspeech 2025</p>
<p><strong>æ‘˜è¦</strong><br>éŸ³é¢‘ç”Ÿæˆç³»ç»Ÿæ‰€åˆ›é€ çš„å£°éŸ³ç¯å¢ƒè¶Šæ¥è¶ŠçœŸå®ï¼Œæ—¢å¢å¼ºäº†åª’ä½“åˆ¶ä½œçš„æ•ˆæœï¼Œä¹Ÿå¸¦æ¥äº†æ½œåœ¨é£é™©ã€‚å°½ç®¡å·²æœ‰ç ”ç©¶æ¢è®¨äº†è¯­éŸ³æˆ–æ­Œå”±å£°éŸ³çš„æ·±åº¦ä¼ªé€ é—®é¢˜ï¼Œä½†ç¯å¢ƒéŸ³å…·æœ‰ä¸åŒçš„ç‰¹æ€§ï¼Œä½¿å¾—æ£€æµ‹è¯­éŸ³å’Œæ­Œå”±æ·±åº¦ä¼ªé€ çš„æ–¹æ³•å¯¹çœŸå®ä¸–ç•Œå£°éŸ³çš„æ•ˆæœå¯èƒ½ä¸ä½³ã€‚æ­¤å¤–ï¼Œç¯å¢ƒéŸ³æ·±åº¦ä¼ªé€ æ£€æµ‹çš„ç°æœ‰æ•°æ®é›†è§„æ¨¡æœ‰é™ã€éŸ³é¢‘ç±»å‹æœ‰é™ã€‚ä¸ºè§£å†³è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æ¨å‡ºäº†EnvSDDï¼Œè¿™æ˜¯ä¸“é—¨ä¸ºè¿™ä¸€ä»»åŠ¡è®¾è®¡çš„å¤§å‹ç²¾é€‰æ•°æ®é›†ï¼ŒåŒ…å«45.25å°æ—¶çš„çœŸå®éŸ³é¢‘å’Œ316.74å°æ—¶çš„ä¼ªé€ éŸ³é¢‘ã€‚æµ‹è¯•é›†åŒ…æ‹¬å„ç§æ¡ä»¶ï¼Œä»¥è¯„ä¼°å¯¹æœªè§è¿‡çš„ç”Ÿæˆæ¨¡å‹å’Œæœªè§è¿‡çš„æ•°æ®é›†çš„æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬è¿˜åŸºäºé¢„è®­ç»ƒçš„éŸ³é¢‘åŸºç¡€æ¨¡å‹æå‡ºäº†éŸ³é¢‘æ·±åº¦ä¼ªé€ æ£€æµ‹ç³»ç»Ÿã€‚åœ¨EnvSDDä¸Šçš„ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„ç³»ç»Ÿä¼˜äºè¯­éŸ³å’Œæ­Œå”±é¢†åŸŸçš„æœ€æ–°ç³»ç»Ÿã€‚</p>
<p><strong>è¦ç‚¹æŒæ¡</strong></p>
<ol>
<li>éŸ³é¢‘ç”Ÿæˆç³»ç»Ÿèƒ½åˆ›é€ é€¼çœŸçš„å£°éŸ³ç¯å¢ƒï¼Œæ—¢æœ‰ç§¯æå½±å“ï¼Œä¹Ÿå­˜åœ¨æ½œåœ¨é£é™©ã€‚</li>
<li>ç¯å¢ƒéŸ³å…·æœ‰ç‹¬ç‰¹ç‰¹æ€§ï¼Œä½¿å¾—ç°æœ‰çš„è¯­éŸ³å’Œæ­Œå”±æ·±åº¦ä¼ªé€ æ£€æµ‹æ–¹æ³•å¯¹å…¶å¯èƒ½ä¸å¤Ÿæœ‰æ•ˆã€‚</li>
<li>å½“å‰é’ˆå¯¹ç¯å¢ƒéŸ³æ·±åº¦ä¼ªé€ æ£€æµ‹çš„æ•°æ®é›†å­˜åœ¨è§„æ¨¡åŠéŸ³é¢‘ç±»å‹ä¸Šçš„å±€é™ã€‚</li>
<li>æ¨å‡ºEnvSDDæ•°æ®é›†ï¼Œä¸“é—¨ä¸ºç¯å¢ƒéŸ³æ·±åº¦ä¼ªé€ æ£€æµ‹ä»»åŠ¡è®¾è®¡ï¼ŒåŒ…å«çœŸå®å’Œä¼ªé€ éŸ³é¢‘ã€‚</li>
<li>æµ‹è¯•é›†åŒ…å«å¤šç§æ¡ä»¶ï¼Œä»¥è¯„ä¼°ç³»ç»Ÿçš„æ³›åŒ–èƒ½åŠ›ï¼Œå¦‚å¯¹æœªè§è¿‡çš„ç”Ÿæˆæ¨¡å‹å’Œæ•°æ®é›†ã€‚</li>
<li>åŸºäºé¢„è®­ç»ƒçš„éŸ³é¢‘åŸºç¡€æ¨¡å‹æå‡ºéŸ³é¢‘æ·±åº¦ä¼ªé€ æ£€æµ‹ç³»ç»Ÿã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19203">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8954880742626ed6ba2f8c4c78b11ab7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-175f2ceebf353149462cd6398c7ebad7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-53336b8d31bbb4cbbbd2842b14a696b1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a6cd9d5898ab52261cb44eb6eccf311a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-66ee140547c030fe34bf3065db693d9e.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="BR-ASR-Efficient-and-Scalable-Bias-Retrieval-Framework-for-Contextual-Biasing-ASR-in-Speech-LLM"><a href="#BR-ASR-Efficient-and-Scalable-Bias-Retrieval-Framework-for-Contextual-Biasing-ASR-in-Speech-LLM" class="headerlink" title="BR-ASR: Efficient and Scalable Bias Retrieval Framework for Contextual   Biasing ASR in Speech LLM"></a>BR-ASR: Efficient and Scalable Bias Retrieval Framework for Contextual   Biasing ASR in Speech LLM</h2><p><strong>Authors:Xun Gong, Anqi Lv, Zhiming Wang, Huijia Zhu, Yanmin Qian</strong></p>
<p>While speech large language models (SpeechLLMs) have advanced standard automatic speech recognition (ASR), contextual biasing for named entities and rare words remains challenging, especially at scale. To address this, we propose BR-ASR: a Bias Retrieval framework for large-scale contextual biasing (up to 200k entries) via two innovations: (1) speech-and-bias contrastive learning to retrieve semantically relevant candidates; (2) dynamic curriculum learning that mitigates homophone confusion which negatively impacts the final performance. The is a general framework that allows seamless integration of the retrieved candidates into diverse ASR systems without fine-tuning. Experiments on LibriSpeech test-clean&#x2F;-other achieve state-of-the-art (SOTA) biased word error rates (B-WER) of 2.8%&#x2F;7.1% with 2000 bias words, delivering 45% relative improvement over prior methods. BR-ASR also demonstrates high scalability: when expanding the bias list to 200k where traditional methods generally fail, it induces only 0.3 &#x2F; 2.9% absolute WER &#x2F; B-WER degradation with a 99.99% pruning rate and only 20ms latency per query on test-other. </p>
<blockquote>
<p>è™½ç„¶è¯­éŸ³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆSpeechLLMsï¼‰å·²ç»æ¨åŠ¨äº†æ ‡å‡†è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰çš„å‘å±•ï¼Œä½†å¯¹å‘½åå®ä½“å’Œç½•è§è¯æ±‡çš„ä¸Šä¸‹æ–‡åè§ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨å¤§è§„æ¨¡æƒ…å†µä¸‹ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†BR-ASRï¼šä¸€ç§ç”¨äºå¤§è§„æ¨¡ä¸Šä¸‹æ–‡åè§ï¼ˆé«˜è¾¾20ä¸‡ä¸ªæ¡ç›®ï¼‰çš„åè§æ£€ç´¢æ¡†æ¶ï¼Œé€šè¿‡ä¸¤ä¸ªåˆ›æ–°ç‚¹æ¥å®ç°ï¼šï¼ˆ1ï¼‰è¯­éŸ³å’Œåè§çš„å¯¹æ¯”å­¦ä¹ ï¼Œä»¥æ£€ç´¢è¯­ä¹‰ç›¸å…³çš„å€™é€‰è¯ï¼›ï¼ˆ2ï¼‰åŠ¨æ€è¯¾ç¨‹å­¦ä¹ ï¼Œå‡è½»åŒéŸ³å­—æ··æ·†å¯¹æœ€ç»ˆæ€§èƒ½çš„è´Ÿé¢å½±å“ã€‚è¿™æ˜¯ä¸€ä¸ªé€šç”¨æ¡†æ¶ï¼Œå…è®¸æ— ç¼åœ°å°†æ£€ç´¢åˆ°çš„å€™é€‰è¯é›†æˆåˆ°å„ç§ASRç³»ç»Ÿä¸­ï¼Œè€Œæ— éœ€å¾®è°ƒã€‚åœ¨LibriSpeechæµ‹è¯•-æ¸…æ´&#x2F;å…¶ä»–å®éªŒä¸Šï¼Œä½¿ç”¨2000ä¸ªåè§è¯æ±‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„åç½®è¯é”™è¯¯ç‡ï¼ˆB-WERï¼‰2.8%&#x2F;7.1%ï¼Œç›¸è¾ƒäºå…ˆå‰çš„æ–¹æ³•å®ç°äº†45%çš„ç›¸å¯¹æ”¹è¿›ã€‚BR-ASRè¿˜æ˜¾ç¤ºå‡ºé«˜åº¦å¯æ‰©å±•æ€§ï¼šå½“å°†åè§åˆ—è¡¨æ‰©å¤§åˆ°20ä¸‡ä¸ªæ¡ç›®æ—¶ï¼Œä¼ ç»Ÿæ–¹æ³•é€šå¸¸æ— æ³•å¤„ç†ï¼Œè€ŒBR-ASRåªå¼•å…¥äº†0.3&#x2F;2.9%çš„ç»å¯¹è¯é”™è¯¯ç‡ï¼ˆWERï¼‰&#x2F; B-WERé€€åŒ–ï¼Œå…·æœ‰99.99%çš„ä¿®å‰ªç‡å’Œä»…20msçš„æŸ¥è¯¢å»¶è¿Ÿï¼ˆé’ˆå¯¹æµ‹è¯•å…¶ä»–æƒ…å†µï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19179v1">PDF</a> Accepted by InterSpeech 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºBR-ASRæ¨¡å‹ï¼Œé€šè¿‡ä¸¤é¡¹åˆ›æ–°è§£å†³äº†å¤§è§„æ¨¡è¯­å¢ƒåå·®é—®é¢˜ï¼šä¸€æ˜¯è¯­éŸ³ä¸åå·®å¯¹æ¯”å­¦ä¹ ï¼Œç”¨äºæ£€ç´¢è¯­ä¹‰ç›¸å…³å€™é€‰è¯ï¼›äºŒæ˜¯åŠ¨æ€è¯¾ç¨‹å­¦ä¹ ï¼Œå‡å°‘åŒéŸ³å­—æ··æ·†å¯¹æœ€ç»ˆæ€§èƒ½çš„å½±å“ã€‚è¯¥æ¨¡å‹å¯ä»¥æ— ç¼é›†æˆåˆ°ä¸åŒçš„è¯­éŸ³è¯†åˆ«ç³»ç»Ÿä¸­ï¼Œå¹¶åœ¨LibriSpeechæ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„è¯†åˆ«æ€§èƒ½æå‡ã€‚ç‰¹åˆ«æ˜¯å¯¹äºå«æœ‰ç‰¹å®šè¯­å¢ƒåå·®çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œå…¶æ˜¾è‘—çš„ä¼˜åŠ¿è¡¨ç°ä¸ºé«˜åº¦å¯æ‰©å±•æ€§å’Œå‡ºè‰²çš„æ•ˆç‡ã€‚æ­¤å¤–ï¼Œå®éªŒç»“æœåœ¨åˆ©ç”¨æœ€å…ˆè¿›çš„å®éªŒèµ„æºè¯„ä¼°æµ‹è¯•è¡¨ç°æ—¶ä¹Ÿå®ç°äº†è‰¯å¥½çš„æ•ˆæœã€‚åœ¨è¿›ä¸€æ­¥æ‰©å¤§åå·®åˆ—è¡¨çš„æƒ…å†µä¸‹ï¼ŒBR-ASRä»ç„¶ä¿æŒäº†å‡ºè‰²çš„æ€§èƒ½è¡¨ç°ã€‚æ€»ä½“æ¥è¯´ï¼ŒBR-ASRä¸ºå¤§è§„æ¨¡è¯­å¢ƒåå·®é—®é¢˜æä¾›äº†ä¸€ç§æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>BR-ASRæ¨¡å‹é‡‡ç”¨ä¸¤é¡¹åˆ›æ–°è§£å†³å¤§è§„æ¨¡è¯­å¢ƒåå·®é—®é¢˜ï¼šè¯­éŸ³ä¸åå·®å¯¹æ¯”å­¦ä¹ ä»¥åŠåŠ¨æ€è¯¾ç¨‹å­¦ä¹ ã€‚</li>
<li>å¯¹æ¯”å­¦ä¹ èƒ½å¤Ÿæ£€ç´¢è¯­ä¹‰ç›¸å…³å€™é€‰è¯ï¼ŒåŠ¨æ€è¯¾ç¨‹å­¦ä¹ åˆ™æœ‰åŠ©äºå‡å°‘åŒéŸ³å­—æ··æ·†å¯¹æ€§èƒ½çš„å½±å“ã€‚</li>
<li>æ¨¡å‹å¯ä»¥æ— ç¼é›†æˆåˆ°ä¸åŒçš„è¯­éŸ³è¯†åˆ«ç³»ç»Ÿä¸­ï¼Œæ— éœ€å¾®è°ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19179">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a362be12675fae2d577b22a4ab59ce7c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f9840aef884420ff8a0646bda90ff488.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4ac1913c4f572e1ec320073d9c17cc49.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="FiLLM-â€“-A-Filipino-optimized-Large-Language-Model-based-on-Southeast-Asia-Large-Language-Model-SEALLM"><a href="#FiLLM-â€“-A-Filipino-optimized-Large-Language-Model-based-on-Southeast-Asia-Large-Language-Model-SEALLM" class="headerlink" title="FiLLM â€“ A Filipino-optimized Large Language Model based on Southeast   Asia Large Language Model (SEALLM)"></a>FiLLM â€“ A Filipino-optimized Large Language Model based on Southeast   Asia Large Language Model (SEALLM)</h2><p><strong>Authors:Carlos Jude G. Maminta, Isaiah Job Enriquez, Deandre Nigel Nunez, Michael B. Dela Fuente</strong></p>
<p>This study presents FiLLM, a Filipino-optimized large language model, designed to enhance natural language processing (NLP) capabilities in the Filipino language. Built upon the SeaLLM-7B 2.5 model, FiLLM leverages Low-Rank Adaptation (LoRA) fine-tuning to optimize memory efficiency while maintaining task-specific performance. The model was trained and evaluated on diverse Filipino datasets to address key NLP tasks, including Named Entity Recognition (NER), Part-of-Speech (POS) tagging, Dependency Parsing, and Text Summarization. Performance comparisons with the CalamanCy model were conducted using F1 Score, Precision, Recall, Compression Rate, and Keyword Overlap metrics. Results indicate that Calamancy outperforms FILLM in several aspects, demonstrating its effectiveness in processing Filipino text with improved linguistic comprehension and adaptability. This research contributes to the advancement of Filipino NLP applications by providing an optimized, efficient, and scalable language model tailored for local linguistic needs. </p>
<blockquote>
<p>æœ¬ç ”ç©¶ä»‹ç»äº†FiLLMï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹è²å¾‹å®¾è¯­è¿›è¡Œä¼˜åŒ–çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨æé«˜è²å¾‹å®¾è¯­çš„è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰èƒ½åŠ›ã€‚FiLLMåŸºäºSeaLLM-7B 2.5æ¨¡å‹æ„å»ºï¼Œåˆ©ç”¨ä½ç§©é€‚é…ï¼ˆLoRAï¼‰å¾®è°ƒæŠ€æœ¯ï¼Œåœ¨ä¿æŒç‰¹å®šä»»åŠ¡æ€§èƒ½çš„åŒæ—¶ä¼˜åŒ–å†…å­˜æ•ˆç‡ã€‚è¯¥æ¨¡å‹åœ¨å¤šç§è²å¾‹å®¾è¯­æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒå’Œè¯„ä¼°ï¼Œä»¥åº”å¯¹å…³é”®çš„è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ï¼ŒåŒ…æ‹¬å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰ã€è¯æ€§æ ‡æ³¨ï¼ˆPOSï¼‰ã€ä¾å­˜è§£æå’Œæ–‡æœ¬æ‘˜è¦ã€‚é€šè¿‡ä¸CalamanCyæ¨¡å‹çš„F1åˆ†æ•°ã€ç²¾ç¡®åº¦ã€å¬å›ç‡ã€å‹ç¼©ç‡å’Œå…³é”®è¯é‡å ç­‰æŒ‡æ ‡è¿›è¡Œæ€§èƒ½æ¯”è¾ƒã€‚ç»“æœè¡¨æ˜ï¼Œåœ¨å¤šä¸ªæ–¹é¢ï¼ŒCalamancyçš„è¡¨ç°ä¼˜äºFILLMï¼Œè¯æ˜å…¶åœ¨å¤„ç†è²å¾‹å®¾æ–‡æœ¬æ—¶çš„æœ‰æ•ˆæ€§ï¼Œæé«˜äº†è¯­è¨€ç†è§£å’Œé€‚åº”æ€§ã€‚æœ¬ç ”ç©¶é€šè¿‡æä¾›ä¸€ä¸ªé’ˆå¯¹æœ¬åœ°è¯­è¨€éœ€æ±‚è¿›è¡Œä¼˜åŒ–ã€é«˜æ•ˆä¸”å¯æ‰©å±•çš„è¯­è¨€æ¨¡å‹ï¼Œä¸ºè²å¾‹å®¾è‡ªç„¶è¯­è¨€å¤„ç†åº”ç”¨çš„è¿›æ­¥åšå‡ºäº†è´¡çŒ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.18995v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶ä»‹ç»äº†FiLLMï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹è²å¾‹å®¾è¯­ä¼˜åŒ–çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨æé«˜è²å¾‹å®¾è¯­çš„è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰èƒ½åŠ›ã€‚è¯¥æ¨¡å‹åŸºäºSeaLLM-7B 2.5æ„å»ºï¼Œé‡‡ç”¨ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰å¾®è°ƒæŠ€æœ¯ï¼Œåœ¨ä¼˜åŒ–å†…å­˜æ•ˆç‡çš„åŒæ—¶ä¿æŒç‰¹å®šä»»åŠ¡æ€§èƒ½ã€‚æ¨¡å‹åœ¨å¤šç§è²å¾‹å®¾è¯­æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒå’Œè¯„ä¼°ï¼Œä»¥åº”å¯¹å…³é”®çš„è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ï¼ŒåŒ…æ‹¬å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰ã€è¯æ€§æ ‡æ³¨ï¼ˆPOSï¼‰ã€ä¾å­˜è§£æå’Œæ–‡æœ¬æ‘˜è¦ã€‚é€šè¿‡ä¸CalamanCyæ¨¡å‹çš„F1åˆ†æ•°ã€ç²¾ç¡®åº¦ã€å¬å›ç‡ã€å‹ç¼©ç‡å’Œå…³é”®è¯é‡å åº¦ç­‰æŒ‡æ ‡çš„æ¯”è¾ƒï¼Œç»“æœæ˜¾ç¤ºCalamancyåœ¨å¤šä¸ªæ–¹é¢ä¼˜äºFILLMï¼Œè¯æ˜å…¶åœ¨å¤„ç†è²å¾‹å®¾è¯­æ–‡æœ¬æ—¶çš„æœ‰æ•ˆæ€§å’Œé€‚åº”æ€§ã€‚è¯¥ç ”ç©¶ä¸ºè²å¾‹å®¾è¯­NLPåº”ç”¨çš„è¿›æ­¥åšå‡ºäº†è´¡çŒ®ï¼Œæä¾›äº†ä¸€ä¸ªé’ˆå¯¹æœ¬åœ°è¯­è¨€éœ€æ±‚çš„ä¼˜åŒ–ã€é«˜æ•ˆå’Œå¯æ‰©å±•çš„è¯­è¨€æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FiLLMæ˜¯ä¸€ä¸ªé’ˆå¯¹è²å¾‹å®¾è¯­ä¼˜åŒ–çš„è¯­è¨€æ¨¡å‹ï¼ŒåŸºäºSeaLLM-7B 2.5æ„å»ºã€‚</li>
<li>FiLLMåˆ©ç”¨ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰å¾®è°ƒæŠ€æœ¯ä»¥æé«˜å†…å­˜æ•ˆç‡å’Œç‰¹å®šä»»åŠ¡æ€§èƒ½ã€‚</li>
<li>æ¨¡å‹åœ¨å¤šç§è²å¾‹å®¾è¯­æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒå’Œè¯„ä¼°ï¼Œæ¶µç›–NERã€POSæ ‡æ³¨ã€ä¾å­˜è§£æå’Œæ–‡æœ¬æ‘˜è¦ç­‰å…³é”®NLPä»»åŠ¡ã€‚</li>
<li>ä¸CalamanCyæ¨¡å‹çš„æ¯”è¾ƒè¡¨æ˜ï¼ŒCalamancyåœ¨å¤šä¸ªæ–¹é¢è¡¨ç°ä¼˜äºFiLLMã€‚</li>
<li>Calamancyåœ¨å¤„ç†è²å¾‹å®¾è¯­æ–‡æœ¬æ—¶è¡¨ç°å‡ºæ›´å¥½çš„è¯­è¨€ç†è§£å’Œé€‚åº”æ€§ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºè²å¾‹å®¾è¯­NLPåº”ç”¨çš„è¿›æ­¥åšå‡ºäº†è´¡çŒ®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.18995">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-d46e40f8c82b1cfc455db69c794736a3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-434a5214b536e09a4d282dcd6050afe0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f73811d96062e1ca21b08b9fa908a5a5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0c604e9a77e3a7c32f7862a907d259fe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9209cc14cc0a278bcd7ee34c402106cf.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="StandUp4AI-A-New-Multilingual-Dataset-for-Humor-Detection-in-Stand-up-Comedy-Videos"><a href="#StandUp4AI-A-New-Multilingual-Dataset-for-Humor-Detection-in-Stand-up-Comedy-Videos" class="headerlink" title="StandUp4AI: A New Multilingual Dataset for Humor Detection in Stand-up   Comedy Videos"></a>StandUp4AI: A New Multilingual Dataset for Humor Detection in Stand-up   Comedy Videos</h2><p><strong>Authors:Valentin Barriere, Nahuel Gomez, Leo Hemamou, Sofia Callejas, Brian Ravenet</strong></p>
<p>Aiming towards improving current computational models of humor detection, we propose a new multimodal dataset of stand-up comedies, in seven languages: English, French, Spanish, Italian, Portuguese, Hungarian and Czech. Our dataset of more than 330 hours, is at the time of writing the biggest available for this type of task, and the most diverse. The whole dataset is automatically annotated in laughter (from the audience), and the subpart left for model validation is manually annotated. Contrary to contemporary approaches, we do not frame the task of humor detection as a binary sequence classification, but as word-level sequence labeling, in order to take into account all the context of the sequence and to capture the continuous joke tagging mechanism typically occurring in natural conversations. As par with unimodal baselines results, we propose a method for e propose a method to enhance the automatic laughter detection based on Audio Speech Recognition errors. Our code and data are available online: <a target="_blank" rel="noopener" href="https://tinyurl.com/EMNLPHumourStandUpPublic">https://tinyurl.com/EMNLPHumourStandUpPublic</a> </p>
<blockquote>
<p>é’ˆå¯¹æ”¹è¿›å½“å‰çš„å¹½é»˜æ£€æµ‹è®¡ç®—æ¨¡å‹ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°çš„å¤šæ¨¡å¼å–œå‰§æ•°æ®é›†ï¼ŒåŒ…å«ä¸ƒç§è¯­è¨€ï¼šè‹±è¯­ã€æ³•è¯­ã€è¥¿ç­ç‰™è¯­ã€æ„å¤§åˆ©è¯­ã€è‘¡è„ç‰™è¯­ã€åŒˆç‰™åˆ©è¯­å’Œæ·å…‹è¯­ã€‚æˆ‘ä»¬çš„æ•°æ®é›†æ—¶é•¿è¶…è¿‡330å°æ—¶ï¼Œæ˜¯æ­¤ç±»ä»»åŠ¡ä¸­ç›®å‰å¯ç”¨æœ€å¤§ä¸”æœ€å¤šå…ƒçš„æ•°æ®é›†ã€‚æ•´ä¸ªæ•°æ®é›†éƒ½è‡ªåŠ¨æ ‡æ³¨äº†è§‚ä¼—çš„ç¬‘å£°ï¼Œç”¨äºæ¨¡å‹éªŒè¯çš„å­é›†åˆ™æ˜¯æ‰‹åŠ¨æ ‡æ³¨çš„ã€‚ä¸å½“å‰çš„æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬ä¸æŠŠå¹½é»˜æ£€æµ‹ä»»åŠ¡è§†ä¸ºä¸€ä¸ªäºŒå…ƒåºåˆ—åˆ†ç±»é—®é¢˜ï¼Œè€Œæ˜¯å°†å…¶è§†ä¸ºå•è¯çº§åˆ«çš„åºåˆ—æ ‡è®°é—®é¢˜ï¼Œä»¥è€ƒè™‘æ•´ä¸ªåºåˆ—çš„ä¸Šä¸‹æ–‡å¹¶æ•æ‰é€šå¸¸åœ¨è‡ªç„¶å¯¹è¯ä¸­å‘ç”Ÿçš„è¿ç»­ç¬‘è¯æ ‡è®°æœºåˆ¶ã€‚å¯¹äºå•æ¨¡æ€åŸºçº¿ç»“æœï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºè¯­éŸ³è¯†åˆ«é”™è¯¯æ¥æé«˜è‡ªåŠ¨ç¬‘å£°æ£€æµ‹çš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®å¯ä»¥åœ¨çº¿è·å–ï¼š<a target="_blank" rel="noopener" href="https://tinyurl.com/EMNLPHumourStandUpPublic">https://tinyurl.com/EMNLPHumourStandUpPublic</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.18903v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ä¸ªæ–°çš„å¤šæ¨¡æ€æ•°æ®é›†ï¼ŒåŒ…å«ä¸ƒç§è¯­è¨€çš„è„±å£ç§€å–œå‰§å†…å®¹ï¼Œæ€»è®¡è¶…è¿‡330å°æ—¶ï¼Œæ˜¯ç›®å‰æ­¤ç±»ä»»åŠ¡ä¸­æœ€å¤§ä¸”æœ€å¤šå…ƒçš„æ•°æ®é›†ã€‚æ•°æ®é›†é€šè¿‡è§‚ä¼—ç¬‘å£°è‡ªåŠ¨æ ‡æ³¨ï¼Œéƒ¨åˆ†æ•°æ®ç”¨äºæ¨¡å‹éªŒè¯å¹¶è¿›è¡Œæ‰‹åŠ¨æ ‡æ³¨ã€‚ä¸å½“ä»£æ–¹æ³•ä¸åŒï¼Œæœ¬æ–‡ä¸å°†å¹½é»˜æ£€æµ‹è§†ä¸ºäºŒå…ƒåºåˆ—åˆ†ç±»ä»»åŠ¡ï¼Œè€Œæ˜¯ä½œä¸ºè¯çº§åºåˆ—æ ‡è®°ï¼Œä»¥è€ƒè™‘åºåˆ—çš„ä¸Šä¸‹æ–‡å¹¶æ•æ‰è‡ªç„¶å¯¹è¯ä¸­è¿ç»­çš„ç¬‘è¯æ ‡è®°æœºåˆ¶ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æå‡ºä¸€ç§åŸºäºè¯­éŸ³è¯†åˆ«é”™è¯¯å¢å¼ºè‡ªåŠ¨ç¬‘å£°æ£€æµ‹çš„æ–¹æ³•ã€‚æ•°æ®å’Œä»£ç å·²åœ¨çº¿å‘å¸ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥ä¸€ä¸ªåŒ…å«ä¸ƒç§è¯­è¨€çš„è„±å£ç§€å–œå‰§å¤šæ¨¡æ€æ•°æ®é›†ï¼Œè§„æ¨¡è¶…è¿‡330å°æ—¶ï¼Œä¸ºå½“å‰æœ€å¤§çš„å¹½é»˜æ£€æµ‹æ•°æ®é›†ã€‚</li>
<li>æ•°æ®é›†é€šè¿‡è§‚ä¼—ç¬‘å£°è‡ªåŠ¨æ ‡æ³¨ï¼Œéƒ¨åˆ†æ•°æ®æ‰‹åŠ¨æ ‡æ³¨ï¼Œç”¨äºæ¨¡å‹éªŒè¯ã€‚</li>
<li>ä¸åŒäºç°æœ‰çš„å¹½é»˜æ£€æµ‹äºŒå…ƒåºåˆ—åˆ†ç±»æ–¹æ³•ï¼Œæœ¬æ–‡å°†å…¶è§†ä¸ºè¯çº§åºåˆ—æ ‡è®°ï¼Œä»¥æ•æ‰è‡ªç„¶å¯¹è¯ä¸­çš„è¿ç»­ç¬‘è¯ã€‚</li>
<li>è€ƒè™‘åˆ°åºåˆ—çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œæé«˜å¹½é»˜æ£€æµ‹çš„å‡†ç¡®æ€§ã€‚</li>
<li>æå‡ºä¸€ç§åŸºäºè¯­éŸ³è¯†åˆ«é”™è¯¯å¢å¼ºè‡ªåŠ¨ç¬‘å£°æ£€æµ‹çš„æ–¹æ³•ã€‚</li>
<li>æ•°æ®é›†å’Œä»£ç å·²åœ¨çº¿å‘å¸ƒï¼Œä¾¿äºå…¶ä»–ç ”ç©¶è€…ä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.18903">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-df9c9b54d9da5d95d0cbcff959bcc395.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1d9e4d3f4be6d8801763ed184da1d63e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e5d3cebec21a601c1dc62de1b31136cf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ab78912122561260d79a74bc5caded7e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c66776262e4eeac662aaed4a6f08bdf4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c4e577a26362e8c02148d1ac6d9b44ef.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Enhancing-Generalization-of-Speech-Large-Language-Models-with-Multi-Task-Behavior-Imitation-and-Speech-Text-Interleaving"><a href="#Enhancing-Generalization-of-Speech-Large-Language-Models-with-Multi-Task-Behavior-Imitation-and-Speech-Text-Interleaving" class="headerlink" title="Enhancing Generalization of Speech Large Language Models with Multi-Task   Behavior Imitation and Speech-Text Interleaving"></a>Enhancing Generalization of Speech Large Language Models with Multi-Task   Behavior Imitation and Speech-Text Interleaving</h2><p><strong>Authors:Jingran Xie, Xiang Li, Hui Wang, Yue Yu, Yang Xiang, Xixin Wu, Zhiyong Wu</strong></p>
<p>Large language models (LLMs) have shown remarkable generalization across tasks, leading to increased interest in integrating speech with LLMs. These speech LLMs (SLLMs) typically use supervised fine-tuning to align speech with text-based LLMs. However, the lack of annotated speech data across a wide range of tasks hinders alignment efficiency, resulting in poor generalization. To address these issues, we propose a novel multi-task â€˜behavior imitationâ€™ method with speech-text interleaving, called MTBI, which relies solely on paired speech and transcripts. By ensuring the LLM decoder generates equivalent responses to paired speech and text, we achieve a more generalized SLLM. Interleaving is used to further enhance alignment efficiency. We introduce a simple benchmark to evaluate prompt and task generalization across different models. Experimental results demonstrate that our MTBI outperforms SOTA SLLMs on both prompt and task generalization, while requiring less supervised speech data. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„ç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†æ˜¾è‘—çš„æ³›åŒ–èƒ½åŠ›ï¼Œè¿™å¼•å‘äº†å°†è¯­éŸ³ä¸LLMé›†æˆçš„å…´è¶£å¢é•¿ã€‚è¿™äº›è¯­éŸ³LLMï¼ˆSLLMï¼‰é€šå¸¸ä½¿ç”¨æœ‰ç›‘ç£å¾®è°ƒæ¥å°†è¯­éŸ³ä¸åŸºäºæ–‡æœ¬çš„LLMå¯¹é½ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹å¹¿æ³›ä»»åŠ¡çš„æ ‡æ³¨è¯­éŸ³æ•°æ®ï¼Œé˜»ç¢äº†å¯¹é½æ•ˆç‡ï¼Œå¯¼è‡´æ³›åŒ–æ€§èƒ½è¾ƒå·®ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹å¤šä»»åŠ¡â€œè¡Œä¸ºæ¨¡ä»¿â€æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é‡‡ç”¨è¯­éŸ³æ–‡æœ¬äº¤æ›¿æ–¹å¼ï¼Œç§°ä¸ºMTBIï¼Œä»…ä¾èµ–äºé…å¯¹è¯­éŸ³å’Œæ–‡æœ¬ã€‚é€šè¿‡ç¡®ä¿LLMè§£ç å™¨å¯¹é…å¯¹è¯­éŸ³å’Œæ–‡æœ¬äº§ç”Ÿç­‰æ•ˆå“åº”ï¼Œæˆ‘ä»¬å®ç°äº†æ›´é€šç”¨çš„SLLMã€‚äº¤æ›¿æ–¹å¼è¢«ç”¨æ¥è¿›ä¸€æ­¥æé«˜å¯¹é½æ•ˆç‡ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªç®€å•çš„åŸºå‡†æµ‹è¯•æ¥è¯„ä¼°ä¸åŒæ¨¡å‹åœ¨æç¤ºå’Œä»»åŠ¡æ–¹é¢çš„æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„MTBIåœ¨æç¤ºå’Œä»»åŠ¡æ³›åŒ–æ–¹é¢éƒ½ä¼˜äºæœ€æ–°çš„SLLMï¼ŒåŒæ—¶éœ€è¦æ›´å°‘çš„ç›‘ç£è¯­éŸ³æ•°æ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.18644v1">PDF</a> Accepted by Interspeech 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>è¯­è¨€å¤§æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è·¨ä»»åŠ¡ä¸­å±•ç°å‡ºæ˜¾è‘—çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¼•å‘äº†å°†è¯­éŸ³ä¸LLMé›†æˆçš„å…´è¶£å¢åŠ ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹è·¨å„ç§ä»»åŠ¡çš„æ ‡æ³¨è¯­éŸ³æ•°æ®ï¼Œå½±å“äº†ä¸æ–‡æœ¬å‹LLMçš„å¯¹é½æ•ˆç‡ï¼Œå¯¼è‡´æ³›åŒ–æ€§èƒ½ä¸ä½³ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹å¤šä»»åŠ¡â€œè¡Œä¸ºæ¨¡ä»¿â€æ–¹æ³•ï¼Œé‡‡ç”¨è¯­éŸ³ä¸æ–‡æœ¬çš„äº¤é”™æ–¹å¼ï¼Œç§°ä¸ºMTBIï¼Œä»…ä¾èµ–äºæˆå¯¹çš„è¯­éŸ³å’Œæ–‡æœ¬ã€‚é€šè¿‡ç¡®ä¿LLMè§£ç å™¨å¯¹æˆå¯¹çš„è¯­éŸ³å’Œæ–‡æœ¬äº§ç”Ÿç­‰æ•ˆå“åº”ï¼Œæˆ‘ä»¬å®ç°äº†æ›´é€šç”¨çš„SLLMã€‚äº¤é”™æ–¹å¼è¿›ä¸€æ­¥æé«˜äº†å¯¹é½æ•ˆç‡ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªç®€å•çš„åŸºå‡†æµ‹è¯•æ¥è¯„ä¼°ä¸åŒæ¨¡å‹åœ¨æç¤ºå’Œä»»åŠ¡ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„MTBIåœ¨æç¤ºå’Œä»»åŠ¡æ³›åŒ–æ–¹é¢ä¼˜äºå½“å‰æœ€ä½³æ°´å¹³çš„SLLMï¼ŒåŒæ—¶éœ€è¦æ›´å°‘çš„ç›‘ç£è¯­éŸ³æ•°æ®ã€‚</p>
<p><strong>è¦ç‚¹æŒæ¡</strong></p>
<ol>
<li>LLMåœ¨è·¨ä»»åŠ¡ä¸­å±•ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¼•å‘äº†å¯¹é›†æˆè¯­éŸ³å’ŒLLMçš„ç ”ç©¶å…´è¶£ã€‚</li>
<li>ç°æœ‰çš„è¯­éŸ³LLMï¼ˆSLLMï¼‰é€šå¸¸ä½¿ç”¨ç›‘ç£å¾®è°ƒä¸æ–‡æœ¬å‹LLMå¯¹é½ï¼Œä½†ç¼ºä¹æ ‡æ³¨è¯­éŸ³æ•°æ®å½±å“äº†å¯¹é½æ•ˆç‡å’Œæ³›åŒ–æ€§èƒ½ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹å¤šä»»åŠ¡â€œè¡Œä¸ºæ¨¡ä»¿â€æ–¹æ³•ï¼ˆMTBIï¼‰ï¼Œä»…åˆ©ç”¨æˆå¯¹çš„è¯­éŸ³å’Œæ–‡æœ¬ï¼Œç¡®ä¿LLMè§£ç å™¨å¯¹è¯­éŸ³å’Œæ–‡æœ¬äº§ç”Ÿç­‰æ•ˆå“åº”ï¼Œæé«˜æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>MTBIé‡‡ç”¨è¯­éŸ³ä¸æ–‡æœ¬çš„äº¤é”™æ–¹å¼ï¼Œè¿›ä¸€æ­¥æé«˜å¯¹é½æ•ˆç‡ã€‚</li>
<li>å¼•å…¥äº†ä¸€ä¸ªç®€å•çš„åŸºå‡†æµ‹è¯•æ¥è¯„ä¼°æ¨¡å‹åœ¨æç¤ºå’Œä»»åŠ¡ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒMTBIåœ¨æ³›åŒ–æ€§èƒ½å’Œç›‘ç£è¯­éŸ³æ•°æ®éœ€æ±‚æ–¹é¢ä¼˜äºç°æœ‰SLLMã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.18644">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2fd928cc49a7ef06d21310ee9719f9e5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ca2fe11c1e73d2ad0d2ac7f84ae83dd4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4efd518ca8d86a58ff4c848018351c1c.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="TS-URGENet-A-Three-stage-Universal-Robust-and-Generalizable-Speech-Enhancement-Network"><a href="#TS-URGENet-A-Three-stage-Universal-Robust-and-Generalizable-Speech-Enhancement-Network" class="headerlink" title="TS-URGENet: A Three-stage Universal Robust and Generalizable Speech   Enhancement Network"></a>TS-URGENet: A Three-stage Universal Robust and Generalizable Speech   Enhancement Network</h2><p><strong>Authors:Xiaobin Rong, Dahan Wang, Qinwen Hu, Yushi Wang, Yuxiang Hu, Jing Lu</strong></p>
<p>Universal speech enhancement aims to handle input speech with different distortions and input formats. To tackle this challenge, we present TS-URGENet, a Three-Stage Universal, Robust, and Generalizable speech Enhancement Network. To address various distortions, the proposed system employs a novel three-stage architecture consisting of a filling stage, a separation stage, and a restoration stage. The filling stage mitigates packet loss by preliminarily filling lost regions under noise interference, ensuring signal continuity. The separation stage suppresses noise, reverberation, and clipping distortion to improve speech clarity. Finally, the restoration stage compensates for bandwidth limitation, codec artifacts, and residual packet loss distortion, refining the overall speech quality. Our proposed TS-URGENet achieved outstanding performance in the Interspeech 2025 URGENT Challenge, ranking 2nd in Track 1. </p>
<blockquote>
<p>é€šç”¨çš„è¯­éŸ³å¢å¼ºæ—¨åœ¨å¤„ç†å…·æœ‰ä¸åŒå¤±çœŸå’Œè¾“å…¥æ ¼å¼çš„è¾“å…¥è¯­éŸ³ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†TS-URGENetï¼Œè¿™æ˜¯ä¸€ä¸ªä¸‰é˜¶æ®µçš„é€šç”¨ã€ç¨³å¥å’Œå¯æ¨å¹¿çš„è¯­éŸ³å¢å¼ºç½‘ç»œã€‚ä¸ºäº†è§£å†³å„ç§å¤±çœŸé—®é¢˜ï¼Œæ‰€æå‡ºç³»ç»Ÿé‡‡ç”¨äº†ä¸€ç§æ–°é¢–çš„ä¸‰é˜¶æ®µæ¶æ„ï¼ŒåŒ…æ‹¬å¡«å……é˜¶æ®µã€åˆ†ç¦»é˜¶æ®µå’Œæ¢å¤é˜¶æ®µã€‚å¡«å……é˜¶æ®µé€šè¿‡åˆæ­¥å¡«å……å™ªå£°å¹²æ‰°ä¸‹ä¸¢å¤±çš„åŒºåŸŸï¼Œå‡è½»æ•°æ®åŒ…ä¸¢å¤±çš„æƒ…å†µï¼Œç¡®ä¿ä¿¡å·è¿ç»­æ€§ã€‚åˆ†ç¦»é˜¶æ®µæŠ‘åˆ¶å™ªå£°ã€å›å£°å’Œå‰Šæ³¢å¤±çœŸï¼Œä»¥æé«˜è¯­éŸ³æ¸…æ™°åº¦ã€‚æœ€åï¼Œæ¢å¤é˜¶æ®µå¯¹å¸¦å®½é™åˆ¶ã€ç¼–ç è§£ç å™¨äº§ç”Ÿçš„ä¼ªå½±å’Œæ®‹ç•™çš„ä¸¢åŒ…å¤±çœŸè¿›è¡Œè¡¥å¿ï¼Œæé«˜æ•´ä½“è¯­éŸ³è´¨é‡ã€‚æˆ‘ä»¬æå‡ºçš„TS-URGENetåœ¨Interspeech 2025ç´§æ€¥æŒ‘æˆ˜ä¸­å–å¾—äº†å“è¶Šè¡¨ç°ï¼Œåœ¨èµ›é“ä¸€æ’åç¬¬äºŒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.18533v1">PDF</a> Accepted by Interspeech 2025</p>
<p><strong>Summary</strong></p>
<p>TS-URGENetæ˜¯ä¸€ä¸ªä¸‰é˜¶æ®µé€šç”¨ã€ç¨³å¥å’Œé€šç”¨çš„è¯­éŸ³å¢å¼ºç½‘ç»œï¼Œç”¨äºå¤„ç†ä¸åŒå¤±çœŸå’Œè¾“å…¥æ ¼å¼çš„è¯­éŸ³ã€‚å®ƒé€šè¿‡å¡«å……ã€åˆ†ç¦»å’Œæ¢å¤ä¸‰ä¸ªé˜¶æ®µæ¥è§£å†³å„ç§å¤±çœŸé—®é¢˜ï¼Œæé«˜è¯­éŸ³æ¸…æ™°åº¦å¹¶ä¼˜åŒ–è¯­éŸ³è´¨é‡ã€‚åœ¨Interspeech 2025 URGENT Challengeä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ’åç¬¬äºŒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TS-URGENetæ˜¯ä¸€ä¸ªä¸‰é˜¶æ®µæ¶æ„çš„é€šç”¨è¯­éŸ³å¢å¼ºç½‘ç»œã€‚</li>
<li>å®ƒåŒ…æ‹¬å¡«å……ã€åˆ†ç¦»å’Œæ¢å¤ä¸‰ä¸ªé˜¶æ®µï¼Œé’ˆå¯¹å„ç§è¯­éŸ³å¤±çœŸé—®é¢˜ã€‚</li>
<li>å¡«å……é˜¶æ®µåˆæ­¥å¡«å……å™ªå£°å¹²æ‰°ä¸‹ä¸¢å¤±çš„åŒºåŸŸï¼Œç¡®ä¿ä¿¡å·è¿ç»­æ€§ã€‚</li>
<li>åˆ†ç¦»é˜¶æ®µæŠ‘åˆ¶å™ªå£°ã€å›å£°å’Œå‰Šå³°å¤±çœŸï¼Œæé«˜è¯­éŸ³æ¸…æ™°åº¦ã€‚</li>
<li>æ¢å¤é˜¶æ®µè¡¥å¿å¸¦å®½é™åˆ¶ã€ç¼–ç è§£ç å™¨äº§ç”Ÿçš„ä¼ªå½±å’Œæ®‹ç•™åŒ…ä¸¢å¤±å¤±çœŸï¼Œä¼˜åŒ–è¯­éŸ³è´¨é‡ã€‚</li>
<li>TS-URGENetåœ¨Interspeech 2025 URGENT Challengeä¸­å–å¾—äº†ä¼˜å¼‚è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.18533">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-9f41b56e9ce2fac03340eb7aeb2e3c2a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-19c740dbd422e6ebeb1e914a9f7defb9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-85c78deffe246ec778c823de049bf78d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eeaca8d37fa572fe3f58dc6fd2ce6849.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-48eff2e7dc62ec053fbe00dd88fa1609.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fbc13a8b882b7a298d1cb481169d91b0.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="ABHINAYA-â€“-A-System-for-Speech-Emotion-Recognition-In-Naturalistic-Conditions-Challenge"><a href="#ABHINAYA-â€“-A-System-for-Speech-Emotion-Recognition-In-Naturalistic-Conditions-Challenge" class="headerlink" title="ABHINAYA â€“ A System for Speech Emotion Recognition In Naturalistic   Conditions Challenge"></a>ABHINAYA â€“ A System for Speech Emotion Recognition In Naturalistic   Conditions Challenge</h2><p><strong>Authors:Soumya Dutta, Smruthi Balaji, Varada R, Viveka Salinamakki, Sriram Ganapathy</strong></p>
<p>Speech emotion recognition (SER) in naturalistic settings remains a challenge due to the intrinsic variability, diverse recording conditions, and class imbalance. As participants in the Interspeech Naturalistic SER Challenge which focused on these complexities, we present Abhinaya, a system integrating speech-based, text-based, and speech-text models. Our approach fine-tunes self-supervised and speech large language models (SLLM) for speech representations, leverages large language models (LLM) for textual context, and employs speech-text modeling with an SLLM to capture nuanced emotional cues. To combat class imbalance, we apply tailored loss functions and generate categorical decisions through majority voting. Despite one model not being fully trained, the Abhinaya system ranked 4th among 166 submissions. Upon completion of training, it achieved state-of-the-art performance among published results, demonstrating the effectiveness of our approach for SER in real-world conditions. </p>
<blockquote>
<p>åœ¨è‡ªç„¶åœºæ™¯ä¸­çš„è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰ç”±äºå†…åœ¨çš„å¯å˜æ€§ã€å¤šæ ·çš„å½•éŸ³æ¡ä»¶å’Œç±»åˆ«ä¸å¹³è¡¡ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ä½œä¸ºä¸“æ³¨äºè¿™äº›å¤æ‚æ€§çš„Interspeechè‡ªç„¶ä¸»ä¹‰SERæŒ‘æˆ˜çš„å‚ä¸è€…ï¼Œæˆ‘ä»¬æ¨å‡ºäº†é˜¿å®¾äºšé›…ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿèåˆäº†åŸºäºè¯­éŸ³ã€æ–‡æœ¬å’Œè¯­éŸ³æ–‡æœ¬æ¨¡å‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡å¾®è°ƒè‡ªæˆ‘ç›‘ç£å’Œè¯­éŸ³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆSLLMï¼‰è¿›è¡Œè¯­éŸ³è¡¨ç¤ºï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æä¾›æ–‡æœ¬ä¸Šä¸‹æ–‡ï¼Œå¹¶é‡‡ç”¨å¸¦æœ‰SLLMçš„è¯­éŸ³æ–‡æœ¬å»ºæ¨¡æ¥æ•æ‰å¾®å¦™çš„æƒ…æ„Ÿçº¿ç´¢ã€‚ä¸ºäº†è§£å†³ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ï¼Œæˆ‘ä»¬åº”ç”¨äº†ä¸“é—¨çš„æŸå¤±å‡½æ•°ï¼Œå¹¶é€šè¿‡å¤šæ•°æŠ•ç¥¨äº§ç”Ÿåˆ†ç±»å†³ç­–ã€‚å°½ç®¡æœ‰ä¸€ä¸ªæ¨¡å‹å°šæœªå®Œå…¨è®­ç»ƒï¼Œä½†é˜¿å®¾äºšé›…ç³»ç»Ÿåœ¨166ä¸ªæäº¤ä½œå“ä¸­æ’åç¬¬4ã€‚è®­ç»ƒå®Œæˆåï¼Œå®ƒåœ¨å·²å‘å¸ƒçš„ç»“æœä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨çœŸå®ä¸–ç•Œæ¡ä»¶ä¸‹çš„SERä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.18217v1">PDF</a> 5 pages, 2 figures, 4 tables, accepted at Interspeech 2025</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†é’ˆå¯¹è‡ªç„¶åœºæ™¯ä¸‹çš„è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºAbhinayaçš„ç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿç»“åˆäº†åŸºäºè¯­éŸ³ã€æ–‡æœ¬å’Œè¯­éŸ³æ–‡æœ¬æ¨¡å‹ï¼Œé€šè¿‡å¾®è°ƒè‡ªæˆ‘ç›‘ç£å’Œè¯­éŸ³å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œè¯­éŸ³è¡¨ç¤ºï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æä¾›æ–‡æœ¬èƒŒæ™¯ï¼Œå¹¶é€šè¿‡è¯­éŸ³æ–‡æœ¬å»ºæ¨¡æ•æ‰å¾®å¦™çš„æƒ…æ„Ÿçº¿ç´¢ã€‚ä¸ºåº”å¯¹ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ï¼Œé‡‡ç”¨äº†å®šåˆ¶çš„æŸå¤±å‡½æ•°ï¼Œå¹¶é€šè¿‡å¤šæ•°æŠ•ç¥¨è¿›è¡Œç±»åˆ«å†³ç­–ã€‚Abhinayaç³»ç»Ÿåœ¨æœªå®Œå…¨è®­ç»ƒçš„æƒ…å†µä¸‹æ’åç¬¬4ï¼Œä¸”åœ¨å®Œæˆè®­ç»ƒåè¾¾åˆ°äº†æœ€æ–°çš„æ€§èƒ½æ°´å¹³ï¼Œè¯æ˜äº†è¯¥ç³»ç»Ÿåœ¨è‡ªç„¶æ¡ä»¶ä¸‹çš„SERçš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†é’ˆå¯¹è‡ªç„¶åœºæ™¯ä¸‹çš„è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰çš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†åä¸ºAbhinayaçš„ç³»ç»Ÿï¼Œç»“åˆäº†åŸºäºè¯­éŸ³ã€æ–‡æœ¬å’Œè¯­éŸ³æ–‡æœ¬æ¨¡å‹çš„æ–¹æ³•ã€‚</li>
<li>é€šè¿‡å¾®è°ƒè‡ªæˆ‘ç›‘ç£å’Œè¯­éŸ³å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œè¯­éŸ³è¡¨ç¤ºã€‚</li>
<li>åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æä¾›æ–‡æœ¬èƒŒæ™¯ä¿¡æ¯ã€‚</li>
<li>é‡‡ç”¨è¯­éŸ³æ–‡æœ¬å»ºæ¨¡ä»¥æ•æ‰å¾®å¦™çš„æƒ…æ„Ÿçº¿ç´¢ã€‚</li>
<li>é‡‡ç”¨å®šåˆ¶çš„æŸå¤±å‡½æ•°å’Œå¤šæ•°æŠ•ç¥¨æœºåˆ¶åº”å¯¹ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.18217">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bac56ca59f187eacecea26e29cf09c71.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fc5fe34cdccde021049dc0fc5b11c05a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-63af187fac36173816bc48325fa845a3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9d14934eb36afb731609b8f90a8a787d.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Dysfluent-WFST-A-Framework-for-Zero-Shot-Speech-Dysfluency-Transcription-and-Detection"><a href="#Dysfluent-WFST-A-Framework-for-Zero-Shot-Speech-Dysfluency-Transcription-and-Detection" class="headerlink" title="Dysfluent WFST: A Framework for Zero-Shot Speech Dysfluency   Transcription and Detection"></a>Dysfluent WFST: A Framework for Zero-Shot Speech Dysfluency   Transcription and Detection</h2><p><strong>Authors:Chenxu Guo, Jiachen Lian, Xuanru Zhou, Jinming Zhang, Shuhe Li, Zongli Ye, Hwi Joo Park, Anaisha Das, Zoe Ezzes, Jet Vonk, Brittany Morin, Rian Bogley, Lisa Wauters, Zachary Miller, Maria Gorno-Tempini, Gopala Anumanchipalli</strong></p>
<p>Automatic detection of speech dysfluency aids speech-language pathologists in efficient transcription of disordered speech, enhancing diagnostics and treatment planning. Traditional methods, often limited to classification, provide insufficient clinical insight, and text-independent models misclassify dysfluency, especially in context-dependent cases. This work introduces Dysfluent-WFST, a zero-shot decoder that simultaneously transcribes phonemes and detects dysfluency. Unlike previous models, Dysfluent-WFST operates with upstream encoders like WavLM and requires no additional training. It achieves state-of-the-art performance in both phonetic error rate and dysfluency detection on simulated and real speech data. Our approach is lightweight, interpretable, and effective, demonstrating that explicit modeling of pronunciation behavior in decoding, rather than complex architectures, is key to improving dysfluency processing systems. </p>
<blockquote>
<p>è‡ªåŠ¨æ£€æµ‹è¯­è¨€æµç•…éšœç¢æœ‰åŠ©äºè¯­è¨€ç—…ç†å­¦å®¶é«˜æ•ˆåœ°è½¬å½•å¼‚å¸¸è¯­éŸ³ï¼Œæé«˜è¯Šæ–­å’Œåˆ¶å®šæ²»ç–—æ–¹æ¡ˆã€‚ä¼ ç»Ÿæ–¹æ³•å¾€å¾€å±€é™äºåˆ†ç±»ï¼Œæ— æ³•æä¾›è¶³å¤Ÿçš„ä¸´åºŠè§è§£ï¼Œè€Œç‹¬ç«‹äºæ–‡æœ¬ä¹‹å¤–çš„æ¨¡å‹åœ¨æ£€æµ‹è¯­è¨€æµç•…éšœç¢æ—¶ä¼šå‡ºç°è¯¯åˆ¤ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¾èµ–äºè¯­å¢ƒçš„æ¡ˆä¾‹ä¸­ã€‚æœ¬ç ”ç©¶å¼•å…¥äº†Dysfluent-WFSTï¼Œè¿™æ˜¯ä¸€ç§é›¶æ ·æœ¬è§£ç å™¨ï¼Œå¯åŒæ—¶è½¬å½•éŸ³ç´ å¹¶æ£€æµ‹è¯­è¨€æµç•…éšœç¢ã€‚ä¸åŒäºä¹‹å‰çš„æ¨¡å‹ï¼ŒDysfluent-WFSTä¸ä¸Šæ¸¸ç¼–ç å™¨ï¼ˆå¦‚WavLMï¼‰ååŒå·¥ä½œï¼Œæ— éœ€é¢å¤–è®­ç»ƒã€‚å®ƒåœ¨æ¨¡æ‹Ÿå’ŒçœŸå®è¯­éŸ³æ•°æ®ä¸Šå®ç°äº†æœ€ä½³çš„æ€§èƒ½ï¼Œæ— è®ºæ˜¯åœ¨éŸ³ç´ é”™è¯¯ç‡è¿˜æ˜¯è¯­è¨€æµç•…æ€§æ£€æµ‹æ–¹é¢ã€‚æˆ‘ä»¬çš„æ–¹æ³•è½»å·§ã€å¯è§£é‡Šã€æœ‰æ•ˆï¼Œè¡¨æ˜åœ¨è§£ç è¿‡ç¨‹ä¸­æ˜ç¡®å»ºæ¨¡å‘éŸ³è¡Œä¸ºï¼Œè€Œä¸æ˜¯å¤æ‚çš„æ¶æ„ï¼Œæ˜¯æé«˜è¯­è¨€æµç•…éšœç¢å¤„ç†ç³»ç»Ÿçš„å…³é”®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16351v2">PDF</a> Accepted for Interspeech2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºDysfluent-WFSTçš„é›¶æ ·æœ¬è§£ç å™¨ï¼Œå®ƒèƒ½åŒæ—¶è½¬å½•è¯­éŸ³å¹¶æ£€æµ‹è¯­è¨€æµç•…æ€§é—®é¢˜ã€‚ä¸ä¼ ç»Ÿçš„åˆ†ç±»æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥è§£ç å™¨åœ¨æ¨¡æ‹Ÿå’Œå®é™…è¯­éŸ³æ•°æ®çš„è¯­éŸ³é”™è¯¯ç‡å’Œè¯­è¨€æµç•…æ€§æ£€æµ‹æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚å…¶é‡‡ç”¨è½»é‡çº§ã€å¯è§£é‡Šçš„è®¾è®¡ï¼Œå¼ºè°ƒåœ¨è§£ç è¿‡ç¨‹ä¸­æ˜ç¡®å»ºæ¨¡å‘éŸ³è¡Œä¸ºæ˜¯æé«˜è¯­è¨€æµç•…æ€§å¤„ç†ç³»ç»Ÿçš„å…³é”®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªåŠ¨æ£€æµ‹è¯­è¨€æµç•…æ€§æœ‰åŠ©äºè¯­è¨€ç—…ç†å­¦å®¶æœ‰æ•ˆè½¬å½•éšœç¢æ€§è¯­è¨€ï¼Œæå‡è¯Šæ–­å’Œåˆ¶å®šæ²»ç–—æ–¹æ¡ˆã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•ä¸»è¦é™äºåˆ†ç±»ï¼Œæä¾›æœ‰é™çš„ä¸´åºŠæ´å¯Ÿï¼Œè€Œæ–‡æœ¬ç‹¬ç«‹æ¨¡å‹åœ¨è¯­å¢ƒä¾èµ–çš„æ¡ˆä¾‹ä¸­è¯¯åˆ†ç±»è¯­è¨€æµç•…æ€§é—®é¢˜ã€‚</li>
<li>Dysfluent-WFSTæ˜¯ä¸€ç§é›¶æ ·æœ¬è§£ç å™¨ï¼Œå¯åŒæ—¶è¿›è¡Œè¯­éŸ³è½¬å½•å’Œè¯­è¨€æµç•…æ€§æ£€æµ‹ã€‚</li>
<li>ä¸å…¶ä»–æ¨¡å‹ä¸åŒï¼ŒDysfluent-WFSTä½¿ç”¨ä¸Šæ¸¸ç¼–ç å™¨å¦‚WavLMï¼Œæ— éœ€é¢å¤–è®­ç»ƒã€‚</li>
<li>Dysfluent-WFSTåœ¨æ¨¡æ‹Ÿå’Œå®é™…è¯­éŸ³æ•°æ®ä¸Šå®ç°äº†è¯­éŸ³é”™è¯¯ç‡å’Œè¯­è¨€æµç•…æ€§æ£€æµ‹çš„æœ€æ–°æ€§èƒ½ã€‚</li>
<li>è¯¥æ–¹æ³•é‡‡ç”¨è½»é‡çº§ã€å¯è§£é‡Šçš„è®¾è®¡ï¼Œå¼ºè°ƒåœ¨è§£ç è¿‡ç¨‹ä¸­æ˜ç¡®å»ºæ¨¡å‘éŸ³è¡Œä¸ºçš„é‡è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16351">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-78b9bb9e4efe2137a8d24d8858a976c0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-72dbb715284d0f9e467b80eaea1fb515.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c698578373eddacab7d8c2ec84c99f22.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-db83b56946c95c5fc3097276cb396e02.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7fc99bdb2d2621beb13e9ce8a2babca7.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Data-Efficient-Hate-Speech-Detection-via-Cross-Lingual-Nearest-Neighbor-Retrieval-with-Limited-Labeled-Data"><a href="#Data-Efficient-Hate-Speech-Detection-via-Cross-Lingual-Nearest-Neighbor-Retrieval-with-Limited-Labeled-Data" class="headerlink" title="Data-Efficient Hate Speech Detection via Cross-Lingual Nearest Neighbor   Retrieval with Limited Labeled Data"></a>Data-Efficient Hate Speech Detection via Cross-Lingual Nearest Neighbor   Retrieval with Limited Labeled Data</h2><p><strong>Authors:Faeze Ghorbanpour, Daryna Dementieva, Alexander Fraser</strong></p>
<p>Considering the importance of detecting hateful language, labeled hate speech data is expensive and time-consuming to collect, particularly for low-resource languages. Prior work has demonstrated the effectiveness of cross-lingual transfer learning and data augmentation in improving performance on tasks with limited labeled data. To develop an efficient and scalable cross-lingual transfer learning approach, we leverage nearest-neighbor retrieval to augment minimal labeled data in the target language, thereby enhancing detection performance. Specifically, we assume access to a small set of labeled training instances in the target language and use these to retrieve the most relevant labeled examples from a large multilingual hate speech detection pool. We evaluate our approach on eight languages and demonstrate that it consistently outperforms models trained solely on the target language data. Furthermore, in most cases, our method surpasses the current state-of-the-art. Notably, our approach is highly data-efficient, retrieving as small as 200 instances in some cases while maintaining superior performance. Moreover, it is scalable, as the retrieval pool can be easily expanded, and the method can be readily adapted to new languages and tasks. We also apply maximum marginal relevance to mitigate redundancy and filter out highly similar retrieved instances, resulting in improvements in some languages. </p>
<blockquote>
<p>è€ƒè™‘åˆ°æ£€æµ‹ä»‡æ¨æ€§è¨€è®ºçš„é‡è¦æ€§ï¼Œæ ‡æ³¨ä»‡æ¨è¨€è®ºçš„æ•°æ®æ”¶é›†æ—¢æ˜‚è´µåˆè€—æ—¶ï¼Œç‰¹åˆ«æ˜¯å¯¹äºèµ„æºè´«ä¹çš„è¯­è¨€æ¥è¯´ã€‚å…ˆå‰çš„ç ”ç©¶å·²ç»è¯æ˜äº†è·¨è¯­è¨€è¿ç§»å­¦ä¹ å’Œæ•°æ®å¢å¼ºåœ¨æ”¹å–„æœ‰é™æ ‡è®°æ•°æ®ä»»åŠ¡æ€§èƒ½æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚ä¸ºäº†å¼€å‘ä¸€ç§é«˜æ•ˆä¸”å¯æ‰©å±•çš„è·¨è¯­è¨€è¿ç§»å­¦ä¹ æ–¹æ³•ï¼Œæˆ‘ä»¬åˆ©ç”¨æœ€è¿‘é‚»æ£€ç´¢æ¥å¢å¼ºç›®æ ‡è¯­è¨€çš„å°‘é‡æ ‡è®°æ•°æ®ï¼Œä»è€Œæé«˜æ£€æµ‹æ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å‡è®¾ç›®æ ‡è¯­è¨€ä¸­æœ‰å°‘é‡æ ‡è®°çš„è®­ç»ƒå®ä¾‹å¯ä¾›è®¿é—®ï¼Œå¹¶ä½¿ç”¨è¿™äº›å®ä¾‹ä»å¤§å‹å¤šè¯­è¨€ä»‡æ¨è¨€è®ºæ£€æµ‹æ± ä¸­æ£€ç´¢æœ€ç›¸å…³çš„æ ‡è®°ç¤ºä¾‹ã€‚æˆ‘ä»¬åœ¨å…«ç§è¯­è¨€ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œå¹¶è¯æ˜å®ƒå§‹ç»ˆä¼˜äºä»…ä½¿ç”¨ç›®æ ‡è¯­è¨€æ•°æ®è®­ç»ƒçš„æ¨¡å‹ã€‚æ­¤å¤–ï¼Œåœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¶…è¶Šäº†å½“å‰çš„æœ€ä½³æ°´å¹³ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•éå¸¸é«˜æ•ˆï¼Œåœ¨æŸäº›æƒ…å†µä¸‹åªéœ€æ£€ç´¢200ä¸ªå®ä¾‹å°±èƒ½ä¿æŒå“è¶Šçš„æ€§èƒ½ã€‚è€Œä¸”ï¼Œå®ƒæ˜¯å¯æ‰©å±•çš„ï¼Œå› ä¸ºæ£€ç´¢æ± å¯ä»¥å¾ˆå®¹æ˜“åœ°æ‰©å±•ï¼Œå¹¶ä¸”è¯¥æ–¹æ³•å¯ä»¥å¾ˆå®¹æ˜“åœ°é€‚åº”æ–°çš„è¯­è¨€å’Œä»»åŠ¡ã€‚æˆ‘ä»¬è¿˜åº”ç”¨æœ€å¤§è¾¹ç¼˜ç›¸å…³æ€§æ¥ç¼“è§£å†—ä½™å¹¶è¿‡æ»¤æ‰é«˜åº¦ç›¸ä¼¼çš„æ£€ç´¢å®ä¾‹ï¼Œä»è€Œåœ¨æŸäº›è¯­è¨€ä¸­å–å¾—äº†æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14272v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶å…³æ³¨ä»‡æ¨è¨€è®ºæ£€æµ‹çš„é‡è¦æ€§ï¼Œé’ˆå¯¹ä½èµ„æºè¯­è¨€æ ‡æ³¨ä»‡æ¨è¨€è®ºæ•°æ®æ”¶é›†çš„é«˜æˆæœ¬å’Œè€—æ—¶çš„æŒ‘æˆ˜ï¼Œæå‡ºä¸€ç§é«˜æ•ˆä¸”å¯æ‰©å±•çš„è·¨è¯­è¨€è¿ç§»å­¦ä¹ æ–¹æ³•ã€‚é€šè¿‡åˆ©ç”¨æœ€è¿‘é‚»æ£€ç´¢æŠ€æœ¯å¢å¼ºç›®æ ‡è¯­è¨€çš„å°‘é‡æ ‡æ³¨æ•°æ®ï¼Œæé«˜æ£€æµ‹æ€§èƒ½ã€‚ç ”ç©¶å‡è®¾æ‹¥æœ‰ç›®æ ‡è¯­è¨€çš„å°‘é‡æ ‡æ³¨è®­ç»ƒå®ä¾‹ï¼Œå¹¶ä»å¤§å‹å¤šè¯­è¨€ä»‡æ¨è¨€è®ºæ£€æµ‹æ± ä¸­æ£€ç´¢æœ€ç›¸å…³çš„å®ä¾‹ã€‚åœ¨å…«ç§è¯­è¨€ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•ä¼˜äºä»…ä½¿ç”¨ç›®æ ‡è¯­è¨€æ•°æ®çš„æ¨¡å‹ï¼Œå¹¶åœ¨å¤šæ•°æƒ…å†µä¸‹è¾¾åˆ°å½“å‰æœ€ä½³æ°´å¹³ã€‚è¯¥æ–¹æ³•å…·æœ‰é«˜æ•ˆæ€§å’Œå¯æ‰©å±•æ€§ï¼Œå¯è½»æ¾æ‰©å±•æ£€ç´¢æ± ï¼Œå¹¶æ˜“äºé€‚åº”æ–°è¯­è¨€å’Œä»»åŠ¡ã€‚åŒæ—¶ï¼Œç ”ç©¶è¿˜é‡‡ç”¨æœ€å¤§è¾¹é™…ç›¸å…³æ€§æ–¹æ³•å‡å°‘å†—ä½™ï¼Œè¿‡æ»¤å‡ºé«˜åº¦ç›¸ä¼¼çš„æ£€ç´¢å®ä¾‹ï¼Œè¿›ä¸€æ­¥æé«˜æŸäº›è¯­è¨€çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è·¨è¯­è¨€è¿ç§»å­¦ä¹ ç”¨äºæé«˜ä½èµ„æºè¯­è¨€çš„ä»‡æ¨è¨€è®ºæ£€æµ‹æ€§èƒ½ã€‚</li>
<li>åˆ©ç”¨æœ€è¿‘é‚»æ£€ç´¢æŠ€æœ¯å¢å¼ºç›®æ ‡è¯­è¨€çš„å°‘é‡æ ‡æ³¨æ•°æ®ã€‚</li>
<li>æ–¹æ³•é€šè¿‡æ£€ç´¢æœ€ç›¸å…³çš„å®ä¾‹ä»å¤§å‹å¤šè¯­è¨€ä»‡æ¨è¨€è®ºæ£€æµ‹æ± ä¸­ä¸°å¯Œæ•°æ®ã€‚</li>
<li>åœ¨å…«ç§è¯­è¨€ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºè¯¥æ–¹æ³•ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</li>
<li>æ–¹æ³•å…·æœ‰é«˜æ•ˆæ€§å’Œå¯æ‰©å±•æ€§ï¼Œå¯é€‚åº”æ–°è¯­è¨€å’Œä»»åŠ¡ã€‚</li>
<li>é‡‡ç”¨æœ€å¤§è¾¹é™…ç›¸å…³æ€§æ–¹æ³•å‡å°‘å†—ä½™æ£€ç´¢å®ä¾‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14272">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-37be0e9e3c247cc09d7d80cc71c3e764.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f467bca691d37698d9f05f6135e64e4d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-331cbf8c3b53eb673a660dd4621f4ead.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="LipDiffuser-Lip-to-Speech-Generation-with-Conditional-Diffusion-Models"><a href="#LipDiffuser-Lip-to-Speech-Generation-with-Conditional-Diffusion-Models" class="headerlink" title="LipDiffuser: Lip-to-Speech Generation with Conditional Diffusion Models"></a>LipDiffuser: Lip-to-Speech Generation with Conditional Diffusion Models</h2><p><strong>Authors:Danilo de Oliveira, Julius Richter, Tal Peer, Timo Gerkmann</strong></p>
<p>We present LipDiffuser, a conditional diffusion model for lip-to-speech generation synthesizing natural and intelligible speech directly from silent video recordings. Our approach leverages the magnitude-preserving ablated diffusion model (MP-ADM) architecture as a denoiser model. To effectively condition the model, we incorporate visual features using magnitude-preserving feature-wise linear modulation (MP-FiLM) alongside speaker embeddings. A neural vocoder then reconstructs the speech waveform from the generated mel-spectrograms. Evaluations on LRS3 and TCD-TIMIT demonstrate that LipDiffuser outperforms existing lip-to-speech baselines in perceptual speech quality and speaker similarity, while remaining competitive in downstream automatic speech recognition (ASR). These findings are also supported by a formal listening experiment. Extensive ablation studies and cross-dataset evaluation confirm the effectiveness and generalization capabilities of our approach. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†LipDiffuserï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºå”‡è¯­åˆ°è¯­éŸ³ç”Ÿæˆçš„æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼Œå®ƒå¯ä»¥ç›´æ¥ä»æ— å£°çš„è§†é¢‘è®°å½•ä¸­åˆæˆè‡ªç„¶å’Œå¯ç†è§£çš„è¯­éŸ³ã€‚æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨å¹…åº¦ä¿æŒæ¶ˆèæ‰©æ•£æ¨¡å‹ï¼ˆMP-ADMï¼‰æ¶æ„ä½œä¸ºå»å™ªæ¨¡å‹ã€‚ä¸ºäº†æœ‰æ•ˆåœ°å¯¹æ¨¡å‹è¿›è¡Œæ¡ä»¶å¤„ç†ï¼Œæˆ‘ä»¬é‡‡ç”¨å¹…åº¦ä¿æŒç‰¹å¾çº¿æ€§è°ƒåˆ¶ï¼ˆMP-FiLMï¼‰ç»“åˆè¯­éŸ³è€…åµŒå…¥æ¥èå…¥è§†è§‰ç‰¹å¾ã€‚ç„¶åï¼Œç¥ç»ç½‘ç»œvocoderä»ç”Ÿæˆçš„æ¢…å°”é¢‘è°±ä¸­é‡å»ºè¯­éŸ³æ³¢å½¢ã€‚åœ¨LRS3å’ŒTCD-TIMITä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒLipDiffuseråœ¨æ„ŸçŸ¥è¯­éŸ³è´¨é‡å’Œè¯­éŸ³è€…ç›¸ä¼¼æ€§æ–¹é¢ä¼˜äºç°æœ‰çš„å”‡è¯­åˆ°è¯­éŸ³åŸºçº¿ï¼ŒåŒæ—¶åœ¨ä¸‹æ¸¸è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ä¸­ä¿æŒç«äº‰åŠ›ã€‚æ­£å¼å¬åŠ›å­¦å®éªŒçš„ç»“æœä¹Ÿæ”¯æŒäº†è¿™ä¸€å‘ç°ã€‚å¹¿æ³›çš„æ¶ˆèç ”ç©¶å’Œè·¨æ•°æ®é›†è¯„ä¼°è¯å®äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11391v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†LipDiffuserï¼Œä¸€ç§ç”¨äºå”‡è¯­ç”Ÿæˆçš„æ¡ä»¶æ‰©æ•£æ¨¡å‹ã€‚å®ƒé€šè¿‡é‡‡ç”¨MP-ADMæ¶æ„ä½œä¸ºå»å™ªæ¨¡å‹ï¼Œç»“åˆMP-FiLMè§†è§‰ç‰¹å¾å’Œè¯´è¯äººåµŒå…¥è¿›è¡Œæ¡ä»¶åŒ–å»ºæ¨¡ï¼Œå®ç°äº†ä»æ— å£°è§†é¢‘å½•åˆ¶ä¸­ç›´æ¥åˆæˆè‡ªç„¶å¯ç†è§£çš„è¯­éŸ³ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨LRS3å’ŒTCD-TIMITæ•°æ®é›†ä¸Šï¼ŒLipDiffuseråœ¨æ„ŸçŸ¥è¯­éŸ³è´¨é‡å’Œè¯´è¯äººç›¸ä¼¼æ€§æ–¹é¢ä¼˜äºç°æœ‰çš„å”‡è¯­åŸºçº¿ï¼ŒåŒæ—¶åœ¨ä¸‹æ¸¸è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ–¹é¢ä¿æŒç«äº‰åŠ›ã€‚æ­£å¼å¬åŠ›å­¦å®éªŒè¿›ä¸€æ­¥è¯å®äº†è¿™äº›å‘ç°ã€‚å¹¿æ³›çš„æ¶ˆèç ”ç©¶å’Œè·¨æ•°æ®é›†è¯„ä¼°è¯å®äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LipDiffuseræ˜¯ä¸€ç§æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼Œç”¨äºä»æ— å£°è§†é¢‘ç”Ÿæˆè‡ªç„¶å¯ç†è§£çš„è¯­éŸ³ã€‚</li>
<li>MP-ADMæ¶æ„ä½œä¸ºå»å™ªæ¨¡å‹ç”¨äºå¤„ç†æ ¸å¿ƒä»»åŠ¡ã€‚</li>
<li>MP-FiLMè§†è§‰ç‰¹å¾å’Œè¯´è¯äººåµŒå…¥ç”¨äºæœ‰æ•ˆåœ°æ¡ä»¶åŒ–æ¨¡å‹ã€‚</li>
<li>ç¥ç»vocoderä»ç”Ÿæˆçš„mel-spectrogramsé‡å»ºè¯­éŸ³æ³¢å½¢ã€‚</li>
<li>åœ¨LRS3å’ŒTCD-TIMITæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒLipDiffuseråœ¨æ„ŸçŸ¥è¯­éŸ³è´¨é‡å’Œè¯´è¯äººç›¸ä¼¼æ€§æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>æ­£å¼å¬åŠ›å­¦å®éªŒè¯å®äº†è¿™äº›å‘ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11391">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3eb0b4c9a2d97d9dbaddb39e3fd53949.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d0d352f2b6b3f6603cbc19a09cafcf1e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3ed1eadf42050b23610113edaeee7b2f.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="SepALM-Audio-Language-Models-Are-Error-Correctors-for-Robust-Speech-Separation"><a href="#SepALM-Audio-Language-Models-Are-Error-Correctors-for-Robust-Speech-Separation" class="headerlink" title="SepALM: Audio Language Models Are Error Correctors for Robust Speech   Separation"></a>SepALM: Audio Language Models Are Error Correctors for Robust Speech   Separation</h2><p><strong>Authors:Zhaoxi Mu, Xinyu Yang, Gang Wang</strong></p>
<p>While contemporary speech separation technologies adeptly process lengthy mixed audio waveforms, they are frequently challenged by the intricacies of real-world environments, including noisy and reverberant settings, which can result in artifacts or distortions in the separated speech. To overcome these limitations, we introduce SepALM, a pioneering approach that employs audio language models (ALMs) to rectify and re-synthesize speech within the text domain following preliminary separation. SepALM comprises four core components: a separator, a corrector, a synthesizer, and an aligner. By integrating an ALM-based end-to-end error correction mechanism, we mitigate the risk of error accumulation and circumvent the optimization hurdles typically encountered in conventional methods that amalgamate automatic speech recognition (ASR) with large language models (LLMs). Additionally, we have developed Chain-of-Thought (CoT) prompting and knowledge distillation techniques to facilitate the reasoning and training processes of the ALM. Our experiments substantiate that SepALM not only elevates the precision of speech separation but also markedly bolsters adaptability in novel acoustic environments. </p>
<blockquote>
<p>è™½ç„¶å½“ä»£çš„è¯­éŸ³è¯†åˆ«åˆ†ç¦»æŠ€æœ¯èƒ½å¤Ÿå¤„ç†å†—é•¿çš„æ··åˆéŸ³é¢‘æ³¢å½¢ï¼Œä½†å®ƒä»¬åœ¨å®é™…ç¯å¢ƒä¸­å¸¸å¸¸é¢ä¸´å¤æ‚æƒ…å†µï¼ŒåŒ…æ‹¬å™ªå£°å’Œæ··å“ç¯å¢ƒç­‰ï¼Œè¿™äº›éƒ½å¯èƒ½å¯¼è‡´åˆ†ç¦»åçš„è¯­éŸ³å‡ºç°ä¼ªåƒæˆ–å¤±çœŸã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™ï¼Œæˆ‘ä»¬å¼•å…¥äº†SepALMï¼Œè¿™æ˜¯ä¸€ç§é‡‡ç”¨éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆALMï¼‰åœ¨åˆæ­¥åˆ†ç¦»åä¿®å¤å’Œé‡æ–°åˆæˆæ–‡æœ¬é¢†åŸŸå†…çš„è¯­éŸ³çš„å¼€åˆ›æ€§æ–¹æ³•ã€‚SepALMåŒ…å«å››ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šåˆ†ç¦»å™¨ã€æ ¡æ­£å™¨ã€åˆæˆå™¨å’Œå¯¹é½å™¨ã€‚é€šè¿‡é›†æˆåŸºäºALMçš„ç«¯åˆ°ç«¯é”™è¯¯æ ¡æ­£æœºåˆ¶ï¼Œæˆ‘ä»¬é™ä½äº†è¯¯å·®ç´¯ç§¯çš„é£é™©ï¼Œå¹¶ç»•è¿‡äº†ä¼ ç»Ÿæ–¹æ³•ä¸­é‡åˆ°çš„ä¼˜åŒ–éšœç¢ï¼Œè¿™äº›æ–¹æ³•å°†è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åˆå¹¶ä½¿ç”¨ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼€å‘äº†é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æç¤ºå’ŒçŸ¥è¯†è’¸é¦æŠ€æœ¯ï¼Œä»¥ä¿ƒè¿›ALMçš„æ¨ç†å’Œè®­ç»ƒè¿‡ç¨‹ã€‚æˆ‘ä»¬çš„å®éªŒè¯å®ï¼ŒSepALMä¸ä»…æé«˜äº†è¯­éŸ³åˆ†ç¦»çš„ç²¾åº¦ï¼Œè¿˜æ˜¾è‘—æé«˜äº†å¯¹æ–°ç¯å¢ƒéŸ³çš„é€‚åº”æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.03273v2">PDF</a> Appears in IJCAI 2025</p>
<p><strong>Summary</strong>ï¼šé’ˆå¯¹å½“å‰è¯­éŸ³åˆ†ç¦»æŠ€æœ¯åœ¨å¤„ç†å¤æ‚ç°å®ç¯å¢ƒï¼ˆå¦‚å™ªå£°å’Œå›å£°ç¯å¢ƒï¼‰æ—¶é¢ä¸´çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºSepALMçš„åˆ›æ–°æ–¹æ³•ã€‚è¯¥æ–¹æ³•é‡‡ç”¨éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆALMï¼‰åœ¨æ–‡æœ¬åŸŸå†…è¿›è¡Œä¿®æ­£å’Œé‡æ–°åˆæˆã€‚SepALMåŒ…æ‹¬å››ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šåˆ†ç¦»å™¨ã€æ ¡æ­£å™¨ã€åˆæˆå™¨å’Œå¯¹é½å™¨ã€‚é€šè¿‡ç«¯åˆ°ç«¯çš„é”™è¯¯æ ¡æ­£æœºåˆ¶ï¼Œå‡å°‘äº†è¯¯å·®ç´¯ç§¯çš„é£é™©ï¼Œå¹¶å…‹æœäº†ä¼ ç»Ÿæ–¹æ³•ä¸­è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èåˆçš„ä¼˜åŒ–éš¾é¢˜ã€‚å®éªŒè¯æ˜ï¼ŒSepALMä¸ä»…æé«˜äº†è¯­éŸ³åˆ†ç¦»çš„ç²¾åº¦ï¼Œè¿˜æ˜¾è‘—æé«˜äº†å¯¹æ–°ç¯å¢ƒçš„é€‚åº”èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>SepALMæ˜¯ä¸€ç§é’ˆå¯¹å¤æ‚ç°å®ç¯å¢ƒè®¾è®¡çš„åˆ›æ–°è¯­éŸ³åˆ†ç¦»æ–¹æ³•ã€‚</li>
<li>SepALMé‡‡ç”¨éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆALMï¼‰åœ¨æ–‡æœ¬åŸŸè¿›è¡Œä¿®æ­£å’Œé‡æ–°åˆæˆã€‚</li>
<li>SepALMåŒ…æ‹¬åˆ†ç¦»å™¨ã€æ ¡æ­£å™¨ã€åˆæˆå™¨å’Œå¯¹é½å™¨å››ä¸ªæ ¸å¿ƒç»„ä»¶ã€‚</li>
<li>é€šè¿‡ç«¯åˆ°ç«¯çš„é”™è¯¯æ ¡æ­£æœºåˆ¶ï¼ŒSepALMå‡å°‘äº†è¯¯å·®ç´¯ç§¯çš„é£é™©ã€‚</li>
<li>SepALMå…‹æœäº†ä¼ ç»Ÿæ–¹æ³•ä¸­ASRä¸LLMèåˆçš„ä¼˜åŒ–éš¾é¢˜ã€‚</li>
<li>å®éªŒè¯æ˜SepALMæé«˜äº†è¯­éŸ³åˆ†ç¦»çš„ç²¾åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.03273">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-80444f5565c4b1078184199006344d0f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5bd4438546dbe67bd97e63c99d89b160.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ed17bf3bba7f560f07e4be5a3917455e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6c1aa4cfbfb4479b1589955bea09bb0a.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Sugar-Coated-Poison-Benign-Generation-Unlocks-LLM-Jailbreaking"><a href="#Sugar-Coated-Poison-Benign-Generation-Unlocks-LLM-Jailbreaking" class="headerlink" title="Sugar-Coated Poison: Benign Generation Unlocks LLM Jailbreaking"></a>Sugar-Coated Poison: Benign Generation Unlocks LLM Jailbreaking</h2><p><strong>Authors:Yu-Hang Wu, Yu-Jie Xiong, Hao Zhang, Jia-Chen Zhang, Zheng Zhou</strong></p>
<p>With the increasingly deep integration of large language models (LLMs) across diverse domains, the effectiveness of their safety mechanisms is encountering severe challenges. Currently, jailbreak attacks based on prompt engineering have become a major safety threat. However, existing methods primarily rely on black-box manipulation of prompt templates, resulting in poor interpretability and limited generalization. To break through the bottleneck, this study first introduces the concept of Defense Threshold Decay (DTD), revealing the potential safety impact caused by LLMsâ€™ benign generation: as benign content generation in LLMs increases, the modelâ€™s focus on input instructions progressively diminishes. Building on this insight, we propose the Sugar-Coated Poison (SCP) attack paradigm, which uses a â€œsemantic reversalâ€ strategy to craft benign inputs that are opposite in meaning to malicious intent. This strategy induces the models to generate extensive benign content, thereby enabling adversarial reasoning to bypass safety mechanisms. Experiments show that SCP outperforms existing baselines. Remarkably, it achieves an average attack success rate of 87.23% across six LLMs. For defense, we propose Part-of-Speech Defense (POSD), leveraging verb-noun dependencies for syntactic analysis to enhance safety of LLMs while preserving their generalization ability. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å„ä¸ªé¢†åŸŸè¶Šæ¥è¶Šæ·±å…¥çš„èåˆï¼Œå…¶å®‰å…¨æœºåˆ¶çš„æœ‰æ•ˆæ€§æ­£é¢ä¸´ä¸¥å³»æŒ‘æˆ˜ã€‚ç›®å‰ï¼ŒåŸºäºæç¤ºå·¥ç¨‹çš„è¶Šç‹±æ”»å‡»å·²æˆä¸ºä¸»è¦çš„å®‰å…¨å¨èƒã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–äºæç¤ºæ¨¡æ¿çš„é»‘ç›’æ“ä½œï¼Œå¯¼è‡´è§£é‡Šæ€§å·®å’Œæ³›åŒ–èƒ½åŠ›æœ‰é™ã€‚ä¸ºäº†çªç ´ç“¶é¢ˆï¼Œæœ¬ç ”ç©¶é¦–å…ˆå¼•å…¥é˜²å¾¡é˜ˆå€¼è¡°å‡ï¼ˆDTDï¼‰çš„æ¦‚å¿µï¼Œæ­ç¤ºLLMsè‰¯æ€§ç”Ÿæˆæ‰€æ½œåœ¨çš„å®‰å…¨å½±å“ï¼šéšç€LLMsä¸­è‰¯æ€§å†…å®¹ç”Ÿæˆçš„å¢åŠ ï¼Œæ¨¡å‹å¯¹è¾“å…¥æŒ‡ä»¤çš„å…³æ³¨é€æ¸å‡å°‘ã€‚åŸºäºè¿™ä¸€è§è§£ï¼Œæˆ‘ä»¬æå‡ºäº†â€œç³–è¡£æ¯’è¯â€ï¼ˆSCPï¼‰æ”»å‡»èŒƒå¼ï¼Œé‡‡ç”¨â€œè¯­ä¹‰åè½¬â€ç­–ç•¥æ¥åˆ¶ä½œä¸æ¶æ„æ„å›¾ç›¸åå«ä¹‰çš„è‰¯æ€§è¾“å…¥ã€‚è¯¥ç­–ç•¥è¯±å¯¼æ¨¡å‹ç”Ÿæˆå¤§é‡è‰¯æ€§å†…å®¹ï¼Œä»è€Œé€šè¿‡å¯¹æŠ—æ€§æ¨ç†ç»•è¿‡å®‰å…¨æœºåˆ¶ã€‚å®éªŒè¡¨æ˜ï¼ŒSCPä¼˜äºç°æœ‰åŸºçº¿ã€‚å€¼å¾—ä¸€æçš„æ˜¯ï¼Œå®ƒåœ¨å…­ä¸ªLLMsä¸Šå¹³å‡æ”»å‡»æˆåŠŸç‡è¾¾åˆ°87.23%ã€‚ä¸ºé˜²å¾¡æ­¤æ”»å‡»ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºè¯æ€§çš„é˜²å¾¡ï¼ˆPOSDï¼‰æ–¹æ³•ï¼Œåˆ©ç”¨åŠ¨è¯-åè¯ä¾èµ–è¿›è¡Œå¥æ³•åˆ†ææ¥æé«˜LLMsçš„å®‰å…¨æ€§ï¼ŒåŒæ—¶ä¿ç•™å…¶æ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05652v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šé¢†åŸŸæ·±åº¦é›†æˆä¸­é¢ä¸´å®‰å…¨æŒ‘æˆ˜ï¼Œå‡ºç°äº†åŸºäºæç¤ºå·¥ç¨‹çš„è¶Šç‹±æ”»å‡»ç­‰å®‰å…¨å¨èƒã€‚ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–é»‘ç›’æ“ä½œæç¤ºæ¨¡æ¿ï¼Œå­˜åœ¨è§£é‡Šæ€§å·®å’Œæ³›åŒ–èƒ½åŠ›æœ‰é™çš„é—®é¢˜ã€‚æœ¬ç ”ç©¶å¼•å…¥é˜²å¾¡é˜ˆå€¼è¡°å‡ï¼ˆDTDï¼‰æ¦‚å¿µï¼Œæ­ç¤ºLLMè‰¯æ€§ç”Ÿæˆå¯¹å®‰å…¨çš„å½±å“ï¼šéšç€è‰¯æ€§å†…å®¹ç”Ÿæˆå¢åŠ ï¼Œæ¨¡å‹å¯¹è¾“å…¥æŒ‡ä»¤çš„å…³æ³¨é€æ¸å‡å°‘ã€‚åŸºäºæ­¤ï¼Œæå‡ºâ€œç³–è¡£æ¯’è¯â€ï¼ˆSCPï¼‰æ”»å‡»èŒƒå¼ï¼Œé‡‡ç”¨â€œè¯­ä¹‰åè½¬â€ç­–ç•¥åˆ¶ä½œä¸æ¶æ„æ„å›¾ç›¸åçš„è‰¯æ€§è¾“å…¥ï¼Œè¯±å¯¼æ¨¡å‹ç”Ÿæˆå¤§é‡è‰¯æ€§å†…å®¹ï¼Œå®ç°å¯¹æŠ—æ¨ç†ï¼Œç»•è¿‡å®‰å…¨æœºåˆ¶ã€‚å®éªŒæ˜¾ç¤ºï¼ŒSCPä¼˜äºç°æœ‰åŸºçº¿ï¼Œå¹³å‡æ”»å‡»æˆåŠŸç‡è¾¾87.23%ï¼Œè¦†ç›–å…­ç§LLMã€‚ä¸ºé˜²å¾¡ï¼Œæå‡ºåˆ©ç”¨åŠ¨è¯-åè¯ä¾èµ–è¿›è¡Œå¥æ³•åˆ†æçš„è¯æ€§é˜²å¾¡ï¼ˆPOSDï¼‰ï¼Œå¢å¼ºLLMå®‰å…¨æ€§åŒæ—¶ä¿æŒå…¶æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šä¸ªé¢†åŸŸé›†æˆæ—¶é¢ä¸´ä¸¥é‡çš„å®‰å…¨æŒ‘æˆ˜ã€‚</li>
<li>ç›®å‰å­˜åœ¨çš„æ”»å‡»æ–¹æ³•ä¸»è¦åŸºäºæç¤ºå·¥ç¨‹ï¼Œå­˜åœ¨è§£é‡Šæ€§å·®å’Œæ³›åŒ–èƒ½åŠ›æœ‰é™çš„é—®é¢˜ã€‚</li>
<li>å¼•å…¥é˜²å¾¡é˜ˆå€¼è¡°å‡ï¼ˆDTDï¼‰æ¦‚å¿µï¼Œæ­ç¤ºLLMè‰¯æ€§ç”Ÿæˆå¯¹å®‰å…¨çš„å½±å“ã€‚</li>
<li>æå‡ºâ€œç³–è¡£æ¯’è¯â€ï¼ˆSCPï¼‰æ”»å‡»ç­–ç•¥ï¼Œé€šè¿‡è¯­ä¹‰åè½¬ç»•è¿‡LLMçš„å®‰å…¨æœºåˆ¶ã€‚</li>
<li>SCPæ”»å‡»ç­–ç•¥åœ¨å®éªŒä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå¹³å‡æ”»å‡»æˆåŠŸç‡è¾¾87.23%ã€‚</li>
<li>ä¸ºåº”å¯¹æ­¤æ”»å‡»ï¼Œæå‡ºè¯æ€§é˜²å¾¡ï¼ˆPOSDï¼‰æ–¹æ³•ï¼Œç»“åˆå¥æ³•åˆ†æå¢å¼ºLLMå®‰å…¨æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05652">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f62f5a590735dd5e784e7893019b3a64.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2af7147a0bcd95ab1898082ac9737731.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-db3e77398928b5778a035b59da55a609.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b5d464b6bb8c449baa5cecf2881b16de.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-246a6aeaa2379e0b2623555fcef60a42.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4b6fd0f1af0553b9deeda62d4dbca168.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Speech-FT-Merging-Pre-trained-And-Fine-Tuned-Speech-Representation-Models-For-Cross-Task-Generalization"><a href="#Speech-FT-Merging-Pre-trained-And-Fine-Tuned-Speech-Representation-Models-For-Cross-Task-Generalization" class="headerlink" title="Speech-FT: Merging Pre-trained And Fine-Tuned Speech Representation   Models For Cross-Task Generalization"></a>Speech-FT: Merging Pre-trained And Fine-Tuned Speech Representation   Models For Cross-Task Generalization</h2><p><strong>Authors:Tzu-Quan Lin, Wei-Ping Huang, Hao Tang, Hung-yi Lee</strong></p>
<p>Fine-tuning speech representation models can enhance performance on specific tasks but often compromises their cross-task generalization ability. This degradation is often caused by excessive changes in the representations, making it difficult to retain information learned during pre-training. Existing approaches, such as regularizing weight changes during fine-tuning, may fail to maintain sufficiently high feature similarity with the pre-trained model, and thus could possibly lose cross-task generalization. To address this issue, we propose Speech-FT, a novel two-stage fine-tuning framework designed to maintain cross-task generalization while benefiting from fine-tuning. Speech-FT first applies fine-tuning specifically designed to reduce representational drift, followed by weight-space interpolation with the pre-trained model to restore cross-task generalization. Extensive experiments on HuBERT, wav2vec 2.0, DeCoAR 2.0, and WavLM Base+ demonstrate that Speech-FT consistently improves performance across a wide range of supervised, unsupervised, and multitask fine-tuning scenarios. Moreover, Speech-FT achieves superior cross-task generalization compared to fine-tuning baselines that explicitly constrain weight changes, such as weight-space regularization and LoRA fine-tuning. Our analysis reveals that Speech-FT maintains higher feature similarity to the pre-trained model compared to alternative strategies, despite allowing larger weight-space updates. Notably, Speech-FT achieves significant improvements on the SUPERB benchmark. For example, when fine-tuning HuBERT on automatic speech recognition, Speech-FT is able to reduce phone error rate from 5.17% to 3.94%, lower word error rate from 6.38% to 5.75%, and increase speaker identification accuracy from 81.86% to 84.11%. Speech-FT provides a simple yet powerful solution for further refining speech representation models after pre-training. </p>
<blockquote>
<p>å¯¹è¯­éŸ³è¡¨å¾æ¨¡å‹è¿›è¡Œå¾®è°ƒå¯ä»¥æé«˜ç‰¹å®šä»»åŠ¡çš„æ€§èƒ½ï¼Œä½†é€šå¸¸ä¼šæŸå®³å…¶è·¨ä»»åŠ¡æ³›åŒ–èƒ½åŠ›ã€‚è¿™ç§é€€åŒ–é€šå¸¸æ˜¯ç”±äºè¡¨å¾çš„è¿‡åº¦å˜åŒ–ï¼Œä½¿å¾—éš¾ä»¥ä¿ç•™é¢„è®­ç»ƒæœŸé—´å­¦åˆ°çš„ä¿¡æ¯ã€‚ç°æœ‰æ–¹æ³•ï¼Œå¦‚æ­£åˆ™åŒ–å¾®è°ƒè¿‡ç¨‹ä¸­çš„æƒé‡å˜åŒ–ï¼Œå¯èƒ½æ— æ³•ä¿æŒä¸é¢„è®­ç»ƒæ¨¡å‹è¶³å¤Ÿé«˜çš„ç‰¹å¾ç›¸ä¼¼æ€§ï¼Œå› æ­¤å¯èƒ½ä¸§å¤±è·¨ä»»åŠ¡æ³›åŒ–èƒ½åŠ›ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Speech-FTï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„ä¸¤é˜¶æ®µå¾®è°ƒæ¡†æ¶ï¼Œæ—¨åœ¨ä¿æŒè·¨ä»»åŠ¡æ³›åŒ–èƒ½åŠ›çš„åŒæ—¶å—ç›Šäºå¾®è°ƒã€‚Speech-FTé¦–å…ˆåº”ç”¨ä¸“é—¨è®¾è®¡çš„å‡å°‘è¡¨å¾æ¼‚ç§»çš„å¾®è°ƒï¼Œç„¶åé€šè¿‡æƒé‡ç©ºé—´æ’å€¼ä¸é¢„è®­ç»ƒæ¨¡å‹æ¢å¤è·¨ä»»åŠ¡æ³›åŒ–ã€‚åœ¨HuBERTã€wav2vec 2.0ã€DeCoAR 2. 0å’ŒWavLM Base+ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSpeech-FTåœ¨å¹¿æ³›çš„ç›‘ç£ã€æ— ç›‘ç£å’Œå¤šä»»åŠ¡å¾®è°ƒåœºæ™¯ä¸­å§‹ç»ˆæé«˜äº†æ€§èƒ½ã€‚è€Œä¸”ï¼ŒSpeech-FTåœ¨è·¨ä»»åŠ¡æ³›åŒ–æ–¹é¢ä¼˜äºæ˜¾å¼çº¦æŸæƒé‡å˜åŒ–çš„å¾®è°ƒåŸºå‡†ï¼Œå¦‚æƒé‡ç©ºé—´æ­£åˆ™åŒ–å’ŒLoRAå¾®è°ƒã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œå°½ç®¡å…è®¸æ›´å¤§çš„æƒé‡ç©ºé—´æ›´æ–°ï¼ŒSpeech-FTä¸é¢„è®­ç»ƒæ¨¡å‹çš„ç‰¹å¾ç›¸ä¼¼æ€§ä»ç„¶é«˜äºå…¶ä»–ç­–ç•¥ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒSpeech-FTåœ¨SUPERBåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚ä¾‹å¦‚ï¼Œåœ¨å¯¹HuBERTè¿›è¡Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«å¾®è°ƒæ—¶ï¼ŒSpeech-FTèƒ½å¤Ÿå°†ç”µè¯é”™è¯¯ç‡ä»5.17%é™ä½åˆ°3.94%ï¼Œå•è¯é”™è¯¯ç‡ä»6.38%é™ä½åˆ°5.75%ï¼Œå¹¶æé«˜è¯´è¯äººè¯†åˆ«å‡†ç¡®ç‡è‡³84.11%ã€‚Speech-FTä¸ºé¢„è®­ç»ƒåè¿›ä¸€æ­¥ç²¾ç‚¼è¯­éŸ³è¡¨å¾æ¨¡å‹æä¾›äº†ä¸€ç§ç®€å•è€Œå¼ºå¤§çš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12672v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºSpeech-FTçš„æ–°å‹ä¸¤é˜¶æ®µå¾®è°ƒæ¡†æ¶ï¼Œæ—¨åœ¨ä¿æŒè·¨ä»»åŠ¡æ³›åŒ–èƒ½åŠ›çš„åŒæ—¶ï¼Œå—ç›Šäºå¾®è°ƒã€‚Speech-FTé¦–å…ˆåº”ç”¨ä¸“é—¨è®¾è®¡çš„å‡å°‘è¡¨ç¤ºæ¼‚ç§»çš„å¾®è°ƒï¼Œç„¶åé€šè¿‡é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œæƒé‡ç©ºé—´æ’å€¼ä»¥æ¢å¤è·¨ä»»åŠ¡æ³›åŒ–ã€‚å®éªŒè¡¨æ˜ï¼ŒSpeech-FTåœ¨å¤šç§ç›‘ç£ã€æ— ç›‘ç£å’Œå¤šä»»åŠ¡å¾®è°ƒåœºæ™¯ä¸‹å‡èƒ½æé«˜æ€§èƒ½ï¼Œå¹¶åœ¨è·¨ä»»åŠ¡æ³›åŒ–æ–¹é¢ä¼˜äºå…¶ä»–å¾®è°ƒæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰å¾®è°ƒæ–¹æ³•åœ¨æé«˜ç‰¹å®šä»»åŠ¡æ€§èƒ½æ—¶ï¼Œå¯èƒ½ä¼šæŸå®³æ¨¡å‹çš„è·¨ä»»åŠ¡æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>Speech-FTé‡‡ç”¨ä¸¤é˜¶æ®µå¾®è°ƒç­–ç•¥ï¼Œæ—¨åœ¨å‡å°‘è¡¨ç¤ºæ¼‚ç§»å¹¶æ¢å¤è·¨ä»»åŠ¡æ³›åŒ–ã€‚</li>
<li>Speech-FTé€šè¿‡æƒé‡ç©ºé—´æ’å€¼ä¸é¢„è®­ç»ƒæ¨¡å‹ç›¸ç»“åˆï¼Œä¿æŒä¸é¢„è®­ç»ƒæ¨¡å‹çš„é«˜ç‰¹å¾ç›¸ä¼¼æ€§ã€‚</li>
<li>Speech-FTåœ¨å¤šç§è¯­éŸ³æ¨¡å‹åŠä¸åŒä»»åŠ¡åœºæ™¯ä¸‹å‡è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½æå‡ã€‚</li>
<li>Speech-FTåœ¨è·¨ä»»åŠ¡æ³›åŒ–æ–¹é¢ä¼˜äºå…¶ä»–æ˜¾å¼çº¦æŸæƒé‡å˜åŒ–çš„å¾®è°ƒæ–¹æ³•ã€‚</li>
<li>Speech-FTé™ä½äº†è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ä¸­çš„ç”µè¯é”™è¯¯ç‡ã€å•è¯é”™è¯¯ç‡ï¼Œå¹¶æé«˜äº†è¯´è¯äººè¯†åˆ«å‡†ç¡®ç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12672">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e90374152195b315d4b35c4031f02c75.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-73c8aebf1b5dbcd78d3eacf49aed080a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3954990023620f8dac02eacd533db9c4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-68ce3edffcd701675c0754affd37b9c9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d8e56a6218a636af031ea5d0d6653e60.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-30f59dc36ac336c08ee0cb3191ac2538.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-28/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-28/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-28/Face%20Swapping/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-30f9e0fed936b7a031404e34c5ce3556.jpg" class="responsive-img" alt="Face Swapping">
                        
                        <span class="card-title">Face Swapping</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Face Swapping æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-28  Towards Generalized Proactive Defense against Face Swappingwith   Contour-Hybrid Watermark
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-28
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Face-Swapping/" class="post-category">
                                    Face Swapping
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Face-Swapping/">
                        <span class="chip bg-color">Face Swapping</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-28/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-54042b707075d18d4db98ee14c047c9b.jpg" class="responsive-img" alt="æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ ">
                        
                        <span class="card-title">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹  æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-28  Hard Negative Contrastive Learning for Fine-Grained Geometric   Understanding in Large Multimodal Models
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-28
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ 
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">25691.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
