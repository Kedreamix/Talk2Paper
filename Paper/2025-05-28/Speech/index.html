<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech 方向最新论文已更新，请持续关注 Update in 2025-05-28  Mixture of LoRA Experts for Low-Resourced Multi-Accent Automatic Speech   Recognition">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-66ee140547c030fe34bf3065db693d9e.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-05-28
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-06-24
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    17.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    71 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-05-28-更新"><a href="#2025-05-28-更新" class="headerlink" title="2025-05-28 更新"></a>2025-05-28 更新</h1><h2 id="Mixture-of-LoRA-Experts-for-Low-Resourced-Multi-Accent-Automatic-Speech-Recognition"><a href="#Mixture-of-LoRA-Experts-for-Low-Resourced-Multi-Accent-Automatic-Speech-Recognition" class="headerlink" title="Mixture of LoRA Experts for Low-Resourced Multi-Accent Automatic Speech   Recognition"></a>Mixture of LoRA Experts for Low-Resourced Multi-Accent Automatic Speech   Recognition</h2><p><strong>Authors:Raphaël Bagat, Irina Illina, Emmanuel Vincent</strong></p>
<p>We aim to improve the robustness of Automatic Speech Recognition (ASR) systems against non-native speech, particularly in low-resourced multi-accent settings. We introduce Mixture of Accent-Specific LoRAs (MAS-LoRA), a fine-tuning method that leverages a mixture of Low-Rank Adaptation (LoRA) experts, each specialized in a specific accent. This method can be used when the accent is known or unknown at inference time, without the need to fine-tune the model again. Our experiments, conducted using Whisper on the L2-ARCTIC corpus, demonstrate significant improvements in Word Error Rate compared to regular LoRA and full fine-tuning when the accent is unknown. When the accent is known, the results further improve. Furthermore, MAS-LoRA shows less catastrophic forgetting than the other fine-tuning methods. To the best of our knowledge, this is the first use of a mixture of LoRA experts for non-native multi-accent ASR. </p>
<blockquote>
<p>我们的目标是提高自动语音识别（ASR）系统对非母语语音的稳健性，特别是在资源稀缺的多口音环境中。我们引入了口音特定LoRA的混合方法（MAS-LoRA），这是一种微调方法，利用一系列低秩适应（LoRA）专家，每个专家都专门针对一种口音。这种方法可以在推理阶段使用，无需知道口音信息即可重新微调模型。我们在L2-ARCTIC语料库上使用whisper进行了实验，证明当口音未知时，与传统的LoRA和全量微调相比，MAS-LoRA在词错误率方面取得了显著的改进。当知道口音时，结果会进一步改善。此外，MAS-LoRA与其他微调方法相比，显示出较少的灾难性遗忘。据我们所知，这是首次使用LoRA专家的混合方法来解决非母语的多口音ASR问题。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20006v1">PDF</a> Submitted to Interspeech 2025</p>
<p><strong>Summary</strong>：<br>针对非母语语音的自动语音识别（ASR）系统，在资源稀缺的多口音环境下，我们引入了口音特定LoRA混合（MAS-LoRA）的微调方法。该方法利用一系列专门针对特定口音的Low-Rank Adaptation（LoRA）专家混合而成，可在推理阶段应用于已知或未知口音的情况，无需再次对模型进行微调。实验表明，与常规LoRA和全量微调相比，MAS-LoRA在未知口音情况下显著提高了单词错误率。当口音已知时，结果进一步改善。此外，MAS-LoRA还显示出比其他微调方法更少的灾难性遗忘。这是首次将LoRA专家混合用于非母语多口音ASR。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>研究目标是提高自动语音识别（ASR）系统对非母语语音的稳健性，特别是在资源有限的多口音环境中。</li>
<li>引入了MAS-LoRA方法，它结合了多个针对特定口音的LoRA专家，以提高ASR系统的性能。</li>
<li>MAS-LoRA可在推理阶段应用于已知或未知口音的情况，无需对模型进行二次微调。</li>
<li>实验表明，MAS-LoRA在未知口音情况下显著提高了单词错误率，并且当口音已知时，效果更佳。</li>
<li>与其他微调方法相比，MAS-LoRA显示出较少的灾难性遗忘。</li>
<li>MAS-LoRA是首次将LoRA专家混合应用于非母语多口音ASR的研究。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20006">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-4a17620977606ed2a665fcee7e3d2648.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4939afa92874a09b83773f6fa9c541a3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-76d279dc2d40d806f9f1733c900b0b90.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-867acaf5ed51baf5654e8828f8b500a2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9833f4680951bc4b15efd243bcff5740.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-240ee36427784d181308e9b073815cff.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Mel-McNet-A-Mel-Scale-Framework-for-Online-Multichannel-Speech-Enhancement"><a href="#Mel-McNet-A-Mel-Scale-Framework-for-Online-Multichannel-Speech-Enhancement" class="headerlink" title="Mel-McNet: A Mel-Scale Framework for Online Multichannel Speech   Enhancement"></a>Mel-McNet: A Mel-Scale Framework for Online Multichannel Speech   Enhancement</h2><p><strong>Authors:Yujie Yang, Bing Yang, Xiaofei Li</strong></p>
<p>Online multichannel speech enhancement has been intensively studied recently. Though Mel-scale frequency is more matched with human auditory perception and computationally efficient than linear frequency, few works are implemented in a Mel-frequency domain. To this end, this work proposes a Mel-scale framework (namely Mel-McNet). It processes spectral and spatial information with two key components: an effective STFT-to-Mel module compressing multi-channel STFT features into Mel-frequency representations, and a modified McNet backbone directly operating in the Mel domain to generate enhanced LogMel spectra. The spectra can be directly fed to vocoders for waveform reconstruction or ASR systems for transcription. Experiments on CHiME-3 show that Mel-McNet can reduce computational complexity by 60% while maintaining comparable enhancement and ASR performance to the original McNet. Mel-McNet also outperforms other SOTA methods, verifying the potential of Mel-scale speech enhancement. </p>
<blockquote>
<p>近期，在线多通道语音增强得到了广泛的研究。虽然梅尔尺度频率与人类听觉感知更加匹配且计算效率更高，但很少有人在梅尔频率域进行实现。为此，这项工作提出了一个梅尔尺度框架（即Mel-McNet）。它通过两个关键组件处理谱和空间信息：一个有效的STFT-to-Mel模块，将多通道STFT特征压缩为梅尔频率表示，和一个修改的McNet骨干网直接在梅尔域操作以生成增强的LogMel谱。这些谱可以直接馈送给vocoders进行波形重建或ASR系统进行转录。在CHiME-3上的实验表明，Mel-McNet可以将计算复杂度降低60%，同时保持与原始McNet相当的增强和ASR性能。Mel-McNet还优于其他SOTA方法，验证了梅尔尺度语音增强的潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19576v1">PDF</a> Accepted by Interspeech 2025</p>
<p><strong>总结</strong></p>
<p>本研究提出了一种基于Mel频域的多通道语音增强框架（Mel-McNet）。它通过两个关键组件处理频谱和空间信息：有效的STFT-to-Mel模块将多通道STFT特征压缩成Mel频域表示，以及修改的McNet骨干网直接在Mel域生成增强的LogMel频谱。该框架可以应用于语音合成或语音识别系统，实验表明，Mel-McNet在降低计算复杂度60%的同时，保持了与原始McNet相当的增强和语音识别性能。与其他先进方法相比，Mel-McNet的表现更加出色，验证了Mel频域语音增强的潜力。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>Mel频域在多通道语音增强中具有重要的应用价值。</li>
<li>Mel-McNet框架利用STFT-to-Mel模块将多通道STFT特征转换为Mel频域表示。</li>
<li>Mel-McNet通过直接生成增强的LogMel频谱，提高了语音质量和语音识别性能。</li>
<li>Mel-McNet在计算复杂度方面相比原始McNet降低了60%。</li>
<li>Mel-McNet在CHiME-3数据集上的表现优于其他先进方法。</li>
<li>实验结果证明了Mel频域语音增强的潜力和优势。</li>
<li>Mel-McNet框架可为语音合成和语音识别系统提供有效的语音增强解决方案。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19576">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-d643398e1b3e66d1d47bb17b4bef4076.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-81217b798e8587d12f0f7225b85292a9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6ca2b76c186ba843f7c6c6c3581d2371.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Training-Free-Multi-Step-Audio-Source-Separation"><a href="#Training-Free-Multi-Step-Audio-Source-Separation" class="headerlink" title="Training-Free Multi-Step Audio Source Separation"></a>Training-Free Multi-Step Audio Source Separation</h2><p><strong>Authors:Yongyi Zang, Jingyi Li, Qiuqiang Kong</strong></p>
<p>Audio source separation aims to separate a mixture into target sources. Previous audio source separation systems usually conduct one-step inference, which does not fully explore the separation ability of models. In this work, we reveal that pretrained one-step audio source separation models can be leveraged for multi-step separation without additional training. We propose a simple yet effective inference method that iteratively applies separation by optimally blending the input mixture with the previous step’s separation result. At each step, we determine the optimal blending ratio by maximizing a metric. We prove that our method always yield improvement over one-step inference, provide error bounds based on model smoothness and metric robustness, and provide theoretical analysis connecting our method to denoising along linear interpolation paths between noise and clean distributions, a property we link to denoising diffusion bridge models. Our approach effectively delivers improved separation performance as a “free lunch” from existing models. Our empirical results demonstrate that our multi-step separation approach consistently outperforms one-step inference across both speech enhancement and music source separation tasks, and can achieve scaling performance similar to training a larger model, using more data, or in some cases employing a multi-step training objective. These improvements appear not only on the optimization metric during multi-step inference, but also extend to nearly all non-optimized metrics (with one exception). We also discuss limitations of our approach and directions for future research. </p>
<blockquote>
<p>音频源分离旨在将混合音频分离为目标源。之前的音频源分离系统通常进行一次推理，没有充分探索模型的分离能力。在这项工作中，我们发现预训练的一步音频源分离模型可用于多步分离而无需额外训练。我们提出了一种简单有效的推理方法，通过迭代应用分离，将输入混合物与上一步的分离结果最优地混合。每一步，我们通过最大化一个指标来确定最佳混合比例。我们证明我们的方法总是优于一步推理，提供基于模型平滑和指标稳健性的误差界限，并提供理论分析，将我们的方法与去噪联系到噪声和清洁分布之间的线性插值路径上的属性，这与去噪扩散桥模型相关。我们的方法有效地提高了现有模型的分离性能，就像“免费午餐”一样。我们的实验结果表明，我们的多步分离方法在一系列语音增强和音乐源分离任务上始终优于一步推理，并且可以实现与训练更大的模型、使用更多的数据或在某些情况下采用多步训练目标相似的可扩展性能。这些改进不仅出现在多步推理过程中的优化指标上，而且几乎扩展到所有未优化的指标（有一个例外）。我们还讨论了我们的方法的局限性以及未来研究的方向。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19534v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文揭示了一站式音频源分离模型可以应用于多步分离，而无需额外训练。通过迭代应用分离方法并优化混合输入与上一步分离结果的混合比例，提出了简单有效的推理方法。该方法在理论上与去噪扩散桥模型相联系，可改善现有模型的分离性能。经验结果表明，多步分离方法在一站式推理中表现更优秀，并能在语音增强和音乐源分离任务上实现类似更大模型的性能提升。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>音频源分离旨在将混合音频分离为目标源。</li>
<li>一站式音频源分离模型可以应用于多步分离。</li>
<li>通过迭代应用分离方法并优化混合比例，提出简单有效的推理方法。</li>
<li>方法在理论上与去噪扩散桥模型相联系。</li>
<li>多步分离方法在一站式推理中表现更优秀。</li>
<li>该方法在语音增强和音乐源分离任务上实现性能提升。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19534">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-1203e2ea45c27b04270d21d259af49df.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="AmpleHate-Amplifying-the-Attention-for-Versatile-Implicit-Hate-Detection"><a href="#AmpleHate-Amplifying-the-Attention-for-Versatile-Implicit-Hate-Detection" class="headerlink" title="AmpleHate: Amplifying the Attention for Versatile Implicit Hate   Detection"></a>AmpleHate: Amplifying the Attention for Versatile Implicit Hate   Detection</h2><p><strong>Authors:Yejin Lee, Joonghyuk Hahn, Hyeseon Ahn, Yo-Sub Han</strong></p>
<p>Implicit hate speech detection is challenging due to its subtlety and reliance on contextual interpretation rather than explicit offensive words. Current approaches rely on contrastive learning, which are shown to be effective on distinguishing hate and non-hate sentences. Humans, however, detect implicit hate speech by first identifying specific targets within the text and subsequently interpreting how these target relate to their surrounding context. Motivated by this reasoning process, we propose AmpleHate, a novel approach designed to mirror human inference for implicit hate detection. AmpleHate identifies explicit target using a pretrained Named Entity Recognition model and capture implicit target information via [CLS] tokens. It computes attention-based relationships between explicit, implicit targets and sentence context and then, directly injects these relational vectors into the final sentence representation. This amplifies the critical signals of target-context relations for determining implicit hate. Experiments demonstrate that AmpleHate achieves state-of-the-art performance, outperforming contrastive learning baselines by an average of 82.14% and achieve faster convergence. Qualitative analyses further reveal that attention patterns produced by AmpleHate closely align with human judgement, underscoring its interpretability and robustness. </p>
<blockquote>
<p>隐式仇恨言论检测因其细微性和依赖于上下文解读而非明显的冒犯性词汇而具有挑战性。当前的方法依赖于对比学习，在区分仇恨和非仇恨句子方面显示出有效性。然而，人类检测隐式仇恨言论是先识别文本中的特定目标，然后解释这些目标如何与他们的上下文相关联。受这种推理过程的启发，我们提出了AmpleHate，这是一种旨在模拟人类推断隐式仇恨检测的新方法。AmpleHate使用预训练的命名实体识别模型识别显式目标，并通过[CLS]令牌捕获隐式目标信息。它计算显式、隐式目标与句子上下文之间的基于注意力的关系，然后将这些关系向量直接注入最终的句子表示中。这放大了确定隐式仇恨的目标-上下文关系的关键信号。实验表明，AmpleHate达到了最先进的性能，平均比对比学习基线高出82.14%，并且实现了更快的收敛速度。定性分析进一步表明，AmpleHate产生的注意力模式与人类判断紧密吻合，强调了其可解释性和稳健性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19528v1">PDF</a> 13 pages, 4 figures, Under Review</p>
<p><strong>摘要</strong></p>
<p>隐恶言检测面临诸多挑战，因其表达微妙且依赖于语境解读而非直接冒犯性词汇。当前方法主要依赖对比学习，在区分仇恨与非仇恨语句方面效果显著。然而，人类检测隐恶言时首先识别文本中的特定目标，随后解读这些目标如何与周遭语境关联。受这一过程启发，我们提出AmpleHate这一新方法，旨在模拟人类推理过程进行隐恶言检测。AmpleHate运用预训练命名实体识别模型识别显性目标，并通过[CLS]令牌捕捉隐性目标信息。它计算显性、隐性目标与句子语境间的关注关系，随后将这些关系向量直接注入最终句子表述中。这放大了目标语境关系的关键信号，以确定隐恶言的存在。实验证明，AmpleHate实现了最先进的性能表现，较对比学习基线平均高出82.14%，且收敛速度更快。定性分析进一步表明，AmpleHate产生的关注模式与人类判断高度一致，凸显其可解释性和稳健性。</p>
<p><strong>要点概括</strong></p>
<ol>
<li>隐恶言检测的挑战性在于其表达方式的微妙和依赖语境解读的特点。</li>
<li>当前的主要方法是通过对比学习来区分仇恨和非仇恨语句。</li>
<li>人类检测隐恶言时，首先识别文本中的特定目标，并解读这些目标与语境的关系。</li>
<li>AmpleHate方法模拟人类推理过程进行隐恶言检测，通过识别显性目标和隐性目标信息，计算它们与句子语境的关注关系。</li>
<li>AmpleHate将关系向量直接注入句子最终表述中，放大目标语境关系的关键信号。</li>
<li>实验证明AmpleHate性能卓越，较对比学习基线有显著提高，且收敛速度更快。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19528">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-1a09bb6f8dc3ba7b853278a97f68e4b4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-49e983d6bc3ca2daa2d582f60538f1d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ea743330f209fb7503c044ab5cb61344.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d86e5eb9e0cd9c490823477cb0659ab0.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Multi-Channel-Acoustic-Echo-Cancellation-Based-on-Direction-of-Arrival-Estimation"><a href="#Multi-Channel-Acoustic-Echo-Cancellation-Based-on-Direction-of-Arrival-Estimation" class="headerlink" title="Multi-Channel Acoustic Echo Cancellation Based on Direction-of-Arrival   Estimation"></a>Multi-Channel Acoustic Echo Cancellation Based on Direction-of-Arrival   Estimation</h2><p><strong>Authors:Fei Zhao, Xueliang Zhang, Zhong-Qiu Wang</strong></p>
<p>Acoustic echo cancellation (AEC) is an important speech signal processing technology that can remove echoes from microphone signals to enable natural-sounding full-duplex speech communication. While single-channel AEC is widely adopted, multi-channel AEC can leverage spatial cues afforded by multiple microphones to achieve better performance. Existing multi-channel AEC approaches typically combine beamforming with deep neural networks (DNN). This work proposes a two-stage algorithm that enhances multi-channel AEC by incorporating sound source directional cues. Specifically, a lightweight DNN is first trained to predict the sound source directions, and then the predicted directional information, multi-channel microphone signals, and single-channel far-end signal are jointly fed into an AEC network to estimate the near-end signal. Evaluation results show that the proposed algorithm outperforms baseline approaches and exhibits robust generalization across diverse acoustic environments. </p>
<blockquote>
<p>声学回声消除（AEC）是一项重要的语音信号处理 technologies，能够消除麦克风信号中的回声，从而实现自然的全双工语音通信。虽然单通道AEC已得到广泛应用，但多通道AEC可以利用多个麦克风提供的空间线索来实现更好的性能。现有的多通道AEC方法通常将波束形成与深度神经网络（DNN）相结合。这项工作提出了一个两阶段的算法，通过融入声源方向线索来增强多通道AEC。具体来说，首先训练一个轻量级的DNN来预测声源方向，然后将预测的方位信息、多通道麦克风信号和单通道远端信号一起输入到AEC网络中来估计近端信号。评估结果表明，该算法优于基线方法，并在各种声学环境中表现出稳健的泛化能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19493v1">PDF</a> Accepted by Interspeech 2025</p>
<p><strong>Summary</strong>：<br>多通道声学回声消除（AEC）技术利用多个麦克风的空域线索，通过结合波束成形和深度神经网络（DNN）实现更好的性能。本研究提出了一种两阶段算法，通过引入声源方向线索来增强多通道AEC。首先训练一个轻量级的DNN来预测声源方向，然后将预测的方位信息、多通道麦克风信号和单通道远端信号一起输入到AEC网络中以估计近端信号。评估结果表明，该算法优于基准方法，并在各种声学环境中表现出稳健的泛化能力。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>多通道AEC技术利用多个麦克风的空域线索，能够实现更好的回声消除性能。</li>
<li>现有多通道AEC方法通常结合波束成形和深度神经网络（DNN）。</li>
<li>本研究提出了一种两阶段算法，通过引入声源方向线索来增强多通道AEC。</li>
<li>该算法首先使用轻量级DNN预测声源方向。</li>
<li>该算法将预测的声源方向、多通道麦克风信号和单通道远端信号一起输入到AEC网络。</li>
<li>评估结果表明，该算法性能优于基准方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19493">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-602bdefab088e7c53769a62576bf7cea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-531a8c9a7d55e706a095f5f303739a98.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e1bfe980e4a32e910e8896c6a613da70.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8ad2c346fb681c6c583d90a572362090.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4a71773f81adfc0ec97d57389c44fcdb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-23ee6205bde9cf43b0d1e304d93fc308.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="FlowSE-Efficient-and-High-Quality-Speech-Enhancement-via-Flow-Matching"><a href="#FlowSE-Efficient-and-High-Quality-Speech-Enhancement-via-Flow-Matching" class="headerlink" title="FlowSE: Efficient and High-Quality Speech Enhancement via Flow Matching"></a>FlowSE: Efficient and High-Quality Speech Enhancement via Flow Matching</h2><p><strong>Authors:Ziqian Wang, Zikai Liu, Xinfa Zhu, Yike Zhu, Mingshuai Liu, Jun Chen, Longshuai Xiao, Chao Weng, Lei Xie</strong></p>
<p>Generative models have excelled in audio tasks using approaches such as language models, diffusion, and flow matching. However, existing generative approaches for speech enhancement (SE) face notable challenges: language model-based methods suffer from quantization loss, leading to compromised speaker similarity and intelligibility, while diffusion models require complex training and high inference latency. To address these challenges, we propose FlowSE, a flow-matching-based model for SE. Flow matching learns a continuous transformation between noisy and clean speech distributions in a single pass, significantly reducing inference latency while maintaining high-quality reconstruction. Specifically, FlowSE trains on noisy mel spectrograms and optional character sequences, optimizing a conditional flow matching loss with ground-truth mel spectrograms as supervision. It implicitly learns speech’s temporal-spectral structure and text-speech alignment. During inference, FlowSE can operate with or without textual information, achieving impressive results in both scenarios, with further improvements when transcripts are available. Extensive experiments demonstrate that FlowSE significantly outperforms state-of-the-art generative methods, establishing a new paradigm for generative-based SE and demonstrating the potential of flow matching to advance the field. Our code, pre-trained checkpoints, and audio samples are available. </p>
<blockquote>
<p>生成模型在音频任务方面表现出色，采用了语言模型、扩散和流匹配等方法。然而，现有的语音增强（SE）生成方法面临显著挑战：基于语言模型的方法遭受量化损失，导致说话人相似性和可懂度受损，而扩散模型则需要复杂的训练和高的推理延迟。为了应对这些挑战，我们提出了基于流匹配的SE模型FlowSE。流匹配一次性学习带噪和干净语音分布之间的连续转换，在保持高质量重建的同时，显著减少了推理延迟。具体来说，FlowSE在带噪梅尔频谱和可选字符序列上进行训练，以地面真实梅尔频谱图为监督，优化条件流匹配损失。它隐式地学习语音的时空频谱结构和文本语音对齐。在推理过程中，FlowSE可以带有或不带文本信息进行操作，在这两种情况下都取得了令人印象深刻的结果，在有可用转录时进一步改进。大量实验表明，FlowSE显著优于最先进的生成方法，为基于生成的SE建立了新范式，并展示了流匹配推动该领域发展的潜力。我们的代码、预训练检查点和音频样本可供使用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19476v1">PDF</a> Accepted to InterSpeech 2025</p>
<p><strong>Summary</strong></p>
<p>基于流匹配技术的新型语音增强模型FlowSE表现出优异性能。它通过一次过程学习噪声语音和纯净语音的分布转换，具备高效推理时间和高重建质量。FlowSE可通过优化有条件流匹配损失进行训练，在有声谱图监督下，可隐式学习语音的时频结构和文本语音对齐。在推理过程中，FlowSE可在有或无文本信息的情况下运行，并在两种情况下均取得了令人印象深刻的结果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>生成模型在音频任务中表现出卓越性能，采用语言模型、扩散和流匹配等方法。</li>
<li>现有语音增强生成方法面临挑战，如语言模型的量化损失和扩散模型的高复杂度和推理延迟。</li>
<li>FlowSE是一个基于流匹配的模型，旨在解决这些挑战。</li>
<li>FlowSE通过优化有条件流匹配损失进行训练，可在声谱图监督下隐式学习语音结构。</li>
<li>FlowSE在推理过程中，可灵活处理有或无文本信息的情况，并均取得良好效果。</li>
<li>FlowSE显著优于现有生成方法，为生成式语音增强建立了新范例。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19476">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-7b2d8035e64daf9aa6646ad7ca9eb554.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5fea74b2eb2ceff0e0a40bee92bd325b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-62fcfc69000fcb5926ea36cb567ea599.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="SoloSpeech-Enhancing-Intelligibility-and-Quality-in-Target-Speech-Extraction-through-a-Cascaded-Generative-Pipeline"><a href="#SoloSpeech-Enhancing-Intelligibility-and-Quality-in-Target-Speech-Extraction-through-a-Cascaded-Generative-Pipeline" class="headerlink" title="SoloSpeech: Enhancing Intelligibility and Quality in Target Speech   Extraction through a Cascaded Generative Pipeline"></a>SoloSpeech: Enhancing Intelligibility and Quality in Target Speech   Extraction through a Cascaded Generative Pipeline</h2><p><strong>Authors:Helin Wang, Jiarui Hai, Dongchao Yang, Chen Chen, Kai Li, Junyi Peng, Thomas Thebaud, Laureano Moro Velazquez, Jesus Villalba, Najim Dehak</strong></p>
<p>Target Speech Extraction (TSE) aims to isolate a target speaker’s voice from a mixture of multiple speakers by leveraging speaker-specific cues, typically provided as auxiliary audio (a.k.a. cue audio). Although recent advancements in TSE have primarily employed discriminative models that offer high perceptual quality, these models often introduce unwanted artifacts, reduce naturalness, and are sensitive to discrepancies between training and testing environments. On the other hand, generative models for TSE lag in perceptual quality and intelligibility. To address these challenges, we present SoloSpeech, a novel cascaded generative pipeline that integrates compression, extraction, reconstruction, and correction processes. SoloSpeech features a speaker-embedding-free target extractor that utilizes conditional information from the cue audio’s latent space, aligning it with the mixture audio’s latent space to prevent mismatches. Evaluated on the widely-used Libri2Mix dataset, SoloSpeech achieves the new state-of-the-art intelligibility and quality in target speech extraction and speech separation tasks while demonstrating exceptional generalization on out-of-domain data and real-world scenarios. </p>
<blockquote>
<p>目标语音提取（TSE）旨在利用特定于说话人的线索（通常作为辅助音频（又称线索音频）提供）从多个说话人的混合声音中分离出目标说话人的声音。尽管最近的TSE进展主要采用了提供高感知质量的判别模型，但这些模型往往会引入不需要的伪影，降低自然度，并对训练和测试环境之间的差异敏感。另一方面，TSE的生成模型在感知质量和清晰度方面有所不足。为了解决这些挑战，我们提出了SoloSpeech，这是一种新型级联生成管道，它集成了压缩、提取、重建和校正过程。SoloSpeech的特点是无需说话人嵌入的目标提取器，它利用线索音频的潜在空间的条件信息，将其与混合音频的潜在空间对齐，以防止不匹配。在广泛使用的Libri2Mix数据集上，SoloSpeech在目标语音提取和语音分离任务中实现了最新的清晰度和质量水平，同时在跨领域数据和真实世界场景上展示了出色的泛化能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19314v1">PDF</a> </p>
<p><strong>Summary</strong><br>目标语音提取（TSE）旨在利用特定于说话人的线索（通常作为辅助音频提供），从多个说话人的混合声音中分离出目标说话人的声音。尽管最近的研究主要采用了提供高感知质量的判别模型，但这些模型常常引入不必要的伪影，降低了自然度，并对训练和测试环境之间的差异敏感。为解决这些问题，我们提出了名为SoloSpeech的新型级联生成管道，它集成了压缩、提取、重建和校正过程。SoloSpeech的特点是无需使用说话人嵌入的目标提取器，利用来自提示音频的潜在空间的条件信息，与混合音频的潜在空间对齐，以避免不匹配。在广泛使用的Libri2Mix数据集上，SoloSpeech在目标语音提取和语音分离任务中实现了新的最先进的可理解性和质量，并在超出领域的数据和真实世界场景中表现出卓越的泛化能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>目标语音提取（TSE）旨在从混合声音中分离出目标说话人的声音，利用特定于说话人的线索。</li>
<li>现有模型（判别模型）存在引入伪影、降低自然度和对环境差异敏感的问题。</li>
<li>SoloSpeech是一种新型的级联生成管道，集成了压缩、提取、重建和校正过程。</li>
<li>SoloSpeech采用无说话人嵌入的目标提取器设计，利用提示音频和混合音频的潜在空间条件信息。</li>
<li>SoloSpeech在Libri2Mix数据集上实现了新的最先进的可理解性和质量。</li>
<li>SoloSpeech在超出领域的数据和真实世界场景中表现出良好的泛化能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19314">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-639373a27c06fdf66efd5372039c03c8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-427f1d98f44b606a591a7df43fc34727.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4d4b3411393f5cd9d4a494f56120efb4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f7a660eb64f8d76b8fc7c66d5640d567.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6b1a4ab8a735330ac8c22f03b1727d5d.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="EnvSDD-Benchmarking-Environmental-Sound-Deepfake-Detection"><a href="#EnvSDD-Benchmarking-Environmental-Sound-Deepfake-Detection" class="headerlink" title="EnvSDD: Benchmarking Environmental Sound Deepfake Detection"></a>EnvSDD: Benchmarking Environmental Sound Deepfake Detection</h2><p><strong>Authors:Han Yin, Yang Xiao, Rohan Kumar Das, Jisheng Bai, Haohe Liu, Wenwu Wang, Mark D Plumbley</strong></p>
<p>Audio generation systems now create very realistic soundscapes that can enhance media production, but also pose potential risks. Several studies have examined deepfakes in speech or singing voice. However, environmental sounds have different characteristics, which may make methods for detecting speech and singing deepfakes less effective for real-world sounds. In addition, existing datasets for environmental sound deepfake detection are limited in scale and audio types. To address this gap, we introduce EnvSDD, the first large-scale curated dataset designed for this task, consisting of 45.25 hours of real and 316.74 hours of fake audio. The test set includes diverse conditions to evaluate the generalizability, such as unseen generation models and unseen datasets. We also propose an audio deepfake detection system, based on a pre-trained audio foundation model. Results on EnvSDD show that our proposed system outperforms the state-of-the-art systems from speech and singing domains. </p>
<blockquote>
<p>音频生成系统如今能够创建非常逼真的声音场景，这可以增强媒体制作，但也存在潜在风险。一些研究已经研究了语音或歌唱声音的深度伪造。然而，环境声音具有不同的特性，这可能使检测语音和歌唱深度伪造的方法对现实世界的声音效果较差。此外，用于环境声音深度伪造检测的现有数据集在规模和音频类型方面受到限制。为了弥补这一空白，我们引入了EnvSDD，这是为此任务设计的大规模定制数据集，包含45.25小时的真实音频和316.74小时的虚假音频。测试集包括各种条件，以评估其泛化能力，例如未见过的生成模型和未见过的数据集。我们还提出了一个基于预训练音频基础模型的音频深度伪造检测系统。在EnvSDD上的结果表显示，我们提出的系统优于语音和歌唱领域的最先进的系统。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19203v1">PDF</a> Accepted by Interspeech 2025</p>
<p><strong>摘要</strong><br>音频生成系统所创造的声音环境越来越真实，既增强了媒体制作的效果，也带来了潜在风险。尽管已有研究探讨了语音或歌唱声音的深度伪造问题，但环境音具有不同的特性，使得检测语音和歌唱深度伪造的方法对真实世界声音的效果可能不佳。此外，环境音深度伪造检测的现有数据集规模有限、音频类型有限。为解决这一空白，我们推出了EnvSDD，这是专门为这一任务设计的大型精选数据集，包含45.25小时的真实音频和316.74小时的伪造音频。测试集包括各种条件，以评估对未见过的生成模型和未见过的数据集的泛化能力。我们还基于预训练的音频基础模型提出了音频深度伪造检测系统。在EnvSDD上的结果表明，我们提出的系统优于语音和歌唱领域的最新系统。</p>
<p><strong>要点掌握</strong></p>
<ol>
<li>音频生成系统能创造逼真的声音环境，既有积极影响，也存在潜在风险。</li>
<li>环境音具有独特特性，使得现有的语音和歌唱深度伪造检测方法对其可能不够有效。</li>
<li>当前针对环境音深度伪造检测的数据集存在规模及音频类型上的局限。</li>
<li>推出EnvSDD数据集，专门为环境音深度伪造检测任务设计，包含真实和伪造音频。</li>
<li>测试集包含多种条件，以评估系统的泛化能力，如对未见过的生成模型和数据集。</li>
<li>基于预训练的音频基础模型提出音频深度伪造检测系统。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19203">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-8954880742626ed6ba2f8c4c78b11ab7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-175f2ceebf353149462cd6398c7ebad7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-53336b8d31bbb4cbbbd2842b14a696b1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a6cd9d5898ab52261cb44eb6eccf311a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-66ee140547c030fe34bf3065db693d9e.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="BR-ASR-Efficient-and-Scalable-Bias-Retrieval-Framework-for-Contextual-Biasing-ASR-in-Speech-LLM"><a href="#BR-ASR-Efficient-and-Scalable-Bias-Retrieval-Framework-for-Contextual-Biasing-ASR-in-Speech-LLM" class="headerlink" title="BR-ASR: Efficient and Scalable Bias Retrieval Framework for Contextual   Biasing ASR in Speech LLM"></a>BR-ASR: Efficient and Scalable Bias Retrieval Framework for Contextual   Biasing ASR in Speech LLM</h2><p><strong>Authors:Xun Gong, Anqi Lv, Zhiming Wang, Huijia Zhu, Yanmin Qian</strong></p>
<p>While speech large language models (SpeechLLMs) have advanced standard automatic speech recognition (ASR), contextual biasing for named entities and rare words remains challenging, especially at scale. To address this, we propose BR-ASR: a Bias Retrieval framework for large-scale contextual biasing (up to 200k entries) via two innovations: (1) speech-and-bias contrastive learning to retrieve semantically relevant candidates; (2) dynamic curriculum learning that mitigates homophone confusion which negatively impacts the final performance. The is a general framework that allows seamless integration of the retrieved candidates into diverse ASR systems without fine-tuning. Experiments on LibriSpeech test-clean&#x2F;-other achieve state-of-the-art (SOTA) biased word error rates (B-WER) of 2.8%&#x2F;7.1% with 2000 bias words, delivering 45% relative improvement over prior methods. BR-ASR also demonstrates high scalability: when expanding the bias list to 200k where traditional methods generally fail, it induces only 0.3 &#x2F; 2.9% absolute WER &#x2F; B-WER degradation with a 99.99% pruning rate and only 20ms latency per query on test-other. </p>
<blockquote>
<p>虽然语音大型语言模型（SpeechLLMs）已经推动了标准自动语音识别（ASR）的发展，但对命名实体和罕见词汇的上下文偏见仍然是一个挑战，尤其是在大规模情况下。为了解决这一问题，我们提出了BR-ASR：一种用于大规模上下文偏见（高达20万个条目）的偏见检索框架，通过两个创新点来实现：（1）语音和偏见的对比学习，以检索语义相关的候选词；（2）动态课程学习，减轻同音字混淆对最终性能的负面影响。这是一个通用框架，允许无缝地将检索到的候选词集成到各种ASR系统中，而无需微调。在LibriSpeech测试-清洁&#x2F;其他实验上，使用2000个偏见词汇达到了最先进的偏置词错误率（B-WER）2.8%&#x2F;7.1%，相较于先前的方法实现了45%的相对改进。BR-ASR还显示出高度可扩展性：当将偏见列表扩大到20万个条目时，传统方法通常无法处理，而BR-ASR只引入了0.3&#x2F;2.9%的绝对词错误率（WER）&#x2F; B-WER退化，具有99.99%的修剪率和仅20ms的查询延迟（针对测试其他情况）。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19179v1">PDF</a> Accepted by InterSpeech 2025</p>
<p><strong>Summary</strong></p>
<p>本文提出BR-ASR模型，通过两项创新解决了大规模语境偏差问题：一是语音与偏差对比学习，用于检索语义相关候选词；二是动态课程学习，减少同音字混淆对最终性能的影响。该模型可以无缝集成到不同的语音识别系统中，并在LibriSpeech数据集上取得了显著的识别性能提升。特别是对于含有特定语境偏差的大规模数据集，其显著的优势表现为高度可扩展性和出色的效率。此外，实验结果在利用最先进的实验资源评估测试表现时也实现了良好的效果。在进一步扩大偏差列表的情况下，BR-ASR仍然保持了出色的性能表现。总体来说，BR-ASR为大规模语境偏差问题提供了一种有效的解决方案。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>BR-ASR模型采用两项创新解决大规模语境偏差问题：语音与偏差对比学习以及动态课程学习。</li>
<li>对比学习能够检索语义相关候选词，动态课程学习则有助于减少同音字混淆对性能的影响。</li>
<li>模型可以无缝集成到不同的语音识别系统中，无需微调。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19179">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-a362be12675fae2d577b22a4ab59ce7c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f9840aef884420ff8a0646bda90ff488.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4ac1913c4f572e1ec320073d9c17cc49.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="FiLLM-–-A-Filipino-optimized-Large-Language-Model-based-on-Southeast-Asia-Large-Language-Model-SEALLM"><a href="#FiLLM-–-A-Filipino-optimized-Large-Language-Model-based-on-Southeast-Asia-Large-Language-Model-SEALLM" class="headerlink" title="FiLLM – A Filipino-optimized Large Language Model based on Southeast   Asia Large Language Model (SEALLM)"></a>FiLLM – A Filipino-optimized Large Language Model based on Southeast   Asia Large Language Model (SEALLM)</h2><p><strong>Authors:Carlos Jude G. Maminta, Isaiah Job Enriquez, Deandre Nigel Nunez, Michael B. Dela Fuente</strong></p>
<p>This study presents FiLLM, a Filipino-optimized large language model, designed to enhance natural language processing (NLP) capabilities in the Filipino language. Built upon the SeaLLM-7B 2.5 model, FiLLM leverages Low-Rank Adaptation (LoRA) fine-tuning to optimize memory efficiency while maintaining task-specific performance. The model was trained and evaluated on diverse Filipino datasets to address key NLP tasks, including Named Entity Recognition (NER), Part-of-Speech (POS) tagging, Dependency Parsing, and Text Summarization. Performance comparisons with the CalamanCy model were conducted using F1 Score, Precision, Recall, Compression Rate, and Keyword Overlap metrics. Results indicate that Calamancy outperforms FILLM in several aspects, demonstrating its effectiveness in processing Filipino text with improved linguistic comprehension and adaptability. This research contributes to the advancement of Filipino NLP applications by providing an optimized, efficient, and scalable language model tailored for local linguistic needs. </p>
<blockquote>
<p>本研究介绍了FiLLM，这是一个针对菲律宾语进行优化的大型语言模型，旨在提高菲律宾语的自然语言处理（NLP）能力。FiLLM基于SeaLLM-7B 2.5模型构建，利用低秩适配（LoRA）微调技术，在保持特定任务性能的同时优化内存效率。该模型在多种菲律宾语数据集上进行训练和评估，以应对关键的自然语言处理任务，包括命名实体识别（NER）、词性标注（POS）、依存解析和文本摘要。通过与CalamanCy模型的F1分数、精确度、召回率、压缩率和关键词重叠等指标进行性能比较。结果表明，在多个方面，Calamancy的表现优于FILLM，证明其在处理菲律宾文本时的有效性，提高了语言理解和适应性。本研究通过提供一个针对本地语言需求进行优化、高效且可扩展的语言模型，为菲律宾自然语言处理应用的进步做出了贡献。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.18995v1">PDF</a> </p>
<p><strong>Summary</strong><br>     本研究介绍了FiLLM，这是一个针对菲律宾语优化的大型语言模型，旨在提高菲律宾语的自然语言处理（NLP）能力。该模型基于SeaLLM-7B 2.5构建，采用低秩适应（LoRA）微调技术，在优化内存效率的同时保持特定任务性能。模型在多种菲律宾语数据集上进行训练和评估，以应对关键的自然语言处理任务，包括命名实体识别（NER）、词性标注（POS）、依存解析和文本摘要。通过与CalamanCy模型的F1分数、精确度、召回率、压缩率和关键词重叠度等指标的比较，结果显示Calamancy在多个方面优于FILLM，证明其在处理菲律宾语文本时的有效性和适应性。该研究为菲律宾语NLP应用的进步做出了贡献，提供了一个针对本地语言需求的优化、高效和可扩展的语言模型。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FiLLM是一个针对菲律宾语优化的语言模型，基于SeaLLM-7B 2.5构建。</li>
<li>FiLLM利用低秩适应（LoRA）微调技术以提高内存效率和特定任务性能。</li>
<li>模型在多种菲律宾语数据集上进行训练和评估，涵盖NER、POS标注、依存解析和文本摘要等关键NLP任务。</li>
<li>与CalamanCy模型的比较表明，Calamancy在多个方面表现优于FiLLM。</li>
<li>Calamancy在处理菲律宾语文本时表现出更好的语言理解和适应性。</li>
<li>该研究为菲律宾语NLP应用的进步做出了贡献。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.18995">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-d46e40f8c82b1cfc455db69c794736a3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-434a5214b536e09a4d282dcd6050afe0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f73811d96062e1ca21b08b9fa908a5a5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0c604e9a77e3a7c32f7862a907d259fe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9209cc14cc0a278bcd7ee34c402106cf.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="StandUp4AI-A-New-Multilingual-Dataset-for-Humor-Detection-in-Stand-up-Comedy-Videos"><a href="#StandUp4AI-A-New-Multilingual-Dataset-for-Humor-Detection-in-Stand-up-Comedy-Videos" class="headerlink" title="StandUp4AI: A New Multilingual Dataset for Humor Detection in Stand-up   Comedy Videos"></a>StandUp4AI: A New Multilingual Dataset for Humor Detection in Stand-up   Comedy Videos</h2><p><strong>Authors:Valentin Barriere, Nahuel Gomez, Leo Hemamou, Sofia Callejas, Brian Ravenet</strong></p>
<p>Aiming towards improving current computational models of humor detection, we propose a new multimodal dataset of stand-up comedies, in seven languages: English, French, Spanish, Italian, Portuguese, Hungarian and Czech. Our dataset of more than 330 hours, is at the time of writing the biggest available for this type of task, and the most diverse. The whole dataset is automatically annotated in laughter (from the audience), and the subpart left for model validation is manually annotated. Contrary to contemporary approaches, we do not frame the task of humor detection as a binary sequence classification, but as word-level sequence labeling, in order to take into account all the context of the sequence and to capture the continuous joke tagging mechanism typically occurring in natural conversations. As par with unimodal baselines results, we propose a method for e propose a method to enhance the automatic laughter detection based on Audio Speech Recognition errors. Our code and data are available online: <a target="_blank" rel="noopener" href="https://tinyurl.com/EMNLPHumourStandUpPublic">https://tinyurl.com/EMNLPHumourStandUpPublic</a> </p>
<blockquote>
<p>针对改进当前的幽默检测计算模型，我们提出了一个新的多模式喜剧数据集，包含七种语言：英语、法语、西班牙语、意大利语、葡萄牙语、匈牙利语和捷克语。我们的数据集时长超过330小时，是此类任务中目前可用最大且最多元的数据集。整个数据集都自动标注了观众的笑声，用于模型验证的子集则是手动标注的。与当前的方法不同，我们不把幽默检测任务视为一个二元序列分类问题，而是将其视为单词级别的序列标记问题，以考虑整个序列的上下文并捕捉通常在自然对话中发生的连续笑话标记机制。对于单模态基线结果，我们提出了一种基于语音识别错误来提高自动笑声检测的方法。我们的代码和数据可以在线获取：<a target="_blank" rel="noopener" href="https://tinyurl.com/EMNLPHumourStandUpPublic">https://tinyurl.com/EMNLPHumourStandUpPublic</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.18903v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一个新的多模态数据集，包含七种语言的脱口秀喜剧内容，总计超过330小时，是目前此类任务中最大且最多元的数据集。数据集通过观众笑声自动标注，部分数据用于模型验证并进行手动标注。与当代方法不同，本文不将幽默检测视为二元序列分类任务，而是作为词级序列标记，以考虑序列的上下文并捕捉自然对话中连续的笑话标记机制。此外，本文还提出一种基于语音识别错误增强自动笑声检测的方法。数据和代码已在线发布。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>引入一个包含七种语言的脱口秀喜剧多模态数据集，规模超过330小时，为当前最大的幽默检测数据集。</li>
<li>数据集通过观众笑声自动标注，部分数据手动标注，用于模型验证。</li>
<li>不同于现有的幽默检测二元序列分类方法，本文将其视为词级序列标记，以捕捉自然对话中的连续笑话。</li>
<li>考虑到序列的上下文信息，提高幽默检测的准确性。</li>
<li>提出一种基于语音识别错误增强自动笑声检测的方法。</li>
<li>数据集和代码已在线发布，便于其他研究者使用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.18903">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-df9c9b54d9da5d95d0cbcff959bcc395.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1d9e4d3f4be6d8801763ed184da1d63e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e5d3cebec21a601c1dc62de1b31136cf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ab78912122561260d79a74bc5caded7e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c66776262e4eeac662aaed4a6f08bdf4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c4e577a26362e8c02148d1ac6d9b44ef.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Enhancing-Generalization-of-Speech-Large-Language-Models-with-Multi-Task-Behavior-Imitation-and-Speech-Text-Interleaving"><a href="#Enhancing-Generalization-of-Speech-Large-Language-Models-with-Multi-Task-Behavior-Imitation-and-Speech-Text-Interleaving" class="headerlink" title="Enhancing Generalization of Speech Large Language Models with Multi-Task   Behavior Imitation and Speech-Text Interleaving"></a>Enhancing Generalization of Speech Large Language Models with Multi-Task   Behavior Imitation and Speech-Text Interleaving</h2><p><strong>Authors:Jingran Xie, Xiang Li, Hui Wang, Yue Yu, Yang Xiang, Xixin Wu, Zhiyong Wu</strong></p>
<p>Large language models (LLMs) have shown remarkable generalization across tasks, leading to increased interest in integrating speech with LLMs. These speech LLMs (SLLMs) typically use supervised fine-tuning to align speech with text-based LLMs. However, the lack of annotated speech data across a wide range of tasks hinders alignment efficiency, resulting in poor generalization. To address these issues, we propose a novel multi-task ‘behavior imitation’ method with speech-text interleaving, called MTBI, which relies solely on paired speech and transcripts. By ensuring the LLM decoder generates equivalent responses to paired speech and text, we achieve a more generalized SLLM. Interleaving is used to further enhance alignment efficiency. We introduce a simple benchmark to evaluate prompt and task generalization across different models. Experimental results demonstrate that our MTBI outperforms SOTA SLLMs on both prompt and task generalization, while requiring less supervised speech data. </p>
<blockquote>
<p>大型语言模型（LLM）在各种任务中表现出了显著的泛化能力，这引发了将语音与LLM集成的兴趣增长。这些语音LLM（SLLM）通常使用有监督微调来将语音与基于文本的LLM对齐。然而，由于缺乏广泛任务的标注语音数据，阻碍了对齐效率，导致泛化性能较差。为了解决这些问题，我们提出了一种新型多任务“行为模仿”方法，该方法采用语音文本交替方式，称为MTBI，仅依赖于配对语音和文本。通过确保LLM解码器对配对语音和文本产生等效响应，我们实现了更通用的SLLM。交替方式被用来进一步提高对齐效率。我们引入了一个简单的基准测试来评估不同模型在提示和任务方面的泛化能力。实验结果表明，我们的MTBI在提示和任务泛化方面都优于最新的SLLM，同时需要更少的监督语音数据。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.18644v1">PDF</a> Accepted by Interspeech 2025</p>
<p><strong>摘要</strong></p>
<p>语言大模型（LLM）在跨任务中展现出显著的泛化能力，引发了将语音与LLM集成的兴趣增加。然而，由于缺乏跨各种任务的标注语音数据，影响了与文本型LLM的对齐效率，导致泛化性能不佳。为解决这些问题，我们提出了一种新型多任务“行为模仿”方法，采用语音与文本的交错方式，称为MTBI，仅依赖于成对的语音和文本。通过确保LLM解码器对成对的语音和文本产生等效响应，我们实现了更通用的SLLM。交错方式进一步提高了对齐效率。我们还引入了一个简单的基准测试来评估不同模型在提示和任务上的泛化能力。实验结果表明，我们的MTBI在提示和任务泛化方面优于当前最佳水平的SLLM，同时需要更少的监督语音数据。</p>
<p><strong>要点掌握</strong></p>
<ol>
<li>LLM在跨任务中展现出良好的泛化能力，引发了对集成语音和LLM的研究兴趣。</li>
<li>现有的语音LLM（SLLM）通常使用监督微调与文本型LLM对齐，但缺乏标注语音数据影响了对齐效率和泛化性能。</li>
<li>提出了一种新型多任务“行为模仿”方法（MTBI），仅利用成对的语音和文本，确保LLM解码器对语音和文本产生等效响应，提高泛化能力。</li>
<li>MTBI采用语音与文本的交错方式，进一步提高对齐效率。</li>
<li>引入了一个简单的基准测试来评估模型在提示和任务上的泛化能力。</li>
<li>实验结果表明，MTBI在泛化性能和监督语音数据需求方面优于现有SLLM。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.18644">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2fd928cc49a7ef06d21310ee9719f9e5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ca2fe11c1e73d2ad0d2ac7f84ae83dd4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4efd518ca8d86a58ff4c848018351c1c.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="TS-URGENet-A-Three-stage-Universal-Robust-and-Generalizable-Speech-Enhancement-Network"><a href="#TS-URGENet-A-Three-stage-Universal-Robust-and-Generalizable-Speech-Enhancement-Network" class="headerlink" title="TS-URGENet: A Three-stage Universal Robust and Generalizable Speech   Enhancement Network"></a>TS-URGENet: A Three-stage Universal Robust and Generalizable Speech   Enhancement Network</h2><p><strong>Authors:Xiaobin Rong, Dahan Wang, Qinwen Hu, Yushi Wang, Yuxiang Hu, Jing Lu</strong></p>
<p>Universal speech enhancement aims to handle input speech with different distortions and input formats. To tackle this challenge, we present TS-URGENet, a Three-Stage Universal, Robust, and Generalizable speech Enhancement Network. To address various distortions, the proposed system employs a novel three-stage architecture consisting of a filling stage, a separation stage, and a restoration stage. The filling stage mitigates packet loss by preliminarily filling lost regions under noise interference, ensuring signal continuity. The separation stage suppresses noise, reverberation, and clipping distortion to improve speech clarity. Finally, the restoration stage compensates for bandwidth limitation, codec artifacts, and residual packet loss distortion, refining the overall speech quality. Our proposed TS-URGENet achieved outstanding performance in the Interspeech 2025 URGENT Challenge, ranking 2nd in Track 1. </p>
<blockquote>
<p>通用的语音增强旨在处理具有不同失真和输入格式的输入语音。为了应对这一挑战，我们提出了TS-URGENet，这是一个三阶段的通用、稳健和可推广的语音增强网络。为了解决各种失真问题，所提出系统采用了一种新颖的三阶段架构，包括填充阶段、分离阶段和恢复阶段。填充阶段通过初步填充噪声干扰下丢失的区域，减轻数据包丢失的情况，确保信号连续性。分离阶段抑制噪声、回声和削波失真，以提高语音清晰度。最后，恢复阶段对带宽限制、编码解码器产生的伪影和残留的丢包失真进行补偿，提高整体语音质量。我们提出的TS-URGENet在Interspeech 2025紧急挑战中取得了卓越表现，在赛道一排名第二。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.18533v1">PDF</a> Accepted by Interspeech 2025</p>
<p><strong>Summary</strong></p>
<p>TS-URGENet是一个三阶段通用、稳健和通用的语音增强网络，用于处理不同失真和输入格式的语音。它通过填充、分离和恢复三个阶段来解决各种失真问题，提高语音清晰度并优化语音质量。在Interspeech 2025 URGENT Challenge中表现优异，排名第二。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TS-URGENet是一个三阶段架构的通用语音增强网络。</li>
<li>它包括填充、分离和恢复三个阶段，针对各种语音失真问题。</li>
<li>填充阶段初步填充噪声干扰下丢失的区域，确保信号连续性。</li>
<li>分离阶段抑制噪声、回声和削峰失真，提高语音清晰度。</li>
<li>恢复阶段补偿带宽限制、编码解码器产生的伪影和残留包丢失失真，优化语音质量。</li>
<li>TS-URGENet在Interspeech 2025 URGENT Challenge中取得了优异表现。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.18533">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-9f41b56e9ce2fac03340eb7aeb2e3c2a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-19c740dbd422e6ebeb1e914a9f7defb9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-85c78deffe246ec778c823de049bf78d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eeaca8d37fa572fe3f58dc6fd2ce6849.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-48eff2e7dc62ec053fbe00dd88fa1609.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fbc13a8b882b7a298d1cb481169d91b0.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="ABHINAYA-–-A-System-for-Speech-Emotion-Recognition-In-Naturalistic-Conditions-Challenge"><a href="#ABHINAYA-–-A-System-for-Speech-Emotion-Recognition-In-Naturalistic-Conditions-Challenge" class="headerlink" title="ABHINAYA – A System for Speech Emotion Recognition In Naturalistic   Conditions Challenge"></a>ABHINAYA – A System for Speech Emotion Recognition In Naturalistic   Conditions Challenge</h2><p><strong>Authors:Soumya Dutta, Smruthi Balaji, Varada R, Viveka Salinamakki, Sriram Ganapathy</strong></p>
<p>Speech emotion recognition (SER) in naturalistic settings remains a challenge due to the intrinsic variability, diverse recording conditions, and class imbalance. As participants in the Interspeech Naturalistic SER Challenge which focused on these complexities, we present Abhinaya, a system integrating speech-based, text-based, and speech-text models. Our approach fine-tunes self-supervised and speech large language models (SLLM) for speech representations, leverages large language models (LLM) for textual context, and employs speech-text modeling with an SLLM to capture nuanced emotional cues. To combat class imbalance, we apply tailored loss functions and generate categorical decisions through majority voting. Despite one model not being fully trained, the Abhinaya system ranked 4th among 166 submissions. Upon completion of training, it achieved state-of-the-art performance among published results, demonstrating the effectiveness of our approach for SER in real-world conditions. </p>
<blockquote>
<p>在自然场景中的语音情感识别（SER）由于内在的可变性、多样的录音条件和类别不平衡仍然是一个挑战。作为专注于这些复杂性的Interspeech自然主义SER挑战的参与者，我们推出了阿宾亚雅系统，该系统融合了基于语音、文本和语音文本模型。我们的方法通过微调自我监督和语音大型语言模型（SLLM）进行语音表示，利用大型语言模型（LLM）提供文本上下文，并采用带有SLLM的语音文本建模来捕捉微妙的情感线索。为了解决类别不平衡问题，我们应用了专门的损失函数，并通过多数投票产生分类决策。尽管有一个模型尚未完全训练，但阿宾亚雅系统在166个提交作品中排名第4。训练完成后，它在已发布的结果中取得了最先进的性能，证明了我们的方法在真实世界条件下的SER中的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.18217v1">PDF</a> 5 pages, 2 figures, 4 tables, accepted at Interspeech 2025</p>
<p><strong>Summary</strong></p>
<p>该文本介绍了针对自然场景下的语音情感识别（SER）挑战，提出了一种名为Abhinaya的系统。该系统结合了基于语音、文本和语音文本模型，通过微调自我监督和语音大型语言模型进行语音表示，利用大型语言模型提供文本背景，并通过语音文本建模捕捉微妙的情感线索。为应对类别不平衡问题，采用了定制的损失函数，并通过多数投票进行类别决策。Abhinaya系统在未完全训练的情况下排名第4，且在完成训练后达到了最新的性能水平，证明了该系统在自然条件下的SER的有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>介绍了针对自然场景下的语音情感识别（SER）的挑战。</li>
<li>提出了名为Abhinaya的系统，结合了基于语音、文本和语音文本模型的方法。</li>
<li>通过微调自我监督和语音大型语言模型进行语音表示。</li>
<li>利用大型语言模型提供文本背景信息。</li>
<li>采用语音文本建模以捕捉微妙的情感线索。</li>
<li>采用定制的损失函数和多数投票机制应对类别不平衡问题。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.18217">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-bac56ca59f187eacecea26e29cf09c71.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fc5fe34cdccde021049dc0fc5b11c05a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-63af187fac36173816bc48325fa845a3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9d14934eb36afb731609b8f90a8a787d.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Dysfluent-WFST-A-Framework-for-Zero-Shot-Speech-Dysfluency-Transcription-and-Detection"><a href="#Dysfluent-WFST-A-Framework-for-Zero-Shot-Speech-Dysfluency-Transcription-and-Detection" class="headerlink" title="Dysfluent WFST: A Framework for Zero-Shot Speech Dysfluency   Transcription and Detection"></a>Dysfluent WFST: A Framework for Zero-Shot Speech Dysfluency   Transcription and Detection</h2><p><strong>Authors:Chenxu Guo, Jiachen Lian, Xuanru Zhou, Jinming Zhang, Shuhe Li, Zongli Ye, Hwi Joo Park, Anaisha Das, Zoe Ezzes, Jet Vonk, Brittany Morin, Rian Bogley, Lisa Wauters, Zachary Miller, Maria Gorno-Tempini, Gopala Anumanchipalli</strong></p>
<p>Automatic detection of speech dysfluency aids speech-language pathologists in efficient transcription of disordered speech, enhancing diagnostics and treatment planning. Traditional methods, often limited to classification, provide insufficient clinical insight, and text-independent models misclassify dysfluency, especially in context-dependent cases. This work introduces Dysfluent-WFST, a zero-shot decoder that simultaneously transcribes phonemes and detects dysfluency. Unlike previous models, Dysfluent-WFST operates with upstream encoders like WavLM and requires no additional training. It achieves state-of-the-art performance in both phonetic error rate and dysfluency detection on simulated and real speech data. Our approach is lightweight, interpretable, and effective, demonstrating that explicit modeling of pronunciation behavior in decoding, rather than complex architectures, is key to improving dysfluency processing systems. </p>
<blockquote>
<p>自动检测语言流畅障碍有助于语言病理学家高效地转录异常语音，提高诊断和制定治疗方案。传统方法往往局限于分类，无法提供足够的临床见解，而独立于文本之外的模型在检测语言流畅障碍时会出现误判，特别是在依赖于语境的案例中。本研究引入了Dysfluent-WFST，这是一种零样本解码器，可同时转录音素并检测语言流畅障碍。不同于之前的模型，Dysfluent-WFST与上游编码器（如WavLM）协同工作，无需额外训练。它在模拟和真实语音数据上实现了最佳的性能，无论是在音素错误率还是语言流畅性检测方面。我们的方法轻巧、可解释、有效，表明在解码过程中明确建模发音行为，而不是复杂的架构，是提高语言流畅障碍处理系统的关键。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16351v2">PDF</a> Accepted for Interspeech2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种名为Dysfluent-WFST的零样本解码器，它能同时转录语音并检测语言流畅性问题。与传统的分类方法相比，该解码器在模拟和实际语音数据的语音错误率和语言流畅性检测方面表现出卓越的性能。其采用轻量级、可解释的设计，强调在解码过程中明确建模发音行为是提高语言流畅性处理系统的关键。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>自动检测语言流畅性有助于语言病理学家有效转录障碍性语言，提升诊断和制定治疗方案。</li>
<li>传统方法主要限于分类，提供有限的临床洞察，而文本独立模型在语境依赖的案例中误分类语言流畅性问题。</li>
<li>Dysfluent-WFST是一种零样本解码器，可同时进行语音转录和语言流畅性检测。</li>
<li>与其他模型不同，Dysfluent-WFST使用上游编码器如WavLM，无需额外训练。</li>
<li>Dysfluent-WFST在模拟和实际语音数据上实现了语音错误率和语言流畅性检测的最新性能。</li>
<li>该方法采用轻量级、可解释的设计，强调在解码过程中明确建模发音行为的重要性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16351">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-78b9bb9e4efe2137a8d24d8858a976c0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-72dbb715284d0f9e467b80eaea1fb515.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c698578373eddacab7d8c2ec84c99f22.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-db83b56946c95c5fc3097276cb396e02.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7fc99bdb2d2621beb13e9ce8a2babca7.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Data-Efficient-Hate-Speech-Detection-via-Cross-Lingual-Nearest-Neighbor-Retrieval-with-Limited-Labeled-Data"><a href="#Data-Efficient-Hate-Speech-Detection-via-Cross-Lingual-Nearest-Neighbor-Retrieval-with-Limited-Labeled-Data" class="headerlink" title="Data-Efficient Hate Speech Detection via Cross-Lingual Nearest Neighbor   Retrieval with Limited Labeled Data"></a>Data-Efficient Hate Speech Detection via Cross-Lingual Nearest Neighbor   Retrieval with Limited Labeled Data</h2><p><strong>Authors:Faeze Ghorbanpour, Daryna Dementieva, Alexander Fraser</strong></p>
<p>Considering the importance of detecting hateful language, labeled hate speech data is expensive and time-consuming to collect, particularly for low-resource languages. Prior work has demonstrated the effectiveness of cross-lingual transfer learning and data augmentation in improving performance on tasks with limited labeled data. To develop an efficient and scalable cross-lingual transfer learning approach, we leverage nearest-neighbor retrieval to augment minimal labeled data in the target language, thereby enhancing detection performance. Specifically, we assume access to a small set of labeled training instances in the target language and use these to retrieve the most relevant labeled examples from a large multilingual hate speech detection pool. We evaluate our approach on eight languages and demonstrate that it consistently outperforms models trained solely on the target language data. Furthermore, in most cases, our method surpasses the current state-of-the-art. Notably, our approach is highly data-efficient, retrieving as small as 200 instances in some cases while maintaining superior performance. Moreover, it is scalable, as the retrieval pool can be easily expanded, and the method can be readily adapted to new languages and tasks. We also apply maximum marginal relevance to mitigate redundancy and filter out highly similar retrieved instances, resulting in improvements in some languages. </p>
<blockquote>
<p>考虑到检测仇恨性言论的重要性，标注仇恨言论的数据收集既昂贵又耗时，特别是对于资源贫乏的语言来说。先前的研究已经证明了跨语言迁移学习和数据增强在改善有限标记数据任务性能方面的有效性。为了开发一种高效且可扩展的跨语言迁移学习方法，我们利用最近邻检索来增强目标语言的少量标记数据，从而提高检测性能。具体来说，我们假设目标语言中有少量标记的训练实例可供访问，并使用这些实例从大型多语言仇恨言论检测池中检索最相关的标记示例。我们在八种语言上评估了我们的方法，并证明它始终优于仅使用目标语言数据训练的模型。此外，在大多数情况下，我们的方法超越了当前的最佳水平。值得注意的是，我们的方法非常高效，在某些情况下只需检索200个实例就能保持卓越的性能。而且，它是可扩展的，因为检索池可以很容易地扩展，并且该方法可以很容易地适应新的语言和任务。我们还应用最大边缘相关性来缓解冗余并过滤掉高度相似的检索实例，从而在某些语言中取得了改进。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14272v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本研究关注仇恨言论检测的重要性，针对低资源语言标注仇恨言论数据收集的高成本和耗时的挑战，提出一种高效且可扩展的跨语言迁移学习方法。通过利用最近邻检索技术增强目标语言的少量标注数据，提高检测性能。研究假设拥有目标语言的少量标注训练实例，并从大型多语言仇恨言论检测池中检索最相关的实例。在八种语言上的评估显示，该方法优于仅使用目标语言数据的模型，并在多数情况下达到当前最佳水平。该方法具有高效性和可扩展性，可轻松扩展检索池，并易于适应新语言和任务。同时，研究还采用最大边际相关性方法减少冗余，过滤出高度相似的检索实例，进一步提高某些语言的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>跨语言迁移学习用于提高低资源语言的仇恨言论检测性能。</li>
<li>利用最近邻检索技术增强目标语言的少量标注数据。</li>
<li>方法通过检索最相关的实例从大型多语言仇恨言论检测池中丰富数据。</li>
<li>在八种语言上的评估显示该方法优于其他方法。</li>
<li>方法具有高效性和可扩展性，可适应新语言和任务。</li>
<li>采用最大边际相关性方法减少冗余检索实例。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14272">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-37be0e9e3c247cc09d7d80cc71c3e764.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f467bca691d37698d9f05f6135e64e4d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-331cbf8c3b53eb673a660dd4621f4ead.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="LipDiffuser-Lip-to-Speech-Generation-with-Conditional-Diffusion-Models"><a href="#LipDiffuser-Lip-to-Speech-Generation-with-Conditional-Diffusion-Models" class="headerlink" title="LipDiffuser: Lip-to-Speech Generation with Conditional Diffusion Models"></a>LipDiffuser: Lip-to-Speech Generation with Conditional Diffusion Models</h2><p><strong>Authors:Danilo de Oliveira, Julius Richter, Tal Peer, Timo Gerkmann</strong></p>
<p>We present LipDiffuser, a conditional diffusion model for lip-to-speech generation synthesizing natural and intelligible speech directly from silent video recordings. Our approach leverages the magnitude-preserving ablated diffusion model (MP-ADM) architecture as a denoiser model. To effectively condition the model, we incorporate visual features using magnitude-preserving feature-wise linear modulation (MP-FiLM) alongside speaker embeddings. A neural vocoder then reconstructs the speech waveform from the generated mel-spectrograms. Evaluations on LRS3 and TCD-TIMIT demonstrate that LipDiffuser outperforms existing lip-to-speech baselines in perceptual speech quality and speaker similarity, while remaining competitive in downstream automatic speech recognition (ASR). These findings are also supported by a formal listening experiment. Extensive ablation studies and cross-dataset evaluation confirm the effectiveness and generalization capabilities of our approach. </p>
<blockquote>
<p>我们提出了LipDiffuser，这是一种用于唇语到语音生成的条件扩散模型，它可以直接从无声的视频记录中合成自然和可理解的语音。我们的方法采用幅度保持消融扩散模型（MP-ADM）架构作为去噪模型。为了有效地对模型进行条件处理，我们采用幅度保持特征线性调制（MP-FiLM）结合语音者嵌入来融入视觉特征。然后，神经网络vocoder从生成的梅尔频谱中重建语音波形。在LRS3和TCD-TIMIT上的评估表明，LipDiffuser在感知语音质量和语音者相似性方面优于现有的唇语到语音基线，同时在下游自动语音识别（ASR）中保持竞争力。正式听力学实验的结果也支持了这一发现。广泛的消融研究和跨数据集评估证实了我们的方法的有效性和泛化能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11391v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了LipDiffuser，一种用于唇语生成的条件扩散模型。它通过采用MP-ADM架构作为去噪模型，结合MP-FiLM视觉特征和说话人嵌入进行条件化建模，实现了从无声视频录制中直接合成自然可理解的语音。实验结果表明，在LRS3和TCD-TIMIT数据集上，LipDiffuser在感知语音质量和说话人相似性方面优于现有的唇语基线，同时在下游自动语音识别（ASR）方面保持竞争力。正式听力学实验进一步证实了这些发现。广泛的消融研究和跨数据集评估证实了该方法的有效性和泛化能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LipDiffuser是一种条件扩散模型，用于从无声视频生成自然可理解的语音。</li>
<li>MP-ADM架构作为去噪模型用于处理核心任务。</li>
<li>MP-FiLM视觉特征和说话人嵌入用于有效地条件化模型。</li>
<li>神经vocoder从生成的mel-spectrograms重建语音波形。</li>
<li>在LRS3和TCD-TIMIT数据集上的实验表明，LipDiffuser在感知语音质量和说话人相似性方面优于现有方法。</li>
<li>正式听力学实验证实了这些发现。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11391">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-3eb0b4c9a2d97d9dbaddb39e3fd53949.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d0d352f2b6b3f6603cbc19a09cafcf1e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3ed1eadf42050b23610113edaeee7b2f.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="SepALM-Audio-Language-Models-Are-Error-Correctors-for-Robust-Speech-Separation"><a href="#SepALM-Audio-Language-Models-Are-Error-Correctors-for-Robust-Speech-Separation" class="headerlink" title="SepALM: Audio Language Models Are Error Correctors for Robust Speech   Separation"></a>SepALM: Audio Language Models Are Error Correctors for Robust Speech   Separation</h2><p><strong>Authors:Zhaoxi Mu, Xinyu Yang, Gang Wang</strong></p>
<p>While contemporary speech separation technologies adeptly process lengthy mixed audio waveforms, they are frequently challenged by the intricacies of real-world environments, including noisy and reverberant settings, which can result in artifacts or distortions in the separated speech. To overcome these limitations, we introduce SepALM, a pioneering approach that employs audio language models (ALMs) to rectify and re-synthesize speech within the text domain following preliminary separation. SepALM comprises four core components: a separator, a corrector, a synthesizer, and an aligner. By integrating an ALM-based end-to-end error correction mechanism, we mitigate the risk of error accumulation and circumvent the optimization hurdles typically encountered in conventional methods that amalgamate automatic speech recognition (ASR) with large language models (LLMs). Additionally, we have developed Chain-of-Thought (CoT) prompting and knowledge distillation techniques to facilitate the reasoning and training processes of the ALM. Our experiments substantiate that SepALM not only elevates the precision of speech separation but also markedly bolsters adaptability in novel acoustic environments. </p>
<blockquote>
<p>虽然当代的语音识别分离技术能够处理冗长的混合音频波形，但它们在实际环境中常常面临复杂情况，包括噪声和混响环境等，这些都可能导致分离后的语音出现伪像或失真。为了克服这些局限，我们引入了SepALM，这是一种采用音频语言模型（ALM）在初步分离后修复和重新合成文本领域内的语音的开创性方法。SepALM包含四个核心组件：分离器、校正器、合成器和对齐器。通过集成基于ALM的端到端错误校正机制，我们降低了误差累积的风险，并绕过了传统方法中遇到的优化障碍，这些方法将自动语音识别（ASR）与大型语言模型（LLM）合并使用。此外，我们还开发了链式思维（CoT）提示和知识蒸馏技术，以促进ALM的推理和训练过程。我们的实验证实，SepALM不仅提高了语音分离的精度，还显著提高了对新环境音的适应性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.03273v2">PDF</a> Appears in IJCAI 2025</p>
<p><strong>Summary</strong>：针对当前语音分离技术在处理复杂现实环境（如噪声和回声环境）时面临的挑战，提出了一种名为SepALM的创新方法。该方法采用音频语言模型（ALM）在文本域内进行修正和重新合成。SepALM包括四个核心组件：分离器、校正器、合成器和对齐器。通过端到端的错误校正机制，减少了误差累积的风险，并克服了传统方法中自动语音识别（ASR）与大型语言模型（LLM）融合的优化难题。实验证明，SepALM不仅提高了语音分离的精度，还显著提高了对新环境的适应能力。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>SepALM是一种针对复杂现实环境设计的创新语音分离方法。</li>
<li>SepALM采用音频语言模型（ALM）在文本域进行修正和重新合成。</li>
<li>SepALM包括分离器、校正器、合成器和对齐器四个核心组件。</li>
<li>通过端到端的错误校正机制，SepALM减少了误差累积的风险。</li>
<li>SepALM克服了传统方法中ASR与LLM融合的优化难题。</li>
<li>实验证明SepALM提高了语音分离的精度。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.03273">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-80444f5565c4b1078184199006344d0f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5bd4438546dbe67bd97e63c99d89b160.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ed17bf3bba7f560f07e4be5a3917455e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6c1aa4cfbfb4479b1589955bea09bb0a.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Sugar-Coated-Poison-Benign-Generation-Unlocks-LLM-Jailbreaking"><a href="#Sugar-Coated-Poison-Benign-Generation-Unlocks-LLM-Jailbreaking" class="headerlink" title="Sugar-Coated Poison: Benign Generation Unlocks LLM Jailbreaking"></a>Sugar-Coated Poison: Benign Generation Unlocks LLM Jailbreaking</h2><p><strong>Authors:Yu-Hang Wu, Yu-Jie Xiong, Hao Zhang, Jia-Chen Zhang, Zheng Zhou</strong></p>
<p>With the increasingly deep integration of large language models (LLMs) across diverse domains, the effectiveness of their safety mechanisms is encountering severe challenges. Currently, jailbreak attacks based on prompt engineering have become a major safety threat. However, existing methods primarily rely on black-box manipulation of prompt templates, resulting in poor interpretability and limited generalization. To break through the bottleneck, this study first introduces the concept of Defense Threshold Decay (DTD), revealing the potential safety impact caused by LLMs’ benign generation: as benign content generation in LLMs increases, the model’s focus on input instructions progressively diminishes. Building on this insight, we propose the Sugar-Coated Poison (SCP) attack paradigm, which uses a “semantic reversal” strategy to craft benign inputs that are opposite in meaning to malicious intent. This strategy induces the models to generate extensive benign content, thereby enabling adversarial reasoning to bypass safety mechanisms. Experiments show that SCP outperforms existing baselines. Remarkably, it achieves an average attack success rate of 87.23% across six LLMs. For defense, we propose Part-of-Speech Defense (POSD), leveraging verb-noun dependencies for syntactic analysis to enhance safety of LLMs while preserving their generalization ability. </p>
<blockquote>
<p>随着大型语言模型（LLMs）在各个领域越来越深入的融合，其安全机制的有效性正面临严峻挑战。目前，基于提示工程的越狱攻击已成为主要的安全威胁。然而，现有方法主要依赖于提示模板的黑盒操作，导致解释性差和泛化能力有限。为了突破瓶颈，本研究首先引入防御阈值衰减（DTD）的概念，揭示LLMs良性生成所潜在的安全影响：随着LLMs中良性内容生成的增加，模型对输入指令的关注逐渐减少。基于这一见解，我们提出了“糖衣毒药”（SCP）攻击范式，采用“语义反转”策略来制作与恶意意图相反含义的良性输入。该策略诱导模型生成大量良性内容，从而通过对抗性推理绕过安全机制。实验表明，SCP优于现有基线。值得一提的是，它在六个LLMs上平均攻击成功率达到87.23%。为防御此攻击，我们提出了基于词性的防御（POSD）方法，利用动词-名词依赖进行句法分析来提高LLMs的安全性，同时保留其泛化能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05652v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大语言模型（LLM）在多领域深度集成中面临安全挑战，出现了基于提示工程的越狱攻击等安全威胁。现有方法主要依赖黑盒操作提示模板，存在解释性差和泛化能力有限的问题。本研究引入防御阈值衰减（DTD）概念，揭示LLM良性生成对安全的影响：随着良性内容生成增加，模型对输入指令的关注逐渐减少。基于此，提出“糖衣毒药”（SCP）攻击范式，采用“语义反转”策略制作与恶意意图相反的良性输入，诱导模型生成大量良性内容，实现对抗推理，绕过安全机制。实验显示，SCP优于现有基线，平均攻击成功率达87.23%，覆盖六种LLM。为防御，提出利用动词-名词依赖进行句法分析的词性防御（POSD），增强LLM安全性同时保持其泛化能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大语言模型（LLM）在多个领域集成时面临严重的安全挑战。</li>
<li>目前存在的攻击方法主要基于提示工程，存在解释性差和泛化能力有限的问题。</li>
<li>引入防御阈值衰减（DTD）概念，揭示LLM良性生成对安全的影响。</li>
<li>提出“糖衣毒药”（SCP）攻击策略，通过语义反转绕过LLM的安全机制。</li>
<li>SCP攻击策略在实验中表现优异，平均攻击成功率达87.23%。</li>
<li>为应对此攻击，提出词性防御（POSD）方法，结合句法分析增强LLM安全性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05652">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f62f5a590735dd5e784e7893019b3a64.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2af7147a0bcd95ab1898082ac9737731.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-db3e77398928b5778a035b59da55a609.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b5d464b6bb8c449baa5cecf2881b16de.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-246a6aeaa2379e0b2623555fcef60a42.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4b6fd0f1af0553b9deeda62d4dbca168.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Speech-FT-Merging-Pre-trained-And-Fine-Tuned-Speech-Representation-Models-For-Cross-Task-Generalization"><a href="#Speech-FT-Merging-Pre-trained-And-Fine-Tuned-Speech-Representation-Models-For-Cross-Task-Generalization" class="headerlink" title="Speech-FT: Merging Pre-trained And Fine-Tuned Speech Representation   Models For Cross-Task Generalization"></a>Speech-FT: Merging Pre-trained And Fine-Tuned Speech Representation   Models For Cross-Task Generalization</h2><p><strong>Authors:Tzu-Quan Lin, Wei-Ping Huang, Hao Tang, Hung-yi Lee</strong></p>
<p>Fine-tuning speech representation models can enhance performance on specific tasks but often compromises their cross-task generalization ability. This degradation is often caused by excessive changes in the representations, making it difficult to retain information learned during pre-training. Existing approaches, such as regularizing weight changes during fine-tuning, may fail to maintain sufficiently high feature similarity with the pre-trained model, and thus could possibly lose cross-task generalization. To address this issue, we propose Speech-FT, a novel two-stage fine-tuning framework designed to maintain cross-task generalization while benefiting from fine-tuning. Speech-FT first applies fine-tuning specifically designed to reduce representational drift, followed by weight-space interpolation with the pre-trained model to restore cross-task generalization. Extensive experiments on HuBERT, wav2vec 2.0, DeCoAR 2.0, and WavLM Base+ demonstrate that Speech-FT consistently improves performance across a wide range of supervised, unsupervised, and multitask fine-tuning scenarios. Moreover, Speech-FT achieves superior cross-task generalization compared to fine-tuning baselines that explicitly constrain weight changes, such as weight-space regularization and LoRA fine-tuning. Our analysis reveals that Speech-FT maintains higher feature similarity to the pre-trained model compared to alternative strategies, despite allowing larger weight-space updates. Notably, Speech-FT achieves significant improvements on the SUPERB benchmark. For example, when fine-tuning HuBERT on automatic speech recognition, Speech-FT is able to reduce phone error rate from 5.17% to 3.94%, lower word error rate from 6.38% to 5.75%, and increase speaker identification accuracy from 81.86% to 84.11%. Speech-FT provides a simple yet powerful solution for further refining speech representation models after pre-training. </p>
<blockquote>
<p>对语音表征模型进行微调可以提高特定任务的性能，但通常会损害其跨任务泛化能力。这种退化通常是由于表征的过度变化，使得难以保留预训练期间学到的信息。现有方法，如正则化微调过程中的权重变化，可能无法保持与预训练模型足够高的特征相似性，因此可能丧失跨任务泛化能力。为解决这一问题，我们提出了Speech-FT，这是一种新型的两阶段微调框架，旨在保持跨任务泛化能力的同时受益于微调。Speech-FT首先应用专门设计的减少表征漂移的微调，然后通过权重空间插值与预训练模型恢复跨任务泛化。在HuBERT、wav2vec 2.0、DeCoAR 2. 0和WavLM Base+上的大量实验表明，Speech-FT在广泛的监督、无监督和多任务微调场景中始终提高了性能。而且，Speech-FT在跨任务泛化方面优于显式约束权重变化的微调基准，如权重空间正则化和LoRA微调。我们的分析表明，尽管允许更大的权重空间更新，Speech-FT与预训练模型的特征相似性仍然高于其他策略。值得注意的是，Speech-FT在SUPERB基准测试中取得了显著的改进。例如，在对HuBERT进行自动语音识别微调时，Speech-FT能够将电话错误率从5.17%降低到3.94%，单词错误率从6.38%降低到5.75%，并提高说话人识别准确率至84.11%。Speech-FT为预训练后进一步精炼语音表征模型提供了一种简单而强大的解决方案。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12672v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种名为Speech-FT的新型两阶段微调框架，旨在保持跨任务泛化能力的同时，受益于微调。Speech-FT首先应用专门设计的减少表示漂移的微调，然后通过预训练模型进行权重空间插值以恢复跨任务泛化。实验表明，Speech-FT在多种监督、无监督和多任务微调场景下均能提高性能，并在跨任务泛化方面优于其他微调方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>现有微调方法在提高特定任务性能时，可能会损害模型的跨任务泛化能力。</li>
<li>Speech-FT采用两阶段微调策略，旨在减少表示漂移并恢复跨任务泛化。</li>
<li>Speech-FT通过权重空间插值与预训练模型相结合，保持与预训练模型的高特征相似性。</li>
<li>Speech-FT在多种语音模型及不同任务场景下均表现出优异的性能提升。</li>
<li>Speech-FT在跨任务泛化方面优于其他显式约束权重变化的微调方法。</li>
<li>Speech-FT降低了自动语音识别中的电话错误率、单词错误率，并提高了说话人识别准确率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12672">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e90374152195b315d4b35c4031f02c75.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-73c8aebf1b5dbcd78d3eacf49aed080a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3954990023620f8dac02eacd533db9c4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-68ce3edffcd701675c0754affd37b9c9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d8e56a6218a636af031ea5d0d6653e60.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-30f59dc36ac336c08ee0cb3191ac2538.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-28/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-28/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-28/Face%20Swapping/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-30f9e0fed936b7a031404e34c5ce3556.jpg" class="responsive-img" alt="Face Swapping">
                        
                        <span class="card-title">Face Swapping</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Face Swapping 方向最新论文已更新，请持续关注 Update in 2025-05-28  Towards Generalized Proactive Defense against Face Swappingwith   Contour-Hybrid Watermark
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-28
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Face-Swapping/" class="post-category">
                                    Face Swapping
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Face-Swapping/">
                        <span class="chip bg-color">Face Swapping</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-28/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-54042b707075d18d4db98ee14c047c9b.jpg" class="responsive-img" alt="无监督/半监督/对比学习">
                        
                        <span class="card-title">无监督/半监督/对比学习</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            无监督/半监督/对比学习 方向最新论文已更新，请持续关注 Update in 2025-05-28  Hard Negative Contrastive Learning for Fine-Grained Geometric   Understanding in Large Multimodal Models
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-28
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    无监督/半监督/对比学习
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">无监督/半监督/对比学习</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">25691.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
