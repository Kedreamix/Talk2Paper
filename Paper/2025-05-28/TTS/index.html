<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="TTS">
    <meta name="description" content="TTS 方向最新论文已更新，请持续关注 Update in 2025-05-28  Zero-Shot Streaming Text to Speech Synthesis with Transducer and   Auto-Regressive Modeling">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>TTS | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-f9fa5d40cacd81f3f87412b0f4b1980b.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">TTS</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/TTS/">
                                <span class="chip bg-color">TTS</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                TTS
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-05-28
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-06-24
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    8.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    34 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-05-28-更新"><a href="#2025-05-28-更新" class="headerlink" title="2025-05-28 更新"></a>2025-05-28 更新</h1><h2 id="Zero-Shot-Streaming-Text-to-Speech-Synthesis-with-Transducer-and-Auto-Regressive-Modeling"><a href="#Zero-Shot-Streaming-Text-to-Speech-Synthesis-with-Transducer-and-Auto-Regressive-Modeling" class="headerlink" title="Zero-Shot Streaming Text to Speech Synthesis with Transducer and   Auto-Regressive Modeling"></a>Zero-Shot Streaming Text to Speech Synthesis with Transducer and   Auto-Regressive Modeling</h2><p><strong>Authors:Haiyang Sun, Shujie Hu, Shujie Liu, Lingwei Meng, Hui Wang, Bing Han, Yifan Yang, Yanqing Liu, Sheng Zhao, Yan Lu, Yanmin Qian</strong></p>
<p>Zero-shot streaming text-to-speech is an important research topic in human-computer interaction. Existing methods primarily use a lookahead mechanism, relying on future text to achieve natural streaming speech synthesis, which introduces high processing latency. To address this issue, we propose SMLLE, a streaming framework for generating high-quality speech frame-by-frame. SMLLE employs a Transducer to convert text into semantic tokens in real time while simultaneously obtaining duration alignment information. The combined outputs are then fed into a fully autoregressive (AR) streaming model to reconstruct mel-spectrograms. To further stabilize the generation process, we design a Delete &lt; Bos &gt; Mechanism that allows the AR model to access future text introducing as minimal delay as possible. Experimental results suggest that the SMLLE outperforms current streaming TTS methods and achieves comparable performance over sentence-level TTS systems. Samples are available on <a target="_blank" rel="noopener" href="https://anonymous.4open.science/w/demo_page-48B7/">https://anonymous.4open.science/w/demo_page-48B7/</a>. </p>
<blockquote>
<p>零样本流式文本到语音转换是计算机人机交互领域的一个重要研究课题。现有的方法主要使用前视机制，依靠未来的文本实现自然的流式语音合成，这引入了很高的处理延迟。为了解决这一问题，我们提出了SMLLE，这是一个用于逐帧生成高质量语音的流式框架。SMLLE使用转换器将文本实时转换为语义令牌，同时获得持续时间对齐信息。然后将这些组合输出馈入完全自回归（AR）流式模型，以重建梅尔频谱图。为了进一步稳定生成过程，我们设计了一种删除<Bos>机制，允许AR模型访问未来的文本，并尽量减少延迟。实验结果表明，SMLLE优于当前的流式TTS方法，并在句子级TTS系统上取得了相当的性能。样本可通过<a target="_blank" rel="noopener" href="https://anonymous.4open.science/w/demo_page-48B7/%E8%AE%BF%E9%97%AE%E3%80%82">https://anonymous.4open.science/w/demo_page-48B7/访问。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19669v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种名为SMLLE的零样本流式文本转语音生成框架，解决了现有方法处理延迟高的问题。通过实时将文本转换为语义标记并获取时长对齐信息，再输入全自回归（AR）流式模型重建梅尔频谱图，实现高质量语音帧生成。设计删除<Bos>机制以进一步稳定生成过程，减少未来文本引入的延迟。实验结果表明，SMLLE在流式TTS方法中表现优异，与句子级TTS系统性能相当。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SMLLE是一个用于生成高质量语音的零样本流式文本转语音框架。</li>
<li>采用Transducer实时将文本转换为语义标记并获取时长对齐信息。</li>
<li>结合语义标记和时长对齐信息输入全自回归（AR）流式模型。</li>
<li>通过重建梅尔频谱图实现高质量语音帧生成。</li>
<li>设计删除<Bos>机制以稳定生成过程并减少延迟。</li>
<li>实验结果表明，SMLLE在流式TTS方法中表现优异。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19669">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-a16b6c187c1a80a3555a1b251a3c2fea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ac85b5d00b85bfd04fc781af2e01efa1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-624ee28d8c6c16670ea64fde61dfc4ca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ae8b672898fa672e2e6c8d4804e8340f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6ab7cd7db3d39a98ef74fcf43b056641.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4feefe0e383fe21e443a7da5a256caf2.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Faster-and-Better-LLMs-via-Latency-Aware-Test-Time-Scaling"><a href="#Faster-and-Better-LLMs-via-Latency-Aware-Test-Time-Scaling" class="headerlink" title="Faster and Better LLMs via Latency-Aware Test-Time Scaling"></a>Faster and Better LLMs via Latency-Aware Test-Time Scaling</h2><p><strong>Authors:Zili Wang, Tianyu Zhang, Haoli Bai, Lu Hou, Xianzhi Yu, Wulong Liu, Shiming Xiang, Lei Zhu</strong></p>
<p>Test-Time Scaling (TTS) has proven effective in improving the performance of Large Language Models (LLMs) during inference. However, existing research has overlooked the efficiency of TTS from a latency-sensitive perspective. Through a latency-aware evaluation of representative TTS methods, we demonstrate that a compute-optimal TTS does not always result in the lowest latency in scenarios where latency is critical. To address this gap and achieve latency-optimal TTS, we propose two key approaches by optimizing the concurrency configurations: (1) branch-wise parallelism, which leverages multiple concurrent inference branches, and (2) sequence-wise parallelism, enabled by speculative decoding. By integrating these two approaches and allocating computational resources properly to each, our latency-optimal TTS enables a 32B model to reach 82.3% accuracy on MATH-500 within 1 minute and a smaller 3B model to achieve 72.4% within 10 seconds. Our work emphasizes the importance of latency-aware TTS and demonstrates its ability to deliver both speed and accuracy in latency-sensitive scenarios. </p>
<blockquote>
<p>测试时间缩放（TTS）已被证明可以有效提高大型语言模型（LLM）在推理过程中的性能。然而，现有研究从延迟敏感的角度忽视了TTS的效率。通过对代表性TTS方法进行延迟感知评估，我们证明计算最优的TTS并不总是导致最低延迟，这在延迟至关重要的场景中尤为重要。为了解决这一差距并实现延迟最优的TTS，我们提出了两种通过优化并发配置的关键方法：（1）分支并行性，利用多个并发推理分支；（2）序列并行性，通过投机解码实现。通过整合这两种方法并为每种方法适当分配计算资源，我们的延迟最优TTS使32B模型在MATH-500上1分钟内达到82.3%的准确率，较小的3B模型在10秒内达到72.4%的准确率。我们的工作强调了延迟感知TTS的重要性，并展示了其在延迟敏感场景中实现速度和准确性的能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19634v1">PDF</a> </p>
<p><strong>Summary</strong><br>     测试时间缩放（TTS）在提升大型语言模型（LLM）推理性能上效果显著。然而，现有研究从延迟敏感的角度忽视了TTS的效率。我们通过延迟感知的代表性TTS方法评估，证明了计算最优的TTS并不总是导致最低延迟，这在延迟至关重要的情况下尤为如此。为了弥补这一差距并实现延迟最优的TTS，我们提出了两种通过优化并发配置的关键方法：（1）分支并行性，利用多个并发推理分支；（2）序列并行性，通过投机解码实现。通过整合这两种方法并为每种方法适当分配计算资源，我们的延迟最优TTS使32B模型在MATH-500上1分钟内达到82.3%的准确率，较小的3B模型在10秒内达到72.4%的准确率。我们的工作强调了延迟感知TTS的重要性，并展示了其在延迟敏感场景中实现速度和准确性的能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TTS在提高LLM推理性能上有效，但现有研究未充分关注其延迟效率。</li>
<li>计算最优的TTS并不总是导致最低延迟，特别是在延迟敏感的情况下。</li>
<li>提出两种优化TTS并发配置的方法：分支并行性和序列并行性。</li>
<li>分支并行性利用多个并发推理分支。</li>
<li>序列并行性通过投机解码实现。</li>
<li>整合这两种方法并适当分配计算资源，可实现延迟最优的TTS。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19634">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-b2d016ab7b0cf554ded9888b2dd58f5c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-235616815750083229d9ac47ab79bf1a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bc11049b2a067213003b77acf7e59eda.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1fa7f64c34abc1a76feb38474c46ef11.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1cb2ab025abe0288742ee07fedafd092.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5c9a08d1815a39687e6ed039c7f59d70.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Accelerating-Diffusion-based-Text-to-Speech-Model-Training-with-Dual-Modality-Alignment"><a href="#Accelerating-Diffusion-based-Text-to-Speech-Model-Training-with-Dual-Modality-Alignment" class="headerlink" title="Accelerating Diffusion-based Text-to-Speech Model Training with Dual   Modality Alignment"></a>Accelerating Diffusion-based Text-to-Speech Model Training with Dual   Modality Alignment</h2><p><strong>Authors:Jeongsoo Choi, Zhikang Niu, Ji-Hoon Kim, Chunhui Wang, Joon Son Chung, Chen Xie</strong></p>
<p>The goal of this paper is to optimize the training process of diffusion-based text-to-speech models. While recent studies have achieved remarkable advancements, their training demands substantial time and computational costs, largely due to the implicit guidance of diffusion models in learning complex intermediate representations. To address this, we propose A-DMA, an effective strategy for Accelerating training with Dual Modality Alignment. Our method introduces a novel alignment pipeline leveraging both text and speech modalities: text-guided alignment, which incorporates contextual representations, and speech-guided alignment, which refines semantic representations. By aligning hidden states with discriminative features, our training scheme reduces the reliance on diffusion models for learning complex representations. Extensive experiments demonstrate that A-DMA doubles the convergence speed while achieving superior performance over baselines. Code and demo samples are available at: <a target="_blank" rel="noopener" href="https://github.com/ZhikangNiu/A-DMA">https://github.com/ZhikangNiu/A-DMA</a> </p>
<blockquote>
<p>本文的目标是优化基于扩散的文本到语音模型的训练过程。虽然最近的研究取得了显著的进步，但它们的训练需要大量的时间和计算成本，这主要是因为扩散模型在学习复杂中间表示时的隐式指导。为了解决这一问题，我们提出了A-DMA，一种利用双模态对齐加速训练的有效策略。我们的方法引入了一个新的对齐管道，利用文本和语音两种模式：文本引导对齐，它结合了上下文表示；语音引导对齐，它改进了语义表示。通过隐藏状态与判别特征的对齐，我们的训练方案减少了扩散模型在学习复杂表示上的依赖。大量实验表明，A-DMA的收敛速度提高了一倍，同时相对于基线达到了卓越的性能。代码和演示样本可在：&lt;<a target="_blank" rel="noopener" href="https://github.com/ZhikangNiu/A-DMA">https://github.com/ZhikangNiu/A-DMA</a> 观看。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19595v1">PDF</a> Interspeech 2025</p>
<p><strong>Summary</strong><br>文本主要介绍了如何优化基于扩散的文本到语音模型的训练过程。为了解决扩散模型训练时间长、计算成本高的痛点，提出了一种新的加速训练策略A-DMA。通过融合文本和语音模态的双模态对齐方式，加快模型收敛速度并提升性能。实验结果证明，使用A-DMA策略的模型训练速度翻倍，性能优于基线模型。相关代码和演示样本已上传至GitHub。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>该论文旨在优化基于扩散的文本到语音模型的训练过程。</li>
<li>训练过程中存在时间长和计算成本高的痛点。</li>
<li>提出了一种新的加速训练策略A-DMA，通过双模态对齐方式融合文本和语音模态。</li>
<li>A-DMA策略包括文本引导对齐和语音引导对齐两种方式。</li>
<li>通过与基线模型对比实验，证明使用A-DMA策略的模型训练速度翻倍且性能更优。</li>
<li>A-DMA策略通过减少扩散模型对复杂表示的依赖，提高了模型的收敛速度和性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19595">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-be4196815080104956016de557bd5edc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dbc0a1f0dd1a2401ce44e38ebe87d555.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f66c2a08392fed4658d3d0bf370e0087.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0fbdfc50b69995f2063ece097ae9aa3d.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="VoiceStar-Robust-Zero-Shot-Autoregressive-TTS-with-Duration-Control-and-Extrapolation"><a href="#VoiceStar-Robust-Zero-Shot-Autoregressive-TTS-with-Duration-Control-and-Extrapolation" class="headerlink" title="VoiceStar: Robust Zero-Shot Autoregressive TTS with Duration Control and   Extrapolation"></a>VoiceStar: Robust Zero-Shot Autoregressive TTS with Duration Control and   Extrapolation</h2><p><strong>Authors:Puyuan Peng, Shang-Wen Li, Abdelrahman Mohamed, David Harwath</strong></p>
<p>We present VoiceStar, the first zero-shot TTS model that achieves both output duration control and extrapolation. VoiceStar is an autoregressive encoder-decoder neural codec language model, that leverages a novel Progress-Monitoring Rotary Position Embedding (PM-RoPE) and is trained with Continuation-Prompt Mixed (CPM) training. PM-RoPE enables the model to better align text and speech tokens, indicates the target duration for the generated speech, and also allows the model to generate speech waveforms much longer in duration than those seen during. CPM training also helps to mitigate the training&#x2F;inference mismatch, and significantly improves the quality of the generated speech in terms of speaker similarity and intelligibility. VoiceStar outperforms or is on par with current state-of-the-art models on short-form benchmarks such as Librispeech and Seed-TTS, and significantly outperforms these models on long-form&#x2F;extrapolation benchmarks (20-50s) in terms of intelligibility and naturalness. Code and model weights: <a target="_blank" rel="noopener" href="https://github.com/jasonppy/VoiceStar">https://github.com/jasonppy/VoiceStar</a> </p>
<blockquote>
<p>我们推出了VoiceStar，这是第一个实现输出时长控制和外推功能的零样本TTS模型。VoiceStar是一种基于自回归编码器-解码器神经网络的语言模型，它采用新型进度监控旋转位置嵌入（PM-RoPE），并通过连续提示混合（CPM）进行训练。PM-RoPE使模型能够更好地对齐文本和语音标记，指示生成语音的目标时长，并允许模型生成比训练过程中更长的语音波形。CPM训练也有助于减轻训练&#x2F;推断不匹配的问题，并显著提高生成语音的说话人相似度和清晰度。VoiceStar在诸如Librispeech和Seed-TTS等短期基准测试上的表现优于或相当于当前的最先进模型，在长期&#x2F;外推基准测试（20-50秒）的清晰度和自然度方面则大大优于这些模型。代码和模型权重：<a target="_blank" rel="noopener" href="https://github.com/jasonppy/VoiceStar">https://github.com/jasonppy/VoiceStar</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19462v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>VoiceStar是一款零样本TTS模型，具备输出时长控制与外推能力。它采用自回归编码解码器神经网络模型结构，并运用新型Progress-Monitoring Rotary Position Embedding（PM-RoPE）技术与Continuation-Prompt Mixed（CPM）训练方法。PM-RoPE使模型能更好地对齐文本与语音标记，并指示生成语音的目标时长，同时能生成比训练过程中更长的语音波形。CPM训练有助于减轻训练&#x2F;推理不匹配问题，并显著提高生成语音的说话人相似度与清晰度。VoiceStar在短形式基准测试（如Librispeech和Seed-TTS）上表现优异或相当，并在长形式&#x2F;外推基准测试（20-50秒）上显著优于其他模型，在清晰度和自然度方面尤为突出。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VoiceStar是首款实现输出时长控制与外推的零样本TTS模型。</li>
<li>采用自回归编码解码器神经网络模型结构。</li>
<li>引入Progress-Monitoring Rotary Position Embedding（PM-RoPE）技术，改善文本与语音标记的对齐，并指示目标时长。</li>
<li>使用Continuation-Prompt Mixed（CPM）训练方法，提高生成语音质量，并缓解训练&#x2F;推理不匹配问题。</li>
<li>VoiceStar在短形式基准测试上表现优秀，同时在长形式基准测试上显著优于其他模型。</li>
<li>模型仓库与权重公开，便于研究与应用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19462">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-3e81b9e1cc6132d357daec19f21617d9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-67589cfc4a2288a24068acd9be72cdbe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8b32e4dc01e1610a6c97e5f17873e1dd.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="SpeakStream-Streaming-Text-to-Speech-with-Interleaved-Data"><a href="#SpeakStream-Streaming-Text-to-Speech-with-Interleaved-Data" class="headerlink" title="SpeakStream: Streaming Text-to-Speech with Interleaved Data"></a>SpeakStream: Streaming Text-to-Speech with Interleaved Data</h2><p><strong>Authors:Richard He Bai, Zijin Gu, Tatiana Likhomanenko, Navdeep Jaitly</strong></p>
<p>The latency bottleneck of traditional text-to-speech (TTS) systems fundamentally hinders the potential of streaming large language models (LLMs) in conversational AI. These TTS systems, typically trained and inferenced on complete utterances, introduce unacceptable delays, even with optimized inference speeds, when coupled with streaming LLM outputs. This is particularly problematic for creating responsive conversational agents where low first-token latency is critical. In this paper, we present SpeakStream, a streaming TTS system that generates audio incrementally from streaming text using a decoder-only architecture. SpeakStream is trained using a next-step prediction loss on interleaved text-speech data. During inference, it generates speech incrementally while absorbing streaming input text, making it particularly suitable for cascaded conversational AI agents where an LLM streams text to a TTS system. Our experiments demonstrate that SpeakStream achieves state-of-the-art latency results in terms of first-token latency while maintaining the quality of non-streaming TTS systems. </p>
<blockquote>
<p>传统文本转语音（TTS）系统的延迟瓶颈从根本上阻碍了流式大型语言模型（LLM）在对话式人工智能中的潜力。这些TTS系统通常在对完整的发言进行训练和推理，当与流式LLM输出相结合时，即使以优化的推理速度，也会引入不可接受的延迟。这对于创建响应式的对话代理来说特别成问题，其中低首字延迟至关重要。在本文中，我们提出了SpeakStream，这是一个流式TTS系统，它使用仅解码器架构从流式文本中增量生成音频。SpeakStream使用交替的文本-语音数据上的下一步预测损失进行训练。在推理过程中，它在吸收流式输入文本的同时增量生成语音，因此特别适合用于级联对话AI代理，其中LLM将文本流式传输到TTS系统。我们的实验表明，SpeakStream在首字延迟方面达到了最先进的延迟结果，同时保持了非流式TTS系统的质量。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19206v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了SpeakStream这一流式文本转语音（TTS）系统，它能从流式文本中增量生成音频。与传统的TTS系统相比，SpeakStream在生成语音时能够处理流式输入文本，解决了传统TTS系统延迟的问题，尤其适用于级联对话式AI代理。实验表明，SpeakStream在首字延迟方面达到了最先进的性能，同时保持了非流式TTS系统的质量。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>传统TTS系统的延迟问题限制了其在对话式AI中流式大型语言模型（LLM）的潜力。</li>
<li>SpeakStream是一种流式TTS系统，能从流式文本中增量生成音频，解决了延迟问题。</li>
<li>SpeakStream采用解码器仅架构，使用下一步预测损失对交替的文本-语音数据进行训练。</li>
<li>在推理过程中，SpeakStream能增量生成语音，同时吸收流式输入文本。</li>
<li>SpeakStream特别适用于级联对话式AI代理，其中LLM将文本流式传输到TTS系统。</li>
<li>实验表明，SpeakStream在首字延迟方面达到最新技术状态，同时保持语音质量。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19206">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-ad4baf697890303a2310e74e98a4100f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-abcd1ee3b7f478b228e3fbaf03a72b0b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d704f14462fef0cbce57c578b9fa63d1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-31815a3d5d371b3752fdc1a834bf0bdd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2ee136e5a4697d21377c6cb46ff6610f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-983d8e16e0f3e7142b9bf933fdef95e6.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="CloneShield-A-Framework-for-Universal-Perturbation-Against-Zero-Shot-Voice-Cloning"><a href="#CloneShield-A-Framework-for-Universal-Perturbation-Against-Zero-Shot-Voice-Cloning" class="headerlink" title="CloneShield: A Framework for Universal Perturbation Against Zero-Shot   Voice Cloning"></a>CloneShield: A Framework for Universal Perturbation Against Zero-Shot   Voice Cloning</h2><p><strong>Authors:Renyuan Li, Zhibo Liang, Haichuan Zhang, Tianyu Shi, Zhiyuan Cheng, Jia Shi, Carl Yang, Mingjie Tang</strong></p>
<p>Recent breakthroughs in text-to-speech (TTS) voice cloning have raised serious privacy concerns, allowing highly accurate vocal identity replication from just a few seconds of reference audio, while retaining the speaker’s vocal authenticity. In this paper, we introduce CloneShield, a universal time-domain adversarial perturbation framework specifically designed to defend against zero-shot voice cloning. Our method provides protection that is robust across speakers and utterances, without requiring any prior knowledge of the synthesized text. We formulate perturbation generation as a multi-objective optimization problem, and propose Multi-Gradient Descent Algorithm (MGDA) to ensure the robust protection across diverse utterances. To preserve natural auditory perception for users, we decompose the adversarial perturbation via Mel-spectrogram representations and fine-tune it for each sample. This design ensures imperceptibility while maintaining strong degradation effects on zero-shot cloned outputs. Experiments on three state-of-the-art zero-shot TTS systems, five benchmark datasets and evaluations from 60 human listeners demonstrate that our method preserves near-original audio quality in protected inputs (PESQ &#x3D; 3.90, SRS &#x3D; 0.93) while substantially degrading both speaker similarity and speech quality in cloned samples (PESQ &#x3D; 1.07, SRS &#x3D; 0.08). </p>
<blockquote>
<p>在文本转语音（TTS）声音克隆方面的最新突破引发了严重的隐私担忧。现在只需几秒钟的参考音频，就可以实现高度精确的嗓音身份复制，同时保留说话者的嗓音真实性。本文介绍了CloneShield，这是一个专门设计用于防范零样本声音克隆的通用时间域对抗性扰动框架。我们的方法为跨说话者和话语提供了保护，无需对合成文本有任何先验知识。我们将扰动生成制定为一个多目标优化问题，并提出多梯度下降算法（MGDA），以确保在不同的话语中提供稳健的保护。为了保持用户自然的听觉感知，我们通过梅尔频谱表示法分解对抗性扰动，并针对每个样本进行微调。这种设计确保了不可察觉性，同时在对零样本克隆输出进行强烈降解时保持强大的保护效果。在三个最先进的零样本TTS系统、五个基准数据集以及60名人类听者的评估中进行的实验表明，我们的方法在保证受保护输入接近原始音频质量的同时（PESQ &#x3D; 3.90，SRS &#x3D; 0.93），大幅度降低了克隆样本中的说话人相似度和语音质量（PESQ &#x3D; 1.07，SRS &#x3D; 0.08）。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19119v1">PDF</a> 10pages, 4figures</p>
<p><strong>Summary</strong><br>     最新文本转语音（TTS）语音克隆技术的突破引发了严重的隐私担忧，该技术仅需几秒的参考音频即可实现高度准确的语音身份复制，同时保留说话者的语音真实性。本文介绍CloneShield，一种专门针对零样本语音克隆的通用时间域对抗性扰动框架。该方法提供跨说话者和不同话语的鲁棒保护，无需了解合成文本的背景知识。通过制定多目标优化问题，提出了多梯度下降算法（MGDA），以确保在不同话语之间的强大保护能力。为了保留用户听觉的自然感知效果并保留每个样本的独特性，我们通过梅尔频谱图表示法分解对抗性扰动并进行微调。实验表明，该方法在保护输入音频质量的同时，对零样本克隆输出产生了显著的干扰效果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>最新TTS语音克隆技术引发隐私担忧，能仅通过几秒参考音频实现高度准确的语音身份复制。</li>
<li>CloneShield是一个对抗性扰动框架，旨在防御零样本语音克隆，提供跨说话者和话语的鲁棒保护。</li>
<li>CloneShield不需要了解合成文本的背景知识。</li>
<li>通过多目标优化问题制定扰动生成，并提出多梯度下降算法（MGDA）确保保护效果。</li>
<li>为保留自然听觉感知和用户体验，通过梅尔频谱图表示法分解并微调对抗性扰动。</li>
<li>实验证明CloneShield能有效保护输入音频质量，同时显著干扰零样本克隆输出。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19119">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f9fa5d40cacd81f3f87412b0f4b1980b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f48159884e007271b45a6d7ef31becb6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-77d5ab548076a2a7d472a31acfc84848.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1b4dfc995d63dd1d5b5448a7a7a4384b.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="MPE-TTS-Customized-Emotion-Zero-Shot-Text-To-Speech-Using-Multi-Modal-Prompt"><a href="#MPE-TTS-Customized-Emotion-Zero-Shot-Text-To-Speech-Using-Multi-Modal-Prompt" class="headerlink" title="MPE-TTS: Customized Emotion Zero-Shot Text-To-Speech Using Multi-Modal   Prompt"></a>MPE-TTS: Customized Emotion Zero-Shot Text-To-Speech Using Multi-Modal   Prompt</h2><p><strong>Authors:Zhichao Wu, Yueteng Kang, Songjun Cao, Long Ma, Qiulin Li, Qun Yang</strong></p>
<p>Most existing Zero-Shot Text-To-Speech(ZS-TTS) systems generate the unseen speech based on single prompt, such as reference speech or text descriptions, which limits their flexibility. We propose a customized emotion ZS-TTS system based on multi-modal prompt. The system disentangles speech into the content, timbre, emotion and prosody, allowing emotion prompts to be provided as text, image or speech. To extract emotion information from different prompts, we propose a multi-modal prompt emotion encoder. Additionally, we introduce an prosody predictor to fit the distribution of prosody and propose an emotion consistency loss to preserve emotion information in the predicted prosody. A diffusion-based acoustic model is employed to generate the target mel-spectrogram. Both objective and subjective experiments demonstrate that our system outperforms existing systems in terms of naturalness and similarity. The samples are available at <a target="_blank" rel="noopener" href="https://mpetts-demo.github.io/mpetts_demo/">https://mpetts-demo.github.io/mpetts_demo/</a>. </p>
<blockquote>
<p>现有的大多数零样本文本转语音（ZS-TTS）系统都是基于单一提示（如参考语音或文本描述）来生成未见过的语音，这限制了它们的灵活性。我们提出了一种基于多模态提示的定制情感ZS-TTS系统。该系统将语音分解为内容、音色、情感和韵律，允许以文本、图像或语音的形式提供情感提示。为了从不同提示中提取情感信息，我们提出了一种多模态提示情感编码器。此外，我们引入了一个韵律预测器以适应韵律分布，并提出了一种情感一致性损失，以在预测的韵律中保留情感信息。采用基于扩散的声学模型来生成目标梅尔频谱图。客观和主观实验均表明，我们的系统在自然度和相似性方面优于现有系统。样本可通过链接<a target="_blank" rel="noopener" href="https://mpetts-demo.github.io/mpetts">https://mpetts-demo.github.io/mpetts</a> 科普小验Demo试用版. 下载查看。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.18453v1">PDF</a> Accepted by InterSpeech</p>
<p><strong>Summary</strong></p>
<p>本文提出一种基于多模态提示的定制情感Zero-Shot Text-To-Speech（ZS-TTS）系统。该系统将语音分解为内容、音质、情感和语调，允许以文本、图像或语音的形式提供情感提示。通过多模态提示情感编码器和情感一致性损失等技术，提高了系统的情感表达能力和自然度。采用扩散声学模型生成目标梅尔频谱图，在客观和主观实验中表现出优异性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ZS-TTS系统通常基于单一提示生成未见语音，限制了其灵活性。</li>
<li>本文提出了一种基于多模态提示的定制情感ZS-TTS系统，可接收多种形式的情感提示（如文本、图像或语音）。</li>
<li>系统将语音分解为内容、音质、情感和语调，提高了情感表达的丰富性。</li>
<li>引入多模态提示情感编码器，用于从不同提示中提取情感信息。</li>
<li>引入语调预测器，以拟合语调分布。</li>
<li>通过情感一致性损失技术，保留预测语调中的情感信息。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.18453">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b4666f75b86335808f947b1012a1c8d6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-13cfbff8c4ae704addf722e6005ca63c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-054686241ca34c4fac9a0d598f54e6f3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-33ef319d09b325e3f34d8353e754d4cb.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="OpenOmni-Advancing-Open-Source-Omnimodal-Large-Language-Models-with-Progressive-Multimodal-Alignment-and-Real-Time-Self-Aware-Emotional-Speech-Synthesis"><a href="#OpenOmni-Advancing-Open-Source-Omnimodal-Large-Language-Models-with-Progressive-Multimodal-Alignment-and-Real-Time-Self-Aware-Emotional-Speech-Synthesis" class="headerlink" title="OpenOmni: Advancing Open-Source Omnimodal Large Language Models with   Progressive Multimodal Alignment and Real-Time Self-Aware Emotional Speech   Synthesis"></a>OpenOmni: Advancing Open-Source Omnimodal Large Language Models with   Progressive Multimodal Alignment and Real-Time Self-Aware Emotional Speech   Synthesis</h2><p><strong>Authors:Run Luo, Ting-En Lin, Haonan Zhang, Yuchuan Wu, Xiong Liu, Min Yang, Yongbin Li, Longze Chen, Jiaming Li, Lei Zhang, Yangyi Chen, Xiaobo Xia, Hamid Alinejad-Rokny, Fei Huang</strong></p>
<p>Recent advancements in omnimodal learning have significantly improved understanding and generation across images, text, and speech, yet these developments remain predominantly confined to proprietary models. The lack of high-quality omnimodal datasets and the challenges of real-time emotional speech synthesis have notably hindered progress in open-source research. To address these limitations, we introduce \name, a two-stage training framework that integrates omnimodal alignment and speech generation to develop a state-of-the-art omnimodal large language model. In the alignment phase, a pre-trained speech model undergoes further training on text-image tasks, enabling (near) zero-shot generalization from vision to speech, outperforming models trained on tri-modal datasets. In the speech generation phase, a lightweight decoder is trained on speech tasks with direct preference optimization, enabling real-time emotional speech synthesis with high fidelity. Experiments show that \name surpasses state-of-the-art models across omnimodal, vision-language, and speech-language benchmarks. It achieves a 4-point absolute improvement on OmniBench over the leading open-source model VITA, despite using 5x fewer training samples and a smaller model size (7B vs. 7x8B). Additionally, \name achieves real-time speech generation with &lt;1s latency at non-autoregressive mode, reducing inference time by 5x compared to autoregressive methods, and improves emotion classification accuracy by 7.7% </p>
<blockquote>
<p>虽然近期多模态学习的进展在图像、文本和语音的理解与生成方面有了显著提升，但这些发展主要局限于专有模型。缺乏高质量的多模态数据集以及实时情感语音合成的挑战，显著阻碍了开源研究的进展。为了解决这些限制，我们引入了名为“XX”的两阶段训练框架，该框架融合了多模态对齐和语音生成，以开发先进的多模态大型语言模型。在对齐阶段，预训练的语音模型在文本-图像任务上接受进一步训练，实现了从视觉到语音的（接近）零样本泛化，超越了那些在三元模态数据集上训练的模型。在语音生成阶段，一个轻量级的解码器在语音任务上进行直接偏好优化训练，能够实现高保真度的实时情感语音合成。实验表明，“XX”在多模态、视觉语言和语音语言基准测试中超越了最先进的模型。尽管使用了较少的训练样本和较小的模型规模（7B vs. 7x8B），但在OmniBench上相对于领先的开源模型VITA仍有4个绝对点的改进。此外，“XX”在非自回归模式下实现了实时语音生成，延迟时间小于1秒，与自回归方法相比减少了5倍的推理时间，并提高了情感分类准确率7. 除此之外，“XX”在非自回归模式下实现了实时语音生成，延迟时间低于每秒一帧（小于1秒），相较于自回归方法减少了五倍的推理时间，并提升了情感分类准确率至百分之七点七。这是一个在多模态融合领域的重要突破，有望为未来的研究和应用开辟新的道路。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.04561v5">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>近期多模态学习领域的进展在图像、文本和语音的理解与生成方面取得了显著成果，但主要局限于专有模型。缺乏高质量的多模态数据集以及实时情感语音合成的挑战限制了开源研究的进展。为解决这些问题，我们推出了名为“名称”的两阶段训练框架，它融合了多模态对齐和语音生成，以开发先进的多模态大型语言模型。“名称”包含对齐阶段和语音生成阶段，通过对预训练语音模型进行文本-图像任务的进一步训练，实现了从视觉到语音的零样本或近似零样本泛化，并在三模态数据集上表现出优越性能。在语音生成阶段，使用轻量级解码器进行语音任务的直接偏好优化训练，实现了高保真度的实时情感语音合成。实验表明，“名称”在多模态、视觉语言和语音语言基准测试中超越了现有技术最先进的模型。相较于领先的开源模型VITA，“名称”在OmniBench上实现了4个点的绝对改进，尽管其使用的训练样本数量更少（5倍），模型规模也更小（7B对7x8B）。此外，“名称”实现了非自回归模式下的实时语音生成，延迟时间小于1秒，与自回归方法相比，推理时间缩短了5倍，情感分类准确率提高了7.7%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>“名称”是一个两阶段的多模态训练框架，用于开发先进的大型语言模型。</li>
<li>对齐阶段通过文本-图像任务进一步训练预训练语音模型，实现视觉到语音的泛化。</li>
<li>在对齐阶段，“名称”表现优于三模态数据集上的模型。</li>
<li>语音生成阶段使用轻量级解码器进行直接偏好优化训练，实现实时情感语音合成。</li>
<li>“名称”在多模态基准测试中表现出卓越性能，超越了现有技术最先进的模型。</li>
<li>“名称”相较于其他模型使用更少的训练样本和更小的模型规模取得了优越的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.04561">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-16c81dab5d028e236ea1e0b0150e022d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-690a783941b225a014971a58607274a4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d06601ddbc306c7eb36f84b79e6c22f3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-595bb46727c5f970cd56721b57c8389d.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="vec2wav-2-0-Advancing-Voice-Conversion-via-Discrete-Token-Vocoders"><a href="#vec2wav-2-0-Advancing-Voice-Conversion-via-Discrete-Token-Vocoders" class="headerlink" title="vec2wav 2.0: Advancing Voice Conversion via Discrete Token Vocoders"></a>vec2wav 2.0: Advancing Voice Conversion via Discrete Token Vocoders</h2><p><strong>Authors:Yiwei Guo, Zhihan Li, Junjie Li, Chenpeng Du, Hankun Wang, Shuai Wang, Xie Chen, Kai Yu</strong></p>
<p>We propose a new speech discrete token vocoder, vec2wav 2.0, which advances voice conversion (VC). We use discrete tokens from speech self-supervised models as the content features of source speech, and treat VC as a prompted vocoding task. To amend the loss of speaker timbre in the content tokens, vec2wav 2.0 utilizes the WavLM features to provide strong timbre-dependent information. A novel adaptive Snake activation function is proposed to better incorporate timbre into the waveform reconstruction process. In this way, vec2wav 2.0 learns to alter the speaker timbre appropriately given different reference prompts. Also, no supervised data is required for vec2wav 2.0 to be effectively trained. Experimental results demonstrate that vec2wav 2.0 outperforms all other baselines to a considerable margin in terms of audio quality and speaker similarity in any-to-any VC. Ablation studies verify the effects made by the proposed techniques. Moreover, vec2wav 2.0 achieves competitive cross-lingual VC even only trained on monolingual corpus. Thus, vec2wav 2.0 shows timbre can potentially be manipulated only by speech token vocoders, pushing the frontiers of VC and speech synthesis. </p>
<blockquote>
<p>我们提出了一种新的语音离散令牌编码器，vec2wav 2.0，它推进了语音转换（VC）的技术。我们使用语音自监督模型的离散令牌作为源语音的内容特征，并将VC视为一个提示性的编码任务。为了弥补内容令牌中说话者音色的损失，vec2wav 2.0利用WavLM特征提供强大的音色相关信息。提出了一种新型的自适应Snake激活函数，以更好地将音色融入波形重建过程。通过这种方式，vec2wav 2.0能够在给定不同的参考提示的情况下，学会适当地改变说话者的音色。此外，不需要监督数据来有效地训练vec2wav 2.0。实验结果表明，在任意到任意的VC中，vec2wav 2.0在音频质量和说话者相似性方面大大优于所有其他基线。消融研究验证了所提出技术的效果。此外，vec2wav 2.0即使在仅使用单语语料库进行训练的情况下，也实现了具有竞争力的跨语言VC。因此，vec2wav 2.0表明，音色可能仅通过语音令牌编码器进行操纵，从而推动了VC和语音合成的边界。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.01995v4">PDF</a> 5 pages, 3 figures, 2 tables. Demo page:   <a target="_blank" rel="noopener" href="https://cantabile-kwok.github.io/vec2wav2/">https://cantabile-kwok.github.io/vec2wav2/</a></p>
<p><strong>Summary</strong></p>
<p>基于离散令牌的新语音离散令牌编解码器vec2wav 2.0的提出推动了语音转换（VC）的进步。该编解码器使用语音自监督模型的离散令牌作为源语音的内容特征，并将VC视为提示编解码任务。为了弥补内容令牌中的说话者音色损失，vec2wav 2.0利用WavLM特征提供强大的音色相关信息。此外，提出了一种新的自适应Snake激活函数，以更好地将音色融入波形重建过程。因此，vec2wav 2.0能够在给定不同参考提示的情况下学习适当地改变说话者的音色。此外，训练vec2wav 2.0无需监督数据。实验结果证明，在任意到任意的VC中，vec2wav 2.0在音频质量和说话者相似性方面都大大优于所有其他基线。消融研究验证了所提出技术的效果。而且，即使在仅使用单语语料库进行训练的情况下，vec2wav 2.0也能实现跨语言VC的竞争性能。因此，vec2wav 2.0显示了音色仅通过语音令牌编解码器进行操作的可能性，推动了VC和语音合成的前沿。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>vec2wav 2.0是一种新的语音离散令牌编解码器，用于推动语音转换的进步。</li>
<li>它使用语音自监督模型的离散令牌作为源语音的内容特征，并将语音转换视为提示编解码任务。</li>
<li>WavLM特征用于提供强大的音色相关信息，以弥补内容令牌中的音色损失。</li>
<li>引入了一种新的自适应Snake激活函数，以改善音色在波形重建过程中的融入。</li>
<li>vec2wav 2.0能够在给定不同参考提示的情况下学习改变说话者的音色，且无需监督数据。</li>
<li>在音频质量和说话者相似性方面，vec2wav 2.0显著优于其他方法，实现了有竞争力的跨语言语音转换性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.01995">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e78a920ce223f857f99cd837f97b6e22.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0dfbfcbfff0492ae7e8ff36449502800.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-73a8dbd51459bb3ece0c9e06d1c5fa46.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c347f8ab3f3ce7bc6cbfd7bb007d8826.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-851ed10c7811817f3a679b63f4c422bb.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-28/TTS/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-28/TTS/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/TTS/">
                                    <span class="chip bg-color">TTS</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-28/Interactive/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-05a2bb620386588e4b17c42f9dcfe475.jpg" class="responsive-img" alt="Interactive">
                        
                        <span class="card-title">Interactive</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Interactive 方向最新论文已更新，请持续关注 Update in 2025-05-28  LeCoDe A Benchmark Dataset for Interactive Legal Consultation Dialogue   Evaluation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-28
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Interactive/" class="post-category">
                                    Interactive
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Interactive/">
                        <span class="chip bg-color">Interactive</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-28/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-008555b808d4437acca293ee2013d487.jpg" class="responsive-img" alt="医学图像">
                        
                        <span class="card-title">医学图像</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学图像 方向最新论文已更新，请持续关注 Update in 2025-05-28  Deep Spectral Prior
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-28
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    医学图像
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">医学图像</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">26384.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
