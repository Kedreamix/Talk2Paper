<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="NeRF">
    <meta name="description" content="NeRF 方向最新论文已更新，请持续关注 Update in 2025-05-28  ErpGS Equirectangular Image Rendering enhanced with 3D Gaussian   Regularization">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>NeRF | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-7fe2c747dd5836bf6aa5a9e3c8c1f253.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">NeRF</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/NeRF/">
                                <span class="chip bg-color">NeRF</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                NeRF
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-05-28
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-06-24
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    6.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    27 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-05-28-更新"><a href="#2025-05-28-更新" class="headerlink" title="2025-05-28 更新"></a>2025-05-28 更新</h1><h2 id="ErpGS-Equirectangular-Image-Rendering-enhanced-with-3D-Gaussian-Regularization"><a href="#ErpGS-Equirectangular-Image-Rendering-enhanced-with-3D-Gaussian-Regularization" class="headerlink" title="ErpGS: Equirectangular Image Rendering enhanced with 3D Gaussian   Regularization"></a>ErpGS: Equirectangular Image Rendering enhanced with 3D Gaussian   Regularization</h2><p><strong>Authors:Shintaro Ito, Natsuki Takama, Koichi Ito, Hwann-Tzong Chen, Takafumi Aoki</strong></p>
<p>The use of multi-view images acquired by a 360-degree camera can reconstruct a 3D space with a wide area. There are 3D reconstruction methods from equirectangular images based on NeRF and 3DGS, as well as Novel View Synthesis (NVS) methods. On the other hand, it is necessary to overcome the large distortion caused by the projection model of a 360-degree camera when equirectangular images are used. In 3DGS-based methods, the large distortion of the 360-degree camera model generates extremely large 3D Gaussians, resulting in poor rendering accuracy. We propose ErpGS, which is Omnidirectional GS based on 3DGS to realize NVS addressing the problems. ErpGS introduce some rendering accuracy improvement techniques: geometric regularization, scale regularization, and distortion-aware weights and a mask to suppress the effects of obstacles in equirectangular images. Through experiments on public datasets, we demonstrate that ErpGS can render novel view images more accurately than conventional methods. </p>
<blockquote>
<p>使用360度相机获取的多视角图像可以重建一个大面积的3D空间。有基于NeRF和3DGS的等距图像3D重建方法，以及新型视图合成（NVS）方法。另一方面，在使用等距图像时，需要克服由360度相机投影模型造成的大畸变。在基于3DGS的方法中，360度相机模型的大畸变会产生非常大的3D高斯，导致渲染精度较低。我们提出了基于3DGS的全方位GS（ErpGS），以实现NVS并解决这些问题。ErpGS引入了一些提高渲染精度的技术：几何正则化、尺度正则化，以及考虑畸变的权重和掩膜，以抑制等距图像中障碍物的影响。在公共数据集上的实验表明，ErpGS可以比传统方法更准确地进行新型视图图像的渲染。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19883v1">PDF</a> Accepted to ICIP2025</p>
<p><strong>Summary</strong></p>
<p>基于NeRF和3DGS的等距图像三维重建方法，利用全景相机采集的多视角图像可以重建出大面积的三维空间。然而，使用全景相机投影模型时会产生较大的失真问题，特别是在基于3DGS的方法中，会导致渲染精度降低。本文提出了一种基于Omnidirectional GS（ErpGS）的解决方案，旨在解决NVS中的这些问题。ErpGS引入了几项提高渲染精度的技术，包括几何正则化、尺度正则化以及考虑失真的权重和掩码，以抑制等距图像中障碍物的影响。实验表明，与传统方法相比，ErpGS可以更准确地渲染新型视图图像。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>使用全景相机采集的多视角图像可以重建大面积的三维空间。</li>
<li>基于NeRF和3DGS的等距图像三维重建方法存在失真问题。</li>
<li>基于Omnidirectional GS的ErpGS方法旨在解决NVS中的失真问题。</li>
<li>ErpGS引入了几项技术提高渲染精度，包括几何正则化、尺度正则化和考虑失真的权重与掩码。</li>
<li>ErpGS能有效抑制等距图像中的障碍物影响。</li>
<li>实验表明，与传统方法相比，ErpGS能更准确地渲染新型视图图像。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19883">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-a14dccdd2945182b03c1c087b7ecdba5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d22623ada854bbfff5c13dc06e2d3a3b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-81f52fb08c82bcbbf1a3aa9fbbb88d87.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-62a2707acda6c834d4b0817b36813e94.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9d96e80edc74fb61cff88b10cb9771ce.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1e258827504ba8a651a3835ad1c1982f.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="FruitNeRF-A-Generalized-Multi-Fruit-Counting-Method-Utilizing-Contrastive-Learning-and-Neural-Radiance-Fields"><a href="#FruitNeRF-A-Generalized-Multi-Fruit-Counting-Method-Utilizing-Contrastive-Learning-and-Neural-Radiance-Fields" class="headerlink" title="FruitNeRF++: A Generalized Multi-Fruit Counting Method Utilizing   Contrastive Learning and Neural Radiance Fields"></a>FruitNeRF++: A Generalized Multi-Fruit Counting Method Utilizing   Contrastive Learning and Neural Radiance Fields</h2><p><strong>Authors:Lukas Meyer, Andrei-Timotei Ardelean, Tim Weyrich, Marc Stamminger</strong></p>
<p>We introduce FruitNeRF++, a novel fruit-counting approach that combines contrastive learning with neural radiance fields to count fruits from unstructured input photographs of orchards. Our work is based on FruitNeRF, which employs a neural semantic field combined with a fruit-specific clustering approach. The requirement for adaptation for each fruit type limits the applicability of the method, and makes it difficult to use in practice. To lift this limitation, we design a shape-agnostic multi-fruit counting framework, that complements the RGB and semantic data with instance masks predicted by a vision foundation model. The masks are used to encode the identity of each fruit as instance embeddings into a neural instance field. By volumetrically sampling the neural fields, we extract a point cloud embedded with the instance features, which can be clustered in a fruit-agnostic manner to obtain the fruit count. We evaluate our approach using a synthetic dataset containing apples, plums, lemons, pears, peaches, and mangoes, as well as a real-world benchmark apple dataset. Our results demonstrate that FruitNeRF++ is easier to control and compares favorably to other state-of-the-art methods. </p>
<blockquote>
<p>我们介绍了FruitNeRF++，这是一种结合对比学习和神经辐射场的新型水果计数方法，可从果园的非结构化输入照片中计算水果数量。我们的工作基于FruitNeRF，它采用神经语义场与水果特定聚类方法相结合。针对每种水果类型都需要进行适应的要求限制了该方法的应用，并使其在实践中难以使用。为了消除这一限制，我们设计了一个与形状无关的多水果计数框架，该框架用由视觉基础模型预测的实例掩模来补充RGB和语义数据。掩模用于将每个水果的身份编码为实例嵌入到神经实例场中。通过体积采样神经场，我们提取嵌入实例特征的点云，可以以与水果无关的方式对这些点云进行聚类，从而获得水果计数。我们使用包含苹果、李子、柠檬、梨、桃子和芒果的合成数据集以及现实世界苹果基准数据集来评估我们的方法。结果表明，FruitNeRF++更易于控制，并与其他最先进的方法相比具有优势。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19863v1">PDF</a> for project website, see <a target="_blank" rel="noopener" href="https://meyerls.github.io/fruit_nerfpp">https://meyerls.github.io/fruit_nerfpp</a></p>
<p><strong>Summary</strong><br>FruitNeRF++是一种结合对比学习和神经辐射场的新型水果计数方法，可从果园的无结构输入照片中计数水果。与基于FruitNeRF的方法相比，该方法设计了一个形状无关的多水果计数框架，通过视觉基础模型预测的实例掩膜来补充RGB和语义数据。通过体积采样神经场，提取嵌入实例特征的点云，以果实无关的方式进行聚类，从而获得果实计数。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FruitNeRF++结合了对比学习和神经辐射场技术，用于从果园照片中计数水果。</li>
<li>该方法基于FruitNeRF，但解决了其针对每种果实类型需要适应的问题。</li>
<li>通过设计形状无关的多水果计数框架，使用实例掩膜编码每种水果的身份为实例嵌入，进入神经实例场。</li>
<li>使用体积采样神经场来提取嵌入实例特征的点云，以进行果实无关的聚类。</li>
<li>方法在合成数据集（包含多种水果）和真实世界苹果数据集上进行了评估。</li>
<li>FruitNeRF++易于控制，并与其他最先进的方法相比表现良好。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19863">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9dba5f3fc4dc19dd8c3574453e2e300b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-53f4632ebe2dd7b43fc1c0eddafe271d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ac639f3d419be45766778c1239568e3e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d5df50da8bb2e26d6a5836692e9d3fde.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f3694529c8c5907219c42f6d91af7549.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-253b469c772ea69b9e88df37cfe04fd0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="GoLF-NRT-Integrating-Global-Context-and-Local-Geometry-for-Few-Shot-View-Synthesis"><a href="#GoLF-NRT-Integrating-Global-Context-and-Local-Geometry-for-Few-Shot-View-Synthesis" class="headerlink" title="GoLF-NRT: Integrating Global Context and Local Geometry for Few-Shot   View Synthesis"></a>GoLF-NRT: Integrating Global Context and Local Geometry for Few-Shot   View Synthesis</h2><p><strong>Authors:You Wang, Li Fang, Hao Zhu, Fei Hu, Long Ye, Zhan Ma</strong></p>
<p>Neural Radiance Fields (NeRF) have transformed novel view synthesis by modeling scene-specific volumetric representations directly from images. While generalizable NeRF models can generate novel views across unknown scenes by learning latent ray representations, their performance heavily depends on a large number of multi-view observations. However, with limited input views, these methods experience significant degradation in rendering quality. To address this limitation, we propose GoLF-NRT: a Global and Local feature Fusion-based Neural Rendering Transformer. GoLF-NRT enhances generalizable neural rendering from few input views by leveraging a 3D transformer with efficient sparse attention to capture global scene context. In parallel, it integrates local geometric features extracted along the epipolar line, enabling high-quality scene reconstruction from as few as 1 to 3 input views. Furthermore, we introduce an adaptive sampling strategy based on attention weights and kernel regression, improving the accuracy of transformer-based neural rendering. Extensive experiments on public datasets show that GoLF-NRT achieves state-of-the-art performance across varying numbers of input views, highlighting the effectiveness and superiority of our approach. Code is available at <a target="_blank" rel="noopener" href="https://github.com/KLMAV-CUC/GoLF-NRT">https://github.com/KLMAV-CUC/GoLF-NRT</a>. </p>
<blockquote>
<p>神经辐射场（NeRF）通过直接从图像建模场景特定的体积表示，从而实现了新颖视图合成的变革。虽然通用NeRF模型可以通过学习潜在射线表示来生成未知场景的新颖视图，但其性能严重依赖于大量多视图观察。然而，在输入视图有限的情况下，这些方法在渲染质量方面会出现显著下降。为了解决这一局限性，我们提出了基于全局和局部特征融合的神经渲染变压器GoLF-NRT。GoLF-NRT通过利用带有高效稀疏注意力的3D变压器，增强从少数输入视图进行通用神经渲染的能力，以捕获场景的全局上下文。同时，它结合了沿极线提取的局部几何特征，从而能够从仅1到3个输入视图实现高质量的场景重建。此外，我们引入了一种基于注意力权重和核回归的自适应采样策略，提高了基于变压器的神经渲染的准确性。在公共数据集上的广泛实验表明，GoLF-NRT在不同数量的输入视图上实现了最先进的性能，凸显了我们方法的有效性和优越性。代码可在<a target="_blank" rel="noopener" href="https://github.com/KLMAV-CUC/GoLF-NRT%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/KLMAV-CUC/GoLF-NRT找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19813v1">PDF</a> CVPR 2025</p>
<p><strong>总结</strong></p>
<p>NeRF技术已经实现了从图像直接建模场景体积表示的新视角合成。尽管通用NeRF模型可以通过学习潜在射线表示来生成未知场景的新视角，但其性能高度依赖于大量的多视角观察。然而，在输入视角有限的情况下，这些方法在渲染质量上会出现显著下降。为解决这一问题，提出了基于全局和局部特征融合的神经渲染转换器GoLF-NRT。GoLF-NRT通过利用具有高效稀疏注意力的3D变压器，增强了从少量输入视角进行通用神经渲染的能力，捕捉全局场景上下文。同时，它融合了沿极线提取的局部几何特征，使得即使仅从1到3个输入视角也能实现高质量的场景重建。此外，基于注意力权重和核回归的自适应采样策略，提高了基于变压器的神经渲染的准确性。在公开数据集上的广泛实验表明，GoLF-NRT在多种输入视角数量下实现了最先进的性能，凸显了该方法的有效性和优越性。相关代码可在“<a target="_blank" rel="noopener" href="https://github.com/KLMAV-CUC/GoLF-NRT">链接</a>”中找到。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>NeRF实现了从图像直接建模场景体积表示的新视角合成。</li>
<li>通用NeRF模型依赖于多视角观察，在输入视角有限时性能下降。</li>
<li>GoLF-NRT基于全局和局部特征融合，增强了从少量输入视角进行神经渲染的能力。</li>
<li>GoLF-NRT利用3D变压器捕捉全局场景上下文，并结合局部几何特征实现高质量场景重建。</li>
<li>GoLF-NRT引入自适应采样策略，提高了基于神经渲染的准确性。</li>
<li>GoLF-NRT在多种公开数据集上实现了最先进的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19813">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-b51bcce7a609a5dac67c9bc8f5280687.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2d99bb1f3f43376d88e9fba45acd117e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-64ff787be63c845bf786735e4f9869f6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-85b686bcdecc847ef876fc1ebb76ab6d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4fd79e178e2f4773a3c5d8e22fb296c1.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Depth-Guided-Bundle-Sampling-for-Efficient-Generalizable-Neural-Radiance-Field-Reconstruction"><a href="#Depth-Guided-Bundle-Sampling-for-Efficient-Generalizable-Neural-Radiance-Field-Reconstruction" class="headerlink" title="Depth-Guided Bundle Sampling for Efficient Generalizable Neural Radiance   Field Reconstruction"></a>Depth-Guided Bundle Sampling for Efficient Generalizable Neural Radiance   Field Reconstruction</h2><p><strong>Authors:Li Fang, Hao Zhu, Longlong Chen, Fei Hu, Long Ye, Zhan Ma</strong></p>
<p>Recent advancements in generalizable novel view synthesis have achieved impressive quality through interpolation between nearby views. However, rendering high-resolution images remains computationally intensive due to the need for dense sampling of all rays. Recognizing that natural scenes are typically piecewise smooth and sampling all rays is often redundant, we propose a novel depth-guided bundle sampling strategy to accelerate rendering. By grouping adjacent rays into a bundle and sampling them collectively, a shared representation is generated for decoding all rays within the bundle. To further optimize efficiency, our adaptive sampling strategy dynamically allocates samples based on depth confidence, concentrating more samples in complex regions while reducing them in smoother areas. When applied to ENeRF, our method achieves up to a 1.27 dB PSNR improvement and a 47% increase in FPS on the DTU dataset. Extensive experiments on synthetic and real-world datasets demonstrate state-of-the-art rendering quality and up to 2x faster rendering compared to existing generalizable methods. Code is available at <a target="_blank" rel="noopener" href="https://github.com/KLMAV-CUC/GDB-NeRF">https://github.com/KLMAV-CUC/GDB-NeRF</a>. </p>
<blockquote>
<p>最近，在可泛化的新型视角合成方面取得了进展，通过邻近视角的插值达到了令人印象深刻的品质。然而，由于需要对所有光线进行密集采样，渲染高分辨率图像在计算上仍然很密集。我们认识到自然场景通常是分段平滑的，并且采样所有光线通常是冗余的，因此我们提出了一种加速渲染的新型深度引导捆绑采样策略。通过将相邻的光线分组为束并集体采样它们，为束内的所有光线生成共享表示来进行解码。为了进一步优化效率，我们的自适应采样策略根据深度置信动态分配样本，在复杂区域集中更多样本，同时在平滑区域减少样本。当应用于ENeRF时，我们的方法在DTU数据集上实现了高达1.27 dB的PSNR改进和47%的FPS提升。在合成和真实世界数据集上的大量实验证明了其最先进的渲染质量和与现有可泛化方法相比最多快2倍的渲染速度。代码可在<a target="_blank" rel="noopener" href="https://github.com/KLMAV-CUC/GDB-NeRF%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/KLMAV-CUC/GDB-NeRF找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19793v1">PDF</a> CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>近期在通用化新型视角合成技术上的进展已取得了令人印象深刻的插值效果。然而，由于需要对所有光线进行密集采样，渲染高分辨率图像的计算仍然很密集。考虑到自然场景通常是分段平滑的，并且对所有光线进行采样往往是冗余的，我们提出了一种新型的深度引导捆绑采样策略来加速渲染。通过将相邻光线分组为捆绑包并集体采样，为捆绑包内的所有光线生成共享表示。为了进一步优化效率，我们的自适应采样策略根据深度置信动态分配样本，在复杂区域集中更多样本，同时在平滑区域减少样本。应用于ENeRF时，我们的方法在DTU数据集上实现了高达1.27 dB的PSNR改进和47%的FPS提升。在合成和真实世界数据集上的广泛实验证明了其最先进的渲染质量和与现有通用方法相比高达2倍的快速渲染能力。代码已发布在[GitHub链接]。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出了深度引导捆绑采样策略来加速渲染过程。</li>
<li>通过将相邻光线分组为捆绑并集体采样，减少了计算复杂性。</li>
<li>自适应采样策略根据场景复杂性动态分配样本。</li>
<li>方法在DTU数据集上实现了PSNR和FPS的双重提升。</li>
<li>在合成和真实世界数据集上的实验证明了其优越性能。</li>
<li>与现有方法相比，该方法提供了高达2倍的快速渲染能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19793">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c358fbc58017b7de7be93c827b33796e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5c504f990f14bf8b05133337ef8780ff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-02d42bac5a09caadd70c20484c4f33af.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="ADD-SLAM-Adaptive-Dynamic-Dense-SLAM-with-Gaussian-Splatting"><a href="#ADD-SLAM-Adaptive-Dynamic-Dense-SLAM-with-Gaussian-Splatting" class="headerlink" title="ADD-SLAM: Adaptive Dynamic Dense SLAM with Gaussian Splatting"></a>ADD-SLAM: Adaptive Dynamic Dense SLAM with Gaussian Splatting</h2><p><strong>Authors:Wenhua Wu, Chenpeng Su, Siting Zhu, Tianchen Deng, Zhe Liu, Hesheng Wang</strong></p>
<p>Recent advancements in Neural Radiance Fields (NeRF) and 3D Gaussian-based Simultaneous Localization and Mapping (SLAM) methods have demonstrated exceptional localization precision and remarkable dense mapping performance. However, dynamic objects introduce critical challenges by disrupting scene consistency, leading to tracking drift and mapping artifacts. Existing methods that employ semantic segmentation or object detection for dynamic identification and filtering typically rely on predefined categorical priors, while discarding dynamic scene information crucial for robotic applications such as dynamic obstacle avoidance and environmental interaction. To overcome these challenges, we propose ADD-SLAM: an Adaptive Dynamic Dense SLAM framework based on Gaussian splitting. We design an adaptive dynamic identification mechanism grounded in scene consistency analysis, comparing geometric and textural discrepancies between real-time observations and historical maps. Ours requires no predefined semantic category priors and adaptively discovers scene dynamics. Precise dynamic object recognition effectively mitigates interference from moving targets during localization. Furthermore, we propose a dynamic-static separation mapping strategy that constructs a temporal Gaussian model to achieve online incremental dynamic modeling. Experiments conducted on multiple dynamic datasets demonstrate our method’s flexible and accurate dynamic segmentation capabilities, along with state-of-the-art performance in both localization and mapping. </p>
<blockquote>
<p>最近，神经辐射场（NeRF）和基于3D高斯的同时定位与地图构建（SLAM）方法的进展，已经展现了出色的定位精度和显著的密集映射性能。然而，动态物体通过破坏场景一致性，引入了关键挑战，导致跟踪漂移和映射伪影。现有方法采用语义分割或目标检测进行动态识别和过滤，通常依赖于预设的类别先验知识，同时丢弃了对于机器人应用至关重要的动态场景信息，如动态避障和环境交互。为了克服这些挑战，我们提出了ADD-SLAM：一种基于高斯分裂的自适应动态密集SLAM框架。我们设计了一种基于场景一致性分析的自适应动态识别机制，通过比较实时观察与历史地图之间的几何和纹理差异。我们的方法不需要预设的语义类别先验知识，并能自适应地发现场景动态。精确的动态目标识别有效地减轻了移动目标在定位过程中的干扰。此外，我们提出了一种动静分离映射策略，建立了一个临时高斯模型，以实现在线增量动态建模。在多个动态数据集上进行的实验证明了我们方法在动态分割方面的灵活性和准确性，以及在定位和映射方面的卓越性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19420v1">PDF</a> </p>
<p><strong>Summary</strong><br>基于NeRF和3D高斯SLAM方法的最新进展，动态物体对场景一致性的干扰造成了定位漂移和映射伪影的问题。为此，本文提出了基于高斯分裂的ADD-SLAM自适应动态密集SLAM框架，设计了一种基于场景一致性分析的动态识别机制，并采用了动态静态分离映射策略，实现了在线增量动态建模。实验证明，该方法具有灵活准确的动态分割能力和先进的定位和映射性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>NeRF和3D高斯SLAM的最新进展在定位和密集映射方面表现出色。</li>
<li>动态物体干扰场景一致性，导致跟踪漂移和映射伪影。</li>
<li>现有方法使用语义分割或对象检测来识别动态物体，依赖于预定义的类别先验信息。</li>
<li>ADD-SLAM框架通过自适应动态识别机制克服这些挑战，无需预定义的语义类别先验信息。</li>
<li>动态识别机制基于场景一致性分析，比较实时观察和历史地图之间的几何和纹理差异。</li>
<li>动态物体识别的精确性有效减轻了移动目标对定位的影响。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19420">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-f2ae8aa3b98840a8b033c2a286e6ef91.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6366afc04112ad09e87a3f3ae417b61e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1a5ec0ac6360978deb11cac5fcedf657.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Triangle-Splatting-for-Real-Time-Radiance-Field-Rendering"><a href="#Triangle-Splatting-for-Real-Time-Radiance-Field-Rendering" class="headerlink" title="Triangle Splatting for Real-Time Radiance Field Rendering"></a>Triangle Splatting for Real-Time Radiance Field Rendering</h2><p><strong>Authors:Jan Held, Renaud Vandeghen, Adrien Deliege, Abdullah Hamdi, Silvio Giancola, Anthony Cioppa, Andrea Vedaldi, Bernard Ghanem, Andrea Tagliasacchi, Marc Van Droogenbroeck</strong></p>
<p>The field of computer graphics was revolutionized by models such as Neural Radiance Fields and 3D Gaussian Splatting, displacing triangles as the dominant representation for photogrammetry. In this paper, we argue for a triangle comeback. We develop a differentiable renderer that directly optimizes triangles via end-to-end gradients. We achieve this by rendering each triangle as differentiable splats, combining the efficiency of triangles with the adaptive density of representations based on independent primitives. Compared to popular 2D and 3D Gaussian Splatting methods, our approach achieves higher visual fidelity, faster convergence, and increased rendering throughput. On the Mip-NeRF360 dataset, our method outperforms concurrent non-volumetric primitives in visual fidelity and achieves higher perceptual quality than the state-of-the-art Zip-NeRF on indoor scenes. Triangles are simple, compatible with standard graphics stacks and GPU hardware, and highly efficient: for the \textit{Garden} scene, we achieve over 2,400 FPS at 1280x720 resolution using an off-the-shelf mesh renderer. These results highlight the efficiency and effectiveness of triangle-based representations for high-quality novel view synthesis. Triangles bring us closer to mesh-based optimization by combining classical computer graphics with modern differentiable rendering frameworks. The project page is <a target="_blank" rel="noopener" href="https://trianglesplatting.github.io/">https://trianglesplatting.github.io/</a> </p>
<blockquote>
<p>神经网络辐射场和三维高斯拼贴等技术彻底改变了计算机图形学领域，取代了三角测量作为摄影测量的主要表现形式。在本文中，我们主张三角测量的复兴。我们开发了一种可微分渲染器，该渲染器通过端到端梯度直接优化三角测量。我们通过将每个三角形渲染为可微分的拼贴来实现这一点，结合了三角形的高效性和基于独立原始数据的自适应密度表示。与流行的二维和三维高斯拼贴方法相比，我们的方法实现了更高的视觉保真度、更快的收敛速度和增加的渲染吞吐量。在Mip-NeRF360数据集上，我们的方法在视觉保真度上超越了并发非体积原始数据，并在室内场景上达到了最先进的Zip-NeRF的感知质量。三角形简单、兼容标准图形堆栈和GPU硬件，并且高效：对于“花园”场景，我们使用现成的网格渲染器在1280x720分辨率下实现了超过2400帧每秒。这些结果突出了基于三角形的表示法在高质量新颖视图合成中的效率和有效性。通过结合经典计算机图形和现代可微分渲染框架，三角形使我们离基于网格的优化更近了一步。项目页面为<a target="_blank" rel="noopener" href="https://trianglesplatting.github.io/%E3%80%82">https://trianglesplatting.github.io/。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19175v1">PDF</a> 18 pages, 13 figures, 10 tables</p>
<p><strong>Summary</strong></p>
<p>神经网络辐射场（NeRF）等模型在计算机图形学领域引发了革命性的变革，本文通过开发一种可微分渲染器让三角形成为主流的光测学表示方式。通过直接将三角形优化为可微分的splat，结合三角形的效率和基于独立原始数据的自适应密度表示，我们的方法在视觉保真度、收敛速度和渲染吞吐量方面取得了显著的成果。三角形简单、兼容标准图形堆栈和GPU硬件，并且高效。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>本文主张三角形的回归，开发了一种可微分渲染器，直接优化三角形。</li>
<li>通过将三角形渲染为可微分的splat，实现了三角形和自适应密度表示的融合。</li>
<li>与流行的2D和3D高斯Splatting方法相比，该方法在视觉保真度、收敛速度和渲染吞吐量方面表现更优秀。</li>
<li>在Mip-NeRF360数据集上，该方法在视觉保真度上超越了并发非体积原始方法，并在室内场景上实现了比Zip-NeRF更高的感知质量。</li>
<li>三角形表示方法具有高效性，对于“花园”场景，使用现成的网格渲染器在1280x720分辨率下实现了超过2400 FPS的帧率。</li>
<li>三角形结合经典计算机图形和现代化可微分渲染框架，带来网格优化的可能性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19175">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-375612dc1d2a1d9adebe4ba90e5c8a47.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-990e5318d43a467cb70917a7b9a65403.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8d54aac9e7e70471fcf4f459c92c7897.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7fe2c747dd5836bf6aa5a9e3c8c1f253.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="STAF-Sinusoidal-Trainable-Activation-Functions-for-Implicit-Neural-Representation"><a href="#STAF-Sinusoidal-Trainable-Activation-Functions-for-Implicit-Neural-Representation" class="headerlink" title="STAF: Sinusoidal Trainable Activation Functions for Implicit Neural   Representation"></a>STAF: Sinusoidal Trainable Activation Functions for Implicit Neural   Representation</h2><p><strong>Authors:Alireza Morsali, MohammadJavad Vaez, Mohammadhossein Soltani, Amirhossein Kazerouni, Babak Taati, Morteza Mohammad-Noori</strong></p>
<p>Implicit Neural Representations (INRs) have emerged as a powerful framework for modeling continuous signals. The spectral bias of ReLU-based networks is a well-established limitation, restricting their ability to capture fine-grained details in target signals. While previous works have attempted to mitigate this issue through frequency-based encodings or architectural modifications, these approaches often introduce additional complexity and do not fully address the underlying challenge of learning high-frequency components efficiently. We introduce Sinusoidal Trainable Activation Functions (STAF), designed to directly tackle this limitation by enabling networks to adaptively learn and represent complex signals with higher precision and efficiency. STAF inherently modulates its frequency components, allowing for self-adaptive spectral learning. This capability significantly improves convergence speed and expressivity, making STAF highly effective for both signal representations and inverse problems. Through extensive evaluations across a range of tasks, including signal representation (shape, image, audio) and inverse problems (super-resolution, denoising), as well as neural radiance fields (NeRF), we demonstrate that STAF consistently outperforms state-of-the-art methods in accuracy and reconstruction fidelity. These results establish STAF as a robust solution to spectral bias and the capacity–convergence tradeoff, with broad applicability in computer vision and graphics. Our codebase is publicly accessible at <a target="_blank" rel="noopener" href="https://github.com/AlirezaMorsali/STAF">https://github.com/AlirezaMorsali/STAF</a>. </p>
<blockquote>
<p>隐式神经网络表示（INR）已经成为连续信号建模的强大框架。ReLU网络的谱偏置是一个公认的局限性，限制了它们捕捉目标信号中的精细细节的能力。虽然以前的工作试图通过基于频率的编码或架构修改来解决这个问题，但这些方法通常引入了额外的复杂性，并没有完全解决高效学习高频成分的基本挑战。我们引入了可训练正弦激活函数（STAF），旨在通过使网络能够自适应地学习和表示具有更高精度和效率的复杂信号来直接解决这一局限性。STAF固有地调制其频率成分，从而实现自适应谱学习。这种能力显著提高了收敛速度和表现力，使STAF在信号表示和反向问题中都高度有效。通过广泛的任务评估，包括信号表示（形状、图像、音频）和反向问题（超分辨率、去噪），以及神经辐射场（NeRF），我们证明了STAF在精度和重建保真度方面始终优于最先进的方法。这些结果证明了STAF是解决谱偏置和容量收敛权衡的稳健解决方案，在计算机视觉和图形中具有广泛的应用性。我们的代码库可在<a target="_blank" rel="noopener" href="https://github.com/AlirezaMorsali/STAF%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/AlirezaMorsali/STAF公开访问。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.00869v2">PDF</a> </p>
<p><strong>Summary</strong>：</p>
<p>提出一种新的可训练激活函数——正弦式可训练激活函数（STAF），旨在解决ReLU基网络在捕捉精细信号时的频谱偏差问题。STAF通过自适应学习并代表复杂信号，提高了网络的精度和效率，特别适用于信号表示和逆问题。在多种任务上，包括信号表示（形状、图像、音频）和逆问题（超分辨率、去噪）以及神经辐射场（NeRF），STAF表现出卓越的性能。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>介绍了正弦式可训练激活函数（STAF），旨在解决ReLU基网络捕捉精细信号时的频谱偏差问题。</li>
<li>STAF能够自适应学习和表示复杂信号，提高网络的精度和效率。</li>
<li>STAF具有内在的频率组件调制能力，可实现自适应谱学习。</li>
<li>STAF在信号表示和逆问题中表现出显著的优势，如形状、图像、音频等任务上的表现优秀。</li>
<li>在神经辐射场（NeRF）相关任务中，STAF也表现出卓越性能。</li>
<li>广泛的实验评估证明STAF在准确性和重建保真度方面优于现有方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.00869">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-79aaec591907ac07936bb206e42acdf5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2f3dfd3daef7d8817d11950a01defaf2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b976948bb5688b53cfced8ba7970a7fe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2b5e113dbad5483bb0c160fa9d3db9d0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-28/NeRF/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-28/NeRF/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/NeRF/">
                                    <span class="chip bg-color">NeRF</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-28/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-5206070dc91963da2bc84d1a570f2ada.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-05-28  Multimodal LLM-Guided Semantic Correction in Text-to-Image Diffusion
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-28
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-28/3DGS/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-d12751dc9a4979d5ce61f49c820378c1.jpg" class="responsive-img" alt="3DGS">
                        
                        <span class="card-title">3DGS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            3DGS 方向最新论文已更新，请持续关注 Update in 2025-05-28  ErpGS Equirectangular Image Rendering enhanced with 3D Gaussian   Regularization
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-28
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/3DGS/" class="post-category">
                                    3DGS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/3DGS/">
                        <span class="chip bg-color">3DGS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">29885.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
