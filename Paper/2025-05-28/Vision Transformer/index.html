<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Vision Transformer">
    <meta name="description" content="Vision Transformer æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-28  The Missing Point in Vision Transformers for Universal Image   Segmentation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Vision Transformer | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-9d4a2774ce9b95efa34198af13a80a6b.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Vision Transformer</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Vision-Transformer/">
                                <span class="chip bg-color">Vision Transformer</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                Vision Transformer
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-28
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-24
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    7.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    30 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-28-æ›´æ–°"><a href="#2025-05-28-æ›´æ–°" class="headerlink" title="2025-05-28 æ›´æ–°"></a>2025-05-28 æ›´æ–°</h1><h2 id="The-Missing-Point-in-Vision-Transformers-for-Universal-Image-Segmentation"><a href="#The-Missing-Point-in-Vision-Transformers-for-Universal-Image-Segmentation" class="headerlink" title="The Missing Point in Vision Transformers for Universal Image   Segmentation"></a>The Missing Point in Vision Transformers for Universal Image   Segmentation</h2><p><strong>Authors:Sajjad Shahabodini, Mobina Mansoori, Farnoush Bayatmakou, Jamshid Abouei, Konstantinos N. Plataniotis, Arash Mohammadi</strong></p>
<p>Image segmentation remains a challenging task in computer vision, demanding robust mask generation and precise classification. Recent mask-based approaches yield high-quality masks by capturing global context. However, accurately classifying these masks, especially in the presence of ambiguous boundaries and imbalanced class distributions, remains an open challenge. In this work, we introduce ViT-P, a novel two-stage segmentation framework that decouples mask generation from classification. The first stage employs a proposal generator to produce class-agnostic mask proposals, while the second stage utilizes a point-based classification model built on the Vision Transformer (ViT) to refine predictions by focusing on mask central points. ViT-P serves as a pre-training-free adapter, allowing the integration of various pre-trained vision transformers without modifying their architecture, ensuring adaptability to dense prediction tasks. Furthermore, we demonstrate that coarse and bounding box annotations can effectively enhance classification without requiring additional training on fine annotation datasets, reducing annotation costs while maintaining strong performance. Extensive experiments across COCO, ADE20K, and Cityscapes datasets validate the effectiveness of ViT-P, achieving state-of-the-art results with 54.0 PQ on ADE20K panoptic segmentation, 87.4 mIoU on Cityscapes semantic segmentation, and 63.6 mIoU on ADE20K semantic segmentation. The code and pretrained models are available at: <a target="_blank" rel="noopener" href="https://github.com/sajjad-sh33/ViT-P%7D%7Bhttps://github.com/sajjad-sh33/ViT-P">https://github.com/sajjad-sh33/ViT-P}{https://github.com/sajjad-sh33/ViT-P</a>. </p>
<blockquote>
<p>å›¾åƒåˆ†å‰²ä»æ˜¯è®¡ç®—æœºè§†è§‰ä¸­çš„ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œéœ€è¦ç”Ÿæˆç¨³å¥çš„æ©è†œå’Œç²¾ç¡®çš„åˆ†ç±»ã€‚æœ€è¿‘çš„åŸºäºæ©è†œçš„æ–¹æ³•é€šè¿‡æ•æ‰å…¨å±€ä¸Šä¸‹æ–‡æ¥ç”Ÿæˆé«˜è´¨é‡æ©è†œã€‚ç„¶è€Œï¼Œç‰¹åˆ«æ˜¯åœ¨å­˜åœ¨æ¨¡ç³Šè¾¹ç•Œå’Œç±»åˆ«åˆ†å¸ƒä¸å¹³è¡¡çš„æƒ…å†µä¸‹ï¼Œå¯¹è¿™äº›æ©è†œè¿›è¡Œå‡†ç¡®åˆ†ç±»ä»ç„¶æ˜¯ä¸€ä¸ªå¼€æ”¾æ€§é—®é¢˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ViT-Pï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„ä¸¤é˜¶æ®µåˆ†å‰²æ¡†æ¶ï¼Œå®ƒå°†æ©è†œç”Ÿæˆä¸åˆ†ç±»è§£è€¦ã€‚ç¬¬ä¸€é˜¶æ®µé‡‡ç”¨ææ¡ˆç”Ÿæˆå™¨ç”Ÿæˆç±»åˆ«æ— å…³çš„æ©è†œææ¡ˆï¼Œè€Œç¬¬äºŒé˜¶æ®µåˆ™åŸºäºè§†è§‰å˜å‹å™¨ï¼ˆViTï¼‰çš„ç‚¹åˆ†ç±»æ¨¡å‹å¯¹é¢„æµ‹è¿›è¡Œç»†åŒ–ï¼Œä¸“æ³¨äºæ©è†œä¸­å¿ƒç‚¹ã€‚ViT-Pä½œä¸ºä¸€ç§æ— éœ€é¢„è®­ç»ƒçš„é€‚é…å™¨ï¼Œå¯ä»¥é›†æˆå„ç§é¢„è®­ç»ƒçš„è§†è§‰å˜å‹å™¨è€Œæ— éœ€ä¿®æ”¹å…¶æ¶æ„ï¼Œç¡®ä¿é€‚åº”å¯†é›†é¢„æµ‹ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¯æ˜ç²—æ ‡æ³¨å’Œè¾¹ç•Œæ¡†æ ‡æ³¨å¯ä»¥æœ‰æ•ˆåœ°å¢å¼ºåˆ†ç±»ï¼Œè€Œæ— éœ€åœ¨ç²¾ç»†æ ‡æ³¨æ•°æ®é›†ä¸Šè¿›è¡Œé¢å¤–è®­ç»ƒï¼Œä»è€Œåœ¨é™ä½æ ‡æ³¨æˆæœ¬çš„åŒæ—¶ä¿æŒå¼ºåŠ²æ€§èƒ½ã€‚åœ¨COCOã€ADE20Kå’ŒCityscapesæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒéªŒè¯äº†ViT-Pçš„æœ‰æ•ˆæ€§ï¼Œåœ¨ADE20Kå…¨æ™¯åˆ†å‰²ä¸Šå®ç°äº†54.0 PQçš„å…ˆè¿›ç»“æœï¼ŒCityscapesè¯­ä¹‰åˆ†å‰²ä¸Šè¾¾åˆ°äº†87.4 mIoUï¼ŒADE20Kè¯­ä¹‰åˆ†å‰²ä¸Šè¾¾åˆ°äº†63.6 mIoUã€‚ä»£ç å’Œé¢„å…ˆè®­ç»ƒçš„æ¨¡å‹å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/sajjad-sh33/ViT-P%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/sajjad-sh33/ViT-Pä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19795v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºViT-Pçš„æ–°å‹ä¸¤é˜¶æ®µå›¾åƒåˆ†å‰²æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†æ©è†œç”Ÿæˆä¸åˆ†ç±»é˜¶æ®µè§£è€¦ã€‚ç¬¬ä¸€é˜¶æ®µç”Ÿæˆç±»åˆ«æ— å…³çš„æ©è†œææ¡ˆï¼Œç¬¬äºŒé˜¶æ®µåˆ©ç”¨åŸºäºè§†è§‰Transformerï¼ˆViTï¼‰çš„ç‚¹åˆ†ç±»æ¨¡å‹è¿›è¡Œé¢„æµ‹ç²¾ç»†åŒ–ã€‚ViT-På¯ä½œä¸ºæ— éœ€é¢„è®­ç»ƒçš„é€‚é…å™¨ï¼Œå¯æ•´åˆå„ç§é¢„è®­ç»ƒè§†è§‰Transformerï¼Œç¡®ä¿é€‚åº”å¯†é›†é¢„æµ‹ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¯æ˜ç²—ç•¥å’Œè¾¹ç•Œæ¡†æ³¨é‡Šèƒ½æœ‰æ•ˆæé«˜åˆ†ç±»æ€§èƒ½ï¼ŒåŒæ—¶é™ä½ç²¾ç»†æ ‡æ³¨æ•°æ®é›†çš„è®­ç»ƒæˆæœ¬ã€‚åœ¨COCOã€ADE20Kå’ŒCityscapesæ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯äº†ViT-Pçš„æœ‰æ•ˆæ€§ï¼Œå–å¾—äº†å…ˆè¿›çš„ç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ViT-Pæ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µçš„å›¾åƒåˆ†å‰²æ¡†æ¶ï¼Œå°†æ©è†œç”Ÿæˆä¸åˆ†ç±»é˜¶æ®µåˆ†å¼€ï¼Œä»¥æé«˜æ€§èƒ½å’Œå‡†ç¡®æ€§ã€‚</li>
<li>ç¬¬ä¸€é˜¶æ®µç”Ÿæˆç±»åˆ«æ— å…³çš„æ©è†œææ¡ˆï¼Œä¸ºåç»­çš„åˆ†ç±»é˜¶æ®µæä¾›åŸºç¡€ã€‚</li>
<li>ç¬¬äºŒé˜¶æ®µåˆ©ç”¨åŸºäºè§†è§‰Transformerï¼ˆViTï¼‰çš„ç‚¹åˆ†ç±»æ¨¡å‹è¿›è¡Œç²¾ç»†åŒ–é¢„æµ‹ã€‚</li>
<li>ViT-På¯ä½œä¸ºæ— éœ€é¢„è®­ç»ƒçš„é€‚é…å™¨ï¼Œæ•´åˆå„ç§é¢„è®­ç»ƒè§†è§‰Transformerã€‚</li>
<li>ç²—ç•¥å’Œè¾¹ç•Œæ¡†æ³¨é‡Šèƒ½æé«˜åˆ†ç±»æ€§èƒ½ï¼ŒåŒæ—¶é™ä½æ ‡æ³¨æˆæœ¬ã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯äº†ViT-Pçš„æœ‰æ•ˆæ€§ï¼Œå–å¾—äº†æ˜¾è‘—æˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19795">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-52d7272cd63f408b69c9f82ecb5bdadc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b398f954148eaef3d0c3aa00e369ed0d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-912a3191a01709501f6829aa2dcc1c17.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="DiSa-Directional-Saliency-Aware-Prompt-Learning-for-Generalizable-Vision-Language-Models"><a href="#DiSa-Directional-Saliency-Aware-Prompt-Learning-for-Generalizable-Vision-Language-Models" class="headerlink" title="DiSa: Directional Saliency-Aware Prompt Learning for Generalizable   Vision-Language Models"></a>DiSa: Directional Saliency-Aware Prompt Learning for Generalizable   Vision-Language Models</h2><p><strong>Authors:Niloufar Alipour Talemi, Hossein Kashiani, Hossein R. Nowdeh, Fatemeh Afghah</strong></p>
<p>Prompt learning has emerged as a powerful paradigm for adapting vision-language models such as CLIP to downstream tasks. However, existing methods often overfit to seen data, leading to significant performance degradation when generalizing to novel classes or unseen domains. To address this limitation, we propose DiSa, a Directional Saliency-Aware Prompt Learning framework that integrates two complementary regularization strategies to enhance generalization. First, our Cross-Interactive Regularization (CIR) fosters cross-modal alignment by enabling cooperative learning between prompted and frozen encoders. Within CIR, a saliency-aware masking strategy guides the image encoder to prioritize semantically critical image regions, reducing reliance on less informative patches. Second, we introduce a directional regularization strategy that aligns visual embeddings with class-wise prototype features in a directional manner to prioritize consistency in feature orientation over strict proximity. This approach ensures robust generalization by leveraging stable prototype directions derived from class-mean statistics. Extensive evaluations on 11 diverse image classification benchmarks demonstrate that DiSa consistently outperforms state-of-the-art prompt learning methods across various settings, including base-to-novel generalization, cross-dataset transfer, domain generalization, and few-shot learning. </p>
<blockquote>
<p>æç¤ºå­¦ä¹ å·²ç»æˆä¸ºä¸€ç§å¼ºå¤§çš„èŒƒå¼ï¼Œç”¨äºå°†è¯¸å¦‚CLIPä¹‹ç±»çš„è§†è§‰è¯­è¨€æ¨¡å‹é€‚åº”äºä¸‹æ¸¸ä»»åŠ¡ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¾€å¾€ä¼šå¯¹å·²è§æ•°æ®è¿›è¡Œè¿‡åº¦æ‹Ÿåˆï¼Œå¯¼è‡´åœ¨æ¨å¹¿åˆ°æ–°å‹ç±»åˆ«æˆ–æœªè§é¢†åŸŸæ—¶æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†DiSaï¼Œä¸€ä¸ªé¢å‘æ–¹å‘çš„æ˜¾è‘—æ€§æ„ŸçŸ¥æç¤ºå­¦ä¹ æ¡†æ¶ï¼Œå®ƒé›†æˆäº†ä¸¤ç§äº’è¡¥çš„æ­£åˆ™åŒ–ç­–ç•¥ï¼Œä»¥å¢å¼ºæ³›åŒ–èƒ½åŠ›ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬çš„äº¤å‰äº¤äº’æ­£åˆ™åŒ–ï¼ˆCIRï¼‰é€šè¿‡ä¿ƒè¿›æç¤ºå’Œå†»ç»“ç¼–ç å™¨ä¹‹é—´çš„åˆä½œå­¦ä¹ ï¼Œå®ç°è·¨æ¨¡æ€å¯¹é½ã€‚åœ¨CIRä¸­ï¼Œä¸€ç§æ˜¾è‘—æ€§æ„ŸçŸ¥æ©ç ç­–ç•¥å¼•å¯¼å›¾åƒç¼–ç å™¨ä¼˜å…ˆå¤„ç†è¯­ä¹‰ä¸Šå…³é”®çš„å›¾åƒåŒºåŸŸï¼Œå‡å°‘å¯¹äºä¿¡æ¯é‡è¾ƒå°‘çš„è¡¥ä¸çš„ä¾èµ–ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–¹å‘æ€§æ­£åˆ™åŒ–ç­–ç•¥ï¼Œä»¥æ–¹å‘æ€§æ–¹å¼å°†è§†è§‰åµŒå…¥ä¸ç±»åŸå‹ç‰¹å¾å¯¹é½ï¼Œä¼˜å…ˆä¿è¯ç‰¹å¾æ–¹å‘çš„ä¸€è‡´æ€§ï¼Œè€Œéä¸¥æ ¼çš„æ¥è¿‘ç¨‹åº¦ã€‚è¿™ç§æ–¹æ³•é€šè¿‡åˆ©ç”¨ä»ç±»å‡å€¼ç»Ÿè®¡å¾—å‡ºçš„ç¨³å®šåŸå‹æ–¹å‘ï¼Œç¡®ä¿äº†ç¨³å¥çš„æ³›åŒ–ã€‚åœ¨11ä¸ªä¸åŒçš„å›¾åƒåˆ†ç±»åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒDiSaåœ¨å„ç§è®¾ç½®ä¸‹å‡ä¸€è‡´ä¼˜äºæœ€å…ˆè¿›çš„æç¤ºå­¦ä¹ æ–¹æ³•ï¼ŒåŒ…æ‹¬åŸºç¡€åˆ°æ–°å‹çš„æ³›åŒ–ã€è·¨æ•°æ®é›†è½¬æ¢ã€åŸŸæ³›åŒ–å’Œå°æ ·æœ¬å­¦ä¹ ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19373v1">PDF</a> Accepted at the 31st ACM SIGKDD Conference on Knowledge Discovery and   Data Mining (KDD 2025)</p>
<p><strong>Summary</strong><br>     é’ˆå¯¹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆå¦‚CLIPï¼‰ä¸‹æ¸¸ä»»åŠ¡çš„é€‚åº”æ€§ï¼Œæç¤ºå­¦ä¹ å·²æˆä¸ºä¸€ç§å¼ºå¤§çš„èŒƒå¼ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¸¸å¸¸å¯¹å¯è§æ•°æ®è¿‡åº¦æ‹Ÿåˆï¼Œåœ¨æ¨å¹¿åˆ°æ–°å‹ç±»åˆ«æˆ–æœªè§é¢†åŸŸæ—¶æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚ä¸ºè§£å†³æ­¤å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºDiSaï¼Œä¸€ç§æ–¹å‘æ€§æ˜¾è‘—æ€§æ„ŸçŸ¥æç¤ºå­¦ä¹ æ¡†æ¶ï¼Œå®ƒèåˆäº†ä¸¤ç§äº’è¡¥çš„æ­£åˆ™åŒ–ç­–ç•¥ä»¥å¢å¼ºæ³›åŒ–èƒ½åŠ›ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬çš„äº¤äº’æ­£åˆ™åŒ–ï¼ˆCIRï¼‰é€šè¿‡ä¿ƒè¿›æç¤ºå’Œå†»ç»“ç¼–ç å™¨ä¹‹é—´çš„åˆä½œæ€§å­¦ä¹ æ¥å¢å¼ºè·¨æ¨¡æ€å¯¹é½ã€‚CIRå†…çš„æ˜¾è‘—æ€§æ„ŸçŸ¥æ©ç ç­–ç•¥å¼•å¯¼å›¾åƒç¼–ç å™¨ä¼˜å…ˆå…³æ³¨è¯­ä¹‰å…³é”®å›¾åƒåŒºåŸŸï¼Œå‡å°‘äº†å¯¹ä¿¡æ¯è¾ƒå°‘åŒºåŸŸçš„ä¾èµ–ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–¹å‘æ€§æ­£åˆ™åŒ–ç­–ç•¥ï¼Œä»¥æ–¹å‘æ€§æ–¹å¼å°†è§†è§‰åµŒå…¥ä¸ç±»åŸå‹ç‰¹å¾å¯¹é½ï¼Œä»¥ä¼˜å…ˆä¿è¯ç‰¹å¾æ–¹å‘çš„è¿ç»­æ€§è€Œéä¸¥æ ¼æ¥è¿‘æ€§ã€‚æ­¤æ–¹æ³•é€šè¿‡åˆ©ç”¨ä»ç±»å‡å€¼ç»Ÿè®¡å¾—å‡ºçš„ç¨³å®šåŸå‹æ–¹å‘ï¼Œç¡®ä¿äº†ç¨³å¥æ³›åŒ–ã€‚åœ¨11ä¸ªä¸åŒçš„å›¾åƒåˆ†ç±»åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒDiSaåœ¨å¤šç§è®¾ç½®ä¸‹å§‹ç»ˆä¼˜äºæœ€æ–°æç¤ºå­¦ä¹ æ–¹æ³•ï¼ŒåŒ…æ‹¬åŸºç¡€åˆ°æ–°å‹çš„æ³›åŒ–ã€è·¨æ•°æ®é›†è¿ç§»ã€åŸŸæ³›åŒ–å’Œå°æ ·æœ¬å­¦ä¹ ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æç¤ºå­¦ä¹ æ˜¯é€‚åº”è§†è§‰è¯­è¨€æ¨¡å‹ä¸‹æ¸¸ä»»åŠ¡çš„æœ‰æ•ˆèŒƒå¼ã€‚</li>
<li>ç°æœ‰æç¤ºå­¦ä¹ æ–¹æ³•å­˜åœ¨è¿‡åº¦æ‹Ÿåˆé—®é¢˜ï¼Œé™åˆ¶äº†æ¨¡å‹åœ¨æ–°å‹ç±»åˆ«æˆ–æœªè§é¢†åŸŸçš„æ€§èƒ½ã€‚</li>
<li>DiSaæ¡†æ¶é€šè¿‡é›†æˆä¸¤ç§æ­£åˆ™åŒ–ç­–ç•¥æ¥è§£å†³æ­¤é—®é¢˜ï¼Œå¢å¼ºæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>Cross-Interactive Regularization (CIR) ä¿ƒè¿›è·¨æ¨¡æ€å¯¹é½ï¼Œé€šè¿‡æ˜¾è‘—æ€§æ„ŸçŸ¥æ©ç ç­–ç•¥å¼•å¯¼å›¾åƒç¼–ç å™¨çš„å…³æ³¨é‡ç‚¹ã€‚</li>
<li>å¼•å…¥çš„æ–¹å‘æ€§æ­£åˆ™åŒ–ç­–ç•¥æ³¨é‡ç‰¹å¾æ–¹å‘çš„è¿ç»­æ€§ï¼Œé€šè¿‡åˆ©ç”¨ç±»åŸå‹ç‰¹å¾çš„ç¨³å®šæ–¹å‘æ¥æé«˜æ¨¡å‹æ³›åŒ–ã€‚</li>
<li>DiSaåœ¨å¤šç§å›¾åƒåˆ†ç±»åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜è¶Šï¼ŒåŒ…æ‹¬åŸºç¡€åˆ°æ–°å‹çš„æ³›åŒ–ã€è·¨æ•°æ®é›†è¿ç§»ã€åŸŸæ³›åŒ–å’Œå°æ ·æœ¬å­¦ä¹ ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19373">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f3198aa09e17f01167d04f3738706f74.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6f6371aad24937072484eb2e54107d45.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c505e2115baf52c8baa60ed5a6aa96a4.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="A-Smart-Healthcare-System-for-Monkeypox-Skin-Lesion-Detection-and-Tracking"><a href="#A-Smart-Healthcare-System-for-Monkeypox-Skin-Lesion-Detection-and-Tracking" class="headerlink" title="A Smart Healthcare System for Monkeypox Skin Lesion Detection and   Tracking"></a>A Smart Healthcare System for Monkeypox Skin Lesion Detection and   Tracking</h2><p><strong>Authors:Huda Alghoraibi, Nuha Alqurashi, Sarah Alotaibi, Renad Alkhudaydi, Bdoor Aldajani, Lubna Alqurashi, Jood Batweel, Maha A. Thafar</strong></p>
<p>Monkeypox is a viral disease characterized by distinctive skin lesions and has been reported in many countries. The recent global outbreak has emphasized the urgent need for scalable, accessible, and accurate diagnostic solutions to support public health responses.   In this study, we developed ITMAINN, an intelligent, AI-driven healthcare system specifically designed to detect Monkeypox from skin lesion images using advanced deep learning techniques. Our system consists of three main components. First, we trained and evaluated several pretrained models using transfer learning on publicly available skin lesion datasets to identify the most effective models. For binary classification (Monkeypox vs. non-Monkeypox), the Vision Transformer, MobileViT, Transformer-in-Transformer, and VGG16 achieved the highest performance, each with an accuracy and F1-score of 97.8%. For multiclass classification, which contains images of patients with Monkeypox and five other classes (chickenpox, measles, hand-foot-mouth disease, cowpox, and healthy), ResNetViT and ViT Hybrid models achieved 92% accuracy, with F1 scores of 92.24% and 92.19%, respectively. The best-performing and most lightweight model, MobileViT, was deployed within the mobile application. The second component is a cross-platform smartphone application that enables users to detect Monkeypox through image analysis, track symptoms, and receive recommendations for nearby healthcare centers based on their location. The third component is a real-time monitoring dashboard designed for health authorities to support them in tracking cases, analyzing symptom trends, guiding public health interventions, and taking proactive measures.   This system is fundamental in developing responsive healthcare infrastructure within smart cities. Our solution, ITMAINN, is part of revolutionizing public health management. </p>
<blockquote>
<p>å¤©èŠ±çš„ç—…æ¯’æ€§ç–¾ç—…ç‰¹å¾åœ¨äºå…·æœ‰æ˜¾è‘—ç‰¹å¾çš„çš®è‚¤ç—…å˜ï¼Œå·²ç»å‡ºç°åœ¨å¤šä¸ªå›½å®¶ã€‚æœ€è¿‘çš„å…¨çƒçˆ†å‘å¼ºè°ƒäº†è¿«åˆ‡éœ€è¦å¯¹å¯æ‰©å±•ã€å¯è®¿é—®å’Œå‡†ç¡®çš„è¯Šæ–­è§£å†³æ–¹æ¡ˆçš„æ”¯æŒï¼Œä»¥åº”å¯¹å…¬å…±å«ç”ŸæŒ‘æˆ˜ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å¼€å‘äº†ITMAINNç³»ç»Ÿï¼Œè¿™æ˜¯ä¸€ä¸ªæ™ºèƒ½çš„AIé©±åŠ¨çš„åŒ»ç–—ä¿å¥ç³»ç»Ÿï¼Œä¸“é—¨ç”¨äºåˆ©ç”¨å…ˆè¿›çš„æ·±åº¦å­¦ä¹ æŠ€æœ¯ä»çš®è‚¤ç—…å˜å›¾åƒä¸­æ£€æµ‹å¤©èŠ±ç—…æ¯’ã€‚æˆ‘ä»¬çš„ç³»ç»Ÿç”±ä¸‰ä¸ªä¸»è¦ç»„æˆéƒ¨åˆ†æ„æˆã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä½¿ç”¨è¿ç§»å­¦ä¹ åœ¨å…¬å¼€çš„çš®è‚¤ç—…æ•°æ®é›†ä¸Šè®­ç»ƒå’Œè¯„ä¼°äº†ä¸€äº›é¢„è®­ç»ƒæ¨¡å‹ï¼Œä»¥ç¡®å®šæœ€æœ‰æ•ˆçš„æ¨¡å‹ã€‚å¯¹äºäºŒå…ƒåˆ†ç±»ï¼ˆå¤©èŠ±ä¸éå¤©èŠ±ï¼‰ï¼ŒVision Transformerã€MobileViTã€Transformer-in-Transformerå’ŒVGG16è¡¨ç°æœ€ä½³ï¼Œå‡†ç¡®ç‡å’ŒF1åˆ†æ•°å‡è¾¾åˆ°97.8%ã€‚å¯¹äºåŒ…å«å¤©èŠ±å›¾åƒä»¥åŠå…¶ä»–äº”ç§ç±»åˆ«ï¼ˆæ°´ç—˜ã€éº»ç–¹ã€æ‰‹è¶³å£ã€ç‰›ç—˜å’Œæ­£å¸¸ï¼‰æ‚£è€…çš„å¤šåˆ†ç±»ï¼ŒResNetViTå’ŒViTæ··åˆæ¨¡å‹å‡†ç¡®ç‡è¾¾åˆ°92%ï¼ŒF1åˆ†æ•°åˆ†åˆ«ä¸º92.24%å’Œ92.19%ã€‚è¡¨ç°æœ€ä½³ä¸”æœ€è½»é‡çº§æ¨¡å‹MobileViTå·²éƒ¨ç½²åœ¨ç§»åŠ¨åº”ç”¨ç¨‹åºä¸­ã€‚ç¬¬äºŒä¸ªç»„æˆéƒ¨åˆ†æ˜¯ä¸€ä¸ªè·¨å¹³å°çš„æ™ºèƒ½æ‰‹æœºåº”ç”¨ç¨‹åºï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿé€šè¿‡å›¾åƒåˆ†ææ£€æµ‹å¤©èŠ±ç—…æ¯’ï¼Œè·Ÿè¸ªç—‡çŠ¶ï¼Œå¹¶æ ¹æ®å…¶ä½ç½®è·å¾—é™„è¿‘åŒ»ç–—ä¿å¥ä¸­å¿ƒçš„æ¨èã€‚ç¬¬ä¸‰ä¸ªç»„æˆéƒ¨åˆ†æ˜¯ä¸€ä¸ªå®æ—¶ç›‘è§†ä»ªè¡¨æ¿ï¼Œä¸“ä¸ºå«ç”Ÿå½“å±€è®¾è®¡ï¼Œä»¥æ”¯æŒä»–ä»¬è·Ÿè¸ªç—…ä¾‹ã€åˆ†æç—‡çŠ¶è¶‹åŠ¿ã€æŒ‡å¯¼å…¬å…±å«ç”Ÿå¹²é¢„å’Œé‡‡å–ç§¯ææªæ–½ã€‚è¿™ä¸€ç³»ç»Ÿåœ¨æ™ºèƒ½åŸå¸‚å»ºç«‹å“åº”è¿…é€Ÿçš„å«ç”Ÿä¿å¥åŸºç¡€è®¾æ–½æ–¹é¢è‡³å…³é‡è¦ã€‚æˆ‘ä»¬çš„è§£å†³æ–¹æ¡ˆITMAINNæ˜¯å…¬å…±å«ç”Ÿç®¡ç†é©å‘½çš„ä¸€éƒ¨åˆ†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19023v1">PDF</a> 23 pages, 5 figures</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬ç ”ç©¶å¼€å‘äº†ä¸€ç§åä¸ºITMAINNçš„æ™ºèƒ½AIåŒ»ç–—ç³»ç»Ÿï¼Œç”¨äºé€šè¿‡çš®è‚¤ç—…å˜å›¾åƒæ£€æµ‹çŒ´ç—˜ç—…æ¯’ã€‚è¯¥ç³»ç»ŸåŒ…å«ä¸‰ä¸ªä¸»è¦ç»„ä»¶ï¼Œé€šè¿‡å…ˆè¿›çš„æ·±åº¦å­¦ä¹ æŠ€æœ¯å®ç°å¯¹çŒ´ç—˜çš„äºŒå…ƒåˆ†ç±»å’Œå¤šåˆ†ç±»ã€‚è¯¥ç³»ç»Ÿè®­ç»ƒå¹¶ä½¿ç”¨å¤šä¸ªé¢„è®­ç»ƒæ¨¡å‹åœ¨å…¬å¼€çš„çš®è‚¤ç—…å˜æ•°æ®é›†ä¸Šè¿›è¡Œè¿ç§»å­¦ä¹ ï¼Œä»¥è¯†åˆ«æœ€æœ‰æ•ˆçš„æ¨¡å‹ã€‚å¯¹äºäºŒå…ƒåˆ†ç±»ï¼ˆçŒ´ç—˜ä¸éçŒ´ç—˜ï¼‰ï¼Œå…¶å‡†ç¡®ç‡è¾¾åˆ°äº†æƒŠäººçš„97.8%ã€‚å¯¹äºåŒ…å«çŒ´ç—˜å’Œå…¶ä»–äº”ç§ç–¾ç—…ï¼ˆé¸¡ç—˜ã€éº»ç–¹ã€æ‰‹è¶³å£ã€ç‰›ç—˜å’Œæ­£å¸¸ï¼‰çš„å¤šç±»åˆ†ç±»ï¼Œå…¶å‡†ç¡®ç‡ä¹Ÿè¾¾åˆ°äº†92%ã€‚å…¶ä¸­è¡¨ç°æœ€ä½³ä¸”æœ€è½»é‡çº§æ¨¡å‹MobileViTå·²è¢«éƒ¨ç½²åœ¨æ‰‹æœºåº”ç”¨ç¨‹åºä¸­ã€‚æ­¤å¤–ï¼Œè¯¥ç³»ç»Ÿè¿˜åŒ…æ‹¬ä¸€ä¸ªè·¨å¹³å°æ‰‹æœºåº”ç”¨ç¨‹åºå’Œç”¨äºè¿½è¸ªç—…ä¾‹ã€åˆ†æç—‡çŠ¶è¶‹åŠ¿å¹¶æ”¯æŒå…¬å…±å«ç”Ÿå¹²é¢„å’Œé‡‡å–é¢„é˜²æªæ–½çš„å®æ—¶ç›‘è§†ä»ªè¡¨æ¿ã€‚ITMAINNç³»ç»Ÿæ˜¯æ™ºèƒ½åŸå¸‚å“åº”åŒ»ç–—åŸºç¡€è®¾æ–½å‘å±•çš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼Œå¯¹å…¬å…±å«ç”Ÿç®¡ç†é©å‘½å…·æœ‰é‡è¦æ„ä¹‰ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ITMAINNç³»ç»Ÿæ˜¯ä¸€ç§åŸºäºAIçš„æ™ºèƒ½åŒ»ç–—ç³»ç»Ÿï¼Œå¯é€šè¿‡å¯¹çš®è‚¤ç—…å˜å›¾åƒçš„åˆ†ææ¥æ£€æµ‹çŒ´ç—˜ç—…æ¯’ã€‚</li>
<li>ä½¿ç”¨æ·±åº¦å­¦ä¹ æŠ€æœ¯è¿›è¡ŒäºŒå…ƒåˆ†ç±»å’Œå¤šåˆ†ç±»ï¼Œä»¥åŒºåˆ†çŒ´ç—˜ä¸å…¶ä»–çš®è‚¤ç–¾ç—…ã€‚</li>
<li>åœ¨äºŒå…ƒåˆ†ç±»ä¸­ï¼Œå¤šä¸ªæ¨¡å‹ï¼ˆå¦‚Vision Transformerã€MobileViTç­‰ï¼‰è¡¨ç°å‡ºé«˜å‡†ç¡®ç‡ï¼ˆ97.8%ï¼‰ã€‚</li>
<li>åœ¨å¤šç±»åˆ†ç±»ä¸­ï¼ŒResNetViTå’ŒViT Hybridæ¨¡å‹è¾¾åˆ°92%çš„å‡†ç¡®ç‡å’Œç›¸åº”çš„F1åˆ†æ•°ã€‚</li>
<li>MobileViTæ¨¡å‹å› å…¶é«˜æ•ˆæ€§èƒ½è¢«éƒ¨ç½²åœ¨æ‰‹æœºåº”ç”¨ç¨‹åºä¸­ã€‚</li>
<li>ç³»ç»ŸåŒ…æ‹¬ä¸€ä¸ªæ‰‹æœºåº”ç”¨ç¨‹åºï¼Œç”¨äºå›¾åƒåˆ†æã€ç—‡çŠ¶è¿½è¸ªå’Œé™„è¿‘åŒ»ç–—ä¸­å¿ƒçš„æ¨èã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19023">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fb0818c56e263d070f70d6842ed5ca91.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-afe3ea56e4c28d7179fc39e5679ab065.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="CDPDNet-Integrating-Text-Guidance-with-Hybrid-Vision-Encoders-for-Medical-Image-Segmentation"><a href="#CDPDNet-Integrating-Text-Guidance-with-Hybrid-Vision-Encoders-for-Medical-Image-Segmentation" class="headerlink" title="CDPDNet: Integrating Text Guidance with Hybrid Vision Encoders for   Medical Image Segmentation"></a>CDPDNet: Integrating Text Guidance with Hybrid Vision Encoders for   Medical Image Segmentation</h2><p><strong>Authors:Jiong Wu, Yang Xing, Boxiao Yu, Wei Shao, Kuang Gong</strong></p>
<p>Most publicly available medical segmentation datasets are only partially labeled, with annotations provided for a subset of anatomical structures. When multiple datasets are combined for training, this incomplete annotation poses challenges, as it limits the modelâ€™s ability to learn shared anatomical representations among datasets. Furthermore, vision-only frameworks often fail to capture complex anatomical relationships and task-specific distinctions, leading to reduced segmentation accuracy and poor generalizability to unseen datasets. In this study, we proposed a novel CLIP-DINO Prompt-Driven Segmentation Network (CDPDNet), which combined a self-supervised vision transformer with CLIP-based text embedding and introduced task-specific text prompts to tackle these challenges. Specifically, the framework was constructed upon a convolutional neural network (CNN) and incorporated DINOv2 to extract both fine-grained and global visual features, which were then fused using a multi-head cross-attention module to overcome the limited long-range modeling capability of CNNs. In addition, CLIP-derived text embeddings were projected into the visual space to help model complex relationships among organs and tumors. To further address the partial label challenge and enhance inter-task discriminative capability, a Text-based Task Prompt Generation (TTPG) module that generated task-specific prompts was designed to guide the segmentation. Extensive experiments on multiple medical imaging datasets demonstrated that CDPDNet consistently outperformed existing state-of-the-art segmentation methods. Code and pretrained model are available at: <a target="_blank" rel="noopener" href="https://github.com/wujiong-hub/CDPDNet.git">https://github.com/wujiong-hub/CDPDNet.git</a>. </p>
<blockquote>
<p>å¤§éƒ¨åˆ†å…¬å¼€å¯ç”¨çš„åŒ»å­¦åˆ†å‰²æ•°æ®é›†ä»…éƒ¨åˆ†æ ‡æ³¨ï¼Œåªä¸ºéƒ¨åˆ†è§£å‰–ç»“æ„æä¾›æ³¨é‡Šã€‚å½“å¤šä¸ªæ•°æ®é›†ç»„åˆè¿›è¡Œè®­ç»ƒæ—¶ï¼Œè¿™ç§ä¸å®Œå…¨çš„æ³¨é‡Šå¸¦æ¥äº†æŒ‘æˆ˜ï¼Œå› ä¸ºå®ƒé™åˆ¶äº†æ¨¡å‹åœ¨æ•°æ®é›†ä¹‹é—´å­¦ä¹ å…±äº«è§£å‰–è¡¨å¾çš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œä»…ä¾èµ–è§†è§‰çš„æ¡†æ¶é€šå¸¸æ— æ³•æ•æ‰å¤æ‚çš„è§£å‰–å…³ç³»å’Œä»»åŠ¡ç‰¹å®šçš„åŒºåˆ«ï¼Œä»è€Œå¯¼è‡´åˆ†å‰²ç²¾åº¦é™ä½å’Œæœªè§æ•°æ®é›†çš„æ³›åŒ–èƒ½åŠ›è¾ƒå·®ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„CLIP-DINO Prompté©±åŠ¨åˆ†å‰²ç½‘ç»œï¼ˆCDPDNetï¼‰ï¼Œå®ƒå°†è‡ªç›‘ç£çš„è§†è§‰å˜å‹å™¨ä¸CLIPåŸºäºæ–‡æœ¬çš„åµŒå…¥ç›¸ç»“åˆï¼Œå¹¶å¼•å…¥ä»»åŠ¡ç‰¹å®šçš„æ–‡æœ¬æç¤ºæ¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚å…·ä½“æ¥è¯´ï¼Œè¯¥æ¡†æ¶å»ºç«‹åœ¨å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ä¹‹ä¸Šï¼Œå¹¶èå…¥äº†DINOv2æ¥æå–ç²¾ç»†ç²’åº¦å’Œå…¨å±€è§†è§‰ç‰¹å¾ï¼Œç„¶åä½¿ç”¨å¤šå¤´äº¤å‰æ³¨æ„æ¨¡å—èåˆè¿™äº›ç‰¹å¾ï¼Œä»¥å…‹æœCNNæœ‰é™çš„é•¿æœŸå»ºæ¨¡èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒCLIPè¡ç”Ÿçš„æ–‡æœ¬åµŒå…¥è¢«æŠ•å°„åˆ°è§†è§‰ç©ºé—´ä¸­ï¼Œä»¥å¸®åŠ©æ¨¡å‹æ¨¡æ‹Ÿå™¨å®˜å’Œè‚¿ç˜¤ä¹‹é—´çš„å¤æ‚å…³ç³»ã€‚ä¸ºäº†è¿›ä¸€æ­¥è§£å†³éƒ¨åˆ†æ ‡ç­¾æŒ‘æˆ˜å¹¶å¢å¼ºä»»åŠ¡é—´çš„åˆ¤åˆ«èƒ½åŠ›ï¼Œè®¾è®¡äº†ä¸€ä¸ªåŸºäºæ–‡æœ¬çš„ä»»åŠ¡æç¤ºç”Ÿæˆï¼ˆTTPGï¼‰æ¨¡å—ï¼Œè¯¥æ¨¡å—ç”Ÿæˆç‰¹å®šä»»åŠ¡çš„æç¤ºæ¥æŒ‡å¯¼åˆ†å‰²ã€‚åœ¨å¤šä¸ªåŒ»å­¦æˆåƒæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒCDPDNetæŒç»­ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›çš„åˆ†å‰²æ–¹æ³•ã€‚ä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/wujiong-hub/CDPDNet.git%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/wujiong-hub/CDPDNet.gitä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.18958v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºCLIP-DINO Prompté©±åŠ¨çš„åˆ†æ®µç½‘ç»œï¼ˆCDPDNetï¼‰ï¼Œç»“åˆäº†è‡ªç›‘ç£è§†è§‰å˜å‹å™¨å’ŒCLIPæ–‡æœ¬åµŒå…¥æŠ€æœ¯ï¼Œè§£å†³äº†åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„éƒ¨åˆ†æ ‡æ³¨é—®é¢˜å’Œå¤æ‚è§£å‰–å…³ç³»æ•æ‰é—®é¢˜ã€‚CDPDNeté€šè¿‡CNNå’ŒDINOv2æå–ç²¾ç»†å’Œå…¨å±€è§†è§‰ç‰¹å¾ï¼Œå¹¶é‡‡ç”¨å¤šå¤´äº¤å‰æ³¨æ„åŠ›æ¨¡å—èåˆç‰¹å¾ï¼Œç”Ÿæˆä»»åŠ¡ç‰¹å®šæç¤ºæ¥æŒ‡å¯¼åˆ†å‰²ï¼ŒåŒæ—¶åœ¨å¤šä¸ªåŒ»å­¦æˆåƒæ•°æ®é›†ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CDPDNetè§£å†³äº†åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­æ•°æ®é›†éƒ¨åˆ†æ ‡æ³¨çš„é—®é¢˜ï¼Œé€šè¿‡ç»“åˆè‡ªç›‘ç£è§†è§‰å˜å‹å™¨å’ŒCLIPæ–‡æœ¬åµŒå…¥æŠ€æœ¯ï¼Œæé«˜äº†æ¨¡å‹å­¦ä¹ å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>CDPDNetç»“åˆCNNå’ŒDINOv2æå–è§†è§‰ç‰¹å¾ï¼Œé€šè¿‡å¤šå¤´äº¤å‰æ³¨æ„åŠ›æ¨¡å—èåˆç‰¹å¾ï¼Œæé«˜äº†æ¨¡å‹çš„é•¿æœŸå»ºæ¨¡èƒ½åŠ›ã€‚</li>
<li>CLIPæ–‡æœ¬åµŒå…¥è¢«å¼•å…¥åˆ°è§†è§‰ç©ºé—´ï¼Œå¸®åŠ©æ¨¡å‹æ•æ‰å¤æ‚çš„å™¨å®˜å’Œè‚¿ç˜¤å…³ç³»ã€‚</li>
<li>Text-based Task Prompt Generationï¼ˆTTPGï¼‰æ¨¡å—ç”Ÿæˆä»»åŠ¡ç‰¹å®šæç¤ºï¼Œè¿›ä¸€æ­¥è§£å†³äº†éƒ¨åˆ†æ ‡ç­¾æŒ‘æˆ˜ï¼Œå¢å¼ºäº†ä»»åŠ¡é—´çš„åˆ¤åˆ«èƒ½åŠ›ã€‚</li>
<li>CDPDNetåœ¨å¤šä¸ªåŒ»å­¦æˆåƒæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„åˆ†å‰²æ–¹æ³•ã€‚</li>
<li>CDPDNetçš„å®ç°ä»£ç å’Œé¢„å…ˆè®­ç»ƒçš„æ¨¡å‹å¯åœ¨å…¬å¼€ä»“åº“ä¸­æ‰¾åˆ°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.18958">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-008555b808d4437acca293ee2013d487.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1caa4ca25d9da3c713d834debb18b05b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d5e4448fd40791b96a39fe2b6a143e81.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a597aaeb4933c4e5f5eed4b2f45bd828.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c3656a1fb73ee40b14268f03ed2ad006.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="A-Dataset-and-Benchmarks-for-Deep-Learning-Based-Optical-Microrobot-Pose-and-Depth-Perception"><a href="#A-Dataset-and-Benchmarks-for-Deep-Learning-Based-Optical-Microrobot-Pose-and-Depth-Perception" class="headerlink" title="A Dataset and Benchmarks for Deep Learning-Based Optical Microrobot Pose   and Depth Perception"></a>A Dataset and Benchmarks for Deep Learning-Based Optical Microrobot Pose   and Depth Perception</h2><p><strong>Authors:Lan Wei, Dandan Zhang</strong></p>
<p>Optical microrobots, manipulated via optical tweezers (OT), have broad applications in biomedicine. However, reliable pose and depth perception remain fundamental challenges due to the transparent or low-contrast nature of the microrobots, as well as the noisy and dynamic conditions of the microscale environments in which they operate. An open dataset is crucial for enabling reproducible research, facilitating benchmarking, and accelerating the development of perception models tailored to microscale challenges. Standardised evaluation enables consistent comparison across algorithms, ensuring objective benchmarking and facilitating reproducible research. Here, we introduce the OpTical MicroRobot dataset (OTMR), the first publicly available dataset designed to support microrobot perception under the optical microscope. OTMR contains 232,881 images spanning 18 microrobot types and 176 distinct poses. We benchmarked the performance of eight deep learning models, including architectures derived via neural architecture search (NAS), on two key tasks: pose classification and depth regression. Results indicated that Vision Transformer (ViT) achieve the highest accuracy in pose classification, while depth regression benefits from deeper architectures. Additionally, increasing the size of the training dataset leads to substantial improvements across both tasks, highlighting OTMRâ€™s potential as a foundational resource for robust and generalisable microrobot perception in complex microscale environments. </p>
<blockquote>
<p>å…‰å­¦å¾®æœºå™¨äººé€šè¿‡å…‰å­¦æ‰³æ‰‹ï¼ˆOTï¼‰è¿›è¡Œæ“ä½œï¼Œåœ¨ç”Ÿç‰©åŒ»å­¦é¢†åŸŸæœ‰ç€å¹¿æ³›çš„åº”ç”¨ã€‚ç„¶è€Œï¼Œç”±äºå¾®æœºå™¨äººçš„é€æ˜æ€§æˆ–ä½å¯¹æ¯”åº¦ç‰¹æ€§ï¼Œä»¥åŠå®ƒä»¬æ‰€æ“ä½œçš„å¾®å°ºåº¦ç¯å¢ƒçš„å™ªå£°å’ŒåŠ¨æ€æ¡ä»¶ï¼Œå¯é çš„å§¿æ€å’Œæ·±åº¦æ„ŸçŸ¥ä»ç„¶æ˜¯ä¸€ä¸ªåŸºæœ¬æŒ‘æˆ˜ã€‚å¯¹äºä¿ƒè¿›å¯é‡å¤ç ”ç©¶ã€æ¨åŠ¨åŸºå‡†æµ‹è¯•å¹¶åŠ é€Ÿé’ˆå¯¹å¾®å°ºåº¦æŒ‘æˆ˜çš„æ„ŸçŸ¥æ¨¡å‹å¼€å‘è€Œè¨€ï¼Œå¼€æ”¾æ•°æ®é›†è‡³å…³é‡è¦ã€‚æ ‡å‡†åŒ–è¯„ä¼°å¯ä»¥ç¡®ä¿ç®—æ³•ä¹‹é—´çš„ä¸€è‡´æ¯”è¾ƒï¼Œä¿è¯å®¢è§‚åŸºå‡†æµ‹è¯•å¹¶ä¿ƒè¿›å¯é‡å¤ç ”ç©¶ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä»‹ç»äº†OpTical MicroRobotæ•°æ®é›†ï¼ˆOTMRï¼‰ï¼Œè¿™æ˜¯ä¸ºäº†æ”¯æŒå…‰å­¦æ˜¾å¾®é•œä¸‹å¾®æœºå™¨äººæ„ŸçŸ¥è€Œè®¾è®¡çš„ç¬¬ä¸€ä¸ªå…¬å¼€æ•°æ®é›†ã€‚OTMRåŒ…å«è·¨è¶Š18ç§å¾®æœºå™¨äººç±»å‹å’Œ176ç§ä¸åŒå§¿æ€çš„232881å¼ å›¾åƒã€‚æˆ‘ä»¬å¯¹å…«ç§æ·±åº¦å­¦ä¹ æ¨¡å‹è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬é€šè¿‡ç¥ç»ç½‘ç»œæ¶æ„æœç´¢ï¼ˆNASï¼‰æ´¾ç”Ÿå‡ºçš„æ¶æ„ï¼Œä¸»è¦é’ˆå¯¹ä¸¤é¡¹ä»»åŠ¡ï¼šå§¿æ€åˆ†ç±»å’Œæ·±åº¦å›å½’ã€‚ç»“æœè¡¨æ˜ï¼Œåœ¨å§¿æ€åˆ†ç±»æ–¹é¢ï¼ŒVision Transformerï¼ˆViTï¼‰çš„å‡†ç¡®åº¦æœ€é«˜ï¼Œè€Œæ·±åº¦å›å½’åˆ™å—ç›Šäºæ›´æ·±çš„æ¶æ„ã€‚æ­¤å¤–ï¼Œå¢åŠ è®­ç»ƒæ•°æ®é›†çš„å¤§å°å¯¹è¿™ä¸¤é¡¹ä»»åŠ¡éƒ½æœ‰å®è´¨æ€§çš„æ”¹è¿›ï¼Œçªæ˜¾äº†OTMRä½œä¸ºåœ¨å¤æ‚å¾®å°ºåº¦ç¯å¢ƒä¸­è¿›è¡Œç¨³å¥å’Œé€šç”¨å¾®æœºå™¨äººæ„ŸçŸ¥çš„åŸºç¡€èµ„æºçš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.18303v1">PDF</a> Accepted by the 2025 International Conference on Manipulation,   Automation and Robotics at Small Scales (MARSS)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å…‰å­¦å¾®æœºå™¨äººæ•°æ®é›†ï¼ˆOTMRï¼‰çš„åˆ›å»ºåŠå…¶é‡è¦æ€§ã€‚OTMRæ˜¯é¦–ä¸ªæ”¯æŒå…‰å­¦æ˜¾å¾®é•œä¸‹çš„å¾®æœºå™¨äººæ„ŸçŸ¥çš„æ•°æ®é›†ï¼ŒåŒ…å«æ¶µç›–å¤šç§ç±»å‹å’Œå§¿æ€çš„å¤§é‡å›¾åƒã€‚ç ”ç©¶è€…åœ¨è¯¥æ•°æ®é›†ä¸Šè¯„ä¼°äº†å¤šä¸ªæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œå‘ç°Vision Transformerï¼ˆViTï¼‰åœ¨å§¿æ€åˆ†ç±»ä»»åŠ¡ä¸Šè¡¨ç°æœ€ä½³ï¼Œæ·±åº¦å›å½’ä»»åŠ¡å—ç›Šäºæ›´æ·±çš„æ¶æ„ã€‚æ­¤å¤–ï¼Œæ‰©å¤§è®­ç»ƒæ•°æ®é›†å¤§å°å¯¹ä¸¤é¡¹ä»»åŠ¡éƒ½æœ‰æ˜¾è‘—æ”¹è¿›ï¼Œå±•ç¤ºäº†OTMRåœ¨å¤æ‚å¾®å°ºåº¦ç¯å¢ƒä¸­å®ç°ç¨³å¥å’Œé€šç”¨å¾®æœºå™¨äººæ„ŸçŸ¥çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å…‰å­¦å¾®æœºå™¨äººé¢†åŸŸé¢ä¸´å§¿æ€å’Œæ·±åº¦æ„ŸçŸ¥çš„æŒ‘æˆ˜ï¼Œä¸»è¦ç”±äºå¾®æœºå™¨äººçš„é€æ˜æ€§æˆ–ä½å¯¹æ¯”åº¦ä»¥åŠå¾®å°ºåº¦ç¯å¢ƒçš„å™ªå£°å’ŒåŠ¨æ€æ¡ä»¶ã€‚</li>
<li>å…¬å¼€æ•°æ®é›†å¯¹äºæ¨åŠ¨å¯é‡å¤ç ”ç©¶ã€åŸºå‡†æµ‹è¯•ä»¥åŠåŠ é€Ÿé’ˆå¯¹å¾®å°ºåº¦æŒ‘æˆ˜çš„æ„ŸçŸ¥æ¨¡å‹å¼€å‘è‡³å…³é‡è¦ã€‚</li>
<li>ä»‹ç»äº†é¦–ä¸ªæ”¯æŒå…‰å­¦æ˜¾å¾®é•œä¸‹çš„å¾®æœºå™¨äººæ„ŸçŸ¥çš„æ•°æ®é›†â€”â€”OpTical MicroRobotï¼ˆOTMRï¼‰æ•°æ®é›†ã€‚</li>
<li>OTMRåŒ…å«æ¶µç›–å¤šç§ç±»å‹å’Œå§¿æ€çš„å¤§é‡å›¾åƒï¼Œä¸ºå¾®æœºå™¨äººæ„ŸçŸ¥ç ”ç©¶æä¾›äº†ä¸°å¯Œèµ„æºã€‚</li>
<li>ç ”ç©¶è¯„ä¼°äº†å¤šä¸ªæ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨OTMRä¸Šçš„è¡¨ç°ï¼Œå‘ç°Vision Transformerï¼ˆViTï¼‰åœ¨å§¿æ€åˆ†ç±»ä»»åŠ¡ä¸Šè¡¨ç°æœ€ä½³ã€‚</li>
<li>æ·±åº¦å›å½’ä»»åŠ¡å—ç›Šäºæ›´æ·±çš„æ¶æ„ï¼Œè¡¨æ˜æ·±åº¦ç¥ç»ç½‘ç»œåœ¨è§£å†³å¾®æœºå™¨äººæ·±åº¦æ„ŸçŸ¥é—®é¢˜ä¸Šçš„æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.18303">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-bb28133b38a5f78f88ad2233e053d7a8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fda9f243188f675a53dd1ee49332b133.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1184ccf221559f08b3042f65569e1d1d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-43dfd46f0af38848479ed4be71052d4b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-99e0b053faa66ef5570efa04a5ef61ca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-180a5d98ba218a12ab972b474cdf1105.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-96ac49b2793af576ad0b4671e328198e.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="ViSIR-Vision-Transformer-Single-Image-Reconstruction-Method-for-Earth-System-Models"><a href="#ViSIR-Vision-Transformer-Single-Image-Reconstruction-Method-for-Earth-System-Models" class="headerlink" title="ViSIR: Vision Transformer Single Image Reconstruction Method for Earth   System Models"></a>ViSIR: Vision Transformer Single Image Reconstruction Method for Earth   System Models</h2><p><strong>Authors:Ehsan Zeraatkar, Salah Faroughi, Jelena TeÅ¡iÄ‡</strong></p>
<p>Purpose: Earth system models (ESMs) integrate the interactions of the atmosphere, ocean, land, ice, and biosphere to estimate the state of regional and global climate under a wide variety of conditions. The ESMs are highly complex; thus, deep neural network architectures are used to model the complexity and store the down-sampled data. This paper proposes the Vision Transformer Sinusoidal Representation Networks (ViSIR) to improve the ESM dataâ€™s single image SR (SR) reconstruction task.   Methods: ViSIR combines the SR capability of Vision Transformers (ViT) with the high-frequency detail preservation of the Sinusoidal Representation Network (SIREN) to address the spectral bias observed in SR tasks.   Results: The ViSIR outperforms SRCNN by 2.16 db, ViT by 6.29 dB, SIREN by 8.34 dB, and SR-Generative Adversarial (SRGANs) by 7.93 dB PSNR on average for three different measurements.   Conclusion: The proposed ViSIR is evaluated and compared with state-of-the-art methods. The results show that the proposed algorithm is outperforming other methods in terms of Mean Square Error(MSE), Peak-Signal-to-Noise-Ratio(PSNR), and Structural Similarity Index Measure(SSIM). </p>
<blockquote>
<p>ç›®çš„ï¼šåœ°çƒç³»ç»Ÿæ¨¡å‹ï¼ˆESMï¼‰æ•´åˆäº†å¤§æ°”ã€æµ·æ´‹ã€é™†åœ°ã€å†°å·å’Œç”Ÿç‰©åœˆçš„ç›¸äº’ä½œç”¨ï¼Œä»¥åœ¨å¤šç§æ¡ä»¶ä¸‹ä¼°è®¡åŒºåŸŸå’Œå…¨çƒæ°”å€™çš„çŠ¶æ€ã€‚ç”±äºåœ°çƒç³»ç»Ÿæ¨¡å‹é«˜åº¦å¤æ‚ï¼Œå› æ­¤ä½¿ç”¨æ·±åº¦ç¥ç»ç½‘ç»œæ¶æ„æ¥å¯¹å…¶è¿›è¡Œå»ºæ¨¡å¹¶å­˜å‚¨é™é‡‡æ ·æ•°æ®ã€‚æœ¬æ–‡æå‡ºVision Transformeræ­£å¼¦è¡¨ç¤ºç½‘ç»œï¼ˆViSIRï¼‰æ¥æ”¹å–„ESMæ•°æ®çš„å•å›¾åƒè¶…åˆ†è¾¨ç‡ï¼ˆSRï¼‰é‡å»ºä»»åŠ¡ã€‚æ–¹æ³•ï¼šViSIRç»“åˆäº†Vision Transformerï¼ˆViTï¼‰çš„è¶…åˆ†è¾¨ç‡èƒ½åŠ›å’Œæ­£å¼¦è¡¨ç¤ºç½‘ç»œï¼ˆSIRENï¼‰çš„é«˜é¢‘ç»†èŠ‚ä¿ç•™åŠŸèƒ½ï¼Œä»¥è§£å†³SRä»»åŠ¡ä¸­è§‚å¯Ÿåˆ°çš„é¢‘è°±åå·®é—®é¢˜ã€‚ç»“æœï¼šViSIRåœ¨ä¸‰ä¸ªä¸åŒæµ‹é‡æŒ‡æ ‡ä¸Šçš„æ€§èƒ½å¹³å‡ä¼˜äºSRCNN 2.16 dBã€ViT 6.29 dBã€SIREN 8.34 dBä»¥åŠSRç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆSRGANsï¼‰7.93 dBçš„å³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰ã€‚ç»“è®ºï¼šå¯¹æå‡ºçš„ViSIRè¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶ä¸æœ€æ–°æ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒã€‚ç»“æœè¡¨æ˜ï¼Œè¯¥ç®—æ³•åœ¨å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰ã€å³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰å’Œç»“æ„ç›¸ä¼¼æ€§æŒ‡æ•°åº¦é‡ï¼ˆSSIMï¼‰æ–¹é¢ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.06741v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ°çƒç³»ç»Ÿæ¨¡å‹ï¼ˆESMsï¼‰çš„å¤æ‚æ€§åŠå…¶åˆ©ç”¨æ·±åº¦ç¥ç»ç½‘ç»œè¿›è¡Œå»ºæ¨¡çš„æ–¹æ³•ã€‚ä¸ºæ”¹è¿›ESMæ•°æ®çš„å•å›¾åƒè¶…åˆ†è¾¨ç‡ï¼ˆSRï¼‰é‡å»ºä»»åŠ¡ï¼Œæœ¬æ–‡æå‡ºäº†ç»“åˆVision Transformerä¸Sinusoidal Representation Networkï¼ˆViSIRï¼‰çš„æ–¹æ³•ã€‚ViSIRç»“åˆäº†Vision Transformerçš„è¶…åˆ†è¾¨ç‡èƒ½åŠ›ä¸Sinusoidal Representation Networkçš„é«˜é¢‘ç»†èŠ‚ä¿ç•™åŠŸèƒ½ï¼Œè§£å†³äº†SRä»»åŠ¡ä¸­çš„å…‰è°±åå·®é—®é¢˜ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒViSIRåœ¨å¤šä¸ªæµ‹é‡æŒ‡æ ‡ä¸Šå‡ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åœ°çƒç³»ç»Ÿæ¨¡å‹ï¼ˆESMsï¼‰æ•´åˆå¤§æ°”ã€æµ·æ´‹ã€é™†åœ°ã€å†°å’Œç”Ÿç‰©åœˆäº¤äº’ä½œç”¨ï¼Œä»¥ä¼°ç®—å„ç§æ¡ä»¶ä¸‹çš„åŒºåŸŸå’Œå…¨çƒæ°”å€™çŠ¶æ€ã€‚</li>
<li>ESMsé«˜åº¦å¤æ‚ï¼Œéœ€è¦ä½¿ç”¨æ·±åº¦ç¥ç»ç½‘ç»œè¿›è¡Œå»ºæ¨¡å¹¶å¤„ç†ä¸‹é‡‡æ ·æ•°æ®ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†Vision Transformer Sinusoidal Representation Networksï¼ˆViSIRï¼‰ä»¥æ”¹è¿›ESMæ•°æ®çš„å•å›¾åƒè¶…åˆ†è¾¨ç‡ï¼ˆSRï¼‰é‡å»ºä»»åŠ¡ã€‚</li>
<li>ViSIRç»“åˆäº†Vision Transformerï¼ˆViTï¼‰çš„è¶…åˆ†è¾¨ç‡èƒ½åŠ›ä¸Sinusoidal Representation Networkï¼ˆSIRENï¼‰çš„é«˜é¢‘ç»†èŠ‚ä¿ç•™åŠŸèƒ½ã€‚</li>
<li>ViSIRè§£å†³äº†SRä»»åŠ¡ä¸­çš„å…‰è°±åå·®é—®é¢˜ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒViSIRåœ¨PSNRä¸Šå¹³å‡ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œå¦‚SRCNNã€ViTã€SIRENå’ŒSRGANsã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.06741">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3e62b054b2998655b1184a31a5dbeca5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-be174a2e28d07e426ffd6d7614642bf4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5095e785dd88f8cebb8a518abf925a5f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1748eeffce68a6376e96a7a853303a70.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b8322996bb773e99a211afca19ea09d9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-00b15907c4908857d6e776dbba49685d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-173e1edcad176d0b8038d2a530a6f571.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Efficient-Lung-Ultrasound-Severity-Scoring-Using-Dedicated-Feature-Extractor"><a href="#Efficient-Lung-Ultrasound-Severity-Scoring-Using-Dedicated-Feature-Extractor" class="headerlink" title="Efficient Lung Ultrasound Severity Scoring Using Dedicated Feature   Extractor"></a>Efficient Lung Ultrasound Severity Scoring Using Dedicated Feature   Extractor</h2><p><strong>Authors:Jiaqi Guo, Yunan Wu, Evangelos Kaimakamis, Georgios Petmezas, Vasileios E. Papageorgiou, Nicos Maglaveras, Aggelos K. Katsaggelos</strong></p>
<p>With the advent of the COVID-19 pandemic, ultrasound imaging has emerged as a promising technique for COVID-19 detection, due to its non-invasive nature, affordability, and portability. In response, researchers have focused on developing AI-based scoring systems to provide real-time diagnostic support. However, the limited size and lack of proper annotation in publicly available ultrasound datasets pose significant challenges for training a robust AI model. This paper proposes MeDiVLAD, a novel pipeline to address the above issue for multi-level lung-ultrasound (LUS) severity scoring. In particular, we leverage self-knowledge distillation to pretrain a vision transformer (ViT) without label and aggregate frame-level features via dual-level VLAD aggregation. We show that with minimal finetuning, MeDiVLAD outperforms conventional fully-supervised methods in both frame- and video-level scoring, while offering classification reasoning with exceptional quality. This superior performance enables key applications such as the automatic identification of critical lung pathology areas and provides a robust solution for broader medical video classification tasks. </p>
<blockquote>
<p>éšç€COVID-19å¤§æµè¡Œçš„åˆ°æ¥ï¼Œç”±äºå…¶æ— åˆ›ã€è´Ÿæ‹…å¾—èµ·å’Œä¾¿æºçš„ç‰¹ç‚¹ï¼Œè¶…å£°æˆåƒä½œä¸ºæ£€æµ‹COVID-19çš„ä¸€ç§æœ‰å‰é€”çš„æŠ€æœ¯åº”è¿è€Œç”Ÿã€‚å› æ­¤ï¼Œç ”ç©¶äººå‘˜è‡´åŠ›äºå¼€å‘åŸºäºäººå·¥æ™ºèƒ½çš„è¯„åˆ†ç³»ç»Ÿï¼Œä»¥æä¾›å®æ—¶è¯Šæ–­æ”¯æŒã€‚ç„¶è€Œï¼Œå…¬å¼€å¯ç”¨çš„è¶…å£°æ•°æ®é›†çš„å¤§å°æœ‰é™ä¸”ç¼ºä¹é€‚å½“çš„æ³¨é‡Šï¼Œè¿™ç»™è®­ç»ƒç¨³å¥çš„äººå·¥æ™ºèƒ½æ¨¡å‹å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ã€‚æœ¬æ–‡é’ˆå¯¹ä¸Šè¿°é—®é¢˜æå‡ºäº†MeDiVLADï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºå¤šçº§è‚ºè¶…å£°ï¼ˆLUSï¼‰ä¸¥é‡ç¨‹åº¦è¯„åˆ†çš„æ–°å‹ç®¡é“ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬åˆ©ç”¨è‡ªæˆ‘çŸ¥è¯†è’¸é¦å¯¹æ— éœ€æ ‡ç­¾çš„è§†ç•Œè½¬æ¢å™¨ï¼ˆViTï¼‰è¿›è¡Œé¢„è®­ç»ƒï¼Œå¹¶é€šè¿‡åŒçº§VLADèšåˆæŠ€æœ¯èšåˆå¸§çº§ç‰¹å¾ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œé€šè¿‡æœ€å°çš„å¾®è°ƒï¼ŒMeDiVLADåœ¨å¸§çº§å’Œè§†é¢‘çº§è¯„åˆ†æ–¹é¢éƒ½ä¼˜äºä¼ ç»Ÿçš„å…¨ç›‘ç£æ–¹æ³•ï¼ŒåŒæ—¶æä¾›å‡ºè‰²çš„åˆ†ç±»æ¨ç†è´¨é‡ã€‚è¿™ç§å“è¶Šçš„æ€§èƒ½èƒ½å¤Ÿå®ç°å…³é”®åº”ç”¨ï¼Œå¦‚è‡ªåŠ¨è¯†åˆ«å…³é”®çš„è‚ºéƒ¨ç—…ç†åŒºåŸŸï¼Œå¹¶ä¸ºæ›´å¹¿æ³›çš„åŒ»å­¦è§†é¢‘åˆ†ç±»ä»»åŠ¡æä¾›ç¨³å¥çš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.12524v3">PDF</a> Accepted by IEEE ISBI 2025 (Selected for oral presentation);   2025&#x2F;4&#x2F;15 (v2): Corrected a notation error in Figure 2</p>
<p><strong>Summary</strong></p>
<p>éšç€COVID-19ç–«æƒ…çš„å‡ºç°ï¼Œè¶…å£°æˆåƒåœ¨æ–°å† ç—…æ¯’æ£€æµ‹æ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚ç ”ç©¶äººå‘˜ä¸ºæ­¤å¼€å‘å‡ºåŸºäºAIçš„è¯„åˆ†ç³»ç»Ÿä»¥æä¾›å®æ—¶è¯Šæ–­æ”¯æŒã€‚ç„¶è€Œï¼Œå…¬å¼€å¯ç”¨çš„è¶…å£°æ•°æ®é›†è§„æ¨¡æœ‰é™ä¸”ç¼ºä¹é€‚å½“æ ‡æ³¨ï¼Œç»™è®­ç»ƒç¨³å¥çš„AIæ¨¡å‹å¸¦æ¥æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºMeDiVLADï¼Œä¸€ä¸ªé’ˆå¯¹å¤šå±‚æ¬¡è‚ºéƒ¨è¶…å£°ï¼ˆLUSï¼‰ä¸¥é‡ç¨‹åº¦è¯„åˆ†é—®é¢˜çš„æ–°å‹ç®¡é“ã€‚æˆ‘ä»¬åˆ©ç”¨è‡ªæˆ‘çŸ¥è¯†è’¸é¦å¯¹è§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰è¿›è¡Œé¢„è®­ç»ƒï¼Œå¹¶é€šè¿‡åŒå±‚æ¬¡VLADèšåˆæ¥æ±‡æ€»å¸§çº§ç‰¹å¾ã€‚ç»“æœè¡¨æ˜ï¼ŒMeDiVLADåœ¨å¸§çº§å’Œè§†é¢‘çº§è¯„åˆ†ä¸Šå‡ä¼˜äºä¼ ç»Ÿå…¨ç›‘ç£æ–¹æ³•ï¼ŒåŒæ—¶åˆ†ç±»æ¨ç†è´¨é‡å“è¶Šã€‚æ­¤å“è¶Šæ€§èƒ½ä½¿å¾—è‡ªåŠ¨è¯†åˆ«å…³é”®è‚ºéƒ¨ç—…ç†åŒºåŸŸæˆä¸ºå¯èƒ½ï¼Œå¹¶ä¸ºæ›´å¹¿æ³›çš„åŒ»ç–—è§†é¢‘åˆ†ç±»ä»»åŠ¡æä¾›äº†ç¨³å¥çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¶…å£°æˆåƒåœ¨æ–°å† ç—…æ¯’æ£€æµ‹ä¸­å…·æœ‰æ½œåŠ›ï¼Œå› å…¶éä¾µå…¥æ€§ã€ç»æµæ€§å’Œä¾¿æºæ€§è€Œå—åˆ°å…³æ³¨ã€‚</li>
<li>AIè¯„åˆ†ç³»ç»Ÿä¸ºå®æ—¶è¯Šæ–­æä¾›äº†æ”¯æŒï¼Œä½†å…¬å¼€å¯ç”¨çš„è¶…å£°æ•°æ®é›†å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>MeDiVLADæ˜¯ä¸€ä¸ªé’ˆå¯¹å¤šå±‚æ¬¡è‚ºéƒ¨è¶…å£°ï¼ˆLUSï¼‰ä¸¥é‡ç¨‹åº¦è¯„åˆ†çš„æ–°å‹ç®¡é“ã€‚</li>
<li>åˆ©ç”¨è‡ªæˆ‘çŸ¥è¯†è’¸é¦å¯¹è§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰è¿›è¡Œé¢„è®­ç»ƒã€‚</li>
<li>é€šè¿‡åŒå±‚æ¬¡VLADèšåˆæ±‡æ€»å¸§çº§ç‰¹å¾ã€‚</li>
<li>MeDiVLADåœ¨å¸§çº§å’Œè§†é¢‘çº§è¯„åˆ†ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä¼˜äºä¼ ç»Ÿå…¨ç›‘ç£æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.12524">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3f66aba725a6778d8db84e965226f6db.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f9e4aaaa220d3f51ac38f15e05ae5cf5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9d4a2774ce9b95efa34198af13a80a6b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d5142fd1ddf9b34d7897321bf085dc1f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6531beca76054e51b66f549a4ae5918e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f2cf6961262eb514e63d8f00efada832.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-28/Vision%20Transformer/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-28/Vision%20Transformer/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Vision-Transformer/">
                                    <span class="chip bg-color">Vision Transformer</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-28/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-5daf8d84c091e449fe0911ad46a33efb.jpg" class="responsive-img" alt="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª">
                        
                        <span class="card-title">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-28  Zero-Shot Pseudo Labels Generation Using SAM and CLIP for   Semi-Supervised Semantic Segmentation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-28
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                    æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                        <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-28/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-973ace8d2abdb60e73065f562f3b487f.jpg" class="responsive-img" alt="è§†é¢‘ç†è§£">
                        
                        <span class="card-title">è§†é¢‘ç†è§£</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            è§†é¢‘ç†è§£ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-28  TUNA Comprehensive Fine-grained Temporal Understanding Evaluation on   Dense Dynamic Videos
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-28
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                    è§†é¢‘ç†è§£
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                        <span class="chip bg-color">è§†é¢‘ç†è§£</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32102k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
