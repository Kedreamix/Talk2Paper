<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-28  Multimodal LLM-Guided Semantic Correction in Text-to-Image Diffusion">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-5206070dc91963da2bc84d1a570f2ada.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-28
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-24
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    16.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    67 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-28-æ›´æ–°"><a href="#2025-05-28-æ›´æ–°" class="headerlink" title="2025-05-28 æ›´æ–°"></a>2025-05-28 æ›´æ–°</h1><h2 id="Multimodal-LLM-Guided-Semantic-Correction-in-Text-to-Image-Diffusion"><a href="#Multimodal-LLM-Guided-Semantic-Correction-in-Text-to-Image-Diffusion" class="headerlink" title="Multimodal LLM-Guided Semantic Correction in Text-to-Image Diffusion"></a>Multimodal LLM-Guided Semantic Correction in Text-to-Image Diffusion</h2><p><strong>Authors:Zheqi Lv, Junhao Chen, Qi Tian, Keting Yin, Shengyu Zhang, Fei Wu</strong></p>
<p>Diffusion models have become the mainstream architecture for text-to-image generation, achieving remarkable progress in visual quality and prompt controllability. However, current inference pipelines generally lack interpretable semantic supervision and correction mechanisms throughout the denoising process. Most existing approaches rely solely on post-hoc scoring of the final image, prompt filtering, or heuristic resampling strategies-making them ineffective in providing actionable guidance for correcting the generative trajectory. As a result, models often suffer from object confusion, spatial errors, inaccurate counts, and missing semantic elements, severely compromising prompt-image alignment and image quality. To tackle these challenges, we propose MLLM Semantic-Corrected Ping-Pong-Ahead Diffusion (PPAD), a novel framework that, for the first time, introduces a Multimodal Large Language Model (MLLM) as a semantic observer during inference. PPAD performs real-time analysis on intermediate generations, identifies latent semantic inconsistencies, and translates feedback into controllable signals that actively guide the remaining denoising steps. The framework supports both inference-only and training-enhanced settings, and performs semantic correction at only extremely few diffusion steps, offering strong generality and scalability. Extensive experiments demonstrate PPADâ€™s significant improvements. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹å·²æˆä¸ºæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„ä¸»æµæ¶æ„ï¼Œåœ¨è§†è§‰è´¨é‡å’Œæç¤ºå¯æ§æ€§æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ¨ç†ç®¡é“é€šå¸¸åœ¨å»å™ªè¿‡ç¨‹ä¸­ç¼ºä¹å¯è§£é‡Šçš„è¯­ä¹‰ç›‘ç£å’Œæ ¡æ­£æœºåˆ¶ã€‚å¤§å¤šæ•°ç°æœ‰æ–¹æ³•ä»…ä¾èµ–äºæœ€ç»ˆå›¾åƒçš„åéªŒè¯„åˆ†ã€æç¤ºè¿‡æ»¤æˆ–å¯å‘å¼é‡é‡‡æ ·ç­–ç•¥ï¼Œè¿™ä½¿å¾—å®ƒä»¬æ— æ³•ä¸ºçº æ­£ç”Ÿæˆè½¨è¿¹æä¾›æœ‰æ•ˆçš„æŒ‡å¯¼ã€‚å› æ­¤ï¼Œæ¨¡å‹ç»å¸¸é­å—ç›®æ ‡æ··æ·†ã€ç©ºé—´é”™è¯¯ã€è®¡æ•°ä¸å‡†ç¡®å’Œç¼ºå¤±è¯­ä¹‰å…ƒç´ ç­‰é—®é¢˜çš„å½±å“ï¼Œä¸¥é‡æŸå®³äº†æç¤º-å›¾åƒå¯¹é½å’Œå›¾åƒè´¨é‡ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†MLLMè¯­ä¹‰æ ¡æ­£ä¹’ä¹“å‰å‘æ‰©æ•£ï¼ˆPPADï¼‰è¿™ä¸€æ–°æ¡†æ¶ï¼Œå®ƒé¦–æ¬¡åœ¨æ¨ç†è¿‡ç¨‹ä¸­å¼•å…¥äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ä½œä¸ºè¯­ä¹‰è§‚å¯Ÿè€…ã€‚PPADå¯¹ä¸­é—´ç”Ÿæˆç‰©è¿›è¡Œå®æ—¶åˆ†æï¼Œè¯†åˆ«æ½œåœ¨çš„è¯­ä¹‰ä¸ä¸€è‡´ï¼Œå¹¶å°†åé¦ˆè½¬åŒ–ä¸ºå¯æ§ä¿¡å·ï¼Œä¸»åŠ¨å¼•å¯¼å‰©ä½™çš„å»å™ªæ­¥éª¤ã€‚è¯¥æ¡†æ¶æ”¯æŒä»…æ¨ç†å’Œè®­ç»ƒå¢å¼ºçš„è®¾ç½®ï¼Œå¹¶åœ¨æå°‘çš„æ‰©æ•£æ­¥éª¤ä¸­è¿›è¡Œè¯­ä¹‰æ ¡æ­£ï¼Œè¡¨ç°å‡ºå¼ºå¤§çš„é€šç”¨æ€§å’Œå¯æ‰©å±•æ€§ã€‚å¤§é‡å®éªŒè¯æ˜äº†PPADçš„æ˜¾è‘—æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20053v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹å·²æˆä¸ºæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„ä¸»æµæ¶æ„ï¼Œå…¶åœ¨è§†è§‰è´¨é‡å’Œæç¤ºå¯æ§æ€§æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œå½“å‰æ¨ç†ç®¡é“æ™®éç¼ºä¹å¯è§£é‡Šçš„è¯­ä¹‰ç›‘ç£å’Œæ ¡æ­£æœºåˆ¶è´¯ç©¿å»å™ªè¿‡ç¨‹ã€‚é’ˆå¯¹æ¨¡å‹å¯¹è±¡æ··æ·†ã€ç©ºé—´è¯¯å·®ã€è®¡æ•°ä¸å‡†ç¡®å’Œç¼ºå°‘è¯­ä¹‰å…ƒç´ ç­‰é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†MLLMè¯­ä¹‰æ ¡æ­£ä¹’ä¹“å‰å‘æ‰©æ•£ï¼ˆPPADï¼‰æ–°æ¡†æ¶ã€‚è¯¥æ¡†æ¶é¦–æ¬¡å¼•å…¥å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ä½œä¸ºæ¨ç†è¿‡ç¨‹ä¸­çš„è¯­ä¹‰è§‚å¯Ÿè€…ï¼Œå®æ—¶åˆ†æä¸­é—´ç”Ÿæˆå†…å®¹ï¼Œè¯†åˆ«æ½œåœ¨è¯­ä¹‰ä¸ä¸€è‡´æ€§ï¼Œå¹¶å°†åé¦ˆè½¬åŒ–ä¸ºå¯æ§ä¿¡å·ï¼Œä¸»åŠ¨å¼•å¯¼å‰©ä½™çš„å»å™ªæ­¥éª¤ã€‚è¯¥æ¡†æ¶æ”¯æŒä»…æ¨ç†å’Œè®­ç»ƒå¢å¼ºè®¾ç½®ï¼Œåœ¨æå°‘æ‰©æ•£æ­¥éª¤ä¸­è¿›è¡Œè¯­ä¹‰æ ¡æ­£ï¼Œè¡¨ç°å‡ºå¼ºå¤§çš„é€šç”¨æ€§å’Œå¯æ‰©å±•æ€§ã€‚å®éªŒè¯æ˜PPADçš„æ”¹è¿›éå¸¸æ˜¾è‘—ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹å·²æˆä¸ºæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„ä¸»æµæ¶æ„ã€‚</li>
<li>å½“å‰æ‰©æ•£æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­ç¼ºä¹è¯­ä¹‰ç›‘ç£å’Œæ ¡æ­£æœºåˆ¶ã€‚</li>
<li>æ¨¡å‹å¸¸é¢ä¸´å¯¹è±¡æ··æ·†ã€ç©ºé—´è¯¯å·®ã€è®¡æ•°ä¸å‡†ç¡®å’Œç¼ºå°‘è¯­ä¹‰å…ƒç´ ç­‰é—®é¢˜ã€‚</li>
<li>PPADæ¡†æ¶é€šè¿‡å¼•å…¥å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ä½œä¸ºè¯­ä¹‰è§‚å¯Ÿè€…æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>PPADèƒ½å®æ—¶åˆ†æä¸­é—´ç”Ÿæˆå†…å®¹ï¼Œè¯†åˆ«è¯­ä¹‰ä¸ä¸€è‡´æ€§ï¼Œå¹¶è½¬åŒ–ä¸ºå¯æ§ä¿¡å·å¼•å¯¼å»å™ªæ­¥éª¤ã€‚</li>
<li>PPADæ¡†æ¶æ”¯æŒä»…æ¨ç†å’Œè®­ç»ƒå¢å¼ºä¸¤ç§è®¾ç½®ï¼Œå¯åœ¨æå°‘æ­¥éª¤ä¸­è¿›è¡Œè¯­ä¹‰æ ¡æ­£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20053">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2d163edf1f85418b31048a66acedbf2c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-74b84af17a6060d1bc5f8930042dae0a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e04e42cfe4477aee2cb00f5a1f1fe01a.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="UltraVSR-Achieving-Ultra-Realistic-Video-Super-Resolution-with-Efficient-One-Step-Diffusion-Space"><a href="#UltraVSR-Achieving-Ultra-Realistic-Video-Super-Resolution-with-Efficient-One-Step-Diffusion-Space" class="headerlink" title="UltraVSR: Achieving Ultra-Realistic Video Super-Resolution with   Efficient One-Step Diffusion Space"></a>UltraVSR: Achieving Ultra-Realistic Video Super-Resolution with   Efficient One-Step Diffusion Space</h2><p><strong>Authors:Yong Liu, Jinshan Pan, Yinchuan Li, Qingji Dong, Chao Zhu, Yu Guo, Fei Wang</strong></p>
<p>Diffusion models have shown great potential in generating realistic image detail. However, adapting these models to video super-resolution (VSR) remains challenging due to their inherent stochasticity and lack of temporal modeling. In this paper, we propose UltraVSR, a novel framework that enables ultra-realistic and temporal-coherent VSR through an efficient one-step diffusion space. A central component of UltraVSR is the Degradation-aware Restoration Schedule (DRS), which estimates a degradation factor from the low-resolution input and transforms iterative denoising process into a single-step reconstruction from from low-resolution to high-resolution videos. This design eliminates randomness from diffusion noise and significantly speeds up inference. To ensure temporal consistency, we propose a lightweight yet effective Recurrent Temporal Shift (RTS) module, composed of an RTS-convolution unit and an RTS-attention unit. By partially shifting feature components along the temporal dimension, these two units collaboratively facilitate effective feature propagation, fusion, and alignment across neighboring frames, without relying on explicit temporal layers. The RTS module is integrated into a pretrained text-to-image diffusion model and is further enhanced through Spatio-temporal Joint Distillation (SJD), which improves temporal coherence while preserving realistic details. Additionally, we introduce a Temporally Asynchronous Inference (TAI) strategy to capture long-range temporal dependencies under limited memory constraints. Extensive experiments show that UltraVSR achieves state-of-the-art performance, both qualitatively and quantitatively, in a single sampling step. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆé€¼çœŸçš„å›¾åƒç»†èŠ‚æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œç”±äºè¿™äº›æ¨¡å‹å›ºæœ‰çš„éšæœºæ€§å’Œç¼ºä¹æ—¶é—´å»ºæ¨¡ï¼Œå°†è¿™äº›æ¨¡å‹é€‚åº”äºè§†é¢‘è¶…åˆ†è¾¨ç‡ï¼ˆVSRï¼‰ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†UltraVSRï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æ¡†æ¶ï¼Œèƒ½å¤Ÿé€šè¿‡é«˜æ•ˆçš„ä¸€æ¬¡æ€§æ‰©æ•£ç©ºé—´å®ç°è¶…é€¼çœŸå’Œæ—¶é—´è¿è´¯çš„è§†é¢‘è¶…åˆ†è¾¨ç‡ã€‚UltraVSRçš„æ ¸å¿ƒç»„ä»¶æ˜¯é€€åŒ–æ„ŸçŸ¥æ¢å¤è®¡åˆ’ï¼ˆDRSï¼‰ï¼Œå®ƒæ ¹æ®ä½åˆ†è¾¨ç‡è¾“å…¥ä¼°è®¡é€€åŒ–å› å­ï¼Œå¹¶å°†è¿­ä»£å»å™ªè¿‡ç¨‹è½¬å˜ä¸ºä»ä½åˆ†è¾¨ç‡åˆ°é«˜åˆ†è¾¨ç‡è§†é¢‘çš„ä¸€æ¬¡æ€§é‡å»ºã€‚è¿™ç§è®¾è®¡æ¶ˆé™¤äº†æ‰©æ•£å™ªå£°çš„éšæœºæ€§ï¼Œå¹¶æ˜¾è‘—åŠ å¿«äº†æ¨ç†é€Ÿåº¦ã€‚ä¸ºäº†ç¡®ä¿æ—¶é—´è¿è´¯æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªè½»é‡çº§ä½†æœ‰æ•ˆçš„å¾ªç¯æ—¶é—´ç§»ä½ï¼ˆRTSï¼‰æ¨¡å—ï¼Œå®ƒç”±RTSå·ç§¯å•å…ƒå’ŒRTSæ³¨æ„åŠ›å•å…ƒç»„æˆã€‚è¿™ä¸¤ä¸ªå•å…ƒé€šè¿‡æ²¿æ—¶é—´ç»´åº¦éƒ¨åˆ†ç§»åŠ¨ç‰¹å¾åˆ†é‡ï¼ŒååŒå®ç°æœ‰æ•ˆç‰¹å¾ä¼ æ’­ã€èåˆå’Œå¯¹é½ç›¸é‚»å¸§ï¼Œæ— éœ€ä¾èµ–æ˜¾å¼çš„æ—¶é—´å±‚ã€‚RTSæ¨¡å—è¢«é›†æˆåˆ°é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­ï¼Œå¹¶é€šè¿‡æ—¶ç©ºè”åˆè’¸é¦ï¼ˆSJDï¼‰å¾—åˆ°è¿›ä¸€æ­¥æ”¹è¿›ï¼Œè¿™æé«˜äº†æ—¶é—´è¿è´¯æ€§ï¼ŒåŒæ—¶ä¿ç•™äº†é€¼çœŸçš„ç»†èŠ‚ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ—¶é—´ä¸Šå¼‚æ­¥æ¨ç†ï¼ˆTAIï¼‰ç­–ç•¥ï¼Œä»¥åœ¨æœ‰é™çš„å†…å­˜çº¦æŸä¸‹æ•è·é•¿æœŸæ—¶é—´ä¾èµ–æ€§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒUltraVSRåœ¨å•æ¬¡é‡‡æ ·æ­¥éª¤ä¸­å®ç°äº†å®šæ€§å’Œå®šé‡ä¸Šçš„å…ˆè¿›æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19958v1">PDF</a> Under review, 10 pages, 7 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºUltraVSRçš„æ–°å‹æ¡†æ¶ï¼Œç”¨äºé€šè¿‡é«˜æ•ˆçš„ä¸€æ­¥æ‰©æ•£ç©ºé—´å®ç°è¶…é€¼çœŸä¸”æ—¶é—´è¿è´¯çš„è§†é¢‘è¶…åˆ†è¾¨ç‡ï¼ˆVSRï¼‰ã€‚è¯¥æ¡†æ¶é€šè¿‡å¼•å…¥é™è´¨æ„ŸçŸ¥æ¢å¤è°ƒåº¦ï¼ˆDRSï¼‰æ¶ˆé™¤æ‰©æ•£å™ªå£°çš„éšæœºæ€§ï¼Œå¹¶åŠ é€Ÿæ¨ç†ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜æå‡ºä¸€ç§è½»é‡çº§çš„å¾ªç¯æ—¶é—´ç§»ä½ï¼ˆRTSï¼‰æ¨¡å—ï¼Œé€šè¿‡éƒ¨åˆ†æ²¿æ—¶é—´ç»´åº¦ç§»åŠ¨ç‰¹å¾ç»„ä»¶ï¼Œå®ç°è·¨ç›¸é‚»å¸§çš„æœ‰æ•ˆç‰¹å¾ä¼ æ’­ã€èåˆå’Œå¯¹é½ã€‚ç»“åˆæ—¶ç©ºè”åˆè’¸é¦ï¼ˆSJDï¼‰æŠ€æœ¯ï¼ŒUltraVSRåœ¨ä¸ä¾èµ–æ˜ç¡®æ—¶é—´å±‚çš„æƒ…å†µä¸‹ï¼Œæå‡äº†æ—¶é—´è¿è´¯æ€§å¹¶ä¿æŒçœŸå®ç»†èŠ‚ã€‚å®éªŒè¡¨æ˜ï¼ŒUltraVSRåœ¨å•æ­¥é‡‡æ ·ä¸­å®ç°äº†å…ˆè¿›æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>UltraVSRæ¡†æ¶æˆåŠŸå°†æ‰©æ•£æ¨¡å‹åº”ç”¨äºè§†é¢‘è¶…åˆ†è¾¨ç‡ï¼Œç”Ÿæˆè¶…é€¼çœŸä¸”æ—¶é—´è¿è´¯çš„è§†é¢‘ã€‚</li>
<li>é€šè¿‡é™è´¨æ„ŸçŸ¥æ¢å¤è°ƒåº¦ï¼ˆDRSï¼‰ï¼Œå®ç°äº†ä»ä½åˆ†è¾¨ç‡åˆ°é«˜åˆ†è¾¨ç‡è§†é¢‘çš„å•ä¸€æ­¥éª¤é‡å»ºï¼Œæ¶ˆé™¤äº†æ‰©æ•£æ¨¡å‹çš„éšæœºæ€§å¹¶åŠ é€Ÿäº†æ¨ç†è¿‡ç¨‹ã€‚</li>
<li>å¾ªç¯æ—¶é—´ç§»ä½ï¼ˆRTSï¼‰æ¨¡å—ç¡®ä¿æ—¶é—´è¿è´¯æ€§ï¼Œé€šè¿‡ç‰¹å¾ä¼ æ’­ã€èåˆå’Œå¯¹é½æå‡æ€§èƒ½ã€‚</li>
<li>æ—¶ç©ºè”åˆè’¸é¦ï¼ˆSJDï¼‰æŠ€æœ¯æé«˜äº†æ—¶é—´è¿è´¯æ€§çš„åŒæ—¶ï¼Œä¿ç•™äº†çœŸå®ç»†èŠ‚ã€‚</li>
<li>UltraVSRé€šè¿‡å¼•å…¥å¼‚æ­¥æ¨ç†ç­–ç•¥ï¼ˆTAIï¼‰ï¼Œåœ¨æœ‰é™å†…å­˜çº¦æŸä¸‹æ•è·é•¿ç¨‹æ—¶é—´ä¾èµ–æ€§ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒUltraVSRåœ¨å•æ­¥é‡‡æ ·ä¸­å®ç°äº†å…ˆè¿›æ€§èƒ½ï¼Œæ— è®ºæ˜¯å®šæ€§è¿˜æ˜¯å®šé‡è¯„ä¼°ã€‚</li>
<li>UltraVSRæ¡†æ¶å¯¹äºè§†é¢‘è¶…åˆ†è¾¨ç‡ä»»åŠ¡å…·æœ‰æ½œåœ¨çš„å®é™…åº”ç”¨ä»·å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19958">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9989c01659ad2456d7cf856954fd06d6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-edace4782909523e5a8867125a023dab.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-52a865aa52cdb4337f98065661dc9a38.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-702b4109e658118033b1f3253d9389ad.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1c14cc06ee43da87a6f2432beda18b0c.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="TeViR-Text-to-Video-Reward-with-Diffusion-Models-for-Efficient-Reinforcement-Learning"><a href="#TeViR-Text-to-Video-Reward-with-Diffusion-Models-for-Efficient-Reinforcement-Learning" class="headerlink" title="TeViR: Text-to-Video Reward with Diffusion Models for Efficient   Reinforcement Learning"></a>TeViR: Text-to-Video Reward with Diffusion Models for Efficient   Reinforcement Learning</h2><p><strong>Authors:Yuhui Chen, Haoran Li, Zhennan Jiang, Haowei Wen, Dongbin Zhao</strong></p>
<p>Developing scalable and generalizable reward engineering for reinforcement learning (RL) is crucial for creating general-purpose agents, especially in the challenging domain of robotic manipulation. While recent advances in reward engineering with Vision-Language Models (VLMs) have shown promise, their sparse reward nature significantly limits sample efficiency. This paper introduces TeViR, a novel method that leverages a pre-trained text-to-video diffusion model to generate dense rewards by comparing the predicted image sequence with current observations. Experimental results across 11 complex robotic tasks demonstrate that TeViR outperforms traditional methods leveraging sparse rewards and other state-of-the-art (SOTA) methods, achieving better sample efficiency and performance without ground truth environmental rewards. TeViRâ€™s ability to efficiently guide agents in complex environments highlights its potential to advance reinforcement learning applications in robotic manipulation. </p>
<blockquote>
<p>å¼€å‘å¯æ‰©å±•ä¸”å¯é€šç”¨çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¥–åŠ±å·¥ç¨‹å¯¹äºåˆ›å»ºé€šç”¨æ™ºèƒ½ä½“è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨æœºå™¨äººæ“ä½œè¿™ä¸€æŒ‘æˆ˜æ€§é¢†åŸŸã€‚è™½ç„¶æœ€è¿‘ä½¿ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„å¥–åŠ±å·¥ç¨‹è¿›å±•æ˜¾ç¤ºå‡ºäº†ä¸€å®šçš„å‰æ™¯ï¼Œä½†å…¶ç¨€ç–å¥–åŠ±çš„ç‰¹æ€§æå¤§åœ°é™åˆ¶äº†æ ·æœ¬æ•ˆç‡ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°æ–¹æ³•TeViRï¼Œå®ƒåˆ©ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°è§†é¢‘æ‰©æ•£æ¨¡å‹æ¥ç”Ÿæˆå¯†é›†å¥–åŠ±ï¼Œé€šè¿‡æ¯”è¾ƒé¢„æµ‹çš„å›¾åƒåºåˆ—ä¸å½“å‰è§‚å¯Ÿç»“æœæ¥å®ç°ã€‚åœ¨è·¨è¶Š11ä¸ªå¤æ‚æœºå™¨äººä»»åŠ¡çš„å®éªŒä¸­ï¼ŒTeViRçš„è¡¨ç°ä¼˜äºåˆ©ç”¨ç¨€ç–å¥–åŠ±çš„ä¼ ç»Ÿæ–¹æ³•ä»¥åŠå…¶ä»–æœ€æ–°æŠ€æœ¯æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨æ²¡æœ‰çœŸå®ç¯å¢ƒå¥–åŠ±çš„æƒ…å†µä¸‹å®ç°æ›´å¥½çš„æ ·æœ¬æ•ˆç‡å’Œæ€§èƒ½ã€‚TeViRåœ¨å¤æ‚ç¯å¢ƒä¸­æœ‰æ•ˆå¼•å¯¼æ™ºèƒ½ä½“çš„èƒ½åŠ›å‡¸æ˜¾äº†å…¶æ¨åŠ¨æœºå™¨äººåœ¨å¼ºåŒ–å­¦ä¹ åº”ç”¨æ–¹é¢çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19769v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºé¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°è§†é¢‘æ‰©æ•£æ¨¡å‹TeViRï¼Œé€šè¿‡å¯¹æ¯”é¢„æµ‹å›¾åƒåºåˆ—ä¸å½“å‰è§‚æµ‹å€¼ç”Ÿæˆå¯†é›†å¥–åŠ±ï¼Œæé«˜äº†æ ·æœ¬æ•ˆç‡å’Œæ€§èƒ½ã€‚åœ¨11é¡¹å¤æ‚çš„æœºå™¨äººä»»åŠ¡ä¸­ï¼ŒTeViRè¡¨ç°ä¼˜äºä¼ ç»Ÿç¨€ç–å¥–åŠ±æ–¹æ³•å’Œå…¶å®ƒå…ˆè¿›æ–¹æ³•ï¼Œæ— éœ€ç¯å¢ƒçœŸå®å¥–åŠ±å³å¯æŒ‡å¯¼æ™ºèƒ½ä½“åœ¨å¤æ‚ç¯å¢ƒä¸­é«˜æ•ˆè¿è¡Œï¼Œä¸ºæœºå™¨äººæ“çºµé¢†åŸŸçš„å¼ºåŒ–å­¦ä¹ åº”ç”¨å¸¦æ¥æ½œåœ¨è¿›æ­¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TeViRåˆ©ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°è§†é¢‘æ‰©æ•£æ¨¡å‹ç”Ÿæˆå¯†é›†å¥–åŠ±ï¼Œè§£å†³äº†ä¼ ç»Ÿå¥–åŠ±å·¥ç¨‹ä¸­çš„æ ·æœ¬æ•ˆç‡é—®é¢˜ã€‚</li>
<li>TeViRé€šè¿‡å¯¹æ¯”é¢„æµ‹å›¾åƒåºåˆ—ä¸å½“å‰è§‚æµ‹å€¼æ¥å·¥ä½œï¼Œè¿™åœ¨æœºå™¨äººæ“çºµé¢†åŸŸå…·æœ‰å¹¿æ³›åº”ç”¨ã€‚</li>
<li>åœ¨11é¡¹å¤æ‚çš„æœºå™¨äººä»»åŠ¡ä¸­ï¼ŒTeViRè¡¨ç°ä¼˜äºå…¶å®ƒæ–¹æ³•å’Œä¼ ç»Ÿç¨€ç–å¥–åŠ±æ–¹æ³•ã€‚</li>
<li>TeViRèƒ½å¤Ÿåœ¨æ²¡æœ‰ç¯å¢ƒçœŸå®å¥–åŠ±çš„æƒ…å†µä¸‹æŒ‡å¯¼æ™ºèƒ½ä½“è¿è¡Œï¼Œå¢å¼ºäº†å…¶åœ¨å¤æ‚ç¯å¢ƒä¸­çš„é€‚åº”æ€§ã€‚</li>
<li>TeViRçš„å¼•å…¥æœ‰åŠ©äºæ¨åŠ¨å¼ºåŒ–å­¦ä¹ åœ¨æœºå™¨äººæ“çºµé¢†åŸŸçš„åº”ç”¨è¿›æ­¥ã€‚</li>
<li>è¯¥æ–¹æ³•å……åˆ†åˆ©ç”¨äº†æ‰©æ•£æ¨¡å‹çš„æ½œåŠ›ï¼Œå±•ç¤ºäº†å…¶åœ¨ç”Ÿæˆå¯†é›†å¥–åŠ±æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19769">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-14fde474d73c2baff819744a0fe59eb4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f21cc14c296b6a3ec3c1830f86d1f503.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aa8ae46913a97b71da037b2ad7de72d6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6151e6ad66c5f8c98c7dd2ea078ea248.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1c56f47cabdb3dc3a389181c0c7fdea1.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Extremum-Flow-Matching-for-Offline-Goal-Conditioned-Reinforcement-Learning"><a href="#Extremum-Flow-Matching-for-Offline-Goal-Conditioned-Reinforcement-Learning" class="headerlink" title="Extremum Flow Matching for Offline Goal Conditioned Reinforcement   Learning"></a>Extremum Flow Matching for Offline Goal Conditioned Reinforcement   Learning</h2><p><strong>Authors:Quentin Rouxel, Clemente Donoso, Fei Chen, Serena Ivaldi, Jean-Baptiste Mouret</strong></p>
<p>Imitation learning is a promising approach for enabling generalist capabilities in humanoid robots, but its scaling is fundamentally constrained by the scarcity of high-quality expert demonstrations. This limitation can be mitigated by leveraging suboptimal, open-ended play data, often easier to collect and offering greater diversity. This work builds upon recent advances in generative modeling, specifically Flow Matching, an alternative to Diffusion models. We introduce a method for estimating the extremum of the learned distribution by leveraging the unique properties of Flow Matching, namely, deterministic transport and support for arbitrary source distributions. We apply this method to develop several goal-conditioned imitation and reinforcement learning algorithms based on Flow Matching, where policies are conditioned on both current and goal observations. We explore and compare different architectural configurations by combining core components, such as critic, planner, actor, or world model, in various ways. We evaluated our agents on the OGBench benchmark and analyzed how different demonstration behaviors during data collection affect performance in a 2D non-prehensile pushing task. Furthermore, we validated our approach on real hardware by deploying it on the Talos humanoid robot to perform complex manipulation tasks based on high-dimensional image observations, featuring a sequence of pick-and-place and articulated object manipulation in a realistic kitchen environment. Experimental videos and code are available at: <a target="_blank" rel="noopener" href="https://hucebot.github.io/extremum_flow_matching_website/">https://hucebot.github.io/extremum_flow_matching_website/</a> </p>
<blockquote>
<p>æ¨¡ä»¿å­¦ä¹ æ˜¯åœ¨äººå½¢æœºå™¨äººä¸­å®ç°é€šç”¨èƒ½åŠ›çš„ä¸€ç§æœ‰å‰é€”çš„æ–¹æ³•ï¼Œä½†å…¶æ‰©å±•ä»æ ¹æœ¬ä¸Šå—åˆ°é«˜è´¨é‡ä¸“å®¶æ¼”ç¤ºç¨€ç¼ºæ€§çš„é™åˆ¶ã€‚é€šè¿‡åˆ©ç”¨æ¬¡ä¼˜ã€å¼€æ”¾å¼çš„æ¸¸æˆæ•°æ®ï¼Œå¯ä»¥ç¼“è§£è¿™ç§é™åˆ¶ï¼Œè¿™äº›æ•°æ®é€šå¸¸æ›´å®¹æ˜“æ”¶é›†å¹¶ä¸”å…·æœ‰æ›´å¤§çš„å¤šæ ·æ€§ã€‚è¿™é¡¹å·¥ä½œå»ºç«‹åœ¨ç”Ÿæˆå»ºæ¨¡çš„æœ€æ–°è¿›å±•ä¹‹ä¸Šï¼Œç‰¹åˆ«æ˜¯æµåŒ¹é…ï¼ˆFlow Matchingï¼‰æŠ€æœ¯ï¼Œå®ƒæ˜¯æ‰©æ•£æ¨¡å‹çš„ä¸€ç§æ›¿ä»£æ–¹æ¡ˆã€‚æˆ‘ä»¬ä»‹ç»äº†ä¸€ç§åˆ©ç”¨æµåŒ¹é…ç‹¬ç‰¹å±æ€§æ¥ä¼°è®¡æ‰€å­¦åˆ†å¸ƒæå€¼çš„æ–¹æ³•ï¼Œå³ç¡®å®šæ€§ä¼ è¾“å’Œå¯¹ä»»æ„æºåˆ†å¸ƒçš„æ”¯æŒã€‚æˆ‘ä»¬åº”ç”¨è¿™ç§æ–¹æ³•ï¼ŒåŸºäºæµåŒ¹é…å¼€å‘äº†å‡ ç§ç›®æ ‡æ¡ä»¶æ¨¡ä»¿å’Œå¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œè¿™äº›ç®—æ³•çš„ç­–ç•¥æ—¢å–å†³äºå½“å‰è§‚å¯Ÿä¹Ÿå–å†³äºç›®æ ‡è§‚å¯Ÿã€‚æˆ‘ä»¬é€šè¿‡ç»„åˆæ ¸å¿ƒç»„ä»¶ï¼ˆå¦‚è¯„è®ºå®¶ã€è§„åˆ’å¸ˆã€æ¼”å‘˜æˆ–ä¸–ç•Œæ¨¡å‹ï¼‰çš„ä¸åŒæ–¹å¼ï¼Œæ¢ç´¢å’Œæ¯”è¾ƒäº†ä¸åŒçš„æ¶æ„é…ç½®ã€‚æˆ‘ä»¬åœ¨OGBenchåŸºå‡†æµ‹è¯•ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„ä»£ç†ï¼Œå¹¶åˆ†æäº†åœ¨æ•°æ®æ”¶é›†è¿‡ç¨‹ä¸­ä¸åŒæ¼”ç¤ºè¡Œä¸ºå¯¹äºŒç»´éæŠ“å–æ¨åŠ¨ä»»åŠ¡æ€§èƒ½çš„å½±å“ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡å°†æ–¹æ³•éƒ¨ç½²åœ¨Talosäººå½¢æœºå™¨äººä¸Šæ‰§è¡ŒåŸºäºé«˜ç»´å›¾åƒè§‚å¯Ÿçš„å¤æ‚æ“ä½œä»»åŠ¡æ¥éªŒè¯æˆ‘ä»¬çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬ä¸€ç³»åˆ—æ‹¿èµ·æ”¾ä¸‹å’Œåœ¨ç°å®å¨æˆ¿ç¯å¢ƒä¸­æ“ä½œå…³èŠ‚ç‰©ä½“ã€‚å®éªŒè§†é¢‘å’Œä»£ç å¯é€šè¿‡ä»¥ä¸‹ç½‘å€è·å¾—ï¼š[<a target="_blank" rel="noopener" href="https://hucebot.github.io/extremum_flow_matching_website/]">https://hucebot.github.io/extremum_flow_matching_website/]</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19717v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶åˆ©ç”¨ç”Ÿæˆå»ºæ¨¡çš„æœ€æ–°è¿›å±•ï¼Œç‰¹åˆ«æ˜¯æ›¿ä»£æ‰©æ•£æ¨¡å‹çš„æµé‡åŒ¹é…æŠ€æœ¯ï¼Œé€šè¿‡åˆ©ç”¨å…¶ç‹¬ç‰¹å±æ€§ï¼ˆå¦‚ç¡®å®šæ€§ä¼ è¾“å’Œä»»æ„æºåˆ†å¸ƒæ”¯æŒï¼‰ï¼Œä¼°è®¡å­¦ä¹ åˆ†å¸ƒæå€¼çš„æ–¹æ³•ã€‚ç ”ç©¶å¼€å‘äº†åŸºäºæµé‡åŒ¹é…çš„å¤šç§ç›®æ ‡å¯¼å‘æ¨¡ä»¿å’Œå¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œç­–ç•¥åŸºäºå½“å‰å’Œç›®æ ‡è§‚å¯Ÿç»“æœã€‚åœ¨OGBenchåŸºå‡†æµ‹è¯•ä¸Šè¯„ä¼°äº†ä»£ç†ï¼Œå¹¶åˆ†æäº†æ•°æ®æ”¶é›†è¿‡ç¨‹ä¸­ä¸åŒæ¼”ç¤ºè¡Œä¸ºå¯¹äºŒç»´éæŠ“å–æ¨åŠ¨ä»»åŠ¡æ€§èƒ½çš„å½±å“ã€‚æ­¤å¤–ï¼Œåœ¨Talosäººå½¢æœºå™¨äººä¸ŠéªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œä»¥æ‰§è¡ŒåŸºäºé«˜ç»´å›¾åƒè§‚å¯Ÿçš„å¤æ‚æ“çºµä»»åŠ¡ã€‚è¯¦æƒ…å¯è§ï¼š<a target="_blank" rel="noopener" href="https://hucebot.github.io/extremum_flow_matching_website/%E3%80%82">https://hucebot.github.io/extremum_flow_matching_website/ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶ä½¿ç”¨æµé‡åŒ¹é…æŠ€æœ¯ï¼Œä¸€ç§ç”Ÿæˆå»ºæ¨¡çš„æ›¿ä»£æ–¹æ³•ï¼Œæ¥ä¼°è®¡å­¦ä¹ åˆ†å¸ƒçš„æå€¼ã€‚</li>
<li>æ–¹æ³•åˆ©ç”¨æµé‡åŒ¹é…çš„ç‹¬ç‰¹å±æ€§ï¼ŒåŒ…æ‹¬ç¡®å®šæ€§ä¼ è¾“å’Œä»»æ„æºåˆ†å¸ƒæ”¯æŒã€‚</li>
<li>å¼€å‘åŸºäºæµé‡åŒ¹é…çš„æ¨¡ä»¿å’Œå¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œè¿™äº›ç®—æ³•åŸºäºå½“å‰å’Œç›®æ ‡è§‚å¯Ÿç»“æœã€‚</li>
<li>åœ¨OGBenchåŸºå‡†æµ‹è¯•ä¸Šè¯„ä¼°äº†ç®—æ³•æ€§èƒ½ã€‚</li>
<li>ç ”ç©¶åˆ†æäº†æ¼”ç¤ºè¡Œä¸ºåœ¨æ•°æ®æ”¶é›†è¿‡ç¨‹ä¸­å¯¹ä»»åŠ¡æ€§èƒ½çš„å½±å“ã€‚</li>
<li>åœ¨Talosäººå½¢æœºå™¨äººä¸ŠéªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå±•ç¤ºäº†æ‰§è¡Œå¤æ‚æ“çºµä»»åŠ¡çš„èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19717">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7ce8b9dda87090ebe9589bc3f567d4bc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1f7ed9ba20c9134f82f012e171d89602.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-88c207c997a7faaca95c9b5b746a51c3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e9d6d5f5cfd62bba364483e8517cd0de.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cd15559aebb3472db53c1661590af29e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-950e9b877e4861dd73ea56c32bd6c88f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-332d894f96a718ec5029d2baf701b1e6.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Knowledge-Aligned-Counterfactual-Enhancement-Diffusion-Perception-for-Unsupervised-Cross-Domain-Visual-Emotion-Recognition"><a href="#Knowledge-Aligned-Counterfactual-Enhancement-Diffusion-Perception-for-Unsupervised-Cross-Domain-Visual-Emotion-Recognition" class="headerlink" title="Knowledge-Aligned Counterfactual-Enhancement Diffusion Perception for   Unsupervised Cross-Domain Visual Emotion Recognition"></a>Knowledge-Aligned Counterfactual-Enhancement Diffusion Perception for   Unsupervised Cross-Domain Visual Emotion Recognition</h2><p><strong>Authors:Wen Yin, Yong Wang, Guiduo Duan, Dongyang Zhang, Xin Hu, Yuan-Fang Li, Tao He</strong></p>
<p>Visual Emotion Recognition (VER) is a critical yet challenging task aimed at inferring emotional states of individuals based on visual cues. However, existing works focus on single domains, e.g., realistic images or stickers, limiting VER modelsâ€™ cross-domain generalizability. To fill this gap, we introduce an Unsupervised Cross-Domain Visual Emotion Recognition (UCDVER) task, which aims to generalize visual emotion recognition from the source domain (e.g., realistic images) to the low-resource target domain (e.g., stickers) in an unsupervised manner. Compared to the conventional unsupervised domain adaptation problems, UCDVER presents two key challenges: a significant emotional expression variability and an affective distribution shift. To mitigate these issues, we propose the Knowledge-aligned Counterfactual-enhancement Diffusion Perception (KCDP) framework. Specifically, KCDP leverages a VLM to align emotional representations in a shared knowledge space and guides diffusion models for improved visual affective perception. Furthermore, a Counterfactual-Enhanced Language-image Emotional Alignment (CLIEA) method generates high-quality pseudo-labels for the target domain. Extensive experiments demonstrate that our model surpasses SOTA models in both perceptibility and generalization, e.g., gaining 12% improvements over the SOTA VER model TGCA-PVT. The project page is at <a target="_blank" rel="noopener" href="https://yinwen2019.github.io/ucdver">https://yinwen2019.github.io/ucdver</a>. </p>
<blockquote>
<p>è§†è§‰æƒ…æ„Ÿè¯†åˆ«ï¼ˆVERï¼‰æ˜¯ä¸€é¡¹è‡³å…³é‡è¦ä½†å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œæ—¨åœ¨åŸºäºè§†è§‰çº¿ç´¢æ¨æ–­ä¸ªä½“çš„æƒ…æ„ŸçŠ¶æ€ã€‚ç„¶è€Œï¼Œç°æœ‰å·¥ä½œä¸»è¦é›†ä¸­åœ¨å•ä¸€é¢†åŸŸï¼Œä¾‹å¦‚çœŸå®å›¾åƒæˆ–è´´çº¸ï¼Œè¿™é™åˆ¶äº†VERæ¨¡å‹è·¨é¢†åŸŸçš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ— ç›‘ç£è·¨åŸŸè§†è§‰æƒ…æ„Ÿè¯†åˆ«ï¼ˆUCDVERï¼‰ä»»åŠ¡ï¼Œæ—¨åœ¨ä»¥æ— ç›‘ç£çš„æ–¹å¼å°†ä»æºåŸŸï¼ˆä¾‹å¦‚çœŸå®å›¾åƒï¼‰å­¦åˆ°çš„è§†è§‰æƒ…æ„Ÿè¯†åˆ«èƒ½åŠ›æ¨å¹¿åˆ°èµ„æºæœ‰é™çš„ç›®æ ‡åŸŸï¼ˆä¾‹å¦‚è´´çº¸ï¼‰ã€‚ä¸ä¼ ç»Ÿçš„æ— ç›‘ç£åŸŸé€‚åº”é—®é¢˜ç›¸æ¯”ï¼ŒUCDVERé¢ä¸´ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ï¼šæƒ…æ„Ÿè¡¨è¾¾çš„æ˜¾è‘—å¯å˜æ€§å’Œæƒ…æ„Ÿåˆ†å¸ƒåç§»ã€‚ä¸ºäº†ç¼“è§£è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†çŸ¥è¯†å¯¹é½çš„é€†å‘å¢å¼ºæ‰©æ•£æ„ŸçŸ¥ï¼ˆKCDPï¼‰æ¡†æ¶ã€‚å…·ä½“æ¥è¯´ï¼ŒKCDPåˆ©ç”¨VLMå¯¹é½å…±äº«çŸ¥è¯†ç©ºé—´ä¸­çš„æƒ…æ„Ÿè¡¨ç¤ºï¼Œå¹¶æŒ‡å¯¼æ‰©æ•£æ¨¡å‹ä»¥æ”¹è¿›è§†è§‰æƒ…æ„Ÿæ„ŸçŸ¥ã€‚æ­¤å¤–ï¼Œé€†å‘å¢å¼ºè¯­è¨€-å›¾åƒæƒ…æ„Ÿå¯¹é½ï¼ˆCLIEAï¼‰æ–¹æ³•ä¸ºç›®æ ‡åŸŸç”Ÿæˆé«˜è´¨é‡ä¼ªæ ‡ç­¾ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨æ„ŸçŸ¥å’Œæ³›åŒ–æ–¹é¢éƒ½è¶…è¶Šäº†æœ€å…ˆè¿›æ¨¡å‹ï¼Œä¾‹å¦‚æ¯”æœ€å…ˆè¿›çš„VERæ¨¡å‹TGCA-PVTæé«˜äº†12%ã€‚é¡¹ç›®é¡µé¢ä½äº<a target="_blank" rel="noopener" href="https://yinwen2019.github.io/ucdver%E3%80%82">https://yinwen2019.github.io/ucdverã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19694v1">PDF</a> Accepted at CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è§†è§‰æƒ…ç»ªè¯†åˆ«ï¼ˆVERï¼‰çš„é‡è¦æ€§åŠå…¶é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¦‚è·¨åŸŸæ³›åŒ–èƒ½åŠ›æœ‰é™ã€‚ä¸ºæ­¤ï¼Œæ–‡ç« æå‡ºäº†æ— ç›‘ç£è·¨åŸŸè§†è§‰æƒ…ç»ªè¯†åˆ«ï¼ˆUCDVERï¼‰ä»»åŠ¡ï¼Œæ—¨åœ¨ä»æºåŸŸï¼ˆå¦‚çœŸå®å›¾åƒï¼‰å‘ä½èµ„æºç›®æ ‡åŸŸï¼ˆå¦‚è´´çº¸ï¼‰è¿›è¡Œæ— ç›‘ç£çš„è§†è§‰æƒ…æ„Ÿè¯†åˆ«æ³›åŒ–ã€‚æ–‡ç« è¿˜ä»‹ç»äº†çŸ¥è¯†å¯¹é½çš„å› æœå¢å¼ºæ‰©æ•£æ„ŸçŸ¥ï¼ˆKCDPï¼‰æ¡†æ¶å’Œå¯¹æŠ—æ€§è¯­è¨€å›¾åƒæƒ…æ„Ÿå¯¹é½ï¼ˆCLIEAï¼‰æ–¹æ³•ï¼Œä»¥æé«˜æ„ŸçŸ¥å’Œæ³›åŒ–æ€§èƒ½ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¨¡å‹åœ¨æ„ŸçŸ¥å’Œæ³›åŒ–æ–¹é¢éƒ½è¶…è¿‡äº†æœ€å…ˆè¿›çš„VERæ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VERï¼ˆè§†è§‰æƒ…ç»ªè¯†åˆ«ï¼‰æ˜¯ä¸€é¡¹é‡è¦ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œæ—¨åœ¨åŸºäºè§†è§‰çº¿ç´¢æ¨æ–­ä¸ªä½“çš„æƒ…ç»ªçŠ¶æ€ã€‚</li>
<li>ç°æœ‰ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨å•ä¸€é¢†åŸŸï¼Œé™åˆ¶äº†VERæ¨¡å‹çš„è·¨åŸŸæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>UCDVERï¼ˆæ— ç›‘ç£è·¨åŸŸè§†è§‰æƒ…ç»ªè¯†åˆ«ï¼‰ä»»åŠ¡æ—¨åœ¨ä»æºåŸŸå‘ä½èµ„æºçš„ç›®æ ‡åŸŸè¿›è¡Œæ— ç›‘ç£çš„è§†è§‰æƒ…æ„Ÿè¯†åˆ«æ³›åŒ–ã€‚</li>
<li>UCDVERé¢ä¸´ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ï¼šæƒ…ç»ªè¡¨è¾¾çš„æ˜¾è‘—å¯å˜æ€§å’Œæƒ…æ„Ÿåˆ†å¸ƒè½¬ç§»ã€‚</li>
<li>KCDPï¼ˆçŸ¥è¯†å¯¹é½çš„å› æœå¢å¼ºæ‰©æ•£æ„ŸçŸ¥ï¼‰æ¡†æ¶ç”¨äºè§£å†³è¿™äº›é—®é¢˜ï¼Œé€šè¿‡åˆ©ç”¨VLMå¯¹é½æƒ…æ„Ÿè¡¨ç¤ºï¼Œå¹¶å¼•å¯¼æ‰©æ•£æ¨¡å‹æé«˜è§†è§‰æƒ…æ„Ÿæ„ŸçŸ¥ã€‚</li>
<li>CLIEAï¼ˆå¯¹æŠ—æ€§è¯­è¨€å›¾åƒæƒ…æ„Ÿå¯¹é½ï¼‰æ–¹æ³•ç”¨äºç”Ÿæˆé«˜è´¨é‡çš„ç›®æ ‡åŸŸä¼ªæ ‡ç­¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19694">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a3d450f4e57aec9a414a177f3b037d4e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-05ed14cc47d40e09ce53c144375f919d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3c130eb4d1a8b99a9dfc14a70b35023f.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Regularized-Personalization-of-Text-to-Image-Diffusion-Models-without-Distributional-Drift"><a href="#Regularized-Personalization-of-Text-to-Image-Diffusion-Models-without-Distributional-Drift" class="headerlink" title="Regularized Personalization of Text-to-Image Diffusion Models without   Distributional Drift"></a>Regularized Personalization of Text-to-Image Diffusion Models without   Distributional Drift</h2><p><strong>Authors:Gihoon Kim, Hyungjin Park, Taesup Kim</strong></p>
<p>Personalization using text-to-image diffusion models involves adapting a pretrained model to novel subjects with only a few image examples. This task presents a fundamental challenge, as the model must not only learn the new subject effectively but also preserve its ability to generate diverse and coherent outputs across a wide range of prompts. In other words, successful personalization requires integrating new concepts without forgetting previously learned generative capabilities. Forgetting denotes unintended distributional drift, where the modelâ€™s output distribution deviates from that of the original pretrained model. In this paper, we provide an analysis of this issue and identify a mismatch between standard training objectives and the goals of personalization. To address this, we propose a new training objective based on a Lipschitz-bounded formulation that explicitly constrains deviation from the pretrained distribution. Our method provides improved control over distributional drift and performs well even in data-scarce scenarios. Experimental results demonstrate that our approach consistently outperforms existing personalization methods, achieving higher CLIP-T, CLIP-I, and DINO scores. </p>
<blockquote>
<p>ä½¿ç”¨æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹è¿›è¡Œä¸ªæ€§åŒ–æ¶‰åŠä»…ä½¿ç”¨å°‘é‡å›¾åƒç¤ºä¾‹æ¥é€‚åº”é¢„è®­ç»ƒæ¨¡å‹ä»¥åº”å¯¹æ–°ä¸»é¢˜ã€‚æ­¤ä»»åŠ¡å‘ˆç°äº†ä¸€ä¸ªåŸºæœ¬æŒ‘æˆ˜ï¼Œå› ä¸ºæ¨¡å‹ä¸ä»…éœ€è¦æœ‰æ•ˆåœ°å­¦ä¹ æ–°ä¸»é¢˜ï¼Œè€Œä¸”è¿˜è¦ä¿æŒå…¶åœ¨å¹¿æ³›æç¤ºä¸­ç”Ÿæˆå¤šæ ·åŒ–å’Œè¿è´¯è¾“å‡ºçš„èƒ½åŠ›ã€‚æ¢å¥è¯è¯´ï¼ŒæˆåŠŸçš„ä¸ªæ€§åŒ–è¦æ±‚æ•´åˆæ–°æ¦‚å¿µè€Œä¸ä¼šå¿˜è®°å…ˆå‰å­¦ä¹ çš„ç”Ÿæˆèƒ½åŠ›ã€‚é—å¿˜è¡¨ç¤ºæ„å¤–çš„åˆ†å¸ƒæ¼‚ç§»ï¼Œå³æ¨¡å‹è¾“å‡ºåˆ†å¸ƒåç¦»åŸå§‹é¢„è®­ç»ƒæ¨¡å‹çš„åˆ†å¸ƒã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¯¹æ­¤é—®é¢˜è¿›è¡Œäº†åˆ†æï¼Œå¹¶ç¡®å®šäº†æ ‡å‡†åŸ¹è®­ç›®æ ‡ä¸ä¸ªæ€§åŒ–ç›®æ ‡ä¹‹é—´çš„ä¸åŒ¹é…ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºLipschitzç•Œå®šçš„æ–°åŸ¹è®­ç›®æ ‡ï¼Œè¯¥ç›®æ ‡æ˜¾å¼çº¦æŸäº†ä¸é¢„è®­ç»ƒåˆ†å¸ƒçš„åå·®ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨æ§åˆ¶åˆ†å¸ƒæ¼‚ç§»æ–¹é¢æä¾›äº†æ›´å¥½çš„æ§åˆ¶ï¼Œå³ä½¿åœ¨æ•°æ®ç¨€ç¼ºçš„æƒ…å†µä¸‹ä¹Ÿèƒ½è¡¨ç°è‰¯å¥½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å§‹ç»ˆä¼˜äºç°æœ‰çš„ä¸ªæ€§åŒ–æ–¹æ³•ï¼Œå®ç°äº†æ›´é«˜çš„CLIP-Tã€CLIP-Iå’ŒDINOåˆ†æ•°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19519v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    æœ¬æ–‡ç ”ç©¶äº†åˆ©ç”¨æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹è¿›è¡Œä¸ªæ€§åŒ–ä»»åŠ¡æ—¶é¢ä¸´çš„æŒ‘æˆ˜ã€‚åœ¨åªæœ‰å°‘é‡å›¾åƒç¤ºä¾‹çš„æƒ…å†µä¸‹ï¼Œæ¨¡å‹éœ€è¦é€‚åº”æ–°ä¸»é¢˜ï¼ŒåŒæ—¶ä¿æŒç”Ÿæˆå¤šæ ·åŒ–å’Œè¿è´¯æ€§è¾“å‡ºçš„èƒ½åŠ›ã€‚æœ¬æ–‡åˆ†æäº†è¿™ä¸ªé—®é¢˜ï¼Œå¹¶æŒ‡å‡ºæ ‡å‡†è®­ç»ƒç›®æ ‡ä¸ä¸ªæ€§åŒ–ç›®æ ‡ä¹‹é—´çš„ä¸åŒ¹é…ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºLipschitzçº¦æŸçš„æ–°è®­ç»ƒç›®æ ‡ï¼Œè¯¥ç›®æ ‡æ˜¾å¼çº¦æŸæ¨¡å‹è¾“å‡ºåˆ†å¸ƒä¸é¢„è®­ç»ƒåˆ†å¸ƒçš„åå·®ã€‚è¯¥æ–¹æ³•åœ¨æ•°æ®ç¨€ç¼ºçš„åœºæ™¯ä¸‹è¡¨ç°è‰¯å¥½ï¼Œå®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸€è‡´æ€§ã€å›¾åƒç†è§£å’Œå¾—åˆ†æ–¹é¢å‡ä¼˜äºç°æœ‰ä¸ªæ€§åŒ–æ–¹æ³•ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„ä¸ªæ€§åŒ–ä»»åŠ¡é¢ä¸´æŒ‘æˆ˜ï¼Œéœ€è¦åœ¨é€‚åº”æ–°ä¸»é¢˜çš„åŒæ—¶ä¿æŒç”Ÿæˆå¤šæ ·åŒ–è¾“å‡ºçš„èƒ½åŠ›ã€‚</li>
<li>æœ¬æ–‡åˆ†æäº†æ ‡å‡†è®­ç»ƒç›®æ ‡ä¸ä¸ªæ€§åŒ–ç›®æ ‡ä¹‹é—´çš„ä¸åŒ¹é…é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºLipschitzçº¦æŸçš„æ–°è®­ç»ƒç›®æ ‡ï¼Œä»¥æ§åˆ¶æ¨¡å‹è¾“å‡ºåˆ†å¸ƒä¸é¢„è®­ç»ƒåˆ†å¸ƒä¹‹é—´çš„åå·®ã€‚</li>
<li>æ–°æ–¹æ³•èƒ½æœ‰æ•ˆé€‚åº”æ•°æ®ç¨€ç¼ºçš„åœºæ™¯ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨CLIP-Tã€CLIP-Iå’ŒDINOè¯„åˆ†æ–¹é¢å‡ä¼˜äºç°æœ‰ä¸ªæ€§åŒ–æ–¹æ³•ã€‚</li>
<li>ä¸ªäººåŒ–è¿‡ç¨‹éœ€æ•´åˆæ–°æ¦‚å¿µçš„åŒæ—¶ä¿ç•™å·²å­¦èƒ½åŠ›ï¼Œé¿å…é—å¿˜ï¼ˆåˆ†å¸ƒæ¼‚ç§»ï¼‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19519">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-316955500aabc6d96e35e2b802e5790f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c84c63b7632631afe1598f4f66bf5907.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fcc99a82243ff3b3b47e7e67131fb3af.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-48dd73ed0fb7b79abe12b9ef4dd2023e.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Structure-Disruption-Subverting-Malicious-Diffusion-Based-Inpainting-via-Self-Attention-Query-Perturbation"><a href="#Structure-Disruption-Subverting-Malicious-Diffusion-Based-Inpainting-via-Self-Attention-Query-Perturbation" class="headerlink" title="Structure Disruption: Subverting Malicious Diffusion-Based Inpainting   via Self-Attention Query Perturbation"></a>Structure Disruption: Subverting Malicious Diffusion-Based Inpainting   via Self-Attention Query Perturbation</h2><p><strong>Authors:Yuhao He, Jinyu Tian, Haiwei Wu, Jianqing Li</strong></p>
<p>The rapid advancement of diffusion models has enhanced their image inpainting and editing capabilities but also introduced significant societal risks. Adversaries can exploit user images from social media to generate misleading or harmful content. While adversarial perturbations can disrupt inpainting, global perturbation-based methods fail in mask-guided editing tasks due to spatial constraints. To address these challenges, we propose Structure Disruption Attack (SDA), a powerful protection framework for safeguarding sensitive image regions against inpainting-based editing. Building upon the contour-focused nature of self-attention mechanisms of diffusion models, SDA optimizes perturbations by disrupting queries in self-attention during the initial denoising step to destroy the contour generation process. This targeted interference directly disrupts the structural generation capability of diffusion models, effectively preventing them from producing coherent images. We validate our motivation through visualization techniques and extensive experiments on public datasets, demonstrating that SDA achieves state-of-the-art (SOTA) protection performance while maintaining strong robustness. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹çš„å¿«é€Ÿå‘å±•å¢å¼ºäº†å…¶å›¾åƒè¡¥å…¨å’Œç¼–è¾‘åŠŸèƒ½ï¼Œä½†åŒæ—¶ä¹Ÿå¸¦æ¥äº†é‡å¤§çš„ç¤¾ä¼šé£é™©ã€‚å¯¹æ‰‹å¯èƒ½ä¼šåˆ©ç”¨ç¤¾äº¤åª’ä½“ä¸Šçš„ç”¨æˆ·å›¾åƒç”Ÿæˆè¯¯å¯¼æ€§æˆ–æœ‰å®³å†…å®¹ã€‚è™½ç„¶å¯¹æŠ—æ€§æ‰°åŠ¨å¯ä»¥ç ´åå›¾åƒè¡¥å…¨ï¼Œä½†åŸºäºå…¨å±€æ‰°åŠ¨çš„æ–¹æ³•åœ¨æ©è†œå¼•å¯¼ç¼–è¾‘ä»»åŠ¡ä¸­ä¼šå› ç©ºé—´çº¦æŸè€Œå¤±è´¥ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ç»“æ„ç ´åæ”»å‡»ï¼ˆSDAï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªå¼ºå¤§çš„ä¿æŠ¤æ¡†æ¶ï¼Œç”¨äºä¿æŠ¤æ•æ„Ÿå›¾åƒåŒºåŸŸå…å—åŸºäºè¡¥å…¨çš„ç¼–è¾‘æ”»å‡»ã€‚SDAå»ºç«‹åœ¨æ‰©æ•£æ¨¡å‹çš„è‡ªæ³¨æ„æœºåˆ¶è½®å»“é‡ç‚¹çš„åŸºç¡€ä¸Šï¼Œé€šè¿‡ä¼˜åŒ–åˆå§‹å»å™ªæ­¥éª¤ä¸­è‡ªæ³¨æ„æŸ¥è¯¢çš„æ‰°åŠ¨æ¥ç ´åè½®å»“ç”Ÿæˆè¿‡ç¨‹ï¼Œä»è€Œç ´åç›®æ ‡å¹²æ‰°ã€‚è¿™ç§æœ‰é’ˆå¯¹æ€§çš„å¹²æ‰°ç›´æ¥ç ´åäº†æ‰©æ•£æ¨¡å‹çš„ç»“æ„ç”Ÿæˆèƒ½åŠ›ï¼Œæœ‰æ•ˆåœ°é˜²æ­¢å…¶ç”Ÿæˆè¿è´¯çš„å›¾åƒã€‚æˆ‘ä»¬é€šè¿‡å¯è§†åŒ–æŠ€æœ¯å’Œå…¬å¼€æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒéªŒè¯äº†æˆ‘ä»¬çš„åŠ¨æœºï¼Œè¯æ˜SDAåœ¨ä¿æŠ¤æ€§èƒ½æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼ŒåŒæ—¶ä¿æŒäº†å¼ºå¤§çš„ç¨³å¥æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19425v1">PDF</a> </p>
<p><strong>Summary</strong><br>æ‰©æ•£æ¨¡å‹çš„å¿«é€Ÿå‘å±•æå‡äº†å›¾åƒè¡¥å…¨å’Œç¼–è¾‘èƒ½åŠ›ï¼Œä½†ä¹Ÿå¸¦æ¥äº†ä¸€å®šçš„ç¤¾ä¼šé£é™©ã€‚å¯¹æ‰‹å¯èƒ½ä¼šåˆ©ç”¨ç¤¾äº¤åª’ä½“ä¸­çš„ç”¨æˆ·å›¾åƒç”Ÿæˆè¯¯å¯¼æ€§æˆ–æœ‰å®³å†…å®¹ã€‚ä¸ºåº”å¯¹æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ç»“æ„ç ´åæ”»å‡»ï¼ˆSDAï¼‰è¿™ä¸€ä¿æŠ¤æ¡†æ¶ï¼Œä¿æŠ¤æ•æ„Ÿå›¾åƒåŒºåŸŸå…å—åŸºäºè¡¥å…¨çš„ç¼–è¾‘æ”»å‡»ã€‚SDAé€šè¿‡å¹²æ‰°å»å™ªæ­¥éª¤ä¸­çš„è‡ªæ³¨æ„åŠ›æŸ¥è¯¢æ¥ç ´åè½®å»“ç”Ÿæˆè¿‡ç¨‹ï¼Œä¼˜åŒ–å¹²æ‰°ï¼Œå®ç°å¯¹æ‰©æ•£æ¨¡å‹ç»“æ„ç”Ÿæˆèƒ½åŠ›çš„ç›´æ¥ç ´åï¼Œæœ‰æ•ˆé˜²æ­¢å…¶ç”Ÿæˆè¿è´¯å›¾åƒã€‚é€šè¿‡å¯è§†åŒ–æŠ€æœ¯å’Œå…¬å¼€æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒéªŒè¯ï¼ŒSDAè¾¾åˆ°äº†ä¸šç•Œæœ€ä½³çš„é˜²æŠ¤æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒäº†å¼ºå¤§çš„ç¨³å¥æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹çš„å¿«é€Ÿå‘å±•å¸¦æ¥äº†å›¾åƒè¡¥å…¨å’Œç¼–è¾‘èƒ½åŠ›çš„æå‡ï¼Œä½†ä¹Ÿå­˜åœ¨ç¤¾ä¼šé£é™©ã€‚</li>
<li>å¯¹æ‰‹å¯èƒ½åˆ©ç”¨ç¤¾äº¤åª’ä½“ç”¨æˆ·å›¾åƒç”Ÿæˆè¯¯å¯¼æˆ–æœ‰å®³å†…å®¹ã€‚</li>
<li>ç»“æ„ç ´åæ”»å‡»ï¼ˆSDAï¼‰æ˜¯ä¸€ç§ä¿æŠ¤æ¡†æ¶ï¼Œæ—¨åœ¨ä¿æŠ¤æ•æ„Ÿå›¾åƒåŒºåŸŸå…å—åŸºäºè¡¥å…¨çš„ç¼–è¾‘æ”»å‡»ã€‚</li>
<li>SDAé€šè¿‡å¹²æ‰°æ‰©æ•£æ¨¡å‹çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶æ¥ç ´åè½®å»“ç”Ÿæˆè¿‡ç¨‹ã€‚</li>
<li>SDAèƒ½æœ‰æ•ˆé˜²æ­¢æ‰©æ•£æ¨¡å‹ç”Ÿæˆè¿è´¯å›¾åƒã€‚</li>
<li>é€šè¿‡å¯è§†åŒ–å’Œå…¬å¼€æ•°æ®é›†å®éªŒéªŒè¯ï¼ŒSDAè¾¾åˆ°ä¸šç•Œæœ€ä½³é˜²æŠ¤æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19425">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f25621761c741b7d99afac45c12f4870.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1ff6f5bfd6ca04dc966986d537b2663a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ff23634e65869c7b352e99a3b0a4ab75.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6eae7f728025ae4d40e87dde7da0f4b1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fbcb4c1f5945aca908df0612c2f2b315.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Advancing-Limited-Angle-CT-Reconstruction-Through-Diffusion-Based-Sinogram-Completion"><a href="#Advancing-Limited-Angle-CT-Reconstruction-Through-Diffusion-Based-Sinogram-Completion" class="headerlink" title="Advancing Limited-Angle CT Reconstruction Through Diffusion-Based   Sinogram Completion"></a>Advancing Limited-Angle CT Reconstruction Through Diffusion-Based   Sinogram Completion</h2><p><strong>Authors:Jiaqi Guo, Santiago Lopez-Tapia, Aggelos K. Katsaggelos</strong></p>
<p>Limited Angle Computed Tomography (LACT) often faces significant challenges due to missing angular information. Unlike previous methods that operate in the image domain, we propose a new method that focuses on sinogram inpainting. We leverage MR-SDEs, a variant of diffusion models that characterize the diffusion process with mean-reverting stochastic differential equations, to fill in missing angular data at the projection level. Furthermore, by combining distillation with constraining the output of the model using the pseudo-inverse of the inpainting matrix, the diffusion process is accelerated and done in a step, enabling efficient and accurate sinogram completion. A subsequent post-processing module back-projects the inpainted sinogram into the image domain and further refines the reconstruction, effectively suppressing artifacts while preserving critical structural details. Quantitative experimental results demonstrate that the proposed method achieves state-of-the-art performance in both perceptual and fidelity quality, offering a promising solution for LACT reconstruction in scientific and clinical applications. </p>
<blockquote>
<p>æœ‰é™è§’åº¦è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆLACTï¼‰ç»å¸¸ç”±äºç¼ºå°‘è§’åº¦ä¿¡æ¯è€Œé¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚ä¸åŒäºä¹‹å‰åœ¨å›¾åƒåŸŸæ“ä½œçš„æ–¹æ³•ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œä¸“æ³¨äºè¾›æ ¼æ‹‰å§†è¡¥å…¨ã€‚æˆ‘ä»¬åˆ©ç”¨MR-SDEsï¼ˆä¸€ç§ç”¨å‡å€¼å›å½’éšæœºå¾®åˆ†æ–¹ç¨‹æè¿°æ‰©æ•£è¿‡ç¨‹çš„æ‰©æ•£æ¨¡å‹çš„å˜ä½“ï¼‰åœ¨æŠ•å½±å±‚é¢å¡«å……ç¼ºå¤±çš„è§’åº¦æ•°æ®ã€‚æ­¤å¤–ï¼Œé€šè¿‡ç»“åˆè’¸é¦æŠ€æœ¯å¹¶ç”¨è¡¥å…¨çŸ©é˜µçš„ä¼ªé€†çº¦æŸæ¨¡å‹çš„è¾“å‡ºï¼Œæ‰©æ•£è¿‡ç¨‹å¾—åˆ°åŠ é€Ÿå¹¶ä¸€æ¬¡æ€§å®Œæˆï¼Œå®ç°äº†é«˜æ•ˆå‡†ç¡®çš„è¾›æ ¼æ‹‰å§†è¡¥å…¨ã€‚éšåçš„åå¤„ç†æ¨¡å—å°†è¡¥å…¨çš„è¾›æ ¼æ‹‰å§†åå‘æŠ•å½±åˆ°å›¾åƒåŸŸï¼Œå¹¶è¿›ä¸€æ­¥æ”¹è¿›é‡å»ºæ•ˆæœï¼Œæœ‰æ•ˆåœ°æŠ‘åˆ¶ä¼ªå½±çš„åŒæ—¶ä¿ç•™å…³é”®çš„ç»“æ„ç»†èŠ‚ã€‚å®šé‡å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€ææ–¹æ³•åœ¨å®ç°æ„ŸçŸ¥å’Œä¿çœŸåº¦è´¨é‡æ–¹é¢å‡è¾¾åˆ°æœ€æ–°æ°´å¹³ï¼Œä¸ºç§‘å­¦ç ”ç©¶å’Œä¸´åºŠåº”ç”¨ä¸­çš„LACTé‡å»ºæä¾›äº†æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19385v1">PDF</a> Accepted at the 2025 IEEE International Conference on Image   Processing (Oral)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„æ–°æ–¹æ³•ï¼Œä¸“æ³¨äºå¤„ç†Limited Angle Computed Tomographyï¼ˆLACTï¼‰ä¸­çš„ç¼ºå¤±è§’åº¦ä¿¡æ¯é—®é¢˜ã€‚è¯¥æ–¹æ³•åœ¨æŠ•å½±å±‚é¢ä½¿ç”¨MR-SDEsè¿›è¡Œå¡«å……ç¼ºå¤±çš„è§’æ•°æ®ï¼Œå¹¶é‡‡ç”¨è’¸é¦æŠ€æœ¯å’Œä¼ªé€†è¡¥å…¨çŸ©é˜µè¿›è¡Œè¾“å‡ºçº¦æŸä»¥åŠ é€Ÿæ‰©æ•£è¿‡ç¨‹ã€‚å®Œæˆåçš„å¤„ç†æ¨¡å—èƒ½å¤Ÿå°†è¡¥å…¨çš„sinogramè½¬å›å›¾åƒåŸŸï¼Œå¯¹é‡å»ºå›¾åƒè¿›è¡Œè¿›ä¸€æ­¥çš„ä¼˜åŒ–å’Œç»†èŠ‚ä¿®å¤ã€‚å®éªŒç»“æœè¯æ˜äº†è¯¥æ–¹æ³•åœ¨æ„ŸçŸ¥å’Œä¿çœŸè´¨é‡æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼Œä¸ºç§‘å­¦ç ”ç©¶å’Œä¸´åºŠåº”ç”¨ä¸­çš„LACTé‡å»ºæä¾›äº†å‰æ™¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§æ–°çš„å¤„ç†Limited Angle Computed Tomographyï¼ˆLACTï¼‰ç¼ºå¤±è§’åº¦ä¿¡æ¯çš„æ–¹æ³•ã€‚</li>
<li>é‡‡ç”¨MR-SDEsæ‰©æ•£æ¨¡å‹è¿›è¡Œsinogramè¡¥å…¨ã€‚</li>
<li>ç»“åˆè’¸é¦æŠ€æœ¯å’Œä¼ªé€†è¡¥å…¨çŸ©é˜µè¾“å‡ºçº¦æŸæ¥åŠ é€Ÿæ‰©æ•£è¿‡ç¨‹ã€‚</li>
<li>é€šè¿‡åå¤„ç†æ¨¡å—å°†è¡¥å…¨çš„sinogramè½¬å›å›¾åƒåŸŸè¿›è¡Œä¼˜åŒ–å’Œç»†èŠ‚ä¿®å¤ã€‚</li>
<li>å®éªŒç»“æœè¯æ˜äº†è¯¥æ–¹æ³•åœ¨æ„ŸçŸ¥å’Œä¿çœŸè´¨é‡æ–¹é¢çš„æœ€æ–°æ°´å¹³è¡¨ç°ã€‚</li>
<li>è¯¥æ–¹æ³•å…·æœ‰é«˜æ•ˆæ€§å’Œå‡†ç¡®æ€§ï¼Œä¸ºç§‘å­¦ç ”ç©¶å’Œä¸´åºŠåº”ç”¨ä¸­çš„LACTé‡å»ºæä¾›äº†å‰æ™¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19385">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9b0de5c928fc1bca3633f57120ba5a3d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ab27e2612a03c1f249b254237f34cae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5206070dc91963da2bc84d1a570f2ada.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3fb9935aee4c21cf52cc87a7e219daca.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1b18c2a3f9ad8156d4fc67b302dca6e1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-05d8e195cb1d43252e54c3a9df80d6c7.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="TextDiffuser-RL-Efficient-and-Robust-Text-Layout-Optimization-for-High-Fidelity-Text-to-Image-Synthesis"><a href="#TextDiffuser-RL-Efficient-and-Robust-Text-Layout-Optimization-for-High-Fidelity-Text-to-Image-Synthesis" class="headerlink" title="TextDiffuser-RL: Efficient and Robust Text Layout Optimization for   High-Fidelity Text-to-Image Synthesis"></a>TextDiffuser-RL: Efficient and Robust Text Layout Optimization for   High-Fidelity Text-to-Image Synthesis</h2><p><strong>Authors:Kazi Mahathir Rahman, Showrin Rahman, Sharmin Sultana Srishty</strong></p>
<p>Text-embedded image generation plays a critical role in industries such as graphic design, advertising, and digital content creation. Text-to-Image generation methods leveraging diffusion models, such as TextDiffuser-2, have demonstrated promising results in producing images with embedded text. TextDiffuser-2 effectively generates bounding box layouts that guide the rendering of visual text, achieving high fidelity and coherence. However, existing approaches often rely on resource-intensive processes and are limited in their ability to run efficiently on both CPU and GPU platforms. To address these challenges, we propose a novel two-stage pipeline that integrates reinforcement learning (RL) for rapid and optimized text layout generation with a diffusion-based image synthesis model. Our RL-based approach significantly accelerates the bounding box prediction step while reducing overlaps, allowing the system to run efficiently on both CPUs and GPUs. Extensive evaluations demonstrate that our framework maintains or surpasses TextDiffuser-2â€™s quality in text placement and image synthesis, with markedly faster runtime and increased flexibility. Extensive evaluations demonstrate that our framework maintains or surpasses TextDiffuser-2â€™s quality in text placement and image synthesis, with markedly faster runtime and increased flexibility. Our approach has been evaluated on the MARIOEval benchmark, achieving OCR and CLIPScore metrics close to state-of-the-art models, while being 97.64% more faster and requiring only 2MB of memory to run. </p>
<blockquote>
<p>æ–‡æœ¬åµŒå…¥å›¾åƒç”Ÿæˆåœ¨å›¾å½¢è®¾è®¡ã€å¹¿å‘Šå’Œæ•°å­—å†…å®¹åˆ›å»ºç­‰è¡Œä¸šæ‰®æ¼”ç€è‡³å…³é‡è¦çš„è§’è‰²ã€‚åˆ©ç”¨æ‰©æ•£æ¨¡å‹è¿›è¡Œæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„æ–¹æ³•ï¼Œå¦‚TextDiffuser-2ï¼Œå·²å±•ç°å‡ºåœ¨ç”ŸæˆåµŒå…¥æ–‡æœ¬çš„å›¾åƒæ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚TextDiffuser-2é€šè¿‡ç”Ÿæˆæœ‰æ•ˆçš„è¾¹ç•Œæ¡†å¸ƒå±€æ¥æŒ‡å¯¼è§†è§‰æ–‡æœ¬çš„æ¸²æŸ“ï¼Œè¾¾åˆ°é«˜ä¿çœŸåº¦å’Œè¿è´¯æ€§ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¾€å¾€ä¾èµ–äºèµ„æºå¯†é›†å‹è¿‡ç¨‹ï¼Œå¹¶ä¸”åœ¨CPUå’ŒGPUå¹³å°ä¸Šçš„è¿è¡Œæ•ˆç‡æœ‰é™ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„ä¸¤é˜¶æ®µç®¡é“ï¼Œå®ƒé›†æˆäº†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¿›è¡Œå¿«é€Ÿä¼˜åŒ–çš„æ–‡æœ¬å¸ƒå±€ç”Ÿæˆå’ŒåŸºäºæ‰©æ•£çš„å›¾åƒåˆæˆæ¨¡å‹ã€‚æˆ‘ä»¬çš„åŸºäºRLçš„æ–¹æ³•æ˜¾è‘—åŠ é€Ÿäº†è¾¹ç•Œæ¡†é¢„æµ‹æ­¥éª¤ï¼ŒåŒæ—¶å‡å°‘äº†é‡å ï¼Œä½¿ç³»ç»Ÿèƒ½å¤Ÿåœ¨CPUå’ŒGPUä¸Šé«˜æ•ˆè¿è¡Œã€‚å¹¿æ³›çš„è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶åœ¨æ–‡æœ¬æ”¾ç½®å’Œå›¾åƒåˆæˆæ–¹é¢ä¿æŒæˆ–è¶…è¶Šäº†TextDiffuser-2çš„è´¨é‡ï¼Œè¿è¡Œæ—¶æ˜æ˜¾æ›´å¿«ï¼Œçµæ´»æ€§å¢å¼ºã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨MARIOEvalåŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œå®ç°äº†æ¥è¿‘æœ€æ–°æ¨¡å‹çš„OCRå’ŒCLIPScoreæŒ‡æ ‡ï¼ŒåŒæ—¶è¿è¡Œé€Ÿåº¦æé«˜äº†97.64%ï¼Œå¹¶ä¸”åªéœ€2MBçš„å†…å­˜å³å¯è¿è¡Œã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19291v1">PDF</a> 14 pages, 26 figures. Submitted to arXiv for dissemination. Intended   for future submission to a Generative AI conference</p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬åµŒå…¥å›¾åƒç”Ÿæˆåœ¨å›¾å½¢è®¾è®¡ã€å¹¿å‘Šå’Œæ•°å­—åŒ–å†…å®¹åˆ›ä½œç­‰è¡Œä¸šæ‰®æ¼”ç€å…³é”®è§’è‰²ã€‚åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ–¹æ³•ï¼Œå¦‚TextDiffuser-2ï¼Œå·²å±•ç°å‡ºç”ŸæˆåµŒå…¥æ–‡æœ¬å›¾åƒæ—¶çš„æ½œåŠ›ã€‚ä¸ºåº”å¯¹ç°æœ‰æ–¹æ³•èµ„æºæ¶ˆè€—å¤§ã€CPUå’ŒGPUå¹³å°è¿è¡Œæ•ˆç‡æœ‰é™ç­‰æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»“åˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„ä¸¤é˜¶æ®µæµç¨‹ï¼Œç”¨äºåŠ é€Ÿæ–‡æœ¬å¸ƒå±€ç”Ÿæˆï¼Œå¹¶ä¸åŸºäºæ‰©æ•£çš„å›¾åƒåˆæˆæ¨¡å‹ç›¸ç»“åˆã€‚è¯¥æ–¹æ³•æ˜¾è‘—åŠ é€Ÿäº†è¾¹ç•Œæ¡†é¢„æµ‹æ­¥éª¤ï¼Œå‡å°‘äº†é‡å ï¼Œä½¿ç³»ç»Ÿèƒ½åœ¨CPUå’ŒGPUä¸Šé«˜æ•ˆè¿è¡Œã€‚è¯„ä¼°æ˜¾ç¤ºï¼Œæˆ‘ä»¬çš„æ¡†æ¶åœ¨æ–‡æœ¬æ”¾ç½®å’Œå›¾åƒåˆæˆæ–¹é¢ä¿æŒæˆ–è¶…è¶Šäº†TextDiffuser-2çš„è´¨é‡ï¼Œè¿è¡Œæ—¶æ˜¾è‘—åŠ å¿«ï¼Œçµæ´»æ€§å¢å¼ºã€‚åœ¨MARIOEvalåŸºå‡†æµ‹è¯•ä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ¥è¿‘æœ€æ–°æ¨¡å‹çš„OCRå’ŒCLIPScoreæŒ‡æ ‡ï¼ŒåŒæ—¶è¿è¡Œé€Ÿåº¦å¿«97.64%ï¼Œä»…éœ€è¦2MBå†…å­˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬åµŒå…¥å›¾åƒç”Ÿæˆåœ¨å¤šä¸ªè¡Œä¸šå…·æœ‰å…³é”®ä½œç”¨ã€‚</li>
<li>TextDiffuser-2åˆ©ç”¨æ‰©æ•£æ¨¡å‹ç”ŸæˆåµŒå…¥æ–‡æœ¬å›¾åƒæ—¶è¡¨ç°å‡ºè‰¯å¥½æ€§èƒ½ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å­˜åœ¨èµ„æºæ¶ˆè€—å¤§å’Œåœ¨CPUåŠGPUå¹³å°è¿è¡Œæ•ˆç‡æœ‰é™çš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§ç»“åˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„ä¸¤é˜¶æ®µæµç¨‹æ¥ä¼˜åŒ–æ–‡æœ¬å¸ƒå±€ç”Ÿæˆå’Œå›¾åƒåˆæˆã€‚</li>
<li>RLæ–¹æ³•æ˜¾è‘—åŠ é€Ÿäº†è¾¹ç•Œæ¡†é¢„æµ‹ï¼Œå‡å°‘äº†é‡å ï¼Œæé«˜äº†è¿è¡Œæ•ˆç‡ã€‚</li>
<li>è¯„ä¼°æ˜¾ç¤ºï¼Œæ–°æ¡†æ¶åœ¨è´¨é‡ä¸Šä¿æŒæˆ–è¶…è¶Šäº†TextDiffuser-2ï¼Œè¿è¡Œæ—¶æ›´å¿«ï¼Œçµæ´»æ€§å¢å¼ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19291">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-33429f2f63b2edd61ead3ace05d88948.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-57ad27adefef932423938033acc685e8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-732a3f68a4b3eeaa923e78e4691a295c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-849fb883876d3ae4b2cedc5d3d703c15.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Towards-Understanding-the-Mechanisms-of-Classifier-Free-Guidance"><a href="#Towards-Understanding-the-Mechanisms-of-Classifier-Free-Guidance" class="headerlink" title="Towards Understanding the Mechanisms of Classifier-Free Guidance"></a>Towards Understanding the Mechanisms of Classifier-Free Guidance</h2><p><strong>Authors:Xiang Li, Rongrong Wang, Qing Qu</strong></p>
<p>Classifier-free guidance (CFG) is a core technique powering state-of-the-art image generation systems, yet its underlying mechanisms remain poorly understood. In this work, we begin by analyzing CFG in a simplified linear diffusion model, where we show its behavior closely resembles that observed in the nonlinear case. Our analysis reveals that linear CFG improves generation quality via three distinct components: (i) a mean-shift term that approximately steers samples in the direction of class means, (ii) a positive Contrastive Principal Components (CPC) term that amplifies class-specific features, and (iii) a negative CPC term that suppresses generic features prevalent in unconditional data. We then verify that these insights in real-world, nonlinear diffusion models: over a broad range of noise levels, linear CFG resembles the behavior of its nonlinear counterpart. Although the two eventually diverge at low noise levels, we discuss how the insights from the linear analysis still shed light on the CFGâ€™s mechanism in the nonlinear regime. </p>
<blockquote>
<p>æ— åˆ†ç±»å™¨å¼•å¯¼ï¼ˆCFGï¼‰æ˜¯é©±åŠ¨å½“å‰å›¾åƒç”Ÿæˆç³»ç»Ÿæ ¸å¿ƒæŠ€æœ¯çš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼Œä½†å…¶åº•å±‚æœºåˆ¶ä»çŸ¥ä¹‹ç”šå°‘ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆåœ¨ä¸€ä¸ªç®€åŒ–çš„çº¿æ€§æ‰©æ•£æ¨¡å‹ä¸­åˆ†æCFGï¼Œè¯¥æ¨¡å‹çš„è¡Œä¸ºä¸åœ¨éçº¿æ€§æƒ…å†µä¸‹è§‚å¯Ÿåˆ°çš„è¡Œä¸ºéå¸¸ç›¸ä¼¼ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œçº¿æ€§CFGé€šè¿‡ä¸‰ä¸ªä¸åŒéƒ¨åˆ†æé«˜äº†ç”Ÿæˆè´¨é‡ï¼šï¼ˆiï¼‰å‡å€¼åç§»é¡¹ï¼Œå®ƒå¤§è‡´å°†æ ·æœ¬å¼•å¯¼åˆ°ç±»åˆ«å‡å€¼çš„æ–¹å‘ï¼›ï¼ˆiiï¼‰æ­£å‘å¯¹æ¯”ä¸»æˆåˆ†ï¼ˆCPCï¼‰é¡¹ï¼Œå®ƒæ”¾å¤§äº†ç±»åˆ«ç‰¹å®šçš„ç‰¹å¾ï¼›ï¼ˆiiiï¼‰è´ŸCPCé¡¹ï¼Œå®ƒæŠ‘åˆ¶äº†æ— æ¡ä»¶æ•°æ®ä¸­æ™®éå­˜åœ¨çš„é€šç”¨ç‰¹å¾ã€‚ç„¶åï¼Œæˆ‘ä»¬åœ¨ç°å®ä¸–ç•Œçš„éçº¿æ€§æ‰©æ•£æ¨¡å‹ä¸­éªŒè¯äº†è¿™äº›è§è§£ï¼šåœ¨å¹¿æ³›çš„å™ªå£°æ°´å¹³ä¸‹ï¼Œçº¿æ€§CFGçš„è¡Œä¸ºä¸å…¶éçº¿æ€§å¯¹åº”ç‰©ç›¸ä¼¼ã€‚å°½ç®¡ä¸¤è€…åœ¨ä½å™ªå£°æ°´å¹³ä¸‹æœ€ç»ˆä¼šåˆ†é“æ‰¬é•³ï¼Œä½†æˆ‘ä»¬è®¨è®ºäº†çº¿æ€§åˆ†æä¸­çš„è§è§£å¦‚ä½•ä»ç„¶ä¸ºéçº¿æ€§çŠ¶æ€ä¸‹çš„CFGæœºåˆ¶æä¾›å¯ç¤ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19210v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†åˆ†ç±»å™¨å…è´¹æŒ‡å¯¼ï¼ˆCFGï¼‰åœ¨ç®€åŒ–çº¿æ€§æ‰©æ•£æ¨¡å‹ä¸­çš„åº”ç”¨è¡Œä¸ºï¼Œå¹¶æ­ç¤ºäº†å…¶é€šè¿‡ä¸‰ç§ä¸åŒæœºåˆ¶æé«˜ç”Ÿæˆè´¨é‡ï¼šå¹³å‡åç§»é¡¹å¼•å¯¼æ ·æœ¬æœå‘ç±»åˆ«å‡å€¼æ–¹å‘ï¼Œæ­£å‘å¯¹æ¯”ä¸»æˆåˆ†ï¼ˆCPCï¼‰é¡¹æ”¾å¤§ç±»åˆ«ç‰¹å®šç‰¹å¾ï¼Œä»¥åŠè´Ÿå‘CPCé¡¹æŠ‘åˆ¶æ— æ¡ä»¶æ•°æ®ä¸­çš„é€šç”¨ç‰¹å¾ã€‚åœ¨ç°å®ä¸–ç•Œä¸­çš„éçº¿æ€§æ‰©æ•£æ¨¡å‹ä¸­ï¼ŒéªŒè¯äº†è¿™äº›è§è§£åœ¨å¹¿æ³›å™ªå£°æ°´å¹³ä¸‹çš„é€‚ç”¨æ€§ã€‚å°½ç®¡ä¸¤è€…åœ¨ä½å™ªå£°æ°´å¹³ä¸‹æœ€ç»ˆä¼šå‘æ•£ï¼Œä½†æœ¬æ–‡è®¨è®ºå¦‚ä½•ä»çº¿æ€§åˆ†æä¸­æ­ç¤ºCFGæœºåˆ¶åœ¨éçº¿æ€§è´¨åœ°ä¸­çš„åº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>åˆ†ç±»å™¨å…è´¹æŒ‡å¯¼ï¼ˆCFGï¼‰æ˜¯æé«˜å›¾åƒç”Ÿæˆç³»ç»Ÿæ€§èƒ½çš„æ ¸å¿ƒæŠ€æœ¯ã€‚</li>
<li>åœ¨ç®€åŒ–çº¿æ€§æ‰©æ•£æ¨¡å‹ä¸­ï¼ŒCFGçš„è¡Œä¸ºä¸åœ¨éçº¿æ€§æ¨¡å‹ä¸­çš„è¡Œä¸ºç±»ä¼¼ã€‚</li>
<li>CFGé€šè¿‡ä¸‰ç§æœºåˆ¶æé«˜ç”Ÿæˆè´¨é‡ï¼šå¹³å‡åç§»é¡¹ã€æ­£å‘å¯¹æ¯”ä¸»æˆåˆ†ï¼ˆCPCï¼‰é¡¹å’Œè´Ÿå‘CPCé¡¹ã€‚</li>
<li>å¹³å‡åç§»é¡¹å¼•å¯¼æ ·æœ¬æœå‘ç±»åˆ«å‡å€¼æ–¹å‘ã€‚</li>
<li>æ­£å‘CPCé¡¹æ”¾å¤§ç±»åˆ«ç‰¹å®šç‰¹å¾ï¼Œè€Œè´Ÿå‘CPCé¡¹æŠ‘åˆ¶æ— æ¡ä»¶æ•°æ®ä¸­çš„é€šç”¨ç‰¹å¾ã€‚</li>
<li>åœ¨ç°å®ä¸–ç•Œçš„éçº¿æ€§æ‰©æ•£æ¨¡å‹ä¸­ï¼ŒCFGçš„è§è§£åœ¨å¹¿æ³›å™ªå£°æ°´å¹³ä¸‹æœ‰æ•ˆã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19210">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-367cc81f36dc3a4a439e4a1cb405fb02.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-65c7db188aa29ade970c0422ecf5bb1b.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="MIND-Edit-MLLM-Insight-Driven-Editing-via-Language-Vision-Projection"><a href="#MIND-Edit-MLLM-Insight-Driven-Editing-via-Language-Vision-Projection" class="headerlink" title="MIND-Edit: MLLM Insight-Driven Editing via Language-Vision Projection"></a>MIND-Edit: MLLM Insight-Driven Editing via Language-Vision Projection</h2><p><strong>Authors:Shuyu Wang, Weiqi Li, Qian Wang, Shijie Zhao, Jian Zhang</strong></p>
<p>Recent advances in AI-generated content (AIGC) have significantly accelerated image editing techniques, driving increasing demand for diverse and fine-grained edits. Despite these advances, existing image editing methods still face challenges in achieving high precision and semantic accuracy in complex scenarios. Recent studies address this issue by incorporating multimodal large language models (MLLMs) into image editing pipelines. However, current MLLM-based methods mainly rely on interpreting textual instructions, leaving the intrinsic visual understanding of large models largely unexplored, thus resulting in insufficient alignment between textual semantics and visual outcomes. To overcome these limitations, we propose MIND-Edit, an end-to-end image-editing framework integrating pretrained diffusion model with MLLM. MIND-Edit introduces two complementary strategies: (1) a text instruction optimization strategy that clarifies ambiguous user instructions based on semantic reasoning from the MLLM, and (2) an MLLM insight-driven editing strategy that explicitly leverages the intrinsic visual understanding capability of the MLLM to infer editing intent and guide the diffusion process via generated visual embeddings. Furthermore, we propose a joint training approach to effectively integrate both strategies, allowing them to reinforce each other for more accurate instruction interpretation and visually coherent edits aligned with user intent. Extensive experiments demonstrate that MIND-Edit outperforms state-of-the-art image editing methods in both quantitative metrics and visual quality, particularly under complex and challenging scenarios. </p>
<blockquote>
<p>è¿‘æœŸäººå·¥æ™ºèƒ½ç”Ÿæˆå†…å®¹ï¼ˆAIGCï¼‰çš„è¿›æ­¥æå¤§åœ°åŠ é€Ÿäº†å›¾åƒç¼–è¾‘æŠ€æœ¯ï¼Œæ¨åŠ¨äº†å¯¹äºå¤šæ ·åŒ–å’Œç²¾ç»†åŒ–çš„ç¼–è¾‘éœ€æ±‚çš„å¢é•¿ã€‚å°½ç®¡æœ‰è¿™äº›è¿›æ­¥ï¼Œç°æœ‰çš„å›¾åƒç¼–è¾‘æ–¹æ³•ä»ç„¶é¢ä¸´åœ¨å¤æ‚åœºæ™¯ä¸­å®ç°é«˜ç²¾åº¦å’Œè¯­ä¹‰å‡†ç¡®æ€§çš„æŒ‘æˆ˜ã€‚æœ€è¿‘çš„ç ”ç©¶é€šè¿‡æŠŠå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰èå…¥å›¾åƒç¼–è¾‘æµç¨‹æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚ç„¶è€Œï¼Œå½“å‰çš„åŸºäºMLLMçš„æ–¹æ³•ä¸»è¦ä¾èµ–äºè§£è¯»æ–‡æœ¬æŒ‡ä»¤ï¼Œä½¿å¾—å¤§å‹æ¨¡å‹çš„å†…åœ¨è§†è§‰ç†è§£åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šè¢«å¿½ç•¥ï¼Œå› æ­¤å¯¼è‡´æ–‡æœ¬è¯­ä¹‰å’Œè§†è§‰ç»“æœä¹‹é—´çš„å¯¹é½ä¸è¶³ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™ï¼Œæˆ‘ä»¬æå‡ºäº†MIND-Editï¼Œè¿™æ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯çš„å›¾åƒç¼–è¾‘æ¡†æ¶ï¼Œå®ƒæ•´åˆäº†é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹å’ŒMLLMã€‚MIND-Editå¼•å…¥äº†ä¸¤ç§äº’è¡¥çš„ç­–ç•¥ï¼š1ï¼‰ä¸€ç§æ–‡æœ¬æŒ‡ä»¤ä¼˜åŒ–ç­–ç•¥ï¼Œå®ƒåŸºäºMLLMçš„è¯­ä¹‰æ¨ç†æ¥æ¾„æ¸…æ¨¡ç³Šçš„ç”¨æˆ·æŒ‡ä»¤ï¼›2ï¼‰ä¸€ç§MLLMæ´å¯Ÿé©±åŠ¨çš„ç¼–è¾‘ç­–ç•¥ï¼Œå®ƒæ˜ç¡®åˆ©ç”¨MLLMçš„å†…åœ¨è§†è§‰ç†è§£èƒ½åŠ›æ¥æ¨æ–­ç¼–è¾‘æ„å›¾ï¼Œå¹¶é€šè¿‡ç”Ÿæˆçš„è§†è§‰åµŒå…¥æ¥æŒ‡å¯¼æ‰©æ•£è¿‡ç¨‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è”åˆè®­ç»ƒæ–¹æ³•ï¼Œæœ‰æ•ˆåœ°æ•´åˆäº†è¿™ä¸¤ç§ç­–ç•¥ï¼Œä½¿å®ƒä»¬åœ¨ç›¸äº’å¼ºåŒ–ä¸­å®ç°äº†æ›´ç²¾ç¡®çš„æŒ‡ä»¤è§£è¯»å’Œä¸ç”¨æˆ·æ„å›¾ç›¸ä¸€è‡´çš„è§†è§‰è¿è´¯ç¼–è¾‘ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMIND-Editåœ¨å®šé‡æŒ‡æ ‡å’Œè§†è§‰è´¨é‡ä¸Šè¶…è¶Šäº†æœ€å…ˆè¿›çš„å›¾åƒç¼–è¾‘æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚å’ŒæŒ‘æˆ˜æ€§çš„åœºæ™¯ä¸­ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19149v1">PDF</a> </p>
<p><strong>Summary</strong><br>     éšç€äººå·¥æ™ºèƒ½ç”Ÿæˆå†…å®¹ï¼ˆAIGCï¼‰çš„è¿›å±•ï¼Œå›¾åƒç¼–è¾‘æŠ€æœ¯å¾—åˆ°åŠ é€Ÿå‘å±•ï¼Œå¯¹å¤šæ ·åŒ–ã€ç²¾ç»†åŒ–çš„ç¼–è¾‘éœ€æ±‚ä¸æ–­å¢é•¿ã€‚ç°æœ‰å›¾åƒç¼–è¾‘æ–¹æ³•é¢ä¸´å¤æ‚åœºæ™¯ä¸‹é«˜ç²¾åº¦å’Œè¯­ä¹‰å‡†ç¡®åº¦çš„æŒ‘æˆ˜ã€‚æœ€æ–°ç ”ç©¶é€šè¿‡ç»“åˆå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚ç„¶è€Œï¼Œå½“å‰MLLMæ–¹æ³•ä¸»è¦ä¾èµ–æ–‡æœ¬æŒ‡ä»¤è§£è¯»ï¼Œå¯¹å¤§å‹æ¨¡å‹çš„å†…åœ¨è§†è§‰ç†è§£æ¢ç´¢ä¸è¶³ï¼Œå¯¼è‡´æ–‡æœ¬è¯­ä¹‰ä¸è§†è§‰ç»“æœå¯¹é½ä¸è¶³ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºMIND-Editï¼Œä¸€ä¸ªç»“åˆé¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ä¸MLLMçš„ç«¯åˆ°ç«¯å›¾åƒç¼–è¾‘æ¡†æ¶ã€‚MIND-Editå¼•å…¥ä¸¤ç§äº’è¡¥ç­–ç•¥ï¼šä¸€æ˜¯åŸºäºMLLMè¯­ä¹‰æ¨ç†çš„æ–‡æœ¬æŒ‡ä»¤ä¼˜åŒ–ç­–ç•¥ï¼Œæ¾„æ¸…æ¨¡ç³Šçš„ç”¨æˆ·æŒ‡ä»¤ï¼›äºŒæ˜¯åˆ©ç”¨MLLMå†…åœ¨è§†è§‰ç†è§£èƒ½åŠ›çš„ç¼–è¾‘ç­–ç•¥ï¼Œé€šè¿‡ç”Ÿæˆçš„è§†è§‰åµŒå…¥ç‰©æ¥æ¨æ–­ç¼–è¾‘æ„å›¾å¹¶å¼•å¯¼æ‰©æ•£è¿‡ç¨‹ã€‚æˆ‘ä»¬é€šè¿‡è”åˆè®­ç»ƒæ–¹æ³•æ¥æœ‰æ•ˆæ•´åˆè¿™ä¸¤ç§ç­–ç•¥ï¼Œä½¿å®ƒä»¬åœ¨æŒ‡ä»¤è§£è¯»å’Œè§†è§‰ç¼–è¾‘æ–¹é¢ç›¸äº’å¼ºåŒ–ï¼Œå®ç°æ›´ç¬¦åˆç”¨æˆ·æ„å›¾çš„å‡†ç¡®ç¼–è¾‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AI-generated content (AIGC) çš„è¿›æ­¥æ¨åŠ¨äº†å›¾åƒç¼–è¾‘æŠ€æœ¯çš„å¿«é€Ÿå‘å±•ï¼Œå¯¹ç¼–è¾‘çš„å¤šæ ·æ€§å’Œç²¾ç»†åº¦æå‡ºäº†æ›´é«˜è¦æ±‚ã€‚</li>
<li>ç°æœ‰å›¾åƒç¼–è¾‘æ–¹æ³•åœ¨å¤æ‚åœºæ™¯ä¸‹å®ç°é«˜ç²¾åº¦å’Œè¯­ä¹‰å‡†ç¡®åº¦æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰è¢«å¼•å…¥å›¾åƒç¼–è¾‘æµç¨‹ä»¥è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>å½“å‰MLLMæ–¹æ³•ä¸»è¦ä¾èµ–æ–‡æœ¬æŒ‡ä»¤è§£è¯»ï¼Œç¼ºä¹å†…åœ¨è§†è§‰ç†è§£çš„æ¢ç´¢ã€‚</li>
<li>MIND-Editæ¡†æ¶ç»“åˆé¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ä¸MLLMï¼Œæ—¨åœ¨è§£å†³ä¸Šè¿°æŒ‘æˆ˜ã€‚</li>
<li>MIND-Editå¼•å…¥æ–‡æœ¬æŒ‡ä»¤ä¼˜åŒ–ç­–ç•¥å’ŒåŸºäºMLLMè§†è§‰ç†è§£çš„ç¼–è¾‘ç­–ç•¥ã€‚</li>
<li>é€šè¿‡è”åˆè®­ç»ƒæ–¹æ³•æ•´åˆä¸¤ç§ç­–ç•¥ï¼Œå®ç°æ›´å‡†ç¡®ã€è§†è§‰è¿è´¯çš„ç¼–è¾‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19149">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-e1cdd623e67f90298706228441548c2d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-93861ce95effb63fe94fb2c1613bbaf6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9b3782c9b4aca9d08d55673fe985378f.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="MoRE-Brain-Routed-Mixture-of-Experts-for-Interpretable-and-Generalizable-Cross-Subject-fMRI-Visual-Decoding"><a href="#MoRE-Brain-Routed-Mixture-of-Experts-for-Interpretable-and-Generalizable-Cross-Subject-fMRI-Visual-Decoding" class="headerlink" title="MoRE-Brain: Routed Mixture of Experts for Interpretable and   Generalizable Cross-Subject fMRI Visual Decoding"></a>MoRE-Brain: Routed Mixture of Experts for Interpretable and   Generalizable Cross-Subject fMRI Visual Decoding</h2><p><strong>Authors:Yuxiang Wei, Yanteng Zhang, Xi Xiao, Tianyang Wang, Xiao Wang, Vince D. Calhoun</strong></p>
<p>Decoding visual experiences from fMRI offers a powerful avenue to understand human perception and develop advanced brain-computer interfaces. However, current progress often prioritizes maximizing reconstruction fidelity while overlooking interpretability, an essential aspect for deriving neuroscientific insight. To address this gap, we propose MoRE-Brain, a neuro-inspired framework designed for high-fidelity, adaptable, and interpretable visual reconstruction. MoRE-Brain uniquely employs a hierarchical Mixture-of-Experts architecture where distinct experts process fMRI signals from functionally related voxel groups, mimicking specialized brain networks. The experts are first trained to encode fMRI into the frozen CLIP space. A finetuned diffusion model then synthesizes images, guided by expert outputs through a novel dual-stage routing mechanism that dynamically weighs expert contributions across the diffusion process. MoRE-Brain offers three main advancements: First, it introduces a novel Mixture-of-Experts architecture grounded in brain network principles for neuro-decoding. Second, it achieves efficient cross-subject generalization by sharing core expert networks while adapting only subject-specific routers. Third, it provides enhanced mechanistic insight, as the explicit routing reveals precisely how different modeled brain regions shape the semantic and spatial attributes of the reconstructed image. Extensive experiments validate MoRE-Brainâ€™s high reconstruction fidelity, with bottleneck analyses further demonstrating its effective utilization of fMRI signals, distinguishing genuine neural decoding from over-reliance on generative priors. Consequently, MoRE-Brain marks a substantial advance towards more generalizable and interpretable fMRI-based visual decoding. Code will be publicly available soon: <a target="_blank" rel="noopener" href="https://github.com/yuxiangwei0808/MoRE-Brain">https://github.com/yuxiangwei0808/MoRE-Brain</a>. </p>
<blockquote>
<p>ä»åŠŸèƒ½ç£å…±æŒ¯æˆåƒï¼ˆfMRIï¼‰ä¸­è§£ç è§†è§‰ç»éªŒä¸ºæˆ‘ä»¬ç†è§£äººç±»æ„ŸçŸ¥å¹¶å¼€å‘å…ˆè¿›çš„è„‘æœºæ¥å£æä¾›äº†å¼ºå¤§çš„é€”å¾„ã€‚ç„¶è€Œï¼Œç›®å‰çš„è¿›å±•å¾€å¾€ä¼˜å…ˆæœ€å¤§åŒ–é‡å»ºä¿çœŸåº¦ï¼Œå´å¿½è§†äº†å¯è§£é‡Šæ€§è¿™ä¸€å¯¹äºè·å–ç¥ç»ç§‘å­¦æ´å¯ŸåŠ›çš„å¿…è¦æ–¹é¢ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†MoRE-Brainï¼Œè¿™æ˜¯ä¸€ä¸ªç¥ç»å¯å‘çš„æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°é«˜ä¿çœŸã€å¯é€‚åº”å’Œå¯è§£é‡Šçš„è§†è§‰é‡å»ºã€‚MoRE-Brainç‹¬ç‰¹åœ°é‡‡ç”¨äº†ä¸€ç§å±‚æ¬¡åŒ–çš„æ··åˆä¸“å®¶æ¶æ„ï¼Œå…¶ä¸­ä¸åŒçš„ä¸“å®¶å¤„ç†æ¥è‡ªåŠŸèƒ½ç›¸å…³ä½“ç´ ç»„çš„fMRIä¿¡å·ï¼Œæ¨¡ä»¿ä¸“é—¨çš„è„‘ç½‘ç»œã€‚ä¸“å®¶é¦–å…ˆè¢«è®­ç»ƒå°†fMRIç¼–ç åˆ°å›ºå®šçš„CLIPç©ºé—´ä¸­ã€‚ç„¶åï¼Œä¸€ä¸ªç»è¿‡å¾®è°ƒçš„åˆ†æ­¥æ¨¡å‹åœ¨ä¸“å®¶è¾“å‡ºçš„å¼•å¯¼ä¸‹é€šè¿‡ä¸€ç§æ–°å‹çš„åŒé˜¶æ®µè·¯ç”±æœºåˆ¶åˆæˆå›¾åƒï¼Œè¯¥æœºåˆ¶åœ¨æ‰©æ•£è¿‡ç¨‹ä¸­åŠ¨æ€æƒè¡¡ä¸“å®¶çš„è´¡çŒ®ã€‚MoRE-Brainæä¾›äº†ä¸‰ä¸ªä¸»è¦è¿›å±•ï¼šé¦–å…ˆï¼Œå®ƒå¼•å…¥äº†ä¸€ç§åŸºäºè„‘ç½‘ç»œåŸç†çš„æ–°å‹æ··åˆä¸“å®¶æ¶æ„æ¥è¿›è¡Œç¥ç»è§£ç ã€‚å…¶æ¬¡ï¼Œå®ƒé€šè¿‡å…±äº«æ ¸å¿ƒä¸“å®¶ç½‘ç»œå¹¶ä»…è°ƒæ•´ç‰¹å®šä¸»é¢˜çš„è·¯ç”±å™¨ï¼Œå®ç°äº†è·¨ä¸»é¢˜çš„æœ‰æ•ˆæ³›åŒ–ã€‚ç¬¬ä¸‰ï¼Œå®ƒæä¾›äº†å¢å¼ºçš„æœºæ¢°æ´å¯ŸåŠ›ï¼Œå› ä¸ºæ˜ç¡®çš„è·¯ç”±æ­ç¤ºäº†ä¸åŒçš„æ¨¡æ‹Ÿè„‘åŒºåŸŸå¦‚ä½•å¡‘é€ é‡å»ºå›¾åƒçš„è¯­ä¹‰å’Œç©ºé—´å±æ€§ã€‚å¤§é‡å®éªŒéªŒè¯äº†MoRE-Brainçš„é«˜é‡å»ºä¿çœŸåº¦ï¼Œç“¶é¢ˆåˆ†æè¿›ä¸€æ­¥è¯æ˜äº†å®ƒæœ‰æ•ˆåˆ©ç”¨fMRIä¿¡å·çš„èƒ½åŠ›ï¼ŒåŒºåˆ†äº†çœŸæ­£çš„ç¥ç»è§£ç å’Œå¯¹ç”Ÿæˆä¼˜å…ˆçº§çš„ä¾èµ–ã€‚å› æ­¤ï¼ŒMoRE-Brainæ ‡å¿—ç€æœç€æ›´é€šç”¨å’Œå¯è§£é‡Šçš„åŸºäºfMRIçš„è§†è§‰è§£ç å–å¾—äº†é‡å¤§è¿›å±•ã€‚ä»£ç å°†åœ¨è¿‘æœŸå…¬å¼€å‘å¸ƒäºï¼š<a target="_blank" rel="noopener" href="https://github.com/yuxiangwei0808/MoRE-Brain">ç½‘ç«™é“¾æ¥</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15946v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æ¢è®¨äº†è§£ç åŠŸèƒ½ç£å…±æŒ¯æˆåƒï¼ˆfMRIï¼‰è§†è§‰ä½“éªŒçš„æ–°æ–¹æ³•ï¼Œä¸ºè§£å†³å½“å‰ç ”ç©¶ä¸­è¿‡äºå¼ºè°ƒé‡å»ºä¿çœŸåº¦è€Œå¿½è§†è§£é‡Šæ€§çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºMoRE-Brainçš„ç¥ç»å¯å‘æ¡†æ¶ã€‚è¯¥æ¡†æ¶å…·æœ‰é«˜ç²¾åº¦ã€å¯é€‚åº”å’Œå¯è§£é‡Šçš„ç‰¹ç‚¹ï¼Œé‡‡ç”¨å±‚æ¬¡åŒ–çš„æ··åˆä¸“å®¶æ¶æ„å¤„ç†fMRIä¿¡å·ï¼Œå¹¶é€šè¿‡è®­ç»ƒä¸“å®¶å¯¹fMRIè¿›è¡Œç¼–ç ï¼Œå†å€ŸåŠ©å¾®è°ƒåçš„æ‰©æ•£æ¨¡å‹åˆæˆå›¾åƒã€‚MoRE-Brainå…·æœ‰ä¸‰å¤§ä¼˜åŠ¿ï¼šå¼•å…¥åŸºäºè„‘ç½‘ç»œåŸç†çš„æ··åˆä¸“å®¶æ¶æ„ã€å®ç°è·¨ä¸»ä½“é«˜æ•ˆæ³›åŒ–ä»¥åŠæä¾›å¢å¼ºæœºåˆ¶æ´å¯ŸåŠ›ã€‚ä»£ç å³å°†å…¬å¼€å‘å¸ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰ä½“éªŒçš„è§£ç æä¾›äº†ç†è§£äººç±»æ„ŸçŸ¥å’Œç ”å‘å…ˆè¿›è„‘æœºæ¥å£çš„å¼ºå¤§é€”å¾„ã€‚</li>
<li>å½“å‰ç ”ç©¶ä¸­è¿‡äºé‡è§†é‡å»ºä¿çœŸåº¦è€Œå¿½è§†äº†è§£é‡Šæ€§ï¼Œè¿™æ˜¯ç¥ç»ç§‘å­¦æ´å¯Ÿçš„å…³é”®æ–¹é¢ã€‚</li>
<li>MoRE-Brainæ¡†æ¶å…·æœ‰é«˜ç²¾åº¦ã€å¯é€‚åº”å’Œå¯è§£é‡Šçš„ç‰¹ç‚¹ï¼Œè§£å†³äº†ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>MoRE-Brainé‡‡ç”¨æ··åˆä¸“å®¶æ¶æ„å¤„ç†fMRIä¿¡å·ï¼Œæ¨¡ä»¿ä¸“é—¨åŒ–çš„è„‘ç½‘ç»œã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡è®­ç»ƒä¸“å®¶å¯¹fMRIè¿›è¡Œç¼–ç ï¼Œå¹¶é€šè¿‡æ‰©æ•£æ¨¡å‹åˆæˆå›¾åƒï¼Œæä¾›ä¸€ç§æ–°çš„è§£ç æ–¹æ³•ã€‚</li>
<li>MoRE-Brainå®ç°è·¨ä¸»ä½“æ³›åŒ–ï¼Œé€šè¿‡å…±äº«æ ¸å¿ƒä¸“å®¶ç½‘ç»œå¹¶ä»…è°ƒæ•´ä¸»ä½“ç‰¹å®šè·¯ç”±å™¨æ¥å®ç°é«˜æ•ˆæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15946">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f7daf3e2d613d53b19aa104fbd1f9ae9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bc92da07951ccfe9eb89d2a0344d7a5d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-645e01c92190ccd6f4f3b22d1c901e02.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8de6a61ae5f5f1b4e04a184b09a34118.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-669d4398a30ac8aa316d07615f14e37b.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Improving-Compositional-Generation-with-Diffusion-Models-Using-Lift-Scores"><a href="#Improving-Compositional-Generation-with-Diffusion-Models-Using-Lift-Scores" class="headerlink" title="Improving Compositional Generation with Diffusion Models Using Lift   Scores"></a>Improving Compositional Generation with Diffusion Models Using Lift   Scores</h2><p><strong>Authors:Chenning Yu, Sicun Gao</strong></p>
<p>We introduce a novel resampling criterion using lift scores, for improving compositional generation in diffusion models. By leveraging the lift scores, we evaluate whether generated samples align with each single condition and then compose the results to determine whether the composed prompt is satisfied. Our key insight is that lift scores can be efficiently approximated using only the original diffusion model, requiring no additional training or external modules. We develop an optimized variant that achieves relatively lower computational overhead during inference while maintaining effectiveness. Through extensive experiments, we demonstrate that lift scores significantly improved the condition alignment for compositional generation across 2D synthetic data, CLEVR position tasks, and text-to-image synthesis. Our code is available at <a target="_blank" rel="noopener" href="http://rainorangelemon.github.io/complift">http://rainorangelemon.github.io/complift</a>. </p>
<blockquote>
<p>æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„é‡é‡‡æ ·å‡†åˆ™ï¼Œåˆ©ç”¨æå‡åˆ†æ•°æ¥æé«˜æ‰©æ•£æ¨¡å‹ä¸­çš„ç»„åˆç”Ÿæˆã€‚é€šè¿‡åˆ©ç”¨æå‡åˆ†æ•°ï¼Œæˆ‘ä»¬è¯„ä¼°ç”Ÿæˆçš„æ ·æœ¬æ˜¯å¦ç¬¦åˆå•ä¸ªæ¡ä»¶ï¼Œç„¶åå°†ç»“æœç»„åˆèµ·æ¥ï¼Œä»¥ç¡®å®šç»„åˆæç¤ºæ˜¯å¦æ»¡è¶³è¦æ±‚ã€‚æˆ‘ä»¬çš„å…³é”®è§è§£æ˜¯ï¼Œä»…ä½¿ç”¨åŸå§‹çš„æ‰©æ•£æ¨¡å‹å°±å¯ä»¥æœ‰æ•ˆåœ°è¿‘ä¼¼æå‡åˆ†æ•°ï¼Œè€Œæ— éœ€è¿›è¡Œé¢å¤–çš„è®­ç»ƒæˆ–å¤–éƒ¨æ¨¡å—ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªä¼˜åŒ–ç‰ˆæœ¬ï¼Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­å®ç°äº†è¾ƒä½çš„è®¡ç®—å¼€é”€ï¼ŒåŒæ—¶ä¿æŒäº†æœ‰æ•ˆæ€§ã€‚é€šè¿‡å¤§é‡å®éªŒï¼Œæˆ‘ä»¬è¯æ˜æå‡åˆ†æ•°åœ¨2Dåˆæˆæ•°æ®ã€CLEVRä½ç½®ä»»åŠ¡å’Œæ–‡æœ¬åˆ°å›¾åƒåˆæˆä¸­çš„ç»„åˆç”Ÿæˆæ¡ä»¶å¯¹é½æ–¹é¢æœ‰äº†æ˜¾è‘—æ”¹è¿›ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="http://rainorangelemon.github.io/complift%E4%B8%8A%E8%8E%B7%E5%8F%96%E3%80%82">http://rainorangelemon.github.io/compliftä¸Šè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13740v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ä½¿ç”¨æå‡åˆ†æ•°ï¼ˆlift scoresï¼‰çš„æ–°å‹é‡é‡‡æ ·æ ‡å‡†ï¼Œç”¨äºæ”¹è¿›æ‰©æ•£æ¨¡å‹ä¸­çš„ç»„åˆç”Ÿæˆã€‚è¯¥æ ‡å‡†èƒ½å¤Ÿè¯„ä¼°ç”Ÿæˆçš„æ ·æœ¬æ˜¯å¦ç¬¦åˆå•ä¸ªæ¡ä»¶ï¼Œå¹¶å°†ç»“æœç»„åˆèµ·æ¥åˆ¤æ–­ç»„åˆæç¤ºæ˜¯å¦æ»¡è¶³è¦æ±‚ã€‚å…³é”®ä¹‹å¤„åœ¨äºï¼Œåªéœ€ä½¿ç”¨åŸå§‹æ‰©æ•£æ¨¡å‹å³å¯æœ‰æ•ˆä¼°ç®—æå‡åˆ†æ•°ï¼Œæ— éœ€é¢å¤–çš„è®­ç»ƒæˆ–å¤–éƒ¨æ¨¡å—ã€‚å®éªŒè¡¨æ˜ï¼Œæå‡åˆ†æ•°èƒ½æ˜¾è‘—æé«˜ç»„åˆç”Ÿæˆçš„æ¡ä»¶å¯¹é½æ€§ï¼Œé€‚ç”¨äºäºŒç»´åˆæˆæ•°æ®ã€CLEVRä½ç½®ä»»åŠ¡å’Œæ–‡æœ¬åˆ°å›¾åƒåˆæˆç­‰é¢†åŸŸã€‚ç›¸å…³ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="http://rainorangelemon.github.io/complift%E8%8E%B7%E5%8F%96%E3%80%82">http://rainorangelemon.github.io/compliftè·å–ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥äº†ä¸€ç§æ–°çš„é‡é‡‡æ ·æ ‡å‡†â€”â€”æå‡åˆ†æ•°ï¼ˆlift scoresï¼‰ï¼Œç”¨äºæ”¹è¿›æ‰©æ•£æ¨¡å‹ä¸­çš„ç»„åˆç”Ÿæˆã€‚</li>
<li>æå‡åˆ†æ•°èƒ½å¤Ÿè¯„ä¼°ç”Ÿæˆçš„æ ·æœ¬æ˜¯å¦ç¬¦åˆå•ä¸ªæ¡ä»¶ï¼Œè¿›è€Œåˆ¤æ–­ç»„åˆæç¤ºæ˜¯å¦æ»¡è¶³è¦æ±‚ã€‚</li>
<li>è¯¥æ–¹æ³•æ— éœ€é¢å¤–çš„è®­ç»ƒæˆ–å¤–éƒ¨æ¨¡å—ï¼Œä»…ä½¿ç”¨åŸå§‹æ‰©æ•£æ¨¡å‹å³å¯ä¼°ç®—æå‡åˆ†æ•°ã€‚</li>
<li>å®éªŒè¡¨æ˜æå‡åˆ†æ•°åœ¨äºŒç»´åˆæˆæ•°æ®ã€CLEVRä½ç½®ä»»åŠ¡å’Œæ–‡æœ¬åˆ°å›¾åƒåˆæˆç­‰æ–¹é¢èƒ½å¤Ÿæ˜¾è‘—æé«˜æ¡ä»¶å¯¹é½æ€§ã€‚</li>
<li>æ‰€æå‡ºçš„æ–¹æ³•å…·æœ‰è¾ƒä½çš„è®¡ç®—å¼€é”€ï¼ŒåŒæ—¶ä¿æŒäº†æœ‰æ•ˆæ€§ã€‚</li>
<li>è¯¥æ–¹æ³•çš„å®ç°ä»£ç å·²å…¬å¼€å¯ç”¨ã€‚</li>
<li>è¯¥æ–¹æ³•å¯¹äºæå‡æ‰©æ•£æ¨¡å‹çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚çš„æ•°æ®ç”Ÿæˆä»»åŠ¡ä¸­å…·æœ‰æ½œåœ¨çš„åº”ç”¨ä»·å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13740">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bc12247da19a7bb41dd63e3b4862d908.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb601352cea36c65df1e2ff04831fe26.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9775cc942c4b39724d411f90a976bf5a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fadb393d93e9024fe7821eebbee35acc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-90505589fe6965214af3842f98dcd66c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f76fec77ac13212fb481bb0f213ad001.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f856c3596af2599fbca5cfe38566d461.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="One-Image-is-Worth-a-Thousand-Words-A-Usability-Preservable-Text-Image-Collaborative-Erasing-Framework"><a href="#One-Image-is-Worth-a-Thousand-Words-A-Usability-Preservable-Text-Image-Collaborative-Erasing-Framework" class="headerlink" title="One Image is Worth a Thousand Words: A Usability Preservable Text-Image   Collaborative Erasing Framework"></a>One Image is Worth a Thousand Words: A Usability Preservable Text-Image   Collaborative Erasing Framework</h2><p><strong>Authors:Feiran Li, Qianqian Xu, Shilong Bao, Zhiyong Yang, Xiaochun Cao, Qingming Huang</strong></p>
<p>Concept erasing has recently emerged as an effective paradigm to prevent text-to-image diffusion models from generating visually undesirable or even harmful content. However, current removal methods heavily rely on manually crafted text prompts, making it challenging to achieve a high erasure (efficacy) while minimizing the impact on other benign concepts (usability). In this paper, we attribute the limitations to the inherent gap between the text and image modalities, which makes it hard to transfer the intricately entangled concept knowledge from text prompts to the image generation process. To address this, we propose a novel solution by directly integrating visual supervision into the erasure process, introducing the first text-image Collaborative Concept Erasing (Co-Erasing) framework. Specifically, Co-Erasing describes the concept jointly by text prompts and the corresponding undesirable images induced by the prompts, and then reduces the generating probability of the target concept through negative guidance. This approach effectively bypasses the knowledge gap between text and image, significantly enhancing erasure efficacy. Additionally, we design a text-guided image concept refinement strategy that directs the model to focus on visual features most relevant to the specified text concept, minimizing disruption to other benign concepts. Finally, comprehensive experiments suggest that Co-Erasing outperforms state-of-the-art erasure approaches significantly with a better trade-off between efficacy and usability. Codes are available at <a target="_blank" rel="noopener" href="https://github.com/Ferry-Li/Co-Erasing">https://github.com/Ferry-Li/Co-Erasing</a>. </p>
<blockquote>
<p>æ¦‚å¿µæ“¦é™¤ä½œä¸ºä¸€ç§æœ‰æ•ˆçš„èŒƒå¼ï¼Œæ—¨åœ¨é˜²æ­¢æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹ç”Ÿæˆè§†è§‰ä¸è‰¯ç”šè‡³æœ‰å®³çš„å†…å®¹ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ“¦é™¤æ–¹æ³•ä¸¥é‡ä¾èµ–äºæ‰‹åŠ¨æ„å»ºçš„æ–‡æœ¬æç¤ºï¼Œè¿™ä½¿å¾—åœ¨å‡å°‘å¯¹å…¶ä»–è‰¯æ€§æ¦‚å¿µï¼ˆå¯ç”¨æ€§ï¼‰å½±å“çš„åŒæ—¶å®ç°é«˜æ“¦é™¤ç‡ï¼ˆæœ‰æ•ˆæ€§ï¼‰å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æœ¬æ–‡è®¤ä¸ºè¿™äº›å±€é™æ€§æºäºæ–‡æœ¬å’Œå›¾åƒæ¨¡æ€ä¹‹é—´çš„å›ºæœ‰å·®è·ï¼Œè¿™ä½¿å¾—ä»æ–‡æœ¬æç¤ºå°†å¤æ‚çº ç¼ çš„æ¦‚å¿µçŸ¥è¯†è½¬ç§»åˆ°å›¾åƒç”Ÿæˆè¿‡ç¨‹å˜å¾—å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é€šè¿‡ç›´æ¥åœ¨æ“¦é™¤è¿‡ç¨‹ä¸­å¼•å…¥è§†è§‰ç›‘ç£çš„è§£å†³æ–¹æ³•ï¼Œæå‡ºäº†é¦–ä¸ªæ–‡æœ¬å›¾åƒååŒæ¦‚å¿µæ“¦é™¤ï¼ˆCo-Erasingï¼‰æ¡†æ¶ã€‚å…·ä½“æ¥è¯´ï¼ŒCo-Erasingé€šè¿‡æ–‡æœ¬æç¤ºå’Œç”±æç¤ºè¯±å¯¼çš„ç›¸åº”ä¸è‰¯å›¾åƒå…±åŒæè¿°æ¦‚å¿µï¼Œç„¶åé€šè¿‡è´Ÿå¼•å¯¼é™ä½ç›®æ ‡æ¦‚å¿µçš„äº§ç”Ÿæ¦‚ç‡ã€‚è¿™ç§æ–¹æ³•æœ‰æ•ˆåœ°ç»•è¿‡äº†æ–‡æœ¬å’Œå›¾åƒä¹‹é—´çš„çŸ¥è¯†å·®è·ï¼Œå¤§å¤§æé«˜äº†æ“¦é™¤æ•ˆæœã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è®¾è®¡äº†ä¸€ç§æ–‡æœ¬å¼•å¯¼çš„å›¾åƒæ¦‚å¿µç»†åŒ–ç­–ç•¥ï¼Œå¼•å¯¼æ¨¡å‹å…³æ³¨ä¸æŒ‡å®šæ–‡æœ¬æ¦‚å¿µæœ€ç›¸å…³çš„è§†è§‰ç‰¹å¾ï¼Œå°½é‡å‡å°‘å¯¹å…¶ä»–è‰¯æ€§æ¦‚å¿µçš„å¹²æ‰°ã€‚æœ€åï¼Œç»¼åˆå®éªŒè¡¨æ˜ï¼ŒCo-Erasingåœ¨æ•ˆæœå’Œå¯ç”¨æ€§ä¹‹é—´å–å¾—äº†æ›´å¥½çš„å¹³è¡¡ï¼Œæ˜¾è‘—ä¼˜äºæœ€å…ˆè¿›çš„æ“¦é™¤æ–¹æ³•ã€‚ä»£ç å¯ä»<a target="_blank" rel="noopener" href="https://github.com/Ferry-Li/Co-Erasing%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/Ferry-Li/Co-Erasingè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11131v2">PDF</a> This paper has been accepeted to ICML 2025</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹æ–‡æœ¬-å›¾åƒååŒæ¦‚å¿µæ“¦é™¤ï¼ˆCo-Erasingï¼‰æ¡†æ¶ï¼Œé€šè¿‡ç›´æ¥æ•´åˆè§†è§‰ç›‘ç£åˆ°æ“¦é™¤è¿‡ç¨‹ä¸­ï¼Œä»¥æœ‰æ•ˆé˜²æ­¢æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ç”Ÿæˆä¸è‰¯å†…å®¹ã€‚è¯¥æ¡†æ¶é€šè¿‡è”åˆæ–‡æœ¬æç¤ºå’Œå¯¹åº”çš„ä¸è‰¯å›¾åƒæè¿°æ¦‚å¿µï¼Œç„¶åé€šè¿‡è´Ÿå¼•å¯¼é™ä½ç›®æ ‡æ¦‚å¿µçš„äº§ç”Ÿæ¦‚ç‡ï¼Œä»è€Œæ˜¾è‘—æé«˜äº†æ“¦é™¤æ•ˆæœã€‚æ­¤å¤–ï¼Œè¿˜è®¾è®¡äº†ä¸€ç§æ–‡æœ¬å¼•å¯¼çš„å›¾åƒæ¦‚å¿µä¼˜åŒ–ç­–ç•¥ï¼Œä½¿æ¨¡å‹æ›´å…³æ³¨ä¸æŒ‡å®šæ–‡æœ¬æ¦‚å¿µç›¸å…³çš„è§†è§‰ç‰¹å¾ï¼Œæœ€å°åŒ–å¯¹å…¶ä»–è‰¯æ€§æ¦‚å¿µçš„å¹²æ‰°ã€‚å®éªŒè¡¨æ˜ï¼ŒCo-Erasingåœ¨æ“¦é™¤æ•ˆæœå’Œå¯ç”¨æ€§ä¹‹é—´çš„æƒè¡¡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰æ–‡æœ¬æ“¦é™¤æ–¹æ³•å­˜åœ¨æŒ‘æˆ˜ï¼Œéœ€è¦åœ¨å®ç°é«˜æ“¦é™¤æ•ˆæœçš„åŒæ—¶æœ€å°åŒ–å¯¹å…¶ä»–è‰¯æ€§æ¦‚å¿µçš„å½±å“ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹æ–‡æœ¬-å›¾åƒååŒæ¦‚å¿µæ“¦é™¤æ¡†æ¶ï¼ˆCo-Erasingï¼‰ï¼Œé€šè¿‡ç›´æ¥æ•´åˆè§†è§‰ç›‘ç£åˆ°æ“¦é™¤è¿‡ç¨‹ä¸­æ¥è§£å†³æŒ‘æˆ˜ã€‚</li>
<li>Co-Erasingé€šè¿‡è”åˆæ–‡æœ¬æç¤ºå’Œä¸è‰¯å›¾åƒæè¿°æ¦‚å¿µæ¥æœ‰æ•ˆå‡å°‘ç›®æ ‡æ¦‚å¿µçš„ç”Ÿæˆæ¦‚ç‡ã€‚</li>
<li>Co-Erasingè®¾è®¡äº†æ–‡æœ¬å¼•å¯¼çš„å›¾åƒæ¦‚å¿µä¼˜åŒ–ç­–ç•¥ï¼Œä¸“æ³¨äºä¸æŒ‡å®šæ–‡æœ¬æ¦‚å¿µç›¸å…³çš„è§†è§‰ç‰¹å¾ï¼Œå‡å°‘å¯¹å…¶ä»–è‰¯æ€§æ¦‚å¿µçš„å¹²æ‰°ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒCo-Erasingåœ¨æ“¦é™¤æ•ˆæœå’Œå¯ç”¨æ€§ä¹‹é—´çš„æƒè¡¡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>Co-Erasingæ¡†æ¶çš„ä»£ç å·²å…¬å¼€å¯ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11131">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-16bf8f0f2c0b701f34c0bf04fac1d805.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8fb72710972d4d5e62ae2284f2f9b665.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-016182232ad7e9656dd22c9fbebcf50d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-61fb0c91b190c8da87800954f57efef9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2b8b1806c78253c43f3adc9d074decd7.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Inference-Time-Scaling-for-Flow-Models-via-Stochastic-Generation-and-Rollover-Budget-Forcing"><a href="#Inference-Time-Scaling-for-Flow-Models-via-Stochastic-Generation-and-Rollover-Budget-Forcing" class="headerlink" title="Inference-Time Scaling for Flow Models via Stochastic Generation and   Rollover Budget Forcing"></a>Inference-Time Scaling for Flow Models via Stochastic Generation and   Rollover Budget Forcing</h2><p><strong>Authors:Jaihoon Kim, Taehoon Yoon, Jisung Hwang, Minhyuk Sung</strong></p>
<p>We propose an inference-time scaling approach for pretrained flow models. Recently, inference-time scaling has gained significant attention in LLMs and diffusion models, improving sample quality or better aligning outputs with user preferences by leveraging additional computation. For diffusion models, particle sampling has allowed more efficient scaling due to the stochasticity at intermediate denoising steps. On the contrary, while flow models have gained popularity as an alternative to diffusion modelsâ€“offering faster generation and high-quality outputs in state-of-the-art image and video generative modelsâ€“efficient inference-time scaling methods used for diffusion models cannot be directly applied due to their deterministic generative process. To enable efficient inference-time scaling for flow models, we propose three key ideas: 1) SDE-based generation, enabling particle sampling in flow models, 2) Interpolant conversion, broadening the search space and enhancing sample diversity, and 3) Rollover Budget Forcing (RBF), an adaptive allocation of computational resources across timesteps to maximize budget utilization. Our experiments show that SDE-based generation, particularly variance-preserving (VP) interpolant-based generation, improves the performance of particle sampling methods for inference-time scaling in flow models. Additionally, we demonstrate that RBF with VP-SDE achieves the best performance, outperforming all previous inference-time scaling approaches. </p>
<blockquote>
<p>æˆ‘ä»¬ä¸ºé¢„è®­ç»ƒçš„æµæ¨¡å‹æå‡ºäº†ä¸€ç§æ¨ç†æ—¶é—´å°ºåº¦æ–¹æ³•ã€‚æœ€è¿‘ï¼Œæ¨ç†æ—¶é—´å°ºåº¦åœ¨å¤§å‹è¯­è¨€æ¨¡å‹å’Œæ‰©æ•£æ¨¡å‹ä¸­å¼•èµ·äº†å¹¿æ³›å…³æ³¨ï¼Œé€šè¿‡åˆ©ç”¨é¢å¤–çš„è®¡ç®—æ¥æé«˜æ ·æœ¬è´¨é‡æˆ–æ›´å¥½åœ°ä½¿è¾“å‡ºä¸ç”¨æˆ·åå¥½å¯¹é½ã€‚å¯¹äºæ‰©æ•£æ¨¡å‹ï¼Œç”±äºä¸­é—´å»å™ªæ­¥éª¤çš„éšæœºæ€§ï¼Œç²’å­é‡‡æ ·å…è®¸æ›´æœ‰æ•ˆçš„å°ºåº¦æ‰©å±•ã€‚ç›¸åï¼Œè™½ç„¶æµæ¨¡å‹ä½œä¸ºæ‰©æ•£æ¨¡å‹çš„æ›¿ä»£å“è€Œå¹¿å—æ¬¢è¿ï¼Œæä¾›äº†æ›´å¿«çš„ç”Ÿæˆé€Ÿåº¦å’Œé«˜è´¨é‡è¾“å‡ºï¼Œåœ¨å…ˆè¿›çš„å›¾åƒå’Œè§†é¢‘ç”Ÿæˆæ¨¡å‹ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†ç”¨äºæ‰©æ•£æ¨¡å‹çš„æœ‰æ•ˆæ¨ç†æ—¶é—´å°ºåº¦æ–¹æ³•ä¸èƒ½ç›´æ¥åº”ç”¨äºæµæ¨¡å‹ï¼Œå› ä¸ºå®ƒä»¬å…·æœ‰ç¡®å®šçš„ç”Ÿæˆè¿‡ç¨‹ã€‚ä¸ºäº†å®ç°æµæ¨¡å‹çš„æ¨ç†æ—¶é—´å°ºåº¦çš„é«˜æ•ˆæ‰©å±•ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸‰ä¸ªå…³é”®æƒ³æ³•ï¼š1ï¼‰åŸºäºSDEçš„ç”Ÿæˆï¼Œä½¿æµæ¨¡å‹èƒ½å¤Ÿè¿›è¡Œç²’å­é‡‡æ ·ï¼›2ï¼‰æ’å€¼è½¬æ¢ï¼Œæ‰©å¤§æœç´¢ç©ºé—´å¹¶å¢å¼ºæ ·æœ¬å¤šæ ·æ€§ï¼›3ï¼‰æ»šåŠ¨é¢„ç®—å¼ºåˆ¶ï¼ˆRBFï¼‰ï¼Œè‡ªé€‚åº”åœ°åœ¨æ—¶é—´æ­¥é•¿ä¹‹é—´åˆ†é…è®¡ç®—èµ„æºï¼Œä»¥æœ€å¤§åŒ–é¢„ç®—åˆ©ç”¨ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒåŸºäºSDEçš„ç”Ÿæˆï¼Œç‰¹åˆ«æ˜¯åŸºäºæ–¹å·®ä¿æŒï¼ˆVPï¼‰æ’å€¼çš„ç”Ÿæˆï¼Œæ”¹è¿›äº†æµæ¨¡å‹ä¸­æ¨ç†æ—¶é—´å°ºåº¦çš„ç²’å­é‡‡æ ·çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¯æ˜äº†VP-SDEä¸RBFç›¸ç»“åˆçš„æ–¹æ³•å–å¾—äº†æœ€ä½³æ€§èƒ½ï¼Œä¼˜äºæ‰€æœ‰å…ˆå‰çš„æ¨ç†æ—¶é—´å°ºåº¦æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19385v3">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://flow-inference-time-scaling.github.io/">https://flow-inference-time-scaling.github.io/</a></p>
<p><strong>Summary</strong><br>     é’ˆå¯¹é¢„è®­ç»ƒçš„æµç¨‹æ¨¡å‹ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ¨ç†æ—¶é—´ç¼©æ”¾æ–¹æ³•ã€‚é€šè¿‡å¼•å…¥åŸºäºSDEçš„ç”Ÿæˆæ–¹æ³•ã€æ’å€¼è½¬æ¢å’Œæ»šå­˜é¢„ç®—å¼ºåˆ¶ï¼ˆRBFï¼‰ç­‰æŠ€æœ¯ï¼Œå®ç°äº†æµç¨‹æ¨¡å‹çš„æ¨ç†æ—¶é—´é«˜æ•ˆç¼©æ”¾ã€‚å®éªŒè¡¨æ˜ï¼ŒVPæ’å€¼åŸºç”Ÿæˆå’ŒRBFæŠ€æœ¯èƒ½å¤Ÿæ˜¾è‘—æé«˜ç²’å­é‡‡æ ·æ–¹æ³•åœ¨æµç¨‹æ¨¡å‹æ¨ç†æ—¶é—´ç¼©æ”¾ä¸­çš„æ€§èƒ½ï¼Œå¹¶ä¼˜äºä»¥å‰çš„æ¨ç†æ—¶é—´ç¼©æ”¾æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†é’ˆå¯¹é¢„è®­ç»ƒæµç¨‹æ¨¡å‹çš„æ¨ç†æ—¶é—´ç¼©æ”¾æ–¹æ³•ã€‚</li>
<li>åˆ©ç”¨SDEï¼ˆéšæœºå¾®åˆ†æ–¹ç¨‹ï¼‰åŸºç¡€ç”Ÿæˆæ–¹æ³•ï¼Œä½¿æµç¨‹æ¨¡å‹èƒ½å¤Ÿå®ç°ç²’å­é‡‡æ ·ã€‚</li>
<li>é€šè¿‡æ’å€¼è½¬æ¢æŠ€æœ¯æ‹“å®½æœç´¢ç©ºé—´ï¼Œæé«˜æ ·æœ¬å¤šæ ·æ€§ã€‚</li>
<li>ä»‹ç»äº†ä¸€ç§åä¸ºæ»šå­˜é¢„ç®—å¼ºåˆ¶ï¼ˆRBFï¼‰çš„æŠ€æœ¯ï¼Œå¯è‡ªé€‚åº”åˆ†é…è®¡ç®—èµ„æºä»¥æœ€å¤§åŒ–é¢„ç®—åˆ©ç”¨ã€‚</li>
<li>VPæ’å€¼åŸºç”Ÿæˆä¸RBFæŠ€æœ¯ç»“åˆä½¿ç”¨åœ¨æµç¨‹æ¨¡å‹æ¨ç†æ—¶é—´ç¼©æ”¾ä¸­è¡¨ç°å‡ºæœ€ä½³æ€§èƒ½ã€‚</li>
<li>è¯¥æ–¹æ³•æ˜¾è‘—æé«˜äº†æµç¨‹æ¨¡å‹çš„ç²’å­é‡‡æ ·æ–¹æ³•çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19385">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5c52c894a2fb396d2283b2602eedd9bc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-55ab56c662782ce854fe7e239918aad6.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Attentive-Eraser-Unleashing-Diffusion-Modelâ€™s-Object-Removal-Potential-via-Self-Attention-Redirection-Guidance"><a href="#Attentive-Eraser-Unleashing-Diffusion-Modelâ€™s-Object-Removal-Potential-via-Self-Attention-Redirection-Guidance" class="headerlink" title="Attentive Eraser: Unleashing Diffusion Modelâ€™s Object Removal Potential   via Self-Attention Redirection Guidance"></a>Attentive Eraser: Unleashing Diffusion Modelâ€™s Object Removal Potential   via Self-Attention Redirection Guidance</h2><p><strong>Authors:Wenhao Sun, Benlei Cui, Xue-Mei Dong, Jingqun Tang, Yi Liu</strong></p>
<p>Recently, diffusion models have emerged as promising newcomers in the field of generative models, shining brightly in image generation. However, when employed for object removal tasks, they still encounter issues such as generating random artifacts and the incapacity to repaint foreground object areas with appropriate content after removal. To tackle these problems, we propose Attentive Eraser, a tuning-free method to empower pre-trained diffusion models for stable and effective object removal. Firstly, in light of the observation that the self-attention maps influence the structure and shape details of the generated images, we propose Attention Activation and Suppression (ASS), which re-engineers the self-attention mechanism within the pre-trained diffusion models based on the given mask, thereby prioritizing the background over the foreground object during the reverse generation process. Moreover, we introduce Self-Attention Redirection Guidance (SARG), which utilizes the self-attention redirected by ASS to guide the generation process, effectively removing foreground objects within the mask while simultaneously generating content that is both plausible and coherent. Experiments demonstrate the stability and effectiveness of Attentive Eraser in object removal across a variety of pre-trained diffusion models, outperforming even training-based methods. Furthermore, Attentive Eraser can be implemented in various diffusion model architectures and checkpoints, enabling excellent scalability. Code is available at <a target="_blank" rel="noopener" href="https://github.com/Anonym0u3/AttentiveEraser">https://github.com/Anonym0u3/AttentiveEraser</a>. </p>
<blockquote>
<p>è¿‘æœŸï¼Œæ‰©æ•£æ¨¡å‹ä½œä¸ºç”Ÿæˆæ¨¡å‹é¢†åŸŸçš„æ–°æ™‹çƒ­é—¨ï¼Œåœ¨å›¾åƒç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ã€‚ç„¶è€Œï¼Œå½“åº”ç”¨äºç›®æ ‡ç§»é™¤ä»»åŠ¡æ—¶ï¼Œå®ƒä»¬ä»ç„¶é¢ä¸´ä¸€äº›é—®é¢˜ï¼Œä¾‹å¦‚äº§ç”Ÿéšæœºä¼ªå½±ä»¥åŠåœ¨ç§»é™¤åæ— æ³•é‡æ–°ç»˜åˆ¶å‰æ™¯å¯¹è±¡åŒºåŸŸä»¥å¡«å……é€‚å½“å†…å®¹ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†â€œæ³¨æ„åŠ›æ“¦é™¤å™¨â€ï¼ˆAttentive Eraserï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€è°ƒæ•´çš„æ–¹æ³•ï¼Œå¯å¢å¼ºé¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ï¼Œä»¥å®ç°ç¨³å®šæœ‰æ•ˆçš„ç›®æ ‡ç§»é™¤ã€‚é¦–å…ˆï¼ŒåŸºäºè§‚å¯Ÿåˆ°è‡ªæ³¨æ„åŠ›å›¾å½±å“ç”Ÿæˆå›¾åƒçš„ç»“æ„å’Œå½¢çŠ¶ç»†èŠ‚ï¼Œæˆ‘ä»¬æå‡ºäº†æ³¨æ„åŠ›æ¿€æ´»å’ŒæŠ‘åˆ¶ï¼ˆASSï¼‰ï¼Œè¯¥æ–¹æ³•æ ¹æ®ç»™å®šçš„æ©è†œé‡æ–°è®¾è®¡é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹å†…çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œä»è€Œåœ¨åå‘ç”Ÿæˆè¿‡ç¨‹ä¸­ä¼˜å…ˆå¤„ç†èƒŒæ™¯è€Œéå‰æ™¯ç›®æ ‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†è‡ªæ³¨æ„åŠ›é‡å®šå‘æŒ‡å¯¼ï¼ˆSARGï¼‰ï¼Œå®ƒåˆ©ç”¨ASSå¼•å¯¼çš„è‡ªæ³¨æ„åŠ›æ¥æŒ‡å¯¼ç”Ÿæˆè¿‡ç¨‹ï¼Œä»è€Œåœ¨æ©è†œå†…æœ‰æ•ˆåœ°ç§»é™¤å‰æ™¯å¯¹è±¡ï¼ŒåŒæ—¶ç”Ÿæˆæ—¢åˆç†åˆè¿è´¯çš„å†…å®¹ã€‚å®éªŒè¡¨æ˜ï¼ŒAttentive Eraseråœ¨ä¸åŒé¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ä¸­çš„ç›®æ ‡ç§»é™¤è¡¨ç°ç¨³å®šä¸”æœ‰æ•ˆï¼Œç”šè‡³è¶…è¶Šäº†åŸºäºè®­ç»ƒçš„æ–¹æ³•ã€‚æ­¤å¤–ï¼ŒAttentive Eraserå¯åœ¨å„ç§æ‰©æ•£æ¨¡å‹æ¶æ„å’Œæ£€æŸ¥ç‚¹ä¸­å®ç°ï¼Œå±•ç°å‡ºå“è¶Šçš„å¯æ‰©å±•æ€§ã€‚ä»£ç å¯è®¿é—®<a target="_blank" rel="noopener" href="https://github.com/Anonym0u3/AttentiveEraser%E3%80%82">https://github.com/Anonym0u3/AttentiveEraserã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.12974v6">PDF</a> Accepted by AAAI 2025(Oral)</p>
<p><strong>Summary</strong><br>     æ‰©æ•£æ¨¡å‹åœ¨æ–°å…´çš„ç”Ÿæˆæ¨¡å‹é¢†åŸŸå±•ç°å‡ºæ½œåŠ›ï¼Œå°¤å…¶åœ¨å›¾åƒç”Ÿæˆæ–¹é¢ã€‚ä½†åœ¨å¯¹è±¡ç§»é™¤ä»»åŠ¡ä¸­ä»å­˜åœ¨é—®é¢˜ï¼Œå¦‚ç”Ÿæˆéšæœºä¼ªå½±å’Œåœ¨ç§»é™¤å‰æ™¯å¯¹è±¡åæ— æ³•é‡æ–°ç»˜åˆ¶é€‚å½“å†…å®¹ã€‚ä¸ºæ­¤ï¼Œæå‡ºæ— éœ€è°ƒæ•´çš„â€œAttentive Eraserâ€æ–¹æ³•ï¼Œä½¿é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹èƒ½å¤Ÿè¿›è¡Œç¨³å®šæœ‰æ•ˆçš„å¯¹è±¡ç§»é™¤ã€‚é€šè¿‡é‡æ–°è®¾è®¡é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹å†…çš„è‡ªæˆ‘å…³æ³¨æœºåˆ¶ï¼Œä¼˜å…ˆå¤„ç†èƒŒæ™¯è€Œéå‰æ™¯å¯¹è±¡ï¼ŒåŒæ—¶å¼•å…¥è‡ªæˆ‘å…³æ³¨é‡å®šå‘æŒ‡å¯¼ï¼ˆSARGï¼‰ï¼Œæœ‰æ•ˆç§»é™¤å‰æ™¯å¯¹è±¡å¹¶ç”Ÿæˆåˆç†è¿è´¯çš„å†…å®¹ã€‚è¯¥æ–¹æ³•åœ¨å¤šç§é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ä¸­è¡¨ç°ç¨³å®šæœ‰æ•ˆï¼Œè¶…è¶ŠåŸºäºè®­ç»ƒçš„æ–¹æ³•ã€‚æ­¤å¤–ï¼ŒAttentive Eraserå¯åœ¨å„ç§æ‰©æ•£æ¨¡å‹æ¶æ„å’Œæ£€æŸ¥ç‚¹å®æ–½ï¼Œå…·æœ‰è‰¯å¥½çš„å¯æ‰©å±•æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆæ¨¡å‹é¢†åŸŸæœ‰æ½œåŠ›ï¼Œå°¤å…¶åœ¨å›¾åƒç”Ÿæˆæ–¹é¢ã€‚</li>
<li>åœ¨å¯¹è±¡ç§»é™¤ä»»åŠ¡ä¸­ï¼Œæ‰©æ•£æ¨¡å‹é¢ä¸´ç”Ÿæˆéšæœºä¼ªå½±å’Œæ— æ³•é‡æ–°ç»˜åˆ¶å†…å®¹çš„é—®é¢˜ã€‚</li>
<li>æå‡ºâ€œAttentive Eraserâ€æ–¹æ³•ï¼Œä½¿é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹èƒ½è¿›è¡Œç¨³å®šæœ‰æ•ˆçš„å¯¹è±¡ç§»é™¤ã€‚</li>
<li>é€šè¿‡é‡æ–°è®¾è®¡è‡ªæˆ‘å…³æ³¨æœºåˆ¶ï¼Œä¼˜å…ˆå¤„ç†èƒŒæ™¯ï¼Œå¼•å…¥Attention Activation and Suppressionï¼ˆASSï¼‰ã€‚</li>
<li>å¼•å…¥Self-Attention Redirection Guidanceï¼ˆSARGï¼‰ï¼Œæœ‰æ•ˆç§»é™¤å‰æ™¯å¯¹è±¡å¹¶ç”Ÿæˆåˆç†å†…å®¹ã€‚</li>
<li>Attentive Eraseråœ¨å¤šç§é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶ŠåŸºäºè®­ç»ƒçš„æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.12974">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fe075c5ab4559dfada8d88c24e67d299.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-22a85791f9ca71a67255ff3526b87793.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-60d90a9fc622e916ad2be2cbe76154b5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d38d1501446a78f9354c70defe4087cb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8a0f7e55399d95265f218e3fdd27c817.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Diff-Def-Diffusion-Generated-Deformation-Fields-for-Conditional-Atlases"><a href="#Diff-Def-Diffusion-Generated-Deformation-Fields-for-Conditional-Atlases" class="headerlink" title="Diff-Def: Diffusion-Generated Deformation Fields for Conditional Atlases"></a>Diff-Def: Diffusion-Generated Deformation Fields for Conditional Atlases</h2><p><strong>Authors:Sophie Starck, Vasiliki Sideri-Lampretsa, Bernhard Kainz, Martin J. Menten, Tamara T. Mueller, Daniel Rueckert</strong></p>
<p>Anatomical atlases are widely used for population studies and analysis. Conditional atlases target a specific sub-population defined via certain conditions, such as demographics or pathologies, and allow for the investigation of fine-grained anatomical differences like morphological changes associated with ageing or disease. Existing approaches use either registration-based methods that are often unable to handle large anatomical variations or generative adversarial models, which are challenging to train since they can suffer from training instabilities. Instead of generating atlases directly in as intensities, we propose using latent diffusion models to generate deformation fields, which transform a general population atlas into one representing a specific sub-population. Our approach ensures structural integrity, enhances interpretability and avoids hallucinations that may arise during direct image synthesis by generating this deformation field and regularising it using a neighbourhood of images. We compare our method to several state-of-the-art atlas generation methods using brain MR images from the UK Biobank. Our method generates highly realistic atlases with smooth transformations and high anatomical fidelity, outperforming existing baselines. We demonstrate the quality of these atlases through comprehensive evaluations, including quantitative metrics for anatomical accuracy, perceptual similarity, and qualitative analyses displaying the consistency and realism of the generated atlases. </p>
<blockquote>
<p>è§£å‰–å›¾è°±åœ¨äººç¾¤ç ”ç©¶å’Œåˆ†æä¸­å¾—åˆ°äº†å¹¿æ³›åº”ç”¨ã€‚æ¡ä»¶å›¾è°±é’ˆå¯¹é€šè¿‡æŸäº›æ¡ä»¶ï¼ˆå¦‚äººå£ç»Ÿè®¡å­¦æˆ–ç—…ç†å­¦ï¼‰å®šä¹‰çš„ç‰¹å®šäºšç¾¤ä½“ï¼Œå¹¶å…è®¸è°ƒæŸ¥ä¸è¡°è€æˆ–ç–¾ç—…ç›¸å…³çš„å½¢æ€å­¦å˜åŒ–çš„ç»†å¾®è§£å‰–å·®å¼‚ã€‚ç°æœ‰æ–¹æ³•ä½¿ç”¨åŸºäºæ³¨å†Œçš„æ–¹æ³•ï¼Œè¿™äº›æ–¹æ³•é€šå¸¸æ— æ³•å¤„ç†å¤§çš„è§£å‰–å˜å¼‚ï¼Œæˆ–è€…ä½¿ç”¨ç”Ÿæˆå¯¹æŠ—æ¨¡å‹ï¼Œç”±äºè®­ç»ƒä¸ç¨³å®šæ€§çš„æŒ‘æˆ˜ï¼Œè¿™äº›æ¨¡å‹çš„è®­ç»ƒæ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æˆ‘ä»¬å¹¶ä¸ç›´æ¥åœ¨å¼ºåº¦ä¸Šç”Ÿæˆå›¾è°±ï¼Œè€Œæ˜¯å»ºè®®ä½¿ç”¨æ½œåœ¨æ‰©æ•£æ¨¡å‹æ¥ç”Ÿæˆå˜å½¢åœºï¼Œè¯¥å˜å½¢åœºå°†ä¸€ä¸ªé€šç”¨äººç¾¤å›¾è°±è½¬å˜ä¸ºä»£è¡¨ç‰¹å®šäºšäººç¾¤çš„å›¾è°±ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç¡®ä¿äº†ç»“æ„å®Œæ•´æ€§ï¼Œæé«˜äº†å¯è§£é‡Šæ€§ï¼Œå¹¶ä¸”é€šè¿‡ç”Ÿæˆè¿™ç§å˜å½¢åœºå¹¶ä½¿ç”¨å›¾åƒé‚»åŸŸè¿›è¡Œæ­£åˆ™åŒ–ï¼Œé¿å…äº†åœ¨ç›´æ¥å›¾åƒåˆæˆè¿‡ç¨‹ä¸­å¯èƒ½å‡ºç°çš„å¹»è§‰ã€‚æˆ‘ä»¬ä½¿ç”¨è‹±å›½ç”Ÿç‰©é“¶è¡Œçš„å¤§è„‘MRIå›¾åƒï¼Œå°†æˆ‘ä»¬çš„æ–¹æ³•ä¸å‡ ç§æœ€å…ˆè¿›çš„å›¾è°±ç”Ÿæˆæ–¹æ³•è¿›è¡Œæ¯”è¾ƒã€‚æˆ‘ä»¬çš„æ–¹æ³•ç”Ÿæˆäº†é«˜åº¦é€¼çœŸçš„å›¾è°±ï¼Œå…·æœ‰å¹³æ»‘çš„å˜æ¢å’Œé«˜åº¦çš„è§£å‰–ä¿çœŸåº¦ï¼Œè¶…è¿‡äº†ç°æœ‰çš„åŸºçº¿ã€‚æˆ‘ä»¬é€šè¿‡åŒ…æ‹¬è§£å‰–å‡†ç¡®æ€§çš„å®šé‡æŒ‡æ ‡ã€æ„ŸçŸ¥ç›¸ä¼¼æ€§ä»¥åŠæ˜¾ç¤ºç”Ÿæˆå›¾è°±çš„ä¸€è‡´æ€§å’Œç°å®æ€§çš„å®šæ€§åˆ†æåœ¨å†…çš„å…¨é¢è¯„ä¼°ï¼Œè¯æ˜äº†è¿™äº›å›¾è°±çš„è´¨é‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.16776v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºæ½œåœ¨æ‰©æ•£æ¨¡å‹çš„æ–¹æ³•ï¼Œç”¨äºç”Ÿæˆè¡¨ç¤ºç‰¹å®šäºšäººç¾¤çš„æµ·å›¾å˜å½¢åœºã€‚è¯¥æ–¹æ³•é€šè¿‡å°†é€šç”¨äººç¾¤æµ·å›¾è½¬åŒ–ä¸ºç‰¹å®šäºšäººç¾¤æµ·å›¾ï¼Œç¡®ä¿äº†ç»“æ„å®Œæ•´æ€§ï¼Œæé«˜äº†å¯è§£é‡Šæ€§ï¼Œé¿å…äº†ç›´æ¥å›¾åƒåˆæˆä¸­å¯èƒ½å‡ºç°çš„å¹»è§‰ã€‚é€šè¿‡å¯¹æ¯”ä½¿ç”¨è‹±å›½ç”Ÿç‰©é“¶è¡Œå¤§è„‘MRIå›¾åƒçš„å…¶ä»–å…ˆè¿›æµ·å›¾ç”Ÿæˆæ–¹æ³•ï¼Œæœ¬ç ”ç©¶çš„æ–¹æ³•ç”Ÿæˆçš„æµ·å›¾å…·æœ‰é«˜åº¦çœŸå®æ€§å’Œå¹³æ»‘è½¬æ¢ä»¥åŠé«˜è§£å‰–å­¦ä¿çœŸåº¦ï¼Œä¼˜äºç°æœ‰åŸºçº¿ã€‚é€šè¿‡ç»¼åˆè¯„ä¼°è¯æ˜äº†è¿™äº›æµ·å›¾çš„è´¨é‡ï¼ŒåŒ…æ‹¬å®šé‡è¯„ä¼°è§£å‰–ç²¾åº¦ã€æ„ŸçŸ¥ç›¸ä¼¼æ€§ä»¥åŠå®šæ€§åˆ†ææ˜¾ç¤ºç”Ÿæˆæµ·å›¾çš„ä¸€è‡´æ€§å’Œé€¼çœŸæ€§ã€‚ </p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¡ä»¶æ€§å›¾è°±å¯ä»¥é’ˆå¯¹å…·æœ‰ç‰¹å®šæ¡ä»¶çš„äºšäººç¾¤è¿›è¡Œç²¾ç»†çš„è§£å‰–å­¦å·®å¼‚ç ”ç©¶ï¼Œå¦‚å¹´é¾„å¢é•¿æˆ–ç–¾ç—…ç›¸å…³çš„å½¢æ€å˜åŒ–ã€‚</li>
<li>ç°æœ‰æ–¹æ³•åŒ…æ‹¬åŸºäºæ³¨å†Œçš„æ–¹æ³•å’ŒåŸºäºç”Ÿæˆå¯¹æŠ—æ¨¡å‹çš„æ–¹æ³•ï¼Œåˆ†åˆ«å­˜åœ¨å¤„ç†å¤§è§£å‰–å˜å¼‚å’Œè®­ç»ƒä¸ç¨³å®šçš„é—®é¢˜ã€‚</li>
<li>æœ¬ç ”ç©¶é‡‡ç”¨æ½œåœ¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆå˜å½¢åœºï¼Œä»¥è½¬åŒ–é€šç”¨äººç¾¤å›¾è°±ä¸ºç‰¹å®šäºšäººç¾¤å›¾è°±ï¼Œå¢å¼ºäº†ç»“æ„å®Œæ•´æ€§ã€å¯è§£é‡Šæ€§å¹¶é¿å…äº†å›¾åƒåˆæˆä¸­çš„å¹»è§‰é—®é¢˜ã€‚</li>
<li>æ–¹æ³•åœ¨UK Biobankçš„è„‘éƒ¨MRIå›¾åƒä¸Šè¿›è¡Œäº†éªŒè¯ï¼Œç”Ÿæˆçš„æµ·å›¾å…·æœ‰é«˜åº¦çœŸå®æ€§å’Œå¹³æ»‘è½¬æ¢ä»¥åŠé«˜è§£å‰–å­¦ä¿çœŸåº¦ã€‚</li>
<li>è¯„ä¼°åŒ…æ‹¬å®šé‡è¯„ä¼°è§£å‰–ç²¾åº¦ã€æ„ŸçŸ¥ç›¸ä¼¼æ€§ä»¥åŠå®šæ€§åˆ†ææ˜¾ç¤ºç”Ÿæˆå›¾è°±çš„ä¸€è‡´æ€§å’Œé€¼çœŸæ€§ã€‚</li>
<li>ä¸å…¶ä»–å…ˆè¿›æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•è¡¨ç°ä¼˜è¶Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.16776">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b2cf16ad7fec8963dd9a0b937792ba37.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-408ed9206ccb695f45bd321c900cc6ad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2fefe61ba1cf4400212f7e357723252b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3c74306708cdd16a8cc8034842e1465a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-217b0f532f3520b2fbff502761696f58.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-28/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-28/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-28/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-008555b808d4437acca293ee2013d487.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-28  Deep Spectral Prior
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-28
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-28/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-7fe2c747dd5836bf6aa5a9e3c8c1f253.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-28  ErpGS Equirectangular Image Rendering enhanced with 3D Gaussian   Regularization
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-28
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">30166.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
