<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-28  Hard Negative Contrastive Learning for Fine-Grained Geometric   Understanding in Large Multimodal Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-9bcd953fb8217d52ce1dc2e4bc877dc8.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-28
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-24
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    82 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-28-æ›´æ–°"><a href="#2025-05-28-æ›´æ–°" class="headerlink" title="2025-05-28 æ›´æ–°"></a>2025-05-28 æ›´æ–°</h1><h2 id="Hard-Negative-Contrastive-Learning-for-Fine-Grained-Geometric-Understanding-in-Large-Multimodal-Models"><a href="#Hard-Negative-Contrastive-Learning-for-Fine-Grained-Geometric-Understanding-in-Large-Multimodal-Models" class="headerlink" title="Hard Negative Contrastive Learning for Fine-Grained Geometric   Understanding in Large Multimodal Models"></a>Hard Negative Contrastive Learning for Fine-Grained Geometric   Understanding in Large Multimodal Models</h2><p><strong>Authors:Kai Sun, Yushi Bai, Zhen Yang, Jiajie Zhang, Ji Qi, Lei Hou, Juanzi Li</strong></p>
<p>Benefiting from contrastively trained visual encoders on large-scale natural scene images, Large Multimodal Models (LMMs) have achieved remarkable performance across various visual perception tasks. However, the inherent limitations of contrastive learning upon summarized descriptions fundamentally restrict the capabilities of models in meticulous reasoning, particularly in crucial scenarios of geometric problem-solving. To enhance geometric understanding, we propose a novel hard negative contrastive learning framework for the vision encoder, which combines image-based contrastive learning using generation-based hard negatives created by perturbing diagram generation code, and text-based contrastive learning using rule-based negatives derived from modified geometric descriptions and retrieval-based negatives selected based on caption similarity. We train CLIP using our strong negative learning method, namely MMCLIP (Multimodal Math CLIP), and subsequently train an LMM for geometric problem-solving. Experiments show that our trained model, MMGeoLM, significantly outperforms other open-source models on three geometric reasoning benchmarks. Even with a size of 7B, it can rival powerful closed-source models like GPT-4o. We further study the impact of different negative sample construction methods and the number of negative samples on the geometric reasoning performance of LMM, yielding fruitful conclusions. The code and dataset are available at <a target="_blank" rel="noopener" href="https://github.com/THU-KEG/MMGeoLM">https://github.com/THU-KEG/MMGeoLM</a>. </p>
<blockquote>
<p>å—ç›Šäºå¤§è§„æ¨¡è‡ªç„¶åœºæ™¯å›¾åƒå¯¹æ¯”è®­ç»ƒçš„è§†è§‰ç¼–ç å™¨ï¼Œå¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨å„ç§è§†è§‰æ„ŸçŸ¥ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æˆç»©ã€‚ç„¶è€Œï¼Œå¯¹æ¯”å­¦ä¹ åœ¨æ‘˜è¦æè¿°ä¸Šçš„å›ºæœ‰å±€é™æ€§ï¼Œä»æ ¹æœ¬ä¸Šé™åˆ¶äº†æ¨¡å‹åœ¨ç²¾ç»†æ¨ç†æ–¹é¢çš„èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨è§£å†³å‡ ä½•é—®é¢˜çš„å…³é”®åœºæ™¯ä¸­ã€‚ä¸ºäº†å¢å¼ºå‡ ä½•ç†è§£ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„ç¡¬è´Ÿå¯¹æ¯”å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºè§†è§‰ç¼–ç å™¨ã€‚è¯¥æ¡†æ¶ç»“åˆäº†åŸºäºå›¾åƒå¯¹æ¯”å­¦ä¹ ï¼Œä½¿ç”¨ç”Ÿæˆå‹ç¡¬è´Ÿæ ·æœ¬ï¼Œé€šè¿‡æ‰°åŠ¨å›¾è¡¨ç”Ÿæˆä»£ç è¿›è¡Œåˆ›å»ºï¼Œä»¥åŠåŸºäºæ–‡æœ¬çš„å¯¹æ¯”å­¦ä¹ ï¼Œä½¿ç”¨è§„åˆ™å‹è´Ÿæ ·æœ¬ï¼Œæ¥æºäºä¿®æ”¹åçš„å‡ ä½•æè¿°å’ŒåŸºäºæ£€ç´¢çš„è´Ÿæ ·æœ¬ï¼Œæ ¹æ®æ ‡é¢˜ç›¸ä¼¼æ€§è¿›è¡Œé€‰æ‹©ã€‚æˆ‘ä»¬ä½¿ç”¨å¼ºå¤§çš„è´Ÿå­¦ä¹ æ–¹æ³•è®­ç»ƒCLIPï¼Œå³MMCLIPï¼ˆå¤šæ¨¡æ€æ•°å­¦CLIPï¼‰ï¼Œç„¶åè®­ç»ƒç”¨äºè§£å†³å‡ ä½•é—®é¢˜çš„LMMã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬è®­ç»ƒçš„MMGeoLMæ¨¡å‹åœ¨ä¸‰ä¸ªå‡ ä½•æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šå¤§å¤§ä¼˜äºå…¶ä»–å¼€æºæ¨¡å‹ã€‚å³ä½¿è§„æ¨¡è¾¾åˆ°7Bï¼Œå®ƒä¹Ÿèƒ½ä¸å¼ºå¤§çš„é—­æºæ¨¡å‹å¦‚GPT-4oç›¸æŠ—è¡¡ã€‚æˆ‘ä»¬è¿˜ç ”ç©¶äº†ä¸åŒè´Ÿæ ·æœ¬æ„å»ºæ–¹æ³•å’Œè´Ÿæ ·æœ¬æ•°é‡å¯¹LMMå‡ ä½•æ¨ç†æ€§èƒ½çš„å½±å“ï¼Œå¾—å‡ºäº†æœ‰ç›Šçš„ç»“è®ºã€‚ä»£ç å’Œæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/THU-KEG/MMGeoLM%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/THU-KEG/MMGeoLMè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20152v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§è§„æ¨¡è‡ªç„¶åœºæ™¯å›¾åƒå¯¹æ¯”è®­ç»ƒçš„è§†è§‰ç¼–ç å™¨ï¼Œå¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨å„ç§è§†è§‰æ„ŸçŸ¥ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå¯¹æ¯”å­¦ä¹ åœ¨æ‘˜è¦æè¿°ä¸Šçš„å›ºæœ‰å±€é™æ€§ï¼Œé™åˆ¶äº†æ¨¡å‹åœ¨ç²¾ç»†æ¨ç†ä¸Šçš„èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨è§£å†³å‡ ä½•é—®é¢˜çš„å…³é”®åœºæ™¯ä¸­ã€‚ä¸ºå¢å¼ºå‡ ä½•ç†è§£ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹ç¡¬è´Ÿå¯¹æ¯”å­¦ä¹ æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†åŸºäºå›¾åƒå’Œæ–‡æœ¬çš„å¯¹æ¯”å­¦ä¹ ã€‚é€šè¿‡æ‰°åŠ¨å›¾è¡¨ç”Ÿæˆä»£ç ç”ŸæˆåŸºäºç”Ÿæˆçš„ç¡¬è´Ÿæ ·æœ¬ï¼Œä»¥åŠé€šè¿‡ä¿®æ”¹å‡ ä½•æè¿°å’ŒåŸºäºæ£€ç´¢çš„è´Ÿæ ·æœ¬é€‰æ‹©ï¼Œå®ç°åŸºäºè§„åˆ™çš„è´Ÿæ ·æœ¬å’ŒåŸºäºæ–‡æœ¬çš„æè¿°å¯¹æ¯”å­¦ä¹ ã€‚æœ¬æ–‡ä½¿ç”¨MMCLIPï¼ˆå¤šæ¨¡æ€æ•°å­¦CLIPï¼‰çš„å¼ºå¤§è´Ÿå­¦ä¹ æ–¹æ³•è®­ç»ƒCLIPï¼Œå¹¶éšåå¯¹ç”¨äºè§£å†³å‡ ä½•é—®é¢˜çš„LMMè¿›è¡Œè®­ç»ƒã€‚å®éªŒè¡¨æ˜ï¼Œè®­ç»ƒåçš„MMGeoLMæ¨¡å‹åœ¨ä¸‰ä¸ªå‡ ä½•æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šå¤§å¤§ä¼˜äºå…¶ä»–å¼€æºæ¨¡å‹ï¼Œå³ä½¿è§„æ¨¡è¾¾åˆ°7Bï¼Œä¹Ÿèƒ½ä¸å¼ºå¤§çš„é—­æºæ¨¡å‹GPT-4oç›¸æŠ—è¡¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Large Multimodal Models (LMMs)åˆ©ç”¨å¯¹æ¯”è®­ç»ƒè§†è§‰ç¼–ç å™¨åœ¨å¤§è§„æ¨¡è‡ªç„¶åœºæ™¯å›¾åƒä¸Šï¼Œå·²è¾¾æˆåœ¨å„ç§è§†è§‰æ„ŸçŸ¥ä»»åŠ¡ä¸Šçš„æ˜¾è‘—æ€§èƒ½ã€‚</li>
<li>å¯¹æ¯”å­¦ä¹ çš„å†…åœ¨å±€é™æ€§åœ¨äºå…¶åŸºäºæ‘˜è¦æè¿°çš„æ€§è´¨ï¼Œè¿™åœ¨ç²¾ç»†æ¨ç†å’Œå‡ ä½•é—®é¢˜è§£å†³æ–¹é¢å°¤ä¸ºæ˜æ˜¾ã€‚</li>
<li>ä¸ºæ”¹å–„å‡ ä½•ç†è§£ï¼Œæå‡ºäº†æ–°å‹ç¡¬è´Ÿå¯¹æ¯”å­¦ä¹ æ¡†æ¶ï¼Œç»“åˆäº†å›¾åƒå’Œæ–‡æœ¬å¯¹æ¯”å­¦ä¹ ã€‚</li>
<li>é€šè¿‡æ‰°åŠ¨å›¾è¡¨ç”Ÿæˆä»£ç åˆ›å»ºåŸºäºç”Ÿæˆçš„ç¡¬è´Ÿæ ·æœ¬ï¼Œä»¥åŠé€šè¿‡ä¿®æ”¹å‡ ä½•æè¿°å’ŒåŸºäºæ£€ç´¢çš„è´Ÿæ ·æœ¬é€‰æ‹©æ¥å®ç°åŸºäºè§„åˆ™çš„è´Ÿæ ·æœ¬å’Œæ–‡æœ¬æè¿°å¯¹æ¯”å­¦ä¹ ã€‚</li>
<li>ä½¿ç”¨MMCLIPæ–¹æ³•è®­ç»ƒCLIPï¼Œå¹¶éšåå¯¹ç”¨äºè§£å†³å‡ ä½•é—®é¢˜çš„LMMè¿›è¡Œè®­ç»ƒã€‚</li>
<li>MMGeoLMæ¨¡å‹åœ¨å‡ ä½•æ¨ç†æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œæ˜¾è‘—ä¼˜äºå…¶ä»–å¼€æºæ¨¡å‹ï¼Œå¹¶æœ‰æ½œåŠ›ä¸å¼ºå¤§çš„é—­æºæ¨¡å‹å¦‚GPT-4oç›¸ç«äº‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20152">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a6a96535643d6421416cd3e7853acfa3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4262a348eed972ebf034db6313cdcb81.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ce66cc59467c5b930b07e5b39e0e2ee0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="FUDOKI-Discrete-Flow-based-Unified-Understanding-and-Generation-via-Kinetic-Optimal-Velocities"><a href="#FUDOKI-Discrete-Flow-based-Unified-Understanding-and-Generation-via-Kinetic-Optimal-Velocities" class="headerlink" title="FUDOKI: Discrete Flow-based Unified Understanding and Generation via   Kinetic-Optimal Velocities"></a>FUDOKI: Discrete Flow-based Unified Understanding and Generation via   Kinetic-Optimal Velocities</h2><p><strong>Authors:Jin Wang, Yao Lai, Aoxue Li, Shifeng Zhang, Jiacheng Sun, Ning Kang, Chengyue Wu, Zhenguo Li, Ping Luo</strong></p>
<p>The rapid progress of large language models (LLMs) has catalyzed the emergence of multimodal large language models (MLLMs) that unify visual understanding and image generation within a single framework. However, most existing MLLMs rely on autoregressive (AR) architectures, which impose inherent limitations on future development, such as the raster-scan order in image generation and restricted reasoning abilities in causal context modeling. In this work, we challenge the dominance of AR-based approaches by introducing FUDOKI, a unified multimodal model purely based on discrete flow matching, as an alternative to conventional AR paradigms. By leveraging metric-induced probability paths with kinetic optimal velocities, our framework goes beyond the previous masking-based corruption process, enabling iterative refinement with self-correction capability and richer bidirectional context integration during generation. To mitigate the high cost of training from scratch, we initialize FUDOKI from pre-trained AR-based MLLMs and adaptively transition to the discrete flow matching paradigm. Experimental results show that FUDOKI achieves performance comparable to state-of-the-art AR-based MLLMs across both visual understanding and image generation tasks, highlighting its potential as a foundation for next-generation unified multimodal models. Furthermore, we show that applying test-time scaling techniques to FUDOKI yields significant performance gains, further underscoring its promise for future enhancement through reinforcement learning. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•å‚¬ç”Ÿäº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„å‡ºç°ï¼Œåè€…èƒ½å¤Ÿåœ¨å•ä¸€æ¡†æ¶å†…ç»Ÿä¸€è§†è§‰ç†è§£å’Œå›¾åƒç”Ÿæˆã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰çš„MLLMä¾èµ–äºè‡ªå›å½’ï¼ˆARï¼‰æ¶æ„ï¼Œè¿™å¯¹å…¶æœªæ¥å‘å±•é€ æˆäº†å›ºæœ‰å±€é™ï¼Œä¾‹å¦‚å›¾åƒç”Ÿæˆä¸­çš„æ‰«æé¡ºåºå’Œå› æœä¸Šä¸‹æ–‡å»ºæ¨¡ä¸­çš„æœ‰é™æ¨ç†èƒ½åŠ›ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æŒ‘æˆ˜åŸºäºARçš„æ–¹æ³•çš„ä¸»å¯¼åœ°ä½ï¼Œå¼•å…¥äº†åŸºäºç¦»æ•£æµåŒ¹é…çš„ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹FUDOKIï¼Œä½œä¸ºä¼ ç»ŸARèŒƒå¼çš„æ›¿ä»£æ–¹æ¡ˆã€‚é€šè¿‡åˆ©ç”¨åŸºäºåº¦é‡çš„æ¦‚ç‡è·¯å¾„å’ŒåŠ¨åŠ›å­¦æœ€ä¼˜é€Ÿåº¦ï¼Œæˆ‘ä»¬çš„æ¡†æ¶è¶…è¶Šäº†åŸºäºæ©ç çš„æŸåè¿‡ç¨‹ï¼Œå®ç°äº†è¿­ä»£ä¼˜åŒ–å’Œè‡ªæˆ‘æ ¡æ­£èƒ½åŠ›ï¼Œå¹¶åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­å®ç°äº†æ›´ä¸°å¯Œçš„åŒå‘ä¸Šä¸‹æ–‡é›†æˆã€‚ä¸ºäº†é™ä½ä»å¤´å¼€å§‹è®­ç»ƒçš„æˆæœ¬ï¼Œæˆ‘ä»¬ä»é¢„è®­ç»ƒçš„AR-based MLLMåˆå§‹åŒ–äº†FUDOKIï¼Œå¹¶è‡ªé€‚åº”åœ°è¿‡æ¸¡åˆ°ç¦»æ•£æµåŒ¹é…èŒƒå¼ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFUDOKIåœ¨è§†è§‰ç†è§£å’Œå›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šçš„æ€§èƒ½ä¸æœ€å…ˆè¿›çš„AR-based MLLMç›¸å½“ï¼Œè¿™å‡¸æ˜¾äº†å…¶ä½œä¸ºä¸‹ä¸€ä»£ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹åŸºç¡€çš„æ½œåŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¯¹FUDOKIåº”ç”¨äº†æµ‹è¯•æ—¶ç¼©æ”¾æŠ€æœ¯ï¼Œå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¿™è¿›ä¸€æ­¥è¯æ˜äº†å…¶é€šè¿‡å¼ºåŒ–å­¦ä¹ è¿›è¡Œæœªæ¥å¢å¼ºçš„å‰æ™¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20147v1">PDF</a> 37 pages, 12 figures</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹çš„å¿«é€Ÿå‘å±•å‚¬ç”Ÿäº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¶Œç°ï¼Œä½†ç°æœ‰æ¨¡å‹å¤§å¤šåŸºäºè‡ªå›å½’æ¶æ„ï¼Œå­˜åœ¨å›¾åƒç”Ÿæˆå’Œå› æœä¸Šä¸‹æ–‡å»ºæ¨¡ä¸Šçš„å±€é™ã€‚æœ¬ç ”ç©¶æŒ‘æˆ˜è‡ªå›å½’æ¶æ„çš„å„æ–­åœ°ä½ï¼Œæå‡ºåŸºäºç¦»æ•£æµåŒ¹é…çš„ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹FUDOKIã€‚å®ƒåˆ©ç”¨åº¦é‡è¯±å¯¼æ¦‚ç‡è·¯å¾„å’ŒåŠ¨åŠ›å­¦æœ€ä¼˜é€Ÿåº¦ï¼Œå®ç°è‡ªæˆ‘ä¿®æ­£å’Œä¸°å¯Œçš„åŒå‘ä¸Šä¸‹æ–‡é›†æˆã€‚å®éªŒè¡¨æ˜ï¼ŒFUDOKIæ€§èƒ½ä¸æœ€å…ˆè¿›çš„è‡ªå›å½’å¤šæ¨¡æ€æ¨¡å‹ç›¸å½“ï¼Œå¹¶å¯é€šè¿‡æµ‹è¯•æ—¶ç¼©æ”¾æŠ€æœ¯è¿›ä¸€æ­¥æé«˜æ€§èƒ½ï¼Œæœ‰æœ›æˆä¸ºä¸‹ä¸€ä»£ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹çš„åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ç»Ÿä¸€è§†è§‰ç†è§£å’Œå›¾åƒç”Ÿæˆã€‚</li>
<li>ç°æœ‰æ¨¡å‹å¤§å¤šä¾èµ–è‡ªå›å½’æ¶æ„ï¼Œå­˜åœ¨å±€é™ã€‚</li>
<li>FUDOKIåŸºäºç¦»æ•£æµåŒ¹é…ï¼ŒæŒ‘æˆ˜è‡ªå›å½’æ¶æ„ã€‚</li>
<li>FUDOKIåˆ©ç”¨åº¦é‡è¯±å¯¼æ¦‚ç‡è·¯å¾„å’ŒåŠ¨åŠ›å­¦æœ€ä¼˜é€Ÿåº¦ã€‚</li>
<li>ç›¸è¾ƒäºä¼ ç»Ÿçš„è‡ªå›å½’MLLMsï¼ŒFUDOKIå…·æœ‰è‡ªæˆ‘ä¿®æ­£å’Œä¸°å¯Œçš„ä¸Šä¸‹æ–‡é›†æˆèƒ½åŠ›ã€‚</li>
<li>FUDOKIæ€§èƒ½ä¸é¡¶å°–çš„è‡ªå›å½’å¤šæ¨¡æ€æ¨¡å‹ç›¸å½“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20147">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0d773cf8df9dfb0791896237a004bb17.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e81f3ed2b6bfad7996309164d2154467.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-29ebde777a49a2a237c7bd1ca1207807.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="MA-RAG-Multi-Agent-Retrieval-Augmented-Generation-via-Collaborative-Chain-of-Thought-Reasoning"><a href="#MA-RAG-Multi-Agent-Retrieval-Augmented-Generation-via-Collaborative-Chain-of-Thought-Reasoning" class="headerlink" title="MA-RAG: Multi-Agent Retrieval-Augmented Generation via Collaborative   Chain-of-Thought Reasoning"></a>MA-RAG: Multi-Agent Retrieval-Augmented Generation via Collaborative   Chain-of-Thought Reasoning</h2><p><strong>Authors:Thang Nguyen, Peter Chin, Yu-Wing Tai</strong></p>
<p>We present MA-RAG, a Multi-Agent framework for Retrieval-Augmented Generation (RAG) that addresses the inherent ambiguities and reasoning challenges in complex information-seeking tasks. Unlike conventional RAG methods that rely on either end-to-end fine-tuning or isolated component enhancements, MA-RAG orchestrates a collaborative set of specialized AI agents: Planner, Step Definer, Extractor, and QA Agents, to tackle each stage of the RAG pipeline with task-aware reasoning. Ambiguities may arise from underspecified queries, sparse or indirect evidence in retrieved documents, or the need to integrate information scattered across multiple sources. MA-RAG mitigates these challenges by decomposing the problem into subtasks, such as query disambiguation, evidence extraction, and answer synthesis, and dispatching them to dedicated agents equipped with chain-of-thought prompting. These agents communicate intermediate reasoning and progressively refine the retrieval and synthesis process. Our design allows fine-grained control over information flow without any model fine-tuning. Crucially, agents are invoked on demand, enabling a dynamic and efficient workflow that avoids unnecessary computation. This modular and reasoning-driven architecture enables MA-RAG to deliver robust, interpretable results. Experiments on multi-hop and ambiguous QA benchmarks demonstrate that MA-RAG outperforms state-of-the-art training-free baselines and rivals fine-tuned systems, validating the effectiveness of collaborative agent-based reasoning in RAG. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†MA-RAGï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºå¢å¼ºæ£€ç´¢ç”Ÿæˆï¼ˆRAGï¼‰çš„å¤šä»£ç†æ¡†æ¶ï¼Œå®ƒè§£å†³äº†å¤æ‚ä¿¡æ¯æ£€ç´¢ä»»åŠ¡ä¸­å›ºæœ‰çš„æ¨¡ç³Šæ€§å’Œæ¨ç†æŒ‘æˆ˜ã€‚ä¸ä¼ ç»Ÿçš„RAGæ–¹æ³•ä¸åŒï¼Œè¿™äº›æ–¹æ³•ä¾èµ–äºç«¯åˆ°ç«¯çš„å¾®è°ƒæˆ–å­¤ç«‹çš„ç»„ä»¶å¢å¼ºï¼ŒMA-RAGååŒå·¥ä½œä¸€ç»„ä¸“ä¸šçš„AIä»£ç†ï¼šè®¡åˆ’ä»£ç†ã€æ­¥éª¤å®šä¹‰ä»£ç†ã€æå–ä»£ç†å’Œé—®ç­”ä»£ç†ï¼Œä»¥ä»»åŠ¡æ„ŸçŸ¥æ¨ç†è§£å†³RAGç®¡é“çš„æ¯ä¸ªé˜¶æ®µã€‚æ¨¡ç³Šæ€§å¯èƒ½æ¥è‡ªæœªæŒ‡å®šçš„æŸ¥è¯¢ã€æ£€ç´¢åˆ°çš„æ–‡æ¡£ä¸­çš„ç¨€ç–æˆ–é—´æ¥è¯æ®ï¼Œæˆ–éœ€è¦æ•´åˆæ¥è‡ªå¤šä¸ªæ¥æºçš„åˆ†æ•£ä¿¡æ¯ã€‚MA-RAGé€šè¿‡åˆ†è§£é—®é¢˜ä¸ºå­ä»»åŠ¡æ¥ç¼“è§£è¿™äº›æŒ‘æˆ˜ï¼Œå¦‚æŸ¥è¯¢å»æ¨¡ç³ŠåŒ–ã€è¯æ®æå–å’Œç­”æ¡ˆåˆæˆï¼Œå¹¶å°†å®ƒä»¬åˆ†é…ç»™ä¸“é—¨çš„ä»£ç†ï¼Œè¿™äº›ä»£ç†é…å¤‡äº†é“¾å¼æ€ç»´æç¤ºã€‚è¿™äº›ä»£ç†æ²Ÿé€šä¸­é—´æ¨ç†å¹¶é€æ­¥æ”¹è¿›æ£€ç´¢å’Œåˆæˆè¿‡ç¨‹ã€‚æˆ‘ä»¬çš„è®¾è®¡å…è®¸å¯¹ä¿¡æ¯æµè¿›è¡Œç²¾ç»†æ§åˆ¶ï¼Œæ— éœ€ä»»ä½•æ¨¡å‹å¾®è°ƒã€‚å…³é”®çš„æ˜¯ï¼Œä»£ç†æ˜¯æŒ‰éœ€è°ƒç”¨çš„ï¼Œè¿™æ ·å¯ä»¥å®ç°åŠ¨æ€é«˜æ•ˆçš„å·¥ä½œæµç¨‹ï¼Œé¿å…ä¸å¿…è¦çš„è®¡ç®—ã€‚è¿™ç§æ¨¡å—åŒ–å’Œæ¨ç†é©±åŠ¨çš„æ¶æ„ä½¿MA-RAGèƒ½å¤Ÿæä¾›ç¨³å¥ã€å¯è§£é‡Šçš„ç»“æœã€‚åœ¨å¤šè·³å’Œæ¨¡ç³Šé—®ç­”åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMA-RAGè¶…è¶Šäº†æœ€å…ˆè¿›çš„æ— è®­ç»ƒåŸºå‡†å¹¶æ¥è¿‘å¾®è°ƒè¿‡çš„ç³»ç»Ÿï¼ŒéªŒè¯äº†åŸºäºåä½œä»£ç†æ¨ç†åœ¨RAGä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20096v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šæå‡ºä¸€ç§åŸºäºå¤šæ™ºèƒ½ä½“æ¡†æ¶MA-RAGçš„ä¿¡æ¯æ£€ç´¢å¢å¼ºç”ŸæˆæŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯è§£å†³å¤æ‚ä¿¡æ¯æ£€ç´¢ä»»åŠ¡ä¸­çš„å†…åœ¨æ¨¡ç³Šæ€§å’Œæ¨ç†æŒ‘æˆ˜ã€‚å®ƒé‡‡ç”¨æ¨¡å—åŒ–è®¾è®¡ï¼Œé€šè¿‡å¤šä¸ªä¸“é—¨AIæ™ºèƒ½ä½“ååŒå·¥ä½œï¼ŒåŒ…æ‹¬è§„åˆ’è€…ã€æ­¥éª¤å®šä¹‰å™¨ã€æå–å™¨å’Œé—®ç­”æ™ºèƒ½ä½“ç­‰ï¼Œé’ˆå¯¹RAGç®¡é“çš„æ¯ä¸ªé˜¶æ®µè¿›è¡Œä»»åŠ¡æ„ŸçŸ¥æ¨ç†ã€‚å®éªŒè¡¨æ˜ï¼ŒMA-RAGåœ¨è·¨å¤šè·³å’Œæ¨¡ç³Šé—®ç­”åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼ŒéªŒè¯äº†åŸºäºæ™ºèƒ½ä½“çš„åä½œæ¨ç†åœ¨RAGä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>MA-RAGæ˜¯ä¸€ä¸ªå¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œç”¨äºè§£å†³å¤æ‚ä¿¡æ¯æ£€ç´¢ä»»åŠ¡çš„æ¨¡ç³Šæ€§å’Œæ¨ç†æŒ‘æˆ˜ã€‚</li>
<li>ä¸ä¼ ç»ŸRAGæ–¹æ³•ä¸åŒï¼ŒMA-RAGé‡‡ç”¨æ¨¡å—åŒ–è®¾è®¡ï¼Œé€šè¿‡å¤šä¸ªAIæ™ºèƒ½ä½“ååŒå·¥ä½œã€‚</li>
<li>MA-RAGé€šè¿‡åˆ†è§£é—®é¢˜ä¸ºå­ä»»åŠ¡ï¼Œå¦‚æŸ¥è¯¢å»æ¨¡ç³ŠåŒ–ã€è¯æ®æå–å’Œç­”æ¡ˆåˆæˆç­‰ï¼Œå¹¶åˆ†é…ç»™ä¸“é—¨çš„æ™ºèƒ½ä½“è¿›è¡Œå¤„ç†ã€‚</li>
<li>æ™ºèƒ½ä½“é‡‡ç”¨é“¾å¼æ€ç»´æç¤ºè¿›è¡Œé€šä¿¡ï¼Œé€æ­¥ä¼˜åŒ–æ£€ç´¢å’Œåˆæˆè¿‡ç¨‹ã€‚</li>
<li>MA-RAGå…è®¸å¯¹ä¿¡æ¯æµè¿›è¡Œç²¾ç»†æ§åˆ¶ï¼Œæ— éœ€æ¨¡å‹å¾®è°ƒã€‚</li>
<li>æ™ºèƒ½ä½“æŒ‰éœ€è°ƒç”¨ï¼Œå®ç°åŠ¨æ€é«˜æ•ˆçš„å·¥ä½œæµç¨‹ï¼Œé¿å…ä¸å¿…è¦çš„è®¡ç®—ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20096">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-133f097e14a6911446e88b95a5c2ffda.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b3a870ec682e45cc69ff848013d886c1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0b57f68a91fc9cd78c8efb0ccc19bea7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-19709e05e98f05a442b726501b8a946e.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Incentivizing-Reasoning-from-Weak-Supervision"><a href="#Incentivizing-Reasoning-from-Weak-Supervision" class="headerlink" title="Incentivizing Reasoning from Weak Supervision"></a>Incentivizing Reasoning from Weak Supervision</h2><p><strong>Authors:Yige Yuan, Teng Xiao, Shuchang Tao, Xue Wang, Jinyang Gao, Bolin Ding, Bingbing Xu</strong></p>
<p>Large language models (LLMs) have demonstrated impressive performance on reasoning-intensive tasks, but enhancing their reasoning abilities typically relies on either reinforcement learning (RL) with verifiable signals or supervised fine-tuning (SFT) with high-quality long chain-of-thought (CoT) demonstrations, both of which are expensive. In this paper, we study a novel problem of incentivizing the reasoning capacity of LLMs without expensive high-quality demonstrations and reinforcement learning. We investigate whether the reasoning capabilities of LLMs can be effectively incentivized via supervision from significantly weaker models. We further analyze when and why such weak supervision succeeds in eliciting reasoning abilities in stronger models. Our findings show that supervision from significantly weaker reasoners can substantially improve student reasoning performance, recovering close to 94% of the gains of expensive RL at a fraction of the cost. Experiments across diverse benchmarks and model architectures demonstrate that weak reasoners can effectively incentivize reasoning in stronger student models, consistently improving performance across a wide range of reasoning tasks. Our results suggest that this simple weak-to-strong paradigm is a promising and generalizable alternative to costly methods for incentivizing strong reasoning capabilities at inference-time in LLMs. The code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/yuanyige/W2SR">https://github.com/yuanyige/W2SR</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ¨ç†å¯†é›†å‹ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ï¼Œä½†å¢å¼ºå…¶æ¨ç†èƒ½åŠ›é€šå¸¸ä¾èµ–äºå¯éªŒè¯ä¿¡å·çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æˆ–é«˜è´¨é‡é•¿æ€è€ƒé“¾ï¼ˆCoTï¼‰æ¼”ç¤ºçš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œè¿™ä¸¤è€…æˆæœ¬éƒ½å¾ˆé«˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†ä¸€ä¸ªæ–°é—®é¢˜ï¼Œå³åœ¨ä¸ä¾èµ–æ˜‚è´µçš„é«˜è´¨é‡æ¼”ç¤ºå’Œå¼ºåŒ–å­¦ä¹ çš„æƒ…å†µä¸‹ï¼Œæ¿€åŠ±LLMçš„æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬è°ƒæŸ¥äº†LLMçš„æ¨ç†èƒ½åŠ›æ˜¯å¦å¯ä»¥é€šè¿‡æ¥è‡ªè¾ƒå¼±æ¨¡å‹çš„ç›‘ç£æ¥æœ‰æ•ˆæ¿€åŠ±ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥åˆ†æäº†è¿™ç§å¼±ç›‘ç£ä½•æ—¶ä»¥åŠä¸ºä½•ä¼šåœ¨æ›´å¼ºå¤§çš„æ¨¡å‹ä¸­æ¿€å‘æ¨ç†èƒ½åŠ›å–å¾—æˆåŠŸã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæ¥è‡ªè¾ƒå¼±æ¨ç†æ¨¡å‹çš„ç›‘ç£å¯ä»¥å¤§å¹…åº¦æé«˜å­¦ç”Ÿçš„æ¨ç†æ€§èƒ½ï¼Œæ¥è¿‘æ¢å¤ä½¿ç”¨æ˜‚è´µå¼ºåŒ–å­¦ä¹ æ—¶çš„æ”¶ç›Šï¼ŒåŒæ—¶æˆæœ¬æ›´ä½ã€‚åœ¨ä¸åŒåŸºå‡†æµ‹è¯•å’Œæ¨¡å‹æ¶æ„ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œå¼±æ¨ç†æ¨¡å‹å¯ä»¥æœ‰æ•ˆåœ°æ¿€åŠ±æ›´å¼ºå­¦ç”Ÿæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œåœ¨å¹¿æ³›çš„æ¨ç†ä»»åŠ¡ä¸ŠæŒç»­æé«˜æ€§èƒ½ã€‚æˆ‘ä»¬çš„ç»“æœæš—ç¤ºï¼Œè¿™ç§ç®€å•åœ°ä»å¼±åˆ°å¼ºçš„æ¨¡å¼æ˜¯ä¸€ä¸ªæœ‰å‰æ™¯ä¸”å¯æ¨å¹¿çš„æ›¿ä»£æ–¹æ³•ï¼Œå¯ä»¥æ›¿ä»£æ¿€åŠ±LLMåœ¨æ¨ç†æ—¶é—´æ—¶çš„å¼ºå¤§æ¨ç†èƒ½åŠ›çš„æ˜‚è´µæ–¹æ³•ã€‚ç›¸å…³ä»£ç å·²å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/yuanyige/W2SR%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/yuanyige/W2SRä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20072v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨ç†å¯†é›†å‹ä»»åŠ¡ä¸Šè¡¨ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ï¼Œä½†æé«˜å…¶æ¨ç†èƒ½åŠ›é€šå¸¸ä¾èµ–äºå¯éªŒè¯ä¿¡å·çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æˆ–é«˜è´¨é‡çš„é•¿é“¾æ€ç»´ï¼ˆCoTï¼‰æ¼”ç¤ºçš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œè¿™ä¸¤è€…æˆæœ¬éƒ½è¾ƒé«˜ã€‚æœ¬æ–‡ç ”ç©¶äº†ä¸€ç§æ¿€åŠ±LLMsæ¨ç†èƒ½åŠ›çš„æ–°é—®é¢˜ï¼Œæ— éœ€æ˜‚è´µçš„é«˜è´¨é‡æ¼”ç¤ºå’Œå¼ºåŒ–å­¦ä¹ ã€‚é€šè¿‡æ¢ç©¶æ¥è‡ªè¾ƒå¼±æ¨¡å‹çš„ç›‘ç£æ˜¯å¦èƒ½æœ‰æ•ˆæ¿€åŠ±LLMsçš„æ¨ç†èƒ½åŠ›ï¼Œå¹¶è¿›ä¸€æ­¥åˆ†æä½•æ—¶åŠä¸ºä½•è¿™ç§å¼±ç›‘ç£èƒ½åœ¨å¼ºæ¨¡å‹ä¸­æ¿€å‘æ¨ç†èƒ½åŠ›ã€‚ç ”ç©¶å‘ç°ï¼Œæ¥è‡ªè¾ƒå¼±æ¨ç†æ¨¡å‹çš„ç›‘ç£å¯ä»¥å¤§å¹…åº¦æé«˜å­¦ç”Ÿæ¨¡å‹çš„æ¨ç†æ€§èƒ½ï¼Œæ¢å¤è¿‘94%çš„æ˜‚è´µRLçš„æ”¶ç›Šï¼Œä¸”æˆæœ¬è¾ƒä½ã€‚åœ¨ä¸åŒåŸºå‡†æµ‹è¯•å’Œæ¨¡å‹æ¶æ„ä¸‹çš„å®éªŒè¡¨æ˜ï¼Œå¼±æ¨ç†æ¨¡å‹èƒ½æœ‰æ•ˆæ¿€åŠ±å¼ºå­¦ç”Ÿæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œåœ¨å¹¿æ³›çš„æ¨ç†ä»»åŠ¡ä¸Šéƒ½èƒ½æé«˜æ€§èƒ½ã€‚è¿™ä¸€ç®€å•ä»å¼±åˆ°å¼ºçš„èŒƒå¼ä¸ºæ¿€åŠ±LLMsåœ¨æ¨ç†æ—¶çš„å¼ºå¤§èƒ½åŠ›æä¾›äº†æœ‰å‰æ™¯å’Œå¯æ¨å¹¿çš„æ›¿ä»£æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†æé«˜å…¶æ¨ç†èƒ½åŠ›é€šå¸¸æ¶‰åŠæ˜‚è´µçš„å¼ºåŒ–å­¦ä¹ æˆ–é«˜è´¨é‡ç¤ºèŒƒç›‘ç£å¾®è°ƒã€‚</li>
<li>ç ”ç©¶æ¢ç´¢äº†ä¸€ç§æ–°å‹æ–¹æ³•ï¼Œé€šè¿‡å¼±æ¨¡å‹çš„ç›‘ç£æ¥æ¿€åŠ±LLMsçš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>æ¥è‡ªè¾ƒå¼±æ¨ç†æ¨¡å‹çš„ç›‘ç£å¯ä»¥å¤§å¹…åº¦æé«˜å­¦ç”Ÿæ¨¡å‹çš„æ¨ç†æ€§èƒ½ã€‚</li>
<li>æ­¤æ–¹æ³•èƒ½æœ‰æ•ˆæé«˜æ€§èƒ½ï¼Œæ¥è¿‘æ˜‚è´µå¼ºåŒ–å­¦ä¹ çš„æ”¶ç›Šï¼Œä½†æˆæœ¬è¾ƒä½ã€‚</li>
<li>å®éªŒè¯æ˜ï¼Œè¿™ä¸€æ–¹æ³•åœ¨ä¸åŒåŸºå‡†æµ‹è¯•å’Œæ¨¡å‹æ¶æ„ä¸‹å‡æœ‰æ•ˆã€‚</li>
<li>å¼±æ¨ç†æ¨¡å‹èƒ½æœ‰æ•ˆæ¿€åŠ±å¼ºå­¦ç”Ÿæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œé€‚ç”¨äºå¹¿æ³›çš„ä»»åŠ¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20072">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7313beff6f9f2ce32c22e0b5a68b98c5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-07060384255c4a78037174dcc5479f95.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c9ae9b982613d5fb4f5ee2ad0fb9a9ec.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="SafeDPO-A-Simple-Approach-to-Direct-Preference-Optimization-with-Enhanced-Safety"><a href="#SafeDPO-A-Simple-Approach-to-Direct-Preference-Optimization-with-Enhanced-Safety" class="headerlink" title="SafeDPO: A Simple Approach to Direct Preference Optimization with   Enhanced Safety"></a>SafeDPO: A Simple Approach to Direct Preference Optimization with   Enhanced Safety</h2><p><strong>Authors:Geon-Hyeong Kim, Youngsoo Jang, Yu Jin Kim, Byoungjip Kim, Honglak Lee, Kyunghoon Bae, Moontae Lee</strong></p>
<p>As Large Language Models (LLMs) continue to advance and find applications across a growing number of fields, ensuring the safety of LLMs has become increasingly critical. To address safety concerns, recent studies have proposed integrating safety constraints into Reinforcement Learning from Human Feedback (RLHF). However, these approaches tend to be complex, as they encompass complicated procedures in RLHF along with additional steps required by the safety constraints. Inspired by Direct Preference Optimization (DPO), we introduce a new algorithm called SafeDPO, which is designed to directly optimize the safety alignment objective in a single stage of policy learning, without requiring relaxation. SafeDPO introduces only one additional hyperparameter to further enhance safety and requires only minor modifications to standard DPO. As a result, it eliminates the need to fit separate reward and cost models or to sample from the language model during fine-tuning, while still enhancing the safety of LLMs. Finally, we demonstrate that SafeDPO achieves competitive performance compared to state-of-the-art safety alignment algorithms, both in terms of aligning with human preferences and improving safety. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æŒç»­è¿›æ­¥å¹¶åœ¨è¶Šæ¥è¶Šå¤šçš„é¢†åŸŸæ‰¾åˆ°åº”ç”¨ï¼Œç¡®ä¿LLMsçš„å®‰å…¨å˜å¾—è‡³å…³é‡è¦ã€‚ä¸ºäº†è§£å†³å®‰å…¨æ‹…å¿§ï¼Œæœ€è¿‘çš„ç ”ç©¶æå‡ºäº†å°†å®‰å…¨çº¦æŸæ•´åˆåˆ°äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰ä¸­ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å¾€å¾€å¾ˆå¤æ‚ï¼Œå› ä¸ºå®ƒä»¬æ¶µç›–äº†RLHFä¸­çš„å¤æ‚ç¨‹åºä»¥åŠå®‰å…¨çº¦æŸæ‰€éœ€çš„å…¶ä»–æ­¥éª¤ã€‚å—ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰çš„å¯å‘ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„ç®—æ³•ï¼Œç§°ä¸ºSafeDPOï¼Œå®ƒæ˜¯ä¸ºäº†ç›´æ¥ä¼˜åŒ–å®‰å…¨å¯¹é½ç›®æ ‡è€Œè®¾è®¡çš„ï¼Œåœ¨ä¸€ä¸ªæ”¿ç­–å­¦ä¹ é˜¶æ®µå³å¯å®Œæˆï¼Œæ— éœ€æ”¾æ¾ã€‚SafeDPOåªå¼•å…¥äº†ä¸€ä¸ªé¢å¤–çš„è¶…å‚æ•°æ¥è¿›ä¸€æ­¥å¢å¼ºå®‰å…¨æ€§ï¼Œå¹¶ä¸”åªéœ€è¦å¯¹æ ‡å‡†DPOè¿›è¡Œå¾®å°çš„ä¿®æ”¹ã€‚å› æ­¤ï¼Œå®ƒæ¶ˆé™¤äº†éœ€è¦æ‹Ÿåˆå•ç‹¬çš„å¥–åŠ±å’Œæˆæœ¬æ¨¡å‹æˆ–åœ¨å¾®è°ƒæœŸé—´ä»è¯­è¨€æ¨¡å‹é‡‡æ ·çš„éœ€è¦ï¼ŒåŒæ—¶ä»èƒ½æé«˜LLMsçš„å®‰å…¨æ€§ã€‚æœ€åï¼Œæˆ‘ä»¬è¯æ˜SafeDPOä¸æœ€æ–°çš„å®‰å…¨å¯¹é½ç®—æ³•ç›¸æ¯”ï¼Œåœ¨å®ç°ä¸äººç±»åå¥½å¯¹é½å’Œæé«˜å®‰å…¨æ€§æ–¹é¢è¡¨ç°å‡ºç«äº‰åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20065v1">PDF</a> 34 pages</p>
<p><strong>Summary</strong></p>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„ä¸æ–­å‘å±•åŠå…¶åœ¨å„ä¸ªé¢†åŸŸçš„åº”ç”¨æ‰©å±•ï¼Œç¡®ä¿LLMsçš„å®‰å…¨æ€§å˜å¾—è‡³å…³é‡è¦ã€‚ä¸ºè§£å†³å®‰å…¨é—®é¢˜ï¼Œæœ€è¿‘çš„ç ”ç©¶æå‡ºäº†å°†å®‰å…¨çº¦æŸæ•´åˆåˆ°äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰ä¸­ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å¾€å¾€å¾ˆå¤æ‚ï¼Œæ¶‰åŠRLHFçš„å¤æ‚ç¨‹åºä»¥åŠå®‰å…¨çº¦æŸæ‰€éœ€çš„å…¶ä»–æ­¥éª¤ã€‚å—ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„ç®—æ³•SafeDPOï¼Œæ—¨åœ¨åœ¨ä¸€ä¸ªæ”¿ç­–å­¦ä¹ é˜¶æ®µç›´æ¥ä¼˜åŒ–å®‰å…¨å¯¹é½ç›®æ ‡ï¼Œæ— éœ€æ”¾æ¾ã€‚SafeDPOåªå¼•å…¥äº†ä¸€ä¸ªé¢å¤–çš„è¶…å‚æ•°æ¥è¿›ä¸€æ­¥æé«˜å®‰å…¨æ€§ï¼Œå¹¶ä¸”åªéœ€è¦å¯¹æ ‡å‡†DPOè¿›è¡Œå¾®å°ä¿®æ”¹ã€‚æœ€åï¼Œæˆ‘ä»¬è¯æ˜SafeDPOä¸æœ€æ–°çš„å®‰å…¨å¯¹é½ç®—æ³•ç›¸æ¯”ï¼Œåœ¨ç¬¦åˆäººç±»åå¥½å’Œæé«˜å®‰å…¨æ€§æ–¹é¢éƒ½å–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å®‰å…¨æ€§é—®é¢˜éšç€å…¶åº”ç”¨çš„æ‰©å±•è€Œå˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚</li>
<li>ç°æœ‰æ–¹æ³•æ•´åˆå®‰å…¨çº¦æŸåˆ°RLHFä¸­å¾€å¾€è¿‡ç¨‹å¤æ‚ï¼Œæ¶‰åŠå¤šä¸ªæ­¥éª¤ã€‚</li>
<li>SafeDPOç®—æ³•å—Direct Preference Optimizationï¼ˆDPOï¼‰å¯å‘ï¼Œæ—¨åœ¨åœ¨ä¸€ä¸ªæ”¿ç­–å­¦ä¹ é˜¶æ®µç›´æ¥ä¼˜åŒ–å®‰å…¨å¯¹é½ç›®æ ‡ã€‚</li>
<li>SafeDPOä»…å¼•å…¥ä¸€ä¸ªé¢å¤–çš„è¶…å‚æ•°æ¥æé«˜å®‰å…¨æ€§ï¼Œä¸”åªéœ€å¯¹æ ‡å‡†DPOè¿›è¡Œå¾®å°è°ƒæ•´ã€‚</li>
<li>SafeDPOæ¶ˆé™¤äº†éœ€è¦æ‹Ÿåˆå•ç‹¬çš„å¥–åŠ±å’Œæˆæœ¬æ¨¡å‹æˆ–åœ¨å¾®è°ƒæ—¶ä»è¯­è¨€æ¨¡å‹ä¸­é‡‡æ ·çš„éœ€æ±‚ã€‚</li>
<li>SafeDPOåœ¨ç¬¦åˆäººç±»åå¥½å’Œæé«˜å®‰å…¨æ€§æ–¹é¢å–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20065">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d56c4f2ef5aff059af8ff2e1082c2cb6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a434a2c1f302b6c12a927a6c621233b9.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Towards-Video-to-Piano-Music-Generation-with-Chain-of-Perform-Support-Benchmarks"><a href="#Towards-Video-to-Piano-Music-Generation-with-Chain-of-Perform-Support-Benchmarks" class="headerlink" title="Towards Video to Piano Music Generation with Chain-of-Perform Support   Benchmarks"></a>Towards Video to Piano Music Generation with Chain-of-Perform Support   Benchmarks</h2><p><strong>Authors:Chang Liu, Haomin Zhang, Shiyu Xia, Zihao Chen, Chaofan Ding, Xin Yue, Huizhe Chen, Xinhan Di</strong></p>
<p>Generating high-quality piano audio from video requires precise synchronization between visual cues and musical output, ensuring accurate semantic and temporal alignment.However, existing evaluation datasets do not fully capture the intricate synchronization required for piano music generation. A comprehensive benchmark is essential for two primary reasons: (1) existing metrics fail to reflect the complexity of video-to-piano music interactions, and (2) a dedicated benchmark dataset can provide valuable insights to accelerate progress in high-quality piano music generation. To address these challenges, we introduce the CoP Benchmark Dataset-a fully open-sourced, multimodal benchmark designed specifically for video-guided piano music generation. The proposed Chain-of-Perform (CoP) benchmark offers several compelling features: (1) detailed multimodal annotations, enabling precise semantic and temporal alignment between video content and piano audio via step-by-step Chain-of-Perform guidance; (2) a versatile evaluation framework for rigorous assessment of both general-purpose and specialized video-to-piano generation tasks; and (3) full open-sourcing of the dataset, annotations, and evaluation protocols. The dataset is publicly available at <a target="_blank" rel="noopener" href="https://github.com/acappemin/Video-to-Audio-and-Piano">https://github.com/acappemin/Video-to-Audio-and-Piano</a>, with a continuously updated leaderboard to promote ongoing research in this domain. </p>
<blockquote>
<p>ä»è§†é¢‘ä¸­ç”Ÿæˆé«˜è´¨é‡é’¢ç´éŸ³é¢‘éœ€è¦è§†è§‰çº¿ç´¢å’ŒéŸ³ä¹è¾“å‡ºä¹‹é—´çš„ç²¾ç¡®åŒæ­¥ï¼Œä»¥ç¡®ä¿å‡†ç¡®è¯­ä¹‰å’Œæ—¶é—´å¯¹é½ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è¯„ä¼°æ•°æ®é›†å¹¶æ²¡æœ‰å®Œå…¨æ•æ‰åˆ°é’¢ç´éŸ³ä¹ç”Ÿæˆæ‰€éœ€çš„ç²¾ç»†åŒæ­¥ã€‚ç»¼åˆåŸºå‡†æµ‹è¯•è‡³å…³é‡è¦ï¼Œä¸»è¦åŸå› æœ‰ä¸¤ç‚¹ï¼šï¼ˆ1ï¼‰ç°æœ‰æŒ‡æ ‡æœªèƒ½åæ˜ å‡ºè§†é¢‘åˆ°é’¢ç´éŸ³ä¹äº’åŠ¨çš„å¤æ‚æ€§ï¼›ï¼ˆ2ï¼‰ä¸“ç”¨åŸºå‡†æ•°æ®é›†å¯ä»¥ä¸ºåŠ é€Ÿé«˜è´¨é‡é’¢ç´éŸ³ä¹ç”Ÿæˆçš„ç ”ç©¶æä¾›å®è´µè§è§£ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†CoPåŸºå‡†æ•°æ®é›†â€”â€”ä¸€ä¸ªå®Œå…¨å¼€æºçš„å¤šæ¨¡å¼åŸºå‡†æµ‹è¯•ï¼Œä¸“é—¨ç”¨äºè§†é¢‘æŒ‡å¯¼çš„é’¢ç´éŸ³ä¹ç”Ÿæˆã€‚æ‰€æå‡ºçš„â€œæ¼”å¥é“¾â€ï¼ˆCoPï¼‰åŸºå‡†æµ‹è¯•å…·æœ‰å‡ ä¸ªå¼•äººæ³¨ç›®çš„ç‰¹ç‚¹ï¼šï¼ˆ1ï¼‰è¯¦ç»†çš„å¤šæ¨¡å¼æ³¨é‡Šï¼Œé€šè¿‡åˆ†æ­¥â€œæ¼”å¥é“¾â€æŒ‡å¯¼ï¼Œå®ç°è§†é¢‘å†…å®¹å’Œé’¢ç´éŸ³é¢‘ä¹‹é—´çš„ç²¾ç¡®è¯­ä¹‰å’Œæ—¶é—´å¯¹é½ï¼›ï¼ˆ2ï¼‰ç”¨äºä¸¥æ ¼è¯„ä¼°é€šç”¨å’Œä¸“ç”¨è§†é¢‘åˆ°é’¢ç´ç”Ÿæˆä»»åŠ¡çš„é€šç”¨è¯„ä¼°æ¡†æ¶ï¼›ï¼ˆ3ï¼‰æ•°æ®é›†ã€æ³¨é‡Šå’Œè¯„ä¼°åè®®çš„å®Œå…¨å¼€æºã€‚æ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/acappemin/Video-to-Audio-and-Piano">https://github.com/acappemin/Video-to-Audio-and-Piano</a>å…¬å¼€è®¿é—®ï¼Œå¹¶è®¾æœ‰æŒç»­æ›´æ–°çš„æ’è¡Œæ¦œï¼Œä»¥ä¿ƒè¿›è¯¥é¢†åŸŸçš„ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20038v1">PDF</a> 4 pages, 1 figure, accepted by CVPR 2025 MMFM Workshop</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªä¸“ä¸ºè§†é¢‘æŒ‡å¯¼ä¸‹çš„é’¢ç´éŸ³ä¹ç”Ÿæˆè®¾è®¡çš„å¼€æ”¾ã€å¤šæ¨¡å¼åŸºå‡†æ•°æ®é›†â€”â€”CoP Benchmark Datasetã€‚è¯¥æ•°æ®é›†å…·å¤‡è¯¦ç»†çš„å¤šæ¨¡å¼æ³¨é‡Šã€æ”¯æŒå¤šç§è¯„ä¼°çš„è§†é¢‘åˆ°é’¢ç´ç”Ÿæˆä»»åŠ¡æ¡†æ¶ï¼Œä»¥åŠå…¬å¼€çš„æ•°æ®é›†ã€æ³¨é‡Šå’Œè¯„ä¼°åè®®ã€‚æ•°æ®é›†å…¬å¼€åœ¨[é“¾æ¥åœ°å€]ï¼Œå¹¶è®¾æœ‰æŒç»­æ›´æ–°çš„æ’è¡Œæ¦œï¼Œä»¥ä¿ƒè¿›è¯¥é¢†åŸŸçš„ç ”ç©¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆé«˜è´¨é‡é’¢ç´éŸ³é¢‘éœ€è¦è§†è§‰çº¿ç´¢å’ŒéŸ³ä¹è¾“å‡ºçš„ç²¾ç¡®åŒæ­¥ï¼Œç¡®ä¿è¯­ä¹‰å’Œæ—¶é—´å¯¹é½çš„ç²¾ç¡®åº¦ã€‚</li>
<li>ç°æœ‰è¯„ä¼°æ•°æ®é›†æœªèƒ½å®Œå…¨æ•æ‰é’¢ç´éŸ³ä¹ç”Ÿæˆæ‰€éœ€çš„ç²¾ç»†åŒæ­¥ã€‚</li>
<li>å…¨é¢çš„åŸºå‡†æµ‹è¯•å¯¹äºè§£å†³ç°æœ‰æŒ‡æ ‡æ— æ³•åæ˜ è§†é¢‘åˆ°é’¢ç´éŸ³ä¹äº’åŠ¨çš„å¤æ‚æ€§è‡³å…³é‡è¦ã€‚</li>
<li>å¼•å…¥CoP Benchmarkæ•°æ®é›†ï¼Œä¸“ä¸ºè§†é¢‘æŒ‡å¯¼çš„é’¢ç´éŸ³ä¹ç”Ÿæˆè®¾è®¡ã€‚</li>
<li>CoP Benchmarkæ•°æ®é›†å…·å¤‡è¯¦ç»†çš„å¤šæ¨¡å¼æ³¨é‡Šï¼Œé€šè¿‡é€æ­¥çš„Chain-of-PerformæŒ‡å¯¼å®ç°è§†é¢‘å†…å®¹ä¸é’¢ç´éŸ³é¢‘çš„ç²¾ç¡®è¯­ä¹‰å’Œæ—¶é—´å¯¹é½ã€‚</li>
<li>æ•°æ®é›†æä¾›çµæ´»çš„è¯„ä¼°æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°é€šç”¨çš„ä»¥åŠä¸“é—¨åŒ–çš„è§†é¢‘åˆ°é’¢ç´ç”Ÿæˆä»»åŠ¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20038">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7892acaffef27f042bb5190e4d3fcdee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f52706a135ae1a1c27116920b58d7c6d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-698fb095898d50c88a8511041e0db5b8.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="ReasonPlan-Unified-Scene-Prediction-and-Decision-Reasoning-for-Closed-loop-Autonomous-Driving"><a href="#ReasonPlan-Unified-Scene-Prediction-and-Decision-Reasoning-for-Closed-loop-Autonomous-Driving" class="headerlink" title="ReasonPlan: Unified Scene Prediction and Decision Reasoning for   Closed-loop Autonomous Driving"></a>ReasonPlan: Unified Scene Prediction and Decision Reasoning for   Closed-loop Autonomous Driving</h2><p><strong>Authors:Xueyi Liu, Zuodong Zhong, Yuxin Guo, Yun-Fu Liu, Zhiguo Su, Qichao Zhang, Junli Wang, Yinfeng Gao, Yupeng Zheng, Qiao Lin, Huiyong Chen, Dongbin Zhao</strong></p>
<p>Due to the powerful vision-language reasoning and generalization abilities, multimodal large language models (MLLMs) have garnered significant attention in the field of end-to-end (E2E) autonomous driving. However, their application to closed-loop systems remains underexplored, and current MLLM-based methods have not shown clear superiority to mainstream E2E imitation learning approaches. In this work, we propose ReasonPlan, a novel MLLM fine-tuning framework designed for closed-loop driving through holistic reasoning with a self-supervised Next Scene Prediction task and supervised Decision Chain-of-Thought process. This dual mechanism encourages the model to align visual representations with actionable driving context, while promoting interpretable and causally grounded decision making. We curate a planning-oriented decision reasoning dataset, namely PDR, comprising 210k diverse and high-quality samples. Our method outperforms the mainstream E2E imitation learning method by a large margin of 19% L2 and 16.1 driving score on Bench2Drive benchmark. Furthermore, ReasonPlan demonstrates strong zero-shot generalization on unseen DOS benchmark, highlighting its adaptability in handling zero-shot corner cases. Code and dataset will be found in <a target="_blank" rel="noopener" href="https://github.com/Liuxueyi/ReasonPlan">https://github.com/Liuxueyi/ReasonPlan</a>. </p>
<blockquote>
<p>ç”±äºå¼ºå¤§çš„è§†è§‰è¯­è¨€æ¨ç†å’Œæ³›åŒ–èƒ½åŠ›ï¼Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨ç«¯åˆ°ç«¯ï¼ˆE2Eï¼‰è‡ªåŠ¨é©¾é©¶é¢†åŸŸå¼•èµ·äº†å¹¿æ³›å…³æ³¨ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨é—­ç¯ç³»ç»Ÿä¸­çš„åº”ç”¨ä»ç„¶è¢«æ¢ç´¢å¾—ä¸å¤Ÿæ·±å…¥ï¼Œå½“å‰çš„åŸºäºMLLMçš„æ–¹æ³•å°šæœªæ˜¾ç¤ºå‡ºå¯¹ä¸»æµçš„E2Eæ¨¡ä»¿å­¦ä¹ æ–¹æ³•çš„æ˜æ˜¾ä¼˜åŠ¿ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ReasonPlanï¼Œè¿™æ˜¯ä¸€ä¸ªä¸ºé—­ç¯é©¾é©¶è®¾è®¡çš„æ–°å‹MLLMå¾®è°ƒæ¡†æ¶ï¼Œå®ƒå¯ä»¥é€šè¿‡è‡ªæˆ‘ç›‘ç£çš„ä¸‹ä¸€åœºæ™¯é¢„æµ‹ä»»åŠ¡å’Œå—ç›‘ç£çš„å†³ç­–æ€ç»´é“¾è¿‡ç¨‹è¿›è¡Œæ•´ä½“æ¨ç†ã€‚è¿™ç§åŒé‡æœºåˆ¶é¼“åŠ±æ¨¡å‹å°†è§†è§‰è¡¨ç¤ºä¸å¯æ“ä½œæ€§çš„é©¾é©¶ä¸Šä¸‹æ–‡å¯¹é½ï¼ŒåŒæ—¶ä¿ƒè¿›å¯è§£é‡Šå’Œå› æœå†³ç­–çš„æ ¹åŸºã€‚æˆ‘ä»¬ç¼–åˆ¶äº†ä¸€ä¸ªé¢å‘è§„åˆ’å†³ç­–æ¨ç†çš„æ•°æ®é›†ï¼Œå³PDRï¼ŒåŒ…å«21ä¸‡ä¸ªå¤šæ ·ä¸”é«˜è´¨é‡æ ·æœ¬ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨Bench2DriveåŸºå‡†æµ‹è¯•ä¸Šå¤§å¹…è¶…è¶Šäº†ä¸»æµçš„E2Eæ¨¡ä»¿å­¦ä¹ æ–¹æ³•ï¼ŒL2å¾—åˆ†æé«˜äº†19%ï¼Œé©¾é©¶å¾—åˆ†æé«˜äº†16.1%ã€‚æ­¤å¤–ï¼ŒReasonPlanåœ¨æœªè§è¿‡çš„DOSåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºäº†å¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œçªæ˜¾äº†å…¶åœ¨å¤„ç†é›¶æ ·æœ¬è¾¹è§’æ¡ˆä¾‹æ—¶çš„é€‚åº”æ€§ã€‚ä»£ç å’Œæ•°æ®é›†å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/Liuxueyi/ReasonPlan">https://github.com/Liuxueyi/ReasonPlan</a>ä¸­æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20024v1">PDF</a> 18 pages; 9 figures; <a target="_blank" rel="noopener" href="https://github.com/Liuxueyi/ReasonPlan">https://github.com/Liuxueyi/ReasonPlan</a></p>
<p><strong>Summary</strong></p>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨ç«¯åˆ°ç«¯ï¼ˆE2Eï¼‰è‡ªåŠ¨é©¾é©¶é¢†åŸŸå…·æœ‰å¼ºå¤§çš„è§†è§‰è¯­è¨€æ¨ç†å’Œæ³›åŒ–èƒ½åŠ›ï¼Œä½†å®ƒä»¬åœ¨é—­ç¯ç³»ç»Ÿä¸­çš„åº”ç”¨ä»ç„¶è¢«å¿½è§†ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºReasonPlançš„æ–°å‹MLLMå¾®è°ƒæ¡†æ¶ï¼Œç”¨äºé€šè¿‡æ•´ä½“æ¨ç†è¿›è¡Œé—­ç¯é©¾é©¶ï¼Œå…·æœ‰è‡ªç›‘ç£çš„ä¸‹ä¸€åœºæ™¯é¢„æµ‹ä»»åŠ¡å’Œç›‘ç£çš„å†³ç­–æ€ç»´é“¾è¿‡ç¨‹ã€‚è¯¥åŒæœºåˆ¶é¼“åŠ±æ¨¡å‹å°†è§†è§‰è¡¨ç¤ºä¸å¯æ“ä½œçš„é©¾é©¶ä¸Šä¸‹æ–‡å¯¹é½ï¼ŒåŒæ—¶ä¿ƒè¿›å¯è§£é‡Šå’Œå› æœæ€§çš„å†³ç­–åˆ¶å®šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨ç«¯åˆ°ç«¯ï¼ˆE2Eï¼‰è‡ªåŠ¨é©¾é©¶é¢†åŸŸå—åˆ°å…³æ³¨ï¼Œä½†å®ƒä»¬åœ¨é—­ç¯ç³»ç»Ÿä¸­çš„åº”ç”¨ä»ç„¶ä¸è¶³ã€‚</li>
<li>å½“å‰MLLMæ–¹æ³•å°šæœªæ˜ç¡®ä¼˜äºä¸»æµçš„E2Eæ¨¡ä»¿å­¦ä¹ æ–¹æ³•ã€‚</li>
<li>ReasonPlanæ˜¯ä¸€ç§æ–°å‹çš„MLLMå¾®è°ƒæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡æ•´ä½“æ¨ç†è¿›è¡Œé—­ç¯é©¾é©¶ã€‚</li>
<li>ReasonPlanå…·æœ‰è‡ªç›‘ç£çš„ä¸‹ä¸€åœºæ™¯é¢„æµ‹ä»»åŠ¡å’Œç›‘ç£çš„å†³ç­–æ€ç»´é“¾è¿‡ç¨‹ã€‚</li>
<li>è¯¥æ¡†æ¶é¼“åŠ±æ¨¡å‹å°†è§†è§‰è¡¨ç¤ºä¸å¯æ“ä½œçš„é©¾é©¶ä¸Šä¸‹æ–‡å¯¹é½ã€‚</li>
<li>ReasonPlanåœ¨Bench2DriveåŸºå‡†æµ‹è¯•ä¸Šæ˜¾è‘—ä¼˜äºä¸»æµçš„E2Eæ¨¡ä»¿å­¦ä¹ æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20024">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-593ed0b2532361a07eb63db74009cae2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7750cabc8a6913530f36590dee31c0d9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b160570eb485c062153b5d4b37bcaaa6.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="WebCoT-Enhancing-Web-Agent-Reasoning-by-Reconstructing-Chain-of-Thought-in-Reflection-Branching-and-Rollback"><a href="#WebCoT-Enhancing-Web-Agent-Reasoning-by-Reconstructing-Chain-of-Thought-in-Reflection-Branching-and-Rollback" class="headerlink" title="WebCoT: Enhancing Web Agent Reasoning by Reconstructing Chain-of-Thought   in Reflection, Branching, and Rollback"></a>WebCoT: Enhancing Web Agent Reasoning by Reconstructing Chain-of-Thought   in Reflection, Branching, and Rollback</h2><p><strong>Authors:Minda Hu, Tianqing Fang, Jianshu Zhang, Junyu Ma, Zhisong Zhang, Jingyan Zhou, Hongming Zhang, Haitao Mi, Dong Yu, Irwin King</strong></p>
<p>Web agents powered by Large Language Models (LLMs) show promise for next-generation AI, but their limited reasoning in uncertain, dynamic web environments hinders robust deployment. In this paper, we identify key reasoning skills essential for effective web agents, i.e., reflection &amp; lookahead, branching, and rollback, and curate trajectory data that exemplifies these abilities by reconstructing the agentâ€™s (inference-time) reasoning algorithms into chain-of-thought rationales. We conduct experiments in the agent self-improving benchmark, OpenWebVoyager, and demonstrate that distilling salient reasoning patterns into the backbone LLM via simple fine-tuning can substantially enhance its performance. Our approach yields significant improvements across multiple benchmarks, including WebVoyager, Mind2web-live, and SimpleQA (web search), highlighting the potential of targeted reasoning skill enhancement for web agents. </p>
<blockquote>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç½‘ç»œä»£ç†å¯¹äºä¸‹ä¸€ä»£äººå·¥æ™ºèƒ½æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ï¼Œä½†å®ƒä»¬åœ¨å¤„ç†ä¸ç¡®å®šæ€§å’ŒåŠ¨æ€å˜åŒ–çš„ç½‘ç»œç¯å¢ƒä¸­çš„æ¨ç†èƒ½åŠ›æœ‰é™ï¼Œé˜»ç¢äº†å…¶ç¨³å¥éƒ¨ç½²ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ç¡®å®šäº†ç½‘ç»œä»£ç†è¿›è¡Œæœ‰æ•ˆæ¨ç†æ‰€å¿…éœ€çš„å…³é”®æŠ€èƒ½ï¼Œå³åæ€ä¸å‰ç»æ€§ã€åˆ†æ”¯å’Œå›æ»šï¼Œå¹¶é€šè¿‡é‡å»ºä»£ç†çš„ï¼ˆæ¨ç†æ—¶é—´ï¼‰æ¨ç†ç®—æ³•æ¥æ„å»ºä½“ç°è¿™äº›èƒ½åŠ›çš„è½¨è¿¹æ•°æ®ã€‚æˆ‘ä»¬åœ¨ä»£ç†è‡ªæˆ‘æ”¹è¿›åŸºå‡†æµ‹è¯•OpenWebVoyagerä¸­è¿›è¡Œäº†å®éªŒï¼Œå¹¶è¯æ˜é€šè¿‡ç®€å•å¾®è°ƒå°†æ˜¾è‘—æ¨ç†æ¨¡å¼è’¸é¦åˆ°éª¨å¹²LLMä¸­å¯ä»¥æå¤§åœ°æå‡å…¶æ€§èƒ½ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šåŸºå‡†æµ‹è¯•ä¸­äº§ç”Ÿäº†æ˜¾è‘—çš„æ”¹è¿›ï¼ŒåŒ…æ‹¬WebVoyagerã€Mind2web-liveå’ŒSimpleQAï¼ˆç½‘ç»œæœç´¢ï¼‰ï¼Œçªæ˜¾äº†é’ˆå¯¹ç½‘ç»œä»£ç†è¿›è¡Œé’ˆå¯¹æ€§æ¨ç†æŠ€èƒ½å¢å¼ºçš„é‡è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20013v1">PDF</a> 18 pages</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹é©±åŠ¨çš„Webä»£ç†å±•ç°å‡ºä¸‹ä¸€ä»£äººå·¥æ™ºèƒ½çš„æ½œåŠ›ï¼Œä½†åœ¨ä¸ç¡®å®šã€åŠ¨æ€çš„Webç¯å¢ƒä¸­æœ‰é™çš„æ¨ç†èƒ½åŠ›é˜»ç¢äº†å…¶ç¨³å¥éƒ¨ç½²ã€‚æœ¬æ–‡ç¡®å®šäº†æœ‰æ•ˆWebä»£ç†æ‰€éœ€çš„å…³é”®æ¨ç†æŠ€èƒ½ï¼ŒåŒ…æ‹¬åæ€ä¸å‰ç»æ€§ã€åˆ†æ”¯å’Œå›æ»šï¼Œå¹¶é€šè¿‡é‡å»ºä»£ç†ï¼ˆæ¨ç†æ—¶é—´ï¼‰çš„æ¨ç†ç®—æ³•æ¥å±•ç¤ºè¿™äº›èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼Œé€šè¿‡ç®€å•å¾®è°ƒå°†æ˜¾è‘—æ¨ç†æ¨¡å¼è’¸é¦åˆ°åŸºç¡€å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ï¼Œå¯ä»¥æ˜¾è‘—æé«˜å…¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬WebVoyagerã€Mind2web-liveå’ŒSimpleQAï¼ˆç½‘é¡µæœç´¢ï¼‰ã€‚è¿™çªæ˜¾äº†é’ˆå¯¹Webä»£ç†è¿›è¡Œæœ‰é’ˆå¯¹æ€§çš„æ¨ç†æŠ€èƒ½å¢å¼ºçš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Webä»£ç†åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é©±åŠ¨ä¸‹å±•ç°å‡ºä¸‹ä¸€ä»£äººå·¥æ™ºèƒ½çš„æ½œåŠ›ã€‚</li>
<li>åœ¨ä¸ç¡®å®šã€åŠ¨æ€çš„Webç¯å¢ƒä¸­ï¼ŒWebä»£ç†çš„æœ‰é™æ¨ç†èƒ½åŠ›æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚</li>
<li>æœ‰æ•ˆçš„Webä»£ç†éœ€è¦å…·å¤‡å…³é”®æ¨ç†æŠ€èƒ½ï¼ŒåŒ…æ‹¬åæ€ä¸å‰ç»æ€§ã€åˆ†æ”¯å’Œå›æ»šã€‚</li>
<li>é€šè¿‡é‡å»ºä»£ç†çš„æ¨ç†ç®—æ³•ï¼Œå¯ä»¥å±•ç¤ºè¿™äº›å…³é”®æ¨ç†æŠ€èƒ½ã€‚</li>
<li>åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼Œé€šè¿‡ç®€å•å¾®è°ƒå°†æ¨ç†æ¨¡å¼è’¸é¦åˆ°LLMä¸­ï¼Œå¯ä»¥æ˜¾è‘—æé«˜ä»£ç†çš„æ€§èƒ½ã€‚</li>
<li>å®éªŒç»“æœè¯æ˜äº†è¯¥æ–¹æ³•åœ¨WebVoyagerã€Mind2web-liveå’ŒSimpleQAç­‰åŸºå‡†æµ‹è¯•ä¸­çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20013">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-63406a02c500d7588dca48c66f8f64a0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d40c966d83b8576537fcb83fac07759f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ae6eb6cfc29b4a8abb0ba4f5e3fc940f.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Enigmata-Scaling-Logical-Reasoning-in-Large-Language-Models-with-Synthetic-Verifiable-Puzzles"><a href="#Enigmata-Scaling-Logical-Reasoning-in-Large-Language-Models-with-Synthetic-Verifiable-Puzzles" class="headerlink" title="Enigmata: Scaling Logical Reasoning in Large Language Models with   Synthetic Verifiable Puzzles"></a>Enigmata: Scaling Logical Reasoning in Large Language Models with   Synthetic Verifiable Puzzles</h2><p><strong>Authors:Jiangjie Chen, Qianyu He, Siyu Yuan, Aili Chen, Zhicheng Cai, Weinan Dai, Hongli Yu, Qiying Yu, Xuefeng Li, Jiaze Chen, Hao Zhou, Mingxuan Wang</strong></p>
<p>Large Language Models (LLMs), such as OpenAIâ€™s o1 and DeepSeekâ€™s R1, excel at advanced reasoning tasks like math and coding via Reinforcement Learning with Verifiable Rewards (RLVR), but still struggle with puzzles solvable by humans without domain knowledge. We introduce Enigmata, the first comprehensive suite tailored for improving LLMs with puzzle reasoning skills. It includes 36 tasks across seven categories, each with 1) a generator that produces unlimited examples with controllable difficulty and 2) a rule-based verifier for automatic evaluation. This generator-verifier design supports scalable, multi-task RL training, fine-grained analysis, and seamless RLVR integration. We further propose Enigmata-Eval, a rigorous benchmark, and develop optimized multi-task RLVR strategies. Our trained model, Qwen2.5-32B-Enigmata, consistently surpasses o3-mini-high and o1 on the puzzle reasoning benchmarks like Enigmata-Eval, ARC-AGI (32.8%), and ARC-AGI 2 (0.6%). It also generalizes well to out-of-domain puzzle benchmarks and mathematical reasoning, with little multi-tasking trade-off. When trained on larger models like Seed1.5-Thinking (20B activated parameters and 200B total parameters), puzzle data from Enigmata further boosts SoTA performance on advanced math and STEM reasoning tasks such as AIME (2024-2025), BeyondAIME and GPQA (Diamond), showing nice generalization benefits of Enigmata. This work offers a unified, controllable framework for advancing logical reasoning in LLMs. Resources of this work can be found at <a target="_blank" rel="noopener" href="https://seed-enigmata.github.io/">https://seed-enigmata.github.io</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œå¦‚OpenAIçš„o1å’ŒDeepSeekçš„R1ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰æœºåˆ¶æ“…é•¿æ•°å­¦å’Œç¼–ç ç­‰é«˜çº§æ¨ç†ä»»åŠ¡ï¼Œä½†åœ¨è§£å†³äººç±»æ— éœ€é¢†åŸŸçŸ¥è¯†å³å¯è§£å†³çš„è°œé¢˜æ–¹é¢ä»å­˜åœ¨å›°éš¾ã€‚æˆ‘ä»¬æ¨å‡ºäº†Enigmataï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªæ—¨åœ¨æé«˜LLMè°œé¢˜æ¨ç†èƒ½åŠ›çš„ç»¼åˆå¥—ä»¶ã€‚å®ƒåŒ…æ‹¬7ä¸ªç±»åˆ«çš„36é¡¹ä»»åŠ¡ï¼Œæ¯ä¸ªç±»åˆ«éƒ½æœ‰1ï¼‰ä¸€ä¸ªå¯æ ¹æ®éš¾åº¦äº§ç”Ÿæ— é™ä¾‹å­çš„ç”Ÿæˆå™¨ï¼Œä»¥åŠ2ï¼‰ä¸€ä¸ªåŸºäºè§„åˆ™çš„è‡ªåŠ¨æœºè¿›è¡Œè‡ªåŠ¨è¯„ä¼°ã€‚è¿™ç§ç”Ÿæˆå™¨-éªŒè¯å™¨è®¾è®¡æ”¯æŒå¯æ‰©å±•çš„å¤šä»»åŠ¡å¼ºåŒ–å­¦ä¹ è®­ç»ƒã€ç²¾ç»†åˆ†æä»¥åŠæ— ç¼RLVRé›†æˆã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†ä¸¥æ ¼çš„åŸºå‡†æµ‹è¯•Enigmata-Evalï¼Œå¹¶å¼€å‘äº†ä¼˜åŒ–çš„å¤šä»»åŠ¡RLVRç­–ç•¥ã€‚æˆ‘ä»¬è®­ç»ƒçš„æ¨¡å‹Qwen2.5-32B-Enigmataåœ¨è°œé¢˜æ¨ç†åŸºå‡†æµ‹è¯•ï¼ˆå¦‚Enigmata-Evalã€ARC-AGI 32.8ï¼…å’ŒARC-AGI 2 0.6ï¼…ï¼‰ä¸Šå§‹ç»ˆè¶…è¿‡o3-mini-highå’Œo1ã€‚å®ƒè¿˜å¾ˆå¥½åœ°æ¨å¹¿åˆ°åŸŸå¤–è°œé¢˜åŸºå‡†æµ‹è¯•å’Œæ•°å­¦æ¨ç†ï¼Œå¤šä»»åŠ¡æƒè¡¡å¾®ä¹å…¶å¾®ã€‚åœ¨æ›´å¤§çš„æ¨¡å‹ï¼ˆå¦‚å…·æœ‰20Bæ¿€æ´»å‚æ•°å’Œæ€»è®¡200Bå‚æ•°çš„Seed1.5-Thinkingï¼‰ä¸Šè¿›è¡Œè®­ç»ƒæ—¶ï¼Œæ¥è‡ªEnigmataçš„è°œé¢˜æ•°æ®è¿›ä¸€æ­¥æé«˜äº†å…ˆè¿›æ•°å­¦å’ŒSTEMæ¨ç†ä»»åŠ¡ï¼ˆå¦‚AIMEï¼ˆé¢å‘æœªæ¥çš„é«˜ä¸­è€ƒè¯•å†…å®¹ï¼‰ã€BeyondAIMEå’ŒGPQA Diamondï¼‰çš„é¡¶å°–è¡¨ç°æ°´å¹³ï¼Œå±•ç¤ºäº†Enigmataçš„è‰¯å¥½æ³›åŒ–æ•ˆç›Šã€‚æœ¬å·¥ä½œæä¾›äº†ä¸€ä¸ªç»Ÿä¸€ä¸”å¯æ§çš„æ¡†æ¶ï¼Œä»¥æ¨åŠ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„é€»è¾‘æ¨ç†èƒ½åŠ›çš„å‘å±•ã€‚æœ‰å…³è¿™é¡¹å·¥ä½œçš„èµ„æºå¯ä»¥åœ¨<a target="_blank" rel="noopener" href="https://seed-enigmata.github.io/">https://seed-enigmata.github.io</a>æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19914v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é€šè¿‡å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰è¿›è¡Œæ•°å­¦å’Œç¼–ç ç­‰é«˜çº§æ¨ç†ä»»åŠ¡æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨è§£å†³äººç±»æ— éœ€é¢†åŸŸçŸ¥è¯†å³å¯è§£å†³çš„è°œé¢˜æ–¹é¢ä»å­˜åœ¨å›°éš¾ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†Enigmataï¼Œå®ƒæ˜¯ä¸“ä¸ºæé«˜LLMsè§£å†³è°œé¢˜æ¨ç†èƒ½åŠ›è€Œè®¾è®¡çš„ç¬¬ä¸€ä¸ªç»¼åˆå¥—ä»¶ã€‚EnigmataåŒ…æ‹¬ä¸ƒä¸ªç±»åˆ«å…±36ä¸ªä»»åŠ¡ï¼Œæ¯ä¸ªä»»åŠ¡éƒ½æœ‰1ï¼‰å¯ç”Ÿæˆæ— é™ä¾‹å­å¹¶æ§åˆ¶éš¾åº¦çš„ç”Ÿæˆå™¨ï¼Œä»¥åŠ2ï¼‰ç”¨äºè‡ªåŠ¨è¯„ä¼°çš„è§„åˆ™éªŒè¯å™¨ã€‚è¿™ç§ç”Ÿæˆå™¨-éªŒè¯å™¨è®¾è®¡æ”¯æŒå¯æ‰©å±•çš„å¤šä»»åŠ¡RLè®­ç»ƒã€ç²¾ç»†åˆ†æä»¥åŠä¸RLVRæ— ç¼é›†æˆã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸¥æ ¼çš„åŸºå‡†æµ‹è¯•Enigmata-Evalï¼Œå¹¶å¼€å‘äº†ä¼˜åŒ–çš„å¤šä»»åŠ¡RLVRç­–ç•¥ã€‚ç»è¿‡Enigmataè®­ç»ƒçš„æ¨¡å‹Qwen2.5-32Båœ¨è°œé¢˜æ¨ç†åŸºå‡†æµ‹è¯•ï¼ˆå¦‚Enigmata-Evalã€ARC-AGI 32.8%å’ŒARC-AGI 2 0.6%ï¼‰ä¸Šå§‹ç»ˆè¶…è¿‡o3-mini-highå’Œo1ã€‚è¯¥æ¨¡å‹è¿˜èƒ½å¤Ÿåœ¨è„±ç¦»é¢†åŸŸçš„è°œé¢˜åŸºå‡†æµ‹è¯•å’Œæ•°å­¦æ¨ç†ä¸Šè¡¨ç°è‰¯å¥½ï¼Œå¤šä»»åŠ¡äº¤æ¢ä»£ä»·è¾ƒå°ã€‚å½“åœ¨æ›´å¤§çš„æ¨¡å‹ï¼ˆå¦‚å…·æœ‰20Bæ¿€æ´»å‚æ•°å’Œ200Bæ€»å‚æ•°çš„Seed1.5-Thinkingï¼‰ä¸Šè¿›è¡Œè®­ç»ƒæ—¶ï¼ŒEnigmataçš„è°œé¢˜æ•°æ®è¿›ä¸€æ­¥æé«˜äº†åœ¨é«˜çº§æ•°å­¦å’ŒSTEMæ¨ç†ä»»åŠ¡ï¼ˆå¦‚AIMEã€BeyondAIMEå’ŒGPQA Diamondï¼‰ä¸Šçš„æœ€æ–°æ€§èƒ½ï¼Œæ˜¾ç¤ºå‡ºEnigmataè‰¯å¥½çš„æ³›åŒ–æ•ˆç›Šã€‚æ­¤å·¥ä½œæä¾›äº†ä¸€ä¸ªç»Ÿä¸€ã€å¯æ§çš„æ¡†æ¶ï¼Œä»¥æ¨åŠ¨LLMsä¸­çš„é€»è¾‘æ¨ç†çš„å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é«˜çº§æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†è§£å†³è°œé¢˜æ¨ç†çš„èƒ½åŠ›ä»ç„¶æœ‰é™ã€‚</li>
<li>Enigmataæ˜¯ä¸ºæé«˜LLMsè§£å†³è°œé¢˜æ¨ç†èƒ½åŠ›è€Œè®¾è®¡çš„ç¬¬ä¸€ä¸ªç»¼åˆå¥—ä»¶ï¼ŒåŒ…å«36ä¸ªä»»åŠ¡ï¼Œè¦†ç›–ä¸ƒä¸ªç±»åˆ«ã€‚</li>
<li>Enigmataé‡‡ç”¨ç”Ÿæˆå™¨-éªŒè¯å™¨çš„è®¾è®¡ç»“æ„ï¼Œæ”¯æŒå¤šä»»åŠ¡RLè®­ç»ƒã€ç²¾ç»†åˆ†æä»¥åŠä¸RLVRæ— ç¼é›†æˆã€‚</li>
<li>Enigmataæå‡ºçš„ä¸¥æ ¼åŸºå‡†æµ‹è¯•Enigmata-Evalæœ‰åŠ©äºè¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>Qwen2.5-32Bæ¨¡å‹åœ¨å¤šä¸ªè°œé¢˜æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä¸”èƒ½å¤Ÿæ³›åŒ–åˆ°ä¸åŒé¢†åŸŸçš„è°œé¢˜å’Œæ•°å­¦æ¨ç†ä»»åŠ¡ã€‚</li>
<li>å½“åœ¨æ›´å¤§çš„æ¨¡å‹ä¸Šè¿›è¡Œè®­ç»ƒæ—¶ï¼ŒEnigmataçš„è°œé¢˜æ•°æ®å¯è¿›ä¸€æ­¥æé«˜åœ¨é«˜çº§æ•°å­¦å’ŒSTEMæ¨ç†ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19914">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-428c01b5d459018eb8ee1cd2667944e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4d1f206baf012458973de3c909a84b01.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bff5af8d287bbac16ca8f1029cb5fcb8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1e1883f8ee733dc50ea93f5816f41fc7.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Vad-R1-Towards-Video-Anomaly-Reasoning-via-Perception-to-Cognition-Chain-of-Thought"><a href="#Vad-R1-Towards-Video-Anomaly-Reasoning-via-Perception-to-Cognition-Chain-of-Thought" class="headerlink" title="Vad-R1: Towards Video Anomaly Reasoning via Perception-to-Cognition   Chain-of-Thought"></a>Vad-R1: Towards Video Anomaly Reasoning via Perception-to-Cognition   Chain-of-Thought</h2><p><strong>Authors:Chao Huang, Benfeng Wang, Jie Wen, Chengliang Liu, Wei Wang, Li Shen, Xiaochun Cao</strong></p>
<p>Recent advancements in reasoning capability of Multimodal Large Language Models (MLLMs) demonstrate its effectiveness in tackling complex visual tasks. However, existing MLLM-based Video Anomaly Detection (VAD) methods remain limited to shallow anomaly descriptions without deep reasoning. In this paper, we propose a new task named Video Anomaly Reasoning (VAR), which aims to enable deep analysis and understanding of anomalies in the video by requiring MLLMs to think explicitly before answering. To this end, we propose Vad-R1, an end-to-end MLLM-based framework for VAR. Specifically, we design a Perception-to-Cognition Chain-of-Thought (P2C-CoT) that simulates the human process of recognizing anomalies, guiding the MLLM to reason anomaly step-by-step. Based on the structured P2C-CoT, we construct Vad-Reasoning, a dedicated dataset for VAR. Furthermore, we propose an improved reinforcement learning algorithm AVA-GRPO, which explicitly incentivizes the anomaly reasoning capability of MLLMs through a self-verification mechanism with limited annotations. Experimental results demonstrate that Vad-R1 achieves superior performance, outperforming both open-source and proprietary models on VAD and VAR tasks. Codes and datasets will be released at <a target="_blank" rel="noopener" href="https://github.com/wbfwonderful/Vad-R1">https://github.com/wbfwonderful/Vad-R1</a>. </p>
<blockquote>
<p>è¿‘æœŸå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æ¨ç†èƒ½åŠ›è¿›å±•è¯æ˜äº†å…¶åœ¨å¤„ç†å¤æ‚è§†è§‰ä»»åŠ¡ä¸Šçš„æœ‰æ•ˆæ€§ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºäºMLLMçš„è§†é¢‘å¼‚å¸¸æ£€æµ‹ï¼ˆVADï¼‰æ–¹æ³•åœ¨æè¿°å¼‚å¸¸ç°è±¡æ—¶ä»ç„¶å±€é™äºæµ…å±‚æè¿°ï¼Œç¼ºä¹æ·±åº¦æ¨ç†ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°çš„ä»»åŠ¡ï¼Œåä¸ºè§†é¢‘å¼‚å¸¸æ¨ç†ï¼ˆVARï¼‰ï¼Œæ—¨åœ¨é€šè¿‡è¦æ±‚MLLMåœ¨å›ç­”é—®é¢˜å‰å…ˆè¿›è¡Œæ·±å…¥æ€è€ƒï¼Œå®ç°å¯¹è§†é¢‘ä¸­å¼‚å¸¸çš„æ·±åº¦åˆ†æå’Œç†è§£ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºMLLMçš„ç«¯åˆ°ç«¯æ¡†æ¶Vad-R1ç”¨äºVARã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä»æ„ŸçŸ¥åˆ°è®¤çŸ¥çš„æ€ç»´é“¾ï¼ˆP2C-CoTï¼‰ï¼Œæ¨¡æ‹Ÿäººç±»è¯†åˆ«å¼‚å¸¸çš„è¿‡ç¨‹ï¼Œå¼•å¯¼MLLMé€æ­¥æ¨ç†å¼‚å¸¸ã€‚åŸºäºç»“æ„åŒ–P2C-CoTï¼Œæˆ‘ä»¬æ„å»ºäº†Vad-Reasoningæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸“é—¨ä¸ºVARè®¾è®¡çš„æ•°æ®é›†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ”¹è¿›çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•AVA-GRPOï¼Œå®ƒé€šè¿‡è‡ªæˆ‘éªŒè¯æœºåˆ¶æœ‰é™æ ‡æ³¨æ¥æ˜ç¡®æ¿€åŠ±MLLMçš„å¼‚å¸¸æ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVad-R1åœ¨VADå’ŒVARä»»åŠ¡ä¸Šçš„æ€§èƒ½å‡ä¼˜äºå¼€æºå’Œä¸“æœ‰æ¨¡å‹ã€‚ç›¸å…³ä»£ç å’Œæ•°æ®é›†å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/wbfwonderful/Vad-R1%E4%B8%8A%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/wbfwonderful/Vad-R1ä¸Šå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19877v1">PDF</a> 9 pages, 4 figures</p>
<p><strong>Summary</strong><br>è§†é¢‘å¼‚å¸¸æ¨ç†ï¼ˆVARï¼‰æ˜¯è§†é¢‘å¼‚å¸¸æ£€æµ‹ï¼ˆVADï¼‰çš„æ–°ä»»åŠ¡ï¼Œæ—¨åœ¨é€šè¿‡è¦æ±‚å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å›ç­”é—®é¢˜å‰å…ˆæ€è€ƒï¼Œå®ç°å¯¹è§†é¢‘ä¸­çš„å¼‚å¸¸è¿›è¡Œæ·±åº¦åˆ†æå’Œç†è§£ã€‚æœ¬æ–‡æå‡ºäº†åŸºäºMLLMsçš„VARæ¡†æ¶Vad-R1ï¼Œè®¾è®¡äº†ä¸€ä¸ªæ¨¡æ‹Ÿäººç±»è¯†åˆ«å¼‚å¸¸çš„æ„ŸçŸ¥åˆ°è®¤çŸ¥æ€ç»´é“¾ï¼ˆP2C-CoTï¼‰ï¼Œå¹¶é€šè¿‡ç»“æ„åŒ–P2C-CoTæ„å»ºäº†Vad-Reasoningæ•°æ®é›†ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†ä¸€ç§æ”¹è¿›å¼ºåŒ–å­¦ä¹ ç®—æ³•AVA-GRPOï¼Œé€šè¿‡è‡ªæˆ‘éªŒè¯æœºåˆ¶æœ‰é™æ ‡æ³¨çš„æƒ…å†µä¸‹æ¿€åŠ±MLLMçš„å¼‚å¸¸æ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒVad-R1åœ¨VADå’ŒVARä»»åŠ¡ä¸Šå‡è¡¨ç°ä¼˜è¶Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†é¢‘å¼‚å¸¸æ¨ç†ï¼ˆVARï¼‰æ˜¯ä¸€ä¸ªæ–°å…´ä»»åŠ¡ï¼Œæ—¨åœ¨é€šè¿‡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰è¿›è¡Œæ·±åº¦åˆ†æå’Œç†è§£è§†é¢‘ä¸­çš„å¼‚å¸¸ã€‚</li>
<li>Vad-R1æ˜¯ä¸€ä¸ªåŸºäºMLLMsçš„VARæ¡†æ¶ï¼ŒåŒ…å«æ„ŸçŸ¥åˆ°è®¤çŸ¥æ€ç»´é“¾ï¼ˆP2C-CoTï¼‰ï¼Œæ¨¡æ‹Ÿäººç±»è¯†åˆ«å¼‚å¸¸çš„æ­¥éª¤ã€‚</li>
<li>Vad-Reasoningæ•°æ®é›†ç”¨äºVARä»»åŠ¡ï¼ŒåŸºäºç»“æ„åŒ–P2C-CoTæ„å»ºã€‚</li>
<li>AVA-GRPOæ˜¯ä¸€ç§æ”¹è¿›å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œé€šè¿‡è‡ªæˆ‘éªŒè¯æœºåˆ¶æœ‰é™æ ‡æ³¨æƒ…å†µä¸‹æé«˜MLLMçš„å¼‚å¸¸æ¨ç†èƒ½åŠ›ã€‚</li>
<li>Vad-R1åœ¨VADå’ŒVARä»»åŠ¡ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œè¶…è¿‡äº†å¼€æºå’Œä¸“æœ‰æ¨¡å‹ã€‚</li>
<li>Vad-R1æ¡†æ¶å’Œæ•°æ®é›†å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/wbfwonderful/Vad-R1%E5%85%AC%E5%BC%80%E3%80%82">https://github.com/wbfwonderful/Vad-R1å…¬å¼€ã€‚</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19877">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-75230883678db0033808aee8142bb7f6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-306702670c83e5a4036eb981c732a103.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-50b1145611dced1f15edde7c9f94dc80.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5f45dc01e1975881802c65a651f602a8.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="HS-STAR-Hierarchical-Sampling-for-Self-Taught-Reasoners-via-Difficulty-Estimation-and-Budget-Reallocation"><a href="#HS-STAR-Hierarchical-Sampling-for-Self-Taught-Reasoners-via-Difficulty-Estimation-and-Budget-Reallocation" class="headerlink" title="HS-STAR: Hierarchical Sampling for Self-Taught Reasoners via Difficulty   Estimation and Budget Reallocation"></a>HS-STAR: Hierarchical Sampling for Self-Taught Reasoners via Difficulty   Estimation and Budget Reallocation</h2><p><strong>Authors:Feng Xiong, Hongling Xu, Yifei Wang, Runxi Cheng, Yong Wang, Xiangxiang Chu</strong></p>
<p>Self-taught reasoners (STaRs) enhance the mathematical reasoning abilities of large language models (LLMs) by leveraging self-generated responses for self-training. Recent studies have incorporated reward models to guide response selection or decoding, aiming to obtain higher-quality data. However, they typically allocate a uniform sampling budget across all problems, overlooking the varying utility of problems at different difficulty levels. In this work, we conduct an empirical study and find that problems near the boundary of the LLMâ€™s reasoning capability offer significantly greater learning utility than both easy and overly difficult ones. To identify and exploit such problems, we propose HS-STaR, a Hierarchical Sampling framework for Self-Taught Reasoners. Given a fixed sampling budget, HS-STaR first performs lightweight pre-sampling with a reward-guided difficulty estimation strategy to efficiently identify boundary-level problems. Subsequently, it dynamically reallocates the remaining budget toward these high-utility problems during a re-sampling phase, maximizing the generation of valuable training data. Extensive experiments across multiple reasoning benchmarks and backbone LLMs demonstrate that HS-STaR significantly outperforms other baselines without requiring additional sampling budget. </p>
<blockquote>
<p>è‡ªæˆ‘æ•™æˆæ¨ç†è€…ï¼ˆSTaRsï¼‰é€šè¿‡åˆ©ç”¨è‡ªæˆ‘ç”Ÿæˆçš„å›åº”è¿›è¡Œè‡ªæˆ‘è®­ç»ƒï¼Œå¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚è¿‘æœŸçš„ç ”ç©¶çº³å…¥äº†å¥–åŠ±æ¨¡å‹æ¥æŒ‡å¯¼å›åº”é€‰æ‹©æˆ–è§£ç ï¼Œæ—¨åœ¨è·å¾—æ›´é«˜è´¨é‡çš„æ•°æ®ã€‚ç„¶è€Œï¼Œå®ƒä»¬é€šå¸¸ä¼šåœ¨æ‰€æœ‰é—®é¢˜ä¸Šåˆ†é…ç»Ÿä¸€çš„é‡‡æ ·é¢„ç®—ï¼Œå¿½ç•¥äº†ä¸åŒéš¾åº¦çº§åˆ«é—®é¢˜çš„ä¸åŒæ•ˆç”¨ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è¿›è¡Œäº†å®è¯ç ”ç©¶ï¼Œå‘ç°æ¥è¿‘LLMæ¨ç†èƒ½åŠ›è¾¹ç•Œçš„é—®é¢˜ä¸ç®€å•å’Œè¿‡äºå›°éš¾çš„é—®é¢˜ç›¸æ¯”ï¼Œæä¾›äº†æ›´å¤§çš„å­¦ä¹ æ•ˆç”¨ã€‚ä¸ºäº†è¯†åˆ«å’Œåˆ©ç”¨è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†HS-STaRï¼Œä¸€ç§ç”¨äºè‡ªæˆ‘æ•™æˆæ¨ç†è€…çš„åˆ†å±‚é‡‡æ ·æ¡†æ¶ã€‚ç»™å®šå›ºå®šçš„é‡‡æ ·é¢„ç®—ï¼ŒHS-STaRé¦–å…ˆä½¿ç”¨å¥–åŠ±æŒ‡å¯¼çš„éš¾åº¦ä¼°è®¡ç­–ç•¥è¿›è¡Œè½»é‡çº§é¢„é‡‡æ ·ï¼Œä»¥æœ‰æ•ˆè¯†åˆ«è¾¹ç•Œçº§åˆ«çš„é—®é¢˜ã€‚éšåï¼Œåœ¨é‡æ–°é‡‡æ ·é˜¶æ®µï¼Œå®ƒå°†è¿™äº›é«˜ä»·å€¼é—®é¢˜ä½œä¸ºé¦–è¦ä»»åŠ¡é‡æ–°åˆ†é…å‰©ä½™çš„é¢„ç®—ï¼Œä»¥æœ€å¤§åŒ–æœ‰ä»·å€¼çš„è®­ç»ƒæ•°æ®çš„ç”Ÿæˆã€‚åœ¨å¤šä¸ªæ¨ç†åŸºå‡†æµ‹è¯•å’Œå¤§å‹è¯­è¨€æ¨¡å‹ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒHS-STaRåœ¨ä¸éœ€è¦é¢å¤–é‡‡æ ·é¢„ç®—çš„æƒ…å†µä¸‹æ˜¾è‘—ä¼˜äºå…¶ä»–åŸºçº¿æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19866v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è‡ªæˆ‘è®­ç»ƒæ¨ç†è€…ï¼ˆSTaRsï¼‰é€šè¿‡åˆ©ç”¨è‡ªæˆ‘ç”Ÿæˆçš„å“åº”è¿›è¡Œè‡ªè®­ç»ƒï¼Œå¢å¼ºäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚æœ€æ–°ç ”ç©¶å¼•å…¥äº†å¥–åŠ±æ¨¡å‹æ¥æŒ‡å¯¼å“åº”é€‰æ‹©æˆ–è§£ç ï¼Œä»¥è·å–æ›´é«˜è´¨é‡çš„æ•°æ®ã€‚ç„¶è€Œï¼Œå®ƒä»¬é€šå¸¸åœ¨æ‰€æœ‰é—®é¢˜ä¸Šåˆ†é…å‡åŒ€çš„é‡‡æ ·é¢„ç®—ï¼Œå¿½ç•¥äº†ä¸åŒéš¾åº¦çº§åˆ«é—®é¢˜çš„ä¸åŒæ•ˆç”¨ã€‚æœ¬ç ”ç©¶å‘ç°ï¼Œåœ¨LLMæ¨ç†èƒ½åŠ›è¾¹ç•Œé™„è¿‘çš„é—®é¢˜æä¾›çš„å­¦ä¹ æ•ˆç”¨è¿œå¤§äºç®€å•å’Œè¿‡äºå›°éš¾çš„é—®é¢˜ã€‚ä¸ºäº†è¯†åˆ«å’Œåˆ©ç”¨è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†HS-STaRï¼Œä¸€ç§ç”¨äºè‡ªæˆ‘è®­ç»ƒæ¨ç†è€…çš„åˆ†å±‚é‡‡æ ·æ¡†æ¶ã€‚ç»™å®šå›ºå®šçš„é‡‡æ ·é¢„ç®—ï¼ŒHS-STaRé¦–å…ˆä½¿ç”¨å¥–åŠ±æŒ‡å¯¼çš„éš¾åº¦ä¼°è®¡ç­–ç•¥è¿›è¡Œè½»é‡çº§é¢„é‡‡æ ·ï¼Œä»¥æœ‰æ•ˆåœ°è¯†åˆ«è¾¹ç•Œçº§åˆ«çš„é—®é¢˜ã€‚éšåï¼Œåœ¨é‡æ–°é‡‡æ ·é˜¶æ®µï¼Œå®ƒåŠ¨æ€åœ°å°†å‰©ä½™é¢„ç®—é‡æ–°åˆ†é…ç»™è¿™äº›é«˜æ•ˆç”¨é—®é¢˜ï¼Œæœ€å¤§é™åº¦åœ°ç”Ÿæˆæœ‰ä»·å€¼çš„è®­ç»ƒæ•°æ®ã€‚åœ¨å¤šä¸ªæ¨ç†åŸºå‡†æµ‹è¯•å’Œéª¨å¹²LLMsä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒHS-STaRåœ¨ä¸éœ€è¦é¢å¤–é‡‡æ ·é¢„ç®—çš„æƒ…å†µä¸‹æ˜¾è‘—ä¼˜äºå…¶ä»–åŸºå‡†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>STaRsé€šè¿‡è‡ªæˆ‘è®­ç»ƒå¢å¼ºLLMçš„æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚</li>
<li>ç°æœ‰ç ”ç©¶å¿½ç•¥ä¸åŒéš¾åº¦é—®é¢˜çš„ä¸åŒå­¦ä¹ æ•ˆç”¨ã€‚</li>
<li>è¾¹ç•Œçº§åˆ«çš„é—®é¢˜æä¾›æœ€å¤§çš„å­¦ä¹ æ•ˆç”¨ã€‚</li>
<li>HS-STaRæ¡†æ¶é€šè¿‡åˆ†å±‚é‡‡æ ·è¯†åˆ«å’Œåˆ©ç”¨è¾¹ç•Œçº§åˆ«é—®é¢˜ã€‚</li>
<li>HS-STaRåœ¨é¢„é‡‡æ ·é˜¶æ®µä½¿ç”¨å¥–åŠ±æŒ‡å¯¼çš„éš¾åº¦ä¼°è®¡ç­–ç•¥ã€‚</li>
<li>HS-STaRåŠ¨æ€è°ƒæ•´é‡‡æ ·é¢„ç®—ï¼Œä¼˜å…ˆé«˜æ•ˆç”¨é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19866">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-81d4208b174d419b43cfa5d5c0ad9057.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-10cd8b21b5242ca81dc321f2e6b9fd32.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d916c925af34a708b41cc8c9d818c6e0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b866e924be9b2e25e0d09c31b7ab881c.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="REA-RL-Reflection-Aware-Online-Reinforcement-Learning-for-Efficient-Large-Reasoning-Models"><a href="#REA-RL-Reflection-Aware-Online-Reinforcement-Learning-for-Efficient-Large-Reasoning-Models" class="headerlink" title="REA-RL: Reflection-Aware Online Reinforcement Learning for Efficient   Large Reasoning Models"></a>REA-RL: Reflection-Aware Online Reinforcement Learning for Efficient   Large Reasoning Models</h2><p><strong>Authors:Hexuan Deng, Wenxiang Jiao, Xuebo Liu, Jun Rao, Min Zhang</strong></p>
<p>Large Reasoning Models (LRMs) demonstrate strong performance in complex tasks but often face the challenge of overthinking, leading to substantially high inference costs. Existing approaches synthesize shorter reasoning responses for LRMs to learn, but are inefficient for online usage due to the time-consuming data generation and filtering processes. Meanwhile, online reinforcement learning mainly adopts a length reward to encourage short reasoning responses, but tends to lose the reflection ability and harm the performance. To address these issues, we propose REA-RL, which introduces a small reflection model for efficient scaling in online training, offering both parallel sampling and sequential revision. Besides, a reflection reward is designed to further prevent LRMs from favoring short yet non-reflective responses. Experiments show that both methods maintain or enhance performance while significantly improving inference efficiency. Their combination achieves a good balance between performance and efficiency, reducing inference costs by 35% without compromising performance. Further analysis demonstrates that our methods are effective by maintaining reflection frequency for hard problems while appropriately reducing it for simpler ones without losing reflection ability. Codes are available at <a target="_blank" rel="noopener" href="https://github.com/hexuandeng/REA-RL">https://github.com/hexuandeng/REA-RL</a>. </p>
<blockquote>
<p>å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰åœ¨å¤æ‚ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†å¸¸å¸¸é¢ä¸´è¿‡åº¦æ€è€ƒçš„æŒ‘æˆ˜ï¼Œå¯¼è‡´æ¨ç†æˆæœ¬æ˜¾è‘—æé«˜ã€‚ç°æœ‰æ–¹æ³•ä¸ºLRMsçš„åˆæˆè¾ƒçŸ­æ¨ç†å“åº”è€Œè¿›è¡Œå­¦ä¹ ï¼Œä½†ç”±äºè€—æ—¶è¾ƒé•¿çš„æ•°æ®ç”Ÿæˆå’Œè¿‡æ»¤è¿‡ç¨‹ï¼Œå› æ­¤åœ¨çº¿ä½¿ç”¨æ•ˆç‡è¾ƒä½ã€‚åŒæ—¶ï¼Œåœ¨çº¿å¼ºåŒ–å­¦ä¹ ä¸»è¦é‡‡ç”¨é•¿åº¦å¥–åŠ±æ¥é¼“åŠ±çŸ­æ¨ç†å“åº”ï¼Œä½†å®¹æ˜“ä¸§å¤±åæ€èƒ½åŠ›å¹¶æŸå®³æ€§èƒ½ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºREA-RLï¼Œå®ƒå¼•å…¥äº†ä¸€ä¸ªå°å‹åæ€æ¨¡å‹ï¼Œç”¨äºåœ¨çº¿åŸ¹è®­ä¸­çš„æœ‰æ•ˆæ‰©å±•ï¼Œæä¾›å¹¶è¡Œé‡‡æ ·å’Œé¡ºåºä¿®è®¢ã€‚æ­¤å¤–ï¼Œè®¾è®¡åæ€å¥–åŠ±æ˜¯ä¸ºäº†è¿›ä¸€æ­¥é˜²æ­¢LRMsåå‘äºçŸ­è€Œç¼ºä¹åæ€çš„å“åº”ã€‚å®éªŒè¡¨æ˜ï¼Œè¿™ä¸¤ç§æ–¹æ³•åœ¨ä¿æŒæˆ–æé«˜æ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—æé«˜æ¨ç†æ•ˆç‡ã€‚å®ƒä»¬çš„ç»“åˆåœ¨æ€§èƒ½å’Œæ•ˆç‡ä¹‹é—´è¾¾åˆ°äº†è‰¯å¥½çš„å¹³è¡¡ï¼Œåœ¨ä¸å½±å“æ€§èƒ½çš„æƒ…å†µä¸‹å°†æ¨ç†æˆæœ¬é™ä½äº†35%ã€‚è¿›ä¸€æ­¥åˆ†æè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡ä¿æŒå¯¹éš¾é¢˜çš„åæ€é¢‘ç‡ï¼ŒåŒæ—¶é€‚å½“å‡å°‘å¯¹ç®€å•é—®é¢˜çš„åæ€ï¼Œæ—¢ä¸å¤±åæ€èƒ½åŠ›ï¼Œåˆå¯æœ‰æ•ˆã€‚ç›¸å…³ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/hexuandeng/REA-RL%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/hexuandeng/REA-RLè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19862v1">PDF</a> Work in Progress</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æ¢è®¨äº†å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„é«˜æ€§èƒ½åŠæ‰€é¢ä¸´çš„è¿‡åº¦æ€è€ƒå¯¼è‡´çš„é«˜æ¨ç†æˆæœ¬é—®é¢˜ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•çš„ä¸è¶³ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆå°å‹åå°„æ¨¡å‹ï¼ˆREA-RLï¼‰çš„åœ¨çº¿å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œå®ç°äº†é«˜æ•ˆçš„åœ¨çº¿è®­ç»ƒå¹¶è¡Œé‡‡æ ·å’Œé¡ºåºä¿®è®¢åŠŸèƒ½ã€‚æ­¤å¤–ï¼Œè¿˜è®¾è®¡äº†åå°„å¥–åŠ±æ¥é˜²æ­¢LRMåå¥½çŸ­è€Œç¼ºä¹åæ€çš„å“åº”ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒæˆ–æé«˜æ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—æé«˜äº†æ¨ç†æ•ˆç‡ï¼Œé™ä½äº†æ¨ç†æˆæœ¬ã€‚è¿›ä¸€æ­¥çš„åˆ†ææ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•èƒ½æœ‰æ•ˆç»´æŒå¯¹éš¾é¢˜çš„åæ€é¢‘ç‡ï¼Œå¹¶é€‚å½“å‡å°‘å¯¹ç®€å•é—®é¢˜çš„åæ€èƒ½åŠ›ä¸ä¼šä¸§å¤±ã€‚ä»£ç å·²åœ¨GitHubä¸Šå…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰åœ¨å¤æ‚ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†é¢ä¸´è¿‡åº¦æ€è€ƒå’Œæ¨ç†æˆæœ¬é«˜æ˜‚çš„æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•é€šè¿‡åˆæˆæ›´çŸ­çš„æ¨ç†å“åº”æ¥è®©LRMså­¦ä¹ ï¼Œä½†è¿™ç§æ–¹æ³•å¯¹äºåœ¨çº¿ä½¿ç”¨æ¥è¯´æ•ˆç‡è¾ƒä½ï¼Œå› ä¸ºæ•°æ®ç”Ÿæˆå’Œè¿‡æ»¤è¿‡ç¨‹å¾ˆè€—æ—¶ã€‚</li>
<li>åœ¨çº¿å¼ºåŒ–å­¦ä¹ ä¸»è¦é€šè¿‡é•¿åº¦å¥–åŠ±æ¥é¼“åŠ±ç®€çŸ­çš„æ¨ç†å“åº”ï¼Œä½†è¿™ç§æ–¹æ³•å®¹æ˜“ä½¿æ¨¡å‹å¤±å»åæ€èƒ½åŠ›å¹¶å½±å“æ€§èƒ½ã€‚</li>
<li>REA-RLæ–¹æ³•ç»“åˆäº†å°å‹åå°„æ¨¡å‹ï¼Œå®ç°äº†åœ¨çº¿è®­ç»ƒä¸­çš„é«˜æ•ˆå¹¶è¡Œé‡‡æ ·å’Œé¡ºåºä¿®è®¢ã€‚</li>
<li>REA-RLå¼•å…¥äº†åå°„å¥–åŠ±ï¼Œæ—¨åœ¨é˜²æ­¢LRMåå¥½çŸ­è€Œç¼ºä¹åæ€çš„å“åº”ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒREA-RLæ–¹æ³•åœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶æ˜¾è‘—æé«˜äº†æ¨ç†æ•ˆç‡ï¼Œé™ä½äº†æ¨ç†æˆæœ¬ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19862">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ce0c0f6effc41c80af9b53fb8b965f91.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-72a17c05a3a6eb988ed18e4c743ce018.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1ce817ac390e4a129497f6be63a0b722.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Improving-Multilingual-Math-Reasoning-for-African-Languages"><a href="#Improving-Multilingual-Math-Reasoning-for-African-Languages" class="headerlink" title="Improving Multilingual Math Reasoning for African Languages"></a>Improving Multilingual Math Reasoning for African Languages</h2><p><strong>Authors:Odunayo Ogundepo, Akintunde Oladipo, Kelechi Ogueji, Esther Adenuga, David Ifeoluwa Adelani, Jimmy Lin</strong></p>
<p>Researchers working on low-resource languages face persistent challenges due to limited data availability and restricted access to computational resources. Although most large language models (LLMs) are predominantly trained in high-resource languages, adapting them to low-resource contexts, particularly African languages, requires specialized techniques. Several strategies have emerged for adapting models to low-resource languages in todays LLM landscape, defined by multi-stage pre-training and post-training paradigms. However, the most effective approaches remain uncertain. This work systematically investigates which adaptation strategies yield the best performance when extending existing LLMs to African languages. We conduct extensive experiments and ablation studies to evaluate different combinations of data types (translated versus synthetically generated), training stages (pre-training versus post-training), and other model adaptation configurations. Our experiments focuses on mathematical reasoning tasks, using the Llama 3.1 model family as our base model. </p>
<blockquote>
<p>ç ”ç©¶ä½èµ„æºè¯­è¨€çš„ç ”ç©¶äººå‘˜é¢ä¸´ç€æŒç»­ä¸æ–­çš„æŒ‘æˆ˜ï¼Œè¿™æ˜¯ç”±äºæ•°æ®æœ‰é™ä¸”è®¡ç®—èµ„æºè®¿é—®å—é™ã€‚å°½ç®¡å¤§å¤šæ•°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸»è¦åœ¨é«˜èµ„æºè¯­è¨€ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä½†å°†å®ƒä»¬é€‚åº”åˆ°ä½èµ„æºç¯å¢ƒï¼Œå°¤å…¶æ˜¯éæ´²è¯­è¨€ï¼Œéœ€è¦ä¸“é—¨çš„æŠ€æœ¯ã€‚åœ¨å½“ä»Šç”±å¤šé˜¶æ®µé¢„è®­ç»ƒå’Œåç»­è®­ç»ƒèŒƒå¼å®šä¹‰çš„LLMæ™¯è§‚ä¸­ï¼Œå·²ç»å‡ºç°äº†å‡ ç§é€‚åº”æ¨¡å‹ä»¥é€‚åº”ä½èµ„æºè¯­è¨€çš„ç­–ç•¥ã€‚ç„¶è€Œï¼Œæœ€æœ‰æ•ˆçš„ç­–ç•¥ä»ä¸ç¡®å®šã€‚è¿™é¡¹å·¥ä½œç³»ç»Ÿåœ°ç ”ç©¶äº†åœ¨å°†ç°æœ‰LLMæ‰©å±•åˆ°éæ´²è¯­è¨€æ—¶ï¼Œå“ªäº›é€‚åº”ç­–ç•¥èƒ½äº§ç”Ÿæœ€ä½³æ€§èƒ½ã€‚æˆ‘ä»¬è¿›è¡Œäº†å¹¿æ³›çš„å®éªŒå’Œå‰”é™¤ç ”ç©¶ï¼Œä»¥è¯„ä¼°æ•°æ®ç±»å‹ï¼ˆç¿»è¯‘ä¸åˆæˆç”Ÿæˆï¼‰ã€è®­ç»ƒé˜¶æ®µï¼ˆé¢„è®­ç»ƒä¸åç»­è®­ç»ƒï¼‰å’Œå…¶ä»–æ¨¡å‹é€‚åº”é…ç½®çš„ä¸åŒç»„åˆã€‚æˆ‘ä»¬çš„å®éªŒä¾§é‡äºæ•°å­¦æ¨ç†ä»»åŠ¡ï¼Œä»¥Llama 3.1æ¨¡å‹å®¶æ—ä½œä¸ºæˆ‘ä»¬çš„åŸºç¡€æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19848v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>é’ˆå¯¹ä½èµ„æºè¯­è¨€ï¼Œç ”ç©¶äººå‘˜é¢ä¸´æ•°æ®æœ‰é™å’Œè®¡ç®—èµ„æºè®¿é—®å—é™çš„æŒç»­æŒ‘æˆ˜ã€‚å°½ç®¡å¤§å¤šæ•°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸»è¦åœ¨é«˜èµ„æºè¯­è¨€ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä½†å°†å®ƒä»¬é€‚åº”åˆ°ä½èµ„æºç¯å¢ƒï¼Œå°¤å…¶æ˜¯éæ´²è¯­è¨€ï¼Œéœ€è¦ç‰¹æ®ŠæŠ€æœ¯ã€‚æœ¬ç ”ç©¶ç³»ç»Ÿåœ°æ¢è®¨äº†å°†ç°æœ‰LLMæ‰©å±•åˆ°éæ´²è¯­è¨€æ—¶ï¼Œå“ªäº›é€‚åº”ç­–ç•¥èƒ½å¸¦æ¥æœ€ä½³æ€§èƒ½ã€‚æˆ‘ä»¬è¿›è¡Œäº†å¹¿æ³›çš„å®éªŒå’Œå‰”é™¤ç ”ç©¶ï¼Œä»¥è¯„ä¼°æ•°æ®ç±»å‹ï¼ˆç¿»è¯‘ä¸åˆæˆç”Ÿæˆï¼‰ã€è®­ç»ƒé˜¶æ®µï¼ˆé¢„è®­ç»ƒä¸åè®­ç»ƒï¼‰å’Œå…¶ä»–æ¨¡å‹é€‚åº”é…ç½®çš„ä¸åŒç»„åˆã€‚æˆ‘ä»¬çš„å®éªŒé‡ç‚¹æ˜¯é€šè¿‡æ•°å­¦æ¨ç†ä»»åŠ¡ï¼Œä»¥Llama 3.1æ¨¡å‹å®¶æ—ä½œä¸ºåŸºå‡†æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä½èµ„æºè¯­è¨€çš„ç ”ç©¶æŒ‘æˆ˜åœ¨äºæ•°æ®æœ‰é™å’Œè®¡ç®—èµ„æºå—é™ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸»è¦åœ¨é«˜èµ„æºè¯­è¨€ä¸Šè¿›è¡Œè®­ç»ƒã€‚</li>
<li>é€‚åº”ä½èµ„æºç¯å¢ƒï¼Œå°¤å…¶æ˜¯éæ´²è¯­è¨€ï¼Œéœ€è¦ç‰¹æ®ŠæŠ€æœ¯ã€‚</li>
<li>ç›®å‰æœ‰å¤šç§ç­–ç•¥ç”¨äºå°†LLMé€‚åº”åˆ°ä½èµ„æºè¯­è¨€ã€‚</li>
<li>æœ€æœ‰æ•ˆçš„é€‚åº”ç­–ç•¥å°šä¸ç¡®å®šã€‚</li>
<li>ç ”ç©¶é€šè¿‡æ•°å­¦æ¨ç†ä»»åŠ¡ç³»ç»Ÿåœ°æ¢ç´¢äº†ä¸åŒçš„é€‚åº”ç­–ç•¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19848">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0fc4db7213297d865529d6ae86347d07.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7ab23ba7281f0aced889ee5d3fe8b5df.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0107c553c2a16762d5ac7bcec7045ff3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a34f2189bb78f78fa8e5141628eec251.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="What-Can-RL-Bring-to-VLA-Generalization-An-Empirical-Study"><a href="#What-Can-RL-Bring-to-VLA-Generalization-An-Empirical-Study" class="headerlink" title="What Can RL Bring to VLA Generalization? An Empirical Study"></a>What Can RL Bring to VLA Generalization? An Empirical Study</h2><p><strong>Authors:Jijia Liu, Feng Gao, Bingwen Wei, Xinlei Chen, Qingmin Liao, Yi Wu, Chao Yu, Yu Wang</strong></p>
<p>Large Vision-Language Action (VLA) models have shown significant potential for embodied AI. However, their predominant training via supervised fine-tuning (SFT) limits generalization due to susceptibility to compounding errors under distribution shifts. Reinforcement learning (RL) offers a path to overcome these limitations by optimizing for task objectives via trial-and-error, yet a systematic understanding of its specific generalization benefits for VLAs compared to SFT is lacking. To address this, our study introduces a comprehensive benchmark for evaluating VLA generalization and systematically investigates the impact of RL fine-tuning across diverse visual, semantic, and execution dimensions. Our extensive experiments reveal that RL fine-tuning, particularly with PPO, significantly enhances generalization in semantic understanding and execution robustness over SFT, while maintaining comparable visual robustness. We identify PPO as a more effective RL algorithm for VLAs than LLM-derived methods like DPO and GRPO. We also develop a simple recipe for efficient PPO training on VLAs, and demonstrate its practical utility for improving VLA generalization. The project page is at <a target="_blank" rel="noopener" href="https://rlvla.github.io/">https://rlvla.github.io</a> </p>
<blockquote>
<p>å¤§å‹è§†è§‰è¯­è¨€è¡ŒåŠ¨ï¼ˆVLAï¼‰æ¨¡å‹åœ¨åµŒå…¥å¼äººå·¥æ™ºèƒ½é¢†åŸŸè¡¨ç°å‡ºäº†å·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œå®ƒä»¬ä¸»è¦é€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰è¿›è¡Œè®­ç»ƒï¼Œè¿™åœ¨åˆ†å¸ƒè½¬ç§»ä¸‹å®¹æ˜“å—åˆ°å¤åˆé”™è¯¯çš„å½±å“ï¼Œä»è€Œé™åˆ¶äº†å…¶æ³›åŒ–èƒ½åŠ›ã€‚å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰é€šè¿‡è¯•é”™æ¥ä¼˜åŒ–ä»»åŠ¡ç›®æ ‡ï¼Œæä¾›äº†å…‹æœè¿™äº›é™åˆ¶çš„é€”å¾„ï¼Œä½†å…³äºä¸SFTç›¸æ¯”ï¼Œå…¶åœ¨VLAç‰¹å®šæ³›åŒ–æ–¹é¢çš„å…·ä½“ä¼˜åŠ¿ç¼ºä¹ç³»ç»Ÿäº†è§£ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬çš„ç ”ç©¶å¼•å…¥äº†ä¸€ä¸ªç»¼åˆåŸºå‡†æµ‹è¯•ï¼Œä»¥è¯„ä¼°VLAçš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶ç³»ç»Ÿåœ°ç ”ç©¶äº†RLå¾®è°ƒåœ¨ä¸åŒè§†è§‰ã€è¯­ä¹‰å’Œæ‰§è¡Œç»´åº¦çš„å½±å“ã€‚æˆ‘ä»¬çš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œç‰¹åˆ«æ˜¯ä½¿ç”¨PPOçš„RLå¾®è°ƒåœ¨è¯­ä¹‰ç†è§£å’Œæ‰§è¡Œç¨³å¥æ€§æ–¹é¢æ˜¾è‘—æé«˜äº†æ³›åŒ–èƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒäº†ä¸SFTç›¸å½“çš„è§†è§‰ç¨³å¥æ€§ã€‚æˆ‘ä»¬ç¡®å®šPPOå¯¹äºVLAæ¥è¯´ï¼Œæ¯”LLMè¡ç”Ÿçš„æ–¹æ³•ï¼ˆå¦‚DPOå’ŒGRPOï¼‰æ›´ä¸ºæœ‰æ•ˆçš„RLç®—æ³•ã€‚æˆ‘ä»¬è¿˜ä¸ºVLAä¸Šçš„é«˜æ•ˆPPOè®­ç»ƒå¼€å‘äº†ä¸€ä¸ªç®€å•çš„é…æ–¹ï¼Œå¹¶å±•ç¤ºäº†å…¶æé«˜VLAæ³›åŒ–çš„å®ç”¨æ€§ã€‚é¡¹ç›®é¡µé¢ä¸ºï¼š<a target="_blank" rel="noopener" href="https://rlvla.github.io/">https://rlvla.github.io</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19789v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è§†è§‰è¯­è¨€è¡ŒåŠ¨ï¼ˆVLAï¼‰æ¨¡å‹åœ¨åµŒå…¥å¼äººå·¥æ™ºèƒ½é¢†åŸŸå±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†ä¸»è¦é€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰è¿›è¡Œè®­ç»ƒçš„æ–¹å¼é™åˆ¶äº†å…¶æ³›åŒ–èƒ½åŠ›ï¼Œå®¹æ˜“åœ¨åˆ†å¸ƒè½¬ç§»æ—¶å‡ºç°ç´¯ç§¯é”™è¯¯ã€‚å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¯•å›¾é€šè¿‡è¯•é”™ä¼˜åŒ–ä»»åŠ¡ç›®æ ‡æ¥å…‹æœè¿™äº›å±€é™ã€‚æœ¬ç ”ç©¶ä¸ºè¯„ä¼°VLAæ³›åŒ–èƒ½åŠ›å¼•å…¥äº†ç»¼åˆåŸºå‡†æµ‹è¯•ï¼Œå¹¶ç³»ç»Ÿåœ°ç ”ç©¶äº†RLå¾®è°ƒåœ¨ä¸åŒè§†è§‰ã€è¯­ä¹‰å’Œæ‰§è¡Œç»´åº¦çš„å½±å“ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRLå¾®è°ƒï¼Œå°¤å…¶æ˜¯ä½¿ç”¨PPOï¼Œåœ¨è¯­ä¹‰ç†è§£å’Œæ‰§è¡Œç¨³å¥æ€§æ–¹é¢æ˜¾è‘—æé«˜äº†æ³›åŒ–èƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒç›¸å½“çš„è§†è§‰ç¨³å¥æ€§ã€‚æœ¬ç ”ç©¶è®¤ä¸ºPPOå¯¹äºVLAè€Œè¨€ï¼Œç›¸æ¯”LLMè¡ç”Ÿæ–¹æ³•å¦‚DPOå’ŒGRPOæ›´ä¸ºæœ‰æ•ˆã€‚æ­¤å¤–ï¼Œæœ¬ç ”ç©¶è¿˜å¼€å‘äº†é’ˆå¯¹VLAçš„PPOè®­ç»ƒç®€æ˜“é…æ–¹ï¼Œå¹¶éªŒè¯äº†å…¶åœ¨æé«˜VLAæ³›åŒ–æ–¹é¢çš„å®ç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VLAæ¨¡å‹åœ¨åµŒå…¥å¼AIé¢†åŸŸæ½œåŠ›å·¨å¤§ï¼Œä½†ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰è®­ç»ƒé™åˆ¶äº†å…¶æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰èƒ½æé«˜VLAæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯é€šè¿‡è¯•é”™ä¼˜åŒ–ã€‚</li>
<li>PPOç›¸æ¯”LLMè¡ç”Ÿæ–¹æ³•å¦‚DPOå’ŒGRPOæ›´æœ‰æ•ˆäºæé«˜VLAæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>ç ”ç©¶è¡¨æ˜RLå¾®è°ƒèƒ½å¢å¼ºVLAæ¨¡å‹åœ¨è¯­ä¹‰ç†è§£å’Œæ‰§è¡Œç¨³å¥æ€§æ–¹é¢çš„è¡¨ç°ã€‚</li>
<li>VLAæ¨¡å‹çš„è§†è§‰ç¨³å¥æ€§åœ¨RLå¾®è°ƒåå¾—ä»¥ä¿æŒã€‚</li>
<li>æœ¬ç ”ç©¶ä¸ºVLAæ¨¡å‹æä¾›äº†PPOè®­ç»ƒçš„ç®€æ˜“é…æ–¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19789">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-58fe3f3ec5a10924554c5b4da4eecbec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2478ad81e7317a495bf2ad16924de14c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4cf23bd938f7b7a653c854361c7136e0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dba0cd3e713a80ecf10417414813ef96.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Divide-and-Conquer-Grounding-LLMs-as-Efficient-Decision-Making-Agents-via-Offline-Hierarchical-Reinforcement-Learning"><a href="#Divide-and-Conquer-Grounding-LLMs-as-Efficient-Decision-Making-Agents-via-Offline-Hierarchical-Reinforcement-Learning" class="headerlink" title="Divide and Conquer: Grounding LLMs as Efficient Decision-Making Agents   via Offline Hierarchical Reinforcement Learning"></a>Divide and Conquer: Grounding LLMs as Efficient Decision-Making Agents   via Offline Hierarchical Reinforcement Learning</h2><p><strong>Authors:Zican Hu, Wei Liu, Xiaoye Qu, Xiangyu Yue, Chunlin Chen, Zhi Wang, Yu Cheng</strong></p>
<p>While showing sophisticated reasoning abilities, large language models (LLMs) still struggle with long-horizon decision-making tasks due to deficient exploration and long-term credit assignment, especially in sparse-reward scenarios. Inspired by the divide-and-conquer principle, we propose an innovative framework <strong>GLIDER</strong> (<strong>G</strong>rounding <strong>L</strong>anguage Models as Eff<strong>I</strong>cient <strong>D</strong>ecision-Making Agents via Offline Hi<strong>E</strong>rarchical <strong>R</strong>einforcement Learning) that introduces a parameter-efficient and generally applicable hierarchy to LLM policies. We develop a scheme where the low-level controller is supervised with abstract, step-by-step plans that are learned and instructed by the high-level policy. This design decomposes complicated problems into a series of coherent chain-of-thought reasoning sub-tasks, providing flexible temporal abstraction to significantly enhance exploration and learning for long-horizon tasks. Furthermore, GLIDER facilitates fast online adaptation to non-stationary environments owing to the strong transferability of its task-agnostic low-level skills. Experiments on ScienceWorld and ALFWorld benchmarks show that GLIDER achieves consistent performance gains, along with enhanced generalization capabilities. </p>
<blockquote>
<p>åœ¨å±•ç°å¤æ‚çš„æ¨ç†èƒ½åŠ›çš„åŒæ—¶ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»ç„¶é¢ä¸´ç€é•¿æœŸå†³ç­–ä»»åŠ¡çš„æŒ‘æˆ˜ï¼Œè¿™æ˜¯ç”±äºç¼ºä¹æ¢ç´¢å’Œé•¿æœŸä¿¡ç”¨åˆ†é…ï¼Œç‰¹åˆ«æ˜¯åœ¨ç¨€ç–å¥–åŠ±åœºæ™¯ä¸­ã€‚å—â€œåˆ†è€Œæ²»ä¹‹â€åŸåˆ™çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ›æ–°çš„æ¡†æ¶â€”â€”GLIDERï¼ˆé€šè¿‡ç¦»çº¿åˆ†å±‚å¼ºåŒ–å­¦ä¹ å°†è¯­è¨€æ¨¡å‹ä½œä¸ºé«˜æ•ˆå†³ç­–ä»£ç†ï¼‰ã€‚å®ƒå¼•å…¥äº†å¯¹LLMç­–ç•¥å…·æœ‰æ™®éé€‚ç”¨æ€§å’Œå‚æ•°æ•ˆç‡çš„åˆ†å±‚æ¬¡ç»“æ„ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ç§æ–¹æ¡ˆï¼Œä½çº§æ§åˆ¶å™¨å—åˆ°æŠ½è±¡ã€åˆ†æ­¥è®¡åˆ’çš„ç›‘ç£ï¼Œè¿™äº›è®¡åˆ’ç”±é«˜çº§ç­–ç•¥å­¦ä¹ å’ŒæŒ‡å¯¼ã€‚è¿™ç§è®¾è®¡å°†å¤æ‚çš„é—®é¢˜åˆ†è§£æˆä¸€ç³»åˆ—è¿è´¯çš„é“¾å¼æ€ç»´æ¨ç†å­ä»»åŠ¡ï¼Œæä¾›çµæ´»çš„æ—¶ç©ºæŠ½è±¡ï¼Œä»¥æ˜¾è‘—å¢å¼ºé•¿æœŸä»»åŠ¡çš„æ¢ç´¢å’Œå­¦ä¹ ã€‚æ­¤å¤–ï¼ŒGLIDERç”±äºå…¶ä»»åŠ¡æ— å…³çš„åº•å±‚æŠ€èƒ½çš„å¼ºå¤§å¯è¿ç§»æ€§ï¼Œèƒ½å¤Ÿè¿…é€Ÿé€‚åº”éé™æ€ç¯å¢ƒã€‚åœ¨ScienceWorldå’ŒALFWorldåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒGLIDERå®ç°äº†æ€§èƒ½çš„ä¸€è‡´æ€§æå‡ï¼Œå¹¶å¢å¼ºäº†æ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19761v1">PDF</a> Accepted by ICML 2025, 21 pages</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å†³ç­–åˆ¶å®šä»»åŠ¡ä¸­å±•ç°å‡ºå¤æ‚çš„æ¨ç†èƒ½åŠ›ï¼Œä½†åœ¨é•¿æœŸå†³ç­–ä¸­ä»å­˜åœ¨æ¢ç´¢ä¸è¶³å’Œé•¿æœŸä¿¡ç”¨åˆ†é…çš„é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å¥–åŠ±ç¨€ç–çš„åœºæ™¯ä¸‹ã€‚å—â€œåˆ†è€Œæ²»ä¹‹â€åŸåˆ™çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåˆ›æ–°çš„æ¡†æ¶GLIDERï¼Œå®ƒé€šè¿‡ç¦»çº¿åˆ†å±‚å¼ºåŒ–å­¦ä¹ å°†LLMä½œä¸ºé«˜æ•ˆå†³ç­–ä»£ç†ã€‚GLIDERå¼•å…¥äº†ä¸€ç§å¯¹LLMç­–ç•¥é€šç”¨çš„ã€å‚æ•°æ•ˆç‡é«˜çš„å±‚æ¬¡ç»“æ„ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ç§ä½çº§æ§åˆ¶å™¨å—é«˜çº§ç­–ç•¥å­¦ä¹ å’ŒæŒ‡å¯¼çš„æŠ½è±¡ã€åˆ†æ­¥è®¡åˆ’çš„ç›‘ç£çš„æ–¹æ¡ˆã€‚è¿™ç§è®¾è®¡å°†å¤æ‚çš„é—®é¢˜åˆ†è§£æˆä¸€ç³»åˆ—è¿è´¯çš„é“¾å¼æ€ç»´æ¨ç†å­ä»»åŠ¡ï¼Œä¸ºé•¿æœŸä»»åŠ¡æä¾›çµæ´»çš„æ—¶é—´æŠ½è±¡ï¼Œæ˜¾è‘—æé«˜äº†æ¢ç´¢å’Œå­¦ä¹ èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒGLIDERè¿˜ä¾¿äºå¿«é€Ÿé€‚åº”éç¨³å®šç¯å¢ƒï¼Œå…¶ä»»åŠ¡æ— å…³çš„ä½çº§æŠ€èƒ½å…·æœ‰å¾ˆå¼ºçš„å¯è½¬ç§»æ€§ã€‚åœ¨ScienceWorldå’ŒALFWorldåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒGLIDERåœ¨æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ä¸Šå‡æœ‰æ‰€æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å†³ç­–åˆ¶å®šä»»åŠ¡ä¸­å­˜åœ¨é•¿æœŸå†³ç­–çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯é¢å¯¹ç¨€ç–å¥–åŠ±åœºæ™¯æ—¶å­˜åœ¨æ¢ç´¢ä¸è¶³å’Œé•¿æœŸä¿¡ç”¨åˆ†é…é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶GLIDERï¼Œç»“åˆè¯­è¨€æ¨¡å‹å’Œå¼ºåŒ–å­¦ä¹ è¿›è¡Œå†³ç­–åˆ¶å®šï¼Œä»¥æé«˜é•¿æœŸä»»åŠ¡çš„æ¢ç´¢å’Œå­¦ä¹ æ•ˆç‡ã€‚</li>
<li>GLIDERæ¡†æ¶å¼•å…¥äº†å±‚æ¬¡ç»“æ„ï¼Œå°†å¤æ‚é—®é¢˜åˆ†è§£ä¸ºä¸€ç³»åˆ—è¿è´¯çš„é“¾å¼æ€ç»´æ¨ç†å­ä»»åŠ¡ï¼Œå¢å¼ºäº†æ¨¡å‹çš„çµæ´»æ€§ã€‚</li>
<li>GLIDERé€šè¿‡å‚æ•°é«˜æ•ˆçš„æ–¹æ¡ˆå®ç°äº†ä»»åŠ¡æ— å…³çš„ä½çº§æŠ€èƒ½ï¼Œå…·æœ‰è‰¯å¥½çš„å¯è½¬ç§»æ€§ï¼Œèƒ½å¿«é€Ÿé€‚åº”éç¨³å®šç¯å¢ƒã€‚</li>
<li>åœ¨ScienceWorldå’ŒALFWorldåŸºå‡†æµ‹è¯•ä¸Šï¼ŒGLIDERå±•ç°äº†æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›çš„æå‡ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19761">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a2fa000c33cda83395d18e76af4f6ef1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9bcd953fb8217d52ce1dc2e4bc877dc8.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="MT-3-Scaling-MLLM-based-Text-Image-Machine-Translation-via-Multi-Task-Reinforcement-Learning"><a href="#MT-3-Scaling-MLLM-based-Text-Image-Machine-Translation-via-Multi-Task-Reinforcement-Learning" class="headerlink" title="MT$^{3}$: Scaling MLLM-based Text Image Machine Translation via   Multi-Task Reinforcement Learning"></a>MT$^{3}$: Scaling MLLM-based Text Image Machine Translation via   Multi-Task Reinforcement Learning</h2><p><strong>Authors:Zhaopeng Feng, Yupu Liang, Shaosheng Cao, Jiayuan Su, Jiahan Ren, Zhe Xu, Yao Hu, Wenxuan Huang, Jian Wu, Zuozhu Liu</strong></p>
<p>Text Image Machine Translation (TIMT)-the task of translating textual content embedded in images-is critical for applications in accessibility, cross-lingual information access, and real-world document understanding. However, TIMT remains a complex challenge due to the need for accurate optical character recognition (OCR), robust visual-text reasoning, and high-quality translation, often requiring cascading multi-stage pipelines. Recent advances in large-scale Reinforcement Learning (RL) have improved reasoning in Large Language Models (LLMs) and Multimodal LLMs (MLLMs), but their application to end-to-end TIMT is still underexplored. To bridge this gap, we introduce MT$^{3}$, the first framework to apply Multi-Task RL to MLLMs for end-to-end TIMT. MT$^{3}$ adopts a multi-task optimization paradigm targeting three key sub-skills: text recognition, context-aware reasoning, and translation. It is trained using a novel multi-mixed reward mechanism that adapts rule-based RL strategies to TIMTâ€™s intricacies, offering fine-grained, non-binary feedback across tasks. Furthermore, to facilitate the evaluation of TIMT in authentic cross-cultural and real-world social media contexts, we introduced XHSPost, the first social media TIMT benchmark. Our MT$^{3}$-7B-Zero achieves state-of-the-art results on the latest in-domain MIT-10M benchmark, outperforming strong baselines such as Qwen2.5-VL-72B and InternVL2.5-78B by notable margins across multiple metrics. Additionally, the model shows strong generalization to out-of-distribution language pairs and datasets. In-depth analyses reveal how multi-task synergy, reinforcement learning initialization, curriculum design, and reward formulation contribute to advancing MLLM-driven TIMT. </p>
<blockquote>
<p>æ–‡æœ¬å›¾åƒæœºå™¨ç¿»è¯‘ï¼ˆTIMTï¼‰â€”â€”å°†åµŒå…¥å›¾åƒä¸­çš„æ–‡æœ¬å†…å®¹è¿›è¡Œç¿»è¯‘çš„ä»»åŠ¡â€”â€”å¯¹äºæ— éšœç¢åº”ç”¨ã€è·¨è¯­è¨€ä¿¡æ¯è®¿é—®å’Œç°å®ä¸–ç•Œæ–‡æ¡£ç†è§£ç­‰åº”ç”¨è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç”±äºéœ€è¦å‡†ç¡®çš„å…‰å­¦å­—ç¬¦è¯†åˆ«ï¼ˆOCRï¼‰ã€ç¨³å¥çš„è§†è§‰æ–‡æœ¬æ¨ç†å’Œé«˜è´¨é‡çš„ç¿»è¯‘ï¼Œé€šå¸¸æ¶‰åŠçº§è”çš„å¤šé˜¶æ®µç®¡é“ï¼ŒTIMTä»ç„¶æ˜¯ä¸€ä¸ªå¤æ‚çš„æŒ‘æˆ˜ã€‚æœ€è¿‘å¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„è¿›å±•æé«˜äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œå¤šæ¨¡æ€LLMï¼ˆMLLMsï¼‰ä¸­çš„æ¨ç†èƒ½åŠ›ï¼Œä½†å…¶åœ¨ç«¯åˆ°ç«¯TIMTä¸­çš„åº”ç”¨ä»ç„¶è¢«æ¢ç´¢ä¸è¶³ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†MT$^{3}$ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå°†å¤šä»»åŠ¡RLåº”ç”¨äºMLLMsè¿›è¡Œç«¯åˆ°ç«¯TIMTçš„æ¡†æ¶ã€‚MT$^{3}$é‡‡ç”¨äº†ä¸€ç§å¤šä»»åŠ¡ä¼˜åŒ–èŒƒå¼ï¼Œé’ˆå¯¹æ–‡æœ¬è¯†åˆ«ã€ä¸Šä¸‹æ–‡æ„ŸçŸ¥æ¨ç†å’Œç¿»è¯‘ä¸‰ä¸ªå…³é”®å­æŠ€èƒ½ã€‚å®ƒä½¿ç”¨ä¸€ç§æ–°å‹çš„å¤šæ··åˆå¥–åŠ±æœºåˆ¶è¿›è¡Œè®­ç»ƒï¼Œè¯¥æœºåˆ¶å°†åŸºäºè§„åˆ™çš„RLç­–ç•¥é€‚åº”åˆ°TIMTçš„å¤æ‚æ€§ä¸Šï¼Œå¹¶åœ¨ä»»åŠ¡ä¹‹é—´æä¾›ç²¾ç»†çš„ã€éäºŒè¿›åˆ¶çš„åé¦ˆã€‚æ­¤å¤–ï¼Œä¸ºäº†ä¿ƒè¿›åœ¨çœŸå®è·¨æ–‡åŒ–å’Œç°å®ä¸–ç•Œç¤¾äº¤åª’ä½“èƒŒæ™¯ä¸‹å¯¹TIMTçš„è¯„ä¼°ï¼Œæˆ‘ä»¬æ¨å‡ºäº†XHSPostï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç¤¾äº¤åª’ä½“TIMTåŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬çš„MT$^{3}$-7B-Zeroåœ¨æœ€æ–°çš„é¢†åŸŸMIT-10MåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€æ–°ç»“æœï¼Œä¸Qwen2.5-VL-72Bå’ŒInternVL2.5-78Bç­‰å¼ºå¤§åŸºå‡†ç›¸æ¯”ï¼Œåœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹å¯¹è¶…å‡ºåˆ†å¸ƒçš„è¯­è¨€å¯¹å’Œæ•°æ®é›†è¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚æ·±å…¥åˆ†æè¡¨æ˜ï¼Œå¤šä»»åŠ¡ååŒã€å¼ºåŒ–å­¦ä¹ åˆå§‹åŒ–ã€è¯¾ç¨‹è®¾è®¡ä»¥åŠå¥–åŠ±å…¬å¼ç­‰å¯¹æ¨åŠ¨MLLMé©±åŠ¨çš„TIMTè‡³å…³é‡è¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19714v1">PDF</a> Work in progress</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æ–‡æœ¬å›¾åƒæœºå™¨ç¿»è¯‘ï¼ˆTIMTï¼‰çš„é‡è¦æ€§åŠå…¶é¢ä¸´çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬å‡†ç¡®çš„å…‰å­¦å­—ç¬¦è¯†åˆ«ï¼ˆOCRï¼‰ã€ç¨³å¥çš„è§†è§‰æ–‡æœ¬æ¨ç†å’Œé«˜å“è´¨ç¿»è¯‘çš„éœ€æ±‚ã€‚é’ˆå¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†MT^3æ¡†æ¶ï¼Œé¦–æ¬¡å°†å¤šä»»åŠ¡å¼ºåŒ–å­¦ä¹ åº”ç”¨äºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰è¿›è¡Œç«¯åˆ°ç«¯çš„TIMTã€‚MT^3é‡‡ç”¨å¤šä»»åŠ¡ä¼˜åŒ–èŒƒå¼ï¼Œé’ˆå¯¹æ–‡æœ¬è¯†åˆ«ã€ä¸Šä¸‹æ–‡æ„ŸçŸ¥æ¨ç†å’Œç¿»è¯‘ä¸‰ä¸ªå…³é”®å­æŠ€èƒ½è¿›è¡Œè®­ç»ƒã€‚ä½¿ç”¨æ–°å‹çš„å¤šæ··åˆå¥–åŠ±æœºåˆ¶ï¼Œé€‚åº”åŸºäºè§„åˆ™çš„RLç­–ç•¥ä»¥é€‚åº”TIMTçš„å¤æ‚æ€§ï¼Œæä¾›è·¨ä»»åŠ¡çš„ç²¾ç»†ç²’åº¦éäºŒè¿›åˆ¶åé¦ˆã€‚æ­¤å¤–ï¼Œä¸ºäº†è¯„ä¼°TIMTåœ¨çœŸå®è·¨æ–‡åŒ–å’Œç¤¾ä¼šåª’ä½“èƒŒæ™¯ä¸‹çš„æ€§èƒ½ï¼Œå¼•å…¥äº†XHSPostç¤¾äº¤åª’ä½“TIMTåŸºå‡†æµ‹è¯•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMT^3-7B-Zeroåœ¨æœ€æ–°çš„MIT-10MåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€æ–°çš„æŠ€æœ¯æ°´å¹³ï¼Œå¹¶åœ¨å¤šç§æŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äºQwen2.5-VL-72Bå’ŒInternVL2.5-78Bç­‰å¼ºåŸºçº¿ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åœ¨è·¨è¯­è¨€æ•°æ®é›†ä¸Šæ˜¾ç¤ºå‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚æ·±å…¥åˆ†ææ­ç¤ºäº†å¤šä»»åŠ¡ååŒã€å¼ºåŒ–å­¦ä¹ åˆå§‹åŒ–ã€è¯¾ç¨‹è®¾è®¡å’Œå¥–åŠ±åˆ¶å®šå¯¹æ¨åŠ¨MLLMé©±åŠ¨çš„TIMTçš„è´¡çŒ®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬å›¾åƒæœºå™¨ç¿»è¯‘ï¼ˆTIMTï¼‰æ˜¯ä¿¡æ¯è®¿é—®å’Œæ–‡æ¡£ç†è§£çš„å…³é”®æŠ€æœ¯ï¼Œä½†ä»é¢ä¸´å¤æ‚æŒ‘æˆ˜ã€‚</li>
<li>MT^3æ¡†æ¶é¦–æ¬¡å°†å¤šä»»åŠ¡å¼ºåŒ–å­¦ä¹ åº”ç”¨äºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰è¿›è¡Œç«¯åˆ°ç«¯çš„TIMTã€‚</li>
<li>MT^3é€šè¿‡å¤šä»»åŠ¡ä¼˜åŒ–èŒƒå¼ï¼Œé’ˆå¯¹æ–‡æœ¬è¯†åˆ«ã€ä¸Šä¸‹æ–‡æ„ŸçŸ¥æ¨ç†å’Œç¿»è¯‘ä¸‰ä¸ªå…³é”®å­æŠ€èƒ½è¿›è¡Œè®­ç»ƒã€‚</li>
<li>å¼•å…¥æ–°å‹çš„å¤šæ··åˆå¥–åŠ±æœºåˆ¶ä»¥é€‚åº”åŸºäºè§„åˆ™çš„RLç­–ç•¥ï¼Œæä¾›è·¨ä»»åŠ¡çš„ç²¾ç»†åé¦ˆã€‚</li>
<li>XHSPostç¤¾äº¤åª’ä½“TIMTåŸºå‡†æµ‹è¯•ç”¨äºè¯„ä¼°TIMTåœ¨çœŸå®è·¨æ–‡åŒ–å’Œç¤¾ä¼šåª’ä½“èƒŒæ™¯ä¸‹çš„æ€§èƒ½ã€‚</li>
<li>MT^3æ¨¡å‹åœ¨MIT-10MåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—ä¼˜äºå…¶ä»–åŸºçº¿æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19714">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c352fa0445f14944c812cdbc0a65d8a4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b038dc88549d04236e94c3910ec97109.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-686f51f5a4b71e473c7ac974fb376f57.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="CAD-Coder-Text-to-CAD-Generation-with-Chain-of-Thought-and-Geometric-Reward"><a href="#CAD-Coder-Text-to-CAD-Generation-with-Chain-of-Thought-and-Geometric-Reward" class="headerlink" title="CAD-Coder: Text-to-CAD Generation with Chain-of-Thought and Geometric   Reward"></a>CAD-Coder: Text-to-CAD Generation with Chain-of-Thought and Geometric   Reward</h2><p><strong>Authors:Yandong Guan, Xilin Wang, Xingxi Ming, Jing Zhang, Dong Xu, Qian Yu</strong></p>
<p>In this work, we introduce CAD-Coder, a novel framework that reformulates text-to-CAD as the generation of CadQuery scripts - a Python-based, parametric CAD language. This representation enables direct geometric validation, a richer modeling vocabulary, and seamless integration with existing LLMs. To further enhance code validity and geometric fidelity, we propose a two-stage learning pipeline: (1) supervised fine-tuning on paired text-CadQuery data, and (2) reinforcement learning with Group Reward Policy Optimization (GRPO), guided by a CAD-specific reward comprising both a geometric reward (Chamfer Distance) and a format reward. We also introduce a chain-of-thought (CoT) planning process to improve model reasoning, and construct a large-scale, high-quality dataset of 110K text-CadQuery-3D model triplets and 1.5K CoT samples via an automated pipeline. Extensive experiments demonstrate that CAD-Coder enables LLMs to generate diverse, valid, and complex CAD models directly from natural language, advancing the state of the art of text-to-CAD generation and geometric reasoning. </p>
<blockquote>
<p>åœ¨æœ¬æ¬¡å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†CAD-Coderè¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œå®ƒå°†æ–‡æœ¬åˆ°CADçš„è½¬æ¢é‡æ–°å®šä¹‰ä¸ºCadQueryè„šæœ¬çš„ç”Ÿæˆâ€”â€”ä¸€ç§åŸºäºPythonçš„å‚æ•°åŒ–CADè¯­è¨€ã€‚è¿™ç§è¡¨ç¤ºæ–¹æ³•å®ç°äº†ç›´æ¥çš„å‡ ä½•éªŒè¯ã€æ›´ä¸°å¯Œçš„å»ºæ¨¡è¯æ±‡ï¼Œä»¥åŠä¸ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹çš„æ— ç¼é›†æˆã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜ä»£ç çš„æœ‰æ•ˆæ€§å’Œå‡ ä½•ä¿çœŸåº¦ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªä¸¤é˜¶æ®µçš„å­¦ä¹ æµç¨‹ï¼ŒåŒ…æ‹¬ï¼šï¼ˆ1ï¼‰åœ¨æˆå¯¹çš„æ–‡æœ¬-CadQueryæ•°æ®ä¸Šè¿›è¡Œç›‘ç£å¾®è°ƒï¼›ï¼ˆ2ï¼‰ä½¿ç”¨é›†å›¢å¥–åŠ±æ”¿ç­–ä¼˜åŒ–ï¼ˆGRPOï¼‰è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼Œç”±åŒ…æ‹¬å‡ ä½•å¥–åŠ±ï¼ˆChamferè·ç¦»ï¼‰å’Œæ ¼å¼å¥–åŠ±åœ¨å†…çš„CADç‰¹å®šå¥–åŠ±æŒ‡å¯¼ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†æ€ç»´é“¾ï¼ˆCoTï¼‰è§„åˆ’è¿‡ç¨‹æ¥æ”¹å–„æ¨¡å‹æ¨ç†ï¼Œå¹¶é€šè¿‡è‡ªåŠ¨åŒ–ç®¡é“æ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡ã€é«˜è´¨é‡çš„åŒ…å«110Kä¸ªæ–‡æœ¬-CadQuery-3Dæ¨¡å‹ä¸‰å…ƒç»„å’Œ1.5Kä¸ªCoTæ ·æœ¬çš„æ•°æ®é›†ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒCAD-Coderä½¿å¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿç›´æ¥ä»è‡ªç„¶è¯­è¨€ç”Ÿæˆå¤šæ ·ã€æœ‰æ•ˆå’Œå¤æ‚çš„CADæ¨¡å‹ï¼Œä»è€Œæ¨åŠ¨äº†æ–‡æœ¬åˆ°CADç”Ÿæˆå’Œå‡ ä½•æ¨ç†çš„æœ€æ–°æŠ€æœ¯è¿›å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19713v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ä»‹ç»äº†CAD-Coderæ¡†æ¶ï¼Œå®ƒå°†æ–‡æœ¬åˆ°CADçš„è½¬æ¢é‡æ–°å®šä¹‰ä¸ºCadQueryè„šæœ¬çš„ç”Ÿæˆã€‚è¯¥æ¡†æ¶åˆ©ç”¨åŸºäºPythonçš„å‚æ•°åŒ–CADè¯­è¨€ï¼Œå®ç°äº†ç›´æ¥å‡ ä½•éªŒè¯ã€ä¸°å¯Œçš„å»ºæ¨¡è¯æ±‡ä»¥åŠä¸ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹çš„æ— ç¼é›†æˆã€‚ä¸ºæé«˜ä»£ç çš„æœ‰æ•ˆæ€§å’Œå‡ ä½•ä¿çœŸåº¦ï¼Œæå‡ºäº†åŒ…æ‹¬ç›‘ç£å¾®è°ƒå’Œå¯¹å¸¦é…å¯¹æ–‡æœ¬-CadQueryæ•°æ®çš„å¼ºåŒ–å­¦ä¹ åœ¨å†…çš„ä¸¤é˜¶æ®µå­¦ä¹ ç®¡é“ã€‚é€šè¿‡ç»„åˆå‡ ä½•å¥–åŠ±ï¼ˆChamferè·ç¦»ï¼‰å’Œæ ¼å¼å¥–åŠ±çš„CADç‰¹å®šå¥–åŠ±ç­–ç•¥ï¼Œä»¥åŠå¼•å…¥æ€ç»´é“¾è§„åˆ’è¿‡ç¨‹æ¥æ”¹å–„æ¨¡å‹æ¨ç†ã€‚é€šè¿‡è‡ªåŠ¨åŒ–ç®¡é“æ„å»ºçš„å¤§å‹ã€é«˜è´¨é‡æ•°æ®é›†åŒ…å«11ä¸‡æ–‡æœ¬-CadQuery-3Dæ¨¡å‹ä¸‰å…ƒç»„å’Œ15ä¸‡æ€ç»´é“¾æ ·æœ¬ã€‚å®éªŒè¯æ˜ï¼ŒCAD-Coderä½¿å¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿç›´æ¥ä»è‡ªç„¶è¯­è¨€ç”Ÿæˆå¤šæ ·ã€æœ‰æ•ˆå’Œå¤æ‚çš„CADæ¨¡å‹ï¼Œæ¨åŠ¨äº†æ–‡æœ¬åˆ°CADç”Ÿæˆå’Œå‡ ä½•æ¨ç†çš„æœ€æ–°è¿›å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CAD-Coderæ¡†æ¶å°†æ–‡æœ¬è½¬æ¢ä¸ºCADæ¨¡å‹çš„è¿‡ç¨‹é‡æ–°å®šä¹‰ä¸ºç”ŸæˆCadQueryè„šæœ¬ã€‚</li>
<li>åˆ©ç”¨åŸºäºPythonçš„å‚æ•°åŒ–CADè¯­è¨€ï¼Œæä¾›ç›´æ¥å‡ ä½•éªŒè¯ã€ä¸°å¯Œçš„å»ºæ¨¡è¯æ±‡å’Œæ— ç¼çš„è¯­è¨€æ¨¡å‹é›†æˆã€‚</li>
<li>æå‡ºä¸¤é˜¶æ®µå­¦ä¹ ç®¡é“ï¼ŒåŒ…æ‹¬ç›‘ç£å¾®è°ƒå’Œå¯¹å¸¦é…å¯¹æ–‡æœ¬-CadQueryæ•°æ®çš„å¼ºåŒ–å­¦ä¹ ï¼Œä»¥æé«˜ä»£ç çš„æœ‰æ•ˆæ€§å’Œå‡ ä½•ä¿çœŸåº¦ã€‚</li>
<li>å¼•å…¥ç»„åˆå‡ ä½•å¥–åŠ±ï¼ˆChamferè·ç¦»ï¼‰å’Œæ ¼å¼å¥–åŠ±çš„CADç‰¹å®šå¥–åŠ±ç­–ç•¥ä¼˜åŒ–ã€‚</li>
<li>å¼•å…¥æ€ç»´é“¾è§„åˆ’è¿‡ç¨‹æ”¹å–„æ¨¡å‹æ¨ç†èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡è‡ªåŠ¨åŒ–ç®¡é“æ„å»ºçš„å¤§å‹æ•°æ®é›†åŒ…å«å¤šç§æ–‡æœ¬ã€CadQueryè„šæœ¬å’Œ3Dæ¨¡å‹çš„ä¸‰å…ƒç»„ç»„åˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19713">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ee2e8429815181010302285a44073724.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3f227d4738b014e8188273432dfbe8a8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f8b6fc049ecf570cbd11a087a8e656d4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1cb506603d9fdb4d883c01ece9acd88f.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Large-Language-Models-for-Planning-A-Comprehensive-and-Systematic-Survey"><a href="#Large-Language-Models-for-Planning-A-Comprehensive-and-Systematic-Survey" class="headerlink" title="Large Language Models for Planning: A Comprehensive and Systematic   Survey"></a>Large Language Models for Planning: A Comprehensive and Systematic   Survey</h2><p><strong>Authors:Pengfei Cao, Tianyi Men, Wencan Liu, Jingwen Zhang, Xuzhao Li, Xixun Lin, Dianbo Sui, Yanan Cao, Kang Liu, Jun Zhao</strong></p>
<p>Planning represents a fundamental capability of intelligent agents, requiring comprehensive environmental understanding, rigorous logical reasoning, and effective sequential decision-making. While Large Language Models (LLMs) have demonstrated remarkable performance on certain planning tasks, their broader application in this domain warrants systematic investigation. This paper presents a comprehensive review of LLM-based planning. Specifically, this survey is structured as follows: First, we establish the theoretical foundations by introducing essential definitions and categories about automated planning. Next, we provide a detailed taxonomy and analysis of contemporary LLM-based planning methodologies, categorizing them into three principal approaches: 1) External Module Augmented Methods that combine LLMs with additional components for planning, 2) Finetuning-based Methods that involve using trajectory data and feedback signals to adjust LLMs in order to improve their planning abilities, and 3) Searching-based Methods that break down complex tasks into simpler components, navigate the planning space, or enhance decoding strategies to find the best solutions. Subsequently, we systematically summarize existing evaluation frameworks, including benchmark datasets, evaluation metrics and performance comparisons between representative planning methods. Finally, we discuss the underlying mechanisms enabling LLM-based planning and outline promising research directions for this rapidly evolving field. We hope this survey will serve as a valuable resource to inspire innovation and drive progress in this field. </p>
<blockquote>
<p>è§„åˆ’æ˜¯æ™ºèƒ½ä¸»ä½“çš„ä¸€é¡¹åŸºæœ¬èƒ½åŠ›ï¼Œéœ€è¦å…¨é¢çš„ç¯å¢ƒç†è§£ã€ä¸¥è°¨çš„é€»è¾‘æ¨ç†å’Œæœ‰æ•ˆçš„åºåˆ—å†³ç­–ã€‚è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æŸäº›è§„åˆ’ä»»åŠ¡ä¸Šè¡¨ç°å‡ºäº†æ˜¾è‘—çš„æ€§èƒ½ï¼Œä½†å®ƒä»¬åœ¨è§„åˆ’é¢†åŸŸçš„å¹¿æ³›åº”ç”¨å€¼å¾—è¿›è¡Œç³»ç»Ÿæ€§çš„ç ”ç©¶ã€‚æœ¬æ–‡å…¨é¢å›é¡¾äº†åŸºäºLLMçš„è§„åˆ’æŠ€æœ¯ã€‚å…·ä½“åœ°ï¼Œæœ¬æ–‡çš„ç»“æ„å¦‚ä¸‹ï¼šé¦–å…ˆï¼Œæˆ‘ä»¬é€šè¿‡ä»‹ç»è‡ªåŠ¨åŒ–è§„åˆ’çš„åŸºæœ¬å®šä¹‰å’Œç±»åˆ«æ¥å»ºç«‹ç†è®ºåŸºç¡€ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æä¾›äº†å½“ä»£åŸºäºLLMçš„è§„åˆ’æ–¹æ³•çš„è¯¦ç»†åˆ†ç±»å’Œåˆ†æï¼Œå°†å®ƒä»¬å½’çº³ä¸ºä¸‰ç§ä¸»è¦æ–¹æ³•ï¼š1) ç»“åˆLLMä¸å…¶ä»–ç»„ä»¶è¿›è¡Œè§„åˆ’çš„å¤–éƒ¨æ¨¡å—å¢å¼ºæ–¹æ³•ï¼›2) åŸºäºå¾®è°ƒçš„æ–¹æ³•ï¼Œä½¿ç”¨è½¨è¿¹æ•°æ®å’Œåé¦ˆä¿¡å·æ¥è°ƒæ•´LLMä»¥æé«˜å…¶è§„åˆ’èƒ½åŠ›ï¼›ä»¥åŠ3) åŸºäºæœç´¢çš„æ–¹æ³•ï¼Œå°†å¤æ‚ä»»åŠ¡åˆ†è§£ä¸ºæ›´ç®€å•çš„ç»„ä»¶ï¼Œåœ¨è§„åˆ’ç©ºé—´ä¸­è¿›è¡Œå¯¼èˆªï¼Œæˆ–æ”¹è¿›è§£ç ç­–ç•¥ä»¥æ‰¾åˆ°æœ€ä½³è§£å†³æ–¹æ¡ˆã€‚ç„¶åï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°æ€»ç»“äº†ç°æœ‰çš„è¯„ä¼°æ¡†æ¶ï¼ŒåŒ…æ‹¬åŸºå‡†æ•°æ®é›†ã€è¯„ä¼°æŒ‡æ ‡ä»¥åŠä»£è¡¨æ€§è§„åˆ’æ–¹æ³•ä¹‹é—´çš„æ€§èƒ½æ¯”è¾ƒã€‚æœ€åï¼Œæˆ‘ä»¬è®¨è®ºäº†å®ç°åŸºäºLLMçš„è§„åˆ’çš„åŸºç¡€æœºåˆ¶ï¼Œå¹¶æ¦‚è¿°äº†è¿™ä¸ªå¿«é€Ÿå‘å±•çš„é¢†åŸŸçš„æœ‰å‰é€”çš„ç ”ç©¶æ–¹å‘ã€‚æˆ‘ä»¬å¸Œæœ›è¿™ä»½ç»¼è¿°èƒ½æˆä¸ºæ¿€å‘åˆ›æ–°å’Œæ¨åŠ¨è¯¥é¢†åŸŸè¿›æ­¥çš„å®è´µèµ„æºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19683v1">PDF</a> </p>
<p><strong>Summary</strong><br>æ™ºèƒ½è§„åˆ’æ˜¯æ™ºèƒ½ä¸»ä½“çš„ä¸€é¡¹åŸºæœ¬èƒ½åŠ›ï¼Œæ¶‰åŠå…¨é¢çš„ç¯å¢ƒç†è§£ã€ä¸¥è°¨çš„é€»è¾‘æ¨ç†å’Œæœ‰æ•ˆçš„åºåˆ—å†³ç­–åˆ¶å®šã€‚å½“å‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è§„åˆ’ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ˜¾è‘—æ€§èƒ½ï¼Œä½†å…¶åœ¨è¯¥é¢†åŸŸçš„å¹¿æ³›åº”ç”¨å°šå¾…ç³»ç»Ÿç ”ç©¶ã€‚æœ¬æ–‡å…¨é¢ç»¼è¿°äº†åŸºäºLLMçš„è§„åˆ’æ–¹æ³•ï¼ŒåŒ…æ‹¬è‡ªåŠ¨åŒ–è§„åˆ’çš„åŸºç¡€ç†è®ºã€åŸºäºLLMçš„è§„åˆ’æ–¹æ³•çš„åˆ†ç±»åŠå…¶è¯¦æƒ…ã€ç°æœ‰è¯„ä¼°æ¡†æ¶ä»¥åŠç ”ç©¶å±•æœ›ã€‚æ—¨åœ¨ä¸ºæ­¤é¢†åŸŸçš„åˆ›æ–°è¿›æ­¥æä¾›æœ‰ä»·å€¼çš„èµ„æºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>è§„åˆ’æ˜¯æ™ºèƒ½ä¸»ä½“çš„æ ¸å¿ƒåŠŸèƒ½ï¼Œæ¶‰åŠå¤šæ–¹é¢çš„æŠ€èƒ½åŒ…æ‹¬ç¯å¢ƒè®¤çŸ¥ã€é€»è¾‘æ¨ç†å’Œå†³ç­–åˆ¶å®šã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è§„åˆ’ä»»åŠ¡ä¸Šè¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ã€‚</li>
<li>åŸºäºLLMçš„è§„åˆ’æ–¹æ³•ä¸»è¦åŒ…æ‹¬ä¸‰ç§ç±»å‹ï¼šå¤–éƒ¨æ¨¡å—å¢å¼ºæ³•ã€å¾®è°ƒæ³•å’Œæœç´¢æ³•ã€‚</li>
<li>å½“å‰å¯¹åŸºäºLLMçš„è§„åˆ’æ–¹æ³•çš„è¯„ä¼°ä¸»è¦åŒ…æ‹¬åŸºå‡†æ•°æ®é›†ã€è¯„ä¼°æŒ‡æ ‡ä»¥åŠä»£è¡¨æ€§è§„åˆ’æ–¹æ³•çš„æ€§èƒ½æ¯”è¾ƒã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19683">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-d74605f745301bbdbc54f12535471aed.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-34687c4d01a5c19edc88fa97284b6b7e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c269f900a7649745fe007642b376b2bf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f1c9ef5ff0b986e347764eb735b4664b.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Large-Language-Modelsâ€™-Reasoning-Stalls-An-Investigation-into-the-Capabilities-of-Frontier-Models"><a href="#Large-Language-Modelsâ€™-Reasoning-Stalls-An-Investigation-into-the-Capabilities-of-Frontier-Models" class="headerlink" title="Large Language Modelsâ€™ Reasoning Stalls: An Investigation into the   Capabilities of Frontier Models"></a>Large Language Modelsâ€™ Reasoning Stalls: An Investigation into the   Capabilities of Frontier Models</h2><p><strong>Authors:Lachlan McGinness, Peter Baumgartner</strong></p>
<p>Empirical methods to examine the capability of Large Language Models (LLMs) to use Automated Theorem Prover (ATP) reasoning strategies are studied. We evaluate the performance of State of the Art models from December 2023 and August 2024 on PRONTOQA steamroller reasoning problems. For that, we develop methods for assessing LLM response accuracy and correct answer correlation.   Our results show that progress in improving LLM reasoning abilities has stalled over the nine month period. By tracking completion tokens, we show that almost all improvement in reasoning ability since GPT-4 was released can be attributed to either hidden system prompts or the training of models to automatically use generic Chain of Thought prompting strategies. Among the ATP reasoning strategies tried, we found that current frontier LLMs are best able to follow the bottom-up (also known as forward-chaining) strategy. A low positive correlation was found between an LLM response containing correct reasoning and arriving at the correct conclusion. </p>
<blockquote>
<p>æœ¬æ–‡ç ”ç©¶äº†å®è¯æ–¹æ³•ï¼Œä»¥æ£€éªŒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½¿ç”¨è‡ªåŠ¨åŒ–å®šç†è¯æ˜å™¨ï¼ˆATPï¼‰æ¨ç†ç­–ç•¥çš„èƒ½åŠ›ã€‚æˆ‘ä»¬è¯„ä¼°äº†2 0 2 3å¹´1 2æœˆå’Œ2 0 2 4å¹´8æœˆçš„æœ€æ–°æ¨¡å‹åœ¨PRONTOQAè’¸æ±½å‹è·¯æœºæ¨ç†é—®é¢˜ä¸Šçš„è¡¨ç°ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼€å‘äº†è¯„ä¼°LLMå“åº”å‡†ç¡®æ€§å’Œæ­£ç¡®ç­”æ¡ˆå…³è”åº¦çš„æ–¹æ³•ã€‚ç»“æœè¡¨æ˜ï¼Œåœ¨é•¿è¾¾ä¹ä¸ªæœˆçš„æœŸé—´å†…ï¼Œæ”¹å–„LLMæ¨ç†èƒ½åŠ›çš„è¿›å±•å·²åœæ»ã€‚é€šè¿‡è·Ÿè¸ªå®Œæˆä»¤ç‰Œï¼Œæˆ‘ä»¬å‘ç°è‡ªä»GPT-4å‘å¸ƒä»¥æ¥ï¼Œæ¨ç†èƒ½åŠ›çš„å‡ ä¹æ‰€æœ‰æ”¹è¿›éƒ½å¯ä»¥å½’å› äºéšè—ç³»ç»Ÿæç¤ºæˆ–è®­ç»ƒæ¨¡å‹è‡ªåŠ¨ä½¿ç”¨é€šç”¨çš„æ€ç»´é“¾æç¤ºç­–ç•¥ã€‚åœ¨å°è¯•çš„ATPæ¨ç†ç­–ç•¥ä¸­ï¼Œæˆ‘ä»¬å‘ç°ç›®å‰çš„å‰æ²¿LLMæœ€èƒ½å¤Ÿé‡‡ç”¨è‡ªä¸‹è€Œä¸Šï¼ˆä¹Ÿç§°ä¸ºå‰å‘é“¾æ¥ï¼‰çš„ç­–ç•¥ã€‚LLMçš„å›åº”ä¸­åŒ…å«æ­£ç¡®çš„æ¨ç†å’Œå¾—å‡ºæ­£ç¡®ç»“è®ºä¹‹é—´çš„æ­£ç›¸å…³å…³ç³»è¾ƒä½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19676v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªåŠ¨åŒ–å®šç†è¯æ˜ï¼ˆATPï¼‰æ¨ç†ç­–ç•¥æ–¹é¢çš„èƒ½åŠ›è¿›è¡Œäº†å®è¯ç ”ç©¶ã€‚è¯„ä¼°äº†2023å¹´12æœˆå’Œ2024å¹´8æœˆçš„å…ˆè¿›æ¨¡å‹åœ¨PRONTOQAè’¸æ±½å‹è·¯æœºæ¨ç†é—®é¢˜ä¸Šçš„è¡¨ç°ã€‚é€šè¿‡è¯„ä¼°LLMå“åº”å‡†ç¡®æ€§å’Œæ­£ç¡®ç­”æ¡ˆç›¸å…³æ€§ï¼Œå‘ç°è¿‡å»ä¹ä¸ªæœˆåœ¨æå‡LLMæ¨ç†èƒ½åŠ›æ–¹é¢è¿›å±•åœæ»ã€‚è·Ÿè¸ªå®Œæˆä»¤ç‰Œè¡¨æ˜ï¼Œè‡ªGPT-4å‘å¸ƒä»¥æ¥ï¼Œæ¨ç†èƒ½åŠ›çš„æå‡å¤§å¤šå½’åŠŸäºéšè”½ç³»ç»Ÿæç¤ºæˆ–æ¨¡å‹è®­ç»ƒè‡ªåŠ¨ä½¿ç”¨é€šç”¨æ€ç»´é“¾æç¤ºç­–ç•¥ã€‚åœ¨å°è¯•çš„ATPæ¨ç†ç­–ç•¥ä¸­ï¼Œå½“å‰æœ€å‰æ²¿çš„LLMæœ€èƒ½é‡‡ç”¨è‡ªä¸‹è€Œä¸Šï¼ˆä¹Ÿç§°ä¸ºå‰å‘é“¾æ¥ï¼‰çš„ç­–ç•¥ã€‚LLMå“åº”ä¸­çš„æ­£ç¡®æ¨ç†ä¸å¾—å‡ºæ­£ç¡®ç»“è®ºä¹‹é—´å­˜åœ¨å¾®å¼±çš„æ­£ç›¸å…³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªåŠ¨åŒ–å®šç†è¯æ˜ï¼ˆATPï¼‰æ¨ç†ç­–ç•¥æ–¹é¢çš„èƒ½åŠ›å¾—åˆ°äº†ç ”ç©¶ã€‚</li>
<li>è¯„ä¼°äº†ä¸åŒæ—¶é—´æ®µå…ˆè¿›æ¨¡å‹åœ¨ç‰¹å®šæ¨ç†é—®é¢˜ä¸Šçš„è¡¨ç°ã€‚</li>
<li>LLMå“åº”å‡†ç¡®æ€§å’Œæ­£ç¡®ç­”æ¡ˆç›¸å…³æ€§çš„è¯„ä¼°æ˜¾ç¤ºæ¨ç†èƒ½åŠ›è¿›å±•åœæ»ã€‚</li>
<li>è·Ÿè¸ªå®Œæˆä»¤ç‰Œæ˜¾ç¤ºï¼Œæå‡ä¸»è¦å½’å› äºç³»ç»Ÿæç¤ºå’Œæ¨¡å‹è®­ç»ƒä¸­çš„é€šç”¨æ€ç»´é“¾æç¤ºç­–ç•¥ã€‚</li>
<li>åœ¨å°è¯•çš„ATPæ¨ç†ç­–ç•¥ä¸­ï¼ŒLLMsæœ€èƒ½é‡‡ç”¨è‡ªä¸‹è€Œä¸Šçš„ç­–ç•¥ã€‚</li>
<li>å­˜åœ¨å¾®å¼±çš„æ­£ç›¸å…³å…³ç³»ï¼Œå³LLMå“åº”ä¸­çš„æ­£ç¡®æ¨ç†ä¸å¾—å‡ºæ­£ç¡®ç»“è®ºç›¸å…³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19676">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-38b72bc4964c58fb082b78f0989517db.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2e39d466c9fdd57480ea9f4da1ef13da.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-db10cf02b11a4fc31fe376f90aecf0a0.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="LeCoDe-A-Benchmark-Dataset-for-Interactive-Legal-Consultation-Dialogue-Evaluation"><a href="#LeCoDe-A-Benchmark-Dataset-for-Interactive-Legal-Consultation-Dialogue-Evaluation" class="headerlink" title="LeCoDe: A Benchmark Dataset for Interactive Legal Consultation Dialogue   Evaluation"></a>LeCoDe: A Benchmark Dataset for Interactive Legal Consultation Dialogue   Evaluation</h2><p><strong>Authors:Weikang Yuan, Kaisong Song, Zhuoren Jiang, Junjie Cao, Yujie Zhang, Jun Lin, Kun Kuang, Ji Zhang, Xiaozhong Liu</strong></p>
<p>Legal consultation is essential for safeguarding individual rights and ensuring access to justice, yet remains costly and inaccessible to many individuals due to the shortage of professionals. While recent advances in Large Language Models (LLMs) offer a promising path toward scalable, low-cost legal assistance, current systems fall short in handling the interactive and knowledge-intensive nature of real-world consultations. To address these challenges, we introduce LeCoDe, a real-world multi-turn benchmark dataset comprising 3,696 legal consultation dialogues with 110,008 dialogue turns, designed to evaluate and improve LLMsâ€™ legal consultation capability. With LeCoDe, we innovatively collect live-streamed consultations from short-video platforms, providing authentic multi-turn legal consultation dialogues. The rigorous annotation by legal experts further enhances the dataset with professional insights and expertise. Furthermore, we propose a comprehensive evaluation framework that assesses LLMsâ€™ consultation capabilities in terms of (1) clarification capability and (2) professional advice quality. This unified framework incorporates 12 metrics across two dimensions. Through extensive experiments on various general and domain-specific LLMs, our results reveal significant challenges in this task, with even state-of-the-art models like GPT-4 achieving only 39.8% recall for clarification and 59% overall score for advice quality, highlighting the complexity of professional consultation scenarios. Based on these findings, we further explore several strategies to enhance LLMsâ€™ legal consultation abilities. Our benchmark contributes to advancing research in legal domain dialogue systems, particularly in simulating more real-world user-expert interactions. </p>
<blockquote>
<p>æ³•å¾‹å’¨è¯¢å¯¹äºä¿æŠ¤ä¸ªäººæƒåˆ©å’Œç¡®ä¿è·å¾—å…¬æ­£è‡³å…³é‡è¦ï¼Œç„¶è€Œç”±äºä¸“ä¸šäººå‘˜çŸ­ç¼ºï¼Œè®¸å¤šä¸ªäººä»ç„¶è§‰å¾—æˆæœ¬é«˜æ˜‚ä¸”éš¾ä»¥è·å¾—å’¨è¯¢ã€‚è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ€æ–°è¿›å±•ä¸ºå¯æ‰©å±•å’Œä½æˆæœ¬æ³•å¾‹æ´åŠ©æä¾›äº†ä¸€æ¡å……æ»¡å¸Œæœ›çš„é“è·¯ï¼Œä½†å½“å‰çš„ç³»ç»Ÿåœ¨å¤„ç†ç°å®ä¸–ç•Œä¸­äº¤äº’å’ŒçŸ¥è¯†å¯†é›†å‹çš„å’¨è¯¢æ–¹é¢è¿˜å­˜åœ¨ä¸è¶³ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†LeCoDeï¼Œè¿™æ˜¯ä¸€ä¸ªç°å®ä¸–ç•Œå¤šè½®åŸºå‡†æ•°æ®é›†ï¼ŒåŒ…å«3696ä¸ªæ³•å¾‹å’¨è¯¢å¯¹è¯å’Œ110008ä¸ªå¯¹è¯å›åˆã€‚LeCoDeæ—¨åœ¨è¯„ä¼°å’Œæ”¹è¿›LLMçš„æ³•å¾‹å’¨è¯¢èƒ½åŠ›ã€‚æˆ‘ä»¬é€šè¿‡ä»çŸ­è§†é¢‘å¹³å°ç›´æ’­æ”¶é›†å’¨è¯¢å¯¹è¯ï¼Œæä¾›çœŸå®çš„å¤šè½®æ³•å¾‹å’¨è¯¢å¯¹è¯ã€‚æ³•å¾‹ä¸“å®¶çš„ä¸¥æ ¼æ³¨é‡Šè¿›ä¸€æ­¥å¢å¼ºäº†æ•°æ®é›†çš„ä¸“ä¸šè§è§£å’Œä¸“ä¸šçŸ¥è¯†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå…¨é¢çš„è¯„ä¼°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä»ï¼ˆ1ï¼‰æ¾„æ¸…èƒ½åŠ›å’Œï¼ˆ2ï¼‰ä¸“ä¸šå»ºè®®è´¨é‡ä¸¤ä¸ªæ–¹é¢è¯„ä¼°LLMçš„å’¨è¯¢èƒ½åŠ›ã€‚è¿™ä¸€ç»Ÿä¸€æ¡†æ¶åŒ…å«ä¸¤ä¸ªç»´åº¦çš„12ä¸ªæŒ‡æ ‡ã€‚é€šè¿‡å¯¹å„ç§é€šç”¨å’Œç‰¹å®šé¢†åŸŸçš„LLMè¿›è¡Œå¹¿æ³›å®éªŒï¼Œæˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œåœ¨è¿™ä¸€ä»»åŠ¡ä¸­ä»å­˜åœ¨é‡å¤§æŒ‘æˆ˜ï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„æ¨¡å‹å¦‚GPT-4ï¼Œå…¶æ¾„æ¸…å¬å›ç‡ä»…ä¸º39.8%ï¼Œå»ºè®®è´¨é‡çš„æ€»ä½“å¾—åˆ†ä¹Ÿåªæœ‰59%ï¼Œè¿™å‡¸æ˜¾äº†ä¸“ä¸šå’¨è¯¢åœºæ™¯çš„å¤æ‚æ€§ã€‚åŸºäºè¿™äº›å‘ç°ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æ¢ç´¢äº†å‡ ç§å¢å¼ºLLMæ³•å¾‹å’¨è¯¢èƒ½åŠ›çš„ç­–ç•¥ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•æœ‰åŠ©äºæ¨è¿›æ³•å¾‹é¢†åŸŸå¯¹è¯ç³»ç»Ÿçš„ç ”ç©¶ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¨¡æ‹Ÿæ›´çœŸå®ä¸–ç•Œç”¨æˆ·ä¸ä¸“å®¶äº’åŠ¨æ–¹é¢ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19667v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>æ³•å¾‹å’¨è¯¢æœåŠ¡å¯¹äºä¿éšœä¸ªäººæƒç›Šå’Œç¡®ä¿å…¬æ­£è‡³å…³é‡è¦ï¼Œä½†å…¶æˆæœ¬é«˜æ˜‚ä¸”éš¾ä»¥è§¦åŠè®¸å¤šäººï¼Œä¸»è¦ç”±äºä¸“ä¸šäººæ‰çŸ­ç¼ºã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ€æ–°è¿›å±•ä¸ºå¤§è§„æ¨¡ã€ä½æˆæœ¬æ³•å¾‹æ´åŠ©é“ºå¹³äº†é“è·¯ï¼Œä½†ç°æœ‰ç³»ç»Ÿä»éš¾ä»¥å¤„ç†ç°å®å’¨è¯¢ä¸­çš„äº¤äº’å’ŒçŸ¥è¯†å¯†é›†å‹ç‰¹æ€§ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œå¼•å…¥äº†LeCoDeæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«3696ä¸ªæ³•å¾‹å’¨è¯¢å¯¹è¯å’Œ110008ä¸ªå¯¹è¯å›åˆçš„çœŸå®ä¸–ç•Œå¤šè½®åŸºå‡†æ•°æ®é›†ï¼Œæ—¨åœ¨è¯„ä¼°å’Œæ”¹è¿›LLMçš„æ³•å¾‹å’¨è¯¢èƒ½åŠ›ã€‚è¯¥æ•°æ®é›†é€šè¿‡çŸ­è§†é¢‘å¹³å°æ”¶é›†ç°åœºæ³•å¾‹å’¨è¯¢å¯¹è¯ï¼Œç”±æ³•å¾‹ä¸“å®¶è¿›è¡Œä¸¥æ ¼æ³¨é‡Šï¼Œæä¾›ä¸“ä¸šçš„è§è§£å’Œä¸“ä¸šçŸ¥è¯†ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†ä¸€ä¸ªå…¨é¢çš„è¯„ä¼°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶è¯„ä¼°LLMçš„å’¨è¯¢èƒ½åŠ›åŒ…æ‹¬æ¾„æ¸…èƒ½åŠ›å’Œä¸“ä¸šå»ºè®®è´¨é‡ä¸¤ä¸ªæ–¹é¢ï¼Œæ¶µç›–12ä¸ªæŒ‡æ ‡ã€‚é€šè¿‡åœ¨ä¸åŒçš„ä¸€èˆ¬å’Œç‰¹å®šé¢†åŸŸçš„LLMä¸Šè¿›è¡Œå¹¿æ³›å®éªŒï¼Œç»“æœæ˜¾ç¤ºï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„æ¨¡å‹å¦‚GPT-4ï¼Œåœ¨æ¾„æ¸…å’Œå»ºè®®è´¨é‡æ–¹é¢ä»å­˜åœ¨æ˜¾è‘—æŒ‘æˆ˜ã€‚åŸºäºæ­¤ï¼Œè¿›ä¸€æ­¥æ¢ç´¢äº†å¢å¼ºLLMæ³•å¾‹å’¨è¯¢èƒ½åŠ›çš„ç­–ç•¥ã€‚è¯¥åŸºå‡†æ•°æ®é›†å¯¹æ¨è¿›æ³•å¾‹é¢†åŸŸå¯¹è¯ç³»ç»Ÿç ”ç©¶ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¨¡æ‹Ÿæ›´å¤šç°å®ä¸–ç•Œç”¨æˆ·ä¸ä¸“å®¶äº’åŠ¨æ–¹é¢å…·æœ‰é‡è¦æ„ä¹‰ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æ³•å¾‹å’¨è¯¢æœåŠ¡å¯¹ä¸ªäººæƒç›Šä¿éšœå’Œå…¬æ­£è‡³å…³é‡è¦ï¼Œä½†æˆæœ¬é«˜æ˜‚ä¸”éš¾ä»¥è§¦åŠè®¸å¤šäººã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸ºæ³•å¾‹æ´åŠ©æä¾›äº†æ–°çš„å¯èƒ½æ€§ï¼Œä½†ä»é¢ä¸´å¤„ç†ç°å®å’¨è¯¢çš„äº¤äº’å’ŒçŸ¥è¯†å¯†é›†å‹ç‰¹æ€§çš„æŒ‘æˆ˜ã€‚</li>
<li>å¼•å…¥äº†LeCoDeæ•°æ®é›†ï¼ŒåŒ…å«çœŸå®çš„æ³•å¾‹å’¨è¯¢å¯¹è¯å’Œä¸¥æ ¼çš„æ³•å¾‹ä¸“å®¶æ³¨é‡Šï¼Œä¸ºè¯„ä¼°å’Œæ”¹è¿›LLMçš„æ³•å¾‹å’¨è¯¢èƒ½åŠ›æä¾›äº†åŸºå‡†ã€‚</li>
<li>å…¨é¢çš„è¯„ä¼°æ¡†æ¶åŒ…æ‹¬æ¾„æ¸…èƒ½åŠ›å’Œä¸“ä¸šå»ºè®®è´¨é‡ä¸¤ä¸ªæ–¹é¢ï¼Œæ¶µç›–12ä¸ªæŒ‡æ ‡ã€‚</li>
<li>å…ˆè¿›æ¨¡å‹å¦‚GPT-4åœ¨æ³•å¾‹å’¨è¯¢æ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>æ¢ç´¢äº†å¢å¼ºLLMæ³•å¾‹å’¨è¯¢èƒ½åŠ›çš„ç­–ç•¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19667">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cca2abaaab80b2dc81de596916f4b938.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-12b77e2b5aee79815390efa5541eedf1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-69170b674db0ac75f41bca6c0813cce1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6e122ce3a2f65319a767fbc94e2fbe1d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9c0b154c390ea78a40544439aed4be39.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-28/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-28/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-28/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-ca768b8bb8003dfc3cec794ae8f9eb01.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-28  KnowTrace Bootstrapping Iterative Retrieval-Augmented Generation with   Structured Knowledge Tracing
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-28
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-27/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-ab9e03f26c8d1cdf221d6148f199fd01.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-27  DualTalk Dual-Speaker Interaction for 3D Talking Head Conversations
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-27
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">28791.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
