<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-28  KnowTrace Bootstrapping Iterative Retrieval-Augmented Generation with   Structured Knowledge Tracing">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-ca768b8bb8003dfc3cec794ae8f9eb01.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-28
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-24
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    85 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-28-æ›´æ–°"><a href="#2025-05-28-æ›´æ–°" class="headerlink" title="2025-05-28 æ›´æ–°"></a>2025-05-28 æ›´æ–°</h1><h2 id="KnowTrace-Bootstrapping-Iterative-Retrieval-Augmented-Generation-with-Structured-Knowledge-Tracing"><a href="#KnowTrace-Bootstrapping-Iterative-Retrieval-Augmented-Generation-with-Structured-Knowledge-Tracing" class="headerlink" title="KnowTrace: Bootstrapping Iterative Retrieval-Augmented Generation with   Structured Knowledge Tracing"></a>KnowTrace: Bootstrapping Iterative Retrieval-Augmented Generation with   Structured Knowledge Tracing</h2><p><strong>Authors:Rui Li, Quanyu Dai, Zeyu Zhang, Xu Chen, Zhenhua Dong, Ji-Rong Wen</strong></p>
<p>Recent advances in retrieval-augmented generation (RAG) furnish large language models (LLMs) with iterative retrievals of relevant information to handle complex multi-hop questions. These methods typically alternate between LLM reasoning and retrieval to accumulate external information into the LLMâ€™s context. However, the ever-growing context inherently imposes an increasing burden on the LLM to perceive connections among critical information pieces, with futile reasoning steps further exacerbating this overload issue. In this paper, we present KnowTrace, an elegant RAG framework to (1) mitigate the context overload and (2) bootstrap higher-quality multi-step reasoning. Instead of simply piling the retrieved contents, KnowTrace autonomously traces out desired knowledge triplets to organize a specific knowledge graph relevant to the input question. Such a structured workflow not only empowers the LLM with an intelligible context for inference, but also naturally inspires a reflective mechanism of knowledge backtracing to identify contributive LLM generations as process supervision data for self-bootstrapping. Extensive experiments show that KnowTrace consistently surpasses existing methods across three multi-hop question answering benchmarks, and the bootstrapped version further amplifies the gains. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯çš„è¿›å±•ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æä¾›äº†ç›¸å…³çš„è¿­ä»£æ£€ç´¢ä¿¡æ¯ï¼Œä»¥å¤„ç†å¤æ‚çš„å¤šè·³é—®é¢˜ã€‚è¿™äº›æ–¹æ³•é€šå¸¸åœ¨LLMæ¨ç†å’Œæ£€ç´¢ä¹‹é—´è¿›è¡Œäº¤æ›¿ï¼Œä»¥å°†å¤–éƒ¨ä¿¡æ¯ç´¯ç§¯åˆ°LLMçš„ä¸Šä¸‹æ–‡ä¸­ã€‚ç„¶è€Œï¼Œä¸æ–­å¢é•¿çš„ä¸Šä¸‹æ–‡å›ºæœ‰åœ°å¯¹LLMæ„ŸçŸ¥å…³é”®ä¿¡æ¯ç‰‡æ®µä¹‹é—´çš„è”ç³»æ–½åŠ äº†è¶Šæ¥è¶Šé‡çš„è´Ÿæ‹…ï¼Œå¾’åŠ³çš„æ¨ç†æ­¥éª¤è¿›ä¸€æ­¥åŠ å‰§äº†è¿™ä¸€è¿‡è½½é—®é¢˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†KnowTraceï¼Œè¿™æ˜¯ä¸€ä¸ªä¼˜é›…çš„RAGæ¡†æ¶ï¼Œæ—¨åœ¨ï¼ˆ1ï¼‰å‡è½»ä¸Šä¸‹æ–‡è¿‡è½½é—®é¢˜ï¼›ï¼ˆ2ï¼‰å¯åŠ¨æ›´é«˜è´¨é‡çš„å¤šæ­¥æ¨ç†ã€‚KnowTraceå¹¶æ²¡æœ‰ç®€å•åœ°å †ç Œæ£€ç´¢å†…å®¹ï¼Œè€Œæ˜¯è‡ªä¸»åœ°è¿½è¸ªæ‰€éœ€çš„çŸ¥è¯†ä¸‰å…ƒç»„ï¼Œä»¥ç»„ç»‡å‡ºä¸è¾“å…¥é—®é¢˜ç›¸å…³çš„ç‰¹å®šçŸ¥è¯†å›¾è°±ã€‚è¿™ç§ç»“æ„åŒ–çš„å·¥ä½œæµç¨‹ä¸ä»…ä¸ºLLMæä¾›äº†ç”¨äºæ¨æ–­çš„å¯ç†è§£ä¸Šä¸‹æ–‡ï¼Œè€Œä¸”è¿˜è‡ªç„¶åœ°æ¿€å‘äº†çŸ¥è¯†å›æº¯çš„åæ€æœºåˆ¶ï¼Œä»¥è¯†åˆ«ä½œä¸ºè¿‡ç¨‹ç›‘ç£æ•°æ®çš„è´¡çŒ®LLMä¸–ä»£ï¼Œä»¥å®ç°è‡ªæˆ‘å¼•å¯¼ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒKnowTraceåœ¨ä¸‰ä¸ªå¤šè·³é—®é¢˜å›ç­”åŸºå‡†æµ‹è¯•ä¸Šå§‹ç»ˆè¶…è¿‡ç°æœ‰æ–¹æ³•ï¼Œå¹¶ä¸”å¯åŠ¨çš„ç‰ˆæœ¬è¿›ä¸€æ­¥æ‰©å¤§äº†æ”¶ç›Šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20245v1">PDF</a> Accepted by KDD 2025</p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹é€šè¿‡æ£€ç´¢å¢å¼ºç”ŸæˆæŠ€æœ¯å¤„ç†å¤æ‚å¤šè·³é—®é¢˜æ—¶ï¼Œé¢ä¸´ä¸Šä¸‹æ–‡ä¿¡æ¯è¿‡è½½çš„é—®é¢˜ã€‚æœ¬æ–‡æå‡ºKnowTraceæ¡†æ¶ï¼Œé€šè¿‡è¿½è¸ªæ‰€éœ€çŸ¥è¯†ä¸‰å…ƒç»„ç»„ç»‡ç‰¹å®šçŸ¥è¯†å›¾è°±ï¼Œç¼“è§£ä¸Šä¸‹æ–‡ä¿¡æ¯è¿‡è½½é—®é¢˜ï¼Œå¹¶ä¿ƒè¿›é«˜è´¨é‡å¤šæ­¥æ¨ç†ã€‚KnowTraceæ¡†æ¶è‡ªä¸»æ„å»ºçŸ¥è¯†å›¾è°±ï¼Œæä¾›æ˜“äºç†è§£çš„ä¸Šä¸‹æ–‡è¿›è¡Œæ¨ç†ï¼Œå¹¶é€šè¿‡çŸ¥è¯†å›æº¯æœºåˆ¶è‡ªæˆ‘æå‡ã€‚å®éªŒè¡¨æ˜ï¼ŒKnowTraceåœ¨ä¸‰ä¸ªå¤šè·³é—®ç­”åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å¤æ‚å¤šè·³é—®é¢˜æ—¶ï¼Œä¸Šä¸‹æ–‡ä¿¡æ¯è¿‡è½½æ˜¯ä¸€ä¸ªå…³é”®é—®é¢˜ã€‚</li>
<li>KnowTraceæ¡†æ¶é€šè¿‡è¿½è¸ªæ‰€éœ€çŸ¥è¯†ä¸‰å…ƒç»„æ¥ç¼“è§£ä¸Šä¸‹æ–‡ä¿¡æ¯è¿‡è½½é—®é¢˜ã€‚</li>
<li>KnowTraceèƒ½å¤Ÿè‡ªä¸»æ„å»ºä¸è¾“å…¥é—®é¢˜ç›¸å…³çš„ç‰¹å®šçŸ¥è¯†å›¾è°±ã€‚</li>
<li>KnowTraceæ¡†æ¶æä¾›äº†ä¸€ç§æ˜“äºç†è§£çš„ä¸Šä¸‹æ–‡ï¼Œå¢å¼ºäº†å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>KnowTraceé€šè¿‡çŸ¥è¯†å›æº¯æœºåˆ¶ï¼Œå®ç°äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„è‡ªæˆ‘æå‡ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒKnowTraceåœ¨å¤šä¸ªå¤šè·³é—®ç­”æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20245">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c24e065743e7517b023e0bd03beb5f40.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1e24f51148b619a516a6504fbd1afebf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-39aa66edee9ad516e537b8148898c64e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4660cf2b154da9ea9db6fd28d0be3044.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4e8dc923e9653e553a19125e2107f6d7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0b4d2f9edcdca99a6c759d62ba047a5c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a407e70e2eafde68cb76f0be9a57e24b.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="RedAHD-Reduction-Based-End-to-End-Automatic-Heuristic-Design-with-Large-Language-Models"><a href="#RedAHD-Reduction-Based-End-to-End-Automatic-Heuristic-Design-with-Large-Language-Models" class="headerlink" title="RedAHD: Reduction-Based End-to-End Automatic Heuristic Design with Large   Language Models"></a>RedAHD: Reduction-Based End-to-End Automatic Heuristic Design with Large   Language Models</h2><p><strong>Authors:Nguyen Thach, Aida Riahifar, Nathan Huynh, Hau Chan</strong></p>
<p>Solving NP-hard combinatorial optimization problems (COPs) (e.g., traveling salesman problems (TSPs) and capacitated vehicle routing problems (CVRPs)) in practice traditionally involves handcrafting heuristics or specifying a search space for finding effective heuristics. The main challenges from these approaches, however, are the sheer amount of domain knowledge and implementation efforts required from human experts. Recently, significant progress has been made to address these challenges, particularly by using large language models (LLMs) to design heuristics within some predetermined generalized algorithmic framework (GAF, e.g., ant colony optimization and guided local search) for building key functions&#x2F;components (e.g., a priori information on how promising it is to include each edge in a solution for TSP and CVRP). Although existing methods leveraging this idea have shown to yield impressive optimization performance, they are not fully end-to-end and still require considerable manual interventions. In this paper, we propose a novel end-to-end framework, named RedAHD, that enables these LLM-based heuristic design methods to operate without the need of GAFs. More specifically, RedAHD employs LLMs to automate the process of reduction, i.e., transforming the COP at hand into similar COPs that are better-understood, from which LLM-based heuristic design methods can design effective heuristics for directly solving the transformed COPs and, in turn, indirectly solving the original COP. Our experimental results, evaluated on six COPs, show that RedAHD is capable of designing heuristics with competitive or improved results over the state-of-the-art methods with minimal human involvement. </p>
<blockquote>
<p>è§£å†³NPéš¾çš„ç»„åˆä¼˜åŒ–é—®é¢˜ï¼ˆCOPsï¼‰ï¼ˆä¾‹å¦‚æ—…è¡Œæ¨é”€å‘˜é—®é¢˜ï¼ˆTSPï¼‰å’Œå®¹é‡è½¦è¾†è·¯å¾„é—®é¢˜ï¼ˆCVRPï¼‰ï¼‰åœ¨å®è·µä¸­é€šå¸¸æ¶‰åŠæ‰‹å·¥åˆ¶ä½œå¯å‘å¼æ–¹æ³•æˆ–æŒ‡å®šä¸€ä¸ªæœç´¢ç©ºé—´ä»¥æ‰¾åˆ°æœ‰æ•ˆçš„å¯å‘å¼æ–¹æ³•ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜æ˜¯ï¼šéœ€è¦å¤§é‡é¢†åŸŸçŸ¥è¯†å’Œå®ç°åŠªåŠ›æ¥è‡ªäººç±»ä¸“å®¶ã€‚æœ€è¿‘ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é¢„å®šçš„é€šç”¨ç®—æ³•æ¡†æ¶ï¼ˆGAFï¼Œä¾‹å¦‚èšç¾¤ä¼˜åŒ–å’Œå¼•å¯¼å±€éƒ¨æœç´¢ï¼‰å†…è®¾è®¡å¯å‘å¼æ–¹æ³•æ¥æ„å»ºå…³é”®åŠŸèƒ½&#x2F;ç»„ä»¶æ–¹é¢å–å¾—äº†é‡å¤§è¿›å±•ï¼ˆä¾‹å¦‚ï¼Œæœ‰å…³TSPå’ŒCVRPçš„è§£å†³æ–¹æ¡ˆä¸­åŒ…å«æ¯æ¡è¾¹çš„æ½œåœ¨å‰æ™¯çš„å…ˆéªŒä¿¡æ¯ï¼‰ã€‚å°½ç®¡ç°æœ‰çš„åˆ©ç”¨è¿™ä¸€æ€æƒ³çš„æ–¹æ³•å·²ç»æ˜¾ç¤ºå‡ºä»¤äººå°è±¡æ·±åˆ»çš„ä¼˜åŒ–æ€§èƒ½ï¼Œä½†å®ƒä»¬å¹¶ä¸å®Œå…¨ç«¯åˆ°ç«¯ï¼Œä»ç„¶éœ€è¦å¤§é‡çš„æ‰‹åŠ¨å¹²é¢„ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹ç«¯åˆ°ç«¯æ¡†æ¶RedAHDï¼Œä½¿è¿™äº›åŸºäºLLMçš„å¯å‘å¼è®¾è®¡æ–¹æ³•èƒ½å¤Ÿåœ¨ä¸éœ€è¦GAFçš„æƒ…å†µä¸‹è¿›è¡Œæ“ä½œã€‚æ›´å…·ä½“åœ°è¯´ï¼ŒRedAHDé‡‡ç”¨LLMè‡ªåŠ¨åŒ–ç¼©å‡è¿‡ç¨‹ï¼Œå³å°†æ‰‹å¤´çš„COPè½¬æ¢ä¸ºæ›´æ˜“äºç†è§£çš„ç›¸ä¼¼COPï¼ŒåŸºäºLLMçš„å¯å‘å¼è®¾è®¡æ–¹æ³•å¯ä»¥ç›´æ¥ä¸ºè½¬æ¢åçš„COPè®¾è®¡æœ‰æ•ˆçš„å¯å‘å¼æ–¹æ³•ï¼Œä»è€Œé—´æ¥è§£å†³åŸå§‹çš„COPã€‚æˆ‘ä»¬åœ¨å…­ç§COPä¸Šè¿›è¡Œçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒRedAHDè®¾è®¡çš„å¯å‘å¼æ–¹æ³•ä¸æœ€æ–°æŠ€æœ¯ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›æˆ–æ›´ä½³çš„ç»“æœï¼Œå¹¶ä¸”å‡ ä¹ä¸éœ€è¦äººå·¥å‚ä¸ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20242v1">PDF</a> Under Review</p>
<p><strong>Summary</strong></p>
<p>åœ¨è§£å†³NPéš¾çš„ç»„åˆä¼˜åŒ–é—®é¢˜ï¼ˆå¦‚æ—…è¡Œå•†é—®é¢˜å’Œå¸¦å®¹é‡çº¦æŸçš„è½¦è¾†è·¯å¾„é—®é¢˜ï¼‰æ—¶ï¼Œä¼ ç»Ÿçš„æ–¹æ³•é€šå¸¸éœ€è¦äººç±»ä¸“å®¶æ‰‹åŠ¨è®¾è®¡å¯å‘å¼ç®—æ³•æˆ–åœ¨é¢„å®šçš„é€šç”¨ç®—æ³•æ¡†æ¶å†…å¯»æ‰¾æœ‰æ•ˆçš„å¯å‘å¼ç®—æ³•ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•éœ€è¦å¤§é‡çš„é¢†åŸŸçŸ¥è¯†å’Œå®æ–½åŠªåŠ›ã€‚æœ€è¿‘ï¼Œé€šè¿‡ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥è®¾è®¡å¯å‘å¼ç®—æ³•ï¼Œå·²ç»å–å¾—äº†æ˜¾è‘—çš„è¿›å±•ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹ç«¯åˆ°ç«¯æ¡†æ¶RedAHDï¼Œè¯¥æ¡†æ¶ä½¿ç”¨LLMè‡ªåŠ¨åŒ–å‡å°‘ç»„åˆä¼˜åŒ–é—®é¢˜çš„å¤æ‚æ€§ï¼Œä»è€Œæ— éœ€é€šç”¨ç®—æ³•æ¡†æ¶å³å¯è®¾è®¡å¯å‘å¼ç®—æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRedAHDèƒ½å¤Ÿåœ¨å…­ä¸ªç»„åˆä¼˜åŒ–é—®é¢˜ä¸Šè®¾è®¡å…·æœ‰ç«äº‰åŠ›çš„å¯å‘å¼ç®—æ³•ï¼Œä¸”åªéœ€æœ€å°‘çš„äººå·¥å‚ä¸ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§£å†³NPéš¾çš„ç»„åˆä¼˜åŒ–é—®é¢˜ä¼ ç»Ÿä¸Šéœ€è¦æ‰‹åŠ¨è®¾è®¡å¯å‘å¼ç®—æ³•æˆ–å¯»æ‰¾æœ‰æ•ˆçš„å¯å‘å¼ç®—æ³•ï¼Œè¿™éœ€è¦å¤§é‡çš„é¢†åŸŸçŸ¥è¯†å’Œå®æ–½åŠªåŠ›ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è§£å†³è¿™äº›æŒ‘æˆ˜æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œå°¤å…¶æ˜¯åœ¨è®¾è®¡å¯å‘å¼ç®—æ³•æ–¹é¢ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹ç«¯åˆ°ç«¯æ¡†æ¶RedAHDï¼Œè¯¥æ¡†æ¶ä½¿ç”¨LLMè‡ªåŠ¨åŒ–å‡å°‘é—®é¢˜çš„å¤æ‚æ€§ï¼Œä»è€Œæ— éœ€é€šç”¨ç®—æ³•æ¡†æ¶å³å¯è®¾è®¡å¯å‘å¼ç®—æ³•ã€‚</li>
<li>RedAHDæ¡†æ¶å¯ä»¥ç›´æ¥è§£å†³è½¬æ¢åçš„ç»„åˆä¼˜åŒ–é—®é¢˜ï¼Œä»è€Œé—´æ¥è§£å†³åŸå§‹é—®é¢˜ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒRedAHDèƒ½å¤Ÿåœ¨å¤šä¸ªç»„åˆä¼˜åŒ–é—®é¢˜ä¸Šè®¾è®¡å…·æœ‰ç«äº‰åŠ›çš„å¯å‘å¼ç®—æ³•ã€‚</li>
<li>RedAHDè®¾è®¡çš„å¯å‘å¼ç®—æ³•åœ¨ç»“æœä¸Šå¯ä»¥ä¸ç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸ç«äº‰æˆ–æœ‰æ‰€æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20242">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b2026664beacc0264d712374764a1838.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-10a753d8d7137b6e5ad161eeee9bd49b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ad8b91548af532cb0d3b1a0284f0d18e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1e93766abb75a826f01f37f05338b37e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-15fa9f54b80a97e3deb76953f0d8383e.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="DreamPRM-Domain-Reweighted-Process-Reward-Model-for-Multimodal-Reasoning"><a href="#DreamPRM-Domain-Reweighted-Process-Reward-Model-for-Multimodal-Reasoning" class="headerlink" title="DreamPRM: Domain-Reweighted Process Reward Model for Multimodal   Reasoning"></a>DreamPRM: Domain-Reweighted Process Reward Model for Multimodal   Reasoning</h2><p><strong>Authors:Qi Cao, Ruiyi Wang, Ruiyi Zhang, Sai Ashish Somayajula, Pengtao Xie</strong></p>
<p>Reasoning has substantially improved the performance of large language models (LLMs) on complicated tasks. Central to the current reasoning studies, Process Reward Models (PRMs) offer a fine-grained evaluation of intermediate reasoning steps and guide the reasoning process. However, extending PRMs to multimodal large language models (MLLMs) introduces challenges. Since multimodal reasoning covers a wider range of tasks compared to text-only scenarios, the resulting distribution shift from the training to testing sets is more severe, leading to greater generalization difficulty. Training a reliable multimodal PRM, therefore, demands large and diverse datasets to ensure sufficient coverage. However, current multimodal reasoning datasets suffer from a marked quality imbalance, which degrades PRM performance and highlights the need for an effective data selection strategy. To address the issues, we introduce DreamPRM, a domain-reweighted training framework for multimodal PRMs which employs bi-level optimization. In the lower-level optimization, DreamPRM performs fine-tuning on multiple datasets with domain weights, allowing the PRM to prioritize high-quality reasoning signals and alleviating the impact of dataset quality imbalance. In the upper-level optimization, the PRM is evaluated on a separate meta-learning dataset; this feedback updates the domain weights through an aggregation loss function, thereby improving the generalization capability of trained PRM. Extensive experiments on multiple multimodal reasoning benchmarks covering both mathematical and general reasoning show that test-time scaling with DreamPRM consistently improves the performance of state-of-the-art MLLMs. Further comparisons reveal that DreamPRMâ€™s domain-reweighting strategy surpasses other data selection methods and yields higher accuracy gains than existing test-time scaling approaches. </p>
<blockquote>
<p>æ¨ç†æŠ€æœ¯å·²æ˜¾è‘—æé«˜äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤æ‚ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚å½“å‰çš„æ¨ç†ç ”ç©¶çš„æ ¸å¿ƒï¼Œè¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰å¯¹ä¸­é—´æ¨ç†æ­¥éª¤è¿›è¡Œäº†ç²¾ç»†è¯„ä¼°ï¼Œå¹¶å¼•å¯¼äº†æ¨ç†è¿‡ç¨‹ã€‚ç„¶è€Œï¼Œå°†PRMæ‰©å±•åˆ°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚ç”±äºå¤šæ¨¡æ€æ¨ç†æ¶µç›–äº†ä¸çº¯æ–‡æœ¬åœºæ™¯ç›¸æ¯”æ›´å¹¿æ³›çš„èŒƒå›´çš„ä»»åŠ¡ï¼Œä»è®­ç»ƒé›†åˆ°æµ‹è¯•é›†çš„åˆ†å¸ƒå˜åŒ–æ›´åŠ ä¸¥é‡ï¼Œå¯¼è‡´æ›´å¤§çš„æ³›åŒ–éš¾åº¦ã€‚å› æ­¤ï¼Œè®­ç»ƒå¯é çš„å¤šæ¨¡æ€PRMéœ€è¦å¤§å‹ä¸”å¤šæ ·åŒ–çš„æ•°æ®é›†ä»¥ç¡®ä¿è¶³å¤Ÿçš„è¦†ç›–èŒƒå›´ã€‚ç„¶è€Œï¼Œå½“å‰çš„å¤šæ¨¡æ€æ¨ç†æ•°æ®é›†å­˜åœ¨æ˜æ˜¾çš„è´¨é‡ä¸å¹³è¡¡é—®é¢˜ï¼Œè¿™é™ä½äº†PRMçš„æ€§èƒ½ï¼Œå¹¶çªå‡ºäº†å¯¹æœ‰æ•ˆæ•°æ®é€‰æ‹©ç­–ç•¥çš„éœ€æ±‚ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†DreamPRMï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šæ¨¡æ€PRMçš„åŸŸåŠ æƒè®­ç»ƒæ¡†æ¶ï¼Œå®ƒé‡‡ç”¨åŒå±‚ä¼˜åŒ–ã€‚åœ¨ä¸‹å±‚ä¼˜åŒ–ä¸­ï¼ŒDreamPRMåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒå¹¶ä½¿ç”¨åŸŸæƒé‡ï¼Œä½¿PRMèƒ½å¤Ÿä¼˜å…ˆè€ƒè™‘é«˜è´¨é‡æ¨ç†ä¿¡å·ï¼Œå¹¶å‡è½»æ•°æ®é›†è´¨é‡ä¸å¹³è¡¡çš„å½±å“ã€‚åœ¨ä¸Šå±‚ä¼˜åŒ–ä¸­ï¼ŒPRMåœ¨ä¸€ä¸ªå•ç‹¬çš„å…ƒå­¦ä¹ æ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°ï¼›æ­¤åé¦ˆé€šè¿‡èšåˆæŸå¤±å‡½æ•°æ›´æ–°åŸŸæƒé‡ï¼Œä»è€Œæé«˜è®­ç»ƒæœ‰ç´ çš„PRMçš„æ³›åŒ–èƒ½åŠ›ã€‚åœ¨æ¶µç›–æ•°å­¦å’Œä¸€èˆ¬æ¨ç†çš„å¤šä¸ªå¤šæ¨¡æ€æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨DreamPRMçš„æµ‹è¯•æ—¶é—´ç¼©æ”¾ä¸€è‡´åœ°æé«˜äº†æœ€å…ˆè¿›çš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹æ€§èƒ½ã€‚è¿›ä¸€æ­¥çš„æ¯”è¾ƒè¡¨æ˜ï¼ŒDreamPRMçš„åŸŸé‡æƒç­–ç•¥ä¼˜äºå…¶ä»–æ•°æ®é€‰æ‹©æ–¹æ³•ï¼Œå¹¶äº§ç”Ÿäº†é«˜äºç°æœ‰æµ‹è¯•æ—¶é—´ç¼©æ”¾æ–¹æ³•çš„å‡†ç¡®æ€§å¢ç›Šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20241v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤æ‚ä»»åŠ¡ä¸Šçš„è¡¨ç°é€šè¿‡å¼•å…¥æ¨ç†æœºåˆ¶å¾—åˆ°äº†æ˜¾è‘—æå‡ã€‚è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰ä½œä¸ºå½“å‰æ¨ç†ç ”ç©¶çš„ä¸­å¿ƒï¼Œèƒ½å¯¹ä¸­é—´æ¨ç†æ­¥éª¤è¿›è¡Œç²¾ç»†è¯„ä¼°å¹¶å¼•å¯¼æ¨ç†è¿‡ç¨‹ã€‚ç„¶è€Œï¼Œå°†PRMæ‰©å±•åˆ°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚å¤šæ¨¡æ€æ¨ç†æ¶µç›–çš„ä»»åŠ¡èŒƒå›´æ›´å¹¿ï¼Œå¯¼è‡´ä»è®­ç»ƒé›†åˆ°æµ‹è¯•é›†çš„åˆ†å¸ƒè½¬ç§»æ›´åŠ ä¸¥é‡ï¼ŒåŠ å¤§äº†æ³›åŒ–éš¾åº¦ã€‚å› æ­¤ï¼Œè®­ç»ƒå¯é çš„å¤šæ¨¡æ€PRMéœ€è¦å¤§è§„æ¨¡ã€å¤šæ ·åŒ–çš„æ•°æ®é›†ä»¥ç¡®ä¿å……åˆ†è¦†ç›–ã€‚ç„¶è€Œï¼Œå½“å‰çš„å¤šæ¨¡æ€æ¨ç†æ•°æ®é›†å­˜åœ¨æ˜æ˜¾è´¨é‡ä¸å¹³è¡¡é—®é¢˜ï¼Œé™ä½äº†PRMçš„æ€§èƒ½ï¼Œå‡¸æ˜¾å‡ºæœ‰æ•ˆæ•°æ®é€‰æ‹©ç­–ç•¥çš„å¿…è¦æ€§ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†DreamPRMï¼Œä¸€ç§å¤šæ¨¡æ€PRMçš„é¢†åŸŸåŠ æƒè®­ç»ƒæ¡†æ¶ï¼Œé‡‡ç”¨ä¸¤çº§ä¼˜åŒ–ã€‚DreamPRMé€šè¿‡ç²¾ç»†è°ƒæ•´å¤šä¸ªæ•°æ®é›†ä¸Šçš„é¢†åŸŸæƒé‡æ¥è¿›è¡Œä½çº§ä¼˜åŒ–ï¼Œä½¿PRMèƒ½å¤Ÿä¼˜å…ˆè·å–é«˜è´¨é‡çš„æ¨ç†ä¿¡å·ï¼Œå‡è½»æ•°æ®é›†è´¨é‡ä¸å¹³è¡¡çš„å½±å“ã€‚åœ¨é«˜çº§ä¼˜åŒ–ä¸­ï¼ŒPRMåœ¨å•ç‹¬çš„å…ƒå­¦ä¹ æ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œé€šè¿‡èšåˆæŸå¤±å‡½æ•°æ›´æ–°é¢†åŸŸæƒé‡ï¼Œä»è€Œæé«˜è®­ç»ƒå¥½çš„PRMçš„æ³›åŒ–èƒ½åŠ›ã€‚åœ¨å¤šä¸ªæ¶µç›–æ•°å­¦å’Œé€šç”¨æ¨ç†çš„å¤šæ¨¡æ€æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨DreamPRMçš„æµ‹è¯•æ—¶é—´ç¼©æ”¾ä¸€è‡´åœ°æé«˜äº†æœ€å…ˆè¿›çš„å¤šæ¨¡æ€LLMçš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¨ç†æœºåˆ¶å·²æ˜¾è‘—æå‡äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤æ‚ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚</li>
<li>è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰èƒ½å¤Ÿç²¾ç»†è¯„ä¼°ä¸­é—´æ¨ç†æ­¥éª¤å¹¶å¼•å¯¼æ¨ç†è¿‡ç¨‹ã€‚</li>
<li>å°†PRMæ‰©å±•åˆ°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰æ—¶å­˜åœ¨æŒ‘æˆ˜ï¼Œå› ä¸ºå¤šæ¨¡æ€æ¨ç†æ¶µç›–æ›´å¹¿æ³›çš„ä»»åŠ¡èŒƒå›´ã€‚</li>
<li>å¤šæ¨¡æ€æ¨ç†æ•°æ®é›†å­˜åœ¨è´¨é‡ä¸å¹³è¡¡é—®é¢˜ï¼Œéœ€è¦æœ‰æ•ˆæ•°æ®é€‰æ‹©ç­–ç•¥ã€‚</li>
<li>DreamPRMæ˜¯ä¸€ç§å¤šæ¨¡æ€PRMçš„é¢†åŸŸåŠ æƒè®­ç»ƒæ¡†æ¶ï¼Œé‡‡ç”¨ä¸¤çº§ä¼˜åŒ–æ¥æé«˜PRMsçš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>DreamPRMé€šè¿‡ç²¾ç»†è°ƒæ•´é¢†åŸŸæƒé‡æ¥åº”å¯¹æ•°æ®é›†è´¨é‡ä¸å¹³è¡¡é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20241">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b9e5e0d38a0b5471b3dac35dd2399794.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2aaeb4e6f7a0e663b1cf1b21d1333114.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bed56c293da61098555fea3d3ff897f0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ca768b8bb8003dfc3cec794ae8f9eb01.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="FLAME-MoE-A-Transparent-End-to-End-Research-Platform-for-Mixture-of-Experts-Language-Models"><a href="#FLAME-MoE-A-Transparent-End-to-End-Research-Platform-for-Mixture-of-Experts-Language-Models" class="headerlink" title="FLAME-MoE: A Transparent End-to-End Research Platform for   Mixture-of-Experts Language Models"></a>FLAME-MoE: A Transparent End-to-End Research Platform for   Mixture-of-Experts Language Models</h2><p><strong>Authors:Hao Kang, Zichun Yu, Chenyan Xiong</strong></p>
<p>Recent large language models such as Gemini-1.5, DeepSeek-V3, and Llama-4 increasingly adopt Mixture-of-Experts (MoE) architectures, which offer strong efficiency-performance trade-offs by activating only a fraction of the model per token. Yet academic researchers still lack a fully open, end-to-end MoE platform for investigating scaling, routing, and expert behavior. We release FLAME-MoE, a completely open-source research suite composed of seven decoder-only models, ranging from 38M to 1.7B active parameters, whose architectureâ€“64 experts with top-8 gating and 2 shared expertsâ€“closely reflects modern production LLMs. All training data pipelines, scripts, logs, and checkpoints are publicly available to enable reproducible experimentation. Across six evaluation tasks, FLAME-MoE improves average accuracy by up to 3.4 points over dense baselines trained with identical FLOPs. Leveraging full training trace transparency, we present initial analyses showing that (i) experts increasingly specialize on distinct token subsets, (ii) co-activation matrices remain sparse, reflecting diverse expert usage, and (iii) routing behavior stabilizes early in training. All code, training logs, and model checkpoints are available at <a target="_blank" rel="noopener" href="https://github.com/cmu-flame/FLAME-MoE">https://github.com/cmu-flame/FLAME-MoE</a>. </p>
<blockquote>
<p>æœ€è¿‘çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå¦‚Gemini-1.5ã€DeepSeek-V3å’ŒLlama-4ï¼Œè¶Šæ¥è¶Šé‡‡ç”¨Mixture-of-Expertsï¼ˆMoEï¼‰æ¶æ„ã€‚è¿™ç§æ¶æ„é€šè¿‡æ¯ä»¤ç‰Œä»…æ¿€æ´»æ¨¡å‹çš„ä¸€éƒ¨åˆ†ï¼Œå®ç°äº†å¼ºå¤§çš„æ•ˆç‡æ€§èƒ½æƒè¡¡ã€‚ç„¶è€Œï¼Œç ”ç©¶äººå‘˜ä»ç¼ºä¹ä¸€ä¸ªå®Œå…¨å¼€æ”¾çš„ç«¯åˆ°ç«¯MoEå¹³å°æ¥è°ƒæŸ¥è§„æ¨¡æ‰©å±•ã€è·¯ç”±å’Œä¸“å®¶è¡Œä¸ºã€‚æˆ‘ä»¬å‘å¸ƒFLAME-MoEï¼Œè¿™æ˜¯ä¸€ä¸ªå®Œå…¨å¼€æºçš„ç ”ç©¶å¥—ä»¶ï¼ŒåŒ…å«ä¸ƒä¸ªä»…è§£ç å™¨æ¨¡å‹ï¼Œæ´»è·ƒå‚æ•°èŒƒå›´ä»38Måˆ°1.7Bã€‚å…¶æ¶æ„åŒ…æ‹¬64ä¸ªä¸“å®¶ã€å‰8ä¸ªé—¨æ§å’Œ2ä¸ªå…±äº«ä¸“å®¶ï¼Œç´§å¯†åœ°åæ˜ äº†ç°ä»£ç”Ÿäº§å¤§å‹è¯­è¨€æ¨¡å‹ã€‚æ‰€æœ‰è®­ç»ƒæ•°æ®ç®¡é“ã€è„šæœ¬ã€æ—¥å¿—å’Œæ£€æŸ¥ç‚¹éƒ½å…¬å¼€å¯ç”¨ï¼Œä»¥ä¾¿è¿›è¡Œå¯é‡å¤çš„å®éªŒã€‚åœ¨å…­ä¸ªè¯„ä¼°ä»»åŠ¡ä¸­ï¼ŒFLAME-MoEåœ¨å¯†é›†åŸºå‡†çº¿ä¸Šå¹³å‡æé«˜äº†é«˜è¾¾3.4ç‚¹çš„å‡†ç¡®æ€§ï¼Œè¿™æ˜¯ä½¿ç”¨ç›¸åŒFLOPsè¿›è¡Œè®­ç»ƒçš„ã€‚åˆ©ç”¨å®Œå…¨çš„è®­ç»ƒè½¨è¿¹é€æ˜åº¦ï¼Œæˆ‘ä»¬åˆæ­¥åˆ†æè¡¨æ˜ï¼šï¼ˆiï¼‰ä¸“å®¶è¶Šæ¥è¶Šæ“…é•¿å¤„ç†ä¸åŒçš„ä»¤ç‰Œå­é›†ï¼›ï¼ˆiiï¼‰ååŒæ¿€æ´»çŸ©é˜µä¿æŒç¨€ç–ï¼Œåæ˜ äº†ä¸“å®¶çš„å¤šæ ·åŒ–ä½¿ç”¨ï¼›ï¼ˆiiiï¼‰è·¯ç”±è¡Œä¸ºåœ¨è®­ç»ƒæ—©æœŸè¶‹äºç¨³å®šã€‚æ‰€æœ‰ä»£ç ã€è®­ç»ƒæ—¥å¿—å’Œæ¨¡å‹æ£€æŸ¥ç‚¹å‡å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/cmu-flame/FLAME-MoE">https://github.com/cmu-flame/FLAME-MoE</a>è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20225v1">PDF</a> All code, training logs, and model checkpoints are available at   <a target="_blank" rel="noopener" href="https://github.com/cmu-flame/FLAME-MoE">https://github.com/cmu-flame/FLAME-MoE</a></p>
<p><strong>Summary</strong>ï¼š</p>
<p>è¿‘æœŸçš„å¤§å‹è¯­è¨€æ¨¡å‹å¦‚Gemini-1.5ã€DeepSeek-V3å’ŒLlama-4é‡‡ç”¨Mixture-of-Expertsï¼ˆMoEï¼‰æ¶æ„ï¼Œé€šè¿‡æ¯ä»¤ç‰Œä»…æ¿€æ´»æ¨¡å‹çš„ä¸€éƒ¨åˆ†æ¥å®ç°é«˜æ•ˆçš„æ€§èƒ½æƒè¡¡ã€‚ç„¶è€Œï¼Œç ”ç©¶äººå‘˜ä»ç¼ºä¹ä¸€ä¸ªå®Œå…¨å¼€æ”¾çš„ç«¯åˆ°ç«¯MoEå¹³å°æ¥æ¢ç©¶è§„æ¨¡æ‰©å±•ã€è·¯ç”±å’Œä¸“å®¶è¡Œä¸ºã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å‘å¸ƒäº†å®Œå…¨å¼€æºçš„ç ”ç©¶å¥—ä»¶FLAME-MoEï¼ŒåŒ…å«7ä¸ªè§£ç å™¨æ¨¡å‹ï¼Œæ´»è·ƒå‚æ•°èŒƒå›´ä»3.8äº¿åˆ°17äº¿ä¸ç­‰ã€‚å…¶æ¶æ„ç”±ç°ä»£ç”Ÿäº§å‹LLMå€Ÿé‰´ï¼Œé‡‡ç”¨ä¸“å®¶æ•°é‡è¾¾64ä¸ªï¼Œå‰å…«ä¸ªè¿›è¡Œé—¨æ§é€‰æ‹©ï¼Œå¹¶å…±äº«ä¸¤ä¸ªä¸“å®¶ã€‚æ‰€æœ‰è®­ç»ƒæ•°æ®ç®¡é“ã€è„šæœ¬ã€æ—¥å¿—å’Œæ£€æŸ¥ç‚¹éƒ½å·²å…¬å¼€ï¼Œä¾¿äºè¿›è¡Œå¯é‡å¤çš„å®éªŒã€‚åœ¨å…­ä¸ªè¯„ä¼°ä»»åŠ¡ä¸­ï¼ŒFLAME-MoEç›¸è¾ƒäºä½¿ç”¨ç›¸åŒæµ®ç‚¹è¿ç®—æ¬¡æ•°è®­ç»ƒçš„å¯†é›†åŸºå‡†æ¨¡å‹ï¼Œå¹³å‡å‡†ç¡®åº¦æé«˜äº†é«˜è¾¾3.4ä¸ªç‚¹ã€‚å€ŸåŠ©å®Œæ•´çš„è®­ç»ƒè½¨è¿¹é€æ˜åº¦ï¼Œæˆ‘ä»¬åˆæ­¥åˆ†æè¡¨æ˜ä¸“å®¶è¶Šæ¥è¶Šä¸“æ³¨äºç‰¹å®šçš„ä»¤ç‰Œå­é›†ï¼Œè”åˆæ¿€æ´»çŸ©é˜µä¿æŒç¨€ç–åæ˜ å¤šæ ·çš„ä¸“å®¶ä½¿ç”¨çŠ¶å†µï¼Œä¸”è·¯ç”±è¡Œä¸ºåœ¨è®­ç»ƒæ—©æœŸå°±ç¨³å®šä¸‹æ¥ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¼€å§‹é‡‡ç”¨Mixture-of-Expertsï¼ˆMoEï¼‰æ¶æ„æ¥æå‡æ•ˆç‡å’Œæ€§èƒ½ã€‚</li>
<li>ç¼ºä¹å…¨é¢çš„MoEå¹³å°æ¥æ·±å…¥ç ”ç©¶å…¶ç‰¹ç‚¹å¦‚è§„æ¨¡æ‰©å±•ã€è·¯ç”±å’Œä¸“å®¶è¡Œä¸ºã€‚</li>
<li>æ–°æ¨å‡ºçš„å¼€æºç ”ç©¶å¥—ä»¶FLAME-MoEæ—¨åœ¨è§£å†³ä¸Šè¿°é—®é¢˜ï¼ŒåŒ…å«ä¸€ç³»åˆ—è§£ç å™¨æ¨¡å‹ï¼Œå¹¶å…¬å¼€æ‰€æœ‰è®­ç»ƒæ•°æ®å’Œè„šæœ¬ã€‚</li>
<li>FLAME-MoEåœ¨å…­ä¸ªè¯„ä¼°ä»»åŠ¡ä¸­çš„æ€§èƒ½ä¼˜äºåŸºå‡†æ¨¡å‹ï¼Œå¹³å‡å‡†ç¡®åº¦æé«˜è¾¾3.4ä¸ªç‚¹ã€‚</li>
<li>ä¸“å®¶åˆ†ææ˜¾ç¤ºä¸åŒä¸“å®¶åœ¨ä¸åŒä»¤ç‰Œå­é›†ä¸Šçš„ä¸“ä¸šåˆ†å·¥è¶Šæ¥è¶Šæ˜ç¡®ã€‚</li>
<li>è”åˆæ¿€æ´»çŸ©é˜µä¿æŒç¨€ç–åæ˜ äº†ä¸“å®¶ä½¿ç”¨æ–¹å¼çš„å¤šæ ·æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20225">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-72369fdf7a3114be9e4ae4285a9f4cfb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a3ddc30a94e804e8d2b47929f7e80e06.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-161c65626169d3b0a7f6657f057289c1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-973849b868aa026876c28e372f72ced0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b07fcee53c6be84740568259f04aee1b.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Chain-of-Thought-for-Autonomous-Driving-A-Comprehensive-Survey-and-Future-Prospects"><a href="#Chain-of-Thought-for-Autonomous-Driving-A-Comprehensive-Survey-and-Future-Prospects" class="headerlink" title="Chain-of-Thought for Autonomous Driving: A Comprehensive Survey and   Future Prospects"></a>Chain-of-Thought for Autonomous Driving: A Comprehensive Survey and   Future Prospects</h2><p><strong>Authors:Yixin Cui, Haotian Lin, Shuo Yang, Yixiao Wang, Yanjun Huang, Hong Chen</strong></p>
<p>The rapid evolution of large language models in natural language processing has substantially elevated their semantic understanding and logical reasoning capabilities. Such proficiencies have been leveraged in autonomous driving systems, contributing to significant improvements in system performance. Models such as OpenAI o1 and DeepSeek-R1, leverage Chain-of-Thought (CoT) reasoning, an advanced cognitive method that simulates human thinking processes, demonstrating remarkable reasoning capabilities in complex tasks. By structuring complex driving scenarios within a systematic reasoning framework, this approach has emerged as a prominent research focus in autonomous driving, substantially improving the systemâ€™s ability to handle challenging cases. This paper investigates how CoT methods improve the reasoning abilities of autonomous driving models. Based on a comprehensive literature review, we present a systematic analysis of the motivations, methodologies, challenges, and future research directions of CoT in autonomous driving. Furthermore, we propose the insight of combining CoT with self-learning to facilitate self-evolution in driving systems. To ensure the relevance and timeliness of this study, we have compiled a dynamic repository of literature and open-source projects, diligently updated to incorporate forefront developments. The repository is publicly available at <a target="_blank" rel="noopener" href="https://github.com/cuiyx1720/Awesome-CoT4AD">https://github.com/cuiyx1720/Awesome-CoT4AD</a>. </p>
<blockquote>
<p>è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„å¤§å‹è¯­è¨€æ¨¡å‹è¿…é€Ÿè¿›åŒ–ï¼Œæå¤§åœ°æå‡äº†å…¶è¯­ä¹‰ç†è§£å’Œé€»è¾‘æ¨ç†èƒ½åŠ›ã€‚è¿™ç§èƒ½åŠ›å·²è¢«åº”ç”¨äºè‡ªåŠ¨é©¾é©¶ç³»ç»Ÿï¼Œä¸ºç³»ç»Ÿæ€§èƒ½å¸¦æ¥äº†æ˜¾è‘—æ”¹è¿›ã€‚è¯¸å¦‚OpenAI o1å’ŒDeepSeek-R1ç­‰æ¨¡å‹ï¼Œé‡‡ç”¨æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†ï¼Œè¿™æ˜¯ä¸€ç§é«˜çº§è®¤çŸ¥æ–¹æ³•ï¼Œæ¨¡æ‹Ÿäººç±»æ€ç»´è¿‡ç¨‹ï¼Œåœ¨å¤æ‚ä»»åŠ¡ä¸­å±•ç°å‡ºå“è¶Šçš„æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡åœ¨ä¸€ä¸ªç³»ç»Ÿçš„æ¨ç†æ¡†æ¶å†…æ„å»ºå¤æ‚çš„é©¾é©¶åœºæ™¯ï¼Œè¿™ç§æ–¹æ³•å·²æˆä¸ºè‡ªåŠ¨é©¾é©¶çš„ä¸€ä¸ªä¸»è¦ç ”ç©¶æ–¹å‘ï¼Œæå¤§åœ°æé«˜äº†ç³»ç»Ÿå¤„ç†æŒ‘æˆ˜æ¡ˆä¾‹çš„èƒ½åŠ›ã€‚æœ¬æ–‡ç ”ç©¶äº†CoTæ–¹æ³•å¦‚ä½•æ”¹å–„è‡ªåŠ¨é©¾é©¶æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚åŸºäºå…¨é¢çš„æ–‡çŒ®ç»¼è¿°ï¼Œæˆ‘ä»¬å¯¹CoTåœ¨è‡ªåŠ¨é©¾é©¶ä¸­çš„åŠ¨æœºã€æ–¹æ³•ã€æŒ‘æˆ˜å’Œæœªæ¥ç ”ç©¶æ–¹å‘è¿›è¡Œäº†ç³»ç»Ÿçš„åˆ†æã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†å°†CoTä¸è‡ªæˆ‘å­¦ä¹ ç›¸ç»“åˆï¼Œä»¥ä¿ƒè¿›é©¾é©¶ç³»ç»Ÿçš„è‡ªæˆ‘è¿›åŒ–çš„è§è§£ã€‚ä¸ºç¡®ä¿ç ”ç©¶çš„æ—¶æ•ˆæ€§å’Œæ—¶æ•ˆæ€§ï¼Œæˆ‘ä»¬ç²¾å¿ƒæ”¶é›†å’Œæ›´æ–°å‰æ²¿æ–‡çŒ®å’Œå¼€æºé¡¹ç›®ï¼Œå¹¶å°†å…¶åŠ¨æ€ç¼–å…¥çŸ¥è¯†åº“ï¼Œä¾›å…¬ä¼—è®¿é—®ã€‚çŸ¥è¯†åº“å¯é€šè¿‡ä»¥ä¸‹é“¾æ¥è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/cuiyx1720/Awesome-CoT4AD">https://github.com/cuiyx1720/Awesome-CoT4AD</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20223v1">PDF</a> 18 pages, 6 figures</p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸçš„å¿«é€Ÿå‘å±•æ˜¾è‘—æå‡äº†å…¶è¯­ä¹‰ç†è§£å’Œé€»è¾‘æ¨ç†èƒ½åŠ›ã€‚è¿™äº›èƒ½åŠ›å·²è¢«åº”ç”¨äºè‡ªåŠ¨é©¾é©¶ç³»ç»Ÿï¼Œæå¤§åœ°æå‡äº†ç³»ç»Ÿæ€§èƒ½ã€‚æœ¬æ–‡æ¢è®¨äº†å¦‚ä½•é€šè¿‡é“¾å¼æ€ç»´ï¼ˆChain-of-Thoughtï¼ŒCoTï¼‰æ–¹æ³•æé«˜è‡ªåŠ¨é©¾é©¶æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œå±•ç°äº†ä¸€ç§æ¨¡æ‹Ÿäººç±»æ€ç»´è¿‡ç¨‹çš„å…ˆè¿›è®¤çŸ¥æ–¹æ³•ã€‚é€šè¿‡ç³»ç»Ÿåˆ†æCoTæ–¹æ³•åœ¨è‡ªåŠ¨é©¾é©¶ä¸­çš„åŠ¨æœºã€æ–¹æ³•ã€æŒ‘æˆ˜å’Œæœªæ¥ç ”ç©¶æ–¹å‘ï¼Œç»“åˆè‡ªæˆ‘å­¦ä¹ æ–¹æ³•çš„èåˆï¼Œä¿ƒè¿›é©¾é©¶ç³»ç»Ÿçš„è‡ªæˆ‘è¿›åŒ–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹è¯­ä¹‰ç†è§£å’Œé€»è¾‘æ¨ç†èƒ½åŠ›çš„æ˜¾è‘—æå‡ã€‚</li>
<li>è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿä¸­åº”ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæå¤§æå‡äº†ç³»ç»Ÿæ€§èƒ½ã€‚</li>
<li>é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ–¹æ³•æ¨¡æ‹Ÿäººç±»æ€ç»´è¿‡ç¨‹ï¼Œå±•ç°å¼ºå¤§æ¨ç†èƒ½åŠ›ã€‚</li>
<li>CoTæ–¹æ³•åœ¨è‡ªåŠ¨é©¾é©¶ä¸­çš„åŠ¨æœºã€æ–¹æ³•ã€æŒ‘æˆ˜å’Œæœªæ¥ç ”ç©¶æ–¹å‘çš„ç³»ç»Ÿåˆ†æã€‚</li>
<li>CoTæ–¹æ³•ä¸è‡ªæˆ‘å­¦ä¹ æ–¹æ³•çš„ç»“åˆï¼Œä¿ƒè¿›é©¾é©¶ç³»ç»Ÿçš„è‡ªæˆ‘è¿›åŒ–ã€‚</li>
<li>è®ºæ–‡é€šè¿‡æ–‡çŒ®ç»¼è¿°å’Œå¼€æºé¡¹ç›®åŠ¨æ€æ›´æ–°ï¼Œç¡®ä¿ç ”ç©¶çš„æ—¶æ•ˆæ€§å’Œç›¸å…³æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20223">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-6bf4d112e9d1ee81a0a7c5c86de5a43e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8351ccee770ea1336176452c940b27e8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-20dc47b61a003a41b36558ef6d97b223.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6b5f094c1df1d8db9e108fa7112d47a9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ac7c405680891f6eefdda6f69e59a8b4.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Fine-grained-List-wise-Alignment-for-Generative-Medication-Recommendation"><a href="#Fine-grained-List-wise-Alignment-for-Generative-Medication-Recommendation" class="headerlink" title="Fine-grained List-wise Alignment for Generative Medication   Recommendation"></a>Fine-grained List-wise Alignment for Generative Medication   Recommendation</h2><p><strong>Authors:Chenxiao Fan, Chongming Gao, Wentao Shi, Yaxin Gong, Zihao Zhao, Fuli Feng</strong></p>
<p>Accurate and safe medication recommendations are critical for effective clinical decision-making, especially in multimorbidity cases. However, existing systems rely on point-wise prediction paradigms that overlook synergistic drug effects and potential adverse drug-drug interactions (DDIs). We propose FLAME, a fine-grained list-wise alignment framework for large language models (LLMs), enabling drug-by-drug generation of drug lists. FLAME formulates recommendation as a sequential decision process, where each step adds or removes a single drug. To provide fine-grained learning signals, we devise step-wise Group Relative Policy Optimization (GRPO) with potential-based reward shaping, which explicitly models DDIs and optimizes the contribution of each drug to the overall prescription. Furthermore, FLAME enhances patient modeling by integrating structured clinical knowledge and collaborative information into the representation space of LLMs. Experiments on benchmark datasets demonstrate that FLAME achieves state-of-the-art performance, delivering superior accuracy, controllable safety-accuracy trade-offs, and strong generalization across diverse clinical scenarios. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/cxfann/Flame">https://github.com/cxfann/Flame</a>. </p>
<blockquote>
<p>å‡†ç¡®ä¸”å®‰å…¨çš„ç”¨è¯æ¨èå¯¹äºæœ‰æ•ˆçš„ä¸´åºŠå†³ç­–è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤šç—…å…±å­˜çš„æƒ…å†µä¸‹ã€‚ç„¶è€Œï¼Œç°æœ‰ç³»ç»Ÿä¾èµ–äºç‚¹å¯¹ç‚¹é¢„æµ‹èŒƒå¼ï¼Œå¿½ç•¥äº†è¯ç‰©é—´çš„ååŒä½œç”¨å’Œæ½œåœ¨çš„è¯ç‰©ç›¸äº’ä½œç”¨ï¼ˆDDIï¼‰ã€‚æˆ‘ä»¬æå‡ºäº†FLAMEï¼Œä¸€ç§ç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç²¾ç»†ç²’åº¦åˆ—è¡¨å¯¹é½æ¡†æ¶ï¼Œèƒ½å¤Ÿå®ç°æŒ‰è¯ç‰©é€ä¸ªç”Ÿæˆè¯ç‰©åˆ—è¡¨ã€‚FLAMEå°†æ¨èè¡¨è¿°ä¸ºä¸€ä¸ªé¡ºåºå†³ç­–è¿‡ç¨‹ï¼Œæ¯ä¸€æ­¥éƒ½ä¼šå¢åŠ æˆ–åˆ é™¤ä¸€ç§è¯ç‰©ã€‚ä¸ºäº†æä¾›ç²¾ç»†åŒ–çš„å­¦ä¹ ä¿¡å·ï¼Œæˆ‘ä»¬è®¾è®¡äº†åŸºäºæ­¥éª¤çš„ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰å’ŒåŸºäºæ½œåŠ›çš„å¥–åŠ±å¡‘é€ ï¼Œè¿™æ˜ç¡®åœ°å»ºæ¨¡äº†è¯ç‰©ç›¸äº’ä½œç”¨ï¼Œå¹¶ä¼˜åŒ–äº†æ¯ä¸ªè¯ç‰©å¯¹æ•´ä½“å¤„æ–¹çš„è´¡çŒ®ã€‚æ­¤å¤–ï¼ŒFLAMEé€šè¿‡æ•´åˆç»“æ„åŒ–ä¸´åºŠçŸ¥è¯†å’Œåä½œä¿¡æ¯åˆ°LLMçš„è¡¨ç¤ºç©ºé—´æ¥å¢å¼ºæ‚£è€…å»ºæ¨¡ã€‚åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒFLAMEè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå…·æœ‰å‡ºè‰²çš„å‡†ç¡®æ€§ã€å¯æ§çš„å®‰å…¨æ€§å’Œå‡†ç¡®æ€§æƒè¡¡ä»¥åŠåœ¨å„ç§ä¸´åºŠåœºæ™¯ä¸­çš„å¼ºå¤§æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/cxfann/Flame%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/cxfann/Flameæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20218v1">PDF</a> </p>
<p><strong>Summary</strong><br>ç²¾å‡†ä¸”å®‰å…¨çš„ç”¨è¯æ¨èå¯¹äºæœ‰æ•ˆçš„ä¸´åºŠå†³ç­–è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤šç—…å…±å­˜çš„æƒ…å†µä¸‹ã€‚ç°æœ‰ç³»ç»Ÿå¤šé‡‡ç”¨ç‚¹é¢„æµ‹æ¨¡å¼ï¼Œå¿½ç•¥äº†è¯ç‰©é—´çš„ååŒä½œç”¨å’Œæ½œåœ¨çš„è¯å“ä¸è‰¯ç›¸äº’ä½œç”¨ï¼ˆDDIï¼‰ã€‚æœ¬ç ”ç©¶æå‡ºFLAMEæ¨¡å‹ï¼ŒåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç²¾ç»†åˆ—è¡¨å¯¹é½æ¡†æ¶ï¼Œå®ç°è¯ç‰©é€ä¸ªç”Ÿæˆçš„è¯ç‰©åˆ—è¡¨æ¨èã€‚FLAMEå°†æ¨èè¿‡ç¨‹è§†ä¸ºä¸€ä¸ªé¡ºåºå†³ç­–è¿‡ç¨‹ï¼Œæ¯ä¸€æ­¥æ·»åŠ æˆ–ç§»é™¤ä¸€ç§è¯ç‰©ã€‚é€šè¿‡é‡‡ç”¨ç²¾ç»†åŒ–å­¦ä¹ ä¿¡å·çš„åˆ†ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œç»“åˆåŸºäºæ½œèƒ½çš„å¥–åŠ±å¡‘é€ æŠ€æœ¯ï¼Œè¯¥æ¨¡å‹æ˜ç¡®æ¨¡æ‹Ÿè¯ç‰©é—´çš„ç›¸äº’ä½œç”¨ï¼Œä¼˜åŒ–æ¯ç§è¯ç‰©å¯¹æ•´ä½“å¤„æ–¹çš„è´¡çŒ®ã€‚æ­¤å¤–ï¼ŒFLAMEé€šè¿‡æ•´åˆç»“æ„åŒ–ä¸´åºŠçŸ¥è¯†å’Œåä½œä¿¡æ¯æå‡æ‚£è€…å»ºæ¨¡èƒ½åŠ›ã€‚åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒæ˜¾ç¤ºï¼ŒFLAMEå®ç°æœ€æ–°æ€§èƒ½ï¼Œè¡¨ç°å‡ºä¼˜è¶Šçš„å‡†ç¡®æ€§ã€å¯æ§çš„å®‰å…¨å‡†ç¡®æ€§å’Œè‰¯å¥½çš„è·¨å¤šç§ä¸´åºŠåœºæ™¯çš„æ³›åŒ–èƒ½åŠ›ã€‚ä»£ç å·²å…¬å¼€äº<a target="_blank" rel="noopener" href="https://github.com/cxfann/Flame%E3%80%82">https://github.com/cxfann/Flameã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ç²¾å‡†å®‰å…¨çš„ç”¨è¯æ¨èå¯¹ä¸´åºŠå†³ç­–è‡³å…³é‡è¦ã€‚ç‰¹åˆ«æ˜¯åœ¨å¤šç—…å…±å­˜çš„æƒ…å†µä¸‹ã€‚</li>
<li>ç°æœ‰ç³»ç»ŸåŸºäºç‚¹é¢„æµ‹æ¨¡å¼ï¼Œå¿½ç•¥äº†è¯ç‰©é—´çš„ååŒä½œç”¨å’Œæ½œåœ¨ä¸è‰¯ç›¸äº’ä½œç”¨ï¼ˆDDIï¼‰ã€‚</li>
<li>FLAMEæ¨¡å‹é‡‡ç”¨åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„ç²¾ç»†åˆ—è¡¨å¯¹é½æ¡†æ¶è¿›è¡Œè¯ç‰©æ¨èã€‚</li>
<li>æ¨èè¿‡ç¨‹è¢«è§†ä¸ºé¡ºåºå†³ç­–è¿‡ç¨‹ï¼Œæ¯ä¸€æ­¥éƒ½æ¶‰åŠè¯ç‰©çš„æ·»åŠ æˆ–ç§»é™¤ã€‚</li>
<li>é€šè¿‡åˆ†ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰å’ŒåŸºäºæ½œèƒ½çš„å¥–åŠ±å¡‘é€ æŠ€æœ¯æä¾›ç²¾ç»†åŒ–å­¦ä¹ ä¿¡å·ã€‚</li>
<li>æ¨¡å‹è€ƒè™‘äº†æ¯ç§è¯ç‰©å¯¹æ•´ä½“å¤„æ–¹çš„è´¡çŒ®ä»¥åŠè¯ç‰©é—´çš„ç›¸äº’ä½œç”¨æ¨¡æ‹Ÿã€‚</li>
<li>FLAMEç»“åˆäº†ç»“æ„åŒ–ä¸´åºŠçŸ¥è¯†å’Œåä½œä¿¡æ¯æ¥æå‡æ‚£è€…å»ºæ¨¡èƒ½åŠ›ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20218">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f3a2fbd5cde88376fc07d7338df91324.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5902e4c21db88553425b29eda719bbdb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d66cba1b72448466f21efb20cc089156.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4b8785c6de1937853da9123230fa3139.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Parameter-Efficient-Fine-Tuning-with-Column-Space-Projection"><a href="#Parameter-Efficient-Fine-Tuning-with-Column-Space-Projection" class="headerlink" title="Parameter-Efficient Fine-Tuning with Column Space Projection"></a>Parameter-Efficient Fine-Tuning with Column Space Projection</h2><p><strong>Authors:Junseo Hwang, Wonguk Cho, Taesup Kim</strong></p>
<p>Fine-tuning large language models (LLMs) with minimal computational overhead is essential for efficiently adapting them to downstream tasks under resource constraints. Parameter-efficient fine-tuning (PEFT) methods, such as Low-Rank Adaptation (LoRA), facilitate this by updating only a small subset of parameters. However, recent studies show that LoRA diverges from full fine-tuning (Full FT) in its learning behavior, particularly in terms of spectral properties. Motivated by these findings, we propose PiCa, the first theoretically grounded PEFT method based on the spectral properties of fine-tuned weights. PiCa projects gradients onto the low-rank column subspace of pre-trained weights and exhibits learning patterns more closely aligned with Full FT. Furthermore, we show that combining PiCa with weight sharing drastically reduces the number of trainable parameters without compromising performance, enabling to achieve superior performance than LoRA using 13x fewer trainable parameters. Extensive experiments demonstrate PiCa achieves the state-of-the-art performance compared to existing PEFT methods. </p>
<blockquote>
<p>ä½¿ç”¨æœ€å°çš„è®¡ç®—å¼€é”€å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œåœ¨èµ„æºçº¦æŸæ¡ä»¶ä¸‹æœ‰æ•ˆåœ°å°†å®ƒä»¬é€‚åº”ä¸‹æ¸¸ä»»åŠ¡è‡³å…³é‡è¦ã€‚å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•ï¼Œå¦‚ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰ï¼Œåªèƒ½æ›´æ–°ä¸€å°éƒ¨åˆ†å‚æ•°ï¼Œä»è€Œä¿ƒè¿›äº†è¿™ä¸€ç‚¹ã€‚ç„¶è€Œï¼Œæœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼ŒLoRAåœ¨å…¨å¾®è°ƒï¼ˆFull FTï¼‰çš„å­¦ä¹ è¡Œä¸ºä¸Šå­˜åœ¨å·®å¼‚ï¼Œç‰¹åˆ«æ˜¯åœ¨å…‰è°±ç‰¹æ€§æ–¹é¢ã€‚å—è¿™äº›å‘ç°çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†PiCaï¼Œè¿™æ˜¯åŸºäºå¾®è°ƒæƒé‡çš„å…‰è°±ç‰¹æ€§çš„é¦–ä¸ªæœ‰ç†è®ºåŸºç¡€çš„PEFTæ–¹æ³•ã€‚PiCaå°†æ¢¯åº¦æŠ•å½±åˆ°é¢„è®­ç»ƒæƒé‡çš„ä½ç§©åˆ—å­ç©ºé—´ä¸Šï¼Œå¹¶è¡¨ç°å‡ºä¸å…¨FTæ›´ä¸ºä¸€è‡´çš„å­¦ä¹ æ¨¡å¼ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¡¨æ˜ï¼Œå°†PiCaä¸æƒé‡å…±äº«ç›¸ç»“åˆï¼Œå¯ä»¥å¤§å¹…åº¦å‡å°‘å¯è®­ç»ƒå‚æ•°çš„æ•°é‡ï¼Œè€Œä¸”ä¸ä¼šæŸå®³æ€§èƒ½ï¼Œèƒ½å¤Ÿå®ç°ä»¥è¾ƒå°‘çš„å¯è®­ç»ƒå‚æ•°ä¼˜äºLoRAçš„æ€§èƒ½ã€‚å¤§é‡å®éªŒè¯æ˜ï¼ŒPiCaç›¸è¾ƒäºç°æœ‰çš„PEFTæ–¹æ³•è¾¾åˆ°äº†æœ€å…ˆè¿›çš„è¡¨ç°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20211v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¾®è°ƒæ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯å‚æ•°æœ‰æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•çš„é‡è¦æ€§åŠå…¶åœ¨èµ„æºå—é™æ¡ä»¶ä¸‹å¯¹å…¶è¿›è¡Œé«˜æ•ˆé€‚åº”ä¸‹æ¸¸ä»»åŠ¡çš„åº”ç”¨ã€‚æå‡ºçš„PiCaæ–¹æ³•åŸºäºfine-tunedæƒé‡çš„è°±å±æ€§ï¼Œæ˜¯é¦–ä¸ªæœ‰ç†è®ºåŸºç¡€çš„PEFTæ–¹æ³•ã€‚PiCaé€šè¿‡å°†æ¢¯åº¦æŠ•å½±åˆ°é¢„è®­ç»ƒæƒé‡çš„ä½ç§©åˆ—å­ç©ºé—´æ¥å±•ç°å­¦ä¹ æ¨¡å¼ï¼Œä¸å…¨å¾®è°ƒï¼ˆFull FTï¼‰æ›´ä¸ºæ¥è¿‘ã€‚æ­¤å¤–ï¼Œç»“åˆæƒé‡å…±äº«ï¼ŒPiCaå¤§å¹…å‡å°‘äº†å¯è®­ç»ƒå‚æ•°æ•°é‡ï¼ŒåŒæ—¶ä¸å¦¥åæ€§èƒ½ï¼Œä»¥è¿œä¼˜äºLoRAçš„æ€§èƒ½å®ç°äº†ä»…ä½¿ç”¨13å€å‚æ•°çš„è®­ç»ƒã€‚å®éªŒè¯æ˜ï¼ŒPiCaç›¸è¾ƒäºç°æœ‰PEFTæ–¹æ³•å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å‚æ•°æœ‰æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•åœ¨èµ„æºå—é™æ¡ä»¶ä¸‹å¯¹äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é€‚åº”è‡³å…³é‡è¦ã€‚</li>
<li>LoRAç­‰PEFTæ–¹æ³•åœ¨è°±å±æ€§æ–¹é¢ä¸å…¨å¾®è°ƒå­˜åœ¨å­¦ä¹ è¡Œä¸ºå·®å¼‚ã€‚</li>
<li>PiCaæ˜¯ä¸€ç§åŸºäºfine-tunedæƒé‡è°±å±æ€§çš„PEFTæ–¹æ³•ï¼Œé€šè¿‡å°†æ¢¯åº¦æŠ•å½±åˆ°é¢„è®­ç»ƒæƒé‡çš„ä½ç§©å­ç©ºé—´æ¥å±•ç°å­¦ä¹ æ¨¡å¼ã€‚</li>
<li>PiCaä¸æƒé‡å…±äº«ç»“åˆï¼Œå¤§å¹…å‡å°‘å¯è®­ç»ƒå‚æ•°æ•°é‡ï¼ŒåŒæ—¶ä¿æŒé«˜æ€§èƒ½ã€‚</li>
<li>PiCaåœ¨å®éªŒä¸­ç›¸è¾ƒäºå…¶ä»–PEFTæ–¹æ³•å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20211">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a41fa25f63c78d1217e5d9c45ec484e2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-94af8576b38098fbfd8805cb37895fde.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9afec1678db4cef70474ffc0f0098772.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1ba9a79e59e3bcc245b05fbb3e1c0cee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2282c3923c40985fcdcd31939125156d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5893324ac935b13228680ed96cc3f437.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="FunReason-Enhancing-Large-Language-Modelsâ€™-Function-Calling-via-Self-Refinement-Multiscale-Loss-and-Automated-Data-Refinement"><a href="#FunReason-Enhancing-Large-Language-Modelsâ€™-Function-Calling-via-Self-Refinement-Multiscale-Loss-and-Automated-Data-Refinement" class="headerlink" title="FunReason: Enhancing Large Language Modelsâ€™ Function Calling via   Self-Refinement Multiscale Loss and Automated Data Refinement"></a>FunReason: Enhancing Large Language Modelsâ€™ Function Calling via   Self-Refinement Multiscale Loss and Automated Data Refinement</h2><p><strong>Authors:Bingguang Hao, Maolin Wang, Zengzhuang Xu, Cunyin Peng, Yicheng Chen, Xiangyu Zhao, Jinjie Gu, Chenyi Zhuang</strong></p>
<p>The integration of large language models (LLMs) with function calling has emerged as a crucial capability for enhancing their practical utility in real-world applications. However, effectively combining reasoning processes with accurate function execution remains a significant challenge. Traditional training approaches often struggle to balance the detailed reasoning steps with the precision of function calls, leading to suboptimal performance. To address these limitations, we introduce FunReason, a novel framework that enhances LLMsâ€™ function calling capabilities through an automated data refinement strategy and a Self-Refinement Multiscale Loss (SRML) approach. FunReason leverages LLMsâ€™ natural reasoning abilities to generate high-quality training examples, focusing on query parseability, reasoning coherence, and function call precision. The SRML approach dynamically balances the contribution of reasoning processes and function call accuracy during training, addressing the inherent trade-off between these two critical aspects. FunReason achieves performance comparable to GPT-4o while effectively mitigating catastrophic forgetting during fine-tuning. FunReason provides a comprehensive solution for enhancing LLMsâ€™ function calling capabilities by introducing a balanced training methodology and a data refinement pipeline. For code and dataset, please refer to our repository at GitHub <a target="_blank" rel="noopener" href="https://github.com/BingguangHao/FunReason">https://github.com/BingguangHao/FunReason</a> </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸å‡½æ•°è°ƒç”¨çš„é›†æˆåœ¨å¢å¼ºå…¶åœ¨å®é™…åº”ç”¨ä¸­çš„å®ç”¨æ€§æ–¹é¢è¡¨ç°å‡ºäº†å…³é”®èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå°†æ¨ç†è¿‡ç¨‹ä¸å‡†ç¡®çš„å‡½æ•°æ‰§è¡Œæœ‰æ•ˆåœ°ç»“åˆèµ·æ¥ä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚ä¼ ç»Ÿçš„è®­ç»ƒæ–¹æ³•å¾€å¾€å¾ˆéš¾å¹³è¡¡è¯¦ç»†çš„æ¨ç†æ­¥éª¤ä¸å‡½æ•°è°ƒç”¨çš„ç²¾ç¡®æ€§ï¼Œå¯¼è‡´æ€§èƒ½ä¸ä½³ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†FunReasonè¿™ä¸€æ–°å‹æ¡†æ¶ã€‚FunReasoné€šè¿‡è‡ªåŠ¨åŒ–æ•°æ®ç»†åŒ–ç­–ç•¥å’Œè‡ªç»†åŒ–å¤šå°ºåº¦æŸå¤±ï¼ˆSRMLï¼‰æ–¹æ³•ï¼Œå¢å¼ºLLMçš„å‡½æ•°è°ƒç”¨èƒ½åŠ›ã€‚FunReasonåˆ©ç”¨LLMçš„è‡ªç„¶æ¨ç†èƒ½åŠ›ç”Ÿæˆé«˜è´¨é‡çš„è®­ç»ƒæ ·æœ¬ï¼Œä¸“æ³¨äºæŸ¥è¯¢è§£ææ€§ã€æ¨ç†è¿è´¯æ€§å’Œå‡½æ•°è°ƒç”¨ç²¾åº¦ã€‚SRMLæ–¹æ³•åŠ¨æ€å¹³è¡¡äº†æ¨ç†è¿‡ç¨‹å’Œå‡½æ•°è°ƒç”¨å‡†ç¡®æ€§åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„è´¡çŒ®ï¼Œè§£å†³äº†è¿™ä¸¤è€…ä¹‹é—´çš„å›ºæœ‰æƒè¡¡ã€‚FunReasonçš„æ€§èƒ½å¯ä¸GPT-4oç›¸åª²ç¾ï¼ŒåŒæ—¶æœ‰æ•ˆå‡è½»äº†å¾®è°ƒè¿‡ç¨‹ä¸­çš„ç¾éš¾æ€§é—å¿˜ã€‚FunReasoné€šè¿‡å¼•å…¥å¹³è¡¡çš„è®­ç»ƒæ–¹æ³•å’Œæ•°æ®ç»†åŒ–æµç¨‹ï¼Œä¸ºå¢å¼ºLLMçš„å‡½æ•°è°ƒç”¨èƒ½åŠ›æä¾›äº†å…¨é¢è§£å†³æ–¹æ¡ˆã€‚æœ‰å…³ä»£ç å’Œæ•°æ®é›†ï¼Œè¯·å‚è§æˆ‘ä»¬åœ¨GitHubä¸Šçš„å­˜å‚¨åº“ï¼š[<a target="_blank" rel="noopener" href="https://github.com/BingguangHao/FunReason%E3%80%82%EF%BC%88%E8%BF%99%E9%87%8C%E5%8F%AF%E8%83%BD%E6%9C%89%E4%BA%9B%E5%86%85%E5%AE%B9%E6%B2%A1%E6%9C%89%E5%AE%8C%E5%85%A8%E7%AC%A6%E5%90%88%E5%8E%9F%E6%84%8F%EF%BC%89]">https://github.com/BingguangHao/FunReasonã€‚ï¼ˆè¿™é‡Œå¯èƒ½æœ‰äº›å†…å®¹æ²¡æœ‰å®Œå…¨ç¬¦åˆåŸæ„ï¼‰]</a>(<a target="_blank" rel="noopener" href="https://github.com/BingguangHao/FunReason%E3%80%82%EF%BC%88%E8%BF%99%E9%87%8C%E5%AF%BC%E8%83%BD%E6%9C%89%E4%BA%9B%E5%86%85%E5%AE%B9%E6%B2%A1%E6%9C%89%E5%AE%9A%E5%BC%BA%E7%BB%B4%E6%8C%AF%EF%BC%89">https://github.com/BingguangHao/FunReason%E3%80%82%EF%BC%88%E8%BF%99%E9%87%8C%E5%AF%BC%E8%83%BD%E6%9C%89%E4%BA%9B%E5%86%85%E5%AE%B9%E6%B2%A1%E6%9C%89%E5%AE%9A%E5%BC%BA%E7%BB%B4%E6%8C%AFï¼‰</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20192v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸å‡½æ•°è°ƒç”¨çš„èåˆå¯¹äºæå‡å…¶åœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„å®ç”¨æ€§è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œå°†æ¨ç†è¿‡ç¨‹ä¸å‡†ç¡®å‡½æ•°è°ƒç”¨æœ‰æ•ˆç»“åˆå­˜åœ¨æŒ‘æˆ˜ã€‚ä¼ ç»Ÿè®­ç»ƒæ–¹æ³•éš¾ä»¥å¹³è¡¡æ¨ç†æ­¥éª¤ä¸å‡½æ•°è°ƒç”¨ç²¾åº¦ï¼Œå¯¼è‡´æ€§èƒ½ä¸ä½³ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼ŒFunReasonæ¡†æ¶é€šè¿‡è‡ªåŠ¨åŒ–æ•°æ®ç²¾ç‚¼ç­–ç•¥å’Œè‡ªé€‚åº”å¤šå°ºåº¦æŸå¤±ï¼ˆSRMLï¼‰æ–¹æ³•ï¼Œæå‡LLMçš„å‡½æ•°è°ƒç”¨èƒ½åŠ›ã€‚FunReasonåˆ©ç”¨LLMçš„è‡ªç„¶æ¨ç†èƒ½åŠ›ç”Ÿæˆé«˜è´¨é‡è®­ç»ƒæ ·æœ¬ï¼Œå…³æ³¨æŸ¥è¯¢è§£ææ€§ã€æ¨ç†è¿è´¯æ€§å’Œå‡½æ•°è°ƒç”¨ç²¾åº¦ã€‚SRMLæ–¹æ³•åŠ¨æ€å¹³è¡¡æ¨ç†è¿‡ç¨‹å’Œå‡½æ•°è°ƒç”¨å‡†ç¡®æ€§çš„è´¡çŒ®ï¼Œè§£å†³ä¸¤è€…ä¹‹é—´çš„å›ºæœ‰æƒè¡¡ã€‚FunReasonæ€§èƒ½ä¸GPT-4ç›¸å½“ï¼ŒåŒæ—¶æœ‰æ•ˆç¼“è§£å¾®è°ƒæ—¶çš„ç¾éš¾æ€§é—å¿˜é—®é¢˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸å‡½æ•°è°ƒç”¨çš„ç»“åˆå¯¹äºæå‡æ¨¡å‹å®ç”¨æ€§è‡³å…³é‡è¦ã€‚</li>
<li>ä¼ ç»Ÿè®­ç»ƒæ–¹æ³•éš¾ä»¥å¹³è¡¡æ¨ç†ä¸å‡½æ•°è°ƒç”¨çš„å‡†ç¡®æ€§ã€‚</li>
<li>FunReasonæ¡†æ¶é€šè¿‡è‡ªåŠ¨åŒ–æ•°æ®ç²¾ç‚¼ç­–ç•¥æå‡LLMçš„å‡½æ•°è°ƒç”¨èƒ½åŠ›ã€‚</li>
<li>FunReasonåˆ©ç”¨LLMçš„è‡ªç„¶æ¨ç†èƒ½åŠ›ï¼Œå…³æ³¨æŸ¥è¯¢è§£ææ€§ã€æ¨ç†è¿è´¯æ€§å’Œå‡½æ•°è°ƒç”¨ç²¾åº¦ã€‚</li>
<li>SRMLæ–¹æ³•åŠ¨æ€å¹³è¡¡æ¨ç†å’Œå‡½æ•°è°ƒç”¨ä¹‹é—´çš„è´¡çŒ®ï¼Œè§£å†³ä¸¤è€…ä¹‹é—´çš„æƒè¡¡é—®é¢˜ã€‚</li>
<li>FunReasonæ€§èƒ½è¡¨ç°ä¼˜å¼‚ï¼Œä¸GPT-4ç›¸å½“ã€‚</li>
<li>FunReasonæœ‰æ•ˆç¼“è§£å¾®è°ƒè¿‡ç¨‹ä¸­çš„ç¾éš¾æ€§é—å¿˜é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20192">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-227c1e2e0e37df3d6069ead71f4a156e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e0f23d294e44852d4c046ba9ec98906b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b685b4fb56f568d4544fb4d2d7ea5ede.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-76b0375301df615d6a7955971137095c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-20401d955b0e76ed054dfb59e7f9998c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-71a1031870e522896bc8ffee7ba531b1.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="THiNK-Can-Large-Language-Models-Think-aloud"><a href="#THiNK-Can-Large-Language-Models-Think-aloud" class="headerlink" title="THiNK: Can Large Language Models Think-aloud?"></a>THiNK: Can Large Language Models Think-aloud?</h2><p><strong>Authors:Yongan Yu, Mengqian Wu, Yiran Lin, Nikki G. Lobczowski</strong></p>
<p>Assessing higher-order thinking skills in large language models (LLMs) remains a fundamental challenge, especially in tasks that go beyond surface-level accuracy. In this work, we propose THiNK (Testing Higher-order Notion of Knowledge), a multi-agent, feedback-driven evaluation framework grounded in Bloomâ€™s Taxonomy. THiNK frames reasoning assessment as an iterative task of problem generation, critique, and revision, encouraging LLMs to think-aloud through step-by-step reflection and refinement. This enables a systematic evaluation of both lower-order (e.g., remember, understand) and higher-order (e.g., evaluate, create) thinking skills. We apply THiNK to seven state-of-the-art LLMs and perform a detailed cognitive analysis of their outputs. Results reveal that while models reliably perform lower-order categories well, they struggle with applying knowledge in realistic contexts and exhibit limited abstraction. Structured feedback loops significantly improve reasoning performance, particularly in higher-order thinking. Qualitative evaluations further confirm that THiNK-guided outputs better align with domain logic and problem structure. The code of our framework provides a scalable methodology for probing and enhancing LLM reasoning, offering new directions for evaluation grounded in learning science, which is available at our GitHub repository. </p>
<blockquote>
<p>è¯„ä¼°å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é«˜çº§æ€ç»´æŠ€èƒ½ä»ç„¶æ˜¯ä¸€ä¸ªåŸºæœ¬æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨è¶…è¶Šè¡¨é¢å±‚æ¬¡å‡†ç¡®æ€§çš„ä»»åŠ¡ä¸­ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†THiNKï¼ˆçŸ¥è¯†é«˜é˜¶è®¤çŸ¥æµ‹è¯•ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºå¸ƒé²å§†åˆ†ç±»æ³•çš„å¤šä¸»ä½“ã€åé¦ˆé©±åŠ¨çš„è¯„ä»·æ¡†æ¶ã€‚THiNKå°†æ¨ç†è¯„ä¼°è§†ä¸ºé—®é¢˜ç”Ÿæˆã€æ‰¹åˆ¤å’Œä¿®è®¢çš„è¿­ä»£ä»»åŠ¡ï¼Œé¼“åŠ±LLMé€šè¿‡é€æ­¥åæ€å’Œå®Œå–„è¿›è¡Œâ€œæ€è€ƒâ€ï¼Œè¿™å¯¹äºæµ‹è¯•çŸ¥è¯†å¾ˆé‡è¦ã€‚è¿™èƒ½å¤Ÿç³»ç»Ÿåœ°è¯„ä¼°ä½çº§ï¼ˆä¾‹å¦‚ï¼Œè®°å¿†ã€ç†è§£ï¼‰å’Œé«˜çº§ï¼ˆä¾‹å¦‚ï¼Œè¯„ä¼°ã€åˆ›é€ ï¼‰æ€ç»´æŠ€èƒ½ã€‚æˆ‘ä»¬å°†THiNKåº”ç”¨äºä¸ƒä¸ªæœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå¹¶å¯¹å…¶è¾“å‡ºè¿›è¡Œäº†è¯¦ç»†çš„è®¤çŸ¥åˆ†æã€‚ç»“æœè¡¨æ˜ï¼Œè™½ç„¶æ¨¡å‹åœ¨ä½çº§ç±»åˆ«ä¸Šè¡¨ç°å¯é ï¼Œä½†åœ¨ç°å®è¯­å¢ƒä¸­åº”ç”¨çŸ¥è¯†æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œè¡¨ç°å‡ºæœ‰é™çš„æŠ½è±¡èƒ½åŠ›ã€‚ç»“æ„åŒ–åé¦ˆå¾ªç¯æ˜¾è‘—æé«˜äº†æ¨ç†æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨é«˜çº§æ€ç»´æ–¹é¢ã€‚å®šæ€§è¯„ä¼°è¿›ä¸€æ­¥è¯å®ï¼ŒTHiNKæŒ‡å¯¼çš„è¾“å‡ºæ›´ç¬¦åˆé¢†åŸŸé€»è¾‘å’Œé—®é¢˜ç»“æ„ã€‚æˆ‘ä»¬çš„æ¡†æ¶ä»£ç æä¾›äº†ä¸€ç§å¯æ‰©å±•çš„æ–¹æ³•ï¼Œç”¨äºæ¢æµ‹å’Œæ”¹è¿›LLMæ¨ç†èƒ½åŠ›ï¼Œä¸ºåŸºäºå­¦ä¹ ç§‘å­¦çš„è¯„ä¼°æä¾›äº†æ–°æ–¹å‘ï¼Œå¯åœ¨æˆ‘ä»¬çš„GitHubå­˜å‚¨åº“ä¸­æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20184v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºTHiNKæ¡†æ¶ï¼ŒåŸºäºå¸ƒéš†è®¤çŸ¥é¢†åŸŸåˆ†ç±»è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é«˜çº§æ€ç»´æŠ€èƒ½ã€‚è¯¥æ¡†æ¶ä¸ºå¤šæ™ºèƒ½ä½“ã€åé¦ˆé©±åŠ¨å‹è¯„ä»·æ¡†æ¶ï¼Œé€šè¿‡é—®é¢˜ç”Ÿæˆã€æ‰¹åˆ¤å’Œä¿®è®¢çš„è¿­ä»£ä»»åŠ¡æ¥è¯„ä¼°æ¨ç†èƒ½åŠ›ï¼Œé¼“åŠ±LLMé€šè¿‡é€æ­¥åæ€å’Œä¿®æ­£è¿›è¡Œâ€œæ€è€ƒâ€ã€‚THiNKæ¡†æ¶èƒ½å¤Ÿç³»ç»Ÿåœ°è¯„ä¼°LLMçš„ä½é˜¶æ€ç»´æŠ€èƒ½ï¼ˆå¦‚è®°å¿†ã€ç†è§£ï¼‰å’Œé«˜é˜¶æ€ç»´æŠ€èƒ½ï¼ˆå¦‚è¯„ä¼°ã€åˆ›é€ ï¼‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè™½ç„¶LLMåœ¨ä½é˜¶æŠ€èƒ½ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨çœŸå®åœºæ™¯ä¸­åº”ç”¨çŸ¥è¯†æ–¹é¢å­˜åœ¨å›°éš¾ï¼ŒæŠ½è±¡èƒ½åŠ›æœ‰é™ã€‚åé¦ˆç¯è·¯èƒ½æ˜¾è‘—æé«˜æ¨ç†æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜é˜¶æ€ç»´æ–¹é¢ã€‚å®šæ€§è¯„ä»·è¿›ä¸€æ­¥è¯å®ï¼ŒTHiNKå¼•å¯¼çš„è¾“å‡ºæ›´ç¬¦åˆé¢†åŸŸé€»è¾‘å’Œé—®é¢˜çš„ç»“æ„ã€‚è¯¥æ¡†æ¶ä»£ç æä¾›äº†ä¸€ç§å¯é çš„æ–¹æ³•æ¥æ¢æµ‹å’Œæå‡LLMçš„æ¨ç†èƒ½åŠ›ï¼Œå¹¶ä¸ºåŸºäºå­¦ä¹ ç§‘å­¦çš„è¯„ä¼°æä¾›äº†æ–°çš„æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>THiNKæ¡†æ¶ç”¨äºè¯„ä¼°LLMçš„é«˜çº§æ€ç»´æŠ€èƒ½ï¼Œéµå¾ªå¸ƒéš†è®¤çŸ¥é¢†åŸŸåˆ†ç±»ã€‚</li>
<li>è¯¥æ¡†æ¶æ˜¯å¤šæ™ºèƒ½ä½“ã€åé¦ˆé©±åŠ¨å‹çš„è¯„ä»·æ¡†æ¶ï¼Œå¼ºè°ƒé—®é¢˜ç”Ÿæˆã€æ‰¹åˆ¤å’Œä¿®è®¢çš„è¿­ä»£è¿‡ç¨‹ã€‚</li>
<li>LLMåœ¨ä½é˜¶æ€ç»´æŠ€èƒ½ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨çœŸå®åœºæ™¯ä¸­åº”ç”¨çŸ¥è¯†å’ŒæŠ½è±¡èƒ½åŠ›æ–¹é¢å­˜åœ¨å±€é™ã€‚</li>
<li>åé¦ˆç¯è·¯å¯¹æå‡LLMçš„æ¨ç†æ€§èƒ½æœ‰é‡è¦ä½œç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜é˜¶æ€ç»´æ–¹é¢ã€‚</li>
<li>THiNKå¼•å¯¼çš„è¾“å‡ºæ›´ç¬¦åˆé¢†åŸŸé€»è¾‘å’Œé—®é¢˜ç»“æ„ï¼Œè¡¨æ˜å…¶åœ¨å®šæ€§è¯„ä»·ä¸­çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>THiNKæ¡†æ¶ä»£ç æä¾›äº†ä¸€å¯é çš„æ–¹æ³•æ¥è¿›è¡ŒLLMæ¨ç†èƒ½åŠ›çš„æ¢æµ‹å’Œæå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20184">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c520d7315811dd17de2e12a78133b475.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0b204cd8d3a9f5a310376dcfb7c592f3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8ae1ea557731acecf0da0099b58f2b09.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a00c8ae1fbc9b4b5379149982dffd05b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-200955cc5ab1f3d2aa278ff79d2db006.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Program-of-Equations-Thoughts-to-Solve-Algebra-Word-Problems"><a href="#Program-of-Equations-Thoughts-to-Solve-Algebra-Word-Problems" class="headerlink" title="Program of Equations Thoughts to Solve Algebra Word Problems"></a>Program of Equations Thoughts to Solve Algebra Word Problems</h2><p><strong>Authors:Yunze Lin</strong></p>
<p>Solving algebraic word problems (AWPs) has recently emerged as an important natural language processing task. Recently, large language models (LLMs) have demonstrated powerful mathematical capabilities, and the Chain-of-Thought technique, which guides LLMs through step-by-step reasoning, has yielded impressive results. However, this reasoning ability is limited by the computational weaknesses of LLMs themselves, where calculation errors can accumulate, leading to incorrect final answers. To address this, we propose Program of Equations Thoughts (POET), which transforms the task of generating step-by-step reasoning answers into a two-stage task of predicting equations and generating code, offloading complex computations to a Python interpreter to avoid calculation errors in LLMs. Furthermore, we propose Zero-shot POET, which utilizes a manually designed template to enable LLMs to directly generate Python code for one-step solving. Our method achieves accuracies of 95.3% and 98.0% on the PEN and ALG514 datasets, respectively, setting a new state-of-the-art (SOTA). Zero-shot POET also achieves the SOTA result of 95.5% on the DRAW-1K dataset. </p>
<blockquote>
<p>è§£å†³ä»£æ•°æ–‡å­—é¢˜ï¼ˆAWPsï¼‰æ˜¯ä¸€é¡¹æ–°è¿‘å‡ºç°çš„è‡ªç„¶è¯­è¨€å¤„ç†é‡è¦ä»»åŠ¡ã€‚è¿‘æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²å±•ç°å‡ºå¼ºå¤§çš„æ•°å­¦èƒ½åŠ›ï¼Œé€šè¿‡é€æ­¥æ¨ç†çš„â€œæ€ç»´é“¾â€æŠ€æœ¯ä¹Ÿå–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„ç»“æœã€‚ç„¶è€Œï¼Œè¿™ç§æ¨ç†èƒ½åŠ›å—åˆ°LLMè‡ªèº«è®¡ç®—å¼±ç‚¹çš„é™åˆ¶ï¼Œè®¡ç®—é”™è¯¯ä¼šç´¯ç§¯ï¼Œå¯¼è‡´æœ€ç»ˆç­”æ¡ˆé”™è¯¯ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºâ€œæ–¹ç¨‹å¼æ€ç»´ç¨‹åºâ€ï¼ˆPOETï¼‰ï¼Œå°†ç”Ÿæˆé€æ­¥æ¨ç†ç­”æ¡ˆçš„ä»»åŠ¡è½¬å˜ä¸ºé¢„æµ‹æ–¹ç¨‹å¼å’Œç”Ÿæˆä»£ç çš„ä¸¤é˜¶æ®µä»»åŠ¡ï¼Œå°†å¤æ‚è®¡ç®—å¸è½½ç»™Pythonè§£é‡Šå™¨ï¼Œé¿å…åœ¨LLMä¸­å‡ºç°è®¡ç®—é”™è¯¯ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºé›¶æ ·æœ¬POETï¼Œå®ƒåˆ©ç”¨æ‰‹åŠ¨è®¾è®¡çš„æ¨¡æ¿ï¼Œä½¿LLMèƒ½å¤Ÿç›´æ¥ç”Ÿæˆä¸€æ­¥è§£å†³çš„Pythonä»£ç ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨PENå’ŒALG514æ•°æ®é›†ä¸Šåˆ†åˆ«è¾¾åˆ°äº†95.3%å’Œ98.0%çš„å‡†ç¡®ç‡ï¼Œåˆ›é€ äº†æ–°çš„æœ€ä½³æ°´å¹³ï¼ˆSOTAï¼‰ã€‚é›¶æ ·æœ¬POETåœ¨DRAW-1Kæ•°æ®é›†ä¸Šä¹Ÿè¾¾åˆ°äº†95.5%çš„SOTAç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20170v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è§£å†³ä»£æ•°æ–‡å­—é—®é¢˜ï¼ˆAWPsï¼‰æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸçš„é‡è¦ä»»åŠ¡ï¼Œè¿‘å¹´æ¥å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å±•ç°äº†å¼ºå¤§çš„æ•°å­¦èƒ½åŠ›ï¼Œåˆ©ç”¨é“¾å¼æ€ç»´æ–¹æ³•ç”Ÿæˆåˆ†æ­¥è§£ç­”æ•ˆæœæ˜¾è‘—ã€‚ç„¶è€Œï¼Œè®¡ç®—ä¸­çš„é”™è¯¯ç´¯ç§¯é™åˆ¶äº†è¿™ä¸€èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œæœ¬ç ”ç©¶æå‡ºäº†å…¬å¼åŒ–æ€ç»´ï¼ˆPOETï¼‰æ–¹æ³•ï¼Œå°†ç”Ÿæˆåˆ†æ­¥æ¨ç†ç­”æ¡ˆçš„ä»»åŠ¡è½¬åŒ–ä¸ºé¢„æµ‹æ–¹ç¨‹å¼å’Œç”Ÿæˆä»£ç çš„ä¸¤é˜¶æ®µä»»åŠ¡ï¼Œå°†å¤æ‚è®¡ç®—äº¤ç»™Pythonè§£é‡Šå™¨æ‰§è¡Œï¼Œé¿å…LLMsä¸­çš„è®¡ç®—é”™è¯¯ã€‚æ­¤å¤–ï¼Œæœ¬ç ”ç©¶è¿˜æå‡ºäº†é›¶æ ·æœ¬POETæ–¹æ³•ï¼Œåˆ©ç”¨æ‰‹åŠ¨è®¾è®¡çš„æ¨¡æ¿ç›´æ¥ç”Ÿæˆä¸€æ­¥è§£å†³é—®é¢˜çš„Pythonä»£ç ã€‚è¯¥æ–¹æ³•åœ¨PENå’ŒALG514æ•°æ®é›†ä¸Šçš„å‡†ç¡®ç‡åˆ†åˆ«è¾¾åˆ°95.3%å’Œ98%ï¼Œåˆ›é€ äº†æ–°çš„æœ€ä½³è®°å½•ã€‚é›¶æ ·æœ¬POETåœ¨DRAW-1Kæ•°æ®é›†ä¸Šä¹Ÿå–å¾—äº†æœ€ä½³ç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>è§£å†³ä»£æ•°æ–‡å­—é—®é¢˜æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†çš„é‡è¦ä»»åŠ¡ã€‚</li>
<li>å¤§è¯­è¨€æ¨¡å‹å±•ç°å‡ºå¼ºå¤§çš„æ•°å­¦èƒ½åŠ›ï¼Œé€šè¿‡é“¾å¼æ€ç»´æ–¹æ³•åˆ†æ­¥è§£ç­”æ•ˆæœæ˜¾è‘—ã€‚</li>
<li>è®¡ç®—é”™è¯¯é™åˆ¶äº†è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>å…¬å¼åŒ–æ€ç»´ï¼ˆPOETï¼‰æ–¹æ³•é€šè¿‡å°†é—®é¢˜è½¬åŒ–ä¸ºé¢„æµ‹æ–¹ç¨‹å¼å’Œç”Ÿæˆä»£ç çš„ä¸¤é˜¶æ®µä»»åŠ¡æ¥é¿å…è®¡ç®—é”™è¯¯ã€‚</li>
<li>POETåˆ©ç”¨Pythonè§£é‡Šå™¨æ‰§è¡Œå¤æ‚è®¡ç®—ã€‚</li>
<li>é›¶æ ·æœ¬POETæ–¹æ³•åˆ©ç”¨æ‰‹åŠ¨è®¾è®¡çš„æ¨¡æ¿ç›´æ¥ç”Ÿæˆä¸€æ­¥è§£å†³Pythonä»£ç ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20170">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-14da5b544a6e01dfccd39a3a18dd447f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-09b7828c9b32afafcd7772e602d0804d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-72e82b23e978a47c2800c607c65e582b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-96174021544e73edf15abe1e17c1ad03.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-47b30a4ff914de705cc022b85fe30c9c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fd657888767ade163ecf8e1e19bffd59.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Prismatic-Synthesis-Gradient-based-Data-Diversification-Boosts-Generalization-in-LLM-Reasoning"><a href="#Prismatic-Synthesis-Gradient-based-Data-Diversification-Boosts-Generalization-in-LLM-Reasoning" class="headerlink" title="Prismatic Synthesis: Gradient-based Data Diversification Boosts   Generalization in LLM Reasoning"></a>Prismatic Synthesis: Gradient-based Data Diversification Boosts   Generalization in LLM Reasoning</h2><p><strong>Authors:Jaehun Jung, Seungju Han, Ximing Lu, Skyler Hallinan, David Acuna, Shrimai Prabhumoye, Mostafa Patwary, Mohammad Shoeybi, Bryan Catanzaro, Yejin Choi</strong></p>
<p>Effective generalization in language models depends critically on the diversity of their training data. Yet existing diversity metrics often fall short of this goal, relying on surface-level heuristics that are decoupled from model behavior. This motivates us to ask: What kind of diversity in training data actually drives generalization in language models â€“ and how can we measure and amplify it? Through large-scale empirical analyses spanning over 300 training runs, carefully controlled for data scale and quality, we show that data diversity can be a strong predictor of generalization in LLM reasoning â€“ as measured by average model performance on unseen out-of-distribution benchmarks. We introduce G-Vendi, a metric that quantifies diversity via the entropy of model-induced gradients. Despite using a small off-the-shelf proxy model for gradients, G-Vendi consistently outperforms alternative measures, achieving strong correlation (Spearmanâ€™s $\rho \approx 0.9$) with out-of-distribution (OOD) performance on both natural language inference (NLI) and math reasoning tasks. Building on this insight, we present Prismatic Synthesis, a framework for generating diverse synthetic data by targeting underrepresented regions in gradient space. Experimental results show that Prismatic Synthesis consistently improves model performance as we scale synthetic data â€“ not just on in-distribution test but across unseen, out-of-distribution benchmarks â€“ significantly outperforming state-of-the-art models that rely on 20 times larger data generator than ours. For example, PrismMath-7B, our model distilled from a 32B LLM, outperforms R1-Distill-Qwen-7B â€“ the same base model trained on proprietary data generated by 671B R1 â€“ on 6 out of 7 challenging benchmarks. </p>
<blockquote>
<p>è®­ç»ƒæ•°æ®çš„æœ‰æ•ˆæ€§åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå–å†³äºè®­ç»ƒæ•°æ®çš„å¤šæ ·æ€§ï¼Œä½†åœ¨è‡ªç„¶è¯­è¨€æ¨¡å‹çš„ç°å®åœºæ™¯ä¸­ï¼Œç°æœ‰çš„å¤šæ ·æ€§åº¦é‡é€šå¸¸éš¾ä»¥å®ç°è¿™ä¸€ç›®æ ‡ã€‚å®ƒä»¬ä¾èµ–äºä¸æ¨¡å‹è¡Œä¸ºè„±é’©çš„è¡¨é¢å¯å‘å¼ç­–ç•¥ã€‚è¿™ä¿ƒä½¿æˆ‘ä»¬æå‡ºä»¥ä¸‹é—®é¢˜ï¼šè®­ç»ƒæ•°æ®ä¸­ä»€ä¹ˆæ ·çš„å¤šæ ·æ€§å®é™…ä¸Šå¯ä»¥é©±åŠ¨è¯­è¨€æ¨¡å‹çš„æ³›åŒ–â€”â€”æˆ‘ä»¬å¦‚ä½•è¡¡é‡å¹¶å¢å¼ºè¿™ç§å¤šæ ·æ€§ï¼Ÿé€šè¿‡å¯¹è¶…è¿‡300æ¬¡è®­ç»ƒè¿è¡Œçš„å¤§è§„æ¨¡å®è¯åˆ†æï¼Œåœ¨ä¸¥æ ¼æ§åˆ¶æ•°æ®è§„æ¨¡å’Œè´¨é‡çš„æ¡ä»¶ä¸‹ï¼Œæˆ‘ä»¬å‘ç°æ•°æ®å¤šæ ·æ€§å¯ä»¥ä½œä¸ºè‡ªç„¶è¯­è¨€æ¨¡å‹æ¨ç†æ³›åŒ–çš„å¼ºå¤§é¢„æµ‹æŒ‡æ ‡â€”â€”è¿™æ˜¯é€šè¿‡æ¨¡å‹åœ¨æœªè§è¿‡çš„ä¸ç¬¦åˆåˆ†å¸ƒåŸºå‡†æµ‹è¯•ä¸Šçš„å¹³å‡æ€§èƒ½æ¥è¡¡é‡çš„ã€‚æˆ‘ä»¬å¼•å…¥äº†G-VendiæŒ‡æ ‡ï¼Œå®ƒé€šè¿‡æ¨¡å‹è¯±å¯¼æ¢¯åº¦çš„ç†µæ¥è¡¡é‡å¤šæ ·æ€§ã€‚å°½ç®¡å®ƒä½¿ç”¨ç°æˆçš„æ¢¯åº¦ä»£ç†æ¨¡å‹è§„æ¨¡è¾ƒå°ï¼Œä½†G-Vendiå§‹ç»ˆä¼˜äºå…¶ä»–åº¦é‡æ ‡å‡†ï¼Œä¸æœªè§åˆ†å¸ƒï¼ˆOODï¼‰çš„æ€§èƒ½å…·æœ‰å¾ˆå¼ºçš„ç›¸å…³æ€§ï¼ˆæ–¯çš®å°”æ›¼çš„Ïå€¼çº¦ä¸º0.9ï¼‰ï¼Œæ— è®ºæ˜¯åœ¨è‡ªç„¶è¯­è¨€æ¨ç†ï¼ˆNLIï¼‰è¿˜æ˜¯æ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šã€‚åŸºäºè¿™ä¸€è§è§£ï¼Œæˆ‘ä»¬æå‡ºäº†Prismaticåˆæˆæ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ—¨åœ¨ç”Ÿæˆé’ˆå¯¹æ¢¯åº¦ç©ºé—´ä¸­ä»£è¡¨æ€§ä¸è¶³åŒºåŸŸçš„å¤šæ ·åŒ–åˆæˆæ•°æ®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œéšç€åˆæˆæ•°æ®çš„è§„æ¨¡æ‰©å¤§ï¼ŒPrismaticåˆæˆå§‹ç»ˆæé«˜äº†æ¨¡å‹æ€§èƒ½â€”â€”ä¸ä»…åœ¨å†…éƒ¨æµ‹è¯•é›†ä¸Šè¡¨ç°è‰¯å¥½ï¼Œè€Œä¸”åœ¨æœªè§çš„ä¸ç¬¦åˆåˆ†å¸ƒåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—ä¼˜äºä¾èµ–æ¯”æˆ‘ä»¬å¤§20å€çš„æ•°æ®ç”Ÿæˆå™¨çš„å½“å‰æœ€å…ˆè¿›æ¨¡å‹ã€‚ä¾‹å¦‚ï¼ŒPrismMath-7Bï¼ˆç”±æˆ‘ä»¬çš„ä»è§„æ¨¡ä¸º32Bçš„è‡ªç„¶è¯­è¨€æ¨¡å‹è’¸é¦è€Œæˆï¼‰åœ¨ä¸ƒä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†å…­ä¸ªé¢†å…ˆçš„ç»“æœï¼Œè¶…è¶Šäº†ä»¥ç›¸åŒåŸºç¡€æ¨¡å‹è®­ç»ƒçš„R1-Distill-Qwen-7Bï¼ˆåŸºäºä½¿ç”¨671Bçš„R1ç”Ÿæˆçš„ä¸“æœ‰æ•°æ®ï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20161v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>è®­ç»ƒæ•°æ®çš„å¤šæ ·æ€§å¯¹äºè¯­è¨€æ¨¡å‹çš„æ³›åŒ–è‡³å…³é‡è¦ã€‚ç°æœ‰å¤šæ ·æ€§æŒ‡æ ‡å¸¸å¸¸éš¾ä»¥è¾¾åˆ°è¿™ä¸€ç›®æ ‡ï¼Œä¾èµ–äºä¸æ¨¡å‹è¡Œä¸ºè„±èŠ‚çš„è¡¨é¢å¯å‘å¼æ–¹æ³•ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºä»¥ä¸‹é—®é¢˜ï¼šä»€ä¹ˆæ ·çš„è®­ç»ƒæ•°æ®å¤šæ ·æ€§å®é™…ä¸Šèƒ½é©±åŠ¨è¯­è¨€æ¨¡å‹çš„æ³›åŒ–â€”â€”æˆ‘ä»¬å¦‚ä½•è¡¡é‡å¹¶å¢å¼ºè¿™ç§å¤šæ ·æ€§ï¼Ÿé€šè¿‡æ¶µç›–è¶…è¿‡300æ¬¡è®­ç»ƒè¿è¡Œçš„å¤§è§„æ¨¡å®è¯åˆ†æï¼Œåœ¨ä¸¥æ ¼æ§åˆ¶æ•°æ®è§„æ¨¡å’Œè´¨é‡çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å‘ç°æ•°æ®å¤šæ ·æ€§å¯ä»¥æ˜¯è¯­è¨€æ¨¡å‹æ¨ç†æ³›åŒ–çš„æœ‰åŠ›é¢„æµ‹æŒ‡æ ‡â€”â€”é€šè¿‡æ¨¡å‹åœ¨æœªè§è¿‡çš„åˆ†å¸ƒå¤–åŸºå‡†æµ‹è¯•ä¸Šçš„å¹³å‡æ€§èƒ½æ¥è¡¡é‡ã€‚æˆ‘ä»¬å¼•å…¥äº†G-VendiæŒ‡æ ‡ï¼Œè¯¥æŒ‡æ ‡é€šè¿‡æ¨¡å‹è¯±å¯¼æ¢¯åº¦çš„ç†µæ¥é‡åŒ–å¤šæ ·æ€§ã€‚å°½ç®¡ä½¿ç”¨äº†å°å‹ç°æˆçš„æ¢¯åº¦ä»£ç†æ¨¡å‹ï¼Œä½†G-Vendiå§‹ç»ˆä¼˜äºå…¶ä»–æªæ–½ï¼Œä¸æœªè§åˆ†å¸ƒï¼ˆOODï¼‰çš„è¡¨ç°åœ¨è‡ªç„¶è¯­è¨€æ¨ç†ï¼ˆNLIï¼‰å’Œæ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šå®ç°äº†å¼ºçƒˆçš„ç›¸å…³æ€§ï¼ˆSpearmanâ€™sÏâ‰ˆ0.9ï¼‰ã€‚åŸºäºè¿™ä¸€è§è§£ï¼Œæˆ‘ä»¬æå‡ºäº†Prismaticåˆæˆæ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡é’ˆå¯¹æ¢¯åº¦ç©ºé—´ä¸­çš„è¡¨ç¤ºä¸è¶³åŒºåŸŸæ¥ç”Ÿæˆå¤šæ ·åŒ–çš„åˆæˆæ•°æ®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œéšç€åˆæˆæ•°æ®çš„è§„æ¨¡æ‰©å¤§ï¼ŒPrismaticåˆæˆæŒç»­æé«˜äº†æ¨¡å‹æ€§èƒ½â€”â€”ä¸ä»…åœ¨å†…éƒ¨æµ‹è¯•ä¸Šï¼Œè€Œä¸”åœ¨æœªè§è¿‡çš„åˆ†å¸ƒå¤–åŸºå‡†æµ‹è¯•ä¸Šï¼Œæ˜¾è‘—ä¼˜äºä¾èµ–æ¯”æˆ‘ä»¬å¤§20å€æ•°æ®ç”Ÿæˆå™¨çš„æœ€å…ˆè¿›æ¨¡å‹ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬ä»è§„æ¨¡ä¸º32Bçš„å¤§å‹è¯­è¨€æ¨¡å‹ä¸­æç‚¼å‡ºçš„PrismMath-7Bæ¨¡å‹ï¼Œåœ¨å…­ä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºåŸºäºè§„æ¨¡ä¸º671Bçš„R1æ¨¡å‹è®­ç»ƒçš„R1-Distill-Qwen-7Bæ¨¡å‹ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è®­ç»ƒæ•°æ®çš„å¤šæ ·æ€§å¯¹è¯­è¨€æ¨¡å‹çš„æ³›åŒ–è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰å¤šæ ·æ€§æŒ‡æ ‡å¾€å¾€æ— æ³•å‡†ç¡®é¢„æµ‹æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>å¼•å…¥G-VendiæŒ‡æ ‡ï¼Œé€šè¿‡æ¨¡å‹è¯±å¯¼æ¢¯åº¦çš„ç†µæ¥é‡åŒ–æ•°æ®å¤šæ ·æ€§ï¼Œä¸æœªè§åˆ†å¸ƒçš„ä»»åŠ¡è¡¨ç°å‘ˆç°å¼ºçƒˆç›¸å…³æ€§ã€‚</li>
<li>æå‡ºPrismaticåˆæˆæ¡†æ¶ï¼Œç”Ÿæˆé’ˆå¯¹æ¢¯åº¦ç©ºé—´ä¸­è¡¨ç¤ºä¸è¶³åŒºåŸŸçš„å¤šæ ·åŒ–åˆæˆæ•°æ®ã€‚</li>
<li>Prismaticåˆæˆèƒ½æé«˜æ¨¡å‹æ€§èƒ½ï¼Œä¸ä»…åœ¨å†…éƒ¨æµ‹è¯•ä¸Šï¼Œè€Œä¸”åœ¨æœªè§è¿‡çš„åˆ†å¸ƒå¤–åŸºå‡†æµ‹è¯•ä¸Šã€‚</li>
<li>å®éªŒæ˜¾ç¤ºï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼Œä½¿ç”¨Prismaticåˆæˆçš„è¾ƒå°è§„æ¨¡æ¨¡å‹è¡¨ç°ä¼˜äºä½¿ç”¨æ›´å¤§è§„æ¨¡æ•°æ®ç”Ÿæˆçš„å…ˆè¿›æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20161">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-22f3ab85fb45e37f092d4cea1d5a33ef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-424ae132dcf1848f844c3881fd4baffd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2e49c04adc349d5fad6d3ece45eaacfe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0efad3be325d45052d91e0eae2b23b66.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b18f261a9d7feb6228ca21364c776927.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Pangu-Light-Weight-Re-Initialization-for-Pruning-and-Accelerating-LLMs"><a href="#Pangu-Light-Weight-Re-Initialization-for-Pruning-and-Accelerating-LLMs" class="headerlink" title="Pangu Light: Weight Re-Initialization for Pruning and Accelerating LLMs"></a>Pangu Light: Weight Re-Initialization for Pruning and Accelerating LLMs</h2><p><strong>Authors:Hanting Chen, Jiarui Qin, Jialong Guo, Tao Yuan, Yichun Yin, Huiling Zhen, Yasheng Wang, Jinpeng Li, Xiaojun Meng, Meng Zhang, Rongju Ruan, Zheyuan Bai, Yehui Tang, Can Chen, Xinghao Chen, Fisher Yu, Ruiming Tang, Yunhe Wang</strong></p>
<p>Large Language Models (LLMs) deliver state-of-the-art capabilities across numerous tasks, but their immense size and inference costs pose significant computational challenges for practical deployment. While structured pruning offers a promising avenue for model compression, existing methods often struggle with the detrimental effects of aggressive, simultaneous width and depth reductions, leading to substantial performance degradation. This paper argues that a critical, often overlooked, aspect in making such aggressive joint pruning viable is the strategic re-initialization and adjustment of remaining weights to improve the model post-pruning training accuracies. We introduce Pangu Light, a framework for LLM acceleration centered around structured pruning coupled with novel weight re-initialization techniques designed to address this &#96;&#96;missing pieceâ€™â€™. Our framework systematically targets multiple axes, including model width, depth, attention heads, and RMSNorm, with its effectiveness rooted in novel re-initialization methods like Cross-Layer Attention Pruning (CLAP) and Stabilized LayerNorm Pruning (SLNP) that mitigate performance drops by providing the network a better training starting point. Further enhancing efficiency, Pangu Light incorporates specialized optimizations such as absorbing Post-RMSNorm computations and tailors its strategies to Ascend NPU characteristics. The Pangu Light models consistently exhibit a superior accuracy-efficiency trade-off, outperforming prominent baseline pruning methods like Nemotron and established LLMs like Qwen3 series. For instance, on Ascend NPUs, Pangu Light-32Bâ€™s 81.6 average score and 2585 tokens&#x2F;s throughput exceed Qwen3-32Bâ€™s 80.9 average score and 2225 tokens&#x2F;s. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šä¸ªä»»åŠ¡ä¸Šæä¾›äº†æœ€å…ˆè¿›çš„æŠ€æœ¯èƒ½åŠ›ï¼Œä½†å®ƒä»¬çš„åºå¤§è§„æ¨¡å’Œæ¨ç†æˆæœ¬ä¸ºå®é™…éƒ¨ç½²å¸¦æ¥äº†é‡å¤§çš„è®¡ç®—æŒ‘æˆ˜ã€‚è™½ç„¶ç»“æ„åŒ–å‰ªæä¸ºæ¨¡å‹å‹ç¼©æä¾›äº†æœ‰å‰æ™¯çš„é€”å¾„ï¼Œä½†ç°æœ‰æ–¹æ³•å¾€å¾€éš¾ä»¥åº”å¯¹æ¿€çƒˆçš„åŒæ—¶å®½åº¦å’Œæ·±åº¦ç¼©å‡çš„è´Ÿé¢å½±å“ï¼Œå¯¼è‡´æ€§èƒ½å¤§å¹…ä¸‹é™ã€‚æœ¬æ–‡è®¤ä¸ºï¼Œåœ¨é‡‡å–è¿™ç§æ¿€çƒˆçš„è”åˆå‰ªææ—¶ï¼Œç»å¸¸è¢«å¿½è§†çš„ä¸€ä¸ªå…³é”®æ–¹é¢æ˜¯æˆ˜ç•¥æ€§åœ°é‡æ–°åˆå§‹åŒ–å’Œè°ƒæ•´å‰©ä½™æƒé‡ï¼Œä»¥æé«˜æ¨¡å‹å‰ªæåçš„è®­ç»ƒç²¾åº¦ã€‚æˆ‘ä»¬æ¨å‡ºäº†â€œç›˜å¤å…‰â€æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä»¥åŠ é€Ÿå¤§å‹è¯­è¨€æ¨¡å‹ä¸ºä¸­å¿ƒï¼Œç»“åˆäº†ç»“æ„åŒ–å‰ªæå’Œæ–°å‹æƒé‡é‡æ–°åˆå§‹åŒ–æŠ€æœ¯ï¼Œæ—¨åœ¨è§£å†³è¿™ä¸€â€œç¼ºå¤±çš„éƒ¨åˆ†â€ã€‚æˆ‘ä»¬çš„æ¡†æ¶ç³»ç»Ÿåœ°é’ˆå¯¹å¤šä¸ªè½´è¿›è¡Œä¼˜åŒ–ï¼ŒåŒ…æ‹¬æ¨¡å‹å®½åº¦ã€æ·±åº¦ã€æ³¨æ„åŠ›å¤´å’ŒRMSNormï¼Œå…¶æœ‰æ•ˆæ€§æºäºæ–°çš„é‡æ–°åˆå§‹åŒ–æ–¹æ³•ï¼Œå¦‚è·¨å±‚æ³¨æ„åŠ›å‰ªæï¼ˆCLAPï¼‰å’Œç¨³å®šçš„å±‚æ ‡å‡†åŒ–å‰ªæï¼ˆSLNPï¼‰ï¼Œè¿™äº›æ–¹æ³•é€šè¿‡ä¸ºç½‘ç»œæä¾›æ›´å¥½çš„è®­ç»ƒèµ·ç‚¹æ¥å‡è½»æ€§èƒ½ä¸‹é™ã€‚è¿›ä¸€æ­¥æé«˜æ•ˆç‡ï¼Œâ€œç›˜å¤å…‰â€ç»“åˆäº†ä¸“é—¨ä¼˜åŒ–ï¼Œå¦‚å¸æ”¶Post-RMSNormè®¡ç®—ï¼Œå¹¶é’ˆå¯¹Ascend NPUç‰¹æ€§è°ƒæ•´å…¶ç­–ç•¥ã€‚ç›˜å¤å…‰æ¨¡å‹åœ¨å‡†ç¡®æ€§å’Œæ•ˆç‡ä¹‹é—´è¡¨ç°å‡ºå“è¶Šçš„æƒè¡¡ï¼Œä¼˜äºä¸»æµçš„åŸºçº¿å‰ªææ–¹æ³•ï¼Œå¦‚Nemotronå’Œç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå¦‚Qwen3ç³»åˆ—ã€‚ä¾‹å¦‚ï¼Œåœ¨Ascend NPUä¸Šï¼ŒPangu Light-32Bçš„å¹³å‡å¾—åˆ†81.6å’Œååé‡2585ä»¤ç‰Œ&#x2F;ç§’è¶…è¿‡äº†Qwen3-32Bçš„å¹³å‡å¾—åˆ†80.9å’Œååé‡2225ä»¤ç‰Œ&#x2F;ç§’ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20155v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šä»»åŠ¡ä¸­å±•ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œä½†å…¶åºå¤§çš„è§„æ¨¡å’Œæ¨ç†æˆæœ¬ä¸ºå®é™…åº”ç”¨å¸¦æ¥äº†é‡å¤§è®¡ç®—æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºPangu Lightçš„æ¡†æ¶ï¼Œå®ƒé€šè¿‡ç»“åˆç»“æ„å‰ªæå’Œæƒé‡å†åˆå§‹åŒ–æŠ€æœ¯ï¼Œä»¥åŠ é€ŸLLMçš„è¿è¡Œã€‚è¯¥æ¡†æ¶é’ˆå¯¹æ¨¡å‹çš„å®½åº¦ã€æ·±åº¦ã€æ³¨æ„åŠ›å¤´å’ŒRMSNormç­‰å¤šä¸ªæ–¹é¢è¿›è¡Œä¼˜åŒ–ï¼Œé€šè¿‡åˆ›æ–°çš„å†åˆå§‹åŒ–æ–¹æ³•å¦‚è·¨å±‚æ³¨æ„åŠ›å‰ªæï¼ˆCLAPï¼‰å’Œç¨³å®šLayerNormå‰ªæï¼ˆSLNPï¼‰æ¥å‡è½»æ€§èƒ½ä¸‹é™ã€‚åŒæ—¶ï¼ŒPangu Lightè¿˜ç»“åˆäº†ç‰¹æ®Šä¼˜åŒ–ï¼Œå¦‚å¸æ”¶Post-RMSNormè®¡ç®—ï¼Œå¹¶é’ˆå¯¹Ascend NPUçš„ç‰¹æ€§å®šåˆ¶ç­–ç•¥ã€‚åœ¨Ascend NPUsä¸Šï¼ŒPangu Lightæ¨¡å‹åœ¨å‡†ç¡®æ€§å’Œæ•ˆç‡æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œè¶…è¶Šäº†Nemotronç­‰åŸºçº¿å‰ªææ–¹æ³•å’ŒQwen3ç³»åˆ—LLMã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é¢ä¸´è®¡ç®—æŒ‘æˆ˜ï¼Œå…¶å®è·µéƒ¨ç½²éœ€è¦è§£å†³è§„æ¨¡åºå¤§å’Œæ¨ç†æˆæœ¬é«˜æ˜‚çš„é—®é¢˜ã€‚</li>
<li>Pangu Lightæ¡†æ¶æ—¨åœ¨é€šè¿‡ç»“åˆç»“æ„å‰ªæå’Œæƒé‡å†åˆå§‹åŒ–æŠ€æœ¯æ¥åŠ é€ŸLLMã€‚</li>
<li>Pangu Lightæ¡†æ¶é’ˆå¯¹æ¨¡å‹çš„å¤šä¸ªæ–¹é¢è¿›è¡Œä¼˜åŒ–ï¼ŒåŒ…æ‹¬å®½åº¦ã€æ·±åº¦ã€æ³¨æ„åŠ›å¤´å’ŒRMSNormç­‰ã€‚</li>
<li>é€šè¿‡åˆ›æ–°çš„å†åˆå§‹åŒ–æ–¹æ³•å¦‚CLAPå’ŒSLNPï¼ŒPangu Lightå‡è½»æ€§èƒ½ä¸‹é™ï¼Œæä¾›æ›´å¥½çš„è®­ç»ƒèµ·ç‚¹ã€‚</li>
<li>Pangu Lightç»“åˆäº†ç‰¹æ®Šä¼˜åŒ–ï¼Œå¦‚å¸æ”¶Post-RMSNormè®¡ç®—ï¼Œä»¥æé«˜æ•ˆç‡ã€‚</li>
<li>Pangu Lightçš„ç­–ç•¥æ ¹æ®Ascend NPUçš„ç‰¹æ€§å®šåˆ¶ï¼Œå®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20155">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ea7a6cc4b6c5bc84e0ee14093366f9ce.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a9d863879813ef68ea7800ea8aa8af27.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="UORA-Uniform-Orthogonal-Reinitialization-Adaptation-in-Parameter-Efficient-Fine-Tuning-of-Large-Models"><a href="#UORA-Uniform-Orthogonal-Reinitialization-Adaptation-in-Parameter-Efficient-Fine-Tuning-of-Large-Models" class="headerlink" title="UORA: Uniform Orthogonal Reinitialization Adaptation in   Parameter-Efficient Fine-Tuning of Large Models"></a>UORA: Uniform Orthogonal Reinitialization Adaptation in   Parameter-Efficient Fine-Tuning of Large Models</h2><p><strong>Authors:Xueyan Zhang, Jinman Zhao, Zhifei Yang, Yibo Zhong, Shuhao Guan, Linbo Cao, Yining Wang</strong></p>
<p>This paper introduces Uniform Orthogonal Reinitialization Adaptation (UORA), a novel parameter-efficient fine-tuning (PEFT) approach for Large Language Models (LLMs). UORA achieves state-of-the-art performance and parameter efficiency by leveraging a low-rank approximation method to reduce the number of trainable parameters. Unlike existing methods such as LoRA and VeRA, UORA employs an interpolation-based reparametrization mechanism that selectively reinitializes rows and columns in frozen projection matrices, guided by the vector magnitude heuristic. This results in substantially fewer trainable parameters compared to LoRA and outperforms VeRA in computation and storage efficiency. Comprehensive experiments across various benchmarks demonstrate UORAâ€™s superiority in achieving competitive fine-tuning performance with negligible computational overhead. We demonstrate its performance on GLUE and E2E benchmarks and its effectiveness in instruction-tuning large language models and image classification models. Our contributions establish a new paradigm for scalable and resource-efficient fine-tuning of LLMs. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†ç»Ÿä¸€æ­£äº¤å†åˆå§‹åŒ–é€‚é…ï¼ˆUORAï¼‰ï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ–°å‹å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•ã€‚UORAåˆ©ç”¨ä½ç§©é€¼è¿‘æ–¹æ³•å‡å°‘è®­ç»ƒå‚æ•°çš„æ•°é‡ï¼Œå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½å’Œå‚æ•°æ•ˆç‡ã€‚ä¸ç°æœ‰çš„LoRAå’ŒVeRAæ–¹æ³•ä¸åŒï¼ŒUORAé‡‡ç”¨åŸºäºæ’å€¼çš„é‡æ–°å‚æ•°åŒ–æœºåˆ¶ï¼Œè¯¥æœºåˆ¶æ ¹æ®å‘é‡å¹…åº¦å¯å‘å¼é€‰æ‹©æ€§åœ°é‡æ–°åˆå§‹åŒ–å†»ç»“æŠ•å½±çŸ©é˜µçš„è¡Œå’Œåˆ—ã€‚è¿™å¯¼è‡´ä¸LoRAç›¸æ¯”ï¼Œè®­ç»ƒå‚æ•°å¤§å¤§å‡å°‘ï¼Œå¹¶ä¸”åœ¨è®¡ç®—å’Œå­˜å‚¨æ•ˆç‡æ–¹é¢ä¼˜äºVeRAã€‚åœ¨ä¸åŒåŸºå‡†æµ‹è¯•ä¸Šçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒUORAåœ¨ç«äº‰å¾®è°ƒæ€§èƒ½æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œè®¡ç®—å¼€é”€å¾®ä¹å…¶å¾®ã€‚æˆ‘ä»¬åœ¨GLUEå’ŒE2EåŸºå‡†æµ‹è¯•ä¸Šå±•ç¤ºäº†å…¶æ€§èƒ½ï¼Œä»¥åŠå…¶åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹å’Œå›¾åƒåˆ†ç±»æ¨¡å‹çš„æŒ‡ä»¤è°ƒæ•´ä¸­çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„è´¡çŒ®ä¸ºLLMçš„å¯æ‰©å±•å’Œèµ„æºé«˜æ•ˆçš„å¾®è°ƒå»ºç«‹äº†æ–°çš„èŒƒå¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20154v1">PDF</a> 20 pages, 2 figures, 15 tables</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ç»Ÿä¸€æ­£äº¤é‡åˆå§‹åŒ–é€‚åº”ï¼ˆUORAï¼‰æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ–°å‹å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•ã€‚UORAé€šè¿‡åˆ©ç”¨ä½ç§©é€¼è¿‘æ–¹æ³•å‡å°‘è®­ç»ƒå‚æ•°çš„æ•°é‡ï¼Œå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½å’Œå‚æ•°æ•ˆç‡ã€‚ä¸ç°æœ‰çš„LoRAå’ŒVeRAæ–¹æ³•ä¸åŒï¼ŒUORAé‡‡ç”¨åŸºäºæ’å€¼çš„é‡æ–°å‚æ•°åŒ–æœºåˆ¶ï¼Œè¯¥æœºåˆ¶æœ‰é€‰æ‹©åœ°é‡æ–°åˆå§‹åŒ–å†»ç»“æŠ•å½±çŸ©é˜µçš„è¡Œå’Œåˆ—ï¼Œç”±å‘é‡å¹…åº¦å¯å‘å¼æŒ‡å¯¼ã€‚è¿™å¯¼è‡´ä¸LoRAç›¸æ¯”å¤§å¤§å‡å°‘äº†å¯è®­ç»ƒå‚æ•°çš„æ•°é‡ï¼Œå¹¶ä¸”åœ¨è®¡ç®—å’Œå­˜å‚¨æ•ˆç‡æ–¹é¢ä¼˜äºVeRAã€‚åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒUORAåœ¨ç«äº‰å¾®è°ƒæ€§èƒ½æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œè®¡ç®—å¼€é”€å¾®ä¹å…¶å¾®ã€‚æˆ‘ä»¬åœ¨GLUEå’ŒE2EåŸºå‡†æµ‹è¯•ä¸Šå±•ç¤ºäº†å…¶æ€§èƒ½ï¼Œå¹¶è¯æ˜äº†å…¶åœ¨æŒ‡ä»¤å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹å’Œå›¾åƒåˆ†ç±»æ¨¡å‹ä¸­çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„è´¡çŒ®ä¸ºå¯ä¼¸ç¼©å’Œèµ„æºé«˜æ•ˆçš„å¤§å‹è¯­è¨€æ¨¡å‹å¾®è°ƒå»ºç«‹äº†æ–°çš„èŒƒä¾‹ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>UORAæ˜¯ä¸€ç§é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•ã€‚</li>
<li>UORAåˆ©ç”¨ä½ç§©é€¼è¿‘æ–¹æ³•å‡å°‘è®­ç»ƒå‚æ•°æ•°é‡ã€‚</li>
<li>UORAé‡‡ç”¨åŸºäºæ’å€¼çš„é‡æ–°å‚æ•°åŒ–æœºåˆ¶ï¼Œæœ‰é€‰æ‹©åœ°é‡æ–°åˆå§‹åŒ–å†»ç»“æŠ•å½±çŸ©é˜µã€‚</li>
<li>UORAåœ¨å‘é‡å¹…åº¦å¯å‘å¼æŒ‡å¯¼ä¸‹è¿›è¡Œå·¥ä½œï¼Œè¿™å½±å“äº†å…¶é‡æ–°å‚æ•°åŒ–æœºåˆ¶çš„é€‰æ‹©ã€‚</li>
<li>ä¸LoRAç›¸æ¯”ï¼ŒUORAå¤§å¹…å‡å°‘äº†å¯è®­ç»ƒå‚æ•°çš„æ•°é‡ã€‚</li>
<li>UORAåœ¨è®¡ç®—å’Œå­˜å‚¨æ•ˆç‡ä¸Šä¼˜äºVeRAã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20154">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5cef1cb2898e90daa4bd8053bdde1ecd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e1a4c6ed3bceda37465f4eeb2003766c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b9ffc15f6809b255a9a73f75192f115a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4a6868d818e3fd909ed7e399d99ec4db.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="FUDOKI-Discrete-Flow-based-Unified-Understanding-and-Generation-via-Kinetic-Optimal-Velocities"><a href="#FUDOKI-Discrete-Flow-based-Unified-Understanding-and-Generation-via-Kinetic-Optimal-Velocities" class="headerlink" title="FUDOKI: Discrete Flow-based Unified Understanding and Generation via   Kinetic-Optimal Velocities"></a>FUDOKI: Discrete Flow-based Unified Understanding and Generation via   Kinetic-Optimal Velocities</h2><p><strong>Authors:Jin Wang, Yao Lai, Aoxue Li, Shifeng Zhang, Jiacheng Sun, Ning Kang, Chengyue Wu, Zhenguo Li, Ping Luo</strong></p>
<p>The rapid progress of large language models (LLMs) has catalyzed the emergence of multimodal large language models (MLLMs) that unify visual understanding and image generation within a single framework. However, most existing MLLMs rely on autoregressive (AR) architectures, which impose inherent limitations on future development, such as the raster-scan order in image generation and restricted reasoning abilities in causal context modeling. In this work, we challenge the dominance of AR-based approaches by introducing FUDOKI, a unified multimodal model purely based on discrete flow matching, as an alternative to conventional AR paradigms. By leveraging metric-induced probability paths with kinetic optimal velocities, our framework goes beyond the previous masking-based corruption process, enabling iterative refinement with self-correction capability and richer bidirectional context integration during generation. To mitigate the high cost of training from scratch, we initialize FUDOKI from pre-trained AR-based MLLMs and adaptively transition to the discrete flow matching paradigm. Experimental results show that FUDOKI achieves performance comparable to state-of-the-art AR-based MLLMs across both visual understanding and image generation tasks, highlighting its potential as a foundation for next-generation unified multimodal models. Furthermore, we show that applying test-time scaling techniques to FUDOKI yields significant performance gains, further underscoring its promise for future enhancement through reinforcement learning. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•å‚¬ç”Ÿäº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„å‡ºç°ï¼Œè¿™äº›æ¨¡å‹åœ¨ä¸€ä¸ªæ¡†æ¶å†…ç»Ÿä¸€äº†è§†è§‰ç†è§£å’Œå›¾åƒç”Ÿæˆã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰çš„MLLMä¾èµ–äºè‡ªå›å½’ï¼ˆARï¼‰æ¶æ„ï¼Œè¿™å¯¹å…¶æœªæ¥å‘å±•é€ æˆäº†å›ºæœ‰å±€é™ï¼Œä¾‹å¦‚å›¾åƒç”Ÿæˆä¸­çš„æ‰«æé¡ºåºå’Œå› æœä¸Šä¸‹æ–‡å»ºæ¨¡ä¸­çš„æœ‰é™æ¨ç†èƒ½åŠ›ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å¯¹ä¼ ç»ŸARæ–¹æ³•æå‡ºæŒ‘æˆ˜ï¼Œå¼•å…¥äº†ä¸€ç§åŸºäºç¦»æ•£æµåŒ¹é…çš„ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹FUDOKIä½œä¸ºæ›¿ä»£æ–¹æ¡ˆã€‚é€šè¿‡åˆ©ç”¨åº¦é‡è¯±å¯¼æ¦‚ç‡è·¯å¾„å’ŒåŠ¨åŠ›å­¦æœ€ä¼˜é€Ÿåº¦ï¼Œæˆ‘ä»¬çš„æ¡†æ¶è¶…è¶Šäº†ä¹‹å‰çš„åŸºäºæ©ç çš„è…èš€è¿‡ç¨‹ï¼Œå®ç°äº†è¿­ä»£ç»†åŒ–ã€è‡ªæˆ‘æ ¡æ­£èƒ½åŠ›å’Œç”Ÿæˆè¿‡ç¨‹ä¸­çš„æ›´ä¸°å¯ŒåŒå‘ä¸Šä¸‹æ–‡é›†æˆã€‚ä¸ºäº†é™ä½ä»å¤´å¼€å§‹è®­ç»ƒçš„æˆæœ¬ï¼Œæˆ‘ä»¬ä»é¢„è®­ç»ƒçš„AR-based MLLMåˆå§‹åŒ–FUDOKIï¼Œå¹¶é€æ­¥è¿‡æ¸¡åˆ°ç¦»æ•£æµåŒ¹é…èŒƒå¼ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFUDOKIåœ¨è§†è§‰ç†è§£å’Œå›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šçš„æ€§èƒ½ä¸æœ€å…ˆè¿›çš„AR-based MLLMç›¸å½“ï¼Œå‡¸æ˜¾äº†å…¶ä½œä¸ºä¸‹ä¸€ä»£ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹åŸºç¡€æ½œåŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¯¹FUDOKIåº”ç”¨æµ‹è¯•æ—¶ç¼©æ”¾æŠ€æœ¯ï¼Œå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¿›ä¸€æ­¥è¯æ˜å…¶é€šè¿‡å¼ºåŒ–å­¦ä¹ è¿›è¡Œæœªæ¥å¢å¼ºçš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20147v1">PDF</a> 37 pages, 12 figures</p>
<p><strong>Summary</strong>ï¼šå¤§å‹è¯­è¨€æ¨¡å‹çš„å¿«é€Ÿå‘å±•æ¨åŠ¨äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„å…´èµ·ï¼Œè¯¥æ¨¡å‹åœ¨ä¸€ä¸ªæ¡†æ¶å†…èåˆäº†è§†è§‰ç†è§£å’Œå›¾åƒç”Ÿæˆã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰æ¨¡å‹ä¾èµ–äºè‡ªå›å½’æ¶æ„ï¼Œå­˜åœ¨å›¾åƒç”Ÿæˆé¡ºåºå’Œå› æœä¸Šä¸‹æ–‡å»ºæ¨¡èƒ½åŠ›çš„å±€é™æ€§ã€‚æœ¬ç ”ç©¶æŒ‘æˆ˜äº†è‡ªå›å½’æ–¹æ³•çš„ä¸»å¯¼åœ°ä½ï¼Œæå‡ºäº†åŸºäºç¦»æ•£æµåŒ¹é…çš„ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹FUDOKIï¼Œè¶…è¶Šäº†ä¼ ç»Ÿçš„åŸºäºé®è”½çš„è…èš€è¿‡ç¨‹ï¼Œå…·æœ‰è‡ªæˆ‘ä¿®æ­£èƒ½åŠ›å’Œä¸°å¯Œçš„åŒå‘ä¸Šä¸‹æ–‡é›†æˆåŠŸèƒ½ã€‚æˆ‘ä»¬ä»é¢„è®­ç»ƒçš„åŸºäºè‡ªå›å½’çš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ä¸­åˆå§‹åŒ–FUDOKIå¹¶é€æ­¥é€‚åº”ç¦»æ•£æµåŒ¹é…èŒƒå¼ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFUDOKIåœ¨è§†è§‰ç†è§£å’Œå›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šçš„æ€§èƒ½ä¸åŸºäºè‡ªå›å½’çš„æ¨¡å‹ç›¸å½“ï¼Œæ˜¾ç¤ºå‡ºå…¶ä½œä¸ºä¸‹ä¸€ä»£ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹åŸºç¡€çš„æ½œåŠ›ã€‚åº”ç”¨æµ‹è¯•æ—¶ç¼©æ”¾æŠ€æœ¯èƒ½æ˜¾è‘—æå‡å…¶æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ç»“åˆäº†è§†è§‰ç†è§£å’Œå›¾åƒç”Ÿæˆåœ¨ä¸€ä¸ªæ¡†æ¶å†…ã€‚</li>
<li>ç°æœ‰MLLMså¤§å¤šä¾èµ–è‡ªå›å½’ï¼ˆARï¼‰æ¶æ„ï¼Œå­˜åœ¨å±€é™æ€§ã€‚</li>
<li>FUDOKIæ˜¯ä¸€ä¸ªåŸºäºç¦»æ•£æµåŒ¹é…çš„éARå¤šæ¨¡æ€æ¨¡å‹ï¼Œå…·æœ‰è‡ªæˆ‘ä¿®æ­£èƒ½åŠ›å’Œä¸°å¯Œçš„ä¸Šä¸‹æ–‡é›†æˆåŠŸèƒ½ã€‚</li>
<li>FUDOKIé€šè¿‡åˆ©ç”¨é¢„è®­ç»ƒçš„ARæ¨¡å‹è¿›è¡Œåˆå§‹åŒ–å¹¶é€æ­¥è¿‡æ¸¡åˆ°ç¦»æ•£æµåŒ¹é…èŒƒå¼ï¼Œé™ä½è®­ç»ƒæˆæœ¬ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºFUDOKIåœ¨è§†è§‰ç†è§£å’Œå›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šè¡¨ç°ä¼˜ç§€ï¼Œå…·æœ‰æˆä¸ºä¸‹ä¸€ä»£å¤šæ¨¡æ€æ¨¡å‹åŸºç¡€çš„æ½œåŠ›ã€‚</li>
<li>æµ‹è¯•æ—¶ç¼©æ”¾æŠ€æœ¯èƒ½æ˜¾è‘—æå‡FUDOKIçš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20147">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0d773cf8df9dfb0791896237a004bb17.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e81f3ed2b6bfad7996309164d2154467.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-29ebde777a49a2a237c7bd1ca1207807.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="StructEval-Benchmarking-LLMsâ€™-Capabilities-to-Generate-Structural-Outputs"><a href="#StructEval-Benchmarking-LLMsâ€™-Capabilities-to-Generate-Structural-Outputs" class="headerlink" title="StructEval: Benchmarking LLMsâ€™ Capabilities to Generate Structural   Outputs"></a>StructEval: Benchmarking LLMsâ€™ Capabilities to Generate Structural   Outputs</h2><p><strong>Authors:Jialin Yang, Dongfu Jiang, Lipeng He, Sherman Siu, Yuxuan Zhang, Disen Liao, Zhuofeng Li, Huaye Zeng, Yiming Jia, Haozhe Wang, Benjamin Schneider, Chi Ruan, Wentao Ma, Zhiheng Lyu, Yifei Wang, Yi Lu, Quy Duc Do, Ziyan Jiang, Ping Nie, Wenhu Chen</strong></p>
<p>As Large Language Models (LLMs) become integral to software development workflows, their ability to generate structured outputs has become critically important. We introduce StructEval, a comprehensive benchmark for evaluating LLMsâ€™ capabilities in producing both non-renderable (JSON, YAML, CSV) and renderable (HTML, React, SVG) structured formats. Unlike prior benchmarks, StructEval systematically evaluates structural fidelity across diverse formats through two paradigms: 1) generation tasks, producing structured output from natural language prompts, and 2) conversion tasks, translating between structured formats. Our benchmark encompasses 18 formats and 44 types of task, with novel metrics for format adherence and structural correctness. Results reveal significant performance gaps, even state-of-the-art models like o1-mini achieve only 75.58 average score, with open-source alternatives lagging approximately 10 points behind. We find generation tasks more challenging than conversion tasks, and producing correct visual content more difficult than generating text-only structures. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è½¯ä»¶å¼€å‘æµç¨‹ä¸­ä¸å¯æˆ–ç¼ºï¼Œå®ƒä»¬ç”Ÿæˆç»“æ„åŒ–è¾“å‡ºçš„èƒ½åŠ›å˜å¾—è‡³å…³é‡è¦ã€‚æˆ‘ä»¬æ¨å‡ºäº†StructEvalï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°LLMç”Ÿæˆéæ¸²æŸ“ï¼ˆJSONã€YAMLã€CSVï¼‰å’Œå¯æ¸²æŸ“ï¼ˆHTMLã€Reactã€SVGï¼‰ç»“æ„åŒ–æ ¼å¼çš„èƒ½åŠ›ã€‚ä¸ä¹‹å‰çš„åŸºå‡†æµ‹è¯•ä¸åŒï¼ŒStructEvalé€šè¿‡ä¸¤ç§èŒƒå¼ç³»ç»Ÿåœ°è¯„ä¼°äº†è·¨å¤šç§æ ¼å¼çš„ç»“æ„ä¿çœŸåº¦ï¼š1ï¼‰ç”Ÿæˆä»»åŠ¡ï¼Œæ ¹æ®è‡ªç„¶è¯­è¨€æç¤ºç”Ÿæˆç»“æ„åŒ–è¾“å‡ºï¼›2ï¼‰è½¬æ¢ä»»åŠ¡ï¼Œåœ¨ç»“æ„åŒ–æ ¼å¼ä¹‹é—´è¿›è¡Œç¿»è¯‘ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•æ¶µç›–äº†18ç§æ ¼å¼å’Œ44ç§ä»»åŠ¡ç±»å‹ï¼ŒåŒ…æ‹¬æ–°çš„æ ¼å¼éµå¾ªåº¦å’Œç»“æ„æ­£ç¡®æ€§çš„æŒ‡æ ‡ã€‚ç»“æœè¡¨æ˜ï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„æ¨¡å‹ï¼Œå¦‚o1-miniä¹Ÿä»…è¾¾åˆ°75.58çš„å¹³å‡åˆ†ï¼Œå¼€æºæ›¿ä»£æ–¹æ¡ˆå¤§çº¦è½å10åˆ†ã€‚æˆ‘ä»¬å‘ç°ç”Ÿæˆä»»åŠ¡æ¯”è½¬æ¢ä»»åŠ¡æ›´å…·æŒ‘æˆ˜æ€§ï¼Œç”Ÿæˆæ­£ç¡®çš„è§†è§‰å†…å®¹æ¯”ç”Ÿæˆçº¯æ–‡æœ¬ç»“æ„æ›´ä¸ºå›°éš¾ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20139v1">PDF</a> 16 pages, 9 figures, 13 tables</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç”Ÿæˆç»“æ„åŒ–è¾“å‡ºæ–¹é¢çš„èƒ½åŠ›å¯¹äºè½¯ä»¶å¼€å‘æµç¨‹è‡³å…³é‡è¦ã€‚æœ¬æ–‡ä»‹ç»äº†StructEvalï¼Œä¸€ä¸ªå…¨é¢è¯„ä¼°LLMç”Ÿæˆå¤šç§éæ¸²æŸ“ï¼ˆJSONã€YAMLã€CSVï¼‰å’Œå¯æ¸²æŸ“ï¼ˆHTMLã€Reactã€SVGï¼‰ç»“æ„åŒ–æ ¼å¼èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚StructEvalé€šè¿‡ç”Ÿæˆä»»åŠ¡å’Œè½¬æ¢ä»»åŠ¡ä¸¤ä¸ªèŒƒå¼ç³»ç»Ÿåœ°è¯„ä¼°äº†ç»“æ„ä¿çœŸåº¦ã€‚ç»“æœæ˜¾ç¤ºï¼Œå³ä½¿æ˜¯æœ€æ–°æ¨¡å‹ä¹Ÿå­˜åœ¨æ˜¾è‘—æ€§èƒ½å·®è·ï¼Œç”Ÿæˆä»»åŠ¡æ¯”è½¬æ¢ä»»åŠ¡æ›´å…·æŒ‘æˆ˜æ€§ï¼Œç”Ÿæˆæ­£ç¡®è§†è§‰å†…å®¹æ¯”ç”Ÿæˆçº¯æ–‡æœ¬ç»“æ„æ›´ä¸ºå›°éš¾ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è½¯ä»¶å¼€å‘ç”Ÿæˆç»“æ„åŒ–è¾“å‡ºæ–¹é¢çš„èƒ½åŠ›æ—¥ç›Šå…³é”®ã€‚</li>
<li>StructEvalæ˜¯ä¸€ç§æ–°çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°LLMç”Ÿæˆå¤šç§ç»“æ„åŒ–æ ¼å¼çš„èƒ½åŠ›ã€‚</li>
<li>StructEvalåŒ…å«ç”Ÿæˆä»»åŠ¡å’Œè½¬æ¢ä»»åŠ¡ä¸¤å¤§èŒƒå¼ï¼Œæ¶µç›–18ç§æ ¼å¼å’Œ44ç§ä»»åŠ¡ç±»å‹ã€‚</li>
<li>æœ€æ–°æ¨¡å‹åœ¨StructEvalä¸Šçš„è¡¨ç°å­˜åœ¨æ˜¾è‘—æ€§èƒ½å·®è·ã€‚</li>
<li>ç”Ÿæˆä»»åŠ¡æ¯”è½¬æ¢ä»»åŠ¡æ›´å…·æŒ‘æˆ˜æ€§ã€‚</li>
<li>ç”Ÿæˆæ­£ç¡®è§†è§‰å†…å®¹æ¯”ç”Ÿæˆçº¯æ–‡æœ¬ç»“æ„æ›´ä¸ºå›°éš¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20139">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3688aa3c5f9de6465dddbd35c37b0b31.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bac1021f7a937d6de0251596203f4108.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5e8125e576e1d14727d7fac495687e4f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-272b1bb33c587a9acbd250b7f3272861.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d263c247fc6298c75e441f23f4c59121.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7e959fab753b2873948081f936476c6c.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Named-Entity-Recognition-in-Historical-Italian-The-Case-of-Giacomo-Leopardiâ€™s-Zibaldone"><a href="#Named-Entity-Recognition-in-Historical-Italian-The-Case-of-Giacomo-Leopardiâ€™s-Zibaldone" class="headerlink" title="Named Entity Recognition in Historical Italian: The Case of Giacomo   Leopardiâ€™s Zibaldone"></a>Named Entity Recognition in Historical Italian: The Case of Giacomo   Leopardiâ€™s Zibaldone</h2><p><strong>Authors:Cristian Santini, Laura Melosi, Emanuele Frontoni</strong></p>
<p>The increased digitization of worldâ€™s textual heritage poses significant challenges for both computer science and literary studies. Overall, there is an urgent need of computational techniques able to adapt to the challenges of historical texts, such as orthographic and spelling variations, fragmentary structure and digitization errors. The rise of large language models (LLMs) has revolutionized natural language processing, suggesting promising applications for Named Entity Recognition (NER) on historical documents. In spite of this, no thorough evaluation has been proposed for Italian texts. This research tries to fill the gap by proposing a new challenging dataset for entity extraction based on a corpus of 19th century scholarly notes, i.e. Giacomo Leopardiâ€™s Zibaldone (1898), containing 2,899 references to people, locations and literary works. This dataset was used to carry out reproducible experiments with both domain-specific BERT-based models and state-of-the-art LLMs such as LLaMa3.1. Results show that instruction-tuned models encounter multiple difficulties handling historical humanistic texts, while fine-tuned NER models offer more robust performance even with challenging entity types such as bibliographic references. </p>
<blockquote>
<p>éšç€ä¸–ç•Œæ–‡æœ¬é—äº§çš„æ•°å­—åŒ–ç¨‹åº¦ä¸æ–­æé«˜ï¼Œè¿™ç»™è®¡ç®—æœºç§‘å­¦å’Œæ–‡å­¦ç ”ç©¶éƒ½å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ã€‚æ€»ä½“è€Œè¨€ï¼Œæ€¥éœ€èƒ½å¤Ÿé€‚åº”å†å²æ–‡æœ¬æŒ‘æˆ˜çš„è®¡ç®—æŠ€æœ¯ï¼Œä¾‹å¦‚æ­£å­—æ³•ã€æ‹¼å†™å˜åŒ–ã€ç‰‡æ®µç»“æ„å’Œæ•°å­—åŒ–é”™è¯¯ç­‰ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å´›èµ·å·²ç»å½»åº•æ”¹å˜äº†è‡ªç„¶è¯­è¨€å¤„ç†ï¼Œä¸ºå†å²æ–‡çŒ®ä¸­çš„å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰å¸¦æ¥äº†å……æ»¡å¸Œæœ›çš„åº”ç”¨å‰æ™¯ã€‚å°½ç®¡å¦‚æ­¤ï¼Œå°šæœªæœ‰é’ˆå¯¹æ„å¤§åˆ©è¯­æ–‡æœ¬çš„å…¨é¢è¯„ä¼°ã€‚æœ¬ç ”ç©¶è¯•å›¾é€šè¿‡æå‡ºä¸€ä¸ªåŸºäº19ä¸–çºªå­¦æœ¯ç¬”è®°çš„æ–°æ•°æ®é›†æ¥å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œè¯¥æ•°æ®é›†æ¥è‡ªè´¾ç§‘è«Â·è±å¥¥å¸•è¿ªçš„ã€Šé½å·´å°”å¤šå†…ã€‹ï¼ˆZibaldoneï¼‰ï¼ˆ1898å¹´ï¼‰ï¼ŒåŒ…å«å¯¹äººåã€åœ°åå’Œæ–‡å­¦ä½œå“çš„2899å¤„å¼•ç”¨ã€‚è¯¥æ•°æ®é›†è¢«ç”¨äºè¿›è¡Œå¯é‡å¤å®éªŒï¼Œé‡‡ç”¨åŸºäºBERTçš„ç‰¹å®šé¢†åŸŸæ¨¡å‹å’Œæœ€æ–°çš„å¤§å‹è¯­è¨€æ¨¡å‹LLaMa 3.1ç­‰å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå¤„ç†ã€‚ç»“æœè¡¨æ˜ï¼Œç»è¿‡æŒ‡ä»¤è°ƒä¼˜çš„æ¨¡å‹åœ¨å¤„ç†å†å²äººæ–‡æ–‡æœ¬æ—¶é‡åˆ°å¤šé‡å›°éš¾ï¼Œè€Œç»è¿‡ç²¾ç»†è°ƒæ•´çš„NERæ¨¡å‹å³ä½¿åœ¨å¤„ç†å¦‚å‚è€ƒæ–‡çŒ®ç­‰å…·æœ‰æŒ‘æˆ˜æ€§çš„å®ä½“ç±»å‹æ—¶ä¹Ÿèƒ½æä¾›æ›´ç¨³å¥çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20113v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>éšç€ä¸–ç•Œæ–‡æœ¬é—äº§çš„æ•°å­—åŒ–è¿›ç¨‹åŠ é€Ÿï¼Œè®¡ç®—æœºç§‘å­¦ä¸æ–‡å­¦ç ”ç©¶é¢†åŸŸé¢ä¸´ç€è¯¸å¤šæŒ‘æˆ˜ã€‚ä¸ºé€‚åº”å†å²æ–‡æœ¬çš„ç‰¹æ®Šæ€§ï¼Œå¦‚æ­£å­—æ³•ã€æ‹¼å†™å˜åŒ–ã€ç»“æ„ç‰‡æ®µåŒ–å’Œæ•°å­—åŒ–é”™è¯¯ç­‰ï¼Œæ€¥éœ€å‘å±•è®¡ç®—æŠ€æœ¯ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸçš„åº”ç”¨å…·æœ‰é©å‘½æ€§æ„ä¹‰ï¼Œç‰¹åˆ«æ˜¯åœ¨å†å²æ–‡çŒ®çš„å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œé’ˆå¯¹æ„å¤§åˆ©æ–‡æœ¬çš„ç ”ç©¶å°šç¼ºä¹å…¨é¢è¯„ä¼°ã€‚æœ¬ç ”ç©¶å°è¯•ä»¥19ä¸–çºªçš„å­¦æœ¯ç¬”è®°â€”â€”è´¾ç§‘è«Â·åˆ—å¥¥å¸•è’‚çš„ã€Šå‰å·´å°”å¤šå†…ã€‹ï¼ˆ1898ï¼‰ä¸ºåŸºç¡€ï¼Œå»ºç«‹ä¸€ä¸ªåŒ…å«2899ä¸ªå…³äºäººç‰©ã€åœ°ç‚¹å’Œæ–‡å­¦ä½œå“å¼•ç”¨çš„å®ä½“æå–æ•°æ®é›†ï¼Œä»¥å¡«è¡¥è¿™ä¸€ç©ºç™½ã€‚è¯¥æ•°æ®é›†ç”¨äºè¿›è¡Œå¯é‡å¤å®éªŒï¼Œæ¯”è¾ƒç‰¹å®šé¢†åŸŸçš„BERTæ¨¡å‹å’Œæœ€æ–°çš„LLMsï¼ˆå¦‚LLaMa 3.1ï¼‰ã€‚ç»“æœæ˜¾ç¤ºï¼ŒæŒ‡ä»¤å¾®è°ƒæ¨¡å‹åœ¨å¤„ç†å†å²äººæ–‡æ–‡æœ¬æ—¶é¢ä¸´è¯¸å¤šå›°éš¾ï¼Œè€Œç²¾ç»†è°ƒæ•´çš„NERæ¨¡å‹å³ä½¿åœ¨å¤„ç†å…·æœ‰æŒ‘æˆ˜æ€§çš„å®ä½“ç±»å‹ï¼ˆå¦‚å‚è€ƒæ–‡çŒ®ï¼‰æ—¶ä¹Ÿèƒ½æä¾›æ›´ç¨³å¥çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬é—äº§æ•°å­—åŒ–å¯¹è®¡ç®—æœºç§‘å­¦å’Œæ–‡å­¦ç ”ç©¶é¢†åŸŸå¸¦æ¥æŒ‘æˆ˜ã€‚</li>
<li>å†å²æ–‡æœ¬çš„ç‰¹æ®Šæ€§åŒ…æ‹¬æ­£å­—æ³•ã€æ‹¼å†™å˜åŒ–ã€ç»“æ„ç‰‡æ®µåŒ–å’Œæ•°å­—åŒ–é”™è¯¯ç­‰ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå…·æœ‰æ˜¾è‘—æ½œåŠ›ã€‚</li>
<li>å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰åœ¨å†å²æ–‡çŒ®ä¸­çš„åº”ç”¨æ˜¯LLMçš„ä¸€ä¸ªé‡è¦åº”ç”¨é¢†åŸŸã€‚</li>
<li>é’ˆå¯¹æ„å¤§åˆ©æ–‡æœ¬çš„ç ”ç©¶åœ¨LLMåº”ç”¨æ–¹é¢ç¼ºä¹å…¨é¢è¯„ä¼°ã€‚</li>
<li>ç ”ç©¶ä½¿ç”¨19ä¸–çºªå­¦æœ¯ç¬”è®°å»ºç«‹äº†ä¸€ä¸ªå®ä½“æå–æ•°æ®é›†ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œç²¾ç»†è°ƒæ•´çš„NERæ¨¡å‹åœ¨å¤„ç†å…·æœ‰æŒ‘æˆ˜æ€§çš„å®ä½“ç±»å‹æ—¶è¡¨ç°æ›´ç¨³å¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20113">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-f310c9b0a9e890920c40154db0169f79.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b60ac1d69d52406fdea33e0d93fdbafa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-12c5c169f2cf42d7f13a60038a0189de.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b7f4a3f7e272760e5bd89fe7a431db9d.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="ViTaPEs-Visuotactile-Position-Encodings-for-Cross-Modal-Alignment-in-Multimodal-Transformers"><a href="#ViTaPEs-Visuotactile-Position-Encodings-for-Cross-Modal-Alignment-in-Multimodal-Transformers" class="headerlink" title="ViTaPEs: Visuotactile Position Encodings for Cross-Modal Alignment in   Multimodal Transformers"></a>ViTaPEs: Visuotactile Position Encodings for Cross-Modal Alignment in   Multimodal Transformers</h2><p><strong>Authors:Fotios Lygerakis, Ozan Ã–zdenizci, Elmar RÃ¼ckert</strong></p>
<p>Tactile sensing provides local essential information that is complementary to visual perception, such as texture, compliance, and force. Despite recent advances in visuotactile representation learning, challenges remain in fusing these modalities and generalizing across tasks and environments without heavy reliance on pre-trained vision-language models. Moreover, existing methods do not study positional encodings, thereby overlooking the multi-scale spatial reasoning needed to capture fine-grained visuotactile correlations. We introduce ViTaPEs, a transformer-based framework that robustly integrates visual and tactile input data to learn task-agnostic representations for visuotactile perception. Our approach exploits a novel multi-scale positional encoding scheme to capture intra-modal structures, while simultaneously modeling cross-modal cues. Unlike prior work, we provide provable guarantees in visuotactile fusion, showing that our encodings are injective, rigid-motion-equivariant, and information-preserving, validating these properties empirically. Experiments on multiple large-scale real-world datasets show that ViTaPEs not only surpasses state-of-the-art baselines across various recognition tasks but also demonstrates zero-shot generalization to unseen, out-of-domain scenarios. We further demonstrate the transfer-learning strength of ViTaPEs in a robotic grasping task, where it outperforms state-of-the-art baselines in predicting grasp success. Project page: <a target="_blank" rel="noopener" href="https://sites.google.com/view/vitapes">https://sites.google.com/view/vitapes</a> </p>
<blockquote>
<p>è§¦è§‰æ„ŸçŸ¥æä¾›äº†ä¸è§†è§‰æ„ŸçŸ¥äº’è¡¥çš„å±€éƒ¨é‡è¦ä¿¡æ¯ï¼Œå¦‚çº¹ç†ã€å¼¹æ€§å’ŒåŠ›åº¦ã€‚å°½ç®¡è§†è§‰è§¦è§‰è¡¨ç¤ºå­¦ä¹ æ–¹é¢æœ€è¿‘å–å¾—äº†è¿›å±•ï¼Œä½†èåˆè¿™äº›æ¨¡å¼ã€è·¨ä»»åŠ¡å’Œè·¨ç¯å¢ƒæ¨å¹¿ä»ç„¶é¢ä¸´æŒ‘æˆ˜ï¼Œè€Œä¸”ä¸èƒ½è¿‡åº¦ä¾èµ–é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ã€‚æ­¤å¤–ï¼Œç°æœ‰æ–¹æ³•å¹¶æœªç ”ç©¶ä½ç½®ç¼–ç ï¼Œä»è€Œå¿½ç•¥äº†æ•æ‰ç²¾ç»†è§†è§‰è§¦è§‰å…³è”æ‰€éœ€çš„å¤šå°ºåº¦ç©ºé—´æ¨ç†ã€‚æˆ‘ä»¬å¼•å…¥äº†ViTaPEsï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºtransformerçš„æ¡†æ¶ï¼Œèƒ½å¤Ÿç¨³å¥åœ°é›†æˆè§†è§‰å’Œè§¦è§‰è¾“å…¥æ•°æ®ï¼Œå­¦ä¹ ç”¨äºè§†è§‰è§¦è§‰æ„ŸçŸ¥çš„ä»»åŠ¡é€šç”¨è¡¨ç¤ºã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨äº†ä¸€ç§æ–°å‹çš„å¤šå°ºåº¦ä½ç½®ç¼–ç æ–¹æ¡ˆï¼Œæ¥æ•æ‰å†…éƒ¨æ¨¡å¼ç»“æ„ï¼ŒåŒæ—¶å»ºæ¨¡è·¨æ¨¡å¼çº¿ç´¢ã€‚ä¸å…ˆå‰çš„å·¥ä½œä¸åŒï¼Œæˆ‘ä»¬åœ¨è§†è§‰è§¦è§‰èåˆæ–¹é¢æä¾›äº†å¯è¯æ˜çš„ä¿è¯ï¼Œè¡¨æ˜æˆ‘ä»¬çš„ç¼–ç æ˜¯æ³¨å…¥çš„ã€åˆšæ€§è¿åŠ¨ç­‰å˜çš„ã€ä¿¡æ¯ä¿æŒçš„ï¼Œå¹¶é€šè¿‡å®éªŒéªŒè¯äº†è¿™äº›å±æ€§ã€‚åœ¨å¤šä¸ªå¤§è§„æ¨¡ç°å®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒViTaPEsä¸ä»…åœ¨å„ç§è¯†åˆ«ä»»åŠ¡ä¸Šè¶…è¶Šäº†æœ€æ–°åŸºçº¿ï¼Œè€Œä¸”è¿˜è¡¨ç°å‡ºå¯¹æœªè§è¿‡çš„ã€è¶…å‡ºé¢†åŸŸèŒƒå›´çš„åœºæ™¯çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥å±•ç¤ºäº†ViTaPEsåœ¨æœºå™¨äººæŠ“å–ä»»åŠ¡ä¸­çš„è¿ç§»å­¦ä¹ ä¼˜åŠ¿ï¼Œåœ¨é¢„æµ‹æŠ“å–æˆåŠŸæ–¹é¢è¶…è¶Šäº†æœ€æ–°åŸºçº¿ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://sites.google.com/view/vitapes">https://sites.google.com/view/vitapes</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20032v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è§†è§‰å’Œè§¦è§‰æ„ŸçŸ¥çš„ç»“åˆå¯¹äºè·å–ç‰©ä½“çš„çº¹ç†ã€å¼¹æ€§å’ŒåŠ›åº¦ç­‰å±€éƒ¨é‡è¦ä¿¡æ¯è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•åœ¨èåˆè¿™ä¸¤ç§æ¨¡æ€æ—¶ä»é¢ä¸´æŒ‘æˆ˜ï¼Œç¼ºä¹è·¨ä»»åŠ¡å’Œç¯å¢ƒçš„é€šç”¨æ€§ï¼Œä¸”è¿‡åº¦ä¾èµ–é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå˜å‹å™¨çš„æ–°æ¡†æ¶ViTaPEsï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿç¨³å¥åœ°æ•´åˆè§†è§‰å’Œè§¦è§‰è¾“å…¥æ•°æ®ï¼Œå­¦ä¹ ç”¨äºè§†è§‰è§¦è§‰æ„ŸçŸ¥çš„ä»»åŠ¡æ— å…³è¡¨ç¤ºã€‚é€šè¿‡ä¸€ç§æ–°çš„å¤šå°ºåº¦ä½ç½®ç¼–ç æ–¹æ¡ˆï¼ŒViTaPEsèƒ½å¤Ÿæ•æ‰è·¨æ¨¡æ€çº¿ç´¢çš„æ¨¡æ€å†…ç»“æ„å’Œè·¨æ¨¡æ€çº¿ç´¢ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡ä¸ºè§¦è§‰è§†è§‰èåˆæä¾›äº†å¯è¯æ˜çš„ç†è®ºä¿è¯ï¼Œå¹¶åœ¨å¤šä¸ªå¤§è§„æ¨¡ç°å®æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒViTaPEsä¸ä»…è¶…è¶Šäº†å„ç§è¯†åˆ«ä»»åŠ¡çš„æœ€æ–°åŸºçº¿ï¼Œè€Œä¸”åœ¨æœªè§è¿‡çš„ã€è¶…å‡ºé¢†åŸŸèŒƒå›´çš„åœºæ™¯ä¸­å®ç°äº†é›¶æ ·æœ¬æ³›åŒ–ã€‚åŒæ—¶ï¼Œåœ¨æœºå™¨äººæŠ“å–ä»»åŠ¡ä¸­ï¼ŒViTaPEsçš„è¿ç§»å­¦ä¹ èƒ½åŠ›ä¹Ÿå¾—åˆ°äº†éªŒè¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰å’Œè§¦è§‰æ„ŸçŸ¥ç»“åˆå¯¹äºè·å–ç‰©ä½“å±€éƒ¨é‡è¦ä¿¡æ¯è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰æ–¹æ³•åœ¨èåˆè§†è§‰å’Œè§¦è§‰æ¨¡æ€æ—¶å­˜åœ¨æŒ‘æˆ˜ï¼Œç¼ºä¹è·¨ä»»åŠ¡å’Œç¯å¢ƒçš„é€šç”¨æ€§ã€‚</li>
<li>ViTaPEsæ¡†æ¶èƒ½å¤Ÿç¨³å¥åœ°æ•´åˆè§†è§‰å’Œè§¦è§‰è¾“å…¥æ•°æ®ï¼Œå­¦ä¹ ä»»åŠ¡æ— å…³è¡¨ç¤ºã€‚</li>
<li>ViTaPEsé‡‡ç”¨æ–°çš„å¤šå°ºåº¦ä½ç½®ç¼–ç æ–¹æ¡ˆï¼Œæ•æ‰è·¨æ¨¡æ€çº¿ç´¢çš„æ¨¡æ€å†…ç»“æ„å’Œè·¨æ¨¡æ€ä¿¡æ¯ã€‚</li>
<li>ViTaPEsä¸ºè§¦è§‰è§†è§‰èåˆæä¾›äº†å¯è¯æ˜çš„ç†è®ºä¿è¯ã€‚</li>
<li>ViTaPEsåœ¨å¤šä¸ªå¤§è§„æ¨¡ç°å®æ•°æ®é›†ä¸Šè¡¨ç°è¶…è¶Šæœ€æ–°åŸºçº¿ï¼Œå®ç°é›¶æ ·æœ¬æ³›åŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20032">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-52dff2c9a71288bbd428be00c9b92b14.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-25904e254aff2f6cd9178d97ddcb5b11.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3c6208be9eefba08aab7c4d42e05e27d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-295fd0c1fde767616fd0e9a21fd20efd.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Correlating-instruction-tuning-in-multimodal-models-with-vision-language-processing-in-the-brain"><a href="#Correlating-instruction-tuning-in-multimodal-models-with-vision-language-processing-in-the-brain" class="headerlink" title="Correlating instruction-tuning (in multimodal models) with   vision-language processing (in the brain)"></a>Correlating instruction-tuning (in multimodal models) with   vision-language processing (in the brain)</h2><p><strong>Authors:Subba Reddy Oota, Akshett Jindal, Ishani Mondal, Khushbu Pahwa, Satya Sai Srinath Namburi, Manish Shrivastava, Maneesh Singh, Bapi S. Raju, Manish Gupta</strong></p>
<p>Transformer-based language models, though not explicitly trained to mimic brain recordings, have demonstrated surprising alignment with brain activity. Progress in these models-through increased size, instruction-tuning, and multimodality-has led to better representational alignment with neural data. Recently, a new class of instruction-tuned multimodal LLMs (MLLMs) have emerged, showing remarkable zero-shot capabilities in open-ended multimodal vision tasks. However, it is unknown whether MLLMs, when prompted with natural instructions, lead to better brain alignment and effectively capture instruction-specific representations. To address this, we first investigate brain alignment, i.e., measuring the degree of predictivity of neural visual activity using text output response embeddings from MLLMs as participants engage in watching natural scenes. Experiments with 10 different instructions show that MLLMs exhibit significantly better brain alignment than vision-only models and perform comparably to non-instruction-tuned multimodal models like CLIP. We also find that while these MLLMs are effective at generating high-quality responses suitable to the task-specific instructions, not all instructions are relevant for brain alignment. Further, by varying instructions, we make the MLLMs encode instruction-specific visual concepts related to the input image. This analysis shows that MLLMs effectively capture count-related and recognition-related concepts, demonstrating strong alignment with brain activity. Notably, the majority of the explained variance of the brain encoding models is shared between MLLM embeddings of image captioning and other instructions. These results suggest that enhancing MLLMsâ€™ ability to capture task-specific information could lead to better differentiation between various types of instructions, and thereby improving their precision in predicting brain responses. </p>
<blockquote>
<p>è™½ç„¶åŸºäºTransformerçš„è¯­è¨€æ¨¡å‹å¹¶æœªç»è¿‡æ˜ç¡®çš„è®­ç»ƒæ¥æ¨¡ä»¿è„‘ç”µæ³¢è®°å½•ï¼Œä½†å®ƒä»¬å·²ç»è¡¨ç°å‡ºä»¤äººæƒŠè®¶çš„ä¸è„‘æ´»åŠ¨çš„å¯¹é½æ€§ã€‚è¿™äº›æ¨¡å‹é€šè¿‡æ‰©å¤§è§„æ¨¡ã€æŒ‡ä»¤å¾®è°ƒä»¥åŠå¤šæ¨¡æ€èåˆçš„å‘å±•ï¼Œä½¿å¾—å®ƒä»¬åœ¨ç¥ç»æ•°æ®è¡¨å¾å¯¹é½æ–¹é¢è¡¨ç°æ›´å¥½ã€‚æœ€è¿‘ï¼Œæ–°å…´çš„ä¸€ç±»ç»è¿‡æŒ‡ä»¤å¾®è°ƒçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¼€æ”¾å¼çš„å¤šæ¨¡æ€è§†è§‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†å¼•äººæ³¨ç›®çš„é›¶æ ·æœ¬èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå½“äººä»¬ç”¨è‡ªç„¶è¯­è¨€æŒ‡ä»¤æ¥æç¤ºMLLMsæ—¶ï¼Œå®ƒä»¬æ˜¯å¦èƒ½å®ç°å¯¹å¤§è„‘çš„æ›´å¥½å¯¹é½å¹¶æœ‰æ•ˆæ•è·æŒ‡ä»¤ç‰¹å®šçš„è¡¨å¾ä»æ˜¯æœªçŸ¥çš„ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬é¦–å…ˆç ”ç©¶å¤§è„‘å¯¹é½é—®é¢˜ï¼Œå³æµ‹é‡å‚ä¸è€…è§‚çœ‹è‡ªç„¶åœºæ™¯æ—¶ï¼Œä½¿ç”¨MLLMsäº§ç”Ÿçš„æ–‡æœ¬è¾“å‡ºå“åº”åµŒå…¥æ¥æé«˜ç¥ç»è§†è§‰æ´»åŠ¨çš„é¢„æµ‹æ€§ã€‚ç”¨1:é¡¹ä¸åŒæŒ‡ä»¤è¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒMLLMsåœ¨å¤§è„‘å¯¹é½æ–¹é¢æ˜¾è‘—ä¼˜äºä»…ç”¨äºè§†è§‰çš„æ¨¡å‹ï¼Œå¹¶ä¸”ä¸æœªç»æŒ‡ä»¤è°ƒæ•´çš„å¤šæ¨¡æ€æ¨¡å‹ï¼ˆå¦‚CLIPï¼‰è¡¨ç°ç›¸å½“ã€‚æˆ‘ä»¬è¿˜å‘ç°ï¼Œè™½ç„¶è¿™äº›MLLMsåœ¨ç”Ÿæˆé€‚åˆç‰¹å®šä»»åŠ¡æŒ‡ä»¤çš„é«˜è´¨é‡å“åº”æ–¹é¢éå¸¸æœ‰æ•ˆï¼Œä½†å¹¶éæ‰€æœ‰æŒ‡ä»¤éƒ½ä¸å¤§è„‘å¯¹é½ç›¸å…³ã€‚æ­¤å¤–ï¼Œé€šè¿‡æ”¹å˜æŒ‡ä»¤ï¼Œæˆ‘ä»¬è®©MLLMså¯¹è¾“å…¥å›¾åƒè¿›è¡Œç¼–ç ï¼Œæ¶‰åŠç‰¹å®šæŒ‡ä»¤çš„è§†è§‰æ¦‚å¿µã€‚è¿™ä¸€åˆ†æè¡¨æ˜ï¼ŒMLLMsèƒ½å¤Ÿæœ‰æ•ˆåœ°æ•è·è®¡æ•°ç›¸å…³å’Œè¯†åˆ«ç›¸å…³çš„æ¦‚å¿µï¼Œè¡¨ç°å‡ºä¸è„‘æ´»åŠ¨çš„å¼ºçƒˆå¯¹é½æ€§ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå¤§è„‘ç¼–ç æ¨¡å‹çš„å¤§éƒ¨åˆ†è§£é‡Šæ–¹å·®å­˜åœ¨äºMLLMçš„å›¾åƒæè¿°å’Œå…¶ä»–æŒ‡ä»¤çš„åµŒå…¥ä¹‹é—´ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œæé«˜MLLMsæ•è·ç‰¹å®šä»»åŠ¡ä¿¡æ¯çš„èƒ½åŠ›å¯èƒ½ä¼šä½¿å®ƒä»¬èƒ½å¤Ÿæ›´å¥½åœ°åŒºåˆ†ä¸åŒç±»å‹çš„æŒ‡ä»¤ï¼Œä»è€Œæé«˜å®ƒä»¬åœ¨é¢„æµ‹å¤§è„‘ååº”æ–¹é¢çš„å‡†ç¡®æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20029v1">PDF</a> 30 pages, 22 figures, The Thirteenth International Conference on   Learning Representations, ICLR-2025, Singapore.   <a target="_blank" rel="noopener" href="https://openreview.net/pdf?id=xkgfLXZ4e0">https://openreview.net/pdf?id=xkgfLXZ4e0</a></p>
<p><strong>æ‘˜è¦</strong></p>
<p>åŸºäºTransformerçš„è¯­è¨€æ¨¡å‹è™½æœªç»è¿‡æ¨¡ä»¿è„‘è®°å½•çš„æ˜ç¡®è®­ç»ƒï¼Œä½†å·²æ˜¾ç¤ºå‡ºä¸è„‘æ´»åŠ¨æƒŠäººçš„å¯¹é½ç¨‹åº¦ã€‚è¿™äº›æ¨¡å‹çš„è¿›æ­¥â€”â€”é€šè¿‡è§„æ¨¡æ‰©å¤§ã€æŒ‡ä»¤å¾®è°ƒå’Œå¤šæ¨¡æ€â€”â€”å¯¼è‡´äº†ä¸ç¥ç»æ•°æ®çš„ä»£è¡¨æ€§å¯¹é½èƒ½åŠ›çš„æé«˜ã€‚æœ€è¿‘ï¼Œä¸€ç±»æ–°çš„æŒ‡ä»¤å¾®è°ƒå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„å‡ºç°ï¼Œåœ¨å¼€æ”¾çš„å¤šæ¨¡æ€è§†è§‰ä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºæƒŠäººçš„é›¶é•œå¤´èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå°šä¸æ¸…æ¥šå½“MLLMsæ¥æ”¶åˆ°è‡ªç„¶æŒ‡ä»¤æ—¶ï¼Œæ˜¯å¦ä¼šå¯¹è„‘å¯¹é½äº§ç”Ÿæ›´å¥½çš„å½±å“ï¼Œå¹¶æœ‰æ•ˆåœ°æ•è·æŒ‡ä»¤ç‰¹å®šçš„è¡¨ç¤ºã€‚æœ¬ç ”ç©¶é¦–å…ˆè°ƒæŸ¥äº†è„‘å¯¹é½ï¼Œå³ä½¿ç”¨MLLMsçš„æ–‡æœ¬è¾“å‡ºå“åº”åµŒå…¥æ¥æµ‹é‡å‚ä¸è€…è§‚çœ‹è‡ªç„¶åœºæ™¯æ—¶ç¥ç»è§†è§‰æ´»åŠ¨çš„é¢„æµ‹ç¨‹åº¦ã€‚å®éªŒæ˜¾ç¤ºï¼Œä¸ä»…å…³æ³¨è§†è§‰çš„æ¨¡å‹ç›¸æ¯”ï¼ŒMLLMsåœ¨è„‘å¯¹é½æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—çš„ä¼˜åŠ¿ï¼Œå¹¶ä¸”ä¸éæŒ‡ä»¤è°ƒæ•´çš„å¤šæ¨¡æ€æ¨¡å‹å¦‚CLIPçš„è¡¨ç°ç›¸å½“ã€‚æˆ‘ä»¬è¿˜å‘ç°ï¼Œè™½ç„¶è¿™äº›MLLMsåœ¨ç”Ÿæˆé€‚åˆç‰¹å®šä»»åŠ¡æŒ‡ä»¤çš„é«˜è´¨é‡å“åº”æ–¹é¢éå¸¸æœ‰æ•ˆï¼Œä½†å¹¶ä¸æ˜¯æ‰€æœ‰æŒ‡ä»¤éƒ½ä¸è„‘å¯¹é½ç›¸å…³ã€‚æ­¤å¤–ï¼Œé€šè¿‡æ”¹å˜æŒ‡ä»¤ï¼Œæˆ‘ä»¬ä½¿MLLMsç¼–ç ä¸è¾“å…¥å›¾åƒç›¸å…³çš„æŒ‡ä»¤ç‰¹å®šè§†è§‰æ¦‚å¿µã€‚åˆ†ææ˜¾ç¤ºï¼ŒMLLMsæœ‰æ•ˆåœ°æ•è·äº†è®¡æ•°ç›¸å…³å’Œè¯†åˆ«ç›¸å…³çš„æ¦‚å¿µï¼Œæ˜¾ç¤ºå‡ºä¸è„‘æ´»åŠ¨çš„å¼ºçƒˆå¯¹é½ã€‚ç‰¹åˆ«æ˜¯ï¼Œå¤§è„‘ç¼–ç æ¨¡å‹çš„å¤§éƒ¨åˆ†è§£é‡Šæ–¹å·®éƒ½å­˜åœ¨äºMLLMçš„å›¾åƒæè¿°å’Œå…¶ä»–æŒ‡ä»¤åµŒå…¥ä¹‹é—´ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œæé«˜MLLMsæ•è·ç‰¹å®šä»»åŠ¡ä¿¡æ¯çš„èƒ½åŠ›å¯èƒ½ä¼šå¯¼è‡´ä¸åŒç±»å‹æŒ‡ä»¤ä¹‹é—´çš„æ›´å¥½åŒºåˆ†ï¼Œä»è€Œæ”¹å–„å®ƒä»¬åœ¨é¢„æµ‹å¤§è„‘ååº”æ–¹é¢çš„ç²¾ç¡®åº¦ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>åŸºäºTransformerçš„è¯­è¨€æ¨¡å‹ä¸è„‘æ´»åŠ¨æ˜¾ç¤ºå‡ºæƒŠäººçš„å¯¹é½ç¨‹åº¦ã€‚</li>
<li>æŒ‡ä»¤å¾®è°ƒçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è„‘å¯¹é½æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚</li>
<li>MLLMsåœ¨ç”Ÿæˆé€‚åˆç‰¹å®šä»»åŠ¡æŒ‡ä»¤çš„é«˜è´¨é‡å“åº”æ–¹é¢éå¸¸æœ‰æ•ˆã€‚</li>
<li>å¹¶éæ‰€æœ‰æŒ‡ä»¤éƒ½ä¸è„‘å¯¹é½ç›¸å…³ã€‚</li>
<li>MLLMsèƒ½å¤Ÿé€šè¿‡ç¼–ç æŒ‡ä»¤ç‰¹å®šçš„è§†è§‰æ¦‚å¿µæ¥å“åº”è¾“å…¥å›¾åƒã€‚</li>
<li>MLLMsæœ‰æ•ˆåœ°æ•è·è®¡æ•°å’Œè¯†åˆ«ç›¸å…³çš„æ¦‚å¿µï¼Œä¸è„‘æ´»åŠ¨å¯¹é½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20029">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-78d421c20b127e895d43d18e3f6a02b5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1f4423b8d556593e0d3950d7878a8085.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b863642b2b6b034b8c7a40a35bdaa28.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7bd807a05fd8d0d6eaa6295c3154acb0.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Automated-CAD-Modeling-Sequence-Generation-from-Text-Descriptions-via-Transformer-Based-Large-Language-Models"><a href="#Automated-CAD-Modeling-Sequence-Generation-from-Text-Descriptions-via-Transformer-Based-Large-Language-Models" class="headerlink" title="Automated CAD Modeling Sequence Generation from Text Descriptions via   Transformer-Based Large Language Models"></a>Automated CAD Modeling Sequence Generation from Text Descriptions via   Transformer-Based Large Language Models</h2><p><strong>Authors:Jianxing Liao, Junyan Xu, Yatao Sun, Maowen Tang, Sicheng He, Jingxian Liao, Shui Yu, Yun Li, Hongguan Xiao</strong></p>
<p>Designing complex computer-aided design (CAD) models is often time-consuming due to challenges such as computational inefficiency and the difficulty of generating precise models. We propose a novel language-guided framework for industrial design automation to address these issues, integrating large language models (LLMs) with computer-automated design (CAutoD).Through this framework, CAD models are automatically generated from parameters and appearance descriptions, supporting the automation of design tasks during the detailed CAD design phase. Our approach introduces three key innovations: (1) a semi-automated data annotation pipeline that leverages LLMs and vision-language large models (VLLMs) to generate high-quality parameters and appearance descriptions; (2) a Transformer-based CAD generator (TCADGen) that predicts modeling sequences via dual-channel feature aggregation; (3) an enhanced CAD modeling generation model, called CADLLM, that is designed to refine the generated sequences by incorporating the confidence scores from TCADGen. Experimental results demonstrate that the proposed approach outperforms traditional methods in both accuracy and efficiency, providing a powerful tool for automating industrial workflows and generating complex CAD models from textual prompts. The code is available at <a target="_blank" rel="noopener" href="https://jianxliao.github.io/cadllm-page/">https://jianxliao.github.io/cadllm-page/</a> </p>
<blockquote>
<p>è®¾è®¡å¤æ‚çš„è®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰æ¨¡å‹é€šå¸¸å¾ˆè€—æ—¶ï¼Œå› ä¸ºå­˜åœ¨è®¡ç®—æ•ˆç‡ä½ä¸‹å’Œéš¾ä»¥ç”Ÿæˆç²¾ç¡®æ¨¡å‹ç­‰æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºå·¥ä¸šè‡ªåŠ¨åŒ–è®¾è®¡çš„æ–°å‹è¯­è¨€å¼•å¯¼æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸è®¡ç®—æœºè‡ªåŠ¨åŒ–è®¾è®¡ï¼ˆCAutoDï¼‰ç›¸ç»“åˆã€‚é€šè¿‡æ­¤æ¡†æ¶ï¼ŒCADæ¨¡å‹å¯ä»¥æ ¹æ®å‚æ•°å’Œå¤–è§‚æè¿°è‡ªåŠ¨ç”Ÿæˆï¼Œæ”¯æŒè¯¦ç»†CADè®¾è®¡é˜¶æ®µçš„è®¾è®¡ä»»åŠ¡è‡ªåŠ¨åŒ–ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†ä¸‰ä¸ªå…³é”®åˆ›æ–°ç‚¹ï¼šï¼ˆ1ï¼‰ä¸€ç§åŠè‡ªåŠ¨æ•°æ®æ³¨é‡Šç®¡é“ï¼Œåˆ©ç”¨LLMå’Œè§†è§‰è¯­è¨€å¤§å‹æ¨¡å‹ï¼ˆVLLMï¼‰ç”Ÿæˆé«˜è´¨é‡å‚æ•°å’Œå¤–è§‚æè¿°ï¼›ï¼ˆ2ï¼‰åŸºäºTransformerçš„CADç”Ÿæˆå™¨ï¼ˆTCADGenï¼‰ï¼Œé€šè¿‡åŒé€šé“ç‰¹å¾èšåˆæ¥é¢„æµ‹å»ºæ¨¡åºåˆ—ï¼›ï¼ˆ3ï¼‰å¢å¼ºå‹CADå»ºæ¨¡ç”Ÿæˆæ¨¡å‹ï¼Œå³CADLLMï¼Œæ—¨åœ¨é€šè¿‡ç»“åˆTCADGençš„ä¿¡å¿ƒåˆ†æ•°æ¥ä¼˜åŒ–ç”Ÿæˆçš„åºåˆ—ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å‡†ç¡®æ€§å’Œæ•ˆç‡æ–¹é¢éƒ½ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œä¸ºè‡ªåŠ¨åŒ–å·¥ä¸šå·¥ä½œæµç¨‹å’Œæ ¹æ®æ–‡æœ¬æç¤ºç”Ÿæˆå¤æ‚çš„CADæ¨¡å‹æä¾›äº†å¼ºå¤§çš„å·¥å…·ã€‚ä»£ç å¯åœ¨[<a target="_blank" rel="noopener" href="https://jianxliao.github.io/cadllm-page/]%E6%89%BE%E5%88%B0%E3%80%82">https://jianxliao.github.io/cadllm-page/]æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19490v1">PDF</a> Accepted by ACL 2025 Main Conference</p>
<p><strong>Summary</strong></p>
<p>ä¸€ä¸ªå…¨æ–°çš„è¯­è¨€å¼•å¯¼å·¥ä¸šè®¾è®¡è‡ªåŠ¨åŒ–æ¡†æ¶è¢«æå‡ºï¼Œè¯¥æ¡†æ¶æ•´åˆäº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸è®¡ç®—æœºè‡ªåŠ¨åŒ–è®¾è®¡ï¼ˆCAutoDï¼‰ã€‚é€šè¿‡è¿™ä¸€æ¡†æ¶ï¼Œå¯ä»¥ä»å‚æ•°å’Œå¤–è§‚æè¿°è‡ªåŠ¨ç”ŸæˆCADæ¨¡å‹ï¼Œæ”¯æŒè¯¦ç»†è®¾è®¡é˜¶æ®µçš„è‡ªåŠ¨åŒ–è®¾è®¡ä»»åŠ¡ã€‚æ­¤æ¡†æ¶åŒ…å«ä¸‰ä¸ªä¸»è¦åˆ›æ–°ç‚¹ï¼šåŠè‡ªåŠ¨æ•°æ®æ ‡æ³¨ç®¡é“ã€åŸºäºTransformerçš„CADç”Ÿæˆå™¨ï¼ˆTCADGenï¼‰å’Œå¢å¼ºçš„CADå»ºæ¨¡ç”Ÿæˆæ¨¡å‹CADLLMã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å‡†ç¡®æ€§å’Œæ•ˆç‡ä¸Šå‡ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œä¸ºè‡ªåŠ¨åŒ–å·¥ä¸šæµç¨‹å’Œä»æ–‡æœ¬æç¤ºç”Ÿæˆå¤æ‚CADæ¨¡å‹æä¾›äº†å¼ºå¤§å·¥å…·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ•´åˆå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸è®¡ç®—æœºè‡ªåŠ¨åŒ–è®¾è®¡ï¼ˆCAutoDï¼‰ï¼Œå½¢æˆæ–°çš„è®¾è®¡è‡ªåŠ¨åŒ–æ¡†æ¶ã€‚</li>
<li>é€šè¿‡è¯¥æ¡†æ¶ï¼Œå¯ä»¥ä»å‚æ•°å’Œå¤–è§‚æè¿°è‡ªåŠ¨ç”ŸæˆCADæ¨¡å‹ã€‚</li>
<li>é‡‡ç”¨åŠè‡ªåŠ¨æ•°æ®æ ‡æ³¨ç®¡é“ï¼Œåˆ©ç”¨LLMså’Œè§†è§‰è¯­è¨€å¤§å‹æ¨¡å‹ï¼ˆVLLMsï¼‰ç”Ÿæˆé«˜è´¨é‡å‚æ•°å’Œå¤–è§‚æè¿°ã€‚</li>
<li>åˆ©ç”¨åŸºäºTransformerçš„CADç”Ÿæˆå™¨ï¼ˆTCADGenï¼‰é€šè¿‡åŒé€šé“ç‰¹å¾èšåˆé¢„æµ‹å»ºæ¨¡åºåˆ—ã€‚</li>
<li>æ¨å‡ºå¢å¼ºçš„CADå»ºæ¨¡ç”Ÿæˆæ¨¡å‹CADLLMï¼Œç»“åˆTCADGençš„ä¿¡å¿ƒåˆ†æ•°ä¼˜åŒ–ç”Ÿæˆåºåˆ—ã€‚</li>
<li>å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å‡†ç¡®æ€§å’Œæ•ˆç‡ä¸Šå‡ä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19490">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8bf87cea09b4d7d92a23af6cd91eea52.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-476cd34b77b90f40bdc6d477d3c51572.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-570e66c82a74441f84ce8df5509fc916.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="RTime-QA-A-Benchmark-for-Atomic-Temporal-Event-Understanding-in-Large-Multi-modal-Models"><a href="#RTime-QA-A-Benchmark-for-Atomic-Temporal-Event-Understanding-in-Large-Multi-modal-Models" class="headerlink" title="RTime-QA: A Benchmark for Atomic Temporal Event Understanding in Large   Multi-modal Models"></a>RTime-QA: A Benchmark for Atomic Temporal Event Understanding in Large   Multi-modal Models</h2><p><strong>Authors:Yuqi Liu, Qin Jin, Tianyuan Qu, Xuan Liu, Yang Du, Bei Yu, Jiaya Jia</strong></p>
<p>Understanding accurate atomic temporal event is essential for video comprehension. However, current video-language benchmarks often fall short to evaluate Large Multi-modal Modelsâ€™ (LMMs) temporal event understanding capabilities, as they can be effectively addressed using image-language models. In this paper, we introduce RTime-QA, a novel benchmark specifically designed to assess the atomic temporal event understanding ability of LMMs. RTime-QA comprises 822 high-quality, carefully-curated video-text questions, each meticulously annotated by human experts. Each question features a video depicting an atomic temporal event, paired with both correct answers and temporal negative descriptions, specifically designed to evaluate temporal understanding. To advance LMMsâ€™ temporal event understanding ability, we further introduce RTime-IT, a 14k instruction-tuning dataset that employs a similar annotation process as RTime-QA. Extensive experimental analysis demonstrates that RTime-QA presents a significant challenge for LMMs: the state-of-the-art model Qwen2-VL achieves only 34.6 on strict-ACC metric, substantially lagging behind human performance. Furthermore, our experiments reveal that RTime-IT effectively enhance LMMsâ€™ capacity in temporal understanding. By fine-tuning on RTime-IT, our Qwen2-VL achieves 65.9 on RTime-QA. </p>
<blockquote>
<p>ç†è§£å‡†ç¡®çš„åŸå­æ—¶åºäº‹ä»¶å¯¹è§†é¢‘ç†è§£è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è§†é¢‘è¯­è¨€åŸºå‡†æµ‹è¯•é€šå¸¸éš¾ä»¥è¯„ä¼°å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰çš„æ—¶åºäº‹ä»¶ç†è§£èƒ½åŠ›ï¼Œå› ä¸ºä½¿ç”¨å›¾åƒè¯­è¨€æ¨¡å‹å¯ä»¥æœ‰æ•ˆåœ°è§£å†³è¿™äº›é—®é¢˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†RTime-QAï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨è®¾è®¡ç”¨äºè¯„ä¼°LMMsçš„åŸå­æ—¶åºäº‹ä»¶ç†è§£èƒ½åŠ›çš„æ–°å‹åŸºå‡†æµ‹è¯•ã€‚RTime-QAåŒ…å«822ä¸ªé«˜è´¨é‡ã€ç²¾å¿ƒç­–åˆ’çš„è§†é¢‘æ–‡æœ¬é—®é¢˜ï¼Œæ¯ä¸ªé—®é¢˜éƒ½ç”±äººç±»ä¸“å®¶ç²¾å¿ƒæ ‡æ³¨ã€‚æ¯ä¸ªé—®é¢˜éƒ½æœ‰ä¸€ä¸ªæè¿°åŸå­æ—¶åºäº‹ä»¶çš„è§†é¢‘ï¼Œé…æœ‰æ­£ç¡®ç­”æ¡ˆå’Œä¸“é—¨è®¾è®¡ç”¨äºè¯„ä¼°æ—¶é—´ç†è§£çš„ä¸´æ—¶è´Ÿé¢æè¿°ã€‚ä¸ºäº†æå‡LMMsçš„æ—¶åºäº‹ä»¶ç†è§£èƒ½åŠ›ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†RTime-ITï¼Œè¿™æ˜¯ä¸€ä¸ªæ‹¥æœ‰1.4ä¸‡æ¡æŒ‡ä»¤çš„å¾®è°ƒæ•°æ®é›†ï¼Œé‡‡ç”¨ä¸RTime-QAç±»ä¼¼çš„æ³¨é‡Šæµç¨‹ã€‚å¹¿æ³›çš„å®éªŒåˆ†æè¡¨æ˜ï¼ŒRTime-QAå¯¹LMMsæ„æˆäº†ä¸€é¡¹é‡å¤§æŒ‘æˆ˜ï¼šæœ€å…ˆè¿›çš„æ¨¡å‹Qwen2-VLåœ¨ä¸¥æ ¼å‡†ç¡®æ€§æŒ‡æ ‡ä¸Šä»…è¾¾åˆ°34.6ï¼Œè¿œè¿œè½åäºäººç±»è¡¨ç°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒRTime-ITç¡®å®æé«˜äº†LMMsåœ¨æ—¶åºç†è§£æ–¹é¢çš„èƒ½åŠ›ã€‚é€šè¿‡åœ¨RTime-ITä¸Šè¿›è¡Œå¾®è°ƒï¼Œæˆ‘ä»¬çš„Qwen2-VLåœ¨RTime-QAä¸Šè¾¾åˆ°äº†65.9ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19125v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>ä¸ºè¯„ä¼°å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰å¯¹åŸå­æ—¶æ€äº‹ä»¶çš„ç†è§£èƒ½åŠ›ï¼Œæœ¬æ–‡å¼•å…¥äº†RTime-QAåŸºå‡†æµ‹è¯•ã€‚è¯¥åŸºå‡†æµ‹è¯•åŒ…å«822ä¸ªé«˜è´¨é‡ã€ç²¾å¿ƒç­–åˆ’çš„è§†é¢‘æ–‡æœ¬é—®é¢˜ï¼Œæ¯ä¸ªé—®é¢˜å‡ç”±äººç±»ä¸“å®¶ç²¾å¿ƒæ ‡æ³¨ã€‚æ­¤å¤–ï¼Œä¸ºæå‡LMMsçš„æ—¶æ€äº‹ä»¶ç†è§£èƒ½åŠ›ï¼Œè¿˜æ¨å‡ºäº†RTime-ITæŒ‡ä»¤è°ƒæ•´æ•°æ®é›†ã€‚å®éªŒè¡¨æ˜ï¼ŒRTime-QAå¯¹LMMsæ„æˆé‡å¤§æŒ‘æˆ˜ï¼Œè€Œåœ¨RTime-ITä¸Šè¿›è¡Œå¾®è°ƒåï¼Œæ¨¡å‹æ€§èƒ½æœ‰æ‰€æå‡ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>RTime-QAåŸºå‡†æµ‹è¯•ä¸“ä¸ºè¯„ä¼°LMMså¯¹åŸå­æ—¶æ€äº‹ä»¶çš„ç†è§£èƒ½åŠ›è€Œè®¾è®¡ã€‚</li>
<li>RTime-QAåŒ…å«822ä¸ªé«˜è´¨é‡ã€ç²¾å¿ƒç­–åˆ’çš„è§†é¢‘æ–‡æœ¬é—®é¢˜ï¼Œæ¯ä¸ªé—®é¢˜éƒ½æœ‰ä¸“å®¶æ ‡æ³¨ã€‚</li>
<li>RTime-QAä¸­çš„é—®é¢˜æ—¨åœ¨è¯„ä»·æ¨¡å‹å¯¹æ—¶æ€äº‹ä»¶çš„ç†è§£ï¼ŒåŒ…æ‹¬æ­£ç¡®ç­”æ¡ˆæ˜¯å’Œç‰¹æ„è®¾è®¡çš„æ—¶æ€è´Ÿæè¿°ã€‚</li>
<li>æ¨å‡ºRTime-ITæ•°æ®é›†ï¼Œç”¨äºæå‡LMMsçš„æ—¶æ€äº‹ä»¶ç†è§£èƒ½åŠ›ï¼Œé‡‡ç”¨ä¸RTime-QAç›¸ä¼¼çš„æ ‡æ³¨è¿‡ç¨‹ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºï¼ŒRTime-QAå¯¹LMMsæ„æˆæŒ‘æˆ˜ï¼Œç°æœ‰æœ€ä½³æ¨¡å‹åœ¨ä¸¥æ ¼å‡†ç¡®ç‡æŒ‡æ ‡ä¸Šçš„è¡¨ç°ä»…ä¸º34.6%ã€‚</li>
<li>RTime-ITèƒ½æœ‰æ•ˆæå‡LMMsçš„æ—¶ç©ºç†è§£èƒ½åŠ›ï¼Œç»è¿‡RTime-ITå¾®è°ƒåï¼Œæ¨¡å‹æ€§èƒ½æå‡è‡³65.9%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19125">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ac57766875e29a9b7a2a2c74eb6be947.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a07c20551382923a9e35b7b51927aca2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-149675b3ff8f99cf5d95c697165b3902.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-70bdd1587629c08a57bb54764c5c4852.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-991d740e0cd1914be78477e6ec564f5c.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-28/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-28/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-28/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-f7c9531ab7a4ed8d9c82292f41ece53a.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-28  Iterative Self-Incentivization Empowers Large Language Models as Agentic   Searchers
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-28
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-28/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-9bcd953fb8217d52ce1dc2e4bc877dc8.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-28  Hard Negative Contrastive Learning for Fine-Grained Geometric   Understanding in Large Multimodal Models
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-28
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">25370.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
