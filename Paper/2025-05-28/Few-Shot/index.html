<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-28  Improvement Strategies for Few-Shot Learning in OCT Image Classification   of Rare Retinal Diseases">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-c4ae5d97b9465735d11ed9bebeaaed5f.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-28
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-24
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    18.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    75 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-28-æ›´æ–°"><a href="#2025-05-28-æ›´æ–°" class="headerlink" title="2025-05-28 æ›´æ–°"></a>2025-05-28 æ›´æ–°</h1><h2 id="Improvement-Strategies-for-Few-Shot-Learning-in-OCT-Image-Classification-of-Rare-Retinal-Diseases"><a href="#Improvement-Strategies-for-Few-Shot-Learning-in-OCT-Image-Classification-of-Rare-Retinal-Diseases" class="headerlink" title="Improvement Strategies for Few-Shot Learning in OCT Image Classification   of Rare Retinal Diseases"></a>Improvement Strategies for Few-Shot Learning in OCT Image Classification   of Rare Retinal Diseases</h2><p><strong>Authors:Cheng-Yu Tai, Ching-Wen Chen, Chi-Chin Wu, Bo-Chen Chiu,  Cheng-Hung,  Lin, Cheng-Kai Lu, Jia-Kang Wang, Tzu-Lun Huang</strong></p>
<p>This paper focuses on using few-shot learning to improve the accuracy of classifying OCT diagnosis images with major and rare classes. We used the GAN-based augmentation strategy as a baseline and introduced several novel methods to further enhance our model. The proposed strategy contains U-GAT-IT for improving the generative part and uses the data balance technique to narrow down the skew of accuracy between all categories. The best model obtained was built with CBAM attention mechanism and fine-tuned InceptionV3, and achieved an overall accuracy of 97.85%, representing a significant improvement over the original baseline. </p>
<blockquote>
<p>æœ¬æ–‡é‡ç‚¹å…³æ³¨ä½¿ç”¨å°‘é‡æ ·æœ¬å­¦ä¹ æ¥æé«˜å¯¹OCTè¯Šæ–­å›¾åƒä¸­ä¸»è¦å’Œç½•è§ç±»åˆ«åˆ†ç±»çš„å‡†ç¡®æ€§ã€‚æˆ‘ä»¬é‡‡ç”¨äº†åŸºäºGANçš„å¢å¼ºç­–ç•¥ä½œä¸ºåŸºçº¿ï¼Œå¹¶å¼•å…¥äº†å‡ ç§æ–°æ–¹æ³•è¿›ä¸€æ­¥æ”¹è¿›æˆ‘ä»¬çš„æ¨¡å‹ã€‚æå‡ºçš„ç­–ç•¥åŒ…å«ç”¨äºæ”¹è¿›ç”Ÿæˆéƒ¨åˆ†çš„U-GAT-ITï¼Œå¹¶ä½¿ç”¨æ•°æ®å¹³è¡¡æŠ€æœ¯æ¥ç¼©å°æ‰€æœ‰ç±»åˆ«ä¹‹é—´çš„å‡†ç¡®ç‡åå·®ã€‚è·å¾—çš„æœ€ä½³æ¨¡å‹æ˜¯é‡‡ç”¨CBAMæ³¨æ„åŠ›æœºåˆ¶å’Œå¾®è°ƒè¿‡çš„InceptionV3æ„å»ºçš„ï¼Œæ€»ä½“å‡†ç¡®ç‡è¾¾åˆ°97.85%ï¼Œç›¸è¾ƒäºåŸå§‹åŸºçº¿æœ‰äº†æ˜¾è‘—çš„æå‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20149v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿™ç¯‡è®ºæ–‡åˆ©ç”¨å°‘æ ·æœ¬å­¦ä¹ æŠ€æœ¯æé«˜äº†OCTè¯Šæ–­å›¾åƒåˆ†ç±»çš„å‡†ç¡®æ€§ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹ä¸»è¦å’Œç½•è§ç±»åˆ«çš„åˆ†ç±»ã€‚é‡‡ç”¨åŸºäºGANçš„å¢å¼ºç­–ç•¥ä½œä¸ºåŸºçº¿ï¼Œå¹¶å¼•å…¥äº†å‡ ç§æ–°æ–¹æ³•è¿›ä¸€æ­¥æå‡æ¨¡å‹æ€§èƒ½ã€‚é€šè¿‡U-GAT-ITæ”¹è¿›ç”Ÿæˆéƒ¨åˆ†ï¼Œå¹¶è¿ç”¨æ•°æ®å¹³è¡¡æŠ€æœ¯ç¼©å°å„ç±»åˆ«å‡†ç¡®åº¦çš„åå·®ã€‚æœ€ä½³æ¨¡å‹é‡‡ç”¨CBAMæ³¨æ„åŠ›æœºåˆ¶å’Œå¾®è°ƒè¿‡çš„InceptionV3ï¼Œæ€»ä½“å‡†ç¡®åº¦è¾¾åˆ°97.85%ï¼Œè¾ƒåŸºçº¿æœ‰æ˜¾è‘—æ”¹å–„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®ºæ–‡å…³æ³¨äºåˆ©ç”¨å°‘æ ·æœ¬å­¦ä¹ æé«˜OCTè¯Šæ–­å›¾åƒåˆ†ç±»çš„å‡†ç¡®æ€§ã€‚</li>
<li>é‡‡ç”¨åŸºäºGANçš„å¢å¼ºç­–ç•¥ä½œä¸ºåŸºçº¿æ–¹æ³•ã€‚</li>
<li>å¼•å…¥U-GAT-ITæ”¹è¿›ç”Ÿæˆæ¨¡å‹éƒ¨åˆ†ã€‚</li>
<li>è¿ç”¨æ•°æ®å¹³è¡¡æŠ€æœ¯ç¼©å°å„ç±»åˆ«ä¹‹é—´çš„å‡†ç¡®åº¦åå·®ã€‚</li>
<li>æœ€ä½³æ¨¡å‹ç»“åˆäº†CBAMæ³¨æ„åŠ›æœºåˆ¶å’Œå¾®è°ƒè¿‡çš„InceptionV3ã€‚</li>
<li>æœ€ä½³æ¨¡å‹çš„æ€»ä½“å‡†ç¡®åº¦è¾¾åˆ°97.85%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20149">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-23caef0563313eceb1f76549158563d1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4f6d1fd026c4693008f443d6b3f5bd61.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ac07083d186e2c5e8cac7bcaaa4ec82e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7e345ff77521dee94e29344c25fc3852.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="SaSi-A-Self-augmented-and-Self-interpreted-Deep-Learning-Approach-for-Few-shot-Cryo-ET-Particle-Detection"><a href="#SaSi-A-Self-augmented-and-Self-interpreted-Deep-Learning-Approach-for-Few-shot-Cryo-ET-Particle-Detection" class="headerlink" title="SaSi: A Self-augmented and Self-interpreted Deep Learning Approach for   Few-shot Cryo-ET Particle Detection"></a>SaSi: A Self-augmented and Self-interpreted Deep Learning Approach for   Few-shot Cryo-ET Particle Detection</h2><p><strong>Authors:Gokul Adethya, Bhanu Pratyush Mantha, Tianyang Wang, Xingjian Li, Min Xu</strong></p>
<p>Cryo-electron tomography (cryo-ET) has emerged as a powerful technique for imaging macromolecular complexes in their near-native states. However, the localization of 3D particles in cellular environments still presents a significant challenge due to low signal-to-noise ratios and missing wedge artifacts. Deep learning approaches have shown great potential, but they need huge amounts of data, which can be a challenge in cryo-ET scenarios where labeled data is often scarce. In this paper, we propose a novel Self-augmented and Self-interpreted (SaSi) deep learning approach towards few-shot particle detection in 3D cryo-ET images. Our method builds upon self-augmentation techniques to further boost data utilization and introduces a self-interpreted segmentation strategy for alleviating dependency on labeled data, hence improving generalization and robustness. As demonstrated by experiments conducted on both simulated and real-world cryo-ET datasets, the SaSi approach significantly outperforms existing state-of-the-art methods for particle localization. This research increases understanding of how to detect particles with very few labels in cryo-ET and thus sets a new benchmark for few-shot learning in structural biology. </p>
<blockquote>
<p>å†·å†»ç”µå­æ–­å±‚æ‰«æï¼ˆcryo-ETï¼‰å·²æˆä¸ºå¯è§†åŒ–è¿‘å¤©ç„¶æ€å¤§åˆ†å­å¤åˆç‰©çš„ä¸€ç§å¼ºå¤§æŠ€æœ¯ã€‚ç„¶è€Œï¼Œç”±äºä¿¡å™ªæ¯”ä½å’Œç¼ºå¤±çš„æ¥”å½¢ä¼ªå½±ï¼Œåœ¨ç»†èƒç¯å¢ƒä¸­å¯¹ä¸‰ç»´ç²’å­çš„å®šä½ä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚æ·±åº¦å­¦ä¹ çš„æ–¹æ³•æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ï¼Œä½†å®ƒä»¬éœ€è¦å¤§é‡çš„æ•°æ®ï¼Œè¿™åœ¨å†·å†»ç”µé•œæŠ€æœ¯åœºæ™¯ä¸­å¯èƒ½æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œå› ä¸ºæ ‡è®°çš„æ•°æ®é€šå¸¸å¾ˆç¨€ç¼ºã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„è‡ªæˆ‘å¢å¼ºå’Œè‡ªæˆ‘è§£é‡Šï¼ˆSaSiï¼‰æ·±åº¦å­¦ä¹ çš„æ–¹æ³•ï¼Œç”¨äºåœ¨å°‘é‡3Då†·å†»ç”µå­æ–­å±‚æ‰«æå›¾åƒä¸­è¿›è¡Œç²’å­æ£€æµ‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•å»ºç«‹åœ¨è‡ªæˆ‘å¢å¼ºæŠ€æœ¯ä¹‹ä¸Šï¼Œè¿›ä¸€æ­¥æé«˜äº†æ•°æ®åˆ©ç”¨ç‡ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§è‡ªæˆ‘è§£é‡Šåˆ†å‰²ç­–ç•¥ï¼Œä»¥å‡è½»å¯¹æ ‡è®°æ•°æ®çš„ä¾èµ–ï¼Œä»è€Œæé«˜æ³›åŒ–å’Œé²æ£’æ€§ã€‚é€šè¿‡æ¨¡æ‹Ÿå’ŒçœŸå®å†·å†»ç”µé•œæ•°æ®é›†çš„å®éªŒè¯æ˜ï¼ŒSaSiæ–¹æ³•åœ¨ç²’å­å®šä½æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚è¯¥ç ”ç©¶æé«˜äº†åœ¨å†·å†»ç”µé•œä¸­å¦‚ä½•æ£€æµ‹å°‘é‡æ ‡ç­¾ç²’å­çš„ç†è§£ï¼Œä»è€Œä¸ºç»“æ„ç”Ÿç‰©å­¦ä¸­çš„å°æ ·æœ¬å­¦ä¹ è®¾å®šäº†ä¸€ä¸ªæ–°çš„åŸºå‡†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19948v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹çš„åŸºäºè‡ªå¢å¼ºå’Œè‡ªè§£é‡Šï¼ˆSaSiï¼‰çš„æ·±åº¦å­¦ä¹ æ–¹æ³•ï¼Œç”¨äºåœ¨å°‘é‡æ ‡æ³¨æ•°æ®çš„æƒ…å†µä¸‹å®ç°ä¸‰ç»´å†·å†»ç”µå­æ–­å±‚æ‰«æï¼ˆcryo-ETï¼‰å›¾åƒä¸­çš„ç²’å­æ£€æµ‹ã€‚è¯¥æ–¹æ³•é€šè¿‡è‡ªå¢å¼ºæŠ€æœ¯è¿›ä¸€æ­¥æé«˜æ•°æ®åˆ©ç”¨ç‡ï¼Œå¹¶å¼•å…¥è‡ªè§£é‡Šåˆ†å‰²ç­–ç•¥ï¼Œå‡å°‘å¯¹æ ‡æ³¨æ•°æ®çš„ä¾èµ–ï¼Œä»è€Œæé«˜æ¨¡å‹çš„é€šç”¨æ€§å’Œé²æ£’æ€§ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ¨¡æ‹Ÿå’Œå®é™…cryo-ETæ•°æ®é›†ä¸Šçš„è¡¨ç°å‡ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚è¯¥ç ”ç©¶æé«˜äº†åœ¨å†·å†»ç”µé•œä¸­æ£€æµ‹ç²’å­çš„èƒ½åŠ›ï¼Œä¸ºç»“æ„ç”Ÿç‰©å­¦ä¸­çš„å°æ ·æœ¬å­¦ä¹ æ ‘ç«‹äº†æ–°çš„æ ‡æ†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å†·å†»ç”µå­æ–­å±‚æ‰«æï¼ˆcryo-ETï¼‰æŠ€æœ¯ç”¨äºæˆåƒå¤§åˆ†å­å¤åˆç‰©ã€‚</li>
<li>åœ¨ç»†èƒç¯å¢ƒä¸­å®šä½ä¸‰ç»´ç²’å­å­˜åœ¨æŒ‘æˆ˜ï¼Œä¸»è¦ç”±äºä½ä¿¡å™ªæ¯”å’Œç¼ºå¤±æ¥”å½¢ä¼ªå½±ã€‚</li>
<li>æ·±åº¦å­¦ä¹ åœ¨è§£å†³æ­¤é—®é¢˜ä¸Šæ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†éœ€è¦å¤§é‡æ•°æ®ï¼Œè€Œå†·å†»ç”µé•œåœºæ™¯ä¸­çš„æ ‡æ³¨æ•°æ®å¾€å¾€ç¨€ç¼ºã€‚</li>
<li>æå‡ºäº†æ–°å‹çš„åŸºäºè‡ªå¢å¼ºå’Œè‡ªè§£é‡Šï¼ˆSaSiï¼‰çš„æ·±åº¦å­¦ä¹ æ–¹æ³•ç”¨äºå°‘æ ·æœ¬ç²’å­æ£€æµ‹ã€‚</li>
<li>SaSiæ–¹æ³•é€šè¿‡è‡ªå¢å¼ºæŠ€æœ¯æé«˜æ•°æ®åˆ©ç”¨ç‡ï¼Œå¹¶å¼•å…¥è‡ªè§£é‡Šåˆ†å‰²ç­–ç•¥å‡å°‘å¯¹æ ‡æ³¨æ•°æ®çš„ä¾èµ–ã€‚</li>
<li>å®éªŒè¯æ˜SaSiæ–¹æ³•åœ¨æ¨¡æ‹Ÿå’Œå®é™…æ•°æ®é›†ä¸Šå‡è¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19948">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-713608727aa0f30e7651d7f3195626b2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-244603c8058707a44c3fa10a6b538301.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cafacfb9797997f9d1abc7497cc058bb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-87c321ade6f90b17bafc0bdc8643639f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c63c6e73cd9f3d4da4c26d447f0241c5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d5b8a7833c814d5865fc0084593242bf.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Large-Language-Models-as-Autonomous-Spacecraft-Operators-in-Kerbal-Space-Program"><a href="#Large-Language-Models-as-Autonomous-Spacecraft-Operators-in-Kerbal-Space-Program" class="headerlink" title="Large Language Models as Autonomous Spacecraft Operators in Kerbal Space   Program"></a>Large Language Models as Autonomous Spacecraft Operators in Kerbal Space   Program</h2><p><strong>Authors:Alejandro Carrasco, Victor Rodriguez-Fernandez, Richard Linares</strong></p>
<p>Recent trends are emerging in the use of Large Language Models (LLMs) as autonomous agents that take actions based on the content of the user text prompts. We intend to apply these concepts to the field of Control in space, enabling LLMs to play a significant role in the decision-making process for autonomous satellite operations. As a first step towards this goal, we have developed a pure LLM-based solution for the Kerbal Space Program Differential Games (KSPDG) challenge, a public software design competition where participants create autonomous agents for maneuvering satellites involved in non-cooperative space operations, running on the KSP game engine. Our approach leverages prompt engineering, few-shot prompting, and fine-tuning techniques to create an effective LLM-based agent that ranked 2nd in the competition. To the best of our knowledge, this work pioneers the integration of LLM agents into space research. The project comprises several open repositories to facilitate replication and further research. The codebase is accessible on \href{<a target="_blank" rel="noopener" href="https://github.com/ARCLab-MIT/kspdg%7D%7BGitHub%7D">https://github.com/ARCLab-MIT/kspdg}{GitHub}</a>, while the trained models and datasets are available on \href{<a target="_blank" rel="noopener" href="https://huggingface.co/OhhTuRnz%7D%7BHugging">https://huggingface.co/OhhTuRnz}{Hugging</a> Face}. Additionally, experiment tracking and detailed results can be reviewed on \href{<a target="_blank" rel="noopener" href="https://wandb.ai/carrusk/huggingface%7D%7BWeights">https://wandb.ai/carrusk/huggingface}{Weights</a> &amp; Biases </p>
<blockquote>
<p>æœ€è¿‘å‡ºç°äº†ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºè‡ªä¸»ä»£ç†çš„è¶‹åŠ¿ï¼Œè¿™äº›ä»£ç†ä¼šæ ¹æ®ç”¨æˆ·æ–‡æœ¬æç¤ºçš„å†…å®¹é‡‡å–è¡ŒåŠ¨ã€‚æˆ‘ä»¬æ‰“ç®—å°†è¿™äº›æ¦‚å¿µåº”ç”¨äºç©ºé—´æ§åˆ¶é¢†åŸŸï¼Œä½¿LLMåœ¨è‡ªä¸»å«æ˜Ÿæ“ä½œçš„å†³ç­–è¿‡ç¨‹ä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚ä½œä¸ºå®ç°è¿™ä¸€ç›®æ ‡çš„ç¬¬ä¸€æ­¥ï¼Œæˆ‘ä»¬ä¸ºKerbal Space Program Differential Gamesï¼ˆKSPDGï¼‰æŒ‘æˆ˜å¼€å‘äº†ä¸€ç§çº¯LLMè§£å†³æ–¹æ¡ˆã€‚KSPDGæ˜¯ä¸€åœºå…¬å¼€çš„è½¯ä»¶è®¾è®¡ç«èµ›ï¼Œå‚èµ›è€…éœ€ä¸ºéåˆä½œç©ºé—´æ“ä½œåˆ›å»ºè‡ªä¸»ä»£ç†å«æ˜Ÿï¼Œè¯¥è§£å†³æ–¹æ¡ˆåŸºäºLLMåœ¨KSPæ¸¸æˆå¼•æ“ä¸Šè¿è¡Œã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨æç¤ºå·¥ç¨‹ã€å°‘æç¤ºå’Œå¾®è°ƒæŠ€æœ¯åˆ›å»ºäº†ä¸€ä¸ªæœ‰æ•ˆçš„åŸºäºLLMçš„ä»£ç†ï¼Œåœ¨æ¯”èµ›ä¸­æ’åç¬¬äºŒã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™é¡¹å·¥ä½œé¦–åˆ›äº†å°†LLMä»£ç†é›†æˆåˆ°ç©ºé—´ç ”ç©¶ä¸­ã€‚è¯¥é¡¹ç›®åŒ…å«å¤šä¸ªå¼€æºä»“åº“ï¼Œä»¥ä¿ƒè¿›å¤åˆ¶å’Œè¿›ä¸€æ­¥ç ”ç©¶ã€‚ä»£ç åº“å¯åœ¨GitHubä¸Šè®¿é—®ï¼ˆ[<a target="_blank" rel="noopener" href="https://github.com/ARCLab-MIT/kspdg%EF%BC%89%EF%BC%8C%E8%80%8C%E8%AE%AD%E7%BB%83%E5%A5%BD%E7%9A%84%E6%A8%A1%E5%9E%8B%E5%92%8C%E6%95%B0%E6%8D%AE%E9%9B%86%E5%8F%AF%E5%9C%A8Hugging">https://github.com/ARCLab-MIT/kspdgï¼‰ï¼Œè€Œè®­ç»ƒå¥½çš„æ¨¡å‹å’Œæ•°æ®é›†å¯åœ¨Hugging</a> Faceä¸Šæ‰¾åˆ°ï¼ˆ<a target="_blank" rel="noopener" href="https://huggingface.co/OhhTuRnz%EF%BC%89%E3%80%82%E6%AD%A4%E5%A4%96%EF%BC%8C%E5%AE%9E%E9%AA%8C%E8%B7%9F%E8%B8%AA%E5%92%8C%E8%AF%A6%E7%BB%86%E7%BB%93%E6%9E%9C%E5%8F%AF%E5%9C%A8Weights">https://huggingface.co/OhhTuRnzï¼‰ã€‚æ­¤å¤–ï¼Œå®éªŒè·Ÿè¸ªå’Œè¯¦ç»†ç»“æœå¯åœ¨Weights</a> &amp; Biasesä¸ŠæŸ¥çœ‹ï¼ˆ<a target="_blank" rel="noopener" href="https://wandb.ai/carrusk/huggingface%EF%BC%89%E3%80%82">https://wandb.ai/carrusk/huggingfaceï¼‰ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19896v1">PDF</a> Non revised version for paper going to be published in Journal of   Advances in Space Research</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¢«åº”ç”¨äºç©ºé—´æ§åˆ¶é¢†åŸŸï¼Œä½œä¸ºè‡ªä¸»ä»£ç†è¿›è¡ŒåŸºäºç”¨æˆ·æ–‡æœ¬æç¤ºçš„è¡ŒåŠ¨å†³ç­–ã€‚ç ”ç©¶å›¢é˜Ÿåœ¨Kerbal Space Program Differential GamesæŒ‘æˆ˜ä¸­ï¼Œå¼€å‘äº†ä¸€ç§çº¯LLMè§£å†³æ–¹æ¡ˆï¼Œåˆ©ç”¨æç¤ºå·¥ç¨‹ã€å°‘æ ·æœ¬æç¤ºå’Œå¾®è°ƒæŠ€æœ¯åˆ›å»ºäº†ä¸€ä¸ªæœ‰æ•ˆçš„LLMä»£ç†ï¼Œå¹¶åœ¨ç«èµ›ä¸­å–å¾—äº†ç¬¬äºŒåã€‚è¯¥é¡¹ç›®ä¸ºLLMä»£ç†åœ¨ç©ºé—´ç ”ç©¶ä¸­çš„é›†æˆå¼€è¾Ÿäº†å…ˆæ²³ï¼Œå…¶ä»£ç åº“ã€è®­ç»ƒæ¨¡å‹å’Œæ•°æ®é›†å‡å·²å…¬å¼€ä¾›ç ”ç©¶ä½¿ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¢«åº”ç”¨äºè‡ªä¸»å«æ˜Ÿæ“ä½œçš„å†³ç­–è¿‡ç¨‹ä¸­ã€‚</li>
<li>LLMsè¢«ç”¨ä½œè‡ªä¸»ä»£ç†ï¼Œèƒ½å¤Ÿæ ¹æ®ç”¨æˆ·æ–‡æœ¬æç¤ºé‡‡å–è¡ŒåŠ¨ã€‚</li>
<li>åœ¨Kerbal Space Program Differential GamesæŒ‘æˆ˜ä¸­ï¼Œå¼€å‘äº†ä¸€ç§åŸºäºLLMçš„çº¯è§£å†³æ–¹æ¡ˆã€‚</li>
<li>è¯¥è§£å†³æ–¹æ¡ˆåˆ©ç”¨æç¤ºå·¥ç¨‹ã€å°‘æ ·æœ¬æç¤ºå’Œå¾®è°ƒæŠ€æœ¯ï¼Œå–å¾—äº†ç«èµ›ç¬¬äºŒåçš„å¥½æˆç»©ã€‚</li>
<li>æ­¤é¡¹ç›®ä¸ºLLMä»£ç†åœ¨ç©ºé—´ç ”ç©¶ä¸­çš„é›†æˆæä¾›äº†å¼€åˆ›æ€§çš„å®è·µã€‚</li>
<li>é¡¹ç›®å…¬å¼€äº†ä»£ç åº“ã€è®­ç»ƒæ¨¡å‹å’Œæ•°æ®é›†ä»¥ä¾›ç ”ç©¶ä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19896">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0d436e1b6a6d3cf5aa61329f275f59be.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9f7671213e5400d37c87831dd8e04b81.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="GoLF-NRT-Integrating-Global-Context-and-Local-Geometry-for-Few-Shot-View-Synthesis"><a href="#GoLF-NRT-Integrating-Global-Context-and-Local-Geometry-for-Few-Shot-View-Synthesis" class="headerlink" title="GoLF-NRT: Integrating Global Context and Local Geometry for Few-Shot   View Synthesis"></a>GoLF-NRT: Integrating Global Context and Local Geometry for Few-Shot   View Synthesis</h2><p><strong>Authors:You Wang, Li Fang, Hao Zhu, Fei Hu, Long Ye, Zhan Ma</strong></p>
<p>Neural Radiance Fields (NeRF) have transformed novel view synthesis by modeling scene-specific volumetric representations directly from images. While generalizable NeRF models can generate novel views across unknown scenes by learning latent ray representations, their performance heavily depends on a large number of multi-view observations. However, with limited input views, these methods experience significant degradation in rendering quality. To address this limitation, we propose GoLF-NRT: a Global and Local feature Fusion-based Neural Rendering Transformer. GoLF-NRT enhances generalizable neural rendering from few input views by leveraging a 3D transformer with efficient sparse attention to capture global scene context. In parallel, it integrates local geometric features extracted along the epipolar line, enabling high-quality scene reconstruction from as few as 1 to 3 input views. Furthermore, we introduce an adaptive sampling strategy based on attention weights and kernel regression, improving the accuracy of transformer-based neural rendering. Extensive experiments on public datasets show that GoLF-NRT achieves state-of-the-art performance across varying numbers of input views, highlighting the effectiveness and superiority of our approach. Code is available at <a target="_blank" rel="noopener" href="https://github.com/KLMAV-CUC/GoLF-NRT">https://github.com/KLMAV-CUC/GoLF-NRT</a>. </p>
<blockquote>
<p>ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰é€šè¿‡ç›´æ¥ä»å›¾åƒå»ºæ¨¡åœºæ™¯ç‰¹å®šçš„ä½“ç§¯è¡¨ç¤ºï¼Œä»è€Œå®ç°äº†æ–°é¢–è§†å›¾åˆæˆçš„å˜é©ã€‚è™½ç„¶é€šç”¨NeRFæ¨¡å‹å¯ä»¥é€šè¿‡å­¦ä¹ æ½œåœ¨å°„çº¿è¡¨ç¤ºæ¥ç”ŸæˆæœªçŸ¥åœºæ™¯çš„æ–°é¢–è§†å›¾ï¼Œä½†å…¶æ€§èƒ½åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä¾èµ–äºå¤§é‡å¤šè§†å›¾è§‚å¯Ÿã€‚ç„¶è€Œï¼Œåœ¨è¾“å…¥è§†å›¾æœ‰é™çš„æƒ…å†µä¸‹ï¼Œè¿™äº›æ–¹æ³•åœ¨æ¸²æŸ“è´¨é‡æ–¹é¢ä¼šç»å†æ˜¾è‘—ä¸‹é™ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºå…¨å±€å’Œå±€éƒ¨ç‰¹å¾èåˆçš„ç¥ç»æ¸²æŸ“è½¬æ¢å™¨GoLF-NRTã€‚GoLF-NRTé€šè¿‡åˆ©ç”¨å…·æœ‰é«˜æ•ˆç¨€ç–æ³¨æ„åŠ›çš„3Dè½¬æ¢å™¨æ¥å¢å¼ºä»å°‘æ•°è¾“å…¥è§†å›¾è¿›è¡Œé€šç”¨ç¥ç»æ¸²æŸ“ï¼Œä»¥æ•æ‰åœºæ™¯çš„å…¨å±€ä¸Šä¸‹æ–‡ã€‚åŒæ—¶ï¼Œå®ƒé›†æˆäº†æ²¿æçº¿æå–çš„å±€éƒ¨å‡ ä½•ç‰¹å¾ï¼Œä»è€Œèƒ½å¤Ÿä»ä»…1åˆ°3ä¸ªè¾“å…¥è§†å›¾å®ç°é«˜è´¨é‡çš„åœºæ™¯é‡å»ºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºæ³¨æ„åŠ›æƒé‡å’Œæ ¸å›å½’çš„è‡ªé€‚åº”é‡‡æ ·ç­–ç•¥ï¼Œæé«˜äº†åŸºäºè½¬æ¢å™¨çš„ç¥ç»æ¸²æŸ“çš„å‡†ç¡®æ€§ã€‚åœ¨å…¬å…±æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒGoLF-NRTåœ¨ä¸åŒæ•°é‡çš„è¾“å…¥è§†å›¾ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå‡¸æ˜¾äº†æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚ä»£ç å¯è®¿é—®<a target="_blank" rel="noopener" href="https://github.com/KLMAV-CUC/GoLF-NRT%E3%80%82">https://github.com/KLMAV-CUC/GoLF-NRTã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19813v1">PDF</a> CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>ç¥ç»ç½‘ç»œè¾å°„åœºï¼ˆNeRFï¼‰é€šè¿‡å›¾åƒå»ºæ¨¡åœºæ™¯ç‰¹å®šçš„ä½“ç§¯è¡¨ç¤ºï¼Œä»è€Œè½¬å˜äº†æ–°å‹è§†å›¾åˆæˆæŠ€æœ¯ã€‚é’ˆå¯¹åŸºäºæ½œåœ¨å°„çº¿è¡¨ç¤ºçš„ä¸€èˆ¬åŒ–NeRFæ¨¡å‹åœ¨æœ‰é™è¾“å…¥è§†å›¾ä¸‹ç”Ÿæˆæ–°è§†å›¾æ—¶æ€§èƒ½æ˜¾è‘—ä¸‹é™çš„é—®é¢˜ï¼Œæå‡ºäº†åŸºäºå…¨å±€å’Œå±€éƒ¨ç‰¹å¾èåˆçš„ç¥ç»æ¸²æŸ“è½¬æ¢å™¨GoLF-NRTã€‚GoLF-NRTé€šè¿‡é«˜æ•ˆçš„ç¨€ç–æ³¨æ„åŠ›3Dè½¬æ¢å™¨æ•æ‰åœºæ™¯ä¸Šä¸‹æ–‡ï¼ŒåŒæ—¶é›†æˆæ²¿æçº¿çš„å±€éƒ¨å‡ ä½•ç‰¹å¾ï¼Œå®ç°ä»…ä»ä¸€è‡³ä¸‰ä¸ªè¾“å…¥è§†å›¾çš„é«˜è´¨é‡åœºæ™¯é‡å»ºã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†ä¸€ç§åŸºäºæ³¨æ„åŠ›æƒé‡å’Œæ ¸å›å½’çš„è‡ªé€‚åº”é‡‡æ ·ç­–ç•¥ï¼Œæé«˜äº†åŸºäºè½¬æ¢å™¨çš„ç¥ç»æ¸²æŸ“çš„å‡†ç¡®æ€§ã€‚åœ¨å…¬å¼€æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒGoLF-NRTåœ¨ä¸åŒè¾“å…¥è§†å›¾æ•°é‡ä¸Šå‡è¾¾åˆ°æœ€ä½³æ€§èƒ½ï¼Œå‡¸æ˜¾äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>NeRFæŠ€æœ¯é€šè¿‡å›¾åƒå»ºæ¨¡åœºæ™¯ç‰¹å®šçš„ä½“ç§¯è¡¨ç¤ºï¼Œå®ç°äº†æ–°å‹è§†å›¾åˆæˆã€‚</li>
<li>ä¸€èˆ¬åŒ–çš„NeRFæ¨¡å‹åœ¨æœ‰é™è¾“å…¥è§†å›¾ä¸‹æ€§èƒ½ä¸‹é™ï¼Œéœ€è¦æ›´å¤šè§†å›¾æ¥æé«˜æ€§èƒ½ã€‚</li>
<li>GoLF-NRTé€šè¿‡å…¨å±€å’Œå±€éƒ¨ç‰¹å¾èåˆå¢å¼ºä»æœ‰é™è¾“å…¥è§†å›¾è¿›è¡Œç¥ç»æ¸²æŸ“çš„èƒ½åŠ›ã€‚</li>
<li>GoLF-NRTåˆ©ç”¨3Dè½¬æ¢å™¨æ•æ‰åœºæ™¯ä¸Šä¸‹æ–‡ï¼Œå¹¶ç»“åˆæ²¿æçº¿çš„å±€éƒ¨å‡ ä½•ç‰¹å¾ã€‚</li>
<li>GoLF-NRTå®ç°äº†ä»…ä»ä¸€è‡³ä¸‰ä¸ªè¾“å…¥è§†å›¾çš„é«˜è´¨é‡åœºæ™¯é‡å»ºã€‚</li>
<li>GoLF-NRTå¼•å…¥äº†è‡ªé€‚åº”é‡‡æ ·ç­–ç•¥ï¼ŒåŸºäºæ³¨æ„åŠ›æƒé‡å’Œæ ¸å›å½’ï¼Œæé«˜äº†ç¥ç»æ¸²æŸ“çš„å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19813">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b51bcce7a609a5dac67c9bc8f5280687.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2d99bb1f3f43376d88e9fba45acd117e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-64ff787be63c845bf786735e4f9869f6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-85b686bcdecc847ef876fc1ebb76ab6d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4fd79e178e2f4773a3c5d8e22fb296c1.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="MOLE-Metadata-Extraction-and-Validation-in-Scientific-Papers-Using-LLMs"><a href="#MOLE-Metadata-Extraction-and-Validation-in-Scientific-Papers-Using-LLMs" class="headerlink" title="MOLE: Metadata Extraction and Validation in Scientific Papers Using LLMs"></a>MOLE: Metadata Extraction and Validation in Scientific Papers Using LLMs</h2><p><strong>Authors:Zaid Alyafeai, Maged S. Al-Shaibani, Bernard Ghanem</strong></p>
<p>Metadata extraction is essential for cataloging and preserving datasets, enabling effective research discovery and reproducibility, especially given the current exponential growth in scientific research. While Masader (Alyafeai et al.,2021) laid the groundwork for extracting a wide range of metadata attributes from Arabic NLP datasetsâ€™ scholarly articles, it relies heavily on manual annotation. In this paper, we present MOLE, a framework that leverages Large Language Models (LLMs) to automatically extract metadata attributes from scientific papers covering datasets of languages other than Arabic. Our schema-driven methodology processes entire documents across multiple input formats and incorporates robust validation mechanisms for consistent output. Additionally, we introduce a new benchmark to evaluate the research progress on this task. Through systematic analysis of context length, few-shot learning, and web browsing integration, we demonstrate that modern LLMs show promising results in automating this task, highlighting the need for further future work improvements to ensure consistent and reliable performance. We release the code: <a target="_blank" rel="noopener" href="https://github.com/IVUL-KAUST/MOLE">https://github.com/IVUL-KAUST/MOLE</a> and dataset: <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/IVUL-KAUST/MOLE">https://huggingface.co/datasets/IVUL-KAUST/MOLE</a> for the research community. </p>
<blockquote>
<p>å…ƒæ•°æ®æå–å¯¹äºæ•°æ®é›†ç¼–ç›®å’Œä¿å­˜è‡³å…³é‡è¦ï¼Œå®ƒä¿ƒè¿›äº†æœ‰æ•ˆçš„ç ”ç©¶å‘ç°å’Œå¯é‡å¤æ€§ï¼Œå°¤å…¶æ˜¯åœ¨å½“å‰ç§‘å­¦ç ”ç©¶å‘ˆæŒ‡æ•°çº§å¢é•¿çš„æƒ…å†µä¸‹ã€‚è™½ç„¶Masaderï¼ˆAlyafeaiç­‰äººï¼Œ2021å¹´ï¼‰å¥ å®šäº†ä»é˜¿æ‹‰ä¼¯è¯­NLPæ•°æ®é›†å­¦æœ¯è®ºæ–‡ä¸­æå–å¹¿æ³›å…ƒæ•°æ®å±æ€§çš„åŸºç¡€ï¼Œä½†å®ƒä¸»è¦ä¾èµ–äºäººå·¥æ ‡æ³¨ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†MOLEæ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è‡ªåŠ¨ä»éé˜¿æ‹‰ä¼¯è¯­æ•°æ®é›†çš„ç§‘å­¦è®ºæ–‡ä¸­æå–å…ƒæ•°æ®å±æ€§ã€‚æˆ‘ä»¬çš„åŸºäºæ¨¡å¼çš„æ–¹æ³•å¤„ç†å¤šç§è¾“å…¥æ ¼å¼çš„æ•´ä¸ªæ–‡æ¡£ï¼Œå¹¶çº³å…¥ç¨³å¥çš„éªŒè¯æœºåˆ¶ä»¥å®ç°ä¸€è‡´è¾“å‡ºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•æ¥è¯„ä¼°æ­¤ä»»åŠ¡çš„ç ”ç©¶è¿›å±•ã€‚é€šè¿‡å¯¹ä¸Šä¸‹æ–‡é•¿åº¦ã€å°æ ·æœ¬å­¦ä¹ å’Œç½‘é¡µæµè§ˆæ•´åˆçš„ç³»ç»Ÿåˆ†æï¼Œæˆ‘ä»¬è¯æ˜äº†ç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è¯¥ä»»åŠ¡çš„è‡ªåŠ¨åŒ–æ–¹é¢æ˜¾ç¤ºå‡ºæœ‰å‰é€”çš„ç»“æœï¼Œå¹¶å¼ºè°ƒäº†æœªæ¥éœ€è¦è¿›ä¸€æ­¥æ”¹è¿›å·¥ä½œä»¥ç¡®ä¿æ€§èƒ½å’Œå¯é æ€§çš„ä¸€è‡´æ€§ã€‚æˆ‘ä»¬å‘ç ”ç©¶ç¤¾åŒºå‘å¸ƒäº†ä»£ç ï¼š<a target="_blank" rel="noopener" href="https://github.com/IVUL-KAUST/MOLE%E5%92%8C%E6%95%B0%E6%8D%AE%E9%9B%86%EF%BC%9Ahttps://huggingface.co/datasets/IVUL-KAUST/MOLE%E3%80%82">https://github.com/IVUL-KAUST/MOLEå’Œæ•°æ®é›†ï¼šhttps://huggingface.co/datasets/IVUL-KAUST/MOLEã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19800v1">PDF</a> </p>
<p><strong>Summary</strong><br>å…ƒæ•°æ®æå–å¯¹äºæ•°æ®é›†çš„ç¼–ç›®å’Œä¿å­˜ã€ç ”ç©¶å‘ç°çš„æ•ˆç‡å’Œå¯é‡å¤æ€§è‡³å…³é‡è¦ï¼Œå°¤å…¶è€ƒè™‘åˆ°ç§‘å­¦ç ”ç©¶çš„æŒ‡æ•°çº§å¢é•¿ã€‚é¢å¯¹æ­¤å‰é˜¿æ‹‰ä¼¯è¯­ç³»NLPæ•°æ®é›†çš„è‡ªåŠ¨åŒ–æå–å¤§å¤šä¾èµ–æ‰‹åŠ¨æ ‡æ³¨çš„é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºMOLEæ¡†æ¶ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è‡ªåŠ¨æå–éé˜¿æ‹‰ä¼¯è¯­ç³»çš„ç§‘å­¦è®ºæ–‡ä¸­çš„å…ƒæ•°æ®å±æ€§ã€‚è¯¥æ¡†æ¶é‡‡ç”¨åŸºäºæ¶æ„çš„æ–¹æ³•å¤„ç†å¤šç§è¾“å…¥æ ¼å¼çš„æ–‡æ¡£ï¼Œå¹¶å…·å¤‡ç¨³å¥çš„éªŒè¯æœºåˆ¶ä»¥ç¡®ä¿è¾“å‡ºçš„ä¸€è‡´æ€§ã€‚åŒæ—¶å¼•å…¥æ–°çš„åŸºå‡†æµ‹è¯•æ¥è¡¡é‡æ­¤é¡¹ç ”ç©¶çš„è¿›å±•ã€‚é€šè¿‡ä¸Šä¸‹æ–‡é•¿åº¦åˆ†æã€å°æ ·å­¦ä¹ æ³•å’Œç½‘ç»œæµè§ˆé›†æˆè¿›è¡Œç³»ç»Ÿåˆ†æï¼Œå±•ç¤ºç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è‡ªåŠ¨åŒ–æ­¤ä»»åŠ¡ä¸Šçš„æ½œåŠ›ï¼Œå¹¶æŒ‡æ˜æœªæ¥æ”¹è¿›æ–¹å‘ä»¥ç¡®ä¿æ€§èƒ½å’Œå¯é æ€§ã€‚ç›¸å…³ä»£ç å’Œæ•°æ®é›†å·²å‘å¸ƒä¾›ç ”ç©¶ç¤¾åŒºä½¿ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å…ƒæ•°æ®æå–å¯¹äºæ•°æ®é›†ç®¡ç†è‡³å…³é‡è¦ï¼Œæœ‰åŠ©äºç ”ç©¶å‘ç°å’Œå¯é‡å¤æ€§ã€‚</li>
<li>Masaderä¸»è¦ä¾§é‡äºé˜¿æ‹‰ä¼¯NLPæ•°æ®é›†çš„å…ƒæ•°æ®æå–ï¼Œä½†ä¾èµ–æ‰‹åŠ¨æ ‡æ³¨ã€‚</li>
<li>MOLEæ¡†æ¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è‡ªåŠ¨æå–éé˜¿æ‹‰ä¼¯è¯­ç³»çš„ç§‘å­¦è®ºæ–‡ä¸­çš„å…ƒæ•°æ®å±æ€§ã€‚</li>
<li>MOLEæ¡†æ¶å…·å¤‡å¤„ç†å¤šç§è¾“å…¥æ ¼å¼æ–‡æ¡£çš„èƒ½åŠ›ï¼Œå¹¶å…·å¤‡ç¨³å¥çš„éªŒè¯æœºåˆ¶ã€‚</li>
<li>å¼•å…¥æ–°çš„åŸºå‡†æµ‹è¯•æ¥è¡¡é‡å…ƒæ•°æ®æå–çš„ç ”ç©¶è¿›å±•ã€‚</li>
<li>ç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è‡ªåŠ¨åŒ–å…ƒæ•°æ®æå–ä»»åŠ¡ä¸Šå±•ç°å‡ºæ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19800">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-55aec0f8bee048797ab6b64cca3616b5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7a596582e74c447a758b4a5170c2d88a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-017ef48e040ef42c0b67d7ff3576b33b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8b8baa2dab16dfcdc98dda0a10ccaf8c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7bcbc2e352093d067e45f6bfa30eab2d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-addc4ba5eb14c1b45f17ac8a01ecc250.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-515fd37e00643795451eb440efdd0451.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d5cad11ca80a1b517220d908bc945f72.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dd971103b4bda32ca343b0cf461af38d.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Few-Shot-Class-Incremental-Learning-For-Efficient-SAR-Automatic-Target-Recognition"><a href="#Few-Shot-Class-Incremental-Learning-For-Efficient-SAR-Automatic-Target-Recognition" class="headerlink" title="Few-Shot Class-Incremental Learning For Efficient SAR Automatic Target   Recognition"></a>Few-Shot Class-Incremental Learning For Efficient SAR Automatic Target   Recognition</h2><p><strong>Authors:George Karantaidis, Athanasios Pantsios, Ioannis Kompatsiaris, Symeon Papadopoulos</strong></p>
<p>Synthetic aperture radar automatic target recognition (SAR-ATR) systems have rapidly evolved to tackle incremental recognition challenges in operational settings. Data scarcity remains a major hurdle that conventional SAR-ATR techniques struggle to address. To cope with this challenge, we propose a few-shot class-incremental learning (FSCIL) framework based on a dual-branch architecture that focuses on local feature extraction and leverages the discrete Fourier transform and global filters to capture long-term spatial dependencies. This incorporates a lightweight cross-attention mechanism that fuses domain-specific features with global dependencies to ensure robust feature interaction, while maintaining computational efficiency by introducing minimal scale-shift parameters. The framework combines focal loss for class distinction under imbalance and center loss for compact intra-class distributions to enhance class separation boundaries. Experimental results on the MSTAR benchmark dataset demonstrate that the proposed framework consistently outperforms state-of-the-art methods in FSCIL SAR-ATR, attesting to its effectiveness in real-world scenarios. </p>
<blockquote>
<p>åˆæˆå­”å¾„é›·è¾¾è‡ªåŠ¨ç›®æ ‡è¯†åˆ«ï¼ˆSAR-ATRï¼‰ç³»ç»Ÿå·²è¿…é€Ÿè¿›åŒ–ï¼Œä»¥åº”å¯¹æ“ä½œç¯å¢ƒä¸­ä¸æ–­å¢é•¿çš„è¯†åˆ«æŒ‘æˆ˜ã€‚æ•°æ®ç¨€ç¼ºä»æ˜¯ä¼ ç»ŸSAR-ATRæŠ€æœ¯éš¾ä»¥è§£å†³çš„ä¸€å¤§éšœç¢ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºåŒåˆ†æ”¯æ¶æ„çš„å°‘é‡ç±»åˆ«å¢é‡å­¦ä¹ ï¼ˆFSCILï¼‰æ¡†æ¶ï¼Œä¾§é‡äºå±€éƒ¨ç‰¹å¾æå–ï¼Œå¹¶åˆ©ç”¨ç¦»æ•£å‚…é‡Œå¶å˜æ¢å’Œå…¨å±€æ»¤æ³¢å™¨æ•è·é•¿æœŸç©ºé—´ä¾èµ–æ€§ã€‚è¿™ç»“åˆäº†è½»é‡çº§çš„è·¨æ³¨æ„åŠ›æœºåˆ¶ï¼Œèåˆäº†é¢†åŸŸç‰¹å®šç‰¹å¾ä¸å…¨å±€ä¾èµ–æ€§ï¼Œä»¥ç¡®ä¿ç¨³å¥çš„ç‰¹å¾äº¤äº’ï¼ŒåŒæ—¶é€šè¿‡å¼•å…¥æœ€å°çš„å°ºåº¦åç§»å‚æ•°æ¥ä¿æŒè®¡ç®—æ•ˆç‡ã€‚è¯¥æ¡†æ¶ç»“åˆäº†ç„¦ç‚¹æŸå¤±ï¼Œåœ¨ä¸å¹³è¡¡æƒ…å†µä¸‹è¿›è¡Œç±»åˆ«åŒºåˆ†ï¼Œä»¥åŠä¸­å¿ƒæŸå¤±ï¼Œç”¨äºç´§å‡‘çš„ç±»å†…åˆ†å¸ƒï¼Œä»¥å¢å¼ºç±»é—´åˆ†ç¦»è¾¹ç•Œã€‚åœ¨MSTARåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨FSCIL SAR-ATRæ–¹é¢å§‹ç»ˆä¼˜äºæœ€æ–°æ–¹æ³•ï¼Œè¯æ˜äº†å…¶åœ¨ç°å®åœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19565v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>SAR-ATRç³»ç»Ÿåœ¨é¢ä¸´ç°å®åœºæ™¯ä¸­é€æ¸å¢å¼ºçš„è¯†åˆ«æŒ‘æˆ˜æ—¶ï¼Œä»åœ¨è¿…é€Ÿæ¼”è¿›å‘å±•ã€‚ä¸ºè§£å†³å…¶ä¸­çš„æ•°æ®ç¨€ç¼ºéš¾é¢˜ï¼Œç ”ç©¶äººå‘˜æå‡ºäº†åŸºäºåŒåˆ†æ”¯æ¶æ„çš„å°‘é‡ç±»å¢é‡å­¦ä¹ ï¼ˆFSCILï¼‰æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡ç¦»æ•£å‚…é‡Œå¶å˜æ¢å’Œå…¨å±€æ»¤æ³¢å™¨ï¼Œå¯¹å±€éƒ¨ç‰¹å¾è¿›è¡Œæå–ï¼Œæ•æ‰é•¿æœŸç©ºé—´ä¾èµ–æ€§ã€‚åŒæ—¶ï¼Œå®ƒèåˆäº†é¢†åŸŸç‰¹å®šç‰¹å¾ä¸å…¨å±€ä¾èµ–æ€§ï¼Œç¡®ä¿ç¨³å¥çš„ç‰¹å¾äº¤äº’ï¼Œå¹¶é€šè¿‡å¼•å…¥æœ€å°çš„å°ºåº¦åç§»å‚æ•°ç»´æŒè®¡ç®—æ•ˆç‡ã€‚è¯¥æ¡†æ¶ç»“åˆäº†ç„¦ç‚¹æŸå¤±ä¸ä¸­å¿ƒæŸå¤±æ¥æå‡ç±»åˆ«ä¹‹é—´çš„ç•Œé™ã€‚åœ¨MSTARåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ¡†æ¶åœ¨FSCIL SAR-ATRæ–¹é¢è¡¨ç°ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œè¯æ˜äº†å…¶åœ¨ç°å®åœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>SAR-ATRç³»ç»Ÿé¢ä¸´æ•°æ®ç¨€ç¼ºçš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºåŒåˆ†æ”¯æ¶æ„çš„å°‘é‡ç±»å¢é‡å­¦ä¹ ï¼ˆFSCILï¼‰æ¡†æ¶æ¥è§£å†³æ•°æ®ç¨€ç¼ºé—®é¢˜ã€‚</li>
<li>åˆ©ç”¨ç¦»æ•£å‚…é‡Œå¶å˜æ¢å’Œå…¨å±€æ»¤æ³¢å™¨æ•æ‰é•¿æœŸç©ºé—´ä¾èµ–æ€§ã€‚</li>
<li>é€šè¿‡èåˆé¢†åŸŸç‰¹å®šç‰¹å¾å’Œå…¨å±€ä¾èµ–æ€§ç¡®ä¿ç¨³å¥çš„ç‰¹å¾äº¤äº’ã€‚</li>
<li>å¼•å…¥æœ€å°çš„å°ºåº¦åç§»å‚æ•°ä»¥ä¿æŒè®¡ç®—æ•ˆç‡ã€‚</li>
<li>ç»“åˆç„¦ç‚¹æŸå¤±å’Œä¸­å¿ƒæŸå¤±ä»¥å¢å¼ºç±»åˆ«ä¹‹é—´çš„ç•Œé™ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19565">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0a264a8763d970094975eadd920c9f1c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d5c822a22eacc5f38079389b2a5bbc61.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-00ebd1791887661b76adc3f17dd269ac.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3e8043c007b24fb5c42205de8d480f68.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b471aaeaba0e17769f91726c17386322.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="DiSa-Directional-Saliency-Aware-Prompt-Learning-for-Generalizable-Vision-Language-Models"><a href="#DiSa-Directional-Saliency-Aware-Prompt-Learning-for-Generalizable-Vision-Language-Models" class="headerlink" title="DiSa: Directional Saliency-Aware Prompt Learning for Generalizable   Vision-Language Models"></a>DiSa: Directional Saliency-Aware Prompt Learning for Generalizable   Vision-Language Models</h2><p><strong>Authors:Niloufar Alipour Talemi, Hossein Kashiani, Hossein R. Nowdeh, Fatemeh Afghah</strong></p>
<p>Prompt learning has emerged as a powerful paradigm for adapting vision-language models such as CLIP to downstream tasks. However, existing methods often overfit to seen data, leading to significant performance degradation when generalizing to novel classes or unseen domains. To address this limitation, we propose DiSa, a Directional Saliency-Aware Prompt Learning framework that integrates two complementary regularization strategies to enhance generalization. First, our Cross-Interactive Regularization (CIR) fosters cross-modal alignment by enabling cooperative learning between prompted and frozen encoders. Within CIR, a saliency-aware masking strategy guides the image encoder to prioritize semantically critical image regions, reducing reliance on less informative patches. Second, we introduce a directional regularization strategy that aligns visual embeddings with class-wise prototype features in a directional manner to prioritize consistency in feature orientation over strict proximity. This approach ensures robust generalization by leveraging stable prototype directions derived from class-mean statistics. Extensive evaluations on 11 diverse image classification benchmarks demonstrate that DiSa consistently outperforms state-of-the-art prompt learning methods across various settings, including base-to-novel generalization, cross-dataset transfer, domain generalization, and few-shot learning. </p>
<blockquote>
<p>æç¤ºå­¦ä¹ å·²ç»æˆä¸ºå°†CLIPç­‰è§†è§‰è¯­è¨€æ¨¡å‹é€‚åº”ä¸‹æ¸¸ä»»åŠ¡çš„ä¸€ç§å¼ºå¤§èŒƒå¼ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¾€å¾€ä¼šå¯¹å¯è§æ•°æ®è¿›è¡Œè¿‡åº¦æ‹Ÿåˆï¼Œå¯¼è‡´åœ¨é€‚åº”æ–°ç±»åˆ«æˆ–æœªè§é¢†åŸŸæ—¶æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†DiSaï¼Œè¿™æ˜¯ä¸€ä¸ªæ–¹å‘æ˜¾è‘—æ€§æ„ŸçŸ¥æç¤ºå­¦ä¹ æ¡†æ¶ï¼Œå®ƒé›†æˆäº†ä¸¤ç§äº’è¡¥çš„æ­£åˆ™åŒ–ç­–ç•¥æ¥æé«˜æ³›åŒ–èƒ½åŠ›ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬çš„è·¨äº¤äº’å¼æ­£åˆ™åŒ–ï¼ˆCIRï¼‰é€šè¿‡ä¿ƒè¿›æç¤ºå’Œå†»ç»“ç¼–ç å™¨ä¹‹é—´çš„åˆä½œæ€§å­¦ä¹ æ¥ä¿ƒè¿›è·¨æ¨¡æ€å¯¹é½ã€‚åœ¨CIRä¸­ï¼Œæ˜¾è‘—æ€§æ„ŸçŸ¥æ©ç ç­–ç•¥æŒ‡å¯¼å›¾åƒç¼–ç å™¨ä¼˜å…ˆå¤„ç†è¯­ä¹‰ä¸Šå…³é”®çš„å›¾åƒåŒºåŸŸï¼Œå‡å°‘å¯¹ä¿¡æ¯è¾ƒå°‘åŒºåŸŸçš„ä¾èµ–ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–¹å‘æ€§æ­£åˆ™åŒ–ç­–ç•¥ï¼Œè¯¥ç­–ç•¥ä»¥æ–¹å‘æ–¹å¼å¯¹é½è§†è§‰åµŒå…¥å’Œç±»åŸå‹ç‰¹å¾ï¼Œä»¥ä¼˜å…ˆç¡®ä¿ç‰¹å¾æ–¹å‘çš„è¿ç»­æ€§è€Œéä¸¥æ ¼æ¥è¿‘ã€‚è¿™ç§æ–¹æ³•é€šè¿‡åˆ©ç”¨ä»ç±»åˆ«å‡å€¼ç»Ÿè®¡å¾—å‡ºçš„ç¨³å®šåŸå‹æ–¹å‘æ¥ç¡®ä¿ç¨³å¥æ³›åŒ–ã€‚åœ¨11ä¸ªä¸åŒçš„å›¾åƒåˆ†ç±»åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒDiSaåœ¨å„ç§è®¾ç½®ä¸‹å§‹ç»ˆä¼˜äºæœ€æ–°æç¤ºå­¦ä¹ æ–¹æ³•ï¼ŒåŒ…æ‹¬åŸºæœ¬åˆ°æ–°ç±»åˆ«çš„æ³›åŒ–ã€è·¨æ•°æ®é›†è¿ç§»ã€é¢†åŸŸæ³›åŒ–å’Œå°æ ·æœ¬å­¦ä¹ ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19373v1">PDF</a> Accepted at the 31st ACM SIGKDD Conference on Knowledge Discovery and   Data Mining (KDD 2025)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆå¦‚CLIPï¼‰ä¸‹æ¸¸ä»»åŠ¡çš„æç¤ºå­¦ä¹ èŒƒå¼ï¼Œå¹¶æå‡ºäº†ä¸€ç§åä¸ºDiSaçš„æ–¹å‘æ€§æ˜¾è‘—æ€§æ„ŸçŸ¥æç¤ºå­¦ä¹ æ¡†æ¶ï¼Œä»¥å¢å¼ºæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚è¯¥æ¡†æ¶ç»“åˆäº†ä¸¤ç§äº’è¡¥çš„æ­£åˆ™åŒ–ç­–ç•¥ï¼šä¸€æ˜¯è·¨äº¤äº’å¼æ­£åˆ™åŒ–ï¼ˆCIRï¼‰ï¼Œé€šè¿‡åˆä½œå¼å­¦ä¹ ä¿ƒè¿›æç¤ºå’Œå†»ç»“ç¼–ç å™¨ä¹‹é—´çš„è·¨æ¨¡æ€å¯¹é½ï¼›äºŒæ˜¯æ–¹å‘æ€§æ­£åˆ™åŒ–ç­–ç•¥ï¼Œä»¥æ–¹å‘æ€§æ–¹å¼å¯¹é½è§†è§‰åµŒå…¥å’Œç±»åŸå‹ç‰¹å¾ï¼Œå¼ºè°ƒç‰¹å¾æ–¹å‘çš„ç¨³å®šæ€§ã€‚åœ¨å¹¿æ³›çš„å›¾åƒåˆ†ç±»åŸºå‡†æµ‹è¯•ä¸­ï¼ŒDiSaè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œåœ¨å„ç§è®¾ç½®ä¸­å‡ä¼˜äºæœ€æ–°çš„æç¤ºå­¦ä¹ æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DiSaæ˜¯ä¸€ä¸ªç”¨äºå¢å¼ºè§†è§‰è¯­è¨€æ¨¡å‹æ³›åŒ–èƒ½åŠ›çš„æ–¹å‘æ€§æ˜¾è‘—æ€§æ„ŸçŸ¥æç¤ºå­¦ä¹ æ¡†æ¶ã€‚</li>
<li>ç»“åˆä¸¤ç§äº’è¡¥çš„æ­£åˆ™åŒ–ç­–ç•¥ï¼šè·¨äº¤äº’å¼æ­£åˆ™åŒ–ï¼ˆCIRï¼‰å’Œæ–¹å‘æ€§æ­£åˆ™åŒ–ã€‚</li>
<li>CIRé€šè¿‡åˆä½œå¼å­¦ä¹ ä¿ƒè¿›è·¨æ¨¡æ€å¯¹é½ï¼Œå¹¶å¼•å…¥æ˜¾è‘—æ€§æ„ŸçŸ¥æ©ç ç­–ç•¥æ¥æŒ‡å¯¼å›¾åƒç¼–ç å™¨çš„å…³æ³¨é‡ç‚¹ã€‚</li>
<li>æ–¹å‘æ€§æ­£åˆ™åŒ–ç­–ç•¥å¼ºè°ƒç‰¹å¾æ–¹å‘çš„ç¨³å®šæ€§ï¼Œä½¿è§†è§‰åµŒå…¥ä¸ç±»åŸå‹ç‰¹å¾å¯¹é½ã€‚</li>
<li>DiSaé€šè¿‡åˆ©ç”¨ç¨³å®šçš„åŸå‹æ–¹å‘ï¼Œä»ç±»å‡å€¼ç»Ÿè®¡ä¸­è¡ç”Ÿè€Œæ¥ã€‚</li>
<li>åœ¨å¹¿æ³›çš„å›¾åƒåˆ†ç±»åŸºå‡†æµ‹è¯•ä¸­ï¼ŒDiSaåœ¨å¤šç§è®¾ç½®ä¸‹å‡ä¼˜äºæœ€æ–°çš„æç¤ºå­¦ä¹ æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19373">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f3198aa09e17f01167d04f3738706f74.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6f6371aad24937072484eb2e54107d45.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c505e2115baf52c8baa60ed5a6aa96a4.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="CloneShield-A-Framework-for-Universal-Perturbation-Against-Zero-Shot-Voice-Cloning"><a href="#CloneShield-A-Framework-for-Universal-Perturbation-Against-Zero-Shot-Voice-Cloning" class="headerlink" title="CloneShield: A Framework for Universal Perturbation Against Zero-Shot   Voice Cloning"></a>CloneShield: A Framework for Universal Perturbation Against Zero-Shot   Voice Cloning</h2><p><strong>Authors:Renyuan Li, Zhibo Liang, Haichuan Zhang, Tianyu Shi, Zhiyuan Cheng, Jia Shi, Carl Yang, Mingjie Tang</strong></p>
<p>Recent breakthroughs in text-to-speech (TTS) voice cloning have raised serious privacy concerns, allowing highly accurate vocal identity replication from just a few seconds of reference audio, while retaining the speakerâ€™s vocal authenticity. In this paper, we introduce CloneShield, a universal time-domain adversarial perturbation framework specifically designed to defend against zero-shot voice cloning. Our method provides protection that is robust across speakers and utterances, without requiring any prior knowledge of the synthesized text. We formulate perturbation generation as a multi-objective optimization problem, and propose Multi-Gradient Descent Algorithm (MGDA) to ensure the robust protection across diverse utterances. To preserve natural auditory perception for users, we decompose the adversarial perturbation via Mel-spectrogram representations and fine-tune it for each sample. This design ensures imperceptibility while maintaining strong degradation effects on zero-shot cloned outputs. Experiments on three state-of-the-art zero-shot TTS systems, five benchmark datasets and evaluations from 60 human listeners demonstrate that our method preserves near-original audio quality in protected inputs (PESQ &#x3D; 3.90, SRS &#x3D; 0.93) while substantially degrading both speaker similarity and speech quality in cloned samples (PESQ &#x3D; 1.07, SRS &#x3D; 0.08). </p>
<blockquote>
<p>åœ¨æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰è¯­éŸ³å…‹éš†é¢†åŸŸçš„æœ€æ–°çªç ´å¼•å‘äº†ä¸¥é‡çš„éšç§æ‹…å¿§ã€‚è¯¥æŠ€æœ¯ä»…ä»å‡ ç§’çš„å‚è€ƒéŸ³é¢‘å°±èƒ½å®ç°é«˜åº¦ç²¾ç¡®çš„è¯­éŸ³èº«ä»½å¤åˆ¶ï¼ŒåŒæ—¶ä¿ç•™è¯´è¯è€…çš„è¯­éŸ³çœŸå®æ€§ã€‚æœ¬æ–‡ä»‹ç»äº†CloneShieldï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨è®¾è®¡ç”¨äºé˜²èŒƒé›¶æ ·æœ¬è¯­éŸ³å…‹éš†çš„é€šç”¨æ—¶åŸŸå¯¹æŠ—æ‰°åŠ¨æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•æä¾›äº†è·¨è¯´è¯è€…å’Œè¯è¯­çš„ç¨³å¥ä¿æŠ¤ï¼Œæ— éœ€å¯¹åˆæˆæ–‡æœ¬æœ‰ä»»ä½•äº‹å…ˆäº†è§£ã€‚æˆ‘ä»¬å°†æ‰°åŠ¨ç”Ÿæˆåˆ¶å®šä¸ºä¸€ä¸ªå¤šç›®æ ‡ä¼˜åŒ–é—®é¢˜ï¼Œå¹¶æå‡ºå¤šæ¢¯åº¦ä¸‹é™ç®—æ³•ï¼ˆMGDAï¼‰ï¼Œä»¥ç¡®ä¿åœ¨ä¸åŒçš„è¯è¯­ä¸­æä¾›ç¨³å¥çš„ä¿æŠ¤ã€‚ä¸ºäº†ä¿æŒç”¨æˆ·è‡ªç„¶çš„å¬è§‰æ„ŸçŸ¥ï¼Œæˆ‘ä»¬é€šè¿‡æ¢…å°”é¢‘è°±è¡¨ç¤ºæ³•åˆ†è§£å¯¹æŠ—æ€§æ‰°åŠ¨ï¼Œå¹¶é’ˆå¯¹æ¯ä¸ªæ ·æœ¬è¿›è¡Œå¾®è°ƒã€‚è¿™ç§è®¾è®¡ç¡®ä¿äº†ä¸å¯æ„ŸçŸ¥æ€§ï¼ŒåŒæ—¶åœ¨å¯¹é›¶æ ·æœ¬å…‹éš†è¾“å‡ºè¿›è¡Œå¼ºé™è§£æ—¶ä¿æŒéŸ³é¢‘è´¨é‡ã€‚åœ¨ä¸‰ä¸ªæœ€å…ˆè¿›çš„é›¶æ ·æœ¬TTSç³»ç»Ÿã€äº”ä¸ªåŸºå‡†æ•°æ®é›†ä»¥åŠ60åäººç±»å¬è€…çš„è¯„ä¼°å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¿è¯å—ä¿æŠ¤è¾“å…¥æ¥è¿‘åŸå§‹éŸ³é¢‘è´¨é‡çš„åŒæ—¶ï¼ˆPESQ &#x3D; 3.90ï¼ŒSRS &#x3D; 0.93ï¼‰ï¼Œæ˜¾è‘—é™ä½äº†å…‹éš†æ ·æœ¬ä¸­çš„è¯´è¯äººç›¸ä¼¼åº¦å’Œè¯­éŸ³è´¨é‡ï¼ˆPESQ &#x3D; 1.07ï¼ŒSRS &#x3D; 0.08ï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19119v1">PDF</a> 10pages, 4figures</p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰è¯­éŸ³å…‹éš†æŠ€æœ¯çš„æœ€æ–°çªç ´å¼•å‘äº†ä¸¥é‡çš„éšç§æ‹…å¿§ï¼Œå› ä¸ºä»…éœ€å‡ ç§’çš„å‚è€ƒéŸ³é¢‘å³å¯å®ç°é«˜åº¦ç²¾ç¡®çš„è¯­éŸ³èº«ä»½å¤åˆ¶ï¼ŒåŒæ—¶ä¿æŒè¯´è¯è€…çš„è¯­éŸ³çœŸå®æ€§ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºCloneShieldçš„é€šç”¨æ—¶é—´åŸŸå¯¹æŠ—æ‰°åŠ¨æ¡†æ¶ï¼Œä¸“é—¨ç”¨äºé˜²èŒƒé›¶æ ·æœ¬è¯­éŸ³å…‹éš†ã€‚è¯¥æ–¹æ³•å…·æœ‰è·¨è¯´è¯è€…å’Œè¯è¯­çš„é²æ£’æ€§ä¿æŠ¤èƒ½åŠ›ï¼Œæ— éœ€å¯¹åˆæˆæ–‡æœ¬æœ‰ä»»ä½•å…ˆéªŒçŸ¥è¯†ã€‚æˆ‘ä»¬åˆ¶å®šæ‰°åŠ¨ç”Ÿæˆä½œä¸ºå¤šç›®æ ‡ä¼˜åŒ–é—®é¢˜ï¼Œå¹¶æå‡ºå¤šæ¢¯åº¦ä¸‹é™ç®—æ³•ï¼ˆMGDAï¼‰ä»¥ç¡®ä¿è·¨ä¸åŒè¯è¯­çš„é²æ£’æ€§ä¿æŠ¤ã€‚ä¸ºäº†ä¿æŒç”¨æˆ·è‡ªç„¶çš„å¬è§‰æ„ŸçŸ¥ï¼Œæˆ‘ä»¬é€šè¿‡æ¢…å°”é¢‘è°±å›¾è¡¨ç¤ºå°†å¯¹æŠ—æ€§æ‰°åŠ¨åˆ†è§£ï¼Œå¹¶é’ˆå¯¹æ¯ä¸ªæ ·æœ¬è¿›è¡Œå¾®è°ƒã€‚è¿™ä¸€è®¾è®¡ç¡®ä¿äº†ä¸å¯å¯Ÿè§‰æ€§ï¼ŒåŒæ—¶ä¿æŒäº†å¯¹é›¶æ ·æœ¬å…‹éš†è¾“å‡ºçš„å¼ºçƒˆé™è§£æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰è¯­éŸ³å…‹éš†æŠ€æœ¯çš„æ–°è¿›å±•å¼•å‘äº†å…³äºéšç§çš„æ‹…å¿§ï¼Œå› ä¸ºèƒ½å¤Ÿåˆ©ç”¨æçŸ­å‚è€ƒéŸ³é¢‘è¿›è¡Œç²¾ç¡®çš„è¯­éŸ³èº«ä»½å¤åˆ¶ã€‚</li>
<li>CloneShieldæ¡†æ¶è¢«æå‡ºç”¨äºå¯¹æŠ—é›¶æ ·æœ¬è¯­éŸ³å…‹éš†ï¼Œæä¾›è·¨è¯´è¯è€…å’Œè¯è¯­çš„é²æ£’ä¿æŠ¤ã€‚</li>
<li>CloneShieldä¸éœ€è¦å¯¹åˆæˆæ–‡æœ¬æœ‰å…ˆéªŒçŸ¥è¯†ã€‚</li>
<li>é€šè¿‡å¤šç›®æ ‡ä¼˜åŒ–é—®é¢˜å’Œå¤šæ¢¯åº¦ä¸‹é™ç®—æ³•ï¼ˆMGDAï¼‰ç¡®ä¿é²æ£’æ€§ä¿æŠ¤ã€‚</li>
<li>å¯¹æŠ—æ‰°åŠ¨é€šè¿‡æ¢…å°”é¢‘è°±å›¾è¡¨ç¤ºè¿›è¡Œåˆ†è§£ï¼Œå¹¶é’ˆå¯¹æ¯ä¸ªæ ·æœ¬å¾®è°ƒï¼Œä»¥ç»´æŒè‡ªç„¶å¬è§‰æ„ŸçŸ¥å¹¶æŠµæŠ—é›¶æ ·æœ¬å…‹éš†æ”»å‡»ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ä¿æŠ¤æ–¹æ³•èƒ½å¤Ÿåœ¨ä¿æŠ¤è¾“å…¥æ—¶ä¿æŒæ¥è¿‘åŸå§‹éŸ³é¢‘è´¨é‡ï¼ŒåŒæ—¶æ˜¾è‘—é™ä½äº†å…‹éš†æ ·æœ¬çš„è¯´è¯äººç›¸ä¼¼æ€§å’Œè¯­éŸ³è´¨é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19119">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f9fa5d40cacd81f3f87412b0f4b1980b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f48159884e007271b45a6d7ef31becb6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-77d5ab548076a2a7d472a31acfc84848.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1b4dfc995d63dd1d5b5448a7a7a4384b.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="CLaDMoP-Learning-Transferrable-Models-from-Successful-Clinical-Trials-via-LLMs"><a href="#CLaDMoP-Learning-Transferrable-Models-from-Successful-Clinical-Trials-via-LLMs" class="headerlink" title="CLaDMoP: Learning Transferrable Models from Successful Clinical Trials   via LLMs"></a>CLaDMoP: Learning Transferrable Models from Successful Clinical Trials   via LLMs</h2><p><strong>Authors:Yiqing Zhang, Xiaozhong Liu, Fabricio Murai</strong></p>
<p>Many existing models for clinical trial outcome prediction are optimized using task-specific loss functions on trial phase-specific data. While this scheme may boost prediction for common diseases and drugs, it can hinder learning of generalizable representations, leading to more false positives&#x2F;negatives. To address this limitation, we introduce CLaDMoP, a new pre-training approach for clinical trial outcome prediction, alongside the Successful Clinical Trials dataset(SCT), specifically designed for this task. CLaDMoP leverages a Large Language Model-to encode trialsâ€™ eligibility criteria-linked to a lightweight Drug-Molecule branch through a novel multi-level fusion technique. To efficiently fuse long embeddings across levels, we incorporate a grouping block, drastically reducing computational overhead. CLaDMoP avoids reliance on task-specific objectives by pre-training on a â€œpair matchingâ€ proxy task. Compared to established zero-shot and few-shot baselines, our method significantly improves both PR-AUC and ROC-AUC, especially for phase I and phase II trials. We further evaluate and perform ablation on CLaDMoP after Parameter-Efficient Fine-Tuning, comparing it to state-of-the-art supervised baselines, including MEXA-CTP, on the Trial Outcome Prediction(TOP) benchmark. CLaDMoP achieves up to 10.5% improvement in PR-AUC and 3.6% in ROC-AUC, while attaining comparable F1 score to MEXA-CTP, highlighting its potential for clinical trial outcome prediction. Code and SCT dataset can be downloaded from <a target="_blank" rel="noopener" href="https://github.com/murai-lab/CLaDMoP">https://github.com/murai-lab/CLaDMoP</a>. </p>
<blockquote>
<p>ç°æœ‰è®¸å¤šç”¨äºä¸´åºŠè¯•éªŒç»“æœé¢„æµ‹çš„æ¨¡å‹ï¼Œå®ƒä»¬é€šè¿‡ä½¿ç”¨é’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„æŸå¤±å‡½æ•°å¯¹ç‰¹å®šè¯•éªŒé˜¶æ®µçš„æ•°æ®è¿›è¡Œä¼˜åŒ–ã€‚è™½ç„¶è¿™ç§æ–¹æ¡ˆå¯èƒ½ä¼šæé«˜å¸¸è§ç–¾ç—…å’Œè¯ç‰©çš„é¢„æµ‹èƒ½åŠ›ï¼Œä½†å®ƒå¯èƒ½ä¼šé˜»ç¢é€šç”¨è¡¨ç¤ºçš„å­¦ä¹ ï¼Œä»è€Œå¯¼è‡´æ›´å¤šçš„è¯¯æŠ¥ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†CLaDMoPï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„ä¸´åºŠè¯•éªŒç»“æœé¢„æµ‹é¢„è®­ç»ƒæ–¹æ³•ï¼Œä»¥åŠä¸ºæ­¤ä»»åŠ¡ä¸“é—¨è®¾è®¡çš„æˆåŠŸä¸´åºŠè¯•éªŒæ•°æ®é›†ï¼ˆSCTï¼‰ã€‚CLaDMoPåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å¯¹è¯•éªŒèµ„æ ¼æ ‡å‡†è¿›è¡Œç¼–ç ï¼Œå¹¶ä¸è½»é‡çº§çš„è¯ç‰©åˆ†å­åˆ†æ”¯é€šè¿‡æ–°å‹çš„å¤šå±‚æ¬¡èåˆæŠ€æœ¯ç›¸è”ç³»ã€‚ä¸ºäº†æœ‰æ•ˆåœ°èåˆå„çº§çš„é•¿åµŒå…¥ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåˆ†ç»„å—ï¼Œå¤§å¤§é™ä½äº†è®¡ç®—å¼€é”€ã€‚CLaDMoPé€šè¿‡åœ¨ä¸€ä¸ªâ€œé…å¯¹åŒ¹é…â€ä»£ç†ä»»åŠ¡ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œé¿å…äº†ä¾èµ–ç‰¹å®šä»»åŠ¡ç›®æ ‡ã€‚ä¸ç°æœ‰çš„é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬åŸºçº¿ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨PR-AUCå’ŒROC-AUCä¸Šæœ‰äº†æ˜¾è‘—çš„æé«˜ï¼Œç‰¹åˆ«æ˜¯åœ¨IæœŸå’ŒIIæœŸè¯•éªŒé˜¶æ®µã€‚æˆ‘ä»¬è¿›ä¸€æ­¥å¯¹CLaDMoPè¿›è¡Œå‚æ•°é«˜æ•ˆå¾®è°ƒåçš„è¯„ä¼°å¹¶è¿›è¡Œæ¶ˆèå®éªŒï¼Œå°†å…¶ä¸åŒ…æ‹¬MEXA-CTPåœ¨å†…çš„æœ€æ–°ç›‘ç£åŸºçº¿åœ¨ä¸´åºŠè¯•éªŒç»“æœé¢„æµ‹åŸºå‡†æµ‹è¯•é›†ä¸Šè¿›è¡Œæ¯”è¾ƒã€‚CLaDMoPåœ¨PR-AUCä¸Šæé«˜äº†é«˜è¾¾10.5%ï¼Œåœ¨ROC-AUCä¸Šæé«˜äº†3.6%ï¼ŒåŒæ—¶ä¸MEXA-CTPçš„F1å¾—åˆ†ç›¸å½“ï¼Œè¿™çªæ˜¾äº†å…¶åœ¨ä¸´åºŠè¯•éªŒç»“æœé¢„æµ‹æ–¹é¢çš„æ½œåŠ›ã€‚ä»£ç å’ŒSCTæ•°æ®é›†å¯ä»<a target="_blank" rel="noopener" href="https://github.com/murai-lab/CLaDMoP%E4%B8%8B%E8%BD%BD%E3%80%82">https://github.com/murai-lab/CLaDMoPä¸‹è½½ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.18527v1">PDF</a> Accepted and to be published in KDD2025</p>
<p><strong>Summary</strong></p>
<p>CLaDMoPæ˜¯ä¸€ç§é’ˆå¯¹ä¸´åºŠè¯•éªŒç»“æœé¢„æµ‹çš„æ–°å‹é¢„è®­ç»ƒç­–ç•¥ï¼Œå®ƒå¼•å…¥äº†å¤§å‹è¯­è¨€æ¨¡å‹æ¥ç¼–ç ä¸è¯ç‰©åˆ†å­ç›¸å…³çš„è¯•éªŒèµ„æ ¼æ ‡å‡†ã€‚é€šè¿‡å¤šå±‚æ¬¡èåˆæŠ€æœ¯ï¼ŒCLaDMoPèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°èåˆé•¿åµŒå…¥ä¿¡æ¯ï¼Œä»è€Œæé«˜é¢„æµ‹å‡†ç¡®æ€§ã€‚è¯¥ç­–ç•¥é¿å…äº†ä¾èµ–ç‰¹å®šä»»åŠ¡ç›®æ ‡ï¼Œè€Œæ˜¯é€šè¿‡â€œé…å¯¹åŒ¹é…â€ä»£ç†ä»»åŠ¡è¿›è¡Œé¢„è®­ç»ƒã€‚ä¸ç°æœ‰çš„é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬åŸºçº¿ç›¸æ¯”ï¼ŒCLaDMoPåœ¨PR-AUCå’ŒROC-AUCä¸Šè¡¨ç°æ›´ä¼˜ï¼Œç‰¹åˆ«æ˜¯åœ¨æ—©æœŸä¸´åºŠè¯•éªŒé˜¶æ®µã€‚ç»è¿‡å‚æ•°æœ‰æ•ˆå¾®è°ƒåï¼ŒCLaDMoPä¸å½“å‰å…ˆè¿›çš„ç›‘ç£åŸºçº¿ç›¸æ¯”ï¼Œåœ¨PR-AUCä¸Šæé«˜äº†10.5%ï¼ŒROC-AUCæé«˜äº†3.6%ï¼ŒåŒæ—¶ä¿æŒäº†ä¸MEXA-CTPç›¸å½“çš„F1åˆ†æ•°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CLaDMoPæ˜¯ä¸€ç§æ–°çš„é¢„è®­ç»ƒç­–ç•¥ï¼Œç”¨äºä¸´åºŠè¯•éªŒç»“æœé¢„æµ‹ã€‚</li>
<li>å®ƒç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹ä¸è¯ç‰©åˆ†å­ä¿¡æ¯ï¼Œé€šè¿‡å¤šå±‚æ¬¡èåˆæŠ€æœ¯å¤„ç†æ•°æ®ã€‚</li>
<li>CLaDMoPé‡‡ç”¨â€œé…å¯¹åŒ¹é…â€ä»£ç†ä»»åŠ¡è¿›è¡Œé¢„è®­ç»ƒï¼Œä¸ä¾èµ–ç‰¹å®šä»»åŠ¡ç›®æ ‡ã€‚</li>
<li>ä¸å…¶ä»–åŸºçº¿ç›¸æ¯”ï¼ŒCLaDMoPåœ¨PR-AUCå’ŒROC-AUCæŒ‡æ ‡ä¸Šè¡¨ç°æ›´ä¼˜ç§€ã€‚</li>
<li>CLaDMoPåœ¨æ—©æœŸä¸´åºŠè¯•éªŒé˜¶æ®µï¼ˆå¦‚IæœŸå’ŒIIæœŸï¼‰å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚</li>
<li>ç»è¿‡å‚æ•°æœ‰æ•ˆå¾®è°ƒåï¼ŒCLaDMoPä¸å½“å‰å…ˆè¿›çš„ç›‘ç£åŸºçº¿ç›¸æ¯”ï¼Œå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</li>
<li>CLaDMoPçš„ä»£ç å’Œæ‰€ç”¨æ•°æ®é›†å¯ä»æŒ‡å®šé“¾æ¥ä¸‹è½½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.18527">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c4ae5d97b9465735d11ed9bebeaaed5f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5cce6383d86c2932898146e88267f9b8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-21718625b460849dd24f74044d8b47c0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f6cf771d2850f626d7cadc39c6aad172.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-dcfa2e09a6b64add34f684d4dc878cc1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f9580d2462c11a1020bc2cd297c0bfe2.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="MAPLE-Many-Shot-Adaptive-Pseudo-Labeling-for-In-Context-Learning"><a href="#MAPLE-Many-Shot-Adaptive-Pseudo-Labeling-for-In-Context-Learning" class="headerlink" title="MAPLE: Many-Shot Adaptive Pseudo-Labeling for In-Context Learning"></a>MAPLE: Many-Shot Adaptive Pseudo-Labeling for In-Context Learning</h2><p><strong>Authors:Zihan Chen, Song Wang, Zhen Tan, Jundong Li, Cong Shen</strong></p>
<p>In-Context Learning (ICL) empowers Large Language Models (LLMs) to tackle diverse tasks by incorporating multiple input-output examples, known as demonstrations, into the input of LLMs. More recently, advancements in the expanded context windows of LLMs have led to many-shot ICL, which uses hundreds of demonstrations and outperforms few-shot ICL, which relies on fewer examples. However, this approach is often hindered by the high cost of obtaining large amounts of labeled data. To address this challenge, we propose Many-Shot Adaptive Pseudo-LabEling, namely MAPLE, a novel influence-based many-shot ICL framework that utilizes pseudo-labeled samples to compensate for the lack of label information. We first identify a subset of impactful unlabeled samples and perform pseudo-labeling on them by querying LLMs. These pseudo-labeled samples are then adaptively selected and tailored to each test query as input to improve the performance of many-shot ICL, without significant labeling costs. Extensive experiments on real-world datasets demonstrate the effectiveness of our framework, showcasing its ability to enhance LLM adaptability and performance with limited labeled data. </p>
<blockquote>
<p>ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰é€šè¿‡åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¾“å…¥ä¸­èå…¥å¤šä¸ªè¾“å…¥è¾“å‡ºç¤ºä¾‹ï¼ˆå³æ¼”ç¤ºï¼‰ï¼Œä½¿å…¶èƒ½å¤Ÿåº”å¯¹å„ç§ä»»åŠ¡ã€‚æœ€è¿‘ï¼ŒLLMæ‰©å±•çš„ä¸Šä¸‹æ–‡çª—å£çš„è¿›æ­¥å¯¼è‡´äº†å¤šé•œå¤´ICLçš„å‡ºç°ï¼Œå®ƒä½¿ç”¨æ•°ç™¾ä¸ªæ¼”ç¤ºï¼Œå¹¶ä¸”è¡¨ç°ä¼˜äºä¾èµ–è¾ƒå°‘ç¤ºä¾‹çš„å°‘é•œå¤´ICLã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•å¾€å¾€å—åˆ°è·å–å¤§é‡æ ‡è®°æ•°æ®çš„é«˜æˆæœ¬çš„é˜»ç¢ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºå½±å“çš„å¤šé•œå¤´è‡ªé€‚åº”ä¼ªæ ‡ç­¾æŠ€æœ¯ï¼ˆMAPLEï¼‰ã€‚è¿™æ˜¯ä¸€ä¸ªæ–°çš„å¤šé•œå¤´ICLæ¡†æ¶ï¼Œå®ƒåˆ©ç”¨ä¼ªæ ‡ç­¾æ ·æœ¬æ¥å¼¥è¡¥æ ‡ç­¾ä¿¡æ¯çš„ç¼ºå¤±ã€‚æˆ‘ä»¬é¦–å…ˆè¯†åˆ«å‡ºæœ‰å½±å“åŠ›çš„æœªæ ‡è®°æ ·æœ¬å­é›†ï¼Œå¹¶é€šè¿‡æŸ¥è¯¢LLMå¯¹å®ƒä»¬è¿›è¡Œä¼ªæ ‡ç­¾æ ‡æ³¨ã€‚è¿™äº›ä¼ªæ ‡ç­¾æ ·æœ¬éšåè¢«è‡ªé€‚åº”åœ°é€‰æ‹©å’Œå®šåˆ¶ä¸ºé’ˆå¯¹æ¯ä¸ªæµ‹è¯•æŸ¥è¯¢çš„è¾“å…¥ï¼Œä»¥æé«˜å¤šé•œå¤´ICLçš„æ€§èƒ½ï¼Œè€Œæ— éœ€å¢åŠ å¤§é‡çš„æ ‡è®°æˆæœ¬ã€‚åœ¨çœŸå®æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¯æ˜äº†æˆ‘ä»¬æ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼Œå±•ç¤ºäº†å…¶åœ¨æœ‰é™æ ‡è®°æ•°æ®ä¸‹æé«˜LLMé€‚åº”æ€§å’Œæ€§èƒ½çš„èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16225v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åœ¨ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰ä¸­ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡èå…¥å¤šä¸ªè¾“å…¥è¾“å‡ºç¤ºä¾‹ï¼ˆå³ç¤ºèŒƒï¼‰æ¥å¤„ç†å„ç§ä»»åŠ¡ã€‚æœ€è¿‘ï¼ŒLLMæ‰©å±•çš„ä¸Šä¸‹æ–‡çª—å£çš„è¿›æ­¥å¯¼è‡´äº†å¤šç¤ºä¾‹ICLçš„å‡ºç°ï¼Œå®ƒä½¿ç”¨æ•°ç™¾ä¸ªç¤ºèŒƒï¼Œå¹¶ä¼˜äºä¾èµ–è¾ƒå°‘ç¤ºä¾‹çš„few-shot ICLã€‚ç„¶è€Œï¼Œè·å–å¤§é‡æ ‡è®°æ•°æ®çš„æˆæœ¬é«˜æ˜‚å¸¸å¸¸æˆä¸ºè¯¥æ–¹æ³•çš„ç“¶é¢ˆã€‚ä¸ºè§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†åä¸ºMAPLEçš„æ–°å‹åŸºäºå½±å“çš„å¤šç¤ºä¾‹ICLæ¡†æ¶ï¼Œåˆ©ç”¨ä¼ªæ ‡è®°æ ·æœ¬æ¥å¼¥è¡¥æ ‡ç­¾ä¿¡æ¯çš„ä¸è¶³ã€‚æˆ‘ä»¬é¦–å…ˆè¯†åˆ«å‡ºæœ‰å½±å“åŠ›çš„æœªæ ‡è®°æ ·æœ¬å­é›†ï¼Œå¹¶é€šè¿‡æŸ¥è¯¢LLMå¯¹å®ƒä»¬è¿›è¡Œä¼ªæ ‡è®°ã€‚è¿™äº›ä¼ªæ ‡è®°çš„æ ·æœ¬éšåè¢«è‡ªé€‚åº”åœ°é€‰æ‹©å’Œå®šåˆ¶ä¸ºé’ˆå¯¹æ¯ä¸ªæµ‹è¯•æŸ¥è¯¢çš„è¾“å…¥ï¼Œä»¥æé«˜å¤šç¤ºä¾‹ICLçš„æ€§èƒ½ï¼ŒåŒæ—¶æ— éœ€å¤§é‡çš„æ ‡è®°æˆæœ¬ã€‚åœ¨çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¯æ˜äº†æˆ‘ä»¬çš„æ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼Œå±•ç¤ºäº†å…¶åœ¨æœ‰é™æ ‡è®°æ•°æ®ä¸‹æé«˜LLMé€‚åº”æ€§å’Œæ€§èƒ½çš„èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ICLä½¿LLMèƒ½å¤Ÿé€šè¿‡èå…¥å¤šä¸ªè¾“å…¥è¾“å‡ºç¤ºä¾‹ï¼ˆå³ç¤ºèŒƒï¼‰æ¥å¤„ç†å¤šç§ä»»åŠ¡ã€‚</li>
<li>å¤šç¤ºä¾‹ICLä½¿ç”¨æ•°ç™¾ä¸ªç¤ºèŒƒï¼Œå¹¶é€šå¸¸ä¼˜äºfew-shot ICLã€‚</li>
<li>è·å–å¤§é‡æ ‡è®°æ•°æ®çš„æˆæœ¬é«˜æ˜‚æ˜¯ICLçš„ä¸€ä¸ªæŒ‘æˆ˜ã€‚</li>
<li>MAPLEæ˜¯ä¸€ç§æ–°å‹å¤šç¤ºä¾‹ICLæ¡†æ¶ï¼Œåˆ©ç”¨ä¼ªæ ‡è®°æ ·æœ¬æ¥å¼¥è¡¥æ ‡ç­¾ä¿¡æ¯çš„ä¸è¶³ã€‚</li>
<li>MAPLEé€šè¿‡è¯†åˆ«æœ‰å½±å“åŠ›çš„æœªæ ‡è®°æ ·æœ¬å¹¶è¿›è¡Œä¼ªæ ‡è®°æ¥å·¥ä½œã€‚</li>
<li>ä¼ªæ ‡è®°çš„æ ·æœ¬è¢«è‡ªé€‚åº”åœ°é€‰æ‹©å’Œå®šåˆ¶ä¸ºé’ˆå¯¹æ¯ä¸ªæµ‹è¯•æŸ¥è¯¢çš„è¾“å…¥ï¼Œæé«˜å¤šç¤ºä¾‹ICLçš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16225">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-1a973b9fae5e602452608acc87477a61.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-23433cd48904f7524800a513b34c50ed.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f4e07756865fc8648ea5f8b6817b2541.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="CLEVER-A-Curated-Benchmark-for-Formally-Verified-Code-Generation"><a href="#CLEVER-A-Curated-Benchmark-for-Formally-Verified-Code-Generation" class="headerlink" title="CLEVER: A Curated Benchmark for Formally Verified Code Generation"></a>CLEVER: A Curated Benchmark for Formally Verified Code Generation</h2><p><strong>Authors:Amitayush Thakur, Jasper Lee, George Tsoukalas, Meghana Sistla, Matthew Zhao, Stefan Zetzsche, Greg Durrett, Yisong Yue, Swarat Chaudhuri</strong></p>
<p>We introduce ${\rm C{\small LEVER}}$, a high-quality, curated benchmark of 161 problems for end-to-end verified code generation in Lean. Each problem consists of (1) the task of generating a specification that matches a held-out ground-truth specification, and (2) the task of generating a Lean implementation that provably satisfies this specification. Unlike prior benchmarks, ${\rm C{\small LEVER}}$ avoids test-case supervision, LLM-generated annotations, and specifications that leak implementation logic or allow vacuous solutions. All outputs are verified post-hoc using Leanâ€™s type checker to ensure machine-checkable correctness. We use ${\rm C{\small LEVER}}$ to evaluate several few-shot and agentic approaches based on state-of-the-art language models. These methods all struggle to achieve full verification, establishing it as a challenging frontier benchmark for program synthesis and formal reasoning. Our benchmark can be found on GitHub(<a target="_blank" rel="noopener" href="https://github.com/trishullab/clever">https://github.com/trishullab/clever</a>) as well as HuggingFace(<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/amitayusht/clever">https://huggingface.co/datasets/amitayusht/clever</a>). All our evaluation code is also available online(<a target="_blank" rel="noopener" href="https://github.com/trishullab/clever-prover">https://github.com/trishullab/clever-prover</a>). </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†${\rm C{\small LEVER}}$ï¼Œè¿™æ˜¯ä¸€ä¸ªé«˜è´¨é‡çš„ã€ç»è¿‡ç­›é€‰çš„åŒ…å«161ä¸ªé—®é¢˜çš„åŸºå‡†æµ‹è¯•ï¼Œç”¨äºç«¯åˆ°ç«¯çš„Leanä»£ç ç”ŸæˆéªŒè¯ã€‚æ¯ä¸ªé—®é¢˜ç”±ä¸¤éƒ¨åˆ†ç»„æˆï¼šï¼ˆ1ï¼‰ç”Ÿæˆä¸ä¿ç•™çš„çœŸå®è§„æ ¼ç›¸åŒ¹é…çš„è§„æ ¼çš„ä»»åŠ¡ï¼›ï¼ˆ2ï¼‰ç”Ÿæˆå¯è¯æ˜æ»¡è¶³æ­¤è§„æ ¼è¦æ±‚çš„Leanå®ç°çš„ä»»åŠ¡ã€‚ä¸åŒäºä»¥å¾€çš„åŸºå‡†æµ‹è¯•ï¼Œ${\rm C{\small LEVER}}$é¿å…äº†æµ‹è¯•ç”¨ä¾‹çš„ç›‘ç£ã€LLMç”Ÿæˆçš„æ³¨é‡Šä»¥åŠæ³„éœ²å®ç°é€»è¾‘æˆ–å…è®¸ç©ºæ´è§£å†³æ–¹æ¡ˆçš„è§„æ ¼ã€‚æ‰€æœ‰è¾“å‡ºéƒ½ä½¿ç”¨Leançš„ç±»å‹æ£€æŸ¥å™¨è¿›è¡Œäº‹åéªŒè¯ï¼Œä»¥ç¡®ä¿å¯æœºå™¨æ£€æŸ¥çš„æ­£ç¡®æ€§ã€‚æˆ‘ä»¬ä½¿ç”¨${\rm C{\small LEVER}}$æ¥è¯„ä¼°åŸºäºæœ€æ–°è¯­è¨€æ¨¡å‹çš„å‡ ç§å°‘é•œå¤´å’Œè‡ªä¸»æ–¹æ³•ã€‚è¿™äº›æ–¹æ³•åœ¨å…¨é¢å®ç°éªŒè¯æ–¹é¢å‡é¢ä¸´å›°éš¾ï¼Œè¿™ä½¿å…¶æˆä¸ºç¨‹åºåˆæˆå’Œå½¢å¼æ¨ç†çš„å‰ç»æ€§æŒ‘æˆ˜åŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•å¯ä»¥åœ¨GitHubï¼ˆ<a target="_blank" rel="noopener" href="https://github.com/trishullab/clever%EF%BC%89%E4%BB%A5%E5%8F%8AHuggingFace%EF%BC%88https://huggingface.co/datasets/amitayusht/clever%EF%BC%89%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82%E6%89%80%E6%9C%89%E7%9A%84%E8%AF%84%E4%BC%B0%E4%BB%A3%E7%A0%81%E4%B9%9F%E9%83%BD%E5%9C%A8%E7%BD%91%E4%B8%8A%E5%8F%AF%E7%94%A8%EF%BC%88https://github.com/trishullab/clever-prover%EF%BC%89%E3%80%82">https://github.com/trishullab/cleverï¼‰ä»¥åŠHuggingFaceï¼ˆhttps://huggingface.co/datasets/amitayusht/cleverï¼‰ä¸Šæ‰¾åˆ°ã€‚æ‰€æœ‰çš„è¯„ä¼°ä»£ç ä¹Ÿéƒ½åœ¨ç½‘ä¸Šå¯ç”¨ï¼ˆhttps://github.com/trishullab/clever-proverï¼‰ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13938v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>${\rm C{\small LEVER}}$æ˜¯ä¸€ä¸ªé«˜è´¨é‡çš„ã€é’ˆå¯¹ç«¯å¯¹ç«¯éªŒè¯çš„ä»£ç ç”Ÿæˆä»»åŠ¡çš„åŸºå‡†æµ‹è¯•é›†ï¼ŒåŒ…å«161ä¸ªé—®é¢˜ã€‚å®ƒé¿å…äº†æµ‹è¯•ç”¨ä¾‹ç›‘ç£ã€å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„æ³¨è§£ä»¥åŠæ³„éœ²å®ç°é€»è¾‘æˆ–å…è®¸ç©ºæ´è§£å†³æ–¹æ¡ˆçš„è§„æ ¼ã€‚æ‰€æœ‰è¾“å‡ºéƒ½ä½¿ç”¨Leançš„ç±»å‹æ£€æŸ¥å™¨è¿›è¡Œäº‹åéªŒè¯ï¼Œä»¥ç¡®ä¿æœºå™¨å¯æ£€æŸ¥çš„æ­£ç¡®æ€§ã€‚è¯¥åŸºå‡†æµ‹è¯•é›†ç”¨äºè¯„ä¼°åŸºäºæœ€æ–°è¯­è¨€æ¨¡å‹çš„å‡ ç§å°æ ·æœ¬å’Œæ™ºèƒ½æ–¹æ³•ï¼Œä½†ç°æœ‰çš„æ–¹æ³•éƒ½éš¾ä»¥å®ç°å®Œå…¨éªŒè¯ï¼Œä½¿å…¶æˆä¸ºç¨‹åºåˆæˆå’Œå½¢å¼æ¨ç†çš„å…·æœ‰æŒ‘æˆ˜æ€§çš„å‰æ²¿åŸºå‡†æµ‹è¯•é›†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>${\rm C{\small LEVER}}$æ˜¯ä¸€ä¸ªç”¨äºç«¯åˆ°ç«¯éªŒè¯çš„ä»£ç ç”Ÿæˆä»»åŠ¡çš„åŸºå‡†æµ‹è¯•é›†ï¼ŒåŒ…å«161ä¸ªé«˜è´¨é‡é—®é¢˜ã€‚</li>
<li>å®ƒé¿å…äº†æµ‹è¯•ç”¨ä¾‹ç›‘ç£ã€å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„æ³¨è§£ä»¥åŠå¯èƒ½å¯¼è‡´é”™è¯¯æˆ–ç©ºæ´è§£å†³æ–¹æ¡ˆçš„è§„æ ¼ã€‚</li>
<li>æ‰€æœ‰è¾“å‡ºéƒ½ä½¿ç”¨Leançš„ç±»å‹æ£€æŸ¥å™¨è¿›è¡Œäº‹åéªŒè¯ï¼Œä»¥ç¡®ä¿æœºå™¨å¯æ£€æŸ¥çš„æ­£ç¡®æ€§ã€‚</li>
<li>${\rm C{\small LEVER}}$è¢«ç”¨æ¥è¯„ä¼°æœ€æ–°çš„è¯­è¨€æ¨¡å‹åœ¨ç¨‹åºåˆæˆæ–¹é¢çš„èƒ½åŠ›ã€‚</li>
<li>ç›®å‰çš„æ–¹æ³•åœ¨å®Œå…¨éªŒè¯æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œè¡¨æ˜è¿™æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„å‰æ²¿åŸºå‡†æµ‹è¯•é›†ã€‚</li>
<li>è¯¥åŸºå‡†æµ‹è¯•é›†å¯ä»¥åœ¨GitHubå’ŒHuggingFaceä¸Šæ‰¾åˆ°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13938">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-768e2c574d893c6a85068eaf021ee65d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bf4bb3beeb9a906f276484cb351bd82c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-059b0011d1e2a8174241dab92289ec5b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4e1a8461994df8449d3c7925819fe90e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-92dbad38362c5c981f79f4e240389835.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="SPKLIP-Aligning-Spike-Video-Streams-with-Natural-Language"><a href="#SPKLIP-Aligning-Spike-Video-Streams-with-Natural-Language" class="headerlink" title="SPKLIP: Aligning Spike Video Streams with Natural Language"></a>SPKLIP: Aligning Spike Video Streams with Natural Language</h2><p><strong>Authors:Yongchang Gao, Meiling Jin, Zhaofei Yu, Tiejun Huang, Guozhang Chen</strong></p>
<p>Spike cameras offer unique sensing capabilities but their sparse, asynchronous output challenges semantic understanding, especially for Spike Video-Language Alignment (Spike-VLA) where models like CLIP underperform due to modality mismatch. We introduce SPKLIP, the first architecture specifically for Spike-VLA. SPKLIP employs a hierarchical spike feature extractor that adaptively models multi-scale temporal dynamics in event streams, and uses spike-text contrastive learning to directly align spike video with language, enabling effective few-shot learning. A full-spiking visual encoder variant, integrating SNN components into our pipeline, demonstrates enhanced energy efficiency. Experiments show state-of-the-art performance on benchmark spike datasets and strong few-shot generalization on a newly contributed real-world dataset. SPKLIPâ€™s energy efficiency highlights its potential for neuromorphic deployment, advancing event-based multimodal research. The source code and dataset are available at [link removed for anonymity]. </p>
<blockquote>
<p>è„‰å†²æ‘„åƒå¤´æä¾›äº†ç‹¬ç‰¹çš„æ„ŸçŸ¥èƒ½åŠ›ï¼Œä½†å…¶ç¨€ç–ã€å¼‚æ­¥çš„è¾“å‡ºç»™è¯­ä¹‰ç†è§£å¸¦æ¥äº†æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯å¯¹äºè„‰å†²è§†é¢‘è¯­è¨€å¯¹é½ï¼ˆSpike-VLAï¼‰è€Œè¨€ï¼ŒCLIPç­‰æ¨¡å‹ç”±äºæ¨¡æ€ä¸åŒ¹é…è€Œè¡¨ç°ä¸ä½³ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸“é—¨ç”¨äºSpike-VLAçš„SPKLIPæ¶æ„ã€‚SPKLIPé‡‡ç”¨åˆ†å±‚è„‰å†²ç‰¹å¾æå–å™¨ï¼Œè‡ªé€‚åº”åœ°æ¨¡æ‹Ÿäº‹ä»¶æµä¸­çš„å¤šå°ºåº¦æ—¶é—´åŠ¨æ€ï¼Œå¹¶ä½¿ç”¨è„‰å†²æ–‡æœ¬å¯¹æ¯”å­¦ä¹ æ¥ç›´æ¥å¯¹é½è„‰å†²è§†é¢‘å’Œè¯­è¨€ï¼Œä»è€Œå®ç°æœ‰æ•ˆçš„å°‘æ ·æœ¬å­¦ä¹ ã€‚ä¸€ç§å…¨è„‰å†²è§†è§‰ç¼–ç å™¨å˜ä½“å°†è„‰å†²ç¥ç»ç½‘ç»œï¼ˆSNNï¼‰ç»„ä»¶é›†æˆåˆ°æˆ‘ä»¬çš„æµç¨‹ä¸­ï¼Œå±•ç°äº†æ›´é«˜çš„èƒ½æ•ˆã€‚å®éªŒè¡¨æ˜åœ¨åŸºå‡†è„‰å†²æ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶åœ¨æ–°è´¡çŒ®çš„ç°å®æ•°æ®é›†ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„å°‘æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚SPKLIPçš„èƒ½æ•ˆçªæ˜¾äº†å…¶ç”¨äºç¥ç»å½¢æ€éƒ¨ç½²çš„æ½œåŠ›ï¼Œæ¨åŠ¨äº†åŸºäºäº‹ä»¶çš„å¤šåª’ä½“ç ”ç©¶çš„å‘å±•ã€‚æºä»£ç å’Œæ•°æ®é›†å¯é€šè¿‡[åŒ¿åé“¾æ¥]è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.12656v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>SPKLIPæ˜¯é’ˆå¯¹Spike-VLAä»»åŠ¡çš„é¦–ä¸ªæ¶æ„ï¼Œé‡‡ç”¨åˆ†å±‚è„‰å†²ç‰¹å¾æå–å™¨è‡ªé€‚åº”å»ºæ¨¡äº‹ä»¶æµä¸­çš„å¤šå°ºåº¦æ—¶é—´åŠ¨æ€ï¼Œå¹¶åˆ©ç”¨è„‰å†²æ–‡æœ¬å¯¹æ¯”å­¦ä¹ ç›´æ¥å¯¹é½è„‰å†²è§†é¢‘ä¸è¯­è¨€ï¼Œä»è€Œå®ç°æœ‰æ•ˆçš„å°‘æ ·æœ¬å­¦ä¹ ã€‚æ¨å‡ºå…¨è„‰å†²è§†è§‰ç¼–ç å™¨å˜ä½“ï¼Œå°†SNNç»„ä»¶é›†æˆåˆ°æˆ‘ä»¬çš„æµç¨‹ä¸­ï¼Œå±•ç°äº†å‡ºè‰²çš„èƒ½æºæ•ˆç‡ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¶æ„åœ¨åŸºå‡†è„‰å†²æ•°æ®é›†ä¸Šè¡¨ç°é¢†å…ˆï¼Œå¹¶åœ¨æ–°æ¨å‡ºçš„ç°å®æ•°æ®é›†ä¸Šå…·æœ‰è¾ƒå¼ºçš„å°‘æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SPKLIPæ˜¯ä¸“é—¨ä¸ºSpike-VLAè®¾è®¡çš„é¦–ä¸ªæ¶æ„ã€‚</li>
<li>é‡‡ç”¨åˆ†å±‚è„‰å†²ç‰¹å¾æå–å™¨è‡ªé€‚åº”å»ºæ¨¡äº‹ä»¶æµä¸­çš„å¤šå°ºåº¦æ—¶é—´åŠ¨æ€ã€‚</li>
<li>åˆ©ç”¨è„‰å†²æ–‡æœ¬å¯¹æ¯”å­¦ä¹ ç›´æ¥å¯¹é½è„‰å†²è§†é¢‘ä¸è¯­è¨€ï¼Œå®ç°æœ‰æ•ˆçš„å°‘æ ·æœ¬å­¦ä¹ ã€‚</li>
<li>å…¨è„‰å†²è§†è§‰ç¼–ç å™¨å˜ä½“é›†æˆäº†SNNç»„ä»¶ï¼Œæé«˜äº†èƒ½æºæ•ˆç‡ã€‚</li>
<li>åœ¨åŸºå‡†è„‰å†²æ•°æ®é›†ä¸Šè¡¨ç°é¢†å…ˆã€‚</li>
<li>åœ¨æ–°æ¨å‡ºçš„ç°å®æ•°æ®é›†ä¸Šå…·æœ‰è¾ƒå¼ºçš„å°‘æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.12656">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b720906c979e6418294c27e7d5db905c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e1b2de04e1396392b6fe71d56e208032.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e541bee5d5be2d1f227d87073a38d951.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a7ed586859797a3fedcaeeeac70401a7.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="LLM-based-Prompt-Ensemble-for-Reliable-Medical-Entity-Recognition-from-EHRs"><a href="#LLM-based-Prompt-Ensemble-for-Reliable-Medical-Entity-Recognition-from-EHRs" class="headerlink" title="LLM-based Prompt Ensemble for Reliable Medical Entity Recognition from   EHRs"></a>LLM-based Prompt Ensemble for Reliable Medical Entity Recognition from   EHRs</h2><p><strong>Authors:K M Sajjadul Islam, Ayesha Siddika Nipu, Jiawei Wu, Praveen Madiraju</strong></p>
<p>Electronic Health Records (EHRs) are digital records of patient information, often containing unstructured clinical text. Named Entity Recognition (NER) is essential in EHRs for extracting key medical entities like problems, tests, and treatments to support downstream clinical applications. This paper explores prompt-based medical entity recognition using large language models (LLMs), specifically GPT-4o and DeepSeek-R1, guided by various prompt engineering techniques, including zero-shot, few-shot, and an ensemble approach. Among all strategies, GPT-4o with prompt ensemble achieved the highest classification performance with an F1-score of 0.95 and recall of 0.98, outperforming DeepSeek-R1 on the task. The ensemble method improved reliability by aggregating outputs through embedding-based similarity and majority voting. </p>
<blockquote>
<p>ç”µå­å¥åº·è®°å½•ï¼ˆEHRsï¼‰æ˜¯æ‚£è€…ä¿¡æ¯çš„æ•°å­—è®°å½•ï¼Œé€šå¸¸åŒ…å«éç»“æ„çš„ä¸´åºŠæ–‡æœ¬ã€‚åœ¨EHRsä¸­ï¼Œå‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰å¯¹äºæå–å…³é”®åŒ»ç–—å®ä½“è‡³å…³é‡è¦ï¼Œå¦‚ç–¾ç—…ã€æ£€æŸ¥å’Œæ²»ç–—æ–¹æ³•ç­‰ï¼Œä¸ºä¸‹æ¸¸ä¸´åºŠåº”ç”¨æä¾›æ”¯æŒã€‚æœ¬æ–‡æ¢è®¨äº†åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„åŒ»ç–—å®ä½“è¯†åˆ«ï¼Œå°¤å…¶æ˜¯ä½¿ç”¨å„ç§æç¤ºå·¥ç¨‹æŠ€æœ¯æŒ‡å¯¼çš„GPT-4oå’ŒDeepSeek-R1ã€‚è¿™äº›æŠ€æœ¯åŒ…æ‹¬é›¶æ ·æœ¬ã€å°‘æ ·æœ¬å’Œé›†æˆæ–¹æ³•ã€‚åœ¨æ‰€æœ‰ç­–ç•¥ä¸­ï¼ŒGPT-4oä¸æç¤ºé›†æˆå–å¾—äº†æœ€é«˜çš„åˆ†ç±»æ€§èƒ½ï¼ŒF1åˆ†æ•°ä¸º0.95ï¼Œå¬å›ç‡ä¸º0.98ï¼Œåœ¨ä»»åŠ¡ä¸Šä¼˜äºDeepSeek-R1ã€‚é›†æˆæ–¹æ³•é€šè¿‡åŸºäºåµŒå…¥çš„ç›¸ä¼¼æ€§å’Œå¤šæ•°æŠ•ç¥¨æ³•èšåˆè¾“å‡ºï¼Œæé«˜äº†å¯é æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.08704v2">PDF</a> IEEE 26th International Conference on Information Reuse and   Integration for Data Science (IRI 2025), San Jose, CA, USA</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦æ¢è®¨äº†åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å‘½åå®ä½“è¯†åˆ«æŠ€æœ¯åœ¨ç”µå­å¥åº·è®°å½•ï¼ˆEHRsï¼‰ä¸­çš„åº”ç”¨ã€‚é€šè¿‡é‡‡ç”¨GPT-4oæ¨¡å‹å’ŒDeepSeek-R1æ¨¡å‹ï¼Œç»“åˆå¤šç§æç¤ºå·¥ç¨‹æŠ€æœ¯ï¼ˆå¦‚é›¶æ ·æœ¬ã€å°æ ·ä¾‹å’Œé›†æˆæ–¹æ³•ï¼‰ï¼Œè¿›è¡ŒåŒ»å­¦å®ä½“è¯†åˆ«çš„ç ”ç©¶ã€‚ç»“æœæ˜¾ç¤ºï¼ŒGPT-4oæ¨¡å‹é‡‡ç”¨æç¤ºé›†æˆç­–ç•¥åœ¨åˆ†ç±»æ€§èƒ½ä¸Šè¡¨ç°æœ€ä½³ï¼ŒF1åˆ†æ•°è¾¾åˆ°0.95ï¼Œå¬å›ç‡è¾¾åˆ°0.98ï¼Œä¼˜äºDeepSeek-R1æ¨¡å‹ã€‚é›†æˆæ–¹æ³•é€šè¿‡åŸºäºåµŒå…¥çš„ç›¸ä¼¼æ€§å’Œå¤šæ•°æŠ•ç¥¨æ³•èšåˆè¾“å‡ºï¼Œæé«˜äº†å¯é æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”µå­å¥åº·è®°å½•ï¼ˆEHRsï¼‰ä¸­çš„å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰å¯¹äºæå–å…³é”®åŒ»ç–—å®ä½“è‡³å…³é‡è¦ï¼Œå¦‚é—®é¢˜ã€æµ‹è¯•å’Œæ²»ç–—æ–¹æ³•ï¼Œä»¥æ”¯æŒä¸‹æ¸¸ä¸´åºŠåº”ç”¨ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨åŒ»å­¦å®ä½“è¯†åˆ«ä¸­å±•ç°å‡ºæ½œåŠ›ã€‚</li>
<li>GPT-4oæ¨¡å‹å’ŒDeepSeek-R1æ¨¡å‹è¢«ç”¨äºåŒ»å­¦å®ä½“è¯†åˆ«çš„ç ”ç©¶ã€‚</li>
<li>æç¤ºå·¥ç¨‹æŠ€æœ¯ï¼ŒåŒ…æ‹¬é›¶æ ·æœ¬ã€å°æ ·ä¾‹å’Œé›†æˆæ–¹æ³•ï¼Œåœ¨åŒ»å­¦å®ä½“è¯†åˆ«ä¸­å‘æŒ¥äº†ä½œç”¨ã€‚</li>
<li>GPT-4oæ¨¡å‹é‡‡ç”¨æç¤ºé›†æˆç­–ç•¥åœ¨åˆ†ç±»æ€§èƒ½ä¸Šè¡¨ç°æœ€ä½³ï¼ŒF1åˆ†æ•°å’Œå¬å›ç‡å‡è¾ƒé«˜ã€‚</li>
<li>é›†æˆæ–¹æ³•é€šè¿‡åŸºäºåµŒå…¥çš„ç›¸ä¼¼æ€§å’Œå¤šæ•°æŠ•ç¥¨æ³•æé«˜äº†å¯é æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.08704">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cf946109af6fd0062a644329ea718e8d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-629d509339aac6aaa5a0276c0c40c6b5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e61795bda67a7a018dcf609069aa2c3a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3921432b3d1e2a445e857c03a2917396.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ee8a0816c818be8f3d8ad48739197dc9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2ea5b898bc1b1646a2ede2a83cab71e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8f977aab0ec910fe8a2200e0df2a0b50.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dbb6368dff6e6695e00263ec08182ee1.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="CacheFL-Privacy-Preserving-and-Efficient-Federated-Cache-Model-Fine-Tuning-for-Vision-Language-Models"><a href="#CacheFL-Privacy-Preserving-and-Efficient-Federated-Cache-Model-Fine-Tuning-for-Vision-Language-Models" class="headerlink" title="CacheFL: Privacy-Preserving and Efficient Federated Cache Model   Fine-Tuning for Vision-Language Models"></a>CacheFL: Privacy-Preserving and Efficient Federated Cache Model   Fine-Tuning for Vision-Language Models</h2><p><strong>Authors:Mengjun Yi, Hanwen Zhang, Hui Dou, Jian Zhao, Furao Shen</strong></p>
<p>Large pre-trained Vision-Language Models (VLMs), such as Contrastive Language-Image Pre-training (CLIP), have exhibited remarkable zero-shot performance across various image classification tasks. Fine-tuning these models on domain-specific datasets further enhances their effectiveness for downstream applications. However, fine-tuning in cloud environments raises significant concerns regarding data security and privacy. Federated Learning (FL) offers a decentralized solution by enabling model training across local clients without centralizing sensitive data, but the high communication and computation costs of transmitting full pre-trained models during training limit its scalability. Additionally, non-Independent and Identically Distributed (non-IID) data across local clients can negatively impact model convergence and performance. To address these challenges, we propose CacheFL, a novel federated learning method that replaces traditional full model fine-tuning with lightweight cache model fine-tuning. The cache model is initialized using a class-balanced dataset generated by a generative pre-trained model, effectively mitigating the impact of non-IID data. This cache model is then distributed to local clients for fine-tuning, and the updated parameters from each client are aggregated on the server and redistributed. With the updated cache model, the classification performance of CLIP is improved after just a few epochs. By limiting the training and communication to the cache model, CacheFL significantly reduces resource demands while ensuring data privacy and security. Extensive experiments conducted on ImageNet and 10 additional datasets demonstrate that CacheFL outperforms traditional approaches in terms of classification accuracy, resource efficiency, and privacy preservation. </p>
<blockquote>
<p>å¤§å‹é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ï¼Œå¦‚å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰ï¼Œåœ¨å„ç§å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­å±•ç°å‡ºäº†å“è¶Šçš„é›¶æ ·æœ¬æ€§èƒ½ã€‚åœ¨ç‰¹å®šé¢†åŸŸæ•°æ®é›†ä¸Šå¯¹è¿™äº›æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå¯ä»¥è¿›ä¸€æ­¥æé«˜å…¶ä¸‹æ¸¸åº”ç”¨çš„æœ‰æ•ˆæ€§ã€‚ç„¶è€Œï¼Œåœ¨äº‘ç¯å¢ƒä¸­è¿›è¡Œå¾®è°ƒå¼•å‘äº†å…³äºæ•°æ®å®‰å…¨å’Œéšç§çš„é‡å¤§æ‹…å¿§ã€‚è”é‚¦å­¦ä¹ ï¼ˆFLï¼‰æä¾›äº†ä¸€ç§åˆ†æ•£çš„è§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡æ”¯æŒæœ¬åœ°å®¢æˆ·ç«¯çš„æ¨¡å‹è®­ç»ƒï¼Œæ— éœ€é›†ä¸­æ•æ„Ÿæ•°æ®ã€‚ç„¶è€Œï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¼ è¾“å®Œæ•´çš„é¢„è®­ç»ƒæ¨¡å‹çš„é«˜é€šä¿¡å’Œè®¡ç®—æˆæœ¬é™åˆ¶äº†å…¶å¯æ‰©å±•æ€§ã€‚æ­¤å¤–ï¼Œæœ¬åœ°å®¢æˆ·ç«¯ä¹‹é—´çš„éç‹¬ç«‹åŒåˆ†å¸ƒï¼ˆnon-IIDï¼‰æ•°æ®å¯èƒ½ä¼šå¯¹æ¨¡å‹æ”¶æ•›å’Œæ€§èƒ½äº§ç”Ÿè´Ÿé¢å½±å“ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†CacheFLï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„è”é‚¦å­¦ä¹ æ–¹æ³•ï¼Œå®ƒç”¨è½»é‡çº§çš„ç¼“å­˜æ¨¡å‹å¾®è°ƒå–ä»£äº†ä¼ ç»Ÿçš„å…¨æ¨¡å‹å¾®è°ƒã€‚ç¼“å­˜æ¨¡å‹ä½¿ç”¨ç”±ç”Ÿæˆå¼é¢„è®­ç»ƒæ¨¡å‹ç”Ÿæˆçš„ç±»åˆ«å¹³è¡¡æ•°æ®é›†è¿›è¡Œåˆå§‹åŒ–ï¼Œæœ‰æ•ˆåœ°å‡è½»äº†éIIDæ•°æ®çš„å½±å“ã€‚ç„¶åï¼Œè¯¥ç¼“å­˜æ¨¡å‹å°†åˆ†å‘ç»™æœ¬åœ°å®¢æˆ·ç«¯è¿›è¡Œå¾®è°ƒï¼Œä»æ¯ä¸ªå®¢æˆ·ç«¯æ›´æ–°çš„å‚æ•°å°†åœ¨æœåŠ¡å™¨ä¸Šèšåˆå¹¶é‡æ–°åˆ†å‘ã€‚ä»…é€šè¿‡å‡ ä¸ªå‘¨æœŸçš„è®­ç»ƒï¼ŒCLIPçš„åˆ†ç±»æ€§èƒ½å°±èƒ½é€šè¿‡æ›´æ–°çš„ç¼“å­˜æ¨¡å‹å¾—åˆ°æé«˜ã€‚é€šè¿‡å°†è®­ç»ƒå’Œé€šä¿¡é™åˆ¶åœ¨ç¼“å­˜æ¨¡å‹ä¸Šï¼ŒCacheFLåœ¨é™ä½èµ„æºéœ€æ±‚çš„åŒæ—¶ç¡®ä¿äº†æ•°æ®éšç§å’Œå®‰å…¨ã€‚åœ¨ImageNetå’Œå¦å¤–10ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒCacheFLåœ¨åˆ†ç±»ç²¾åº¦ã€èµ„æºæ•ˆç‡å’Œéšç§ä¿æŠ¤æ–¹é¢å‡ä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.05130v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ï¼Œå¦‚CLIPï¼Œåœ¨ä¸åŒå›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­è¡¨ç°å‡ºå‡ºè‰²çš„é›¶æ ·æœ¬æ€§èƒ½ã€‚é€šè¿‡é¢†åŸŸç‰¹å®šæ•°æ®é›†è¿›è¡Œå¾®è°ƒå¯æé«˜å…¶åœ¨ä¸‹æ¸¸åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ã€‚ä½†åœ¨äº‘ç¯å¢ƒä¸­è¿›è¡Œå¾®è°ƒå¼•å‘æ•°æ®å®‰å…¨å’Œéšç§çš„å¿§è™‘ã€‚è”é‚¦å­¦ä¹ ï¼ˆFLï¼‰é€šè¿‡æœ¬åœ°å®¢æˆ·ç«¯è¿›è¡Œæ¨¡å‹è®­ç»ƒè€Œä¸é›†ä¸­æ•æ„Ÿæ•°æ®æ¥æä¾›åˆ†æ•£è§£å†³æ–¹æ¡ˆï¼Œä½†ä¼ è¾“é¢„è®­ç»ƒæ¨¡å‹æ—¶çš„é€šä¿¡å’Œè®¡ç®—æˆæœ¬é™åˆ¶äº†å…¶å¯æ‰©å±•æ€§ã€‚æ­¤å¤–ï¼Œæœ¬åœ°å®¢æˆ·ç«¯çš„éç‹¬ç«‹åŒåˆ†å¸ƒï¼ˆnon-IIDï¼‰æ•°æ®å¯èƒ½ä¸åˆ©äºæ¨¡å‹æ”¶æ•›å’Œæ€§èƒ½ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºCacheFLè¿™ä¸€æ–°å‹è”é‚¦å­¦ä¹ æ–¹æ³•ï¼Œä»¥è½»é‡åŒ–ç¼“å­˜æ¨¡å‹å¾®è°ƒæ›¿ä»£ä¼ ç»Ÿå…¨æ¨¡å‹å¾®è°ƒã€‚ç¼“å­˜æ¨¡å‹ä½¿ç”¨ç”Ÿæˆå¼é¢„è®­ç»ƒæ¨¡å‹ç”Ÿæˆçš„ç±»åˆ«å¹³è¡¡æ•°æ®é›†è¿›è¡Œåˆå§‹åŒ–ï¼Œæœ‰æ•ˆå‡è½»éIIDæ•°æ®çš„å½±å“ã€‚ç¼“å­˜æ¨¡å‹éšååˆ†å‘ç»™æœ¬åœ°å®¢æˆ·ç«¯è¿›è¡Œå¾®è°ƒï¼Œä»æ¯ä¸ªå®¢æˆ·ç«¯æ›´æ–°çš„å‚æ•°åœ¨æœåŠ¡å™¨ä¸Šèšåˆåé‡æ–°åˆ†å‘ã€‚ä½¿ç”¨æ›´æ–°åçš„ç¼“å­˜æ¨¡å‹ï¼Œåªéœ€å‡ ä¸ªå‘¨æœŸå³å¯æé«˜CLIPçš„åˆ†ç±»æ€§èƒ½ã€‚é€šè¿‡é™åˆ¶è®­ç»ƒå’Œé€šä¿¡ä»…é™äºç¼“å­˜æ¨¡å‹ï¼ŒCacheFLåœ¨èµ„æºéœ€æ±‚æ–¹é¢å¤§å¤§å‡å°‘äº†è´Ÿæ‹…ï¼ŒåŒæ—¶ç¡®ä¿äº†æ•°æ®éšç§å’Œå®‰å…¨ã€‚åœ¨ImageNetå’Œå¦å¤–10ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒCacheFLåœ¨åˆ†ç±»ç²¾åº¦ã€èµ„æºæ•ˆç‡å’Œéšç§ä¿æŠ¤æ–¹é¢ä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å¦‚CLIPåœ¨å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†éœ€è¦é€šè¿‡å¾®è°ƒè¿›ä¸€æ­¥æé«˜æ€§èƒ½ã€‚</li>
<li>äº‘ç¯å¢ƒä¸­çš„æ¨¡å‹å¾®è°ƒå¼•å‘æ•°æ®å®‰å…¨å’Œéšç§çš„æ‹…å¿§ã€‚</li>
<li>è”é‚¦å­¦ä¹ ï¼ˆFLï¼‰æä¾›äº†ä¸€ç§è§£å†³æ–¹æ¡ˆï¼Œä½†åœ¨ä¼ è¾“å¤§å‹é¢„è®­ç»ƒæ¨¡å‹æ—¶é¢ä¸´é€šä¿¡å’Œè®¡ç®—æˆæœ¬é«˜çš„æŒ‘æˆ˜ã€‚</li>
<li>CacheFLæ˜¯ä¸€ç§æ–°å‹è”é‚¦å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡è½»é‡åŒ–ç¼“å­˜æ¨¡å‹å¾®è°ƒæ›¿ä»£ä¼ ç»Ÿå…¨æ¨¡å‹å¾®è°ƒï¼Œæé«˜äº†èµ„æºæ•ˆç‡å’Œéšç§ä¿æŠ¤ã€‚</li>
<li>CacheFLä½¿ç”¨ç”Ÿæˆå¼é¢„è®­ç»ƒæ¨¡å‹å’Œç±»åˆ«å¹³è¡¡æ•°æ®é›†åˆå§‹åŒ–ç¼“å­˜æ¨¡å‹ï¼Œæœ‰æ•ˆåº”å¯¹éIIDæ•°æ®æŒ‘æˆ˜ã€‚</li>
<li>ç¼“å­˜æ¨¡å‹çš„æ›´æ–°å‚æ•°é€šè¿‡èšåˆå’Œé‡æ–°åˆ†å‘ï¼Œä»…éœ€è¦å°‘é‡å‘¨æœŸå°±èƒ½æé«˜æ¨¡å‹çš„åˆ†ç±»æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.05130">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8664d15abb078cedbed770f84599277d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-237bbf25cb4db68595b57a9d0a6c18a1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f47d9372d408f932721d5b66875185a4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-be521c3dab6a8931677c8edfe47aada2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5a2bb12bb61e8d8a7a778ae341b94c56.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Identifying-Legal-Holdings-with-LLMs-A-Systematic-Study-of-Performance-Scale-and-Memorization"><a href="#Identifying-Legal-Holdings-with-LLMs-A-Systematic-Study-of-Performance-Scale-and-Memorization" class="headerlink" title="Identifying Legal Holdings with LLMs: A Systematic Study of Performance,   Scale, and Memorization"></a>Identifying Legal Holdings with LLMs: A Systematic Study of Performance,   Scale, and Memorization</h2><p><strong>Authors:Chuck Arvin</strong></p>
<p>As large language models (LLMs) continue to advance in capabilities, it is essential to assess how they perform on established benchmarks. In this study, we present a suite of experiments to assess the performance of modern LLMs (ranging from 3B to 90B+ parameters) on CaseHOLD, a legal benchmark dataset for identifying case holdings. Our experiments demonstrate scaling effects - performance on this task improves with model size, with more capable models like GPT4o and AmazonNovaPro achieving macro F1 scores of 0.744 and 0.720 respectively. These scores are competitive with the best published results on this dataset, and do not require any technically sophisticated model training, fine-tuning or few-shot prompting. To ensure that these strong results are not due to memorization of judicial opinions contained in the training data, we develop and utilize a novel citation anonymization test that preserves semantic meaning while ensuring case names and citations are fictitious. Models maintain strong performance under these conditions (macro F1 of 0.728), suggesting the performance is not due to rote memorization. These findings demonstrate both the promise and current limitations of LLMs for legal tasks with important implications for the development and measurement of automated legal analytics and legal benchmarks. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„èƒ½åŠ›ä¸æ–­æå‡ï¼Œè¯„ä¼°å®ƒä»¬åœ¨æ—¢å®šåŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°å˜å¾—è‡³å…³é‡è¦ã€‚æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬è¿›è¡Œäº†ä¸€ç³»åˆ—å®éªŒï¼Œæ—¨åœ¨è¯„ä¼°ç°ä»£LLMï¼ˆå‚æ•°èŒƒå›´ä»3Båˆ°90B+ï¼‰åœ¨CaseHOLDè¿™ä¸€æ³•å¾‹åŸºå‡†æ•°æ®é›†ä¸Šçš„è¡¨ç°ï¼Œè¯¥æ•°æ®é›†ç”¨äºè¯†åˆ«åˆ¤ä¾‹æ³•æ‘˜è¦ã€‚æˆ‘ä»¬çš„å®éªŒå±•ç¤ºäº†è§„æ¨¡æ•ˆç›Šâ€”â€”éšç€æ¨¡å‹è§„æ¨¡çš„æ‰©å¤§ï¼Œæ­¤ä»»åŠ¡çš„æ€§èƒ½å¾—åˆ°æå‡ã€‚æ›´å…ˆè¿›çš„æ¨¡å‹å¦‚GPT4oå’ŒAmazonNovaProåˆ†åˆ«è·å¾—äº†0.744å’Œ0.720çš„å®è§‚F1åˆ†æ•°ã€‚è¿™äº›åˆ†æ•°ä¸æ•°æ®é›†ä¸Šçš„æœ€ä½³å·²å‘å¸ƒç»“æœå…·æœ‰ç«äº‰åŠ›ï¼Œå¹¶ä¸”ä¸éœ€è¦ä»»ä½•æŠ€æœ¯å¤æ‚æ¨¡å‹çš„è®­ç»ƒã€å¾®è°ƒæˆ–å°‘æ ·æœ¬æç¤ºã€‚ä¸ºç¡®ä¿è¿™äº›å¼ºæœ‰åŠ›çš„ç»“æœå¹¶éç”±äºè®­ç»ƒæ•°æ®ä¸­å¸æ³•æ„è§çš„æœºæ¢°åŒ–è®°å¿†ï¼Œæˆ‘ä»¬å¼€å‘å¹¶åˆ©ç”¨äº†ä¸€ç§æ–°å‹å¼•æ–‡åŒ¿åæµ‹è¯•ï¼Œè¯¥æµ‹è¯•åœ¨ä¿ç•™è¯­ä¹‰çš„åŒæ—¶ç¡®ä¿æ¡ˆä»¶åç§°å’Œå¼•æ–‡æ˜¯è™šæ„çš„ã€‚åœ¨è¿™äº›æ¡ä»¶ä¸‹ï¼Œæ¨¡å‹ä¿æŒå¼ºå¤§çš„æ€§èƒ½ï¼ˆå®è§‚F1ä¸º0.728ï¼‰ï¼Œè¡¨æ˜å…¶è¡¨ç°å¹¶éç”±äºæ­»è®°ç¡¬èƒŒã€‚è¿™äº›å‘ç°å±•ç¤ºäº†LLMåœ¨æ³•å¾‹ä»»åŠ¡ä¸Šçš„æ½œåŠ›ä»¥åŠå½“å‰å­˜åœ¨çš„å±€é™æ€§ï¼Œå¯¹äºè‡ªåŠ¨æ³•å¾‹åˆ†æå’Œæ³•å¾‹åŸºå‡†çš„å¼€å‘å’Œè¡¡é‡å…·æœ‰é‡è¦æ„ä¹‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.02172v3">PDF</a> Presented as a short paper at International Conference on Artificial   Intelligence and Law 2025 (Chicago, IL)</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨èƒ½åŠ›ä¸Šä¸æ–­è¿›æ­¥ï¼Œæœ¬ç ”ç©¶é€šè¿‡ä¸€ç³»åˆ—å®éªŒè¯„ä¼°äº†ç°ä»£LLMsåœ¨CaseHOLDæ³•å¾‹åŸºå‡†æ•°æ®é›†ä¸Šçš„è¡¨ç°ï¼Œç”¨äºè¯†åˆ«æ¡ˆä¾‹æŒæœ‰ã€‚å®éªŒè¡¨æ˜ï¼Œæ¨¡å‹è§„æ¨¡æ‰©å¤§æœ‰åŠ©äºæé«˜ä»»åŠ¡æ€§èƒ½ï¼ŒGPT4oå’ŒAmazonNovaProç­‰æ›´å…ˆè¿›çš„æ¨¡å‹å–å¾—äº†æœ‰ç«äº‰åŠ›çš„æˆç»©ã€‚æ­¤å¤–ï¼Œæ¨¡å‹è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œå³ä½¿ç»è¿‡æ–°å‹å¼•æ–‡åŒ¿ååŒ–æµ‹è¯•ä¹Ÿä¾ç„¶å¦‚æ­¤ã€‚è¿™ä¸ºæ³•å¾‹ä»»åŠ¡çš„è‡ªåŠ¨åŒ–åˆ†æå’ŒåŸºå‡†æµ‹è¯•æä¾›äº†é‡è¦å¯ç¤ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨CaseHOLDæ³•å¾‹åŸºå‡†æ•°æ®é›†ä¸Šçš„æ€§èƒ½è¯„ä¼°å¾ˆé‡è¦ã€‚</li>
<li>æ¨¡å‹è§„æ¨¡æ‰©å¤§æœ‰åŠ©äºæé«˜ä»»åŠ¡æ€§èƒ½ã€‚</li>
<li>GPT4oå’ŒAmazonNovaProç­‰å…ˆè¿›æ¨¡å‹åœ¨CaseHOLDæ•°æ®é›†ä¸Šå–å¾—äº†æœ‰ç«äº‰åŠ›çš„æˆç»©ã€‚</li>
<li>æ¨¡å‹è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œå³ä½¿ç»è¿‡æ–°å‹å¼•æ–‡åŒ¿ååŒ–æµ‹è¯•ä¹Ÿä¾ç„¶ç¨³å¥ã€‚</li>
<li>æ¨¡å‹æ€§èƒ½å¹¶éä»…ä»…å› ä¸ºå¯¹è®­ç»ƒæ•°æ®ä¸­å¸æ³•æ„è§çš„åˆ»æ¿è®°å¿†ã€‚</li>
<li>LLMsåœ¨æ³•å¾‹ä»»åŠ¡ä¸Šçš„æ½œåŠ›å’Œå½“å‰å±€é™æ€§å¾—åˆ°äº†å±•ç¤ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.02172">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-4bfd996a19054604a9107c691781067e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-71bc7606223dcc74f30def7c8527a088.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-75e2103642a9da7683094663aaf68369.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-959dfe168ca066c99cb2d84366311d6a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7a129b5a293125fa43d20f74c4504e61.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Probabilistic-Interactive-3D-Segmentation-with-Hierarchical-Neural-Processes"><a href="#Probabilistic-Interactive-3D-Segmentation-with-Hierarchical-Neural-Processes" class="headerlink" title="Probabilistic Interactive 3D Segmentation with Hierarchical Neural   Processes"></a>Probabilistic Interactive 3D Segmentation with Hierarchical Neural   Processes</h2><p><strong>Authors:Jie Liu, Pan Zhou, Zehao Xiao, Jiayi Shen, Wenzhe Yin, Jan-Jakob Sonke, Efstratios Gavves</strong></p>
<p>Interactive 3D segmentation has emerged as a promising solution for generating accurate object masks in complex 3D scenes by incorporating user-provided clicks. However, two critical challenges remain underexplored: (1) effectively generalizing from sparse user clicks to produce accurate segmentation, and (2) quantifying predictive uncertainty to help users identify unreliable regions. In this work, we propose NPISeg3D, a novel probabilistic framework that builds upon Neural Processes (NPs) to address these challenges. Specifically, NPISeg3D introduces a hierarchical latent variable structure with scene-specific and object-specific latent variables to enhance few-shot generalization by capturing both global context and object-specific characteristics. Additionally, we design a probabilistic prototype modulator that adaptively modulates click prototypes with object-specific latent variables, improving the modelâ€™s ability to capture object-aware context and quantify predictive uncertainty. Experiments on four 3D point cloud datasets demonstrate that NPISeg3D achieves superior segmentation performance with fewer clicks while providing reliable uncertainty estimations. </p>
<blockquote>
<p>äº¤äº’å¼3Dåˆ†å‰²æŠ€æœ¯é€šè¿‡ç»“åˆç”¨æˆ·æä¾›çš„ç‚¹å‡»ï¼Œåœ¨å¤æ‚çš„3Dåœºæ™¯ä¸­ç”Ÿæˆç²¾ç¡®çš„å¯¹è±¡æ©è†œæ–¹é¢å±•ç°å‡ºå·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œè¿˜æœ‰ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ï¼šï¼ˆ1ï¼‰å¦‚ä½•ä»ç¨€ç–çš„ç”¨æˆ·ç‚¹å‡»ä¸­æœ‰æ•ˆæ¦‚æ‹¬å‡ºå‡†ç¡®çš„åˆ†å‰²ç»“æœï¼›ï¼ˆ2ï¼‰é‡åŒ–é¢„æµ‹ä¸ç¡®å®šæ€§ï¼Œå¸®åŠ©ç”¨æˆ·è¯†åˆ«ä¸å¯é çš„åŒºåŸŸã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†NPISeg3Dï¼Œä¸€ä¸ªåŸºäºç¥ç»ç½‘ç»œè¿‡ç¨‹ï¼ˆNPsï¼‰çš„æ–°å‹æ¦‚ç‡æ¡†æ¶ï¼Œä»¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚å…·ä½“æ¥è¯´ï¼ŒNPISeg3Då¼•å…¥äº†ä¸€ç§åˆ†å±‚æ½œåœ¨å˜é‡ç»“æ„ï¼ŒåŒ…æ‹¬åœºæ™¯ç‰¹å®šå’Œå¯¹è±¡ç‰¹å®šçš„æ½œåœ¨å˜é‡ï¼Œé€šè¿‡æ•æ‰å…¨å±€ä¸Šä¸‹æ–‡å’Œå¯¹è±¡ç‰¹å®šç‰¹å¾ï¼Œå¢å¼ºå°æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªæ¦‚ç‡åŸå‹è°ƒåˆ¶å™¨ï¼Œå®ƒè‡ªé€‚åº”åœ°åˆ©ç”¨å¯¹è±¡ç‰¹å®šçš„æ½œåœ¨å˜é‡è°ƒæ•´ç‚¹å‡»åŸå‹ï¼Œæé«˜äº†æ¨¡å‹æ•æ‰å¯¹è±¡æ„ŸçŸ¥ä¸Šä¸‹æ–‡å’Œé‡åŒ–é¢„æµ‹ä¸ç¡®å®šæ€§çš„èƒ½åŠ›ã€‚åœ¨å››ä¸ª3Dç‚¹äº‘æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒNPISeg3Dåœ¨ç‚¹å‡»æ¬¡æ•°æ›´å°‘çš„æƒ…å†µä¸‹å®ç°äº†ä¼˜è¶Šçš„åˆ†å‰²æ€§èƒ½ï¼ŒåŒæ—¶æä¾›äº†å¯é çš„ä¸ç¡®å®šæ€§ä¼°è®¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.01726v2">PDF</a> ICML 2025 Proceedings</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æ¢è®¨äº†åœ¨å¤æ‚ä¸‰ç»´åœºæ™¯ä¸­è¿›è¡Œäº¤äº’å¼ä¸‰ç»´åˆ†å‰²æ—¶é¢ä¸´çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯ä»ç¨€ç–ç”¨æˆ·ç‚¹å‡»æ¨å¹¿åˆ°å‡†ç¡®åˆ†å‰²ä»¥åŠé‡åŒ–é¢„æµ‹ä¸ç¡®å®šæ€§ä¸¤ä¸ªå…³é”®é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†ä¸€ç§åŸºäºç¥ç»è¿‡ç¨‹çš„æ¦‚ç‡æ¡†æ¶NPISeg3Dï¼Œé€šè¿‡å¼•å…¥å±‚æ¬¡åŒ–æ½œåœ¨å˜é‡ç»“æ„å’Œæ¦‚ç‡åŸå‹è°ƒåˆ¶å™¨ï¼Œæé«˜å°‘æ ·æœ¬æ¨å¹¿èƒ½åŠ›å’Œä¸ç¡®å®šæ€§é‡åŒ–èƒ½åŠ›ã€‚å®éªŒè¯æ˜ï¼ŒNPISeg3Dåœ¨å››ä¸ªä¸‰ç»´ç‚¹äº‘æ•°æ®é›†ä¸Šå®ç°äº†æ›´å°‘çš„ç‚¹å‡»æ¬¡æ•°å³å¯è·å¾—ä¼˜è¶Šçš„åˆ†éš”æ€§èƒ½ï¼ŒåŒæ—¶æä¾›äº†å¯é çš„ä¸ç¡®å®šæ€§ä¼°è®¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>äº¤äº’å¼ä¸‰ç»´åˆ†å‰²æ˜¯ç”Ÿæˆå¤æ‚ä¸‰ç»´åœºæ™¯ä¸­å‡†ç¡®å¯¹è±¡æ©æ¨¡çš„æœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>ä»ç¨€ç–ç”¨æˆ·ç‚¹å‡»æ¨å¹¿åˆ°å‡†ç¡®åˆ†å‰²å’Œé‡åŒ–é¢„æµ‹ä¸ç¡®å®šæ€§æ˜¯å°šæœªå……åˆ†ç ”ç©¶çš„æŒ‘æˆ˜ã€‚</li>
<li>NPISeg3Dæ˜¯ä¸€ç§åŸºäºç¥ç»è¿‡ç¨‹çš„æ¦‚ç‡æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è¿™ä¸¤ä¸ªæŒ‘æˆ˜ã€‚</li>
<li>NPISeg3Dé€šè¿‡å¼•å…¥å±‚æ¬¡åŒ–æ½œåœ¨å˜é‡ç»“æ„ï¼Œå¢å¼ºäº†å¯¹å…¨å±€ä¸Šä¸‹æ–‡å’Œå¯¹è±¡ç‰¹å®šç‰¹å¾çš„æ•æ‰èƒ½åŠ›ã€‚</li>
<li>æ¦‚ç‡åŸå‹è°ƒåˆ¶å™¨çš„è®¾è®¡èƒ½å¤Ÿè‡ªé€‚åº”åœ°è°ƒèŠ‚ç‚¹å‡»åŸå‹ä¸å¯¹è±¡ç‰¹å®šæ½œåœ¨å˜é‡ï¼Œæé«˜æ¨¡å‹æ•æ‰å¯¹è±¡æ„ŸçŸ¥ä¸Šä¸‹æ–‡å’Œé‡åŒ–é¢„æµ‹ä¸ç¡®å®šæ€§çš„èƒ½åŠ›ã€‚</li>
<li>å®éªŒè¯æ˜NPISeg3Dåœ¨å››ä¸ªä¸‰ç»´ç‚¹äº‘æ•°æ®é›†ä¸Šå®ç°äº†ä¼˜è¶Šçš„åˆ†éš”æ€§èƒ½ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.01726">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3498888392d09f36db7959d498544a4b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8691076e0be62e1bc1629c4624cfb469.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6fac8d588025dd44bcba0b2dbcd1c55f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-73706838dc030343b05743f08730f685.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Single-Agent-vs-Multi-Agent-LLM-Strategies-for-Automated-Student-Reflection-Assessment"><a href="#Single-Agent-vs-Multi-Agent-LLM-Strategies-for-Automated-Student-Reflection-Assessment" class="headerlink" title="Single-Agent vs. Multi-Agent LLM Strategies for Automated Student   Reflection Assessment"></a>Single-Agent vs. Multi-Agent LLM Strategies for Automated Student   Reflection Assessment</h2><p><strong>Authors:Gen Li, Li Chen, Cheng Tang, Valdemar Å vÃ¡benskÃ½, Daisuke Deguchi, Takayoshi Yamashita, Atsushi Shimada</strong></p>
<p>We explore the use of Large Language Models (LLMs) for automated assessment of open-text student reflections and prediction of academic performance. Traditional methods for evaluating reflections are time-consuming and may not scale effectively in educational settings. In this work, we employ LLMs to transform student reflections into quantitative scores using two assessment strategies (single-agent and multi-agent) and two prompting techniques (zero-shot and few-shot). Our experiments, conducted on a dataset of 5,278 reflections from 377 students over three academic terms, demonstrate that the single-agent with few-shot strategy achieves the highest match rate with human evaluations. Furthermore, models utilizing LLM-assessed reflection scores outperform baselines in both at-risk student identification and grade prediction tasks. These findings suggest that LLMs can effectively automate reflection assessment, reduce educatorsâ€™ workload, and enable timely support for students who may need additional assistance. Our work emphasizes the potential of integrating advanced generative AI technologies into educational practices to enhance student engagement and academic success. </p>
<blockquote>
<p>æˆ‘ä»¬æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªåŠ¨è¯„ä¼°å­¦ç”Ÿå¼€æ”¾æ€§åæ€å’Œé¢„æµ‹å­¦ä¸šè¡¨ç°æ–¹é¢çš„åº”ç”¨ã€‚ä¼ ç»Ÿçš„åæ€è¯„ä¼°æ–¹æ³•è€—æ—¶ä¸”åœ¨æ•™è‚²ç¯å¢ƒä¸­å¯èƒ½æ— æ³•æœ‰æ•ˆæ‰©å±•ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é‡‡ç”¨LLMï¼Œä½¿ç”¨ä¸¤ç§è¯„ä¼°ç­–ç•¥ï¼ˆå•æ™ºèƒ½ä½“å’Œå¤šæ™ºèƒ½ä½“ï¼‰å’Œä¸¤ç§æç¤ºæŠ€æœ¯ï¼ˆé›¶æ ·æœ¬å’Œå°‘æ ·æœ¬ï¼‰ï¼Œå°†å­¦ç”Ÿåæ€è½¬åŒ–ä¸ºé‡åŒ–åˆ†æ•°ã€‚æˆ‘ä»¬åœ¨åŒ…å«æ¥è‡ª377åå­¦ç”Ÿåœ¨ä¸‰ä¸ªå­¦æœ¯å­¦æœŸå†…å…±5,278ç¯‡åæ€çš„æ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œé‡‡ç”¨å°‘æ ·æœ¬ç­–ç•¥çš„å•æ™ºèƒ½ä½“åœ¨äººä¸ºè¯„ä¼°æ–¹é¢çš„åŒ¹é…ç‡æœ€é«˜ã€‚æ­¤å¤–ï¼Œåˆ©ç”¨LLMè¯„ä¼°çš„åæ€å¾—åˆ†çš„æ¨¡å‹åœ¨å¤„äºå±é™©çš„å­¦ç”Ÿè¯†åˆ«å’Œæˆç»©é¢„æµ‹ä»»åŠ¡ä¸­çš„è¡¨ç°å‡ä¼˜äºåŸºçº¿ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼ŒLLMå¯ä»¥æœ‰æ•ˆåœ°è‡ªåŠ¨è¿›è¡Œåæ€è¯„ä¼°ï¼Œå‡å°‘æ•™è‚²å·¥ä½œè€…çš„å·¥ä½œé‡ï¼Œå¹¶ä¸ºå¯èƒ½éœ€è¦é¢å¤–å¸®åŠ©çš„å­¦ç”Ÿæä¾›åŠæ—¶çš„æ”¯æŒã€‚æˆ‘ä»¬çš„å·¥ä½œå¼ºè°ƒäº†å°†å…ˆè¿›çš„ç”Ÿæˆå¼AIæŠ€æœ¯èå…¥æ•™è‚²å®è·µï¼Œä»¥æé«˜å­¦ç”Ÿå‚ä¸åº¦å’Œå­¦ä¸šæˆå°±çš„å¯èƒ½æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05716v2">PDF</a> Published in Proceedings of the 29th Pacific-Asia Conference on   Knowledge Discovery and Data Mining (PAKDD 2025)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è‡ªåŠ¨è¯„ä¼°å­¦ç”Ÿå¼€æ”¾æ–‡æœ¬åæ€å’Œé¢„æµ‹å­¦ä¸šæˆç»©çš„æ–¹æ³•ã€‚ä¼ ç»Ÿè¯„ä¼°åæ€çš„æ–¹æ³•è€—æ—¶ä¸”éš¾ä»¥åœ¨æ•™è‚²ç¯å¢ƒä¸­æœ‰æ•ˆæ‰©å±•ã€‚æœ¬ç ”ç©¶åˆ©ç”¨LLMså°†å­¦ç”Ÿçš„åæ€è½¬åŒ–ä¸ºé‡åŒ–åˆ†æ•°ï¼Œé‡‡ç”¨ä¸¤ç§è¯„ä¼°ç­–ç•¥ï¼ˆå•ä»£ç†å’Œå¤šä»£ç†ï¼‰å’Œä¸¤ç§æç¤ºæŠ€æœ¯ï¼ˆé›¶å°„å‡»å’Œå°‘å°„å‡»ï¼‰ã€‚åœ¨æ¥è‡ª377åå­¦ç”Ÿä¸‰ä¸ªå­¦æœŸçš„5278ç¯‡åæ€æ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œé‡‡ç”¨å°‘å°„å‡»ç­–ç•¥çš„å•ä»£ç†æ–¹æ³•ä¸äººç±»è¯„ä»·çš„åŒ¹é…åº¦æœ€é«˜ã€‚æ­¤å¤–ï¼Œåˆ©ç”¨LLMè¯„ä¼°çš„åæ€åˆ†æ•°çš„æ¨¡å‹åœ¨é£é™©å­¦ç”Ÿè¯†åˆ«å’Œæˆç»©é¢„æµ‹ä»»åŠ¡ä¸­çš„è¡¨ç°å‡ä¼˜äºåŸºçº¿ã€‚è¿™è¡¨æ˜LLMså¯ä»¥æœ‰æ•ˆè‡ªåŠ¨åŒ–åæ€è¯„ä¼°ï¼Œå‡è½»æ•™è‚²å·¥ä½œè€…çš„å·¥ä½œé‡ï¼Œå¹¶ä¸ºå¯èƒ½éœ€è¦é¢å¤–å¸®åŠ©çš„å­¦ç”Ÿæä¾›åŠæ—¶çš„æ”¯æŒã€‚æœ¬ç ”ç©¶å¼ºè°ƒäº†å°†å…ˆè¿›çš„ç”Ÿæˆæ€§AIæŠ€æœ¯èå…¥æ•™è‚²å®è·µä»¥æé«˜å­¦ç”Ÿå‚ä¸åº¦å’Œå­¦ä¸šæˆåŠŸçš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯ç”¨äºè‡ªåŠ¨è¯„ä¼°å­¦ç”Ÿçš„å¼€æ”¾æ–‡æœ¬åæ€å’Œé¢„æµ‹å­¦ä¸šæˆç»©ã€‚</li>
<li>ä¼ ç»Ÿè¯„ä¼°åæ€çš„æ–¹æ³•å­˜åœ¨è€—æ—¶ä¸”éš¾ä»¥æœ‰æ•ˆæ‰©å±•çš„é—®é¢˜ã€‚</li>
<li>é‡‡ç”¨å•ä»£ç†å’Œå°‘å°„å‡»ç­–ç•¥çš„LLMåœ¨åŒ¹é…äººç±»è¯„ä»·æ–¹é¢è¡¨ç°æœ€ä½³ã€‚</li>
<li>LLMsåœ¨é£é™©å­¦ç”Ÿè¯†åˆ«å’Œæˆç»©é¢„æµ‹ä»»åŠ¡ä¸­çš„è¡¨ç°ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚</li>
<li>LLMsèƒ½æœ‰æ•ˆè‡ªåŠ¨åŒ–åæ€è¯„ä¼°ï¼Œå‡è½»æ•™å¸ˆçš„å·¥ä½œè´Ÿæ‹…ã€‚</li>
<li>LLMså¯ä¸ºå­¦ç”Ÿæä¾›åŠæ—¶çš„æ”¯æŒï¼Œç‰¹åˆ«æ˜¯é‚£äº›å¯èƒ½éœ€è¦é¢å¤–å¸®åŠ©çš„å­¦ç”Ÿã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05716">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-741679233d151bf4b93641dce6c81b5b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9f36f8bc2ee799fda8b05d31568d376e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a1e58fb5bda2006bfef3be73859d505f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0ff0799879c85a715d451fa0ee35b3a8.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Time-VLM-Exploring-Multimodal-Vision-Language-Models-for-Augmented-Time-Series-Forecasting"><a href="#Time-VLM-Exploring-Multimodal-Vision-Language-Models-for-Augmented-Time-Series-Forecasting" class="headerlink" title="Time-VLM: Exploring Multimodal Vision-Language Models for Augmented Time   Series Forecasting"></a>Time-VLM: Exploring Multimodal Vision-Language Models for Augmented Time   Series Forecasting</h2><p><strong>Authors:Siru Zhong, Weilin Ruan, Ming Jin, Huan Li, Qingsong Wen, Yuxuan Liang</strong></p>
<p>Recent advancements in time series forecasting have explored augmenting models with text or vision modalities to improve accuracy. While text provides contextual understanding, it often lacks fine-grained temporal details. Conversely, vision captures intricate temporal patterns but lacks semantic context, limiting the complementary potential of these modalities. To address this, we propose \method, a novel multimodal framework that leverages pre-trained Vision-Language Models (VLMs) to bridge temporal, visual, and textual modalities for enhanced forecasting. Our framework comprises three key components: (1) a Retrieval-Augmented Learner, which extracts enriched temporal features through memory bank interactions; (2) a Vision-Augmented Learner, which encodes time series as informative images; and (3) a Text-Augmented Learner, which generates contextual textual descriptions. These components collaborate with frozen pre-trained VLMs to produce multimodal embeddings, which are then fused with temporal features for final prediction. Extensive experiments demonstrate that Time-VLM achieves superior performance, particularly in few-shot and zero-shot scenarios, thereby establishing a new direction for multimodal time series forecasting. Code is available at <a target="_blank" rel="noopener" href="https://github.com/CityMind-Lab/ICML25-TimeVLM">https://github.com/CityMind-Lab/ICML25-TimeVLM</a>. </p>
<blockquote>
<p>è¿‘æœŸæ—¶é—´åºåˆ—é¢„æµ‹é¢†åŸŸçš„è¿›å±•æ¢ç´¢äº†é€šè¿‡æ·»åŠ æ–‡æœ¬æˆ–è§†è§‰æ¨¡å¼æ¥å¢å¼ºæ¨¡å‹ä»¥æé«˜é¢„æµ‹ç²¾åº¦çš„æ–¹æ³•ã€‚è™½ç„¶æ–‡æœ¬æä¾›äº†ä¸Šä¸‹æ–‡ç†è§£ï¼Œä½†å®ƒå¸¸å¸¸ç¼ºä¹ç²¾ç»†çš„æ—¶é—´ç»†èŠ‚ã€‚ç›¸åï¼Œè§†è§‰æ¨¡å¼æ•æ‰åˆ°äº†å¤æ‚çš„æ—¶é—´æ¨¡å¼ï¼Œä½†ç¼ºä¹è¯­ä¹‰ä¸Šä¸‹æ–‡ï¼Œé™åˆ¶äº†è¿™äº›æ¨¡å¼çš„äº’è¡¥æ½œåŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†\methodï¼ˆæ–¹æ³•ï¼‰è¿™ä¸€æ–°å‹å¤šæ¨¡å¼æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰æ¥æ¡¥æ¥æ—¶é—´ã€è§†è§‰å’Œæ–‡æœ¬æ¨¡å¼ä»¥å¢å¼ºé¢„æµ‹èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ¡†æ¶åŒ…å«ä¸‰ä¸ªå…³é”®ç»„æˆéƒ¨åˆ†ï¼šï¼ˆ1ï¼‰æ£€ç´¢å¢å¼ºå­¦ä¹ è€…ï¼Œå®ƒé€šè¿‡å†…å­˜é“¶è¡Œäº’åŠ¨æå–ä¸°å¯Œçš„æ—¶é—´ç‰¹å¾ï¼›ï¼ˆ2ï¼‰è§†è§‰å¢å¼ºå­¦ä¹ è€…ï¼Œå®ƒå°†æ—¶é—´åºåˆ—ç¼–ç ä¸ºä¿¡æ¯å›¾åƒï¼›ï¼ˆ3ï¼‰æ–‡æœ¬å¢å¼ºå­¦ä¹ è€…ï¼Œå®ƒç”Ÿæˆä¸Šä¸‹æ–‡æ–‡æœ¬æè¿°ã€‚è¿™ä¸‰ä¸ªç»„æˆéƒ¨åˆ†ä¸å†»ç»“çš„é¢„è®­ç»ƒVLMsåä½œï¼Œç”Ÿæˆå¤šæ¨¡å¼åµŒå…¥ï¼Œç„¶åå…¶ä¸æ—¶é—´ç‰¹å¾èåˆä»¥è¿›è¡Œæœ€ç»ˆé¢„æµ‹ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒTime-VLMï¼ˆæ—¶é—´-è§†è§‰è¯­è¨€æ¨¡å‹ï¼‰åœ¨å°‘æ ·æœ¬å’Œæ— æ ·æœ¬åœºæ™¯ä¸‹å®ç°äº†å“è¶Šçš„æ€§èƒ½ï¼Œä¸ºå¤šåª’ä½“æ—¶é—´åºåˆ—é¢„æµ‹å¼€è¾Ÿäº†æ–°çš„æ–¹å‘ã€‚ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/CityMind-Lab/ICML25-TimeVLM%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/CityMind-Lab/ICML25-TimeVLMè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.04395v2">PDF</a> 20 pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹æ—¶é—´åºåˆ—é¢„æµ‹çš„æ–°è¿›å±•ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„å¤šæ¨¡æ€æ¡†æ¶Time-VLMï¼Œè¯¥æ¡†æ¶ç»“åˆäº†é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰æ¥èåˆæ—¶é—´ã€è§†è§‰å’Œæ–‡æœ¬æ¨¡æ€ï¼Œä»¥æé«˜é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šæ£€ç´¢å¢å¼ºå­¦ä¹ è€…ã€è§†è§‰å¢å¼ºå­¦ä¹ è€…å’Œæ–‡æœ¬å¢å¼ºå­¦ä¹ è€…ã€‚å®éªŒè¯æ˜ï¼ŒTime-VLMåœ¨å°‘æ ·æœ¬å’Œæ— æ ·æœ¬åœºæ™¯ä¸‹å…·æœ‰å“è¶Šæ€§èƒ½ï¼Œä¸ºå¤šåª’ä½“æ—¶é—´åºåˆ—é¢„æµ‹æä¾›äº†æ–°çš„æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬æä¾›ä¸Šä¸‹æ–‡ç†è§£ä½†å¯èƒ½ç¼ºä¹ç²¾ç»†çš„æ—¶é—´ç»†èŠ‚ï¼Œè€Œè§†è§‰æ•æ‰å¤æ‚çš„æ—¶é—´æ¨¡å¼ä½†ç¼ºä¹è¯­ä¹‰ä¸Šä¸‹æ–‡ã€‚</li>
<li>Time-VLMæ¡†æ¶åˆ©ç”¨é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰æ¥èåˆæ—¶é—´ã€è§†è§‰å’Œæ–‡æœ¬æ¨¡æ€ã€‚</li>
<li>Time-VLMæ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šæ£€ç´¢å¢å¼ºå­¦ä¹ è€…ã€è§†è§‰å¢å¼ºå­¦ä¹ è€…å’Œæ–‡æœ¬å¢å¼ºå­¦ä¹ è€…ã€‚</li>
<li>æ£€ç´¢å¢å¼ºå­¦ä¹ è€…é€šè¿‡å†…å­˜é“¶è¡Œäº¤äº’æå–ä¸°å¯Œçš„æ—¶é—´ç‰¹å¾ã€‚</li>
<li>è§†è§‰å¢å¼ºå­¦ä¹ è€…å°†æ—¶é—´åºåˆ—ç¼–ç ä¸ºä¿¡æ¯å›¾åƒã€‚</li>
<li>æ–‡æœ¬å¢å¼ºå­¦ä¹ è€…ç”Ÿæˆä¸Šä¸‹æ–‡æ–‡æœ¬æè¿°ã€‚</li>
<li>Time-VLMæ¡†æ¶åœ¨å°‘æ ·æœ¬å’Œæ— æ ·æœ¬åœºæ™¯ä¸‹çš„é¢„æµ‹æ€§èƒ½å“è¶Šï¼Œä¸ºå¤šåª’ä½“æ—¶é—´åºåˆ—é¢„æµ‹æä¾›äº†æ–°çš„æ–¹å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.04395">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-488dd4c73e871e954245cddc5b225e3e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c2e08b0b3a07809d91ee2b85263c5e9d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c88a486fa0d42512233734cf7eea5f24.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Are-Transformers-Able-to-Reason-by-Connecting-Separated-Knowledge-in-Training-Data"><a href="#Are-Transformers-Able-to-Reason-by-Connecting-Separated-Knowledge-in-Training-Data" class="headerlink" title="Are Transformers Able to Reason by Connecting Separated Knowledge in   Training Data?"></a>Are Transformers Able to Reason by Connecting Separated Knowledge in   Training Data?</h2><p><strong>Authors:Yutong Yin, Zhaoran Wang</strong></p>
<p>Humans exhibit remarkable compositional reasoning by integrating knowledge from various sources. For example, if someone learns ( B &#x3D; f(A) ) from one source and ( C &#x3D; g(B) ) from another, they can deduce ( C&#x3D;g(B)&#x3D;g(f(A)) ) even without encountering ( ABC ) together, showcasing the generalization ability of human intelligence. In this paper, we introduce a synthetic learning task, â€œFTCTâ€ (Fragmented at Training, Chained at Testing), to validate the potential of Transformers in replicating this skill and interpret its inner mechanism. In the training phase, data consist of separated knowledge fragments from an overall causal graph. During testing, Transformers must infer complete causal graph traces by integrating these fragments. Our findings demonstrate that few-shot Chain-of-Thought prompting enables Transformers to perform compositional reasoning on FTCT by revealing correct combinations of fragments, even if such combinations were absent in the training data. Furthermore, the emergence of compositional reasoning ability is strongly correlated with the model complexity and training-testing data similarity. We propose, both theoretically and empirically, that Transformers learn an underlying generalizable program from training, enabling effective compositional reasoning during testing. </p>
<blockquote>
<p>äººç±»èƒ½å¤Ÿé€šè¿‡æ•´åˆæ¥è‡ªä¸åŒæ¥æºçš„çŸ¥è¯†å±•ç°å‡ºæƒŠäººçš„ç»„åˆæ¨ç†èƒ½åŠ›ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæœ‰äººä»æŸä¸€æ¥æºå­¦ä¹ åˆ°ï¼ˆB&#x3D;fï¼ˆAï¼‰ï¼‰ï¼Œå¹¶ä»å¦ä¸€æ¥æºå­¦ä¹ åˆ°ï¼ˆC&#x3D;gï¼ˆBï¼‰ï¼‰ï¼Œé‚£ä¹ˆå³ä½¿æ²¡æœ‰åŒæ—¶é‡åˆ°ï¼ˆABCï¼‰ï¼Œä»–ä»¬ä¹Ÿèƒ½æ¨å¯¼å‡ºï¼ˆC&#x3D;gï¼ˆBï¼‰&#x3D;gï¼ˆfï¼ˆAï¼‰ï¼‰ï¼‰ï¼Œè¿™å±•ç¤ºäº†äººç±»æ™ºåŠ›çš„æ³›åŒ–èƒ½åŠ›ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€é¡¹åˆæˆå­¦ä¹ ä»»åŠ¡â€œFTCTâ€ï¼ˆè®­ç»ƒæ—¶ç¢ç‰‡åŒ–ï¼Œæµ‹è¯•æ—¶é“¾æ¥ï¼‰ï¼Œä»¥éªŒè¯Transformeråœ¨å¤åˆ¶è¿™é¡¹æŠ€èƒ½æ–¹é¢çš„æ½œåŠ›å¹¶è§£é‡Šå…¶å†…åœ¨æœºåˆ¶ã€‚åœ¨è®­ç»ƒé˜¶æ®µï¼Œæ•°æ®ç”±æ¥è‡ªæ•´ä½“å› æœå›¾çš„åˆ†ç¦»çŸ¥è¯†ç‰‡æ®µç»„æˆã€‚åœ¨æµ‹è¯•é˜¶æ®µï¼ŒTransformerå¿…é¡»é€šè¿‡æ•´åˆè¿™äº›ç‰‡æ®µæ¥æ¨æ–­å®Œæ•´çš„å› æœå›¾è½¨è¿¹ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå³ä½¿è¿™äº›ç»„åˆåœ¨è®­ç»ƒæ•°æ®ä¸­ä¸å­˜åœ¨ï¼ŒåŸºäºå°‘é‡ç¢ç‰‡çš„â€œChain-of-Thoughtâ€æç¤ºä¹Ÿèƒ½ä½¿Transformeråœ¨FTCTä¸Šæ‰§è¡Œç»„åˆæ¨ç†ï¼Œæ­ç¤ºæ­£ç¡®çš„ç¢ç‰‡ç»„åˆã€‚æ­¤å¤–ï¼Œç»„åˆæ¨ç†èƒ½åŠ›çš„å‡ºç°ä¸æ¨¡å‹å¤æ‚åº¦å’Œè®­ç»ƒ-æµ‹è¯•æ•°æ®ç›¸ä¼¼æ€§å¯†åˆ‡ç›¸å…³ã€‚æˆ‘ä»¬ä»ç†è®ºå’Œå®è·µä¸¤æ–¹é¢æå‡ºï¼ŒTransformeré€šè¿‡è®­ç»ƒå­¦ä¹ åˆ°ä¸€ä¸ªå¯æ³›åŒ–çš„åº•å±‚ç¨‹åºï¼Œä»è€Œåœ¨æµ‹è¯•æ—¶èƒ½å¤Ÿå®ç°æœ‰æ•ˆçš„ç»„åˆæ¨ç†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.15857v6">PDF</a> Accepted by ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†äººç±»å…·æœ‰å‡ºè‰²çš„ç»„åˆæ¨ç†èƒ½åŠ›ï¼Œèƒ½å¤Ÿæ•´åˆä¸åŒæ¥æºçš„çŸ¥è¯†ã€‚æ–‡ä¸­é€šè¿‡ä¸€ä¸ªç¤ºä¾‹é˜è¿°äº†äººç±»å¦‚ä½•è¿ç”¨ç»„åˆæ¨ç†è¿›è¡Œæ¨ç†è¿‡ç¨‹ã€‚é’ˆå¯¹è¿™ç§èƒ½åŠ›ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºâ€œFTCTâ€ï¼ˆè®­ç»ƒæ—¶ç¢ç‰‡åŒ–ï¼Œæµ‹è¯•æ—¶ä¸²è”åŒ–ï¼‰çš„åˆæˆå­¦ä¹ ä»»åŠ¡æ¥éªŒè¯Transformeræ¨¡å‹æ˜¯å¦å…·å¤‡è¿™ç§èƒ½åŠ›å¹¶è§£é‡Šå…¶å†…éƒ¨æœºåˆ¶ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå°‘é‡çš„â€œæ€ç»´é“¾â€ï¼ˆChain-of-Thoughtï¼‰æç¤ºèƒ½å¤Ÿå¸®åŠ©Transformeråœ¨FTCTä»»åŠ¡ä¸Šè¿›è¡Œç»„åˆæ¨ç†ï¼Œæ­£ç¡®ç»„åˆçŸ¥è¯†ç‰‡æ®µï¼Œå³ä½¿è¿™äº›ç»„åˆåœ¨è®­ç»ƒæ•°æ®ä¸­ä¸å­˜åœ¨ã€‚æ­¤å¤–ï¼Œæ¨¡å‹çš„å¤æ‚æ€§å’Œè®­ç»ƒæµ‹è¯•æ•°æ®çš„ç›¸ä¼¼æ€§å¯¹ç»„åˆæ¨ç†èƒ½åŠ›çš„å‡ºç°æœ‰é‡è¦å½±å“ã€‚æœ¬æ–‡æå‡ºï¼ŒTransformerä»è®­ç»ƒä¸­å­¦ä¹ äº†ä¸€ç§å¯æ¨å¹¿çš„åº•å±‚ç¨‹åºï¼Œä»è€Œåœ¨æµ‹è¯•æ—¶èƒ½å¤Ÿè¿›è¡Œæœ‰æ•ˆçš„ç»„åˆæ¨ç†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äººç±»å…·æœ‰å‡ºè‰²çš„ç»„åˆæ¨ç†èƒ½åŠ›ï¼Œèƒ½å¤Ÿæ•´åˆä¸åŒæ¥æºçš„çŸ¥è¯†è¿›è¡Œæ¨ç†ã€‚</li>
<li>â€œFTCTâ€ä»»åŠ¡ç”¨äºéªŒè¯Transformeræ¨¡å‹åœ¨ç»„åˆæ¨ç†æ–¹é¢çš„æ½œåŠ›ã€‚</li>
<li>å°‘é‡â€œæ€ç»´é“¾â€æç¤ºæœ‰åŠ©äºTransformeråœ¨FTCTä»»åŠ¡ä¸Šæ­£ç¡®ç»„åˆçŸ¥è¯†ç‰‡æ®µã€‚</li>
<li>æ¨¡å‹å¤æ‚æ€§å’Œè®­ç»ƒæµ‹è¯•æ•°æ®ç›¸ä¼¼æ€§å¯¹ç»„åˆæ¨ç†èƒ½åŠ›æœ‰é‡è¦å½±å“ã€‚</li>
<li>Transformeråœ¨è®­ç»ƒä¸­å­¦ä¹ äº†ä¸€ç§å¯æ¨å¹¿çš„åº•å±‚ç¨‹åºã€‚</li>
<li>è¿™ç§åº•å±‚ç¨‹åºä½¿å¾—Transformeråœ¨æµ‹è¯•æ—¶èƒ½å¤Ÿè¿›è¡Œæœ‰æ•ˆçš„ç»„åˆæ¨ç†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.15857">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7ae97626b29c6786fd9a978ce3cf62ed.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-533af2e18d5d670eff8d393c65789045.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8e4d424947576c136c0ec5353921e72a.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Can-LLMs-assist-with-Ambiguity-A-Quantitative-Evaluation-of-various-Large-Language-Models-on-Word-Sense-Disambiguation"><a href="#Can-LLMs-assist-with-Ambiguity-A-Quantitative-Evaluation-of-various-Large-Language-Models-on-Word-Sense-Disambiguation" class="headerlink" title="Can LLMs assist with Ambiguity? A Quantitative Evaluation of various   Large Language Models on Word Sense Disambiguation"></a>Can LLMs assist with Ambiguity? A Quantitative Evaluation of various   Large Language Models on Word Sense Disambiguation</h2><p><strong>Authors:T. G. D. K. Sumanathilaka, Nicholas Micallef, Julian Hough</strong></p>
<p>Ambiguous words are often found in modern digital communications. Lexical ambiguity challenges traditional Word Sense Disambiguation (WSD) methods, due to limited data. Consequently, the efficiency of translation, information retrieval, and question-answering systems is hindered by these limitations. This study investigates the use of Large Language Models (LLMs) to improve WSD using a novel approach combining a systematic prompt augmentation mechanism with a knowledge base (KB) consisting of different sense interpretations. The proposed method incorporates a human-in-loop approach for prompt augmentation where prompt is supported by Part-of-Speech (POS) tagging, synonyms of ambiguous words, aspect-based sense filtering and few-shot prompting to guide the LLM. By utilizing a few-shot Chain of Thought (COT) prompting-based approach, this work demonstrates a substantial improvement in performance. The evaluation was conducted using FEWS test data and sense tags. This research advances accurate word interpretation in social media and digital communication. </p>
<blockquote>
<p>åœ¨ç°ä»£æ•°å­—é€šä¿¡ä¸­ç»å¸¸å¯ä»¥å‘ç°è¯ä¹‰æ¨¡ç³Šçš„è¯è¯­ã€‚ç”±äºæ•°æ®æœ‰é™ï¼Œè¯æ±‡çš„æ¨¡ç³Šæ€§ç»™ä¼ ç»Ÿçš„è¯ä¹‰æ¶ˆæ­§ï¼ˆWord Sense Disambiguation, WSDï¼‰æ–¹æ³•å¸¦æ¥äº†æŒ‘æˆ˜ã€‚å› æ­¤ï¼Œç¿»è¯‘ã€ä¿¡æ¯æ£€ç´¢å’Œé—®ç­”ç³»ç»Ÿçš„æ•ˆç‡å—åˆ°äº†è¿™äº›é™åˆ¶çš„é˜»ç¢ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç»“åˆä¸€ç§ç³»ç»Ÿçš„æç¤ºå¢å¼ºæœºåˆ¶å’ŒåŒ…å«ä¸åŒè¯ä¹‰è§£é‡Šçš„çŸ¥è¯†åº“ï¼ˆKBï¼‰æ¥æ”¹å–„è¯ä¹‰æ¶ˆæ­§çš„æ–¹æ³•ã€‚æ‰€æå‡ºçš„æ–¹æ³•é‡‡ç”¨äº†ä¸€ç§äººæœºå¾ªç¯çš„æç¤ºå¢å¼ºæ–¹æ³•ï¼Œå…¶ä¸­æç¤ºå¾—åˆ°è¯ç±»æ ‡æ³¨ã€æ¨¡ç³Šè¯åŒä¹‰è¯ã€åŸºäºæ–¹é¢çš„è¯ä¹‰è¿‡æ»¤å’Œå°‘é‡æç¤ºçš„æ”¯æŒï¼Œä»¥å¼•å¯¼å¤§å‹è¯­è¨€æ¨¡å‹ã€‚é€šè¿‡é‡‡ç”¨åŸºäºå°‘é‡æ€ç»´çš„æ€ç»´é“¾ï¼ˆCOTï¼‰æç¤ºæ–¹æ³•ï¼Œè¿™é¡¹å·¥ä½œåœ¨æ€§èƒ½ä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚è¯„ä¼°æ˜¯ä½¿ç”¨FEWSæµ‹è¯•æ•°æ®å’Œè¯ä¹‰æ ‡ç­¾è¿›è¡Œçš„ã€‚è¯¥ç ”ç©¶æ¨åŠ¨äº†ç¤¾äº¤åª’ä½“å’Œæ•°å­—é€šä¿¡ä¸­çš„å‡†ç¡®è¯ä¹‰è§£è¯»ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.18337v3">PDF</a> 12 pages,6 tables, 1 figure, Proceedings of the 1st International   Conference on NLP &amp; AI for Cyber Security</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†ç°ä»£æ•°å­—é€šä¿¡ä¸­å¸¸è§çš„è¯æ±‡æ­§ä¹‰é—®é¢˜ã€‚ç”±äºæ•°æ®æœ‰é™ï¼Œä¼ ç»Ÿçš„è¯è¯­æ„ŸçŸ¥æ¶ˆæ­§ï¼ˆWSDï¼‰æ–¹æ³•é¢ä¸´æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶é‡‡ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç»“åˆç³»ç»Ÿæç¤ºå¢å¼ºæœºåˆ¶å’ŒåŒ…å«ä¸åŒæ„ŸçŸ¥è§£è¯»çš„çŸ¥è¯†åº“ï¼ˆKBï¼‰æ¥æ”¹å–„WSDã€‚é€šè¿‡ç»“åˆäººç±»å‚ä¸çš„æç¤ºå¢å¼ºæ–¹æ³•ï¼Œåˆ©ç”¨è¯æ€§æ ‡æ³¨ã€æ¨¡ç³Šè¯çš„åŒä¹‰è¯ã€åŸºäºæ–¹é¢çš„æ„ŸçŸ¥è¿‡æ»¤å’Œå°‘é‡æç¤ºå¼•å¯¼LLMã€‚é€šè¿‡åŸºäºå°‘é‡æç¤ºçš„æ€è€ƒé“¾ï¼ˆCOTï¼‰æç¤ºæ–¹æ³•ï¼Œè¯¥ç ”ç©¶åœ¨æ€§èƒ½ä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚è¯„ä¼°é‡‡ç”¨FEWSæµ‹è¯•æ•°æ®å’Œæ„ŸçŸ¥æ ‡ç­¾è¿›è¡Œã€‚æ­¤ç ”ç©¶æœ‰åŠ©äºæ”¹è¿›ç¤¾äº¤åª’ä½“å’Œæ•°å­—é€šä¿¡ä¸­çš„å‡†ç¡®è¯æ±‡è§£è¯»ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯æ±‡æ­§ä¹‰åœ¨ç°ä»£æ•°å­—é€šä¿¡ä¸­æ˜¯å¸¸è§é—®é¢˜ã€‚</li>
<li>ä¼ ç»ŸWSDæ–¹æ³•å› æ•°æ®æœ‰é™è€Œé¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>æœ¬ç ”ç©¶é‡‡ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ”¹å–„WSDã€‚</li>
<li>ç»“åˆç³»ç»Ÿæç¤ºå¢å¼ºæœºåˆ¶å’ŒçŸ¥è¯†åº“è¿›è¡Œæ„ŸçŸ¥è§£è¯»ã€‚</li>
<li>äººç±»å‚ä¸çš„æç¤ºå¢å¼ºæ–¹æ³•åŒ…æ‹¬è¯æ€§æ ‡æ³¨ã€åŒä¹‰è¯ç­‰ã€‚</li>
<li>é‡‡ç”¨å°‘é‡æç¤ºçš„æ€è€ƒé“¾ï¼ˆCOTï¼‰æ–¹æ³•æ˜¾è‘—æé«˜äº†æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.18337">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cbf709fa9a7dd0b311058c27411f02dc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d7f227667733ec050b1493a89217bb38.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f3bbeb948a0dce8ab986295a7dcd69b9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f10c3db177cf4a894c58705c963e7489.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-28/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-28/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-28/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-4043fa641afee0e3275d6009246b698a.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-28  Multimodal LLM-Guided Semantic Correction in Text-to-Image Diffusion
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-28
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-28/MMT/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-686f51f5a4b71e473c7ac974fb376f57.jpg" class="responsive-img" alt="MMT">
                        
                        <span class="card-title">MMT</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            MMT æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-28  MT$^{3}$ Scaling MLLM-based Text Image Machine Translation via   Multi-Task Reinforcement Learning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-28
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/MMT/" class="post-category">
                                    MMT
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/MMT/">
                        <span class="chip bg-color">MMT</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32298.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
