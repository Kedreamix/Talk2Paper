<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot 方向最新论文已更新，请持续关注 Update in 2025-05-28  Improvement Strategies for Few-Shot Learning in OCT Image Classification   of Rare Retinal Diseases">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-c4ae5d97b9465735d11ed9bebeaaed5f.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-05-28
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-06-02
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    18.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    75 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-05-28-更新"><a href="#2025-05-28-更新" class="headerlink" title="2025-05-28 更新"></a>2025-05-28 更新</h1><h2 id="Improvement-Strategies-for-Few-Shot-Learning-in-OCT-Image-Classification-of-Rare-Retinal-Diseases"><a href="#Improvement-Strategies-for-Few-Shot-Learning-in-OCT-Image-Classification-of-Rare-Retinal-Diseases" class="headerlink" title="Improvement Strategies for Few-Shot Learning in OCT Image Classification   of Rare Retinal Diseases"></a>Improvement Strategies for Few-Shot Learning in OCT Image Classification   of Rare Retinal Diseases</h2><p><strong>Authors:Cheng-Yu Tai, Ching-Wen Chen, Chi-Chin Wu, Bo-Chen Chiu,  Cheng-Hung,  Lin, Cheng-Kai Lu, Jia-Kang Wang, Tzu-Lun Huang</strong></p>
<p>This paper focuses on using few-shot learning to improve the accuracy of classifying OCT diagnosis images with major and rare classes. We used the GAN-based augmentation strategy as a baseline and introduced several novel methods to further enhance our model. The proposed strategy contains U-GAT-IT for improving the generative part and uses the data balance technique to narrow down the skew of accuracy between all categories. The best model obtained was built with CBAM attention mechanism and fine-tuned InceptionV3, and achieved an overall accuracy of 97.85%, representing a significant improvement over the original baseline. </p>
<blockquote>
<p>本文重点关注使用少量样本学习来提高对OCT诊断图像中主要和罕见类别分类的准确性。我们采用了基于GAN的增强策略作为基线，并引入了几种新方法进一步改进我们的模型。提出的策略包含用于改进生成部分的U-GAT-IT，并使用数据平衡技术来缩小所有类别之间的准确率偏差。获得的最佳模型是采用CBAM注意力机制和微调过的InceptionV3构建的，总体准确率达到97.85%，相较于原始基线有了显著的提升。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20149v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>这篇论文利用少样本学习技术提高了OCT诊断图像分类的准确性，特别是针对主要和罕见类别的分类。采用基于GAN的增强策略作为基线，并引入了几种新方法进一步提升模型性能。通过U-GAT-IT改进生成部分，并运用数据平衡技术缩小各类别准确度的偏差。最佳模型采用CBAM注意力机制和微调过的InceptionV3，总体准确度达到97.85%，较基线有显著改善。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>论文关注于利用少样本学习提高OCT诊断图像分类的准确性。</li>
<li>采用基于GAN的增强策略作为基线方法。</li>
<li>引入U-GAT-IT改进生成模型部分。</li>
<li>运用数据平衡技术缩小各类别之间的准确度偏差。</li>
<li>最佳模型结合了CBAM注意力机制和微调过的InceptionV3。</li>
<li>最佳模型的总体准确度达到97.85%。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20149">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-23caef0563313eceb1f76549158563d1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4f6d1fd026c4693008f443d6b3f5bd61.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ac07083d186e2c5e8cac7bcaaa4ec82e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7e345ff77521dee94e29344c25fc3852.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="SaSi-A-Self-augmented-and-Self-interpreted-Deep-Learning-Approach-for-Few-shot-Cryo-ET-Particle-Detection"><a href="#SaSi-A-Self-augmented-and-Self-interpreted-Deep-Learning-Approach-for-Few-shot-Cryo-ET-Particle-Detection" class="headerlink" title="SaSi: A Self-augmented and Self-interpreted Deep Learning Approach for   Few-shot Cryo-ET Particle Detection"></a>SaSi: A Self-augmented and Self-interpreted Deep Learning Approach for   Few-shot Cryo-ET Particle Detection</h2><p><strong>Authors:Gokul Adethya, Bhanu Pratyush Mantha, Tianyang Wang, Xingjian Li, Min Xu</strong></p>
<p>Cryo-electron tomography (cryo-ET) has emerged as a powerful technique for imaging macromolecular complexes in their near-native states. However, the localization of 3D particles in cellular environments still presents a significant challenge due to low signal-to-noise ratios and missing wedge artifacts. Deep learning approaches have shown great potential, but they need huge amounts of data, which can be a challenge in cryo-ET scenarios where labeled data is often scarce. In this paper, we propose a novel Self-augmented and Self-interpreted (SaSi) deep learning approach towards few-shot particle detection in 3D cryo-ET images. Our method builds upon self-augmentation techniques to further boost data utilization and introduces a self-interpreted segmentation strategy for alleviating dependency on labeled data, hence improving generalization and robustness. As demonstrated by experiments conducted on both simulated and real-world cryo-ET datasets, the SaSi approach significantly outperforms existing state-of-the-art methods for particle localization. This research increases understanding of how to detect particles with very few labels in cryo-ET and thus sets a new benchmark for few-shot learning in structural biology. </p>
<blockquote>
<p>冷冻电子断层扫描（cryo-ET）已成为可视化近天然态大分子复合物的一种强大技术。然而，由于信噪比低和缺失的楔形伪影，在细胞环境中对三维粒子的定位仍然是一个巨大的挑战。深度学习的方法显示出巨大的潜力，但它们需要大量的数据，这在冷冻电镜技术场景中可能是一个挑战，因为标记的数据通常很稀缺。在本文中，我们提出了一种新型的自我增强和自我解释（SaSi）深度学习的方法，用于在少量3D冷冻电子断层扫描图像中进行粒子检测。我们的方法建立在自我增强技术之上，进一步提高了数据利用率，并引入了一种自我解释分割策略，以减轻对标记数据的依赖，从而提高泛化和鲁棒性。通过模拟和真实冷冻电镜数据集的实验证明，SaSi方法在粒子定位方面显著优于现有最先进的方法。该研究提高了在冷冻电镜中如何检测少量标签粒子的理解，从而为结构生物学中的小样本学习设定了一个新的基准。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19948v1">PDF</a> </p>
<p><strong>Summary</strong><br>     本文提出一种新型的基于自增强和自解释（SaSi）的深度学习方法，用于在少量标注数据的情况下实现三维冷冻电子断层扫描（cryo-ET）图像中的粒子检测。该方法通过自增强技术进一步提高数据利用率，并引入自解释分割策略，减少对标注数据的依赖，从而提高模型的通用性和鲁棒性。实验证明，该方法在模拟和实际cryo-ET数据集上的表现均优于现有技术。该研究提高了在冷冻电镜中检测粒子的能力，为结构生物学中的小样本学习树立了新的标杆。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>冷冻电子断层扫描（cryo-ET）技术用于成像大分子复合物。</li>
<li>在细胞环境中定位三维粒子存在挑战，主要由于低信噪比和缺失楔形伪影。</li>
<li>深度学习在解决此问题上显示出潜力，但需要大量数据，而冷冻电镜场景中的标注数据往往稀缺。</li>
<li>提出了新型的基于自增强和自解释（SaSi）的深度学习方法用于少样本粒子检测。</li>
<li>SaSi方法通过自增强技术提高数据利用率，并引入自解释分割策略减少对标注数据的依赖。</li>
<li>实验证明SaSi方法在模拟和实际数据集上均表现优异。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19948">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-713608727aa0f30e7651d7f3195626b2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-244603c8058707a44c3fa10a6b538301.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cafacfb9797997f9d1abc7497cc058bb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-87c321ade6f90b17bafc0bdc8643639f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c63c6e73cd9f3d4da4c26d447f0241c5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d5b8a7833c814d5865fc0084593242bf.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Large-Language-Models-as-Autonomous-Spacecraft-Operators-in-Kerbal-Space-Program"><a href="#Large-Language-Models-as-Autonomous-Spacecraft-Operators-in-Kerbal-Space-Program" class="headerlink" title="Large Language Models as Autonomous Spacecraft Operators in Kerbal Space   Program"></a>Large Language Models as Autonomous Spacecraft Operators in Kerbal Space   Program</h2><p><strong>Authors:Alejandro Carrasco, Victor Rodriguez-Fernandez, Richard Linares</strong></p>
<p>Recent trends are emerging in the use of Large Language Models (LLMs) as autonomous agents that take actions based on the content of the user text prompts. We intend to apply these concepts to the field of Control in space, enabling LLMs to play a significant role in the decision-making process for autonomous satellite operations. As a first step towards this goal, we have developed a pure LLM-based solution for the Kerbal Space Program Differential Games (KSPDG) challenge, a public software design competition where participants create autonomous agents for maneuvering satellites involved in non-cooperative space operations, running on the KSP game engine. Our approach leverages prompt engineering, few-shot prompting, and fine-tuning techniques to create an effective LLM-based agent that ranked 2nd in the competition. To the best of our knowledge, this work pioneers the integration of LLM agents into space research. The project comprises several open repositories to facilitate replication and further research. The codebase is accessible on \href{<a target="_blank" rel="noopener" href="https://github.com/ARCLab-MIT/kspdg%7D%7BGitHub%7D">https://github.com/ARCLab-MIT/kspdg}{GitHub}</a>, while the trained models and datasets are available on \href{<a target="_blank" rel="noopener" href="https://huggingface.co/OhhTuRnz%7D%7BHugging">https://huggingface.co/OhhTuRnz}{Hugging</a> Face}. Additionally, experiment tracking and detailed results can be reviewed on \href{<a target="_blank" rel="noopener" href="https://wandb.ai/carrusk/huggingface%7D%7BWeights">https://wandb.ai/carrusk/huggingface}{Weights</a> &amp; Biases </p>
<blockquote>
<p>最近出现了使用大型语言模型（LLM）作为自主代理的趋势，这些代理会根据用户文本提示的内容采取行动。我们打算将这些概念应用于空间控制领域，使LLM在自主卫星操作的决策过程中发挥重要作用。作为实现这一目标的第一步，我们为Kerbal Space Program Differential Games（KSPDG）挑战开发了一种纯LLM解决方案。KSPDG是一场公开的软件设计竞赛，参赛者需为非合作空间操作创建自主代理卫星，该解决方案基于LLM在KSP游戏引擎上运行。我们的方法利用提示工程、少提示和微调技术创建了一个有效的基于LLM的代理，在比赛中排名第二。据我们所知，这项工作首创了将LLM代理集成到空间研究中。该项目包含多个开源仓库，以促进复制和进一步研究。代码库可在GitHub上访问（[<a target="_blank" rel="noopener" href="https://github.com/ARCLab-MIT/kspdg%EF%BC%89%EF%BC%8C%E8%80%8C%E8%AE%AD%E7%BB%83%E5%A5%BD%E7%9A%84%E6%A8%A1%E5%9E%8B%E5%92%8C%E6%95%B0%E6%8D%AE%E9%9B%86%E5%8F%AF%E5%9C%A8Hugging">https://github.com/ARCLab-MIT/kspdg），而训练好的模型和数据集可在Hugging</a> Face上找到（<a target="_blank" rel="noopener" href="https://huggingface.co/OhhTuRnz%EF%BC%89%E3%80%82%E6%AD%A4%E5%A4%96%EF%BC%8C%E5%AE%9E%E9%AA%8C%E8%B7%9F%E8%B8%AA%E5%92%8C%E8%AF%A6%E7%BB%86%E7%BB%93%E6%9E%9C%E5%8F%AF%E5%9C%A8Weights">https://huggingface.co/OhhTuRnz）。此外，实验跟踪和详细结果可在Weights</a> &amp; Biases上查看（<a target="_blank" rel="noopener" href="https://wandb.ai/carrusk/huggingface%EF%BC%89%E3%80%82">https://wandb.ai/carrusk/huggingface）。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19896v1">PDF</a> Non revised version for paper going to be published in Journal of   Advances in Space Research</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）被应用于空间控制领域，作为自主代理进行基于用户文本提示的行动决策。研究团队在Kerbal Space Program Differential Games挑战中，开发了一种纯LLM解决方案，利用提示工程、少样本提示和微调技术创建了一个有效的LLM代理，并在竞赛中取得了第二名。该项目为LLM代理在空间研究中的集成开辟了先河，其代码库、训练模型和数据集均已公开供研究使用。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）被应用于自主卫星操作的决策过程中。</li>
<li>LLMs被用作自主代理，能够根据用户文本提示采取行动。</li>
<li>在Kerbal Space Program Differential Games挑战中，开发了一种基于LLM的纯解决方案。</li>
<li>该解决方案利用提示工程、少样本提示和微调技术，取得了竞赛第二名的好成绩。</li>
<li>此项目为LLM代理在空间研究中的集成提供了开创性的实践。</li>
<li>项目公开了代码库、训练模型和数据集以供研究使用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19896">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0d436e1b6a6d3cf5aa61329f275f59be.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9f7671213e5400d37c87831dd8e04b81.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="GoLF-NRT-Integrating-Global-Context-and-Local-Geometry-for-Few-Shot-View-Synthesis"><a href="#GoLF-NRT-Integrating-Global-Context-and-Local-Geometry-for-Few-Shot-View-Synthesis" class="headerlink" title="GoLF-NRT: Integrating Global Context and Local Geometry for Few-Shot   View Synthesis"></a>GoLF-NRT: Integrating Global Context and Local Geometry for Few-Shot   View Synthesis</h2><p><strong>Authors:You Wang, Li Fang, Hao Zhu, Fei Hu, Long Ye, Zhan Ma</strong></p>
<p>Neural Radiance Fields (NeRF) have transformed novel view synthesis by modeling scene-specific volumetric representations directly from images. While generalizable NeRF models can generate novel views across unknown scenes by learning latent ray representations, their performance heavily depends on a large number of multi-view observations. However, with limited input views, these methods experience significant degradation in rendering quality. To address this limitation, we propose GoLF-NRT: a Global and Local feature Fusion-based Neural Rendering Transformer. GoLF-NRT enhances generalizable neural rendering from few input views by leveraging a 3D transformer with efficient sparse attention to capture global scene context. In parallel, it integrates local geometric features extracted along the epipolar line, enabling high-quality scene reconstruction from as few as 1 to 3 input views. Furthermore, we introduce an adaptive sampling strategy based on attention weights and kernel regression, improving the accuracy of transformer-based neural rendering. Extensive experiments on public datasets show that GoLF-NRT achieves state-of-the-art performance across varying numbers of input views, highlighting the effectiveness and superiority of our approach. Code is available at <a target="_blank" rel="noopener" href="https://github.com/KLMAV-CUC/GoLF-NRT">https://github.com/KLMAV-CUC/GoLF-NRT</a>. </p>
<blockquote>
<p>神经辐射场（NeRF）通过直接从图像建模场景特定的体积表示，从而实现了新颖视图合成的变革。虽然通用NeRF模型可以通过学习潜在射线表示来生成未知场景的新颖视图，但其性能在很大程度上依赖于大量多视图观察。然而，在输入视图有限的情况下，这些方法在渲染质量方面会经历显著下降。为了解决这一局限性，我们提出了基于全局和局部特征融合的神经渲染转换器GoLF-NRT。GoLF-NRT通过利用具有高效稀疏注意力的3D转换器来增强从少数输入视图进行通用神经渲染，以捕捉场景的全局上下文。同时，它集成了沿极线提取的局部几何特征，从而能够从仅1到3个输入视图实现高质量的场景重建。此外，我们引入了一种基于注意力权重和核回归的自适应采样策略，提高了基于转换器的神经渲染的准确性。在公共数据集上的广泛实验表明，GoLF-NRT在不同数量的输入视图上实现了最先进的性能，凸显了我们方法的有效性和优越性。代码可访问<a target="_blank" rel="noopener" href="https://github.com/KLMAV-CUC/GoLF-NRT%E3%80%82">https://github.com/KLMAV-CUC/GoLF-NRT。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19813v1">PDF</a> CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>神经网络辐射场（NeRF）通过图像建模场景特定的体积表示，从而转变了新型视图合成技术。针对基于潜在射线表示的一般化NeRF模型在有限输入视图下生成新视图时性能显著下降的问题，提出了基于全局和局部特征融合的神经渲染转换器GoLF-NRT。GoLF-NRT通过高效的稀疏注意力3D转换器捕捉场景上下文，同时集成沿极线的局部几何特征，实现仅从一至三个输入视图的高质量场景重建。此外，还引入了一种基于注意力权重和核回归的自适应采样策略，提高了基于转换器的神经渲染的准确性。在公开数据集上的广泛实验表明，GoLF-NRT在不同输入视图数量上均达到最佳性能，凸显了该方法的有效性和优越性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>NeRF技术通过图像建模场景特定的体积表示，实现了新型视图合成。</li>
<li>一般化的NeRF模型在有限输入视图下性能下降，需要更多视图来提高性能。</li>
<li>GoLF-NRT通过全局和局部特征融合增强从有限输入视图进行神经渲染的能力。</li>
<li>GoLF-NRT利用3D转换器捕捉场景上下文，并结合沿极线的局部几何特征。</li>
<li>GoLF-NRT实现了仅从一至三个输入视图的高质量场景重建。</li>
<li>GoLF-NRT引入了自适应采样策略，基于注意力权重和核回归，提高了神经渲染的准确性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19813">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b51bcce7a609a5dac67c9bc8f5280687.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2d99bb1f3f43376d88e9fba45acd117e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-64ff787be63c845bf786735e4f9869f6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-85b686bcdecc847ef876fc1ebb76ab6d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4fd79e178e2f4773a3c5d8e22fb296c1.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="MOLE-Metadata-Extraction-and-Validation-in-Scientific-Papers-Using-LLMs"><a href="#MOLE-Metadata-Extraction-and-Validation-in-Scientific-Papers-Using-LLMs" class="headerlink" title="MOLE: Metadata Extraction and Validation in Scientific Papers Using LLMs"></a>MOLE: Metadata Extraction and Validation in Scientific Papers Using LLMs</h2><p><strong>Authors:Zaid Alyafeai, Maged S. Al-Shaibani, Bernard Ghanem</strong></p>
<p>Metadata extraction is essential for cataloging and preserving datasets, enabling effective research discovery and reproducibility, especially given the current exponential growth in scientific research. While Masader (Alyafeai et al.,2021) laid the groundwork for extracting a wide range of metadata attributes from Arabic NLP datasets’ scholarly articles, it relies heavily on manual annotation. In this paper, we present MOLE, a framework that leverages Large Language Models (LLMs) to automatically extract metadata attributes from scientific papers covering datasets of languages other than Arabic. Our schema-driven methodology processes entire documents across multiple input formats and incorporates robust validation mechanisms for consistent output. Additionally, we introduce a new benchmark to evaluate the research progress on this task. Through systematic analysis of context length, few-shot learning, and web browsing integration, we demonstrate that modern LLMs show promising results in automating this task, highlighting the need for further future work improvements to ensure consistent and reliable performance. We release the code: <a target="_blank" rel="noopener" href="https://github.com/IVUL-KAUST/MOLE">https://github.com/IVUL-KAUST/MOLE</a> and dataset: <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/IVUL-KAUST/MOLE">https://huggingface.co/datasets/IVUL-KAUST/MOLE</a> for the research community. </p>
<blockquote>
<p>元数据提取对于数据集编目和保存至关重要，它促进了有效的研究发现和可重复性，尤其是在当前科学研究呈指数级增长的情况下。虽然Masader（Alyafeai等人，2021年）奠定了从阿拉伯语NLP数据集学术论文中提取广泛元数据属性的基础，但它主要依赖于人工标注。在本文中，我们介绍了MOLE框架，该框架利用大型语言模型（LLM）自动从非阿拉伯语数据集的科学论文中提取元数据属性。我们的基于模式的方法处理多种输入格式的整个文档，并纳入稳健的验证机制以实现一致输出。此外，我们还引入了一个新的基准测试来评估此任务的研究进展。通过对上下文长度、小样本学习和网页浏览整合的系统分析，我们证明了现代大型语言模型在该任务的自动化方面显示出有前途的结果，并强调了未来需要进一步改进工作以确保性能和可靠性的一致性。我们向研究社区发布了代码：<a target="_blank" rel="noopener" href="https://github.com/IVUL-KAUST/MOLE%E5%92%8C%E6%95%B0%E6%8D%AE%E9%9B%86%EF%BC%9Ahttps://huggingface.co/datasets/IVUL-KAUST/MOLE%E3%80%82">https://github.com/IVUL-KAUST/MOLE和数据集：https://huggingface.co/datasets/IVUL-KAUST/MOLE。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19800v1">PDF</a> </p>
<p><strong>Summary</strong><br>元数据提取对于数据集的编目和保存、研究发现的效率和可重复性至关重要，尤其考虑到科学研究的指数级增长。面对此前阿拉伯语系NLP数据集的自动化提取大多依赖手动标注的问题，本文提出MOLE框架，利用大型语言模型（LLM）自动提取非阿拉伯语系的科学论文中的元数据属性。该框架采用基于架构的方法处理多种输入格式的文档，并具备稳健的验证机制以确保输出的一致性。同时引入新的基准测试来衡量此项研究的进展。通过上下文长度分析、小样学习法和网络浏览集成进行系统分析，展示现代大型语言模型在自动化此任务上的潜力，并指明未来改进方向以确保性能和可靠性。相关代码和数据集已发布供研究社区使用。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>元数据提取对于数据集管理至关重要，有助于研究发现和可重复性。</li>
<li>Masader主要侧重于阿拉伯NLP数据集的元数据提取，但依赖手动标注。</li>
<li>MOLE框架利用大型语言模型自动提取非阿拉伯语系的科学论文中的元数据属性。</li>
<li>MOLE框架具备处理多种输入格式文档的能力，并具备稳健的验证机制。</li>
<li>引入新的基准测试来衡量元数据提取的研究进展。</li>
<li>现代大型语言模型在自动化元数据提取任务上展现出潜力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19800">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-55aec0f8bee048797ab6b64cca3616b5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7a596582e74c447a758b4a5170c2d88a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-017ef48e040ef42c0b67d7ff3576b33b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8b8baa2dab16dfcdc98dda0a10ccaf8c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7bcbc2e352093d067e45f6bfa30eab2d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-addc4ba5eb14c1b45f17ac8a01ecc250.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-515fd37e00643795451eb440efdd0451.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d5cad11ca80a1b517220d908bc945f72.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dd971103b4bda32ca343b0cf461af38d.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Few-Shot-Class-Incremental-Learning-For-Efficient-SAR-Automatic-Target-Recognition"><a href="#Few-Shot-Class-Incremental-Learning-For-Efficient-SAR-Automatic-Target-Recognition" class="headerlink" title="Few-Shot Class-Incremental Learning For Efficient SAR Automatic Target   Recognition"></a>Few-Shot Class-Incremental Learning For Efficient SAR Automatic Target   Recognition</h2><p><strong>Authors:George Karantaidis, Athanasios Pantsios, Ioannis Kompatsiaris, Symeon Papadopoulos</strong></p>
<p>Synthetic aperture radar automatic target recognition (SAR-ATR) systems have rapidly evolved to tackle incremental recognition challenges in operational settings. Data scarcity remains a major hurdle that conventional SAR-ATR techniques struggle to address. To cope with this challenge, we propose a few-shot class-incremental learning (FSCIL) framework based on a dual-branch architecture that focuses on local feature extraction and leverages the discrete Fourier transform and global filters to capture long-term spatial dependencies. This incorporates a lightweight cross-attention mechanism that fuses domain-specific features with global dependencies to ensure robust feature interaction, while maintaining computational efficiency by introducing minimal scale-shift parameters. The framework combines focal loss for class distinction under imbalance and center loss for compact intra-class distributions to enhance class separation boundaries. Experimental results on the MSTAR benchmark dataset demonstrate that the proposed framework consistently outperforms state-of-the-art methods in FSCIL SAR-ATR, attesting to its effectiveness in real-world scenarios. </p>
<blockquote>
<p>合成孔径雷达自动目标识别（SAR-ATR）系统已迅速进化，以应对操作环境中不断增长的识别挑战。数据稀缺仍是传统SAR-ATR技术难以解决的一大障碍。为了应对这一挑战，我们提出了一种基于双分支架构的少量类别增量学习（FSCIL）框架，侧重于局部特征提取，并利用离散傅里叶变换和全局滤波器捕获长期空间依赖性。这结合了轻量级的跨注意力机制，融合了领域特定特征与全局依赖性，以确保稳健的特征交互，同时通过引入最小的尺度偏移参数来保持计算效率。该框架结合了焦点损失，在不平衡情况下进行类别区分，以及中心损失，用于紧凑的类内分布，以增强类间分离边界。在MSTAR基准数据集上的实验结果表明，该框架在FSCIL SAR-ATR方面始终优于最新方法，证明了其在现实场景中的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19565v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>SAR-ATR系统在面临现实场景中逐渐增强的识别挑战时，仍在迅速演进发展。为解决其中的数据稀缺难题，研究人员提出了基于双分支架构的少量类增量学习（FSCIL）框架。该框架通过离散傅里叶变换和全局滤波器，对局部特征进行提取，捕捉长期空间依赖性。同时，它融合了领域特定特征与全局依赖性，确保稳健的特征交互，并通过引入最小的尺度偏移参数维持计算效率。该框架结合了焦点损失与中心损失来提升类别之间的界限。在MSTAR基准数据集上的实验结果表明，所提出的框架在FSCIL SAR-ATR方面表现优于现有技术，证明了其在现实场景中的有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>SAR-ATR系统面临数据稀缺的挑战。</li>
<li>提出了一种基于双分支架构的少量类增量学习（FSCIL）框架来解决数据稀缺问题。</li>
<li>利用离散傅里叶变换和全局滤波器捕捉长期空间依赖性。</li>
<li>通过融合领域特定特征和全局依赖性确保稳健的特征交互。</li>
<li>引入最小的尺度偏移参数以保持计算效率。</li>
<li>结合焦点损失和中心损失以增强类别之间的界限。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19565">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0a264a8763d970094975eadd920c9f1c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d5c822a22eacc5f38079389b2a5bbc61.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-00ebd1791887661b76adc3f17dd269ac.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3e8043c007b24fb5c42205de8d480f68.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b471aaeaba0e17769f91726c17386322.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="DiSa-Directional-Saliency-Aware-Prompt-Learning-for-Generalizable-Vision-Language-Models"><a href="#DiSa-Directional-Saliency-Aware-Prompt-Learning-for-Generalizable-Vision-Language-Models" class="headerlink" title="DiSa: Directional Saliency-Aware Prompt Learning for Generalizable   Vision-Language Models"></a>DiSa: Directional Saliency-Aware Prompt Learning for Generalizable   Vision-Language Models</h2><p><strong>Authors:Niloufar Alipour Talemi, Hossein Kashiani, Hossein R. Nowdeh, Fatemeh Afghah</strong></p>
<p>Prompt learning has emerged as a powerful paradigm for adapting vision-language models such as CLIP to downstream tasks. However, existing methods often overfit to seen data, leading to significant performance degradation when generalizing to novel classes or unseen domains. To address this limitation, we propose DiSa, a Directional Saliency-Aware Prompt Learning framework that integrates two complementary regularization strategies to enhance generalization. First, our Cross-Interactive Regularization (CIR) fosters cross-modal alignment by enabling cooperative learning between prompted and frozen encoders. Within CIR, a saliency-aware masking strategy guides the image encoder to prioritize semantically critical image regions, reducing reliance on less informative patches. Second, we introduce a directional regularization strategy that aligns visual embeddings with class-wise prototype features in a directional manner to prioritize consistency in feature orientation over strict proximity. This approach ensures robust generalization by leveraging stable prototype directions derived from class-mean statistics. Extensive evaluations on 11 diverse image classification benchmarks demonstrate that DiSa consistently outperforms state-of-the-art prompt learning methods across various settings, including base-to-novel generalization, cross-dataset transfer, domain generalization, and few-shot learning. </p>
<blockquote>
<p>提示学习已经成为将CLIP等视觉语言模型适应下游任务的一种强大范式。然而，现有方法往往会对可见数据进行过度拟合，导致在适应新类别或未见领域时性能显著下降。为了解决这个问题，我们提出了DiSa，这是一个方向显著性感知提示学习框架，它集成了两种互补的正则化策略来提高泛化能力。首先，我们的跨交互式正则化（CIR）通过促进提示和冻结编码器之间的合作性学习来促进跨模态对齐。在CIR中，显著性感知掩码策略指导图像编码器优先处理语义上关键的图像区域，减少对信息较少区域的依赖。其次，我们引入了一种方向性正则化策略，该策略以方向方式对齐视觉嵌入和类原型特征，以优先确保特征方向的连续性而非严格接近。这种方法通过利用从类别均值统计得出的稳定原型方向来确保稳健泛化。在11个不同的图像分类基准测试上的广泛评估表明，DiSa在各种设置下始终优于最新提示学习方法，包括基本到新类别的泛化、跨数据集迁移、领域泛化和小样本学习。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19373v1">PDF</a> Accepted at the 31st ACM SIGKDD Conference on Knowledge Discovery and   Data Mining (KDD 2025)</p>
<p><strong>Summary</strong></p>
<p>本文介绍了针对视觉语言模型（如CLIP）下游任务的提示学习范式，并提出了一种名为DiSa的方向性显著性感知提示学习框架，以增强模型的泛化能力。该框架结合了两种互补的正则化策略：一是跨交互式正则化（CIR），通过合作式学习促进提示和冻结编码器之间的跨模态对齐；二是方向性正则化策略，以方向性方式对齐视觉嵌入和类原型特征，强调特征方向的稳定性。在广泛的图像分类基准测试中，DiSa表现出卓越的性能，在各种设置中均优于最新的提示学习方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DiSa是一个用于增强视觉语言模型泛化能力的方向性显著性感知提示学习框架。</li>
<li>结合两种互补的正则化策略：跨交互式正则化（CIR）和方向性正则化。</li>
<li>CIR通过合作式学习促进跨模态对齐，并引入显著性感知掩码策略来指导图像编码器的关注重点。</li>
<li>方向性正则化策略强调特征方向的稳定性，使视觉嵌入与类原型特征对齐。</li>
<li>DiSa通过利用稳定的原型方向，从类均值统计中衍生而来。</li>
<li>在广泛的图像分类基准测试中，DiSa在多种设置下均优于最新的提示学习方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19373">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-f3198aa09e17f01167d04f3738706f74.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6f6371aad24937072484eb2e54107d45.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c505e2115baf52c8baa60ed5a6aa96a4.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="CloneShield-A-Framework-for-Universal-Perturbation-Against-Zero-Shot-Voice-Cloning"><a href="#CloneShield-A-Framework-for-Universal-Perturbation-Against-Zero-Shot-Voice-Cloning" class="headerlink" title="CloneShield: A Framework for Universal Perturbation Against Zero-Shot   Voice Cloning"></a>CloneShield: A Framework for Universal Perturbation Against Zero-Shot   Voice Cloning</h2><p><strong>Authors:Renyuan Li, Zhibo Liang, Haichuan Zhang, Tianyu Shi, Zhiyuan Cheng, Jia Shi, Carl Yang, Mingjie Tang</strong></p>
<p>Recent breakthroughs in text-to-speech (TTS) voice cloning have raised serious privacy concerns, allowing highly accurate vocal identity replication from just a few seconds of reference audio, while retaining the speaker’s vocal authenticity. In this paper, we introduce CloneShield, a universal time-domain adversarial perturbation framework specifically designed to defend against zero-shot voice cloning. Our method provides protection that is robust across speakers and utterances, without requiring any prior knowledge of the synthesized text. We formulate perturbation generation as a multi-objective optimization problem, and propose Multi-Gradient Descent Algorithm (MGDA) to ensure the robust protection across diverse utterances. To preserve natural auditory perception for users, we decompose the adversarial perturbation via Mel-spectrogram representations and fine-tune it for each sample. This design ensures imperceptibility while maintaining strong degradation effects on zero-shot cloned outputs. Experiments on three state-of-the-art zero-shot TTS systems, five benchmark datasets and evaluations from 60 human listeners demonstrate that our method preserves near-original audio quality in protected inputs (PESQ &#x3D; 3.90, SRS &#x3D; 0.93) while substantially degrading both speaker similarity and speech quality in cloned samples (PESQ &#x3D; 1.07, SRS &#x3D; 0.08). </p>
<blockquote>
<p>在文本转语音（TTS）语音克隆领域的最新突破引发了严重的隐私担忧。该技术仅从几秒的参考音频就能实现高度精确的语音身份复制，同时保留说话者的语音真实性。本文介绍了CloneShield，这是一个专门设计用于防范零样本语音克隆的通用时域对抗扰动框架。我们的方法提供了跨说话者和话语的稳健保护，无需对合成文本有任何事先了解。我们将扰动生成制定为一个多目标优化问题，并提出多梯度下降算法（MGDA），以确保在不同的话语中提供稳健的保护。为了保持用户自然的听觉感知，我们通过梅尔频谱表示法分解对抗性扰动，并针对每个样本进行微调。这种设计确保了不可感知性，同时在对零样本克隆输出进行强降解时保持音频质量。在三个最先进的零样本TTS系统、五个基准数据集以及60名人类听者的评估实验表明，我们的方法在保证受保护输入接近原始音频质量的同时（PESQ &#x3D; 3.90，SRS &#x3D; 0.93），显著降低了克隆样本中的说话人相似度和语音质量（PESQ &#x3D; 1.07，SRS &#x3D; 0.08）。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19119v1">PDF</a> 10pages, 4figures</p>
<p><strong>Summary</strong></p>
<p>文本转语音（TTS）语音克隆技术的最新突破引发了严重的隐私担忧，因为仅需几秒的参考音频即可实现高度精确的语音身份复制，同时保持说话者的语音真实性。本文介绍了一种名为CloneShield的通用时间域对抗扰动框架，专门用于防范零样本语音克隆。该方法具有跨说话者和话语的鲁棒性保护能力，无需对合成文本有任何先验知识。我们制定扰动生成作为多目标优化问题，并提出多梯度下降算法（MGDA）以确保跨不同话语的鲁棒性保护。为了保持用户自然的听觉感知，我们通过梅尔频谱图表示将对抗性扰动分解，并针对每个样本进行微调。这一设计确保了不可察觉性，同时保持了对零样本克隆输出的强烈降解效果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文本转语音（TTS）语音克隆技术的新进展引发了关于隐私的担忧，因为能够利用极短参考音频进行精确的语音身份复制。</li>
<li>CloneShield框架被提出用于对抗零样本语音克隆，提供跨说话者和话语的鲁棒保护。</li>
<li>CloneShield不需要对合成文本有先验知识。</li>
<li>通过多目标优化问题和多梯度下降算法（MGDA）确保鲁棒性保护。</li>
<li>对抗扰动通过梅尔频谱图表示进行分解，并针对每个样本微调，以维持自然听觉感知并抵抗零样本克隆攻击。</li>
<li>实验结果表明，该保护方法能够在保护输入时保持接近原始音频质量，同时显著降低了克隆样本的说话人相似性和语音质量。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19119">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f9fa5d40cacd81f3f87412b0f4b1980b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f48159884e007271b45a6d7ef31becb6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-77d5ab548076a2a7d472a31acfc84848.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1b4dfc995d63dd1d5b5448a7a7a4384b.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="CLaDMoP-Learning-Transferrable-Models-from-Successful-Clinical-Trials-via-LLMs"><a href="#CLaDMoP-Learning-Transferrable-Models-from-Successful-Clinical-Trials-via-LLMs" class="headerlink" title="CLaDMoP: Learning Transferrable Models from Successful Clinical Trials   via LLMs"></a>CLaDMoP: Learning Transferrable Models from Successful Clinical Trials   via LLMs</h2><p><strong>Authors:Yiqing Zhang, Xiaozhong Liu, Fabricio Murai</strong></p>
<p>Many existing models for clinical trial outcome prediction are optimized using task-specific loss functions on trial phase-specific data. While this scheme may boost prediction for common diseases and drugs, it can hinder learning of generalizable representations, leading to more false positives&#x2F;negatives. To address this limitation, we introduce CLaDMoP, a new pre-training approach for clinical trial outcome prediction, alongside the Successful Clinical Trials dataset(SCT), specifically designed for this task. CLaDMoP leverages a Large Language Model-to encode trials’ eligibility criteria-linked to a lightweight Drug-Molecule branch through a novel multi-level fusion technique. To efficiently fuse long embeddings across levels, we incorporate a grouping block, drastically reducing computational overhead. CLaDMoP avoids reliance on task-specific objectives by pre-training on a “pair matching” proxy task. Compared to established zero-shot and few-shot baselines, our method significantly improves both PR-AUC and ROC-AUC, especially for phase I and phase II trials. We further evaluate and perform ablation on CLaDMoP after Parameter-Efficient Fine-Tuning, comparing it to state-of-the-art supervised baselines, including MEXA-CTP, on the Trial Outcome Prediction(TOP) benchmark. CLaDMoP achieves up to 10.5% improvement in PR-AUC and 3.6% in ROC-AUC, while attaining comparable F1 score to MEXA-CTP, highlighting its potential for clinical trial outcome prediction. Code and SCT dataset can be downloaded from <a target="_blank" rel="noopener" href="https://github.com/murai-lab/CLaDMoP">https://github.com/murai-lab/CLaDMoP</a>. </p>
<blockquote>
<p>现有许多用于临床试验结果预测的模型，它们通过使用针对特定任务的损失函数对特定试验阶段的数据进行优化。虽然这种方案可能会提高常见疾病和药物的预测能力，但它可能会阻碍通用表示的学习，从而导致更多的误报。为了解决这个问题，我们引入了CLaDMoP，这是一种新的临床试验结果预测预训练方法，以及为此任务专门设计的成功临床试验数据集（SCT）。CLaDMoP利用大型语言模型对试验资格标准进行编码，并与轻量级的药物分子分支通过新型的多层次融合技术相联系。为了有效地融合各级的长嵌入，我们引入了一个分组块，大大降低了计算开销。CLaDMoP通过在一个“配对匹配”代理任务上进行预训练，避免了依赖特定任务目标。与现有的零样本和少样本基线相比，我们的方法在PR-AUC和ROC-AUC上有了显著的提高，特别是在I期和II期试验阶段。我们进一步对CLaDMoP进行参数高效微调后的评估并进行消融实验，将其与包括MEXA-CTP在内的最新监督基线在临床试验结果预测基准测试集上进行比较。CLaDMoP在PR-AUC上提高了高达10.5%，在ROC-AUC上提高了3.6%，同时与MEXA-CTP的F1得分相当，这突显了其在临床试验结果预测方面的潜力。代码和SCT数据集可从<a target="_blank" rel="noopener" href="https://github.com/murai-lab/CLaDMoP%E4%B8%8B%E8%BD%BD%E3%80%82">https://github.com/murai-lab/CLaDMoP下载。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.18527v1">PDF</a> Accepted and to be published in KDD2025</p>
<p><strong>Summary</strong></p>
<p>CLaDMoP是一种针对临床试验结果预测的新型预训练策略，它引入了大型语言模型来编码与药物分子相关的试验资格标准。通过多层次融合技术，CLaDMoP能够更有效地融合长嵌入信息，从而提高预测准确性。该策略避免了依赖特定任务目标，而是通过“配对匹配”代理任务进行预训练。与现有的零样本和少样本基线相比，CLaDMoP在PR-AUC和ROC-AUC上表现更优，特别是在早期临床试验阶段。经过参数有效微调后，CLaDMoP与当前先进的监督基线相比，在PR-AUC上提高了10.5%，ROC-AUC提高了3.6%，同时保持了与MEXA-CTP相当的F1分数。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CLaDMoP是一种新的预训练策略，用于临床试验结果预测。</li>
<li>它结合了大型语言模型与药物分子信息，通过多层次融合技术处理数据。</li>
<li>CLaDMoP采用“配对匹配”代理任务进行预训练，不依赖特定任务目标。</li>
<li>与其他基线相比，CLaDMoP在PR-AUC和ROC-AUC指标上表现更优秀。</li>
<li>CLaDMoP在早期临床试验阶段（如I期和II期）具有显著优势。</li>
<li>经过参数有效微调后，CLaDMoP与当前先进的监督基线相比，取得了显著的性能提升。</li>
<li>CLaDMoP的代码和所用数据集可从指定链接下载。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.18527">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c4ae5d97b9465735d11ed9bebeaaed5f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5cce6383d86c2932898146e88267f9b8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-21718625b460849dd24f74044d8b47c0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f6cf771d2850f626d7cadc39c6aad172.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-dcfa2e09a6b64add34f684d4dc878cc1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f9580d2462c11a1020bc2cd297c0bfe2.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="MAPLE-Many-Shot-Adaptive-Pseudo-Labeling-for-In-Context-Learning"><a href="#MAPLE-Many-Shot-Adaptive-Pseudo-Labeling-for-In-Context-Learning" class="headerlink" title="MAPLE: Many-Shot Adaptive Pseudo-Labeling for In-Context Learning"></a>MAPLE: Many-Shot Adaptive Pseudo-Labeling for In-Context Learning</h2><p><strong>Authors:Zihan Chen, Song Wang, Zhen Tan, Jundong Li, Cong Shen</strong></p>
<p>In-Context Learning (ICL) empowers Large Language Models (LLMs) to tackle diverse tasks by incorporating multiple input-output examples, known as demonstrations, into the input of LLMs. More recently, advancements in the expanded context windows of LLMs have led to many-shot ICL, which uses hundreds of demonstrations and outperforms few-shot ICL, which relies on fewer examples. However, this approach is often hindered by the high cost of obtaining large amounts of labeled data. To address this challenge, we propose Many-Shot Adaptive Pseudo-LabEling, namely MAPLE, a novel influence-based many-shot ICL framework that utilizes pseudo-labeled samples to compensate for the lack of label information. We first identify a subset of impactful unlabeled samples and perform pseudo-labeling on them by querying LLMs. These pseudo-labeled samples are then adaptively selected and tailored to each test query as input to improve the performance of many-shot ICL, without significant labeling costs. Extensive experiments on real-world datasets demonstrate the effectiveness of our framework, showcasing its ability to enhance LLM adaptability and performance with limited labeled data. </p>
<blockquote>
<p>上下文学习（ICL）通过在大语言模型（LLM）的输入中融入多个输入输出示例（即演示），使其能够应对各种任务。最近，LLM扩展的上下文窗口的进步导致了多镜头ICL的出现，它使用数百个演示，并且表现优于依赖较少示例的少镜头ICL。然而，这种方法往往受到获取大量标记数据的高成本的阻碍。为了应对这一挑战，我们提出了基于影响的多镜头自适应伪标签技术（MAPLE）。这是一个新的多镜头ICL框架，它利用伪标签样本来弥补标签信息的缺失。我们首先识别出有影响力的未标记样本子集，并通过查询LLM对它们进行伪标签标注。这些伪标签样本随后被自适应地选择和定制为针对每个测试查询的输入，以提高多镜头ICL的性能，而无需增加大量的标记成本。在真实数据集上的大量实验证明了我们框架的有效性，展示了其在有限标记数据下提高LLM适应性和性能的能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16225v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>在上下文学习（ICL）中，大型语言模型（LLM）通过融入多个输入输出示例（即示范）来处理各种任务。最近，LLM扩展的上下文窗口的进步导致了多示例ICL的出现，它使用数百个示范，并优于依赖较少示例的few-shot ICL。然而，获取大量标记数据的成本高昂常常成为该方法的瓶颈。为解决这一挑战，我们提出了名为MAPLE的新型基于影响的多示例ICL框架，利用伪标记样本来弥补标签信息的不足。我们首先识别出有影响力的未标记样本子集，并通过查询LLM对它们进行伪标记。这些伪标记的样本随后被自适应地选择和定制为针对每个测试查询的输入，以提高多示例ICL的性能，同时无需大量的标记成本。在真实世界数据集上的广泛实验证明了我们的框架的有效性，展示了其在有限标记数据下提高LLM适应性和性能的能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ICL使LLM能够通过融入多个输入输出示例（即示范）来处理多种任务。</li>
<li>多示例ICL使用数百个示范，并通常优于few-shot ICL。</li>
<li>获取大量标记数据的成本高昂是ICL的一个挑战。</li>
<li>MAPLE是一种新型多示例ICL框架，利用伪标记样本来弥补标签信息的不足。</li>
<li>MAPLE通过识别有影响力的未标记样本并进行伪标记来工作。</li>
<li>伪标记的样本被自适应地选择和定制为针对每个测试查询的输入，提高多示例ICL的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16225">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-1a973b9fae5e602452608acc87477a61.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-23433cd48904f7524800a513b34c50ed.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f4e07756865fc8648ea5f8b6817b2541.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="CLEVER-A-Curated-Benchmark-for-Formally-Verified-Code-Generation"><a href="#CLEVER-A-Curated-Benchmark-for-Formally-Verified-Code-Generation" class="headerlink" title="CLEVER: A Curated Benchmark for Formally Verified Code Generation"></a>CLEVER: A Curated Benchmark for Formally Verified Code Generation</h2><p><strong>Authors:Amitayush Thakur, Jasper Lee, George Tsoukalas, Meghana Sistla, Matthew Zhao, Stefan Zetzsche, Greg Durrett, Yisong Yue, Swarat Chaudhuri</strong></p>
<p>We introduce ${\rm C{\small LEVER}}$, a high-quality, curated benchmark of 161 problems for end-to-end verified code generation in Lean. Each problem consists of (1) the task of generating a specification that matches a held-out ground-truth specification, and (2) the task of generating a Lean implementation that provably satisfies this specification. Unlike prior benchmarks, ${\rm C{\small LEVER}}$ avoids test-case supervision, LLM-generated annotations, and specifications that leak implementation logic or allow vacuous solutions. All outputs are verified post-hoc using Lean’s type checker to ensure machine-checkable correctness. We use ${\rm C{\small LEVER}}$ to evaluate several few-shot and agentic approaches based on state-of-the-art language models. These methods all struggle to achieve full verification, establishing it as a challenging frontier benchmark for program synthesis and formal reasoning. Our benchmark can be found on GitHub(<a target="_blank" rel="noopener" href="https://github.com/trishullab/clever">https://github.com/trishullab/clever</a>) as well as HuggingFace(<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/amitayusht/clever">https://huggingface.co/datasets/amitayusht/clever</a>). All our evaluation code is also available online(<a target="_blank" rel="noopener" href="https://github.com/trishullab/clever-prover">https://github.com/trishullab/clever-prover</a>). </p>
<blockquote>
<p>我们介绍了${\rm C{\small LEVER}}$，这是一个高质量的、经过筛选的包含161个问题的基准测试，用于端到端的Lean代码生成验证。每个问题由两部分组成：（1）生成与保留的真实规格相匹配的规格的任务；（2）生成可证明满足此规格要求的Lean实现的任务。不同于以往的基准测试，${\rm C{\small LEVER}}$避免了测试用例的监督、LLM生成的注释以及泄露实现逻辑或允许空洞解决方案的规格。所有输出都使用Lean的类型检查器进行事后验证，以确保可机器检查的正确性。我们使用${\rm C{\small LEVER}}$来评估基于最新语言模型的几种少镜头和自主方法。这些方法在全面实现验证方面均面临困难，这使其成为程序合成和形式推理的前瞻性挑战基准测试。我们的基准测试可以在GitHub（<a target="_blank" rel="noopener" href="https://github.com/trishullab/clever%EF%BC%89%E4%BB%A5%E5%8F%8AHuggingFace%EF%BC%88https://huggingface.co/datasets/amitayusht/clever%EF%BC%89%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82%E6%89%80%E6%9C%89%E7%9A%84%E8%AF%84%E4%BC%B0%E4%BB%A3%E7%A0%81%E4%B9%9F%E9%83%BD%E5%9C%A8%E7%BD%91%E4%B8%8A%E5%8F%AF%E7%94%A8%EF%BC%88https://github.com/trishullab/clever-prover%EF%BC%89%E3%80%82">https://github.com/trishullab/clever）以及HuggingFace（https://huggingface.co/datasets/amitayusht/clever）上找到。所有的评估代码也都在网上可用（https://github.com/trishullab/clever-prover）。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13938v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>${\rm C{\small LEVER}}$是一个高质量的、针对端对端验证的代码生成任务的基准测试集，包含161个问题。它避免了测试用例监督、大型语言模型生成的注解以及泄露实现逻辑或允许空洞解决方案的规格。所有输出都使用Lean的类型检查器进行事后验证，以确保机器可检查的正确性。该基准测试集用于评估基于最新语言模型的几种小样本和智能方法，但现有的方法都难以实现完全验证，使其成为程序合成和形式推理的具有挑战性的前沿基准测试集。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>${\rm C{\small LEVER}}$是一个用于端到端验证的代码生成任务的基准测试集，包含161个高质量问题。</li>
<li>它避免了测试用例监督、大型语言模型生成的注解以及可能导致错误或空洞解决方案的规格。</li>
<li>所有输出都使用Lean的类型检查器进行事后验证，以确保机器可检查的正确性。</li>
<li>${\rm C{\small LEVER}}$被用来评估最新的语言模型在程序合成方面的能力。</li>
<li>目前的方法在完全验证方面存在困难，表明这是一个具有挑战性的前沿基准测试集。</li>
<li>该基准测试集可以在GitHub和HuggingFace上找到。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13938">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-768e2c574d893c6a85068eaf021ee65d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bf4bb3beeb9a906f276484cb351bd82c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-059b0011d1e2a8174241dab92289ec5b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4e1a8461994df8449d3c7925819fe90e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-92dbad38362c5c981f79f4e240389835.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="SPKLIP-Aligning-Spike-Video-Streams-with-Natural-Language"><a href="#SPKLIP-Aligning-Spike-Video-Streams-with-Natural-Language" class="headerlink" title="SPKLIP: Aligning Spike Video Streams with Natural Language"></a>SPKLIP: Aligning Spike Video Streams with Natural Language</h2><p><strong>Authors:Yongchang Gao, Meiling Jin, Zhaofei Yu, Tiejun Huang, Guozhang Chen</strong></p>
<p>Spike cameras offer unique sensing capabilities but their sparse, asynchronous output challenges semantic understanding, especially for Spike Video-Language Alignment (Spike-VLA) where models like CLIP underperform due to modality mismatch. We introduce SPKLIP, the first architecture specifically for Spike-VLA. SPKLIP employs a hierarchical spike feature extractor that adaptively models multi-scale temporal dynamics in event streams, and uses spike-text contrastive learning to directly align spike video with language, enabling effective few-shot learning. A full-spiking visual encoder variant, integrating SNN components into our pipeline, demonstrates enhanced energy efficiency. Experiments show state-of-the-art performance on benchmark spike datasets and strong few-shot generalization on a newly contributed real-world dataset. SPKLIP’s energy efficiency highlights its potential for neuromorphic deployment, advancing event-based multimodal research. The source code and dataset are available at [link removed for anonymity]. </p>
<blockquote>
<p>脉冲摄像头提供了独特的感知能力，但其稀疏、异步的输出给语义理解带来了挑战，特别是对于脉冲视频语言对齐（Spike-VLA）而言，CLIP等模型由于模态不匹配而表现不佳。我们引入了专门用于Spike-VLA的SPKLIP架构。SPKLIP采用分层脉冲特征提取器，自适应地模拟事件流中的多尺度时间动态，并使用脉冲文本对比学习来直接对齐脉冲视频和语言，从而实现有效的少样本学习。一种全脉冲视觉编码器变体将脉冲神经网络（SNN）组件集成到我们的流程中，展现了更高的能效。实验表明在基准脉冲数据集上达到了最先进的性能，并在新贡献的现实数据集上表现出强大的少样本泛化能力。SPKLIP的能效突显了其用于神经形态部署的潜力，推动了基于事件的多媒体研究的发展。源代码和数据集可通过[匿名链接]获取。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.12656v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>SPKLIP是针对Spike-VLA任务的首个架构，采用分层脉冲特征提取器自适应建模事件流中的多尺度时间动态，并利用脉冲文本对比学习直接对齐脉冲视频与语言，从而实现有效的少样本学习。推出全脉冲视觉编码器变体，将SNN组件集成到我们的流程中，展现了出色的能源效率。实验表明，该架构在基准脉冲数据集上表现领先，并在新推出的现实数据集上具有较强的少样本泛化能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SPKLIP是专门为Spike-VLA设计的首个架构。</li>
<li>采用分层脉冲特征提取器自适应建模事件流中的多尺度时间动态。</li>
<li>利用脉冲文本对比学习直接对齐脉冲视频与语言，实现有效的少样本学习。</li>
<li>全脉冲视觉编码器变体集成了SNN组件，提高了能源效率。</li>
<li>在基准脉冲数据集上表现领先。</li>
<li>在新推出的现实数据集上具有较强的少样本泛化能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.12656">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b720906c979e6418294c27e7d5db905c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e1b2de04e1396392b6fe71d56e208032.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e541bee5d5be2d1f227d87073a38d951.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a7ed586859797a3fedcaeeeac70401a7.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="LLM-based-Prompt-Ensemble-for-Reliable-Medical-Entity-Recognition-from-EHRs"><a href="#LLM-based-Prompt-Ensemble-for-Reliable-Medical-Entity-Recognition-from-EHRs" class="headerlink" title="LLM-based Prompt Ensemble for Reliable Medical Entity Recognition from   EHRs"></a>LLM-based Prompt Ensemble for Reliable Medical Entity Recognition from   EHRs</h2><p><strong>Authors:K M Sajjadul Islam, Ayesha Siddika Nipu, Jiawei Wu, Praveen Madiraju</strong></p>
<p>Electronic Health Records (EHRs) are digital records of patient information, often containing unstructured clinical text. Named Entity Recognition (NER) is essential in EHRs for extracting key medical entities like problems, tests, and treatments to support downstream clinical applications. This paper explores prompt-based medical entity recognition using large language models (LLMs), specifically GPT-4o and DeepSeek-R1, guided by various prompt engineering techniques, including zero-shot, few-shot, and an ensemble approach. Among all strategies, GPT-4o with prompt ensemble achieved the highest classification performance with an F1-score of 0.95 and recall of 0.98, outperforming DeepSeek-R1 on the task. The ensemble method improved reliability by aggregating outputs through embedding-based similarity and majority voting. </p>
<blockquote>
<p>电子健康记录（EHRs）是患者信息的数字记录，通常包含非结构的临床文本。在EHRs中，命名实体识别（NER）对于提取关键医疗实体至关重要，如疾病、检查和治疗方法等，为下游临床应用提供支持。本文探讨了基于大型语言模型的医疗实体识别，尤其是使用各种提示工程技术指导的GPT-4o和DeepSeek-R1。这些技术包括零样本、少样本和集成方法。在所有策略中，GPT-4o与提示集成取得了最高的分类性能，F1分数为0.95，召回率为0.98，在任务上优于DeepSeek-R1。集成方法通过基于嵌入的相似性和多数投票法聚合输出，提高了可靠性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.08704v2">PDF</a> IEEE 26th International Conference on Information Reuse and   Integration for Data Science (IRI 2025), San Jose, CA, USA</p>
<p><strong>Summary</strong></p>
<p>本文主要探讨了基于大型语言模型（LLMs）的命名实体识别技术在电子健康记录（EHRs）中的应用。通过采用GPT-4o模型和DeepSeek-R1模型，结合多种提示工程技术（如零样本、小样例和集成方法），进行医学实体识别的研究。结果显示，GPT-4o模型采用提示集成策略在分类性能上表现最佳，F1分数达到0.95，召回率达到0.98，优于DeepSeek-R1模型。集成方法通过基于嵌入的相似性和多数投票法聚合输出，提高了可靠性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>电子健康记录（EHRs）中的命名实体识别（NER）对于提取关键医疗实体至关重要，如问题、测试和治疗方法，以支持下游临床应用。</li>
<li>大型语言模型（LLMs）在医学实体识别中展现出潜力。</li>
<li>GPT-4o模型和DeepSeek-R1模型被用于医学实体识别的研究。</li>
<li>提示工程技术，包括零样本、小样例和集成方法，在医学实体识别中发挥了作用。</li>
<li>GPT-4o模型采用提示集成策略在分类性能上表现最佳，F1分数和召回率均较高。</li>
<li>集成方法通过基于嵌入的相似性和多数投票法提高了可靠性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.08704">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-cf946109af6fd0062a644329ea718e8d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-629d509339aac6aaa5a0276c0c40c6b5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e61795bda67a7a018dcf609069aa2c3a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3921432b3d1e2a445e857c03a2917396.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ee8a0816c818be8f3d8ad48739197dc9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2ea5b898bc1b1646a2ede2a83cab71e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8f977aab0ec910fe8a2200e0df2a0b50.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dbb6368dff6e6695e00263ec08182ee1.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="CacheFL-Privacy-Preserving-and-Efficient-Federated-Cache-Model-Fine-Tuning-for-Vision-Language-Models"><a href="#CacheFL-Privacy-Preserving-and-Efficient-Federated-Cache-Model-Fine-Tuning-for-Vision-Language-Models" class="headerlink" title="CacheFL: Privacy-Preserving and Efficient Federated Cache Model   Fine-Tuning for Vision-Language Models"></a>CacheFL: Privacy-Preserving and Efficient Federated Cache Model   Fine-Tuning for Vision-Language Models</h2><p><strong>Authors:Mengjun Yi, Hanwen Zhang, Hui Dou, Jian Zhao, Furao Shen</strong></p>
<p>Large pre-trained Vision-Language Models (VLMs), such as Contrastive Language-Image Pre-training (CLIP), have exhibited remarkable zero-shot performance across various image classification tasks. Fine-tuning these models on domain-specific datasets further enhances their effectiveness for downstream applications. However, fine-tuning in cloud environments raises significant concerns regarding data security and privacy. Federated Learning (FL) offers a decentralized solution by enabling model training across local clients without centralizing sensitive data, but the high communication and computation costs of transmitting full pre-trained models during training limit its scalability. Additionally, non-Independent and Identically Distributed (non-IID) data across local clients can negatively impact model convergence and performance. To address these challenges, we propose CacheFL, a novel federated learning method that replaces traditional full model fine-tuning with lightweight cache model fine-tuning. The cache model is initialized using a class-balanced dataset generated by a generative pre-trained model, effectively mitigating the impact of non-IID data. This cache model is then distributed to local clients for fine-tuning, and the updated parameters from each client are aggregated on the server and redistributed. With the updated cache model, the classification performance of CLIP is improved after just a few epochs. By limiting the training and communication to the cache model, CacheFL significantly reduces resource demands while ensuring data privacy and security. Extensive experiments conducted on ImageNet and 10 additional datasets demonstrate that CacheFL outperforms traditional approaches in terms of classification accuracy, resource efficiency, and privacy preservation. </p>
<blockquote>
<p>大型预训练视觉语言模型（VLMs），如对比语言图像预训练（CLIP），在各种图像分类任务中展现出了卓越的零样本性能。在特定领域数据集上对这些模型进行微调，可以进一步提高其下游应用的有效性。然而，在云环境中进行微调引发了关于数据安全和隐私的重大担忧。联邦学习（FL）提供了一种分散的解决方案，通过支持本地客户端的模型训练，无需集中敏感数据。然而，在训练过程中传输完整的预训练模型的高通信和计算成本限制了其可扩展性。此外，本地客户端之间的非独立同分布（non-IID）数据可能会对模型收敛和性能产生负面影响。为了解决这些挑战，我们提出了CacheFL，这是一种新型的联邦学习方法，它用轻量级的缓存模型微调取代了传统的全模型微调。缓存模型使用由生成式预训练模型生成的类别平衡数据集进行初始化，有效地减轻了非IID数据的影响。然后，该缓存模型将分发给本地客户端进行微调，从每个客户端更新的参数将在服务器上聚合并重新分发。仅通过几个周期的训练，CLIP的分类性能就能通过更新的缓存模型得到提高。通过将训练和通信限制在缓存模型上，CacheFL在降低资源需求的同时确保了数据隐私和安全。在ImageNet和另外10个数据集上进行的广泛实验表明，CacheFL在分类精度、资源效率和隐私保护方面均优于传统方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.05130v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型预训练视觉语言模型（VLMs），如CLIP，在不同图像分类任务中表现出出色的零样本性能。通过领域特定数据集进行微调可提高其在下游应用中的有效性。但在云环境中进行微调引发数据安全和隐私的忧虑。联邦学习（FL）通过本地客户端进行模型训练而不集中敏感数据来提供分散解决方案，但传输预训练模型时的通信和计算成本限制了其可扩展性。此外，本地客户端的非独立同分布（non-IID）数据可能不利于模型收敛和性能。为解决这些挑战，我们提出CacheFL这一新型联邦学习方法，以轻量化缓存模型微调替代传统全模型微调。缓存模型使用生成式预训练模型生成的类别平衡数据集进行初始化，有效减轻非IID数据的影响。缓存模型随后分发给本地客户端进行微调，从每个客户端更新的参数在服务器上聚合后重新分发。使用更新后的缓存模型，只需几个周期即可提高CLIP的分类性能。通过限制训练和通信仅限于缓存模型，CacheFL在资源需求方面大大减少了负担，同时确保了数据隐私和安全。在ImageNet和另外10个数据集上进行的广泛实验表明，CacheFL在分类精度、资源效率和隐私保护方面优于传统方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型预训练视觉语言模型（VLMs）如CLIP在图像分类任务中表现出色，但需要通过微调进一步提高性能。</li>
<li>云环境中的模型微调引发数据安全和隐私的担忧。</li>
<li>联邦学习（FL）提供了一种解决方案，但在传输大型预训练模型时面临通信和计算成本高的挑战。</li>
<li>CacheFL是一种新型联邦学习方法，通过轻量化缓存模型微调替代传统全模型微调，提高了资源效率和隐私保护。</li>
<li>CacheFL使用生成式预训练模型和类别平衡数据集初始化缓存模型，有效应对非IID数据挑战。</li>
<li>缓存模型的更新参数通过聚合和重新分发，仅需要少量周期就能提高模型的分类性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.05130">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-8664d15abb078cedbed770f84599277d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-237bbf25cb4db68595b57a9d0a6c18a1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f47d9372d408f932721d5b66875185a4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-be521c3dab6a8931677c8edfe47aada2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5a2bb12bb61e8d8a7a778ae341b94c56.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Identifying-Legal-Holdings-with-LLMs-A-Systematic-Study-of-Performance-Scale-and-Memorization"><a href="#Identifying-Legal-Holdings-with-LLMs-A-Systematic-Study-of-Performance-Scale-and-Memorization" class="headerlink" title="Identifying Legal Holdings with LLMs: A Systematic Study of Performance,   Scale, and Memorization"></a>Identifying Legal Holdings with LLMs: A Systematic Study of Performance,   Scale, and Memorization</h2><p><strong>Authors:Chuck Arvin</strong></p>
<p>As large language models (LLMs) continue to advance in capabilities, it is essential to assess how they perform on established benchmarks. In this study, we present a suite of experiments to assess the performance of modern LLMs (ranging from 3B to 90B+ parameters) on CaseHOLD, a legal benchmark dataset for identifying case holdings. Our experiments demonstrate scaling effects - performance on this task improves with model size, with more capable models like GPT4o and AmazonNovaPro achieving macro F1 scores of 0.744 and 0.720 respectively. These scores are competitive with the best published results on this dataset, and do not require any technically sophisticated model training, fine-tuning or few-shot prompting. To ensure that these strong results are not due to memorization of judicial opinions contained in the training data, we develop and utilize a novel citation anonymization test that preserves semantic meaning while ensuring case names and citations are fictitious. Models maintain strong performance under these conditions (macro F1 of 0.728), suggesting the performance is not due to rote memorization. These findings demonstrate both the promise and current limitations of LLMs for legal tasks with important implications for the development and measurement of automated legal analytics and legal benchmarks. </p>
<blockquote>
<p>随着大型语言模型（LLM）的能力不断提升，评估它们在既定基准测试上的表现变得至关重要。本研究中，我们进行了一系列实验，旨在评估现代LLM（参数范围从3B到90B+）在CaseHOLD这一法律基准数据集上的表现，该数据集用于识别判例法摘要。我们的实验展示了规模效益——随着模型规模的扩大，此任务的性能得到提升。更先进的模型如GPT4o和AmazonNovaPro分别获得了0.744和0.720的宏观F1分数。这些分数与数据集上的最佳已发布结果具有竞争力，并且不需要任何技术复杂模型的训练、微调或少样本提示。为确保这些强有力的结果并非由于训练数据中司法意见的机械化记忆，我们开发并利用了一种新型引文匿名测试，该测试在保留语义的同时确保案件名称和引文是虚构的。在这些条件下，模型保持强大的性能（宏观F1为0.728），表明其表现并非由于死记硬背。这些发现展示了LLM在法律任务上的潜力以及当前存在的局限性，对于自动法律分析和法律基准的开发和衡量具有重要意义。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.02172v3">PDF</a> Presented as a short paper at International Conference on Artificial   Intelligence and Law 2025 (Chicago, IL)</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLMs）在能力上不断进步，本研究通过一系列实验评估了现代LLMs在CaseHOLD法律基准数据集上的表现，用于识别案例持有。实验表明，模型规模扩大有助于提高任务性能，GPT4o和AmazonNovaPro等更先进的模型取得了有竞争力的成绩。此外，模型表现出强大的性能，即使经过新型引文匿名化测试也依然如此。这为法律任务的自动化分析和基准测试提供了重要启示。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLMs）在CaseHOLD法律基准数据集上的性能评估很重要。</li>
<li>模型规模扩大有助于提高任务性能。</li>
<li>GPT4o和AmazonNovaPro等先进模型在CaseHOLD数据集上取得了有竞争力的成绩。</li>
<li>模型表现出强大的性能，即使经过新型引文匿名化测试也依然稳健。</li>
<li>模型性能并非仅仅因为对训练数据中司法意见的刻板记忆。</li>
<li>LLMs在法律任务上的潜力和当前局限性得到了展示。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.02172">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-4bfd996a19054604a9107c691781067e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-71bc7606223dcc74f30def7c8527a088.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-75e2103642a9da7683094663aaf68369.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-959dfe168ca066c99cb2d84366311d6a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7a129b5a293125fa43d20f74c4504e61.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Probabilistic-Interactive-3D-Segmentation-with-Hierarchical-Neural-Processes"><a href="#Probabilistic-Interactive-3D-Segmentation-with-Hierarchical-Neural-Processes" class="headerlink" title="Probabilistic Interactive 3D Segmentation with Hierarchical Neural   Processes"></a>Probabilistic Interactive 3D Segmentation with Hierarchical Neural   Processes</h2><p><strong>Authors:Jie Liu, Pan Zhou, Zehao Xiao, Jiayi Shen, Wenzhe Yin, Jan-Jakob Sonke, Efstratios Gavves</strong></p>
<p>Interactive 3D segmentation has emerged as a promising solution for generating accurate object masks in complex 3D scenes by incorporating user-provided clicks. However, two critical challenges remain underexplored: (1) effectively generalizing from sparse user clicks to produce accurate segmentation, and (2) quantifying predictive uncertainty to help users identify unreliable regions. In this work, we propose NPISeg3D, a novel probabilistic framework that builds upon Neural Processes (NPs) to address these challenges. Specifically, NPISeg3D introduces a hierarchical latent variable structure with scene-specific and object-specific latent variables to enhance few-shot generalization by capturing both global context and object-specific characteristics. Additionally, we design a probabilistic prototype modulator that adaptively modulates click prototypes with object-specific latent variables, improving the model’s ability to capture object-aware context and quantify predictive uncertainty. Experiments on four 3D point cloud datasets demonstrate that NPISeg3D achieves superior segmentation performance with fewer clicks while providing reliable uncertainty estimations. </p>
<blockquote>
<p>交互式3D分割技术通过结合用户提供的点击，在复杂的3D场景中生成精确的对象掩膜方面展现出巨大的潜力。然而，还有两个关键挑战尚未得到充分探索：（1）如何从稀疏的用户点击中有效概括出准确的分割结果；（2）量化预测不确定性，帮助用户识别不可靠的区域。在这项工作中，我们提出了NPISeg3D，一个基于神经网络过程（NPs）的新型概率框架，以解决这些挑战。具体来说，NPISeg3D引入了一种分层潜在变量结构，包括场景特定和对象特定的潜在变量，通过捕捉全局上下文和对象特定特征，增强小样本泛化能力。此外，我们设计了一个概率原型调制器，它自适应地利用对象特定的潜在变量调整点击原型，提高了模型捕捉对象感知上下文和量化预测不确定性的能力。在四个3D点云数据集上的实验表明，NPISeg3D在点击次数更少的情况下实现了优越的分割性能，同时提供了可靠的不确定性估计。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.01726v2">PDF</a> ICML 2025 Proceedings</p>
<p><strong>Summary</strong></p>
<p>该文探讨了在复杂三维场景中进行交互式三维分割时面临的挑战，特别是从稀疏用户点击推广到准确分割以及量化预测不确定性两个关键问题。为此，提出了一种基于神经过程的概率框架NPISeg3D，通过引入层次化潜在变量结构和概率原型调制器，提高少样本推广能力和不确定性量化能力。实验证明，NPISeg3D在四个三维点云数据集上实现了更少的点击次数即可获得优越的分隔性能，同时提供了可靠的不确定性估计。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>交互式三维分割是生成复杂三维场景中准确对象掩模的有前途的解决方案。</li>
<li>从稀疏用户点击推广到准确分割和量化预测不确定性是尚未充分研究的挑战。</li>
<li>NPISeg3D是一种基于神经过程的概率框架，旨在解决这两个挑战。</li>
<li>NPISeg3D通过引入层次化潜在变量结构，增强了对全局上下文和对象特定特征的捕捉能力。</li>
<li>概率原型调制器的设计能够自适应地调节点击原型与对象特定潜在变量，提高模型捕捉对象感知上下文和量化预测不确定性的能力。</li>
<li>实验证明NPISeg3D在四个三维点云数据集上实现了优越的分隔性能。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.01726">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-3498888392d09f36db7959d498544a4b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8691076e0be62e1bc1629c4624cfb469.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6fac8d588025dd44bcba0b2dbcd1c55f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-73706838dc030343b05743f08730f685.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Single-Agent-vs-Multi-Agent-LLM-Strategies-for-Automated-Student-Reflection-Assessment"><a href="#Single-Agent-vs-Multi-Agent-LLM-Strategies-for-Automated-Student-Reflection-Assessment" class="headerlink" title="Single-Agent vs. Multi-Agent LLM Strategies for Automated Student   Reflection Assessment"></a>Single-Agent vs. Multi-Agent LLM Strategies for Automated Student   Reflection Assessment</h2><p><strong>Authors:Gen Li, Li Chen, Cheng Tang, Valdemar Švábenský, Daisuke Deguchi, Takayoshi Yamashita, Atsushi Shimada</strong></p>
<p>We explore the use of Large Language Models (LLMs) for automated assessment of open-text student reflections and prediction of academic performance. Traditional methods for evaluating reflections are time-consuming and may not scale effectively in educational settings. In this work, we employ LLMs to transform student reflections into quantitative scores using two assessment strategies (single-agent and multi-agent) and two prompting techniques (zero-shot and few-shot). Our experiments, conducted on a dataset of 5,278 reflections from 377 students over three academic terms, demonstrate that the single-agent with few-shot strategy achieves the highest match rate with human evaluations. Furthermore, models utilizing LLM-assessed reflection scores outperform baselines in both at-risk student identification and grade prediction tasks. These findings suggest that LLMs can effectively automate reflection assessment, reduce educators’ workload, and enable timely support for students who may need additional assistance. Our work emphasizes the potential of integrating advanced generative AI technologies into educational practices to enhance student engagement and academic success. </p>
<blockquote>
<p>我们探讨了大型语言模型（LLM）在自动评估学生开放性反思和预测学业表现方面的应用。传统的反思评估方法耗时且在教育环境中可能无法有效扩展。在这项工作中，我们采用LLM，使用两种评估策略（单智能体和多智能体）和两种提示技术（零样本和少样本），将学生反思转化为量化分数。我们在包含来自377名学生在三个学术学期内共5,278篇反思的数据集上进行的实验表明，采用少样本策略的单智能体在人为评估方面的匹配率最高。此外，利用LLM评估的反思得分的模型在处于危险的学生识别和成绩预测任务中的表现均优于基线。这些发现表明，LLM可以有效地自动进行反思评估，减少教育工作者的工作量，并为可能需要额外帮助的学生提供及时的支持。我们的工作强调了将先进的生成式AI技术融入教育实践，以提高学生参与度和学业成就的可能性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05716v2">PDF</a> Published in Proceedings of the 29th Pacific-Asia Conference on   Knowledge Discovery and Data Mining (PAKDD 2025)</p>
<p><strong>Summary</strong></p>
<p>本文探讨了使用大型语言模型（LLMs）自动评估学生开放文本反思和预测学业成绩的方法。传统评估反思的方法耗时且难以在教育环境中有效扩展。本研究利用LLMs将学生的反思转化为量化分数，采用两种评估策略（单代理和多代理）和两种提示技术（零射击和少射击）。在来自377名学生三个学期的5278篇反思数据集上进行的实验表明，采用少射击策略的单代理方法与人类评价的匹配度最高。此外，利用LLM评估的反思分数的模型在风险学生识别和成绩预测任务中的表现均优于基线。这表明LLMs可以有效自动化反思评估，减轻教育工作者的工作量，并为可能需要额外帮助的学生提供及时的支持。本研究强调了将先进的生成性AI技术融入教育实践以提高学生参与度和学业成功的潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLMs）可用于自动评估学生的开放文本反思和预测学业成绩。</li>
<li>传统评估反思的方法存在耗时且难以有效扩展的问题。</li>
<li>采用单代理和少射击策略的LLM在匹配人类评价方面表现最佳。</li>
<li>LLMs在风险学生识别和成绩预测任务中的表现优于基线方法。</li>
<li>LLMs能有效自动化反思评估，减轻教师的工作负担。</li>
<li>LLMs可为学生提供及时的支持，特别是那些可能需要额外帮助的学生。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05716">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-741679233d151bf4b93641dce6c81b5b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9f36f8bc2ee799fda8b05d31568d376e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a1e58fb5bda2006bfef3be73859d505f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0ff0799879c85a715d451fa0ee35b3a8.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Time-VLM-Exploring-Multimodal-Vision-Language-Models-for-Augmented-Time-Series-Forecasting"><a href="#Time-VLM-Exploring-Multimodal-Vision-Language-Models-for-Augmented-Time-Series-Forecasting" class="headerlink" title="Time-VLM: Exploring Multimodal Vision-Language Models for Augmented Time   Series Forecasting"></a>Time-VLM: Exploring Multimodal Vision-Language Models for Augmented Time   Series Forecasting</h2><p><strong>Authors:Siru Zhong, Weilin Ruan, Ming Jin, Huan Li, Qingsong Wen, Yuxuan Liang</strong></p>
<p>Recent advancements in time series forecasting have explored augmenting models with text or vision modalities to improve accuracy. While text provides contextual understanding, it often lacks fine-grained temporal details. Conversely, vision captures intricate temporal patterns but lacks semantic context, limiting the complementary potential of these modalities. To address this, we propose \method, a novel multimodal framework that leverages pre-trained Vision-Language Models (VLMs) to bridge temporal, visual, and textual modalities for enhanced forecasting. Our framework comprises three key components: (1) a Retrieval-Augmented Learner, which extracts enriched temporal features through memory bank interactions; (2) a Vision-Augmented Learner, which encodes time series as informative images; and (3) a Text-Augmented Learner, which generates contextual textual descriptions. These components collaborate with frozen pre-trained VLMs to produce multimodal embeddings, which are then fused with temporal features for final prediction. Extensive experiments demonstrate that Time-VLM achieves superior performance, particularly in few-shot and zero-shot scenarios, thereby establishing a new direction for multimodal time series forecasting. Code is available at <a target="_blank" rel="noopener" href="https://github.com/CityMind-Lab/ICML25-TimeVLM">https://github.com/CityMind-Lab/ICML25-TimeVLM</a>. </p>
<blockquote>
<p>近期时间序列预测领域的进展探索了通过添加文本或视觉模式来增强模型以提高预测精度的方法。虽然文本提供了上下文理解，但它常常缺乏精细的时间细节。相反，视觉模式捕捉到了复杂的时间模式，但缺乏语义上下文，限制了这些模式的互补潜力。为了解决这一问题，我们提出了\method（方法）这一新型多模式框架，它利用预训练的视觉语言模型（VLMs）来桥接时间、视觉和文本模式以增强预测能力。我们的框架包含三个关键组成部分：（1）检索增强学习者，它通过内存银行互动提取丰富的时间特征；（2）视觉增强学习者，它将时间序列编码为信息图像；（3）文本增强学习者，它生成上下文文本描述。这三个组成部分与冻结的预训练VLMs协作，生成多模式嵌入，然后其与时间特征融合以进行最终预测。大量实验表明，Time-VLM（时间-视觉语言模型）在少样本和无样本场景下实现了卓越的性能，为多媒体时间序列预测开辟了新的方向。代码可通过<a target="_blank" rel="noopener" href="https://github.com/CityMind-Lab/ICML25-TimeVLM%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/CityMind-Lab/ICML25-TimeVLM获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.04395v2">PDF</a> 20 pages</p>
<p><strong>Summary</strong></p>
<p>本文介绍了针对时间序列预测的新进展，提出了一种新的多模态框架Time-VLM，该框架结合了预训练的视觉语言模型（VLMs）来融合时间、视觉和文本模态，以提高预测的准确性。该框架包括三个关键组件：检索增强学习者、视觉增强学习者和文本增强学习者。实验证明，Time-VLM在少样本和无样本场景下具有卓越性能，为多媒体时间序列预测提供了新的方向。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文本提供上下文理解但可能缺乏精细的时间细节，而视觉捕捉复杂的时间模式但缺乏语义上下文。</li>
<li>Time-VLM框架利用预训练的视觉语言模型（VLMs）来融合时间、视觉和文本模态。</li>
<li>Time-VLM框架包括三个关键组件：检索增强学习者、视觉增强学习者和文本增强学习者。</li>
<li>检索增强学习者通过内存银行交互提取丰富的时间特征。</li>
<li>视觉增强学习者将时间序列编码为信息图像。</li>
<li>文本增强学习者生成上下文文本描述。</li>
<li>Time-VLM框架在少样本和无样本场景下的预测性能卓越，为多媒体时间序列预测提供了新的方向。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.04395">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-488dd4c73e871e954245cddc5b225e3e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c2e08b0b3a07809d91ee2b85263c5e9d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c88a486fa0d42512233734cf7eea5f24.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Are-Transformers-Able-to-Reason-by-Connecting-Separated-Knowledge-in-Training-Data"><a href="#Are-Transformers-Able-to-Reason-by-Connecting-Separated-Knowledge-in-Training-Data" class="headerlink" title="Are Transformers Able to Reason by Connecting Separated Knowledge in   Training Data?"></a>Are Transformers Able to Reason by Connecting Separated Knowledge in   Training Data?</h2><p><strong>Authors:Yutong Yin, Zhaoran Wang</strong></p>
<p>Humans exhibit remarkable compositional reasoning by integrating knowledge from various sources. For example, if someone learns ( B &#x3D; f(A) ) from one source and ( C &#x3D; g(B) ) from another, they can deduce ( C&#x3D;g(B)&#x3D;g(f(A)) ) even without encountering ( ABC ) together, showcasing the generalization ability of human intelligence. In this paper, we introduce a synthetic learning task, “FTCT” (Fragmented at Training, Chained at Testing), to validate the potential of Transformers in replicating this skill and interpret its inner mechanism. In the training phase, data consist of separated knowledge fragments from an overall causal graph. During testing, Transformers must infer complete causal graph traces by integrating these fragments. Our findings demonstrate that few-shot Chain-of-Thought prompting enables Transformers to perform compositional reasoning on FTCT by revealing correct combinations of fragments, even if such combinations were absent in the training data. Furthermore, the emergence of compositional reasoning ability is strongly correlated with the model complexity and training-testing data similarity. We propose, both theoretically and empirically, that Transformers learn an underlying generalizable program from training, enabling effective compositional reasoning during testing. </p>
<blockquote>
<p>人类能够通过整合来自不同来源的知识展现出惊人的组合推理能力。例如，如果有人从某一来源学习到（B&#x3D;f（A）），并从另一来源学习到（C&#x3D;g（B）），那么即使没有同时遇到（ABC），他们也能推导出（C&#x3D;g（B）&#x3D;g（f（A））），这展示了人类智力的泛化能力。在本文中，我们引入了一项合成学习任务“FTCT”（训练时碎片化，测试时链接），以验证Transformer在复制这项技能方面的潜力并解释其内在机制。在训练阶段，数据由来自整体因果图的分离知识片段组成。在测试阶段，Transformer必须通过整合这些片段来推断完整的因果图轨迹。我们的研究结果表明，即使这些组合在训练数据中不存在，基于少量碎片的“Chain-of-Thought”提示也能使Transformer在FTCT上执行组合推理，揭示正确的碎片组合。此外，组合推理能力的出现与模型复杂度和训练-测试数据相似性密切相关。我们从理论和实践两方面提出，Transformer通过训练学习到一个可泛化的底层程序，从而在测试时能够实现有效的组合推理。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.15857v6">PDF</a> Accepted by ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了人类具有出色的组合推理能力，能够整合不同来源的知识。文中通过一个示例阐述了人类如何运用组合推理进行推理过程。针对这种能力，本文提出了一个名为“FTCT”（训练时碎片化，测试时串联化）的合成学习任务来验证Transformer模型是否具备这种能力并解释其内部机制。研究结果表明，少量的“思维链”（Chain-of-Thought）提示能够帮助Transformer在FTCT任务上进行组合推理，正确组合知识片段，即使这些组合在训练数据中不存在。此外，模型的复杂性和训练测试数据的相似性对组合推理能力的出现有重要影响。本文提出，Transformer从训练中学习了一种可推广的底层程序，从而在测试时能够进行有效的组合推理。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>人类具有出色的组合推理能力，能够整合不同来源的知识进行推理。</li>
<li>“FTCT”任务用于验证Transformer模型在组合推理方面的潜力。</li>
<li>少量“思维链”提示有助于Transformer在FTCT任务上正确组合知识片段。</li>
<li>模型复杂性和训练测试数据相似性对组合推理能力有重要影响。</li>
<li>Transformer在训练中学习了一种可推广的底层程序。</li>
<li>这种底层程序使得Transformer在测试时能够进行有效的组合推理。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.15857">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-7ae97626b29c6786fd9a978ce3cf62ed.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-533af2e18d5d670eff8d393c65789045.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8e4d424947576c136c0ec5353921e72a.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Can-LLMs-assist-with-Ambiguity-A-Quantitative-Evaluation-of-various-Large-Language-Models-on-Word-Sense-Disambiguation"><a href="#Can-LLMs-assist-with-Ambiguity-A-Quantitative-Evaluation-of-various-Large-Language-Models-on-Word-Sense-Disambiguation" class="headerlink" title="Can LLMs assist with Ambiguity? A Quantitative Evaluation of various   Large Language Models on Word Sense Disambiguation"></a>Can LLMs assist with Ambiguity? A Quantitative Evaluation of various   Large Language Models on Word Sense Disambiguation</h2><p><strong>Authors:T. G. D. K. Sumanathilaka, Nicholas Micallef, Julian Hough</strong></p>
<p>Ambiguous words are often found in modern digital communications. Lexical ambiguity challenges traditional Word Sense Disambiguation (WSD) methods, due to limited data. Consequently, the efficiency of translation, information retrieval, and question-answering systems is hindered by these limitations. This study investigates the use of Large Language Models (LLMs) to improve WSD using a novel approach combining a systematic prompt augmentation mechanism with a knowledge base (KB) consisting of different sense interpretations. The proposed method incorporates a human-in-loop approach for prompt augmentation where prompt is supported by Part-of-Speech (POS) tagging, synonyms of ambiguous words, aspect-based sense filtering and few-shot prompting to guide the LLM. By utilizing a few-shot Chain of Thought (COT) prompting-based approach, this work demonstrates a substantial improvement in performance. The evaluation was conducted using FEWS test data and sense tags. This research advances accurate word interpretation in social media and digital communication. </p>
<blockquote>
<p>在现代数字通信中经常可以发现词义模糊的词语。由于数据有限，词汇的模糊性给传统的词义消歧（Word Sense Disambiguation, WSD）方法带来了挑战。因此，翻译、信息检索和问答系统的效率受到了这些限制的阻碍。本研究探讨了使用大型语言模型（LLMs）结合一种系统的提示增强机制和包含不同词义解释的知识库（KB）来改善词义消歧的方法。所提出的方法采用了一种人机循环的提示增强方法，其中提示得到词类标注、模糊词同义词、基于方面的词义过滤和少量提示的支持，以引导大型语言模型。通过采用基于少量思维的思维链（COT）提示方法，这项工作在性能上取得了显著改进。评估是使用FEWS测试数据和词义标签进行的。该研究推动了社交媒体和数字通信中的准确词义解读。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.18337v3">PDF</a> 12 pages,6 tables, 1 figure, Proceedings of the 1st International   Conference on NLP &amp; AI for Cyber Security</p>
<p><strong>Summary</strong></p>
<p>本文探讨了现代数字通信中常见的词汇歧义问题。由于数据有限，传统的词语感知消歧（WSD）方法面临挑战。本研究采用大型语言模型（LLMs）结合系统提示增强机制和包含不同感知解读的知识库（KB）来改善WSD。通过结合人类参与的提示增强方法，利用词性标注、模糊词的同义词、基于方面的感知过滤和少量提示引导LLM。通过基于少量提示的思考链（COT）提示方法，该研究在性能上取得了显著改进。评估采用FEWS测试数据和感知标签进行。此研究有助于改进社交媒体和数字通信中的准确词汇解读。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>词汇歧义在现代数字通信中是常见问题。</li>
<li>传统WSD方法因数据有限而面临挑战。</li>
<li>本研究采用大型语言模型（LLMs）改善WSD。</li>
<li>结合系统提示增强机制和知识库进行感知解读。</li>
<li>人类参与的提示增强方法包括词性标注、同义词等。</li>
<li>采用少量提示的思考链（COT）方法显著提高了性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.18337">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-cbf709fa9a7dd0b311058c27411f02dc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d7f227667733ec050b1493a89217bb38.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f3bbeb948a0dce8ab986295a7dcd69b9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f10c3db177cf4a894c58705c963e7489.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-28/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-28/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-28/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-4043fa641afee0e3275d6009246b698a.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation 方向最新论文已更新，请持续关注 Update in 2025-05-28  Multimodal LLM-Guided Semantic Correction in Text-to-Image Diffusion
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-28
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-28/MMT/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-686f51f5a4b71e473c7ac974fb376f57.jpg" class="responsive-img" alt="MMT">
                        
                        <span class="card-title">MMT</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            MMT 方向最新论文已更新，请持续关注 Update in 2025-05-28  MT$^{3}$ Scaling MLLM-based Text Image Machine Translation via   Multi-Task Reinforcement Learning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-28
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/MMT/" class="post-category">
                                    MMT
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/MMT/">
                        <span class="chip bg-color">MMT</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">19778.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
