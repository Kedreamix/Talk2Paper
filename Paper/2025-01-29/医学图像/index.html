<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-29  Lightweight Weighted Average Ensemble Model for Pneumonia Detection in   Chest X-Ray Images">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-ee77dd7964417233a4a5362a8846566c.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-29
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-02-12
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    83 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-01-29-æ›´æ–°"><a href="#2025-01-29-æ›´æ–°" class="headerlink" title="2025-01-29 æ›´æ–°"></a>2025-01-29 æ›´æ–°</h1><h2 id="Lightweight-Weighted-Average-Ensemble-Model-for-Pneumonia-Detection-in-Chest-X-Ray-Images"><a href="#Lightweight-Weighted-Average-Ensemble-Model-for-Pneumonia-Detection-in-Chest-X-Ray-Images" class="headerlink" title="Lightweight Weighted Average Ensemble Model for Pneumonia Detection in   Chest X-Ray Images"></a>Lightweight Weighted Average Ensemble Model for Pneumonia Detection in   Chest X-Ray Images</h2><p><strong>Authors:Suresh Babu Nettur, Shanthi Karpurapu, Unnati Nettur, Likhit Sagar Gajja, Sravanthy Myneni, Akhil Dusi, Lalithya Posham</strong></p>
<p>Pneumonia is a leading cause of illness and death in children, underscoring the need for early and accurate detection. In this study, we propose a novel lightweight ensemble model for detecting pneumonia in children using chest X-ray images. This ensemble model integrates two pre-trained convolutional neural networks (CNNs), MobileNetV2 and NASNetMobile, selected for their balance of computational efficiency and accuracy. These models were fine-tuned on a pediatric chest X-ray dataset and combined to enhance classification performance. Our proposed ensemble model achieved a classification accuracy of 98.63%, significantly outperforming individual models such as MobileNetV2 (97.10%) and NASNetMobile(96.25%) in terms of accuracy, precision, recall, and F1 score. Moreover, the ensemble model outperformed state-of-the-art architectures, including ResNet50, InceptionV3, and DenseNet201, while maintaining computational efficiency. The proposed lightweight ensemble model presents a highly effective and resource-efficient solution for pneumonia detection, making it particularly suitable for deployment in resource-constrained settings. </p>
<blockquote>
<p>è‚ºç‚æ˜¯å„¿ç«¥ç–¾ç—…å’Œæ­»äº¡çš„ä¸»è¦åŸå› ä¹‹ä¸€ï¼Œå¼ºè°ƒäº†å¯¹æ—©æœŸå’Œå‡†ç¡®æ£€æµ‹çš„éœ€æ±‚ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„è½»é‡çº§é›†æˆæ¨¡å‹ï¼Œç”¨äºåˆ©ç”¨èƒ¸éƒ¨Xå°„çº¿å›¾åƒæ£€æµ‹å„¿ç«¥è‚ºç‚ã€‚è¯¥é›†æˆæ¨¡å‹é›†æˆäº†ä¸¤ä¸ªç»è¿‡é¢„è®­ç»ƒçš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ï¼Œå³MobileNetV2å’ŒNASNetMobileï¼Œå®ƒä»¬è¢«é€‰ä¸­æ˜¯å› ä¸ºåœ¨è®¡ç®—æ•ˆç‡å’Œå‡†ç¡®æ€§ä¹‹é—´è¾¾åˆ°äº†å¹³è¡¡ã€‚è¿™äº›æ¨¡å‹åœ¨ä¸€ä¸ªå„¿ç«¥èƒ¸éƒ¨Xå°„çº¿æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒï¼Œå¹¶ç»“åˆä½¿ç”¨ä»¥æé«˜åˆ†ç±»æ€§èƒ½ã€‚æˆ‘ä»¬æå‡ºçš„é›†æˆæ¨¡å‹è¾¾åˆ°äº†98.63%çš„åˆ†ç±»å‡†ç¡®ç‡ï¼Œåœ¨å‡†ç¡®ç‡ã€ç²¾ç¡®åº¦ã€å¬å›ç‡å’ŒF1åˆ†æ•°æ–¹é¢æ˜¾è‘—ä¼˜äºMobileNetV2ï¼ˆ97.10%ï¼‰å’ŒNASNetMobileï¼ˆ96.25%ï¼‰ç­‰å•ä¸ªæ¨¡å‹ã€‚æ­¤å¤–ï¼Œè¯¥é›†æˆæ¨¡å‹åœ¨ä¿æŒè®¡ç®—æ•ˆç‡çš„åŒæ—¶ï¼Œè¿˜ä¼˜äºæœ€æ–°çš„æ¶æ„ï¼ŒåŒ…æ‹¬ResNet50ã€InceptionV3å’ŒDenseNet201ã€‚æ‰€æå‡ºçš„è½»é‡çº§é›†æˆæ¨¡å‹ä¸ºè‚ºç‚æ£€æµ‹æä¾›äº†ä¸€ç§é«˜æ•ˆä¸”èµ„æºåˆ©ç”¨åˆç†çš„è§£å†³æ–¹æ¡ˆï¼Œç‰¹åˆ«é€‚ç”¨äºèµ„æºå—é™çš„ç¯å¢ƒä¸­çš„éƒ¨ç½²ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.16249v1">PDF</a> Corresponding authors: Shanthi Karpurapu   (<a href="mailto:&#115;&#104;&#x61;&#110;&#x74;&#x68;&#x69;&#46;&#107;&#97;&#x72;&#112;&#117;&#114;&#x61;&#112;&#x75;&#64;&#x67;&#x6d;&#97;&#x69;&#108;&#46;&#99;&#111;&#109;">&#115;&#104;&#x61;&#110;&#x74;&#x68;&#x69;&#46;&#107;&#97;&#x72;&#112;&#117;&#114;&#x61;&#112;&#x75;&#64;&#x67;&#x6d;&#97;&#x69;&#108;&#46;&#99;&#111;&#109;</a>), Suresh Babu Nettur (<a href="mailto:&#110;&#101;&#116;&#116;&#117;&#114;&#115;&#x75;&#114;&#101;&#115;&#104;&#64;&#103;&#x6d;&#97;&#x69;&#108;&#46;&#99;&#111;&#x6d;">&#110;&#101;&#116;&#116;&#117;&#114;&#115;&#x75;&#114;&#101;&#115;&#104;&#64;&#103;&#x6d;&#97;&#x69;&#108;&#46;&#99;&#111;&#x6d;</a>)</p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶æå‡ºä¸€ç§ç”¨äºå„¿ç«¥è‚ºç‚æ£€æµ‹çš„è½»é‡çº§é›†æˆæ¨¡å‹ï¼Œè¯¥æ¨¡å‹ç»“åˆäº†MobileNetV2å’ŒNASNetMobileä¸¤ä¸ªé¢„è®­ç»ƒå·ç§¯ç¥ç»ç½‘ç»œï¼Œå®ç°äº†é«˜æ•ˆç‡å’Œå‡†ç¡®æ€§çš„å¹³è¡¡ã€‚åœ¨å„¿ç§‘èƒ¸éƒ¨Xå…‰å›¾åƒæ•°æ®é›†ä¸Šå¾®è°ƒåï¼Œé›†æˆæ¨¡å‹åœ¨åˆ†ç±»æ€§èƒ½ä¸Šæœ‰æ‰€æå‡ï¼Œè¾¾åˆ°äº†98.63%çš„åˆ†ç±»å‡†ç¡®ç‡ï¼Œæ˜¾è‘—ä¼˜äºå•ä¸ªæ¨¡å‹ä»¥åŠä¸€äº›æœ€å…ˆè¿›æ¶æ„çš„æ¨¡å‹ã€‚è¯¥æ¨¡å‹åœ¨ä¿æŒè®¡ç®—æ•ˆç‡çš„åŒæ—¶ï¼Œå±•ç°äº†é«˜æ•ˆä¸”èµ„æºåˆ©ç”¨ä¼˜åŒ–çš„è‚ºç‚æ£€æµ‹è§£å†³æ–¹æ¡ˆï¼Œå°¤å…¶é€‚ç”¨äºèµ„æºå—é™çš„ç¯å¢ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§è½»é‡çº§é›†æˆæ¨¡å‹ï¼Œç”¨äºå„¿ç«¥è‚ºç‚çš„èƒ¸éƒ¨Xå…‰å›¾åƒæ£€æµ‹ã€‚</li>
<li>é›†æˆæ¨¡å‹ç»“åˆäº†MobileNetV2å’ŒNASNetMobileä¸¤ä¸ªé¢„è®­ç»ƒå·ç§¯ç¥ç»ç½‘ç»œã€‚</li>
<li>é›†æˆæ¨¡å‹åœ¨å„¿ç§‘èƒ¸éƒ¨Xå…‰å›¾åƒæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¾®è°ƒï¼Œæé«˜äº†åˆ†ç±»æ€§èƒ½ã€‚</li>
<li>æ¨¡å‹è¾¾åˆ°äº†98.63%çš„åˆ†ç±»å‡†ç¡®ç‡ï¼Œä¼˜äºå•ä¸ªæ¨¡å‹å’Œå…¶ä»–å…ˆè¿›æ¶æ„çš„æ¨¡å‹ã€‚</li>
<li>æ¨¡å‹åœ¨ä¿æŒé«˜å‡†ç¡®ç‡çš„åŒæ—¶ï¼Œå…·æœ‰è‰¯å¥½çš„è®¡ç®—æ•ˆç‡ã€‚</li>
<li>è¯¥æ¨¡å‹ç‰¹åˆ«é€‚ç”¨äºèµ„æºå—é™çš„ç¯å¢ƒä¸­çš„è‚ºç‚æ£€æµ‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.16249">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e58c826dec835b5d1eba9a0c8412ffaa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4c8822ec2ffc73b13ac24c9654a217f9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3dde02515ad0e8f05f2939d574c3d558.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3889be61f02e28387dc8c1a88888c862.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c4d98018b45dd539b768e5e7ceeb11e7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-78e1ce67276985844d78dcc7f49327b2.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="CLISC-Bridging-clip-and-sam-by-enhanced-cam-for-unsupervised-brain-tumor-segmentation"><a href="#CLISC-Bridging-clip-and-sam-by-enhanced-cam-for-unsupervised-brain-tumor-segmentation" class="headerlink" title="CLISC: Bridging clip and sam by enhanced cam for unsupervised brain   tumor segmentation"></a>CLISC: Bridging clip and sam by enhanced cam for unsupervised brain   tumor segmentation</h2><p><strong>Authors:Xiaochuan Ma, Jia Fu, Wenjun Liao, Shichuan Zhang, Guotai Wang</strong></p>
<p>Brain tumor segmentation is important for diagnosis of the tumor, and current deep-learning methods rely on a large set of annotated images for training, with high annotation costs. Unsupervised segmentation is promising to avoid human annotations while the performance is often limited. In this study, we present a novel unsupervised segmentation approach that leverages the capabilities of foundation models, and it consists of three main steps: (1) A vision-language model (i.e., CLIP) is employed to obtain image-level pseudo-labels for training a classification network. Class Activation Mapping (CAM) is then employed to extract Regions of Interest (ROIs), where an adaptive masking-based data augmentation is used to enhance ROI identification.(2) The ROIs are used to generate bounding box and point prompts for the Segment Anything Model (SAM) to obtain segmentation pseudo-labels. (3) A 3D segmentation network is trained with the SAM-derived pseudo-labels, where low-quality pseudo-labels are filtered out in a self-learning process based on the similarity between the SAMâ€™s output and the networkâ€™s prediction. Evaluation on the BraTS2020 dataset demonstrates that our approach obtained an average Dice Similarity Score (DSC) of 85.60%, outperforming five state-of-the-art unsupervised segmentation methods by more than 10 percentage points. Besides, our approach outperforms directly using SAM for zero-shot inference, and its performance is close to fully supervised learning. </p>
<blockquote>
<p>è„‘è‚¿ç˜¤åˆ†å‰²å¯¹äºè‚¿ç˜¤è¯Šæ–­å…·æœ‰é‡è¦æ„ä¹‰ï¼Œç›®å‰æ·±åº¦å­¦ä¹ æ–¹æ³•ä¾èµ–äºå¤§é‡æ ‡æ³¨å›¾åƒè¿›è¡Œè®­ç»ƒï¼Œæ ‡æ³¨æˆæœ¬é«˜æ˜‚ã€‚æ— ç›‘ç£åˆ†å‰²æ–¹æ³•æœ‰æœ›åœ¨é¿å…äººå·¥æ ‡æ³¨çš„åŒæ—¶å®ç°æ€§èƒ½æå‡ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹æ— ç›‘ç£åˆ†å‰²æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨åŸºç¡€æ¨¡å‹çš„èƒ½åŠ›ï¼Œä¸»è¦åŒ…æ‹¬ä¸‰ä¸ªæ­¥éª¤ï¼šï¼ˆ1ï¼‰é‡‡ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆä¾‹å¦‚CLIPï¼‰è·å–å›¾åƒçº§åˆ«çš„ä¼ªæ ‡ç­¾ï¼Œç”¨äºè®­ç»ƒåˆ†ç±»ç½‘ç»œã€‚ç„¶åé‡‡ç”¨ç±»æ¿€æ´»æ˜ å°„ï¼ˆCAMï¼‰æå–æ„Ÿå…´è¶£åŒºåŸŸï¼ˆROIï¼‰ï¼Œå¹¶ä½¿ç”¨åŸºäºè‡ªé€‚åº”æ©æ¨¡çš„æ•°æ®å¢å¼ºæ¥æé«˜ROIè¯†åˆ«ã€‚ï¼ˆ2ï¼‰ä½¿ç”¨ROIç”Ÿæˆè¾¹ç•Œæ¡†å’Œç‚¹æç¤ºï¼Œä»¥ä¾›Segment Anything Modelï¼ˆSAMï¼‰è·å¾—åˆ†å‰²ä¼ªæ ‡ç­¾ã€‚ï¼ˆ3ï¼‰ä½¿ç”¨SAMç”Ÿæˆçš„ä¼ªæ ‡ç­¾è®­ç»ƒ3Dåˆ†å‰²ç½‘ç»œï¼Œåœ¨è‡ªæˆ‘å­¦ä¹ è¿‡ç¨‹ä¸­è¿‡æ»¤æ‰ä½è´¨é‡çš„ä¼ªæ ‡ç­¾ï¼Œè¯¥è¿‡ç¨‹åŸºäºSAMè¾“å‡ºä¸ç½‘ç»œé¢„æµ‹ä¹‹é—´çš„ç›¸ä¼¼æ€§ã€‚åœ¨BraTS2020æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è·å¾—äº†å¹³å‡Diceç›¸ä¼¼åº¦å¾—åˆ†ï¼ˆDSCï¼‰ä¸º85.60%ï¼Œæ¯”äº”ç§æœ€å…ˆè¿›çš„æ— ç›‘ç£åˆ†å‰²æ–¹æ³•çš„æ€§èƒ½é«˜å‡º10ä¸ªç™¾åˆ†ç‚¹ä»¥ä¸Šã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºç›´æ¥ä½¿ç”¨SAMè¿›è¡Œé›¶æ ·æœ¬æ¨ç†ï¼Œå…¶æ€§èƒ½æ¥è¿‘å…¨ç›‘ç£å­¦ä¹ ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.16246v1">PDF</a> 22st IEEE International Symposium on Biomedical Imaging (ISBI 2025)</p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºåŸºç¡€æ¨¡å‹çš„æ— ç›‘ç£åˆ†å‰²æ–¹æ³•ï¼Œç”¨äºè„‘è‚¿ç˜¤åˆ†å‰²ã€‚è¯¥æ–¹æ³•åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ­¥éª¤ï¼šåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹è·å¾—å›¾åƒçº§åˆ«çš„ä¼ªæ ‡ç­¾ä»¥è®­ç»ƒåˆ†ç±»ç½‘ç»œï¼›ä½¿ç”¨åŒºåŸŸæ¿€æ´»æ˜ å°„æå–æ„Ÿå…´è¶£åŒºåŸŸï¼Œå¹¶è¿›è¡Œè‡ªé€‚åº”æ©è†œå¢å¼ºï¼›åˆ©ç”¨æ„Ÿå…´è¶£åŒºåŸŸç”Ÿæˆè¾¹ç•Œæ¡†å’Œç‚¹æç¤ºï¼Œè®­ç»ƒä¸‰ç»´åˆ†å‰²ç½‘ç»œã€‚åœ¨BraTS2020æ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•å¹³å‡Diceç›¸ä¼¼åº¦å¾—åˆ†ä¸º85.6%ï¼Œæ¯”äº”ç§æœ€å…ˆè¿›çš„æ— ç›‘ç£åˆ†å‰²æ–¹æ³•é«˜å‡ºè¶…è¿‡10ä¸ªç™¾åˆ†ç‚¹ã€‚åŒæ—¶ï¼Œä¸ç›´æ¥åº”ç”¨SAMè¿›è¡Œé›¶æ ·æœ¬æ¨æ–­ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•æ€§èƒ½ä¼˜è¶Šï¼Œå¹¶æ¥è¿‘å…¨ç›‘ç£å­¦ä¹ çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„æ— ç›‘ç£åˆ†å‰²æ–¹æ³•ç”¨äºè„‘è‚¿ç˜¤åˆ†å‰²ï¼ŒåŸºäºåŸºç¡€æ¨¡å‹ã€‚</li>
<li>æ–¹æ³•åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ­¥éª¤ï¼šè·å–å›¾åƒçº§ä¼ªæ ‡ç­¾ã€æå–æ„Ÿå…´è¶£åŒºåŸŸï¼Œä»¥åŠç”Ÿæˆè¾¹ç•Œæ¡†å’Œç‚¹æç¤ºè¿›è¡Œç½‘ç»œè®­ç»ƒã€‚</li>
<li>ä½¿ç”¨åŒºåŸŸæ¿€æ´»æ˜ å°„å’Œè‡ªé€‚åº”æ©è†œå¢å¼ºæé«˜æ„Ÿå…´è¶£åŒºåŸŸçš„è¯†åˆ«èƒ½åŠ›ã€‚</li>
<li>åœ¨BraTS2020æ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•æ€§èƒ½ä¼˜è¶Šï¼Œå¹³å‡Diceç›¸ä¼¼åº¦å¾—åˆ†ä¸º85.6%ï¼Œè¿œè¶…å…¶ä»–æ— ç›‘ç£æ–¹æ³•ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.16246">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-907df3bb2692b386c97871893275f494.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a3f4b82c817075eaf7166e9e9614ad5c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7460c1714c8dca2e50cf7696ca2c1e2e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3b555dd087a3d3e830c559e175742566.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-38d0bb6f0bb475b33d941753e5a643ff.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Spatial-Angular-Representation-Learning-for-High-Fidelity-Continuous-Super-Resolution-in-Diffusion-MRI"><a href="#Spatial-Angular-Representation-Learning-for-High-Fidelity-Continuous-Super-Resolution-in-Diffusion-MRI" class="headerlink" title="Spatial-Angular Representation Learning for High-Fidelity Continuous   Super-Resolution in Diffusion MRI"></a>Spatial-Angular Representation Learning for High-Fidelity Continuous   Super-Resolution in Diffusion MRI</h2><p><strong>Authors:Ruoyou Wu, Jian Cheng, Cheng Li, Juan Zou, Wenxin Fan, Hua Guo, Yong Liang, Shanshan Wang</strong></p>
<p>Diffusion magnetic resonance imaging (dMRI) often suffers from low spatial and angular resolution due to inherent limitations in imaging hardware and system noise, adversely affecting the accurate estimation of microstructural parameters with fine anatomical details. Deep learning-based super-resolution techniques have shown promise in enhancing dMRI resolution without increasing acquisition time. However, most existing methods are confined to either spatial or angular super-resolution, limiting their effectiveness in capturing detailed microstructural features. Furthermore, traditional pixel-wise loss functions struggle to recover intricate image details essential for high-resolution reconstruction. To address these challenges, we propose SARL-dMRI, a novel Spatial-Angular Representation Learning framework for high-fidelity, continuous super-resolution in dMRI. SARL-dMRI explores implicit neural representations and spherical harmonics to model continuous spatial and angular representations, simultaneously enhancing both spatial and angular resolution while improving microstructural parameter estimation accuracy. To further preserve image fidelity, a data-fidelity module and wavelet-based frequency loss are introduced, ensuring the super-resolved images remain consistent with the original input and retain fine details. Extensive experiments demonstrate that, compared to five other state-of-the-art methods, our method significantly enhances dMRI data resolution, improves the accuracy of microstructural parameter estimation, and provides better generalization capabilities. It maintains stable performance even under a 45$\times$ downsampling factor. </p>
<blockquote>
<p>æ‰©æ•£ç£å…±æŒ¯æˆåƒï¼ˆdMRIï¼‰ç”±äºæˆåƒç¡¬ä»¶å’Œç³»ç»Ÿå™ªå£°çš„å›ºæœ‰å±€é™æ€§ï¼Œå¸¸å¸¸å­˜åœ¨ç©ºé—´åˆ†è¾¨ç‡å’Œè§’åº¦åˆ†è¾¨ç‡ä½çš„é—®é¢˜ï¼Œè¿™ä¸åˆ©äºå¯¹å…·æœ‰ç²¾ç»†è§£å‰–ç»“æ„çš„å¾®è§‚ç»“æ„å‚æ•°è¿›è¡Œå‡†ç¡®ä¼°è®¡ã€‚åŸºäºæ·±åº¦å­¦ä¹ æŠ€æœ¯çš„è¶…åˆ†è¾¨ç‡æŠ€æœ¯æ˜¾ç¤ºå‡ºæé«˜dMRIåˆ†è¾¨ç‡çš„æ½œåŠ›ï¼Œè€Œæ— éœ€å¢åŠ é‡‡é›†æ—¶é—´ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰æ–¹æ³•ä»…é™äºç©ºé—´æˆ–è§’åº¦è¶…åˆ†è¾¨ç‡ï¼Œåœ¨æ•æ‰è¯¦ç»†çš„å¾®è§‚ç»“æ„ç‰¹å¾æ–¹é¢çš„æ•ˆæœæœ‰é™ã€‚æ­¤å¤–ï¼Œä¼ ç»Ÿçš„é€åƒç´ æŸå¤±å‡½æ•°éš¾ä»¥æ¢å¤å¯¹äºé«˜åˆ†è¾¨ç‡é‡å»ºè‡³å…³é‡è¦çš„å¤æ‚å›¾åƒç»†èŠ‚ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†SARL-dMRIï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºdMRIä¸­é«˜è´¨é‡è¿ç»­è¶…åˆ†è¾¨ç‡çš„æ–°å‹ç©ºé—´-è§’åº¦è¡¨ç¤ºå­¦ä¹ æ¡†æ¶ã€‚SARL-dMRIæ¢ç´¢éšå¼ç¥ç»è¡¨ç¤ºå’Œçƒé¢è°æ³¢æ¥å¯¹è¿ç»­çš„ç©ºé—´å’Œè§’åº¦è¡¨ç¤ºè¿›è¡Œå»ºæ¨¡ï¼ŒåŒæ—¶æé«˜ç©ºé—´å’Œè§’åº¦åˆ†è¾¨ç‡ï¼Œå¹¶æ”¹å–„å¾®è§‚ç»“æ„å‚æ•°ä¼°è®¡çš„å‡†ç¡®æ€§ã€‚ä¸ºäº†è¿›ä¸€æ­¥ä¿æŒå›¾åƒä¿çœŸåº¦ï¼Œå¼•å…¥äº†æ•°æ®ä¿çœŸåº¦æ¨¡å—å’Œå°æ³¢é¢‘æŸå¤±æŠ€æœ¯ï¼Œç¡®ä¿è¶…åˆ†è¾¨ç‡å›¾åƒä¸åŸå§‹è¾“å…¥ä¿æŒä¸€è‡´å¹¶ä¿ç•™ç»†èŠ‚ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œä¸å…¶ä»–äº”ç§å…ˆè¿›æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¢å¼ºdMRIæ•°æ®åˆ†è¾¨ç‡ã€æé«˜å¾®è§‚ç»“æ„å‚æ•°ä¼°è®¡å‡†ç¡®æ€§ä»¥åŠæä¾›æ›´å¥½æ³›åŒ–èƒ½åŠ›æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚å³ä½¿åœ¨45å€çš„é™é‡‡æ ·ç³»æ•°ä¸‹ï¼Œå®ƒä¹Ÿèƒ½ä¿æŒç¨³å®šçš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.16014v1">PDF</a> 10 pages, 6 figures</p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ·±åº¦å­¦ä¹ æŠ€æœ¯çš„è¶…åˆ†è¾¨ç‡æŠ€æœ¯å¯ä»¥æé«˜æ‰©æ•£ç£å…±æŒ¯æˆåƒï¼ˆdMRIï¼‰çš„åˆ†è¾¨ç‡ï¼Œè€Œä¸å¢åŠ é‡‡é›†æ—¶é—´ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰æ–¹æ³•ä»…é™äºç©ºé—´æˆ–è§’è¶…åˆ†è¾¨ç‡ï¼Œéš¾ä»¥æ•æ‰è¯¦ç»†çš„å¾®è§‚ç»“æ„ç‰¹å¾ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†SARL-dMRIï¼Œä¸€ç§ç”¨äºdMRIé«˜ä¿çœŸè¿ç»­è¶…åˆ†è¾¨ç‡çš„æ–°å‹ç©ºé—´è§’è¡¨ç¤ºå­¦ä¹ æ¡†æ¶ã€‚SARL-dMRIé‡‡ç”¨éšå¼ç¥ç»è¡¨ç¤ºå’Œçƒé¢è°æ³¢å»ºæ¨¡è¿ç»­çš„ç©ºé—´å’Œè§’è¡¨ç¤ºï¼Œå¯åŒæ—¶æé«˜ç©ºé—´å’Œè§’åˆ†è¾¨ç‡ï¼Œæé«˜å¾®è§‚ç»“æ„å‚æ•°ä¼°è®¡çš„å‡†ç¡®æ€§ã€‚å¼•å…¥æ•°æ®ä¿çœŸæ¨¡å—å’Œå°æ³¢åŸºé¢‘æŸå¤±ä»¥ä¿ç•™å›¾åƒçš„ç»†èŠ‚å’Œä¿çœŸåº¦ã€‚å®éªŒè¡¨æ˜ï¼Œä¸å…¶ä»–äº”ç§å…ˆè¿›æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æé«˜dMRIæ•°æ®åˆ†è¾¨ç‡ã€æé«˜å¾®è§‚ç»“æ„å‚æ•°ä¼°è®¡çš„å‡†ç¡®æ€§ï¼Œå¹¶å…·æœ‰è¾ƒå¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œåœ¨45å€é™é‡‡æ ·ä¸‹ä»èƒ½ä¿æŒç¨³å®šçš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£ç£å…±æŒ¯æˆåƒï¼ˆdMRIï¼‰å—åˆ°ç¡¬ä»¶å’Œç³»ç»Ÿå™ªå£°çš„é™åˆ¶ï¼Œå­˜åœ¨ç©ºé—´å’Œè§’åˆ†è¾¨ç‡ä½çš„é—®é¢˜ã€‚</li>
<li>æ·±åº¦å­¦ä¹ è¶…åˆ†è¾¨ç‡æŠ€æœ¯å¯æé«˜dMRIåˆ†è¾¨ç‡ï¼Œæ— éœ€å¢åŠ é‡‡é›†æ—¶é—´ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦é™äºç©ºé—´æˆ–è§’è¶…åˆ†è¾¨ç‡ï¼Œéš¾ä»¥å…¨é¢æ•æ‰å¾®è§‚ç»“æ„ç‰¹å¾ã€‚</li>
<li>SARL-dMRIæ˜¯ä¸€ä¸ªæ–°å‹æ¡†æ¶ï¼Œé‡‡ç”¨éšå¼ç¥ç»è¡¨ç¤ºå’Œçƒé¢è°æ³¢è¿›è¡Œç©ºé—´è§’è¡¨ç¤ºå­¦ä¹ ï¼Œæé«˜ç©ºé—´å’Œè§’åˆ†è¾¨ç‡ã€‚</li>
<li>SARL-dMRIå¼•å…¥æ•°æ®ä¿çœŸæ¨¡å—å’Œå°æ³¢åŸºé¢‘æŸå¤±ï¼Œç¡®ä¿å›¾åƒç»†èŠ‚å’Œä¿çœŸåº¦çš„ä¿ç•™ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒSARL-dMRIåœ¨dMRIæ•°æ®åˆ†è¾¨ç‡ã€å¾®è§‚ç»“æ„å‚æ•°ä¼°è®¡å‡†ç¡®æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œå¹¶å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.16014">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4743cdaaf0b3f6ca9a0a008148938bb9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bf5fce5767eabdc0854b47a4497cfa77.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-576226f4551b78ea106d4a1780be38e7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ae7eec23ca635c3a32b13dd1ec8d0b5d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2ce4a8061ec10d551d905174d8992129.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Real-Time-Brain-Tumor-Detection-in-Intraoperative-Ultrasound-Using-YOLO11-From-Model-Training-to-Deployment-in-the-Operating-Room"><a href="#Real-Time-Brain-Tumor-Detection-in-Intraoperative-Ultrasound-Using-YOLO11-From-Model-Training-to-Deployment-in-the-Operating-Room" class="headerlink" title="Real-Time Brain Tumor Detection in Intraoperative Ultrasound Using   YOLO11: From Model Training to Deployment in the Operating Room"></a>Real-Time Brain Tumor Detection in Intraoperative Ultrasound Using   YOLO11: From Model Training to Deployment in the Operating Room</h2><p><strong>Authors:Santiago Cepeda, Olga Esteban-Sinovas, Roberto Romero, Vikas Singh, Prakash Shetty, Aliasgar Moiyadi, Ilyess Zemmoura, Giuseppe Roberto Giammalva, Massimiliano Del Bene, Arianna Barbotti, Francesco DiMeco, Timothy R. West, Brian V. Nahed, Ignacio Arrese, Roberto Hornero, Rosario Sarabia</strong></p>
<p>Intraoperative ultrasound (ioUS) is a valuable tool in brain tumor surgery due to its versatility, affordability, and seamless integration into the surgical workflow. However, its adoption remains limited, primarily because of the challenges associated with image interpretation and the steep learning curve required for effective use. This study aimed to enhance the interpretability of ioUS images by developing a real-time brain tumor detection system deployable in the operating room. We collected 2D ioUS images from the Brain Tumor Intraoperative Database (BraTioUS) and the public ReMIND dataset, annotated with expert-refined tumor labels. Using the YOLO11 architecture and its variants, we trained object detection models to identify brain tumors. The dataset included 1,732 images from 192 patients, divided into training, validation, and test sets. Data augmentation expanded the training set to 11,570 images. In the test dataset, YOLO11s achieved the best balance of precision and computational efficiency, with a mAP@50 of 0.95, mAP@50-95 of 0.65, and a processing speed of 34.16 frames per second. The proposed solution was prospectively validated in a cohort of 15 consecutively operated patients diagnosed with brain tumors. Neurosurgeons confirmed its seamless integration into the surgical workflow, with real-time predictions accurately delineating tumor regions. These findings highlight the potential of real-time object detection algorithms to enhance ioUS-guided brain tumor surgery, addressing key challenges in interpretation and providing a foundation for future development of computer vision-based tools for neuro-oncological surgery. </p>
<blockquote>
<p>æœ¯ä¸­è¶…å£°ï¼ˆioUSï¼‰åœ¨è„‘è‚¿ç˜¤æ‰‹æœ¯ä¸­æ˜¯ä¸€ä¸ªå¾ˆæœ‰ä»·å€¼çš„å·¥å…·ï¼Œå› ä¸ºå®ƒå…·æœ‰é€šç”¨æ€§ã€æ€§ä»·æ¯”ï¼Œå¹¶èƒ½æ— ç¼èå…¥æ‰‹æœ¯æµç¨‹ã€‚ç„¶è€Œï¼Œå®ƒçš„åº”ç”¨ä»ç„¶æœ‰é™ï¼Œä¸»è¦æ˜¯å› ä¸ºä¸å›¾åƒè§£è¯»ç›¸å…³çš„æŒ‘æˆ˜ä»¥åŠæœ‰æ•ˆä½¿ç”¨æ‰€éœ€çš„é«˜éš¾åº¦å­¦ä¹ æ›²çº¿ã€‚æœ¬ç ”ç©¶æ—¨åœ¨é€šè¿‡å¼€å‘ä¸€ç§å¯åœ¨æ‰‹æœ¯å®¤éƒ¨ç½²çš„å®æ—¶è„‘è‚¿ç˜¤æ£€æµ‹ç³»ç»Ÿï¼Œæé«˜ioUSå›¾åƒçš„è§£è¯»æ€§ã€‚æˆ‘ä»¬ä»è„‘è‚¿ç˜¤æœ¯ä¸­æ•°æ®åº“ï¼ˆBraTioUSï¼‰å’Œå…¬å…±ReMINDæ•°æ®é›†æ”¶é›†äº†äºŒç»´ioUSå›¾åƒï¼Œè¿™äº›å›¾åƒéƒ½ç»è¿‡ä¸“å®¶ä¿®æ­£çš„è‚¿ç˜¤æ ‡ç­¾è¿›è¡Œæ ‡æ³¨ã€‚æˆ‘ä»¬ä½¿ç”¨YOLO11æ¶æ„åŠå…¶å˜ä½“è®­ç»ƒç›®æ ‡æ£€æµ‹æ¨¡å‹ï¼Œä»¥è¯†åˆ«è„‘è‚¿ç˜¤ã€‚æ•°æ®é›†åŒ…å«æ¥è‡ª192åæ‚£è€…çš„1732å¼ å›¾åƒï¼Œåˆ†ä¸ºè®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†ã€‚é€šè¿‡æ•°æ®å¢å¼ºï¼Œè®­ç»ƒé›†æ‰©å……è‡³11570å¼ å›¾åƒã€‚åœ¨æµ‹è¯•æ•°æ®é›†ä¸­ï¼ŒYOLO11såœ¨ç²¾åº¦å’Œè®¡ç®—æ•ˆç‡æ–¹é¢è¾¾åˆ°äº†æœ€ä½³å¹³è¡¡ï¼ŒmAP@50ä¸º0.95ï¼ŒmAP@50-95ä¸º0.65ï¼Œå¤„ç†é€Ÿåº¦ä¸ºæ¯ç§’34.16å¸§ã€‚æ‰€æå‡ºçš„è§£å†³æ–¹æ¡ˆåœ¨è¿ç»­æ¥å—æ‰‹æœ¯çš„15åè„‘è‚¿ç˜¤æ‚£è€…é˜Ÿåˆ—ä¸­è¿›è¡Œäº†å‰ç»æ€§éªŒè¯ã€‚ç¥ç»å¤–ç§‘åŒ»ç”Ÿè¯å®ï¼Œå®ƒèƒ½æ— ç¼èå…¥æ‰‹æœ¯æµç¨‹ï¼Œå®æ—¶é¢„æµ‹å‡†ç¡®å‹¾ç”»å‡ºè‚¿ç˜¤åŒºåŸŸã€‚è¿™äº›å‘ç°çªæ˜¾äº†å®æ—¶ç›®æ ‡æ£€æµ‹ç®—æ³•åœ¨å¢å¼ºioUSå¼•å¯¼çš„è„‘è‚¿ç˜¤æ‰‹æœ¯ä¸­çš„æ½œåŠ›ï¼Œè§£å†³äº†è§£è¯»æ–¹é¢çš„å…³é”®æŒ‘æˆ˜ï¼Œå¹¶ä¸ºåŸºäºè®¡ç®—æœºè§†è§‰çš„ç¥ç»è‚¿ç˜¤æ‰‹æœ¯å·¥å…·çš„æœªæ¥å¼€å‘å¥ å®šäº†åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.15994v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    æœ¬ç ”ç©¶æ—¨åœ¨å¼€å‘å®æ—¶è„‘è‚¿ç˜¤æ£€æµ‹ä½“ç³»ä»¥å¢å¼ºæœ¯ä¸­è¶…å£°ï¼ˆioUSï¼‰å›¾åƒçš„å¯è§£é‡Šæ€§ï¼Œä»¥æé«˜è„‘è‚¿ç˜¤æ‰‹æœ¯ä¸­ä½¿ç”¨ioUSçš„ä»·å€¼ã€‚ç ”ç©¶ä½¿ç”¨YOLO11æ¶æ„åŠå…¶å˜ä½“è®­ç»ƒç›®æ ‡æ£€æµ‹æ¨¡å‹ï¼Œå¯¹æ¥è‡ªBrain Tumor Intraoperative Databaseï¼ˆBraTioUSï¼‰å’Œå…¬å…±ReMINDæ•°æ®é›†çš„äºŒç»´ioUSå›¾åƒè¿›è¡Œè®­ç»ƒã€‚æ•°æ®é›†åŒ…å«æ¥è‡ª192åæ‚£è€…çš„1732å¼ å›¾åƒï¼Œåˆ†ä¸ºè®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†ã€‚æ•°æ®å¢å¼ºå°†è®­ç»ƒé›†æ‰©å±•åˆ°11570å¼ å›¾åƒã€‚åœ¨æµ‹è¯•æ•°æ®é›†ä¸­ï¼ŒYOLO11så®ç°äº†æœ€ä½³çš„ç²¾åº¦å’Œè®¡ç®—æ•ˆç‡å¹³è¡¡ï¼ŒmAP@50ä¸º0.95ï¼ŒmAP@50-95ä¸º0.65ï¼Œå¤„ç†é€Ÿåº¦ä¸ºæ¯ç§’34.16å¸§ã€‚åœ¨è¿ç»­æ¥å—æ‰‹æœ¯çš„15åè„‘è‚¿ç˜¤æ‚£è€…é˜Ÿåˆ—ä¸­è¿›è¡Œçš„è¯•éªŒéªŒè¯äº†è¯¥è§£å†³æ–¹æ¡ˆçš„æœ‰æ•ˆæ€§ï¼Œç¥ç»å¤–ç§‘åŒ»ç”Ÿç¡®è®¤å…¶æ— ç¼é›†æˆåˆ°æ‰‹æœ¯å·¥ä½œæµç¨‹ä¸­ï¼Œå¯å®æ—¶å‡†ç¡®é¢„æµ‹è‚¿ç˜¤åŒºåŸŸã€‚è¯¥ç ”ç©¶å¼ºè°ƒäº†å®æ—¶ç›®æ ‡æ£€æµ‹ç®—æ³•åœ¨å¢å¼ºioUSå¼•å¯¼çš„è„‘è‚¿ç˜¤æ‰‹æœ¯ä¸­çš„æ½œåŠ›ï¼Œè§£å†³äº†è§£é‡Šä¸Šçš„å…³é”®æŒ‘æˆ˜ï¼Œå¹¶ä¸ºåŸºäºè®¡ç®—æœºè§†è§‰çš„ç¥ç»è‚¿ç˜¤å¤–ç§‘æ‰‹æœ¯çš„æœªæ¥å‘å±•å¥ å®šäº†åŸºç¡€ã€‚</p>
<p><strong>å…³é”®å‘ç°</strong></p>
<ol>
<li>å¼€å‘äº†ä¸€ä¸ªå®æ—¶è„‘è‚¿ç˜¤æ£€æµ‹ä½“ç³»ï¼Œç”¨äºå¢å¼ºæœ¯ä¸­è¶…å£°ï¼ˆioUSï¼‰å›¾åƒçš„å¯è§£é‡Šæ€§ã€‚</li>
<li>é‡‡ç”¨YOLO11æ¶æ„åŠå…¶å˜ä½“è¿›è¡Œç›®æ ‡æ£€æµ‹æ¨¡å‹çš„è®­ç»ƒã€‚</li>
<li>æ•°æ®é›†åŒ…æ‹¬æ¥è‡ªBraTioUSå’ŒReMINDçš„äºŒç»´ioUSå›¾åƒæ•°æ®ã€‚</li>
<li>æ•°æ®å¢å¼ºæŠ€æœ¯æœ‰æ•ˆæ‰©å±•äº†è®­ç»ƒé›†è§„æ¨¡ã€‚</li>
<li>YOLO11sæ¨¡å‹åœ¨æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¾¾åˆ°äº†è¾ƒé«˜çš„ç²¾åº¦å’Œå¤„ç†é€Ÿåº¦ã€‚</li>
<li>è§£å†³æ–¹æ¡ˆåœ¨å®é™…æ‰‹æœ¯ä¸­å¾—åˆ°éªŒè¯ï¼Œèƒ½æ— ç¼èå…¥æ‰‹æœ¯æµç¨‹ï¼Œå¹¶å‡†ç¡®é¢„æµ‹è‚¿ç˜¤åŒºåŸŸã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.15994">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-59e2e11a12e1a400bdf834eb50e58901.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9728b45bf430f1b596429a857cb41d72.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Leveraging-Video-Vision-Transformer-for-Alzheimerâ€™s-Disease-Diagnosis-from-3D-Brain-MRI"><a href="#Leveraging-Video-Vision-Transformer-for-Alzheimerâ€™s-Disease-Diagnosis-from-3D-Brain-MRI" class="headerlink" title="Leveraging Video Vision Transformer for Alzheimerâ€™s Disease Diagnosis   from 3D Brain MRI"></a>Leveraging Video Vision Transformer for Alzheimerâ€™s Disease Diagnosis   from 3D Brain MRI</h2><p><strong>Authors:Taymaz Akan, Sait Alp, Md. Shenuarin Bhuiyan, Elizabeth A. Disbrow, Steven A. Conrad, John A. Vanchiere, Christopher G. Kevil, Mohammad A. N. Bhuiyan</strong></p>
<p>Alzheimerâ€™s disease (AD) is a neurodegenerative disorder affecting millions worldwide, necessitating early and accurate diagnosis for optimal patient management. In recent years, advancements in deep learning have shown remarkable potential in medical image analysis. Methods In this study, we present â€œViTranZheimer,â€ an AD diagnosis approach which leverages video vision transformers to analyze 3D brain MRI data. By treating the 3D MRI volumes as videos, we exploit the temporal dependencies between slices to capture intricate structural relationships. The video vision transformerâ€™s self-attention mechanisms enable the model to learn long-range dependencies and identify subtle patterns that may indicate AD progression. Our proposed deep learning framework seeks to enhance the accuracy and sensitivity of AD diagnosis, empowering clinicians with a tool for early detection and intervention. We validate the performance of the video vision transformer using the ADNI dataset and conduct comparative analyses with other relevant models. Results The proposed ViTranZheimer model is compared with two hybrid models, CNN-BiLSTM and ViT-BiLSTM. CNN-BiLSTM is the combination of a convolutional neural network (CNN) and a bidirectional long-short-term memory network (BiLSTM), while ViT-BiLSTM is the combination of a vision transformer (ViT) with BiLSTM. The accuracy levels achieved in the ViTranZheimer, CNN-BiLSTM, and ViT-BiLSTM models are 98.6%, 96.479%, and 97.465%, respectively. ViTranZheimer demonstrated the highest accuracy at 98.6%, outperforming other models in this evaluation metric, indicating its superior performance in this specific evaluation metric. Conclusion This research advances the understanding of applying deep learning techniques in neuroimaging and Alzheimerâ€™s disease research, paving the way for earlier and less invasive clinical diagnosis. </p>
<blockquote>
<p>é˜¿å°”èŒ¨æµ·é»˜ç—‡ï¼ˆADï¼‰æ˜¯ä¸€ç§å½±å“å…¨çƒæ•°ç™¾ä¸‡äººçš„ç¥ç»é€€è¡Œæ€§ç–¾ç—…ï¼Œéœ€è¦æ—©æœŸå’Œå‡†ç¡®çš„è¯Šæ–­ä»¥å®ç°æœ€ä½³çš„æ‚£è€…ç®¡ç†ã€‚è¿‘å¹´æ¥ï¼Œæ·±åº¦å­¦ä¹ åœ¨åŒ»å­¦å›¾åƒåˆ†ææ–¹é¢å·²æ˜¾ç¤ºå‡ºæ˜¾è‘—æ½œåŠ›ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºâ€œViTranZheimerâ€çš„ADè¯Šæ–­æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨è§†é¢‘è§†è§‰å˜å‹å™¨ï¼ˆVideo Vision Transformerï¼‰åˆ†æ3Dè„‘éƒ¨MRIæ•°æ®ã€‚é€šè¿‡å°†3D MRIä½“ç§¯è§†ä¸ºè§†é¢‘ï¼Œæˆ‘ä»¬åˆ©ç”¨åˆ‡ç‰‡ä¹‹é—´çš„æ—¶é—´ä¾èµ–æ€§æ¥æ•æ‰å¤æ‚ç»“æ„å…³ç³»ã€‚è§†é¢‘è§†è§‰å˜å‹å™¨çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶ä½¿æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ é•¿æœŸä¾èµ–æ€§å¹¶è¯†åˆ«å¯èƒ½æŒ‡ç¤ºADè¿›å±•çš„å¾®å¦™æ¨¡å¼ã€‚æˆ‘ä»¬æå‡ºçš„æ·±åº¦å­¦ä¹ æ¡†æ¶æ—¨åœ¨æé«˜ADè¯Šæ–­çš„å‡†ç¡®æ€§å’Œæ•æ„Ÿæ€§ï¼Œä¸ºä¸´åºŠåŒ»ç”Ÿæä¾›ä¸€ç§æ—©æœŸæ£€æµ‹å’Œå¹²é¢„çš„å·¥å…·ã€‚æˆ‘ä»¬ä½¿ç”¨ADNIæ•°æ®é›†éªŒè¯äº†è§†é¢‘è§†è§‰å˜å‹å™¨çš„æ€§èƒ½ï¼Œå¹¶ä¸å…¶ä»–ç›¸å…³æ¨¡å‹è¿›è¡Œäº†æ¯”è¾ƒåˆ†æã€‚ç»“æœæå‡ºçš„ViTranZheimeræ¨¡å‹ä¸ä¸¤ç§æ··åˆæ¨¡å‹ï¼ˆCNN-BiLSTMå’ŒViT-BiLSTMï¼‰è¿›è¡Œäº†æ¯”è¾ƒã€‚CNN-BiLSTMæ˜¯å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å’ŒåŒå‘é•¿çŸ­æœŸè®°å¿†ç½‘ç»œï¼ˆBiLSTMï¼‰çš„ç»„åˆï¼Œè€ŒViT-BiLSTMåˆ™æ˜¯è§†è§‰å˜å‹å™¨ï¼ˆViTï¼‰ä¸BiLSTMçš„ç»„åˆã€‚ViTranZheimerã€CNN-BiLSTMå’ŒViT-BiLSTMæ¨¡å‹ä¸­å®ç°çš„å‡†ç¡®åº¦åˆ†åˆ«ä¸º98.6%ã€96.479%å’Œ97.465%ã€‚ViTranZheimeråœ¨å‡†ç¡®æ€§æ–¹é¢è¾¾åˆ°äº†æœ€é«˜çš„98.6%ï¼Œåœ¨è¯„ä¼°æŒ‡æ ‡ä¸Šä¼˜äºå…¶ä»–æ¨¡å‹ï¼Œè¡¨æ˜å…¶åœ¨è¯¥ç‰¹å®šè¯„ä¼°æŒ‡æ ‡ä¸Šçš„å“è¶Šæ€§èƒ½ã€‚ç»“è®ºæœ¬ç ”ç©¶æ·±åŒ–äº†å¯¹æ·±åº¦å­¦ä¹ æŠ€æœ¯åœ¨ç¥ç»å½±åƒå­¦å’Œé˜¿å°”èŒ¨æµ·é»˜ç—‡ç ”ç©¶ä¸­çš„åº”ç”¨ç†è§£ï¼Œä¸ºæ›´æ—©å’Œä¾µå…¥æ€§è¾ƒå°çš„ä¸´åºŠè¯Šæ–­é“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.15733v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶æå‡ºä¸€ç§åˆ©ç”¨è§†é¢‘è§†è§‰è½¬æ¢å™¨ï¼ˆViTranZheimerï¼‰åˆ†æ3Dè„‘MRIæ•°æ®ï¼Œä»¥è¯Šæ–­é˜¿å°”èŒ¨æµ·é»˜ç—…ï¼ˆADï¼‰çš„æ–¹æ³•ã€‚é€šè¿‡å¤„ç†3D MRIä½“ç§¯æ•°æ®ä½œä¸ºè§†é¢‘ï¼Œæ•æ‰åˆ‡ç‰‡é—´çš„æ—¶åºä¾èµ–æ€§ï¼Œå¹¶å­¦ä¹ é•¿æœŸä¾èµ–å…³ç³»å’Œè¯†åˆ«å¾®å¦™çš„æ¨¡å¼ä»¥æŒ‡ç¤ºADè¿›å±•ã€‚ä¸ç°æœ‰æ¨¡å‹ç›¸æ¯”ï¼ŒViTranZheimeræ¨¡å‹çš„è¯Šæ–­å‡†ç¡®ç‡å’Œçµæ•åº¦æ›´é«˜ï¼Œä¸ºä¸´åºŠåŒ»ç”Ÿæä¾›æ—©æœŸæ£€æµ‹å’Œå¹²é¢„çš„å·¥å…·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬ç ”ç©¶åˆ©ç”¨è§†é¢‘è§†è§‰è½¬æ¢å™¨ï¼ˆViTranZheimerï¼‰è¿›è¡Œé˜¿å°”èŒ¨æµ·é»˜ç—…ï¼ˆADï¼‰çš„è¯Šæ–­ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ã€‚</li>
<li>è¯¥æ¨¡å‹é‡‡ç”¨3Dè„‘MRIæ•°æ®ä½œä¸ºè¾“å…¥ï¼Œå¹¶å°†å…¶è§†ä¸ºè§†é¢‘è¿›è¡Œå¤„ç†ã€‚</li>
<li>é€šè¿‡æ•æ‰åˆ‡ç‰‡é—´çš„æ—¶åºä¾èµ–æ€§ï¼ŒViTranZheimerèƒ½å¤Ÿå­¦ä¹ é•¿æœŸä¾èµ–å…³ç³»å¹¶è¯†åˆ«å¾®å¦™çš„æ¨¡å¼ä»¥æŒ‡ç¤ºADçš„è¿›å±•ã€‚</li>
<li>ç ”ç©¶éªŒè¯äº†ViTranZheimeræ¨¡å‹çš„æ€§èƒ½ï¼Œå¹¶é€šè¿‡ä¸å…¶ä»–ç›¸å…³æ¨¡å‹çš„æ¯”è¾ƒåˆ†æï¼Œæ˜¾ç¤ºå‡ºå…¶è¾ƒé«˜çš„è¯Šæ–­å‡†ç¡®ç‡ã€‚</li>
<li>ViTranZheimeræ¨¡å‹ä¸CNN-BiLSTMå’ŒViT-BiLSTMä¸¤ç§æ··åˆæ¨¡å‹è¿›è¡Œäº†æ¯”è¾ƒï¼Œå‡†ç¡®ç‡åˆ†åˆ«ä¸º98.6%ã€96.479%å’Œ97.465%ï¼Œæ˜¾ç¤ºå‡ºViTranZheimeråœ¨è¯Šæ–­ADæ–¹é¢çš„ä¼˜è¶Šæ€§ã€‚</li>
<li>æœ¬ç ”ç©¶æ¨åŠ¨äº†æ·±åº¦å­¦ä¹ åœ¨ç¥ç»å½±åƒå­¦å’Œé˜¿å°”èŒ¨æµ·é»˜ç—…ç ”ç©¶ä¸­çš„åº”ç”¨ï¼Œä¸ºæ—©æœŸå’Œå¾®åˆ›çš„ä¸´åºŠè¯Šæ–­é“ºå¹³äº†é“è·¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.15733">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5ab129f2734ce488357d5baee79c5c4f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-44c98036fdbb06021f984cdd59f0366e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8cf19cdebca2773d6909270fd702ec58.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-21799bc0ac2ef129db7f0a03901be463.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Compensator-based-small-animal-IMRT-enables-conformal-preclinical-dose-painting-application-to-tumor-hypoxia"><a href="#Compensator-based-small-animal-IMRT-enables-conformal-preclinical-dose-painting-application-to-tumor-hypoxia" class="headerlink" title="Compensator-based small animal IMRT enables conformal preclinical dose   painting: application to tumor hypoxia"></a>Compensator-based small animal IMRT enables conformal preclinical dose   painting: application to tumor hypoxia</h2><p><strong>Authors:Jordan M. Slagowski, Erik Pearson, Rajit Tummala, Gage Redler, Daniela Olivera Velarde, Boris Epel, Howard J. Halpern, Bulent Aydogan</strong></p>
<p>Techniques for preclinical intensity modulated radiation therapy are being developed to improve translation by replicating the clinical paradigm. This study presents the first treatment planning comparison between small animal IMRT (SA-IMRT) and three-dimensional conformal radiotherapy (CRT) in a model application, oxygen-guided dose painting of tumor hypoxia, using actual mouse data. A novel compensator-based platform was employed to generate SA-IMRT and CRT plans with 2-15 beam angles for seventeen mice with fibrosarcoma tumors. The whole tumor received a dose of 22.5 Gy, with a simultaneous integrated boost of 13 Gy to hypoxic voxels identified via electron paramagnetic resonance imaging. Plan quality was assessed using the Paddick conformity index (CI), uniformity, and dose volume histograms. For 3-angles, SA-IMRT yielded significantly improved dose conformity (median hypoxic CI &#x3D;0.45 versus 0.17), tumor dose uniformity (11.0% versus 14.3%), and dosimetric spread between boost and non-boost targets (D50% difference &#x3D; 13.0 Gy [ideal], 13.1 Gy [SA-IMRT], 7. 3 Gy [CRT]). No significant improvement in CI was associated with &gt;3 beam angles (Wilcoxon signed-rank test, p &lt; 0.05). This study demonstrates that SA-IMRT provides significant improvements in radiation plan quality and yields dose distributions that more closely mimic the clinical setting relative to current CRT approaches. </p>
<blockquote>
<p>ç›®å‰æ­£å¼€å‘ç”¨äºä¸´åºŠå‰å¼ºåº¦è°ƒèŠ‚æ”¾å°„æ²»ç–—çš„æŠ€æœ¯ï¼Œä»¥å¤åˆ¶ä¸´åºŠèŒƒä¾‹æ¥æ”¹å–„ç¿»è¯‘ã€‚æœ¬ç ”ç©¶æ—¨åœ¨å‘ˆç°å°åŠ¨ç‰©IMRTï¼ˆSA-IMRTï¼‰ä¸ä¸‰ç»´é€‚å½¢æ”¾å°„æ²»ç–—ï¼ˆCRTï¼‰ä¹‹é—´çš„é¦–æ¬¡æ²»ç–—è®¡åˆ’æ¯”è¾ƒã€‚åœ¨ä¸€é¡¹æ¨¡å‹åº”ç”¨ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨å®é™…çš„é¼ æ•°æ®ï¼Œå€ŸåŠ©æ°§å¯¼å‘çš„å‰‚é‡ç»˜åˆ¶è‚¿ç˜¤ç¼ºæ°§å›¾è°±ã€‚é‡‡ç”¨æ–°å‹è¡¥å¿å™¨å¹³å°ç”ŸæˆSA-IMRTå’ŒCRTè®¡åˆ’ï¼Œä¸º17åªæ‚£æœ‰çº¤ç»´è‚‰ç˜¤è‚¿ç˜¤çš„é¼ æ ‡è®¾ç½®äº†2-15ä¸ªå…‰æŸè§’åº¦ã€‚æ•´ä¸ªè‚¿ç˜¤æ¥å—22.5 Gyçš„å‰‚é‡ï¼ŒåŒæ—¶å¯¹é€šè¿‡ç”µå­é¡ºç£å…±æŒ¯æˆåƒè¯†åˆ«å‡ºçš„ç¼ºæ°§ä½“ç´ ç»™äºˆ13 Gyçš„åŒæ—¶é›†æˆæå‡å‰‚é‡ã€‚è®¡åˆ’è´¨é‡ä½¿ç”¨Paddickä¸€è‡´æ€§æŒ‡æ•°ï¼ˆCIï¼‰ã€å‡åŒ€æ€§å’Œå‰‚é‡ä½“ç§¯ç›´æ–¹å›¾è¿›è¡Œè¯„ä¼°ã€‚å¯¹äº3ä¸ªè§’åº¦ï¼ŒSA-IMRTçš„å‰‚é‡ä¸€è‡´æ€§ã€è‚¿ç˜¤å‰‚é‡å‡åŒ€æ€§å’Œæå‡ä¸éæå‡ç›®æ ‡ä¹‹é—´çš„å‰‚é‡å­¦æ‰©æ•£å‡æœ‰æ˜¾è‘—æ”¹å–„ï¼ˆç¼ºæ°§CIä¸­ä½æ•°&#x3D; 0.45æ¯”0.17ï¼›11.0%æ¯”14.3%ï¼›D50%å·®å¼‚&#x3D; 13.0 Gy [ç†æƒ³]ï¼Œ13.1 Gy [SA-IMRT]ï¼Œ7.3 Gy [CRT]ï¼‰ã€‚è€Œå¯¹äºå¤§äº3ä¸ªå…‰æŸè§’åº¦çš„æƒ…å†µï¼Œå¹¶æœªå‡ºç°CIçš„æ˜¾è‘—æ”¹å–„ï¼ˆWilcoxonç¬¦å·ç§©æ£€éªŒï¼Œp &lt; 0.05ï¼‰ã€‚æœ¬ç ”ç©¶è¡¨æ˜ï¼Œç›¸å¯¹äºå½“å‰çš„CRTæ–¹æ³•ï¼ŒSA-IMRTåœ¨æ”¾å°„æ²»ç–—è®¡åˆ’è´¨é‡æ–¹é¢æä¾›äº†æ˜¾è‘—æ”¹å–„ï¼Œå¹¶äº§ç”Ÿäº†æ›´æ¥è¿‘äºä¸´åºŠç¯å¢ƒçš„å‰‚é‡åˆ†å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.15684v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶æ¯”è¾ƒäº†å°åŠ¨ç‰©IMRTï¼ˆSA-IMRTï¼‰ä¸ä¼ ç»Ÿä¸‰ç»´é€‚å½¢æ”¾ç–—ï¼ˆCRTï¼‰åœ¨æ²»ç–—è‚¿ç˜¤ç¼ºæ°§ä¸­çš„è®¡åˆ’å·®å¼‚ã€‚é‡‡ç”¨æ–°å‹è¡¥å¿å™¨å¹³å°ï¼Œå¯¹17åªæºå¸¦çº¤ç»´è‚‰ç˜¤è‚¿ç˜¤çš„é¼ æ ‡è¿›è¡Œ2-15ä¸ªå…‰æŸè§’åº¦çš„è®¡åˆ’åˆ¶å®šã€‚ç»“æœæ˜¾ç¤ºï¼Œå¯¹äº3ä¸ªè§’åº¦ï¼ŒSA-IMRTåœ¨å‰‚é‡ç¬¦åˆåº¦ã€è‚¿ç˜¤å‰‚é‡å‡åŒ€æ€§å’Œå‰‚é‡ä½“ç§¯ç›´æ–¹å›¾ç­‰æ–¹é¢æ˜¾è‘—ä¼˜äºCRTã€‚éšç€å…‰æŸè§’åº¦çš„å¢åŠ ï¼Œæ”¹å–„å¹¶ä¸æ˜¾è‘—ã€‚æ€»ä½“è€Œè¨€ï¼ŒSA-IMRTåœ¨è®¡åˆ’è´¨é‡å’Œå‰‚é‡åˆ†å¸ƒæ–¹é¢æä¾›æ›´ä¼˜è¶Šçš„æ¨¡æ‹Ÿä¸´åºŠç¯å¢ƒæ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬ç ”ç©¶å¯¹æ¯”äº†å°åŠ¨ç‰©IMRTï¼ˆSA-IMRTï¼‰ä¸ä¸‰ç»´é€‚å½¢æ”¾ç–—ï¼ˆCRTï¼‰åœ¨æ²»ç–—è‚¿ç˜¤ç¼ºæ°§æ–¹é¢çš„è®¡åˆ’å·®å¼‚ã€‚</li>
<li>é‡‡ç”¨æ–°å‹è¡¥å¿å™¨å¹³å°è¿›è¡Œæ²»ç–—è®¡åˆ’ã€‚</li>
<li>å¯¹äº3ä¸ªå…‰æŸè§’åº¦ï¼ŒSA-IMRTåœ¨å‰‚é‡ç¬¦åˆåº¦ã€è‚¿ç˜¤å‰‚é‡å‡åŒ€æ€§å’Œå‰‚é‡ä½“ç§¯ç›´æ–¹å›¾ç­‰æ–¹é¢æ˜¾è‘—ä¼˜äºCRTã€‚</li>
<li>SA-IMRTæä¾›çš„è®¡åˆ’è´¨é‡æ”¹å–„æ˜¾è‘—ï¼Œå‰‚é‡åˆ†å¸ƒæ›´è´´è¿‘ä¸´åºŠç¯å¢ƒã€‚</li>
<li>éšç€å…‰æŸè§’åº¦å¢åŠ ï¼Œæ”¹å–„å¹¶ä¸æ˜¾è‘—ã€‚</li>
<li>ç ”ç©¶ä½¿ç”¨çš„æ¨¡å‹åº”ç”¨æ˜¯æ°§å¯¼å‘çš„å‰‚é‡æç»˜è‚¿ç˜¤ç¼ºæ°§ï¼Œä½¿ç”¨å®é™…çš„è€é¼ æ•°æ®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.15684">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c398b21b919c46f7284e2fae84847af8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d1fbeadb3594628e47d8f6e980b126d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c0a08dbdfbf2e929157476ed019d133e.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Marker-Track-Accurate-Fiducial-Marker-Tracking-for-Evaluation-of-Residual-Motions-During-Breath-Hold-Radiotherapy"><a href="#Marker-Track-Accurate-Fiducial-Marker-Tracking-for-Evaluation-of-Residual-Motions-During-Breath-Hold-Radiotherapy" class="headerlink" title="Marker Track: Accurate Fiducial Marker Tracking for Evaluation of   Residual Motions During Breath-Hold Radiotherapy"></a>Marker Track: Accurate Fiducial Marker Tracking for Evaluation of   Residual Motions During Breath-Hold Radiotherapy</h2><p><strong>Authors:Aimee Guo, Weihua Mao</strong></p>
<p>Fiducial marker positions in projection image of cone-beam computed tomography (CBCT) scans have been studied to evaluate daily residual motion during breath-hold radiation therapy. Fiducial marker migration posed challenges in accurately locating markers, prompting the development of a novel algorithm that reconstructs volumetric probability maps of marker locations from filtered gradient maps of projections. This guides the development of a Python-based algorithm to detect fiducial markers in projection images using Meta AIâ€™s Segment Anything Model 2 (SAM 2). Retrospective data from a pancreatic cancer patient with two fiducial markers were analyzed. The three-dimensional (3D) marker positions from simulation computed tomography (CT) were compared to those reconstructed from CBCT images, revealing a decrease in relative distances between markers over time. Fiducial markers were successfully detected in 2777 out of 2786 projection frames. The average standard deviation of superior-inferior (SI) marker positions was 0.56 mm per breath-hold, with differences in average SI positions between two breath-holds in the same scan reaching up to 5.2 mm, and a gap of up to 7.3 mm between the end of the first and beginning of the second breath-hold. 3D marker positions were calculated using projection positions and confirmed marker migration. This method effectively calculates marker probability volume and enables accurate fiducial marker tracking during treatment without requiring any specialized equipment, additional radiation doses, or manual initialization and labeling. It has significant potential for automatically assessing daily residual motion to adjust planning margins, functioning as an adaptive radiation therapy tool. </p>
<blockquote>
<p>åœ¨ç ”ç©¶é”¥æŸè®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCBCTï¼‰æŠ•å½±å›¾åƒä¸­çš„æ ‡è®°ç‚¹ä½ç½®æ—¶ï¼Œæ—¨åœ¨è¯„ä¼°å‘¼å¸æ§åˆ¶æ”¾å°„æ²»ç–—æœŸé—´çš„æ¯æ—¥æ®‹ç•™è¿åŠ¨ã€‚æ ‡è®°ç‚¹çš„è¿ç§»åœ¨å‡†ç¡®å®šä½æ ‡è®°ç‚¹æ–¹é¢å¸¦æ¥äº†æŒ‘æˆ˜ï¼Œä¿ƒä½¿å¼€å‘äº†ä¸€ç§æ–°å‹ç®—æ³•ï¼Œè¯¥ç®—æ³•é€šè¿‡æŠ•å½±çš„æ¢¯åº¦å›¾è¿‡æ»¤æ¥é‡å»ºæ ‡è®°ç‚¹ä½ç½®çš„ä½“ç§¯æ¦‚ç‡å›¾ã€‚è¿™ä¸ºåŸºäºPythonçš„ç®—æ³•å¼€å‘æä¾›äº†æŒ‡å¯¼ï¼Œè¯¥ç®—æ³•ä½¿ç”¨Meta AIçš„Segment Anything Model 2ï¼ˆSAM 2ï¼‰æ¥æ£€æµ‹æŠ•å½±å›¾åƒä¸­çš„æ ‡è®°ç‚¹ã€‚åˆ†æäº†ä¸€ä½èƒ°è…ºç™Œæ‚£è€…çš„å›é¡¾æ€§æ•°æ®ï¼Œè¯¥æ‚£è€…æœ‰ä¸¤ä¸ªæ ‡è®°ç‚¹ã€‚æ¨¡æ‹Ÿè®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰ä¸­çš„ä¸‰ç»´ï¼ˆ3Dï¼‰æ ‡è®°ç‚¹ä½ç½®ä¸ä»CBCTå›¾åƒé‡å»ºçš„ä½ç½®è¿›è¡Œäº†æ¯”è¾ƒï¼Œç»“æœæ˜¾ç¤ºæ ‡è®°ç‚¹ä¹‹é—´çš„ç›¸å¯¹è·ç¦»éšæ—¶é—´å‡å°‘ã€‚åœ¨2786ä¸ªæŠ•å½±å¸§ä¸­ï¼Œæ ‡è®°ç‚¹æˆåŠŸæ£€æµ‹äº†2777ä¸ªã€‚åŒä¸€æ‰«æä¸­ä¸¤æ¬¡å‘¼å¸ä¹‹é—´ä¸Šä½-ä¸‹ä½ï¼ˆSIï¼‰æ ‡è®°ç‚¹ä½ç½®çš„å¹³å‡æ ‡å‡†åå·®ä¸ºæ¯æ¬¡å‘¼å¸ä¿æŒ0.56æ¯«ç±³ï¼Œä¸¤æ¬¡å‘¼å¸ä¹‹é—´çš„å¹³å‡SIä½ç½®å·®å¼‚å¯è¾¾5.2æ¯«ç±³ï¼Œç¬¬ä¸€æ¬¡å‘¼å¸ä¿æŒç»“æŸä¸ç¬¬äºŒæ¬¡å¼€å§‹ä¹‹é—´çš„é—´éš”å¯è¾¾7.3æ¯«ç±³ã€‚ä½¿ç”¨æŠ•å½±ä½ç½®å’Œç¡®è®¤çš„æ ‡è®°è¿ç§»è®¡ç®—äº†3Dæ ‡è®°ç‚¹ä½ç½®ã€‚è¯¥æ–¹æ³•æœ‰æ•ˆåœ°è®¡ç®—äº†æ ‡è®°æ¦‚ç‡ä½“ç§¯ï¼Œå¹¶èƒ½å¤Ÿåœ¨æ²»ç–—è¿‡ç¨‹ä¸­å®ç°å‡†ç¡®çš„æ ‡è®°ç‚¹è·Ÿè¸ªï¼Œæ— éœ€ä»»ä½•ä¸“ç”¨è®¾å¤‡ã€é¢å¤–çš„è¾å°„å‰‚é‡æˆ–æ‰‹åŠ¨åˆå§‹åŒ–å’Œæ ‡è®°ã€‚å¯¹äºè‡ªåŠ¨è¯„ä¼°æ¯æ—¥æ®‹ç•™è¿åŠ¨ä»¥è°ƒæ•´è®¡åˆ’ä½™é‡å¹¶ä½œä¸ºè‡ªé€‚åº”æ”¾å°„æ²»ç–—å·¥å…·ï¼Œè¯¥æ–¹æ³•å…·æœ‰å·¨å¤§çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.15660v1">PDF</a> 14 pages, 9 figures, Regeneron STS 2025 project. Project page:   <a target="_blank" rel="noopener" href="https://sites.google.com/view/markertrack?usp=sharing">https://sites.google.com/view/markertrack?usp=sharing</a></p>
<p><strong>æ‘˜è¦</strong><br>    åŸºäºé”¥å½¢æŸè®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCBCTï¼‰æŠ•å½±å›¾åƒçš„æ ‡è®°ç‚¹ä½ç½®ç ”ç©¶ï¼Œç”¨äºè¯„ä¼°å‘¼å¸æ§åˆ¶æ”¾å°„æ²»ç–—æœŸé—´çš„æ¯æ—¥æ®‹ä½™è¿åŠ¨ã€‚ç”±äºæ ‡è®°ç‚¹è¿ç§»å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œç ”ç©¶è€…å¼€å‘å‡ºä¸€ç§åŸºäºæŠ•å½±æ¢¯åº¦å›¾è¿‡æ»¤é‡å»ºæ ‡è®°ç‚¹ä½ç½®æ¦‚ç‡å›¾çš„ç®—æ³•ã€‚è¿™å¼•å¯¼äº†åŸºäºPythonçš„ç®—æ³•å¼€å‘ï¼Œåˆ©ç”¨Meta AIçš„SAM 2æ¨¡å‹æ£€æµ‹æŠ•å½±å›¾åƒä¸­çš„æ ‡è®°ç‚¹ã€‚å›é¡¾åˆ†æèƒ°è…ºç™Œæ‚£è€…çš„æ•°æ®ï¼Œå¯¹æ¯”æ¨¡æ‹Ÿè®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰çš„3Dæ ‡è®°ç‚¹ä½ç½®ä¸ä»CBCTå›¾åƒé‡å»ºçš„ä½ç½®ï¼Œå‘ç°éšæ—¶é—´æ¨ç§»æ ‡è®°ç‚¹é—´çš„ç›¸å¯¹è·ç¦»ç¼©çŸ­ã€‚åœ¨2786ä¸ªæŠ•å½±å¸§ä¸­æˆåŠŸæ£€æµ‹åˆ°2777ä¸ªæ ‡è®°ç‚¹ã€‚å‘¼å¸æ§åˆ¶æœŸé—´ä¸Š-ä¸‹ï¼ˆSIï¼‰æ ‡è®°ç‚¹ä½ç½®çš„å¹³å‡æ ‡å‡†åå·®ä¸ºæ¯å‘¼å¸æ§åˆ¶æœŸ0.56æ¯«ç±³ï¼ŒåŒä¸€æ‰«æä¸­ä¸¤ä¸ªå‘¼å¸æ§åˆ¶æœŸä¹‹é—´çš„å¹³å‡SIä½ç½®å·®å¼‚æœ€å¤§å¯è¾¾5.2æ¯«ç±³ï¼Œé¦–ä¸ªå‘¼å¸æ§åˆ¶æœŸç»“æŸä¸ç¬¬äºŒä¸ªå‘¼å¸æ§åˆ¶æœŸå¼€å§‹ä¹‹é—´çš„é—´éš”æœ€å¤§å¯è¾¾7.3æ¯«ç±³ã€‚è¯¥æ–¹æ³•æœ‰æ•ˆè®¡ç®—æ ‡è®°ç‚¹æ¦‚ç‡ä½“ç§¯ï¼Œå¯åœ¨ä¸å¢åŠ ç‰¹æ®Šè®¾å¤‡ã€è¾å°„å‰‚é‡æˆ–æ‰‹åŠ¨åˆå§‹åŒ–å’Œæ ‡è®°çš„æƒ…å†µä¸‹ï¼Œå‡†ç¡®è·Ÿè¸ªæ²»ç–—è¿‡ç¨‹ä¸­çš„æ ‡è®°ç‚¹ä½ç½®ã€‚è¿™å¯¹äºè‡ªåŠ¨è¯„ä¼°æ¯æ—¥æ®‹ä½™è¿åŠ¨ä»¥è°ƒæ•´è®¡åˆ’ä½™é‡ã€ä½œä¸ºè‡ªé€‚åº”æ”¾å°„æ²»ç–—å·¥å…·å…·æœ‰æ˜¾è‘—æ½œåŠ›ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç ”ç©¶åˆ©ç”¨CBCTæŠ•å½±å›¾åƒçš„æ ‡è®°ç‚¹ä½ç½®è¯„ä¼°å‘¼å¸æ§åˆ¶æ”¾å°„æ²»ç–—æœŸé—´çš„æ¯æ—¥æ®‹ä½™è¿åŠ¨ã€‚</li>
<li>æ ‡è®°ç‚¹è¿ç§»å¸¦æ¥å®šä½æŒ‘æˆ˜ï¼Œæ¨åŠ¨å¼€å‘æ–°å‹ç®—æ³•ä»¥é‡å»ºæ ‡è®°ç‚¹ä½ç½®æ¦‚ç‡å›¾ã€‚</li>
<li>ä½¿ç”¨Pythonå’ŒMeta AIçš„SAM 2æ¨¡å‹æ£€æµ‹æŠ•å½±å›¾åƒä¸­çš„æ ‡è®°ç‚¹ã€‚</li>
<li>å¯¹æ¯”æ¨¡æ‹ŸCTä¸CBCTå›¾åƒçš„3Dæ ‡è®°ç‚¹ä½ç½®ï¼Œå‘ç°éšæ—¶é—´æ¨ç§»æ ‡è®°ç‚¹é—´çš„ç›¸å¯¹è·ç¦»ç¼©çŸ­ã€‚</li>
<li>åœ¨å¤§éƒ¨åˆ†æŠ•å½±å¸§ä¸­æˆåŠŸæ£€æµ‹åˆ°æ ‡è®°ç‚¹ï¼Œéƒ¨åˆ†ä½ç½®å·®å¼‚åœ¨æ¯«ç±³çº§åˆ«ã€‚</li>
<li>è¯¥æ–¹æ³•æœ‰æ•ˆè®¡ç®—æ ‡è®°ç‚¹æ¦‚ç‡ä½“ç§¯ï¼Œå¯å‡†ç¡®è·Ÿè¸ªæ²»ç–—è¿‡ç¨‹ä¸­çš„æ ‡è®°ç‚¹ä½ç½®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.15660">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-914e6c4c8e0202be6b778a8ff8c7ca08.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e46961d0a0f29c53a93813bf25356d40.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-acad49e4d20270c89842836b96e97676.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bf36adde48deeabf7ba9eb42987a30c3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-afa5f84a422c9b4edba50d10abd1af99.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Radiologist-in-the-Loop-Self-Training-for-Generalizable-CT-Metal-Artifact-Reduction"><a href="#Radiologist-in-the-Loop-Self-Training-for-Generalizable-CT-Metal-Artifact-Reduction" class="headerlink" title="Radiologist-in-the-Loop Self-Training for Generalizable CT Metal   Artifact Reduction"></a>Radiologist-in-the-Loop Self-Training for Generalizable CT Metal   Artifact Reduction</h2><p><strong>Authors:Chenglong Ma, Zilong Li, Yuanlin Li, Jing Han, Junping Zhang, Yi Zhang, Jiannan Liu, Hongming Shan</strong></p>
<p>Metal artifacts in computed tomography (CT) images can significantly degrade image quality and impede accurate diagnosis. Supervised metal artifact reduction (MAR) methods, trained using simulated datasets, often struggle to perform well on real clinical CT images due to a substantial domain gap. Although state-of-the-art semi-supervised methods use pseudo ground-truths generated by a prior network to mitigate this issue, their reliance on a fixed prior limits both the quality and quantity of these pseudo ground-truths, introducing confirmation bias and reducing clinical applicability. To address these limitations, we propose a novel Radiologist-In-the-loop SElf-training framework for MAR, termed RISE-MAR, which can integrate radiologistsâ€™ feedback into the semi-supervised learning process, progressively improving the quality and quantity of pseudo ground-truths for enhanced generalization on real clinical CT images. For quality assurance, we introduce a clinical quality assessor model that emulates radiologist evaluations, effectively selecting high-quality pseudo ground-truths for semi-supervised training. For quantity assurance, our self-training framework iteratively generates additional high-quality pseudo ground-truths, expanding the clinical dataset and further improving model generalization. Extensive experimental results on multiple clinical datasets demonstrate the superior generalization performance of our RISE-MAR over state-of-the-art methods, advancing the development of MAR models for practical application. Code is available at <a target="_blank" rel="noopener" href="https://github.com/Masaaki-75/rise-mar">https://github.com/Masaaki-75/rise-mar</a>. </p>
<blockquote>
<p>è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰å›¾åƒä¸­çš„é‡‘å±ä¼ªå½±ä¼šæ˜¾è‘—é™ä½å›¾åƒè´¨é‡å¹¶é˜»ç¢å‡†ç¡®è¯Šæ–­ã€‚ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®é›†è®­ç»ƒçš„ç›‘ç£å‹é‡‘å±ä¼ªå½±å‡å°‘ï¼ˆMARï¼‰æ–¹æ³•é€šå¸¸åœ¨å®é™…çš„ä¸´åºŠCTå›¾åƒä¸Šè¡¨ç°ä¸ä½³ï¼Œè¿™ä¸»è¦æ˜¯å› ä¸ºå­˜åœ¨è¾ƒå¤§çš„é¢†åŸŸå·®è·ã€‚å°½ç®¡æœ€å…ˆè¿›çš„åŠç›‘ç£æ–¹æ³•ä½¿ç”¨ç”±å…ˆå‰ç½‘ç»œç”Ÿæˆçš„ä¼ªçœŸå®æ ‡ç­¾æ¥ç¼“è§£è¿™ä¸ªé—®é¢˜ï¼Œä½†å®ƒä»¬å¯¹å›ºå®šå…ˆéªŒçš„ä¾èµ–é™åˆ¶äº†è¿™äº›ä¼ªçœŸå®æ ‡ç­¾çš„è´¨é‡å’Œæ•°é‡ï¼Œå¼•å…¥äº†ç¡®è®¤åè§å¹¶é™ä½äº†ä¸´åºŠé€‚ç”¨æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„é‡‘å±ä¼ªå½±å‡å°‘çš„åŠç›‘ç£è‡ªè®­ç»ƒæ¡†æ¶ï¼Œåä¸ºRISE-MARã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿå°†æ”¾å°„ç§‘åŒ»ç”Ÿçš„åé¦ˆæ•´åˆåˆ°åŠç›‘ç£å­¦ä¹ è¿‡ç¨‹ä¸­ï¼Œé€æ­¥æ”¹å–„ä¼ªçœŸå®æ ‡ç­¾çš„è´¨é‡å’Œæ•°é‡ï¼Œä»¥æé«˜åœ¨å®é™…ä¸´åºŠCTå›¾åƒä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸ºä¿è¯è´¨é‡ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸´åºŠè´¨é‡è¯„ä¼°æ¨¡å‹ï¼Œæ¨¡æ‹Ÿæ”¾å°„ç§‘åŒ»ç”Ÿè¯„ä¼°ï¼Œæœ‰æ•ˆåœ°é€‰æ‹©é«˜è´¨é‡çš„ä¼ªçœŸå®æ ‡ç­¾ç”¨äºåŠç›‘ç£è®­ç»ƒã€‚ä¸ºä¿è¯æ•°é‡ï¼Œæˆ‘ä»¬çš„è‡ªè®­ç»ƒæ¡†æ¶é€šè¿‡è¿­ä»£ç”Ÿæˆé¢å¤–çš„é«˜è´¨é‡ä¼ªçœŸå®æ ‡ç­¾ï¼Œæ‰©å¤§ä¸´åºŠæ•°æ®é›†ï¼Œè¿›ä¸€æ­¥æ”¹å–„æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚åœ¨å¤šä¸ªä¸´åºŠæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒç»“æœè¯æ˜ï¼Œæˆ‘ä»¬çš„RISE-MARçš„æ³›åŒ–æ€§èƒ½ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œæ¨åŠ¨äº†MARæ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„å‘å±•ã€‚ç›¸å…³ä»£ç å¯é€šè¿‡ä»¥ä¸‹é“¾æ¥è·å–ï¼š<a target="_blank" rel="noopener" href="https://github.com/Masaaki-75/rise-mar%E3%80%82">https://github.com/Masaaki-75/rise-marã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.15610v1">PDF</a> IEEE TMI 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§èåˆæ”¾å°„ç§‘åŒ»ç”Ÿåé¦ˆçš„åŠç›‘ç£å­¦ä¹ æ–¹æ³•RISE-MARï¼Œç”¨äºå‡å°‘è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰å›¾åƒä¸­çš„é‡‘å±ä¼ªå½±ã€‚è¯¥æ–¹æ³•é€šè¿‡é€æ­¥æ”¹è¿›ä¼ªçœŸå®æ ‡ç­¾çš„è´¨é‡å’Œæ•°é‡ï¼Œæé«˜äº†æ¨¡å‹åœ¨çœŸå®ä¸´åºŠCTå›¾åƒä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚å¼•å…¥ä¸´åºŠè´¨é‡è¯„ä¼°æ¨¡å‹æ¨¡æ‹Ÿæ”¾å°„ç§‘åŒ»ç”Ÿè¯„ä¼°ï¼Œé€‰æ‹©é«˜è´¨é‡çš„ä¼ªçœŸå®æ ‡ç­¾è¿›è¡ŒåŠç›‘ç£è®­ç»ƒï¼ŒåŒæ—¶è¿­ä»£ç”Ÿæˆæ›´å¤šé«˜è´¨é‡çš„ä¼ªçœŸå®æ ‡ç­¾ï¼Œæ‰©å¤§ä¸´åºŠæ•°æ®é›†ï¼Œè¿›ä¸€æ­¥æé«˜æ¨¡å‹çš„æ³›åŒ–æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRISE-MARæ–¹æ³•ç›¸è¾ƒäºç°æœ‰æ–¹æ³•åœ¨é‡‘å±ä¼ªå½±å»é™¤æ–¹é¢è¡¨ç°å‡ºæ›´ä¼˜è¶Šçš„æ³›åŒ–æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>é‡‘å±ä¼ªå½±åœ¨CTå›¾åƒä¸­ä¼šå¯¼è‡´å›¾åƒè´¨é‡ä¸‹é™ï¼Œå½±å“å‡†ç¡®è¯Šæ–­ã€‚</li>
<li>ç°æœ‰ç›‘ç£å‹é‡‘å±ä¼ªå½±å‡å°‘ï¼ˆMARï¼‰æ–¹æ³•åœ¨å®é™…ä¸´åºŠCTå›¾åƒä¸Šçš„è¡¨ç°å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>æå‡ºçš„RISE-MARæ–¹æ³•ç»“åˆæ”¾å°„ç§‘åŒ»ç”Ÿåé¦ˆï¼Œé€æ­¥æ”¹è¿›ä¼ªçœŸå®æ ‡ç­¾çš„è´¨é‡å’Œæ•°é‡ã€‚</li>
<li>å¼•å…¥ä¸´åºŠè´¨é‡è¯„ä¼°æ¨¡å‹æ¨¡æ‹Ÿæ”¾å°„ç§‘åŒ»ç”Ÿè¯„ä¼°ï¼Œé€‰æ‹©é«˜è´¨é‡çš„ä¼ªçœŸå®æ ‡ç­¾è¿›è¡ŒåŠç›‘ç£è®­ç»ƒã€‚</li>
<li>RISE-MARæ–¹æ³•é€šè¿‡è¿­ä»£ç”Ÿæˆæ›´å¤šé«˜è´¨é‡çš„ä¼ªçœŸå®æ ‡ç­¾ï¼Œæ‰©å¤§ä¸´åºŠæ•°æ®é›†ï¼Œæé«˜æ¨¡å‹çš„æ³›åŒ–æ€§èƒ½ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜RISE-MARæ–¹æ³•ç›¸è¾ƒäºç°æœ‰æ–¹æ³•åœ¨é‡‘å±ä¼ªå½±å»é™¤æ–¹é¢è¡¨ç°ä¼˜è¶Šã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.15610">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-328937aa2e234228f4e766a9f322bce8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-06cf73ab76132ee256c8890636acd3e2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e0af66933c9e283e9201608cad06b620.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0ac7e34420ef71322b6f7f5fb4e6c305.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9d86224c104ede1b8c55cf43c11f5e9a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-db4c8f966d1bafa724625139f67ac697.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Tumor-Detection-Segmentation-and-Classification-Challenge-on-Automated-3D-Breast-Ultrasound-The-TDSC-ABUS-Challenge"><a href="#Tumor-Detection-Segmentation-and-Classification-Challenge-on-Automated-3D-Breast-Ultrasound-The-TDSC-ABUS-Challenge" class="headerlink" title="Tumor Detection, Segmentation and Classification Challenge on Automated   3D Breast Ultrasound: The TDSC-ABUS Challenge"></a>Tumor Detection, Segmentation and Classification Challenge on Automated   3D Breast Ultrasound: The TDSC-ABUS Challenge</h2><p><strong>Authors:Gongning Luo, Mingwang Xu, Hongyu Chen, Xinjie Liang, Xing Tao, Dong Ni, Hyunsu Jeong, Chulhong Kim, Raphael Stock, Michael Baumgartner, Yannick Kirchhoff, Maximilian Rokuss, Klaus Maier-Hein, Zhikai Yang, Tianyu Fan, Nicolas Boutry, Dmitry Tereshchenko, Arthur Moine, Maximilien Charmetant, Jan Sauer, Hao Du, Xiang-Hui Bai, Vipul Pai Raikar, Ricardo Montoya-del-Angel, Robert Marti, Miguel Luna, Dongmin Lee, Abdul Qayyum, Moona Mazher, Qihui Guo, Changyan Wang, Navchetan Awasthi, Qiaochu Zhao, Wei Wang, Kuanquan Wang, Qiucheng Wang, Suyu Dong</strong></p>
<p>Breast cancer is one of the most common causes of death among women worldwide. Early detection helps in reducing the number of deaths. Automated 3D Breast Ultrasound (ABUS) is a newer approach for breast screening, which has many advantages over handheld mammography such as safety, speed, and higher detection rate of breast cancer. Tumor detection, segmentation, and classification are key components in the analysis of medical images, especially challenging in the context of 3D ABUS due to the significant variability in tumor size and shape, unclear tumor boundaries, and a low signal-to-noise ratio. The lack of publicly accessible, well-labeled ABUS datasets further hinders the advancement of systems for breast tumor analysis. Addressing this gap, we have organized the inaugural Tumor Detection, Segmentation, and Classification Challenge on Automated 3D Breast Ultrasound 2023 (TDSC-ABUS2023). This initiative aims to spearhead research in this field and create a definitive benchmark for tasks associated with 3D ABUS image analysis. In this paper, we summarize the top-performing algorithms from the challenge and provide critical analysis for ABUS image examination. We offer the TDSC-ABUS challenge as an open-access platform at <a target="_blank" rel="noopener" href="https://tdsc-abus2023.grand-challenge.org/">https://tdsc-abus2023.grand-challenge.org/</a> to benchmark and inspire future developments in algorithmic research. </p>
<blockquote>
<p>ä¹³è…ºç™Œæ˜¯å…¨çƒå¥³æ€§æ­»äº¡çš„ä¸»è¦åŸå› ä¹‹ä¸€ã€‚æ—©æœŸå‘ç°æœ‰åŠ©äºå‡å°‘æ­»äº¡äººæ•°ã€‚è‡ªåŠ¨ä¸‰ç»´ä¹³è…ºè¶…å£°ï¼ˆABUSï¼‰æ˜¯ä¸€ç§æ–°å‹çš„ä¹³è…ºç­›æŸ¥æ–¹æ³•ï¼Œç›¸å¯¹äºæ‰‹æŒå¼ä¹³è…ºXçº¿æ‘„å½±ï¼Œå®ƒå…·æœ‰å®‰å…¨ã€å¿«é€Ÿå’Œæ›´é«˜çš„ä¹³è…ºç™Œæ£€æµ‹ç‡ç­‰ä¼˜ç‚¹ã€‚è‚¿ç˜¤æ£€æµ‹ã€åˆ†å‰²å’Œåˆ†ç±»æ˜¯åŒ»å­¦å›¾åƒåˆ†æçš„å…³é”®ç»„æˆéƒ¨åˆ†ï¼Œåœ¨3D ABUSçš„ä¸Šä¸‹æ–‡ä¸­å°¤å…¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œè¿™æ˜¯ç”±äºè‚¿ç˜¤å¤§å°ã€å½¢çŠ¶å­˜åœ¨é‡å¤§å·®å¼‚ï¼Œè‚¿ç˜¤è¾¹ç•Œä¸æ¸…ï¼Œä»¥åŠä¿¡å™ªæ¯”ä½ã€‚ç¼ºä¹å¯å…¬å¼€è®¿é—®ã€æ ‡æ³¨è‰¯å¥½çš„ABUSæ•°æ®é›†è¿›ä¸€æ­¥é˜»ç¢äº†ä¹³è…ºè‚¿ç˜¤åˆ†æç³»ç»Ÿçš„è¿›æ­¥ã€‚ä¸ºè§£å†³è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬ç»„ç»‡äº†é¦–å±Šè‡ªåŠ¨ä¸‰ç»´ä¹³è…ºè¶…å£°è‚¿ç˜¤æ£€æµ‹ã€åˆ†å‰²å’Œåˆ†ç±»æŒ‘æˆ˜èµ›ï¼ˆTDSC-ABUS2023ï¼‰ã€‚è¿™ä¸€å€¡è®®æ—¨åœ¨å¼•é¢†è¯¥é¢†åŸŸçš„ç ”ç©¶ï¼Œå¹¶ä¸ºä¸3D ABUSå›¾åƒåˆ†æç›¸å…³çš„ä»»åŠ¡å»ºç«‹æ˜ç¡®çš„åŸºå‡†ã€‚æœ¬æ–‡æ€»ç»“äº†æŒ‘æˆ˜èµ›ä¸­è¡¨ç°æœ€ä½³çš„ç®—æ³•ï¼Œå¹¶å¯¹ABUSå›¾åƒæ£€æŸ¥è¿›è¡Œäº†å…³é”®åˆ†æã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://tdsc-abus2023.grand-challenge.org/">https://tdsc-abus2023.grand-challenge.org/</a>ä¸Šæä¾›TDSC-ABUSæŒ‘æˆ˜ä½œä¸ºå¼€æ”¾è®¿é—®å¹³å°ï¼Œä»¥è¯„ä¼°å’Œæ¿€åŠ±ç®—æ³•ç ”ç©¶çš„æœªæ¥å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.15588v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¹³è…ºç™Œçš„æ™®éæ€§å’Œæ—©æœŸæ£€æµ‹çš„é‡è¦æ€§ï¼Œé‡ç‚¹ä»‹ç»äº†è‡ªåŠ¨åŒ–3Dä¹³è…ºè¶…å£°ï¼ˆABUSï¼‰è¿™ä¸€æ–°å…´ä¹³è…ºç­›æŸ¥æ–¹æ³•ç›¸è¾ƒäºæ‰‹æŒå¼ä¹³è…ºæ‘„å½±æœ¯çš„ä¼˜åŠ¿ã€‚æ–‡ç« æŒ‡å‡ºï¼Œè‚¿ç˜¤æ£€æµ‹ã€åˆ†å‰²å’Œåˆ†ç±»åœ¨åŒ»å­¦å›¾åƒåˆ†æä¸­çš„å…³é”®ä½œç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨3D ABUSä¸­çš„æŒ‘æˆ˜ã€‚ä¸ºè§£å†³ç¼ºä¹å…¬å¼€ã€æ ‡æ³¨è‰¯å¥½çš„ABUSæ•°æ®é›†çš„é—®é¢˜ï¼Œä¸¾åŠäº†é¦–å±Šè‚¿ç˜¤æ£€æµ‹ã€åˆ†å‰²å’Œåˆ†ç±»æŒ‘æˆ˜èµ›â€”â€”è‡ªåŠ¨åŒ–3Dä¹³è…ºè¶…å£°2023ï¼ˆTDSC-ABUS2023ï¼‰ã€‚è¯¥æŒ‘æˆ˜æ—¨åœ¨æ¨åŠ¨è¯¥é¢†åŸŸçš„ç ”ç©¶ï¼Œå¹¶ä¸º3D ABUSå›¾åƒåˆ†æä»»åŠ¡å»ºç«‹åŸºå‡†æ ‡å‡†ã€‚æœ¬æ–‡æ€»ç»“äº†æŒ‘æˆ˜ä¸­è¡¨ç°æœ€ä½³çš„ç®—æ³•ï¼Œå¹¶å¯¹ABUSå›¾åƒæ£€æŸ¥è¿›è¡Œäº†å…³é”®åˆ†æã€‚åŒæ—¶æä¾›äº†TDSC-ABUSæŒ‘æˆ˜çš„å¼€æ”¾è®¿é—®å¹³å°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¹³è…ºç™Œæ˜¯ä¸–ç•ŒèŒƒå›´å†…å¯¼è‡´å¥³æ€§æ­»äº¡çš„ä¸»è¦åŸå› ä¹‹ä¸€ï¼Œæ—©æœŸå‘ç°å¯¹å‡å°‘æ­»äº¡äººæ•°è‡³å…³é‡è¦ã€‚</li>
<li>è‡ªåŠ¨åŒ–3Dä¹³è…ºè¶…å£°ï¼ˆABUSï¼‰æ˜¯ä¸€ç§æ–°å…´çš„ä¹³è…ºç­›æŸ¥æ–¹æ³•ï¼Œç›¸å¯¹äºæ‰‹æŒå¼ä¹³è…ºæ‘„å½±æœ¯å…·æœ‰å®‰å…¨ã€å¿«é€Ÿå’Œæ›´é«˜çš„ç™Œç—‡æ£€æµ‹ç‡ç­‰ä¼˜ç‚¹ã€‚</li>
<li>è‚¿ç˜¤æ£€æµ‹ã€åˆ†å‰²å’Œåˆ†ç±»æ˜¯åŒ»å­¦å›¾åƒåˆ†æçš„å…³é”®éƒ¨åˆ†ï¼Œå°¤å…¶åœ¨3D ABUSä¸­é¢ä¸´æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬è‚¿ç˜¤å¤§å°ã€å½¢çŠ¶çš„å·¨å¤§å·®å¼‚ã€è‚¿ç˜¤è¾¹ç•Œä¸æ¸…æ™°å’Œä¿¡å™ªæ¯”ä½ç­‰ã€‚</li>
<li>ç¼ºä¹å…¬å¼€ã€æ ‡æ³¨è‰¯å¥½çš„ABUSæ•°æ®é›†é˜»ç¢äº†ä¹³è…ºè‚¿ç˜¤åˆ†æç³»ç»Ÿçš„è¿›å±•ã€‚</li>
<li>TDSC-ABUS2023æŒ‘æˆ˜èµ›æ—¨åœ¨æ¨åŠ¨3D ABUSå›¾åƒåˆ†æé¢†åŸŸçš„ç ”ç©¶ï¼Œå¹¶å»ºç«‹åŸºå‡†æ ‡å‡†ã€‚</li>
<li>æŒ‘æˆ˜èµ›ä¸­è¡¨ç°æœ€ä½³çš„ç®—æ³•è¢«æ€»ç»“åœ¨æœ¬è®ºæ–‡ä¸­ï¼Œå¹¶å¯¹ABUSå›¾åƒæ£€æŸ¥è¿›è¡Œäº†å…³é”®åˆ†æã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.15588">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2811f111cf82d1b45ea48dbd665cbe52.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-abcb217b9d9b9974c29473f9a5f06119.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-74e94c59488bbe7da9ebdc7f9eb56c52.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2400d7bab2d75c1b9f9b3c94428109cf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-85e577b8a7b03b77a0c544992d5b16b6.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Comparative-clinical-evaluation-of-â€œmemory-efficientâ€-synthetic-3d-generative-adversarial-networks-gan-head-to-head-to-state-of-art-results-on-computed-tomography-of-the-chest"><a href="#Comparative-clinical-evaluation-of-â€œmemory-efficientâ€-synthetic-3d-generative-adversarial-networks-gan-head-to-head-to-state-of-art-results-on-computed-tomography-of-the-chest" class="headerlink" title="Comparative clinical evaluation of â€œmemory-efficientâ€ synthetic 3d   generative adversarial networks (gan) head-to-head to state of art: results   on computed tomography of the chest"></a>Comparative clinical evaluation of â€œmemory-efficientâ€ synthetic 3d   generative adversarial networks (gan) head-to-head to state of art: results   on computed tomography of the chest</h2><p><strong>Authors:Mahshid shiri, Chandra Bortolotto, Alessandro Bruno, Alessio Consonni, Daniela Maria Grasso, Leonardo Brizzi, Daniele Loiacono, Lorenzo Preda</strong></p>
<p>Introduction: Generative Adversarial Networks (GANs) are increasingly used to generate synthetic medical images, addressing the critical shortage of annotated data for training Artificial Intelligence (AI) systems. This study introduces a novel memory-efficient GAN architecture, incorporating Conditional Random Fields (CRFs) to generate high-resolution 3D medical images and evaluates its performance against the state-of-the-art hierarchical (HA)-GAN model.   Materials and Methods: The CRF-GAN was trained using the open-source lung CT LUNA16 dataset. The architecture was compared to HA-GAN through a quantitative evaluation, using Frechet Inception Distance (FID) and Maximum Mean Discrepancy (MMD) metrics, and a qualitative evaluation, through a two-alternative forced choice (2AFC) test completed by a pool of 12 resident radiologists, in order to assess the realism of the generated images.   Results: CRF-GAN outperformed HA-GAN with lower FID (0.047 vs. 0.061) and MMD (0.084 vs. 0.086) scores, indicating better image fidelity. The 2AFC test showed a significant preference for images generated by CRF-Gan over those generated by HA-GAN with a p-value of 1.93e-05. Additionally, CRF-GAN demonstrated 9.34% lower memory usage at 256 resolution and achieved up to 14.6% faster training speeds, offering substantial computational savings.   Discussion: CRF-GAN model successfully generates high-resolution 3D medical images with non-inferior quality to conventional models, while being more memory-efficient and faster. Computational power and time saved can be used to improve the spatial resolution and anatomical accuracy of generated images, which is still a critical factor limiting their direct clinical applicability. </p>
<blockquote>
<p>å¼•è¨€ï¼šç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰è¶Šæ¥è¶Šå¤šåœ°è¢«ç”¨äºç”ŸæˆåˆæˆåŒ»å­¦å›¾åƒï¼Œä»¥è§£å†³è®­ç»ƒäººå·¥æ™ºèƒ½ï¼ˆAIï¼‰ç³»ç»Ÿæ—¶æ³¨é‡Šæ•°æ®çš„ä¸¥é‡çŸ­ç¼ºé—®é¢˜ã€‚æœ¬ç ”ç©¶ä»‹ç»äº†ä¸€ç§æ–°çš„å†…å­˜é«˜æ•ˆçš„GANæ¶æ„ï¼Œå®ƒç»“åˆäº†æ¡ä»¶éšæœºåœºï¼ˆCRFsï¼‰æ¥ç”Ÿæˆé«˜åˆ†è¾¨ç‡çš„3DåŒ»å­¦å›¾åƒï¼Œå¹¶ä¸æœ€å…ˆè¿›çš„åˆ†å±‚ï¼ˆHAï¼‰-GANæ¨¡å‹è¯„ä¼°å…¶æ€§èƒ½ã€‚ææ–™ä¸æ–¹æ³•ï¼šCRF-GANä½¿ç”¨å¼€æºçš„è‚ºéƒ¨CT LUNA16æ•°æ®é›†è¿›è¡Œè®­ç»ƒã€‚é€šè¿‡ä¸HA-GANè¿›è¡Œå®šé‡è¯„ä¼°ï¼Œä½¿ç”¨Frechet Inception Distanceï¼ˆFIDï¼‰å’ŒMaximum Mean Discrepancyï¼ˆMMDï¼‰æŒ‡æ ‡ï¼Œä»¥åŠå®šæ€§è¯„ä¼°ï¼Œé€šè¿‡ç”±12åä½é™¢åŒ»å¸ˆå®Œæˆçš„ä¸¤ç§æ›¿ä»£å¼ºåˆ¶é€‰æ‹©ï¼ˆ2AFCï¼‰æµ‹è¯•ï¼Œä»¥è¯„ä¼°ç”Ÿæˆå›¾åƒçš„çœŸå®æ€§ã€‚ç»“æœï¼šCRF-GANåœ¨FIDï¼ˆ0.047 vs 0.061ï¼‰å’ŒMMDï¼ˆ0.084 vs 0.086ï¼‰å¾—åˆ†ä¸Šä¼˜äºHA-GANï¼Œè¡¨æ˜å›¾åƒä¿çœŸåº¦æ›´é«˜ã€‚2AFCæµ‹è¯•æ˜¾ç¤ºï¼ŒCRF-Ganç”Ÿæˆçš„å›¾åƒæ¯”HA-GANç”Ÿæˆçš„å›¾åƒæ›´å—æ¬¢è¿ï¼Œpå€¼ä¸º1.93e-05ã€‚æ­¤å¤–ï¼ŒCRF-GANåœ¨256åˆ†è¾¨ç‡ä¸‹å†…å­˜ä½¿ç”¨ç‡ä½9.34%ï¼Œè®­ç»ƒé€Ÿåº¦æé«˜äº†é«˜è¾¾14.6%ï¼Œä»è€Œå®ç°äº†å¤§é‡çš„è®¡ç®—èŠ‚çœã€‚è®¨è®ºï¼šCRF-GANæ¨¡å‹èƒ½å¤ŸæˆåŠŸç”Ÿæˆé«˜åˆ†è¾¨ç‡çš„3DåŒ»å­¦å›¾åƒï¼Œå…¶è´¨é‡ä¸ä¼ ç»Ÿæ¨¡å‹ç›¸å½“æˆ–æ›´å¥½ï¼ŒåŒæ—¶æ›´èŠ‚çœå†…å­˜ã€é€Ÿåº¦æ›´å¿«ã€‚æ‰€èŠ‚çœçš„è®¡ç®—èƒ½åŠ›å’Œæ—¶é—´å¯ç”¨äºæé«˜ç”Ÿæˆå›¾åƒçš„ç©ºé—´åˆ†è¾¨ç‡å’Œè§£å‰–å‡†ç¡®æ€§ï¼Œè¿™ä»ç„¶æ˜¯é™åˆ¶å…¶ç›´æ¥ä¸´åºŠåº”ç”¨çš„å…³é”®å› ç´ ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.15572v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰çš„ä¼˜å¼‚æ€§èƒ½ï¼Œå®ƒå·²è¢«å¹¿æ³›ç”¨äºç”ŸæˆåˆæˆåŒ»å­¦å›¾åƒä»¥è§£å†³è®­ç»ƒäººå·¥æ™ºèƒ½ç³»ç»Ÿæ‰€éœ€çš„æ ‡æ³¨æ•°æ®çŸ­ç¼ºçš„é—®é¢˜ã€‚æœ¬ç ”ç©¶ä»‹ç»äº†ä¸€ç§æ–°å‹çš„ã€å†…å­˜æ•ˆç‡é«˜çš„GANæ¶æ„â€”â€”CRF-GANï¼Œå®ƒèƒ½ç”Ÿæˆé«˜åˆ†è¾¨ç‡çš„3DåŒ»å­¦å›¾åƒï¼Œå¹¶ä¸”ç›¸è¾ƒäºç°æœ‰çš„åˆ†å±‚ï¼ˆHAï¼‰-GANæ¨¡å‹è¡¨ç°å‡ºäº†æ›´å¥½çš„æ€§èƒ½ã€‚CRF-GANé€šè¿‡å®šé‡è¯„ä¼°å’Œå®šæ€§è¯„ä¼°ä¸¤ç§æ–¹æ³•è¿›è¡Œäº†éªŒè¯ï¼Œç»“æœè¯æ˜äº†å…¶åœ¨å›¾åƒçœŸå®æ„Ÿã€å†…å­˜ä½¿ç”¨å’Œè®­ç»ƒé€Ÿåº¦ä¸Šçš„ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CRF-GANç»“åˆäº†æ¡ä»¶éšæœºåœºï¼ˆCRFsï¼‰ä»¥ç”Ÿæˆé«˜åˆ†è¾¨ç‡çš„3DåŒ»å­¦å›¾åƒï¼Œè¯¥æ¶æ„ä¸ºè§£å†³åŒ»å­¦å›¾åƒæ•°æ®çŸ­ç¼ºçš„é—®é¢˜æä¾›äº†æ–°çš„æ€è·¯ã€‚</li>
<li>CRF-GANä½¿ç”¨å…¬å¼€çš„è‚ºéƒ¨CT LUNAæ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œæ˜¾ç¤ºå‡ºå‡ºè‰²çš„å›¾åƒä¿çœŸåº¦ã€‚</li>
<li>ä¸å…ˆè¿›çš„HA-GANæ¨¡å‹ç›¸æ¯”ï¼ŒCRF-GANåœ¨Frechet Inception Distance (FID)å’ŒMaximum Mean Discrepancy (MMD)ä¸¤ä¸ªè¯„ä¼°æŒ‡æ ‡ä¸Šè¡¨ç°æ›´ä¼˜ï¼Œæ˜¾ç¤ºå‡ºæ›´é«˜çš„å›¾åƒè´¨é‡ã€‚</li>
<li>CRF-GANç”Ÿæˆçš„å›¾åƒåœ¨ç”±æ”¾å°„ç§‘åŒ»ç”Ÿè¿›è¡Œçš„çœŸå®æ„Ÿæµ‹è¯•ä¸­è·å¾—äº†æ˜¾è‘—åå¥½ã€‚</li>
<li>CRF-GANå…·æœ‰æ›´ä½çš„å†…å­˜ä½¿ç”¨ç‡å’Œæ›´å¿«çš„è®­ç»ƒé€Ÿåº¦ï¼Œå…·å¤‡æ›´å¤§çš„å®ç”¨ä»·å€¼ã€‚</li>
<li>è®¡ç®—èƒ½åŠ›çš„èŠ‚çœå¯ç”¨äºæå‡ç”Ÿæˆå›¾åƒçš„ç©ºé—´åˆ†è¾¨ç‡å’Œè§£å‰–ç²¾åº¦ï¼Œè¿™åœ¨ç›´æ¥åº”ç”¨äºä¸´åºŠæ—¶å°¤ä¸ºé‡è¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.15572">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-371fb457fe5eb1f4228eb80c629e4236.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6adcbca722fad090cdb893f40da33fc1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2b501678aa4879d1456324d975f55982.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f9d4f22e3748b971f75a5bd191452704.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ec5a7015da3765334d124244a0f66d9e.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Efficient-Self-Supervised-Grading-of-Prostate-Cancer-Pathology"><a href="#Efficient-Self-Supervised-Grading-of-Prostate-Cancer-Pathology" class="headerlink" title="Efficient Self-Supervised Grading of Prostate Cancer Pathology"></a>Efficient Self-Supervised Grading of Prostate Cancer Pathology</h2><p><strong>Authors:Riddhasree Bhattacharyya, Surochita Pal Das, Sushmita Mitra</strong></p>
<p>Prostate cancer grading using the ISUP system (International Society of Urological Pathology) for treatment decisions is highly subjective and requires considerable expertise. Despite advances in computer-aided diagnosis systems, few have handled efficient ISUP grading on Whole Slide Images (WSIs) of prostate biopsies based only on slide-level labels. Some of the general challenges include managing gigapixel WSIs, obtaining patch-level annotations, and dealing with stain variability across centers. One of the main task-specific challenges faced by deep learning in ISUP grading, is the learning of patch-level features of Gleason patterns (GPs) based only on their slide labels. In this scenario, an efficient framework for ISUP grading is developed.   The proposed TSOR is based on a novel Task-specific Self-supervised learning (SSL) model, which is fine-tuned using Ordinal Regression. Since the diversity of training samples plays a crucial role in SSL, a patch-level dataset is created to be relatively balanced w.r.t. the Gleason grades (GGs). This balanced dataset is used for pre-training, so that the model can effectively learn stain-agnostic features of the GP for better generalization. In medical image grading, it is desirable that misclassifications be as close as possible to the actual grade. From this perspective, the model is then fine-tuned for the task of ISUP grading using an ordinal regression-based approach. Experimental results on the most extensive multicenter prostate biopsies dataset (PANDA challenge), as well as the SICAP dataset, demonstrate the effectiveness of this novel framework compared to state-of-the-art methods. </p>
<blockquote>
<p>ä½¿ç”¨å›½é™…æ³Œå°¿ç—…ç†å­¦å­¦ä¼šï¼ˆISUPï¼‰ç³»ç»Ÿå¯¹å‰åˆ—è…ºç™Œè¿›è¡Œåˆ†çº§ä»¥åšå‡ºæ²»ç–—å†³ç­–æ˜¯éå¸¸ä¸»è§‚çš„ï¼Œå¹¶ä¸”éœ€è¦ç›¸å½“çš„ä¸“ä¸šçŸ¥è¯†ã€‚å°½ç®¡è®¡ç®—æœºè¾…åŠ©è¯Šæ–­ç³»ç»Ÿæœ‰æ‰€è¿›æ­¥ï¼Œä½†å¾ˆå°‘æœ‰äººèƒ½æœ‰æ•ˆåœ°å¤„ç†åŸºäºå‰åˆ—è…ºæ´»æ£€å…¨åˆ‡ç‰‡å›¾åƒï¼ˆWSIï¼‰çš„ISUPåˆ†çº§ï¼Œä»…ä»…åŸºäºåˆ‡ç‰‡çº§åˆ«çš„æ ‡ç­¾ã€‚ä¸€äº›é€šç”¨æŒ‘æˆ˜åŒ…æ‹¬å¤„ç†åƒå…†åƒç´ çš„WSIã€è·å–è¡¥ä¸çº§åˆ«çš„æ³¨é‡Šä»¥åŠå¤„ç†è·¨ä¸­å¿ƒçš„æŸ“è‰²å˜é‡ã€‚æ·±åº¦å­¦ä¹ åœ¨ISUPåˆ†çº§ä¸­é¢ä¸´çš„ä¸»è¦ä»»åŠ¡ç‰¹å®šæŒ‘æˆ˜ä¹‹ä¸€æ˜¯ï¼Œä»…åŸºäºå…¶åˆ‡ç‰‡æ ‡ç­¾å­¦ä¹ Gleasonæ¨¡å¼ï¼ˆGPï¼‰çš„è¡¥ä¸çº§åˆ«ç‰¹å¾ã€‚é’ˆå¯¹è¿™ç§æƒ…å†µï¼Œå¼€å‘äº†ä¸€ä¸ªæœ‰æ•ˆçš„ISUPåˆ†çº§æ¡†æ¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.15520v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºä»»åŠ¡ç‰¹å®šè‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰æ¨¡å‹çš„å…¨æ–°æ¡†æ¶ï¼Œç”¨äºå‰åˆ—è…ºç™Œåˆ†çº§ã€‚è¯¥æ¡†æ¶é€šè¿‡åºæ•°å›å½’è¿›è¡Œå¾®è°ƒï¼Œå¹¶åˆ›å»ºäº†ä¸€ä¸ªç›¸å¯¹å¹³è¡¡çš„è¡¥ä¸çº§åˆ«æ•°æ®é›†ï¼Œä»¥æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚åœ¨åŒ»å­¦å›¾åƒåˆ†çº§ä¸­ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿå°†è¯¯åˆ†ç±»å°½å¯èƒ½æ¥è¿‘å®é™…ç­‰çº§ã€‚åœ¨å¤šä¸­å¿ƒå‰åˆ—è…ºæ´»æ£€æ•°æ®é›†ï¼ˆPANDAæŒ‘æˆ˜æ•°æ®é›†ï¼‰ä»¥åŠSICAPæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶ç›¸è¾ƒäºç°æœ‰å…ˆè¿›æŠ€æœ¯æ›´ä¸ºæœ‰æ•ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å‰åˆ—è…ºç™Œåˆ†çº§é‡‡ç”¨å›½é™…æ³Œå°¿ç—…ç†å­¦ä¼šï¼ˆISUPï¼‰ç³»ç»Ÿå¯¹äºæ²»ç–—å†³ç­–è‡³å…³é‡è¦ï¼Œä½†å­˜åœ¨ä¸»è§‚æ€§å’ŒæŒ‘æˆ˜ã€‚</li>
<li>è®¡ç®—æœºè¾…åŠ©è¯Šæ–­ç³»ç»Ÿåœ¨å¤„ç†å…¨å¹»ç¯ç‰‡å›¾åƒï¼ˆWSIsï¼‰çš„ISUPåˆ†çº§æ–¹é¢æœ‰æ‰€è¿›å±•ï¼Œä½†ä»é¢ä¸´å¤„ç†gigapixel WSIsã€è·å–è¡¥ä¸çº§åˆ«æ³¨é‡Šå’Œåº”å¯¹ä¸åŒä¸­å¿ƒæŸ“è‰²å˜å¼‚çš„æŒ‘æˆ˜ã€‚</li>
<li>æ·±åº¦å­¦ä¹ åœ¨ISUPåˆ†çº§ä¸­çš„ä¸»è¦æŒ‘æˆ˜æ˜¯å­¦ä¹ åŸºäºå¹»ç¯ç‰‡æ ‡ç­¾çš„è¡¥ä¸çº§åˆ«çš„æ ¼è±æ£®æ¨¡å¼ï¼ˆGPï¼‰çš„ç‰¹å¾ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºä»»åŠ¡ç‰¹å®šè‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰æ¨¡å‹çš„æ¡†æ¶ï¼Œé€šè¿‡åºæ•°å›å½’å¾®è°ƒè¿›è¡Œå‰åˆ—è…ºç™Œçš„ISUPåˆ†çº§ã€‚</li>
<li>åˆ›å»ºäº†ä¸€ä¸ªç›¸å¯¹å¹³è¡¡çš„è¡¥ä¸çº§åˆ«æ•°æ®é›†ï¼Œä»¥æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶å…³æ³¨æŸ“è‰²æ— å…³çš„ç‰¹å¾å­¦ä¹ ã€‚</li>
<li>æ¨¡å‹åœ¨å¤šä¸­å¿ƒå‰åˆ—è…ºæ´»æ£€æ•°æ®é›†ï¼ˆå¦‚PANDAæŒ‘æˆ˜æ•°æ®é›†ï¼‰å’ŒSICAPæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜å…¶æœ‰æ•ˆæ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.15520">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a1e03c80dc09dbfd9c5d91834f4534ae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e8405d7b512466ff1bc61b2a9bf663b7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-38003ddacd616d9ab57a389e1613830c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c3e14cced9598e03303c4652b89c4010.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-06a98fd3c3019ac51cfefb969d694a31.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Identifying-Critical-Tokens-for-Accurate-Predictions-in-Transformer-based-Medical-Imaging-Models"><a href="#Identifying-Critical-Tokens-for-Accurate-Predictions-in-Transformer-based-Medical-Imaging-Models" class="headerlink" title="Identifying Critical Tokens for Accurate Predictions in   Transformer-based Medical Imaging Models"></a>Identifying Critical Tokens for Accurate Predictions in   Transformer-based Medical Imaging Models</h2><p><strong>Authors:Solha Kang, Joris Vankerschaver, Utku Ozbulak</strong></p>
<p>With the advancements in self-supervised learning (SSL), transformer-based computer vision models have recently demonstrated superior results compared to convolutional neural networks (CNNs) and are poised to dominate the field of artificial intelligence (AI)-based medical imaging in the upcoming years. Nevertheless, similar to CNNs, unveiling the decision-making process of transformer-based models remains a challenge. In this work, we take a step towards demystifying the decision-making process of transformer-based medical imaging models and propose Token Insight, a novel method that identifies the critical tokens that contribute to the prediction made by the model. Our method relies on the principled approach of token discarding native to transformer-based models, requires no additional module, and can be applied to any transformer model. Using the proposed approach, we quantify the importance of each token based on its contribution to the prediction and enable a more nuanced understanding of the modelâ€™s decisions. Our experimental results which are showcased on the problem of colonic polyp identification using both supervised and self-supervised pretrained vision transformers indicate that Token Insight contributes to a more transparent and interpretable transformer-based medical imaging model, fostering trust and facilitating broader adoption in clinical settings. </p>
<blockquote>
<p>éšç€è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰çš„è¿›æ­¥ï¼ŒåŸºäºè½¬æ¢å™¨çš„è®¡ç®—æœºè§†è§‰æ¨¡å‹æœ€è¿‘å±•ç°å‡ºæ¯”å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰æ›´ä¼˜è¶Šçš„ç»“æœï¼Œå¹¶æœ‰æœ›åœ¨æ¥ä¸‹æ¥çš„å‡ å¹´å†…åœ¨åŸºäºäººå·¥æ™ºèƒ½ï¼ˆAIï¼‰çš„åŒ»ç–—æˆåƒé¢†åŸŸå æ®ä¸»å¯¼åœ°ä½ã€‚ç„¶è€Œï¼Œä¸CNNç±»ä¼¼ï¼Œæ­ç¤ºåŸºäºè½¬æ¢å™¨æ¨¡å‹çš„å†³ç­–è¿‡ç¨‹ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æœç€æ­ç¤ºåŸºäºè½¬æ¢å™¨åŒ»ç–—æˆåƒæ¨¡å‹çš„å†³ç­–è¿‡ç¨‹è¿ˆå‡ºäº†ä¸€æ­¥ï¼Œå¹¶æå‡ºäº†ä¸€ç§åä¸ºâ€œToken Insightâ€çš„æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯ä»¥è¯†åˆ«å¯¹æ¨¡å‹é¢„æµ‹åšå‡ºè´¡çŒ®çš„å…³é”®ä»¤ç‰Œã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¾èµ–äºåŸºäºè½¬æ¢å™¨æ¨¡å‹çš„å›ºæœ‰ä»¤ç‰Œä¸¢å¼ƒåŸåˆ™æ€§æ–¹æ³•ï¼Œæ— éœ€é¢å¤–çš„æ¨¡å—ï¼Œå¹¶ä¸”å¯ä»¥åº”ç”¨äºä»»ä½•è½¬æ¢å™¨æ¨¡å‹ã€‚é€šè¿‡ä½¿ç”¨æ‰€æå‡ºçš„æ–¹æ³•ï¼Œæˆ‘ä»¬æ ¹æ®æ¯ä¸ªä»¤ç‰Œå¯¹é¢„æµ‹çš„è´¡çŒ®æ¥é‡åŒ–å…¶é‡è¦æ€§ï¼Œå¹¶å®ç°å¯¹æ¨¡å‹å†³ç­–çš„æ›´å¾®å¦™ç†è§£ã€‚æˆ‘ä»¬åœ¨ç»“è‚ æ¯è‚‰è¯†åˆ«é—®é¢˜ä¸Šå±•ç¤ºçš„å®éªŒç»“æœï¼Œä½¿ç”¨ç›‘ç£å­¦ä¹ å’Œè‡ªç›‘ç£é¢„è®­ç»ƒçš„è§†è§‰è½¬æ¢å™¨éƒ½è¡¨æ˜ï¼ŒToken Insightæœ‰åŠ©äºåˆ›å»ºæ›´é€æ˜ã€å¯è§£é‡Šçš„åŸºäºè½¬æ¢å™¨çš„åŒ»ç–—æˆåƒæ¨¡å‹ï¼Œå¢å¼ºä¿¡ä»»ï¼Œä¿ƒè¿›åœ¨ä¸´åºŠç¯å¢ƒä¸­çš„æ›´å¹¿æ³›åº”ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.15452v1">PDF</a> Accepted for publication in MICCAI 2024 Workshop on Machine Learning   in Medical Imaging (MLMI)</p>
<p><strong>Summary</strong><br>     éšç€è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰çš„å‘å±•ï¼ŒåŸºäºè½¬æ¢å™¨çš„è®¡ç®—æœºè§†è§‰æ¨¡å‹åœ¨åŒ»ç–—å›¾åƒé¢†åŸŸå±•ç°å‡ºä¼˜äºå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰çš„ç»“æœï¼Œå¹¶æœ‰æœ›åœ¨æ¥ä¸‹æ¥çš„å‡ å¹´å†…åœ¨äººå·¥æ™ºèƒ½åŒ»ç–—æˆåƒé¢†åŸŸå æ®ä¸»å¯¼åœ°ä½ã€‚ç„¶è€Œï¼Œæ­å¼€åŸºäºè½¬æ¢å™¨æ¨¡å‹çš„å†³ç­–è¿‡ç¨‹ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶æœç€è§£å¯†åŸºäºè½¬æ¢å™¨çš„åŒ»ç–—æˆåƒæ¨¡å‹çš„å†³ç­–è¿‡ç¨‹è¿ˆå‡ºäº†ä¸€æ­¥ï¼Œæå‡ºäº†ä¸€ç§åä¸ºToken Insightçš„æ–°æ–¹æ³•ï¼Œå¯è¯†åˆ«å¯¹æ¨¡å‹é¢„æµ‹è‡³å…³é‡è¦çš„å…³é”®ä»¤ç‰Œã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¾èµ–äºè½¬æ¢å™¨æ¨¡å‹ç‰¹æœ‰çš„ä»¤ç‰Œä¸¢å¼ƒåŸåˆ™ï¼Œæ— éœ€é¢å¤–çš„æ¨¡å—ï¼Œå¯åº”ç”¨äºä»»ä½•è½¬æ¢å™¨æ¨¡å‹ã€‚é€šè¿‡æ‰€æå‡ºçš„æ–¹æ³•ï¼Œæˆ‘ä»¬æ ¹æ®æ¯ä¸ªä»¤ç‰Œå¯¹é¢„æµ‹çš„è´¡çŒ®æ¥é‡åŒ–å…¶é‡è¦æ€§ï¼Œä½¿æˆ‘ä»¬å¯¹æ¨¡å‹çš„å†³ç­–æœ‰æ›´ç»†è‡´çš„ç†è§£ã€‚æˆ‘ä»¬åœ¨ç»“è‚ æ¯è‚‰è¯†åˆ«é—®é¢˜ä¸Šå±•ç¤ºäº†ä½¿ç”¨ç›‘ç£å­¦ä¹ å’Œè‡ªç›‘ç£é¢„è®­ç»ƒè§†è§‰è½¬æ¢å™¨çš„æ–¹æ³•ï¼Œç»“æœè¡¨æ˜Token Insightæœ‰åŠ©äºå»ºç«‹æ›´é€æ˜ã€å¯è§£é‡Šçš„åŸºäºè½¬æ¢å™¨çš„åŒ»ç–—æˆåƒæ¨¡å‹ï¼Œå¢å¼ºä¿¡ä»»ï¼Œä¾¿äºåœ¨ä¸´åºŠç¯å¢ƒä¸­æ›´å¹¿æ³›çš„åº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªç›‘ç£å­¦ä¹ åœ¨åŒ»ç–—å›¾åƒé¢†åŸŸæ¨åŠ¨äº†åŸºäºè½¬æ¢å™¨æ¨¡å‹çš„è¿›æ­¥ï¼Œä½¿å…¶æœ‰æœ›åœ¨æœªæ¥å‡ å¹´å†…ä¸»å¯¼äººå·¥æ™ºèƒ½åŒ»ç–—æˆåƒé¢†åŸŸã€‚</li>
<li>åŸºäºè½¬æ¢å™¨çš„æ¨¡å‹å†³ç­–è¿‡ç¨‹ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œéœ€è¦æ›´æ·±å…¥çš„ç†è§£å’Œè§£é‡Šã€‚</li>
<li>æå‡ºäº†ä¸€ç§åä¸ºToken Insightçš„æ–°æ–¹æ³•ï¼Œç”¨äºè¯†åˆ«å¯¹æ¨¡å‹é¢„æµ‹è‡³å…³é‡è¦çš„å…³é”®ä»¤ç‰Œã€‚</li>
<li>Token Insightæ–¹æ³•åŸºäºè½¬æ¢å™¨æ¨¡å‹çš„ä»¤ç‰Œä¸¢å¼ƒåŸåˆ™ï¼Œæ— éœ€é¢å¤–çš„æ¨¡å—ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ€§ã€‚</li>
<li>Token Insightèƒ½å¤Ÿé‡åŒ–æ¯ä¸ªä»¤ç‰Œå¯¹é¢„æµ‹çš„è´¡çŒ®ï¼Œæä¾›æ›´æ·±å…¥çš„æ¨¡å‹å†³ç­–ç†è§£ã€‚</li>
<li>åœ¨ç»“è‚ æ¯è‚‰è¯†åˆ«é—®é¢˜ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒToken Insightæœ‰åŠ©äºæé«˜æ¨¡å‹çš„é€æ˜åº¦å’Œå¯è§£é‡Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.15452">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0d699fb0e76e6be16c50c18993ea566d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cb9c4dd5f2f2a81cfc57ad1afbe80c15.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ee77dd7964417233a4a5362a8846566c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-831592b6ee4f7ab60329b7717a86574b.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Gland-Segmentation-Using-SAM-With-Cancer-Grade-as-a-Prompt"><a href="#Gland-Segmentation-Using-SAM-With-Cancer-Grade-as-a-Prompt" class="headerlink" title="Gland Segmentation Using SAM With Cancer Grade as a Prompt"></a>Gland Segmentation Using SAM With Cancer Grade as a Prompt</h2><p><strong>Authors:Yijie Zhu, Shan E Ahmed Raza</strong></p>
<p>Cancer grade is a critical clinical criterion that can be used to determine the degree of cancer malignancy. Revealing the condition of the glands, a precise gland segmentation can assist in a more effective cancer grade classification. In machine learning, binary classification information about glands (i.e., benign and malignant) can be utilized as a prompt for gland segmentation and cancer grade classification. By incorporating prior knowledge of the benign or malignant classification of the gland, the model can anticipate the likely appearance of the target, leading to better segmentation performance. We utilize Segment Anything Model to solve the segmentation task, by taking advantage of its prompt function and applying appropriate modifications to the model structure and training strategies. We improve the results from fine-tuned Segment Anything Model and produce SOTA results using this approach. </p>
<blockquote>
<p>ç™Œç—‡åˆ†çº§æ˜¯ä¸€ä¸ªé‡è¦çš„ä¸´åºŠæ ‡å‡†ï¼Œå¯ä»¥ç”¨æ¥ç¡®å®šç™Œç—‡çš„æ¶æ€§ç¨‹åº¦ã€‚ç²¾ç¡®çš„è…ºä½“åˆ†å‰²å¯ä»¥æ­ç¤ºè…ºä½“çš„çŠ¶å†µï¼Œä»è€Œæœ‰åŠ©äºæ›´æœ‰æ•ˆåœ°è¿›è¡Œç™Œç—‡åˆ†çº§åˆ†ç±»ã€‚åœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œå…³äºè…ºä½“ï¼ˆå³è‰¯æ€§å’Œæ¶æ€§ï¼‰çš„äºŒå…ƒåˆ†ç±»ä¿¡æ¯å¯ä»¥ä½œä¸ºè…ºä½“åˆ†å‰²å’Œç™Œç—‡åˆ†çº§åˆ†ç±»çš„æç¤ºã€‚é€šè¿‡ç»“åˆè…ºä½“è‰¯æ¶æ€§åˆ†ç±»çš„å…ˆéªŒçŸ¥è¯†ï¼Œæ¨¡å‹å¯ä»¥é¢„æµ‹ç›®æ ‡çš„å¯èƒ½å¤–è§‚ï¼Œä»è€Œæé«˜åˆ†å‰²æ€§èƒ½ã€‚æˆ‘ä»¬åˆ©ç”¨Segment Anything Modelè§£å†³åˆ†å‰²ä»»åŠ¡ï¼Œé€šè¿‡å‘æŒ¥å…¶æç¤ºåŠŸèƒ½å¹¶å¯¹æ¨¡å‹ç»“æ„å’Œè®­ç»ƒç­–ç•¥è¿›è¡Œé€‚å½“çš„ä¿®æ”¹ã€‚æˆ‘ä»¬æ”¹è¿›äº†å¾®è°ƒåçš„Segment Anything Modelçš„ç»“æœï¼Œå¹¶ä½¿ç”¨è¿™ç§æ–¹æ³•å–å¾—äº†æœ€å…ˆè¿›çš„æˆæœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.14718v2">PDF</a> Accepted by ISBI 2025</p>
<p><strong>Summary</strong><br>     ç™Œç—‡åˆ†çº§æ˜¯åˆ¤æ–­ç™Œç—‡æ¶æ€§ç¨‹åº¦çš„å…³é”®ä¸´åºŠæ ‡å‡†ã€‚ç²¾ç¡®çš„è…ºä½“åˆ†å‰²æœ‰åŠ©äºæ›´ç²¾ç¡®åœ°åˆ’åˆ†ç™Œç—‡ç­‰çº§ã€‚åœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œå¯ä»¥åˆ©ç”¨è…ºä½“ï¼ˆè‰¯æ€§æˆ–æ¶æ€§ï¼‰çš„äºŒå…ƒåˆ†ç±»ä¿¡æ¯ï¼Œé€šè¿‡æ¨¡å‹é¢„æµ‹ç›®æ ‡çš„å¯èƒ½å¤–è§‚ï¼Œæé«˜åˆ†å‰²æ€§èƒ½ã€‚æˆ‘ä»¬åˆ©ç”¨Segment Anything Modelè§£å†³åˆ†å‰²ä»»åŠ¡ï¼Œé€šè¿‡è°ƒæ•´æ¨¡å‹ç»“æ„å’Œè®­ç»ƒç­–ç•¥ï¼Œæ”¹è¿›äº†fine-tuned Segment Anything Modelçš„ç»“æœï¼Œå–å¾—ç›®å‰æœ€å…ˆè¿›çš„ç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç™Œç—‡åˆ†çº§æ˜¯è¯„ä¼°ç™Œç—‡æ¶æ€§ç¨‹åº¦çš„é‡è¦ä¸´åºŠæ ‡å‡†ã€‚</li>
<li>ç²¾ç¡®çš„è…ºä½“åˆ†å‰²æœ‰åŠ©äºæ›´å‡†ç¡®åœ°è¯„ä¼°ç™Œç—‡ç­‰çº§ã€‚</li>
<li>åœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œå¯ä»¥åˆ©ç”¨è…ºä½“çš„äºŒå…ƒåˆ†ç±»ä¿¡æ¯æ¥è¿›è¡Œè…ºä½“åˆ†å‰²å’Œç™Œç—‡ç­‰çº§åˆ†ç±»ã€‚</li>
<li>é€šè¿‡ç»“åˆè…ºä½“çš„è‰¯æ€§æˆ–æ¶æ€§åˆ†ç±»çš„å…ˆéªŒçŸ¥è¯†ï¼Œæ¨¡å‹å¯ä»¥æ›´å¥½åœ°é¢„æµ‹ç›®æ ‡çš„å¯èƒ½å¤–è§‚ï¼Œä»è€Œæé«˜åˆ†å‰²æ€§èƒ½ã€‚</li>
<li>Segment Anything Modelè¢«ç”¨äºè§£å†³åˆ†å‰²ä»»åŠ¡ã€‚</li>
<li>æˆ‘ä»¬é€šè¿‡è°ƒæ•´æ¨¡å‹ç»“æ„å’Œè®­ç»ƒç­–ç•¥ï¼Œæ”¹è¿›äº†fine-tuned Segment Anything Modelçš„ç»“æœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.14718">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5111f9723063afb6d9399380832a8119.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4f8fe84793976935f9568dbf9989bde1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-13d096aab6e33c31914805e8591610e8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4b7af297c1f31fa84e9966d029647a94.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3d0bb0f8fadda2ed984e22d83d2de510.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="A-Detailed-Spectral-Study-of-Intermittent-Accreting-Millisecond-X-ray-Pulsar-Aql-X-1-during-Pulse-on-and-Pulse-off-Stages"><a href="#A-Detailed-Spectral-Study-of-Intermittent-Accreting-Millisecond-X-ray-Pulsar-Aql-X-1-during-Pulse-on-and-Pulse-off-Stages" class="headerlink" title="A Detailed Spectral Study of Intermittent-Accreting Millisecond X-ray   Pulsar Aql X-1 during Pulse-on and Pulse-off Stages"></a>A Detailed Spectral Study of Intermittent-Accreting Millisecond X-ray   Pulsar Aql X-1 during Pulse-on and Pulse-off Stages</h2><p><strong>Authors:T. KocabÄ±yÄ±k, C. GÃ¼ngÃ¶r, M. T. SaÄŸlam, T. GÃ¼ver, Z. F. BostancÄ±</strong></p>
<p>We present a detailed spectral study of an intermittent-AMXP Aql X-1 during the pulse-on and pulse-off stages by using the archival RXTE data. We first perform temporal analysis by using Z$_n^2$ technique in three different energy bands, 3.0 â€“ 13.0 keV, 13.0 â€“ 23.0 keV and 23.0 â€“ 33.0 keV, for the last 128 s time segment of the RXTE data including pulse-on region. We show that the pulse is the most significant in the softest band. We, then, show that the spectrum is represented the best via combination of absorbed blackbody, disk blackbody and a gaussian line. We modeled the last four segments of the data 30188-03-05-00 to better compare pulse-on and pulse-off stages. We found a vague residual in the spectral fit of the pulse-on segment between $\sim$3.0 â€“ 13.0 keV which agrees with the result of temporal analysis. We show that the residual may be represented with an extra blackbody component with the temperature of 1.75 keV and the radius of 0.75$\pm$0.49 km. For deeper analysis, we performed phase-resolved spectroscopy to the last 128 s, pulse-on, segment. We obtain two separate spectra for the spin phase range of 0.75 â€“ 0.25, pulse-high and 0.25 â€“ 0.75, pulse-low and followed the same procedure. We display that the residual becomes more clear for pulse-high compared to the pulse-low. We report that the additional blackbody component, which models the residual, indicates a hotspot from the surface of the neutron star with the radius of 1.65$\pm$0.74 km whose temperature is 1.65 keV. </p>
<blockquote>
<p>æˆ‘ä»¬å¯¹é—´æ­‡å‹AMXPæ°´å§”ä¸€ï¼ˆAql X-1ï¼‰åœ¨è„‰å†²å¼€å¯å’Œè„‰å†²å…³é—­é˜¶æ®µè¿›è¡Œäº†è¯¦ç»†çš„å…‰è°±ç ”ç©¶ï¼Œä½¿ç”¨çš„æ˜¯æ¡£æ¡ˆä¸­çš„RXTEæ•°æ®ã€‚æˆ‘ä»¬é¦–å…ˆä½¿ç”¨$Z_n^2$æŠ€æœ¯ï¼Œåœ¨ä¸‰ä¸ªä¸åŒçš„èƒ½æ®µï¼ˆ3.0-13.0 keVã€13.0-23.0 keVå’Œ23.0-33.0 keVï¼‰å¯¹åŒ…æ‹¬è„‰å†²å¼€å¯åŒºåŸŸåœ¨å†…çš„RXTEæ•°æ®æœ€å128ç§’çš„æ—¶é—´æ®µè¿›è¡Œæ—¶é—´åˆ†æã€‚æˆ‘ä»¬æ˜¾ç¤ºè„‰å†²åœ¨æœ€è½¯çš„æ³¢æ®µä¸­æœ€ä¸ºæ˜¾è‘—ã€‚æ¥ç€ï¼Œæˆ‘ä»¬å±•ç¤ºå…‰è°±æœ€å¥½ç”¨å¸æ”¶çš„é»‘ä½“ã€ç›˜é»‘ä½“å’Œä¸€æ¡é«˜æ–¯çº¿ç»„åˆè¡¨ç¤ºã€‚æˆ‘ä»¬å¯¹æ•°æ®æœ€åå››ä¸ªç‰‡æ®µï¼ˆç¼–å·30188-03-05-00ï¼‰è¿›è¡Œå»ºæ¨¡ï¼Œä»¥ä¾¿æ›´å¥½åœ°æ¯”è¾ƒè„‰å†²å¼€å¯å’Œè„‰å†²å…³é—­é˜¶æ®µã€‚æˆ‘ä»¬åœ¨è„‰å†²å¼€å¯æ®µçº¦3.0-13.0 keVçš„å…‰è°±æ‹Ÿåˆä¸­å‘ç°äº†æ¨¡ç³Šçš„æ®‹å·®ï¼Œè¿™ä¸æ—¶é—´åˆ†æç»“æœä¸€è‡´ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œæ®‹å·®å¯ä»¥ç”¨é¢å¤–çš„é»‘ä½“æˆåˆ†æ¥è¡¨ç¤ºï¼Œå…¶æ¸©åº¦ä¸º1.75 keVï¼ŒåŠå¾„ä¸º0.75Â±0.49å…¬é‡Œã€‚ä¸ºäº†è¿›è¡Œæ›´æ·±å…¥çš„åˆ†æï¼Œæˆ‘ä»¬å¯¹æœ€å128ç§’çš„è„‰å†²å¼€å¯æ®µæ‰§è¡Œäº†ç›¸ä½è§£æå…‰è°±ã€‚æˆ‘ä»¬ä¸ºè„‰å†²ç›¸ä½èŒƒå›´åˆ†åˆ«ä¸º0.75-0.25ï¼ˆè„‰å†²é«˜ï¼‰å’Œ0.25-0.75ï¼ˆè„‰å†²ä½ï¼‰çš„ä¸¤ä¸ªé˜¶æ®µè·å¾—äº†ä¸¤ä¸ªç‹¬ç«‹å…‰è°±ï¼Œå¹¶éµå¾ªäº†ç›¸åŒçš„ç¨‹åºã€‚æˆ‘ä»¬æ˜¾ç¤ºä¸è„‰å†²ä½ç›¸æ¯”ï¼Œè„‰å†²é«˜çš„æ®‹å·®å˜å¾—æ›´åŠ æ¸…æ™°ã€‚æˆ‘ä»¬æŠ¥å‘Šè¯´ï¼Œé¢å¤–çš„é»‘ä½“æˆåˆ†æ¨¡æ‹Ÿäº†æ®‹å·®ï¼Œå¹¶æŒ‡ç¤ºæ¥è‡ªä¸­å­æ˜Ÿè¡¨é¢çš„çƒ­ç‚¹ï¼Œå…¶åŠå¾„ä¸º1.65Â±0.74å…¬é‡Œï¼Œæ¸©åº¦ä¸º1.65 keVã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.04542v2">PDF</a> Published in MNRAS. 7 Pages, 4 Figures</p>
<p><strong>Summary</strong><br>     åˆ©ç”¨RXTEæ•°æ®å¯¹é—´æ­‡æ€§AMXP Aql X-1çš„è„‰å†²å¼€å¯å’Œè„‰å†²å…³é—­é˜¶æ®µè¿›è¡Œäº†è¯¦ç»†çš„è°±ç ”ç©¶ã€‚é€šè¿‡å¯¹ä¸åŒèƒ½é‡æ®µçš„æ—¶åºåˆ†æï¼Œå‘ç°è„‰å†²åœ¨è¾ƒè½¯æ³¢æ®µæœ€ä¸ºæ˜¾è‘—ã€‚è°±æ‹Ÿåˆé‡‡ç”¨å¸æ”¶é»‘ä½“ã€ç›˜é»‘ä½“å’Œé«˜æ–¯çº¿ç»„åˆè¡¨ç¤ºæœ€ä½³ã€‚å¯¹æ¯”è„‰å†²å¼€å¯å’Œè„‰å†²å…³é—­é˜¶æ®µï¼Œå‘ç°åœ¨ç‰¹å®šèƒ½é‡æ®µçš„è°±æ‹Ÿåˆå­˜åœ¨æ¨¡ç³Šæ®‹ç•™ï¼Œå¯é€šè¿‡é¢å¤–çš„é»‘ä½“ç»„ä»¶è¿›è¡Œå»ºæ¨¡ï¼Œè¡¨ç¤ºæ¥è‡ªä¸­å­æ˜Ÿè¡¨é¢çš„çƒ­ç‚¹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä½¿ç”¨RXTEæ•°æ®å¯¹Aql X-1çš„è„‰å†²å¼€å¯å’Œè„‰å†²å…³é—­é˜¶æ®µè¿›è¡Œäº†è¯¦ç»†è°±ç ”ç©¶ã€‚</li>
<li>åœ¨ä¸åŒèƒ½é‡æ®µè¿›è¡Œæ—¶åºåˆ†æï¼Œå‘ç°æœ€æ˜¾è‘—çš„è„‰å†²ä½äºè¾ƒè½¯æ³¢æ®µã€‚</li>
<li>è°±å¯æœ€ä½³è¡¨ç¤ºä¸ºå¸æ”¶é»‘ä½“ã€ç›˜é»‘ä½“å’Œé«˜æ–¯çº¿ç»„åˆã€‚</li>
<li>å¯¹æ¯”è„‰å†²å¼€å¯å’Œè„‰å†²å…³é—­é˜¶æ®µï¼Œå‘ç°ç‰¹å®šèƒ½é‡æ®µçš„è°±æ‹Ÿåˆå­˜åœ¨æ¨¡ç³Šæ®‹ç•™ã€‚</li>
<li>æ¨¡ç³Šæ®‹ç•™å¯é€šè¿‡é¢å¤–çš„é»‘ä½“ç»„ä»¶è¿›è¡Œå»ºæ¨¡ï¼Œè¡¨ç¤ºä¸­å­æ˜Ÿè¡¨é¢çš„çƒ­ç‚¹ã€‚</li>
<li>é¢å¤–é»‘ä½“ç»„ä»¶çš„åŠå¾„çº¦ä¸º1.65Â±0.74å…¬é‡Œï¼Œæ¸©åº¦ä¸º1.65åƒç”µå­ä¼ç‰¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.04542">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c38cafa0b4009de0ea4b6d508537384c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-276bddc43f24d1c9452324db39761e91.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3d5e3a890c1c2defa99994d48518baa2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-687bd486c9a1355889341cb0190b20a0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e690e67530710fe380228d2e5ea7024d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-95efcf60e0c5c1e73fbd71618e4d29ea.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Accessing-the-topological-properties-of-human-brain-functional-sub-circuits-in-Echo-State-Networks"><a href="#Accessing-the-topological-properties-of-human-brain-functional-sub-circuits-in-Echo-State-Networks" class="headerlink" title="Accessing the topological properties of human brain functional   sub-circuits in Echo State Networks"></a>Accessing the topological properties of human brain functional   sub-circuits in Echo State Networks</h2><p><strong>Authors:Bach Nguyen, Tianlong Chen, Shu Yang, Bojian Hou, Li Shen, Duy Duong-Tran</strong></p>
<p>Recent years have witnessed an emerging trend in neuromorphic computing that centers around the use of brain connectomics as a blueprint for artificial neural networks. Connectomics-based neuromorphic computing has primarily focused on embedding human brain large-scale structural connectomes (SCs), as estimated from diffusion Magnetic Resonance Imaging (dMRI) modality, to echo-state networks (ESNs). A critical step in ESN embedding requires pre-determined read-in and read-out layers constructed by the induced subgraphs of the embedded reservoir. As \textit{a priori} set of functional sub-circuits are derived from functional MRI (fMRI) modality, it is unknown, till this point, whether the embedding of fMRI-induced sub-circuits&#x2F;networks onto SCs is well justified from the neuro-physiological perspective and ESN performance across a variety of tasks. This paper proposes a pipeline to implement and evaluate ESNs with various embedded topologies and processing&#x2F;memorization tasks. To this end, we showed that different performance optimums highly depend on the neuro-physiological characteristics of these pre-determined fMRI-induced sub-circuits. In general, fMRI-induced sub-circuit-embedded ESN outperforms simple bipartite and various null models with feed-forward properties commonly seen in MLP for different tasks and reservoir criticality conditions. We provided a thorough analysis of the topological properties of pre-determined fMRI-induced sub-circuits and highlighted their graph-theoretical properties that play significant roles in determining ESN performance. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œç¥ç»å½¢æ€è®¡ç®—é¢†åŸŸå‡ºç°äº†ä¸€ç§ä»¥è„‘è¿æ¥ç»„ä½œä¸ºäººå·¥ç¥ç»ç½‘ç»œè“å›¾çš„æ–°å…´è¶‹åŠ¿ã€‚åŸºäºè¿æ¥ç»„çš„ç¥ç»å½¢æ€è®¡ç®—ä¸»è¦ä¾§é‡äºå°†åŸºäºæ‰©æ•£ç£å…±æŒ¯æˆåƒï¼ˆdMRIï¼‰æŠ€æœ¯ä¼°è®¡å¾—åˆ°çš„å¤§è§„æ¨¡äººç±»è„‘ç»“æ„è¿æ¥ç»„ï¼ˆSCsï¼‰åµŒå…¥å›å£°çŠ¶æ€ç½‘ç»œï¼ˆESNsï¼‰ã€‚åœ¨ESNåµŒå…¥è¿‡ç¨‹ä¸­ï¼Œä¸€ä¸ªå…³é”®æ­¥éª¤æ˜¯æ„å»ºç”±åµŒå…¥å­˜å‚¨åº“ä¸­çš„è¯±å¯¼å­å›¾æ„æˆçš„é¢„å®šä¹‰è¯»å…¥å’Œè¯»å‡ºå±‚ã€‚ç”±äºå…ˆéªŒçš„åŠŸèƒ½æ€§å­ç”µè·¯æ˜¯ä»åŠŸèƒ½ç£å…±æŒ¯æˆåƒï¼ˆfMRIï¼‰æŠ€æœ¯ä¸­æ´¾ç”Ÿå‡ºæ¥çš„ï¼Œåˆ°ç›®å‰ä¸ºæ­¢ï¼Œå°šä¸æ¸…æ¥šä»ç¥ç»ç”Ÿç†å­¦è§’åº¦å°†fMRIè¯±å¯¼çš„å­ç”µè·¯&#x2F;ç½‘ç»œåµŒå…¥åˆ°ç»“æ„è¿æ¥ç»„æ˜¯å¦åˆç†ï¼Œä»¥åŠåœ¨ä¸åŒä»»åŠ¡ä¸­ESNçš„æ€§èƒ½è¡¨ç°å¦‚ä½•ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§å®ç°å’Œè¯„ä¼°å…·æœ‰ä¸åŒåµŒå…¥æ‹“æ‰‘ç»“æ„å’Œå¤„ç†&#x2F;è®°å¿†ä»»åŠ¡çš„ESNsçš„ç®¡é“ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬è¡¨æ˜ä¸åŒçš„æ€§èƒ½æœ€ä¼˜å€¼åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå–å†³äºè¿™äº›é¢„å…ˆç¡®å®šçš„fMRIè¯±å¯¼çš„å­ç”µè·¯çš„ç¥ç»ç”Ÿç†å­¦ç‰¹å¾ã€‚æ€»ä½“è€Œè¨€ï¼ŒåŸºäºfMRIè¯±å¯¼çš„å­ç”µè·¯åµŒå…¥çš„ESNåœ¨å¤šç§ä»»åŠ¡ä¸‹ä¼˜äºå¸¸è§äºå¤šå±‚æ„ŸçŸ¥å™¨çš„å‰é¦ˆç‰¹æ€§çš„ç®€å•äºŒåˆ†å›¾å’Œå„ç§ç©ºæ¨¡å‹ï¼Œå¹¶ä¸”é€‚åº”äºå­˜å‚¨åº“çš„å…³é”®æ¡ä»¶ã€‚æˆ‘ä»¬å¯¹é¢„å…ˆç¡®å®šçš„fMRIè¯±å¯¼çš„å­ç”µè·¯è¿›è¡Œäº†å½»åº•çš„åˆ†æï¼Œå¹¶é‡ç‚¹ä»‹ç»äº†å®ƒä»¬åœ¨ç¡®å®šESNæ€§èƒ½ä¸­å‘æŒ¥é‡è¦ä½œç”¨çš„å›¾è®ºå±æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.14999v2">PDF</a> 10 pages, 12 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºè„‘è¿æ¥ç»„ï¼ˆconnectomicsï¼‰çš„ç±»è„‘è®¡ç®—è¶‹åŠ¿ï¼Œé‡ç‚¹ç ”ç©¶å°†äººç±»å¤§è„‘çš„å¤§è§„æ¨¡ç»“æ„è¿æ¥ä½“ï¼ˆSCsï¼‰åµŒå…¥å›å£°çŠ¶æ€ç½‘ç»œï¼ˆESNsï¼‰çš„æ–¹æ³•ã€‚æ–‡ç« æå‡ºäº†ä¸€ä¸ªå®æ–½å’Œè¯„ä¼°å…·æœ‰ä¸åŒåµŒå…¥æ‹“æ‰‘ç»“æ„å’Œå¤„ç†&#x2F;è®°å¿†ä»»åŠ¡çš„ESNsçš„ç®¡é“ï¼Œå¹¶æ¢è®¨äº†åŸºäºfMRIè¯±å¯¼çš„å­ç”µè·¯åµŒå…¥å¯¹ESNæ€§èƒ½çš„å½±å“ã€‚ç ”ç©¶å‘ç°ï¼Œä¸åŒä»»åŠ¡çš„æœ€ä¼˜æ€§èƒ½é«˜åº¦ä¾èµ–äºè¿™äº›é¢„å®šä¹‰çš„fMRIè¯±å¯¼å­ç”µè·¯çš„ç¥ç»ç”Ÿç†å­¦ç‰¹æ€§ã€‚æ€»ä½“ä¸Šï¼ŒfMRIè¯±å¯¼çš„å­ç”µè·¯åµŒå…¥çš„ESNåœ¨ä¸åŒä»»åŠ¡å’Œå‚¨å¤‡æ± ä¸´ç•Œæ¡ä»¶ä¸‹ï¼Œè¡¨ç°å‡ºä¼˜äºç®€å•çš„å‰é¦ˆå¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰æ¨¡å‹çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰è¶‹åŠ¿ï¼šåˆ©ç”¨è„‘è¿æ¥ç»„ä½œä¸ºäººå·¥ç¥ç»ç½‘ç»œè“å›¾ï¼Œè¿›è¡Œç±»è„‘è®¡ç®—ç ”ç©¶ã€‚</li>
<li>ç ”ç©¶é‡ç‚¹ï¼šå°†äººç±»å¤§è„‘ç»“æ„è¿æ¥ä½“åµŒå…¥å›å£°çŠ¶æ€ç½‘ç»œï¼ˆESNï¼‰ã€‚</li>
<li>ESNåµŒå…¥çš„å…³é”®æ­¥éª¤ï¼šéœ€è¦é¢„å®šä¹‰çš„è¯»å†™å±‚ï¼Œç”±åµŒå…¥å‚¨å¤‡åº“è¯±å¯¼çš„å­å›¾æ„å»ºã€‚</li>
<li>ç ”ç©¶ç©ºç™½ï¼šå°šä¸æ¸…æ¥šä»ç¥ç»ç”Ÿç†å­¦è§’åº¦å°†fMRIè¯±å¯¼çš„å­ç”µè·¯&#x2F;ç½‘ç»œåµŒå…¥SCæ˜¯å¦åˆç†ã€‚</li>
<li>ç ”ç©¶æ–¹æ³•ï¼šæå‡ºä¸€ä¸ªç®¡é“æ¥å®æ–½å’Œè¯„ä¼°å…·æœ‰ä¸åŒåµŒå…¥æ‹“æ‰‘ç»“æ„å’Œå¤„ç†ä»»åŠ¡çš„ESNsã€‚</li>
<li>å®éªŒç»“æœï¼šfMRIè¯±å¯¼çš„å­ç”µè·¯åµŒå…¥çš„ESNåœ¨ä¸åŒä»»åŠ¡ä¸­è¡¨ç°å‡ºæ›´å¥½çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.14999">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-bdf2ea0927ed155edcb65772783da209.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-88900a5c48ccfea0d6cc8278edca969f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a420d94e2544e895adf2a13c2393aec9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f34c035f8f932b0c139d7d1e508abc9d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-399cd6ab66cea23ac9ad535b6a4cf4d8.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Agent-Skill-Acquisition-for-Large-Language-Models-via-CycleQD"><a href="#Agent-Skill-Acquisition-for-Large-Language-Models-via-CycleQD" class="headerlink" title="Agent Skill Acquisition for Large Language Models via CycleQD"></a>Agent Skill Acquisition for Large Language Models via CycleQD</h2><p><strong>Authors:So Kuroki, Taishi Nakamura, Takuya Akiba, Yujin Tang</strong></p>
<p>Training large language models to acquire specific skills remains a challenging endeavor. Conventional training approaches often struggle with data distribution imbalances and inadequacies in objective functions that do not align well with task-specific performance. To address these challenges, we introduce CycleQD, a novel approach that leverages the Quality Diversity framework through a cyclic adaptation of the algorithm, along with a model merging based crossover and an SVD-based mutation. In CycleQD, each taskâ€™s performance metric is alternated as the quality measure while the others serve as the behavioral characteristics. This cyclic focus on individual tasks allows for concentrated effort on one task at a time, eliminating the need for data ratio tuning and simplifying the design of the objective function. Empirical results from AgentBench indicate that applying CycleQD to LLAMA3-8B-INSTRUCT based models not only enables them to surpass traditional fine-tuning methods in coding, operating systems, and database tasks, but also achieves performance on par with GPT-3.5-TURBO, which potentially contains much more parameters, across these domains. Crucially, this enhanced performance is achieved while retaining robust language capabilities, as evidenced by its performance on widely adopted language benchmark tasks. We highlight the key design choices in CycleQD, detailing how these contribute to its effectiveness. Furthermore, our method is general and can be applied to image segmentation models, highlighting its applicability across different domains. </p>
<blockquote>
<p>è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ä»¥è·å–ç‰¹å®šæŠ€èƒ½ä»ç„¶æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„å·¥ä½œã€‚ä¼ ç»Ÿçš„è®­ç»ƒæ–¹æ³•å¸¸å¸¸é¢ä¸´æ•°æ®åˆ†å¸ƒä¸å¹³è¡¡å’Œå®¢è§‚å‡½æ•°ä¸ç›®æ ‡ç‰¹å®šæ€§èƒ½ä¸åŒ¹é…çš„é—®é¢˜ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†CycleQDè¿™ä¸€æ–°æ–¹æ³•ï¼Œå®ƒåˆ©ç”¨è´¨é‡å¤šæ ·æ€§æ¡†æ¶ï¼Œé€šè¿‡ç®—æ³•çš„å¾ªç¯é€‚åº”ï¼Œç»“åˆåŸºäºæ¨¡å‹åˆå¹¶çš„äº¤å‰å’ŒåŸºäºSVDçš„çªå˜ã€‚åœ¨CycleQDä¸­ï¼Œæ¯ä¸ªä»»åŠ¡çš„æ€§èƒ½æŒ‡æ ‡è¢«äº¤æ›¿ç”¨ä½œè´¨é‡åº¦é‡ï¼Œè€Œå…¶ä»–ä»»åŠ¡åˆ™ä½œä¸ºè¡Œä¸ºç‰¹å¾ã€‚è¿™ç§å¯¹å•ä¸ªä»»åŠ¡çš„å¾ªç¯å…³æ³¨å…è®¸æ¯æ¬¡é›†ä¸­ç²¾åŠ›å®Œæˆä¸€ä¸ªä»»åŠ¡ï¼Œä»è€Œæ¶ˆé™¤äº†å¯¹æ•°æ®æ¯”ç‡è°ƒæ•´çš„éœ€æ±‚ï¼Œå¹¶ç®€åŒ–äº†ç›®æ ‡å‡½æ•°çš„è®¾è®¡ã€‚æ¥è‡ªAgentBenchçš„ç»éªŒç»“æœè¡¨æ˜ï¼Œå°†CycleQDåº”ç”¨äºLLAMA3-8B-INSTRUCTåŸºç¡€æ¨¡å‹ï¼Œä¸ä»…èƒ½ä½¿å®ƒä»¬åœ¨ç¼–ç ã€æ“ä½œç³»ç»Ÿå’Œæ•°æ®åº“ä»»åŠ¡ä¸Šè¶…è¶Šä¼ ç»Ÿçš„å¾®è°ƒæ–¹æ³•ï¼Œè€Œä¸”åœ¨è¿™å‡ ä¸ªé¢†åŸŸå®ç°äº†ä¸GPT-3.5-TURBOç›¸å½“çš„æ€§èƒ½ï¼Œå°½ç®¡GPT-3.5-TURBOå¯èƒ½åŒ…å«æ›´å¤šçš„å‚æ•°ã€‚å…³é”®çš„æ˜¯ï¼Œè¿™ç§å¢å¼ºçš„æ€§èƒ½æ˜¯åœ¨ä¿ç•™ç¨³å¥çš„è¯­è¨€èƒ½åŠ›çš„åŒæ—¶å®ç°çš„ï¼Œè¿™åœ¨å…¶å¹¿æ³›é‡‡ç”¨çš„è¯­è¨€åŸºå‡†ä»»åŠ¡ä¸Šçš„è¡¨ç°å¾—åˆ°äº†è¯æ˜ã€‚æˆ‘ä»¬å¼ºè°ƒäº†CycleQDä¸­çš„å…³é”®è®¾è®¡é€‰æ‹©ï¼Œè¯¦ç»†è¯´æ˜äº†è¿™äº›æ˜¯å¦‚ä½•ä¿ƒæˆå…¶æœ‰æ•ˆæ€§çš„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¯é€šç”¨çš„ï¼Œå¯åº”ç”¨äºå›¾åƒåˆ†å‰²æ¨¡å‹ï¼Œçªå‡ºäº†å…¶åœ¨ä¸åŒé¢†åŸŸçš„åº”ç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.14735v3">PDF</a> To appear at the 13th International Conference on Learning   Representations (ICLR 2025)</p>
<p><strong>Summary</strong></p>
<p>CycleQDæ˜¯ä¸€ç§é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„æ–°å‹è®­ç»ƒæ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿè®­ç»ƒæ–¹å¼é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¦‚æ•°æ®åˆ†å¸ƒä¸å‡è¡¡å’Œç›®æ ‡å‡½æ•°ä¸ç‰¹å®šä»»åŠ¡æ€§èƒ½ä¸åŒ¹é…çš„é—®é¢˜ã€‚å®ƒé€šè¿‡è´¨é‡å¤šæ ·æ€§æ¡†æ¶çš„å¾ªç¯é€‚åº”ç®—æ³•ã€æ¨¡å‹åˆå¹¶åŸºç¡€ä¸Šçš„äº¤å‰ä»¥åŠSVDåŸºç¡€ä¸Šçš„çªå˜æ¥å®ç°ã€‚CycleQDèƒ½é›†ä¸­å…³æ³¨å•ä¸€ä»»åŠ¡ï¼Œç®€åŒ–ç›®æ ‡å‡½æ•°è®¾è®¡ï¼Œå‡å°‘æ•°æ®æ¯”ç‡è°ƒæ•´çš„éœ€è¦ã€‚åœ¨AgentBenchä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œå¯¹LLAMA3-8B-INSTRUCTåŸºç¡€æ¨¡å‹åº”ç”¨CycleQDï¼Œåœ¨ç¼–ç ã€æ“ä½œç³»ç»Ÿå’Œæ•°æ®åº“ä»»åŠ¡ä¸Šè¶…è¶Šäº†ä¼ ç»Ÿå¾®è°ƒæ–¹æ³•ï¼Œæ€§èƒ½ä¸GPT-3.5-TURBOç›¸å½“ã€‚åŒæ—¶ï¼Œè¯¥æ–¹æ³•ä¿ç•™äº†å¼ºå¤§çš„è¯­è¨€èƒ½åŠ›ï¼Œå¹¶åœ¨å¹¿æ³›é‡‡ç”¨çš„è¯­è¨€åŸºå‡†ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>CycleQDæ˜¯ä¸€ç§é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„æ–°å‹è®­ç»ƒæ–¹æ³•ã€‚</li>
<li>CycleQDè§£å†³äº†ä¼ ç»Ÿè®­ç»ƒæ–¹å¼ä¸­çš„æ•°æ®åˆ†å¸ƒä¸å‡è¡¡å’Œç›®æ ‡å‡½æ•°ä¸ä»»åŠ¡æ€§èƒ½ä¸åŒ¹é…çš„é—®é¢˜ã€‚</li>
<li>CycleQDé€šè¿‡è´¨é‡å¤šæ ·æ€§æ¡†æ¶çš„å¾ªç¯é€‚åº”ç®—æ³•ã€æ¨¡å‹åˆå¹¶åŸºç¡€ä¸Šçš„äº¤å‰å’ŒSVDåŸºç¡€ä¸Šçš„çªå˜æ¥å®ç°ã€‚</li>
<li>CycleQDèƒ½é›†ä¸­å…³æ³¨å•ä¸€ä»»åŠ¡ï¼Œç®€åŒ–ç›®æ ‡å‡½æ•°è®¾è®¡ï¼Œå‡å°‘æ•°æ®æ¯”ç‡è°ƒæ•´çš„éœ€è¦ã€‚</li>
<li>åœ¨AgentBenchçš„å®éªŒä¸­ï¼Œåº”ç”¨CycleQDçš„æ¨¡å‹åœ¨ç¼–ç ã€æ“ä½œç³»ç»Ÿå’Œæ•°æ®åº“ä»»åŠ¡ä¸Šè¶…è¶Šäº†ä¼ ç»Ÿå¾®è°ƒæ–¹æ³•ã€‚</li>
<li>CycleQDçš„æ€§èƒ½ä¸GPT-3.5-TURBOç›¸å½“ï¼ŒåŒæ—¶ä¿ç•™äº†å¼ºå¤§çš„è¯­è¨€èƒ½åŠ›ã€‚</li>
<li>CycleQDæ–¹æ³•åœ¨å¹¿æ³›é‡‡ç”¨çš„è¯­è¨€åŸºå‡†ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.14735">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e94e886e9de7695e0936f8151d9c22c1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b4da8ead3391d46521dd5d4f4dd251ba.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-db210a04bf3de59b821d362ea3c78899.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Multi-Tiered-Self-Contrastive-Learning-for-Medical-Microwave-Radiometry-MWR-Breast-Cancer-Detection"><a href="#Multi-Tiered-Self-Contrastive-Learning-for-Medical-Microwave-Radiometry-MWR-Breast-Cancer-Detection" class="headerlink" title="Multi-Tiered Self-Contrastive Learning for Medical Microwave Radiometry   (MWR) Breast Cancer Detection"></a>Multi-Tiered Self-Contrastive Learning for Medical Microwave Radiometry   (MWR) Breast Cancer Detection</h2><p><strong>Authors:Christoforos Galazis, Huiyi Wu, Igor Goryanin</strong></p>
<p>Improving breast cancer detection and monitoring techniques is a critical objective in healthcare, driving the need for innovative imaging technologies and diagnostic approaches. This study introduces a novel multi-tiered self-contrastive model tailored for microwave radiometry (MWR) in breast cancer detection. Our approach incorporates three distinct models: Local-MWR (L-MWR), Regional-MWR (R-MWR), and Global-MWR (G-MWR), designed to analyze varying sub-regional comparisons within the breasts. These models are integrated through the Joint-MWR (J-MWR) network, which leverages self-contrastive results at each analytical level to improve diagnostic accuracy. Utilizing a dataset of 4,932 female patients, our research demonstrates the efficacy of our proposed models. Notably, the J-MWR model achieves a Matthewâ€™s correlation coefficient of 0.74 $\pm$ 0.018, surpassing existing MWR neural networks and contrastive methods. These findings highlight the potential of self-contrastive learning techniques in improving the diagnostic accuracy and generalizability for MWR-based breast cancer detection. This advancement holds considerable promise for future investigations into enabling point-of-care testing. The source code is available at: <a target="_blank" rel="noopener" href="https://github.com/cgalaz01/self_contrastive_mwr">https://github.com/cgalaz01/self_contrastive_mwr</a>. </p>
<blockquote>
<p>æé«˜ä¹³è…ºç™Œæ£€æµ‹å’Œç›‘æµ‹æŠ€æœ¯æ˜¯åŒ»ç–—ä¿å¥çš„é‡è¦ç›®æ ‡ï¼Œè¿™æ¨åŠ¨äº†åˆ›æ–°æˆåƒæŠ€æœ¯å’Œè¯Šæ–­æ–¹æ³•çš„éœ€è¦ã€‚æœ¬ç ”ç©¶å¼•å…¥äº†ä¸€ç§é’ˆå¯¹å¾®æ³¢è¾å°„ä»ªï¼ˆMWRï¼‰ä¹³è…ºç™Œæ£€æµ‹çš„æ–°å‹åˆ†å±‚è‡ªå¯¹æ¯”æ¨¡å‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†ä¸‰ç§ä¸åŒçš„æ¨¡å‹ï¼šLocal-MWRï¼ˆL-MWRï¼‰ã€Regional-MWRï¼ˆR-MWRï¼‰å’ŒGlobal-MWRï¼ˆG-MWRï¼‰ï¼Œæ—¨åœ¨åˆ†æä¹³æˆ¿å†…éƒ¨ä¸åŒäºšåŒºåŸŸçš„æ¯”è¾ƒã€‚è¿™äº›æ¨¡å‹é€šè¿‡Joint-MWRï¼ˆJ-MWRï¼‰ç½‘ç»œè¿›è¡Œé›†æˆï¼Œè¯¥ç½‘ç»œåˆ©ç”¨æ¯ä¸ªåˆ†æçº§åˆ«çš„è‡ªå¯¹æ¯”ç»“æœæ¥æé«˜è¯Šæ–­å‡†ç¡®æ€§ã€‚åˆ©ç”¨4932åå¥³æ€§æ‚£è€…çš„æ•°æ®é›†ï¼Œæˆ‘ä»¬çš„ç ”ç©¶è¯æ˜äº†æ‰€æå‡ºæ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒJ-MWRæ¨¡å‹çš„Matthewç›¸å…³ç³»æ•°è¾¾åˆ°0.74Â±0.018ï¼Œè¶…è¶Šäº†ç°æœ‰çš„MWRç¥ç»ç½‘ç»œå’Œå¯¹æ¯”æ–¹æ³•ã€‚è¿™äº›å‘ç°çªæ˜¾äº†è‡ªå¯¹æ¯”å­¦ä¹ æŠ€æœ¯åœ¨æé«˜åŸºäºMWRçš„ä¹³è…ºç™Œæ£€æµ‹çš„è¯Šæ–­å‡†ç¡®æ€§å’Œé€šç”¨æ€§æ–¹é¢çš„æ½œåŠ›ã€‚è¿™ä¸€è¿›å±•ä¸ºæœªæ¥çš„å³æ—¶æ£€æµ‹ç ”ç©¶å¸¦æ¥äº†å·¨å¤§çš„å¸Œæœ›ã€‚æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/cgalaz01/self_contrastive_mwr%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/cgalaz01/self_contrastive_mwrè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.04636v2">PDF</a> </p>
<p><strong>Summary</strong><br>     è¯¥ç ”ç©¶é‡‡ç”¨æ–°å‹å¤šå±‚è‡ªå¯¹æ¯”æ¨¡å‹ï¼Œç»“åˆå¾®æ³¢è¾å°„è®¡æŠ€æœ¯ï¼Œæé«˜ä¹³è…ºç™Œæ£€æµ‹ä¸ç›‘æ§æŠ€æœ¯ã€‚é€šè¿‡æœ¬åœ°ã€åŒºåŸŸå’Œå…¨å±€ä¸‰ä¸ªå±‚æ¬¡çš„å¾®æ³¢è¾å°„è®¡æ¨¡å‹åˆ†æï¼Œç»“åˆè”åˆå¾®æ³¢è¾å°„è®¡ç½‘ç»œï¼Œæé«˜è¯Šæ–­å‡†ç¡®æ€§ã€‚ç ”ç©¶ä½¿ç”¨4932åå¥³æ€§æ‚£è€…æ•°æ®é›†ï¼Œç»“æœæ˜¾ç¤ºè”åˆå¾®æ³¢è¾å°„è®¡æ¨¡å‹é©¬ä¿®ç›¸å…³ç³»æ•°è¾¾0.74Â±0.018ï¼Œä¼˜äºç°æœ‰æ–¹æ³•ã€‚è¿™çªæ˜¾å‡ºè‡ªå¯¹æ¯”å­¦ä¹ æŠ€æœ¯åœ¨å¾®æ³¢è¾å°„è®¡ä¹³è…ºç™Œæ£€æµ‹ä¸­çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶è‡´åŠ›äºæé«˜ä¹³è…ºç™Œæ£€æµ‹å’Œç›‘æ§æŠ€æœ¯ï¼Œå¼•å…¥æ–°å‹å¤šå±‚è‡ªå¯¹æ¯”æ¨¡å‹ã€‚</li>
<li>é‡‡ç”¨å¾®æ³¢è¾å°„è®¡æŠ€æœ¯ï¼Œç»“åˆä¸‰ç§ä¸åŒå±‚æ¬¡çš„æ¨¡å‹ï¼ˆLocal-MWRã€Regional-MWRã€Global-MWRï¼‰è¿›è¡Œåˆ†æã€‚</li>
<li>é€šè¿‡è”åˆå¾®æ³¢è¾å°„è®¡ç½‘ç»œï¼ˆJ-MWRï¼‰æ•´åˆæ¨¡å‹ï¼Œåˆ©ç”¨å„åˆ†æå±‚æ¬¡çš„è‡ªå¯¹æ¯”ç»“æœæé«˜è¯Šæ–­å‡†ç¡®æ€§ã€‚</li>
<li>ä½¿ç”¨4932åå¥³æ€§æ‚£è€…æ•°æ®é›†è¿›è¡Œå®è¯ç ”ç©¶ï¼Œç»“æœæ˜¾ç¤ºJ-MWRæ¨¡å‹è¡¨ç°ä¼˜å¼‚ï¼Œé©¬ä¿®ç›¸å…³ç³»æ•°è¾¾0.74Â±0.018ã€‚</li>
<li>J-MWRæ¨¡å‹ä¼˜äºç°æœ‰çš„å¾®æ³¢è¾å°„è®¡ç¥ç»ç½‘ç»œå’Œå¯¹æ¯”æ–¹æ³•ï¼Œçªæ˜¾è‡ªå¯¹æ¯”å­¦ä¹ æŠ€æœ¯åœ¨ä¹³è…ºç™Œæ£€æµ‹ä¸­çš„æ½œåŠ›ã€‚</li>
<li>æºä»£ç å…¬å¼€å¯ç”¨ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶å’Œåº”ç”¨æä¾›äº†åŸºç¡€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.04636">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-912b7ee120de410c355722626efa1a5b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-45b36a9cd4312033429a94e4122584d5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5de819cdd28cd9cc61b672fd90d86c89.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ef3ba50716130011e80fcb6f7de0ebec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c9d6682ae5f53188fe55153ca4c458a2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-55317862d84d8ef81671d957027f1998.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Efficient-and-Accurate-Pneumonia-Detection-Using-a-Novel-Multi-Scale-Transformer-Approach"><a href="#Efficient-and-Accurate-Pneumonia-Detection-Using-a-Novel-Multi-Scale-Transformer-Approach" class="headerlink" title="Efficient and Accurate Pneumonia Detection Using a Novel Multi-Scale   Transformer Approach"></a>Efficient and Accurate Pneumonia Detection Using a Novel Multi-Scale   Transformer Approach</h2><p><strong>Authors:Alireza Saber, Pouria Parhami, Alimohammad Siahkarzadeh, Mansoor Fateh, Amirreza Fateh</strong></p>
<p>Pneumonia, a prevalent respiratory infection, remains a leading cause of morbidity and mortality worldwide, particularly among vulnerable populations. Chest X-rays serve as a primary tool for pneumonia detection; however, variations in imaging conditions and subtle visual indicators complicate consistent interpretation. Automated tools can enhance traditional methods by improving diagnostic reliability and supporting clinical decision-making. In this study, we propose a novel multi-scale transformer approach for pneumonia detection that integrates lung segmentation and classification into a unified framework. Our method introduces a lightweight transformer-enhanced TransUNet for precise lung segmentation, achieving a Dice score of 95.68% on the â€œChest X-ray Masks and Labelsâ€ dataset with fewer parameters than traditional transformers. For classification, we employ pre-trained ResNet models (ResNet-50 and ResNet-101) to extract multi-scale feature maps, which are then processed through a modified transformer module to enhance pneumonia detection. This integration of multi-scale feature extraction and lightweight transformer modules ensures robust performance, making our method suitable for resource-constrained clinical environments. Our approach achieves 93.75% accuracy on the â€œKermanyâ€ dataset and 96.04% accuracy on the â€œCohenâ€ dataset, outperforming existing methods while maintaining computational efficiency. This work demonstrates the potential of multi-scale transformer architectures to improve pneumonia diagnosis, offering a scalable and accurate solution to global healthcare challenges.â€<a target="_blank" rel="noopener" href="https://github.com/amirrezafateh/Multi-Scale-Transformer-Pneumonia">https://github.com/amirrezafateh/Multi-Scale-Transformer-Pneumonia</a>â€œ </p>
<blockquote>
<p>è‚ºç‚æ˜¯ä¸€ç§å¸¸è§çš„å‘¼å¸é“æ„ŸæŸ“ï¼Œä»ç„¶æ˜¯å…¨çƒå‘ç—…ç‡å’Œæ­»äº¡ç‡çš„ä¸»è¦åŸå› ï¼Œç‰¹åˆ«æ˜¯åœ¨è„†å¼±äººç¾¤ä¸­ã€‚èƒ¸éƒ¨Xå°„çº¿æ˜¯æ£€æµ‹è‚ºç‚çš„ä¸»è¦å·¥å…·ï¼›ç„¶è€Œï¼Œæˆåƒæ¡ä»¶çš„å·®å¼‚å’Œå¾®å¦™çš„è§†è§‰æŒ‡æ ‡ä½¿ä¸€è‡´çš„è§£è¯»å˜å¾—å¤æ‚ã€‚è‡ªåŠ¨åŒ–å·¥å…·å¯ä»¥é€šè¿‡æé«˜è¯Šæ–­çš„å¯é æ€§å’Œæ”¯æŒä¸´åºŠå†³ç­–æ¥å¢å¼ºä¼ ç»Ÿæ–¹æ³•ã€‚åœ¨ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„å¤šå°ºåº¦å˜æ¢å™¨æ–¹æ³•ï¼Œç”¨äºè‚ºç‚æ£€æµ‹ï¼Œè¯¥æ–¹æ³•å°†è‚ºéƒ¨åˆ†å‰²å’Œåˆ†ç±»æ•´åˆåˆ°ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ä¸­ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†ä¸€ç§è½»é‡çº§çš„å˜å‹å™¨å¢å¼ºå‹TransUNetï¼Œç”¨äºç²¾ç¡®çš„è‚ºéƒ¨åˆ†å‰²ï¼Œåœ¨â€œèƒ¸éƒ¨Xå°„çº¿æ©è†œå’Œæ ‡ç­¾â€æ•°æ®é›†ä¸Šå®ç°äº†95.68%çš„Diceå¾—åˆ†ï¼ŒåŒæ—¶å‚æ•°å°‘äºä¼ ç»Ÿå˜å‹å™¨ã€‚å¯¹äºåˆ†ç±»ï¼Œæˆ‘ä»¬é‡‡ç”¨é¢„è®­ç»ƒçš„ResNetæ¨¡å‹ï¼ˆResNet-50å’ŒResNet-101ï¼‰æå–å¤šå°ºåº¦ç‰¹å¾å›¾ï¼Œç„¶åé€šè¿‡ä¿®æ”¹åçš„å˜å‹å™¨æ¨¡å—è¿›è¡Œå¤„ç†ï¼Œä»¥æé«˜è‚ºç‚æ£€æµ‹çš„å‡†ç¡®æ€§ã€‚å¤šå°ºåº¦ç‰¹å¾æå–å’Œè½»é‡çº§å˜å‹å™¨æ¨¡å—çš„é›†æˆç¡®ä¿äº†ç¨³å¥çš„æ€§èƒ½ï¼Œä½¿æˆ‘ä»¬çš„æ–¹æ³•é€‚åˆèµ„æºå—é™çš„ä¸´åºŠç¯å¢ƒã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨â€œKermanyâ€æ•°æ®é›†ä¸Šå®ç°äº†93.75%çš„å‡†ç¡®ç‡ï¼Œåœ¨â€œCohenâ€æ•°æ®é›†ä¸Šå®ç°äº†96.04%çš„å‡†ç¡®ç‡ï¼Œè¶…è¶Šäº†ç°æœ‰æ–¹æ³•çš„åŒæ—¶ä¿æŒäº†è®¡ç®—æ•ˆç‡ã€‚è¿™é¡¹å·¥ä½œè¯æ˜äº†å¤šå°ºåº¦å˜æ¢å™¨æ¶æ„åœ¨è‚ºç‚è¯Šæ–­ä¸­çš„æ½œåŠ›ï¼Œä¸ºå…¨çƒåŒ»ç–—ä¿å¥æŒ‘æˆ˜æä¾›äº†å¯ä¼¸ç¼©å’Œå‡†ç¡®çš„è§£å†³æ–¹æ¡ˆã€‚<a target="_blank" rel="noopener" href="https://github.com/amirrezafateh/Multi-Scale-Transformer-Pneumonia">https://github.com/amirrezafateh/Multi-Scale-Transformer-Pneumonia</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.04290v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åˆ©ç”¨å¤šå°ºåº¦Transformerè¿›è¡Œè‚ºç‚æ£€æµ‹çš„æ–°æ–¹æ³•ï¼Œå°†è‚ºéƒ¨åˆ†å‰²å’Œåˆ†ç±»æ•´åˆåˆ°ç»Ÿä¸€æ¡†æ¶ä¸­ã€‚é‡‡ç”¨è½»é‡çº§Transformerå¢å¼ºçš„TransUNetè¿›è¡Œç²¾ç¡®è‚ºåˆ†å‰²ï¼Œåœ¨â€œChest X-ray Masks and Labelsâ€æ•°æ®é›†ä¸Šè¾¾åˆ°95.68%çš„Diceå¾—åˆ†ã€‚åˆ†ç±»æ–¹é¢ï¼Œé‡‡ç”¨é¢„è®­ç»ƒçš„ResNetæ¨¡å‹æå–å¤šå°ºåº¦ç‰¹å¾å›¾ï¼Œé€šè¿‡ä¿®æ”¹åçš„Transformeræ¨¡å—è¿›è¡Œå¤„ç†ï¼Œæé«˜è‚ºç‚æ£€æµ‹çš„å‡†ç¡®æ€§ã€‚è¯¥æ–¹æ³•åœ¨â€œKermanyâ€å’Œâ€œCohenâ€æ•°æ®é›†ä¸Šåˆ†åˆ«è¾¾åˆ°äº†93.75%å’Œ96.04%çš„å‡†ç¡®ç‡ï¼Œå±•ç°å‡ºå¤šå°ºåº¦Transformeræ¶æ„åœ¨è‚ºç‚è¯Šæ–­ä¸­çš„æ½œåŠ›ï¼Œä¸ºå…¨çƒåŒ»ç–—æŒ‘æˆ˜æä¾›å¯ä¼¸ç¼©å’Œå‡†ç¡®çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‚ºç‚æ˜¯ä¸€ç§å…¨çƒæ€§çš„å¸¸è§ç—…ï¼Œä»ç„¶æ˜¯å‘ç—…ç‡å’Œæ­»äº¡ç‡çš„ä¸»è¦åŸå› ä¹‹ä¸€ï¼Œç‰¹åˆ«æ˜¯åœ¨è„†å¼±äººç¾¤ä¸­ã€‚</li>
<li>èƒ¸éƒ¨Xå°„çº¿æ˜¯æ£€æµ‹è‚ºç‚çš„ä¸»è¦å·¥å…·ï¼Œä½†æˆåƒæ¡ä»¶å’Œç»†å¾®çš„è§†è§‰æŒ‡æ ‡ä¼šå½±å“ä¸€è‡´çš„è§£è¯»ã€‚</li>
<li>è‡ªåŠ¨åŒ–å·¥å…·é€šè¿‡æé«˜è¯Šæ–­å¯é æ€§å’Œæ”¯æŒä¸´åºŠå†³ç­–ï¼Œèƒ½å¢å¼ºä¼ ç»Ÿæ–¹æ³•çš„æ•ˆèƒ½ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„å¤šå°ºåº¦Transformeræ–¹æ³•ç”¨äºè‚ºç‚æ£€æµ‹ï¼Œæ•´åˆè‚ºéƒ¨åˆ†å‰²å’Œåˆ†ç±»åˆ°ç»Ÿä¸€æ¡†æ¶ä¸­ã€‚</li>
<li>é‡‡ç”¨è½»é‡çº§Transformerè¿›è¡Œç²¾ç¡®è‚ºåˆ†å‰²ï¼Œåœ¨ç‰¹å®šæ•°æ®é›†ä¸Šå–å¾—äº†é«˜Diceå¾—åˆ†ã€‚</li>
<li>åˆ©ç”¨é¢„è®­ç»ƒçš„ResNetæ¨¡å‹æå–å¤šå°ºåº¦ç‰¹å¾å›¾ï¼Œå¹¶é€šè¿‡ä¿®æ”¹åçš„Transformeræ¨¡å—æé«˜è‚ºç‚æ£€æµ‹çš„å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.04290">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-57e3bcf3028636420c1107982b8c2944.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5e8d446a34cf1a17b03376fd89fcf7a1.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="MedPromptX-Grounded-Multimodal-Prompting-for-Chest-X-ray-Diagnosis"><a href="#MedPromptX-Grounded-Multimodal-Prompting-for-Chest-X-ray-Diagnosis" class="headerlink" title="MedPromptX: Grounded Multimodal Prompting for Chest X-ray Diagnosis"></a>MedPromptX: Grounded Multimodal Prompting for Chest X-ray Diagnosis</h2><p><strong>Authors:Mai A. Shaaban, Adnan Khan, Mohammad Yaqub</strong></p>
<p>Chest X-ray images are commonly used for predicting acute and chronic cardiopulmonary conditions, but efforts to integrate them with structured clinical data face challenges due to incomplete electronic health records (EHR). This paper introduces MedPromptX, the first clinical decision support system that integrates multimodal large language models (MLLMs), few-shot prompting (FP) and visual grounding (VG) to combine imagery with EHR data for chest X-ray diagnosis. A pre-trained MLLM is utilized to complement the missing EHR information, providing a comprehensive understanding of patientsâ€™ medical history. Additionally, FP reduces the necessity for extensive training of MLLMs while effectively tackling the issue of hallucination. Nevertheless, the process of determining the optimal number of few-shot examples and selecting high-quality candidates can be burdensome, yet it profoundly influences model performance. Hence, we propose a new technique that dynamically refines few-shot data for real-time adjustment to new patient scenarios. Moreover, VG narrows the search area in X-ray images, thereby enhancing the identification of abnormalities. We also release MedPromptX-VQA, a new in-context visual question answering dataset encompassing interleaved images and EHR data derived from MIMIC-IV and MIMIC-CXR-JPG databases. Results demonstrate the SOTA performance of MedPromptX, achieving an 11% improvement in F1-score compared to the baselines. Code and data are publicly available on <a target="_blank" rel="noopener" href="https://github.com/BioMedIA-MBZUAI/MedPromptX">https://github.com/BioMedIA-MBZUAI/MedPromptX</a>. </p>
<blockquote>
<p>èƒ¸éƒ¨Xå…‰ç‰‡å›¾åƒå¸¸ç”¨äºé¢„æµ‹æ€¥æ€§å’Œæ…¢æ€§å¿ƒè‚ºç–¾ç—…ï¼Œä½†å°†å…¶ä¸ç»“æ„åŒ–ä¸´åºŠæ•°æ®ç›¸ç»“åˆçš„å°è¯•é¢ä¸´ç”±äºç”µå­å¥åº·è®°å½•ï¼ˆEHRï¼‰ä¸å®Œæ•´è€Œå¸¦æ¥çš„æŒ‘æˆ˜ã€‚æœ¬æ–‡ä»‹ç»äº†MedPromptXï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç»“åˆå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ã€å°‘æ ·æœ¬æç¤ºï¼ˆFPï¼‰å’Œè§†è§‰å®šä½ï¼ˆVGï¼‰çš„ä¸´åºŠå†³ç­–æ”¯æŒç³»ç»Ÿï¼Œç”¨äºå°†å›¾åƒä¸EHRæ•°æ®ç›¸ç»“åˆè¿›è¡Œèƒ¸éƒ¨Xå…‰è¯Šæ–­ã€‚é¢„è®­ç»ƒçš„MLLMç”¨äºè¡¥å……ç¼ºå¤±çš„EHRä¿¡æ¯ï¼Œä¸ºæ‚£è€…ç—…å²æä¾›å…¨é¢çš„ç†è§£ã€‚æ­¤å¤–ï¼ŒFPé™ä½äº†å¯¹MLLMè¿›è¡Œå¤§é‡åŸ¹è®­çš„å¿…è¦ï¼ŒåŒæ—¶æœ‰æ•ˆåœ°è§£å†³äº†è™šæ„é—®é¢˜ã€‚ç„¶è€Œï¼Œç¡®å®šæœ€ä½³æ•°é‡çš„å°‘æ•°æ ·æœ¬å¹¶é€‰æ‹©é«˜è´¨é‡å€™é€‰è€…çš„è¿‡ç¨‹å¯èƒ½æ˜¯ä¸€ä¸ªç¹é‡çš„ä»»åŠ¡ï¼Œä½†å®ƒå¯¹æ¨¡å‹æ€§èƒ½äº§ç”Ÿæ·±è¿œå½±å“ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æŠ€æœ¯ï¼Œå¯ä»¥åŠ¨æ€åœ°å®Œå–„å°‘æ•°æ ·æœ¬æ•°æ®ï¼Œä»¥å®æ—¶é€‚åº”æ–°æ‚£è€…æƒ…å†µã€‚æ­¤å¤–ï¼ŒVGç¼©å°äº†Xå…‰å›¾åƒä¸­çš„æœç´¢åŒºåŸŸï¼Œä»è€Œæé«˜äº†å¼‚å¸¸æ£€æµ‹çš„å‡†ç¡®æ€§ã€‚æˆ‘ä»¬è¿˜å‘å¸ƒäº†MedPromptX-VQAï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„ä¸Šä¸‹æ–‡è§†è§‰é—®ç­”æ•°æ®é›†ï¼Œæ¶µç›–äº†æ¥è‡ªMIMIC-IVå’ŒMIMIC-CXR-JPGæ•°æ®åº“äº¤ç»‡çš„å›¾åƒå’ŒEHRæ•°æ®ã€‚ç»“æœè¡¨æ˜ï¼ŒMedPromptXçš„æ€§èƒ½å¤„äºé¢†å…ˆæ°´å¹³ï¼Œä¸åŸºçº¿ç›¸æ¯”ï¼ŒF1åˆ†æ•°æé«˜äº†11%ã€‚ä»£ç å’Œæ•°æ®å·²åœ¨<a target="_blank" rel="noopener" href="https://github.com/BioMedIA-MBZUAI/MedPromptX%E4%B8%8A%E5%85%AC%E5%BC%80%E3%80%82">https://github.com/BioMedIA-MBZUAI/MedPromptXä¸Šå…¬å¼€ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.15585v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„ä¸´åºŠå†³ç­–æ”¯æŒç³»ç»Ÿâ€”â€”MedPromptXï¼Œå®ƒé›†æˆäº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ã€å°‘æ ·æœ¬æç¤ºï¼ˆFPï¼‰å’Œè§†è§‰å®šä½ï¼ˆVGï¼‰æŠ€æœ¯ï¼Œèƒ½å¤Ÿç»“åˆèƒ¸éƒ¨Xå…‰å›¾åƒä¸ç”µå­å¥åº·è®°å½•ï¼ˆEHRï¼‰æ•°æ®è¿›è¡Œè¯Šæ–­ã€‚é€šè¿‡ä½¿ç”¨é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹æ¥è¡¥å……ç¼ºå¤±çš„EHRä¿¡æ¯ï¼Œæé«˜å¯¹ç—…äººåŒ»ç–—å†å²çš„å…¨é¢ç†è§£ã€‚æ­¤å¤–ï¼Œå°‘æ ·æœ¬æç¤ºæŠ€æœ¯é™ä½äº†æ¨¡å‹è®­ç»ƒçš„å¤æ‚æ€§å¹¶è§£å†³äº†è™šæ„é—®é¢˜ï¼Œè€Œè§†è§‰å®šä½æŠ€æœ¯åˆ™æé«˜äº†å¼‚å¸¸æ£€æµ‹çš„å‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MedPromptXæ˜¯é¦–ä¸ªç»“åˆå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ã€å°‘æ ·æœ¬æç¤ºå’Œè§†è§‰å®šä½æŠ€æœ¯çš„ä¸´åºŠå†³ç­–æ”¯æŒç³»ç»Ÿã€‚</li>
<li>å®ƒèƒ½å¤Ÿæ•´åˆèƒ¸éƒ¨Xå…‰å›¾åƒä¸ç”µå­å¥åº·è®°å½•ï¼ˆEHRï¼‰æ•°æ®ï¼Œè¿›è¡Œæ›´å…¨é¢çš„è¯Šæ–­ã€‚</li>
<li>åˆ©ç”¨é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹è¡¥å……ç¼ºå¤±çš„EHRä¿¡æ¯ï¼Œæé«˜ç—…äººåŒ»ç–—å†å²çš„å…¨é¢ç†è§£ã€‚</li>
<li>å°‘æ ·æœ¬æç¤ºæŠ€æœ¯é™ä½äº†æ¨¡å‹è®­ç»ƒçš„éœ€æ±‚å¹¶è§£å†³äº†è™šæ„é—®é¢˜ã€‚</li>
<li>è§†è§‰å®šä½æŠ€æœ¯æé«˜äº†åœ¨Xå…‰å›¾åƒä¸­æ£€æµ‹å¼‚å¸¸çš„å‡†ç¡®æ€§ã€‚</li>
<li>MedPromptX-VQAæ•°æ®é›†åŒ…å«é—´æ–­æ€§å›¾åƒå’ŒEHRæ•°æ®ï¼Œç”¨äºè§†è§‰é—®ç­”ä»»åŠ¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.15585">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b0705f22c5f7b3c8d46ffb2ed7cfa312.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e0abba40901827e899421518cd239548.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-94d60e7a0dd4b3433a37eaadfcd8a78e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-80d6732030403ab97eb7d9c4ef77cdea.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-29/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-29/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-29/TTS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-6602dda176d192481ec844fb0ecd2432.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-29  Estimating the Black Hole Spin for the X-Ray Binary MAXI J1727-203 Based   on Insight-HXMT
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-29
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-29/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-1097014ca847b9e29fb0d83966213471.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-29  UDBE Unsupervised Diffusion-based Brightness Enhancement in Underwater   Images
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-29
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">11189.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
