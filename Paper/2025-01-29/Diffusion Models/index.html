<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-29  UDBE Unsupervised Diffusion-based Brightness Enhancement in Underwater   Images">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-1097014ca847b9e29fb0d83966213471.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-29
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-02-12
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    14.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    58 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-01-29-æ›´æ–°"><a href="#2025-01-29-æ›´æ–°" class="headerlink" title="2025-01-29 æ›´æ–°"></a>2025-01-29 æ›´æ–°</h1><h2 id="UDBE-Unsupervised-Diffusion-based-Brightness-Enhancement-in-Underwater-Images"><a href="#UDBE-Unsupervised-Diffusion-based-Brightness-Enhancement-in-Underwater-Images" class="headerlink" title="UDBE: Unsupervised Diffusion-based Brightness Enhancement in Underwater   Images"></a>UDBE: Unsupervised Diffusion-based Brightness Enhancement in Underwater   Images</h2><p><strong>Authors:Tatiana TaÃ­s Schein, Gustavo Pereira de Almeira, Stephanie Loi BriÃ£o, Rodrigo Andrade de Bem, Felipe Gomes de Oliveira, Paulo L. J. Drews-Jr</strong></p>
<p>Activities in underwater environments are paramount in several scenarios, which drives the continuous development of underwater image enhancement techniques. A major challenge in this domain is the depth at which images are captured, with increasing depth resulting in a darker environment. Most existing methods for underwater image enhancement focus on noise removal and color adjustment, with few works dedicated to brightness enhancement. This work introduces a novel unsupervised learning approach to underwater image enhancement using a diffusion model. Our method, called UDBE, is based on conditional diffusion to maintain the brightness details of the unpaired input images. The input image is combined with a color map and a Signal-Noise Relation map (SNR) to ensure stable training and prevent color distortion in the output images. The results demonstrate that our approach achieves an impressive accuracy rate in the datasets UIEB, SUIM and RUIE, well-established underwater image benchmarks. Additionally, the experiments validate the robustness of our approach, regarding the image quality metrics PSNR, SSIM, UIQM, and UISM, indicating the good performance of the brightness enhancement process. The source code is available here: <a target="_blank" rel="noopener" href="https://github.com/gusanagy/UDBE">https://github.com/gusanagy/UDBE</a>. </p>
<blockquote>
<p>åœ¨æ°´ä¸‹ç¯å¢ƒä¸­çš„åº”ç”¨åœ¨å„ç§åœºæ™¯ä¸­è‡³å…³é‡è¦ï¼Œè¿™æ¨åŠ¨äº†æ°´ä¸‹å›¾åƒå¢å¼ºæŠ€æœ¯çš„ä¸æ–­å‘å±•ã€‚æ­¤é¢†åŸŸçš„ä¸€ä¸ªä¸»è¦æŒ‘æˆ˜æ˜¯å›¾åƒæ•è·çš„æ·±åº¦ï¼Œéšç€æ·±åº¦çš„å¢åŠ ï¼Œç¯å¢ƒä¼šå˜æš—ã€‚å¤§å¤šæ•°ç°æœ‰çš„æ°´ä¸‹å›¾åƒå¢å¼ºæ–¹æ³•ä¸»è¦å…³æ³¨å»å™ªå’Œè‰²å½©è°ƒæ•´ï¼Œå¾ˆå°‘æœ‰å·¥ä½œä¸“æ³¨äºäº®åº¦å¢å¼ºã€‚è¿™é¡¹å·¥ä½œå¼•å…¥äº†ä¸€ç§ä½¿ç”¨æ‰©æ•£æ¨¡å‹è¿›è¡Œæ°´ä¸‹å›¾åƒå¢å¼ºçš„æ–°å‹æ— ç›‘ç£å­¦ä¹ æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç§°ä¸ºUDBEï¼Œå®ƒåŸºäºæ¡ä»¶æ‰©æ•£æ¥ä¿æŒéé…å¯¹è¾“å…¥å›¾åƒçš„äº®åº¦ç»†èŠ‚ã€‚è¾“å…¥å›¾åƒä¸è‰²å›¾å’Œä¿¡å·å™ªå£°å…³ç³»å›¾ï¼ˆSNRï¼‰ç›¸ç»“åˆï¼Œä»¥ç¡®ä¿ç¨³å®šçš„è®­ç»ƒå¹¶é˜²æ­¢è¾“å‡ºå›¾åƒä¸­çš„é¢œè‰²å¤±çœŸã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨UIEBã€SUIMå’ŒRUIEç­‰æ°´ä¸‹å›¾åƒåŸºå‡†æµ‹è¯•é›†ä¸Šå–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„å‡†ç¡®ç‡ã€‚æ­¤å¤–ï¼Œå®éªŒéªŒè¯äº†æˆ‘ä»¬æ–¹æ³•åœ¨å›¾åƒè´¨é‡æŒ‡æ ‡PSNRã€SSIMã€UIQMå’ŒUISMæ–¹é¢çš„ç¨³å¥æ€§ï¼Œè¡¨æ˜äº®åº¦å¢å¼ºè¿‡ç¨‹è¡¨ç°è‰¯å¥½ã€‚æºä»£ç å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/gusanagy/UDBE%E5%A4%84%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/gusanagy/UDBEå¤„è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.16211v1">PDF</a> Paper presented at ICMLA 2024</p>
<p><strong>Summary</strong></p>
<p>æ°´ä¸‹ç¯å¢ƒåœ¨å¤šç§åœºæ™¯ä¸­éƒ½æä¸ºé‡è¦ï¼Œæ¨åŠ¨äº†æ°´ä¸‹å›¾åƒå¢å¼ºæŠ€æœ¯çš„æŒç»­å‘å±•ã€‚åœ¨æ­¤é¢†åŸŸçš„ä¸€å¤§æŒ‘æˆ˜æ˜¯å›¾åƒæ•è·çš„æ·±åº¦ï¼Œéšç€æ·±åº¦çš„å¢åŠ ï¼Œç¯å¢ƒä¼šå˜æš—ã€‚ç›®å‰å¤§å¤šæ•°æ°´ä¸‹å›¾åƒå¢å¼ºæ–¹æ³•ä¸»è¦å…³æ³¨å»å™ªå’Œè‰²å½©è°ƒæ•´ï¼Œå¾ˆå°‘æœ‰å·¥ä½œè‡´åŠ›äºäº®åº¦å¢å¼ºã€‚æœ¬æ–‡å¼•å…¥äº†ä¸€ç§ä½¿ç”¨æ‰©æ•£æ¨¡å‹è¿›è¡Œæ°´ä¸‹å›¾åƒå¢å¼ºçš„æ–°å‹æ— ç›‘ç£å­¦ä¹ æ–¹æ³•ã€‚æ‰€ææ–¹æ³•UDBEåŸºäºæ¡ä»¶æ‰©æ•£ï¼Œä»¥ä¿æŒæœªé…å¯¹è¾“å…¥å›¾åƒçš„äº®åº¦ç»†èŠ‚ã€‚è¾“å…¥å›¾åƒä¸è‰²å½©å›¾å’Œä¿¡å™ªæ¯”å…³ç³»å›¾ï¼ˆSNRï¼‰ç›¸ç»“åˆï¼Œä»¥ç¡®ä¿è®­ç»ƒç¨³å®šå¹¶é˜²æ­¢è¾“å‡ºå›¾åƒå‡ºç°è‰²å½©å¤±çœŸã€‚ç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨UIEBã€SUIMå’ŒRUIEç­‰æ°´ä¸‹å›¾åƒåŸºå‡†æ•°æ®é›†ä¸Šå®ç°äº†è¾ƒé«˜çš„å‡†ç¡®ç‡ã€‚å®éªŒè¿˜éªŒè¯äº†è¯¥æ–¹æ³•åœ¨å›¾åƒè´¨é‡æŒ‡æ ‡PSNRã€SSIMã€UIQMå’ŒUISMæ–¹é¢çš„ç¨³å¥æ€§ï¼Œè¡¨æ˜äº®åº¦å¢å¼ºè¿‡ç¨‹è¡¨ç°è‰¯å¥½ã€‚æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/gusanagy/UDBE">https://github.com/gusanagy/UDBE</a>è·å–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ°´ä¸‹å›¾åƒå¢å¼ºå¯¹äºå¤šç§åº”ç”¨åœºæ™¯è‡³å…³é‡è¦ï¼Œå°¤å…¶éšç€æ°´ä¸‹æ´»åŠ¨çš„ä¸æ–­å¢åŠ ã€‚</li>
<li>å½“å‰æŠ€æœ¯æŒ‘æˆ˜åœ¨äºå¤„ç†ä¸åŒæ°´æ·±å¸¦æ¥çš„ç¯å¢ƒå˜æš—é—®é¢˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨å»å™ªå’Œè‰²å½©è°ƒæ•´ï¼Œè€Œäº®åº¦çš„å¢å¼ºåˆ™ç›¸å¯¹è¢«å¿½è§†ã€‚</li>
<li>è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„æ— ç›‘ç£å­¦ä¹ æ–¹æ³•UDBEï¼ŒåŸºäºæ¡ä»¶æ‰©æ•£æ¨¡å‹è¿›è¡Œæ°´ä¸‹å›¾åƒå¢å¼ºã€‚</li>
<li>UDBEèƒ½å¤Ÿä¿æŒè¾“å…¥å›¾åƒçš„äº®åº¦ç»†èŠ‚ï¼Œç»“åˆè‰²å½©å›¾å’ŒSNRæ¥ç¡®ä¿è®­ç»ƒç¨³å®šæ€§å’Œå›¾åƒè´¨é‡ã€‚</li>
<li>UDBEåœ¨å¤šä¸ªæ°´ä¸‹å›¾åƒåŸºå‡†æ•°æ®é›†ä¸Šå®ç°äº†è¾ƒé«˜çš„å‡†ç¡®ç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.16211">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-75f556dec914c9c3239368a6cabf65ea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ac8b86c7f1d848b6379130169ca4be4d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5767c33ac04b4c6c1892926057c8dc0e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-838d928090b481eb7a2ee1183fccbcdf.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="BAG-Body-Aligned-3D-Wearable-Asset-Generation"><a href="#BAG-Body-Aligned-3D-Wearable-Asset-Generation" class="headerlink" title="BAG: Body-Aligned 3D Wearable Asset Generation"></a>BAG: Body-Aligned 3D Wearable Asset Generation</h2><p><strong>Authors:Zhongjin Luo, Yang Li, Mingrui Zhang, Senbo Wang, Han Yan, Xibin Song, Taizhang Shang, Wei Mao, Hongdong Li, Xiaoguang Han, Pan Ji</strong></p>
<p>While recent advancements have shown remarkable progress in general 3D shape generation models, the challenge of leveraging these approaches to automatically generate wearable 3D assets remains unexplored. To this end, we present BAG, a Body-aligned Asset Generation method to output 3D wearable asset that can be automatically dressed on given 3D human bodies. This is achived by controlling the 3D generation process using human body shape and pose information. Specifically, we first build a general single-image to consistent multiview image diffusion model, and train it on the large Objaverse dataset to achieve diversity and generalizability. Then we train a Controlnet to guide the multiview generator to produce body-aligned multiview images. The control signal utilizes the multiview 2D projections of the target human body, where pixel values represent the XYZ coordinates of the body surface in a canonical space. The body-conditioned multiview diffusion generates body-aligned multiview images, which are then fed into a native 3D diffusion model to produce the 3D shape of the asset. Finally, by recovering the similarity transformation using multiview silhouette supervision and addressing asset-body penetration with physics simulators, the 3D asset can be accurately fitted onto the target human body. Experimental results demonstrate significant advantages over existing methods in terms of image prompt-following capability, shape diversity, and shape quality. Our project page is available at <a target="_blank" rel="noopener" href="https://bag-3d.github.io/">https://bag-3d.github.io/</a>. </p>
<blockquote>
<p>å°½ç®¡æœ€è¿‘çš„è¿›å±•åœ¨é€šç”¨3Då½¢çŠ¶ç”Ÿæˆæ¨¡å‹æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œä½†åˆ©ç”¨è¿™äº›æ–¹æ³•è‡ªåŠ¨ç”Ÿæˆå¯ç©¿æˆ´3Dèµ„äº§çš„æŒ‘æˆ˜ä»ç„¶æœªè¢«æ¢ç´¢ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†BAGï¼ˆBody-aligned Asset Generationï¼‰æ–¹æ³•ï¼Œå®ƒå¯è¾“å‡ºèƒ½å¤Ÿåœ¨ç»™å®š3Däººä½“ä¸Šè‡ªåŠ¨ç©¿æˆ´çš„3Då¯ç©¿æˆ´èµ„äº§ã€‚è¿™æ˜¯é€šè¿‡æ§åˆ¶ä½¿ç”¨äººä½“å½¢çŠ¶å’Œå§¿åŠ¿ä¿¡æ¯çš„3Dç”Ÿæˆè¿‡ç¨‹æ¥å®ç°çš„ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆæ„å»ºäº†ä¸€ä¸ªé€šç”¨çš„å•å›¾åƒåˆ°ä¸€è‡´çš„å¤šè§†å›¾å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œå¹¶åœ¨å¤§å‹Objaverseæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä»¥å®ç°å¤šæ ·æ€§å’Œé€šç”¨æ€§ã€‚ç„¶åï¼Œæˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªControlnetæ¥æŒ‡å¯¼å¤šè§†å›¾ç”Ÿæˆå™¨äº§ç”Ÿä¸èº«ä½“å¯¹é½çš„å¤šè§†å›¾å›¾åƒã€‚æ§åˆ¶ä¿¡å·åˆ©ç”¨ç›®æ ‡äººä½“çš„å¤šè§†å›¾2DæŠ•å½±ï¼Œå…¶ä¸­åƒç´ å€¼ä»£è¡¨æ ‡å‡†ç©ºé—´ä¸­ä½“è¡¨é¢çš„XYZåæ ‡ã€‚ä»¥èº«ä½“ä¸ºæ¡ä»¶çš„å¤šè§†å›¾æ‰©æ•£äº§ç”Ÿä¸èº«ä½“å¯¹é½çš„å¤šè§†å›¾å›¾åƒï¼Œç„¶åå°†å…¶è¾“å…¥åˆ°æœ¬åœ°3Dæ‰©æ•£æ¨¡å‹ä¸­ï¼Œä»¥äº§ç”Ÿèµ„äº§çš„ä¸‰ç»´å½¢çŠ¶ã€‚æœ€åï¼Œé€šè¿‡åˆ©ç”¨å¤šè§†å›¾è½®å»“ç›‘ç£æ¢å¤ç›¸ä¼¼å˜æ¢å¹¶å€ŸåŠ©ç‰©ç†æ¨¡æ‹Ÿå™¨è§£å†³èµ„äº§ä¸èº«ä½“ä¹‹é—´çš„ç©¿é€é—®é¢˜ï¼Œå¯ä»¥å‡†ç¡®åœ°å°†3Dèµ„äº§åŒ¹é…åˆ°ç›®æ ‡äººä½“ä¸Šã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å›¾åƒæç¤ºéµå¾ªèƒ½åŠ›ã€å½¢çŠ¶å¤šæ ·æ€§å’Œå½¢çŠ¶è´¨é‡æ–¹é¢ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢å¯åœ¨<a target="_blank" rel="noopener" href="https://bag-3d.github.ioæ‰¾åˆ°./">https://bag-3d.github.io/æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.16177v1">PDF</a> video: <a target="_blank" rel="noopener" href="https://youtu.be/XJtG82LjQKc">https://youtu.be/XJtG82LjQKc</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºBAGçš„è‡ªåŠ¨ç”Ÿæˆå¯ç©¿æˆ´3Dèµ„äº§çš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•åˆ©ç”¨äººä½“å½¢æ€å’Œå§¿æ€ä¿¡æ¯æ§åˆ¶3Dç”Ÿæˆè¿‡ç¨‹ï¼Œå®ç°è‡ªåŠ¨å°†ç»™å®šçš„3Däººä½“ç©¿ä¸Šæœè£…çš„æ•ˆæœã€‚é€šè¿‡æ„å»ºå•å›¾åƒåˆ°ä¸€è‡´çš„å¤šè§†è§’å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œå¹¶åœ¨Objaverseæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå®ç°äº†å¤šæ ·æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚è®­ç»ƒæ§åˆ¶ç½‘ç»œControlnetæ¥æŒ‡å¯¼å¤šè§†è§’ç”Ÿæˆå™¨ç”Ÿæˆä¸äººä½“å¯¹é½çš„å¤šè§†è§’å›¾åƒï¼Œç„¶åé€šè¿‡3Dæ‰©æ•£æ¨¡å‹ç”Ÿæˆèµ„äº§çš„ä¸‰ç»´å½¢çŠ¶ã€‚æœ€åï¼Œé€šè¿‡å¤šè§†è§’è½®å»“ç›‘ç£æ¢å¤ç›¸ä¼¼å˜æ¢å¹¶è§£å†³èµ„äº§ä¸äººä½“ä¹‹é—´çš„ç©¿é€é—®é¢˜ï¼Œä½¿ç”¨ç‰©ç†æ¨¡æ‹Ÿå™¨å‡†ç¡®åœ°å°†3Dèµ„äº§é€‚é…åˆ°ç›®æ ‡äººä½“ä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>BAGæ–¹æ³•å®ç°äº†è‡ªåŠ¨ç”Ÿæˆå¯ç©¿æˆ´3Dèµ„äº§çš„èƒ½åŠ›ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•çš„æŒ‘æˆ˜ã€‚</li>
<li>é€šè¿‡æ„å»ºå•å›¾åƒåˆ°å¤šè§†è§’å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œå®ç°äº†å¤šæ ·æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>æ§åˆ¶ç½‘ç»œControlnetç”¨äºæŒ‡å¯¼å¤šè§†è§’ç”Ÿæˆå™¨äº§ç”Ÿä¸äººä½“å¯¹é½çš„å›¾åƒã€‚</li>
<li>åˆ©ç”¨äººä½“å½¢æ€å’Œå§¿æ€ä¿¡æ¯çš„æ§åˆ¶ä¿¡å·æŒ‡å¯¼æ‰©æ•£è¿‡ç¨‹ï¼Œç”Ÿæˆä¸äººä½“å¯¹é½çš„å¤šè§†è§’å›¾åƒã€‚</li>
<li>é€šè¿‡å¤šè§†è§’è½®å»“ç›‘ç£æ¢å¤ç›¸ä¼¼å˜æ¢ï¼Œè§£å†³äº†èµ„äº§ä¸äººä½“ä¹‹é—´çš„ç©¿é€é—®é¢˜ã€‚</li>
<li>ä½¿ç”¨ç‰©ç†æ¨¡æ‹Ÿå™¨å°†3Dèµ„äº§å‡†ç¡®é€‚é…åˆ°ç›®æ ‡äººä½“ä¸Šã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒBAGæ–¹æ³•åœ¨å›¾åƒæç¤ºéµå¾ªèƒ½åŠ›ã€å½¢çŠ¶å¤šæ ·æ€§å’Œå½¢çŠ¶è´¨é‡æ–¹é¢å‡æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.16177">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4677ea409dc4b99921866e845cd10320.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3791c8a708880ef14f6366754671c2cf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-213c9ba104018c1cf5ad0cff40c58f36.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-17678ba83fe412c8a93cb9b40c6b52e0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6a65626cfab76fdb49d38ef6c082022e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-755cbb59ef4acbe2a718644dff98c5d4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bc735c5b5344a1b0e7708bc001c09c68.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Efficient-Portrait-Matte-Creation-With-Layer-Diffusion-and-Connectivity-Priors"><a href="#Efficient-Portrait-Matte-Creation-With-Layer-Diffusion-and-Connectivity-Priors" class="headerlink" title="Efficient Portrait Matte Creation With Layer Diffusion and Connectivity   Priors"></a>Efficient Portrait Matte Creation With Layer Diffusion and Connectivity   Priors</h2><p><strong>Authors:Zhiyuan Lu, Hao Lu, Hua Huang</strong></p>
<p>Learning effective deep portrait matting models requires training data of both high quality and large quantity. Neither quality nor quantity can be easily met for portrait matting, however. Since the most accurate ground-truth portrait mattes are acquired in front of the green screen, it is almost impossible to harvest a large-scale portrait matting dataset in reality. This work shows that one can leverage text prompts and the recent Layer Diffusion model to generate high-quality portrait foregrounds and extract latent portrait mattes. However, the portrait mattes cannot be readily in use due to significant generation artifacts. Inspired by the connectivity priors observed in portrait images, that is, the border of portrait foregrounds always appears connected, a connectivity-aware approach is introduced to refine portrait mattes. Building on this, a large-scale portrait matting dataset is created, termed LD-Portrait-20K, with $20,051$ portrait foregrounds and high-quality alpha mattes. Extensive experiments demonstrated the value of the LD-Portrait-20K dataset, with models trained on it significantly outperforming those trained on other datasets. In addition, comparisons with the chroma keying algorithm and an ablation study on dataset capacity further confirmed the effectiveness of the proposed matte creation approach. Further, the dataset also contributes to state-of-the-art video portrait matting, implemented by simple video segmentation and a trimap-based image matting model trained on this dataset. </p>
<blockquote>
<p>å­¦ä¹ æœ‰æ•ˆçš„æ·±åº¦è‚–åƒæŠ å›¾æ¨¡å‹éœ€è¦é«˜è´¨é‡ä¸”å¤§é‡çš„è®­ç»ƒæ•°æ®ã€‚ç„¶è€Œï¼Œè‚–åƒæŠ å›¾å¾ˆéš¾åŒæ—¶æ»¡è¶³è´¨é‡å’Œæ•°é‡çš„è¦æ±‚ã€‚ç”±äºæœ€å‡†ç¡®çš„åœ°é¢çœŸå®è‚–åƒæŠ å›¾æ˜¯æ˜¯åœ¨ç»¿å¹•å‰è·å–çš„ï¼Œå› æ­¤åœ¨ç°å®ä¸­å‡ ä¹ä¸å¯èƒ½æ”¶é›†å¤§è§„æ¨¡çš„è‚–åƒæŠ å›¾æ•°æ®é›†ã€‚è¿™é¡¹å·¥ä½œè¡¨æ˜ï¼Œå¯ä»¥åˆ©ç”¨æ–‡æœ¬æç¤ºå’Œæœ€æ–°çš„å±‚æ‰©æ•£æ¨¡å‹ç”Ÿæˆé«˜è´¨é‡çš„è‚–åƒå‰æ™¯å¹¶æå–æ½œåœ¨è‚–åƒæŠ å›¾ã€‚ç„¶è€Œï¼Œç”±äºå­˜åœ¨æ˜æ˜¾çš„ç”Ÿæˆç‘•ç–µï¼Œè¿™äº›è‚–åƒæŠ å›¾ä¸èƒ½ç›´æ¥ä½¿ç”¨ã€‚å—è‚–åƒå›¾åƒä¸­è§‚å¯Ÿåˆ°çš„è¿é€šæ€§å…ˆéªŒçš„å¯å‘ï¼Œå³è‚–åƒå‰æ™¯çš„è¾¹ç•Œæ€»æ˜¯ç›¸äº’è¿æ¥çš„ï¼Œå¼•å…¥äº†ä¸€ç§è¿é€šæ€§æ„ŸçŸ¥æ–¹æ³•æ¥ä¼˜åŒ–è‚–åƒæŠ å›¾ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œåˆ›å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡çš„è‚–åƒæŠ å›¾æ•°æ®é›†ï¼Œç§°ä¸ºLD-Portrait-20Kï¼ŒåŒ…å«20,051ä¸ªè‚–åƒå‰æ™¯å’Œé«˜è´¨é‡alphaæŠ å›¾ã€‚å¤§é‡å®éªŒè¯æ˜äº†LD-Portrait-20Kæ•°æ®é›†çš„ä»·å€¼ï¼Œåœ¨æ­¤æ•°æ®é›†ä¸Šè®­ç»ƒçš„æ¨¡å‹æ˜¾è‘—ä¼˜äºåœ¨å…¶ä»–æ•°æ®é›†ä¸Šè®­ç»ƒçš„æ¨¡å‹ã€‚æ­¤å¤–ï¼Œä¸è‰²é”®ç®—æ³•çš„æ¯”è¾ƒä»¥åŠå¯¹æ•°æ®é›†å®¹é‡çš„æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥è¯å®äº†æ‰€æå‡ºçš„æŠ å›¾åˆ›å»ºæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ•°æ®é›†è¿˜ä¸ºå…ˆè¿›çš„è§†é¢‘è‚–åƒæŠ å›¾åšå‡ºäº†è´¡çŒ®ï¼Œé€šè¿‡ç®€å•çš„è§†é¢‘åˆ†å‰²å’ŒåŸºäºæ­¤æ•°æ®é›†è®­ç»ƒçš„åŸºäºä¿®è¾¹å›¾çš„å›¾åƒæŠ å›¾æ¨¡å‹å®ç°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.16147v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æ·±åº¦è‚–åƒæŠ å›¾æ¨¡å‹çš„å­¦ä¹ éœ€æ±‚åŠæŒ‘æˆ˜ã€‚ä¸ºè§£å†³é«˜è´¨é‡å¤§è§„æ¨¡è®­ç»ƒæ•°æ®çš„è·å–éš¾é¢˜ï¼Œç ”ç©¶åˆ©ç”¨æ–‡æœ¬æç¤ºå’ŒLayer Diffusionæ¨¡å‹ç”Ÿæˆé«˜è´¨é‡è‚–åƒå‰æ™¯ï¼Œå¹¶å¼•å…¥è¿æ¥æ€§æ„ŸçŸ¥æ–¹æ³•ä¼˜åŒ–ç”Ÿæˆçš„è‚–åƒæŠ å›¾ã€‚ä¸ºæ­¤åˆ›å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡è‚–åƒæŠ å›¾æ•°æ®é›†LD-Portrait-20Kï¼Œå®éªŒè¯æ˜è¯¥æ•°æ®é›†çš„æœ‰æ•ˆæ€§ï¼Œå¹¶åœ¨è§†é¢‘è‚–åƒæŠ å›¾ä¸­å¾—åˆ°åº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦è‚–åƒæŠ å›¾æ¨¡å‹éœ€è¦é«˜è´¨é‡å¤§è§„æ¨¡çš„è®­ç»ƒæ•°æ®ã€‚</li>
<li>ç”±äºç°å®æ¡ä»¶ä¸‹è·å–å‡†ç¡®åœ°é¢çœŸå®è‚–åƒæŠ å›¾æ•°æ®çš„å›°éš¾ï¼Œç ”ç©¶åˆ©ç”¨æ–‡æœ¬æç¤ºå’ŒLayer Diffusionæ¨¡å‹ç”Ÿæˆé«˜è´¨é‡è‚–åƒå‰æ™¯ã€‚</li>
<li>ç”Ÿæˆçš„è‚–åƒæŠ å›¾å­˜åœ¨ç”Ÿæˆç‘•ç–µï¼Œå¼•å…¥è¿æ¥æ€§æ„ŸçŸ¥æ–¹æ³•æ¥ä¼˜åŒ–ã€‚</li>
<li>åˆ›å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡çš„è‚–åƒæŠ å›¾æ•°æ®é›†LD-Portrait-20Kï¼ŒåŒ…å«20,051ä¸ªè‚–åƒå‰æ™¯å’Œé«˜è´¨é‡alphaæŠ å›¾ã€‚</li>
<li>å®éªŒè¯æ˜LD-Portrait-20Kæ•°æ®é›†çš„æœ‰æ•ˆæ€§ï¼Œè®­ç»ƒåœ¨æ­¤æ•°æ®é›†ä¸Šçš„æ¨¡å‹æ€§èƒ½æ˜¾è‘—ä¼˜äºå…¶ä»–æ•°æ®é›†ã€‚</li>
<li>ä¸è‰²é”®ç®—æ³•çš„æ¯”è¾ƒåŠæ•°æ®é›†å®¹é‡çš„æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥è¯å®äº†æ‰€æå‡ºçš„æŠ å›¾åˆ›å»ºæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.16147">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0c8af10f4be01f776f500ebce06a8701.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7c08d7e1fa3ab3f6635a8e5bca6aa1bf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c73315e1923de811f5e590a38749a33f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-11dfb51d158677ba34c22b0cd445508f.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="MatCLIP-Light-and-Shape-Insensitive-Assignment-of-PBR-Material-Models"><a href="#MatCLIP-Light-and-Shape-Insensitive-Assignment-of-PBR-Material-Models" class="headerlink" title="MatCLIP: Light- and Shape-Insensitive Assignment of PBR Material Models"></a>MatCLIP: Light- and Shape-Insensitive Assignment of PBR Material Models</h2><p><strong>Authors:Michael Birsak, John Femiani, Biao Zhang, Peter Wonka</strong></p>
<p>Assigning realistic materials to 3D models remains a significant challenge in computer graphics. We propose MatCLIP, a novel method that extracts shape- and lighting-insensitive descriptors of Physically Based Rendering (PBR) materials to assign plausible textures to 3D objects based on images, such as the output of Latent Diffusion Models (LDMs) or photographs. Matching PBR materials to static images is challenging because the PBR representation captures the dynamic appearance of materials under varying viewing angles, shapes, and lighting conditions. By extending an Alpha-CLIP-based model on material renderings across diverse shapes and lighting, and encoding multiple viewing conditions for PBR materials, our approach generates descriptors that bridge the domains of PBR representations with photographs or renderings, including LDM outputs. This enables consistent material assignments without requiring explicit knowledge of material relationships between different parts of an object. MatCLIP achieves a top-1 classification accuracy of 76.6%, outperforming state-of-the-art methods such as PhotoShape and MatAtlas by over 15 percentage points on publicly available datasets. Our method can be used to construct material assignments for 3D shape datasets such as ShapeNet, 3DCoMPaT++, and Objaverse. All code and data will be released. </p>
<blockquote>
<p>ä¸ºä¸‰ç»´æ¨¡å‹åˆ†é…ç°å®ææ–™ä»ç„¶æ˜¯è®¡ç®—æœºå›¾å½¢å­¦ä¸­çš„ä¸€é¡¹é‡å¤§æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºMatCLIPçš„æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•æå–åŸºäºç‰©ç†æ¸²æŸ“ï¼ˆPBRï¼‰ææ–™çš„å½¢çŠ¶å’Œå…‰ç…§ä¸æ•æ„Ÿæè¿°ç¬¦ï¼Œæ ¹æ®å›¾åƒï¼ˆå¦‚æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰çš„è¾“å‡ºæˆ–ç…§ç‰‡ï¼‰ä¸ºä¸‰ç»´å¯¹è±¡åˆ†é…åˆç†çš„çº¹ç†ã€‚å°†PBRææ–™ä¸é™æ€å›¾åƒç›¸åŒ¹é…æ˜¯å…·æœ‰æŒ‘æˆ˜æ€§çš„ï¼Œå› ä¸ºPBRè¡¨ç¤ºæ•æ‰äº†åœ¨å„ç§è§‚çœ‹è§’åº¦ã€å½¢çŠ¶å’Œå…‰ç…§æ¡ä»¶ä¸‹ææ–™çš„åŠ¨æ€å¤–è§‚ã€‚æˆ‘ä»¬é€šè¿‡æ‰©å±•åŸºäºAlpha-CLIPçš„æ¨¡å‹ï¼Œåœ¨å¤šç§å½¢çŠ¶å’Œå…‰ç…§ä¸‹è¿›è¡Œææ–™æ¸²æŸ“ï¼Œå¹¶å¯¹PBRææ–™è¿›è¡Œå¤šè§†è§’ç¼–ç ï¼Œäº§ç”Ÿæè¿°ç¬¦ï¼Œæ¶èµ·PBRè¡¨ç¤ºä¸ç…§ç‰‡æˆ–æ¸²æŸ“å›¾åƒï¼ˆåŒ…æ‹¬LDMè¾“å‡ºï¼‰ä¹‹é—´çš„æ¡¥æ¢ã€‚è¿™å®ç°äº†ææ–™åˆ†é…çš„ä¸€è‡´æ€§ï¼Œæ— éœ€æ˜ç¡®å¯¹è±¡ä¸åŒéƒ¨åˆ†ä¹‹é—´çš„ææ–™å…³ç³»çŸ¥è¯†ã€‚MatCLIPåœ¨å…¬å¼€æ•°æ®é›†ä¸Šå–å¾—äº†76.6%çš„top-1åˆ†ç±»å‡†ç¡®ç‡ï¼Œç›¸è¾ƒäºå½“å‰çš„ä¸»æµæ–¹æ³•PhotoShapeå’ŒMatAtlasåœ¨å…¬å¼€æ•°æ®é›†ä¸Šçš„å‡†ç¡®ç‡æœ‰è¶…è¿‡15ä¸ªç™¾åˆ†ç‚¹çš„æå‡ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¯ç”¨äºæ„å»ºå¦‚ShapeNetã€3DCoMPaT++å’ŒObjaverseç­‰ä¸‰ç»´å½¢çŠ¶æ•°æ®é›†çš„ææ–™åˆ†é…ã€‚æ‰€æœ‰ä»£ç å’Œæ•°æ®éƒ½å°†å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.15981v1">PDF</a> Preprint, 10 pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºMatCLIPçš„æ–°æ–¹æ³•ï¼Œç”¨äºä¸ºä¸‰ç»´æ¨¡å‹åˆ†é…çœŸå®ææ–™ã€‚è¯¥æ–¹æ³•æå–ç‰©ç†åŸºç¡€æ¸²æŸ“ï¼ˆPBRï¼‰ææ–™çš„å½¢çŠ¶å’Œå…‰ç…§ä¸æ•æ„Ÿæè¿°ç¬¦ï¼Œæ ¹æ®å›¾åƒï¼ˆå¦‚æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰çš„è¾“å‡ºæˆ–ç…§ç‰‡ï¼‰ä¸º3Då¯¹è±¡åˆ†é…åˆç†çš„çº¹ç†ã€‚MatCLIPé€šè¿‡æ‰©å±•Alpha-CLIPæ¨¡å‹åœ¨ææ–™æ¸²æŸ“æ–¹é¢çš„åº”ç”¨ï¼Œå¹¶ç¼–ç PBRææ–™çš„å¤šè§†è§’æ¡ä»¶ï¼Œç”Ÿæˆäº†è¿æ¥PBRè¡¨ç¤ºä¸ç…§ç‰‡æˆ–æ¸²æŸ“çš„å›¾åƒæè¿°ç¬¦ã€‚è¿™ç§æ–¹æ³•å¯ä»¥åœ¨ä¸éœ€è¦æ˜ç¡®äº†è§£å¯¹è±¡å„éƒ¨åˆ†ææ–™å…³ç³»çš„æƒ…å†µä¸‹ï¼Œå®ç°ä¸€è‡´çš„ææ–™åˆ†é…ï¼Œå‡†ç¡®ç‡è¾¾åˆ°äº†76.6%ï¼Œåœ¨å…¬å¼€æ•°æ®é›†ä¸Šè¾ƒå…¶ä»–å…ˆè¿›æ–¹æ³•å¦‚PhotoShapeå’ŒMatAtlasé«˜å‡ºè¶…è¿‡15ä¸ªç™¾åˆ†ç‚¹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MatCLIPæ˜¯ä¸€ç§æ–°å‹æ–¹æ³•ï¼Œç”¨äºä¸ºä¸‰ç»´æ¨¡å‹åˆ†é…çœŸå®ææ–™ï¼Œç‰¹åˆ«é€‚ç”¨äºç‰©ç†åŸºç¡€æ¸²æŸ“ï¼ˆPBRï¼‰ææ–™ã€‚</li>
<li>MatCLIPé€šè¿‡æå–å½¢çŠ¶å’Œå…‰ç…§ä¸æ•æ„Ÿæè¿°ç¬¦ï¼Œæ ¹æ®å›¾åƒä¸º3Då¯¹è±¡åˆ†é…çº¹ç†ã€‚</li>
<li>MatCLIPåˆ©ç”¨Alpha-CLIPæ¨¡å‹å¤„ç†ææ–™æ¸²æŸ“ï¼Œå¹¶ç¼–ç å¤šç§è§†è§’æ¡ä»¶å’Œå…‰ç…§æ¡ä»¶ä¸‹çš„PBRææ–™ã€‚</li>
<li>MatCLIPç”Ÿæˆå›¾åƒæè¿°ç¬¦ï¼Œå¯è¿æ¥PBRè¡¨ç¤ºä¸ç…§ç‰‡æˆ–æ¸²æŸ“çš„å›¾åƒã€‚</li>
<li>MatCLIPå®ç°äº†åœ¨ä¸äº†è§£å¯¹è±¡å„éƒ¨åˆ†ææ–™å…³ç³»çš„æƒ…å†µä¸‹çš„ä¸€è‡´ææ–™åˆ†é…ã€‚</li>
<li>MatCLIPçš„å‡†ç¡®ç‡è¾¾åˆ°äº†76.6%ï¼Œåœ¨å…¬å¼€æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–å…ˆè¿›æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.15981">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-206c36eaf8fa845cd87d86d91a56b8f1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f038661c5788df6595b9a2a04da5d393.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5fa1a001e19e24666468e4ded88fe119.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4f60e85a153ae619fac25226a0fa1c77.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-113861cea2fee3d09a8d1c82cb7b8935.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Slot-Guided-Adaptation-of-Pre-trained-Diffusion-Models-for-Object-Centric-Learning-and-Compositional-Generation"><a href="#Slot-Guided-Adaptation-of-Pre-trained-Diffusion-Models-for-Object-Centric-Learning-and-Compositional-Generation" class="headerlink" title="Slot-Guided Adaptation of Pre-trained Diffusion Models for   Object-Centric Learning and Compositional Generation"></a>Slot-Guided Adaptation of Pre-trained Diffusion Models for   Object-Centric Learning and Compositional Generation</h2><p><strong>Authors:Adil Kaan Akan, Yucel Yemez</strong></p>
<p>We present SlotAdapt, an object-centric learning method that combines slot attention with pretrained diffusion models by introducing adapters for slot-based conditioning. Our method preserves the generative power of pretrained diffusion models, while avoiding their text-centric conditioning bias. We also incorporate an additional guidance loss into our architecture to align cross-attention from adapter layers with slot attention. This enhances the alignment of our model with the objects in the input image without using external supervision. Experimental results show that our method outperforms state-of-the-art techniques in object discovery and image generation tasks across multiple datasets, including those with real images. Furthermore, we demonstrate through experiments that our method performs remarkably well on complex real-world images for compositional generation, in contrast to other slot-based generative methods in the literature. The project page can be found at $\href{<a target="_blank" rel="noopener" href="https://kaanakan.github.io/SlotAdapt/%7D%7B/text%7Bthis">https://kaanakan.github.io/SlotAdapt/}{\text{this</a> https url}}$. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†SlotAdaptï¼Œè¿™æ˜¯ä¸€ç§ç»“åˆæ§½ä½æ³¨æ„åŠ›å’Œé¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„é¢å‘å¯¹è±¡çš„å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥é€‚é…å™¨æ¥å®ç°åŸºäºæ§½ä½çš„æ¡ä»¶è®¾ç½®ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¿ç•™äº†é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ï¼ŒåŒæ—¶é¿å…äº†å®ƒä»¬åŸºäºæ–‡æœ¬çš„æ¡ä»¶è®¾ç½®çš„åè§ã€‚æˆ‘ä»¬è¿˜åœ¨æ¶æ„ä¸­åŠ å…¥äº†é¢å¤–çš„æŒ‡å¯¼æŸå¤±ï¼Œä»¥è°ƒæ•´é€‚é…å™¨å±‚çš„äº¤å‰æ³¨æ„åŠ›ä¸æ§½ä½æ³¨æ„åŠ›çš„å¯¹é½ã€‚è¿™å¢å¼ºäº†æˆ‘ä»¬çš„æ¨¡å‹ä¸è¾“å…¥å›¾åƒä¸­çš„å¯¹è±¡çš„å¯¹é½ï¼Œè€Œæ— éœ€ä½¿ç”¨å¤–éƒ¨ç›‘ç£ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å¯¹è±¡å‘ç°å’Œå›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šä¼˜äºæœ€æ–°æŠ€æœ¯ï¼ŒåŒ…æ‹¬çœŸå®å›¾åƒæ•°æ®é›†ã€‚æ­¤å¤–ï¼Œé€šè¿‡å®éªŒè¯æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤æ‚çš„çœŸå®å›¾åƒä¸Šçš„ç»„åˆç”Ÿæˆè¡¨ç°å°¤ä¸ºå‡ºè‰²ï¼Œä¸æ–‡çŒ®ä¸­çš„å…¶ä»–åŸºäºæ§½ä½çš„ç”Ÿæˆæ–¹æ³•å½¢æˆé²œæ˜å¯¹æ¯”ã€‚é¡¹ç›®é¡µé¢å¯åœ¨æ­¤å¤„æ‰¾åˆ°ï¼š[<a target="_blank" rel="noopener" href="https://kaanakan.github.io/SlotAdapt/]">https://kaanakan.github.io/SlotAdapt/]</a> ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.15878v1">PDF</a> Accepted to ICLR2025.   $\href{<a target="_blank" rel="noopener" href="https://kaanakan.github.io/SlotAdapt/%7D%7B/text%7BProject">https://kaanakan.github.io/SlotAdapt/}{\text{Project</a> Page}}$</p>
<p><strong>Summary</strong></p>
<p>SlotAdaptæ˜¯ä¸€ç§ç»“åˆæ’æ§½æ³¨æ„åŠ›å’Œé¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„å¯¹è±¡çº§å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥é€‚é…å™¨å®ç°æ’æ§½å¼æ¡ä»¶ã€‚è¯¥æ–¹æ³•ä¿ç•™äº†é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ï¼ŒåŒæ—¶é¿å…äº†å…¶æ–‡æœ¬ä¸ºä¸­å¿ƒçš„æ¡ä»¶åè§ã€‚é€šè¿‡å¼•å…¥é¢å¤–çš„æŒ‡å¯¼æŸå¤±ï¼Œå¢å¼ºäº†æ¨¡å‹ä¸è¾“å…¥å›¾åƒä¸­å¯¹è±¡çš„å¯¹é½åº¦ï¼Œæ— éœ€å¤–éƒ¨ç›‘ç£ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„ç›®æ ‡æ£€æµ‹å’Œå›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šå‡ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œå¹¶ä¸”åœ¨å¤æ‚çœŸå®å›¾åƒä¸Šçš„ç»„åˆç”Ÿæˆè¡¨ç°å°¤ä¸ºå‡ºè‰²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SlotAdaptç»“åˆäº†æ’æ§½æ³¨æ„åŠ›å’Œé¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡å¼•å…¥é€‚é…å™¨å®ç°åŸºäºæ’æ§½çš„æ¡ä»¶å­¦ä¹ ã€‚</li>
<li>è¯¥æ–¹æ³•ä¿ç•™äº†é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ï¼Œå¹¶é¿å…äº†æ–‡æœ¬ä¸ºä¸­å¿ƒçš„æ¡ä»¶åè§ã€‚</li>
<li>é€šè¿‡å¼•å…¥é¢å¤–çš„æŒ‡å¯¼æŸå¤±ï¼Œå¢å¼ºäº†æ¨¡å‹ä¸è¾“å…¥å›¾åƒä¸­å¯¹è±¡çš„å¯¹é½ç²¾åº¦ã€‚</li>
<li>SlotAdaptåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„ç›®æ ‡æ£€æµ‹å’Œå›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å¤æ‚çœŸå®å›¾åƒä¸Šçš„ç»„åˆç”Ÿæˆè¡¨ç°å°¤ä¸ºå‡ºè‰²ã€‚</li>
<li>æ­¤æ–¹æ³•å®ç°äº†åœ¨ä¸ä½¿ç”¨å¤–éƒ¨ç›‘ç£çš„æƒ…å†µä¸‹æé«˜æ¨¡å‹ä¸å¯¹è±¡å¯¹é½çš„èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.15878">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-6f29fa25e480904a8126f41bf31bd772.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-05a8b9e8034e74bb394fb6eb8b24573d.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Can-Location-Embeddings-Enhance-Super-Resolution-of-Satellite-Imagery"><a href="#Can-Location-Embeddings-Enhance-Super-Resolution-of-Satellite-Imagery" class="headerlink" title="Can Location Embeddings Enhance Super-Resolution of Satellite Imagery?"></a>Can Location Embeddings Enhance Super-Resolution of Satellite Imagery?</h2><p><strong>Authors:Daniel Panangian, Ksenia Bittner</strong></p>
<p>Publicly available satellite imagery, such as Sentinel- 2, often lacks the spatial resolution required for accurate analysis of remote sensing tasks including urban planning and disaster response. Current super-resolution techniques are typically trained on limited datasets, leading to poor generalization across diverse geographic regions. In this work, we propose a novel super-resolution framework that enhances generalization by incorporating geographic context through location embeddings. Our framework employs Generative Adversarial Networks (GANs) and incorporates techniques from diffusion models to enhance image quality. Furthermore, we address tiling artifacts by integrating information from neighboring images, enabling the generation of seamless, high-resolution outputs. We demonstrate the effectiveness of our method on the building segmentation task, showing significant improvements over state-of-the-art methods and highlighting its potential for real-world applications. </p>
<blockquote>
<p>å…¬å¼€å¯ç”¨çš„å«æ˜Ÿå›¾åƒï¼Œå¦‚Sentinel-2ï¼Œé€šå¸¸ç¼ºä¹è¿›è¡ŒåŸå¸‚è§„åˆ’ã€ç¾å®³åº”å¯¹ç­‰é¥æ„Ÿä»»åŠ¡ç²¾ç¡®åˆ†ææ‰€éœ€çš„ç©ºé—´åˆ†è¾¨ç‡ã€‚å½“å‰çš„è¶…åˆ†è¾¨ç‡æŠ€æœ¯é€šå¸¸å±€é™äºæœ‰é™çš„æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œå¯¼è‡´åœ¨ä¸åŒåœ°ç†åŒºåŸŸçš„æ³›åŒ–èƒ½åŠ›è¾ƒå·®ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹è¶…åˆ†è¾¨ç‡æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡å¼•å…¥ä½ç½®åµŒå…¥æ¥æå‡å¯¹åœ°ç†ä¸Šä¸‹æ–‡çš„åˆ©ç”¨ï¼Œä»è€Œå¢å¼ºæ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ¡†æ¶é‡‡ç”¨ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰ï¼Œå¹¶ç»“åˆæ‰©æ•£æ¨¡å‹çš„æŠ€å·§æ¥æå‡å›¾åƒè´¨é‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡å¯¹é‚»è¿‘å›¾åƒçš„ä¿¡æ¯è¿›è¡Œæ•´åˆæ¥è§£å†³æ‹¼è´´ç—•è¿¹é—®é¢˜ï¼Œä»è€Œç”Ÿæˆæ— ç¼ã€é«˜åˆ†è¾¨ç‡çš„è¾“å‡ºã€‚æˆ‘ä»¬åœ¨å»ºç­‘åˆ†å‰²ä»»åŠ¡ä¸Šå±•ç¤ºäº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œè¯æ˜äº†å…¶ç›¸è¾ƒäºæœ€å…ˆè¿›çš„æ–¹æ³•æœ‰ç€æ˜¾è‘—çš„æå‡ï¼Œå¹¶å¼ºè°ƒäº†å…¶åœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.15847v1">PDF</a> Accepted to IEEE&#x2F;CVF Winter Conference on Applications of Computer   Vision (WACV)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹çš„è¶…åˆ†è¾¨ç‡æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆåœ°ç†ä¸Šä¸‹æ–‡é€šè¿‡ä½ç½®åµŒå…¥å¢å¼ºå›¾åƒè´¨é‡ï¼Œå¹¶é‡‡ç”¨ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰å’Œæ‰©æ•£æ¨¡å‹æŠ€æœ¯æé«˜å›¾åƒè´¨é‡ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶è§£å†³äº†æ‹¼æ¥äº§ç”Ÿçš„ä¼ªå½±é—®é¢˜ï¼Œé€šè¿‡æ•´åˆç›¸é‚»å›¾åƒçš„ä¿¡æ¯ç”Ÿæˆæ— ç¼ã€é«˜åˆ†è¾¨ç‡çš„è¾“å‡ºã€‚åœ¨å»ºç­‘åˆ†å‰²ä»»åŠ¡ä¸ŠéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå±•ç°äº†å…¶åœ¨çœŸå®ä¸–ç•Œåº”ç”¨ä¸­çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºçš„æ–°å‹è¶…åˆ†è¾¨ç‡æ¡†æ¶ç»“åˆåœ°ç†ä¸Šä¸‹æ–‡é€šè¿‡ä½ç½®åµŒå…¥å¢å¼ºå›¾åƒè´¨é‡ã€‚</li>
<li>åˆ©ç”¨ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰å’Œæ‰©æ•£æ¨¡å‹æŠ€æœ¯æé«˜å›¾åƒè´¨é‡ã€‚</li>
<li>è§£å†³äº†æ‹¼æ¥äº§ç”Ÿçš„ä¼ªå½±é—®é¢˜ã€‚</li>
<li>é€šè¿‡æ•´åˆç›¸é‚»å›¾åƒä¿¡æ¯ç”Ÿæˆæ— ç¼ã€é«˜åˆ†è¾¨ç‡çš„è¾“å‡ºã€‚</li>
<li>åœ¨å»ºç­‘åˆ†å‰²ä»»åŠ¡ä¸ŠéªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”æœ‰æ˜¾è‘—æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.15847">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b29432ffbd21e2744991a28cc165ad9e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-557fd575c806a577ff9095ae6b519170.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b14916c16d3d32298e69c2323e02fc21.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4b1919cb1b16c24a870540fda20c0312.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b766eac585b1fe425eabd4dfb6a861b6.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="StochSync-Stochastic-Diffusion-Synchronization-for-Image-Generation-in-Arbitrary-Spaces"><a href="#StochSync-Stochastic-Diffusion-Synchronization-for-Image-Generation-in-Arbitrary-Spaces" class="headerlink" title="StochSync: Stochastic Diffusion Synchronization for Image Generation in   Arbitrary Spaces"></a>StochSync: Stochastic Diffusion Synchronization for Image Generation in   Arbitrary Spaces</h2><p><strong>Authors:Kyeongmin Yeo, Jaihoon Kim, Minhyuk Sung</strong></p>
<p>We propose a zero-shot method for generating images in arbitrary spaces (e.g., a sphere for 360{\deg} panoramas and a mesh surface for texture) using a pretrained image diffusion model. The zero-shot generation of various visual content using a pretrained image diffusion model has been explored mainly in two directions. First, Diffusion Synchronization-performing reverse diffusion processes jointly across different projected spaces while synchronizing them in the target space-generates high-quality outputs when enough conditioning is provided, but it struggles in its absence. Second, Score Distillation Sampling-gradually updating the target space data through gradient descent-results in better coherence but often lacks detail. In this paper, we reveal for the first time the interconnection between these two methods while highlighting their differences. To this end, we propose StochSync, a novel approach that combines the strengths of both, enabling effective performance with weak conditioning. Our experiments demonstrate that StochSync provides the best performance in 360{\deg} panorama generation (where image conditioning is not given), outperforming previous finetuning-based methods, and also delivers comparable results in 3D mesh texturing (where depth conditioning is provided) with previous methods. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºé¢„è®­ç»ƒå›¾åƒæ‰©æ•£æ¨¡å‹çš„é›¶æ ·æœ¬æ–¹æ³•åœ¨ä»»æ„ç©ºé—´ï¼ˆä¾‹å¦‚360Â°å…¨æ™¯çš„çƒä½“æˆ–çº¹ç†çš„ç½‘æ ¼è¡¨é¢ï¼‰ç”Ÿæˆå›¾åƒã€‚ä½¿ç”¨é¢„è®­ç»ƒçš„å›¾åƒæ‰©æ•£æ¨¡å‹è¿›è¡Œå„ç§è§†è§‰å†…å®¹çš„é›¶æ ·æœ¬ç”Ÿæˆä¸»è¦æ¢ç´¢äº†ä¸¤ä¸ªæ–¹å‘ã€‚é¦–å…ˆï¼Œæ‰©æ•£åŒæ­¥æ³•æ˜¯åœ¨ä¸åŒçš„æŠ•å½±ç©ºé—´ä¸Šè”åˆæ‰§è¡Œåå‘æ‰©æ•£è¿‡ç¨‹ï¼ŒåŒæ—¶åœ¨ç›®æ ‡ç©ºé—´è¿›è¡ŒåŒæ­¥ï¼Œå½“æä¾›è¶³å¤Ÿçš„æ¡ä»¶æ—¶ï¼Œå¯ä»¥ç”Ÿæˆé«˜è´¨é‡çš„ç»“æœï¼Œä½†åœ¨ç¼ºä¹æ¡ä»¶æ—¶ï¼Œè¯¥æ–¹æ³•ä¼šé‡åˆ°å›°éš¾ã€‚å…¶æ¬¡ï¼Œåˆ†æ•°è’¸é¦é‡‡æ ·æ³•æ˜¯é€šè¿‡æ¢¯åº¦ä¸‹é™é€æ­¥æ›´æ–°ç›®æ ‡ç©ºé—´æ•°æ®ï¼Œè¿™ç§æ–¹æ³•è™½ç„¶ä¿è¯äº†æ›´å¥½çš„è¿è´¯æ€§ï¼Œä½†å¾€å¾€ç¼ºä¹ç»†èŠ‚ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é¦–æ¬¡æ­ç¤ºäº†è¿™ä¸¤ç§æ–¹æ³•ä¹‹é—´çš„ç›¸äº’è”ç³»ï¼ŒåŒæ—¶å¼ºè°ƒäº†å®ƒä»¬çš„å·®å¼‚ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†StochSyncè¿™ä¸€æ–°æ–¹æ³•ï¼Œå®ƒç»“åˆäº†è¿™ä¸¤ç§æ–¹æ³•çš„ä¼˜ç‚¹ï¼Œèƒ½å¤Ÿåœ¨å¼±æ¡ä»¶ä¸‹å®ç°æœ‰æ•ˆæ€§èƒ½ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒStochSyncåœ¨æ— å›¾åƒæ¡ä»¶çš„å…¨æ™¯å›¾ç”Ÿæˆä¸­å–å¾—äº†æœ€ä½³æ€§èƒ½ï¼Œä¼˜äºåŸºäºå¾®è°ƒçš„æ–¹æ³•ï¼Œå¹¶ä¸”åœ¨æä¾›æ·±åº¦æ¡ä»¶çš„ä¸‰ç»´ç½‘æ ¼çº¹ç†ç”Ÿæˆä¸­å–å¾—äº†ä¸ä¹‹å‰çš„æ–¹æ³•ç›¸å½“çš„ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.15445v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://stochsync.github.io/">https://stochsync.github.io/</a> (ICLR 2025)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åˆ©ç”¨é¢„è®­ç»ƒå›¾åƒæ‰©æ•£æ¨¡å‹åœ¨ä»»æ„ç©ºé—´ï¼ˆå¦‚çƒä½“ç”Ÿæˆ360Â°å…¨æ™¯å›¾å’Œç½‘æ ¼è¡¨é¢ç”Ÿæˆçº¹ç†ï¼‰è¿›è¡Œé›¶æ ·æœ¬å›¾åƒç”Ÿæˆçš„æ–¹æ³•ã€‚æ–‡ç« æ¢è®¨äº†ä¸¤ç§ä¸»è¦çš„é›¶æ ·æœ¬ç”Ÿæˆæ–¹å¼ï¼šDiffusion Synchronizationå’ŒScore Distillation Samplingï¼Œå¹¶é¦–æ¬¡æ­ç¤ºäº†è¿™ä¸¤ç§æ–¹æ³•ä¹‹é—´çš„å…³è”å’Œå·®å¼‚ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“åˆä¸¤è€…ä¼˜ç‚¹çš„æ–°æ–¹æ³•StochSyncï¼Œèƒ½å¤Ÿåœ¨å¼±æ¡ä»¶ä¸‹å®ç°æœ‰æ•ˆæ€§èƒ½ã€‚å®éªŒè¡¨æ˜ï¼ŒStochSyncåœ¨æ— éœ€å›¾åƒæ¡ä»¶çš„æƒ…å†µä¸‹ï¼Œæ€§èƒ½æœ€ä½³ï¼Œä¸”åœ¨éœ€è¦æ·±åº¦æ¡ä»¶çš„3Dç½‘æ ¼çº¹ç†ç”Ÿæˆæ–¹é¢ä¹Ÿæœ‰è¾ƒå¥½çš„è¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§é›¶æ ·æœ¬æ–¹æ³•åœ¨ä»»æ„ç©ºé—´ç”Ÿæˆå›¾åƒçš„æ–°æŠ€æœ¯ï¼Œä½¿ç”¨é¢„è®­ç»ƒçš„å›¾åƒæ‰©æ•£æ¨¡å‹ã€‚</li>
<li>æ–‡ç« ä¸»è¦æ¢è®¨äº†Diffusion Synchronizationå’ŒScore Distillation Samplingä¸¤ç§é›¶æ ·æœ¬ç”Ÿæˆæ–¹å¼ã€‚</li>
<li>é¦–æ¬¡æ­ç¤ºäº†Diffusion Synchronizationå’ŒScore Distillation Samplingä¸¤ç§æ–¹æ³•çš„å…³è”ä¸å·®å¼‚ã€‚</li>
<li>æå‡ºäº†ä¸€ç§ç»“åˆä¸¤è€…ä¼˜ç‚¹çš„æ–°æ–¹æ³•StochSyncï¼Œèƒ½å¤Ÿåœ¨å¼±æ¡ä»¶ä¸‹å®ç°æœ‰æ•ˆæ€§èƒ½ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒStochSyncåœ¨æ— éœ€å›¾åƒæ¡ä»¶çš„æƒ…å†µä¸‹ï¼Œæ€§èƒ½æœ€ä½³ï¼Œå°¤å…¶åœ¨ç”Ÿæˆå…¨æ™¯å›¾æ–¹é¢ã€‚</li>
<li>åœ¨éœ€è¦æ·±åº¦æ¡ä»¶çš„3Dç½‘æ ¼çº¹ç†ç”Ÿæˆæ–¹é¢ï¼ŒStochSyncä¹Ÿæœ‰è¾ƒå¥½çš„è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.15445">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-84255535e8e74711f441500238f8a8b3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-54598e53fe0d61fedb6c61baec78b3ff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5665254822446be3d3066a82fc0a797b.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Dfilled-Repurposing-Edge-Enhancing-Diffusion-for-Guided-DSM-Void-Filling"><a href="#Dfilled-Repurposing-Edge-Enhancing-Diffusion-for-Guided-DSM-Void-Filling" class="headerlink" title="Dfilled: Repurposing Edge-Enhancing Diffusion for Guided DSM Void   Filling"></a>Dfilled: Repurposing Edge-Enhancing Diffusion for Guided DSM Void   Filling</h2><p><strong>Authors:Daniel Panangian, Ksenia Bittner</strong></p>
<p>Digital Surface Models (DSMs) are essential for accurately representing Earthâ€™s topography in geospatial analyses. DSMs capture detailed elevations of natural and manmade features, crucial for applications like urban planning, vegetation studies, and 3D reconstruction. However, DSMs derived from stereo satellite imagery often contain voids or missing data due to occlusions, shadows, and lowsignal areas. Previous studies have primarily focused on void filling for digital elevation models (DEMs) and Digital Terrain Models (DTMs), employing methods such as inverse distance weighting (IDW), kriging, and spline interpolation. While effective for simpler terrains, these approaches often fail to handle the intricate structures present in DSMs. To overcome these limitations, we introduce Dfilled, a guided DSM void filling method that leverages optical remote sensing images through edge-enhancing diffusion. Dfilled repurposes deep anisotropic diffusion models, which originally designed for super-resolution tasks, to inpaint DSMs. Additionally, we utilize Perlin noise to create inpainting masks that mimic natural void patterns in DSMs. Experimental evaluations demonstrate that Dfilled surpasses traditional interpolation methods and deep learning approaches in DSM void filling tasks. Both quantitative and qualitative assessments highlight the methodâ€™s ability to manage complex features and deliver accurate, visually coherent results. </p>
<blockquote>
<p>æ•°å­—è¡¨é¢æ¨¡å‹ï¼ˆDSMsï¼‰åœ¨åœ°ç†ç©ºé—´åˆ†æä¸­å‡†ç¡®è¡¨ç¤ºåœ°çƒåœ°å½¢æ–¹é¢èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚DSMæ•è·è‡ªç„¶å’Œäººé€ ç‰¹å¾çš„è¯¦ç»†é«˜ç¨‹ä¿¡æ¯ï¼Œå¯¹äºåŸå¸‚è§„åˆ’ã€æ¤è¢«ç ”ç©¶å’Œ3Dé‡å»ºç­‰åº”ç”¨è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œä»ç«‹ä½“å«æ˜Ÿå½±åƒæ´¾ç”Ÿçš„DSMsé€šå¸¸ç”±äºé®æŒ¡ã€é˜´å½±å’Œä½ä¿¡å·åŒºåŸŸè€ŒåŒ…å«ç©ºéš™æˆ–ç¼ºå¤±æ•°æ®ã€‚ä»¥å¾€çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨æ•°å­—é«˜ç¨‹æ¨¡å‹ï¼ˆDEMsï¼‰å’Œæ•°å­—åœ°å½¢æ¨¡å‹ï¼ˆDTMsï¼‰çš„ç©ºéš™å¡«å……ä¸Šï¼Œé‡‡ç”¨çš„æ–¹æ³•åŒ…æ‹¬åè·ç¦»åŠ æƒï¼ˆIDWï¼‰ã€å…‹é‡Œé‡‘æ³•å’Œæ ·æ¡æ’å€¼ç­‰ã€‚è¿™äº›æ–¹æ³•å¯¹äºç®€å•åœ°å½¢è™½ç„¶æœ‰æ•ˆï¼Œä½†å¾€å¾€éš¾ä»¥å¤„ç†DSMä¸­å­˜åœ¨çš„å¤æ‚ç»“æ„ã€‚ä¸ºäº†å…‹æœè¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬å¼•å…¥äº†Dfilledï¼Œè¿™æ˜¯ä¸€ç§æœ‰æŒ‡å¯¼çš„DSMç©ºéš™å¡«å……æ–¹æ³•ï¼Œå®ƒé€šè¿‡è¾¹ç¼˜å¢å¼ºæ‰©æ•£åˆ©ç”¨å…‰å­¦é¥æ„Ÿå›¾åƒã€‚Dfilledå°†åŸæœ¬ä¸ºè¶…åˆ†è¾¨ç‡ä»»åŠ¡è®¾è®¡çš„æ·±åº¦å„å‘å¼‚æ€§æ‰©æ•£æ¨¡å‹æ”¹ç”¨äºå¡«è¡¥DSMã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜åˆ©ç”¨Perlinå™ªå£°ç”Ÿæˆæ¨¡ä»¿DSMä¸­è‡ªç„¶ç©ºéš™æ¨¡å¼çš„å¡«å……æ©è†œã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œåœ¨DSMç©ºéš™å¡«å……ä»»åŠ¡ä¸­ï¼ŒDfilledè¶…è¶Šäº†ä¼ ç»Ÿæ’å€¼æ–¹æ³•å’Œæ·±åº¦å­¦ä¹ æ–¹æ³•ã€‚å®šé‡å’Œå®šæ€§è¯„ä¼°éƒ½çªå‡ºäº†è¯¥æ–¹æ³•åœ¨å¤„ç†å¤æ‚ç‰¹å¾æ–¹é¢çš„èƒ½åŠ›ï¼Œå¹¶èƒ½æä¾›å‡†ç¡®ã€è§†è§‰è¿è´¯çš„ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.15440v1">PDF</a> Accepted to IEEE&#x2F;CVF Winter Conference on Applications of Computer   Vision Workshops (WACVW)</p>
<p><strong>Summary</strong>ï¼š</p>
<p>æ•°å­—è¡¨é¢æ¨¡å‹ï¼ˆDSMsï¼‰åœ¨åœ°ç†ç©ºé—´åˆ†æä¸­å‡†ç¡®è¡¨ç¤ºåœ°çƒåœ°å½¢è‡³å…³é‡è¦ã€‚DSMsæ•æ‰è‡ªç„¶å’Œäººé€ ç‰¹å¾çš„è¯¦ç»†é«˜ç¨‹ï¼Œå¯¹äºåŸå¸‚è§„åˆ’ã€æ¤è¢«ç ”ç©¶å’Œ3Dé‡å»ºç­‰åº”ç”¨è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œä»ç«‹ä½“å«æ˜Ÿå½±åƒæ´¾ç”Ÿçš„DSMså¸¸å¸¸ç”±äºé®æŒ¡ã€é˜´å½±å’Œä½ä¿¡å·åŒºåŸŸè€Œå‡ºç°ç©ºç™½æˆ–ç¼ºå¤±æ•°æ®ã€‚å°½ç®¡ä¼ ç»Ÿçš„æ’å€¼æ–¹æ³•å¦‚åè·ç¦»æƒé‡ï¼ˆIDWï¼‰ã€å…‹é‡Œæ ¼å’Œæ ·æ¡æ’å€¼å¯¹ç®€å•åœ°å½¢æœ‰æ•ˆï¼Œä½†å®ƒä»¬éš¾ä»¥å¤„ç†DSMsä¸­çš„å¤æ‚ç»“æ„ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†Dfilledæ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨å…‰å­¦é¥æ„Ÿå›¾åƒè¿›è¡Œè¾¹ç¼˜å¢å¼ºæ‰©æ•£çš„DSMç©ºç™½å¡«å……æ–¹æ³•ã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒDfilledåœ¨DSMç©ºç™½å¡«å……ä»»åŠ¡ä¸Šè¶…è¶Šäº†ä¼ ç»Ÿçš„æ’å€¼æ–¹æ³•å’Œæ·±åº¦å­¦ä¹ æ–¹æ³•ï¼Œæ— è®ºæ˜¯å®šé‡è¿˜æ˜¯å®šæ€§è¯„ä¼°ï¼Œéƒ½è¯æ˜äº†è¯¥æ–¹æ³•ç®¡ç†å¤æ‚ç‰¹å¾å¹¶äº§ç”Ÿå‡†ç¡®ã€è§†è§‰è¿è´¯ç»“æœçš„èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æ•°å­—è¡¨é¢æ¨¡å‹ï¼ˆDSMsï¼‰åœ¨åœ°ç†ç©ºé—´åˆ†æä¸­æ‰®æ¼”é‡è¦è§’è‰²ï¼Œç”¨äºå‡†ç¡®è¡¨ç¤ºåœ°çƒçš„åœ°å½¢ç‰¹å¾ã€‚</li>
<li>DSMsé€šè¿‡æ•æ‰è‡ªç„¶å’Œäººé€ ç‰¹å¾çš„è¯¦ç»†é«˜ç¨‹ï¼Œæ”¯æŒå¤šç§åº”ç”¨ï¼Œå¦‚åŸå¸‚è§„åˆ’ã€æ¤è¢«ç ”ç©¶å’Œ3Dé‡å»ºã€‚</li>
<li>ä»ç«‹ä½“å«æ˜Ÿå½±åƒæ´¾ç”Ÿçš„DSMså¯èƒ½ä¼šå‡ºç°ç©ºç™½æˆ–ç¼ºå¤±æ•°æ®ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºé®æŒ¡ã€é˜´å½±å’Œä½ä¿¡å·åŒºåŸŸå¯¼è‡´çš„ã€‚</li>
<li>ä¼ ç»Ÿæ’å€¼æ–¹æ³•åœ¨å¤„ç†DSMsä¸­çš„å¤æ‚ç»“æ„æ—¶å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>Dfilledæ–¹æ³•æ˜¯ä¸€ç§æ–°å‹çš„DSMç©ºç™½å¡«å……æŠ€æœ¯ï¼Œåˆ©ç”¨å…‰å­¦é¥æ„Ÿå›¾åƒè¿›è¡Œè¾¹ç¼˜å¢å¼ºæ‰©æ•£ã€‚</li>
<li>Dfilledæ–¹æ³•é€šè¿‡åˆ©ç”¨æ·±åº¦å„å‘å¼‚æ€§æ‰©æ•£æ¨¡å‹å’ŒPerlinå™ªå£°ç”Ÿæˆæ¨¡ä»¿DSMè‡ªç„¶ç©ºç™½æ¨¡å¼çš„å¡«å……æ©è†œæ¥å®ç°é«˜æ•ˆçš„ç©ºç™½å¡«å……ã€‚</li>
<li>å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒDfilledåœ¨DSMç©ºç™½å¡«å……ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä¼˜äºä¼ ç»Ÿçš„æ’å€¼æ–¹æ³•å’Œæ·±åº¦å­¦ä¹ æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.15440">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-601a580560dc20c6b2e111a0b6c5fbbb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-292cef999959e8924af92e7532be1e04.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6ff2a3261420ce68569adeb580483906.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1be4c41117b0821db6ef49e720e5cc6f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1892a00733ebe3b2056d0f07f3daf1e5.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Zero-Shot-Interactive-Text-to-Image-Retrieval-via-Diffusion-Augmented-Representations"><a href="#Zero-Shot-Interactive-Text-to-Image-Retrieval-via-Diffusion-Augmented-Representations" class="headerlink" title="Zero-Shot Interactive Text-to-Image Retrieval via Diffusion-Augmented   Representations"></a>Zero-Shot Interactive Text-to-Image Retrieval via Diffusion-Augmented   Representations</h2><p><strong>Authors:Zijun Long, Kangheng Liang, Gerardo Aragon-Camarasa, Richard Mccreadie, Paul Henderson</strong></p>
<p>Interactive Text-to-Image Retrieval (I-TIR) has emerged as a transformative user-interactive tool for applications in domains such as e-commerce and education. Yet, current methodologies predominantly depend on finetuned Multimodal Large Language Models (MLLMs), which face two critical limitations: (1) Finetuning imposes prohibitive computational overhead and long-term maintenance costs. (2) Finetuning narrows the pretrained knowledge distribution of MLLMs, reducing their adaptability to novel scenarios. These issues are exacerbated by the inherently dynamic nature of real-world I-TIR systems, where queries and image databases evolve in complexity and diversity, often deviating from static training distributions. To overcome these constraints, we propose Diffusion Augmented Retrieval (DAR), a paradigm-shifting framework that bypasses MLLM finetuning entirely. DAR synergizes Large Language Model (LLM)-guided query refinement with Diffusion Model (DM)-based visual synthesis to create contextually enriched intermediate representations. This dual-modality approach deciphers nuanced user intent more holistically, enabling precise alignment between textual queries and visually relevant images. Rigorous evaluations across four benchmarks reveal DARâ€™s dual strengths: (1) Matches state-of-the-art finetuned I-TIR models on straightforward queries without task-specific training. (2) Scalable Generalization: Surpasses finetuned baselines by 7.61% in Hits@10 (top-10 accuracy) under multi-turn conversational complexity, demonstrating robustness to intricate, distributionally shifted interactions. By eliminating finetuning dependencies and leveraging generative-augmented representations, DAR establishes a new trajectory for efficient, adaptive, and scalable cross-modal retrieval systems. </p>
<blockquote>
<p>äº¤äº’å¼æ–‡æœ¬åˆ°å›¾åƒæ£€ç´¢ï¼ˆI-TIRï¼‰å·²ç»æˆä¸ºç”µå­å•†åŠ¡å’Œæ•™è‚²ç­‰é¢†åŸŸåº”ç”¨çš„ä¸€ç§å˜é©æ€§çš„ç”¨æˆ·äº¤äº’å·¥å…·ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ–¹æ³•ä¸»è¦ä¾èµ–äºå¾®è°ƒçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ï¼Œå®ƒä»¬é¢ä¸´ä¸¤ä¸ªå…³é”®çš„å±€é™æ€§ï¼šï¼ˆ1ï¼‰å¾®è°ƒå¸¦æ¥äº†å·¨å¤§çš„è®¡ç®—å¼€é”€å’Œé•¿æœŸç»´æŠ¤æˆæœ¬ã€‚ï¼ˆ2ï¼‰å¾®è°ƒç¼©å°äº†MLLMsçš„é¢„è®­ç»ƒçŸ¥è¯†åˆ†å¸ƒï¼Œé™ä½äº†å®ƒä»¬å¯¹æ–°åœºæ™¯çš„é€‚åº”æ€§ã€‚è¿™äº›é—®é¢˜è¢«çœŸå®ä¸–ç•Œä¸­I-TIRç³»ç»Ÿçš„å›ºæœ‰åŠ¨æ€æ€§è´¨æ‰€åŠ å‰§ï¼Œå…¶ä¸­æŸ¥è¯¢å’Œå›¾åƒæ•°æ®åº“çš„å¤æ‚æ€§å’Œå¤šæ ·æ€§åœ¨ä¸æ–­å‘å±•å˜åŒ–ï¼Œå¸¸å¸¸åç¦»é™æ€è®­ç»ƒåˆ†å¸ƒã€‚ä¸ºäº†å…‹æœè¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†æ‰©æ•£å¢å¼ºæ£€ç´¢ï¼ˆDARï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªå½»åº•ç»•è¿‡MLLMå¾®è°ƒçš„æ¨¡å¼è½¬å˜æ¡†æ¶ã€‚DARå°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¼•å¯¼æŸ¥è¯¢ç»†åŒ–ä¸åŸºäºæ‰©æ•£æ¨¡å‹ï¼ˆDMï¼‰çš„è§†è§‰åˆæˆç›¸ç»“åˆï¼Œä»¥åˆ›å»ºä¸Šä¸‹æ–‡ä¸°å¯Œçš„ä¸­é—´è¡¨ç¤ºã€‚è¿™ç§åŒæ¨¡æ€æ–¹æ³•æ›´å…¨é¢åœ°è§£é‡Šäº†å¾®å¦™çš„ç”¨æˆ·æ„å›¾ï¼Œå®ç°äº†æ–‡æœ¬æŸ¥è¯¢å’Œè§†è§‰ç›¸å…³å›¾åƒä¹‹é—´çš„ç²¾ç¡®å¯¹é½ã€‚åœ¨å››ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„ä¸¥æ ¼è¯„ä¼°æ­ç¤ºäº†DARçš„åŒé‡ä¼˜åŠ¿ï¼šï¼ˆ1ï¼‰åœ¨ç›´æ¥æŸ¥è¯¢ä¸ŠåŒ¹é…æœ€å…ˆè¿›çš„å¾®è°ƒI-TIRæ¨¡å‹ï¼Œæ— éœ€ç‰¹å®šä»»åŠ¡è®­ç»ƒã€‚ï¼ˆ2ï¼‰å¯æ‰©å±•çš„æ³›åŒ–èƒ½åŠ›ï¼šåœ¨å¤šå›åˆå¯¹è¯å¤æ‚æ€§çš„æƒ…å†µä¸‹ï¼Œåœ¨å‘½ä¸­æ•°@10ï¼ˆå‰10åçš„å‡†ç¡®ç‡ï¼‰ä¸Šè¶…è¶Šå¾®è°ƒåŸºçº¿7.61%ï¼Œè¯æ˜äº†å¯¹å¤æ‚ã€åˆ†å¸ƒåç§»äº¤äº’çš„ç¨³å¥æ€§ã€‚é€šè¿‡æ¶ˆé™¤å¾®è°ƒä¾èµ–å¹¶åˆ©ç”¨ç”Ÿæˆå¢å¼ºè¡¨ç¤ºï¼ŒDARä¸ºé«˜æ•ˆã€è‡ªé€‚åº”å’Œå¯æ‰©å±•çš„è·¨æ¨¡æ€æ£€ç´¢ç³»ç»Ÿå»ºç«‹äº†æ–°çš„è½¨è¿¹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.15379v1">PDF</a> </p>
<p><strong>Summary</strong><br>     äº¤äº’å¼æ–‡æœ¬åˆ°å›¾åƒæ£€ç´¢ï¼ˆI-TIRï¼‰å·²æˆä¸ºç”µå­å•†åŠ¡å’Œæ•™è‚²ç­‰é¢†åŸŸçš„é‡è¦å·¥å…·ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ–¹æ³•ä¸»è¦ä¾èµ–äºå¾®è°ƒçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ï¼Œå­˜åœ¨ä¸¤ä¸ªå…³é”®é—®é¢˜ï¼šä¸€æ˜¯è®¡ç®—å¼€é”€å¤§ä¸”ç»´æŠ¤æˆæœ¬é«˜ï¼›äºŒæ˜¯éš¾ä»¥é€‚åº”æ–°åœºæ™¯ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºä¸€ç§åä¸ºDiffusion Augmented Retrievalï¼ˆDARï¼‰çš„æ–°æ¡†æ¶ï¼Œé€šè¿‡ç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¼•å¯¼æŸ¥è¯¢ä¼˜åŒ–å’Œæ‰©æ•£æ¨¡å‹ï¼ˆDMï¼‰çš„è§†è§‰åˆæˆï¼Œåˆ›å»ºä¸Šä¸‹æ–‡ä¸°å¯Œçš„ä¸­é—´è¡¨ç¤ºã€‚è¿™ä¸€åŒæ¨¡æ€æ–¹æ³•æ›´å…¨é¢åœ°è§£æç”¨æˆ·æ„å›¾ï¼Œå®ç°æ–‡æœ¬æŸ¥è¯¢å’Œè§†è§‰ç›¸å…³å›¾åƒä¹‹é—´çš„ç²¾ç¡®å¯¹é½ã€‚è¯„ä¼°è¡¨æ˜ï¼ŒDARåœ¨åŒ¹é…ç°æœ‰æŠ€æœ¯çš„åŒæ—¶ï¼Œå…·æœ‰å‡ºè‰²çš„é€šç”¨æ€§å’Œå¯æ‰©å±•æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰I-TIRæ–¹æ³•ä¸»è¦ä¾èµ–å¾®è°ƒçš„MLLMsï¼Œå­˜åœ¨è®¡ç®—å¼€é”€å¤§å’Œç»´æŠ¤æˆæœ¬é«˜çš„å±€é™æ€§ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹ï¼ˆDMï¼‰åœ¨è§†è§‰åˆæˆæ–¹é¢å…·æœ‰æ½œåŠ›ï¼Œå¯ä»¥ä¸LLMç»“åˆä½¿ç”¨ä»¥æé«˜æ£€ç´¢ç²¾åº¦ã€‚</li>
<li>DARæ¡†æ¶é€šè¿‡ç»“åˆLLMå’ŒDMåˆ›å»ºä¸Šä¸‹æ–‡ä¸°å¯Œçš„ä¸­é—´è¡¨ç¤ºï¼Œå®ç°æ–‡æœ¬æŸ¥è¯¢å’Œè§†è§‰å›¾åƒçš„ç²¾ç¡®å¯¹é½ã€‚</li>
<li>DARåœ¨æ ‡å‡†è¯„ä¼°ä¸Šå®ç°äº†ä¸ç°æœ‰æŠ€æœ¯ç›¸å½“çš„æ€§èƒ½ï¼ŒåŒæ—¶å…·æœ‰å‡ºè‰²çš„å¯æ‰©å±•æ€§å’Œé€šç”¨æ€§ã€‚</li>
<li>DARé€šè¿‡æ¶ˆé™¤å¯¹å¾®è°ƒçš„éœ€æ±‚å¹¶åˆ©ç”¨ç”Ÿæˆå¢å¼ºè¡¨ç¤ºï¼Œä¸ºé«˜æ•ˆã€è‡ªé€‚åº”å’Œå¯æ‰©å±•çš„è·¨æ¨¡æ€æ£€ç´¢ç³»ç»Ÿå¼€è¾Ÿäº†æ–°é€”å¾„ã€‚</li>
<li>DARæ¡†æ¶é€‚ç”¨äºåŠ¨æ€å˜åŒ–çš„æŸ¥è¯¢å’Œå›¾åƒæ•°æ®åº“ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”ç°å®ä¸–ç•Œçš„å¤æ‚æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.15379">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4cfdce012f60c763a7b71fa918dd0834.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-172271c6ddce71627fe3a2ddd403f2a1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e5fad1759d407113b687d797a2048b28.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-45ae0caf9769e462e4207bc1051ab672.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Generalizable-Deepfake-Detection-via-Effective-Local-Global-Feature-Extraction"><a href="#Generalizable-Deepfake-Detection-via-Effective-Local-Global-Feature-Extraction" class="headerlink" title="Generalizable Deepfake Detection via Effective Local-Global Feature   Extraction"></a>Generalizable Deepfake Detection via Effective Local-Global Feature   Extraction</h2><p><strong>Authors:Jiazhen Yan, Ziqiang Li, Ziwen He, Zhangjie Fu</strong></p>
<p>The rapid advancement of GANs and diffusion models has led to the generation of increasingly realistic fake images, posing significant hidden dangers and threats to society. Consequently, deepfake detection has become a pressing issue in todayâ€™s world. While some existing methods focus on forgery features from either a local or global perspective, they often overlook the complementary nature of these features. Other approaches attempt to incorporate both local and global features but rely on simplistic strategies, such as cropping, which fail to capture the intricate relationships between local features. To address these limitations, we propose a novel method that effectively combines local spatial-frequency domain features with global frequency domain information, capturing detailed and holistic forgery traces. Specifically, our method uses Discrete Wavelet Transform (DWT) and sliding windows to tile forged features and leverages attention mechanisms to extract local spatial-frequency domain information. Simultaneously, the phase component of the Fast Fourier Transform (FFT) is integrated with attention mechanisms to extract global frequency domain information, complementing the local features and ensuring the integrity of forgery detection. Comprehensive evaluations on open-world datasets generated by 34 distinct generative models demonstrate a significant improvement of 2.9% over existing state-of-the-art methods. </p>
<blockquote>
<p>ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰å’Œæ‰©æ•£æ¨¡å‹çš„å¿«é€Ÿå‘å±•å¯¼è‡´äº†è¶Šæ¥è¶Šé€¼çœŸçš„è™šå‡å›¾åƒç”Ÿæˆï¼Œç»™ç¤¾ä¼šå¸¦æ¥äº†é‡å¤§çš„æ½œåœ¨å±é™©å’Œå¨èƒã€‚å› æ­¤ï¼Œæ·±åº¦ä¼ªé€ æ£€æµ‹å·²æˆä¸ºå½“ä»Šä¸–ç•Œçš„ç´§è¿«é—®é¢˜ã€‚è™½ç„¶ä¸€äº›ç°æœ‰çš„æ–¹æ³•ä¾§é‡äºä»å±€éƒ¨æˆ–å…¨å±€è§’åº¦æå–ä¼ªé€ ç‰¹å¾ï¼Œä½†å®ƒä»¬å¾€å¾€å¿½è§†äº†è¿™äº›ç‰¹å¾çš„äº’è¡¥æ€§ã€‚å…¶ä»–æ–¹æ³•è¯•å›¾ç»“åˆå±€éƒ¨å’Œå…¨å±€ç‰¹å¾ï¼Œä½†ä¾èµ–äºç®€å•çš„ç­–ç•¥ï¼Œå¦‚è£å‰ªï¼Œè¿™æ— æ³•æ•æ‰å±€éƒ¨ç‰¹å¾ä¹‹é—´çš„å¤æ‚å…³ç³»ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•æœ‰æ•ˆåœ°ç»“åˆäº†å±€éƒ¨ç©ºé—´é¢‘åŸŸç‰¹å¾ä¸å…¨å±€é¢‘åŸŸä¿¡æ¯ï¼Œæ•æ‰è¯¦ç»†ä¸”æ•´ä½“çš„ä¼ªé€ ç—•è¿¹ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä½¿ç”¨ç¦»æ•£å°æ³¢å˜æ¢ï¼ˆDWTï¼‰å’Œæ»‘åŠ¨çª—å£æ¥åˆ’åˆ†ä¼ªé€ ç‰¹å¾ï¼Œå¹¶åˆ©ç”¨æ³¨æ„åŠ›æœºåˆ¶æå–å±€éƒ¨ç©ºé—´é¢‘åŸŸä¿¡æ¯ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬å°†å¿«é€Ÿå‚…é‡Œå¶å˜æ¢ï¼ˆFFTï¼‰çš„ç›¸ä½æˆåˆ†ä¸æ³¨æ„åŠ›æœºåˆ¶ç›¸ç»“åˆï¼Œæå–å…¨å±€é¢‘åŸŸä¿¡æ¯ï¼Œä»¥è¡¥å……å±€éƒ¨ç‰¹å¾å¹¶ç¡®ä¿ä¼ªé€ æ£€æµ‹çš„å®Œæ•´æ€§ã€‚åœ¨ç”±34ç§ä¸åŒçš„ç”Ÿæˆæ¨¡å‹ç”Ÿæˆçš„å¼€æ”¾ä¸–ç•Œæ•°æ®é›†ä¸Šçš„ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼Œä¸ç°æœ‰çš„æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å–å¾—äº†2.9%çš„æ˜¾è‘—æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.15253v1">PDF</a> under review</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ç”Ÿæˆå¯¹æŠ—ç½‘ç»œå’Œæ‰©æ•£æ¨¡å‹çš„å¿«é€Ÿå‘å±•å¸¦æ¥çš„è™šå‡å›¾åƒç”Ÿæˆé—®é¢˜ï¼Œå¼ºè°ƒäº†æ·±åº¦ä¼ªé€ æ£€æµ‹åœ¨ç°ä»Šç¤¾ä¼šçš„ç´§è¿«æ€§ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•çš„å±€é™æ€§ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆå±€éƒ¨ç©ºé—´é¢‘ç‡åŸŸç‰¹å¾å’Œå…¨å±€é¢‘ç‡åŸŸä¿¡æ¯çš„æ–°æ–¹æ³•ï¼Œåˆ©ç”¨ç¦»æ•£å°æ³¢å˜æ¢å’Œæ»‘åŠ¨çª—å£è¿›è¡Œä¼ªé€ ç‰¹å¾æå–ï¼Œå¹¶é€šè¿‡æ³¨æ„åŠ›æœºåˆ¶èåˆå±€éƒ¨å’Œå…¨å±€ç‰¹å¾ï¼Œæé«˜äº†ä¼ªé€ æ£€æµ‹çš„å‡†ç¡®æ€§å’Œå®Œæ•´æ€§ã€‚åœ¨å…¬å¼€æ•°æ®é›†ä¸Šçš„ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•ç›¸è¾ƒäºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•æœ‰2.9%çš„æ˜¾è‘—æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GANså’Œæ‰©æ•£æ¨¡å‹çš„å¿«é€Ÿå‘å±•å¯¼è‡´è™šå‡å›¾åƒè¶Šæ¥è¶Šé€¼çœŸï¼Œç»™ç¤¾ä¼šå¸¦æ¥æ½œåœ¨å¨èƒï¼Œæ·±åº¦ä¼ªé€ æ£€æµ‹æˆä¸ºé‡è¦è®®é¢˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨å±€éƒ¨æˆ–å…¨å±€çš„ä¼ªé€ ç‰¹å¾ï¼Œä½†å¿½ç•¥äº†å®ƒä»¬ä¹‹é—´çš„äº’è¡¥æ€§ã€‚</li>
<li>ä¸€äº›æ–¹æ³•è¯•å›¾ç»“åˆå±€éƒ¨å’Œå…¨å±€ç‰¹å¾ï¼Œä½†é‡‡ç”¨ç®€å•ç­–ç•¥å¦‚è£å‰ªï¼Œæ— æ³•æ•æ‰å¤æ‚å…³ç³»ã€‚</li>
<li>æ–°æ–¹æ³•ç»“åˆå±€éƒ¨ç©ºé—´é¢‘ç‡åŸŸç‰¹å¾å’Œå…¨å±€é¢‘ç‡åŸŸä¿¡æ¯ï¼Œä½¿ç”¨ç¦»æ•£å°æ³¢å˜æ¢å’Œæ»‘åŠ¨çª—å£è¿›è¡Œç‰¹å¾æå–ã€‚</li>
<li>æ³¨æ„åŠ›æœºåˆ¶ç”¨äºèåˆå±€éƒ¨å’Œå…¨å±€ç‰¹å¾ï¼Œç¡®ä¿æ£€æµ‹å‡†ç¡®æ€§å’Œå®Œæ•´æ€§ã€‚</li>
<li>ç»¼åˆè¯„ä¼°æ˜¾ç¤ºæ–°æ–¹æ³•åœ¨å…¬å¼€æ•°æ®é›†ä¸Šçš„æ€§èƒ½ç›¸è¾ƒäºç°æœ‰æ–¹æ³•æœ‰æ˜¾è‘—æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.15253">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ce9220fdd60f534e06780b32944e8e8a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cef969ed29c2cc55faf92dd75ea373d1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ca572cd001d155dcdb87a4c26b1c3f8f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9f20c784d4304b01061a0f23fa2740e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1e2e94947a5ab6fdfd294c6afc642918.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="MATCHA-Towards-Matching-Anything"><a href="#MATCHA-Towards-Matching-Anything" class="headerlink" title="MATCHA:Towards Matching Anything"></a>MATCHA:Towards Matching Anything</h2><p><strong>Authors:Fei Xue, Sven Elflein, Laura Leal-TaixÃ©, Qunjie Zhou</strong></p>
<p>Establishing correspondences across images is a fundamental challenge in computer vision, underpinning tasks like Structure-from-Motion, image editing, and point tracking. Traditional methods are often specialized for specific correspondence types, geometric, semantic, or temporal, whereas humans naturally identify alignments across these domains. Inspired by this flexibility, we propose MATCHA, a unified feature model designed to &#96;&#96;rule them allâ€™â€™, establishing robust correspondences across diverse matching tasks. Building on insights that diffusion model features can encode multiple correspondence types, MATCHA augments this capacity by dynamically fusing high-level semantic and low-level geometric features through an attention-based module, creating expressive, versatile, and robust features. Additionally, MATCHA integrates object-level features from DINOv2 to further boost generalization, enabling a single feature capable of matching anything. Extensive experiments validate that MATCHA consistently surpasses state-of-the-art methods across geometric, semantic, and temporal matching tasks, setting a new foundation for a unified approach for the fundamental correspondence problem in computer vision. To the best of our knowledge, MATCHA is the first approach that is able to effectively tackle diverse matching tasks with a single unified feature. </p>
<blockquote>
<p>å»ºç«‹å›¾åƒä¹‹é—´çš„å¯¹åº”å…³ç³»æ˜¯è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„ä¸€ä¸ªåŸºæœ¬æŒ‘æˆ˜ï¼Œä¹Ÿæ˜¯ç»“æ„ä»è¿åŠ¨ã€å›¾åƒç¼–è¾‘å’Œç‚¹è·Ÿè¸ªç­‰ä»»åŠ¡çš„åŸºç¡€ã€‚ä¼ ç»Ÿæ–¹æ³•é€šå¸¸é’ˆå¯¹ç‰¹å®šçš„å¯¹åº”å…³ç³»ç±»å‹ï¼ˆå¦‚å‡ ä½•ã€è¯­ä¹‰æˆ–æ—¶é—´ï¼‰è¿›è¡Œä¸“ä¸šåŒ–å¤„ç†ï¼Œè€Œäººç±»è‡ªç„¶åœ°èƒ½å¤Ÿè¯†åˆ«è¿™äº›é¢†åŸŸçš„å¯¹é½æ–¹å¼ã€‚å—è¿™ç§çµæ´»æ€§çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†MATCHAï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„ç‰¹å¾æ¨¡å‹ï¼Œæ—¨åœ¨â€œä¸€ç»Ÿå¤©ä¸‹â€ï¼Œåœ¨å¤šç§åŒ¹é…ä»»åŠ¡ä¸­å»ºç«‹ç¨³å¥çš„å¯¹åº”å…³ç³»ã€‚åŸºäºæ‰©æ•£æ¨¡å‹ç‰¹å¾å¯ä»¥ç¼–ç å¤šç§å¯¹åº”å…³ç³»ç±»å‹çš„è§è§£ï¼ŒMATCHAé€šè¿‡åŸºäºæ³¨æ„åŠ›çš„æ¨¡å—åŠ¨æ€èåˆé«˜çº§è¯­ä¹‰å’Œä½çº§å‡ ä½•ç‰¹å¾ï¼Œå¢å¼ºè¿™ç§èƒ½åŠ›ï¼Œä»è€Œç”Ÿæˆè¡¨è¾¾åŠ›å¼ºã€é€šç”¨æ€§å¼ºã€é²æ£’æ€§é«˜çš„ç‰¹å¾ã€‚æ­¤å¤–ï¼ŒMATCHAè¿˜é›†æˆäº†DINOv2çš„å¯¹è±¡çº§ç‰¹å¾ï¼Œè¿›ä¸€æ­¥æé«˜äº†æ³›åŒ–èƒ½åŠ›ï¼Œä½¿å¾—å•ä¸ªç‰¹å¾å°±èƒ½å¤ŸåŒ¹é…ä»»ä½•å†…å®¹ã€‚å¤§é‡å®éªŒéªŒè¯äº†MATCHAåœ¨å‡ ä½•ã€è¯­ä¹‰å’Œæ—¶é—´åŒ¹é…ä»»åŠ¡ä¸Šå‡è¶…è¶Šäº†æœ€æ–°æ–¹æ³•ï¼Œä¸ºè®¡ç®—æœºè§†è§‰ä¸­åŸºæœ¬å¯¹åº”å…³ç³»é—®é¢˜çš„ç»Ÿä¸€æ–¹æ³•è®¾å®šäº†æ–°çš„åŸºç¡€ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼ŒMATCHAæ˜¯ç¬¬ä¸€ä¸ªèƒ½å¤Ÿæœ‰æ•ˆè§£å†³å¤šç§åŒ¹é…ä»»åŠ¡çš„å•ç»Ÿä¸€ç‰¹å¾æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.14945v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€ç‰¹å¾æ¨¡å‹MATCHAï¼Œæ—¨åœ¨é€šè¿‡èåˆæ‰©æ•£æ¨¡å‹ç‰¹å¾æ¥å»ºç«‹è·¨ä¸åŒåŒ¹é…ä»»åŠ¡çš„ç¨³å¥å¯¹åº”å…³ç³»ã€‚MATCHAé€šè¿‡åŠ¨æ€èåˆé«˜çº§è¯­ä¹‰å’Œä½çº§å‡ ä½•ç‰¹å¾ï¼Œå¢å¼ºå¯¹åº”å…³ç³»çš„è¡¨è¾¾èƒ½åŠ›ã€é€šç”¨æ€§å’Œç¨³å¥æ€§ã€‚æ­¤å¤–ï¼ŒMATCHAé›†æˆäº†DINOv2çš„å¯¹è±¡çº§ç‰¹å¾ï¼Œè¿›ä¸€æ­¥æé«˜æ³›åŒ–èƒ½åŠ›ï¼Œä½¿å¾—å•ä¸ªç‰¹å¾èƒ½å¤ŸåŒ¹é…ä»»ä½•å†…å®¹ã€‚å®éªŒè¯æ˜ï¼ŒMATCHAåœ¨å‡ ä½•ã€è¯­ä¹‰å’Œæ—¶åºåŒ¹é…ä»»åŠ¡ä¸Šå‡è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œä¸ºè®¡ç®—æœºè§†è§‰ä¸­çš„åŸºæœ¬å¯¹åº”å…³ç³»é—®é¢˜æä¾›äº†æ–°çš„ç»Ÿä¸€è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MATCHAæ˜¯ä¸€ä¸ªç»Ÿä¸€ç‰¹å¾æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³è®¡ç®—æœºè§†è§‰ä¸­è·¨å›¾åƒå¯¹åº”å…³ç³»çš„æ ¸å¿ƒæŒ‘æˆ˜ã€‚</li>
<li>MATCHAèåˆæ‰©æ•£æ¨¡å‹ç‰¹å¾ï¼Œå…·å¤‡å¤„ç†å¤šç§å¯¹åº”å…³ç³»ç±»å‹çš„èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡åŠ¨æ€èåˆé«˜çº§è¯­ä¹‰å’Œä½çº§å‡ ä½•ç‰¹å¾ï¼ŒMATCHAå¢å¼ºäº†å¯¹åº”å…³ç³»çš„è¡¨è¾¾èƒ½åŠ›ã€é€šç”¨æ€§å’Œç¨³å¥æ€§ã€‚</li>
<li>MATCHAé›†æˆäº†DINOv2çš„å¯¹è±¡çº§ç‰¹å¾ï¼Œæé«˜äº†æ³›åŒ–èƒ½åŠ›ï¼Œå®ç°äº†å•ä¸€ç‰¹å¾å¯¹ä»»ä½•å†…å®¹çš„åŒ¹é…ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒMATCHAåœ¨å‡ ä½•ã€è¯­ä¹‰å’Œæ—¶åºåŒ¹é…ä»»åŠ¡ä¸Šçš„æ€§èƒ½å‡è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ã€‚</li>
<li>MATCHAæ˜¯é¦–ä¸ªèƒ½å¤Ÿæœ‰æ•ˆå¤„ç†å¤šç§åŒ¹é…ä»»åŠ¡çš„å•ä¸€ç»Ÿä¸€ç‰¹å¾æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.14945">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-a3b490523141b2fe17e7490e208ddeda.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a1e7911508f46c508b79175857b48ef3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0b15bf0e1de811f42429257181814160.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ad46d12d0e91a1b3ee030b44047dc1ed.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f3e2371dc5e77f574ee450dc398bf8ac.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="GlyphDraw2-Automatic-Generation-of-Complex-Glyph-Posters-with-Diffusion-Models-and-Large-Language-Models"><a href="#GlyphDraw2-Automatic-Generation-of-Complex-Glyph-Posters-with-Diffusion-Models-and-Large-Language-Models" class="headerlink" title="GlyphDraw2: Automatic Generation of Complex Glyph Posters with Diffusion   Models and Large Language Models"></a>GlyphDraw2: Automatic Generation of Complex Glyph Posters with Diffusion   Models and Large Language Models</h2><p><strong>Authors:Jian Ma, Yonglin Deng, Chen Chen, Haonan Lu, Zhenyu Yang</strong></p>
<p>Posters play a crucial role in marketing and advertising by enhancing visual communication and brand visibility, making significant contributions to industrial design. With the latest advancements in controllable T2I diffusion models, increasing research has focused on rendering text within synthesized images. Despite improvements in text rendering accuracy, the field of automatic poster generation remains underexplored. In this paper, we propose an automatic poster generation framework with text rendering capabilities leveraging LLMs, utilizing a triple-cross attention mechanism based on alignment learning. This framework aims to create precise poster text within a detailed contextual background. Additionally, the framework supports controllable fonts, adjustable image resolution, and the rendering of posters with descriptions and text in both English and Chinese.Furthermore, we introduce a high-resolution font dataset and a poster dataset with resolutions exceeding 1024 pixels. Our approach leverages the SDXL architecture. Extensive experiments validate our methodâ€™s capability in generating poster images with complex and contextually rich backgrounds.Codes is available at <a target="_blank" rel="noopener" href="https://github.com/OPPO-Mente-Lab/GlyphDraw2">https://github.com/OPPO-Mente-Lab/GlyphDraw2</a>. </p>
<blockquote>
<p>æµ·æŠ¥åœ¨è¥é”€å’Œå¹¿å‘Šä¸­æ‰®æ¼”ç€è‡³å…³é‡è¦çš„è§’è‰²ï¼Œé€šè¿‡å¢å¼ºè§†è§‰äº¤æµå’Œå“ç‰Œå¯è§åº¦ï¼Œä¸ºå·¥ä¸šè®¾è®¡åšå‡ºäº†é‡å¤§è´¡çŒ®ã€‚éšç€æœ€æ–°çš„å¯æ§T2Iæ‰©æ•£æ¨¡å‹çš„è¿›æ­¥ï¼Œè¶Šæ¥è¶Šå¤šçš„ç ”ç©¶ä¸“æ³¨äºåœ¨åˆæˆå›¾åƒä¸­è¿›è¡Œæ–‡æœ¬æ¸²æŸ“ã€‚å°½ç®¡æ–‡æœ¬æ¸²æŸ“çš„å‡†ç¡®æ€§æœ‰æ‰€æé«˜ï¼Œä½†è‡ªåŠ¨æµ·æŠ¥ç”Ÿæˆé¢†åŸŸä»ç„¶æœªè¢«å……åˆ†æ¢ç´¢ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå…·æœ‰æ–‡æœ¬æ¸²æŸ“èƒ½åŠ›çš„è‡ªåŠ¨æµ·æŠ¥ç”Ÿæˆæ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨åŸºäºå¯¹é½å­¦ä¹ çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œé‡‡ç”¨ä¸‰é‡äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ã€‚è¯¥æ¡†æ¶æ—¨åœ¨åœ¨ä¸€ä¸ªè¯¦ç»†çš„ä¸Šä¸‹æ–‡èƒŒæ™¯ä¸­åˆ›å»ºç²¾ç¡®çš„æµ·æŠ¥æ–‡æœ¬ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶æ”¯æŒå¯æ§å­—ä½“ã€å¯è°ƒæ•´çš„å›¾åƒåˆ†è¾¨ç‡ä»¥åŠä¸­è‹±æ–‡æè¿°å’Œæ–‡æœ¬çš„æµ·æŠ¥æ¸²æŸ“ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä»‹ç»äº†ä¸€ä¸ªé«˜åˆ†è¾¨ç‡å­—ä½“æ•°æ®é›†å’Œä¸€ä¸ªåˆ†è¾¨ç‡è¶…è¿‡1024åƒç´ çš„æµ·æŠ¥æ•°æ®é›†ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨SDXLæ¶æ„ã€‚å¤§é‡å®éªŒéªŒè¯äº†æˆ‘ä»¬æ–¹æ³•åœ¨ç”Ÿæˆå…·æœ‰å¤æ‚å’Œä¸Šä¸‹æ–‡ä¸°å¯Œçš„èƒŒæ™¯çš„æµ·æŠ¥å›¾åƒæ–¹é¢çš„èƒ½åŠ›ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/OPPO-Mente-Lab/GlyphDraw2%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/OPPO-Mente-Lab/GlyphDraw2ä¸­æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.02252v3">PDF</a> Accepted by AAAI2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åˆ©ç”¨LLMså’ŒåŸºäºå¯¹é½å­¦ä¹ çš„ä¸‰é‡äº¤å‰æ³¨æ„åŠ›æœºåˆ¶çš„è‡ªåŠ¨æµ·æŠ¥ç”Ÿæˆæ¡†æ¶ï¼Œå¯å®ç°æ–‡æœ¬æ¸²æŸ“ã€‚è¯¥æ¡†æ¶æ—¨åœ¨åˆ›å»ºå…·æœ‰è¯¦ç»†ä¸Šä¸‹æ–‡èƒŒæ™¯çš„æµ·æŠ¥æ–‡æœ¬ï¼Œå¹¶æ”¯æŒå¯æ§å­—ä½“ã€å¯è°ƒå›¾åƒåˆ†è¾¨ç‡ä»¥åŠä¸­è‹±æ–‡æè¿°å’Œæ–‡æœ¬çš„æµ·æŠ¥æ¸²æŸ“ã€‚æ­¤å¤–ï¼Œå¼•å…¥é«˜åˆ†è¾¨ç‡å­—ä½“æ•°æ®é›†å’Œåˆ†è¾¨ç‡è¶…è¿‡1024åƒç´ çš„æµ·æŠ¥æ•°æ®é›†ã€‚å®éªŒè¯æ˜è¯¥æ–¹æ³•èƒ½å¤Ÿç”Ÿæˆå…·æœ‰å¤æ‚å’Œä¸°å¯Œä¸Šä¸‹æ–‡èƒŒæ™¯çš„æµ·æŠ¥å›¾åƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æµ·æŠ¥åœ¨è¥é”€å’Œå¹¿å‘Šä¸­èµ·åˆ°å…³é”®ä½œç”¨ï¼Œæå‡è§†è§‰æ²Ÿé€šå’Œå“ç‰Œå¯è§åº¦ï¼Œå¯¹å·¥ä¸šè®¾è®¡æœ‰é‡è¦è´¡çŒ®ã€‚</li>
<li>æ–‡æœ¬åœ¨åˆæˆå›¾åƒä¸­çš„æ¸²æŸ“æ˜¯è¿‘å¹´æ¥çš„ç ”ç©¶çƒ­ç‚¹ï¼Œå°¤å…¶æ˜¯åˆ©ç”¨å¯æ§çš„T2Iæ‰©æ•£æ¨¡å‹ã€‚</li>
<li>å½“å‰ç ”ç©¶å°šæœªå……åˆ†æ¢ç´¢è‡ªåŠ¨æµ·æŠ¥ç”Ÿæˆé¢†åŸŸã€‚</li>
<li>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºLLMså’ŒåŸºäºå¯¹é½å­¦ä¹ çš„ä¸‰é‡äº¤å‰æ³¨æ„åŠ›æœºåˆ¶çš„è‡ªåŠ¨æµ·æŠ¥ç”Ÿæˆæ¡†æ¶ï¼Œå…·æœ‰æ–‡æœ¬æ¸²æŸ“èƒ½åŠ›ã€‚</li>
<li>è¯¥æ¡†æ¶æ—¨åœ¨åˆ›å»ºå…·æœ‰è¯¦ç»†ä¸Šä¸‹æ–‡èƒŒæ™¯çš„æµ·æŠ¥æ–‡æœ¬ï¼Œæ”¯æŒå¤šç§å­—ä½“ã€å›¾åƒåˆ†è¾¨ç‡åŠä¸­è‹±æ–‡æè¿°ã€‚</li>
<li>å¼•å…¥é«˜åˆ†è¾¨ç‡å­—ä½“æ•°æ®é›†å’Œæµ·æŠ¥æ•°æ®é›†ï¼Œåˆ†è¾¨ç‡è¶…è¿‡1024åƒç´ ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.02252">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5682c31a56e60e41bbadca6c8a0954bc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-efb25ad2005f40b029459258372ee168.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b3f98df45dbccbfa58e8162d46c29fe6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cfab3794dc454afcf58961c6212c4b9c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-070be88a127805c1794b2b20449f5096.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Zero-shot-Video-Restoration-and-Enhancement-Using-Pre-Trained-Image-Diffusion-Model"><a href="#Zero-shot-Video-Restoration-and-Enhancement-Using-Pre-Trained-Image-Diffusion-Model" class="headerlink" title="Zero-shot Video Restoration and Enhancement Using Pre-Trained Image   Diffusion Model"></a>Zero-shot Video Restoration and Enhancement Using Pre-Trained Image   Diffusion Model</h2><p><strong>Authors:Cong Cao, Huanjing Yue, Xin Liu, Jingyu Yang</strong></p>
<p>Diffusion-based zero-shot image restoration and enhancement models have achieved great success in various tasks of image restoration and enhancement. However, directly applying them to video restoration and enhancement results in severe temporal flickering artifacts. In this paper, we propose the first framework for zero-shot video restoration and enhancement based on the pre-trained image diffusion model. By replacing the spatial self-attention layer with the proposed short-long-range (SLR) temporal attention layer, the pre-trained image diffusion model can take advantage of the temporal correlation between frames. We further propose temporal consistency guidance, spatial-temporal noise sharing, and an early stopping sampling strategy to improve temporally consistent sampling. Our method is a plug-and-play module that can be inserted into any diffusion-based image restoration or enhancement methods to further improve their performance. Experimental results demonstrate the superiority of our proposed method. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/cao-cong/ZVRD">https://github.com/cao-cong/ZVRD</a>. </p>
<blockquote>
<p>åŸºäºæ‰©æ•£çš„é›¶æ ·æœ¬å›¾åƒä¿®å¤å’Œå¢å¼ºæ¨¡å‹åœ¨å›¾åƒä¿®å¤å’Œå¢å¼ºçš„å„ç§ä»»åŠ¡ä¸­å–å¾—äº†å·¨å¤§çš„æˆåŠŸã€‚ç„¶è€Œï¼Œç›´æ¥å°†å…¶åº”ç”¨äºè§†é¢‘ä¿®å¤å’Œå¢å¼ºä¼šå¯¼è‡´ä¸¥é‡çš„æš‚æ—¶é—ªçƒä¼ªå½±ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºé¢„è®­ç»ƒå›¾åƒæ‰©æ•£æ¨¡å‹çš„é›¶æ ·æœ¬è§†é¢‘ä¿®å¤å’Œå¢å¼ºçš„ç¬¬ä¸€ä¸ªæ¡†æ¶ã€‚é€šè¿‡ç”¨æ‰€æå‡ºçš„çŸ­é•¿ç¨‹ï¼ˆSLRï¼‰æ—¶é—´æ³¨æ„åŠ›å±‚æ›¿æ¢ç©ºé—´è‡ªæ³¨æ„åŠ›å±‚ï¼Œé¢„è®­ç»ƒçš„å›¾åƒæ‰©æ•£æ¨¡å‹å¯ä»¥åˆ©ç”¨å¸§ä¹‹é—´çš„æ—¶é—´ç›¸å…³æ€§ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†æ—¶é—´ä¸€è‡´æ€§æŒ‡å¯¼ã€æ—¶ç©ºå™ªå£°å…±äº«å’Œæ—©æœŸåœæ­¢é‡‡æ ·ç­–ç•¥ï¼Œä»¥æ”¹å–„æ—¶é—´ä¸€è‡´æ€§çš„é‡‡æ ·ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ˜¯ä¸€ä¸ªå³æ’å³ç”¨çš„æ¨¡å—ï¼Œå¯ä»¥æ’å…¥åˆ°ä»»ä½•åŸºäºæ‰©æ•£çš„å›¾åƒä¿®å¤æˆ–å¢å¼ºæ–¹æ³•ä¸­ï¼Œä»¥è¿›ä¸€æ­¥æé«˜å…¶æ€§èƒ½ã€‚å®éªŒç»“æœè¯æ˜äº†æˆ‘ä»¬æå‡ºæ–¹æ³•çš„ä¼˜è¶Šæ€§ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨[<a target="_blank" rel="noopener" href="https://github.com/cao-cong/ZVRD%E6%89%BE%E5%88%B0%E3%80%82]">https://github.com/cao-cong/ZVRDæ‰¾åˆ°ã€‚]</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.01960v3">PDF</a> Accepted by AAAI 2025</p>
<p><strong>Summary</strong><br>æ‰©æ•£æ¨¡å‹åœ¨é›¶æ ·æœ¬å›¾åƒä¿®å¤å’Œå¢å¼ºä»»åŠ¡ä¸­å–å¾—äº†å·¨å¤§æˆåŠŸï¼Œä½†ç›´æ¥åº”ç”¨äºè§†é¢‘ä¿®å¤å’Œå¢å¼ºä¼šå¯¼è‡´ä¸¥é‡çš„æ—¶åºé—ªçƒä¼ªå½±ã€‚æœ¬æ–‡é¦–æ¬¡æå‡ºåŸºäºé¢„è®­ç»ƒå›¾åƒæ‰©æ•£æ¨¡å‹çš„é›¶æ ·æœ¬è§†é¢‘ä¿®å¤å’Œå¢å¼ºæ¡†æ¶ã€‚é€šè¿‡ç”¨æ‰€æå‡ºçš„é•¿çŸ­ç¨‹ï¼ˆSLRï¼‰æ—¶åºæ³¨æ„åŠ›å±‚æ›¿æ¢ç©ºé—´è‡ªæ³¨æ„åŠ›å±‚ï¼Œé¢„è®­ç»ƒçš„å›¾åƒæ‰©æ•£æ¨¡å‹å¯ä»¥åˆ©ç”¨å¸§ä¹‹é—´çš„æ—¶åºç›¸å…³æ€§ã€‚æœ¬æ–‡è¿˜æå‡ºäº†æ—¶åºä¸€è‡´æ€§æŒ‡å¯¼ã€æ—¶ç©ºå™ªå£°å…±äº«å’Œæ—©æœŸåœæ­¢é‡‡æ ·ç­–ç•¥ï¼Œä»¥æ”¹è¿›æ—¶åºä¸€è‡´é‡‡æ ·ã€‚è¯¥æ–¹æ³•æ˜¯ä¸€ä¸ªå³æ’å³ç”¨çš„æ¨¡å—ï¼Œå¯ä»¥æ’å…¥ä»»ä½•åŸºäºæ‰©æ•£çš„å›¾åƒä¿®å¤æˆ–å¢å¼ºæ–¹æ³•ä¸­ï¼Œä»¥è¿›ä¸€æ­¥æé«˜å…¶æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒä¿®å¤å’Œå¢å¼ºä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½†ç›´æ¥åº”ç”¨äºè§†é¢‘ä¼šå¯¼è‡´æ—¶åºä¼ªå½±ã€‚</li>
<li>æœ¬æ–‡é¦–æ¬¡æå‡ºäº†åŸºäºé¢„è®­ç»ƒå›¾åƒæ‰©æ•£æ¨¡å‹çš„é›¶æ ·æœ¬è§†é¢‘ä¿®å¤å’Œå¢å¼ºæ¡†æ¶ã€‚</li>
<li>é€šè¿‡å¼•å…¥é•¿çŸ­ç¨‹ï¼ˆSLRï¼‰æ—¶åºæ³¨æ„åŠ›å±‚ï¼Œæ¨¡å‹èƒ½åˆ©ç”¨å¸§é—´çš„æ—¶åºç›¸å…³æ€§ã€‚</li>
<li>æå‡ºäº†æ—¶åºä¸€è‡´æ€§æŒ‡å¯¼ã€æ—¶ç©ºå™ªå£°å…±äº«å’Œæ—©æœŸåœæ­¢é‡‡æ ·ç­–ç•¥ï¼Œä»¥æ”¹å–„æ—¶åºä¸€è‡´é‡‡æ ·ã€‚</li>
<li>è¯¥æ–¹æ³•æ˜¯ä¸€ä¸ªå¯æ’å…¥ä»»ä½•æ‰©æ•£å›¾åƒä¿®å¤æˆ–å¢å¼ºæ–¹æ³•çš„æ¨¡å—ï¼Œèƒ½è¿›ä¸€æ­¥æå‡æ€§èƒ½ã€‚</li>
<li>å®éªŒç»“æœè¯æ˜è¯¥æ–¹æ³•å…·æœ‰ä¼˜è¶Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.01960">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-dc33e2332143fcce684deef1e907382f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b9bd168d030a48c98ce59393965aa883.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b6a64643a15995a15fda5df68c86c9e2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d213286e9dba67074b0adc4c7cff3db5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5c881d379739978dc0b43df2881dabd0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ee5dbd0ba005be8ba772bb8385035e2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e031237a2d8a6faa09b672a092b367c7.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Information-Theoretic-Text-to-Image-Alignment"><a href="#Information-Theoretic-Text-to-Image-Alignment" class="headerlink" title="Information Theoretic Text-to-Image Alignment"></a>Information Theoretic Text-to-Image Alignment</h2><p><strong>Authors:Chao Wang, Giulio Franzese, Alessandro Finamore, Massimo Gallo, Pietro Michiardi</strong></p>
<p>Diffusion models for Text-to-Image (T2I) conditional generation have recently achieved tremendous success. Yet, aligning these models with userâ€™s intentions still involves a laborious trial-and-error process, and this challenging alignment problem has attracted considerable attention from the research community. In this work, instead of relying on fine-grained linguistic analyses of prompts, human annotation, or auxiliary vision-language models, we use Mutual Information (MI) to guide model alignment. In brief, our method uses self-supervised fine-tuning and relies on a point-wise (MI) estimation between prompts and images to create a synthetic fine-tuning set for improving model alignment. Our analysis indicates that our method is superior to the state-of-the-art, yet it only requires the pre-trained denoising network of the T2I model itself to estimate MI, and a simple fine-tuning strategy that improves alignment while maintaining image quality. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰çš„æ¡ä»¶ç”Ÿæˆæ‰©æ•£æ¨¡å‹è¿‘æœŸå–å¾—äº†å·¨å¤§æˆåŠŸã€‚ç„¶è€Œï¼Œå°†è¿™äº›æ¨¡å‹ä¸ç”¨æˆ·æ„å›¾å¯¹é½ä»ç„¶éœ€è¦ç¹ççš„è¯•é”™è¿‡ç¨‹ï¼Œè¿™ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„å¯¹é½é—®é¢˜å·²å¼•èµ·äº†ç ”ç©¶ç•Œçš„å¹¿æ³›å…³æ³¨ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ²¡æœ‰ä¾èµ–æç¤ºçš„ç²¾ç»†è¯­è¨€åˆ†æã€äººå·¥æ ‡æ³¨æˆ–è¾…åŠ©çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œè€Œæ˜¯ä½¿ç”¨äº’ä¿¡æ¯ï¼ˆMIï¼‰æ¥å¼•å¯¼æ¨¡å‹å¯¹é½ã€‚ç®€è€Œè¨€ä¹‹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨è‡ªç›‘ç£å¾®è°ƒï¼Œå¹¶ä¾èµ–äºæç¤ºå’Œå›¾åƒä¹‹é—´çš„ç‚¹ï¼ˆMIï¼‰ä¼°è®¡æ¥åˆ›å»ºä¸€ä¸ªåˆæˆå¾®è°ƒé›†ï¼Œä»¥æé«˜æ¨¡å‹å¯¹é½èƒ½åŠ›ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œä½†å®ƒåªéœ€è¦T2Iæ¨¡å‹æœ¬èº«çš„é¢„è®­ç»ƒå»å™ªç½‘ç»œæ¥ä¼°è®¡MIï¼Œä»¥åŠä¸€ç§ç®€å•çš„å¾®è°ƒç­–ç•¥ï¼Œå¯ä»¥åœ¨ä¿æŒå›¾åƒè´¨é‡çš„åŒæ—¶æé«˜å¯¹é½ç²¾åº¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.20759v2">PDF</a> to appear at ICLR25</p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ–‡æœ¬çš„æ‰©æ•£æ¨¡å‹ï¼ˆT2Iï¼‰æ¡ä»¶ç”Ÿæˆå·²å–å¾—äº†å·¨å¤§æˆåŠŸï¼Œä½†åœ¨ä¸ç”¨æˆ·æ„å›¾å¯¹é½æ—¶ä»é¢ä¸´ç¹ççš„è¯•é”™è¿‡ç¨‹ã€‚æœ¬ç ”ç©¶é‡‡ç”¨äº’ä¿¡æ¯ï¼ˆMIï¼‰å¼•å¯¼æ¨¡å‹å¯¹é½ï¼Œæ— éœ€ä¾èµ–ç²¾ç»†çš„è¯­è¨€åˆ†ææç¤ºã€äººå·¥æ ‡æ³¨æˆ–è¾…åŠ©è§†è§‰è¯­è¨€æ¨¡å‹ã€‚é€šè¿‡è‡ªç›‘ç£å¾®è°ƒå¹¶åˆ©ç”¨æç¤ºä¸å›¾åƒä¹‹é—´çš„ç‚¹çº§äº’ä¿¡æ¯ä¼°è®¡ï¼Œåˆ›å»ºåˆæˆå¾®è°ƒé›†ä»¥æ”¹å–„æ¨¡å‹å¯¹é½ã€‚åˆ†æè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œä»…ä¾èµ–T2Iæ¨¡å‹çš„é¢„è®­ç»ƒå»å™ªç½‘ç»œè¿›è¡Œäº’ä¿¡æ¯ä¼°è®¡ï¼Œé‡‡ç”¨ç®€å•çš„å¾®è°ƒç­–ç•¥æé«˜å¯¹é½åº¦åŒæ—¶ä¿æŒå›¾åƒè´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹å·²å–å¾—æ˜¾è‘—æˆåŠŸã€‚</li>
<li>å¯¹é½æ¨¡å‹ä¸ç”¨æˆ·æ„å›¾ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œéœ€è¦é‡‡ç”¨æ–°çš„æ–¹æ³•è§£å†³ã€‚</li>
<li>æœ¬ç ”ç©¶ä½¿ç”¨äº’ä¿¡æ¯ï¼ˆMIï¼‰æ¥å¼•å¯¼æ¨¡å‹å¯¹é½ï¼Œæ— éœ€ä¾èµ–ç²¾ç»†çš„è¯­è¨€åˆ†ææˆ–äººå·¥æ ‡æ³¨ã€‚</li>
<li>é€šè¿‡è‡ªç›‘ç£å¾®è°ƒæ”¹å–„æ¨¡å‹å¯¹é½ã€‚</li>
<li>åˆ©ç”¨æç¤ºå’Œå›¾åƒä¹‹é—´çš„ç‚¹çº§äº’ä¿¡æ¯ä¼°è®¡æ¥åˆ›å»ºåˆæˆå¾®è°ƒé›†ã€‚</li>
<li>è¯¥æ–¹æ³•ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
<li>è¯¥æ–¹æ³•ä»…ä¾èµ–T2Iæ¨¡å‹çš„é¢„è®­ç»ƒå»å™ªç½‘ç»œè¿›è¡Œäº’ä¿¡æ¯ä¼°è®¡ï¼Œå¹¶é‡‡ç”¨ç®€å•çš„å¾®è°ƒç­–ç•¥ç»´æŒå›¾åƒè´¨é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.20759">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d84c99d3168f3eb9c0f0c3f202c1e724.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0d1e5e8d09ae713de8b8e529cdffd3a7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2da240faba8287e57de1f9dd892c78f3.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="An-Item-is-Worth-a-Prompt-Versatile-Image-Editing-with-Disentangled-Control"><a href="#An-Item-is-Worth-a-Prompt-Versatile-Image-Editing-with-Disentangled-Control" class="headerlink" title="An Item is Worth a Prompt: Versatile Image Editing with Disentangled   Control"></a>An Item is Worth a Prompt: Versatile Image Editing with Disentangled   Control</h2><p><strong>Authors:Aosong Feng, Weikang Qiu, Jinbin Bai, Xiao Zhang, Zhen Dong, Kaicheng Zhou, Rex Ying, Leandros Tassiulas</strong></p>
<p>Building on the success of text-to-image diffusion models (DPMs), image editing is an important application to enable human interaction with AI-generated content. Among various editing methods, editing within the prompt space gains more attention due to its capacity and simplicity of controlling semantics. However, since diffusion models are commonly pretrained on descriptive text captions, direct editing of words in text prompts usually leads to completely different generated images, violating the requirements for image editing. On the other hand, existing editing methods usually consider introducing spatial masks to preserve the identity of unedited regions, which are usually ignored by DPMs and therefore lead to inharmonic editing results. Targeting these two challenges, in this work, we propose to disentangle the comprehensive image-prompt interaction into several item-prompt interactions, with each item linked to a special learned prompt. The resulting framework, named D-Edit, is based on pretrained diffusion models with cross-attention layers disentangled and adopts a two-step optimization to build item-prompt associations. Versatile image editing can then be applied to specific items by manipulating the corresponding prompts. We demonstrate state-of-the-art results in four types of editing operations including image-based, text-based, mask-based editing, and item removal, covering most types of editing applications, all within a single unified framework. Notably, D-Edit is the first framework that can (1) achieve item editing through mask editing and (2) combine image and text-based editing. We demonstrate the quality and versatility of the editing results for a diverse collection of images through both qualitative and quantitative evaluations. </p>
<blockquote>
<p>åŸºäºæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼ˆDPMsï¼‰çš„æˆåŠŸï¼Œå›¾åƒç¼–è¾‘æ˜¯ä¸AIç”Ÿæˆå†…å®¹è¿›è¡Œäº¤äº’çš„é‡è¦åº”ç”¨ã€‚åœ¨å„ç§ç¼–è¾‘æ–¹æ³•ä¸­ï¼Œæç¤ºç©ºé—´å†…çš„ç¼–è¾‘å› å…¶æ§åˆ¶è¯­ä¹‰çš„èƒ½åŠ›å’Œç®€æ´æ€§è€Œå—åˆ°æ›´å¤šå…³æ³¨ã€‚ç„¶è€Œï¼Œç”±äºæ‰©æ•£æ¨¡å‹é€šå¸¸æ˜¯åœ¨æè¿°æ€§æ–‡æœ¬æ ‡é¢˜ä¸Šè¿›è¡Œé¢„è®­ç»ƒçš„ï¼Œç›´æ¥åœ¨æ–‡æœ¬æç¤ºä¸­è¿›è¡Œç¼–è¾‘é€šå¸¸ä¼šå¯¼è‡´ç”Ÿæˆçš„å›¾åƒå®Œå…¨ä¸åŒï¼Œè¿™è¿åäº†å›¾åƒç¼–è¾‘çš„è¦æ±‚ã€‚å¦ä¸€æ–¹é¢ï¼Œç°æœ‰çš„ç¼–è¾‘æ–¹æ³•é€šå¸¸è€ƒè™‘å¼•å…¥ç©ºé—´æ©è†œæ¥ä¿ç•™æœªç¼–è¾‘åŒºåŸŸçš„èº«ä»½ï¼Œè€Œè¿™äº›é€šå¸¸è¢«DPMså¿½ç•¥ï¼Œä»è€Œå¯¼è‡´ä¸å’Œè°çš„ç¼–è¾‘ç»“æœã€‚é’ˆå¯¹è¿™ä¸¤ä¸ªæŒ‘æˆ˜ï¼Œåœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºå°†å…¨é¢çš„å›¾åƒæç¤ºäº¤äº’åˆ†è§£ä¸ºå¤šä¸ªé¡¹ç›®æç¤ºäº¤äº’ï¼Œæ¯ä¸ªé¡¹ç›®ä¸ç‰¹æ®Šçš„é¢„è®­ç»ƒæç¤ºç›¸å…³è”ã€‚å¾—åˆ°çš„æ¡†æ¶è¢«ç§°ä¸ºD-Editï¼Œå®ƒåŸºäºé¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡è§£å¼€äº¤å‰æ³¨æ„å±‚æ¥æ„å»ºç‰©å“æç¤ºå…³è”çš„ä¸¤æ­¥ä¼˜åŒ–ã€‚ç„¶åï¼Œå¯ä»¥é€šè¿‡æ“çºµç›¸åº”çš„æç¤ºæ¥å¯¹ç‰¹å®šé¡¹ç›®è¿›è¡Œçµæ´»çš„å›¾åƒç¼–è¾‘ã€‚æˆ‘ä»¬åœ¨å››ç§ç¼–è¾‘æ“ä½œä¸­å±•ç¤ºäº†æœ€å…ˆè¿›çš„æˆæœï¼ŒåŒ…æ‹¬åŸºäºå›¾åƒçš„ç¼–è¾‘ã€åŸºäºæ–‡æœ¬çš„ç¼–è¾‘ã€åŸºäºæ©è†œçš„ç¼–è¾‘å’Œé¡¹ç›®ç§»é™¤ï¼Œæ¶µç›–äº†å¤§å¤šæ•°ç±»å‹çš„ç¼–è¾‘åº”ç”¨ï¼Œéƒ½åœ¨ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶å†…å®Œæˆã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒD-Editæ˜¯ç¬¬ä¸€ä¸ªèƒ½å¤Ÿï¼ˆ1ï¼‰é€šè¿‡æ©è†œç¼–è¾‘å®ç°é¡¹ç›®ç¼–è¾‘å¹¶ç»“åˆå›¾åƒå’Œæ–‡æœ¬åŸºç¡€çš„ç¼–è¾‘æ¡†æ¶ã€‚æˆ‘ä»¬é€šè¿‡å®šæ€§å’Œå®šé‡è¯„ä¼°ï¼Œå±•ç¤ºäº†ç¼–è¾‘ç»“æœçš„è´¨é‡å’Œå¤šæ ·æ€§ï¼Œé€‚ç”¨äºå„ç§å›¾åƒé›†åˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.04880v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼ˆDPMsï¼‰çš„æˆåŠŸï¼Œå›¾åƒç¼–è¾‘æ˜¯ä¸AIç”Ÿæˆå†…å®¹è¿›è¡Œäº¤äº’çš„é‡è¦åº”ç”¨ä¹‹ä¸€ã€‚æœ¬æ–‡å…³æ³¨æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç¼–è¾‘ä¸­çš„ä¸¤ä¸ªæŒ‘æˆ˜ï¼šæ–‡æœ¬æç¤ºç©ºé—´å†…çš„ç¼–è¾‘å’Œæœªç¼–è¾‘åŒºåŸŸçš„èº«ä»½ä¿ç•™ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºD-Editçš„æ¡†æ¶ï¼ŒåŸºäºé¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡åˆ†è§£å›¾åƒä¸æç¤ºçš„äº¤äº’ä¸ºå¤šä¸ªç‰©å“ä¸æç¤ºçš„äº¤äº’æ¥è§£å†³è¿™ä¸¤ä¸ªæŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶é‡‡ç”¨ä¸¤æ­¥ä¼˜åŒ–æ¥å»ºç«‹ç‰©å“ä¸æç¤ºçš„å…³è”ï¼Œå¯ä»¥é€šè¿‡æ“ä½œç›¸åº”çš„æç¤ºæ¥å¯¹ç‰¹å®šç‰©å“è¿›è¡Œå¤šæ ·åŒ–çš„å›¾åƒç¼–è¾‘ã€‚D-Editåœ¨å››ç§ç±»å‹çš„ç¼–è¾‘æ“ä½œï¼ˆåŸºäºå›¾åƒã€åŸºäºæ–‡æœ¬ã€åŸºäºæ©ç çš„ç¼–è¾‘ä»¥åŠç‰©å“ç§»é™¤ï¼‰ä¸­å‡å±•ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œä¸”æ˜¯å”¯ä¸€èƒ½å¤Ÿç»“åˆå›¾åƒå’Œæ–‡æœ¬è¿›è¡Œç¼–è¾‘çš„æ¡†æ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç¼–è¾‘ä¸­é¢ä¸´ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šæ–‡æœ¬æç¤ºç©ºé—´å†…çš„ç¼–è¾‘å’Œä¿ç•™æœªç¼–è¾‘åŒºåŸŸçš„èº«ä»½ã€‚</li>
<li>D-Editæ¡†æ¶è§£å†³äº†è¿™ä¸¤ä¸ªæŒ‘æˆ˜ï¼Œé€šè¿‡å°†å›¾åƒä¸æç¤ºçš„äº¤äº’åˆ†è§£ä¸ºå¤šä¸ªç‰©å“ä¸æç¤ºçš„äº¤äº’ã€‚</li>
<li>D-EditåŸºäºé¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ï¼Œé‡‡ç”¨ä¸¤æ­¥ä¼˜åŒ–å»ºç«‹ç‰©å“ä¸æç¤ºçš„å…³è”ã€‚</li>
<li>D-Editæ”¯æŒå¤šæ ·åŒ–çš„å›¾åƒç¼–è¾‘ï¼Œé€šè¿‡æ“ä½œç›¸åº”çš„æç¤ºæ¥å¯¹ç‰¹å®šç‰©å“è¿›è¡Œç¼–è¾‘ã€‚</li>
<li>D-Editåœ¨å¤šç§ç±»å‹çš„ç¼–è¾‘æ“ä½œä¸­å±•ç°å‡ºå“è¶Šæ€§èƒ½ï¼ŒåŒ…æ‹¬å›¾åƒã€æ–‡æœ¬ã€æ©ç ç¼–è¾‘ä»¥åŠç‰©å“ç§»é™¤ã€‚</li>
<li>D-Editæ˜¯é¦–ä¸ªèƒ½å¤Ÿç»“åˆå›¾åƒå’Œæ–‡æœ¬è¿›è¡Œç¼–è¾‘çš„æ¡†æ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.04880">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bc47436d52e6e16d60fca5dd9beb685e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-82c02189750afd53c0f44d1986cd5125.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d3a17b3eb0f89e44d0d51c7f4ec0fc59.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-687ce5a789311219795d2bb3bbc43ea2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1097014ca847b9e29fb0d83966213471.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a0fcd8709b962e9f24b159d3cdf70014.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-85f81725edf2946dbf7d43895263e1e5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-038379016dd7eca327254cbba05f382f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5be8e8c47a6563d351c9839a9083d171.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-29/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-29/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-29/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-ee77dd7964417233a4a5362a8846566c.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-29  Lightweight Weighted Average Ensemble Model for Pneumonia Detection in   Chest X-Ray Images
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-29
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-29/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-b1158380bab7cb28fb237f42c24c0038.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-29  LinPrim Linear Primitives for Differentiable Volumetric Rendering
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-29
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">18723.3k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
