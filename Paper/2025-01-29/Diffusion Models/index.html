<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-01-29  UDBE Unsupervised Diffusion-based Brightness Enhancement in Underwater   Images">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-1097014ca847b9e29fb0d83966213471.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-01-29
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-02-12
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    14.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    58 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-01-29-更新"><a href="#2025-01-29-更新" class="headerlink" title="2025-01-29 更新"></a>2025-01-29 更新</h1><h2 id="UDBE-Unsupervised-Diffusion-based-Brightness-Enhancement-in-Underwater-Images"><a href="#UDBE-Unsupervised-Diffusion-based-Brightness-Enhancement-in-Underwater-Images" class="headerlink" title="UDBE: Unsupervised Diffusion-based Brightness Enhancement in Underwater   Images"></a>UDBE: Unsupervised Diffusion-based Brightness Enhancement in Underwater   Images</h2><p><strong>Authors:Tatiana Taís Schein, Gustavo Pereira de Almeira, Stephanie Loi Brião, Rodrigo Andrade de Bem, Felipe Gomes de Oliveira, Paulo L. J. Drews-Jr</strong></p>
<p>Activities in underwater environments are paramount in several scenarios, which drives the continuous development of underwater image enhancement techniques. A major challenge in this domain is the depth at which images are captured, with increasing depth resulting in a darker environment. Most existing methods for underwater image enhancement focus on noise removal and color adjustment, with few works dedicated to brightness enhancement. This work introduces a novel unsupervised learning approach to underwater image enhancement using a diffusion model. Our method, called UDBE, is based on conditional diffusion to maintain the brightness details of the unpaired input images. The input image is combined with a color map and a Signal-Noise Relation map (SNR) to ensure stable training and prevent color distortion in the output images. The results demonstrate that our approach achieves an impressive accuracy rate in the datasets UIEB, SUIM and RUIE, well-established underwater image benchmarks. Additionally, the experiments validate the robustness of our approach, regarding the image quality metrics PSNR, SSIM, UIQM, and UISM, indicating the good performance of the brightness enhancement process. The source code is available here: <a target="_blank" rel="noopener" href="https://github.com/gusanagy/UDBE">https://github.com/gusanagy/UDBE</a>. </p>
<blockquote>
<p>在水下环境中的应用在各种场景中至关重要，这推动了水下图像增强技术的不断发展。此领域的一个主要挑战是图像捕获的深度，随着深度的增加，环境会变暗。大多数现有的水下图像增强方法主要关注去噪和色彩调整，很少有工作专注于亮度增强。这项工作引入了一种使用扩散模型进行水下图像增强的新型无监督学习方法。我们的方法称为UDBE，它基于条件扩散来保持非配对输入图像的亮度细节。输入图像与色图和信号噪声关系图（SNR）相结合，以确保稳定的训练并防止输出图像中的颜色失真。结果表明，我们的方法在UIEB、SUIM和RUIE等水下图像基准测试集上取得了令人印象深刻的准确率。此外，实验验证了我们方法在图像质量指标PSNR、SSIM、UIQM和UISM方面的稳健性，表明亮度增强过程表现良好。源代码可在：<a target="_blank" rel="noopener" href="https://github.com/gusanagy/UDBE%E5%A4%84%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/gusanagy/UDBE处获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.16211v1">PDF</a> Paper presented at ICMLA 2024</p>
<p><strong>Summary</strong></p>
<p>水下环境在多种场景中都极为重要，推动了水下图像增强技术的持续发展。在此领域的一大挑战是图像捕获的深度，随着深度的增加，环境会变暗。目前大多数水下图像增强方法主要关注去噪和色彩调整，很少有工作致力于亮度增强。本文引入了一种使用扩散模型进行水下图像增强的新型无监督学习方法。所提方法UDBE基于条件扩散，以保持未配对输入图像的亮度细节。输入图像与色彩图和信噪比关系图（SNR）相结合，以确保训练稳定并防止输出图像出现色彩失真。结果证明，该方法在UIEB、SUIM和RUIE等水下图像基准数据集上实现了较高的准确率。实验还验证了该方法在图像质量指标PSNR、SSIM、UIQM和UISM方面的稳健性，表明亮度增强过程表现良好。源代码可在<a target="_blank" rel="noopener" href="https://github.com/gusanagy/UDBE">https://github.com/gusanagy/UDBE</a>获取。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>水下图像增强对于多种应用场景至关重要，尤其随着水下活动的不断增加。</li>
<li>当前技术挑战在于处理不同水深带来的环境变暗问题。</li>
<li>现有方法主要关注去噪和色彩调整，而亮度的增强则相对被忽视。</li>
<li>论文提出了一种新型的无监督学习方法UDBE，基于条件扩散模型进行水下图像增强。</li>
<li>UDBE能够保持输入图像的亮度细节，结合色彩图和SNR来确保训练稳定性和图像质量。</li>
<li>UDBE在多个水下图像基准数据集上实现了较高的准确率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.16211">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-75f556dec914c9c3239368a6cabf65ea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ac8b86c7f1d848b6379130169ca4be4d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5767c33ac04b4c6c1892926057c8dc0e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-838d928090b481eb7a2ee1183fccbcdf.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="BAG-Body-Aligned-3D-Wearable-Asset-Generation"><a href="#BAG-Body-Aligned-3D-Wearable-Asset-Generation" class="headerlink" title="BAG: Body-Aligned 3D Wearable Asset Generation"></a>BAG: Body-Aligned 3D Wearable Asset Generation</h2><p><strong>Authors:Zhongjin Luo, Yang Li, Mingrui Zhang, Senbo Wang, Han Yan, Xibin Song, Taizhang Shang, Wei Mao, Hongdong Li, Xiaoguang Han, Pan Ji</strong></p>
<p>While recent advancements have shown remarkable progress in general 3D shape generation models, the challenge of leveraging these approaches to automatically generate wearable 3D assets remains unexplored. To this end, we present BAG, a Body-aligned Asset Generation method to output 3D wearable asset that can be automatically dressed on given 3D human bodies. This is achived by controlling the 3D generation process using human body shape and pose information. Specifically, we first build a general single-image to consistent multiview image diffusion model, and train it on the large Objaverse dataset to achieve diversity and generalizability. Then we train a Controlnet to guide the multiview generator to produce body-aligned multiview images. The control signal utilizes the multiview 2D projections of the target human body, where pixel values represent the XYZ coordinates of the body surface in a canonical space. The body-conditioned multiview diffusion generates body-aligned multiview images, which are then fed into a native 3D diffusion model to produce the 3D shape of the asset. Finally, by recovering the similarity transformation using multiview silhouette supervision and addressing asset-body penetration with physics simulators, the 3D asset can be accurately fitted onto the target human body. Experimental results demonstrate significant advantages over existing methods in terms of image prompt-following capability, shape diversity, and shape quality. Our project page is available at <a target="_blank" rel="noopener" href="https://bag-3d.github.io/">https://bag-3d.github.io/</a>. </p>
<blockquote>
<p>尽管最近的进展在通用3D形状生成模型方面取得了显著的进步，但利用这些方法自动生成可穿戴3D资产的挑战仍然未被探索。为此，我们提出了BAG（Body-aligned Asset Generation）方法，它可输出能够在给定3D人体上自动穿戴的3D可穿戴资产。这是通过控制使用人体形状和姿势信息的3D生成过程来实现的。具体来说，我们首先构建了一个通用的单图像到一致的多视图图像扩散模型，并在大型Objaverse数据集上进行训练，以实现多样性和通用性。然后，我们训练了一个Controlnet来指导多视图生成器产生与身体对齐的多视图图像。控制信号利用目标人体的多视图2D投影，其中像素值代表标准空间中体表面的XYZ坐标。以身体为条件的多视图扩散产生与身体对齐的多视图图像，然后将其输入到本地3D扩散模型中，以产生资产的三维形状。最后，通过利用多视图轮廓监督恢复相似变换并借助物理模拟器解决资产与身体之间的穿透问题，可以准确地将3D资产匹配到目标人体上。实验结果表明，在图像提示遵循能力、形状多样性和形状质量方面，与现有方法相比具有显著优势。我们的项目页面可在<a target="_blank" rel="noopener" href="https://bag-3d.github.io找到./">https://bag-3d.github.io/找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.16177v1">PDF</a> video: <a target="_blank" rel="noopener" href="https://youtu.be/XJtG82LjQKc">https://youtu.be/XJtG82LjQKc</a></p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种名为BAG的自动生成可穿戴3D资产的方法。该方法利用人体形态和姿态信息控制3D生成过程，实现自动将给定的3D人体穿上服装的效果。通过构建单图像到一致的多视角图像扩散模型，并在Objaverse数据集上进行训练，实现了多样性和泛化能力。训练控制网络Controlnet来指导多视角生成器生成与人体对齐的多视角图像，然后通过3D扩散模型生成资产的三维形状。最后，通过多视角轮廓监督恢复相似变换并解决资产与人体之间的穿透问题，使用物理模拟器准确地将3D资产适配到目标人体上。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>BAG方法实现了自动生成可穿戴3D资产的能力，解决了现有方法的挑战。</li>
<li>通过构建单图像到多视角图像扩散模型，实现了多样性和泛化能力。</li>
<li>控制网络Controlnet用于指导多视角生成器产生与人体对齐的图像。</li>
<li>利用人体形态和姿态信息的控制信号指导扩散过程，生成与人体对齐的多视角图像。</li>
<li>通过多视角轮廓监督恢复相似变换，解决了资产与人体之间的穿透问题。</li>
<li>使用物理模拟器将3D资产准确适配到目标人体上。</li>
<li>实验结果表明，BAG方法在图像提示遵循能力、形状多样性和形状质量方面均有显著优势。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.16177">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-4677ea409dc4b99921866e845cd10320.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3791c8a708880ef14f6366754671c2cf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-213c9ba104018c1cf5ad0cff40c58f36.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-17678ba83fe412c8a93cb9b40c6b52e0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6a65626cfab76fdb49d38ef6c082022e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-755cbb59ef4acbe2a718644dff98c5d4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bc735c5b5344a1b0e7708bc001c09c68.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Efficient-Portrait-Matte-Creation-With-Layer-Diffusion-and-Connectivity-Priors"><a href="#Efficient-Portrait-Matte-Creation-With-Layer-Diffusion-and-Connectivity-Priors" class="headerlink" title="Efficient Portrait Matte Creation With Layer Diffusion and Connectivity   Priors"></a>Efficient Portrait Matte Creation With Layer Diffusion and Connectivity   Priors</h2><p><strong>Authors:Zhiyuan Lu, Hao Lu, Hua Huang</strong></p>
<p>Learning effective deep portrait matting models requires training data of both high quality and large quantity. Neither quality nor quantity can be easily met for portrait matting, however. Since the most accurate ground-truth portrait mattes are acquired in front of the green screen, it is almost impossible to harvest a large-scale portrait matting dataset in reality. This work shows that one can leverage text prompts and the recent Layer Diffusion model to generate high-quality portrait foregrounds and extract latent portrait mattes. However, the portrait mattes cannot be readily in use due to significant generation artifacts. Inspired by the connectivity priors observed in portrait images, that is, the border of portrait foregrounds always appears connected, a connectivity-aware approach is introduced to refine portrait mattes. Building on this, a large-scale portrait matting dataset is created, termed LD-Portrait-20K, with $20,051$ portrait foregrounds and high-quality alpha mattes. Extensive experiments demonstrated the value of the LD-Portrait-20K dataset, with models trained on it significantly outperforming those trained on other datasets. In addition, comparisons with the chroma keying algorithm and an ablation study on dataset capacity further confirmed the effectiveness of the proposed matte creation approach. Further, the dataset also contributes to state-of-the-art video portrait matting, implemented by simple video segmentation and a trimap-based image matting model trained on this dataset. </p>
<blockquote>
<p>学习有效的深度肖像抠图模型需要高质量且大量的训练数据。然而，肖像抠图很难同时满足质量和数量的要求。由于最准确的地面真实肖像抠图是是在绿幕前获取的，因此在现实中几乎不可能收集大规模的肖像抠图数据集。这项工作表明，可以利用文本提示和最新的层扩散模型生成高质量的肖像前景并提取潜在肖像抠图。然而，由于存在明显的生成瑕疵，这些肖像抠图不能直接使用。受肖像图像中观察到的连通性先验的启发，即肖像前景的边界总是相互连接的，引入了一种连通性感知方法来优化肖像抠图。在此基础上，创建了一个大规模的肖像抠图数据集，称为LD-Portrait-20K，包含20,051个肖像前景和高质量alpha抠图。大量实验证明了LD-Portrait-20K数据集的价值，在此数据集上训练的模型显著优于在其他数据集上训练的模型。此外，与色键算法的比较以及对数据集容量的消融研究进一步证实了所提出的抠图创建方法的有效性。此外，该数据集还为先进的视频肖像抠图做出了贡献，通过简单的视频分割和基于此数据集训练的基于修边图的图像抠图模型实现。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.16147v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了深度肖像抠图模型的学习需求及挑战。为解决高质量大规模训练数据的获取难题，研究利用文本提示和Layer Diffusion模型生成高质量肖像前景，并引入连接性感知方法优化生成的肖像抠图。为此创建了一个大规模肖像抠图数据集LD-Portrait-20K，实验证明该数据集的有效性，并在视频肖像抠图中得到应用。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>深度肖像抠图模型需要高质量大规模的训练数据。</li>
<li>由于现实条件下获取准确地面真实肖像抠图数据的困难，研究利用文本提示和Layer Diffusion模型生成高质量肖像前景。</li>
<li>生成的肖像抠图存在生成瑕疵，引入连接性感知方法来优化。</li>
<li>创建了一个大规模的肖像抠图数据集LD-Portrait-20K，包含20,051个肖像前景和高质量alpha抠图。</li>
<li>实验证明LD-Portrait-20K数据集的有效性，训练在此数据集上的模型性能显著优于其他数据集。</li>
<li>与色键算法的比较及数据集容量的消融研究进一步证实了所提出的抠图创建方法的有效性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.16147">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-0c8af10f4be01f776f500ebce06a8701.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7c08d7e1fa3ab3f6635a8e5bca6aa1bf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c73315e1923de811f5e590a38749a33f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-11dfb51d158677ba34c22b0cd445508f.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="MatCLIP-Light-and-Shape-Insensitive-Assignment-of-PBR-Material-Models"><a href="#MatCLIP-Light-and-Shape-Insensitive-Assignment-of-PBR-Material-Models" class="headerlink" title="MatCLIP: Light- and Shape-Insensitive Assignment of PBR Material Models"></a>MatCLIP: Light- and Shape-Insensitive Assignment of PBR Material Models</h2><p><strong>Authors:Michael Birsak, John Femiani, Biao Zhang, Peter Wonka</strong></p>
<p>Assigning realistic materials to 3D models remains a significant challenge in computer graphics. We propose MatCLIP, a novel method that extracts shape- and lighting-insensitive descriptors of Physically Based Rendering (PBR) materials to assign plausible textures to 3D objects based on images, such as the output of Latent Diffusion Models (LDMs) or photographs. Matching PBR materials to static images is challenging because the PBR representation captures the dynamic appearance of materials under varying viewing angles, shapes, and lighting conditions. By extending an Alpha-CLIP-based model on material renderings across diverse shapes and lighting, and encoding multiple viewing conditions for PBR materials, our approach generates descriptors that bridge the domains of PBR representations with photographs or renderings, including LDM outputs. This enables consistent material assignments without requiring explicit knowledge of material relationships between different parts of an object. MatCLIP achieves a top-1 classification accuracy of 76.6%, outperforming state-of-the-art methods such as PhotoShape and MatAtlas by over 15 percentage points on publicly available datasets. Our method can be used to construct material assignments for 3D shape datasets such as ShapeNet, 3DCoMPaT++, and Objaverse. All code and data will be released. </p>
<blockquote>
<p>为三维模型分配现实材料仍然是计算机图形学中的一项重大挑战。我们提出了一种名为MatCLIP的新方法，该方法提取基于物理渲染（PBR）材料的形状和光照不敏感描述符，根据图像（如潜在扩散模型（LDM）的输出或照片）为三维对象分配合理的纹理。将PBR材料与静态图像相匹配是具有挑战性的，因为PBR表示捕捉了在各种观看角度、形状和光照条件下材料的动态外观。我们通过扩展基于Alpha-CLIP的模型，在多种形状和光照下进行材料渲染，并对PBR材料进行多视角编码，产生描述符，架起PBR表示与照片或渲染图像（包括LDM输出）之间的桥梁。这实现了材料分配的一致性，无需明确对象不同部分之间的材料关系知识。MatCLIP在公开数据集上取得了76.6%的top-1分类准确率，相较于当前的主流方法PhotoShape和MatAtlas在公开数据集上的准确率有超过15个百分点的提升。我们的方法可用于构建如ShapeNet、3DCoMPaT++和Objaverse等三维形状数据集的材料分配。所有代码和数据都将发布。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.15981v1">PDF</a> Preprint, 10 pages</p>
<p><strong>Summary</strong></p>
<p>本文提出一种名为MatCLIP的新方法，用于为三维模型分配真实材料。该方法提取物理基础渲染（PBR）材料的形状和光照不敏感描述符，根据图像（如潜在扩散模型（LDM）的输出或照片）为3D对象分配合理的纹理。MatCLIP通过扩展Alpha-CLIP模型在材料渲染方面的应用，并编码PBR材料的多视角条件，生成了连接PBR表示与照片或渲染的图像描述符。这种方法可以在不需要明确了解对象各部分材料关系的情况下，实现一致的材料分配，准确率达到了76.6%，在公开数据集上较其他先进方法如PhotoShape和MatAtlas高出超过15个百分点。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MatCLIP是一种新型方法，用于为三维模型分配真实材料，特别适用于物理基础渲染（PBR）材料。</li>
<li>MatCLIP通过提取形状和光照不敏感描述符，根据图像为3D对象分配纹理。</li>
<li>MatCLIP利用Alpha-CLIP模型处理材料渲染，并编码多种视角条件和光照条件下的PBR材料。</li>
<li>MatCLIP生成图像描述符，可连接PBR表示与照片或渲染的图像。</li>
<li>MatCLIP实现了在不了解对象各部分材料关系的情况下的一致材料分配。</li>
<li>MatCLIP的准确率达到了76.6%，在公开数据集上的表现优于其他先进方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.15981">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-206c36eaf8fa845cd87d86d91a56b8f1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f038661c5788df6595b9a2a04da5d393.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5fa1a001e19e24666468e4ded88fe119.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4f60e85a153ae619fac25226a0fa1c77.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-113861cea2fee3d09a8d1c82cb7b8935.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Slot-Guided-Adaptation-of-Pre-trained-Diffusion-Models-for-Object-Centric-Learning-and-Compositional-Generation"><a href="#Slot-Guided-Adaptation-of-Pre-trained-Diffusion-Models-for-Object-Centric-Learning-and-Compositional-Generation" class="headerlink" title="Slot-Guided Adaptation of Pre-trained Diffusion Models for   Object-Centric Learning and Compositional Generation"></a>Slot-Guided Adaptation of Pre-trained Diffusion Models for   Object-Centric Learning and Compositional Generation</h2><p><strong>Authors:Adil Kaan Akan, Yucel Yemez</strong></p>
<p>We present SlotAdapt, an object-centric learning method that combines slot attention with pretrained diffusion models by introducing adapters for slot-based conditioning. Our method preserves the generative power of pretrained diffusion models, while avoiding their text-centric conditioning bias. We also incorporate an additional guidance loss into our architecture to align cross-attention from adapter layers with slot attention. This enhances the alignment of our model with the objects in the input image without using external supervision. Experimental results show that our method outperforms state-of-the-art techniques in object discovery and image generation tasks across multiple datasets, including those with real images. Furthermore, we demonstrate through experiments that our method performs remarkably well on complex real-world images for compositional generation, in contrast to other slot-based generative methods in the literature. The project page can be found at $\href{<a target="_blank" rel="noopener" href="https://kaanakan.github.io/SlotAdapt/%7D%7B/text%7Bthis">https://kaanakan.github.io/SlotAdapt/}{\text{this</a> https url}}$. </p>
<blockquote>
<p>我们提出了SlotAdapt，这是一种结合槽位注意力和预训练扩散模型的面向对象的学习方法，通过引入适配器来实现基于槽位的条件设置。我们的方法保留了预训练扩散模型的生成能力，同时避免了它们基于文本的条件设置的偏见。我们还在架构中加入了额外的指导损失，以调整适配器层的交叉注意力与槽位注意力的对齐。这增强了我们的模型与输入图像中的对象的对齐，而无需使用外部监督。实验结果表明，我们的方法在多个数据集上的对象发现和图像生成任务上优于最新技术，包括真实图像数据集。此外，通过实验证明，我们的方法在复杂的真实图像上的组合生成表现尤为出色，与文献中的其他基于槽位的生成方法形成鲜明对比。项目页面可在此处找到：[<a target="_blank" rel="noopener" href="https://kaanakan.github.io/SlotAdapt/]">https://kaanakan.github.io/SlotAdapt/]</a> 。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.15878v1">PDF</a> Accepted to ICLR2025.   $\href{<a target="_blank" rel="noopener" href="https://kaanakan.github.io/SlotAdapt/%7D%7B/text%7BProject">https://kaanakan.github.io/SlotAdapt/}{\text{Project</a> Page}}$</p>
<p><strong>Summary</strong></p>
<p>SlotAdapt是一种结合插槽注意力和预训练扩散模型的对象级学习方法，通过引入适配器实现插槽式条件。该方法保留了预训练扩散模型的生成能力，同时避免了其文本为中心的条件偏见。通过引入额外的指导损失，增强了模型与输入图像中对象的对齐度，无需外部监督。实验结果表明，该方法在多个数据集上的目标检测和图像生成任务上均优于现有技术，并且在复杂真实图像上的组合生成表现尤为出色。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SlotAdapt结合了插槽注意力和预训练的扩散模型，通过引入适配器实现基于插槽的条件学习。</li>
<li>该方法保留了预训练扩散模型的生成能力，并避免了文本为中心的条件偏见。</li>
<li>通过引入额外的指导损失，增强了模型与输入图像中对象的对齐精度。</li>
<li>SlotAdapt在多个数据集上的目标检测和图像生成任务上表现出卓越性能。</li>
<li>该方法在复杂真实图像上的组合生成表现尤为出色。</li>
<li>此方法实现了在不使用外部监督的情况下提高模型与对象对齐的能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.15878">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-6f29fa25e480904a8126f41bf31bd772.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-05a8b9e8034e74bb394fb6eb8b24573d.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Can-Location-Embeddings-Enhance-Super-Resolution-of-Satellite-Imagery"><a href="#Can-Location-Embeddings-Enhance-Super-Resolution-of-Satellite-Imagery" class="headerlink" title="Can Location Embeddings Enhance Super-Resolution of Satellite Imagery?"></a>Can Location Embeddings Enhance Super-Resolution of Satellite Imagery?</h2><p><strong>Authors:Daniel Panangian, Ksenia Bittner</strong></p>
<p>Publicly available satellite imagery, such as Sentinel- 2, often lacks the spatial resolution required for accurate analysis of remote sensing tasks including urban planning and disaster response. Current super-resolution techniques are typically trained on limited datasets, leading to poor generalization across diverse geographic regions. In this work, we propose a novel super-resolution framework that enhances generalization by incorporating geographic context through location embeddings. Our framework employs Generative Adversarial Networks (GANs) and incorporates techniques from diffusion models to enhance image quality. Furthermore, we address tiling artifacts by integrating information from neighboring images, enabling the generation of seamless, high-resolution outputs. We demonstrate the effectiveness of our method on the building segmentation task, showing significant improvements over state-of-the-art methods and highlighting its potential for real-world applications. </p>
<blockquote>
<p>公开可用的卫星图像，如Sentinel-2，通常缺乏进行城市规划、灾害应对等遥感任务精确分析所需的空间分辨率。当前的超分辨率技术通常局限于有限的数据集进行训练，导致在不同地理区域的泛化能力较差。在本研究中，我们提出了一种新型超分辨率框架，该框架通过引入位置嵌入来提升对地理上下文的利用，从而增强泛化能力。我们的框架采用生成对抗网络（GANs），并结合扩散模型的技巧来提升图像质量。此外，我们通过对邻近图像的信息进行整合来解决拼贴痕迹问题，从而生成无缝、高分辨率的输出。我们在建筑分割任务上展示了该方法的有效性，证明了其相较于最先进的方法有着显著的提升，并强调了其在现实世界应用中的潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.15847v1">PDF</a> Accepted to IEEE&#x2F;CVF Winter Conference on Applications of Computer   Vision (WACV)</p>
<p><strong>Summary</strong></p>
<p>本文提出一种新型的超分辨率框架，该框架结合地理上下文通过位置嵌入增强图像质量，并采用生成对抗网络（GANs）和扩散模型技术提高图像质量。此外，该框架解决了拼接产生的伪影问题，通过整合相邻图像的信息生成无缝、高分辨率的输出。在建筑分割任务上验证了该方法的有效性，显著优于现有方法，展现了其在真实世界应用中的潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出的新型超分辨率框架结合地理上下文通过位置嵌入增强图像质量。</li>
<li>利用生成对抗网络（GANs）和扩散模型技术提高图像质量。</li>
<li>解决了拼接产生的伪影问题。</li>
<li>通过整合相邻图像信息生成无缝、高分辨率的输出。</li>
<li>在建筑分割任务上验证了方法的有效性。</li>
<li>与现有方法相比有显著改进。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.15847">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b29432ffbd21e2744991a28cc165ad9e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-557fd575c806a577ff9095ae6b519170.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b14916c16d3d32298e69c2323e02fc21.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4b1919cb1b16c24a870540fda20c0312.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b766eac585b1fe425eabd4dfb6a861b6.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="StochSync-Stochastic-Diffusion-Synchronization-for-Image-Generation-in-Arbitrary-Spaces"><a href="#StochSync-Stochastic-Diffusion-Synchronization-for-Image-Generation-in-Arbitrary-Spaces" class="headerlink" title="StochSync: Stochastic Diffusion Synchronization for Image Generation in   Arbitrary Spaces"></a>StochSync: Stochastic Diffusion Synchronization for Image Generation in   Arbitrary Spaces</h2><p><strong>Authors:Kyeongmin Yeo, Jaihoon Kim, Minhyuk Sung</strong></p>
<p>We propose a zero-shot method for generating images in arbitrary spaces (e.g., a sphere for 360{\deg} panoramas and a mesh surface for texture) using a pretrained image diffusion model. The zero-shot generation of various visual content using a pretrained image diffusion model has been explored mainly in two directions. First, Diffusion Synchronization-performing reverse diffusion processes jointly across different projected spaces while synchronizing them in the target space-generates high-quality outputs when enough conditioning is provided, but it struggles in its absence. Second, Score Distillation Sampling-gradually updating the target space data through gradient descent-results in better coherence but often lacks detail. In this paper, we reveal for the first time the interconnection between these two methods while highlighting their differences. To this end, we propose StochSync, a novel approach that combines the strengths of both, enabling effective performance with weak conditioning. Our experiments demonstrate that StochSync provides the best performance in 360{\deg} panorama generation (where image conditioning is not given), outperforming previous finetuning-based methods, and also delivers comparable results in 3D mesh texturing (where depth conditioning is provided) with previous methods. </p>
<blockquote>
<p>我们提出了一种基于预训练图像扩散模型的零样本方法在任意空间（例如360°全景的球体或纹理的网格表面）生成图像。使用预训练的图像扩散模型进行各种视觉内容的零样本生成主要探索了两个方向。首先，扩散同步法是在不同的投影空间上联合执行反向扩散过程，同时在目标空间进行同步，当提供足够的条件时，可以生成高质量的结果，但在缺乏条件时，该方法会遇到困难。其次，分数蒸馏采样法是通过梯度下降逐步更新目标空间数据，这种方法虽然保证了更好的连贯性，但往往缺乏细节。在本文中，我们首次揭示了这两种方法之间的相互联系，同时强调了它们的差异。为此，我们提出了StochSync这一新方法，它结合了这两种方法的优点，能够在弱条件下实现有效性能。我们的实验表明，StochSync在无图像条件的全景图生成中取得了最佳性能，优于基于微调的方法，并且在提供深度条件的三维网格纹理生成中取得了与之前的方法相当的结果。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.15445v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://stochsync.github.io/">https://stochsync.github.io/</a> (ICLR 2025)</p>
<p><strong>Summary</strong></p>
<p>本文提出一种利用预训练图像扩散模型在任意空间（如球体生成360°全景图和网格表面生成纹理）进行零样本图像生成的方法。文章探讨了两种主要的零样本生成方式：Diffusion Synchronization和Score Distillation Sampling，并首次揭示了这两种方法之间的关联和差异。为此，本文提出了一种结合两者优点的新方法StochSync，能够在弱条件下实现有效性能。实验表明，StochSync在无需图像条件的情况下，性能最佳，且在需要深度条件的3D网格纹理生成方面也有较好的表现。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出了一种零样本方法在任意空间生成图像的新技术，使用预训练的图像扩散模型。</li>
<li>文章主要探讨了Diffusion Synchronization和Score Distillation Sampling两种零样本生成方式。</li>
<li>首次揭示了Diffusion Synchronization和Score Distillation Sampling两种方法的关联与差异。</li>
<li>提出了一种结合两者优点的新方法StochSync，能够在弱条件下实现有效性能。</li>
<li>实验表明，StochSync在无需图像条件的情况下，性能最佳，尤其在生成全景图方面。</li>
<li>在需要深度条件的3D网格纹理生成方面，StochSync也有较好的表现。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.15445">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-84255535e8e74711f441500238f8a8b3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-54598e53fe0d61fedb6c61baec78b3ff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5665254822446be3d3066a82fc0a797b.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Dfilled-Repurposing-Edge-Enhancing-Diffusion-for-Guided-DSM-Void-Filling"><a href="#Dfilled-Repurposing-Edge-Enhancing-Diffusion-for-Guided-DSM-Void-Filling" class="headerlink" title="Dfilled: Repurposing Edge-Enhancing Diffusion for Guided DSM Void   Filling"></a>Dfilled: Repurposing Edge-Enhancing Diffusion for Guided DSM Void   Filling</h2><p><strong>Authors:Daniel Panangian, Ksenia Bittner</strong></p>
<p>Digital Surface Models (DSMs) are essential for accurately representing Earth’s topography in geospatial analyses. DSMs capture detailed elevations of natural and manmade features, crucial for applications like urban planning, vegetation studies, and 3D reconstruction. However, DSMs derived from stereo satellite imagery often contain voids or missing data due to occlusions, shadows, and lowsignal areas. Previous studies have primarily focused on void filling for digital elevation models (DEMs) and Digital Terrain Models (DTMs), employing methods such as inverse distance weighting (IDW), kriging, and spline interpolation. While effective for simpler terrains, these approaches often fail to handle the intricate structures present in DSMs. To overcome these limitations, we introduce Dfilled, a guided DSM void filling method that leverages optical remote sensing images through edge-enhancing diffusion. Dfilled repurposes deep anisotropic diffusion models, which originally designed for super-resolution tasks, to inpaint DSMs. Additionally, we utilize Perlin noise to create inpainting masks that mimic natural void patterns in DSMs. Experimental evaluations demonstrate that Dfilled surpasses traditional interpolation methods and deep learning approaches in DSM void filling tasks. Both quantitative and qualitative assessments highlight the method’s ability to manage complex features and deliver accurate, visually coherent results. </p>
<blockquote>
<p>数字表面模型（DSMs）在地理空间分析中准确表示地球地形方面起着至关重要的作用。DSM捕获自然和人造特征的详细高程信息，对于城市规划、植被研究和3D重建等应用至关重要。然而，从立体卫星影像派生的DSMs通常由于遮挡、阴影和低信号区域而包含空隙或缺失数据。以往的研究主要集中在数字高程模型（DEMs）和数字地形模型（DTMs）的空隙填充上，采用的方法包括反距离加权（IDW）、克里金法和样条插值等。这些方法对于简单地形虽然有效，但往往难以处理DSM中存在的复杂结构。为了克服这些限制，我们引入了Dfilled，这是一种有指导的DSM空隙填充方法，它通过边缘增强扩散利用光学遥感图像。Dfilled将原本为超分辨率任务设计的深度各向异性扩散模型改用于填补DSM。此外，我们还利用Perlin噪声生成模仿DSM中自然空隙模式的填充掩膜。实验评估表明，在DSM空隙填充任务中，Dfilled超越了传统插值方法和深度学习方法。定量和定性评估都突出了该方法在处理复杂特征方面的能力，并能提供准确、视觉连贯的结果。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.15440v1">PDF</a> Accepted to IEEE&#x2F;CVF Winter Conference on Applications of Computer   Vision Workshops (WACVW)</p>
<p><strong>Summary</strong>：</p>
<p>数字表面模型（DSMs）在地理空间分析中准确表示地球地形至关重要。DSMs捕捉自然和人造特征的详细高程，对于城市规划、植被研究和3D重建等应用至关重要。然而，从立体卫星影像派生的DSMs常常由于遮挡、阴影和低信号区域而出现空白或缺失数据。尽管传统的插值方法如反距离权重（IDW）、克里格和样条插值对简单地形有效，但它们难以处理DSMs中的复杂结构。为了克服这些局限性，我们引入了Dfilled方法，这是一种利用光学遥感图像进行边缘增强扩散的DSM空白填充方法。实验评估表明，Dfilled在DSM空白填充任务上超越了传统的插值方法和深度学习方法，无论是定量还是定性评估，都证明了该方法管理复杂特征并产生准确、视觉连贯结果的能力。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>数字表面模型（DSMs）在地理空间分析中扮演重要角色，用于准确表示地球的地形特征。</li>
<li>DSMs通过捕捉自然和人造特征的详细高程，支持多种应用，如城市规划、植被研究和3D重建。</li>
<li>从立体卫星影像派生的DSMs可能会出现空白或缺失数据，这主要是由于遮挡、阴影和低信号区域导致的。</li>
<li>传统插值方法在处理DSMs中的复杂结构时存在局限性。</li>
<li>Dfilled方法是一种新型的DSM空白填充技术，利用光学遥感图像进行边缘增强扩散。</li>
<li>Dfilled方法通过利用深度各向异性扩散模型和Perlin噪声生成模仿DSM自然空白模式的填充掩膜来实现高效的空白填充。</li>
<li>实验评估表明，Dfilled在DSM空白填充任务上表现出色，优于传统的插值方法和深度学习方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.15440">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-601a580560dc20c6b2e111a0b6c5fbbb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-292cef999959e8924af92e7532be1e04.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6ff2a3261420ce68569adeb580483906.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1be4c41117b0821db6ef49e720e5cc6f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1892a00733ebe3b2056d0f07f3daf1e5.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Zero-Shot-Interactive-Text-to-Image-Retrieval-via-Diffusion-Augmented-Representations"><a href="#Zero-Shot-Interactive-Text-to-Image-Retrieval-via-Diffusion-Augmented-Representations" class="headerlink" title="Zero-Shot Interactive Text-to-Image Retrieval via Diffusion-Augmented   Representations"></a>Zero-Shot Interactive Text-to-Image Retrieval via Diffusion-Augmented   Representations</h2><p><strong>Authors:Zijun Long, Kangheng Liang, Gerardo Aragon-Camarasa, Richard Mccreadie, Paul Henderson</strong></p>
<p>Interactive Text-to-Image Retrieval (I-TIR) has emerged as a transformative user-interactive tool for applications in domains such as e-commerce and education. Yet, current methodologies predominantly depend on finetuned Multimodal Large Language Models (MLLMs), which face two critical limitations: (1) Finetuning imposes prohibitive computational overhead and long-term maintenance costs. (2) Finetuning narrows the pretrained knowledge distribution of MLLMs, reducing their adaptability to novel scenarios. These issues are exacerbated by the inherently dynamic nature of real-world I-TIR systems, where queries and image databases evolve in complexity and diversity, often deviating from static training distributions. To overcome these constraints, we propose Diffusion Augmented Retrieval (DAR), a paradigm-shifting framework that bypasses MLLM finetuning entirely. DAR synergizes Large Language Model (LLM)-guided query refinement with Diffusion Model (DM)-based visual synthesis to create contextually enriched intermediate representations. This dual-modality approach deciphers nuanced user intent more holistically, enabling precise alignment between textual queries and visually relevant images. Rigorous evaluations across four benchmarks reveal DAR’s dual strengths: (1) Matches state-of-the-art finetuned I-TIR models on straightforward queries without task-specific training. (2) Scalable Generalization: Surpasses finetuned baselines by 7.61% in Hits@10 (top-10 accuracy) under multi-turn conversational complexity, demonstrating robustness to intricate, distributionally shifted interactions. By eliminating finetuning dependencies and leveraging generative-augmented representations, DAR establishes a new trajectory for efficient, adaptive, and scalable cross-modal retrieval systems. </p>
<blockquote>
<p>交互式文本到图像检索（I-TIR）已经成为电子商务和教育等领域应用的一种变革性的用户交互工具。然而，当前的方法主要依赖于微调的多模态大型语言模型（MLLMs），它们面临两个关键的局限性：（1）微调带来了巨大的计算开销和长期维护成本。（2）微调缩小了MLLMs的预训练知识分布，降低了它们对新场景的适应性。这些问题被真实世界中I-TIR系统的固有动态性质所加剧，其中查询和图像数据库的复杂性和多样性在不断发展变化，常常偏离静态训练分布。为了克服这些限制，我们提出了扩散增强检索（DAR），这是一个彻底绕过MLLM微调的模式转变框架。DAR将大型语言模型（LLM）引导查询细化与基于扩散模型（DM）的视觉合成相结合，以创建上下文丰富的中间表示。这种双模态方法更全面地解释了微妙的用户意图，实现了文本查询和视觉相关图像之间的精确对齐。在四个基准测试上的严格评估揭示了DAR的双重优势：（1）在直接查询上匹配最先进的微调I-TIR模型，无需特定任务训练。（2）可扩展的泛化能力：在多回合对话复杂性的情况下，在命中数@10（前10名的准确率）上超越微调基线7.61%，证明了对复杂、分布偏移交互的稳健性。通过消除微调依赖并利用生成增强表示，DAR为高效、自适应和可扩展的跨模态检索系统建立了新的轨迹。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.15379v1">PDF</a> </p>
<p><strong>Summary</strong><br>     交互式文本到图像检索（I-TIR）已成为电子商务和教育等领域的重要工具。然而，当前的方法主要依赖于微调的多模态大型语言模型（MLLMs），存在两个关键问题：一是计算开销大且维护成本高；二是难以适应新场景。为解决这些问题，提出一种名为Diffusion Augmented Retrieval（DAR）的新框架，通过结合大型语言模型（LLM）引导查询优化和扩散模型（DM）的视觉合成，创建上下文丰富的中间表示。这一双模态方法更全面地解析用户意图，实现文本查询和视觉相关图像之间的精确对齐。评估表明，DAR在匹配现有技术的同时，具有出色的通用性和可扩展性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>当前I-TIR方法主要依赖微调的MLLMs，存在计算开销大和维护成本高的局限性。</li>
<li>扩散模型（DM）在视觉合成方面具有潜力，可以与LLM结合使用以提高检索精度。</li>
<li>DAR框架通过结合LLM和DM创建上下文丰富的中间表示，实现文本查询和视觉图像的精确对齐。</li>
<li>DAR在标准评估上实现了与现有技术相当的性能，同时具有出色的可扩展性和通用性。</li>
<li>DAR通过消除对微调的需求并利用生成增强表示，为高效、自适应和可扩展的跨模态检索系统开辟了新途径。</li>
<li>DAR框架适用于动态变化的查询和图像数据库，能够更好地适应现实世界的复杂性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.15379">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-4cfdce012f60c763a7b71fa918dd0834.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-172271c6ddce71627fe3a2ddd403f2a1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e5fad1759d407113b687d797a2048b28.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-45ae0caf9769e462e4207bc1051ab672.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Generalizable-Deepfake-Detection-via-Effective-Local-Global-Feature-Extraction"><a href="#Generalizable-Deepfake-Detection-via-Effective-Local-Global-Feature-Extraction" class="headerlink" title="Generalizable Deepfake Detection via Effective Local-Global Feature   Extraction"></a>Generalizable Deepfake Detection via Effective Local-Global Feature   Extraction</h2><p><strong>Authors:Jiazhen Yan, Ziqiang Li, Ziwen He, Zhangjie Fu</strong></p>
<p>The rapid advancement of GANs and diffusion models has led to the generation of increasingly realistic fake images, posing significant hidden dangers and threats to society. Consequently, deepfake detection has become a pressing issue in today’s world. While some existing methods focus on forgery features from either a local or global perspective, they often overlook the complementary nature of these features. Other approaches attempt to incorporate both local and global features but rely on simplistic strategies, such as cropping, which fail to capture the intricate relationships between local features. To address these limitations, we propose a novel method that effectively combines local spatial-frequency domain features with global frequency domain information, capturing detailed and holistic forgery traces. Specifically, our method uses Discrete Wavelet Transform (DWT) and sliding windows to tile forged features and leverages attention mechanisms to extract local spatial-frequency domain information. Simultaneously, the phase component of the Fast Fourier Transform (FFT) is integrated with attention mechanisms to extract global frequency domain information, complementing the local features and ensuring the integrity of forgery detection. Comprehensive evaluations on open-world datasets generated by 34 distinct generative models demonstrate a significant improvement of 2.9% over existing state-of-the-art methods. </p>
<blockquote>
<p>生成对抗网络（GANs）和扩散模型的快速发展导致了越来越逼真的虚假图像生成，给社会带来了重大的潜在危险和威胁。因此，深度伪造检测已成为当今世界的紧迫问题。虽然一些现有的方法侧重于从局部或全局角度提取伪造特征，但它们往往忽视了这些特征的互补性。其他方法试图结合局部和全局特征，但依赖于简单的策略，如裁剪，这无法捕捉局部特征之间的复杂关系。为了解决这些局限性，我们提出了一种新的方法，该方法有效地结合了局部空间频域特征与全局频域信息，捕捉详细且整体的伪造痕迹。具体来说，我们的方法使用离散小波变换（DWT）和滑动窗口来划分伪造特征，并利用注意力机制提取局部空间频域信息。同时，我们将快速傅里叶变换（FFT）的相位成分与注意力机制相结合，提取全局频域信息，以补充局部特征并确保伪造检测的完整性。在由34种不同的生成模型生成的开放世界数据集上的综合评估表明，与现有的最先进的方法相比，我们的方法取得了2.9%的显著改进。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.15253v1">PDF</a> under review</p>
<p><strong>Summary</strong></p>
<p>本文介绍了生成对抗网络和扩散模型的快速发展带来的虚假图像生成问题，强调了深度伪造检测在现今社会的紧迫性。针对现有方法的局限性，提出了一种结合局部空间频率域特征和全局频率域信息的新方法，利用离散小波变换和滑动窗口进行伪造特征提取，并通过注意力机制融合局部和全局特征，提高了伪造检测的准确性和完整性。在公开数据集上的综合评估表明，该方法相较于现有最先进的方法有2.9%的显著提升。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GANs和扩散模型的快速发展导致虚假图像越来越逼真，给社会带来潜在威胁，深度伪造检测成为重要议题。</li>
<li>现有方法主要关注局部或全局的伪造特征，但忽略了它们之间的互补性。</li>
<li>一些方法试图结合局部和全局特征，但采用简单策略如裁剪，无法捕捉复杂关系。</li>
<li>新方法结合局部空间频率域特征和全局频率域信息，使用离散小波变换和滑动窗口进行特征提取。</li>
<li>注意力机制用于融合局部和全局特征，确保检测准确性和完整性。</li>
<li>综合评估显示新方法在公开数据集上的性能相较于现有方法有显著提升。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.15253">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ce9220fdd60f534e06780b32944e8e8a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cef969ed29c2cc55faf92dd75ea373d1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ca572cd001d155dcdb87a4c26b1c3f8f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9f20c784d4304b01061a0f23fa2740e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1e2e94947a5ab6fdfd294c6afc642918.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="MATCHA-Towards-Matching-Anything"><a href="#MATCHA-Towards-Matching-Anything" class="headerlink" title="MATCHA:Towards Matching Anything"></a>MATCHA:Towards Matching Anything</h2><p><strong>Authors:Fei Xue, Sven Elflein, Laura Leal-Taixé, Qunjie Zhou</strong></p>
<p>Establishing correspondences across images is a fundamental challenge in computer vision, underpinning tasks like Structure-from-Motion, image editing, and point tracking. Traditional methods are often specialized for specific correspondence types, geometric, semantic, or temporal, whereas humans naturally identify alignments across these domains. Inspired by this flexibility, we propose MATCHA, a unified feature model designed to &#96;&#96;rule them all’’, establishing robust correspondences across diverse matching tasks. Building on insights that diffusion model features can encode multiple correspondence types, MATCHA augments this capacity by dynamically fusing high-level semantic and low-level geometric features through an attention-based module, creating expressive, versatile, and robust features. Additionally, MATCHA integrates object-level features from DINOv2 to further boost generalization, enabling a single feature capable of matching anything. Extensive experiments validate that MATCHA consistently surpasses state-of-the-art methods across geometric, semantic, and temporal matching tasks, setting a new foundation for a unified approach for the fundamental correspondence problem in computer vision. To the best of our knowledge, MATCHA is the first approach that is able to effectively tackle diverse matching tasks with a single unified feature. </p>
<blockquote>
<p>建立图像之间的对应关系是计算机视觉领域的一个基本挑战，也是结构从运动、图像编辑和点跟踪等任务的基础。传统方法通常针对特定的对应关系类型（如几何、语义或时间）进行专业化处理，而人类自然地能够识别这些领域的对齐方式。受这种灵活性的启发，我们提出了MATCHA，这是一个统一的特征模型，旨在“一统天下”，在多种匹配任务中建立稳健的对应关系。基于扩散模型特征可以编码多种对应关系类型的见解，MATCHA通过基于注意力的模块动态融合高级语义和低级几何特征，增强这种能力，从而生成表达力强、通用性强、鲁棒性高的特征。此外，MATCHA还集成了DINOv2的对象级特征，进一步提高了泛化能力，使得单个特征就能够匹配任何内容。大量实验验证了MATCHA在几何、语义和时间匹配任务上均超越了最新方法，为计算机视觉中基本对应关系问题的统一方法设定了新的基础。据我们所知，MATCHA是第一个能够有效解决多种匹配任务的单统一特征方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.14945v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了一个统一特征模型MATCHA，旨在通过融合扩散模型特征来建立跨不同匹配任务的稳健对应关系。MATCHA通过动态融合高级语义和低级几何特征，增强对应关系的表达能力、通用性和稳健性。此外，MATCHA集成了DINOv2的对象级特征，进一步提高泛化能力，使得单个特征能够匹配任何内容。实验证明，MATCHA在几何、语义和时序匹配任务上均超越了现有方法，为计算机视觉中的基本对应关系问题提供了新的统一解决方案。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MATCHA是一个统一特征模型，旨在解决计算机视觉中跨图像对应关系的核心挑战。</li>
<li>MATCHA融合扩散模型特征，具备处理多种对应关系类型的能力。</li>
<li>通过动态融合高级语义和低级几何特征，MATCHA增强了对应关系的表达能力、通用性和稳健性。</li>
<li>MATCHA集成了DINOv2的对象级特征，提高了泛化能力，实现了单一特征对任何内容的匹配。</li>
<li>实验证明，MATCHA在几何、语义和时序匹配任务上的性能均超越了现有方法。</li>
<li>MATCHA是首个能够有效处理多种匹配任务的单一统一特征方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.14945">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-a3b490523141b2fe17e7490e208ddeda.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a1e7911508f46c508b79175857b48ef3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0b15bf0e1de811f42429257181814160.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ad46d12d0e91a1b3ee030b44047dc1ed.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f3e2371dc5e77f574ee450dc398bf8ac.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="GlyphDraw2-Automatic-Generation-of-Complex-Glyph-Posters-with-Diffusion-Models-and-Large-Language-Models"><a href="#GlyphDraw2-Automatic-Generation-of-Complex-Glyph-Posters-with-Diffusion-Models-and-Large-Language-Models" class="headerlink" title="GlyphDraw2: Automatic Generation of Complex Glyph Posters with Diffusion   Models and Large Language Models"></a>GlyphDraw2: Automatic Generation of Complex Glyph Posters with Diffusion   Models and Large Language Models</h2><p><strong>Authors:Jian Ma, Yonglin Deng, Chen Chen, Haonan Lu, Zhenyu Yang</strong></p>
<p>Posters play a crucial role in marketing and advertising by enhancing visual communication and brand visibility, making significant contributions to industrial design. With the latest advancements in controllable T2I diffusion models, increasing research has focused on rendering text within synthesized images. Despite improvements in text rendering accuracy, the field of automatic poster generation remains underexplored. In this paper, we propose an automatic poster generation framework with text rendering capabilities leveraging LLMs, utilizing a triple-cross attention mechanism based on alignment learning. This framework aims to create precise poster text within a detailed contextual background. Additionally, the framework supports controllable fonts, adjustable image resolution, and the rendering of posters with descriptions and text in both English and Chinese.Furthermore, we introduce a high-resolution font dataset and a poster dataset with resolutions exceeding 1024 pixels. Our approach leverages the SDXL architecture. Extensive experiments validate our method’s capability in generating poster images with complex and contextually rich backgrounds.Codes is available at <a target="_blank" rel="noopener" href="https://github.com/OPPO-Mente-Lab/GlyphDraw2">https://github.com/OPPO-Mente-Lab/GlyphDraw2</a>. </p>
<blockquote>
<p>海报在营销和广告中扮演着至关重要的角色，通过增强视觉交流和品牌可见度，为工业设计做出了重大贡献。随着最新的可控T2I扩散模型的进步，越来越多的研究专注于在合成图像中进行文本渲染。尽管文本渲染的准确性有所提高，但自动海报生成领域仍然未被充分探索。在本文中，我们提出了一个具有文本渲染能力的自动海报生成框架，该框架利用基于对齐学习的大型语言模型（LLMs），采用三重交叉注意力机制。该框架旨在在一个详细的上下文背景中创建精确的海报文本。此外，该框架支持可控字体、可调整的图像分辨率以及中英文描述和文本的海报渲染。此外，我们还介绍了一个高分辨率字体数据集和一个分辨率超过1024像素的海报数据集。我们的方法利用SDXL架构。大量实验验证了我们方法在生成具有复杂和上下文丰富的背景的海报图像方面的能力。代码可在<a target="_blank" rel="noopener" href="https://github.com/OPPO-Mente-Lab/GlyphDraw2%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/OPPO-Mente-Lab/GlyphDraw2中找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.02252v3">PDF</a> Accepted by AAAI2025</p>
<p><strong>Summary</strong></p>
<p>本文提出一种利用LLMs和基于对齐学习的三重交叉注意力机制的自动海报生成框架，可实现文本渲染。该框架旨在创建具有详细上下文背景的海报文本，并支持可控字体、可调图像分辨率以及中英文描述和文本的海报渲染。此外，引入高分辨率字体数据集和分辨率超过1024像素的海报数据集。实验证明该方法能够生成具有复杂和丰富上下文背景的海报图像。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>海报在营销和广告中起到关键作用，提升视觉沟通和品牌可见度，对工业设计有重要贡献。</li>
<li>文本在合成图像中的渲染是近年来的研究热点，尤其是利用可控的T2I扩散模型。</li>
<li>当前研究尚未充分探索自动海报生成领域。</li>
<li>本文提出一种基于LLMs和基于对齐学习的三重交叉注意力机制的自动海报生成框架，具有文本渲染能力。</li>
<li>该框架旨在创建具有详细上下文背景的海报文本，支持多种字体、图像分辨率及中英文描述。</li>
<li>引入高分辨率字体数据集和海报数据集，分辨率超过1024像素。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.02252">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-5682c31a56e60e41bbadca6c8a0954bc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-efb25ad2005f40b029459258372ee168.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b3f98df45dbccbfa58e8162d46c29fe6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cfab3794dc454afcf58961c6212c4b9c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-070be88a127805c1794b2b20449f5096.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Zero-shot-Video-Restoration-and-Enhancement-Using-Pre-Trained-Image-Diffusion-Model"><a href="#Zero-shot-Video-Restoration-and-Enhancement-Using-Pre-Trained-Image-Diffusion-Model" class="headerlink" title="Zero-shot Video Restoration and Enhancement Using Pre-Trained Image   Diffusion Model"></a>Zero-shot Video Restoration and Enhancement Using Pre-Trained Image   Diffusion Model</h2><p><strong>Authors:Cong Cao, Huanjing Yue, Xin Liu, Jingyu Yang</strong></p>
<p>Diffusion-based zero-shot image restoration and enhancement models have achieved great success in various tasks of image restoration and enhancement. However, directly applying them to video restoration and enhancement results in severe temporal flickering artifacts. In this paper, we propose the first framework for zero-shot video restoration and enhancement based on the pre-trained image diffusion model. By replacing the spatial self-attention layer with the proposed short-long-range (SLR) temporal attention layer, the pre-trained image diffusion model can take advantage of the temporal correlation between frames. We further propose temporal consistency guidance, spatial-temporal noise sharing, and an early stopping sampling strategy to improve temporally consistent sampling. Our method is a plug-and-play module that can be inserted into any diffusion-based image restoration or enhancement methods to further improve their performance. Experimental results demonstrate the superiority of our proposed method. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/cao-cong/ZVRD">https://github.com/cao-cong/ZVRD</a>. </p>
<blockquote>
<p>基于扩散的零样本图像修复和增强模型在图像修复和增强的各种任务中取得了巨大的成功。然而，直接将其应用于视频修复和增强会导致严重的暂时闪烁伪影。在本文中，我们提出了基于预训练图像扩散模型的零样本视频修复和增强的第一个框架。通过用所提出的短长程（SLR）时间注意力层替换空间自注意力层，预训练的图像扩散模型可以利用帧之间的时间相关性。我们还提出了时间一致性指导、时空噪声共享和早期停止采样策略，以改善时间一致性的采样。我们的方法是一个即插即用的模块，可以插入到任何基于扩散的图像修复或增强方法中，以进一步提高其性能。实验结果证明了我们提出方法的优越性。我们的代码可在[<a target="_blank" rel="noopener" href="https://github.com/cao-cong/ZVRD%E6%89%BE%E5%88%B0%E3%80%82]">https://github.com/cao-cong/ZVRD找到。]</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.01960v3">PDF</a> Accepted by AAAI 2025</p>
<p><strong>Summary</strong><br>扩散模型在零样本图像修复和增强任务中取得了巨大成功，但直接应用于视频修复和增强会导致严重的时序闪烁伪影。本文首次提出基于预训练图像扩散模型的零样本视频修复和增强框架。通过用所提出的长短程（SLR）时序注意力层替换空间自注意力层，预训练的图像扩散模型可以利用帧之间的时序相关性。本文还提出了时序一致性指导、时空噪声共享和早期停止采样策略，以改进时序一致采样。该方法是一个即插即用的模块，可以插入任何基于扩散的图像修复或增强方法中，以进一步提高其性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型在图像修复和增强任务中表现优异，但直接应用于视频会导致时序伪影。</li>
<li>本文首次提出了基于预训练图像扩散模型的零样本视频修复和增强框架。</li>
<li>通过引入长短程（SLR）时序注意力层，模型能利用帧间的时序相关性。</li>
<li>提出了时序一致性指导、时空噪声共享和早期停止采样策略，以改善时序一致采样。</li>
<li>该方法是一个可插入任何扩散图像修复或增强方法的模块，能进一步提升性能。</li>
<li>实验结果证明该方法具有优越性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.01960">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-dc33e2332143fcce684deef1e907382f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b9bd168d030a48c98ce59393965aa883.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b6a64643a15995a15fda5df68c86c9e2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d213286e9dba67074b0adc4c7cff3db5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5c881d379739978dc0b43df2881dabd0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ee5dbd0ba005be8ba772bb8385035e2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e031237a2d8a6faa09b672a092b367c7.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Information-Theoretic-Text-to-Image-Alignment"><a href="#Information-Theoretic-Text-to-Image-Alignment" class="headerlink" title="Information Theoretic Text-to-Image Alignment"></a>Information Theoretic Text-to-Image Alignment</h2><p><strong>Authors:Chao Wang, Giulio Franzese, Alessandro Finamore, Massimo Gallo, Pietro Michiardi</strong></p>
<p>Diffusion models for Text-to-Image (T2I) conditional generation have recently achieved tremendous success. Yet, aligning these models with user’s intentions still involves a laborious trial-and-error process, and this challenging alignment problem has attracted considerable attention from the research community. In this work, instead of relying on fine-grained linguistic analyses of prompts, human annotation, or auxiliary vision-language models, we use Mutual Information (MI) to guide model alignment. In brief, our method uses self-supervised fine-tuning and relies on a point-wise (MI) estimation between prompts and images to create a synthetic fine-tuning set for improving model alignment. Our analysis indicates that our method is superior to the state-of-the-art, yet it only requires the pre-trained denoising network of the T2I model itself to estimate MI, and a simple fine-tuning strategy that improves alignment while maintaining image quality. </p>
<blockquote>
<p>文本到图像（T2I）的条件生成扩散模型近期取得了巨大成功。然而，将这些模型与用户意图对齐仍然需要繁琐的试错过程，这个具有挑战性的对齐问题已引起了研究界的广泛关注。在这项工作中，我们没有依赖提示的精细语言分析、人工标注或辅助的视觉语言模型，而是使用互信息（MI）来引导模型对齐。简而言之，我们的方法采用自监督微调，并依赖于提示和图像之间的点（MI）估计来创建一个合成微调集，以提高模型对齐能力。我们的分析表明，我们的方法优于现有技术，但它只需要T2I模型本身的预训练去噪网络来估计MI，以及一种简单的微调策略，可以在保持图像质量的同时提高对齐精度。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.20759v2">PDF</a> to appear at ICLR25</p>
<p><strong>Summary</strong></p>
<p>基于文本的扩散模型（T2I）条件生成已取得了巨大成功，但在与用户意图对齐时仍面临繁琐的试错过程。本研究采用互信息（MI）引导模型对齐，无需依赖精细的语言分析提示、人工标注或辅助视觉语言模型。通过自监督微调并利用提示与图像之间的点级互信息估计，创建合成微调集以改善模型对齐。分析表明，该方法优于现有技术，仅依赖T2I模型的预训练去噪网络进行互信息估计，采用简单的微调策略提高对齐度同时保持图像质量。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文本到图像的扩散模型已取得显著成功。</li>
<li>对齐模型与用户意图仍然是一个挑战，需要采用新的方法解决。</li>
<li>本研究使用互信息（MI）来引导模型对齐，无需依赖精细的语言分析或人工标注。</li>
<li>通过自监督微调改善模型对齐。</li>
<li>利用提示和图像之间的点级互信息估计来创建合成微调集。</li>
<li>该方法优于现有技术。</li>
<li>该方法仅依赖T2I模型的预训练去噪网络进行互信息估计，并采用简单的微调策略维持图像质量。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.20759">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-d84c99d3168f3eb9c0f0c3f202c1e724.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0d1e5e8d09ae713de8b8e529cdffd3a7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2da240faba8287e57de1f9dd892c78f3.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="An-Item-is-Worth-a-Prompt-Versatile-Image-Editing-with-Disentangled-Control"><a href="#An-Item-is-Worth-a-Prompt-Versatile-Image-Editing-with-Disentangled-Control" class="headerlink" title="An Item is Worth a Prompt: Versatile Image Editing with Disentangled   Control"></a>An Item is Worth a Prompt: Versatile Image Editing with Disentangled   Control</h2><p><strong>Authors:Aosong Feng, Weikang Qiu, Jinbin Bai, Xiao Zhang, Zhen Dong, Kaicheng Zhou, Rex Ying, Leandros Tassiulas</strong></p>
<p>Building on the success of text-to-image diffusion models (DPMs), image editing is an important application to enable human interaction with AI-generated content. Among various editing methods, editing within the prompt space gains more attention due to its capacity and simplicity of controlling semantics. However, since diffusion models are commonly pretrained on descriptive text captions, direct editing of words in text prompts usually leads to completely different generated images, violating the requirements for image editing. On the other hand, existing editing methods usually consider introducing spatial masks to preserve the identity of unedited regions, which are usually ignored by DPMs and therefore lead to inharmonic editing results. Targeting these two challenges, in this work, we propose to disentangle the comprehensive image-prompt interaction into several item-prompt interactions, with each item linked to a special learned prompt. The resulting framework, named D-Edit, is based on pretrained diffusion models with cross-attention layers disentangled and adopts a two-step optimization to build item-prompt associations. Versatile image editing can then be applied to specific items by manipulating the corresponding prompts. We demonstrate state-of-the-art results in four types of editing operations including image-based, text-based, mask-based editing, and item removal, covering most types of editing applications, all within a single unified framework. Notably, D-Edit is the first framework that can (1) achieve item editing through mask editing and (2) combine image and text-based editing. We demonstrate the quality and versatility of the editing results for a diverse collection of images through both qualitative and quantitative evaluations. </p>
<blockquote>
<p>基于文本到图像扩散模型（DPMs）的成功，图像编辑是与AI生成内容进行交互的重要应用。在各种编辑方法中，提示空间内的编辑因其控制语义的能力和简洁性而受到更多关注。然而，由于扩散模型通常是在描述性文本标题上进行预训练的，直接在文本提示中进行编辑通常会导致生成的图像完全不同，这违反了图像编辑的要求。另一方面，现有的编辑方法通常考虑引入空间掩膜来保留未编辑区域的身份，而这些通常被DPMs忽略，从而导致不和谐的编辑结果。针对这两个挑战，在这项工作中，我们提出将全面的图像提示交互分解为多个项目提示交互，每个项目与特殊的预训练提示相关联。得到的框架被称为D-Edit，它基于预训练的扩散模型，通过解开交叉注意层来构建物品提示关联的两步优化。然后，可以通过操纵相应的提示来对特定项目进行灵活的图像编辑。我们在四种编辑操作中展示了最先进的成果，包括基于图像的编辑、基于文本的编辑、基于掩膜的编辑和项目移除，涵盖了大多数类型的编辑应用，都在一个统一的框架内完成。值得注意的是，D-Edit是第一个能够（1）通过掩膜编辑实现项目编辑并结合图像和文本基础的编辑框架。我们通过定性和定量评估，展示了编辑结果的质量和多样性，适用于各种图像集合。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.04880v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于文本到图像扩散模型（DPMs）的成功，图像编辑是与AI生成内容进行交互的重要应用之一。本文关注扩散模型在图像编辑中的两个挑战：文本提示空间内的编辑和未编辑区域的身份保留。为此，本文提出了一个名为D-Edit的框架，基于预训练的扩散模型，通过分解图像与提示的交互为多个物品与提示的交互来解决这两个挑战。该框架采用两步优化来建立物品与提示的关联，可以通过操作相应的提示来对特定物品进行多样化的图像编辑。D-Edit在四种类型的编辑操作（基于图像、基于文本、基于掩码的编辑以及物品移除）中均展现出卓越的性能，且是唯一能够结合图像和文本进行编辑的框架。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型在图像编辑中面临两个主要挑战：文本提示空间内的编辑和保留未编辑区域的身份。</li>
<li>D-Edit框架解决了这两个挑战，通过将图像与提示的交互分解为多个物品与提示的交互。</li>
<li>D-Edit基于预训练的扩散模型，采用两步优化建立物品与提示的关联。</li>
<li>D-Edit支持多样化的图像编辑，通过操作相应的提示来对特定物品进行编辑。</li>
<li>D-Edit在多种类型的编辑操作中展现出卓越性能，包括图像、文本、掩码编辑以及物品移除。</li>
<li>D-Edit是首个能够结合图像和文本进行编辑的框架。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.04880">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-bc47436d52e6e16d60fca5dd9beb685e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-82c02189750afd53c0f44d1986cd5125.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d3a17b3eb0f89e44d0d51c7f4ec0fc59.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-687ce5a789311219795d2bb3bbc43ea2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1097014ca847b9e29fb0d83966213471.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a0fcd8709b962e9f24b159d3cdf70014.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-85f81725edf2946dbf7d43895263e1e5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-038379016dd7eca327254cbba05f382f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5be8e8c47a6563d351c9839a9083d171.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-29/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-29/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-29/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-ee77dd7964417233a4a5362a8846566c.jpg" class="responsive-img" alt="医学图像">
                        
                        <span class="card-title">医学图像</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学图像 方向最新论文已更新，请持续关注 Update in 2025-01-29  Lightweight Weighted Average Ensemble Model for Pneumonia Detection in   Chest X-Ray Images
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-29
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    医学图像
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">医学图像</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-29/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-b1158380bab7cb28fb237f42c24c0038.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF 方向最新论文已更新，请持续关注 Update in 2025-01-29  LinPrim Linear Primitives for Differentiable Volumetric Rendering
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-29
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">18723.3k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
