<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-29  Controllable Forgetting Mechanism for Few-Shot Class-Incremental   Learning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-eeff5798c423994e72daf5ad7e40e66f.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-29
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-02-12
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    9.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    38 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-01-29-æ›´æ–°"><a href="#2025-01-29-æ›´æ–°" class="headerlink" title="2025-01-29 æ›´æ–°"></a>2025-01-29 æ›´æ–°</h1><h2 id="Controllable-Forgetting-Mechanism-for-Few-Shot-Class-Incremental-Learning"><a href="#Controllable-Forgetting-Mechanism-for-Few-Shot-Class-Incremental-Learning" class="headerlink" title="Controllable Forgetting Mechanism for Few-Shot Class-Incremental   Learning"></a>Controllable Forgetting Mechanism for Few-Shot Class-Incremental   Learning</h2><p><strong>Authors:Kirill Paramonov, Mete Ozay, Eunju Yang, Jijoong Moon, Umberto Michieli</strong></p>
<p>Class-incremental learning in the context of limited personal labeled samples (few-shot) is critical for numerous real-world applications, such as smart home devices. A key challenge in these scenarios is balancing the trade-off between adapting to new, personalized classes and maintaining the performance of the model on the original, base classes. Fine-tuning the model on novel classes often leads to the phenomenon of catastrophic forgetting, where the accuracy of base classes declines unpredictably and significantly. In this paper, we propose a simple yet effective mechanism to address this challenge by controlling the trade-off between novel and base class accuracy. We specifically target the ultra-low-shot scenario, where only a single example is available per novel class. Our approach introduces a Novel Class Detection (NCD) rule, which adjusts the degree of forgetting a priori while simultaneously enhancing performance on novel classes. We demonstrate the versatility of our solution by applying it to state-of-the-art Few-Shot Class-Incremental Learning (FSCIL) methods, showing consistent improvements across different settings. To better quantify the trade-off between novel and base class performance, we introduce new metrics: NCR@2FOR and NCR@5FOR. Our approach achieves up to a 30% improvement in novel class accuracy on the CIFAR100 dataset (1-shot, 1 novel class) while maintaining a controlled base class forgetting rate of 2%. </p>
<blockquote>
<p>åœ¨æœ‰é™çš„ä¸ªäººæ ‡æ³¨æ ·æœ¬ï¼ˆå°æ ·æœ¬ï¼‰çš„èƒŒæ™¯ä¸‹ï¼Œç±»å¢é‡å­¦ä¹ å¯¹äºè®¸å¤šå®é™…åº”ç”¨ï¼ˆå¦‚æ™ºèƒ½å®¶å±…è®¾å¤‡ï¼‰è‡³å…³é‡è¦ã€‚è¿™äº›åœºæ™¯ä¸­çš„ä¸€ä¸ªå…³é”®æŒ‘æˆ˜æ˜¯åœ¨é€‚åº”æ–°ã€ä¸ªæ€§åŒ–ç±»åˆ«çš„åŒæ—¶ä¿æŒæ¨¡å‹å¯¹åŸå§‹åŸºç¡€ç±»åˆ«çš„æ€§èƒ½ã€‚å¯¹æ–°ç±»åˆ«æ¨¡å‹è¿›è¡Œå¾®è°ƒå¸¸å¸¸ä¼šå¯¼è‡´ç¾éš¾æ€§é—å¿˜ç°è±¡ï¼Œå³åŸºç¡€ç±»åˆ«çš„å‡†ç¡®ç‡ä¸å¯é¢„æµ‹ä¸”æ˜¾è‘—ä¸‹é™ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„æœºåˆ¶æ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œé€šè¿‡æ§åˆ¶æ–°ç±»åˆ«å’ŒåŸºç¡€ç±»åˆ«å‡†ç¡®ç‡ä¹‹é—´çš„æƒè¡¡æ¥å®ç°ã€‚æˆ‘ä»¬ç‰¹åˆ«é’ˆå¯¹è¶…ä½æ ·æœ¬åœºæ™¯ï¼Œæ¯ä¸ªæ–°ç±»åˆ«åªæœ‰ä¸€ä¸ªæ ·æœ¬å¯ç”¨ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†æ–°é¢–ç±»åˆ«æ£€æµ‹ï¼ˆNCDï¼‰è§„åˆ™ï¼Œè¯¥è§„åˆ™å¯ä»¥é¢„å…ˆè°ƒæ•´é—å¿˜ç¨‹åº¦ï¼ŒåŒæ—¶æé«˜æ–°ç±»åˆ«çš„æ€§èƒ½ã€‚æˆ‘ä»¬å°†è§£å†³æ–¹æ¡ˆåº”ç”¨äºæœ€å…ˆè¿›çš„å°‘é‡ç±»åˆ«å¢é‡å­¦ä¹ ï¼ˆFSCILï¼‰æ–¹æ³•ï¼Œå¹¶åœ¨ä¸åŒè®¾ç½®ä¸­æ˜¾ç¤ºå‡ºæŒç»­çš„æ”¹è¿›ã€‚ä¸ºäº†æ›´å¥½åœ°é‡åŒ–æ–°ç±»åˆ«å’ŒåŸºç¡€ç±»åˆ«æ€§èƒ½ä¹‹é—´çš„æƒè¡¡ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ–°çš„æŒ‡æ ‡ï¼šNCR@2FORå’ŒNCR@5FORã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨CIFAR100æ•°æ®é›†ä¸Šå®ç°äº†æ–°ç±»åˆ«å‡†ç¡®ç‡é«˜è¾¾30%çš„æ”¹è¿›ï¼ˆ1ä¸ªæ ·æœ¬ï¼Œ1ä¸ªæ–°ç±»åˆ«ï¼‰ï¼ŒåŒæ—¶ä¿æŒåŸºç¡€ç±»åˆ«é—å¿˜ç‡æ§åˆ¶åœ¨2%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.15998v1">PDF</a> ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>è¿™ç¯‡è®ºæ–‡é’ˆå¯¹æœ‰é™ä¸ªäººæ ‡æ³¨æ ·æœ¬ä¸‹çš„ç±»å¢é‡å­¦ä¹ ï¼ˆfew-shotï¼‰é—®é¢˜è¿›è¡Œç ”ç©¶ï¼Œç‰¹åˆ«æ˜¯åœ¨è¶…ä½æ ·æœ¬åœºæ™¯ï¼ˆæ¯ä¸ªæ–°ç±»åˆ«åªæœ‰ä¸€ä¸ªæ ·æœ¬ï¼‰ä¸‹ã€‚ä¸ºè§£å†³é€‚åº”æ–°ç±»åˆ«ä¸ä¿æŒåŸºç¡€ç±»åˆ«æ€§èƒ½ä¹‹é—´çš„æƒè¡¡é—®é¢˜ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„æœºåˆ¶ï¼Œå¹¶å¼•å…¥äº†ä¸€ä¸ªæ–°ç±»æ£€æµ‹è§„åˆ™ï¼ˆNovel Class Detectionï¼ŒNCDï¼‰ã€‚è¯¥æœºåˆ¶æœ‰åŠ©äºè°ƒæ•´é—å¿˜ç¨‹åº¦å¹¶æé«˜æ–°ç±»åˆ«çš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸åŒçš„FSCILæ–¹æ³•ä¸­å…·æœ‰è‰¯å¥½çš„é€‚ç”¨æ€§ï¼Œåœ¨CIFAR100æ•°æ®é›†ä¸Šçš„æ–°ç±»åˆ«å‡†ç¡®ç‡æé«˜äº†30%ï¼ŒåŒæ—¶ä¿æŒåŸºç¡€ç±»åˆ«é—å¿˜ç‡ä»…ä¸º2%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®ºæ–‡èšç„¦äºæœ‰é™ä¸ªäººæ ‡æ³¨æ ·æœ¬ä¸‹çš„ç±»å¢é‡å­¦ä¹ é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨è¶…ä½æ ·æœ¬åœºæ™¯ä¸‹çš„æŒ‘æˆ˜ã€‚</li>
<li>è®ºæ–‡æå‡ºäº†ä¸€ä¸ªç®€å•æœ‰æ•ˆçš„æœºåˆ¶æ¥è§£å†³é€‚åº”æ–°ç±»åˆ«ä¸ä¿æŒåŸºç¡€ç±»åˆ«æ€§èƒ½ä¹‹é—´çš„æƒè¡¡é—®é¢˜ã€‚</li>
<li>å¼•å…¥äº†ä¸€ä¸ªåä¸ºNovel Class Detectionï¼ˆNCDï¼‰çš„æ–°ç±»æ£€æµ‹è§„åˆ™ï¼Œç”¨äºè°ƒæ•´é—å¿˜ç¨‹åº¦å’Œæé«˜æ–°ç±»åˆ«çš„æ€§èƒ½ã€‚</li>
<li>è®ºæ–‡å¼•å…¥äº†æ–°çš„è¯„ä¼°æŒ‡æ ‡ï¼šNCR@2FORå’ŒNCR@5FORï¼Œä»¥æ›´å¥½åœ°é‡åŒ–æ–°ç±»åˆ«å’ŒåŸºç¡€ç±»åˆ«æ€§èƒ½ä¹‹é—´çš„æƒè¡¡ã€‚</li>
<li>è®ºæ–‡æ–¹æ³•åœ¨ä¸åŒçš„FSCILæ–¹æ³•ä¸­å…·æœ‰è‰¯å¥½é€‚ç”¨æ€§ã€‚</li>
<li>åœ¨CIFAR100æ•°æ®é›†ä¸Šï¼Œæ–°ç±»åˆ«å‡†ç¡®ç‡æé«˜äº†30%ï¼ŒåŒæ—¶åŸºç¡€ç±»åˆ«é—å¿˜ç‡ä»…ä¸º2%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.15998">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0ec8e84a5e29cb0429329747302da5c7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-45fc895398e908afdc77eb5d7c148af7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-23753de4f0d67b69da7653625877e002.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="MM-Retinal-V2-Transfer-an-Elite-Knowledge-Spark-into-Fundus-Vision-Language-Pretraining"><a href="#MM-Retinal-V2-Transfer-an-Elite-Knowledge-Spark-into-Fundus-Vision-Language-Pretraining" class="headerlink" title="MM-Retinal V2: Transfer an Elite Knowledge Spark into Fundus   Vision-Language Pretraining"></a>MM-Retinal V2: Transfer an Elite Knowledge Spark into Fundus   Vision-Language Pretraining</h2><p><strong>Authors:Ruiqi Wu, Na Su, Chenran Zhang, Tengfei Ma, Tao Zhou, Zhiting Cui, Nianfeng Tang, Tianyu Mao, Yi Zhou, Wen Fan, Tianxing Wu, Shenqi Jing, Huazhu Fu</strong></p>
<p>Vision-language pretraining (VLP) has been investigated to generalize across diverse downstream tasks for fundus image analysis. Although recent methods showcase promising achievements, they significantly rely on large-scale private image-text data but pay less attention to the pretraining manner, which limits their further advancements. In this work, we introduce MM-Retinal V2, a high-quality image-text paired dataset comprising CFP, FFA, and OCT image modalities. Then, we propose a novel fundus vision-language pretraining model, namely KeepFIT V2, which is pretrained by integrating knowledge from the elite data spark into categorical public datasets. Specifically, a preliminary textual pretraining is adopted to equip the text encoder with primarily ophthalmic textual knowledge. Moreover, a hybrid image-text knowledge injection module is designed for knowledge transfer, which is essentially based on a combination of global semantic concepts from contrastive learning and local appearance details from generative learning. Extensive experiments across zero-shot, few-shot, and linear probing settings highlight the generalization and transferability of KeepFIT V2, delivering performance competitive to state-of-the-art fundus VLP models trained on large-scale private image-text datasets. Our dataset and model are publicly available via <a target="_blank" rel="noopener" href="https://github.com/lxirich/MM-Retinal">https://github.com/lxirich/MM-Retinal</a>. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€é¢„è®­ç»ƒï¼ˆVLPï¼‰åœ¨çœ¼åº•å›¾åƒåˆ†æçš„å¤šç§ä¸‹æ¸¸ä»»åŠ¡ä¸­å¾—åˆ°äº†å¹¿æ³›çš„åº”ç”¨ã€‚å°½ç®¡æœ€è¿‘çš„æ–¹æ³•å–å¾—äº†ä»¤äººç©ç›®çš„æˆæœï¼Œä½†å®ƒä»¬ä¸¥é‡ä¾èµ–äºå¤§è§„æ¨¡çš„ç§æœ‰å›¾åƒæ–‡æœ¬æ•°æ®ï¼Œå´å¿½è§†äº†é¢„è®­ç»ƒçš„æ–¹å¼ï¼Œè¿™é™åˆ¶äº†å…¶è¿›ä¸€æ­¥çš„è¿›å±•ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†MM-Retinal V2ï¼Œè¿™æ˜¯ä¸€ä¸ªé«˜è´¨é‡çš„å›¾æ–‡æœ¬é…å¯¹æ•°æ®é›†ï¼ŒåŒ…å«CFPã€FFAå’ŒOCTå›¾åƒæ¨¡å¼ã€‚ç„¶åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„çœ¼åº•è§†è§‰è¯­è¨€é¢„è®­ç»ƒæ¨¡å‹ï¼Œå³KeepFIT V2ã€‚è¯¥æ¨¡å‹é€šè¿‡ç²¾è‹±æ•°æ®çš„ç«èŠ±çŸ¥è¯†èå…¥åˆ†ç±»å…¬å…±æ•°æ®é›†è¿›è¡Œé¢„è®­ç»ƒã€‚å…·ä½“æ¥è¯´ï¼Œé‡‡ç”¨åˆæ­¥çš„æ–‡æœ¬é¢„è®­ç»ƒï¼Œä¸ºæ–‡æœ¬ç¼–ç å™¨é…å¤‡ä¸»è¦çš„çœ¼ç§‘æ–‡æœ¬çŸ¥è¯†ã€‚æ­¤å¤–ï¼Œè®¾è®¡äº†ä¸€ä¸ªæ··åˆå›¾åƒæ–‡æœ¬çŸ¥è¯†æ³¨å…¥æ¨¡å—è¿›è¡ŒçŸ¥è¯†è½¬ç§»ï¼Œè¿™åŸºæœ¬ä¸Šæ˜¯åŸºäºå¯¹æ¯”å­¦ä¹ ä¸­å…¨å±€è¯­ä¹‰æ¦‚å¿µå’Œç”Ÿæˆå­¦ä¹ ä¸­å±€éƒ¨å¤–è§‚ç»†èŠ‚çš„èåˆã€‚åœ¨é›¶æ ·æœ¬ã€å°‘æ ·æœ¬å’Œçº¿æ€§æ¢æµ‹è®¾ç½®ä¸­çš„å¤§é‡å®éªŒè¡¨æ˜äº†KeepFIT V2çš„é€šç”¨æ€§å’Œå¯è¿ç§»æ€§ï¼Œå…¶æ€§èƒ½ä¸åœ¨å¤§è§„æ¨¡ç§æœ‰å›¾åƒæ–‡æœ¬æ•°æ®é›†ä¸Šè®­ç»ƒçš„æœ€æ–°çœ¼åº•VLPæ¨¡å‹ç›¸å½“ã€‚æˆ‘ä»¬çš„æ•°æ®é›†å’Œæ¨¡å‹å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/lxirich/MM-Retinal%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/lxirich/MM-Retinalå…¬å¼€è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.15798v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹çœ¼åº•å›¾åƒåˆ†æçš„è·¨ä»»åŠ¡é€šç”¨åŒ–çš„è§†è§‰è¯­è¨€é¢„è®­ç»ƒï¼ˆVLPï¼‰ç ”ç©¶ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•è¿‡åº¦ä¾èµ–å¤§è§„æ¨¡ç§æœ‰å›¾åƒæ–‡æœ¬æ•°æ®è€Œå¿½è§†é¢„è®­ç»ƒæ–¹å¼çš„é—®é¢˜ï¼Œæå‡ºäº†æ–°å‹çš„çœ¼åº•è§†è§‰è¯­è¨€é¢„è®­ç»ƒæ¨¡å‹KeepFIT V2ã€‚è¯¥æ¨¡å‹é€šè¿‡æ•´åˆç²¾è‹±æ•°æ®å’Œå…¬å…±åˆ†ç±»æ•°æ®é›†çš„çŸ¥è¯†è¿›è¡Œé¢„è®­ç»ƒã€‚å®éªŒè¡¨æ˜ï¼ŒKeepFIT V2å…·æœ‰è‰¯å¥½çš„æ³›åŒ–æ€§å’Œè¿ç§»æ€§ï¼Œæ€§èƒ½ä¸åœ¨å¤§è§„æ¨¡ç§æœ‰å›¾åƒæ–‡æœ¬æ•°æ®é›†ä¸Šè®­ç»ƒçš„æœ€æ–°çœ¼åº•VLPæ¨¡å‹ç›¸å½“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥MM-Retinal V2æ•°æ®é›†ï¼ŒåŒ…å«å¤šç§çœ¼åº•å›¾åƒæ¨¡æ€ï¼Œå¦‚CFPã€FFAå’ŒOCTã€‚</li>
<li>æå‡ºæ–°å‹çœ¼åº•è§†è§‰è¯­è¨€é¢„è®­ç»ƒæ¨¡å‹KeepFIT V2ã€‚</li>
<li>KeepFIT V2æ•´åˆç²¾è‹±æ•°æ®å’Œå…¬å…±åˆ†ç±»æ•°æ®é›†çš„çŸ¥è¯†è¿›è¡Œé¢„è®­ç»ƒã€‚</li>
<li>é‡‡ç”¨åˆæ­¥æ–‡æœ¬é¢„è®­ç»ƒï¼Œä¸ºæ–‡æœ¬ç¼–ç å™¨æä¾›çœ¼ç§‘æ–‡æœ¬çŸ¥è¯†ã€‚</li>
<li>è®¾è®¡äº†æ··åˆå›¾åƒ-æ–‡æœ¬çŸ¥è¯†æ³¨å…¥æ¨¡å—ï¼Œå®ç°çŸ¥è¯†è¿ç§»ã€‚</li>
<li>å®éªŒè¯æ˜KeepFIT V2å…·æœ‰è‰¯å¥½çš„æ³›åŒ–æ€§å’Œè¿ç§»æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.15798">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-82bb9142517024dff4b501760efb3ecd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-898edab09416bd4ee4d4bc9e188f303c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b8dee35409ed0a658d1732e24ab5e575.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-26f629cf511fc715688d9431b0548539.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="OptiSeq-Optimizing-Example-Ordering-for-In-Context-Learning"><a href="#OptiSeq-Optimizing-Example-Ordering-for-In-Context-Learning" class="headerlink" title="OptiSeq: Optimizing Example Ordering for In-Context Learning"></a>OptiSeq: Optimizing Example Ordering for In-Context Learning</h2><p><strong>Authors:Rahul Atul Bhope, Praveen Venkateswaran, K. R. Jayaram, Vatche Isahagian, Vinod Muthusamy, Nalini Venkatasubramanian</strong></p>
<p>Developers using LLMs in their applications and agents have provided plenty of anecdotal evidence that in-context-learning (ICL) is fragile. In addition to the quantity and quality of examples, we show that the order in which the in-context examples are listed in the prompt affects the output of the LLM and, consequently, their performance. In this paper, we present OptiSeq, which introduces a score based on log probabilities of LLM outputs to prune the universe of possible example orderings in few-shot ICL and recommend the best order(s) by distinguishing between correct and incorrect outputs resulting from different order permutations. Through a detailed empirical evaluation on multiple LLMs, datasets and prompts, we demonstrate that OptiSeq improves accuracy by 6 - 10.5 percentage points across multiple tasks. </p>
<blockquote>
<p>å¯¹äºåœ¨å…¶åº”ç”¨ç¨‹åºå’Œä»£ç†ä¸­ä½¿ç”¨LLMçš„å¼€å‘äººå‘˜æ¥è¯´ï¼Œä»–ä»¬æä¾›äº†å¤§é‡çš„ç»éªŒè¯æ®è¡¨æ˜ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰æ˜¯è„†å¼±çš„ã€‚é™¤äº†ç¤ºä¾‹çš„æ•°é‡å’Œè´¨é‡ä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜è¡¨æ˜æç¤ºä¸­ä¸Šä¸‹æ–‡ç¤ºä¾‹çš„åˆ—è¡¨é¡ºåºä¼šå½±å“LLMçš„è¾“å‡ºä»¥åŠä»–ä»¬çš„æ€§èƒ½ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†OptiSeqï¼Œå®ƒé€šè¿‡åŸºäºLLMè¾“å‡ºçš„å¯¹æ•°æ¦‚ç‡çš„è¯„åˆ†æ¥ç¼©å‡å°‘é‡ä¸Šä¸‹æ–‡å­¦ä¹ ä¸­çš„å¯èƒ½ç¤ºä¾‹é¡ºåºå®‡å®™ï¼Œå¹¶é€šè¿‡åŒºåˆ†ä¸åŒé¡ºåºæ’åˆ—å¯¼è‡´çš„æ­£ç¡®å’Œé”™è¯¯è¾“å‡ºï¼Œæ¨èæœ€ä½³é¡ºåºã€‚é€šè¿‡å¯¹å¤šä¸ªLLMã€æ•°æ®é›†å’Œæç¤ºçš„è¯¦ç»†ç»éªŒè¯„ä¼°ï¼Œæˆ‘ä»¬è¯æ˜äº†OptiSeqåœ¨å¤šä¸ªä»»åŠ¡ä¸Šçš„å‡†ç¡®ç‡æé«˜äº†6-10.5ä¸ªç™¾åˆ†ç‚¹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.15030v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>LLMåº”ç”¨ä¸­çš„ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰å­˜åœ¨è„†å¼±æ€§ï¼Œé™¤äº†ç¤ºä¾‹çš„æ•°é‡å’Œè´¨é‡å¤–ï¼Œç¤ºä¾‹åœ¨æç¤ºä¸­çš„é¡ºåºä¹Ÿä¼šå½±å“LLMçš„è¾“å‡ºå’Œæ€§èƒ½ã€‚æœ¬æ–‡æå‡ºOptiSeqæ–¹æ³•ï¼Œé€šè¿‡è®¡ç®—LLMè¾“å‡ºçš„å¯¹æ•°æ¦‚ç‡å¾—åˆ†æ¥è¯„ä¼°ä¸åŒç¤ºä¾‹é¡ºåºçš„å¯èƒ½æ€§ï¼Œå¹¶æ¨èæœ€ä½³é¡ºåºç»„åˆã€‚ç»éªŒè¯„ä¼°è¡¨æ˜ï¼ŒOptiSeqèƒ½æ˜¾è‘—æé«˜å¤šä¸ªä»»åŠ¡çš„å‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsåœ¨åº”ç”¨ä¸­è¡¨ç°å‡ºä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰çš„è„†å¼±æ€§ã€‚</li>
<li>ç¤ºä¾‹åœ¨æç¤ºä¸­çš„é¡ºåºå¯¹LLMçš„è¾“å‡ºå’Œæ€§èƒ½æœ‰å½±å“ã€‚</li>
<li>OptiSeqæ–¹æ³•é€šè¿‡è®¡ç®—LLMè¾“å‡ºçš„å¯¹æ•°æ¦‚ç‡å¾—åˆ†æ¥è¯„ä¼°ç¤ºä¾‹é¡ºåºã€‚</li>
<li>OptiSeqèƒ½æé«˜å¤šä¸ªä»»åŠ¡çš„å‡†ç¡®æ€§ï¼Œæé«˜å¹…åº¦è¾¾åˆ°6-10.5ä¸ªç™¾åˆ†ç‚¹ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½åœ¨ä¸åŒLLMã€æ•°æ®é›†å’Œæç¤ºä¸­æœ‰æ•ˆåº”ç”¨ã€‚</li>
<li>OptiSeqèƒ½åŒºåˆ†ä¸åŒé¡ºåºç»„åˆäº§ç”Ÿçš„æ­£ç¡®å’Œé”™è¯¯è¾“å‡ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.15030">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-b97971296debcec21a3b213a2e39d374.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f7f3523dadd0949b4663d3e1c8c6d74f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2e075ee8cbfcef2828060622400c1a05.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5c4d2828d425c014cc9211ed86676153.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-21b06bc27b8630a5248672f6b4a18bb7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d7bad618bd8cd328ce1073a2972d4c0f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a111eb10247d1db118f5dc7cd529d14e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-30dc685adcd54efc512b082b88c4c0df.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2e2a2bd12f16d0024f841b2aa83f40be.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Training-Dialogue-Systems-by-AI-Feedback-for-Improving-Overall-Dialogue-Impression"><a href="#Training-Dialogue-Systems-by-AI-Feedback-for-Improving-Overall-Dialogue-Impression" class="headerlink" title="Training Dialogue Systems by AI Feedback for Improving Overall Dialogue   Impression"></a>Training Dialogue Systems by AI Feedback for Improving Overall Dialogue   Impression</h2><p><strong>Authors:Kai Yoshida, Masahiro Mizukami, Seiya Kawano, Canasai Kruengkrai, Hiroaki Sugiyama, Koichiro Yoshino</strong></p>
<p>To improve user engagement during conversations with dialogue systems, we must improve individual dialogue responses and dialogue impressions such as consistency, personality, and empathy throughout the entire dialogue. While such dialogue systems have been developing rapidly with the help of large language models (LLMs), reinforcement learning from AI feedback (RLAIF) has attracted attention to align LLM-based dialogue models for such dialogue impressions. In RLAIF, a reward model based on another LLM is used to create a training signal for an LLM-based dialogue model using zero-shot&#x2F;few-shot prompting techniques. However, evaluating an entire dialogue only by prompting LLMs is challenging. In this study, the supervised fine-tuning (SFT) of LLMs prepared reward models corresponding to 12 metrics related to the impression of the entire dialogue for evaluating dialogue responses. We tuned our dialogue models using the reward model signals as feedback to improve the impression of the system. The results of automatic and human evaluations showed that tuning the dialogue model using our reward model corresponding to dialogue impression improved the evaluation of individual metrics and the naturalness of the dialogue response. </p>
<blockquote>
<p>ä¸ºäº†æé«˜ç”¨æˆ·ä¸å¯¹è¯ç³»ç»Ÿå¯¹è¯æ—¶çš„å‚ä¸åº¦ï¼Œæˆ‘ä»¬å¿…é¡»æ”¹å–„å•ç‹¬çš„å¯¹è¯å›åº”å’Œæ•´ä¸ªå¯¹è¯è¿‡ç¨‹ä¸­çš„ä¸€è‡´æ€§ã€ä¸ªæ€§å’ŒåŒç†å¿ƒç­‰å¯¹è¯å°è±¡ã€‚è™½ç„¶å€ŸåŠ©å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œæ­¤ç±»å¯¹è¯ç³»ç»Ÿå‘å±•è¿…é€Ÿï¼Œä½†é€šè¿‡äººå·¥æ™ºèƒ½åé¦ˆè¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLAIFï¼‰å·²å¼•èµ·å…³æ³¨ï¼Œä»¥è°ƒæ•´åŸºäºLLMçš„å¯¹è¯æ¨¡å‹ï¼Œä½¿å…¶å…·æœ‰æ­¤ç±»å¯¹è¯å°è±¡ã€‚åœ¨RLAIFä¸­ï¼Œä½¿ç”¨åŸºäºå¦ä¸€ä¸ªLLMçš„å¥–åŠ±æ¨¡å‹ï¼Œé€šè¿‡é›¶æ ·æœ¬&#x2F;å°æ ·æœ¬æç¤ºæŠ€æœ¯ä¸ºåŸºäºLLMçš„å¯¹è¯æ¨¡å‹åˆ›å»ºè®­ç»ƒä¿¡å·ã€‚ç„¶è€Œï¼Œä»…é€šè¿‡æç¤ºLLMæ¥è¯„ä¼°æ•´ä¸ªå¯¹è¯æ˜¯æœ‰æŒ‘æˆ˜æ€§çš„ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å‡†å¤‡äº†å¯¹åº”äºä¸æ•´ä¸ªå¯¹è¯å°è±¡ç›¸å…³çš„12ä¸ªæŒ‡æ ‡çš„å¥–åŠ±æ¨¡å‹ï¼Œå¯¹LLMè¿›è¡Œæœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œä»¥è¯„ä¼°å¯¹è¯å›åº”ã€‚æˆ‘ä»¬ä½¿ç”¨å¥–åŠ±æ¨¡å‹ä¿¡å·ä½œä¸ºåé¦ˆæ¥è°ƒæ•´æˆ‘ä»¬çš„å¯¹è¯æ¨¡å‹ï¼Œä»¥æé«˜ç³»ç»Ÿå°è±¡ã€‚è‡ªåŠ¨å’Œäººç±»è¯„ä¼°çš„ç»“æœè¡¨æ˜ï¼Œä½¿ç”¨å¯¹åº”äºå¯¹è¯å°è±¡çš„æˆ‘ä»¬çš„å¥–åŠ±æ¨¡å‹è°ƒæ•´å¯¹è¯æ¨¡å‹ï¼Œæé«˜äº†å•ä¸ªæŒ‡æ ‡çš„è¯„ä»·å’Œå¯¹è¯å›åº”çš„è‡ªç„¶æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.12698v2">PDF</a> Accepted to ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æ¢è®¨äº†å¦‚ä½•é€šè¿‡æ”¹è¿›å¯¹è¯ç³»ç»Ÿçš„å“åº”å’Œå°è±¡æ¥æå‡ç”¨æˆ·å‚ä¸åº¦ã€‚ç ”ç©¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œå¼ºåŒ–å­¦ä¹ æŠ€æœ¯ï¼Œé€šè¿‡æ„å»ºå¥–åŠ±æ¨¡å‹æ¥è°ƒæ•´å¯¹è¯æ¨¡å‹ï¼Œä½¿å…¶æ›´å…·ä¸€è‡´æ€§ã€ä¸ªæ€§å’ŒåŒç†å¿ƒã€‚ä¸ºæé«˜æ•´ä¸ªå¯¹è¯çš„å°è±¡è¯„ä»·ï¼Œè¯¥ç ”ç©¶å¯¹å¥–åŠ±æ¨¡å‹è¿›è¡Œäº†å¾®è°ƒï¼Œå¯¹åº”12ä¸ªä¸å¯¹è¯å°è±¡ç›¸å…³çš„æŒ‡æ ‡ã€‚è‡ªåŠ¨å’Œäººç±»è¯„ä¼°ç»“æœå‡æ˜¾ç¤ºï¼Œä½¿ç”¨å¯¹åº”å¯¹è¯å°è±¡çš„å¥–åŠ±æ¨¡å‹è°ƒæ•´å¯¹è¯æ¨¡å‹ï¼Œèƒ½æå‡ä¸ªåˆ«æŒ‡æ ‡çš„è¯„ä»·å’Œå¯¹è¯çš„è‡ªç„¶æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¯¹è¯ç³»ç»Ÿçš„ç”¨æˆ·å‚ä¸åº¦å¯é€šè¿‡ä¼˜åŒ–å“åº”å’Œå°è±¡æå‡ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œå¼ºåŒ–å­¦ä¹ æŠ€æœ¯ç”¨äºè°ƒæ•´å¯¹è¯æ¨¡å‹ã€‚</li>
<li>å¥–åŠ±æ¨¡å‹ç”¨äºåˆ›å»ºè®­ç»ƒä¿¡å·ï¼Œä»¥è°ƒæ•´LLMçš„å¯¹è¯æ¨¡å‹ã€‚</li>
<li>é€šè¿‡é›¶æ ·æœ¬æˆ–å°‘æ ·æœ¬æç¤ºæŠ€æœ¯å®ç°è®­ç»ƒä¿¡å·çš„åˆ›å»ºã€‚</li>
<li>ä»…é€šè¿‡æç¤ºLLMæ¥è¯„ä¼°æ•´ä¸ªå¯¹è¯å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>ç ”ç©¶é€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰LLMçš„å¥–åŠ±æ¨¡å‹ï¼Œå¯¹åº”12ä¸ªä¸å¯¹è¯å°è±¡ç›¸å…³çš„æŒ‡æ ‡ä»¥è¯„ä¼°å¯¹è¯å“åº”ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.12698">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-663cbd52a3e6783205b0c5d6b2205dd8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d9bed7ee4bf23bbd5aab512ed146e754.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1eee9f51a76bf6a45e49ed9e7f2a8c47.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e7824fcb58abd822c5d41d72175c1a34.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-17297d572a8908ccbe62fcb645a86940.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fed41c889a3d50c820464427bbd4a5cc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3eb33e4dc3287342d7e78f631a8e4c48.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Text-driven-Adaptation-of-Foundation-Models-for-Few-shot-Surgical-Workflow-Analysis"><a href="#Text-driven-Adaptation-of-Foundation-Models-for-Few-shot-Surgical-Workflow-Analysis" class="headerlink" title="Text-driven Adaptation of Foundation Models for Few-shot Surgical   Workflow Analysis"></a>Text-driven Adaptation of Foundation Models for Few-shot Surgical   Workflow Analysis</h2><p><strong>Authors:Tingxuan Chen, Kun Yuan, Vinkle Srivastav, Nassir Navab, Nicolas Padoy</strong></p>
<p>Purpose: Surgical workflow analysis is crucial for improving surgical efficiency and safety. However, previous studies rely heavily on large-scale annotated datasets, posing challenges in cost, scalability, and reliance on expert annotations. To address this, we propose Surg-FTDA (Few-shot Text-driven Adaptation), designed to handle various surgical workflow analysis tasks with minimal paired image-label data.   Methods: Our approach has two key components. First, Few-shot selection-based modality alignment selects a small subset of images and aligns their embeddings with text embeddings from the downstream task, bridging the modality gap. Second, Text-driven adaptation leverages only text data to train a decoder, eliminating the need for paired image-text data. This decoder is then applied to aligned image embeddings, enabling image-related tasks without explicit image-text pairs.   Results: We evaluate our approach to generative tasks (image captioning) and discriminative tasks (triplet recognition and phase recognition). Results show that Surg-FTDA outperforms baselines and generalizes well across downstream tasks.   Conclusion: We propose a text-driven adaptation approach that mitigates the modality gap and handles multiple downstream tasks in surgical workflow analysis, with minimal reliance on large annotated datasets. The code and dataset will be released in <a target="_blank" rel="noopener" href="https://github.com/CAMMA-public/Surg-FTDA">https://github.com/CAMMA-public/Surg-FTDA</a> </p>
<blockquote>
<p>ç›®çš„ï¼šæ‰‹æœ¯æµç¨‹åˆ†æå¯¹äºæé«˜æ‰‹æœ¯æ•ˆç‡å’Œå®‰å…¨æ€§è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œå…ˆå‰çš„ç ”ç©¶ä¸¥é‡ä¾èµ–äºå¤§è§„æ¨¡æ ‡æ³¨æ•°æ®é›†ï¼Œè¿™å¸¦æ¥äº†æˆæœ¬ã€å¯æ‰©å±•æ€§å’Œå¯¹ä¸“å®¶æ ‡æ³¨çš„ä¾èµ–ç­‰æ–¹é¢çš„æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Surg-FTDAï¼ˆFew-shotæ–‡æœ¬é©±åŠ¨é€‚åº”ï¼‰æ–¹æ³•ï¼Œæ—¨åœ¨ä½¿ç”¨æœ€å°‘çš„é…å¯¹å›¾åƒæ ‡ç­¾æ•°æ®æ¥å¤„ç†å„ç§æ‰‹æœ¯æµç¨‹åˆ†æä»»åŠ¡ã€‚</p>
</blockquote>
<p>æ–¹æ³•ï¼šæˆ‘ä»¬çš„æ–¹æ³•æœ‰ä¸¤ä¸ªå…³é”®ç»„æˆéƒ¨åˆ†ã€‚é¦–å…ˆï¼ŒåŸºäºFew-shoté€‰æ‹©çš„æ¨¡æ€å¯¹é½ä¼šé€‰æ‹©ä¸€å°éƒ¨åˆ†å›¾åƒï¼Œå¹¶å°†å®ƒä»¬çš„åµŒå…¥ä¸ä¸‹æ¸¸ä»»åŠ¡çš„æ–‡æœ¬åµŒå…¥è¿›è¡Œå¯¹é½ï¼Œä»è€Œå¼¥åˆäº†æ¨¡æ€ä¹‹é—´çš„å·®è·ã€‚å…¶æ¬¡ï¼Œæ–‡æœ¬é©±åŠ¨é€‚åº”ä»…åˆ©ç”¨æ–‡æœ¬æ•°æ®è¿›è¡Œè§£ç å™¨è®­ç»ƒï¼Œæ¶ˆé™¤äº†å¯¹é…å¯¹å›¾åƒæ–‡æœ¬æ•°æ®çš„éœ€æ±‚ã€‚ç„¶åï¼Œè¯¥è§£ç å™¨åº”ç”¨äºå·²å¯¹é½çš„å›¾åƒåµŒå…¥ï¼Œèƒ½å¤Ÿåœ¨æ²¡æœ‰æ˜ç¡®çš„å›¾åƒæ–‡æœ¬å¯¹çš„æƒ…å†µä¸‹æ‰§è¡Œå›¾åƒç›¸å…³ä»»åŠ¡ã€‚</p>
<p>ç»“æœï¼šæˆ‘ä»¬å¯¹ç”Ÿæˆä»»åŠ¡ï¼ˆå›¾åƒå­—å¹•ï¼‰å’Œåˆ¤åˆ«ä»»åŠ¡ï¼ˆä¸‰å…ƒç»„è¯†åˆ«å’Œé˜¶æ®µè¯†åˆ«ï¼‰è¿›è¡Œäº†è¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼ŒSurg-FTDAä¼˜äºåŸºçº¿ï¼Œå¹¶åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.09555v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºSurg-FTDAï¼ˆFew-shotæ–‡æœ¬é©±åŠ¨é€‚åº”ï¼‰çš„æ–¹æ³•ï¼Œç”¨äºæ”¹è¿›æ‰‹æœ¯æ•ˆç‡å’Œå®‰å…¨æ€§çš„æ‰‹æœ¯å·¥ä½œæµç¨‹åˆ†æã€‚è¯¥æ–¹æ³•ä¸»è¦è§£å†³äº†å¤§è§„æ¨¡æ³¨é‡Šæ•°æ®é›†å¸¦æ¥çš„æˆæœ¬ã€å¯æ‰©å±•æ€§å’Œä¾èµ–ä¸“å®¶æ ‡æ³¨çš„é—®é¢˜ã€‚å®ƒé€šè¿‡é€‰æ‹©å°‘é‡å›¾åƒä¸ä¸‹æ¸¸ä»»åŠ¡çš„æ–‡æœ¬åµŒå…¥è¿›è¡Œå¯¹é½ï¼Œå¹¶æ¶ˆé™¤å¯¹é…å¯¹å›¾åƒæ–‡æœ¬æ•°æ®çš„éœ€æ±‚ï¼Œä»è€Œå¤„ç†å„ç§æ‰‹æœ¯å·¥ä½œæµç¨‹åˆ†æä»»åŠ¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç”Ÿæˆä»»åŠ¡ï¼ˆå›¾åƒæè¿°ï¼‰å’Œåˆ¤åˆ«ä»»åŠ¡ï¼ˆä¸‰å…ƒç»„è¯†åˆ«å’Œé˜¶æ®µè¯†åˆ«ï¼‰ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå…·æœ‰è‰¯å¥½çš„è·¨ä»»åŠ¡æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Surg-FTDAæ–¹æ³•æ—¨åœ¨è§£å†³æ‰‹æœ¯å·¥ä½œæµç¨‹åˆ†æä¸­å¤§è§„æ¨¡æ³¨é‡Šæ•°æ®é›†å¸¦æ¥çš„æŒ‘æˆ˜ã€‚</li>
<li>æ–¹æ³•åŒ…å«ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šFew-shoté€‰æ‹©å¼æ¨¡æ€å¯¹é½å’Œæ–‡æœ¬é©±åŠ¨é€‚åº”ã€‚</li>
<li>Few-shoté€‰æ‹©å¼æ¨¡æ€å¯¹é½é€šè¿‡é€‰æ‹©å°‘é‡å›¾åƒå¹¶ä¸ä¸‹æ¸¸ä»»åŠ¡çš„æ–‡æœ¬åµŒå…¥å¯¹é½ï¼Œç¼©å°äº†æ¨¡æ€å·®è·ã€‚</li>
<li>æ–‡æœ¬é©±åŠ¨é€‚åº”ä»…åˆ©ç”¨æ–‡æœ¬æ•°æ®è¿›è¡Œè§£ç å™¨è®­ç»ƒï¼Œæ¶ˆé™¤äº†å¯¹é…å¯¹å›¾åƒæ–‡æœ¬æ•°æ®çš„éœ€æ±‚ã€‚</li>
<li>Surg-FTDAåœ¨ç”Ÿæˆä»»åŠ¡å’Œåˆ¤åˆ«ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼ŒåŒ…æ‹¬å›¾åƒæè¿°ã€ä¸‰å…ƒç»„è¯†åˆ«å’Œé˜¶æ®µè¯†åˆ«ã€‚</li>
<li>è¯¥æ–¹æ³•å…·æœ‰è‰¯å¥½çš„è·¨ä»»åŠ¡æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.09555">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4bf509319ea7a4792171231782534989.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-741f3496eb39e29b1d523f763f7297d8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-964423334114aa83cdedea9c263296c6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d6cd0dd712f9125aeed779a98e5312ba.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c97290226d910425b794303da5958fdd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0f7713590a82ff60cd156881b244c56e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-359b7d32c8d2e3115557c0e7d2f42cbd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6de8039cc24d7d8ab1286ebf897e9b70.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2cbc91f323f3004945d059d79884d39f.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="2-5-Years-in-Class-A-Multimodal-Textbook-for-Vision-Language-Pretraining"><a href="#2-5-Years-in-Class-A-Multimodal-Textbook-for-Vision-Language-Pretraining" class="headerlink" title="2.5 Years in Class: A Multimodal Textbook for Vision-Language   Pretraining"></a>2.5 Years in Class: A Multimodal Textbook for Vision-Language   Pretraining</h2><p><strong>Authors:Wenqi Zhang, Hang Zhang, Xin Li, Jiashuo Sun, Yongliang Shen, Weiming Lu, Deli Zhao, Yueting Zhuang, Lidong Bing</strong></p>
<p>Compared to image-text pair data, interleaved corpora enable Vision-Language Models (VLMs) to understand the world more naturally like humans. However, such existing datasets are crawled from webpage, facing challenges like low knowledge density, loose image-text relations, and poor logical coherence between images. On the other hand, the internet hosts vast instructional videos (e.g., online geometry courses) that are widely used by humans to learn foundational subjects, yet these valuable resources remain underexplored in VLM training. In this paper, we introduce a high-quality \textbf{multimodal textbook} corpus with richer foundational knowledge for VLM pretraining. It collects over 2.5 years of instructional videos, totaling 22,000 class hours. We first use an LLM-proposed taxonomy to systematically gather instructional videos. Then we progressively extract and refine visual (keyframes), audio (ASR), and textual knowledge (OCR) from the videos, and organize as an image-text interleaved corpus based on temporal order. Compared to its counterparts, our video-centric textbook offers more coherent context, richer knowledge, and better image-text alignment. Experiments demonstrate its superb pretraining performance, particularly in knowledge- and reasoning-intensive tasks like ScienceQA and MathVista. Moreover, VLMs pre-trained on our textbook exhibit outstanding interleaved context awareness, leveraging visual and textual cues in their few-shot context for task solving. Our code are available at <a target="_blank" rel="noopener" href="https://github.com/DAMO-NLP-SG/multimodal_textbook">https://github.com/DAMO-NLP-SG/multimodal_textbook</a>. </p>
<blockquote>
<p>ä¸å›¾åƒæ–‡æœ¬é…å¯¹æ•°æ®ç›¸æ¯”ï¼Œäº¤ç»‡è¯­æ–™åº“ä½¿è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰èƒ½å¤Ÿæ›´è‡ªç„¶åœ°åƒäººç±»ä¸€æ ·ç†è§£ä¸–ç•Œã€‚ç„¶è€Œï¼Œè¿™äº›ç°æœ‰çš„æ•°æ®é›†éƒ½æ˜¯ä»ç½‘é¡µä¸Šçˆ¬å–çš„ï¼Œé¢ä¸´ç€çŸ¥è¯†å¯†åº¦ä½ã€å›¾åƒæ–‡æœ¬å…³ç³»æ¾æ•£ã€å›¾åƒä¹‹é—´é€»è¾‘è¿è´¯æ€§å·®ç­‰æŒ‘æˆ˜ã€‚å¦ä¸€æ–¹é¢ï¼Œäº’è”ç½‘ä¸Šæœ‰å¤§é‡æ•™å­¦è§†é¢‘ï¼ˆå¦‚åœ¨çº¿å‡ ä½•è¯¾ç¨‹ï¼‰ï¼Œäººç±»å¹¿æ³›ç”¨æ¥å­¦ä¹ åŸºç¡€å­¦ç§‘ï¼Œä½†è¿™äº›å®è´µèµ„æºåœ¨VLMè®­ç»ƒä¸­å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ä¸ªé«˜è´¨é‡çš„<strong>å¤šæ¨¡å¼æ•™ç§‘ä¹¦</strong>è¯­æ–™åº“ï¼Œå…¶ä¸­åŒ…å«æ›´ä¸°å¯Œçš„åŸºç¡€çŸ¥è¯†ï¼Œç”¨äºVLMé¢„è®­ç»ƒã€‚å®ƒæ”¶é›†äº†è¶…è¿‡2.5å¹´çš„æ•™å­¦è§†é¢‘ï¼Œæ€»è®¡22,000è¯¾æ—¶ã€‚æˆ‘ä»¬é¦–å…ˆä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æå‡ºçš„åˆ†ç±»æ³•æ¥ç³»ç»Ÿåœ°æ”¶é›†æ•™å­¦è§†é¢‘ã€‚ç„¶åï¼Œæˆ‘ä»¬ä»è§†é¢‘ä¸­é€æ­¥æå–å’Œç²¾ç‚¼è§†è§‰ï¼ˆå…³é”®å¸§ï¼‰ã€éŸ³é¢‘ï¼ˆè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼‰å’Œæ–‡æœ¬çŸ¥è¯†ï¼ˆå…‰å­¦å­—ç¬¦è¯†åˆ«ï¼‰ï¼Œå¹¶æŒ‰æ—¶é—´é¡ºåºç»„ç»‡æˆå›¾åƒæ–‡æœ¬äº¤ç»‡è¯­æ–™åº“ã€‚ä¸å…¶ä»–èµ„æºç›¸æ¯”ï¼Œæˆ‘ä»¬ä»¥å­¦ç”Ÿä¸ºä¸­å¿ƒçš„è§†é¢‘æ•™ç§‘ä¹¦æä¾›äº†æ›´è¿è´¯çš„ä¸Šä¸‹æ–‡ã€æ›´ä¸°å¯Œçš„çŸ¥è¯†å’Œæ›´å¥½çš„å›¾åƒæ–‡æœ¬å¯¹é½ã€‚å®éªŒè¯æ˜å…¶åœ¨é¢„è®­ç»ƒæ–¹é¢çš„å“è¶Šæ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨çŸ¥è¯†å’Œæ¨ç†å¯†é›†å‹ä»»åŠ¡ï¼ˆå¦‚ScienceQAå’ŒMathVistaï¼‰ä¸­ã€‚æ­¤å¤–ï¼Œåœ¨æˆ‘ä»¬æ•™ç§‘ä¹¦ä¸Šè¿›è¡Œé¢„è®­ç»ƒçš„VLMè¡¨ç°å‡ºå‡ºè‰²äº¤ç»‡ä¸Šä¸‹æ–‡æ„è¯†ï¼Œåˆ©ç”¨è§†è§‰å’Œæ–‡æœ¬çº¿ç´¢è§£å†³å°‘é‡ä¸Šä¸‹æ–‡ä¸­çš„ä»»åŠ¡ã€‚æˆ‘ä»¬çš„ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/DAMO-NLP-SG/multimodal_textbook%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/DAMO-NLP-SG/multimodal_textbookè®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.00958v3">PDF</a> Under review</p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ç”¨äºè§†è§‰è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒçš„å¤šæ¨¡æ€æ•™ç§‘ä¹¦è¯­æ–™åº“ã€‚è¯¥è¯­æ–™åº“é€šè¿‡æ”¶é›†è¶…è¿‡2.5å¹´çš„æ•™å­¦è§†é¢‘ï¼Œä»¥æ—¶é—´é¡ºåºç»„ç»‡æˆå›¾åƒæ–‡æœ¬äº¤é”™è¯­æ–™åº“ã€‚ä¸ä¼ ç»Ÿæ•°æ®é›†ç›¸æ¯”ï¼Œè¯¥è§†é¢‘ä¸ºä¸­å¿ƒçš„æ•™å­¦ä¹¦ç±æä¾›äº†æ›´è¿è´¯çš„ä¸Šä¸‹æ–‡ã€æ›´ä¸°å¯ŒçŸ¥è¯†å’Œæ›´å¥½çš„å›¾åƒæ–‡æœ¬å¯¹é½ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨çŸ¥è¯†å¯†é›†å’Œæ¨ç†å¯†é›†å‹ä»»åŠ¡ä¸Šï¼Œè¯¥è¯­æ–™åº“çš„é¢„è®­ç»ƒæ€§èƒ½å“è¶Šã€‚æ­¤å¤–ï¼Œåœ¨è¯¥æ•™ç§‘ä¹¦ä¸Šè¿›è¡Œé¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹å±•ç°å‡ºå‡ºè‰²çš„äº¤é”™ä¸Šä¸‹æ–‡æ„è¯†ï¼Œèƒ½å¤Ÿåœ¨è§£å†³ä»»åŠ¡æ—¶åˆ©ç”¨è§†è§‰å’Œæ–‡æœ¬çº¿ç´¢ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤šæ¨¡æ€æ•™ç§‘ä¹¦è¯­æ–™åº“ç»“åˆäº†æ•™å­¦è§†é¢‘ã€å›¾åƒå’Œæ–‡æœ¬ï¼Œä¸ºè§†è§‰è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒæä¾›äº†æ›´ä¸°å¯Œã€æ›´è¿è´¯çš„çŸ¥è¯†ã€‚</li>
<li>ä¸ä¼ ç»Ÿå›¾åƒæ–‡æœ¬å¯¹æ•°æ®ç›¸æ¯”ï¼Œè¯¥è¯­æ–™åº“å…·æœ‰æ›´å¥½çš„å›¾åƒæ–‡æœ¬å¯¹é½å’Œé€»è¾‘è¿è´¯æ€§ã€‚</li>
<li>é€šè¿‡æ”¶é›†å¤§é‡æ•™å­¦è§†é¢‘ï¼Œè¯¥è¯­æ–™åº“æä¾›äº†ä¸€ä¸ªè§†é¢‘ä¸ºä¸­å¿ƒçš„å­¦ä¹ èµ„æºå¹³å°ã€‚</li>
<li>åœ¨çŸ¥è¯†å¯†é›†å’Œæ¨ç†å¯†é›†å‹ä»»åŠ¡ä¸Šï¼Œè¯¥è¯­æ–™åº“çš„é¢„è®­ç»ƒæ€§èƒ½å“è¶Šã€‚</li>
<li>é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹å±•ç°å‡ºå‡ºè‰²çš„äº¤é”™ä¸Šä¸‹æ–‡æ„è¯†ï¼Œèƒ½åˆ©ç”¨è§†è§‰å’Œæ–‡æœ¬çº¿ç´¢è§£å†³ä»»åŠ¡ã€‚</li>
<li>è¯¥ç ”ç©¶æä¾›äº†ä¸€ä¸ªå…¬å¼€å¯ç”¨çš„ä»£ç åº“ï¼Œä¾¿äºå…¶ä»–äººä½¿ç”¨å’Œç ”ç©¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.00958">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d516ba446c3965f22a1692dffef38013.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cb24204dd4e6ab7ecc70e3204b01c907.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a9ad43a10ebfc11a2ed1edf5a99a6bc4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7cf76536efe3ec3e379acb807abb9863.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Revisiting-In-context-Learning-Inference-Circuit-in-Large-Language-Models"><a href="#Revisiting-In-context-Learning-Inference-Circuit-in-Large-Language-Models" class="headerlink" title="Revisiting In-context Learning Inference Circuit in Large Language   Models"></a>Revisiting In-context Learning Inference Circuit in Large Language   Models</h2><p><strong>Authors:Hakaze Cho, Mariko Kato, Yoshihiro Sakai, Naoya Inoue</strong></p>
<p>In-context Learning (ICL) is an emerging few-shot learning paradigm on Language Models (LMs) with inner mechanisms un-explored. There are already existing works describing the inner processing of ICL, while they struggle to capture all the inference phenomena in large language models. Therefore, this paper proposes a comprehensive circuit to model the inference dynamics and try to explain the observed phenomena of ICL. In detail, we divide ICL inference into 3 major operations: (1) Input Text Encode: LMs encode every input text (demonstrations and queries) into linear representation in the hidden states with sufficient information to solve ICL tasks. (2) Semantics Merge: LMs merge the encoded representations of demonstrations with their corresponding label tokens to produce joint representations of labels and demonstrations. (3) Feature Retrieval and Copy: LMs search the joint representations similar to the query representation on a task subspace, and copy the searched representations into the query. Then, language model heads capture these copied label representations to a certain extent and decode them into predicted labels. The proposed inference circuit successfully captured many phenomena observed during the ICL process, making it a comprehensive and practical explanation of the ICL inference process. Moreover, ablation analysis by disabling the proposed steps seriously damages the ICL performance, suggesting the proposed inference circuit is a dominating mechanism. Additionally, we confirm and list some bypass mechanisms that solve ICL tasks in parallel with the proposed circuit. </p>
<blockquote>
<p>ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰æ˜¯ä¸€ç§æ–°å…´çš„è¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰å°æ ·æœ¬å­¦ä¹ èŒƒå¼ï¼Œå…¶å†…éƒ¨æœºåˆ¶å°šæœªè¢«æ¢ç´¢ã€‚è™½ç„¶å·²æœ‰ä½œå“æè¿°ICLçš„å†…éƒ¨å¤„ç†è¿‡ç¨‹ï¼Œä½†å®ƒä»¬å¾ˆéš¾æ•æ‰å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„æ‰€æœ‰æ¨ç†ç°è±¡ã€‚å› æ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªå…¨é¢çš„ç”µè·¯æ¨¡å‹ï¼Œä»¥æ¨¡æ‹Ÿæ¨ç†åŠ¨æ€ï¼Œå¹¶è¯•å›¾è§£é‡Šè§‚å¯Ÿåˆ°çš„ICLç°è±¡ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å°†ICLæ¨ç†è¿‡ç¨‹åˆ†ä¸ºä¸‰å¤§æ“ä½œï¼š</p>
</blockquote>
<p>ï¼ˆ1ï¼‰è¾“å…¥æ–‡æœ¬ç¼–ç ï¼šè¯­è¨€æ¨¡å‹å°†æ¯ä¸ªè¾“å…¥æ–‡æœ¬ï¼ˆæ¼”ç¤ºå’ŒæŸ¥è¯¢ï¼‰ç¼–ç ä¸ºéšè—çŠ¶æ€çš„çº¿æ€§è¡¨ç¤ºï¼Œå…¶ä¸­åŒ…å«è¶³å¤Ÿçš„ä¿¡æ¯æ¥è§£å†³ICLä»»åŠ¡ã€‚</p>
<p>ï¼ˆ2ï¼‰è¯­ä¹‰åˆå¹¶ï¼šè¯­è¨€æ¨¡å‹å°†æ¼”ç¤ºçš„ç¼–ç è¡¨ç¤ºä¸å…¶ç›¸åº”çš„æ ‡ç­¾æ ‡è®°åˆå¹¶ï¼Œä»¥äº§ç”Ÿæ ‡ç­¾å’Œæ¼”ç¤ºçš„è”åˆè¡¨ç¤ºã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.04468v3">PDF</a> 37 pages, 41 figures, 8 tables. ICLR 2025 Accepted</p>
<p><strong>Summary</strong></p>
<p>ICLï¼ˆIn-context Learningï¼‰æ˜¯ä¸€ç§æ–°å…´çš„è¯­è¨€æ¨¡å‹ï¼ˆLMsï¼‰å°‘æ ·æœ¬å­¦ä¹ èŒƒå¼ï¼Œå…¶å†…åœ¨æœºåˆ¶å°šæœªè¢«å®Œå…¨æ¢ç´¢ã€‚ç°æœ‰ç ”ç©¶è™½è¯•å›¾æ•æ‰å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„æ¨ç†ç°è±¡ï¼Œä½†ä»å­˜åœ¨ä¸è¶³ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªå…¨é¢çš„æ¨ç†ç”µè·¯æ¥æ¨¡æ‹ŸICLçš„æ¨ç†è¿‡ç¨‹ï¼Œå°†ICLæ¨ç†åˆ†ä¸ºä¸‰å¤§æ“ä½œï¼šè¾“å…¥æ–‡æœ¬ç¼–ç ã€è¯­ä¹‰åˆå¹¶ã€ç‰¹å¾æ£€ç´¢ä¸å¤åˆ¶ã€‚æ­¤ç”µè·¯æˆåŠŸæ•æ‰äº†ICLè¿‡ç¨‹ä¸­çš„è®¸å¤šç°è±¡ï¼Œä¸ºICLçš„æ¨ç†è¿‡ç¨‹æä¾›äº†å…¨é¢è€Œå®ç”¨çš„è§£é‡Šã€‚å»é™¤è¯¥ç”µè·¯æ­¥éª¤ä¼šå¯¼è‡´ICLæ€§èƒ½ä¸¥é‡ä¸‹é™ï¼ŒéªŒè¯äº†å…¶ä¸»å¯¼æœºåˆ¶çš„åœ°ä½ã€‚åŒæ—¶ï¼Œæœ¬æ–‡è¿˜ç¡®è®¤å¹¶åˆ—å‡ºäº†ä¸€äº›ä¸æè®®ç”µè·¯å¹¶è¡Œè§£å†³ICLä»»åŠ¡çš„æ—è·¯æœºåˆ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ICLæ˜¯ä¸€ç§æ–°å…´çš„è¯­è¨€æ¨¡å‹å°‘æ ·æœ¬å­¦ä¹ èŒƒå¼ï¼Œå…¶å†…åœ¨æœºåˆ¶å°šæœªè¢«å®Œå…¨æ¢ç´¢ã€‚</li>
<li>ç°æœ‰ç ”ç©¶åœ¨æ•æ‰å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†ç°è±¡æ—¶å­˜åœ¨å›°éš¾ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªå…¨é¢çš„æ¨ç†ç”µè·¯ï¼Œå°†ICLæ¨ç†åˆ†ä¸ºè¾“å…¥æ–‡æœ¬ç¼–ç ã€è¯­ä¹‰åˆå¹¶ã€ç‰¹å¾æ£€ç´¢ä¸å¤åˆ¶ä¸‰å¤§æ“ä½œã€‚</li>
<li>è¯¥ç”µè·¯æˆåŠŸæ•æ‰äº†ICLè¿‡ç¨‹ä¸­çš„è®¸å¤šç°è±¡ï¼Œä¸ºICLçš„æ¨ç†è¿‡ç¨‹æä¾›äº†å…¨é¢è€Œå®ç”¨çš„è§£é‡Šã€‚</li>
<li>å»é™¤æ¨ç†ç”µè·¯æ­¥éª¤ä¼šå¯¼è‡´ICLæ€§èƒ½ä¸¥é‡ä¸‹é™ï¼ŒéªŒè¯äº†è¯¥ç”µè·¯çš„ä¸»å¯¼æœºåˆ¶åœ°ä½ã€‚</li>
<li>é™¤äº†ä¸»å¯¼æœºåˆ¶å¤–ï¼Œè¿˜å­˜åœ¨ä¸€äº›æ—è·¯æœºåˆ¶å¹¶è¡Œè§£å†³ICLä»»åŠ¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.04468">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-eeff5798c423994e72daf5ad7e40e66f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-976fc8eebd3658c714e67cba9ecfe1d2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b9f2bc2ba4df900d15bff2a53d7fa5a4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2c759ab722c13c3d7958c15492bca666.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-eb37d0583df6eb9969dd11c2134c78a7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-26a6f136289c08ca5dea931ed1806646.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Unleashing-the-Potential-of-Large-Language-Models-for-Predictive-Tabular-Tasks-in-Data-Science"><a href="#Unleashing-the-Potential-of-Large-Language-Models-for-Predictive-Tabular-Tasks-in-Data-Science" class="headerlink" title="Unleashing the Potential of Large Language Models for Predictive Tabular   Tasks in Data Science"></a>Unleashing the Potential of Large Language Models for Predictive Tabular   Tasks in Data Science</h2><p><strong>Authors:Yazheng Yang, Yuqi Wang, Yaxuan Li, Sankalok Sen, Lei Li, Qi Liu</strong></p>
<p>In the domain of data science, the predictive tasks of classification, regression, and imputation of missing values are commonly encountered challenges associated with tabular data. This research endeavors to apply Large Language Models (LLMs) towards addressing these predictive tasks. Despite their proficiency in comprehending natural language, LLMs fall short in dealing with structured tabular data. This limitation stems from their lacking exposure to the intricacies of tabular data during their foundational training. Our research aims to mitigate this gap by compiling a comprehensive corpus of tables annotated with instructions and executing large-scale training of Llama-2 on this enriched dataset. Furthermore, we investigate the practical application of applying the trained model to zero-shot prediction, few-shot prediction, and in-context learning scenarios. Through extensive experiments, our methodology has shown significant improvements over existing benchmarks. These advancements highlight the efficacy of tailoring LLM training to solve table-related problems in data science, thereby establishing a new benchmark in the utilization of LLMs for enhancing tabular intelligence. </p>
<blockquote>
<p>åœ¨æ•°æ®ç§‘å­¦é¢†åŸŸï¼Œåˆ†ç±»ã€å›å½’å’Œç¼ºå¤±å€¼å¡«è¡¥çš„é¢„æµ‹ä»»åŠ¡æ˜¯å¤„ç†è¡¨æ ¼æ•°æ®æ—¶å¸¸è§æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶æ—¨åœ¨å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åº”ç”¨äºè§£å†³è¿™äº›é¢„æµ‹ä»»åŠ¡ã€‚å°½ç®¡LLMåœ¨è‡ªç„¶è¯­è¨€ç†è§£æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¤„ç†ç»“æ„åŒ–è¡¨æ ¼æ•°æ®æ—¶å´æ˜¾å¾—ä¸è¶³ã€‚è¿™ç§å±€é™æ€§æºäºå…¶åœ¨åŸºç¡€è®­ç»ƒè¿‡ç¨‹ä¸­å¯¹è¡¨æ ¼æ•°æ®å¤æ‚æ€§ç¼ºä¹æ¥è§¦ã€‚æˆ‘ä»¬çš„ç ”ç©¶æ—¨åœ¨é€šè¿‡ç¼–è¯‘å¸¦æœ‰æŒ‡ä»¤æ³¨é‡Šçš„è¡¨æ ¼ç»¼åˆè¯­æ–™åº“ï¼Œå¹¶åœ¨ä¸°å¯Œçš„æ•°æ®é›†ä¸Šæ‰§è¡Œå¤§è§„æ¨¡è®­ç»ƒæ¥ç¼©å°è¿™ä¸€å·®è·ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å°†ç ”ç©¶å°†è®­ç»ƒæœ‰ç´ çš„æ¨¡å‹åº”ç”¨äºé›¶å°„å‡»é¢„æµ‹ã€å°‘å°„å‡»é¢„æµ‹å’Œä¸Šä¸‹æ–‡å­¦ä¹ åœºæ™¯çš„å®é™…åº”ç”¨ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç°æœ‰åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚è¿™äº›è¿›æ­¥çªæ˜¾äº†é’ˆå¯¹æ•°æ®ç§‘å­¦ä¸­è¡¨æ ¼ç›¸å…³é—®é¢˜çš„LLMè®­ç»ƒçš„é‡è¦æ€§ï¼Œä»è€Œä¸ºæé«˜è¡¨æ ¼æ™ºèƒ½åˆ©ç”¨LLMå»ºç«‹äº†æ–°çš„åŸºå‡†æµ‹è¯•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.20208v7">PDF</a> 10 pages</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯¹åˆ†ç±»é¢„æµ‹ä»»åŠ¡ã€å›å½’é¢„æµ‹ä»»åŠ¡å’Œç¼ºå¤±å€¼å¡«è¡¥ä»»åŠ¡çš„ç ”ç©¶è¡¨æ˜ï¼Œå°½ç®¡LLMåœ¨è‡ªç„¶è¯­è¨€ç†è§£æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¤„ç†ç»“æ„åŒ–è¡¨æ ¼æ•°æ®æ—¶å­˜åœ¨çŸ­æ¿ã€‚æœ¬ç ”ç©¶æ—¨åœ¨é€šè¿‡ç¼–è¯‘å¸¦æœ‰æŒ‡ä»¤æ³¨è§£çš„è¡¨æ ¼ç»¼åˆè¯­æ–™åº“ï¼Œå¹¶å¯¹Llama-2æ¨¡å‹è¿›è¡Œå¤§è§„æ¨¡è®­ç»ƒæ¥å¼¥è¡¥è¿™ä¸€å·®è·ã€‚åŒæ—¶ï¼Œè¯¥ç ”ç©¶è¿˜æ¢è®¨äº†è®­ç»ƒæ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„é›¶æ ·æœ¬é¢„æµ‹ã€å°æ ·æœ¬é¢„æµ‹å’Œä¸Šä¸‹æ–‡å­¦ä¹ åœºæ™¯çš„åº”ç”¨æ•ˆæœã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨ç°æœ‰åŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼Œè¯æ˜äº†é’ˆå¯¹è¡¨æ ¼ç›¸å…³é—®é¢˜çš„æ•°æ®ç§‘å­¦è®­ç»ƒå¯¹æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ•ˆèƒ½å…·æœ‰å…³é”®ä½œç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤„ç†ç»“æ„åŒ–è¡¨æ ¼æ•°æ®æ—¶å­˜åœ¨çŸ­æ¿ã€‚</li>
<li>æœ¬ç ”ç©¶æ—¨åœ¨é€šè¿‡ç¼–è¯‘å¸¦æœ‰æŒ‡ä»¤æ³¨è§£çš„è¡¨æ ¼ç»¼åˆè¯­æ–™åº“æ¥å¼¥è¡¥å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†è¡¨æ ¼æ•°æ®æ—¶çš„ä¸è¶³ã€‚</li>
<li>å¯¹Llama-2æ¨¡å‹è¿›è¡Œå¤§è§„æ¨¡è®­ç»ƒï¼Œæå‡å…¶å¤„ç†è¡¨æ ¼æ•°æ®çš„èƒ½åŠ›ã€‚</li>
<li>ç ”ç©¶æ¢è®¨äº†è®­ç»ƒæ¨¡å‹åœ¨é›¶æ ·æœ¬é¢„æµ‹ã€å°æ ·æœ¬é¢„æµ‹å’Œä¸Šä¸‹æ–‡å­¦ä¹ åœºæ™¯çš„åº”ç”¨æ•ˆæœã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨ç°æœ‰åŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚</li>
<li>é’ˆå¯¹è¡¨æ ¼ç›¸å…³é—®é¢˜çš„æ•°æ®ç§‘å­¦è®­ç»ƒå¯¹æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ•ˆèƒ½å…·æœ‰å…³é”®ä½œç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.20208">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-a3c2545b4a4fcf4ca7e72e1890c841ae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7d04169eb1ebd5e728f5d3d8d13e98b4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bffdae60187c904c52b7cc71c90d7640.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e79f2e5f8a0500b85ede524159eeb5b5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-67876ab0b9b5c1053cb475e462925712.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="MedPromptX-Grounded-Multimodal-Prompting-for-Chest-X-ray-Diagnosis"><a href="#MedPromptX-Grounded-Multimodal-Prompting-for-Chest-X-ray-Diagnosis" class="headerlink" title="MedPromptX: Grounded Multimodal Prompting for Chest X-ray Diagnosis"></a>MedPromptX: Grounded Multimodal Prompting for Chest X-ray Diagnosis</h2><p><strong>Authors:Mai A. Shaaban, Adnan Khan, Mohammad Yaqub</strong></p>
<p>Chest X-ray images are commonly used for predicting acute and chronic cardiopulmonary conditions, but efforts to integrate them with structured clinical data face challenges due to incomplete electronic health records (EHR). This paper introduces MedPromptX, the first clinical decision support system that integrates multimodal large language models (MLLMs), few-shot prompting (FP) and visual grounding (VG) to combine imagery with EHR data for chest X-ray diagnosis. A pre-trained MLLM is utilized to complement the missing EHR information, providing a comprehensive understanding of patientsâ€™ medical history. Additionally, FP reduces the necessity for extensive training of MLLMs while effectively tackling the issue of hallucination. Nevertheless, the process of determining the optimal number of few-shot examples and selecting high-quality candidates can be burdensome, yet it profoundly influences model performance. Hence, we propose a new technique that dynamically refines few-shot data for real-time adjustment to new patient scenarios. Moreover, VG narrows the search area in X-ray images, thereby enhancing the identification of abnormalities. We also release MedPromptX-VQA, a new in-context visual question answering dataset encompassing interleaved images and EHR data derived from MIMIC-IV and MIMIC-CXR-JPG databases. Results demonstrate the SOTA performance of MedPromptX, achieving an 11% improvement in F1-score compared to the baselines. Code and data are publicly available on <a target="_blank" rel="noopener" href="https://github.com/BioMedIA-MBZUAI/MedPromptX">https://github.com/BioMedIA-MBZUAI/MedPromptX</a>. </p>
<blockquote>
<p>èƒ¸éƒ¨Xå…‰ç‰‡å›¾åƒé€šå¸¸ç”¨äºé¢„æµ‹æ€¥æ€§å’Œæ…¢æ€§å¿ƒè‚ºçŠ¶å†µï¼Œä½†å°†å…¶ä¸ç»“æ„åŒ–ä¸´åºŠæ•°æ®ç»“åˆçš„åŠªåŠ›é¢ä¸´ç€ç”±äºç”µå­å¥åº·è®°å½•ï¼ˆEHRï¼‰ä¸å®Œæ•´è€Œå¸¦æ¥çš„æŒ‘æˆ˜ã€‚æœ¬æ–‡ä»‹ç»äº†MedPromptXï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç»“åˆå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ã€å°æ ·æœ¬æç¤ºï¼ˆFPï¼‰å’Œè§†è§‰å®šä½ï¼ˆVGï¼‰çš„ä¸´åºŠå†³ç­–æ”¯æŒç³»ç»Ÿï¼Œç”¨äºå°†å›¾åƒä¸EHRæ•°æ®ç›¸ç»“åˆè¿›è¡Œèƒ¸éƒ¨Xå…‰è¯Šæ–­ã€‚é¢„è®­ç»ƒçš„MLLMè¢«ç”¨æ¥è¡¥å……ç¼ºå¤±çš„EHRä¿¡æ¯ï¼Œä¸ºæ‚£è€…ç—…å²æä¾›å…¨é¢çš„ç†è§£ã€‚æ­¤å¤–ï¼ŒFPé™ä½äº†å¯¹MLLMsè¿›è¡Œå¤§é‡åŸ¹è®­çš„å¿…è¦ï¼ŒåŒæ—¶æœ‰æ•ˆåœ°è§£å†³äº†å¹»è§‰é—®é¢˜ã€‚ç„¶è€Œï¼Œç¡®å®šæœ€ä½³çš„å°æ ·æœ¬æ•°é‡å¹¶é€‰æ‹©é«˜è´¨é‡å€™é€‰å¯¹è±¡çš„è¿‡ç¨‹å¯èƒ½æ˜¯ä¸€ä¸ªç¹é‡çš„ä»»åŠ¡ï¼Œä½†å®ƒå¯¹æ¨¡å‹æ€§èƒ½äº§ç”Ÿæ·±è¿œå½±å“ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æŠ€æœ¯ï¼Œå¯ä»¥åŠ¨æ€ç»†åŒ–å°æ ·æœ¬æ•°æ®ï¼Œä»¥é€‚åº”ç”¨äºæ–°æ‚£è€…æƒ…å¢ƒçš„å®æ—¶è°ƒæ•´ã€‚æ­¤å¤–ï¼ŒVGç¼©å°äº†Xå…‰ç‰‡å›¾åƒä¸­çš„æœç´¢åŒºåŸŸï¼Œä»è€Œæé«˜äº†å¼‚å¸¸è¯†åˆ«çš„å‡†ç¡®æ€§ã€‚æˆ‘ä»¬è¿˜å‘å¸ƒäº†MedPromptX-VQAï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„ä¸Šä¸‹æ–‡è§†è§‰é—®ç­”æ•°æ®é›†ï¼Œæ¶µç›–äº†ä»MIMIC-IVå’ŒMIMIC-CXR-JPGæ•°æ®åº“æ´¾ç”Ÿçš„äº¤é”™å›¾åƒå’ŒEHRæ•°æ®ã€‚ç»“æœè¡¨æ˜ï¼ŒMedPromptXçš„æ€§èƒ½å¤„äºé¢†å…ˆæ°´å¹³ï¼Œä¸åŸºçº¿ç›¸æ¯”ï¼ŒF1åˆ†æ•°æé«˜äº†11%ã€‚ä»£ç å’Œæ•°æ®å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/BioMedIA-MBZUAI/MedPromptX%E4%B8%8A%E5%85%AC%E5%BC%80%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/BioMedIA-MBZUAI/MedPromptXä¸Šå…¬å¼€è·å¾—ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.15585v4">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªåä¸ºMedPromptXçš„ä¸´åºŠå†³ç­–æ”¯æŒç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿç»“åˆäº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ã€å°‘æ ·æœ¬æç¤ºï¼ˆFPï¼‰å’Œè§†è§‰å®šä½ï¼ˆVGï¼‰æŠ€æœ¯ï¼Œå°†èƒ¸éƒ¨Xå…‰å›¾åƒä¸ç”µå­å¥åº·è®°å½•ï¼ˆEHRï¼‰æ•°æ®ç›¸ç»“åˆï¼Œç”¨äºèƒ¸éƒ¨Xå…‰è¯Šæ–­ã€‚é€šè¿‡ä½¿ç”¨é¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹æ¥è¡¥å……ç¼ºå¤±çš„EHRä¿¡æ¯ï¼Œæä¾›å…¨é¢çš„æ‚£è€…ç—…å²ç†è§£ã€‚æ­¤å¤–ï¼Œè¯¥ç³»ç»Ÿé‡‡ç”¨å°‘æ ·æœ¬æç¤ºæŠ€æœ¯ï¼Œå‡å°‘äº†å¯¹MLLMsçš„å¹¿æ³›è®­ç»ƒéœ€æ±‚ï¼Œå¹¶è§£å†³äº†è™šæ„é—®é¢˜ã€‚æ–‡ç« è¿˜æå‡ºäº†ä¸€ç§æ–°æŠ€æœ¯ï¼Œå¯åŠ¨æ€å®Œå–„å°‘æ ·æœ¬æ•°æ®ï¼Œä»¥é€‚åº”æ–°æ‚£è€…æƒ…æ™¯ã€‚è§†è§‰å®šä½æŠ€æœ¯åˆ™ç¼©å°äº†Xå…‰å›¾åƒçš„æœç´¢èŒƒå›´ï¼Œæé«˜äº†å¼‚å¸¸è¯†åˆ«çš„å‡†ç¡®æ€§ã€‚æœ€åï¼Œæ–‡ç« å‘å¸ƒäº†ä¸€ä¸ªåä¸ºMedPromptX-VQAçš„æ–°æ•°æ®é›†ï¼ŒåŒ…å«æ¥è‡ªMIMIC-IVå’ŒMIMIC-CXR-JPGæ•°æ®åº“çš„å›¾ç‰‡å’ŒEHRæ•°æ®ã€‚MedPromptXç³»ç»Ÿçš„æ€§èƒ½ä¼˜äºåŸºçº¿ï¼ŒF1åˆ†æ•°æé«˜äº†11%ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>MedPromptXæ˜¯é¦–ä¸ªç»“åˆå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ã€å°‘æ ·æœ¬æç¤ºå’Œè§†è§‰å®šä½æŠ€æœ¯çš„ä¸´åºŠå†³ç­–æ”¯æŒç³»ç»Ÿã€‚</li>
<li>ç³»ç»Ÿé€šè¿‡é¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹è¡¥å……ç¼ºå¤±çš„EHRä¿¡æ¯ï¼Œæä¾›å…¨é¢çš„æ‚£è€…ç—…å²ç†è§£ã€‚</li>
<li>å°‘æ ·æœ¬æç¤ºæŠ€æœ¯å‡å°‘äº†MLLMsçš„è®­ç»ƒéœ€æ±‚ï¼Œå¹¶è§£å†³äº†è™šæ„é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŠ¨æ€å®Œå–„å°‘æ ·æœ¬æ•°æ®çš„æ–°æŠ€æœ¯ï¼Œä»¥é€‚åº”æ–°æ‚£è€…æƒ…æ™¯ã€‚</li>
<li>è§†è§‰å®šä½æŠ€æœ¯æé«˜äº†Xå…‰å›¾åƒä¸­å¼‚å¸¸è¯†åˆ«çš„å‡†ç¡®æ€§ã€‚</li>
<li>å‘å¸ƒäº†MedPromptX-VQAæ•°æ®é›†ï¼ŒåŒ…å«å›¾ç‰‡å’ŒEHRæ•°æ®ï¼Œç”¨äºè§†è§‰é—®ç­”ä»»åŠ¡ã€‚</li>
<li>MedPromptXç³»ç»Ÿæ€§èƒ½ä¼˜è¶Šï¼Œä¸åŸºçº¿ç›¸æ¯”F1åˆ†æ•°æé«˜äº†11%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.15585">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b0705f22c5f7b3c8d46ffb2ed7cfa312.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e0abba40901827e899421518cd239548.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-94d60e7a0dd4b3433a37eaadfcd8a78e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-80d6732030403ab97eb7d9c4ef77cdea.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Agent-OM-Leveraging-LLM-Agents-for-Ontology-Matching"><a href="#Agent-OM-Leveraging-LLM-Agents-for-Ontology-Matching" class="headerlink" title="Agent-OM: Leveraging LLM Agents for Ontology Matching"></a>Agent-OM: Leveraging LLM Agents for Ontology Matching</h2><p><strong>Authors:Zhangcheng Qiang, Weiqing Wang, Kerry Taylor</strong></p>
<p>Ontology matching (OM) enables semantic interoperability between different ontologies and resolves their conceptual heterogeneity by aligning related entities. OM systems currently have two prevailing design paradigms: conventional knowledge-based expert systems and newer machine learning-based predictive systems. While large language models (LLMs) and LLM agents have revolutionised data engineering and have been applied creatively in many domains, their potential for OM remains underexplored. This study introduces a novel agent-powered LLM-based design paradigm for OM systems. With consideration of several specific challenges in leveraging LLM agents for OM, we propose a generic framework, namely Agent-OM (Agent for Ontology Matching), consisting of two Siamese agents for retrieval and matching, with a set of OM tools. Our framework is implemented in a proof-of-concept system. Evaluations of three Ontology Alignment Evaluation Initiative (OAEI) tracks over state-of-the-art OM systems show that our system can achieve results very close to the long-standing best performance on simple OM tasks and can significantly improve the performance on complex and few-shot OM tasks. </p>
<blockquote>
<p>æœ¬ä½“åŒ¹é…ï¼ˆOMï¼‰é€šè¿‡å¯¹é½ç›¸å…³å®ä½“ï¼Œå®ç°äº†ä¸åŒæœ¬ä½“ä¹‹é—´çš„è¯­ä¹‰äº’æ“ä½œï¼Œå¹¶è§£å†³äº†å…¶æ¦‚å¿µä¸Šçš„å¼‚è´¨æ€§ã€‚ç›®å‰ï¼ŒOMç³»ç»Ÿä¸»è¦æœ‰ä¸¤ç§æµè¡Œçš„è®¾è®¡èŒƒå¼ï¼šä¼ ç»Ÿçš„åŸºäºçŸ¥è¯†çš„ä¸“å®¶ç³»ç»Ÿå’Œè¾ƒæ–°çš„åŸºäºæœºå™¨å­¦ä¹ çš„é¢„æµ‹ç³»ç»Ÿã€‚è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’ŒLLMä»£ç†å·²ç»å½»åº•æ”¹å˜äº†æ•°æ®å·¥ç¨‹ï¼Œå¹¶å·²åœ¨è®¸å¤šé¢†åŸŸå¾—åˆ°åˆ›é€ æ€§åº”ç”¨ï¼Œä½†å®ƒä»¬åœ¨OMä¸­çš„æ½œåŠ›ä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æœ¬ç ”ç©¶å¼•å…¥äº†ä¸€ç§åŸºäºä»£ç†çš„LLMè®¾è®¡èŒƒå¼çš„æ–°å‹OMç³»ç»Ÿã€‚è€ƒè™‘åˆ°åˆ©ç”¨LLMä»£ç†è¿›è¡ŒOMé¢ä¸´çš„è‹¥å¹²ç‰¹å®šæŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªé€šç”¨æ¡†æ¶ï¼Œå³Agent-OMï¼ˆç”¨äºæœ¬ä½“åŒ¹é…çš„ä»£ç†ï¼‰ï¼Œè¯¥æ¡†æ¶åŒ…å«ä¸¤ä¸ªç”¨äºæ£€ç´¢å’ŒåŒ¹é…çš„Siameseä»£ç†ä»¥åŠä¸€ç»„OMå·¥å…·ã€‚æˆ‘ä»¬çš„æ¡†æ¶åœ¨ä¸€ä¸ªæ¦‚å¿µéªŒè¯ç³»ç»Ÿä¸­å®ç°ã€‚å¯¹ä¸‰ä¸ªæœ¬ä½“å¯¹é½è¯„ä¼°å€¡è®®ï¼ˆOAEIï¼‰èµ›é“ä¸Šçš„æœ€æ–°OMç³»ç»Ÿçš„è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„ç³»ç»Ÿåœ¨ç®€å•OMä»»åŠ¡ä¸Šçš„ç»“æœéå¸¸æ¥è¿‘é•¿æœŸä»¥æ¥çš„æœ€ä½³æ€§èƒ½ï¼Œå¹¶ä¸”åœ¨å¤æ‚å’Œå°‘é•œå¤´OMä»»åŠ¡ä¸Šå¯ä»¥æ˜¾è‘—æé«˜æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.00326v7">PDF</a> 19 pages, 12 figures, 3 tables</p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶å¼•å…¥äº†ä¸€ç§æ–°å‹åŸºäºLLMä»£ç†çš„OMç³»ç»Ÿè®¾è®¡èŒƒå¼ï¼Œé€šè¿‡ä¸¤ä¸ªSiameseä»£ç†è¿›è¡Œæ£€ç´¢å’ŒåŒ¹é…ï¼Œå¹¶ä½¿ç”¨ä¸€å¥—OMå·¥å…·æ„å»ºäº†ä¸€ä¸ªé€šç”¨çš„Agent-OMæ¡†æ¶ã€‚è¯¥æ¡†æ¶åœ¨è¯æ˜æ¦‚å¿µç³»ç»Ÿä¸­å®ç°ï¼Œå¹¶åœ¨ä¸‰ä¸ªOAEIè½¨é“ä¸Šå¯¹æœ€æ–°OMç³»ç»Ÿè¿›è¡Œçš„è¯„ä¼°è¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿåœ¨ç®€å•OMä»»åŠ¡ä¸Šè¡¨ç°æ¥è¿‘æœ€ä½³æ°´å¹³ï¼Œå¹¶åœ¨å¤æ‚å’Œå°‘æ ·æœ¬OMä»»åŠ¡ä¸Šæ˜¾è‘—æé«˜æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>OMï¼ˆæœ¬ä½“åŒ¹é…ï¼‰æœ‰åŠ©äºä¸åŒæœ¬ä½“ä¹‹é—´çš„è¯­ä¹‰äº’æ“ä½œæ€§ï¼Œå¹¶é€šè¿‡å¯¹é½ç›¸å…³å®ä½“è§£å†³å…¶æ¦‚å¿µä¸Šçš„å¼‚è´¨æ€§ã€‚</li>
<li>å½“å‰OMç³»ç»Ÿä¸»è¦æœ‰ä¸¤ç§è®¾è®¡èŒƒå¼ï¼šä¼ ç»Ÿçš„çŸ¥è¯†å‹ä¸“å®¶ç³»ç»Ÿå’Œæ–°çš„åŸºäºæœºå™¨å­¦ä¹ çš„é¢„æµ‹ç³»ç»Ÿã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’ŒLLMä»£ç†åœ¨æ•°æ®å·¥ç¨‹ä¸­å…·æœ‰é©å‘½æ€§ä½œç”¨ï¼Œä½†åœ¨OMæ–¹é¢çš„æ½œåŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚</li>
<li>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹çš„åŸºäºLLMä»£ç†çš„OMç³»ç»Ÿè®¾è®¡èŒƒå¼ï¼Œè§£å†³äº†åˆ©ç”¨LLMä»£ç†è¿›è¡ŒOMæ—¶é¢ä¸´çš„ä¸€äº›ç‰¹å®šæŒ‘æˆ˜ã€‚</li>
<li>æ„å»ºäº†ä¸€ä¸ªé€šç”¨çš„Agent-OMæ¡†æ¶ï¼ŒåŒ…æ‹¬ç”¨äºæ£€ç´¢å’ŒåŒ¹é…çš„Siameseä»£ç†ã€‚</li>
<li>è¯¥æ¡†æ¶åœ¨è¯æ˜æ¦‚å¿µç³»ç»Ÿä¸­å®ç°ï¼Œå¹¶åœ¨å¤šä¸ªè¯„ä¼°ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚å’Œå°‘æ ·æœ¬OMä»»åŠ¡ä¸Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2312.00326">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-52161e898ad82f80ae924e0739023488.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3be1f670b4b3e5e54e8dc998918fda9d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1cde61049b12372f9a36b0b53054ed8c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1489e8850d1b5cc66334d1a52d1e015b.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-29/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-29/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-29/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-d1fbeadb3594628e47d8f6e980b126d4.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-29  Compensator-based small animal IMRT enables conformal preclinical dose   painting application to tumor hypoxia
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-29
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-29/MMT/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-5b387b561138112390a193753a2d886d.jpg" class="responsive-img" alt="MMT">
                        
                        <span class="card-title">MMT</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            MMT æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-29  Pangea A Fully Open Multilingual Multimodal LLM for 39 Languages
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-29
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/MMT/" class="post-category">
                                    MMT
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/MMT/">
                        <span class="chip bg-color">MMT</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">14773.6k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
