<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Vision Transformer">
    <meta name="description" content="Vision Transformer æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-29  CLISC Bridging clip and sam by enhanced cam for unsupervised brain   tumor segmentation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Vision Transformer | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-5ab129f2734ce488357d5baee79c5c4f.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Vision Transformer</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Vision-Transformer/">
                                <span class="chip bg-color">Vision Transformer</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                Vision Transformer
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-29
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-02-12
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    6.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    26 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-01-29-æ›´æ–°"><a href="#2025-01-29-æ›´æ–°" class="headerlink" title="2025-01-29 æ›´æ–°"></a>2025-01-29 æ›´æ–°</h1><h2 id="CLISC-Bridging-clip-and-sam-by-enhanced-cam-for-unsupervised-brain-tumor-segmentation"><a href="#CLISC-Bridging-clip-and-sam-by-enhanced-cam-for-unsupervised-brain-tumor-segmentation" class="headerlink" title="CLISC: Bridging clip and sam by enhanced cam for unsupervised brain   tumor segmentation"></a>CLISC: Bridging clip and sam by enhanced cam for unsupervised brain   tumor segmentation</h2><p><strong>Authors:Xiaochuan Ma, Jia Fu, Wenjun Liao, Shichuan Zhang, Guotai Wang</strong></p>
<p>Brain tumor segmentation is important for diagnosis of the tumor, and current deep-learning methods rely on a large set of annotated images for training, with high annotation costs. Unsupervised segmentation is promising to avoid human annotations while the performance is often limited. In this study, we present a novel unsupervised segmentation approach that leverages the capabilities of foundation models, and it consists of three main steps: (1) A vision-language model (i.e., CLIP) is employed to obtain image-level pseudo-labels for training a classification network. Class Activation Mapping (CAM) is then employed to extract Regions of Interest (ROIs), where an adaptive masking-based data augmentation is used to enhance ROI identification.(2) The ROIs are used to generate bounding box and point prompts for the Segment Anything Model (SAM) to obtain segmentation pseudo-labels. (3) A 3D segmentation network is trained with the SAM-derived pseudo-labels, where low-quality pseudo-labels are filtered out in a self-learning process based on the similarity between the SAMâ€™s output and the networkâ€™s prediction. Evaluation on the BraTS2020 dataset demonstrates that our approach obtained an average Dice Similarity Score (DSC) of 85.60%, outperforming five state-of-the-art unsupervised segmentation methods by more than 10 percentage points. Besides, our approach outperforms directly using SAM for zero-shot inference, and its performance is close to fully supervised learning. </p>
<blockquote>
<p>è„‘è‚¿ç˜¤åˆ†å‰²å¯¹äºè‚¿ç˜¤è¯Šæ–­å…·æœ‰é‡è¦æ„ä¹‰ï¼Œå½“å‰æ·±åº¦å­¦ä¹ æ–¹æ³•ä¾èµ–äºå¤§é‡æ ‡æ³¨å›¾åƒè¿›è¡Œè®­ç»ƒï¼Œæ ‡æ³¨æˆæœ¬é«˜æ˜‚ã€‚æ— ç›‘ç£åˆ†å‰²æ–¹æ³•å…·æœ‰é¿å…äººå·¥æ ‡æ³¨çš„æ½œåŠ›ï¼Œä½†æ€§èƒ½å¾€å¾€å—åˆ°é™åˆ¶ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹æ— ç›‘ç£åˆ†å‰²æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨åŸºç¡€æ¨¡å‹çš„èƒ½åŠ›ï¼Œä¸»è¦åŒ…å«ä¸‰ä¸ªæ­¥éª¤ï¼šï¼ˆ1ï¼‰é‡‡ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆä¾‹å¦‚CLIPï¼‰è·å–ç”¨äºè®­ç»ƒåˆ†ç±»ç½‘ç»œçš„å›¾åƒçº§ä¼ªæ ‡ç­¾ã€‚ç„¶åé‡‡ç”¨ç±»æ¿€æ´»æ˜ å°„ï¼ˆCAMï¼‰æå–æ„Ÿå…´è¶£åŒºåŸŸï¼ˆROIï¼‰ï¼Œå¹¶ä½¿ç”¨è‡ªé€‚åº”æ©è†œå¼æ•°æ®å¢å¼ºæ¥æé«˜ROIè¯†åˆ«ã€‚ï¼ˆ2ï¼‰ä½¿ç”¨ROIç”Ÿæˆè¾¹ç•Œæ¡†å’Œç‚¹æç¤ºï¼Œä»¥ä¾›åˆ†æ®µä»»ä½•æ¨¡å‹ï¼ˆSAMï¼‰è·å¾—åˆ†å‰²ä¼ªæ ‡ç­¾ã€‚ï¼ˆ3ï¼‰ä½¿ç”¨SAMè¡ç”Ÿçš„ä¼ªæ ‡ç­¾è®­ç»ƒ3Dåˆ†å‰²ç½‘ç»œï¼Œåœ¨è‡ªæˆ‘å­¦ä¹ è¿‡ç¨‹ä¸­è¿‡æ»¤æ‰ä½è´¨é‡çš„ä¼ªæ ‡ç­¾ï¼Œè¯¥è¿‡ç¨‹åŸºäºSAMè¾“å‡ºä¸ç½‘ç»œé¢„æµ‹ä¹‹é—´çš„ç›¸ä¼¼æ€§ã€‚åœ¨BraTS2020æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è·å¾—äº†å¹³å‡Diceç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆDSCï¼‰ä¸º85.60%ï¼Œæ¯”äº”ç§æœ€å…ˆè¿›çš„æ— ç›‘ç£åˆ†å‰²æ–¹æ³•çš„æ€§èƒ½é«˜å‡ºè¶…è¿‡10ä¸ªç™¾åˆ†ç‚¹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºç›´æ¥ä½¿ç”¨SAMè¿›è¡Œé›¶æ ·æœ¬æ¨ç†ï¼Œå…¶æ€§èƒ½æ¥è¿‘å®Œå…¨ç›‘ç£å­¦ä¹ ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.16246v1">PDF</a> 22st IEEE International Symposium on Biomedical Imaging (ISBI 2025)</p>
<p><strong>Summary</strong><br>åŸºäºè®¡ç®—æœºè§†è§‰æ¨¡å‹çš„æ— ç›‘ç£è„‘è‚¿ç˜¤åˆ†å‰²æ–¹æ³•å–å¾—äº†çªç ´æ€§è¿›å±•ã€‚é€šè¿‡CLIPæ¨¡å‹è·å–å›¾åƒçº§åˆ«çš„ä¼ªæ ‡ç­¾è®­ç»ƒåˆ†ç±»ç½‘ç»œï¼Œåˆ©ç”¨CAMæå–æ„Ÿå…´è¶£åŒºåŸŸå¹¶è¿›è¡Œè‡ªé€‚åº”æ©è†œå¢å¼ºï¼›å†ç»“åˆSAMæ¨¡å‹ç”Ÿæˆè¾¹ç•Œæ¡†å’Œç‚¹æç¤ºè·å¾—åˆ†å‰²ä¼ªæ ‡ç­¾ï¼Œæœ€åé€šè¿‡è‡ªå­¦ä¹ è¿‡ç¨‹ç­›é€‰å‡ºé«˜è´¨é‡ä¼ªæ ‡ç­¾ç”¨äºè®­ç»ƒä¸‰ç»´åˆ†å‰²ç½‘ç»œã€‚è¯¥æŠ€æœ¯åœ¨BraTS2020æ•°æ®é›†ä¸Šçš„å¹³å‡Diceç›¸ä¼¼åº¦è¾¾åˆ°85.6%ï¼Œè¶…è¶Šå…¶ä»–äº”ç§å‰æ²¿æ— ç›‘ç£åˆ†å‰²æŠ€æœ¯å¹¶æ¥è¿‘å…¨ç›‘ç£å­¦ä¹ çš„æ€§èƒ½ã€‚ </p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹æ— ç›‘ç£è„‘è‚¿ç˜¤åˆ†å‰²æ–¹æ³•ï¼Œç»“åˆè®¡ç®—æœºè§†è§‰æ¨¡å‹å’Œè‡ªé€‚åº”æ•°æ®å¢å¼ºæŠ€æœ¯ã€‚</li>
<li>åˆ©ç”¨CLIPæ¨¡å‹è·å–å›¾åƒçº§åˆ«çš„ä¼ªæ ‡ç­¾è¿›è¡Œè®­ç»ƒåˆ†ç±»ç½‘ç»œï¼Œå¹¶é€šè¿‡CAMæå–æ„Ÿå…´è¶£åŒºåŸŸã€‚</li>
<li>åˆ©ç”¨SAMæ¨¡å‹ç”Ÿæˆè¾¹ç•Œæ¡†å’Œç‚¹æç¤ºä»¥è·å–åˆ†å‰²ä¼ªæ ‡ç­¾ã€‚</li>
<li>é€šè¿‡è‡ªå­¦ä¹ è¿‡ç¨‹ç­›é€‰å‡ºé«˜è´¨é‡ä¼ªæ ‡ç­¾ç”¨äºè®­ç»ƒä¸‰ç»´åˆ†å‰²ç½‘ç»œã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨BraTS2020æ•°æ®é›†ä¸Šçš„æ€§èƒ½å“è¶Šï¼Œå¹³å‡Diceç›¸ä¼¼åº¦è¾¾åˆ°85.6%ã€‚</li>
<li>æ­¤æ–¹æ³•è¶…è¶Šäº†äº”ç§æœ€æ–°çš„æ— ç›‘ç£åˆ†å‰²æŠ€æœ¯ï¼Œå¹¶æ¥è¿‘å…¨ç›‘ç£å­¦ä¹ çš„æ€§èƒ½è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.16246">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-907df3bb2692b386c97871893275f494.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a3f4b82c817075eaf7166e9e9614ad5c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7460c1714c8dca2e50cf7696ca2c1e2e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3b555dd087a3d3e830c559e175742566.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-38d0bb6f0bb475b33d941753e5a643ff.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="PDC-ViT-Source-Camera-Identification-using-Pixel-Difference-Convolution-and-Vision-Transformer"><a href="#PDC-ViT-Source-Camera-Identification-using-Pixel-Difference-Convolution-and-Vision-Transformer" class="headerlink" title="PDC-ViT : Source Camera Identification using Pixel Difference   Convolution and Vision Transformer"></a>PDC-ViT : Source Camera Identification using Pixel Difference   Convolution and Vision Transformer</h2><p><strong>Authors:Omar Elharrouss, Younes Akbari, Noor Almaadeed, Somaya Al-Maadeed, Fouad Khelifi, Ahmed Bouridane</strong></p>
<p>Source camera identification has emerged as a vital solution to unlock incidents involving critical cases like terrorism, violence, and other criminal activities. The ability to trace the origin of an image&#x2F;video can aid law enforcement agencies in gathering evidence and constructing the timeline of events. Moreover, identifying the owner of a certain device narrows down the area of search in a criminal investigation where smartphone devices are involved. This paper proposes a new pixel-based method for source camera identification, integrating Pixel Difference Convolution (PDC) with a Vision Transformer network (ViT), and named PDC-ViT. While the PDC acts as the backbone for feature extraction by exploiting Angular PDC (APDC) and Radial PDC (RPDC). These techniques enhance the capability to capture subtle variations in pixel information, which are crucial for distinguishing between different source cameras. The second part of the methodology focuses on classification, which is based on a Vision Transformer network. Unlike traditional methods that utilize image patches directly for training the classification network, the proposed approach uniquely inputs PDC features into the Vision Transformer network. To demonstrate the effectiveness of the PDC-ViT approach, it has been assessed on five different datasets, which include various image contents and video scenes. The method has also been compared with state-of-the-art source camera identification methods. Experimental results demonstrate the effectiveness and superiority of the proposed system in terms of accuracy and robustness when compared to its competitors. For example, our proposed PDC-ViT has achieved an accuracy of 94.30%, 84%, 94.22% and 92.29% using the Vision dataset, Daxing dataset, Socrates dataset and QUFVD dataset, respectively. </p>
<blockquote>
<p>æºç›¸æœºè¯†åˆ«å·²ä½œä¸ºä¸€ç§å…³é”®è§£å†³æ–¹æ¡ˆï¼Œç”¨äºè§£å†³æ¶‰åŠææ€–ä¸»ä¹‰ã€æš´åŠ›å’Œå…¶ä»–çŠ¯ç½ªæ´»åŠ¨çš„å…³é”®æ¡ˆä»¶ã€‚è¿½è¸ªå›¾åƒ&#x2F;è§†é¢‘çš„æ¥æºå¯ä»¥å¸®åŠ©æ‰§æ³•æœºæ„æ”¶é›†è¯æ®å’Œæ„å»ºäº‹ä»¶çš„æ—¶é—´çº¿ã€‚æ­¤å¤–ï¼Œç¡®å®šæŸä¸€è®¾å¤‡çš„æ‰€æœ‰è€…å¯ä»¥ç¼©å°æ¶‰åŠæ™ºèƒ½æ‰‹æœºçš„åˆ‘äº‹è°ƒæŸ¥çš„æœç´¢èŒƒå›´ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºåƒç´ çš„æºç›¸æœºè¯†åˆ«æ–°æ–¹æ³•ï¼Œå°†åƒç´ å·®å¼‚å·ç§¯ï¼ˆPDCï¼‰ä¸è§†è§‰è½¬æ¢å™¨ç½‘ç»œï¼ˆViTï¼‰ç›¸ç»“åˆï¼Œåä¸ºPDC-ViTã€‚å…¶ä¸­ï¼ŒPDCä½œä¸ºç‰¹å¾æå–çš„ä¸»å¹²ï¼Œåˆ©ç”¨è§’åº¦PDCï¼ˆAPDCï¼‰å’Œå¾„å‘PDCï¼ˆRPDCï¼‰ã€‚è¿™äº›æŠ€æœ¯æé«˜äº†æ•æ‰åƒç´ ä¿¡æ¯ä¸­ç»†å¾®å˜åŒ–çš„èƒ½åŠ›ï¼Œè¿™å¯¹äºåŒºåˆ†ä¸åŒæºç›¸æœºè‡³å…³é‡è¦ã€‚è¯¥æ–¹æ³•çš„ç¬¬äºŒéƒ¨åˆ†ä¾§é‡äºåˆ†ç±»ï¼ŒåŸºäºè§†è§‰è½¬æ¢å™¨ç½‘ç»œã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼Œä¼ ç»Ÿæ–¹æ³•ç›´æ¥ä½¿ç”¨å›¾åƒè¡¥ä¸æ¥è®­ç»ƒåˆ†ç±»ç½‘ç»œï¼Œæ‰€æå‡ºçš„æ–¹æ³•ç‹¬ç‰¹åœ°å°†PDCç‰¹å¾è¾“å…¥åˆ°è§†è§‰è½¬æ¢å™¨ç½‘ç»œä¸­ã€‚ä¸ºäº†è¯æ˜PDC-ViTæ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå®ƒå·²åœ¨äº”ä¸ªä¸åŒçš„æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œè¿™äº›æ•°æ®é›†åŒ…æ‹¬å„ç§å›¾åƒå†…å®¹å’Œè§†é¢‘åœºæ™¯ã€‚è¯¥æ–¹æ³•è¿˜ä¸å›½å®¶å…ˆè¿›çš„æºç›¸æœºè¯†åˆ«æ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ç«äº‰å¯¹æ‰‹ç›¸æ¯”ï¼Œè¯¥ç³»ç»Ÿåœ¨å‡†ç¡®æ€§å’Œç¨³å¥æ€§æ–¹é¢è¡¨ç°å‡ºæœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬æå‡ºçš„PDC-ViTåœ¨Visionæ•°æ®é›†ã€å¤§å…´æ•°æ®é›†ã€è‹æ ¼æ‹‰åº•æ•°æ®é›†å’ŒQUFVDæ•°æ®é›†ä¸Šåˆ†åˆ«å®ç°äº†94.30%ã€84%ã€94.22%å’Œ92.29%çš„å‡†ç¡®ç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.16227v1">PDF</a> </p>
<p><strong>Summary</strong><br>è¯¥æ–‡æœ¬æå‡ºäº†ä¸€ç§æ–°çš„åŸºäºåƒç´ çš„æºç›¸æœºè¯†åˆ«æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç»“åˆåƒç´ å·®å¼‚å·ç§¯ï¼ˆPDCï¼‰å’Œè§†è§‰è½¬æ¢å™¨ç½‘ç»œï¼ˆViTï¼‰ï¼Œè¢«ç§°ä¸ºPDC-ViTã€‚è¯¥æ–¹æ³•é€šè¿‡åˆ©ç”¨è§’PDCï¼ˆAPDCï¼‰å’Œå¾„å‘PDCï¼ˆRPDCï¼‰ä½œä¸ºç‰¹å¾æå–çš„åå¤‡ï¼Œå¹¶åˆ›æ–°åœ°å°†PDCç‰¹å¾è¾“å…¥åˆ°è§†è§‰è½¬æ¢å™¨ç½‘ç»œä¸­è¿›è¡Œåˆ†ç±»ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ç°æœ‰æºç›¸æœºè¯†åˆ«æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥ç³»ç»Ÿçš„å‡†ç¡®æ€§å’Œé²æ£’æ€§æ›´é«˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æºç›¸æœºè¯†åˆ«åœ¨è§£å†³æ¶‰åŠææ€–ä¸»ä¹‰ã€æš´åŠ›å’Œå…¶ä»–çŠ¯ç½ªæ´»åŠ¨çš„å…³é”®æ¡ˆä»¶ä¸­è‡³å…³é‡è¦ã€‚</li>
<li>æ–°çš„åƒç´ å·®å¼‚å·ç§¯ï¼ˆPDCï¼‰ä¸è§†è§‰è½¬æ¢å™¨ç½‘ç»œï¼ˆViTï¼‰ç»“åˆçš„PDC-ViTæ–¹æ³•è¢«æå‡ºç”¨äºæºç›¸æœºè¯†åˆ«ã€‚</li>
<li>PDCç‰¹å¾æå–æŠ€æœ¯é€šè¿‡åˆ©ç”¨è§’PDCï¼ˆAPDCï¼‰å’Œå¾„å‘PDCï¼ˆRPDCï¼‰å¢å¼ºæ•æ‰åƒç´ ç»†å¾®å˜åŒ–çš„èƒ½åŠ›ï¼Œè¿™å¯¹äºåŒºåˆ†ä¸åŒæºç›¸æœºè‡³å…³é‡è¦ã€‚</li>
<li>PDC-ViTçš„ç‹¬ç‰¹ä¹‹å¤„åœ¨äºå°†PDCç‰¹å¾è¾“å…¥åˆ°è§†è§‰è½¬æ¢å™¨ç½‘ç»œä¸­è¿›è¡Œåˆ†ç±»ï¼Œä¸åŒäºä¼ ç»Ÿæ–¹æ³•ç›´æ¥ä½¿ç”¨å›¾åƒå—è¿›è¡Œè®­ç»ƒã€‚</li>
<li>PDC-ViTæ–¹æ³•å·²åœ¨äº”ä¸ªä¸åŒçš„æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼ŒåŒ…æ‹¬å„ç§å›¾åƒå†…å®¹å’Œè§†é¢‘åœºæ™¯ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒPDC-ViTç³»ç»Ÿåœ¨å‡†ç¡®æ€§å’Œé²æ£’æ€§æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚</li>
<li>PDC-ViTåœ¨Visionæ•°æ®é›†ã€å¤§å…´æ•°æ®é›†ã€è‹æ ¼æ‹‰åº•æ•°æ®é›†å’ŒQUFVDæ•°æ®é›†ä¸Šçš„å‡†ç¡®ç‡åˆ†åˆ«ä¸º94.30%ã€84%ã€94.22%å’Œ92.29%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.16227">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-afcfe5e652ac8c5f458b76a5ae9521a8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d3e5f0b1a5ad246680a26d73f0d93213.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7da299c1c487d2a45759b9a0852fecf2.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Leveraging-Video-Vision-Transformer-for-Alzheimerâ€™s-Disease-Diagnosis-from-3D-Brain-MRI"><a href="#Leveraging-Video-Vision-Transformer-for-Alzheimerâ€™s-Disease-Diagnosis-from-3D-Brain-MRI" class="headerlink" title="Leveraging Video Vision Transformer for Alzheimerâ€™s Disease Diagnosis   from 3D Brain MRI"></a>Leveraging Video Vision Transformer for Alzheimerâ€™s Disease Diagnosis   from 3D Brain MRI</h2><p><strong>Authors:Taymaz Akan, Sait Alp, Md. Shenuarin Bhuiyan, Elizabeth A. Disbrow, Steven A. Conrad, John A. Vanchiere, Christopher G. Kevil, Mohammad A. N. Bhuiyan</strong></p>
<p>Alzheimerâ€™s disease (AD) is a neurodegenerative disorder affecting millions worldwide, necessitating early and accurate diagnosis for optimal patient management. In recent years, advancements in deep learning have shown remarkable potential in medical image analysis. Methods In this study, we present â€œViTranZheimer,â€ an AD diagnosis approach which leverages video vision transformers to analyze 3D brain MRI data. By treating the 3D MRI volumes as videos, we exploit the temporal dependencies between slices to capture intricate structural relationships. The video vision transformerâ€™s self-attention mechanisms enable the model to learn long-range dependencies and identify subtle patterns that may indicate AD progression. Our proposed deep learning framework seeks to enhance the accuracy and sensitivity of AD diagnosis, empowering clinicians with a tool for early detection and intervention. We validate the performance of the video vision transformer using the ADNI dataset and conduct comparative analyses with other relevant models. Results The proposed ViTranZheimer model is compared with two hybrid models, CNN-BiLSTM and ViT-BiLSTM. CNN-BiLSTM is the combination of a convolutional neural network (CNN) and a bidirectional long-short-term memory network (BiLSTM), while ViT-BiLSTM is the combination of a vision transformer (ViT) with BiLSTM. The accuracy levels achieved in the ViTranZheimer, CNN-BiLSTM, and ViT-BiLSTM models are 98.6%, 96.479%, and 97.465%, respectively. ViTranZheimer demonstrated the highest accuracy at 98.6%, outperforming other models in this evaluation metric, indicating its superior performance in this specific evaluation metric. Conclusion This research advances the understanding of applying deep learning techniques in neuroimaging and Alzheimerâ€™s disease research, paving the way for earlier and less invasive clinical diagnosis. </p>
<blockquote>
<p>é˜¿å°”èŒ¨æµ·é»˜ç—‡ï¼ˆADï¼‰æ˜¯ä¸€ç§å½±å“å…¨çƒæ•°ç™¾ä¸‡äººçš„ç¥ç»é€€è¡Œæ€§ç–¾ç—…ï¼Œéœ€è¦è¿›è¡Œæ—©æœŸå’Œå‡†ç¡®çš„è¯Šæ–­ä»¥è¿›è¡Œæœ€ä½³çš„æ‚£è€…ç®¡ç†ã€‚è¿‘å¹´æ¥ï¼Œæ·±åº¦å­¦ä¹ åœ¨åŒ»å­¦å›¾åƒåˆ†ææ–¹é¢çš„è¿›å±•æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ã€‚åœ¨æ­¤ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†â€œViTranZheimerâ€çš„ADè¯Šæ–­æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨è§†é¢‘è§†è§‰è½¬æ¢å™¨ï¼ˆVideo Vision Transformerï¼‰åˆ†æ3Då¤§è„‘MRIæ•°æ®ã€‚é€šè¿‡å°†3D MRIä½“ç§¯è§†ä¸ºè§†é¢‘ï¼Œæˆ‘ä»¬åˆ©ç”¨åˆ‡ç‰‡ä¹‹é—´çš„æ—¶é—´ä¾èµ–æ€§æ¥æ•æ‰å¤æ‚ç»“æ„å…³ç³»ã€‚è§†é¢‘è§†è§‰è½¬æ¢å™¨çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶ä½¿æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ é•¿æœŸä¾èµ–æ€§å¹¶è¯†åˆ«å¯èƒ½æŒ‡ç¤ºADè¿›å±•çš„å¾®å¦™æ¨¡å¼ã€‚æˆ‘ä»¬æå‡ºçš„æ·±åº¦å­¦ä¹ æ¡†æ¶æ—¨åœ¨æé«˜ADè¯Šæ–­çš„å‡†ç¡®æ€§å’Œæ•æ„Ÿæ€§ï¼Œä¸ºä¸´åºŠåŒ»ç”Ÿæä¾›ä¸€ç§æ—©æœŸæ£€æµ‹å’Œå¹²é¢„çš„å·¥å…·ã€‚æˆ‘ä»¬ä½¿ç”¨ADNIæ•°æ®é›†éªŒè¯äº†è§†é¢‘è§†è§‰è½¬æ¢å™¨çš„æ€§èƒ½ï¼Œå¹¶ä¸å…¶ä»–ç›¸å…³æ¨¡å‹è¿›è¡Œäº†æ¯”è¾ƒåˆ†æã€‚ç»“æœï¼šæ‰€æå‡ºçš„ViTranZheimeræ¨¡å‹ä¸ä¸¤ç§æ··åˆæ¨¡å‹ï¼ˆCNN-BiLSTMå’ŒViT-BiLSTMï¼‰è¿›è¡Œäº†æ¯”è¾ƒã€‚CNN-BiLSTMæ˜¯å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å’ŒåŒå‘é•¿çŸ­æœŸè®°å¿†ç½‘ç»œï¼ˆBiLSTMï¼‰çš„ç»„åˆï¼Œè€ŒViT-BiLSTMæ˜¯è§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰ä¸BiLSTMçš„ç»„åˆã€‚ViTranZheimerã€CNN-BiLSTMå’ŒViT-BiLSTMæ¨¡å‹ä¸­å®ç°çš„å‡†ç¡®åº¦åˆ†åˆ«ä¸º98.6%ã€96.479%å’Œ97.465%ã€‚ViTranZheimeråœ¨å‡†ç¡®åº¦æ–¹é¢è¡¨ç°å‡ºæœ€é«˜æ°´å¹³ï¼Œè¾¾åˆ°98.6%ï¼Œåœ¨æ­¤è¯„ä¼°æŒ‡æ ‡ä¸Šä¼˜äºå…¶ä»–æ¨¡å‹ï¼Œè¡¨æ˜å…¶åœ¨è¿™ä¸€ç‰¹å®šè¯„ä¼°æŒ‡æ ‡ä¸Šçš„æ€§èƒ½å“è¶Šã€‚ç»“è®ºï¼šè¯¥ç ”ç©¶æ·±åŒ–äº†æ·±åº¦å­¦ä¹ æŠ€æœ¯åœ¨ç¥ç»å½±åƒå­¦å’Œé˜¿å°”èŒ¨æµ·é»˜ç—‡ç ”ç©¶ä¸­çš„åº”ç”¨ï¼Œä¸ºæ›´æ—©ä¸”ä¾µå…¥æ€§è¾ƒå°çš„ä¸´åºŠè¯Šæ–­é“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.15733v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºâ€œViTranZheimerâ€çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºåˆ©ç”¨è§†é¢‘è§†è§‰è½¬æ¢å™¨åˆ†æä¸‰ç»´è„‘MRIæ•°æ®ï¼Œä»¥è¯Šæ–­é˜¿å°”èŒ¨æµ·é»˜ç—…ï¼ˆADï¼‰ã€‚è¯¥æ¡†æ¶é€šè¿‡åˆ©ç”¨è§†é¢‘è§†è§‰è½¬æ¢å™¨çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œå­¦ä¹ é•¿æœŸä¾èµ–å…³ç³»å¹¶è¯†åˆ«å¯èƒ½æŒ‡ç¤ºADè¿›å±•çš„å¾®å¦™æ¨¡å¼ã€‚è¯¥ç ”ç©¶ä½¿ç”¨ADNIæ•°æ®é›†éªŒè¯äº†è§†é¢‘è§†è§‰è½¬æ¢å™¨çš„æ€§èƒ½ï¼Œå¹¶ä¸å…¶å®ƒç›¸å…³æ¨¡å‹è¿›è¡Œäº†æ¯”è¾ƒåˆ†æã€‚ç»“æœæ˜¾ç¤ºï¼ŒViTranZheimeræ¨¡å‹çš„å‡†ç¡®ç‡ä¸º98.6%ï¼Œä¼˜äºCNN-BiLSTMå’ŒViT-BiLSTMæ¨¡å‹ã€‚æ­¤ç ”ç©¶ä¸ºæ—©æœŸè¯Šæ–­é˜¿å°”èŒ¨æµ·é»˜ç—…æä¾›äº†æ–°çš„å·¥å…·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶æå‡ºäº†â€œViTranZheimerâ€æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºé˜¿å°”èŒ¨æµ·é»˜ç—…çš„è¯Šæ–­ã€‚</li>
<li>åˆ©ç”¨è§†é¢‘è§†è§‰è½¬æ¢å™¨åˆ†æä¸‰ç»´è„‘MRIæ•°æ®ã€‚</li>
<li>è§†é¢‘è§†è§‰è½¬æ¢å™¨çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶èƒ½å­¦ä¹ é•¿æœŸä¾èµ–å…³ç³»å¹¶è¯†åˆ«ADè¿›å±•çš„å¾®å¦™æ¨¡å¼ã€‚</li>
<li>ä½¿ç”¨ADNIæ•°æ®é›†éªŒè¯äº†è§†é¢‘è§†è§‰è½¬æ¢å™¨çš„æ€§èƒ½ã€‚</li>
<li>ViTranZheimeræ¨¡å‹çš„è¯Šæ–­å‡†ç¡®ç‡è¾¾åˆ°äº†98.6%ã€‚</li>
<li>ViTranZheimeræ¨¡å‹çš„æ€§èƒ½ä¼˜äºCNN-BiLSTMå’ŒViT-BiLSTMæ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.15733">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5ab129f2734ce488357d5baee79c5c4f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-44c98036fdbb06021f984cdd59f0366e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8cf19cdebca2773d6909270fd702ec58.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-21799bc0ac2ef129db7f0a03901be463.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="GeoGround-A-Unified-Large-Vision-Language-Model-for-Remote-Sensing-Visual-Grounding"><a href="#GeoGround-A-Unified-Large-Vision-Language-Model-for-Remote-Sensing-Visual-Grounding" class="headerlink" title="GeoGround: A Unified Large Vision-Language Model for Remote Sensing   Visual Grounding"></a>GeoGround: A Unified Large Vision-Language Model for Remote Sensing   Visual Grounding</h2><p><strong>Authors:Yue Zhou, Mengcheng Lan, Xiang Li, Yiping Ke, Xue Jiang, Litong Feng, Wayne Zhang</strong></p>
<p>Remote sensing (RS) visual grounding aims to use natural language expression to locate specific objects (in the form of the bounding box or segmentation mask) in RS images, enhancing human interaction with intelligent RS interpretation systems. Early research in this area was primarily based on horizontal bounding boxes (HBBs), but as more diverse RS datasets have become available, tasks involving oriented bounding boxes (OBBs) and segmentation masks have emerged. In practical applications, different targets require different grounding types: HBB can localize an objectâ€™s position, OBB provides its orientation, and mask depicts its shape. However, existing specialized methods are typically tailored to a single type of RS visual grounding task and are hard to generalize across tasks. In contrast, large vision-language models (VLMs) exhibit powerful multi-task learning capabilities but struggle to handle dense prediction tasks like segmentation. This paper proposes GeoGround, a novel framework that unifies support for HBB, OBB, and mask RS visual grounding tasks, allowing flexible output selection. Rather than customizing the architecture of VLM, our work aims to elegantly support pixel-level visual grounding output through the Text-Mask technique. We define prompt-assisted and geometry-guided learning to enhance consistency across different signals. To support model training, we present refGeo, a large-scale RS visual instruction-following dataset containing 161k image-text pairs. Experimental results show that GeoGround demonstrates strong performance across four RS visual grounding tasks, matching or surpassing the performance of specialized methods on multiple benchmarks. Code available at <a target="_blank" rel="noopener" href="https://github.com/zytx121/GeoGround">https://github.com/zytx121/GeoGround</a> </p>
<blockquote>
<p>é¥æ„Ÿï¼ˆRSï¼‰è§†è§‰å®šä½æ—¨åœ¨åˆ©ç”¨è‡ªç„¶è¯­è¨€è¡¨è¾¾å¼åœ¨é¥æ„Ÿå›¾åƒä¸­å®šä½ç‰¹å®šå¯¹è±¡ï¼ˆä»¥è¾¹ç•Œæ¡†æˆ–åˆ†å‰²æ©ç çš„å½¢å¼ï¼‰ï¼Œå¢å¼ºäººç±»ä¸æ™ºèƒ½é¥æ„Ÿè§£é‡Šç³»ç»Ÿçš„äº¤äº’ã€‚æ—©æœŸçš„ç ”ç©¶ä¸»è¦åŸºäºæ°´å¹³è¾¹ç•Œæ¡†ï¼ˆHBBï¼‰ï¼Œä½†éšç€æ›´å¤šé¥æ„Ÿæ•°æ®é›†çš„å¯ç”¨ï¼Œå‡ºç°äº†æ¶‰åŠå®šå‘è¾¹ç•Œæ¡†ï¼ˆOBBï¼‰å’Œåˆ†å‰²æ©ç çš„ä»»åŠ¡ã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼Œä¸åŒçš„ç›®æ ‡éœ€è¦ä¸åŒç±»å‹çš„å®šä½ï¼šHBBå¯ä»¥å®šä½å¯¹è±¡çš„ä½ç½®ï¼ŒOBBæä¾›å…¶æ–¹å‘ï¼Œè€Œæ©ç æç»˜å…¶å½¢çŠ¶ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ä¸“ç”¨æ–¹æ³•é€šå¸¸é’ˆå¯¹å•ä¸€çš„é¥æ„Ÿè§†è§‰å®šä½ä»»åŠ¡ï¼Œéš¾ä»¥è·¨ä»»åŠ¡æ¨å¹¿ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰è¡¨ç°å‡ºå¼ºå¤§çš„å¤šä»»åŠ¡å­¦ä¹ èƒ½åŠ›ï¼Œä½†åœ¨å¤„ç†å¦‚åˆ†å‰²ä¹‹ç±»çš„å¯†é›†é¢„æµ‹ä»»åŠ¡æ—¶å´è¡¨ç°æŒ£æ‰ã€‚æœ¬æ–‡æå‡ºäº†GeoGroundï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–çš„ç»Ÿä¸€æ¡†æ¶ï¼Œæ”¯æŒHBBã€OBBå’Œæ©ç é¥æ„Ÿè§†è§‰å®šä½ä»»åŠ¡ï¼Œå…è®¸çµæ´»è¾“å‡ºé€‰æ‹©ã€‚æˆ‘ä»¬çš„å·¥ä½œæ—¨åœ¨é€šè¿‡æ–‡æœ¬æ©ç æŠ€æœ¯å·§å¦™åœ°æ”¯æŒåƒç´ çº§è§†è§‰å®šä½è¾“å‡ºï¼Œè€Œä¸æ˜¯å®šåˆ¶VLMçš„æ¶æ„ã€‚æˆ‘ä»¬å®šä¹‰äº†æç¤ºè¾…åŠ©å’Œå‡ ä½•å¼•å¯¼å­¦ä¹ ä»¥å¢å¼ºä¸åŒä¿¡å·ä¹‹é—´çš„ä¸€è‡´æ€§ã€‚ä¸ºäº†æ”¯æŒæ¨¡å‹è®­ç»ƒï¼Œæˆ‘ä»¬æä¾›äº†refGeoï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„é¥æ„Ÿè§†è§‰æŒ‡ä»¤è·Ÿéšæ•°æ®é›†ï¼ŒåŒ…å«16.1ä¸‡å¼ å›¾åƒæ–‡æœ¬å¯¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGeoGroundåœ¨å››ä¸ªé¥æ„Ÿè§†è§‰å®šä½ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå¼ºåŠ²çš„æ€§èƒ½ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æˆ–è¶…è¶Šäº†ä¸“ç”¨æ–¹æ³•çš„è¡¨ç°ã€‚ä»£ç å¯è®¿é—® <a target="_blank" rel="noopener" href="https://github.com/zytx121/GeoGround">https://github.com/zytx121/GeoGround</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.11904v2">PDF</a> 25 pages, 19 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é¥æ„Ÿè§†è§‰å®šä½æŠ€æœ¯ä¸­çš„ä¸€é¡¹æ–°ç ”ç©¶ï¼Œåä¸ºGeoGroundã€‚è¯¥æŠ€æœ¯æ—¨åœ¨ç»Ÿä¸€æ”¯æŒæ°´å¹³è¾¹ç•Œæ¡†ï¼ˆHBBï¼‰ã€å®šå‘è¾¹ç•Œæ¡†ï¼ˆOBBï¼‰å’Œæ©è†œä¸‰ç§é¥æ„Ÿè§†è§‰å®šä½ä»»åŠ¡ï¼Œå…è®¸çµæ´»è¾“å‡ºé€‰æ‹©ã€‚è¯¥ç ”ç©¶é€šè¿‡Text-MaskæŠ€æœ¯å®ç°åƒç´ çº§è§†è§‰å®šä½è¾“å‡ºï¼Œå¹¶æå‡ºpromptè¾…åŠ©å’Œå‡ ä½•å¼•å¯¼å­¦ä¹ ä»¥å¢å¼ºä¸åŒä¿¡å·ä¹‹é—´çš„ä¸€è‡´æ€§ã€‚ä¸ºæ”¯æŒæ¨¡å‹è®­ç»ƒï¼Œç ”ç©¶å›¢é˜Ÿè¿˜æ¨å‡ºäº†å¤§è§„æ¨¡çš„é¥æ„Ÿè§†è§‰æŒ‡ä»¤æ•°æ®é›†refGeoï¼ŒåŒ…å«16.1ä¸‡å¼ å›¾åƒæ–‡æœ¬å¯¹ã€‚GeoGroundåœ¨å››ä¸ªé¥æ„Ÿè§†è§‰å®šä½ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­åŒ¹é…æˆ–è¶…è¶Šäº†ä¸“ç”¨æ–¹æ³•çš„è¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¥æ„Ÿè§†è§‰å®šä½æŠ€æœ¯ï¼ˆRS visual groundingï¼‰åˆ©ç”¨è‡ªç„¶è¯­è¨€è¡¨è¾¾å¼åœ¨é¥æ„Ÿå›¾åƒä¸­å®šä½ç‰¹å®šå¯¹è±¡ï¼Œå¢å¼ºäººä¸æ™ºèƒ½é¥æ„Ÿè§£é‡Šç³»ç»Ÿçš„äº¤äº’ã€‚</li>
<li>æ—©æœŸç ”ç©¶ä¸»è¦åŸºäºæ°´å¹³è¾¹ç•Œæ¡†ï¼ˆHBBï¼‰ï¼Œä½†ç°åœ¨éšç€æ›´ä¸°å¯Œçš„é¥æ„Ÿæ•°æ®é›†çš„å‡ºç°ï¼Œæ¶‰åŠå®šå‘è¾¹ç•Œæ¡†ï¼ˆOBBï¼‰å’Œæ©è†œçš„ä»»åŠ¡å·²å…´èµ·ã€‚</li>
<li>ä¸åŒç›®æ ‡éœ€è¦ä¸åŒç±»å‹çš„å®šä½æ–¹å¼ï¼šHBBå¯å®šä½å¯¹è±¡ä½ç½®ï¼ŒOBBæä¾›æ–¹å‘ä¿¡æ¯ï¼Œæ©è†œæç»˜å¯¹è±¡å½¢çŠ¶ã€‚</li>
<li>ç°æœ‰æ–¹æ³•é€šå¸¸ä¸“ä¸ºå•ä¸€ç±»å‹çš„é¥æ„Ÿè§†è§‰å®šä½ä»»åŠ¡è®¾è®¡ï¼Œéš¾ä»¥è·¨ä»»åŠ¡æ¨å¹¿ã€‚</li>
<li>GeoGroundæ¡†æ¶ç»Ÿä¸€æ”¯æŒHBBã€OBBå’Œæ©è†œé¥æ„Ÿè§†è§‰å®šä½ä»»åŠ¡ï¼Œå…è®¸çµæ´»è¾“å‡ºé€‰æ‹©ã€‚</li>
<li>é€šè¿‡Text-MaskæŠ€æœ¯å®ç°åƒç´ çº§è§†è§‰å®šä½è¾“å‡ºã€‚</li>
<li>GeoGroundåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼ŒåŒ¹é…æˆ–è¶…è¶Šä¸“ç”¨æ–¹æ³•çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.11904">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c0a8d14c9608dc99014dd31608012f72.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-255dbd687ee2aafa1c1782d9075a4890.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-87b54a4527bccf7f9abc6227f9b297c1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-92a30b11aa565ee3a9b81a76ae7350a3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4cb0c7d6901412ccce3cead3b4189f8c.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="QCS-Feature-Refining-from-Quadruplet-Cross-Similarity-for-Facial-Expression-Recognition"><a href="#QCS-Feature-Refining-from-Quadruplet-Cross-Similarity-for-Facial-Expression-Recognition" class="headerlink" title="QCS: Feature Refining from Quadruplet Cross Similarity for Facial   Expression Recognition"></a>QCS: Feature Refining from Quadruplet Cross Similarity for Facial   Expression Recognition</h2><p><strong>Authors:Chengpeng Wang, Li Chen, Lili Wang, Zhaofan Li, Xuebin Lv</strong></p>
<p>Facial expression recognition faces challenges where labeled significant features in datasets are mixed with unlabeled redundant ones. In this paper, we introduce Cross Similarity Attention (CSA) to mine richer intrinsic information from image pairs, overcoming a limitation when the Scaled Dot-Product Attention of ViT is directly applied to calculate the similarity between two different images. Based on CSA, we simultaneously minimize intra-class differences and maximize inter-class differences at the fine-grained feature level through interactions among multiple branches. Contrastive residual distillation is utilized to transfer the information learned in the cross module back to the base network. We ingeniously design a four-branch centrally symmetric network, named Quadruplet Cross Similarity (QCS), which alleviates gradient conflicts arising from the cross module and achieves balanced and stable training. It can adaptively extract discriminative features while isolating redundant ones. The cross-attention modules exist during training, and only one base branch is retained during inference, resulting in no increase in inference time. Extensive experiments show that our proposed method achieves state-of-the-art performance on several FER datasets. </p>
<blockquote>
<p>é¢éƒ¨è¡¨æƒ…è¯†åˆ«é¢ä¸´ç€æ•°æ®é›†ä¸­çš„æ ‡è®°é‡è¦ç‰¹å¾ä¸æœªæ ‡è®°å†—ä½™ç‰¹å¾æ··åˆçš„æŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº¤å‰ç›¸ä¼¼æ€§æ³¨æ„åŠ›ï¼ˆCSAï¼‰æ¥ä»å›¾åƒå¯¹ä¸­æå–æ›´ä¸°å¯Œçš„å†…åœ¨ä¿¡æ¯ï¼Œå…‹æœäº†å½“ç›´æ¥ä½¿ç”¨ViTçš„ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›è®¡ç®—ä¸¤ä¸ªä¸åŒå›¾åƒä¹‹é—´çš„ç›¸ä¼¼æ€§æ—¶å­˜åœ¨çš„å±€é™æ€§ã€‚åŸºäºCSAï¼Œæˆ‘ä»¬é€šè¿‡å¤šä¸ªåˆ†æ”¯ä¹‹é—´çš„äº¤äº’ï¼Œåœ¨ç»†ç²’åº¦ç‰¹å¾çº§åˆ«åŒæ—¶å‡å°ç±»å†…å·®å¼‚å¹¶æœ€å¤§åŒ–ç±»é—´å·®å¼‚ã€‚åˆ©ç”¨å¯¹æ¯”æ®‹å·®è’¸é¦å°†è·¨æ¨¡å—ä¸­å­¦ä¹ åˆ°çš„ä¿¡æ¯è½¬å›åŸºç¡€ç½‘ç»œã€‚æˆ‘ä»¬å·§å¦™åœ°è®¾è®¡äº†ä¸€ä¸ªå››åˆ†æ”¯ä¸­å¿ƒå¯¹ç§°ç½‘ç»œï¼Œå‘½åä¸ºå››å…ƒç»„äº¤å‰ç›¸ä¼¼æ€§ï¼ˆQCSï¼‰ï¼Œè¯¥ç½‘ç»œç¼“è§£äº†è·¨æ¨¡å—äº§ç”Ÿçš„æ¢¯åº¦å†²çªï¼Œå®ç°äº†å¹³è¡¡ç¨³å®šçš„è®­ç»ƒã€‚å®ƒå¯ä»¥è‡ªé€‚åº”åœ°æå–åˆ¤åˆ«ç‰¹å¾ï¼ŒåŒæ—¶éš”ç¦»å†—ä½™ç‰¹å¾ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å­˜åœ¨äº¤å‰æ³¨æ„åŠ›æ¨¡å—ï¼Œè€Œåœ¨æ¨ç†æœŸé—´ä»…ä¿ç•™ä¸€ä¸ªåŸºç¡€åˆ†æ”¯ï¼Œå› æ­¤ä¸ä¼šå¢åŠ æ¨ç†æ—¶é—´ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•åœ¨å¤šä¸ªé¢éƒ¨è¡¨æƒ…è¯†åˆ«æ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€æ–°æ€§èƒ½æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.01988v5">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å¼•å…¥äº†ä¸€ç§åä¸ºCross Similarity Attentionï¼ˆCSAï¼‰çš„æŠ€æœ¯ï¼Œç”¨äºä»å›¾åƒå¯¹ä¸­æŒ–æ˜æ›´ä¸°å¯Œçš„å†…åœ¨ä¿¡æ¯ï¼Œè§£å†³äº†åœ¨é¢éƒ¨è¡¨æƒ…è¯†åˆ«ä¸­ï¼Œæ•°æ®é›†ä¸­æ ‡ç­¾é‡è¦çš„ç‰¹å¾ä¸æœªæ ‡ç­¾çš„å†—ä½™ç‰¹å¾æ··åˆçš„é—®é¢˜ã€‚é€šè¿‡CSAï¼Œæœ¬æ–‡åœ¨ç»†ç²’åº¦ç‰¹å¾å±‚é¢åŒæ—¶å‡å°äº†ç±»å†…å·®å¼‚å¹¶æ”¾å¤§äº†ç±»é—´å·®å¼‚ã€‚æ­¤å¤–ï¼Œè¿˜å·§å¦™åœ°è®¾è®¡äº†åä¸ºQuadruplet Cross Similarityï¼ˆQCSï¼‰çš„å››åˆ†æ”¯ä¸­å¿ƒå¯¹ç§°ç½‘ç»œï¼Œç¼“è§£äº†è·¨æ¨¡å—å¼•èµ·çš„æ¢¯åº¦å†²çªï¼Œå®ç°äº†å¹³è¡¡ç¨³å®šçš„è®­ç»ƒã€‚è¯¥æ–¹æ³•å¯åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è‡ªé€‚åº”æå–åˆ¤åˆ«ç‰¹å¾å¹¶éš”ç¦»å†—ä½™ç‰¹å¾ï¼Œä¸”æ¨ç†æ—¶é—´å¹¶æœªå¢åŠ ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªé¢éƒ¨è¡¨æƒ…è¯†åˆ«æ•°æ®é›†ä¸Šè¾¾åˆ°äº†é¢†å…ˆæ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥Cross Similarity Attentionï¼ˆCSAï¼‰æŠ€æœ¯ï¼Œä»å›¾åƒå¯¹ä¸­æŒ–æ˜æ›´ä¸°å¯Œå†…åœ¨ä¿¡æ¯ã€‚</li>
<li>è§£å†³æ•°æ®é›†ä¸­æ ‡ç­¾é‡è¦ç‰¹å¾ä¸æœªæ ‡ç­¾å†—ä½™ç‰¹å¾æ··åˆçš„é—®é¢˜ã€‚</li>
<li>é€šè¿‡CSAæŠ€æœ¯ï¼ŒåŒæ—¶å‡å°ç±»å†…å·®å¼‚å¹¶æ”¾å¤§ç±»é—´å·®å¼‚ã€‚</li>
<li>é‡‡ç”¨å¯¹æ¯”æ®‹å·®è’¸é¦æŠ€æœ¯å°†è·¨æ¨¡å—å­¦åˆ°çš„ä¿¡æ¯å›é¦ˆåˆ°åŸºç¡€ç½‘ç»œã€‚</li>
<li>è®¾è®¡äº†å››åˆ†æ”¯ä¸­å¿ƒå¯¹ç§°ç½‘ç»œQuadruplet Cross Similarityï¼ˆQCSï¼‰ã€‚</li>
<li>QCSç½‘ç»œç¼“è§£äº†è·¨æ¨¡å—å¼•èµ·çš„æ¢¯åº¦å†²çªï¼Œå®ç°äº†å¹³è¡¡ç¨³å®šçš„è®­ç»ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.01988">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d8191ba90d8d41a5f3d41c89bbf2eb9d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2ae0a54bada1b956761cabf60c383b0a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5dff649372844a68dbfbe48588ed9e1d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aab64baa1b65a6e2937d30320b880d0d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-62f32a70ff09108a8a0086b72ee97d27.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4e6a2c0b2dc406b1acd0d458e03d78e1.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Random-Set-Neural-Networks-RS-NN"><a href="#Random-Set-Neural-Networks-RS-NN" class="headerlink" title="Random-Set Neural Networks (RS-NN)"></a>Random-Set Neural Networks (RS-NN)</h2><p><strong>Authors:Shireen Kudukkil Manchingal, Muhammad Mubashar, Kaizheng Wang, Keivan Shariatmadar, Fabio Cuzzolin</strong></p>
<p>Machine learning is increasingly deployed in safety-critical domains where erroneous predictions may lead to potentially catastrophic consequences, highlighting the need for learning systems to be aware of how confident they are in their own predictions: in other words, â€˜to know when they do not knowâ€™. In this paper, we propose a novel Random-Set Neural Network (RS-NN) approach to classification which predicts belief functions (rather than classical probability vectors) over the class list using the mathematics of random sets, i.e., distributions over the collection of sets of classes. RS-NN encodes the â€˜epistemicâ€™ uncertainty induced by training sets that are insufficiently representative or limited in size via the size of the convex set of probability vectors associated with a predicted belief function. Our approach outperforms state-of-the-art Bayesian and Ensemble methods in terms of accuracy, uncertainty estimation and out-of-distribution (OoD) detection on multiple benchmarks (CIFAR-10 vs SVHN&#x2F;Intel-Image, MNIST vs FMNIST&#x2F;KMNIST, ImageNet vs ImageNet-O). RS-NN also scales up effectively to large-scale architectures (e.g. WideResNet-28-10, VGG16, Inception V3, EfficientNetB2 and ViT-Base-16), exhibits remarkable robustness to adversarial attacks and can provide statistical guarantees in a conformal learning setting. </p>
<blockquote>
<p>æœºå™¨å­¦ä¹ åœ¨å®‰å…¨æ€§è‡³å…³é‡è¦çš„é¢†åŸŸå¾—åˆ°äº†è¶Šæ¥è¶Šå¹¿æ³›çš„åº”ç”¨ï¼Œé”™è¯¯çš„é¢„æµ‹å¯èƒ½å¯¼è‡´ç¾éš¾æ€§çš„åæœï¼Œè¿™å‡¸æ˜¾äº†å­¦ä¹ ç³»ç»Ÿéœ€è¦äº†è§£è‡ªå·±å¯¹é¢„æµ‹çš„ä¿¡å¿ƒç¨‹åº¦çš„é‡è¦æ€§ï¼Œæ¢å¥è¯è¯´ï¼Œå°±æ˜¯â€œè¦çŸ¥é“è‡ªå·±ä¸çŸ¥é“â€ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„éšæœºé›†ç¥ç»ç½‘ç»œï¼ˆRS-NNï¼‰åˆ†ç±»æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä½¿ç”¨éšæœºé›†çš„æ•°å­¦åŸç†é¢„æµ‹ç±»åˆ—è¡¨ä¸Šçš„ä¿¡å¿µå‡½æ•°ï¼ˆè€Œä¸æ˜¯ä¼ ç»Ÿçš„æ¦‚ç‡å‘é‡ï¼‰ã€‚RS-NNé€šè¿‡é¢„æµ‹çš„ä¿¡å¿µå‡½æ•°å…³è”çš„å‡¸æ¦‚ç‡å‘é‡é›†çš„å¤§å°ï¼Œå¯¹ç”±è®­ç»ƒé›†å¼•èµ·çš„ä¸å……åˆ†ä»£è¡¨æ€§æˆ–æœ‰é™å¤§å°çš„â€œä¸»è§‚â€ä¸ç¡®å®šæ€§è¿›è¡Œç¼–ç ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ï¼ˆCIFAR-10ä¸SVHN&#x2F;Intel-Imageã€MNISTä¸FMNIST&#x2F;KMNISTã€ImageNetä¸ImageNet-Oï¼‰ä¸Šçš„å‡†ç¡®æ€§ã€ä¸ç¡®å®šæ€§ä¼°è®¡å’Œè¶…å‡ºåˆ†å¸ƒï¼ˆOoDï¼‰æ£€æµ‹æ–¹é¢å‡ä¼˜äºæœ€å…ˆè¿›çš„è´å¶æ–¯å’Œé›†æˆæ–¹æ³•ã€‚RS-NNè¿˜èƒ½æœ‰æ•ˆåœ°æ‰©å±•åˆ°å¤§è§„æ¨¡æ¶æ„ï¼ˆå¦‚WideResNet-28-10ã€VGG16ã€Inception V3ã€EfficientNetB2å’ŒViT-Base-16ï¼‰ï¼Œå¯¹å¯¹æŠ—æ€§æ”»å‡»è¡¨ç°å‡ºæƒŠäººçš„ç¨³å¥æ€§ï¼Œå¹¶åœ¨ç¬¦åˆå­¦ä¹ ç¯å¢ƒä¸­æä¾›ç»Ÿè®¡ä¿è¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05772v3">PDF</a> Published at ICLR 2025</p>
<p><strong>Summary</strong><br>     æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºéšæœºé›†ç¥ç»ç½‘ç»œï¼ˆRS-NNï¼‰çš„åˆ†ç±»æ–¹æ³•ï¼Œé€šè¿‡éšæœºé›†æ•°å­¦ç†è®ºé¢„æµ‹ç±»åˆ«ä¿¡å¿µå‡½æ•°è€Œéä¼ ç»Ÿæ¦‚ç‡å‘é‡ã€‚è¯¥æ–¹æ³•é€šè¿‡è®­ç»ƒé›†ç›¸å…³çš„ä¸ç¡®å®šæ€§å’Œé›†åˆå‡¸æ¦‚ç‡å‘é‡å¤§å°ç¼–ç â€œçŸ¥è¯†ä¸ç¡®å®šæ€§â€ï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å±•ç°å‡ºä¼˜å¼‚çš„å‡†ç¡®æ€§ã€ä¸ç¡®å®šæ€§ä¼°è®¡å’Œè¶…å‡ºåˆ†å¸ƒæ£€æµ‹æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒRS-NNå¯æœ‰æ•ˆæ‰©å±•åˆ°å¤§å‹æ¶æ„ï¼Œå¯¹æŠ—æ”»å‡»å…·æœ‰å‡ºè‰²ç¨³å¥æ€§ï¼Œå¹¶åœ¨åˆè§„å­¦ä¹ ç¯å¢ƒä¸­æä¾›ç»Ÿè®¡ä¿éšœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬è®ºæ–‡å…³æ³¨æœºå™¨å­¦ä¹ åœ¨å…³é”®å®‰å…¨é¢†åŸŸä¸­çš„é¢„æµ‹ç½®ä¿¡åº¦é—®é¢˜ï¼Œæå‡ºäº†åŸºäºéšæœºé›†ç¥ç»ç½‘ç»œï¼ˆRS-NNï¼‰çš„åˆ†ç±»æ–¹æ³•ã€‚</li>
<li>RS-NNé€šè¿‡é¢„æµ‹ä¿¡å¿µå‡½æ•°è€Œéä¼ ç»Ÿæ¦‚ç‡å‘é‡æ¥è¯„ä¼°æ¨¡å‹é¢„æµ‹çš„ä¸ç¡®å®šæ€§ã€‚</li>
<li>RS-NNé€šè¿‡ç¼–ç è®­ç»ƒé›†ç›¸å…³çš„ä¸ç¡®å®šæ€§ï¼Œå±•ç°å‡ºè‰²çš„æ€§èƒ½ï¼Œå°¤å…¶åœ¨æ•°æ®é›†ä¸è¶³æˆ–è§„æ¨¡æœ‰é™çš„æƒ…å†µä¸‹ã€‚</li>
<li>åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒRS-NNåœ¨å‡†ç¡®æ€§ã€ä¸ç¡®å®šæ€§ä¼°è®¡å’Œè¶…å‡ºåˆ†å¸ƒæ£€æµ‹æ–¹é¢ä¼˜äºç°æœ‰å…ˆè¿›è´å¶æ–¯å’Œé›†æˆæ–¹æ³•ã€‚</li>
<li>RS-NNèƒ½æœ‰æ•ˆæ‰©å±•åˆ°å¤§å‹æ¶æ„ï¼Œå¹¶ä¸”å¯¹æŠ—æ”»å‡»å…·æœ‰ç¨³å¥æ€§ã€‚</li>
<li>RS-NNå¯åœ¨åˆè§„å­¦ä¹ ç¯å¢ƒä¸­æä¾›ç»Ÿè®¡ä¿éšœï¼Œä¸ºæœºå™¨å­¦ä¹ æ¨¡å‹çš„å¯é æ€§å’Œå®‰å…¨æ€§æä¾›ä¿éšœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2307.05772">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-77eec650234f37b302d62056705d07f0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f2fa83146a7ab9ee961d944241317d38.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fed6e6e4f62c8c05b4af93f4c86a49fc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-65f3b0f8e6003fabd55609f1a9c2cdd7.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-29/Vision%20Transformer/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-29/Vision%20Transformer/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Vision-Transformer/">
                                    <span class="chip bg-color">Vision Transformer</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-29/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-2a92bbd0d23754a71f03b16e8c088d0f.jpg" class="responsive-img" alt="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª">
                        
                        <span class="card-title">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-29  A Training-free Synthetic Data Selection Method for Semantic   Segmentation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-29
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                    æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                        <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-29/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-32f1968153fce730abef50ea70a5d71d.jpg" class="responsive-img" alt="è§†é¢‘ç†è§£">
                        
                        <span class="card-title">è§†é¢‘ç†è§£</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            è§†é¢‘ç†è§£ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-29  TinyLLaVA-Video A Simple Framework of Small-scale Large Multimodal   Models for Video Understanding
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-29
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                    è§†é¢‘ç†è§£
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                        <span class="chip bg-color">è§†é¢‘ç†è§£</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">18181.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
