<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Vision Transformer">
    <meta name="description" content="Vision Transformer 方向最新论文已更新，请持续关注 Update in 2025-01-29  CLISC Bridging clip and sam by enhanced cam for unsupervised brain   tumor segmentation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Vision Transformer | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-5ab129f2734ce488357d5baee79c5c4f.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Vision Transformer</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Vision-Transformer/">
                                <span class="chip bg-color">Vision Transformer</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                Vision Transformer
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-01-29
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-02-12
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    6.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    26 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-01-29-更新"><a href="#2025-01-29-更新" class="headerlink" title="2025-01-29 更新"></a>2025-01-29 更新</h1><h2 id="CLISC-Bridging-clip-and-sam-by-enhanced-cam-for-unsupervised-brain-tumor-segmentation"><a href="#CLISC-Bridging-clip-and-sam-by-enhanced-cam-for-unsupervised-brain-tumor-segmentation" class="headerlink" title="CLISC: Bridging clip and sam by enhanced cam for unsupervised brain   tumor segmentation"></a>CLISC: Bridging clip and sam by enhanced cam for unsupervised brain   tumor segmentation</h2><p><strong>Authors:Xiaochuan Ma, Jia Fu, Wenjun Liao, Shichuan Zhang, Guotai Wang</strong></p>
<p>Brain tumor segmentation is important for diagnosis of the tumor, and current deep-learning methods rely on a large set of annotated images for training, with high annotation costs. Unsupervised segmentation is promising to avoid human annotations while the performance is often limited. In this study, we present a novel unsupervised segmentation approach that leverages the capabilities of foundation models, and it consists of three main steps: (1) A vision-language model (i.e., CLIP) is employed to obtain image-level pseudo-labels for training a classification network. Class Activation Mapping (CAM) is then employed to extract Regions of Interest (ROIs), where an adaptive masking-based data augmentation is used to enhance ROI identification.(2) The ROIs are used to generate bounding box and point prompts for the Segment Anything Model (SAM) to obtain segmentation pseudo-labels. (3) A 3D segmentation network is trained with the SAM-derived pseudo-labels, where low-quality pseudo-labels are filtered out in a self-learning process based on the similarity between the SAM’s output and the network’s prediction. Evaluation on the BraTS2020 dataset demonstrates that our approach obtained an average Dice Similarity Score (DSC) of 85.60%, outperforming five state-of-the-art unsupervised segmentation methods by more than 10 percentage points. Besides, our approach outperforms directly using SAM for zero-shot inference, and its performance is close to fully supervised learning. </p>
<blockquote>
<p>脑肿瘤分割对于肿瘤诊断具有重要意义，当前深度学习方法依赖于大量标注图像进行训练，标注成本高昂。无监督分割方法具有避免人工标注的潜力，但性能往往受到限制。本研究提出了一种新型无监督分割方法，该方法利用基础模型的能力，主要包含三个步骤：（1）采用视觉语言模型（例如CLIP）获取用于训练分类网络的图像级伪标签。然后采用类激活映射（CAM）提取感兴趣区域（ROI），并使用自适应掩膜式数据增强来提高ROI识别。（2）使用ROI生成边界框和点提示，以供分段任何模型（SAM）获得分割伪标签。（3）使用SAM衍生的伪标签训练3D分割网络，在自我学习过程中过滤掉低质量的伪标签，该过程基于SAM输出与网络预测之间的相似性。在BraTS2020数据集上的评估表明，我们的方法获得了平均Dice相似度分数（DSC）为85.60%，比五种最先进的无监督分割方法的性能高出超过10个百分点。此外，我们的方法优于直接使用SAM进行零样本推理，其性能接近完全监督学习。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.16246v1">PDF</a> 22st IEEE International Symposium on Biomedical Imaging (ISBI 2025)</p>
<p><strong>Summary</strong><br>基于计算机视觉模型的无监督脑肿瘤分割方法取得了突破性进展。通过CLIP模型获取图像级别的伪标签训练分类网络，利用CAM提取感兴趣区域并进行自适应掩膜增强；再结合SAM模型生成边界框和点提示获得分割伪标签，最后通过自学习过程筛选出高质量伪标签用于训练三维分割网络。该技术在BraTS2020数据集上的平均Dice相似度达到85.6%，超越其他五种前沿无监督分割技术并接近全监督学习的性能。 </p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>本研究提出了一种新型无监督脑肿瘤分割方法，结合计算机视觉模型和自适应数据增强技术。</li>
<li>利用CLIP模型获取图像级别的伪标签进行训练分类网络，并通过CAM提取感兴趣区域。</li>
<li>利用SAM模型生成边界框和点提示以获取分割伪标签。</li>
<li>通过自学习过程筛选出高质量伪标签用于训练三维分割网络。</li>
<li>该方法在BraTS2020数据集上的性能卓越，平均Dice相似度达到85.6%。</li>
<li>此方法超越了五种最新的无监督分割技术，并接近全监督学习的性能表现。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.16246">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-907df3bb2692b386c97871893275f494.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a3f4b82c817075eaf7166e9e9614ad5c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7460c1714c8dca2e50cf7696ca2c1e2e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3b555dd087a3d3e830c559e175742566.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-38d0bb6f0bb475b33d941753e5a643ff.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="PDC-ViT-Source-Camera-Identification-using-Pixel-Difference-Convolution-and-Vision-Transformer"><a href="#PDC-ViT-Source-Camera-Identification-using-Pixel-Difference-Convolution-and-Vision-Transformer" class="headerlink" title="PDC-ViT : Source Camera Identification using Pixel Difference   Convolution and Vision Transformer"></a>PDC-ViT : Source Camera Identification using Pixel Difference   Convolution and Vision Transformer</h2><p><strong>Authors:Omar Elharrouss, Younes Akbari, Noor Almaadeed, Somaya Al-Maadeed, Fouad Khelifi, Ahmed Bouridane</strong></p>
<p>Source camera identification has emerged as a vital solution to unlock incidents involving critical cases like terrorism, violence, and other criminal activities. The ability to trace the origin of an image&#x2F;video can aid law enforcement agencies in gathering evidence and constructing the timeline of events. Moreover, identifying the owner of a certain device narrows down the area of search in a criminal investigation where smartphone devices are involved. This paper proposes a new pixel-based method for source camera identification, integrating Pixel Difference Convolution (PDC) with a Vision Transformer network (ViT), and named PDC-ViT. While the PDC acts as the backbone for feature extraction by exploiting Angular PDC (APDC) and Radial PDC (RPDC). These techniques enhance the capability to capture subtle variations in pixel information, which are crucial for distinguishing between different source cameras. The second part of the methodology focuses on classification, which is based on a Vision Transformer network. Unlike traditional methods that utilize image patches directly for training the classification network, the proposed approach uniquely inputs PDC features into the Vision Transformer network. To demonstrate the effectiveness of the PDC-ViT approach, it has been assessed on five different datasets, which include various image contents and video scenes. The method has also been compared with state-of-the-art source camera identification methods. Experimental results demonstrate the effectiveness and superiority of the proposed system in terms of accuracy and robustness when compared to its competitors. For example, our proposed PDC-ViT has achieved an accuracy of 94.30%, 84%, 94.22% and 92.29% using the Vision dataset, Daxing dataset, Socrates dataset and QUFVD dataset, respectively. </p>
<blockquote>
<p>源相机识别已作为一种关键解决方案，用于解决涉及恐怖主义、暴力和其他犯罪活动的关键案件。追踪图像&#x2F;视频的来源可以帮助执法机构收集证据和构建事件的时间线。此外，确定某一设备的所有者可以缩小涉及智能手机的刑事调查的搜索范围。本文提出了一种基于像素的源相机识别新方法，将像素差异卷积（PDC）与视觉转换器网络（ViT）相结合，名为PDC-ViT。其中，PDC作为特征提取的主干，利用角度PDC（APDC）和径向PDC（RPDC）。这些技术提高了捕捉像素信息中细微变化的能力，这对于区分不同源相机至关重要。该方法的第二部分侧重于分类，基于视觉转换器网络。与传统方法不同，传统方法直接使用图像补丁来训练分类网络，所提出的方法独特地将PDC特征输入到视觉转换器网络中。为了证明PDC-ViT方法的有效性，它已在五个不同的数据集上进行了评估，这些数据集包括各种图像内容和视频场景。该方法还与国家先进的源相机识别方法进行了比较。实验结果表明，与竞争对手相比，该系统在准确性和稳健性方面表现出有效性和优越性。例如，我们提出的PDC-ViT在Vision数据集、大兴数据集、苏格拉底数据集和QUFVD数据集上分别实现了94.30%、84%、94.22%和92.29%的准确率。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.16227v1">PDF</a> </p>
<p><strong>Summary</strong><br>该文本提出了一种新的基于像素的源相机识别方法，该方法结合像素差异卷积（PDC）和视觉转换器网络（ViT），被称为PDC-ViT。该方法通过利用角PDC（APDC）和径向PDC（RPDC）作为特征提取的后备，并创新地将PDC特征输入到视觉转换器网络中进行分类。实验结果表明，与现有源相机识别方法相比，该系统的准确性和鲁棒性更高。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>源相机识别在解决涉及恐怖主义、暴力和其他犯罪活动的关键案件中至关重要。</li>
<li>新的像素差异卷积（PDC）与视觉转换器网络（ViT）结合的PDC-ViT方法被提出用于源相机识别。</li>
<li>PDC特征提取技术通过利用角PDC（APDC）和径向PDC（RPDC）增强捕捉像素细微变化的能力，这对于区分不同源相机至关重要。</li>
<li>PDC-ViT的独特之处在于将PDC特征输入到视觉转换器网络中进行分类，不同于传统方法直接使用图像块进行训练。</li>
<li>PDC-ViT方法已在五个不同的数据集上进行了评估，包括各种图像内容和视频场景。</li>
<li>实验结果表明，与现有方法相比，PDC-ViT系统在准确性和鲁棒性方面表现出卓越的性能。</li>
<li>PDC-ViT在Vision数据集、大兴数据集、苏格拉底数据集和QUFVD数据集上的准确率分别为94.30%、84%、94.22%和92.29%。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.16227">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-afcfe5e652ac8c5f458b76a5ae9521a8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d3e5f0b1a5ad246680a26d73f0d93213.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7da299c1c487d2a45759b9a0852fecf2.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Leveraging-Video-Vision-Transformer-for-Alzheimer’s-Disease-Diagnosis-from-3D-Brain-MRI"><a href="#Leveraging-Video-Vision-Transformer-for-Alzheimer’s-Disease-Diagnosis-from-3D-Brain-MRI" class="headerlink" title="Leveraging Video Vision Transformer for Alzheimer’s Disease Diagnosis   from 3D Brain MRI"></a>Leveraging Video Vision Transformer for Alzheimer’s Disease Diagnosis   from 3D Brain MRI</h2><p><strong>Authors:Taymaz Akan, Sait Alp, Md. Shenuarin Bhuiyan, Elizabeth A. Disbrow, Steven A. Conrad, John A. Vanchiere, Christopher G. Kevil, Mohammad A. N. Bhuiyan</strong></p>
<p>Alzheimer’s disease (AD) is a neurodegenerative disorder affecting millions worldwide, necessitating early and accurate diagnosis for optimal patient management. In recent years, advancements in deep learning have shown remarkable potential in medical image analysis. Methods In this study, we present “ViTranZheimer,” an AD diagnosis approach which leverages video vision transformers to analyze 3D brain MRI data. By treating the 3D MRI volumes as videos, we exploit the temporal dependencies between slices to capture intricate structural relationships. The video vision transformer’s self-attention mechanisms enable the model to learn long-range dependencies and identify subtle patterns that may indicate AD progression. Our proposed deep learning framework seeks to enhance the accuracy and sensitivity of AD diagnosis, empowering clinicians with a tool for early detection and intervention. We validate the performance of the video vision transformer using the ADNI dataset and conduct comparative analyses with other relevant models. Results The proposed ViTranZheimer model is compared with two hybrid models, CNN-BiLSTM and ViT-BiLSTM. CNN-BiLSTM is the combination of a convolutional neural network (CNN) and a bidirectional long-short-term memory network (BiLSTM), while ViT-BiLSTM is the combination of a vision transformer (ViT) with BiLSTM. The accuracy levels achieved in the ViTranZheimer, CNN-BiLSTM, and ViT-BiLSTM models are 98.6%, 96.479%, and 97.465%, respectively. ViTranZheimer demonstrated the highest accuracy at 98.6%, outperforming other models in this evaluation metric, indicating its superior performance in this specific evaluation metric. Conclusion This research advances the understanding of applying deep learning techniques in neuroimaging and Alzheimer’s disease research, paving the way for earlier and less invasive clinical diagnosis. </p>
<blockquote>
<p>阿尔茨海默症（AD）是一种影响全球数百万人的神经退行性疾病，需要进行早期和准确的诊断以进行最佳的患者管理。近年来，深度学习在医学图像分析方面的进展显示出巨大的潜力。在此研究中，我们提出了“ViTranZheimer”的AD诊断方法，该方法利用视频视觉转换器（Video Vision Transformer）分析3D大脑MRI数据。通过将3D MRI体积视为视频，我们利用切片之间的时间依赖性来捕捉复杂结构关系。视频视觉转换器的自注意力机制使模型能够学习长期依赖性并识别可能指示AD进展的微妙模式。我们提出的深度学习框架旨在提高AD诊断的准确性和敏感性，为临床医生提供一种早期检测和干预的工具。我们使用ADNI数据集验证了视频视觉转换器的性能，并与其他相关模型进行了比较分析。结果：所提出的ViTranZheimer模型与两种混合模型（CNN-BiLSTM和ViT-BiLSTM）进行了比较。CNN-BiLSTM是卷积神经网络（CNN）和双向长短期记忆网络（BiLSTM）的组合，而ViT-BiLSTM是视觉转换器（ViT）与BiLSTM的组合。ViTranZheimer、CNN-BiLSTM和ViT-BiLSTM模型中实现的准确度分别为98.6%、96.479%和97.465%。ViTranZheimer在准确度方面表现出最高水平，达到98.6%，在此评估指标上优于其他模型，表明其在这一特定评估指标上的性能卓越。结论：该研究深化了深度学习技术在神经影像学和阿尔茨海默症研究中的应用，为更早且侵入性较小的临床诊断铺平了道路。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.15733v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该研究提出了一种名为“ViTranZheimer”的深度学习框架，用于利用视频视觉转换器分析三维脑MRI数据，以诊断阿尔茨海默病（AD）。该框架通过利用视频视觉转换器的自注意力机制，学习长期依赖关系并识别可能指示AD进展的微妙模式。该研究使用ADNI数据集验证了视频视觉转换器的性能，并与其它相关模型进行了比较分析。结果显示，ViTranZheimer模型的准确率为98.6%，优于CNN-BiLSTM和ViT-BiLSTM模型。此研究为早期诊断阿尔茨海默病提供了新的工具。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究提出了“ViTranZheimer”深度学习框架，用于阿尔茨海默病的诊断。</li>
<li>利用视频视觉转换器分析三维脑MRI数据。</li>
<li>视频视觉转换器的自注意力机制能学习长期依赖关系并识别AD进展的微妙模式。</li>
<li>使用ADNI数据集验证了视频视觉转换器的性能。</li>
<li>ViTranZheimer模型的诊断准确率达到了98.6%。</li>
<li>ViTranZheimer模型的性能优于CNN-BiLSTM和ViT-BiLSTM模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.15733">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-5ab129f2734ce488357d5baee79c5c4f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-44c98036fdbb06021f984cdd59f0366e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8cf19cdebca2773d6909270fd702ec58.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-21799bc0ac2ef129db7f0a03901be463.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="GeoGround-A-Unified-Large-Vision-Language-Model-for-Remote-Sensing-Visual-Grounding"><a href="#GeoGround-A-Unified-Large-Vision-Language-Model-for-Remote-Sensing-Visual-Grounding" class="headerlink" title="GeoGround: A Unified Large Vision-Language Model for Remote Sensing   Visual Grounding"></a>GeoGround: A Unified Large Vision-Language Model for Remote Sensing   Visual Grounding</h2><p><strong>Authors:Yue Zhou, Mengcheng Lan, Xiang Li, Yiping Ke, Xue Jiang, Litong Feng, Wayne Zhang</strong></p>
<p>Remote sensing (RS) visual grounding aims to use natural language expression to locate specific objects (in the form of the bounding box or segmentation mask) in RS images, enhancing human interaction with intelligent RS interpretation systems. Early research in this area was primarily based on horizontal bounding boxes (HBBs), but as more diverse RS datasets have become available, tasks involving oriented bounding boxes (OBBs) and segmentation masks have emerged. In practical applications, different targets require different grounding types: HBB can localize an object’s position, OBB provides its orientation, and mask depicts its shape. However, existing specialized methods are typically tailored to a single type of RS visual grounding task and are hard to generalize across tasks. In contrast, large vision-language models (VLMs) exhibit powerful multi-task learning capabilities but struggle to handle dense prediction tasks like segmentation. This paper proposes GeoGround, a novel framework that unifies support for HBB, OBB, and mask RS visual grounding tasks, allowing flexible output selection. Rather than customizing the architecture of VLM, our work aims to elegantly support pixel-level visual grounding output through the Text-Mask technique. We define prompt-assisted and geometry-guided learning to enhance consistency across different signals. To support model training, we present refGeo, a large-scale RS visual instruction-following dataset containing 161k image-text pairs. Experimental results show that GeoGround demonstrates strong performance across four RS visual grounding tasks, matching or surpassing the performance of specialized methods on multiple benchmarks. Code available at <a target="_blank" rel="noopener" href="https://github.com/zytx121/GeoGround">https://github.com/zytx121/GeoGround</a> </p>
<blockquote>
<p>遥感（RS）视觉定位旨在利用自然语言表达式在遥感图像中定位特定对象（以边界框或分割掩码的形式），增强人类与智能遥感解释系统的交互。早期的研究主要基于水平边界框（HBB），但随着更多遥感数据集的可用，出现了涉及定向边界框（OBB）和分割掩码的任务。在实际应用中，不同的目标需要不同类型的定位：HBB可以定位对象的位置，OBB提供其方向，而掩码描绘其形状。然而，现有的专用方法通常针对单一的遥感视觉定位任务，难以跨任务推广。相比之下，大型视觉语言模型（VLM）表现出强大的多任务学习能力，但在处理如分割之类的密集预测任务时却表现挣扎。本文提出了GeoGround，这是一个新颖的统一框架，支持HBB、OBB和掩码遥感视觉定位任务，允许灵活输出选择。我们的工作旨在通过文本掩码技术巧妙地支持像素级视觉定位输出，而不是定制VLM的架构。我们定义了提示辅助和几何引导学习以增强不同信号之间的一致性。为了支持模型训练，我们提供了refGeo，这是一个大规模的遥感视觉指令跟随数据集，包含16.1万张图像文本对。实验结果表明，GeoGround在四个遥感视觉定位任务上表现出强劲的性能，在多个基准测试中达到或超越了专用方法的表现。代码可访问 <a target="_blank" rel="noopener" href="https://github.com/zytx121/GeoGround">https://github.com/zytx121/GeoGround</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.11904v2">PDF</a> 25 pages, 19 figures</p>
<p><strong>Summary</strong></p>
<p>本文介绍了遥感视觉定位技术中的一项新研究，名为GeoGround。该技术旨在统一支持水平边界框（HBB）、定向边界框（OBB）和掩膜三种遥感视觉定位任务，允许灵活输出选择。该研究通过Text-Mask技术实现像素级视觉定位输出，并提出prompt辅助和几何引导学习以增强不同信号之间的一致性。为支持模型训练，研究团队还推出了大规模的遥感视觉指令数据集refGeo，包含16.1万张图像文本对。GeoGround在四个遥感视觉定位任务中表现出强大的性能，在多个基准测试中匹配或超越了专用方法的表现。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>遥感视觉定位技术（RS visual grounding）利用自然语言表达式在遥感图像中定位特定对象，增强人与智能遥感解释系统的交互。</li>
<li>早期研究主要基于水平边界框（HBB），但现在随着更丰富的遥感数据集的出现，涉及定向边界框（OBB）和掩膜的任务已兴起。</li>
<li>不同目标需要不同类型的定位方式：HBB可定位对象位置，OBB提供方向信息，掩膜描绘对象形状。</li>
<li>现有方法通常专为单一类型的遥感视觉定位任务设计，难以跨任务推广。</li>
<li>GeoGround框架统一支持HBB、OBB和掩膜遥感视觉定位任务，允许灵活输出选择。</li>
<li>通过Text-Mask技术实现像素级视觉定位输出。</li>
<li>GeoGround在多个基准测试中表现优异，匹配或超越专用方法的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.11904">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c0a8d14c9608dc99014dd31608012f72.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-255dbd687ee2aafa1c1782d9075a4890.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-87b54a4527bccf7f9abc6227f9b297c1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-92a30b11aa565ee3a9b81a76ae7350a3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4cb0c7d6901412ccce3cead3b4189f8c.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="QCS-Feature-Refining-from-Quadruplet-Cross-Similarity-for-Facial-Expression-Recognition"><a href="#QCS-Feature-Refining-from-Quadruplet-Cross-Similarity-for-Facial-Expression-Recognition" class="headerlink" title="QCS: Feature Refining from Quadruplet Cross Similarity for Facial   Expression Recognition"></a>QCS: Feature Refining from Quadruplet Cross Similarity for Facial   Expression Recognition</h2><p><strong>Authors:Chengpeng Wang, Li Chen, Lili Wang, Zhaofan Li, Xuebin Lv</strong></p>
<p>Facial expression recognition faces challenges where labeled significant features in datasets are mixed with unlabeled redundant ones. In this paper, we introduce Cross Similarity Attention (CSA) to mine richer intrinsic information from image pairs, overcoming a limitation when the Scaled Dot-Product Attention of ViT is directly applied to calculate the similarity between two different images. Based on CSA, we simultaneously minimize intra-class differences and maximize inter-class differences at the fine-grained feature level through interactions among multiple branches. Contrastive residual distillation is utilized to transfer the information learned in the cross module back to the base network. We ingeniously design a four-branch centrally symmetric network, named Quadruplet Cross Similarity (QCS), which alleviates gradient conflicts arising from the cross module and achieves balanced and stable training. It can adaptively extract discriminative features while isolating redundant ones. The cross-attention modules exist during training, and only one base branch is retained during inference, resulting in no increase in inference time. Extensive experiments show that our proposed method achieves state-of-the-art performance on several FER datasets. </p>
<blockquote>
<p>面部表情识别面临着数据集中的标记重要特征与未标记冗余特征混合的挑战。在本文中，我们引入交叉相似性注意力（CSA）来从图像对中提取更丰富的内在信息，克服了当直接使用ViT的缩放点积注意力计算两个不同图像之间的相似性时存在的局限性。基于CSA，我们通过多个分支之间的交互，在细粒度特征级别同时减小类内差异并最大化类间差异。利用对比残差蒸馏将跨模块中学习到的信息转回基础网络。我们巧妙地设计了一个四分支中心对称网络，命名为四元组交叉相似性（QCS），该网络缓解了跨模块产生的梯度冲突，实现了平衡稳定的训练。它可以自适应地提取判别特征，同时隔离冗余特征。在训练过程中存在交叉注意力模块，而在推理期间仅保留一个基础分支，因此不会增加推理时间。大量实验表明，我们提出的方法在多个面部表情识别数据集上达到了最新性能水平。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.01988v5">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文引入了一种名为Cross Similarity Attention（CSA）的技术，用于从图像对中挖掘更丰富的内在信息，解决了在面部表情识别中，数据集中标签重要的特征与未标签的冗余特征混合的问题。通过CSA，本文在细粒度特征层面同时减小了类内差异并放大了类间差异。此外，还巧妙地设计了名为Quadruplet Cross Similarity（QCS）的四分支中心对称网络，缓解了跨模块引起的梯度冲突，实现了平衡稳定的训练。该方法可在训练过程中自适应提取判别特征并隔离冗余特征，且推理时间并未增加。实验表明，该方法在多个面部表情识别数据集上达到了领先水平。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>引入Cross Similarity Attention（CSA）技术，从图像对中挖掘更丰富内在信息。</li>
<li>解决数据集中标签重要特征与未标签冗余特征混合的问题。</li>
<li>通过CSA技术，同时减小类内差异并放大类间差异。</li>
<li>采用对比残差蒸馏技术将跨模块学到的信息回馈到基础网络。</li>
<li>设计了四分支中心对称网络Quadruplet Cross Similarity（QCS）。</li>
<li>QCS网络缓解了跨模块引起的梯度冲突，实现了平衡稳定的训练。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.01988">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d8191ba90d8d41a5f3d41c89bbf2eb9d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2ae0a54bada1b956761cabf60c383b0a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5dff649372844a68dbfbe48588ed9e1d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aab64baa1b65a6e2937d30320b880d0d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-62f32a70ff09108a8a0086b72ee97d27.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4e6a2c0b2dc406b1acd0d458e03d78e1.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Random-Set-Neural-Networks-RS-NN"><a href="#Random-Set-Neural-Networks-RS-NN" class="headerlink" title="Random-Set Neural Networks (RS-NN)"></a>Random-Set Neural Networks (RS-NN)</h2><p><strong>Authors:Shireen Kudukkil Manchingal, Muhammad Mubashar, Kaizheng Wang, Keivan Shariatmadar, Fabio Cuzzolin</strong></p>
<p>Machine learning is increasingly deployed in safety-critical domains where erroneous predictions may lead to potentially catastrophic consequences, highlighting the need for learning systems to be aware of how confident they are in their own predictions: in other words, ‘to know when they do not know’. In this paper, we propose a novel Random-Set Neural Network (RS-NN) approach to classification which predicts belief functions (rather than classical probability vectors) over the class list using the mathematics of random sets, i.e., distributions over the collection of sets of classes. RS-NN encodes the ‘epistemic’ uncertainty induced by training sets that are insufficiently representative or limited in size via the size of the convex set of probability vectors associated with a predicted belief function. Our approach outperforms state-of-the-art Bayesian and Ensemble methods in terms of accuracy, uncertainty estimation and out-of-distribution (OoD) detection on multiple benchmarks (CIFAR-10 vs SVHN&#x2F;Intel-Image, MNIST vs FMNIST&#x2F;KMNIST, ImageNet vs ImageNet-O). RS-NN also scales up effectively to large-scale architectures (e.g. WideResNet-28-10, VGG16, Inception V3, EfficientNetB2 and ViT-Base-16), exhibits remarkable robustness to adversarial attacks and can provide statistical guarantees in a conformal learning setting. </p>
<blockquote>
<p>机器学习在安全性至关重要的领域得到了越来越广泛的应用，错误的预测可能导致灾难性的后果，这凸显了学习系统需要了解自己对预测的信心程度的重要性，换句话说，就是“要知道自己不知道”。在本文中，我们提出了一种新型的随机集神经网络（RS-NN）分类方法，该方法使用随机集的数学原理预测类列表上的信念函数（而不是传统的概率向量）。RS-NN通过预测的信念函数关联的凸概率向量集的大小，对由训练集引起的不充分代表性或有限大小的“主观”不确定性进行编码。我们的方法在多个基准测试（CIFAR-10与SVHN&#x2F;Intel-Image、MNIST与FMNIST&#x2F;KMNIST、ImageNet与ImageNet-O）上的准确性、不确定性估计和超出分布（OoD）检测方面均优于最先进的贝叶斯和集成方法。RS-NN还能有效地扩展到大规模架构（如WideResNet-28-10、VGG16、Inception V3、EfficientNetB2和ViT-Base-16），对对抗性攻击表现出惊人的稳健性，并在符合学习环境中提供统计保证。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05772v3">PDF</a> Published at ICLR 2025</p>
<p><strong>Summary</strong><br>     本论文提出了一种基于随机集神经网络（RS-NN）的分类方法，通过随机集数学理论预测类别信念函数而非传统概率向量。该方法通过训练集相关的不确定性和集合凸概率向量大小编码“知识不确定性”，并在多个基准测试中展现出优异的准确性、不确定性估计和超出分布检测性能。此外，RS-NN可有效扩展到大型架构，对抗攻击具有出色稳健性，并在合规学习环境中提供统计保障。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>本论文关注机器学习在关键安全领域中的预测置信度问题，提出了基于随机集神经网络（RS-NN）的分类方法。</li>
<li>RS-NN通过预测信念函数而非传统概率向量来评估模型预测的不确定性。</li>
<li>RS-NN通过编码训练集相关的不确定性，展现出色的性能，尤其在数据集不足或规模有限的情况下。</li>
<li>在多个基准测试中，RS-NN在准确性、不确定性估计和超出分布检测方面优于现有先进贝叶斯和集成方法。</li>
<li>RS-NN能有效扩展到大型架构，并且对抗攻击具有稳健性。</li>
<li>RS-NN可在合规学习环境中提供统计保障，为机器学习模型的可靠性和安全性提供保障。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2307.05772">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-77eec650234f37b302d62056705d07f0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f2fa83146a7ab9ee961d944241317d38.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fed6e6e4f62c8c05b4af93f4c86a49fc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-65f3b0f8e6003fabd55609f1a9c2cdd7.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-29/Vision%20Transformer/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-29/Vision%20Transformer/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Vision-Transformer/">
                                    <span class="chip bg-color">Vision Transformer</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-29/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-2a92bbd0d23754a71f03b16e8c088d0f.jpg" class="responsive-img" alt="检测/分割/跟踪">
                        
                        <span class="card-title">检测/分割/跟踪</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            检测/分割/跟踪 方向最新论文已更新，请持续关注 Update in 2025-01-29  A Training-free Synthetic Data Selection Method for Semantic   Segmentation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-29
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                    检测/分割/跟踪
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                        <span class="chip bg-color">检测/分割/跟踪</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-29/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-32f1968153fce730abef50ea70a5d71d.jpg" class="responsive-img" alt="视频理解">
                        
                        <span class="card-title">视频理解</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            视频理解 方向最新论文已更新，请持续关注 Update in 2025-01-29  TinyLLaVA-Video A Simple Framework of Small-scale Large Multimodal   Models for Video Understanding
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-29
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                    视频理解
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                        <span class="chip bg-color">视频理解</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">18181.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
