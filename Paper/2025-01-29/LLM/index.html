<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-29  RAPID Retrieval-Augmented Parallel Inference Drafting for Text-Based   Video Event Retrieval">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-2c75a7bea1840643d0720bc3bfbfb1ce.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-29
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-02-12
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    78 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-01-29-æ›´æ–°"><a href="#2025-01-29-æ›´æ–°" class="headerlink" title="2025-01-29 æ›´æ–°"></a>2025-01-29 æ›´æ–°</h1><h2 id="RAPID-Retrieval-Augmented-Parallel-Inference-Drafting-for-Text-Based-Video-Event-Retrieval"><a href="#RAPID-Retrieval-Augmented-Parallel-Inference-Drafting-for-Text-Based-Video-Event-Retrieval" class="headerlink" title="RAPID: Retrieval-Augmented Parallel Inference Drafting for Text-Based   Video Event Retrieval"></a>RAPID: Retrieval-Augmented Parallel Inference Drafting for Text-Based   Video Event Retrieval</h2><p><strong>Authors:Long Nguyen, Huy Nguyen, Bao Khuu, Huy Luu, Huy Le, Tuan Nguyen, Tho Quan</strong></p>
<p>Retrieving events from videos using text queries has become increasingly challenging due to the rapid growth of multimedia content. Existing methods for text-based video event retrieval often focus heavily on object-level descriptions, overlooking the crucial role of contextual information. This limitation is especially apparent when queries lack sufficient context, such as missing location details or ambiguous background elements. To address these challenges, we propose a novel system called RAPID (Retrieval-Augmented Parallel Inference Drafting), which leverages advancements in Large Language Models (LLMs) and prompt-based learning to semantically correct and enrich user queries with relevant contextual information. These enriched queries are then processed through parallel retrieval, followed by an evaluation step to select the most relevant results based on their alignment with the original query. Through extensive experiments on our custom-developed dataset, we demonstrate that RAPID significantly outperforms traditional retrieval methods, particularly for contextually incomplete queries. Our system was validated for both speed and accuracy through participation in the Ho Chi Minh City AI Challenge 2024, where it successfully retrieved events from over 300 hours of video. Further evaluation comparing RAPID with the baseline proposed by the competition organizers demonstrated its superior effectiveness, highlighting the strength and robustness of our approach. </p>
<blockquote>
<p>ä»è§†é¢‘ä¸­æ£€ç´¢äº‹ä»¶å¹¶ä½¿ç”¨æ–‡æœ¬æŸ¥è¯¢æ˜¯ä¸€é¡¹å› å¤šåª’ä½“å†…å®¹çš„å¿«é€Ÿå¢é•¿è€Œæ—¥ç›Šå…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚ç°æœ‰çš„åŸºäºæ–‡æœ¬çš„è§†é¢‘äº‹ä»¶æ£€ç´¢æ–¹æ³•é€šå¸¸è¿‡äºå…³æ³¨å¯¹è±¡çº§åˆ«çš„æè¿°ï¼Œè€Œå¿½ç•¥äº†ä¸Šä¸‹æ–‡ä¿¡æ¯çš„å…³é”®ä½œç”¨ã€‚å½“æŸ¥è¯¢ç¼ºä¹è¶³å¤Ÿçš„ä¸Šä¸‹æ–‡æ—¶ï¼Œè¿™ç§å±€é™æ€§å°¤ä¸ºæ˜æ˜¾ï¼Œä¾‹å¦‚ç¼ºå°‘åœ°ç‚¹ç»†èŠ‚æˆ–èƒŒæ™¯å…ƒç´ æ¨¡ç³Šã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºRAPIDï¼ˆåŸºäºæ£€ç´¢å¢å¼ºçš„å¹¶è¡Œæ¨ç†è‰ç¨¿ï¼‰çš„æ–°ç³»ç»Ÿï¼Œå®ƒåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’ŒåŸºäºæç¤ºçš„å­¦ä¹ æ¥è¯­ä¹‰åœ°ä¿®æ­£å’Œä¸°å¯Œç”¨æˆ·æŸ¥è¯¢çš„ç›¸å…³ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚è¿™äº›ä¸°å¯Œçš„æŸ¥è¯¢ç„¶åé€šè¿‡å¹¶è¡Œæ£€ç´¢è¿›è¡Œå¤„ç†ï¼Œæ¥ç€æ˜¯ä¸€ä¸ªè¯„ä¼°æ­¥éª¤ï¼Œæ ¹æ®å…¶ä¸åŸå§‹æŸ¥è¯¢çš„åŒ¹é…ç¨‹åº¦é€‰æ‹©æœ€ç›¸å…³çš„ç»“æœã€‚åœ¨æˆ‘ä»¬è‡ªå®šä¹‰å¼€å‘çš„æ•°æ®é›†ä¸Šè¿›è¡Œçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒRAPIDæ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ£€ç´¢æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯å¯¹äºä¸Šä¸‹æ–‡ä¸å®Œæ•´çš„æŸ¥è¯¢ã€‚æˆ‘ä»¬çš„ç³»ç»Ÿé€šè¿‡å‚ä¸2024å¹´èƒ¡å¿—æ˜å¸‚äººå·¥æ™ºèƒ½æŒ‘æˆ˜èµ›ï¼Œåœ¨é€Ÿåº¦å’Œå‡†ç¡®æ€§æ–¹é¢éƒ½å¾—åˆ°äº†éªŒè¯ï¼ŒæˆåŠŸåœ°ä»è¶…è¿‡300å°æ—¶çš„è§†é¢‘ä¸­æ£€ç´¢åˆ°äº†äº‹ä»¶ã€‚ä¸ç«èµ›ç»„ç»‡è€…æå‡ºçš„åŸºçº¿ç›¸æ¯”ï¼Œå¯¹RAPIDçš„è¿›ä¸€æ­¥è¯„ä¼°è¯æ˜äº†å…¶å“è¶Šçš„æœ‰æ•ˆæ€§ï¼Œçªå‡ºäº†æˆ‘ä»¬æ–¹æ³•çš„å®åŠ›å’Œç¨³å¥æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.16303v1">PDF</a> Under review at SoICTâ€™24</p>
<p><strong>Summary</strong><br>å¤šåª’ä½“å†…å®¹çš„è¿…é€Ÿå¢é•¿ä½¿å¾—åŸºäºæ–‡æœ¬æŸ¥è¯¢ä»è§†é¢‘ä¸­æ£€ç´¢äº‹ä»¶å˜å¾—è¶Šæ¥è¶Šå…·æœ‰æŒ‘æˆ˜æ€§ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€ä¾§é‡äºå¯¹è±¡çº§åˆ«çš„æè¿°ï¼Œå¿½ç•¥äº†ä¸Šä¸‹æ–‡ä¿¡æ¯çš„é‡è¦ä½œç”¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºRAPIDçš„æ–°ç³»ç»Ÿï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’ŒåŸºäºæç¤ºçš„å­¦ä¹ ï¼Œå¯¹ç”¨æˆ·çš„æŸ¥è¯¢è¿›è¡Œè¯­ä¹‰ä¸Šçš„ä¿®æ­£å’Œä¸°å¯Œï¼Œå¹¶æ·»åŠ ç›¸å…³çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚é€šè¿‡æˆ‘ä»¬è‡ªè¡Œå¼€å‘çš„å®šåˆ¶æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒï¼Œè¯æ˜äº†RAPIDç³»ç»Ÿåœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ£€ç´¢æ–¹æ³•ï¼Œå°¤å…¶æ˜¯å¯¹äºä¸Šä¸‹æ–‡ç¼ºå¤±çš„æŸ¥è¯¢ã€‚æˆ‘ä»¬å·²åœ¨èƒ¡å¿—æ˜å¸‚äººå·¥æ™ºèƒ½æŒ‘æˆ˜ä¸­éªŒè¯å…¶é€Ÿåº¦å’Œå‡†ç¡®æ€§ã€‚å¯¹æ¯”ä¸»åŠæ–¹æå‡ºçš„åŸºçº¿æ¨¡å‹ï¼Œæˆ‘ä»¬å±•ç¤ºäº†è‡ªå·±çš„ä¼˜åŠ¿åŠç¨³å¥æ€§ã€‚ </p>
<p><strong>Key Takeaways</strong> </p>
<ol>
<li>é¢å¯¹å¤šåª’ä½“å†…å®¹çš„è¿…é€Ÿå¢é•¿ï¼Œä»è§†é¢‘ä¸­åŸºäºæ–‡æœ¬æŸ¥è¯¢æ£€ç´¢äº‹ä»¶å˜å¾—æ›´åŠ å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ </li>
<li>ä¼ ç»Ÿæ–¹æ³•ä¸»è¦å…³æ³¨å¯¹è±¡çº§åˆ«çš„æè¿°ï¼Œå¿½ç•¥äº†ä¸Šä¸‹æ–‡ä¿¡æ¯çš„é‡è¦æ€§ã€‚ </li>
<li>RAPIDç³»ç»Ÿé€šè¿‡åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å’ŒåŸºäºæç¤ºçš„å­¦ä¹ ï¼Œä¸°å¯Œç”¨æˆ·æŸ¥è¯¢çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚ </li>
<li>RAPIDåœ¨å¤„ç†å’Œè§£å†³ä¸Šä¸‹æ–‡ç¼ºå¤±çš„æŸ¥è¯¢æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚ </li>
<li>RAPIDç³»ç»Ÿå·²åœ¨å®šåˆ¶çš„åŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›å®éªŒéªŒè¯ï¼Œè¯æ˜å…¶æ€§èƒ½ä¼˜äºä¼ ç»Ÿæ£€ç´¢æ–¹æ³•ã€‚ </li>
<li>RAPIDåœ¨èƒ¡å¿—æ˜å¸‚äººå·¥æ™ºèƒ½æŒ‘æˆ˜ä¸­æˆåŠŸéªŒè¯äº†å…¶é€Ÿåº¦å’Œå‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.16303">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-895102064a78db18d398f05159fd7c6f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6a2edfd2b6e487bef9b6a5ebb677b704.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Matryoshka-Re-Ranker-A-Flexible-Re-Ranking-Architecture-With-Configurable-Depth-and-Width"><a href="#Matryoshka-Re-Ranker-A-Flexible-Re-Ranking-Architecture-With-Configurable-Depth-and-Width" class="headerlink" title="Matryoshka Re-Ranker: A Flexible Re-Ranking Architecture With   Configurable Depth and Width"></a>Matryoshka Re-Ranker: A Flexible Re-Ranking Architecture With   Configurable Depth and Width</h2><p><strong>Authors:Zheng Liu, Chaofan Li, Shitao Xiao, Chaozhuo Li, Defu Lian, Yingxia Shao</strong></p>
<p>Large language models (LLMs) provide powerful foundations to perform fine-grained text re-ranking. However, they are often prohibitive in reality due to constraints on computation bandwidth. In this work, we propose a \textbf{flexible} architecture called \textbf{Matroyshka Re-Ranker}, which is designed to facilitate \textbf{runtime customization} of model layers and sequence lengths at each layer based on usersâ€™ configurations. Consequently, the LLM-based re-rankers can be made applicable across various real-world situations. The increased flexibility may come at the cost of precision loss. To address this problem, we introduce a suite of techniques to optimize the performance. First, we propose \textbf{cascaded self-distillation}, where each sub-architecture learns to preserve a precise re-ranking performance from its super components, whose predictions can be exploited as smooth and informative teacher signals. Second, we design a \textbf{factorized compensation mechanism}, where two collaborative Low-Rank Adaptation modules, vertical and horizontal, are jointly employed to compensate for the precision loss resulted from arbitrary combinations of layer and sequence compression. We perform comprehensive experiments based on the passage and document retrieval datasets from MSMARCO, along with all public datasets from BEIR benchmark. In our experiments, Matryoshka Re-Ranker substantially outperforms the existing methods, while effectively preserving its superior performance across various forms of compression and different application scenarios. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸ºè¿›è¡Œç²¾ç»†æ–‡æœ¬é‡æ–°æ’åºæä¾›äº†å¼ºå¤§çš„åŸºç¡€ã€‚ç„¶è€Œï¼Œç”±äºè®¡ç®—å¸¦å®½çš„é™åˆ¶ï¼Œå®ƒä»¬åœ¨ç°å®ä¸­å¾€å¾€å—åˆ°é˜»ç¢ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºâ€œMatroyshka Re-Rankerâ€çš„çµæ´»æ¶æ„ï¼Œæ—¨åœ¨æ ¹æ®ç”¨æˆ·çš„é…ç½®ï¼Œåœ¨è¿è¡Œæ—¶å®šåˆ¶æ¨¡å‹å±‚å’Œæ¯å±‚çš„åºåˆ—é•¿åº¦ã€‚å› æ­¤ï¼ŒåŸºäºLLMçš„é‡æ–°æ’åºå™¨å¯åº”ç”¨äºå„ç§ç°å®æƒ…å†µã€‚çµæ´»æ€§çš„å¢åŠ å¯èƒ½ä¼šä»¥ç²¾åº¦æŸå¤±ä¸ºä»£ä»·ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç³»åˆ—æŠ€æœ¯æ¥ä¼˜åŒ–æ€§èƒ½ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æå‡ºäº†çº§è”è‡ªè’¸é¦æŠ€æœ¯ï¼Œå…¶ä¸­æ¯ä¸ªå­æ¶æ„å­¦ä¹ ä»å…¶é«˜çº§ç»„ä»¶ä¸­ä¿ç•™ç²¾ç¡®çš„é‡æ–°æ’åºæ€§èƒ½ï¼Œè¿™äº›é¢„æµ‹ç»“æœå¯ä»¥è¢«ç”¨ä½œå¹³æ»‘ä¸”ä¿¡æ¯ä¸°å¯Œçš„æ•™å¸ˆä¿¡å·ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§åˆ†è§£è¡¥å¿æœºåˆ¶ï¼Œå…¶ä¸­ä¸¤ä¸ªåä½œçš„ä½ç§©é€‚åº”æ¨¡å—ï¼ˆå‚ç›´å’Œæ°´å¹³ï¼‰è¢«è”åˆç”¨äºè¡¥å¿ç”±äºä»»æ„ç»„åˆçš„å±‚å’Œåºåˆ—å‹ç¼©è€Œäº§ç”Ÿçš„ç²¾åº¦æŸå¤±ã€‚æˆ‘ä»¬åœ¨MSMARCOçš„æ®µè½å’Œæ–‡æ¡£æ£€ç´¢æ•°æ®é›†ä»¥åŠBEIRåŸºå‡†æµ‹è¯•çš„æ‰€æœ‰å…¬å…±æ•°æ®é›†ä¸Šè¿›è¡Œäº†å…¨é¢çš„å®éªŒã€‚åœ¨æˆ‘ä»¬çš„å®éªŒä¸­ï¼ŒMatroyshka Re-Rankeræ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒåŒæ—¶åœ¨å„ç§å½¢å¼çš„å‹ç¼©å’Œä¸åŒçš„åº”ç”¨åœºæ™¯ä¸­æœ‰æ•ˆåœ°ä¿æŒäº†å…¶å“è¶Šçš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.16302v1">PDF</a> The Web Conference 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªçµæ´»çš„æ¶æ„Matroyshka Re-Rankerï¼Œå®ƒèƒ½å¤Ÿåœ¨è¿è¡Œæ—¶æ ¹æ®ç”¨æˆ·éœ€æ±‚å®šåˆ¶æ¨¡å‹å±‚å’Œåºåˆ—é•¿åº¦ï¼Œä½¿å¾—å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ›´é€‚ç”¨äºå„ç§å®é™…æƒ…å†µã€‚ä¸ºè§£å†³çµæ´»æ€§å¯èƒ½å¸¦æ¥çš„ç²¾åº¦æŸå¤±é—®é¢˜ï¼Œæ–‡ç« å¼•å…¥äº†ä¸€ç³»åˆ—ä¼˜åŒ–æŠ€æœ¯ï¼ŒåŒ…æ‹¬çº§è”è‡ªè’¸é¦å’Œå› å­åŒ–è¡¥å¿æœºåˆ¶ã€‚å®éªŒè¡¨æ˜ï¼ŒMatryoshka Re-Rankeråœ¨å‹ç¼©å’Œå„ç§åº”ç”¨åœºæ™¯ä¸‹å‡ä¿æŒäº†å‡ºè‰²çš„æ€§èƒ½ï¼Œå¹¶æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Matroyshka Re-Rankeræ¶æ„æä¾›äº†çµæ´»çš„å®šåˆ¶é€‰é¡¹ï¼Œå…è®¸æ ¹æ®ç”¨æˆ·éœ€æ±‚è°ƒæ•´æ¨¡å‹å±‚å’Œåºåˆ—é•¿åº¦ã€‚</li>
<li>çµæ´»æ€§å¯èƒ½ä¼šå¸¦æ¥ç²¾åº¦æŸå¤±ã€‚</li>
<li>ä¸ºä¼˜åŒ–æ€§èƒ½ï¼Œå¼•å…¥äº†çº§è”è‡ªè’¸é¦æŠ€æœ¯ï¼Œä½¿å­æ¶æ„èƒ½å¤Ÿä¿æŒç²¾ç¡®çš„é‡æ’æ€§èƒ½ã€‚</li>
<li>å› å­åŒ–è¡¥å¿æœºåˆ¶é€šè¿‡ä¸¤ä¸ªåä½œçš„ä½ç§©é€‚åº”æ¨¡å—ï¼ˆå‚ç›´å’Œæ°´å¹³ï¼‰æ¥è¡¥å¿å› å±‚å’Œåºåˆ—å‹ç¼©è€Œäº§ç”Ÿçš„ç²¾åº¦æŸå¤±ã€‚</li>
<li>Matroyshka Re-Rankeråœ¨MSMARCOçš„passageå’Œæ–‡æ¡£æ£€ç´¢æ•°æ®é›†ä»¥åŠBEIRåŸºå‡†æµ‹è¯•çš„æ‰€æœ‰å…¬å…±æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒMatroyshka Re-Rankeråœ¨å¤šç§å‹ç¼©å½¢å¼å’Œå„ç§åº”ç”¨åœºæ™¯ä¸‹å‡æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.16302">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-940a23f487a4fde164c25c0d9e461ac6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0b1d48c40b95792b076b8dbd9f8f0f72.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ecffdbadab369a4c604f8caf13953fca.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-87fdfbe627e6ebb4abba7b7aa4dd1142.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="URAG-Implementing-a-Unified-Hybrid-RAG-for-Precise-Answers-in-University-Admission-Chatbots-â€“-A-Case-Study-at-HCMUT"><a href="#URAG-Implementing-a-Unified-Hybrid-RAG-for-Precise-Answers-in-University-Admission-Chatbots-â€“-A-Case-Study-at-HCMUT" class="headerlink" title="URAG: Implementing a Unified Hybrid RAG for Precise Answers in   University Admission Chatbots â€“ A Case Study at HCMUT"></a>URAG: Implementing a Unified Hybrid RAG for Precise Answers in   University Admission Chatbots â€“ A Case Study at HCMUT</h2><p><strong>Authors:Long Nguyen, Tho Quan</strong></p>
<p>With the rapid advancement of Artificial Intelligence, particularly in Natural Language Processing, Large Language Models (LLMs) have become pivotal in educational question-answering systems, especially university admission chatbots. Concepts such as Retrieval-Augmented Generation (RAG) and other advanced techniques have been developed to enhance these systems by integrating specific university data, enabling LLMs to provide informed responses on admissions and academic counseling. However, these enhanced RAG techniques often involve high operational costs and require the training of complex, specialized modules, which poses challenges for practical deployment. Additionally, in the educational context, it is crucial to provide accurate answers to prevent misinformation, a task that LLM-based systems find challenging without appropriate strategies and methods. In this paper, we introduce the Unified RAG (URAG) Framework, a hybrid approach that significantly improves the accuracy of responses, particularly for critical queries. Experimental results demonstrate that URAG enhances our in-house, lightweight model to perform comparably to state-of-the-art commercial models. Moreover, to validate its practical applicability, we conducted a case study at our educational institution, which received positive feedback and acclaim. This study not only proves the effectiveness of URAG but also highlights its feasibility for real-world implementation in educational settings. </p>
<blockquote>
<p>éšç€äººå·¥æ™ºèƒ½çš„å¿«é€Ÿè¿›æ­¥ï¼Œç‰¹åˆ«æ˜¯åœ¨è‡ªç„¶è¯­è¨€å¤„ç†æ–¹é¢ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ•™è‚²é—®ç­”ç³»ç»Ÿï¼Œå°¤å…¶æ˜¯å¤§å­¦æ‹›ç”ŸèŠå¤©æœºå™¨äººä¸­ï¼Œèµ·åˆ°äº†è‡³å…³é‡è¦çš„ä½œç”¨ã€‚è¯¸å¦‚æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç­‰å…¶ä»–å…ˆè¿›æŠ€æœ¯æ¦‚å¿µçš„å‘å±•ï¼Œé€šè¿‡æ•´åˆç‰¹å®šå¤§å­¦æ•°æ®ï¼Œå¢å¼ºäº†è¿™äº›ç³»ç»Ÿçš„åŠŸèƒ½ï¼Œä½¿LLMèƒ½å¤Ÿæä¾›æœ‰å…³æ‹›ç”Ÿå’Œå­¦æœ¯å’¨è¯¢çš„ä¿¡æ¯å›å¤ã€‚ç„¶è€Œï¼Œè¿™äº›å¢å¼ºçš„RAGæŠ€æœ¯é€šå¸¸æ¶‰åŠè¾ƒé«˜çš„æ“ä½œæˆæœ¬ï¼Œå¹¶ä¸”éœ€è¦è®­ç»ƒå¤æ‚çš„ä¸“ä¸šæ¨¡å—ï¼Œè¿™ç»™å®é™…éƒ¨ç½²å¸¦æ¥äº†æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œåœ¨æ•™è‚²èƒŒæ™¯ä¸‹ï¼Œä¸ºäº†é¢„é˜²é”™ä¿¡ï¼Œæä¾›å‡†ç¡®ç­”æ¡ˆè‡³å…³é‡è¦ï¼Œè€ŒLLMç³»ç»Ÿåœ¨æ²¡æœ‰é€‚å½“çš„ç­–ç•¥å’Œæ–¹æ³•çš„æƒ…å†µä¸‹ï¼Œå‘ç°è¿™æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ç»Ÿä¸€RAGï¼ˆURAGï¼‰æ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ç§æ··åˆæ–¹æ³•ï¼Œå¯ä»¥æ˜¾è‘—æé«˜å“åº”çš„å‡†ç¡®æ€§ï¼Œç‰¹åˆ«æ˜¯å¯¹äºå…³é”®æŸ¥è¯¢ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒURAGå¯ä»¥å¢å¼ºæˆ‘ä»¬å†…éƒ¨çš„è½»é‡çº§æ¨¡å‹ï¼Œä½¿å…¶æ€§èƒ½ä¸æœ€å…ˆè¿›çš„å•†ä¸šæ¨¡å‹ç›¸å½“ã€‚æ­¤å¤–ï¼Œä¸ºäº†éªŒè¯å…¶å®ç”¨æ€§ï¼Œæˆ‘ä»¬åœ¨æ•™è‚²æœºæ„è¿›è¡Œäº†æ¡ˆä¾‹ç ”ç©¶ï¼Œå¹¶è·å¾—äº†ç§¯æçš„åé¦ˆå’Œèµèª‰ã€‚è¿™é¡¹ç ”ç©¶ä¸ä»…è¯æ˜äº†URAGçš„æœ‰æ•ˆæ€§ï¼Œè€Œä¸”å¼ºè°ƒäº†å…¶åœ¨æ•™è‚²ç¯å¢ƒä¸­å®é™…åº”ç”¨çš„å¯è¡Œæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.16276v1">PDF</a> Under review at SoICTâ€™24</p>
<p><strong>Summary</strong>ï¼šéšç€äººå·¥æ™ºèƒ½çš„å¿«é€Ÿå‘å±•ï¼Œç‰¹åˆ«æ˜¯åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ•™è‚²é—®ç­”ç³»ç»Ÿä¸­æ‰®æ¼”ç€è‡³å…³é‡è¦çš„è§’è‰²ï¼Œå¦‚å¤§å­¦æ‹›ç”ŸèŠå¤©æœºå™¨äººã€‚ä¸ºæé«˜ç³»ç»Ÿçš„æ€§èƒ½ï¼Œç ”ç©¶è€…å¼€å‘äº†è¯¸å¦‚æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç­‰å…ˆè¿›æŠ€æœ¯ï¼Œé›†æˆç‰¹å®šå¤§å­¦æ•°æ®ï¼Œä½¿LLMèƒ½å¤Ÿæä¾›æœ‰å…³æ‹›ç”Ÿå’Œå­¦æœ¯å’¨è¯¢çš„ä¿¡æ¯å›å¤ã€‚ç„¶è€Œï¼ŒRAGæŠ€æœ¯çš„é«˜è¿è¥æˆæœ¬å’Œå¯¹å¤æ‚ä¸“ä¸šæ¨¡å—çš„è¿«åˆ‡éœ€æ±‚ï¼Œä¸ºå…¶å®é™…åº”ç”¨å¸¦æ¥äº†æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶ä»‹ç»äº†ä¸€ç§æ··åˆæ–¹æ³•â€”â€”ç»Ÿä¸€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆURAGï¼‰æ¡†æ¶ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æé«˜äº†å“åº”çš„å‡†ç¡®æ€§ï¼Œå°¤å…¶æ˜¯å¯¹å…³é”®æŸ¥è¯¢çš„å“åº”ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒURAGèƒ½å¤Ÿå¢å¼ºæˆ‘ä»¬å†…éƒ¨çš„è½»é‡çº§æ¨¡å‹ï¼Œä½¿å…¶è¡¨ç°å ªæ¯”æœ€å…ˆè¿›çš„å•†ä¸šæ¨¡å‹ã€‚åœ¨æ•™è‚²æœºæ„è¿›è¡Œçš„ç ”ç©¶æ¡ˆä¾‹è·å¾—äº†ç§¯æçš„åé¦ˆå’Œèµèª‰ï¼Œè¯æ˜äº†URAGçš„æœ‰æ•ˆæ€§å’Œç°å®åº”ç”¨ä¸­çš„å¯è¡Œæ€§ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ•™è‚²é—®ç­”ç³»ç»Ÿä¸­æ‰®æ¼”é‡è¦è§’è‰²ï¼Œå¦‚å¤§å­¦æ‹›ç”ŸèŠå¤©æœºå™¨äººã€‚</li>
<li>æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯ä¸ºæé«˜æ•™è‚²é—®ç­”ç³»ç»Ÿçš„æ€§èƒ½èµ·åˆ°äº†å…³é”®ä½œç”¨ã€‚</li>
<li>RAGæŠ€æœ¯é¢ä¸´é«˜è¿è¥æˆæœ¬å’Œå¯¹å¤æ‚ä¸“ä¸šæ¨¡å—çš„è¿«åˆ‡éœ€æ±‚ç­‰æŒ‘æˆ˜ã€‚</li>
<li>ç»Ÿä¸€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆURAGï¼‰æ¡†æ¶æ˜¯ä¸€ç§æ··åˆæ–¹æ³•ï¼Œæ—¨åœ¨æé«˜LLMåœ¨æ•™è‚²é¢†åŸŸçš„å“åº”å‡†ç¡®æ€§ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒURAGèƒ½å¤Ÿå¢å¼ºè½»é‡çº§æ¨¡å‹çš„è¡¨ç°ï¼Œä½¿å…¶ä¸æœ€å…ˆè¿›çš„å•†ä¸šæ¨¡å‹ç›¸åª²ç¾ã€‚</li>
<li>åœ¨æ•™è‚²æœºæ„è¿›è¡Œçš„ç ”ç©¶æ¡ˆä¾‹è¯æ˜äº†URAGçš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.16276">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b7efecfdd53b1a6a9076a2883a685dab.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7718d8aa520ab7cc130bceb32bfa5b2e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2c75a7bea1840643d0720bc3bfbfb1ce.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c41176ecc54c264bdaa2b44f0efcc3d4.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Multi-Agent-Geospatial-Copilots-for-Remote-Sensing-Workflows"><a href="#Multi-Agent-Geospatial-Copilots-for-Remote-Sensing-Workflows" class="headerlink" title="Multi-Agent Geospatial Copilots for Remote Sensing Workflows"></a>Multi-Agent Geospatial Copilots for Remote Sensing Workflows</h2><p><strong>Authors:Chaehong Lee, Varatheepan Paramanayakam, Andreas Karatzas, Yanan Jian, Michael Fore, Heming Liao, Fuxun Yu, Ruopu Li, Iraklis Anagnostopoulos, Dimitrios Stamoulis</strong></p>
<p>We present GeoLLM-Squad, a geospatial Copilot that introduces the novel multi-agent paradigm to remote sensing (RS) workflows. Unlike existing single-agent approaches that rely on monolithic large language models (LLM), GeoLLM-Squad separates agentic orchestration from geospatial task-solving, by delegating RS tasks to specialized sub-agents. Built on the open-source AutoGen and GeoLLM-Engine frameworks, our work enables the modular integration of diverse applications, spanning urban monitoring, forestry protection, climate analysis, and agriculture studies. Our results demonstrate that while single-agent systems struggle to scale with increasing RS task complexity, GeoLLM-Squad maintains robust performance, achieving a 17% improvement in agentic correctness over state-of-the-art baselines. Our findings highlight the potential of multi-agent AI in advancing RS workflows. </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºäº†GeoLLM-Squadï¼Œè¿™æ˜¯ä¸€ä¸ªåœ°ç†ç©ºé—´Copilotï¼Œå®ƒä¸ºé¥æ„Ÿï¼ˆRSï¼‰å·¥ä½œæµç¨‹å¼•å…¥äº†æ–°å‹çš„å¤šæ™ºèƒ½ä½“èŒƒå¼ã€‚ä¸ä¾èµ–å•ä¸€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç°æœ‰å•æ™ºèƒ½ä½“æ–¹æ³•ä¸åŒï¼ŒGeoLLM-Squadé€šè¿‡å°†é¥æ„Ÿä»»åŠ¡å§”æ´¾ç»™ä¸“ä¸šå­æ™ºèƒ½ä½“ï¼Œå®ç°äº†æ™ºèƒ½ç¼–æ’ä¸åœ°ç†ç©ºé—´ä»»åŠ¡è§£å†³çš„åˆ†ç¦»ã€‚æˆ‘ä»¬çš„å·¥ä½œå»ºç«‹åœ¨å¼€æºçš„AutoGenå’ŒGeoLLM-Engineæ¡†æ¶ä¹‹ä¸Šï¼Œèƒ½å¤Ÿå®ç°æ¨¡å—åŒ–é›†æˆå„ç§åº”ç”¨ï¼Œæ¶µç›–åŸå¸‚ç›‘æµ‹ã€æ—ä¸šä¿æŠ¤ã€æ°”å€™åˆ†æå’Œå†œä¸šç ”ç©¶ç­‰é¢†åŸŸã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œè™½ç„¶å•æ™ºèƒ½ä½“ç³»ç»Ÿåœ¨å¤„ç†æ—¥ç›Šå¤æ‚çš„é¥æ„Ÿä»»åŠ¡æ—¶éš¾ä»¥æ‰©å±•ï¼Œä½†GeoLLM-Squadèƒ½å¤Ÿä¿æŒç¨³å¥çš„æ€§èƒ½ï¼Œåœ¨æ™ºèƒ½æ­£ç¡®æ€§æ–¹é¢æ¯”æœ€æ–°åŸºçº¿æé«˜äº†17%ã€‚æˆ‘ä»¬çš„ç ”ç©¶çªå‡ºäº†å¤šæ™ºèƒ½ä½“äººå·¥æ™ºèƒ½åœ¨æ¨åŠ¨é¥æ„Ÿå·¥ä½œæµç¨‹æ–¹é¢çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.16254v1">PDF</a> </p>
<p><strong>Summary</strong><br>åŸºäºå¤šæ™ºèƒ½ä½“èŒƒå¼æå‡ºçš„GeoLLM-Squadç³»ç»Ÿï¼Œå¼•å…¥æ–°å‹è¿œç¨‹é¥æ„Ÿå·¥ä½œæµç¨‹ã€‚ä¸åŒäºä¾èµ–å•ä¸€å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¼ ç»Ÿæ–¹æ³•ï¼ŒGeoLLM-Squadé€šè¿‡ä¸“ä¸šå­æ™ºèƒ½ä½“å®Œæˆé¥æ„Ÿä»»åŠ¡ï¼Œå®ç°æ™ºèƒ½ä½“ç¼–æ’ä¸åœ°ç†ç©ºé—´ä»»åŠ¡è§£å†³çš„åˆ†ç¦»ã€‚å€ŸåŠ©å¼€æºçš„AutoGenå’ŒGeoLLM-Engineæ¡†æ¶ï¼Œè¯¥ç³»ç»Ÿæ”¯æŒæ¨¡å—åŒ–é›†æˆå¤šç§åº”ç”¨ï¼Œå¦‚åŸå¸‚ç›‘æµ‹ã€æ—ä¸šä¿æŠ¤ã€æ°”å€™åˆ†æå’Œå†œä¸šç ”ç©¶ç­‰ã€‚åœ¨å•æ™ºèƒ½ä½“ç³»ç»Ÿéš¾ä»¥åº”å¯¹æ—¥ç›Šå¤æ‚çš„é¥æ„Ÿä»»åŠ¡æ—¶ï¼ŒGeoLLM-Squadå±•ç°äº†ç¨³å¥æ€§èƒ½ï¼Œå¹¶åœ¨æœ€æ–°åŸºå‡†æµ‹è¯•ä¸­æé«˜äº†æ™ºèƒ½ä½“æ­£ç¡®æ€§æ–¹é¢çš„è¡¨ç°ã€‚ç ”ç©¶å‡¸æ˜¾äº†å¤šæ™ºèƒ½ä½“äººå·¥æ™ºèƒ½åœ¨æ¨åŠ¨é¥æ„Ÿå·¥ä½œæµç¨‹å‘å±•æ–¹é¢çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GeoLLM-Squadæ˜¯ä¸€ä¸ªåŸºäºå¤šæ™ºèƒ½ä½“èŒƒå¼çš„åœ°ç†ç©ºé—´Copilotç³»ç»Ÿï¼Œç”¨äºå¤„ç†é¥æ„Ÿä»»åŠ¡ã€‚</li>
<li>ä¸ä¼ ç»Ÿä¾èµ–å•ä¸€å¤§å‹è¯­è¨€æ¨¡å‹çš„è¿œç¨‹æ„Ÿåº”æ–¹æ³•ä¸åŒï¼ŒGeoLLM-Squadé‡‡ç”¨ä¸“ä¸šå­æ™ºèƒ½ä½“æ¥å®Œæˆä»»åŠ¡ã€‚</li>
<li>GeoLLM-Squadå®ç°äº†æ™ºèƒ½ä½“ç¼–æ’ä¸åœ°ç†ç©ºé—´ä»»åŠ¡è§£å†³çš„åˆ†ç¦»ã€‚</li>
<li>è¯¥ç³»ç»ŸåŸºäºå¼€æºçš„AutoGenå’ŒGeoLLM-Engineæ¡†æ¶æ„å»ºï¼Œæ”¯æŒå¤šç§æ¨¡å—åŒ–é›†æˆåº”ç”¨ã€‚</li>
<li>ä¸å•æ™ºèƒ½ä½“ç³»ç»Ÿç›¸æ¯”ï¼ŒGeoLLM-Squadåœ¨å¤æ‚çš„é¥æ„Ÿä»»åŠ¡ä¸­å±•ç°äº†ç¨³å¥æ€§èƒ½ã€‚</li>
<li>GeoLLM-Squadåœ¨æœ€æ–°åŸºå‡†æµ‹è¯•ä¸­æé«˜äº†æ™ºèƒ½ä½“çš„æ­£ç¡®æ€§è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.16254">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-70f2a39df2b44462be9818db93127572.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fd7b06e0688daeaf387436ca9db10e9a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ea6d2e284e3c320ffd00fc4df32c7dd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a77d32e96d7e54f66dd1270080346ba7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-51725de2da9640223f00aa98770926d7.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Enhancing-Visual-Inspection-Capability-of-Multi-Modal-Large-Language-Models-on-Medical-Time-Series-with-Supportive-Conformalized-and-Interpretable-Small-Specialized-Models"><a href="#Enhancing-Visual-Inspection-Capability-of-Multi-Modal-Large-Language-Models-on-Medical-Time-Series-with-Supportive-Conformalized-and-Interpretable-Small-Specialized-Models" class="headerlink" title="Enhancing Visual Inspection Capability of Multi-Modal Large Language   Models on Medical Time Series with Supportive Conformalized and Interpretable   Small Specialized Models"></a>Enhancing Visual Inspection Capability of Multi-Modal Large Language   Models on Medical Time Series with Supportive Conformalized and Interpretable   Small Specialized Models</h2><p><strong>Authors:Huayu Li, Xiwen Chen, Ci Zhang, Stuart F. Quan, William D. S. Killgore, Shu-Fen Wung, Chen X. Chen, Geng Yuan, Jin Lu, Ao Li</strong></p>
<p>Large language models (LLMs) exhibit remarkable capabilities in visual inspection of medical time-series data, achieving proficiency comparable to human clinicians. However, their broad scope limits domain-specific precision, and proprietary weights hinder fine-tuning for specialized datasets. In contrast, small specialized models (SSMs) excel in targeted tasks but lack the contextual reasoning required for complex clinical decision-making. To address these challenges, we propose ConMIL (Conformalized Multiple Instance Learning), a decision-support SSM that integrates seamlessly with LLMs. By using Multiple Instance Learning (MIL) to identify clinically significant signal segments and conformal prediction for calibrated set-valued outputs, ConMIL enhances LLMsâ€™ interpretative capabilities for medical time-series analysis. Experimental results demonstrate that ConMIL significantly improves the performance of state-of-the-art LLMs, such as ChatGPT4.0 and Qwen2-VL-7B. Specifically, \ConMIL{}-supported Qwen2-VL-7B achieves 94.92% and 96.82% precision for confident samples in arrhythmia detection and sleep staging, compared to standalone LLM accuracy of 46.13% and 13.16%. These findings highlight the potential of ConMIL to bridge task-specific precision and broader contextual reasoning, enabling more reliable and interpretable AI-driven clinical decision support. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨åŒ»ç–—æ—¶é—´åºåˆ—æ•°æ®çš„è§†è§‰æ£€æŸ¥æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—çš„èƒ½åŠ›ï¼Œå…¶ç†Ÿç»ƒç¨‹åº¦å¯ä¸äººç±»ä¸´åºŠåŒ»ç”Ÿç›¸åª²ç¾ã€‚ç„¶è€Œï¼Œå…¶å¹¿æ³›çš„èŒƒå›´é™åˆ¶äº†ç‰¹å®šé¢†åŸŸçš„ç²¾åº¦ï¼Œä¸“æœ‰æƒé‡é˜»ç¢äº†é’ˆå¯¹ç‰¹å®šæ•°æ®é›†çš„å¾®è°ƒã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå°å‹ä¸“ä¸šåŒ–æ¨¡å‹ï¼ˆSSMsï¼‰åœ¨ç›®æ ‡ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†ç¼ºä¹ç”¨äºå¤æ‚ä¸´åºŠå†³ç­–æ‰€éœ€çš„ä¸Šä¸‹æ–‡æ¨ç†èƒ½åŠ›ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ConMILï¼ˆåˆè§„åŒ–å¤šå®ä¾‹å­¦ä¹ ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å†³ç­–æ”¯æŒSSMï¼Œå¯ä¸LLMæ— ç¼é›†æˆã€‚é€šè¿‡ä½¿ç”¨å¤šå®ä¾‹å­¦ä¹ ï¼ˆMILï¼‰æ¥è¯†åˆ«ä¸´åºŠä¸Šé‡è¦çš„ä¿¡å·æ®µå’Œåˆè§„é¢„æµ‹æ¥è¿›è¡Œæ ¡å‡†é›†å€¼è¾“å‡ºï¼ŒConMILå¢å¼ºäº†LLMå¯¹åŒ»ç–—æ—¶é—´åºåˆ—åˆ†æçš„è§£è¯»èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒConMILæ˜¾è‘—æé«˜äº†æœ€æ–°LLMçš„æ€§èƒ½ï¼Œå¦‚ChatGPT4.0å’ŒQwen2-VL-7Bã€‚å…·ä½“æ¥è¯´ï¼Œåœ¨å¿ƒå¾‹å¤±å¸¸æ£€æµ‹å’Œç¡çœ åˆ†æœŸæ–¹é¢ï¼Œç”±ConMILæ”¯æŒçš„Qwen2-VL-7Bå¯¹è‡ªä¿¡æ ·æœ¬çš„ç²¾ç¡®åº¦è¾¾åˆ°94.92%å’Œ96.82%ï¼Œè€Œå•ç‹¬çš„LLMå‡†ç¡®åº¦ä»…ä¸º46.13%å’Œ13.16%ã€‚è¿™äº›å‘ç°çªæ˜¾äº†ConMilåœ¨ä»»åŠ¡ç‰¹å®šç²¾åº¦å’Œæ›´å¹¿æ³›çš„ä¸Šä¸‹æ–‡æ¨ç†ä¹‹é—´çš„æ¡¥æ¢ä½œç”¨ï¼Œä¸ºå®ç°æ›´å¯é å’Œå¯è§£é‡Šçš„AIé©±åŠ¨çš„ä¸´åºŠå†³ç­–æ”¯æŒæä¾›äº†æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.16215v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨åŒ»ç–—æ—¶é—´åºåˆ—æ•°æ®çš„è§†è§‰æ£€æŸ¥ä¸­å±•ç°å‡ºä»¤äººç©ç›®çš„èƒ½åŠ›ï¼Œå…¶ä¸“ä¸šæ€§å ªæ¯”ä¸´åºŠåŒ»ç”Ÿã€‚ç„¶è€Œï¼Œå…¶å¹¿æ³›çš„åº”ç”¨èŒƒå›´é™åˆ¶äº†ç‰¹å®šé¢†åŸŸçš„ç²¾åº¦ï¼Œä¸”ä¸“æœ‰æƒé‡é˜»ç¢äº†é’ˆå¯¹ç‰¹å®šæ•°æ®é›†çš„å¾®è°ƒã€‚ç›¸åï¼Œå°å‹ä¸“ä¸šåŒ–æ¨¡å‹ï¼ˆSSMï¼‰åœ¨ç›®æ ‡ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¤æ‚ä¸´åºŠå†³ç­–æ‰€éœ€çš„ä¸Šä¸‹æ–‡æ¨ç†æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ConMILï¼ˆConformalized Multiple Instance Learningï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å†³ç­–æ”¯æŒSSMï¼Œå¯ä¸LLMsæ— ç¼é›†æˆã€‚é€šè¿‡å¤šé‡å®ä¾‹å­¦ä¹ ï¼ˆMILï¼‰è¯†åˆ«ä¸´åºŠé‡è¦ä¿¡å·æ®µå’Œæ ¡å‡†é›†åˆå€¼è¾“å‡ºçš„åˆè§„é¢„æµ‹ï¼ŒConMILå¢å¼ºäº†LLMåœ¨åŒ»ç–—æ—¶é—´åºåˆ—åˆ†æä¸­çš„è§£é‡Šèƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒConMILæ˜¾è‘—æé«˜äº†å…ˆè¿›LLMçš„æ€§èƒ½ï¼Œå¦‚ChatGPT4.0å’ŒQwen2-VL-7Bã€‚ç‰¹åˆ«æ˜¯ConMILæ”¯æŒçš„Qwen2-VL-7Båœ¨å¿ƒå¾‹å¤±å¸¸æ£€æµ‹å’Œç¡çœ åˆ†æœŸä¸­çš„è‡ªä¿¡æ ·æœ¬ç²¾åº¦åˆ†åˆ«è¾¾åˆ°94.92%å’Œ96.82%ï¼Œè€Œå•ç‹¬çš„LLMç²¾åº¦ä»…ä¸º46.13%å’Œ13.16%ã€‚è¿™äº›å‘ç°çªæ˜¾äº†ConMilåœ¨ä»»åŠ¡ç‰¹å®šç²¾åº¦å’Œæ›´å¹¿æ³›çš„ä¸Šä¸‹æ–‡æ¨ç†ä¹‹é—´çš„æ¡¥æ¢ä½œç”¨ï¼Œä¸ºæ›´å¯é å’Œå¯è§£é‡Šçš„AIé©±åŠ¨çš„ä¸´åºŠå†³ç­–æ”¯æŒæä¾›äº†æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨åŒ»ç–—æ—¶é—´åºåˆ—æ•°æ®çš„è§†è§‰æ£€æŸ¥æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ã€‚</li>
<li>LLMçš„å¹¿æ³›åº”ç”¨èŒƒå›´é™åˆ¶äº†å…¶åœ¨ç‰¹å®šé¢†åŸŸçš„ç²¾åº¦ã€‚</li>
<li>SSMåœ¨ç›®æ ‡ä»»åŠ¡ä¸€ä¸Šçš„è¡¨ç°ä¼˜ç§€ï¼Œä½†åœ¨ä¸Šä¸‹æ–‡æ¨ç†æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚</li>
<li>ConMILé€šè¿‡ç»“åˆMILå’Œåˆè§„é¢„æµ‹å¢å¼ºäº†LLMçš„è§£é‡Šèƒ½åŠ›ã€‚</li>
<li>ConMILæ˜¾è‘—æé«˜äº†å…ˆè¿›LLMåœ¨åŒ»ç–—æ—¶é—´åºåˆ—åˆ†æä¸­çš„æ€§èƒ½ã€‚</li>
<li>ConMILæ”¯æŒçš„Qwen2-VL-7Båœ¨å¿ƒå¾‹å¤±å¸¸æ£€æµ‹å’Œç¡çœ åˆ†æœŸæ–¹é¢è¡¨ç°å‡ºé«˜ç²¾ç¡®åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.16215">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f5f456636d0671332033b1d1db1a64ea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-afa21a5860e77816e8f63f613ceb756c.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Provence-efficient-and-robust-context-pruning-for-retrieval-augmented-generation"><a href="#Provence-efficient-and-robust-context-pruning-for-retrieval-augmented-generation" class="headerlink" title="Provence: efficient and robust context pruning for retrieval-augmented   generation"></a>Provence: efficient and robust context pruning for retrieval-augmented   generation</h2><p><strong>Authors:Nadezhda Chirkova, Thibault Formal, Vassilina Nikoulina, StÃ©phane Clinchant</strong></p>
<p>Retrieval-augmented generation improves various aspects of large language models (LLMs) generation, but suffers from computational overhead caused by long contexts as well as the propagation of irrelevant retrieved information into generated responses. Context pruning deals with both aspects, by removing irrelevant parts of retrieved contexts before LLM generation. Existing context pruning approaches are however limited, and do not provide a universal model that would be both efficient and robust in a wide range of scenarios, e.g., when contexts contain a variable amount of relevant information or vary in length, or when evaluated on various domains. In this work, we close this gap and introduce Provence (Pruning and Reranking Of retrieVEd relevaNt ContExts), an efficient and robust context pruner for Question Answering, which dynamically detects the needed amount of pruning for a given context and can be used out-of-the-box for various domains. The three key ingredients of Provence are formulating the context pruning task as sequence labeling, unifying context pruning capabilities with context reranking, and training on diverse data. Our experimental results show that Provence enables context pruning with negligible to no drop in performance, in various domains and settings, at almost no cost in a standard RAG pipeline. We also conduct a deeper analysis alongside various ablations to provide insights into training context pruners for future work. </p>
<blockquote>
<p>å¢å¼ºæ£€ç´¢ç”ŸæˆæŠ€æœ¯èƒ½å¤Ÿæå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç”Ÿæˆèƒ½åŠ›ï¼Œä½†å­˜åœ¨ç”±äºé•¿æ–‡æœ¬è¯­å¢ƒå¯¼è‡´çš„è®¡ç®—è´Ÿæ‹…ä»¥åŠå°†ä¸ç›¸å…³çš„æ£€ç´¢ä¿¡æ¯ä¼ æ’­åˆ°ç”Ÿæˆå“åº”ä¸­çš„é—®é¢˜ã€‚è¯­å¢ƒä¿®å‰ªæŠ€æœ¯èƒ½å¤Ÿå¤„ç†è¿™ä¸¤ä¸ªé—®é¢˜ï¼Œé€šè¿‡åœ¨LLMç”Ÿæˆä¹‹å‰å»é™¤æ£€ç´¢è¯­å¢ƒä¸­çš„ä¸ç›¸å…³éƒ¨åˆ†ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è¯­å¢ƒä¿®å‰ªæ–¹æ³•å­˜åœ¨å±€é™æ€§ï¼Œå¹¶æœªæä¾›ä¸€ç§é€šç”¨æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨å¤šç§åœºæ™¯ä¸‹æ—¢é«˜æ•ˆåˆç¨³å¥ï¼Œä¾‹å¦‚åœ¨è¯­å¢ƒåŒ…å«å¯å˜é‡çš„ç›¸å…³ä¿¡æ¯æˆ–é•¿åº¦ä¸åŒï¼Œæˆ–åœ¨å„ç§é¢†åŸŸè¿›è¡Œè¯„ä¼°æ—¶ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼¥è¡¥äº†è¿™ä¸€ç©ºç™½ï¼Œå¹¶å¼•å…¥äº†åä¸ºâ€œProvenceâ€ï¼ˆæ£€ç´¢ç›¸å…³è¯­å¢ƒçš„ä¿®å‰ªä¸é‡æ–°æ’åºï¼‰çš„è¯­å¢ƒä¿®å‰ªå™¨ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºé—®ç­”çš„é«˜æ•ˆä¸”ç¨³å¥çš„è¯­å¢ƒä¿®å‰ªå™¨ï¼Œèƒ½å¤Ÿé’ˆå¯¹ç»™å®šè¯­å¢ƒåŠ¨æ€æ£€æµ‹æ‰€éœ€ä¿®å‰ªçš„é‡ï¼Œå¹¶å¯ç›´æ¥ç”¨äºå„ç§é¢†åŸŸã€‚Provenceçš„ä¸‰ä¸ªå…³é”®è¦ç´ æ˜¯å°†è¯­å¢ƒä¿®å‰ªä»»åŠ¡åˆ¶å®šä¸ºåºåˆ—æ ‡æ³¨ï¼Œå°†è¯­å¢ƒä¿®å‰ªèƒ½åŠ›ä¸è¯­å¢ƒé‡æ–°æ’åºç›¸ç»“åˆï¼Œå¹¶åœ¨å¤šæ ·åŒ–æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å„ç§é¢†åŸŸå’Œè®¾ç½®ä¸­ï¼Œä½¿ç”¨Provenceè¿›è¡Œè¯­å¢ƒä¿®å‰ªå‡ ä¹ä¸ä¼šå¯¹æ€§èƒ½äº§ç”Ÿè´Ÿé¢å½±å“ï¼Œå¹¶ä¸”åœ¨æ ‡å‡†RAGç®¡é“ä¸­çš„æˆæœ¬å‡ ä¹ä¸ºé›¶ã€‚æˆ‘ä»¬è¿˜è¿›è¡Œäº†æ›´æ·±å…¥çš„åˆ†æï¼ŒåŒæ—¶è¿›è¡Œäº†å„ç§åˆ‡é™¤ç ”ç©¶ï¼Œä¸ºæœªæ¥è®­ç»ƒè¯­å¢ƒä¿®å‰ªå™¨æä¾›è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.16214v1">PDF</a> Accepted to ICLR 2025</p>
<p><strong>Summary</strong>ï¼šåŸºäºæ‘˜è¦çš„æŠ€æœ¯è®ºæ–‡ã€‚åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œå¼•å…¥æ£€ç´¢å¢å¼ºæŠ€æœ¯çš„èƒŒæ™¯èƒ½å¤Ÿæé«˜å¤šç§æ€§èƒ½ã€‚ä½†é•¿è¿œèƒŒæ™¯å’Œæ— å…³æ£€ç´¢ä¿¡æ¯å¯èƒ½è¢«çº³å…¥ç”Ÿæˆå“åº”ï¼Œä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œç ”ç©¶è€…æå‡ºäº†ä¸Šä¸‹æ–‡ä¿®å‰ªæŠ€æœ¯ã€‚ç„¶è€Œï¼Œç°æœ‰æŠ€æœ¯å­˜åœ¨å±€é™æ€§ï¼Œéš¾ä»¥åœ¨å¤šç§åœºæ™¯ä¸‹å®ç°é«˜æ•ˆå’Œç¨³å¥çš„ä¸Šä¸‹æ–‡ä¿®å‰ªã€‚å› æ­¤ï¼Œæœ¬æ–‡å¼•å…¥äº†ä¸€ç§é’ˆå¯¹é—®ç­”çš„ä¸Šä¸‹æ–‡ä¿®å‰ªæŠ€æœ¯â€”â€”Provenceï¼ˆæ£€ç´¢ç›¸å…³ä¸Šä¸‹æ–‡ä¿®å‰ªå’Œé‡æ–°æ’åºï¼‰ã€‚å®ƒå¯åŠ¨æ€æ£€æµ‹ç»™å®šä¸Šä¸‹æ–‡æ‰€éœ€çš„ä¿®å‰ªé‡ï¼Œé€‚ç”¨äºå„ç§é¢†åŸŸã€‚è¯¥æŠ€æœ¯çš„å…³é”®åŒ…æ‹¬å°†ä¸Šä¸‹æ–‡ä¿®å‰ªä»»åŠ¡è½¬åŒ–ä¸ºåºåˆ—æ ‡æ³¨é—®é¢˜ï¼Œç»Ÿä¸€ä¸Šä¸‹æ–‡ä¿®å‰ªå’Œé‡æ–°æ’åºèƒ½åŠ›ï¼Œä»¥åŠè¿›è¡Œå¤šæ ·åŒ–æ•°æ®çš„è®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨æ ‡å‡†RAGç®¡é“ä¸­ï¼Œä½¿ç”¨Provenceè¿›è¡Œä¸Šä¸‹æ–‡ä¿®å‰ªå‡ ä¹ä¸ä¼šé™ä½æ€§èƒ½ã€‚åŒæ—¶ï¼Œæœ¬æ–‡è¿˜è¿›è¡Œäº†æ·±å…¥çš„åˆ†æå’Œæ¶ˆèç ”ç©¶ï¼Œä¸ºæœªæ¥çš„ä¸Šä¸‹æ–‡ä¿®å‰ªè®­ç»ƒæä¾›äº†è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æ£€ç´¢å¢å¼ºæŠ€æœ¯èƒ½æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>ç°æœ‰ä¸Šä¸‹æ–‡ä¿®å‰ªæŠ€æœ¯å­˜åœ¨å±€é™æ€§ï¼Œéš¾ä»¥åº”å¯¹å¤šç§åœºæ™¯çš„æŒ‘æˆ˜ã€‚</li>
<li>å¼•å…¥çš„â€œProvenceâ€æŠ€æœ¯èƒ½åŠ¨æ€æ£€æµ‹æ‰€éœ€ä¿®å‰ªçš„ä¸Šä¸‹æ–‡é‡ï¼Œé€‚ç”¨äºå„ç§é¢†åŸŸã€‚</li>
<li>â€œProvenceâ€æŠ€æœ¯çš„å…³é”®åŒ…æ‹¬å°†ä¸Šä¸‹æ–‡ä¿®å‰ªä»»åŠ¡è½¬åŒ–ä¸ºåºåˆ—æ ‡æ³¨é—®é¢˜ï¼Œç»Ÿä¸€ä¸Šä¸‹æ–‡ä¿®å‰ªå’Œé‡æ–°æ’åºèƒ½åŠ›ï¼Œä»¥åŠè¿›è¡Œå¤šæ ·åŒ–æ•°æ®çš„è®­ç»ƒã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œâ€œProvenceâ€æŠ€æœ¯åœ¨å®ç°ä¸Šä¸‹æ–‡ä¿®å‰ªçš„åŒæ—¶å‡ ä¹ä¸å½±å“æ€§èƒ½ã€‚</li>
<li>é€šè¿‡æ·±å…¥åˆ†æå’Œæ¶ˆèç ”ç©¶ï¼Œä¸ºæœªæ¥çš„ä¸Šä¸‹æ–‡ä¿®å‰ªè®­ç»ƒæä¾›äº†è§è§£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.16214">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bd31bc2827b7a4bfdef8a98ed4ef453b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-98d003087964674aaa23e7f1b6709971.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6a68857aa850d736a2264f265935d7c0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bd09655bda7fd763e4216293259c2dc7.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Raiders-of-the-Lost-Dependency-Fixing-Dependency-Conflicts-in-Python-using-LLMs"><a href="#Raiders-of-the-Lost-Dependency-Fixing-Dependency-Conflicts-in-Python-using-LLMs" class="headerlink" title="Raiders of the Lost Dependency: Fixing Dependency Conflicts in Python   using LLMs"></a>Raiders of the Lost Dependency: Fixing Dependency Conflicts in Python   using LLMs</h2><p><strong>Authors:Antony Bartlett, Cynthia Liem, Annibale Panichella</strong></p>
<p>Fixing Python dependency issues is a tedious and error-prone task for developers, who must manually identify and resolve environment dependencies and version constraints of third-party modules and Python interpreters. Researchers have attempted to automate this process by relying on large knowledge graphs and database lookup tables. However, these traditional approaches face limitations due to the variety of dependency error types, large sets of possible module versions, and conflicts among transitive dependencies. This study explores the potential of using large language models (LLMs) to automatically fix dependency issues in Python programs. We introduce PLLM (pronounced â€œplumâ€), a novel technique that employs retrieval-augmented generation (RAG) to help an LLM infer Python versions and required modules for a given Python file. PLLM builds a testing environment that iteratively (1) prompts the LLM for module combinations, (2) tests the suggested changes, and (3) provides feedback (error messages) to the LLM to refine the fix. This feedback cycle leverages natural language processing (NLP) to intelligently parse and interpret build error messages. We benchmark PLLM on the Gistable HG2.9K dataset, a collection of challenging single-file Python gists. We compare PLLM against two state-of-the-art automatic dependency inference approaches, namely PyEGo and ReadPyE, w.r.t. the ability to resolve dependency issues. Our results indicate that PLLM can fix more dependency issues than the two baselines, with +218 (+15.97%) more fixes over ReadPyE and +281 (+21.58%) over PyEGo. Our deeper analyses suggest that PLLM is particularly beneficial for projects with many dependencies and for specific third-party numerical and machine-learning modules. Our findings demonstrate the potential of LLM-based approaches to iteratively resolve Python dependency issues. </p>
<blockquote>
<p>è§£å†³Pythonä¾èµ–æ€§é—®é¢˜å¯¹äºå¼€å‘è€…æ¥è¯´æ˜¯ä¸€é¡¹ä¹å‘³ä¸”æ˜“å‡ºé”™çš„ä»»åŠ¡ï¼Œä»–ä»¬å¿…é¡»æ‰‹åŠ¨è¯†åˆ«å’Œè§£å†³ç¬¬ä¸‰æ–¹æ¨¡å—å’ŒPythonè§£é‡Šå™¨çš„ç¯å¢ƒä¾èµ–æ€§å’Œç‰ˆæœ¬çº¦æŸã€‚ç ”ç©¶è€…è¯•å›¾é€šè¿‡ä¾èµ–å¤§å‹çŸ¥è¯†å›¾è°±å’Œæ•°æ®åº“æŸ¥æ‰¾è¡¨æ¥è‡ªåŠ¨åŒ–è¿™ä¸ªè¿‡ç¨‹ã€‚ç„¶è€Œï¼Œè¿™äº›ä¼ ç»Ÿæ–¹æ³•ç”±äºä¾èµ–é”™è¯¯ç±»å‹çš„å¤šæ ·æ€§ã€å¯èƒ½çš„æ¨¡å—ç‰ˆæœ¬é›†çš„å¤§é‡å’Œä¼ é€’ä¾èµ–ä¹‹é—´çš„å†²çªè€Œé¢ä¸´å±€é™ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è‡ªåŠ¨è§£å†³Pythonç¨‹åºä¸­çš„ä¾èµ–é—®é¢˜çš„æ½œåŠ›ã€‚æˆ‘ä»¬ä»‹ç»äº†PLMMï¼ˆå‘éŸ³ä¸ºâ€œplumâ€ï¼‰,å®ƒæ˜¯ä¸€ç§é‡‡ç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„æ–°æŠ€æœ¯ï¼Œå¸®åŠ©LLMæ¨æ–­ç»™å®šPythonæ–‡ä»¶çš„Pythonç‰ˆæœ¬å’Œæ‰€éœ€æ¨¡å—ã€‚PLMMå»ºç«‹ä¸€ä¸ªæµ‹è¯•ç¯å¢ƒï¼Œè¯¥ç¯å¢ƒå¯ä»¥è¿­ä»£åœ°ï¼ˆ1ï¼‰å‘LLMæç¤ºæ¨¡å—ç»„åˆï¼Œï¼ˆ2ï¼‰æµ‹è¯•å»ºè®®çš„æ›´æ”¹ï¼Œå¹¶æä¾›åé¦ˆï¼ˆé”™è¯¯æ¶ˆæ¯ï¼‰ç»™LLMä»¥å®Œå–„ä¿®å¤æ–¹æ¡ˆã€‚è¿™ä¸ªåé¦ˆå¾ªç¯åˆ©ç”¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰æ™ºèƒ½åœ°è§£æå’Œè§£é‡Šæ„å»ºé”™è¯¯æ¶ˆæ¯ã€‚æˆ‘ä»¬åœ¨Gistable HG2.9Kæ•°æ®é›†ä¸Šå¯¹PLMMè¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œè¿™æ˜¯ä¸€ä¸ªå……æ»¡æŒ‘æˆ˜çš„å•æ–‡ä»¶Pythonä»£ç ç‰‡æ®µé›†åˆã€‚æˆ‘ä»¬å°†PLMMä¸ä¸¤ç§æœ€æ–°çš„è‡ªåŠ¨ä¾èµ–æ¨æ–­æ–¹æ³•PyEGoå’ŒReadPyEè¿›è¡Œäº†æ¯”è¾ƒï¼Œå°±è§£å†³ä¾èµ–é—®é¢˜çš„èƒ½åŠ›è¿›è¡Œäº†è¯„ä¼°ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒPLMMæ¯”è¿™ä¸¤ä¸ªåŸºå‡†èƒ½ä¿®å¤æ›´å¤šçš„ä¾èµ–é—®é¢˜ï¼Œç›¸è¾ƒäºReadPyEå¤šå‡º+218ï¼ˆ+15.97%ï¼‰ï¼Œç›¸è¾ƒäºPyEGoå¤šå‡º+281ï¼ˆ+21.58%ï¼‰ã€‚æˆ‘ä»¬çš„æ·±å…¥åˆ†æè¡¨æ˜ï¼ŒPLMMå¯¹äºå…·æœ‰è®¸å¤šä¾èµ–é¡¹çš„é¡¹ç›®å’Œç‰¹å®šçš„ç¬¬ä¸‰æ–¹æ•°å€¼å’Œæœºå™¨å­¦ä¹ æ¨¡å—ç‰¹åˆ«æœ‰ç›Šã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜åŸºäºLLMçš„æ–¹æ³•åœ¨è¿­ä»£è§£å†³Pythonä¾èµ–é—®é¢˜æ–¹é¢çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.16191v1">PDF</a> Under submission to TOSEM, 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è‡ªåŠ¨ä¿®å¤Pythonç¨‹åºä¸­çš„ä¾èµ–é—®é¢˜æ˜¯ä¸€é¡¹å…·æœ‰æ½œåŠ›çš„ç ”ç©¶ã€‚PLLMï¼ˆå‘éŸ³ä¸ºâ€œplumâ€ï¼‰æ˜¯ä¸€ç§é‡‡ç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯çš„æ–°æ–¹æ³•ï¼Œå¯å¸®åŠ©LLMæ¨æ–­ç»™å®šPythonæ–‡ä»¶æ‰€éœ€çš„Pythonç‰ˆæœ¬å’Œæ¨¡å—ã€‚PLLMå»ºç«‹ä¸€ä¸ªæµ‹è¯•ç¯å¢ƒï¼Œé€šè¿‡æç¤ºLLMè¿›è¡Œæ¨¡å—ç»„åˆã€æµ‹è¯•å»ºè®®çš„æ›´æ”¹ä»¥åŠæä¾›åé¦ˆï¼ˆé”™è¯¯æ¶ˆæ¯ï¼‰æ¥å®Œå–„ä¿®å¤è¿‡ç¨‹ã€‚æ­¤æ–¹æ³•åˆ©ç”¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰æ™ºèƒ½è§£æå’Œè§£é‡Šæ„å»ºé”™è¯¯æ¶ˆæ¯ã€‚åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„å•æ–‡ä»¶Python gistsé›†åˆGistable HG2.9Kæ•°æ®é›†ä¸Šè¯„ä¼°PLLMä¸ä¸¤ä¸ªæœ€å…ˆè¿›çš„è‡ªåŠ¨ä¾èµ–æ¨æ–­æ–¹æ³•PyEGoå’ŒReadPyEçš„è§£å†³ä¾èµ–é—®é¢˜çš„èƒ½åŠ›ã€‚ç»“æœè¡¨æ˜ï¼ŒPLLMæ¯”ä¸¤ä¸ªåŸºå‡†æ–¹æ³•æ›´èƒ½è§£å†³ä¾èµ–é—®é¢˜ï¼Œç›¸å¯¹äºReadPyEæœ‰+218ï¼ˆ+15.97%ï¼‰çš„æ”¹è¿›ï¼Œç›¸å¯¹äºPyEGoæœ‰+281ï¼ˆ+21.58%ï¼‰çš„æ”¹è¿›ã€‚æ·±å…¥çš„åˆ†æè¡¨æ˜ï¼ŒPLLMå¯¹äºå…·æœ‰è®¸å¤šä¾èµ–é¡¹å’Œç‰¹å®šç¬¬ä¸‰æ–¹æ•°å€¼å’Œæœºå™¨å­¦ä¹ æ¨¡å—çš„é¡¹ç›®ç‰¹åˆ«æœ‰ç›Šã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>Pythonä¾èµ–æ€§é—®é¢˜è§£å†³æ˜¯ä¸€é¡¹å¤æ‚ä¸”å®¹æ˜“å‡ºé”™çš„ä»»åŠ¡ï¼Œä¼ ç»Ÿçš„æ–¹æ³•å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªåŠ¨ä¿®å¤Pythonä¾èµ–æ€§é—®é¢˜æ–¹é¢å…·æœ‰æ½œåŠ›ã€‚</li>
<li>PLLMæ˜¯ä¸€ç§åˆ©ç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„æ–°æŠ€æœ¯ï¼Œèƒ½å¤Ÿæ¨æ–­Pythonæ–‡ä»¶çš„ä¾èµ–éœ€æ±‚ã€‚</li>
<li>PLLMé€šè¿‡æç¤ºLLMè¿›è¡Œæ¨¡å—ç»„åˆã€æµ‹è¯•å»ºè®®æ›´æ”¹å’Œæä¾›é”™è¯¯åé¦ˆæ¥å®Œå–„ä¿®å¤è¿‡ç¨‹ã€‚</li>
<li>PLLMåˆ©ç”¨è‡ªç„¶è¯­è¨€å¤„ç†è§£æå’Œè§£é‡Šæ„å»ºé”™è¯¯æ¶ˆæ¯ã€‚</li>
<li>åœ¨Gistable HG2.9Kæ•°æ®é›†ä¸Šè¯„ä¼°ï¼ŒPLLMç›¸å¯¹äºå…¶ä»–æ–¹æ³•èƒ½æ›´é«˜æ•ˆåœ°è§£å†³ä¾èµ–é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.16191">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fc63de89db88e8c1c225e5cce52f3a61.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fb849ecceaa196d634baf6521e0ad41d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-616682927f14206f3a2278af65b8778f.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="SWIFT-Mapping-Sub-series-with-Wavelet-Decomposition-Improves-Time-Series-Forecasting"><a href="#SWIFT-Mapping-Sub-series-with-Wavelet-Decomposition-Improves-Time-Series-Forecasting" class="headerlink" title="SWIFT: Mapping Sub-series with Wavelet Decomposition Improves Time   Series Forecasting"></a>SWIFT: Mapping Sub-series with Wavelet Decomposition Improves Time   Series Forecasting</h2><p><strong>Authors:Wenxuan Xie, Fanpu Cao</strong></p>
<p>In recent work on time-series prediction, Transformers and even large language models have garnered significant attention due to their strong capabilities in sequence modeling. However, in practical deployments, time-series prediction often requires operation in resource-constrained environments, such as edge devices, which are unable to handle the computational overhead of large models. To address such scenarios, some lightweight models have been proposed, but they exhibit poor performance on non-stationary sequences. In this paper, we propose $\textit{SWIFT}$, a lightweight model that is not only powerful, but also efficient in deployment and inference for Long-term Time Series Forecasting (LTSF). Our model is based on three key points: (i) Utilizing wavelet transform to perform lossless downsampling of time series. (ii) Achieving cross-band information fusion with a learnable filter. (iii) Using only one shared linear layer or one shallow MLP for sub-seriesâ€™ mapping. We conduct comprehensive experiments, and the results show that $\textit{SWIFT}$ achieves state-of-the-art (SOTA) performance on multiple datasets, offering a promising method for edge computing and deployment in this task. Moreover, it is noteworthy that the number of parameters in $\textit{SWIFT-Linear}$ is only 25% of what it would be with a single-layer linear model for time-domain prediction. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/LancelotXWX/SWIFT">https://github.com/LancelotXWX/SWIFT</a>. </p>
<blockquote>
<p>åœ¨æœ€è¿‘å…³äºæ—¶é—´åºåˆ—é¢„æµ‹çš„ç ”ç©¶ä¸­ï¼ŒTransformerç”šè‡³å¤§å‹è¯­è¨€æ¨¡å‹ç”±äºå…¶å¼ºå¤§çš„åºåˆ—å»ºæ¨¡èƒ½åŠ›è€Œå¤‡å—å…³æ³¨ã€‚ç„¶è€Œï¼Œåœ¨å®é™…éƒ¨ç½²ä¸­ï¼Œæ—¶é—´åºåˆ—é¢„æµ‹é€šå¸¸éœ€è¦åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­è¿è¡Œï¼Œä¾‹å¦‚è¾¹ç¼˜è®¾å¤‡æ— æ³•å¤„ç†å¤§å‹æ¨¡å‹çš„è®¡ç®—è´Ÿæ‹…ã€‚ä¸ºäº†åº”å¯¹è¿™ç§æƒ…å†µï¼Œå·²ç»æå‡ºäº†ä¸€äº›è½»é‡çº§æ¨¡å‹ï¼Œä½†å®ƒä»¬åœ¨éå¹³ç¨³åºåˆ—ä¸Šçš„è¡¨ç°è¾ƒå·®ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºSWIFTçš„è½»é‡çº§æ¨¡å‹ï¼Œå®ƒä¸ä»…èƒ½å¤Ÿè¿›è¡Œé•¿æœŸæ—¶é—´åºåˆ—é¢„æµ‹ï¼ˆLTSFï¼‰ï¼Œè€Œä¸”åœ¨éƒ¨ç½²å’Œæ¨ç†æ–¹é¢ä¹Ÿéå¸¸å¼ºå¤§å’Œé«˜æ•ˆã€‚æˆ‘ä»¬çš„æ¨¡å‹åŸºäºä¸‰ä¸ªå…³é”®ç‚¹ï¼šï¼ˆiï¼‰åˆ©ç”¨å°æ³¢å˜æ¢å¯¹æ—¶é—´åºåˆ—è¿›è¡Œæ— æŸé™é‡‡æ ·ã€‚ï¼ˆiiï¼‰ä½¿ç”¨å¯å­¦ä¹ æ»¤æ³¢å™¨å®ç°è·¨é¢‘å¸¦ä¿¡æ¯èåˆã€‚ï¼ˆiiiï¼‰ä»…ä½¿ç”¨ä¸€ä¸ªå…±äº«çº¿æ€§å±‚æˆ–ä¸€ä¸ªæµ…å±‚MLPè¿›è¡Œå­åºåˆ—æ˜ å°„ã€‚æˆ‘ä»¬è¿›è¡Œäº†å…¨é¢çš„å®éªŒï¼Œç»“æœè¡¨æ˜SWIFTåœ¨å¤šæ•°æ®é›†ä¸Šå®ç°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼ˆSOTAï¼‰çš„æ€§èƒ½ï¼Œä¸ºè¾¹ç¼˜è®¡ç®—éƒ¨ç½²åœ¨è¯¥ä»»åŠ¡ä¸Šæä¾›äº†ä¸€ç§æœ‰å‰é€”çš„æ–¹æ³•ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒSWIFT-Linearçš„å‚æ•°æ•°é‡ä»…ä¸ºæ—¶é—´åºåˆ—åŸŸé¢„æµ‹çš„å•å±‚çº¿æ€§æ¨¡å‹çš„å››åˆ†ä¹‹ä¸€ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/LancelotXWX/SWIFT%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/LancelotXWX/SWIFTæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.16178v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå°æ³¢å˜æ¢çš„è½»é‡çº§æ—¶é—´åºåˆ—é¢„æµ‹æ¨¡å‹SWIFTï¼Œé€‚ç”¨äºé•¿æœŸæ—¶é—´åºåˆ—é¢„æµ‹ï¼ˆLTSFï¼‰ã€‚æ¨¡å‹å…·æœ‰ä¸‰å¤§ç‰¹ç‚¹ï¼šæ— æŸä¸‹é‡‡æ ·æ—¶é—´åºåˆ—çš„å°æ³¢å˜æ¢ã€å¯å­¦ä¹ æ»¤æ³¢å™¨çš„è·¨é¢‘ä¿¡æ¯èåˆï¼Œä»¥åŠä»…ä½¿ç”¨ä¸€å±‚å…±äº«çº¿æ€§å±‚æˆ–æµ…å±‚MLPè¿›è¡Œå­åºåˆ—æ˜ å°„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSWIFTåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†æœ€ä½³æ€§èƒ½ï¼Œä¸ºè¾¹ç¼˜è®¡ç®—å’Œéƒ¨ç½²æä¾›äº†æœ‰å‰é€”çš„æ–¹æ³•ã€‚æ­¤å¤–ï¼ŒSWIFT-Linearçš„å‚æ•°æ•°é‡ä»…ä¸ºæ—¶é—´åºåˆ—åŸŸé¢„æµ‹å•å±‚çº¿æ€§æ¨¡å‹çš„25%ã€‚ä»£ç å…¬å¼€äº[ç½‘å€]ã€‚</p>
<p><strong>è¦ç‚¹æ€»ç»“</strong></p>
<ul>
<li>è®ºæ–‡æ¢è®¨äº†æ—¶é—´åºåˆ—é¢„æµ‹ä¸­çš„è½»é‡çº§æ¨¡å‹é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹åœ¨èµ„æºå—é™ç¯å¢ƒä¸­çš„éƒ¨ç½²æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„è½»é‡çº§æ—¶é—´åºåˆ—é¢„æµ‹æ¨¡å‹SWIFTï¼Œé€‚ç”¨äºé•¿æœŸæ—¶é—´åºåˆ—é¢„æµ‹ï¼ˆLTSFï¼‰ã€‚</li>
<li>SWIFTæ¨¡å‹åŸºäºä¸‰å¤§å…³é”®è®¾è®¡ï¼šæ— æŸä¸‹é‡‡æ ·ã€è·¨é¢‘ä¿¡æ¯èåˆå’Œå­åºåˆ—æ˜ å°„çš„ç®€å•çº¿æ€§å±‚æˆ–æµ…å±‚MLPã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒSWIFTåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†æœ€ä½³æ€§èƒ½ã€‚</li>
<li>SWIFTæ¨¡å‹ç‰¹åˆ«é€‚ç”¨äºè¾¹ç¼˜è®¡ç®—å’Œéƒ¨ç½²åœºæ™¯ã€‚å…¶å‚æ•°æ•°é‡æ˜¾è‘—å‡å°‘ï¼Œé™ä½äº†è®¡ç®—å¼€é”€ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.16178">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9168b9c14f0a7ccb3a48e449fa7219aa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6417b0fbaddd54e1f5125b74c6c9693f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-084a313d730f6d9342fc3e9b52d03c7f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cc921aa2b987fac747c75e5d0b8d0201.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8be1841fbc23f4b7ae3738e41ff47def.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e905574ed051f18599a367675655c034.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="GraphICL-Unlocking-Graph-Learning-Potential-in-LLMs-through-Structured-Prompt-Design"><a href="#GraphICL-Unlocking-Graph-Learning-Potential-in-LLMs-through-Structured-Prompt-Design" class="headerlink" title="GraphICL: Unlocking Graph Learning Potential in LLMs through Structured   Prompt Design"></a>GraphICL: Unlocking Graph Learning Potential in LLMs through Structured   Prompt Design</h2><p><strong>Authors:Yuanfu Sun, Zhengnan Ma, Yi Fang, Jing Ma, Qiaoyu Tan</strong></p>
<p>The growing importance of textual and relational systems has driven interest in enhancing large language models (LLMs) for graph-structured data, particularly Text-Attributed Graphs (TAGs), where samples are represented by textual descriptions interconnected by edges. While research has largely focused on developing specialized graph LLMs through task-specific instruction tuning, a comprehensive benchmark for evaluating LLMs solely through prompt design remains surprisingly absent. Without such a carefully crafted evaluation benchmark, most if not all, tailored graph LLMs are compared against general LLMs using simplistic queries (e.g., zero-shot reasoning with LLaMA), which can potentially camouflage many advantages as well as unexpected predicaments of them. To achieve more general evaluations and unveil the true potential of LLMs for graph tasks, we introduce Graph In-context Learning (GraphICL) Benchmark, a comprehensive benchmark comprising novel prompt templates designed to capture graph structure and handle limited label knowledge. Our systematic evaluation shows that general-purpose LLMs equipped with our GraphICL outperform state-of-the-art specialized graph LLMs and graph neural network models in resource-constrained settings and out-of-domain tasks. These findings highlight the significant potential of prompt engineering to enhance LLM performance on graph learning tasks without training and offer a strong baseline for advancing research in graph LLMs. </p>
<blockquote>
<p>æ–‡æœ¬å’Œå…³ç³»ç³»ç»Ÿçš„é‡è¦æ€§ä¸æ–­å¢é•¿ï¼Œæ¨åŠ¨äº†å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤„ç†å›¾å½¢ç»“æ„åŒ–æ•°æ®æ–¹é¢çš„å¢å¼ºéœ€æ±‚ï¼Œç‰¹åˆ«æ˜¯æ–‡æœ¬å±æ€§å›¾ï¼ˆTAGï¼‰ï¼Œå…¶ä¸­çš„æ ·æœ¬ç”±æ–‡æœ¬æè¿°è¡¨ç¤ºï¼Œå¹¶é€šè¿‡è¾¹ç¼˜ç›¸äº’è¿æ¥ã€‚è™½ç„¶ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨é€šè¿‡ç‰¹å®šä»»åŠ¡çš„æŒ‡ä»¤è°ƒæ•´æ¥å¼€å‘ä¸“ä¸šåŒ–çš„å›¾å½¢LLMï¼Œä½†ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œä»…é€šè¿‡æç¤ºè®¾è®¡è¯„ä¼°LLMçš„ç»¼åˆåŸºå‡†æµ‹è¯•ä»ç„¶ç¼ºå¤±ã€‚ç”±äºæ²¡æœ‰å¦‚æ­¤ç²¾å¿ƒè®¾è®¡çš„è¯„ä¼°åŸºå‡†ï¼Œå¤§å¤šæ•°ï¼ˆå¦‚æœä¸æ˜¯å…¨éƒ¨çš„è¯ï¼‰å®šåˆ¶åŒ–çš„å›¾å½¢LLMä¼šä½¿ç”¨ç®€å•çš„æŸ¥è¯¢ä¸é€šç”¨LLMè¿›è¡Œæ¯”è¾ƒï¼ˆä¾‹å¦‚ä½¿ç”¨LLaMAè¿›è¡Œé›¶å°„å‡»æ¨ç†ï¼‰ï¼Œè¿™å¯èƒ½ä¼šæ©ç›–å®ƒä»¬çš„è®¸å¤šä¼˜åŠ¿ä»¥åŠæœªé¢„æ–™åˆ°çš„å›°å¢ƒã€‚ä¸ºäº†å®ç°æ›´é€šç”¨çš„è¯„ä¼°å’Œæ­ç¤ºLLMåœ¨å›¾ä»»åŠ¡ä¸­çš„çœŸæ­£æ½œåŠ›ï¼Œæˆ‘ä»¬å¼•å…¥äº†Graph In-context Learningï¼ˆGraphICLï¼‰åŸºå‡†æµ‹è¯•ï¼Œè¿™æ˜¯ä¸€é¡¹å…¨é¢çš„åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«ä¸“é—¨è®¾è®¡çš„æç¤ºæ¨¡æ¿ï¼Œæ—¨åœ¨æ•æ‰å›¾å½¢ç»“æ„å¹¶å¤„ç†æœ‰é™çš„æ ‡ç­¾çŸ¥è¯†ã€‚æˆ‘ä»¬çš„ç³»ç»Ÿè¯„ä¼°è¡¨æ˜ï¼Œé…å¤‡äº†æˆ‘ä»¬GraphICLçš„é€šç”¨LLMåœ¨èµ„æºå—é™è®¾ç½®å’Œè·¨åŸŸä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºæœ€æ–°çš„ä¸“ä¸šå›¾å½¢LLMå’Œå›¾ç¥ç»ç½‘ç»œæ¨¡å‹ã€‚è¿™äº›å‘ç°çªæ˜¾äº†æç¤ºå·¥ç¨‹åœ¨å¢å¼ºLLMåœ¨å›¾å­¦ä¹ ä»»åŠ¡ä¸Šçš„æ€§èƒ½çš„æ½œåŠ›ï¼Œæ— éœ€è¿›è¡Œè®­ç»ƒï¼Œå¹¶ä¸ºæ¨åŠ¨å›¾å½¢LLMçš„ç ”ç©¶æä¾›äº†å¼ºå¤§çš„åŸºå‡†çº¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.15755v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æ–‡æœ¬å’Œå…³ç³»ç³»ç»Ÿçš„é‡è¦æ€§ä¸æ–­å¢é•¿ï¼Œæ¨åŠ¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å›¾å½¢ç»“æ„åŒ–æ•°æ®æ–¹é¢çš„æå‡ï¼Œç‰¹åˆ«æ˜¯æ–‡æœ¬å±æ€§å›¾ï¼ˆTAGï¼‰ã€‚å½“å‰ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨é€šè¿‡ç‰¹å®šä»»åŠ¡æŒ‡ä»¤è°ƒæ•´å¼€å‘ä¸“ä¸šå›¾å½¢LLMä¸Šï¼Œä½†ä»…é€šè¿‡æç¤ºè®¾è®¡è¯„ä¼°LLMçš„ç»¼åˆåŸºå‡†æµ‹è¯•å´æ„å¤–ç¼ºå¤±ã€‚ä¸ºå®ç°æ›´é€šç”¨çš„è¯„ä¼°å’Œæ­ç¤ºLLMåœ¨å›¾ä»»åŠ¡ä¸­çš„çœŸæ­£æ½œåŠ›ï¼Œæˆ‘ä»¬æ¨å‡ºäº†Graph In-context Learning (GraphICL) Benchmarkã€‚è¯¥åŸºå‡†æµ‹è¯•åŒ…å«æ–°å‹æç¤ºæ¨¡æ¿ï¼Œæ—¨åœ¨æ•æ‰å›¾å½¢ç»“æ„å¹¶å¤„ç†æœ‰é™çš„æ ‡ç­¾çŸ¥è¯†ã€‚ç³»ç»Ÿè¯„ä¼°æ˜¾ç¤ºï¼Œé…å¤‡GraphICLçš„é€šç”¨LLMåœ¨èµ„æºå—é™è®¾ç½®å’Œè·¨åŸŸä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºæœ€å…ˆè¿›çš„ä¸“ç”¨å›¾å½¢LLMå’Œå›¾ç¥ç»ç½‘ç»œæ¨¡å‹ã€‚è¿™äº›å‘ç°çªæ˜¾äº†æç¤ºå·¥ç¨‹åœ¨å¢å¼ºLLMå›¾å­¦ä¹ ä»»åŠ¡æ€§èƒ½æ–¹é¢çš„å·¨å¤§æ½œåŠ›ï¼Œæ— éœ€è®­ç»ƒå¹¶æä¾›äº†ä¸€ä¸ªå¼ºå¤§çš„åŸºçº¿ï¼Œä»¥æ¨åŠ¨å›¾å½¢LLMçš„ç ”ç©¶è¿›æ­¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬å’Œå…³ç³»ç³»ç»Ÿçš„å¢é•¿å¯¹LLMåœ¨å›¾å½¢ç»“æ„åŒ–æ•°æ®æ–¹é¢çš„æå‡æå‡ºäº†éœ€æ±‚ã€‚</li>
<li>ç›®å‰é’ˆå¯¹LLMåœ¨å›¾å½¢æ•°æ®æ–¹é¢çš„è¡¨ç°ç¼ºä¹å…¨é¢çš„åŸºå‡†æµ‹è¯•ã€‚</li>
<li>å¼•å…¥Graph In-context Learning (GraphICL) Benchmarkæ¥å…¨é¢è¯„ä¼°LLMåœ¨å›¾ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚</li>
<li>GraphICLåŒ…å«æ–°å‹æç¤ºæ¨¡æ¿ï¼Œæ—¨åœ¨æ•æ‰å›¾å½¢ç»“æ„å’Œå¤„ç†æœ‰é™çš„æ ‡ç­¾çŸ¥è¯†ã€‚</li>
<li>é…å¤‡GraphICLçš„é€šç”¨LLMåœ¨èµ„æºå—é™è®¾ç½®å’Œè·¨åŸŸä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºä¸“ç”¨å›¾å½¢LLMå’Œå›¾ç¥ç»ç½‘ç»œæ¨¡å‹ã€‚</li>
<li>æç¤ºå·¥ç¨‹åœ¨å¢å¼ºLLMå›¾å­¦ä¹ ä»»åŠ¡çš„æ€§èƒ½ä¸Šå…·æœ‰å·¨å¤§æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.15755">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-22947de5b0ca85a2ab0d15b3e83fd7cd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-383bff1bd441072db66486e92d768d92.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cc3e4c3b74a4043d04f0386def7c0ca9.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Instruction-Tuning-for-Story-Understanding-and-Generation-with-Weak-Supervision"><a href="#Instruction-Tuning-for-Story-Understanding-and-Generation-with-Weak-Supervision" class="headerlink" title="Instruction Tuning for Story Understanding and Generation with Weak   Supervision"></a>Instruction Tuning for Story Understanding and Generation with Weak   Supervision</h2><p><strong>Authors:Yangshu Yuan, Heng Chen, Christian Ng</strong></p>
<p>Story understanding and generation have long been a challenging task in natural language processing (NLP), especially when dealing with various levels of instruction specificity. In this paper, we propose a novel approach called â€œWeak to Strong Instruction Tuningâ€ for improving story generation by tuning models with instructions of varying clarity. We explore the potential of large language models (LLMs) to adapt to different types of instructions, weak and strong, and show that our method significantly enhances performance in story comprehension and generation. By leveraging the strength of instruction tuning, we train models to understand the nuances of story plots, characters, and themes while generating coherent and engaging narratives. Through extensive experiments on several benchmark datasets and comparison with state-of-the-art baselines, we demonstrate that our method outperforms existing techniques, yielding substantial improvements in both automatic evaluation metrics and human evaluations. Our work shows that adaptive instruction tuning can be a powerful tool in refining generative models for complex narrative tasks. </p>
<blockquote>
<p>æ•…äº‹ç†è§£å’Œç”Ÿæˆä¸€ç›´æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä¸­çš„ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†ä¸åŒå±‚æ¬¡çš„æŒ‡ä»¤ç‰¹å¼‚æ€§æ—¶æ›´æ˜¯å¦‚æ­¤ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•â€œå¼±åˆ°å¼ºæŒ‡ä»¤å¾®è°ƒâ€ï¼ˆWeak to Strong Instruction Tuningï¼‰ï¼Œé€šè¿‡ç”¨ä¸åŒæ¸…æ™°åº¦çš„æŒ‡ä»¤è°ƒæ•´æ¨¡å‹æ¥æ”¹å–„æ•…äº‹ç”Ÿæˆã€‚æˆ‘ä»¬æ¢ç´¢äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€‚åº”ä¸åŒç±»å‹æŒ‡ä»¤ï¼ˆæ— è®ºæ˜¯å¼±æŒ‡ä»¤è¿˜æ˜¯å¼ºæŒ‡ä»¤ï¼‰çš„æ½œåŠ›ï¼Œå¹¶è¯æ˜æˆ‘ä»¬çš„æ–¹æ³•åœ¨æ•…äº‹ç†è§£å’Œç”Ÿæˆæ–¹é¢çš„è¡¨ç°æœ‰äº†æ˜¾è‘—æå‡ã€‚é€šè¿‡åˆ©ç”¨æŒ‡ä»¤è°ƒæ•´çš„å¨åŠ›ï¼Œæˆ‘ä»¬è®­ç»ƒæ¨¡å‹ç†è§£æ•…äº‹æƒ…èŠ‚ã€è§’è‰²å’Œä¸»é¢˜çš„ç»†å¾®å·®åˆ«ï¼ŒåŒæ—¶ç”Ÿæˆè¿è´¯ä¸”å¼•äººå…¥èƒœçš„å™äº‹ã€‚é€šè¿‡å¤§é‡å®éªŒå’Œä¸æœ€æ–°åŸºçº¿æŠ€æœ¯çš„æ¯”è¾ƒï¼Œæˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œåœ¨è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡å’Œäººç±»è¯„ä¼°ä¸­éƒ½å–å¾—äº†å®è´¨æ€§çš„æ”¹è¿›ã€‚æˆ‘ä»¬çš„å·¥ä½œè¡¨æ˜ï¼Œè‡ªé€‚åº”æŒ‡ä»¤è°ƒæ•´å¯ä»¥æ˜¯æ”¹è¿›å¤æ‚å™äº‹ä»»åŠ¡çš„ç”Ÿæˆæ¨¡å‹çš„å¼ºå¤§å·¥å…·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.15574v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šé’ˆå¯¹è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„æ•…äº‹ç†è§£å’Œç”Ÿæˆä»»åŠ¡ï¼Œå°¤å…¶æ˜¯å¤„ç†ä¸åŒå±‚æ¬¡çš„æŒ‡ä»¤ç‰¹å®šæ€§é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºâ€œå¼±åˆ°å¼ºæŒ‡ä»¤è°ƒä¼˜â€çš„æ–°æ–¹æ³•ã€‚é€šè¿‡åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„æ½œåŠ›ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿé€‚åº”ä¸åŒç±»å‹çš„æŒ‡ä»¤ï¼Œæ— è®ºæ˜¯å¼±æŒ‡ä»¤è¿˜æ˜¯å¼ºæŒ‡ä»¤ï¼Œå¹¶æ˜¾è‘—æé«˜äº†æ•…äº‹ç†è§£å’Œç”Ÿæˆæ–¹é¢çš„æ€§èƒ½ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒå’Œä¸æœ€æ–°æŠ€æœ¯çš„æ¯”è¾ƒï¼Œè¯æ˜äº†è¯¥æ–¹æ³•åœ¨è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡å’Œäººç±»è¯„ä¼°æ–¹é¢çš„ä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºâ€œå¼±åˆ°å¼ºæŒ‡ä»¤è°ƒä¼˜â€çš„æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜æ•…äº‹ç”Ÿæˆæ€§èƒ½ã€‚</li>
<li>æ–¹æ³•çš„æ ¸å¿ƒåœ¨äºåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„æ½œåŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿé€‚åº”ä¸åŒæ¸…æ™°åº¦çš„æŒ‡ä»¤ã€‚</li>
<li>é€šè¿‡å¹¿æ³›çš„å®éªŒéªŒè¯ï¼Œè¯¥æ–¹æ³•åœ¨æ•…äº‹ç†è§£å’Œç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</li>
<li>ä¸ç°æœ‰æŠ€æœ¯ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡å’Œäººç±»è¯„ä¼°æ–¹é¢éƒ½è¡¨ç°å‡ºä¼˜è¶Šæ€§ã€‚</li>
<li>è¯¥æ–¹æ³•ä¸ä»…èƒ½å¤Ÿæé«˜æ•…äº‹ç”Ÿæˆçš„è¿è´¯æ€§å’Œå¸å¼•åŠ›ï¼Œè¿˜èƒ½æ›´å¥½åœ°ç†è§£æ•…äº‹æƒ…èŠ‚ã€è§’è‰²å’Œä¸»é¢˜ã€‚</li>
<li>è®ºæ–‡é€šè¿‡å¤šä¸ªåŸºå‡†æ•°æ®é›†è¿›è¡Œå®éªŒï¼Œè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.15574">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b5eaefa85cc54ae3018c2b696fd91e58.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9360a910ce6768c0ceb673b228e22d6c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8de25df90d9bb07334e04c0ad7b172de.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="TractoGPT-A-GPT-architecture-for-White-Matter-Segmentation"><a href="#TractoGPT-A-GPT-architecture-for-White-Matter-Segmentation" class="headerlink" title="TractoGPT: A GPT architecture for White Matter Segmentation"></a>TractoGPT: A GPT architecture for White Matter Segmentation</h2><p><strong>Authors:Anoushkrit Goel, Simroop Singh, Ankita Joshi, Ranjeet Ranjan Jha, Chirag Ahuja, Aditya Nigam, Arnav Bhavsar</strong></p>
<p>White matter bundle segmentation is crucial for studying brain structural connectivity, neurosurgical planning, and neurological disorders. White Matter Segmentation remains challenging due to structural similarity in streamlines, subject variability, symmetry in 2 hemispheres, etc. To address these challenges, we propose TractoGPT, a GPT-based architecture trained on streamline, cluster, and fusion data representations separately. TractoGPT is a fully-automatic method that generalizes across datasets and retains shape information of the white matter bundles. Experiments also show that TractoGPT outperforms state-of-the-art methods on average DICE, Overlap and Overreach scores. We use TractoInferno and 105HCP datasets and validate generalization across dataset. </p>
<blockquote>
<p>ç™½è´¨æŸåˆ†å‰²å¯¹äºç ”ç©¶è„‘ç»“æ„è¿æ¥ã€ç¥ç»æ‰‹æœ¯è§„åˆ’å’Œç¥ç»ç–¾ç—…å…·æœ‰é‡è¦æ„ä¹‰ã€‚ç”±äºæµçº¿ç»“æ„ç›¸ä¼¼æ€§ã€å—è¯•è€…å·®å¼‚æ€§ã€ä¸¤ä¸ªåŠçƒçš„å¯¹ç§°æ€§ç­‰ï¼Œç™½è´¨åˆ†å‰²ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†TractoGPTï¼Œè¿™æ˜¯ä¸€ç§åŸºäºGPTçš„æ¶æ„ï¼Œåˆ†åˆ«è®­ç»ƒæµçº¿ã€é›†ç¾¤å’Œèåˆæ•°æ®è¡¨ç¤ºã€‚TractoGPTæ˜¯ä¸€ç§å…¨è‡ªåŠ¨æ–¹æ³•ï¼Œå¯è·¨æ•°æ®é›†æ¨å¹¿å¹¶ä¿ç•™ç™½è´¨æŸçš„å½¢çŠ¶ä¿¡æ¯ã€‚å®éªŒè¿˜è¡¨æ˜ï¼ŒTractoGPTåœ¨å¹³å‡DICEã€é‡å å’Œè¶…å‡ºå¾—åˆ†æ–¹é¢ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚æˆ‘ä»¬ä½¿ç”¨TractoInfernoå’Œ105HCPæ•°æ®é›†ï¼Œå¹¶éªŒè¯è·¨æ•°æ®é›†çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.15464v1">PDF</a> Accepted as a conference paper at 23rd IEEE International Symposium   on Biomedical Imaging 2025. IEEE holds the copyright for this publication</p>
<p><strong>Summary</strong><br>æœ¬æ–‡ä¸»è¦ä»‹ç»äº†ç™½è´¨æŸåˆ†å‰²åœ¨è„‘ç»“æ„è¿æ¥æ€§ç ”ç©¶ä¸­çš„é‡è¦æ€§åŠå…¶åœ¨ç¥ç»æ‰‹æœ¯è§„åˆ’å’Œç¥ç»éšœç¢ç ”ç©¶ä¸­çš„å…³é”®åº”ç”¨ã€‚æ–‡ç« æå‡ºäº†ä¸€ç§åä¸ºTractoGPTçš„GPTæ¶æ„ä»¥è§£å†³ç™½è´¨åˆ†å‰²çš„æŒ‘æˆ˜æ€§é—®é¢˜ï¼Œè¯¥æ¶æ„èƒ½å¤„ç†æµçº¿ã€ç°‡å’Œèåˆæ•°æ®è¡¨ç¤ºï¼Œå…·æœ‰å…¨è‡ªåŠ¨æ€§ã€è·¨æ•°æ®é›†æ³›åŒ–èƒ½åŠ›å’Œä¿ç•™ç™½è´¨æŸå½¢çŠ¶ä¿¡æ¯çš„ç‰¹ç‚¹ã€‚å®éªŒè¡¨æ˜ï¼ŒTractoGPTåœ¨å¹³å‡DICEã€é‡å å’Œè¶…è¶Šå¾—åˆ†ä¸Šä¼˜äºç°æœ‰æŠ€æœ¯ã€‚ä½¿ç”¨TractoInfernoå’Œ105HCPæ•°æ®é›†éªŒè¯äº†å…¶åœ¨ä¸åŒæ•°æ®é›†ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ç™½è´¨æŸåˆ†å‰²åœ¨ç ”ç©¶è„‘ç»“æ„è¿æ¥æ€§ã€ç¥ç»æ‰‹æœ¯è§„åˆ’å’Œç¥ç»éšœç¢ç­‰æ–¹é¢å…·æœ‰é‡è¦æ„ä¹‰ã€‚</li>
<li>å½“å‰ç ”ç©¶ä¸­ï¼Œç™½è´¨åˆ†å‰²é¢ä¸´å¦‚æµçº¿ç»“æ„ç›¸ä¼¼æ€§ã€ä¸ªä½“å·®å¼‚ã€å·¦å³åŠçƒå¯¹ç§°æ€§ç­‰æŒ‘æˆ˜ã€‚</li>
<li>TractoGPTæ˜¯ä¸€ä¸ªåŸºäºGPTæ¶æ„çš„è§£å†³æ–¹æ¡ˆï¼Œå¯åˆ†åˆ«å¤„ç†æµçº¿ã€ç°‡å’Œèåˆæ•°æ®è¡¨ç¤ºå½¢å¼ã€‚</li>
<li>TractoGPTå…·æœ‰å…¨è‡ªåŠ¨æ€§ã€è·¨æ•°æ®é›†æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½ä¿ç•™ç™½è´¨æŸçš„å½¢çŠ¶ä¿¡æ¯ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒTractoGPTåœ¨å¹³å‡DICEå¾—åˆ†ã€é‡å å¾—åˆ†å’Œè¶…è¶Šå¾—åˆ†æ–¹é¢ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.15464">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0ae9f0f40879aa53b949c7edd1a33fa0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f6ff294cb3877e42ce1aaba1610c8278.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-764cfb168e0a22cace6d4a7d55ddaf23.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-45bdf0c850cccce5634b1e7ad0188c31.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="OpenCharacter-Training-Customizable-Role-Playing-LLMs-with-Large-Scale-Synthetic-Personas"><a href="#OpenCharacter-Training-Customizable-Role-Playing-LLMs-with-Large-Scale-Synthetic-Personas" class="headerlink" title="OpenCharacter: Training Customizable Role-Playing LLMs with Large-Scale   Synthetic Personas"></a>OpenCharacter: Training Customizable Role-Playing LLMs with Large-Scale   Synthetic Personas</h2><p><strong>Authors:Xiaoyang Wang, Hongming Zhang, Tao Ge, Wenhao Yu, Dian Yu, Dong Yu</strong></p>
<p>Customizable role-playing in large language models (LLMs), also known as character generalization, is gaining increasing attention for its versatility and cost-efficiency in developing and deploying role-playing dialogue agents. This study explores a large-scale data synthesis approach to equip LLMs with character generalization capabilities. We begin by synthesizing large-scale character profiles using personas from Persona Hub and then explore two strategies: response rewriting and response generation, to create character-aligned instructional responses. To validate the effectiveness of our synthetic instruction tuning data for character generalization, we perform supervised fine-tuning (SFT) using the LLaMA-3 8B model. Our best-performing model strengthens the original LLaMA-3 8B Instruct model and achieves performance comparable to GPT-4o models on role-playing dialogue. We release our synthetic characters and instruction-tuning dialogues to support public research. </p>
<blockquote>
<p>åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„è§’è‰²å®šåˆ¶ï¼Œä¹Ÿè¢«ç§°ä¸ºè§’è‰²æ³›åŒ–ï¼Œç”±äºå…¶å¤šæ‰å¤šè‰ºå’Œåœ¨å¼€å‘å’Œéƒ¨ç½²è§’è‰²æ‰®æ¼”å¯¹è¯ä»£ç†æ–¹é¢çš„æˆæœ¬æ•ˆç›Šï¼Œè€Œè¶Šæ¥è¶Šå—åˆ°äººä»¬çš„å…³æ³¨ã€‚æœ¬ç ”ç©¶æ¢ç´¢äº†ä¸€ç§å¤§è§„æ¨¡æ•°æ®åˆæˆæ–¹æ³•æ¥ä¸ºLLMæä¾›è§’è‰²æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬é¦–å…ˆä½¿ç”¨Persona Hubä¸­çš„äººç‰©æ¥åˆæˆå¤§è§„æ¨¡çš„è§’è‰²æè¿°ï¼Œç„¶åæ¢ç´¢ä¸¤ç§ç­–ç•¥ï¼šå“åº”é‡å†™å’Œå“åº”ç”Ÿæˆï¼Œæ¥åˆ›å»ºä¸è§’è‰²å¯¹é½çš„æŒ‡ä»¤å“åº”ã€‚ä¸ºäº†éªŒè¯æˆ‘ä»¬åˆæˆçš„æŒ‡ä»¤å¾®è°ƒæ•°æ®åœ¨è§’è‰²æ³›åŒ–æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬ä½¿ç”¨LLaMA-3 8Bæ¨¡å‹è¿›è¡Œæœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€‚æˆ‘ä»¬è¡¨ç°æœ€å¥½çš„æ¨¡å‹åŠ å¼ºäº†åŸå§‹çš„LLaMA-3 8BæŒ‡ä»¤æ¨¡å‹ï¼Œå¹¶åœ¨è§’è‰²æ‰®æ¼”å¯¹è¯æ–¹é¢çš„æ€§èƒ½ä¸GPT-4oæ¨¡å‹ç›¸å½“ã€‚æˆ‘ä»¬å‘å¸ƒæˆ‘ä»¬åˆæˆçš„è§’è‰²å’ŒæŒ‡ä»¤è°ƒæ•´å¯¹è¯ä»¥æ”¯æŒå…¬å¼€ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.15427v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„è§’è‰²å®šåˆ¶ï¼ˆåˆç§°è§’è‰²æ³›åŒ–ï¼‰å› å…¶çµæ´»æ€§å’Œå¼€å‘éƒ¨ç½²è§’è‰²å¯¹è¯ä»£ç†çš„æˆæœ¬æ•ˆç›Šè€Œå—åˆ°è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ã€‚æœ¬ç ”ç©¶æ¢ç´¢äº†ä¸€ç§å¤§è§„æ¨¡æ•°æ®åˆæˆæ–¹æ³•æ¥ä¸ºLLMæä¾›è§’è‰²æ³›åŒ–èƒ½åŠ›ã€‚é¦–å…ˆï¼Œä½¿ç”¨Persona Hubä¸­çš„äººç‰©åˆæˆå¤§è§„æ¨¡è§’è‰²ç‰¹å¾ï¼Œç„¶åæ¢ç´¢ä¸¤ç§ç­–ç•¥ï¼šå“åº”é‡å†™å’Œå“åº”ç”Ÿæˆï¼Œä»¥åˆ›å»ºä¸è§’è‰²å¯¹é½çš„æŒ‡ä»¤å“åº”ã€‚ä¸ºäº†éªŒè¯æˆ‘ä»¬åˆæˆæŒ‡ä»¤å¾®è°ƒæ•°æ®åœ¨è§’è‰²æ³›åŒ–æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬ä½¿ç”¨LLaMA-3 8Bæ¨¡å‹è¿›è¡Œæœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€‚æˆ‘ä»¬è¡¨ç°æœ€ä½³çš„æ¨¡å‹å¼ºåŒ–äº†åŸå§‹çš„LLaMA-3 8BæŒ‡ä»¤æ¨¡å‹ï¼Œåœ¨è§’è‰²æ‰®æ¼”å¯¹è¯æ–¹é¢çš„æ€§èƒ½ä¸GPT-4oæ¨¡å‹ç›¸å½“ã€‚æˆ‘ä»¬å‘å¸ƒåˆæˆè§’è‰²å’ŒæŒ‡ä»¤è°ƒæ•´å¯¹è¯ä»¥æ”¯æŒå…¬å¼€ç ”ç©¶ã€‚</p>
<p><strong>è¦ç‚¹æç‚¼</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„è§’è‰²å®šåˆ¶å—åˆ°å…³æ³¨ï¼Œå› ä¸ºå®ƒå¢åŠ äº†æ¨¡å‹çš„çµæ´»æ€§å’Œå¼€å‘éƒ¨ç½²æˆæœ¬æ•ˆç›Šã€‚</li>
<li>é€šè¿‡æ•°æ®åˆæˆæ–¹æ³•ä¸ºLLMèµ‹äºˆè§’è‰²æ³›åŒ–èƒ½åŠ›æ˜¯ä¸€ç§æ–°å…´ç­–ç•¥ã€‚</li>
<li>ç ”ç©¶è€…ä½¿ç”¨Persona Hubæ¥åˆæˆå¤§è§„æ¨¡è§’è‰²ç‰¹å¾ã€‚</li>
<li>æœ‰ä¸¤ç§ç­–ç•¥ç”¨äºåˆ›å»ºä¸è§’è‰²å¯¹é½çš„æŒ‡ä»¤å“åº”ï¼šå“åº”é‡å†™å’Œå“åº”ç”Ÿæˆã€‚</li>
<li>å¯¹LLaMA-3 8Bæ¨¡å‹è¿›è¡Œæœ‰ç›‘ç£å¾®è°ƒåï¼Œæ€§èƒ½å¾—åˆ°æ”¹è¿›ï¼Œå¹¶æ¥è¿‘GPT-4oæ¨¡å‹åœ¨è§’è‰²æ‰®æ¼”å¯¹è¯æ–¹é¢çš„è¡¨ç°ã€‚</li>
<li>ç ”ç©¶è€…å‘å¸ƒçš„åˆæˆè§’è‰²å’ŒæŒ‡ä»¤è°ƒæ•´å¯¹è¯å¯ä¸ºå…¬ä¼—ç ”ç©¶æä¾›æ”¯æŒã€‚</li>
<li>æ­¤æ–¹æ³•å¯¹äºå¼€å‘æ›´å…·è¡¨ç°åŠ›å’Œé€‚åº”æ€§çš„å¯¹è¯ä»£ç†å…·æœ‰æ½œåœ¨æ„ä¹‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.15427">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-300a7428f7a12b659d5002902ad086eb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bbf8ffbb1f8f0dd502d8dff02fd396c6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-52893481f520f7317ccd75365c1fc5cb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0be37e2beea1aef4f4e249a313b6ea53.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6da12a949d262d05370b0cc3252ab74f.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Mirage-in-the-Eyes-Hallucination-Attack-on-Multi-modal-Large-Language-Models-with-Only-Attention-Sink"><a href="#Mirage-in-the-Eyes-Hallucination-Attack-on-Multi-modal-Large-Language-Models-with-Only-Attention-Sink" class="headerlink" title="Mirage in the Eyes: Hallucination Attack on Multi-modal Large Language   Models with Only Attention Sink"></a>Mirage in the Eyes: Hallucination Attack on Multi-modal Large Language   Models with Only Attention Sink</h2><p><strong>Authors:Yining Wang, Mi Zhang, Junjie Sun, Chenyue Wang, Min Yang, Hui Xue, Jialing Tao, Ranjie Duan, Jiexi Liu</strong></p>
<p>Fusing visual understanding into language generation, Multi-modal Large Language Models (MLLMs) are revolutionizing visual-language applications. Yet, these models are often plagued by the hallucination problem, which involves generating inaccurate objects, attributes, and relationships that do not match the visual content. In this work, we delve into the internal attention mechanisms of MLLMs to reveal the underlying causes of hallucination, exposing the inherent vulnerabilities in the instruction-tuning process.   We propose a novel hallucination attack against MLLMs that exploits attention sink behaviors to trigger hallucinated content with minimal image-text relevance, posing a significant threat to critical downstream applications. Distinguished from previous adversarial methods that rely on fixed patterns, our approach generates dynamic, effective, and highly transferable visual adversarial inputs, without sacrificing the quality of model responses. Comprehensive experiments on 6 prominent MLLMs demonstrate the efficacy of our attack in compromising black-box MLLMs even with extensive mitigating mechanisms, as well as the promising results against cutting-edge commercial APIs, such as GPT-4o and Gemini 1.5. Our code is available at <a target="_blank" rel="noopener" href="https://huggingface.co/RachelHGF/Mirage-in-the-Eyes">https://huggingface.co/RachelHGF/Mirage-in-the-Eyes</a>. </p>
<blockquote>
<p>å°†è§†è§‰ç†è§£èå…¥è¯­è¨€ç”Ÿæˆä¸­ï¼Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ­£åœ¨è§†è§‰è¯­è¨€åº”ç”¨é¢†åŸŸæ€èµ·é©å‘½ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹ç»å¸¸å—åˆ°å¹»è§‰é—®é¢˜çš„å›°æ‰°ï¼ŒåŒ…æ‹¬ç”Ÿæˆä¸å‡†ç¡®çš„å¯¹è±¡ã€å±æ€§å’Œå…³ç³»ï¼Œå®ƒä»¬ä¸è§†è§‰å†…å®¹ä¸åŒ¹é…ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ·±å…¥ç ”ç©¶äº†MLLMsçš„å†…éƒ¨æ³¨æ„åŠ›æœºåˆ¶ï¼Œä»¥æ­ç¤ºå¹»è§‰é—®é¢˜çš„æ ¹æœ¬åŸå› ï¼Œå¹¶æš´éœ²äº†æŒ‡ä»¤è°ƒæ•´è¿‡ç¨‹ä¸­çš„å›ºæœ‰æ¼æ´ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§é’ˆå¯¹MLLMçš„æ–°å‹å¹»è§‰æ”»å‡»ï¼Œè¯¥æ”»å‡»åˆ©ç”¨æ³¨æ„åŠ›æ±‡èšè¡Œä¸ºè§¦å‘ä¸å›¾åƒæ–‡æœ¬ç›¸å…³æ€§æœ€å°çš„å¹»è§‰å†…å®¹ï¼Œå¯¹å…³é”®ä¸‹æ¸¸åº”ç”¨ç¨‹åºæ„æˆäº†é‡å¤§å¨èƒã€‚ä¸ä¹‹å‰ä¾èµ–äºå›ºå®šæ¨¡å¼çš„å¯¹æŠ—æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç”ŸæˆåŠ¨æ€ã€æœ‰æ•ˆä¸”é«˜åº¦å¯è½¬ç§»çš„è§†è§‰å¯¹æŠ—è¾“å…¥ï¼Œä¸ä¼šç‰ºç‰²æ¨¡å‹å“åº”çš„è´¨é‡ã€‚åœ¨6ä¸ªçªå‡ºçš„MLLMä¸Šè¿›è¡Œçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ”»å‡»å³ä½¿åœ¨æœ‰å¹¿æ³›ç¼“è§£æœºåˆ¶çš„é»‘ç›’MLLMä¸­ä¹Ÿèƒ½æœ‰æ•ˆå‘æŒ¥ä½œç”¨ï¼Œå¹¶ä¸”åœ¨æœ€å‰æ²¿çš„å•†ä¸šAPIï¼ˆå¦‚GPT-4oå’ŒGemini 1.5ï¼‰ä¸Šä¹Ÿèƒ½å–å¾—ä»¤äººé¼“èˆçš„ç»“æœã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://huggingface.co/RachelHGF/Mirage-in-the-Eyes%E6%89%BE%E5%88%B0%E3%80%82">https://huggingface.co/RachelHGF/Mirage-in-the-Eyesæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.15269v1">PDF</a> USENIX Security 2025</p>
<p><strong>Summary</strong></p>
<p>å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰èåˆäº†è§†è§‰ç†è§£ä¸è¯­è¨€ç”Ÿæˆï¼Œæ­£åœ¨æ¨åŠ¨è§†è§‰è¯­è¨€åº”ç”¨çš„é©æ–°ã€‚ç„¶è€Œï¼Œæ¨¡å‹å¸¸é­é‡â€œå¹»è§‰â€é—®é¢˜ï¼Œå³ç”Ÿæˆä¸è§†è§‰å†…å®¹ä¸åŒ¹é…çš„ç‰©ä½“ã€å±æ€§å’Œå…³ç³»ã€‚æœ¬ç ”ç©¶æ·±å…¥æ¢ç©¶MLLMsçš„å†…éƒ¨æ³¨æ„åŠ›æœºåˆ¶ï¼Œæ­ç¤ºå¹»è§‰é—®é¢˜èƒŒåçš„åŸå› ï¼Œæš´éœ²æŒ‡ä»¤å¾®è°ƒè¿‡ç¨‹çš„å›ºæœ‰æ¼æ´ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§é’ˆå¯¹MLLMsçš„æ–°å‹å¹»è§‰æ”»å‡»æ–¹æ³•ï¼Œåˆ©ç”¨æ³¨æ„åŠ›ä¸‹æ²‰è¡Œä¸ºè§¦å‘ä¸å›¾åƒæ–‡æœ¬ç›¸å…³æ€§æä½çš„å¹»è§‰å†…å®¹ï¼Œå¯¹å…³é”®ä¸‹æ¸¸åº”ç”¨æ„æˆé‡å¤§å¨èƒã€‚ä¸ä»¥å¾€ä¾èµ–å›ºå®šæ¨¡å¼å¯¹æŠ—æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½ç”ŸæˆåŠ¨æ€ã€é«˜æ•ˆã€é«˜åº¦è¿ç§»æ€§çš„è§†è§‰å¯¹æŠ—è¾“å…¥ï¼Œä¸”ä¸ç‰ºç‰²æ¨¡å‹å“åº”è´¨é‡ã€‚åœ¨6ä¸ªä¸»æµMLLMsä¸Šçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ”»å‡»æ–¹æ³•å³ä½¿åœ¨å¼ºå¤§çš„ç¼“è§£æœºåˆ¶ä¸‹ï¼Œä¹Ÿèƒ½æœ‰æ•ˆå½±å“é»‘ç›’MLLMsï¼Œå¹¶ä¸”å¯¹å‰æ²¿å•†ä¸šAPIå¦‚GPT-4oå’ŒGemini 1.5ä¹Ÿæ˜¾ç¤ºå‡ºè‰¯å¥½æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰èåˆè§†è§‰ç†è§£ä¸è¯­è¨€ç”Ÿæˆï¼Œæ¨åŠ¨è§†è§‰è¯­è¨€åº”ç”¨å‘å±•ã€‚</li>
<li>MLLMsé¢ä¸´â€œå¹»è§‰â€é—®é¢˜ï¼Œç”Ÿæˆå†…å®¹ä¸è§†è§‰ä¸åŒ¹é…ã€‚</li>
<li>ç ”ç©¶æ­ç¤ºMLLMså†…éƒ¨æ³¨æ„åŠ›æœºåˆ¶æ˜¯å¹»è§‰é—®é¢˜çš„æ ¹æœ¬åŸå› ã€‚</li>
<li>æå‡ºä¸€ç§æ–°å‹å¹»è§‰æ”»å‡»æ–¹æ³•ï¼Œåˆ©ç”¨æ³¨æ„åŠ›ä¸‹æ²‰è¡Œä¸ºè§¦å‘å¹»è§‰å†…å®¹ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½ç”ŸæˆåŠ¨æ€ã€é«˜æ•ˆçš„è§†è§‰å¯¹æŠ—è¾“å…¥ï¼Œå¯¹MLLMsæ„æˆå¨èƒã€‚</li>
<li>æ”»å‡»æ–¹æ³•æœ‰æ•ˆå½±å“é»‘ç›’MLLMsï¼Œå¯¹å•†ä¸šAPIå¦‚GPT-4oå’ŒGemini 1.5ä¹Ÿæœ‰æ•ˆã€‚</li>
<li>ç›¸å…³ä»£ç å·²å…¬å¼€åˆ†äº«ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.15269">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-518bed30b920c9705faf9c0add979de0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5de6e96d0ef22d23e83286fdef86d0eb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-37e067d1fe373449088291f1c4bea1d5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9329d5aa6022c1ff08a11058f2edf342.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fddf0ed1426a68c4ff73c0bd40d7d52d.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="MoColl-Agent-Based-Specific-and-General-Model-Collaboration-for-Image-Captioning"><a href="#MoColl-Agent-Based-Specific-and-General-Model-Collaboration-for-Image-Captioning" class="headerlink" title="MoColl: Agent-Based Specific and General Model Collaboration for Image   Captioning"></a>MoColl: Agent-Based Specific and General Model Collaboration for Image   Captioning</h2><p><strong>Authors:Pu Yang, Bin Dong</strong></p>
<p>Image captioning is a critical task at the intersection of computer vision and natural language processing, with wide-ranging applications across various domains. For complex tasks such as diagnostic report generation, deep learning models require not only domain-specific image-caption datasets but also the incorporation of relevant general knowledge to provide contextual accuracy. Existing approaches exhibit inherent limitations: specialized models excel in capturing domain-specific details but lack generalization, while vision-language models (VLMs) built on large language models (LLMs) leverage general knowledge but struggle with domain-specific adaptation. To address these limitations, this paper proposes a novel agent-enhanced model collaboration framework, which we call MoColl, designed to effectively integrate domain-specific and general knowledge. Specifically, our approach is to decompose complex image captioning tasks into a series of interconnected question-answer subtasks. A trainable visual question answering (VQA) model is employed as a specialized tool to focus on domain-specific visual analysis, answering task-specific questions based on image content. Concurrently, an LLM-based agent with general knowledge formulates these questions and synthesizes the resulting question-answer pairs into coherent captions. Beyond its role in leveraging the VQA model, the agent further guides its training to enhance its domain-specific capabilities. Experimental results on radiology report generation validate the effectiveness of the proposed framework, demonstrating significant improvements in the quality of generated reports. </p>
<blockquote>
<p>å›¾åƒæè¿°æ˜¯è®¡ç®—æœºè§†è§‰å’Œè‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸçš„äº¤å‰ä»»åŠ¡ï¼Œåœ¨å„ä¸ªé¢†åŸŸéƒ½æœ‰å¹¿æ³›çš„åº”ç”¨ã€‚å¯¹äºè¯Šæ–­æŠ¥å‘Šç”Ÿæˆç­‰å¤æ‚ä»»åŠ¡ï¼Œæ·±åº¦å­¦ä¹ æ¨¡å‹ä¸ä»…éœ€è¦ç‰¹å®šé¢†åŸŸçš„å›¾åƒæè¿°æ•°æ®é›†ï¼Œè¿˜éœ€è¦èå…¥ç›¸å…³çš„é€šç”¨çŸ¥è¯†ä»¥æä¾›ä¸Šä¸‹æ–‡å‡†ç¡®æ€§ã€‚ç°æœ‰æ–¹æ³•å­˜åœ¨å›ºæœ‰çš„å±€é™æ€§ï¼šä¸“ä¸šæ¨¡å‹æ“…é•¿æ•æ‰ç‰¹å®šé¢†åŸŸçš„ç»†èŠ‚ï¼Œä½†ç¼ºä¹æ³›åŒ–èƒ½åŠ›ï¼›è€ŒåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åˆ©ç”¨äº†ä¸€èˆ¬çŸ¥è¯†ï¼Œä½†åœ¨ç‰¹å®šé¢†åŸŸçš„é€‚åº”æ–¹é¢å´è¡¨ç°å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºMoCollçš„æ–°å‹ä»£ç†å¢å¼ºæ¨¡å‹åä½œæ¡†æ¶ï¼Œæ—¨åœ¨æœ‰æ•ˆåœ°æ•´åˆç‰¹å®šé¢†åŸŸçŸ¥è¯†å’Œä¸€èˆ¬çŸ¥è¯†ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¯å°†å¤æ‚çš„å›¾åƒæè¿°ä»»åŠ¡åˆ†è§£ä¸ºä¸€ç³»åˆ—ç›¸äº’å…³è”çš„é—®ç­”å­ä»»åŠ¡ã€‚æˆ‘ä»¬é‡‡ç”¨å¯è®­ç»ƒçš„è§†è§‰é—®ç­”ï¼ˆVQAï¼‰æ¨¡å‹ä½œä¸ºä¸“ä¸šå·¥å…·ï¼Œä¸“æ³¨äºç‰¹å®šé¢†åŸŸçš„è§†è§‰åˆ†æï¼Œæ ¹æ®å›¾åƒå†…å®¹å›ç­”ç‰¹å®šä»»åŠ¡çš„é—®é¢˜ã€‚åŒæ—¶ï¼Œä¸€ä¸ªåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„ä»£ç†åˆ©ç”¨é€šç”¨çŸ¥è¯†æ¥åˆ¶å®šè¿™äº›é—®é¢˜ï¼Œå¹¶å°†å¾—åˆ°çš„é—®é¢˜ç­”æ¡ˆå¯¹åˆæˆè¿è´¯çš„æè¿°ã€‚é™¤äº†åˆ©ç”¨VQAæ¨¡å‹çš„ä½œç”¨å¤–ï¼Œè¯¥ä»£ç†è¿˜è¿›ä¸€æ­¥æŒ‡å¯¼å…¶è®­ç»ƒï¼Œä»¥å¢å¼ºå…¶ç‰¹å®šé¢†åŸŸçš„èƒ½åŠ›ã€‚åœ¨æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆæ–¹é¢çš„å®éªŒç»“æœéªŒè¯äº†æ‰€æå‡ºæ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼Œæ˜¾ç¤ºå‡ºç”Ÿæˆçš„æŠ¥å‘Šè´¨é‡æœ‰æ˜¾è‘—æé«˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.01834v3">PDF</a> </p>
<p><strong>Summary</strong><br>     å›¾åƒæè¿°æ˜¯è®¡ç®—æœºè§†è§‰å’Œè‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸçš„æ ¸å¿ƒä»»åŠ¡ï¼Œå¹¿æ³›åº”ç”¨äºå„ä¸ªé¢†åŸŸã€‚é’ˆå¯¹è¯Šæ–­æŠ¥å‘Šç”Ÿæˆç­‰å¤æ‚ä»»åŠ¡ï¼Œæ·±åº¦å­¦ä¹ æ¨¡å‹ä¸ä»…éœ€è¦é¢†åŸŸç‰¹å®šçš„å›¾åƒæè¿°æ•°æ®é›†ï¼Œè¿˜éœ€è¦èå…¥ç›¸å…³çš„é€šç”¨çŸ¥è¯†ä»¥æé«˜ä¸Šä¸‹æ–‡å‡†ç¡®æ€§ã€‚ç°æœ‰æ–¹æ³•å­˜åœ¨å›ºæœ‰å±€é™æ€§ï¼šç‰¹å®šé¢†åŸŸæ¨¡å‹æ“…é•¿æ•æ‰é¢†åŸŸç»†èŠ‚ä½†ç¼ºä¹æ³›åŒ–èƒ½åŠ›ï¼Œè€ŒåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åˆ©ç”¨é€šç”¨çŸ¥è¯†ä½†åœ¨é¢†åŸŸç‰¹å®šé€‚åº”æ–¹é¢é‡åˆ°å›°éš¾ã€‚ä¸ºè§£å†³è¿™äº›å±€é™æ€§ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„äººæœºåä½œæ¡†æ¶â€”â€”MoCollï¼Œæ—¨åœ¨æœ‰æ•ˆæ•´åˆé¢†åŸŸç‰¹å®šçŸ¥è¯†å’Œé€šç”¨çŸ¥è¯†ã€‚è¯¥æ¡†æ¶é€šè¿‡å°†å¤æ‚çš„å›¾åƒæè¿°ä»»åŠ¡åˆ†è§£ä¸ºä¸€ç³»åˆ—ç›¸äº’å…³è”çš„é—®é¢˜å›ç­”å­ä»»åŠ¡æ¥è§£å†³ã€‚ä½¿ç”¨å¯è®­ç»ƒçš„è§†è§‰é—®ç­”ï¼ˆVQAï¼‰æ¨¡å‹ä½œä¸ºä¸“ç”¨å·¥å…·ï¼Œä¸“æ³¨äºé¢†åŸŸç‰¹å®šçš„è§†è§‰åˆ†æï¼Œå¹¶æ ¹æ®å›¾åƒå†…å®¹å›ç­”ç‰¹å®šä»»åŠ¡çš„é—®é¢˜ã€‚åŒæ—¶ï¼Œå…·æœ‰é€šç”¨çŸ¥è¯†çš„LLMæ™ºèƒ½ä½“åˆ¶å®šè¿™äº›é—®é¢˜å¹¶å°†é—®ç­”å¯¹åˆæˆè¿è´¯çš„æè¿°ã€‚é™¤äº†å¼•å¯¼VQAæ¨¡å‹çš„ä½œç”¨å¤–ï¼Œæ™ºèƒ½ä½“è¿˜è¿›ä¸€æ­¥æŒ‡å¯¼å…¶è®­ç»ƒï¼Œä»¥æé«˜é¢†åŸŸç‰¹å®šèƒ½åŠ›ã€‚åœ¨æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆæ–¹é¢çš„å®éªŒéªŒè¯äº†æ‰€ææ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼Œæ˜¾ç¤ºå‡ºç”Ÿæˆçš„æŠ¥å‘Šè´¨é‡æ˜¾è‘—æé«˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å›¾åƒæè¿°æ˜¯è®¡ç®—æœºè§†è§‰å’Œè‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸçš„äº¤å‰ä»»åŠ¡ï¼Œå¹¿æ³›åº”ç”¨äºå¤šä¸ªé¢†åŸŸã€‚</li>
<li>ç°æœ‰æ–¹æ³•åœ¨ç»“åˆé¢†åŸŸç‰¹å®šçŸ¥è¯†å’Œé€šç”¨çŸ¥è¯†æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>MoCollæ¡†æ¶é€šè¿‡åˆ†è§£å¤æ‚ä»»åŠ¡ä¸ºé—®é¢˜å›ç­”å­ä»»åŠ¡æ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ã€‚</li>
<li>VQAæ¨¡å‹ä¸“æ³¨äºé¢†åŸŸç‰¹å®šçš„è§†è§‰åˆ†æï¼Œå›ç­”ä¸å›¾åƒå†…å®¹ç›¸å…³çš„ç‰¹å®šä»»åŠ¡é—®é¢˜ã€‚</li>
<li>LLMæ™ºèƒ½ä½“åˆ©ç”¨é€šç”¨çŸ¥è¯†åˆ¶å®šé—®é¢˜å¹¶å°†é—®ç­”å¯¹ç»„åˆæˆè¿è´¯æè¿°ã€‚</li>
<li>æ™ºèƒ½ä½“ä¸ä»…å¼•å¯¼VQAæ¨¡å‹ï¼Œè¿˜é€šè¿‡è®­ç»ƒæé«˜å…¶é¢†åŸŸç‰¹å®šèƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.01834">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-cb69644e91a7b349961cf1c04f25d063.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-811ad4a5b368c246b4952b26c019fc17.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f0283a42a9aa48959a6aa7894357d615.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-04a640bc94b9f14871940594caf37f26.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Hands-On-Tutorial-Labeling-with-LLM-and-Human-in-the-Loop"><a href="#Hands-On-Tutorial-Labeling-with-LLM-and-Human-in-the-Loop" class="headerlink" title="Hands-On Tutorial: Labeling with LLM and Human-in-the-Loop"></a>Hands-On Tutorial: Labeling with LLM and Human-in-the-Loop</h2><p><strong>Authors:Ekaterina Artemova, Akim Tsvigun, Dominik Schlechtweg, Natalia Fedorova, Konstantin Chernyshev, Sergei Tilga, Boris Obmoroshev</strong></p>
<p>Training and deploying machine learning models relies on a large amount of human-annotated data. As human labeling becomes increasingly expensive and time-consuming, recent research has developed multiple strategies to speed up annotation and reduce costs and human workload: generating synthetic training data, active learning, and hybrid labeling. This tutorial is oriented toward practical applications: we will present the basics of each strategy, highlight their benefits and limitations, and discuss in detail real-life case studies. Additionally, we will walk through best practices for managing human annotators and controlling the quality of the final dataset. The tutorial includes a hands-on workshop, where attendees will be guided in implementing a hybrid annotation setup. This tutorial is designed for NLP practitioners from both research and industry backgrounds who are involved in or interested in optimizing data labeling projects. </p>
<blockquote>
<p>æœºå™¨å­¦ä¹ æ¨¡å‹çš„è®­ç»ƒä¸éƒ¨ç½²ä¾èµ–äºå¤§é‡äººå·¥æ ‡æ³¨æ•°æ®ã€‚éšç€äººå·¥æ ‡æ³¨çš„æˆæœ¬è¶Šæ¥è¶Šé«˜ä¸”è€—æ—¶è¶Šæ¥è¶Šé•¿ï¼Œè¿‘æœŸçš„ç ”ç©¶å·²ç»å¼€å‘å‡ºå¤šç§ç­–ç•¥æ¥åŠ é€Ÿæ ‡æ³¨è¿‡ç¨‹å¹¶é™ä½æˆæœ¬å’ŒäººåŠ›å·¥ä½œé‡ï¼šç”Ÿæˆåˆæˆè®­ç»ƒæ•°æ®ã€ä¸»åŠ¨å­¦ä¹ å’Œæ··åˆæ ‡æ³¨ã€‚æœ¬æ•™ç¨‹ä»¥å®é™…åº”ç”¨ä¸ºå¯¼å‘ï¼šæˆ‘ä»¬å°†ä»‹ç»æ¯ç§ç­–ç•¥çš„åŸºç¡€çŸ¥è¯†ï¼Œçªå‡ºå…¶ä¼˜ç‚¹å’Œå±€é™æ€§ï¼Œå¹¶è¯¦ç»†è®¨è®ºç°å®æ¡ˆä¾‹ç ”ç©¶ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å°†è®²è§£ç®¡ç†äººå·¥æ ‡æ³¨è€…ä»¥åŠæ§åˆ¶æœ€ç»ˆæ•°æ®é›†è´¨é‡çš„æœ€ä½³å®è·µã€‚æœ¬æ•™ç¨‹åŒ…æ‹¬ä¸€ä¸ªåŠ¨æ‰‹å®è·µç ”è®¨ä¼šï¼Œå‚ä¼šè€…å°†åœ¨å…¶ä¸­å­¦ä¹ å¦‚ä½•å®æ–½æ··åˆæ ‡æ³¨è®¾ç½®ã€‚æœ¬æ•™ç¨‹é¢å‘NLPé¢†åŸŸçš„ä»ä¸šè€…ï¼Œæ— è®ºå…¶æ¥è‡ªç ”ç©¶è¿˜æ˜¯å·¥ä¸šèƒŒæ™¯ï¼Œåªè¦å‚ä¸æˆ–æœ‰å…´è¶£ä¼˜åŒ–æ•°æ®æ ‡æ³¨é¡¹ç›®å³å¯å‚åŠ ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.04637v3">PDF</a> To be presented at COLING 2025</p>
<p><strong>Summary</strong><br>æœºå™¨å­¦ä¹ æ¨¡å‹è®­ç»ƒéƒ¨ç½²ä¾èµ–å¤§é‡äººå·¥æ ‡æ³¨æ•°æ®ï¼Œæˆæœ¬é«˜ä¸”è€—æ—¶ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶äººå‘˜å¼€å‘äº†å¤šç§åŠ é€Ÿæ ‡æ³¨å’Œé™ä½æˆæœ¬çš„æ–¹æ³•ï¼Œå¦‚ç”Ÿæˆåˆæˆè®­ç»ƒæ•°æ®ã€ä¸»åŠ¨å­¦ä¹ å’Œæ··åˆæ ‡æ³¨ç­‰ã€‚æœ¬æ•™ç¨‹æ³¨é‡å®é™…åº”ç”¨ï¼Œä»‹ç»æ¯ç§ç­–ç•¥çš„åŸºæœ¬åŸç†ã€ä¼˜ç¼ºç‚¹åŠç°å®æ¡ˆä¾‹ã€‚åŒæ—¶è®²è§£å¦‚ä½•ç®¡ç†äººå·¥æ ‡æ³¨è€…å¹¶æ§åˆ¶æ•°æ®é›†è´¨é‡ã€‚æœ¬æ•™ç¨‹é€‚åˆNLPä»ä¸šäººå‘˜å’Œç ”ç©¶äººå‘˜ï¼Œå°¤å…¶å¯¹æ•°æ®æ ‡æ³¨é¡¹ç›®æ„Ÿå…´è¶£æˆ–æ­£åœ¨å‚ä¸ä¼˜åŒ–è€…ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœºå™¨å­¦ä¹ æ¨¡å‹è®­ç»ƒéœ€è¦å¤§é‡äººå·¥æ ‡æ³¨æ•°æ®ï¼Œæˆæœ¬é«˜æ˜‚ä¸”è€—æ—¶ã€‚</li>
<li>å­˜åœ¨å¤šç§åŠ é€Ÿæ ‡æ³¨å’Œé™ä½æˆæœ¬çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬ç”Ÿæˆåˆæˆè®­ç»ƒæ•°æ®ã€ä¸»åŠ¨å­¦ä¹ å’Œæ··åˆæ ‡æ³¨ç­‰ã€‚</li>
<li>æœ¬æ•™ç¨‹ä»‹ç»è¿™äº›æ–¹æ³•çš„åŸºæœ¬åŸç†å’Œä¼˜ç¼ºç‚¹ã€‚</li>
<li>æ•™ç¨‹æ³¨é‡å®é™…åº”ç”¨ï¼Œæä¾›çœŸå®æ¡ˆä¾‹å’Œæœ€ä½³å®è·µã€‚</li>
<li>æ•™ç¨‹åŒ…å«å®è·µå·¥ä½œåŠï¼ŒæŒ‡å¯¼å‚ä¸è€…å®æ–½æ··åˆæ ‡æ³¨è®¾ç½®ã€‚</li>
<li>é€‚åˆNLPä»ä¸šäººå‘˜å’Œç ”ç©¶äººå‘˜ï¼Œå¯¹æ•°æ®æ ‡æ³¨é¡¹ç›®æ„Ÿå…´è¶£æˆ–æ­£åœ¨å‚ä¸ä¼˜åŒ–è€…å°¤å…¶æœ‰ç›Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.04637">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-59d005d4497b7613becb0fc21dfa6803.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-51e77220a1beb5ce380b606bb78a939d.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="MetRex-A-Benchmark-for-Verilog-Code-Metric-Reasoning-Using-LLMs"><a href="#MetRex-A-Benchmark-for-Verilog-Code-Metric-Reasoning-Using-LLMs" class="headerlink" title="MetRex: A Benchmark for Verilog Code Metric Reasoning Using LLMs"></a>MetRex: A Benchmark for Verilog Code Metric Reasoning Using LLMs</h2><p><strong>Authors:Manar Abdelatty, Jingxiao Ma, Sherief Reda</strong></p>
<p>Large Language Models (LLMs) have been applied to various hardware design tasks, including Verilog code generation, EDA tool scripting, and RTL bug fixing. Despite this extensive exploration, LLMs are yet to be used for the task of post-synthesis metric reasoning and estimation of HDL designs. In this paper, we assess the ability of LLMs to reason about post-synthesis metrics of Verilog designs. We introduce MetRex, a large-scale dataset comprising 25,868 Verilog HDL designs and their corresponding post-synthesis metrics, namely area, delay, and static power. MetRex incorporates a Chain of Thought (CoT) template to enhance LLMsâ€™ reasoning about these metrics. Extensive experiments show that Supervised Fine-Tuning (SFT) boosts the LLMâ€™s reasoning capabilities on average by 37.0%, 25.3%, and 25.7% on the area, delay, and static power, respectively. While SFT improves performance on our benchmark, it remains far from achieving optimal results, especially on complex problems. Comparing to state-of-the-art regression models, our approach delivers accurate post-synthesis predictions for 17.4% more designs (within a 5% error margin), in addition to offering a 1.7x speedup by eliminating the need for pre-processing. This work lays the groundwork for advancing LLM-based Verilog code metric reasoning. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²è¢«åº”ç”¨äºå„ç§ç¡¬ä»¶è®¾è®¡ä»»åŠ¡ï¼ŒåŒ…æ‹¬Verilogä»£ç ç”Ÿæˆã€EDAå·¥å…·è„šæœ¬ç¼–å†™å’ŒRTLé”™è¯¯ä¿®å¤ã€‚å°½ç®¡è¿›è¡Œäº†å¹¿æ³›çš„æ¢ç´¢ï¼Œä½†LLMå°šæœªç”¨äºHDLè®¾è®¡çš„åç»¼åˆåº¦é‡æ¨ç†å’Œä¼°ç®—ä»»åŠ¡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è¯„ä¼°äº†LLMå¯¹Verilogè®¾è®¡åç»¼åˆæŒ‡æ ‡è¿›è¡Œæ¨ç†çš„èƒ½åŠ›ã€‚æˆ‘ä»¬å¼•å…¥äº†MetRexï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡æ•°æ®é›†ï¼ŒåŒ…å«25,868ä¸ªVerilog HDLè®¾è®¡åŠå…¶ç›¸åº”çš„åç»¼åˆåº¦é‡ï¼Œå³é¢ç§¯ã€å»¶è¿Ÿå’Œé™æ€åŠŸè€—ã€‚MetRexé‡‡ç”¨â€œæ€ç»´é“¾â€ï¼ˆChain of Thoughtï¼ŒCoTï¼‰æ¨¡æ¿ï¼Œä»¥å¢å¼ºLLMå¯¹è¿™äº›æŒ‡æ ‡çš„æ¨ç†èƒ½åŠ›ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å¹³å‡æé«˜äº†LLMåœ¨é¢ç§¯ã€å»¶è¿Ÿå’Œé™æ€åŠŸç‡æ–¹é¢çš„æ¨ç†èƒ½åŠ›ï¼Œåˆ†åˆ«æé«˜äº†37.0%ã€25.3%å’Œ25.7%ã€‚è™½ç„¶SFTåœ¨æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•ä¸­æé«˜äº†æ€§èƒ½ï¼Œä½†è·ç¦»è¾¾åˆ°æœ€ä½³ç»“æœè¿˜æœ‰å¾ˆå¤§å·®è·ï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚é—®é¢˜ä¸Šã€‚ä¸æœ€å…ˆè¿›çš„å›å½’æ¨¡å‹ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨5%è¯¯å·®èŒƒå›´å†…ä¸ºæ›´å¤šè®¾è®¡æä¾›äº†å‡†ç¡®çš„åç»¼åˆé¢„æµ‹ï¼ˆå¢åŠ äº†17.4%ï¼‰ï¼Œå¹¶ä¸”ç”±äºä¸éœ€è¦é¢„å¤„ç†ï¼Œæä¾›äº†1.7å€çš„åŠ é€Ÿã€‚è¿™é¡¹å·¥ä½œä¸ºæ¨è¿›åŸºäºLLMçš„Verilogä»£ç åº¦é‡æ¨ç†å¥ å®šäº†åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.03471v2">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é¦–æ¬¡è¢«åº”ç”¨äºVerilogè®¾è®¡åç»¼åˆåº¦é‡æ¨ç†ä»»åŠ¡ï¼Œå¹¶æˆåŠŸç”Ÿæˆç›¸åº”çš„æ•°æ®é›†MetRexã€‚é€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å¢å¼ºLLMçš„æ¨ç†èƒ½åŠ›ï¼Œæ˜¾è‘—æé«˜åŒºåŸŸã€å»¶è¿Ÿå’Œé™æ€åŠŸç‡çš„é¢„æµ‹ç²¾åº¦ã€‚ä¸ç°æœ‰å›å½’æ¨¡å‹ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•çš„é¢„æµ‹å‡†ç¡®æ€§æ›´é«˜ï¼Œå¤„ç†é€Ÿåº¦æ›´å¿«ã€‚è¿™ä¸ºLLMåœ¨Verilogä»£ç åº¦é‡æ¨ç†ä¸­çš„åº”ç”¨å¥ å®šäº†åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é¦–æ¬¡åº”ç”¨äºVerilogè®¾è®¡åç»¼åˆåº¦é‡æ¨ç†ä»»åŠ¡ã€‚</li>
<li>å¼•å…¥äº†MetRexæ•°æ®é›†ï¼ŒåŒ…å«25,868ä¸ªVerilog HDLè®¾è®¡åŠå¯¹åº”çš„åç»¼åˆåº¦é‡ã€‚</li>
<li>é‡‡ç”¨ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å¢å¼ºLLMçš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>SFTæ˜¾è‘—æé«˜åŒºåŸŸã€å»¶è¿Ÿå’Œé™æ€åŠŸç‡çš„é¢„æµ‹ç²¾åº¦ã€‚</li>
<li>ä¸ç°æœ‰å›å½’æ¨¡å‹ç›¸æ¯”ï¼ŒLLMæ–¹æ³•é¢„æµ‹æ›´å‡†ç¡®ï¼Œå¤„ç†é€Ÿåº¦æ›´å¿«ã€‚</li>
<li>MetRexæ•°æ®é›†é€šè¿‡Chain of Thoughtï¼ˆCoTï¼‰æ¨¡æ¿å¢å¼ºLLMçš„æ¨ç†èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.03471">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-47147858222532f0c118fdc74427d980.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-94a3c3b60f70dbdc77a62ad5cd994d97.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fbe2d09fa7f43614d374238f916085d7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-84c83e74b736cdf401f895607e1405e7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1cbb5acf61ec085b94d3caa38cee9747.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d5c5c7ce80cfb0c2e300c5f647db950a.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Large-Language-Model-based-Augmentation-for-Imbalanced-Node-Classification-on-Text-Attributed-Graphs"><a href="#Large-Language-Model-based-Augmentation-for-Imbalanced-Node-Classification-on-Text-Attributed-Graphs" class="headerlink" title="Large Language Model-based Augmentation for Imbalanced Node   Classification on Text-Attributed Graphs"></a>Large Language Model-based Augmentation for Imbalanced Node   Classification on Text-Attributed Graphs</h2><p><strong>Authors:Leyao Wang, Yu Wang, Bo Ni, Yuying Zhao, Tyler Derr</strong></p>
<p>Node classification on graphs often suffers from class imbalance, leading to biased predictions and significant risks in real-world applications. While data-centric solutions have been explored, they largely overlook Text-Attributed Graphs (TAGs) and the potential of using rich textual semantics to improve the classification of minority nodes. Given this gap, we propose Large Language Model-based Augmentation on Text-Attributed Graphs (LA-TAG), a novel framework that leverages Large Language Models (LLMs) to handle imbalanced node classification. Specifically, we develop prompting strategies inspired by interpolation to synthesize textual node attributes. Additionally, to effectively integrate synthetic nodes into the graph structure, we introduce a textual link predictor that connects the generated nodes to the original graph, preserving structural and contextual information. Experiments across various datasets and evaluation metrics demonstrate that LA-TAG outperforms existing textual augmentation and graph imbalance learning methods, emphasizing the efficacy of our approach in addressing class imbalance in TAGs. </p>
<blockquote>
<p>å›¾ä¸Šçš„èŠ‚ç‚¹åˆ†ç±»å¸¸å¸¸å—åˆ°ç±»åˆ«ä¸å¹³è¡¡çš„å½±å“ï¼Œä»è€Œå¯¼è‡´é¢„æµ‹ç»“æœæœ‰åå‘æ€§ï¼Œå¹¶ä¸”åœ¨ç°å®åº”ç”¨ä¸­å­˜åœ¨è¾ƒå¤§çš„é£é™©ã€‚è™½ç„¶æ•°æ®é©±åŠ¨è§£å†³æ–¹æ¡ˆå·²è¢«æ¢ç´¢ï¼Œä½†å®ƒä»¬åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå¿½ç•¥äº†æ–‡æœ¬å±æ€§å›¾ï¼ˆTAGsï¼‰å’Œåˆ©ç”¨ä¸°å¯Œçš„æ–‡æœ¬è¯­ä¹‰æ¥æ”¹å–„å°‘æ•°èŠ‚ç‚¹åˆ†ç±»çš„æ½œåŠ›ã€‚é’ˆå¯¹è¿™ä¸€ç©ºç™½é¢†åŸŸï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ–‡æœ¬å±æ€§å›¾å¢å¼ºï¼ˆLA-TAGï¼‰è¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥å¤„ç†ä¸å¹³è¡¡èŠ‚ç‚¹åˆ†ç±»é—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§åŸºäºæ’å€¼çš„æç¤ºç­–ç•¥æ¥åˆæˆæ–‡æœ¬èŠ‚ç‚¹å±æ€§ã€‚æ­¤å¤–ï¼Œä¸ºäº†æœ‰æ•ˆåœ°å°†åˆæˆèŠ‚ç‚¹é›†æˆåˆ°å›¾ç»“æ„ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–‡æœ¬é“¾æ¥é¢„æµ‹å™¨ï¼Œå°†ç”Ÿæˆçš„èŠ‚ç‚¹ä¸åŸå§‹å›¾è¿æ¥èµ·æ¥ï¼Œä¿ç•™ç»“æ„å’Œä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚åœ¨ä¸åŒæ•°æ®é›†å’Œè¯„ä»·æŒ‡æ ‡ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒLA-TAGåœ¨æ–‡æœ¬å¢å¼ºå’Œå›¾ä¸å¹³è¡¡å­¦ä¹ æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œçªæ˜¾äº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨è§£å†³TAGä¸­çš„ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.16882v2">PDF</a> 13 pages</p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬é’ˆå¯¹å›¾ç»“æ„ä¸­çš„èŠ‚ç‚¹åˆ†ç±»é—®é¢˜è¿›è¡Œäº†æ¢è®¨ï¼Œç‰¹åˆ«æ˜¯é¢ä¸´ç±»åˆ«ä¸å‡è¡¡å¯¼è‡´çš„é¢„æµ‹åå·®å’Œå®é™…åº”ç”¨é£é™©ã€‚é’ˆå¯¹æ–‡æœ¬å±æ€§å›¾ï¼ˆTAGsï¼‰ä¸­çš„å°‘æ•°èŠ‚ç‚¹åˆ†ç±»é—®é¢˜ï¼Œæå‡ºäº†åŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¢å¼ºæ–¹æ³•LA-TAGã€‚è¯¥æ–¹æ³•åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹å¼€å‘æ’å€¼å¯å‘å¼çš„æç¤ºç­–ç•¥ï¼Œåˆæˆæ–‡æœ¬èŠ‚ç‚¹å±æ€§ï¼Œå¹¶é€šè¿‡æ–‡æœ¬é“¾æ¥é¢„æµ‹å™¨å°†åˆæˆèŠ‚ç‚¹æœ‰æ•ˆåœ°èå…¥å›¾ç»“æ„ï¼Œä¿ç•™ç»“æ„å’Œä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚å®éªŒè¡¨æ˜ï¼ŒLA-TAGåœ¨å¤šç§æ•°æ®é›†å’Œè¯„ä»·æŒ‡æ ‡ä¸Šä¼˜äºç°æœ‰çš„æ–‡æœ¬å¢å¼ºå’Œå›¾ä¸å¹³è¡¡å­¦ä¹ æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>èŠ‚ç‚¹åˆ†ç±»åœ¨å›¾ç»“æ„ä¸­é¢ä¸´ç±»åˆ«ä¸å‡è¡¡é—®é¢˜ï¼Œå¯¼è‡´é¢„æµ‹åå·®å’Œå®é™…åº”ç”¨é£é™©ã€‚</li>
<li>ç°æœ‰æ•°æ®ä¸­å¿ƒçš„è§£å†³æ–¹æ¡ˆå¤§å¤šå¿½ç•¥äº†æ–‡æœ¬å±æ€§å›¾ï¼ˆTAGsï¼‰å’Œä¸°å¯Œæ–‡æœ¬è¯­ä¹‰åœ¨æ”¹è¿›å°‘æ•°èŠ‚ç‚¹åˆ†ç±»ä¸­çš„æ½œåŠ›ã€‚</li>
<li>LA-TAGåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¤„ç†ä¸å¹³è¡¡èŠ‚ç‚¹åˆ†ç±»é—®é¢˜ï¼Œé€šè¿‡åˆæˆæ–‡æœ¬èŠ‚ç‚¹å±æ€§æå‡åˆ†ç±»æ•ˆæœã€‚</li>
<li>LA-TAGé‡‡ç”¨æ’å€¼å¯å‘å¼çš„æç¤ºç­–ç•¥æ¥å¼€å‘æç¤ºæ–¹æ³•ã€‚</li>
<li>ä¸ºå°†åˆæˆèŠ‚ç‚¹æœ‰æ•ˆåœ°èå…¥å›¾ç»“æ„ï¼ŒLA-TAGå¼•å…¥äº†æ–‡æœ¬é“¾æ¥é¢„æµ‹å™¨ã€‚</li>
<li>æ–‡æœ¬é“¾æ¥é¢„æµ‹å™¨èƒ½å¤Ÿä¿ç•™ç»“æ„å’Œä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒLA-TAGåœ¨å¤šä¸ªæ•°æ®é›†å’Œè¯„ä»·æŒ‡æ ‡ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.16882">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9e3af39f3f72faa04697583185e54952.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-314fa8eb36d8d5514b7f73dc78579c7f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-776603f5940b9b34d506361c19e018b7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ff5f78393a3e3ea195994af0693d65ac.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ad685a15508321058395e24542c3e7a8.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="DeSTA2-Developing-Instruction-Following-Speech-Language-Model-Without-Speech-Instruction-Tuning-Data"><a href="#DeSTA2-Developing-Instruction-Following-Speech-Language-Model-Without-Speech-Instruction-Tuning-Data" class="headerlink" title="DeSTA2: Developing Instruction-Following Speech Language Model Without   Speech Instruction-Tuning Data"></a>DeSTA2: Developing Instruction-Following Speech Language Model Without   Speech Instruction-Tuning Data</h2><p><strong>Authors:Ke-Han Lu, Zhehuai Chen, Szu-Wei Fu, Chao-Han Huck Yang, Jagadeesh Balam, Boris Ginsburg, Yu-Chiang Frank Wang, Hung-yi Lee</strong></p>
<p>Recent end-to-end speech language models (SLMs) have expanded upon the capabilities of large language models (LLMs) by incorporating pre-trained speech models. However, these SLMs often undergo extensive speech instruction-tuning to bridge the gap between speech and text modalities. This requires significant annotation efforts and risks catastrophic forgetting of the original language capabilities. In this work, we present a simple yet effective automatic process for creating speech-text pair data that carefully injects speech paralinguistic understanding abilities into SLMs while preserving the inherent language capabilities of the text-based LLM. Our model demonstrates general capabilities for speech-related tasks without the need for speech instruction-tuning data, achieving impressive performance on Dynamic-SUPERB and AIR-Bench-Chat benchmarks. Furthermore, our model exhibits the ability to follow complex instructions derived from LLMs, such as specific output formatting and chain-of-thought reasoning. Our approach not only enhances the versatility and effectiveness of SLMs but also reduces reliance on extensive annotated datasets, paving the way for more efficient and capable speech understanding systems. </p>
<blockquote>
<p>æœ€è¿‘çš„ç«¯åˆ°ç«¯è¯­éŸ³è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰é€šè¿‡èå…¥é¢„è®­ç»ƒçš„è¯­éŸ³æ¨¡å‹ï¼Œæ‰©å¤§äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åŠŸèƒ½ã€‚ç„¶è€Œï¼Œè¿™äº›SLMé€šå¸¸éœ€è¦ç»è¿‡å¤§é‡çš„è¯­éŸ³æŒ‡ä»¤è°ƒæ•´ï¼Œä»¥å¼¥è¡¥è¯­éŸ³å’Œæ–‡æœ¬æ¨¡æ€ä¹‹é—´çš„å·®è·ã€‚è¿™éœ€è¦å¤§é‡çš„æ ‡æ³¨å·¥ä½œï¼Œå¹¶å­˜åœ¨é—å¿˜åŸå§‹è¯­è¨€èƒ½åŠ›çš„é£é™©ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„è‡ªåŠ¨åˆ›å»ºè¯­éŸ³æ–‡æœ¬é…å¯¹æ•°æ®çš„è¿‡ç¨‹ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿç²¾å¿ƒåœ°å°†è¯­éŸ³å‰¯è¯­è¨€ç†è§£çš„èƒ½åŠ›æ³¨å…¥SLMä¸­ï¼ŒåŒæ—¶ä¿ç•™åŸºäºæ–‡æœ¬çš„LLMçš„å›ºæœ‰è¯­è¨€èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ¨¡å‹å±•ç¤ºäº†åœ¨ä¸éœ€è¦è¯­éŸ³æŒ‡ä»¤è°ƒæ•´æ•°æ®çš„æƒ…å†µä¸‹ï¼Œè¿›è¡Œè¯­éŸ³ç›¸å…³ä»»åŠ¡çš„é€šç”¨èƒ½åŠ›ï¼Œåœ¨Dynamic-SUPERBå’ŒAIR-Bench-ChatåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„è¡¨ç°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ¨¡å‹è¿˜å±•ç¤ºäº†éµå¾ªLLMè¡ç”Ÿçš„å¤æ‚æŒ‡ä»¤çš„èƒ½åŠ›ï¼Œå¦‚ç‰¹å®šçš„è¾“å‡ºæ ¼å¼å’Œé“¾å¼æ€ç»´æ¨ç†ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…æé«˜äº†SLMçš„é€šç”¨æ€§å’Œæœ‰æ•ˆæ€§ï¼Œè€Œä¸”å‡å°‘äº†å¯¹äºå¤§é‡æ ‡æ³¨æ•°æ®é›†çš„ä¾èµ–ï¼Œä¸ºæ›´é«˜æ•ˆã€æ›´å¼ºå¤§çš„è¯­éŸ³ç†è§£ç³»ç»Ÿé“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.20007v2">PDF</a> Accepted by ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸï¼Œç«¯å¯¹ç«¯è¯­éŸ³è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰é€šè¿‡èå…¥é¢„è®­ç»ƒè¯­éŸ³æ¨¡å‹ï¼Œæ‰©å¤§äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™äº›SLMéœ€è¦ç»è¿‡å¤§é‡çš„è¯­éŸ³æŒ‡ä»¤è°ƒæ•´æ¥å¼¥åˆè¯­éŸ³ä¸æ–‡æœ¬æ¨¡å¼ä¹‹é—´çš„å·®è·ï¼Œè¿™éœ€è¦å¤§é‡çš„æ ‡æ³¨å·¥ä½œå¹¶å­˜åœ¨é—å¿˜åŸæœ‰è¯­è¨€èƒ½åŠ›çš„é£é™©ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„è‡ªåŠ¨åˆ›å»ºè¯­éŸ³æ–‡æœ¬é…å¯¹æ•°æ®çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿè°¨æ…åœ°å°†è¯­éŸ³å‰¯è¯­è¨€ç†è§£åŠ›æ³¨å…¥SLMä¸­ï¼ŒåŒæ—¶ä¿ç•™åŸºäºæ–‡æœ¬çš„LLMçš„å›ºæœ‰è¯­è¨€èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨åŠ¨æ€SUPERBå’ŒAIR-Bench-ChatåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œæ— éœ€è¯­éŸ³æŒ‡ä»¤è°ƒæ•´æ•°æ®å³å¯è¿›è¡Œè¯­éŸ³ç›¸å…³ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å±•ç°å‡ºéµå¾ªLLMæ´¾ç”Ÿçš„å¤æ‚æŒ‡ä»¤çš„èƒ½åŠ›ï¼Œå¦‚ç‰¹å®šè¾“å‡ºæ ¼å¼å’Œé“¾å¼æ€ç»´æ¨ç†ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…æé«˜äº†SLMçš„é€šç”¨æ€§å’Œæ•ˆç‡ï¼Œè¿˜é™ä½äº†å¯¹å¤§é‡æ ‡æ³¨æ•°æ®é›†çš„ä¾èµ–ï¼Œä¸ºæ›´æœ‰æ•ˆç‡ã€èƒ½åŠ›æ›´å¼ºçš„è¯­éŸ³ç†è§£ç³»ç»Ÿé“ºå¹³äº†é“è·¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç«¯å¯¹ç«¯è¯­éŸ³è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰ç»“åˆäº†é¢„è®­ç»ƒè¯­éŸ³æ¨¡å‹å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œå¢å¼ºäº†è¯­è¨€å¤„ç†èƒ½åŠ›ã€‚</li>
<li>SLMéœ€è¦é€šè¿‡å¤§é‡çš„è¯­éŸ³æŒ‡ä»¤è°ƒæ•´æ¥é€‚åº”è¯­éŸ³å’Œæ–‡æœ¬æ¨¡å¼ä¹‹é—´çš„å·®è·ï¼Œè¿™å¢åŠ äº†æ ‡æ³¨å·¥ä½œçš„è´Ÿæ‹…å¹¶å¯èƒ½å¯¼è‡´åŸæœ‰è¯­è¨€èƒ½åŠ›çš„é—å¿˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§è‡ªåŠ¨åˆ›å»ºè¯­éŸ³æ–‡æœ¬é…å¯¹æ•°æ®çš„æ–¹æ³•ï¼Œèƒ½å·§å¦™åœ°å°†è¯­éŸ³å‰¯è¯­è¨€ç†è§£åŠ›æ³¨å…¥SLMï¼ŒåŒæ—¶ä¿æŒLLMçš„å›ºæœ‰è¯­è¨€èƒ½åŠ›ã€‚</li>
<li>æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œæ— éœ€é¢å¤–çš„è¯­éŸ³æŒ‡ä»¤è°ƒæ•´å³å¯å¤„ç†è¯­éŸ³ç›¸å…³ä»»åŠ¡ã€‚</li>
<li>æ¨¡å‹èƒ½å¤Ÿéµå¾ªå¤æ‚çš„æŒ‡ä»¤ï¼ŒåŒ…æ‹¬ç‰¹å®šçš„è¾“å‡ºæ ¼å¼å’Œé“¾å¼æ€ç»´æ¨ç†ï¼Œå±•ç°äº†å…¶å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚</li>
<li>è¯¥æ–¹æ³•æé«˜äº†SLMçš„é€šç”¨æ€§å’Œæ•ˆç‡ï¼Œä¸ºæ›´é«˜æ•ˆçš„è¯­éŸ³ç†è§£ç³»ç»Ÿæä¾›äº†å¯èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.20007">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-bb12237f5028b2ef8c4688ba3bd7db85.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-58f6ab46ca8afa96573693f49a1013dd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1e1917e83ea95b32bc22dbb5f4a38fc4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ef2fd4ebdb88d017ed0400cb0f713b7.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="MOSAIC-Multiple-Observers-Spotting-AI-Content-a-Robust-Approach-to-Machine-Generated-Text-Detection"><a href="#MOSAIC-Multiple-Observers-Spotting-AI-Content-a-Robust-Approach-to-Machine-Generated-Text-Detection" class="headerlink" title="MOSAIC: Multiple Observers Spotting AI Content, a Robust Approach to   Machine-Generated Text Detection"></a>MOSAIC: Multiple Observers Spotting AI Content, a Robust Approach to   Machine-Generated Text Detection</h2><p><strong>Authors:Matthieu Dubois, FranÃ§ois Yvon, Pablo Piantanida</strong></p>
<p>The dissemination of Large Language Models (LLMs), trained at scale, and endowed with powerful text-generating abilities has vastly increased the threats posed by generative AI technologies by reducing the cost of producing harmful, toxic, faked or forged content. In response, various proposals have been made to automatically discriminate artificially generated from human-written texts, typically framing the problem as a classification problem. Most approaches evaluate an input document by a well-chosen detector LLM, assuming that low-perplexity scores reliably signal machine-made content. As using one single detector can induce brittleness of performance, we instead consider several and derive a new, theoretically grounded approach to combine their respective strengths. Our experiments, using a variety of generator LLMs, suggest that our method effectively leads to robust detection performances. An early version of the code is available at <a target="_blank" rel="noopener" href="https://github.com/BaggerOfWords/MOSAIC">https://github.com/BaggerOfWords/MOSAIC</a>. </p>
<blockquote>
<p>å¤§è§„æ¨¡è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å…·æœ‰å¼ºå¤§çš„æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›ï¼Œå…¶ä¼ æ’­å¤§å¤§é™ä½äº†ç”Ÿæˆäººå·¥æ™ºèƒ½æŠ€æœ¯çš„å¨èƒï¼Œå‡å°‘äº†åˆ¶é€ æœ‰å®³ã€æœ‰æ¯’ã€ä¼ªé€ æˆ–ä¼ªé€ å†…å®¹çš„æˆæœ¬ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€é—®é¢˜ï¼Œå·²ç»æå‡ºäº†å¤šç§è‡ªåŠ¨åŒºåˆ†äººå·¥ç”Ÿæˆæ–‡æœ¬å’Œäººç±»ç¼–å†™æ–‡æœ¬çš„æ–¹æ³•ï¼Œé€šå¸¸å°†è¿™ä¸€é—®é¢˜æ„å»ºä¸ºåˆ†ç±»é—®é¢˜ã€‚å¤§å¤šæ•°æ–¹æ³•é€šè¿‡ç²¾å¿ƒé€‰æ‹©çš„æ£€æµ‹å™¨LLMæ¥è¯„ä¼°è¾“å…¥æ–‡æ¡£ï¼Œå‡è®¾ä½å›°æƒ‘åº¦åˆ†æ•°å¯é åœ°è¡¨ç¤ºæœºå™¨ç”Ÿæˆçš„å†…å®¹ã€‚ç”±äºä½¿ç”¨å•ä¸ªæ£€æµ‹å™¨å¯èƒ½ä¼šå¯¼è‡´æ€§èƒ½è„†å¼±ï¼Œå› æ­¤æˆ‘ä»¬è€ƒè™‘å¤šä¸ªå› ç´ ï¼Œå¹¶æ¨å¯¼å‡ºä¸€ç§ç»“åˆå„è‡ªä¼˜åŠ¿çš„æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨ç†è®ºä¸Šæœ‰æ‰€ä¾æ®ã€‚æˆ‘ä»¬çš„å®éªŒä½¿ç”¨äº†å„ç§ç”Ÿæˆå¼LLMï¼Œç»“æœè¡¨æ˜æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥æœ‰æ•ˆåœ°å®ç°ç¨³å¥çš„æ£€æµ‹æ€§èƒ½ã€‚è¯¥ä»£ç çš„åˆæ­¥ç‰ˆæœ¬å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/BaggerOfWords/MOSAIC%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/BaggerOfWords/MOSAICä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.07615v2">PDF</a> Still a work in progress, early version of the code can be found here   :<a target="_blank" rel="noopener" href="https://github.com/BaggerOfWords/MOSAIC">https://github.com/BaggerOfWords/MOSAIC</a></p>
<p><strong>Summary</strong></p>
<p>å¤§è§„æ¨¡è®­ç»ƒçš„è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å…·å¤‡å¼ºå¤§çš„æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›ï¼Œé™ä½äº†äº§ç”Ÿæœ‰å®³ã€æœ‰æ¯’ã€è™šå‡æˆ–ä¼ªé€ å†…å®¹çš„æˆæœ¬ï¼Œä»è€Œæå¤§åœ°å¢åŠ äº†ç”Ÿæˆå¼äººå·¥æ™ºèƒ½æŠ€æœ¯çš„å¨èƒã€‚ä¸ºåº”å¯¹è¿™ä¸€é—®é¢˜ï¼Œå·²æå‡ºå¤šç§è‡ªåŠ¨åŒºåˆ†äººå·¥ç”Ÿæˆæ–‡æœ¬å’Œäººç±»æ’°å†™æ–‡æœ¬çš„æ–¹æ³•ï¼Œé€šå¸¸å°†é—®é¢˜è§†ä¸ºåˆ†ç±»é—®é¢˜ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œç»“åˆå¤šä¸ªæ£€æµ‹å™¨çš„æ–¹æ³•å¯ä»¥æœ‰æ•ˆåœ°æé«˜æ£€æµ‹æ€§èƒ½ã€‚ç›¸å…³ä»£ç çš„æ—©æœŸç‰ˆæœ¬å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/BaggerOfWords/MOSAIC%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/BaggerOfWords/MOSAICè·å–ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMçš„æ™®åŠé™ä½äº†äº§ç”Ÿæœ‰å®³ã€æœ‰æ¯’ã€è™šå‡æˆ–ä¼ªé€ å†…å®¹çš„æˆæœ¬ï¼Œå¢åŠ äº†ç”Ÿæˆå¼AIæŠ€æœ¯çš„å¨èƒã€‚</li>
<li>è‡ªåŠ¨åŒºåˆ†äººå·¥ç”Ÿæˆæ–‡æœ¬å’Œäººç±»æ’°å†™æ–‡æœ¬çš„æ–¹æ³•é€šå¸¸å°†é—®é¢˜è§†ä¸ºåˆ†ç±»é—®é¢˜ã€‚</li>
<li>å¤§å¤šæ•°æ–¹æ³•é€šè¿‡é€‰æ‹©é€‚å½“çš„æ£€æµ‹LLMæ¥è¯„ä¼°è¾“å…¥æ–‡æ¡£ï¼Œä½å›°æƒ‘åº¦åˆ†æ•°å¯é åœ°è¡¨ç¤ºæœºå™¨ç”Ÿæˆçš„å†…å®¹ã€‚</li>
<li>ä½¿ç”¨å•ä¸ªæ£€æµ‹å™¨å¯èƒ½å¯¼è‡´æ€§èƒ½ä¸Šçš„è„†å¼±æ€§ã€‚</li>
<li>ç»“åˆå¤šä¸ªæ£€æµ‹å™¨çš„æ–¹æ³•å¯ä»¥æé«˜æ£€æµ‹æ€§èƒ½ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§ç”ŸæˆLLMä¸Šçš„æ£€æµ‹æ€§èƒ½è¡¨ç°ç¨³å¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.07615">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9a4c527a947c0d0a132e959013c6bb64.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a35e25c70ddbc842b542e22b9b4877c7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-82e2eb561e3e8340271ab8d1ba674dce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c97bca984c13dd0a3a170579896b1f12.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Emergence-of-a-High-Dimensional-Abstraction-Phase-in-Language-Transformers"><a href="#Emergence-of-a-High-Dimensional-Abstraction-Phase-in-Language-Transformers" class="headerlink" title="Emergence of a High-Dimensional Abstraction Phase in Language   Transformers"></a>Emergence of a High-Dimensional Abstraction Phase in Language   Transformers</h2><p><strong>Authors:Emily Cheng, Diego Doimo, Corentin Kervadec, Iuri Macocco, Jade Yu, Alessandro Laio, Marco Baroni</strong></p>
<p>A language model (LM) is a mapping from a linguistic context to an output token. However, much remains to be known about this mapping, including how its geometric properties relate to its function. We take a high-level geometric approach to its analysis, observing, across five pre-trained transformer-based LMs and three input datasets, a distinct phase characterized by high intrinsic dimensionality. During this phase, representations (1) correspond to the first full linguistic abstraction of the input; (2) are the first to viably transfer to downstream tasks; (3) predict each other across different LMs. Moreover, we find that an earlier onset of the phase strongly predicts better language modelling performance. In short, our results suggest that a central high-dimensionality phase underlies core linguistic processing in many common LM architectures. </p>
<blockquote>
<p>è¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰æ˜¯ä¸€ç§å°†è¯­è¨€ä¸Šä¸‹æ–‡æ˜ å°„åˆ°è¾“å‡ºæ ‡è®°çš„æ¨¡å‹ã€‚ç„¶è€Œï¼Œå…³äºè¿™ç§æ˜ å°„è¿˜æœ‰å¾ˆå¤šæœªçŸ¥ï¼ŒåŒ…æ‹¬å…¶å‡ ä½•å±æ€§å¦‚ä½•ä¸å…¶åŠŸèƒ½ç›¸å…³è”ã€‚æˆ‘ä»¬é‡‡ç”¨é«˜çº§å‡ ä½•æ–¹æ³•å¯¹å…¶è¿›è¡Œåˆ†æï¼Œè§‚å¯Ÿäº†äº”ä¸ªåŸºäºé¢„è®­ç»ƒè½¬æ¢å™¨çš„è¯­è¨€æ¨¡å‹å’Œä¸‰ä¸ªè¾“å…¥æ•°æ®é›†ï¼Œå‘ç°äº†ä¸€ä¸ªä»¥é«˜å†…åœ¨ç»´åº¦ä¸ºç‰¹å¾çš„ç‹¬ç‰¹é˜¶æ®µã€‚åœ¨æ­¤é˜¶æ®µï¼Œè¡¨ç¤ºï¼ˆ1ï¼‰å¯¹åº”äºè¾“å…¥çš„ç¬¬ä¸€ä¸ªå®Œæ•´è¯­è¨€æŠ½è±¡ï¼›ï¼ˆ2ï¼‰æ˜¯ç¬¬ä¸€ä¸ªåˆ‡å®è½¬ç§»åˆ°ä¸‹æ¸¸ä»»åŠ¡çš„è¡¨ç¤ºï¼›ï¼ˆ3ï¼‰åœ¨ä¸åŒè¯­è¨€æ¨¡å‹ä¸­ç›¸äº’é¢„æµ‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°è¯¥é˜¶æ®µçš„æ—©æœŸå‘ç”Ÿå¼ºçƒˆé¢„ç¤ºç€æ›´å¥½çš„è¯­è¨€å»ºæ¨¡æ€§èƒ½ã€‚ç®€è€Œè¨€ä¹‹ï¼Œæˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œè®¸å¤šå¸¸è§çš„è¯­è¨€æ¨¡å‹æ¶æ„ä¸­çš„æ ¸å¿ƒè¯­è¨€å¤„ç†éƒ½ä»¥ä¸€ä¸ªé«˜ç»´åº¦çš„ä¸­å¿ƒé˜¶æ®µä¸ºåŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.15471v3">PDF</a> Published as conference paper at ICLR 2025</p>
<p><strong>Summary</strong>ï¼šè¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰å°†è¯­è¨€ä¸Šä¸‹æ–‡æ˜ å°„åˆ°è¾“å‡ºæ ‡è®°ä¸Šï¼Œä½†å…¶æ˜ å°„çš„å‡ ä½•ç‰¹æ€§ä¸å…¶åŠŸèƒ½ä¹‹é—´çš„å…³ç³»å°šä¸æ¸…æ¥šã€‚ç ”ç©¶é‡‡ç”¨é«˜çº§å‡ ä½•åˆ†ææ–¹æ³•ï¼Œè§‚å¯Ÿåˆ°äº”ä¸ªé¢„è®­ç»ƒåŸºäºè½¬æ¢å™¨çš„LMå’Œä¸‰ä¸ªè¾“å…¥æ•°æ®é›†ä¹‹é—´å­˜åœ¨ä¸€ä¸ªé«˜å†…åœ¨ç»´åº¦ç‰¹å¾æ˜æ˜¾çš„é˜¶æ®µã€‚åœ¨è¿™ä¸ªé˜¶æ®µï¼Œè¯­è¨€æ¨¡å‹çš„è¡¨ç¤ºå¯¹åº”äºè¾“å…¥çš„ç¬¬ä¸€ä¸ªå®Œæ•´è¯­è¨€æŠ½è±¡ï¼Œèƒ½å¤Ÿåˆ‡å®è½¬ç§»åˆ°ä¸‹æ¸¸ä»»åŠ¡ï¼Œå¹¶åœ¨ä¸åŒçš„LMä¹‹é—´äº’ç›¸é¢„æµ‹ã€‚æ­¤å¤–ï¼Œé˜¶æ®µå¼€å§‹çš„æ—©æ™šå¼ºçƒˆé¢„ç¤ºç€è¯­è¨€å»ºæ¨¡æ€§èƒ½çš„å¥½åã€‚æ€»ä¹‹ï¼Œç»“æœè¡¨æ˜æ ¸å¿ƒè¯­è¨€å¤„ç†åœ¨è®¸å¤šå¸¸è§LMæ¶æ„ä¸­éƒ½æœ‰ä¸€ä¸ªé«˜ç»´åº¦çš„ä¸­å¿ƒé˜¶æ®µã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>è¯­è¨€æ¨¡å‹å°†è¯­è¨€ä¸Šä¸‹æ–‡æ˜ å°„åˆ°è¾“å‡ºæ ‡è®°ä¸Šï¼Œä½†å…¶æ˜ å°„çš„å‡ ä½•ç‰¹æ€§ä¸åŠŸèƒ½å…³ç³»å°šä¸æ¸…æ¥šã€‚</li>
<li>å­˜åœ¨ä¸€ä¸ªé«˜å†…åœ¨ç»´åº¦ç‰¹å¾æ˜æ˜¾çš„é˜¶æ®µï¼Œè¿™æ˜¯è®¸å¤šé¢„è®­ç»ƒåŸºäºè½¬æ¢å™¨çš„LMå’Œè¾“å…¥æ•°æ®é›†ä¹‹é—´çš„å…±åŒç‰¹å¾ã€‚</li>
<li>åœ¨è¿™ä¸ªé˜¶æ®µï¼Œè¯­è¨€æ¨¡å‹çš„è¡¨ç¤ºå¯¹åº”äºè¾“å…¥çš„ç¬¬ä¸€ä¸ªå®Œæ•´è¯­è¨€æŠ½è±¡ã€‚</li>
<li>è¿™ä¸€é˜¶æ®µçš„è¡¨ç¤ºèƒ½å¤Ÿåˆ‡å®è½¬ç§»åˆ°ä¸‹æ¸¸ä»»åŠ¡ã€‚</li>
<li>ä¸åŒLMä¹‹é—´çš„è¡¨ç¤ºåœ¨è¿™ä¸€é˜¶æ®µäº’ç›¸é¢„æµ‹ã€‚</li>
<li>é˜¶æ®µå¼€å§‹çš„æ—©æ™šå¼ºçƒˆé¢„ç¤ºè¯­è¨€å»ºæ¨¡æ€§èƒ½çš„å¥½åã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.15471">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-32ba7aa8f9b16da83b25afce60b81830.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9156bdb4aa747bdc4743c5b9412a6333.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bbb57232f8edff3469b65bdfc8a1d24a.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-29/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-29/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-29/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-caf86bb6b82999c627b07f01da571c08.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-29  Multi-Agent Geospatial Copilots for Remote Sensing Workflows
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-29
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-28/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-f78bf0bcc38d4120b1f1b1dc59cb2409.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-28  SyncAnimation A Real-Time End-to-End Framework for Audio-Driven Human   Pose and Talking Head Animation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-28
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">28879.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
