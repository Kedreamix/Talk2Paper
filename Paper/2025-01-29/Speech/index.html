<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech 方向最新论文已更新，请持续关注 Update in 2025-01-29  Mixture-of-Mamba Enhancing Multi-Modal State-Space Models with   Modality-Aware Sparsity">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-856797e45217b0a2fa09970219a9b08c.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-01-29
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-02-12
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    6.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    26 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-01-29-更新"><a href="#2025-01-29-更新" class="headerlink" title="2025-01-29 更新"></a>2025-01-29 更新</h1><h2 id="Mixture-of-Mamba-Enhancing-Multi-Modal-State-Space-Models-with-Modality-Aware-Sparsity"><a href="#Mixture-of-Mamba-Enhancing-Multi-Modal-State-Space-Models-with-Modality-Aware-Sparsity" class="headerlink" title="Mixture-of-Mamba: Enhancing Multi-Modal State-Space Models with   Modality-Aware Sparsity"></a>Mixture-of-Mamba: Enhancing Multi-Modal State-Space Models with   Modality-Aware Sparsity</h2><p><strong>Authors:Weixin Liang, Junhong Shen, Genghan Zhang, Ning Dong, Luke Zettlemoyer, Lili Yu</strong></p>
<p>State Space Models (SSMs) have emerged as efficient alternatives to Transformers for sequential modeling, but their inability to leverage modality-specific features limits their performance in multi-modal pretraining. Here, we propose Mixture-of-Mamba, a novel SSM architecture that introduces modality-aware sparsity through modality-specific parameterization of the Mamba block. Building on Mixture-of-Transformers (W. Liang et al. arXiv:2411.04996; 2024), we extend the benefits of modality-aware sparsity to SSMs while preserving their computational efficiency. We evaluate Mixture-of-Mamba across three multi-modal pretraining settings: Transfusion (interleaved text and continuous image tokens with diffusion loss), Chameleon (interleaved text and discrete image tokens), and an extended three-modality framework incorporating speech. Mixture-of-Mamba consistently reaches the same loss values at earlier training steps with significantly reduced computational costs. In the Transfusion setting, Mixture-of-Mamba achieves equivalent image loss using only 34.76% of the training FLOPs at the 1.4B scale. In the Chameleon setting, Mixture-of-Mamba reaches similar image loss with just 42.50% of the FLOPs at the 1.4B scale, and similar text loss with just 65.40% of the FLOPs. In the three-modality setting, MoM matches speech loss at 24.80% of the FLOPs at the 1.4B scale. Our ablation study highlights the synergistic effects of decoupling projection components, where joint decoupling yields greater gains than individual modifications. These results establish modality-aware sparsity as a versatile and effective design principle, extending its impact from Transformers to SSMs and setting new benchmarks in multi-modal pretraining. Our code can be accessed at <a target="_blank" rel="noopener" href="https://github.com/Weixin-Liang/Mixture-of-Mamba">https://github.com/Weixin-Liang/Mixture-of-Mamba</a> </p>
<blockquote>
<p>状态空间模型（SSMs）作为序列建模的有效替代方案，已经崭露头角。然而，它们无法利用特定模态的特征，这在多模态预训练中限制了性能。在这里，我们提出了名为Mixture-of-Mamba的新型SSM架构，它通过Mamba块的特定模态参数化引入了模态感知稀疏性。基于Mixture-of-Transformers（W. Liang等人，arXiv:2411.04996；2024），我们将模态感知稀疏性的好处扩展到SSM，同时保持其计算效率。我们在三种多模态预训练环境中评估了Mixture-of-Mamba的效果：Transfusion（交替文本和连续图像令牌与扩散损失）、Chameleon（交替文本和离散图像令牌），以及一个包含语音的扩展三模态框架。Mixture-of-Mamba在较早的训练步骤中始终达到相同的损失值，并大大降低了计算成本。在Transfusion设置中，Mixture-of-Mamba仅使用34.76%的训练浮点运算量就达到了相当的图像损失值在规模为1.4B时。在Chameleon设置中，Mixture-of-Mamba在规模为1.4B时仅以42.5%的浮点运算量达到相似的图像损失值，并以仅65.4%的浮点运算量达到相似的文本损失值。在三模态设置中，MoM在规模为1.4B时仅以24.8%的浮点运算量达到语音损失匹配水平。我们的消融研究突出了分离投影组件的协同作用，联合分离比单独修改能获得更大的收益。这些结果确立了模态感知稀疏性作为一种通用且有效的设计原则的地位，将其影响从Transformer扩展到SSM，并为多模态预训练设定了新的基准。我们的代码可访问于 <a target="_blank" rel="noopener" href="https://github.com/Weixin-Liang/Mixture-of-Mamba">https://github.com/Weixin-Liang/Mixture-of-Mamba</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.16295v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>状态空间模型（SSMs）作为序列建模的有效替代方案，展现出其高效性，但在多模态预训练中的性能受限于无法利用模态特定特征。为此，我们提出了Mixture-of-Mamba这一新型SSM架构，它通过模态特定参数化Mamba块引入模态感知稀疏性。在Mixture-of-Transformers的基础上，我们拓展了模态感知稀疏性的优势至SSM，同时保持了其计算效率。Mixture-of-Mamba在多模态预训练的三种设置中进行了评估：融合（文本和连续图像标记交替出现并使用扩散损失）、变色龙（交替使用文本和离散图像标记），以及一个包含语音的三模态扩展框架。Mixture-of-Mamba在减少计算成本的同时，能够在早期训练步骤达到相同的损失值。在融合设置中，Mixture-of-Mamba仅使用34.76%的训练浮点运算（FLOPs）便达到相同的图像损失值。在变色龙设置中，它在1.4B规模下以42.5%的FLOPs达到相似的图像损失，并以65.4%的FLOPs达到相似的文本损失。在三模态设置中，MoM在1.4B规模下以24.8%的FLOPs匹配语音损失。我们的消融研究突出了分离投影组件的协同作用，联合分离比单独修改产生更大的收益。这些结果证明了模态感知稀疏性是一种通用且有效的设计原则，它从Transformer扩展至SSM，并为多模态预训练设定了新的基准。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>状态空间模型（SSMs）在计算效率上展现出优势，但多模态预训练性能受限。</li>
<li>Mixture-of-Mamba架构通过引入模态感知稀疏性，提升了SSM在多模态预训练中的性能。</li>
<li>Mixture-of-Mamba在多种多模态预训练设置中进行评估，包括Transfusion、Chameleon和三模态扩展框架。</li>
<li>Mixture-of-Mamba能在早期训练步骤达到相同损失值，同时显著降低计算成本。</li>
<li>在特定的多模态预训练设置中，Mixture-of-Mamba使用较少的计算资源即可达到相似的性能。</li>
<li>消融研究强调了分离投影组件的协同作用，联合分离效果更佳。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.16295">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-14747c41ace67ff7773e867250de6aac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c593bd28eb817c10d325b6d340b0adff.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bfd5921954709e2b60233a89cba5b74c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-56724732d79d512d18d92353a3000a0e.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Enhancing-and-Exploring-Mild-Cognitive-Impairment-Detection-with-W2V-BERT-2-0"><a href="#Enhancing-and-Exploring-Mild-Cognitive-Impairment-Detection-with-W2V-BERT-2-0" class="headerlink" title="Enhancing and Exploring Mild Cognitive Impairment Detection with   W2V-BERT-2.0"></a>Enhancing and Exploring Mild Cognitive Impairment Detection with   W2V-BERT-2.0</h2><p><strong>Authors:Yueguan Wang, Tatsunari Matsushima, Soichiro Matsushima, Toshimitsu Sakai</strong></p>
<p>This study explores a multi-lingual audio self-supervised learning model for detecting mild cognitive impairment (MCI) using the TAUKADIAL cross-lingual dataset. While speech transcription-based detection with BERT models is effective, limitations exist due to a lack of transcriptions and temporal information. To address these issues, the study utilizes features directly from speech utterances with W2V-BERT-2.0. We propose a visualization method to detect essential layers of the model for MCI classification and design a specific inference logic considering the characteristics of MCI. The experiment shows competitive results, and the proposed inference logic significantly contributes to the improvements from the baseline. We also conduct detailed analysis which reveals the challenges related to speaker bias in the features and the sensitivity of MCI classification accuracy to the data split, providing valuable insights for future research. </p>
<blockquote>
<p>本研究探索了一种多语言音频自监督学习模型，该模型使用TAUKADIAL跨语言数据集检测轻度认知障碍（MCI）。虽然基于BERT模型的语音转录检测是有效的，但由于缺乏转录和时序信息，仍存在局限性。为了解决这些问题，研究直接从语音片段中提取特征，使用W2V-BERT-2.0。我们提出了一种可视化方法来检测用于MCI分类的关键模型层，并设计了一种考虑MCI特征的特定推理逻辑。实验显示结果具有竞争力，所提出的推理逻辑对基线改进做出了重大贡献。我们还进行了详细分析，揭示了与说话者偏见相关的挑战以及MCI分类精度对数据分割的敏感性，为未来研究提供了宝贵见解。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.16201v1">PDF</a> Submitted to ICASSP-SPADE workshop 2025</p>
<p><strong>Summary</strong></p>
<p>本研究利用多语言音频自监督学习模型，结合TAUKADIAL跨语言数据集，对轻度认知障碍（MCI）进行检测。研究采用基于语音特征的W2V-BERT-2.0模型，克服了依赖转录的BERT模型因缺乏转录和时序信息而存在的局限性。此外，研究还提出了一种可视化方法，用于检测用于MCI分类的关键模型层，并设计了针对MCI特性的推理逻辑。实验表明，该方法具有竞争力，提出的推理逻辑对改进基线有显著贡献。同时，研究还详细分析了与说话者相关的特征挑战和数据分割对MCI分类精度的影响，为未来的研究提供了宝贵见解。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究采用多语言音频自监督学习模型，利用TAUKADIAL跨语言数据集进行轻度认知障碍（MCI）检测。</li>
<li>针对现有基于语音转录的BERT模型的局限性，引入基于语音特征的W2V-BERT-2.0模型。</li>
<li>首次提出可视化方法以检测用于MCI分类的关键模型层。</li>
<li>设计针对MCI特性的推理逻辑，显著提高了检测准确率。</li>
<li>实验结果显示所提出的方法具有竞争力。</li>
<li>研究发现说话者相关的特征挑战对MCI分类的影响。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.16201">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-9ba56485901ec948017c82e099fbf417.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fd3e724db987cb92b9df5ee2c4c7fe24.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8617c8707a3dba1458050f175736f363.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d8414d8a5772f0fec74d6d9861688e20.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Optimized-Self-supervised-Training-with-BEST-RQ-for-Speech-Recognition"><a href="#Optimized-Self-supervised-Training-with-BEST-RQ-for-Speech-Recognition" class="headerlink" title="Optimized Self-supervised Training with BEST-RQ for Speech Recognition"></a>Optimized Self-supervised Training with BEST-RQ for Speech Recognition</h2><p><strong>Authors:Ilja Baumann, Dominik Wagner, Korbinian Riedhammer, Tobias Bocklet</strong></p>
<p>Self-supervised learning has been successfully used for various speech related tasks, including automatic speech recognition. BERT-based Speech pre-Training with Random-projection Quantizer (BEST-RQ) has achieved state-of-the-art results in speech recognition. In this work, we further optimize the BEST-RQ approach using Kullback-Leibler divergence as an additional regularizing loss and multi-codebook extension per cluster derived from low-level feature clustering. Preliminary experiments on train-100 split of LibriSpeech result in a relative improvement of 11.2% on test-clean by using multiple codebooks, utilizing a combination of cross-entropy and Kullback-Leibler divergence further reduces the word error rate by 4.5%. The proposed optimizations on full LibriSpeech pre-training and fine-tuning result in relative word error rate improvements of up to 23.8% on test-clean and 30.6% on test-other using 6 codebooks. Furthermore, the proposed setup leads to faster convergence in pre-training and fine-tuning and additionally stabilizes the pre-training. </p>
<blockquote>
<p>自监督学习已成功应用于各种语音相关任务，包括自动语音识别。基于BERT的随机投影量化器（BEST-RQ）的语音预训练在语音识别方面取得了最先进的成果。在这项工作中，我们进一步使用Kullback-Leibler散度作为额外的正则化损失，并利用基于低级别特征聚类的每个集群的多代码本扩展来优化BEST-RQ方法。在LibriSpeech的train-100分割上进行初步实验，通过使用多个代码本，测试clean上的相对改进了11.2%。结合交叉熵和Kullback-Leibler散度，进一步降低了单词错误率4.5%。对LibriSpeech的完全预训练和微调提出的优化，在测试clean上相对单词错误率提高了23.8%，在测试其他上提高了30.6%，使用6个代码本。此外，该设置还导致了预训练和微调中的更快收敛，并额外稳定了预训练。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.16131v1">PDF</a> ICASSP 2025</p>
<p><strong>Summary</strong><br>本文介绍了基于BERT的语音预训练方法BEST-RQ的优化研究。通过使用Kullback-Leibler散度作为额外的正则化损失和多代码本扩展，对LibriSpeech数据集进行初步实验，取得了显著的词错误率改进。优化后的方法在测试集clean上相对改进了高达23.8%，在其他测试集上相对改进了30.6%，同时使用多种代码本还能加速预训练和微调过程的收敛，并增强其稳定性。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>BERT-based Speech pre-Training with Random-projection Quantizer (BEST-RQ)已用于语音识别任务，并获得了业界最佳结果。</li>
<li>研究者对BEST-RQ方法进行了优化，采用Kullback-Leibler散度作为正则化损失和多代码本扩展方法。</li>
<li>在LibriSpeech数据集上进行的初步实验显示，使用多个代码本相较于未优化模型降低了词错误率（WER）。在测试集clean上的相对改进达到了高达23.8%，在其他测试集上的相对改进达到了高达30.6%。</li>
<li>结合交叉熵和Kullback-Leibler散度的优化方法进一步降低了词错误率。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.16131">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-126122bfcc926849ac66a5899fe2ff4c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f7b4949477a259e0d62cba0d80ab1181.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b01835cbc5c8b0b8ad613ad3d63e3453.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-59d955566b1d17e136570dacd7b7f899.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-87289884f3e654ed26f54f21002e7088.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4d845c74f918cb28e3802d2cd1458f6b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-80829edbd42aa38d3aa9cad9b45fec92.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Classification-Error-Bound-for-Low-Bayes-Error-Conditions-in-Machine-Learning"><a href="#Classification-Error-Bound-for-Low-Bayes-Error-Conditions-in-Machine-Learning" class="headerlink" title="Classification Error Bound for Low Bayes Error Conditions in Machine   Learning"></a>Classification Error Bound for Low Bayes Error Conditions in Machine   Learning</h2><p><strong>Authors:Zijian Yang, Vahe Eminyan, Ralf Schlüter, Hermann Ney</strong></p>
<p>In statistical classification and machine learning, classification error is an important performance measure, which is minimized by the Bayes decision rule. In practice, the unknown true distribution is usually replaced with a model distribution estimated from the training data in the Bayes decision rule. This substitution introduces a mismatch between the Bayes error and the model-based classification error. In this work, we apply classification error bounds to study the relationship between the error mismatch and the Kullback-Leibler divergence in machine learning. Motivated by recent observations of low model-based classification errors in many machine learning tasks, bounding the Bayes error to be lower, we propose a linear approximation of the classification error bound for low Bayes error conditions. Then, the bound for class priors are discussed. Moreover, we extend the classification error bound for sequences. Using automatic speech recognition as a representative example of machine learning applications, this work analytically discusses the correlations among different performance measures with extended bounds, including cross-entropy loss, language model perplexity, and word error rate. </p>
<blockquote>
<p>在统计分类和机器学习领域，分类误差是一项重要的性能度量，通过贝叶斯决策规则来最小化。在实践中，贝叶斯决策规则中的未知真实分布通常会用基于训练数据估计的模型分布来替代。这种替代引入了贝叶斯误差和基于模型的分类误差之间的不匹配。在这项工作中，我们应用分类误差界限来研究机器学习中的误差不匹配与Kullback-Leibler散度之间的关系。受近期许多机器学习任务中观察到的低模型分类误差的启发，为了将贝叶斯误差限定在较低范围，我们提出了低贝叶斯误差条件下的分类误差界限的线性近似。然后，讨论了类别先验的界限。此外，我们扩展了序列的分类误差界限。以自动语音识别作为机器学习的代表性应用，这项工作通过扩展的界限分析讨论了不同的性能度量之间的相关性，包括交叉熵损失、语言模型困惑度和词错误率。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.15977v1">PDF</a> accepted at ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>本文探讨了统计分类和机器学习中的分类错误衡量标准，介绍了贝叶斯决策规则中的误差与模型分布之间的不匹配问题。文章通过分类误差界限研究了误差不匹配与Kullback-Leibler散度的关系，提出了低贝叶斯误差条件下的分类误差界限的线性近似方法，并讨论了类别先验的界限。此外，文章还将分类误差界限扩展到序列，以自动语音识别为例，讨论了不同性能衡量标准（包括交叉熵损失、语言模型困惑度和词错误率）的关联。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>分类错误是衡量机器学习模型性能的重要标准，贝叶斯决策规则旨在最小化此误差。</li>
<li>在实践中，模型的未知真实分布通常使用从训练数据中估计出的模型分布来替代，这引入了贝叶斯误差和模型基于的分类误差之间的不匹配。</li>
<li>文章通过分类误差界限研究了误差不匹配与Kullback-Leibler散度的关系。</li>
<li>对于低贝叶斯误差条件，文章提出了分类误差界限的线性近似方法。</li>
<li>文章讨论了类别先验的界限。</li>
<li>文章将分类误差界限扩展到序列，并探讨了自动语音识别等机器学习应用中不同性能衡量标准之间的关联。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.15977">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-b47ba7063698dae18fd3c177488f22b9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-88a2dac0fc0a959e0df501f23a725a8b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-20f8f278d19704a66fad6f352e965534.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="AnyEnhance-A-Unified-Generative-Model-with-Prompt-Guidance-and-Self-Critic-for-Voice-Enhancement"><a href="#AnyEnhance-A-Unified-Generative-Model-with-Prompt-Guidance-and-Self-Critic-for-Voice-Enhancement" class="headerlink" title="AnyEnhance: A Unified Generative Model with Prompt-Guidance and   Self-Critic for Voice Enhancement"></a>AnyEnhance: A Unified Generative Model with Prompt-Guidance and   Self-Critic for Voice Enhancement</h2><p><strong>Authors:Junan Zhang, Jing Yang, Zihao Fang, Yuancheng Wang, Zehua Zhang, Zhuo Wang, Fan Fan, Zhizheng Wu</strong></p>
<p>We introduce AnyEnhance, a unified generative model for voice enhancement that processes both speech and singing voices. Based on a masked generative model, AnyEnhance is capable of handling both speech and singing voices, supporting a wide range of enhancement tasks including denoising, dereverberation, declipping, super-resolution, and target speaker extraction, all simultaneously and without fine-tuning. AnyEnhance introduces a prompt-guidance mechanism for in-context learning, which allows the model to natively accept a reference speaker’s timbre. In this way, it could boost enhancement performance when a reference audio is available and enable the target speaker extraction task without altering the underlying architecture. Moreover, we also introduce a self-critic mechanism into the generative process for masked generative models, yielding higher-quality outputs through iterative self-assessment and refinement. Extensive experiments on various enhancement tasks demonstrate AnyEnhance outperforms existing methods in terms of both objective metrics and subjective listening tests. Demo audios are publicly available at <a target="_blank" rel="noopener" href="https://amphionspace.github.io/anyenhance/">https://amphionspace.github.io/anyenhance/</a>. </p>
<blockquote>
<p>我们介绍了AnyEnhance，这是一个统一的生成模型，用于处理语音和歌声的增强。基于掩模生成模型，AnyEnhance能够同时处理语音和歌声，支持广泛的增强任务，包括去噪、去混响、去剪辑、超分辨率和目标说话人提取，而且无需微调。AnyEnhance引入了一种上下文学习中的提示引导机制，允许模型原生接受参考说话人的音色。这样，当参考音频可用时，它可以提高增强性能，并在不改变底层架构的情况下实现目标说话人提取任务。此外，我们还将在生成过程中为掩模生成模型引入自我批判机制，通过迭代自我评估和细化产生更高质量的输出。对各种增强任务的广泛实验表明，AnyEnhance在客观指标和主观听觉测试方面均优于现有方法。演示音频可在<a target="_blank" rel="noopener" href="https://amphionspace.github.io/anyenhance/%E5%85%AC%E5%BC%BA%E3%80%82">https://amphionspace.github.io/anyenhance/公开访问。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.15417v1">PDF</a> 12 pages, 4 figures</p>
<p><strong>Summary</strong></p>
<p>AnyEnhance是一个统一的生成模型，用于处理语音和歌唱声音的增强。它基于掩模生成模型，支持多种增强任务，如去噪、去混响、去剪辑、超分辨率和目标说话人提取。AnyEnhance引入提示指导机制，可在有参考音频的情况下提高增强性能，并启用目标说话人提取任务。此外，它还引入了自我批判机制，通过迭代自我评估和细化产生更高质量的输出。实验表明，AnyEnhance在客观指标和主观听觉测试方面优于现有方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AnyEnhance是一个统一的生成模型，用于语音和歌唱声音增强。</li>
<li>支持多种增强任务，包括去噪、去混响、去剪辑、超分辨率和目标说话人提取。</li>
<li>AnyEnhance通过引入提示指导机制，在有参考音频的情况下提高增强性能。</li>
<li>引入自我批判机制，通过迭代自我评估和细化提高输出质量。</li>
<li>该模型可以同时处理多种任务，无需微调。</li>
<li>AnyEnhance在客观指标和主观听觉测试方面的性能优于现有方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.15417">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-d77a7db8d6a59a7eb4cb95c821a7884a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b397074400c2907518f223b2e1be6a4c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6dc93f2ba7f3f4dcbb459304d0351e88.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-772bd3a411096a0ad3324ffd7af9bfb8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8453d9b3f560a716e3b29a7d6d9fbc8c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-856797e45217b0a2fa09970219a9b08c.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Robust-Cross-Etiology-and-Speaker-Independent-Dysarthric-Speech-Recognition"><a href="#Robust-Cross-Etiology-and-Speaker-Independent-Dysarthric-Speech-Recognition" class="headerlink" title="Robust Cross-Etiology and Speaker-Independent Dysarthric Speech   Recognition"></a>Robust Cross-Etiology and Speaker-Independent Dysarthric Speech   Recognition</h2><p><strong>Authors:Satwinder Singh, Qianli Wang, Zihan Zhong, Clarion Mendes, Mark Hasegawa-Johnson, Waleed Abdulla, Seyed Reza Shahamiri</strong></p>
<p>In this paper, we present a speaker-independent dysarthric speech recognition system, with a focus on evaluating the recently released Speech Accessibility Project (SAP-1005) dataset, which includes speech data from individuals with Parkinson’s disease (PD). Despite the growing body of research in dysarthric speech recognition, many existing systems are speaker-dependent and adaptive, limiting their generalizability across different speakers and etiologies. Our primary objective is to develop a robust speaker-independent model capable of accurately recognizing dysarthric speech, irrespective of the speaker. Additionally, as a secondary objective, we aim to test the cross-etiology performance of our model by evaluating it on the TORGO dataset, which contains speech samples from individuals with cerebral palsy (CP) and amyotrophic lateral sclerosis (ALS). By leveraging the Whisper model, our speaker-independent system achieved a CER of 6.99% and a WER of 10.71% on the SAP-1005 dataset. Further, in cross-etiology settings, we achieved a CER of 25.08% and a WER of 39.56% on the TORGO dataset. These results highlight the potential of our approach to generalize across unseen speakers and different etiologies of dysarthria. </p>
<blockquote>
<p>本文介绍了一个独立于说话者的言语障碍语音识别系统。我们重点关注于评估最近发布的语音无障碍项目（SAP-1005）数据集，该数据集包含帕金森病患者（PD）的语音数据。尽管言语障碍语音识别的研究日益增多，但许多现有系统都是依赖于说话者和自适应的，限制了它们在不同说话者和病因中的通用性。我们的主要目标是开发一个稳健的独立于说话者的模型，能够准确识别言语障碍语音，而无论说话者如何。此外，我们的次要目标是通过在TORGO数据集上评估模型来测试模型的跨病因性能。TORGO数据集包含来自脑瘫（CP）和肌萎缩性侧索硬化症（ALS）患者的语音样本。通过利用Whisper模型，我们的独立于说话者的系统实现了SAP-1005数据集上的字符错误率（CER）为6.99%，单词错误率（WER）为10.71%。此外，在跨病因设置下，我们在TORGO数据集上实现了CER为25.08%，WER为39.56%。这些结果突显了我们的方法在不同未见过的说话者和不同的言语障碍病因中的通用潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.14994v1">PDF</a> Accepted to ICASSP 2025</p>
<p><strong>Summary</strong>：<br>本研究提出一个不依赖于特定说话者的语言障碍语音识别系统，重点在于评估最新发布的语音可访问性项目（SAP-1005）数据集，其中包含帕金森病患者（PD）的语音数据。系统旨在开发一个稳健的跨说话者模型，能够准确识别语言障碍语音，无论说话者如何。使用whisper模型，该模型在SAP-1005数据集上取得了字符错误率（CER）为6.99%，词错误率（WER）为10.71%的成绩。此外，在跨疾病设置下，模型在TORGO数据集上取得了CER为25.08%，WER为39.56%的成绩。这些结果表明，该方法具有跨未见说话者和不同语言障碍类型的推广潜力。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>研究提出了一个不依赖于特定说话者的语言障碍语音识别系统。</li>
<li>系统评估了SAP-1005数据集，包含帕金森病患者（PD）的语音数据。</li>
<li>利用whisper模型，系统在SAP-1005数据集上实现了较好的识别效果。</li>
<li>系统尝试在TORGO数据集上进行跨疾病测试，取得了一定成绩。</li>
<li>该系统具有跨未见说话者和不同语言障碍类型的推广潜力。</li>
<li>该研究填补了关于语言障碍语音识别的现有系统的不足，如依赖特定说话者或缺乏跨不同说话者和病因的泛化能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.14994">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-81c3bd3c93a12b84de617404ee1febf8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c091d923febf2fd1328c49543f1c7752.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5247a86fdd00756951f49cac853e4ea0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6999031f2367bf57302d11d6042f29c6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c27f3b12cd882f0a61b0620c7363e852.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="DeSTA2-Developing-Instruction-Following-Speech-Language-Model-Without-Speech-Instruction-Tuning-Data"><a href="#DeSTA2-Developing-Instruction-Following-Speech-Language-Model-Without-Speech-Instruction-Tuning-Data" class="headerlink" title="DeSTA2: Developing Instruction-Following Speech Language Model Without   Speech Instruction-Tuning Data"></a>DeSTA2: Developing Instruction-Following Speech Language Model Without   Speech Instruction-Tuning Data</h2><p><strong>Authors:Ke-Han Lu, Zhehuai Chen, Szu-Wei Fu, Chao-Han Huck Yang, Jagadeesh Balam, Boris Ginsburg, Yu-Chiang Frank Wang, Hung-yi Lee</strong></p>
<p>Recent end-to-end speech language models (SLMs) have expanded upon the capabilities of large language models (LLMs) by incorporating pre-trained speech models. However, these SLMs often undergo extensive speech instruction-tuning to bridge the gap between speech and text modalities. This requires significant annotation efforts and risks catastrophic forgetting of the original language capabilities. In this work, we present a simple yet effective automatic process for creating speech-text pair data that carefully injects speech paralinguistic understanding abilities into SLMs while preserving the inherent language capabilities of the text-based LLM. Our model demonstrates general capabilities for speech-related tasks without the need for speech instruction-tuning data, achieving impressive performance on Dynamic-SUPERB and AIR-Bench-Chat benchmarks. Furthermore, our model exhibits the ability to follow complex instructions derived from LLMs, such as specific output formatting and chain-of-thought reasoning. Our approach not only enhances the versatility and effectiveness of SLMs but also reduces reliance on extensive annotated datasets, paving the way for more efficient and capable speech understanding systems. </p>
<blockquote>
<p>最近的端到端语音语言模型（SLM）通过融入预训练的语音模型，扩大了大型语言模型（LLM）的能力。然而，这些SLM通常需要进行大量的语音指令调整，以弥合语音和文本模态之间的差距。这需要大量的标注工作，并存在原有语言能力灾难性遗忘的风险。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.20007v2">PDF</a> Accepted by ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>近期端对端语音语言模型（SLM）结合预训练语音模型，提升了大型语言模型（LLM）的功能。然而，这些SLM需要经过大量的语音指令调整来弥合语音与文本模态之间的差距，这需要大量的标注工作并存在遗忘原始语言能力的风险。本研究提出了一种简单有效的自动创建语音文本对数据的过程，该方法能够谨慎地将语音副语言理解力注入SLM中，同时保留文本基础LLM的固有语言能力。我们的模型在动态SUPERB和AIR-Bench-Chat基准测试中表现出色，无需语音指令调整数据即可进行语音相关任务。此外，我们的模型展现出遵循LLM派生的复杂指令的能力，如特定的输出格式和链式思维推理。我们的方法不仅提高了SLM的通用性和效率，还减少了对面大量标注数据集的依赖，为构建更高效、更强大的语音理解系统铺平了道路。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>端对端语音语言模型（SLM）结合了预训练语音模型以提升大型语言模型（LLM）的功能。</li>
<li>SLMs需要通过语音指令调整来桥接语音和文本模态之间的差距，这需要大量标注数据并存在遗忘原始语言能力的风险。</li>
<li>提出了一种自动创建语音文本对数据的方法，以注入语音副语言理解力并保留LLM的固有语言能力。</li>
<li>模型在多个基准测试中表现出色，无需额外的语音指令调整即可执行语音相关任务。</li>
<li>模型能够遵循复杂的指令，包括特定的输出格式和链式思维推理。</li>
<li>方法提高了SLM的通用性和效率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.20007">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-bb12237f5028b2ef8c4688ba3bd7db85.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-58f6ab46ca8afa96573693f49a1013dd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1e1917e83ea95b32bc22dbb5f4a38fc4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ef2fd4ebdb88d017ed0400cb0f713b7.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-29/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-29/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-29/GAN/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-b14916c16d3d32298e69c2323e02fc21.jpg" class="responsive-img" alt="GAN">
                        
                        <span class="card-title">GAN</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            GAN 方向最新论文已更新，请持续关注 Update in 2025-01-29  Can Location Embeddings Enhance Super-Resolution of Satellite Imagery?
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-29
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/GAN/" class="post-category">
                                    GAN
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/GAN/">
                        <span class="chip bg-color">GAN</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-29/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F_Breast%20Ultrasound/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-2811f111cf82d1b45ea48dbd665cbe52.jpg" class="responsive-img" alt="医学影像/Breast Ultrasound">
                        
                        <span class="card-title">医学影像/Breast Ultrasound</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学影像/Breast Ultrasound 方向最新论文已更新，请持续关注 Update in 2025-01-29  Tumor Detection, Segmentation and Classification Challenge on Automated   3D Breast Ultrasound The TDSC-ABUS Challenge
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-29
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/" class="post-category">
                                    医学影像/Breast Ultrasound
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/">
                        <span class="chip bg-color">医学影像/Breast Ultrasound</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">18863.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
