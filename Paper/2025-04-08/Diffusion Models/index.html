<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-08  Multimodal Diffusion Bridge with Attention-Based SAR Fusion for   Satellite Image Cloud Removal">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-41e1a901a3024111ae1b95393e3c2a50.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-08
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    15.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    62 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-04-08-æ›´æ–°"><a href="#2025-04-08-æ›´æ–°" class="headerlink" title="2025-04-08 æ›´æ–°"></a>2025-04-08 æ›´æ–°</h1><h2 id="Multimodal-Diffusion-Bridge-with-Attention-Based-SAR-Fusion-for-Satellite-Image-Cloud-Removal"><a href="#Multimodal-Diffusion-Bridge-with-Attention-Based-SAR-Fusion-for-Satellite-Image-Cloud-Removal" class="headerlink" title="Multimodal Diffusion Bridge with Attention-Based SAR Fusion for   Satellite Image Cloud Removal"></a>Multimodal Diffusion Bridge with Attention-Based SAR Fusion for   Satellite Image Cloud Removal</h2><p><strong>Authors:Yuyang Hu, Suhas Lohit, Ulugbek S. Kamilov, Tim K. Marks</strong></p>
<p>Deep learning has achieved some success in addressing the challenge of cloud removal in optical satellite images, by fusing with synthetic aperture radar (SAR) images. Recently, diffusion models have emerged as powerful tools for cloud removal, delivering higher-quality estimation by sampling from cloud-free distributions, compared to earlier methods. However, diffusion models initiate sampling from pure Gaussian noise, which complicates the sampling trajectory and results in suboptimal performance. Also, current methods fall short in effectively fusing SAR and optical data. To address these limitations, we propose Diffusion Bridges for Cloud Removal, DB-CR, which directly bridges between the cloudy and cloud-free image distributions. In addition, we propose a novel multimodal diffusion bridge architecture with a two-branch backbone for multimodal image restoration, incorporating an efficient backbone and dedicated cross-modality fusion blocks to effectively extract and fuse features from synthetic aperture radar (SAR) and optical images. By formulating cloud removal as a diffusion-bridge problem and leveraging this tailored architecture, DB-CR achieves high-fidelity results while being computationally efficient. We evaluated DB-CR on the SEN12MS-CR cloud-removal dataset, demonstrating that it achieves state-of-the-art results. </p>
<blockquote>
<p>æ·±åº¦å­¦ä¹ é€šè¿‡èåˆåˆæˆå­”å¾„é›·è¾¾ï¼ˆSARï¼‰å›¾åƒï¼Œåœ¨è§£å†³å…‰å­¦å«æ˜Ÿå›¾åƒä¸­çš„å»äº‘æŒ‘æˆ˜æ–¹é¢å–å¾—äº†ä¸€äº›æˆåŠŸã€‚æœ€è¿‘ï¼Œæ‰©æ•£æ¨¡å‹ä½œä¸ºå»äº‘çš„æœ‰åŠ›å·¥å…·å´­éœ²å¤´è§’ï¼Œé€šè¿‡ä»æ— äº‘åˆ†å¸ƒä¸­è¿›è¡Œé‡‡æ ·ï¼Œä¸æ—©æœŸæ–¹æ³•ç›¸æ¯”ï¼Œå®ç°äº†æ›´é«˜è´¨é‡ä¼°è®¡ã€‚ç„¶è€Œï¼Œæ‰©æ•£æ¨¡å‹ä»çº¯é«˜æ–¯å™ªå£°å¼€å§‹è¿›è¡Œé‡‡æ ·ï¼Œè¿™ä½¿å¾—é‡‡æ ·è½¨è¿¹å¤æ‚åŒ–ï¼Œå¯¼è‡´æ€§èƒ½ä¸ä½³ã€‚æ­¤å¤–ï¼Œå½“å‰çš„æ–¹æ³•åœ¨æœ‰æ•ˆåœ°èåˆSARå’Œå…‰å­¦æ•°æ®æ–¹é¢è¿˜å­˜åœ¨ä¸è¶³ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†å»äº‘æ‰©æ•£æ¡¥ï¼ˆDB-CRï¼‰ï¼Œå®ƒç›´æ¥è¿æ¥äº†å¸¦äº‘å’Œæ— äº‘å›¾åƒåˆ†å¸ƒä¹‹é—´çš„æ¡¥æ¢ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„å¤šæ¨¡æ€æ‰©æ•£æ¡¥æ¶æ„ï¼Œè¯¥æ¶æ„å…·æœ‰ä¸¤åˆ†æ”¯ä¸»å¹²ï¼Œç”¨äºå¤šæ¨¡æ€å›¾åƒæ¢å¤ï¼Œç»“åˆäº†é«˜æ•ˆçš„ä¸»å¹²å’Œä¸“ç”¨çš„è·¨æ¨¡æ€èåˆå—ï¼Œä»¥æœ‰æ•ˆåœ°ä»åˆæˆå­”å¾„é›·è¾¾ï¼ˆSARï¼‰å’Œå…‰å­¦å›¾åƒä¸­æå–å’Œèåˆç‰¹å¾ã€‚é€šè¿‡å°†å»äº‘é—®é¢˜è¡¨è¿°ä¸ºæ‰©æ•£æ¡¥é—®é¢˜å¹¶åˆ©ç”¨è¿™ä¸€é‡èº«å®šåˆ¶çš„æ¶æ„ï¼ŒDB-CRåœ¨è·å¾—é«˜ä¿çœŸç»“æœçš„åŒæ—¶è¿˜å…·æœ‰è®¡ç®—æ•ˆç‡é«˜çš„ç‰¹ç‚¹ã€‚æˆ‘ä»¬åœ¨SEN12MS-CRå»äº‘æ•°æ®é›†ä¸Šè¯„ä¼°äº†DB-CRï¼Œç»“æœè¡¨æ˜å®ƒè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.03607v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æ·±åº¦å­¦ä¹ åœ¨è§£å†³å…‰å­¦å«æ˜Ÿå›¾åƒä¸­çš„äº‘å»é™¤æŒ‘æˆ˜æ–¹é¢çš„è¿›å±•ï¼Œç‰¹åˆ«æ˜¯é€šè¿‡åˆæˆå­”å¾„é›·è¾¾ï¼ˆSARï¼‰å›¾åƒèåˆçš„æ–¹æ³•ã€‚è™½ç„¶æ‰©æ•£æ¨¡å‹åœ¨äº‘å»é™¤æ–¹é¢å±•ç°å‡ºå¼ºå¤§çš„æ½œåŠ›ï¼Œä½†ç”±äºå…¶ä»çº¯é«˜æ–¯å™ªå£°å¼€å§‹é‡‡æ ·ï¼Œå¯¼è‡´é‡‡æ ·è½¨è¿¹å¤æ‚ä¸”æ€§èƒ½ä¸ä½³ã€‚ä¸ºäº†å…‹æœè¿™äº›é™åˆ¶ï¼Œæå‡ºäº†åä¸ºâ€œDB-CRâ€çš„æ‰©æ•£æ¡¥äº‘å»é™¤æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç›´æ¥è¿æ¥å¸¦äº‘å’Œå»äº‘å›¾åƒåˆ†å¸ƒï¼Œå¹¶é‡‡ç”¨äº†æ–°é¢–çš„å¤šæ¨¡æ€æ‰©æ•£æ¡¥æ¶æ„ï¼Œå®ç°é«˜æ•ˆçš„å¤šæ¨¡æ€å›¾åƒæ¢å¤ã€‚DB-CRåœ¨äº‘å»é™¤é—®é¢˜ä¸Šè¡¨ç°å‡ºå“è¶Šæ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ å·²æˆåŠŸåº”ç”¨äºå…‰å­¦å«æ˜Ÿå›¾åƒä¸­çš„äº‘å»é™¤æŒ‘æˆ˜ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹æ˜¯äº‘å»é™¤çš„æœ‰åŠ›å·¥å…·ï¼Œä½†é‡‡æ ·è¿‡ç¨‹å¤æ‚ä¸”æ€§èƒ½æœ‰å¾…æå‡ã€‚</li>
<li>å½“å‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆèåˆSARå’Œå…‰å­¦æ•°æ®ã€‚</li>
<li>æå‡ºäº†åä¸ºâ€œDB-CRâ€çš„æ‰©æ•£æ¡¥äº‘å»é™¤æ–¹æ³•ï¼Œç›´æ¥è¿æ¥å¸¦äº‘å’Œå»äº‘å›¾åƒåˆ†å¸ƒã€‚</li>
<li>DB-CRé‡‡ç”¨æ–°é¢–çš„å¤šæ¨¡æ€æ‰©æ•£æ¡¥æ¶æ„ï¼ŒåŒ…æ‹¬ä¸¤åˆ†æ”¯éª¨å¹²ç½‘å’Œå¤šæ¨¡æ€ç‰¹å¾èåˆæ¨¡å—ï¼Œæé«˜ç‰¹å¾æå–ä¸èåˆèƒ½åŠ›ã€‚</li>
<li>DB-CRæ–¹æ³•åœ¨äº‘å»é™¤é—®é¢˜ä¸Šå®ç°é«˜ä¿çœŸåº¦ä¸”è®¡ç®—é«˜æ•ˆçš„ç»“æœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.03607">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ccd4cbbc0e3384b11cadd4df29d94e0b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5174af32dfa215fc722d078612320454.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1eb6648f8795b5eff016fabc68374960.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8d0f4365181560b7c6f36917717cdbbd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a9f9a634e9c983ef0ac005a7264c923c.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="BUFF-Bayesian-Uncertainty-Guided-Diffusion-Probabilistic-Model-for-Single-Image-Super-Resolution"><a href="#BUFF-Bayesian-Uncertainty-Guided-Diffusion-Probabilistic-Model-for-Single-Image-Super-Resolution" class="headerlink" title="BUFF: Bayesian Uncertainty Guided Diffusion Probabilistic Model for   Single Image Super-Resolution"></a>BUFF: Bayesian Uncertainty Guided Diffusion Probabilistic Model for   Single Image Super-Resolution</h2><p><strong>Authors:Zihao He, Shengchuan Zhang, Runze Hu, Yunhang Shen, Yan Zhang</strong></p>
<p>Super-resolution (SR) techniques are critical for enhancing image quality, particularly in scenarios where high-resolution imagery is essential yet limited by hardware constraints. Existing diffusion models for SR have relied predominantly on Gaussian models for noise generation, which often fall short when dealing with the complex and variable texture inherent in natural scenes. To address these deficiencies, we introduce the Bayesian Uncertainty Guided Diffusion Probabilistic Model (BUFF). BUFF distinguishes itself by incorporating a Bayesian network to generate high-resolution uncertainty masks. These masks guide the diffusion process, allowing for the adjustment of noise intensity in a manner that is both context-aware and adaptive. This novel approach not only enhances the fidelity of super-resolved images to their original high-resolution counterparts but also significantly mitigates artifacts and blurring in areas characterized by complex textures and fine details. The model demonstrates exceptional robustness against complex noise patterns and showcases superior adaptability in handling textures and edges within images. Empirical evidence, supported by visual results, illustrates the modelâ€™s robustness, especially in challenging scenarios, and its effectiveness in addressing common SR issues such as blurring. Experimental evaluations conducted on the DIV2K dataset reveal that BUFF achieves a notable improvement, with a +0.61 increase compared to baseline in SSIM on BSD100, surpassing traditional diffusion approaches by an average additional +0.20dB PSNR gain. These findings underscore the potential of Bayesian methods in enhancing diffusion processes for SR, paving the way for future advancements in the field. </p>
<blockquote>
<p>è¶…åˆ†è¾¨ç‡ï¼ˆSRï¼‰æŠ€æœ¯å¯¹äºæé«˜å›¾åƒè´¨é‡è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦é«˜åˆ†è¾¨ç‡å›¾åƒä½†ç”±äºç¡¬ä»¶é™åˆ¶è€Œå—é™çš„åœºæ™¯ä¸­å°¤ä¸ºé‡è¦ã€‚ç°æœ‰çš„æ‰©æ•£æ¨¡å‹ä¸»è¦åº”ç”¨äºSRä¸­çš„å™ªå£°ç”Ÿæˆï¼Œä¸»è¦ä¾èµ–äºé«˜æ–¯æ¨¡å‹ï¼Œåœ¨å¤„ç†è‡ªç„¶åœºæ™¯ä¸­å›ºæœ‰çš„å¤æ‚å¤šå˜çº¹ç†æ—¶ï¼Œå¾€å¾€è¡¨ç°ä¸ä½³ã€‚ä¸ºäº†è§£å†³è¿™äº›ä¸è¶³ï¼Œæˆ‘ä»¬å¼•å…¥äº†è´å¶æ–¯ä¸ç¡®å®šæ€§å¼•å¯¼æ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆBUFFï¼‰ã€‚BUFFé€šè¿‡èå…¥è´å¶æ–¯ç½‘ç»œæ¥ç”Ÿæˆé«˜åˆ†è¾¨ç‡ä¸ç¡®å®šæ€§æ©è†œï¼Œä»è€Œå®ç°è‡ªæˆ‘åŒºåˆ†ã€‚è¿™äº›æ©è†œå¼•å¯¼æ‰©æ•£è¿‡ç¨‹ï¼Œä½¿å™ªå£°å¼ºåº¦çš„è°ƒæ•´å…·æœ‰æƒ…å¢ƒæ„ŸçŸ¥å’Œé€‚åº”æ€§ã€‚è¿™ç§æ–°é¢–çš„æ–¹æ³•ä¸ä»…æé«˜äº†è¶…åˆ†è¾¨ç‡å›¾åƒçš„ä¿çœŸåº¦ï¼Œä½¿å…¶æ›´æ¥è¿‘åŸå§‹é«˜åˆ†è¾¨ç‡å›¾åƒï¼Œè¿˜æ˜¾è‘—å‡è½»äº†å¤æ‚çº¹ç†å’Œç»†èŠ‚åŒºåŸŸçš„ä¼ªå½±å’Œæ¨¡ç³Šã€‚è¯¥æ¨¡å‹åœ¨åº”å¯¹å¤æ‚çš„å™ªå£°æ¨¡å¼æ—¶è¡¨ç°å‡ºå‡ºè‰²çš„ç¨³å¥æ€§ï¼Œåœ¨å¤„ç†å›¾åƒä¸­çš„çº¹ç†å’Œè¾¹ç¼˜æ—¶å±•ç°å‡ºå“è¶Šçš„é€‚åº”æ€§ã€‚æœ‰è§†è§‰ç»“æœæ”¯æŒçš„å®è¯è¯æ®è¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„åœºæ™¯ä¸­ç‰¹åˆ«ç¨³å¥ï¼Œå¹¶ä¸”åœ¨è§£å†³SRçš„å¸¸è§é—®é¢˜ï¼ˆå¦‚æ¨¡ç³Šï¼‰æ–¹é¢éå¸¸æœ‰æ•ˆã€‚åœ¨DIV2Kæ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒBUFFåœ¨BSD100ä¸Šçš„SSIMæŒ‡æ•°æé«˜äº†+0.61ï¼Œä¸ä¼ ç»Ÿæ‰©æ•£æ–¹æ³•ç›¸æ¯”ï¼Œå¹³å‡PSNRå¢ç›Šæé«˜äº†+0.20dBã€‚è¿™äº›å‘ç°å‡¸æ˜¾äº†è´å¶æ–¯æ–¹æ³•åœ¨å¢å¼ºSRä¸­çš„æ‰©æ•£è¿‡ç¨‹çš„æ½œåŠ›ï¼Œä¸ºæœªæ¥çš„æŠ€æœ¯è¿›æ­¥å¥ å®šäº†åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.03490v1">PDF</a> 9 pages, 5 figures, AAAI 2025</p>
<p><strong>Summary</strong><br>    æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºè´å¶æ–¯ä¸ç¡®å®šæ€§å¼•å¯¼æ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆBUFFï¼‰çš„è¶…åˆ†è¾¨ç‡æŠ€æœ¯ã€‚è¯¥æ¨¡å‹é€šè¿‡å¼•å…¥è´å¶æ–¯ç½‘ç»œç”Ÿæˆé«˜åˆ†è¾¨ç‡ä¸ç¡®å®šæ€§æ©è†œï¼ŒæŒ‡å¯¼æ‰©æ•£è¿‡ç¨‹ï¼Œè‡ªé€‚åº”åœ°è°ƒæ•´å™ªå£°å¼ºåº¦ï¼Œä»è€Œæé«˜è¶…åˆ†è¾¨ç‡å›¾åƒçš„ä¿çœŸåº¦ï¼Œå¹¶æ˜¾è‘—å‡å°‘å¤æ‚çº¹ç†å’Œç»†èŠ‚åŒºåŸŸçš„ä¼ªå½±å’Œæ¨¡ç³Šã€‚æ¨¡å‹åœ¨å¤æ‚å™ªå£°æ¨¡å¼ä¸‹è¡¨ç°å‡ºå¼ºå¤§çš„ç¨³å¥æ€§ï¼Œå¹¶åœ¨å¤„ç†å›¾åƒçº¹ç†å’Œè¾¹ç¼˜æ–¹é¢è¡¨ç°å‡ºä¼˜å¼‚çš„é€‚åº”æ€§ã€‚åœ¨DIV2Kæ•°æ®é›†ä¸Šçš„å®éªŒè¯„ä¼°æ˜¾ç¤ºï¼Œä¸åŸºçº¿ç›¸æ¯”ï¼ŒBUFFåœ¨BSD100ä¸Šçš„SSIMå¢åŠ äº†0.61ï¼Œå¹³å‡PSNRå¢ç›Šæ¯”ä¼ ç»Ÿæ‰©æ•£æ–¹æ³•é«˜å‡º+0.20dBã€‚è¿™æ˜¾ç¤ºäº†è´å¶æ–¯æ–¹æ³•åœ¨å¢å¼ºæ‰©æ•£è¿‡ç¨‹çš„è¶…åˆ†è¾¨ç‡æŠ€æœ¯ä¸­çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>BUFFæ¨¡å‹å¼•å…¥è´å¶æ–¯ç½‘ç»œç”Ÿæˆé«˜åˆ†è¾¨ç‡ä¸ç¡®å®šæ€§æ©è†œï¼Œç”¨äºæŒ‡å¯¼æ‰©æ•£è¿‡ç¨‹ã€‚</li>
<li>è¯¥æ¨¡å‹èƒ½å¤Ÿè‡ªé€‚åº”åœ°è°ƒæ•´å™ªå£°å¼ºåº¦ï¼Œæé«˜è¶…åˆ†è¾¨ç‡å›¾åƒçš„ä¿çœŸåº¦ã€‚</li>
<li>BUFFæ¨¡å‹åœ¨å¤æ‚çº¹ç†å’Œç»†èŠ‚åŒºåŸŸçš„ä¼ªå½±å’Œæ¨¡ç³Šå‡å°‘æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚</li>
<li>æ¨¡å‹åœ¨å¤æ‚å™ªå£°æ¨¡å¼ä¸‹å…·æœ‰å¼ºå¤§çš„ç¨³å¥æ€§ã€‚</li>
<li>BUFFæ¨¡å‹åœ¨å¤„ç†å›¾åƒçº¹ç†å’Œè¾¹ç¼˜æ–¹é¢è¡¨ç°å‡ºä¼˜å¼‚çš„é€‚åº”æ€§ã€‚</li>
<li>åœ¨DIV2Kæ•°æ®é›†ä¸Šçš„å®éªŒè¯„ä¼°æ˜¾ç¤ºï¼Œä¸åŸºçº¿ç›¸æ¯”ï¼ŒBUFFæ¨¡å‹åœ¨SSIMå’ŒPSNRæŒ‡æ ‡ä¸Šå‡æœ‰æ˜¾è‘—æé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.03490">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a017ae859768a464571729cd5a3704a1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-96b10b9cfaaeeeeb245175ccf01c4835.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2337e67c4d597e1cf00e524a68802fef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-393b42d51f12831f409fae2aa7d07081.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Dynamic-Importance-in-Diffusion-U-Net-for-Enhanced-Image-Synthesis"><a href="#Dynamic-Importance-in-Diffusion-U-Net-for-Enhanced-Image-Synthesis" class="headerlink" title="Dynamic Importance in Diffusion U-Net for Enhanced Image Synthesis"></a>Dynamic Importance in Diffusion U-Net for Enhanced Image Synthesis</h2><p><strong>Authors:Xi Wang, Ziqi He, Yang Zhou</strong></p>
<p>Traditional diffusion models typically employ a U-Net architecture. Previous studies have unveiled the roles of attention blocks in the U-Net. However, they overlook the dynamic evolution of their importance during the inference process, which hinders their further exploitation to improve image applications. In this study, we first theoretically proved that, re-weighting the outputs of the Transformer blocks within the U-Net is a â€œfree lunchâ€ for improving the signal-to-noise ratio during the sampling process. Next, we proposed Importance Probe to uncover and quantify the dynamic shifts in importance of the Transformer blocks throughout the denoising process. Finally, we design an adaptive importance-based re-weighting schedule tailored to specific image generation and editing tasks. Experimental results demonstrate that, our approach significantly improves the efficiency of the inference process, and enhances the aesthetic quality of the samples with identity consistency. Our method can be seamlessly integrated into any U-Net-based architecture. Code: <a target="_blank" rel="noopener" href="https://github.com/Hytidel/UNetReweighting">https://github.com/Hytidel/UNetReweighting</a> </p>
<blockquote>
<p>ä¼ ç»Ÿæ‰©æ•£æ¨¡å‹é€šå¸¸é‡‡ç”¨U-Netæ¶æ„ã€‚ä¹‹å‰çš„ç ”ç©¶å·²ç»æ­ç¤ºäº†æ³¨æ„åŠ›å—åœ¨U-Netä¸­çš„ä½œç”¨ã€‚ç„¶è€Œï¼Œä»–ä»¬å¿½ç•¥äº†æ¨ç†è¿‡ç¨‹ä¸­æ³¨æ„åŠ›å—é‡è¦æ€§åŠ¨æ€å˜åŒ–çš„å½±å“ï¼Œè¿™é˜»ç¢äº†è¿›ä¸€æ­¥åˆ©ç”¨æ¥æé«˜å›¾åƒåº”ç”¨çš„æ•ˆæœã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆä»ç†è®ºä¸Šè¯æ˜äº†é‡æ–°è°ƒæ•´U-Netä¸­Transformerå—çš„è¾“å‡ºï¼Œå¯ä»¥æé«˜é‡‡æ ·è¿‡ç¨‹ä¸­çš„ä¿¡å™ªæ¯”ï¼Œè¿™æ˜¯ä¸€ä¸ªâ€œå…è´¹åˆé¤â€ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æå‡ºäº†é‡è¦æ€§æ¢é’ˆï¼ˆImportance Probeï¼‰ï¼Œä»¥æ­ç¤ºå¹¶é‡åŒ–é™å™ªè¿‡ç¨‹ä¸­Transformerå—é‡è¦æ€§çš„åŠ¨æ€å˜åŒ–ã€‚æœ€åï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§åŸºäºç‰¹å®šå›¾åƒç”Ÿæˆå’Œç¼–è¾‘ä»»åŠ¡çš„è‡ªé€‚åº”é‡è¦æ€§é‡æ–°åŠ æƒè°ƒåº¦ç­–ç•¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—æé«˜äº†æ¨ç†è¿‡ç¨‹çš„æ•ˆç‡ï¼Œå¹¶æé«˜äº†æ ·æœ¬çš„ä¸€è‡´æ€§å’Œç¾å­¦è´¨é‡ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥æ— ç¼é›†æˆåˆ°ä»»ä½•åŸºäºU-Netçš„æ¶æ„ä¸­ã€‚ä»£ç ï¼š<a target="_blank" rel="noopener" href="https://github.com/Hytidel/UNetReweighting%E3%80%82">https://github.com/Hytidel/UNetReweightingã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.03471v1">PDF</a> Accepted to ICME 2025. Appendix &amp; Code:   <a target="_blank" rel="noopener" href="https://github.com/Hytidel/UNetReweighting">https://github.com/Hytidel/UNetReweighting</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†ä¼ ç»Ÿæ‰©æ•£æ¨¡å‹ä¸­çš„U-Netæ¶æ„ï¼Œå¹¶æŒ‡å‡ºä»¥å¾€ç ”ç©¶å¿½è§†äº†æ³¨æ„åŠ›å—åœ¨æ¨ç†è¿‡ç¨‹ä¸­çš„åŠ¨æ€é‡è¦æ€§å˜åŒ–ã€‚æœ¬ç ”ç©¶é€šè¿‡ç†è®ºè¯æ˜äº†é‡æ–°åŠ æƒU-Netä¸­Transformerå—çš„è¾“å‡ºå¯ä»¥æé«˜é‡‡æ ·è¿‡ç¨‹ä¸­çš„ä¿¡å·å™ªå£°æ¯”ã€‚åŒæ—¶ï¼Œæå‡ºäº†é‡è¦æ€§æ¢é’ˆï¼ˆImportance Probeï¼‰æ¥æ­ç¤ºå’Œé‡åŒ–å»å™ªè¿‡ç¨‹ä¸­Transformerå—é‡è¦æ€§çš„åŠ¨æ€å˜åŒ–ã€‚æœ€ç»ˆè®¾è®¡äº†ä¸€ç§åŸºäºè‡ªé€‚åº”é‡è¦æ€§çš„é‡æ–°åŠ æƒè°ƒåº¦ï¼Œé€‚ç”¨äºç‰¹å®šçš„å›¾åƒç”Ÿæˆå’Œç¼–è¾‘ä»»åŠ¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æé«˜äº†æ¨ç†è¿‡ç¨‹çš„æ•ˆç‡ï¼Œå¹¶æé«˜äº†æ ·æœ¬çš„ç¾å­¦è´¨é‡å’Œä¸€è‡´æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¼ ç»Ÿæ‰©æ•£æ¨¡å‹ä¸»è¦é‡‡ç”¨U-Netæ¶æ„ï¼Œä½†å¯¹å…¶å†…éƒ¨æœºåˆ¶çš„ç ”ç©¶ä¸å¤Ÿæ·±å…¥ã€‚</li>
<li>æœ¬ç ”ç©¶å¼ºè°ƒäº†Transformerå—åœ¨U-Netä¸­çš„é‡è¦æ€§åŠå…¶åŠ¨æ€å˜åŒ–çš„é‡è¦æ€§ã€‚</li>
<li>é€šè¿‡ç†è®ºè¯æ˜ï¼Œé‡æ–°åŠ æƒTransformerå—çš„è¾“å‡ºèƒ½å¤Ÿæé«˜ä¿¡å·å™ªå£°æ¯”ï¼Œæ”¹å–„é‡‡æ ·è¿‡ç¨‹ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åä¸ºâ€œé‡è¦æ€§æ¢é’ˆâ€çš„æŠ€æœ¯æ¥é‡åŒ–Transformerå—åœ¨æ¨ç†è¿‡ç¨‹ä¸­çš„é‡è¦æ€§å˜åŒ–ã€‚</li>
<li>è®¾è®¡äº†ä¸€ç§è‡ªé€‚åº”é‡è¦æ€§çš„é‡æ–°åŠ æƒè°ƒåº¦ç­–ç•¥ï¼Œé€‚ç”¨äºå›¾åƒç”Ÿæˆå’Œç¼–è¾‘ä»»åŠ¡ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæé«˜æ¨ç†æ•ˆç‡å¹¶æ”¹å–„å›¾åƒè´¨é‡ï¼ŒåŒæ—¶ä¿æŒä¸€è‡´æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.03471">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-11ee49fe64f9791af7d2c032951695ec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e34a6d8318fc376b9d724af08cbf3b71.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-41e1a901a3024111ae1b95393e3c2a50.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-861e26b90aa8bb65a61070a19f3f4c76.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2bc3418610d20a05e157793dfca67901.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="FaR-Enhancing-Multi-Concept-Text-to-Image-Diffusion-via-Concept-Fusion-and-Localized-Refinement"><a href="#FaR-Enhancing-Multi-Concept-Text-to-Image-Diffusion-via-Concept-Fusion-and-Localized-Refinement" class="headerlink" title="FaR: Enhancing Multi-Concept Text-to-Image Diffusion via Concept Fusion   and Localized Refinement"></a>FaR: Enhancing Multi-Concept Text-to-Image Diffusion via Concept Fusion   and Localized Refinement</h2><p><strong>Authors:Gia-Nghia Tran, Quang-Huy Che, Trong-Tai Dam Vu, Bich-Nga Pham, Vinh-Tiep Nguyen, Trung-Nghia Le, Minh-Triet Tran</strong></p>
<p>Generating multiple new concepts remains a challenging problem in the text-to-image task. Current methods often overfit when trained on a small number of samples and struggle with attribute leakage, particularly for class-similar subjects (e.g., two specific dogs). In this paper, we introduce Fuse-and-Refine (FaR), a novel approach that tackles these challenges through two key contributions: Concept Fusion technique and Localized Refinement loss function. Concept Fusion systematically augments the training data by separating reference subjects from backgrounds and recombining them into composite images to increase diversity. This augmentation technique tackles the overfitting problem by mitigating the narrow distribution of the limited training samples. In addition, Localized Refinement loss function is introduced to preserve subject representative attributes by aligning each conceptâ€™s attention map to its correct region. This approach effectively prevents attribute leakage by ensuring that the diffusion model distinguishes similar subjects without mixing their attention maps during the denoising process. By fine-tuning specific modules at the same time, FaR balances the learning of new concepts with the retention of previously learned knowledge. Empirical results show that FaR not only prevents overfitting and attribute leakage while maintaining photorealism, but also outperforms other state-of-the-art methods. </p>
<blockquote>
<p>åœ¨æ–‡æœ¬åˆ°å›¾åƒçš„ä»»åŠ¡ä¸­ï¼Œç”Ÿæˆå¤šä¸ªæ–°æ¦‚å¿µä»ç„¶æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ã€‚å½“å‰çš„æ–¹æ³•åœ¨å°‘é‡æ ·æœ¬ä¸Šè¿›è¡Œè®­ç»ƒæ—¶ç»å¸¸ä¼šå‡ºç°è¿‡æ‹Ÿåˆçš„æƒ…å†µï¼Œå¹¶ä¸”åœ¨å±æ€§æ³„éœ²æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œç‰¹åˆ«æ˜¯å¯¹äºç±»ä¼¼ç±»åˆ«çš„ä¸»é¢˜ï¼ˆä¾‹å¦‚ä¸¤åªç‰¹å®šçš„ç‹—ï¼‰ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†Fuse-and-Refineï¼ˆFaRï¼‰ï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡ä¸¤ä¸ªå…³é”®è´¡çŒ®è§£å†³è¿™äº›æŒ‘æˆ˜çš„æ–°æ–¹æ³•ï¼šConcept FusionæŠ€æœ¯å’ŒLocalized RefinementæŸå¤±å‡½æ•°ã€‚Concept Fusioné€šè¿‡åˆ†ç¦»å‚è€ƒä¸»é¢˜å’ŒèƒŒæ™¯å¹¶å°†å…¶é‡æ–°ç»„åˆæˆå¤åˆå›¾åƒæ¥ç³»ç»Ÿåœ°å¢å¼ºè®­ç»ƒæ•°æ®ï¼Œä»è€Œå¢åŠ å¤šæ ·æ€§ã€‚è¿™ç§å¢å¼ºæŠ€æœ¯é€šè¿‡å‡è½»æœ‰é™è®­ç»ƒæ ·æœ¬çš„ç‹­çª„åˆ†å¸ƒæ¥è§£å†³è¿‡æ‹Ÿåˆé—®é¢˜ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†Localized RefinementæŸå¤±å‡½æ•°ï¼Œé€šè¿‡å°†å¯¹æ¯ä¸ªæ¦‚å¿µçš„æ³¨æ„åŠ›å›¾ä¸å…¶æ­£ç¡®åŒºåŸŸå¯¹é½æ¥ä¿ç•™ä¸»é¢˜ä»£è¡¨æ€§å±æ€§ã€‚è¿™ç§æ–¹æ³•æœ‰æ•ˆåœ°é˜²æ­¢äº†å±æ€§æ³„éœ²ï¼Œç¡®ä¿æ‰©æ•£æ¨¡å‹åœ¨é™å™ªè¿‡ç¨‹ä¸­åŒºåˆ†ç±»ä¼¼ä¸»é¢˜ï¼Œè€Œä¸ä¼šæ··æ·†å…¶æ³¨æ„åŠ›å›¾ã€‚é€šè¿‡åŒæ—¶å¾®è°ƒç‰¹å®šæ¨¡å—ï¼ŒFaRåœ¨å­¦ä¹ æ–°æ¦‚å¿µå’Œä¿ç•™å·²å­¦çŸ¥è¯†ä¹‹é—´å–å¾—äº†å¹³è¡¡ã€‚ç»éªŒç»“æœè¡¨æ˜ï¼ŒFaRåœ¨ä¿æŒé€¼çœŸæ€§çš„åŒæ—¶ï¼Œé˜²æ­¢äº†è¿‡æ‹Ÿåˆå’Œå±æ€§æ³„éœ²ï¼Œå¹¶ä¸”ä¼˜äºå…¶ä»–æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.03292v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºFuse-and-Refineï¼ˆFaRï¼‰çš„æ–°æ–¹æ³•ï¼Œç”¨äºè§£å†³æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­çš„å¤šä¸ªæ–°æ¦‚å¿µç”Ÿæˆéš¾é¢˜ã€‚è¯¥æ–¹æ³•é€šè¿‡æ¦‚å¿µèåˆæŠ€æœ¯å’Œå±€éƒ¨ç»†åŒ–æŸå¤±å‡½æ•°ä¸¤å¤§å…³é”®è´¡çŒ®ï¼Œè§£å†³äº†å°æ ·æœ¬è®­ç»ƒæ—¶çš„è¿‡æ‹Ÿåˆé—®é¢˜å’Œç±»ä¼¼ç±»åˆ«çš„å±æ€§æ³„éœ²é—®é¢˜ã€‚æ¦‚å¿µèåˆé€šè¿‡åˆ†ç¦»å‚è€ƒä¸»ä½“ä¸èƒŒæ™¯å¹¶é‡æ–°ç»„åˆæˆå¤åˆå›¾åƒï¼Œå¢åŠ äº†è®­ç»ƒçš„å¤šæ ·æ€§ã€‚å±€éƒ¨ç»†åŒ–æŸå¤±å‡½æ•°åˆ™ç¡®ä¿æ¯ä¸ªæ¦‚å¿µçš„æ³¨æ„åŠ›å›¾ä¸å…¶æ­£ç¡®åŒºåŸŸå¯¹é½ï¼Œé˜²æ­¢å±æ€§æ³„éœ²ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒFaRåœ¨é˜²æ­¢è¿‡æ‹Ÿåˆå’Œå±æ€§æ³„éœ²çš„åŒæ—¶ä¿æŒé«˜åº¦é€¼çœŸæ„Ÿï¼Œå¹¶ä¼˜äºå…¶ä»–å…ˆè¿›æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ–¹æ³•åœ¨ç”Ÿæˆå¤šä¸ªæ–°æ¦‚å¿µæ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å°æ ·æœ¬è®­ç»ƒæ—¶å®¹æ˜“è¿‡æ‹Ÿåˆï¼Œä¸”é¢ä¸´ç±»ä¼¼ç±»åˆ«çš„å±æ€§æ³„éœ²é—®é¢˜ã€‚</li>
<li>å¼•å…¥çš„Fuse-and-Refineï¼ˆFaRï¼‰æ–¹æ³•é€šè¿‡æ¦‚å¿µèåˆæŠ€æœ¯å’Œå±€éƒ¨ç»†åŒ–æŸå¤±å‡½æ•°è§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>æ¦‚å¿µèåˆé€šè¿‡åˆ†ç¦»å’Œé‡ç»„å‚è€ƒä¸»ä½“ä¸èƒŒæ™¯ï¼Œå¢åŠ è®­ç»ƒæ•°æ®çš„å¤šæ ·æ€§ï¼Œç¼“è§£è¿‡æ‹Ÿåˆé—®é¢˜ã€‚</li>
<li>å±€éƒ¨ç»†åŒ–æŸå¤±å‡½æ•°ç¡®ä¿æ¯ä¸ªæ¦‚å¿µçš„æ³¨æ„åŠ›å›¾æ­£ç¡®å¯¹é½ï¼Œé˜²æ­¢å±æ€§æ³„éœ²ã€‚</li>
<li>FaRæ–¹æ³•é€šè¿‡åŒæ—¶å¾®è°ƒç‰¹å®šæ¨¡å—ï¼Œå¹³è¡¡æ–°æ¦‚å¿µçš„å­¦ä¹ ä¸å·²å­¦çŸ¥è¯†çš„ä¿ç•™ã€‚</li>
<li>å®è¯ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼ŒFaRåœ¨é˜²æ­¢è¿‡æ‹Ÿåˆå’Œå±æ€§æ³„éœ²çš„åŒæ—¶ä¿æŒå›¾åƒçš„é€¼çœŸåº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.03292">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c08d709386517b1f8913b7ebce761984.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3dcc4cf3bdc007852588ca75fd78d902.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d1e9331ace3549ee26d61d80a6e0fa69.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="How-I-Warped-Your-Noise-a-Temporally-Correlated-Noise-Prior-for-Diffusion-Models"><a href="#How-I-Warped-Your-Noise-a-Temporally-Correlated-Noise-Prior-for-Diffusion-Models" class="headerlink" title="How I Warped Your Noise: a Temporally-Correlated Noise Prior for   Diffusion Models"></a>How I Warped Your Noise: a Temporally-Correlated Noise Prior for   Diffusion Models</h2><p><strong>Authors:Pascal Chang, Jingwei Tang, Markus Gross, Vinicius C. Azevedo</strong></p>
<p>Video editing and generation methods often rely on pre-trained image-based diffusion models. During the diffusion process, however, the reliance on rudimentary noise sampling techniques that do not preserve correlations present in subsequent frames of a video is detrimental to the quality of the results. This either produces high-frequency flickering, or texture-sticking artifacts that are not amenable to post-processing. With this in mind, we propose a novel method for preserving temporal correlations in a sequence of noise samples. This approach is materialized by a novel noise representation, dubbed $\int$-noise (integral noise), that reinterprets individual noise samples as a continuously integrated noise field: pixel values do not represent discrete values, but are rather the integral of an underlying infinite-resolution noise over the pixel area. Additionally, we propose a carefully tailored transport method that uses $\int$-noise to accurately advect noise samples over a sequence of frames, maximizing the correlation between different frames while also preserving the noise properties. Our results demonstrate that the proposed $\int$-noise can be used for a variety of tasks, such as video restoration, surrogate rendering, and conditional video generation. See <a target="_blank" rel="noopener" href="https://warpyournoise.github.io/">https://warpyournoise.github.io/</a> for video results. </p>
<blockquote>
<p>è§†é¢‘ç¼–è¾‘å’Œç”Ÿæˆæ–¹æ³•é€šå¸¸ä¾èµ–äºé¢„å…ˆè®­ç»ƒçš„åŸºäºå›¾åƒçš„æ‰©æ•£æ¨¡å‹ã€‚ç„¶è€Œï¼Œåœ¨æ‰©æ•£è¿‡ç¨‹ä¸­ï¼Œä¾èµ–äºç®€å•çš„å™ªå£°é‡‡æ ·æŠ€æœ¯ä¼šæŸå®³ç»“æœçš„è´¨é‡ï¼Œå› ä¸ºè¿™äº›æŠ€æœ¯ä¸ä¼šä¿ç•™è§†é¢‘ä¸­åç»­å¸§ä¸­å­˜åœ¨çš„ç›¸å…³æ€§ã€‚è¿™ä¼šäº§ç”Ÿé«˜é¢‘é—ªçƒæˆ–çº¹ç†ç²˜è´´ä¼ªå½±ï¼Œè¿™äº›ä¼ªå½±ä¸åˆ©äºè¿›è¡Œåå¤„ç†ã€‚é‰´äºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¿ç•™å™ªå£°æ ·æœ¬åºåˆ—ä¸­æ—¶é—´ç›¸å…³æ€§çš„æ–°æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡ä¸€ç§æ–°å‹å™ªå£°è¡¨ç¤ºæ¥å®ç°ï¼Œç§°ä¸º$\int$-å™ªå£°ï¼ˆç§¯åˆ†å™ªå£°ï¼‰ï¼Œå®ƒå°†å•ä¸ªå™ªå£°æ ·æœ¬é‡æ–°è§£é‡Šä¸ºè¿ç»­é›†æˆçš„å™ªå£°åœºï¼šåƒç´ å€¼ä¸ä»£è¡¨ç¦»æ•£å€¼ï¼Œè€Œæ˜¯åŸºæœ¬æ— é™åˆ†è¾¨ç‡å™ªå£°åœ¨åƒç´ åŒºåŸŸå†…çš„ç§¯åˆ†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç²¾å¿ƒå®šåˆ¶çš„ä¼ è¾“æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä½¿ç”¨$\int$-å™ªå£°å‡†ç¡®åœ°å°†å™ªå£°æ ·æœ¬ä¼ è¾“åˆ°ä¸€ç³»åˆ—å¸§ä¸Šï¼Œæœ€å¤§é™åº¦åœ°æé«˜ä¸åŒå¸§ä¹‹é—´çš„ç›¸å…³æ€§ï¼ŒåŒæ—¶ä¿ç•™å™ªå£°å±æ€§ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„$\int$-å™ªå£°å¯ç”¨äºå¤šç§ä»»åŠ¡ï¼Œå¦‚è§†é¢‘ä¿®å¤ã€æ›¿ä»£æ¸²æŸ“å’Œæ¡ä»¶è§†é¢‘ç”Ÿæˆã€‚è§†é¢‘ç»“æœè¯·å‚è§ï¼š[<a target="_blank" rel="noopener" href="https://warpyournoise.github.io/]">https://warpyournoise.github.io/]</a> ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.03072v1">PDF</a> Accepted at ICLR 2024 (Oral)</p>
<p><strong>æ‘˜è¦</strong><br>    è§†é¢‘ç¼–è¾‘ä¸ç”Ÿæˆæ–¹æ³•å¸¸ä¾èµ–äºé¢„è®­ç»ƒçš„å›¾åƒæ‰©æ•£æ¨¡å‹ã€‚å½“å‰æ–¹æ³•ä¸­çš„å™ªå£°é‡‡æ ·æŠ€æœ¯ç®€é™‹ï¼Œæ— æ³•ä¿ç•™è§†é¢‘åç»­å¸§ä¸­çš„ç›¸å…³æ€§ï¼Œå½±å“ç»“æœè´¨é‡ï¼Œå¯¼è‡´é«˜é¢‘é—ªçƒæˆ–çº¹ç†ç²˜è´´ç­‰ç‘•ç–µï¼Œéš¾ä»¥è¿›è¡ŒåæœŸå¤„ç†ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºä¸€ç§ä¿ç•™å™ªå£°æ ·æœ¬ä¸­æ—¶åºç›¸å…³æ€§çš„æ–°æ–¹æ³•ï¼Œé€šè¿‡ä¸€ç§åä¸º$\int$-noiseï¼ˆç§¯åˆ†å™ªå£°ï¼‰çš„æ–°å‹å™ªå£°è¡¨å¾å®ç°ï¼Œå®ƒå°†å•ä¸ªå™ªå£°æ ·æœ¬é‡æ–°è§£é‡Šä¸ºè¿ç»­æ•´åˆçš„å™ªå£°åœºï¼šåƒç´ å€¼ä¸ä»£è¡¨ç¦»æ•£å€¼ï¼Œè€Œæ˜¯åº•å±‚æ— é™åˆ†è¾¨ç‡å™ªå£°åœ¨åƒç´ åŒºåŸŸå†…çš„ç§¯åˆ†ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§ç²¾å¿ƒè®¾è®¡çš„ä¼ è¾“æ–¹æ³•ï¼Œåˆ©ç”¨$\int$-noiseå‡†ç¡®åœ°å°†å™ªå£°æ ·æœ¬ä¼ è¾“åˆ°ä¸€ç³»åˆ—å¸§ä¸Šï¼Œæœ€å¤§åŒ–å¸§é—´çš„ç›¸å…³æ€§ï¼ŒåŒæ—¶ä¿ç•™å™ªå£°ç‰¹æ€§ã€‚å®éªŒè¯æ˜ï¼Œ$\int$-noiseå¯ç”¨äºè§†é¢‘ä¿®å¤ã€ä»£ç†æ¸²æŸ“å’Œæ¡ä»¶è§†é¢‘ç”Ÿæˆç­‰å¤šé¡¹ä»»åŠ¡ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å½“å‰è§†é¢‘ç¼–è¾‘å’Œç”Ÿæˆæ–¹æ³•ä¾èµ–é¢„è®­ç»ƒçš„å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œä½†å™ªå£°é‡‡æ ·æŠ€æœ¯ç®€é™‹ï¼Œå¯¼è‡´ç»“æœè´¨é‡ä¸‹é™ã€‚</li>
<li>ç§¯åˆ†å™ªå£°ï¼ˆ$\int$-noiseï¼‰æ˜¯ä¸€ç§æ–°å‹å™ªå£°è¡¨å¾ï¼Œå°†å™ªå£°æ ·æœ¬è§£é‡Šä¸ºè¿ç»­æ•´åˆçš„å™ªå£°åœºã€‚</li>
<li>$\int$-noiseèƒ½æé«˜åƒç´ å€¼çš„è¿ç»­æ€§ï¼Œé€šè¿‡ç§¯åˆ†å½¢å¼è¡¨ç¤ºï¼Œä½¿åƒç´ å€¼æˆä¸ºåº•å±‚æ— é™åˆ†è¾¨ç‡å™ªå£°åœ¨åƒç´ åŒºåŸŸå†…çš„ç§¯åˆ†ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäº$\int$-noiseçš„ä¼ è¾“æ–¹æ³•ï¼Œèƒ½å‡†ç¡®åœ°å°†å™ªå£°æ ·æœ¬ä¼ è¾“åˆ°ä¸€ç³»åˆ—å¸§ä¸Šã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½æœ€å¤§åŒ–å¸§é—´çš„ç›¸å…³æ€§ï¼ŒåŒæ—¶ä¿ç•™å™ªå£°ç‰¹æ€§ã€‚</li>
<li>$\int$-noiseå¯ç”¨äºå¤šç§ä»»åŠ¡ï¼ŒåŒ…æ‹¬è§†é¢‘ä¿®å¤ã€ä»£ç†æ¸²æŸ“å’Œæ¡ä»¶è§†é¢‘ç”Ÿæˆã€‚</li>
<li>å¯åœ¨warpyournoise.github.ioç½‘ç«™æŸ¥çœ‹åˆ©ç”¨$\int$-noiseçš„è§†é¢‘ç»“æœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.03072">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5e50f2d11f8fa2a8bcd59bd9dc2739ad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e58c022bc8e4adc3a6705a49fce41799.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7adb4f81d523d9862721fe5ed2c28fee.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1f52d5f6539c55e355673619b84100af.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6d6ad498450e46ab50b666f2cf244cb6.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Comprehensive-Relighting-Generalizable-and-Consistent-Monocular-Human-Relighting-and-Harmonization"><a href="#Comprehensive-Relighting-Generalizable-and-Consistent-Monocular-Human-Relighting-and-Harmonization" class="headerlink" title="Comprehensive Relighting: Generalizable and Consistent Monocular Human   Relighting and Harmonization"></a>Comprehensive Relighting: Generalizable and Consistent Monocular Human   Relighting and Harmonization</h2><p><strong>Authors:Junying Wang, Jingyuan Liu, Xin Sun, Krishna Kumar Singh, Zhixin Shu, He Zhang, Jimei Yang, Nanxuan Zhao, Tuanfeng Y. Wang, Simon S. Chen, Ulrich Neumann, Jae Shin Yoon</strong></p>
<p>This paper introduces Comprehensive Relighting, the first all-in-one approach that can both control and harmonize the lighting from an image or video of humans with arbitrary body parts from any scene. Building such a generalizable model is extremely challenging due to the lack of dataset, restricting existing image-based relighting models to a specific scenario (e.g., face or static human). To address this challenge, we repurpose a pre-trained diffusion model as a general image prior and jointly model the human relighting and background harmonization in the coarse-to-fine framework. To further enhance the temporal coherence of the relighting, we introduce an unsupervised temporal lighting model that learns the lighting cycle consistency from many real-world videos without any ground truth. In inference time, our temporal lighting module is combined with the diffusion models through the spatio-temporal feature blending algorithms without extra training; and we apply a new guided refinement as a post-processing to preserve the high-frequency details from the input image. In the experiments, Comprehensive Relighting shows a strong generalizability and lighting temporal coherence, outperforming existing image-based human relighting and harmonization methods. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†å…¨é¢é‡ç…§æ˜æŠ€æœ¯ï¼Œè¿™æ˜¯ä¸€ç§å…¨æ–°çš„å…¨æ ˆæ–¹æ³•ï¼Œæ—¢èƒ½æ§åˆ¶ä¹Ÿèƒ½åè°ƒæ¥è‡ªå›¾åƒæˆ–è§†é¢‘çš„äººç‰©çš„ä»»æ„èº«ä½“éƒ¨åˆ†åœ¨ä»»ä½•åœºæ™¯ä¸­çš„ç…§æ˜ã€‚æ„å»ºè¿™ç§é€šç”¨æ¨¡å‹æå…·æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºç¼ºä¹æ•°æ®é›†ï¼Œç°æœ‰çš„åŸºäºå›¾åƒçš„é‡æ–°ç…§æ˜æ¨¡å‹ä»…é™äºç‰¹å®šåœºæ™¯ï¼ˆä¾‹å¦‚è„¸éƒ¨æˆ–é™æ€äººç‰©ï¼‰ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å°†é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹é‡æ–°å®šä½ä¸ºé€šç”¨å›¾åƒå…ˆéªŒï¼Œå¹¶åœ¨ç”±ç²—åˆ°ç»†çš„æ¡†æ¶ä¸­è”åˆæ¨¡æ‹Ÿäººç‰©é‡æ–°ç…§æ˜å’ŒèƒŒæ™¯åè°ƒã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜é‡æ–°ç…§æ˜çš„æ—¶åºä¸€è‡´æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ— ç›‘ç£çš„æ—¶åºç…§æ˜æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯ä»¥ä»è®¸å¤šçœŸå®ä¸–ç•Œçš„è§†é¢‘ä¸­å­¦ä¹ ç…§æ˜å‘¨æœŸçš„ä¸€è‡´æ€§ï¼Œæ— éœ€ä»»ä½•çœŸå®æ•°æ®ã€‚åœ¨æ¨ç†é˜¶æ®µï¼Œæˆ‘ä»¬çš„æ—¶åºç…§æ˜æ¨¡å—é€šè¿‡æ—¶ç©ºç‰¹å¾èåˆç®—æ³•ä¸æ‰©æ•£æ¨¡å‹ç›¸ç»“åˆï¼Œæ— éœ€é¢å¤–è®­ç»ƒï¼›æˆ‘ä»¬è¿˜åº”ç”¨äº†ä¸€ç§æ–°çš„å¼•å¯¼ç²¾ç‚¼ä½œä¸ºåå¤„ç†ï¼Œä»¥ä¿ç•™è¾“å…¥å›¾åƒçš„é«˜é¢‘ç»†èŠ‚ã€‚åœ¨å®éªŒä¸­ï¼Œå…¨é¢é‡ç…§æ˜æŠ€æœ¯è¡¨ç°å‡ºå¼ºå¤§çš„é€šç”¨æ€§å’Œç…§æ˜æ—¶åºä¸€è‡´æ€§ï¼Œä¼˜äºç°æœ‰çš„åŸºäºå›¾åƒçš„é‡æ–°ç…§æ˜å’Œåè°ƒæ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.03011v1">PDF</a> Project page:<a target="_blank" rel="noopener" href="https://junyingw.github.io/paper/relighting">https://junyingw.github.io/paper/relighting</a>. Accepted by   CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†å…¨é¢çš„é‡ç…§æ˜æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯èƒ½åœ¨å›¾åƒæˆ–è§†é¢‘ä¸­åŒæ—¶æ§åˆ¶å’Œåè°ƒäººä½“å„éƒ¨ä½çš„ç…§æ˜ã€‚é€šè¿‡å»ºç«‹é€šç”¨æ¨¡å‹æ¥åº”å¯¹æ•°æ®é›†ç¼ºå¤±çš„æŒ‘æˆ˜ï¼Œä½¿å¾—ç°æœ‰åŸºäºå›¾åƒçš„é‡ç…§æ˜æ¨¡å‹å±€é™äºç‰¹å®šåœºæ™¯ï¼ˆå¦‚é¢éƒ¨æˆ–é™æ€äººä½“ï¼‰ã€‚æœ¬æ–‡åˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ä½œä¸ºé€šç”¨å›¾åƒå…ˆéªŒï¼Œå¹¶åœ¨ç²—åˆ°ç»†çš„æ¡†æ¶ä¸­è”åˆå»ºæ¨¡äººä½“é‡ç…§æ˜å’ŒèƒŒæ™¯åè°ƒã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜é‡ç…§æ˜çš„æ—¶ç©ºä¸€è‡´æ€§ï¼Œå¼•å…¥äº†æ— ç›‘ç£çš„æ—¶ç©ºå…‰ç…§æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ä»ä¼—å¤šçœŸå®è§†é¢‘ä¸­å­¦ä¹ å…‰ç…§å‘¨æœŸä¸€è‡´æ€§ï¼Œæ— éœ€ä»»ä½•çœŸå®æ•°æ®ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œå°†æ—¶ç©ºå…‰ç…§æ¨¡å—ä¸æ‰©æ•£æ¨¡å‹é€šè¿‡æ—¶ç©ºç‰¹å¾èåˆç®—æ³•ç›¸ç»“åˆï¼Œå¹¶åº”ç”¨æ–°çš„å¼•å¯¼ç»†åŒ–ä½œä¸ºåå¤„ç†ä»¥ä¿ç•™è¾“å…¥å›¾åƒçš„é«˜é¢‘ç»†èŠ‚ã€‚å®éªŒè¡¨æ˜ï¼Œå…¨é¢é‡ç…§æ˜æŠ€æœ¯å…·æœ‰è‰¯å¥½çš„é€šç”¨æ€§å’Œå…‰ç…§æ—¶é—´ä¸€è‡´æ€§ï¼Œä¼˜äºç°æœ‰çš„åŸºäºå›¾åƒçš„äººä½“é‡ç…§æ˜å’Œåè°ƒæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†å…¨æ–°çš„å…¨é¢é‡ç…§æ˜æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯å¯æ§åˆ¶å¹¶åè°ƒå›¾åƒæˆ–è§†é¢‘ä¸­äººä½“ä»»æ„éƒ¨ä½çš„ç…§æ˜ã€‚</li>
<li>åˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ä½œä¸ºé€šç”¨å›¾åƒå…ˆéªŒï¼Œå®ç°äº†æ¨¡å‹çš„å¹¿æ³›åº”ç”¨ã€‚</li>
<li>åœ¨ç²—åˆ°ç»†çš„æ¡†æ¶ä¸­è”åˆå»ºæ¨¡äººä½“é‡ç…§æ˜å’ŒèƒŒæ™¯åè°ƒã€‚</li>
<li>å¼•å…¥æ— ç›‘ç£çš„æ—¶ç©ºå…‰ç…§æ¨¡å‹ï¼Œæé«˜äº†é‡ç…§æ˜çš„æ—¶ç©ºä¸€è‡´æ€§ã€‚</li>
<li>åœ¨æ¨ç†è¿‡ç¨‹ä¸­ç»“åˆäº†æ—¶ç©ºå…‰ç…§æ¨¡å—ä¸æ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡æ—¶ç©ºç‰¹å¾èåˆç®—æ³•å®ç°ã€‚</li>
<li>é‡‡ç”¨æ–°çš„å¼•å¯¼ç»†åŒ–åå¤„ç†æ¥ä¿ç•™è¾“å…¥å›¾åƒçš„é«˜é¢‘ç»†èŠ‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.03011">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-605bba25a53614d30ee3150c89270656.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c9e8177321b46dfd2b23858c0e4d4544.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6aa3e548da7cb1dd0a53ab8b008c6c8b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4ae9c049e8af8903117b89d3fd658ead.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="DiSRT-In-Bed-Diffusion-Based-Sim-to-Real-Transfer-Framework-for-In-Bed-Human-Mesh-Recovery"><a href="#DiSRT-In-Bed-Diffusion-Based-Sim-to-Real-Transfer-Framework-for-In-Bed-Human-Mesh-Recovery" class="headerlink" title="DiSRT-In-Bed: Diffusion-Based Sim-to-Real Transfer Framework for In-Bed   Human Mesh Recovery"></a>DiSRT-In-Bed: Diffusion-Based Sim-to-Real Transfer Framework for In-Bed   Human Mesh Recovery</h2><p><strong>Authors:Jing Gao, Ce Zheng, Laszlo A. Jeni, Zackory Erickson</strong></p>
<p>In-bed human mesh recovery can be crucial and enabling for several healthcare applications, including sleep pattern monitoring, rehabilitation support, and pressure ulcer prevention. However, it is difficult to collect large real-world visual datasets in this domain, in part due to privacy and expense constraints, which in turn presents significant challenges for training and deploying deep learning models. Existing in-bed human mesh estimation methods often rely heavily on real-world data, limiting their ability to generalize across different in-bed scenarios, such as varying coverings and environmental settings. To address this, we propose a Sim-to-Real Transfer Framework for in-bed human mesh recovery from overhead depth images, which leverages large-scale synthetic data alongside limited or no real-world samples. We introduce a diffusion model that bridges the gap between synthetic data and real data to support generalization in real-world in-bed pose and body inference scenarios. Extensive experiments and ablation studies validate the effectiveness of our framework, demonstrating significant improvements in robustness and adaptability across diverse healthcare scenarios. </p>
<blockquote>
<p>å§åºŠäººä½“ç½‘æ ¼æ¢å¤å¯¹äºå¤šç§åŒ»ç–—å¥åº·åº”ç”¨è‡³å…³é‡è¦ï¼ŒåŒ…æ‹¬ç¡çœ æ¨¡å¼ç›‘æµ‹ã€åº·å¤æ”¯æŒå’Œå‹åŠ›æºƒç–¡é¢„é˜²ã€‚ç„¶è€Œï¼Œç”±äºéšç§å’Œç»è´¹ç­‰æ–¹é¢çš„é™åˆ¶ï¼Œåœ¨è¿™ä¸ªé¢†åŸŸæ”¶é›†å¤§è§„æ¨¡ç°å®ä¸–ç•Œè§†è§‰æ•°æ®é›†æ˜¯éå¸¸å›°éš¾çš„ï¼Œè¿™ç»™æ·±åº¦å­¦ä¹ çš„è®­ç»ƒå’Œéƒ¨ç½²å¸¦æ¥äº†å·¨å¤§æŒ‘æˆ˜ã€‚ç°æœ‰çš„å§åºŠäººä½“ç½‘æ ¼ä¼°è®¡æ–¹æ³•å¾€å¾€ä¸¥é‡ä¾èµ–äºç°å®ä¸–ç•Œæ•°æ®ï¼Œé™åˆ¶äº†å®ƒä»¬åœ¨ä¸åŒå§åºŠåœºæ™¯ä¸­çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¦‚ä¸åŒçš„è¦†ç›–ç‰©å’Œç¯å¢ƒè®¾ç½®ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåŸºäºSim-to-Realè¿ç§»æ¡†æ¶çš„å§åºŠäººä½“ç½‘æ ¼æ¢å¤æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨å¤§è§„æ¨¡åˆæˆæ•°æ®ä»¥åŠæœ‰é™æˆ–æ— ç°å®ä¸–ç•Œæ ·æœ¬ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ‰©æ•£æ¨¡å‹æ¥å¼¥è¡¥åˆæˆæ•°æ®å’ŒçœŸå®æ•°æ®ä¹‹é—´çš„å·®è·ï¼Œä»¥æ”¯æŒåœ¨ç°å®ä¸–ç•Œå§åºŠå§¿åŠ¿å’Œäººä½“æ¨æ–­åœºæ™¯ä¸­çš„æ³›åŒ–ã€‚å¤§é‡çš„å®éªŒå’Œæ¶ˆèç ”ç©¶éªŒè¯äº†æˆ‘ä»¬çš„æ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼Œè¡¨æ˜å®ƒåœ¨å„ç§åŒ»ç–—å¥åº·åœºæ™¯ä¸­æ˜¾è‘—æé«˜äº†ç¨³å¥æ€§å’Œé€‚åº”æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.03006v1">PDF</a> 16 pages, 19 figures. Accepted to CVPR 2025</p>
<p><strong>Summary</strong><br>åœ¨åŒ»ç–—ä¿å¥åº”ç”¨ä¸­ï¼ŒåºŠä½äººä½“ç½‘æ ¼æ¢å¤éå¸¸é‡è¦ä¸”è‡³å…³é‡è¦ï¼ŒåŒ…æ‹¬ç¡çœ æ¨¡å¼ç›‘æµ‹ã€åº·å¤æ”¯æŒå’Œå‹åŠ›æºƒç–¡é¢„é˜²ç­‰ã€‚ç„¶è€Œï¼Œç”±äºå…¶éšç§å’Œæˆæœ¬ç­‰é™åˆ¶å› ç´ ï¼Œåœ¨æ­¤é¢†åŸŸæ”¶é›†å¤§å‹çœŸå®ä¸–ç•Œè§†è§‰æ•°æ®é›†å˜å¾—ååˆ†å›°éš¾ï¼Œç»™æ·±åº¦å­¦ä¹ æ¨¡å‹çš„è®­ç»ƒå’Œéƒ¨ç½²å¸¦æ¥äº†å·¨å¤§æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåŸºäºåˆæˆåˆ°çœŸå®åœºæ™¯çš„è½¬ç§»æ¡†æ¶ï¼Œç”¨äºä»å¤´é¡¶æ·±åº¦å›¾åƒä¸­è¿›è¡ŒåºŠä½äººä½“ç½‘æ ¼æ¢å¤ã€‚è¯¥æ¡†æ¶åˆ©ç”¨å¤§è§„æ¨¡åˆæˆæ•°æ®ä»¥åŠæœ‰é™çš„çœŸå®ä¸–ç•Œæ ·æœ¬æˆ–æ— çœŸå®ä¸–ç•Œæ ·æœ¬ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ‰©æ•£æ¨¡å‹ï¼Œä»¥ç¼©å°åˆæˆæ•°æ®å’ŒçœŸå®æ•°æ®ä¹‹é—´çš„å·®è·ï¼Œæ”¯æŒçœŸå®ä¸–ç•ŒåºŠä½å§¿åŠ¿å’Œäººä½“æ¨æ–­åœºæ™¯çš„æ³›åŒ–ã€‚ç»è¿‡å¹¿æ³›çš„å®éªŒå’Œæ¶ˆèç ”ç©¶ï¼ŒéªŒè¯äº†è¯¥æ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼Œåœ¨å¤šç§åŒ»ç–—ä¿å¥åœºæ™¯ä¸­è¡¨ç°å‡ºäº†æ˜¾è‘—çš„ç¨³å¥æ€§å’Œé€‚åº”æ€§æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åºŠä½äººä½“ç½‘æ ¼æ¢å¤åœ¨åŒ»ç–—ä¿å¥åº”ç”¨ä¸­è‡³å…³é‡è¦ï¼Œå¦‚ç¡çœ ç›‘æµ‹ã€åº·å¤æ”¯æŒå’Œå‹åŠ›æºƒç–¡é¢„é˜²ç­‰ã€‚</li>
<li>æ”¶é›†çœŸå®ä¸–ç•Œè§†è§‰æ•°æ®é›†å­˜åœ¨å›°éš¾ï¼Œéšç§å’Œæˆæœ¬æ˜¯ä¸»è¦é™åˆ¶å› ç´ ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºåˆæˆåˆ°çœŸå®åœºæ™¯çš„è½¬ç§»æ¡†æ¶ï¼Œç”¨äºä»å¤´é¡¶æ·±åº¦å›¾åƒè¿›è¡ŒåºŠä½äººä½“ç½‘æ ¼æ¢å¤ã€‚</li>
<li>æ¡†æ¶åˆ©ç”¨å¤§è§„æ¨¡åˆæˆæ•°æ®å’Œæœ‰é™çš„çœŸå®ä¸–ç•Œæ ·æœ¬æˆ–æ— çœŸå®ä¸–ç•Œæ ·æœ¬ã€‚</li>
<li>å¼•å…¥æ‰©æ•£æ¨¡å‹ä»¥ç¼©å°åˆæˆæ•°æ®å’ŒçœŸå®æ•°æ®ä¹‹é—´çš„å·®è·ã€‚</li>
<li>æ¡†æ¶ç»è¿‡å¹¿æ³›å®éªŒå’Œæ¶ˆèç ”ç©¶éªŒè¯ï¼Œè¡¨ç°æ˜¾è‘—æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.03006">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7dc7bb996087a51c285cb982cbed62ca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-86041bb1a2d23f32c48962522e5d65b5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-80f4d0565125c6a91c61c00f59d82cae.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-41491a6fd0fef7dcc219e38237518e83.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="VLIPP-Towards-Physically-Plausible-Video-Generation-with-Vision-and-Language-Informed-Physical-Prior"><a href="#VLIPP-Towards-Physically-Plausible-Video-Generation-with-Vision-and-Language-Informed-Physical-Prior" class="headerlink" title="VLIPP: Towards Physically Plausible Video Generation with Vision and   Language Informed Physical Prior"></a>VLIPP: Towards Physically Plausible Video Generation with Vision and   Language Informed Physical Prior</h2><p><strong>Authors:Xindi Yang, Baolu Li, Yiming Zhang, Zhenfei Yin, Lei Bai, Liqian Ma, Zhiyong Wang, Jianfei Cai, Tien-Tsin Wong, Huchuan Lu, Xu Jia</strong></p>
<p>Video diffusion models (VDMs) have advanced significantly in recent years, enabling the generation of highly realistic videos and drawing the attention of the community in their potential as world simulators. However, despite their capabilities, VDMs often fail to produce physically plausible videos due to an inherent lack of understanding of physics, resulting in incorrect dynamics and event sequences. To address this limitation, we propose a novel two-stage image-to-video generation framework that explicitly incorporates physics with vision and language informed physical prior. In the first stage, we employ a Vision Language Model (VLM) as a coarse-grained motion planner, integrating chain-of-thought and physics-aware reasoning to predict a rough motion trajectories&#x2F;changes that approximate real-world physical dynamics while ensuring the inter-frame consistency. In the second stage, we use the predicted motion trajectories&#x2F;changes to guide the video generation of a VDM. As the predicted motion trajectories&#x2F;changes are rough, noise is added during inference to provide freedom to the VDM in generating motion with more fine details. Extensive experimental results demonstrate that our framework can produce physically plausible motion, and comparative evaluations highlight the notable superiority of our approach over existing methods. More video results are available on our Project Page: <a target="_blank" rel="noopener" href="https://madaoer.github.io/projects/physically_plausible_video_generation">https://madaoer.github.io/projects/physically_plausible_video_generation</a>. </p>
<blockquote>
<p>è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ˆVDMsï¼‰è¿‘å¹´æ¥å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜åº¦é€¼çœŸçš„è§†é¢‘ï¼Œå¹¶ä½œä¸ºä¸–ç•Œæ¨¡æ‹Ÿå™¨å¼•èµ·äº†ç¤¾åŒºçš„å¹¿æ³›å…³æ³¨ã€‚ç„¶è€Œï¼Œå°½ç®¡VDMså…·æœ‰å¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†ç”±äºç¼ºä¹å¯¹ç‰©ç†çš„å†…åœ¨ç†è§£ï¼Œå®ƒä»¬å¾€å¾€æ— æ³•ç”Ÿæˆç‰©ç†ä¸Šåˆç†çš„è§†é¢‘ï¼Œä»è€Œå¯¼è‡´åŠ¨æ€å’Œäº‹ä»¶åºåˆ—ä¸æ­£ç¡®ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„ä¸¤é˜¶æ®µå›¾åƒåˆ°è§†é¢‘ç”Ÿæˆæ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ˜¾å¼åœ°å°†ç‰©ç†ä¸è§†è§‰å’Œè¯­è¨€ç›¸ç»“åˆï¼Œå½¢æˆç‰©ç†å…ˆéªŒã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œæˆ‘ä»¬é‡‡ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ä½œä¸ºç²—ç•¥çš„è¿åŠ¨è§„åˆ’å™¨ï¼Œé€šè¿‡èå…¥æ€è€ƒå’Œç‰©ç†æ„ŸçŸ¥æ¨ç†æ¥é¢„æµ‹å¤§è‡´çš„è¿åŠ¨è½¨è¿¹æˆ–å˜åŒ–ï¼Œä»¥è¿‘ä¼¼ç°å®ä¸–ç•Œçš„ç‰©ç†åŠ¨æ€å¹¶ç¡®ä¿å¸§é—´çš„ä¸€è‡´æ€§ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œæˆ‘ä»¬ä½¿ç”¨é¢„æµ‹çš„è¿åŠ¨è½¨è¿¹æˆ–å˜åŒ–æ¥æŒ‡å¯¼VDMçš„è§†é¢‘ç”Ÿæˆã€‚ç”±äºé¢„æµ‹çš„è¿åŠ¨è½¨è¿¹æˆ–å˜åŒ–æ˜¯ç²—ç•¥çš„ï¼Œå› æ­¤åœ¨æ¨ç†è¿‡ç¨‹ä¸­åŠ å…¥äº†å™ªå£°ï¼Œä¸ºVDMç”Ÿæˆå…·æœ‰æ›´å¤šç»†èŠ‚çš„è¿åŠ¨æä¾›äº†è‡ªç”±ã€‚å¤§é‡çš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶å¯ä»¥ç”Ÿæˆç‰©ç†ä¸Šåˆç†çš„è¿åŠ¨ï¼Œå¹¶ä¸”ä¸å…¶ä»–æ–¹æ³•çš„æ¯”è¾ƒè¯„ä¼°å‡¸æ˜¾äº†æˆ‘ä»¬æ–¹æ³•çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚æ›´å¤šè§†é¢‘ç»“æœè¯·è®¿é—®æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://madaoer.github.io/projects/physically_plausible_video_generation">https://madaoer.github.io/projects/physically_plausible_video_generation</a> ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.23368v3">PDF</a> 18 pages, 11 figures</p>
<p><strong>æ‘˜è¦</strong></p>
<p>è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ˆVDMsï¼‰è¿‘å¹´å‘å±•è¿…é€Ÿï¼Œèƒ½ç”Ÿæˆé«˜åº¦é€¼çœŸçš„è§†é¢‘ï¼Œä½œä¸ºä¸–ç•Œæ¨¡æ‹Ÿå™¨å¼•èµ·äº†äººä»¬çš„å…³æ³¨ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹å¯¹ç‰©ç†çš„å†…åœ¨ç†è§£ï¼ŒVDMså¾€å¾€æ— æ³•ç”Ÿæˆç‰©ç†ä¸Šå¯è¡Œçš„è§†é¢‘ï¼Œå¯¼è‡´åŠ¨æ€å’Œäº‹ä»¶åºåˆ—ä¸æ­£ç¡®ã€‚ä¸ºè§£å†³è¿™ä¸€é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºä¸€ç§æ–°é¢–çš„ä¸¤é˜¶æ®µå›¾åƒåˆ°è§†é¢‘ç”Ÿæˆæ¡†æ¶ï¼Œæ˜¾å¼åœ°å°†ç‰©ç†ä¸è§†è§‰å’Œè¯­è¨€ç›¸ç»“åˆï¼Œå½¢æˆç‰©ç†å…ˆéªŒã€‚ç¬¬ä¸€é˜¶æ®µï¼Œæˆ‘ä»¬é‡‡ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ä½œä¸ºç²—ç²’åº¦è¿åŠ¨è§„åˆ’å™¨ï¼Œæ•´åˆæ€ç»´é“¾å’Œç‰©ç†æ„ŸçŸ¥æ¨ç†ï¼Œé¢„æµ‹å¤§è‡´çš„è¿åŠ¨è½¨è¿¹&#x2F;å˜åŒ–ï¼Œè¿‘ä¼¼çœŸå®ä¸–ç•Œçš„ç‰©ç†åŠ¨æ€ï¼ŒåŒæ—¶ç¡®ä¿å¸§é—´ä¸€è‡´æ€§ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œæˆ‘ä»¬ä½¿ç”¨é¢„æµ‹çš„è¿åŠ¨è½¨è¿¹&#x2F;å˜åŒ–æ¥æŒ‡å¯¼VDMçš„è§†é¢‘ç”Ÿæˆã€‚ç”±äºé¢„æµ‹çš„è¿åŠ¨è½¨è¿¹&#x2F;å˜åŒ–æ˜¯ç²—ç•¥çš„ï¼Œå› æ­¤åœ¨æ¨ç†è¿‡ç¨‹ä¸­åŠ å…¥äº†å™ªå£°ï¼Œä¸ºVDMç”Ÿæˆå…·æœ‰æ›´å¤šç»†èŠ‚çš„è¿åŠ¨æä¾›äº†è‡ªç”±ã€‚å¤§é‡çš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶èƒ½ç”Ÿæˆç‰©ç†ä¸Šå¯è¡Œçš„è¿åŠ¨ï¼Œå¯¹æ¯”è¯„ä¼°å‡¸æ˜¾äº†æˆ‘ä»¬çš„æ–¹æ³•ç›¸è¾ƒäºç°æœ‰æ–¹æ³•çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚æ›´å¤šè§†é¢‘ç»“æœè¯·è®¿é—®æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://madaoer.github.io/projects/physically_plausible_video_generation">é“¾æ¥</a>ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>VDMsè™½èƒ½ç”Ÿæˆé«˜åº¦é€¼çœŸçš„è§†é¢‘ï¼Œä½†ç¼ºä¹ç‰©ç†ç†è§£ï¼Œå¯¼è‡´ç”Ÿæˆçš„è§†é¢‘åœ¨ç‰©ç†ä¸Šä¸å¯è¡Œã€‚</li>
<li>æå‡ºä¸€ç§æ–°é¢–çš„ä¸¤é˜¶æ®µå›¾åƒåˆ°è§†é¢‘ç”Ÿæˆæ¡†æ¶ï¼Œæ˜¾å¼ç»“åˆç‰©ç†ã€è§†è§‰å’Œè¯­è¨€ã€‚</li>
<li>åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œä½¿ç”¨VLMä½œä¸ºç²—ç²’åº¦è¿åŠ¨è§„åˆ’å™¨ï¼Œé¢„æµ‹å¤§è‡´çš„è¿åŠ¨è½¨è¿¹&#x2F;å˜åŒ–ï¼Œæ¨¡æ‹ŸçœŸå®ä¸–ç•Œçš„ç‰©ç†åŠ¨æ€ã€‚</li>
<li>ç¬¬äºŒé˜¶æ®µåˆ©ç”¨é¢„æµ‹çš„è¿åŠ¨è½¨è¿¹&#x2F;å˜åŒ–æŒ‡å¯¼VDMçš„è§†é¢‘ç”Ÿæˆã€‚</li>
<li>é€šè¿‡æ·»åŠ å™ªå£°åˆ°é¢„æµ‹çš„è¿åŠ¨è½¨è¿¹&#x2F;å˜åŒ–ï¼Œä¸ºVDMç”Ÿæˆæ›´ç²¾ç»†çš„è¿åŠ¨ç»†èŠ‚æä¾›è‡ªç”±ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶èƒ½ç”Ÿæˆç‰©ç†ä¸Šå¯è¡Œçš„è¿åŠ¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.23368">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8ff8165939c420441830e8983084ca09.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6e4dc07b6191aa167a85cb050f37b41b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8062bce093b353679b1a436c4fa86bb1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-27c4bff33ee89fe81ce0a2430e5fef72.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Segment-Any-Quality-Images-with-Generative-Latent-Space-Enhancement"><a href="#Segment-Any-Quality-Images-with-Generative-Latent-Space-Enhancement" class="headerlink" title="Segment Any-Quality Images with Generative Latent Space Enhancement"></a>Segment Any-Quality Images with Generative Latent Space Enhancement</h2><p><strong>Authors:Guangqian Guo, Yong Guo, Xuehui Yu, Wenbo Li, Yaoxing Wang, Shan Gao</strong></p>
<p>Despite their success, Segment Anything Models (SAMs) experience significant performance drops on severely degraded, low-quality images, limiting their effectiveness in real-world scenarios. To address this, we propose GleSAM, which utilizes Generative Latent space Enhancement to boost robustness on low-quality images, thus enabling generalization across various image qualities. Specifically, we adapt the concept of latent diffusion to SAM-based segmentation frameworks and perform the generative diffusion process in the latent space of SAM to reconstruct high-quality representation, thereby improving segmentation. Additionally, we introduce two techniques to improve compatibility between the pre-trained diffusion model and the segmentation framework. Our method can be applied to pre-trained SAM and SAM2 with only minimal additional learnable parameters, allowing for efficient optimization. We also construct the LQSeg dataset with a greater diversity of degradation types and levels for training and evaluating the model. Extensive experiments demonstrate that GleSAM significantly improves segmentation robustness on complex degradations while maintaining generalization to clear images. Furthermore, GleSAM also performs well on unseen degradations, underscoring the versatility of our approach and dataset. </p>
<blockquote>
<p>å°½ç®¡å–å¾—äº†æˆåŠŸï¼Œä½†Segment Anything Modelsï¼ˆSAMï¼‰åœ¨ä¸¥é‡é€€åŒ–ã€ä½è´¨é‡çš„å›¾åƒä¸Šæ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œé™åˆ¶äº†å…¶åœ¨ç°å®åœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†GleSAMã€‚å®ƒåˆ©ç”¨ç”Ÿæˆæ½œåœ¨ç©ºé—´å¢å¼ºæŠ€æœ¯æ¥æé«˜å¯¹ä½è´¨é‡å›¾åƒçš„ç¨³å¥æ€§ï¼Œä»è€Œå®ç°è·¨å„ç§å›¾åƒè´¨é‡çš„æ³›åŒ–ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é€‚åº”åŸºäºSAMçš„åˆ†å‰²æ¡†æ¶ä¸­çš„æ½œåœ¨æ‰©æ•£æ¦‚å¿µï¼Œå¹¶åœ¨SAMçš„æ½œåœ¨ç©ºé—´ä¸­æ‰§è¡Œç”Ÿæˆæ‰©æ•£è¿‡ç¨‹ï¼Œä»¥é‡å»ºé«˜è´¨é‡è¡¨ç¤ºï¼Œä»è€Œæé«˜åˆ†å‰²æ•ˆæœã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸¤ç§æŠ€æœ¯æ¥æé«˜é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹å’Œåˆ†å‰²æ¡†æ¶ä¹‹é—´çš„å…¼å®¹æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥åº”ç”¨äºé¢„è®­ç»ƒçš„SAMå’ŒSAM2ï¼Œå¹¶ä¸”åªéœ€è¦æå°‘é‡çš„é¢å¤–å¯å­¦ä¹ å‚æ•°ï¼Œä»è€Œå®ç°é«˜æ•ˆä¼˜åŒ–ã€‚æˆ‘ä»¬è¿˜æ„å»ºäº†LQSegæ•°æ®é›†ï¼ŒåŒ…å«æ›´å¤šç±»å‹å’Œç¨‹åº¦çš„é€€åŒ–ç±»å‹ï¼Œç”¨äºè®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒGleSAMåœ¨å¤æ‚é€€åŒ–æƒ…å†µä¸‹æ˜¾è‘—æé«˜åˆ†å‰²ç¨³å¥æ€§ï¼ŒåŒæ—¶ä¿æŒå¯¹æ¸…æ™°å›¾åƒçš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒGleSAMåœ¨æœªè§è¿‡çš„é€€åŒ–æƒ…å†µä¸‹ä¹Ÿè¡¨ç°è‰¯å¥½ï¼Œçªæ˜¾äº†æˆ‘ä»¬æ–¹æ³•å’Œæ•°æ®é›†çš„é€šç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.12507v2">PDF</a> Accepted by CVPR2025</p>
<p><strong>Summary</strong></p>
<p>SAMæ¨¡å‹åœ¨å¤„ç†ä½è´¨é‡å›¾åƒæ—¶æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œé™åˆ¶äº†å…¶åœ¨ç°å®åœºæ™¯ä¸­çš„åº”ç”¨ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†GleSAMæ¨¡å‹ï¼Œåˆ©ç”¨ç”Ÿæˆæ½œåœ¨ç©ºé—´å¢å¼ºæŠ€æœ¯æé«˜ä½è´¨é‡å›¾åƒä¸Šçš„ç¨³å¥æ€§ï¼Œä»è€Œå®ç°è·¨ä¸åŒå›¾åƒè´¨é‡çš„æ³›åŒ–ã€‚æˆ‘ä»¬åœ¨SAMåˆ†å‰²æ¡†æ¶ä¸­å¼•å…¥æ½œåœ¨æ‰©æ•£çš„æ¦‚å¿µï¼Œåœ¨SAMçš„æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œç”Ÿæˆæ‰©æ•£è¿‡ç¨‹ï¼Œé‡å»ºé«˜è´¨é‡è¡¨ç¤ºï¼Œä»è€Œæé«˜åˆ†å‰²æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸¤ç§æŠ€æœ¯æ¥æé«˜é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹å’Œåˆ†å‰²æ¡†æ¶ä¹‹é—´çš„å…¼å®¹æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥åº”ç”¨äºé¢„è®­ç»ƒçš„SAMå’ŒSAM2æ¨¡å‹ï¼Œåªéœ€æå°‘é‡çš„é¢å¤–å¯å­¦ä¹ å‚æ•°ï¼Œå°±èƒ½å®ç°é«˜æ•ˆä¼˜åŒ–ã€‚æˆ‘ä»¬è¿˜æ„å»ºäº†LQSegæ•°æ®é›†ï¼ŒåŒ…å«æ›´å¤šç±»å‹å’Œç¨‹åº¦çš„é€€åŒ–ï¼Œç”¨äºè®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹ã€‚å®éªŒè¡¨æ˜ï¼ŒGleSAMåœ¨å¤æ‚é€€åŒ–æƒ…å†µä¸‹æ˜¾è‘—æé«˜åˆ†å‰²ç¨³å¥æ€§ï¼ŒåŒæ—¶ä¿æŒå¯¹æ¸…æ™°å›¾åƒçš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶åœ¨æœªè§é€€åŒ–æƒ…å†µä¸‹è¡¨ç°è‰¯å¥½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SAMæ¨¡å‹åœ¨å¤„ç†ä½è´¨é‡å›¾åƒæ—¶æ€§èƒ½ä¸‹é™ï¼Œé™åˆ¶äº†å…¶åœ¨ç°å®åœºæ™¯çš„åº”ç”¨ã€‚</li>
<li>GleSAMæ¨¡å‹é€šè¿‡åˆ©ç”¨ç”Ÿæˆæ½œåœ¨ç©ºé—´å¢å¼ºæŠ€æœ¯æé«˜SAMæ¨¡å‹åœ¨ä½è´¨é‡å›¾åƒä¸Šçš„ç¨³å¥æ€§ã€‚</li>
<li>GleSAMå¼•å…¥æ½œåœ¨æ‰©æ•£æ¦‚å¿µåˆ°SAMåˆ†å‰²æ¡†æ¶ä¸­ï¼Œæé«˜åˆ†å‰²æ€§èƒ½ã€‚</li>
<li>GleSAMåªéœ€è¦æå°‘é‡çš„é¢å¤–å¯å­¦ä¹ å‚æ•°ï¼Œèƒ½å¤Ÿé«˜æ•ˆä¼˜åŒ–ã€‚</li>
<li>LQSegæ•°æ®é›†ç”¨äºè®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹ï¼ŒåŒ…å«æ›´å¤šç±»å‹å’Œç¨‹åº¦çš„é€€åŒ–ã€‚</li>
<li>GleSAMåœ¨å¤æ‚é€€åŒ–æƒ…å†µä¸‹æ˜¾è‘—æé«˜åˆ†å‰²ç¨³å¥æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.12507">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b276b1fb3993d57bf0c80cb52df61e56.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-706c0f5a7403f2a5702695e4ac981e25.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-18b73d57009227f25e77993249dcefed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8c06d776c69d4c6a959ee190e03d0463.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Training-Free-Style-and-Content-Transfer-by-Leveraging-U-Net-Skip-Connections-in-Stable-Diffusion"><a href="#Training-Free-Style-and-Content-Transfer-by-Leveraging-U-Net-Skip-Connections-in-Stable-Diffusion" class="headerlink" title="Training-Free Style and Content Transfer by Leveraging U-Net Skip   Connections in Stable Diffusion"></a>Training-Free Style and Content Transfer by Leveraging U-Net Skip   Connections in Stable Diffusion</h2><p><strong>Authors:Ludovica Schaerf, Andrea Alfarano, Fabrizio Silvestri, Leonardo Impett</strong></p>
<p>Recent advances in diffusion models for image generation have led to detailed examinations of several components within the U-Net architecture for image editing. While previous studies have focused on the bottleneck layer (h-space), cross-attention, self-attention, and decoding layers, the overall role of the skip connections of the U-Net itself has not been specifically addressed. We conduct thorough analyses on the role of the skip connections and find that the residual connections passed by the third encoder block carry most of the spatial information of the reconstructed image, splitting the content from the style, passed by the remaining stream in the opposed decoding layer. We show that injecting the representations from this block can be used for text-based editing, precise modifications, and style transfer. We compare our method, SkipInject, to state-of-the-art style transfer and image editing methods and demonstrate that our method obtains the best content alignment and optimal structural preservation tradeoff. </p>
<blockquote>
<p>å…³äºæ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆæ–¹é¢çš„æœ€æ–°è¿›å±•ï¼Œå¼•å‘äº†äººä»¬å¯¹å›¾åƒç¼–è¾‘ä¸­U-Netæ¶æ„å†…éƒ¨å‡ ä¸ªç»„ä»¶çš„è¯¦ç»†ç ”ç©¶ã€‚è™½ç„¶ä¹‹å‰çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨ç“¶é¢ˆå±‚ï¼ˆhç©ºé—´ï¼‰ã€è·¨æ³¨æ„åŠ›ã€è‡ªæ³¨æ„åŠ›ä»¥åŠè§£ç å±‚ä¸Šï¼Œä½†å°šæœªå¯¹U-Netæœ¬èº«çš„è·³è·ƒè¿æ¥çš„æ•´ä½“ä½œç”¨è¿›è¡Œä¸“é—¨ç ”ç©¶ã€‚æˆ‘ä»¬å¯¹è·³è·ƒè¿æ¥çš„ä½œç”¨è¿›è¡Œäº†æ·±å…¥åˆ†æï¼Œå‘ç°é€šè¿‡ç¬¬ä¸‰ä¸ªç¼–ç å™¨å—çš„æ®‹å·®è¿æ¥æºå¸¦äº†é‡å»ºå›¾åƒçš„å¤§éƒ¨åˆ†ç©ºé—´ä¿¡æ¯ï¼Œå°†ä»é£æ ¼ä¸­åˆ†ç¦»å‡ºçš„å†…å®¹ä¼ é€’ç»™ç›¸å¯¹è§£ç å±‚ä¸­çš„å‰©ä½™æµã€‚æˆ‘ä»¬è¯æ˜äº†æ³¨å…¥æ­¤å—çš„è¡¨ç¤ºå¯ç”¨äºåŸºäºæ–‡æœ¬ç¼–è¾‘ã€ç²¾ç¡®ä¿®æ”¹å’Œé£æ ¼è½¬æ¢ã€‚æˆ‘ä»¬å°†æˆ‘ä»¬çš„SkipInjectæ–¹æ³•ä¸æœ€å…ˆè¿›çš„é£æ ¼è½¬æ¢å’Œå›¾åƒç¼–è¾‘æ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒï¼Œå¹¶è¯æ˜æˆ‘ä»¬çš„æ–¹æ³•åœ¨å†…å®¹å¯¹é½å’Œæœ€ä½³ç»“æ„ä¿ç•™æ–¹é¢å–å¾—äº†æœ€ä½³å¹³è¡¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.14524v2">PDF</a> Accepted to CVPR Workshop on AI for Creative Visual Content   Generation Editing and Understanding 2025</p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸå…³äºæ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆé¢†åŸŸçš„ç ”ç©¶è¿›å±•å¼•å‘äº†äººä»¬å¯¹U-Netæ¶æ„ä¸­ä¸åŒç»„ä»¶çš„æ·±å…¥ç ”ç©¶ã€‚ç ”ç©¶é›†ä¸­åœ¨ç“¶é¢ˆå±‚ï¼ˆh-spaceï¼‰ã€è·¨æ³¨æ„åŠ›ã€è‡ªæ³¨æ„åŠ›ä»¥åŠè§£ç å±‚ç­‰æ–¹é¢ï¼Œä½†å°šæœªå…·ä½“æ¢è®¨U-Netçš„è·³è·ƒè¿æ¥çš„æ•´ä½“ä½œç”¨ã€‚æœ¬ç ”ç©¶å¯¹è·³è·ƒè¿æ¥è¿›è¡Œäº†æ·±å…¥åˆ†æï¼Œå‘ç°é€šè¿‡ç¬¬ä¸‰ä¸ªç¼–ç å™¨å—çš„æ®‹å·®è¿æ¥ä¼ é€’äº†é‡å»ºå›¾åƒçš„å¤§éƒ¨åˆ†ç©ºé—´ä¿¡æ¯ï¼Œå®ç°äº†å†…å®¹å’Œé£æ ¼çš„åˆ†ç¦»ï¼Œå¹¶åœ¨ç›¸å¯¹åº”çš„è§£ç å±‚ä¼ é€’å‰©ä½™ä¿¡æ¯ã€‚æœ¬ç ”ç©¶å±•ç¤ºäº†æ³¨å…¥æ­¤å—è¡¨ç¤ºçš„æ–¹æ³•å¯ç”¨äºåŸºäºæ–‡æœ¬ç¼–è¾‘ã€ç²¾ç¡®ä¿®æ”¹å’Œé£æ ¼è½¬æ¢ã€‚å¯¹æ¯”å…¶ä»–å…ˆè¿›çš„é£æ ¼è½¬æ¢å’Œå›¾åƒç¼–è¾‘æ–¹æ³•ï¼Œè¯æ˜æœ¬ç ”ç©¶æ–¹æ³•åœ¨å†…å®¹å¯¹é½å’Œç»“æ„ä¿æŒæ–¹é¢å–å¾—æœ€ä½³å¹³è¡¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è·³è·ƒè¿æ¥åœ¨å›¾åƒç¼–è¾‘çš„U-Netæ¶æ„ä¸­æ‰®æ¼”ç€é‡è¦è§’è‰²ï¼Œå°¤å…¶æ˜¯é€šè¿‡ç¬¬ä¸‰ä¸ªç¼–ç å™¨å—çš„æ®‹å·®è¿æ¥ä¼ é€’äº†é‡å»ºå›¾åƒçš„å¤§éƒ¨åˆ†ç©ºé—´ä¿¡æ¯ã€‚</li>
<li>é€šè¿‡è·³è·ƒè¿æ¥å®ç°äº†å†…å®¹å’Œé£æ ¼çš„åˆ†ç¦»ï¼Œæœ‰åŠ©äºç²¾ç¡®ä¿®æ”¹å’Œé£æ ¼è½¬æ¢ã€‚</li>
<li>æœ¬ç ”ç©¶æå‡ºçš„æ–¹æ³•â€”â€”SkipInjectï¼Œåœ¨æ–‡æœ¬ç¼–è¾‘ã€ç²¾ç¡®ä¿®æ”¹å’Œé£æ ¼è½¬æ¢æ–¹é¢è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ã€‚</li>
<li>SkipInjectä¸å…¶ä»–å…ˆè¿›çš„é£æ ¼è½¬æ¢å’Œå›¾åƒç¼–è¾‘æ–¹æ³•ç›¸æ¯”ï¼Œåœ¨å†…å®¹å¯¹é½å’Œç»“æ„ä¿æŒæ–¹é¢å–å¾—äº†æœ€ä½³å¹³è¡¡ã€‚</li>
<li>ç ”ç©¶ç»“æœçªå‡ºäº†è·³è·ƒè¿æ¥åœ¨æ‰©æ•£æ¨¡å‹ä¸­çš„é‡è¦æ€§ï¼Œä¸ºæœªæ¥ç ”ç©¶æä¾›äº†æ–°æ–¹å‘ã€‚</li>
<li>è¯¥æ–¹æ³•å…·æœ‰æ½œåœ¨åº”ç”¨äºå„ç§å›¾åƒç¼–è¾‘åœºæ™¯ï¼Œå¦‚ä¿®å¤ã€åˆæˆå’Œç¾åŒ–ç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.14524">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1793a4722d49d2aa0da4d5ba1fac7152.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-167545f0727bcd57aec737388b98e63c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4acc60771f677f6791fd9e63af28725c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d88bd727efa756ef09a9eb356ec8ff5f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9efb920ef4c5f503d9eb8f6877ad8601.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4fd715d8c37e67d02f53356b913a8892.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a3430dc6eae189c217606ebf62ee1a44.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Zero-Shot-Image-Restoration-Using-Few-Step-Guidance-of-Consistency-Models-and-Beyond"><a href="#Zero-Shot-Image-Restoration-Using-Few-Step-Guidance-of-Consistency-Models-and-Beyond" class="headerlink" title="Zero-Shot Image Restoration Using Few-Step Guidance of Consistency   Models (and Beyond)"></a>Zero-Shot Image Restoration Using Few-Step Guidance of Consistency   Models (and Beyond)</h2><p><strong>Authors:Tomer Garber, Tom Tirer</strong></p>
<p>In recent years, it has become popular to tackle image restoration tasks with a single pretrained diffusion model (DM) and data-fidelity guidance, instead of training a dedicated deep neural network per task. However, such â€œzero-shotâ€ restoration schemes currently require many Neural Function Evaluations (NFEs) for performing well, which may be attributed to the many NFEs needed in the original generative functionality of the DMs. Recently, faster variants of DMs have been explored for image generation. These include Consistency Models (CMs), which can generate samples via a couple of NFEs. However, existing works that use guided CMs for restoration still require tens of NFEs or fine-tuning of the model per task that leads to performance drop if the assumptions during the fine-tuning are not accurate. In this paper, we propose a zero-shot restoration scheme that uses CMs and operates well with as little as 4 NFEs. It is based on a wise combination of several ingredients: better initialization, back-projection guidance, and above all a novel noise injection mechanism. We demonstrate the advantages of our approach for image super-resolution, deblurring and inpainting. Interestingly, we show that the usefulness of our noise injection technique goes beyond CMs: it can also mitigate the performance degradation of existing guided DM methods when reducing their NFE count. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œä½¿ç”¨å•ä¸ªé¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ï¼ˆDMï¼‰å’Œæ•°æ®ä¿çœŸåº¦æŒ‡å¯¼æ¥è§£å†³å›¾åƒæ¢å¤ä»»åŠ¡å˜å¾—éå¸¸æµè¡Œï¼Œè€Œä¸æ˜¯é’ˆå¯¹æ¯ä¸ªä»»åŠ¡è®­ç»ƒä¸€ä¸ªä¸“ç”¨çš„æ·±åº¦ç¥ç»ç½‘ç»œã€‚ç„¶è€Œï¼Œè¿™æ ·çš„â€œé›¶å°„å‡»â€æ¢å¤æ–¹æ¡ˆç›®å‰éœ€è¦å¤§é‡çš„ç¥ç»åŠŸèƒ½è¯„ä¼°ï¼ˆNFEï¼‰æ‰èƒ½è¡¨ç°è‰¯å¥½ï¼Œè¿™å¯èƒ½æ˜¯ç”±äºDMçš„åŸå§‹ç”ŸæˆåŠŸèƒ½éœ€è¦å¤§é‡çš„NFEã€‚æœ€è¿‘ï¼Œå·²ç»æ¢ç´¢äº†ç”¨äºå›¾åƒç”Ÿæˆçš„æ›´å¿«çš„DMå˜ä½“ã€‚è¿™åŒ…æ‹¬ä¸€è‡´æ€§æ¨¡å‹ï¼ˆCMï¼‰ï¼Œå¯ä»¥é€šè¿‡å‡ ä¸ªNFEç”Ÿæˆæ ·æœ¬ã€‚ç„¶è€Œï¼Œç°æœ‰ä½œå“åœ¨ä½¿ç”¨æŒ‡å¯¼æ€§çš„CMè¿›è¡Œæ¢å¤æ—¶ï¼Œä»éœ€è¦æ•°åæ¬¡çš„NFEæˆ–å¯¹æ¨¡å‹çš„å¾®è°ƒï¼Œå¦‚æœåœ¨å¾®è°ƒæœŸé—´çš„å‡è®¾ä¸å‡†ç¡®ï¼Œä¼šå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä½¿ç”¨CMçš„é›¶å°„å‡»æ¢å¤æ–¹æ¡ˆï¼Œä»…éœ€æå°‘çš„4ä¸ªNFEå³å¯æœ‰æ•ˆè¿è¡Œã€‚å®ƒåŸºäºæ˜æ™ºåœ°ç»“åˆäº†å¤šç§æˆåˆ†ï¼šæ›´å¥½çš„åˆå§‹åŒ–ã€åå‘æŠ•å½±æŒ‡å¯¼å’Œæœ€é‡è¦çš„æ˜¯ä¸€ç§æ–°å‹å™ªå£°æ³¨å…¥æœºåˆ¶ã€‚æˆ‘ä»¬å±•ç¤ºäº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨å›¾åƒè¶…åˆ†è¾¨ç‡ã€å»æ¨¡ç³Šå’Œå›¾åƒä¿®å¤æ–¹é¢çš„ä¼˜åŠ¿ã€‚æœ‰è¶£çš„æ˜¯ï¼Œæˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬çš„å™ªå£°æ³¨å…¥æŠ€æœ¯çš„å®ç”¨æ€§è¶…å‡ºäº†CMçš„èŒƒå›´ï¼šå®ƒè¿˜å¯ä»¥ç¼“è§£ç°æœ‰æŒ‡å¯¼å‹DMæ–¹æ³•å‡å°‘NFEè®¡æ•°æ—¶çš„æ€§èƒ½ä¸‹é™é—®é¢˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.20596v2">PDF</a> CVPR 2025 (camera-ready). Code can be found at:   <a target="_blank" rel="noopener" href="https://github.com/tirer-lab/CM4IR">https://github.com/tirer-lab/CM4IR</a></p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æå‡ºä¸€ç§åŸºäºä¸€è‡´æ€§æ¨¡å‹ï¼ˆCMsï¼‰çš„é›¶å°„å›¾åƒæ¢å¤æ–¹æ¡ˆï¼Œä½¿ç”¨å°‘é‡ï¼ˆä»…éœ€4æ¬¡ï¼‰ç¥ç»åŠŸèƒ½è¯„ä¼°ï¼ˆNFEï¼‰å³å¯å®ç°å›¾åƒè¶…åˆ†è¾¨ç‡ã€å»æ¨¡ç³Šå’Œä¿®å¤ç­‰åŠŸèƒ½ã€‚è¯¥æ–¹æ¡ˆé€šè¿‡ä¼˜åŒ–åˆå§‹åŒ–ã€åå‘æŠ•å½±å¼•å¯¼å’Œæ–°å‹å™ªå£°æ³¨å…¥æœºåˆ¶ï¼Œä¸ä»…æé«˜äº†CMsçš„æ•ˆç‡ï¼Œè€Œä¸”èƒ½å¤Ÿç¼“è§£ç°æœ‰å¼•å¯¼æ‰©æ•£æ¨¡å‹æ–¹æ³•å‡å°‘NFEè®¡æ•°æ—¶çš„æ€§èƒ½ä¸‹é™é—®é¢˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†åŸºäºä¸€è‡´æ€§æ¨¡å‹ï¼ˆCMsï¼‰çš„é›¶å°„å›¾åƒæ¢å¤æ–¹æ¡ˆã€‚</li>
<li>ä½¿ç”¨è¾ƒå°‘çš„ç¥ç»åŠŸèƒ½è¯„ä¼°ï¼ˆNFEï¼‰æ¬¡æ•°ï¼ˆä»…éœ€4æ¬¡ï¼‰å³å¯å®ç°å›¾åƒæ¢å¤ã€‚</li>
<li>é€šè¿‡ä¼˜åŒ–åˆå§‹åŒ–ã€åå‘æŠ•å½±å¼•å¯¼å’Œæ–°å‹å™ªå£°æ³¨å…¥æœºåˆ¶ï¼Œæé«˜äº†æ¢å¤æ•ˆæœã€‚</li>
<li>æ–°å‹å™ªå£°æ³¨å…¥æŠ€æœ¯ä¸ä»…é€‚ç”¨äºä¸€è‡´æ€§æ¨¡å‹ï¼Œè¿˜å¯ç¼“è§£ç°æœ‰å¼•å¯¼æ‰©æ•£æ¨¡å‹æ–¹æ³•å‡å°‘NFEæ—¶çš„æ€§èƒ½ä¸‹é™ã€‚</li>
<li>è¯¥æ–¹æ¡ˆå¯¹å›¾åƒè¶…åˆ†è¾¨ç‡ã€å»æ¨¡ç³Šå’Œä¿®å¤ç­‰ä»»åŠ¡æœ‰ä¼˜åŠ¿ã€‚</li>
<li>æ­¤æ–¹æ³•æä¾›äº†ä¸€ç§æ›´é«˜æ•ˆçš„å›¾åƒæ¢å¤é€”å¾„ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚</li>
<li>ç ”ç©¶ä¸ºæ‰©æ•£æ¨¡å‹åœ¨å›¾åƒæ¢å¤ä»»åŠ¡ä¸­çš„åº”ç”¨æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.20596">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-396e6bdb0ba9080c17def9b2bc439f2f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-de2fd2387511a0f144968e898167f92a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-06170f127fbf5af11da2f4dcdf0279ae.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="FADA-Fast-Diffusion-Avatar-Synthesis-with-Mixed-Supervised-Multi-CFG-Distillation"><a href="#FADA-Fast-Diffusion-Avatar-Synthesis-with-Mixed-Supervised-Multi-CFG-Distillation" class="headerlink" title="FADA: Fast Diffusion Avatar Synthesis with Mixed-Supervised Multi-CFG   Distillation"></a>FADA: Fast Diffusion Avatar Synthesis with Mixed-Supervised Multi-CFG   Distillation</h2><p><strong>Authors:Tianyun Zhong, Chao Liang, Jianwen Jiang, Gaojie Lin, Jiaqi Yang, Zhou Zhao</strong></p>
<p>Diffusion-based audio-driven talking avatar methods have recently gained attention for their high-fidelity, vivid, and expressive results. However, their slow inference speed limits practical applications. Despite the development of various distillation techniques for diffusion models, we found that naive diffusion distillation methods do not yield satisfactory results. Distilled models exhibit reduced robustness with open-set input images and a decreased correlation between audio and video compared to teacher models, undermining the advantages of diffusion models. To address this, we propose FADA (Fast Diffusion Avatar Synthesis with Mixed-Supervised Multi-CFG Distillation). We first designed a mixed-supervised loss to leverage data of varying quality and enhance the overall model capability as well as robustness. Additionally, we propose a multi-CFG distillation with learnable tokens to utilize the correlation between audio and reference image conditions, reducing the threefold inference runs caused by multi-CFG with acceptable quality degradation. Extensive experiments across multiple datasets show that FADA generates vivid videos comparable to recent diffusion model-based methods while achieving an NFE speedup of 4.17-12.5 times. Demos are available at our webpage <a target="_blank" rel="noopener" href="http://fadavatar.github.io/">http://fadavatar.github.io</a>. </p>
<blockquote>
<p>åŸºäºæ‰©æ•£çš„éŸ³é¢‘é©±åŠ¨å¯¹è¯å¼åŒ–èº«æ–¹æ³•å› å…¶é«˜ä¿çœŸã€ç”ŸåŠ¨ã€è¡¨è¾¾æ€§å¼ºçš„ç»“æœè€Œè¿‘æœŸå¤‡å—å…³æ³¨ã€‚ç„¶è€Œï¼Œå…¶ç¼“æ…¢çš„æ¨ç†é€Ÿåº¦é™åˆ¶äº†å®é™…åº”ç”¨ã€‚å°½ç®¡å·²ç»å¼€å‘äº†å„ç§ç”¨äºæ‰©æ•£æ¨¡å‹çš„è’¸é¦æŠ€æœ¯ï¼Œä½†æˆ‘ä»¬å‘ç°ç®€å•çš„æ‰©æ•£è’¸é¦æ–¹æ³•å¹¶ä¸èƒ½äº§ç”Ÿä»¤äººæ»¡æ„çš„ç»“æœã€‚ä¸åŸå§‹æ¨¡å‹ç›¸æ¯”ï¼Œè’¸é¦æ¨¡å‹å¯¹å¼€æ”¾é›†è¾“å…¥å›¾åƒçš„ç¨³å¥æ€§é™ä½ï¼ŒéŸ³é¢‘å’Œè§†é¢‘çš„å…³è”æ€§å‡å¼±ï¼Œè¿™å‰Šå¼±äº†æ‰©æ•£æ¨¡å‹çš„ä¼˜åŠ¿ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†FADAï¼ˆå…·æœ‰æ··åˆç›‘ç£å¤šCFGè’¸é¦çš„å¿«é€Ÿæ‰©æ•£åŒ–èº«åˆæˆæ³•ï¼‰ã€‚æˆ‘ä»¬é¦–å…ˆè®¾è®¡äº†ä¸€ç§æ··åˆç›‘ç£æŸå¤±ï¼Œä»¥åˆ©ç”¨ä¸åŒè´¨é‡çš„æ•°æ®ï¼Œå¢å¼ºæ¨¡å‹çš„æ•´ä½“èƒ½åŠ›å’Œç¨³å¥æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å…·æœ‰å¯å­¦ä¹ æ ‡è®°çš„å¤šCFGè’¸é¦æ³•ï¼Œä»¥åˆ©ç”¨éŸ³é¢‘å’Œå‚è€ƒå›¾åƒæ¡ä»¶ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œå‡å°‘å› å¤šCFGå¯¼è‡´çš„ä¸‰å€æ¨ç†è¿è¡Œï¼ŒåŒæ—¶ä¿æŒå¯æ¥å—çš„è´¨é‡ä¸‹é™ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒFADAç”Ÿæˆçš„è§†é¢‘ç”ŸåŠ¨ç¨‹åº¦ä¸æœ€æ–°çš„æ‰©æ•£æ¨¡å‹æ–¹æ³•ç›¸å½“ï¼ŒåŒæ—¶å®ç°äº†4.17è‡³12.5å€çš„NFEåŠ é€Ÿã€‚ç›¸å…³æ¼”ç¤ºè¯·è®¿é—®æˆ‘ä»¬çš„ç½‘é¡µï¼š<a target="_blank" rel="noopener" href="http://fadavatar.github.io/">ç½‘é¡µé“¾æ¥</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.16915v2">PDF</a> CVPR 2025, Homepage <a target="_blank" rel="noopener" href="https://fadavatar.github.io/">https://fadavatar.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºæ‰©æ•£çš„éŸ³é¢‘é©±åŠ¨è¯´è¯äººå¶æ–¹æ³•çš„é«˜ä¿çœŸã€ç”ŸåŠ¨ã€è¡¨è¾¾æ€§å¼ºçš„ç»“æœï¼Œä½†å…¶ç¼“æ…¢çš„æ¨ç†é€Ÿåº¦é™åˆ¶äº†å®é™…åº”ç”¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œä½œè€…æå‡ºäº†FADAï¼ˆå¸¦æœ‰æ··åˆç›‘ç£å¤šCFGè’¸é¦çš„å¿«é€Ÿæ‰©æ•£åŒ–èº«åˆæˆï¼‰ã€‚FADAåˆ©ç”¨ä¸åŒè´¨é‡çš„æ•°æ®è®¾è®¡äº†ä¸€ç§æ··åˆç›‘ç£æŸå¤±ï¼Œæé«˜äº†æ¨¡å‹çš„æ•´ä½“èƒ½åŠ›å’Œé²æ£’æ€§ã€‚æ­¤å¤–ï¼Œä½œè€…è¿˜æå‡ºäº†ä¸€ç§å¸¦æœ‰å¯å­¦ä¹ ä»¤ç‰Œçš„å¤šCFGè’¸é¦æ–¹æ³•ï¼Œåˆ©ç”¨éŸ³é¢‘å’Œå‚è€ƒå›¾åƒæ¡ä»¶ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œå‡å°‘äº†å¤šCFGå¼•èµ·çš„ä¸‰å€æ¨ç†è¿è¡Œçš„å¯æ¥å—çš„æ€§èƒ½ä¸‹é™ã€‚å®éªŒè¡¨æ˜ï¼ŒFADAç”Ÿæˆçš„è§†é¢‘ç”ŸåŠ¨ï¼Œä¸åŸºäºæ‰©æ•£æ¨¡å‹çš„æœ€æ–°æ–¹æ³•ç›¸å½“ï¼ŒåŒæ—¶å®ç°äº†NFEé€Ÿåº¦çš„4.17è‡³12.5å€æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨éŸ³é¢‘é©±åŠ¨çš„è¯´è¯äººå¶æ–¹æ³•ä¸­è¡¨ç°å‡ºé«˜ä¿çœŸã€ç”ŸåŠ¨å’Œè¡¨è¾¾æ€§å¼ºçš„ç»“æœã€‚</li>
<li>ç°æœ‰æ–¹æ³•çš„æ¨ç†é€Ÿåº¦è¾ƒæ…¢ï¼Œé™åˆ¶äº†å®é™…åº”ç”¨ã€‚</li>
<li>FADAæ–¹æ³•é€šè¿‡æ··åˆç›‘ç£æŸå¤±æé«˜æ¨¡å‹çš„æ•´ä½“èƒ½åŠ›å’Œé²æ£’æ€§ã€‚</li>
<li>FADAåˆ©ç”¨éŸ³é¢‘å’Œå‚è€ƒå›¾åƒæ¡ä»¶ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œæå‡ºå¤šCFGè’¸é¦æ–¹æ³•ã€‚</li>
<li>FADAæ–¹æ³•å‡å°‘äº†å¤šCFGå¼•èµ·çš„æ¨ç†è¿è¡Œæ¬¡æ•°ã€‚</li>
<li>å®éªŒè¯æ˜FADAç”Ÿæˆçš„è§†é¢‘è´¨é‡ç”ŸåŠ¨ï¼Œä¸æœ€æ–°æ‰©æ•£æ¨¡å‹ç›¸å½“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.16915">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-21a3d40daf1254ce7b87f9f34a9501b0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0bdd9e54abedc694bf62a3579c231785.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Edge-SD-SR-Low-Latency-and-Parameter-Efficient-On-device-Super-Resolution-with-Stable-Diffusion-via-Bidirectional-Conditioning"><a href="#Edge-SD-SR-Low-Latency-and-Parameter-Efficient-On-device-Super-Resolution-with-Stable-Diffusion-via-Bidirectional-Conditioning" class="headerlink" title="Edge-SD-SR: Low Latency and Parameter Efficient On-device   Super-Resolution with Stable Diffusion via Bidirectional Conditioning"></a>Edge-SD-SR: Low Latency and Parameter Efficient On-device   Super-Resolution with Stable Diffusion via Bidirectional Conditioning</h2><p><strong>Authors:Mehdi Noroozi, Isma Hadji, Victor Escorcia, Anestis Zaganidis, Brais Martinez, Georgios Tzimiropoulos</strong></p>
<p>There has been immense progress recently in the visual quality of Stable Diffusion-based Super Resolution (SD-SR). However, deploying large diffusion models on computationally restricted devices such as mobile phones remains impractical due to the large model size and high latency. This is compounded for SR as it often operates at high res (e.g. 4Kx3K). In this work, we introduce Edge-SD-SR, the first parameter efficient and low latency diffusion model for image super-resolution. Edge-SD-SR consists of ~169M parameters, including UNet, encoder and decoder, and has a complexity of only ~142 GFLOPs. To maintain a high visual quality on such low compute budget, we introduce a number of training strategies: (i) A novel conditioning mechanism on the low resolution input, coined bidirectional conditioning, which tailors the SD model for the SR task. (ii) Joint training of the UNet and encoder, while decoupling the encodings of the HR and LR images and using a dedicated schedule. (iii) Finetuning the decoder using the UNetâ€™s output to directly tailor the decoder to the latents obtained at inference time. Edge-SD-SR runs efficiently on device, e.g. it can upscale a 128x128 patch to 512x512 in 38 msec while running on a Samsung S24 DSP, and of a 512x512 to 2048x2048 (requiring 25 model evaluations) in just ~1.1 sec. Furthermore, we show that Edge-SD-SR matches or even outperforms state-of-the-art SR approaches on the most established SR benchmarks. </p>
<blockquote>
<p>æœ€è¿‘ï¼ŒåŸºäºStable Diffusionçš„è¶…åˆ†è¾¨ç‡ï¼ˆSD-SRï¼‰åœ¨è§†è§‰è´¨é‡æ–¹é¢å–å¾—äº†å·¨å¤§çš„è¿›æ­¥ã€‚ç„¶è€Œï¼Œç”±äºæ¨¡å‹è§„æ¨¡åºå¤§å’Œå»¶è¿Ÿè¾ƒé«˜ï¼Œå°†å¤§å‹æ‰©æ•£æ¨¡å‹éƒ¨ç½²åœ¨è®¡ç®—å—é™çš„è®¾å¤‡ä¸Šï¼ˆå¦‚æ‰‹æœºï¼‰ä»ç„¶ä¸åˆ‡å®é™…ã€‚å¯¹äºè¶…åˆ†è¾¨ç‡ï¼ˆSRï¼‰ä»»åŠ¡æ¥è¯´ï¼Œè¿™ç§æƒ…å†µæ›´ä¸ºä¸¥é‡ï¼Œå› ä¸ºå®ƒé€šå¸¸åœ¨é«˜åˆ†è¾¨ç‡ï¼ˆä¾‹å¦‚4Kx3Kï¼‰ä¸‹è¿è¡Œã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†Edge-SD-SRï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç”¨äºå›¾åƒè¶…åˆ†è¾¨ç‡çš„å‚æ•°é«˜æ•ˆã€ä½å»¶è¿Ÿçš„æ‰©æ•£æ¨¡å‹ã€‚Edge-SD-SRåŒ…å«çº¦1.69äº¿ä¸ªå‚æ•°ï¼ŒåŒ…æ‹¬UNetã€ç¼–ç å™¨å’Œè§£ç å™¨ï¼Œå¤æ‚åº¦ä»…ä¸ºçº¦142 GFLOPsã€‚ä¸ºäº†åœ¨è¿™ç§ä½è®¡ç®—é¢„ç®—ä¸‹ä¿æŒé«˜è§†è§‰è´¨é‡ï¼Œæˆ‘ä»¬å¼•å…¥äº†è®¸å¤šè®­ç»ƒç­–ç•¥ï¼šï¼ˆiï¼‰ä¸€ç§æ–°å‹çš„é’ˆå¯¹ä½åˆ†è¾¨ç‡è¾“å…¥çš„æ¡ä»¶æœºåˆ¶ï¼Œç§°ä¸ºåŒå‘æ¡ä»¶æœºåˆ¶ï¼Œè¯¥æœºåˆ¶ä¸ºSDæ¨¡å‹å®šåˆ¶äº†SRä»»åŠ¡ã€‚ï¼ˆiiï¼‰è”åˆè®­ç»ƒUNetå’Œç¼–ç å™¨ï¼ŒåŒæ—¶è§£è€¦é«˜ä½åˆ†è¾¨ç‡å›¾åƒçš„ç¼–ç å¹¶ä½¿ç”¨ä¸“é—¨çš„è°ƒåº¦æ–¹æ¡ˆã€‚ï¼ˆiiiï¼‰ä½¿ç”¨UNetçš„è¾“å‡ºå¯¹è§£ç å™¨è¿›è¡Œå¾®è°ƒï¼Œä»¥ç›´æ¥é’ˆå¯¹æ¨ç†æ—¶é—´è·å¾—çš„æ½œåœ¨è¡¨ç¤ºå®šåˆ¶è§£ç å™¨ã€‚Edge-SD-SRåœ¨è®¾å¤‡ä¸Šè¿è¡Œé«˜æ•ˆï¼Œä¾‹å¦‚ï¼Œå®ƒå¯ä»¥åœ¨Samsung S24 DSPä¸Šè¿è¡Œï¼Œå°†128x128çš„è¡¥ä¸æ”¾å¤§åˆ°512x512éœ€è¦38æ¯«ç§’ï¼Œè€Œå°†512x512æ”¾å¤§åˆ°2048x2048ï¼ˆéœ€è¦25æ¬¡æ¨¡å‹è¯„ä¼°ï¼‰åªéœ€çº¦1.1ç§’ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¡¨æ˜ï¼ŒEdge-SD-SRåœ¨æœ€æˆç†Ÿçš„è¶…åˆ†è¾¨ç‡åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æˆ–è¶…è¶Šäº†æœ€å…ˆè¿›çš„è¶…åˆ†è¾¨ç‡æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06978v2">PDF</a> Accepted to CVPR 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>è¿‘æœŸStable DiffusionåŸºäºSuper Resolutionï¼ˆSD-SRï¼‰çš„è§†è§‰è´¨é‡å–å¾—äº†å·¨å¤§è¿›å±•ã€‚ç„¶è€Œï¼Œåœ¨ç§»åŠ¨è®¾å¤‡ç­‰è®¡ç®—å—é™çš„è®¾å¤‡ä¸Šéƒ¨ç½²å¤§å‹æ‰©æ•£æ¨¡å‹ä»ä¸å®é™…ã€‚æœ¬ç ”ç©¶å¼•å…¥Edge-SD-SRï¼Œè¿™æ˜¯ä¸€ä¸ªå‚æ•°é«˜æ•ˆã€ä½å»¶è¿Ÿçš„å›¾åƒè¶…åˆ†è¾¨ç‡æ‰©æ•£æ¨¡å‹ã€‚Edge-SD-SRåŒ…å«çº¦1.69äº¿ä¸ªå‚æ•°ï¼ŒåŒ…æ‹¬UNetã€ç¼–ç å™¨å’Œè§£ç å™¨ï¼Œå¤æ‚åº¦ä»…ä¸ºçº¦142 GFLOPsã€‚ä¸ºäº†åœ¨è¿™ç§ä½è®¡ç®—é¢„ç®—ä¸Šä¿æŒé«˜è§†è§‰è´¨é‡ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†å¤šç§è®­ç»ƒç­–ç•¥ã€‚Edge-SD-SRåœ¨è®¾å¤‡ä¸Šè¿è¡Œé«˜æ•ˆï¼Œä¾‹å¦‚ï¼Œåœ¨ä¸‰æ˜ŸS24 DSPä¸Šè¿è¡Œå¯å°†ä¸€ä¸ª128x128çš„è¡¥ä¸æ”¾å¤§åˆ°512x512ï¼Œè€—æ—¶ä»…38æ¯«ç§’ï¼Œå°†ä¸€ä¸ª512x512æ”¾å¤§åˆ°2048x2048ï¼ˆéœ€è¦25æ¬¡æ¨¡å‹è¯„ä¼°ï¼‰ä¹Ÿä»…éœ€çº¦1.1ç§’ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¯æ˜Edge-SD-SRåœ¨æœ€å…·ä»£è¡¨æ€§çš„è¶…åˆ†è¾¨ç‡åŸºå‡†æµ‹è¯•ä¸­ï¼ŒåŒ¹é…ç”šè‡³è¶…è¶Šäº†æœ€å…ˆè¿›çš„è¶…åˆ†è¾¨ç‡æ–¹æ³•ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>Edge-SD-SRæ˜¯é¦–ä¸ªé’ˆå¯¹å›¾åƒè¶…åˆ†è¾¨ç‡çš„å‚æ•°é«˜æ•ˆã€ä½å»¶è¿Ÿçš„æ‰©æ•£æ¨¡å‹ã€‚</li>
<li>Edge-SD-SRæ¨¡å‹åŒ…å«çº¦169Må‚æ•°ï¼Œå¤æ‚åº¦ä¸º~142 GFLOPsï¼Œå®ç°äº†åœ¨æœ‰é™è®¡ç®—èµ„æºä¸‹çš„é«˜æ•ˆè¿è¡Œã€‚</li>
<li>å¼•å…¥æ–°å‹æ¡ä»¶æœºåˆ¶â€”â€”åŒå‘æ¡ä»¶ï¼Œé’ˆå¯¹è¶…åˆ†è¾¨ç‡ä»»åŠ¡å®šåˆ¶SDæ¨¡å‹ã€‚</li>
<li>è”åˆè®­ç»ƒUNetå’Œç¼–ç å™¨ï¼ŒåŒæ—¶è§£ç é«˜åˆ†è¾¨ç‡å’Œä½åˆ†è¾¨ç‡å›¾åƒçš„ç¼–ç ï¼Œå¹¶ä½¿ç”¨ä¸“ç”¨æ—¶é—´è¡¨ã€‚</li>
<li>é€šè¿‡åˆ©ç”¨UNetçš„è¾“å‡ºå¯¹è§£ç å™¨è¿›è¡Œå¾®è°ƒï¼Œä½¿å…¶é€‚åº”æ¨ç†æ—¶é—´è·å¾—çš„æ½œåœ¨è¡¨ç¤ºã€‚</li>
<li>Edge-SD-SRåœ¨ç§»åŠ¨è®¾å¤‡ä¸Šçš„è¿è¡Œæ•ˆç‡æé«˜ï¼Œå¦‚æ”¾å¤§å›¾åƒè¡¥ä¸çš„é€Ÿåº¦éå¸¸å¿«ã€‚</li>
<li>Edge-SD-SRåœ¨è¶…åˆ†è¾¨ç‡åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç”šè‡³è¶…è¶Šç°æœ‰å…ˆè¿›æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.06978">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1c9419cf26c3748f7ebfb6f1f9a79fc1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-da430528a382a3dbb1838b08a5729ff5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0d7ac97500d86a54e2e8eeeaa90d7501.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Minority-Focused-Text-to-Image-Generation-via-Prompt-Optimization"><a href="#Minority-Focused-Text-to-Image-Generation-via-Prompt-Optimization" class="headerlink" title="Minority-Focused Text-to-Image Generation via Prompt Optimization"></a>Minority-Focused Text-to-Image Generation via Prompt Optimization</h2><p><strong>Authors:Soobin Um, Jong Chul Ye</strong></p>
<p>We investigate the generation of minority samples using pretrained text-to-image (T2I) latent diffusion models. Minority instances, in the context of T2I generation, can be defined as ones living on low-density regions of text-conditional data distributions. They are valuable for various applications of modern T2I generators, such as data augmentation and creative AI. Unfortunately, existing pretrained T2I diffusion models primarily focus on high-density regions, largely due to the influence of guided samplers (like CFG) that are essential for high-quality generation. To address this, we present a novel framework to counter the high-density-focus of T2I diffusion models. Specifically, we first develop an online prompt optimization framework that encourages emergence of desired properties during inference while preserving semantic contents of user-provided prompts. We subsequently tailor this generic prompt optimizer into a specialized solver that promotes generation of minority features by incorporating a carefully-crafted likelihood objective. Extensive experiments conducted across various types of T2I models demonstrate that our approach significantly enhances the capability to produce high-quality minority instances compared to existing samplers. Code is available at <a target="_blank" rel="noopener" href="https://github.com/soobin-um/MinorityPrompt">https://github.com/soobin-um/MinorityPrompt</a>. </p>
<blockquote>
<p>æˆ‘ä»¬ç ”ç©¶äº†ä½¿ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ½œåœ¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆå°‘æ•°æ ·æœ¬ã€‚åœ¨T2Iç”Ÿæˆçš„èƒŒæ™¯ä¸‹ï¼Œå°‘æ•°å®ä¾‹å¯ä»¥è¢«å®šä¹‰ä¸ºå­˜åœ¨äºæ–‡æœ¬æ¡ä»¶æ•°æ®åˆ†å¸ƒçš„ä½å¯†åº¦åŒºåŸŸçš„å®ä¾‹ã€‚å®ƒä»¬å¯¹äºç°ä»£T2Iç”Ÿæˆå™¨çš„å„ç§åº”ç”¨ï¼Œå¦‚æ•°æ®å¢å¼ºå’Œåˆ›æ„AIï¼Œéƒ½æ˜¯éå¸¸æœ‰ä»·å€¼çš„ã€‚ç„¶è€Œï¼Œç°æœ‰çš„é¢„è®­ç»ƒT2Iæ‰©æ•£æ¨¡å‹ä¸»è¦å…³æ³¨é«˜å¯†åº¦åŒºåŸŸï¼Œè¿™ä¸»è¦æ˜¯å› ä¸ºå¼•å¯¼é‡‡æ ·å™¨ï¼ˆå¦‚CFGï¼‰çš„å½±å“ï¼Œå¯¹äºé«˜è´¨é‡ç”Ÿæˆè‡³å…³é‡è¦ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°å‹æ¡†æ¶æ¥å¯¹æŠ—T2Iæ‰©æ•£æ¨¡å‹çš„é«˜å¯†åº¦èšç„¦ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆå¼€å‘äº†ä¸€ä¸ªåœ¨çº¿æç¤ºä¼˜åŒ–æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åœ¨æ¨ç†è¿‡ç¨‹ä¸­é¼“åŠ±æ‰€éœ€å±æ€§çš„å‡ºç°ï¼ŒåŒæ—¶ä¿ç•™ç”¨æˆ·æä¾›çš„æç¤ºçš„è¯­ä¹‰å†…å®¹ã€‚éšåï¼Œæˆ‘ä»¬å°†è¿™ä¸ªé€šç”¨æç¤ºä¼˜åŒ–å™¨å®šåˆ¶ä¸ºä¸€ä¸ªä¸“ç”¨æ±‚è§£å™¨ï¼Œé€šè¿‡å¼•å…¥ç²¾å¿ƒè®¾è®¡çš„å¯èƒ½æ€§ç›®æ ‡æ¥ä¿ƒè¿›å°‘æ•°ç‰¹å¾çš„äº§ç”Ÿã€‚å¯¹å¤šç§ç±»å‹çš„T2Iæ¨¡å‹è¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç›¸è¾ƒäºç°æœ‰é‡‡æ ·å™¨ï¼Œåœ¨ç”Ÿæˆé«˜è´¨é‡å°‘æ•°æ ·æœ¬æ–¹é¢èƒ½åŠ›æ˜¾è‘—å¢å¼ºã€‚ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/soobin-um/MinorityPrompt%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/soobin-um/MinorityPromptè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.07838v3">PDF</a> CVPR 2025 (Oral), 21 pages, 10 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†ä½¿ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ½œåœ¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆå°‘æ•°æ ·æœ¬çš„æ–¹æ³•ã€‚å°‘æ•°å®ä¾‹åœ¨T2Iç”Ÿæˆä¸­æŒ‡çš„æ˜¯åœ¨æ–‡æœ¬æ¡ä»¶æ•°æ®åˆ†å¸ƒçš„ä½å¯†åº¦åŒºåŸŸçš„æ ·æœ¬ï¼Œå¯¹äºç°ä»£T2Iç”Ÿæˆå™¨çš„å„ç§åº”ç”¨ï¼Œå¦‚æ•°æ®å¢å¼ºå’Œåˆ›æ„AIï¼Œå…·æœ‰é‡è¦çš„ä»·å€¼ã€‚ç„¶è€Œï¼Œç°æœ‰çš„é¢„è®­ç»ƒT2Iæ‰©æ•£æ¨¡å‹ä¸»è¦å…³æ³¨é«˜å¯†åº¦åŒºåŸŸï¼Œè¿™ä¸»è¦æ˜¯ç”±äºå¼•å¯¼é‡‡æ ·å™¨ï¼ˆå¦‚CFGï¼‰çš„å½±å“ï¼Œå¯¹äºé«˜è´¨é‡ç”Ÿæˆè‡³å…³é‡è¦ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ¡†æ¶æ¥å¯¹æŠ—T2Iæ‰©æ•£æ¨¡å‹çš„é«˜å¯†åº¦å…³æ³¨ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªåœ¨çº¿æç¤ºä¼˜åŒ–æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åœ¨æ¨ç†è¿‡ç¨‹ä¸­é¼“åŠ±æ‰€éœ€å±æ€§çš„å‡ºç°ï¼ŒåŒæ—¶ä¿ç•™ç”¨æˆ·æä¾›çš„æç¤ºçš„è¯­ä¹‰å†…å®¹ã€‚éšåï¼Œæˆ‘ä»¬å°†è¿™ä¸ªé€šç”¨çš„æç¤ºä¼˜åŒ–å™¨å®šåˆ¶æˆä¸€ä¸ªä¸“ç”¨æ±‚è§£å™¨ï¼Œé€šè¿‡å¼•å…¥ç²¾å¿ƒè®¾è®¡çš„ä¼¼ç„¶ç›®æ ‡æ¥ä¿ƒè¿›å°‘æ•°ç‰¹å¾çš„äº§ç”Ÿã€‚åœ¨å¤šç§ç±»å‹çš„T2Iæ¨¡å‹ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œä¸ç°æœ‰çš„é‡‡æ ·å™¨ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—æé«˜äº†ç”Ÿæˆå°‘æ•°å®ä¾‹çš„èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶äº†å¦‚ä½•åˆ©ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ½œåœ¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆå°‘æ•°æ ·æœ¬ã€‚</li>
<li>å°‘æ•°å®ä¾‹æŒ‡çš„æ˜¯åœ¨æ–‡æœ¬æ¡ä»¶æ•°æ®åˆ†å¸ƒçš„ä½å¯†åº¦åŒºåŸŸçš„æ ·æœ¬ï¼Œå¯¹ç°ä»£T2Iç”Ÿæˆå™¨çš„åº”ç”¨è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰é¢„è®­ç»ƒT2Iæ‰©æ•£æ¨¡å‹ä¸»è¦å…³æ³¨é«˜å¯†åº¦åŒºåŸŸï¼Œè¿™æ˜¯ç”±äºå¼•å¯¼é‡‡æ ·å™¨çš„å½±å“ã€‚</li>
<li>æå‡ºä¸€ä¸ªæ¡†æ¶æ¥å¯¹æŠ—T2Iæ‰©æ•£æ¨¡å‹çš„é«˜å¯†åº¦å…³æ³¨ï¼ŒåŒ…æ‹¬åœ¨çº¿æç¤ºä¼˜åŒ–å’Œä¸“ç”¨æ±‚è§£å™¨çš„å¼€å‘ã€‚</li>
<li>åœ¨çº¿æç¤ºä¼˜åŒ–æ¡†æ¶åœ¨æ¨ç†è¿‡ç¨‹ä¸­é¼“åŠ±æ‰€éœ€å±æ€§çš„å‡ºç°ï¼ŒåŒæ—¶ä¿ç•™ç”¨æˆ·æä¾›çš„æç¤ºçš„è¯­ä¹‰å†…å®¹ã€‚</li>
<li>ä¸“ç”¨æ±‚è§£å™¨é€šè¿‡å¼•å…¥ç²¾å¿ƒè®¾è®¡çš„ä¼¼ç„¶ç›®æ ‡æ¥ä¿ƒè¿›å°‘æ•°ç‰¹å¾çš„äº§ç”Ÿã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.07838">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-49994ffb664a73f55925da149f3fc38b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ed6cce36bac4fd126cab2b85ca40c82f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-af39e011915f50e881a9d3c0cf41241a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-411499d23e955764d6d3aed1a9efb9c5.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="ReviveDiff-A-Universal-Diffusion-Model-for-Restoring-Images-in-Adverse-Weather-Conditions"><a href="#ReviveDiff-A-Universal-Diffusion-Model-for-Restoring-Images-in-Adverse-Weather-Conditions" class="headerlink" title="ReviveDiff: A Universal Diffusion Model for Restoring Images in Adverse   Weather Conditions"></a>ReviveDiff: A Universal Diffusion Model for Restoring Images in Adverse   Weather Conditions</h2><p><strong>Authors:Wenfeng Huang, Guoan Xu, Wenjing Jia, Stuart Perry, Guangwei Gao</strong></p>
<p>Images captured in challenging environmentsâ€“such as nighttime, smoke, rainy weather, and underwaterâ€“often suffer from significant degradation, resulting in a substantial loss of visual quality. The effective restoration of these degraded images is critical for the subsequent vision tasks. While many existing approaches have successfully incorporated specific priors for individual tasks, these tailored solutions limit their applicability to other degradations. In this work, we propose a universal network architecture, dubbed &#96;&#96;ReviveDiffâ€™â€™, which can address various degradations and bring images back to life by enhancing and restoring their quality. Our approach is inspired by the observation that, unlike degradation caused by movement or electronic issues, quality degradation under adverse conditions primarily stems from natural media (such as fog, water, and low luminance), which generally preserves the original structures of objects. To restore the quality of such images, we leveraged the latest advancements in diffusion models and developed ReviveDiff to restore image quality from both macro and micro levels across some key factors determining image quality, such as sharpness, distortion, noise level, dynamic range, and color accuracy. We rigorously evaluated ReviveDiff on seven benchmark datasets covering five types of degrading conditions: Rainy, Underwater, Low-light, Smoke, and Nighttime Hazy. Our experimental results demonstrate that ReviveDiff outperforms the state-of-the-art methods both quantitatively and visually. </p>
<blockquote>
<p>åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„ç¯å¢ƒä¸­æ•æ‰çš„å›¾åƒï¼Œå¦‚å¤œé—´ã€çƒŸé›¾ã€é›¨å¤©å’Œæ°´ä¸‹ç¯å¢ƒï¼Œå¸¸å¸¸ä¼šå‡ºç°æ˜¾è‘—çš„é€€åŒ–ï¼Œå¯¼è‡´è§†è§‰è´¨é‡çš„å¤§é‡æŸå¤±ã€‚è¿™äº›é€€åŒ–å›¾åƒçš„æœ‰æ•ˆæ¢å¤å¯¹åç»­çš„è§†è§‰ä»»åŠ¡è‡³å…³é‡è¦ã€‚è™½ç„¶è®¸å¤šç°æœ‰æ–¹æ³•å·²ç»æˆåŠŸåœ°ç»“åˆäº†é’ˆå¯¹å„ä¸ªä»»åŠ¡çš„ç‰¹å®šå…ˆéªŒçŸ¥è¯†ï¼Œä½†è¿™äº›å®šåˆ¶è§£å†³æ–¹æ¡ˆé™åˆ¶äº†å®ƒä»¬åœ¨å¤„ç†å…¶ä»–é€€åŒ–é—®é¢˜æ—¶çš„é€‚ç”¨æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºâ€œReviveDiffâ€çš„é€šç”¨ç½‘ç»œæ¶æ„ï¼Œå®ƒå¯ä»¥è§£å†³å„ç§é€€åŒ–é—®é¢˜ï¼Œå¹¶é€šè¿‡å¢å¼ºå’Œæ¢å¤å›¾åƒè´¨é‡ä½¿å›¾åƒæ¢å¤ç”Ÿæœºã€‚æˆ‘ä»¬çš„æ–¹æ³•å—åˆ°ä»¥ä¸‹è§‚å¯Ÿç»“æœçš„å¯å‘ï¼šä¸è¿åŠ¨æˆ–ç”µå­é—®é¢˜å¼•èµ·çš„é€€åŒ–ä¸åŒï¼Œæ¶åŠ£æ¡ä»¶ä¸‹çš„è´¨é‡é€€åŒ–ä¸»è¦æºäºè‡ªç„¶åª’ä»‹ï¼ˆå¦‚é›¾ã€æ°´å’Œä½äº®åº¦ï¼‰ï¼Œè¿™äº›åª’ä»‹é€šå¸¸ä¿ç•™äº†ç‰©ä½“çš„åŸå§‹ç»“æ„ã€‚ä¸ºäº†æ¢å¤æ­¤ç±»å›¾åƒçš„è´¨é‡ï¼Œæˆ‘ä»¬åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„æœ€æ–°è¿›å±•ï¼Œå¹¶å¼€å‘äº†ReviveDiffæ¥ä»å®è§‚å’Œå¾®è§‚å±‚é¢æ¢å¤å›¾åƒè´¨é‡ï¼Œæ¶‰åŠå†³å®šå›¾åƒè´¨é‡çš„å…³é”®å› ç´ ï¼Œå¦‚æ¸…æ™°åº¦ã€å¤±çœŸã€å™ªå£°æ°´å¹³ã€åŠ¨æ€èŒƒå›´å’Œè‰²å½©å‡†ç¡®æ€§ã€‚æˆ‘ä»¬åœ¨ä¸ƒä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå¯¹ReviveDiffè¿›è¡Œäº†ä¸¥æ ¼è¯„ä¼°ï¼Œæ¶µç›–äº†äº”ç§é€€åŒ–æ¡ä»¶ï¼šé›¨å¤©ã€æ°´ä¸‹ã€ä½å…‰ã€çƒŸé›¾å’Œå¤œé—´æœ¦èƒ§ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒReviveDiffåœ¨å®šé‡å’Œè§†è§‰ä¸Šå‡ä¼˜äºæœ€æ–°æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.18932v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºâ€œReviveDiffâ€çš„é€šç”¨ç½‘ç»œæ¶æ„ï¼Œè¯¥æ¶æ„èƒ½å¤Ÿåº”å¯¹å„ç§å›¾åƒé€€åŒ–é—®é¢˜ï¼Œå¹¶åœ¨æ¶åŠ£æ¡ä»¶ä¸‹æ¢å¤å›¾åƒè´¨é‡ã€‚é€šè¿‡å¯¹æ‰©æ•£æ¨¡å‹çš„æœ€æ–°è¿›å±•åŠ ä»¥åˆ©ç”¨ï¼ŒReviveDiffèƒ½å¤Ÿä»å®è§‚å’Œå¾®è§‚å±‚é¢æ¢å¤å›¾åƒè´¨é‡ï¼Œæ¶µç›–å†³å®šå›¾åƒè´¨é‡çš„å…³é”®å› ç´ ï¼Œå¦‚æ¸…æ™°åº¦ã€å¤±çœŸã€å™ªå£°æ°´å¹³ã€åŠ¨æ€èŒƒå›´å’Œè‰²å½©å‡†ç¡®æ€§ã€‚åœ¨ä¸ƒä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒReviveDiffåœ¨äº”ç§é€€åŒ–æ¡ä»¶ä¸‹å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å›¾åƒåœ¨æ¶åŠ£ç¯å¢ƒï¼ˆå¦‚å¤œé—´ã€çƒŸé›¾ã€é›¨å¤©ã€æ°´ä¸‹ç­‰ï¼‰ä¸­æ•æ‰æ—¶ï¼Œä¼šé­å—æ˜¾è‘—é€€åŒ–ï¼Œå¯¼è‡´è§†è§‰è´¨é‡æŸå¤±ã€‚</li>
<li>æœ‰æ•ˆæ¢å¤è¿™äº›é€€åŒ–å›¾åƒå¯¹äºåç»­è§†è§‰ä»»åŠ¡è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰æ–¹æ³•é€šå¸¸é’ˆå¯¹ç‰¹å®šä»»åŠ¡èå…¥ç‰¹å®šå…ˆéªŒçŸ¥è¯†ï¼Œä½†è¿™ç§æ–¹æ³•é™åˆ¶äº†å®ƒä»¬åœ¨å¤„ç†å…¶ä»–é€€åŒ–é—®é¢˜æ—¶çš„é€‚ç”¨æ€§ã€‚</li>
<li>æå‡ºçš„â€œReviveDiffâ€ç½‘ç»œæ¶æ„æ—¨åœ¨è§£å†³å„ç§é€€åŒ–é—®é¢˜ï¼Œä½¿å›¾åƒæ¢å¤ç”Ÿæœºã€‚</li>
<li>ReviveDiffåˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„æœ€æ–°è¿›å±•ï¼Œèƒ½å¤Ÿä»å®è§‚å’Œå¾®è§‚å±‚é¢æ¢å¤å›¾åƒè´¨é‡ã€‚</li>
<li>ReviveDiffåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œäº†ä¸¥æ ¼è¯„ä¼°ï¼Œè¦†ç›–å¤šç§é€€åŒ–æ¡ä»¶ï¼ŒåŒ…æ‹¬é›¨å¤©ã€æ°´ä¸‹ã€ä½å…‰ã€çƒŸé›¾å’Œå¤œé—´é›¾éœ¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.18932">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6c8347e063606fc7dd61060aebcd6695.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9f89dba463ad367cd4791f3c8b632a2f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-29d7bbddbd1f3e7b6f245ffee2233c3b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-281b1df8a70d2ce169c829514579183b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-113a9ef2bbe4b073e7a6e700a8c06b9a.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-08/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-08/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-08/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-6f8c688d7ce9e5478adb5c7dd2a88011.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-08  MedSAM2 Segment Anything in 3D Medical Images and Videos
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-08
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-08/3DGS/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-279580ae30867b5b04353b11a406ca27.jpg" class="responsive-img" alt="3DGS">
                        
                        <span class="card-title">3DGS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            3DGS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-08  Self-Calibrating Gaussian Splatting for Large Field of View   Reconstruction
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-08
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/3DGS/" class="post-category">
                                    3DGS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/3DGS/">
                        <span class="chip bg-color">3DGS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">26254.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
