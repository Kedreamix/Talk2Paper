<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-08  MedSAM2 Segment Anything in 3D Medical Images and Videos">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-6f8c688d7ce9e5478adb5c7dd2a88011.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-08
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-21
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    11.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    47 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-04-08-æ›´æ–°"><a href="#2025-04-08-æ›´æ–°" class="headerlink" title="2025-04-08 æ›´æ–°"></a>2025-04-08 æ›´æ–°</h1><h2 id="MedSAM2-Segment-Anything-in-3D-Medical-Images-and-Videos"><a href="#MedSAM2-Segment-Anything-in-3D-Medical-Images-and-Videos" class="headerlink" title="MedSAM2: Segment Anything in 3D Medical Images and Videos"></a>MedSAM2: Segment Anything in 3D Medical Images and Videos</h2><p><strong>Authors:Jun Ma, Zongxin Yang, Sumin Kim, Bihui Chen, Mohammed Baharoon, Adibvafa Fallahpour, Reza Asakereh, Hongwei Lyu, Bo Wang</strong></p>
<p>Medical image and video segmentation is a critical task for precision medicine, which has witnessed considerable progress in developing task or modality-specific and generalist models for 2D images. However, there have been limited studies on building general-purpose models for 3D images and videos with comprehensive user studies. Here, we present MedSAM2, a promptable segmentation foundation model for 3D image and video segmentation. The model is developed by fine-tuning the Segment Anything Model 2 on a large medical dataset with over 455,000 3D image-mask pairs and 76,000 frames, outperforming previous models across a wide range of organs, lesions, and imaging modalities. Furthermore, we implement a human-in-the-loop pipeline to facilitate the creation of large-scale datasets resulting in, to the best of our knowledge, the most extensive user study to date, involving the annotation of 5,000 CT lesions, 3,984 liver MRI lesions, and 251,550 echocardiogram video frames, demonstrating that MedSAM2 can reduce manual costs by more than 85%. MedSAM2 is also integrated into widely used platforms with user-friendly interfaces for local and cloud deployment, making it a practical tool for supporting efficient, scalable, and high-quality segmentation in both research and healthcare environments. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒå’Œè§†é¢‘åˆ†å‰²æ˜¯ç²¾å‡†åŒ»å­¦ä¸­çš„å…³é”®ä»»åŠ¡ã€‚é’ˆå¯¹äºŒç»´å›¾åƒçš„ç‰¹å®šä»»åŠ¡æˆ–é€šç”¨æ¨¡å‹å·²ç»å–å¾—äº†ç›¸å½“çš„è¿›å±•ã€‚ç„¶è€Œï¼Œå…³äºæ„å»ºç”¨äºä¸‰ç»´å›¾åƒå’Œè§†é¢‘çš„é€šç”¨æ¨¡å‹çš„ç»¼åˆç”¨æˆ·ç ”ç©¶ä»ç„¶æœ‰é™ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æ¨å‡ºäº†MedSAM2ï¼Œè¿™æ˜¯ä¸€ä¸ªå¯ç”¨äºä¸‰ç»´å›¾åƒå’Œè§†é¢‘åˆ†å‰²çš„æç¤ºåˆ†å‰²åŸºç¡€æ¨¡å‹ã€‚è¯¥æ¨¡å‹æ˜¯é€šè¿‡åœ¨åŒ…å«è¶…è¿‡45.5ä¸‡å¼ ä¸‰ç»´å›¾åƒ-æ©è†œå¯¹å’Œ7.6ä¸‡å¸§çš„å¤§å‹åŒ»ç–—æ•°æ®é›†ä¸Šå¯¹Segment Anything Model 2è¿›è¡Œå¾®è°ƒè€Œå¼€å‘å‡ºæ¥çš„ï¼Œåœ¨å¤šç§å™¨å®˜ã€ç—…å˜å’Œæˆåƒæ–¹å¼æ–¹é¢è¶…è¶Šäº†ä»¥å‰çš„æ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å®ç°äº†ä¸€ä¸ªäººæœºå¾ªç¯ç®¡é“ï¼Œä¿ƒè¿›äº†å¤§è§„æ¨¡æ•°æ®é›†çš„åˆ›å»ºï¼Œæ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯è¿„ä»Šä¸ºæ­¢æœ€å¹¿æ³›çš„ç”¨æˆ·ç ”ç©¶ï¼Œæ¶‰åŠå¯¹5000ä¸ªCTç—…å˜ã€3984ä¸ªè‚è„MRIç—…å˜å’Œ251550ä¸ªå¿ƒç”µå›¾è§†é¢‘å¸§çš„æ ‡æ³¨ï¼Œè¯æ˜äº†MedSAM2å¯ä»¥èŠ‚çœè¶…è¿‡85%çš„äººå·¥æˆæœ¬ã€‚MedSAM2è¿˜é›†æˆåˆ°äº†å¹¿æ³›ä½¿ç”¨çš„æœ¬åœ°å’Œäº‘å¹³å°ç”¨æˆ·å‹å¥½å‹ç•Œé¢ä¸­ï¼Œæˆä¸ºæ”¯æŒç ”ç©¶å’ŒåŒ»ç–—ç¯å¢ƒä¸­é«˜æ•ˆã€å¯æ‰©å±•å’Œé«˜è´¨é‡åˆ†å‰²çš„å®é™…å·¥å…·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.03600v1">PDF</a> <a target="_blank" rel="noopener" href="https://medsam2.github.io/">https://medsam2.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§è§„æ¨¡åŒ»ç–—æ•°æ®é›†è®­ç»ƒçš„MedSAM2æ¨¡å‹ï¼Œèƒ½å¤Ÿå¯¹ä¸‰ç»´å›¾åƒå’Œè§†é¢‘è¿›è¡Œç²¾ç»†åˆ†å‰²ï¼Œå¹¶åœ¨å¤šç§å™¨å®˜ã€ç—…å˜å’Œæˆåƒæ¨¡å¼ä¸‹è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚é€šè¿‡äººç±»å‚ä¸çš„å¾ªç¯ç®¡é“ï¼Œå®ç°å¤§è§„æ¨¡æ•°æ®é›†çš„åˆ›å»ºï¼Œå¹¶æ˜¾è‘—é™ä½æ‰‹åŠ¨æˆæœ¬ã€‚åŒæ—¶ï¼Œè¯¥æ¨¡å‹æ˜“äºé›†æˆåˆ°æœ¬åœ°å’Œäº‘ç«¯éƒ¨ç½²çš„å¹³å°ä¸Šï¼Œä¸ºç ”ç©¶å’ŒåŒ»ç–—ç¯å¢ƒæä¾›é«˜æ•ˆã€å¯æ‰©å±•å’Œé«˜è´¨é‡çš„åˆ†å‰²å·¥å…·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MedSAM2æ¨¡å‹å¯ä»¥å¯¹ä¸‰ç»´å›¾åƒå’Œè§†é¢‘è¿›è¡Œåˆ†å‰²ï¼Œä¸”æ€§èƒ½å“è¶Šã€‚</li>
<li>è¯¥æ¨¡å‹åœ¨å¤šç§å™¨å®˜ã€ç—…å˜å’Œæˆåƒæ¨¡å¼ä¸‹éƒ½æœ‰å‡ºè‰²è¡¨ç°ã€‚</li>
<li>MedSAM2é€šè¿‡äººç±»å‚ä¸çš„å¾ªç¯ç®¡é“å®ç°å¤§è§„æ¨¡æ•°æ®é›†çš„åˆ›å»ºã€‚</li>
<li>MedSAM2èƒ½æ˜¾è‘—é™ä½æ‰‹åŠ¨æˆæœ¬ï¼Œå‡å°‘è¶…è¿‡85%ã€‚</li>
<li>MedSAM2æ¨¡å‹æ˜“äºé›†æˆåˆ°å¹¿æ³›ä½¿ç”¨çš„å¹³å°ä¸Šï¼Œæ”¯æŒæœ¬åœ°å’Œäº‘ç«¯éƒ¨ç½²ã€‚</li>
<li>è¯¥æ¨¡å‹ä¸ºç ”ç©¶å’ŒåŒ»ç–—ç¯å¢ƒæä¾›é«˜æ•ˆã€å¯æ‰©å±•å’Œé«˜è´¨é‡çš„åˆ†å‰²å·¥å…·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.03600">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-019b971716d2624276ab165c2e356414.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3acd0bfb127338ff68e2cad92d37c9c4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9d70e40a0cf51ba5a80b021c7b07df85.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f72b0c92682339686ae005395ff214d1.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Physics-informed-4D-X-ray-image-reconstruction-from-ultra-sparse-spatiotemporal-data"><a href="#Physics-informed-4D-X-ray-image-reconstruction-from-ultra-sparse-spatiotemporal-data" class="headerlink" title="Physics-informed 4D X-ray image reconstruction from ultra-sparse   spatiotemporal data"></a>Physics-informed 4D X-ray image reconstruction from ultra-sparse   spatiotemporal data</h2><p><strong>Authors:Zisheng Yao, Yuhe Zhang, Zhe Hu, Robert KlÃ¶fkorn, Tobias Ritschel, Pablo Villanueva-Perez</strong></p>
<p>The unprecedented X-ray flux density provided by modern X-ray sources offers new spatiotemporal possibilities for X-ray imaging of fast dynamic processes. Approaches to exploit such possibilities often result in either i) a limited number of projections or spatial information due to limited scanning speed, as in time-resolved tomography, or ii) a limited number of time points, as in stroboscopic imaging, making the reconstruction problem ill-posed and unlikely to be solved by classical reconstruction approaches. 4D reconstruction from such data requires sample priors, which can be included via deep learning (DL). State-of-the-art 4D reconstruction methods for X-ray imaging combine the power of AI and the physics of X-ray propagation to tackle the challenge of sparse views. However, most approaches do not constrain the physics of the studied process, i.e., a full physical model. Here we present 4D physics-informed optimized neural implicit X-ray imaging (4D-PIONIX), a novel physics-informed 4D X-ray image reconstruction method combining the full physical model and a state-of-the-art DL-based reconstruction method for 4D X-ray imaging from sparse views. We demonstrate and evaluate the potential of our approach by retrieving 4D information from ultra-sparse spatiotemporal acquisitions of simulated binary droplet collisions, a relevant fluid dynamic process. We envision that this work will open new spatiotemporal possibilities for various 4D X-ray imaging modalities, such as time-resolved X-ray tomography and more novel sparse acquisition approaches like X-ray multi-projection imaging, which will pave the way for investigations of various rapid 4D dynamics, such as fluid dynamics and composite testing. </p>
<blockquote>
<p>ç°ä»£Xå°„çº¿æºæä¾›çš„å‰æ‰€æœªæœ‰çš„Xå°„çº¿æµé‡å¯†åº¦ï¼Œä¸ºå¿«é€ŸåŠ¨æ€è¿‡ç¨‹çš„Xå°„çº¿æˆåƒæä¾›äº†æ–°çš„æ—¶ç©ºå¯èƒ½æ€§ã€‚åˆ©ç”¨è¿™äº›å¯èƒ½æ€§çš„æ–¹æ³•é€šå¸¸ä¼šå¯¼è‡´ï¼ˆiï¼‰ç”±äºæ‰«æé€Ÿåº¦æœ‰é™ï¼Œæ—¶é—´è§£æå±‚ææˆåƒä¸­çš„æŠ•å½±æˆ–ç©ºé—´ä¿¡æ¯æ•°é‡æœ‰é™ï¼Œæˆ–è€…ï¼ˆiiï¼‰å¦‚é—ªå…‰æˆåƒä¸­çš„æ—¶é—´ç‚¹æ•°æœ‰é™ï¼Œè¿™ä½¿å¾—é‡å»ºé—®é¢˜ä¸é€‚å®šï¼Œä¸å¤ªå¯èƒ½é€šè¿‡ç»å…¸é‡å»ºæ–¹æ³•è§£å†³ã€‚ä»è¿™äº›æ•°æ®ä¸­è¿›è¡Œ4Dé‡å»ºéœ€è¦æ ·æœ¬å…ˆéªŒï¼Œå¯ä»¥é€šè¿‡æ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰åŒ…å«è¿™äº›å…ˆéªŒã€‚æœ€å…ˆè¿›çš„Xå°„çº¿æˆåƒ4Dé‡å»ºæ–¹æ³•ç»“åˆäº†äººå·¥æ™ºèƒ½å’ŒXå°„çº¿ä¼ æ’­ç‰©ç†å­¦ï¼Œä»¥åº”å¯¹ç¨€ç–è§†è§’çš„æŒ‘æˆ˜ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°æ–¹æ³•å¹¶æ²¡æœ‰çº¦æŸæ‰€ç ”ç©¶è¿‡ç¨‹çš„ç‰©ç†å­¦ï¼Œå³ä¸€ä¸ªå®Œæ•´çš„ç‰©ç†æ¨¡å‹ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æå‡ºäº†ç»“åˆå®Œæ•´ç‰©ç†æ¨¡å‹å’ŒåŸºäºæœ€æ–°æŠ€æœ¯çš„æ·±åº¦å­¦ä¹ é‡å»ºæ–¹æ³•çš„4Dç‰©ç†ä¿¡æ¯ä¼˜åŒ–ç¥ç»éšå¼Xå°„çº¿æˆåƒï¼ˆ4D-PIONIXï¼‰ã€‚æˆ‘ä»¬é€šè¿‡ä»æ¨¡æ‹ŸäºŒè¿›åˆ¶æ¶²æ»´ç¢°æ’çš„è¶…ç¨€ç–æ—¶ç©ºé‡‡é›†æ•°æ®ä¸­æ¢å¤4Dä¿¡æ¯æ¥å±•ç¤ºå’Œè¯„ä¼°æˆ‘ä»¬æ–¹æ³•çš„æ½œåŠ›ã€‚æˆ‘ä»¬é¢„è§ï¼Œè¿™é¡¹å·¥ä½œå°†ä¸ºå„ç§4D Xå°„çº¿æˆåƒæ¨¡å¼å¼€å¯æ–°çš„æ—¶ç©ºå¯èƒ½æ€§ï¼Œå¦‚æ—¶é—´è§£æXå°„çº¿å±‚ææˆåƒå’Œæ›´æ–°é¢–çš„ç¨€ç–é‡‡é›†æ–¹æ³•ï¼Œå¦‚Xå°„çº¿å¤šæŠ•å½±æˆåƒï¼Œè¿™å°†ä¸ºå„ç§å¿«é€Ÿ4DåŠ¨åŠ›å­¦çš„ç ”ç©¶ï¼Œå¦‚æµä½“åŠ›å­¦å’Œå¤åˆæµ‹è¯•é“ºå¹³é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.03469v1">PDF</a> </p>
<p><strong>Summary</strong><br>     ç°ä»£Xå°„çº¿æºæä¾›çš„å‰æ‰€æœªæœ‰çš„Xå°„çº¿æµé‡å¯†åº¦ï¼Œä¸ºå¿«é€ŸåŠ¨æ€è¿‡ç¨‹çš„Xå°„çº¿æˆåƒæä¾›äº†æ–°çš„æ—¶ç©ºå¯èƒ½æ€§ã€‚æ–¹æ³•å¸¸å¯¼è‡´æŠ•å½±æ•°é‡æˆ–ç©ºé—´ä¿¡æ¯æœ‰é™ï¼Œé‡å»ºé—®é¢˜æˆä¸ºç—…æ€çš„ï¼Œæ— æ³•ç”¨ä¼ ç»Ÿé‡å»ºæ–¹æ³•è§£å†³ã€‚4Dé‡å»ºéœ€è¦æ ·æœ¬å…ˆéªŒï¼Œå¯é€šè¿‡æ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰å®ç°ã€‚æœ€å…ˆè¿›çš„4Dé‡å»ºæ–¹æ³•ç»“åˆäººå·¥æ™ºèƒ½å’ŒXå°„çº¿ä¼ æ’­ç‰©ç†å­¦æ¥è§£å†³ç¨€ç–è§†å›¾æŒ‘æˆ˜ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°æ–¹æ³•å¹¶ä¸çº¦æŸæ‰€ç ”ç©¶è¿‡ç¨‹çš„ç‰©ç†å­¦æ¨¡å‹ã€‚æ­¤å¤„æˆ‘ä»¬æå‡ºä¸€ç§ç»“åˆå…¨ç‰©ç†æ¨¡å‹å’ŒåŸºäºæ·±åº¦å­¦ä¹ çš„é‡å»ºæ–¹æ³•çš„å…¨æ–°ç‰©ç†ä¿¡æ¯4D Xå°„çº¿æˆåƒæŠ€æœ¯â€”â€”4Dç‰©ç†ä¿¡æ¯ä¼˜åŒ–ç¥ç»ç½‘ç»œéšå¼å°„çº¿æˆåƒï¼ˆ4D-PIONIXï¼‰ã€‚é€šè¿‡æ¨¡æ‹ŸäºŒè¿›åˆ¶æ¶²æ»´ç¢°æ’ç­‰å®éªŒï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•ä»è¶…ç¨€ç–æ—¶ç©ºé‡‡é›†æ•°æ®ä¸­æ¢å¤4Dä¿¡æ¯çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°ä»£Xå°„çº¿æºæä¾›äº†æ›´é«˜çš„Xå°„çº¿æµé‡å¯†åº¦ï¼Œä¸ºå¿«é€ŸåŠ¨æ€è¿‡ç¨‹çš„Xå°„çº¿æˆåƒæä¾›äº†æ–°æ—¶ç©ºå¯èƒ½æ€§ã€‚</li>
<li>å½“å‰4Dé‡å»ºæ–¹æ³•é¢ä¸´æŠ•å½±æˆ–æ—¶é—´ç‚¹çš„é™åˆ¶ï¼Œå¯¼è‡´é‡å»ºé—®é¢˜æˆä¸ºç—…æ€çš„ã€‚</li>
<li>æ·±åº¦å­¦ä¹ è¢«ç”¨äºè§£å†³4Dé‡å»ºä¸­çš„æ ·æœ¬å…ˆéªŒé—®é¢˜ã€‚</li>
<li>æœ€å…ˆè¿›çš„4Dé‡å»ºæ–¹æ³•ç»“åˆAIå’ŒXå°„çº¿ä¼ æ’­ç‰©ç†å­¦å¤„ç†ç¨€ç–è§†å›¾æŒ‘æˆ˜ã€‚</li>
<li>å¤§å¤šæ•°ç°æœ‰æ–¹æ³•ç¼ºä¹ç‰©ç†è¿‡ç¨‹çš„çº¦æŸæ¨¡å‹ã€‚</li>
<li>å¼•å…¥å…¨æ–°ç‰©ç†ä¿¡æ¯4D Xå°„çº¿æˆåƒæŠ€æœ¯â€”â€”4D-PIONIXï¼Œç»“åˆäº†å…¨ç‰©ç†æ¨¡å‹å’Œæ·±åº¦å­¦ä¹ æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.03469">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-cb3a39edbcf0497a94a1f55c29671488.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e39ac22ef5bfe5e5ee61d58544d55337.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2e44da660cd79be0a4c1361cfdf82adb.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="FLAIRBrainSeg-Fine-grained-brain-segmentation-using-FLAIR-MRI-only"><a href="#FLAIRBrainSeg-Fine-grained-brain-segmentation-using-FLAIR-MRI-only" class="headerlink" title="FLAIRBrainSeg: Fine-grained brain segmentation using FLAIR MRI only"></a>FLAIRBrainSeg: Fine-grained brain segmentation using FLAIR MRI only</h2><p><strong>Authors:Edern Le Bot, RÃ©mi Giraud, Boris Mansencal, Thomas Tourdias, JosÃ¨ V. Manjon, Pierrick CoupÃ©</strong></p>
<p>This paper introduces a novel method for brain segmentation using only FLAIR MRIs, specifically targeting cases where access to other imaging modalities is limited. By leveraging existing automatic segmentation methods, we train a network to approximate segmentations, typically obtained from T1-weighted MRIs. Our method, called FLAIRBrainSeg, produces segmentations of 132 structures and is robust to multiple sclerosis lesions. Experiments on both in-domain and out-of-domain datasets demonstrate that our method outperforms modality-agnostic approaches based on image synthesis, the only currently available alternative for performing brain parcellation using FLAIR MRI alone. This technique holds promise for scenarios where T1-weighted MRIs are unavailable and offers a valuable alternative for clinicians and researchers in need of reliable anatomical segmentation. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ä»…ä½¿ç”¨FLAIR MRIè¿›è¡Œå¤§è„‘åˆ†å‰²çš„æ–°æ–¹æ³•ï¼Œç‰¹åˆ«é€‚ç”¨äºæ— æ³•è·å¾—å…¶ä»–æˆåƒæ¨¡å¼çš„æƒ…å†µã€‚æˆ‘ä»¬å€ŸåŠ©ç°æœ‰çš„è‡ªåŠ¨åˆ†å‰²æ–¹æ³•ï¼Œè®­ç»ƒäº†ä¸€ä¸ªç½‘ç»œæ¥è¿‘ä¼¼ä»T1åŠ æƒMRIä¸­è·å¾—çš„ç»“æœã€‚æˆ‘ä»¬çš„æ–¹æ³•ç§°ä¸ºFLAIRBrainSegï¼Œèƒ½å¤Ÿå¯¹132ä¸ªç»“æ„è¿›è¡Œåˆ†å‰²ï¼Œå¹¶ä¸”å¯¹äºå¤šå‘æ€§ç¡¬åŒ–ç—‡ç—…å˜å…·æœ‰å¾ˆå¼ºçš„é²æ£’æ€§ã€‚åœ¨é¢†åŸŸå†…çš„æ•°æ®é›†å’Œé¢†åŸŸå¤–çš„æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä»…ä½¿ç”¨FLAIR MRIè¿›è¡Œå¤§è„‘åˆ†åŒºçš„æƒ…å†µä¸‹ï¼Œä¼˜äºåŸºäºå›¾åƒåˆæˆçš„æ¨¡æ€æ— å…³æ–¹æ³•ï¼Œè¿™æ˜¯ç›®å‰å”¯ä¸€å¯ç”¨çš„æ›¿ä»£æ–¹æ³•ã€‚å¯¹äºæ— æ³•ä½¿ç”¨T1åŠ æƒMRIçš„åœºæ™¯ï¼Œè¿™é¡¹æŠ€æœ¯å…·æœ‰å·¨å¤§çš„æ½œåŠ›ï¼Œå¹¶ä¸ºä¸´åºŠåŒ»ç”Ÿå’Œç ”ç©¶äººå‘˜æä¾›äº†ä¸€ä¸ªå¯é çš„è§£å‰–åˆ†å‰²çš„æ›¿ä»£æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.03376v1">PDF</a> 9 pages, 6 figures</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ä»…ä½¿ç”¨FLAIR MRIè¿›è¡Œè„‘åˆ†å‰²çš„æ–°æ–¹æ³•ï¼Œç‰¹åˆ«é€‚ç”¨äºæ— æ³•è·å–å…¶ä»–æˆåƒæ¨¡å¼çš„æƒ…å†µã€‚è¯¥æ–¹æ³•åˆ©ç”¨ç°æœ‰çš„è‡ªåŠ¨åˆ†å‰²æ–¹æ³•ï¼Œè®­ç»ƒç½‘ç»œæ¨¡æ‹Ÿé€šå¸¸ä»T1åŠ æƒMRIè·å¾—çš„ç»“æœã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åŸŸå†…å’ŒåŸŸå¤–æ•°æ®é›†ä¸Šçš„è¡¨ç°å‡ä¼˜äºåŸºäºå›¾åƒåˆæˆçš„æ¨¡æ€æ— å…³æ–¹æ³•ï¼Œä¸ºæ— æ³•ä½¿ç”¨T1åŠ æƒMRIçš„åœºæ™¯æä¾›äº†å¯é è§£å‰–åˆ†å‰²çš„å®è´µæ›¿ä»£æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®ºæ–‡æå‡ºäº†ä¸€ç§ä»…ä½¿ç”¨FLAIR MRIsè¿›è¡Œè„‘åˆ†å‰²çš„æ–°æ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•ä¸»è¦é’ˆå¯¹æ— æ³•è·å–å…¶ä»–æˆåƒæ¨¡å¼çš„æƒ…å†µã€‚</li>
<li>è¯¥æ–¹æ³•åˆ©ç”¨ç°æœ‰è‡ªåŠ¨åˆ†å‰²æ–¹æ³•è®­ç»ƒç½‘ç»œæ¥æ¨¡æ‹Ÿä»T1åŠ æƒMRIå¾—åˆ°çš„ç»“æœã€‚</li>
<li>è®ºæ–‡ä»‹ç»çš„æ–¹æ³•è¢«å‘½åä¸ºFLAIRBrainSegï¼Œèƒ½å¤Ÿåˆ†å‰²132ä¸ªç»“æ„ã€‚</li>
<li>è¯¥æ–¹æ³•å¯¹å¤šå‘æ€§ç¡¬åŒ–ç—…ç¶å…·æœ‰é²æ£’æ€§ã€‚</li>
<li>å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åŸŸå†…å’ŒåŸŸå¤–æ•°æ®é›†ä¸Šçš„è¡¨ç°å‡ä¼˜äºåŸºäºå›¾åƒåˆæˆçš„æ¨¡æ€æ— å…³æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.03376">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-05f71e693e6cf0b4c6b76fb172dae2ae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-90a6e9bed882a5b37daf4ad6ae184164.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a406e170c2b61a0125958779f3476da2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a92d76f5c457a0515ccde3cb7dfb59cd.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="A-Novel-Optical-Design-for-Wide-Field-Imaging-in-X-ray-Astronomy"><a href="#A-Novel-Optical-Design-for-Wide-Field-Imaging-in-X-ray-Astronomy" class="headerlink" title="A Novel Optical Design for Wide-Field Imaging in X-ray Astronomy"></a>A Novel Optical Design for Wide-Field Imaging in X-ray Astronomy</h2><p><strong>Authors:Neeraj K. Tiwari, Santosh V. Vadawale, N. P. S. Mithun</strong></p>
<p>Over the decades, astronomical X-ray telescopes have utilized the Wolter type-1 optical design, which provides stigmatic imaging in axial direction but suffers from coma and higher-order aberrations for off-axis sources. The Wolter-Schwarzschild design, with stigmatic imaging in the axial direction, while suffering from higher-order aberrations, is corrected for coma, thus performing better than the Wolter type-1. The Wolter type-1 and Wolter-Schwarzschild designs are optimized for on-axis but have reduced angular resolution when averaged over a wide field of view, with the averaging weighted by the area covered in the field of view. An optical design that maximizes angular resolution at the edge of the field of view rather than at the center is more suitable for wide-field X-ray telescopes required for deep-sky astronomical surveys or solar observations. A Hyperboloid-Hyperboloid optical design can compromise axial resolution to enhance field angle resolution, hence providing improved area-weighted average angular resolution over the Wolter-Schwarzschild design, but only for fields of view exceeding a specific size. Here, we introduce a new optical design that is free from coma aberration and capable of maximizing angular resolution at any desired field angle. This design consistently outperforms Wolter-1, Wolter-Schwarzschild, and Hyperboloid-Hyperboloid designs when averaged over any field of view size. The improvement in performance remains consistent across variations in other telescope parameters such as diameter, focal length, and mirror lengths. By utilizing this new optical design, we also present a design for a full-disk imaging solar X-ray telescope. </p>
<blockquote>
<p>å‡ åå¹´æ¥ï¼Œå¤©æ–‡Xå°„çº¿æœ›è¿œé•œé‡‡ç”¨äº†Wolter 1å‹å…‰å­¦è®¾è®¡ï¼Œè¯¥è®¾è®¡åœ¨è½´å‘æä¾›æ•£å…‰æˆåƒï¼Œä½†å¯¹äºç¦»è½´æºåˆ™å­˜åœ¨å½—æ˜Ÿå½¢å’Œæ›´é«˜é˜¶çš„åƒå·®é—®é¢˜ã€‚Wolter-Schwarzchildè®¾è®¡åœ¨è½´å‘å…·æœ‰æ•£å…‰æˆåƒçš„ä¼˜ç‚¹ï¼Œå°½ç®¡å­˜åœ¨é«˜é˜¶åƒå·®ï¼Œä½†å®ƒç»è¿‡ä¿®æ­£ä»¥æ¶ˆé™¤å½—æ˜Ÿå½¢åƒå·®ï¼Œå› æ­¤æ€§èƒ½ä¼˜äºWolter 1å‹è®¾è®¡ã€‚Wolter 1å‹å’ŒWolter-Schwarzchildè®¾è®¡ä»¥è½´å‘ä¼˜åŒ–ä¸ºä¸»ï¼Œä½†åœ¨å®½è§†åœºå†…çš„å¹³å‡è§’åˆ†è¾¨ç‡ä¼šé™ä½ï¼Œå¹³å‡ç»“æœç”±è§†åœºæ‰€è¦†ç›–çš„åŒºåŸŸåŠ æƒå¾—å‡ºã€‚å¯¹äºæ·±ç©ºå¤©æ–‡æ™®æŸ¥æˆ–å¤ªé˜³è§‚æµ‹æ‰€éœ€çš„å®½è§†åœºXå°„çº¿æœ›è¿œé•œè€Œè¨€ï¼Œä¸€ç§èƒ½åœ¨è§†åœºè¾¹ç¼˜è€Œä¸æ˜¯ä¸­å¿ƒæœ€å¤§åŒ–è§’åˆ†è¾¨ç‡çš„å…‰å­¦è®¾è®¡æ›´ä¸ºåˆé€‚ã€‚è¶…æ¤­åœ†ä½“è¶…æ¤­åœ†å…‰å­¦è®¾è®¡å¯ä»¥å¦¥åè½´å‘åˆ†è¾¨ç‡ä»¥å¢å¼ºåœºè§’åˆ†è¾¨ç‡ï¼Œä»è€Œåœ¨å¯¹Wolter-Schwarzchildè®¾è®¡è¿›è¡Œæ”¹è¿›æ—¶æé«˜é¢ç§¯åŠ æƒå¹³å‡è§’åˆ†è¾¨ç‡ï¼Œä½†è¿™ä»…é€‚ç”¨äºè¶…è¿‡ç‰¹å®šå¤§å°çš„è§†åœºã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§æ–°å‹å…‰å­¦è®¾è®¡ï¼Œå®ƒæ— å½—æ˜Ÿå½¢åƒå·®é—®é¢˜ï¼Œèƒ½å¤Ÿåœ¨ä»»ä½•æƒ³è¦çš„è§†åœºè§’æœ€å¤§åŒ–è§’åˆ†è¾¨ç‡ã€‚åœ¨ä»»ä½•è§†åœºå¤§å°è¿›è¡Œå¹³å‡æ—¶ï¼Œè¯¥è®¾è®¡å§‹ç»ˆä¼˜äºWolter 1å‹ã€Wolter-Schwarzchildå’Œè¶…æ¤­åœ†ä½“è¶…æ¤­åœ†å…‰å­¦è®¾è®¡ã€‚å³ä½¿åœ¨å…¶ä»–æœ›è¿œé•œå‚æ•°ï¼ˆå¦‚ç›´å¾„ã€ç„¦è·å’Œé•œé¢é•¿åº¦ï¼‰å‘ç”Ÿå˜åŒ–æ—¶ï¼Œæ€§èƒ½çš„æå‡ä¹Ÿèƒ½ä¿æŒä¸€è‡´ã€‚åˆ©ç”¨è¿™ç§æ–°å‹å…‰å­¦è®¾è®¡ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§å…¨ç›˜æˆåƒå¤ªé˜³Xå°„çº¿æœ›è¿œé•œçš„è®¾è®¡æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.03265v1">PDF</a> This paper has been published in Experimental Astronomy</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹å…‰å­¦è®¾è®¡ï¼Œè¯¥è®¾è®¡æ¶ˆé™¤äº†å½—æ˜Ÿåƒå·®ï¼Œå¯åœ¨ä»»ä½•æœŸæœ›çš„è§†åœºè§’ä¸Šæœ€å¤§åŒ–è§’åˆ†è¾¨ç‡ã€‚è¿™ç§è®¾è®¡åœ¨å„ç§è§†åœºå¤§å°ä¸Šå‡ä¼˜äºWolter-1ã€Wolter-Schwarzchildå’ŒHyperboloid-Hyperboloidè®¾è®¡ï¼Œå¹¶ä¸”åœ¨æœ›è¿œé•œå‚æ•°å¦‚ç›´å¾„ã€ç„¦è·å’Œé•œé•¿å˜åŒ–çš„æƒ…å†µä¸‹ï¼Œæ€§èƒ½æ”¹è¿›ä¿æŒä¸€è‡´ã€‚åº”ç”¨è¿™ç§æ–°å…‰å­¦è®¾è®¡ï¼Œè¿˜æå‡ºäº†ä¸€ç§å…¨ç›˜æˆåƒå¤ªé˜³Xå°„çº¿æœ›è¿œé•œçš„è®¾è®¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤©æ–‡Xå°„çº¿æœ›è¿œé•œå‡ åå¹´æ¥ä¸€ç›´ä½¿ç”¨Wolter type-1å…‰å­¦è®¾è®¡ï¼Œå®ƒåœ¨è½´å‘æä¾›stigmaticæˆåƒï¼Œä½†å¯¹ç¦»è½´æºå­˜åœ¨å½—æ˜Ÿåƒå·®å’Œé«˜é˜¶åƒå·®ã€‚</li>
<li>Wolter-Schwarzschildè®¾è®¡è™½å­˜åœ¨é«˜é˜¶åƒå·®ï¼Œä½†å·²æ ¡æ­£å½—æ˜Ÿåƒå·®ï¼Œå› æ­¤åœ¨æŸäº›æ–¹é¢ä¼˜äºWolter type-1è®¾è®¡ã€‚</li>
<li>å¯¹äºå®½è§†åœºçš„æ·±ç©ºå¤©æ–‡è§‚æµ‹æˆ–å¤ªé˜³è§‚æµ‹ï¼Œæ›´é€‚åˆä¸€ç§èƒ½åœ¨è§†åœºè¾¹ç¼˜æœ€å¤§åŒ–è§’åˆ†è¾¨ç‡è€Œä¸æ˜¯åœ¨ä¸­å¿ƒæœ€å¤§åŒ–çš„å…‰å­¦è®¾è®¡ã€‚</li>
<li>Hyperboloid-Hyperboloidå…‰å­¦è®¾è®¡å¯é€šè¿‡ç‰ºç‰²è½´å‘åˆ†è¾¨ç‡æ¥æé«˜è§†åœºè§’åˆ†è¾¨ç‡ï¼Œä½†å¯¹äºè§†é‡è¶…è¿‡ä¸€å®šå¤§å°çš„é¢†åŸŸæ•ˆæœæ›´ä½³ã€‚</li>
<li>æ–°å‹å…‰å­¦è®¾è®¡æ¶ˆé™¤äº†å½—æ˜Ÿåƒå·®ï¼Œå¯åœ¨ä»»ä½•è§†åœºè§’ä¸Šæœ€å¤§åŒ–è§’åˆ†è¾¨ç‡ï¼Œä¸”æ€§èƒ½æ”¹è¿›åœ¨ä¸åŒè§†åœºå¤§å°å’Œå…¶ä»–æœ›è¿œé•œå‚æ•°å˜åŒ–æ—¶å‡ä¿æŒä¸€è‡´ã€‚</li>
<li>è¿™ç§æ–°è®¾è®¡çš„ä¼˜è¶Šæ€§åœ¨äºå®ƒèƒ½æä¾›æ›´ä¸ºæ¸…æ™°ã€å‡†ç¡®çš„å›¾åƒï¼Œä»è€Œå¢å¼ºäº†æœ›è¿œé•œçš„è§‚æµ‹èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.03265">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e920deb364f9a1d28f05cf6c70bbe605.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-49d9356c2f53270143a763819e0700d2.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="GraphSeg-Segmented-3D-Representations-via-Graph-Edge-Addition-and-Contraction"><a href="#GraphSeg-Segmented-3D-Representations-via-Graph-Edge-Addition-and-Contraction" class="headerlink" title="GraphSeg: Segmented 3D Representations via Graph Edge Addition and   Contraction"></a>GraphSeg: Segmented 3D Representations via Graph Edge Addition and   Contraction</h2><p><strong>Authors:Haozhan Tang, Tianyi Zhang, Oliver Kroemer, Matthew Johnson-Roberson, Weiming Zhi</strong></p>
<p>Robots operating in unstructured environments often require accurate and consistent object-level representations. This typically requires segmenting individual objects from the robotâ€™s surroundings. While recent large models such as Segment Anything (SAM) offer strong performance in 2D image segmentation. These advances do not translate directly to performance in the physical 3D world, where they often over-segment objects and fail to produce consistent mask correspondences across views. In this paper, we present GraphSeg, a framework for generating consistent 3D object segmentations from a sparse set of 2D images of the environment without any depth information. GraphSeg adds edges to graphs and constructs dual correspondence graphs: one from 2D pixel-level similarities and one from inferred 3D structure. We formulate segmentation as a problem of edge addition, then subsequent graph contraction, which merges multiple 2D masks into unified object-level segmentations. We can then leverage \emph{3D foundation models} to produce segmented 3D representations. GraphSeg achieves robust segmentation with significantly fewer images and greater accuracy than prior methods. We demonstrate state-of-the-art performance on tabletop scenes and show that GraphSeg enables improved performance on downstream robotic manipulation tasks. Code available at <a target="_blank" rel="noopener" href="https://github.com/tomtang502/graphseg.git">https://github.com/tomtang502/graphseg.git</a>. </p>
<blockquote>
<p>åœ¨ç»“æ„åŒ–ç¯å¢ƒä¸­æ“ä½œçš„æœºå™¨äººé€šå¸¸éœ€è¦å‡†ç¡®ä¸”ä¸€è‡´çš„å¯¹è±¡çº§è¡¨ç¤ºã€‚è¿™é€šå¸¸éœ€è¦ä»æœºå™¨äººçš„å‘¨å›´ç¯å¢ƒä¸­åˆ†å‰²å‡ºå•ä¸ªç‰©ä½“ã€‚è™½ç„¶æœ€è¿‘çš„å¤§å‹æ¨¡å‹ï¼Œå¦‚Anythingåˆ†å‰²ï¼ˆSAMï¼‰åœ¨2Då›¾åƒåˆ†å‰²æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚ä½†è¿™äº›è¿›å±•å¹¶ä¸èƒ½ç›´æ¥è½¬åŒ–ä¸ºåœ¨ç‰©ç†3Dä¸–ç•Œä¸­çš„è¡¨ç°ï¼Œå®ƒä»¬åœ¨3Dä¸–ç•Œä¸­ç»å¸¸è¿‡åº¦åˆ†å‰²ç‰©ä½“ï¼Œå¹¶ä¸”åœ¨ä¸åŒè§†è§’ä¹‹é—´æ— æ³•äº§ç”Ÿä¸€è‡´çš„æ©è†œå¯¹åº”ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†GraphSegï¼Œè¿™æ˜¯ä¸€ä¸ªä»ç¯å¢ƒçš„ç¨€ç–2Då›¾åƒç”Ÿæˆä¸€è‡´3Då¯¹è±¡åˆ†å‰²çš„æ¡†æ¶ï¼Œæ— éœ€ä»»ä½•æ·±åº¦ä¿¡æ¯ã€‚GraphSegå‘å›¾ä¸­æ·»åŠ è¾¹å¹¶æ„å»ºåŒå¯¹åº”å›¾ï¼šä¸€ä¸ªåŸºäº2Dåƒç´ çº§çš„ç›¸ä¼¼æ€§ï¼Œå¦ä¸€ä¸ªåŸºäºæ¨æ–­çš„3Dç»“æ„ã€‚æˆ‘ä»¬å°†åˆ†å‰²åˆ¶å®šä¸ºå¢è¾¹é—®é¢˜ï¼Œç„¶åè¿›è¡Œå›¾æ”¶ç¼©ï¼Œå°†å¤šä¸ª2Dæ©è†œåˆå¹¶ä¸ºç»Ÿä¸€çš„å¯¹è±¡çº§åˆ†å‰²ã€‚ç„¶åæˆ‘ä»¬å¯ä»¥åˆ©ç”¨<em>3DåŸºç¡€æ¨¡å‹</em>æ¥äº§ç”Ÿåˆ†å‰²çš„3Dè¡¨ç¤ºã€‚GraphSegå®ç°äº†ç¨³å¥çš„åˆ†å‰²ï¼Œä½¿ç”¨è¾ƒå°‘çš„å›¾åƒå¹¶ä¸”å…·æœ‰æ¯”ä»¥å‰çš„æ–¹æ³•æ›´é«˜çš„å‡†ç¡®æ€§ã€‚æˆ‘ä»¬åœ¨æ¡Œé¢åœºæ™¯ä¸Šå±•ç¤ºäº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶è¯æ˜GraphSegèƒ½æ”¹å–„ä¸‹æ¸¸æœºå™¨äººæ“ä½œä»»åŠ¡çš„æ€§èƒ½ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/tomtang502/graphseg.git%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/tomtang502/graphseg.gitä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.03129v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†GraphSegæ¡†æ¶ï¼Œç”¨äºä»ç¨€ç–çš„äºŒç»´å›¾åƒé›†ä¸­ç”Ÿæˆä¸€è‡´çš„3Då¯¹è±¡åˆ†å‰²ï¼Œæ— éœ€æ·±åº¦ä¿¡æ¯ã€‚GraphSegé€šè¿‡æ·»åŠ è¾¹ç¼˜æ„å»ºåŒé‡å¯¹åº”å›¾ï¼Œå°†åƒç´ çº§åˆ«çš„äºŒç»´å›¾åƒè½¬åŒ–ä¸ºç‰©ä½“çº§åˆ«çš„ä¸‰ç»´æ¨¡å‹ã€‚åˆ†å‰²è¢«è§†ä¸ºä¸€ä¸ªæ·»åŠ è¾¹ç¼˜çš„é—®é¢˜ï¼Œå†é€šè¿‡æ”¶ç¼©å›¾å½¢å®ç°ä¸åŒäºŒç»´é®ç½©çš„ç»Ÿä¸€ã€‚è¯¥æ–¹æ³•åœ¨æ¡Œé¢åœºæ™¯åˆ†å‰²ä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæé«˜äº†æœºå™¨äººæ“ä½œä»»åŠ¡çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯æœ¬æ–‡çš„å…³é”®è¦ç‚¹ï¼Œä»¥ç®€ç»ƒçš„åˆ—è¡¨å½¢å¼å‘ˆç°ï¼š</p>
<ul>
<li>GraphSegæ˜¯ä¸€ä¸ªç”¨äºä»ç¨€ç–çš„äºŒç»´å›¾åƒé›†ç”Ÿæˆä¸€è‡´çš„3Då¯¹è±¡åˆ†å‰²çš„æ¡†æ¶ã€‚</li>
<li>GraphSegæ„å»ºåŒé‡å¯¹åº”å›¾ï¼šä¸€ä¸ªåŸºäºåƒç´ çº§åˆ«çš„äºŒç»´ç›¸ä¼¼æ€§ï¼Œå¦ä¸€ä¸ªåŸºäºæ¨æ–­çš„ä¸‰ç»´ç»“æ„ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.03129">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b29ca9548dd9925a58fe3f1f2e462e1f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d9b4af0b706621e7ec159f8a5566603f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b9dc46908385544031a1b18439bc73e9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2667954f95e9fe4c28f66dec32a6deba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-df480a365958f96af3a4ee08cc98ac63.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6256b71bebf932287bc9c8f5ccf7291b.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Multi-Granularity-Vision-Fastformer-with-Fusion-Mechanism-for-Skin-Lesion-Segmentation"><a href="#Multi-Granularity-Vision-Fastformer-with-Fusion-Mechanism-for-Skin-Lesion-Segmentation" class="headerlink" title="Multi-Granularity Vision Fastformer with Fusion Mechanism for Skin   Lesion Segmentation"></a>Multi-Granularity Vision Fastformer with Fusion Mechanism for Skin   Lesion Segmentation</h2><p><strong>Authors:Xuanyu Liu, Huiyun Yao, Jinggui Gao, Zhongyi Guo, Xue Zhang, Yulin Dong</strong></p>
<p>Background:Convolutional Neural Networks(CNN) and Vision Transformers(ViT) are the main techniques used in Medical image segmentation. However, CNN is limited to local contextual information, and ViTâ€™s quadratic complexity results in significant computational costs. At the same time, equipping the model to distinguish lesion boundaries with varying degrees of severity is also a challenge encountered in skin lesion segmentation. Purpose:This research aims to optimize the balance between computational costs and long-range dependency modelling and achieve excellent generalization across lesions with different degrees of severity. Methods:we propose a lightweight U-shape network that utilizes Vision Fastformer with Fusion Mechanism (VFFM-UNet). We inherit the advantages of Fastformerâ€™s additive attention mechanism, combining element-wise product and matrix product for comprehensive feature extraction and channel reduction to save computational costs. In order to accurately identify the lesion boundaries with varying degrees of severity, we designed Fusion Mechanism including Multi-Granularity Fusion and Channel Fusion, which can process the feature maps in the granularity and channel levels to obtain different contextual information. Results:Comprehensive experiments on the ISIC2017, ISIC2018 and PH2 datasets demonstrate that VFFM-UNet outperforms existing state-of-the-art models regarding parameter numbers, computational complexity and segmentation performance. In short, compared to MISSFormer, our model achieves superior segmentation performance while reducing parameter and computation costs by 101x and 15x, respectively. Conclusions:Both quantitative and qualitative analyses show that VFFM-UNet sets a new benchmark by reaching an ideal balance between parameter numbers, computational complexity, and segmentation performance compared to existing state-of-the-art models. </p>
<blockquote>
<p>èƒŒæ™¯ï¼šå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å’Œè§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰æ˜¯åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„ä¸»è¦æŠ€æœ¯ã€‚ç„¶è€Œï¼ŒCNNå—é™äºå±€éƒ¨ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œè€ŒViTçš„äºŒæ¬¡å¤æ‚æ€§å¯¼è‡´è®¡ç®—æˆæœ¬è¾ƒé«˜ã€‚åŒæ—¶ï¼Œä½¿æ¨¡å‹èƒ½å¤ŸåŒºåˆ†ä¸åŒç¨‹åº¦ä¸¥é‡æ€§çš„ç—…å˜è¾¹ç•Œä¹Ÿæ˜¯çš®è‚¤ç—…å˜åˆ†å‰²ä¸­é‡åˆ°çš„æŒ‘æˆ˜ã€‚</p>
</blockquote>
<p>ç›®çš„ï¼šæœ¬ç ”ç©¶æ—¨åœ¨ä¼˜åŒ–è®¡ç®—æˆæœ¬ä¸é•¿æœŸä¾èµ–å…³ç³»å»ºæ¨¡ä¹‹é—´çš„å¹³è¡¡ï¼Œå¹¶åœ¨ä¸åŒä¸¥é‡ç¨‹åº¦ç—…å˜ä¹‹é—´å®ç°å‡ºè‰²çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p>æ–¹æ³•ï¼šæˆ‘ä»¬æå‡ºäº†ä¸€ç§è½»é‡çº§çš„Uå½¢ç½‘ç»œï¼Œè¯¥ç½‘ç»œé‡‡ç”¨å¸¦æœ‰èåˆæœºåˆ¶çš„è§†è§‰å¿«é€Ÿæˆå½¢å™¨ï¼ˆVFFM-UNetï¼‰ã€‚æˆ‘ä»¬ç»§æ‰¿äº†FastformeråŠ æ³•æ³¨æ„åŠ›æœºåˆ¶çš„ä¼˜åŠ¿ï¼Œç»“åˆå…ƒç´ ä¹˜ç§¯å’ŒçŸ©é˜µä¹˜ç§¯è¿›è¡Œç‰¹å¾æå–å’Œç»¼åˆé€šé“ç¼©å‡ï¼Œä»¥èŠ‚çœè®¡ç®—æˆæœ¬ã€‚ä¸ºäº†å‡†ç¡®è¯†åˆ«ä¸åŒç¨‹åº¦ä¸¥é‡æ€§çš„ç—…å˜è¾¹ç•Œï¼Œæˆ‘ä»¬è®¾è®¡äº†èåˆæœºåˆ¶ï¼ŒåŒ…æ‹¬å¤šç²’åº¦èåˆå’Œé€šé“èåˆï¼Œå¯ä»¥åœ¨ç²’åº¦å’Œé€šé“çº§åˆ«å¤„ç†ç‰¹å¾æ˜ å°„ä»¥è·å¾—ä¸åŒçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚</p>
<p>ç»“æœï¼šåœ¨ISIC2017ã€ISIC2018å’ŒPH2æ•°æ®é›†ä¸Šçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒVFFM-UNetåœ¨å‚æ•°æ•°é‡ã€è®¡ç®—å¤æ‚æ€§å’Œåˆ†å‰²æ€§èƒ½ä¸Šä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ¨¡å‹ã€‚ç®€è€Œè¨€ä¹‹ï¼Œä¸MISSFormerç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨å‡å°‘å‚æ•°å’Œè®¡ç®—æˆæœ¬çš„åŒæ—¶å®ç°äº†ä¼˜è¶Šçš„åˆ†å‰²æ€§èƒ½ï¼Œåˆ†åˆ«é™ä½äº†101å€å’Œ15å€ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.03108v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºè½»é‡åŒ–Uå‹ç½‘ç»œä¸èåˆäº†å¿«é€Ÿå½¢æ€æ³¨æ„åŠ›çš„ä¼˜åŒ–æœºåˆ¶ç”¨äºåŒ»å­¦å›¾åƒåˆ†å‰²ã€‚è¯¥æ–¹æ³•ç»“åˆäº†å¿«é€Ÿå½¢æ€æ¨¡å‹çš„ä¼˜ç‚¹ï¼Œå¹¶é‡‡ç”¨å¤šç§èåˆæœºåˆ¶æ¥å¤„ç†ä¸åŒä¸¥é‡ç¨‹åº¦çš„ç—…å˜è¾¹ç•Œã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å‚æ•°æ•°é‡ã€è®¡ç®—å¤æ‚åº¦å’Œåˆ†å‰²æ€§èƒ½ä¸Šå‡ä¼˜äºç°æœ‰æŠ€æœ¯æ¨¡å‹ã€‚ç®€è€Œè¨€ä¹‹ï¼Œæ–°æ–¹æ³•åœ¨é™ä½æˆæœ¬çš„åŒæ—¶å–å¾—äº†è‰¯å¥½çš„åˆ†å‰²æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ç ”ç©¶æ—¨åœ¨ä¼˜åŒ–è®¡ç®—æˆæœ¬ä¸é•¿è·ç¦»ä¾èµ–å»ºæ¨¡ä¹‹é—´çš„å¹³è¡¡ï¼Œå¹¶åœ¨ä¸åŒä¸¥é‡ç¨‹åº¦ç—…å˜ä¸­å®ç°äº†å‡ºè‰²çš„æ³›åŒ–æ€§èƒ½ã€‚</li>
<li>é‡‡ç”¨äº†è½»é‡çº§çš„Uå‹ç½‘ç»œç»“æ„ï¼Œç»“åˆäº†å¿«é€Ÿå½¢æ€æ¨¡å‹çš„ä¼˜ç‚¹è¿›è¡Œç‰¹å¾æå–å’Œé€šé“ç¼©å‡ä»¥é™ä½è®¡ç®—æˆæœ¬ã€‚</li>
<li>è®¾è®¡äº†èåˆæœºåˆ¶ï¼ŒåŒ…æ‹¬å¤šç²’åº¦èåˆå’Œé€šé“èåˆï¼Œå¯ä»¥åœ¨ç²’åº¦å’Œé€šé“çº§åˆ«å¤„ç†ç‰¹å¾å›¾ä»¥è·å–ä¸åŒçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä»è€Œå‡†ç¡®è¯†åˆ«ä¸åŒä¸¥é‡ç¨‹åº¦ç—…å˜çš„è¾¹ç•Œã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.03108">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f6e1ca967a380f3ef507e83d8fde8190.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-12bd973b7f15b97a87211c76c3794333.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-772f50855f0d665b5d8cb6bdd96e2f63.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-be7edd7d0b049d7405bdb196ff4fc09b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-279e59c8b1a54b3e400549d5a34d326b.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Rethinking-domain-generalization-in-medical-image-segmentation-One-image-as-one-domain"><a href="#Rethinking-domain-generalization-in-medical-image-segmentation-One-image-as-one-domain" class="headerlink" title="Rethinking domain generalization in medical image segmentation: One   image as one domain"></a>Rethinking domain generalization in medical image segmentation: One   image as one domain</h2><p><strong>Authors:Jin Hong, Bo Liu, Guoli Long, Siyue Li, Khan Muhammad</strong></p>
<p>Domain shifts in medical image segmentation, particularly when data comes from different centers, pose significant challenges. Intra-center variability, such as differences in scanner models or imaging protocols, can cause domain shifts as large as, or even larger than, those between centers. To address this, we propose the â€œone image as one domainâ€ (OIOD) hypothesis, which treats each image as a unique domain, enabling flexible and robust domain generalization. Based on this hypothesis, we develop a unified disentanglement-based domain generalization (UniDDG) framework, which simultaneously handles both multi-source and single-source domain generalization without requiring explicit domain labels. This approach simplifies training with a fixed architecture, independent of the number of source domains, reducing complexity and enhancing scalability. We decouple each input image into content representation and style code, then exchange and combine these within the batch for segmentation, reconstruction, and further disentanglement. By maintaining distinct style codes for each image, our model ensures thorough decoupling of content representations and style codes, improving domain invariance of the content representations. Additionally, we enhance generalization with expansion mask attention (EMA) for boundary preservation and style augmentation (SA) to simulate diverse image styles, improving robustness to domain shifts. Extensive experiments show that our method achieves Dice scores of 84.43% and 88.91% for multi-source to single-center and single-center generalization in optic disc and optic cup segmentation, respectively, and 86.96% and 88.56% for prostate segmentation, outperforming current state-of-the-art domain generalization methods, offering superior performance and adaptability across clinical settings. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„åŸŸåç§»ï¼Œå°¤å…¶æ˜¯å½“æ•°æ®æ¥è‡ªä¸åŒçš„ä¸­å¿ƒæ—¶ï¼Œä¼šå¸¦æ¥é‡å¤§æŒ‘æˆ˜ã€‚æ¥è‡ªåŒä¸€ä¸­å¿ƒçš„å†…éƒ¨æ•°æ®å˜åŒ–ï¼Œå¦‚æ‰«æä»ªå‹å·æˆ–æˆåƒåè®®çš„ä¸åŒï¼Œå¯èƒ½ä¼šå¯¼è‡´ä¸è·¨ä¸­å¿ƒä¸€æ ·ç”šè‡³æ›´å¤§çš„åŸŸåç§»ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†â€œä¸€å›¾ä¸€åŸŸâ€ï¼ˆOIODï¼‰å‡è®¾ï¼Œå°†æ¯å¼ å›¾åƒè§†ä¸ºä¸€ä¸ªç‹¬ç‰¹çš„åŸŸï¼Œä»¥å®ç°çµæ´»å’Œç¨³å¥çš„åŸŸæ³›åŒ–ã€‚åŸºäºè¿™ä¸€å‡è®¾ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªç»Ÿä¸€çš„åŸºäºè§£çº ç¼ çš„åŸŸæ³›åŒ–ï¼ˆUniDDGï¼‰æ¡†æ¶ï¼Œå¯ä»¥åŒæ—¶å¤„ç†å¤šæºå’Œå•æºåŸŸæ³›åŒ–ï¼Œæ— éœ€æ˜ç¡®çš„åŸŸæ ‡ç­¾ã€‚è¿™ç§æ–¹æ³•ç®€åŒ–äº†ä½¿ç”¨å›ºå®šæ¶æ„çš„è®­ç»ƒè¿‡ç¨‹ï¼Œç‹¬ç«‹äºæºåŸŸçš„æ•°é‡ï¼Œé™ä½äº†å¤æ‚æ€§å¹¶å¢å¼ºäº†å¯æ‰©å±•æ€§ã€‚æˆ‘ä»¬å°†æ¯ä¸ªè¾“å…¥å›¾åƒè§£è€¦ä¸ºå†…å®¹è¡¨ç¤ºå’Œé£æ ¼ä»£ç ï¼Œç„¶ååœ¨æ‰¹æ¬¡å†…äº¤æ¢å¹¶ç»“åˆè¿™äº›ä¿¡æ¯è¿›è¡Œåˆ†å‰²ã€é‡å»ºå’Œè¿›ä¸€æ­¥çš„è§£çº ç¼ ã€‚é€šè¿‡ä¸ºæ¯ä¸ªå›¾åƒä¿ç•™ç‹¬ç‰¹çš„é£æ ¼ä»£ç ï¼Œæˆ‘ä»¬çš„æ¨¡å‹ç¡®ä¿äº†å†…å®¹è¡¨ç¤ºå’Œé£æ ¼ä»£ç çš„å½»åº•è§£è€¦ï¼Œæé«˜äº†å†…å®¹è¡¨ç¤ºçš„é¢†åŸŸä¸å˜æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡æ‰©å±•æ©è†œæ³¨æ„åŠ›ï¼ˆEMAï¼‰è¿›è¡Œè¾¹ç•Œä¿ç•™å’Œé£æ ¼å¢å¼ºï¼ˆSAï¼‰æ¥æ¨¡æ‹Ÿå„ç§å›¾åƒé£æ ¼ï¼Œä»¥å¢å¼ºå¯¹åŸŸåç§»çš„ç¨³å¥æ€§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è§†ç›˜å’Œè§†æ¯åˆ†å‰²ä¸­ï¼Œå¤šæºåˆ°å•ä¸­å¿ƒçš„æ³›åŒ–å’Œå•ä¸­å¿ƒæ³›åŒ–åˆ†åˆ«å®ç°äº†84.43%å’Œ88.91%çš„Diceå¾—åˆ†ï¼›åœ¨å‰åˆ—è…ºåˆ†å‰²ä¸­ï¼Œæˆ‘ä»¬æ–¹æ³•çš„Diceå¾—åˆ†ä¸º86.96%å’Œ88.56%ï¼Œè¶…è¶Šäº†å½“å‰æœ€å…ˆè¿›çš„åŸŸæ³›åŒ–æ–¹æ³•ï¼Œåœ¨ä¸´åºŠç¯å¢ƒä¸­æä¾›äº†å“è¶Šçš„æ€§èƒ½å’Œé€‚åº”æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.04741v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    åŒ»å­¦å›¾åƒåˆ†å‰²é¢†åŸŸé¢ä¸´æ¥è‡ªä¸åŒä¸­å¿ƒæ•°æ®çš„é¢†åŸŸæ¼‚ç§»æŒ‘æˆ˜ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºâ€œä¸€å¹…å›¾åƒä½œä¸ºä¸€ä¸ªé¢†åŸŸâ€ï¼ˆOIODï¼‰å‡è®¾ï¼Œå¹¶åŸºäºæ­¤å¼€å‘äº†ä¸€ä¸ªç»Ÿä¸€è§£çº ç¼ é¢†åŸŸæ³›åŒ–ï¼ˆUniDDGï¼‰æ¡†æ¶ã€‚è¯¥æ¡†æ¶æ— éœ€æ˜ç¡®çš„é¢†åŸŸæ ‡ç­¾å³å¯åŒæ—¶å¤„ç†å¤šæºå’Œå•æºé¢†åŸŸæ³›åŒ–é—®é¢˜ã€‚é€šè¿‡å›¾åƒå†…å®¹è¡¨ç¤ºå’Œé£æ ¼ä»£ç çš„è§£è€¦ï¼Œè¯¥æ¡†æ¶æé«˜äº†é¢†åŸŸä¸å˜æ€§ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å¼•å…¥äº†æ‰©å±•æ©è†œæ³¨æ„åŠ›ï¼ˆEMAï¼‰å’Œé£æ ¼å¢å¼ºï¼ˆSAï¼‰æŠ€æœ¯ä»¥å¢å¼ºæ¨¡å‹çš„é²æ£’æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå¦‚è§†ç›˜å’Œè§†æ¯åˆ†å‰²ä»¥åŠå‰åˆ—è…ºåˆ†å‰²ç­‰ä»»åŠ¡ä¸Šçš„è¡¨ç°å‡ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„é¢†åŸŸæ³›åŒ–æ–¹æ³•ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>é¢†åŸŸæ¼‚ç§»æ˜¯åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„é‡å¤§æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯å½“æ•°æ®æ¥è‡ªä¸åŒä¸­å¿ƒæ—¶ã€‚</li>
<li>â€œä¸€å¹…å›¾åƒä½œä¸ºä¸€ä¸ªé¢†åŸŸâ€ï¼ˆOIODï¼‰å‡è®¾è§£å†³äº†è¿™ä¸€æŒ‘æˆ˜ï¼Œå…è®¸çµæ´»çš„é¢†åŸŸæ³›åŒ–ã€‚</li>
<li>æå‡ºçš„UniDDGæ¡†æ¶èƒ½å¤ŸåŒæ—¶å¤„ç†å¤šæºå’Œå•æºé¢†åŸŸæ³›åŒ–é—®é¢˜ï¼Œæ— éœ€æ˜ç¡®çš„é¢†åŸŸæ ‡ç­¾ã€‚</li>
<li>é€šè¿‡è§£è€¦å›¾åƒå†…å®¹è¡¨ç¤ºå’Œé£æ ¼ä»£ç ï¼ŒUniDDGæ¡†æ¶æé«˜äº†é¢†åŸŸä¸å˜æ€§ã€‚</li>
<li>æ‰©å±•æ©è†œæ³¨æ„åŠ›ï¼ˆEMAï¼‰å’Œé£æ ¼å¢å¼ºï¼ˆSAï¼‰æŠ€æœ¯å¢å¼ºäº†æ¨¡å‹çš„é²æ£’æ€§ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå¦‚è§†ç›˜å’Œè§†æ¯åˆ†å‰²ä»¥åŠå‰åˆ—è…ºåˆ†å‰²ç­‰ä»»åŠ¡ä¸Šå‡ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„é¢†åŸŸæ³›åŒ–æ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•ç®€åŒ–äº†è®­ç»ƒè¿‡ç¨‹ï¼Œæé«˜äº†æ¨¡å‹çš„é€‚åº”æ€§å’Œå¯æ‰©å±•æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.04741">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6d29a3e16ef3b2bd0a1147c51920a365.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-53bbb5a9582ae5ca9099d4f3c2e8020c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1572d15bbfb382232e86e46e0ea2ad4d.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="CALICO-Part-Focused-Semantic-Co-Segmentation-with-Large-Vision-Language-Models"><a href="#CALICO-Part-Focused-Semantic-Co-Segmentation-with-Large-Vision-Language-Models" class="headerlink" title="CALICO: Part-Focused Semantic Co-Segmentation with Large Vision-Language   Models"></a>CALICO: Part-Focused Semantic Co-Segmentation with Large Vision-Language   Models</h2><p><strong>Authors:Kiet A. Nguyen, Adheesh Juvekar, Tianjiao Yu, Muntasir Wahed, Ismini Lourentzou</strong></p>
<p>Recent advances in Large Vision-Language Models (LVLMs) have enabled general-purpose vision tasks through visual instruction tuning. While existing LVLMs can generate segmentation masks from text prompts for single images, they struggle with segmentation-grounded reasoning across images, especially at finer granularities such as object parts. In this paper, we introduce the new task of part-focused semantic co-segmentation, which involves identifying and segmenting common objects, as well as common and unique object parts across images. To address this task, we present CALICO, the first LVLM designed for multi-image part-level reasoning segmentation. CALICO features two key components, a novel Correspondence Extraction Module that identifies semantic part-level correspondences, and Correspondence Adaptation Modules that embed this information into the LVLM to facilitate multi-image understanding in a parameter-efficient manner. To support training and evaluation, we curate MixedParts, a large-scale multi-image segmentation dataset containing $\sim$2.4M samples across $\sim$44K images spanning diverse object and part categories. Experimental results demonstrate that CALICO, with just 0.3% of its parameters finetuned, achieves strong performance on this challenging task. </p>
<blockquote>
<p>è¿‘æœŸå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰çš„è¿›æ­¥é€šè¿‡è§†è§‰æŒ‡ä»¤è°ƒæ•´å®ç°äº†é€šç”¨è§†è§‰ä»»åŠ¡ã€‚è™½ç„¶ç°æœ‰çš„LVLMså¯ä»¥ä¸ºå•å¼ å›¾ç‰‡ç”Ÿæˆæ–‡æœ¬æç¤ºçš„åˆ†å‰²æ©è†œï¼Œä½†å®ƒä»¬åœ¨è¿›è¡Œè·¨å›¾åƒçš„åˆ†å‰²æ¨ç†æ—¶é¢ä¸´å›°éš¾ï¼Œå°¤å…¶æ˜¯åœ¨æ›´ç²¾ç»†çš„ç²’åº¦ï¼ˆå¦‚ç‰©ä½“éƒ¨åˆ†ï¼‰ä¸Šã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ–°çš„éƒ¨åˆ†èšç„¦è¯­ä¹‰ååŒåˆ†å‰²ä»»åŠ¡ï¼Œè¯¥ä»»åŠ¡æ¶‰åŠè¯†åˆ«å’Œåˆ†å‰²è·¨å›¾åƒçš„å¸¸è§å¯¹è±¡å’Œå¸¸è§åŠç‹¬ç‰¹çš„å¯¹è±¡éƒ¨åˆ†ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€ä»»åŠ¡ï¼Œæˆ‘ä»¬æå‡ºäº†CALICOï¼Œè¿™æ˜¯ä¸“ä¸ºå¤šå›¾åƒéƒ¨åˆ†çº§æ¨ç†åˆ†å‰²è®¾è®¡çš„é¦–ä¸ªLVLMã€‚CALICOå…·æœ‰ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼Œä¸€ä¸ªæ˜¯æ–°é¢–çš„å¯¹åº”æå–æ¨¡å—ï¼Œç”¨äºè¯†åˆ«è¯­ä¹‰éƒ¨åˆ†çº§çš„å¯¹åº”ï¼Œå¦ä¸€ä¸ªæ˜¯å¯¹åº”é€‚åº”æ¨¡å—ï¼Œç”¨äºå°†æ­¤ä¿¡æ¯åµŒå…¥LVLMä¸­ï¼Œä»¥é«˜æ•ˆçš„æ–¹å¼ä¿ƒè¿›å¤šå›¾åƒç†è§£ã€‚ä¸ºäº†æ”¯æŒè®­ç»ƒå’Œè¯„ä¼°ï¼Œæˆ‘ä»¬åˆ›å»ºäº†MixedPartsæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„å¤šå›¾åƒåˆ†å‰²æ•°æ®é›†ï¼ŒåŒ…å«çº¦4.4ä¸‡å¼ å›¾åƒå’Œçº¦24ä¸‡ä¸ªæ ·æœ¬ï¼Œæ¶µç›–å¤šç§å¯¹è±¡å’Œéƒ¨ä»¶ç±»åˆ«ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ä¸­ï¼Œåªéœ€å¾®è°ƒCALICOçš„0.3%å‚æ•°å³å¯å®ç°å¼ºå¤§çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.19331v2">PDF</a> Accepted to CVPR 2025. Project page:   <a target="_blank" rel="noopener" href="https://plan-lab.github.io/calico/">https://plan-lab.github.io/calico/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰çš„æœ€æ–°è¿›å±•ï¼Œé€šè¿‡è§†è§‰æŒ‡ä»¤å¾®è°ƒå®ç°äº†é€šç”¨è§†è§‰ä»»åŠ¡ã€‚é’ˆå¯¹ç°æœ‰LVLMsåœ¨è·¨å›¾åƒåˆ†å‰²æ¨ç†æ–¹é¢çš„ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯åœ¨æ›´ç²¾ç»†çš„ç²’åº¦å¦‚ç‰©ä½“éƒ¨åˆ†ä¸Šçš„æŒ‘æˆ˜ï¼Œæœ¬æ–‡å¼•å…¥äº†éƒ¨åˆ†èšç„¦è¯­ä¹‰ååŒåˆ†å‰²çš„æ–°ä»»åŠ¡ã€‚ä¸ºåº”å¯¹æ­¤ä»»åŠ¡ï¼Œæå‡ºäº†ä¸“ä¸ºå¤šå›¾åƒéƒ¨åˆ†çº§æ¨ç†åˆ†å‰²è®¾è®¡çš„CALICOæ¨¡å‹ã€‚CALICOåŒ…å«ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šä¸€ç§æ–°å‹å¯¹åº”å…³ç³»æå–æ¨¡å—ï¼Œç”¨äºè¯†åˆ«è¯­ä¹‰éƒ¨åˆ†çº§å¯¹åº”å…³ç³»ï¼›ä»¥åŠå¯¹åº”å…³ç³»é€‚é…æ¨¡å—ï¼Œä»¥å‚æ•°æœ‰æ•ˆçš„æ–¹å¼å°†æ­¤ä¿¡æ¯åµŒå…¥LVLMï¼Œä¿ƒè¿›å¤šå›¾åƒç†è§£ã€‚ä¸ºæ”¯æŒå’Œè¯„ä¼°æ¨¡å‹è®­ç»ƒï¼Œæˆ‘ä»¬æ•´ç†äº†MixedPartså¤§å‹å¤šå›¾åƒåˆ†å‰²æ•°æ®é›†ï¼ŒåŒ…å«çº¦240ä¸‡æ ·æœ¬å’Œçº¦4ä¸‡å¼ æ¶µç›–å¤šç§å¯¹è±¡å’Œç±»åˆ«å›¾åƒçš„å›¾ç‰‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ä¸­ï¼Œä»…éœ€å¾®è°ƒCALICOçš„0.3%å‚æ•°å³å¯å®ç°å¼ºå¤§æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰èƒ½å¤Ÿé€šè¿‡è§†è§‰æŒ‡ä»¤å¾®è°ƒå®Œæˆé€šç”¨è§†è§‰ä»»åŠ¡ã€‚</li>
<li>ç°æœ‰LVLMsåœ¨è·¨å›¾åƒåˆ†å‰²æ¨ç†æ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨ç‰©ä½“éƒ¨åˆ†çš„ç²¾ç»†ç²’åº¦ä¸Šã€‚</li>
<li>å¼•å…¥äº†éƒ¨åˆ†èšç„¦è¯­ä¹‰ååŒåˆ†å‰²çš„æ–°ä»»åŠ¡ï¼Œæ—¨åœ¨è§£å†³è·¨å›¾åƒçš„ç‰©ä½“å’Œéƒ¨åˆ†è¯†åˆ«ä¸åˆ†å‰²é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸“ä¸ºå¤šå›¾åƒéƒ¨åˆ†çº§æ¨ç†åˆ†å‰²è®¾è®¡çš„CALICOæ¨¡å‹ã€‚</li>
<li>CALICOåŒ…å«ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šå¯¹åº”å…³ç³»æå–æ¨¡å—å’Œå¯¹åº”å…³ç³»é€‚é…æ¨¡å—ã€‚</li>
<li>ä¸ºäº†è®­ç»ƒå’Œè¯„ä¼°CALICOæ¨¡å‹ï¼Œæ•´ç†äº†ä¸€ä¸ªå¤§å‹å¤šå›¾åƒåˆ†å‰²æ•°æ®é›†MixedPartsã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.19331">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a6cc99700f1b1bfa75c2a451f860ecc3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a21e00ccb1f8020e81751da33796b829.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bf2079458df343e6a0e5678170128344.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5f294b1649b43f0755d74680f498bb62.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-90b28495fc73d1cda7d98f8d14cf7263.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Revisiting-MAE-pre-training-for-3D-medical-image-segmentation"><a href="#Revisiting-MAE-pre-training-for-3D-medical-image-segmentation" class="headerlink" title="Revisiting MAE pre-training for 3D medical image segmentation"></a>Revisiting MAE pre-training for 3D medical image segmentation</h2><p><strong>Authors:Tassilo Wald, Constantin Ulrich, Stanislav Lukyanenko, Andrei Goncharov, Alberto Paderno, Maximilian Miller, Leander Maerkisch, Paul F. JÃ¤ger, Klaus Maier-Hein</strong></p>
<p>Self-Supervised Learning (SSL) presents an exciting opportunity to unlock the potential of vast, untapped clinical datasets, for various downstream applications that suffer from the scarcity of labeled data. While SSL has revolutionized fields like natural language processing and computer vision, its adoption in 3D medical image computing has been limited by three key pitfalls: Small pre-training dataset sizes, architectures inadequate for 3D medical image analysis, and insufficient evaluation practices. In this paper, we address these issues by i) leveraging a large-scale dataset of 39k 3D brain MRI volumes and ii) using a Residual Encoder U-Net architecture within the state-of-the-art nnU-Net framework. iii) A robust development framework, incorporating 5 development and 8 testing brain MRI segmentation datasets, allowed performance-driven design decisions to optimize the simple concept of Masked Auto Encoders (MAEs) for 3D CNNs. The resulting model not only surpasses previous SSL methods but also outperforms the strong nnU-Net baseline by an average of approximately 3 Dice points setting a new state-of-the-art. Our code and models are made available here. </p>
<blockquote>
<p>è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰ä¸ºè§£é”å¤§é‡æœªå¼€å‘çš„ä¸´åºŠæ•°æ®é›†æ½œåŠ›æä¾›äº†æ¿€åŠ¨äººå¿ƒçš„æœºä¼šï¼Œç”¨äºå„ç§å› ç¼ºä¹æ ‡è®°æ•°æ®è€Œå—å½±å“çš„ä¸‹æ¸¸åº”ç”¨ã€‚è™½ç„¶SSLå·²ç»å½»åº•æ”¹å˜äº†è‡ªç„¶è¯­è¨€å¤„ç†å’Œè®¡ç®—æœºè§†è§‰ç­‰é¢†åŸŸï¼Œä½†å…¶åœ¨3DåŒ»å­¦å›¾åƒè®¡ç®—ä¸­çš„åº”ç”¨å—åˆ°ä¸‰ä¸ªä¸»è¦é—®é¢˜çš„é™åˆ¶ï¼šé¢„è®­ç»ƒæ•°æ®é›†è§„æ¨¡å°ã€ç”¨äº3DåŒ»å­¦å›¾åƒåˆ†æçš„æ¶æ„ä¸è¶³ä»¥åŠè¯„ä¼°å®è·µä¸å……åˆ†ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡ä»¥ä¸‹æ–¹æ³•è§£å†³è¿™äº›é—®é¢˜ï¼ši)åˆ©ç”¨åŒ…å«39kä¸ª3Då¤§è„‘MRIä½“ç§¯çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼›ii)åœ¨æœ€æ–°nnU-Netæ¡†æ¶å†…ä½¿ç”¨Residual Encoder U-Netæ¶æ„ï¼›iii)ä¸€ä¸ªç¨³å¥çš„å¼€å‘æ¡†æ¶ï¼Œç»“åˆ5ä¸ªå¼€å‘é›†å’Œ8ä¸ªæµ‹è¯•å¤§è„‘MRIåˆ†å‰²æ•°æ®é›†ï¼Œæ¨åŠ¨ä»¥æ€§èƒ½ä¸ºå¯¼å‘çš„è®¾è®¡å†³ç­–ï¼Œä¼˜åŒ–3Då·ç§¯ç¥ç»ç½‘ç»œçš„Masked Auto Encodersï¼ˆMAEsï¼‰çš„ç®€æ´æ¦‚å¿µã€‚ç”±æ­¤äº§ç”Ÿçš„æ¨¡å‹ä¸ä»…è¶…è¶Šäº†ä»¥å‰çš„SSLæ–¹æ³•ï¼Œè€Œä¸”è¿˜ä¼˜äºå¼ºå¤§çš„nnU-NetåŸºçº¿ï¼Œå¹³å‡æé«˜äº†å¤§çº¦3ä¸ªDiceç‚¹ï¼Œåˆ›é€ äº†æ–°çš„æœ€å…ˆè¿›çš„æ°´å¹³ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ¨¡å‹å¯åœ¨æ­¤å¤„è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.23132v3">PDF</a> CVPR 2025. Update to Camera-Ready</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰åœ¨è§£å†³ä¸‰ç»´åŒ»å­¦å›¾åƒè®¡ç®—é¢†åŸŸçš„é—®é¢˜ä¸­çš„æ½œåŠ›ä¸åº”ç”¨ã€‚é€šè¿‡è§£å†³æ•°æ®è§„æ¨¡å°ã€æ¶æ„ä¸è¶³ä»¥åŠè¯„ä¼°æ–¹æ³•ä¸å®Œå–„ä¸‰å¤§æŒ‘æˆ˜ï¼Œé‡‡ç”¨å¤§è§„æ¨¡çš„ä¸‰ç»´åŒ»å­¦å›¾åƒæ•°æ®é›†è¿›è¡Œé¢„è®­ç»ƒã€é‡‡ç”¨æ®‹å·®ç¼–ç å™¨U-Netæ¶æ„å¹¶ä½¿ç”¨å¯é çš„è¯„ä¼°æ¡†æ¶ï¼ŒæˆåŠŸå®ç°äº†å¯¹è‡ªç›‘ç£å­¦ä¹ æ¨¡å‹çš„ä¼˜åŒ–ã€‚æ¨¡å‹ä¸ä»…åœ¨è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ä¸Šå–å¾—äº†çªç ´ï¼Œè¿˜è¶…è¶Šäº†å¼ºå¤§çš„nnU-NetåŸºçº¿æ¨¡å‹ï¼Œä¸ºä¸‰ç»´åŒ»å­¦å›¾åƒåˆ†å‰²æä¾›äº†æ–°çš„æœ€ä½³å®è·µã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰å¯¹äºè§£å†³ä¸‰ç»´åŒ»å­¦å›¾åƒè®¡ç®—é—®é¢˜æœ‰å·¨å¤§æ½œåŠ›ã€‚</li>
<li>è§£å†³SSLåœ¨åŒ»å­¦å›¾åƒé¢†åŸŸåº”ç”¨çš„ä¸‰å¤§æŒ‘æˆ˜ï¼šå°è§„æ¨¡çš„é¢„è®­ç»ƒæ•°æ®é›†ã€ä¸é€‚åˆä¸‰ç»´åŒ»å­¦å›¾åƒåˆ†æçš„æ¶æ„ä»¥åŠä¸è¶³çš„è¯„ä¼°æ–¹æ³•ã€‚</li>
<li>åˆ©ç”¨å¤§è§„æ¨¡çš„ä¸‰ç»´åŒ»å­¦å›¾åƒæ•°æ®é›†è¿›è¡Œé¢„è®­ç»ƒï¼Œæé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>é‡‡ç”¨æ®‹å·®ç¼–ç å™¨U-Netæ¶æ„å¹¶ç»“åˆnnU-Netæ¡†æ¶è¿›è¡Œä¼˜åŒ–ã€‚</li>
<li>é€šè¿‡é‡‡ç”¨Masked Auto Encodersï¼ˆMAEsï¼‰çš„ä¼˜åŒ–æ¦‚å¿µï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>æ¨¡å‹ä¸ä»…åœ¨è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ä¸Šè¶…è¶Šä¹‹å‰çš„æ¨¡å‹ï¼Œè¿˜è¶…è¶Šäº†å¼ºå¤§çš„åŸºçº¿æ¨¡å‹nnU-Netï¼Œè®¾ç½®äº†ä¸€ä¸ªæ–°çš„æ€§èƒ½åŸºå‡†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.23132">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-96c4cfc2342bc3796163bcb1a4d5a58a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bcafeb76de033e2df60be57189aae835.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7b9ac770c204dda306c94566501c385b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-baac8376eaef86de2b4c3f2c39e159bb.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="A-Unified-Model-for-Compressed-Sensing-MRI-Across-Undersampling-Patterns"><a href="#A-Unified-Model-for-Compressed-Sensing-MRI-Across-Undersampling-Patterns" class="headerlink" title="A Unified Model for Compressed Sensing MRI Across Undersampling Patterns"></a>A Unified Model for Compressed Sensing MRI Across Undersampling Patterns</h2><p><strong>Authors:Armeet Singh Jatyani, Jiayun Wang, Aditi Chandrashekar, Zihui Wu, Miguel Liu-Schiaffini, Bahareh Tolooshams, Anima Anandkumar</strong></p>
<p>Compressed Sensing MRI reconstructs images of the bodyâ€™s internal anatomy from undersampled measurements, thereby reducing scan time. Recently, deep learning has shown great potential for reconstructing high-fidelity images from highly undersampled measurements. However, one needs to train multiple models for different undersampling patterns and desired output image resolutions, since most networks operate on a fixed discretization. Such approaches are highly impractical in clinical settings, where undersampling patterns and image resolutions are frequently changed to accommodate different real-time imaging and diagnostic requirements.   We propose a unified MRI reconstruction model robust to various measurement undersampling patterns and image resolutions. Our approach uses neural operators, a discretization-agnostic architecture applied in both image and measurement spaces, to capture local and global features. Empirically, our model improves SSIM by 11% and PSNR by 4 dB over a state-of-the-art CNN (End-to-End VarNet), with 600$\times$ faster inference than diffusion methods. The resolution-agnostic design also enables zero-shot super-resolution and extended field-of-view reconstruction, offering a versatile and efficient solution for clinical MR imaging. Our unified model offers a versatile solution for MRI, adapting seamlessly to various measurement undersampling and imaging resolutions, making it highly effective for flexible and reliable clinical imaging. Our code is available at <a target="_blank" rel="noopener" href="https://armeet.ca/nomri">https://armeet.ca/nomri</a>. </p>
<blockquote>
<p>å‹ç¼©æ„ŸçŸ¥MRIé€šè¿‡æ¬ é‡‡æ ·çš„æµ‹é‡å€¼é‡å»ºèº«ä½“å†…éƒ¨è§£å‰–ç»“æ„çš„å›¾åƒï¼Œä»è€Œç¼©çŸ­æ‰«ææ—¶é—´ã€‚æœ€è¿‘ï¼Œæ·±åº¦å­¦ä¹ åœ¨ä»é«˜åº¦æ¬ é‡‡æ ·çš„æµ‹é‡å€¼é‡å»ºé«˜ä¿çœŸå›¾åƒæ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œç”±äºå¤§å¤šæ•°ç½‘ç»œåœ¨å›ºå®šçš„ç¦»æ•£åŒ–ä¸Šæ“ä½œï¼Œå› æ­¤éœ€è¦é’ˆå¯¹ä¸åŒçš„æ¬ é‡‡æ ·æ¨¡å¼å’Œæ‰€éœ€çš„è¾“å‡ºå›¾åƒåˆ†è¾¨ç‡è®­ç»ƒå¤šä¸ªæ¨¡å‹ã€‚è¿™ç§æ–¹æ³•åœ¨ä¸´åºŠç¯å¢ƒä¸­éå¸¸ä¸å®ç”¨ï¼Œå› ä¸ºæ¬ é‡‡æ ·æ¨¡å¼å’Œå›¾åƒåˆ†è¾¨ç‡ç»å¸¸æ ¹æ®ä¸åŒçš„å®æ—¶æˆåƒå’Œè¯Šæ–­è¦æ±‚è¿›è¡Œæ›´æ”¹ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»Ÿä¸€ä¸”å¯¹å„ç§æµ‹é‡æ¬ é‡‡æ ·æ¨¡å¼å’Œå›¾åƒåˆ†è¾¨ç‡å…·æœ‰é²æ£’æ€§çš„MRIé‡å»ºæ¨¡å‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä½¿ç”¨ç¥ç»ç®—å­ï¼Œè¿™æ˜¯ä¸€ç§åº”ç”¨äºå›¾åƒå’Œæµ‹é‡ç©ºé—´çš„ç¦»æ•£åŒ–æ— å…³æ¶æ„ï¼Œä»¥æ•è·å±€éƒ¨å’Œå…¨å±€ç‰¹å¾ã€‚ç»éªŒä¸Šï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨æœ€æ–°CNNï¼ˆç«¯åˆ°ç«¯VarNetï¼‰çš„åŸºç¡€ä¸Šæé«˜äº†SSIM 11%å’ŒPSNR 4 dBï¼Œæ¨ç†é€Ÿåº¦æ˜¯æ‰©æ•£æ–¹æ³•çš„600å€ã€‚åˆ†è¾¨ç‡æ— å…³çš„è®¾è®¡è¿˜å®ç°äº†é›¶å¿«é—¨è¶…åˆ†è¾¨ç‡å’Œæ‰©å±•è§†é‡é‡å»ºï¼Œä¸ºä¸´åºŠMRæˆåƒæä¾›äº†é€šç”¨ä¸”é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬çš„ç»Ÿä¸€æ¨¡å‹ä¸ºMRIæä¾›äº†é€šç”¨è§£å†³æ–¹æ¡ˆï¼Œæ— ç¼é€‚åº”å„ç§æµ‹é‡æ¬ é‡‡æ ·å’Œæˆåƒåˆ†è¾¨ç‡ï¼Œä½¿å…¶æˆä¸ºçµæ´»å¯é çš„ä¸´åºŠæˆåƒä¸­çš„é«˜æ•ˆå·¥å…·ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://armeet.ca/nomri%E6%89%BE%E5%88%B0%E3%80%82">https://armeet.ca/nomriæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.16290v4">PDF</a> Accepted at 2025 Conference on Computer Vision and Pattern   Recognition</p>
<p><strong>Summary</strong></p>
<p>å‹ç¼©æ„ŸçŸ¥MRIæŠ€æœ¯èƒ½ä»æ¬ é‡‡æ ·çš„æµ‹é‡å€¼é‡å»ºèº«ä½“å†…éƒ¨ç»“æ„å›¾åƒï¼Œä»è€Œç¼©çŸ­æ‰«ææ—¶é—´ã€‚æ·±åº¦å­¦ä¹ åœ¨ä»é«˜åº¦æ¬ é‡‡æ ·çš„æµ‹é‡å€¼é‡å»ºé«˜ä¿çœŸå›¾åƒæ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œç”±äºå¤§å¤šæ•°ç½‘ç»œåœ¨å›ºå®šçš„ç¦»æ•£åŒ–ä¸Šæ“ä½œï¼Œéœ€è¦é’ˆå¯¹ä¸åŒçš„æ¬ é‡‡æ ·æ¨¡å¼å’Œç›®æ ‡è¾“å‡ºå›¾åƒåˆ†è¾¨ç‡è®­ç»ƒå¤šä¸ªæ¨¡å‹ï¼Œè¿™åœ¨ä¸´åºŠç¯å¢ƒä¸­éå¸¸ä¸å®ç”¨ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„MRIé‡å»ºæ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯¹å¤šç§æµ‹é‡æ¬ é‡‡æ ·æ¨¡å¼å’Œå›¾åƒåˆ†è¾¨ç‡å…·æœ‰é²æ£’æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä½¿ç”¨ç¥ç»ç®—å­ï¼Œä¸€ç§åº”ç”¨äºå›¾åƒå’Œæµ‹é‡ç©ºé—´çš„ç¦»æ•£åŒ–æ— å…³æ¶æ„ï¼Œæ¥æ•æ‰å±€éƒ¨å’Œå…¨å±€ç‰¹å¾ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨SSIMä¸Šæ¯”æœ€å…ˆè¿›çš„CNNï¼ˆç«¯åˆ°ç«¯VarNetï¼‰æé«˜äº†11%ï¼ŒPSNRæé«˜äº†4åˆ†è´ï¼Œå¹¶ä¸”æ¨ç†é€Ÿåº¦æ¯”æ‰©æ•£æ–¹æ³•å¿«600å€ã€‚åˆ†è¾¨ç‡æ— å…³çš„è®¾è®¡è¿˜å®ç°äº†é›¶å°„å‡»è¶…åˆ†è¾¨ç‡å’Œæ‰©å±•è§†é‡é‡å»ºï¼Œä¸ºä¸´åºŠMRæˆåƒæä¾›äº†é€šç”¨å’Œé«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬çš„ç»Ÿä¸€æ¨¡å‹ä¸ºMRIæä¾›äº†ä¸€ä¸ªé€šç”¨è§£å†³æ–¹æ¡ˆï¼Œèƒ½å¤Ÿè½»æ¾é€‚åº”å„ç§æµ‹é‡æ¬ é‡‡æ ·å’Œæˆåƒåˆ†è¾¨ç‡ï¼Œå¯¹äºçµæ´»å¯é çš„ä¸´åºŠæˆåƒéå¸¸æœ‰æ•ˆã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://armeet.ca/nomri%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://armeet.ca/nomriä¸Šæ‰¾åˆ°ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å‹ç¼©æ„ŸçŸ¥MRIèƒ½ä»æ¬ é‡‡æ ·æµ‹é‡å€¼é‡å»ºå›¾åƒï¼Œç¼©çŸ­æ‰«ææ—¶é—´ã€‚</li>
<li>æ·±åº¦å­¦ä¹ åœ¨MRIé‡å»ºä¸­å…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œå°¤å…¶åœ¨å¤„ç†é«˜åº¦æ¬ é‡‡æ ·æ•°æ®æ–¹é¢ã€‚</li>
<li>å½“å‰æ–¹æ³•éœ€è¦é’ˆå¯¹å¤šç§æ¬ é‡‡æ ·æ¨¡å¼å’Œå›¾åƒåˆ†è¾¨ç‡è®­ç»ƒå¤šä¸ªæ¨¡å‹ï¼Œä¸ä¾¿äºä¸´åºŠåº”ç”¨ã€‚</li>
<li>æå‡ºçš„ç»Ÿä¸€MRIé‡å»ºæ¨¡å‹å¯¹å„ç§æ¬ é‡‡æ ·æ¨¡å¼å’Œå›¾åƒåˆ†è¾¨ç‡å…·æœ‰é²æ£’æ€§ã€‚</li>
<li>ä½¿ç”¨ç¥ç»ç®—å­æ¥æ•æ‰å±€éƒ¨å’Œå…¨å±€ç‰¹å¾ï¼Œå®ç°æ›´å¥½çš„å›¾åƒé‡å»ºæ•ˆæœã€‚</li>
<li>æ¨¡å‹åœ¨SSIMå’ŒPSNRæŒ‡æ ‡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä¸”æ¨ç†é€Ÿåº¦è¾ƒå¿«ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.16290">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6f8c688d7ce9e5478adb5c7dd2a88011.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d7d366b2178abe4b1fb612c708f7c684.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-15f5b1c62ee8a5e384497bc5ee3a9433.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-086f1fbbbd22b0cf338569e755f5ee20.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3a2415493975f95a81ce37f0eb993b2b.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Developing-Generalist-Foundation-Models-from-a-Multimodal-Dataset-for-3D-Computed-Tomography"><a href="#Developing-Generalist-Foundation-Models-from-a-Multimodal-Dataset-for-3D-Computed-Tomography" class="headerlink" title="Developing Generalist Foundation Models from a Multimodal Dataset for 3D   Computed Tomography"></a>Developing Generalist Foundation Models from a Multimodal Dataset for 3D   Computed Tomography</h2><p><strong>Authors:Ibrahim Ethem Hamamci, Sezgin Er, Chenyu Wang, Furkan Almas, Ayse Gulnihan Simsek, Sevval Nil Esirgun, Irem Doga, Omer Faruk Durugol, Weicheng Dai, Murong Xu, Muhammed Furkan Dasdelen, Bastian Wittmann, Tamaz Amiranashvili, Enis Simsar, Mehmet Simsar, Emine Bensu Erdemir, Abdullah Alanbay, Anjany Sekuboyina, Berkan Lafci, Christian Bluethgen, Kayhan Batmanghelich, Mehmet Kemal Ozdemir, Bjoern Menze</strong></p>
<p>While computer vision has achieved tremendous success with multimodal encoding and direct textual interaction with images via chat-based large language models, similar advancements in medical imaging AI, particularly in 3D imaging, have been limited due to the scarcity of comprehensive datasets. To address this critical gap, we introduce CT-RATE, the first dataset that pairs 3D medical images with corresponding textual reports. CT-RATE comprises 25,692 non-contrast 3D chest CT scans from 21,304 unique patients. Through various reconstructions, these scans are expanded to 50,188 volumes, totaling over 14.3 million 2D slices. Each scan is accompanied by its corresponding radiology report. Leveraging CT-RATE, we develop CT-CLIP, a CT-focused contrastive language-image pretraining framework designed for broad applications without the need for task-specific training. We demonstrate how CT-CLIP can be used in two tasks: multi-abnormality detection and case retrieval. Remarkably, in multi-abnormality detection, CT-CLIP outperforms state-of-the-art fully supervised models across all key metrics, effectively eliminating the need for manual annotation. In case retrieval, it efficiently retrieves relevant cases using either image or textual queries, thereby enhancing knowledge dissemination. By combining CT-CLIPâ€™s vision encoder with a pretrained large language model, we create CT-CHAT, a vision-language foundational chat model for 3D chest CT volumes. Finetuned on over 2.7 million question-answer pairs derived from the CT-RATE dataset, CT-CHAT surpasses other multimodal AI assistants, underscoring the necessity for specialized methods in 3D medical imaging. Collectively, the open-source release of CT-RATE, CT-CLIP, and CT-CHAT not only addresses critical challenges in 3D medical imaging, but also lays the groundwork for future innovations in medical AI and improved patient care. </p>
<blockquote>
<p>è™½ç„¶è®¡ç®—æœºè§†è§‰åœ¨å¤šæ¨¡æ€ç¼–ç å’ŒåŸºäºèŠå¤©çš„å¤§å‹è¯­è¨€æ¨¡å‹çš„ç›´æ¥æ–‡æœ¬ä¸å›¾åƒäº¤äº’æ–¹é¢å–å¾—äº†å·¨å¤§çš„æˆåŠŸï¼Œä½†åœ¨åŒ»å­¦å½±åƒäººå·¥æ™ºèƒ½é¢†åŸŸï¼Œç‰¹åˆ«æ˜¯åœ¨3Dæˆåƒæ–¹é¢çš„è¿›å±•å´ç›¸å¯¹æœ‰é™ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºç¼ºä¹ç»¼åˆæ•°æ®é›†ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å…³é”®å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†CT-RATEæ•°æ®é›†ï¼Œå®ƒæ˜¯é¦–ä¸ªå°†3DåŒ»å­¦å›¾åƒä¸ç›¸åº”çš„æ–‡æœ¬æŠ¥å‘Šé…å¯¹çš„æ•°æ®é›†ã€‚CT-RATEåŒ…å«äº†æ¥è‡ª21,304åç‹¬ç‰¹æ‚£è€…çš„25,692ä¸ªéå¯¹æ¯”å‰‚3Dèƒ¸éƒ¨CTæ‰«æã€‚é€šè¿‡å„ç§é‡å»ºï¼Œè¿™äº›æ‰«æè¢«æ‰©å±•ä¸ºè¶…è¿‡5ä¸‡ä¸ªä½“ç§¯ï¼Œæ€»è®¡è¶…è¿‡ä¸€åƒå››ç™¾ä¸‡ä¸ªäºŒç»´åˆ‡ç‰‡ã€‚æ¯ä¸ªæ‰«æéƒ½é™„æœ‰ç›¸åº”çš„æ”¾å°„å­¦æŠ¥å‘Šã€‚å€ŸåŠ©CT-RATEï¼Œæˆ‘ä»¬å¼€å‘äº†CT-CLIPæ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“æ³¨äºCTçš„å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒæ¡†æ¶ï¼Œé€‚ç”¨äºå¹¿æ³›çš„åº”ç”¨åœºæ™¯ï¼Œæ— éœ€ç‰¹å®šä»»åŠ¡è®­ç»ƒã€‚æˆ‘ä»¬å±•ç¤ºäº†å¦‚ä½•åœ¨ä¸¤ä¸ªä»»åŠ¡ä¸­ä½¿ç”¨CT-CLIPï¼šå¤šå¼‚å¸¸æ£€æµ‹ä¸æ¡ˆä¾‹æ£€ç´¢ã€‚åœ¨å¤šå¼‚å¸¸æ£€æµ‹ä¸­ï¼ŒCT-CLIPåœ¨æ‰€æœ‰å…³é”®æŒ‡æ ‡ä¸Šéƒ½è¶…è¶Šäº†æœ€å…ˆè¿›çš„å…¨ç›‘ç£æ¨¡å‹ï¼Œæœ‰æ•ˆåœ°æ¶ˆé™¤äº†å¯¹äººå·¥æ³¨é‡Šçš„éœ€æ±‚ã€‚åœ¨æ¡ˆä¾‹æ£€ç´¢ä¸­ï¼Œå®ƒèƒ½å¤Ÿé«˜æ•ˆåœ°ä½¿ç”¨å›¾åƒæˆ–æ–‡æœ¬æŸ¥è¯¢æ¥æ£€ç´¢ç›¸å…³æ¡ˆä¾‹ï¼Œä»è€Œæé«˜çŸ¥è¯†ä¼ æ’­æ•ˆç‡ã€‚é€šè¿‡å°†CT-CLIPçš„è§†è§‰ç¼–ç å™¨ä¸é¢„è®­ç»ƒçš„çš„å¤§å‹è¯­è¨€æ¨¡å‹ç›¸ç»“åˆï¼Œæˆ‘ä»¬åˆ›å»ºäº†ç”¨äºä¸‰ç»´èƒ¸éƒ¨CTä½“ç§¯çš„CT-CHATåŸºç¡€èŠå¤©æ¨¡å‹ã€‚ä½¿ç”¨ç»è¿‡åœ¨è¶…è¿‡åƒä¸‡é—®ç­”æ¡ˆå¯¹è¿›è¡Œè®­ç»ƒçš„CT-CHATè¶…è¶Šå…¶ä»–å¤šæ¨¡æ€AIåŠ©æ‰‹ï¼Œå‡¸æ˜¾å‡ºåœ¨ä¸‰ç»´åŒ»å­¦å½±åƒä¸­é‡‡ç”¨ä¸“é—¨æ–¹æ³•çš„å¿…è¦æ€§ã€‚æ€»ä¹‹ï¼ŒCT-RATEã€CT-CLIPå’ŒCT-CHATçš„å¼€æºå‘å¸ƒä¸ä»…è§£å†³äº†ä¸‰ç»´åŒ»å­¦å½±åƒä¸­çš„å…³é”®æŒ‘æˆ˜ï¼Œä¹Ÿä¸ºæœªæ¥åŒ»ç–—äººå·¥æ™ºèƒ½çš„åˆ›æ–°å’Œæ‚£è€…æŠ¤ç†çš„æ”¹å–„å¥ å®šäº†åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.17834v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºè®¡ç®—æœºè§†è§‰æŠ€æœ¯åœ¨å¤šæ¨¡æ€ç¼–ç å’ŒåŸºäºèŠå¤©çš„å¤§å‹è¯­è¨€æ¨¡å‹ä¸å›¾åƒç›´æ¥äº¤äº’æ–¹é¢çš„å·¨å¤§æˆåŠŸï¼ŒåŒ»ç–—å½±åƒäººå·¥æ™ºèƒ½ï¼ˆAIï¼‰åœ¨3Dæˆåƒæ–¹é¢çš„è¿›å±•å´ç›¸å¯¹æœ‰é™ï¼Œä¸»è¦ç”±äºç¼ºä¹å…¨é¢çš„æ•°æ®é›†ã€‚ä¸ºè§£å†³è¿™ä¸€å…³é”®ç©ºç™½ï¼Œæˆ‘ä»¬æ¨å‡ºäº†CT-RATEæ•°æ®é›†ï¼Œå®ƒé¦–æ¬¡å°†3DåŒ»å­¦å›¾åƒä¸ç›¸åº”çš„æ–‡æœ¬æŠ¥å‘Šé…å¯¹ã€‚CT-RATEåŒ…å«äº†æ¥è‡ª21,304åç‹¬ç‰¹æ‚£è€…çš„25,692ä¸ªéå¯¹æ¯”æ€§3Dèƒ¸éƒ¨CTæ‰«æã€‚é€šè¿‡è¿™äº›æ‰«æçš„å¤šç§é‡å»ºï¼Œæ•°æ®é‡æ‰©å±•è‡³è¶…è¿‡åƒä¸‡ä¸ªäºŒç»´åˆ‡ç‰‡ã€‚æ¯ä¸ªæ‰«æå‡é™„æœ‰ç›¸åº”çš„æ”¾å°„å­¦æŠ¥å‘Šã€‚æˆ‘ä»¬åˆ©ç”¨CT-RATEå¼€å‘å‡ºäº†CT-CLIPï¼Œä¸€ä¸ªæ— éœ€ç‰¹å®šä»»åŠ¡è®­ç»ƒçš„é€šç”¨å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒæ¡†æ¶ã€‚åœ¨å¼‚å¸¸æ£€æµ‹ä¸æ¡ˆä¾‹æ£€ç´¢ä¸¤é¡¹ä»»åŠ¡ä¸­ï¼ŒCT-CLIPè¡¨ç°å‡ºäº†å¼ºå¤§çš„æ€§èƒ½ã€‚å¼‚å¸¸æ£€æµ‹æ–¹é¢ï¼ŒCT-CLIPåœ¨æ‰€æœ‰å…³é”®æŒ‡æ ‡ä¸Šå‡è¶…è¶Šäº†æœ€å…ˆè¿›çš„å…¨ç›‘ç£æ¨¡å‹ï¼Œæ— éœ€æ‰‹åŠ¨æ ‡æ³¨ï¼›æ¡ˆä¾‹æ£€ç´¢æ–¹é¢ï¼Œå®ƒèƒ½å¤Ÿé«˜æ•ˆåœ°ä½¿ç”¨å›¾åƒæˆ–æ–‡æœ¬æŸ¥è¯¢æ£€ç´¢ç›¸å…³æ¡ˆä¾‹ï¼Œä¿ƒè¿›çŸ¥è¯†ä¼ æ’­ã€‚é€šè¿‡ç»“åˆCT-CLIPçš„è§†è§‰ç¼–ç å™¨å’Œé¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæˆ‘ä»¬åˆ›å»ºäº†é’ˆå¯¹ä¸‰ç»´èƒ¸éƒ¨CTä½“ç§¯çš„è§†è¯­è¨€åŸºç¡€èŠå¤©æ¨¡å‹CT-CHATã€‚åœ¨ç”±CT-RATEæ•°æ®é›†æ´¾ç”Ÿçš„æ•°ç™¾ä¸‡é—®ç­”å¯¹ä¸Šå¾®è°ƒåï¼ŒCT-CHATè¶…è¶Šäº†å…¶ä»–å¤šæ¨¡æ€AIåŠ©æ‰‹ï¼Œçªæ˜¾äº†åŒ»å­¦ä¸‰ç»´æˆåƒéœ€è¦ä¸“ä¸šæ–¹æ³•çš„é‡è¦æ€§ã€‚æ€»ä½“è€Œè¨€ï¼ŒCT-RATEã€CT-CLIPå’ŒCT-CHATçš„å¼€æºå‘å¸ƒä¸ä»…è§£å†³äº†åŒ»å­¦ä¸‰ç»´æˆåƒçš„å…³é”®æŒ‘æˆ˜ï¼Œè¿˜ä¸ºåŒ»ç–—äººå·¥æ™ºèƒ½çš„æœªæ¥å‘å±•åŠæ‚£è€…æŠ¤ç†çš„æ”¹å–„å¥ å®šäº†åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¼ºä¹å…¨é¢çš„æ•°æ®é›†é™åˆ¶äº†åŒ»ç–—å½±åƒAIåœ¨3Dæˆåƒæ–¹é¢çš„è¿›å±•ã€‚</li>
<li>å¼•å…¥CT-RATEæ•°æ®é›†ï¼Œå®ç°3DåŒ»å­¦å›¾åƒä¸ç›¸åº”æ–‡æœ¬æŠ¥å‘Šçš„é…å¯¹ã€‚</li>
<li>å¼€å‘CT-CLIPé¢„è®­ç»ƒæ¡†æ¶ï¼Œç”¨äºå¤šæ¨¡æ€è¯­è¨€å›¾åƒå¯¹æ¯”ä»»åŠ¡ï¼Œè¡¨ç°ä¼˜å¼‚äºå¤šé¡¹å…³é”®ä»»åŠ¡æŒ‡æ ‡ã€‚</li>
<li>CT-CLIPåœ¨å¤šå¼‚å¸¸æ£€æµ‹æ–¹é¢è¶…è¶Šå…ˆè¿›æ¨¡å‹ï¼Œæ— éœ€æ‰‹åŠ¨æ ‡æ³¨ã€‚</li>
<li>CT-CLIPæ”¯æŒæ¡ˆä¾‹æ£€ç´¢åŠŸèƒ½ï¼Œä¿ƒè¿›çŸ¥è¯†ä¼ æ’­ã€‚</li>
<li>ç»“åˆè§†è§‰ç¼–ç å™¨å’Œå¤§å‹è¯­è¨€æ¨¡å‹åˆ›å»ºCT-CHATèŠå¤©æ¨¡å‹ï¼Œé’ˆå¯¹ä¸‰ç»´èƒ¸éƒ¨CTä½“ç§¯æä¾›åŸºç¡€åŒ»ç–—æœåŠ¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.17834">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9c97db3d6d4caa85209a4213a73ab6df.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-08/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-08/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-08/TTS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-fa2f50df8e5531a194a7cb9112035c3f.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-08  RWKVTTS Yet another TTS based on RWKV-7
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-08
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-08/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-41e1a901a3024111ae1b95393e3c2a50.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-08  Multimodal Diffusion Bridge with Attention-Based SAR Fusion for   Satellite Image Cloud Removal
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-08
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">17124.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
